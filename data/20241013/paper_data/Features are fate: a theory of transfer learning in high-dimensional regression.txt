Features are fate: a theory of transfer learning in
high-dimensional regression
JavanTahir*,SuryaGanguli†,GrantM.Rotskoff‡
Abstract
With the emergence of large-scale pre-trained neural networks, methods to adapt such “foun-
dation”modelstodata-limiteddownstreamtaskshavebecomeanecessity. Fine-tuning, preference
optimization,andtransferlearninghaveallbeensuccessfullyemployedforthesepurposeswhenthe
targettaskcloselyresemblesthesourcetask,butaprecisetheoreticalunderstandingof“tasksimilar-
ity”isstilllacking. Whileconventionalwisdomsuggeststhatsimplemeasuresofsimilaritybetween
sourceandtargetdistributions,suchasϕ-divergencesorintegralprobabilitymetrics,candirectlypre-
dict the success of transfer, we prove the surprising fact that, in general, this is not the case. We
adopt,instead,afeature-centricviewpointontransferlearningandestablishanumberoftheoretical
resultsthatdemonstratethatwhenthetargettaskiswellrepresentedbythefeaturespaceofthepre-
trainedmodel, transferlearningoutperformstrainingfromscratch. Westudydeeplinearnetworks
asaminimalmodeloftransferlearninginwhichwecananalyticallycharacterizethetransferability
phasediagramasafunctionofthetargetdatasetsizeandthefeaturespaceoverlap. Forthismodel,
weestablishrigorouslythatwhenthefeaturespaceoverlapbetweenthesourceandtargettasksissuf-
ficientlystrong,bothlineartransferandfine-tuningimproveperformance,especiallyinthelowdata
limit. Theseresultsbuildonanemergingunderstandingoffeaturelearningdynamicsindeeplinear
networks,andwedemonstratenumericallythattherigorousresultswederiveforthelinearcasealso
applytononlinearnetworks.
1 Introduction
Stateoftheartneuralnetworkmodelshavebillionstotrillionsofparametersandaretrainedondatasets
ofasimilarscale. Thebenefitsofdatasetscalearemanifestintheastoundinggeneralizationcapability
ofthese foundationmodels (Bahriet al.,2024). Formany applications, however, datasets ofthe scale
usedfornaturallanguageprocessingorcomputervisionarehard,ifnotimpossible,togenerate.
Toalleviatetheproblemofinadequatedatasetscale,therepresentationsofafoundationmodelseem
toprovideausefulinductivebiasforadaptationtoatargettask.
Whiletheyarenowubiquitous,transferlearningmethodslackasolidtheoreticalfoundationoral-
gorithmicdesignprinciples. Assuch,itremainsdifficulttopredictwhen—andwithwhichapproach—
transferlearningwilloutperformtrainingonthetargettaskalone.Intuitively,ifthesourcetaskresembles
*DepartmentofAppliedPhysics,StanfordUniversity,348ViaPueblo,Stanford,CA94305,USAjavan@stanford.edu
†Department of Applied Physics, Stanford University, 348 Via Pueblo, Stanford, CA 94305, USA
sganguli@stanford.edu
‡DepartmentofChemistry,InstituteforComputationalandMathematicalEngineering,StanfordUniversity,380RothWay,
Stanford,CA94305,USArotskoff@stanford.edu
1
4202
tcO
01
]LM.tats[
1v49180.0142:viXrathe target task, transfer learning should be beneficial. The important question of how to quantify task
relatedness is one that remains unanswered. In this work, we address this question and prove the sur-
prising fact that discrepancies between source and target data distributions can be misleading when it
comestotransferability. Weinsteadfindthatthefeaturespacelearnedduringpretrainingistherelevant
objectforpredictingtransferperformance,whichmeansthatmodel-agnosticmetricsbetweentasksare
unlikely to successfully predict task overlap. Of course, adopting a feature-centric viewpoint creates
model-specificchallengesbecauseunambiguouslyidentifyinglearnedfeaturesremainsanoutstanding
anddifficultcharacterizationproblemfordeepneuralnetworks. Forthisreason,inthisworkwefocus
ondeeplinearnetworkstrainedwithgradientflow,asfeaturelearningdynamicsarewell-understoodin
thissetting. Wedevelopanintuitiveunderstandingoflineartransferandfullfine-tuninginthismodel.
Incontrasttootherrecentwork,wequantifytransferperformancerelativetotrainingonthetargettask
aloneandpreciselyidentifywhentransferlearningleadstoimprovedperformance,effectivelybuilding
aphasediagramfortransferefficiency.Finally,weshowinnumericalexperimentsthatthispictureholds
qualitativelyfornonlinearnetworks,aswell.
1.1 Relatedwork
• Theoretical aspects of transfer learning A number of recent works have studied theoretical
aspectsoftransferlearning,focusingontheriskassociatedwithvarioustransferalgorithms. Wu
et al. (2020) use information theory to derive bounds on the risk of transfer learning using a
mixtureofsourceandtargetdata. Shiltonetal.(2017)analyzetransferinthecontextofgaussian
process regression. Tripuraneni et al. (2020) work in a fairly general setting, and derive bounds
on the generalization error of transferred models through a complexity argument, highlighting
theimportanceoffeaturediversityamongtasks. Liuetal.(2019);Neyshaburetal.(2020)study
transferlearningfromtheperspectiveofthelosslandscapeandfindthattransferredmodelsoften
findflatterminimathanthosetrainedfromscratch. Consistentwithourfeature-centricviewpoint,
Kumaretal.(2022)showthatfine-tuningcandistortthepretrainedfeatures,leadingtopoorout
ofdistributionbehavior.
• TransferlearninginsolvablemodelsSimilartoourapproach,severaltheoryworkshaveworked
withanalyticallytractablemodelstomorepreciselycharacterizetransferperformance. Lampinen
& Ganguli (2018); Atanasov et al. (2021); Shachaf et al. (2021) also study transfer learning in
deeplinearnetworks,butfocusonthegeneralizationerroralone,notthetransferabilityrelativeto
ascratchtrainedbaseline, whichobfuscatestheconditionsfortransferlearningtobebeneficial.
Geraceetal.(2022)studiestransferlearningwithsmallnonlinearnetworkswithdatagenerated
froma“hiddenmanifold“(Goldtetal.,2020)andfindtransferlearningtobeeffectivewhentasks
areverysimilar,anddataisscarce,butdonottheoreticallydescriberegionsofnegativetransfer.
Saglietti & Zdeborova (2022) studies knowledge distillation in a solvable model, which can be
viewedasaspecialcaseoftransferlearning.
• Metrics for transferability In an effort to define task similarity, there has been recent interest
(Gao&Chaudhari,2021;Nguyenetal.,2021;Alvarez-Melis&Fusi,2020)indesigningmetrics
topredicttransferperformance,anapproachwhosevalidityweassessinSection2.1.
2• FeaturelearningThenotionoffeaturelearningiscentraltoourresults. Whiletherich,feature
learningregimeisoftenheuristicallydefinedastheoppositeoftheneuraltangentregime(Jacot
etal.,2018),aprecisedefinitionisstilllacking. Nevertheless,therehasbeenanexplosionofin-
terestinunderstandingdynamicsinthesetworegimesofneuralnetworkoptimizationWoodworth
et al. (2020); Atanasov et al. (2021); Yang & Hu (2021); Kunin et al. (2024); Yun et al. (2021);
Chizat(2020)focusontheroleofinitialization,learningrate,andimplicitbiasinfeaturelearning.
Petrini et al. (2022) highlights the potential for overfitting when training in the feature learning
regime.
2 General theoretical setting
Webeginbyintroducingthegeneraltheoreticalframeworkunderwhichwestudytransferlearning. We
consider source and target regression tasks defined by probability distributions p (x,y) and p (x,y)
s t
over inputs x Rd and labels y R. We focus on label shift, in which p (x,y) = p(x)p (y x)
s s
∈ ∈ |
andp (x,y)=p(x)p (y x)forthesameinputdistributionp(x). Thelabelsaregeneratedfromnoisy
t t
|
samplesofsourceandtargetfunctions
y =f∗(x)+ϵ (1)
s s
y =f∗(x)+ϵ (2)
t t
wheref∗(x),f∗(x) L (p(x))andϵ (0,σ2). Duringpretraining,wetrainamodelwithparam-
s t ∈ 2 ∼ N
etersΘ=(c,θ)oftheform
m
(cid:88)
f(x;Θ)= c ϕ (x;θ) (3)
i i
i=1
onthesourcetaskusingameansquaredloss. Notethatthefeaturesϕ(x,θ)areleftgeneralandcould
forexamplerepresentfinalhiddenactivationsofadeepneuralnetwork. Afterpretraining,themodelis
transferredbytrainingasubsetoftheparametersΘ′ Θonthetargettask,whileleavingΘ Θ′ at
⊂ −
theirpretrainedvalues. Tomodelthemodernsettingfortransferlearning,inwhichthenumberofdata
pointsinthesourcetaskfarexceedsthoseinthetargettask,wetrainthesourcetaskonthepopulation
distributionandthetargettaskonafinitedataset ofnindependentsamples.
D
1
(Θ)= E [(f(x,Θ) y)2] (4)
Ls 2 ps(x,y) −
1
(Θ′)= Eˆ [(f(x,Θ) y)2] (5)
Lt 2 pt(x,y) −
where Eˆ (h(x,y)) = 1 (cid:80)n h(x ,y ) is the expectation over the empirical distribution of . We
p n i=1 i i D
focusontwowidelyemployedtransfermethods,lineartransfer andfine-tuning. Inlineartransfer,the
pretrained features ϕ(x,θ) are frozen and only the output weights c are trained on the target task. In
fine-tuning,theentiresetofparametersΘaretrainedfromthepretrainedinitializationonthetargettask.
Tooptimizethelossfunctions(4)and(5),weusegradientflow,
dΘ
i = (Θ), (6)
dt −∇ΘiL
wherewehavesetthelearningrateequaltounityforthepurposeofanalysis. Toassesstheperformance
of transfer learning we compare the performance of the transferred model to a scratch-trained model
3withthesamearchitecture(3)trainedonlyonthetargettaskfromarandominitialization. Weintroduce
thetransferabilitytoquantifythisrelationship:
=E ( ) (7)
D sc tx
T R −R
where E is the expectation over iid draws of the training set and and are the generalization
D tx sc
R R
errors of the transferred model and scratch trained model, respectively, where the generalization error
(orpopulationrisk)isgivenby,
=E [(f(x,Θ) f∗(x)2]. (8)
p(x)
R −
Weconsidertransferlearningsuccessfulwhen >0,i.e.,whentheexpectedgeneralizationoftransfer
T
learningoutperformstrainingfromscratchonthetargettask.Werefertothesituation <0asnegative
T
transfer,sincepretrainingleadstodegradationofthegeneralizationerror.
2.1 Datasetsimilarityisnotpredictiveoftransferefficiency
Thecommonwisdomintransferlearningisthatrelatedtasksshouldtransfereffectivelytooneanother.
However,astandardandmathematicallyprecisedefinitionoftaskrelatednessiscurrentlylacking. One
reasonablenotionoftaskrelatednessistocomparethesourceandtargetdatadistributionsp andp us-
s t
ingadiscrepancymeasurebetweenprobabilitydistributions,forexampleanintegralprobabilitymetric
(IPM) or a ϕ-divergence Sriperumbudur et al. (2009). While ϕ-divergences like the Kullback-Leibler
divergencearewell-known,theyareoftenhardtocomputeforhigh-dimensionaldistributions. Wuetal.
(2020); Nguyen et al. (2021) suggest that these kinds of measures will correlate with transfer perfor-
mance, as measured by generalization error on the target task. However, we argue that a meaningful
measureoftransferperformancemustcomparetoascratchtrainedbaseline,nottargettaskperformance
alone. IPMs, such as the Wasserstein-1 Distance, Dudley Metric, and Kernel Mean Discrepency, are
bonafidemetricsonthespaceofprobabilitydistributionsandareoftheoreticalimportanceinoptimal
transport,statistics,andprobabilitytheory. Usingideasfromoptimaltransport,(Alvarez-Melis&Fusi,
2020)attempttocorrelatetransferperformancewiththeWassersteindistance. Whileitmayseemthat
closenessoftaskdistributionsshouldcorrelatewithtransferperformance,weshowthatthisisnotnec-
essarilythecase. Inparticular,weselectamemberofeachfamilyandprovethat,withinourmodel,one
canachievepositivetransfer( >0)withdistributionsthatarearbitrarilyfarapart.
T
Twofunctionsrepresentablewiththesamefeaturescanbe“farapart”.Weformalizethisnotionwith
thefollowingtheorem.
Assumption 2.1. We assume f L (Rd,p) and for each x Rd we define the random variable
2
y : Rd R through the relatio∈ n y = f(x)+ϵ with ϵ (∈ 0,σ2). Let p (x,y) denote the joint
f
→ ∼ N
probability density of x and y. We assume Φ L (p) is a linear subspace with orthonormal basis
2
⊂
ϕ M andM maybeinfinite.
{ i }i=1
Theorem2.2. Assume2.1. Thenforanyf Φ,andanyδ >0thereexistsg Φsuchthat
∈ ∈
γ (p ,p ) δ
β f g
≥
whereγ (p,p′)istheDudleyMetric. Similarly,foranyf Φ,andanyδ > 0thereexistsg Φsuch
β
∈ ∈
that
D (p p ) δ
KL f g
∥ ≥
whereD (p p )istheKullbackLeiblerdivergence.
KL f g
∥
4WeprovethistheoreminAppendixB.1. Wenotethatthisrelationshiptheoremalsoholdsforany
IPMoverafunctionclassthatislargerthantheclassofBoundedLipschitzfunctions. Inparticular,the
theoremholdsfortheMonge-Kantorovich(W )metric,sinceanyfunctionthatsatisfies f 1also
1 BL
∥ ∥ ≤
satisfies f 1. BecauseitissimplertocomputeW numerically,inFig.5weplottheWasserstein
L 1
∥ ∥≤
distance,nottheDudleymetric.
Theorem 2.2 demonstrates that for a given source distribution, one can always find a target distri-
bution generated from the same feature space that is arbitrarily distant with respect to these metrics.
However, despite such distance, intuitively, if the source and target functions lie in the same feature
spaceandpretrainingcreatesabasisforthisspace,transfertothetargettaskshouldlikelybepositive.
Weshowthisisindeedthecasefordeeplinearnetworksinthefollowingsection.
3 Deep linear networks: an exactly solvable model
AsananalyticallysolvablemodeloftransferlearningweconsideradeeplinearnetworkwithLlayers
f(x)=xTW W ...W (9)
1 2 L
where W
l
Rdl−1×dl for l [1,2,...L 1] and W
L
RdL−1×1. For notational convenience we
∈ ∈ − ∈
haverenamedcin(3)asW andforsimplicitywesetd = d = = d = d,thedimensionof
L 0 1 L−1
thedata. TheparametermatricesareinitializedasW (0)=αW¯ wh· e· r· eα R. Sincetransferlearning
l l
∈
relies on learning features in the source task, we initialize the network in the feature learning regime
α 0. Inthefollowing,weassume:
→
Assumption3.1. Assumethattheinputdatax Rd isnormallydistributedandthateachdataset
∈ D
consistsofnpairs (x ,y ) n samplediidfromp withGaussianlabelnoiseofvarianceσ2.
{ i i }i=1 t
Assumption3.2. WeassumethatthesourceandtargetfunctionsareeachlinearfunctionsinL (Rd,p);
2
equivalently,f∗(x)=βTx,f∗(x)=βTxwith β 2 = β 2.
s s t t ∥ s ∥2 ∥ t ∥2
To control the level of source-target task similarity, we fix the angle θ between the ground truth
source and target functions so that βTβ = cosθ. The source and target loss functions are given
s t
by (4) and (5). When training over the empirical loss, it is convenient to work in vector notation
( W ) = 1 y XW W ...W 2 whereX Rn×d andy Rn. Westudythismodelin
Lt { l≤L } 2n∥ t − 1 2 L ∥2 ∈ ∈
thehighdimensionallimitinwhichγ =n/dremainsconstantasn,d .
→∞
3.1 Pretrainedmodelsrepresentsourcefeatures
To describe transfer efficiency in this setup, we need to understand the function that the model imple-
mentsaftertrainingonthesourcetask. Wecandescribethenetworkinfunctionspacebytrackingthe
evolution of β(t) = W W ...W under gradient flow, such that the network function at any point
1 2 L
in the optimization is f(x;t) = β(t)Tx. The following Lemma establishes that pretraining perfectly
learnsthesourcetaskinthelargesourcedatalimit.
Lemma3.3. Undergradientflow(6)onthepopulationriskobjective(4)withinitializationsatisfying
(22),lim β(t)=β
t→∞ s
We prove Lemma 3.3 in Appendix B.2. While this result establishes recovery of the ground truth
onthesourcetask,itdoesnotdescribethefeaturespaceofthepretrainedmodel,whichisrelevantfor
transferability. To this end, following Yun et al. (2021), we show that in the feature learning regime
α 0,thehiddenfeaturesofthemodelsparsifytothosepresentinthesourcetask.
→
5Theorem3.4(Yunetal). LetthecolumnsofΦ = W W W denotethehiddenfeaturesofthe
1 2 L−1
···
model. Afterpretraining
lim lim Φ=β vT
s L−1
α→0t→∞
forsomevectorv .
L−1
WeproveTheorem3.4inAppendixB.3. Theorem3.4demonstratesthatafterpretraininginthefea-
turelearningregime,thed-dimensionalfeaturespaceofthemodelparsimoniouslyrepresentstheground
truthfunctioninasingle, rank-onecomponent. Werefertothisphenomenonasfeaturesparsification,
whichisahallmarkofthefeaturelearningregime,andhasimportantconsequencesfortransferability,
particularlyinthelinearsettingSection3.3.
3.2 Scratchtrainedmodelsrepresentminimumnormsolutions
Fortheempiricaltrainingobjective5,therearemultiplezerotrainingerrorsolutionswhenthemodelis
overparameterizedγ < 1. AsnotedinYunetal.(2021)andAtanasovetal.(2021),thereisanimplicit
biasofgradientflowtotheminimumnormsolutionwhenα 0
→
Theorem3.5((Yunetal.,2021)). Undergradientflowontheempiricalriskminimizationobjective(5)
withinitializationsatisfying(22),lim lim β(t)=βˆ,whereβˆistheminimumnormsolutionto
α→0 t→∞
thelinearleastsquaresproblem
1
βˆ=argmin y Xβ 2 =X+y
2n∥ t − ∥2 t
β
WeproveTheorem3.5inAppendixB.4. Knowingthefinalpredictoroftheempiricaltrainingalso
allowsustocomputethegeneralizationerrorofthescratchtrainedmodel
Theorem 3.6. Under gradient flow on the empirical objective (5), in the high dimensional limit the
expectationofthefinalgeneralizationerrorovertrainingdatais
(cid:40) (1−γ)2+γσ2
γ <1
E = 1−γ (10)
D R σ2 γ >1
γ−1
Theorem(3.6)isaknownresultforlinearregression(Hastieetal.,2022;Canataretal.,2021;Belkin
etal.,2019;Advani&Ganguli,2016;Mel&Ganguli,2021;Bartlettetal.,2020),butweprovideaproof
basedonrandomprojectionsandrandommatrixtheoryinSectionB.5. Thisexpressionexhibitsdouble
descentbehavior:intheoverparameterizedregime,thegeneralizationerrorfirstdecreases,thenbecomes
infiniteasγ 1, whileintheunderparameterizedregime, thegeneralizationerrormonotonicallyde-
→
creases with increasing γ. As we will see in Section 3.3, this double descent behavior leads to two
distinctregionsinthetransferabilityphasediagram.
3.3 Lineartransfer
Thesimplesttransferlearningmethodisknownaslineartransfer,inwhichonlythefinallayerweightsof
thepretrainednetworkaretrainedonthetargettask.Inparticular, W arefixedafterpretraining
l l≤L−1
{ }
andWˆ solvesthelinearregressionproblemwithfeaturesΦ=XW ...W .
L 1 L−1
1
Wˆ =argmin ΦW y 2 (11)
L 2n∥ L − t ∥2
Wˆ L∈Rd
6Whentherearemultiplesolutionstotheoptimizationproblem(11),wechoosethesolutionwithmini-
mumnorm. Wecharacterizethegeneralizationerroroflineartransferinthefollowingtheorem.
(a) (b)
2.0
(c)
1.00
θ=π/8
0.75 θ=π/4
1.5 0.50 θ=3π/8
1 0.25
γ
0.5 1.0 T 0.00
0 0.25
−
-0.5 0.5 −0.50
-1
0
−0.75
0.5 γ1.0 1.5 2.0 π/2 π/3 θπ/6 0 0 π/6 π/3 π/2 −1.00 0 0.5 1 1.5 2
θ γ
Figure 1: Linear transferability phase diagram. We pretrain a linear network (9) with L = 2 and
d = 500 to produce labels from linear source function y = βTx+ϵ using the population loss (4).
s
We then retrain the final layer weights on a sample of n = γd points (x ,y = βTx + ϵ ) where
i i t i i
βTβ = cosθandϵ (0,σ = 0.2),andcompareitsgeneralizationerrortothatofamodeltrained
s t i ∼ N
fromscratchonthetargetdataset. (a)Thetheoreticaltransferabilitysurface(13)asafunctionoftarget
datasetsizeγ andtaskoverlapθ. Lightbluelinesindicatetheboundarybetweenpositiveandnegative
transfer. (b)Top-downviewofFig. 1(a)shadedbysignoftransferability. Redregionsindicatenegative
transfer < 0, blue region indicates positive transfer > 0. (c) Slices of the transferability surface
T T
(13)forconstantθ. Solidlinesrepresenttheoreticalvalues,circlesarepointsfromexperiments. Error
barsrepresentthestandarddeviationover20drawsofthetargetset.
Theorem3.7. UnderAssumptions3.1and3.2,andassumingthesource-targetoverlapisθ,theexpected
generalizationerrorofthelinearlytransferredmodelisanexplicitfunctionofθ,thelabelnoiseσ,and
thedatasetsizen:
σ2+sin2θ
E =sin2θ+ . (12)
D Rlt n 2
−
We prove Theorem 3.7 in Appendix B.6. The structure of the result in Theorem 3.7 merits some
discussion. After pretraining in the feature learning regime α 0, the feature space of the network
→
has sparsified so that it can only express functions along β (Theorem 3.4). Since the features of the
s
network cannot change in linear transfer, the main contribution to the generalization error is sin2θ,
whichcanbeviewedasthenormoftheprojectionofthetargetfunctionintothespaceorthogonaltothe
featuresspannedbythepretrainednetwork. Thisisanirreducibleerrorthatisthebestcaseriskgiven
that the features cannot change. The second term comes from the finiteness of the training set. Since
lineartransferlearnsfromafinitesampleoftrainingpoints,minimizingthetrainingerrorcaneffectively
“overfitthenoise”andthelearnedfunctiondistortsawayfromthegroundtruth.
Luckily, sincethepretrainedfeaturespacehassparsified, theeffectoffinitesamplingandadditive
label noise decays as 1/n, effectively filtering out the d-dimensional noise by projecting it onto
∼
7a single vector. Compare this to the generalization of the scratch trained network (10). There, the
featuresoftheequivalentlinearregressionproblem,X,havesupportoveralld-dimensions,sothereis
noirreducibleerrorterm. Theexpressivity,however,comesatacost. Eachdimensionoftheregression
vectorisvulnerabletonoiseinthetrainingdata,andtheprojectionofthetargetfunctionontothefeature
spaceisstronglydistortedduetofinitesampling(i.e. γ). Wecanpreciselyanalyzethistradeoffby
∼
comparing(10)and(12). Inthelimitn,d ,thetransferability(7)is
→∞
(cid:40) (1−γ)2+γσ2 sin2θ γ <1
= 1−γ − (13)
Tlt σ2 sin2θ γ >1
γ−1 −
whichisplottedinFig. 1(a). From(13)wecanidentifytheregionsofnegativetransferforthismodel,
whichareshadedinredinFig.1(b).Intheunderparameterizedregime(γ >1),thereisnegativetransfer
forallγ 1 > σ2 . Inwords,atfixedγ andσ,i.e.,fixingthenumberofdatapointsandlabelnoise,
− sin2θ
asthenormoftheout-of-subspacecomponentincreases,transferefficiencydegrades.
In the overparameterized regime (γ < 1), negative transfer only occurs when σ < 1.This can
be viewed as a condition on the signal-to-noise ratio of the target data: SNR = β 2/σ2 = 1/σ2.
∥ t ∥2
WhenSNR < 1,scratchtrainingcanneverrecovertheunderlyingvectorβ andpretrainingisalways
t
beneficial. WhenSNR>1,negativetransferoccurswhenθ (arccos(1 σ),π/2)andγ (γ ,γ )
+ −
where γ = 1[(1+cos2θ σ2) (cid:112) (1+cos2θ σ2)2 ∈ 4cos2θ]. I− n the noiseless ca∈ se σ 0,
± 2 − ± − − →
this expression simplifies to θ (0,π/2), cos2θ < γ < 1 (see Appendix E Fig. 6). This condition
∈
requires that there is more data than the there is target function power in the direction learned during
pretraining. Asσ increases,theregionofnegativetransfershrinks,sincethenoisecorruptsthescratch
trainedaccuracy. FinallywementionthatthetworegionsofnegativetransferinFig. 1areseparatedby
positivetransferthatpersistsevenwhenθ =π/2. Wedubthiseffectanomalouspositivetransfer,since
thepretrainedfeaturesarecompletelyorthogonaltothoseinthetarget,yettransferabilityisstillpositive.
Thiseffectstemsfromthedoubledescentphenomenoninscratchtraining—eventhoughlineartransfer
performance is as bad as possible, transferability is still positive, since the scratch trained model has
infinite generalization error. In Appendix E Fig. 5 we demonstrate that the dataset based discrepency
measures of Section 2.1 are indeed misleading: neither D , nor W are negatively correlated with
KL 1
increasedtransferability.
3.3.1 Ridgeregularizationcannotfixnegativetransfer
Intheprevioussection,thenetworksparsifiedtofeaturesthatincompletelydescribedthetargetfunction,
leading to negative transfer given sufficient target data. A common approach to mitigate this kind of
multicollinearityinlinearregressionistoaddanℓ penaltytotheregressionobjective(11)sothat
2
1 λ
Wˆ =argmin ΦW y 2+ W 2. (14)
L 2n∥ L − t ∥2 2∥ L ∥2
Wˆ L∈Rd
In the following theorem, which we prove in Appendix B.7, we show that the generalization error for
regularizedlineartransferisastrictlyincreasingfunctionoftheridgeparameterλ, leadingtoalarger
regionofnegativetransferforanyλ>0(Fig. 2).
Theorem3.8. UnderAssumptions3.1and3.2,andassumingthesource-targetoverlapisθ,theexpected
generalizationerroroftheridgelineartransfermodeloverthetrainingdatais
(1+2λ)
lim E λ =1 cos2θ (15)
n→∞ D Rlt − (1+λ)2
80.26 theory
experiment
0.24
0.22
λ
Rlt
0.20
0.18
0.16
0.0 0.2 0.4 0.6
λ
Figure 2: Ridge regularization leads to worse generalization in linear transfer. Linear transfer
generalization error for γ = 0.5 as a function of regularization parameter λ. The generalization error
isastrictlyincreasingfunctionofλ,whichimpliesthattheoptimalregularizerisλ = 0. Solidlineis
∗
theory(3.8),pointsareexperiments. Errorbarsrepresentthestandarddeviationover20realizationsof
thetargetdataset.
Ridge regression attenuates the power of the predictor in all directions of the data, including the
directionparalleltothesignal. DuetosparisificationofTheorem3.4,ℓ regularizationisnon-optimal
2
andhenceregularizationimpairsgeneralizationbyattenuatingusefulfeatures,i.e.,thosewithθ <π/2.
3.4 Fine-tuning
Another common transfer learning strategy is fine-tuning, in which all model parameters are trained
onthetargettaskfromthepre-trainedinitialization. Forgeneralnonlinearmodels,analyzingthelimit
points of gradient flow from arbitrary initialization is a notoriously difficult task. For the deep linear
modelhowever,wecansolvefortheexpectedgeneralizationerroroffine-tuningexactly.
Theorem3.9. UnderAssumptions3.1and3.2,andassumingthesource-targetoverlapisθ,theexpected
generalizationerrorofthefine-tunedmodeloverthetrainingdatais:
(cid:40)
E +(1 γ)(1 2cosθ) γ 1
E = D Rsc − − ≤ (16)
D Rft E γ >1
D sc
R
whereE istheexpectedgeneralizationerrorofthescratchtrainedmodel
D sc
R
Theorem3.9isproveninAppendixB.8. Theorem3.9yieldsanexpressionforthefine-tuningtrans-
ferability,whichisplottedinFig. 3(a):
(cid:40)
(γ 1)(1 2cosθ) γ 1
= − − ≤ (17)
ft
T 0 γ >1
92.0
(a) (b) (c) 0.8 θ=π/8
θ=π/4
1 1.5 0.6 θ=3π/8
0.5
0.4
0 γ 1.0 T
0.2
-0.5
0.5 0.0
-1 0
0.5 γ1.0 1.5 2.0 π/2 π/3 π θ/6 0 −0.2
0 π/6 π/3 π/2 0 0.5 1 1.5 2
θ γ
Figure3:Fine-tuningtransferabilitysurfaceUsingthesametransfersetupasinFig.1wefinetuneall
oftheweightsonthetargetdatasetstartingfromthepretrainedweightinitialization. (a)Thetheoretical
transferabilitysurface(17)asafunctionoftargetdatasetsizeγ andtaskoverlapθ. Thelightblueline
paralleltotheγaxisindicatestheboundarybetweenpositiveandnegativetransfer,whiletheoneparallel
totheθ axisindicatestheboundaryforzerotransferability. (b)Top-downviewofFig. 3(a)shadedby
sign of transferability. Red region indicates negative transfer < 0, blue region indicates positive
T
transfer > 0. Thewhiteregionindicatesnotransferbenefit = 0. (c)Slicesofthetransferability
T T
surface(17)forconstantθ. Solidlinesrepresenttheoreticalvalues,circlesarepointsfromexperiments.
Errorbarsrepresentthestandarddeviationover20drawsofthetargetset.
When the model is underparameterized γ > 1, there is a unique global minimum in the space of
β = W W W . Sincegradientflowconvergestoaglobalminimum, (Theorem3.5), finetuning
1 2 L
···
loses the memory of the pretrained initialization leading to zero transferability (white region in Fig.
3(b)). Whenthenetworkisoverparameterized,however,thereisasubspaceofglobalminima. Weshow
intheSectionB.8thatthepretrainedinitializationinducesanimplicitbiasofgradientflowawayfrom
the minimum norm solution. For θ < π/3, the pretrained features are beneficial, leading to positive
transfer. Forθ >π/3,however,thepretrainedfeaturesbiasthenetworktoostronglytowardthesource
task,leadingtonegativetransfer.
4 Student-teacher ReLU networks
Inthefollowing,wedemonstratethatmanyoftheresultsfromouranalyticallysolvablemodelalsohold,
qualitatively, in the more complicated setting of linear transfer with nonlinear networks. In particular,
wechooseamodeloftheform
m
1 (cid:88)
f(x)= c σ(wTx) (18)
m i i
i=1
whereσ(y)=max 0,y istheReLUactivation.Wescalethemodelby1/mtoplacethenetworkin
{ }
themeanfield,featurelearningregime(Chizatetal.,2019;Meietal.,2018;Rotskoff&Vanden-Eijnden,
2022). Asinthedeeplinearmodel,wechoosesourceandtargetfunctionsthatarerepresentablebythe
10(a) (c)
2.00 10−2 experiment (e)
1.75 fitν=-1.18 1.5
1.50 1000
1.25
1.00 Rsc 1.0
0.75 800
00 .. 25 50 10−3 0.5
0.00 102 103 n 600
(b) (d) n 0.0
2.00
11 .. 57 05 0.04 400 −0.5
11 .. 02 05 (cid:31)P (cid:31)f⊥ t∗f (cid:31)t∗ 2(cid:31)2 0.03 200 −1.0
0.75 0.02
0.50 0.01 1.5
0.25 0.2 0.4 0.6 0.8 −
0.00 0.00 µ
0.25 0.50 0.75
µ
Figure 4: Linear transfer in two-layer ReLU networks We train a two layer ReLU network (18)
with m = 1000 neurons on a teacher with m = 100 neurons and d = 100 dimensional gaussian
∗
data, according to the ablated transfer setup (19), (20). For these experiments, we set the label noise
σ =0. (a)Grammatrixfromthekernelofthepretrainedmodel(b)Grammatrixfromthekernelofthe
groundtruthsourcefunctionf∗(x). Thetwogrammatricesarenearlyindistinguishablesuggestingthat
s
thekernelsparsifiestotherepresentfeaturesinthesourcetask. (c)Generalizationerrorofthescratch
trainedmodelasafunctionofdatasetsizen,fittoapowerlaw(d)Normofout-of-RKHScomponentof
targetfunction P⊥f∗(x) 2 , normalizedbytargetfunctionnorm f∗(x) 2 asafunctionofexcess
∥ t ∥L2 ∥ t ∥L2
target features µ. (e) Heat map of transferability as a function of excess target features µ and dataset
sizen. Wenormalizethetransferabilitybyvarianceinthetargetdata. Graycirclesrepresentthepoint
ofnegativetransferpredictedbyourtheory. Resultsareaveragedover100realizationsofthedataand
10realizationsofrandomdrawsoftheteacher.
network. Thatis,westudythismodelinthestudentteachersetting. Tovarytheleveloffeaturespace
overlapbetweenthesourceandtargetfunctions,wedefineanetworkofm neuronsforthetargettask,
∗
andgeneratethesource networkbyablatingafraction µ ofthehiddenneurons formthetarget. More
precisely,let beauniformlyrandomsubsetoftheindexset 1,2, m with =µm . Then
∗ ∗
A { ··· } |A|
1 (cid:88)
f∗(x)= c∗σ(w∗Tx) (19)
s (1 µ)m i i
∗
− i∈Ac
1
(cid:88)m∗
f∗(x)= c∗σ(w∗Tx) (20)
t m i i
∗
i=1
Thusthesourcehasµm∗ fewerhiddenfeaturesthanthetargettask, andsothefractionµcontrols
thedegreeofdiscrepancybetweensourceandtargetfeaturespaces. Inessencewhenµ = 0thesource
andtargetspacesareidentical. However, asµincreases, anincreasingfractionofnewtargetfeatures,
that were not present in pre-training, must be learned. We constrain the hidden features in the model,
source, and target to the d-dimensional unit sphere w ,w∗ Sd−1. As in the deep linear model, we
i i ∈
11choosex (0,I ),trainthesourcetaskonthepopulationloss,whichcanbecomputedexactlyfor
d
∼ N
thismodel,andthetargettaskonafinitesampleofndatapoints.
Previous work (Rotskoff & Vanden-Eijnden, 2022; Mei et al., 2018; Chizat, 2020) has shown that
in the overparameterized setting m m , gradient flow will converge to a global minimizer of the
∗
≫
population loss, so that lim lim f(x) = f∗(x), which establishes that the trained network
m→∞ t→∞ s
buildsarepresentationoff∗(x)inthemeanfieldlimit. Thisdoesnotnecessarilymeanthatallofthe
s
hiddenneuronsofthemodelconvergetothoseoftheteacher,sinceanysuperfluousweightdirectionscan
beeliminatedbysettingthecorrespondingoutputweighttozero. However,wedemonstrateempirically
inFig. 4(a)-(b)thatthisrelationshipispreservedatthelevelofthemodel’skernel,sothatk(x,x′) =
1 (cid:80)m σ(wTx)σ(wTx′) 1 (cid:80) σ(w∗Tx)σ(w∗Tx′). This observation is analogous to
m i=1 i i ≈ (1−µ)m∗ i∈Ac i i
Theorem 3.4: training in the feature learning regime causes the model’s features to sparsify to those
presentinthetargetfunction.
Now, linear transfer in this model can be formulated as a kernel interpolation problem with this
kernel.Thegeneralizationerrorofkernelinterpolationcanbeseparatedintoann-dependentcomponent,
andanirreducibleerrortermwhichcorrespondstothenormoftheprojectionofthetargetfunctioninto
thesubspaceofL (p)orthogonaltotheRKHSdefinedbythekernel:
2
E =C(n)+ P⊥f∗(x) 2 . (21)
D Rlt ∥ t ∥L2
Asexpected,thenormofthisprojectionincreasesmonotonicallywithµasshowninFig.4(d).Weshow
howtocomputethisprojectioninAppendixC.Inthedeeplinearsetting, P⊥f∗(x) 2 = sin2θ,and
∥ t ∥L2
C(n) 1/n. Whiletheasymptotic,typicalgeneralizationerrorofkernelregressionhasbeenstudiedin
∼
(Canataretal.,2021), forthepurposesofestimatingthegeneralizationerrorofthetransferredmodel,
weassumeherethatthisgeneralizationerrorisdominatedbythisirreducibletermforthelargentarget
datasetsizesweconsider,justasweshowedforthedeeplinearmodel.
However, an expression for the generalization error of the scratch-trained model is also needed to
derivethetransferability. Wearenotawareofatheoryofgeneralizationerrorforinfinitewidthnonlin-
ear networks trained on a finite data in the mean field regime. Intriguingly, however, we demonstrate
empirically(Fig. 4(c))thatthegeneralizationerrorobeysapowerlaw An−ν withν =1.18. By
sc
R ∼
settingourtheoreticallypredictedgeneralizationerrorofourtransferredmodel P⊥f∗(x) 2 equalto
theempiricallyobservedscalinglawAn−νforourscratch-trainedmodel,wecan∥ approt xima∥ teL l2
yidentify
thepointofnegativetransferinnforanygivenµ(graycirclesinFig.4(e)).ItisclearfromFig.4(e)that
thisheuristicforfindingtheboundarybetweenpositiveandnegativetransferbecomesmoreaccurateas
the number of target points becomes large, since the n-dependent component of the kernel regression
generalizationerrorgoestozerointhislimit. ThephasediagraminFig. 4fornoiselessReLUnetworks
resemblesthephasediagramforlineartransferwithdeeplinearnetworksinthenoiselesssettingwith
σ =0(AppendixEFig.6. 7). Overall,thisdemonstratesthatweareabletopredictthephaseboundary
betweenpositiveandnegativetransferintheReLUcase,usingourconceptualunderstandinginthedeep
linearcase.
5 Conclusion
Inthispaper,wehighlighttheimportanceofthinkingabouttransferlearninginthecontextofthefeature
space of the pretrained model. In particular, we show that certain Integral Probability Metrics and
ϕ-divergence can be misleading when it comes to predicting transfer learning performance using the
datasetsalone. Wethenrigorouslyidentifythenumberofdatapointsnecessaryfortransferlearningto
12outperformscratchtrainingasafunctionoffeaturespaceoverlapindeeplinearnetworks. Finally,we
demonstratethatourunderstandingoflineartransfercarriesovertoshallownonlinearnetworksaswell.
References
Madhu Advani and Surya Ganguli. Statistical mechanics of optimal convex inference in high dimen-
sions. PhysicalReviewX,6(3):031034,2016.
DavidAlvarez-MelisandNicoloFusi. Geometricdatasetdistancesviaoptimaltransport. Advancesin
NeuralInformationProcessingSystems,33:21428–21439,2020.
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The
silentalignmenteffect,2021. URLhttps://arxiv.org/abs/2111.00034.
YasamanBahri,EthanDyer,JaredKaplan,JaehoonLee,andUtkarshSharma.Explainingneuralscaling
laws. ProceedingsoftheNationalAcademyofSciences,121(27):e2311878121,2024.
Peter L Bartlett, Philip M Long, Ga´bor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. ProceedingsoftheNationalAcademyofSciences,117(48):30063–30070,2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practiceandtheclassicalbias–variancetrade-off. ProceedingsoftheNationalAcademyofSciences,
116(32):15849–15854,2019.
AbdulkadirCanatar,BlakeBordelon,andCengizPehlevan. Spectralbiasandtask-modelalignmentex-
plaingeneralizationinkernelregressionandinfinitelywideneuralnetworks.Naturecommunications,
12(1):2914,2021.
LenaicChizat. Sparseoptimizationonmeasureswithover-parameterizedgradientdescent,2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
Advancesinneuralinformationprocessingsystems,32,2019.
YoungminChoandLawrenceSaul. KernelMethodsforDeepLearning. InAdvancesinNeuralInfor-
mationProcessingSystems,volume22.CurranAssociates,Inc.,2009.
YansongGaoandPratikChaudhari. Aninformation-geometricdistanceonthespaceoftasks. InInter-
nationalConferenceonMachineLearning,pp.3553–3563.PMLR,2021.
FedericaGerace,LucaSaglietti,StefanoSaraoMannelli,AndrewSaxe,andLenkaZdeborova´. Probing
transfer learning with a model of synthetic correlated datasets. Machine Learning: Science and
Technology,3(1):015030,2022.
SebastianGoldt,MarcMe´zard,FlorentKrzakala,andLenkaZdeborova´. Modelingtheinfluenceofdata
structure on learning in neural networks: The hidden manifold model. Physical Review X, 10(4):
041044,2020.
TrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani.Surprisesinhigh-dimensional
ridgelessleastsquaresinterpolation. Annalsofstatistics,50(2):949,2022.
13ArthurJacot,FranckGabriel,andCle´mentHongler. Neuraltangentkernel: Convergenceandgeneral-
izationinneuralnetworks. Advancesinneuralinformationprocessingsystems,31,2018.
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-Tuning can
DistortPretrainedFeaturesandUnderperformOut-of-Distribution,February2022.
DanielKunin,AllanRavento´s,Cle´mentineDomine´,FengChen,DavidKlindt,AndrewSaxe,andSurya
Ganguli. Getrichquick: exactsolutionsrevealhowunbalancedinitializationspromoterapidfeature
learning. arXivpreprintarXiv:2406.06158,2024.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learningindeeplinearnetworks. arXivpreprintarXiv:1809.10374,2018.
HongLiu,MingshengLong,JianminWang,andMichaelIJordan. Towardsunderstandingthetransfer-
abilityofdeeprepresentations. arXivpreprintarXiv:1909.12031,2019.
SongMei,AndreaMontanari,andPhan-MinhNguyen. Ameanfieldviewofthelandscapeoftwo-layer
neuralnetworks. ProceedingsoftheNationalAcademyofSciences,115(33):E7665–E7671,2018.
Gabriel Mel and Surya Ganguli. A theory of high dimensional regression with arbitrary correlations
betweeninputfeaturesandtargetfunctions: samplecomplexity,multipledescentcurvesandahierar-
chyofphasetransitions. InInternationalConferenceonMachineLearning,pp.7578–7587.PMLR,
2021.
BehnamNeyshabur,HanieSedghi,andChiyuanZhang. Whatisbeingtransferredintransferlearning?
Advancesinneuralinformationprocessingsystems,33:512–523,2020.
CuongNguyen,Thanh-ToanDo,andGustavoCarneiro.Similarityofclassificationtasks.arXivpreprint
arXiv:2101.11201,2021.
LeonardoPetrini,FrancescoCagnetta,EricVanden-Eijnden,andMatthieuWyart. Learningsparsefea-
turescanleadtooverfittinginneuralnetworks. AdvancesinNeuralInformationProcessingSystems,
35:9403–9416,December2022.
Marc Potters and Jean-Philippe Bouchaud. A First Course in Random Matrix Theory: for Physicists,
EngineersandDataScientists. CambridgeUniversityPress,2020.
Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An
interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):
1889–1935,2022.
LucaSagliettiandLenkaZdeborova. SolvableModelforInheritingtheRegularizationthroughKnowl-
edgeDistillation. InProceedingsofthe2ndMathematicalandScientificMachineLearningConfer-
ence,pp.809–846.PMLR,April2022.
Gal Shachaf, Alon Brutzkus, and Amir Globerson. A theoretical analysis of fine-tuning with linear
teachers. AdvancesinNeuralInformationProcessingSystems,34:15382–15394,2021.
AlistairShilton,SunilGupta,SantuRana,andSvethaVenkatesh. Regretboundsfortransferlearningin
bayesianoptimisation. InArtificialIntelligenceandStatistics,pp.307–315.PMLR,2017.
14Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Gert R. G. Lanckriet, and Bernhard
Scho¨lkopf. A note on integral probability metrics and $ phi$-divergences. CoRR, abs/0901.2698,
\
2009. URLhttp://arxiv.org/abs/0901.2698.
NileshTripuraneni,MichaelJordan,andChiJin. Onthetheoryoftransferlearning: Theimportanceof
taskdiversity. Advancesinneuralinformationprocessingsystems,33:7852–7862,2020.
RomanVershynin. High-DimensionalProbability: AnIntroductionwithApplicationsinDataScience.
CambridgeSeriesinStatisticalandProbabilisticMathematics.CambridgeUniversityPress,2018.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
DanielSoudry,andNathanSrebro. Kernelandrichregimesinoverparametrizedmodels. InConfer-
enceonLearningTheory,pp.3635–3673.PMLR,2020.
LeiWu,QingcanWang,andChaoMa. Globalconvergenceofgradientdescentfordeeplinearresidual
networks. AdvancesinNeuralInformationProcessingSystems,32,2019.
Xuetong Wu, Jonathan H. Manton, Uwe Aickelin, and Jingge Zhu. Information-theoretic analysis for
transfer learning. In 2020 IEEE International Symposium on Information Theory (ISIT), pp. 2819–
2824,2020. doi: 10.1109/ISIT44484.2020.9173989.
GregYangandEdwardJHu. Tensorprogramsiv: Featurelearningininfinite-widthneuralnetworks.
InInternationalConferenceonMachineLearning,pp.11727–11737.PMLR,2021.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training
linearneuralnetworks,2021. URLhttps://arxiv.org/abs/2010.02501.
A Initialization assumption
FollowingYunetal.(2021)weplacethefollowingconstraintontheinitializationforsomeλ>0.
W¯ TW¯ W¯ W¯ T ≽λI (22)
l l − l+1 l+1
Toourknowledge,thisisthemostgeneralassumptionontheweightinitializationsintheliterature
thatleadstotheimplicitbiasesthatarecrucialforouranalysis. Thisinitializationschemegeneralizes
thatinWuetal.(2019);Atanasovetal.(2021).
B Proofs
B.1 ProofofTheorem2.2
WebeginbyrecallingthedefinitionoftheDudleyMetric
γ (p,q)= sup E h E h (23)
β p q
| − |
∥h∥BL≤1
h = h + h (24)
BL L ∞
∥ ∥ ∥ ∥ ∥ ∥
15Byconditioningp (x,y)andp (x,y)onx,wecanwrite
f g
(cid:12) (cid:12) 1 (cid:90) (cid:20) −(y−f(x))2 −(y−g(x))2(cid:21) (cid:12) (cid:12)
γ β(p f,p g)= sup (cid:12) h(x,y)e 2σ2 h(x,y)e 2σ2 p(x)dxdy(cid:12) (25)
∥h∥BL≤1(cid:12)√2πσ2 − (cid:12)
(cid:12) (cid:12) 1 (cid:90) (cid:20) cos(y) −(y−f(x))2 cos(y) −(y−g(x))2(cid:21) (cid:12) (cid:12)
(cid:12) e 2σ2 e 2σ2 p(x)dxdy(cid:12) (26)
≥(cid:12)√2πσ2 2 − 2 (cid:12)
(cid:12) (cid:12)
(cid:12)e−σ2/2 (cid:90) (cid:12)
=(cid:12) [cos(f(x)) cos(g(x))]p(x)dx(cid:12) (27)
(cid:12) (cid:12) 2 − (cid:12) (cid:12)
e−σ2/2 (cid:90)
(cid:2) f(x)2+g(x)2(cid:3)
p(x)dx (28)
≥ 2
(29)
Thefirstinequalityfollowsfromthefactthat cos(y) =1,andthesecondfollowsfromtheidentity
cos(x)+x2 cos(z) z2 for any x,z
R∥
.
W2
e
c∥ aB nL
expand f in the orthonormal basis ϕ M as
f
=(cid:80)M α≥
ϕ
,sothat− ∈ { i }i=1
i=1 i i
(cid:90) (cid:90)
(cid:88) (cid:88)
f(x)2p(x)dx= α α p(x)ϕ (x)ϕ (x)dx= α2 (30)
i j i j i
i,j i
Since, f L (p), thesumonrighthandsideofEq. (30)convergestosomea < . Wecanchoose
2
(cid:114) ∈ ∞
(cid:12) (cid:12)
g =
(cid:12)2δeσ2/2−a(cid:12)(cid:80)M
α ϕ whichcompletesthefirsthalfoftheproof. Toprovetheresultaboutthe
(cid:12) a (cid:12) i=1 i i
KLdivergence,candirectlycalculate (p p )
KL f g
D ∥
DKL(p
f
∥p g)=
√21
πσ2
(cid:90) p(x)e−(y− 2f σ( 2x))2 (cid:20) (y
−
2g σ( 2x))2
−
(y
−
2f σ( 2x))2(cid:21)
dxdy (31)
= 1 (cid:90) p(x)e−(y− 2f σ( 2x))2 (cid:2) g(x)2 f(x)2 2yg(x)+2yf(x)(cid:3) dxdy (32)
√2πσ2 − −
= 1 (cid:2) f 2 + g 2 2 f,g (cid:3) (33)
2σ2 ∥ ∥L2 ∥ ∥L2 − ⟨ ⟩
1
= f g 2 (34)
2σ2∥ − ∥L2(p)
Foranyδ >0wecanchooseg = αf withα> σδ1/2 whichcompletestheproof.
− ∥f∥L2(p)
B.2 ProofofLemma3.3
We proceed by bounding the dynamics of the loss by an exponentially decaying dynamics, proving
convergencetoaglobalminimum. Thenweshowthatthevalueofβataglobalminimumisunique. To
begin,notethatthematrix
D =WTW W WT (35)
l l l − l+1 l+1
isaninvarianceofthegradientflowdynamics,sothatD(t)=D(0)=α2(W¯ TW¯ W¯ W¯ T )for
l l − l+1 l+1
alltime(Atanasovetal.,2021;Kuninetal.,2024;Yunetal.,2021). Letr = (W W ...W β )
1 2 L s
−
16andnotethat
L
˙ =(cid:88) ,W˙ (36)
l l
L ⟨∇ L ⟩
l=1
L
(cid:88)
= 2 (37)
− ∥∇l L∥F
l=1
2 (38)
≤∥∇L L∥F
= WT ...WTr 2 (39)
−∥ L−1 1 ∥2
2σ2 (WT ...WT) (40)
≤− min L−1 1 L
(41)
where σ (WT ...WT) is the smallest singular value of WT ...WT. To proceed we bound
min L−1 1 L−1 1
σ (WT ...WT)awayfromzerobyshowingthatW ...W WT ...WT ispositivedefinite
min L−1 1 L−1 1 1 L−1
WT ...WTW ...W =WT ...WT(W WT +D )W ...W (42)
L−1 1 1 L−1 L−1 2 2 2 1 2 L−1
≽WT ...WT(WTW )2W ...W (43)
L−1 3 2 2 3 L−1
.
.
.
≽(WT W )L−1 (44)
L−1 L−1
=(W WT +D )L−1 (45)
L L L
≽(α2λ)L−1 (46)
wherewehaveusedtheconservationlaw(35)andtheinitializationassumption(22). Wenowhave
˙ 2(α2λ)L−1 (47)
L≤− L
= (t)=
(0)e−2(α2λ)L−1t
(48)
⇒ L L
= lim (t)=0 (49)
⇒ t→∞L
Since the loss converges to zero, lim W W ...W = lim β = β , which is unique. Note
t→∞ 1 2 L t→∞ s
thatwhilethissolutionisuniqueinfunctionspace,itisdegenerateinparameterspace.
B.3 ProofofTheorem3.4
To prove the feature space sparsification, we rely on the following Lemma, which is proven in (Yun
etal.,2021)(seeSectionH.2). Sothatthisworkisself-contained,weincludetheproofhere.
LemmaB.1. Undergradientflowonthepopulationobjective(4)ortheempiricalobjective(5),
W =σ (t)u (t)v (t)+ (α2) (50)
l l l l
O
foralltime. Furthermore
lim lim(u (t)Tv (t))2 =1 (51)
l+1 l
α→0t→∞
17Proof. ToproveLemmaB.1weboundthedifference W 2 W 2 whichisequaltothenormof
∥ l ∥F −∥ l ∥op
the subleading singular vectors of W and show that this bound is proportional to α2. The argument
l
herefollowsthatin(Yunetal.(2021)). Takingthetraceofbothsidesin(35)wehave
W 2 W 2 =α2( W¯ 2 W¯ 2) (52)
∥ l ∥F −∥ l+1 ∥F ∥ l ∥F −∥ l+1 ∥F
L−1 L−1
(cid:88) W 2 W 2 =α2(cid:88) ( W¯ 2 W¯ 2) (53)
∥ k ∥F −∥ k+1 ∥F ∥ k ∥F −∥ k+1 ∥F
k=l k=l
W 2 W 2 =α2( W¯ 2 W¯ 2) (54)
∥ l ∥F −∥ L ∥F ∥ l ∥F −∥ L ∥F
Letu ,v bethetopleftandrightsingularvectorsofW . ToboundthemaximumsingularvalueofW
l l l l
wehave
W 2 =vTWTW v uT WTW u (55)
∥ l ∥op l l l l ≥ l+1 l l l+1
=uT (D +WT W )u (56)
l+1 l l+1 l+1 l+1
= W 2 +α2uT (W¯ TW¯ W¯ W¯ T )u (57)
∥ l+1 ∥op l+1 l l − l+1 l+1 l+1
W 2 +α2( W¯ 2 W¯ 2 ) (58)
≥∥ l+1 ∥op ∥ l+1 ∥op−∥ l ∥op
SummingthisinequalityfromltoL 1wehave
−
W 2 W¯ 2 +α2( W¯ 2 W¯ 2 ) (59)
∥ l ∥op ≥∥ L ∥op ∥ L ∥op−∥ l ∥op
Combining(53)and(59)wehave
W 2 W 2 α2( W¯ 2 W¯ 2 + W¯ 2 W¯ 2 ) (60)
∥ l ∥F −∥ l ∥op ≤ ∥ l ∥F −∥ L ∥F ∥ l ∥op−∥ L ∥op
This shows all of the parameter matrices are approximately rank one with corrections upper bounded
by (α2), proving the first claim. To show the alignment of adjacent singular vectors we again take
O
advantageoftheinvariantquantity(35)
vTW WT v =vTWTW v vTD v (61)
l l+1 l+1 l l l l l − l l l
s2 α2 W¯ TW¯ W¯ W¯ T 2 (62)
≥ l − ∥ l l − l+1 l+1∥op
wealsoderivethefollowingupperboundon(62)
vTW WT v =vT(s2 u u W WT s2 u u )v (63)
l l+1 l+1 l l l+1 l+1 l+1T l+1 l+1− l+1 l+1 l+1T l
s2 (vTu )2+ W 2 W 2 (64)
≤ l+1 l l+1 ∥ l+1 ∥F −∥ l+1 ∥F
combiningthesetwobounds
s2 s2 (vTu )2+α2 W¯ TW¯ W¯ W¯ T 2 + W 2 W 2 (65)
l ≤ l+1 l l+1 ∥ l l − l+1 l+1∥op ∥ l+1 ∥F −∥ l+1 ∥F
s2 (vTu )2+α2 W¯ TW¯ W¯ W¯ T 2 +α2( W¯ 2 W¯ 2 + W¯ 2 W¯ 2 )
≤ l+1 l l+1 ∥ l l − l+1 l+1∥op ∥ l ∥F −∥ L ∥F ∥ l ∥op−∥ L ∥op
(66)
wherewehaveusedtheresultderivedinthepreviousproofforthesecondinequality. Finally,wederive
anupperboundonthisquantity
s2 uT WTW u (67)
l ≥ l+1 l l l+1
s2 α2 W¯ TW¯ W¯ W¯ T 2 (68)
≥ l+1− ∥ l l − l+1 l+1∥op
18Wecancombinetheupperandlowerboundsanddividebys2 toconclude
l+1
C
(vTu )2 1 α2 l (69)
l l+1 ≥ − s2
l+1
C =2 W¯ TW¯ W¯ W¯ T 2 + W¯ 2 W¯ 2 + W¯ 2 W¯ 2 (70)
l ∥ l l − l+1 l+1∥op ∥ l ∥F −∥ L ∥F ∥ l ∥op−∥ L ∥op
This proves that adjacent singular vectors align as long as the singular values are bounded away from
zero. Toshowthatthisrequirementissatisfiedattheendoftraining,notethatintheproofsofLemma
3.3 and Theorem 3.5 we show that gradient flow converges to a global minimizer of the loss. Let
yˆ=lim XW W ...W denotethefinalnetworkpredictions. Then
t→∞ 1 2 L
L
∥yˆ ∥2 lim W W ...W lim (cid:89) s2 (71)
X
op
≤t→∞∥ 1 2 L ∥2 ≤t→∞ l
∥ ∥ l=1
If d n, yˆ is just equal to the vector of target outputs which is larger than zero by construction. If
≥
d<n,yˆistheprojectionofthetargetsintothespacespannedbytherowsofX,whichisalmostsurely
anon-zerovector. Thisimpliesthat
L
(cid:89)
lim s2 >0 (72)
l
t→∞
l=1
whichimpliesthattheindividualsingularvaluesareboundedawayfromzeroattheendoftraining. In
thepopulationtrainingcase,theproofisnearlysame,replacingyˆ=lim W W ...W =β
t→∞ 1 2 L s
ByLemmaB.1,wehave
W W ...W =cu vT (73)
1 2 L−1 1 L−1
afterpretraining,forsomec R. However,fromTheorem3.5weknowthatafterpretraining
∈
W ...W W =β (74)
1 L−1 L s
=cu (vT W ) (75)
1 L1 L
=cu (76)
1
wherewehaveusedLemmaB.1inthethirdequalitytoeliminatetheinnerproductbetweentheadjacent
singularvectors. Thepossiblefactorof 1canbeabsorbedintothedefinitionofu . Thisimplies
1
−
W W ...W =β vT (77)
1 2 L−1 s L−1
B.4 ProofofTheorem3.5
ThisprooffollowsYunetal.(2021)closelybutextendstheirresulttothecasen>d. Wefirstshowthat
gradient flow converges to a global minimum of the empirical loss (5). We then show that as α 0,
→
thisminimumcorrespondstotheminimumnormleastsquaressolution.
Part1: Gradientflowconvergestoaglobalminimum
19This proof follows the same logic as the proof for Lemma 3.3. First, we define the residual vector
r =XW W ...W y. Thenwecanwritetheempiricallossas
1 2 L t
−
1 1
= r 2 = ( r 2+ r 2) (78)
L 2n∥ ∥2 2n ∥ ∥ ∥2 ∥ ⊥ ∥2
wherer isthecomponentofrinim(X)andr isthecomponentofrinker(XT).SinceXW W ...W
∥ ⊥ 1 2 L
∈
im(X),theglobalminimumof(78)isequalto r 2. Therefore,toshowthatgradientflowconverges
∥ ⊥ ∥2
toaglobalminimumitissufficienttoshowthatlim r (t) 2 =0. LetP andP betheorthogo-
t→∞ ∥ ∥ ∥2 ∥ ⊥
nalprojectorsontoim(X)andker(XT)respectively,sothat := r 2 = P (XW W ...W
L∥ ∥ ∥ ∥2 ∥ ∥ 1 2 L −
y) 2and := r 2 = P (XW W ...W y) 2. Thenwehave
t ∥2 L⊥ ∥ ⊥ ∥2 ∥ ⊥ 1 2 L − t ∥2
L
˙ =(cid:88) ,W˙ (79)
∥ l ∥ l
L ⟨∇ L ⟩
l=1
L
(cid:88)
= , (80)
l ∥ l
− ⟨∇ L ∇ L⟩
l=1
L
(cid:88)
= ( 2 + , ) (81)
− ∥∇l L∥ ∥F ⟨∇l L∥ ∇l L⊥ ⟩
l=1
Takingthegradientof wehave
⊥
L
=WT ...WTXTP rWT ...WT =0 (82)
∇l L⊥ l−1 1 ⊥ L l+1
so
L
˙ = (cid:88) 2 (83)
L∥ − ∥∇l L∥ ∥F
l=1
2 (84)
≤−∥∇L L∥ ∥F
= WT ...WTXTP r 2 (85)
−∥ L−1 1 ∥ ∥2
σ2 (WT ...WT) XTP r 2 (86)
≤− min L−1 1 ∥ ∥ ∥2
whereσ (WT ...WT)isthesmallestsingularvalueofWT ...WT. FromEq. (42)-(46)we
min L−1 1 L−1 1
canboundthisquantityawayfromzero. Thenwehave
˙ (α2λ)L−1 XTP r 2 (87)
L∥ ≤− ∥ ∥ ∥2
2(α2λ)L−1λ (88)
min ∥
≤− L
whereλ isthesmallestnonzeroeigenvalueofXXT. Thesolutiontothedynamics(88)is (t)
min ∥
L ≤
L∥(0)e−2(α2λ)L−1λmint, which proves lim
t→∞
∥r ∥(t) ∥2
2
= 0. Note that this part of the theorem holds
foranyα,n,d,andwetakethelimitα 0aftert .
→ →∞
Part2: asα 0,gradientflowfindstheminimumnorminterpolator
→
In the case n > d, the least squares problem () is overdetermined so the solution is unique. That is,
20theuniquesolutionistriviallytheminimumnormsolution. Inthecasen d,therearemultipleβ(t)
≤
thatyieldzerotrainingerror. LemmaB.1showsthattheparametermatricesareapproximatelyrankone
atalltimesandu andv alignattheendoftrainingasα 0,whichmeansthat
l+1 l
→
lim lim β(t)= lim lim W W ...W =cu (89)
1 2 L 1
α→0t→∞ α→0t→∞
wherec > 0. Nextweshowthatu row(X). WecanbreakW intotwocomponentsW∥ andW⊥
l ∈ 1 1 1
wherethecolumnsofW∥areinrow(X)andthecolumnsofW⊥areinker(XT).Thelefthandsideof
1 1
(82)alsoshowsthatthegradientofW⊥ iszero,whichmeansthatthiscomponentremainsunchanged
1
undergradientflowdynamics. Thereforewehave
W⊥(t) = W⊥(0) α W¯ (90)
∥ 1 ∥F ∥ 1 ∥F ≤ ∥ 1 ∥F
which vanishes in the limit α 0. This implies that u row(X) at all times. The only global
1
→ ∈
minimizer with this property is the minimum norm solution. As a final comment, we note that this
theoremisalsoproveninAtanasovetal.(2021)usingdifferenttechniques.
B.5 ProofofTheorem3.6
Let βˆ = lim W W ...W . From Theorem 3.5, βˆ = X+y = X+Xβ +X+ϵ. Then the
t→∞ 1 2 L t
averagegeneralizationerrorattheendoftrainingcanbewritten
E =E β βˆ 2 (91)
X,ϵ R X,ϵ ∥ t − ∥2
=1+E βˆ 2 2E βˆ,β (92)
X,ϵ ∥ ∥2− X,ϵ ⟨ t ⟩
=1+E βT(X+X)T(X+X)β +E ϵT(X+)TX+ϵ+2E ϵT(X+X)β (93)
X t t X,ϵ X,ϵ t
2(E βT(X+X)β +E βTX+ϵ) (94)
− X t t X,ϵ t
=1 E P β 2+σ2E tr((X+)TX+) (95)
− X ∥ row(X) t ∥2 X
where we have used the independence of ϵ and X, as well as the fact that the operator X+X is the
projectorontosubspacespannedbytherowsofX,P . SincetheentriesofthedatamatrixX are
row(X)
independent Gaussians, the n-dimensional subspace row(X) is uniformly random in the Grassmanian
manifold Vershynin(2018),soP β isarandomprojectionofβ . Then
n,d row(X) t t
G
E P β 2 =γ (96)
X ∥ row(X) t ∥2
whichisaclassicresultinthetheoryofrandomprojections(c.f. Vershynin(2018)Lemma5.3.2). We
nowturntothefinaltermin(95). Let σ bethenonzerosingularvaluesofthedatamatrix
l l≤min(n,d)
{ }
X. Then
min(n,d)
(cid:88) 1
E tr((X+)TX+)=E (97)
X X σ2
l=1 l
Firsttakethecaseγ < 1. ThentherearennonzerosingularvaluesofX,whicharetheeigenvaluesof
theWishartmatrixC = 1XXT and
d
γ
E tr((X+)TX+)= E tr(C−1) (98)
X n X
1
= γ lim E[tr((zI C)−1)] (99)
− z→0n −
= γ limg (z) (100)
C
− z→0
21Inthesecondlinewehaveintroducedthecomplexvariablez,whichcaststhequantityofinterestasthe
z 0limitofthenormalizedexpectedtraceoftheresolventofC. Inthelimitoflargen,thisquantity
→
tends to the Stieltjes transform of the Wishart matrix g (z), which has a closed form expression (see
C
Potters&Bouchaud(2020)Ch.4foraproof).
(cid:112) (cid:112)
z (1 γ) z (1+√γ)2 z (1 √γ)2
limg (z)= lim − − − − − − (101)
z→0 C z→0 2γz
1
= (102)
−1 γ
−
so E tr((X+)TX+) = γ for γ < 1. In the case γ > 1, there will be d terms in the sum (97),
X 1−γ
whichareproportionaltotheeigenvaluesofthecovariancematrix 1XTX. Ifwedefinen′ = d,d′ =
n
n,γ′ = n′/d′ andX′ = XT Rn′×d′,equations(98)-(100)holdunderthesubstitutionγ γ′. So
E tr((X+)TX+)= γ′ =∈ 1 forγ >1. Puttingeverythingtogetherwehave →
X 1−γ′ γ−1
(cid:40) (1−γ)2+γσ2
γ <1
E = 1−γ (103)
X,ϵ R σ2 γ >1
γ−1
B.6 ProofofTheorem3.7
Theorem 3.4 implies that the pretrained feature matrix is Φ = (Xβ )vT . Since Φ is a rank one
s L−1
matrixitspseudoinverseiseasytocompute
1
Φ+ = v (Xβ )T (104)
Xβ 2 L−1 s
∥ s ∥2
Thecoefficentvectorβˆafterlineartransferis
βˆ=W ...W Wˆ (105)
1 L−1 L
=W ...W Φ+y (106)
1 L−1 t
=bβ (107)
s
where
βT XTy
b= src t (108)
βTXTXβ
s s
βT XTXβ βT XTϵ
= src t + src (109)
βTXTXβ βTXTXβ
s s s s
AsintheproofofTheorem3.6,wecanwritethetypicalgeneralizationerroras
E = βˆ β 2 (110)
X,ϵ Rlt ∥ − t ∥2
=1+E b2 2cosθE b (111)
X,ϵ X,ϵ
−
Toproceed,wecanwriteβ =cosθβ +sinθνforsomevectorν β ,andintroducetheindependent
t s s
⊥
n dimensionalGaussianvectorsz = Xβ (0,I )andw = Xν (0,I ). Withthischange
s n n
− ∼ N ∼ N
22ofvariableswehave
E b=E b (112)
X,ϵ z,w,ϵ
=cosθ (113)
E b2 =E b2 (114)
X,ϵ z,w,ϵ
1
=cos2θ+(sin2θ+σ2)E (115)
z z 2
∥ ∥2
TheintegralE 1 canbesolvedexactly
z∥z∥2
2
E
1
=
1 (cid:90) ∞ e−(cid:80)n i=1z i2/2
dz (116)
z z 2 (2π)n/2 (cid:80) z2
∥ ∥2 −∞ j=1 j
S (cid:90) ∞
= n−1 rn−3er2/2dr (117)
(2π)n/2
0
S (cid:90) ∞
= n−1 e−ttn 2−2dt (118)
4πn/2
0
S (cid:16)n (cid:17)
= n−1 Γ 1 (119)
4πn/2 2 −
1
= (120)
n 2
−
whichcompletestheproof.
B.7 ProofofTheorem3.8
Webeginbywritingdownthesolutiontotheoptimizationproblem(14)
Wˆ =(ΦTΦ+nλI )−1ΦTy (121)
L d t
AsintheproofofTheorem3.7,wehave
Φ=(Xβ )vT (122)
s L−1
W W ...W =β vT (123)
1 2 L−1 s L−1
Combiningtheseexpressionswecansolveforthelinearfunctionthenetworkimplementsaftertransfer
learningwithridgeregression
βˆ=W W ...W Wˆ (124)
1 2 L−1 L
=β vT ( Xβ 2+nλI )−1v (Xβ )Ty (125)
s L−1 ∥ s ∥2 d L−1 s t
(cid:18) (Xβ )Ty (cid:19)
= s t β (126)
Xβ 2+nλ s
∥ s ∥2
AsintheproofofTheorem3.7,wewriteβ =cosθβ +sinθν forsomevectorν β ,andintroduce
t s s
⊥
theindependentn dimensionalGaussianvectorsz = Xβ (0,I )andw = Xν (0,I ).
s n n
− ∼ N ∼ N
Thenwecangetthefollowingexpressionforthegeneralizationerrorofridgelineartransfer:
E λ = βˆ β 2 (127)
X,ϵ Rlt ∥ − t ∥2
=1+(cos2θ)I (n+2,λ)+(sin2θ+σ2)I (n,λ) (2cos2θ)I (n,λ) (128)
1 1 2
−
23wherewehaveusedsphericalcoordinatestodefinethefollowingintegrals
(cid:18) z m−n+2 (cid:19) S (cid:90) ∞ rm+1e−r2/2
I (m,λ)=E ∥ ∥2 = n−1 dr (129)
1 z ( z 2+nλ)2 (2π)n/2 (r2+nλ)2
∥ ∥2 0
(cid:18) z m−n+2(cid:19) S (cid:90) ∞ rm+1e−r2/2
I (m,λ)=E ∥ ∥2 = n−1 dr (130)
2 z z 2+nλ (2π)n/2 r2+nλ
∥ ∥2 0
WeevaluateI (n,λ),I (n+2,λ)andI (n,λ)forlargen. Toavoidclutteringthenotation,weignore
1 1 2
thecoefficient Sn−1 whilesolvingtheintegralandrestoreitattheendofthecalculation. Then
(2π)n/2
(cid:90) un/2e−u
I (n,λ) 2n/2 du (131)
1 ∝ (2u+nλ)2
(cid:90) tn/2e−nt
=n(2n)n/2 dt (132)
(2nt+nλ)2
(cid:90)
=n(2n)n/2 g(t)enf(t)dt (133)
(cid:115)
2π
n(2n)n/2 g(t )enf(t0) (134)
≈ nf′′(t ) 0
0
| |
Wehaveintroducedthechangeofvariablesu = r2/2inthefirstline,t = u/ninthesecondline,and
finallyevaluatedtheintegralforlargenusingthesaddlepointmethod. Inthelastline, t isacritical
0
pointoff(t)= 1logt tandg(t)=(2nt+nλ)−2. Differentiatingf(t)andsettingequaltozerowe
2 −
findt =1/2. Soforlargen,
0
√πnnn/2e−n/2
I (n,λ) (135)
1 ∝ (n+nλ)2
Wecannowrestoretheangularcoefficienttotheintegral
S √πnnn/2e−n/2
I (n,λ)= n−1 (136)
1 (2π)n/2 (n+nλ)2
nπn/2 (cid:16)n(cid:17)−n/2 √πnnn/2e−n/2
en/2 (137)
≈ √πn 2 (n+nλ)2
1
= (138)
n(1+λ)2
wherewehaveusedStirling’sapproximationinthesecondline. Therefore,lim I (n,λ) = 0. We
n→∞ 1
stressthatalthoughtheintegralwasapproximatedatthesaddlepoint, thelimitn isexactsince
→ ∞
correctionstothesaddlepointvaluearesubleadinginn. Similarcalculationsyield
1
I (n+2,λ)= (139)
1 (1+λ)2
1
I (n,λ)= (140)
2 1+λ
24forlargen. Pluggingthisinto(127),wehave
(1+2λ)
lim E λ =1 cos2θ (141)
n→∞ X,ϵ Rlt − (1+λ)2
This is a strictly increasing function in λ 0 for any θ [0,π/2], which implies that the optimal
≥ ∈
regularizationvalueisλ∗ =0.
B.8 ProofofTheorem3.9
TheproofinvolvesslightlytweakingtheproofofTheorem3.5. Sincethesourcetrainedmodelobeyed
the initialization assumption (22), the invariant matrix (35) is equal to its value at initialization before
pretrainingthroughoutfinetuningaswell. ThisimpliesthatthefirsthalfoftheproofofTheorem(3.5)
holdsinthefinetuningcaseandthemodelwillconvergetoaglobalminimizerofthetrainingloss. The
invariancethroughoutfinetuningalsoimpliesthat(89)holdsandthatW⊥doesnotchangeduringfine
1
tuning,andremainsfixedatitsinitialvaluefrompretraining. Therefore,bytheproofofTheorem3.7,at
thebeginningoffinetuning,u =β and(I P )β isthecomponentofu thatdoesnotevolve.
1 s row(X) s 1
−
Meanwhile,P (X)u willevolvetotheminimumnormsolution. Combiningtheseresults,afterfine
row 1
tuning,
lim lim β (t)=β +(I P )β (142)
ft sc row(X) s
α→0t→∞ −
whereβ istheminimumnormsolution. Wecannowwritetheexpectedgeneralizationerror
sc
E =E [ β β 2]
X,ϵ Rft X,ϵ ∥ t − ft ∥2
=E +E (I P )β 2 2E β ,(I P )β
X,ϵ Rsc X ∥ − row(X) s ∥2− X ⟨ t − row(X) s ⟩
=E +max(0,1 γ) 2E β ,(I P )β
X,ϵ sc X t row(X) s
R − − ⟨ − ⟩
=E +max(0,1 γ) 2cosθE β ,(I P )β
X,ϵ sc X s row(X) s
R − − ⟨ − ⟩
2sinθE ν,(I P )β
X row(X) s
− ⟨ − ⟩
=E +max(0,1 γ)(1 2cosθ) 2E sinθ ν,(I P )β
X,ϵ sc X row(X) s
R − − − ⟨ − ⟩
wherewehaveusedthefactthatP )β isarandomprojectionasintheproofofTheorem3.6and
row(X) s
setβ = cosθβ +sinθν forsomeν β . Thefinaltermisequaltozeroforthefollowingreason.
t s s
⊥
The operator I P is a random projector onto the d n dimensional subspace orthogonal to
row(X)
− −
rowX Sincetheuniformdistributionofrandomsubspacesisrotationallyinvariant,wecaninsteadfixa
particularsubspaceandaverageoverβ Uniform(Sd−1). Usingrotationinvarianceagain,wecanfix
s
∼
theprojectiontobealongthefirstd ncoordinatesofβ . Thenwehave
s
−
n−d
(cid:88)
E ν,(I P )β = ν E(β ) (143)
row(X) s k s k
⟨ − ⟩
k=1
=0 (144)
Thiscompletestheproof
25C ReLU networks
In this section, we describe how to compute projections into (and out of) the RKHS defined by a one
hiddenlayerReLUnetwork. Consideranetworkf(x)andatargetfunctionf (x).
∗
m
1 (cid:88)
f(x)= c σ(wTx) (145)
m i i
i=1
1
(cid:88)m∗
f (x)= c∗σ(w∗Tx) (146)
∗ m i i
∗
i=1
The feature space of the model is span σ(wTx) in L (p). To form projectors into this space
{ i }i≤m 2
and its orthogonal complement, we introduce the Mercer decomposition. For any positive definite,
symmetric kernel k : R we can define features through partial evaluation of the kernel,
X × X →
i.e., ϕ(x) = k(,x). This kernel also induces a reproducing kernel Hilbert space (RKHS) via the
·
Moore–Aronszajn theorem, which is defined as the set of all functions that are linear combinations of
thesefeatures,
(cid:40) (cid:12) (cid:88)M (cid:41)
= f(cid:12)f = α k(,z )forsomeM N,α R,z (147)
k (cid:12) i i i i
H · ∈ ∈ ∈X
i=1
Theassociatednormofafunctionf isgivenby
k
∈H
M
(cid:88)
f 2 = α k(z ,z )α (148)
|| ||k i i j j
ij
WecanalsodefinetheoperatorT :L (p) L (p)withaction
k 2 2
→
(cid:90)
T f = dx′p(x′)k(x,x′)f(x′) (149)
k
Thespectraldecompositionofthisoperator, λ2,ψ ∞ isknownastheMercerdecompositionand
{ l l }l=1
theeigenfunctionsformabasisforL (p). Theeigenfunctionsψ (x)satisfy
2 l
T ψ =λ ψ (150)
k l l l
whereλ istheassociatedeigenvalue. Theeigenfunctionswithnon-zeroeigenvalueformabasisforthe
l
RKHS . Givenafunctionf
=(cid:80)∞
c ψ onecanshowbydirectcomputationthat
Hk l=1 l l
∞
(cid:88)
f 2 = c2/λ2 (151)
|| ||k l l
l=1
whichalsodemonstratesthatfunctionswithsupportoneigenmodeswithzeroeigenvaluearenotin
theRKHS.IfwecanconstructtheMercereigenfunctionswecanbuildorthogonalprojectionoperators
intotheRKHSanditsorthogonalcomplement. TobeginnotethatforGaussiandata,p(x)= (0,I ),
d
N
we can exactly compute the expected overlap between two ReLU functions in terms of their weight
26vectors(Cho&Saul,2009):
(cid:90)
σ(wTx)σ(wTx) = p(x)σ(wTx)σ(wTx) (152)
⟨ i j ⟩L2 i j
1 (cid:16)(cid:113) (cid:17)
= 1 u2 +u(π arccosu ) (153)
2π − ij − ij
whereu
ij
= ∥wiw ∥iT 2∥w wj
j∥2
Withthisinhand,wecandefinethefollowingmatrices:
1
K = σ(wTx)σ(wTx) (154)
ij m⟨ i j ⟩L2
1
K∗ = σ(w∗Tx)σ(w∗Tx) (155)
ij m ⟨ i j ⟩L2
∗
1
K˜ = σ(wTx)σ(w∗Tx) (156)
ij √mm ∗⟨ i j ⟩L2
TheMercereigenfunctionscanbeconstructedbydiagonalizingthematrixK. Ifz isaneigenvectorof
l
K witheigenvalueλ2,then
l
m
1 (cid:88)
ψ (x)= (z ) σ(wTx) (157)
l (cid:112) mλ2 l i i
l l=1
isaMercereigenfuctionwitheigenvalueλ2,whichcanbeverifiedbypluggingtheexpressioninto
l
theeigenvalueequation(150). Sincethefeaturespaceism-dimensional,weknowthatthesemeigen-
functions span the RKHS. We can now write down expressions for the projections of f (x) into this
∗
spaceanditsorthogonalcomplement
1
P f (x) 2 = cTK˜TK−1K˜c (158)
∥ ∥ ∗ ∥L2 m ∗ ∗
∗
1 1
P f (x) 2 = f 2 P f (x) 2 = cTK c cTK˜TK−1K˜c (159)
∥ ⊥ ∗ ∥L2 ∥ ∗ ∥L2 −∥ ∥ ∗ ∥L2 m ∗ ∗ ∗ − m ∗ ∗
∗ ∗
D Experimental details
D.1 Deeplinearmodels
Fortheexperimentsindeeplinearmodels,wetrainatwolayerlinearnetworkwithdimensiond=500.
We initialize the weight matrices with random normal weights and scale parameter α = 10−5. To
approximate gradient flow, we use full batch gradient descent with small learning rate η = 10−3. We
traineachmodelfor105 stepsoruntilthetraininglossreaches10−6. Weperformtargettrainingfor20
instancesofthetrainingdataandagridofdatasetsizesandvaluesofθ
D.2 ReLUnetworks
FortheexperimentsinshallowReLUnetworks,weusetheparametersd=100,m=1000,m =100.
∗
Weinitializetheweightmatricesrandomlyonthesphereandtheoutputweightsareinitializedat10−7.
Weapproximategradientflowwithfullbatchgradientdescentandlearningrate0.01mandtrainfor105
iterationsoruntilthelossreaches10−6. Fortrainingwithafinitedatasetweuse100realizationsofthe
trainingdata,andaverageover10randominitializationseeds.
27E Additional Figures
(a) (b)
0.4 0.4
0.2 0.2
T 0.0 T 0.0
0.2 0.2
− −
0.4 0.4
− −
0 20 40 526 527 528 529
(p p) W (p ,p)
DKL s (cid:30) t 1 s t
Figure5: Transferabilityisnotpredictedbyϕ-divergencesorintegralprobabilitymetricsWegen-
eratesourceandtargetdistributionsp ,p accordingtothesetupinSection3andplotthetransferability
s t
(7)asafunctionof(a)theKLdivergenceD (p p )and(b)theWasserstein1-metric. TheKLdi-
KL s t
T ∥
vergencecanbecomputedexactlyinthissetting(seeSectionB.1). W iscomputedfromfinitesamples
1
usingthealgorithminSriperumbuduretal.(2009).
282.0
(a) (b) (c)
θ=π/8
θ=π/4
1 1.5 0.5 θ=3π/8
0.5
γ
0 1.0 T 0.0
-0.5
0.5 0.5
-1 −
0
0.5 γ1.0 1.5 2.0 π/2 π/3 θπ/6 0
0 π/6 π/3 π/2 0 0.5 1 1.5 2
θ γ
Figure 6: Linear transferability, σ = 0 We pretrain a linear network (9) with L = 2 and d = 500
to produce un-noised labels from linear source function y = βTx using the population loss (4). We
s
then retrain the final layer weights on a sample of n = γd points (x ,y = βTx ) where βTβ =
i i t i s t
cosθ andcompareitsgeneralizationerrortothatofamodeltrainedfromscratchonthetargetdataset.
(a) Theoretical transferability surface (7) as a function of the number of data points γ = n/d and
task overlap θ. (b) Top-down view of (a), shaded by sign of transferability. Red indicates negative
transferability < 0 and blue indicates positive transferability > . Note that transfer is always
T T ′
negativewhenγ > 1, sincethescratchtrainedmodelcanperfectlylearnthetargettaskasthereisno
labelnoise. (c)Slicesof(a)forconstantθ. Solidlinesaretheory,dotsarefromnumericalexperiments.
Errorbarsrepresentthestandarddeviationover20drawsofthetrainingdata.
(a)2.0 (b)2.0 1.00
0.75
1.5 1.5 0.50
0.25
γ γ
1.0 1.0 0.00
0.25
−
0.5 0.5 0.50
−
0.75
−
0 0 1.00
0 π/6 π/3 π/2 0 π/6 π/3 π/2 −
θ θ
Figure 7: Linear transfer σ = 0: theory vs. experiment (a) Identical to Fig. 6(b), but shaded
accordingtothevalueofthetransferability. (b)ResultsofnumericalsimulationswithL=2,d=500
29