Mono-InternVL
MONO-INTERNVL: PUSHING THE BOUNDARIES OF
MONOLITHIC MULTIMODAL LARGE LANGUAGE
MODELS WITH ENDOGENOUS VISUAL PRE-TRAINING
GenLuo1∗,XueYang1∗,WenhanDou2∗,ZhaokaiWang3,1∗
JifengDai2,1, YuQiao1,XizhouZhu2,1(cid:0)
1OpenGVLab,ShanghaiAILaboratory 2TsinghuaUniversity
3ShanghaiJiaoTongUniversity
ProjectPage: Mono-InternVL
ABSTRACT
The rapid advancement of Large Language Models (LLMs) has led to an influx
of efforts to extend their capabilities to multimodal tasks. Among them, grow-
ingattentionhasbeenfocusedonmonolithicMultimodalLargeLanguageMod-
els (MLLMs) that integrate visual encoding and language decoding into a sin-
gleLLM.Despitethestructuralsimplicityanddeployment-friendliness, training
a monolithic MLLM with promising performance still remains challenging. In
particular,thepopularapproachesadoptcontinuouspre-trainingtoextendapre-
trainedLLM toamonolithicMLLM, whichsuffersfromcatastrophic forgetting
andleadstoperformancedegeneration.Inthispaper,weaimtoovercomethislim-
itationfromtheperspectiveofdeltatuning.Specifically,ourcoreideaistoembed
visual parameters into a pre-trained LLM, thereby incrementally learning visual
knowledgefrommassivedataviadeltatuning,i.e.,freezingtheLLMwhenopti-
mizingthevisualparameters.Basedonthisprinciple,wepresentMono-InternVL,
a novel monolithic MLLM that seamlessly integrates a set of visual experts via
a multimodal mixture-of-experts structure. Moreover, we propose an innovative
pre-trainingstrategytomaximizethevisualcapabilityofMono-InternVL,namely
EndogenousVisualPre-training(EViP).Inparticular,EViPisdesignedasapro-
gressivelearningprocessforvisualexperts,whichaimstofullyexploitthevisual
knowledge from noisy data to high-quality data. To validate our approach, we
conductextensiveexperimentson16benchmarks. Experimentalresultsnotonly
validate the superior performance of Mono-InternVL compared to the state-of-
the-artMLLMon6multimodalbenchmarks,e.g.,+113pointsoverInternVL-1.5
onOCRBench,butalsoconfirmitsbetterdeploymentefficiency,withfirsttoken
latencyreducedbyupto67%.
1 INTRODUCTION
Inrecentyears,therapiddevelopmentofLargeLanguageModels(LLMs)(OpenAI,2023;Baietal.,
2023a;Caietal.,2024)hasspurredincreasingeffortstoextendtheirmultimodalcapabilities(Chen
etal.,2023;Liuetal.,2023e). AsshowninFig.1(a), mostexistingMultimodalLargeLanguage
Models(MLLMs)adoptamodulararchitecture,wherevisualencodingandlanguagedecodingare
processed separately. This is typically achieved by combining a pre-trained visual encoder like
CLIP-ViT(Radfordetal.,2021)withanLLM(Liuetal.,2023e;Chenetal.,2024c;Lietal.,2023a).
Recent research has also started exploring monolithic MLLMs (Bavishi et al., 2023; Diao et al.,
2024;Chenetal.,2024b),asshowninFig.1(b),whichintegratevisualperceptionandmultimodal
understandingdirectlyintoasingleLLM.Duetotheirsimplicityandunity,monolithicMLLMscan
bemoreeasilydeployedusingexistingLLMinferencelibraries(LMDeployContributors,2023)and
showsuperiorefficiencythanmodularMLLMs(Diaoetal.,2024;Chenetal.,2024b).
Despite recent progress, training a monolithic MLLM with promising performance still remains
challenging. In particular, monolithic MLLMs struggle to replicate the successful usage of pre-
∗Equalcontribution.(cid:0)Correspondingauthor.
1
4202
tcO
01
]VC.sc[
1v20280.0142:viXraMono-InternVL
SEED-I
AI2D MathVista
67.4
:visual encoding :textual decoding 68.6 45.7
MMVet SQA-I
40.1 93.6
Response Response Response
MMB 65.5 34.8 HallB
LLM Visual
LLM LLM TextVQA72.6 33.7 MMMU
Experts
Connector
Visual 73.7 1875 MME
ChartQA
Encoder
43.0 66.3
Image Text Image Text Image Text InfoVQA CCBench
80.0 767
InternVL, Qwen-VL, EVE, SOLO, Mono-InternVL DocVQA 5 G9 Q.5 A OCRBench
LLaVA, … Chameleon, … (Ours) : EVE-7B : SOLO-7B
: Fuyu-8B : Chameleon-7B
(a) Modular MLLMs (b) Monolithic MLLMs : InternVL-1.5-2B : Mono-InternVL-2B
Figure 1: Comparison of Mono-InternVL and existing MLLMs. Compared with modular
MLLMs,Mono-InternVLembedsvisualexpertsintothepre-trainedLLMandintegratesvisualen-
codingandlanguagedecodingintoasingleLLM.Throughendogenousvisualpre-training,Mono-
InternVLsignificantlypushestheperformanceboundariesofmonolithicMLLMs.
trained visual encoders in modular MLLMs for visual perception. Therefore, researchers often
rely on additional pre-training to compensate for the shortcomings in visual perception in mono-
lithicMLLMs(ChameleonTeam,2024;Diaoetal.,2024). Astraightforwardapproachisthenative
pre-training (ChameleonTeam, 2024), which pre-trains a monolithic MLLM from scratch using a
mixtureoftext-onlyandmultimodaldata. However,itrequiresprohibitivelyhightrainingcostsand
oftensuffersfromchallengesofunstableoptimization(ChameleonTeam,2024). Anothercommon
solution is the continuous pre-training (Diao et al., 2024), which extends the pre-trained LLM to
multimodalityviaadditionalvisualpre-training. Benefitingfromtheknowledgeinthepre-trained
LLM,thecostofcontinuouspre-trainingbecomesmuchmoreaffordable. Nevertheless,duetothe
catastrophic forgetting issue (Zhai et al., 2023), the pre-trained language knowledge is inevitably
underminedduringcontinuouspre-training,therebyweakeningthemultimodalcapabilities.
In this paper, we aim to address the forgetting issue of continuous pre-training through delta tun-
ing (Ding et al., 2022). In particular, we argue that such issue arises from the shared architecture
for joint vision and language modeling, where optimizations for vision can negatively impact lan-
guagecapabilities. Therefore, it isanaturalthoughttointroduceanindependent visualparameter
set into the pre-trained LLM, which allows formulating visual pre-training with partial parameter
tuning. ThiscanhelpretainthelanguageknowledgebyfreezingtheentireLLMduringcontinuous
pre-training,whileimprovingvisuallearning.Thisprincipleisalsoalignedwithpreviousendeavors
inmodularMLLMs,e.g.,QwenVL(Baietal.,2023b)andInternVL-1.5(Chenetal.,2024c),where
thevisualparametersareplacedoutsidetheLLM.
Basedontheaboveprinciple,weproposeanovelmonolithicMLLM,namelyMono-InternVL.As
showninFig.2, thevisualparametersinMono-InternVLaredesignedasasetofexpertnetworks
thatareseamlesslyintegratedintotheLLMviathemixture-of-expertsmechanism.Tobettercapture
thevisualknowledge, theseexpertsareinitializedfromtheFeed-ForwardNetworks(FFNs)inthe
pre-trained LLM. Based on this architecture, we present an innovative visual pre-training method
called Endogenous Visual Pre-training (EViP). Specifically, EViP is formulated as a progressive
learning process of three stages: 1) concept learning to grasp basic visual concepts, 2) semantic
learningtocapturehigh-levelsemantics,e.g.,worldknowledge,and3)alignmentlearningtoalign
knowledge with downstream tasks. Benefiting from the architecture and the pre-training strategy,
the visual scalability of Mono-InternVL is fully unleashed, where the downstream performance
consistentlyimprovesasthescaleofthepre-trainingdataincreases.Aftervisualpre-training,Mono-
InternVLaccommodatescomplexmultimodaltasksviasupervisedinstructiontuning.
To validate our method, we develop Mono-InternVL using the pre-trained LLM InternLM2-
1.8B(Caietal.,2024), andconductextensiveexperimentson16multimodalbenchmarks. Exper-
imentalresultsnotonlydemonstratethesignificantperformanceimprovementsofMono-InternVL
2Mono-InternVL
againstpreviousmonolithicMLLMs, butalsovalidateitssuperiorefficiencycomparedtoexisting
modular MLLMs. For instance, Mono-InternVL with 1.8 billion activated parameters can outper-
form existing monolithic MLLMs with 7 billion parameters by a significant margin, e.g., +15.5%
overEVEonaverage. Comparedtothestate-of-the-artmodularMLLMInternVL-1.5(Chenetal.,
2024c),Mono-InternVLshowssuperiorperformanceon6multimodalbenchmarkswhilereducing
firsttokenlatencyby67%. Inconclusion,ourcontributionscanbesummarizedinthreefold:
• WepresentMono-InternVL,anovelmonolithicMLLMthatseamlesslyintegratesasetofvisual
expertsviaamultimodalmixture-of-expertsarchitecture. Thisarchitecturecaneffectivelyextend
thepre-trainedLLMtoamonolithicMLLMwhileretainingthepre-trainedknowledge.
• We propose a novel visual pre-training approach for Mono-InternVL called endogenous visual
pre-training (EViP). EViP adopts a progressive learning strategy to encourage visual experts of
Mono-InternVLtocontinuouslygraspvisualknowledgefromnoisydatatohigh-qualitydata.
• Mono-InternVLdemonstratesforthefirsttimethattheleadingperformanceofMLLMnolonger
depends on the pre-trained visual encoder, thereby opening new avenues for designing future
MLLMs. Inparticular, Mono-InternVLachievesthestate-of-the-art(SoTA)resultscomparedto
existingMLLMson6multimodalbenchmarks.
2 RELATED WORK
Modularmultimodallargelanguagemodels. Recentprogressinlargelanguagemodels(LLMs)
has catalyzed the integration of vision and language modalities, giving rise to multimodal large
language models (MLLMs). Both commercial entities, such as GPT-4V (Yang et al., 2023) and
Geminiseries(Teametal.,2023),andotheropen-sourceinitiatives,e.g.BLIPseries(Lietal.,2022;
2023a; Dai et al., 2023), LLaVA series (Liu et al., 2023e;d; 2024), InternVL (Chen et al., 2023;
2024c), have pursued this integration, often linking LLMs (Touvron et al., 2023a;b; Team, 2023;
Cai et al., 2024) with large vision models (LVMs) (Radford et al., 2021; Dosovitskiy et al., 2021;
Dehghanietal.,2023;Chenetal.,2023)viaintermediatelayers. Specifically,therearelightweight
MLLMs (i.e., ≤ 4B parameters), such as MobileVLM-V2 (Chu et al., 2024), Mini-Gemini (Li
et al., 2024), MM1 (McKinzie et al., 2024), DeepSeek-VL (Lu et al., 2024), PaliGemma (Beyer
et al., 2024) and MiniCPM-V (Yao et al., 2024). However, such encoder-based vision-language
models (modular MLLMs) encounter challenges like limitations in input processing due to pre-
trainedvisualencoders, deploymentinefficiencies, andcomplexitiesinbalancingthecapacitiesof
LLMsandLVMs,asalsopointedoutby(Diaoetal.,2024).
Monolithic multimodal large language models. The issues related to modular MLLMs have
steered research toward encoder-free architectures, also known as monolithic MLLMs, which can
be summarized into two categories. The first category obtains continuous visual tokens through a
lightweightstructurebeforefeedingintoMLLMs. Forinstance,Fuyu-8B(Bavishietal.,2023)pro-
cessesimagesdirectlythroughasimplelinearprojection,adeptlyhandlinghigh-resolutionimages
without using a dedicated visual encoder. EVE-7B (Diao et al., 2024) prioritizes vision-language
pre-alignmentfromanLLM-centricperspectiveandenhancesimagerecognitionthroughvisualrep-
resentation supervision. SOLO (Chen et al., 2024b) introduces an open-source training recipe for
developing monolithic MLLMs. In contrast, the second category introduces VQ tokenizer-based
models to generate discrete visual tokens to support image generation tasks, with representative
workssuchasChameleon(ChameleonTeam,2024), Show-o(Xieetal.,2024), Transfusion(Zhou
etal.,2024),andEmu3(Wangetal.,2024b).
Multimodalmixture-of-experts. VLMo(Baoetal.,2022)andBEiT-3(Wangetal.,2022)employ
apoolofmodalityexpertstoreplacethefeed-forwardnetworkinstandardTransformertocapture
modality-specific information by switching to different modality experts, and use the shared self-
attentionacrossmodalitiestoalignvisualandlinguisticinformation. VL-MoE(Shenetal.,2023)
introducesmixture-of-experts(MoE)(Yukseletal.,2012)basedontheaboveworksforbettertrain-
inganddeploying.MoMa(Linetal.,2024)alsousesmultimodalmixture-of-expertsforpre-training
ofMLLMs(ChameleonTeam,2024)andcooperateswithsparsecomponents,e.g.MoEandmixture-
of-depths (MoD) (Raposo et al., 2024) to improve the efficiency of pre-training from scratch with
trillionsofmixed-modaltokens. Inspiredbytheaboveliterature,weintroducemultimodalmixture-
of-experts(i.e.,avisualexpertandalanguageexpert)forpre-trainingmonolithicMLLMsanduse
3Mono-InternVL
text tokens visual tokens
RMSNorm
Please provide a detailed Multi-Head Self-Attention
description of the cat's attire.
RMSNorm
Text Tokenizer Patch Embedding
Static Routing MoE
T-Expert V-Expert
Multimodal Large Language Model
The cat wears a yellow flower on its head, a golden �×
necklace around its neck, and pink blushes on its cheeks.
Figure 2: Monolithic architecture of Mono-InternVL. Mono-InternVL is designed as a multi-
modalMoEstructure,wherevisualandtextualtokensareprocessedbythecorrespondingexperts.
Suchadesigngreatlyfacilitatesthevisualpre-trainingwhileretainingthemodelefficiency.
anovelprogressivelearningstrategy,namedendogenousvisualpre-training(EViP),toaddressthe
uniquechallengesfacedbymonolithicMLLMs.
3 MONO-INTERNVL
3.1 THEMONOLITHICARCHITECTURE
AsshowninFig.2,wefirstoutlinethearchitectureofMono-InternVL,whichconsistsoftokenizers
andamultimodalmixture-of-expertsstructure.
Visualandtextualembeddings. ComparedtomodularMLLMs,Mono-InternVLdirectlypatchi-
fiesimagestoinputvisualsequencesusingalightweightmodule.Specifically,giventheinputimage
I ∈RH×W×3,theinputvisualembeddingx ∈R(h×w)×disobtainedby
v
x =MLP(PatchEmbed(I)+PE). (1)
v
Here, PatchEmbed(·) denotes a patch embedding layer with a stride of 28, meaning each visual
token represents a 28×28 image patch. PE ∈ R(h×w)×d is the learnable positional embedding
as similar to InternVL-1.5 (Chen et al., 2024c). Besides, we also add an additional thumbnail to
provideglobalvisualinformation. Afterthat,anMLPlayer,i.e.,MLP(·),isusedtoprojectvisual
patches into the d-dimensional embedding space of the LLM. This simple visual tokenizer allows
Mono-InternVLtoprocessarbitrary-resolutionimageswithupto8millionsofpixels,i.e.,10,240
imagepatches,whichcancovermosthigh-resolutionscenarios.
InMono-InternVL,thetextualtokenizerremainsthesameastheoriginaloneintheLLM.Inpartic-
ular,giventheinputtextT ∈Zn,weobtaintextualembeddingx ∈Rn×dby
t
x =Tokenizer(T). (2)
t
Afterwards, the multimodal embedding is constructed as the concatenation of visual and textual
embeddings,denotedasx ∈Rn′×d.
m
Multimodal mixture-of-experts structure. The key principle of Mono-InternVL is to embed vi-
sual experts into a pre-trained LLM. In this case, Mono-InternVL can not only facilitate the vi-
sual pre-training with the pre-trained LLM knowledge, but also significantly mitigates the catas-
trophicforgettingissueduringpre-training. Specifically,giventhemultimodalinputx ∈ Rn′×d,
m
4Mono-InternVL
S1.1: Concept learning S1.2: Semantic learning S1.3: Alignment learning S2: Instruction tuning
N× N× N× N×
T-Ext V-Ext T-Ext V-Ext T-Ext V-Ext T-Ext V-Ext
MHA MHA MHA MHA
Noisy Image- 922 Millions Synthetic Image- 258 Millions Task-related 143 Millions Instruction 7 Millions
Text Pairs Text Pairs Data Data
Data Type Data Scale Data Type Data Scale Data Type Data Scale Data Type Data Scale
Figure3:ThetrainingrecipeofMono-InternVL.Inthefirststage,Mono-Internvlisprogressively
pre-trainedonmassivedataviathreesub-stages(S1.1,S1.2,S1.3),wheremostparametersofLLM
are frozen to preserve the pre-trained knowledge. In the second stage (S2), the entire model is
optimizedtoaccommodatevariousinstructions.
adecoder-onlyLLMwithasetofvisualexpertsisusedtogeneratethetextualtokensstepbystep,
whichcanbeformulatedby
p =F (y |x ,y ;θ,θ ). (3)
s llm s m 0:s−1 v
Here,y ∈RS andS denotethewordlengthanditslength,respectively. p ∈Rm isthenext-token
s
probability and m is the size of word vocabulary. F and θ denote the LLM and its pre-trained
llm
parameters,respectively. θ istheparametersofpatchembeddinglayerandvisualexperts.
v
AsshowninFig.2,F isdesignedasamultimodalmixture-of-expertsstructure. Inparticular,we
llm
adoptthestaticroutingstrategythatassignsvisualandtextualexpertstothecorrespondingtokens.
Therefore,thel-thLLMlayercanbedefinedby
xl′ =xl−1+MHA(RMSNorm(xl−1)),
m m m
(4)
xl =xl′ +MMoE(RMSNorm(xl′ )).
m m m
Here, MHA(·) and RMSNorm(·) denote the multi-head attention (Vaswani et al., 2017) and the
layernormalization(Zhang&Sennrich,2019),respectively. MMoE(·)istheproposedmultimodal
mixture-of-experts,formulatedby
(cid:26)
FFN (x) ifx∈x ,
MMoE(x)= v v (5)
FFN (x) ifx∈x .
t t
Here,x ∈ Rd istheelementofx . FFN andFFN denotethevisualandtextualexperts,respec-
m v t
tively. Inpractice,FFN isinitializedfromtheFFN toleveragethepre-trainedknowledge.
v t
AsdefinedinEq.4and5,theMMoEstructurehastwodistinctadvantagesovertheexistingmono-
lithicMLLM.Firstly,thevisuallearningofMono-InternVLcangreatlybenefitfromthepre-trained
languageknowledge,whilethelanguageabilitycanstillbepreservedbyfreezingFFN . Secondly,
t
theMMoEstructuresignificantlyenhancesthemodel’scapacityforvision-and-languagemodeling,
whiletheadditionalinferencecostisalmostnegligibleduetotheMoEmechanism.
3.2 ENDOGENOUSVISUALPRE-TRAINING
EndogenousVisualPre-training(EViP)aimstomaximizethebenefitsofMono-InternVLfromvi-
sualexpertsthroughpre-trainingonmassivenoisyandsyntheticdata.Unlikeexistingmethods(Diao
etal.,2024;ChameleonTeam,2024),weformulateEViPfromtheperspectiveofdeltatuning(Ding
etal.,2022),inwhichmostoftheLLMparametersarefrozentopreserveitspre-trainedknowledge.
Therefore,theobjectiveofEViPcanbedefinedby
argminL(F llm(x m;θ,θ v),yˆ), (6)
∆θ
where L(·) and yˆdenote the auto-regressive loss and the ground-truth, respectively. As shown in
Fig.3, ∆θ denotesparametersofpatchembeddingandvisualexpertsintheconceptandsemantic
5Mono-InternVL
Table1: Summaryofdatasetsusedintheendogenousvisualpre-training. InS1.2,captionfor
eachimageissyntheticallyproducedbythepre-trainedInternVL-8B(Chenetal.,2024c).
Stage #Samples Datasets
S1.1 922M Laion-EN(en)(Schuhmannetal.,2022a),COYO(en)(Byeonetal.,2022)
S1.2 258M Laion-EN(en)(Schuhmannetal.,2022a),COYO(en)(Byeonetal.,2022),SAM(en)(Kirillovetal.,2023)
Captioning:Laion-EN(en)(Schuhmannetal.,2022a),Laion-ZH(zh)(Schuhmannetal.,2022a),
COYO(zh)(Byeonetal.,2022),GRIT(zh)(Pengetal.,2023),COCO(en)(Chenetal.,2015),
TextCaps(en)(Sidorovetal.,2020)
Detection:Objects365(en&zh)(Shaoetal.,2019),GRIT(en&zh)(Pengetal.,2023),
All-Seeing(en&zh)(Wangetal.,2024a)
OCR(large):Wukong-OCR(zh)(Guetal.,2022),LaionCOCO-OCR(en)(Schuhmannetal.,2022b),
S1.3 143M
CommonCrawlPDF(en&zh)
OCR(small):MMC-Inst(en)(Liuetal.,2023c),LSVT(zh)(Sunetal.,2019),
ST-VQA(en)(Bitenetal.,2019),RCTW-17(zh)(Shietal.,2017),ReCTs(zh)(Zhangetal.,2019),
ArT(en&zh)(Chngetal.,2019),SynthDoG(en&zh)(Kimetal.,2022),COCO-Text(en)(Veitetal.,2016),
ChartQA(en)(Masryetal.,2022),CTW(zh)(Yuanetal.,2019),DocVQA(en)(Clark&Gardner,2018),
TextOCR(en)(Singhetal.,2021),PlotQA(en)(Methanietal.,2020),InfoVQA(en)(Mathewetal.,2022)
learning, i.e., θ , while in the alignment learning stage ∆θ also includes the parameters of multi-
v
head attentions. Based on Eq. 6, EViP is designed as a progressive learning process. As shown
in Fig. 3 and Tab. 1, EViP consists of three sub-stages, namely concept learning (S1.1), semantic
learning(S1.2)andalignmentlearning(S1.3). Fordifferentsub-stages,weusecarefullypartitioned
datatoachievecoarse-to-finevisuallearning.
Concept learning. Concept learning aims to encourage the model to learn fundamental visual
concepts, such as object categories or basic shapes. Therefore, we first pre-train Mono-InternVL
withabout922millionnoisysamples,whicharesampledfromLaion-2B(Schuhmannetal.,2022a)
andCoyo-700M(Byeonetal.,2022). Inthissub-stage,Mono-InternVLemploysasimpleprompt
to perform generative learning, i.e., “provide a one-sentence caption for the image”. Meanwhile,
we constrain the maximum number of image patches of the visual tokenizer to 1,280 for training
efficiency. Toensurethatthefoundationallanguagecapabilitiesarepreservedwhileenablingvisual
specialization,theentireLLMiskeptfrozenduringconceptlearning,andonlythepatchembedding
andvisualexpertsareoptimized.
Semanticlearning. Afterconceptlearning,Mono-InternVLisabletounderstandbasicconceptsin
theimage,butorganizingthisinformationtoproducereasonabledescriptionsremainschallenging.
Toachieveahigher-levelvisualunderstanding,weutilizethepre-trainedInternVL-8B(Chenetal.,
2024c)toproduceshortcaptionsfor258millionimages. Comparedtotheoriginalnoisycaptions,
syntheticcaptionstypicallydepictcomplexvisualknowledge,suchasrelationshipandworldknowl-
edge,etc.,whilecontaininglessnoisyinformationunrelatedtotheimage,e.g.,timeofshooting,and
the photographer. In this sub-stage, we adopt the same optimization strategy as concept learning,
exceptthatthemaximumnumberofimagepatchesisincreasedto1,792.
Alignment learning. To meet the visual requirements of downstream tasks, we further perform
alignment learning on Mono-InternVL. As shown in Tab. 1, our alignment data is sampled from
thepre-trainingdataofInternVL-1.5(Chenetal.,2024c),including143millionsamplesofimage
captioning,detectionandopticalcharacterrecognition(OCR).Inparticular,captioningdata,detec-
tiondataandOCRdataaccountforabout53.9%,5.2%and40.9%ofthetotal,respectively. Inthis
sub-stage, we utilize the task-specific prompts from InternVL-1.5 for the generative learning, and
increase the maximum number of image patches to 3,328. Compared to previous sub-stages, the
multi-headattentionlayersareadditionallyoptimizedtoachievebettervision-languagealignment.
3.3 INSTRUCTIONTUNING
In this stage, we follow InternVL (Chen et al., 2024c) to use around 5 million bilingual instruc-
tionsforsupervisedlearning,coveringvarioustaskssuchasvisualquestionanswering,multimodal
dialogue, mathematics, knowledge, etc. In addition to this, we further include additional instruc-
tiondataforvideounderstandingandhandwrittentextrecognition. Inthisstage,theentiremodels
6Mono-InternVL
Table2: ComparisonwithexistingMLLMsongeneralMLLMbenchmarks. “#A-Param”de-
notesthenumberofactivatedparameters. ForMME,wesumtheperceptionandcognitionscores.
Averagescoresarecomputedbynormalizingeachmetrictoarangebetween0and100.
Model #A-ParamMMBMMVetMMMUMMEMathVistaSEED-IOCRBenchHallBCCB Avg
ModularMLLMs:
MobileVLM-V2-1.7B 1.7B 57.7 − − − − − − − − −
MobileVLM-V2-3B 3.0B 63.2 − − − − − − − − −
Mini-Gemini-2B 3.5B 59.8 31.1 31.7 1653 29.4 − − − − −
MM1-3B-MoE-Chat 3.5B 70.8 42.2 38.6 1772 32.6 69.4 − − − −
DeepSeek-VL-1.3B 2.0B 64.6 34.8 32.2 1532 31.1 66.7 409 27.6 37.6 43.4
PaliGemma-3B 2.9B 71.0 33.1 34.9 1686 28.7 69.6 614 32.2 29.6 46.7
MiniCPM-V 2.8B 64.1 31.1 38.3 1650 28.9 65.6 366 36.2 41.4 44.6
MiniCPM-V-2 2.8B 69.1 41.0 38.2 1809 38.7 67.1 605 36.1 45.3 51.2
InternVL-1.5-2B 2.2B 70.9 39.3 34.6 1902 41.1 69.8 654 37.5 63.5 54.4
MonolithicMLLMs:
Fuyu-8B(HD) 8B 10.7 21.4 − − − − − − − −
SOLO 7B − − − 1260 34.4 64.4 − − − −
Chameleon-7B1 7B 31.1 8.3 25.4 170 22.3 30.6 7 17.1 3.5 16.1
EVE-7B 7B 49.5 25.6 32.3 1483 25.2 61.3 327 21.1 12.4 34.8
EVE-7B(HD) 7B 52.3 25.7 32.6 1628 34.2 64.6 398 26.4 16.3 38.9
Emu3 8B 58.5 37.2 31.6 − − 68.2 687 − − −
Mono-InternVL-2B 1.8B 65.5 40.1 33.7 1875 45.7 67.4 767 34.8 66.3 55.2
are optimized and the maximum number of image patches is increased to 6,400 to accommodate
high-resolutionimages. DetailsofinstructiondatacanbefoundinAppendix§A.1.
4 EXPERIMENTS
4.1 EVALUATIONBENCHMARKS
We evaluate Mono-InternVL and existing MLLMs on 16 comprehensive multimodal bench-
marks. Specifically,generalMLLMbenchmarksencompassMMBench-ENtest(Liuetal.,2023f),
MMVet(Yuetal.,2023b),MMMUval(Yueetal.,2023),MME(Fuetal.,2023),MathVistatest-
mini (Lu et al., 2023), SEED Image (Ge et al., 2024), OCRBench (Liu et al., 2023g), Hallusion-
Bench(Guanetal.,2023),andCCBenchdev(Liuetal.,2023f). Visualquestionansweringbench-
marksincludeTextVQAval(Singhetal.,2019),SQAtest (Luetal.,2022a),GQAtest-dev(Hud-
son&Manning,2019),DocVQAtest(Clark&Gardner,2018),AI2Dtest(Kembhavietal.,2016),
ChartQAtest(Masryetal.,2022),andInfographicVQAtest(Mathewetal.,2022). Theevaluation
metrics follow the existing methods (Chen et al., 2024c; Diao et al., 2024). Part of the results of
ChameleonandEVEareevaluatedwithVLMEvalKit(Duanetal.,2024)orfromtheOpenCompass
leaderboard(Contributors,2023).
4.2 IMPLEMENTATIONDETAILS
Mono-InternVL is implemented based on InternLM2-1.8B (Cai et al., 2024) with newly added
visual tokenizer and visual experts. The visual experts are initialized from pre-trained MLPs in
InternLM2-1.8Btoleverageexistinglearnedrepresentationsforimprovedvisualfeatureextraction,
which accounts for 1.2 billion parameters. We adopt a similar dynamic high-resolution strategy
from InternVL-1.5 (Chen et al., 2024c) to align an optimal resolution for input image, which is
thenpatchfiedtovisualtokens. TheremainingconfigurationsarekeptidenticaltoInternLM2-1.8B.
Theendogenousvisualpre-trainingandinstructiontuningtakeapproximately16days(646kitera-
tions)and1day(14kiterations)on256NVIDIAA100GPUs,respectively. Moredetailedtraining
configurationsaregiveninAppendix§A.2.
1Chameleon-7Bfrequentlyrejectstoperformthetaskwitharesponseof“Ican’thelpyouwiththis”,thus
resultinginpoorperformance.
7Mono-InternVL
Table3: ComparisonwithexistingMLLMsonvisualquestionansweringbenchmarks.
Model #A-Param TextVQA SQA-I GQA DocVQA AI2D ChartQA InfoVQA Avg
ModularMLLMs:
MobileVLM-V2-1.7B 1.7B 52.1 66.7 59.3 − − − − −
MobileVLM-V2-3B 3.0B 57.5 70.0 66.1 − − − − −
Mini-Gemini-2B 3.5B 56.2 − − 34.2 − − − −
MM1-3B-MoE-Chat 3.5B 72.9 76.1 − − − − − −
DeepSeek-VL-1.3B 2.0B 57.8 − − − 51.5 − − −
PaliGemma-3B 2.9B 68.1 − − − 68.3 − − −
MiniCPM-V 2.8B 60.6 − − 38.2 56.3 − − −
MiniCPM-V-2 2.8B 74.1 − − 71.9 62.9 − − −
InternVL-1.5-2B 2.2B 70.5 84.9 61.6 85.0 69.8 74.8 55.4 71.7
MonolithicMLLMs:
Fuyu-8B(HD) 8B − − − − 64.5 − − −
SOLO 7B − 73.3 − − 61.4 − − −
Chameleon-7B1 7B 4.8 47.2 − 1.5 46.0 2.9 5.0 17.9
EVE-7B 7B 51.9 63.0 60.8 22.0 48.5 19.5 20.0 40.8
EVE-7B(HD) 7B 56.8 64.9 62.6 53.0 61.0 59.1 25.0 54.6
Emu3 8B 64.7 89.2 60.3 76.3 70.0 68.6 43.8 67.6
Mono-InternVL-2B 1.8B 72.6 93.6 59.5 80.0 68.6 73.7 43.0 70.1
Table4: Zero-shotpre-trainingperformanceofMono-InternVLandexistingMLLMs. “S1.2”
and “S1.3” denote pre-training stages of semantic learning and alignment learning, respectively.
ImagesofCOCOhavebeenseeninMono-InternVL-S1.3,sowemarkitsperformanceingray.
Model #A-Param Data Shots COCOCaps Flickr30k NoCaps VQAv2
Flamingo 3B >2.1B 0 73.0 − − 49.2
MM1 3.5B >2.3B 0 73.5 − 55.6 46.2
Chameleon 34B >1.4B 2 120.2 74.7 − 66.0
Mono-InternVL-S1.2 1.8B 0.9B 0 87.3 72.7 54.1 −
Mono-InternVL-S1.3 1.8B 1.1B 0 135.6 77.3 116.5 71.1
4.3 QUANTITATIVEEXPERIMENTS
Comparison with existing MLLMs. In Tab. 2 and 3, we compare Mono-InternVL and existing
MLLMs on 16 multimodal benchmarks. From Tab. 2, the first observation is that most modular
MLLMs outperform existing monolithic MLLMs by significant margins. For example, the aver-
ageperformanceofInternVL-1.5-2B(Chenetal.,2024c)on9MLLMbenchmarksgreatlyexceeds
the SoTA monolithic MLLM, i.e., + 15.5% over EVE-7B (HD) (Diao et al., 2024). These results
strongly suggest the challenges in existing monolithic MLLMs. In contrast, Mono-InternVL-2B
with a slightly smaller model size can even outperform the SoTA modular MLLM, i.e., + 0.8%
against InternVL-1.5-2B on average. Notably, Mono-InternVL-2B demonstrates distinct advan-
tages on MathVista and OCRBench, suggesting its seamless text recognition and reasoning capa-
bilities. Moreover, superiorbilingualabilityofMono-InternVL-2BisalsovalidatedonCCBench,
whichcontainsalargeamountofquestionsrelatedtoChineseculture. Comparedtoexistingmono-
lithicMLLMs,performancegainsofMono-InternVLaremoredistinct,e.g.,+15.4%overEVE-7B
(HD)(Diaoetal.,2024)onMMVetand+7.9%overEmu3(Wangetal.,2024b)onTextVQA,while
using a much smaller parameter scale. Similar advantages of Mono-InternVL can also been wit-
nessedinTab.3,e.g.,+2.1%onTextVQA.Nevertheless,wealsoobservethatMono-InternVLisstill
inferiortoInternVL-1.5onhigh-resolutionbenchmarks,e.g.,-12.4%onInfoVQA.Thisisbecause
specific optimizations for high-resolution encoding are not the focus of the paper, e.g., positional
embeddingandhigh-resolutiontrainingdata,whichweplantoexploreinfutureresearch. Overall,
thesecomparisonssignificantlyvalidatethearchitectureandtrainingstrategyofMono-InternVL.
InTab.4,wefurthercomparethepre-trainingperformanceofMono-InternVLandexistingMLLMs.
From this table, we can observe that with concept and semantic learning, Mono-InternVL-S1.2
8Mono-InternVL
Table 5: Ablation of different strategies for visual pre-training. All models are pre-trained on
61millionimage-textpairsfromLaion-2B(Schuhmannetal.,2022a)andfine-tunedoninstruction
datafromLLaVA-665k.(Liuetal.,2023d). “Full”and“Delta”denotefulltuninganddeltatuning,
respectively. “T-Param”referstotrainableparameters.
Model #T-Param Strategy MME-P DocVQA InfoVQA SQA-I GQA ChartQA AI2D
InternLM2 1.8B Full 753 16.1 11.6 36.7 51.4 10.8 27.7
+V-Expert 3.0B Full 948 18.6 11.9 37.7 53.0 11.1 26.6
+V-Expert 1.2B Delta 995 18.9 14.6 56.5 53.4 13.5 42.7
Captioning (Avg. of 3) VQA (Avg. of 8) MME perception score
100 S1.1 S1.2 S1.3 55 S1.1 S1.2 S1.3 1300 S1.1 S1.2 S1.3
95 1250
90 50 1200
85 1150
80 45
1100
75
40 1050
70
1000
65 35
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400
Data Scale (M) Data Scale (M) Data Scale (M)
Figure 4: Downstream performance breakdown with the increase of pre-training data size
acrossthreesub-stages: (S1.1)Conceptlearning; (S1.2)Semanticlearning; (S1.3)Alignment
learning. Foreachdatapoint,wefine-tunethecorrespondingpre-trainedmodelontheinstruction
dataofLLaVA-665kandobtainthedownstreamperformance. ResultsofcaptioningandVQAare
averagedfrom3and8tasks,respectively. SeeAppendix§A.3forcompleteresults.
already exceeds existing modular MLLMs, e.g., +13.8 CIDEr over MM1 (McKinzie et al., 2024)
onCOCOCaptions,demonstratingthatMono-InternVL-S1.2iseffectiveincapturingfundamental
multimodalrelationships.Itisworthnotingthatpre-traininginMono-InternVL-S1.2onlyconsumes
about 0.9B image-text pairs, but the cost in MM1 and Flamingo is much more expensive, e.g.,
morethan2Bdata. ComparedtomonolithicMLLMs, Mono-InternVLalsodemonstratessuperior
performance. Forinstance,eventhoughChameleonhasamuchlargermodelsize,itisstillinferior
to Mono-InternVL-S1.3 by -2.6 CIDEr on Flickr30k (Young et al., 2014). These results further
confirmtheeffectivenessofEViPforMono-InternVL.
Ablationstudies. TovalidatethedesignofMono-InternVL,weconductextensiveablationstudies
inTab.5andFig.4.Specifically,Tab.5comparesdifferentstrategiesforvisualpre-training.Thefirst
rowisthecommonstrategyusedinexistingmonolithicMLLMs,i.e.,fulltuningoftheLLM,which
yieldstheworstdownstreamperformanceinthetable. Afteremployingvisualexperts(thesecond
row),suchafull-tuningstrategybecomesmoreeffective,e.g.,+1.6%onGQA.Thesecomparisons
well validate that the shared architecture for joint vision and language modeling is sub-optimal in
monolithic MLLMs. Besides, we also observe that the delta tuning strategy greatly benefits the
visual pre-training, providing +18.8% and 16.1% gains on SQA-I and AI2D, respectively. Com-
pared to full tuning, delta tuning can effectively preserve the knowledge of the pre-trained LLM,
whichisalsocrucialformaintainingthelanguageunderstandingcapabilitiesrequiredforeffective
multimodalinteractions.
Fig.4furtherdemonstratestherelationshipbetweendownstreamperformanceandpre-trainingdata
size. From it we can observe that performance of Mono-InternVL will gradually reach an upper
bound in the concept learning. Through additional semantic learning and alignment learning, ca-
pabilities of Mono-InternVL consistently boost as the data size increases. It is important to note
thatthatthealignmentlearningplaysasignificantroleforVQAandMME,whichcanprovidesuf-
ficienttask-related knowledge, e.g., OCRknowledge. Theseresults notonly demonstratethe data
scalabilityofMono-InternVL,butalsoconfirmtheadvantagesofcoarse-to-finelearninginEViP.
Comparisonofinferenceefficiency. InTab.6,wecomparetheinferencespeedofMono-InternVL
andInternVL-1.5usingthepopulardeploymentlibraryLMDeploy(LMDeployContributors,2023).
Fromthistable,wecanfindthatduetotheeliminationofvisualencoder,Mono-InternVLdemon-
stratessuperiorefficiencyunderdifferentnumberofinputtokens. Inparticular,thefirst-tokentime
isgreatlyreducedinMono-InternVL,e.g.,upto-67%againstInternVL-1.5. Benefitingfromthis,
theoverallthroughputiscorrespondinglyincreasedbyaround31%. Theseresultsgreatlyvalidate
9Mono-InternVL
Table 6: Inference speed comparison of Mono-InternVL and InternVL-1.5. Models are de-
ployed on an NVIDIA A100 using LMDeploy with Pytorch backend (LMDeployContributors,
2023),withaconcurrencyof16andthenumberofoutputtokensfixedas120. “TTFT”and“TPS”
denotesthetimetofirsttokeninsecondsandthroughputintokenspersecond,respectively.
#Image #Text #TotalInput
Model TTFT TPS
Tokens Tokens Tokens
InternVL-1.5-2B 768 256 1024 0.24 382
Mono-InternVL-2B 768 256 1024 0.09(-63%) 436(+14%)
InternVL-1.5-2B 1792 256 2048 0.45 183
Mono-InternVL-2B 1792 256 2048 0.15(-67%) 232(+27%)
InternVL-1.5-2B 3840 256 4096 1.93 52
Mono-InternVL-2B 3840 256 4096 0.79(-59%) 68(+31%)
Resolution: 600×600 Layer-1 Layer-8 Layer-16 Layer-24
40
256
Q: Provide a one-sentence
caption for the image.
A: A wooden chair with a 18
woven seat and back.
Resolution: 1024×768 Layer-1 Layer-8 Layer-16 Layer-24
40
1792
Q: what are the words in
white?
A: The words in white 15
are "AI Weiwei".
Figure5: VisualizationofattentionmapsinMono-InternVL.Thefirstbluesegment,greenseg-
ment and the second green segment in the axes represent the system prompt tokens (text), image
tokens(visual)anduserprompttokens(text),respectively. Thenumbersontheleftsideofattention
mapsindicatethenumberoftokens.
theefficiencyofMono-InternVL.Wenotethatthisisonlyaninitialattempt,andusingTurbomind
backendorfurtheroptimizationtechniquesmayyieldbetterperformance.
4.4 QUALITATIVEEXPERIMENTS
Togainin-depthinsightsintoMono-InternVL,wevisualizeitsattentionmapsofdifferentlayersin
Fig.5. Fromthisfigure, wecandrawtwonoteworthyfindings. Firstly, despitetheglobalconnec-
tivity of the Transformer architecture, locality still exists in the visual encoding of shallow layers.
AsshowninFig.5,inthefirstlayer,visualtokensonlyinteractwiththeirnearbycontent,yielding
patterns that closely resemble those produced by convolutional neural networks (He et al., 2016).
Secondly,modalitiesarebarelyinteractiveatshallowlayersbutgraduallyfusedasthelayersdeepen.
AsillustratedinFig.5,theattentionweightsbetweenvisualandtextualtokensareextremelysmall
inthefirstlayerandbecomelargerindeeperlayers. Webelievetheseexampleswillprovideuseful
hintsforthedesignofmonolithicMLLMs. MoreexamplesaregiveninAppendix§A.4.
5 CONCLUSION
Inthispaper,weproposeMono-InternVL,amonolithicmultimodallargelanguagemodel(MLLM)
thatintegratesvisualencodingandtextualdecodingintoasingleLLM.InMono-InternVL,asetof
visualexpertsisembeddedintothepre-trainedLLMviaamixture-of-expertsmechanism.Byfreez-
ingtheLLM,Mono-InternVLensuresthatvisualcapabilitiesareoptimizedwithoutcompromising
the pre-trained language knowledge. Based on this structure, an innovative Endogenous Visual
Pre-training (EViP) is introduced to realize coarse-to-fine visual learning. Extensive experiments
demonstratetheeffectivenessandefficiencyofMono-InternVLcomparedtoexistingMLLMs. Our
workgreatlypushestheboundariesofmonolithicMLLMs, providingnewpossibilitiesforthede-
velopmentofMLLMs.
10Mono-InternVL
REFERENCES
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,GaoLiu,
Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi
Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng
Xu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,BowenYu,HongyiYuan,
Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,
Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv: 2309.16609,
2023a.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023b.
Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng,
Xincheng Zhang, Nuo Ma, Zekun Wang, et al. Coig-cqia: Quality is all you need for chinese
instructionfine-tuning. arXivpreprintarXiv:2403.18058,2024.
HangboBao, WenhuiWang, LiDong, QiangLiu, OwaisKhanMohammed, KritiAggarwal, Sub-
hojit Som, Songhao Piao, and Furu Wei. Vlmo: Unified vision-language pre-training with
mixture-of-modality-experts. Advances in Neural Information Processing Systems, 35:32897–
32912,2022.
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani,
andSag˘nakTas¸ırlar. Introducingourmultimodalmodels,2023. URLhttps://www.adept.
ai/blog/fuyu-8b.
LucasBeyer,AndreasSteiner,Andre´SusanoPinto,AlexanderKolesnikov,XiaoWang,DanielSalz,
Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al.
Paligemma: Aversatile3bvlmfortransfer. arXivpreprintarXiv:2407.07726,2024.
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marc¸al Rusinol, Ernest Valveny,
CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, pp.
4291–4301,2019.
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon
Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/
coyo-dataset,2022.
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui
Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297,
2024.
JieCaoandJingXiao. Anaugmentedbenchmarkdatasetforgeometricquestionansweringthrough
dualparalleltextencoding. InCOLING,pp.1511–1520,2022.
ChameleonTeam. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024.
GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhi-
hongChen,JianquanLi,XiangWan,andBenyouWang. Allava: Harnessinggpt4v-synthesized
dataforalitevision-languagemodel. arXivpreprintarXiv:2402.11684,2024a.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprintarXiv:1504.00325,2015.
Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. A single transformer for scalable vision-
languagemodeling. arXivpreprintarXiv:2407.06438,2024b.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qing-
long Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. In-
ternvl:Scalingupvisionfoundationmodelsandaligningforgenericvisual-linguistictasks.arXiv:
2312.14238,2023.
11Mono-InternVL
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong,
Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to
commercialmultimodalmodelswithopen-sourcesuites. arXiv:2404.16821,2024c.
Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing
Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on
arbitrary-shapedtext-rrc-art. InICDAR,pp.1571–1576,2019.
XiangxiangChu,LimengQiao,XinyuZhang,ShuangXu,FeiWei,YangYang,XiaofeiSun,Yiming
Hu,XinyangLin,BoZhang,etal.Mobilevlmv2:Fasterandstrongerbaselineforvisionlanguage
model. arXivpreprintarXiv:2402.03766,2024.
ChristopherClarkandMattGardner. Simpleandeffectivemulti-paragraphreadingcomprehension.
InACL,pp.845–855,2018.
Contributors. Opencompass: A universal evaluation platform for foundation models. https:
//github.com/open-compass/opencompass,2023.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangLi, PascaleFung, andStevenC.H.Hoi. Instructblip: Towardsgeneral-purposevision-
languagemodelswithinstructiontuning. InNeurIPS,2023.
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose´ MF Moura, Devi
Parikh,andDhruvBatra. Visualdialog. InCVPR,pp.326–335,2017.
MostafaDehghani,JosipDjolonga,BasilMustafa,PiotrPadlewski,JonathanHeek,JustinGilmer,
Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe
Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme
Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste,
GamaleldinFathyElsayed,AravindhMahendran,FisherYu,AvitalOliver,FantineHuot,Jasmijn
Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos,
YiTay,ThomasMensink,AlexanderKolesnikov,FilipPavetic,DustinTran,ThomasKipf,Mario
Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision
transformersto22billionparameters. InICML,volume202,pp.7480–7512,2023.
HaiwenDiao,YufengCui,XiaotongLi,YuezeWang,HuchuanLu,andXinlongWang. Unveiling
encoder-freevision-languagemodels. arXivpreprintarXiv:2406.11832,2024.
NingDing,YujiaQin,GuangYang,FuchaoWei,ZonghanYang,YushengSu,ShengdingHu,Yulin
Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter
efficientmethodsforpre-trainedlanguagemodels. arXivpreprintarXiv:2203.06904,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko-
reit,andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionat
scale. InICLR,2021.
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong,
Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source
toolkitforevaluatinglargemulti-modalitymodels,2024. URLhttps://arxiv.org/abs/
2407.11691.
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,Wei
Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels. arXiv: 2306.13394,2023.
Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making
LLaMASEEanddrawwithSEEDtokenizer. InTheTwelfthInternationalConferenceonLearn-
ingRepresentations,2024. URLhttps://openreview.net/forum?id=0Nui91LBQS.
YashGoyal, TejasKhot, DouglasSummers-Stay, DhruvBatra, andDeviParikh. MakingtheVin
VQAmatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering. InCVPR,
pp.6325–6334,2017.
12Mono-InternVL
JiaxiGu,XiaojunMeng,GuansongLu,LuHou,NiuMinzhe,XiaodanLiang,LeweiYao,Runhui
Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese cross-modal
pre-trainingbenchmark. NeurIPS,35:26418–26431,2022.
TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,ZongxiaLi,XiaoyuLiu,XijunWang,Lichang
Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite
for entangled language hallucination & visual illusion in large vision-language models. arXiv:
2310.14566,2023.
Jean-Philippe Thiran Guillaume Jaume, Hazim Kemal Ekenel. Funsd: A dataset for form under-
standinginnoisyscanneddocuments. InAcceptedtoICDAR-OST,2019.
ConghuiHe,ZhenjiangJin,ChaoXu,JiantaoQiu,BinWang,WeiLi,HangYan,JiaqiWang,and
Dahua Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese
largemodels. arXivpreprintarXiv:2308.10755,2023.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. InCVPR,pp.770–778,2016.
ZhengHuang,KaiChen,JianhuaHe,XiangBai,DimosthenisKaratzas,ShijianLu,andCVJawa-
har. Icdar2019competitiononscannedreceiptocrandinformationextraction. In2019Interna-
tionalConferenceonDocumentAnalysisandRecognition(ICDAR),pp.1516–1520.IEEE,2019.
DrewA.HudsonandChristopherD.Manning. GQA:Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. InCVPR,pp.6700–6709,2019.
Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human
tasksinegocentricvideos. AdvancesinNeuralInformationProcessingSystems,35:3343–3360,
2022.
KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa: Understandingdatavisual-
izationsviaquestionanswering. InCVPR,pp.5648–5656,2018.
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali
Farhadi. Adiagramisworthadozenimages. InECCV,pp.235–251,2016.
AniruddhaKembhavi,MinjoonSeo,DustinSchwenk,JonghyunChoi,AliFarhadi,andHannaneh
Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal
machinecomprehension. InCVPR,pp.4999–5007,2017.
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,
Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document un-
derstandingtransformer. InECCV,2022.
AlexanderKirillov, EricMintun, NikhilaRavi, HanziMao, Chloe´ Rolland, LauraGustafson, Tete
Xiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDolla´r,andRossB.Girshick.
Segmentanything. arXiv: 2304.02643,2023.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei.
Visual genome: Connecting language and vision using crowdsourced dense image annotations.
IJCV,123(1):32–73,2017.
Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang
Bai. Visual information extraction in the wild: practical dataset and end-to-end solution. In
InternationalConferenceonDocumentAnalysisandRecognition,pp.36–53.Springer,2023.
LAION. Gpt-4v dataset. https://huggingface.co/datasets/laion/
gpt4v-dataset,2023.
Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve´ Le Borgne, Romaric Besanc¸on, Jose´ G
Moreno, and Jesu´s Lovo´n Melgarejo. Viquae, a dataset for knowledge-based visual question
answeringaboutnamedentities. InSIGIR,pp.3108–3120,2022.
13Mono-InternVL
JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. BLIP:bootstrappinglanguage-image
pre-trainingforunifiedvision-languageunderstandingandgeneration. InICLR,volume162,pp.
12888–12900,2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-
image pre-training with frozen image encoders and large language models. In ICML, volume
202,pp.19730–19742,2023a.
KunChangLi, YinanHe, YiWang, YizhuoLi, WenhaiWang, PingLuo, YaliWang, LiminWang,
and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355,
2023b.
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXiv: 2403.18814,2024.
Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin
Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robust-
nessinvisualreasoning. InCVPR,pp.14963–14973,2023c.
Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke
Zettlemoyer,andArmenAghajanyan. Moma: Efficientearly-fusionpre-trainingwithmixtureof
modality-awareexperts. arXivpreprintarXiv:2407.21770,2024.
Adam Dahlgren Lindstro¨m and Savitha Sam Abraham. Clevr-math: A dataset for compositional
language,visualandmathematicalreasoning. arXivpreprintarXiv:2208.05358,2022.
FangyuLiu,GuyEmerson,andNigelCollier. Visualspatialreasoning. TACL,11:635–651,2023a.
FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Aligninglarge
multi-modalmodelwithrobustinstructiontuning. arXivpreprintarXiv:2306.14565,2023b.
FuxiaoLiu,XiaoyangWang,WenlinYao,JianshuChen,KaiqiangSong,SangwooCho,YaserYa-
coob,andDongYu. Mmc: Advancingmultimodalchartunderstandingwithlarge-scaleinstruc-
tiontuning. arXivpreprintarXiv:2311.10774,2023c.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. arXiv: 2310.03744,2023d.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023e.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae
Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. URL https:
//llava-vl.github.io/blog/2024-01-30-llava-next/.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modal
modelanall-aroundplayer? arXiv: 2307.06281,2023f.
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu,
MingruiChen,ChunyuanLi,LianwenJin,etal.Onthehiddenmysteryofocrinlargemultimodal
models. arXivpreprintarXiv:2305.07895,2023g.
LMDeployContributors. Lmdeploy: A toolkit for compressing, deploying, and serving llm.
https://github.com/InternLM/lmdeploy,2023.
HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,YaofengSun,etal.Deepseek-vl:Towardsreal-worldvision-languageunderstanding.
arXivpreprintarXiv:2403.05525,2024.
PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-ChunZhu.
Inter-gps: Interpretablegeometryproblemsolvingwithformallanguageandsymbolicreasoning.
arXivpreprintarXiv:2105.04165,2021.
14Mono-InternVL
PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
sciencequestionanswering. InNeurIPS,2022a.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured
mathematicalreasoning. arXivpreprintarXiv:2209.14610,2022b.
PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-
WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoningof
foundationmodelsinvisualcontexts. arXiv: 2310.02255,2023.
JunhuaMao,JonathanHuang,AlexanderToshev,OanaCamburu,AlanL.Yuille,andKevinMur-
phy. Generationandcomprehensionofunambiguousobjectdescriptions. InCVPR,pp.11–20,
2016.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual
questionansweringbenchmarkrequiringexternalknowledge. InCVPR,pp.3195–3204,2019.
AhmedMasry, XuanLongDo,JiaQingTan, ShafiqJoty, andEnamulHoque. Chartqa: Abench-
markforquestionansweringaboutchartswithvisualandlogicalreasoning. InACL,pp.2263–
2279,2022.
MineshMathew,VirajBagal,Rube`nTito,DimosthenisKaratzas,ErnestValveny,andCVJawahar.
Infographicvqa. InWACV,pp.1697–1706,2022.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet
Singh, DougKang, AnkurJain, HongyuHe`, MaxSchwarzer, TomGunter, XiangKong, Aonan
Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee,
ZiruiWang,RuomingPang,PeterGrasch,AlexanderToshev,andYinfeiYang. MM1: methods,
analysis&insightsfrommultimodalLLMpre-training. arXiv: 2403.09611,2024.
NiteshMethani, PrithaGanguly, MiteshMKhapra, andPratyushKumar. Plotqa: Reasoningover
scientificplots. InWACV,pp.1527–1536,2020.
AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. Ocr-vqa: Visual
questionansweringbyreadingtextinimages. InICDAR,pp.947–952,2019.
OpenAI. GPT-4technicalreport. arXiv: 2303.08774,2023.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824,2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In ICML,
volume139,pp.8748–8763,2021.
David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and
Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based lan-
guagemodels. arXivpreprintarXiv:2404.02258,2024.
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Chris Pal, Hugo
Larochelle, Aaron Courville, and Bernt Schiele. Movie description. Interna-
tional Journal of Computer Vision, 2017. URL http://link.springer.com/
article/10.1007/s11263-016-0987-1?wt_mc=Internal.Event.1.SEM.
ArticleAuthorOnlineFirst.
ChristophSchuhmann, RomainBeaumont, RichardVencu, CadeGordon, RossWightman, Mehdi
Cherti, TheoCoombes, AarushKatta, ClaytonMullis, MitchellWortsman, etal. Laion-5b: An
open large-scale dataset for training next generation image-text models. NeurIPS, 35:25278–
25294,2022a.
15Mono-InternVL
Christoph Schuhmann, Andreas Ko¨pf, Richard Vencu, Theo Coombes, and Romain Beaumont.
Laioncoco: 600msyntheticcaptionsfromlaion2b-en. https://laion.ai/blog/laion-coco/,2022b.
DustinSchwenk,ApoorvKhandelwal,ChristopherClark,KennethMarino,andRoozbehMottaghi.
A-okvqa: A benchmark for visual question answering using world knowledge. In ECCV, pp.
146–162,2022.
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-
awarevisualquestionanswering. InAAAI,volume33,pp.8876–8884,2019.
AmirShahroudy,JunLiu,Tian-TsongNg,andGangWang. Nturgb+d:Alargescaledatasetfor3d
humanactivityanalysis. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.1010–1019,2016.
ShuaiShao, ZemingLi, TianyuanZhang,ChaoPeng,GangYu, XiangyuZhang, JingLi,andJian
Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, pp. 8430–
8439,2019.
Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling
vision-languagemodelswithsparsemixtureofexperts. arXivpreprintarXiv:2303.07226,2023.
Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie,
ShijianLu,andXiangBai. Icdar2017competitiononreadingchinesetextinthewild(rctw-17).
InICDAR,volume1,pp.1429–1434,2017.
OleksiiSidorov, RonghangHu, MarcusRohrbach, andAmanpreetSingh. Textcaps: Adatasetfor
imagecaptioningwithreadingcomprehension. InECCV,volume12347,pp.742–758,2020.
AmanpreetSingh, VivekNatarajan, MeetShah, YuJiang, XinleiChen, DhruvBatra, DeviParikh,
andMarcusRohrbach. TowardsVQAmodelsthatcanread. InCVPR,2019.
AmanpreetSingh,GuanPang,MandyToh,JingHuang,WojciechGaluba,andTalHassner.Textocr:
Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In CVPR, pp. 8802–
8812,2021.
Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han,
ErruiDing,JingtuoLiu,DimosthenisKaratzas,etal. Icdar2019competitiononlarge-scalestreet
viewtextwithpartiallabeling-rrc-lsvt. InICDAR,pp.1557–1562,2019.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-
following model. Stanford Center for Research on Foundation Models. https://crfm. stanford.
edu/2023/03/13/alpaca.html,3(6):7,2023.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXiv: 2312.11805,2023.
InternLMTeam.Internlm:Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities.
https://github.com/InternLM/InternLM,2023.
Teknium. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassistants. https:
//huggingface.co/datasets/teknium/OpenHermes-2.5,2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aure´lien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
languagemodels. arXiv: 2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXiv: 2307.09288,2023b.
16Mono-InternVL
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNIPS,pp.5998–6008,2017.
Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text:
Dataset and benchmark for text detection and recognition in natural images. arXiv preprint
arXiv:1601.07140,2016.
Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to
believe: Promptinggpt-4vforbettervisualinstructiontuning. arXivpreprintarXiv:2311.07574,
2023.
WeiyunWang,MinShi,QingyunLi,WenhaiWang,ZhenhangHuang,LinjieXing,ZheChen,Hao
Li,XizhouZhu,ZhiguoCao,etal. Theall-seeingproject: Towardspanopticvisualrecognition
andunderstandingoftheopenworld. InICLR,2024a.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign
language: Beitpretrainingforallvisionandvision-languagetasks. arXiv: 2208.10442,2022.
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan
Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya
Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu,
Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you
need. arXiv: 2409.18869,2024b.
XiyaoWang, YuhangZhou, XiaoyuLiu, HongjinLu, YuanchengXu, FeihongHe, JaehongYoon,
TaixiLu,GedasBertasius,MohitBansal,etal.Mementos:Acomprehensivebenchmarkformul-
timodallargelanguagemodelreasoningoverimagesequences.arXivpreprintarXiv:2401.10529,
2024c.
BoWu,ShoubinYu,ZhenfangChen,JoshuaBTenenbaum,andChuangGan. Star: Abenchmark
for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information
ProcessingSystems(NeurIPS),2021.
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,
YuchaoGu,ZhijieChen,ZhenhengYang,andMikeZhengShou.Show-o:Onesingletransformer
tounifymultimodalunderstandingandgeneration. arXivpreprintarXiv:2408.12528,2024.
ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,andLijuan
Wang. Thedawnoflmms: Preliminaryexplorationswithgpt-4v(ision). arXiv: 2309.17421,9,
2023.
YuanYao, TianyuYu, AoZhang, ChongyiWang, JunboCui, HongjiZhu, TianchiCai, HaoyuLi,
WeilinZhao, ZhihuiHe, etal. Minicpm-v: Agpt-4vlevelmllmonyourphone. arXivpreprint
arXiv:2408.01800,2024.
PeterYoung,AliceLai,MicahHodosh,andJuliaHockenmaier. Fromimagedescriptionstovisual
denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:
67–78,2014.
LichengYu,PatrickPoirson,ShanYang,AlexanderC.Berg,andTamaraL.Berg.Modelingcontext
inreferringexpressions. InECCV,volume9906,pp.69–85,2016.
LonghuiYu,WeisenJiang,HanShi,JinchengYu,ZhengyingLiu,YuZhang,JamesTKwok,Zhen-
guoLi,AdrianWeller,andWeiyangLiu. Metamath:Bootstrapyourownmathematicalquestions
forlargelanguagemodels. arXivpreprintarXiv:2309.12284,2023a.
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
andLijuanWang.Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities.arXiv:
2308.02490,2023b.
Tai-LingYuan,ZheZhu,KunXu,Cheng-JunLi,Tai-JiangMu,andShi-MinHu. Alargechinese
textdatasetinthewild. JournalofComputerScienceandTechnology,34:509–521,2019.
17Mono-InternVL
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
DongfuJiang,WeimingRen,YuxuanSun,etal. Mmmu: Amassivemulti-disciplinemultimodal
understandingandreasoningbenchmarkforexpertagi. arXiv: 2311.16502,2023.
SenihaEsenYuksel,JosephNWilson,andPaulDGader. Twentyyearsofmixtureofexperts. IEEE
transactionsonneuralnetworksandlearningsystems,23(8):1177–1193,2012.
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. In-
vestigating the catastrophic forgetting in multimodal large language models. arXiv preprint
arXiv:2309.10313,2023.
BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization. AdvancesinNeuralInfor-
mationProcessingSystems,32,2019.
Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang,
MinghuiLiao,MingkunYang,etal. Icdar2019robustreadingchallengeonreadingchinesetext
onsignboard. InICDAR,pp.1577–1581,2019.
Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. arXiv:
2307.04087,2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbotarena. NeurIPS,36,2024.
ChuntingZhou,LiliYu,ArunBabu,KushalTirumala,MichihiroYasunaga,LeonidShamis,Jacob
Kahn, XuezheMa, LukeZettlemoyer, andOmerLevy. Transfusion: Predictthenexttokenand
diffuseimageswithonemulti-modalmodel. arXivpreprintarXiv:2408.11039,2024.
18Mono-InternVL
A APPENDIX
A.1 MOREDATASETDETAILS
Table7: Summaryofdatasetsusedininstructiontuning.
Task Dataset
Captioning TextCaps(en)(Sidorovetal.,2020),ShareGPT-4o(en&zh)(Chenetal.,2024c)
VQAv2(en)(Goyaletal.,2017),GQA(en)(Hudson&Manning,2019),OKVQA(en)(Marinoetal.,2019),
GeneralQA
VSR(en)(Liuetal.,2023a),VisualDialog(en)(Dasetal.,2017)
Science AI2D(en)(Kembhavietal.,2016),ScienceQA(en)(Luetal.,2022a),TQA(en)(Kembhavietal.,2017)
ChartQA(en)(Masryetal.,2022),MMC-Inst(en)(Liuetal.,2023c),DVQA(en)(Kafleetal.,2018),
Chart
PlotQA(en)(Methanietal.,2020),LRV-Instruction(en)(Liuetal.,2023b)
GeoQA+(en)(Cao&Xiao,2022),TabMWP(en)(Luetal.,2022b),MathQA(en)(Yuetal.,2023a),
Mathematics
CLEVR-Math/Super(en)(Lindstro¨m&Abraham,2022;Lietal.,2023c),Geometry3K(en)(Luetal.,2021)
KVQA(en)(Shahetal.,2019),A-OKVQA(en)(Schwenketal.,2022),ViQuAE(en)(Lerneretal.,2022),
Knowledge
Wikipedia(en&zh)(Heetal.,2023)
OCRVQA(en)(Mishraetal.,2019),InfoVQA(en)(Mathewetal.,2022),TextVQA(en)(Singhetal.,2019),
ArT(en&zh)(Chngetal.,2019),COCO-Text(en)(Veitetal.,2016),CTW(zh)(Yuanetal.,2019),
OCR
LSVT(zh)(Sunetal.,2019),RCTW-17(zh)(Shietal.,2017),ReCTs(zh)(Zhangetal.,2019),
SynthDoG(en&zh)(Kimetal.,2022),ST-VQA(en)(Bitenetal.,2019)
Document DocVQA(en)(Clark&Gardner,2018),CommonCrawlPDF(en&zh)
Grounding RefCOCO/+/g(en)(Yuetal.,2016;Maoetal.,2016),VisualGenome(en)(Krishnaetal.,2017)
LLaVA-150K(en&zh)(Liuetal.,2023e),LVIS-Instruct4V(en)(Wangetal.,2023),
Conversation ALLaVA(en&zh)(Chenetal.,2024a),Laion-GPT4V(en)(LAION,2023),
ShareGPT(en&zh)(Zhengetal.,2024),SVIT(en&zh)(Zhaoetal.,2023)
OpenHermes2.5(en)(Teknium,2023),Alpaca-GPT4(en)(Taorietal.,2023),COIG-CQIA(zh)(Baietal.,2024),
Text-only
ShareGPT(en&zh)(Zhengetal.,2024)
EgoTaskQA(en)(Jiaetal.,2022),Mementos(en)(Wangetal.,2024c),STAR(en)(Wuetal.,2021),
Video NTURGB+D(en)(Shahroudyetal.,2016),VideoChat2IT(en&zh)(Lietal.,2023b),
LSMDC-QA(en)(Rohrbachetal.,2017),ShareGPT-4o(en&zh)(Chenetal.,2024c)
Handwritten SROIE(en)(Huangetal.,2019),FUNSD(en)(GuillaumeJaume,2019),POIE(en)(Kuangetal.,2023)
Thedatasetsusedintheinstructionfine-tuningstagearelistedinTab.7.
A.2 MORETRAININGDETAILS
Table8:Hyper-parametersusedinthepre-trainingandinstructiontuningofMono-InternVL.
Concept Semantic Alignment Instruction
Configuration
Learning(S1.1) Learning(S1.2) Learning(S1.3) Tuning(S2)
Maximumnumerofpatches 1,280 1,792 3,328 6,400
LLMsequencelength 1,425 1,925 4,096 8,192
Usethumbnail ✓
Optimizer AdamW
Optimizerhyperparameters β =0.9,β =0.999,eps=1e−8
1 2
Peaklearningrate 1e−4 1e−4 5e−5 2e−5
Learningrateschedule constantwithwarm-up constantwithwarm-up cosinedecay cosinedecay
Droppathrate 0.1
Weightdecay 0.01
Trainingsteps 450k 126k 70k 14k
Warm-upsteps 100 100 100 420
Globalbatchsize 2,048 2,048 2,048 1,024
Gradientaccumulation 1 1 1 4
Numericalprecision bfloat16
Hyper-parametersusedinthetrainingstagesarelistedinTab.8.
19Mono-InternVL
A.3 MOREABLATIONSTUDIES
InFig.6,weprovidethefullresultsofFig.4withallthedownstreamtasks.
COCO Captions test Flickr test Nocaps val
120 S1.1 S1.2 S1.3 80 S1.1 S1.2 S1.3 110 S1.1 S1.2 S1.3
110 70 100
90
100 60
80
90 50
70
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400
Data Scale (M) Data Scale (M) Data Scale (M)
MME perception score TextVQA val ChartQA Test avg
1300 S1.1 S1.2 S1.3 65 S1.1 S1.2 S1.3 S1.1 S1.2 S1.3
60
60
1200 50
55
40
1100 50
30
45
20
1000
40
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400
Data Scale (M) Data Scale (M) Data Scale (M)
DocVQA val AI2D test InfoVQA val
28
S1.1 S1.2 S1.3 50 S1.1 S1.2 S1.3 S1.1 S1.2 S1.3
60 26
48 24
50
22
46
40 20
30 44 18
42 16
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400
Data Scale (M) Data Scale (M) Data Scale (M)
GQA test SQA-I test OKVQA val
62 S1.1 S1.2 S1.3 70 S1.1 S1.2 S1.3 48 S1.1 S1.2 S1.3
60 46
65
44
58
60 42
56
40
55
54
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200 1400
Data Scale (M) Data Scale (M) Data Scale (M)
Figure 6: Complete results of downstream performance breakdown with the increase of pre-
trainingdatasize.
InTab.9,weexaminetheeffectsoffreezingandunfreezingattentionlayersinalignmentlearning.
Weobservethatunfreezingattentionresultsinconsistentimprovementsacrossallmetrics,suggest-
ingthatitiscrucialtooptimizethemulti-headattentionsinthissub-stageforbettervision-language
alignment.
Table 9: Results of freezing and unfreezing attention in alignment learning. “T-Param” refers
totrainableparameters. Allmodelsarepre-trainedon20millionsofdatainalignmentlearningand
fine-tunedoninstructiondatafromLLaVA-665k(Liuetal.,2023d).
Methods #T-Param MME-P DocVQA InfoVQA SQA-I GQA ChartQA AI2D
Freezeattention 1.8B 1136 39.5 19.7 56.5 59.1 27.2 44.1
Unfreezeattention 3.0B 1153 49.3 22.7 61.8 59.9 49.5 46.4
20Mono-InternVL
A.4 VISUALIZATIONS
ImagecaptioningandOCR
Prompt:Whatisthecontentinthispicture?
Response: The image is a graphical representation, pri-
marilyinformationalinnature. Itfeaturestwoindividuals
depictedinlineartstyle. Thebackgroundoftheimageis
lightcolored.
At the top of the image, there’s text that reads “THE
NOBEL PRIZE IN PHYSICS 2024.” This indicates that
theawardbeingdiscussedinthisimagepertainstophysics
andisscheduledfortheyear2024.
Below this header, there are two names mentioned:
John J. Hopfield and Geoffrey E. Hinton. Further down,
there is a quote attributed to both named individuals:
“for foundational discoveries and inventions that enable
machine learning with artificial neural networks.” This
quote suggests that both John J. Hopfield and Geoffrey
E. Hinton made significant contributions to the field of
artificialintelligenceandneuralnetworks,whichmayhave
involvedpioneeringdiscoveriesandinnovationsthatwere
crucialfordevelopingmachinelearningtechnologies.
Finally, at the bottom, there’s another text stating: “THE
ROYAL SWEDISH ACADEMY OF SCIENCES.” This
indicates that John J. Hopfield and Geoffrey E. Hinton
have been recognized by the Royal Swedish Academy of
Sciences.
Overall, thisimageappearstoserveasatributeorrecog-
nition of the achievements of these scientists in the field
ofphysics,particularlyrelatingtoartificialintelligenceand
neuralnetworks.
Prompt:Describethisimage.
Response: This image depicts a bustling urban scene at
nightinananimationstyle,withalargeelectronicbillboard
prominently displaying the words ”YUNIKA VISION”
in bright white letters against a black background. The
billboardissurroundedbyseveralothertallbuildings, in-
cludingthoseofwell-knowncompaniessuchasHITACHI,
TOSHIBA,SHARP,SONY,PANASONIC,andNEC.
In the foreground, there are multiple train tracks that
intersect near what appears to be an elevated railway or
subwaysystem.Severaltrainsarevisible,indicatingactive
transportationamidstthebusycityscape.
The skyline is dotted with various other commercial and
residentialstructures,contributingtothesenseofathriving
metropolis.Treescanalsobeseenaroundthearea,provid-
ingsomegreeneryamidsttheurbanenvironment. Overall,
theimagecapturesthedynamicenergyofamoderncityat
night,withitsblendoftechnology,commerce,andhuman
activity.
21Mono-InternVL
Visualgrounding
Prompt:Detectthe<ref>theAmericanShorthair</ref>intheimagewith
itsboundingbox.
Response: <ref>theAmericanShorthair</ref><box>[[229,125,660,
932]]</box>
Prompt:Pleasedescribethedetectedobject.
Response: Theobjectintheimageisacat. Thecathasgrayfurwithblack
stripesandiswearingapinksweater.Itappearstobesittingonawindowsill,
lookingupattentivelytowardsthecamera.
Prompt:Detectthe<ref>thetallestbuilding</ref>intheimagewith
itsboundingbox.
Response: <ref>thetallestbuilding</ref><box>[[529,313,580,
591]]</box>
Prompt:Pleaseinferwherethisplaceis.
Response:ThisimagedepictsastunningviewoftheskylineofShang-
hai,China. Theprominentfeaturesintheskylineincludeseveralhigh-
rise buildings with unique structures such as the Oriental Pearl Tower
and the Shanghai World Financial Center. There is also a distinctive
archbridgespanningacrossariver,whichappearstobepartofShang-
hai’sfamousHuangpuRiverwaterfrontareaknownforitshistoricsites
andmoderndevelopment.
VQA
Prompt:Describetheimage.
Response:Theimagefeaturesalargeadultdoglyingonitsside,likely
the mother of several puppies. The adult dog has brown and white
fur with distinct markings on its face. She is surrounded by several
small puppies, all of which are nursing from her body or resting on a
lightbluetowelthatcoverspartofthefloor. Thesettingappearstobe
indoors, possibly in a home environment given the tiled flooring and
the soft, comfortable-looking towel. The atmosphere seems calm and
nurturingasthedogsinteractcloselytogether.
Prompt: Please tell me the exact number of golden retrievers in the
picture.
Response:Thereareatotalofsixgoldenretrieversinthepicture.
22Mono-InternVL
ChineseOCR
Prompt:识别图中文字。
Response:图中文字如下：
“你知道吗。晚上和早上的风开始冷了，树叶也开始
落了下次见面就该穿长袖了对了。好久不见。”
Codegeneration
Prompt:Converttheformulainthefiguretolatexformat.
Response:Certainly!HereistheformulafromthefigureinLaTeXformat:
\begin{equation}
\begin{aligned}
\mathcal{L}_{self-dist}(G_{AB}, p_A) = \mathbb{E}_{\textbf{x}\sim p_A}|
\frac{1}{\sigma_A}(|| L(\textbf{x}) - R(\textbf{x}) ||_1 - \mu_A)\\
- \frac{1}{\sigma_B}(||G_{AB}(L(\textbf{x})) - G_{AB}(R(\textbf{x}))||_1 - \mu_B)|
\end{aligned}
\end{equation}
Documentunderstanding
Prompt:WhatistheageofsubjectB.L.?
Response: B.L.is5yearsoldatthetimeofstudyonthe
effectofmilkproteindilution.
Prompt:Whoseinitialweightisthelargest?Whatisit?
Response:ThelargestinitialweightinthetableisforP.K.
with61.5pounds(lb.).
Prompt: How many children participated in the experi-
mentintotal?
Response:Thereareatotalof14childrenwhoparticipated
intheexperiment.
23Mono-InternVL
Math
Prompt:Thefigureshowsaright-angledtriangle.Whatisthevalueofx?
Response: Tofindthevalueofx,wecanusethePythagoreantheorem,whichstates: a2+b2 = c2
wherecisthehypotenuse(thesideoppositetherightangle),andaandbaretheothertwosides.
Inthistriangle:
Thelengthofthehypotenuse(c)is15.
Oneleg(a)is9.
Theotherleg(b)isx.
UsingthePythagoreantheorem:
92+x2 =152
81+x2 =225
Now,solveforx:
x2 =225−81
x2 =√144
x= 144
x=12
So,thevalueofxis12.
24