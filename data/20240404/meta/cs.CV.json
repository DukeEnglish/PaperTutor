[
    {
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
        "authors": "Keyu TianYi JiangZehuan YuanBingyue PengLiwei Wang",
        "links": "http://arxiv.org/abs/2404.02905v1",
        "entry_id": "http://arxiv.org/abs/2404.02905v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02905v1",
        "summary": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes AR\nmodels surpass diffusion transformers in image generation. On ImageNet 256x256\nbenchmark, VAR significantly improve AR baseline by improving Frechet inception\ndistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,\nwith around 20x faster inference speed. It is also empirically verified that\nVAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.",
        "updated": "2024-04-03 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02905v1"
    },
    {
        "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
        "authors": "Suzanne PetrykDavid M. ChanAnish KachinthayaHaodi ZouJohn CannyJoseph E. GonzalezTrevor Darrell",
        "links": "http://arxiv.org/abs/2404.02904v1",
        "entry_id": "http://arxiv.org/abs/2404.02904v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02904v1",
        "summary": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.",
        "updated": "2024-04-03 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02904v1"
    },
    {
        "title": "LidarDM: Generative LiDAR Simulation in a Generated World",
        "authors": "Vlas ZyrianovHenry CheZhijian LiuShenlong Wang",
        "links": "http://arxiv.org/abs/2404.02903v1",
        "entry_id": "http://arxiv.org/abs/2404.02903v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02903v1",
        "summary": "We present LidarDM, a novel LiDAR generative model capable of producing\nrealistic, layout-aware, physically plausible, and temporally coherent LiDAR\nvideos. LidarDM stands out with two unprecedented capabilities in LiDAR\ngenerative modeling: (i) LiDAR generation guided by driving scenarios, offering\nsignificant potential for autonomous driving simulations, and (ii) 4D LiDAR\npoint cloud generation, enabling the creation of realistic and temporally\ncoherent sequences. At the heart of our model is a novel integrated 4D world\ngeneration framework. Specifically, we employ latent diffusion models to\ngenerate the 3D scene, combine it with dynamic actors to form the underlying 4D\nworld, and subsequently produce realistic sensory observations within this\nvirtual environment. Our experiments indicate that our approach outperforms\ncompeting algorithms in realism, temporal coherency, and layout consistency. We\nadditionally show that LidarDM can be used as a generative world model\nsimulator for training and testing perception models.",
        "updated": "2024-04-03 17:59:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02903v1"
    },
    {
        "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
        "authors": "Harsh RangwaniPradipto MondalMayank MishraAshish Ramayee AsokanR. Venkatesh Babu",
        "links": "http://arxiv.org/abs/2404.02900v1",
        "entry_id": "http://arxiv.org/abs/2404.02900v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02900v1",
        "summary": "Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.",
        "updated": "2024-04-03 17:58:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02900v1"
    },
    {
        "title": "MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment",
        "authors": "Duygu CeylanValentin DeschaintreThibault GroueixRosalie MartinChun-Hao HuangRomain RouffetVladimir KimGaëtan Lassagne",
        "links": "http://arxiv.org/abs/2404.02899v1",
        "entry_id": "http://arxiv.org/abs/2404.02899v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02899v1",
        "summary": "We present MatAtlas, a method for consistent text-guided 3D model texturing.\nFollowing recent progress we leverage a large scale text-to-image generation\nmodel (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully\ndesign an RGB texturing pipeline that leverages a grid pattern diffusion,\ndriven by depth and edges. By proposing a multi-step texture refinement\nprocess, we significantly improve the quality and 3D consistency of the\ntexturing output. To further address the problem of baked-in lighting, we move\nbeyond RGB colors and pursue assigning parametric materials to the assets.\nGiven the high-quality initial RGB texture, we propose a novel material\nretrieval method capitalized on Large Language Models (LLM), enabling\neditabiliy and relightability. We evaluate our method on a wide variety of\ngeometries and show that our method significantly outperform prior arts. We\nalso analyze the role of each component through a detailed ablation study.",
        "updated": "2024-04-03 17:57:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02899v1"
    }
]