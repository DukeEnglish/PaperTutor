[
    {
        "title": "Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief",
        "authors": "Daisuke KikutaHiroki IkeuchiKengo TajiriYuta ToyamaYuusuke Nakano",
        "links": "http://arxiv.org/abs/2404.02448v1",
        "entry_id": "http://arxiv.org/abs/2404.02448v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02448v1",
        "summary": "As a telecom provider, our company has a critical mission to maintain telecom\nservices even during power outages. To accomplish the mission, it is essential\nto maintain the power of the telecom base stations. Here we consider a solution\nwhere electric vehicles (EVs) directly supply power to base stations by\ntraveling to their locations. The goal is to find EV routes that minimize both\nthe total travel distance of all EVs and the number of downed base stations. In\nthis paper, we formulate this routing problem as a new variant of the Electric\nVehicle Routing Problem (EVRP) and propose a solver that combines a rule-based\nvehicle selector and a reinforcement learning (RL)-based node selector. The\nrule of the vehicle selector ensures the exact environmental states when the\nselected EV starts to move. In addition, the node selection by the RL model\nenables fast route generation, which is critical in emergencies. We evaluate\nour solver on both synthetic datasets and real datasets. The results show that\nour solver outperforms baselines in terms of the objective value and\ncomputation time. Moreover, we analyze the generalization and scalability of\nour solver, demonstrating the capability toward unseen settings and large-scale\nproblems. Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps.",
        "updated": "2024-04-03 04:27:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02448v1"
    },
    {
        "title": "EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management",
        "authors": "Tiago FonsecaLuis FerreiraBernardo CabralRicardo SeverinoIsabel Praca",
        "links": "http://arxiv.org/abs/2404.02361v1",
        "entry_id": "http://arxiv.org/abs/2404.02361v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02361v1",
        "summary": "This paper investigates the increasing roles of Renewable Energy Sources\n(RES) and Electric Vehicles (EVs). While indicating a new era of sustainable\nenergy, these also introduce complex challenges, including the need to balance\nsupply and demand and smooth peak consumptions amidst rising EV adoption rates.\nAddressing these challenges requires innovative solutions such as Demand\nResponse (DR), energy flexibility management, Renewable Energy Communities\n(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing\nV2G approaches often fall short in real-world adaptability, global REC\noptimization with other flexible assets, scalability, and user engagement. To\nbridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement\nLearning (MARL) energy management framework, leveraging the Multi-Agent Deep\nDeterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables\nuser-centric and multi-objective energy management by allowing each prosumer to\nselect from a range of personal management objectives, thus encouraging\nengagement. Additionally, it architects' data protection and ownership through\ndecentralized computing, where each prosumer can situate an energy management\noptimization node directly at their own dwelling. The local node not only\nmanages local energy assets but also fosters REC wide optimization. The\nefficacy of EnergAIze was evaluated through case studies employing the\nCityLearn simulation framework. These simulations were instrumental in\ndemonstrating EnergAIze's adeptness at implementing V2G technology within a REC\nand other energy assets. The results show reduction in peak loads, ramping,\ncarbon emissions, and electricity costs at the REC level while optimizing for\nindividual prosumers objectives.",
        "updated": "2024-04-02 23:16:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02361v1"
    },
    {
        "title": "Federated Multi-Agent Mapping for Planetary Exploration",
        "authors": "Tiberiu-Ioan SzatmariAbhishek Cauligi",
        "links": "http://arxiv.org/abs/2404.02289v1",
        "entry_id": "http://arxiv.org/abs/2404.02289v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02289v1",
        "summary": "In multi-agent robotic exploration, managing and effectively utilizing the\nvast, heterogeneous data generated from dynamic environments poses a\nsignificant challenge. Federated learning (FL) is a promising approach for\ndistributed mapping, addressing the challenges of decentralized data in\ncollaborative learning. FL enables joint model training across multiple agents\nwithout requiring the centralization or sharing of raw data, overcoming\nbandwidth and storage constraints. Our approach leverages implicit neural\nmapping, representing maps as continuous functions learned by neural networks,\nfor compact and adaptable representations. We further enhance this approach\nwith meta-initialization on Earth datasets, pre-training the network to quickly\nlearn new map structures. This combination demonstrates strong generalization\nto diverse domains like Martian terrain and glaciers. We rigorously evaluate\nthis approach, demonstrating its effectiveness for real-world deployment in\nmulti-agent exploration scenarios.",
        "updated": "2024-04-02 20:32:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02289v1"
    },
    {
        "title": "Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning",
        "authors": "Samuel ToveyChristoph LohrmannChristian Holm",
        "links": "http://arxiv.org/abs/2404.01999v1",
        "entry_id": "http://arxiv.org/abs/2404.01999v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01999v1",
        "summary": "Reinforcement learning (RL) is a flexible and efficient method for\nprogramming micro-robots in complex environments. Here we investigate whether\nreinforcement learning can provide insights into biological systems when\ntrained to perform chemotaxis. Namely, whether we can learn about how\nintelligent agents process given information in order to swim towards a target.\nWe run simulations covering a range of agent shapes, sizes, and swim speeds to\ndetermine if the physical constraints on biological swimmers, namely Brownian\nmotion, lead to regions where reinforcement learners' training fails. We find\nthat the RL agents can perform chemotaxis as soon as it is physically possible\nand, in some cases, even before the active swimming overpowers the stochastic\nenvironment. We study the efficiency of the emergent policy and identify\nconvergence in agent size and swim speeds. Finally, we study the strategy\nadopted by the reinforcement learning algorithm to explain how the agents\nperform their tasks. To this end, we identify three emerging dominant\nstrategies and several rare approaches taken. These strategies, whilst\nproducing almost identical trajectories in simulation, are distinct and give\ninsight into the possible mechanisms behind which biological agents explore\ntheir environment and respond to changing conditions.",
        "updated": "2024-04-02 14:42:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01999v1"
    },
    {
        "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
        "authors": "Yoichi IshibashiYoshimasa Nishimura",
        "links": "http://arxiv.org/abs/2404.02183v1",
        "entry_id": "http://arxiv.org/abs/2404.02183v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02183v1",
        "summary": "Recent advancements in automatic code generation using large language model\n(LLM) agent have brought us closer to the future of automated software\ndevelopment. However, existing single-agent approaches face limitations in\ngenerating and improving large-scale, complex codebases due to constraints in\ncontext length. To tackle this challenge, we propose Self-Organized multi-Agent\nframework (SoA), a novel multi-agent framework that enables the scalable and\nefficient generation and optimization of large-scale code. In SoA,\nself-organized agents operate independently to generate and modify code\ncomponents while seamlessly collaborating to construct the overall codebase. A\nkey feature of our framework is the automatic multiplication of agents based on\nproblem complexity, allowing for dynamic scalability. This enables the overall\ncode volume to be increased indefinitely according to the number of agents,\nwhile the amount of code managed by each agent remains constant. We evaluate\nSoA on the HumanEval benchmark and demonstrate that, compared to a single-agent\nsystem, each agent in SoA handles significantly less code, yet the overall\ngenerated code is substantially greater. Moreover, SoA surpasses the powerful\nsingle-agent baseline by 5% in terms of Pass@1 accuracy.",
        "updated": "2024-04-02 13:37:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02183v1"
    }
]