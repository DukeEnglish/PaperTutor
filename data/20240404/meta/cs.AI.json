[
    {
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
        "authors": "Keyu TianYi JiangZehuan YuanBingyue PengLiwei Wang",
        "links": "http://arxiv.org/abs/2404.02905v1",
        "entry_id": "http://arxiv.org/abs/2404.02905v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02905v1",
        "summary": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes AR\nmodels surpass diffusion transformers in image generation. On ImageNet 256x256\nbenchmark, VAR significantly improve AR baseline by improving Frechet inception\ndistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,\nwith around 20x faster inference speed. It is also empirically verified that\nVAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.",
        "updated": "2024-04-03 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02905v1"
    },
    {
        "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
        "authors": "Suzanne PetrykDavid M. ChanAnish KachinthayaHaodi ZouJohn CannyJoseph E. GonzalezTrevor Darrell",
        "links": "http://arxiv.org/abs/2404.02904v1",
        "entry_id": "http://arxiv.org/abs/2404.02904v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02904v1",
        "summary": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.",
        "updated": "2024-04-03 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02904v1"
    },
    {
        "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
        "authors": "Harsh RangwaniPradipto MondalMayank MishraAshish Ramayee AsokanR. Venkatesh Babu",
        "links": "http://arxiv.org/abs/2404.02900v1",
        "entry_id": "http://arxiv.org/abs/2404.02900v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02900v1",
        "summary": "Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.",
        "updated": "2024-04-03 17:58:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02900v1"
    },
    {
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "authors": "Hao LiYang ZouYing WangOrchid MajumderYusheng XieR. ManmathaAshwin SwaminathanZhuowen TuStefano ErmonStefano Soatto",
        "links": "http://arxiv.org/abs/2404.02883v1",
        "entry_id": "http://arxiv.org/abs/2404.02883v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02883v1",
        "summary": "Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.",
        "updated": "2024-04-03 17:34:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02883v1"
    },
    {
        "title": "FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery",
        "authors": "Safouane El GhazoualiArnaud GucciardiNicola VenturiMichael RueegseggerUmberto Michelucci",
        "links": "http://arxiv.org/abs/2404.02877v1",
        "entry_id": "http://arxiv.org/abs/2404.02877v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02877v1",
        "summary": "Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.",
        "updated": "2024-04-03 17:24:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02877v1"
    }
]