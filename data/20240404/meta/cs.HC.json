[
    {
        "title": "Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?",
        "authors": "Jian ZhengGe Gao",
        "links": "http://dx.doi.org/10.1145/3613904.3642608",
        "entry_id": "http://arxiv.org/abs/2404.02880v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02880v1",
        "summary": "Everyone spends some time waiting every day. HCI research has developed tools\nfor boosting productivity while waiting. However, little is known about how\npeople naturally spend their waiting time. We conducted an experience sampling\nstudy with 21 working adults who used a mobile app to report their daily\nwaiting time activities over two weeks. The aim of this study is to understand\nthe activities people do while waiting and the effect of situational factors.\nWe found that participants spent about 60% of their waiting time on leisure\nactivities, 20% on productive activities, and 20% on maintenance activities.\nThese choices are sensitive to situational factors, including accessible\ndevice, location, and certain routines of the day. Our study complements\nprevious ones by demonstrating that people purpose waiting time for various\ngoals beyond productivity and to maintain work-life balance. Our findings shed\nlight on future empirical research and system design for time management.",
        "updated": "2024-04-03 17:32:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02880v1"
    },
    {
        "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
        "authors": "Hussein MozannarValerie ChenMohammed AlsobaySubhro DasSebastian ZhaoDennis WeiManish NagireddyPrasanna SattigeriAmeet TalwalkarDavid Sontag",
        "links": "http://arxiv.org/abs/2404.02806v1",
        "entry_id": "http://arxiv.org/abs/2404.02806v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02806v1",
        "summary": "Evaluation of large language models (LLMs) for code has primarily relied on\nstatic benchmarks, including HumanEval (Chen et al., 2021), which measure the\nability of LLMs to generate complete code that passes unit tests. As LLMs are\nincreasingly used as programmer assistants, we study whether gains on existing\nbenchmarks translate to gains in programmer productivity when coding with LLMs,\nincluding time spent coding. In addition to static benchmarks, we investigate\nthe utility of preference metrics that might be used as proxies to measure LLM\nhelpfulness, such as code acceptance or copy rates. To do so, we introduce\nRealHumanEval, a web interface to measure the ability of LLMs to assist\nprogrammers, through either autocomplete or chat support. We conducted a user\nstudy (N=213) using RealHumanEval in which users interacted with six LLMs of\nvarying base model performance. Despite static benchmarks not incorporating\nhumans-in-the-loop, we find that improvements in benchmark performance lead to\nincreased programmer productivity; however gaps in benchmark versus human\nperformance are not proportional -- a trend that holds across both forms of LLM\nsupport. In contrast, we find that programmer preferences do not correlate with\ntheir actual performance, motivating the need for better, human-centric proxy\nsignals. We also open-source RealHumanEval to enable human-centric evaluation\nof new models and the study data to facilitate efforts to improve code models.",
        "updated": "2024-04-03 15:20:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02806v1"
    },
    {
        "title": "AI and personalized learning: bridging the gap with modern educational goals",
        "authors": "Kristjan-Julius LaakJaan Aru",
        "links": "http://arxiv.org/abs/2404.02798v1",
        "entry_id": "http://arxiv.org/abs/2404.02798v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02798v1",
        "summary": "Personalized learning (PL) aspires to provide an alternative to the\none-size-fits-all approach in education. Technology-based PL solutions have\nshown notable effectiveness in enhancing learning performance. However, their\nalignment with the broader goals of modern education is inconsistent across\ntechnologies and research areas. In this paper, we examine the characteristics\nof AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our\nanalysis indicates a gap between the objectives of modern education and the\ncurrent direction of PL. We identify areas where most present-day PL\ntechnologies could better embrace essential elements of contemporary education,\nsuch as collaboration, cognitive engagement, and the development of general\ncompetencies. While the present PL solutions are instrumental in aiding\nlearning processes, the PL envisioned by educational experts extends beyond\nsimple technological tools and requires a holistic change in the educational\nsystem. Finally, we explore the potential of large language models, such as\nChatGPT, and propose a hybrid model that blends artificial intelligence with a\ncollaborative, teacher-facilitated approach to personalized learning.",
        "updated": "2024-04-03 15:07:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02798v1"
    },
    {
        "title": "IEEE VIS Workshop on Visualization for Climate Action and Sustainability",
        "authors": "Benjamin BachFanny ChevalierHelen-Nicole KostisMark SubbaroYvonne JansenRobert Soden",
        "links": "http://arxiv.org/abs/2404.02743v1",
        "entry_id": "http://arxiv.org/abs/2404.02743v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02743v1",
        "summary": "This first workshop on visualization for climate action and sustainability\naims to explore and consolidate the role of data visualization in accelerating\naction towards addressing the current environmental crisis. Given the urgency\nand impact of the environmental crisis, we ask how our skills, research\nmethods, and innovations can help by empowering people and organizations. We\nbelieve visualization holds an enormous power to aid understanding, decision\nmaking, communication, discussion, participation, education, and exploration of\ncomplex topics around climate action and sustainability. Hence, this workshop\ninvites submissions and discussion around these topics with the goal of\nestablishing a visible and actionable link between these fields and their\nrespective stakeholders. The workshop solicits work-in-progress and research\npapers as well as pictorials and interactive demos from the whole range of\nvisualization research (dashboards, interactive spaces, scientific\nvisualization, storytelling, visual analytics, explainability etc.), within the\ncontext of environmentalism (climate science, sustainability, energy, circular\neconomy, biodiversity, etc.) and across a range of scenarios from public\nawareness and understanding, visual analysis, expert decision making, science\ncommunication, personal decision making etc. After presentations of\nsubmissions, the workshop will feature dedicated discussion groups around data\ndriven interactive experiences for the public, and tools for personal and\nprofessional decision making.",
        "updated": "2024-04-03 13:39:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02743v1"
    },
    {
        "title": "Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities",
        "authors": "Jiale LiJiayang LiJiahao ChenYifan LiShijie WangHugo ZhouMinjun YeYunsheng Su",
        "links": "http://arxiv.org/abs/2404.02718v1",
        "entry_id": "http://arxiv.org/abs/2404.02718v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02718v1",
        "summary": "Human-like Agents with diverse and dynamic personality could serve as an\nimportant design probe in the process of user-centered design, thereby enabling\ndesigners to enhance the user experience of interactive application.In this\narticle, we introduce Evolving Agents, a novel agent architecture that consists\nof two systems: Personality and Behavior. The Personality system includes three\nmodules: Cognition, Emotion and Character Growth. The Behavior system comprises\ntwo modules: Planning and Action. We also build a simulation platform that\nenables agents to interact with the environment and other agents. Evolving\nAgents can simulate the human personality evolution process. Compared to its\ninitial state, agents' personality and behavior patterns undergo believable\ndevelopment after several days of simulation. Agents reflect on their behavior\nto reason and develop new personality traits. These traits, in turn, generate\nnew behavior patterns, forming a feedback loop-like personality evolution.In\nour experiment, we utilized simulation platform with 10 agents for evaluation.\nDuring the evaluation, these agents experienced believable and inspirational\npersonality evolution. Through ablation and control experiments, we\ndemonstrated the outstanding effectiveness of agent personality evolution and\nall modules of our agent architecture contribute to creating believable\nhuman-like agents with diverse and dynamic personalities. We also demonstrated\nthrough workshops how Evolving Agents could inspire designers.",
        "updated": "2024-04-03 13:20:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02718v1"
    }
]