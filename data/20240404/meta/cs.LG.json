[
    {
        "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
        "authors": "Suzanne PetrykDavid M. ChanAnish KachinthayaHaodi ZouJohn CannyJoseph E. GonzalezTrevor Darrell",
        "links": "http://arxiv.org/abs/2404.02904v1",
        "entry_id": "http://arxiv.org/abs/2404.02904v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02904v1",
        "summary": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.",
        "updated": "2024-04-03 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02904v1"
    },
    {
        "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
        "authors": "Harsh RangwaniPradipto MondalMayank MishraAshish Ramayee AsokanR. Venkatesh Babu",
        "links": "http://arxiv.org/abs/2404.02900v1",
        "entry_id": "http://arxiv.org/abs/2404.02900v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02900v1",
        "summary": "Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.",
        "updated": "2024-04-03 17:58:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02900v1"
    },
    {
        "title": "Comment on \"Machine learning conservation laws from differential equations\"",
        "authors": "Michael F. Zimmer",
        "links": "http://arxiv.org/abs/2404.02896v1",
        "entry_id": "http://arxiv.org/abs/2404.02896v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02896v1",
        "summary": "In lieu of abstract, first paragraph reads: Six months after the author\nderived a constant of motion for a 1D damped harmonic oscillator [1], a similar\nresult appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the\nauthor. However, their derivation contained six serious errors, causing both\ntheir method and result to be incorrect. In this Comment, those errors are\nreviewed.",
        "updated": "2024-04-03 17:53:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02896v1"
    },
    {
        "title": "MODNO: Multi Operator Learning With Distributed Neural Operators",
        "authors": "Zecheng Zhang",
        "links": "http://arxiv.org/abs/2404.02892v1",
        "entry_id": "http://arxiv.org/abs/2404.02892v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02892v1",
        "summary": "The study of operator learning involves the utilization of neural networks to\napproximate operators. Traditionally, the focus has been on single-operator\nlearning (SOL). However, recent advances have rapidly expanded this to include\nthe approximation of multiple operators using foundation models equipped with\nmillions or billions of trainable parameters, leading to the research of\nmulti-operator learning (MOL). In this paper, we present a novel distributed\ntraining approach aimed at enabling a single neural operator with significantly\nfewer parameters to effectively tackle multi-operator learning challenges, all\nwithout incurring additional average costs. Our method is applicable to various\nChen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).\nThe core idea is to independently learn the output basis functions for each\noperator using its dedicated data, while simultaneously centralizing the\nlearning of the input function encoding shared by all operators using the\nentire dataset. Through a systematic study of five numerical examples, we\ncompare the accuracy and cost of training a single neural operator for each\noperator independently versus training a MOL model using our proposed method.\nOur results demonstrate enhanced efficiency and satisfactory accuracy.\nMoreover, our approach illustrates that some operators with limited data can be\nmore effectively constructed with the aid of data from analogous operators\nthrough MOL learning. This highlights another MOL's potential to bolster\noperator learning.",
        "updated": "2024-04-03 17:49:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02892v1"
    },
    {
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "authors": "Hao LiYang ZouYing WangOrchid MajumderYusheng XieR. ManmathaAshwin SwaminathanZhuowen TuStefano ErmonStefano Soatto",
        "links": "http://arxiv.org/abs/2404.02883v1",
        "entry_id": "http://arxiv.org/abs/2404.02883v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02883v1",
        "summary": "Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.",
        "updated": "2024-04-03 17:34:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02883v1"
    }
]