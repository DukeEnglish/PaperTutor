MODNO: Multi-Operator Learning With Distributed Neural
Operators
Zecheng Zhang∗
April 4, 2024
Abstract
Thestudyofoperatorlearninginvolvestheutilizationofneuralnetworkstoapproximate
operators. Traditionally, the focus has been on single-operator learning (SOL). However, re-
centadvanceshaverapidlyexpandedthistoincludetheapproximationofmultipleoperators
usingfoundationmodelsequippedwithmillionsorbillionsoftrainableparameters,leadingto
the research of multi-operator learning (MOL). In this paper, we present a novel distributed
trainingapproachaimedatenablingasingleneuraloperatorwithsignificantlyfewerparam-
eterstoeffectivelytacklemulti-operatorlearningchallenges,allwithoutincurringadditional
averagecosts. OurmethodisapplicabletovariousChen-Chen-typeneuraloperators,suchas
Deep Operator Neural Networks (DON). The core idea is to independently learn the output
basis functions for each operator using its dedicated data, while simultaneously centralizing
the learning of the input function encoding shared by all operators using the entire dataset.
Through a systematic study of five numerical examples, we compare the accuracy and cost
of training a single neural operator for each operator independently versus training a MOL
model using our proposed method. Our results demonstrate enhanced efficiency and satis-
factory accuracy. Moreover, our approach illustrates that some operators with limited data
can be more effectively constructed with the aid of data from analogous operators through
MOL learning. This highlights another MOL’s potential to bolster operator learning.
1 Introduction
Operator learning [5, 4, 35, 27, 70], or more specifically, single-operator learning (SOL), is a new
areainscientificmachinelearning[51,53,52,54,23,9,66]thatlearnstheoperatormappingfrom
one function to another function. The early study in the area is the universal approximation
theorem for functionals [4] and the operator proposed in [5].
Operator learning has many applications in solving classic challenging mathematical and scien-
tific machine learning (MSML) problems. One of the most addressed problems is to solve the
parametric PDE problem [28, 1, 21, 17, 6, 33, 11]. For example, researchers want to learn the
mapping from the parametrized initial condition to its corresponding solutions [59, 29, 36, 27].
Researchers also successfully applied the neural operators to solve large-scale real-life applica-
tions such as weather [49], geology [71, 15, 24], physics [40, 41, 8], material science [22, 38],
∗Department of Mathematics, Florida State University, Tallahassee, FL 32304, USA. (Email:
zecheng.zhang.math@gmail.com)
1
4202
rpA
3
]GL.sc[
1v29820.4042:viXrapower systems and control [31, 45, 2] etc. Besides the application, the study of operators
has extended to many other areas. For example, the approximation with the neural operator
[7, 18, 67, 19, 6, 1, 33, 20]; the uncertainty quantification and optimization [31, 46, 39, 10, 32, 3],
etc.
Over recent years, numerous neural operator models have emerged, such as the Deep Operator
Neural Network (DON) and its various iterations [35, 36, 16, 70, 67, 72, 13, 30], as well as the
Fourier Neural Operator and its derivatives [27, 28, 60, 25, 49, 31, 69, 29, 26, 24]. Please refer
to [50, 48, 47, 14, 57] for some other notable structures. DNO stands out for not requiring a
discretization for the output functions [35, 70], thereby granting researchers greater adaptability
in tackling scientific computing challenges. For instance, a trained DON could serve as a mesh-
freenumericalsolverforPDEs[68,37,13]. ThevanillaDON[35]lackeddiscretizationinvariance,
necessitating uniform discretizations across all input functions. However, recent advancements
[70, 67] in universal approximation [5] and distributed training algorithms [69] have rendered
DON discretization-invariant.
Multi-operator learning (MOL) [34, 62, 42] represents a broader extension of SOL, allowing the
simultaneous learning of multiple operators. MOL has received tremendous attention owing
to its potential to generalize and extrapolate to new operators without further training [72].
Notably, PROSE[34]hasdemonstratedtheabilitytoforecastoperatorsresemblingthosewithin
the training set and extend predictions to operators whose output functions’ physical properties
are not encountered during training.
Because of the encoding complexity of operators, the majority of MOL models possess millions
or even billions of trainable parameters and necessitate high-performance computational devices
fortraining. Inthisstudy,weintroduceadistributedtrainingtechniquetermed“Multi-Operator
DistributedNeuralOperator(MODNO)”,aimedatempoweringstandardsingleneuraloperators
withsignificantlyfewerparameters[35,16,70,67]toaddressmulti-operatorlearningchallenges.
Distributed learning [43, 58, 65, 44], also referred to as distributed training, is a paradigm in
machine learning where the training process is distributed across multiple computing nodes
or devices. Unlike traditional centralized training where all computations are performed on a
single machine, distributed learning leverages the computational power of multiple devices to
accelerate the training process and handle large-scale datasets efficiently.
Researchers have adopted the distributed training idea in solving scientific computing problems,
particularly the operator learning problem. For example, in [69], the authors propose to em-
ploy different neural networks to handle heterogeneous input functions with distinct properties,
while using a centralized neural network to learn the common shared basis for output func-
tions corresponding to all input functions. This method enables the DON to be discretization
invariant.
In this study, we propose to employ a distributed approach to learn multiple operators within a
singleDONframework. Theconceptinvolvestrainingspecializedoutputbasisfunctionsforeach
operator using dedicated data associated with that operator, while simultaneously employing
a shared neural network to encode the input functions. One crucial analysis often overlooked
in MOL studies is the comparison with training separate SOL neural operators using distinct
datasets. We demonstrate that MODNO may outperform this approach in many cases (11 out
of 16) even when provided with less data. We summarize the contributions as follows.
2• We propose a distributed training approach named “Multi-operator learning with dis-
tributed neural operators (MODNO)” to learn multiple operators utilizing a single neural
operator (SNO). This training method is applicable to all Chen-Chen-type neural opera-
tors and has significantly less cost when compared to many other multi-operator learning
(MOL) foundation models.
• We compute the training cost, such as the total number of back-propagations and forward
passes in gradient flow and trainable parameters, for constructing one operator within the
MODNO framework. Our analysis reveals that, despite requiring less cost compared to
training an SNO independently, the framework achieves similar or even superior accuracy.
• Through the numerical experiments, we illustrate that MOL may boost the accuracy
for specific operators compared to training these operators separately using individual
SNOs. ThisfindingindicatesanewadvantageofMOL,suggestingitspotentialtoenhance
predictions for certain operators by leveraging data from other operators.
The rest of the paper is organized as follows. In Section 2, we will review the DON structure
and introduce the motivation of the algorithms based on the approximation theorem [5]. Later
in Section 3, we introduce the algorithm and discuss the computation costs of the algorithms.
We will then present the numerical results in Section 4. Finally, we will discuss limitations and
future works in Section 5.
2 Overview
2.1 (Single) Operator Learning (SOL)
The goal of single-operator learning (SOL) is to approximate the operator G : U → V. Here U
and V are function spaces with domain D and D′ separately. In [5, 35], the authors Chen and
Chen proposed the following approximation to the operator:
K
(cid:88)
G(u)(x) ≈ a (u)b (x), (1)
k k
k=1
whereu ∈ U andx ∈ D′,anda (·)andb (·)arefunctionalsonU andfunctionsonD′respectively
k k
withsomeregularityassumptions(pleaserefertoTheorem5in[5],Theorem3.8in[67],Theorem
2 of [35], and Theorem 2.10 in [16]). By employing neural networks to approximate a (·) and
k
b (·), various network architectures have been developed by researchers [35, 36, 70, 67, 16, 59].
k
Amongthese,thefundamentalworkistheDeepOperatorNeuralNetwork(DON)[35,36,16,72]
and its variants. Since all mentioned structures share the approximation structure, we will
call them Chen-Chen-type neural operators, and denote the network as G , with θ being the
θ
parameters, specifically,
K
(cid:88)
G(u)(x) ≈ G (uˆ)(x) = a (uˆ;α)b (x;β).
θ k k
k=1
3Here uˆ = [u(y ),...,u(y )]⊺ , and {y }Ns ⊂ D are sensors. a (·;α) and b (·;β) are neural net-
1 Ns i i=1 k k
work parametrized functions with parameter α and β that approximate a and b in Equation
k k
(1). In this study, we will specifically employ the DON structure [35], which has been demon-
strated to be highly successful across a range of applications and theoretical frameworks. In the
DON literature, a (·;α) is called the branch net and b (·;β) is called the trunk net. Specifically,
k k
let us denote x ∈ Rd ⊂ D′, g be the Tauber-Wiener function [5]. It follows that,
b (x) = g(ω ·x+ζ ), (2)
k k k
 
(cid:88)M (cid:88)Ns
a k(uˆ) = ck
l
g εk lju(y j)+θ lk , (3)
l=1 j=1
here ck,ζ ,θk,εk ∈ R, ω ∈ Rd are parameters. We present one picture demonstration of DON
l k l lj k
in Figure 1, and will design a novel distributed learning algorithm to extend the DON to be
a multi-operator learning framework with significantly less trainable parameters. Notably, this
method can be generalized to all Chen-Chen-type neural operators such as the Basis Enhanced
Learning (Bel) [70, 67] the discretization invariant extension of DON.
branchnet1 b 1
uˆ=[u(y 1),...,u(y Ns)]⊺ branchnetj b j
branchnetK b K
(cid:78)→G(u)(x)
t
1
Outputs
x∈Rd trunknet t j
t
K
Inputs
(cid:78)
Figure 1: Stacked version DON [35], a fundamental work in the area. denotes the inner
product in RK.
2.2 Multi-Operator Learning (MOL)
The multi-operator learning [34, 61, 62, 63, 64, 55, 42] is a further generalization of SOL. MOL
constructs a single neural network and learns multiple operators simultaneously. Specifically,
suppose we have N operators and i−th operator is defined as G : U → V , where U and V
op i i i i i
are input and output function spaces of G . The goal of MOL is to design a neural network N
i
that takes G and u ∈ U as inputs and constructs the output function G (u ) ∈ v . Due to
i ij i i ij i
the encoding of the operator G , the current MOLs models are usually built with millions even
i
billions of parameters, and trained with huge numbers of samples. For example, the ICON [61]
has around 30 millions parameters, and PROSE-PDE [34] has around 105 millions parameters.
The development of large MOL models has been contributing numerous innovative concepts
and techniques to the MSML community. For instance, the adoption of symbolic encoding of
operators proposed in [34, 56], the advocacy for using graphs to encode operators as highlighted
4in [64], and fine-tuning of the foundation large language models (LLMs) [62, 61]. More impor-
tantly, MOL has received a lot of attention as it has the potential to extrapolate to unseen
operators. Some MOL models for example PROSE [34] have shown that PROSE can predict
operators unseen but similar to the training dataset operators.
However, mostMOLsstillstruggletohandlechallengingextrapolationtasks. Consequently, itis
advocated and important to compare the training costs (for example the number of parameters)
and prediction accuracy of employing a single large foundation MOL for multiple operators
versustrainingindividualSOLsforeachoperator. Inthisstudy,wewilldeviseatrainingmethod
toempowerasingleChen-Chen-typeneuraloperatortoaddressthechallengesofmulti-operator
learningandsubsequentlyevaluateitsperformance. Notably, comparedtothewell-knownMOL
foundationmodels,theproposedframeworkhassignificantlyfewertrainableparametersandmay
even outperform individually trained DON for each single operator.
3 Methodology
In this section, we will outline the proposed approach and present the metric to evaluate the
cost of the algorithms.
3.1 Theoretical Motivation
As discussed in Section 2, Equation (1) demonstrates that a single operator can be effectively
approximated. This involves crafting distinct networks tailored to approximate a (·) and b (·),
k k
thereby creating different neural operators.
To establish the universal approximation theory of [5, 67, 16], the final step is to approximate
the function G(u) ∈ V using neural networks, as G(u) =
(cid:80)K
c ◦G(u)b (x), where u ∈ U,
k=1 k k
and c (·) is learnable functional on V. Notably, b (·) or trunk nets serve as the learnable basis
k k
for constructing the output function in V, and is independent of the input functions u ∈ U, i,e.,
all G(u) share the basis.
For the functional c ◦ G(u), by assuming a finite-dimensional structure uˆ ∈ RNs of u, the
k
functional c ◦G(u) can be approximated by the branch nets as in Equation (3), or,
k
 
(cid:88)M (cid:88)Ns (cid:88)M
a k(uˆ) = ck
l
g εk lju(y j)+θ lk  := ck lek l(u),
l=1 j=1 l=1
where ek(u) can be regarded as the basis for an encoding of the input function space U.
l
Motivated by the above approximation theory, with multiple operators G , one should design
i
dedicated unshared trunk networks to approximate the basis b (·) for each operator’s output
k
functions, utilizing the associated data for each operator. Assuming all input functions share
the input function space, one can use centralized networks to learn a shared centralized input
function basis ek(u) that is applicable across all distinct operators.
l
In the implementation phase, an alternative strategy involves federately learning all branch
networks a (·;α) rather than concentrating on the input function space basis ek(·), potentially
k l
5leading to additional computational savings. Further elaboration on the cost savings will be
provided in Section 3.2.
3.2 Multi-Operator Learning Distributed Neural Operator (MODNO)
The approximation to each individual operator G : U → V is the same as Equation 2, but
i i i
the trunk basis and part of branch nets will be dedicated to each operator. To introduce the
algorithm let us first introduce the local loss functions associated with operator G .
i
The local loss is used to train unshared networks to approximate the k−th basis b (·) for i−th
k,i
operator, and let us denote the network approximation to k−th basis b (·) as b (·,β ), where
k,i k,i k,i
β denotes the trainable parameters, and β = ∪ β . Additionally, we use a (·;α ) to denote
k,i i k k,i k k
the centralized shared network used to approximate a (·), and trainable parameters denote as
k
α = ∪ α . Itisworthnotingthatinthiscontext,wemakeallparametersinthebranchnetworks
k k
sharedbyall, contrastingthemotivationoutlinedinSection3.1, yetprovingtobemoreefficient.
More details are presented in Section 3.3.
Additionally, for demonstration and without loss of generality, we use only one point x ∈ D′
i i
for the i−th operator, where D′ is the domain for functions in V . We then can define the local
i i
loss with suitable normalizing as follows,
(cid:88)Nu (cid:88)K
L (β ;α) = ∥G (uˆ )(x )− a (uˆ ;α )b (x ;β )∥2,
i i i i,p i k i,p k k,i i k,i
p=1 k=1
hereN isthetotalnumberofinputfunctionsanduˆ denotesdiscretizationofthecorresponding
u p
functionsu ∈ U . Thislocallossisusedtoupdatethededicatedparametersβ giventheshared
i,p i i
parameters α, and the updating is based on the data associated with i−th operator only.
Now, let us define the global loss function L, we use the global loss to train the centralized
shared network a (·;α ),
k k
Nop
(cid:88)
L(α;β) = L (β ;α), (4)
i i
i=1
here N is the number of operators. The global loss is used to update the globally shared
op
parameters given all dedicated parameters β , with the update process relying on all data asso-
i
ciated with all operators. Let us now summarize the pseudo-algorithm with standard gradient
descent optimization in Algorithm 1. Extending it to other gradient descent-based algorithms
is straightforward.
6Algorithm 1: Multi-Operator Learning Distributed Neural Operator (MODNO)
1 Initialization: parameters β i for i−th operator’s dedicated network (e.g., trunk nets), α for
the shared network (e.g., branch nets), the optimization epochs N , learning rate η and η;
I i
2 for epoch = 1 to N I do
3 Update the unshared parameters β i for operator G i using its own dataset as follow;
4 for i = 1 to N op do
5
β = β −η ∇ L (β ;α).
i i i βi i i
6 Update the shared networks parameters α with all data as follow;
7
α = α−η∇ L(α;β).
α
3.3 Computational Cost
When calculating the costs of training two distinct neural networks, the conventional practice
involves quantifying the total number of trainable parameters and training samples for each net-
work. Nonetheless, within the context of the proposed distributed algorithm, a straightforward
calculation of total parameters across all shared and dedicated networks proves inadequate,
mainly because the dedicated networks undergo training with a reduced number of training
samples (their dedicated dataset).
To ensure a fair assessment of the distributed algorithm’s efficacy, we instead prioritize the
number of back-propagations and forward passes [12] as the metric for the cost and comparison
with a single DON.
To evaluate the total number of back-propagations and forward passes for MODNO, we denote
the following variables for the datasets and training samples: N for the number of operators,
op
N for the number of functions for i−th operator, and N for the number of known training
u,i p,i,j
function values of the j-th output function of the i-th operator.
Topresentamoregeneralconcept,withaslightabuseofnotationwithoutintroducingambiguity,
b (·;β ) denotes the dedicated networks, but are not necessarily limited to the trunk net in the
i i
previoussections,andweuseN todenotethenumberofback-propagationsandforwardpasses
b,i
for one training sample for b (·;β ). Similarly, N denotes the number of back-propagations and
i i a
forward passes for one sample for all shared nets a(·;α).
We will compare the total number of back-propagations and forward passes for training N
op
individual single-operator learning DONs and one for each operator. We then denote N′ for
u,i
the number of functions for i−th SOL DON, and N′ for the number of function values of
p,i,j
the j-th output function of the i-th SOL DON. Besides, the back-propagation and forward pass
counts for i−th individual DON’s branch nets and trunk nets are N′ and N′ .
b,i a,i
The cost for MODNO is then C =
(cid:80)NopN
N (N +N ), and total cost of training
MOL i=1 u,i u,i,j b,i a
N individual DONs for each operator is C = (cid:80)NopN′ N′ (N′ + N′ ). Assuming
op SOL i=1 u,i u,i,j b,i a,i
MODNO shares the identical structure with individual DONS and shares the training samples,
C = C . However, we observe from the numerical experiments that MODNO generally
MOL SOL
7achieves better accuracy and remains accurate even if we reduce N < N′ and N < N′ .
u,i u,i u,i,j u,i,j
In particular, given that MODNO and all individual DONs have the same structure, MODNO’s
centralized shared network α is trained with a significantly larger dataset compared to single
DONs. Since the single DON can achieve satisfying results for many tasks, together with the
overfitting concerns for α, we propose to reduce the number of training samples for the central-
ized shared structure. Our numerical experiments indicate that despite reducing the training
samples, prediction errors remain robust; however, fewer samples contribute to improved effi-
ciency. Specifically, the new cost for MODNO is: C =
(cid:80)NopN
N N +qN N N ,
MOL i=1 u,i u,i,j b,i u,i u,i,j a
here q ∈ [0,1] is the percentage of the data used in training the shared structure. In summary,
this approach motivates us to share more networks (i.e., make N larger) and utilize less data
a
for the centralized structure. Consequently, we make the entire branch networks shareable, and
it has proved to be effective in numerical experiments.
4 Numerical Experiments
In this section, we present numerical evidence to validate our algorithm. We will analyze five
examples, with the first four being experiments involving learning three operators. The final
experiment (the most challenging one) focuses on learning four operators, and we also test the
extrapolation to predict the solution at a time out of the training time range. Additionally,
the testing spatial mesh is different from the training solution spatial mesh in all experiments.
For each example, we will compare MODNO’s performance to training separate DONs for each
operator, all having the same architecture. To ensure reproducibility, all code and experiment
results will be made available on Google Colab upon publication.
4.1 Example One
In this section, we will study the following three equations and the operator of mapping the
initial condition to the solutions at a later time. The wave equation,
u = u ,t ∈ [0,1],x ∈ [0,2], (5)
tt xx
the Klein-Gordon equation, m = 0.1, c = 10,
u +m2c4u = c2u ,t ∈ [0,2],x ∈ [0,2], (6)
tt xx
and the Sine-Gordon equation,
u +csin(u) = u ,t ∈ [0,2],x ∈ [0,2]. (7)
tt xx
Allequationsareequippedwiththeperiodicboundaryconditions. Theinitialconditionsaregen-
eratedusingtheFourierseriestypefunction. Specifically, u (x;w ) = w sin(πx)+w sin(2πx)+
0 i 1 2
w sin(4πx) + w sin(6πx) + w cos(πx) + w cos(2πx) + w cos(4πx) + w cos(6πx) + w , and
3 4 5 6 7 8 9
w ∼ U(−2,2), i = 1,...,8 and w ∼ U(0.1,2). We present one realization of the solution of
i 9
three different equations at the terminal time in Figure 2.
8Figure 2: The terminal solutions of the Wave equation (5), Klein Gordon equation (6) and
Sine-Gordon equation (7) with one same initial condition.
To measure the variation of three equations’ solution data, we also compute the mean of all
training solutions and use it as a prediction for all three different equation solutions. The mean
relative error for Wave, Klein, and Sine are 49.78%, 99.85%, and 40.37% respectively. We finally
present the numerical results in Table 1.
Operator Single DON MODNO MODNO MODNO MODNO
100% data 100% data 90% data 80% data 70% data
Wave (5) 2.59% 2.23% 2.26% 2.44% 2.56%
Klein (6) 4.12% 3.69% 4.03% 4.0% 4.12%
Sine (7) 2.35% 2.09% 2.02% 2.78% 1.92%
Table 1: Results for experiment one. The second column, Single DON, is to train three indi-
vidual DONs with identical structures with full data for each operator. Notably, when reducing
MODNO data (i.e., q = 0.9,0.8,0.7), MODNO has lower costs when compared to three individ-
ual DONs. Please refer to Section 3.3 for the cost evaluation details.
4.1.1 Experiment One Results Analysis
We compare training one multi-operator MODNO with training three separate DONs one for
each operator with identical network structure. We can observe from Table 1 that MONDNO
outperforms training three individual DONs given the same amount of data. Specifically, 2.23%
vs 2.59% for Wave, 3.69% vs 4.12% for Klein-Gordon equation, and 2.09% vs 2.35% for Sine
Gordon equation.
When we decrease the data utilized in training the shared structure, the prediction accuracy for
the Sine Gordon equation remains resilient to the data reduction, with only a slight increase in
errors observed for the Wave and Klein Gordon equations. Nonetheless, a significant advantage
stemming from the enhanced efficiency with reduced data usage is evident. Even with a dataset
approximately 80% smaller than that employed for single DONs in multi-operator learning,
MODNO exhibits superior accuracy in predicting solutions for the Wave and Klein equations.
We do not observe this improved accuracy and enhanced efficiency in every single operator, but
thistrendisconsistentlyobservedformostofthenumericalexperimentsweconducted,highlight-
9ing a potential advantage and novelty of multi-operator learning within the MOL frameworks.
Specifically, multi-operator learning exhibits the capability to predict solutions with satisfactory
accuracy even when provided with less data compared to single-operator learning approaches.
4.2 Experiment Two
Inthissection,wewillexamineanotherthreenonlineartime-dependentequationsalongwiththe
operatorthatmaps the initialconditiontothe solutionatalater time; totesttheextrapolation,
the testing solution time is beyond the training time range. Specifically, we will delve into the
porous media type equation with different order m = 2,3,4,
u = (um) ,x ∈ [0,2],t ∈ [0,0.01]. (8)
t xx
All equations are equipped with periodic boundary conditions. We generate the initial condi-
tion using the Fourier series type functions by uniformly sampling the parameters in [−1,1].
Specifically, u(x;w ) = 0.1(w sin(πx)+w sin(2πx)+w sin(3πx)+w cos(2πx)+w cos(4πx)+
i 1 2 3 4 4
w cos(6πx)), and w ∼ U(−1,1). We present one realization of all three equations’ solutions in
6 i
Figure 3.
Figure3: Theterminalsolutionsofthreeporousmediaequations(8)withdifferentdegrees(two,
three, and four) with one same initial condition.
We first calculate the mean of all training target solutions, and use this solution to predict the
testing solutions of three equations separately. The relative L errors for degree two, three and
2
four porous media equations are 15.43%, 17.93% and 20.29% respectively. We then present the
numerical results in Table 2.
Operator Single DON MODNO MODNO MODNO
100% data 100% data 90% data 80% data
Degree 2 2.52% 1.14% 1.3% 1.38%
Degree 3 2.06% 2.03% 2.11% 2.25%
Degree 4 2.6% 2.54% 2.79% 3.39%
Table 2: The numerical results for the nonlinear porous media equations with different degrees.
The second column, Single DON, is to train three individual DONs with identical structures
with full data for each operator.
104.2.1 Experiment Two Results Analysis
Observations from Table 2 highlight that MODNO surpasses the accuracy of training a single
DON across three distinct operators. Even for equations of degree two and three, MODNO
maintains superior accuracy compared to training a single DON. These findings are consistent
with most of the outcomes from other numerical experiments, showing one potential benefit for
the entire MOL society.
4.3 Example Three
In this section, we will study applying a single network to learn the parabolic equation,
u +u = 1,x ∈ [0,2π],t ∈ [0,0.5], (9)
t xx
and the Viscous Burgers equation,
u2
u +( ) = αu ,x ∈ [0,2π],t ∈ [0,1], (10)
t x xx
2
where α = 0.1, and the Burgers equation,
u2
u +( ) = 0,x ∈ [0,2π],t ∈ [0,0.4]. (11)
t x
2
All equations are equipped with periodic boundary conditions. We study the mapping from
the initial condition to the solutions. The initial conditions are generated as w N(µ ,σ2) +
1 1 1
w N(µ ,σ2) adjusted to be periodic with w ,w ∼ U(0,0.5), µ ,µ ∼ U(2π,4π), and σ ,σ ∼
1 1 2 1 2 1 2 1 2
U(0.3,1). For the demonstration, we present one realization of the terminal solution for three
differenttypesofequationsinFigure4. WecanobservefromtheFigurethatthethreeequations’
solutions exhibit different behaviors, making the problem more challenging.
Figure 4: The terminal solutions of the parabolic equation (9), Viscous Burgers equation (10)
and Burger equation (11) with the same initial conditions. Three equations’ terminal simulation
times are 0.5, 1 and 0.4 respectively.
We first introduce the relative errors for three distinct equations when forecasted utilizing the
mean of all training target solutions. Specifically, the errors amount to 68.10%, 293.10%, and
281.34% for the Parabolic, Viscous Burgers, and Burgers equations respectively.
11We perform a sequence of numerical experiments to investigate the error fluctuation as we vary
the number of samples used to train the shared branch nets. Additionally, we incorporate
supplementary experiments where a single neural operator is trained to learn a single operator
from the entire dataset (comprising all training samples), enabling a comparison with the multi-
operator scenario. The outcomes are presented in Table 3.
Operator Single DON MODNO MODNO MODNO MODNO
100% data 100% data 90% data 80% data 70% data
Parabolic 1.48% 0.22% 0.31% 0.5% 0.3%
Viscous Burgers 3.88% 2.96% 2.67% 3.24% 5.98%
Burgers 4.82% 5.34% 5.41% 5.87% 7.53%
Table 3: The numerical results for the Parabolic (9), Viscous Burgers equation (10) and Burgers
equation(11). Thesecondcolumn, SingleDON,istotrainthreeindividualDONswithidentical
structures with full data for each operator. Notably, when reducing MODNO data to n%,
MODNOhaslowercosts(n%)whencomparedtothreeindividualDONs. PleaserefertoSection
3.3 for the cost evaluation details.
4.3.1 Experiment Three Results Analysis
FromtheresultsinTable3, itisevidentthattheerrorsoftheMODNOforboththeBurgersand
Viscous Burgers equations exhibit a slight increase as the number of training samples decreases
(improved efficiency). However, the prediction errors for the parabolic equation remain highly
robust regardless of the number of training samples. Notably, our experiments involve applying
a single neural operator (DON) with an identical structure (as the multi-operator MODNO)
to learn a single operator. Each operator undergoes training using the same quantity of data
as that used for training the full multi-operator model. As illustrated in Table 3, even when
reducingtheamountoftrainingdatato70%ofwhatisusedfortrainingasingleneuraloperator,
the multi-operator frameworks still outperform the single neural operators. Therefore, we can
infer that the multi-operator MODNO yields satisfactory prediction outcomes while requiring
fewer computational resources. This discovery holds greater significance when there is a scarcity
of data available for a specific operator, as the data from other operators can aid in the learning
process of the target operator.
4.4 Example Four
In this section, we will study mixing the Korteweg–De Vries (Kdv) equation,
u +δ2u +uu = 0,x ∈ [0,1],t ∈ [0,1], (12)
t xxx x
where δ = 0.022, Cahn-Hilliard (CH) equation,
u +ϵ2u +6(uu ) = 0,x ∈ [0,1],t ∈ [0,1], (13)
tt xxxx x x
where ϵ = 0.01, and Advection equation,
u +u = 0,x ∈ [0,1],t ∈ [0,0.1]. (14)
t x
12All equations are equipped with periodic boundary conditions. We study the mapping from
the initial condition to the solutions at a later time. The initial conditions are generated as
w N(µ ,σ2)+w N(µ ,σ2) adjusted to be periodic with w ,w ∼ U(0,0.5), µ ∼ U(0.1,0.9),
1 1 1 1 1 2 1 2 1
µ ∼ U(0.8,1.5), and σ ,σ ∼ U(0.1,0.5). We present the terminal solutions with respect to the
2 1 2
same initial condition for three different equations in Figure 5.
Figure 5: The terminal solutions of the KDV equation (12), Cahn-Hilliard (13) and Advection
equation (14) with the same initial conditions. Three equations’ terminal simulation times are
1, 1 and 0.1 respectively.
Similar to the previous examples, we first compute the mean prediction error, specifically, we
use the mean of all target training solutions to predict all three different operators. The errors
are 68.60%, 35.75%, and 76.68% for KDV, Cahn-Hilliard and Advection equation respectively.
We then present the results of the training and testing in Table 4.
Operator Single DON MODNO MODNO MODNO MODNO
100% data 100% data 90% data 80% data 70% data
KDV 8.39% 7.62% 7.66% 7.53% 7.23%
Cahn Hilliard 0.28% 1.13% 1.49% 1.54% 2.21%
Advection 2.09% 4.5% 4.29% 4.23% 3.96%
Table 4: Examples Four Results. The second column, represents the training of three individual
DONs with identical structures, each trained with full data for a single operator. For MODNO,
we reduce the amount of data used in the shared structure and present the results in the last
three columns.
4.5 Experiment Four Results Analysis
In Table 4, it is evident that MODNO outperforms single DON in predicting the KDV equation
across all datasets with fewer training samples (more efficient). We notice a decrease in errors
in both KDV and Advection prediction as we decrease the amount of data used to train the
centralized shared structure. This partially confirms our conjecture that the centrally trained
structure might overfit in some cases since it is trained by tripling the datasets using the same
input.
134.6 Experiment Five
In this set of experiments, we consider four equations one from each experiment in the previous
sections, however, to even increase the level of difficulty, we choose different simulation terminal
times. Specifically, we will consider the nonlinear degree two porous media equation (8) with
t ∈ [0,2], Sine-Gordon equation (7) with t ∈ [0,2], parabolic equation (9) with t ∈ [0,0.02] and
Canh Hilliard equation (13) with t ∈ [0,0.5].
Figure 6: Demonstrations of one realization of terminal solutions corresponding to different
equations. Porous media (8) at t = 0.01, Cahn Hilliard (CH) (13) at t = 0.5, Sine-Gordon (7)
at t = 2 and Parabolic at t = 0.02.
Allequationsareequippedwiththeperiodicboundaryconditionsandwelearnthemappingfrom
the initial conditions to the solutions at a later time. Specifically, we will test the extrapolation
andpredictthesolutionatterminaltimewhichisnotincludedinthetrainingdataset. Theinitial
conditions are generated the same as in Example 2. We present one realization of the solutions
of four different equations at the terminal time in Figure 6. We summarize the numerical results
in Table 5.
Operator Single DON MODNO MODNO MODNO
100% data 100% data 90% data 80% data
Porous Media 0.98% 1.66% 1.17% 1.44%
Cahn Hilliard 3.24% 3.04% 3.01% 3.24%
Sine Gordon 5.62% 5.07% 5.27% 5.29%
Parabolic 2.12% 2.61% 2.29% 2.73%
Table 5: The numerical results for Example five of mixing four equations. The second column,
Single DON, is to train four individual DONs with identical structures with full data for each
operator. Notably, when reducing MODNO data, MODNO has lower costs when compared to
three individual DONs. Please refer to Section 3.3 for the cost evaluation details.
4.7 Experiment Five Results Analysis
In these experiments, we combine four equations with distinct landscapes, as illustrated in Fig-
ure 6. Analysis from Table 5 reveals that MODNO surpasses SOL in predicting Canh Hilliard
equations and Sine-Gordon equations, even when only utilizing 80% of the total samples for
14training the centralized structure. However, the predictions for Degree two Porous media equa-
tions and Parabolic equations fall short of expectations. Nonetheless, these predictions remain
accurate (1.17% and 2.29%) and closely match those of single DONs, even when trained with
only 90% of the data used for single DONs.
5 Conclusion
In this work, we propose a distributed algorithm to enable Deep Operator Neural Network
(DON) and other Chen-Chen-type neural operators to be a multi-operator learning (MOL)
framework. The idea is to construct the basis for each operator separately using its own data
while centralized learning the input function model reduction. The proposed framework can
learn multiple operators simultaneously with less cost when compared to training a single neural
operator to learn each operator separately. For many tasks, we even observe an improved
accuracy. Compared to other MOL foundation models, the proposed methods have significantly
less training cost and can be run even on personal devices.
However,therearesomelimitationstothemethods. UnlikeMOLfoundationmodels,thecurrent
models have limited extrapolation capabilities to unseen operators. Addressing this limitation
constitutes a potential area for future research. Specifically, exploring the feasibility of fine-
tuning with or without data informed by physics will be pursued. Leveraging the success of
multi-fidelity models, it is anticipated that the proposed method can be readily generalized to
other unseen operators.
Additional future research directions include extending the framework to be discretization-
invariant, allowing for different input functions to be discretized differently. This extension
would offer alternatives to eliminate the “same input function space” assumptions made in the
framework.
Another potential avenue for future research involves the integration of physics-informed loss
to tackle parametric PDEs with free parameters. This enhancement would enable MODNO to
accommodate a broader range of operators generated based on sampling the free parameters of
the base PDEs, thereby advancing it into a more sophisticated MOL framework.
6 Acknoledgement
The author expresses gratitude to Professor Hayden Schaeffer from the Department of Mathe-
matics at UCLA for help. Additionally, the author extends appreciation to Ms. Jingmin Sun,
a Ph.D. candidate from Carnegie Mellon University, for her generous help with numerical PDE
solvers.
References
[1] K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart. Model reduction and
neural networks for parametric pdes. The SMAI journal of computational mathematics,
7:121–157, 2021.
15[2] C. Chen and J. Wu. Neural operator for modeling dynamical systems with trajectories and
statistics matching. Bulletin of the American Physical Society, 2023.
[3] C. Chen and J.-L. Wu. Operator learning for continuous spatial-temporal model with a
hybrid optimization scheme. arXiv preprint arXiv:2311.11798, 2023.
[4] T. Chen and H. Chen. Approximations of continuous functionals by neural networks with
application to dynamic systems. IEEE Transactions on Neural networks, 4(6):910–918,
1993.
[5] T. Chen and H. Chen. Universal approximation to nonlinear operators by neural net-
works with arbitrary activation functions and its application to dynamical systems. IEEE
Transactions on Neural Networks, 6(4):911–917, 1995.
[6] M. V. de Hoop, N. B. Kovachki, N. H. Nelsen, and A. M. Stuart. Convergence rates for
learning linear operators from noisy data. SIAM/ASA Journal on Uncertainty Quantifica-
tion, 11(2):480–513, 2023.
[7] B. Deng, Y. Shin, L. Lu, Z. Zhang, and G. E. Karniadakis. Approximation rates of deep-
onets for learning operators arising from advection–diffusion equations. Neural Networks,
153:411–426, 2022.
[8] P. C. Di Leoni, L. Lu, C. Meneveau, G. E. Karniadakis, and T. A. Zaki. Neural operator
prediction of linear instability waves in high-speed boundary layers. Journal of Computa-
tional Physics, 474:111793, 2023.
[9] Y. Efendiev, W. T. Leung, G. Lin, and Z. Zhang. Efficient hybrid explicit-implicit learning
for multiscale problems. Journal of Computational Physics, page 111326, 2022.
[10] S. Garg and S. Chakraborty. Vb-deeponet: A bayesian operator learning framework for
uncertainty quantification. Engineering Applications of Artificial Intelligence, 118:105685,
2023.
[11] E. Hasani and R. A. Ward. Generating synthetic data for neural operators. arXiv preprint
arXiv:2401.02398, 2024.
[12] C. F. Higham and D. J. Higham. Deep learning: An introduction for applied mathemati-
cians. Siam review, 61(4):860–891, 2019.
[13] A. A. Howard, M. Perego, G. E. Karniadakis, and P. Stinis. Multifidelity deep operator
networks. arXiv preprint arXiv:2204.09157, 2022.
[14] N. Hua and W. Lu. Basis operator network: A neural network-based model for learning
nonlinear operators via neural basis. Neural Networks, 164:21–37, 2023.
[15] Z. Jiang, M. Zhu, D. Li, Q. Li, Y. O. Yuan, and L. Lu. Fourier-mionet: Fourier-enhanced
multiple-input neural operators for multiphase modeling of geological carbon sequestration.
arXiv preprint arXiv:2303.04778, 2023.
[16] P. Jin, S. Meng, and L. Lu. Mionet: Learning multiple-input operators via tensor product.
SIAM Journal on Scientific Computing, 44(6):A3490–A3514, 2022.
16[17] N. Kovachki, S. Lanthaler, and S. Mishra. On universal approximation and error bounds
for fourier neural operators. Journal of Machine Learning Research, 22:Art–No, 2021.
[18] S. Lanthaler, S. Mishra, and G. E. Karniadakis. Error estimates for deeponets: A deep
learning framework in infinite dimensions. Transactions of Mathematics and Its Applica-
tions, 6(1):tnac001, 2022.
[19] S. Lanthaler, R. Molinaro, P. Hadorn, and S. Mishra. Nonlinear reconstruction for operator
learning of pdes with discontinuities. arXiv preprint arXiv:2210.01074, 2022.
[20] S. Lanthaler, T. K. Rusch, and S. Mishra. Neural oscillators are universal. Advances in
Neural Information Processing Systems, 36, 2024.
[21] S. Lanthaler and A. M. Stuart. The curse of dimensionality in operator learning. arXiv
preprint arXiv:2306.15924, 2023.
[22] D. Lee, L. Zhang, Y. Yu, and W. Chen. Deep neural operator enabled concurrent multi-
task design for multifunctional metamaterials under heterogeneous fields. arXiv preprint
arXiv:2312.02403, 2023.
[23] W. T. Leung, G. Lin, and Z. Zhang. Nh-pinn: Neural homogenization-based physics-
informed neural network for multiscale problems. Journal of Computational Physics, page
111539, 2022.
[24] B. Li, H. Wang, S. Feng, X. Yang, and Y. Lin. Solving seismic wave equations on variable
velocitymodelswithfourierneuraloperator. IEEE Transactions on Geoscience and Remote
Sensing, 61:1–18, 2023.
[25] Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar. Fourier neural operator with learned
deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022.
[26] Z.Li,D.Z.Huang,B.Liu,andA.Anandkumar. Fourierneuraloperatorwithlearneddefor-
mations for pdes on general geometries. Journal of Machine Learning Research, 24(388):1–
26, 2023.
[27] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. Fourier neural operator for parametric partial differential equations. arXiv preprint
arXiv:2010.08895, 2020.
[28] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. Neural operator: Graph kernel network for partial differential equations. arXiv
preprint arXiv:2003.03485, 2020.
[29] Z.Li,H.Zheng,N.Kovachki,D.Jin,H.Chen,B.Liu,K.Azizzadenesheli,andA.Anandku-
mar.Physics-informedneuraloperatorforlearningpartialdifferentialequations.ACM/JMS
Journal of Data Science, 2021.
[30] G. Lin, C. Moya, and Z. Zhang. On learning the dynamical response of nonlinear control
systems with deep operator networks. arXiv preprint arXiv:2206.06536, 2022.
17[31] G. Lin, C. Moya, and Z. Zhang. B-deeponet: An enhanced bayesian deeponet for solving
noisy parametric pdes using accelerated replica exchange sgld. Journal of Computational
Physics, 473:111713, 2023.
[32] G. Lin, Y. Wang, and Z. Zhang. Multi-variance replica exchange sgmcmc for inverse and
forward problems via bayesian pinn. Journal of Computational Physics, 460:111173, 2022.
[33] H. Liu, H. Yang, M. Chen, T. Zhao, and W. Liao. Deep nonparametric estimation of
operators between infinite dimensional spaces. Journal of Machine Learning Research,
25(24):1–67, 2024.
[34] Y. Liu, Z. Zhang, and H. Schaeffer. Prose: Predicting operators and symbolic expressions
using multimodal transformers. arXiv preprint arXiv:2309.16816, 2023.
[35] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators
via deeponet based on the universal approximation theorem of operators. Nature Machine
Intelligence, 3(3):218–229, 2021.
[36] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. E. Karniadakis. A compre-
hensive and fair comparison of two neural operators (with practical extensions) based on
fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022.
[37] L. Lu, R. Pestourie, S. G. Johnson, and G. Romano. Multifidelity deep neural operators
for efficient learning of partial differential equations with application to fast inverse design
of nanoscale heat transport. Physical Review Research, 4(2):023210, 2022.
[38] M. Lu, A. Mohammadi, Z. Meng, X. Meng, G. Li, and Z. Li. Deep neural operator for
learning transient response of interpenetrating phase composites subject to dynamic load-
ing. Computational Mechanics, 72(3):563–576, 2023.
[39] Z. Ma, K. Azizzadenesheli, and A. Anandkumar. Calibrated uncertainty quantification for
operator learning via conformal prediction. arXiv preprint arXiv:2402.01960, 2024.
[40] S.Mao,R.Dong,L.Lu,K.M.Yi,S.Wang,andP.Perdikaris. Ppdonet: Deepoperatornet-
worksforfastpredictionofsteady-statesolutionsindisk–planetsystems. The Astrophysical
Journal Letters, 950(2):L12, 2023.
[41] Z. Mao, L. Lu, O. Marxen, T. A. Zaki, and G. E. Karniadakis. Deepm&mnet for hyper-
sonics: Predicting the coupled flow and finite-rate chemistry behind a normal shock using
neural-network approximation of operators. Journal of computational physics, 447:110698,
2021.
[42] M. McCabe, B. R.-S. Blancard, L. H. Parker, R. Ohana, M. Cranmer, A. Bietti, M. Eick-
enberg, S. Golkar, G. Krawezik, F. Lanusse, et al. Multiple physics pretraining for physical
surrogate models. arXiv preprint arXiv:2310.02994, 2023.
[43] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-
efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics, pages 1273–1282. PMLR, 2017.
18[44] C. Moya and G. Lin. Fed-deeponet: Stochastic gradient-based federated training of deep
operator networks. Algorithms, 15(9):325, 2022.
[45] C. Moya, G. Lin, T. Zhao, and M. Yue. On approximating the dynamic response of syn-
chronous generators via operator learning: A step towards building deep operator-based
power grid simulators. arXiv preprint arXiv:2301.12538, 2023.
[46] C.Moya,A.Mollaali,Z.Zhang,L.Lu,andG.Lin. Conformalized-deeponet: Adistribution-
free framework for uncertainty quantification in deep operator networks. arXiv preprint
arXiv:2402.15406, 2024.
[47] T. O’Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas. Derivative-informed projected
neural networks for high-dimensional parametric maps governed by pdes. Computer Meth-
ods in Applied Mechanics and Engineering, 388:114199, 2022.
[48] D.Patel, D.Ray, M.R.Abdelmalik, T.J.Hughes, andA.A.Oberai. Variationallymimetic
operator networks. Computer Methods in Applied Mechanics and Engineering, 419:116536,
2024.
[49] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani,
T. Kurth, D. Hall, Z. Li, K. Azizzadenesheli, et al. Fourcastnet: A global data-driven
high-resolution weather model using adaptive fourier neural operators. arXiv preprint
arXiv:2202.11214, 2022.
[50] Y. Qiu, N. Bridges, and P. Chen. Derivative-enhanced deep operator network. arXiv
preprint arXiv:2402.19242, 2024.
[51] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational physics, 378:686–707, 2019.
[52] H. Schaeffer. Learning partial differential equations via data discovery and sparse optimiza-
tion. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
473(2197):20160446, 2017.
[53] H.Schaeffer,R.Caflisch,C.D.Hauck,andS.Osher. Sparsedynamicsforpartialdifferential
equations. Proceedings of the National Academy of Sciences, 110(17):6634–6639, 2013.
[54] H. Schaeffer and S. G. McCalla. Sparse model selection via integral terms. Physical Review
E, 96(2):023302, 2017.
[55] J. Shen, T. Marwah, and A. Talwalkar. Ups: Towards foundation models for pde solving
via cross-modal adaptation. arXiv preprint arXiv:2403.07187, 2024.
[56] Z.Song,C.Wang,andH.Yang. Finiteexpressionmethodforlearningdynamicsoncomplex
networks. arXiv preprint arXiv:2401.03092, 2024.
[57] T. Tripura and S. Chakraborty. Wavelet neural operator for solving parametric partial
differential equations in computational mechanics problems. Computer Methods in Applied
Mechanics and Engineering, 404:115783, 2023.
19[58] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and J. S. Rellermeyer.
Asurveyondistributedmachinelearning. Acm computing surveys (csur), 53(2):1–33, 2020.
[59] S. Wang, H. Wang, and P. Perdikaris. Learning the solution operator of parametric partial
differential equations with physics-informed deeponets. Science advances, 7(40):eabi8605,
2021.
[60] G.Wen,Z.Li,K.Azizzadenesheli,A.Anandkumar,andS.M.Benson. U-fno—anenhanced
fourier neural operator-based deep-learning model for multiphase flow. Advances in Water
Resources, 163:104180, 2022.
[61] L. Yang, S. Liu, T. Meng, and S. J. Osher. In-context operator learning with data
prompts for differential equation problems. Proceedings of the National Academy of Sci-
ences, 120(39):e2310142120, 2023.
[62] L. Yang, T. Meng, S. Liu, and S. J. Osher. Prompting in-context operator learning with
sensor data, equations, and natural language. arXiv preprint arXiv:2308.05061, 2023.
[63] L. Yang and S. J. Osher. Pde generalization of in-context operator networks: A study on
1d scalar nonlinear conservation laws. arXiv preprint arXiv:2401.07364, 2024.
[64] Z.Ye,X.Huang,L.Chen,H.Liu,Z.Wang,andB.Dong. Pdeformer: Towardsafoundation
model for one-dimensional partial differential equations. arXiv preprint arXiv:2402.12652,
2024.
[65] X.Yin,Y.Zhu,andJ.Hu. Acomprehensivesurveyofprivacy-preservingfederatedlearning:
A taxonomy, review, and future directions. ACM Computing Surveys (CSUR), 54(6):1–36,
2021.
[66] L. Zhang and H. Schaeffer. On the convergence of the sindy algorithm. Multiscale Modeling
& Simulation, 17(3):948–972, 2019.
[67] Z. Zhang, W. T. Leung, and H. Schaeffer. A discretization-invariant extension and analysis
of some deep operator networks. arXiv preprint arXiv:2307.09738, 2023.
[68] Z.Zhang, C.Moya, W.T.Leung, G.Lin, andH.Schaeffer. Bayesiandeepoperatorlearning
for homogenized1 to fine-scale maps for multiscale pde. 2023.
[69] Z. Zhang, C. Moya, L. Lu, G. Lin, and H. Schaeffer. D2no: Efficient handling of het-
erogeneous input function spaces with distributed deep neural operators. arXiv preprint
arXiv:2310.18888, 2023.
[70] Z. Zhang, L. Wing Tat, and H. Schaeffer. Belnet: basis enhanced learning, a mesh-free
neural operator. Proceedings of the Royal Society A, 479(2276):20230043, 2023.
[71] M. Zhu, S. Feng, Y. Lin, and L. Lu. Fourier-deeponet: Fourier-enhanced deep operator net-
works for full waveform inversion with improved accuracy, generalizability, and robustness.
arXiv preprint arXiv:2305.17289, 2023.
[72] M. Zhu, H. Zhang, A. Jiao, G. E. Karniadakis, and L. Lu. Reliable extrapolation of deep
neural operators informed by physics or sparse observations. Computer Methods in Applied
Mechanics and Engineering, 412:116064, 2023.
20