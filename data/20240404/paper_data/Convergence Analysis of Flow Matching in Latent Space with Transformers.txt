JournalofMachineLearningResearch23(2024)1-65 Submitted1/21;Revised5/22;Published9/22
Convergence Analysis of Flow Matching in Latent Space with
Transformers
Yuling Jiao yulingjiaomath@whu.edu.cn
School of Mathematics and Statistics
and Hubei Key Laboratory of Computational Science
Wuhan University, Wuhan 430072, China
Yanming Lai ylaiam@connect.ust.hk
Department of Mathematics
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong, China
Yang Wang yangwang@ust.hk
Department of Mathematics
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong, China
Bokai Yan byanac@connect.ust.hk
Department of Mathematics
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong, China
Editor: My editor
Abstract
WepresenttheoreticalconvergenceguaranteesforODE-basedgenerativemodels,specifically
flow matching. We use a pre-trained autoencoder network to map high-dimensional original
inputs to a low-dimensional latent space, where a transformer network is trained to predict
the velocity field of the transformation from a standard normal distribution to the target
latent distribution. Our error analysis demonstrates the effectiveness of this approach,
showing that the distribution of samples generated via estimated ODE flow converges to
the target distribution in the Wasserstein-2 distance under mild and practical assumptions.
Furthermore, we show that arbitrary smooth functions can be effectively approximated by
transformer networks with Lipschitz continuity, which may be of independent interest.
Keywords: deep generative model, ODE flow, transformer network, end-to-end error
bound
1 Introduction
A wide variety of statistics and machine learning problems can be framed as generative
modeling, especially when there is an emphasis on accurately modeling and efficiently
sampling from intricate distributions, including those associated with images, sound, and
text. The essence of generative modeling lies in its ability to learn a target distribution from
finite samples, a task at which models incorporating deep neural networks have recently
achieved considerable success.
©2024YulingJiao,YanmingLai,YangWangandBokaiYan.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovidedat
http://jmlr.org/papers/v23/21-0000.html.
4202
rpA
3
]LM.tats[
1v83520.4042:viXraJiao, Lai, Wang and Yan
Generative Adversarial Networks (GANs; Goodfellow et al. (2014); Arjovsky et al.
(2017)), as a flagship example of deep generative models, have successfully been applied
to a wide range of application challenges, including the synthesis of photorealistic images
and videos (Radford et al., 2015; Wang et al., 2018; Chan et al., 2019), data augmentation
(Frid-Adar et al., 2018), style transfer (Zhu et al., 2017), and facial editing (Karras et al.,
2019). Additionally, significant research has been conducted to analyze the theoretical
properties of GANs. Bai et al. (2018) demonstrated that GANs could learn distributions
within the Wasserstein distance, provided the discriminator class has sufficient distinguishing
capability against the generator class. Chen et al. (2020b) established a minimax optimal
convergence rate based on optimal transport theory, which necessitates the input and output
dimensions of the generator to be identical. Huang et al. (2022) proved that GANs could
learn any distribution with bounded support. Despite their theoretical elegance and practical
achievements, GANs often encounter challenges such as training instability, mode collapse,
and difficulties in evaluating the quality of generated data.
The recent breakthrough known as the diffusion model has gained notable attention for
its superior sample quality and a significantly more stable and controllable training process
compared to GANs. The initial concept of the diffusion model involves training a denoising
modeltoprogressivelytransformnoisedataintosamplesthatadheretothetargetdistribution
(Hoetal.,2020), whichhassoonbeenmathematicallyproventocorrespondtolearningeither
the drift term of a Stochastic Differential Equation (SDE) or the velocity field of an Ordinary
Differential Equation (ODE) (Song et al., 2021). In SDE-based methods, the target data
density degenerates into a simpler Gaussian density through the Ornstein-Uhlenbeck (OU)
process, followed by solving a reverse-time SDE to generate samples from noise (Ho et al.,
2020; Song et al., 2021; Meng et al., 2021). Researchers have also proposed the diffusion
Schr¨odinger Bridge (SB), which formulates a finite-time SDE, effectively accelerating the
simulation time (De Bortoli et al., 2021). The achievements of ODE-based methods are
equally remarkable, with most adopting an approach involving interpolative trajectory
modeling (Liu et al., 2022b; Albergo and Vanden-Eijnden, 2022; Liu et al., 2023; Xu et al.,
2022). Liu et al. (2022b) employs linear interpolation to connect the target distribution with
a reference distribution, while Albergo and Vanden-Eijnden (2022) extends this interpolation
to nonlinear cases. Further Gao et al. (2023) uses interpolation to analyze the regularity of
a broad class of ODE flows.
In the past few years, there has been an explosive development in SDE/ODE-based
generative models, with many models showcasing outstanding performance across a diverse
array of application challenges. Dhariwal and Nichol (2021) have demonstrated that diffusion
models outperform GANs in both unconditional and conditional image synthesis, setting
a new benchmark in the quality of generated images. Rombach et al. (2022) showed that
generative processes operating in a latent space can significantly reduce computational
resources while maintaining high-quality text-to-image generation. A line of research (Kong
et al., 2020; Chen et al., 2020c; Popov et al., 2021; Liu et al., 2022a) introduced versatile
diffusion models capable of synthesizing high-fidelity audio, marking considerable progress
in the quality of speech and music generation. Additionally, considerable research has
concentrated on text-to-video generation, aiming to create long videos while maintaining
high visual quality and adherence to the user’s prompt (Blattmann et al., 2023a,b; Wu et al.,
2023; Chen et al., 2024; Wang et al., 2024; Brooks et al., 2024). Despite these models being
2Convergence Analysis of Flow Matching
tailored for various tasks, they typically share two common features. Firstly, they utilize an
encoder-decoder architecture to map high-dimensional original inputs to a low-dimensional
latent space, where the SDE/ODE-based generative process takes place. Secondly, they
employ transformers as the backbone architecture.
Although some analyses have attempted to explain the success of SDE/ODE-based
generative models, these analyses either involve technical and unverifiable assumptions or
do not align with the models actually used in practice. In a series of studies (Lee et al.,
2022, 2023; De Bortoli, 2022; Chen et al., 2022, 2023a; Benton et al., 2023; Conforti et al.,
2023), researchers systematically examined the sampling errors of diffusion models across
various target distributions and have determined the optimal sampling error order. Their
analysis assumes that the velocity field or drift term in diffusion models has been well-
trained, without considering the training process and model selection, thus not providing an
end-to-end analysis. It should be noted that end-to-end error analysis is rarely observed
even in the domain of general ODE/SDE generative methods. To our knowledge, Wang et al.
(2021) first proved the consistency of the Schro¨dinger Bridge approach through an end-to-end
analysis. Oko et al. (2023) proved that in an SDE-based generative model, when the true
density function has certain regularities and the empirical score matching loss is properly
minimized, the generated data distribution achieves nearly minimax optimal estimation
rates in total variation distance and Wasserstein-1 distance. Tang and Yang (2024) further
extended the analysis to the intrinsic manifold assumption. Chen et al. (2023b) considered
a special case in which the encoder and decoder are linear models. Chang et al. (2024)
developed an ODE-based framework and derived a non-asymptotic convergence rate in the
Wasserstein-2 distance. However, these analyses do not consider the transformer architecture
or incorporate pre-training, which are commonly used in practical implementations, leaving
a gap in explaining the success of SDE/ODE-based generative models.
In this paper, we mathematically prove that the distribution of the samples generated
via ODE flow converges to the target distribution in the Wasserstein-2 distance under mild
and practical assumptions, providing the first comprehensive end-to-end error analysis that
considers the transformer architecture and allows for domain shift in pre-training.
1.1 Our main contributions
Our main contributions are summarized as follows.
• We establish approximation guarantees for transformer networks subject to Lipschitz
continuity constraints, which may be of independent interest. (Theorem 8 and 9).
Specifically, we prove that the transformer network can approximate any function, with
the Lipschitz continuity of the network remaining independent of the approximation
error. Under the assumption that the target distribution has bounded support, we
show that the ground truth velocity field is a smooth function, allowing it to be
sufficiently approximated by a properly chosen transformer network.
• We establish statistical guarantees for pre-training using the learned encoder and
decoder network (Lemma 16). Choosing transformer networks as our encoder and
decoder, we show that the excessive risk of reconstruction loss converges at a rate of
O(cid:101)(m− D1 +2), where m is the pre-training sample size, only under the assumptions that
3Jiao, Lai, Wang and Yan
the pre-trained data distribution has bounded support and that there exist smooth
functions minimizing the reconstruction loss.
• We establish estimation guarantees for the target distribution using the estimated
velocity field (Theorem 17). By choosing proper discretization step size and early
stoppingtimeforgeneratingsamples, weprovethatE [W (γ ,γ )] = O(ε +ε ),
Y,X 2 (cid:98)T 1 γ (cid:101)1 γ (cid:101)1,γ1
where γ is the generated data distribution, γ is the target distribution, ε
(cid:98)T 1 γ (cid:101)1,γ1
denotes the domain shift between the target distribution and the pre-trained data
distribution, and ε is the minimum reconstruction loss achievable by the encoder-
γ (cid:101)1
decoder architecture. Specifically, if there is no domain shift and the encoder-decoder
architecture can perfectly reconstruct the distribution, our results show that the
generated data distribution converges to the target distribution in Wasserstein-2
distance.
1.2 Organization
Therestofthepaperisorganizedasfollows. InSection2, weprovidenotationsandintroduce
key concepts. In Section 3, we show that the true velocity field can be well approximated
by a Lipschitz transformer network. In Section 4, we show that the true velocity field can
be efficiently estimated, and analyze the error of distribution recovery using the estimated
velocity field. Finally, in Section 5, we analyze the error introduced by the pre-trained
autoencoder.
2 Preliminaries
In this section, we introduce the notations used throughout this paper. Additionally, we
provide details about transformer networks, pre-training, and flow matching.
Notations. Here we summarize the notations. Given a real number α, we denote ⌊α⌋
as the largest integer smaller than α (in particular, if α is an integer, ⌊α⌋ = α−1). For
a vector x ∈ Rd, we denote its ℓ2-norm by ∥x∥, the ℓ∞-norm by ∥x∥ = max |x |. We
∞ i i
define x⊗2 := xx⊤. We define the operator norm of a matrix A as ∥A∥ := sup ∥Ax∥.
op ∥x∥≤1
For two matrices A,B ∈ Rd×d, we say A ⪯ B if B − A is positive semi-definite. We
denote the identity matrix in Rd×d by I . For a twice continuously differentiable function
d
f : Rd → R, let ∇f,∇2f, and ∆f denote its gradient, Hessian, and Laplacian, respectively.
For a probability density function π and a measurable function f : Rd → R, we define
the L2(π)-norm of f as ∥f∥ := ((cid:82) (f(x))2π(x)dx)1/2. We define L∞(K)-norm as
L2(π)
∥f∥ := sup |f(x)|. Thefunctioncompositionoperationismarkedasg◦f := g(f(x))
L∞(K) x∈K
for functions f and g. We use the asymptotic notation f(x) = O(g(x)) to denote the
statement that f(x) ≤ Cg(x) for some constant C > 0 and O(cid:101)(·) to ignore the logarithm.
For a vector function v : Rd → Rd′, we define its L2(π)-norm as ∥v∥ := ∥∥v∥∥
L2(π) L2(π)
and L∞(K)-norm as ∥v∥ := ∥∥v∥∥ . For any dataset D = {x }n , we define the
L∞(K) L∞(K) i i=1
imageofD underv asv(D) := {v(x )}n . Giventwodistributionsµandν, theWasserstein-
i i=1
2 distance is defined as W (µ,ν) := inf E [∥x−y∥2]1/2, where Π(µ,ν) is the set
2 π∈Π(µ,ν) (x,y)∼π
of all couplings of µ and ν. A coupling is a joint distribution on Rd×Rd whose marginals are
µ and ν on first and second factors, respectively. Let f : Rd → Rd′ be a measurable mapping
and µ be a probability measure on Rd. The push-forward measure f µ of a measurable set
#
4Convergence Analysis of Flow Matching
Euler
method
E(cid:98) D(cid:98) E(cid:98) D(cid:98)
Pre-training Flow matching Sampling
Figure 1: An illustration of our framework. Pre-training: Based on m samples Y = {y }m
i i=1
drawn i.i.d. from pre-trained data distribution γ , we minimize the empirical
(cid:101)1
reconstructionlosstoobtainanencoderE(cid:98) : [0,1]D → [0,1]d andthecorresponding
decoder D(cid:98) : [0,1]d → RD. These will serve as the bridge linking the high-
dimensionalinputspaceandthelow-dimensionallatentspace. Flowmatching: For
the target distribution γ and n samples X = {x }n drawn from it, the encoder
1 i i=1
E(cid:98) maps them to the latent space with π
1
= E(cid:98)#γ
1
and E(cid:98)(X) = {E(cid:98)(x i)}n i=1. Flow
matching is then applied within the latent space, where a transformer network is
trained to predict the velocity field of the transformation from a standard normal
distribution π = N(0,I ) to the target latent distribution π . Sampling: Given
0 d 1
the estimated velocity field, we can generate samples from an approximation of
the continuous flow ODE starting from the prior distribution π . The generated
0
latent data distribution π will be mapped back to the high-dimensional space by
(cid:98)T
the decoder D(cid:98), resulting in the generated data distribution γ
(cid:98)T
= D(cid:98)#π (cid:98)T.
K is defined as f µ := µ(f−1(K)). In neural networks, the Rectified Linear Unit (ReLU)
#
activation function is denoted by σ(x) = max{x,0} and is applied element-wise to vectors
or matrices. We define the hardmax operator as σ (x) := lim exp(cx)/∥exp(cx)∥ ,
H c→+∞ 1
where the operation is performed column-wise if the input to σ is a matrix. The Hadamard
H
product ⊙ refers to the element-wise multiplication of two vectors or matrices of the same
dimensions.
2.1 Transformer networks
In the last few years, academic inquiry has concentrated on the approximation power and
generalization capability of ReLU neural networks (Yarotsky, 2017; Suzuki, 2019; Bartlett
et al., 2019; Yarotsky and Zhevnerchuk, 2020; Schmidt-Hieber, 2020; Lu et al., 2021; Shen
et al., 2022). These networks become the preferred choice for theoretical analysis and are
able to achieve the minimax optimal rate in many problems (Huang et al., 2022; Duan et al.,
2022; Jiao et al., 2023; Oko et al., 2023; Liu et al., 2024b). In contrast, the theoretical
understanding of transformer networks remains limited, despite their resounding success in
practical applications. Gurevych et al. (2022) recently provided a framework to study the
approximation properties and generalization abilities of transformer networks. We adopt
5Jiao, Lai, Wang and Yan
their framework and extend it by incorporating control over the regularity of the neural
network functions.
Given d,d′ ∈ N, we define a transformer network ϕ : Rd → Rd′ as follows:
(FF) (SA) (FF) (SA)
ϕ = E ◦F ◦F ◦···◦F ◦F ◦E ◦P. (1)
out N N 1 1 in
The first layer of the transformer network P, known as ”patchify”, divides the spatial
input into patches. Namely, an input x of dimension d is transformed into a sequence X of
l tokens, where each token has a dimension of d . These tokens are explicitly selected
patch
from components of the input, thus this layer does not require training. For simplicity, we
assume d = d ×l.
patch
The input embedding layer E
in
: R(d patch+l)×l → Rd model×l, incorporating position
encoding, is a token-wise linear mapping:
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
X X
Z = E Concat = A +b 1⊤ (2)
0 in I in I in l
l l
where A in ∈ Rd model×(d patch+l) and b in ∈ Rd model represent the weight matrix and bias vector
of the embedding layer, and 1 denotes a vector of l components, each of which is 1.
l
The multi-head attention layer F(SA) : Rd model×l → Rd model×l represents the interaction
among tokens:
h
(cid:88) (cid:104)(cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
F(SA)(Z) = Z + W (W Z) (W Z)⊤(W Z) ⊙σ (W Z)⊤(W Z) (3)
O,s V,s K,s Q,s H K,s Q,s
s=1
where h ∈ N is the number of heads which we compute in parallel, d ∈ N is the dimension
k
of the queries and keys, d ∈ N is the dimension of the values, d = h·d , W ,W ∈
v model v K,s Q,s
Rd k×d model,W V,s ∈ Rdv×d model and W O,s ∈ Rd model×dv are the weight matrices, and σ H is the
hardmax operator. We include a skip-connection in the attention layer.
The token-wise feedforward neural network F(FF) : Rd model×l → Rd model×l processes each
token independently in parallel by applying two feedforward layers:
F(FF)(Y) = Y +W σ(W Y +b 1⊤)+b 1⊤
2 1 1 l 2 l
where d
ff
∈ N denotes the hidden layer size of the feedforward layer, W
1
∈ Rd ff×d model,b
1
∈
Rd ff,W 2 ∈ Rd model×d ff and b 2 ∈ Rd model are parameters, and σ is the ReLU activation
function. The feedforward layer also includes a skip-connection.
The output embedding E
out
: Rd model×l → Rd′,
E (Z) = A z +b
out out 1 out
where Z = (z 1,z 2,...,z l), and A out ∈ Rd′×d model and b out ∈ Rd′ are the weight matrix and
bias vector. It is important to highlight that only the first column of Z, specifically the first
token, is used.
6Convergence Analysis of Flow Matching
Based on the definitions provided, we configure the transformer networks as follows:
(cid:26)
T (N,h,d ,d ,d ,B,J,γ) = ϕ : Rd → Rd′ : ϕ in the form of (1),sup∥ϕ(x)∥ ≤ B,
d,d′ k v ff
x
∥ϕ(x )−ϕ(x )∥ ≤ γ∥x −x ∥ for x ,x ∈ [0,1]d,
1 2 1 2 1 2
N h
(cid:88)(cid:88)(cid:0) (cid:1)
∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥
Q,r,s 0 K,r,s 0 V,r,s 0 O,r,s 0
r=1s=1
N
(cid:88)(cid:0) (cid:1)
+ ∥W ∥ +∥b ∥ +∥W ∥ +∥b ∥
r,1 0 r,1 0 r,2 0 r,2 0
r=1
(cid:27)
+∥A ∥ +∥b ∥ +∥A ∥ +∥b ∥ ≤ J ,
in 0 in 0 out 0 out 0
(4)
where ∥·∥ denotes the number of nonzero entries. In the absence of confusion, we write
0
the defined transformer network class as T for brevity. In later sections, we will take
d,d′
networks based on (4) with appropriate configuration parameters.
Remark 1 Compared to the classical transformer architecture (Vaswani et al., 2017; Yun
et al., 2019; Dosovitskiy et al., 2020), our transformer networks have a similar structure,
with differences primarily in the multi-head attention layer. We would like to point out
that our definition of the multi-head attention layer (3) is an equivalent reformulation of
Gurevych et al. (2022) and Kohler and Krzyzak (2023), where they concatenate attention
heads. While there are similarities in framework, our approach and focus differ from the
aforementioned work. We have incorporated control over the regularity of functions, which
is necessary for subsequent analyses in flow matching and autoencoders. Some research
concerns the properties of hard attention, which involves replacing the softmax function in
the standard attention layer with a hardmax (P´erez et al., 2021; Hao et al., 2022). Our
attention layer, in comparison to hard attention, possesses optimization advantages, owing
to the continuity and almost everywhere differentiability of the function x⊙σ (x).
H
To measure the complexity of transformer network class from a learning theory perspec-
tive, we introduce the following notions for a real-valued function class.
Definition 2 (Pseudo-dimension) Let H be a class of real-valued functions defined on
Ω. The pseudo-dimension of H, denoted by Pdim(H), is the largest integer N for which
there exist points x ,...,x ∈ Ω and constants y ,...,y ∈ R such that
1 N 1 N
|{sgn(h(x )−y ),...,sgn(h(x )−y ) : h ∈ H}| = 2N.
1 1 N N
Definition 3 (Covering number) Let ρ be a pseudo-metric on M and S ⊆ M. For any
δ > 0, a set A ⊆ M is called a δ-covering of S if for any x ∈ S there exists y ∈ A such that
ρ(x,y) ≤ δ. The δ-covering number of S, denoted by N(δ,S,ρ), is the minimum cardinality
of any δ-covering of S.
7Jiao, Lai, Wang and Yan
Next, we introduce the notion of regularity for a function. For a multi-index α =
(α ,...,α ), the monomial on x = (x ,...,x ) is denoted by xα := xα1···xα d, the α-
1 d 1 d 1 d
derivative of a real-valued function ϕ is denoted by ∂αϕ := ∂∥α∥1ϕ/∂xα1···∂xα d with
1 d
∥α∥ = (cid:80)d α as the usual 1-norm for vectors. We use the convention that ∂αϕ := ϕ if
1 i=1 i
∥α∥ = 0.
1
Definition 4 (Lipschitz functions) Let Ω ⊆ Rd and ϕ : Ω → Rd′, the Lipschitz constant
of ϕ is denoted by
∥ϕ(x)−ϕ(y)∥
Lip(ϕ) := sup .
∥x−y∥
x,y∈Ω,x̸=y
Definition 5 (H¨older classes) Let Ω ⊆ Rd and β > 0. A function is said to possess
β-Ho¨lder smoothness if all its partial derivatives up to order ⌊β⌋ exist and are bounded, and
the partial derivatives of order ⌊β⌋ are β −⌊β⌋ Ho¨lder. For d,d′ ∈ N, the Ho¨lder class with
smoothness index β and norm constraint parameter K is then defined as
(cid:40)
Hβ (Ω,K) = f = (f ,...,f )⊤ : Ω → Rd′ ,
d,d′ 1 d′
(cid:41)
(cid:88)
∥∂nf ∥ +
(cid:88)
sup
|∂nf k(x)−∂nf k(y)|
≤ K, k = 1,...,d′ .
k L∞(Ω)
∥x−y∥β−⌊β⌋
x,y∈Ω,x̸=y
n:∥n∥1<β n:∥n∥1=⌊β⌋
Definition 6 (Differentiability classes) Let Ω ⊆ Rd and m ∈ N. For d,d′ ∈ N, the
differentiability class with smoothness index m and norm constraint parameter K is defined
as
(cid:40) (cid:41)
Cm (Ω,K) = f = (f ,...,f )⊤ : Ω → Rd′ , (cid:88) ∥∂nf ∥ ≤ K, k = 1,...,d′ .
d,d′ 1 d′ k L∞(Ω)
n:∥n∥1≤m
2.2 Pre-training
For any measurable functions E : RD → Rd and D : Rd → RD, we minimize the reconstruc-
tion loss w.r.t. the pre-trained data distribution γ
(cid:101)1
(cid:90)
(D∗,E∗) ∈ argmin R(D,E) := ∥(D◦E)(y)−y∥2dγ .
(cid:101)1
D,E measurable Rd
Our analysis on pre-training requires the following assumptions.
Assumption 1 (Bounded support) The pre-trained data distribution γ is supported on
(cid:101)1
[0,1]D.
Assumption 2 (Compressibility) There exist continuously differentiable functions E∗ :
[0,1]D → [0,1]d and D∗ : [0,1]d → RD such that R(D,E) attains its minimum. The
minimum value is denoted by ε := R(D∗,E∗). Furthermore, E∗ ∈ C1 ([0,1]D,K ),D∗ ∈
γ (cid:101)1 D,d E
C1 ([0,1]d,K ).
d,D D
8Convergence Analysis of Flow Matching
Remark 7 The restriction on the range of the image of E∗ in Assumption 2 is not essential.
Since E∗ is a continuous function, there exists a constant R > 0 such that E∗([0,1]D) ⊆
∗ ∗
[−R,R]d. Let E(cid:101) (y) := 21 RE∗(y)+ 1 21
d
and D(cid:101) (y) := D∗(2Ry−R1 d). Then R(D,E) =
∗ ∗ ∗
R(D(cid:101) ,E(cid:101) ) and E(cid:101) ([0,1]D) ⊆ [0,1]d. Therefore, it is permissible to assume, without loss of
generality, E∗([0,1]D) ⊆ [0,1]d and D∗ : [0,1]d → RD.
Assumption 3 (Bounded support) The target distribution γ is supported on [0,1]D.
1
We denote the domain shift between the pre-trained data distribution γ and the target
(cid:101)1
distribution γ in Wasserstein-2 distance as ε := W (γ ,γ ). In our analysis, E∗ and
1 γ (cid:101)1,γ1 2 (cid:101)1 1
D∗ are approximated by neural networks. By constraining the chosen encoder network
E(cid:98) : [0,1]D → [0,1]d, Assumption 3 implies that the latent target distribution π
1
:= E(cid:98)#γ
1
is
supported on [0,1]d.
2.3 Flow matching
Given independent empirical observations of X ∼ π and X ∼ π , we want to find an ODE
0 0 1 1
on time t ∈ [0,1],
dZ = v(Z ,t)dt,
t t
which converts Z from π to Z following π . A line of research points out that the vector
0 0 1 1
field can be found by solving a least square regression problem
m vinL(v) := (cid:90) 1 E X0,X1(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) X 1− √ 1t −t2X 0(cid:19) −v(X t,t)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35) dt (5)
0
√
with X = tX + 1−t2X , where X ∼ π ,X ∼ π , and X is the interpolation between
t 1 0 0 0 1 1 t
X and X . The exact minimum of (5) is achieved by
0 1
(cid:20) (cid:12) (cid:21)
v∗(x,t) = E X0,X1 X 1− √ 1t −t2X 0(cid:12) (cid:12) (cid:12)X t = x . (6)
In practice, the velocity field v∗ is approximated by neural networks. To avoid instability,
we often clip the integral interval [0,1] with T. Namely, we consider the truncated loss
function
m vinL(v) := T1 (cid:90) T E X0,X1(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) X 1− √ 1t −t2X 0(cid:19) −v(X t,t)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35) dt (7)
0
√
with X = tX + 1−t2X .
t 1 0
3 Approximation
This section examines the approximation error involved in estimating the true velocity field.
To begin with, we explore the approximation capabilities of transformer networks with
constrained Lipschitz constants, as detailed in the following theorems.
9Jiao, Lai, Wang and Yan
Theorem 8 Let 0 < ε < 1 and β > 0. For any function f ∈ Hβ ([0,1]d,K), there exists a
d,d′
transformer network ϕ ∈ T (N,h,d ,d ,d ,B,J,γ), where
d,d′ k v ff
(cid:32) (cid:33)
(cid:18) (cid:18) K(cid:19)(cid:19) (cid:18) K(cid:19)d/β
N = O log , h = O , d = 8h, d = O(1), d = O(1)
ff k v
ε ε
(cid:32) (cid:33)
(cid:16) (cid:17)
(cid:18) K(cid:19)d/β (cid:18) K(cid:19)
B = O ∥f∥ , J = O log ,
L∞([0,1]d)
ε ε
such that
∥ϕ(x)−f(x)∥ ≤ ε.
L∞([0,1]d)
Furthermore, if β > 1, we may choose
γ = O(K).
Theorem 9 Let 0 < ε < 1 and m ∈ N. For any function f ∈ Cm ([0,1]d,K), there exists
d,d′
a transformer network ϕ ∈ T (N,h,d ,d ,d ,B,J,γ), where
d,d′ k v ff
(cid:32) (cid:33)
(cid:18) (cid:18) K(cid:19)(cid:19) (cid:18) K(cid:19)d/m
N = O log , h = O , d = 8h, d = O(1), d = O(1)
ff k v
ε ε
(cid:32) (cid:33)
(cid:16) (cid:17)
(cid:18) K(cid:19)d/m (cid:18) K(cid:19)
B = O ∥f∥ , J = O log ,
L∞([0,1]d)
ε ε
such that
∥ϕ(x)−f(x)∥ ≤ ε.
L∞([0,1]d)
Furthermore, if m ≥ 1, we may choose
γ = O(K).
The proof of Theorem 8 and Theorem 9 can be found in Appendix A.3.
Remark 10 Theorem 8 and Theorem 9 provide theoretical guarantees for the approximation
capabilities of transformer networks with constrained Lipschitz constants. To effectively
control the Lipschitz constants of networks in practical applications, various methods are
employed, including spectral normalization (Miyato et al., 2018), batch normalization (Ioffe
and Szegedy, 2015), weight clipping (Arjovsky et al., 2017), gradient clipping (LeCun et al.,
2015), and gradient penalty (Gulrajani et al., 2017).
Remark 11 Theorem 8 and Theorem 9 improve the approximation guarantee in Gurevych
et al. (2022, Theorem 2) with additional Lipschitz continuity characterization. Huang
et al. (2022) introduced control over Lipschitz continuity for ReLU neural networks. Chen
et al. (2020a, Lemma 10) shows that ReLU neural networks can approximate Lipschitz
continuous functions, while the Lipschitz continuity of the network remains independent of
10Convergence Analysis of Flow Matching
the approximation error. Their approach, contingent upon the structure of ReLU networks,
is applicable solely to Lipschitz continuous target functions. We would like to highlight
that we use a distinct proof method, enabling the Lipschitz continuity of the constructed
transformer network to be independent of approximation error, while remaining applicable to
target functions with higher regularity.
We proceed to show that although the spatial input x in (6) can be arbitrary in Rd, by
restricting x to a compact set, the true velocity field v∗ can be effectively approximated. In
our approach, we introduce time t as an extra input dimension to the neural network and
define the rescaled function space as
(cid:26) (cid:18) (cid:19)
1 1
T (N,h,d ,d ,d ,B,J,γ ,γ ,R) = v(x,t) = v (Proj (x)+R1 ), t :
k v ff x t (cid:101)
2R
[−R,R]d d
T
v(x′,t′) ∈ T (N,h,d ,d ,d ,B,J,γ), (8)
(cid:101) d+1,d k v ff
(cid:27)
γ γ
γ = ,γ = ,
x t
2R T
where Proj (x) := argmin ∥y−x∥ denotes the projection operator onto the set Ω. This
Ω y∈Ω
definition ensures that v ∈ T(N,h,d ,d ,d ,B,J,γ ,γ ,R) is Lipschitz continuous over
k v ff x t
Rd×[0,T], i.e.
∥v(x ,t)−v(x ,t)∥ ≤ γ ∥x −x ∥ for any t ∈ [0,T],
1 2 x 1 2
∥v(x,t )−v(x,t )∥ ≤ γ |t −t | for any x ∈ Rd.
1 2 t 1 2
Corollary 12 Suppose Assumption 3 holds. Let 1 < T < 1 and R ≥ 1. Given an
2
approximation error 0 < ε < 1, for any velocity field v∗, we choose the hypothesis class T
with
(cid:32) (cid:33)
(cid:18) (cid:18)
R
(cid:19)(cid:19) (cid:18)
R
(cid:19)d+1
N = O log , h = O , d = 8h,
(1−T)3ε (1−T)3ε ff
(cid:32) (cid:33)
(cid:18)
R
(cid:19) (cid:18)
R
(cid:19)d+1 (cid:18)
R
(cid:19)
d = O(1), d = O(1), B = O , J = O log ,
k v 1−T (1−T)3ε (1−T)3ε
(cid:18) (cid:19) (cid:18) (cid:19)
1 R
γ = O , γ = O .
x (1−T)3 t (1−T)3
Then there exists a v(x,t) ∈ T such that
∥v(x,t)−v∗(x,t)∥ ≤ ε.
L∞([−R,R]d×[0,T])
The proof can be found in Appendix A.4.
4 Generalization and Sampling
In this section, we consider the generalization error of estimating the velocity field and
establish distribution recovery guarantees using the estimated velocity field. We begin with
the following connection between the loss function L(v) and the L2 approximation error
∥v(·,t)−v∗(·,t)∥ .
L2(πt)
11Jiao, Lai, Wang and Yan
Lemma 13 For any velocity field v : Rd×[0,T] → Rd, we have
1 (cid:90) T
L(v)−L(v∗) = ∥v(·,t)−v∗(·,t)∥2 dt.
T L2(πt)
0
The proof can be found in Appendix B.1. According to Lemma 13, minimizing (7) is
equivalent to minimizing the difference between the network and the true velocity field in
L2(π )-norm.
t
In this paper, we choose the standard Gaussian distribution as the prior distribution,
i.e., π = N(0,I ), where d is the dimension of the latent space. Given n independent and
0 d
identically distributed (i.i.d.) samples {x }n from π , and n i.i.d. samples {(t ,x )}n
1,i i=1 1 i 0,i i=1
from Unif[0,T] and π , which are cheap to generate, we denote the dataset as X :=
0
{t ,x ,x }n and consider the empirical risk minimization
i 0,i 1,i i=1
(cid:13) (cid:13)2
v (cid:98) ∈ ar vg ∈m Tin L(cid:98)(v) := n1 (cid:88) i=n 1(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)x 1,i− (cid:113) 1t −i
t2
i
x 0,i−v(cid:18) t ix 1,i+(cid:113) 1−t2 ix 0,i,t i(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
. (9)
Our analysis gives the following generalization bound.
Theorem 14 Suppose Assumption 3 holds. Let 1 < T < 1. For any velocity field v∗,
2
given n i.i.d. samples X = {t ,x ,x }n from Unif[0,T], π and π , we choose T as in
i 0,i 1,i i√=1 0 1
Corollary 12 with ε = n− d+1 3 and R = O( logn). Then it holds
E
X
[L(v (cid:98))−L(v∗)] = E
X
(cid:20) T1 (cid:90) T ∥v (cid:98)(·,t)−v∗(·,t)∥2 L2(πt)dt(cid:21) = O(cid:101)(cid:18) (1−T1 )3d+5n− d+2 3(cid:19) ,
0
where we omit factors in d,logn,log(1−T).
The proof can be found in Appendix B.1. Theorem 14 becomes vacuous when T tends to 1
with fixed sample size n. This is a consequence of the blowup of the velocity field v∗(x,t) as
t tends to 1. Although a smaller early stopping time leads to better generalization error,
stopping the sampling process at an early time results in a bad distribution recovery.
Given the estimated velocity field v, we can generate samples from an approximation of
(cid:98)
the continuous flow ODE starting from the prior distribution:
dX(cid:98)t(x) = v (cid:98)(X(cid:98)t(x),t)dt, X(cid:98)0(x) = x ∼ π 0, 0 ≤ t ≤ T. (10)
In practice, we need to use a discrete-time approximation for the sampling dynamics (10).
Let 0 = t < t < ··· < t = T be the discretization points. We consider the explicit Euler
0 1 N
discretization scheme:
dX(cid:101)t(x) = v (cid:98)(X(cid:101)t k(x),t k)dt, t ∈ [t k,t k+1),
for k = 0,1,...,N −1 and X(cid:101)0(x) = x ∼ π 0. We denote the distribution of X(cid:101)T(x) by π (cid:98)T.
12Convergence Analysis of Flow Matching
Theorem 15 Suppose Assumption 3 holds. Given n samples from latent target distribution
π and the networks as in Corollary 12, we use the estimated velocity field in (9) to generate
1
samples. By choosing the maximal step size max k=0,1...,N−1|t k+1−t k| = O(n− d+1 3) and the
early stopping time T(n) = 1−(logn)−1/6, we achieve
E [W (π ,π )] → 0.
X 2 (cid:98)T 1
The proof can be found in Appendix C.2. Theorem 15 demonstrates the consistency of
flow matching in latent space. The consistency is mainly based on a mild assumption, i.e.
boundedness, which justifies the use of continuous normalizing flows based on flow matching.
5 End-to-end Error Analysis
Data dimension reduction and restoration, serving as key steps in Stable Diffusion (Rombach
et al., 2022), are accomplished by an autoencoder network. An autoencoder is a distinct
neural network architecture designed to learn the low-dimensional features of data (Ballard,
1987; Bourlard and Kamp, 1988; Kramer, 1991; Hinton and Zemel, 1993). The classical
autoencoder typically consists of two subnetworks, an encoder and a decoder. The encoder
processes high-dimensional input data into a low-dimensional latent representation, aiming
to capture the intrinsic structure of the data in a compact form. The decoder then maps
these latent features back to the high-dimensional space, striving to accurately reconstruct
the original input. These processes can be summarized by the following model.
Given m samples Y := {y }m drawn i.i.d. from the pre-trained data distribution γ ,
i i=1 (cid:101)1
we can devise an estimator through empirical risk minimization
m
1 (cid:88)
(D(cid:98),E(cid:98)) ∈ argminR(cid:98)(D,E) := ∥(D◦E)(y )−y ∥2, (11)
m i i
D∈D,E∈E
i=1
where we specify the encoder network architecture as
E = T (N ,h ,d ,d ,d ,B ,J ,γ ) (12)
D,d E E E,k E,v E,ff E E E
and the decoder network architecture as
D = T (N ,h ,d ,d ,d ,B ,J ,γ ). (13)
d,D D D D,k D,v D,ff D D D
Lemma 16 Suppose Assumption 1 and 2 hold. Given m samples from pre-trained data
distribution γ , we choose
(cid:101)1
(cid:16) (cid:17)
N E = O(log(K Em)), h E = O K
EDmDD
+2 , d E,ff = 8h, d E,k = O(1), d E,v = O(1)
(cid:16) (cid:17)
B E = O(K E), J E = O K
EDmDD
+2 log(K Em) , γ E = O(K E)
for the encoder networks in (12) and
(cid:16) (cid:17)
N D = O(log(K Dm)), h D = O K Dd mDd +2 , d D,ff = 8h, d D,k = O(1), d D,v = O(1)
(cid:16) (cid:17)
B D = O(K D), J D = O K Dd mDd +2 log(K Dm) , γ D = O(K D)
13Jiao, Lai, Wang and Yan
for the decoder networks in (13). Then it holds
(cid:16) (cid:17)
E Y[R(D(cid:98),E(cid:98))]−ε γ (cid:101)1 = O m− D1 +2 log5/2m ,
where O hides factors in D,d,K ,K .
E D
The proof can be found in Appendix D. Theorem 16 provides estimation guarantees for
the pre-trained data distribution γ . A line of research (Schonsheck et al., 2019; Liu et al.,
(cid:101)1
2024b,a) relies on the assumption that the data follows x = D ◦ E(x) when analyzing
autoencoders. The Assumption 2 represents a relaxation of the aforementioned assumption,
reverting to the previous case when ε = 0.
γ (cid:101)1
Theorem 17 (Main result) Suppose Assumption 1 - 3 hold. Given m samples from the
pre-trained distribution γ , we use the pre-trained autoencoder in (11). Given n samples from
(cid:101)1
target distribution γ and the networks as in Theorem 14, we use the estimated velocity field
1
in (9) to generate samples. By choosing the maximal step size max |t −t | =
k=0,1...,N−1 k+1 k
O(n− d+1 3) and the early stopping time T(n) = 1−(logn)−1/6, we have
E [W (γ ,γ )] = O(ε +ε )
Y,X 2 (cid:98)T 1 γ (cid:101)1 γ (cid:101)1,γ1
when m,n are sufficiently large. Moreover, if ε = 0 and ε = 0, we have
γ (cid:101)1 γ (cid:101)1,γ1
E [W (γ ,γ )] → 0
Y,X 2 (cid:98)T 1
as m,n → ∞.
The proof can be found in Appendix D. Theorem 17 presents the first convergence analysis
incorporating the transformer architecture and pre-training. Our findings indicate that the
difference in the distribution of generated samples and the target distribution in Wasserstein-
2 distance can be controlled by domain shift and the minimum reconstruction loss achievable
by the encoder-decoder architecture. Furthermore, when domain shift is absent and the
encoder-decoder architecture can perfectly reconstruct the distribution, the distribution of
generated samples converges to the target distribution.
6 Conclusion and Future Work
In this work, we presents a statistical learning theory perspective on continuous normalizing
flows based on flow matching. We demonstrate that a Lipschitz transformer network can
approximate the true velocity field under L∞-norm and provide a sample complexity analysis
for estimating the velocity field. Our analysis has identified the bias introduced by the
encoder-decoder architecture in estimating the target distribution. Furthermore, we prove
that under mild assumptions, the generated distribution based on flow matching converges
to the target data distribution in Wasserstein-2 distance. To the best of our knowledge, we
provide the first end-to-end error analysis that considers the transformer architecture and
pre-training.
14Convergence Analysis of Flow Matching
There are several natural directions for future research. Firstly, our analysis can be
naturally extended to latent conditional SDE/ODE-based diffusion models, as in (Dao et al.,
2023; Peebles and Xie, 2023), where prompts or category information can be incorporated
as conditions into the model. Furthermore, with stronger assumptions, there is hope to
obtain convergence rates that depend only on d, which can help to overcome the curse of
dimensionality. Finally, the existing analysis still cannot fully explain why various designs of
attention layers are effective. This is a challenging task because the nonlinearities introduced
by complex structures are difficult to handle and analyze. Developing a better understanding
of transformer networks is an intriguing direction for future research.
Acknowledgments and Disclosure of Funding
The research of Y. Jiao is supported by the National Nature Science Foundation of China (
No.12371441) and supported by “the Fundamental Research Funds for the Central Universi-
ties” and by the research fund of KLATASDSMOE of China . The research of Y. Wang
is supported by the HK RGC grant 16308518, the HK Innovation Technology Fund Grant
ITS/044/18FX and the Guangdong-Hong Kong-Macao Joint Laboratory for Data Driven
Fluid Dynamics and Engineering Applications (Project 2020B1212030001). We thank the
editor and reviewers for their feedback on our manuscript.
15Jiao, Lai, Wang and Yan
Appendix A. Approximation Error
In this section, we primarily follow the technical proof of the approximation capability of
the transformer network by Gurevych et al. (2022). The fundamental idea is that, similar to
ReLU networks, the transformer network can implement multiplication, and subsequently,
polynomials. Polynomials can sufficiently approximate any continuous function, hence the
constructed transformer network can also approximate continuous functions. In contrast to
Gurevych et al. (2022), we need the network to maintain Lipschitz continuity. Therefore,
we choose polynomials to approximate both the function and its derivative simultaneously.
This ensures that the constructed transformer network and the target function not only
have minimal differences in their functions, but also in their derivatives, which helps control
the Lipschitz constant of the network.
A.1 Input embedding
We follow the construction in Gurevych et al. (2022). For h ∈ N, we define d =
model
h·(d +l+4) and
patch
 
0
 I O  d patch    
Od patch d Opatch,l  1  A(cid:101)in (cid:101)b in
A(cid:101)in =    OO1 l,, dd pp aa tt cc hh OI1 l,l   , (cid:101)b in =   

0 l 1+1    , A in =   A(cid:101). . . in , b in =   (cid:101)b. . . in ,
3,d patch 3,l 0
where O denotes a zero matrix with m rows and n columns, and 0 denotes a zero vector
m,n l
of length l, which satisfies ∥A ∥ +∥b ∥ ≤ h·(d +l +2). Z defined in (2) then
in 0 in 0 patch 0
satisfies
 (s)
x if s ∈ {1,...,d }
  j patch

 1 if s = d +1
 patch

((k−1)(d +l+4)+s)
z patch = δ if s ∈ {d +2,...,d +l+1}
0,j s−d patch−1,j patch patch

 1 if s = d +l+3
 patch


0 if s ∈ {d +l+2,d +l+4}
patch patch
(s)
for k ∈ {1,...,h},j ∈ {1,...,l}, where x denotes the s-th component of the j-th token of
j
X, or in other words, it is the element at the s-th row and j-th column of matrix X. The
attention layer and feedforward layer iteratively compute the representations
Y = F(SA)(Z ), Z = F(FF)(Y )
r r r−1 r r r
for r = 1,...,N.
A.2 Approximation of polynomials with single-head attention
Inthecaseofatransformernetworkwithsingle-headattention, theembeddedinputsequence
is represented by
Z = (z ,...,z ) ∈ Rd model×l,
0 0,1 0,l
16Convergence Analysis of Flow Matching
where d = d +l+4 and
model patch
 (s)
x if s ∈ {1,...,d }
  j patch

 1 if s = d +1
 patch

(s)
z = δ if s ∈ {d +2,...,d +l+1} . (14)
0,j s−d patch−1,j patch patch

 1 if s = d +l+3
 patch


0 if s ∈ {d +l+2,d +l+4}
patch patch
The first lemma shows that single-head attention layer can be used to compute linear
functions in one variable.
Lemma 18 Let x j ∈ Rd patch and b j ∈ R(j = 1,...,l). Let Z ∈ Rd model×l be given by
 (s)
x if s ∈ {1,...,d }
  j patch

 1 if s = d +1
 patch

(s)
z = δ if s ∈ {d +2,...,d +l+1} .
j s−d patch−1,j patch patch

  b j if s = d patch+l+3


0 if s ∈ {d +l+2,d +l+4}
patch patch
Let j ∈ {1,...,l},k ∈ {1,...,d } and u ∈ R be arbitrary. Let
patch
(s)
B > 2· max |x |.
j
s=1,...,d ,j=1,...,l
patch
Then there exist matrices W Q,W K ∈ R2×d model and W V,W O ∈ Rd model×d model, where
∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥ ≤ d +6,
Q 0 K 0 V 0 O 0 model
such that
Y = F(SA)(Z) ∈ Rd model×l
satisfies

(s)
 x 1 if s ∈ {1,...,d patch}


  1 if s = d patch+1


δ if s ∈ {d +2,...,d +l+1}
y(s)
=
s−d patch−1,j patch patch
1 (k)
 x
j
−u if s = d patch+l+2


  b 1 if s = d patch+l+3


0 if s = d +l+4
patch
and
y = z for j ∈ {2,...,l}.
j j
17Jiao, Lai, Wang and Yan
Proof See also Gurevych et al. (2022, Lemma 1). Let W = I ,
O d
model
(cid:18) (cid:19)
0 ... 0 1 0 ... 0
W =
Q 0 ... 0 B 0 ... 0
where all columns are zero except for column number d +2,
patch
(cid:18) (cid:19)
0 ... 0 1 0 ... 0 −u−B 0 ... 0 0 0 ... 0
W =
K 0 ... 0 0 0 ... 0 0 0 ... 0 1 0 ... 0
where all columns are zero except for column number k, column number d +1 and
patch
column number d +1+j, and
patch
 
0 ... 0 0 0 ... 0
. . . . .
 . . . . . 
 . . . . . 
 
 0 ... 0 0 0 ... 0 
 
W V =  0 ... 0 1 0 ... 0 
 
 0 ... 0 0 0 ... 0 
 
. . . . .
 . . . . . 
 . . . . . 
0 ... 0 0 0 ... 0
where all rows and all columns are zero except for row number d +l+2 and column
patch
number d +1. Direct calculations give that
patch
 0 0 ... 0 
. . .
 . . . . . . 
 
(cid:104)(cid:16) (cid:17) (cid:16) (cid:17)(cid:105)  0 0 ... 0 
W (W Z) (W Z)⊤(W Z) ⊙σ (W Z)⊤(W Z) =  ,
O V K Q H K Q  (k) 
 x −u 0 ... 0 
 j 
 0 0 ... 0 
0 0 ... 0
where all rows and all columns are zero except for row number d +l+2 and column
patch
number 1, which completes the proof.
The next lemma shows that single-head attention layer can be used to compute products.
Lemma 19 Let x s ∈ Rd patch and a s,b s ∈ R(s = 1,...,l). Let Z ∈ Rd model×l be given by

(s)
 x
j
if s ∈ {1,...,d patch}


  1 if s = d patch+1


δ if s ∈ {d +2,...,d +l+1}
(s) s−d −1,j patch patch
z = patch .
j
 a j if s = d patch+l+2


  b j if s = d patch+l+3


0 if s = d +l+4
patch
18Convergence Analysis of Flow Matching
Let j ∈ {1,...,l}. Let
B > 2|b |· max |a |.
1 r
r=1,...,l
Then there exist matrices W Q,W K ∈ R2×d model and W V,W O ∈ Rd model×d model, where
∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥ ≤ d +5,
Q 0 K 0 V 0 O 0 model
such that
Y = F(SA)(Z) ∈ Rd model×l
satisfies

(s)
 x
1
if s ∈ {1,...,d patch}


  1 if s = d patch+1


δ if s ∈ {d +2,...,d +l+1}
(s) s−d −1,j patch patch
y = patch
1
 a 1 if s = d patch+l+2


  b 1 if s = d patch+l+3


b ·a +B if s = d +l+4
1 j patch
and
y(i) = z(i) for i ∈ {1,...,d +l+3},s ∈ {1,...,l}.
s s patch
Proof See also Gurevych et al. (2022, Lemma 2). Define W as in the proof of Lemma 18
V
such that all rows and all columns are zero except for row number d +l+4 and column
patch
number d +1. Let W = I ,
patch O d
model
(cid:18) (cid:19)
0 ... 0 0 0 ... 0 1 0 ... 0
W =
Q 0 ... 0 B 0 ... 0 0 0 ... 0
where all columns are zero except for column number d + 2 and column number
patch
d +l+3, and
patch
(cid:18) (cid:19)
0 ... 0 0 0 ... 0 1 0 ... 0
W =
K 0 ... 0 1 0 ... 0 0 0 ... 0
where all columns are zero except for column number d +1+j and column number
patch
d +l+2. Direct calculations give that
patch
 
0 0 ... 0
. . .
(cid:104)(cid:16) (cid:17) (cid:16) (cid:17)(cid:105)  . . . 
W (W Z) (W Z)⊤(W Z) ⊙σ (W Z)⊤(W Z) =  . . . ,
O V K Q H K Q  
 0 0 ... 0 
b ·a +B ∗ ... ∗
1 j
where all rows are zero except for row number d +l+4 and the symbol ∗ indicates the
patch
specific values are not of concern.
The following lemma illustrates that single-head attention layer can be used to compute
squares.
19Jiao, Lai, Wang and Yan
Lemma 20 Let a ∈ R, x s ∈ Rd patch and b s ∈ R(s = 1,...,l). Let Z ∈ Rd model×l be given
by

(s)
 x
j
if s ∈ {1,...,d patch}


  1 if s = d patch+1


δ if s ∈ {d +2,...,d +l+1}
(s) s−d −1,j patch patch
z = patch .
j
 a·δ j,1 if s = d patch+l+2


  b j if s = d patch+l+3


0 if s = d +l+4
patch
Then there exist matrices W Q,W K ∈ R1×d model and W V,W O ∈ Rd model×d model, where
∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥ ≤ d +3,
Q 0 K 0 V 0 O 0 model
such that
Y = F(SA)(Z) ∈ Rd model×l
satisfies

(s)
 x
1
if s ∈ {1,...,d patch}


  1 if s = d patch+1


δ if s ∈ {d +2,...,d +l+1}
(s) s−d −1,j patch patch
y = patch
1
 a if s = d patch+l+2


  b 1 if s = d patch+l+3

 a2 if s = d +l+4
patch
and
y = z for j ∈ {2,...,l}.
j j
Proof Define W as in the proof of Lemma 18 such that all rows and all columns are zero
V
except for row number d +l+4 and column number d +1. Let W = I ,
patch patch O d
model
(cid:0) (cid:1)
W = 0 ... 0 1 0 0
Q
where all columns are zero except for column number d +l+2 and W = W . Direct
patch K Q
calculations give that
 
0 0 ... 0
. . .
(cid:104)(cid:16) (cid:17) (cid:16) (cid:17)(cid:105)  . . . 
W (W Z) (W Z)⊤(W Z) ⊙σ (W Z)⊤(W Z) =  . . . ,
O V K Q H K Q  
 0 0 ... 0 
a2 0 ... 0
where all rows and all columns are zero except for row number d +l+4 and column
patch
number 1, which completes the proof.
20Convergence Analysis of Flow Matching
The following lemma introduces a special token-wise feedforward layer that applies the
function x (cid:55)→ α·(x−B) to the d+l+4-th component of each token and writes the result
into the d+l+3-th component. This layer is usually positioned after the attention layer
defined in Lemma 19.
Lemma 21 Let Y = (y 1,...,y l) ∈ Rd model×l. Let d
ff
≥ 8 and let α,B ∈ R. Then there
exist matrices and vectors
W ∈ Rd ff×d model, b ∈ Rd ff, W ∈ Rd model×d ff, b ∈ Rd model,
1 1 2 2
where
∥W ∥ +∥b ∥ +∥W ∥ +∥b ∥ ≤ 18,
1 0 1 0 2 0 2 0
such that
Z = F(FF)(Y)
satisfies

(i)
 y s if i ∈ {1,...,d patch+l+1}
 (cid:16) (cid:17)
z(i) = α· y(d patch+l+4) −By(d patch+1) if i = d +l+3
s s s patch


0 if i ∈ {d +l+2,d +l+4}
patch patch
for s ∈ {1,...,l}.
Proof See also Gurevych et al. (2022, Lemma 3). We choose b = 0 ,b = 0 ,
1 d 2 d
ff model
 0 ... 0 0 0 ... 0 1 0 0 
 0 ... 0 0 0 ... 0 −1 0 0 
 
 0 ... 0 0 0 ... 0 0 1 0 
 
 0 ... 0 0 0 ... 0 0 −1 0 
W 1 =   0 ... 0 −B 0 ... 0 0 0 1  
 
 0 ... 0 B 0 ... 0 0 0 −1 
 
 0 ... 0 0 0 ... 0 0 0 1 
0 ... 0 0 0 ... 0 0 0 −1
whereallcolumnsexceptcolumnnumberd +1,d +l+2,d +l+3andd +l+4
patch patch patch patch
are zero, and
 
0 0 0 0 0 0 0 0
. . . . . . . .
 . . . . . . . . 
 . . . . . . . . 
 
W =  0 0 0 0 0 0 0 0 
2  
 −1 1 0 0 0 0 0 0 
 
 0 0 −1 1 α −α 0 0 
0 0 0 0 0 0 −1 1
21Jiao, Lai, Wang and Yan
where all rows except row number d +l+2,d +l+3 and d +l+4 are zero.
patch patch patch
Then we have
 
0
.
 . 
 . 
 
 0 
W 2·σ(W 1·y s+b 1)+b 2 =   −σ(cid:16) y(d patch+l+2)(cid:17) +σ(cid:16) −y(d patch+l+2)(cid:17)  ,
 s s 
 
 A 
 (cid:16) (cid:17) (cid:16) (cid:17) 
(d +l+4) (d +l+4)
−σ y patch +σ −y patch
s s
where
(cid:16) (cid:17) (cid:16) (cid:17)
(d +l+3) (d +l+3)
A =−σ y patch +σ −y patch
s s
(cid:16) (cid:17) (cid:16) (cid:17)
(d +l+4) (d +1) (d +1) (d +l+4)
+α·σ y patch −By patch −α·σ By patch −y patch .
s s s s
Using the fact σ(u)−σ(−u) = u for any u ∈ R completes the proof.
Remark 22 It follows from the proof of Lemma 21 that we can modify W and W such
1 2
that

(i)
 y s if i ∈ {1,...,d patch+l+1,d patch+l+2}
 (cid:16) (cid:17)
z(i) = α· y(d patch+l+4) −By(d patch+1) if i = d +l+3
s s s patch


0 if i = d +l+4
patch
for s ∈ {1,...,l}.
Remark 23 It follows from the proof of Lemma 21 that we can modify W and W such
1 2
that
 (i)
y if i ∈ {1,...,d +l+1,d +l+3}
 s patch patch

z(i) = y(d patch+l+4) if i = d +l+2
s s patch

0 if i = d +l+4
patch
for s ∈ {1,...,l}.
Remark 24 It follows from the proof of Lemma 21 that we can modify W and W such
1 2
that
(cid:40)
(i)
y if i ∈ {1,...,d +l+1,d +l+3}
z(i) = s patch patch
s
0 if i ∈ {d +l+2,d +l+4}
patch patch
for s ∈ {1,...,l}.
The following lemma shows that transformer networks with single-head attention can be
used to compute monomials.
22Convergence Analysis of Flow Matching
Lemma 25 Assume X ∈ [0,1]d patch×l. Let M
ε
≥ 2. For any multi-index n ∈ Nd patch×l with
∥n∥ ≤ M , we define
1 ε
(cid:89)l d p (cid:89)atch(cid:16) (s)(cid:17)n
s,k
η (X) = x . (15)
n k
k=1 s=1
Then there exists a transformer network consisting of N pairs of layers, where in each pair
the first layer is a single-head attention layer and the second layer is a token-wise feedforward
layer, and
N ≤ 2ld (log M +1),
patch 2 ε
N N
(cid:88) (cid:88)
(∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥ )+ (∥W ∥ +∥b ∥ +∥W ∥ +∥b ∥ )
Q,r 0 K,r 0 V,r 0 O,r 0 r,1 0 r,1 0 r,2 0 r,2 0
r=1 r=1
≤ (d +l+28)N,
patch
which takes Z as the input defined in (14) and generates Z as the output, where
0 N
(d +l+3)
z patch = η (X).
N,1 n
Proof For each n , we consider its binary representation n = (cid:80)P a 2p, where
s,k s,k p=0 s,k,p
a ∈ {0,1} and P ≤ log (M ). Let
s,k,p 2 ε
 
x x ··· x
1 2 l
 1 1 ··· 1
 
Z 0 =   e 01 e 02 · ·· ·· · e 0l  ,
 
b
1
∗ ··· ∗
0 0 ··· 0
then application of Lemma 18 yields
 . . .
. . .
. . .
 (s) 
Y = x 0 ··· 0,
1  k 
 b 1 ∗ ··· ∗
0 0 ··· 0
where the first d +l+1 rows of Y are identical to Z and we focus on the last three
patch 1 0
rows. We set all components of W ,b ,W and b to zero and the feedforward layer yields
1 1 2 2
 . . .
. . .
. . .
 (s) 
Z = x 0 ··· 0.
1  k 
 b 1 ∗ ··· ∗
0 0 ··· 0
23Jiao, Lai, Wang and Yan
By Lemma 19, we obtain
 . . .
. . .
. . .
 
 x(s) 0 ··· 0
Y 2 =  k .
 b ∗ ··· ∗
 1 
(s)
b x +B ∗ ··· ∗
1 k
By Remark 22, we obtain
 . . .
. . .
. . .
 
 x(s) 0 ··· 0
Z 2 =  k .
 (s) 
b x ∗ ··· ∗
 1 k 
0 0 ··· 0
Let r ∈ {1,...,P}. Assume that we already have
 . . .
. . .
. . .
 (cid:16) (cid:17)2r−1 
 (s) 
 x 0 ··· 0
Z m =   (cid:16) k (cid:17)(cid:80)r−1a 2p  
b · x(s) p=0 s,k,p ∗ ··· ∗
 1 k 
0 0 ··· 0
for some m ∈ N. Then we apply Lemma 20 to obtain
 . . .
. . .
. . .
 (cid:16) (cid:17)2r−1 
 (s) 
x 0 ··· 0
 k 
Y m+1 =   (cid:16) (cid:17)(cid:80)r−1a 2p  .
b · x(s) p=0 s,k,p ∗ ··· ∗
 1 k 
 (cid:16) (cid:17)2r 
(s)
x 0 ··· 0
k
Using Remark 23, we have
 . . .
. . .
. . .
 (cid:16) (cid:17)2r 
 (s) 
x 0 ··· 0
Z m+1 =    (cid:16) (cid:17)k (cid:80)r−1a 2p   .
b · x(s) p=0 s,k,p ∗ ··· ∗
 1 k 
0 0 ··· 0
By consecutively employing Lemma 19 and Remark 22, we obtain
 . . .
. . .
. . .
 (cid:16) (cid:17)2r 
 x(s) 0 ··· 0
 k 
Y m+2 =  
 b
·(cid:16) x(s)(cid:17)(cid:80)r p− =1 0a s,k,p2p
∗ ···
∗ 

 1 k 
 (cid:16) (cid:17)(cid:80)r−1a 2p+2r 
b · x(s) p=0 s,k,p +B 0 ··· 0
1 k
24Convergence Analysis of Flow Matching
and
 . . .
. . .
. . .
 (cid:16) (cid:17)2r 
 (s) 
x 0 ··· 0
Z m+2 =    (cid:16) (cid:17)(cid:80)k r−1a 2p+2r   .
b · x(s) p=0 s,k,p ∗ ··· ∗
 1 k 
0 0 ··· 0
If a = 0, Z meets the next induction hypothesis. Otherwise, if a = 1, Z
s,k,r m+1 s,k,r m+2
becomes the next induction hypothesis.
In the last step, we make the following modifications: if a = 0, we substitute Remark
s,k,P
23 with Remark 24; if a = 1, we replace Remark 22 with Lemma 21. We combine a
s,k,P
total of
(cid:80)P
(a +1) pairs of attention and feedforward layers to achieve
p=0 s,k,p
 . . . . . . . . .  . . . . . . . . .
  0 0 ··· 0    0 0 ··· 0 
Z (cid:80)P p=0(a s,k,p+1) =   b 1·(cid:16) x( ks)(cid:17)(cid:80)P p=0a s,k,p2p ∗ ··· ∗ 

=   b 1·(cid:16) x( ks)(cid:17)n s,k ∗ ··· ∗  .
0 0 ··· 0 0 0 ··· 0
Iterating over all components of n, we obtain a transformer network, which takes Z as the
0
input, as defined in (14), and generates Z as the output, where
N
(d +l+3)
Z patch = η (x)
N,1 n
and
l d patch P l d patch P
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
N = (a +1) ≤ 2 ≤ 2ld (log M +1).
s,k,p patch 2 ε
k=1 s=1 p=0 k=1 s=1 p=0
A.3 Approximation of polynomials with multi-head attention
In this subsection, we generalize the results from the previous subsection to transformer
networkswithmulti-headattention. Thebasicideaistouseeachheadtocomputeamonomial
as in Lemma 25, and employ a linear combination of these monomials to approximate
arbitrary function.
As in input embedding, we represent the input sequence by
Z = (z ,...,z ) ∈ Rd model×l
0 0,1 0,l
where d = h·(d +l+4) and
model patch
 (s)
x if s ∈ {1,...,d }
  j patch

 1 if s = d +1
 patch

((k−1)·(d +l+4)+s)
z patch = δ if s ∈ {d +2,...,d +l+1} (16)
0,j s−d patch−1,j patch patch

 1 if s = d +l+3
 patch


0 if s ∈ {d +l+2,d +l+4}
patch patch
25Jiao, Lai, Wang and Yan
for k ∈ {1,...,h}, j ∈ {1,...,l}.
Lemma 26 Let h ∈ N and let s : {n ∈ Nd patch×l : ∥n∥
1
≤ M ε} → {1,...,h},n (cid:55)→ s(n) be
an injection. Then there exists a transformer network consisting of N pairs of layers, where
in each pair the first layer is a multi-head attention layer with h heads and the second layer
is a token-wise feedforward layer, and
N ≤ 2ld (log M +1), d = 2, d = d +l+4,
patch 2 ε k v patch
(cid:18) (cid:19)
l·d +M
h ≤ patch ε , d = 8h,
l·d ff
patch
N h
(cid:88)(cid:88)(cid:0) (cid:1)
∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥
Q,r,s 0 K,r,s 0 V,r,s 0 O,r,s 0
r=1s=1
N
(cid:88)(cid:0) (cid:1)
+ ∥W ∥ +∥b ∥ +∥W ∥ +∥b ∥ ≤ (d +l+28)·N ·h,
r,1 0 r,1 0 r,2 0 r,2 0 patch
r=1
which gets as input Z defined in (16) and produces as output Z which satisfies
0 N
((s−1)·(d +l+4)+(d +l+3))
z patch patch = η (X).
N,1 n
Proof The result is a straightforward extension of the proof of Lemma 25. To begin with,
we choose
 O 
.
 . 
.
 
 O 
 
W
O,s
=
 W(cid:102)O,s
, W
K,s
=
(cid:16) O,...,O,W(cid:102)K,s,O,...,O(cid:17)
,
 
 O 
 
 . . 
 . 
O
(cid:16) (cid:17) (cid:16) (cid:17)
W
Q,s
= O,...,O,W(cid:102)Q,s,O,...,O , W
V,s
= O,...,O,W(cid:102)V,s,O,...,O ,
where in each matrix all blocks are designated as zero matrices except for the s-th block, and
W(cid:102)K,s,W(cid:102)Q,s ∈ Rd k×(d patch+l+4),W(cid:102)V,s,W(cid:102)O,s ∈ R(d patch+l+4)×(d patch+l+4). We format the input
sequence Z into identical blocks
 
Z
1
Z 2
Z =  . 
 . 
 . 
Z
h
with each Z
s
∈ R(d patch+l+4)×l for s ∈ {1,...,h}. The application of a multi-head attention
layer to Z can be interpreted as the individual application of a single-head attention layer
26Convergence Analysis of Flow Matching
to each Z , that is,
s
 (SA) 
F(cid:101)
1
(Z 1)
.
F(SA)(Z) =   . .  ,
(SA)
F(cid:101)
h
(Z h)
(SA)
where F(cid:101)s (Z s) is a single-head attention layer applied only to Z s. Similarly, we define
 
W(cid:102)1,1 O ... O  (cid:101)b 1,1


O W(cid:102)1,2 ... O 

(cid:101)b 1,2
W 1 =   . . . . . . ... . . .  , b 1 =    . . .   ,
 
O O ... W(cid:102)1,h (cid:101)b
1,h
 
W(cid:102)2,1 O ... O  (cid:101)b 2,1


O W(cid:102)2,2 ... O 

(cid:101)b 2,2
W 2 =   . . . . . . ... . . .  , b 2 =    . . .   ,
 
O O ... W(cid:102)2,h (cid:101)b
2,h
where W(cid:102)1,s ∈ R8×(d patch+l+4),(cid:101)b
1,s
∈ R8,W(cid:102)2,s ∈ R(d patch+l+4)×8,(cid:101)b
1,s
∈ Rd patch+l+4 for s ∈
{1,...,h}, and let
 
Y
1
.
Y =   . .  
Y
h
with each Y
s
∈ R(d patch+l+4)×l. Then
 (FF) 
F(cid:101)
1
(Y 1)
.
F(FF)(Y) =   . .  ,
(FF)
F(cid:101)
h
(Y h)
(FF)
where F(cid:101)s (Y s) is a token-wise feedforward layer that operates only on Y s. Utilizing the
method in Lemma 25, we compute η (X) for each block independently.
n
Proof [Proof of Theorem 8] We first consider the case where f is a real-valued function.
Let β > 0 and f ∈ H dβ ,1([0,1]d patch×l,K). According to Lemma 27, for each M
ε
∈ N, there
exists a polynomial P Mβ εf of degree at most M
ε
such that for all X ∈ [0,1]d patch×l and each
multi-index α with ∥α∥ ≤ min{⌊β⌋,M } we have
1 ε
(cid:12) (cid:16) (cid:17)(cid:12) c K
(cid:12)∂α f(X)−Pβ f(X) (cid:12) ≤ 1 ,
(cid:12) Mε (cid:12) Mβ−∥α∥1
ε
where the constant c is independent of M and K. Since
1 ε
Pβ f(X) = (cid:88) a η (X),
Mε n n
∥n∥1≤Mε
27Jiao, Lai, Wang and Yan
where n ∈ Nd patch×l, a
n
∈ R, and η n(X) is a monomial defined in (15) for each n, we can
implement this linear combination using the output embedding and Lemma 26. Namely, we
define b out = 0 ∈ R and A out ∈ R1×d model a zero matrix except for the ((s−1)·(d patch+l+
4)+(d +l+3))-th position where it takes the value of a for all n with ∥n∥ ≤ M , then
patch n 1 ε
∥A ∥ +∥b ∥ ≤ h
out 0 out 0
and
A ·z +b = Pβ f(X),
out N,1 out Mε
where Z = {z ,...,z } is taken from Lemma 26.
N N,1 N,l
We then consider the case where f is a Rd′-valued function. For function f =
(f ,...,f )⊤ ∈ Hβ ([0,1]d,K),eachcomponentoff isareal-valuedfunction. Aspreviously
1 d′ d,d′
mentioned, we have d = l·d , and we do not differentiate between functions with respect
patch
to either x or X as independent variables. Define
 
Pβ f (x)
Mε 1
Pβ f (x)
P(cid:101)β f(x) =   Mε .2  , (17)
Mε  . . 
 
Pβ f (x)
Mε d′
then for all x ∈ [0,1]d we have
(cid:13) (cid:13) c K
(cid:13)f(x)−P(cid:101)β f(x)(cid:13) ≤ 2
(cid:13) Mε (cid:13) L∞([0,1]d) Mβ
ε
and there exist A out ∈ Rd′×d model and b out ∈ Rd′ such that
∥A ∥ +∥b ∥ ≤ d′·h
out 0 out 0
and
A out·z N,1+b
out
= P(cid:101) Mβ εf(x),
where Z is taken from Lemma 26.
N
We put things together. Let
(cid:38) (cid:39)
(cid:18)
c
K(cid:19)1/β
2
M = , (18)
ε
ε
then
(cid:13) (cid:13)
(cid:13)f(x)−P(cid:101)β f(x)(cid:13) ≤ ε.
(cid:13) Mε (cid:13)
L∞([0,1]d)
28Convergence Analysis of Flow Matching
Combining input embedding, output embedding and Lemma 26, there exists a transformer
network ϕ consisting of N pairs of layers, where
(cid:32)(cid:38) (cid:39)(cid:33)
(cid:18)
c
K(cid:19)1/β (cid:18) (cid:18) K(cid:19)(cid:19)
2
N ≤ 2ld (log M +1) ≤ 4dlog = O log ,
patch 2 ε 2 ε ε
(cid:32) (cid:108) (cid:109) (cid:33) (cid:32) (cid:33)
d+
(cid:0)c2K(cid:1)1/β (cid:18) K(cid:19)d/β
h ≤ ε = O , d = 8h,
ff
d ε
d = 2 = O(1), d = d +l+4 = O(1),
k v patch
N h
(cid:88)(cid:88)(cid:0) (cid:1)
J = ∥W ∥ +∥W ∥ +∥W ∥ +∥W ∥
Q,r,s 0 K,r,s 0 V,r,s 0 O,r,s 0
r=1s=1
N
(cid:88)(cid:0) (cid:1)
+ ∥W ∥ +∥b ∥ +∥W ∥ +∥b ∥
r,1 0 r,1 0 r,2 0 r,2 0
r=1
+∥A ∥ +∥b ∥ +∥A ∥ +∥b ∥
in 0 in 0 out 0 out 0
≤ (d +l+28)·N ·h+(d +l+2)·h+d′·h
patch patch
(cid:32)(cid:38) (cid:39)(cid:33) (cid:32) (cid:108) (cid:109) (cid:33)
≤
(cid:0) 8d2+114d+2+d′(cid:1)
·log
(cid:18)
c
2K(cid:19)1/β
·
d+
(cid:0)c2 εK(cid:1)1/β
2 ε d
(cid:32) (cid:33)
(cid:18) K(cid:19)d/β (cid:18) K(cid:19)
= O log
ε ε
such that
ϕ(x) = P(cid:101)β f(x) (19)
Mε
for all x ∈ [0,1]d. Since
∥ϕ(x)∥ ≤ ∥f(x)∥+∥ϕ(x)−f(x)∥ ≤ sup ∥f(x)∥+ε,
x∈[0,1]d
we may choose B = O(∥f∥ ).
L∞([0,1]d)
Additionally, we examine the Lipschitz constant for ϕ(x). When β > 1, since
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) ∂ (cid:16) (cid:17)(cid:12) (cid:12) ∂ (cid:12) (cid:12) ∂ (cid:16) (cid:17)(cid:12)
(cid:12) Pβ f(x) (cid:12) ≤ (cid:12) (f(x))(cid:12)+(cid:12) Pβ f(x)−f(x) (cid:12)
(cid:12) (cid:12)∂x(s) Mε (cid:12)
(cid:12)
(cid:12) (cid:12)∂x(s) (cid:12)
(cid:12)
(cid:12) (cid:12)∂x(s) Mε (cid:12)
(cid:12)
k k k
c K
2
≤ K +
Mβ−1
ε
≤ (1+c )K,
2
mean value theorem yields
(cid:12) (cid:12)
(cid:12)Pβ f(x)−Pβ f(y)(cid:12) ≤ d(1+c )K∥x−y∥
(cid:12) Mε Mε (cid:12) 2
29Jiao, Lai, Wang and Yan
for all x,y ∈ [0,1]d. Thus, by the definition of P(cid:101)β f(x) in (17), we obtain
Mε
(cid:13) (cid:13)
∥ϕ(x)−ϕ(y)∥ = (cid:13)P(cid:101)β f(x)−P(cid:101)β f(y)(cid:13)
(cid:13) Mε Mε (cid:13)
≤ c K∥x−y∥,
3
which indicates the transformer network ϕ is O(K)-Lipschitz when β > 1.
Proof [Proof of Theorem 9] We only need to modify the proof in Theorem 8 by replacing
Lemma 27 with Lemma 28.
Lemma 27 For each f ∈ Hβ ([0,1]d,K) with β > 0 and positive integer N, there is a
d,1
polynomial p of degree at most N on Rd, such that for each multi-index α with ∥α∥ ≤
N 1
min{⌊β⌋,N} we have
c K
sup |∂α(f −p )| ≤ 4 ,
N
[0,1]d
Nβ−∥α∥1
where c is a positive constant depending only on d,β.
4
Proof Let f be a function of compact support on Rd, of class Cm where 0 ≤ m < ∞, and
let K be a compact subset of Rd which contains the support of f. Then Bagby et al. (2002,
Theorem 1) gives that for each positive integer N there is a polynomial p of degree at most
N
N on Rd with the following property: for each multi-index α with ∥α∥ ≤ min{m,N} we
1
have
(cid:18) (cid:19)
c 1
sup|∂α(f −p )| ≤ 5 ω ,
N f,m
K
Nm−∥α∥1 N
where c is a positive constant depending only on d,m and the diameter of K, and
5
(cid:32) (cid:33)
ω (δ) = sup sup |∂γf(x)−∂γf(y)| .
f,m
∥γ∥1=m ∥x−y∥≤δ
We now consider f ∈ Hβ ([0,1]d,K) with β > 1 and K > 0. The Whitney extension
d,1
theorem provides an extension of f to all of Rd (See Stein (1970, Theorem 4), H¨ormander
(2015, Theorem 2.3.6)). In more detail, there exists a function F of class Hβ (Rd,c K) on
d,1 6
Rd such that for each multi-index α with ∥α∥ ≤ ⌊β⌋ we have ∂αF = ∂αf on [0,1]d, and
1
(cid:88) (cid:88)
|∂αF(x)−∂αF(y)|
sup|∂αF|+ sup ≤ c K,
6
∥x−y∥β−⌊β⌋
α:∥α∥1<β Rd α:∥α∥1=⌊β⌋x, xy ̸=∈ yRd
where c = c (β). We fix a test function Ψ ∈ C∞(Rd) with compact support and Ψ ≡ 1 in
6 6 0
the vicinity of [0,1]d. Since ΨF has compact support and belongs to the class C⌊β⌋, we may
30Convergence Analysis of Flow Matching
apply the aforementioned multivariate simultaneous approximation theorem to ΨF and find
a polynomial p of degree at most N, which satisfies
N
sup |∂α(f −p )| = sup |∂α(ΨF −p )|
N N
[0,1]d [0,1]d
≤ sup|∂α(ΨF −p )|
N (20)
Rd
(cid:18) (cid:19)
c 1
7
≤ ω .
N⌊β⌋−∥α∥1 ΨF,⌊β⌋ N
For each multi-index α with ∥α∥ = ⌊β⌋ and x,y ∈ Rd with ∥x−y∥ ≤ 1, we have
1
|∂α(ΨF)(x)−∂α(ΨF)(y)|
(cid:12) (cid:12)
(cid:12) (cid:18) (cid:19) (cid:12)
=
(cid:12)
(cid:12)
(cid:88) α (cid:16) ∂α−kΨ(x)·∂kF(x)−∂α−kΨ(y)·∂kF(y)(cid:17)(cid:12)
(cid:12)
(cid:12) k (cid:12)
(cid:12)0≤k≤α (cid:12)
(cid:88) (cid:18) α(cid:19)(cid:12) (cid:12)
≤ (cid:12)∂α−kΨ(x)·∂kF(x)−∂α−kΨ(y)·∂kF(y)(cid:12)
k (cid:12) (cid:12)
0≤k≤α
≤ |Ψ(x)|·|∂αF(x)−∂αF(y)|+|Ψ(x)−Ψ(y)|·|∂αF(y)|
(cid:88) (cid:18) α(cid:19)(cid:12) (cid:12)
+ (cid:12)∂α−kΨ(x)·∂kF(x)−∂α−kΨ(y)·∂kF(y)(cid:12)
k (cid:12) (cid:12)
0≤k≤α
k̸=α
≤ |Ψ(x)|·c K∥x−y∥β−⌊β⌋+∥∇Ψ(ξ)∥∥x−y∥·|∂αF(y)|
6
(cid:88) (cid:18) α(cid:19)(cid:13) (cid:16) (cid:17) (cid:13)
+ (cid:13)∇ ∂α−kΨ·∂kF (ξ )(cid:13)∥x−y∥
k (cid:13) k (cid:13)
0≤k≤α
k̸=α
≤ ∥Ψ∥ ·c K∥x−y∥β−⌊β⌋+∥∇Ψ∥ ∥x−y∥·c K
L∞(Rd) 6 L∞(Rd) 6
(cid:88) (cid:18) α(cid:19)(cid:18)(cid:13) (cid:16) (cid:17)(cid:13) (cid:13) (cid:13) (cid:19)
+ (cid:13)∇ ∂α−kΨ (cid:13) +(cid:13)∂α−kΨ(cid:13) c K∥x−y∥
k (cid:13) (cid:13) L∞(Rd) (cid:13) (cid:13) L∞(Rd) 6
0≤k≤α
k̸=α
≤ c (Ψ,c (β),d,β)K ·∥x−y∥β−⌊β⌋,
8 6
where in the last inequality we use 0 < β−⌊β⌋ ≤ 1 and ∥x−y∥ ≤ 1, which indicates
(cid:18) (cid:19)
1 c K
8
ω ≤ . (21)
ΨF,⌊β⌋ N Nβ−⌊β⌋
Combining (20) and (21), we complete the proof for the case when β > 1.
For f ∈ Hβ ([0,1]d,K) with 0 < β ≤ 1, there is a completely analogous proof.
d,1
Lemma 28 (Bagby et al. (2002), Theorem 2) For each f ∈ Cm ([0,1]d,K) with m ∈
d,1
N and positive integer N, there is a polynomial p of degree at most N on Rd, such that for
N
31Jiao, Lai, Wang and Yan
each multi-index α with ∥α∥ ≤ min{m,N} we have
1
c K
sup |∂α(f −p )| ≤ 9 ,
N
[0,1]d
Nm−∥α∥1
where c is a positive constant depending only on d,m.
9
A.4 Approximation of velocity field
Lemma 43, Lemma 44, and Lemma 46 demonstrate that the true velocity field has certain
regularities. As a result, the previous approximation results can be naturally applied to the
approximation of the velocity field.
Proof [Proof of Corollary 12] We define v∗(x,t) := v∗(2Rx−R1,Tt) and restrict the input
(cid:101)
space of v∗ to [0,1]d×[0,1]. According to Lemma 43, Lemma 44 and Lemma 46, we have
(cid:101)
v∗(x,t) ∈ C1 ([0,1]d×[0,1],K) with K = O( R ). It follows from Theorem 9 that for
(cid:101) d+1,d (1−T)3
any ε ∈ (0,1) there exists a transformer network v(x,t) ∈ T (N,h,d ,d ,d ,B,J,γ)
(cid:101) d+1,d k v ff
with configuration
(cid:32) (cid:33)
(cid:18) (cid:18)
R
(cid:19)(cid:19) (cid:18)
R
(cid:19)d+1
N = O log , h = O ,
(1−T)3ε (1−T)3ε
d = 8h, d = O(1), d = O(1),
ff k v
(cid:32) (cid:33)
(cid:18)
R
(cid:19) (cid:18)
R
(cid:19)d+1 (cid:18)
R
(cid:19) (cid:18)
R
(cid:19)
B = O , J = O log , γ = O ,
1−T (1−T)3ε (1−T)3ε (1−T)3
such that
∥v(x,t)−v∗(x,t)∥ ≤ ε,
(cid:101) (cid:101) L∞([0,1]d×[0,1])
which implies
(cid:13) (cid:18) (cid:19) (cid:13)
(cid:13)
(cid:13)v (cid:101)
1
(x+R1),
1
t
−v∗(x,t)(cid:13)
(cid:13) ≤ ε.
(cid:13) 2R T (cid:13)
L∞([−R,R]d×[0,T])
Let v(x,t) := v( 1 (Proj (x)+R1), 1t) for x ∈ Rd, then v(x,t) ∈ T and
(cid:101) 2R [−R,R]d T
∥v(x,t)−v∗(x,t)∥ ≤ ε.
L∞([−R,R]d×[0,T])
By the definition of T as defined in (8), we choose
(cid:18) (cid:19) (cid:18) (cid:19)
γ 1 γ R
γ = = O , γ = = O .
x 2R (1−T)3 t T (1−T)3
Appendix B. Generalization Error
In this section, we provide the proof for the generalization error by estimating the covering
number of the transformer network function class.
32Convergence Analysis of Flow Matching
B.1 Proof of Lemma 13 and Theorem 14
Proof [Proof of Lemma 13] By performing some calculations, we have
E(cid:34)(cid:13)
(cid:13)
(cid:13)X 1− √
t
X 0−v(X
t,t)(cid:13)
(cid:13)
(cid:13)2(cid:35)
(cid:13) 1−t2 (cid:13)
=
E(cid:34)(cid:13)
(cid:13) (cid:13)X 1− √ t X 0−v∗(X t,t)+v∗(X t,t)−v(X
t,t)(cid:13)
(cid:13)
(cid:13)2(cid:35)
(cid:13) 1−t2 (cid:13)
(22)
=
E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)X 1− √ 1t −t2X 0−v∗(X
t,t)(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
+∥v(·,t)−v∗(·,t)∥2
L2(πt)
(cid:20)(cid:28) (cid:29)(cid:21)
t
+2E X − √ X −v∗(X ,t),v∗(X ,t)−v(X ,t) .
1 0 t t t
1−t2
By taking expectation conditioned on X , we have
t
(cid:20)(cid:28) (cid:29)(cid:21)
t
E X − √ X −v∗(X ,t),v∗(X ,t)−v(X ,t)
X0,X1 1
1−t2
0 t t t
(cid:20) (cid:20)(cid:28) (cid:29)(cid:12) (cid:21)(cid:21)
= E Xt E X0,X1 X 1− √ 1t −t2X 0−v∗(X t,t),v∗(X t,t)−v(X t,t) (cid:12) (cid:12) (cid:12)X t
(cid:20)(cid:28) (cid:20) (cid:12) (cid:21) (cid:29)(cid:21)
= E Xt E X0,X1 X 1− √ 1t −t2X 0(cid:12) (cid:12) (cid:12)X t −v∗(X t,t),v∗(X t,t)−v(X t,t)
= E [⟨v∗(X ,t)−v∗(X ,t),v∗(X ,t)−v(X ,t)⟩] = 0.
Xt t t t t
Substituting the aforementioned identity into (22) and integrating over the interval [0,T]
w.r.t. t, we obtain
1 (cid:90) T
L(v) = L(v∗)+ ∥v(·,t)−v∗(·,t)∥2 dt,
T L2(πt)
0
which concludes the proof.
(cid:113)
Proof [Proof of Theorem 14] Let R > 0 be determined later. Let x := t x + 1−t2x .
t,i i 1,i i 0,i
33Jiao, Lai, Wang and Yan
• Error decomposition. We consider the error decomposition in an asymmetric form
L(v)−L(v∗)
(cid:98)
1 (cid:90) T
= ∥v(·,t)−v∗(·,t)∥2 dt
T (cid:98) L2(πt)
0
(cid:104) (cid:105)
=E ∥v(X ,t)−v∗(X ,t)∥2
t,Xt (cid:98) t t
(cid:104) (cid:105)
=E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
t,Xt (cid:98) t t t ∞
(cid:104) (cid:105)
+E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ > R}
t,Xt (cid:98) t t t ∞
(23)
(cid:104) (cid:105)
=E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
t,Xt (cid:98) t t t ∞
n
3 (cid:88)
− ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
(cid:98) t,i i t,i i t,i ∞
n
i=1
n
3 (cid:88)
+ ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
(cid:98) t,i i t,i i t,i ∞
n
i=1
(cid:104) (cid:105)
+E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ > R} .
t,Xt (cid:98) t t t ∞
For simplicity, we denote
(cid:104) (cid:105)
I = E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
t,Xt (cid:98) t t t ∞
n
3 (cid:88)
− ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
(cid:98) t,i i t,i i t,i ∞
n
i=1
n
1 (cid:88)
II = ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
(cid:98) t,i i t,i i t,i ∞
n
i=1
(cid:104) (cid:105)
III = E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ > R} .
t,Xt (cid:98) t t t ∞
• Bound of E [I]. Let H = {h(x ,t) := ∥v(x ,t)−v∗(x ,t)∥21{∥x ∥ ≤ R} : v ∈ T}.
X t t t t ∞
For any h ∈ H, we have
h(x ,t) ≤ 2∥v(x ,t)∥21{∥x ∥ ≤ R}+2∥v∗(x ,t)∥21{∥x ∥ ≤ R}
t t t ∞ t t ∞
2d(1+R)2
≤ 2B2+ ,
(1−T2)2
where in the second equality we use Lemma 43, and subsequently
(cid:34) n n (cid:35)
2 (cid:88) 1 (cid:88)
E [I] ≤ E sup2E[h]−E[h]− h(x ,t )− h(x ,t )
X X t,i i t,i i
n n
h∈H
i=1 i=1

n
1 2 (cid:88)
≤ E X  hs ∈u Hp2E[h]−
2B2+
2d(1+R)2E[h2]−
n
h(x t,i,t i)
(1−T2)2 i=1
34Convergence Analysis of Flow Matching

n
1 (cid:88)
−
(cid:16) (cid:17)
h2(x t,i,t i).
2B2+ 2d(1+R)2 n
(1−T2)2 i=1
We can then use the symmetrization technique to bound it by Rademacher complexity. We
(cid:110) (cid:111)n
introduce a ghost dataset X′ = t′,x′ ,x′ drawn i.i.d. from Unif[0,T],π and π , and
i 0,i 1,i 0 1
i=1
let τ = {τ }n be a sequence of i.i.d. Rademacher variables independent of both X and X′.
i i=1
Then,

n
1 2 (cid:88)
E X  hs ∈u Hp2E[h]−
2B2+
2d(1+R)2E[h2]−
n
h(x t,i,t i)
(1−T2)2 i=1

n
1 (cid:88)
−
(cid:16) (cid:17)
h2(x t,i,t i)
2B2+ 2d(1+R)2 n
(1−T2)2 i=1
  
n n
=E X  hs ∈u HpE X′ n2 (cid:88) h(cid:0) x′ t,i,t′ i(cid:1) − (cid:16)
2B2+
21 d(1+R)2(cid:17)
n
(cid:88) h2(cid:0) x′ t,i,t′ i(cid:1) 
i=1 (1−T2)2 i=1

n n
2 (cid:88) 1 (cid:88)
− h(x t,i,t i)−
(cid:16) (cid:17)
h2(x t,i,t i)
n 2B2+ 2d(1+R)2 n
i=1 (1−T2)2 i=1

n n n
≤E X,X′ hs ∈u Hp n2 (cid:88) h(cid:0) x′ t,i,t′ i(cid:1) − n2 (cid:88) h(x t,i,t i)− (cid:16)
2B2+
21 d(1+R)2(cid:17)
n
(cid:88) h2(cid:0) x′ t,i,t′ i(cid:1)
i=1 i=1 (1−T2)2 i=1

n
1 (cid:88)
−
(cid:16) (cid:17)
h2(x t,i,t i)
2B2+ 2d(1+R)2 n
(1−T2)2 i=1
(cid:34) n
=E sup 2 (cid:88) τ (cid:0) h(cid:0) x′ ,t′(cid:1) −h(x ,t )(cid:1)
X,X′,τ n i t,i i t,i i
h∈H
i=1

n
− (cid:16)
2B2+
21 d(1+R)2(cid:17)
n
(cid:88)(cid:0) h2(cid:0) x′ t,i,t′ i(cid:1) +h2(x t,i,t i)(cid:1) 
(1−T2)2 i=1
  
n n
≤E X,X′,τ  hs ∈u Hp n2 (cid:88) τ ih(cid:0) x′ t,i,t′ i(cid:1) − (cid:16)
2B2+
21 d(1+R)2(cid:17)
n
(cid:88) h2(cid:0) x′ t,i,t′ i(cid:1) 
i=1 (1−T2)2 i=1
 
n n
2 (cid:88) 1 (cid:88)
+sup (−τ i)h(x t,i,t i)−
(cid:16) (cid:17)
h2(x t,i,t i)
h∈H n 2B2+ 2d(1+R)2 n
i=1 (1−T2)2 i=1
 
n n
4 (cid:88) 1 (cid:88)
=E
X,τ
sup τ ih(x t,i,t i)−
(cid:16) (cid:17)
h2(x t,i,t i),
h∈H n B2+ d(1+R)2 n
i=1 (1−T2)2 i=1
35Jiao, Lai, Wang and Yan
where the second equality is due to the fact that randomly interchange of the corresponding
components of X and X′ doesn’t affect the joint distribution of X,X′ and the summation
(cid:80)n (h2(x′ ,t′)+h2(x ,t )), and the last equality is because (x ,t ) and (x′ ,t′) have
i=1 t,i i t,i i t,i i t,i i
the same distribution and the τ and −τ have the same distribution.
i i
For any fixed X, we discretize H with respect to the metric
n
d X,1(cid:0) h,h′(cid:1) := 1 (cid:88)(cid:12) (cid:12)h(x t,i,t i)−h′(x t,i,t i)(cid:12) (cid:12).
n
i=1
Let H (X) be a δ-cover of H with minimal cardinality under the distance d , then for any
δ X,1
h ∈ H, there exists g ∈ H (X) such that 1 (cid:80)n |h(x ,t )−g(x ,t )| ≤ δ. Therefore,
δ n i=1 t,i i t,i i
n n n
1 (cid:88) 1 (cid:88) 1 (cid:88)
τ h(x ,t ) ≤ τ g(x ,t )+ |τ ||h(x ,t )−g(x ,t )|
i t,i i i t,i i i t,i i t,i i
n n n
i=1 i=1 i=1
n
1 (cid:88)
≤ τ g(x ,t )+δ
i t,i i
n
i=1
and since |h(x ,t )|,|g(x ,t )| ≤ 2B2+ 2d(1+R)2 ,
t,i i t,i i (1−T2)2
n n n
1 (cid:88) 1 (cid:88) 1 (cid:88)
h2(x ,t ) = g2(x ,t )+ (h(x ,t )+g(x ,t ))(h(x ,t )−g(x ,t ))
t,i i t,i i t,i i t,i i t,i i t,i i
n n n
i=1 i=1 i=1
1
(cid:88)n (cid:18) 2d(1+R)2(cid:19)
1
(cid:88)n
≥ g2(x ,t )−2 2B2+ |h(x ,t )−g(x ,t )|
n t,i i (1−T2)2 n t,i i t,i i
i=1 i=1
1
(cid:88)n (cid:18) 2d(1+R)2(cid:19)
≥ g2(x ,t )−2 2B2+ δ.
n t,i i (1−T2)2
i=1
It follows that
 
n n
4 (cid:88) 1 (cid:88)
E
X
[I] ≤ E
X,τ
sup τ ih(x t,i,t i)−
(cid:16) (cid:17)
h2(x t,i,t i)
h∈H n B2+ d(1+R)2 n
i=1 (1−T2)2 i=1
 
n n
1 (cid:88) 1 (cid:88)
≤ 4E X,τ  sup τ ig(x t,i,t i)− (cid:16) (cid:17) g2(x t,i,t i)+8δ.
g∈H (X) n 4B2+ 4d(1+R)2 n
δ i=1 (1−T2)2 i=1
36Convergence Analysis of Flow Matching
For fixed X and any λ > 0, we have
  
n n
1 (cid:88) 1 (cid:88)
expλE τ  sup τ ig(x t,i,t i)− (cid:16) (cid:17) g2(x t,i,t i)
g∈H (X) n 4B2+ 4d(1+R)2 n
δ i=1 (1−T2)2 i=1
  
n n
1 (cid:88) 1 (cid:88)
≤E
τ
expλ sup τ ig(x t,i,t i)−
(cid:16) (cid:17)
g2(x t,i,t i)
g∈H (X) n 4B2+ 4d(1+R)2 n
δ i=1 (1−T2)2 i=1
  
n n
(cid:88) λ (cid:88) λ (cid:88)
≤ E
τ
exp τ ig(x t,i,t i)−
(cid:16) (cid:17)
g2(x t,i,t i)
n 4B2+ 4d(1+R)2 n
g∈H δ(X) i=1 (1−T2)2 i=1
  
n
(cid:88) (cid:89) λ λ
= E τiexp nτ ig(x t,i,t i)−
(cid:16)
4B2+
4d(1+R)2(cid:17)
ng2(x t,i,t i)
g∈H δ(X)i=1 (1−T2)2
 
(cid:88) (cid:89)n λ2 λ
≤ exp 2n2g2(x t,i,t i)−
(cid:16)
4B2+
4d(1+R)2(cid:17)
ng2(x t,i,t i),
g∈H δ(X)i=1 (1−T2)2
where we use Lemma 31 in the last inequality. If we take λ = n/(2B2+ 2d(1+R)2 ), then
(1−T2)2
 
n n
1 (cid:88) 1 (cid:88)
E τ  sup τ ig(x t,i,t i)− (cid:16) (cid:17) g2(x t,i,t i)
g∈H (X) n 4B2+ 4d(1+R)2 n
δ i=1 (1−T2)2 i=1
1 2
(cid:18) d(1+R)2(cid:19)
≤ log|H (X)| = B2+ logN (δ,H,d ).
λ δ n (1−T2)2 X,1
As a consequence,
8
(cid:18) d(1+R)2(cid:19)
E [I] ≤ B2+ E logN (δ,H,d )+8δ. (24)
X n (1−T2)2 X X,1
• Bound of E [II]. For any v ∈ T, we have
X
(cid:104) (cid:105)
E
X
L(cid:98)(v)−L(cid:98)(v∗)
(cid:34) n
1 (cid:88)
=E ∥v(x ,t )−v∗(x ,t )∥2
X t,i i t,i i
n
i=1
n (cid:42) (cid:43)(cid:35)
+ 2 (cid:88) x − t i x −v∗(x ,t ),v∗(x ,t )−v(x ,t )
1,i (cid:113) 0,i t,i i t,i i t,i i
n
1−t2
i=1 i
(cid:34) n (cid:35)
1 (cid:88)
=E ∥v(x ,t )−v∗(x ,t )∥2 ,
X t,i i t,i i
n
i=1
whereinthelastequalityweusetheidentityfromLemma13. Recallthatv
(cid:98)
∈ argmin
v∈T
L(cid:98)(v),
we have
(cid:34) n (cid:35)
1 (cid:88)(cid:16) (cid:17)
E ∥v(x ,t )−v∗(x ,t )∥2−∥v(x ,t )−v∗(x ,t )∥2
X (cid:98) t,i i t,i i t,i i t,i i
n
i=1
37Jiao, Lai, Wang and Yan
(cid:104) (cid:105)
= E
X
L(cid:98)(v (cid:98))−L(cid:98)(v) ≤ 0,
and subsequently
E [II]
X
(cid:34) n (cid:35)
1 (cid:88)
= E ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
X (cid:98) t,i i t,i i t,i ∞
n
i=1
(cid:34) n (cid:35)
1 (cid:88)
= E ∥v(x ,t )−v∗(x ,t )∥21{∥x ∥ ≤ R}
X t,i i t,i i t,i ∞
n
i=1
(cid:34) n (cid:35)
1 (cid:88)(cid:16) (cid:17)
+E ∥v(x ,t )−v∗(x ,t )∥2−∥v(x ,t )−v∗(x ,t )∥2 1{∥x ∥ ≤ R}
X (cid:98) t,i i t,i i t,i i t,i i t,i ∞
n
i=1
(cid:104) (cid:105)
≤ E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
t,Xt t t t ∞
(cid:34) n (cid:35)
1 (cid:88)(cid:16) (cid:17)
+E ∥v(x ,t )−v∗(x ,t )∥2−∥v(x ,t )−v∗(x ,t )∥2 1{∥x ∥ > R} .
X t,i i t,i i (cid:98) t,i i t,i i t,i ∞
n
i=1
(25)
On the one hand, for any v ∈ T, we have
∥v(x ,t )−v∗(x ,t )∥2
t,i i t,i i
≤ 2∥v(x ,t )∥2+2∥v∗(x ,t )∥2
t,i i t,i i
4d 4t2
≤ 2B2+ + i ∥x ∥2
(1−t2)2 (1−t2)2 t,i
i i
= 2B2+ (1−4d
t2)2
+ (1−4t2 i
t2)2
(cid:13) (cid:13) (cid:13) (cid:13)t ix 1,i+(cid:113) 1−t2 ix 0,i(cid:13) (cid:13) (cid:13) (cid:13)2
(26)
i i
4d 4t2 (cid:16) (cid:17)
≤ 2B2+ + i 2t2∥x ∥2+2(1−t2)∥x ∥2
(1−t2)2 (1−t2)2 i 1,i i 0,i
i i
4d 4 (cid:16) (cid:17)
≤ 2B2+ + 2d+2∥x ∥2
(1−T2)2 (1−T2)2 0,i
12d 8
= 2B2+ + ∥x ∥2.
(1−T2)2 (1−T2)2 0,i
(cid:113)
On the other hand, since ∥x ∥ ≤ t ∥x ∥ + 1−t2∥x ∥ ≤ 1+∥x ∥ , we have
t,i ∞ i 1,i ∞ i 0,i ∞ 0,i ∞
{∥x ∥ > R} ⊆ {∥x ∥ > R−1}. (27)
t,i ∞ 0,i ∞
38Convergence Analysis of Flow Matching
(k)
Denote the k-coordinate of x by x , we have the following upper bound for the tail
0,i 0,i
probability:
(cid:18) (cid:12) (cid:12) (cid:19)
P(cid:0) ∥x ∥ > R−1(cid:1) = P max (cid:12)x(k)(cid:12) > R−1
0,i ∞ (cid:12) 0,i(cid:12)
k=1,...,d
(cid:32) d (cid:33)
= P
(cid:91) (cid:110)(cid:12) (cid:12)x(k)(cid:12)
(cid:12) >
R−1(cid:111)
(cid:12) 0,i(cid:12)
k=1
(28)
d
≤
(cid:88) P(cid:16)(cid:12) (cid:12)x(k)(cid:12)
(cid:12) >
R−1(cid:17)
(cid:12) 0,i(cid:12)
k=1
(cid:18) (R−1)2(cid:19)
≤ 2dexp − .
2
Combining (26), (27), (28) and the Cauchy-Schwartz inequality, we obtain
(cid:34) n (cid:35)
1 (cid:88)(cid:16) (cid:17)
E ∥v(x ,t )−v∗(x ,t )∥2−∥v(x ,t )−v∗(x ,t )∥2 1{∥x ∥ > R}
X t,i i t,i i (cid:98) t,i i t,i i t,i ∞
n
i=1
(cid:34) n (cid:18) (cid:19) (cid:35)
1 (cid:88) 24d 16
≤E 4B2+ + ∥x ∥2 1{∥x ∥ > R−1}
X n (1−T2)2 (1−T2)2 0,i 0,i ∞
i=1
n (cid:18) (cid:19)
=1 (cid:88) 4B2+ 24d E (cid:2) 1{∥x ∥ > R−1}(cid:3)
n (1−T2)2 x0,i 0,i ∞
i=1
n
1 (cid:88) 16 (cid:104) (cid:105)
+ E ∥x ∥21{∥x ∥ > R−1}
n (1−T2)2 x0,i 0,i 0,i ∞
i=1
n (cid:18) (cid:19)
≤1 (cid:88) 4B2+ 24d P(cid:0) ∥x ∥ > R−1(cid:1)
n (1−T2)2 0,i ∞
i=1
n
+ 1 (cid:88) 16 E (cid:104) ∥x ∥4(cid:105)1/2 P(cid:0) ∥x ∥ > R−1(cid:1)1/2
n (1−T2)2 x0,i 0,i 0,i ∞
i=1
(cid:32) (cid:33) (cid:32) (cid:33)
(cid:18) 48d2 (cid:19) (R−1)2 48d2 (R−1)2
≤ 8dB2+ exp − + exp −
(1−T2)2 2 (1−T2)2 4
(cid:32) (cid:33)
(cid:18) 96d2 (cid:19) (R−1)2
≤ 8dB2+ exp − .
(1−T2)2 4
Substituting into (25) and taking the infimum over all v ∈ T, we have
(cid:104) (cid:105)
E [II] ≤ inf E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
X
v∈T
t,Xt t t t ∞
(cid:18) 96d2 (cid:19) (cid:32) (R−1)2(cid:33) (29)
+ 8dB2+ exp − .
(1−T2)2 4
39Jiao, Lai, Wang and Yan
• Bound of III. Similarly, we have
(cid:104) (cid:105)
III = E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ > R}
t,Xt (cid:98) t t t ∞
(cid:20)(cid:18) (cid:19) (cid:21)
12d 8
≤ E 2B2+ + ∥X ∥2 1{∥X ∥ > R−1}
X0 (1−T2)2 (1−T2)2 0 0 ∞
(cid:18) (cid:19)
12d
≤ 2B2+ P(∥X ∥ > R−1)
(1−T2)2 0 ∞
(30)
8
+ E [∥X ∥4]1/2P(∥X ∥ > R−1)1/2
(1−T2)2 X0 0 0 ∞
(cid:18) 24d2 (cid:19) (cid:18) (R−1)2(cid:19) 24d2 (cid:18) (R−1)2(cid:19)
≤ 4dB2+ exp − + exp −
(1−T2)2 2 (1−T2)2 4
(cid:18) 48d2 (cid:19) (cid:18) (R−1)2(cid:19)
≤ 4dB2+ exp − .
(1−T2)2 4
Combining Corollary 12, (24), (29), (30) and Lemma 29, we get
E [L(v)−L(v∗)]
X (cid:98)
= E [I]+3E [II]+III
X X
8
(cid:18) d(1+R)2(cid:19)
≤ B2+ E logN (δ,H,d )+8δ
n (1−T2)2 X X,1
(cid:104) (cid:105)
+3 inf E ∥v(X ,t)−v∗(X ,t)∥21{∥X ∥ ≤ R}
v∈T
t,Xt t t t ∞
(cid:32) (cid:33)
(cid:18) 336d2 (cid:19) (R−1)2
+ 28dB2+ exp −
(1−T2)2 4
(cid:32) 1 (cid:18) (1+R)2(cid:19) n(B2+ (1+R)B )
= O B2+ N2Jlog(max{N,h,d ,d ,d })log 1−T
n (1−T)2 k v ff δ
(cid:32) (cid:33)(cid:33)
(cid:18) 1 (cid:19) (R−1)2
+δ+ε2+ B2+ exp − .
(1−T)2 4
• Balancing error terms. Based on our choice of T in Corollary 12, setting ε =
(cid:113)
n− d+1 3,δ = ε2 and R = 8 logn+1 gives rise to
d+3
(cid:18) (cid:19)
E X [L(v (cid:98))−L(v∗)] = O(cid:101) (1−T1 )3d+5n− d+2 3 ,
where we omit factors in d,logn,log(1−T).
B.2 Covering number evaluation
n(B2+(1+R)B)
Lemma 29 sup logN (δ,H,d ) = O(N2Jlog(max{N,h,d ,d ,d })log 1−T ),
X X,1 k v ff δ
where d (h,h′) = 1 (cid:80)n |h(x ,t )−h′(x ,t )|.
X,1 n i=1 t,i i t,i i
40Convergence Analysis of Flow Matching
Proof Denote the i-coordinate of v by v(i) and define T(i) := {v(i) : v ∈ T}. Let δ > 0.
Denote by {v(i) }N(i) the centers of a minimal δ/(2d(d+1)B+ 4d3/2(1+R) )-covering of T(i)
k k=1 1−T2
with respect to the metric
n
(cid:16) (cid:17) 1 (cid:88)(cid:12) (cid:12)
d v(i),v(i)′ = (cid:12)v(i)(x ,t )−v(i)′(x ,t )(cid:12).
X,1 n (cid:12) t,i i t,i i (cid:12)
i=1
Triangle inequality gives that for each v(i) there exists a v(i) ∈ T(i) such that {v(i) }N(i)
k (cid:101)k (cid:101)k k=1
is an interior δ/(d(d + 1)B + 2d3/2(1+R) )-cover of T(i). For any h ∈ H with h(x ,t) =
1−T2 t
∥v(x ,t)−v∗(x ,t)∥21{∥x ∥ ≤ R}, where v ∈ T, by the cover property of each component,
t t t ∞
there exists a k ∈ {1,...,N(i)} such that d (v(i),v(i) ) ≤ δ/(d(d+1)B+ 2d3/2(1+R) ). We
i X,1 (cid:101)ki 1−T2
(1) (d)
denote h as determined by v := (v ,...,v ). Then we have
k k (cid:101)k1 (cid:101)k
d
d (h,h )
X,1 k
n
1 (cid:88)
= |h(x ,t )−h (x ,t )|
t,i i k t,i i
n
i=1
n
1 (cid:88)
= |⟨v(x ,t )+v (x ,t )−2v∗(x ,t ),v(x ,t )−v (x ,t )⟩|1{∥x ∥ ≤ R}
t,i i k t,i i t,i i t,i i k t,i i t,i ∞
n
i=1
n
1 (cid:88)
≤ ∥v(x ,t )+v (x ,t )−2v∗(x ,t )∥·∥v(x ,t )−v (x ,t )∥1{∥x ∥ ≤ R}
t,i i k t,i i t,i i t,i i k t,i i t,i ∞
n
i=1
(cid:32) √ (cid:33) n
2 d(1+R) 1 (cid:88)
≤ (d+1)B+ ∥v(x ,t )−v (x ,t )∥
1−T2 n t,i i k t,i i
i=1
(cid:32) √ (cid:33) n
2 d(1+R) 1 (cid:88)
≤ (d+1)B+ ∥v(x ,t )−v (x ,t )∥
1−T2 n t,i i k t,i i 1
i=1
(cid:32) √ (cid:33) d
= (d+1)B+
2 d(1+R) (cid:88)
d
(cid:16) v(i),v(i)(cid:17)
1−T2 X,1 (cid:101)ki
i=1
≤ δ,
where in the first inequality we use the Cauchy-Schwartz inequality, in the third inequality
we use ∥·∥ ≤ ∥·∥ , and in the third equality we rearrange the order of summation. It
1
follows that {h } forms a δ-covering of H. Hence,
k
 
d
(cid:89) δ
N (δ,H,d X,1) ≤ N  ,T(i),d X,1.
4d3/2(1+R)
2d(d+1)B+
i=1 1−T2
Using the fact that T(i) ⊆ T (N,h,d ,d ,d ,B,J,γ) and d (f,f′) ≤ d (f,f′) for
d+1,1 k v ff X,1 X,∞
any f,f′, we obtain
logN (δ,H,d )
X,1
41Jiao, Lai, Wang and Yan
 
d
(cid:88) δ
≤ logN  ,T(i),d X,1
4d3/2(1+R)
2d(d+1)B+
i=1 1−T2
 
d
(cid:88) δ
≤ logN  ,T d+1,1(N,h,d k,d v,d ff,B,J,γ),d X,∞
4d3/2(1+R)
2d(d+1)B+
i=1 1−T2
(cid:16) (cid:17)
n B2+ (1+R)B
1−T
≤ c N2Jlog(max{N,h,d ,d ,d })log ,
10 k v ff
δ
where in the last inequality we use Lemma 30.
Lemma 30 LetX = {x ,...,x }. Thensup logN (δ,T (N,h,d ,d ,d ,B,J,γ),d ) =
1 n X d,1 k v ff X,∞
O(N2Jlog(max{N,h,d ,d ,d })log Bn), whered (ϕ,ϕ′) = max |ϕ(x )−ϕ′(x )|.
k v ff δ X,∞ i=1,...,n i i
Proof For simplicity, let T denote T (N,h,d ,d ,d ,B,J,γ). We initially establish
d,1 d,1 k v ff
an upper bound on the pseudo-dimension of subsets of T , specifically focusing on those
d,1
subsets where the nonzero components are fixed in position. Subsequently, leveraging known
results enables us to control the covering number via the pseudo-dimension. The proof is a
modification of the proof of Lemma 8 in Gurevych et al. (2022) and Theorem 6 in Bartlett
et al. (2019).
Based on the definition of T , in all parameters determining a function v ∈ T , only
d,1 d,1
at most J components are permitted to be nonzero. We fix the positions of these nonzero
parameters and denote by θ the vector in RJ comprising all potential nonzero parameter
values. Then define
(cid:110) (cid:111)
V = v(·,θ) : Rd → R : θ ∈ RJ .
To estimate the pseudo-dimension of V, denoted as Pdim(V), we consider a set of points
(x ,y ),...,(x ,y ) ∈ Rd×R that satisfy
1 1 m m
|{(sgn(v(x ,θ)−y ),...,sgn(v(x ,θ)−y )) : v ∈ V}| = 2m.
1 1 m m
It suffices to bound m.
We view the parameters as the variables for any network v ∈ V when input is fixed. In
the following, we construct a sequence of partitions P ,P ,...,P of RJ by successive
0 1 N+1
refinement such that in the last partition for all S ∈ P we have
N+1
v(x ,θ),...,v(x ,θ)
1 m
are polynomials as functions of θ of degree at most 9N+2 for θ ∈ S. Then application of
Lemma 32 and Lemma 33 yields an upper bound for m.
To begin with, set P = {RJ}. As defined in (2), all components of Z are polynomials
0 0
as functions of θ of degree at most 1 ≤ 9 in θ for θ ∈ RJ. Let r ∈ {1,...,N} and assume
that for all S ∈ P all components in Z are polynomials as functions of θ of degree at
r−1 r−1
most 9r in θ for θ ∈ S. Then all components in
q := W z and k := W z
r−1,s,i Q,r,s r−1,i r−1,s,i K,r,s r−1,i
42Convergence Analysis of Flow Matching
are polynomials of degree at most 9r +1 on each set S ∈ P . Consequently, for S ∈ P ,
r−1 r−1
< q ,k >
r−1,s,i r−1,s,j
is a polynomial of degree at most 2·9r +2 for θ ∈ S. Application of Lemma 32 yields that
< q ,k > − < q ,k > (s ∈ {1,...,h},i,j ,j ∈ {1,...,l})
r−1,s,i r−1,s,j1 r−1,s,i r−1,s,j2 1 2
has at most
(cid:18) 2ehl3(2·9r +2)(cid:19)J
∆ = 2
1
J
difference sign patterns. If we partition in each set in P according to these sign patterns
r−1
in ∆ subsets such that all these polynomials have the same signs within each refined region,
1
then on each set in the new partition all elements in
(cid:16) (cid:17) (cid:16) (cid:17)
(W z )⊤(W z ) ⊙σ (W z )⊤(W z )
K,r,s r−1 Q,r,s r−1 H K,r,s r−1 Q,r,s r−1
are fixed polynomials of degree at most 2·9r +2, indicating that all components in
Y = F(SA)(Z )
r r−1
are polynomials of degree at most 3·9r +4 on each refined region. On each set within the
new partition every component of
W Y +b 1⊤
r,1 r r,1 l
is a polynomial of degree at most 3·9r +5. By applying Lemma 32 once again, we can
refine each set in this partition into
(cid:18) 2ed (3·9r +5)(cid:19)J
ff
∆ = 2
2
J
sets such that all components in W Y +b 1⊤ have the same sign patterns within the
r,1 r r,1 l
refined partition. We refer to the partition obtained by refining P twice as P . Since
r−1 r
on each set of P the sign of all components does not change, we can conclude that all
r
components in
(cid:16) (cid:17)
σ W Y +b 1⊤
r,1 r r,1 l
are either equal to zero or equal to a polynomial of degree at most 3·9r +5. Consequently
we have on each set in P all components of
r
Z = F(FF)(Y )
r r
are equal to a polynomial of degree at most 3·9r +6 ≤ 9r+1.
Proceeding in this way we obtain a partition P of RJ such that on each set S ∈ P all
N N
components of
Z
N
43Jiao, Lai, Wang and Yan
are fixed polynomials of θ ∈ S of degree no more than 9N+1, and hence for all k ∈ {1,...,m}
v(x ,θ)−y
k k
are polynomials of degree at most 9N+1+1 ≤ 9N+2 in θ for θ ∈ S.
According to the refinement process, we have
(cid:89)N |P r| (cid:89)N (cid:18) 2ehl3(2·9r +2)(cid:19)J (cid:18) 2ed
ff
(3·9r +5)(cid:19)J
|P | = ≤ 2 ·2 .
N
|P | J J
r−1
r=1 r=1
Using that
|{(sgn(v(x ,θ)−y ),...,sgn(v(x ,θ)−y )) : v ∈ V}|
1 1 m m
(cid:88)
≤ |{(sgn(v(x ,θ)−y ),...,sgn(v(x ,θ)−y )) : θ ∈ S}|,
1 1 m m
S∈PN
we apply Lemma 32 and obtain
2m = |{(sgn(v(x ,θ)−y ),...,sgn(v(x ,θ)−y )) : v ∈ V}|
1 1 m m
(cid:18) 2em9N+2(cid:19)J
≤ |P |·2
N
J
(cid:18) 2em9N+2(cid:19)J (cid:89)N (cid:18) 2ehl3(2·9r +2)(cid:19)J (cid:18) 2ed
ff
(3·9r +5)(cid:19)J
≤ 2 · 2 ·2
J J J
r=1
(cid:32) m6emax(cid:8) hl3,d (cid:9) 9N+2(cid:33)(2N+1)J
≤ 22N+1 ff .
(2N +1)J
Assume m ≥ (2N +1)J. Due to Lemma 33, we have
m ≤ (2N +1)+(2N +1)Jlog (cid:0) 12emax(cid:8) hl3,d (cid:9) 9N+2log (cid:0) 6emax(cid:8) hl3,d (cid:9) 9N+2(cid:1)(cid:1)
2 ff 2 ff
≤ c N2Jlog(max{h,d }),
11 ff
which implies
Pdim(V) ≤ c N2Jlog(max{h,d }).
11 ff
For any fixed X = {y }n , since {(v(y ),...,v(y )) : v ∈ V} ⊆ {x ∈ Rn : ∥x∥ ≤ B}
i i=1 1 n ∞
can be covered by at most ⌈2B⌉n balls with radius δ in ∥·∥ distance, we always have
δ ∞
N(δ,V,d ) ≤ ⌈2B⌉n. By Theorem 12.2 in Anthony et al. (1999), the covering number
X,∞ δ
N(δ,V,d ) can be bounded by the pseudo-dimension Pdim(V) through
X,∞
(cid:18)
2eBn
(cid:19)Pdim(V)
N(δ,V,d ) ≤
X,∞
δPdim(V)
if n ≥ Pdim(V). In any cases,
2eBn
logN(δ,V,d ) ≤ Pdim(V)log .
X,∞
δ
44Convergence Analysis of Flow Matching
The functions in the function set T depend on at most c Nh2(max{d ,d ,d })3
d,1 12 k v ff
many parameters, and of these parameters at most J are allowed to be nonzero. It follows
that the number of possible ways to select these positions is given by
(cid:18) c Nh2(max{d ,d ,d })3 (cid:19) (cid:16) (cid:17)J
12 k v ff ≤ c Nh2(max{d ,d ,d })3 .
J 12 k v ff
Fixing these positions delineates a specific function space V. From this we can conclude
logN(δ,T ,d )
d,1 X,∞
(cid:16) (cid:17) 2eBn
≤ Jlog c Nh2(max{d ,d ,d })3 +c N2Jlog(max{h,d })log
12 k v ff 11 ff
δ
Bn
≤ c N2Jlog(max{N,h,d ,d ,d })log .
13 k v ff
δ
Taking the supremum over X completes the proof.
B.3 Auxiliary lemma
Lemma 31 Given a Rademacher random variable σ takes the values {−1,1} equiprobably.
We have, for any λ ∈ R,E [eλσ] ≤ eλ2/2.
σ
Proof By taking expectations and using the power-series expansion for the exponential, we
obtain
1 1 (cid:32) (cid:88)∞ (−λ)k (cid:88)∞ (λ)k(cid:33)
E [eλσ] = (e−λ+eλ) = +
σ
2 2 k! k!
k=0 k=0
(cid:88)∞ λ2k
=
(2k)!
k=0
(cid:88)∞ λ2k
≤ 1+
2kk!
k=1
=
eλ2/2.
It concludes the proof.
Lemma 32 (Anthony et al. (1999), Theorem 8.3) SupposeW ≤ mandletf ,...,f
1 m
be polynomials of degree at most D in W variables. Define
K :=
(cid:12) (cid:12)(cid:8)
(sgn(f 1(x)),...,sgn(f m(x))) : x ∈
RW(cid:9)(cid:12)
(cid:12).
Then
(cid:18) 2emD(cid:19)W
K ≤ 2 .
W
Lemma 33 (Bartlett et al. (2019), Lemma 16) Suppose that 2m ≤ 2L(mR/w)w for
some R ≥ 16 and m ≥ w ≥ L ≥ 0. Then,
m ≤ L+wlog (2Rlog R).
2 2
45Jiao, Lai, Wang and Yan
Appendix C. Discretization Analysis
C.1 Estimation Error
Consider the target continuous flow
dX (x) = v∗(X (x),t)dt, X (x) = x ∼ π , 0 ≤ t ≤ T, (31)
t t 0 0
and the estimated continuous flow
dX(cid:98)t(x) = v (cid:98)(X(cid:98)t(x),t)dt, X(cid:98)0(x) = x ∼ π 0, 0 ≤ t ≤ T. (32)
Denote the distribution of X t(x) and X(cid:98)t(x) by π
t
and π (cid:98)t, respectively. We have the following
estimate of the Wasserstein-2 distance W (π ,π ).
2 T (cid:98)T
Proposition 34 Suppose Assumption 3 holds. Given n samples X = {x ,x ,t }n from
1,i 0,i i i=1
π , π and Unif[0,T], we choose neural network as in Theorem 14. Then
1 0
(cid:16) (cid:17)
E X [W 2(π T,π (cid:98)T)] = O(cid:101) eγx(1−T)−3d 2+5 n− d+1 3 .
Proof SinceX t(x)andX(cid:98)t(x)formacouplingofπ
t
andπ (cid:98)t,bythedefinitionofWasserstein-2
distance, we have
(cid:90) (cid:13) (cid:13)2
W 22(π t,π (cid:98)t) ≤ (cid:13) (cid:13)X t(x)−X(cid:98)t(x)(cid:13)
(cid:13)
π 0(x)dx,
Rd
where X
t
is the flow map solution of (31) with the exact v∗ defined in (6) and X(cid:98)t is the flow
map solution of (32). Now, we consider the evolution of
(cid:90) (cid:13) (cid:13)2
R
t
:= (cid:13) (cid:13)X t(x)−X(cid:98)t(x)(cid:13)
(cid:13)
π 0(x)dx.
Rd
Differentiating on both sides, we get
dR (cid:90) (cid:68) (cid:69)
t = 2 v∗(X t(x),t)−v (cid:98)(X(cid:98)t(x),t),X t(x)−X(cid:98)t(x) π 0(x)dx
dt
Rd
(cid:90) (cid:68) (cid:69)
= 2 v∗(X t(x),t)−v (cid:98)(X t(x),t)+v (cid:98)(X t(x),t)−v (cid:98)(X(cid:98)t(x),t),X t(x)−X(cid:98)t(x) π 0(x)dx.
Rd
(33)
Using the inequality 2⟨a,b⟩ ≤ ∥a∥2+∥b∥2, we have
(cid:68) (cid:69)
2 v∗(X t(x),t)−v (cid:98)(X t(x),t),X t(x)−X(cid:98)t(x)
(34)
≤ ∥v∗(X t(x),t)−v (cid:98)(X t(x),t)∥2+∥X t(x)−X(cid:98)t(x)∥2.
Since v ∈ T defined in Theorem 14 is γ -Lipschitz continuous w.r.t. x, the Cauchy-Schwartz
(cid:98) x
inequality implies
(cid:68) (cid:69)
2 v (cid:98)(X t(x),t)−v (cid:98)(X(cid:98)t(x),t),X t(x)−X(cid:98)t(x) ≤ 2γ x∥X t(x)−X(cid:98)t(x)∥2. (35)
46Convergence Analysis of Flow Matching
Combining (33), (34) and (35), we obtain
(cid:90)
dR
t ≤ (1+2γ )R + ∥v∗(X (x),t)−v(X (x),t)∥2π (x)dx.
x t t (cid:98) t 0
dt
Rd
Therefore, by Lemma 37 and since R = 0, we deduce
0
(cid:90) T (cid:90)
R ≤ e1+2γx ∥v∗(X (x),t)−v(X (x),t)∥2π (x)dxdt
T t (cid:98) t 0
0 Rd
(cid:90) T
= e1+2γx ∥v∗(·,t)−v(·,t)∥2 dt.
(cid:98) L2(πt)
0
By Theorem 14 and Jensen’s inequality, we get the desired result.
C.2 Discretization Error
Now we consider the gap between estimated continuous flow and its discretization:
dX(cid:98)t(x) = v (cid:98)(X(cid:98)t(x),t)dt, X(cid:98)0(x) = x ∼ π 0, 0 ≤ t ≤ T,
dX(cid:101)t(x) = v (cid:98)(X(cid:101)t k(x),t k)dt, t
k
≤ t ≤ t k+1, k = 0,1,...,N −1, X(cid:101)0(x) = x ∼ π 0.
Denote the distribution of X(cid:98)t(x) and X(cid:101)t(x) by π
(cid:98)t
and π (cid:98)t, respectively.
Lemma 35 Let 0 = t < t < ··· < t = T be the discretization points. For any neural
0 1 N
network v in T defined in (8), we have
(cid:98)
 (cid:118) 
(cid:117)N−1
(cid:117)(cid:88)
W 2(π (cid:98)T,π (cid:98)T) = Oeγx(γ xB+γ t)(cid:116) (t k+1−t k)3 .
k=0
Proof By the same argument as in the proof of Proposition 34, we have
(cid:90) (cid:13) (cid:13)2
W 22(π (cid:98)t,π (cid:98)t) ≤ (cid:13) (cid:13)X(cid:98)t(x)−X(cid:101)t(x)(cid:13)
(cid:13)
π 0(x)dx.
Rd
Now, we consider the evolution of
(cid:90) (cid:13) (cid:13)2
L
t
:= (cid:13) (cid:13)X(cid:98)t(x)−X(cid:101)t(x)(cid:13)
(cid:13)
π 0(x)dx.
Rd
Since X(cid:101)t(x) is piece-wise linear, we consider the evolution of L
t
on each split interval
[t ,t ]. On interval [t ,t ], we have
k k+1 k k+1
dL (cid:90) (cid:68) (cid:69)
t
dt
= 2 v (cid:98)(X(cid:98)t(x),t)−v (cid:98)(X(cid:101)t k(x),t k),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx
Rd
(cid:90) (cid:68) (cid:69)
= 2 v (cid:98)(X(cid:98)t(x),t)−v (cid:98)(X(cid:101)t(x),t),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx (36)
Rd
47Jiao, Lai, Wang and Yan
(cid:90) (cid:68) (cid:69)
+ 2 v (cid:98)(X(cid:101)t(x),t)−v (cid:98)(X(cid:101)t k(x),t),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx (37)
Rd
(cid:90) (cid:68) (cid:69)
+ 2 v (cid:98)(X(cid:101)t k(x),t)−v (cid:98)(X(cid:101)t k(x),t k),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx. (38)
Rd
For (36), by Cauchy-Schwartz inequality and the fact that v is γ -Lipschitz continuous w.r.t.
(cid:98) x
x, we get
(cid:90) (cid:68) (cid:69)
2 v (cid:98)(X(cid:98)t(x),t)−v (cid:98)(X(cid:101)t(x),t),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx
Rd
(39)
(cid:90) (cid:13) (cid:13)2
≤ 2γ
x
(cid:13) (cid:13)X(cid:98)t(x)−X(cid:101)t(x)(cid:13)
(cid:13)
π 0(x)dx.
Rd
For (37), note that X(cid:101)t(x) = X(cid:101)t k(x)+(t−t k)v (cid:98)(X(cid:101)t k(x),t k), we use the inequality 2⟨a,b⟩ ≤
∥a∥2+∥b∥2 and the fact that v is γ -Lipschitz continuous w.r.t. x to get
(cid:98) x
(cid:90) (cid:68) (cid:69)
2 v (cid:98)(X(cid:101)t(x),t)−v (cid:98)(X(cid:101)t k(x),t),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx
Rd
(cid:90) (cid:13) (cid:13)2 (cid:90) (cid:13) (cid:13)2
≤ (cid:13) (cid:13)v (cid:98)(X(cid:101)t(x),t)−v (cid:98)(X(cid:101)t k(x),t)(cid:13)
(cid:13)
π 0(x)dx+ (cid:13) (cid:13)X(cid:98)t(x)−X(cid:101)t(x)(cid:13)
(cid:13)
π 0(x)dx
(40)
Rd Rd
≤γ2 (t−t )2∥v∥2 +L
x k (cid:98) L∞ t
≤γ2 (t−t )2B2+L ,
x k t
where B is the parameter of the neural networks in (8). For (38), the fact that v is
(cid:98)
γ -Lipschitz continuous w.r.t. t implies
t
(cid:90) (cid:68) (cid:69)
2 v (cid:98)(X(cid:101)t k(x),t)−v (cid:98)(X(cid:101)t k(x),t k),X(cid:98)t(x)−X(cid:101)t(x) π 0(x)dx
Rd
(41)
(cid:90) (cid:13) (cid:13)2
≤ (cid:13) (cid:13)X(cid:98)t(x)−X(cid:101)t(x)(cid:13)
(cid:13)
π 0(x)dx+γ t2(t−t k)2.
Rd
Combining (39), (40) and (41), we obtain
dL t ≤ (2γ +2)L +(cid:0) γ2B2+γ2(cid:1) (t−t )2, on [t ,t ].
dt x t x t k k k+1
Again, by Lemma 37, we obtain
e−(2γx+2)t k+1L −e−(2γx+2)t kL ≤ 1 (cid:0) γ2B2+γ2(cid:1) (t −t )3.
t k+1 t k 3 x t k+1 k
Summing over k and noting that t = T, we get
N
N−1
L ≤ 1 e2(γx+1)T (cid:0) γ2B2+γ2(cid:1) (cid:88) (t −t )3.
T 3 x t k+1 k
k=0
Thus, we have
 (cid:118) 
(cid:117)N−1
(cid:117)(cid:88)
W 2(π (cid:98)T,π (cid:98)T) = Oeγx(γ xB+γ t)(cid:116) (t k+1−t k)3 .
k=0
48Convergence Analysis of Flow Matching
Lemma 36 Suppose Assumption 3 holds, we have
W (π ,π ) = O(1−T).
2 T 1
Proof We consider the error from early stopping. Note that X and X form a coupling of
T 1
π and π , by the definition of Wasserstein-2 distance, we obtain
T 1
W (π ,π ) ≤ E[∥X −X ∥2]1/2
2 T 1 T 1
= E[∥(1−T2)1/2X −(1−T)X ∥2]1/2
0 1
≤ (cid:0) 2(1−T2)E[∥X ∥2]+2(1−T)2E[∥X ∥2](cid:1)1/2 .
0 1
Sinceweassumeπ issupportedon[0,1]d andE[∥X ∥2] = d,wehaveW (π ,π ) = O(1−T).
1 0 2 T 1
Proof [Proof of Theorem 15] Combining Proposition 34, Lemma 35, Lemma 36 and triangle
inequality, we obtain
 (cid:118) 
(cid:117)N−1
E X[W 2(π (cid:98)T,π 1)] = O(cid:101)(1−T)+eγx(γ xB+γ t)(cid:117) (cid:116)(cid:88) (t k+1−t k)3+eγx(1−T)−3d 2+5 n− d+1 3.
k=0
√
By the choice of neural network in Theorem 14, we have γ ≤ c14 ,γ ≤ c15 logn. Letting
x (1−T)3 t (1−T)3
max k=0,1...,N−1|t k+1−t k| = O(n− d+1 3),T(n) = 1−(logn)−1/6 and omitting polynomials of
logarithm, we obtain
(cid:16) √ (cid:17)
E X[W 2(π (cid:98)T,π 1)] = O(cid:101) (logn)−1/6+ec14 lognn− d+1 3 ,
which tends to 0 as n goes to infinity.
C.3 Auxiliary lemma
Lemma 37 (Gr¨onwall’s inequality) Given a function f(t) defined on [a,b](a < b), sat-
df(t)
isfying ≤ αf(t)+g(t) on [a,b] and α ≥ 0, we have
dt
(cid:90) b
f(b) ≤ eα(b−a)f(a)+ eα(b−t)g(t)dt.
a
Proof By multiplying e−αt on both sides of df(t) ≤ αf(t)+g(t) and some manipulation of
dt
algebra, we obtain
df(t)
e−αt −αe−αtf(t) ≤ e−αtg(t).
dt
49Jiao, Lai, Wang and Yan
Integrating on interval [a,b] on both sides, we get
(cid:90) b
e−αbf(b)−e−αaf(a) ≤ eα(b−t)g(t)dt.
a
This concludes the proof.
Appendix D. Compressibility Analysis
Definition 38 (Rademacher complexity) The Rademacher complexity of a set A ⊆ RN
is defined as
(cid:34) N (cid:35)
1 (cid:88)
R (A) = E sup σ a ,
N {σi}N
k=1 a∈A N
k k
k=1
where {σ }N are N i.i.d Rademacher variables with P(σ = 1) = P(σ = −1) = 1. The
k k=1 k k 2
Rademacher complexity of function class F associate with random sample {X }N is defined
k k=1
as
(cid:34) N (cid:35)
1 (cid:88)
R (F) = E sup σ u(X ) .
N {X k,σ k}N k=1 u∈F N k k
k=1
Proof [Proof of Lemma 16]
• Step 1: error decomposition.
For each D ∈ D and E ∈ E, we have
θ θ
R(D(cid:98),E(cid:98))−R(D∗,E∗)
= R(D(cid:98),E(cid:98))−R(cid:98)(D(cid:98),E(cid:98))+R(cid:98)(D(cid:98),E(cid:98))−R(cid:98)(D θ,E θ)+R(cid:98)(D θ,E θ)−R(D θ,E θ)
+R(D ,E )−R(D∗,E∗)
θ θ
≤ sup R(D,E)−R(cid:98)(D,E)+ sup R(cid:98)(D,E)−R(D,E)
D∈D,E∈E D∈D,E∈E
+R(D ,E )−R(D∗,E∗),
θ θ
where the inequality is due to the fact that R(cid:98)(D(cid:98),E(cid:98)) ≤ R(cid:98)(D θ,E θ). Then taking infimum
over D ∈ D and E ∈ E yields
θ θ
R(D(cid:98),E(cid:98))−R(D∗,E∗)
≤ sup R(D,E)−R(cid:98)(D,E)+ sup R(cid:98)(D,E)−R(D,E)
(42)
D∈D,E∈E D∈D,E∈E
+ inf R(D,E)−R(D∗,E∗).
D∈D,E∈E
• Step 2: approximation error.
Suppose Assumption 1 and 2 hold. In (12) and (13), we specify the encoder network
architecture as
E = T (N ,h ,d ,d ,d ,B ,J ,γ ) (43)
D,d E E E,k E,v E,ff E E E
50Convergence Analysis of Flow Matching
and the decoder network architecture as
D = T (N ,h ,d ,d ,d ,B ,J ,γ ). (44)
d,D D D D,k D,v D,ff D D D
ByTheorem9,givenany0 < ε < 1,thereexistsanencodernetworkE ∈ E withconfiguration
(cid:32) (cid:33)
(cid:18) (cid:18)
K
(cid:19)(cid:19) (cid:18)
K
(cid:19)D
E E
N = O log , h = O , d = 8h , d = O(K ),
E E E,ff E E,k E
ε ε
(cid:32) (cid:33)
(cid:18)
K
(cid:19)D (cid:18)
K
(cid:19)
E E
d = O(K ), B = O(K ), J = O log , γ = O(K ),
E,v E E E E E E
ε ε
such that
∥E(y)−E∗(y)∥ ≤ ε.
L∞([0,1]D)
Moreover, there exists an decoder network D ∈ D with configuration
(cid:32) (cid:33)
(cid:18) (cid:18)
K
(cid:19)(cid:19) (cid:18)
K
(cid:19)d
D D
N = O log , h = O , d = 8h , d = O(K ),
D D D,ff D D,k D
ε ε
(cid:32) (cid:33)
(cid:18)
K
(cid:19)d (cid:18)
K
(cid:19)
D D
d = O(K ), B = O(K ), J = O log , γ = O(K ),
D,v D D D D D D
ε ε
such that
∥D(y)−D∗(y)∥ ≤ ε.
L∞([0,1]d)
Then we have
∥(D◦E)(y)−y∥2−∥(D∗◦E∗)(y)−y∥2
= ⟨(D◦E)(y)+(D∗◦E∗)(y)−2y,(D◦E)(y)−(D∗◦E∗)(y)⟩
√
≤ (B +K +2 D)∥(D◦E)(y)−(D∗◦E∗)(y)∥
D D
√
≤ (B +K +2 D)(∥(D◦E)(y)−(D∗◦E)(y)∥+∥(D∗◦E)(y)−(D∗◦E∗)(y)∥)
D D
√
≤ (B +K +2 D)(∥D(E(y))−D∗(E(y))∥+K ∥E(y)−E∗(y)∥)
D D D
= O(ε),
which implies
inf R(D,E)−R(D∗,E∗)
D∈D,E∈E
(cid:90)
= inf ∥(D◦E)(y)−y∥2−∥(D∗◦E∗)(y)−y∥2dγ (45)
(cid:101)1
D∈D,E∈E RD
= O(ε).
• Step 3: generalization error.
For simplicity, denote G = {l(y) = ∥(D◦E)(y)−y∥2 : E ∈ E,D ∈ D}. We introduce a
51Jiao, Lai, Wang and Yan
ghost dataset Y′ = {y′}m drawn i.i.d. from γ , and let σ = {σ }m be a sequence of i.i.d.
i i=1 (cid:101)1 i i=1
Rademacher variables independent of both Y and Y′. Then we have
(cid:34) (cid:35)
E
Y
sup R(D,E)−R(cid:98)(D,E)
D∈D,E∈E
(cid:34) m (cid:35)
1 (cid:88)
= E supE[l(y)]− l(y )
Y m i
l∈G
i=1
(cid:34) (cid:34) m (cid:35) m (cid:35)
1 (cid:88) (cid:88)
= E supE l(y′) − l(y )
m Y Y′ i i
l∈G
i=1 i=1
(cid:34) m (cid:35)
≤ 1 E sup(cid:88)(cid:0) l(y′)−l(y )(cid:1)
m Y,Y′ l∈G i i (46)
i=1
(cid:34) m (cid:35)
= 1 E sup(cid:88) σ (cid:0) l(y′)−l(y )(cid:1)
m Y,Y′,σ i i i
l∈G
i=1
(cid:34) m (cid:35) (cid:34) m (cid:35)
1 (cid:88) 1 (cid:88)
= E sup σ l(y′) + E sup (−σ )l(y )
m Y′,σ i i m Y,σ i i
l∈G l∈G
i=1 i=1
(cid:34) m (cid:35)
1 (cid:88)
= 2E sup σ l(y )
Y,σ m i i
l∈G
i=1
= 2R (G),
m
where we use the fact that randomly interchange of the corresponding components of Y and
Y′ doesn’t affect the joint distribution of Y and Y′, y and y′ have the same distribution,
i i
and σ and −σ have the same distribution. Similarly, we have
i i
(cid:34) (cid:35)
E
Y
sup R(cid:98)(D,E)−R(D,E) ≤ 2R m(G). (47)
D∈D,E∈E
By employing the chaining technique, an upper bound on the Rademacher complexity of
a function class can be established via its covering number. What remains is to determine
a bound for the covering number. To establish a bound on N(δ,G,d ), we first define
Y,∞
necessary subsets of the encoder and decoder function spaces. We use the superscript i
to denote the i-th component. Let E(i) := {E(i) : E ∈ E} for i = 1,...,d, and similarly,
let D(i) := {D(i) : D ∈ D} for i = 1,...,D. We then construct an δ-cover of E(i), denoted
as E(i) , where |E(i) | = N(δ,E(i),d ). This means for every E(i) ∈ E(i), there is an E(i)
δ δ Y,∞ δ
in
E(i)
such that the distance d
(E(i),E(i)
) ≤ δ. Using the triangle inequality, we can
δ Y,∞ δ
assert the existence of E(cid:101) δ(i) ∈ E(i) for which d Y,∞(E(i),E(cid:101) δ(i) ) ≤ 2δ. In a similar manner, we
apply the discretization to the decoder part. With a fixed encoder network E, an δ-cover for
D(i) is established, named D(i) , with |D(i) | = N(δ,D(i),d ), where E(Y) denotes the
δ δ E(Y),∞
set {E(y )}m . This setup ensures that for any D(i) ∈ D(i), there exists a corresponding
i i=1
D δ(i) ∈ D δ(i) such that d E(Y),∞(D(i),D δ(i) ) ≤ δ. We can also find a D(cid:101) δ(i) ∈ D(i) that satisfies
d E(Y),∞(D(i),D(cid:101) δ(i) ) ≤ 2δ.
52Convergence Analysis of Flow Matching
For any l ∈ G with l(y) = ∥(D◦E)(y)−y∥2, where E ∈ E,D ∈ D, the discretization
above determines a (cid:101)l with (cid:101)l(y) = ∥(D(cid:101) ◦E(cid:101))(y)−y∥2, where E(cid:101) = (E(cid:101)(1) ,...,E(cid:101)(d) )⊤ and
δ δ
D(cid:101) = (D(cid:101)(1) ,...,D(cid:101)(D) )⊤. Then
δ δ
d Y,∞(l,(cid:101)l)
(cid:12) (cid:12)
= max (cid:12)l(y )−(cid:101)l(y )(cid:12)
(cid:12) k k (cid:12)
k=1,...,m
(cid:12) (cid:12)
= max (cid:12)∥(D◦E)(y )−y ∥2−∥(D(cid:101) ◦E(cid:101))(y )−y ∥2(cid:12)
(cid:12) k k k k (cid:12)
k=1,...,m
(cid:12)(cid:68) (cid:69)(cid:12)
= max (cid:12) (D◦E)(y )+(D(cid:101) ◦E(cid:101))(y )−2y ,(D◦E)(y )−(D(cid:101) ◦E(cid:101))(y ) (cid:12)
(cid:12) k k k k k (cid:12)
k=1,...,m
(cid:13) (cid:13) (cid:13) (cid:13)
≤ max (cid:13)(D◦E)(y )+(D(cid:101) ◦E(cid:101))(y )−2y (cid:13)·(cid:13)(D◦E)(y )−(D(cid:101) ◦E(cid:101))(y )(cid:13)
(cid:13) k k k(cid:13) (cid:13) k k (cid:13)
k=1,...,m
(cid:16) √ (cid:17) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (D+1)B
D
+2 D max (cid:13) (cid:13)(D◦E)(y k)−(D◦E(cid:101))(y k)(cid:13) (cid:13)+(cid:13) (cid:13)(D◦E(cid:101))(y k)−(D(cid:101) ◦E(cid:101))(y k)(cid:13)
(cid:13)
k=1,...,m
(cid:16) √ (cid:17) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (D+1)B
D
+2 D max γ D(cid:13) (cid:13)E(y k)−E(cid:101)(y k)(cid:13) (cid:13)+(cid:13) (cid:13)D(E(cid:101)(y k))−D(cid:101)(E(cid:101)(y k))(cid:13)
(cid:13)
k=1,...,m
(cid:16) √ (cid:17) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (D+1)B
D
+2 D max γ D(cid:13) (cid:13)E(y k)−E(cid:101)(y k)(cid:13)
(cid:13)
+(cid:13) (cid:13)D(E(cid:101)(y k))−D(cid:101)(E(cid:101)(y k))(cid:13)
(cid:13)
k=1,...,m 1 1
≤
(cid:16)
(D+1)B
D
+2√ D(cid:17)(cid:32)
γ
D(cid:88)d
d
Y,∞(cid:16)
E(i),E(cid:101)
δ(i)(cid:17) +(cid:88)D
d
E(cid:101)(Y),∞(cid:16)
D(i),D(cid:101)
δ(i)(cid:17)(cid:33)
i=1 i=1
(cid:16) √ (cid:17)
≤ (D+1)B +2 D (2dγ +2D)δ,
D D
which implies
(cid:16)(cid:16) √ (cid:17) (cid:17)
N (D+1)B +2 D (2dγ +2D)δ,G,d
D D Y,∞
D
(cid:88)(cid:89) (cid:16) (cid:17)
≤ N δ,D(i),d
E(cid:101)(Y),∞
E(cid:101) i=1
D
(cid:88)(cid:89) (cid:16) (cid:17)
≤ maxN δ,D(i),d
E(cid:101)(Y),∞
E(cid:101) i=1
E(cid:101)
d D
(cid:89) (cid:16) (cid:17) (cid:89) (cid:16) (cid:17)
= N δ,E(i),d · maxN δ,D(i),d .
Y,∞ E(cid:101)(Y),∞
i=1 i=1
E(cid:101)
Using that
E(i) ⊆ T (N ,h ,d ,d ,d ,B ,J ,γ ),
D,1 E E E,k E,v E,ff E E E
D(i) ⊆ T (N ,h ,d ,d ,d ,B ,J ,γ ),
d,1 D D D,k D,v D,ff D D D
we get
(cid:16)(cid:16) √ (cid:17) (cid:17)
logN (D+1)B +2 D (2dγ +2D)δ,G,d
D D Y,∞
53Jiao, Lai, Wang and Yan
d D
(cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
≤ logN δ,E(i),d + maxlogN δ,D(i),d
Y,∞ E(cid:101)(Y),∞
i=1 i=1
E(cid:101)
d
(cid:88)
≤ logN (δ,T (N ,h ,d ,d ,d ,B ,J ,γ ),d )
D,1 E E E,k E,v E,ff E E E Y,∞
i=1
D
(cid:88) (cid:16) (cid:17)
+ maxlogN δ,T (N ,h ,d ,d ,d ,B ,J ,γ ),d .
d,1 D D D,k D,v D,ff D D D E(cid:101)(Y),∞
i=1
E(cid:101)
Lemma 30 yields that
(cid:16)(cid:16) √ (cid:17) (cid:17)
logN (D+1)B +2 D (2dγ +2D)δ,G,d
D D Y,∞
(cid:18)
B m
≤ c N2J log(max{N ,h ,d ,d ,d })log E
16 E E E E E,k E,v E,ff δ
(cid:19)
B m
+N2 J log(max{N ,h ,d ,d ,d })log D .
D D D D D,k D,v D,ff δ
Given the chosen parameters, we deduce that
(cid:32) (cid:33)
1
(cid:18) 1(cid:19)4
m
logN (δ,G,d ) = O log log .
Y,∞ εD ε δ
Applying Lemma 40, we obtain
(cid:32) (cid:33)
3 (cid:90) B D2 +D(cid:113)
R (G) ≤ 4 inf δ+ √ logN (γ,G,d )dγ
m Y,∞
0<δ<B D2 +D m δ
≤ 4 inf (cid:18) δ+ √3 (cid:0) B2 +D(cid:1)(cid:113) logN (δ,G,d )(cid:19) (48)
0<δ<B2 +D m D Y,∞
D
(cid:32) (cid:33)
(cid:18) 1(cid:19)2
= O m−1/2ε−D/2 log (logm)1/2 .
ε
• Step 4: balancing error terms.
Combining (45), (46), (47) and (48), we have
(cid:32) (cid:33)
(cid:104) (cid:105)
(cid:18) 1(cid:19)2
E
Y
R(D(cid:98),E(cid:98))−R(D∗,E∗) = O ε+m−1/2ε−D/2 log (logm)1/2 .
ε
Setting ε = m− D1 +2 gives rise to
(cid:104) (cid:105) (cid:16) (cid:17)
E Y R(D(cid:98),E(cid:98))−R(D∗,E∗) = O m− D1 +2 (logm)5/2 .
Proof [Proof of Theorem 17] Given m samples drawn from the pre-training data distribution
γ (cid:101)1,wedetermineD(cid:98),E(cid:98) throughempiricalriskminimization. Givennsamplesdrawnfromthe
54Convergence Analysis of Flow Matching
target distribution γ 1, the encoder E(cid:98) maps these samples to a low-dimensional latent space,
where flow matching and sampling are completed. The decoder E(cid:98) remaps the sampled data
to the high-dimensional space, conforming to distribution γ . In our end-to-end analysis,
(cid:98)T
we have
E [W (γ ,γ )]
X,Y 2 (cid:98)T 1
= E Y[E X[W 2(D(cid:98)#π (cid:98)T,γ 1)]]
≤ E Y[E X[W 2(D(cid:98)#π (cid:98)T,(D(cid:98) ◦E(cid:98)) #γ 1)]]+E Y[W 2((D(cid:98) ◦E(cid:98)) #γ 1,(D(cid:98) ◦E(cid:98)) #γ (cid:101)1)]
+E Y[W 2((D(cid:98) ◦E(cid:98)) #γ (cid:101)1,γ (cid:101)1)]+W 2(γ (cid:101)1,γ 1)
≤ E Y[Lip(D(cid:98))E X[W 2(π (cid:98)T,π 1)]]+E Y[Lip(D(cid:98))Lip(E(cid:98))W 2(γ 1,γ (cid:101)1)]+E Y[R(D(cid:98),E(cid:98))]+W 2(γ (cid:101)1,γ 1)
≤ γ DE Y[E X[W 2(π (cid:98)T,π 1)]]+E Y[R(D(cid:98),E(cid:98))]+(γ Eγ
D
+1)ε
γ (cid:101)1,γ1
= O(ε +ε ),
γ (cid:101)1 γ (cid:101)1,γ1
where in the second inequality we use Lemma 39, and in the last equality we use Lemma 16
and Theorem 15.
D.1 Auxiliary lemma
Lemma 39 Let µ,ν be distributions on Rd and let f : Rd → Rd′ be a Lipschitz continuous
mapping with Lipschitz constant Lip(f) < ∞. Then,
W (f µ,f ν) ≤ Lip(f)W (µ,ν).
2 # # 2
Proof Similar proof can be found in (Perekrestenko et al., 2021, Lemma 9). Let π be a
coupling between µ and ν and let g : Rd×Rd → Rd′ ×Rd′,(y ,y ) (cid:55)→ (f(y ),f(y )). Then
1 2 1 2
g π is a coupling between f µ and f ν and
# # #
(cid:18)(cid:90) (cid:19)1/2
W (f µ,f ν) ≤ ∥y −y ∥2d(g π)(y ,y )
2 # # 1 2 # 1 2
Rd×Rd
(cid:18)(cid:90) (cid:19)1/2
= ∥f (y )−f (y )∥2dπ(y ,y )
1 2 1 2
Rd×Rd
(cid:18)(cid:90) (cid:19)1/2
≤ Lip(f) ∥y −y ∥2dπ(y ,y ) .
1 2 1 2
Rd×Rd
Taking the infimum over all π ∈ Π(µ,ν), we get
W (f µ,f ν) ≤ Lip(f)W (µ,ν).
2 # # 2
Lemma 40 LetF beafunctionclassdefinedonΩandD = {x ,...,x }. Ifsup ∥f∥ ≤
1 n f∈F L∞(Ω)
B, then
(cid:32) (cid:33)
3 (cid:90) B/2(cid:113)
R (F) ≤ 4 inf δ+ √ logN (ε,F,d )dε .
n D,∞
0<δ<B/2 n δ
55Jiao, Lai, Wang and Yan
Proof See the proof of Shalev-Shwartz and Ben-David (2014, Lemma 27.4) and Duan et al.
(2022, Lemma 3.11).
Appendix E. Properties of true velocity field
E.1 Computation of true velocity field
Lemma 41 The true velocity field v∗ can be written as:
1 1
v∗(x,t) = ∇ logπ (x)+ x, (49)
x t
t t
√
where π is the density of X , and X = 1−t2X +tX .
t t t 0 1
Proof By some manipulation of algebra, (6) implies
(cid:20) (cid:12) (cid:21)
v∗(x,t) = E X 1− √ t X 0(cid:12) (cid:12)X
t
= x
1−t2 (cid:12)
(cid:20) (cid:12) (cid:21)
= E X 1−
1−t
t2
(cid:16)(cid:112)
1−t2X 0+tX 1−tX
1(cid:17)(cid:12)
(cid:12) (cid:12)X t = x
1 t
= E[X |X = x]− x
1−t2 1 t 1−t2
1 (cid:90) x 1π t|1(x|x 1)π 1(x 1) t
= dx − x
1−t2 π (x) 1 1−t2
t
(cid:90) x
exp(cid:16) −∥x−tx1∥2(cid:17)
π (x )
1 1 1 2(1−t2) 1 1 t
= dx − x
1−t2 (cid:112) (2π)d(1−t2)d π t(x) 1 1−t2
(cid:90)
(cid:16)
tx1−x + x
(cid:17) exp(cid:16) −∥x−tx1∥2(cid:17)
π (x )
1 1 1−t2 1−t2 2(1−t2) 1 1 t
= dx − x
t (cid:112) (2π)d(1−t2)d π t(x) 1 1−t2
(cid:90) ∇
exp(cid:16) −∥x−tx1∥2(cid:17)
π (x ) (cid:18) (cid:19)
1 1 x 2(1−t2) 1 1 1 t
= dx + − x
t (cid:112) (2π)d(1−t2)d π t(x) 1 t(1−t2) 1−t2
1 1
= ∇ logπ (x)+ x,
x t
t t
where π is the density of X conditioned on X . It concludes the proof.
t|1 t 1
E.2 Computation of partial derivative regarding t
Lemma 42 ∂ v∗(x,t) = − 1+t2 x + 2t E[X |X = x] + 1+t2 Cov[X |X = x]x −
t (1−t2)2 (1−t2)2 1 t (1−t2)3 1 t
t (E[X ∥X ∥2|X = x]−E[X |X = x]E[∥X ∥2|X = x]), where Cov[X |X = x] is the
(1−t2)3 1 1 t 1 t 1 t 1 t
covariance matrix of X conditioned on X = x.
1 t
56Convergence Analysis of Flow Matching
Proof To ease notation, we define ϕ (x) :=
(cid:82)
exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx ), which is the
t 2(1−t2) 1 1
unnormalized version of π (x). Note that ∇logϕ (x) = ∇logπ (x), using the product rule
t t t
of the derivatives, (49) implies:
1 1 1
∂ v∗(x,t) = − ∇ logπ (x)+ ∂ ∇ logπ (x)− x
t t2 x t t t x t t2
(cid:18) (cid:19)
1 1 1 ∇ϕ (x) 1
= − t(1−t2)E[X 1|X t = x]+ t2(1−t2)x+ t∂ t
ϕ
(t
x)
− t2x (50)
t
(cid:18) (cid:19)
1 1 1 ∂ ∇ϕ (x) ∂ ϕ (x)∇ϕ (x)
= x− E[X |X = x]+ t t − t t t
1−t2 t(1−t2) 1 t t ϕ t(x) (ϕ t(x))2
Then we focus on the computation of the last term above. We first compute
∂t∇ϕt(x)
as
ϕt(x)
follows:
∂ ∇ϕ (x)
t t
ϕ (x)
t
(cid:32) (cid:33)
1 (cid:90) tx −x ∥x−tx ∥2
1 1
= ∂ exp − π (dx )
ϕ (x) t 1−t2 2(1−t2) 1 1
t
(cid:32) (cid:32) (cid:33)
1 (cid:90) (1−t2)x +2t(tx −x) ∥x−tx ∥2
1 1 1
= exp −
ϕ (x) (1−t2)2 2(1−t2)
t
(cid:32) (cid:33) (cid:33)
−tx 1−x
exp
−∥x−tx 1∥2 (t∥x 1∥2−x⊤ 1x)(1−t2)+t∥x−tx 1∥2
π (dx )
(51)
1−t2 2(1−t2) (1−t2)2 1 1
1+t2 2t t2
= E[X |X = x]− x− E[X ∥X ∥2|X = x]
(1−t2)2 1 t (1−t2)2 (1−t2)3 1 1 t
t(1+t2) t2
+ E[X X⊤|X = x]x− E[X |X = x]∥x∥2
(1−t2)3 1 1 t (1−t2)3 1 t
t 1+t2 t
+ E[∥X ∥2|X = x]x− E[X⊤x|X = x]x+ ∥x∥2x
(1−t2)3 1 t (1−t2)3 1 t (1−t2)3
By some calculations, we obtain
∂ ϕ (x) 1 (cid:90) (cid:18) t t 1+t2 (cid:19)
t t = − ∥x∥2− ∥x ∥2+ x⊤x
ϕ (x) ϕ (x) (1−t2)2 (1−t2)2 1 (1−t2)2 1
t t
(cid:32) (cid:33)
∥x−tx ∥2
1
·exp − π (dx ) (52)
2(1−t2) 1 1
t 1+t2 t
= − E[∥X ∥2|X = x]+ E[X⊤x|X = x]− ∥x∥2
(1−t2)2 1 t (1−t2)2 1 t (1−t2)2
and
(cid:32) (cid:33)
∇ϕ (x) 1 (cid:90) tx −x ∥x−tx ∥2
t 1 1
= exp − π (dx )
ϕ (x) ϕ (x) 1−t2 2(1−t2) 1 1
t t (53)
x t
= − + E[X |X = x].
1−t2 1−t2 1 t
57Jiao, Lai, Wang and Yan
Combining (50), (51), (52) and (53), we obtain
1+t2 2t 1+t2
∂ v∗(x,t) = − x+ E[X |X = x]+ Cov[X |X = x]x
t (1−t2)2 (1−t2)2 1 t (1−t2)3 1 t
− t (cid:0)E[X ∥X ∥2|X = x]−E[X |X = x]E[∥X ∥2|X = x](cid:1) .
(1−t2)3 1 1 t 1 t 1 t
It concludes the proof.
E.3 An upper bound for velocity field
Lemma 43 Suppose Assumption 3 holds. Then sup sup |v∗(x,t)| ≤ 1+R .
t∈[0,T] x∈[−R,R]d i 1−T2
Proof For the i-coordinate, we have v∗ = 1 E[X(i) |X = x]− t x , where X(i) denotes
i 1−t2 1 t 1−t2 i 1
the i-coordinate of X . Note that π is supported on [0,1]d as stated in Assumption 3, then
1 1
1+R
sup sup |v∗(x,t)| ≤ .
i 1−T2
t∈[0,T]x∈[−R,R]d
E.4 An upper bound of partial derivative regarding t
(cid:16) (cid:17)
Lemma 44 SupposeAssumption3holds. Thensup sup ∥∂ v∗(x,t)∥ = O R .
t∈[0,T] x∈[−R,R]d t (1−T)3
Proof From Lemma 42, we have
1+t2 2t 1+t2
∥∂ v∗(x,t)∥ ≤ ∥x∥+ ∥E[X |X = x]∥+ ∥Cov[X |X = x]∥ ∥x∥
t (1−t2)2 (1−t2)2 1 t (1−t2)3 1 t op
+ t (cid:0) ∥E[X ∥X ∥2|X = x]∥+∥E[X |X = x]∥∥E[∥X ∥2|X = x]∥(cid:1) .
(1−t2)3 1 1 t 1 t 1 t
Notethatπ isassumedtobesupportedon[0,1]d,wehave∥E[X |X = x]∥ ≤ E[∥X ∥2|X =
1 1 t 1 t
x]1/2 ≤ d1/2 and ∥E[X ∥X ∥2|X = x]∥ ≤ E[∥X ∥6|X = x]1/2 ≤ d3/2. To bound
1 1 t 1 t
∥Cov[X |X = x]∥ , we have the following inequality for any u ∈ Rd,
1 t op
u⊤Cov[X |X = x]u = E[u⊤X X⊤u|X = x]−E[u⊤X |X = x]E[X⊤u|X = x]
1 t 1 1 t 1 t 1 t
= E[(u⊤X )2|X = x]−E[u⊤X |X = x]2
1 t 1 t
≤ 2d∥u∥2
Hence we have ∥Cov[X |X = x]∥ ≤ 2d. Using these above inequalities, we have
1 t op
√ √
d(1+T2)R 2 dT 2d3/2(1+T2)R 2d3/2T
sup sup ∥∂ v∗(x,t)∥ ≤ + + + .
t (1−T2)2 (1−T2)2 (1−T2)3 (1−T2)3
t∈[0,T]x∈[−R,R]d
(cid:16) (cid:17)
Since0 < T < 1,theaboveinequalityimpliessup sup ∥∂ v∗(x,t)∥ = O R .
t∈[0,T] x∈[−R,R]d t (1−T)3
58Convergence Analysis of Flow Matching
E.5 Computation of partial derivative regarding spatial variable x
Lemma 45 We have the following identity:
t t
∇v∗(x,t) = Cov[X |X = x]− I .
(1−t2)2 1 t 1−t2 d
Proof By Lemma 41, we have
1 1
∇v∗(x,t) = ∇2logπ (x)+ I .
t d
t t
Further, the Hessian ∇2logπ (x) can be computed as
t
(cid:82)
tx1−x
exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx
)
∇2logπ t(x) =∇ Rd (cid:82)1− et2 xp(cid:16) −∥x−t2 x( 11 ∥− 2t (cid:17)2)
π
(d1
x )
1 
Rd 2(1−t2) 1 1
(cid:82)
(cid:16) tx1−x(cid:17)⊗2 exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx )
1 Rd 1−t2 2(1−t2) 1 1
=− I +
1−t2 d (cid:82) exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx )
Rd 2(1−t2) 1 1
(cid:82)
tx1−x
exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx
)⊗2
Rd 1−t2 2(1−t2) 1 1
−
(cid:82)
exp(cid:16) −∥x−tx1∥2(cid:17)
π (dx )

Rd 2(1−t2) 1 1
1 t2
=− I + Cov[X |X = x].
1−t2 d (1−t2)2 1 t
Combing the above identities, we get the desired result.
Lemma 46 Suppose Assumption 3 holds. Then sup sup ∥∇v∗(x,t)∥ ≤
t∈[0,T] x∈[−R,R]d op
Td .
(1−T2)2
Proof Since we assume the target distribution π is supported on [0,1]d, we have the
1
following evaluation of the covariance matrix
0 ⪯ Cov[X |X = x] ⪯ dI .
1 t d
Thus, we have
(cid:18) (cid:19)
t td t
− I ⪯ ∇v∗(x,t) ⪯ − I .
1−t2 d (1−t2)2 1−t2 d
The above inequality implies the upper bound.
59Jiao, Lai, Wang and Yan
References
MichaelSamuelAlbergoandEricVanden-Eijnden. Buildingnormalizingflowswithstochastic
interpolants. In International Conference on Learning Representations, 2022.
Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. Neural network learning: Theo-
retical foundations, volume 9. cambridge university press Cambridge, 1999.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial
networks. In International Conference on Machine Learning, 2017.
Thomas Bagby, Len Bos, and Norman Levenberg. Multivariate simultaneous approximation.
Constructive approximation, 18(4):569–577, 2002.
Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity
in gans. arXiv preprint arXiv:1806.10586, 2018.
Dana H Ballard. Modular learning in neural networks. In Proceedings of the sixth National
Conference on artificial intelligence-volume 1, pages 279–284, 1987.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight
vc-dimension and pseudodimension bounds for piecewise linear neural networks. The
Journal of Machine Learning Research, 20(1):2285–2301, 2019.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear
convergence bounds for diffusion models via stochastic localization. arXiv preprint
arXiv:2308.03686, 2023.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian,
Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable
video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint
arXiv:2311.15127, 2023a.
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition,
2023b.
Herv´e Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular
value decomposition. Biological cybernetics, 59(4-5):291–294, 1988.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,
Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
Videogenerationmodelsasworldsimulators.2024.URLhttps://openai.com/research/
video-generation-models-as-world-simulators.
Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In
IEEE International Conference on Computer Vision, 2019.
60Convergence Analysis of Flow Matching
Jinyuan Chang, Zhao Ding, Yuling Jiao, Ruoxuan Li, and Jerry Zhijian Yang. Deep
conditionalgenerativelearning: Modelanderroranalysis. arXivpreprintarXiv:2402.01460,
2024.
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and
Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion
models. arXiv preprint arXiv:2401.09047, 2024.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative
modeling: User-friendly bounds under minimal smoothness assumptions. In International
Conference on Machine Learning, 2023a.
Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Distribution approximation
and statistical estimation guarantees of generative adversarial networks. arXiv preprint
arXiv:2002.03938, 2020a.
Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Statistical guarantees of gen-
erative adversarial networks for distribution estimation. arXiv preprint arXiv:2002.03938,
2020b.
Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation,
estimation and distribution recovery of diffusion models on low-dimensional data. In
International Conference on Machine Learning, 2023b.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan.
Wavegrad: Estimating gradients for waveform generation. In International Conference on
Learning Representations, 2020c.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is
as easy as learning the score: theory for diffusion models with minimal data assumptions.
arXiv preprint arXiv:2209.11215, 2022.
GiovanniConforti,AlainDurmus,andMartaGentiloniSilveri. Scorediffusionmodelswithout
early stopping: finite fisher information is all you need. arXiv preprint arXiv:2308.12240,
2023.
Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv
preprint arXiv:2307.08698, 2023.
ValentinDeBortoli. Convergenceofdenoisingdiffusionmodelsunderthemanifoldhypothesis.
arXiv preprint arXiv:2208.05314, 2022.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion
schr¨odinger bridge with applications to score-based generative modeling. Advances in
Neural Information Processing Systems, 2021.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advances in Neural Information Processing Systems, 2021.
61Jiao, Lai, Wang and Yan
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. In
International Conference on Learning Representations, 2020.
Chenguang Duan, Yuling Jiao, Yanming Lai, Dingwei Li, Jerry Zhijian Yang, et al. Conver-
gence rate analysis for deep ritz method. Communications in Computational Physics, 31
(4):1020–1048, 2022.
Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan.
Synthetic data augmentation using gan for improved liver lesion classification. In 2018
IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages 289–293.
IEEE, 2018.
Yuan Gao, Jian Huang, and Yuling Jiao. Gaussian interpolation flows. arXiv preprint
arXiv:2311.11475, 2023.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in
Neural Information Processing Systems, 2014.
IshaanGulrajani,FarukAhmed,MartinArjovsky,VincentDumoulin,andAaronCCourville.
Improvedtrainingofwassersteingans. Advances in Neural Information Processing Systems,
2017.
Iryna Gurevych, Michael Kohler, and G¨ozde Gu¨l S¸ahin. On the rate of convergence of a
classifier based on a transformer encoder. IEEE Transactions on Information Theory, 68
(12):8139–8155, 2022.
YidingHao,DanaAngluin,andRobertFrank. Formallanguagerecognitionbyhardattention
transformers: Perspectives from circuit complexity. Transactions of the Association for
Computational Linguistics, 10:800–810, 2022.
Geoffrey E Hinton and Richard Zemel. Autoencoders, minimum description length and
helmholtz free energy. Advances in Neural Information Processing Systems, 1993.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
Advances in Neural Information Processing Systems, 2020.
Lars H¨ormander. The analysis of linear partial differential operators I: Distribution theory
and Fourier analysis. Springer, 2015.
Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, and Yunfei Yang. An error
analysis of generative adversarial networks for learning distributions. The Journal of
Machine Learning Research, 23(1):5047–5089, 2022.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning,
2015.
62Convergence Analysis of Flow Matching
Yuling Jiao, Guohao Shen, Yuanyuan Lin, and Jian Huang. Deep nonparametric regression
on approximate manifolds: Nonasymptotic error bounds with polynomial prefactors. The
Annals of Statistics, 51(2):691–716, 2023.
TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition,
2019.
Michael Kohler and Adam Krzyzak. On the rate of convergence of an over-parametrized
transformer classifier learned by gradient descent. arXiv preprint arXiv:2312.17007, 2023.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A
versatile diffusion model for audio synthesis. In International Conference on Learning
Representations, 2020.
Mark A Kramer. Nonlinear principal component analysis using autoassociative neural
networks. AIChE journal, 37(2):233–243, 1991.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):
436–444, 2015.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling
with polynomial complexity. Advances in Neural Information Processing Systems, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for
general data distributions. In International Conference on Algorithmic Learning Theory,
pages 946–985. PMLR, 2023.
Hao Liu, Biraj Dahal, Rongjie Lai, and Wenjing Liao. Generalization error guaranteed
auto-encoder-based nonlinear model reduction for operator learning. arXiv preprint
arXiv:2401.10490, 2024a.
Hao Liu, Alex Havrilla, Rongjie Lai, and Wenjing Liao. Deep nonparametric estimation
of intrinsic data structures by chart autoencoders: Generalization error and robustness.
Applied and Computational Harmonic Analysis, 68:101602, 2024b.
Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice
synthesis via shallow diffusion mechanism. In AAAI Conference on Artificial Intelligence,
2022a.
Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and
transfer data with rectified flow. In International Conference on Learning Representations,
2022b.
Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu.
Flowgrad: Controlling the output of generative odes with gradients. In IEEE Conference
on Computer Vision and Pattern Recognition, 2023.
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation
for smooth functions. SIAM Journal on Mathematical Analysis, 53(5):5465–5506, 2021.
63Jiao, Lai, Wang and Yan
Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073, 2021.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normal-
ization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax opti-
mal distribution estimators. In ICLR 2023 Workshop on Mathematical and Empirical
Understanding of Foundation Models, 2023.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE
International Conference on Computer Vision, 2023.
Dmytro Perekrestenko, L´eandre Eberhard, and Helmut B¨olcskei. High-dimensional dis-
tribution generation through deep neural networks. Partial Differential Equations and
Applications, 2(5):64, 2021.
Jorge P´erez, Pablo Barcel´o, and Javier Marinkovic. Attention is turing complete. The
Journal of Machine Learning Research, 22(1):3463–3497, 2021.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov.
Grad-tts: A diffusion probabilistic model for text-to-speech. In International Conference
on Machine Learning, 2021.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning
with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434,
2015.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer.
High-resolution image synthesis with latent diffusion models. In IEEE Conference on
Computer Vision and Pattern Recognition, 2022.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU
activation function. The Annals of Statistics, 48(4):1875 – 1897, 2020.
Stefan Schonsheck, Jie Chen, and Rongjie Lai. Chart auto-encoders for manifold structured
data. arXiv preprint arXiv:1912.10094, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Optimal approximation rate of relu networks
in terms of width and depth. Journal de Math´ematiques Pures et Appliqu´ees, 157:101–135,
2022.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-based generative modeling through stochastic differential equations.
In International Conference on Learning Representations, 2021.
64Convergence Analysis of Flow Matching
Elias M Stein. Singular integrals and differentiability properties of functions. Princeton
university press, 1970.
Taiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth
besov spaces: optimal rate and curse of dimensionality. In International Conference on
Learning Representations, 2019.
Rong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In
International Conference on Artificial Intelligence and Statistics, 2024.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L(cid:32) ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems, 2017.
Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning
via schr¨odinger bridge. In International Conference on Machine Learning, 2021.
Ting-ChunWang,Ming-YuLiu,Jun-YanZhu,AndrewTao,JanKautz,andBryanCatanzaro.
High-resolution image synthesis and semantic manipulation with conditional gans. In
IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun
Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with
motion controllability. Advances in Neural Information Processing Systems, 2024.
Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne
Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of
image diffusion models for text-to-video generation. In IEEE International Conference on
Computer Vision, 2023.
Yilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola. Poisson flow generative models.
Advances in Neural Information Processing Systems, 2022.
DmitryYarotsky. Errorboundsforapproximationswithdeeprelunetworks. Neural Networks,
94:103–114, 2017.
Dmitry Yarotsky and Anton Zhevnerchuk. The phase diagram of approximation rates for
deep neural networks. Advances in Neural Information Processing Systems, 2020.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv
Kumar. Are transformers universal approximators of sequence-to-sequence functions?
arXiv preprint arXiv:1912.10077, 2019.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In IEEE International Conference
on Computer Vision, 2017.
65