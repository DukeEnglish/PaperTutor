ChatGLM-Math: Improving Math Problem-Solving in
Large Language Models with a Self-Critique Pipeline
YifanXu12†∗,XiaoLiu12∗,XinghanLiu12†,ZhenyuHou12,YueyanLi1†,XiaohanZhang1,
ZihanWang12,AohanZeng12,ZhengxiaoDu12,WenyiZhao1,JieTang2,YuxiaoDong2
1Zhipu.AI 2TsinghuaUniversity
Abstract
Largelanguagemodels(LLMs)haveshownexcellentmasteringofhumanlanguage,
but still struggle in real-world applications that require mathematical problem-
solving. While many strategies and datasets to enhance LLMs’ mathematics
are developed, it remains a challenge to simultaneously maintain and improve
bothlanguageandmathematicalcapabilitiesindeployedLLMsystems. Inthis
work,wetailortheSelf-Critiquepipeline,whichaddressesthechallengeinthe
feedbacklearningstageofLLMalignment. WefirsttrainageneralMath-Critique
modelfromtheLLMitselftoprovidefeedbacksignals. Then, wesequentially
employrejectivefine-tuninganddirectpreferenceoptimizationovertheLLM’s
own generations for data collection. Based on ChatGLM3-32B, we conduct a
seriesofexperimentsonbothacademicandournewlycreatedchallengingdataset,
MATHUSEREVAL. Results show that our pipeline significantly enhances the
LLM’smathematicalproblem-solvingwhilestillimprovingitslanguageability,
outperformingLLMsthatcouldbetwotimeslarger. Relatedtechniqueshavebeen
deployedtoChatGLM1,anonlineservingLLM.Relatedevaluationdatasetand
scriptsarereleasedathttps://github.com/THUDM/ChatGLM-Math.
Model Avg.ofGSM8k AlignBench GLM-4
&MATH Language 70.0
GPT-4-0613
DeepSeek-67B-Chat[12] 58.3 7.11 67.5
DeepSeek-67B-Chat-DPO[12] 57.7(-1.2%) 7.60(+6.8%)
65.0
InternLM2-Chat-20B[43] 57.2 7.68
Math-InternLM2-20B[43] 60.2(+5.1%) 6.53(-14.8%) 62.5 ChatGLM3-32B-SFT-2312 + RFT&DPO
ChatGLM3-32B-SFT-2312 52.4 7.37
60.0 ChatGLM3-32B-SFT-2312 + RFT
+RFT&DPO 61.6(+17.5%) 7.80(+5.85%)
DeepSeek-Chat-67B
57.5
Table1: Ourself-critiquepipelineenablessimul-
taneousimprovementoflanguageandmathemati- 55.0
Qwen-Chat-72B
calabilities.Previousalignmentmethodsenhance GPT-3.5-Turbo-0613
52.5 ChatGLM3-32B-SFT-2312
languagebutcouldpotentiallyimpairmathemati-
calabilities[12],whereasmath-specializedmod- 40 45 50 55 60 65 70 75
Hungarian Exam Score
elscouldharmlanguagecapabilities[43].
Figure1: ResultsofHungarianExamandAverage
ScoresofGSM8kandMATH.
*YifanandXiaocontributedequally.Emails:xu-yf23@mails.tsinghua.edu.cn,shawliu9@gmail.com
†WorkdonewhileXinghanandYueyaninternedatZhipuAI.
1https://chatglm.cn
Preprint.Underreview.
4202
rpA
3
]LC.sc[
1v39820.4042:viXra
HTAM
&
k8MSG
fo
.gvA1 Introduction
LargeLanguageModels(LLMs)[8;10;20;40;44;61;1]havegarneredwidespreadattentionfortheir
remarkableproficiencyinvariouslinguistictaskssuchastextsummarization[18;47;33;26],question
answering[16;24;7],androle-playingconversations[46;67;41]. Furthermore,theirpotentialin
addressing complex problems requiring mathematical reasoning [57; 48; 31] has expanded their
applicabilityacrossreal-worldmissions[30;5].
Despitetheseadvances,optimizingLLMstoexcelsimultaneouslyinlanguageunderstandingand
mathematicalproblem-solvingpresentsanotablechallenge. Theprevalentreinforcementlearning
fromhumanfeedback(RLHF)approachprimarilyenhancestextgenerationbasedonrewardmodels
reflectinghumanpreferences[44;35;45]. Althoughthismethodbooststhequalityofgeneratedtext,
itoftenoverlookstheaccuracyandlogicalcoherenceessentialforsolvingmathematicalproblems,
leadingtoadiscrepancyinperformanceknownasthe"alignmenttax"[2]whenappliedtomathemati-
calreasoning(refertoTable1). Conversely,attemptstobolsterLLMs’mathematicalcapabilities
typicallyentailsupervisedfine-tuning(SFT)thatinadvertentlydiminishestheirlinguisticversatility,
posingadilemmaforpracticalapplicationsofLLMsystems[43;57;31;60].
Pipeline: Self-Critique. Thispaperintroducesanovelapproachaimedatenhancingbothlinguistic
andmathematicalskillsofLLMswithoutcompromisingonefortheother. Ourstrategydeviates
fromtraditionalRLHFbyincorporatingaMath-CritiquemodelderivedfromtheLLMitself,which
evaluatesitsmathematicaloutputs. Thisself-critiquemechanismenablesthemodeltolearnfromAI-
generatedfeedbackspecificallytailoredtomathematicalcontent[4;25]. Ourmethodologycomprises
twoprimaryphases:
• Stage 1: Rejective Fine-tuning (RFT) [58] employs a rejection sampling technique, wherein
responsesfailingtomeetMath-Critiquestandardsarediscarded,whiletherestundergofurther
fine-tuning. Thisstageaimstoenhancethemodel’saccuracyandconsistencyinmathematical
responseswhileensuringdiversityamongtheselectedanswers.
• Stage 2: Direct Preference Optimization (DPO) [38] extends the improvement process by
directlylearningfrompairsofcorrectandincorrectanswers,furtherrefinedthroughMath-Critique,
focusingonthemostchallengingquestionsfromthepreviousstage.
Benchmark: MATHUSEREVAL. ToaccuratelyassessLLMs’capabilitiesinsolvingreal-world
mathematicalproblems,wedevelopthe MATHUSEREVAL dataset. Itfeaturesadiverserangeof
questions,extendingbeyondacademicexercisestoincludepracticalapplicationscenarios,thereby
betterreflectinguserneedscomparedtotraditionalacademicmathdatasets[64;50;11]. Weleverage
bothGPT-4-turboandourMath-Critiquemodelforcomprehensivescoring.
Insummary,ourcontributionsinclude:
• TheintroductionoftheSelf-Critiquepipeline,anovelframeworkthatelevatesboththemathe-
maticalandlinguisticcapabilitiesofLLMsthroughself-generatedfeedback,therebyeliminating
theneedforexternalsupervisorymodelsandmanualannotations. Thisapproachhasbeenvali-
datedonaChatGLM3-32Bmodel,achievingunparalleledperformanceontheMATHUSEREVAL,
Ape210k[64],MATH[16],andthelinguistictasksofAlignBench[29].
• ThecreationoftheMATHUSEREVALbenchmark,tailoredtoassessLLMsoncomplex,open-ended
mathematical queries relevant to real-world applications, setting a new standard in evaluating
practicalmathematicalreasoningcapabilities.
• Adetailedanalysisofthekeyfactorscontributingtotheenhancementofmathematicalproficiency
throughtheSelf-Critiquepipeline,offeringinsightsintofuturedirectionsforautonomousmodel
improvement.
2 RelatedWork
LLMforMathProblem-Solving. Variousapproacheshavebeenexploredtoenhancethemathe-
maticalproblem-solvingabilitiesoflanguagemodels. PromptingMethods,initiatedbyChainof
Thoughtprompting [51],havebeenrefinedtoguidemodelsthroughdetailedreasoning,withnotable
contributionsfrom [55;6;53]enhancingmathematicalandreasoningtasks.Thesemethods,however,
2Q a Mru e a e ts hs at mi Do p an tl aea sdn e d f t .rR omef e or uen r ces RQ eu fee rs eti no cn es s S caa nm dp idle a tm e u anlt sip wl ee r s Questions S ina cm orp rl ee c q tlu ye as nti so wn es rw ede re Questions
from our model. Ans. 1 Ans. 2 during step (2) from Ans. 1 Ans. 2
Generate the pseudo tag Pseudo Tag & … Ans. N our Critique-RFT … Ans. N
from CritiqueLLM and Human Label model.
manually label it.
I Qn us etr su tic ot nio : n A: A hac st ta hs r ea e j u ed igg ee n … values 0, 3, Answers are scored by Math-Critique Model A byn s thw ee Mrs aa tr he - Cre rr ia tin qk ue ed Math-Critique Model
and 5, … Math-Critique model. 10 8 model. Best and worst Ans. 1 9 Chosen
The data is used to train R M v e +of de \fr e re l a n cAc {ne 1s: }… w {5e r }x : w = … ,k wu x h+ e= r1 ep/ 3 u p * + is v \ af + r na y1 c {/ r5 e1 a}* l{ w 3}. … 2 a Cn hs ow se er ns aa nr de l Ra eb je el ce td e da s … …
the Math-Critique model number answers. Ans. N 2 Rejected
from the base model. The Response: Select better model
Score in response refers Analysis: The AI assistant's answer… responses for training Critique-RFT Training Set
to the quality of the Score: 9 our rejected sampling
model answer. supervised fine-tuning Question Response Select Chosen and Critique-DPO Training Set
Math-Critique Model m mo od de el l )( .Critique-RFT R trae ij nec inte gd C a rn is tiw qe ur es - for Question R Re es sp po on ns se e- -C Rh ejo es ce tn ed
DPO Model.
(1) Train Math-Critique (2) Train Critique-RFT Model (3) Train Critique-DPO Model
Figure2: Self-CritiquepipelineforChatGLM-Math. Ourmethodcomprisesthreesteps: trainingthe
Math-Critiquemodel,thenutilizingtheresultsfromMath-Critiqueforsampling,followedbytwo
stagesoftraining: CritiqueRFTandCritiqueDPO.Throughouttheentireprocess,onlyaminimal
amountofmanualinvolvementisrequiredduringtheMath-Critiquetrainingphase. Subsequentsteps
canbefullyautomatedanddonotdependonexternalsupervisorymodels.
arelimitedbythemodel’sinherentcapabilitiesastheydonotmodifythemodelitself. Supervised
Fine-tuningandReinforcementLearning(RL)havealsobeenpivotal. High-qualitysupervisorydata
fromworkslike [31;58;9;57;60;62]aimtoimprovemodelcapabilitiesdirectly. RL’spotentialin
generaldomains,demonstratedby [34;44;12],hasledtomathematicalenhancementsthroughOpe-
nAI’sProcessRewardModel [27]andPPOalgorithmapplications [31;48],despitethechallengesin
applyingtheDPOalgorithm [38]formathematicaltasks. DecodingStrategyandtheuseofExternal
Toolsofferadditionalavenuesforimprovement. Self-Consistency [49]andMATH-SHEPHERD
[48]exploresamplingstrategiestoenhanceproblem-solvingwhilecodeinsertiontechniquesinworks
likeMammoth [60]andTora [15]bypasscomputationallimitations.
MathematicalEvaluation. Complexreasoningtasks,suchasmathematics,havealwaysbeenone
ofthekeyindicatorsinassessingthecapabilitiesoflanguagemodels [22;37;17;14]. Solvinga
mathematicalprobleminvolvessemanticunderstanding,problemdecomposition,symbolicreasoning,
andnumericalcomputation,makingitanunparalleledchoiceforevaluatingthecognitivereasoning
abilityofLLMs. TheGSM8k [11]andMATH [17]datasetshavebecomethemostwidelyused
benchmarks. TheGSM8Kdatasetfocusesonmulti-stepreasoningwithbasicarithmetic,showcasing
the complexity and linguistic diversity inherent in grade school-level mathematics. The MATH
benchmarkfurtherspansvarioussubjectsanddifficultylevels,facilitatingadetailedexaminationofa
model’sproblem-solvingskills.
Additionally,asoneofthemostfocused-onabilitiesinlanguagemodels,thereexistsavastarrayof
datasetsformathematicalcapabilityacrossvariouslanguages. SomestandoutsetslikeAQuA [28],
Mathematics [39]andSAT-Math [66]divedeepintopuremathprowess,whileNumGLUE [32]
andBBH [42]mixmathwithotherabilities. Moreover,inChinese,Math23K [50]andCMath [52]
makeelementaryandmiddleschoolmathaplaygroundforproblem-solvers,withAgiEval [66]and
GaoKaoBench [63]turntoexam-levelchallenges. Moreover,Math401 [59]zeroesinontheability
ofnumbercalculation. However,thesedataareprimarilyinfixedformats,andcurrentworkshave
foundthatsimpleperturbationstothesequestionscansignificantlyimpactperformance [23;68].
Therefore, performance on these datasets needs to accurately reflect how models would perform
whenfacedwithusermathquestions.
3 Math-Critique: AGeneralCriticforMath
Definition. Thefirstpartofourworkinvolvesconstructinganaccurateandrobustevaluationmodel.
WeproposeMath-Critique,inspiredbyworksthatuselargemodelsforevaluationpurposes[21;65].
Thismethodscoresmathematicalresponsesgeneratedbymodelsbasedonquestionsandreference
answers,includinganoutputofexplanatoryanalysisandascorebetween1and10. Comparedto
traditionalrewardmodels,thisapproachleveragesthecontextualcapabilitiesoflanguagemodels,
3enablingmoreaccuratejudgmentsbyintegratingreferenceanswers. Theuseofexplanatoryanalysis
combinedwithscoringisalsoinspiredbytheconceptofthoughtchains,enhancingscoringaccuracy
whileprovidinginterpretability.
Intheinstructions,Math-Critiquemustclassifyresponsesintofourcategories: entirelyincorrect,
partially correct methodology yet erroneous outcome, accurate conclusion with partially flawed
methodology,andwhollycorrect. Thesecategoriesarealignedwithscoringrangesof1–2,3–5,6–8,
and9–10,respectively.
Therefore,theMathcritiquemodelcanbedefinedas:
MathCritique(Question,Reference,Answer)→(Critique,Score)
Here,theQuestionandReferencearetheoriginalproblemandthereferenceanswer,respectively,
andtheModelAnsweristheanswergivenbythemodelbeingevaluated.
Weemployedtwoevaluationmethodsusingmath-critique: averagescoreevaluationandhard-split
evaluation. Theaveragescoreevaluationcalculatesthemeanofthecritiquescoresassignedtoeach
modelanswerforasetofquestions. Ontheotherhand,thehard-splitevaluationcategorizeseach
modelansweraseitherpassingorfailingbasedonapredefinedcorrectnessthreshold. Ifthecritique
score surpasses this threshold for each answer, the answer is considered correct; otherwise, it is
deemedincorrect. Theoverallscoreisthencalculatedastheproportionofcorrectanswersoutofthe
totalnumberofquestions.
DataCollection. Ourconstructionmethodinvolvesthefollowingsteps:
• Weredesignedthescoringrulesandintervalsformathematicalresponses,enablingthemodelto
gradebasedonthecorrectnessoftheresultandtheprocess.
• We filtered a dataset from the training data, which includes mathematics questions along with
their reference answers and model responses, primarily sourced from exam questions ranging
fromjuniorhightouniversitylevels. Weutilizedmodelsamplinganswersfrommultiplesources,
includingdifferentversionsofChatGLMandothermodels.
• We employed CritiqueLLM [21] and ORM to annotate the dataset, selecting annotations that
representedthebestandworstscoringextremesfromthesemodels,anddirectlyusedthesepseudo
tagsfortraining. Thisstepgeneratedatotalof10kannotateddataentries.
• Forresultswithscoresinthemiddlerange,weselectedaportionformanualannotationintofour
categoriesandthenmappedtheseoutcomestoa10-pointscale. Wealsodividedatestsetfromthe
trainingdatasetandusedthesamemethodforfour-categoryannotation. Thisstepgenerated5k
annotateddataentriesforthetrainingsetand800forthetestset.
4 TheSelf-CritiquePipeline
Overview. Based on the construction method of Math-Critique, this section introduces the Self-
Critique pipeline. This pipeline is a weakly supervised iterative training method for enhancing
mathematicalabilities,originatingfromasinglemodel. Initially,wetrainaMath-Critiquemodel
usingthebasemodelandconcurrentlytrainabasicChatModelusingthefundamentalSFTdataset.
Subsequently,weemploytheMath-Critiquemodeltosupervisethefine-tuningoftheChatModel
throughrejectionsampling. Theoutcomeofthisstepcanserveasanewbasemodeltoupdateboth
theMath-Critiquemodelandtherejectionsamplingsupervisedfine-tuningmodel. Buildingupon
thesesteps,ourfinalactioninvolvesutilizingthelatestMath-Critiquemodeltosamplecontrastdata
andthenproceedingwithDPOtraining.
Inthesesteps,thedataconstructionfortheMath-critique-baseinvolvesasmallamountofmanual
annotation. However,thisbatchofannotationsisaone-timeeffort,asonlythisbatchofannotated
dataisneededasabootstrapfortheremainingiterations. Afterthat,inferenceandautomaticmodel
filteringcancompleteallremainingsteps.
Replacing manual annotation with inference can significantly reduce the time required for each
iterationfromthebasemodeltothefinalchatmodel. Thisavoidsaproblematicscenario: afterweeks
ofmanualannotation,thebasemodelmayhaveundergonefurtherpretrainingandbecomestronger,
makingitunclearwhetherthedatasampledweeksagoisstillaccurateorhasasignificantdistribution
gapforthecurrentmodel.
44.1 Stage1: RejectiveFine-tuning
WeutilizedarejectionsamplingmethodbasedonMath-Critique. Were-examinedandredesignedthe
implementationofRFTandfoundthatboththesamplingrangeandthemodelinfluencetheoutcomes
duringtherejectionsamplingprocess. Specifically,wedesignedthefollowingsamplingprinciples:
• Pre-deduplication: Clusterquestionembeddingsfromthetrainingsetandevenlysampleacross
categories,ensuringadiverserangeofquestionswithoutrepetition.
• Post-samplingdeduplication:Weconductedaselectionprocessafter5-10samplingiterationsbased
ontheresultsfromMath-Critique. Afteressentialdeduplication, wechosetheentirelycorrect
responseonlyincaseswheretherewerebothcorrectandincorrectresponsestothesamequestion.
Followingtheprocessoutlinedabove,wehaveobtainedtheCritique-RFTdataset:
(cid:40) (cid:41)
1 (cid:88)
D = (q ,a )| MathCritique(a )<1andMathCritique(a )>correct-bound
RFT i ij n ix ij
x
Inthisdataset,q denotestheithsampledquestion,witheachquestionundergoingnsamplings. a
i ij
representsthejthresponsetotheithquestion. MathCritiquereferstoMath-Critiquescore. ’correct
bound’denotestheminimumacceptablescoreforacorrectanswer,generallysetat0.7.
4.2 Stage2: DirectPreferenceOptimization
WeemployedtheDPOmethodtoenhancemodelcapabilitiesfurtherfollowingCritiqueRFT.The
primaryadvantagesofthismethodareitssimplicityinconstructingdataflows,stability,andspeed
duringtraining. TheDPOmethoddirectlycomparesthecorrectandincorrectanswerstothesame
question. Inourapproach,bothanswersaresampledfromthemodelpost-RFT,whichwefoundto
becriticallyimportant. WealsointegratedsftlossofDPOpositiveexamplesduringtrainingasan
approximatesubstituteforaregularizationterm.
OurDPOdatafilteringprocessissimilartoCritiqueRFT,withthesoledifferencebeingthecon-
structionmethodofDPOtrainingpairs. FortheselectionofDPOpairs,underthepremisethatthere
isatleastonecorrectandoneincorrectanswer,wechoosethedatapairwiththemostsignificant
differenceinMath-Critiquescoringresults.
Followingtheprocessoutlinedabove,wehaveobtainedtheCritique-DPOdataset:
  (cid:12) (cid:12) (cid:12) n1 (cid:80) xMathCritique(a ix)<1,  
D
DPO
= (q i,a chosen,a rejected)(cid:12) MathCritique(a chosen)>correct-bound,
 (cid:12) (cid:12) MathCritique(a rejected)<rejected-bound 
Inthisdataset,eachelementisatuple,whereq istheithsampledquestion. Foreveryquestionq ,
i i
samplednresponses,eachdenotedbya . TheMath-Critique(MathCritique)scoreiscomputed
ix
for each response a , and the average of these scores must be less than 1. The chosen answer
ix
foreachquestion,a ,istheonethatexceedsthe’correct-bound’,whichisapredetermined
i−chosen
thresholdindicatingasatisfactorylevelofcorrectness,oftensetaboveaspecificvalue. Conversely,
a representstheanswerthatfallsbelowthe’rejected-bound’,whichisthethresholdbelow
i−rejected
whichanswersareconsideredincorrectorunsatisfactory.
4.3 Training
4.3.1 Math-CritiqueTraining
WeemploythebasemodelofChatGLM3-32B[61;13]astheinitialMath-Critiquebasemodel. After
eachiteration,themodelcurrentlyrefinedthroughSFT(SupervisedFinetuning)orCritiqueRFTwill
beusedasthebase. Weusealearningrate3e-6andabatchsize128onboth6Band32Bscales.
4.3.2 Critique-RFTTraining
DuringtheCritiqueRFTphase,eachofourfinetuningiterationsincludesthedatasetsfromprevious
stagesafterdeduplication,whichalsoencompassestheinitialsftdataset. WemergeD andD
RFT SFT
5Question: In a 400-meter running race, if the Critique-RFT Training Response Critique-DPO Chosen
diameter of the first semicircular track is 72.6
meters and the width of each lane is 1.25 meters, In a 400-meter running race, to ensure fairness, it is ······
how far apart should the starting points of the necessary to consider the impact of the curve radius on Math-Critique Score: 10
first and second lanes be to ensure fairness in the the distance that athletes run.
competition? Since the starting point of the first runner is on the
diameter, its circumference is π × 72.6 meters.
For the second lane, … (More Steps) Critique-DPO Rejected
Reference Answer Taking π as approximately 3.1416, we get a distance First, we need to calculate the radius of the first track.
difference of 3.1416 × (75.1 - 72.6) meters = 3.1416 × Since the diameter is 72.6 meters, the radius would be
S 3.o 1l 4u t ×io 7n 2: .6T h =e
2
2le 7n .g 9t 6h
4
o (mf eth tee
r
sf )i ,rst curve of the track: 2 T. h5
e
m ree fote rr es
,
=
th
e7 . s8 t5 a4
rt
im nge t pe ors i.
nts of the first and second
7 N2 e. x6
t
,m we ete nrs
e
ed div ti od e dd
e
tb ey
rm
2 i,
n
e eq tu ha el i ·n ·g
·
(3 tr6 u.3
n
cm ate et de )rs.
The length of the second curve of the track: lanes should differ by 7.854 meters to conduct the So, the starting points of the first and second lanes
3.14 × (72.6 ÷ 2 + 1.25) × 2 = 235.814 (meters); race more fairly. should be 2.5 meters apart for the race.
The difference between the two curves: 235.814 -
227.964 = 7.85 (meters); Math-Critique Score: 9 Math-Critique Score: 1
(1) Given question and reference answer (2) An example response adopted by RFT stage (3) An example pair adopted by DPO stage
Figure3: Trainingdatasetsexamples. Thetrainingdatawegeneratedisdividedintotwocategories,
originatingfromthequestionsandreferenceswithinexistingdatasets. Wehaveconstructedseparate
RFTtrainingdataandpairedDPOtrainingdata.
as:
D =D ∪D
RFT SFT
The D dataset encompasses many routine tasks and can be substituted with an open-source
SFT
instruction finetuning dataset. To eliminate the potential interference of this dataset on the final
results, wecomparedtheimpactofincludingorexcludingthesftdatainourablationstudy. We
finetuneabaseLLMmodelπ bystandardmax-loglikelihoodloss:
θ
L(π )=−E [log(π (a |q ))]
θ (qi,aij)∼D θ ij i
Inthisstage,weusealearningrate2e-5andfinetunefor8000stepswithabatchsizeof64.
4.3.3 Critique-DPOTraining
DuringtheCritique-DPOphase,itwasobservedthatthedirectuseofDPOlossledtoinstabilityin
thetrainingprocess. Across-entropylossforthechosenanswerwasintroducedasaregularization
termtothetotallosstomitigatethisissue. Thisadditionaimedtoenhancethestabilityofthemodel
training. TheDPOdataset,previouslyconstructedanddenotedasD ,wasusedasthetraining
DPO
dataset. Thelossfunctionweusedisasfollows:
(cid:20) (cid:18) (cid:19)(cid:21)
π (a |q ) π (a |q )
L (π ;π )=−E logσ βlog θ i−cho i −βlog θ i−rej i
DPO θ ref (qi,ai−cho,ai−rej)∼DDPO π (a |q ) π (a |q )
ref i−cho i ref i−rej i
L (π ;π )=−E [log(π (a |q ))]
CE θ ref (qi,ai−cho)∼D θ i−cho i
L =λ·L +L
merge DPO CE
Inthiscontext,λrepresentsthecoefficientofthecross-entropylossforthechosenanswerinthe
totalloss. Commonly, weexperimentwithvaluesin{0.5, 1, 1.5}. Anothercriticalcoefficientis
β,whichmeasuresthepenaltyintensityofDPOforincorrectanswers. Owingtotheadditionofa
regularizationterm,thevalueofthiscoefficientishigherthanthatofthestandardDPO,withour
testingrangeforthisvaluebeing{0.5,1,2}. Besidesthese,theoveralllearningrateissetat1e-6.
Theexperimentalsectionwillreporttheoptimalresultsunderthesecoefficientsettings. Wetrain500
stepswithabatchsizeof64inthisstage.
5 MATHUSEREVAL: BenchmarkingLLMs’MathematicalReasoningin
Application
MATHUSEREVALisatestsetdesignedforreal-usescenarios,targetingissuesofconcerntousersand
morechallengingmathematicalproblems. Someofourdataoriginatesfromuniversityexamination
questions,whileanotherisfromsimulateddialogues. Forthelatter,weassignedaseriesofannotators
whoposedmath-relatedquestionstooursystembasedontheirdailyexperiencesandobservations
usinglargemodels.
6Table 2: The composition of the MATHUSEREVAL data set. We divided the test set into three
categories: ElementaryandAdvancedMathematics. Forcalculatingthetotalscore, weusedthe
macro-averagescore.
Category Sub-Category Size Source
Calculate 75
Algebra 113
Elementary Dialogues
Geometry 81
Trigonometry 73
DiscreteMath 45
Probability 46
Advanced Dialogues&Exams
LinearAlgebra 58
Calculus 54
Based on the distribution of the collected data, we divided the test set into two main categories,
Elementary and Advanced, and eight sub-categories. Given that Calculate Applications are less
challengingandcloselyalignedwiththescopeofpreviouspublicdatasets,weselectedfewerquestions
fromthiscategory. ThequantityofquestionsineachofthesecategoriesisasshowninTable2. All
questionsareposedinanopen-endedformat. Possibleanswersincludeasinglenumber,multiple
numbers,ormathematicalexpressions.
Weoffertwoevaluationmethods:GPT-4-1106——Preview[34;29;65]evaluationandMath-Critique
evaluation. Theformeradoptstheevaluationmethodofalignbench[29],toprovideamoreaccurate,
fair, andaccessibleevaluationapproach; thelatteremploysthesameusageastheMath-Critique
introducedabove. Similarly,wewillalsoreporttwotypesofscores: avg-scoreandhard-split.
6 Experiment
6.1 DataCollection
Theprimarysourcesofourdatacollectioncanbecategorizedasfollows: trainingsetsfrompublic
datasetsandpubliclyavailablemiddleschoolanduniversityexaminationquestions. Wehaveselected
allpromptsfromGSM8k [11]andMATH [17]trainingsetasthequestionsetfortheEnglishdata,
andusedtheresponsesfromtheoriginaldatasetasthestandardanswers.Regardingpubliclyavailable
middleschoolanduniversityexamquestions,weusedtheanswerformatsprovidedwiththeexam
papersasthecommonanswerswithoutfurtherprocessing.
6.2 EvaluationSetting
6.2.1 Datasets
Inourresearch,weprimarilytestedtheMATHUSEREVALdataset,whichoriginatesfromsimulated
dialogue records and actual exam papers. Compared to academic datasets, this dataset features
a more diverse array of question styles and more closely aligns with real-world usage scenarios.
Additionally,wetestedthefollowingacademicdatasets: 1. Englishacademicdatasets: GSM8k[11]
andMATH[17]. ThesetwodatasetscontainEnglishmathematicsproblemsatthemiddleandhigh
schoolandcompetitionlevels. 2. Chineseacademicdatasets: ape210k[64]andcmath[52]. The
questionsinthesedatasetsalsooriginatefrommiddleandhighschoollevels. Wealsoemployedthe
HungarianNationalExam[36]asanOut-Of-Distributiontestset. Itshouldbenotedthat,acrossall
testsets,weonlyusedthetrainingsetsofGSM8kandMATHasseeddatafordatageneration.
Toevaluategenerallinguisticcapabilities,weselectedtheChineselanguagecomponentofAlign-
Bench[29]andfullMT-Bench[65]fortesting.
6.2.2 Baselines
SincemostofourworkisconductedinChinese,weselectedthreecategoriesofbaselines: open-
sourcemathematics-specificmodels,open-sourceChinesemodels,andleadingproprietarymodels.
7Table3: MainResult. Allresultsreportedarethehighestachievedinzero-shotorfew-shotsettings
andarebasedongreedydecoding. Thebestmodelsaremarkedinboldandtheunderlinesignifies
thesecondbestmodel.
Chinese English General
Models #params
MathUserEval HungaAlignBench
Ape210kCmathGSM8kMATH MT-Bench
-rian
OverallElementaryAdvanced Language
GPT-4-1106-Preview[34] N/A 5.73 5.07 6.81 84.2 89.3 93.6 53.6 92 8.29 9.32
GPT-4-0613[34] N/A 4.14 3.34 5.33 83.6 86.5 91.4 45.8 68 7.59 9.18
GPT-3.5-Turbo-0613[34] N/A 3.42 3.04 4.07 70.4 76.8 78.2 28.0 41 6.82 8.36
Claude-2[1] N/A 3.29 2.63 4.35 72.8 80.5 88.0 - 55 6.78 8.06
GLM-4 N/A 5.11 4.86 5.43 93.5 89.0 91.8 49.0 75 8.38 8.62
Skywork-13B-Math[54] 13B 2.66 2.75 2.54 74.4 77.3 72.3 17.0 39 5.58 4.12
InternLM2-Chat[43] 20B 3.25 3.00 3.68 72.0 80.7 79.6 34.8 48 7.68 8.21
Math-InternLM2[43] 20B 3.17 3.08 3.37 75.2 78.5 82.6 37.7 66 6.53 6.09
Yi-Chat[56] 34B 2.64 2.49 2.87 65.1 77.7 76.0 15.9 39 6.18 6.54
DeepSeek-Chat[12] 67B 3.24 2.76 3.84 76.7 80.3 84.1 32.6 58 7.11 8.35
MetaMath(EN)[57] 70B - - - - - 82.3 26.0 35 - 4.28
Qwen-Chat[3] 72B 3.87 3.99 3.67 77.1 88.1 76.4 31.8 52 7.29 6.43
ChatGLM3-32B-SFT-2312∗ 32B 3.25 3.03 3.60 78.0 79.8 75.8 29.0 39 7.37 8.05
+RFT 32B 4.01 3.86 4.26 87.0 85.3 82.4 39.5 58 7.42 8.03
+RFT,DPO 32B 4.23 4.01 4.59 89.4 85.6 82.6 40.6 73 7.80 8.08
*ChatGLM3-32B-SFT-2312isanewerversionoftheChatGLMseriesandnotidenticaltothemodeldiscussedin[19],despite
sharingthesamemodelsize.
Fortheopen-sourcemathematicsmodels,wechoseSkyMath[54],MetaMath[57],andInternlm2-
Math [43] as our baselines. To effectively compare with the best Chinese models, we selected
Qwen-Chat[3],Yi-Chat[56],DeepSeek-Chat[12],andInternLM2[43]. Additionally,wealsoreport
theresultsforGPT-4-1106-Preview(knownasGPT4-Turbo),GPT-4-0613,GPT-3.5-Turbo[34],and
Claude-2[1].
6.2.3 Metrics
Foralldatasets,weutilizedtheresultsofgreedyinferenceperformedonce. Regardingacademic
datasets,wereporttheself-reportedresultsofcorrespondingmodelsandthehighestzero-shot/few-
shotresultsfromtheOpenCompassandMATHUSEREVALwebsites. ForthemathsubsetofAlign-
Bench[29]andourproposedMATHUSEREVALtestset,wereportthescoringresultsfromGPT-4-
TurboandthescoresgeneratedbyMath-Critique. Moreinformationaboutevaluationsettingscanbe
foundinAppendixC.
6.3 MainResults
Table3displaysourmainresults.Inmodelswithmorethan10billionparameters,ourmodelachieved
ascoreof4.23onMATHUSEREVAL,89.4onape210k[64],and40.6onMATH[16],surpassingall
modelswithpublishedparametersandachievednear-topperformancesonCmathandGSM8k. Our
modelalsoscored73intheHungaryTest[36],thehighestscoreamongallknownparametermodels.
We used the ChatGLM3-32B-SFT-2312 version as our baseline. Our RFT phase significantly
improved across all math datasets. In contrast, the DPO phase’s improvement focused on open-
endedmathproblems,includingMATHUSEREVAL,theHungarianExam,andthegeneral-purpose
AlignBench. EventhoughourimprovementonMT-bench[65]isnotsignificant,giventhatover90%
ofourtrainingdataisinChinese,webelievethatmaintainingparityessentiallydemonstratesthatour
methodhaspreservedtheoriginalEnglishgeneralormulti-turncapabilities.
Compared to proprietary models, especially the GPT series by OpenAI, GLM-4 demonstrates
competitiveorsuperiorperformanceinspecificareas. GPT-4-1106-Preview,forexample,showsthe
bestperformanceinmosttasks,includingthehighestscoresinbothChineseandEnglishbenchmarks,
highlightingitseffectivenessinvariousmathematicalproblem-solvingcontexts. However,GLM-
4 surpasses it in the Ape210k and AlignBench benchmarks, suggesting particular strengths in
mathematicalreasoningandcross-linguisticgeneralization.
8Table4: AblationStudyfor32Bmodel. Allresultsarefine-tunedfromour32Bbasemodel. We
selectedMetamathtrainingsetasbaselinesthatweconsidercomparativelystrong. MATHUSEREVAL
isscoredwithMath-Critiquemodel.
Chinese English
Method
MATHUSEREVAL Ape210k∗ GSM8k MATH∗
Metamath[57] 2.80 75.8 77.9 35.6
ChatGLM3-32B-SFT-2312+RFT 3.74 87.0 82.4 39.5
-Realscenarios&Academic 3.29 85.9 74.8 27.6
-Realscenarios 3.29 74.6 77.4 36.0
-Academic 3.72 75.8 81.0 36.2
ChatGLM3-32B-SFT-2312+RFT&DPO 4.37 89.4 82.6 41.0
-RealScenarios&Academic 4.14 87.8 81.5 37.8
*Ablatedexperimentsareconductedon500-sampletestsubsets.
1.0 ChatGLM3-32B-SFT-2312 + RFT&DPO
ChatGLM3-32B-SFT-2312 + RFT
0.8 ChatGLM3-32B-SFT-2312
0.6
0.4
0.2
0.0
1 2 3 4 5 6 7 8 9
Avg Score of all Models
Figure 4: The Relationship between Different Boosting Methods and Problem Difficulty. The
horizontalaxisdisplaystheaveragescoreofMATHUSEREVALacross24models(scoredbyGPT-4-
1106-Preview),whichweregardasarepresentationofproblemdifficulty. Theverticalaxisrepresents
thehard-splitscoresofthemodelsonthesequestions.
6.4 AblationStudy
Impact of data composition. Table 4 presents the results of ablation experiments. We selected
Metamath [57] training set as baselines that we consider comparatively strong. After applying
Critique-RFT,wefoundthatusingonlyacademicdatasetstoconstructRFTdataresultsininferior
performanceinreal-lifescenario-basedMATHUSEREVALandacademictestsetscomparedtothe
outcomesafterintegratingreal-lifescenariodata. Furthermore,introducingEnglishdatasignificantly
improvesperformanceonEnglishdatasetswithoutsubstantiallyaffectingChinesecapabilities.
TheablationexperimentsconductedduringtheCritique-DPOphaseindicatethat,comparedtousing
general DPO data, the addition of math-specific DPO data significantly enhances mathematical
capabilities. WenolongerconductseparatetestsontheimpactofRealscenariosandAcademicdata
oncapabilities. Thisisbecause,inpreviousstages,questionsthatthemodelcouldsolvecorrectly
were removed, making it impossible to ensure that both datasets still have sufficient size for a
completetrainingsession.
RelationshipbetweenDifferentBoostingMethodsandProblemDifficulty.
Figure4displaystherelationshipbetweentheaverageaccuracyofeachquestioninMATHUSEREVAL
acrossall24modelstested(includingsomeintermediatemodels)andthehard-splitscoresofthe
fourGLMseriesmodels. Theaverageaccuracyacrossalltestedmodelsisconsideredareflection
ofthedifficultylevelofthequestion. ItcanbeobservedthattheRFTstepimprovesperformance
acrossalmostalldifficultylevels,butthemostsignificantimprovementscomefromquestionswith
anaveragescorebetween4and6. TheDPOstepmainlyenhancesperformanceonquestionswithan
9
etar
ssap
tilps-draHTable 5: Evaluation for Math-Critique Model. We report "Acc" as the accuracy of the model in
determiningwhetheranansweriscorrect,aswellasthePearson,Spearman,andKendallcorrelation
coefficientsforMath-Critiqueincomparisonwithhumanannotationsinafour-categoryclassification.
Model Acc. Pearson Spearman Kendall
GPT-3.5-Turbo 62.1 31.8 33.5 30.1
GPT-4-0613 90.2 80.5 78.1 71.0
Math-Critique-32B 90.5 80.4 77.1 70.2
averagescorebetween5and7. Thissuggeststhatourtwo-stepapproachtoenhancingmathematical
capabilitiescanbeseenasaligningthemodelmorecloselywithreal-worldconditions,withthemost
noticeableimprovementsonmedium-difficultyquestions.
Impactongeneralcapabilities. Consideringthatourgoalisnottodevelopaspecializedmathemati-
calmodelforleaderboardclimbingbutratherageneralmodelwithstrongmathematicalcapabilities,
wetestedtheresultsusingAlignbench[29],aChinesegeneralopenquestion-answeringdataset. The
resultsinTable3demonstratethatourmodelexceedsthetrainingoutcomesofsimilarbaselinemodels
thatdonotincorporatespecializedmathematicaldataregardingChineselanguagecapabilities. Addi-
tionally,itperformsexceptionallywellcomparedtootheropen-sourceChinesemathematical/general
models.
IntermsofEnglishgeneralcapabilities,wetestedusingMT-Bench[65]asthetestset. Giventhat
over90%ofourtrainingdataconsistsofChinese,thefactthattheresultsonMT-Bench[65]remained
largelyunchangedduringourtrainingprocessindicatesthattheEnglishlanguagecapabilitieswere
notsignificantlyaffected.
EffectivenessofMath-Critique. Duringtheprocessofmanualannotation,wecollectedatestsetof
800questions,allofwhichweremanuallymarkedforthecorrectnessoftheiranswersandprocedures,
thusformingafour-categorytest; theoutputresultsofMath-Critiqueweremappedtothesefour
categoriesaccordingtotherequirementsoftheinstructions.
WevalidatedtheeffectivenessofMath-Critiqueitselfthroughempiricalexperiments. Wesetup
twoevaluationmethods: theaccuracyofdirectlyscoringtojudgecorrect/incorrectresultsandthe
accuracyofjudgingourdefinedfourcategories. WeextractedtestsetsfromChinesejuniorandsenior
highschoolexamquestionsandMATHUSEREVAL,annotatedcorrectjudgmentbyexperts.
TheresultsshowninTable5indicatethatourMath-Critique-32BmodelsignificantlysurpassesGPT-
3.5-Turboinbothjudgmentaccuracyandcorrelationcoefficientscomparedtohumanannotations
andisessentiallyonparwithGPT-4-0613.
Out-Of-DistributionTest. FollowingtheapproachofGrok-1,totesttheperformanceonOut-Of-
Distributiondatasets,weselectedtheHungariannationalfinalexam[36]. Thisisatestsetof33
examquestionswithoutatrainingset,theadvantagebeingthatitallowsforevaluatingamodel’s
mathematicalcapabilitiesinanutterlyOODenvironment. AsshowninFigure1,usinghumanexpert
evaluation,wefoundthatatamodelscaleof32B,ourRFTmodelscored57,whiletheDPOmodel
scored73. However,itisessentialtonotethatsinceourmodel’sprimarylanguageisChinese,ifthe
modelanswerscorrectlyinChinese,wewouldscoreitusually. Weplantoaddressthisissueinfuture
models.
7 LimitationandFutureWork
Weobservedthefollowingissuesinourmathematicalmodels,andweleaveitforourfuturework:
Graphicthinkinganddrawingabilities. Duetothelimitationsofbeingapurelylinguisticmodel,
our model has deficiencies in handling questions requiring drawing. For example, in a question
fromtheHungaryTest,whichrequiredconnectingsixnumbersasdivisorsofeachother,ourmodel
correctlylistedthedifferentnumbers’connectingtopologybutcouldnotdrawitaccurately. Also,as
alanguagemodel,itstrugglestorespondcorrectlytoquestionsrequiringanunderstandingofimages.
10Apotentialsolutioncouldbeintegratingmultimodalinputandoutputcomponents,anareaweplan
toexplorefurther.
Precisioncalculationcapability. Weobservedthatinincorrectlyansweredquestions,iftheproblem
requiredmultiplication,division,orexponentiationofthreeormoredecimalplaces,ourmodelmight
compute with a deviation of up to 5%. This phenomenon aligns with observations from GPT-4
modelswithoutanintegratedcodeinterpreter. Thisissuemightbeafundamentalproblemtopure
languagemodelsandcouldbemitigatedbutnotresolvedwithincreasingmodelsize. Usingexternal
toolsforcomputationordirectlyemployingcodewithacodeinterpretercouldsolvethisproblem.
However,ourdiscussioninthispaperfocusesonenhancingthemathematicalcapabilitiesofpure
languagemodels,andwewillendeavortoaddresstheseissuesinfuturework.
8 Conclusion
Inthispaper,weintroducetheMath-Critiquemethodforevaluatingthecorrectnessofmathematical
problems,andbasedonthismethod,weproposetheSelf-Critiquemethodaimedatenhancingthe
mathematicalcapabilitiesoflanguagemodelswithouttheneedforexternalsupervisorymodelsand
manualannotations. OurexperimentswereconductedinbothEnglishandChinese,anda32-billion
parametermodelachievedstate-of-the-artresultsamongopen-sourcelanguagemodelsonmultiple
datasets. Additionally,itsurpassedseveralrenownedproprietarymodels,includingGPT-4-0613,on
ourproposedMATHUSEREVALtestset. Ourmethodwasappliedduringthedevelopmentprocessof
GLM-4asacomponenttoimprovemathematicalcapabilities,achievingthebestresultsondatasets
suchasMATHUSEREVAL,ape210k,GSM8k,andtheHungariantest,exceptGPT-4-Turbo.
11References
[1] Anthropic. Introducingclaude,2023.
[2] A.Askell,Y.Bai,A.Chen,D.Drain,D.Ganguli,T.Henighan,A.Jones,N.Joseph,B.Mann,
N.DasSarma,N.Elhage,Z.Hatfield-Dodds,D.Hernandez,J.Kernion,K.Ndousse,C.Olsson,
D.Amodei,T.Brown,J.Clark,S.McCandlish,C.Olah,andJ.Kaplan. Agenerallanguage
assistantasalaboratoryforalignment,2021.
[3] J.Bai,S.Bai,etal. Qwentechnicalreport,2023.
[4] Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones,A.Chen,A.Goldie,A.Mirho-
seini,C.McKinnon,C.Chen,C.Olsson,C.Olah,D.Hernandez,D.Drain,D.Ganguli,D.Li,
E.Tran-Johnson,E.Perez,J.Kerr,J.Mueller,J.Ladish,J.Landau,K.Ndousse,K.Lukosuite,
L.Lovitt,M.Sellitto,N.Elhage,N.Schiefer,N.Mercado,N.DasSarma,R.Lasenby,R.Lar-
son,S.Ringer,S.Johnston,S.Kravec,S.E.Showk,S.Fort,T.Lanham,T.Telleen-Lawton,
T.Conerly,T.Henighan,T.Hume,S.R.Bowman,Z.Hatfield-Dodds,B.Mann,D.Amodei,
N.Joseph,S.McCandlish,T.Brown,andJ.Kaplan. Constitutionalai: Harmlessnessfromai
feedback,2022.
[5] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al.
Longbench: Abilingual,multitaskbenchmarkforlongcontextunderstanding. arXivpreprint
arXiv:2308.14508,2023.
[6] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann,
M. Podstawski, H. Niewiadomski, P. Nyczyk, et al. Graph of thoughts: Solving elaborate
problemswithlargelanguagemodels. arXivpreprintarXiv:2308.09687,2023.
[7] Y. Bisk, R. Zellers, R. Le bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical com-
monsenseinnaturallanguage. ProceedingsoftheAAAIConferenceonArtificialIntelligence,
34(05):7432–7439,Apr.2020.
[8] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
informationprocessingsystems,33:1877–1901,2020.
[9] E. Chern, H. Zou, X. Li, J. Hu, K. Feng, J. Li, and P. Liu. Generative ai for math: Abel.
https://github.com/GAIR-NLP/abel,2023.
[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung,C.Sutton,S.Gehrmann,etal. Palm: Scalinglanguagemodelingwithpathways. arXiv
preprintarXiv:2204.02311,2022.
[11] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word
problems,2021.
[12] DeepSeek-AI,:,X.Bi,D.Chen,G.Chen,S.Chen,D.Dai,C.Deng,H.Ding,K.Dong,Q.Du,
Z.Fu,H.Gao,K.Gao,W.Gao,R.Ge,K.Guan,D.Guo,J.Guo,G.Hao,Z.Hao,Y.He,W.Hu,
P.Huang,E.Li,G.Li,J.Li,Y.Li,Y.K.Li,W.Liang,F.Lin,A.X.Liu,B.Liu,W.Liu,X.Liu,
X.Liu,Y.Liu,H.Lu,S.Lu,F.Luo,S.Ma,X.Nie,T.Pei,Y.Piao,J.Qiu,H.Qu,T.Ren,Z.Ren,
C.Ruan,Z.Sha,Z.Shao,J.Song,X.Su,J.Sun,Y.Sun,M.Tang,B.Wang,P.Wang,S.Wang,
Y.Wang,Y.Wang,T.Wu,Y.Wu,X.Xie,Z.Xie,Z.Xie,Y.Xiong,H.Xu,R.X.Xu,Y.Xu,
D.Yang,Y.You,S.Yu,X.Yu,B.Zhang,H.Zhang,L.Zhang,L.Zhang,M.Zhang,M.Zhang,
W.Zhang,Y.Zhang,C.Zhao,Y.Zhao,S.Zhou,S.Zhou,Q.Zhu,andY.Zou. Deepseekllm:
Scalingopen-sourcelanguagemodelswithlongtermism,2024.
[13] Z.Du,Y.Qian,X.Liu,M.Ding,J.Qiu,Z.Yang,andJ.Tang. Glm: Generallanguagemodel
pretrainingwithautoregressiveblankinfilling. InProceedingsofthe60thAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1: LongPapers),pages320–335,2022.
[14] Y.Fu,L.Ou,M.Chen,Y.Wan,H.Peng,andT.Khot.Chain-of-thoughthub:Acontinuouseffort
tomeasurelargelanguagemodels’reasoningperformance. arXivpreprintarXiv:2305.17306,
2023.
12[15] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A
tool-integratedreasoningagentformathematicalproblemsolving,2023.
[16] D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring
massivemultitasklanguageunderstanding. ProceedingsoftheInternationalConferenceon
LearningRepresentations(ICLR),2021.
[17] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Stein-
hardt. Measuring mathematical problem solving with the math dataset. arXiv preprint
arXiv:2103.03874,2021.
[18] K.M.Hermann,T.Kocisky,E.Grefenstette,L.Espeholt,W.Kay,M.Suleyman,andP.Blunsom.
Teachingmachinestoreadandcomprehend. InC.Cortes,N.Lawrence,D.Lee,M.Sugiyama,
andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume28.Curran
Associates,Inc.,2015.
[19] Z.Hou,Y.Niu,Z.Du,X.Zhang,X.Liu,A.Zeng,Q.Zheng,M.Huang,H.Wang,J.Tang,
andY.Dong. Chatglm-rlhf: Practicesofaligninglargelanguagemodelswithhumanfeedback,
2024.
[20] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
ford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361,2020.
[21] P.Ke,B.Wen,Z.Feng,X.Liu,X.Lei,J.Cheng,S.Wang,A.Zeng,Y.Dong,H.Wang,etal.
Critiquellm: Scalingllm-as-criticforeffectiveandexplainableevaluationoflargelanguage
modelgeneration. arXivpreprintarXiv:2311.18702,2023.
[22] R.Koncel-Kedziorski,S.Roy,A.Amini,N.Kushman,andH.Hajishirzi. Mawps: Amathword
problemrepository. InProceedingsofthe2016conferenceofthenorthamericanchapterof
theassociationforcomputationallinguistics: humanlanguagetechnologies,pages1152–1157,
2016.
[23] V.Kumar,R.Maheshwary,andV.Pudi.Adversarialexamplesforevaluatingmathwordproblem
solvers. InFindingsoftheAssociationforComputationalLinguistics: EMNLP2021,pages
2705–2712,2021.
[24] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,
I.Polosukhin,J.Devlin,K.Lee,etal. Naturalquestions: abenchmarkforquestionanswering
research. TransactionsoftheAssociationforComputationalLinguistics,7:453–466,2019.
[25] H.Lee,S.Phatale,H.Mansoor,T.Mesnard,J.Ferret,K.Lu,C.Bishop,E.Hall,V.Carbune,
A.Rastogi,andS.Prakash. Rlaif: Scalingreinforcementlearningfromhumanfeedbackwithai
feedback,2023.
[26] Y. Li, Y. Zhang, Z. Zhao, L. Shen, W. Liu, W. Mao, and H. Zhang. CSL: A large-scale
Chinese scientific literature dataset. In Proceedings of the 29th International Conference
onComputationalLinguistics,pages3917–3923.InternationalCommitteeonComputational
Linguistics,Oct.2022.
[27] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,
I.Sutskever,andK.Cobbe. Let’sverifystepbystep,2023.
[28] W.Ling,D.Yogatama,C.Dyer,andP.Blunsom. Programinductionbyrationalegeneration:
Learningtosolveandexplainalgebraicwordproblems. InProceedingsofthe55thAnnual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
158–167,2017.
[29] X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam,
etal. Alignbench: Benchmarkingchinesealignmentoflargelanguagemodels. arXivpreprint
arXiv:2311.18743,2023.
[30] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al.
Agentbench: Evaluatingllmsasagents. arXivpreprintarXiv:2308.03688,2023.
13[31] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.
Wizardmath: Empoweringmathematicalreasoningforlargelanguagemodelsviareinforced
evol-instruct. arXivpreprintarXiv:2308.09583,2023.
[32] S.Mishra,A.Mitra,N.Varshney,B.Sachdeva,P.Clark,C.Baral,andA.Kalyan. Numglue: A
suiteoffundamentalyetchallengingmathematicalreasoningtasks. InProceedingsofthe60th
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),
pages3505–3523,2022.
[33] S.Narayan,S.B.Cohen,andM.Lapata. Don’tgivemethedetails,justthesummary! Topic-
awareconvolutionalneuralnetworksforextremesummarization. InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,2018.
[34] OpenAI. Gpt-4technicalreport,2023.
[35] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,
K.Slama,A.Ray,etal. Traininglanguagemodelstofollowinstructionswithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.
[36] K.Paster. Testinglanguagemodelsonaheld-outhighschoolnationalfinalsexam. https:
//huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam,2023.
[37] S.PoluandI.Sutskever. Generativelanguagemodelingforautomatedtheoremproving. arXiv
preprintarXiv:2009.03393,2020.
[38] R.Rafailov,A.Sharma,E.Mitchell,S.Ermon,C.D.Manning,andC.Finn. Directpreference
optimization: Yourlanguagemodelissecretlyarewardmodel,2023.
[39] D.Saxton,E.Grefenstette,F.Hill,andP.Kohli. Analysingmathematicalreasoningabilitiesof
neuralmodels. arXivpreprintarXiv:1904.01557,2019.
[40] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic´, D. Hesslow, R. Castagné, A. S. Luccioni,
F.Yvon,M.Gallé,etal. Bloom: A176b-parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100,2022.
[41] Y.Shao,L.Li,J.Dai,andX.Qiu. Character-llm: Atrainableagentforrole-playing,2023.
[42] M.Suzgun,N.Scales,N.Schärli,S.Gehrmann,Y.Tay,H.W.Chung,A.Chowdhery,Q.V.Le,
E.H.Chi,D.Zhou,etal. Challengingbig-benchtasksandwhetherchain-of-thoughtcansolve
them. arXivpreprintarXiv:2210.09261,2022.
[43] I.Team. Internlm: Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities.
https://github.com/InternLM/InternLM,2023.
[44] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro, F.Azhar, etal. Llama: Openandefficientfoundationlanguagemodels. arXiv
preprintarXiv:2302.13971,2023.
[45] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,D.Bikel,L.Blecher,C.C.Ferrer,M.Chen,G.Cucurull,D.Esiobu,
J.Fernandes,J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hosseini,
R.Hou,H.Inan,M.Kardas,V.Kerkez,M.Khabsa,I.Kloumann,A.Korenev,P.S.Koura,M.-A.
Lachaux,T.Lavril,J.Lee,D.Liskovich,Y.Lu,Y.Mao,X.Martinet,T.Mihaylov,P.Mishra,
I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,R.Silva,E.M.
Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,
I.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,S.Edunov,and
T.Scialom. Llama2: Openfoundationandfine-tunedchatmodels,2023.
[46] Q. Tu, S. Fan, Z. Tian, and R. Yan. Charactereval: A chinese benchmark for role-playing
conversationalagentevaluation,2024.
[47] M. Völske, M. Potthast, S. Syed, and B. Stein. TL;DR: Mining Reddit to learn automatic
summarization. InL.Wang,J.C.K.Cheung,G.Carenini,andF.Liu,editors,Proceedingsof
theWorkshoponNewFrontiersinSummarization,pages59–63,Copenhagen,Denmark,Sept.
2017.AssociationforComputationalLinguistics.
14[48] P.Wang,L.Li,Z.Shao,R.X.Xu,D.Dai,Y.Li,D.Chen,Y.Wu,andZ.Sui. Math-shepherd:
Verifyandreinforcellmsstep-by-stepwithouthumanannotations,2023.
[49] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.
Self-consistencyimproveschainofthoughtreasoninginlanguagemodels,2023.
[50] Y.Wang,X.Liu,andS.Shi. Deepneuralsolverformathwordproblems. InM.Palmer,R.Hwa,
andS.Riedel,editors,Proceedingsofthe2017ConferenceonEmpiricalMethodsinNatural
Language Processing, pages 845–854, Copenhagen, Denmark, Sept. 2017. Association for
ComputationalLinguistics.
[51] J.Wei,X.Wang,D.Schuurmans,M.Bosma,B.Ichter,F.Xia,E.Chi,Q.Le,andD.Zhou.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels,2023.
[52] T.Wei,J.Luan,W.Liu,S.Dong,andB.Wang. Cmath: Canyourlanguagemodelpasschinese
elementaryschoolmathtest?,2023.
[53] C.Yang,X.Wang,Y.Lu,H.Liu,Q.V.Le,D.Zhou,andX.Chen. Largelanguagemodelsas
optimizers. arXivpreprintarXiv:2309.03409,2023.
[54] L.Yang,H.Yang,W.Cheng,L.Lin,C.Li,Y.Chen,L.Liu,J.Pan,T.Wei,B.Li,L.Zhao,
L.Wang,B.Zhu,G.Li,X.Wu,X.Luo,andR.Hu. Skymath: Technicalreport,2023.
[55] S.Yao,D.Yu,J.Zhao,I.Shafran,T.L.Griffiths,Y.Cao,andK.Narasimhan. Treeofthoughts:
Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,
2023.
[56] Yi. A series of large language models trained from scratch by developers at 01-ai. https:
//github.com/01-ai/Yi,2023.
[57] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.
Metamath: Bootstrap your own mathematical questions for large language models. arXiv
preprintarXiv:2309.12284,2023.
[58] Z.Yuan,H.Yuan,C.Li,G.Dong,K.Lu,C.Tan,C.Zhou,andJ.Zhou. Scalingrelationshipon
learningmathematicalreasoningwithlargelanguagemodels,2023.
[59] Z.Yuan,H.Yuan,C.Tan,W.Wang,andS.Huang. Howwelldolargelanguagemodelsperform
inarithmetictasks?,2023.
[60] X.Yue,X.Qu,G.Zhang,Y.Fu,W.Huang,H.Sun,Y.Su,andW.Chen. Mammoth: Building
mathgeneralistmodelsthroughhybridinstructiontuning. arXivpreprintarXiv:2309.05653,
2023.
[61] A.Zeng,X.Liu,Z.Du,Z.Wang,H.Lai,M.Ding,Z.Yang,Y.Xu,W.Zheng,X.Xia,etal.
Glm-130b: Anopenbilingualpre-trainedmodel. arXivpreprintarXiv:2210.02414,2022.
[62] D.Zhang,Z.Hu,S.Zhoubian,Z.Du,K.Yang,Z.Wang,Y.Yue,Y.Dong,andJ.Tang. Sciglm:
Trainingscientificlanguagemodelswithself-reflectiveinstructionannotationandtuning,2024.
[63] X.Zhang,C.Li,Y.Zong,Z.Ying,L.He,andX.Qiu. Evaluatingtheperformanceoflarge
languagemodelsongaokaobenchmark. 2023.
[64] W.Zhao,M.Shang,Y.Liu,L.Wang,andJ.Liu. Ape210k: Alarge-scaleandtemplate-rich
datasetofmathwordproblems,2020.
[65] L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.P.Xing,
H.Zhang,J.E.Gonzalez,andI.Stoica. Judgingllm-as-a-judgewithmt-benchandchatbot
arena,2023.
[66] W.Zhong,R.Cui,Y.Guo,Y.Liang,S.Lu,Y.Wang,A.Saied,W.Chen,andN.Duan. Agieval:
Ahuman-centricbenchmarkforevaluatingfoundationmodels,2023.
15[67] J.Zhou,Z.Chen,D.Wan,B.Wen,Y.Song,J.Yu,Y.Huang,L.Peng,J.Yang,X.Xiao,etal.
Characterglm: Customizingchineseconversationalaicharacterswithlargelanguagemodels.
arXivpreprintarXiv:2311.16832,2023.
[68] Z.Zhou,Q.Wang,M.Jin,J.Yao,J.Ye,W.Liu,W.Wang,X.Huang,andK.Huang.Mathattack:
Attackinglargelanguagemodelstowardsmathsolvingability.arXivpreprintarXiv:2309.01686,
2023.
A CaseStudy
A.1 CaseStudyofMath-Critique
WehaveprovidedseveralexamplesofscoringbyMath-Critique. IntheexamplesfromTable6and
Table7,weobservedthatthemodelprovidedcorrectanswers. However,theanswerformatsdiffered
fromthestandardanswers,specificallyregardingfractionexpressionandtheselectionofunknown
variables. Thesedifferencesareequivalent,yettraditionalevaluationmethodsstruggletojudgethem
accurately. However,Math-Critiquecorrectlyscoredthesetwoexamplesandprovidedreasonable
evaluations.
IntheexamplefromTable8,themodelmadeamistakeinthecalculationprocess. Math-Critique
accuratelypinpointedtheerrorlocation,andsincethemodelcorrectlysolvedapartoftheproblem,
Math-Critiqueawardedascoreof3points.
A.2 CaseStudyofMathematicalModels
Here are a few comparisons between ChatGLM3-32B-Math(ChatGLM3-32B-SFT-2312 +
RFT&DPO) and other models. In the example from Table 9, the problem is a math question
ofChinesejuniorhighschooldifficulty. DuringthesolutionprocessbyGPT-4-0613,anerroroc-
curredinsolvingtheequation. ChatGLM3-32B-SFT-2312didnotcorrectlyunderstandthequestion.
ChatGLM3-32B-Mathcorrectlylistedtheequationandaccuratelysolveditusingthefactorization
method.
IntheexamplefromTable10,bothGPT-4-0613andChatGLM3-32B-Mathprovidedthecorrect
answers, but the difference lies in that ChatGLM3-32B-Math offered a very detailed derivation
process. Webelievethatdetailedderivationaidsinunderstandingforusersandhelpspreventerrors
thatmayoccurduringthemodel’sstep-skipping.
IntheexamplefromTable11,originatingfromtheHungryTest,ChatGLM3-32B-Mathcorrectly
conductedtheanalysisandprovidedthesolution. Incontrast,Qwen-Max,despitebeingaccuratein
mostprocesses,madeasimplificationerrorintheexpressionsforS6andS7,leadingtoanincorrect
resultdespiteprecisecalculations.
A.3 CaseStudyofErrors
In the first error example shown in Table 12, ChatGLM3-32B-Math correctly listed the matrix
equationsbutmadeamistakeintransferringtermsduringthecomplexcalculationprocess. Thisled
toanincorrectsolutiondespitetheoverallprocessbeingcorrect.
InthesecondexampleshowninTable13,thequestionincludesanimage. Thisquestionoriginates
from MATH, and although the image is provided in Asymptote code, the language model still
strugglestounderstandthemeaningoftheimage. Thisdemonstratesadeficiencyinourmodel’s
abilitytoprocessimages. Infact,withintheMATHtestset,forquestionsthatincludeimages,our
model’saccuracyrateisonly23%,significantlylowerthanthe40%accuracyrateacrosstheentire
dataset.
InthethirdexampleshowninTable14,wecanobservecertaindeficienciesinthecomputational
accuracyofthelanguagemodel. Inthefirstcolumnofthetable,themodelcorrectlycalculatesthe
multiplicationofintegersandasingledecimal.Still,inthesecondcolumn,whencalculating424*1.06,
themodelincurredanerrorofapproximately0.2%. Thiserrorledtosubsequentcalculationsbeing
incorrectduetotheaccumulationoferrors.
16Table6: Math-CritiqueExample(1)
Question 1/x+x三次方的原函数是多少。
Question Whatistheoriginalfunctionof1/x+xcubed.
(Translate)
Reference 首先，我们可以将1/x+x3重新写成x−1+x3，然后利用幂函数和多项式函数
的求导原理求出它的原函数。具体来说，我们得到：
(cid:90) (cid:90) (cid:90) 1
(x−1+x3)dx= x−1dx+ x3dx=ln|x|+ x4+C,
4
其中C是不定积分常数。因此，1/x+x3的原函数是ln|x|+ 1x4+C。
4
Reference First,wecanrewrite1/x+x3asx−1+x3,andthenusethederivationprinciplesof
(Translate) powerfunctionsandpolynomialfunctionstofinditsoriginalfunction.Specifically,we
get:
(cid:90) (cid:90) (cid:90) 1
(x−1+x3)dx= x−1dx+ x3dx=ln|x|+ x4+C,
4
whereCistheindefiniteintegralconstant.Therefore,theoriginalfunctionof1/x+x3
isln|x|+ 1x4+C.
4
Model An- 要求函数f(x)= 1+x3的原函数（不定积分），我们需要分别对这两项进行积
x
swer by 分。对于 1，其原函数是ln|x|（自然对数）。对于x3，我们可以利用幂函数
GLM4 的积分规则x ，即对于xn（其中neq−1），其原函数是 xn+1。因此，x3的原函
n+1
数是 x4。将这两项的原函数相加，我们得到f(x)的原函数：
4
(cid:90) (cid:18) 1 (cid:19) x4
+x3 dx=ln|x|+ +C
x 4
其中C 是积分常数。这个结果表示在定义域内（除了x=0的情况，因为 1 在
x
x=0处未定义），f(x)的一个原函数。由于ln|x|在x=0处未定义，所以这
个原函数在x=0处也是不连续的。
Model Tofindtheprimitivefunction(indefiniteintegral)ofthefunctionf(x)= 1 +x3,we
x
Answer needtointegratethesetwotermsseparately.For 1,theoriginalfunctionisln|x|(natural
x
(Translate) logarithm).Forx3,wecanusetheintegrationruleofthepowerfunction,thatis,forxn
(whereneq−1),theoriginalfunctionisxn+1.Therefore,theoriginalfunctionofx3is
n+1
x4. Addingtheoriginalfunctionsofthesetwoterms,wegettheoriginalfunctionof
4
f(x):
(cid:90) (cid:18) 1 (cid:19) x4
+x3 dx=ln|x|+ +C
x 4
whereCistheintegrationconstant.Thisresultmeansthatwithinthedomain(except
forthecaseofx = 0,since 1 isundefinedatx = 0),f(x)Aprimitivefunctionof.
x
Sinceln|x|isundefinedatx=0,theoriginalfunctionisalsodiscontinuousatx=0.
Critique Re- AI助手的回答非常准确和详细，它正确地解释了如何求解给定函数的原函
sult 数。AI助手首先分别对函数的两部分进行积分，然后将它们的结果相加，得
到了原函数。此外，AI助手还注意到了在x=0处，原函数是不连续的，这是一
个重要的观察。总的来说，AI助手的回答与参考答案非常接近，没有明显的缺
点。因此，我会给它一个高分。评分：[[9]
Critique Re- TheAIassistant’sanswerisveryaccurateanddetailed,anditcorrectlyexplainshow
sult (Trans- tosolvetheoriginalfunctionofagivenfunction. TheAIassistantfirstintegratesthe
late) twopartsofthefunctionseparatelyandthenaddstheirresultstoobtaintheoriginal
function.Inaddition,theAIassistantalsonoticedthatatx=0,theoriginalfunctionis
discontinuous,whichisanimportantobservation.Overall,theAIassistant’sanswers
areveryclosetothereferenceanswers,withnoobviousshortcomings. Therefore,I
wouldgiveitahighscore.Rating:[[9]
17Table7: Math-CritiqueExample(2)
Question 三阶矩阵A有三个特征值0,3,5，对应的特征向量分别为u,v,w，则线性方程组Ax=v+w的通
解为？
Question Thethird-ordermatrixAhasthreeeigenvalues0,3,and5,andthecorrespondingeigenvectorsareu,
(Translate) v,andwrespectively.WhatisthegeneralsolutiontothelinearequationsystemAx=v+w?
Reference 给定矩阵A有特征值0,3,5，对应的特征向量为u,v,w。线性方程组Ax=v+w，我们可以
写作Au=0*u，Av=3*v，和Aw=5*w。所以A(1/3*v)=v,A(1/5*w)=w，左右两边相加
得A(1/3*v+1/5*w)=v+w。又因为Ax=v+w，所以1/3*v+1/5*w是Ax=v+w的一个特
解，因为Au=0，所以u是Au=0的基础解系，因此方程Ax=v+w的通解是x=ku+1/3*v+
1/5*w。
Reference AgivenmatrixAhaseigenvalues0,3,and5,andthecorrespondingeigenvectorsareu,v,andw.
(Translate) ForthelinearsystemofequationsAx=v+w,wecanwriteAu=0*u,Av=3*v,andAw=5*
w.SoA(1/3*v)=v,A(1/5*w)=w,addingtheleftandrightsidesgivesA(1/3*v+1/5*w)=v+w.And
becauseAx=v+w,1/3*v+1/5*wisaspecialsolutionofAx=v+w.BecauseAu=0,souisthe
basicsolutionsystemofAu=0,sotheequationAxThegeneralsolutionof=v+wisx=ku+1/3*v
+1/5*w.
Model 给定三阶矩阵A的三个特征值分别为 0,3,5，对应的特征向量分别为 u,v,w。要找线性
Answerby 方程组Ax = v+w的通解，我们可以利用特征值和特征向量的性质。首先，我们知道
ChatGLM3- Av=3v和Aw=5w。因此，v和w分别是特征值3和5对应的特征向量。线性方程组
32B-Math Ax=v+w可以重写为Ax−v−w =0。我们可以将x写成x=pu+qv+rw，其中
p,q,r是待定系数。将x代入方程Ax−v−w=0，我们得到：
A(pu+qv+rw)−v−w=0
...由于v和w是线性无关的特征向量（因为它们对应不同的特征值），上式成立的唯一可
能是3q−1=0和5r−1=0。解这两个方程，我们得到q = 1 和r= 1。因此，方程
3 5
Ax=v+w的通解为：
1 1
x=pu+ v+ w
3 5
其中p是任意实数。所以，通解的形式是：
1 1
x=pu+ v+ w
3 5
这里的u是特征值0对应的特征向量，v和w分别是特征值3和5对应的特征向量。p是
任意实数，可以取任何值。
Model Giventhethreeeigenvaluesofthethird-ordermatrixAare0,3,5,thecorrespondingeigenvectors
Answer areu,v,w.TofindthegeneralsolutiontoasystemoflinearequationsAx=v+w,wecanexploit
(Translate) thepropertiesofeigenvaluesandeigenvectors. Firstofall,weknowAv = 3vandAw = 5w.
Therefore,vandwaretheeigenvectorscorrespondingtotheeigenvalues3and5respectively.The
systemoflinearequationsAx = v+wcanberewrittenasAx−v−w = 0. Wecanwritex
asx = pu+qv+rw,wherep,q,raretheundeterminedcoefficients. Substitutingxintothe
equationAx−v−w=0,weget:
A(pu+qv+rw)−v−w=0
...Therefore,thegeneralsolutionofequationAx=v+wis:
1 1
x=pu+ v+ w
3 5
Wherepisanyrealnumber.So,thegeneralsolutionisintheform:
1 1
x=pu+ v+ w
3 5
Hereuistheeigenvectorcorrespondingtotheeigenvalue0,vandwaretheeigenvectorscorrespond-
ingtotheeigenvalues3and5respectively.pisanyrealnumberandcantakeonanyvalue.
Critique AI助手的回答非常详细，它首先解释了如何利用特征值和特征向量的性质来解决这个问
Result 题，然后逐步进行了详细的步骤，包括将方程重写，将x写成待定系数的形式，然后将这些
系数代入方程，最后解出这些系数。这个答案不仅准确，而且深入，提供了详细的解释和
步骤，使得读者能够理解如何解决这个问题。与参考答案相比，AI助手的答案更加详细和
深入，因此我认为AI助手的回答质量非常高。评分：[[9]]。
Critique TheAIassistant’sanswerisverydetailed.Itfirstexplainshowtousethepropertiesofeigenvalues
Result andeigenvectorstosolvethisproblem,andthengoesthroughthedetailedsteps,includingrewriting
(Translate) theequation,writingxintheformofundeterminedcoefficients,andthenconvertingtheseThe
coefficientsaresubstitutedintotheequationandfinallythecoefficientsaresolved.Thisanswerisnot
onlyaccuratebutalsoin-depth,providingdetailedexplanationsandstepssothereadercanunderstand
howtosolvetheproblem.Comparedwiththereferenceanswers,theAIassistant’sanswersaremore
detailedandin-depth,soIthinkthequalityoftheAIassistant’sanswersisveryhigh.Rating:[[9]].
18Table8: Math-CritiqueExample(3)
Question 二元一次方程4x-3y=12,当x=0,1,2,3时,y=
Question Quadraticequation4x-3y=12,whenx=0,1,2,3,y=
(Translate)
Reference 将二元一次方程4x-3y=12改写为y的表达式： 4x-3y=12 -3y = -4x + 12 y =
(4/3)x - 4 当x=0时，y=(-4)； 当x=1时，y=(4/3)-4=-8/3； 当x=2时，y=(8/3)-4=-
4/3；当x=3时，y=(4)-4=0。因此，当x分别为0、1、2、3时，y的值分别为-4、-
8/3、-4/3和0。
Reference Rewritethelinearequation4x-3y=12intotheexpressionofy: 4x-3y=12-3y=-4x+
(Translate) 12y=(4/3)x-4Whenx=When0, y=(-4); Whenx=1, y=(4/3)-4=-8/3; Whenx=2,
y=(8/3)-4=-4/3;Whenx=3,y=(4)-4=0.Therefore,whenxis0,1,2,and3respectively,
thevaluesofyare-4,-8/3,-4/3,and0respectively.
Model An- *给定二元一次方程:4x-3y=12*将x替换为给定的数值,可以得到y的值:*x=0
swer by 代入方程,得到:-3y=12,y=-4*x=1代入方程,得到:4-3y=12,-3y=8,y=-2*
Claude-2 x=2代入方程,得到:8-3y=12,-3y=4,y=0*x=3代入方程,得到:12-3y=12,
-3y=0,y=2*综上,当x=0,1,2,3时,y依次等于-4,-2,0,2。所以应该填写:-4,-2,0,2。
Model *Givenalinearequationoftwovariables: 4x-3y=12*Replacexwiththegiven
Answer value,youcangetthevalueofy:*x=0Substituteintotheequation,youget:-3y=12,
(Translate) y=-4*x=1Substitutingintotheequation,weget:4-3y=12,-3y=8,y=-2*x=2
Substitutingintotheequation,weget:8-3y=12,-3y=4,y=0*x=3Substituting
intotheequation,weget:12-3y=12,-3y=0,y=2*Insummary,whenx=0,1,2,3,y
isequalto-4inturn,-2,0,2.Soyoushouldfillin:-4,-2,0,2.
Critique Re- AI助手的回答在计算过程中出现了错误。当x=1时，AI助手错误地计算出y=-2，
sult 而实际上应该是y=-8/3。同样，当x=3时，AI助手错误地计算出y=2，而实际上
应该是y=0。因此，AI助手的回答在准确性和深度上都存在问题。然而，AI助
手的回答在结构和详细程度上是正确的，它详细地解释了如何计算y的值，并给
出了每个x值对应的y值。总的来说，AI助手的回答虽然有一些错误，但是仍然
提供了一些有用的信息。因此，我会给AI助手的回答打分为3。评分：[[3]]
Critique Re- TherewasanerrorinthecalculationprocessoftheAIassistant’sanswer.Whenx=1,
sult (Trans- theAIassistantincorrectlycalculatesy=-2,whenitshouldactuallybey=-8/3.Likewise,
late) whenx=3,theAIassistantincorrectlycalculatesy=2,whenitshouldactuallybey=0.
Therefore,theAIassistant’sanswershaveproblemswithaccuracyanddepth.However,
theAIassistant’sansweriscorrectinstructureandlevelofdetail.Itexplainsindetail
howtocalculatethevalueofyandgivesthecorrespondingyvalueforeachxvalue.
Overall,althoughtheAIassistant’sanswerscontainedsomeerrors,theystillprovided
someusefulinformation.Therefore,IwouldratetheAIassistant’sanswera3.Rating:
[[3]]
19Table9: ChatGLM3-32B-MathExample(1)
Question 已知一个两位数的十位数字比个位数字大2，两位数字的积比这个两位数小34，求这个两位
数。
Question Givenatwo-digitnumberwherethetensdigitis2greaterthantheunitsdigit,andtheproductofthe
(Trans- digitsis34lessthanthenumberitself,findthistwo-digitnumber.
late)
Reference 设这个两位数为10x+y（x表示十位数，y表示个位数），根据题意可得：（1）x=y+2
（十位数比个位数大2）（2）xy=10x+y-34（两位数字的积比这个两位数小34）将（1）
式代入（2）式得到：(y+2)y=10(y+2)+y-34整理得到y2+2y = 11y+20−34，整理得
y2−9y+14=0，解方程得到y=7或y=2.所以x=7+2=9或x=2+2=4所以这个数为97或者42.答
案：97或42。
Reference Letthetwo-digitnumberbe10x+y(wherexisthetensdigitandyistheunitsdigit).Accordingto
(Trans- theproblem:(1)x=y+2(thetensdigitis2greaterthantheunitsdigit)(2)xy=10x+y−34
late) (theproductofthedigitsis34lessthanthenumberitself).Substituting(1)into(2)gives:(y+2)y=
10(y+2)+y−34,whichsimplifiestoy2+2y = 11y+20−34,andfurthersimplifiesto
y2−9y+14 = 0. Solvingtheequationyieldsy = 7ory = 2. Thus,x = 7+2 = 9or
x=2+2=4,makingthenumber97or42.Answer:97or42.
GPT-4- 设这个两位数为10x+y，其中x为十位数，y为个位数。根据题意，我们有以下两个方程：x
0613 =y+2（十位数字比个位数字大2）xy=10x+y-34（两位数字的积比这个两位数小34）将
第一个方程代入第二个方程，我们得到：(y+2)y=10(y+2)+y-34解这个方程，我们得到y
=4。将y=4代入第一个方程，我们得到x=6。所以，这个两位数是64。
GPT- Letthetwo-digitnumberbe10x+y,wherexisthetensdigitandyistheunitsdigit.According
4-0613 totheproblem,wehavetwoequations:x=y+2(thetensdigitis2greaterthantheunitsdigit)
(Trans- andxy=10x+y−34(theproductofthedigitsis34lessthanthenumberitself).Substitutingthe
late) firstequationintothesecond,weget:(y+2)y=10(y+2)+y−34.Solvingthisequationyields
y=4.Substitutingy=4backintothefirstequationgivesx=6.Therefore,thetwo-digitnumber
is64.
ChatGLM3- 设这个两位数为 10a+b，其中a是十位数字，b是个位数字。根据题意，我们有以下两
32B-SFT 个条件：1. 十位数字比个位数字大2，即a=b+22. 两位数字的积比这个两位数小34，
即ab<10a+b−34现在我们来解这个两个方程。从第一个条件，我们得到a=b+2。
将a替换进第二个条件中，我们得到：b(b+2) < 10(b+2)−34展开并简化这个不等
式：b2+2b<10b+20−34b2+2b<7b−14移项得到：b2−5b+14<0这是一个一元
二次不等式，我们可以求解这个不等式得到b的范围。因式分解得到：(b−1)(b−14)<0
根据一元二次不等式的性质，我们知道b应该在两个根之间，即：1 < b < 14但是由于
a=b+2，且a是一个十位数（即1≤a≤9），所以b必须小于9。因此b的可能取值
为 2,3,4,5,6,7,8。现在我们来检查b的每一个值，并找出对应的a和两位数。当b = 2
时，a=0，这不满足a为十位数的要求；当b=3时，a=1，这也不满足a为十位数的要
求；...当b=7时，a=5，这满足要求，并且a和b的积是35，确实比35小34，满足题
目的要求。所以这个两位数是57。
ChatGLM3- Letthetwo-digitnumberbe10a+b,whereaisthetensdigitandbistheunitsdigit.Accordingto
32B-SFT theproblem,wehavetwoconditions:1.Thetensdigitis2greaterthantheunitsdigit,i.e.,a=b+2
(Trans- 2. Theproductofthedigitsis34lessthanthenumberitself,i.e.,ab<10a+b−34Nowlet’s
late) solvethesetwoequations.Fromthefirstcondition,wegeta=b+2.Substitutingaintothesecond
condition,weget: b(b+2) < 10(b+2)−34Expandingandsimplifyingthisinequalitygives:
b2+2b < 10b+20−34b2+2b < 7b−14Rearranginggives: b2−5b+14 < 0Thisisa
quadraticinequality,whichwecansolvetofindtherangeforb.Factoringgives:(b−1)(b−14)<0
Accordingtothepropertiesofquadraticinequalities,weknowbshouldbebetweenthetworoots,
i.e.,1<b<14.However,sincea=b+2andaisatensdigit(1≤a≤9),bmustbelessthan
9. Thus,thepossiblevaluesforbare2,3,4,5,6,7,8. Nowwecheckeachvalueofbtofindthe
correspondingaandthetwo-digitnumber.Whenb=2,a=0,whichdoesnotmeettherequirement
foratobeatensdigit;whenb=3,a=1,whichalsodoesnotmeettherequirement;... when
b=7,a=5,whichmeetstherequirement,andtheproductofaandbis35,indeedlessthan35by
34,meetingtheproblem’srequirement.Therefore,thetwo-digitnumberis57.
ChatGLM3- 设这个两位数为10a+b，其中a是十位数字，b是个位数字。根据题目条件，我们有以下两
32B-Math 个方程：1.十位数字比个位数字大2，即a=b+2。2.两位数字的积比这个两位数小34，
即ab=(10a+b)−34。我们可以用第一个方程代入第二个方程来解这个问题。将a=b+2
代入第二个方程，得到：b(b+2)=10(b+2)+b−34展开并简化：b2+2b=10b+20+b−34
b2+2b=11b−14移项得到一个二次方程：b2−9b+14=0这是一个二次方程，我们可
以通过分解因式来解它：(b−7)(b−2)=0所以，b的值可以是7或者2。如果b=7，则
a=b+2=9，所以这个两位数是97。如果b=2，则a=b+2=4，所以这个两位数是
42。因此，这个两位数是42或97。
ChatGLM3- Letthetwo-digitnumberbe10a+b,whereaisthetensdigitandbistheunitsdigit.Accordingto
32B-Math theproblemconditions,wehavetwoequations:1.Thetensdigitis2greaterthantheunitsdigit,i.e.,
(Trans- a=b+2.2.Theproductofthedigitsis34lessthanthenumberitself,i.e.,ab=(10a+b)−34.
late) We can solve this problem by substituting the first equation into the second. Doing so gives:
b(b+2)=10(b+2)+b−34Expandingandsimplifyingyields:b2+2b=10b+20+b−34
b2+2b=11b−14Rearrangingintoaquadraticequationgives:b2−9b+14=0Thisquadratic
equationcanbesolvedbyfactoring:(b2−07)(b−2)=0Thus,bcanbeeither7or2.Ifb=7,then
a=b+2=9,makingthetwo-digitnumber97.Ifb=2,thena=b+2=4,makingthetwo-digit
number42.Therefore,thetwo-digitnumbercouldbeeither42or97.Table10: ChatGLM3-32B-MathExample(2)
Question 两个相互独立的事件A，B都不发生的概率为1/9，A发生而B不发生的概率与A不发生B发生
的概率相等，则P(A)等于？
Question GiventwoindependenteventsAandB,wheretheprobabilityofneitherAnorBoccurringis1/9,
(Trans- andtheprobabilityofAoccurringwithoutBequalstheprobabilityofBoccurringwithoutA,whatis
late) P(A)?
Reference 由题意的：P(AB¯)=P(BA¯),即P(A)(1-P(B))=P(B)(1-P(A))所以P(A)=P(B)又P(A¯B¯)= 1所
9
以P(A¯)= 1所以P(A)= 2
3 3
Reference Fromtheproblemstatement:P(AandnotB)=P(BandnotA),i.e.,P(A)(1-P(B))=P(B)(1-P(A)).
(Trans- Thus,P(A)=P(B).Also,P(neitherAnorB)=1/9.Therefore,P(notA)=1/3,leadingtoP(A)=2/3.
late)
GPT-4- 设A发生的概率为p，B发生的概率为q，则A，B都不发生的概率为(1-p)(1-q)=1/9，A发生
0613 而B不发生的概率为p(1-q)，A不发生B发生的概率为(1-p)q。由题意知，p(1-q)=(1-p)q，解
得p=q。将p=q代入(1-p)(1-q)=1/9，得(1−p)2=1/9，解得p=2/3或p=-1/3。由于概率不能为
负，所以p=2/3，即P(A)=2/3。
GPT- LettheprobabilityofAoccurringbep,andtheprobabilityofBoccurringbeq.Then,theprobability
4-0613 ofneitherAnorBoccurringis(1-p)(1-q)=1/9.TheprobabilityofAoccurringwithoutBisp(1-q),
(Trans- andtheprobabilityofBoccurringwithoutAis(1-p)q.Fromtheproblem,p(1-q)=(1-p)q,leadingto
late) p=q.Substitutingp=qinto(1-p)(1-q)=1/9gives(1−p)2=1/9,resultinginp=2/3orp=-1/3.
Sinceprobabilitycannotbenegative,p=2/3,i.e.,P(A)=2/3.
ChatGLM3- 设事件A发生的概率为P(A)，事件B发生的概率为P(B)。已知A和B相互独立，那么事件A不
32B-SFT 发生而B发生的概率为P(B)(1-P(A))，事件A发生而B不发生的概率为P(A)(1-P(B))。根据题
目条件，A发生而B不发生的概率与A不发生B发生的概率相等，即：P(A)(1-P(B))=P(B)(1-
P(A))由于A和B相互独立，我们可以将上式简化为：P(A)-P(A)P(B)=P(B)-P(B)P(A)由
于A和B的概率都是非负的，上式意味着P(A)=P(B)。但独立事件的概率相等并不一定意味
着A和B是同一个事件，因为它们可以有相同的或不同的概率但独立性保持不变。现在，我
们知道了两个事件独立，且都不发生的概率为1/9。这意味着：P(A)*P(B)=1/9由于P(A)
=P(B)，我们可以将这个等式写成：P(A)2 =1/9因为P(A)是概率，所以它必须在0和1之
间。唯一满足这个等式的概率值是：P(A)=æ(1/9)=1/3因此，P(A)=1/3。既然P(A)=
P(B)，那么P(B)也等于1/3。
ChatGLM3- 设事件A发生的概率为P(A)，事件B发生的概率为P(B)。根据题意，我们可以列出以下关
32B-Math 系：1.事件A和事件B都不发生的概率为1/9，即：
1
P(A不发生且B不发生)=P(A不发生)×P(B不发生)=(1−P(A))×(1−P(B))=
9
2.事件A发生而事件B不发生的概率与事件A不发生而事件B发生的概率相等，即：
P(A发生且B不发生)=P(A不发生且B发生)
根据概率的性质，我们有：
P(A发生且B不发生)=P(A)×(1−P(B))
P(A不发生且B发生)=(1−P(A))×P(B)
由于这两个概率相等，我们可以得到：
P(A)×(1−P(B))=(1−P(A))×P(B)
现在我们有两个方程：1. (1−P(A))×(1−P(B)) = 1 2. P(A)×(1−P(B)) = (1−
9
P(A))×P(B)我们可以通过这两个方程来解出P(A)和P(B)的值。首先，从第二个方程中我
们可以得到：
P(A)−P(A)×P(B)=P(B)−P(A)×P(B)
P(A)=P(B)
这意味着事件A和事件B发生的概率相等。现在，我们用这个结论来解第一个方程：
1
(1−P(A))×(1−P(A))=
9
1
(1−P(A))2=
9
取平方根得到：
1 1
1−P(A)= 或 1−P(A)=−
3 3
由于概率值不能为负，我们只取正值：
1
1−P(A)=
3
2
P(A)=
3
所以，事件A发生的概率P(A)等于2/23。1Table11: ChatGLM3-32B-MathExample(3)
Question Thefirsttermofanarithmeticsequenceis18. Thesumofthefirstsixtermsofthe
sequenceisequaltothesumofthefirstseventerms.Showthatthesumofthefirst13
termsofthesequenceis0andcalculatethe13thtermofthesequence.
Reference Ifthesumofthefirstsixtermsofasequenceisequaltothesumofthefirstseventerms
thentheseventhtermmustbe0.Letdbethecommondifferenceofthesequence.Now
18+6d=0andsod=-3.a13=18+12*(-3)=-18
ChatGLM3- Let’sdenotethecommondifferenceofthearithmeticsequenceasd.Theformulafor
32B-Math then-thtermofanarithmeticsequenceisa = a +(n−1)d,wherea isthefirst
n 1 1
termandnisthetermnumber.
Giventhatthefirstterma = 18,theformulaforthen-thtermbecomesa = 18+
1 n
(n−1)d.
The sum S of the first n terms of an arithmetic sequence is given by the formula
n
S = n[2a +(n−1)d].
n 2 1
Accordingtotheproblem,thesumofthefirstsixterms(S )isequaltothesumofthe
6
firstseventerms(S ).Wecanwritethisrelationshipas:
7
S =S
6 7
6[2·18+(6−1)d]= 7[2·18+(7−1)d]
2 2
3[36+5d]= 7[36+6d]
2
108+15d= 7 ·36+ 7 ·6d
2 2
108+15d=126+21d
Now,wesolveford:
15d−21d=126−108
−6d=18
d=−3
Nowthatwehavethecommondifferenced=−3,wecanfindthe13thterma :
13
a =18+(13−1)·(−3)
13
a =18−12·3
13
a =18−36
13
a =−18
13
Now,let’sfindthesumofthefirst13termsS usingtheformula:
13
S = 13[2·18+(13−1)·(−3)]
13 2
S = 13[36−12·3]
13 2
S = 13[36−36]
13 2
S = 13 ·0
13 2
S =0
13
So,thesumofthefirst13termsofthesequenceisindeed0,andwe’vealsocalculated
thatthe13thtermofthesequenceis−18.
B PromptTemplate
InFigure5,wepresenttheChineseversionexampleofthepromptsutilizedbyMath-Critiqueand
thecorrespondingEnglishtranslations. Inpracticalapplication,weemploytheChineseversionof
thepromptsforallinquiries.
C EvaluationSettings
C.1 EvaluationSettingsforAcademicDatasets
Givenourfocusonevaluatingzero-shotcapabilities,traditionalmethodsofanswerverificationfail
toprovideaccurateassessments. Consequently, wehaveadaptedourevaluationmethodologyas
follows:
1. Extractthefinalsentenceoftheresponse(demarcatedbyadoublenewlinecharacter)or
promptthemodelwith"Therefore,theansweris".
22Table12: ChatGLM3-32B-MathErrorCase(1)
Question Thematrices
   
2 a −1 b
  and  16 
   
−3 −1 3 1
16 8
areinverses.Entertheorderedpair(a,b).
Reference (6,−3)
8
ChatGLM3- Twomatricesareinversesiftheirproductistheidentitymatrix.Theidentitymatrixfor
32B-Math a2x2matrixis
 
1 0
 .
 
0 1
So,weneedtofindthevaluesofaandbsuchthat
    
2 a −1 b 1 0
  16 = .
    
−3 −1 3 1 0 1
16 8
Wecancalculatetheproductasfollows:
    
2 a −1 b 2(−1)+a(3) 2b+a(1)
  16 = 16 16 8 .
    
−3 −1 3 1 −3(−1)−1(3) −3b−1(1)
16 8 16 16 8
Thisgivesustheequations2(−1)+a(3)=1,
16 16
2b+a(1)=0,
8
−3(−1)−1(3)=0
16 16
−3b−1(1)=1
8
Simplifying,weget−1 + 3a =1,
8 16
2b+ a =0,
8
3 − 3 =0,
16 16
−3b− 1 =1.
8
Fromthethirdequation,wehave a =2b.Multiplyingbothsidesby8,wegeta=16b.
8
Substitutingintothefirstequation,weget
1 3(16b)
− + =1,
8 16
whichsimplifiesto3b= 5.Therefore,b= 5.
8 24
Substitutingintothesecondequation,weget
5 16b
2( )+ =0,
24 8
whichsimplifiestob=−5.Thisisconsistentwithourpreviouscalculation,sob= 5
16 24
isthecorrectvalue.
Finally,substitutingintothefourthequation,weget
5 1
−3( )− =1,
24 8
whichsimplifiestoa= 1.
16
(cid:18) (cid:19)
1 5
Therefore,(a,b)= , .
16 24
23Table13: ChatGLM3-32B-MathErrorCase(2)
Question n the graph below, each grid line counts as one unit. The line shown below passes
throughthepoint(1001,n)(notshownongraph).Findn.
y
x
(note:thegraphwaswritteninAsymptotecodeinoriginalquestion)
Reference 575
ChatGLM3- Thelinepassesthroughthepoint(1001,n),whichmeansthatwhenx=1001,y=n.
32B-Math Wecanfindtheslopeofthelinebylookingatthechangeinydividedbythechangein
xbetweentwopointsontheline.
Ifweconsiderthepoint(999,7)andthepoint(1001,n),thechangeinxis1001−999=
2andthechangeinyisn−7.
Sincethelinepassesthroughthepoint(1001,n),wecansaythattheslopeofthelineis
n−7.
2
Weknowthatthelinealsopassesthroughthepoint(0,−2.71),sowecanusethispoint
tofindtheslopeoftheline.Thechangeinxis0−(−10)=10andthechangeinyis
−2.71−(−7)=4.29.
Nowwecansetthetwoslopesequaltoeachotherandsolveforn:
n−7 4.29
=
2 10
Crossmultiplyinggives:
10(n−7)=2·4.29
10n−70=8.58
Adding70tobothsidesgives:
10n=78.58
Finally,dividingbothsidesby10gives:
n=7.858
Butsincethegridlinescountasoneunit, weneedtoroundntothenearestwhole
number.7.858iscloserto8thanitisto7,son= 8 .
24Table14: ChatGLM3-32B-MathErrorCase(3)
Question Intheyear2018factoryAproducedgoodsworth500millionforints,whilefactoryB
producedgoodsworth400millionforints.Accordingtotheirlong-termplans,factory
Awillincreaseproductionby5%eachyear,whilefactoryBwillincreaseproductionby
6%peryear.
The above plans were published on an economy website. A debate started in the
commentsectionofthearticle:oneuserclaimedthatthedifferencebetweenthevolumes
ofproductionbetweenthetwofactorieswouldgetsmallerandsmallerinthenextcouple
ofyears.
Calculatethemissingvaluesofthetablebelowandshowthattheaboveclaimisnot
true.
|Year|ProductionvolumeoffactoryA(millionFt)|ProductionvolumeoffactoryB
(millionFt)|
|——|——————————————–|———————————————|
|2018|500|400|
|2019|||
|2020|||
|2021|||
Reference |Year|ProductionvolumeoffactoryA(millionFt)|ProductionvolumeoffactoryB
(millionFt)|
|——|——————————————–|———————————————|
|2018|500|400|
|2019|525|424|
|2020|551.3|449.4|
|2021|578.8|476.4|
Thedifferencebetweentheproductionvaluesinthegivenyears:
525–424=101,
551.3–449.4=101.9and
578.8–476.4=102.4(millionforints).
Asthedifferencebetweentheproductionvaluesisincreasing(intheperiodinquestion),
thestatementis,infact,false.
ChatGLM3- Tocalculatetheproductionvolumesfortheyears2019,2020,and2021,wewillapply
32B-Math thepercentageincreasestotheproductionofeachfactory.
ForFactoryA:-2018production:500millionFt-5
ForFactoryB:-2018production:400millionFt-6
Let’sfillinthetable:
|Year|ProductionvolumeoffactoryA(millionFt)|ProductionvolumeoffactoryB
(millionFt)|
|——|——————————————–|———————————————|
|2018|500|400|
|2019|500*1.05=525|400*1.06=424|
|2020|525*1.05=551.25|424*1.06=448.24|
|2021|551.25*1.05=578.96|448.24*1.06=475.92|
Now,let’sanalyzethedata.Thedifferenceinproductionbetweenthetwofactoriesin
2018was100millionFt(500million-400million).
-In2019,thedifferenceis21millionFt(525million-424million). -In2020,the
differenceis23.31millionFt(551.25million-448.24million).-In2021,thedifference
is23.94millionFt(578.96million-475.92million).
Thedifferenceisnotgettingsmaller;infact,itisincreasingslightlyeachyear. This
isbecausetherateofincreaseforFactoryA(5%)islowerthantherateforFactoryB
(6%).Theclaimmadeinthedebatethatthedifferencewouldgetsmallerandsmalleris
nottruebasedonthesefigures.
25Math-Critique Prompt Template
[Instruction]
请你扮演⼀名评审，对⼈⼯智能助⼿对数学问题⽣成的回答进⾏评分。你的评价需要优先考虑最终答案的正确性，其次关
注解题步骤每⼀步的正确性，并严格按照下⾯的评分标准进⾏评分：
1)分数：1~2， 标准：最终答案完全错误，并且解答过程中错误出现很早，即解答刚开始便出现明显错误。
2)分数：3~5， 标准：最终答案错误，且解题过程前半部分正确，但后半部分中出现错误。
3)分数：6~8， 标准：最终答案正确，但中间步骤出现部分错误。
4)分数：9~10， 标准：最终答案完全正确，且解题步骤完全正确。
我会提供⼀段参考答案和⼈⼯智能助⼿的答案，请你以参考答案为基础对⼈⼯智能助⼿的答案尽可能客观地评价。请你输
出 1) ⾸先提供⼀段简短的解释，⽤来评价⼈⼯智能助⼿回答的质量，如有最终答案错误或者步骤错误，请指出并简单解
释；2) 然后给出评估分数，必须严格按照以下格式进⾏评分：\"[[rating]]\"，例如：\"评分:[[4]]\".
[Question]
设X服从泊松分布P(λ)，且E[(X-1)(X-2)]=1，则λ等于？
[The Start of Reference Answer]
⾸先我们需要知道泊松分布的期望是λ，⽅差是λ。由题意，E[(X-1)(X-2)]=1，所以 E[(X-1)(X-2)]=E[X^2]-3E[X]+2=1
DX=EX^2-EX^2
EX^2=λ+λ^2
解这个⽅程我们可以得到λ=1
[The End of Reference Answer]
[The Start of Assistant's Answer]
设随机变量X服从泊松分布P(λ)，我们知道泊松分布的期望值E[X]等于λ，⽅差Var[X]也等于λ。
根据题⽬条件，我们有：
E[(X-1)(X-2)] = 1
因为X服从泊松分布，我们可以利⽤⼆项式定理展开(X-1)(X-2)，然后将其与期望值相乘以找到λ。
(X-1)(X-2) = X^2 - 3X + 2
展开后得到：
E[X^2 - 3X + 2] = 1
我们知道E[X^2] = Var[X] + (E[X])^2，且E[X] = λ。因此，我们可以将E[X^2]替换为λ^2 + λ，并且知道E[X] = λ。
所以，我们有：
λ^2 + λ - 3λ + 2 = 1
解这个⽅程得到 λ = 1。
所以，λ的值为1。
[The End of Assistant's Answer]
Figure5: PromptTemplatewithanexampleforMath-CritiqueModel.
2. Fornumericstandardanswers,theresponseundergoesthefollowingrecognitionprocess:
fractionsordecimalsinLaTeXformat, andregularfractions, decimals, orintegers. We
computethenumericalresultandcompareitwiththestandardanswer. Adiscrepancyless
than(1e-6)isdeemedcorrect.
3. Forstandardanswersthatarestrings(exclusivetotheMATHdataset): identifythecontent
following"xxxis"oranequalitysign,orwithinboxed{}. Theevaluationconsiderswhether
thenormalizedstringsmatch.
FortheEnglishdatasetsGSM8kandMATH,ourprincipleforselectingresultsisasfollows:
1. Ifthemodelitselfreportsresultsonthesetwodatasets,wechoosethehigherofeitherthe
zero-shotorfew-shotresults.
2. Ifthemodeldoesnotreportitsresults,weutilizetheresultsreportedbyOpenCompass,
againselectingthehigherofthezero-shotorfew-shotoutcomes. Thisappliestomodels
suchasGPT-4-0613,GPT-3.5-Turbo-0613,Yi-6B-Chat,andQwen-Chat-7B.Shouldthere
beanyomissionsfromtheabovesources,resultswillnotbereported.
FortheHungarianExamresults,wehadannotatorsscorethemaccordingtoagradingstandard,which
isreferencedfrom[36]. It’snotedthatthereportedresultsactuallysumthescoresofallquestions,
makingthetotalscoreeffectively117. Toalignwiththereportedresults,weadoptedthisscoring
methodaswell.
Withreproducibilityinmind,allourresultswereobtainedusingasamplingtemperatureof0and
settingthemax-seq-lengthto4096.
26Table15: Math-User-ResultResult,GPT-4-1106-Preview-rated. AllresultswerescoredbyGPT-
4-1106-Preview, with the scoring method consistent with AlignBench. All Overall scores were
calculatedusingthemacro-average.
Elementary Advanced
Model Overall
Avg algebra calculate geo. tri. Avg calculus discrete linear. Prob.
GPT-4-0125-Preview[34] 5.79 5.26 5.04 7.63 3.98 4.59 6.71 7.26 6.62 5.48 7.72
GPT-4-1106-Preview[34] 5.73 5.07 4.96 7.00 3.78 4.71 6.81 7.39 6.96 5.29 7.91
GLM-4 5.11 4.86 4.47 6.56 3.95 4.74 5.43 6.00 5.67 4.26 6.02
ChatGLM3-32B-SFT-2312+RFT&DPO 4.23 4.01 3.88 5.41 2.90 3.99 4.59 5.22 4.76 3.38 5.20
GPT-4-0613[34] 4.14 3.34 2.88 4.76 3.17 2.78 5.33 5.57 5.49 4.26 6.22
ChatGLM3-32B-SFT-2312+RFT 4.01 3.86 3.84 5.37 2.57 3.77 4.26 4.72 4.69 2.98 4.89
Qwen-72B-Chat[3] 3.87 3.99 3.96 4.81 3.83 3.34 3.67 4.54 3.71 2.84 3.65
GPT-3.5-Turbo-0613[34] 3.42 3.04 2.81 4.07 2.23 3.26 4.07 4.83 4.38 3.26 3.91
ChatGLM3-32B-SFT-2312 3.39 3.35 3.35 4.51 2.51 3.11 3.44 4.04 4.38 2.41 3.13
Claude-2[1] 3.29 2.63 2.35 3.63 2.20 2.53 4.35 4.56 4.53 3.29 5.28
DeepSeek-Chat-67B[12] 3.24 2.76 2.21 4.73 2.12 2.30 3.84 4.41 4.82 2.79 3.52
Yi-34B-Chat[56] 2.64 2.49 2.04 3.61 2.25 2.27 2.87 2.80 3.47 2.03 3.41
C.2 EvaluationSettingsfor2023Hungariannationalhighschoolfinalsinmathematics
FortheHungariannationalhighschoolfinalsinmathematics, wesubmitthemodel’sanswersto
annotatorsformarking. Forresultsofmodelsnotlistedin [36],wescorethembasedontheanswers
providedin[36]accordingtothescoringpoints. Wesumthescoresofallquestionstopresentatotal
score. Allannotationsarecarriedoutbytwoannotators;incaseofinconsistency,athirdannotator
decides.
Consideringthegeneralsituationofmultiplemodels,wedonotrestrictthelanguageusedbythe
language models to answer the questions. Any language used to correctly answer is considered
correct. Additionally,sincemostquestionsdonotrestricttheformoftheanswer,westipulatethat
answersaredeemedcorrectaslongastheyretainmorethanonedecimalplaceaccuratelyorare
providedinfractionform.
D AdditionalResults
D.1 SubcategoryResultsofMathUserEval
In Table 15, we display the results for all subsets of MathUserEval. The reported results were
evaluatedbyGPT-4-1106-Preview,withtheevaluationmethodconsistentwithAlignBench. Itis
notedthatGPT-4-0125-PreviewandGPT-4-1106-Previewstilloccupytheleadingpositions. Except
forProbability,theGLM4model’stotalscoreandindividualscoressurpassedGPT-4-0613. Our
GLM-Math-32Bw/DPOmodelperformedexceptionallywellintheElementarycategory,exceeding
GPT-4-0613,butasignificantgapremainsinAdvancedmathematics. OurSelf-Critiquetraining
methodshowedsignificantprogressinMathUserEval,withanoverallimprovementof24%.
27D.2 SubcategoryResultsofAlignbench[29]
Language
Model
Avg. Fund. Chi. Open. Writ. Role. Pro.
GPT-4-1106-Preview[34] 8.29 7.99 7.33 8.61 8.67 8.47 8.65
ChatGLM3-32B-SFT-2312+RFT&DPO 7.80 7.14 6.90 8.37 8.41 8.09 7.90
GPT-4-0613[34] 7.59 7.81 6.93 7.42 7.93 7.51 7.94
ChatGLM3-32B-SFT-2312+RFT 7.43 6.37 6.95 8.03 7.71 7.97 7.54
ChatGLM3-32B-SFT-2312 7.38 6.84 7.02 8.08 7.37 7.70 7.27
Qwen-72B-Chat[3] 7.29 6.63 7.31 7.24 7.29 7.59 7.71
DeepSeek-67B-Chat[12] 7.11 7.12 6.52 7.58 7.20 6.91 7.37
GPT-3.5-Turbo-0613[34] 6.82 6.71 5.81 7.29 7.03 7.28 6.77
Claude-2[1] 6.78 6.87 6.24 7.08 6.36 6.85 7.31
Yi-34B-Chat[56] 6.18 4.32 6.05 7.37 6.00 6.30 7.06
Table16: ResultsofAlignbench[29],LanguagePart.
1TheChatGLM3-32B-SFT-2312isanewerversionoftheChatGLMseriesandnotidenticaltothe
modeldiscussedin [19],despitesharingthesamemodelsize.
Table16reportsdetailedresultsfromthelanguagecapabilitysubsectionofAlignBench. Within
this,wepresentthescoresofourfourmodelsandhavetestedtheresultsforQwen-72B-Chat[3],
Claude-2[1],andYi-34B-Chat[56]. AdditionalresultsarederivedfromtheAlignBenchpaper,and
theresultsforDeepSeekaretakenfromitsreport[12].
28