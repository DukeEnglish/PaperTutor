DeiT-LT: Distillation Strikes Back for Vision Transformer Training on
Long-Tailed Datasets
HarshRangwani*1 PradiptoMondal*1,2 MayankMishra*1
AshishRamayeeAsokan1 R.VenkateshBabu1
1IndianInstituteofScience,Bangalore 2 IndianInstituteofTechnology,Kharagpur
Abstract through crowd-sourcing to train deep neural networks for
variousapplicationsacrossdomains. Asaresultofcrowd-
Vision Transformer (ViT) has emerged as a prominent sourcing,thesedatasetsoftenexhibitlong-taileddatadistri-
architectureforvariouscomputervisiontasks. InViT,we butionsduetoinherentnaturalstatistics[14,52],i.e.,alarge
dividetheinputimageintopatchtokensandprocessthem number of images belong to a small portion of (majority)
throughastackofself-attentionblocks.However,unlikeCon- classes,whereasother(minority)classescontainfewimage
volutionalNeuralNetwork(CNN),ViT‚Äôssimplearchitecture sampleseach. Alotofrecentworks[5,9,25,32,67]focus
hasnoinformativeinductivebias(e.g.,locality,etc.). Due on training deep neural networks for recognition on such
tothis,ViTrequiresalargeamountofdataforpre-training. long-taileddatasets,suchthatnetworksperformreasonably
Variousdata-efficientapproaches(DeiT)havebeenproposed wellacrossallclasses,includingtheminorityclasses. Loss
totrainViTonbalanceddatasetseffectively. However,lim- manipulation-basedtechniques[5, 9, 23]enhance thenet-
ited literature discusses the use of ViT for datasets with work‚Äôsfocustowardlearningtailclassesbyenforcingalarge
long-tailedimbalances. Inthiswork,weintroduceDeiT-LT margin or increasing the weight for loss for these classes.
totackletheproblemoftrainingViTsfromscratchonlong- As these techniques enhance the focus on the tail classes,
tailed datasets. In DeiT-LT, we introduce an efficient and theyoftenleadtosomeperformancedegradationinthehead
effectivewayofdistillationfromCNNviadistillationDIST (majority)classes. Tomitigatethis,State-of-the-Art(SotA)
tokenbyusingout-of-distributionimagesandre-weighting techniquescurrentlytrainmultipleexpertnetworks[25,56]
thedistillationlosstoenhancefocusontailclasses. This thatspecializeindifferentportionsofthedatadistribution.
leads to the learning of local CNN-like features in early Thepredictionsfromtheseexpertsarethenaggregatedto
ViTblocks,improvinggeneralizationfortailclasses. Fur- producethefinaloutput,whichimprovestheperformance
ther,tomitigateoverfitting,weproposedistillingfromaflat overindividualexperts. However,alltheseeffortshavebeen
CNNteacher,whichleadstolearninglow-rankgeneralizable restrictedtoConvolutionalNeuralNetworks(CNNs),par-
featuresforDISTtokensacrossallViTblocks. Withthepro- ticularlyResNets[15],withlittleattentiontoarchitectures
posedDeiT-LTscheme,thedistillationDISTtokenbecomes suchasTransformers[11,53],MLP-Mixers[47]etc.
anexpertonthetailclasses, andtheclassifierCLStoken Recently,thetransformerarchitectureadaptedforcom-
becomesanexpertontheheadclasses. Theexpertshelpto putervision,namedasVisionTransformer(ViT)[12],has
effectivelylearnfeaturescorrespondingtoboththemajority gainedpopularityduetoitsscalabilityandimpressiveper-
andminorityclassesusingadistinctsetoftokenswithinthe formance on various computer vision tasks [6, 44]. One
sameViTarchitecture. WeshowtheeffectivenessofDeiT-LT caveatbehinditsimpressiveperformanceistherequirement
fortrainingViTfromscratchondatasetsrangingfromsmall- for pre-training on large datasets [11]. The data-efficient
scaleCIFAR-10LTtolarge-scaleiNaturalist-2018. Project transformers(DeiT)[48]aimedtoreducethisrequirement
Page: https://rangwani-harsh.github.io/DeiT-LT. forpre-trainingbydistillinginformationfromapre-trained
CNN.Subsequenteffortshavefurtherimprovedthedataand
compute efficiency [50, 51] of ViTs. However, all these
1.Introduction improvementshavebeenprimarilybasedonincreasingper-
formanceonthebalancedImageNetdataset. Wefindthat
VisualRecognitionhasseenunprecedentedsuccesswiththe theseimprovementsarestillinsufficientforrobustperfor-
adventofdeepneuralnetworkstrainedonlargedatasets[10].
Consequently,effortsarebeingmadetocollectlargedatasets *denotesequalcontribution.Correspondencetoharshr@iisc.ac.in.
4202
rpA
3
]VC.sc[
1v00920.4042:viXraOriginal Basis Reconstructed
100
Features Vectors Features
Low Rank 80 k Feat.
60
m
n 40
d m Hig Fh e R ata .nk 20 D De ei iT T-LT (ours)
Eigen Recon. w/ Recon. w/ ViT
Decomp. DeiT-LT feat. DeiT feat. 0 50 100
Imbalance Factor ùùÜ
(a) Comparison b/w attention maps of (b) DeiT-LT (ours) learns low rank feat. (c) Tail acc. for DeiT-LT
DeiT (left) and DeiT-LT (ours, right) compared to the DeiT baseline (ours) vs baselines
Figure1.WeproposeDeiT-LT(Fig.2,adistillationschemeforVisionTransformer(ViT),tailoredtowardslong-taileddata).InDeit-LT,a)
weintroduceOODdistillationfromCNN,whichleadstolearninglocalgeneralizablefeaturesinearlyblocks.b)weproposetodistillfrom
teacherstrainedviaSAM[13]whichinduceslow-rankfeaturesacrossblocksinViTtoimprovegeneralization.c)Incomparisontoother
SotAViTbaselines,Deit-LT(ours)demonstratessignificantlyimprovedperformanceforminorityclasses,withincreasingimbalance.
manceonlong-taileddatasets(Fig.1c). onboththemajorityandminorityeffectively,whichisnot
possiblewithvanillaDeiTtraining(Fig.5andSec.3.1).
Inthiswork,weaimtoinvestigateandimprovethetrain-
WedemonstratetheeffectivenessofDeiT-LTacrossdiverse
ing of Vision Transformers from scratch without the need
small-scale(CIFAR-10LT,CIFAR-100LT)aswellaslarge-
forlarge-scalepre-trainingondiverselong-taileddatasets,
scaledatasets(ImageNet-LT,iNaturalist-2018). Wefindthat
varyinginimagesizeandresolution. Recentworksshow
DeiT-LTeffectivelyimprovesovertheteacherCNNacross
improvedperformanceforViTsonlong-tailedrecognition
all datasets and achieves performances superior to SotA
tasks,buttheyoftenneedexpensivepre-trainingonlarge-
CNN-basedmethodswithoutrequiringanypre-training.
scale datasets [7, 30]. The requirement of pre-training is
computationallyexpensiveandrestrictstheirapplicationto
2.Background
specializeddomainssuchasmedicine,satellite,speech,etc.
Furthermore,thelarge-scalepre-traineddatasetsoftencon-
Long-TailedLearning. Withtheincreasedscaleofdeep
tain biases that might be inadvertently induced with their
learning,largecrowd-sourcedlong-taileddatasetshavebe-
usage[2,34,54]. Tomitigatetheseshortcomings,weintro-
comecommon. Aplethoraoftechniquesaredevelopedto
duceData-efficientImageTransformersforLong-Tailed
learnmachinelearningmodelsusingsuchdatasets,where
Data(DeiT-LT)-aschemefortrainingViTsfromscratchon
theobjectiveisimprovedperformance,particularlyontail
smallandlarge-scalelongtaileddatasets. DeiT-LTisbased
classes. Themethodscanbebroadlydividedintothreecat-
onthefollowingimportantdesignprinciples:
egories: a) loss re-weighting b) decoupled classifier and
‚Ä¢ DeiT-LT involves distilling knowledge from low- representations and c) expert-based classifier training. In
resolution teacher networks using out-of-distribution addition,therearesometechniquesbasedonthesynthetic
(OOD)imagesgeneratedthroughstrongaugmentations. generationforlong-tailedrecognition[22,37,39,40],which
Notably, this method proves effective even if the CNN are orthogonal to this study. The loss re-weighting-based
teacherwasn‚Äôtoriginallytrainedonsuchaugmentations. techniquesincludemarginbasedtechniqueslikeLDAM[5],
TheoutcomeisthesuccessfulinductionofCNN-likefea- andLogit-Adj[32],whichenforceahighermarginfortail
turelocalityintheViTstudentnetwork,ultimatelyenhanc- classes. Theotherset(eg. CB-Loss[9],VS-Loss[23]etc.)
inggeneralizationperformance,particularlyforminority introducere-weightingfactorsincrossentropylossbased
(tail)classes(Fig.1a,4aandSec.3.1). onthetrainingsetlabeldistribution. Theothersetoftech-
‚Ä¢ Further,toimprovethegeneralityoffeatures,wepropose niquesproposetodecouplethelearningofrepresentations
todistillknowledgeviaflatCNNteacherstrainedthrough withclassifierlearning,asit‚Äôsobservedthatmarginbased
SharpnessAwareMinimization(SAM)[13]. Thisresults lossesleadtosub-optimalrepresentations[20]. Theclassi-
in low-rank generalizable features for long-tailed setup fieristhenlearnedusingLearnableWeightScaling(LWS),
acrossallViTblocks(Fig.1bandSec.3.2). œÑ-normalization, which improves performance on the tail
‚Ä¢ InDeiT[48],theclassificationanddistillationtokenspro- classes [20]. Further, after this follow-up works [55, 60]
ducesimilarpredictions. However,inproposedDeiT-LT, like MiSLAS [66] proposed Mixup [62] based improved
weensuretheirdivergencesuchthattheclassificationto- representationlearningandLADE[16]proposesimproved
kenbecomesanexpertonthemajorityclasses. Whearas, classifiertrainingbyadaptingtotargetlabeldistribution.Fur-
thedistillationtokenlearnslocallow-rankfeatures,becom- ther,contrastivemethods,includingPaCo[8]andBCL[41],
inganexpertontheminority. Hence,DeiT-LTcanfocus havedemonstratedimprovedperformancewithcontrastive
)%(
ycaruccA
liaTFigure2.OverviewofDeiT-LT.TheHeadExpertclassifiertrainsusingCElossagainstgroundtruth,whereastheTailExpertclassifiertrains
usingDRWlossagainsthard-distillationtargetsfromtheflatResNetteachertrainedviaSAM[13]. Thedistillationisperformedusing
out-of-distributionimagescreatedusingstrongaugmentationsandMixup.
learning. However,allthesemethodsleadtoperformance architecture stacked with Multi-Headed Self-Attention
degradationonheadclassestoimproveperformanceontail blocks [53]. To provide input to the Vision Transformer
classes. Tomitigatethisdegradation, thetechniques(like architecture,wefirstconverttheimageintopatches. These
RIDE[56]etc.) learndifferentexpertsondifferentpartsof imagepatchesarepassedthroughalinearlayertoconvert
thedatadistribution. Theseexpertsarelearnedinawaythat themintotokensthatarethenpassedtotheattentionblocks.
makesthemdiverseintheirpredictionsandcanbecombined The attention blocks learn the relationship between these
efficientlytoobtainimprovedpredictions. However,these tokensforperformingagiventask. Inadditiontothis,the
methodsrequireadditionalcomputationtocombineexperts ViTarchitecturealsocontainsoneclassifier(CLS)tokenthat
attheinferencetime. Inourwork,wecanefficientlylearn representsthefeaturestobeusedforclassification. Inthe
expertsonmajorityandminorityusingasingleViTback- DataEfficientTransformer(DeiT)[48],thereisanadditional
bone, the predictions of which we average to prevent any distillation(DIST)tokenintheViTbackbonethatlearnsvia
additionalinferenceoverheadatthedeploymenttime. distillationfromtheteacherCNN.Fortheclassificationhead
andthedistillationhead,L isusedfortraining(Fig.2).
CE
VisionTransformer. Inrecentliterature,VisionTransform-
Thefinallossfunctionforthenetworkis:
ers [12] have emerged as strong competitors for ResNets
as they are easier to scale and lead to improved general- L=L CE(fc(x),y)+L CE(fd(x),y t),y t =argmaxg(x) i
i
ization. DeiT [48] developed a data-efficient way to train (1)
thesemodelsbydistillingthroughConvolutionalNeuralNet- Herefc(x)isoutputfromtheclassifierofstudentCLStoken,
works. However,despitebeingdataefficient,thesemodels fd(x)isoutputfromtheclassifierofstudentDISTtoken,
stillproducesub-optimalperformanceonlong-taileddata. g(x)denotestheoutputoftheteacherCNNnetwork,y ‚àà
RAC[30]utilizespre-trainedtransformerfordata-efficiency [K] is the ground truth, y is the label produced by the
t
onlong-taileddata. However,thesepre-trainedmodelsare teachercorrespondingtothesamplex,andN isthenumber
i
often domain specific and do not generalize well to other ofsamplesinclassi. AtthetimeofinferenceinDeiT,we
domainslikemedical,syntheticetc. Inourwork,wetrain obtain logit outputs from the two heads fd(x) and fc(x),
VisionTransformersfromscratch,evenforsmalldatasets andaveragethemtoproducethefinalprediction.
likeCIFAR-10LT,CIFAR-100LT,whichmakesthemfree
frombiasesduetopre-trainingonlargedatasets[54]. 3.DeiT-LT(DeiTforLong-TailedData)
Data Efficient Vision Transformers (DeiT). The Vision In this section, we introduce DeiT-LT - the Data-efficient
Transformer(ViT)architecture[12]consistsoftransformer Image Transformer that is specialized to be effective for
token
token
CLS
Patch
tokens
DIST
Transformer
Block
Transformer
Block
Transformer
BlockTable1. Effectofaugmentations: Comparisonofteacher(Tch)
Class wise entropy for teacher model outputs
and student (Stu) accuracy (%) and training time (in hours) on 2.300 Sets
CIFAR-10LT(œÅ=100)usingvariousaugmentationstrategieswith In Distribution
Out of Distribution
mixup(‚úì)andwithoutmixup(‚úó). Despitelowteachertraining 2.295
accuracyontheout-of-distributionimages,thestudent(Stu.)per-
formsbetteronthevalidationset. 2.290
Tch Stu Tch Tch Stu Train
2.285
Model Augs. Augs. Acc. Acc. Time
RegNetY 2.280
Strong(‚úì) Strong(‚úì) 79.1 70.2 33.3
16GF
Strong(‚úó) Weak(‚úó) 97.2 54.2 17.8 2.275
ResNet-32 Strong(‚úó) Strong(‚úó) 71.9 69.6 17.8
Strong(‚úì) Strong(‚úì) 56.6 79.4 19.0 2.270 0 1 2 3 4 5 6 7 8 9
Categories
Long-Taileddata. WestartwithaDeiTtransformer-based Figure3.Entropyofteacheroutputs:Comparisonoftheentropy
architecture which, in addition to the classification (CLS) ofin-distributionsamplesandout-of-distributionsampleswiththe
ResNet-32teacheronCIFAR-10LT.Weobserveahigheraccuracy
token,alsocontainsadistillation(DIST)token(Fig.2)that
inTable-1correspondingtoout-of-distributionsamples.
learnsviadistillationfromaCNN.TheDeiT-LTintroduces
threeparticulardesigncomponents,whichare: a)theeffec-
tivedistillationviaout-of-distribution(OOD)images,which improvesthedistillationperformance. Thiscanalsobeseen
induceslocalfeaturesandleadstothecreationofexperts,b) fromtheentropyofpredictionsonteacher,whicharehigh
trainingTailExpertclassifierusingDRWlossandc)learn- (i.e.moreinformative)forOODsamples(Fig.3). Ingen-
ing of low-rank generalizable features from flat teachers eral, we find that increasing diverse amount of out-of-
via distillation. In the following sections, we analyze our distribution[33]datawhiledistillationhelpsimproveper-
designchoicesin detail. WeanalyzeCIFAR-10 LT using formanceandleadstoeffectivedistillationfromtheCNN.
LDAM+DRW+SAMResNet-32[38]CNNteacher,tojustify DetailsregardingtheaugmentationsareinSuppl. Sec. A.4.
therationalebehindeachdesigncomponent. Due to distillation via out-of-distribution images, the
teacher predictions y often differ from the ground truth
t
3.1. DistillationviaOutofDistributionImages y. Hence, the classification token (CLS) and distillation
token(DIST)representationsdivergewhiletraining. This
We now focus on how to distill knowledge from a CNN
phenomenoncanbeobservedinFig. 4a,wherethecosine
architecturetoaViTeffectively. IntheoriginalDeiTwork
distancebetweentherepresentationoftheCLSandDIST
[48], the authors first train a large CNN, specifically Reg-
tokensincreasesasthetrainingprogresses. Thisleadstothe
NetY[35],withstrongaugmentations(A)asusedbyaViT
CLStokenbeinganexpertonheadclasses,whiletheDIST
fordistillation. However,thisincurstheadditionalexpense
tokenspecializesintailclasspredictions. Ourobservation
oftrainingalargeCNNforsubsequenttrainingoftheViT
debunksthemyththatitisrequiredfortheCLStokenpre-
throughdistillation. Incontrast,weproposetotrainasmall
dictions to be similar to DIST for effective distillation in
teacherCNN(ResNet-32)withtheusualweakaugmenta-
transformer,asobservedbyTouvronetal.[48].
tions, but during distillation, we pass strongly augmented
TailExpertwithDRWloss. Furtherinthisstage,wealso
imagestoobtainpredictionstobedistilled.
introduceDeferredRe-Weighting(DRW)[5]fordistillation
Thesestronglyaugmentedimagesareout-of-distribution
loss,whereweweighthelossforeachclassusingafactor
(OOD)imagesfortheResNet-32CNNasthemodel‚Äôsac-
curacyonthesetrainingimagesislow,asseeninTable1. w y =1/{1+(e y‚àí1)1 epoch‚â•K},wheree y = 1‚àí 1‚àíŒ≤N Œ≤y isthe
However, despite the low accuracy, the strong augmenta- effectivenumberofsamplesinclassy[9],afterK number
tionsleadtoeffectivedistillationincomparisontotheweak ofepochs[5]. Hencetheoveralllossisgivenas:
augmentations on which the original ResNet was trained
(Table 1). This works because the ViT student learns to
1 1
mimictheincorrectpredictionsoftheCNNteacheronthe L= 2L CE(fc(x),y)+ 2L DRW(fd(x),y t),
out-of-distributionimages,whichinturnenablesthestudent
whereL =‚àíw log(fd(x) )
tolearntheinductivebiasesoftheteacher. DRW yt yt
TheDRWstagefurtherenhancesthefocusofthedistillation
fd(X)‚âàg(X),X ‚àºA(x) (2)
head(DIST)onthetailclasses,leadingtoimprovedperfor-
Further,wefindthatcreatingadditionalout-of-distribution mance. ThisisalsoobservedinFig. 4a,wherethediversity
samples by mixing up images from two classes [61, 62] betweenthetwotokensimprovesaftertheintroductionof
seulaV600
DeiT (RegNetY Teacher)
0.4 DeiT (LDAM+DRW+SAM ResNet32 Teacher)
DeiT-LT (ours) 100 500
0.3 Out of Distribution 400 Images 80
300
0.2
60
In Distribution 200 DeiT
0.1 Images 40 V Di eT iT 100 D De ei iT T- -L LT T L LD DA AM M+ +D DR RW W +Te Sa Ac Mh e Tr eacher
DeiT-LT(ours) DeiT-LT PaCo Teacher
0.0 DeiT-LT PaCo+SAM
0 200 400 600 800 1000 1200 0 1 2 3 4 5 6 7 8 9 10 11 0 0 2 4 6 8 10
Epochs Transformer Heads Transformer Blocks
(a)DiversityforCLSandDISTexperts (b)LocalityofAttentionHeads (c)RankofViTfromDistillationofCNNteachers.
Figure4. EffectofDistillationinDeiT-LT.Ina)wetrainDeiT-Bwithteacherstrainedonin-distributionimages(RegNetY-16GF)and
out-of-distributionimages(ResNet32).Theout-of-distributiondistillationleadstodiverseexperts,whichbecomemorediversewithdeferred
re-weightingonthedistillationtoken(DRW).Inb)weplottheMeanAttentionDistanceforthepatchesacrosstheearlyselfattentionblock
1(solid)andblock2(dashed)forbaselines,wherewefindthatDeiT-LTleadstohighlylocalandgeneralizablefeatures.Inc)weshowthe
rankoffeaturesforDISTtoken,wherewedemonstratethatstudentstrainedwithSAMaremorelow-rankincomparisontobaselines
theDRWstage. ThisleadstothecreationofdiverseCLS novelfindinginthecontextofdistillationforViTs.
and DIST tokens, which are experts on the majority and
Training Time. In the original DeiT formulation, the au-
minorityclassesrespectively.
thors[51]proposetrainingalargeCNNRegNetY-16GFata
Induction of Local Features: To gain insights into the
highresolution(224√ó224)fordistillationtotheViT.We
generalityandeffectivenessofOODDistillation, wetake
findthatcompetitiveperformancecanbeachievedevenwith
a closer look at the tail features produced by DeiT-LT. In
trainingasmallerResNet-32CNN(32√ó32)atalowerreso-
Fig.4b,weplotthemeanattentiondistanceforeachpatch
lution,asseeninTable1.Thissignificantlyreducescompute
acrossViTheads[36](DetailsinSuppl. Sec. F).
requirement and overall training time by 13 hours, as the
Insight1: DeiT-LTcontainsheadsthatattendlocally,like ResNet-32modelcanbetrainedquickly(Table1). Further,
CNN,intheneighborhoodofthepatchinearlyblocks(1,2). wefindthatwithSAMteachers,thestudentconvergesmuch
Due to this learning of local generalizable class agnostic fasterthanvanillateachermodels,demonstratingtheefficacy
features,weobserveimprovedgeneralizationonminority ofSAMteachersforlow-rankdistillation(Suppl.Sec.G.1).
classes (Fig. 1c). Without the OOD distillation, we find
thatthevanillaDeiT-IIIandViTbaselinesoverfitonlyon 4.Experiments
thespuriousglobalfeatures(Fig.4b)anddonotgeneralize
4.1.Datasets
wellfortailclasses. Hence,thismakesOODdistillationin
DeiT-LTawell-suitablemethodforlong-tailedscenarios. Weanalyzetheperformanceofourproposedmethodonfour
datasets,namelyCIFAR-10LT,CIFAR-100LT,ImageNet-
3.2.Low-RankFeaturesviaSAMteachers
LT, and iNaturalist-2018. We follow [5] to create long-
Tofurtherimprovethegeneralizabilityofthefeatures,par- tailedversionsofCIFAR[24]datasets,wherethenumber
ticularlyforclasseswithlessdata,weproposetodistillvia ofsamplesisexponentiallydecayedusinganimbalancefac-
teacherCNNmodelstrainedviaSharpnessAwareMinimiza- torœÅ = maxiNi (numberofsamplesinthemostfrequent
minjNj
tion(SAM)objective[13].ModelstrainedviaSAMconverge class by that in the least frequent class). For ImageNet-
toflatminima[38]andleadtolow-rankfeatures[3].Forana- LT,wecreateanimbalancedversionoftheImageNet[42]
lyzingtherankoffeaturesfortheViTstudentinLTcase,we datasetasdescribedin[29]. Wealsoreportperformanceon
calculaterankspecificallyforthefeaturesoftailclasses[3]. iNaturalist-2018[52],areal-worldlong-taileddataset. We
We detail the procedure of our rank calculation in Suppl. dividetheclassesintothreesubcategories: Head(Many),
Sec. G.Weconfirmourobservationsacrossdiverseteacher Mid(Medium),andTail(Few)classes. Moredetailsregard-
modelstrainedviaLDAMandPaCo. Wefindthefollowing ingthedatasetscanbefoundinSuppl. Sec. A.1.
insightfordistillationviaDISTtoken:
4.2.ExperimentalSetup
Insight2. WeobservethatdistillingintoViTviapredictions
made using SAM teacher leads to low-rank generalizable We follow the setup mentioned in DeiT [48] to create the
(DIST)tokenfeaturesacrossblocksofViT(Fig. 4c). studentbackboneforourexperiments. WeusetheDeiT-B
ThistransferofaCNNteacher‚Äôscharacteristic(low-rank)to studentbackbonearchitectureforallthedatasets. Wetrain
thestudent,byjustdistillingviafinallogits,isasignificant ourteachermodelsusingre-weightingbasedLDAM-DRW-
ytisreviD
trepxE 0011
=
WRD
ecnatsiD
noitnettA
naeM
knaR
erutaeFTable2. Results onCIFAR-10 LT and CIFAR-100 LTdatasets Sharpener Volcano Tennis ball Chainsaw
withœÅ=50andœÅ=100.Wereporttheoverallaccuracyforavailable
methods. (Theteacherusedtotraintherespectivestudent(DeiT-
LT)modelcanbeidentifiedbymatchingsuperscripts)
CIFAR-10LT CIFAR-100LT
Method
œÅ=100 œÅ=50 œÅ=100 œÅ=50
ResNet32Backbone
CBFocalloss[9] 74.6 79.3 38.3 46.2
LDAM+DRW[5] 77.0 79.3 42.0 45.1
LDAM+DAP[19] 80.0 82.2 44.1 49.2
BBN[67] 79.8 82.2 39.4 47.0
CAM[64] 80.0 83.6 47.8 51.7
Log.Adj.[32] 77.7 - 43.9 - Figure5.Visualcomparisonoftheattentionmapswithrespectto
RIDE[56] - - 49.1 - theCLSandDISTtokensfortailimagesfromtheImageNet-LT
MiSLAS[65] 82.1 85.7 47.0 52.3 dataset.TheattentionmapsarecomputedbyAttentionRollout[1].
Hybrid-SC[55] 81.4 85.4 46.7 51.9
throughouttraining. Bothdatasetsfollowacosinelearning
SSD[27] - - 46.0 50.5
rateschedule,withabaseLRof5√ó10‚àí4. Moredetailson
ACE[4] 81.4 84.9 49.6 51.9
GCL[26] 82.7 85.5 48.7 53.6 theexperimentalprocesscanbefoundinSuppl. SecA.
VS[23] 78.6 - 41.7 Baselines. Weusethepopulardata-efficientbaselinesfor
VS+SAM[38] 82.4 - 46.6 - ViT:a)ViT:ThestandardVisionTransformer(ViT-B)[12]
1L-D-SAM[38] 81.9 84.8 45.4 49.4 architecturetrainedwithCELossagainstthegroundtruth.
2PaCo+SAM[8,38] 86.8 88.6 52.8 56.6 Forafaircomparison,wetrainViTwiththesameaugmenta-
tionstrategyusedfortheDeiT-LTexperiments.b)DeiT[48]:
ViT-BBackbone
VanillaDeiTmodelthatusesRegNetY-16GFteachertrained
ViT [12] 62.6 70.1 35.0 39.0 within-distributionimagesfordistillation. c)DeiT-III: A
ViT(cRT) [20] 68.9 74.5 38.9 42.2
recentimprovedversionofDeiT([51])thatfocusesonim-
DeiT [48] 70.2 77.5 31.3 39.1
provingthesupervisedlearningofViTonbalanceddatasets
DeiT-III [51] 59.1 68.2 38.1 44.1
usingthreesimpleaugmentations(GrayScale,Solarisation,
1DeiT-LT(ours) 84.8 87.5 52.0 54.1 andGaussianBlur)andLayerScale[49],alsodemonstrating
2DeiT-LT(ours) 87.5 89.8 55.6 60.5 theredundancyofdistillationinDeiTs.Thelong-tailedbase-
lineofd)ViT(cRT): adecoupledapproachoffirsttraining
classifier(ViT)andthenre-trainingtheclassifierforasmall
SAMmethod[38]andthecontrastivePaCo+SAM(training
numberofepochswithclass-balancedsampling[20]. We
PaCo[8]withSAM[13]optimizer),employingResNet-32
furtherattemptedtrainingotherbaselineslikeLDAM,etc,
forsmallscaledatasets(CIFAR-10LTandCIFAR-100LT)
onViT.However,wefoundsomeoptimizationdifficultiesin
andResNet-50forlargescaleImageNet-LT,andiNaturalist-
trainingViTs(detailsinSuppl. Sec. A.3).
2018. WetraintheheadexpertclassifierwithCElossL
CE
Wewanttoconveythatwedonotcompareagainstbase-
against the ground truth, while the tail expert classifier is
lines [30, 46, 58, 59], which use pre-training, usually on
trained with the CE+DRW loss L against the hard-
DRW
largedatasets, toproduceresultsonevenCIFARdatasets
distillationtargetsfromtheteachernetwork.
(Ref. Suppl. Sec. C).Ourgoalistodevelopagenerictech-
SmallscaleCIFAR-10LTandCIFAR-100LT.Thesemod- niquefortrainingViTsacrossdomainsandmodalitieson
elsaretrainedfor1200epochs,whereDRWtrainingforthe long-taileddatawithoutrequiringanyexternalsupervision.
Tail Expert Classifier starts from epoch 1100. Except for
theDRWtraining(last100epochs),weuseMixupandCut- 5.Results
mixaugmentationfortheinputimages. Thesedatasetsare Inthissection,wepresentresultsforDeiT-LTacrossvarious
trainedwithacosinelearningrateschedulewithabaseLR datasets. Weusere-weightingbasedLDAM+DRW+SAM
of5√ó10‚àí4usingtheAdamW[31]optimizer.
(referred to as L-D-SAM in Table 2,3,4) and contrastive
Large scale ImageNet-LT and iNaturalist-2018. These PaCo+SAMteachersfortrainingDeiT-LTstudentmodels.
modelsaretrainedfor1400and1000epochs,respectively, ResultsonSmallScaleDatasets. Table2presentsresults
withtheDRWtrainingfortheTailExpertClassifierstarting for the CIFAR-10 LT and CIFAR-100 LT datasets, with
from 1200 and 900 epochs. We use Mixup and Cutmix varying imbalance factors (œÅ = 100 and œÅ = 50). We
egamI
tupnI
nekoT
SLC
nekoT
TSIDTable3.ResultsonImageNet-LT.(Theteacherusedtotrainrespec- Table4. ResultsoniNaturalist-2018. (Theteacherusedtotrain
tivestudent(DeiT-LT)canbeidentifiedbymatchingsuperscripts) student(DeiT-LT)canbeidentifiedbymatchingsuperscripts)
ImageNet-LT iNaturalist-2018
Method Overall Head Mid Tail Method Overall Head Mid Tail
ResNet50Backbone
ResNet50Backbone
c-RT[20] 65.2 69.0 66.0 63.2
CBFocalloss[9] 33.2 39.6 32.7 16.8
œÑ-Norm[21] 65.6 65.6 65.3 65.9
LDAM[5] 49.8 60.4 46.9 30.7
RIDE(3exps)[56] 72.2 70.2 72.2 72.7
c-RT[20] 49.6 61.8 46.2 27.3
MiSLAS[65] 71.6 73.2 72.4 70.4
œÑ-Norm[21] 49.4 59.1 46.9 30.7 Disalign[63] 70.6 69.0 71.1 70.2
Log. Adj.[32] 50.1 61.1 47.5 27.6 TSC[28] 69.7 72.6 70.6 67.8
RIDE(3exps)[56] 54.9 66.2 51.7 34.9 GCL[26] 71.0 67.5 71.3 71.5
MiSLAS[65] 52.7 62.9 50.7 34.3 ImbSAM[68] 71.1 68.2 72.5 72.9
Disalign[63] 52.9 61.3 52.2 31.4 CBD ENS [18] 73.6 75.9 74.7 71.5
TSC[28] 52.4 63.5 49.7 30.4 1L-D-SAM[38] 70.1 64.1 70.5 71.2
GCL[26] 54.5 63.0 52.7 37.1 2PaCo+SAM[38] 73.4 66.3 73.6 75.2
SAFA[17] 53.1 63.8 49.9 33.4
ViT-BBackbone
BCL[41] 57.1 67.9 54.2 36.6
ViT [12] 54.2 64.3 53.9 52.1
ImbSAM[68] 55.3 63.2 53.7 38.3
DeiT-III [51] 61.0 72.9 62.8 55.8
CBD [18] 55.6 68.5 52.7 29.2
ENS
1DeiT-LT(ours) 72.9 69.0 73.3 73.3
1L-D-SAM[38] 53.1 62.0 52.1 32.8
2DeiT-LT(ours) 75.1 70.3 75.2 76.2
2PaCo+SAM[8,38] 57.5 62.1 58.8 39.3
ViT-BBackbone
ourproposedDeiT-LTfortransformersisthefirstworkin
ViT [12] 37.5 56.9 30.4 10.3 literaturethatcanachieveSotAperformanceforlong-tailed
DeiT-III [51] 48.4 70.4 40.9 12.8 dataonsmalldatasetswhentrainedfromscratch. Theother
works[58]requiretransformerpre-trainingonlargedatasets,
1DeiT-LT(ours) 55.6 65.2 54.0 37.1
suchasImageNet,toachievecomparableperformanceon
2DeiT-LT(ours) 59.1 66.6 58.3 40.0
thesesmalldatasets.
primarilycompareourresultstotheSotAmethods,which ResultsonLargeScaleDatasets.Inthissection,wepresent
trainthenetworksfromscratch. Theothertechniquesuti- results attained by DeiT-LT on the large-scale long-tailed
lizeadditionalpre-trainingwithextradata[7,58],making datasets of ImageNet-LT and iNaturalist-2018. We train
thecomparisonunfair. OurproposedstudentnetworkDeiT- alltransformer-basedmethodsforsimilarepochsforapar-
LToutperformedtheteachersusedfortheirtrainingbyan ticulardataset,tokeepthecomparisonfairacrossallbase-
average of 1.9% and 4.5% on CIFAR-10 LT and CIFAR- lines(SeeSuppl. Sec. A.2). Table3presentstheresulton
100 LT, respectively. This demonstrates the advantage of theImageNet-LTdataset, wherewefindthatwhendistill-
trainingtheDeiT-LTtransformer,whichprovidesadditional ing using LDAM+DRW+SAM (L-D-SAM), our DeiT-LT
generalizationimprovementsovertheCNNteacher. Further, significantly improves by 2.5% over the teacher network.
theDeiT-LT(PaCo+SAM)modelsignificantlyimprovesby Notably, it can be seen that our DeiT-LT method, when
24.9%overtheViTbaseline(whichhasthesameaugmen- distilling from PaCo+SAM teacher, achieves a 1.6% per-
tations as in DeiT-LT) and 28.4% over the data efficient formancegainoverthealreadynearSotAteachernetwork.
DeiT-IIItransformerforCIFAR-10LTdatasetforœÅ=100. Further,thedistillation-basedDeiT-LTmethodachievesa
AsimilarimprovementcanalsobeobservedfortheCIFAR- significantgainof21.6%and10.7%overthebaselinetrans-
100LTdataset, whereDeiT-LT(PaCo+SAM)faresbetter formertrainingmethods,ViTandDeiT-IIIrespectively. This
than ViT baseline and DeiT-III by 20.6% and 17.5%, re- demonstrates that improvement due to distillation scales
spectively. This shows the effectiveness of the DeiT-LT wellwithanincreaseinthesizeofdatasets. ForiNaturalist-
distillationprocedureviaCNNteachers. ComparedtoCNN- 2018, we notice an improvement of close to 3% over the
basedmethods,wedemonstratethatthetransformer-based LDAM+DRW+SAM(L-D-SAM)teachernetworkandan
methodscanachieveSotAperformancewhentrainedwith improvementof1.7%overtherecentPaCo+SAMteacher.
DeiT-LTdistillationprocedure,combiningboththescalabil- Additionally,wenoticeasignificantimprovementoverthe
ityoftransformersonheadclassesandutilizinginductive data-efficienttransformer-basedbaselines.Thedata-efficient
biasesofCNNfortailclasses. Tothebestofourknowledge, transformer-basedmethodsstrugglewhilemodelingthetailTable5.TableshowingablationsforvariouscomponentsinDeiT- Table6.AnalysisacrosstransformercapacityforCIFAR-10LTand
LTforCIFAR-10LTandCIFAR-100LT. CIFAR-100LTforDeiT-LTstudent(œÅ=100)withPaCoteacher.
OODDistill DRW SAM C10LT C100LT Model Overall Head Mid Tail
‚úó ‚úó ‚úó 70.2 31.3 CIFAR-10LT(œÅ=100)
‚úì ‚úó ‚úó 84.5 48.9
DeiT-LTTiny(Ti) 80.8 89.7 75.1 79.4
‚úì ‚úì ‚úó 87.3 54.5
DeiT-LTSmall(S) 85.5 92.7 81.5 83.7
‚úì ‚úì ‚úì 87.5 55.6
DeiT-LTBase(B) 87.5 94.5 84.1 85.0
classes,whichissupplementedviaproposedDistillationloss CIFAR-100LT(œÅ=100)
inDeiT-LT.ThisenablesDeiT-LTtoworkwellacrossall DeiT-LTTiny(Ti) 49.3 66.3 50.0 27.3
theclasses;theheadclassesbenefitfromenhancedlearning DeiT-LTSmall(S) 54.3 72.6 54.8 31.1
capacityduetoscalableVisionTransformer(ViT),andtail DeiT-LTBase(B) 55.6 73.1 56.9 32.1
classesarelearnedwellviadistillation. Ourresultsaresu-
periorforbothdatasetscomparedtotheCNN-basedSotA
formervariantshavingdifferentcapacities. Forthis,wefix
methods,demonstratingtheadvantageofDeiT-LT.(Refer
the teacher network and training schedules while varying
Suppl. Sec. Bfordetailedresults.)
thenetworksizes. WeexperimentwiththeViT-Ti,ViT-S,
andViT-Barchitectures,asintroducedintheoriginalViT
6.AnalysisandDiscussion
work[12]. InTable6,weobservethattheproposedDeiT-LT
methodscaleswellwiththeincreasedcapacityoftheTrans-
VisualizationsofAttentions. Ourtrainingmethodologyen-
formernetwork,andleadstoperformanceimprovements.
suresthattheCLSandDISTrepresentationsdivergewhile
Limitations. One limitation of our framework is that the
training. WhiletheCLStokenistrainedagainsttheground
learningfortailclassesisdonemostlythroughdistillation.
truth,itcannotlearnefficientrepresentationfortailclasses‚Äô
Hence,theperformanceontailclassesremainssimilar(Table
imagesduetoViT‚Äôsinabilitytotrainwellonsmallamounts
3and 4)tothatoftheCNNclassifier. Futureworkscanaim
ofdata. Distillingfromateacherviaout-of-distributiondata
todevelopadaptivemethodsthatcanshifttheirfocusfrom
andintroducingre-weightinglosshelpstheDISTtokento
CNNtogroundtruthlabels,astheCNNfeedbacksaturates.
learnbetterrepresentationfortheimagesofminorityclasses
ascomparedtotheCLStoken. Wefurthercorroboratethis
7. Conclusion
bycomparingtheattentionvisualizationobtainedthrough
AttentionRollout [1],fortheCLSandDISTtokenontail In this work, we introduce DeiT-LT, a training scheme to
images,forImageNet-LTdataset(asCIFAR-10istoosmall) trainViTsfromscratchonreal-worldlong-taileddatasets
usingDeiT-LT.AscanbeseeninFig.5, theCLSandthe efficiently. Wereintroducetheideaofknowledgedistillation
DISTtokenfocusondifferentpartsoftheimage.TheDIST intoViTstudentsviateacherCNN,asitenableseffective
tokenisabletoidentifythepatchesofinterest(highredin- learningonthetailclasses. Thisdistillationcomponentwas
tensity)forimagesoftailclasses,whiletheCLStokenfails found to be redundant and removed from the latest DeiT-
to do so. The diversity in localized regions demonstrates III. Further, in DeiT-LT, we introduce out-of-distribution
thecomplementaryinformationpresentacrosstheCLSand (OOD)distillationviatheteacher,inwhichwepassstrongly
DISTexperts,whichisincontrastwithDeiT,whereboth augmented images to teachers originally trained via mild
the tokens CLS and DIST are quite similar. We compare augmentations for distillation. The distillation loss is re-
visualizationwithdifferentmethodsinSuppl. Sec. D. weightedtoenhancethefocusonlearningfromtailclasses.
This helps make the classification token an expert on the
AblationAnalysisAcrossDeiT-LTcomponents. Weana-
headclassesandthedistillationtokenanexpertonthetail
lyzetheinfluenceofthreekeycomponentsofourDeiT-LT
classes.Toimprovegeneralityinminorityclasses,weinduce
method,namelyOODdistillation,trainingtheTailExpert
low-rankfeaturesinViTbydistillingfromteacherstrained
classifierwithDRWloss,andusingSAMteacherfordistil-
fromSharpnessAwareMinimization(SAM).Theproposed
lation. AscanbeseeninTable 5,usingOODdistillation
DeiT-LTschemeallowsViTstobetrainedfromscratchas
bringsaround14%and18%improvementoverDeiT[48]
CNNsandachieveperformancecompetitivetoSotAwithout
forCIFAR-10LTandCIFAR-100LT,respectively,followed
requiringanypre-trainingonlarge-datasets.
by the other two components, which further improve the
accuracy by around 3% and 6.7% for CIFAR-10 LT and Acknowledgements. HarshRangwaniissupportedbythe
CIFAR-100LT,respectively. PMRFFellowship. WethankSumukhforthediscussions
on the draft. This work is supported by the SERB-STAR
AnalysisacrossTransformerVariants. Inthissection,we
Project(STR/2020/000128)andKIACGrant.
aim to analyze the performance of DeiT-LT across trans-DeiT-LT: Distillation Strikes Back for Vision Transformer Training on
Long-Tailed Datasets
Supplementary Material
andCIFAR-100LTdatasetsareevaluatedonheld-outsets
of10,000imageseach,equallydistributedacrossallclasses.
Table of Contents ImageNet-LT.WeusethestandardLTdatasetcreatedoutof
ImageNet[42]. ImageNet-LTconsistsof115,846training
A.ExperimentalDetails 1 images,with1280imagesintheclasswiththemostimages
A.1.Datasets . . . . . . . . . . . . . . . 1 and 5 images in the class with the least images. Out of
the1,000classessortedinthedescendingorderofsample
A.2.TrainingConfiguration. . . . . . . . 1
frequency,weconsiderclasseswithmorethan100samples
A.3.AdditionalBaselines . . . . . . . . . 2
asHeadclasses,theclasseswithsamplesbetween20and
A.4.AugmentationsforOODdistillation . 2
100 to be Mid classes and the classes with less than 20
samplesastheTailclassesasdoneinCuietal.[8].
B.DetailedResults 3
iNaturalist-2018. iNaturalist-2018[52]isareal-worldim-
C.ComparisonwithCLIPbasedmethods 4 balanceddatasetwith437,513trainingimages. Outofthe
8,142classessortedinthedescendingorderofsamplefre-
D.VisualizationofAttention 4 quency,weconsiderclasseswithmorethan100samplesas
Headclasses,theclasseswithsamplesbetween20and100
E.StatisticalSignificanceofExperiments 4 tobeMidclassesandtheclasseswithlessthan20samples
astheTailclasses,similartoImageNet-LT.
F.DetailsonLocalConnectivityAnalysis 4
A.2.TrainingConfiguration
G.Distillinglow-rankfeatures 5 In this subsection, we detail the strategies adopted to
G.1. Convergence Analysis with SAM trainDeiT-LTBase(B)modelonfourbenchmarkdatasets,
Teachers . . . . . . . . . . . . . . 7 namelyCIFAR-10LT,CIFAR-100LT,ImageNet-LT,and
iNaturalist-2018. We use the AdamW optimizer to train
H.ComputationRequirement 7 DeiT-LTfromscratchacrossallthedatasets. Theserunsuse
acosinelearningratedecayschedulewithaninitiallearn-
ingrateof5√ó10‚àí4. Alltherunsusealinearlearningrate
A.ExperimentalDetails warm-upschedulefortheinitialfiveepochs. Furthermore,
wedeploylabelsmoothingwithŒµ=0.1forallourexperi-
A.1.Datasets
mentswherethegroundtruthlabelsareusedtotraintheCLS
CIFAR-10LTandCIFAR-100LT.Weusetheimbalanced expert. Underlabelsmoothing,thetruelabelisassigneda
CIFAR-10andCIFAR-100datasetswithanexponentialde- (1‚àíŒµ)probability,andtheremainingŒµisdistributedamongst
cayinsamplesizeacrossclasses. Thisdecayisguidedby the other labels. We use hard labels as distillation targets
theImbalanceRatio(œÅ= maxiNi). Forourexperimentson fromtheteachernetworktotraintheDISTexpertclassifier
CIFAR-10LTandCIFARm -1in 0j 0N Lj T,weshowtheresultson viadistillationfromCNNteacher(Fig.2). Fortrainingthe
œÅ=100andœÅ=50. CIFAR-10LTcomprises12,406train- teachernetworkswithSAMoptimizer,wefollowthesetup
ingimagesacross10classes(œÅ=100). Outofthe10classes, mentionedin [38]
the first 3 classes are considered Head classes with more CIFAR-10 LT and CIFAR-100 LT : We train DeiT-LT
than1500imagesperclass,thefollowing4classesareMid for1200epochsonimbalancedversionsofCIFARdatasets.
(medium)classeswithmorethan250imageseachclass,and DRWlossisaddedtothetrainingoftheDISTexpertclassi-
thelast3classesaccountfortheTailclasses,witheachclass fierafter1100epochs. MixupandCutmixareusedduring
containinglessthan250imageseach. Followingasimilar theinitial1100epochsofthetraining. Assuggestedin[48],
decay, the100classesofCIFAR-100LT(10,847training weuseRepeatedAugmentationtoimprovetheperformance
sampleswithœÅ=100)arealsodividedintothreesubcate- of the DeiT-LT training. The (32√ó 32) images of CIFAR
gories:thefirst36classesareconsideredastheHeadclasses, datasetsareresizedto(224√ó224)beforefeedingintothe
Mid contains the following 35 classes, and the remaining transformerarchitecture. ForCIFAR-10LTandCIFAR-100
29classesarelabeledasTailclasses. BothCIFAR-10LT LTdatasets,ResNet-32isusedastheteachernetwork. TheTableS.1.SummaryofourtrainingproceduresusedtotrainDeiT-LTBase(B)fromscratchonCIFAR-10LT,CIFAR-100LT,ImageNet-LT
andiNaturalist-2018.
Procedure CIFAR-10LT CIFAR-100LT ImageNet-LT iNaturalist-2018
Epochs 1200 1200 1400 1000
Optimizer AdamW AdamW AdamW AdamW
EffectiveBatchSize 1024 1024 2048 2048
LR 5√ó10‚àí4 5√ó10‚àí4 5√ó10‚àí4 5√ó10‚àí4
LRschedule cosine cosine cosine cosine
WarmupEpochs 5 5 5 5
DRWstartingepoch 1100 1100 1200 900
Mixup(Œ±) 0.8 0.8 0.8 0.8
Cutmix(Œ±) 1.0 1.0 1.0 1.0
MixupandCutmixduringDRW √ó √ó ‚úì ‚úì
HorizontalFlip ‚úì ‚úì ‚úì ‚úì
ColorJitter ‚úì ‚úì ‚úì ‚úì
RandomErase ‚úì ‚úì √ó √ó
Labelsmoothing 0.1 0.1 0.1 0.1
Solarization √ó √ó ‚úì ‚úì
RandomGrayscale √ó √ó ‚úì ‚úì
RepeatedAug ‚úì ‚úì √ó √ó
AutoAug ‚úì ‚úì √ó √ó
teacheristrainedfromscratchontheseimbalanceddatasets DeiTbaselines,weusedsimilaraugmentationandotherhy-
usingLDAM+DRW+SAM[38]andcontrastivePaCo+SAM perparametersfortheViTBaselines. Wethinkthiscanbe
(trainingPaCo[8]withSAM[13]optimizer)frameworks. onereasonforthenon-convergenceoftheViT-LDAMbase-
Theinputimagestotheteacherareofsize(32√ó32),with line. WefindthatsimilarabysmalperformanceforLDAM
thesameaugmentationusedasinputimagestotheteacher baselineisalsoreportedbytherecentwork[59],whichalso
networkduringDeiT-LTtraining. resonateswithourfinding. Wethinkthatinvestigationinto
thisbehaviorisagooddirectionforfuturework.
ImageNet-LT and iNaturalist-2018. DeiT-LT is trained
fromscratchfor1400epochsonImageNet-LTandfor1000 Additionally,forafaircomparison,wedonotcompare
epochsoniNaturalist-2018. DRWlossfordistillationhead againstbaselinesthatusepre-trainingforlong-tailedrecog-
(DIST expert classifier) is initialized from epochs 1200 nition tasks. RAC [46] uses a ViT-B encoder for their re-
and900forImageNet-LTandiNaturalist-2018,respectively. trievalmodulewithweightsobtainedfrompre-trainingon
MixupandCutmixareusedthroughoutthetraining,includ- ImageNet-21K. The authors do not report on small-scale
ing the DRW training phase. More details regarding the datasets,astheyacknowledgetheunfairadvantageofusing
trainingconfigurationcanbefoundinTableS.1. theinformationpresentinthepretrainedencoder. Similarly,
forsmall-scaledatasets,LiVT[58]methodpretrainstheen-
FortheImageNet-LTandiNaturalist-2018datasets,the
coderviaMaskedGenerativePretrainingonImageNet-1k.
ResNet-50teacheristrainedfromscratchontherespective
Onthecontrary,ourDeiT-LTmethodenablestrainingViT
datasetsusingtheLDAM+DRW+SAM[38]andcontrastive
fromscratchforbothsmall-scaleandlarge-scaledatasets.
PaCo+SAM(trainingPaCo[8]withSAM[13]optimizer)
methods. Theinputimagesizeis(224√ó224)forboththe
studentandteachernetwork. A.4.AugmentationsforOODdistillation
WhilebothDeiTandourDeiT-LTpassimageswithstrong
A.3.AdditionalBaselines
augmentationstotheteachernetworkfordistillingintothe
Wewanttohighlightthatweattemptedtrainingbaselines, studentnetwork,thesetofaugmentationsusedtotrainthe
likeLDAMforvanillaViT.However,wefindthattheLDAM teachernetworkitselfdiffersbetweenthetwoapproaches.
baseline(52.75%)performsinferiorlytothevanillaViTbase- DeiTfirsttrainsalargeteacherCNN(RegNetY-16GF)using
lines(62.62%). WefindthatthelossfortheLDAMbaseline thesamesetofstrongaugmentationsasthatusedforthestu-
getsplateauedveryearly,andthemodeldoesnotfittothe dentnetwork. However,wefindthatdistillingfromasmall
trainingdataset(Fig.S.1). Tomakethecomparisonfairwith teacher CNN (such as ResNet32) trained with weak aug-Figure S.1. Comparison of training loss for vanilla ViT and TableS.2.ComparingaugmentationusedtotrainRegNetY-16GF
ViT+LDAMtrainingonCIFAR-10LT (teacher for DeiT training) and ResNet32 (teacher for DeiT-LT
training)forCIFAR-10LT.
CIFAR 10-LT Training
RegNetY-16GF ResNet32
17.5 ViT Procedure (Strong) (Weak)
ViT + LDAM
ImageSize 224√ó224 32√ó32
15.0 RandomCrop ‚úì ‚úì
HorizontalFlip ‚úì ‚úì
12.5 Mixup(Œ±) 0.8 √ó
Cutmix(Œ±) 1.0 √ó
10.0 ColorJitter 0.3 √ó
RandomErase ‚úì √ó
7.5 AutoAug ‚úì √ó
RepeatedAug ‚úì √ó
5.0
2.5 claimofexpertclassifiers. ForCIFAR-10LT(œÅ=100),the
CLSexpertclassifierisabletoreportanaccuracyof96.5%
on images of the head classes, whereas the DIST expert
0 200 400 600 800 1000 1200 classifiersettleswith72.8%onthesamesetofclasses. On
Epochs theotherhand,theDISTexpertclassifierreports93.0%ac-
curacyonthetailclasses,whichisalmost33%morethan
that of the CLS expert classifier. Like CIFAR-10 LT, the
mentationsgivesbetterperformance(seeSec.3.1)formore
CLSexpertclassifierperformsbetterontheheadclassesof
details). TableS.2comparestheaugmentationsusedtotrain
CIFAR-100LT(œÅ=100)thantheDIST,whereastheDIST
theteacherforDeiT(RegNetY-16GF)andforourmethod
expert classifier reports much higher accuracy on the tail
DeiT-LT.OurexperimentsuseResNet32astheteachernet-
classes. TheCLSclassifierachievesanaccuracyof73.7%
workforCIFAR-10LTandCIFAR-100LT,andResNet50
ontheheadclasses,andtheDISTexpertclassifiersecures
fortheImagenet-LTandiNaturalist-2018datasets. Forthe
43.1%accuracyonthetailclasses. Wenoticethatbyaverag-
PaCo teacher, we utilize the mildly strong augmentations
ingtheoutputoftheclassifiers,weareabletoreportgood
usedbythePaCo[8]methoditself. Wewouldliketoconvey,
performanceinboththemajorityandtheminorityclasses.
thatthePaCotrainingdoesnotutilizetheMixupandCutMix
CIFAR-10LTreachesanoverallaccuracyof87.3%,with
augmentationinparticularwhiletraining,whichhelpsusto
93.8%onheadclassesand85.7%ontailclasses. Similarly,
createOODsamplesforthisusingMixupandCutMixitself.
with 72.8% on the head and 31.0% on the tail, DeiT-LT
Distillingviaout-of-distribution(OOD)imagesenablesthe
isabletosecureanoverall54.8%onCIFAR-100LT.The
studenttolearntheinductivebiasesoftheteachereffectively.
resultsdemonstratethatthereisaparalleltrendintheperfor-
Thisisparticularlyhelpfulinimprovingtheperformanceon
manceofexpertsforbothCIFAR-10LTandCIFAR-100LT
thetailclassesthathavesignificantlyfewertrainingimages.
whenœÅissetto50.
Asimilartrendisseenforlarge-scaleImageNet-LTand
B.DetailedResults
iNaturalist-2018inTableS.4. ForImageNet-LT,theCLS
Performanceofindividualexperts: Ourapproachfocuses expertclassifierreports68.3%accuracyontheheadclasses,
ontrainingdiverseexperts,wheretheCLSexpertclassifier which is approximately 11% more reported by the DIST
is able to perform well on Head (majority) classes, while expert classifier. At the same time, we observe that the
theDISTexpertclassifierisabletoperformwellontheTail DISTexpertclassifierisabletogetanaccuracyof46.6%on
(minority)classes. Byaveragingtheoutputoftheindividual thetail,whichissignificantlyhigherthanthe13.5%ofthe
classifiers,weareabletoexploitthebenefitofboth. CLSexpertclassifier. ForiNaturalist-2018aswell,theCLS
Inthisportion,wediscusstheindividualperformanceof expertclassifierachievesahighaccuracyof73.8%onthe
theCLSandDISTexpertclassifiersofourproposedDeiT- headclasses,andtheDISTexpertclassifierreaches77.0%
LTmethodonCIFAR-10LT,CIFAR-100LT,ImageNet-LT, onthetailclasses. Afteraveragingtheoutputsofthetwo
andiNaturalist-2018. AscanbeseeninTableS.3andTa- classifiers, DeiT-LT reports an overall accuracy of 59.1%
ble S.4, the CLS and DIST classifiers give a contrasting for ImageNet-LT and 75.1% for iNaturalist-2018, which
performance on the head and tail classes, supporting our wouldnothavebeenpossiblebytrainingastandardVision
ssoL
niarTTableS.3.AccuracyofexpertclassifiersonHead,Mid,andTailclassesforCIFAR-10(100)LT.
CIFAR-10LT CIFAR-100LT
Imbalance Expert
Overall Head Mid Tail Overall Head Mid Tail
Average 87.3 93.8 83.7 85.7 54.8 72.8 55.9 31.0
¬±0.10 ¬±0.33 ¬±0.26 ¬±0.33 ¬±0.42 ¬±0.16 ¬±0.51 ¬±0.73
CLS 78.6 96.5 79.4 59.7 43.3 73.7 41.7 7.5
100 ¬±0.15 ¬±0.06 ¬±0.39 ¬±0.20 ¬±0.39 ¬±0.19 ¬±0.73 ¬±0.26
DIST 79.9 72.8 75.4 93.0 42.5 39.3 45.1 43.1
¬±0.31 ¬±0.92 ¬±0.18 ¬±0.15 ¬±0.48 ¬±1.64 ¬±0.47 ¬±0.33
Average 89.9 94.5 87.2 88.8 60.6 74.6 60.5 43.1
¬±0.17 ¬±0.18 ¬±0.26 ¬±0.34 ¬±0.03 ¬±0.10 ¬±0.10 ¬±0.06
CLS 84.1 96.5 83.3 72.8 49.6 76.0 50.5 15.9
50 ¬±0.33 ¬±0.12 ¬±0.66 ¬±0.55 ¬±0.21 ¬±0.31 ¬±0.46 ¬±0.41
DIST 83.2 74.6 81.8 93.6 48.0 44.0 48.4 52.6
¬±0.23 ¬±0.51 ¬±0.21 ¬±0.08 ¬±0.20 ¬±0.25 ¬±0.36 ¬±0.07
TableS.4.AccuracyofexpertsonHead,MidandTailclassesforImageNet-LTandiNaturalist-2018.
ImageNet-LT iNaturalist-2018
Expert
Overall Head Mid Tail Overall Head Mid Tail
Average 59.1 66.7 58.3 40.0 75.1 70.3 75.2 76.2
CLSexpertclassifier 47.5 68.3 40.0 13.5 65.6 73.8 65.8 63.1
DISTexpertclassifier 56.4 57.2 58.6 46.6 72.9 56.1 73.2 77.0
Transformer(ViT)withasingleclassifier. almostallcases. WefindthatDeiT-IIIattentionmapsare
betterincomparisontoViT,butitalsooftengetsconfused
C.ComparisonwithCLIPbasedmethods (eg. BellPepper,SeaSnakeetc.) comparedtoDeiT-LT.
Recently,someapproachessuchasVL-LTR[46]andPEL
E.StatisticalSignificanceofExperiments
[43]haveadoptedapre-trainedCLIPbackbonetoaddress
long-tailedrecognitionchallenges. Asindicatedoriginally, Inthissection,wepresenttheresultsofourexperimentson
andalsoreinforcedby[57],CLIPistrainedonlarge-scale CIFAR-10 LT and CIFAR-100 LT (œÅ = 100, 50)(as in Ta-
balanceddataset(400MImage-Textpair).Asthereisalotof bleS.3),withthreedifferentrandomseeds. InTableS.3,we
overlappingconceptsbetweenbalancedCLIPdataandlong- reporttheaverageperformanceoftheexpertclassifiersalong
taileddatasets(ImageNet-LTandiNat-18),theperformance withthestandarderrorforeach. Thelowerrordemonstrates
oftheCLIPfine-tunedmethodsdoesnotindicatemeaningful thattheDeiT-LTtrainingprocedureisstableandquiterobust
progress on long-tail learning tasks, as CLIP has already acrossrandomseeds.
seentailconceptsinabundance. Duetothisunfairnessin
trainingdatasetsused,werefrainfromcomparingtheCLIP F.DetailsonLocalConnectivityAnalysis
fine-tunedmodels(i.e.,VL-LTR,PELetc.) withDeiT-LT
modelstrainedfromscratch. Wecomputethemeanattentiondistanceforsamplesoftail
classes (i.e. 7,8,9 class for CIFAR-10) using the method
D.VisualizationofAttention proposedbyRaghuetal.[36]. Foreachheadpresentinself-
attentionblocks,wecalculatethedistanceofthepatchesit
TodemonstratetheeffectofdistillationinDeiT-LT,wevi-
attendsto. Morespecifically,weweighthedistanceinthe
sualizetheattentionofbaselinemethodsonImageNet-LT
pixelspacewiththeattentionvalueandthenaverageit. This
withoutdistillation(ViTandDeiT-III)andcompareitwith
isaveragedforalltheimagespresentinthetailclasses. We
DeiT-LT.AsDeiT-LTcontainsboththeDISTtokenandthe
utilizethecodeprovidedhereasourreference1. Weshowin
CLStoken,forvisualizationweaveragetheattentionacross
Fig. 4bthatforearlyblocks(1and2)ofViT,theproposed
both. WeusetheAttentionRollout[45]methodforvisual-
DeiT-LTmethodcontainslocalfeatures. AswegofromViT
ization. Fig.S.2showstheresultofattentionfordifferent
todistilledDeiTtoproposedDeiT-LT,wefindthatfeatures
methods. ItcanbeclearlyobservedthatDeiT-LTisableto
localizeattentionatthecorrectpositionofobjects,across 1https://github.com/sayakpaul/probing-vitsLonghorn
Overskirt Crayfish Sea-snake Cuirass Trailer-truck Bell pepper Tree frog
beetle
FigureS.2.VisualcomparisonoftheattentionmapsfromViT-B,DeiT-III[51]andDeiT-LT(ours)ontheImageNet-LTdataset,computed
usingthemethodofAttentionRollout[1].
becomemorelocal,whichexplainsthegeneralizabilityof
DeiT-LTfortailclasses. Tofurtherconfirmourobservations,
wealsoprovidelocalconnectivityplotsforthetailclasses
100
oftheCIFAR-100dataset(Fig.S.3). WeobservethatDeiT-
LT produces highly local features. Further, we find that
the DeiT baseline (Table 2), which is inferior to ViT for
80
CIFAR-100,showsthepresenceofglobalfeatures. Hence,
the local connectivity correlates well with generalization
on tail classes. The correlation of locality of features to
60
generalization has also been observed by [36], who find
thatusingtheImageNet-21kdatasetforpre-trainingleads ViT
tomorelocalandgeneralizablefeaturesincomparisonto 40 DeiT
networkspre-trainedonImageNet-1kdata. DeiT-LT(ours)
G.Distillinglow-rankfeatures 0 1 2 3 4 5 6 7 8 9 10 11
Transformer Heads
In our proposed method, as the DIST token serves as the
expertontailclasses,itisimportanttoensurethatitlearns Figure S.3. Mean attention distance for early blocks (1,2) for
CIFAR-100LTtailtrainingimages.
generalizablefeaturesforminorityclassesthatarelessprone
tooverfitting. Asstatedin[3],traininganetworkwithSAM
optimizerleadstolow-rankfeatures. Inthissubsection,we
dimensionofthefeaturerepresentationfromDISTtoken.
investigatethefeaturerankoftheDISTtokenthatisdistilled
Upon centering the columns of Fall , we decompose
viaaSAM-basedteacher.
nh,d
thefeaturematrixasU,S,VT =SVD(Fall ),andproject
CalculatingFeatureRank. Considertwosetsofimages FminusingtherightsingularvectorsV
an sh,d
X ,X ‚äÇX,whereX ,X refertothesetofimages
nt,d
all min all min
fromalltheclassesandminority(tail)classes,respectively, Fmin(k)=Fmin‚àóV
with X being the set of all images. We construct feature
proj nt,d k
matrices Fall and Fmin, where n and n are the num- whereV containsthetopk singularvectors(principal
nh,d nt,d h t k
ber of images in X and X respectively, and d is the componenets). We calculate our rank as the least k that
all min
B-TL-TieD
egamI
tupnI
B-TiV
B-III-TieD
)sruO(
ecnatsiD
noitnettA
naeM
)noitnetta
hgih
setacidni
deR(
edutingaM
noitnettACLS Token DIST Token
600 600
500 500
400 400
300 300
200 DeiT 200 DeiT
DeiT-LT (LDAM+DRW Teacher) DeiT-LT (LDAM+DRW Teacher)
DeiT-LT (LDAM+DRW+SAM Teacher) DeiT-LT (LDAM+DRW+SAM Teacher)
100 100
DeiT-LT (PaCo Teacher) DeiT-LT (PaCo Teacher)
DeiT-LT (PaCo+SAM Teacher) DeiT-LT (PaCo+SAM Teacher)
0 0
0 2 4 6 8 10 0 2 4 6 8 10
Transformer Blocks Transformer Blocks
(a)RankofViTfromDistillationofCNNteachersusingCLStoken (b)RankofViTfromDistillationofCNNteachersusingDISTtoken
FigureS.4.Wecomparetherankcalculatedusingfeaturesfromthea)CLStokenandb)DISTtokenwhentrainedonCIFAR-10LT.Our
DeiT-LTcapturesbothfine-grainedfeatures(fromhigh-rankCLStoken)andgeneralizablefeatures(fromlow-rankDISTtoken).
ImageNet-LT Training CIFAR-100 LT Training
60
50
50
40
40
30 30
20 20
10
10
DeiT-LT (PaCo+SAM Teacher) DeiT-LT (PaCo+SAM Teacher)
0 DeiT-LT (PaCo Teacher) DeiT-LT (PaCo Teacher)
0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200
Epochs Epochs
FigureS.5.ValidationAccuracyPlotsfortheImageNet-LT(left)andCIFAR-100LT(right).ItcanbeobservedthatDeiT-LTtrainedwith
SAMteachersconvergesfasterthanvanillateachers.
satisfies alizablecharacteristicsrelevantacrossdifferentcategories
ofimagesinanimbalanceddataset. Bylearningsemantic
||Fmin‚àíFmin (k)||2 similarfeatures,ourtrainingofDISTtokenensuresgood
nt,d recon ‚â§0.01 representationlearningforminorityclassesbyleveraging
||Fmin||2
nt,d thediscriminativefeatureslearnedfrommajorityclasses.
whereFmin (k)isanapproximatereconstructedfeature
recon
matrixgivenbyFmin (k)=Fmin(k)‚àóV T.
recon proj k
On the other hand, we observe that CLS token learns
As shown in Fig. S.4b, we find that the DIST token high-rankfeaturerepresentations(Fig.S.4a),signifyingthat
trained with a SAM-based teacher reports a lower rank. it captures intricately detailed information. Our DeiT-LT,
As we are able to use the same principal components to thus, captures a wide range of information by using the
represent both the majority and minority classes‚Äô feature predictionsmadeusingbothfine-graineddetailsfromCLS
representation,itsignifiesthattheDISTtokenlearnsgener- tokenandgeneralizablefeaturesfromDISTtoken.
knaR
erutaeF
ycaruccA
knaR
erutaeF
ycaruccAG.1.ConvergenceAnalysiswithSAMTeachers
Wefindthatmodelsdistilledfromtheteacherstrainedusing
SAM[13]convergefasterthantheusualCNNteachers. We
providetheanalysisfortheDeit-LT(PaCo+SAM)andDeiT-
LT(PaCo)ontheImageNet-LTandCIFAR-100datasetsin
Fig.S.5. WeobservethatmodelswithSAM,coveragemuch
faster,particularlyfortheImageNet-LTdataset,demonstrat-
ingtheincreasedconvergencespeedforthedistillation. This
canbeattributedtothefactthatlow-rankmodelsaresimpler
instructureandaremucheasiertodistilltothetransformer.
H.ComputationRequirement
FortrainingourproposedDeiT-LTmethodonCIFAR-10LT
andCIFAR-100LT,weusetwoNVIDIARTX3090GPU
cardswith24GiBmemoryeach,withbothdatasetsrequiring
about15hourstotraintotraintheViTstudent. Wetrainthe
DeiT-LTstudentnetworkonfourNVIDIARTXA5000GPU
cardsforthelarge-scaleImageNet-LTdatasetandonfour
NVIDIAA100GPUcardsfortheiNaturalist-2018dataset,
in61and63hours,respectively.References [16] Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun
Seo, Beomsu Kim, and Buru Chang. Disentangling label
[1] SamiraAbnarandWillemZuidema. Quantifyingattention
distributionforlong-tailedvisualrecognition.InCVPR,2021.
flowintransformers. arXivpreprintarXiv:2005.00928,2020.
2
6,8,5
[17] Yan Hong, Jianfu Zhang, Zhongyi Sun, and Ke Yan.
[2] SandhiniAgarwal,GretchenKrueger,JackClark,AlecRad-
Safa:sample-adaptive feature augmentation for long-tailed
ford,JongWookKim,andMilesBrundage. Evaluatingclip:
imageclassification. InECCV,2022. 7
towardscharacterizationofbroadercapabilitiesanddown-
[18] Ahmet Iscen, Andre¬¥ Araujo, Boqing Gong, and Cordelia
streamimplications. arXivpreprintarXiv:2108.02818,2021.
Schmid. Class-balanced distillation for long-tailed visual
2
recognition. 2021. 7
[3] MaksymAndriushchenko,DaraBahri,HosseinMobahi,and
[19] MuhammadAbdullahJamal,MatthewBrown,Ming-Hsuan
NicolasFlammarion. Sharpness-awareminimizationleadsto
Yang,LiqiangWang,andBoqingGong. Rethinkingclass-
low-rankfeatures. arXivpreprintarXiv:2305.16292,2023. 5
balancedmethodsforlong-tailedvisualrecognitionfroma
[4] JiaruiCai,YizhouWang,andJenq-NengHwang. Ace:Ally
domainadaptationperspective. InCVPR,2020. 6
complementaryexpertsforsolvinglong-tailedrecognitionin
[20] BingyiKang,SainingXie,MarcusRohrbach,ZhichengYan,
one-shot. InICCV,2021. 6
AlbertGordo,JiashiFeng,andYannisKalantidis.Decoupling
[5] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,
representationandclassifierforlong-tailedrecognition.arXiv
andTengyuMa. Learningimbalanceddatasetswithlabel-
preprintarXiv:1910.09217,2019. 2,6,7
distribution-awaremarginloss. InNeurIPS,2019. 1,2,4,5,
[21] BingyiKang,YuLi,SaXie,ZehuanYuan,andJiashiFeng.
6,7
Exploringbalancedfeaturespacesforrepresentationlearning.
[6] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
In International Conference on Learning Representations,
Usunier, AlexanderKirillov, andSergeyZagoruyko. End-
2020. 7
to-endobjectdetectionwithtransformers. InEuropeancon-
[22] Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m:
ferenceoncomputervision,pages213‚Äì229.Springer,2020.
Imbalancedclassificationviamajor-to-minortranslation. In
1
CVPR,2020. 2
[7] JunChen,AniketAgarwal,SherifAbdelkarim,DeyaoZhu,
[23] GaneshRamachandraKini,OrestisParaskevas,SametOy-
and Mohamed Elhoseiny. Reltransformer: A transformer-
mak, and Christos Thrampoulidis. Label-imbalanced and
based long-tail visual relationship recognition. In CVPR,
group-sensitiveclassificationunderoverparameterization. In
2022. 2,7
AdvancesinNeuralInformationProcessingSystems,pages
[8] JiequanCui,ZhishengZhong,ShuLiu,BeiYu,andJiayaJia.
18970‚Äì18983.CurranAssociates,Inc.,2021. 1,2,6
Parametriccontrastivelearning. InICCV,2021. 2,6,7,1,3
[24] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
[9] YinCui,MenglinJia,Tsung-YiLin,YangSong,andSerge
layersoffeaturesfromtinyimages. 2009. 5
Belongie. Class-balancedlossbasedoneffectivenumberof
samples. InCVPR,2019. 1,2,4,6,7 [25] JunLi,ZichangTan,JunWan,ZhenLei,andGuodongGuo.
Nestedcollaborativelearningforlong-tailedvisualrecogni-
[10] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
tion. InCVPR,pages6949‚Äì6958,2022. 1
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.
InCVPR,2009. 1 [26] MengkeLi,Yiu-mingCheung,andYangLu. Longtailvisual
recognitionviagaussiancloudedlogitadjustment. InCVPR,
[11] AlexeyDosovitskiy,PhilippFischer,JostTobiasSpringen-
2022. 6,7
berg,MartinRiedmiller,andThomasBrox. Discriminative
unsupervisedfeaturelearningwithexemplarconvolutional [27] TianhaoLi,LiminWang,andGangshanWu.Selfsupervision
neuralnetworks. IEEETPAMI,2015. 1 to distillation for long-tailed visual recognition. In ICCV,
2021. 6
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [28] TianhongLi,PengCao,YuanYuan,LijieFan,YuzheYang,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted
vainGelly,etal. Animageisworth16x16words:Transform- supervisedcontrastivelearningforlong-tailedrecognition.
ersforimagerecognitionatscale. InICLR,2021. 1,3,6,7, InProceedingsoftheIEEE/CVFConferenceonComputer
8 VisionandPatternRecognition,pages6918‚Äì6928,2022. 7
[13] PierreForet, ArielKleiner, HosseinMobahi, andBehnam [29] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang,
Neyshabur. Sharpness-aware minimization for efficiently BoqingGong,andStellaXYu.Large-scalelong-tailedrecog-
improvinggeneralization. arXivpreprintarXiv:2010.01412, nitioninanopenworld. InCVPR,2019. 5
2020. 2,3,5,6,7 [30] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu
[14] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A Nguyen,PulakPurkait,RaviGarg,AlanBlair,ChunhuaShen,
datasetforlargevocabularyinstancesegmentation. InPro- andAntonvandenHengel.Retrievalaugmentedclassification
ceedingsoftheIEEEConferenceonComputerVisionand forlong-tailvisualrecognition. InCVPR,2022. 2,3,6
PatternRecognition,2019. 1 [31] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
[15] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. regularization. arXivpreprintarXiv:1711.05101,2017. 6
Deepresiduallearningforimagerecognition.InCVPR,2016. [32] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh
1 Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar.Long-tail learning via logit adjustment. arXiv preprint [45] ZichangTan,YangYang,JunWan,HanyuanHang,Guodong
arXiv:2007.07314,2020. 1,2,6,7 Guo, and Stan Z Li. Attention-based pedestrian attribute
[33] GauravKumarNayak,KondaReddyMopuri,andAnirban analysis. TIP,28(12):6126‚Äì6140,2019. 4
Chakraborty. Effectivenessofarbitrarytransfersetsfordata- [46] ChangyaoTian,WenhaiWang,XizhouZhu,JifengDai,and
freeknowledgedistillation. InProceedingsoftheIEEE/CVF YuQiao. Vl-ltr: Learningclass-wisevisual-linguisticrep-
WinterConferenceonApplicationsofComputerVision,pages resentationforlong-tailedvisualrecognition. InEuropean
1430‚Äì1438,2021. 4 ConferenceonComputerVision,pages73‚Äì91.Springer,2022.
[34] NedjmaOusidhoum,XinranZhao,TianqingFang,Yangqiu 6,2,4
Song,andDit-YanYeung. Probingtoxiccontentinlargepre- [47] IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,Lu-
trainedlanguagemodels. InProceedingsofthe59thAnnual casBeyer,XiaohuaZhai,ThomasUnterthiner,JessicaYung,
MeetingoftheAssociationforComputationalLinguisticsand AndreasSteiner,DanielKeysers,JakobUszkoreit,etal. Mlp-
the11thInternationalJointConferenceonNaturalLanguage mixer:Anall-mlparchitectureforvision. Advancesinneural
Processing(Volume1:LongPapers),pages4262‚Äì4274,2021. informationprocessingsystems,34:24261‚Äì24272,2021. 1
2 [48] HugoTouvron,MatthieuCord,MatthijsDouze,Francisco
[35] IlijaRadosavovic,RajPrateekKosaraju,RossGirshick,Kaim- Massa,AlexandreSablayrolles,andHerveJegou. Training
ingHe,andPiotrDolla¬¥r.Designingnetworkdesignspaces.In data-efficientimagetransformersamp;distillationthrough
ProceedingsoftheIEEE/CVFconferenceoncomputervision attention. InInternationalConferenceonMachineLearning,
andpatternrecognition,pages10428‚Äì10436,2020. 4 pages10347‚Äì10357,2021. 1,2,3,4,5,6,8
[49] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
[36] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,
Gabriel Synnaeve, and Herve¬¥ Je¬¥gou. Going deeper with
ChiyuanZhang,andAlexeyDosovitskiy. Dovisiontrans-
imagetransformers. InProceedingsoftheIEEE/CVFInter-
formersseelikeconvolutionalneuralnetworks? Advances
nationalConferenceonComputerVision,pages32‚Äì42,2021.
inNeuralInformationProcessingSystems,34:12116‚Äì12128,
6
2021. 5,4
[50] HugoTouvron,MatthieuCord,AlaaeldinEl-Nouby,Jakob
[37] HarshRangwani, KondaReddyMopuri, andRVenkatesh
Verbeek, and Herve Jegou. Three things everyone
Babu. Classbalancingganwithaclassifierintheloop. In
should know about vision transformers. arXiv preprint
ConferenceonUncertaintyinArtificialIntelligence(UAI),
arXiv:2203.09795,2022. 1
2021. 2
[51] HugoTouvron, MatthieuCord, andHerve¬¥ Je¬¥gou. Deitiii:
[38] HarshRangwani, SumukhKAithal, Mayank Mishra, and
Revengeofthevit. InComputerVision‚ÄìECCV2022: 17th
VenkateshBabuR. Escapingsaddlepointsforeffectivegen-
EuropeanConference,TelAviv,Israel,October23‚Äì27,2022,
eralizationonclass-imbalanceddata. InAdvancesinNeural
Proceedings,PartXXIV,pages516‚Äì533.Springer,2022. 1,
InformationProcessingSystems,pages22791‚Äì22805.Curran
5,6,7
Associates,Inc.,2022. 4,5,6,7,1,2
[52] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
[39] HarshRangwani,NamanJaswani,TejanKarmali,VarunJam-
ChenSun,AlexShepard,HartwigAdam,PietroPerona,and
pani,andR.VenkateshBabu. Improvinggansforlong-tailed
Serge Belongie. The inaturalist species classification and
datathroughgroupspectralregularization. InEuropeanCon-
detectiondataset. InCVPR,2018. 1,5
ferenceonComputerVision(ECCV),2022. 2
[53] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
[40] HarshRangwani‚àó,LavishBansal‚àó,KartikSharma,TejanKar-
reit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIllia
mali,VarunJampani,andR.VenkateshBabu. Noisytwins:
Polosukhin. Attentionisallyouneed. Advancesinneural
Class-consistentanddiverseimagegenerationthroughstyle-
informationprocessingsystems,30,2017. 1,3
GANs.InConferenceonComputerVisionandPatternRecog-
[54] Angelina Wang and Olga Russakovsky. Overwriting pre-
nition(CVPR),2023. 2
trained bias with finetuning data. In Proceedings of the
[41] JiaweiRen,CunjunYu,ShunanSheng,XiaoMa,HaiyuZhao, IEEE/CVF International Conference on Computer Vision,
ShuaiYi,andHongshengLi.Balancedmeta-softmaxforlong- pages3957‚Äì3968,2023. 2,3
tailedvisualrecognition. arXivpreprintarXiv:2007.10740, [55] Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, and Lei
2020. 2,7 Wang. Contrastivelearningbasedhybridnetworksforlong-
[42] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San- tailedimageclassification. InCVPR,2021. 2,6
jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy, [56] XudongWang,LongLian,ZhongqiMiao,ZiweiLiu,and
Aditya Khosla, Michael Bernstein, et al. Imagenet large Stella X Yu. Long-tailed recognition by routing diverse
scalevisualrecognitionchallenge. Internationaljournalof distribution-awareexperts. InICLR,2021. 1,3,6,7
computervision,115(3):211‚Äì252,2015. 5,1 [57] HuXu,SainingXie,XiaoqingEllenTan,Po-YaoHuang,Rus-
[43] Jiang-XinShi,TongWei,ZhiZhou,Xin-YanHan,Jie-Jing sellHowes,VasuSharma,Shang-WenLi,GargiGhosh,Luke
Shao,andYu-FengLi. Parameter-efficientlong-tailedrecog- Zettlemoyer,andChristophFeichtenhofer. Demystifyingclip
nition. arXivpreprintarXiv:2309.10019,2023. 4 data. arXivpreprintarXiv:2309.16671,2023. 4
[44] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia [58] ZhengzhuoXu,RuikangLiu,ShuoYang,ZenghaoChai,and
Schmid. Segmenter:Transformerforsemanticsegmentation. ChunYuan. Learningimbalanceddatawithvisiontransform-
InProceedingsoftheIEEE/CVFinternationalconferenceon ers. InIEEEConferenceonComputerVisionandPattern
computervision,pages7262‚Äì7272,2021. 1 Recognition(CVPR),2023. 6,7,2[59] ZhengzhuoXu,ShuoYang,XingjunWang,andChunYuan.
Rethinklong-tailedrecognitionwithvisiontransforms. In
ICASSP2023-2023IEEEInternationalConferenceonAcous-
tics, Speech and Signal Processing (ICASSP), pages 1‚Äì5.
IEEE,2023. 6,2
[60] Han-JiaYe,Hong-YouChen,De-ChuanZhan,andWei-Lun
Chao. Identifyingandcompensatingforfeaturedeviationin
imbalanceddeeplearning. arXivpreprintarXiv:2001.01385,
2020. 2
[61] SangdooYun, DongyoonHan, SeongJoonOh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larizationstrategytotrainstrongclassifierswithlocalizable
features. InProceedingsoftheIEEE/CVFinternationalcon-
ferenceoncomputervision,pages6023‚Äì6032,2019. 4
[62] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
DavidLopez-Paz. mixup:Beyondempiricalriskminimiza-
tion. InICLR,2018. 2,4
[63] SongyangZhang,ZemingLi,ShipengYan,XumingHe,and
JianSun. Distributionalignment: Aunifiedframeworkfor
long-tailvisualrecognition. InCVPR,2021. 7
[64] YongshunZhang,Xiu-ShenWei,BoyanZhou,andJianxin
Wu.Bagoftricksforlong-tailedvisualrecognitionwithdeep
convolutionalneuralnetworks. InAAAI,2021. 6
[65] ZhishengZhong,JiequanCui,ShuLiu,andJiayaJia.Improv-
ingcalibrationforlong-tailedrecognition. InCVPR,2021. 6,
7
[66] BoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,
andAntonioTorralba. Places:A10millionimagedatabase
forscenerecognition. IEEETPAMI,2017. 2
[67] BoyanZhou,QuanCui,Xiu-ShenWei,andZhao-MinChen.
Bbn:Bilateral-branchnetworkwithcumulativelearningfor
long-tailedvisualrecognition. InCVPR,2020. 1,6
[68] Yixuan Zhou, Yi Qu, Xing Xu, and Hengtao Shen. Imb-
sam:Acloserlookatsharpness-awareminimizationinclass-
imbalancedrecognition. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages11345‚Äì
11355,2023. 7