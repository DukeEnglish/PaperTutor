FLIGHTSCOPE: A DEEP COMPREHENSIVE
ASSESSMENT OF AIRCRAFT DETECTION
ALGORITHMS IN SATELLITE IMAGERY
SafouaneELGHAZOUALI* ArnaudGUCCIARDI
MachineLearningResearch MachineLearningResearch
&Development,TOELTLLCAIlab &Development,TOELTLLCAIlab
Winterthur,Swintzerland Winterthur,Swintzerland
safouane.elghazouali@toelt.ai arnaud.gucciardi@toelt.ai
NicolaVENTURI MichaelRUEEGSEGGER
CompetenceCenterforAI CompetenceCenterforAI
andSimulation,armasuisseS+T, andSimulation,armasuisseS+T,
Thun,Swintzerland Thun,Swintzerland
nicola.venturi@armasuisse.ch michael.rueegsegger@armasuisse.ch
UmbertoMICHELUCCI
MachineLearningResearch&Development,TOELTLLCAIlab
Winterthur,Switzerland
ComputerScienceDepartment,LucerneUniversity
ofAppliedScienceandArts,Luzern,Swintzerland
umberto.michelucci@toelt.ai/umberto.michelucci@hslu.ch
ABSTRACT
Object detection in remotely sensed satellite pictures is fundamental in
many fields such as biophysical, and environmental monitoring. While
deeplearningalgorithmsareconstantlyevolving,theyhavebeenmostly
implementedandtestedonpopularground-basedtakenphotos. Thispa-
percriticallyevaluatesandcomparesasuiteofadvancedobjectdetection
algorithmscustomizedforthetaskofidentifyingaircraftwithinsatellite
imagery. Using the large HRPlanesV2 dataset, together with a rigorous
validationwiththeGDITdataset,thisresearchencompassesanarrayof
methodologiesincludingYOLOversions5and8,FasterRCNN,CenterNet,
RetinaNet,RTMDet,andDETR,alltrainedfromscratch. Thisexhaustive
trainingandvalidationstudyrevealYOLOv5asthepreeminentmodelfor
thespecificcaseofidentifyingairplanesfromremotesensingdata,show-
casinghighprecisionandadaptabilityacrossdiverseimagingconditions.
Thisresearchhighlightthenuancedperformancelandscapesoftheseal-
gorithms, with YOLOv5 emerging as a robust solution for aerial object
detection,underliningitsimportancethroughsuperiormeanaveragepre-
cision,Recall,andIntersectionoverUnionscores. Thefindingsdescribed
hereunderscorethefundamentalroleofalgorithmselectionalignedwith
thespecificdemandsofsatelliteimageryanalysisandextendacompre-
hensiveframeworktoevaluatemodelefficacy. Thebenchmarktoolkitand
codes,availableviaGitHub,aimstofurtherexplorationandinnovationin
therealmofremotesensingobjectdetection,pavingthewayforimproved
analyticalmethodologiesinsatelliteimageryapplications.
4202
rpA
3
]VC.sc[
1v77820.4042:viXraKeywords Objectdetection·survey·remotesensing·satelliteimage·aircraftlocalization
1 Introduction
RemotesensingplaysafundamentalroleinacquiringinformationabouttheEarth’ssurface
usingvarioustypesofvisionsensorssuchasThermalInfra-Red(TIR)andRGBcameras[1].
Thisfieldencompassesawiderangeoftechnologiesandmethodologiesaimedatcapturing,
analyzing,andinterpretingdatafromvarioussources. Withintherealmofremotesensing,
oneofthemostsignificantapplicationsisthedetectionandlocalizationofsmallobjects[2].
Objectdetectionfromsatelliteimageryholdsgreatimportanceinvariousdomains,suchas
defenceandmilitaryapplications[3],urbanstudies[4],airportsurveillance,vesseltraffic
monitoring[5]andtransportationinfrastructuredetermination[6,7]. Unlikephotographic
pictures,remotesensingimagesobtainedfromsatellitesensorsaremoredifficulttointerpret
duetofactorssuchasatmosphericinterference,viewpointvariation,backgroundclutter,
andilluminationdifferences[8,9]. Additionally,satelliteimagescoverlargerareas,typically
surface like 10 × 10 km2 per frame, representing the intricate landscape of the Earth’s
surfacewithtwo-dimensionalimagesthatpossesslessspatialdetailcomparedtodigital
photographsfromcameras.
Traditionalapproachestoaircraftdetectionreliedonmanualfeatureengineeringandma-
chinelearningtechniques. However,thesemethodsoftenstrugglestohandlethecomplexi-
tiesofsatelliteimageryandachievehighaccuracy. Theadventofdeeplearning,particularly
ConvolutionalNeuralNetworks(CNNs),hasrevolutionizedobjectdetectiontasksbyen-
ablingtheautomaticextractionofintricatevisualrepresentations.Onenotabledeeplearning
methodforobjectdetectionistheYouOnlyLookOnce(YOLO)[10]. YOLOdividestheinput
image into a grid and predicts bounding boxes and class probabilities directly from the
grid cells. Its architecture allows for real-time detection and can handle multiple object
classessimultaneously. However,YOLOmayencounterchallengesinaccuratelylocalizing
smallobjectsduetothegridcellstructureandlimitedreceptivefields. Anotherpopular
deeplearningtechniqueistheSingleShotMultiBoxDetection(SSD)approach[11],which
utilizesaseriesofconvolutionallayerstogenerateadiversesetofdefaultboundingboxesat
differentscalesandaspectratios.Byapplyingasetofpredefinedanchorboxestothefeature
maps,SSDperformsmulti-scaleobjectdetectionefficiently. However,itmayfacedifficulties
in detecting small objects and suffers from a large number of default boxes, leading to
computationaloverhead. Region-basedCNNs(RCNN)[12]anditsvariants,suchasFast
RCNN[13],FasterRCNN[14]andMaskRCNN[15],havealsobeenwidelyusedforobject
detectiontasks. Thesemethodsemployatwo-stageapproach,inwhichpotentialobject
regionsarefirstproposedandthenclassifiedandrefined. Byutilizingregionproposals,
thesemethodsachieveaccuratelocalizationandexhibitstrongperformance. However,they
are computationally expensive and slower than one-stage detectors such as YOLO and
SSD.InadditiontoYOLO,SSD,andRCNNvariants,thereareseveralotherdeeplearning
methodsthathavebeenexploredforobjectdetection,suchasEfficientDet[16],RetinaNet
[17], and CenterNet [18]. More recently, the success of attention mechanism in the field
oflanguageprocessinghasreceivedalotofconsiderationandhasbeenbroughtintothe
fieldofcomputervisionaswell[19]. Thisrisehasledtoaninterestingimprovementof
performanceinmanycomputervisionsubfields,includingimageclassification[20]and
objectdetection[21]. Eachofthesemethodshasitsownuniquearchitectureandadvantages,
aimingtoimprovebothaccuracyandefficiencyinobjectdetectiontasks.
Theobjectiveofthisstudyistobenchmarkandcomparemultiplestate-of-the-artobject
detectionmethodspreparedandtrainedspecificallyfortheusecaseofaircraftdetection
insatelliteimages. Thefocusisontheirapplicationinsatellitesurveillance,andairtraffic
management. This paper significantly contributes to the research on satellite imagery
analysisbyimplementing,trainingandvalidatingonevariantoftheeightleadingobject
detectionneuralnetworkarchitectureslistedinTable1,alongsideothercutting-edgedeep
learningarchitectures. Theauthorshavedirectlyimplementedandtestedeachnetwork
tocontrolthetestingconditionsandpreventbiasorspuriousoutcomesthatmightarise.
In addition to a detailed overview of object detection models that is fundamental for
both researchers and practitioners, this work provides also a thorough examination of
2Table1: Overviewofthedifferentneuralnetworkarchitecturesimplemented,trainedand
validatedinthispaper.
ModelsTested ShortDescription
YOLOv5[22] Real-time object detection model that processes images in
one pass through a CNN, widely used for various applica-
tionsincludingautonomousvehicles,surveillancesystems,and
robotics.
YOLOv8[10] Updated version of the YOLO object detection system, incor-
porating advancements in network architecture and training
techniquestoachievebetterefficiencyinreal-timeobjectdetec-
tiontasks.
RTMDet[23] AnimprovedCNNnetworkforhigheraccuracywhilemaintain-
ingthesamereal-timeperformanceoftheYOLOmodelseries.
DETR[24] New object detection architecture based on transformers and
attentionmechanisms. IthasproveditsefficiencyontheCOCO
dataset.
FasterRCNN[14] Region-BasedCNNisanarchitectureforobjectdetectionthat
has proven its efficiency in accurate bounding box extraction.
FasterRCNNisanalgorithmthatimprovesthedetectionspeed
ofRCNNs.
SSD[11] Anotherreal-timeobjectdetectionmodelthatpredictsbounding
boxesfromfeaturesmapatmultiplescales.
CenterNet[18] Thismodel,initiallybasedontheCornerNet[25]architecture,
hasbeenabletoachievestate-of-the-artperformanceonCOCO
datasetforobjectlocalizationintermofreal-timeandaccuracy.
RetinaNet[26] Real-timeobjectdetectionmodelthataddressestheimbalance
betweenforegroundandbackgroundexamplesduringtraining
usinganovelfocallossfunction.
theirperformance,precision,andcomputationalcomplexity. Thefindingsprovidecritical
information that improves the selection process for the most efficient aircraft detection
methods in satellite imagery. This is supported by comprehensive training and initial
validationontheHRPlanesv2andfurthervalidationonGDITdatasets(fordetailsonthe
datasetsseeSection3),markinganimportantadvancementinthestudyoftheprecisionand
efficiencyofremotesensingtechnologiesandtheirapplicationinreal-worldscenarios,thus
underliningitssubstantialrelevanceandpotentialimpactonfutureresearchinthefield.
Additionally,tomakeamoreusefulcontributiontothecomputervisioncommunity,allthe
codeusedforthebenchmarksinthispaperisavailable,andcouldbereproducedfromthe
GitHubrepositoryathttps://github.com/toelt-llc/FlightScope_Bench.
Followingtheintroduction,therestofthepaperisstructuredasfollow: similarcomparative
studiesarediscussedandreviewedinSection2. Anoverviewofexistingdatasetsincluding
airplanesinsatelliteimagesisdiscussedinSection3,whileinSection4amoredetailed
state-of-the-artdescriptionoftheobjectdetectionarchitectures(listedinTable1)isgiven.
Section5presentthesetupofthebenchmark,alongwiththeresultsanddiscussion. Finally,
Section6concludesandsummarizesthefindingsofthiscomparativestudy.
2 Relatedwork
Manyremotesensingmethodsandmodelshavebeenstudiedandproposedduringthe
past decade in various fields such as environmental monitoring [27], object and image
Geo-localization[28,29],urbanplanning[30],andagriculture[29]. Inthecontextofobject
detection, the methods that are usually proposed are trained, validated and tested on
imagesgatheredfromgroundandusualvisionsensors. Onthosetypesofimages,good
3performances are observed, therefore, the need for performance evaluation of remote
sensingtypesofimages. Remotesensingimagerytypicallyencompassesvastareaswith
varyingresolutions,makingthedetectionofsmallobjects,suchasvehiclesorinfrastructure,
particularlychallenging[31]. Additionally,factorssuchasvaryingilluminationconditions,
occlusionsduetoweather,andobjectscalevariationsfurthercomplicatethetask.
Asimilarstudytothiswork,conductedbyAlganci,et. al. [9]delvesintothedetectionof
smallobjectsfromsatelliteimagery. Thisresearchfocussesonevaluatingtheperformance
ofthreestate-of-the-artconvolutionalneuralnetwork(CNN)-basedobjectdetectionmodels
specificallytailoredforidentifyingairplanesinveryhigh-resolution(VHR)satelliteimages.
The authors underscore the importance of accurate and efficient detection methods in
satelliteimageryduetoitslargedatasizeandexpansiveaerialcoverage. Fortheirstudy,
theauthorshaveusedtheDOTAdataset[32], amultipleclassesopen-sourcerepository
explicitlycreatedforobjectdetectioninremotesensingimages. Thisdatasetencompasses
satellite image patches sourced from platforms such as Google Earth, Jilin 1 (JL-1), and
Gaofen2(GF-2)satellites,featuring15objectcategories,amongwhichistheairplaneclass.
Thecomparativeevaluationconductedin[9]assessesthreeobjectdetectionmodels: Faster
R-CNN[14],SSD[11],andYOLO-v3[33],usingtheDOTAdatasetforbothtrainingand
testing. PerformancemetricsincludingCOCOmetrics,F1scores,andprocessingtimeare
employedforevaluation. ThesummaryofthisworkrevealthatFasterR-CNNexhibits
superior detection accuracy, with YOLO-v3 showcasing faster convergence capabilities.
SSD,althoughproficientinobjectlocalization,faceschallengeswithtrainingconvergence.
Additionally,anotherstudypresentedin[34]focussesonthedevelopmentandevaluation
ofthefirstversionoftheHRPlanesbenchmarkdatasetfordeeplearning-basedairplane
detectionusingsatelliteimageryfromGoogleEarth. TheauthorsdescribetheHRPlanes
datasetandsomeoftheimagescapturedbydifferentsatellitestorepresentdiverseland-
scapes,seasonalvariations,andsatellitegeometryconditions. Thedatasetisthenselected
fortrainingandvalidationoftwowidelyusedobjectdetectionmethods,YOLOv4[35]and
FasterR-CNN[14]. ThecomparativestudybetweenYOLOv4andFasterR-CNNinthe
contextofairplanedetectionfromsatelliteimageryrevealsinterestingfindings. Thestudy
highlightsthattheboundariesofboundingboxesforYOLOv4arebetteratcertainscales
comparedtoFasterR-CNN.Forinstance,insomecases,YOLOv4performsbetterindetect-
ingsmallairplanes,whileFasterR-CNNexcelsindetectinglargerones. Theresultsalso
indicatethatYOLOv4ismoreeffectiveincreatingaccurateboundingboxesforcommercial
planesinlarge-scaleimagery, possiblyduetothepresenceofboardingbridgesnearthe
planes. Additionally,bothdeeplearningmodelsdemonstratetheabilitytodetectmoving
planes,eveninscenarioswithmotionblureffectsintheimages.
3 AircraftDatasets
In the field of aircraft detection and remote sensing, access to high-quality and diverse
datasetsisimportantforthedevelopmentandevaluationofcomputervisionalgorithms. In
somecomparativestudies,DOTAdataset[32]isselected,thislatterencompassesobjects
otherthanaircraftssuchasairports,bridgesandcontainers.Thissectionreviewsacollection
ofdatasetsspecificallyandexclusivelydesignedforaircraftdetectionresearch,eachoffering
uniquefeaturesandadvantages. AnoverviewofthesedatasetsisgiveninTable2.
3.1 AirbusAircraftDataset
TheAirbusAircraftdataset[36]consistsof109high-resolutionimagesthatcaptureairplanes
atvariousairportsaroundtheworld. Theimagesaretakenatairportgatesortarmacsand
arecategorizedintotwofolders: ‘images’and‘extras’. The‘images’foldercontains103
picturesextractedfromPleiadesimagery,offeringaresolutionofapproximately50cm.Each
imageisstoredasaJPEGfilewithdimensionsof2560×2560pixels,correspondingtoa
groundareaof1280m2.Inparticular,thedatasetincludessnapshotsofcertainairportstaken
ondifferentdates, allowingresearcherstoexploretemporalvariations. Someimagesin
thedatasetalsoexhibitchallengingweatherconditionssuchasfogorclouds. Additionally,
4Table2: Overviewoftheavailabledatasetsexclusivelycreatedforaircraftdetectionfrom
aerialimagery.
Dataset ShortDescription Number
of Im-
ages
Airbus aircraft TheAirbusAircraftDatasetisextractedfromalarger 109
[36] deeplearningdataset,createdwiththeuseofAirbus
satellite imagery. The dataset draws its primary im-
ageryfromthePleiadestwinsatellitesoperatedbyAir-
bus. Imageshavearesolutionofapproximately50cm
per pixel, stored as JPEG files. These images have a
resolution of 2560 x 2560 pixels, representing an on-
groundareaof1280metres.
HRPlanesV2[34] The HRPlanesv2 dataset is comprised of 2,120 ultra- 2120
high-resolutionimagesfromGoogleEarth,featuring
atotalof14,335labelledaircrafts. Eachimageispre-
servedinJPEGformat,measuring4800x2703pixels,
andthelabelsforeachaircraftaredocumentedinthe
YOLOtextformat.
RarePlanes[37] Thisdatasetincorporatesrealandsyntheticallygener- 253
atedsatelliteimages. The‘real’portionofthedataset
consistsof253MaxarWorldView-3satellitescenes,in-
cluding112locationsand2142km2 with14700hand-
annotated aircrafts. The ‘synthetic’ portion features
50000syntheticsatelliteimageswithroughly63000air-
craftannotations. Onlythe‘real’partwasusedforthis
paper.
GDIT[38] TheGDITAerialAirportdatasetiscomposedofaerial 338
photographsthatshowparkedairplanes. Allvarieties
of plane are classified under a single category called
‘airplane.’
Planesnet[39] Planesnetisacollectionofimagesextractedfromthe 32000
Planet satellite imagery. The main purpose of the
datasetistheclassificationandlocalizationofairplanes
in medium-resolution images. The dataset includes
32000verysmallimages(20x20pixels)labelledwith
eithera"plane"or"no-plane"class.
FlyingAirpl. [40] FlyingAirplanesisamassivedatasetthatcontainssatel- Not
liteimagesofflyingairplanesthatsurround30different avail-
Europeanairports.ImagesarefromtheSentinel-2satel- able.
lite.
OPT-Aircraft[41] This dataset is a public remote sensing dataset with 3595
imagesstoredin.pngformatthatconsistsof3594data
fileswithanapproximatesizeof69.3MB.Thisdataset
allowstheidentificationofaircraftandclassifiesthem
accordingtotheirtypeandshape.
the‘extras’folderprovidesaseparatesetofimagesthatcanbeusedfortestingpurposes,
ensuringtheevaluationofalgorithmsoncompletelyunseendata.
3.2 HRPlanesv2Dataset
TheGoogleEarthHRPlanesv2[34]datasetisacomprehensivecollectionofhigh-resolution
aerialimagesforaircraftdetectionresearch. Itcomprises2120imagessourcedfromGoogle
Earth,showcasingairportsfromdiverseregionsandservingdifferentpurposes,including
civil, military, and joint airports. These images offer a rich variety of aircraft instances,
providing an extensive dataset for training and evaluation. Each image is stored as a
5JPEGfilewithdimensionsof4800×2703pixels,ensuringdetailedrepresentationsofthe
airportscenes. Tofacilitateobjectdetectiontasks, thedatasetincludespreciselabelsfor
14,335aircraftinstances,providedintheYOLOannotationformat. Moreover,thedataset
is divided into three subsets: 70% for training, 20% for validation, and 10% for testing,
enablingresearcherstoassessandcomparetheperformanceoftheiralgorithmsaccurately.
3.3 RarePlanesDataset
TheRarePlanesdataset[37]comprisesbothrealandsyntheticsatelliteimagery. Developed
byCosmiQWorksandAI.Reverie,thisdatasetaimstoevaluatetheefficacyofAI.Reverie’s
syntheticdatainenhancingcomputervisionalgorithmsforaircraftdetectioninsatellite
imagery. Thenumberoftherealimagesinthedatasetcomprises253MaxarWorldView-
3 satellite scenes, taken at 112 distinct locations and spanning an impressive 2142 km².
These scenes contain hand-annotated aircraft instances, totaling 14,700 annotations. In
addition,thedatasetincludes50,000syntheticsatelliteimagesgeneratedusingAI.Reverie’s
advanced simulation platform. These synthetic images feature approximately 630,000
aircraftannotations,providingavaluableresourcetoexplorethebenefitsofsyntheticdata
inoverheadaircraftdetection.
3.4 GDITDataset
TheGDITAerialAirportdataset[38]isaspecializedcollectionofaerialimagesthatfocuses
onparkedairplanesatairports. Itpresentsanopportunityforresearcherstoexploreaircraft
detectionalgorithmsinthecontextofairportenvironments. Thedatasetconsistsof338
high-qualityimageswitharesolutionof600×600pixels,whicharefurthercategorizedinto
training,validation,andtestingsubsetswith236,68,and34images,respectively. Notably,
some of the training images exhibit variations such as different filters, zoom levels, or
rotations, resulting in an expanded dataset of 810 images. The dataset offers a unified
classificationlabelforalltypesofairplanes,simplifyingthedetectiontask.
3.5 PlanesnetDataset
ThePlanesnetdataset[39]providesanextensivecollectionofsatelliteimageryextracted
fromPlanetsatellites,focusingonmultipleairportsinCalifornia. Thisdatasetcomprises
32000RGBimages,eachmeasuring20×20pixels. Theimagesaremeticulouslylabeled
aseither’plane’or’no-plane’enablingresearcherstotrainandevaluateaircraftdetection
algorithms. DerivedfromPlanetScopefull-framevisualsceneproducts,thedatasetensures
anorthorectified3mpixelsize, capturingfine-graineddetails. ThePlanesnetdatasetis
availableintwoformats: azippeddirectorycontainingthePNGimagesandaJSONfile
containingcorrespondingmetadata. Eachimageisaccompaniedbyafilenamethatincludes
thelabel,sceneID,andmetadatasuchlongitude,andlatitudecoordinates.
3.6 FlyingAirplanesDataset
ThedatasetofFlyingairplanesonsatelliteimages[40]offersvaluableresourcesforre-
searchrelatedtothedetectionofaircraftinsatelliteimagery. Itincludes180satelliteimages
covering areas of interest surrounding 30 European airports. The dataset incorporates
ground-truthannotationsofflyingairplanes,whichcanbeusedtosupportvariousresearch
investigations. Theseannotationsserveasareferencefordevelopingandevaluatingal-
gorithms for flying airplane detection. The dataset comprises modified Sentinel-2 data
processedbyEuroDataCube,providinghigh-qualitysatelliteimagerysuitableformultiple
applications.
3.7 OPT-AircraftDataset
TheOPT-AircraftV1.0[41]datasetfocusesontheidentificationofaircraftgroupsinremote
sensingimages.Itincludes3594airplaneimagesobtainedfromvariouspublicdatasets,such
asDIOR,UCAAOD,NWPUVHR-10,DOTA,andGoogleEarth. Thedatasetencompasses
seven aircraft groups categorized based on wings and propellers, further divided into
6fourteensub-groupsconsideringaircraftcolorandengineposition. Thedataset,storedin
PNGformat,consistsof3594fileswithacompressedsizeof69.3MB.
4 LiteratureReview: ObjectDetection
Manyobjectdetectionmodelshavebeenproposedinthepastdecades,themajorityare
basedonCNNs.Researchershavesofarclassifiedthosemodelsintotwoprimarycategories:
one-stageandtwo-stagearchitectures[42,43]. However, theriseofself-attentionmech-
anism[44]recentlyhasledtoanewcategoryofnetworkarchitecturesthatarebasedon
transformers. Fig. 1)groupsomeofthemostusedandknownmodelsalongwithtwotypes
ofclassificationsofdeeplearningmodels: (1)eithercategorizedbasedontheirnetwork
architecture (one-stage, two-stages, or transformer), or (2) based on their performances
in real-time and detection accuracy highlighted in respectively blue and yellow colors.
Theoverlappingareadrawningreenfeaturetheobjectdetectionmodelswithabalanced
performancesinbothreal-timedetectionandaccuracy.
One-stage models [45] have been mainly known for their real-time deployment perfor-
mances,becausetheyareusuallynotcomputationallyextensiveandasinglepassthrough
thenetworkissufficienttoproduceestimationsofobjectboundingboxes. However,the
mainlimitationinthiscategoryisthedetectionaccuracy,whichmightnotbeenoughfor
someapplicationsrequiringveryhighdetectionconfidence. Inthecaseofatwo-stageobject
detectionmodels,anadditionalstageisintroducedtogenerategenericobjectproposals
whichmakethemodelmoreefficientinitsdetectionoperation[46]. Thepurposeofthis
stage is to produce candidate bounding boxes, which may not be highly accurate, and
effectivelyexcludebackgroundareasfromfurtherprocessing. Subsequently,thenextstage
ofthemodelundertakesthemorecomputationallyintensivetasksofclassifyingobjects
andrefiningtheboundingboxesgeneratedbythepreviousstage. Thethirdcategoryis
transformer-basedarchitecture[47],whichseemstobeagoodbalancebetweenaccuracyon
real-timedetectionasithasbeentestedandcomparedoncommondatasets. Thiscategory
have been tested on common large datasets and it leverage the use of the self-attention
mechanismtoproducereliableandaccurateboundingboxestimationswhilebeingableto
performreal-timedetection.
Thechoicebetweenobjectdetectionarchitecturedependsontheapplicationandthenature
ofimagestoprocesswhichinvolvesatrade-offbetweenspeedandaccuracy[54]. One-stage
modelstendtoofferfasterprocessingspeedsbutmayexhibitloweraccuracycomparedto
theirtwo-stagecounterparts. Theadvantagesofone-stagealgorithmscouldbesummarized
asfollow[55,56,57]:
• SimplicityandEfficiency:One-stagedetectorshaveasimplerarchitecturecompared
totwo-stagealgorithms.Theydirectlypredictobjectlocationsandclassprobabilities
withouttheneedforanintermediateproposalgenerationstep.Thissimplicityleads
tocomputationalefficiency,asone-stagedetectorscanprocessimagesfasterthan
two-stagedetectors.
• Real-time Performance: One-stage detectors are designed to achieve real-time
ornearreal-timeperformance,makingthemsuitableforapplicationswherefast
inferenceiscrucial. Thesealgorithmsarecommonlyusedinscenariosthatrequire
quickresponses,suchasautonomousdriving,videoanalysis,androbotics[58].
• HigherRecall: One-stagedetectorstendtohaveahigherrecallratecomparedto
two-stagedetectors[59,11].Theyarecapableofdetectingalargernumberofobjects
inanimage,includingsmalloroccludedobjects,duetotheirdenseanddense-like
predictionstrategies. Thishigherrecallcanbeadvantageousinapplicationswhere
comprehensiveobjectdetectionismoreimportantthanachievingextremelyprecise
boundingboxlocalization.
• TrainingSimplicity: One-stagedetectorshaveasimplertrainingpipelinecompared
totwo-stagedetectors. Theytypicallyuseasingle-shottrainingstrategy, where
objectlocationsandclasspredictionsaredirectlyregressedfromthenetworkoutput.
Thissimplifiesthetrainingprocess,reducesthenumberofhyperparameterstotune,
andrequiresfewercomputationalresourcesfortraining. Consequently,one-stage
7Deeplearningbased
ObjectDetection
One-stage Two-stages Transformer
YOLO[10] RCNN[51] DETR[52]
RetinaNet[26] Fast-RCNN[13] RT-DETR[24]
RTMDet[18] Faster-RCNN[14] ViTDet[53]
y
c
a
SSD[11] Mask-RCNN[15] r u
c
c A Overlapping
CenterNet[18] Casc.-RCNN[49] n o
itc
e
te
CornerNet[25] R-FCN[50] D
RefineDet[48]
Figure1: Classificationofobjectdetectionmethodsbasedon: (1)theirarchitecture(One-
stage, two-stages and transformer network); (2) on detection accuracy (in orange) and
real-timedetection(blue). Thereddothighlightsthemodelsthatareimplemented,trained
andvalidatedinthiswork. Thegreenoutline(indicatedas‘Overlapping’intheimage)
groupsthemodelsthatusuallyperformwellinbothaccuracyandinferencetimeresponse.
detectorsareeasiertoimplementandexperimentwith,especiallyforresearchersor
practitionersnewtoobjectdetectionalgorithms.
Ontheotherhand,two-stagemodelsgenerallyachievehigheraccuracybutsacrificesome
speedduetotheadditionalprocessingstage. Theadvantagesoftwo-stagealgorithmsover
theotherscouldbesummarizedintothefollowingpoints:
• Samplingefficiency: Two-stagedetectorsemployasamplingmechanismtoselect
a sparse set of region proposals, effectively eliminating a significant portion of
thenegativeproposals. Conversely,one-stagedetectorstakeadifferentapproach
bydirectlyconsideringallregionsintheimagewhichsometimesintroduceclass
imbalance[60].
• Feature extraction: Two-stage detectors can allocate a larger head network for
proposal classification and regression as they only process a small number of
proposals.Thisallowsfortheextractionofricherfeatures,contributingtoimproved
performance.
• Recall: Two-stagedetectorsleveragetheRoIAlign[15,61]operationtoextracthigh-
qualityfeaturesfromeachproposal,ensuringlocationconsistency. Conversely,in
one-stagedetectors,differentregionproposalsmaysharethesamefeature,leading
tocoarseandspatiallyimplicitrepresentationsthatcancausefeaturemisalignment.
• Accuracy: Two-stagedetectorsrefinetheobjectlocationstwice,onceineachstage.
Consequently,theboundingboxesgeneratedbythesemodelsexhibitbetteraccu-
racycomparedtoone-stagemethodsbutattheexpenseofrealtimeperformance.
8
ecnerefniemit-laeRConsequently,thetrade-offbetweenone-stage,two-stageandtransformer-basedarchitec-
turesnecessitatescarefulconsiderationbasedonthespecificrequirementsoftheapplication
athandaswellasthenatureofimages. Inthiscontext,specialattentionisdirectedtowards
thedetectionofairplaneswithinremotesensingimages. Thechoiceofalgorithmsisguided
bytheirwidespreadusage,popularityinperformanceswhentestedonothertypesofim-
ages,andavailabilityasopen-sourceimplementations. ThiswillincludeYOLO,CenterNet,
RTMDet,SSD,RetinaNet,Faster-RCNNandDETR.
4.1 YouOnlyLookOnce
TheYouOnlyLookOnce(YOLO)framework[59]hasemergedasapopulardeeplearning-
based object detection algorithm that revolutionized real-time object detection tasks. It
presentsaunifiedapproachtoobjectdetectionbyformulatingitasaregressionproblem,
enablingthemodeltopredictboundingboxesandclassprobabilitiesofmultipleobjectsin
asinglepassthroughthenetwork.
Figure2: BasicYOLOarchitecture. Reproducedfrom[62].
In the early stages, YOLOv2 [63] introduced batch normalization and high-resolution
classifierstoimprovedetectionperformance. YOLOv3[33]furtherrefinedthearchitecture
byincorporatingskipconnectionsandmulti-scaleprediction,enhancingbothaccuracyand
localizationcapabilities. TheintroductionofCSPDarknet,SPP,PAN,andMishactivationin
YOLOv4[35]ledtosignificantimprovementsinthenetworkarchitecture. YOLOv5[64],
buildinguponthescaled-YOLOv4[65],introducedanchor-freeobjectdetectionandanew
architecture. Thisversionexpandedtheoptionsavailablebyprovidingmodelsofvarying
sizes,allowinguserstobalancespeedandaccuracyaccordingtotheirspecificrequirements
(Fig. 2).
Thelatestknownversions,YOLOv8[22]andYOLO-NAS[66],broughtforththeconcept
ofneuralarchitecturesearch(NAS)[67]toautomaticallydesignnetworkarchitecturesand
achievestate-of-the-artperformanceinobjectdetectiontasks. Thecontinuousevolutionof
theYOLOframeworkhighlightsthetrade-offsbetweenspeedandaccuracy,necessitating
considerationofthespecificapplicationrequirementswhenselectinganappropriateYOLO
model.
4.2 SingleShotDetection
TheSingleShotDetector(SSD)[68]isanefficientandaccurateobjectdetectionalgorithm
thatintroducesaunifiedframeworkforsingle-passdetection. Intheory,SSDaddressesthe
challengeofdetectingobjectsatvariousscalesandaspectratiosbyutilizingpredefined
anchorboxes[69]. TheSSDarchitectureconsistsofthreemaincomponents: abasenetwork,
convolutionalfeaturemaps,andconvolutionalpredictors(Fig.3).Thebasenetwork,usually
apre-trainedCNN[70],actsasafeatureextractor,generatinghigh-levelfeaturemapswith
different spatial resolutions. These feature maps are then processed by convolutional
predictors,whichpredictobjectpresenceandlocationsfortheanchorboxes. Eachpredictor
9correspondstoaspecificfeaturemapandproducesclassscoresandboundingboxoffsets.
Theanchorboxes,distributeddenselyacrossthefeaturemap,serveasreferenceboxesfor
object detection. To capture objects at multiple scales, SSD employs feature maps from
differentstagesofthebasenetwork.Higher-resolutionfeaturemapsareeffectiveatdetecting
smallobjects,whilelower-resolutiononesaresuitableforlargerobjects. Thepredictions
fromeachfeaturemaparecombinedtogeneratefinalclasspredictionsandrefinedbounding
boxcoordinates. Duringtraining,SSDutilizesamulti-tasklossfunction[71]thatoptimizes
the model by considering both localization loss (Smooth L1 loss) and classification loss
(softmaxloss)[72]. Thelocalizationlosspenalizesthediscrepancybetweenpredictedand
groundtruthboundingboxcoordinates,whiletheclassificationlossencouragesaccurate
classpredictions. Thelossiscomputedforpositiveandnegativesamples,includinghard
negativesbasedonconfidencescores.
Figure3: SSDarchitecturediagram. Reproducedfrom[68].
WhileSSDoffersagoodbalancebetweenaccuracyandefficiencywithitsvaryingfeature
map resolutions and fixed anchor boxes, it does have limitations [73]. One limitation is
the dependence on predefined anchor boxes, which may not adequately cover objects
withextremeaspectratiosorunconventionalshapes. Scalingtheanchorboxescanhelp
addressthis,butitintroducesadditionalcomputationaloverhead. Anotherlimitationis
thechallengeofhandlingtinyobjects[74]. SinceSSDreliesonalimitednumberoffeature
maps,itmaystruggletoaccuratelydetectsmallobjectsduetothelossoffine-graineddetails
athigherresolutions. Thiscanresultinreducedlocalizationaccuracyandincreasedfalse
negativesforsmallordenselypackedobjects. Furthermore,SSD’sfixedanchorboxeslimit
itsabilitytohandleobjectsatarbitraryscalesandaspectratios.
4.3 Region-basedCNNs
TheintroductionoftheRegion-basedConvolutionalNeuralNetwork(R-CNN)[75]marked
asignificantmilestoneinthedevelopmentofobjectdetectiontechniques,showcasingthe
substantialimprovementsthatconvolutionalneuralnetworks(CNNs)canbringtodetection
performance. R-CNNintroducedtheconceptofutilizingCNNs[76]incombinationwitha
class-agnosticregionproposal[77]moduletotransformobjectdetectionintoaclassifica-
tionandlocalizationproblem. Thedetectionprocessstartswithamean-subtractedinput
image, which is fed through the region proposal (RPN) module. This module employs
techniquessuchasSelectiveSearch[78]toidentifyregionswithintheimagethathavea
higherlikelihoodofcontainingobjects. Approximately2000objectcandidatesaregenerated
basedonthisregionproposalstep. Thesecandidatesarethenwarpedandpassedthrough
a CNN network, such as the widely used ImageNet [79], to extract a 4096-dimensional
feature vector for each proposal. The feature vectors obtained from the CNN are then
inputtedintoclass-specificSupportVectorMachines(SVMs)[80],whichhavebeentrained
beforehand. TheSVMsgenerateconfidencescoresforeachcandidateregion,aidinginthe
classificationprocess. Torefinetheresults,non-maximumsuppression(NMS)isapplied
basedontheIntersectionoverUnion(IoU)andclassinformation. Oncetheclasshasbeen
identified,atrainedbounding-boxregressorisemployedtopredicttheprecisebounding
boxcoordinates,includingthecentercoordinates,width,andheightoftheobject(Fig. 4).
10als
os
p
o
pr
g
age
ature
maps
RPN
RoI
poolin
m e
I F
conv layers
Classifier
Figure4: FasterRCNNarchitecture. Reproducedfrom[14].
However, despite its groundbreaking contributions, R-CNN has several limitations [81,
82]. The training process of R-CNN is complex and multistage. It involves pretraining
theCNNwithalargeclassificationdataset, followedbyfine-tuningondomain-specific
imagesthatundergomeansubtractionandwarpingtoalignwiththeproposals. TheCNN
classification layer is replaced with a randomly initialized N+1-way classifier, where N
represents the number of classes, and stochastic gradient descent (SGD) is utilized for
optimization. Additionally,separateSVMsandboundingboxregressorsneedtobetrained
foreachclass,addingtothecomputationalcomplexity.
Although R-CNN is capable of performing highly accurate object detection research, it
sufferedfromslowinferencetimes,takingapproximately47secondsperimage,andwas
resourceintensiveintermsofbothtimeandspace[75]. TrainingR-CNNmodelsonsmall
datasetstookdaystocomplete,evenwithsharedcomputations. Theselimitationssparked
theneedforfurtheradvancementsinobjectdetectionalgorithmsthatcouldaddressthese
challengesandimproveoverallefficiency.
4.4 RetinaNetFramework
RetinaNetisaone-stageobjectdetectionmodelthatwasintroducedbyTsung-YiLinet. al
[26]designedtoaddresstheextremeforeground-backgroundclassimbalanceencountered
duringtrainingofdensedetectors. Inobjectdetection,thegoalistodetectobjectsofinterest
inanimageandlocalizethembydrawingboundingboxesaroundthem. However,thevast
majorityoftheimageistypicallybackground,andthereareusuallymanymorenegative
examples(background)thanpositiveexamples(objectsofinterest).Thisclassimbalancecan
makeitdifficultforthedetectortolearntodistinguishbetweenobjectsandbackground,and
canleadtopoorperformance. RetinaNetisasingle,unifiednetworkthatiscomposedofa
backbonenetworkandtwotask-specificsubnetworks(Fig. 5). Thebackboneisresponsible
forcomputingaconvolutionalfeaturemapoveranentireinputimageandisanoff-the-shelf
convolutionalnetwork. Thefirstsubnetworkisadensepredictionsubnetthatproducesa
fixednumberofobjectdetectionsofdifferentscalesandaspectratiosateachspatiallocation
inafeaturemap. Thesecondsubnetworkisasetofclass-specificsubnetsthatfurtherrefine
thepredictionsofthefirstsubnet.
This model uses a novel focal loss function that down-weights the contribution of easy
examplesduringtrainingtofocusonhardexamplesandpreventthevastnumberofeasy
background examples from overwhelming the detector. The focal loss is a dynamically
scaledcross-entropyloss,wherethescalingfactordecaystozeroasconfidenceinthecorrect
11Figure5:OnestageRetinaNetarchitecture. Reproducedfrom[17]. (a)ResNet[83]backbone.
(b)generationofmultiscaleconvolutionalpyramid. Thisisattachedtotwosubnetworks: (c)
anchorboxclassificationand(d)anchorboxregressiontogroundtruthboundingbox.
classincreases. Thislossfunctionisdesignedtoaddressthemechanismsusedbyother
detectorstoaddressclassimbalance,suchasbiasedminibatchsamplingandobjectproposal
mechanisms,inaone-stagedetectionsystemdirectlyviathelossfunction.
Theobtainedresultsshowedagoodperformancecomparedtopreviousone-stageandtwo-
stagedetectors,includingthebestreportedFasterR-CNNsystem,ontheCOCOdataset.
Itachievesstate-of-the-artperformanceonboththeCOCOdetectiontasks,withabetter
COCOtest-devaverageprecisionwhilerunningat5fps. Thisisasignificantimprovement
overpreviousstate-of-the-arttechniquesfortrainingone-stagedetectors,suchastraining
with the sampling heuristics or hard example mining. RetinaNet is also designed to be
efficientandscalable. Itusesanefficientin-networkfeaturepyramidthatallowsittodetect
objectsatmultiplescalesandresolutions,anditusesanchorboxestoimprovelocalization
accuracy. Theanchorboxesarepre-definedboxesofdifferentscalesandaspectratiosthat
areplacedateachspatiallocationinthefeaturemap. Thedensepredictionsubnetpredicts
theoffsetsandscalesoftheanchorboxestogenerateobjectdetections.
RetinaNethasbeenwidelyadoptedinindustryandacademiaandselectedforavarietyof
applications,includingobjectdetectioninautonomousdriving,facedetection,andmedical
imageanalysis. Ithasalsoinspiredfurtherresearchinthefieldofobjectdetection,including
thedevelopmentofothernovellossfunctionsandarchitectures.
4.5 CenterNetFramework
CenterNet[18]isanotherreal-timeobjectdetectionalgorithmdesignedtooperateinreal-
time,withanaverageinferencetimeof270msusinga52-layerhourglassbackboneand
340msusinga104-layerhourglassbackboneperimageaccordingtotheauthor(Fig. 6).
CenterNetisinspiredfromthearchitectureofCornerNet[25]whichisbasedonaone-stage
keypoint-based detector, while introducing several novel components and strategies to
enhanceitseffectiveness.
Figure6: OnestageCenterNetarchitecture. Reproducedfrom[18].
UnlikeCornerNet,whichdetectsobjectboundingboxesusingpairsofkeypoints,CenterNet
introducestheconceptofdetectingeachobjectasatripletofkeypoints. Thisinnovation
12allowsCenterNettocapturemorecomprehensiveinformationabouttheobjects,leading
to improved detection performance while being fast for live inferencing. CenterNet in-
corporates two customized modules named cascade corner pooling and center pooling.
Thesemodulesplaycrucialrolesinenrichinginformationcollectedbybothtop-leftand
bottom-rightcornersandprovidingmorerecognizableinformationatthecentralregions,
respectively. These modules contribute to the improved performance of CenterNet by
enhancing the detection of object keypoints and bounding boxes. In their benchmark
study [18], authors stated that this proposed architecture achieves significant improve-
ments over existing one-stage detectors typically an average precision of 47.0% on the
MS-COCOdataset,outperformingthepreviouslyproposedone-stagedetectorsbyatleast
4.9%. Additionally, itdemonstratescomparableperformancetothetwo-stagedetectors
while maintaining a faster inference speed thanks to the effective reduction of incorrect
boundingboxes, particularlyforsmallobjects. Itachievesnotableimprovementsinthe
detectionofsmallobjects,withanaverageprecision(AP)improvementsof5.5%to8.1%for
differentbackboneconfigurations. Thisreductioninincorrectboundingboxesisattributed
totheeffectivenessofCenterNetinmodelingcenterinformationusingcenterkeypoints.
4.6 End-to-EndTransformer
Theadventoftransformerarchitectureshasrevolutionizedthefieldofartificialintelligence
byintroducinganovelapproachtoprocessingsequentialdata. Unlikeolderneuralnetwork
architectures,transformersrelyonself-attentionmechanisms[44]tocapturedependencies
betweeninputtokens,enablingthemtoeffectivelymodellong-rangedependenciesand
capturecomplexpatternsinthedata.Thisabilitytoprocesssequencesinparallel,ratherthan
sequentially,hassignificantlyimprovedtheefficiencyandeffectivenessspeciallyofnatural
language processing (NLP) tasks [84]. The success of transformers can be attributed to
theirabilitytocaptureglobalcontextandrelationshipswithintheinputdata,makingthem
particularlywell-suitedfortasksthatrequireunderstandingofcomplexinterdependencies.
Later-on,researchershavetranslatedtheuseoftransformersinNLPtocomputervision
includingobjectdetectionoperation. OnepioneeringalgorithmisDETR(DEtectionTRans-
former) [52] belonging to the category of end-to-end object detection systems based on
transformersandbipartitematchinglossfordirectsetprediction. Unliketraditionalobject
detectionmethods,DETRdoesnotfallintotheconventionalone-stageortwo-stagesde-
tectorcategories. Instead,itintroducesanovelapproachbypredictingallobjectsatonce
usingabipartitematchinglossfunction,whichuniquelyassignsapredictiontoaground
truthobjectandisinvarianttoapermutationofpredictedobjects. Thisuniqueapproach
simplifiesthedetectionpipelinebyeliminatingtheneedforhand-designedcomponents
suchasspatialanchorsornon-maximalsuppression[85],makingitoptimalforbothaccu-
racyandreal-timeprocessing. DETRachievesthisbyleveragingtransformerswithparallel
decoding,asopposedtoautoregressivedecodingwithrecurrentneuralnetworks,which
wasthefocusofpreviouswork.
Figure7: DETRarchitecture. Reproducedfrom[52].
ThearchitectureofDETRissimpleyethighlyeffective. Itcomprisesthreemaincompo-
nents: aCNNbackboneforfeatureextraction[86],anencoder-decodertransformer[87]
for modeling relationships between feature representations of different detections, and
a simple feed-forward network for making the final detection predictions (Fig. 7). The
13process begins with the extraction of a lower-resolution activation map from the input
imageusingaconventionalCNNbackbone. Thisactivationmapisthenpassedthrough
thetransformerencoder,alongwithspatialpositionalencodingsthatareaddedtoqueries
and keys at every multi-head self-attention layer. The decoder receives queries, output
positionalencodings(objectqueries),andencodermemory,andproducesthefinalsetof
predictedclasslabelsandboundingboxesthroughmultiplemulti-headself-attentionand
decoder-encoderattention(FFNs). ThesimplicityandmodularityoftheDETRarchitecture
makeiteasilyimplementableinanydeeplearningframeworkthatprovidesacommon
CNNbackboneandatransformerarchitectureimplementation.
Incomparisontoexistingmodels,DETRhasdemonstratedremarkableperformanceonthe
challengingCOCOobjectdetectiondataset. Itachievescompetitiveresultswiththesame
numberofparametersasFasterR-CNN,awidelyusedobjectdetectionmodel,achieving42
APontheCOCOvalidationsubset. Notably,DETRoutperformsFasterR-CNNinterms
ofAPimprovement,particularlyinthecontextofdirectsetprediction. However,itlags
behindintermsofsmallobjectAP.Additionally,DETRwithaResNet-101[88]backbone
showscomparableresultstoFasterR-CNN.ThesuccessofDETRcanbeattributedtoits
uniquecombinationofbipartitematchingloss[89]andtransformerswithparalleldecoding,
whichenablesittoeffectivelymodelrelationsbetweenfeaturerepresentationsofdifferent
detectionsandachievecompetitiveperformanceinobjectdetectiontasks.
4.7 RTMDet
RTMDet [23] is a groundbreaking real-time object detection model designed to achieve
optimalefficiencywithoutcompromisingaccuracy. Itbelongstothefamilyoffullyconvolu-
tionalsingle-stagedetectors[90]suchasYOLOseries. RTMDet[23]operatesasaone-stage
detector,enablingittoswiftlyrecognizeandlocalizeobjectsinreal-worldscenariossuchas
autonomousdriving,robotics,anddrones. Themodelisengineeredtopushtheboundaries
of the YOLO series by introducing a new family of Real-Time Models for object Detec-
tion. Notably,RTMDetiscapableofperforminginstancesegmentationandrotatedobject
detection,featuresthatwerepreviouslyunexplored.
Figure8: OnestageRTMDetarchitecture. Reproducedfrom[23].
RTMDet operates on a model architecture that emphasizes efficiency and compatibility
in both the backbone and neck. The basic building block of the model consists of large-
kerneldepth-wiseconvolutions[91],whichcontributetoitsabilitytocaptureglobalcontext
effectively. Thisarchitecturalchoiceenhancesthemodel’scapacitytorecognizeandlocalize
objects with high precision. Furthermore, RTMDet incorporates soft labels during the
calculationofmatchingcostsinthedynamiclabelassignmentprocess[92],leadingtoan
improvedaccuracy. Thecombinationofthesearchitecturalfeaturesandtrainingtechniques
culminatesinanobjectdetectorthatachievesexceptionalperformance. RTMDet’smacro
architecturefollowstheone-stageobjectdetectorparadigm,anditbalancesmodeldepth,
width, and resolution to optimize efficiency. Additionally, the model is designed to be
versatile,allowingforeasyextensiontoinstancesegmentationandrotatedobjectdetection
taskswithminimalmodifications.
In comparison to the state-of-the-art industrial detectors, the authors [23] demonstrate
remarkable performance of RTMDet in terms of both speed and accuracy. The model
achievesanimpressive52.8%APontheCOCOdatasetwhileoperatingatover300frames
14per second (FPS) on an NVIDIA 3090 GPU. This outperforms the current mainstream
industrialdetectors,showcasingthesuperiorparameter-accuracytrade-offofRTMDet. The
model’sversatilityisevidentinitsabilitytodeliveroptimalperformanceacrossvarious
applicationscenarios,offeringdifferentmodelsizesfordifferentobjectrecognitiontasks.
5 Method: AircraftDetection
Typically,novelmethodsareassessed,validated,andcontrastedwithotheralgorithmsto
demonstratetheireffectiveness. However,thesemethodsareprimarilyevaluatedonthe
COCOdataset,whichisacomprehensiveyethighlyeffectiveperformanceevaluatordue
tothenumberofimageswithmultipleclassesandresolutions. Inthisresearch,themain
focusisevaluatingthesealgorithmsforaircraftdetectionfromremotesensing,wherethe
challengesaresignificantlydifferent,andothertypesofnoiseandobjectsizesmustbedealt
with.
Forthecomparativestudy,thegoogleearthHRPlanesV2datasetwasselectedasatraining
datasetbecauseitcontainsthehighestnumberofhighresolutionimages(2120)amongthe
otherdatasets(Table2)whilealsoprovidingdifferentpositionsorientationsandground
casestothemodelforitsgeneralization. Asforthevalidationandtest,unseenimagefrom
theHRPlanesV2aswellasthetheGDITAerialairportsdatasetwasincludedtoevaluate
withmoreaccuracythepre-trainedmodels(GraphicalsummaryoftheworkintheFig. 9).
TheframeworksusedinthiscomparativestudyaretheoneimplementedbyUltralytics
[10],Detectron2[93]andMMLabdetectiontoolbox[90].
Faster
YOLOv5 RTMDet DETR
RCNN
YOLOv8 CenterNet RetinaNet SSD
Comparison metrics
Intersection of Union Recall Average precision
Figure9: FlowchartoftheFlightScopecomparativestudy: Thetrainingisperformedon
HRPlanesV2 dataset and the Validation and Test conducted on HRPlanesV2 and GDIT
Aerialairportdatasets.
15
dilaV
+
tseT
sledom
niarT
sledom
eht
etaulavE
2VsenalPRH
2VsenalPRH
Val
+
Test
subsets
TIDG subsets
Train
subset
Train
+
val
+
Test5.1 SetupandDataPreparation
Google earth HRPlanesV2 is already annotated and splited into three subsets: 70% for
training, 20% for validation, and 10% for testing but initially available in YOLO format
whileMMLabrequiresthedatasetannotationformattobeinCOCO.Fig. 10providean
samplefromHRPlanesV2datasetwiththecorrespondingannotationboundingboxes.
Figure10: SamplepreviewfromHRPlanesV2dataset: (a)blue: trainingsubset,(b)green:
testsubset,(c)red: validationsubset
AnoverviewofthesetupconfigurationoftheusedtrainingpackageispresentedinTable3.
Forthetrainingandtheevaluationofthemodel3NVIDIARTXA6000eachwith48Gof
memoryhavebeenused. TheseprocessorsweremadeavailablebytheTOELTLLCAILab.
Themaximumnumberofepochshavebeenfixedto500andbatchsizesto32andforsome
neuralnetworkarchitecturesto64becausetheGPUmemoryallowedit.
Table3: SystemConfigurationSetup
SoftwareSetup
Name Version
Ubuntu 20.04.1LTS
Python 3.8
PyTorch 2.0.1
CUDA 12.2
HardwareSetup
GPU NVIDIARTXA600048GB×3
CPU Intel(R)Core(TM)i9-10980XECPU@3.00GHz
Memory 128GB
5.2 EvaluationMetrics
Beforedelvingintothespecificevaluationmetricsemployed,it’sessentialtoestablisha
comprehensiveunderstandingofeachmetric’sroleinassessingtheperformanceofobject
detectionmodels,particularlyconcerningboundingboxestimationaccuracy. Withinthis
study,PASCALVOCmetricshavebeenselected: AP,RecallandboundingboxesIoU[94].
Average Precision (AP) stands as a fundamental metric in object detection evaluation,
providingacomprehensivemeasureofthemodel’sabilitytopreciselyidentifyobjectsof
interestwithinanimage. TheAPmetric,formulatedinEq.1,iscomputedbyintegratingthe
precision-recallcurve p(r),whichrepresentsthetrade-offbetweentruepositivedetections
andfalsepositivesacrossvariousconfidencethresholds. Thisintegrationyieldsascalar
valuereflectingthemodel’soveralldetectionaccuracy,withhigherAPscoresindicating
superiorperformance.
(cid:90) 1
AP= p(r)dr (1)
0
16Recall, another metric in object detection assessment, quantifies the model’s ability to
correctlyidentifyallrelevantinstancesofobjectspresentwithinanimage. Itsignifiesthe
sensitivityofthemodelincapturingtruepositives(TP)whileminimizingfalsenegatives
(FN).Mathematically,Recallisdefinedastheratiooftruepositivedetectionstothetotal
numberofgroundtruthobjects(Eq. 2).
TP
Recall= (2)
TP+FN
Lastly,IntersectionoverUnion(IoU)servesasametricforevaluatingthespatialalignment
betweenpredictedboundingboxesandgroundtruthannotations. IoUquantifiestheextent
ofoverlapbetweentheseboundingboxes,providinginsightintothelocalizationaccuracy
ofdetectedobjects. IoU,expressedbyEq. 3,iscomputedastheratiooftheintersectionarea
betweenthepredictedandground-truthboundingboxestotheirunion.
AreaofIntersection
IoU= (3)
AreaofUnion
Employingthesemetricsintheevaluationprocessenablesanaccurateassessmentofobject
detectionmodelperformance,particularlyconcerningbounding-boxestimationaccuracy.
5.3 Results
This subsection presents a detailed result showcase of the object detection algorithms
trainingduringthe500epochs. Fig. 11displaythemAP(overallmeanavecprecisionaccros
different confidence thresholds) and the mAP50 (for object detected by an Intersection
overUnion(IoU)thresholdof50%andup)curvescollectivelyvisualizingtheperformance
of the object detection algorithms in the task of aircraft detection from remote sensing
imagery. Notably,theperformancesofthealgorithmsarequitecomparablewithanoverall
mAPvaryingbetween0.86and0.99. Amongthemodels,YOLOv5emergesasastandout
performer,achievingthehighestmAPvalueof0.99471atstep150,showcasingitsgreat
precisionandrobustness. YOLOv8closelyfollows,reachingapeakmAPvalueof0.99236
atstep395, emphasizingtheefficacyoftheYOLOarchitectureinaerialobjectdetection.
However,SSDlagsbehindwithacomparativelymodestmAPvalueof0.86atstep74which
stabilizesintherestofthetrainingprocess.
TheresultsinFig.11-bconfirmthepreviousdiscussion,asYOLOv5continuestooutperform,
achievingthehighestmAP50valueof0.84454atstep493. RTMDetandYOLOv8remains
strongwithmAP50valuesof0.838atstep340and0.8372atstep492successively. CenterNet
consistentlyperformswell,achievinganmAP50valueof0.826atstep439. Faster-RCNN
maintainsabalancedmAP50valueof0.775atstep402,showcasingreliabilityindetection.
DETR contributes robustly with an mAP50 value of 0.774 at step 472, while RetinaNet
exhibitsstabilitywithanmAP50valueof0.765atstep250. Finally,SSD,withanoticeable
gapbetweenmAPandmAP50values,suggestspotentialchallengesinlocalizationprecision,
emphasizingtheneedforrefinementinspecificscenarios.
Inaddition,Fig. 12aand12bprovidesuccessivelyavisualizationoftheboundingboxand
totallosscurvesforthealgorithms,whereitisnoticeablethatthemajorityofthealgorithm
convergequicklyaround100epochs(apartfromRTMDetwhichhasajumpvaluearound
theepoch280)andreachaseeminglyhorizontalasymptotelinewithin300epochs. While
extendingthetrainingdurationbeyond500epochsmightpromotefurtherconvergence,
thereisapotentialriskofoverfittingthemodels.
5.4 EvaluationonGDITDataset
Toevaluatetheefficacyofobjectdetectionmodels,originallytrainedontheHRPlanesv2
dataset, a thorough assessment was conducted using the GDIT dataset. The evaluation
encompassedallsubsetsofimages: train,test,andvalidation,eachrepresentingdiverse
scenarios. Thiscomprehensiveevaluationaimedtogaugetheadaptabilityandrobustness
ofthealgorithmsundervariedconditionsencounteredindifferentsubsets. Asampleof
17(a)
Magnification
(b)
Magnification
Figure11: Comparisonofboundingboxmeanaverageprecision(mAP)curvesfortrained
object detection algorithms. To the left raw figures of the curves, the right figures are
magnificationsfromepoch450to500. (a)RepresentsthemAP.(b)IllustratesthemAP50
CenterNet RetinaNet CenterNet RetinaNet
0.2
0.5
1
0.25
0.00 0.0 0 0.0
0 200 400 0 200 400 0 200 400 0 200 400
SSD RTMdet SSD RTMdet
1 0.5 2.5 2
0 0.0 0.0 0
0 200 400 0 200 400 0 200 400 0 200 400
DETR Faster-RCNN DETR Faster-RCNN
0.5
0.1 0.25 0.5
0.0 0.00 0.0 0.0
0 200 400 0 200 400 0 200 400 0 200 400
Yolo5 Yolo8 Yolo5 Yolo8
5 5
0.05 0.05
0.00 0 0.00 0
0 200 400 0 200 400 0 200 400 0 200 400
Epochs Epochs
(a)Boundingboxloss (b)Totalloss
Figure12:Comparisonoflosscurvesforthe8trainedobjectdetectionmodels:(a)Bounding
boxlosscurves(b)Totallosscurves.
18
ssoL
xob
gnidnuoB
ssoL
latoTunseenimagesfrombothdatasetshighlightingtheboundingboxesofestimatedaircrafts
ispresentedinFig. 13andFig. 14wheretheND,FPandIEstandsrespectivelyfor’Not
Detected’,’FalsePositives’and’InaccurateEstimation’. Thefiguresshowsthestruggleof
CenterNet,FasterRCNNandSSDinthedetectionofsmallobjectisobservablewhileboth
YOLOversionsandRTMDetareabletodetectover80%oftheaircraftintheimagewitha
minimumconfidenceof32.6%withsmallamountofFPand/orND.
YOLOv8 SSD DETR CenterNet
ND
FP
FP
ND
FP
IE
ND
FP ND
ND
FP ND FP
ND
IE
IE
Figure13: InferenceexamplesofYOLOv8,DETR,SSDandCenterNetonunseenimages
fromGoogleEarthHRPlanesv2datasetandAirbusGDIT.‘FP’standsforFalsePositives,‘ND’
forNoDetectionand‘IE’forInaccurateEstimation.
TheresultsofthemetricsIoU,RecallandAParepresentedinthehistograms(Fig. 15). On
thetrainsubset,YOLOv8demonstratednotableperformancewithanAPof91.9%,Recall
of68.6%,andIoUof71.1%. YOLOv5exhibitedexcellencewithanAPof96.8%,Recallof
66.1%,andIoUof74.0%. Conversely,SSDdisplayedacomparativelylowerAPof59.4%,
Recallof38.6%,andIoUof49.2%. PerformancemetricssuchasAP,Recall,andIoUshowed
variationsforFasterRCNNandCenterNet.
TheevaluationontheTestsubsetprovidedfurtherinsightsintothegeneralizationcapa-
bilitiesofthepre-trainedalgorithms. YOLOv8maintainedhighperformancewithanAP
of90.3%,Recallof70.9%,andIoUof70.4%. Similarly,YOLOv5exhibitedcommendable
performancewithanAPof95.6%,Recallof70.5%,andIoUof75.2%. However,SSD,Faster
RCNN,andCenterNetdisplayedvariationsinperformancemetricsacrosstheTestsubset.
19
2VsenalPRH
TIDGYOLOv5 RTMDet RetinaNet Faster-RCNN
FP
FP
FP
FP FP FP
ND
FP
ND
ND
ND
Figure14: InferenceexamplesofYOLOv5,RTMDet,RetinaNetandFaster-RCNNonother
setofunseenimagesfromGoogleEarthHRPlanesv2datasetandAirbusGDIT.‘FP’stands
forFalsePositives,‘ND’forNoDetection.
Finally, in the Validation subset, distinct challenges were encountered, revealing the al-
gorithms’ robustness in diverse scenarios. YOLOv8 achieved an AP of 90.0%, Recall of
78.1%,andIoUof69.6%. YOLOv5maintainedhighstandardswithanAPof94.2%,Recall
of77.0%,andIoUof74.2%. PerformancenuanceswereobservedforSSD,FasterRCNN,
andCenterNet,underscoringtheiradaptabilitytodistinctdatasets.
Table4summarizestheobjectdetectionperformancemetricsofvariousmodelsonremote
sensingimagesacrossdifferentsubsets—Train, Test, Validationandallthedataset. The
overallperformancesshowedtheYOLOv5emergedasthetop-performingalgorithmacross
all subsets, with the highest evaluation metrics AP and IoU. Notably, YOLOv5 demon-
stratedcommendablerecallratesandIoUscores,positioningitastheleadingalgorithm
foraircraftdetectioninthisstudy. Conversely,SSDconsistentlyexhibitedcomparatively
lowerperformancemetrics,indicatingchallengesinaccuratelydetectingaircraftinstances
acrosssubsets. Whileotheralgorithms,includingYOLOv8,FasterRCNN,andCenterNet,
displayedvaryingdegreesofsuccess,YOLOv5consistentlyoutperformedthemintermsof
AP,Recall,andIoU.
20
2VsenalPRH
TIDG(a)
1.0
AP
Recall
0.8 IoU
0.6
0.4
0.2
0.0
YOLOv8 YOLOv5 SSD Faster RCNN RetinaNet CenterNet DETR RTMdet
(b)
1.0
AP
Recall
0.8 IoU
0.6
0.4
0.2
0.0
YOLOv8 YOLOv5 SSD Faster RCNN RetinaNet CenterNet DETR RTMdet
Models
(c)
1.0
AP
Recall
0.8 IoU
0.6
0.4
0.2
0.0
YOLOv8 YOLOv5 SSD Faster RCNN RetinaNet CenterNet DETR RTMdet
Figure15: Estimatedevaluationmetricswheninferencingthe8models(initiallytrained
onHRPlanesV2)onallimagesfromunseensubsets’Train’(a),’Test’(b)and’Validation’(c)
fromGDITaircraftdataset.
6 Conclusion
Thisstudypresentsacomprehensiveevaluationofaircraftdetectionalgorithmsinsatellite
imagery, namelyYOLO(v5andv8), FasterRCNN,CenterNet, RetinaNet, RTMDetand
DETR,specificallyfocusingontheHRPlanesV2datasetandextendingtheassessmentto
subsetsoftheGDITdataset: train,test,andvalidation. Theusedtrainingsetupconsists
of training the aformentioned object detection algorithms on the dataset for 500 epochs
on a 3 NVIDIA RTX A6000 GPUs configuration, each GPU has 48 GB of memory. The
resultsoftheevaluationdemonstratetheadaptabilityandrobustnessofthetrainedobject
detectionalgorithms.Amongthesealgorithms,YOLOv5emergesasthestandoutperformer,
achievingthehighestmeanaverageprecision(mAP)of0.99,highlightingitsprecisionand
robustness. YOLOv8closelyfollows,furtheremphasizingtheeffectivenessoftheYOLO
architecture in aerial object detection. On the other side, the SSD displayed the lowest
performancesinboththetrainingandevaluation. Furthermore,theevaluationextendsto
theGDITdataset,providingamorecomprehensiveassessmentbydeployingthetrained
networkonotherscenarios,includingdifferentsatelliteimagerysources. Byemploying
evaluationmetricssuchasAveragePrecision,Recall,andIntersectionoverUnion,YOLOv5
stillconsistentlyoutperformstheotheralgorithms,demonstratingsuperiorperformance
21
seulaV
cirteMTable4: ObjectDetectionPerformanceonRemoteSensingImages. Tr. subsetreferstoall
imagesfromthe’train’,Te. subsetreferstoallimagesfromthe’test’,Val. subsetreferstoall
imagesfromthe’validation’andAlldatasetoverallimagesinthedataset.
Architecture Metric Tr.subset Te.subset Val.subset Alldataset
YOLOv8 AP 0.919 0.903 0.900 0.907
Recall 0.686 0.710 0.781 0.726
IoU 0.711 0.704 0.696 0.704
YOLOv5 AP 0.968 0.956 0.942 0.955
Recall 0.661 0.705 0.770 0.712
IoU 0.740 0.752 0.742 0.745
SSD AP 0.594 0.653 0.638 0.628
Recall 0.386 0.432 0.496 0.438
IoU 0.492 0.517 0.516 0.508
FasterRCNN AP 0.573 0.714 0.726 0.671
Recall 0.422 0.413 0.491 0.442
IoU 0.473 0.564 0.564 0.533
RetinaNet AP 0.819 0.830 0.815 0.821
Recall 0.566 0.620 0.644 0.610
IoU 0.647 0.652 0.641 0.647
CenterNet AP 0.566 0.742 0.758 0.689
Recall 0.434 0.441 0.536 0.470
IoU 0.473 0.593 0.603 0.556
DETR AP 0.705 0.725 0.718 0.716
Recall 0.610 0.636 0.680 0.642
IoU 0.553 0.576 0.558 0.563
RTMDet AP 0.863 0.850 0.836 0.850
Recall 0.670 0.685 0.718 0.691
IoU 0.672 0.676 0.664 0.670
acrossallsubsets. ThissolidifiesYOLOv5’spositionasthetop-performingalgorithmfor
aircraftdetection,characterizedbycommendableAP,Recall,andIoUscores.
Thetopicofthisresearchoffersaextensiveoverviewandcomparativestudy,providing
anin-depthanalysisoftheperformance,accuracy,andcomputationaldemandsofobject
detectionmethod. Theinsightsgainedfromthisstudysignificantlyenhancethedecision-
makingprocessforselectingthemosteffectiveaircraftlocalizationtechniquesinsatellite
imagery,supportedbydetailedtrainingandvalidationprocessesusingtheHRPlanesv2
datasetandadditionalvalidationwiththeGDITdataset.
References
[1] AkashAshapure,JinhaJung,AnjinChang,SungchanOh,MuriloMaeda,andJuan
Landivar. Acomparativestudyofrgbandmultispectralsensor-basedcottoncanopy
covermodellingusingmulti-temporaluasdata. RemoteSensing,11(23):2757,2019.
[2] Shenshen Fu, Yifan He, Xiaofeng Du, and Yi Zhu. Anchor-free object detection in
remotesensingimagesusingavariablereceptivefieldnetwork. EURASIPJournalon
AdvancesinSignalProcessing,2023(1):53,2023.
[3] SushobhanMajumdar. TheRoleofRemoteSensingandGISinMilitaryStrategytoPrevent
TerrorAttacks,pages79–94. 2021.
[4] AlbertaBianchinandLauraBravin. Remotesensingandurbananalysis. InOsvaldo
Gervasi,BeniaminoMurgante,AntonioLaganà,DavidTaniar,YoungsongMun,and
MarinaL.Gavrilova,editors,ComputationalScienceandItsApplications–ICCSA2008,
pages300–315,Berlin,Heidelberg,2008.SpringerBerlinHeidelberg.
[5] HarmGreidanus. Satelliteimagingformaritimesurveillanceoftheeuropeanseas. In
RemoteSensingoftheEuropeanSeas,pages343–358.Springer,2008.
22[6] X.Liu,Y.Zhang,andZ.Wang. Aircraftdetectioninhigh-resolutionsatelliteimages
usingdeeplearning. InternationalJournalofRemoteSensing,43(10):4349–4367,2022.
[7] Z. Pan, X. Wu, J. Ma, B. Hu, Y. Wang, and L. Zhao. Sar-dad: A novel small target
detectionalgorithmforspacebornesarimagery. Complexity,2022:2262549,2022.
[8] IvanPrudyus,VictorTkachenko,PetroKondratov,SerhiyFabirovskyy,LeonidLazko,
andAndriyHryvachevskyi. Factorsaffectingthequalityofformationandresolution
ofimagesinremotesensingsystems. JCPEE,5(1):41–46,2015.
[9] UgurAlganci,MehmetSoydas,andElifSertel. Comparativeresearchondeeplearning
approachesforairplanedetectionfromveryhigh-resolutionsatelliteimages. Remote
Sensing,12(3),2020.
[10] GlennJocher,AyushChaurasia,andJingQiu. Ultralyticsyolov8,2023.
[11] WeiLiu,DragomirAnguelov,DumitruErhan,ChristianSzegedy,ScottReed,Cheng-
Yang Fu, and Alexander C. Berg. SSD: Single Shot MultiBox Detector. In Bastian
Leibe,JiriMatas,NicuSebe,andMaxWelling,editors,ComputerVision–ECCV2016,
LectureNotesinComputerScience,pages21–37,Cham,2016.SpringerInternational
Publishing.
[12] RossGirshick,JeffDonahue,TrevorDarrell,andJitendraMalik. Region-BasedConvo-
lutionalNetworksforAccurateObjectDetectionandSegmentation. IEEETransactions
onPatternAnalysisandMachineIntelligence,38(1):142–158,January2016. Conference
Name: IEEETransactionsonPatternAnalysisandMachineIntelligence.
[13] RossGirshick. FastR-CNN. In2015IEEEInternationalConferenceonComputerVision
(ICCV),pages1440–1448,December2015. ISSN:2380-7504.
[14] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
real-timeobjectdetectionwithregionproposalnetworks,2016.
[15] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. MaskR-CNN. In2017
IEEEInternationalConferenceonComputerVision(ICCV),pages2980–2988,October2017.
ISSN:2380-7504.
[16] MingxingTan,RuomingPang,andQuocV.Le. EfficientDet: ScalableandEfficient
ObjectDetection. pages10778–10787,2020.
[17] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDollár. FocalLossfor
DenseObjectDetection. IEEETransactionsonPatternAnalysisandMachineIntelligence,
42(2):318–327,February2020.ConferenceName:IEEETransactionsonPatternAnalysis
andMachineIntelligence.
[18] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian.
CenterNet: KeypointTripletsforObjectDetection,April2019. arXiv:1904.08189[cs].
[19] Meng-HaoGuo,Tian-XingXu,Jiang-JiangLiu,Zheng-NingLiu,Peng-TaoJiang,Tai-
Jiang Mu, Song-Hai Zhang, Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu.
Attention mechanisms in computer vision: A survey. Computational visual media,
8(3):331–368,2022.
[20] XiaoxiaMeng,XiaoweiWang,ShoulinYin,andHangLi. Few-shotimageclassification
algorithmbasedonattentionmechanismandweightfusion. JournalofEngineeringand
AppliedScience,70(1):14,2023.
[21] WeiLi,KaiLiu,LizheZhang,andFeiCheng. Objectdetectionbasedonanadaptive
attentionmechanism. ScientificReports,10(1):11307,2020.
[22] GlennJocher,AyushChaurasia,andJingQiu. YOLOv8byUltralytics. Software.
[23] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu,
Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time
objectdetectors,2022.
[24] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui,
YuningDu,QingqingDang,andYiLiu. Detrsbeatyolosonreal-timeobjectdetection.
arXivpreprintarXiv:2304.08069,2023.
[25] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. CoRR,
abs/1808.01244,2018.
23[26] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDollár. Focallossfor
denseobjectdetection. IEEETransactionsonPatternAnalysisandMachineIntelligence,
42(2):318–327,2020.
[27] OranR.YoungandMasamiOnoda.SatelliteEarthObservationsinEnvironmentalProblem-
Solving,pages3–27. SpringerSingapore,Singapore,2017.
[28] DanielWilson,XiaohanZhang,WaqasSultani,andSafwanWshah. Imageandobject
geo-localization. InternationalJournalofComputerVision,pages1–43,2023.
[29] Omer Wosner, Guy Farjon, and Aharon Bar-Hillel. Object detection in agricultural
contexts: Amultipleresolutionbenchmarkandcomparisontohuman. Computersand
ElectronicsinAgriculture,189:106404,2021.
[30] HaoZhangandJiayiMa. Stp-som: Scale-transferlearningforpansharpeningviaesti-
matingspectralobservationmodel.InternationalJournalofComputerVision,131(12):3226–
3251,2023.
[31] CSAyushKumar,AdvaithDasMaharana,SrinathMuraliKrishnan,SannidhiSriSai
Hanuma,VSowmya,andVinayakumarRavi. Vehicledetectionfromaerialimagery
usingprincipalcomponentanalysisanddeeplearning. InInternationalConferenceon
InnovationsinBio-InspiredComputingandApplications,pages129–140.Springer,2022.
[32] Gui-SongXia,XiangBai,JianDing,ZhenZhu,SergeBelongie,JieboLuo,MihaiDatcu,
MarcelloPelillo,andLiangpeiZhang. Dota: Alarge-scaledatasetforobjectdetection
in aerial images. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pages3974–3983,2018.
[33] JosephRedmonandAliFarhadi. Yolov3: Anincrementalimprovement, 2018. cite
arxiv:1804.02767Comment: TechReport.
[34] TolgaBAKIRMANandElifSERTEL. Abenchmarkdatasetfordeeplearning-based
airplane detection: Hrplanes. International Journal of Engineering and Geosciences,
8(3):212–223,2023.
[35] AlexeyBochkovskiy,Chien-YaoWang,andHong-YuanMarkLiao. Yolov4: Optimal
speedandaccuracyofobjectdetection. arXivpreprintarXiv:2004.10934,2020.
[36] Class. Airbus aircraft detection dataset. https://universe.roboflow.com/
class-dvpyb/airbus-aircraft-detection,jan2023. visitedon2024-01-21.
[37] JacobShermeyer,ThomasHossler,AdamVanEtten,DanielHogan,RyanLewis,and
DaeilKim. Rareplanes: Syntheticdatatakesflight. CoRR,abs/2006.02963,2020.
[38] GDIT. Aerial airport dataset. https://universe.roboflow.com/gdit/
aerial-airport,sep2023. visitedon2024-01-21.
[39] RobertHammell. Planesinsatelliteimagery,2023.
[40] MauricioPamplonaSegundo,AllanPinto,RodrigoMinetto,RicardodaSilvaTorres,
andSudeepSarkar. Adatasetfordetectingflyingairplanesonsatelliteimages,2021.
[41] JChen,HLi,GZhang,etal. Datasetofaircraftclassificationinremotesensingimages.
J.Glob.ChangeDataDiscov,2:183–190,2020.
[42] C.BhagyaandA.Shyna. Anoverviewofdeeplearningbasedobjectdetectiontech-
niques. In20191stInternationalConferenceonInnovationsinInformationandCommunica-
tionTechnology(ICIICT),pages1–6,2019.
[43] PayalMittal,RamanSingh,andAkashdeepSharma. Deeplearning-basedobjectdetec-
tioninlow-altitudeuavdatasets: Asurvey. ImageandVisionComputing,104:104046,
2020.
[44] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez,ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesin
neuralinformationprocessingsystems,30,2017.
[45] LiLiu,WanliOuyang,XiaogangWang,PaulFieguth,JieChen,XinwangLiu,andMatti
Pietikäinen. Deeplearningforgenericobjectdetection: Asurvey. Internationaljournal
ofcomputervision,128:261–318,2020.
[46] XingxingXie,GongCheng,JiabaoWang,KeLi,XiwenYao,andJunweiHan. Oriented
r-cnnandbeyond. InternationalJournalofComputerVision,pages1–23,2024.
24[47] Hritik Vaidwan, Nikhil Seth, Anil Singh Parihar, and Kavinder Singh. A study on
transformer-basedobjectdetection. In2021InternationalConferenceonIntelligentTech-
nologies(CONIT),pages1–6,2021.
[48] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z Li. Single-shot re-
finementneuralnetworkforobjectdetection. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages4203–4212,2018.
[49] ZhaoweiCaiandNunoVasconcelos. Cascader-cnn: Delvingintohighqualityobject
detection. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages6154–6162,2018.
[50] JifengDai,YiLi,KaimingHe,andJianSun. R-fcn: Objectdetectionviaregion-based
fullyconvolutionalnetworks. Advancesinneuralinformationprocessingsystems,29,2016.
[51] ChenyiChen,Ming-YuLiu,OncelTuzel,andJianxiongXiao. R-cnnforsmallobject
detection. InComputerVision–ACCV2016: 13thAsianConferenceonComputerVision,
Taipei,Taiwan,November20-24,2016,RevisedSelectedPapers,PartV13,pages214–230.
Springer,2017.
[52] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKir-
illov,andSergeyZagoruyko. End-to-endobjectdetectionwithtransformers,2020.
[53] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision
transformerbackbonesforobjectdetection. InEuropeanConferenceonComputerVision,
pages280–296.Springer,2022.
[54] XinLu,QuanquanLi,BuyuLi,andJunjieYan. MimicDet: BridgingtheGapBetween
One-StageandTwo-StageObjectDetection. pages541–557.November2020.
[55] JeremyJordan. Anoverviewofobjectdetection: One-stagemethods. https://www.
jeremyjordan.me/object-detection-one-stage/,2021. Accessedon8June2023.
[56] HangZhangandRayanSCloutier. Reviewonone-stageobjectdetectionbasedon
deeplearning. EAIEndorsedTransactionsone-Learning,7(23),62022.
[57] LichengJiao,FanZhang,FangLiu,ShuyuanYang,LinglingLi,ZhixiFeng,andRong
Qu. Asurveyofdeeplearning-basedobjectdetection. IEEEAccess,7:128837–128868,
2019.
[58] AdityaLohia, KalyaniKadam, RahulJoshi, andArunkumarBongale. Bibliometric
analysisofone-stageandtwo-stageobjectdetection. 022021.
[59] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look
Once: Unified,Real-TimeObjectDetection. pages779–788,June2016. ISSN:1063-6919.
[60] XinLu,QuanquanLi,BuyuLi,andJunjieYan. Mimicdet: Bridgingthegapbetween
one-stageandtwo-stageobjectdetection. CoRR,abs/2009.11528,2020.
[61] TaoGong,KaiChen,XinjiangWang,QiChu,FengZhu,DahuaLin,NenghaiYu,and
HuaminFeng. Temporalroialignforvideoobjectrecognition. CoRR,abs/2109.03495,
2021.
[62] TausifDiwan, GAnirudh, andJitendraVTembhurne. Objectdetectionusingyolo:
Challenges,architecturalsuccessors,datasetsandapplications. multimediaToolsand
Applications,82(6):9243–9275,2023.
[63] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In 2017 IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR),pages6517–6525,2017.
[64] GlennJocher,AyushChaurasia,AlexStoken,JirkaBorovec,NanoCode012,Yonghye
Kwon,KalenMichael,TaoXie,JiacongFang,Imyhxy,”ZengYifu”Lornaand,Colin
Wong,AbhiramV,DiegoMontes,ZhiqiangWang,CristiFati,JebastinNadar,Laughing,
UnglvKitDe, Victor Sonck, Tkianai, YxNONG, Piotr Skalski, Adam Hogan, Dhruv
Nair,MaxStrobel,andMrinalJain. ultralytics/yolov5: v7.0-YOLOv5SOTARealtime
InstanceSegmentation. Zenodo,November2022.
[65] Chien-YaoWang,AlexeyBochkovskiy,andHong-yuanLiao. Scaled-yolov4: Scaling
crossstagepartialnetwork. pages13024–13033,062021.
25[66] ShayAharon,Louis-Dupont,OfriMasad,KateYurkova,LotemFridman,Lkdci,Eugene
Khvedchenya,RanRubin,NatanBagrov,BorysTymchenko,TomerKeren,Alexander
Zhilko,andEran-Deci. Super-gradients,2021.
[67] KrishnaTejaChitty-Venkata,MuraliEmani,VenkatramVishwanath,andArunSomani.
Neuralarchitecturesearchbenchmarks: Insightsandsurvey. IEEEAccess,PP:1–1,01
2023.
[68] WeiLiu,DragomirAnguelov,DumitruErhan,ChristianSzegedy,ScottReed,Cheng-
YangFu,andAlexanderC.Berg. Ssd: Singleshotmultiboxdetector. InBastianLeibe,
JiriMatas,NicuSebe,andMaxWelling,editors,ComputerVision–ECCV2016,pages
21–37,Cham,2016.SpringerInternationalPublishing.
[69] SandroAugustoMagalhães,LuísCastro,GermanoMoreira,FilipeNevesdosSantos,
Mário Cunha, Jorge Dias, and António Paulo Moreira. Evaluating the single-shot
multiboxdetectorandyolodeeplearningmodelsforthedetectionoftomatoesina
greenhouse. Sensors,21(10),2021.
[70] Soulef Bouaafia, Seifeddine Messaoud, Amna Maraoui, Ahmed Chiheb Ammari,
Lazhar Khriji, and Mohsen Machhout. Deep pre-trained models for computer vi-
sionapplications: Trafficsignrecognition. In202118thInternationalMulti-Conferenceon
Systems,Signals&Devices(SSD),pages23–28,2021.
[71] AlexKendall,YarinGal,andRobertoCipolla. Multi-tasklearningusinguncertaintyto
weighlossesforscenegeometryandsemantics,2018.
[72] Juan Terven, Diana M. Cordova-Esparza, Alfonso Ramirez-Pedraza, and Edgar A.
Chavez-Urbiola. Lossfunctionsandmetricsindeeplearning,2023.
[73] JisooJeong,HyojinPark,andNojunKwak. EnhancementofSSDbyconcatenating
featuremapsforobjectdetection. CoRR,abs/1705.09587,2017.
[74] AshwaniKumar,ZuopengJustinZhang,andHongboLyu. Objectdetectioninreal
timebasedonimprovedsingleshotmulti-boxdetectoralgorithm. EURASIPJournalon
WirelessCommunicationsandNetworking,2020(1):204,October2020.
[75] RossGirshick,JeffDonahue,TrevorDarrell,andJitendraMalik. Richfeaturehierar-
chiesforaccurateobjectdetectionandsemanticsegmentation. ProceedingsoftheIEEE
ComputerSocietyConferenceonComputerVisionandPatternRecognition,112013.
[76] Rikiya Yamashita, Mizuho Nishio, Richard Do, and Kaori Togashi. Convolutional
neuralnetworks: anoverviewandapplicationinradiology. InsightsintoImaging,9,06
2018.
[77] AyushJaiswal,YueWu,PradeepNatarajan,andP.Natarajan. Class-agnosticobject
detection. 2021IEEEWinterConferenceonApplicationsofComputerVision(WACV),pages
918–927,2020.
[78] JasperUijlings,K.Sande,T.Gevers,andA.W.M.Smeulders. Selectivesearchforobject
recognition. InternationalJournalofComputerVision,104:154–171,092013.
[79] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwith
deepconvolutionalneuralnetworks. 25,2012.
[80] M.A.Hearst,S.T.Dumais,E.Osuna,J.Platt,andB.Scholkopf.Supportvectormachines.
IEEEIntelligentSystemsandtheirApplications,13(4):18–28,1998.
[81] ZhengxiaZou,KeyanChen,ZhenweiShi,YuhongGuo,andJiepingYe.Objectdetection
in20years: Asurvey,2023.
[82] ShahidKarim,YeZhang,ShoulinYin,IrfanaBibi,andAliBrohi. Abriefreviewand
challengesofobjectdetectioninopticalremotesensingimagery. MultiagentandGrid
Systems,16:227–243,102020.
[83] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningfor
imagerecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages770–778,2016.
[84] TianyangLin,YuxinWang,XiangyangLiu,andXipengQiu. Asurveyoftransformers,
2021.
26[85] Zhong-QiuZhao,PengZheng,ShoutaoXu,andXindongWu. Objectdetectionwith
deeplearning: Areview,2019.
[86] Omar Elharrouss, Younes Akbari, Noor Almaadeed, and Somaya Al-Maadeed.
Backbones-review: Featureextractionnetworksfordeeplearninganddeepreinforce-
mentlearningapproaches,2022.
[87] TianyangLin,YuxinWang,XiangyangLiu,andXipengQiu. Asurveyoftransformers.
AIOpen,3:111–132,2022.
[88] Syifa Auliyah Hasanah, Anindya Apriliyanti Pravitasari, Atje Setiawan Abdullah,
IntanNurmaYulita,andMohammadHamidAsnawi. Adeeplearningreviewofresnet
architectureforlungdiseaseidentificationincxrimage. AppliedSciences,13(24),2023.
[89] WenchaoGu,ShuangBai,andLingxingKong. Areviewon2dinstancesegmentation
basedondeepneuralnetworks. ImageandVisionComputing,120:104401,2022.
[90] KaiChen,JiaqiWang,JiangmiaoPang,YuhangCao,YuXiong,XiaoxiaoLi,Shuyang
Sun,WansenFeng,ZiweiLiu,JiaruiXu,ZhengZhang,DazhiCheng,ChenchenZhu,
TianhengCheng,QijieZhao,BuyuLi,XinLu,RuiZhu,YueWu,JifengDai,Jingdong
Wang,JianpingShi,WanliOuyang,ChenChangeLoy,andDahuaLin. Mmdetection:
Openmmlabdetectiontoolboxandbenchmark,2019.
[91] XiaohanDing,XiangyuZhang,YizhuangZhou,JungongHan,GuiguangDing,and
JianSun. Scalingupyourkernelsto31x31: Revisitinglargekerneldesignincnns,2022.
[92] TianxiaoZhang,BoLuo,AjaySharda,andGuanghuiWang. Dynamiclabelassignment
forobjectdetectionbycombiningpredictediousandanchorious. JournalofImaging,
8(7):193,July2022.
[93] YuxinWu,AlexanderKirillov,FranciscoMassa,Wan-YenLo,andRossGirshick. Detec-
tron2. https://github.com/facebookresearch/detectron2,2019.
[94] RafaelPadilla, WesleyLPassos, ThadeuLBDias, SergioLNetto, andEduardoAB
DaSilva. Acomparativeanalysisofobjectdetectionmetricswithacompanionopen-
sourcetoolkit. Electronics,10(3):279,2021.
27