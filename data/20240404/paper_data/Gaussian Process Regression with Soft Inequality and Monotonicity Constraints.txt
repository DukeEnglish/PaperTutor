GAUSSIAN PROCESS REGRESSION WITH SOFT INEQUALITY
AND MONOTONICITY CONSTRAINTS
A PREPRINT
DidemKochan XiuYang
DepartmentofIndustrialandSystemsEngineering DepartmentofIndustrialandSystemsEngineering
LehighUniversity LehighUniversity
Bethlehem,PA18015 Bethlehem,PA18015
dik318@lehigh.edu xiy518@lehigh.edu
ABSTRACT
Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate com-
plex models. Standard GP regression can lead to an unboundedmodel in which some points can
take infeasible values. We introducea new GP method that enforcesthe physicalconstraintsin a
probabilisticmanner. ThisGPmodelistrainedbythequantum-inspiredHamiltonianMonteCarlo
(QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the
standardHamiltonianMonte Carlo algorithmin which a particle has a fixed mass, QHMC allows
a particle to have a random mass matrix with a probability distribution. Introducing the QHMC
methodtotheinequalityandmonotonicityconstrainedGPregressionintheprobabilisticsense,our
approachimprovestheaccuracyandreducesthevarianceintheresultingGPmodel. Accordingto
ourexperimentsonseveraldatasets,theproposedapproachservesasanefficientmethodasitaccel-
eratesthesamplingprocesswhilemaintainingtheaccuracy,anditisapplicabletohighdimensional
problems.
Keywords Gaussianprocess·Quantum-inspiredMonteCarlo·Softconstraints·Inequalityconstraints
1 Introduction
In many real-world applications, measuring complex systems or evaluating computational models can be time-
consuming,costlyorcomputationallyintensive.Gaussianprocessregression(GPR)isoneoftheBayesiantechniques
that addresses this problem by building a surrogate model. It is a supervised machine learning framework that has
beenwidelyusedinregressionandclassificationtasks. AGPcanbeinterpretedasasuitableprobabilitydistribution
onasetoffunctions,whichcanbeconditionedonobservationsusingBayes’rule[13]. A GPregressionmodelcan
make predictionsincorporatingprior information(kernels)and generateuncertaintymeasuresoverpredictions[22].
However, priorknowledgeoften includesphysicallaws, and using the standardGP regressionframeworkmay lead
toanunboundedmodelinwhichsomepointscantakeinfeasiblevaluesthatviolatephysicallaws[13]. Forexample,
non-negativityisarequirementforvariousphysicalpropertiessuchastemperature,densityandviscosity[20]. There-
fore,incorporatingphysicalinformationinGPframeworkcanregularizethebehaviourofthemodelandprovidemore
realisticuncertainties[26].
AsignificantamountofresearchhasbeenconductedtoincorporatephysicalinformationinGPframework,resulting
invarioustechniquesandmethodologies[26]. Forexample,aprobitmodelforthelikelihoodofderivativeinforma-
tion canbe employedto enforcemonotonicityconstraints[23]. Althoughthis approachcan also be usedto enforce
convexityinonedimension,anadditionalrequirementonHessianisincorporatedforhigherdimensions[5]. In[16]
anadditiveGPapproachisintroducedtoaccountformonotonicityconstraints. Althoughposteriorsamplingstepcan
bechallenging,theadditiveGPframeworkenablestosatisfytheconstraintseverywhereintheinputspace, anditis
scalabletohigherdimensions.
EnforcinginequalityconstraintsintoaGP,ontheotherhand,ismorechallenging,astheconditionalprocess,subject
to these constraints, does not retain the properties of a GP [18]. One of the approaches to handle this problem is
4202
rpA
3
]LM.tats[
1v37820.4042:viXraGPwithSoftConstraints APREPRINT
a data augmentationapproachinwhich theinequalityconstraintsareenforcedatvariouslocationsandapproximate
samplesaredrawnfromthepredictivedistribution[1],orusingablockcovariancekernel[21]. Implicitlyconstrained
GPregressionmethodproposedin[24]showsthatthemeanpredictionofaGPimplicitlysatisfieslinearconstraints,
iftheconstraintsaresatisfied bythetrainingdata. Asimilar approachshowsthatwhenweimposelinearinequality
constraints on a finite set of points in the domain, the resulting process is a compound Gaussian Process with a
truncatedGaussianmean[2].
Most of the approaches assume that the inequalities are satisfied on a finite set of input locations. Based on that
assumption, the methods approximate the posterior distribution given those constraint input points. The approach
introducedin[5]isanexampleofthesemethods,wheremaximumlikelihoodestimationofGPhyperparametersare
investigatedundertheconstraintassumptions.Inpractice,thisshouldalsolimitthenumberofconstraintpointsneeded
foraneffectivediscrete-locationapproximation. Inaddition,themethodisnotefficientonhigh-dimensionaldatasets
asittakesalargeamountoftimetotraintheGPmodel.
ThefirstGaussianmethodthatsatisfiescertaininequalitiesatalltheinputspaceisproposedbyMaatoukandBay[18].
The GP approximationof the samplesare performedin the finite-dimensionalspace functions, and a rejectionsam-
plingmethodisusedforapproximatingtheposterior.Theconvergencepropertiesofthemethodisinvestigatedin[19].
Althoughusingtherejectionsamplingtoobtainposteriorhelpsconvergence,itmightbecomputationallyexpensive.
Similarto thepreviousapproachesinwhicha setofinputssatisfy theconstraints, thismethodalsosuffersfromthe
curse of dimensionality. Later, the truncated Gaussian approach[17] extendsthe frameworkin [18] to generalsets
oflinearinequalities. Buildinguponthe approachesin [18] and[19], the workpresentedin[17] introducesafinite-
dimensionalapproachthatincorporatesinequalitiesforbothdata interpolationandcovarianceparameterestimation.
Inthiswork, the posteriordistributionisexpressedasa truncatedmultinormaldistribution. Themethodusesdiffer-
ent Markov Chain Monte Carlo (MCMC) methods and exact sampling methodsto obtain the posterior distribution.
AmongthevariousMCMCsamplingtechniquesincludingGibbs,Metropolis-Hastings(MH)andHamiltonianMonte
Carlo(HMC),theresultsindicatethatHMCsamplingisthemostefficientone. ThetruncatedGaussianapproaches
offer several advantages, including the ability to achieve high accuracy and the flexibility in satisfying multiple in-
equality conditions. However, although those types of methods address the limitations in [18], they might be time
consumingparticularlyinapplicationswithlargedatasetsorhigh-dimensionalspaces.
Inthiswork,weuseQHMCalgorithmtotraintheGPmodel,andenforcetheinequalityandmonotonicityconstraints
in a probabilistic manner. Our work addresses the computational limitations caused by high dimensions or large
datasets. UnliketruncatedGaussianmethodsin[17]forinequalityconstraints,oradditiveGP[16]withmonotonicity
constraints, the proposed method can maintain its efficiency on higher dimensions. Further, we adopt an adaptive
learning algorithm that selects the constraint locations. The efficiency and accuracy of the QHMC algorithms are
demonstratedon inequalityand monotonicityconstrainedproblems. Inequalityconstrainedexamplesincludelower
and higher dimensionalsynthetic problems, a conservativetracer distribution from sparse tracer concentrationmea-
surementsandathree-dimensionalheattransferproblem,whilemonotonicityconstrainedexamplesprovidelowerand
higherdimensionalsyntheticproblems. Ourcontributionscanbesummarizedinthreekeypoints: (i)QHMCreduces
differencebetweenposteriormeanandthegroundtruth,(ii)utilizingQHMCina probabilisticsensedecreasesvari-
anceanduncertainty,and(iii) theproposedalgorithmisa robust, efficientandflexiblemethodapplicableto a wide
rangeofproblems. We implementedQHMCsamplinginthe truncatedGaussianapproachto enhanceaccuracyand
efficiencywhileworkingwiththeQHMCalgorithm.
2 GaussianProcess under inequality constraints
2.1 StandardGPregressionframework
Supposewehaveatargetfunctionrepresentedbyvaluesy =(y(1),y(2),...,y(T))N,wherey(i) ∈Rareobservations
atlocationsX={x(i)}N . Here,x(i) representsd-dimensionalvectorsinthedomainD∈Rd. Usingtheframework
i=1
providedin[12],weapproximatethetargetfunctionbyaGP,denotedasY(.,.):D×Ω→R. WecanexpressY as
′
Y(x):=GP[µ(x),K(x,x)],
whereµ(.)isthemeanfunctionandK(x,x′)isthecovariancefunctiondefinedas
µ(x)=E[Y(x)], and K(x,x′ )=E[Y(x)−µ(x)][Y(x′ )−µ(x′ )]
Typically,thestandardsquaredexponentialcovariancekernelcanbeusedasakernelfunction:
||x−x′||2
K(x,x′ )=σ2exp (cid:18)−
2l2
2 (cid:19)+σ n2δ x,x′,
2GPwithSoftConstraints APREPRINT
whereσ2 isthesignalvariance,δ x,x′ isthe Kroneckerdeltafunctionandl isthe length-scale. We thenassumethat
theobservationincludesanadditiveindependentidenticallydistributed(i.i.d.) Gaussiannoisetermǫandhavingzero
meanandvarianceσ2. We denotethehyperparametersbyθ = (σ,l,σ ), andestimatethemusingthetrainingdata.
n n
Theparameterscanbeestimatedbyminimizingthenegativemarginallog-likelihood[12,25,29]:
1
−log[p(Y|X,θ)]= [(y−µ)TK−1(y−µ)+log|K|+Nlog(2π)]. (1)
2
Inthefollowingsection,wewillshowhowtheparameterupdatesareperformedusingtheQHMCmethod.
2.2 Quantum-inspiredHamiltonianMonteCarlo
QHMC is an enhancedversionof the HamiltonianMonteCarlo (HMC) algorithmthatincorporatesa randommass
matrixfortheparticles,followingaprobabilitydistribution. InconventionalHMC,thepositionisrepresentedbythe
originalvariables(x),whileGaussianmomentumisrepresentedbyauxiliaryvariables(q). Utilizingtheenergy-time
uncertaintyrelationofquantummechanics,QHMCallowsaparticletohavearandommassmatrixwithaprobability
distribution. Consequently, in addition to the position and momentum variables, a mass variable (m) is introduced
within the QHMC framework. Having a third variable offers the advantage of exploring various landscapes in the
state-space. As a result, unlike standard HMC, QHMC can perform well on discontinuous, non-smooth and spiky
distributions[4,15].
The quantum nature of QHMC can be understood by considering a one-dimensional harmonic oscillator example
providedin [15]. Let us consider a ball with a fixed mass m attached to a spring at the origin. Assuming x is the
displacement, the magnitude of the restoring force that pulls back the ball to the origin is F = −kx, and the ball
oscillatesaroundthe originwith periodT = 2π m. In contrastto standardHMCwherethe mass m is fixedat1,
k
QHMCincorporatesatime-varyingmass,allowinpgtheballtoexperienceaccelerationandexplorevariousdistribution
landscapes. That is, QHMC has the capability to employ a short time period T, correspondingto a small mass m,
toefficientlyexplorebroadbutflatregions. Conversely,inspikyregions,itcanswitchtoalargertimeperiodT,i.e.
largerm,toensurethoroughexplorationofallcornersofthelandscape[15].
TheimplementationofQHMCisstraightforward: we constructastochasticprocessM(t)forthemass, andateach
time t, we sample M(t) from a distribution P (M). Resampling the positive-definite mass matrix is the only ad-
M
ditionalstepto thestandardHMCprocedure. Inpractice,assumingthatP (M)isindependentofxandq, a mass
M
density function P (M) with mean µ and varianceσ2 can be logm ∼ N(µ ,σ2 ), M = mI, where I is the
M m m m m
identitymatrix. QHMCframeworksimulatesthefollowingdynamicalsystem:
x M(t)−1q
d =dt .
(cid:18)q(cid:19) (cid:18)−∇U(x)(cid:19)
Inthissetting,thepotentialenergyfunctionoftheQHMCsystemisU(x) = −log[p(Y|X,θ)], i.e.,thenegativeof
marginallog-likelihood. We summarizethe algorithmin Algorithm1, and, here, we considerthe locationvariables
{x(i)}N in GP model as the position variables x in Algorithm 1. The method evolves the QHMC dynamics to
i=1
updatethelocationsx. Inthiswork,weimplementtheQHMCmethodforinequalityconstrainedGPregressionina
probabilisticmanner.
2.3 Proposedmethod
Instead of enforcing all constraints strictly, the approach introduced in [20] minimizes the negative marginal log-
likelihood function in Equation 1 while allowing constraint violations with a small probability. For example, for
non-negativityconstraints,thefollowingrequirementisimposedtotheproblem:
P[(Y(x)|x,θ) <0]≤η, forall x∈D,
where0<η <<1.
Incontrastto enforcingtheconstraintviatruncatedGaussianassumption[18] orperforminginferencebased onthe
Laplace approximationand expectation propagation[11], the proposed method preservesthe Gaussian posterior of
the standard GP regression. The method uses a slight modification of the existing cost function. Given a model
that follows a Gaussian distribution, we can re-express the constraint by the posterior mean and posterior standard
deviation:
y∗ (x)+φ−1(η)s(x)≥0, forall x∈D, (2)
3GPwithSoftConstraints APREPRINT
wherey∗(x)standsfortheposteriormean,sisthestandarddeviationandφisthecumulativedistributionfunctionof
aGaussianrandomvariable[20]. Apracticalchoiceforη mightbeη = 2.2%,resultinginφ−1(η) = −2. Then,we
canformulatetheoptimizationproblemas
argmin −log[p(Y|X,θ)] suchthat (3)
θ
y∗ (x)−2s(x)≥0. (4)
In this particular form of the optimization problem, we encounter a functionalconstraint described by 4. It can be
prohibitiveorimpossibletosatisfythisconstraintatallpointsacrosstheentiredomain.Therefore,weadoptastrategy
whereweenforceEquation4onlyonaselectedsetofmconstraintpointsdenotedasX =
x(i)m
. Wereformulate
c c i=1
theoptimizationproblemas
argmin −log[p(Y|X,θ)] suchthat (5)
θ
y∗ (x(i))−2s(x(i))≥0 forall i=1,2,...,m, (6)
c c
where we estimate hyperparametersto enforce bounds. Solving this optimization problemcan be very challenging,
andhence,in[20]additionalregularizationtermsareadded.Ratherthandirectlysolvingtheoptimizationproblem,we
adoptthesoft-QHMCmethod,whichintroducesinequalityconstraintswithahighprobability(e.g.,95%)byselecting
a specific set of constraintpointsin the domain. We then enforcenon-negativityon the posterior Gaussian Process
attheseselectedpoints. We minimizethelog-likelihoodinEquation1,usingtheQHMCalgorithm. Leveragingthe
Bayesianestimation[8],wecanapproximatetheposteriordistributionbylog-likelihoodfunctionandpriorprobability
distributionasshowninthefollowing:
p(X,θ|Y)∝p(X,θ,Y)=p(θ)p(X|θ)p(Y|X,θ).
The QHMC training flow starts with this Bayesian learning and proceeds with an MCMC procedure for drawing
samplesgeneratedbytheBayesianframework.Ageneralsamplingprocedureatsteptisgivenas
X(t+1) ∼π(X|θ)=p(X|θ(t),Y),
θ(t+1) ∼π(θ|X)=p(θ|X(t+1),Y).
Algorithm1QHMCTrainingforGPwithInequalityConstraints
Input: Initial point x , step size ǫ, number of simulation steps L, mass distribution parameters µ and
0 m
σ .
m
fort=1,2,...do
ResampleM ∼P (M)
t M
Resampleq ∼N(0,M )
t t
(x ,q )=(x(t),q(t))
0 0
q ←q − ǫ∇U(x )
0 0 2 0
fori=1,2,...,L−1do
x i ←x i−1+ǫM
t−1q
i−1
q
i
←q i−1− 2ǫ∇U(x i)
endfor
x L ←x L−1+ǫM
t−1q
L−1
q
L
←q L−1− 2ǫ∇U(x L)
(xˆ,qˆ)=(x ,q )
L L
MHstep: u∼Uniform[0,1];
ρ=e−H(xˆ,qˆ)+H(x(t),q(t));
ifu<min(1,ρ)then
(x(t+1),q(t+1))=(xˆ,qˆ)
else
(x(t+1),q(t+1) =(x(t),q(t))
endif
endfor
Output: {x(1),x(2),...}
4GPwithSoftConstraints APREPRINT
2.3.1 EnforcingMonotonicityConstraints
We enforce the monotonicity constraints on a GP using the likelihood of derivative observations. We select active
constraintsandenforcethenon-negativityonthepartialderivative,i.e.
∂f
(xi)≥0, (7)
∂x
i
wheref isavectorofN latentvalues. Inoursoft-constrainedmethod,weintroducethenon-negativityinformation
in7onasetofselectedpoints,andapplythesameprocedureasin5. SincethederivativeisalsoaGPwithwithmean
andcovariancematrix[23]:
∂Y(x) ∂ ∂
µ(x′ )=E , and K(x,x′ )= K(x,x′ ),
(cid:20) ∂x (cid:21) ∂x ∂x′
i i i
wecanstatethenewposteriordistributionas
p(y∗ ,θ|y,x,x∗
)=
p(y∗ ,θ|f∗ )p(f∗ |y,x,x∗
)df,
Z
p(f∗ |y,x,x∗
)=
p(f∗ |x∗ ,f,f′ )p(f,f′ |x,y)dfdf′
,
Z Z
wherey∗ andf∗ denotethepredictionsatlocationx∗.
3 Theoretical analysisofthe method
In this section, using Bayes’ Theorem, we will first show that QHMC can generate a steady-state distribution that
approachesthetrueposteriordistribution. Then,wepresenttheconvergencepropertiesoftheprobabilisticapproach
ontheoptimizationproblemintheformof6.
3.1 ConvergenceofQHMCtraining
The study presented in [15] demonstrates that the QHMC framework can effectively capture a correct steady-state
distributionthatdescribesthedesiredposteriordistributionp(x)∝exp(−U(x))viaBayes’rule.Thejointprobability
densityof(x,q,M)canbecalculatedbyBayesiantheorem:
p(x,q,M)=p(x,q|M)P (M),
M
wheretheconditionaldistributionisapproximatedasfollows:
1
p(x,q|M)∝exp(−U(x)−K(q))=exp(−U(x))exp −
qTM−1q
.
(cid:18) 2 (cid:19)
Then,wecanobtainthefollowing
p(x)= dqdMp(x,q,M)∝exp(−U(x)),
Z Z
q M
whichshowsthatthemarginalsteadydistributionapproachesthetrueposteriordistribution[15].
3.2 Convergencepropertiesofprobabilisticapproach
Inthissection,weshowthatsatisfyingtheconstraintsonasetoflocationsxinthedomainDwillpreserveconvergence.
Recallthatwesolvethefollowinginequality-constrainedoptimizationproblem:
argmin −log[p(Y|X,θ)] suchthat
θ
y∗ (x(i))−2s(x(i))≥0 forall i=1,2,...,m.
c c
Now,weneedtoshowthattheresultobtainedbyusingselectedsetofinputlocationswillconvergetothevalueofthe
regressionmodel’soutput.Thisconvergenceensuresthatprobabilisticapproachwilleventuallyresultinamodelthat
satisfythedesiredconditions.
5GPwithSoftConstraints APREPRINT
WeusetheassumptionofDbeingfinitethroughouttheproof.Theproofcanbeconstructedforthecaseswhetherthe
domainiscountableoruncountable.
(i)AssumethatthedomainD isacountablesetcontainingN elements. WeselectasubsetD ∈ D withmpoints,
m
where x(1),x(2),...,x(m) ∈ D . Each point x ∈ D has an associated Gaussian probability distribution, and we
c c c m
define the set of distributionsof x ∈ D as P. For the constraint points x ∈ D , we have m constraints and their
m
correspondingprobabilitydistributions,whichwedefineasP . Additionally,weintroduceasetH(x)suchthat
m
H(x):={θ|p(Y|X,θ)<0},
whichcoversthelocationswherethenon-negativityconstraintisviolated.Foreachfixedx∈D,wedefine
v(x):= inf P(Y|X,θ)<0≡ inf P(H(x)), and
P∈P P∈P
v (x):= inf P(Y|X,θ)<0≡ inf P(H(x)).
m
P∈Pm P∈Pm
(ii)AssumethatthedomainDisafinitebutuncountableset. Inthiscase,wecanconstructacountablesubsetD˜ such
that x ∈ D˜. We define the set of probability distributionsas in case (i). Since D is finite, the set D∪{x} is also
finite. Consequently,thesetsH(x),v(x)andv (x)canbeconstructedasinthefirstcase. Now,wewillestablisha
m
convergenceofv overvasP convergestoP.
m m
First,letusprovidedistancemetricsusedthroughouttheproof.Followingthedefinitionsin[9],let
′
d(x,A):= inf ||x−x||
x′∈A
denotethedistancefromapointxtoasetA. Usingthis,wecandefinethedistanceoftwocompactsetsAandB as
D(A,B):= supd(x,B).
x∈A
Then,theHausdorffdistancebetweenAandBisdefinedasH(A,B):=max{D(A,B),D(B,A)}. Finally,wedefine
apseudo-metricdtodescribethedistancebetweentwoprobabilitydistributionsP andP˜ as
d(P,P˜):= sup|P(H(x))−P˜(H(x))|,
x∈D
whereDisthedomainspecifiedinSection3.2.
Assumption1. WeassumethattheprobabilitydistributionsP andP satisfythefollowingconditions:
m
1. ThereexistsaweaklycompactsetP˜ suchthatP ⊂P˜ andP ⊂P˜.
m
2. lim d(P,P )=0,withprobability1.
m
m→N
3. lim d(P ,P)=0,withprobability1.
m
m→N
Now,weshowthatTheorem1holdsundertheassumptionsinAssumption1. Recallthatwehave
H(convV,convV )=max sup P(H(x))− supP(H(x)) , inf P(H(x))− inf P(H(x)) .
m (cid:26)(cid:12) (cid:12)P∈Pm P∈P (cid:12)
(cid:12)
(cid:12) (cid:12)P∈Pm P∈P (cid:12) (cid:12)(cid:27)
(cid:12) (cid:12) (cid:12) (cid:12)
Basedonthedefinitionandproperty(cid:12)ofHausdorffdistance[10]wealso(cid:12) h(cid:12)ave (cid:12)
H(convV,convV )≤H(V,V )≤max{D(V,V ),D(V ,V)}.
m m m m
Herewehave
D(V,V )= sup inf ||v−v′ ||
m
v∈V v′∈Vm
= sup inf ||P(H(x))−P˜(H(x))||
P∈PP˜∈Pm
≤ sup inf sup||P(H(x))−P˜(H(x))||
P∈PP˜∈Pmx∈D
=d(P,P ).
m
6GPwithSoftConstraints APREPRINT
Applyingthesameprocedure,wealsoobtainD(V ,V)≤d(P ,P). Hence,
m m
H(convV,convV )≤H(V,V )≤H(P ,P).
m m m
Consequently,weobtain
|v (x)−v(x)|≤ inf P(H(x))− inf P(H(x))
m (cid:12) (cid:12)P∈Pm P∈P (cid:12)
(cid:12)
≤(cid:12)H(convV,convV ) (cid:12)
(cid:12) m (cid:12)
≤H(P ,P).
m
Theorem1. v convergestovasP convergestoP,thatis
m m
lim sup|v (x)−v(x)|=0.
m
m→Nx∈D
Proof. Letusassumethatx∈Disfixed,anddefine
V :={P(H(x)):P ∈clP}, and, V :={P(H(x)):P ∈clP },
m m
whereclrepresentstheclosure. NotethatbothV andV areboundedsubsetsinRd. Letusdefinea,b,a andb
m m m
suchthat
a:= inf v, b:= supv, a := inf v, b := sup v,
m m
v∈V v∈V v∈Vm v∈Vm
TheHausdorffdistancebetweenconvexhulls(conv)ofthesetsV andV arecalculatedas[10]
m
H(convV,convV )=max{|b −b|,|a−a |}.
m m m
Sinceweknowthat
b −b= sup v−supv, and a −a= inf v− inf v,
m m
v∈Vm v∈V v∈Vm v∈V
wehave
H(convV,convV )=max sup P(H(x))− sup P(H(x)) , inf P(H(x))− inf P(H(x))
m (cid:26)(cid:12) (cid:12)P∈Pm P∈P (cid:12)
(cid:12)
(cid:12) (cid:12)P∈Pm P∈P (cid:12) (cid:12)(cid:27)
(cid:12) (cid:12) (cid:12) (cid:12)
Basedonthedefinitionandproperty(cid:12)ofHausdorffdistance[10]wehav(cid:12)e (cid:12) (cid:12)
H(convV,convV )≤H(V,V ),
m m
resultingin[9]
|v (x)−v(x)|≤H(V,V )≤H(P,P ).
m m m
Inthissetting,xcanbeanypointinD,andtherighthandsideoftheinequalityisindependentofx. Wecancomplete
theproofbytakingthesupremumofeachsidewithrespecttox[9].
4 Numerical examples
Inthissection,weevaluatetheperformanceoftheproposedalgorithmsonvariousexamplesincludingsyntheticand
realdata.Weconductexperimentsondatasetsthathavedifferentscalesofsizesanddimensions.Weintroduceseveral
versionsofQHMCalgorithmsdependingontheselectionofconstraintpointlocationsandprobabilisticapproach.
Ratherthanrandomlylocatingmconstraintpoints,westartwithanemptyconstraintsetanddeterminethelocations
oftheconstraintpointsonebyoneadaptively. Throughoutthisprocess,weemployvariousstrategiesforaddingthe
constraints.Thespecificapproachesareoutlinedasfollows:
1. Constraint-adaptiveapproach: Thisapproachexamineswhethertheconstraintissatisfiedatalocation. The
functionvalueiscalculated,andiftheconstraintisviolatedatthatlocation,thenaconstraintpointisadded.
2. Variance-adaptiveapproach: Wecalculatethepredictionvarianceinthetestset. Weselectconstraintpoints
atthelocationswhereweobservethelargestvariancevalues. Thegoalisbasicallytoreducethevariancein
predictionsandincreasethestability.
7GPwithSoftConstraints APREPRINT
3. Combination of constraint and variance adaption: In this approach, we determine a threshold value (e.g.
0.20)forthevariance,andthealgorithmlocatesconstraintpointstothelocationswherethelargestprediction
variance is observed. Once the variance reduces to the threshold value, the algorithm switch to the first
strategy,inwhichitlocatesconstraintpointswheretheviolationoccurs.
We representtheconstraint-adaptive,hard-constrainedapproachasQHMCadanditssoft-constrainedcounterpartas
QHMCsoftad. Similarly, QHMCvar refersto the methodfocusingon variance, while QHMCsoftvarcorrespondsto
its soft-constrained version. We denote the combination of the two approaches with hard and soft constraints by
QHMCboth and QHMCsoftboth, respectively. For the sake of comparison, we implement the truncated Gaussian
algorithmsusingan HMCsampler(tnHMC)anda QHMCsampler(tnQHMC)forinequality-constrainedexamples,
whileweimplementadditiveGP(additiveGP)algorithmformonotonicity-constrainedexamples.
Forthesyntheticexamples,weevaluatedthetimeandaccuracyperformancesofthealgorithmswhilesimultaneously
changingthedatasetsizeandnoiselevelinthedata. Following[20],asourmetric,wecalculatetherelativel error
2
betweentheposteriormeany∗ andthetruevalueofthetargetfunctionf(x)onasetoftestpointsX ={x(i)}Nt :
t T i=1
Nt [y∗(x(i))−f(x(i))]2
E =v i=1 T T .
u uP N i=t 1f(x T(i))2
t
P
Additionally,inordertohighlighttheadvantageofQHMCoverHMC,weimplementourapproachusingthestandard
HMC sampling procedure. The relative error, posteriorvarianceand executiontime of each version of QHMC and
HMCalgorithmsarepresented.
4.1 InequaltiyConstraints
Inthissection,weprovidetwosyntheticexamplesandtworeal-lifeapplicationexamplestodemonstratetheeffective-
ness of our algorithmson inequalityconstraints. In syntheticexamples, we comparethe performanceour approach
withtruncatedGaussianmethodsfora2-dimensionalanda10-dimensionalproblems.Forthe2-dimensionalexample,
ourprimaryfocusisonenforcingthenon-negativityconstraintswithintheGPmodel.Inthecaseofthe10-dimensional
example,wegeneralizeouranalysistosatisfyadifferentinequalityconstraint. Weevaluatetheperformancesoftrun-
catedGaussian, QHMCandsoft-QHMCmethods. Inthirdexample,weconsiderconservativetransportina steady-
statevelocityfieldinheterogeneousporousmedia. Despitebeingatwo-dimensionalproblem,thenon-homogeneous
structureofthesoluteconcentrationintroducescomplexityandincreasesthelevelofdifficulty. Thelastexampleisa
3-dimensionalheattransferprobleminahallowsphere.
4.1.1 Example1
Weconsiderthefollowing2Dfunctionundernon-negativityconstraints:
f(x)=arctan5x +arctanx ,
1 2
where{x ,x }∈[0,1]2. WetrainourGPmodelviaQHMCover20randomlyselectedlocations.
1 2
Figure 1 presents the relative error values of the algorithms with respect to two parameters: the size of the dataset
andsignal-to-noiseratio(SNR). Itcan be seenthatthe mostaccurateresultswithoutaddinganynoiseare provided
byQHMCbothandtnQHMCalgorithmswitharound10%relativeerror. However,uponintroducingthenoisetothe
dataandincreasingitsmagnitude,weobserveadistinctpattern. TheQHMCmethodsexhibitrelativeerrorvaluesof
approximately15% within the SNR range of 15% to 20%. In contrast, the relative error of the truncated Gaussian
methodsincreasesto25%withinthesamenoiserange. ThispatterndemonstratesthatQHMCmethodscantolerate
noiseandmaintainhigheraccuracyundertheseconditions.
InTable1,thecomparisonbetweenQHMCandHMCalgorithmswithadatasetsizeof200ispresented.Therelative
errorvaluesindicatethatQHMCyieldsapproximately20%moreaccurateresultsthanHMC,anditachievesthiswith
ashorterprocessingtime.Consequently,QHMCdemonstratesbothhigheraccuracyandefficiencycomparedtoHMC.
Further,we comparethe timeperformancesofthealgorithmsin Figure2whichdemonstratesthatQHMCmethods,
especiallytheprobabilisticQHMCapproachescanperformmuchfasterthanthetruncatedGaussianmethods. Inthis
simple2Dexample,thepresenceofnoisedoesnotsignificantlyimpacttherunningtimesoftheQHMCalgorithms.In
contrast,truncatedGaussianalgorithmsareslowerundernoisyenvironmentevenwhenthedatasetsizeissmall. We
can also observein Figure 3 thatthe QHMC algorithms, especially QHMCvar and QHMCboth are the most robust
ones,astheirsmallrelativeerrorcomeswithasmallposteriorvariance.Incontrast,theposteriorvariancevaluesofthe
truncatedGaussianmethodsarehigherthanQHMCposteriorvariancesevenwhenthereisnonoise,andgetshigher
8GPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.26
15 15 15 15
0.24
10 10 10 10
5 5 5 5
0.22
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.2
QHMCboth QHMCsoftboth tnQHMC tnHMC
20 20 20 20
15 15 15 15 0.18
10 10 10 10
0.16
5 5 5 5
0 0 0 0 0.14
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure1:Relativeerrorofthealgorithmswithdifferentdatasizesandsignaltonoiseratios(SNR)forExample1(2D),
inequality.
Table1:ComparisonofQHMCandHMCon2D,inequality.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.10 0.14 46s HMC-ad 0.12 0.17 52s
QHMC-soft-ad 0.11 0.16 39s HMC-soft-ad 0.13 0.19 48s
QHMC-var 0.11 0.12 40s HMC-var 0.13 0.14 46s
QHMC-soft-var 0.12 0.15 34s HMC-soft-var 0.15 0.14 42s
QHMC-both 0.08 0.13 48s HMC-both 0.10 0.14 53s
QHMC-soft-both 0.09 0.13 39s HMC-soft-both 0.12 0.15 44s
along with the relative error (see Figure 1) when the SNR levels increase. Combining all of these experiments, we
canconcludethatQHMCmethodsachievehigheraccuracywithinashortertimeframe.Consequently,thesemethods
provetobemoreefficientandrobustastheycaneffectivelytoleratechangesinparameters. Additionally,itisworth
notingthatweobservedaslightimprovementintheperformanceoftruncatedGaussianalgorithmsbyimplementing
tnQHMC. Based on the numerical results obtained by tnQHMC, it can be concluded that employing tnQHMC not
onlyyieldshigheraccuracybutalsosavessomecomputationaltimecomparedtotnHMC.
4.1.2 Example2
Next,weconsiderthe10DAckleyfunction[7]definedasfollows:
d d
1 1
f(x)=−aexp−bv
ud
x2 i−exp−bv
ud
coscx i+a+exp1,
u Xi=1 u Xi=1
 t   t 
whered = 10,a = 20,b = 0.2andc = 2π. Westudytheperformanceofthealgorithmsonthedomain[−10,10]10
whileenforcingthefunctiontobegreaterthan5.
Figure4illustratesthatQHMCboth,QHMCsoftbothandtruncatedGaussianalgorithmsyieldthelowesterrorwhen
thereisnonoiseinthedata. However,asthenoiselevelincreases,truncatedGaussianmethodsfallbehindallQHMC
approaches. Specifically, both the QHMCboth and QHMCsofthboth algorithms demonstrate the ability to tolerate
noise levels up to 15% with an associated relative error of approximately15%. However, other variantsof QHMC
methodsdisplaygreaternoisetolerancewhendealingwithlargerdatasets. Withfewerthan100datapoints,theerror
ratereachesaround25%,butitdecreasesto15−20%whenthenumberofdatapointsexceeds100.
Figure5illustratesthetimecomparisonofthealgorithms,wherewecanobservethatQHMCmethodsprovidearound
30−35%time efficiencyforthe datasets largerthan a size of 150. Combiningthis time advantagewith the higher
9
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 80
15 15 15 15
70
10 10 10 10
5 5 5 5
60
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 50
QHMCboth QHMCsoftboth tnQHMC tnHMC
20 20 20 20
15 15 15 15 40
10 10 10 10
30
5 5 5 5
0 0 0 0 20
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure2: Executiontimes(inseconds)ofthealgorithmswithdifferentsignaltonoiseratios(SNR)anddatasizesfor
Example1(2D),inequality.
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.26
15 15 15 15
0.24
10 10 10 10
5 5 5 5
0.22
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.2
QHMCboth QHMCsoftboth tnQHMC tnHMC
20 20 20 20
0.18
15 15 15 15
10 10 10 10
0.16
5 5 5 5
0 0 0 0 0.14
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure3: Posteriorvariancesofthealgorithmswithdifferentsignaltonoiseratios(SNR)anddatasizesforExample
1(2D),inequality.
accuracyof QHMCindicatesthatbothsoftand hardconstrainedQHMCalgorithmsoutperformtruncatedGaussian
methodsacrossvariouscriteria. QHMCmethodsoffertheflexibilitytoemployoneof thealgorithmsdependingon
thepriorityoftheexperiments.Forexample,ifspeedistheprimaryconsideration,QHMCsoftvaristhefastestmethod
whilemaintainingagoodlevelofaccuracy. Ifaccuracyisthemostimportantmetric,employingQHMCbothwould
beawiserchoice,asitstillofferssignificanttimesavingscomparedtoothermethods.
Figure6presentsthattheposteriorvariancevaluesoftruncatedGaussianmethodsaresignificantlyhigherthanthatof
theQHMCalgorithms,especiallywhenthenoiselevelsarehigherthan5%. Asexpected,QHMCvarandQHMCsoft-
varalgorithmsofferthelowestvariance,whileQHMCbothandQHMCsoftbothfollowthem.Aclearpatternisshown
in the figure, in whichQHMCapproachescan toleratehighernoise levelsespeciallywhenthe datasetis large. Itis
notablethatourmethoddemonstratesasignificantincreaseinefficiencyasthedimensionincreases.Whencomparing
this10Dexampletothe2Dcase,theexecutiontimesofthetruncatedGaussianmethodsarenotablyimpactedbythe
dimension,evenin the absenceofnoise in the datasets. Althoughtheir relativeerrorlevelscan remainlow without
10
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
Table2:ComparisonofQHMCandHMCon10D,inequality.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.10 0.13 39m17s HMC-ad 0.12 0.15 43m33s
QHMC-soft-ad 0.11 0.14 36m21s HMC-soft-ad 0.13 0.15 41m10s
QHMC-var 0.11 0.11 37m4s HMC-var 0.13 0.12 41m31s
QHMC-soft-var 0.12 0.11 34m23s HMC-soft-var 0.14 0.12 37m42s
QHMC-both 0.09 0.12 40m8s HMC-both 0.10 0.14 44m23s
QHMC-soft-both 0.10 0.12 37m53s HMC-soft-both 0.12 0.14 42m5s
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.3
15 15 15 15 0.28
10 10 10 10
0.26
5 5 5 5
0.24
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.22
QHMCboth QHMCsoftboth tnQHMC tnHMC
20 20 20 20 0.2
15 15 15 15
0.18
10 10 10 10
0.16
5 5 5 5
0.14
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure 4: Relative error of the algorithms with different data sizes and signal to noise ratios (SNR) for Example
2(10D),inequality.
noise,ittakes1.5timeslongerthantheQHMCmethodstoofferthoseaccuracy. Additionally,thisobservationholds
onlyforcaseswherethedataisnoise-free. Assoonasnoiseispresent,theaccuracyoftruncatedGaussianmethods
deteriorates,whereasQHMCmethodscanwithstandthenoiseandyieldgoodresultsinashortertimespan.
4.1.3 Example3: Solutetransportinheterogeneousporousmedia
Followingthe examplein [27], we examineconservativetransportwithin a constantvelocityfield in heterogeneous
porousmedia. LetusdenotethesoluteconcentrationbyC(x,t)(x =(x,y)T),andsupposethatthemeasurementsof
C(x,t)areavailableatvariouslocationsatdifferenttimes. Conservationlawscanbeusedtodescribetheprocesses
offlowandtransport.Specifically,wecandescribetheflowusingDarcyflowequation[27]
∇·(K∇h)=0, x∈D,
 ∂∂ nh =0, y =0ory =L 2,
(8)
h=H
1, x=0,
h=H , x=L ,
2 1

whereh(x,w) isthehydraulichead,D = [0,L ]×[0,L ] isthesimulationdomainwithL = 256andL = 128,
1 2 1 2
H andH areknownboundaryheadvaluesandK(x,w) isthe unknownhydraulicconductivityfield. Thefield is
1 2
representedasastochasticprocess,withthedistributionofvaluesdescribedbyalog-normaldistribution.Specifically,
itisexpressedasK(x,w)=expZ(x,w),whereisasecond-orderstationaryGPwithaknownexponentialcovariance
function,Cov{Z(x),Z(x′)}=σ2 exp(−|x−x′|/l )wherevarianceσ2 =2andcorrelationlengthl =5. Wecan
Z z Z z
11
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20
180
15 15 15 15
160
10 10 10 10
5 5 5 5 140
0 0 0 0 120
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
QHMCboth QHMCsoftboth tnQHMC tnHMC 100
20 20 20 20
80
15 15 15 15
60
10 10 10 10
5 5 5 5 40
0 0 0 0 20
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure5: Executiontimes(inminutes)ofthealgorithmswithdifferentsignaltonoiseratios(SNR)anddatasizesfor
Example2(10D),inequality.
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.26
15 15 15 15
0.24
10 10 10 10
5 5 5 5
0.22
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.2
QHMCboth QHMCsoftboth tnQHMC tnHMC
20 20 20 20
0.18
15 15 15 15
10 10 10 10
0.16
5 5 5 5
0 0 0 0 0.14
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure6: Posteriorvariancesofthealgorithmswithdifferentsignaltonoiseratios(SNR)anddatasizesforExample
2(10D),inequality.
describethesolutetransportbytheadvection-dispersionequation[6,14,27]as
∂C +∇·(vC)=∇· Dw +α||v|| ∇C, xinD,
∂t τ 2
C =Qδ(x−x∗), (cid:0) (cid:1) t=0,
(9)
∂ ∂C
n
=0, y =0ory =L 2orx=L 1,
C =0, x=0.

Inthiscontext,C(x,t;w)representsthesoluteconcentrationdefinedoverD×[0,T]×Ω,vdenotesthefluidvelocity
given by v(x;w) = −K(x;ω)∇h(x,ω)/φ with φ being porosity; D is the diffusioncoefficient, τ stands for the
w
tortuosity,andαisthedispersivitytensor,withdiagonalcomponentsα andα . Inthisstudy,thetransportparam-
L T
etersaredefinedasfollows: φ = 0.317,τ = φ1/3,D = 2.5×10−5,α = 5andα = 0.5. Lastly, thesolute is
w L T
instantaneouslyinjectedatx∗ = (50,64)att = 0withtheintensityQ = 1[27]. InFigure7,thegroundtruthwith
12
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
Table3:ComparisonofQHMCandHMConsolutetransportwithnonnegativity.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.18 0.13 83s HMC-ad 0.20 0.14 89s
QHMC-soft-ad 0.19 0.13 75s HMC-soft-ad 0.22 0.15 83s
QHMC-var 0.20 0.12 80s HMC-var 0.23 0.13 91s
QHMC-soft-var 0.21 0.13 71s HMC-soft-var 0.24 0.14 79s
QHMC-both 0.13 0.12 86s HMC-both 0.15 0.14 97s
QHMC-soft-both 0.14 0.13 74s HMC-soft-both 0.15 0.15 82s
tnQHMC 0.15 0.13 96s tnHMC 0.16 0.16 103s
observationlocationsandconstraintlocationsarepresentedtoprovideaninsightintothestructureofsoluteconcentra-
tion. Table3presentsacomparisonofallversionsofQHMCandHMCmethods,alongwiththetruncatedGaussian
120
0.015
100
80
0.01
60
40
0.005
20
0
50 100 150 200 250
x
Figure7: Observationlocations(blacksquares)andconstraintlocations(blackstars).
algorithms. Similartotheresultsobservedwithsyntheticexamples,theQHMCboth,QHMCsoftboth,andtnQHMC
algorithms demonstrate the most accurate predictions with a relative error of 13−15%. Notably, QHMCsoftboth
emergesasthefastestamongthemethodswhileachievinghigheraccuracy. Forinstance,theerrorvalueobtainedby
QHMCsoftbothis 0.14, whereastnQHMC’s erroris0.15. However,QHMCsoftbothdeliversa 20%time efficiency
gainwithslightlysuperioraccuracy.InFigure8,acomprehensivecomparisonofthealgorithmsispresented.Wecan
observethedecreaseintherelativeerrorvalueswhileweaddtheconstraintsstepbystep,accordingthetheadopted
adaptiveapproach.Initially,theerroris0.5andgraduallydecreasestoapproximately0.13. Furthermore,itisevident
thattheQHMCbothandQHMCsoftbothmethodsconsistentlydeliverthemostaccurateresultsateachstep,whereas
theperformanceoftheQHMCsoftvarmethodisoutperformedbyotherapproaches.
13
yGPwithSoftConstraints APREPRINT
0.6
QHMCad
0.5 QHMCsoftad
QHMCvar
0.4 QHMCsoftvar
QHMCboth
QHMCsoftboth
0.3
0.2
0.1
0 3 6 9 12 15
Number of constraints
Figure8:Thechangeinrelativeerrorwhileaddingconstraints,solutetransport.
4.1.4 Example4: HeatTransferinaHallowSphere
In this 3-dimensionalexample, we consider a heat transfer problem in a hallow sphere. Let B (0) represent a ball
r
centeredat0withradiusr. DefiningthehallowsphereasD =B (0)−B (0),theequationsaregivenas[28]
4 2
∂u(x,t) −∇·(κ∇u(x,t))=0, x∈D,
∂t
κ∂u(x,t) =θ2(π−θ)2φ2(π−φ)2, ifkx| =4andφ≥0,
κ∂u∂ ∂(xn
n,t) =0,
ifkxk2
2
=4andφ<0,
(10)
u(x,t)=0, ifkxk =2.
 2
Inthiscontext,ndenotesthenormalvectorpointingoutward,whileθandφrepresenttheazimuthalandelevationan-
gles,respectively,ofpointswithinthesphere.Wedeterminethepreciseheatconductivityusingκ=1.0+exp(0.05u).
Thequadraticelementswith12,854degreesoffreedomareemployed,andwesety(x)=u(x,10)tosolvethePDEs.
Startingwith6initiallocationsat0onthesurface,6newconstraintlocationsareintroducedbasedontheactivelearn-
ingapproachoftheQHMCversion. InFigure9, we canobservethedecreaseinrelativeerrorwhiletheconstraints
areaddedstepbystep. Inaddition,Figure10showsthegroundtruthandtheGPresultobtainedbyQHMCsoftboth
algorithm,whereweseethatQHMCsoftbothy∗(x)matchesthereferencemodel. Moreover,itsposteriorvarianceis
smallbasedontheresultsshowninTable4. Thetableprovidestheerror,posteriorvarianceandtimeperformances
of QHMC and HMC algorithms, and we can see the advantages of QHMC over HMC in all categories, even with
the truncatedGaussian algorithm. Althoughallof the algorithmscompletethe GP regressionin less than 1 minute,
comparingthetruncatedGaussianmethodwithQHMC-basedalgorithms,weobserve40−60%timeefficiencyalong
withcompatibleaccuracyofQHMCalgorithms. Inadditiontothetimeandaccuracyperformances,itisshownthat
the posterior variance values are smallest with QHMCvar and QHMCboth approaches, followed by tnQHMC and
QHMCadapproaches.UsingHMCsamplinginallmethodsgenerateslargerposteriorvariances.
4.2 MonotonicityConstraints
Inthissection,weprovidetwonumericalexamplestoinvestigatetheeffectivenessofouralgorithmsonmonotonicity
constraints. We enforcethemonotonicityconstraintsinthedirectionofactivevariables. Similartothecomparisons
inprevioussection,weillustratetheadvantagesofQHMCoverHMC,andthencomparetheperformanceofQHMC
algorithmswithadditiveGPapproachintroducedin[16]withrespecttothesamecriteriaasintheprevioussection.
14
rorre
evitaleRGPwithSoftConstraints APREPRINT
Table4:ComparisonofQHMCandHMConheattransferwithnonnegativity.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.04 0.04 34s HMC-ad 0.06 0.07 40s
QHMC-soft-ad 0.05 0.04 30s HMC-soft-ad 0.07 0.07 32s
QHMC-var 0.05 0.02 30s HMC-var 0.09 0.05 27s
QHMC-soft-var 0.06 0.03 26s HMC-soft-var 0.10 0.05 29s
QHMC-both 0.02 0.03 32s HMC-both 0.04 0.05 37s
QHMC-soft-both 0.03 0.03 27s HMC-soft-both 0.05 0.06 35s
tnQHMC 0.04 0.05 51s tnHMC 0.06 0.07 56s
100
QHMCad
QHMCsoftad
QHMCvar
QHMCsoftvar
QHMCboth
QHMCsoftboth
10-1
10-2
1 2 3 4 5 6
Number of constraints
Figure9: Thechangeinrelativeerrorwhileaddingconstraints,heatequation.
4.2.1 Example1
Weconsiderthefollowing5Dfunctionwithmonotonicityconstraints[16]:
2
f(x)=arctan(5x )+arctan(2x )+x +2x2+ .
1 2 3 4 1+exp−10(x − 1)
5 2
Table 5 shows the performances of HMC and QHMC algorithms, where we observe that QHMC achieves higher
accuracywithlowervarianceinashorteramountoftime. ThecomparisonprovesthateachversionofQHMCismore
efficientthanHMCInaddition,Figure12showstherelativeerrorvaluesofQHMCandadditiveGPalgorithmswith
respecttothe changeinSNR anddatasetsize. Based ontheresults, it isclearthatQHMCbothandQHMCsoftboth
provide the most accurate results under every different condition, while the difference is more remarkable for the
cases in which noise is higher. Although QHMCboth and QHMCsoftboth providesthe most accurate results, other
QHMCversionsalsogeneratemoreaccurateresultsthenadditiveGPmethod.Moreover,Figure13showsthatthesoft-
constrainedQHMCapproachesarefasterthanthehard-constrainedQHMC,whilehard-constrainedQHMCversions
arestillfasterthanadditiveGPalgorithm.
4.2.2 Example2
Weconsiderthetargetfunctionusedin[16,3]
d
i
f(x ,x ,...,x )= arctan5 1− x , where d=2.
1 2 d i
(cid:20) d+1(cid:21)
Xi=1
InTable6, weillustrateaccuracyandtimeadvantagesofQHMCoverHMC.ForeachversionofQHMCandHMC,
weseethatusingQHMCsamplinginaspecificversionacceleratestheprocesswhileincreasingtheaccuracy.Overall
15
rorre
evitaleRGPwithSoftConstraints APREPRINT
(a)Heatequationdata,groundtruthy(x). (b)QHMCsoftbothpredictiony∗ (x).
Figure10:ComparisonofthegroundtruthandQHMCsoftbothresult.
(a)Initiallocations (b)ConstraintlocationsaddedbyQHMC.
Figure11:Initiallocationsandadaptivelyaddedconstraintlocations.
Table5: ComparisonofQHMCandHMCon5D,monotonicity.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.11 0.16 2m23s HMC-ad 0.13 0.17 3m14s
QHMC-soft-ad 0.14 0.18 1m57s HMC-soft-ad 0.17 0.20 2m48s
QHMC-var 0.12 0.15 2m13s HMC-var 0.15 0.17 2m58s
QHMC-soft-var 0.15 0.17 1m42s HMC-soft-var 0.18 0.19 2m16s
QHMC-both 0.10 0.13 2m25s HMC-both 0.12 0.15 2m58s
QHMC-soft-both 0.12 0.14 1m55s HMC-soft-both 0.14 0.15 2m39s
16GPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.2
15 15 15 15
0.19
10 10 10 10
0.18
5 5 5 5
0 0 0 0 0.17
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
QHMCboth QHMCsoftboth HMCsoftboth additiveGP
20 20 20 20 0.16
15 15 15 15
0.15
10 10 10 10
0.14
5 5 5 5
0 0 0 0 0.13
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure 12: Relative error of the algorithms with different data sizes and signal to noise ratios (SNR) for Example
1(5D),monotonicity.
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 220
15 15 15 15
200
10 10 10 10
180
5 5 5 5
0 0 0 0 160
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
QHMCboth QHMCsoftboth HMCsoftboth additiveGP 140
20 20 20 20
15 15 15 15 120
10 10 10 10
100
5 5 5 5
80
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure13: Timecomparisonofthealgorithmswithdifferentdatasizesandsignaltonoiseratios(SNR)forExample
1(5D),monotonicity.
comparison shows that among all versions with QHMC and HMC sampling, QHMCboth is the most accurate ap-
proach, while QHMCsoftboth is the fastest and ranked second in accuracy. In this set of experiments, we included
the results of HMCsoftboth in the comparison of QHMC-based methodsand additive GP. Figure 15 and Figure 16
showtherelativeerrorandtimeperformancesofQHMC-basedalgorithms,HMCsoftbothandadditiveGPalgorithm,
respectively.Inthislastexamplewiththehighestdimension,weobservethesamephenomenaasthepreviousresults,
inwhichsoft-constrainedversionsaremoreefficient,whilehard-constrainedQHMCapproachesarestillfasterthan
additiveGP undervariousconditionssuch as high noise. Dependingon Figure15 and Figure 17, we can state that
QHMCboth can tolerate noise levels up to 10% with the smallest error and posterior variance, and it can still pro-
vide goodaccuracy (erroris around0.15) even when the SNR is higherthan 10%. It is also worth to mentionthat
althoughthe errorvaluesgeneratedbyHMCsoftbothandadditiveGPareprettyclose, HMCsoftbothperformsfaster
thanadditiveGP,especiallywhenthedatasetislargerandnoiselevelishigher.
17
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.26
15 15 15 15
0.24
10 10 10 10
5 5 5 5
0.22
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.2
QHMCboth QHMCsoftboth HMCsoftboth additiveGP
20 20 20 20
0.18
15 15 15 15
10 10 10 10
0.16
5 5 5 5
0 0 0 0 0.14
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure14:Posteriorvariancesofthealgorithmswithdifferentdatasizesandsignaltonoiseratios(SNR)forExample
1(5D),monotonicity.
Table6: ComparisonofQHMCandHMCon20D,monotonicity.
Method Error PosteriorVar Time Method Error PosteriorVar Time
QHMC-ad 0.13 0.18 33m1s HMC-ad 0.15 0.21 35m38s
QHMC-soft-ad 0.15 0.19 31m21s HMC-soft-ad 0.18 0.22 33m41s
QHMC-var 0.14 0.16 32m53s HMC-var 0.17 0.17 34m21s
QHMC-soft-var 0.16 0.17 29m42s HMC-soft-var 0.19 0.18 31m17s
QHMC-both 0.11 0.14 33m45s HMC-both 0.14 0.16 36m21s
QHMC-soft-both 0.12 0.15 29m48s HMC-soft-both 0.15 0.17 33m11s
5 Conclusionand Discussion
Leveraging the accuracy of QHMC training and the efficiency of probabilistic approach, we introduced a soft-
constrainedQHMCalgorithmtoenforceinequalityandmonotonicityconstraintsontheGP.Theproposedalgorithm
reducesthedifferencebetweengroundtruthandtheposteriormeanintheresultingGPmodel,whileincreasingtheef-
ficiencybyattainingtheaccurateresultsinashortamountoftime. TofurtherenhancetheperformanceoftheQHMC
algorithmsacrossvariousscenarios,we haveimplementedmodifiedversionsadoptingadaptivelearning. Thesever-
sionsprovideflexibilityinselectingthemostsuitablealgorithmbasedonthespecificprioritiesofagivenproblem.
WeprovidedtheconvergenceofQHMCbyshowingthatitssteady-statedistributionapproachthetrueposteriorden-
sity, andtheoreticallyjustifiedthattheprobabilisticapproachpreservesconvergence. Finally,we haveimplemented
our methodsto solve severaltypes of optimizationproblems. In each experiment, we initially outlined the benefits
ofQHMCsamplingincomparisontoHMCsampling. Theseadvantagesremainedconsistentacrossallcases,result-
ing in approximatelya 20%time-savingand 15%higheraccuracy. Havingdemonstratedthe advantagesof QHMC
sampling, we proceed to evaluate the performanceof the algorithms across variousscenarios. Our examples cover
higher-dimensionalproblemsfeaturingbothinequalityandmonotonicityconstraints.Furthermore,ourevaluationsin-
cludereal-worldapplicationswhereinjectingphysicalpropertiesisessential,particularlyincasesinvolvinginequality
constraints.
In the context of inequality-constrainedGaussian processes (GPs), we explored 2-dimensionaland 10-dimensional
syntheticproblems,alongwithtworealapplicationsinvolving2-dimensionaland3-dimensionaldata. Forsynthetic
examples, we observethe relative error, posteriorvariance and executiontime of the algorithmswhile graduallyin-
creasingthenoiselevelanddatasetsize.Overall,QHMC-basedalgorithmsoutperformedthetruncatedGaussianmeth-
ods. AlthoughthetruncatedGaussianmethodsprovidehighaccuracyintheabsenceofnoiseandarecompatiblewith
QHMCapproaches,theirrelativeerrorandposteriorvariancesincreaseasthenoiseappearedandincreased.Moreover,
18
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.3
15 15 15 15 0.28
10 10 10 10
0.26
5 5 5 5
0.24
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
0.22
Dataset Size Dataset Size Dataset Size Dataset Size
QHMCboth QHMCsoftboth HMCsoftboth additiveGP
20 20 20 20 0.2
15 15 15 15
0.18
10 10 10 10
0.16
5 5 5 5
0.14
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure 15: Relative error of the algorithms with different data sizes and signal to noise ratios (SNR) for Example
2(20D),monotonicity.
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 110
15 15 15 15
100
10 10 10 10
90
5 5 5 5
80
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 70
QHMCboth QHMCsoftboth HMCsoftboth additiveGP
20 20 20 20
60
15 15 15 15
50
10 10 10 10
5 5 5 5 40
0 0 0 0 30
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure16: Timecomparisonofthealgorithmswithdifferentdatasizesandsignaltonoiseratios(SNR)forExample
2(20D),monotonicity.
theadvantagesofsoft-constrainedQHMCbecamemoreevident,particularlyinhigher-dimensionalcases,whencom-
paredto truncatedGaussianandevenhard-constrainedQHMC.The timecomparisonofthealgorithmsunderscores
thatthetruncatedGaussianmethodsaresignificantlyimpactedbythecurseofdimensionalityandlargedatasets,ex-
hibitingslowerperformanceundertheseconditions. Inreal-worldapplicationscenariosfeaturing2-dimensionaland
3-dimensionaldata,thefindingswereconsistentwiththoseobservedinthesyntheticexamples.Althoughtheaccuracy
levelmaynotreachthehighestlevelsobservedin thesyntheticexamplesand3-dimensionalheatequationproblem,
the observed trend remains consistent. The lower accuracy observed in the latter problem can be attributed to the
non-homogeneousstructureofsoluteconcentration.
Inthecaseofmonotonicity-constrainedGP,weaddressed5-dimensionaland20-dimensionalexamples,utilizingthe
sameconfigurationasemployedforinequality-constrainedGP.Acomprehensivecomparisonwasconductedbetween
all versions of QHMC algorithms and the additive GP method. The results indicate that QHMC-based approaches
holdanotableadvantage,particularlyinscenariosinvolvingnoiseandlargedatasets. WhileadditiveGPprovestobe
19
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
QHMCad QHMCsoftad QHMCvar QHMCsoftvar
20 20 20 20 0.26
15 15 15 15
0.24
10 10 10 10
5 5 5 5
0.22
0 0 0 0
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size 0.2
QHMCboth QHMCsoftboth HMCsoftboth additiveGP
20 20 20 20
0.18
15 15 15 15
10 10 10 10
0.16
5 5 5 5
0 0 0 0 0.14
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Dataset Size Dataset Size Dataset Size Dataset Size
Figure17:Posteriorvariancesofthealgorithmswithdifferentdatasizesandsignaltonoiseratios(SNR)forExample
2(20D),monotonicity.
astrongmethodsuitableforhigh-dimensionalcases,QHMCalgorithmsperformedfasterandyieldedlowervariances.
Inconclusion,theworkhasdemonstratedthatsoft-constrainedQHMCisarobust,efficientandflexiblemethodthat
canbeapplicabletohigherdimensionalcasesandlargedatasets. Numericalresultshaveshownthatsoft-constrained
QHMCispromisingtobegeneralizedtovariousapplicationswithdifferentphysicalproperties.
References
[1] PetterAbrahamsenandFredEspenBenth. Krigingwithinequalityconstraints. MathematicalGeology,33:719–
744,2001.
[2] Christian Agrell. Gaussian processes with linear operator inequality constraints. arXiv preprint
arXiv:1901.03134,2019.
[3] François Bachoc, Andrés F López-Lopera, and Olivier Roustant. Sequential construction and dimension re-
duction of gaussian processes under inequality constraints. SIAM Journal on Mathematics of Data Science,
4(2):772–800,2022.
[4] AdrianBarbuandSong-ChunZhu. MonteCarloMethods,volume35. Springer,2020.
[5] SébastienDaVeigaandAmandineMarrel. Gaussianprocessmodelingwithinequalityconstraints. InAnnales
delaFacultédessciencesdeToulouse: Mathématiques,volume21,pages529–555,2012.
[6] SimonEmmanuelandBrian Berkowitz. Mixing-inducedprecipitationandporosityevolutionin porousmedia.
Advancesinwaterresources,28(4):337–344,2005.
[7] DavidErikssonandMatthiasPoloczek.ScalableconstrainedBayesianoptimization.InInternationalConference
onArtificialIntelligenceandStatistics,pages730–738.PMLR,2021.
[8] AndrewGelman,JohnB.Carlin,HalS.Stern,DavidB.Dunson,AkiVehtari,andDonaldB.Rubin. Bayesian
DataAnalysis. Tyler&FrancisGroup,Inc.,2014.
[9] ShaoyanGuo, HuifuXu, and LiweiZhang. Stability analysisformathematicalprogramswith distributionally
robustchanceconstraint. SIAMJ.Optim(toappear),2015.
[10] ChristianHess. Conditionalexpectationandmartingalesofrandomsets. PatternRecognition,32(9):1543–1567,
1999.
[11] BjørnSandJensen,JensBrehmNielsen,andJanLarsen. BoundedGaussianprocessregression. In2013IEEE
InternationalWorkshoponMachineLearningforSignalProcessing(MLSP),pages1–6.IEEE,2013.
[12] MalteKussandCarlRasmussen. Gaussianprocessesinreinforcementlearning. Advancesinneuralinformation
processingsystems,16,2003.
20
RNS
RNS
RNS
RNS
RNS
RNS
RNS
RNSGPwithSoftConstraints APREPRINT
[13] MarkusLange-Hegermann.LinearlyconstrainedGaussianprocesseswithboundaryconditions.InInternational
ConferenceonArtificialIntelligenceandStatistics,pages1090–1098.PMLR,2021.
[14] GuangLin and AlexandreM Tartakovsky. An efficient, high-orderprobabilisticcollocationmethodon sparse
grids for three-dimensionalflow and solute transport in randomly heterogeneousporous media. Advances in
WaterResources,32(5):712–722,2009.
[15] ZimingLiuandZhengZhang. Quantum-inspiredHamiltonianMonteCarloforBayesiansampling,2020.
[16] Andrés López-Lopera,François Bachoc, and Olivier Roustant. High-dimensionaladditive gaussian processes
undermonotonicityconstraints. AdvancesinNeuralInformationProcessingSystems,35:8041–8053,2022.
[17] AndrésFLópez-Lopera,FrançoisBachoc,NicolasDurrande,andOlivierRoustant.Finite-dimensionalGaussian
approximationwithlinearinequalityconstraints. SIAM/ASAJournalonUncertaintyQuantification,6(3):1224–
1255,2018.
[18] Hassan Maatouk and Xavier Bay. Gaussian process emulators for computer experimentswith inequality con-
straints. MathematicalGeosciences,49:557–582,2017.
[19] HassanMaatouk,OlivierRoustant,andYannRichet. Cross-validationestimationsofhyper-parametersofGaus-
sianprocesseswithinequalityconstraints. ProcediaEnvironmentalSciences,27:38–44,2015.
[20] AndrewPensoneault,XiuYang,andXueyuZhu. Nonnegativity-enforcedGaussianprocessregression. Theoret-
icalandAppliedMechanicsLetters,10(3):182–187,2020.
[21] MaziarRaissi, ParisPerdikaris,andGeorgeEmKarniadakis. Machinelearningoflineardifferentialequations
usinggaussianprocesses. JournalofComputationalPhysics,348:683–693,2017.
[22] CarlEdwardRasmussen, ChristopherKI Williams, etal. Gaussianprocesses formachinelearning, volume1.
Springer,2006.
[23] Jaakko Riihimäkiand Aki Vehtari. Gaussian processeswith monotonicityinformation. In Proceedingsof the
thirteenth international conference on artificial intelligence andƒ statistics, pages 645–652. JMLR Workshop
andConferenceProceedings,2010.
[24] MathieuSalzmannandRaquelUrtasun. ImplicitlyconstrainedGaussianprocessregressionformonocularnon-
rigidposeestimation. Advancesinneuralinformationprocessingsystems,23,2010.
[25] MichaelLStein. Asymptoticallyefficientpredictionofarandomfieldwithamisspecifiedcovariancefunction.
TheAnnalsofStatistics,16(1):55–63,1988.
[26] LauraPSwiler,MamikonGulian,AriLFrankel,CosminSafta,andJohnDJakeman. Asurveyofconstrained
Gaussian process regression: Approaches and implementation challenges. Journal of Machine Learning for
ModelingandComputing,1(2),2020.
[27] XiuYang,DavidBarajas-Solano,GuzelTartakovsky,andAlexandreMTartakovsky. Physics-informedcokrig-
ing: AGaussian-process-regression-basedmultifidelitymethodfordata-modelconvergence. JournalofCompu-
tationalPhysics,395:410–431,2019.
[28] XiuYang,GuzelTartakovsky,andAlexandreMTartakovsky.Physicsinformationaidedkrigingusingstochastic
simulationmodels. SIAMJournalonScientificComputing,43(6):A3862–A3891,2021.
[29] HaoZhang.Inconsistentestimationandasymptoticallyequalinterpolationsinmodel-basedgeostatistics.Journal
oftheAmericanStatisticalAssociation,99(465):250–261,2004.
21