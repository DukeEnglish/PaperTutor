On the Scalability of Diffusion-based Text-to-Image Generation
HaoLi1,2,YangZou1,2,YingWang1,2,OrchidMajumder1,2,YushengXie1,2,R.Manmatha1,
AshwinSwaminathan1,2,ZhuowenTu1,StefanoErmon1,StefanoSoatto1
1AWSAILabs,2AmazonAGI
{haolimax, yanzo, lyiwang, orchid, yushx, manmatha, swashwin, ztu, ermons, soattos}@amazon.com
Abstract
LensArt, 256, SDXL-AE, CFG 7.5, BS=2048
Scalingupmodelanddatasizehasbeenquitesuccessful
0.84
for the evolution of LLMs. However, the scaling law for
thediffusionbasedtext-to-image(T2I)modelsisnotfully
0.82
explored. Itisalsounclearhowtoefficientlyscalethemodel
forbetterperformanceatreducedcost. Thedifferenttrain- 0.80
ingsettingsandexpensivetrainingcostmakeafairmodel
comparisonextremelydifficult. Inthiswork,weempirically 0.78
studythescalingpropertiesofdiffusionbasedT2Imodelsby
performingextensiveandrigoursablationsonscalingboth 0.76 SD2-UNet (0.9B) | LensArt (250M)
IF-XL-C512 (2.0B) | LensArt (250M)
denoising backbones and training set, including training
SDXL-UNet (2.4B) | LensArt (250M)
scaledUNetandTransformervariantsrangingfrom0.4Bto 0.74 SDXL-UNet (2.4B) | LensArt + SSTK (600M)
4Bparametersondatasetsupto600Mimages. Formodel 100000 200000 300000 400000 500000 600000
scaling,wefindthelocationandamountofcrossattention Training Steps
distinguishestheperformanceofexistingUNetdesigns. And
Figure1.PushingtheParetofrontierofthetext-imagealignment
increasingthetransformerblocksismoreparameter-efficient
learningcurvebyefficientlyscalingupbothdenoisingbackbones
forimprovingtext-imagealignmentthanincreasingchannel andtrainingdata.ComparingwiththebaselineSD2UNet[34],the
numbers. WethenidentifyanefficientUNetvariant,whichis combinedscalingwithbothSDXLUNetandenlargeddatasetsig-
45%smallerand28%fasterthanSDXL’sUNet. Onthedata nificantlyincreasestheperformanceandspeedsuptheconvergence
scalingside,weshowthequalityanddiversityofthetrain- ofTIFAscoreby6×.
ing set matters more than simply dataset size. Increasing
captiondensityanddiversityimprovestext-imagealignment
performanceandthelearningefficiency. Finally,weprovide
how does the design of denoising backbone influence the
scalingfunctionstopredictthetext-imagealignmentperfor-
imagegenerationandwhichcomponentsaremoreeffective
manceasfunctionsofthescaleofmodelsize,computeand
toscale? Howshoulddiffusionmodelscalewhenthetrain-
datasetsize.
ingdataincreases? Toanswerthequestions,itisessential
tounderstandhowexactlyeachnewmodelimprovesover
previousones. However,existingdiffusionbasedT2Imod-
1.Introduction
els are mostly trained with different datasets, input space
(latentspaceorpixelspace)andtrainingsettings. Moreover,
Scalingupmodelanddatasetsizehasbeenthekeyenabling
theexpensivetrainingcostofhighresolutionmodelsmakes
factorforthesuccessofLLMs[17,21]andVLMs[6,32].
thefaircomparisonextremelyhard,nottomentionexplor-
The scaling law [4, 21] governs the expectation of perfor-
ingnewones. Therefore,afairandcontrolledcomparison
mance as a function of dataset, model size and compute
ofdifferentdenoisingbackbonesisgreatlydesired,which
budget. However,thescalingpropertiesforrecentdiffusion
canenableseekingofmoreefficientmodelswithreduced
basedText-to-Image(T2I)models[31,33–35]arenotwell
trainingandinferencecost.
studied. ThoughthereisemergingtrendthatT2Imodelscan
be improved with larger denoising backbones [9, 31] and In this paper, we investigate the scaling properties for
stronger text-encoders [1, 31, 35], it is still not clear how trainingdiffusionmodels,especiallyonthedenoisingback-
toeffectivelyandefficientlyscaleupdiffusionmodels,e.g., boneanddataset.Thegoalistounderstandwhichdimension
1
4202
rpA
3
]VC.sc[
1v38820.4042:viXra
erocS
AFITofthemodelismoreeffectiveandefficienttoscale,howto formerbackboneimprovesperformance,butalsoidentify
properlyscalethedataset,andthescalinglawamongmod- thedifficultyoftrainingfromscratchduetolackofinduc-
els,datasetandcompute. Fig.1givesanillustrationofhow tivebiasincomparisonwithUNets.
theParetofrontierofthetext-imagealignmentperformance • Weshowthatproperlyscalingtrainingdatawithsynthetic
curvecanbepushedviaproperscaling. captionsimprovesimagequalityandspeedsupthecon-
vergence. Weseedatascalingcanimprovesmallmodel’s
1.1.Whatwehavedone
performance significantly, a better designed model can
haveahigherperformanceupperbound.
• ComparingexistingUNetsinacontrolledenvironment:
we first compare existing UNet designs from SD2 [34],
2.RelatedWork
DeepFloyd [9] and SDXL [31], to understand why cer-
tain UNet design is significantly better than others. To
DiffusionModels Diffusionmodels[15,16,28,29,36]
allowafaircomparison,wetrainallmodelswiththesame
synthesize samples via an iterative denoising process and
dataset, latentspace, textencoder andtrainingsettings.
have shown superior performance over GAN [13] based
Wemonitormultipleevaluationmetricsduringtraining,in-
methodsforimagegeneration[10]. Recentdiffusionbased
cludingcompositionscoresandimagequalityscores. We
T2I models such as Imagen [35], LDM/SD2 [34], Deep-
verifiedSDXL’sUNetachievessuperiorperformanceover
Floyd [9], SDXL [31], and DALL·E [3, 33] have shown
otherswithsimilaramountofparameters,whichjustifies
consistentlyimprovedperformanceintermsofsamplediver-
theimportanceofarchitecturedesign.
sity,text-imagealignmentandimagefidelity. Pixel-based
• ScalingUNetandcomparingwithTransformers: To
models[9,33,35]usuallyrequirecascadedsuper-resolution
understand why SDXL works so well, we conduct ex-
(SR)modelstoupscaleimagesgeneratedinlowresolution,
tensiveablationstudiesonthedesignsapceofUNetby
whileLDMs[3,31,34]reducetrainingcostbyutilizinga
investigating 15 variations ranging from 0.4B to 4B pa-
compressedlatentspaceandupsamplingwithviaanautoen-
rameters, especially on the choice of channel numbers
coder[22].Thelowresolutionlatentspacemaynotrepresent
andtransformerdepth. Weshowhoweacharchitecture
smallobjects(e.g.,faces)well. SDXLmitigatesthisissue
hyperparameteraffectstheperformanceandconvergence
viaabetterVAEandtrainingmodelsinhigherresolution
speed. Similarly, we ablate and scale the Transformer
latentspace(128×128). Emu[8]showsthatincreasingthe
backbones[5,30]andcomparewithUNet.
latentchannelsimprovesimagequality.
• Ablatingtheeffectofdatasetscalingandcaptionen-
hancement: We study how different dataset properties
affectthetrainingperformance,includingdatasetsize,im- ScalingUNets UNetarchitecturewasfirstintroducedfor
agequalityandcaptionquality. Wecuratetwolarge-scale diffusionmodelsin[16]. [10,28]ablatedUNetwithseveral
datasetswith250Mand350Mimages,bothareaugmented designchoicesandinvestigatedhowFIDscalesasafunction
bysyntheticcaptions. Wetrainbothsmallandlargemod- of training compute. The UNet in LDM or SD2 [34] has
elstoseehowtheycanbenefitfromdatasetscaling. 320initialchannelsand850Mparameters. DeepFloyd[9]
trainsapixelbasedmodelwithaUNetof4Bparametersize
1.2.Contributions and704channels,whichshowsbetterperformancethanits
smallerversions.SDXL[31]employsa3×largerUNetthan
• We conduct large-scale controlled experiments to allow
SD2withmutipleimprovements. Ontheotherhand,there
faircomparisonacrossvariousdenoisingbackbonesfor
arealsoworksonimprovingUNet’sefficiencybyscalingit
T2I synthesis, including both UNets and Transformers.
down,e.g.,SnapFusion[25]studiestheredundancyofUNet
Ourworkverifiestheimportanceofthedenoisingback-
andidentifiesanefficientversionbyemployingthechangeof
bonedesign. Wefindcompositionabilityismainlydevel-
CLIP/Latencytomeasuretheimpactofarchitecturechange.
opedatlowresolution,whichenablesfastmodelablations
withouttraininginhighresolution. Toourbestknowledge,
ourworkisthefirstlarge-scalecontrolledstudyallowing Transformer Backbones Recently there is surge inter-
faircomparisonacrossdifferentdenoisingbackbonesfor est in using Transformer [38] to replace UNet for its gen-
T2Isyntheis. eral architecture design and increased scalability [2, 30,
• WeablatethekeydesignfactorsforUNetandTransform- 42]. DiT [30] replaces UNet with Transformers for class-
ersandcomparedtheirscaledversions. Weshowscalling conditionedimagegenerationandfindthereisastrongcor-
thetransformerdepthinUNetismoreparameterefficient relationbetweenthenetworkcomplexityandsamplequality.
inimprovingthealignmentperformanceincomparison U-ViT[2]showscomparableperformancecanbeachieved
withchannelnumber. WeidentifyanefficientUNetvari- byViTswithlongskipconnection. MDT[12]introducesa
antthatis45%smallerand28%fasterthanSDXLwhile masklatentmodelingschemetoimprovethetrainingeffi-
achievingsimilarperformance. Weconfirmscalingtrans- ciencyoftransformer-baseddiffusionmodels. Thoseworks
24C 4 4C C 4
𝑧! 𝐷0 𝐷0 𝐷0 𝐷0 𝑧!"# 𝑧! 𝑧!"#
2C 2C 2C 2C 2C
𝐷1 𝐷1 𝐷1 𝐷1 𝐷1 𝐷1 𝐷1 𝐷1
4C 4C
𝐷2 𝐷2 𝐷2 𝐷2 𝐷2 𝐷2 𝐷2 𝐷2
4C 𝐷3 𝐷3 𝐷24C
𝐷3
𝐷0, 𝐷1,𝐷2, 𝐷3 = 1, 1, 1, 1 𝐷0, 𝐷1,𝐷2= 0, 2, 10
3x3 conv Residual Block Down/Up Sampling 𝐷 𝐷 Transformer Blocks Identity Mapping 𝐶Channel number
with Cross Attention
Figure2.ComparisonoftheUNetdesignbetweenSD2(left)andSDXL(right).SD2appliescross-attentionatalldown-samplinglevels,
including1×,2×,4×and8×,whileSDXLadoptscross-attentiononlyat2×and4×down-samplinglevels.
aremostlyclassconditionedmodelsandonlytheeffectof settings. Belowweintroducethetrainingconfigurationsand
modelarchitectureonimagefidelityisstudied. PixArt-α[5] evaluationmetrics,basedonwhichwecomparealldifferent
extends DiTs [30] for text-conditioned image generation. backbonesvariants.
Morerecently,SD3[11]proposeMM-DiTdesignandfind
itscaleswell.
Training WetrainmodelsonourcurateddatasetLensArt,
whichcontains250Mtext-imagepairs(detailsinSec4). We
3.ScalingDenoisingBackbone
use SDXL’s VAE and the OpenCLIP-H [20] text encoder
(1024dim),withoutaddingextraembeddinglayerorother
3.1.ExistingUNetDesign
conditioning. We train all models at 256×256 resolution
The UNet in diffusion models adopts a stack of residual withbatchsize2048upto600Ksteps. Wefollowthesetup
blocks and a sequence of downsampling and upsampling ofLDM[34]forDDPMschedules. WeuseAdamW[27]
convolutions, along with additional spatial attention lay- optimizerwith10Kstepswarmupandthenconstantlearning
ers at multiple resolutions [10, 16]. Recent T2I frame- rate8e-5. WeemploymixedprecisiontrainingwithBF16
works [9, 31, 34] mostly employ the ideas in simple dif- andenablesFSDPforlargemodels.
fusion[18]toimprovetheefficiencyofUNet,i.e.,tweaking
more parameters and computation at smaller resolutions.
Inference and Evaluation We use DDIM sampler [37]
Fig.2givesacomparisonoftheUNetsforSD2andSDXL.
in 50 steps with fixed seed and CFG scale (7.5) for infer-
SDXLimprovesoverSD2inmultipledimensions: a)Less
ence. Tounderstandthetrainingdynamics,wemonitorthe
downsamplingrates. SD2uses[1,2,4,4]asthemultipli-
evolution of five metrics during training. We find the the
cationratestoincreasechannelsatdifferentdownsampling
metricsatearlystageoftrainingcanhelppredictfinalmodel
levels. DeepFloyd adopts [1, 2, 3, 4] to reduce computa-
performance. Specifically,wemeasurecompositionability
tion,whileSDXLuses[1,2,4],whichcompletelyremoves
and image quality with metrics including: 1) TIFA [19],
the 4th downsampling level. b) Cross-attention only at
whichmeasuresthefaithfulnessofageneratedimagetoits
lowerresolution. Cross-attentionisonlycomputedatcer-
textinputviavisualquestionanswering(VQA).Itcontains
taindownsamplingrates,e.g.,SD2appliescross-attentionat
4Kcollectedpromptsandcorrespondingquestion-answer
firstthreedownsamplingrates(1×,2×,4×),whileSDXL
pairsgeneratedbyalanguagemodel. Imagefaithfulnessis
onlyintegratestextembeddingatthe2×and4×downsam-
calculatedbycheckingwhetherexistingVQAmodelscan
plinglevels. c)Morecomputeatlowerresolution. SDXL
answer these questions using the generated image. TIFA
appliesmoretransformerblocksatthe2×and4×downsam-
allowsforfine-grainedandinterpretableevaluationsofgen-
plinglevels,whileSD2appliesuniformsingletransformer
eratedimages. 2)ImageReward[40]whichwaslearnedto
blockatallthreedownsamplinglevels.
approximateshumanpreference. Wecalculatetheaverage
ImageRewardscoreoverimagesgeneratedwithMSCOCO-
3.2.ControlledComparisonofUNets
10K prompts. Though ImageReward is not a normalized
To allow fair comparison of different UNets, we train all score,itsscoresareintherangeof[-2,2]andtheaverage
backbonevariantsinthesamecontrolledsettings,including score over the number of images gives meaningful statis-
the same dataset, latent space, text-encoder and training tics to allow comparision across models. Due to space
3
46x46
gniddebme
txeT
23x23
61x61
8x8
8x8
46x46
gniddebme
txeT
23x23
61x61
61x61LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 3.3.AblationofUNetDesign
0.84 0.84
NowwehaveverifiedSDXLhasamuchbetterUNetdesign
0.82 0.82
thanSD2andDeepFloydvariants. Thequestioniswhyit
0.80 0.80
excels and how to further improve it effectively and effi-
0.78 0.78
ciently. HereweinvestigatehowtoimproveSDXL’sUNet
0.76 S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) ) 0.76 S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) )
SD2-UNet-C512 (2.2B) SD2-UNet-C512 (2.2B) byexploringitsdesignspace.
0.74 IF-XL-UNet-C512 (2.0B) 0.74 IF-XL-UNet-C512 (2.0B)
100000 200000 300000 400000 500000 600000 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Training Steps Total GFLOPs 1e12
Search Space Table 1 shows different UNet configura-
Figure3.TheevolutionofTIFAscoreduringtrainingwithdifferent
tions,andtheircorrespondingcomputecomplexityat256
UNetsonthesamedatasetintermsoftrainingstepsandtraining
resolution. We mainly vary the initial channels and trans-
compute(GFLOPs). ThecomputeFLOPsisestimatedwith3×
formerdepth. Tounderstandtheimpactofeachdimension
FLOPsofsingleDDPMstep×batchsize×steps.
ofthedesignspace,weselectasubsetofthevariantmodels
constraints, wemainlyshowTIFAandImageRewardand and train them with the same configurations. This forms
provideresultsofothermetrics(CLIPscore[14,32],FID, our main “search space” for the UNet architecture. More
HPSv2[39])inAppendix. ablationsontheimpactofVAE,trainingiterationsandbatch
sizecanbefoundinAppendix.
SDXLvsSD2vsIF-XL Wecomparethedesignofseveral
existingUNetmodelsfromSDXL[31],DeepFloyd-IF[9], The Effect of Initial Channels We train the following
SD2[34]anditsscaledversionintheabovecontrolledset- SDXLUNetvariantswithdifferentchannelnumbers: 128,
tings. Specifically, we compare a) SD2 UNet (0.9B) b) 192,and384,withparameters0.4B,0.9Band3.4B,respec-
SD2UNetwith512initialchannels(2.2B)c)SDXL’sUNet tively. Fig. 4 (a) shows that UNet with reduced channels
(2.4B)d)DeepFloyd’sIF-XLUNetwith512channels(2.0B). from320to128stillcanoutperformSD2’sUNetwith320
Fig.3showsthatthenaivelyscaledSD2-UNet(C512,2.2B) channels,whichshowsthatlesschannelnumbercanachieve
achievesbetterTIFAscorethanthebaseSD2modelatthe betterqualitywithproperarchitecturedesign. However,the
same training steps. However, the convergence speed is TIFA(alsoImageReward/CLIP)scoresareworseincom-
slowerintermsoftrainingFLOPs,whichindicatesincreas- parisonwithSDXL’sUNet,whichindicatesitsimportance
ing channels is an effective but not an efficient approach. in visual quality. Increasing channel number from 320 to
SDXL’sUNetachieves0.82TIFAwithin150Ksteps,which 384 boosts the number of parameters from 2.4B to 3.4B.
is6×fasterthanSD2UNetand3×fasterthanSD2-C512 It also achieves better metrics than baseline 320 channels
in training iterations. Though its training iteration speed at 600K training steps. Note that the initial channel num-
(FLOPS)is2×slowerthanSD2,itstillachievesthesame berC actuallyconnectswithotherhyperparametersofthe
TIFAscoreat2×reducedtrainingcost. SDXLUNetalso UNet, e.g., 1) the dimension of timestep embedding T is
cangetamuchhigherTIFAscore(0.84)withaclearmar- 4C;2)thenumberofattentionheadislinearwithchannel
ginoverothermodels. ThereforeSDXL’sUNetdesignis numbers, i.e. C/4. As shown in Table 1, the proportion
significantlybetterthanothersintermsofperformanceand of compute for attention layers are stable (64%) when C
trainingefficiency,pushingtheParetofrontier. changes. ThisexplainswhyincreasingthewidthofUNet
Table1.ComparingUNetvariantsintermsoftheirhyperparameter,numberofparameters,andinferencecomplexity(GMACs).Wealsolist
theportionofcomputeallocatedforattentionoperations.Theoriginalarchitecturehyperparametersaremarkedinbold.
GMACs
UNet Channels ChannelMult. Res.Blocks Atten.Res. Tran.Depth Params(B)
Total Atten. Atten.%
320 0.87 86 34 39
SD2[34] [1,2,4,4] 2 [4,2,1] [1,1,1]
512 2.19 219 85 39
512 2.04 194 23 12
IF-XL[9] [1,2,3,4] 3 [4,2,1] [1,1,1]
704 3.83 364 42 12
128 0.42 35 23 65
192 0.90 75 48 65
[1,2,4] 2 [4,2] [0,2,10]
320 2.39 198 127 64
384 3.40 282 179 64
[0,2,2] 0.85 98 43 44
[0,2,4] 1.24 123 64 52
SDXL[31]
[0,2,12] 2.78 223 147 66
320 [1,2,4] 2 [4,2] [0,2,14] 3.16 248 168 68
[0,4,4] 1.32 143 84 59
[0,4,8] 2.09 193 123 64
[0,4,12] 2.86 243 167 69
384 [1,2,4] 2 [4,2] [0,4,12] 4.07 346 237 69
4
erocS
AFIT
erocS
AFITLensArt, 256, SDXL-AE, CFG 7.5, BS=2048 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048
0.84 0.84 0.84
0.82 0.82 0.82
0.80 0.80 0.80
SD2-UNet (0.9B)
SDXL-UNet (2.4B)
0.78 0.78 SDXL-UNet-TD2 (0.9B) 0.78
SDXL-UNet-TD4 (1.2B) SD2-UNet (0.9B)
SD2-UNet (0.9B) SDXL-UNet-TD12 (2.8B) SDXL-UNet (2.4B)
0.76 SDXL-UNet (2.4B) 0.76 SDXL-UNet-TD14 (3.2B) 0.76 SDXL-UNet-TD4_4 (1.2B)
SDXL-UNet-C128 (0.4B) SDXL-UNet-TD4_4 (1.2B) SDXL-UNet-TD4_12 (2.9B)
SDXL-UNet-C192 (0.9B) SDXL-UNet-TD4_8 (2.1B) SDXL-UNet-C384 (3.4B)
0.74 SDXL-UNet-C384 (3.4B) 0.74 SDXL-UNet-TD4_12 (2.9B) 0.74 SDXL-UNet-C384-TD4_12 (4.1B)
100000 200000 300000 400000 500000 600000 100000 200000 300000 400000 500000 600000 100000 200000 300000 400000 500000 600000
Training Steps Training Steps Training Steps
Figure4.EvolutionofTIFAscoreduringtrainingwithscaledUNetvariations.ThebaselinemodelsareUNetsofSD2andSDXL.Wetrain
SDXLUNetvariantswithchangesin(a)channelsC(b)transformerdepth(TD)3)bothchannelsandTD.
alsobringsalignmentimprovementasshowninFig.4.
SD2-C320 SDXL-TD2
TheEffectofTransformerDepth Thetransformerdepth SDXL-C128 SDXL-TD4
(TD)settingcontrolsthenumberoftransformerblocksatcer-
SDXL-C196 SDXL-TD4_4
taininputresolution. SDXLapplylies2and10transformer
blocksat2×and4×downsamplinglevel,respectively. To
SDXL-C320 SDXL-TD4_8
understanditseffect,wetrainedthevariantsshowninTable1
withdifferentTDs,rangingfrom0.9Bto3.2Bparameters. SDXL-C384 SDXL-TD2_10
Specifically,wefirstchangetheTDatthe4×downsampling
rate, obtaining TD2, TD4, TD12 and TD14, then we fur- Figure 5. Visualizing the effect of UNet scaling on text-image
therchangethedepthat2×downsmaplingrate,resultingin alignment. WechangetheUNetalongtwodimensions: channel
TD4_4,TD4_8andTD4_12. Notetheportionofattention number(left)andtransformerdepth(right). Thepromptsare: 1)
"squareblueapplesonatreewithcircularyellowleaves"2)"five
operationsalsoincreaseswithTDsaccordingly. Fig.4(b)
frostedglassbottles"3)"ayellowboxtotherightofabluesphere"
showsthatincreasingTDat4×downsamplingratefrom2to
4)"theInternationalSpaceStationflyinginfrontofthemoon"
14continuouslyimprovesTIFAscore. Fromthecomparison
betweenTD4andTD4_4,weseethatincreasingtransformer
depthat2×resolution(2→− 4)alsoimprovesTIFAscore.
VisualizingtheEffectofUNetScaling Fig.5showsthe
TD4_4 has competitive performance in comparison with
imagesgeneratedbydifferentUNetswiththesameprompts.
SDXL’sUNetwhilehaving45%lessparametersand28%
Wecanseethatasthechannelnumberortransformerdepth
lesscomputeforinference. InAppendix,weshowTD4_4
increases,theimagesbecomemorealignedwiththegiven
achievessameTIFAscore1.7×fasterthanSDXLUNetin
prompts(e.g.,color,counting,spatial,object). Someimages
terms of wall-clock training time. TD4_8 has almost the
generatedbycertainUNetvariantarebetterthantheoriginal
sameperformanceasSDXL’sUNetwith13%lessparame-
SDXLUNet(C320),i.e.,SDXL-C384andSDXL-TD4_8
ters. Sincethetext-imagealignment(TIFA)ismostlyabout
bothgeneratemoreaccurateimageswiththe4thprompt.
largeobjectsintheimage,itishelpfultoallocatemorecross
computeatlowerresolutionorglobalimagelevelbeyond 3.4.ComparingwithTransformers
efficiencyconsiderations.
DiT[30]demonstratesthatscalingupthetransformercom-
plexitycangetconsistentlyimprovedimagefidelityforclass-
ScalingbothChannelsandTransformerDepth Given conditionedimagegenerationonImageNet. PixArt-α[5]
theeffectofchannelsandtransformerdepth,wefurtherex- extendsDiTtotext-conditionedimagegenerationwithsim-
ploredenlargingboththechannelnumbers(from320to384) ilarbackbone. However,thereisalackoffaircomparison
andtransformerdepth([0,2,10]→− [0,4,12]). Fig.4(c) withUNetinacontrolledsetting. TocomparewithUNet
shows that it achieves slightly higher TIFA scores during andunderstanditsscalingproperty,wetrainmultiplescaled
training than SDXL-UNet. However, the advantage over versionofPixArt-α,keepingothercomponentsandsettings
simplyincreasingchannelsortransformerdepthisnotap- thesameaspreviousablations. Table2showstheconfigura-
parent,whichmeansthereisaperformancelimitformodels tionofourscaledvariants. Thedifferencewiththeoriginal
tocontinuescaleundermetricslikeTIFA. PixArt-αmodelliesat: 1)weuseSDXL’sVAEinsteadof
5
erocS
AFIT
erocS
AFIT
erocS
AFITTable2.HyperparametersettingsforTransformer-basedbackbonesatresolution256x256.TheoriginalPixArt-α[5]modelusesT5-XXL
tokenizer,whileweuseOpenCLIP-HtokeepconsistencywithUNetexperiments.Theoriginalarchitecturesettingsaremarkedinbold.p,
handddenotepatchsize,hiddendimension,anddepth.
Model VAE p h d #heads TextEncoder MaxTokens TokenDim. Cap.Emb GMACs Params(B)
PixArt-α-XL/2[5] SD2 2 1152 28 16 4.3BFlan-T5-XXL 120 4096 Y 139 0.61
1152 Y 139 0.61
28 16
1536 Y 247 1.08
Ours SDXL 2 354MOpenCLIP-H 77 1024
28 N 110 0.48
1024 16
56 N 220 0.95
LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 iterationsandmorecomputetoachievesimilarperformance
0.825 0.4
0.800 0.2 asUNet.Weleavethisimprovementforfutureworksandex-
0.775 0.0 pectarchitectureimprovementcanmitigatethisissue,such
0.750 0.2 asworksdonein[11,12,41].
0.4
0.725
0.700 PixArt--h1024-d28 (0.48B) 0.6 PixArt--h1024-d28 (0.48B) PixArt--h1152-d28 (0.61B) 0.8 PixArt--h1152-d28 (0.61B) 4.ScalingTrainingData
0.675 P Pi ix xA Ar rt t- -- -h h1 15 03 26 4- -d d2 58 6 ( (1 0. .1 90 5B B) ) 1.0 P Pi ix xA Ar rt t- -- -h h1 15 03 26 4- -d d2 58 6 ( (1 0. .1 90 5B B) )
0.650 10000020000030000040000S 0D 52 00-U 00N 0et 6 0(0 00.8 07 0B 7) 00000800000 1.2 10000020000030000040000S 0D 52 00-U 00N 0et 6 0(0 00.8 07 0B 7) 00000800000 4.1.DatasetCuration
Training Steps Training Steps
We curate our own datasets named LensArt and SSTK.
Figure6.TheevolutionofTIFAandImageRewardduringtraining
forscaledPixArt-αvariantsaswellasSD2UNet. LensArtisa250Mimage-textpairssourcedfrom1Bnoisy
webimage-textpairs. Weapplyaseriesofautomaticfilters
SD2’s VAE; 2) we use OpenCLIP-H text encoder instead to remove data noise, including but not limited to NSFW
ofT5-XXL[7],thetokenembeddingdimensionisreduced content,lowaestheticimages,duplicatedimages,andsmall
from4096to1024. Thetokenlengthis77insteadof120. images. SSTK isanotherinternaldatasetwithabout350M
cleaned data. Table 3 shows the statistics of the datasets.
AblationSpace WeablatethePixArt-αmodelinthefol- MoredetailedanalysiscanbeseenintheAppendix.
lowingdimensions: 1)hiddendimensionh: PixArt-αinher-
Table3. Datasetstatistics. I-Cindicatesthenumberofunique
itsthedesignofDiT-XL/2[30],whichhas1152dimension.
image-captionpairs.AEindicatestheaverageaestheticscorefor
Wealso consider 1024 and1536. 2) transformer depth d:
thedataset. I-Nindicatesthetotalnumberofimage-nounpairs,
wescalethetransformerdepthfrom28to56. 3)captionem-
where each pair includes the image and a noun that is unique
bedding: thecaptionembeddinglayermapsthetextencoder
withincorrespondingrealandsyntheticcaptions. UNindicates
outputtodimensionh. Whenthehiddendimensionisthe thenumberofuniquenounswithinwholetextcorpus(herenouns
sameastextembedding(i.e.,1024),wecanskipthecaption includesbothnounsandpropernounsforsimplicity).N/Iindicates
embeddingtousethetokenembeddingdirectly. the average number of nouns per image. w. Syn indicates the
datasetincorporatessyntheticcaptions.
TheEffectofModelScaling AsshowninFig.6,scaling
Datasets I AE I-N UN N/I
thehiddendimensionhandmodeldepthdbothresultinim- LensArt-raw 1.0B 5.00 7.1B 3.9M 7.1
provedtext-imagealignmentandimagefidelity,whilescal- LensArt 250M 5.33 1.8B 1.2M 7.0
SSTK 360M 5.20 2.2B 680K 6.0
ingdepthdchangesmodel’scomputeandsizelinearly. Both
LensArt+SSTK 610M 5.25 3.9B 1.7M 6.5
d56 and h1536 variants achieve 1˜.5× faster convergence LensArt(w.Syn) 250M 5.33 3.2B 1.3M 12.8
SSTK(w.Syn) 360M 5.20 4.2B 1.1M 11.6
speedthanthebaselined28modelwithsimilarparameter
LensArt+SSTK(w.Syn) 610M 5.25 7.3B 2.0M 12.2
sizeandcompute.
4.2.DataCleaning
ComparisonwithUNet ThePixArt-αvariantsyieldlower
TIFA and ImageReward scores in comparison with SD2- The quality of the training data is the prerequisite of data
UNet trained in same steps, e.g., SD2 UNet reaches 0.80 scaling. Comparedtotrainingwithnoisydatasource,ahigh-
TIFAand0.2ImageRewardat250Kstepswhilethe0.9B qualitysubsetnotonlyimprovesimagegenerationquality,
PixArt-αvariantgets0.78and0.1. PixArt-α[5]alsoreports butalsopreservestheimage-textalignment. LensArtis4×
that training without ImageNet pre-training tends to gen- smaller than its unfiltered 1B data source, with hundreds
eratedistortedimagesincomparisontomodelsinitialized of million noisy data removed. However, model trained
frompre-trainedDiTweights,whichistrained7Mstepson withthishigh-qualitysubsetimprovestheaverageaesthetic
ImageNet[30]. ThoughDiT[30]provestheUNetisnota score[23]onthegeneratedimagesfrom5.07to5.20. This
mustfordiffusionmodels,thePixArt-αvariantstakelonger iscausedbyLensArthavinganaverageaestheticscoreof
6
erocS
AFIT
erocS
gvA
draweR
egamISD2-256, SD2-AE, CFG 7.5, BS=2048 SD2-256, SD2-AE, CFG 7.5, BS=2048 256, CFG 7.5, BS=2048 256, CFG 7.5, BS=2048
0.6
0.84 0.6
0.4 0.84
0.82 0.4
0.2 0.82
0.80 0.2
0.78 0.0 0.80 0.0
0.76 LensArt-raw (1.0B) 0.2 LensArt-raw (1.0B) 0.78 SD2-2.1AE | LensArt 0.2 SD2-AE-UNet | LensArt
L Se Sn Ts KA (r 3t 5(2 05 M0 )M) 0.4 L Se Sn Ts KA (r 3t 5(2 05 M0 )M) 0.76 S SD D2 X- L2 -A.1 EA -UE N| eL te n | s LA enrt s + Ar tSSTK (syn) 0.4 S SD D2 X- LA -AE E-U -UN Ne et t| |L e Len nsA sArt r t+ SSTK (Syn)
0.74 LensArt + SSTK (Syn) (600M) LensArt + SSTK (Syn) (600M) 0.74 SDXL-AE-UNet | LensArt + SSTK (syn) SDXL-AE-UNet | LensArt + SSTK (Syn)
100000 200000 300000 400000 500000 600000 100000 200000 300000 400000 500000 600000 100000 200000 300000 400000 500000 600000 100000 200000 300000 400000 500000 600000
Training Steps Training Steps Training Steps Training Steps
Figure7.SD2modelstrainedondifferentdatasetsandtheircorresponding Figure 8. Comparing SD2 and SDXL trained on LensArt and
TIFAandaverageImageRewardscoresduringtraining.Increasingthescale LensArt+SSTK(w.Syn). Enlargingtrainingsethelpsimprovingmodel
ofthedatasetbycombiningLensArtandSSTKgetsthebestresults. performance.Strongermodelyieldsbetterperformancewithlargerdataset.
betterthanalwaysselectingtop-1,whichisadoptedasthe
default training scheme for synthetic captions. Different
withPixArt-αwhichalwaysreplacestheoriginalcaptions
withlongsyntheticcaptions,weprovideanalternatewayto
scaleuptheimage-textpairsbyrandomflippingcaptions,
whichisinconcurrentwiththecaptionenhancementwork
Figure9.Syntheticcaptionsprovidedescriptionswithmoredetails.
ofDALL-E3[3].
5.33,higherthan5.00inLensArt-raw. Moreover,asshown
in Fig. 7, a SD2 model trained in LensArt achieves simi- 4.4.DataScalingIncreasesTrainingEfficiency
larTIFAscoreincomparisontheonetrainedwiththeraw
version,demonstratingthatthefilteringdoesnothurtimage- Combined datasets The text-image alignment and im-
textalignment. Thereasonisthatsufficientcommonsense age quality can be further improved as the dataset scale
knowledgeisstillretainedunderaggressivefilteringwhile increases. Here we compare the SD2 models trained on
enormousduplicatedandlong-taildataremoved. differentdatasetsandcomparetheirconvergencespeed: 1)
LensArt 2) SSTK and 3) LensArt + SSTK with synthetic
4.3.ExpandingKnowledgeviaSyntheticCaptions caption. WealsocomparetrainingwithunfilteredLensArt-
rawasthebaseline. Fig.7showsthatcombiningLensArt
To increase the valid text supervision for the smaller yet
andSSTKsignificantlyimprovestheconvergencespeedand
higher-qualitydata,weadoptaninternalimagecaptioning
theupperlimitoftwometricsincomparisonwithmodels
model,similartoBLIP2 [24],togeneratesyntheticcaptions.
trainedonLensArtorSSTKonly. SDXLmodeltrainedon
Thecaptioningmodelproducesfivegenericdescriptionsfor
LensArt+SSTKreaches0.82+TIFAscorein100Ksteps,
each image ranked by prediction confidence as in Fig. 9.
whichis2.5×fasterthanSDXLtrainedwithonlyLensArt.
Oneofthe5syntheticcaptionsandoriginalalt-textisran-
domly selected to pair with the image for model training
under50%chance. Thuswedoubletheimage-textpairsand
Advancedmodelsscalebetteronlargerdataset Fig.8
significantlyincreasetheimage-nounpairsasshowninTa-
showsthatSD2modelcangetsignificantperformancegain
ble3. Thankstothetextsupervisionexpandingbysynthetic
whentrainingonthescaled(combined)dataset. SDXLstill
captions,theimage-textalignmentandfidelitycanbecon-
getsperformancegainovertheSD2modelwhentrainedwith
sistentlyboosted,asshowninTable4. Specifically,theabla-
thescaleddataset,indicatingthatmodelswithlargecapacity
tiononLensArtshowsthatsyntheticcaptionssignificantly
havebetterperformancewhenthedatasetscaleincreases.
improvesImageRewardscore. Inaddition,wefindthatran-
domlyselectingoneintop-5syntheticcaptionsisslightly
5.MoreScalingProperties
Table 4. Synthetic caption ablations by training SD2 for 250K Relationship between performance and model FLOPs
steps.IR:ImageReward.Top5Syn.:randomlyselectoneintop-5 Fig. 11 (a-b) shows the correlation between TIFA score
syntheticcaptionsrankedbythecaptionpredictionconfidences; obtainedatfixedsteps(i.e.,600K)andmodelcomputecom-
Top1Syn.:onlyselecttop-1syntheticcaption
plexity(GFLOPs)aswellasmodelsize(#Params)forall
examinedSD2andSDXLvariants. WeseetheTIFAscore
Model Syntheticcaption TIFA CLIP IR FID
correlates slightly better with FLOPs than parameters, in-
LensArt 0.810 0.269 0.345 17.9
SD2 LensArt+Top5Syn. 0.835 0.270 0.524 18.9 dicating the importance of model compute when training
LensArt+Top1Syn. 0.833 0.271 0.513 18.3 budgetissufficient,whichalignswithourfindingsinSec3.
7
erocS
AFIT
erocS
gvA
draweR
egamI
erocS
AFIT
erocS
gvA
draweR
egamIspearman=0.936 spearman=0.900 spearman=0.900
0.850 0.850 700 model_name
SD2
0.845 0.845 600 S SD DX XL L-C128
SDXL-C192
0.840 0.840 500 SDXL-C384
0.835 S Sm D Do 2 Xd Lel_name 0.835 S Sm D Do 2 Xd Lel_name 400 S S SD D DX X XL L L- - -T T TD D D2 4 4_4
SDXL-C128 SDXL-C128 SDXL-TD4_8
0.830 SDXL-C192 0.830 SDXL-C192 300 SDXL-C384-TD4_12
SDXL-C384 SDXL-C384
0.825 S SD DX XL L- -T TD D2 4 0.825 S SD DX XL L- -T TD D2 4 200
SDXL-TD4_4 SDXL-TD4_4
0.820 SDXL-TD4_8 0.820 SDXL-TD4_8 100
SDXL-C384-TD4_12 SDXL-C384-TD4_12
102 103 103
GFLOPs #Param (M) #Param (M)
Figure10.(a-b)ThecorrelationofTIFAscorewithUNet’sinferencecomplexity(GFLOPs)andnumberofparameterswhentrainedwith
fixedsteps.(c)showsthecorrelationbetweenmodelparametersandFLOPs.
LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 spearman=0.927 spearman=1.000
0.850 SDXL S=0.64D0.03
SDXL-C128 0.85 LensArt
0.84 S SD DX XL L- -C C1 39 82 4 S LeS nK sArt(syn) 0.845 SDXL-TD2 LensArt+SSTK(syn)
SDXL-TD4_4
0.82 S SD DX XL L- -T CD 34 8_ 48 -TD4_12 0.84
0.840 S=0.77N0.11
SDXL (2.4B)
0.80 SDXL-C128 (0.4B) 0.83 SDXL-C192 (0.9B)
SDXL-C384 (3.4B) 0.835
SDXL-TD2 (0.9B)
0.78 S S S SD D D DX X X XL L L L- - - -T T T TD D D D4 1 1 4 2 4 _( 4 1 ( ( . 2 3 (2 1. .B 8 2 .2B B) B) ) ) 0.830 0.82
SDXL-TD4_8 (2.1B)
0.76 SDXL-TD4_12 (2.9B)
SDXL-C384-TD4_12 (4.1B)
S=0.47C0.02 0.825 0.81
1011 1012 103 2×103 3×103 4×103 6×103
Compute (GFLOPs) #Param (M) Image-Noun Pairs (M)
Figure11.FittingthescalinglawofTIFAperformanceSasafunctionofthetrainingcomputeCandmodelsizeN,anddatasetsizeD,
withthetraininghistoryofSDXLvariantsandSD2infixedsteps.The(--)indicatestheParetofrontierofthescalinggraph.
Relationship between performance and data size ModelEvaluationatLowResolution Onemaywonder
Fig.11(c)showsthecorrelationbetweenSD2’sTIFAscore whether the models’ relative performance will change at
and dataset size in terms of number of image-noun pairs. highresolutiontraining,sothatthegapbetweenmodelscan
Each image-noun pair is defined as an image paired with bemitigated. IntheAppendix,weshowcontinuetraining
onenouninitscaption. Itmeasurestheinteractionbetween models at 512 resolution slightly improve their 256 reso-
the fine-grained text unit with the image. We see a linear lution metrics without significant change. Though image
correlationbetweenTIFAandthescaleofimage-nounpairs qualityandaestheticscanbeimprovedviahigh-qualityfine-
whenscalingupthecleaneddata. ComparedtoLensArt-raw tuning[8],itishardfortheinferiormodeltosurpasswhen
withsimilaramountofimage-nounpairs,LensArt+SSTKis trainedonthesamedata,especiallywhenthehighresolution
muchbetter,whichindicatestheimportanceofdataquality. dataismuchlessthanitslowerresolutionversion. Thema-
joritycompositioncapabilityisdevelopedatlowresolution,
whichenablesustoassessmodel’sperformanceattheearly
stageoflowresolutiontraining.
NumericalScalingLaw ThescalinglawofLLMs[17,21]
reveals that LLM’s performance has a precise power-law
scalingasafunctionofdatasetsize,modelsize,andcom- 6.Conclusions
putebudget. HerewefitsimilarscalingfunctionsforSDXL
variantsandSD2. TheTIFAscoreS canbeafunctionof Wepresentasystematicstudyonthescalingpropertiesof
trainingdiffusionbasedT2Imodels, includingtheeffects
totalcomputeC (GFLOPs),modelparametersizeN (Mpa-
of scaling both denoising backbone and dataset. Our
rameters)anddatasetsizeD(Mimage-nounpairs)asshown
study demonstrates practical paths to improve T2I model
inFig.11. Specifically,withtheParetofrontierdatapoints,
performance by properly scaling up existing denoising
we can fit the power-law functions to be S = 0.47C0.02,
backbones with large-scale datasets, which results in
S = 0.77N0.11, and S = 0.64D0.03, which approximate better text-image alignment and image quality, as well
theperformanceinarangegivensufficienttraining. Similar as training efficiency. We hope those findings can bene-
asLLMs, weseelargermodelsaremoresampleefficient fitthecommunityforpursuingmorescaling-efficientmodels.
andsmallermodelsaremorecomputeefficient.
8
erocS
AFIT
spets
K006
@
AFIT
spets
K006
@
AFIT
spets
K006
@
AFIT
sPOLFG
erocS
AFITReferences [14] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,
andYejinChoi.Clipscore:Areference-freeevaluationmetric
[1] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,Ji-
forimagecaptioning.arXivpreprintarXiv:2104.08718,2021.
amingSong,KarstenKreis,MiikaAittala,TimoAila,Samuli
4,11
Laine,BryanCatanzaro,etal. ediffi:Text-to-imagediffusion
[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion
modelswithanensembleofexpertdenoisers. arXivpreprint
guidance. arXivpreprintarXiv:2207.12598,2022. 2
arXiv:2211.01324,2022. 1
[16] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
[2] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,
sionprobabilisticmodels. InNeurIPS,2020. 2,3
HangSu,andJunZhu. Allareworthwords:Avitbackbone
[17] JordanHoffmann,SebastianBorgeaud,ArthurMensch,Elena
fordiffusionmodels. InCVPR,2023. 2
Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las
[3] JamesBetker,GabrielGoh,LiJing,TimBrooks,Jianfeng
Casas,LisaAnneHendricks,JohannesWelbl,AidanClark,
Wang,LinjieLi,LongOuyang,JuntangZhuang,JoyceLee,
etal.Trainingcompute-optimallargelanguagemodels.arXiv
YufeiGuo,WesamManassra,PrafullaDhariwal,CaseyChu,
preprintarXiv:2203.15556,2022. 1,8
YunxinJiao,andAdityaRamesh.Improvingimagegeneration
[18] EmielHoogeboom,JonathanHeek,andTimSalimans.simple
withbettercaptions. https://cdn.openai.com/papers/dall-e-
diffusion: End-to-enddiffusionforhighresolutionimages.
3.pdf,2023. 2,7
arXivpreprintarXiv:2301.11093,2023. 3
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
[19] YushiHu,BenlinLiu,JungoKasai,YizhongWang,MariOs-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
tendorf,RanjayKrishna,andNoahASmith. Tifa:Accurate
PranavShyam,GirishSastry,AmandaAskell,etal.Language
andinterpretabletext-to-imagefaithfulnessevaluationwith
modelsarefew-shotlearners. InNeurIPS,2020. 1
questionanswering. arXivpreprintarXiv:2303.11897,2023.
[5] JunsongChen,JinchengYu,ChongjianGe,LeweiYao,Enze 3,11
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
[20] GabrielIlharco,MitchellWortsman,RossWightman,Cade
Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of
Gordon,NicholasCarlini,RohanTaori,AchalDave,Vaishaal
diffusiontransformerforphotorealistictext-to-imagesynthe-
Shankar,HongseokNamkoong,JohnMiller,HannanehHa-
sis. InICLR,2024. 2,3,5,6
jishirzi,AliFarhadi,andLudwigSchmidt. Openclip,2021.
[6] MehdiCherti,RomainBeaumont,RossWightman,Mitchell 3
Wortsman,GabrielIlharco,CadeGordon,ChristophSchuh-
[21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
mann,LudwigSchmidt,andJeniaJitsev. Reproduciblescal-
Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
inglawsforcontrastivelanguage-imagelearning. InProceed-
Radford,JeffreyWu,andDarioAmodei. Scalinglawsfor
ingsoftheIEEE/CVFConferenceonComputerVisionand
neurallanguagemodels. arXivpreprintarXiv:2001.08361,
PatternRecognition,pages2818–2829,2023. 1
2020. 1,8
[7] HyungWonChung,LeHou,ShayneLongpre,BarretZoph, [22] DiederikPKingmaandMaxWelling. Auto-encodingvaria-
YiTay,WilliamFedus,YunxuanLi,XuezhiWang,Mostafa tionalbayes. arXivpreprintarXiv:1312.6114,2013. 2
Dehghani, Siddhartha Brahma, et al. Scaling instruction-
[23] LAION. Laion aesthetic v2.
finetunedlanguagemodels.arXivpreprintarXiv:2210.11416,
https://github.com/christophschuhmann/improved-aesthetic-
2022. 6
predictor,2022. 6
[8] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang [24] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-
Wang,RuiWang,PeizhaoZhang,SimonVandenhende,Xiao- 2: Bootstrappinglanguage-imagepre-trainingwithfrozen
fangWang,AbhimanyuDubey,etal. Emu:Enhancingimage imageencodersandlargelanguagemodels. arXivpreprint
generationmodelsusingphotogenicneedlesinahaystack. arXiv:2301.12597,2023. 7
arXivpreprintarXiv:2309.15807,2023. 2,8
[25] YanyuLi,HuanWang,QingJin,JuHu,PavloChemerys,Yun
[9] DeepFloyd. Deepfloyd. https://github.com/deep-floyd/IF, Fu,YanzhiWang,SergeyTulyakov,andJianRen.Snapfusion:
2023. 1,2,3,4 Text-to-imagediffusionmodelonmobiledeviceswithintwo
[10] PrafullaDhariwalandAlexanderNichol. Diffusionmodels seconds. arXivpreprintarXiv:2306.00980,2023. 2
beatgansonimagesynthesis. InNeurIPS,2021. 2,3 [26] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
[11] PatrickEsser,SumithKulal,AndreasBlattmann,RahimEn- PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
tezari,JonasMüller,HarrySaini,YamLevi,DominikLorenz, Zitnick. Microsoft coco: Common objects in context. In
AxelSauer,FredericBoesel,etal.Scalingrectifiedflowtrans- ECCV,2014. 11
formersforhigh-resolutionimagesynthesis. arXivpreprint [27] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
arXiv:2403.03206,2024. 3,6 regularization. ICLR,2019. 3
[12] ShanghuaGao,PanZhou,Ming-MingCheng,andShuicheng [28] AlexanderNicholandPrafullaDhariwal.Improveddenoising
Yan. Maskeddiffusiontransformerisastrongimagesynthe- diffusionprobabilisticmodels. InICML,2021. 2
sizer. 2023. 2,6 [29] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and MarkChen. Glide:Towardsphotorealisticimagegeneration
Yoshua Bengio. Generative adversarial nets. In NeurIPS, andeditingwithtext-guideddiffusionmodels. arXivpreprint
2014. 2 arXiv:2112.10741,2021. 2
9[30] WilliamPeeblesandSainingXie. Scalablediffusionmodels
withtransformers. InICCV,2023. 2,3,5,6
[31] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,
TimDockhorn,JonasMüller,JoePenna,andRobinRombach.
Sdxl:improvinglatentdiffusionmodelsforhigh-resolution
imagesynthesis. arXivpreprintarXiv:2307.01952,2023. 1,
2,3,4,12
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learning
transferablevisualmodelsfromnaturallanguagesupervision.
InICML,2021. 1,4,11
[33] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
andMarkChen. Hierarchicaltext-conditionalimagegenera-
tionwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):
3,2022. 1,2
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesiswithlatentdiffusionmodels. InCVPR,2021. 1,2,
3,4
[35] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi,
JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-
torealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. InNeurIPS,2022. 1,2
[36] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibriumthermodynamics. InICML,2015. 2
[37] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising
diffusionimplicitmodels. arXivpreprintarXiv:2010.02502,
2020. 3,11
[38] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. NeurIPS,2017. 2
[39] XiaoshiWu,YimingHao,KeqiangSun,YixiongChen,Feng
Zhu,RuiZhao,andHongshengLi. Humanpreferencescore
v2:Asolidbenchmarkforevaluatinghumanpreferencesof
text-to-imagesynthesis. arXivpreprintarXiv:2306.09341,
2023. 4,11
[40] JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,Qinkai
Li,MingDing,JieTang,andYuxiaoDong. Imagereward:
Learningandevaluatinghumanpreferencesfortext-to-image
generation. InNeurIPS,2023. 3,11
[41] JingNathanYan,JiataoGu,andAlexanderMRush.Diffusion
modelswithoutattention. arXivpreprintarXiv:2311.18257,
2023. 6
[42] HongkaiZheng,WeiliNie,ArashVahdat,andAnimaAnand-
kumar. Fasttrainingofdiffusionmodelswithmaskedtrans-
formers. arXivpreprintarXiv:2306.09305,2023. 2
10On the Scalability of Diffusion-based Text-to-Image Generation
Supplementary Material
A.EvaluationDetails
Prompts We generate images with two prompt sets for evaluation: 1) 4081 prompts from TIFA [19] benchmark. The
benchmark contains questions about 4,550 distinct elements in 12 categories, including object, animal/human, attribute,
activity, spatial, location, color, counting, food, material, shape, and other. 2) randomly sampled 10K prompts from
MSCOCO[26]2014validationset.
Metrics In addition to previously introduced TIFA [19] and ImageReward [40] scores, we also calculate the following
metrics:
• FID:FIDmeasuresthefidelityorsimilarityofthegeneratedimagestothegroundtruthimages. Thescoreiscalculatedbased
ontheMSCOCO-10Kpromptsandtheircorrespondingimages. Weresizethegroundtruthimagestothesameresolution
(256×256or512×512)asthegeneratedimages.
• CLIP:TheCLIPscore[14,32]measureshowthegeneratedimagealignswiththeprompt. Specifically,thecosinesimilarity
betweentheCLIPembeddingsofthepromptandthegeneratedimage. HerewecalculateitwiththeMSCOCO-10Kprompts
andreporttheaveragevalue.
• HumanPreferenceScore(HPS)[39]:HPSv2isapreferencepredictionmodeltrainedwithhumanpreference.Wecalculate
thescoresbasedontheTIFApromptsandreporttheaveragevalue.
InferenceSettings Givenapromptsetandapre-trainedmodel,wegenerateimagesat256×256resolutionwithDDIM[37]
50steps,usingdefaultCFG7.5andfixedseedforallprompts. Foreachmodelcheckpoint,weuseitsnon-EMAweightsfor
evaluation.
B.MoreResultsonUNetScaling
00 .. 88 24 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 000 ... 222 677 802 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 00 .. 46 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 00 .. 22 67 50 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 222 234 LensArt, 256, SDXL-AE, CFG 7 S S S
S
S.5 D D D
D
D, 2 X X
X
X
B - L L
L
LU - -
-
-S U U
U
UN= N N
N
Ne e e
e
et2 t t
t
t(0 -
-
-0 ( C
C
C4 2. 1
1
398 .4 2
9
8B B 8
2
4) ) (
(
(0
0
3.
.
.4
9
4B
B
B)
)
) 000 ... 778 680
S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) )
0000 .... 2222 6666 0246
S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) )
00 0.. .02
2 S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) )
00 .. 22 56 50
S SD D2 X- LU -UN Ne et t( 0 (2.9 .4B B) )
22 01
0.74
100000 200000 T3 r0 a0 in0 in0 g0 Step4
s0000S S S 0D D DX X XL L L- - -U U U 5N N N 0e e e 0t t t 0- - -C C C 001 1 32 9 88 2 4 ( ( (0 0 3 6. . . 04 9 4 0B B B 0) ) )
00
00 .. 22 55 68
100000 200000 T3 r0 a0 in0 in0 g0 Step4
s0000S S S 0D D DX X XL L L- - -U U U 5N N N 0e e e 0t t t 0- - -C C C 001 1 32 9 88 2 4 ( ( (0 0 3 6. . . 04 9 4 0B B B 0) ) )
00
0.4
100000 200000 T3 r0 a0 in0 in0 g0 Step4
s0000S S S 0D D DX X XL L L- - -U U U 5N N N 0e e e 0t t t 0- - -C C C 001 1 32 9 88 2 4 ( ( (0 0 3 6. . . 04 9 4 0B B B 0) ) )
00
0.250
100000 200000 Tra3 i0 n0 in0 g0 0 Steps4 (0 s0
)00S S S 0D D DX X XL L L- - -U U U 5N N N 0e e e 0t t t 0- - -C C C 001 1 32 9 88 2 4 ( ( (0 0 3 6. . . 04 9 4 0B B B 0) ) )
00
19
100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
000000 ...... 777888 468024 LensArt, 256, SDXL-AE, CFG
S S S S S S S S S
7
D D D D D D D D
D.
2 X X X X X X X
X5
- L L L L L L L
L,
U - - - - - - - - U U U U U U U
UB
N N N N N N N N
NS
e e e e e e e e
et=
t t t t t t t t( - - - - - -
-02
T T T T T T T(2. D D D D D D
D0
9 . 2 4 1 1 4 4
444
B 2 4 _ _
_B)8
( ( 4 8 1 ) 0 1 ( ( 2. . 2 3 ( (9 2 1 2. . (B B 8 2 . . 22 1B B) ) .B B 9) ) B) ) )
0000000 ....... 2222222 5666677 7025702 5050505 LensArt, 256, SDXL-AE, CFG
S S S S S S S S S
7
D D D D D D D D
D.
2 X X X X X X X
X5
- L L L L L L L
L,
U - - - - - - - - U U U U U U U
UB
N N N N N N N N
NS
e e e e e e e e
et=
t t t t t t t t( - - - - - -
-02
T T T T T T T(2. D D D D D D
D0
9 . 2 4 1 1 4 4
444
B 2 4 _ _
_B)8
( ( 4 8 1 ) 0 1 ( ( 2. . 2 3 ( (9 2 1 2. . (B B 8 2 . . 22 1B B) ) .B B 9) ) B) ) )
0000 00.... ..0246
42
LensArt, 256, SDXL-AE, CFG
S S S S S S S S S
7
D D D D D D D D
D.
2 X X X X X X X
X5
- L L L L L L L
L,
U - - - - - - - - U U U U U U U
UB
N N N N N N N N
NS
e e e e e e e e
et=
t t t t t t t t( - - - - - -
-02
T T T T T T T(2. D D D D D D
D0
9 . 2 4 1 1 4 4
444
B 2 4 _ _
_B)8
( ( 4 8 1 ) 0 1 ( ( 2. . 2 3 ( (9 2 1 2. . (B B 8 2 . . 22 1B B) ) .B B 9) ) B) ) )
00000 ..... 22222 55667 05050 LensArt, 256, SDXL-AE, CFG
S S S S S S S S S
7
D D D D D D D D
D.
2 X X X X X X X
X5
- L L L L L L L
L,
U - - - - - - - - U U U U U U U
UB
N N N N N N N N
NS
e e e e e e e e
et=
t t t t t t t t( - - - - - -
-02
T T T T T T T(2. D D D D D D
D0
9 . 2 4 1 1 4 4
444
B 2 4 _ _
_B)8
( ( 4 8 1 ) 0 1 ( ( 2. . 2 3 ( (9 2 1 2. . (B B 8 2 . . 22 1B B) ) .B B 9) ) B) ) )
122222 901234 LensArt, 256, SDXL-AE, CFG S S S
S
S S S S
S
7 D D D
D
D D D D
D. 2 X X
X
X X X X
X5 - L L
L
L L L L
L, U - -
-
- - - -
-
U U
U
U U U U
UB N N N
N
N N N N
NS e e e
e
e e e e
et= t t
t
t t t t
t( -
-
- - - -
-02 T
T
T T T T
T(2. D
D
D D D D
D0 9 . 2
4
1 1 4 4
444 B
2 4 _ _
_B)8 (
(
4 8
1
) 0
1
( (
2.
.
2 3 (
(9
2
1 2. .
(B
B
8 2 . . 22 1B
B)
)
.B B 9) ) B) )
)
100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 Tra3 i0 n0 in0 g0 0 Steps4 (0 s0 )000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
00 .. 88 24 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 000 ... 222 677 802 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 00 .. 46 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 00 .. 22 67 50 LensArt, 256, SDXL-AE, CFG 7.5, BS=2048 222 234 LensArt, 256, SDXL-AE, S S S S
S S
C D D D D
D
DF 2 X X X
X
XG - L L L
L
LU - - -
- -
U U U
U
UN7 N N N
N
Ne. e e e
e
et5 t t t
t
t(, - -
-
-0 T T(
C
CB 2. D D
3
39 .S 4 44
8
8B= _ _B
4
4) 4 1 -)2
T(
2 3( D0 1 .( 444 . 22 _B.8 B 9 1)B 2) )
(4.1B) 0.80 0.266 0.2 00 .. 77 68 S S SD D D2 X X- L LU - -U UN N Ne e et t t( -0 T(2. D9 . 44B _B) 4) (1.2B) 000 ... 222 666 024 S S SD D D2 X X- L LU - -U UN N Ne e et t t( -0 T(2. D9 . 44B _B) 4) (1.2B) 0 0. .0 2 S S SD D D2 X X- L LU - -U UN N Ne e et t t( -0 T(2. D9 . 44B _B) 4) (1.2B) 00 .. 22 56 50 S S SD D D2 X X- L LU - -U UN N Ne e et t t( -0 T(2. D9 . 44B _B) 4) (1.2B) 22 01
0.74
100000 200000 T3 r0 a0 in0 in0 g0
SS S S tD D D epX X X
4
sL L L 0- - - 0U U U 0N N N 0e e e 0t t t- - -T C CD 3 34 8 8 5_ 4 4 01 - 0T(2 3 0D .( 0442 0_B.9 1)B 2 ) (4 6. 01 0B 0)
00
00 .. 22 55 68
100000 200000 T3 r0 a0 in0 in0 g0
SS S S tD D D epX X X
4
sL L L 0- - - 0U U U 0N N N 0e e e 0t t t- - -T C CD 3 34 8 8 5_ 4 4 01 - 0T(2 3 0D .( 0442 0_B.9 1)B 2 ) (4 6. 01 0B 0)
00
0.4
100000 200000 T3 r0 a0 in0 in0 g0
SS S S tD D D epX X X
4
sL L L 0- - - 0U U U 0N N N 0e e e 0t t t- - -T C CD 3 34 8 8 5_ 4 4 01 - 0T(2 3 0D .( 0442 0_B.9 1)B 2 ) (4 6. 01 0B 0)
00
0.250
100000 200000 Tra3 i0 n0 in0 g0 0
SteS S S pD D D sX X X
4
(L L L
0
s- - -
0
)U U U 0N N N 0e e e 0t t t- - -T C CD 3 34 8 8 5_ 4 4 01 - 0T(2 3 0D .( 0442 0_B.9 1)B 2 ) (4 6. 01 0B 0)
00
19
100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
Figure12.TheevolutionofallmetricsduringtrainingforUNetvariants.ThebaselinemodelsaretheUNetsofSD2andSDXL.Allmodels
aretrainedwithSDXLVAEat256×256resolution.The1strowshowsSDXLUNetswithdifferentinitialchannels.The2ndrowshows
SDXLUNetswithdifferentTDs.The3rdrowcomparesSDXLUNetswithbothincreasedchannelsandTDs.
11
erocS AFIT
erocS
AFIT
erocS AFIT
erocS gvA 2vSPH
erocS
gvA 2vSPH
erocS gvA 2vSPH
erocS gvA draweR
egamI
erocS gvA
draweR egamI
erocS gvA draweR egamI
erocS PILC
erocS
PILC
erocS PILC
DIF K01-OCOCSM
DIF K01-OCOCSM
DIF K01-OCOCSMEvolutionofallmetricsforUNetvariants WehaveshowntheTIFAevolutioncurvesofSDXL[31]UNetvariantsinSec.3.
HereweshowtheevolutionofothermetricsduringtrainingforallUNetvariantsinFig.12,includingthechangeofchannels,
transformerdepthandbothofthem. ThepatternofothermetricsisverysimilarasTIFAandtherelativeperformanceamong
modelsisstableacrossmetrics,e.g.,the1strowofFig.12showsthatUNetswithmorechannelstendtohavebetterTIFA,
HPSv2,ImageReward,CLIP,andFIDscores. ThoughFIDscorehasmorevariationsduringtraining.
ComparingthetrainingefficiencyofSDXLUNetanditsvariant PreviouslyweintroduceasmallerSDXLUNetvariant,
i.e.,TD4_4,whichis45%smaller,28%faster,andhascompetitiveperformanceasSDXL-UNetwhentrainedwiththesame
steps(Fig.12). Herewecomparetheirmetricsintermsoftrainingstepsaswellasthetotalcompute(GFLOPs). Weextend
thetrainingstepsofTD4_4from600Kto850Ktoseewhethertheperformancecanbefurtherimproved. AsshowninFig.13,
TD4_4achievessimilarorbettermetricsincomparisonwithSDXLUNetwithmuchlesscomputationcost. Itsuggeststhat
TD4_4isamorecomputeefficientmodelwhenthetrainingbudgetislimited.
00 .. 88 45 S SD DX XL L- -A AE E- -L U Ue N Nn e es t t-A Tr Dt 4, _2 456, CFG 7.5, BS=2048
0.272
S SD DX XL L- -A AE E- -L U Ue N Nn e es t t-A Tr Dt 4, _2 456, CFG 7.5, BS=2048 00 .. 56 S SD DX XL L- -A AE E- -L U Ue N Nn e es t t-A Tr Dt 4, _2 456, CFG 7.5, BS=2048 00 .. 22 77 02 S SD DX XL L- -A AE E- -L U Ue N Nn e es t t-A Tr Dt 4, _2 456, CFG 7.5, BS=2048 22 12 .. 50 LensArt, 256, CFG 7.5, BS= S S2 D D0 X X4 L L8 - -A AE E- -U UN Ne et t-TD4_4
0.83 0.82 0.270 0.4 0.268 21.0 0.81 0.268 0.3 0.266 20.5 000 ... 778 890 00 .. 22 66 46 000 ... 012 0000 .... 2222 5666 8024 112 990 ... 050
0.77 0.262 0.1 0.256 18.5
100000200000300000400000500000600000700000800000 100000200000300000400000500000600000700000800000 100000200000300000400000500000600000700000800000 100000200000300000400000500000600000700000800000 100000200000300000400000500000600000700000800000
Training Steps Training Steps Training Steps Training Steps (s) Training Steps
00 .. 88 45 LensArt, 256, CFG 7.5, BS=2048
0.272
LensArt, 256, CFG 7.5, BS=2048 00 .. 56 LensArt, 256, CFG 7.5, BS=2048 00 .. 22 77 02 LensArt, 256, CFG 7.5, BS=2048 22 12 .. 50 LensArt, 256, CFG 7.5, BS= S S2 D D0 X X4 L L8 - -A AE E- -U UN Ne et t-TD4_4
0.83 0.82 0.270 0.4 0.268 21.0 0.81 0.268 0.3 0.266 20.5 0000 .... 7778 7890
S SD DX XL L- -A AE E- -U UN Ne et t-TD4_4
000 ... 222 666 246
S SD DX XL L- -A AE E- -U UN Ne et t-TD4_4
000 0... .012
1
S SD DX XL L- -A AE E- -U UN Ne et t-TD4_4
00000 ..... 22222 55666 68024
S SD DX XL L- -A AE E- -U UN Ne et t-TD4_4
1112 8990 .... 5050
0.0 0.2 0.4 0.6
Total
G0 FL.8
OPs
1.0 1.2 1.41e12 0.0 0.2 0.4 0.6
Total
G0 FL.8
OPs
1.0 1.2 1.41e12 0.0 0.2 0.4 0.6
Total
G0 FL.8
OPs
1.0 1.2 1.41e12 0.0 0.2 0.4 0 T. r6
aining
0 G. F8 LOPs1.0 1.2 1.41e12 0.0 0.2 0.4 0.6
Total
F0 L. O8
Ps
1.0 1.2 1.41e12
Figure13.ComparingmetricsevolutionspeedofSDXLUNetanditsTD4_4variantintermsoftrainingstepsandtotalcompute(GFLOPs).
TD4_4achievessimilarorbettermetricscoresatmuchlesstrainingcost.
C.MoreResultsonDatasetScaling
EvolutionofallmetricsforSD2-UNettrainedondifferentdatasets WehaveshowntheTIFAandImageRewardevolution
curvesofSD2-UNettrainedondifferentdatasetsinSec.4. HereweshowtheevolutionofallmetricsinFig.14. Thetrendof
othermetricsissimilarasTIFA,excepttheHPSv2andCLIPscoresforLensArt-Raw,whichhavehighervaluesthanLensArt.
WefindthereasonisthattheLensArt-Rawmodeltendtogenerateimageswithmorememetextduetoalargeamountofdata
hassuchpatterns,andsuchimagesusuallyresultsinhighervaluesonthosetwometrics. Thosemetricsbecomemoreprecise
andmeaningfulafterthetrainingdataisfilteredbyremovingthosememeimages.
00 .. 88 24 SD2-256, SD2-AE, CFG 7.5, BS=2048 00 .. 22 67 70 50 SD2-256, SD2-AE, CFG 7.5, BS=2048 00 .. 46 SD2-256, SD2-AE, CFG 7.5, BS=2048 0.265 SD2-256, SD2-AE, CFG 7.5, BS=2048 22 67 SD2-256, SD2-AE, CFG L L S L 7 e e eSn n n. T5 s s sKA A A, (r r rB 3t t t- 5S ( +r 2 0a= 5 Mw S2 0 S )( M0 T1 K4 ).2 8 (B s) yn) (600M)
0.2 0.260 0.80 0.2650 25
0.78 0.2625 0.0 0.255 24
0.76 LensArt-raw (1.0B) 0.2600 0.2 LensArt-raw (1.0B) 0.250 23
0.74 L S Le eSn nTs sKA A (r r3t t 5( +2 0 5 M S0 S)M TK) (Syn) (600M) 00 .. 22 55 57 05 L L S Le e eSn n nTs s sKA A A (r r r3t t t- 5( +r 2 0a 5 Mw S0 S )( M T1 K).2 (B s) yn) (600M) 0.4 L S Le eSn nTs sKA A (r r3t t 5( +2 0 5 M S0 S)M TK) (Syn) (600M) 0.245 L L S Le e eSn n nTs s sKA A A (r r r3t t t- 5( +r 2 0a 5 Mw S0 S )( M T1 K).2 (B s) yn) (600M) 22
100000 20000 T0 rai3 n00 in00 g0 St40 e0 p00 s0 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 20000 T0 rai3 n00 in00 g0 St40 e0 p00 s0 500000 600000 100000 200000 Tra3 i0 n0 in0 g0 0 Steps4 (0 s0 )000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
Figure14.TrainingSD2modelwithdifferentdatasets.AllmetricsshowthatLensArt+SSTKhasbetterscoresthanLensArtorSSTKonly.
NotethattheHPSv2andCLIPscoresforLensArt-RawaremuchhigherthanLensArt.Thereasonisthatunfiltereddatasettendstogenerate
imageswithmorememetext.
D.TheEffectofVAEImprovement
SDXL[31]introducedabettertrainedVAEandshowsimprovedreconstructionmetricsincomparisonwithitsSD2version.
However,theimpactsontheevaluationmetricsarenotfullyexplored. HereweablatetheeffectofVAEontheevaluation
12
erocS AFIT
erocS AFIT
erocS
AFIT
erocS gvA 2vSPH
erocS gvA 2vSPH
erocS
gvA 2vSPH
erocS gvA draweR egamI
erocS gvA draweR egamI
erocS
gvA draweR
egamI
erocS PILC
erocS PILC
erocS
PILC
DIF K01-OCOCSM
DIF K01-OCOCSM
DIF K01-OCOCSMmetrics. WecomparethetrainingofsameSD2-UNetwithdifferentVAEs,i.e.,SD2’sVAEandSDXL’sVAE,whilekeeping
allothersettingsthesame. Fig.15showsthattheimprovementofSDXL’sVAEoverSD2’sVAEissignificantforallmetrics.
00 .. 88 12 S SD D2 X- LA -AE ELensArt, 256, CFG 7.5, BS=2048 00 .. 22 67 80 S SD D2 X- LA -AE ELensArt, 256, CFG 7.5, BS=2048 0.4 S SD D2 X- LA -AE EMSCOCO-10K, CFG 7.5, BS=2048 0.265 S SD D2 X- LA -AE EMSCOCO-10K, CFG 7.5, BS=2048 22 78 MSCOCO-10K, CFG 7.5, BS=2048 S SD D2 X- LA -AE E
00 .. 78 90 00 .. 22 66 46 0.2 0.260 22 56 0.78 0.262 0.0 0.255 24 0.77 0.260 0.2 23
0.76 0.258 0.250 22
00 .. 77 45 00 .. 22 55 46 0.4 0.245 22 01
100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 Tra3 i0 n0 in0 g0 0 Steps4 (0 s0 )000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
Figure15.TrainingSD2UNetmodelwithdifferentVAEs.TheSDXL’sVAEhassignificantimprovementonallmetricsoverSD2’sVAE.
E.ScalingtheBatchSize
Toscaleoutthetrainingoflargediffusionmodelswithmoremachines,increasingbatchsizeisusuallyaneffectiveapproach.
Wehavebeenusingconsistentbatchsize2048inallexperimentsforcontrolledstudies. Herewealsoshowtheeffectofbatch
sizeontheevolutionofmetrics. WecomparethetrainingofSDXLUNetwith128channelsindifferentbatchsizes,i.e.,2048
and4096,whilekeepingothertrainingconfigsthesame. Fig.16showsthatlargerbatchsizeyieldsbettermetricsintermsof
sameiterationnumbers. TheconvergencecurveofFIDscoreismoresmooththansmallerbatchsize.
00 .. 88 23 Le B Bn S S= =sA 2 40 0rt 4 9, 8 6 256, SDXL-AE, SDXL-UNet-C128, CFG 7.5 00 .. 22 67 80 Le B Bn S S= =sA 2 40 0rt 4 9, 8 6 256, SDXL-AE, SDXL-UNet-C128, CFG 7.5 00 .. 34 Le B Bn S S= =sA 2 40 0rt 4 9, 8 6 256, SDXL-AE, SDXL-UNet-C128, CFG 7.5 00 .. 22 66 57 05 Le B Bn S S= =sA 2 40 0rt 4 9, 8 6 256, SDXL-AE, SDXL-UNet-C128, CFG 7.5 22 34 LensArt, 256, SDXL-AE, SDXL-UNet-C128, CFG B B S S7 = =.5 2 40 04 98 6
0.81 0.266 0.2 0.2625 00 .. 78 90 0.264 00 .. 01 00 .. 22 56 70 50 22
00 .. 77 78 00 .. 22 66 02 00 .. 21 000 ... 222 555 025 050 22 01
0.76 0.258 0.3 0.2475
100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000 100000 200000 Tra3 i0 n0 in0 g0 0 Steps4 (0 s0 )000 500000 600000 100000 200000 T3 r0 a0 in0 in0 g0 Step4 s00000 500000 600000
Figure16.TrainingSDXL-UNet-C128withdifferentbatchsizes.
F.ModelEvaluationatLowResolutionTraining
Theevaluationmetricsat256resolutioncanprovideearlysignalsontheirperformanceathighresolutions,whichisinformative
forquickmodelablationandselection. Thereasonisthatthehighresolutiontrainingusuallyutilizesasubsetofimagesof
thedataset,andthetext-imagealignmentandimagequalityscoresusuallydonotchangesignificantlyoncetheyarefully
trainedatlowerresolution,especiallythetext-imagealignmentperformance. GiventwowelltrainedSDXLmodels(C128and
C192)at256resolution,whichhasclearperformancegap,wecontinuetrainingthemat512resolutionandmeasuretheir
performancegap. AsshowninFig.17,bothtwoSDXLUNetmodelscangetperformanceimprovementat512resolution,but
C128modelstillyieldsworseperformancethanC192.
LensArt, SDXL-AE, CFG 7.5, BS=2048 LensArt, SDXL-AE, CFG 7.5, BS=2048
0.6
0.84
0.4
0.82
0.2
0.80
0.0
0.78
256-SD2-UNet (0.9B) 256-SD2-UNet (0.9B)
0.76 256-SDXL-UNet-C128 (0.4B) 0.2 256-SDXL-UNet-C128 (0.4B)
256-SDXL-UNet-C192 (0.9B) 256-SDXL-UNet-C192 (0.9B)
512-SDXL-UNet-C128 (0.4B) 512-SDXL-UNet-C128 (0.4B)
0.74 512-SDXL-UNet-C192 (0.9B) 0.4 512-SDXL-UNet-C192 (0.9B)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Training Steps 1e6 Training Steps 1e6
Figure17.TIFAandImageRewarddonotchangemuchduringhighresolutionfine-tuningstage(dashedlines)
13
erocS AFIT
erocS AFIT
erocS
AFIT
erocS gvA 2vSPH
erocS gvA 2vSPH
erocS gvA draweR egamI
erocS gvA draweR
egamI
erocS
gvA
draweR
egamI
erocS PILC
erocS PILC
DIF K01-OCOCSM
DIF K01-OCOCSMG.CaptionAnalysis
For both LensArt and SSTK dataset, we present the histograms of number of words and nouns of original and synthetic
captionsrespectivelyinFig. 18. Notethatweoverloadthenounwithnounandpropernouncombinedforsimplicity. First,as
showninthefirsttwofigures,weseethatsyntheticcaptionsarelongerthanoriginalcaptionsintermsofwords,indicating
augmentingoriginalcaptionswithsyntheticcaptionscanincreasethesupervisionperimage. Second,fromthelasttwofigures,
wenotethatthenumberofnounsofsyntheticcaptionsarelessthanthoseinrealcaptionsonaverage. Thisismainlycaused
bysyntheticcaptionshavelesscoverageinpropernouns,indictingthesyntheticcaptionsalonearenotsufficienttotraina
generalisttext-to-imagemodel.
LensArt SSTK
50% 50%
Original Caption Original Caption
Synthetic Caption Synthetic Caption
40% 40%
30% 30%
20% 20%
10% 10%
0% 0%
0 10 20 30 40 50 60 0 10 20 30 40 50 60
Number of words Number of words
LensArt SSTK
50% 50%
Original Caption Original Caption
Synthetic Caption Synthetic Caption
40% 40%
30% 30%
20% 20%
10% 10%
0% 0%
0 10 20 30 40 50 60 0 10 20 30 40 50 60
Number of nouns Number of nouns
Figure18.Histogramsofwordandnounnumbersintheoriginalandsyntheticcaptionsofdifferentdatasets
14
egatnecreP
egatnecreP
egatnecreP
egatnecreP