MatAtlas: Text-driven Consistent Geometry
Texturing and Material Assignment
Duygu Ceylan, Valentin Deschaintre*, Thibault Groueix*, Rosalie Martin,
Chun-Hao Huang, Romain Rouffet, Vladimir Kim, and Gaëtan Lassagne
Adobe
Abstract. We present MatAtlas, a method for consistent text-guided
3D model texturing. Following recent progress we leverage a large scale
text-to-imagegenerationmodel(e.g.,StableDiffusion)asapriortotex-
ture a 3D model. We carefully design an RGB texturing pipeline that
leveragesagridpatterndiffusion,drivenbydepthandedges.Bypropos-
ingamulti-steptexturerefinementprocess,wesignificantlyimprovethe
quality and 3D consistency of the texturing output. To further address
the problem of baked-in lighting, we move beyond RGB colors and pur-
sue assigning parametric materials to the assets. Given the high-quality
initial RGB texture, we propose a novel material retrieval method cap-
italized on Large Language Models (LLM), enabling editabiliy and re-
lightability. We evaluate our method on a wide variety of geometries
and show that our method significantly outperform prior arts. We also
analyze the role of each component through a detailed ablation study.
1 Introduction
Geometry texturing is a critical element in the virtual assets design pipeline.
Current industry pipelines rely significantly on manually “painting" existing ge-
ometries which is a skill-demanding, cumbersome and time-consuming process
especially for assets that are highly detailed. The recent success of large-scale
language-guided image generation models [53,55] opens up the possibility to
“paint" images by words instead. Hence, an interesting step is to extend the
success of such 2D models to 3D, to provide inspirational texturing workflows.
Severalrecentwork[11,54]attemptstoachievethisgoalbyiterativelygenerating
2D images from different view points which are then aggregated in the texture
space. Creating a seamless and high quality texture, however, is still a challeng-
ing task and existing approaches suffer from typical issues such as the “Janus
problem" (e.g. both the front and back of a head being textured with faces).
Furthermore, existingapproachesfocus on generating RGB textures withbaked
lighting. In practice, however, many applications require relightable assets.
In this work, we propose a method for texturing 3D assets and using the
generated textures to retrieve and assign materials from an existing database
to different parts of the asset. In addition to producing relightable assets, we
* Denotes equal contribution.
4202
rpA
3
]VC.sc[
1v99820.4042:viXra2 D. Ceylan et al.
Fig.1: We present MatAtlas, a novel approach for generation text guided relightable
textures for a given 3D mesh. Our method first generates an RGB texture using pre-
trained large scale image generation models. Using this texture as a guidance, we
retrieve materials from a database and assign to different parts of the mesh resulting
in a plausible and fully relightable asset.
also enable further appearance editing by retrieving parametric materials. In
particular, our texturing step follows a similar trend of utilizing a large scale
text conditioned image generation model but significantly improves the quality
and consistency of the results. This is critical for performing a robust material
searchwhereconsistentvisualcuesarenecessarytoassigncoherentmaterialsto
differentpartsofanasset.Wefurtherobservethathumanshavestrongpriorand
expectation about what type of materials are suitable for different parts of an
asset.Wethereforeproposeamaterialretrievalalgorithmwhichcombinesvisual
cues with global context and priors captured via large language models [46].
Our texturing method uses a cross-frame attention strategy to improve the
consistencyofimagesgeneratedfromdifferentviewpoints.Inparticular,wegen-
erate multiple views in a single image generation pass (we use Stable Diffu-
sion [55] in our experiments) in a 4x4 grid pattern where each view in the grid
can potentially attend to the features of other views in the grid. Compared to
thestandardview-by-viewpipeline[8,11,54]whichprocessesoneviewatatime
and enforces new view to follow the existing texture, our grid-based approach is
more “synchronous" where all views participate in the generation of each other,
improving view consistency. We further enforce consistency with additional 3D
guidance channels through ControlNet [76]. Specifically, we use depth condi-
tioning to generate images that are structurally aligned with the 3D asset. We
also render occluding and suggestive contours to provide lineart conditioning
to avoid texture bleeding across different parts and capture the details of small
parts. Our texturing method operates in multiple passes to refine the quality of
the generated textures. While an initial pass of grid based generation ensures a
globally consistent but potentially blurry texture, a second pass aims to refine
this texture. We perform a final pass of texture inpainting to guarantee a com-
plete coverage through an improved camera sampling process. We evaluate the
influenceofeachofthesecomponentsinSec.4.2,showingthattheircombination
significantly improves the final texturing quality.MatAtlas: Text-driven Consistent Geometry Texturing 3
Given a textured asset and access to a database of material definitions, we
propose a novel material retrieval and assignment pipeline. Given a 3D asset
with sets of parts, we perform part-based material retrieval. Our novel material
search algorithm considers both visual cues from the generated texture as well
as global context information. For the latter, we exploit the recent success of
language models in acquiring priors from large scale data.
Given a 3D asset and a target text prompt, our approach generates high
quality and relightable appearance. We compare our method to recent state-
of-the-art generative texturing approaches and show superior performance. Our
main contributions are as follows:
– A complete pipeline that can generate appearances for 3D assets that are
relightable and editable.
– A generative texturing method that uses large scale text-to-image models
that achieves superior performance compared to state of the art.
– A novel material retrieval algorithm that robustly matches the generated
textures to parametric materials in a database.
2 Related Work
We briefly review early texturing approaches, then discuss the use of recent
vision models for generative texturing.
Early approaches to texturing. Early works [13,25,30,34,39,41,64,71] syn-
thesize 3D textures from an exemplar image by building on classical image syn-
thesis works [19,21,22,32,33,51,70] using hand-crafted 3D geometric priors and
imagepriors.Thesemethodsestablishglobalcoherenceofthe3Dtexturebutare
limited to simple patterns and the texture details usually do not match closely
thegeometryanditssemanticparts.Toaddressthislimitationsometechniques
proposedtotransfertexturesfromimagesofsimilarobjects[29,31,65],however,
they heavily rely on human annotation. One can automate the process by using
symmetry,automaticalignment,andco-analysisofcollectionsofassetsfromthe
same category [47,68], but these methods cannot be applied to out-of-sample
one-of-a-kind shapes and require image references. In contrast our method only
needs off-the-shelf pretrained networks.
Category-specific models. Several works have explored using learning-based
methods to generate shape-specific textures [6,45,59,75]. Texture-Fields [45]
use a variational autoencoder to train a 3D coordinate-based MLP for textures,
Point-UVdiffusion[75]performsdiffusionbothonapointcloudandintheUV-
space,andTexturify[59]combinesdifferentiablerenderingandaGANapproach.
These works are category-specific because of their training data. To go beyond
single category models, recent approaches leverage the prior of foundational 2D
models trained on a massive amount of image data [55].
Texture generation with vision models.SeveralworksproposetouseCLIP
as an energy to optimize mesh properties like texture and geometry [23,43,44].4 D. Ceylan et al.
Dreamfusion [50] and Score Jacobian Chaining [66] concurrently propose Score-
Distillation Sampling (SDS) to distill 2D gradients from a sampling diffusion
model, which strictly improves over CLIP [12,35,36,38,52,61,63,69]. Latent-
Nerf [42] proposes latent-paint, a technique to optimize a texture in the latent
space of Stable Diffusion. These approaches suffer from a few drawbacks. First,
theyrequiremanyforwardpassesthroughlargevisionmodelswhichmakesthem
relativelyslow.Second,themode-seekingbehaviorofscore-distillationsampling,
and strong classifier-free guidance [27] limit the diversity of the textures they
areabletogenerate[50].ParticularlyrelevantisTextMesh[63]whichtilesviews
of a 3D object in a 2x2 grid to enhance consistency in the generated texture.
Closer to our texturing application are recent approaches [8,11,54,62], that
leverage a depth-conditioned Stable Diffusion [55,76] to directly project gener-
atedpixelsonthemeshusingafewpassesthroughthediffusionmodel[28].The
mainchallengesfortheseapproachesistoensureconsistencybetweenthepixels
being projected on the mesh from different views. TEXTure [54] introduces an
iterative approach, where each camera view point is used iteratively to refine
and generate the texture in the missing regions. TexFusion [8] interleaves dif-
fusion steps and projection steps to improve consistency between the different
views. Text2Tex [11] introduces a next best view selection scheme to select the
best camera viewpoint given the current coverage of the texture. HCTM [62]
improves consistency by employing textual inversion on the first generated im-
age as well as performing a LoRA-finetuning of the backbone diffusion model,
though this comes at the cost of added runtime i.e. overall 35 minutes. Wang
et al. [67] use a pipeline leveraging some of these ideas to texture all of the 12K
objects from over 270 categories of ShapeNetSem [10], which shows the gener-
ality and potential of projecting generated 2d pixels in 3D. Compared to those
approaches,weperformbetterconsistencythroughagridpatterndiffusion,and
avoid texture bleeding between different parts by conditioning on both depth
and edges. Furthermore, we go beyond RGB texturing via material retrieval.
This ensures high texture resolution, removes artifacts due to baked in lighting,
and produces relightable assets.
Concurrent work. At the time of this submission, different approaches have
beenproposedonArxivfor3Dgeometryandtexturegeneration.DreamCraft3D[60]
proposes to jointly fine-tune a 2D diffusion model using Dreambooth [56] while
using it in an SDS optimization to refine a mesh texture. TextureDreamer [73]
also uses a finetuned diffusion model based on a few reference images and per-
forms SDS type of optimization to optimize BRDF parameters that represent
the appearance of an object. In these methods, both the finetuning and the
SDS optimization parts that are repeated for each input mesh take significant
amount of time. Paint-it [74] presents a similar SDS optimization strategy to
optimizeforthephysicallybasedrenderingmapsthatarerepresentedviaaneu-
ral networks. Despite high-quality results, this approach also requires a large
number of forward passes through a diffusion model and can be prohibitively
expensive (e.g., 15-30 minutes per mesh), further the available lighting signal
availableforoptimizingspecularcomponentsofthematerialsaresparseandpo-MatAtlas: Text-driven Consistent Geometry Texturing 5
Input mesh Cond w/ depth Cond w/ depth & lines
Fig.2: Conditional generation.Weutilizebothdepthandlineartbasedcondition-
ingtoguidetheimagegenerationmodel.Whiledepthhelpstopreservetheunderlying
geometry, occluded and suggested contours represented in the line renderings help to
capture details and avoid texture bleeding across different parts.
tentiallyinconsistent,makingtheoptimizationformaterialschallenging.Finally,
FlashTex[16]extendssuchanoptimizationprocesswithalightconditionedCon-
trolNet. In contrast, our method avoids such a costly optimization process and
utilizes high quality procedural materials. In addition to relighting, this also en-
ables to further edit the appearance of the objects which sets our method apart
from concurrent work.
3 Method
Thegoalofourmethodistoleverageapre-trainedlargescale2Dimagegenera-
tion model to texture a 3D mesh based on a target text prompt. Specifically, we
use a diffusion model that is guided with additional control signals that we de-
rive from the underlying 3D model (Section 3.1). In order to ensure synthesis of
consistent images across multiple views, we use a cross-frame attention strategy
(Section 3.2). Specifically, we tile multiple views in a grid-like fashion and run
a single image generation call that synthesizes the multiple views in a synchro-
nized manner. Finally, we run our method in multiple passes both to refine the
texture quality and to ensure full coverage (Section 3.3). Given a consistently
textured3Dmodelandaccesstoamaterialdatabase,wealsoproposeamethod
to automatically retrieve and assign appropriate materials to different parts of
the mesh to output a fully relightable asset (Section 3.4). Next, we discuss each
stage of our method in more details.
3.1 Conditioned image generation
Our method uses a pretrained large scale image diffusion model to construct a
texturefora3Dmodelbasedonatextprompt.Inourexperiments,weleverage
Stable Diffusion [55], however, our method can be applied to other image diffu-
sion models. Given the success of such models in generating very high quality
images, many different additional control signals have been proposed to guide6 D. Ceylan et al.
the generation process. We leverage such control mechanisms, namely Control-
Net [76], to generate images that are faithful to the underlying geometry of the
3D model. Specifically, we use both depth and lineart conditioned generation.
Depth conditioning is crucial to generate images that are aligned with the un-
derlyinggeometry.However,sincethedepthisnormalizedtoapredefinedrange
beforebeingpassedthroughthediffusionmodel,alossingeometryresolutionis
inevitable, erasing small details. To address this challenge and to further avoid
color bleeding across different pronounced parts of the 3D model, we use the
additional lineart conditioning. Specifically, we first render the 3D model with a
constantdiffusecolor[48]anddetectoccludedandsuggestivecontoursusingthe
ridgedetectionfilterfromOpenCV[7]asadditionalguidance.Figure2visualizes
how each control signal contributes to the quality of the generated images.
3.2 Cross-frame attention for consistent image generation
Atypicalimagediffusionmodelconsistsofcrossandselfattentionblocks.While
cross attention models the relation between conditioning signals such as text
prompts, self attention layers influence the overall structure and appearance of
the generated images. Recent work (some of which are not published yet during
the time of submission) adapts image generation models for video editing [9,24]
ormulti-viewsynthesis[58]haveshownthatadaptingtheselfattentionlayersto
perform cross-frame attention is critical for consistent appearance synthesis. A
straightforward approach to introduce cross frame attention is to stack multiple
views in a single image in a grid-like fashion (16 images arranged in a 4-by-4
grid in our experiments) and perform a single image generation process (see
Figure3).Ateachspatiallocationinthegridimage,thequeryfeatures,i.e.,the
features of the corresponding view, attend to the key and value features of the
whole grid image, i.e., a union of the self attention features of each view.
Weempiricallyobservethatgeneratedimageswithwhitebackgroundtendto
have more uniform lighting compared to images with cluttered background. To
enforce white background in the generated views, we follow an approach similar
to Blended Diffusion [4]. We encode a pure white image that has the same size
as our grid image into the latent space of Stable Diffusion. We add noise to
the resulting latent representation (based on the first diffusion timestep t=980
in our experiments) to obtain zt . We blend zt with pure Gaussian noise, zt,
w w
basedonamaskimageM constructedbyarrangingtherenderedmasksofthe3D
modelfrommultipleviewsinasimilargrid.Inpractice,wedilateM (wherewhite
denotes the object region and black denotes the background) before blending to
avoidunnecessarywhitecolorleakingintothetexture.Weinitializethediffusion
process with such blended noise: z˜t =(1−M)⊙zt +M ⊙zt.
w
3.3 Multi-pass texture refinement
The cross frame attention strategy described in the previous section ensures
global consistency in the synthesized views. However, we observe that small
inconsistencies might remain due to limited resolution. Furthermore, differentMatAtlas: Text-driven Consistent Geometry Texturing 7
lighteffectsinthegeneratedimagesacrossviewpointscanresultincolorchanges.
To address such issues, we propose a multi-pass texture refinement process as
described below and shown in Figure 3.
“a notebook”
input mesh
line & depth cues first pass grid generation second pass grid refinement inpainting
Fig.3:Multi-passtexturegeneration.Weperformamulti-passtexturegeneration
where an initial pass enables the generation of a globally consistent but potentially
blurry texture. In the second pass, we refine this texture quality. We perform a final
inpaintingpasstoensureafullcoverageofanyuntexturedpartsofthemodel.Weuse
lineart and depth cues to condition the image generation model.
Firstglobalconsistencypass.Inthefirstpass,wegenerateagridimageof
multiple views starting from full noise as described in Section 3.2. For this step,
we normalize the input 3D model into a unit sphere and sample views at three
differentelevationlevelswithuniformlysampledazimuthangles.Wethenblend
the generated views into the texture space using averaging. Averaging helps to
blend the small inconsistencies due to the baked lighting across multiple views
so the texture has a more flat lighting overall. This also results in a more blurry
texture, however, which we refine in the subsequent stage.
Texturerefinementpass.Inthesecondpass,werenderthemodelwiththe
initialtextureobtainedfromthefirstpassfromthesameviewpointsarrangedin
a grid and re-generate an image. Due to the average blending, the texture and
hence the rendered grid images are potentially blurry. To refine these images,
we add partial noise and denoise similar to SDEdit [40] (in our experiments we
perform 20 steps of denoising where a full denoising is defined to be 50 steps).
Performing a small number of denoising steps helps to generate sharper images
without sacrificing the consistency. We then update the texture by blending the
refined views. At this stage instead of average blending, for each triangle of the
input mesh, we grab its texture from the view where the viewing direction best
aligns with the normal of the triangle.
Texture completion pass. Since the initial viewpoints we sample around
the object may not guarantee full coverage, we perform a final pass where we
sampleadditionalviewpointstorevealthepartsoftheobjectwhichmaynothave
beentexturedyet.Wethereforeiterateover150possibleviewpoints,sampledon
theFibonaccisphere,andsortthembytheamountofnewUVspaceinformation
theyprovide.Wedefinenewinformationaseitherasignificantlybetterviewfor
a previously textured pixel or observation of a previously untextured pixel. We
define a contribution score for each viewpoint based on the new information it
brings.8 D. Ceylan et al.
“cover”: leather
“strap”: fabric
“paper”: paper
(a) per-part material types (b) retrieved materials (c) rendered with materials
Fig.4: Material retrieval. (a) Given a textured and segmented object, we rely on
largelanguagemodelstoextracttheglobalcontextandsuggestdifferentmaterialtypes
for each part. (b) We then perform a visual material search for each part within the
corresponding material category. This search utilizes CLIP image embeddings as well
ascolorfeatures.(c)Afterassingingtheretrievedmaterialstothemesh,wecanrelight
the asset. Here we render it with Blender [14] under a soft environment illumination.
We then select the most informative viewpoint and recompute contribution
of the remaining viewpoints. We repeat this process until the contribution score
of the most informative viewpoint falls below a threshold. We refer to the sup-
plementary material for details of how the contribution score and the threshold
is defined. In our experiments, we observe that typically 2−3 additional view-
points are selected in this pass. Given the additional views, we generate the
corresponding images iteratively while using blended diffusion to perform in-
painting –ensuring consistency with pre-existing texture– with depth and edge
conditioning [54]. In cases where parts of the mesh occludes others (e.g., a book
occluding parts of the shelf), it may still be challenging to obtain full coverage.
Hence, we perform a final texture inpainting pass using PatchMatch [5].
3.4 Material retrieval and assignment
In many applications (e.g. architecture, video-games, product staging), having
a fully relightable asset is needed. To address this need, we propose a novel
material retrieval and assignment process, leveraging existing databases of high
quality materials [2,3,17,49].
Material-aware segmentation. For any such retrieval process, the first
task is to segment the input 3D object into parts where each part can be as-
signedthesamematerial.Formany3Drepositories(e.g.,Objaverse[15]),objects
already come with such information (e.g., having part-level semantic segmenta-
tion or having disconnected parts). In cases where this information is not given,
we use an interactive semi-automatic segmentation method. In particular, we
let the user draw a few scribbles on the mesh to indicate material regions to
segment.
Material search. Given the generated texture and segments of the 3D
model, our core material retrieval algorithm leverages both global context as
well as local visual cues. Relying only on visual cues can be ambiguous in de-
termining material types since two very different material types might have a
similarappearance.AshumanswerelyonourpriorknowledgeofwhatmaterialMatAtlas: Text-driven Consistent Geometry Texturing 9
typesareoftensuitablefordifferentobjectsandtheirparts(e.g.,acushionofan
armchair is likely to be made of fabric). Hence, we propose an approach where
we first leverage large language models to extract global context information.
Specifically, given an object and its different parts, we ask GPT-4V [46] to sug-
gestdifferentmaterialcategoriescorrespondingtodifferentpartsoftheobjectas
shown in the inset figure. Then, we render each part of the object from a view-
point that is as frontal as possible and crop a square patch from this rendering.
For each part, we perform a material search based on the visual cues provided
by such a patch among the materials in the database that belong to the same
category as the suggestion from GPT-4V.
To perform visual material search we combine
CLIP feature similarity with color histograms.
CLIPcapturestextureandsemanticconceptsand
hasbeenshowntoworkbetterthanothermetrics frame:
pillow:
for material retrieval from images [18,72]. How- metal
fabric
ever, it is not as sensitive to color, sometimes re-
sulting in color differences between the RGB tex-
ture and the retrieved material. Hence, we aug- bed:
ment it with LAB space color information. In a fabric
one-timepre-processingstage,werendereachma-
terial in our database on a flat surface with a neutral environment illumination.
WecomputeCLIPimageembeddingfeaturesaswellascolorhistogramforeach
material rendering. The color histograms are 3D histograms in the La*b* space
with 8 bins for L and 32 for a* and b*. In particular we empirically found that
extractingthe7mostprominentcolorsisacompactandsufficientrepresentation
for high quality retrieval. Given the patch of the rendering of the part, we com-
putethecorrespondingCLIPimageembeddingandcolorhistograminasimilar
manner.Wecomputethedistanceofthepatchtoallthematerialsofthespecific
categorybothbasedontheCLIPsimilarity(d )aswellasthecolorhistograms
clip
(d ).Wenormalizebothd andd tobeintherange(0,1)basedonthe
color clip color
maximumdistancebetweenthepatchandallothermaterials.Thefinaldistance
isalinearcombinationofthetwodistances:d=d ∗(1−w)+d ∗w where
clip color
w (set to 0.8 in our experiments) is a weight that controls how much the search
should be influenced by color similarity. We assign the material that has the
smallest distance d to each part.
Material assignment. When assigning the selected materials to the mesh
parts, we adjust the material scale based on the physical size of the materi-
als and the mesh parts. The information about the mesh physical size is often
available in high quality asset libraries, but can also be approximated leverag-
ing GPT-4V [46]. We compute the mesh and uv area of each part, and apply a
tiling factor to the selected material so that it matches the physical size of the
part. The height maps are also normalized across materials, in order to get a
consistent mid-level and meaningful relative displacement values. To minimize
the amount of seams when texturing the mesh with the materials, we use a tri-
planar projection onto the mesh instead of a simple uv projection. Finally, the10 D. Ceylan et al.
materials can be blended together to obtain a single texture set for the entire
object, or kept separately to allow for further editing of each part.
4 Evaluation
4.1 Implementation Details
For texture generation, we use Stable Diffusion v1.5 [1] as the base model with
depth and lineart ControlNet models [76] for conditioning. We set the weight
of each control channel to 0.5 in our experiments. In the first two passes we
generate images in 1600-by-1600 resolution where generation with full denoising
(50 steps) in the first pass takes around 54 seconds and partial denoising (20
steps) in the second pass takes 21 seconds. In the final pass, we generate each
additional viewpoint in 512-by-512 resolution where each generation takes 4.4
seconds and PatchMatch based inpainting takes 1 second. Our output textures
are of resolution 1024-by-1024. In total, our texturing step takes around 2.5−3
minutesonaA100GPU.Selectingthebestviewpointtorenderarepresentative
patch for each part and performing part-based material retrieval typically takes
several seconds.
For the material retrieval results shown in this paper, we use the Substance
3D Assets material library [2] containing 8,965 procedural materials across 16
categories (e.g., wood, metal, fabric, paper etc.). For each material we extract
the different presets (typically 1 to 4) defined by the artist in the procedural
material itself resulting in 28,206 variations in total.
Table 1: Quantitative comparison.
Text2Tex [11]TEXTure [54]Ours w/o line guidanceOurs w/ inpainting Ours
FID ↓ 42.133 49.276 46.958 38.535 38.467
4.2 Evaluation of texture generation.
In order to evaluate the performance of the texture generation part of our
method,weusethesubsetoftheObjaversedataset[15]thatcontains410models
as proposed by Text2tex [11] along with the captions that are derived from the
namesofthemodels.Wecompareourmethodtotherecentgenerativetexturing
methods that also utilize Stable Diffusion as the backbone and have provided
code and data. Specifically, we compare to TEXTure [54] and Text2Tex [11]
which both utilize depth conditioned generation with strategies to minimize
seams. We note that both methods report superior performance against otherMatAtlas: Text-driven Consistent Geometry Texturing 11
CLIP-based optimization methods [43,44]. As a quantitative metric, we adopt
the commonly used Frechet Inception Distance (FID) [26] to evaluate the qual-
ity of the textures. Following the protocol introduced by Text2Tex, we use the
renderingsoftheObjaversemodelswithgroundtruthtexturesfortherealdistri-
bution.WeprovidethequantitativeresultsinTable1andprovidevisualresults
inFigure5.Asshownbothvisuallyandquantitatively,ourmethoddemonstrates
superior performance with respect to the related work.
“an apricot” “a barrel” “a bookcase”“a muffin”“an ottoman” “a skateboard” “a ball”
Fig.5: WecompareourmethodtoText2Tex[11]andTEXTure[54],tworecentstate-
of-the-artgenerativetexturingmethodsthatalsoutilizeStableDiffusion.Weshowthe
inputmesheswiththetextpromptsinthefirstrow.Ourresultsbetterpreservetexture
consistency.
User Study. We conduct a user study where we select 5 random pairs of
resultsfromourmethodandeachofthebaselinemethods.Weshowa360-video
ofeachresultrenderedwithsimilarlighting.Weasktheuserswhichresultlooks
better overall considering realism and texture seams. We collect 140 responses
from 28 users for each method comparison. Our method is preferred 67.86% of
thetime(95%confidenceinterval:±8.9%)withrespecttoTEXTureand60%of
the time (95% confidence interval: ±10.1%) with respect to Text2Tex. We refer
to the supplementary material for the details of the user study.
Finally, we also provide both texturing and material retreival variations in
Figure 6 to show how our method generalizes to different text prompts.
AblationStudy.Weablatedifferentdesigndecisionsinourmethod.Specif-
ically, we run our texturing method using only depth conditioned generation
(Table 1, column 3), using depth and line conditioned generation without the
eruTXET
xeT2txeT
sruo12 D. Ceylan et al.
Fig.6: For the same model, we show the texturing (top row) and material retrieval
(bottom row) variations obtained by using different input prompts.
finalpatchmatchbasedinpainting(Table1,column4),andthefullpipeline.As
shown in the table, using line art conditioning significantly improves the perfor-
mance since it provides a strong structural guidance. While the final inpainting
does not have a significant impact on the quantitative metrics, we visually ob-
serve that it improves the quality as shown in the supplementary material.
In Figure 7, we ablate the effect of using multiple texturing passes on the
resulting texture quality. Specifically, we show the texturing results obtained by
(i) using a single pass with view-based blending, and finally (ii) our proposed
multi-passapproach.Asseeninthefigure,ourproposedapproachreducesseams
and generates higher quality textures.
single multi single multi single multi
Fig.7: Compared to a single pass with view-based texture blending, our multi-pass
approach reduces texture seams.
4.3 Evaluation of material assignment.
We evaluate our material assignment pipeline on 3D models obtained from the
Adobe Substance 3D Assets library 1 which contains models across various cat-
egories (e.g., tools, technology, building etc.) The 3D models in this database
are segmented with available semantic part names which we utilize to query
ChatGPT. In Figure 8, we show representative qualitative results. We test our
1 https://substance3d.adobe.com/assets/
”tevlev
der“
”rolocretaw
,laveidem“
”snrettap
drapoel“
”detniap
dnah“
”sroloc
wobniar“
”sgnirts
larutan
,eulb“
”deripsni
tserof“MatAtlas: Text-driven Consistent Geometry Texturing 13
Fig.8: Given an input segmented mesh from the Source 3D assets (top row, each
segment is shown in a different color), we show results from our automatic texturing
(secondandthirdrows)andtherenderingsobtainedbymatchingthetexturestoclosest
materials in our database (fourth and fifth rows).
materialretrievalpipelineonasubsetofObjaversemodelswhereweusethepart
segmentationsalreadyprovidedbythemodelandmaterialquerysuggestionsob-
tained from ChatGPT-4V. Finally, we also demonstrate that our texturing and
materialassignmentpipelinecanbeappliedtoobjectsfromtheGoogleScanned
Objects dataset [20]. In this case, we use our proposed semi-automatic texture
guided segmentation approach. We show results in Figure 9 and refer to the
supplementary material for more details and results. In the supplementary, we
also show relighting and editing results.
We ablate certain design choices in our material search algorithm. First, in-
stead of using a combination of CLIP image and color similarity, we use only
CLIP similarity as our visual metric (w/o color). Next, instead of category spe-
cific search, we perform only visual search within the whole material database
(w/o category). As shown in Figure 10, we find our method provides a good
balance between plausibility and quality. When working with locally cropped
patches from different parts of the 3D model, the CLIP image similarity may
hsem
detnemges
tuptuo
gnirutxet
tuptuo
tnemngissa
lairetam14 D. Ceylan et al.
Objaverse Google Scanned Objects
segmentation texturing material assignment segmentation texturing material assignment
Fig.9: Weevaluateourmethodon3DmodelsfromtheObjaversedatasetwhichcome
asdisconnectedcomponentsaswellasscannedmodelsintheGoogleScannedObjects
dataset where we use a semi-automatic segmentation method.
“beverage “cap”: plastic
carton”
granite
(a) textured (b) segmentation (c) our result (d) w/o color (e) w/o category
Fig.10: Material retrieval comparison. (a) Given a textured mesh, (b) we rely
on LLMs to get material category suggestions for each segment of the object. (c) We
thenperformcategoryspecificmaterialretrievalusingvisualcues.(d)Ifwedonotuse
explicitcolorsimilarityforsearch,weobservethatretrievalresultsarenotsatisfactory
especially for small regions. (e) While omitting material category suggestions might
result in visually similar material retrieval results, they often do not correspond to
realistic suggestions.
not be as sensitive to color, sometimes resulting in color differences between
the RGB texture and the retrieved material. Omitting the category label in the
search results in unrealistic material suggestions. We provide further discussion
in the supplementary.
5 Conclusion
Wepresentamethodtogeneraterelightabletexturesfor3Dmodelsgivenatext
prompt. Our method uses a pre-trained text-to-image generation model in the
first stage to generate a consistent and high quality RGB texture. We then use
this texture in conjunction with priors from a large language model to retrieve
materials from a material database to assign to different parts of the model. We
compare our texturing results against state of the art and demonstrate superior
performance.Wealsoshowthatunlikepreviouswork,ourmethodprovidesfully
relightable and editable outputs, highly desired in many applications.
Whilepromising,ourmethodalsohaslimitationsofinterestforfuturework.
Whilethecross-frameattentionstrategygreatlyimprovestheconsistencyofthe
textures, we still observe that generations across different views might differ in
the appearance of the details. Recent methods have demonstrated successful re-
sults in finetuning the image generation models to output consistent multi view
images [37,57]. Using such finetuned models alongwith depth and line artguid-
ance is a promising direction to pursue. While we can output high quality andMatAtlas: Text-driven Consistent Geometry Texturing 15
relightableassetsbymatchingthegeneratedtexturestomaterialsinadatabase,
we are not able to capture certain details like logos or dirt that exist in the gen-
erated textures. Recovering such details as an additional layer is an interesting
future direction.
References
1. Stablediffusion.https://github.com/Stability-AI/stablediffusion,accessed:
2023 10
2. Adobe: Substance 3D Assets. https://substance3d.adobe.com/assets/ (2023)
8, 10
3. AmbientCG: https://www.ambientcg.com/ (2023) 8
4. Avrahami,O.,Lischinski,D.,Fried,O.:Blendeddiffusionfortext-driveneditingof
naturalimages.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR). pp. 18208–18218 (June 2022) 6
5. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: PatchMatch: A ran-
domizedcorrespondencealgorithmforstructuralimageediting.ACMTransactions
on Graphics (Proc. SIGGRAPH) 28(3) (Aug 2009) 8
6. Berkiten, S., Halber, M., Solomon, J., Ma, C., Li, H., Rusinkiewicz, S.: Learning
detailtransferbasedongeometricfeatures.ComputerGraphicsForum36,361–373
(05 2017). https://doi.org/10.1111/cgf.13132 3
7. Bradski,G.:TheOpenCVLibrary.Dr.Dobb’sJournalofSoftwareTools(2000) 6
8. Cao, T., Kreis, K., Fidler, S., Sharp, N., Yin, K.: Texfusion: Synthesizing 3d tex-
tures with text-guided image diffusion models. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 4169–4181 (2023) 2, 4
9. Ceylan,D.,Huang,C.H.,Mitra,N.J.:Pix2video:Videoeditingusingimagediffu-
sion (2023) 6
10. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015) 4
11. Chen, D.Z., Siddiqui, Y., Lee, H.Y., Tulyakov, S., Nießner, M.: Text2tex: Text-
driven texture synthesis via diffusion models. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV). pp. 18558–18568 (October
2023) 1, 2, 4, 10, 11
12. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry
and appearance for high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 (2023) 4
13. Chen,X.,Funkhouser,T.,Goldman,D.B.,Shechtman,E.:Non-parametrictexture
transfer using MeshMatch. Adobe Technical Report 2012-2 (Nov 2012) 3
14. Community,B.O.:Blender-a3Dmodellingandrenderingpackage.BlenderFoun-
dation, Stichting Blender Foundation, Amsterdam (2018), http://www.blender.
org 8
15. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,
Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of
annotated3dobjects.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 13142–13153 (2023) 8, 1016 D. Ceylan et al.
16. Deng, K., Omernick, T., Weiss, A., Ramanan, D., Zhu, J.Y., Zhou, T., Agrawala,
M.: Flashtex: Fast relightable mesh texturing with lightcontrolnet. arXiv preprint
arXiv:2402.13251 (2024) 5
17. Deschaintre,V.,Drettakis,G.,Bousseau,A.:Guidedfine-tuningforlarge-scalema-
terialtransfer.ComputerGraphicsForum(ProceedingsoftheEurographicsSym-
posium on Rendering) 39(4) (2020), http://www-sop.inria.fr/reves/Basilic/
2020/DDB20 8
18. Deschaintre, V., Guerrero-Viu, J., Gutierrez, D., Boubekeur, T., Masia, B.: The
visual language of fabrics. ACM Trans. Graph. (2023) 9
19. Dong, W., Zhou, N., Paul, J.C.: Optimized tile-based texture synthesis. In: Pro-
ceedings of Graphics Interface 2007. pp. 249–256 (2007) 3
20. Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K.,
McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of
3dscannedhouseholditems.In:2022InternationalConferenceonRoboticsandAu-
tomation (ICRA). p. 2553–2560. IEEE Press (2022). https://doi.org/10.1109/
ICRA46639.2022.9811809, https://doi.org/10.1109/ICRA46639.2022.9811809
13
21. Efros,A.A.,Freeman,W.T.:Imagequiltingfortexturesynthesisandtransfer.In:
Proceedings of the 28th annual conference on Computer Graphics and Interactive
Techniques. pp. 341–346 (2001) 3
22. Efros,A.A.,Leung,T.K.:Texturesynthesisbynon-parametricsampling.In:Pro-
ceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV).
vol. 2, pp. 1033–1038. IEEE (1999) 3
23. Gao,W.,Aigerman,N.,Thibault,G.,Kim,V.,Hanocka,R.:Textdeformer:Geom-
etry manipulation using text guidance. In: ACM Transactions on Graphics (SIG-
GRAPH) (2023) 3
24. Geyer, M., Bar-Tal, O., Bagon, S., Dekel, T.: Tokenflow: Consistent diffusion fea-
tures for consistent video editing. In: arXiv (2023) 6
25. Heeger,D.,Bergen,J.:Pyramid-basedtextureanalysis/synthesis.In:Proceedings.,
International Conference on Image Processing. vol. 3, pp. 648–651 vol.3 (1995).
https://doi.org/10.1109/ICIP.1995.537718 3
26. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.In:Proceed-
ings of the 31st International Conference on Neural Information Processing Sys-
tems.p.6629–6640.NIPS’17,CurranAssociatesInc.,RedHook,NY,USA(2017)
11
27. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 4
28. Höllein, L., Cao, A., Owens, A., Johnson, J., Nießner, M.: Text2room: Extract-
ing textured 3d meshes from 2d text-to-image models. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision (2023) 4
29. Kholgade, N., Simon, T., Efros, A., Sheikh, Y.: 3d object manipulation in a sin-
gle photograph using stock 3d models. ACM Transactions on Computer Graphics
33(4) (2014) 3
30. Kopf, J., Fu, C.W., Cohen-Or, D., Deussen, O., Lischinski, D., Wong, T.T.: Solid
texturesynthesisfrom2dexemplars.ACMTransactionsonGraphics(Proceedings
of SIGGRAPH 2007) 26(3), 2:1–2:9 (2007) 3
31. Kraevoy,V.,Sheffer,A.,Gotsman,C.:Matchmaker:Constructingconstrainedtex-
ture maps. ACM Trans. Graph. 22(3), 326–333 (jul 2003). https://doi.org/10.
1145/882262.882271, https://doi.org/10.1145/882262.882271 3MatAtlas: Text-driven Consistent Geometry Texturing 17
32. Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-
based synthesis. In: ACM Transactions on Graphics (ToG). vol. 24, pp. 795–802.
ACM (2005). https://doi.org/10.1145/1073204.1073263 3
33. Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: image
andvideosynthesisusinggraphcuts.ACMTransactionsonGraphics(ToG)22(3),
277–286 (2003) 3
34. Lefebvre,S.,Hoppe,H.:Appearance-spacetexturesynthesis.ACMTrans.Graph.
25, 541–548 (07 2006). https://doi.org/10.1145/1141911.1141921 3
35. Li, W., Chen, R., Chen, X., Tan, P.: Sweetdreamer: Aligning geometric priors in
2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596 (2023) 4
36. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 300–309 (2023) 4
37. Liu,R.,Wu,R.,VanHoorick,B.,Tokmakov,P.,Zakharov,S.,Vondrick,C.:Zero-
1-to-3:Zero-shotoneimageto3dobject.In:ProceedingsoftheIEEE/CVFInter-
national Conference on Computer Vision. pp. 9298–9309 (2023) 14
38. Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H.,
Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-
domain diffusion. arXiv preprint arXiv:2310.15008 (2023) 4
39. Lu, J., Georghiades, A.S., Glaser, A., Wu, H., Wei, L.Y., Guo, B., Dorsey, J.,
Rushmeier, H.: Context-aware textures. ACM Transactions on Graphics (TOG)
26(1), 3–es (2007) 3
40. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:SDEdit:Guided
imagesynthesisandeditingwithstochasticdifferentialequations.In:International
Conference on Learning Representations (2022) 7
41. Mertens, T., Kautz, J., Chen, J., Bekaert, P., Durand, F.: Texture transfer us-
inggeometrycorrelation.In:Proceedingsofthe17thEurographicsConferenceon
Rendering Techniques. p. 273–284. EGSR ’06, Eurographics Association, Goslar,
DEU (2006) 3
42. Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf
for shape-guided generation of 3d shapes and textures. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12663–
12673 (2023) 4
43. Michel,O.,Bar-On,R.,Liu,R.,Benaim,S.,Hanocka,R.:Text2mesh:Text-driven
neural stylization for meshes. In: Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR).pp.13492–13502(June2022)
3, 11
44. Mohammad Khalid, N., Xie, T., Belilovsky, E., Popa, T.: Clip-mesh: Generating
textured meshes from text using pretrained image-text models. In: SIGGRAPH
Asia 2022 conference papers. pp. 1–8 (2022) 3, 11
45. Oechsle, M., Mescheder, L., Niemeyer, M., Strauss, T., Geiger, A.: Texture
fields: Learning texture representations in function space. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 4531–4540 (2019)
3
46. OpenAI: Gpt-4v(ision) system card. Tech. rep., OpenAI, https://cdn.openai.
com/papers/GPTV_System_Card.pdf 2, 9
47. Park,K.,Rematas,K.,Farhadi,A.,Seitz,S.M.:Photoshape:Photorealisticmate-
rials for large-scale shape collections. ACM Trans. Graph. 37(6) (Nov 2018) 3
48. Phong, B.T.: Illumination for computer generated pictures pp. 95–101 (1998) 618 D. Ceylan et al.
49. PolyHaven: https://www.polyhaven.com/ (2023) 8
50. Poole,B.,Jain,A.,Barron,J.T.,Mildenhall,B.:Dreamfusion:Text-to-3dusing2d
diffusion. In: The Eleventh International Conference on Learning Representations
(2022) 4
51. Portilla, J., Simoncelli, E.P.: A parametric texture model based on joint statistics
of complex wavelet coefficients. International Journal of Computer Vision 40(1),
49–70 (2000) 3
52. Qian,G.,Mai,J.,Hamdi,A.,Ren,J.,Siarohin,A.,Li,B.,Lee,H.Y.,Skorokhodov,
I.,Wonka,P.,Tulyakov,S., etal.:Magic123: Oneimageto high-quality 3d object
generationusingboth2dand3ddiffusionpriors.arXivpreprintarXiv:2306.17843
(2023) 4
53. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022) 1
54. Richardson, E., Metzer, G., Alaluf, Y., Giryes, R., Cohen-Or, D.: Texture: Text-
guided texturing of 3d shapes. In: ACM SIGGRAPH 2023 Conference Proceed-
ings. SIGGRAPH ’23, Association for Computing Machinery, New York, NY,
USA (2023). https://doi.org/10.1145/3588432.3591503, https://doi.org/
10.1145/3588432.3591503 1, 2, 4, 8, 10, 11
55. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684–10695(2022) 1,
2, 3, 4, 5
56. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation
(2022) 4
57. Shi,R.,Chen,H.,Zhang,Z.,Liu,M.,Xu,C.,Wei,X.,Chen,L.,Zeng,C.,Su,H.:
Zero123++: a single image to consistent multi-view diffusion base model. arXiv
preprint arXiv:2310.15110 (2023) 14
58. Shi,Y.,Wang,P.,Ye,J.,Mai,L.,Li,K.,Yang,X.:Mvdream:Multi-viewdiffusion
for 3d generation. arXiv:2308.16512 (2023) 6
59. Siddiqui,Y.,Thies,J.,Ma,F.,Shan,Q.,Nießner,M.,Dai,A.:Texturify:Generat-
ing textures on 3d shape surfaces. In: European Conference on Computer Vision.
pp. 72–88. Springer (2022) 3
60. Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d:
Hierarchical 3d generation with bootstrapped diffusion prior (2023) 4
61. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
4
62. Tang, Z., He, T.: Text-guided high-definition consistency texture model. arXiv
preprint arXiv:2305.05901 (2023) 4
63. Tsalicoglou, C., Manhardt, F., Tonioni, A., Niemeyer, M., Tombari, F.:
Textmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint
arXiv:2304.12439 (2023) 4
64. Turk,G.:Texturesynthesisonsurfaces.In:Proceedingsofthe28thAnnualConfer-
ence on Computer Graphics and Interactive Techniques. p. 347–354. SIGGRAPH
’01, Association for Computing Machinery, New York, NY, USA (2001). https:
//doi.org/10.1145/383259.383297, https://doi.org/10.1145/383259.383297
3MatAtlas: Text-driven Consistent Geometry Texturing 19
65. Tzur,Y.,Tal,A.:Flexistickers:Photogrammetrictexturemappingusingcasualim-
ages. ACM Trans. Graph. 28(3) (jul 2009). https://doi.org/10.1145/1531326.
1531351, https://doi.org/10.1145/1531326.1531351 3
66. Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining:
Lifting pretrained 2d diffusion models for 3d generation. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12619–
12629 (2023) 4
67. Wang,T.,Kanakis,M.,Schindler,K.,VanGool,L.,Obukhov,A.:Breathingnew
lifeinto3dassetswithgenerativerepainting.In:ProceedingsoftheBritishMachine
Vision Conference (BMVC). BMVA Press (2023) 4
68. Wang, T.Y., Su, H., Huang, Q., Huang, J., Guibas, L., Mitra, N.J.: Unsuper-
visedtexturetransferfromimagestomodelcollections.ACMTrans.Graph.35(6),
177:1–177:13 (2016), http://doi.acm.org/10.1145/2980179.2982404 3
69. Wang,Z.,Lu,C.,Wang,Y.,Bao,F.,Li,C.,Su,H.,Zhu,J.:Prolificdreamer:High-
fidelity and diverse text-to-3d generation with variational score distillation. arXiv
preprint arXiv:2305.16213 (2023) 4
70. Wei, L.Y., Lefebvre, S., Kwatra, V., Turk, G.: State of the art in example-based
texture synthesis. Eurographics 2009, State of the Art Report, EG-STAR pp. 93–
117 (2009) 3
71. Wei, L.Y., Levoy, M.: Texture synthesis over arbitrary manifold surfaces. In: Pro-
ceedings of the 28th Annual Conference on Computer Graphics and Interactive
Techniques. p. 355–360. SIGGRAPH ’01, Association for Computing Machin-
ery, New York, NY, USA (2001). https://doi.org/10.1145/383259.383298,
https://doi.org/10.1145/383259.383298 3
72. Yan, K., Luan, F., Hašan, M., Groueix, T., Deschaintre, V., Zhao, S.: Psdr-room:
Single photo to scene using differentiable rendering. In: ACM SIGGRAPH Asia
2023 Conference Proceedings (2023) 9
73. Yeh, Y.Y., Huang, J.B., Kim, C., Xiao, L., Nguyen-Phuoc, T., Khan, N.,
Zhang, C., Chandraker, M., Marshall, C.S., Dong, Z., et al.: Texturedreamer:
Image-guided texture synthesis through geometry-aware diffusion. arXiv preprint
arXiv:2401.09416 (2024) 4
74. Youwang,K.,Oh,T.H.,Pons-Moll,G.:Paint-it:Text-to-texturesynthesisviadeep
convolutional texture map optimization and physically-based rendering. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (June 2024) 4
75. Yu, X., Dai, P., Li, W., Ma, L., Liu, Z., Qi, X.: Texture generation on 3d meshes
with point-uv diffusion. In: Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision. pp. 4206–4216 (2023) 3
76. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836–3847 (2023) 2, 4, 6, 10