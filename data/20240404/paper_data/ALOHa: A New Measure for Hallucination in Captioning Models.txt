ALOHa: ANewMeasureforHallucinationinCaptioningModels
SuzannePetryk*,DavidM.Chan∗,AnishKachinthaya,HaodiZou,
JohnCanny,JosephE.Gonzalez,TrevorDarrell
UniversityofCalifornia,Berkeley
{spetryk,davidchan,anishk,haodi.zou,canny,jegonzal,trevordarrell}@berkeley.edu
https://davidmchan.github.io/aloha
Abstract
Despite recent advances in multimodal pre-
trainingforvisualdescription,state-of-the-art
modelsstillproducecaptionscontainingerrors,
suchashallucinatingobjectsnotpresentina
scene.Theexistingprominentmetricforobject
hallucination,CHAIR,islimitedtoafixedsetof
MSCOCOobjectsandsynonyms.Inthiswork,
we propose a modernized open-vocabulary
metric,ALOHa,whichleverageslargelanguage
Figure1:(Top)TheSOTApriorobjecthallucinationmetric,
CHAIR,islimitedtoMSCOCOobjects,andfailstodetect
models (LLMs) to measure object hallucina-
the hallucinations in this image caption while ALOHa
tions. Specifically,weuseanLLMtoextract
(ours,bottom)correctlyassignslowsimilarityscorestothe
groundable objects from a candidate caption, hallucinations“baseballplayer”and“bat”.ALOHadoesnot
measuretheirsemanticsimilaritytoreference penalizethecaptionfor“catcher”,“umpire”,and“bassdrum”,
objects from captions and object detections, asthecaptionindicatesuncertaintyoftheirpresence.
anduseHungarianmatchingtoproduceafinal
uatingcaptionsfromawiderangeofinputdatasets,
hallucination score. We show that ALOHa
acrossawiderangeofobjectandentitycategories.
correctlyidentifies13.6%morehallucinatedob-
jectsthanCHAIRonHAT,anewgold-standard Recentworksthatmeasureobjecthallucinations
subsetofMSCOCOCaptionsannotatedforhal- ingeneratedtextgenerallyfallintotwocategories:
lucinations,and30.8%moreonnocaps,where
measures that find hallucinations by explicitly
objectsextendbeyondMSCOCOcategories.
matchingfromasetofobjects,andmeasuresthat
compute distances between latent image and/or
1 IntroductionandBackground
textembeddings, indicatingahallucinationifthe
embeddings are too distant. In the first category,
In recent years, vision-language models have
CHAIR(Rohrbachetal.,2018)isameasurethat
demonstrated remarkable performance. Unfortu-
explicitlyextractsobjectsfromcandidatesentences
nately, even state-of-the-artmodels for visualde-
using simple string matching against MS COCO
scriptionstillgeneratecaptionswithobjecthalluci-
classesandasmallsetofsynonyms. Itcompares
nations–objectsorentitiesthatarepresentinthe
these extracted objects against the ground truth
captionyetarenotexplicitlysupportedbyvisual
detections and objects extracted from the ground
evidenceintheimage(Daietal.,2023). Inorder
truthreferencecaptions. CHAIRisbothreliable,
toreducetheoccurrenceofobjecthallucinationsin
as string matching on a fixed set of objects is
vision-languagemodels,itishelpfultounderstand
accurate,consistent,andlocalizable,asindividual
andquantifytheproblemthroughreliable,localiz-
non-matchingstringsareidentified. However, as
able,andgeneralizablemeasuresofobjecthalluci-
seeninFigure1,CHAIRisnotgeneralizable,asit
nation.Reliablemeasuresarecapableofcorrectly
canonlyhandleafixedsetofpredeterminedobjects.
indicatingifagivencaptioncontainsanobjecthallu-
Otheruni-modalmeasuresinthiscategoryinclude
cination.Localizablemeasuresarecapableofindi-
thoseforabstractivesummarization(Durmusetal.,
catingwhichobjectinaparticularcaptionishalluci-
2020;Kryscinskietal.,2020;Maynezetal.,2020;
nated.Generalizablemeasuresarecapableofeval-
Son et al., 2022; Sridhar and Visser, 2022; Yuan
∗*Indicatesequalauthorship. etal.,2021),dialogue(Huangetal.,2022;Shuster
4202
rpA
3
]VC.sc[
1v40920.4042:viXraet al., 2021), and structured knowledge (Dhingra setofground-truthreferencecaptionsandpredicted
et al., 2019). These often generalize poorly to (orgroundtruth)imageobjectdetections.ALOHa
vision-languagetasksastheyrequiregroundingthe consists of three stages (Figure 2). (1) Objects
generatedtextintoinputsofthesamemodality. are extracted from the image, reference set, and
candidatecaptionusingacombinationofanobject
In the second category, CLIPScore (Hessel
detector and LLM. (2) We filter the object sets
etal.,2021)employsCLIP(Radfordetal.,2021)
and compute semantic representations of each
embeddingstoassessimage-textmatches. While
object. (3) We compute a maximum-similarity
itisgeneralizableandreliable,itlackslocalization
linearassignmentbetweencandidateandreference
as it does not pinpoint incorrect spans of text.
objects. The scores from each of the pairs in the
CLIPBERTS (Wan and Bansal, 2022) and Ref-
linearassignment,whichwecallALOHa ,measure
CLIPScore(anextensionofCLIPScoreaccounting o
thedegreeofhallucinationforeachofthecandidate
forreferencecaptions)facesimilarlimitations.
objects. The minimum similarity in this linear
POPE(Lietal.,2023)evaluatesvision-language
assignment(theALOHascore)measuresthedegree
models’ likelihood to hallucinate objects with
ofhallucinationofthecaption.
machine-generatedqueriesconsistingofsamples
(1) Extracting objects from candidates, refer-
extracted from both reference object detections
ences, and images: Parsing visually grounded
andnonexistentobjects,butaddressesadifferent
objectsinacaptioninanopen-domaincontextisa
problem from that which we investigate here – it
surprisinglydifficulttask.CHAIR(Rohrbachetal.,
measureshowoftenmodelshallucinateratherthan
2018)reliesonafixedsetofMSCOCOobjectsand
localizesanddetectsissueswithinasinglecaption.
synonyms,requiringconsiderableefforttoextendto
Inspired by recent successes using LLMs for
otherdatasets,andsometimesfailingatambiguous
evaluation in language-only tasks (Zhang et al.,
parses(suchasmistakingtheadjective“orange”for
2020;Yuanetal.,2021;Bubecketal.,2023;Chiang
anoun). SPICE(Andersonetal.,2016)relieson
et al., 2023; Zheng et al., 2023), we introduce
standardgrammar-basedobjectparsing,whichcan
Assessment with Language models for Object
havesimilarissues,aspurelytext-basedmethods
Hallucination (ALOHa), a modernized measure
fallshortatidentifyingwhichnounsarevisual–for
for object hallucination detection that is reliable,
instance,avoiding“picture”and“background”in
localizable,andgeneralizable.ALOHaextendsthe
Figure2. Captionsmayalsoindicateuncertainty
reliabilityandlocalizationofCHAIRtonewinput
aroundobjectpresence,suchas“abowlorplate”,
domainsbyleveragingin-contextlearningofLLMs
or“adogbitingsomething,possiblyaFrisbee.”We
combinedwithsemanticallyrichtextembeddings
aimtohandlesuchuncertainobjectstoavoidunfair
forobjectparsingandmatching(Figure1).
hallucinationpenalties.
Foragivenimagecaption,wegeneratetwomea-
With the understanding that open-domain
sures:ALOHa ,anumericscoreforeachobjectrat-
o parsing is the primary factor in CHAIR’s lack
ingthedegreetowhichthatobjectisahallucination,
of generalization, we leverage the capability of
andALOHa,anaggregatedscoreratingthedegree
zero-shot in-context learning in large language
towhichthewholecaptioncontainsahallucination.
models.FollowingBrownetal.(2020),weusean
We demonstrate ALOHa on a new gold-standard
LLM (ChatGPT, OpenAI (2022)) along with the
datasetofimagehallucinations,HAT,andshowthat
prompt given in Appendix A to turn the parsing
ALOHa improves on CLIPScore while detecting
taskintoalanguagecompletiontaskeasilysolvable
objecthallucinations,andCHAIRwhilecorrectly
by an LLM. We encourage the LLM to extract
localizing those hallucinations. We conclude by
visualobjectsinthescene,consistingprimarilyof
demonstratingthatALOHaremainsreliableandlo-
nounphrases(includinganyattributes,suchas“big
calizablewhengeneralizingtoout-of-domaindata.
dog” and “purple shirt”), from the candidate and
reference captions. We run the LLM against the
2 ALOHa: Reliable,Localizable,and
candidatecaptiontoproducetheunfilteredobject
GeneralizableHallucinationDetection
set C, and again for the corresponding reference
captionstoproduceobjectsetR.Toextractobjects
ALOHaproducesnumericscoresratingthedegree
from the image context, similar to CHAIR, we
of hallucination for each object in a candidate
augmentthesetofreferenceobjectswithobjects
captionaswellasanoverallcaptionscore,givenaFigure2:OverviewofALOHa.WepromptanLLMtoextractvisuallygroundednounsfromacandidate’smachine-
generateddescriptionandasetofreferences.Weconsideruncertainlanguage(e.g.,“goatorsheep”),addreference
objectswithandwithoutmodifiers(e.g.,both“field”and“grassyfield”),andavoidnon-visualnouns(e.g.,“picture”
and“background”).Then,wecomputeamaximum-similaritylinearassignmentbetweencandidateandreference
objectsets,theweightsofwhichformtheALOHa .MatchedpairswithlowALOHa arelikelyhallucinations(e.g.,
o o
“blackcat”,ALOHa =0.2).WeadditionallyoutputtheminimumALOHa asacaption-levelALOHascore.
o o
detected directly from the image using DETR generateo =ϕ(o)∈RK,whereϕisasemantic
emb
(Carionetal.,2020)fine-tunedonMSCOCO. text embedding model. In our work, we use
S-BERT(ReimersandGurevych,2019). Wethen
(2)Objectfiltering: Wefurtherrefinecandidate
computeasimilarityscoreforeachpairofobjects
(C) and reference (R) object sets to better reflect
(usuallythecosinesimilarity, seeAppendixB.2).
specificchallengesofobjecthallucinationdetection.
For each (C ,R ) pair, we store these scores in
Ideally, hallucination measures should penalize i j
specificity when candidate attributes are not
a similarity matrix S
i,j
∈ [0,1]|Ci|×|Rj|. We then
use the Hungarian method (Kuhn, 1955) to find
supportedbyreferences(e.g.,if“purpleshirt”∈C,
anoptimalmaximum-similarityassignmentM
yet “white shirt” ∈ R), but should not penalize i,j
betweencandidateandreferencesetsofobjects.
generality(e.g.,“shirt”∈C,yet“whiteshirt”∈R).
Thus, we use spaCy (Honnibal et al., 2020a) to TodeterminetheALOHa scoreforeachobject,
o
augmentRwiththerootnounsfromeachreference we take the maximum score across all possible
nounphrase,butleavethecandidatesunchanged. parsings, givingthecandidatecaptionthebenefit
ofthedoubt,foranobjectc∈C
Beyond specificity, captions may also express i
uncertainty about the presence of objects in an
ALOHa (c)=maxw ∈M (1)
image. For conjunctions (e.g., “fork or knife”),
o
i,j
ci,j i,j
we aim to avoid unfair penalties if at least one
While 0 ≤ ALOHa ≤ 1 indicates the degree of
oftheobjectsisgrounded. ALOHaconsidersall o
hallucination for each object, we also want to
combinationsofselectingasingleobjectfromeach
indicateifanentirecaptioncontainsahallucination.
conjunction,denotedasC andR (e.g.,
{1...M} {1...N} Wethusdefine:
“fork”∈R and“knife”∈R ). Additionally,we
0 1
prompt theLLM toindicate uncertain grounding
ALOHa=minALOHa (c) (2)
o
byincluding“possibly”aftertheobject(e.g.,“there c∈C
maybeaFrisbee”becomes“Frisbee(possibly)”)
We choose the minimum as the presence of any
and we remove uncertain objects from C to
i hallucinatedobjectindicatesthatthefullcaptionis
avoidpenaltieswhilemaintainingtheminR for
j ahallucination,andevenseveralcorrectdetections
maximumcoverageofmoregeneralobjects.
shouldnotcompensateforahallucination.
(3)ObjectMatching:Oncewehaveextractedand
parsedthecandidateandreferenceobjectsets,we 3 Evaluation&Discussion
aimtomeasurethedegreeofhallucinationforeach
candidate object. While we could match objects HAT:Topromotethedevelopmentofhigh-quality
basedonstringalone(resultinginabinarydecision), methods for hallucination detection, we collect
as does CHAIR, often it is useful to understand andreleaseHAT(HAllucinationTest),adatasetof
a continuous scale of hallucination – e.g., for a labeledhallucinationsincaptions.HATconsistsof
referenceobject“dog”,hallucinating“wolf”should 490samples(90validationand400test)labeledby
bepenalizedlessthan“potato.”Tocapturethisscale in-domainexpertsforhallucinationonbothaword
of semantic similarity, for each object text o, we levelandcaptionlevel(SeeAppendixC).Measures
are evaluated on two metrics: Average PrecisionMethod LA AP
Baseline(MajorityVote) - 33.75
CHAIRs 6.70 36.85
CLIPScore - 40.10
RefCLIPScore - 48.40
ALOHa(NoSoftObjectMatching) 18.66 47.27
ALOHa(NoDetections) 19.55 48.40
ALOHa(OracleDetections) 19.55 47.86
ALOHa(DETRDetections)* 20.30 48.62
ALOHa(Oracle+DETRDetections) 21.05 48.78
Table1: Testsetperformanceforbinaryhallucination
detectiononHAT.LA:LocalizationAccuracy.AP:Av-
eragePrecision.*indicatestheversionofALOHaused
throughoutthispaper,unlessnotedotherwise. Oracle
detectionarehuman-generatedreferencedetections.
(AP) and Localization Accuracy (LA). The AP
of the method measures reliability and is defined
as how well the measure identifies captions with
Figure 3: Qualitative Flickr30k examples. (Left)
hallucinations. ForCHAIR,decisionsarebinary,
ALOHacorrectlyassignslowscorestothehallucinated
soAP=accuracy.ForALOHa,APistheweighted
“nun” and “bread”, whereas CHAIR does not detect
meanofprecisionsacrossallthresholds. TheLA,
anyhallucinations. (Right)AlthoughALOHaassigns
measured on samples containing hallucinations
highsimilaritybetweenthehallucinated“electricguitar”
in HAT, measures localization and is defined as andreference“(acoustic)guitar”,itassignslowscores
the accuracy of correctly indicating which of the totheother3hallucinations. CHAIRdetectsonlythe
specificobjectswerehallucinated. ForCHAIR,a hallucination“chair”,missingtheothers.
hallucination is correctly localized when at least
one detected string mismatch is a hallucination, dataset.Theresultsillustrateasubtleresult.While
andforALOHawhentheminimumALOHa score ALOHaunder-performsCHAIRsinbothAPand
o
correspondstoahallucinatedobject. LA on the original FOIL dataset, this is because
FOILconstructsnewsamplesbyreplacingstring-
ALOHa’s performance on HAT is shown in
matchedCOCOobjectswithasetofhand-selected
Table 1. On AP, ALOHa with DETR detections
“foil”objects(nearsemanticneighbors). Thisisa
outperformsbothCHAIRandCLIPScoreby11.8%
best-casescenarioforCHAIR,asCHAIRrelieson
and 8.5% respectively. RefCLIPScore attains a
fixedobject-setstringmatchingalone,andthus,is
similar AP; however, is not localizable. ALOHa
easilyabletobothdetectandlocalizethereplaced
achievesmorethantwicetheLAonHATCHAIR,
samples.Whenwemovetonocaps-FOILwithnon-
a particularly challenging task as HAT includes
MS COCO data, however, ALOHa significantly
non-objecthallucinations,suchasincorrectverbs
outperformsCHAIR,asnowtheobjectsetthatwas
orrelations(seeFigureA6).Table1furtherablates
astrengthforin-domainFOILbecomesaliability,
the choice of image detections and indicates that
andCHAIRisunabletodetectanyhallucinations
ALOHaisrobusttomissingdetections.
at all, due to the restricted string matching. Ref-
FOIL object hallucinations: To indicate gen-
CLIPScore,whilecompetitiveinthehallucination
eralizability we evaluate our method on two
detectiontask,cannotperformlocalization.
machine-generated object hallucination datasets.
Qualitative Examples - Flickr30k: In Figure 3
FOIL (Shekhar et al., 2017) contains MS COCO
andFigureA4,wevisualizethebehaviorofCHAIR
images, where objects are randomly replaced
andALOHaonseveralFlickr30ksamples(Young
with similar ones (e.g., “bus“ and “car”), and
etal.,2014),usingcaptionsgeneratedbyarecent
nocaps-FOIL, a similar dataset that we construct
captioning model (Chan et al., 2023) that often
on the nocaps dataset (Agrawal et al., 2019)
producescomplexcaptionswithphrasesexpressing
for novel object captioning beyond MS COCO
uncertainty.
(see Appendix C.1). Table 2 breaks down the
resultsofALOHaontheFOILandnocaps-FOIL Ablation-ChoiceofLLM:ThelanguagemodelFOIL nocaps-FOIL
Overall In-Domain Near-Domain Out-Domain Overall
Method LA AP LA AP LA AP LA AP LA AP
Baseline(MajorityVote) - 50.00 - 50.00 - 50.00 - 50.00 - 50.00
CHAIRs 79.00 92.50 13.47 57.82 17.55 59.14 12.24 58.06 14.42 58.33
CLIPScore - 76.44 - 71.81 - 70.17 - 78.73 - 73.48
RefCLIPScore - 80.64 - 79.63 - 78.70 - 85.89 - 81.31
ALOHa 40.00 61.35 47.35 71.80 47.30 66.67 48.84 70.91 45.17 69.52
Table2: BreakdownofresultsbydomainonFOILandnocapsFOIL.AP:AveragePrecision. LA:Localization
Accuracy.Boldandunderlinedvaluesrepresentthebestandsecond-bestmethodsrespectively.
is critical to the overall performance of ALOHa-
Parser = POS Tagger Parser = LLM (Ours)
language models with insufficient zero-shot
parsingcapabilitywillsufferreduceddownstream StringMatch 20.7
performance. We investigate the performance of GPT3 24.3
Word2Vec 28.1
thelanguagemodelinTable3onHAT.Inaddition
S-BERT 20.2
to LA and AP, we also measure “Parsing error
StringMatch 16.4
rate"(PER),whichistherateoferrorsmadewhen GPT3 26.1
parsing objects from reference captions on HAT, Word2Vec 28.0
and“Parsingrecallrate(PRR),whichistherecall S-BERT 45.7
rateofobjectsinthecaptions(SeeAppendixB.1). 0 10 20 30 40 50
Average Precision (%)
Ablation-ObjectExtractionandSemanticEm- Figure4: PerformanceonHATvalidationsetfiltered
beddingMethods: Inthethiswork,weleverage forhallucinatedobjects, whencomparingembedding
methodsandobjectextractionapproaches.
LLMs(OpenAI,2023)forobjectextraction,anda
BERT-basedmodel(ReimersandGurevych,2019)
for semantic word embedding. In Figure 4, we
LanugageModel LA↑ AP↑ PER↓ PRR↑
explore the difference in overall performance on
GPT-3.5 20.30 48.62 2.97 98.63
HAT’svalidationsetwhenusingdifferentcombi-
Claude(Instant) 20.74 41.48 3.31 -
nations of object extraction and semantic embed- Koala 22.22 38.70 5.07 -
ding.Namely,wecompareLLM-basedextraction
to the parse-tree-based noun extraction in SpaCy Table 3: Exploration of LLM choice for parsing
withinALOHa, onHAT.AP:AveragePrecision, LA:
(Honnibal et al., 2020b), and compare Sentence-
Localization Accuracy, PER: Parsing Error Rate (%),
Transformer (BERT-Based model, (Reimers and
PRR:ParsingRecallRate.
Gurevych, 2019)) to Word2Vec (Mikolov et al.,
2018),GPT-3(Ada)embedding,andCHAIR-style
approaches,webelievethatleveragingalarge-scale
stringmatching(followingCHAIR,stringsarecase-
modeltrainedspecificallyforsemanticsimilarity
normalized and lemmatized). Combining LLMs
betweenwordswouldbeanexcitingandpowerful
with the SentenceTransformer (BERT-Based)
extensiontotheALOHaframework.
modeloutperformedothermethods,andfuzzyem-
beddingmethodsoutperformedexactstringmatch-
ing.Thisisgenerallyexpected:humanshaveawide
4 Conclusion
vocabularythatispoorlycapturedbyexactstring
matching. Word2VecoutperformsGPT-3embed-
dings. We believe that this is because the GPT-3 This paper introduces ALOHa, a scalable LLM-
embeddingsareoptimizedforsentence-levelstruc- augmented metric for open-vocabulary object
tures, and may fail to semantically embed single hallucination. ALOHacorrectlyidentifies13.6%
wordsinameaningfulway.Interestingly,S-BERT more hallucinated objects on HAT and 31% on
isnotawordsimilaritymeasureandwasinsteadde- nocaps-FOILthanCHAIR.ALOHarepresentsan
signedtomeasuredistancesbetweensentences(and importantmodernizationofcaptionhallucination
couldleadtoinaccuratesingle-wordjudgments)– metrics, and detecting complex hallucinations in
WhilewedidfindS-BERTmosteffectiveamongour actions,quantities,andabstractconceptsremainsan
excitingandchallengingtaskforfutureexploration.Limitations/EthicalConsiderations believetobesalientdetailsintheimage,whereas
reference-freemeasuresalwaysrelyondownstream
While ALOHa represents a strong step towards models which approximate what humans believe
open-domainlocalizedhallucinationdetection, it tobeimportant. Thismeansthatreference-based
comes with several limitations which we discuss measurescanoftentransferbettertonewdomains
inthissection. thanreference-freemeasures,whichoftenmustbe
trained/fine-tunedin-domainwithhuman-labeled
Non-determinism Aprimaryconcernwithusing datatoachievestrongperformance.
largelanguagemodelsforanevaluationmeasure
is the natural nondeterminism that comes with
them. Whileintheorylanguagemodelssampled
GeneralcostsassociatedwithLLMs Theuseof
at a temperature of zero (as we do in this work)
largelanguagemodelsforanytaskincurssignificant
aredeterministic,itiswelldocumentedthatsmall
compute, monetary, environmental, and human
randomfluctuationscanstilloccur(OpenAI,2023).
costs. ALOHaisasignificantlyslowerevaluation
Beyond random fluctuations, the availability of
measurethanmethodslikeCHAIR(howevernot
languagemodelslong-termcanimpacttherepro-
thatmuchlessefficientthanCLIPScore),leadingto
ducibilityofthemeasure.Inthiswork,weprimarily
increasedpowerconsumption,andcostduringeval-
relyonclosed-sourcelanguagemodels,whichcan
uation. Inaddition,themodelsthatwerelyonare
changeorbecomeunavailablewithoutnotice. In
generallyclosedsourceandrepresentanon-trivial
Table3,wedemonstratethatALOHastillfunctions
monetaryexpenditure(Experimentsinthispaper,in-
withopensourcemodelssuchasKoala(Gengetal.,
cludingablations,testing,andprototypingrequired
2023), however, the performance is significantly
approximatelyUSD$120inAPIfees).Suchfactors
degraded due to the parsing capabilities of the
canbelimitingtoresearcherswhowishtoevaluate
model.Withtime,andmorepowerfulopen-source
large datasets, however we hope that with the
LLMs,thiswillbecomelessofanissue,howeverre-
adventoflargeropen-sourcemodels,andcontinued
lyingonanondeterministicmetricforcomparative
investmentinhardwareandsystemsresearch,the
evaluationcaneasilybecomealiability.
costwilldecreasesignificantly. Beyondcompute
and financial costs, there are environmental and
AvailabilityofReferenceCaptions(Reference-
humancostsassociatedwithusinglargelanguage
Free vs. Reference-Based Measures) One of
modelsforevaluation,seeBenderetal.(2021)for
theprimarylimitationsoftheALOHaevaluation
adetaileddiscussionofthesefactors.
methodistherequirementthatreferencecaptions
are available for the evaluation dataset (an issue
shared by CHAIR). Not only must reference
captionsbeavailable,buttheyalsomustsufficiently Limited Control of Bias In this work, we do
cover the salient details in the reference image. notevaluatetheperformanceofALOHaonNon-
When the references are impoverished (as can Englishdata,nordoweexplicitlycontrolforormea-
easily happen with a single reference sentence surebiasinthecreationofHAT(Whichisalabeled
(Chanetal.,2023))orwhentherearenoreferences, subset,randomlyselectedoftheMSCOCOdataset),
andALOHamustrelyentirelyondetections, the or the Nocaps-FOIL dataset (which operates on
methodunder-performsmoregeneralmethodssuch thesamesamplesastheNocapsvalidationdataset).
as CLIPScore which are reference-free, and rely WhileHATisasubsetofthecommonMSCOCO
onalargepre-trainingdatasettoencodevisionand dataset,werecognizethatthecreationofsuchpo-
languagecorrespondences.Westronglybelievethat tentially biased datasets has the potential to lead
the area of reference-free localized hallucination researcherstoengineerfeaturesandmethodswhich
detection is an important area of future research; areunintentionallybiasedagainstunderrepresented
how can we leverage the tools from large vision groups.Weaimtoaddresstheseshortcomingsinthe
and language pre-training in a localized way to nextiterationofHAT,whichwillnotonlycontain
understandandinterpretwherehallucinationslie out-of-domaindataforMSCOCO-trainedmodels
in the hallucinated text? That being said, there butalsoaimstobettercontrolforbiasintheunder-
is also a place for reference-based measures, as lyingimageandcaptiondata. Notethatourwork,
reference-basedmeasuresfocusonwhathumans includingHAT,isintendedforresearchpurposes.Acknowledgements 23–28,2020,Proceedings,PartI16,pages213–229.
Springer.
We thank Dr. Kate Saenko for their helpful
David M Chan, Austin Myers, Sudheendra Vijaya-
comments on the work. Authors, as part of their
narasimhan, DavidARoss, andJohnCanny.2023.
affiliation with UC Berkeley, were supported Ic3: Image captioning by committee consensus.
in part by the NSF, DoD, and/or the Berkeley ArXivpreprint,abs/2302.01328.
ArtificialIntelligenceResearch(BAIR)industrial
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
allianceprogram,aswellasgiftsfromAnyscale,As- ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
tronomer,Google,IBM,Intel,Lacework,Microsoft, Zhuang,YonghaoZhuang,JosephEGonzalez,etal.
MohamedBinZayedUniversityofArtificialIntel- 2023. Vicuna: Anopen-sourcechatbotimpressing
gpt-4with90%*chatgptquality. Seehttps://vicuna.
ligence,SamsungSDS,Uber,andVMware.
lmsys.org(accessed14April2023).
WenliangDai,ZihanLiu,ZiweiJi,DanSu,andPascale
References Fung.2023. Plausiblemaynotbefaithful: Probing
objecthallucinationinvision-languagepre-training.
InProceedingsofthe17thConferenceoftheEuro-
Harsh Agrawal, Peter Anderson, Karan Desai, Yufei
peanChapteroftheAssociationforComputational
Wang, Xinlei Chen, Rishabh Jain, Mark Johnson,
Linguistics, pages2136–2148, Dubrovnik, Croatia.
Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
AssociationforComputationalLinguistics.
nocaps: novel object captioning at scale. In 2019
IEEE/CVF International Conference on Computer
Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh,
Vision, ICCV2019, Seoul, Korea(South), October
Ming-WeiChang,DipanjanDas,andWilliamCohen.
27-November2,2019,pages8947–8956.IEEE.
2019. Handling divergent reference texts when
evaluatingtable-to-textgeneration. InProceedingsof
PeterAnderson,BasuraFernando,MarkJohnson,and
the57thAnnualMeetingoftheAssociationforCom-
StephenGould.2016. Spice:Semanticpropositional
putationalLinguistics,pages4884–4895,Florence,
imagecaptionevaluation. InEuropeanconference
Italy.AssociationforComputationalLinguistics.
oncomputervision,pages382–398.Springer.
EsinDurmus,HeHe,andMonaDiab.2020. FEQA:A
EmilyMBender, TimnitGebru, AngelinaMcMillan-
questionansweringevaluationframeworkforfaithful-
Major, and Shmargaret Shmitchell. 2021. On the
nessassessmentinabstractivesummarization. InPro-
dangersofstochasticparrots:Canlanguagemodelsbe
ceedingsofthe58thAnnualMeetingoftheAssocia-
toobig? InProceedingsofthe2021ACMconference
tionforComputationalLinguistics,pages5055–5070,
onfairness,accountability,andtransparency,pages
Online.AssociationforComputationalLinguistics.
610–623.
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric
TomB.Brown,BenjaminMann,NickRyder,Melanie
Wallace, Pieter Abbeel, Sergey Levine, and Dawn
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Song.2023. Koala:Adialoguemodelforacademic
Neelakantan,PranavShyam,GirishSastry,Amanda
research. Blogpost.
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Le Bras, and Yejin Choi. 2021. CLIPScore: A
ClemensWinter,ChristopherHesse,MarkChen,Eric
reference-freeevaluationmetricforimagecaptioning.
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
InProceedingsofthe2021ConferenceonEmpirical
Jack Clark, Christopher Berner, Sam McCandlish,
Methods in Natural Language Processing, pages
Alec Radford, Ilya Sutskever, and Dario Amodei.
7514–7528, Online and Punta Cana, Dominican
2020. Languagemodelsarefew-shotlearners. In
Republic.AssociationforComputationalLinguistics.
AdvancesinNeuralInformationProcessingSystems
33: Annual Conference on Neural Information MatthewHonnibal,InesMontani,SofieVanLandeghem,
ProcessingSystems2020,NeurIPS2020,December andAdrianeBoyd.2020a. spacy:Industrial-strength
6-12,2020,virtual. naturallanguageprocessinginpython.
SébastienBubeck,VarunChandrasekaran,RonenEldan, MatthewHonnibal,InesMontani,SofieVanLandeghem,
JohannesGehrke,EricHorvitz,EceKamar,PeterLee, andAdrianeBoyd.2020b. spacy:Industrial-strength
YinTatLee,YuanzhiLi,ScottLundberg,etal.2023. naturallanguageprocessinginpython,zenodo,2020.
Sparksofartificialgeneralintelligence:Earlyexper-
imentswithgpt-4. ArXivpreprint,abs/2303.12712. SicongHuang,AsliCelikyilmaz,andHaoranLi.2022.
Ed-faith: Evaluating dialogue summarization on
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, faithfulness. ArXivpreprint,abs/2211.08464.
Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko.2020. End-to-endobjectdetectionwith WojciechKryscinski,BryanMcCann,CaimingXiong,
transformers. In Computer Vision–ECCV 2020: and Richard Socher. 2020. Evaluating the factual
16th European Conference, Glasgow, UK, August consistency of abstractive text summarization. InProceedings of the 2020 Conference on Empir- Nils Reimers and Iryna Gurevych. 2019. Sentence-
ical Methods in Natural Language Processing BERT: Sentence embeddings using Siamese
(EMNLP),pages9332–9346,Online.Associationfor BERT-networks. InProceedingsofthe2019Con-
ComputationalLinguistics. ferenceonEmpiricalMethodsinNaturalLanguage
Processingandthe9thInternationalJointConference
Harold W Kuhn. 1955. The hungarian method for onNaturalLanguageProcessing(EMNLP-IJCNLP),
the assignment problem. Naval research logistics
pages3982–3992,HongKong,China.Association
quarterly,2(1-2):83–97.
forComputationalLinguistics.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper
AnnaRohrbach,LisaAnneHendricks,KayleeBurns,
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Trevor Darrell, and Kate Saenko. 2018. Object
Kamali, Stefan Popov, Matteo Malloci, Alexander
hallucinationinimagecaptioning. InProceedingsof
Kolesnikov,etal.2020. Theopenimagesdatasetv4:
the2018ConferenceonEmpiricalMethodsinNatural
Unified image classification, object detection, and
Language Processing, pages 4035–4045, Brussels,
visualrelationshipdetectionatscale. International
Belgium.AssociationforComputationalLinguistics.
JournalofComputerVision,128(7):1956–1981.
Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich,
JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.
AurélieHerbelot,MoinNabi,EnverSangineto,and
Hoi. 2022. BLIP: bootstrapping language-image
RaffaellaBernardi.2017. FOILit!findonemismatch
pre-trainingforunifiedvision-languageunderstand-
betweenimageandlanguagecaption. InProceedings
ing and generation. In International Conference
of the 55th Annual Meeting of the Association for
on Machine Learning, ICML 2022, 17-23 July
ComputationalLinguistics(Volume1:LongPapers),
2022, Baltimore, Maryland, USA, volume 162 of
pages255–265,Vancouver,Canada.Associationfor
ProceedingsofMachineLearningResearch,pages
ComputationalLinguistics.
12888–12900.PMLR.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, KurtShuster,SpencerPoff,MoyaChen,DouweKiela,
WayneXinZhao,andJi-RongWen.2023. Evaluating andJasonWeston.2021. Retrievalaugmentationre-
objecthallucinationinlargevision-languagemodels. duceshallucinationinconversation. InFindingsofthe
AssociationforComputationalLinguistics:EMNLP
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and 2021, pages 3784–3803, Punta Cana, Dominican
RyanMcDonald.2020. Onfaithfulnessandfactuality Republic.AssociationforComputationalLinguistics.
in abstractive summarization. In Proceedings of
the 58th Annual Meeting of the Association for Seonil Simon Son, Junsoo Park, Jeong-in Hwang,
ComputationalLinguistics,pages1906–1919,Online. Junghwa Lee, Hyungjong Noh, and Yeonsoo Lee.
AssociationforComputationalLinguistics. 2022. Harim+: Evaluating summary quality with
hallucination risk: Evaluating summary quality
Tomas Mikolov, Edouard Grave, Piotr Bojanowski, with hallucination risk. In Proceedings of the
Christian Puhrsch, and Armand Joulin. 2018. 2nd Conference of the Asia-Pacific Chapter of the
Advancesinpre-trainingdistributedwordrepresen- Association for Computational Linguistics and the
tations. InProceedingsoftheEleventhInternational 12th International Joint Conference on Natural
ConferenceonLanguageResourcesandEvaluation
LanguageProcessing,pages895–924.
(LREC2018),Miyazaki,Japan.EuropeanLanguage
ResourcesAssociation(ELRA). ArvindKrishnaSridharandErikVisser.2022. Improved
beamsearchforhallucinationmitigationinabstractive
SewonMin,MikeLewis,LukeZettlemoyer,andHan-
summarization. ArXivpreprint,abs/2212.02712.
nanehHajishirzi.2022. MetaICL:Learningtolearn
incontext. InProceedingsofthe2022Conference
DavidWanandMohitBansal.2022. Evaluatingand
of the North American Chapter of the Association
improvingfactualityinmultimodalabstractivesum-
for Computational Linguistics: Human Language
marization. InProceedingsofthe2022Conferenceon
Technologies, pages 2791–2809, Seattle, United
EmpiricalMethodsinNaturalLanguageProcessing,
States.AssociationforComputationalLinguistics.
pages9632–9648,AbuDhabi,UnitedArabEmirates.
AssociationforComputationalLinguistics.
OpenAI.2022. Introducingchatgpt.
OpenAI.2023. Gpt-4technicalreport. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai
Bai,ZhikangLi,JianxinMa,ChangZhou,Jingren
AlecRadford,JongWookKim,ChrisHallacy,Aditya Zhou, and Hongxia Yang. 2022. OFA: unifying
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish architectures,tasks,andmodalitiesthroughasimple
Sastry, Amanda Askell, Pamela Mishkin, Jack sequence-to-sequencelearningframework. InInter-
Clark,GretchenKrueger,andIlyaSutskever.2021. national Conference on Machine Learning, ICML
Learning transferable visual models from natural 2022,17-23July2022,Baltimore,Maryland,USA,
language supervision. In Proceedings of the 38th volume 162 of Proceedings of Machine Learning
International Conference on Machine Learning, Research,pages23318–23340.PMLR.
ICML2021,18-24July2021,VirtualEvent,volume
139ofProceedingsofMachineLearningResearch, JunXu,TaoMei,TingYao,andYongRui.2016. MSR-
pages8748–8763.PMLR. VTT:Alargevideodescriptiondatasetforbridgingvideoandlanguage. In2016IEEEConferenceon You are an assistant that parses
Computer Vision and Pattern Recognition, CVPR
visually present objects from
2016,LasVegas,NV,USA,June27-30,2016,pages
an image caption. Given an image
5288–5296.IEEEComputerSociety.
caption, you list ALL the objects
PeterYoung,AliceLai,MicahHodosh,andJuliaHock- visually present in the image or
enmaier. 2014. From image descriptions to visual
photo described by the captions.
denotations: New similarity metrics for semantic
Strictly abide by the following
inferenceovereventdescriptions. Transactionsofthe
AssociationforComputationalLinguistics,2:67–78. rules:
- Include all attributes and
Weizhe Yuan, Graham Neubig, and Pengfei Liu.
adjectives that describe the object,
2021. Bartscore: Evaluatinggeneratedtextastext
if present
generation. In Advances in Neural Information
ProcessingSystems34:AnnualConferenceonNeural - Do not repeat objects
InformationProcessingSystems2021,NeurIPS2021, - Do not include objects that
December6-14,2021,virtual,pages27263–27277. are mentioned but have no visual
presence in the image, such as light,
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
sound, or emotions
Weinberger,andYoavArtzi.2020. Bertscore:Evalu-
atingtextgenerationwithBERT. In8thInternational - If the caption is uncertain
Conference on Learning Representations, ICLR about an object, YOU MUST include
2020, Addis Ababa, Ethiopia, April 26-30, 2020.
’(possibly)’ after the object
OpenReview.net.
- If the caption thinks an object can
LianminZheng,Wei-LinChiang,YingSheng,Siyuan be one of several things, include
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, ’or’ and all the possible objects
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
- Always give the singular form of
Judging llm-as-a-judge with mt-bench and chatbot
the object, even if the caption uses
arena. ArXivpreprint,abs/2306.05685.
the plural form
DeyaoZhu,JunChen,KilichbekHaydarov,Xiaoqian
Shen, Wenxuan Zhang, and Mohamed Elhoseiny. FigureA1:Thepromptthatweuseforparsingobjects
2023. Chatgpt asks, blip-2 answers: Automatic frombothcaptionsandsetsofreferencecaptions.
questioning towards enriched visual descriptions.
ArXivpreprint,abs/2303.06594.
Attributes: We ask that the language model
include all attributes attached to the object if
Appendix
they are present. By doing so, we can catch
hallucinations such as those shown in Figure 3,
AppendixA describesthepromptofthelanguage
where“electricguitar"appearsinthecandidate,but
model,includingtheexactlanguageused,the
anacousticguitarisshownintheimage.Attributes
designchoices,andthein-contextexamples.
arehandleddifferentlybetweenreferencecaptions
andcandidatecaptions.Forreferencecaptions,we
AppendixB contains additional experimental
addboththeobjectwithattributes,andtheobject
detailsforexperimentsinthepaper.
withoutattributestotheset,sothecandidateisnot
AppendixC describes the datasets that we col- penalizedforbeingmoregeneral.Forthecandidate,
lected and constructed, including HAT and however, we add only the object with attributes,
nocaps-FOIL. soifthecandidateproducesattributes, theymust
matchwithsomethinginthereferenceset.
A Prompt
Repeated Objects: In this work, our primary
Thechoiceofpromptforalargelanguagemodel goal is to determine if a particular object is
using in-context learning is critical to the perfor- hallucinated, and not focus on the quantity of
manceofthemodel.Eachcomponentoftheprompt hallucinations. Thus, we de-duplicate the object
hassomeabilitytoshapethedownstreamlanguage set in both the candidate and reference captions,
distribution.Inthiswork,weusethepromptshown aswellasdetectionscomingfromtheimage. By
inFigureA1.Thisprompthasseveralrules,which doing this, we focus on whether the objects can
wediscusshere. existintheimage,ratherthanfocusongettingtheexactcount,whichmaybeincorrectifacandidate Caption: This image shows two pink
captionmentionsthesameobjectmorethanonce roses in a tulip-shaped vase on a
(andthatobjectisparsedtwice). wooden kitchen counter, next to a
microwave and a toaster oven.
Intangible Object: In many cases, objects Objects:
mentionedinthecandidateorreferencesetmaybe - pink rose
intangible,suchascolor,light,sound,oremotion. - tulip-shaped vase
Toimprovetheaccuracyofthemodel,weexplicitly - wooden kitchen counter
suggestthatsuchobjectsshouldnotbeincluded. - microwave
- toaster oven
Or/Possibly: Moderncaptioningmethodssuch
FigureA2:Anexampleofasingle-captionparsingresult.
asChat-Captioner(Zhuetal.,2023)andIC3(Chan
et al., 2023) are capable of encoding uncertainty
intotheirapproachthroughtheuseofwordslike
“possibly"or“maybe".Additionally,theymaymake Captions:
judgmentsthatareuncertainsuchas“anappleoran - Several people riding on a
orange."Existingcaptioningandhallucinationde- motorcycle with an umbrella open.
tectionmeasuresfailtoaccountforthisuncertainty, - Couples riding motorcycles carrying
andmatchbothobjects,eventhoughthesemantics umbrellas and people sitting at
ofthecaptionsuggeststhattheobjectisuncertain, tables.
ormaybeoneofmanyobjects.Toaccountforthis, - A group of people riding scooters
weencouragetheLLMtoindicateuncertaintyin while holding umbrellas.
afixedway,aswellaslistmultiplealternativesona - Some tables and umbrellas sitting
singleline.Wethenaccountforthisinourmatching next to a building.
method, by giving the candidate the benefit of - Pedestrians and motorcyclists near
the doubt, scoring only the best match from an an open outdoor market.
alternativeset,andignoringanyuncertainty. Objects:
- person
Singularization: Whileitispossibletosingular- - couple
ize objects using rule-based methods, rule-based - motorcycle
methods struggle with challenging nouns, and - umbrella
we found that in general, the LLM was better - table
at performing the singularization set of the - scooter
post-processingbeforeobjectmatching. - building
- pedestrian
- motorcyclist
A.1 In-ContextExamples
- open outdoor market
In addition to the core prompt text, we provide
FigureA3:Anexampleofamulti-captionparsingresult.
several contextual samples, which help with
in-context learning (Brown et al., 2020). The
contextual samples help to align the label space Average Precision We measure the Average
of the model correctly with the target output Precision (AP) of each hallucination metric to
distribution(Minetal.,2022).Anexampleofsuch detectsentence-levelhallucinations. Specifically,
contextsisgiveninFigureA2andFigureA3. we label each sample with 1 if it contains a
hallucinationand0otherwise.WethenmeasureAP
B Experimental betweenthoselabelsandper-samplehallucination
Details&AdditionalExperimentation measures.ForALOHa,thisis:
B.1 Metrics
N
We employ several measures in the paper, which AP= 1 (cid:88) I[label]·(1−ALOHa)(i) (3)
wedescribeindetailhere. N
i=1ForCHAIR,thisis: maximal assignment and equations 1 and 2 both
support negative values (even though they don’t
N
1 (cid:88) appearinthisinstantiationofthealgorithm).
AP= I[label]·I[CHAIRPrediction] (4)
N
i=1
Parsing Error Rate (PER) and Parsing Recall
Rate (PRR) We calculate PER (Parsing Error
Itisworthnotingthatwhencomputingaverage
Rate)withmanualannotationbytakingthefraction
precision, we define the positive label (1) to be
ofobjectsoutputbytheLLMthatdidnotexistin
“hallucination”tomeasuretheabilityofALOHaor
thecaption(inotherwords,measuring1-precision
CHAIRtocorrectlyidentifyhallucinations.Indeed,
of parsed objects). We additionally annotate and
a lower ALOHa indicates that a caption is more
computetheParsingRecallRate(PRR)-thefrac-
likelytohaveahallucination–therefore,wenegate
tionofobjectsinthecaptionthatareincludedinthe
theALOHascorewhencomputingAP.Wefollow
objectsparsedbytheLLM.Thisgivesarecallfor
thestandardmethodofcomputingAPwithbinary
GPT-3.5of98.63%.Intheseexperiments,wefind
labelsandcontinuousconfidencevalues,wherepre-
thatwhileKoala(Gengetal.,2023)hasstrongLA
cisionandrecallareiterativelycomputedwitheach
performanceonHAT,howeverChatGPT(GPT-3.5)
confidencevalue(-ALOHa)asthethreshold. The
(OpenAI,2023)hasboththebestaverageprecision,
APisanaverageofthoseprecisions,eachweighted
andmakesthefewesterrors,thusweleverageGPT-
bytheincreaseinrecallfromthepreviousthreshold.
3.5forourprimaryexperimentsinthemainpaper.
Localization Accuracy Localization accuracy
(LA) measures the fraction of samples where C Datasets
a metric can correctly identify a hallucinated
object, amongsamplesthatareknowntocontain Inthissection,wediscussfurtherthedatathatwe
hallucinatedobjects. useandgointodetailonthedatasetcollectionpro-
cessforHAT(AppendixC.2)andthenocaps-FOIL
dataset(AppendixC.1)
|{≥1correctlyidentifiedhalluc.}|
LA= (5)
|{≥1halluc.}|
C.1 nocaps-FOIL
A sample receives LA of 1 if at least one of the The FOIL dataset (Shekhar et al., 2017) is a
predicted hallucinated objects was correct (for synthetic hallucination dataset based on samples
CHAIR),oriftheobjectwiththeminimummatch- fromtheMS-COCO(Xuetal.,2016)dataset. In
ingscorewasatruehallucination(forALOHa).We thisdataset,foreachcandidate-imagepair,a“foil"
donotmeasureLAforCLIPScores,astheycannot captioniscreatedwhichswapsoneoftheobjects
providehallucinationscoresperobject. (intheMS-COCOdetectionset)inthecaptionwith
a different, and closely related neighbor (chosen
B.2 SemanticSimilarityMeasure byhandtocloselymatch,butbevisuallydistinct).
While the FOIL dataset provides a useful bench-
InALOHa,wecomputethesimilaritybetweenob-
mark for many hallucination detection methods,
jectsusingthecosinedistancebetweenembedding
itisoverlybiasedtowardsmethodsoptimizedfor
vectors generated using the all-MiniLM-L6-v2
the MS-COCO dataset. To help evaluate more
S-BERT implementation in the Sentence-
general methods, we introduce a new dataset
Transformers1 library (Reimers and Gurevych,
“nocaps-FOIL" based on the nocaps (Agrawal
2019).Whileintheorycosinedistancesshouldliein
etal.,2019)dataset. Thenocapsdatasetconsists
theinterval[−1,1],inthislibrary,foroptimization
ofimagesfromtheOpenImages(Kuznetsovaetal.,
stability,modelsaretrainedwithpositivesamples
2020)datasetannotatedwithimagecaptionsina
having similarity 1, and negative samples having
similarstyletoMS-COCO.nocapsissplitintothree
similarity0.This(unintentionally)inducesamodel
sets:anin-domainset,whereobjectsintheimages
which (by optimization) only produces positive
are in the MS-COCO object set, near-domain,
cosine similarity scores. ALOHa can still be
wheretheobjectsintheimagearerelatedtothose
adaptedtonegativesimilarity: ouralgorithmsfor
ofMS-COCO,andout-of-domain,whereobjects
1https://www.sbert.net/ intheimagearenotcontainedinMS-COCO.Tobuildthenocaps-FOILdataset,foreachimage, MS-COCO is under a Creative Commons
we generate the baseline caption by removing a Attribution4.0License.
single caption from the reference set. We then
generatethefoilcaptionasfollows. First,wefind D QualitativeExamples
anywordsinthebaselinecaptionthatarecontained
ineithertheopenimagesclasslist(thereare600) We provide additional qualitative examples from
oranearneighborinWordnet. Wethenrandomly thefollowingscenarios:
selectoneoftheseclassestoreplace.Becausethere
are600classes,wedonothand-pickthefoilclasses, D.1 Flickr30kExamples
and rather, select a near neighbor class based on
FigureA4showsseveralexamplesontheFlickr-30k
sentenceembeddingsfrom(ReimersandGurevych,
datasetYoungetal.(2014)withcaptionsgenerated
2019).Wefindthatinpractice,thenearestneighbor
by IC3 (Chan et al., 2023), a modern image
is often a synonym, thus, to avoid selecting
captioningmodelthatoftengenerateslonger,more
synonyms,wetakethe10thfurthestsample,which
complex captions including uncertain language
isoftenanearneighbor,butisvisuallydistinct.We
such as “possibly.” We highlight objects with
replacethiswordinthecaption,matchingcase,and
ALOHa ≤ 0.5 as likely hallucinations. For
o
then perform a filter for grammatical correctness
samplesgoingfromlefttoright:
usingtheGinger2API.Anycaptionswhicharenot
grammaticallycorrectarefiltered. Thisleavesus
1. Thecaptionhallucinatestheword“mother”,
with2500image/caption/foilpairs,whichweuse
asthereisnovisualevidencethatthewoman
forevaluationinTable2.
isspecificallyamother.CHAIRdoesnot
TheOpenImagesdatasetannotationsareunder capturethis,as“mother”ismappedtoa
aCCBY4.0license,andtheimagesareunderaCC synonymfor“person”,whichitcountsasa
BY2.0license. grounded(non-hallucinated)object.ALOHa
matches“mother”tothereference“person”,
C.2 HAT assigningaborderlineALOHa of0.5.
o
HATisbasedonMS-COCOandaimstobeagold- 2. Theimagedoesnotcontainahallucination.
standardbenchmarkfortheevaluationofhallucina- CHAIRflags“table”ashallucinated,yet
tioninimagecaptioningmethods.Whileitisrela- thecaptionexpresseduncertaintywith
tivelysmall,itisdenselyannotatedbyin-domainex- aconjunction:“chairortable.”ALOHa
pertsforseveraltypesofhallucinationincludingob- successfullyparsesthisconjuctionand
jecthallucination,actionhallucination,andnumeric selects“cloth”withALOHa = 1.0tothe
o
hallucinationamongothers.HATconsistsof90val- exactreferencematch.
idationsamples,and400testsamples,eachcontain-
3. CHAIRdoesnotdetectthehallucinated
ingamachinecandidatecaptiongeneratedbyone
“bridge”,whichissuccessfullyassigneda
ofBLIP(Lietal.,2022),OFA(Wangetal.,2022),
lowALOHa =0.35.
IC3 (Chan et al., 2023) or Chat-Captioner (Zhu o
et al., 2023), and annotations which mark which
4. Thecaptionhallucinatestheword“father”.
wordinthecaptionsarehallucinated(SeeFigureA7
Inmostcases,thespecificrelationshipof
for exact instructions given to annotators). An
“father”isunlikelytobegrounded(similar
image/captionpairisconsideredahallucinationifat
to“mother”insample1);yet,inthisimage,
leastoneofthewordsinthecaptionishallucinated.
itisevenmoreclearasthereareonlychildren
Screenshotsoftheinterfacefordatacollection present.CHAIRmaps“father”asanother
are given in Figure A7. While initial versions of synonymfor“person”anddoesnotconsider
the dataset were collected using AMT workers, itahallucination,whereas“father”hasalow
we found that the quality of annotations was not ALOHa =0.34.
o
sufficientlyhigh,andthus,trainedexpertsexplicitly
in hallucination detection, and leveraged expert D.2 HATExamples
ratingsforthesamplesinthetestdataset.
We present 4 random samples from HAT each
2https://www.gingersoftware.com/ for cases without hallucinations (Figure A5) andwith hallucinations (Figure A6). Because these shows one of the 2.97% of cases (Table 3)
examples contain more nuance than we discuss whereALOHahallucinatesareferenceobject,
below, we do not indicate binary hallucination “diningtable”.Thecandidate“roundwooden
decisionsasinAppendixD.1. table” is matched to it, with an erroneously
highALOHa of0.74.
StartingwithFigureA5),sampleswithcaptions o
thatwerelabeledascorrect,fromlefttoright:
3. Thissamplecontainsacomplexerror,inwhich
thearrowisnot,infact,“pointingindifferent
1. BothCHAIRandALOHasuccessfullydo
directions.” This non-object hallucination
notfindanyhallucinations.
isimpossiblefortheobject-specificCHAIR
and ALOHa to localize correctly. However,
2. CHAIRdoesnotflaganyhallucinations.
itdemonstratesALOHa’scapabilitytoextract
ALOHaassignsalowALOHa =0.36for
o
more complex attributes such as “red street
“sun“,anincorrectparsefromthephrase
sign”and“orangedetoursign.”
“sunnyday”.However,theotherobjectsare
successfullymatched.Interestingly,ALOHa
4. The cat’s location “on top of a small chair”
adds“snowboard”asanobject,inferringthat
is labeled as an error. CHAIR does not flag
thephysicalitemwouldneedtobepresent
anyhallucinations.ALOHa for“smallchair”
o
giventheverb“snowboarding”.
is 0.59, yet both metrics cannot capture the
specificrelation.
3. CHAIRagaindoesnotflaganyhallucina-
tions.ALOHa for“tallbuilding”isthe
o
mid-range0.59,matchedwiththereference
“building”,indicatingasomewhatuncertain
attribute.Thismaybereasonablegiventhe
pointofviewintheimage.
4. CHAIRfindsnohallucinations.“Cloudysky”
receivesasomewhatlowALOHa =0.45.
o
Althoughthisphraseisaccurategiventhe
image,thisisafailurecaseinwhichthe
referencesareincomplete.
Next, we discuss Figure A6, showing samples
thatwerelabeledtocontainahallucination.Recall
thatlabelscapturealltypesofcaptionerrors,includ-
ingthoseotherthanobjecthallucinations,toserve
as a valuable source for research around general
caption correctness. As a result, there exist non-
objecthallucinationsinHATthatareimpossiblefor
CHAIRorALOHatolocalize.Fromlefttoright:
1. Theattribute“tall”islabeledasahallucination,
asthebuildingnexttothebusisonlyonestory.
Similartosample3inFigureA5,ALOHa for
o
“tallbuilding”issomewhatuncertainat0.59.
Otherobjectsarecorrectlygrounded.
2. Theobject“table”isahallucinated,misclas-
sifiedobject; e.g., onereferenceoptsforthe
moregeneral“woodensurface.”However,the
referencementionsa“table”thatitisplaced
on, leading CHAIR to avoid considering it
asahallucination.ForALOHa,thisexampleFigureA4:QualitativesamplesofALOHaevaluatedontheFlickr-30kdataset,withcandidatecaptionsgeneratedby
IC3(Chanetal.,2023).Hallucinatedobjectsinthecaptiontextareredandbolded.SeeAppendixD.1fordiscussion.
FigureA5: RandomlyselectedqualitativeexamplesofALOHaevaluatedontheHATdatasetwhenthereisno
hallucinationinthegroundtruth.SeeAppendixD.2fordiscussion.FigureA6:RandomlyselectedqualitativeexamplesofALOHaevaluatedontheHATdatasetwhenthereisahalluci-
nationinthegroundtruth.Thesehallucinationsaregenerallychallengingtodetect.SeeAppendixD.2fordiscussion.FigureA7:Thehallucinationdatasetcollectioninterface.