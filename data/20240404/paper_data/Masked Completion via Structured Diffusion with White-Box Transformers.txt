PublishedasaconferencepaperatICLR2024
MASKED COMPLETION VIA STRUCTURED DIFFUSION
WITH WHITE-BOX TRANSFORMERS
DruvPai∗ ZiyangWu SamBuchanan YaodongYu YiMa
UCBerkeley UCBerkeley TTIC UCBerkeley UCBerkeley&HKU
ABSTRACT
Modern learning frameworks often train deep neural networks with massive
amountsofunlabeleddatatolearnrepresentationsbysolvingsimplepretexttasks,
thenusetherepresentationsasfoundationsfordownstreamtasks. Thesenetworks
are empirically designed; as such, they are usually not interpretable, their repre-
sentationsarenotstructured,andtheirdesignsarepotentiallyredundant. White-
boxdeepnetworks,inwhicheachlayerexplicitlyidentifiesandtransformsstruc-
tures in the data, present a promising alternative. However, existing white-box
architectures have only been shown to work at scale in supervised settings with
labeled data, such as classification. In this work, we provide the first instantia-
tionofthewhite-boxdesignparadigmthatcanbeappliedtolarge-scaleunsuper-
vised representation learning. We do this by exploiting a fundamental connec-
tion between diffusion, compression, and (masked) completion, deriving a deep
transformer-like masked autoencoder architecture, called CRATE-MAE, in which
theroleofeachlayerismathematicallyfullyinterpretable:theytransformthedata
distributiontoandfromastructuredrepresentation. Extensiveempiricalevalua-
tions confirm our analytical insights. CRATE-MAE demonstrates highly promis-
ing performance on large-scale imagery datasets while using only ∼30% of the
parameters compared to the standard masked autoencoder with the same model
configuration. TherepresentationslearnedbyCRATE-MAEhaveexplicitstructure
andalsocontainsemanticmeaning. CodeisavailableonGitHub.
1 INTRODUCTION
Inrecentyears,deeplearninghasbeencalledupontoprocesscontinuallylargerquantitiesofhigh-
dimensional,noisy,andunlabeleddata.Akeypropertywhichmakestheseever-largertaskstractable
isthatthehigh-dimensionaldatatendstohavelow-dimensionalgeometricandstatisticalstructure.
Modern deep networks tend to learn (implicit or explicit) representations of this structure, which
are then used to efficiently perform downstream tasks. Learning these representations is thus of
centralimportanceinmachinelearning,andtherearesofarseveralcommonmethodologiesforthis
task. We focus our attention below on approaches that incrementally transform the data towards
theendrepresentationwithsimple,mathematically-interpretableprimitives. Discussionofpopular
alternativesispostponedtoAppendixA.
Denoising-diffusion models for high-dimensional data. A popular method for learning implicit
representations of high-dimensional data is learning to denoise: a model that can denoise, i.e.,
remove noise from a corrupted observation from the data distribution (to the extent information-
theoreticallypossible), canbechainedacrossnoiselevelstotransformthedatadistributiontoand
fromcertainhighlystructureddistributions, suchasanisotropicGaussian, enablingefficientsam-
pling (Ho et al., 2020; Hyva¨rinen, 2005; Kadkhodaie & Simoncelli, 2021; Sohl-Dickstein et al.,
2015;Songetal.,2021;2023;Vincent,2011). Crucially,inthecaseofdatawithlow-dimensional
structure—includingthehighlynonlinearstructurecharacteristicofnaturalimages—thesemodels
can be learned efficiently (Chen et al., 2023; Moitra & Risteski, 2020; Oko et al., 2023), and as a
resultthisframeworkhassignificantpracticalimpact(Rombachetal.,2022). Despitethisprogress,
these techniques have been largely limited to use in generative modeling; a key reason is the un-
structured nature of the final ‘noisy’ state of the diffusion process, which makes it challenging to
controlandinterpretthemodel’slearnedimplicitrepresentationofthedata.
∗Correspondenceto:DruvPai,druvpai@berkeley.edu.
1
4202
rpA
3
]GL.sc[
1v64420.4042:viXraPublishedasaconferencepaperatICLR2024
White-box models and structured representation learning. In contrast, white-box models are
designed to produce explicit and structured representations of the data distribution according to a
desiredparsimoniousconfiguration,suchassparsity(Gregor&LeCun,2010;Zhaietal.,2020)or
(piecewise) linearity (Chan et al., 2022). Recent work (Chan et al., 2022; Yu et al., 2023a) has
built white-box deep networks via unrolled optimization: namely, to obtain representations with a
desired set of properties, one constructs an objective function which encourages these desiderata,
then constructs a deep network where each layer is designed to iteratively optimize the objective.
Thisbuildsdeepnetworksasachainofoperators,representingwell-understoodoptimizationprim-
itives,whichiterativelytransformtherepresentationstothedesiredstructure. Forexample,Yuetal.
(2023a)usesaninformation-theoreticobjectivepromotinglossycompressionofthedatatowardsa
fixedstatisticalstructuretobuildatransformer-likearchitecturenamedCRATEintheabovemanner.
However,such-obtaineddeepnetworkshaveyettobeconstructedformostunsupervisedcontexts.
The fundamental difficulty here is that decoder networks must map from representations to data,
andhenceinvert(inadistributionalsense)thetransformationsmadetothedatadistributionbythe
encoder.Thisrenderstheunrolledoptimizationapproachusedtoconstructwhite-boxencoderssuch
asCRATEinfeasibleforconstructingdecoders,andinsteaddemandsafine-grainedunderstandingof
theoperatorsthatimplementtheencoder,andtheir(distributional)inverses.
Our contributions. To overcome this difficulty and extend the applicability of white-box models
tounsupervisedsettings,wedemonstrateinthisworkthatthesetwoparadigmshavemoreincom-
monthanpreviouslyappreciated. First, weshowquantitativelythatundercertainnaturalregimes,
denoisingandcompressionarehighlysimilarprimitivedataprocessingoperations: whenthetarget
distribution has low-dimensional structure, both operations implement a projection operation onto
thisstructure.Second,usingthisinsight,wedemonstrateaquantitativeconnectionbetweenunrolled
discretizeddiffusionmodelsandunrolledoptimization-constructeddeepnetworks. Thisleadstoa
significant expansion of the existing conceptual toolkit for developing white-box neural network
architectures,whichweusetoderivewhite-boxtransformer-likeencoderanddecoderarchitectures
thattogetherformanautoencodingmodelthatwecall CRATE-MAE, illustratedinFig.1. Weeval-
uate CRATE-MAE on the challenging masked autoencoding task (He et al., 2022) and demonstrate
promising performance with large parameter savings over traditional masked autoencoders, along
withmanysidebenefitssuchasemergenceofsemanticmeaningintherepresentations.
2 APPROACH
2.1 SETUPANDNOTATION
WeusethesamenotationandbasicproblemsetupasinYuetal.(2023a). Namely,wehavesome
matrix-valuedrandomvariableX = [x ,...,x ] ∈ RD×N representingthedata,wherethex ∈
1 N i
RD arecalled“tokens”andmaybearbitrarilycorrelated. Toobtainrepresentationsoftheinput,we
learnanencoderf: RD×N →Rd×N;ourrepresentationsaredenotedbytherandomvariableZ =
f(X) = [z ,...,z ] ∈ Rd×N,wherethetokenrepresentationsarez ∈ Rd. Intheautoencoding
1 N i
.
setup,wealsolearnadecoderg: Rd×N →RD×N,suchthatX ≈X(cid:99)=[x (cid:98)1,...,x (cid:98)N]=g(Z).
Ourencoderanddecoderwillbedeepneuralnetworks,andassuchtheywillbecomposedofsev-
eral, say L, layers each. Write f = fL ◦···◦f1 ◦fpre and g = gpost ◦gL−1 ◦···◦g0, where
fℓ: Rd×N → Rd×N andgℓ: Rd×N → Rd×N aretheℓth layeroftheencoderanddecoderrespec-
tively,andfpre: RD×N →Rd×N andgpost: Rd×N →RD×N arethepre-andpost-processinglay-
ersrespectively. Theinputtotheℓth layeroftheencoderisdenotedZℓ =. (cid:2) zℓ,...,zℓ (cid:3) ∈ Rd×N,
andtheinputtotheℓthlayerofthedecoderisdenotedYℓ =. (cid:2) yℓ,...,yℓ (cid:3) ∈1 Rd×N. N
1 N
2.2 DESIDERATA,OBJECTIVE,ANDOPTIMIZATION
Ourgoalistousetheencoderf anddecoderg tolearnrepresentationsZ whichareparsimonious
(Maetal.,2022)andinvertible;namely,theyhavelow-dimensional,sparse,(piecewise)lineargeo-
metricandstatisticalstructure,andare(approximately)bijectivewiththeoriginaldataX. Yuetal.
(2023a)proposestoimplementthesedesideratabypositingasignalmodelfortherepresentations:
Low-Dimensional Gaussian Mixture Codebook. Let Z = [z ,...,z ] ∈ Rd×N be a random
1 N
matrix. We impose the following statistical model on Z, parameterized by orthonormal bases
2PublishedasaconferencepaperatICLR2024
Figure1: Diagramoftheoverallwhite-boxCRATE-MAEpipeline,illustratingtheend-to-end(masked)
autoencoding process. The token representations are transformed iteratively towards a parsimonious (e.g.,
compressedandsparse)representationbyeachencoderlayerfℓ. Furthermore,suchrepresentationsaretrans-
formedbacktotheoriginalimagebythedecoderlayersgℓ. Eachencoderlayerfℓ ismeanttobe(partially)
invertedbyacorrespondingdecoderlayergL−ℓ.
U =(U ) ∈(Rd×p)K: eachtokenz hasmarginaldistributiongivenby
[K] k k∈[K] i
d
z =U α , ∀i∈[N] (2.1)
i si i
where (s ) ∈ [K]N are random variables corresponding to the subspace indices, and
i i∈[N]
(α ) ∈ (Rp)N are zero-mean Gaussian variables. If we optionally specify a noise parame-
i i∈[N]
terσ ≥0,wemeanthatwe“diffuse”thetokenswithGaussiannoise: byanabuseofnotation,each
tokenz hasmarginaldistributiongivenby
i
d
z =U α +σw , ∀i∈[N] (2.2)
i si i i
where(w ) ∈(Rd)N arei.i.d.standardGaussianvariables,independentofs andα .
i i∈[N] i i
IftheU aresufficientlyincoherentandaxis-aligned,weexpectsuchrepresentationstomaximize
k
thesparseratereductionobjectivefunction(Yuetal.,2023a):
E [∆R(Z |U )−λ∥Z∥ ]=E [R(Z)−Rc(Z |U )−λ∥Z∥ ], (2.3)
Z [K] 0 Z [K] 0
where R and Rc are lossy coding rates, or rate distortions (Cover, 1999), which are estimates for
thenumberofbitsrequiredtoencodethesampleuptoprecisionϵ>0usingaGaussiancodebook,
bothunconditionally(forR), andconditionedonthesamplesbeingdrawnfromU summedover
k
allk(forRc). Closed-formestimates(Maetal.,2007;Yuetal.,2023a)forsuchratedistortionsare:
R(Z)=
1 logdet(cid:0)
I
+αZ⊤Z(cid:1)
,
α=. d
(2.4)
2 N Nϵ2
K
Rc(Z |U )= 1(cid:88) logdet(cid:0) I +β(U⊤Z)⊤(U⊤Z)(cid:1) , β =. p . (2.5)
[K] 2 N k k Nϵ2
k=1
Notably,Rcisameasureofcompressionagainstourstatisticalstructure—itmeasureshowclosely
the overall distribution of tokens in Z fit a Gaussian mixture on U . Meanwhile, the other two
[K]
termsRand∥·∥ ensurenon-collapseandsparsityoftherepresentations,respectively.
0
Following Yu et al. (2023a), one then constructs a deep network that incrementally optimizes the
sparse rate reduction in order to transform the data distribution towards the desired parsimonious
configuration(2.1). Specifically,Yuetal.(2023a)proposedtoconstructthedeepneuralnetworkf
as a two-step alternating optimization procedure which compresses the input against the (learned)
local signal model Uℓ at layer ℓ, by taking a step of gradient descent on Rc(Z | Uℓ ), and
[K] [K]
subsequently taking a step of proximal gradient descent on a LASSO objective (Tibshirani, 1996;
Wright&Ma,2022)tosparsifythedataina(learned)dictionaryDℓ ∈Rd×d:
Zℓ+1/2 =Zℓ+MSSA(Zℓ |Uℓ )≈Zℓ−κ∇ Rc(Zℓ |Uℓ ) (2.6)
[K] Z [K]
(cid:20) (cid:21)
1
Zℓ+1 =ISTA(Zℓ+1/2 |Dℓ)≈argmin ∥Zℓ+1/2−DℓZ∥2+λ∥Z∥ , (2.7)
2 2 1
Z≥0
3PublishedasaconferencepaperatICLR2024
Figure2: Thecompression-sparsificationiterationimplementedbyeachlayerofCRATE,andeachen-
coderlayerofCRATE-MAE. Thecompressionstep,implementedbytheMSSAoperator,projectsthetokens
Zℓ towards the subspace model Uℓ to form Zℓ+1/2. The sparsification step, implemented by the ISTA
[K]
operator,rotatesthetokensinZℓ+1/2towardsthecoordinateaxes,usingthesparsifyingdictionaryDℓ,toget
Zℓ+1.ThestepsareperformedinsequenceandcompriseasingleoftheCRATE-MAEencoder.
whereMSSA(·),theMulti-headSubspaceSelf-Attentionblock(Yuetal.,2023a),isdefinedas
 (U⊤Z)softmax((U⊤Z)⊤(U⊤Z))
1 1 1
MSSA(Z |U )=. p [U ··· U ] . . , (2.8)
[K] Nϵ2 1 K  . 
(U⊤Z)softmax((U⊤Z)⊤(U⊤Z))
K K K
andISTA(·),theIterativeShrinkage-ThresholdingAlgorithmblock(Yuetal.,2023a),isdefinedas
.
ISTA(Z |D)=ReLU(Z−ηD⊤(DZ−Z)−ηλ1). (2.9)
TheMSSAblockisexactlythesameasamulti-headself-attentionblockinatransformer,withthe
changesthattheQ /K /V blocksarereplacedbyasinglematrixU ineachheadk. Theresulting
k k k k
layerfℓthusbearssignificantresemblancetoatransformer-likeblock,andsotheCRATEmodelisa
white-boxtransformer-likearchitectureconstructedviaunrolledoptimization. SuchCRATEmodels
obtain competitive performance on standard tasks while enjoying many side benefits (Yu et al.,
2023a;b),yettheyhavesofaronlybeentrainedforsupervisedlearning. Inthesequel,weintroduce
a paradigm to obtain fully white-box networks for unsupervised learning, such as autoencoding,
throughanovelunderstandingoftheCRATEmodel’sdistributionallayerwiseinverse.
2.3 UNIFYINGCOMPRESSIONANDDENOISING
Totransformourrepresentationstotheidealizedsignalmodelgivenby(2.1),weseektoiteratively
removethedisturbancesordeviationsofeachsamplefromthissignalmodel. Onewaytoperform
thistaskistoperformlossydatacompression(Maetal.,2007;Psenkaetal.,2023;Yuetal.,2020;
2023a): compressedversionsofthedata, withoutancillarydisturbances, formtherepresentations.
This approach has been favored in the construction of previous white-box deep networks, such as
CRATEdescribedabove,duetotheexistenceofexplicitinformation-theoreticcriteriaforcompres-
sion. In this case, the term Rc(Z | U ), defined in (2.5), measures the lossy compression of
[K]
the representations Z against the class of statistical models given by (2.1). Thus, an operation to
minimizeRc,suchas(2.6),implementsastepofcompressiontolearnbetterrepresentations.
Anotherwaytoremovedisturbancesfromthesignalmodel(2.1),especiallyiftheperturbedmodel
has the noisy structure given in (2.2), is to denoise. When the data is highly structured or low-
dimensional, one-step denoising becomes statistically and computationally difficult (Pedregosa,
2023). Hence the modern solution to this problem is via denoising diffusion models, which take
many small denoising steps towards the data distribution at progressively decreasing noise levels
(Hoetal.,2020;Karrasetal.,2022;Songetal.,2021). Suchmodelsuseestimatesoftheso-called
scorefunction∇logp (Hyva¨rinen,2005),wherep istheprobabilitydensityfunctionofthenoised
σ σ
inputwhenthenoisehasstandarddeviationσ > 0. Atallsufficientlysmallvaluesofσ,thescore
function ∇logp σ(Z(cid:101)) for a particular noised input Z(cid:101) points towards the closest point to Z(cid:101) on the
datadistributionsupport(Chenetal.,2023;Luetal.,2023;Yuetal.,2023a),ormoregenerallythe
modes of the true data distribution, which guides the denoising diffusion model to project Z onto
thesupportofthedatadistributionanddiffuseitwithinthissupport.1
1AmoremathematicalexpositionofdiffusionmodelsmaybefoundinAppendixA.1.
4PublishedasaconferencepaperatICLR2024
Figure3: Compressionanddenoisingagainstthelow-dimensionalGaussianmixturetokenmodel(2.1)
areequivalent.Left:theeffectofcompressionagainstthelow-dimensionalGaussianmixturemodelfortokens
(2.1),i.e.,takinggradientstepsonthecodingrateRc(· | U )—orequivalently,usingtheMSSA(· | U )
[K] [K]
operator—whichisshowninTheorem1tobeequivalenttoprojectingontotheU . Right: theeffectof
[K]
denoisingagainst(2.1),i.e.,takinggradientstepsonthescorefunctionofthenoisymodel(2.2)atsmallnoise
levelsσ,orequivalentlysmalltimest.Uptoscalingfactors(notpictured),thesetwooperationsareequivalent,
andhavesimilargeometricandstatisticalinterpretationsasaprojectionontothesupportofthedatadistribution.
Thisconnectionmotivatesourstructureddenoising-diffusionframework,aselaboratedinSection2.3.
In the context of (2.1) and (2.2), both denoising and compression operations conceptually remove
additive disturbances from the data, as visualized in Figure 3. In the following result, we make
thisqualitativeobservationmathematicallyprecise: weshowthatunderasimplifiedversionofthe
signalmodel(2.1),takingagradientsteponRc,acompressionprimitive,actsasaprojectiononto
thelocalsignalmodelU ,justaswiththedenoisingprimitiveoftakingagradientsteponlogp .
[K] σ
Theorem1(InformalversionofTheorem3inAppendixA.2). SupposeZ followsthenoisyGaus-
sian codebook model (2.2), with infinitesimal noise level σℓ > 0 and subspace memberships s
i
distributedasi.i.d.categoricalrandomvariablesonthesetofsubspaceindices{1,...,K}, inde-
pendentlyofallothersourcesofrandomness. SupposeinadditionthatthenumberoftokensN,the
representationdimensiond,thenumberofsubspacesK,andthesubspacedimensionsphaverela-
tivesizesmatchingthoseofpracticaltransformerarchitecturesincludingtheCRATE-MAEencoder
(specifiedindetailinAssumption2). Thenthenegativecompressiongradient−∇ Rc(Zℓ |Uℓ )
zi [K]
pointsfromzℓtothenearestUℓ.
i k
Theorem1establishesinarepresentativespecialcaseoftheGaussiancodebookmodel(2.1)thatat
lownoiselevels,compressionagainstthesignalmodel(2.1)isequivalenttodenoisingagainst(2.1).
Inthesequel,weusethisconnectiontounderstandtheMSSAoperatorsoftheCRATE-MAEencoder,
derived in Section 2.2 from a different perspective, as realizing an incremental transformation of
the data distribution towards the signal model (2.1) via approximate denoising. This important
propertyguaranteesthatacorrespondingdeterministicdiffusionprocess—namely,thetimereversal
ofthedenoisingprocess—impliesaninverseoperatorforthecompressionoperationimplemented
byMSSA. Becausetheseapproximatedenoisingprocessestransformthedatatowardsaparametric
structure,wecallthemstructureddenoising-diffusionprocesses.
2.4 CONSTRUCTINGADISTRIBUTIONALLY-INVERTIBLETRANSFORMERLAYER
InSection2.1,wedescribedamethodtoconstructawhite-boxtransformer-likeencodernetworkvia
unrolledoptimizationmeanttocompressthedataagainstlearnedgeometricandstatisticalstructures,
sayagainstadistributionoftokenswhereeachtokenismarginallydistributedasaGaussianmixture
supportedonU . InSection2.3,wedescribedingeneraltermsanapproachthatrelatesdenoising
[K]
andcompressiontoyieldaconceptuallysimilarnetworkusingtheformalismofdiffusionmodels,
thistimetrainableviaautoencoding.Inthissection,wecarryoutthisprocedureconcretelytoobtain
anencoderanddecoderlayerwithsimilarlyinterpretableoperationalcharacteristics.
Tomeasurecompression,weusetheRcfunctiondefinedin(2.5).Byusingastandard(reverse-time)
diffusion process with a scaling of Rc as a drop-in replacement for the score (see Appendix A.3
for details), we obtain that such a denoising diffusion process may be described by the following
5PublishedasaconferencepaperatICLR2024
stochasticdifferentialequation(SDE)(Songetal.,2021).
1 √
dZ(t)=− ∇Rc(Z(t)|U )dt+ 2dB(t), ∀t∈[0,T], (2.10)
T −t [K]
where(B(t)) isaBrownianmotion. Asadesignchoice,wewishtoassertthatourencoder
t∈[0,T]
anddecoderoughttobedeterministic,inparticularpreferringthatourencoder-decoderarchitecture
achieves sample-wise autoencoding as opposed to distribution-wise autoencoding or generation.
Thus we need to construct some ordinary differential equation (ODE) which transports the input
probability distribution in the same way as (2.10). Such an equation is readily obtained as the
probabilityflowODE(Songetal.,2021),whichitselfiscommonlyusedfordenoisingandsampling
(Luetal.,2022;Songetal.,2021;2023)andhastheform
1
dZ(t)=− ∇Rc(Z(t)|U )dt, ∀t∈[0,T]. (2.11)
2(T −t) [K]
Inparticular,theZ(t)generatedby(2.10)and(2.11)havethesamelaw. Afirst-orderdiscretization
(seeAppendixA.3)of(2.11)withstepsizeκobtainstheiteration:
Zℓ+1/2 =Zℓ+MSSA(Zℓ |Uℓ )≈Zℓ−κ∇Rc(Zℓ |Uℓ ), (2.12)
[K] [K]
where MSSA(·) was defined in (2.8). Similar to Yu et al. (2023a), in order to optimize the sparse
ratereductionofthefeatures,andinparticulartosparsifythem,weinstantiatealearnabledictionary
Dℓ ∈Rd×dandsparsifyagainstit,obtaining
Zℓ+1 =ISTA(Zℓ+1/2 |Dℓ), (2.13)
whereISTA(·)wasdefinedin(2.9). Thus,weobtainatwostepiterationfortheℓth encoderlayer
fℓ,whereZℓ+1 =fℓ(Zℓ):
Zℓ+1/2 =Zℓ+MSSA(Zℓ |Uℓ ), Zℓ+1 =ISTA(Zℓ+1/2 |Dℓ). (2.14)
[K]
ThisisthesamelayerasinCRATE,whoseconceptualbehaviorisillustratedinFigure2. Thisequiv-
alencestemsfromthefactthatthediffusionprobabilityflow(2.11)isconceptuallyandmechanically
similar to gradient flow on the compression objective in certain regimes, and so it demonstrates a
usefulconceptualconnectionbetweendiscretizeddiffusionandunrolledoptimizationasiteratively
compressingordenoisingthesignalagainstthelearneddatastructures.
Note that we parameterized a different local signal model Uℓ and dictionary Dℓ at each layer,
[K]
despitethecontinuous-timeflowsin(2.11)usingonlyone(i.e.,thefinal)localsignalmodel. Thisis
becausethesparsificationstep(2.13)transformsthedatadistribution,andsowerequireadifferent
localsignalmodelateachlayertomodelthenew(moresparse)datadistribution; seeFigure1for
intuition on the iterative transformations. Also, having a different signal model at each layer may
allowformoreefficientiterativelinearizationandcompressionofhighlynonlinearstructures.
Now that we have shown how the structured diffusion approach can recover the original CRATE
architecture(Yuetal.,2023a)asanencoderinourautoencodingproblem,weuseournewapproach
toconstructanovelmatchingdecoder. ThetimereversaloftheODE(2.11)is:
1
dY(t)= ∇Rc(Y(t)|U )dt, ∀t∈[0,T], (2.15)
2t [K]
inthesensethattheY(T−t)generatedby(2.15)hasthesamelawastheZ(t)generatedby(2.11),
assumingcompatibleinitialconditions. Afirst-orderdiscretizationof(2.15)obtainstheiteration:
Yℓ+1 =Yℓ+1/2−MSSA(Yℓ+1/2 |Vℓ )≈Yℓ+1/2+κ∇Rc(Yℓ+1/2 |Vℓ ), (2.16)
[K] [K]
whereVℓ =(Vℓ,...,Vℓ)andeachVℓ ∈Rd×parethebasesofthesubspacesto“anti-compress”
[K] 1 K k
against. In our work, we treat them as different from the corresponding UL−ℓ, because the dis-
k
cretization of (2.11) and (2.15) is imperfect, and thus we should not expect a 1-1 correspondence
betweenlocalsignalmodelsintheencoderanddecoder. ToinverttheeffectofasparsifyingISTA
step,whichourmentalmodelinFigure2portraysasarotationofthesubspacesupportstoamore
incoherentconfiguration,wemultiplybyanotherlearnabledictionaryEℓ ∈Rd×d,obtaining
Yℓ+1/2 =EℓYℓ, Yℓ+1 =Yℓ+1/2−MSSA(Yℓ+1/2 |Vℓ ). (2.17)
[K]
Thisconstructsthe(ℓ+1)st layergℓ ofourdecoder. Intheimplementation,weaddlayernormal-
izationstoensurethatthefeaturesareroughlyconstant-sizesothattheaboveapproximationshold.
Figure4hasagraphicaldepictionoftheencoderanddecoderlayers.
6PublishedasaconferencepaperatICLR2024
Figure 4: Diagramofeachencoderlayer(top)anddecoderlayer(bottom)in CRATE-MAE. Noticethat
thetwolayersarehighlyanti-parallel—eachisconstructedtodotheoperationsoftheotherinreverseorder.
Thatis,inthedecoderlayergℓ,theISTAblockoffL−ℓispartiallyinvertedfirstusingalinearlayer,thenthe
MSSAblockoffL−ℓisreversed;thisorderunravelsthetransformationdoneinfL−ℓ.
2.5 ACOMPLETEWHITE-BOXTRANSFORMER-LIKEARCHITECTUREFORAUTOENCODING
Aspreviouslydiscussed,theencoderistheconcatenationofapreprocessingmapfpre: RD×N →
Rd×N,whichhaslearnableparametersWpre ∈Rd×D andEpos ∈Rd×N,andhastheform:
.
fpre(X)=WpreX+Epos, (2.18)
andLtransformer-likelayersfℓ: Rd×N →Rd×N givenby
.
fℓ(Zℓ)=ISTA(Zℓ+MSSA(Zℓ |Uℓ )|Dℓ), ∀ℓ∈[L], (2.19)
[K]
omittingnormalizationforsimplicity. ThedecoderistheconcatenationofLtransformer-likelayers
gℓ: Rd×N →Rd×N givenby
.
gℓ(Yℓ)=EℓYℓ−MSSA(EℓYℓ |Vℓ ), ∀ℓ∈[L]−1, (2.20)
[K]
withapostprocessingmapgpost: Rd×N →RD×N whichisalearnablelinearmapWpost ∈RD×d:
.
gpost(YL)=WpostYL. (2.21)
AfulldiagramoftheautoencodingprocedureisgiveninFigure1.
Ourtrainingprocedureseekstolearnandrepresentthestructuresinthedatadistribution. Forthis,
weuseapretexttaskthatmeasuresthedegreetowhichthesestructureshavebeenlearned: masked
autoencoding(Heetal.,2022),which“masksout”alargepercentageofrandomlyselectedtokens
intheinputX andthenattemptstoreconstructthewholeimage,measuringsuccessbytheresulting
autoencodingperformance. Conceptually,maskedautoencodingcanbeseenasanonlineargeneral-
izationoftheclassicalmatrixcompletiontask,whichexploitslow-dimensionalstructuretoimpute
missingentriesinincompletedata;classicalmatrixcompletioncanbesolvedefficientlyifandonly
ifthedatahavelow-dimensionalstructure(Amelunxenetal.,2014;Cande`s&Recht,2009;Wright
&Ma,2022). Themaskedautoencodinglosswrites
L (f,g)=. E(cid:2) ∥(g◦f)(Mask(X))−X∥2]. (2.22)
MAE 2
FurtherimplementationdetailsofthisarchitecturearediscussedinAppendicesB.1andB.2.
3 EMPIRICAL EVALUATIONS
In this section, we conduct experiments to evaluate CRATE-MAE on real-world datasets and both
supervisedandunsupervisedtasks. SimilarlytoYuetal.(2023a), CRATE-MAE isbuiltusingsim-
ple design choices that we do not claim are optimal. We also do not claim that our results are
optimally engineered; in particular, we do not use the extreme amount of computational resources
required to obtain state-of-the-art performance using vision transformer-backed masked autoen-
coders(MAEs)(Heetal.,2022). Ourgoalsinthissectionaretoverifythatourwhite-boxmasked
autoencoding model CRATE-MAE has promising performance and learns semantically meaningful
representations,andthateachoperatorin CRATE-MAE alignswithourtheoreticaldesign. Wepro-
videadditionalexperimentaldetailsinAppendicesB.1andB.2.
Networkarchitectureandtrainingconfiguration. Weimplementtheencoderanddecoderarchi-
tecturesdescribedinSection2,withafewchangesdetailedinAppendixB.1. Weconsiderdifferent
modelsizesofCRATE-MAEbyvaryingthetokendimensiond,numberofheadsK,andnumberof
7PublishedasaconferencepaperatICLR2024
Table1: ModelconfigurationsfordifferentsizesofCRATE-MAE,parametercounts,andcomparisonsto
ViT-MAEmodelsfromGandelsmanetal.(2022);Heetal.(2022).WeobservethatCRATE-MAE-Baseuses
around30%oftheparametersofViT-MAE-Base,andasimilarnumberofparametersasViT-MAE-Small.
ModelConfiguration L d K N CRATE-MAE#Parameters ViT-MAE#Parameters
Small(-S) 12 576 12 196 25.4M 47.6M
Base(-B) 12 768 12 196 44.6M 143.8M
Measure coding rate across layers Measure output sparsity across layers
1500 0.8
1400
0.7
1300
1200 0.6
1100 0.5
1000
900 0.4
800 val val
0.3
2 4 6 8 10 12 2 4 6 8 10 12
Layer index - Layer index -
Figure5: Left: ThecompressionmeasureRc(Zℓ+1/2 | Uℓ )atdifferentlayersoftheencoder. Right:
[K]
thesparsitymeasure∥Zℓ+1∥ /(d·N),atdifferentlayersoftheencoder.Measurementswerecollectedfrom
0
CRATE-MAE-Baseaveragedover10000randomlychosenImageNetsamples.Weobservethatthecompression
andsparsityimproveconsistentlyovereachlayerandthroughthewholenetwork.
Masked ViT-MAE CRATE-MAE Original Masked ViT-MAE CRATE-MAE Original
Figure 6: Autoencodingvisualizationsof CRATE-MAE-BaseandViT-MAE-Base(Heetal.,2022)with
75%patchesmasked.WeobservethatthereconstructionsfromCRATE-MAE-Baseareonparwiththerecon-
structionsfromViT-MAE-Base,despiteusing<1/3oftheparameters.
layersL;suchparameterswillbekeptthesamefortheencoderanddecoder,whichiscontrarytoHe
etal.(2022)butinlinewithourwhite-boxderivation. Table1displaystheCRATE-MAEmodelcon-
figurationsandnumberofparameters, andcompareswithequivalentViT-MAEmodelsizes(Gan-
delsmanetal.,2022;Heetal.,2022),showingthatCRATE-MAEusesaround30%oftheparameters
of MAE with the same model configuration. We consider ImageNet-1K (Deng et al., 2009) as the
mainexperimentalsettingforourarchitecture. WeapplytheAdamW(Loshchilov&Hutter,2019)
optimizertotrainCRATE-MAEmodelsforbothpre-trainingandfine-tuning. Whenfine-tuning,we
also use several commonly used downstream datasets: CIFAR10, CIFAR100 (Krizhevsky et al.,
2009),OxfordFlowers(Nilsback&Zisserman,2008),andOxford-IIT-Pets(Parkhietal.,2012).
Layer-wisefunctionanalysis. First, weconfirmthatourmodelactuallydoesdolayer-wisecom-
pressionandsparsification,confirmingourconceptualunderstandingasdescribedinSection2. In
Figure5,weobservethateachlayeroftheencodertendstocompressandsparsifytheinputfeatures,
confirmingourtheoreticaldesigningoftheroleofeachoperatorinthenetwork.
Autoencodingperformance. InFigure6,wequalitativelycomparethemaskedautoencodingper-
formance of CRATE-MAE-Base to ViT-MAE-Base (He et al., 2022). We observe that both models
areabletoreconstructthedatawell, despite CRATE-MAE usinglessthanathirdoftheparameters
of ViT-MAE.In Table 4(deferred to AppendixB.4) wedisplay the averagereconstruction loss of
CRATE-MAE-BaseandViT-MAE-Base,showingasimilarquantitativeconclusion.
8
]kcolb
ASS[
)Z(cR
]kcolb
ATSI[
ytisrapSPublishedasaconferencepaperatICLR2024
Table 2: Top-1 classification accuracy of CRATE-MAE models when pre-trained on ImageNet-1K and
evaluated via fine-tuning or linear probing for various datasets. We compare CRATE-MAE to standard
ViT-MAEmodelswithmanymoreparameters. OurresultsshowthatCRATE-MAEachievescompetitiveper-
formanceonthistransferlearningtaskwheneitherfine-tuningthewholemodelorjusttheclassificationhead.
ClassificationAccuracy CRATE-MAE-S CRATE-MAE-B ViT-MAE-S ViT-MAE-B
Fine-Tuning
CIFAR10 96.2 96.8 97.6 98.5
CIFAR100 79.0 80.3 83.0 87.0
OxfordFlowers-102 71.7 78.5 84.2 92.5
Oxford-IIIT-Pets 73.7 76.7 81.7 90.3
LinearProbing
CIFAR10 79.4 80.9 79.9 87.9
CIFAR100 56.6 60.1 62.3 68.0
OxfordFlowers-102 57.7 61.8 66.8 66.4
Oxford-IIIT-Pets 40.6 46.2 51.8 80.1
(a)VisualizingPCAoftokenrepresentations. (b)Visualizingselectedattentionheadoutputs.
Figure7: Left: Visualizationsofthealignmentofeachimage’stokenrepresentationswiththetopthree
principalcomponents(inred,blue,andgreenrespectively)ofalltokenrepresentationsofimagesinthe
givenclass.Right:Visualizationsofhand-pickedattentionmapacrossallattentionheadsinthelastlayer
oftheCRATE-MAEencoderforeachimage.WeobserveinFigure7athatthetopthreeprincipalcomponents
arealignedwithtokensfrompartsoftheimagethatcarryitssemantics,andinFigure7bthattheattentionmaps
correctly“attendto”(activatestronglyon)exactlythepartsoftheimagewhicharesemanticallymeaningful.
Representation learning and emerging semantic properties. In Table 2, we display the perfor-
manceofCRATE-MAEmodelswhenfine-tunedorlinearprobedforsupervisedclassification(precise
methodinAppendixB.1)onavarietyofdatasets. Weobservethattheclassificationaccuraciesof
CRATE-MAE models are competitive with much larger ViT-MAE models. Moreover, the learned
representationsof CRATE-MAE carryusefulsemanticcontent. Bytakingthealignmentoftherep-
resentations of each token with the top few principal components of the representations of tokens
in each class (precise details in Appendix B.3), we observe in Figure 7 (left) that the representa-
tionsarelinearized,andthatthetopfewprincipalcomponentscarrysemanticstructure. InFigure7
(right), we observe that the attention heads in the MSSA operator in CRATE-MAE capture the se-
manticsoftheinputimages. Thesepropertieshavepreviouslybeenobservedinwhite-boxmodels
trainedwithsupervisedcross-entropylosses(Yuetal.,2023b);ourresultsdemonstratethattheyare
consequencesofthewhite-boxarchitecture,ratherthanthelossfunction.
4 CONCLUSION
Inthiswork,weuncoveraquantitativeconnectionbetweendenoisingandcompression,anduseitto
designaconceptualframeworkforbuildingwhite-box(mathematicallyinterpretable)transformer-
like deep neural networks which can learn using unsupervised pretext tasks, such as masked au-
toencoding. Weshowthatsuchmodelsaremoreparameter-efficientovertheirempiricallydesigned
cousins,achievepromisingperformanceonlarge-scalereal-worldimagerydatasets,andlearnstruc-
tured representations that contain semantic meaning. This work demonstrates the potential and
practicalityofwhite-boxnetworksderivedfromfirstprinciplesfortasksoutsidesupervisedclassi-
fication. Wethusbelievethatthisworkhelpstobridgethetheoryandpracticeofdeeplearning,by
unifyingonboththeconceptualandtechnicallevelmanypreviouslyseparatedapproachesinclud-
ing, butnotlimitedto, diffusionanddenoising, compressionandratereduction, transformers, and
(masked)autoencoding.
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGMENTS
TheauthorswouldliketoacknowledgehelpfromTianzheChuinpreparingthemanuscript. Druv
Pai acknowledges support from a UC Berkeley College of Engineering fellowship. Yaodong Yu
acknowledgessupportfromthejointSimonsFoundation-NSFDMSgrant#2031899andAICom-
munityMiniGrantfromFutureofLifeInstitute.YiMaacknowledgessupportfromthejointSimons
Foundation-NSF DMS grant #2031899, the ONR grant N00014-22-1-2102, and the University of
HongKong.
REFERENCES
Samira Abnar and Willem H. Zuidema. Quantifying Attention Flow in Transformers. In Dan Ju-
rafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,
2020,pp.4190–4197.AssociationforComputationalLinguistics,2020. doi: 10.18653/V1/2020.
ACL-MAIN.385. URLhttps://doi.org/10.18653/v1/2020.acl-main.385.
MichaelS.Albergo,NicholasM.Boffi,andEricVanden-Eijnden.StochasticInterpolants:AUnify-
ingFrameworkforFlowsandDiffusions. CoRR,abs/2303.08797,2023. doi: 10.48550/ARXIV.
2303.08797. URLhttps://doi.org/10.48550/arXiv.2303.08797.
DennisAmelunxen,MartinLotz,MichaelBMcCoy,andJoelATropp. Livingontheedge: phase
transitions in convex programs with random data. Information and Inference: A Journal of the
IMA, 3(3):224–294, June 2014. ISSN 2049-8764. doi: 10.1093/imaiai/iau005. URL https:
//academic.oup.com/imaiai/article/3/3/224/714203.
ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. DeepViTFeaturesasDenseVisualDe-
scriptors. CoRR,abs/2112.05814,2021. URLhttps://arxiv.org/abs/2112.05814.
Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-Invariance-covariance Regu-
larization for Self-supervised Learning. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL
https://openreview.net/forum?id=xm6YD62D1Ub.
YoshuaBengio,PascalLamblin,DanPopovici,andHugoLarochelle. GreedyLayer-wiseTraining
ofDeepNetworks. InBernhardScho¨lkopf,JohnC.Platt,andThomasHofmann(eds.),Advances
inNeural InformationProcessingSystems 19, Proceedingsof theTwentiethAnnual Conference
on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
4-7, 2006, pp. 153–160. MIT Press, 2006. URL https://proceedings.neurips.cc/
paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html.
Emmanuel J. Cande`s and Benjamin Recht. Exact Matrix Completion via Convex Optimization.
Found.Comput.Math., 9(6):717–772, 2009. doi: 10.1007/S10208-009-9045-5. URLhttps:
//doi.org/10.1007/s10208-009-9045-5.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je´gou, Julien Mairal, Piotr Bojanowski, and
ArmandJoulin.EmergingPropertiesinSelf-supervisedVisionTransformers.In2021IEEE/CVF
InternationalConferenceonComputerVision,ICCV2021,Montreal,QC,Canada,October10-
17, 2021, pp. 9630–9640. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00951. URL https:
//doi.org/10.1109/ICCV48922.2021.00951.
Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. ReduNet:
AWhite-boxDeepNetworkfromthePrincipleofMaximizingRateReduction. J.Mach.Learn.
Res.,23:114:1–114:103,2022. URLhttp://jmlr.org/papers/v23/21-0631.html.
MinshuoChen, KaixuanHuang, TuoZhao, andMengdiWang. ScoreApproximation, Estimation
and Distribution Recovery of Diffusion Models on Low-dimensional Data. In Andreas Krause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett
(eds.),InternationalConferenceonMachineLearning,ICML2023,23-29July2023,Honolulu,
Hawaii,USA,volume202ofProceedingsofMachineLearningResearch,pp.4672–4712.PMLR,
2023. URLhttps://proceedings.mlr.press/v202/chen23o.html.
10PublishedasaconferencepaperatICLR2024
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A Simple Frame-
work for Contrastive Learning of Visual Representations. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-
ume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020. URL
http://proceedings.mlr.press/v119/chen20j.html.
XinleiChen,ZhuangLiu,SainingXie,andKaimingHe. DeconstructingDenoisingDiffusionMod-
els for Self-supervised Learning. CoRR, abs/2401.14404, 2024. doi: 10.48550/ARXIV.2401.
14404. URLhttps://doi.org/10.48550/arXiv.2401.14404.
ThomasMCover. Elementsofinformationtheory. JohnWiley&Sons,1999.
Timothe´eDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski. VisionTransformersNeed
Registers. CoRR, abs/2309.16588, 2023. doi: 10.48550/ARXIV.2309.16588. URL https:
//doi.org/10.48550/arXiv.2309.16588.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schro¨dinger
bridge with applications to score-based generative modeling. Advances in Neural Information
ProcessingSystems,34:17695–17709,2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchicalimagedatabase.In2009IEEEComputerSocietyConferenceonComputerVisionand
Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255. IEEE
ComputerSociety,2009. doi: 10.1109/CVPR.2009.5206848. URLhttps://doi.org/10.
1109/CVPR.2009.5206848.
BradleyEfron. Tweedie’sformulaandselectionbias. JournaloftheAmericanStatisticalAssocia-
tion,106(496):1602–1614,2011.
YossiGandelsman,YuSun,XinleiChen,andAlexeiA.Efros. Test-timeTrainingwithMaskedAu-
toencoders. InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,andA.Oh
(eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural
InformationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,November28-De-
cember9,2022,2022. URLhttp://papers.nips.cc/paper_files/paper/2022/
hash/bcdec1c2d60f94a93b6e36f937aa0530-Abstract-Conference.html.
Karol Gregor and Yann LeCun. Learning Fast Approximations of Sparse Coding. In Johannes
Fu¨rnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on
MachineLearning(ICML-10), June21-24, 2010, Haifa, Israel, pp.399–406.Omnipress, 2010.
URLhttps://icml.cc/Conferences/2010/papers/449.pdf.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla´r, and Ross B. Girshick. Masked
AutoencodersAreScalableVisionLearners. InIEEE/CVFConferenceonComputerVisionand
PatternRecognition, CVPR2022, NewOrleans, LA,USA,June18-24, 2022, pp.15979–15988.
IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10.1109/
CVPR52688.2022.01553.
Geoffrey E Hinton and Richard Zemel. Autoencoders, Minimum Description Length
and Helmholtz Free Energy. In J. Cowan, G. Tesauro, and J. Alspector (eds.),
Advances in Neural Information Processing Systems, volume 6. Morgan-Kaufmann,
1993. URL https://proceedings.neurips.cc/paper_files/paper/1993/
file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020,virtual,2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
Aapo Hyva¨rinen. Estimation of Non-normalized Statistical Models by Score Matching. J. Mach.
Learn. Res., 6:695–709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.
html.
11PublishedasaconferencepaperatICLR2024
Zahra Kadkhodaie and Eero P. Simoncelli. Stochastic Solutions for Linear Inverse Prob-
lems using the Prior Implicit in a Denoiser. In Marc’Aurelio Ranzato, Alina Beygelz-
imer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances
in Neural Information Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 13242–
13254, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
6e28943943dbed3c7f82fc05f269947a-Abstract.html.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space
of Diffusion-based Generative Models. In Sanmi Koyejo, S. Mohamed, A. Agar-
wal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Informa-
tion Processing Systems 35: Annual Conference on Neural Information Processing Sys-
tems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html.
Valentin Khrulkov, Gleb V. Ryzhakov, Andrei Chertkov, and Ivan V. Oseledets. Understanding
DDPMLatentCodesThroughOptimalTransport. InTheEleventhInternationalConferenceon
LearningRepresentations, ICLR2023, Kigali, Rwanda, May1-5, 2023.OpenReview.net, 2023.
URLhttps://openreview.net/pdf?id=6PIrhAx1j4i.
Diederik P. Kingma and Max Welling. Auto-encoding Variational Bayes. In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http:
//arxiv.org/abs/1312.6114.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. In
8thInternationalConferenceonLearningRepresentations, ICLR2020, AddisAbaba, Ethiopia,
April26-30,2020.OpenReview.net,2020. URLhttps://openreview.net/forum?id=
rkgNKkHtvB.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Mufan (Bill) Li, Mihai Nica, and Daniel M. Roy. The Neural Covariance SDE: Shaped
Infinite Depth-and-width Networks at Initialization. In Sanmi Koyejo, S. Mohamed,
A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural In-
formation Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
45fc4a0da7e7f6fbabaabe2d20a441d1-Abstract-Conference.html.
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net,2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver:
A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. In
Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Ad-
vances in Neural Information Processing Systems 35: Annual Conference on Neural Informa-
tionProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,November28-December
9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html.
YubinLu,ZhongjianWang,andGuillaumeBal. Mathematicalanalysisofsingularitiesinthediffu-
sionmodelunderthesubmanifoldassumption,2023.
Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of Multivariate Mixed Data
viaLossyDataCodingandCompression. IEEETrans.PatternAnal.Mach.Intell.,29(9):1546–
1562,2007. doi:10.1109/TPAMI.2007.1085. URLhttps://doi.org/10.1109/TPAMI.
2007.1085.
12PublishedasaconferencepaperatICLR2024
YiMa,DorisTsao,andHeung-YeungShum. OntheprinciplesofParsimonyandSelf-consistency
fortheemergenceofintelligence. FrontiersInf.Technol.Electron.Eng.,23(9):1298–1323,2022.
doi: 10.1631/FITEE.2200297. URLhttps://doi.org/10.1631/FITEE.2200297.
SongMeiandYuchenWu. DeepNetworksasDenoisingAlgorithms: Sample-efficientLearningof
DiffusionModelsinHigh-dimensionalGraphicalModels.CoRR,abs/2309.11420,2023.doi:10.
48550/ARXIV.2309.11420. URLhttps://doi.org/10.48550/arXiv.2309.11420.
Annie Millet, David Nualart, and Marta Sanz. Integration by parts and time reversal for diffusion
processes. TheAnnalsofProbability,pp.208–238,1989.
AnkurMoitraandAndrejRisteski. FastConvergenceforLangevinDiffusionwithManifoldStruc-
ture. February2020. URLhttp://arxiv.org/abs/2002.05576.
Maria-ElenaNilsbackandAndrewZisserman. AutomatedFlowerClassificationoveraLargeNum-
berofClasses. InSixthIndianConferenceonComputerVision, Graphics&ImageProcessing,
ICVGIP2008, Bhubaneswar, India, 16-19December2008, pp.722–729.IEEEComputerSoci-
ety, 2008. doi: 10.1109/ICVGIP.2008.47. URL https://doi.org/10.1109/ICVGIP.
2008.47.
KazusatoOko,ShuntaAkiyama,andTaijiSuzuki. DiffusionModelsareMinimaxOptimalDistri-
bution Estimators. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
SivanSabato,andJonathanScarlett(eds.),InternationalConferenceonMachineLearning,ICML
2023,23-29July2023,Honolulu,Hawaii,USA,volume202ofProceedingsofMachineLearning
Research, pp. 26517–26582. PMLR, 2023. URL https://proceedings.mlr.press/
v202/oko23a.html.
Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran,
Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,
MichaelG.Rabbat,VasuSharma,GabrielSynnaeve,HuXu,Herve´ Je´gou,JulienMairal,Patrick
Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features
without Supervision. CoRR, abs/2304.07193, 2023. doi: 10.48550/ARXIV.2304.07193. URL
https://doi.org/10.48550/arXiv.2304.07193.
OmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.Jawahar. Catsanddogs. In2012
IEEEConferenceonComputerVisionandPatternRecognition,Providence,RI,USA,June16-21,
2012,pp.3498–3505.IEEEComputerSociety,2012. doi: 10.1109/CVPR.2012.6248092. URL
https://doi.org/10.1109/CVPR.2012.6248092.
F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning
Research,12:2825–2830,2011.
FabianPedregosa.OntheConvergenceoftheUnadjustedLangevinAlgorithm.2023.URLhttps:
//fa.bianp.net/blog/2023/ulaq/.
MichaelPsenka,DruvPai,VishalRaman,ShankarSastry,andYiMa. RepresentationLearningvia
ManifoldFlatteningandReconstruction. CoRR,abs/2305.01777, 2023. doi: 10.48550/ARXIV.
2305.01777. URLhttps://doi.org/10.48550/arXiv.2305.01777.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and
Approximate Inference in Deep Generative Models. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of
JMLR Workshop and Conference Proceedings, pp. 1278–1286. JMLR.org, 2014. URL http:
//proceedings.mlr.press/v32/rezende14.html.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolution Image Synthesis with Latent Diffusion Models. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022,
pp.10674–10685.IEEE,2022. doi: 10.1109/CVPR52688.2022.01042. URLhttps://doi.
org/10.1109/CVPR52688.2022.01042.
13PublishedasaconferencepaperatICLR2024
Simo Sa¨rkka¨ and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge
UniversityPress,2019.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsu-
pervised Learning using Nonequilibrium Thermodynamics. In Francis R. Bach and David M.
Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceed-
ings, pp. 2256–2265. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/
sohl-dickstein15.html.
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the
Data Distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-
rence d’Alche´-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 32: Annual Conference on Neural Information Processing Sys-
tems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11895–
11907, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
3001ef257407d5a371a96dcd947c7d93-Abstract.html.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based Generative Modeling through Stochastic Differential Equations. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=
PxTIG12RRHS.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In Andreas
Krause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScar-
lett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Hon-
olulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 32211–
32252. PMLR, 2023. URL https://proceedings.mlr.press/v202/song23a.
html.
RobertTibshirani. Regressionshrinkageandselectionviathelasso. JournaloftheRoyalStatistical
SocietySeriesB:StatisticalMethodology,58(1):267–288,1996.
NaftaliTishbyandNogaZaslavsky.Deeplearningandtheinformationbottleneckprinciple.In2015
IEEE Information Theory Workshop, ITW 2015, Jerusalem, Israel, April 26 - May 1, 2015, pp.
1–5. IEEE, 2015. doi: 10.1109/ITW.2015.7133169. URL https://doi.org/10.1109/
ITW.2015.7133169.
RomanVershynin.High-dimensionalprobability:Anintroductionwithapplicationsindatascience,
volume47. Cambridgeuniversitypress,2018.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Comput., 23(7):1661–1674, 2011. doi: 10.1162/NECO\ A\ 00142. URL https://doi.
org/10.1162/NECO_a_00142.
JohnWrightandYiMa. High-dimensionaldataanalysiswithlow-dimensionalmodels: Principles,
computation,andapplications. CambridgeUniversityPress,2022.
Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising Diffusion Autoen-
coders are Unified Self-supervised Learners. In IEEE/CVF International Conference on Com-
puter Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 15756–15766. IEEE, 2023.
doi: 10.1109/ICCV51070.2023.01448. URLhttps://doi.org/10.1109/ICCV51070.
2023.01448.
Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning Di-
verse and Discriminative Representations via the Principle of Maximal Coding Rate Reduc-
tion. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020,virtual,2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/
6ad4174eba19ecb5fed17411a34ff5e6-Abstract.html.
14PublishedasaconferencepaperatICLR2024
Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin D.
Haeffele, and Yi Ma. White-box Transformers via Sparse Rate Reduction. In Alice Oh, Tris-
tan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/hash/
1e118ba9ee76c20df728b42a35fb4704-Abstract-Conference.html.
Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, and Yi Ma.
EmergenceofSegmentationwithMinimalisticWhite-boxTransformers. CoRR,abs/2308.16271,
2023b. doi: 10.48550/ARXIV.2308.16271. URLhttps://doi.org/10.48550/arXiv.
2308.16271.
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. Complete dictionary learn-
ing via l 4-norm maximization over the orthogonal group. The Journal of Machine Learning
Research,21(1):6622–6689,2020.
15PublishedasaconferencepaperatICLR2024
A OTHER RELATED WORK
In Section 1, we described the approaches to unsupervised learning of low-dimensional structures
in the data that were most relevant to the rest of the work. Here, we discuss some other popular
alternativesforcompleteness.
Black-box unsupervised representation learning. On the other end from white-box models,
whichlearnrepresentationsfromdatathathaveaprioridesiredstructures,areblack-boxunsuper-
vised learning methods which learn fully data-driven representations. One implementation of this
principleincludescontrastivelearning,whichlearnsrepresentationsfromcomputingthestatisticsof
multipleaugmentationsofthesamedatapoint(Bardesetal.,2022;Chenetal.,2020). Anotheran-
gleistoseekarepresentationwithdesirablecharacteristicsforaspecifictask,suchasclassification;
prior works have considered diffusion models as “representation learners” from this angle (Chen
et al., 2024; Xiang et al., 2023). The notion of representation learning we are interested in in this
work,namelythetransformationofthedatadistributiontowardsastructuredformthatpreservesits
essentialinformationcontent,isdifferentfromthenotioninthislattergroupofworks. Stillanother
implementation is that of autoencoding models, the most recently popular of which is the masked
autoencoder (MAE)(Heetal.,2022). Autoencodersattempttobuildlow-dimensionalrepresenta-
tionsofthedataandusethemtoreconstructinputdata(Hinton&Zemel,1993;Kingma&Welling,
2014;Rezendeetal.,2014;Tishby&Zaslavsky,2015);maskedautoencodersspecificallymaskthe
inputdataintrainingandattempttoimputethemissingentriesthroughreconstruction.
The common point in all such unsupervised learning methods so far is that they use black-box
neuralnetworks, suchasResNets(Chenetal.,2020)ortransformers(Caronetal.,2021), astheir
back-end. Thus, although they sometimes develop semantically meaningful representations of the
data(Bardesetal.,2022;Caronetal.,2021;Chenetal.,2020),theyareuninterpretable,andtheir
trainingproceduresandinternalmechanismsareopaque.
Deepnetworks andstochastic dynamics. Thereare manyquantitative rapprochementsof deep
learning and stochastic dynamics. The most well-known of these is diffusion models, which can
be modeled as discretizations of Ito¨ diffusion processes (Song et al., 2021). The neural network
is usually trained to estimate the so-called score function. Diffusion models can be thought of as
implementing a particular approximation to optimal transport between a template distribution and
thetruedatadistribution(Khrulkovetal.,2023). Differenttypesofstochasticdynamicsusefulfor
generativemodelingmaybederivedfromoptimaltransportbetweenthedatadistributionandapre-
specified template (Albergo et al., 2023; De Bortoli et al., 2021). However, diffusion models are
unique among these methods in that they have an iterative denoising interpretation (Karras et al.,
2022),whichthisworkdrawson. Suchaninterpretationhaspreviouslybeenusedtoconstructdeep
denoisingnetworksfromunrolleddiffusionprocesses(Mei&Wu,2023),insteadofjustusingthe
deep networks to do black-box estimation of the score function. Similar studies have interpreted
deepnetworksasdiscretizationsofdiffusionprocesseswithoutthisparticulardenoisinginterpreta-
tion(Lietal.,2022),buttheaforementionedunrollediterativedenoisingstrategyiswhatwedraw
uponinthiswork.
Otherrelatedwork. Herewealsodiscusssomerelatedworkwithregardstodifferentmodifica-
tionsofthetransformerarchitectureandtrainingprocedureswhichinterfacewellwithourwhite-box
design. Forexample,Kitaevetal.(2020)suggeststhatsharingtheQandK matricesintheregular
transformer is a mechanism to make the transformer more efficient at no performance cost. This
choiceisaheuristic,whereasourwhite-boxdesignsuggeststhatQ,K,andV shouldbesetequal,
andasweseeinthepaperthiscomeswithsomesmalltradeoffs. Also,sincewhite-boxmodelsare
derivedsuchthateachlayerhasadefinedandunderstoodrole,itisnaturaltoaskifsuchmodelscan
betrainedlayer-wise,i.e.,onelayeratatime(Bengioetal.,2006). Whilethisisalsopossible,we
leaveittofuturework;ourexperimentsshowthatwithend-to-endtraining,avastlymorecommon
methodtotraindeepnetworks,eachlayerstillfollowstheroleitwasdesignedfor.
A.1 ANOVERVIEWOFDIFFUSIONPROCESSES
In this section, we give an overview of the basics of time-reversible Itoˆ diffusion processes, the
mathematical foundation for diffusion models. This is to make this paper more self-contained by
16PublishedasaconferencepaperatICLR2024
providing knowledge about general diffusion processes that we will apply to our special models.
ThecoverageadaptsthatofKarrasetal.(2022);Milletetal.(1989);Songetal.(2021).
ConsideragenericItoˆ diffusionprocess(z(t)) ,wherez(t)isanRm-valuedrandomvariable,
t∈[0,T]
givenbytheSDE
dz(t)=b(z(t),t)dt+Σ(z(t),t)dw(t), z(0)∼P, ∀t∈[0,T] (A.1)
wherewisaBrownianmotionandP issomeprobabilitymeasureonRm(inthiscaserepresenting
thedatadistribution).Herethedriftcoefficientb: Rm×R→RmanddiffusioncoefficientΣ: Rm×
R → Rm×m are functions. To make sense of (A.1) and also verify the existence of strong (i.e.,
pathwise well-defined) solutions, we need some regularity on them, and we choose the following
assumption:
A1. bandΣhavesomespatialsmoothnessanddonotgrowtoofast,i.e.,thereisaconstantK ≥0
suchthatforallx,z ∈Rmwehave
(cid:101)
sup [∥Σ(x,t)−Σ(z,t)∥ +∥b(x,t)−b(z,t)∥ ]≤K∥x−z∥ (A.2)
(cid:101) F (cid:101) 2 (cid:101) 2
t∈[0,T]
sup [∥Σ(x,t)∥ +∥b(x,t)∥ ]≤K(1+∥x∥ ). (A.3)
F 2 2
t∈[0,T]
Ingeneral,z(t)maynothaveadensityw.r.t.theLebesguemeasureonRm. Forexample,suppose
thatP issupportedonsomelow-dimensionallinearsubspace(orevenaDiracdeltameasure),and
take Σ to be the orthoprojector onto this subspace. Then z(t) will be supported on this subspace
for all t and thus not have a density w.r.t. the Lebesgue measure. Thus, when further discussing
processesofthetype(A.1),wemakethefollowingassumption
A2. z(t)hasaprobabilitydensityfunctionp(·,t)forallt>0.
Thisisguaranteedbyeitherofthefollowingconditions(Milletetal.,1989):
A2.1 b and Σ are differentiable in (x,t) and have Ho¨lder-continuous derivatives, and P has a
densityw.r.t.theLebesguemeasure;
A2.2 Theevent
{rank(Σ(z(s),s))=mforallsinsomeneighborhoodof0} (A.4)
happensP-almostsurely.
DefineΨ: Rm×R→Rm×mby
.
Ψ(x,t)=Σ(x,t)Σ(x,t)⊤. (A.5)
Todiscusstime-reversibility,wealsoneedthefollowinglocalintegrabilitycondition,whichisan-
othermeasureofsharpgrowthofthecoefficients(orpreciselytheirderivatives):
A3. Thefunctions(x,t)(cid:55)→∇ ·(Ψ(x,t)p(x,t))areintegrableonsetsoftheformD×[t ,1]for
x 0
t >0andDaboundedmeasurablesubsetofRm:
0
(cid:90) 1(cid:90)
∥∇ ·(Ψ(x,t)p(x,t))∥ dxdt<∞. (A.6)
x 2
t0 D
Towritethenotationoutmoreexplicitly,
 
∇ ·(Ψ1(x,t)p(x,t))
x
∇ ·(Ψ(x,t)p(x,t))= . .  (A.7)
x  . 
∇ ·(Ψm(x,t)p(x,t))
x
m
(cid:88) ∂
where ∇ ·(Ψi(x,t)p(x,t))= [Ψij(x,t)p(x,t)] (A.8)
x ∂x
j
j=1
where Ψi is the ith row of Ψ transposed to a column, and Ψij is the (i,j)th entry of Ψ.
Note that Millet et al. (1989) phrases this in terms of an local integrability condition on each
17PublishedasaconferencepaperatICLR2024
|∇ · (Ψi(x,t)p(x,t))|, which would naturally give a local integrability condition on ∥∇ ·
x x
(Ψ(x,t)p(x,t))∥ . However, all norms on Rm are equivalent, and so this leads to a local inte-
∞
grability condition for ∥∇ · (Ψ(x,t)p(x,t))∥ as produced. Note that the assumptions do not
x 2
guaranteethattheinvolvedderivativesexist,inwhichcasetheyaretakeninthedistributional(e.g.,
weak)sense,whencetheyshouldexist(Milletetal.,1989).
Under assumptions A1—A3, Millet et al. (1989) guarantees the existence of another process
(z(t)) such that the laws of z(t) and z(T −t) are the same for all t ∈ [0,T]. This pro-
(cid:101) t∈[0,T] (cid:101)
cess(z(t)) iscalledthetimereversalof(z(t)) ,andisshowntohavelawgivenby
(cid:101) t∈[0,T] t∈[0,T]
dz(t)=b←(z(t),t)dt+Σ←(z(t),t)dw←(t), z(0)∼p(·,T), ∀t∈[0,T] (A.9)
(cid:101) (cid:101) (cid:101) (cid:101)
wherew←(t)isanindependentBrownianmotionand
∇ ·[Ψ(x,T −t)p(x,T −t)]
b←(x,t)=−b(x,T −t)+ x (A.10)
p(x,T −t)
=−b(x,T −t)+∇ ·Ψ(x,T −t)+Ψ(x,T −t)[∇ logp(x,T −t)], (A.11)
x x
Σ←(x,t)=Σ(x,T −t). (A.12)
We would next like to develop an ODE which transports the probability mass P in the same way
as (A.1) — namely, find another process (z(t)) which has deterministic dynamics, yet has
t∈[0,T]
thesamelawas(z(t)) . Songetal.(2021)looksattheFokker-Planckequations(whichcan
t∈[0,T]
be defined, at least in a weak sense, under assumptions A1–A2) and manipulates them to get the
followingdynamicsforz(t):
dz(t)=b(z(t),t)dt, z(0)∼P, ∀t∈[0,T], (A.13)
1 ∇ ·[Ψ(x,t)p(x,t)]
where b(x,t)=b(x,t)− · x (A.14)
2 p(x,t)
1 1
=b(x,t)− ∇ ·Ψ(x,t)− Ψ(x,t)[∇ logp(x,t)]. (A.15)
2 x 2 x
Nowtogetasimilarprocessforz(t),namelyaprocess(z(t)) whichevolvesdeterministically
(cid:101) (cid:101) t∈[0,T]
yet has the same law as (z(t)) , we may either take the time reversal of (A.13) or apply the
(cid:101) t∈[0,T]
Fokker-Planckmethodto(A.9),inbothcasesobtainingthesamedynamics:
dz(t)=b←(z(t),t)dt, z(0)∼p(·,T), ∀t∈[0,T], (A.16)
(cid:101) (cid:101) (cid:101)
where
b←(x,t)=−b(x,T −t) (A.17)
1 ∇ ·[Ψ(x,T −t)p(x,T −t)]
=−b(x,T −t)+ · x (A.18)
2 p(x,T −t)
1 1
=−b(x,t)+ ∇ ·Ψ(x,T −t)+ Ψ(x,T −t)[∇ logp(x,T −t)]. (A.19)
2 x 2 x
Thequantity∇ logp(x,t)isofcentralimportance;itisdenotedthescoreattimet,andweusethe
x.
notations(x,t) = ∇ logp(x,t)forit. Withthissubstitution,wehavethefollowingdynamicsfor
x
ourfourprocesses:
dz(t)=b(z(t),t)dt+Σ(z(t),t)dw(t), z(0)∼P (A.20)
dz(t)=[−b(z(t),T −t)+∇ ·Ψ(z(t),T −t)+Ψ(z(t),T −t)s(z(t),T −t)]dt (A.21)
(cid:101) (cid:101) x (cid:101) (cid:101) (cid:101)
+Σ(z(t),T −t)dw←(t), z(0)∼p(·,T) (A.22)
(cid:101) (cid:101)
(cid:20) (cid:21)
1 1
dz(t)= b(z(t),t)− ∇ ·Ψ(z(t),t)− Ψ(z(t),t)s(z(t),t) dt, z(0)∼P (A.23)
2 x 2
(cid:20)
1
dz(t)= −b(z(t),T −t)+ ∇ ·Ψ(z(t),T −t) (A.24)
(cid:101) (cid:101) 2 x (cid:101)
(cid:21)
1
+ Ψ(z(t),T −t)s(z(t),T −t) dt, z(0)∼p(·,T). (A.25)
2 (cid:101) (cid:101) (cid:101)
18PublishedasaconferencepaperatICLR2024
Inpractice, onefitsanestimatorfors(·,·)andestimatesp(·,T)andrunsadiscretizationofeither
(A.9) or (A.16) to sample approximately from P. One common instantiation used in diffusion
models (Karras et al., 2022) is the so-called variance-exploding diffusion process, which has the
coefficientsettings
√
b(x,t)=0, Σ(x,t)= 2I (A.26)
whichimpliesthat
Ψ(x,t)=2I. (A.27)
Thismeansthatthefourspecifiedprocessesareoftheform
√
dz(t)= 2dw(t), z(0)∼P (A.28)
√
dz(t)=s(z(t),T −t)dt+ 2dw←(t), z(0)∼p(·,T) (A.29)
(cid:101) (cid:101) (cid:101)
dz(t)=s(z(t),t)dt, z(0)∼P (A.30)
dz(t)=−s(z(t),T −t), z(0)∼p(·,T). (A.31)
(cid:101) (cid:101) (cid:101)
Noticethatthedeterminsticflowsareactuallygradientflowsonthescore,whichconcretelyrevealsa
connectionbetweensamplingandoptimization,andthusbetweendiffusionmodels(preciselythose
whichusetheprobabilityflowODEtosample)andunrolledoptimizationnetworks.
In this context, we can also establish the connection between diffusion networks and iterative de-
noising. Inthevariance-explodingsetting,wehave
z(t)∼N(z(0),2tI), (A.32)
whichcanbeeasilycomputedusingresultsfrom,e.g.,Sa¨rkka¨ &Solin(2019). Thusz(t)isanoisy
version of z(0), with noise level increasing monotonically with t, and sampling z(0) from z(t)
conceptuallyremovesthisnoise. Concretely,Tweedie’sformula(Efron,2011)saysthattheoptimal
denoisingfunctionE[z(0)|z(t)]hasasimpleformintermsofthescorefunction:
E[z(0)|z(t)]=z(t)+2t·s(z(t),t). (A.33)
Inotherwords,thescorefunctionspointsfromthecurrentiteratez(t)tothevalueoftheoptimal
denoising function, so it is a negative multiple of the conditionally-expected noise. Following the
scoreby(stochastic)gradientfloworitsdiscretizationisthusequivalenttoiterativedenoising.
A.2 COMPANIONTOSECTION2.3
In this section, we prove a formal version of the result Theorem 1 stated in Section 2.3. That
is, we examine a basic yet representative instantiation of the signal model (2.2), and show that
underthismodel, inanaturalregimeofparameterscalesmotivatedbythearchitectureof CRATE-
MAE applied to standard image classification benchmarks, the operation implemented by taking a
gradientsteponthecompressiontermofthesparseratereductionobjective(2.3)correspondstoa
projectionoperationatquantizationscalesε2proportionaltothesizeofthedeviation. Thisleadsus
inparticulartoaformalversionoftheresultTheorem1.
Signalmodel. Weconsideraninstantiationofthemodel(2.2),elaboratedhere. Thatis,wefixa
distributionovertokensZ ∈Rd×N inducedbythefollowingnaturalsignalmodel: eachtokenz is
i
drawnindependentlyfromthenormalizedisotropicGaussianmeasureononeofK p-dimensional
subspaces with orthonormal bases U ,...,U ∈ Rd×p,2 which comprise the low-dimensional
1 K
structureintheobservedtokens,thencorruptedwithi.i.d.GaussiannoiseN(0,σ2I);thesubspace
d
each token is drawn from is selected uniformly at random, independently of all other randomness
intheproblem. Thissignalmodelthereforecorrespondstothesettingofuncorrelatedtokens,with
maximumentropycoordinatedistributionswithinsubspaces.Itisnaturaltofirstdevelopourtheoret-
icalunderstandingoftheconnectionbetweencompressionandthescorefunctionintheuncorrelated
setting, although in general, the ability of CRATE-MAE to capture correlations in the data through
theMSSAblockisessential. Inconnectionwiththelatterissue,wenotethatourproofswillgen-
eralizestraightforwardlytothesettingof“well-dispersed”correlatedtokens: seethediscussionin
Remark5.
Wemakethefurtherfollowingassumptionswithinthismodel:
2Moreprecisely,z isdistributedaccordingtothepushforwardofthenormalizedisotropicGaussianmea-
i
sureN(0,1I)onRpbythebasesU .
p k
19PublishedasaconferencepaperatICLR2024
1. InspiredbyanablationinYuetal.(2023a), whichsuggeststhatthelearned CRATE-MAE
model on supervised classification on ImageNet has signal models U which are near-
k
incoherent,wewillassumethatthesubspacesU havepairwiseorthogonalcolumnspaces.
k
Ourproofswillgeneralizestraightforwardlytothesettingwherethesubspacesaremerely
incoherent: seethediscussioninRemark5.
2. We assume that the relative scales of these parameters conform to the CRATE-MAE-Base
settings,trainedonImageNet: fromTable1,theseparametersare
(a) d=768;
(b) N =196;
(c) K =12;
(d) p=d/K =64.
Inparticular,d≫N ≫pandKp=d.
Thesepreciseparametervalueswillnotplayaroleinouranalysis. Wemerelyrequirethefollowing
quantitative relationships between the parameter values, which are more general than the above
precisesettings.
Assumption 2. We have ε ≤ 1, U⊤U = 1 I for all k ̸= k′, and the following parameter
k k′ k=k′
settingsandscales:
• d≥N ≥p≥K ≥2;
• Kp=d;
√
• C NlogN ≤ 1N/K,whereC isthesameastheuniversalconstantC inthestatement
1 2 1 1
ofProposition12;
• 6C2N ≤ d,whereC isthesameastheuniversalconstantC inthestatementofPropo-
2 2 3
sition15;
• 2C2N ≤d,whereC isthesameastheuniversalconstantC inProposition11;
4 4 1
Note:thereisnoself-reference,asthethirdinequalityisnotusedtoproveProposition12,thefourth
isnotusedtoproveProposition15,andthefifthisnotusedtoproveProposition11.
The first and second inequalities together imply in particular that p ≥ N/K. The third inequality
√
impliesthatC NlogN <N/K. Thefirst,second,andandthirdinequalitiestogetherimplythat
√ 1 √ √
p>C NlogN,andthat0<N/K−C NlogN <N/K <N/K+C NlogN <N.
1 1 1
Theseinequalitiesareverifiableinpracticeifonewishestoexplicitlycomputetheabsoluteconstants
C 1,C 2,C 3,C 4,andindeedtheyholdforourCRATE-MAE-Basemodel.
Formally,letµ(K,p,σ2)denotetheprobabilitymeasureonRd×N correspondingtothenoisyGaus-
sianmixturedistributionspecifiedabove.WeletZ ∼µdenoteanobservationdistributedaccording
♮
tothissignalmodel: formally,thereexistsa(random)mapi (cid:55)→ s ,fori ∈ [N]ands ∈ [K],such
i i
that
z =U α +δ , i=1,...,n, (A.34)
♮i si i i
where ∆ = [δ ... δ ] ∼ N(0,σ2I), and (independently) α ∼ N(0,1I). It is
1 N i.i.d. d i i.i.d. p
convenient to write this observation model in block form. To this end, let K = (cid:80)N 1 for
k i=1 si=k
k ∈[K]denotethenumberoftimesthek-thsubspaceisrepresentedamongstthecolumnsofZ (a
♮
randomvariable). ThenbyrotationalinvarianceoftheGaussiandistribution,wehave
Z =d [U A ... U A ]Π+∆, (A.35)
♮ 1 1 K K
where =d denotes equality in distribution, Π ∈ RN×N is a uniformly random permutation matrix,
andeachA
k
∈Rp×Kk. WealsodefineX ♮tobethenoise-freeversionofZ ♮.
Becauseofthisequalityindistribution,wewillcommitthemildabuseofnotationofidentifyingthe
blockrepresentation(A.35)withtheobservationmodel(A.34)thatfollowsthedistributionµ.
20PublishedasaconferencepaperatICLR2024
Denoising in the uncorrelated tokens model. In the uncorrelated tokens model (A.35), the
marginaldistributionofeachcolumnofZ isidentical,andequaltoanequiproportionalmixtureof
♮
(normalized)isotropicGaussiansonthesubspacesU ,...U ,convolvedwiththenoisedistribution
1 k
N(0,σ2I).
This marginal distribution was studied in Yu et al. (2023a), where it was shown that
d
whentheperturbationlevelσ2 →0,thescorefunctionforthismarginaldistributionapproximately
implementsaprojectionoperationontothenearestsubspaceU .
k
Hence,wecanconnectcompression,asimplementedintheMSSAblockoftheCRATE-MAEarchi-
tecture, to denoising in the uncorrelated tokens model by showing that at similar local scales, and
for suitable settings of the model parameters, the compression operation implements a projection
ontothelow-dimensionalstructureofthedistribution,aswell.
Compressionoperation. TheMSSAblockoftheCRATE-MAEarchitecturearisesfromtakingan
(approximate) gradient step on the Rc term of the sparse rate reduction objective (2.3). This term
writes
K
Rc(Z |U )= 1(cid:88) logdet(cid:0) I+β(U⊤Z)⊤U⊤Z(cid:1) , (A.36)
[K] 2 k k
k=1
where
p
β = , (A.37)
Nε2
andε>0isthequantizationerror. Calculatingthegradient,wehave
K
∇ Rc(Z |U )=(cid:88) U U⊤Z(cid:0) β−1I+(U⊤Z)⊤U⊤Z(cid:1)−1 . (A.38)
Z [K] k k k k
k=1
Minimizing the sparse rate reduction objective corresponds to taking a gradient descent step on
Rc(· |U ). PerformingthisoperationattheobservationfromtheuncorrelatedtokensmodelZ ,
[K] ♮
theoutputcanbewrittenas
Z+ =Z −η∇Rc(Z |U ), (A.39)
♮ ♮ [K]
whereη >0isthestepsize.
Mainresultonprojection. Wewillseeshortlythatthebehaviorofthecompressionoutput(A.39)
depends on the relative scales of the perturbation about the low-dimensional structure σ2 and the
targetquantizationerrorε2.
Theorem3. ThereareuniversalconstantsC ,C ,C ,C > 0suchthatthefollowingholds. Sup-
1 2 3 4
poseAssumption2holds,andmoreoversupposethatσ ≤1andC βσ ≤ 1. Thenwithprobability
1 2
atleast1−KC 2(cid:0) e−C3d+e−C4N/K +N−2(cid:1) ,itholds
(cid:13) (cid:13)
(cid:13)
(cid:13)Z+−(cid:20) (cid:0)
∆−ηP
U[K](β∆Π⊤)Π(cid:1)
+
1+ 1+β− β1 −−
1
η
X
♮(cid:21)(cid:13) (cid:13)
(cid:13)
(cid:13)
(A.40)
(cid:16) (cid:112) √ (cid:112) (cid:112) (cid:17)
≤C Kη σ2β2+σ(1+ N/d)+ Kβσ2(1+ N/d)+ N/d . (A.41)
5
Here,P implementsaprojectionontotherelevantsubspacesforeachtokeninthelimitingcase
U[K]
asε→0,andispreciselydefinedin(A.116)and(A.117).
WegivetheproofofTheorem3below. First,wemakethreeremarksoninterpretingtheresult,our
technicalassumptions,andouranalysis.
Remark4. Theorem3admitsthefollowinginterestinginterpretationinanasymptoticsetting,where
we can identify the leading-order behavior of the gradient and confirm our hypothesis about the
connection between compression and score-following. Choose η = β−1, so that the guarantee in
Theorem3incurssomecancellation,andmoreoverdelineatemoreprecisedependenciesontheRHS
21PublishedasaconferencepaperatICLR2024
oftheguarantee:
(cid:13) (cid:20) (cid:21)(cid:13)
(cid:13) (cid:13) (cid:13)Z+− (cid:0) ∆−P U[K](∆Π⊤)Π(cid:1) + 1+1 β−1X ♮ (cid:13) (cid:13)
(cid:13)
(A.42)
NK2ε2 (cid:18) σ2d2 (cid:112) dσ2 (cid:112) (cid:112) (cid:19)
≲ +σ(1+ N/d)+ √ (1+ N/d)+ N/d (A.43)
d N2K2ε4 N Kε2
(cid:32) (cid:114) (cid:33)
σ2d NK2 N
≲K3/2σ2+ + σ+ ε2, (A.44)
Nε2 d d
whereweusedAssumption2,whichimpliesp = d/K andN/d ≤ 1. Wewillcheckinduecourse
whether we have satisfied the hypotheses of Theorem 3, so that this guarantee indeed applies. To
thisend,weoptimizethisboundasafunctionofε>0,sincethisisaparameterofthecompression
model. Theoptimalεisstraightforwardtocomputeusingcalculus: itsatisfies
(cid:118)
(cid:117) (cid:117)σ2d(cid:30) K2N (cid:32) (cid:114) N(cid:33)
ε2 =(cid:116) σ+ (A.45)
N d d
σd
= , (A.46)
(cid:114)
(cid:113)
NK σ+ N
d
and the value of the residual arising from Theorem 3 with this choice of ε is no larger than an
absoluteconstantmultipleof
(cid:118) (cid:117) (cid:117)K2σ2d(cid:32) Nσ (cid:18) N(cid:19)3/2(cid:33)  √ (cid:115) (cid:114) N
K3/2σ2+(cid:116) + =Kσ Kσ+ σ+ . (A.47)
N d d d
Moreover,withthischoiceofε,β satisfies
(cid:118)
ε2NK (cid:117) σ
β−1 = =(cid:117) . (A.48)
d (cid:116) 1+(cid:113) N
dσ2
Inparticular,theconditionβσ ≲1inTheorem3demands
(cid:115)
(cid:114)
N
σ+ ≲1, (A.49)
d
which holds for sufficiently small σ and sufficiently large d ≥ N, showing that Theorem 3 can
benontriviallyappliedinthissetting. IfweconsiderasimplifyinglimitingregimewhereN,d →
+∞suchthatN/d → 0andN/K → +∞, weobservethefollowingasymptoticbehaviorofthe
guaranteeofTheorem3:
(cid:13) (cid:13)
(cid:13)
(cid:13)Z+−(cid:20) (cid:0)
∆−P
U[K](∆Π⊤)Π(cid:1)
+
1+1
√ σX
♮(cid:21)(cid:13) (cid:13)
(cid:13)
(cid:13)≲Kσ3/2(cid:16) 1+√ Kσ(cid:17)
. (A.50)
ThisdemonstratesthatagradientsteponRc performsdenoising: thereisanoise-level-dependent
shrinkageeffectappliedtothesignalX ,whichvanishesasσ → 0,andmeanwhilethenoiseterm
♮
∆isreduced.
Moreover,asσ →0,wecanexpressthelimitingformofP exactlyasanorthogonalprojection,
U[K]
sincethisdrivesβ−1 →0: following(A.116)and(A.117),wehavehere
P =[P ... P ], (A.51)
U[K] 1 K
where
(cid:88)
P → U proj U⊤. (A.52)
k k′ im(A k′)⊥ k′
k′̸=k
This shows that, in an asymptotic sense, a gradient step on Rc serves to suppress the effect of the
perturbationappliedtotheobservationsZ aboutthelocalsignalmodelX .Thisverifiesourclaim
♮ ♮
22PublishedasaconferencepaperatICLR2024
previouslythatinthissetting,thereisacorrespondencebetweenascore-followingalgorithmanda
compression-basedapproach: locally,bothprojecttheobservationsontothestructuresofthesignal
model.
It can be shown moreover that the shrinkage effect on X demonstrated here appears as a conse-
♮
quenceofusingtheRc“compression”termforthegradientstepinCRATE-MAE;whenthegradient
stepistakeninsteadonthefull∆Rratereductionobjective(whichiscomputationallyprohibitive,
of course), there is zero shrinkage, and perfect denoising is performed for a wider variety of step
sizesη thanthechoicemadehere. Weseetheintroductionofthisshrinkageeffectthisastheprice
of constructing an efficient and interpretable network architecture. In practice, the ISTA block of
CRATE-MAE counteracts this shrinkage effect, which is anyways minor at reasonable parameter
scales.
Remark 5. We have made two assumptions which may not hold exactly in practice: namely, we
have assumed that the U ’s have orthogonal columns, namely U⊤U = 1 I, and we have
k k k′ k=k′
assumedthatthelinearcombinationcoefficientsA thatformthematrixX arei.i.d.samplesfrom
k ♮
Gaussiandistributions. Boththeseassumptionscanbemademorerealistic,atthecostofadditional
(non-instructive)complexityintheanalysis;webrieflygooverhow.
RelaxingtheorthogonalityconditionU⊤U = 1 I tonear-orthogonality,namely∥U⊤U −
k k′ k=k′ k k′
1 I∥ ≤ ν for a small ν, as observed in practice (Yu et al., 2023a) would introduce additional
k=k′
smallerrortermsintheproof,saypolynomialinν.Themagnitudesoftheseerrorscouldinprinciple
bepreciselytracked,whenceonecouldobtainasimilarresulttoTheorem3.
Secondly,wehaveassumedthattheA ’shaveindependentcolumnswhicharesampledfrom(the
k
same) Gaussian distribution. However, in the conceptual framework for CRATE-MAE, we exploit
thejointdistribution(andinparticularthecorrelations)betweenthetokensinordertoobtaingood
performance for our model. Our analysis is not completely agnostic to this fact; as we will see,
the proof of Theorem 3 only leverages the independence of the columns of each A ’s in order
k
to obtain high-probability upper bounds on the smallest and largest singular value of the token
matrices. If these bounds were ensured by some other method, such as appropriate normalization
andincoherence,asimilarconclusiontoTheorem3couldholdinthemorerealisticcorrelatedtokens
model. Goingbeyondwell-conditionedtokenmatricesforeachsubspacewouldrequireadditional
modelingassumptions,andadditionalinvestigativeexperimentalworktodeterminearealisticbasis
forsuchassumptions.
Remark 6. We have not attempted to optimize constants or rates of concentration in the proof of
Theorem3, preferringinsteadtopursueastraightforwardanalysisthatleadstoaqualitativeinter-
pretationofthebehavioroftheratereductiongradientinourmodelproblem. Minorimprovements
totheconcentrationanalysiswouldenabletheparameterscalingrequirementsinAssumption2tobe
relaxedslightly,andtheprobabilityboundinTheorem3thatscalesasK/N2caneasilybeimproved
toanypositivepowerof1/N.
ProofofTheorem3. Westartbynoticingthat,byorthonormalityofthesubspacesU ,wehaveby
k
(A.35)
U⊤Z =[0 ... A ... 0]Π+U⊤∆, (A.53)
k ♮ k k
sothat
 −1
β−1I  
 
  

... 

 

(cid:16) β−1I+(U k⊤Z ♮)⊤U k⊤Z ♮(cid:17)−1 =Π⊤   

β−1I+A⊤ kA k   +Ξ k 

Π,
 

... 



 
 β−1I 
 
(cid:124) (cid:123)(cid:122) (cid:125)
Dk
(A.54)
23PublishedasaconferencepaperatICLR2024
becausepermutationmatricesareorthogonalmatrices,andwheretheperturbationΞ isdefinedby
k
 0 ... ∆⊤U A ... 0 
1 k k
. . .
 . . . 
 . . . 
Ξ k =Π∆⊤U kU k⊤∆Π⊤+  A⊤ kU k⊤∆ 1 ... ∆⊤ kU kA k+A⊤ kU k⊤∆ k ... A⊤ kU k⊤∆ K  ,
 . . . . . . 
 . . . 
0 ... ∆⊤U A ... 0
K k k
(A.55)
andwherewehavedefined(implicitly)inaddition
[∆ ... ∆ ]=∆Π⊤. (A.56)
1 K
ThematrixD ≻0,sowecanwrite
k
(cid:0) β−1I+(U⊤Z )⊤U⊤Z (cid:1)−1 =Π⊤D−1(cid:0) I+Ξ D−1(cid:1)−1 Π, (A.57)
k ♮ k ♮ k k k
fromwhichitfollows
U⊤Z (cid:0) β−1I+(U⊤Z )⊤U⊤Z (cid:1)−1 (A.58)
k ♮ k ♮ k ♮
=(cid:0)(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3) +U⊤∆Π⊤D−1(cid:1)(cid:0) I+Ξ D−1(cid:1)−1 Π.
k k k k k k k
(A.59)
Thetaskbeforeusisthereforetocontrol∥Ξ D−1∥ < 1,inordertoapplytheNeumannseriesto
k k
furthersimplifythisexpression. Wewilldothisinstages: first,weinvokeseveralauxiliarylemmas
toconstructahigh-probabilityeventonwhichtherandomquantitiesintheprecedingexpressionare
controlledabouttheirnominalvalues;next,weshowthattheNeumannseriescanbeappliedonthis
eventandamaintermextracted;finally,wesimplifythismaintermfurtherinordertoestablishthe
claimedexpression.
High-probabilityeventconstruction. Inordertoachievetheappropriatecontrolonallrandom
quantities, wewouldliketoconstructahigh-probabilityeventonwhichtherandomquantitiesare
nottoolarge. ByPropositions9,10and11andunionbound,thereexistuniversalconstantsC >0
i
forwhich
 (cid:112) 
∥∆∥≤σ(C + N/d)
1
P  ∀k ∈[K]: ∥A k∥≤1+C 2(cid:112) N/d ≥1−C 4K(e−C5d+e−C6N/K+N−2). (A.60)
(cid:112)
∀k ∈[K]: ∥A⊤A −I∥≤C N/d
k k 3
Theeventwecomputetheprobabilityof,whichwedenoteE⋆,ispreciselythegoodeventthatwe
want. Formally,
 (cid:112) 
∥∆∥≤σ(C + N/d)
E⋆ =.  ∀k ∈[K]: ∥A ∥≤1+1 C (cid:112) N/d  . (A.61)
k 2
 ∀k ∈[K]: ∥A⊤A −I∥≤C (cid:112) N/d 
k k 3
WeknowthatE⋆occurswithhighprobability,andareabletostronglycontroltherandomquantities
tothedegreedesired.
Maintermextraction. ByLemma8andourhypothesesontheproblemparameters,wehaveon
E⋆that
∥Ξ D−1∥≤Cβσ <1. (A.62)
k k
WecanthereforeapplytheNeumannseriestoobtain
U⊤Z (cid:0) β−1I+(U⊤Z )⊤U⊤Z (cid:1)−1 (A.63)
k ♮ k ♮ k ♮
=(cid:0)(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3) +U⊤∆Π⊤D−1(cid:1)(cid:16) I−Ξ D−1+(cid:80)∞ (−1)j(cid:0) Ξ D−1(cid:1)j(cid:17) Π.
k k k k k k k j=2 k k
(A.64)
AgainonE⋆,wehave
(cid:13) (cid:13)
(cid:13) ∞ (cid:13) ∞
(cid:13) (cid:13) (cid:13)(cid:88) (−1)j(cid:0) Ξ kD k−1(cid:1)j(cid:13) (cid:13) (cid:13)≤(cid:88)(cid:13) (cid:13)Ξ kD k−1(cid:13) (cid:13)j ≤C(βσ)2 1−1
Cβσ
≤C′(βσ)2. (A.65)
(cid:13)j=2 (cid:13) j=2
24PublishedasaconferencepaperatICLR2024
Moreover,asintheproofofLemma8,wehaveonthepreviouseventthat
(cid:13) (cid:13)U⊤∆Π⊤D−1(cid:13)
(cid:13)≤Cβσ. (A.66)
k k
Thus,ifwedefinea“mainterm”
M =(cid:2)(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3)(cid:0) I−Ξ D−1(cid:1) +U⊤∆Π⊤D−1(cid:3) Π,
k k k k k k k k
(A.67)
wehaveonthesameeventaspreviously
(cid:13) (cid:13)
(cid:13)U⊤Z (cid:0) β−1I+(U⊤Z )⊤U⊤Z (cid:1)−1 −M (cid:13)≤C(βσ)2. (A.68)
(cid:13) k ♮ k ♮ k ♮ k(cid:13)
Toconclude,weneedonlystudythismainterm,sinceU hasoperatornorm1.
k
Simplifyingthemainterm. OurapproachwillbetocontrolthemaintermM aroundasimpler
k
expression, using basic perturbation theory; by the triangle inequality for the operator norm, this
will give control of the desired gradient term. After distributing, M is a sum of three terms; we
k
willstartwiththesimplestterm. Wefirstcompute
(cid:104) (cid:105)
U⊤∆Π⊤D−1 =U⊤ β∆ ...∆ (cid:0) β−1I+A⊤A (cid:1)−1 ... β∆ . (A.69)
k k k 1 k k k K
Wearegoingtoarguethattheresidual
(cid:13) (cid:13)U k⊤∆Π⊤D k−1−U k⊤[β∆ 1 ... 0 ... β∆ K](cid:13) (cid:13) (A.70)
issmall. Tothisend,notethatbythefactthatU hasunitoperatornorm,
k
(cid:13) (cid:13)U k⊤∆Π⊤D k−1−U k⊤[β∆ 1 ... 0 ... β∆ K](cid:13) (cid:13) (A.71)
(cid:13)(cid:104) (cid:105)(cid:13)
≤(cid:13) 0 ... ∆ (cid:0) β−1I+A⊤A (cid:1)−1 ... 0 (cid:13) (A.72)
(cid:13) k k k (cid:13)
(cid:13) (cid:13)
=(cid:13)∆ (cid:0) β−1I+A⊤A (cid:1)−1(cid:13) (A.73)
(cid:13) k k k (cid:13)
(cid:13) (cid:13)
≤∥∆ ∥(cid:13)(cid:0) β−1I+A⊤A (cid:1)−1(cid:13). (A.74)
k (cid:13) k k (cid:13)
By(A.61)and(A.120)fromLemma7,thesecondtermhereiscontrolledonE⋆. Forthefirstterm,
wenotethatbydefinitionandthefactthattheunitsphereisinvarianttorotations(andpermutations
arerotations),
∥∆∥= sup ∥∆u∥ = sup ∥[∆ ... ∆ ]u∥ (A.75)
2 1 K 2
∥u∥2≤1 ∥u∥2≤1
(cid:13) (cid:13)
(cid:13)(cid:88)K (cid:13)
= sup (cid:13) ∆ u (cid:13) , (A.76)
(cid:13) i i(cid:13)
∥u∥2≤1(cid:13)
i=1
(cid:13)
2
whereu arecoordinate-subset-inducedpartitionsofthevectoruinducedbythoseof∆Π⊤. This
i
yieldsimmediately
K (cid:18) (cid:19) K √ (cid:18) (cid:19)
(cid:88) (cid:88)
∥∆∥≤ sup ∥∆ u ∥ ≤ max ∥∆ ∥ sup ∥u ∥ ≤ K max ∥∆ ∥ , (A.77)
i i 2 k i 2 k
∥u∥2≤1i=1 k∈[K] ∥u∥2≤1i=1 k∈[K]
by the triangle inequality and inequalities for ℓp norms. Similarly, choosing a specific u in the
operatornormexpression,namelyonethatissupportedentirelyononeofthecoordinatepartitions
u ,showsthat
i
∥∆∥≥∥∆ u ∥ (A.78)
i i 2
foranyi,whence
max ∥∆ ∥≤∥∆∥. (A.79)
k
k∈[K]
ItfollowsthatwecontrolthefirsttermaboveonE⋆. Combiningthisreasoning,weconcludefrom
theabove
(cid:13) (cid:13)U k⊤∆Π⊤D k−1−U k⊤[β∆ 1 ... 0 ... β∆ K](cid:13) (cid:13) (A.80)
(cid:32) (cid:112) (cid:33)
(cid:112) 1 C′ N/d
≤σ(C+ N/d) + (A.81)
1+β−1 1+β−1
(cid:112)
≲σ(1+C N/d), (A.82)
25PublishedasaconferencepaperatICLR2024
wherethesecondlineusesAssumption2toremovethehigher-orderresidual.
Next,werecallthatΞ isasumoftwoterms;wewilldoonetermatatimeforconcision. Wehave
k
first
(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3) Π∆⊤U U⊤∆Π⊤ (A.83)
k k k k k
 ∆⊤  ∆⊤⊤
1 1
=(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3) . . U U⊤ . .  (A.84)
k k k  .  k k  . 
∆⊤ ∆⊤
K K
=A (β−1I+A⊤A )−1∆⊤U U⊤[∆ ... ∆ ]. (A.85)
k k k k k k 1 K
We then multiply this term by D−1 on the right to get the term that appears in M (ignoring the
k k
multiplicationontherightbyΠ,becauseitdoesnotchangeoperatornorms). Inparticular,wewill
control
(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤[∆
1
... ∆ K]D k−1(cid:13) (cid:13), (A.86)
showingthatthistermissmall. WewillaccomplishthiswiththeblockdiagonalstructureofD :
k
indeed,thisgivesthatD−1isobtainedbyblockwiseinversion,andhence
k
(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤[∆ 1 ... ∆ K]D k−1(cid:13) (cid:13) (A.87)
(cid:13) (cid:104) (cid:105)(cid:13)
=(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤ β∆
1
... ∆ k(cid:0) β−1I+A⊤ kA k(cid:1)−1 ... β∆
K
(cid:13)
(cid:13)
(A.88)
≤√ Kmax(cid:110)(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤∆ k(β−1I+A⊤ kA k)−1(cid:13) (cid:13), (A.89)
max β(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤∆ k′(cid:13) (cid:13)(cid:111) , (A.90)
k′̸=k
where the second line uses (A.77). We will give a coarse control of this term—the error could
be improved further by exploiting more thoroughly independence of the blocks ∆ to show that
k
the maximum over k′ ̸= k in the last line of the preceding expression is smaller. We have by
submultiplicativityoftheoperatornormandthetriangleinequality
(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤∆ k(β−1I+A⊤ kA k)−1(cid:13) (cid:13) (A.91)
(cid:32) (cid:112) (cid:33)2
≤ 1+1
β−1
+ C 1+N β−/ 1d (cid:13) (cid:13)∆⊤ kU kU k⊤∆ k(cid:13) (cid:13) (A.92)
≤(cid:16) 1+C(cid:112) N/d(cid:17)(cid:13) (cid:13)∆⊤ kU kU k⊤∆ k(cid:13) (cid:13), (A.93)
wherethefirstlineusesLemma7, andthesecondlineusesAssumption2tosimplifytheresidual
asabove. WehavemeanwhilefromthedefinitionofE⋆
(cid:13) (cid:13)∆⊤ kU kU k⊤∆ k(cid:13) (cid:13)≤∥∆ k∥2 ≲σ2(cid:16) 1+(cid:112) N/d(cid:17) , (A.94)
becauseU U⊤ isanorthogonalprojection,andagainusingAssumption2tosimplifytheresidual.
k k
We can argue analogously to simplify the other term in the maximum appearing above, and this
yields
(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤[∆ 1 ... ∆ K]D k−1(cid:13) (cid:13) (A.95)
√ (cid:16) (cid:112) (cid:17)(cid:16) (cid:112) (cid:17)
≲ Kβσ2 1+C N/d 1+ N/d , (A.96)
whereweusedthefactthatε ≤ 1andtherestofAssumption2impliesthatβ ≥ 1. Thisresidual
simplifiesusingAssumption2to
(cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1∆⊤ kU kU k⊤[∆
1
... ∆ K]D k−1(cid:13) (cid:13)≲√ Kβσ2(cid:16) 1+C(cid:112) N/d(cid:17) .
(A.97)
26PublishedasaconferencepaperatICLR2024
Next, we examine the last term, which is the other summand arising in the definition of Ξ . We
k
have
 0 ... ∆⊤U A ... 0 
1 k k
 . . . . . . 
 . . . 
(cid:2) 0 ... A k(β−1I+A⊤ kA k)−1 ... 0(cid:3) 

A⊤ kU
.
.k⊤∆ 1 ... ∆⊤ kU kA k+
.
.A⊤ kU k⊤∆ k ... A⊤ kU
.
.k⊤∆ K 
 
 . . . 
0 ... ∆⊤U A ... 0
K k k
(A.98)
=A (β−1I+A⊤A )−1(cid:2) A⊤U⊤∆ ... (cid:0) ∆⊤U A +A⊤U⊤∆ (cid:1) ... A⊤U⊤∆ (cid:3) .
k k k k k 1 k k k k k k k k K
(A.99)
NowmultiplyingontherightbyD−1 givestheterm(againignoringtheright-multiplicationbyΠ,
k
whichdoesnotaffecttheoperatornorm)
(cid:104) (cid:105)
A k(β−1I+A⊤ kA k)−1 βA⊤ kU k⊤∆
1
... (cid:0) ∆⊤ kU kA k+A⊤ kU k⊤∆ k(cid:1)(cid:0) β−1I+A⊤ kA k(cid:1)−1 ... βA⊤ kU k⊤∆
K
.
(A.100)
Wewillarguethatthistermisclosetotheterm
A (β−1I+A⊤A )−1(cid:2) βA⊤U⊤∆ ... 0 ... βA⊤U⊤∆ (cid:3) (A.101)
k k k k k 1 k k K
inoperatornorm. Theargumentissimilartotheprecedingarguments: forthis,itsufficestobound
(cid:13)(cid:104) (cid:105)(cid:13)
(cid:13) 0 ... A (β−1I+A⊤A )−1(cid:0) ∆⊤U A +A⊤U⊤∆ (cid:1)(cid:0) β−1I+A⊤A (cid:1)−1 ... 0 (cid:13), (A.102)
(cid:13) k k k k k k k k k k k (cid:13)
whichisthesameascontrollingtheoperatornormofthenonzeroblock. Usingsubmultiplicativity
and Lemma 7 along with the simplifications we have done above leveraging Assumption 2, we
obtain
(cid:13) (cid:13)
(cid:13)A (β−1I+A⊤A )−1(cid:0) ∆⊤U A +A⊤U⊤∆ (cid:1)(cid:0) β−1I+A⊤A (cid:1)−1(cid:13) (A.103)
(cid:13) k k k k k k k k k k k (cid:13)
≤(cid:16) 1+C(cid:112) N/d(cid:17)(cid:13) (cid:13)∆⊤ kU kA k+A⊤ kU k⊤∆ k(cid:13) (cid:13). (A.104)
Meanwhile, onE⋆ wehavetheoperatornormof∆ andA controlled, usingagain(A.79). Ap-
k k
plyingthenthetriangleinequalityandsubmultiplicativity,weobtain
(cid:13) (cid:13)∆⊤ kU kA k+A⊤ kU k⊤∆ k(cid:13) (cid:13)≲σ(cid:16) 1+(cid:112) N/d(cid:17) , (A.105)
againsimplifyingwithAssumption2. Thisshowsthat(A.100)iscloseto(A.101)withdeviations
oftheorder≲σ(1+(cid:112)
N/d).
Aggregating the previous results. Combining our perturbation analysis above, we have estab-
lishedcontrol
(cid:13) (cid:13)M −(cid:104)(cid:0) I−A (β−1I+A⊤A )−1A⊤(cid:1) U⊤[β∆ ... 0 ... β∆ ] (A.106)
(cid:13) k k k k k k 1 K
+(cid:2) 0 ... A (β−1I+A⊤A )−1 ... 0(cid:3)(cid:105) Π(cid:13) (cid:13) (A.107)
k k k (cid:13)
√
(cid:112) (cid:112)
≲σ(1+ N/d)+ Kβσ2(1+ N/d). (A.108)
It is convenient to include one additional stage of simplification here: namely, we use Lemma 7
once more to simplify the second term in the nominal value of M appearing here. Namely, we
k
have(arguingaswehaveabove,onceagain)
(cid:13) (cid:13)(cid:2) 0 ... A k(β−1I+A⊤ kA k)−1 ... 0(cid:3) −(cid:2) 0 ... 1+β1 −1A k ... 0(cid:3)(cid:13) (cid:13) (A.109)
(cid:13) (cid:13)
=(cid:13) (cid:13) (cid:13)1+1 β−1A k−A k(cid:0) β−1I+A⊤ kA k(cid:1)−1(cid:13) (cid:13)
(cid:13)
(A.110)
(cid:112)
≲ N/d, (A.111)
27PublishedasaconferencepaperatICLR2024
fromwhichitfollows
(cid:13) (cid:13)M −(cid:104)(cid:0) I−A (β−1I+A⊤A )−1A⊤(cid:1) U⊤[β∆ ... 0 ... β∆ ] (A.112)
(cid:13) k k k k k k 1 K
+(cid:2) 0 ... 1 A ... 0(cid:3)(cid:105) Π(cid:13) (cid:13) (A.113)
1+β−1 k (cid:13)
√
(cid:112) (cid:112) (cid:112)
≲σ(1+ N/d)+ Kβσ2(1+ N/d)+ N/d. (A.114)
Meanwhile,recalltheresidualofscale≲(σβ)2arisingwhenwecontrolledthegradienttermaround
M :
k
(cid:13) (cid:13)
(cid:13)U⊤Z (cid:0) β−1I+(U⊤Z )⊤U⊤Z (cid:1)−1 −M (cid:13)≤C(βσ)2. (A.115)
(cid:13) k ♮ k ♮ k ♮ k(cid:13)
Combiningthesetwoboundswiththetriangleinequalitycontrolsthegradienttermarounditsnom-
inal value. Now, we sum these errors over k (again with the triangle inequality) to obtain control
oftheaggregategradientarounditsnominalvalue. Weintroducenotationtoconciselycapturethe
accumulationsofthe(approximate)orthogonalprojectionsarisinginthenominalvalueofthemain
term: foreachk ∈[K],define
P = (cid:88) U (cid:0) I−A (β−1I+A⊤A )−1A⊤(cid:1) U⊤, (A.116)
k k′ k′ k′ k′ k′ k′
k′̸=k
and define an overall (approximate) projection operator (which acts on block matrices partitioned
compatiblywiththeclasssizesN ,asin(A.35))by
k
P =[P ... P ]. (A.117)
U[K] 1 K
Thentheaboveargumentimplies
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)∇ ZRc(Z ♮ |U [K])−P U[K](β∆Π⊤)Π− 1+1 β−1X ♮(cid:13) (cid:13)
(cid:13)
(A.118)
(cid:16) (cid:112) √ (cid:112) (cid:112) (cid:17)
≲K σ2β2+σ(1+ N/d)+ Kβσ2(1+ N/d)+ N/d , (A.119)
whichisenoughtoconclude.
A.2.1 KEYAUXILIARYLEMMAS
Inthissectionwestateandprovetwokeyconcentrationinequalitiesthatareusedintheproofofthe
maintheorem. Theyrelyonsimplerresultswhichwillbeconveyedinsubsequentsubsections.
Lemma7. ThereexistuniversalconstantsC,C′ >0suchthatthefollowingholds.Letd,p,N,K ∈
NbesuchthatAssumption2holds. LetA ,k ∈[K],bedefinedasabove. LetE⋆bethegoodevent
k
definedin(A.61). IfE⋆occurs,thenfork ∈[K]wehave
(cid:13) (cid:13) (cid:112)
(cid:13) (cid:13) (cid:13)(β−1I+A⊤ kA k)−1− 1+1 β−1I(cid:13) (cid:13) (cid:13)≤ (C 1+N β−/ 1d ). (A.120)
andinaddition
(cid:13) (cid:13) (cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1− 1+1 β−1A k(cid:13) (cid:13) (cid:13) (cid:13)≤ (C 1′ +(cid:112) βN −/ 1d ). (A.121)
Proof. SinceE⋆holds,forallk ∈[K]wehave
(cid:112) (cid:112)
∥A ∥≤1+C N/d, ∥A⊤A −I∥≤C N/d. (A.122)
k 1 k k 2
ByAssumption2,wehave∥A⊤A −I∥<1,soA⊤A iswell-conditioned. Write
k k k k
.
Ξ=A⊤A −I, (A.123)
k k
28PublishedasaconferencepaperatICLR2024
sothat
(β−1I+A⊤A )−1 =((1+β−1)I+Ξ)−1 (A.124)
k k
1
(cid:18)
1
(cid:19)−1
= I+ Ξ (A.125)
1+β−1 1+β−1
1
(cid:88)∞ (cid:18)
1
(cid:19)j
= − Ξj (A.126)
1+β−1 1+β−1
j=0
1 1
(cid:88)∞ (cid:18)
1
(cid:19)j
= I+ − Ξj (A.127)
1+β−1 1+β−1 1+β−1
j=1
bytheNeumannseries. Thisgivesus
(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13) (cid:13)(β−1I+A⊤ kA k)−1− 1+1
β−1I(cid:13)
(cid:13) (cid:13)
(cid:13)=(cid:13)
(cid:13) (cid:13) (cid:13)1+1
β−1
(cid:88)∞ (cid:18)
− 1+1
β−1(cid:19)j Ξj(cid:13)
(cid:13) (cid:13)
(cid:13)
(A.128)
(cid:13) j=1 (cid:13)
1
(cid:88)∞ (cid:18)
1
(cid:19)j
≤ ∥Ξ∥j (A.129)
1+β−1 1+β−1
j=1
∞ (cid:32) (cid:112) (cid:33)j
1 (cid:88) C 2 N/d
≤ (A.130)
1+β−1 1+β−1
j=1
(cid:112)
C N/d
2
= . (A.131)
(cid:112)
(1+β−1)(1+β−1−C N/d)
2
Meanwhile,byAssumption2,itholds
(cid:112) (cid:112)
C N/d≤ 1/6, (A.132)
2
soitfollows
(cid:112)
C 2 N/d (cid:112)
≤2C N/d. (A.133)
(cid:112) 2
1+β−1−C N/d
2
Bythesubmultiplicativityoftheoperatornorm,wethushave
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1− 1+1 β−1A k(cid:13) (cid:13) (cid:13)≤∥A k∥(cid:13) (cid:13) (cid:13)(β−1I+A⊤ kA k)−1− 1+1 β−1I(cid:13) (cid:13)
(cid:13)
(A.134)
(cid:112) (cid:112)
[1+C N/d]C N/d
1 2
≤ (A.135)
(cid:112)
(1+β−1)(1+β−1−C N/d)
2
(cid:112) (cid:112)
[1+C N/d]C N/d
1 2
≤2 (A.136)
1+β−1
(cid:112)
C N/d+C C N/d
2 1 2
=2 . (A.137)
1+β−1
(cid:112)
ByAssumption2,wehavethatthereexistssomeabsoluteconstantC >0withC ·N/d≤ N/d,
3 3
whichgives
(cid:13) (cid:13) (cid:13) (cid:13)A k(β−1I+A⊤ kA k)−1− 1+1 β−1A k(cid:13) (cid:13) (cid:13) (cid:13)≤2(C 2+C 11C +2C β−3− 11)(cid:112) N/d , (A.138)
asdesired.
Lemma 8. There exist universal constants C ,C > 0 such that the following holds. Let
1 2
d,p,N,K ∈ N be such that Assumption 2 holds. Let A , k ∈ [K], be defined as above. Let
k
D be defined as in (A.54). Let Ξ be defined as in (A.55). Let E⋆ be the good event defined in
k k
(A.61). IfE⋆occurs,thenfork ∈[K]wehave
(cid:112)
∥Ξ D ∥−1 ≤C β[σ2+σ(C + N/d)]. (A.139)
k k 1 2
29PublishedasaconferencepaperatICLR2024
Proof. Sincewehave
 β−1I 


... 

 
D k =

β−1I+A⊤ kA k 

(A.140)


... 

β−1I
itholdsthat
βI 

...

 
D−1 = (β−1I+A⊤A )−1 . (A.141)
k  k k 


... 

βI
We will use the straightforward estimate ∥Ξ D−1∥ ≤ ∥Ξ ∥∥D−1∥ and bound the two matrices’
k k k k
operatornormsindividually. Bythepreviousexpression,
∥D−1∥=max{β,∥(β−1I+A⊤A )−1∥}≤β, (A.142)
k k k
because A⊤A ⪰ 0, so we need only control the operator norm of Ξ . To this end, note the
k k k
convenientexpression
Ξ =Π∆⊤U U⊤∆Π⊤+2sym(cid:0) (∆Π⊤)⊤[0 ... U A ... 0](cid:1) , (A.143)
k k k k k
where sym(·) denotes the symmetric part operator. By the triangle inequality, the operator norm
of Ξ is no larger than the sum of the operator norms of each term in the previous expression.
k
The operator norm of the first term is no larger than ∥∆∥2, because Π is a permutation matrix
and UU⊤ is an orthogonal projection. Meanwhile, using that the symmetric part operator is the
k
orthogonalprojectionontothespaceofsymmetricmatrices,itfollows
(cid:13) (cid:13)2sym(cid:0) (∆Π⊤)⊤[0 ... U kA
k
... 0](cid:1)(cid:13) (cid:13)≤2(cid:13) (cid:13)(∆Π⊤)⊤[0 ... U kA
k
... 0](cid:13) (cid:13), (A.144)
and then we find as above that the RHS is no larger than 2∥∆∥∥A ∥. Since the good event E⋆
k
definedin(A.61)holdsbyassumption,wehavethatthereareconstantsC ,C >0suchthat
1 2
(cid:32) (cid:114) (cid:33)
N
∥∆∥≤σ C + (A.145)
1 d
(cid:114)
N
∥A ∥≤1+C . (A.146)
k 2 d
(cid:112)
ByAssumption2wehaved≥N,sothat N/d≤1. Thereforewehave
∥∆∥≤σ(C +1)=C σ (A.147)
1 3
.
forC =C +1anotheruniversalconstant. Thusonthisgoodeventwehave
3 1
(cid:16) (cid:112) (cid:17)
2∥∆∥∥A ∥≤C σ 1+C N/d . (A.148)
k 3 2
Therefore,wehave
∥Ξ ∥≤∥∆∥2+2∥∆∥∥A ∥ (A.149)
k k
(cid:112)
≤C2σ2+C σ(1+C N/d) (A.150)
3 3 2
(cid:112)
≤C [σ2+σ(1+C N/d)] (A.151)
4 2
whereC =max{C ,C2}isanotheruniversalconstant. ThusonE⋆wehave
4 3 3
(cid:112) (cid:112)
∥Ξ D−1∥≤C β[σ2+σ(1+C N/d)]≤C β[σ2+σ(1+ N/d)] (A.152)
k k 4 2 5
forC >0anotherobviousuniversalconstant.
5
30PublishedasaconferencepaperatICLR2024
A.2.2 CONCENTRATIONINEQUALITIESFOROURSETTING
In this section we prove some simple concentration inequalities that are adapted to the problem
setting. Theseresultsareusedtoprovethekeylemmataabove,andindeedarealsoinvokedinthe
main theorem. They follow from even simpler concentration inequalities that are abstracted away
fromtheproblemsetting,whichwediscussinthefollowingsubsections.
Proposition 9. There are universal constants C ,C ,C > 0 such that the following holds. Let
1 2 3
d,N ∈NbesuchthatAssumption2holds. Let∆∈Rd×N bedefinedasabove. Then
(cid:34) (cid:32) (cid:114) (cid:33)(cid:35)
N
P ∥∆∥>σ C + ≤C e−C3d. (A.153)
1 d 2
√
Proof. WeuseProposition14withtheparametersq =d,n=N,andx=σ/ d,whichobtains
 (cid:40) √ √ (cid:41)2
s d/σ− N σ √ √
P[∥∆∥>s]≤C 1exp−d √ −1 , ∀s> √ ( N +C
2
d) (A.154)
C d d
2
Noticethatwehave
s√ d/σ−√ N 1 (cid:32) s (cid:114) N(cid:33) σ √ √ (cid:32)(cid:114) N (cid:33)
√ −1= − −1, √ ( N +C d)=σ +C .
C 2 d C 2 σ d d 2 d 2
(A.155)
Tomakethesquaredtermequalto1,wepick
(cid:32)(cid:114) (cid:33)
N
s=σ +2C , (A.156)
d 2
whichgives
(cid:34) (cid:32) (cid:114) (cid:33)(cid:35)
N
P ∥∆∥>σ 2C + ≤C e−d. (A.157)
2 d 2
Proposition 10. There are universal constants C ,C ,C ,C > 0 such that the following holds.
1 2 3 4
Letp,N,K ∈NbesuchthatAssumption2holds. LetA ,k ∈[K],bedefinedasabove. Then
k
(cid:34) (cid:114) (cid:35) (cid:18) (cid:19)
N N C
P ∥A ∥>1+C ≤C exp −C + 4 (A.158)
k 1 d 2 3K N2
√
Proof. By Propositions 12 and 14 with parameters n = n, k = K, q = p, and x = 1/ p, if we
define
(cid:22) (cid:23) (cid:24) (cid:25)
. N (cid:112) . N (cid:112)
N = −C NlogN , N = +C NlogN (A.159)
min K 1 max K 1
thenwehave
P[∥A ∥>s]≤
N (cid:88)max
P[∥A ∥>s|K =n]P[K =n]+
C
2 (A.160)
k k k k N2
n=Nmin
≤
N (cid:88)max
C
exp(cid:32) −n(cid:26)s√ p− √√ p −1(cid:27)2(cid:33)
P[K =n]+
C
2, (A.161)
3 C 4 k N2
n=Nmin 4
forallsobeying
1 (cid:16)√ (cid:112) (cid:17)
s≥ √ p+C N (A.162)
p 4 max
(cid:115)
N
=1+C max. (A.163)
4 p
31PublishedasaconferencepaperatICLR2024
Thuswehavethattheconcentrationholdsforallsobeying
(cid:115)
N
s≥1+C max. (A.164)
4 p
Inordertocanceloutthemostinteriorterms,wechoose
(cid:115)
N
s=1+2C max. (A.165)
4 p
Thischoiceobtains
P[∥A ∥>s]≤
N (cid:88)max
C
exp(cid:32) −n(cid:26)s√ p− √√ p −1(cid:27)2(cid:33)
P[K =n]+
C
2 (A.166)
k 3 C n k N2
4
n=Nmin
 
  2
=
N (cid:88)max
C
exp  −n 2(cid:114) N
max
−1  
P[K =n]+
C
2 (A.167)
3  n  k N2
n=Nmin



(cid:124) (cid:123)(cid:122) (cid:125)
 

 ≥1 
(cid:124) (cid:123)(cid:122) (cid:125)
≥1
≤
N (cid:88)max
C exp(−n)P[K =n]+
C
2 (A.168)
3 k N2
n=Nmin
≤
N (cid:88)max
C exp(−N )P[K =n]+
C
2 (A.169)
3 min k N2
n=Nmin
C
≤C exp(−N )+ 2 (A.170)
3 min N2
(cid:18) (cid:19)
=C exp −N +C (cid:112) NlogN + C 2 (A.171)
3 K 1 N2
(cid:18) (cid:19)
≤C exp
−N
+
1(cid:112)
NlogN +
C
2 (A.172)
3 K 2 N2
(cid:18) (cid:19)
1 N C
≤C exp − · + 2. (A.173)
3 2 K N2
Toobtaintheconclusionofthetheorem,notethatanyssuchthat
(cid:115) (cid:115) √ (cid:115) √
N N/K NlogN N NlogN
s≥1+2C max =1+2C +C =1+2C +C
4 p 4 p 1 p 4 d 1 p
(A.174)
enjoysthesamehigh-probabilitybound. ByAssumption2,wehave
(cid:115) √ (cid:115)
N NlogN N 1 N/K
1+2C +C ≤1+2C + · (A.175)
4 d 1 p 4 d 2 p
(cid:114) (cid:114) (cid:114)
N 1 N 3 N
=1+2C + · =1+2C · (A.176)
4 d 2 d 4 2 d
whencetheultimateconclusionisobtainedbycombiningconstants.
Proposition 11. There are universal constants C ,C ,C ,C > 0 such that the following holds.
1 2 3 4
Letp,N,K ∈NbesuchthatAssumption2holds. LetA ,k ∈[K],bedefinedasabove. Then
k
(cid:34) (cid:114) (cid:35) (cid:18) (cid:19)
N N C
P ∥A⊤A −I∥>C ≤C exp −C + 4. (A.177)
k k 1 d 2 3K N2
32PublishedasaconferencepaperatICLR2024
√
Proof. By Propositions 12 and 15 with parameters n = n, k = K, q = p, and x = 1/ p, if we
define
(cid:22) (cid:23) (cid:24) (cid:25)
. N (cid:112) . N (cid:112)
N = −C NlogN , N = +C NlogN (A.178)
min K 1 max K 1
thenwehave
P[∥A⊤A −I∥>s] (A.179)
k k
≤
N (cid:88)max
P[∥A⊤A −I∥>s|K =n]P[K =n]+
C
2 (A.180)
k k k k N2
n=Nmin
 (cid:32) (cid:26) (cid:27)2(cid:33)
≤ NC 2 2 + nN =(cid:88) Nma mx inP[K k =n]· C
C
33 ee xx pp (cid:32) −− nn (cid:26)C C4 42 CC 55 11 √√ nn // pp √s− s−1 1(cid:27)2(cid:33),
,
i if fC s4 ≥2C C5(cid:112) 42.n/p≤s≤C 42 (A.181)
Inordertocancelthemostterms,wechoose
(cid:115)
N
s=2C2C max. (A.182)
4 5 p
Inordertoassureourselvesthatthischoicestillhass≤C2 (sothatwecanusethefirstcaseforall
4
n),wehave
(cid:115)
N
s=2C2C max (A.183)
4 5 p
(cid:115) √
N/K+C NlogN
=2C2C 1 (A.184)
4 5 p
(cid:115)
N/K+ 1N/K
=2C2C 2 (A.185)
4 5 p
(cid:114) (cid:115)
3 N/K
=2 C2C · (A.186)
2 4 5 p
(cid:114)
√ N
= 6C2C · (A.187)
4 5 d
(cid:114)
√ N
≤C2 when 6C ≤1. (A.188)
4 5 d
Ofcourse,thisconditionisassuredbyAssumption2. Nowthatwehavethis,weknowsfallsinthe
first,andsowehave
(cid:34) (cid:114) (cid:35)
N
P ∥A⊤A −I∥>C (A.189)
k k 1 d
 (cid:32) (cid:26) (cid:27)2(cid:33)
≤ NC 2 2 + nN =(cid:88) Nma mx inP[K k =n]· C
C
33 ee xx pp (cid:32) −− nn (cid:26)C C4 42 CC 55 11 √√ nn // pp √s− s−1 1(cid:27)2(cid:33),
,
i if fC s4 ≥2C C5(cid:112)
42
n/p≤s≤C 42 (A.190)
≤
NC
2
2
+
N (cid:88)max
C
3exp −n(cid:40) 2C C42C 2C5(cid:112) (cid:112)N
nm /a
px/p −1(cid:41)2
P[K
k
=n] (A.191)
n=Nmin 4 5
=
NC
2
2
+
N (cid:88)max
C
3exp −n(cid:40) 2(cid:114) N
m nax
−1(cid:41)2
P[K
k
=n] (A.192)
n=Nmin
(cid:18) (cid:19)
1 N C
≤C exp − · + 2 (A.193)
3 2 K N2
wherethelastinequalityfollowsfromtheexactsameargumentasinProposition10.
33PublishedasaconferencepaperatICLR2024
A.2.3 GENERICCONCENTRATIONINEQUALITIES
Inthissubsectionweprovethebase-levelconcentrationinequalitiesusedthroughouttheproofsin
thispaper.
Binomialconcentration.
Proposition 12. There exist universal constants C ,C > 0 such that the following holds. Let
1 2
n,k ∈ Z. For each i ∈ [k], let B ∼ Bin(n,1/k), such that the B are identically (marginally)
i i
distributedbutnotnecessarilyindependentbinomialrandomvariables. LetEbeanevent. Thenfor
anyi∈[k],wehave
√
⌈n/k+C1 nlogn⌉
P[E]≤ (cid:88) P[E|B =b]P[B =b]+ C 2. (A.194)
i i n2
√
b=⌊n/k−C1 nlogn⌋
Proof. Wehave
n
(cid:88)
P[E]=E[E[E |B ]]= P[E |B =b]P[B =b]. (A.195)
i i i
b=0
EachB isunconditionallydistributedasBin(n,1/k). ByunionboundandHoeffding’sinequality
i
(Vershynin,2018,Theorem2.2.6),wehave
(cid:18) 2t2(cid:19)
P[|B −n/k|≥t]≤2exp − . (A.196)
i n
Invertingthisinequalityobtainsthatthereexistssome(simple)universalconstantsC ,C >0such
1 2
that
P(cid:104) |B −n/k|≥C (cid:112) nlogn(cid:105) ≤ C 2. (A.197)
i 1 n3
Thus,ifwedefine
. (cid:106)n (cid:112) (cid:107) . (cid:108)n (cid:112) (cid:109)
b = −C nlogn , b = +C nlogn , (A.198)
min k 1 max k 1
thenwehave
n
(cid:88)
P[E]= P[E |B =b]P[B =b] (A.199)
i i
b=0
bm(cid:88)in−1 (cid:88)n
= P[E |B =b]P[B =b]+ P[E |B =b ]P[B =b] (A.200)
i i i 1 i
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
b=0
≤1 ≤C2/n3
b=bmax+1
≤1 ≤C2/n3
b (cid:88)max
+ P[E |B =b]P[B =b] (A.201)
i i
b=bmin
≤bm(cid:88)in−1 C
2 +
(cid:88)n C
2 +
b (cid:88)max
P[E |B =b]P[B =b] (A.202)
n3 n3 i i
b=0 b=bmax+1 b=bmin
≤(cid:88)n C
2 +
b (cid:88)max
P[E |B =b]P[B =b] (A.203)
n3 i i
b=0 b=bmin
=
C
2 +
C
2 +
b (cid:88)max
P[E |B =b]P[B =b] (A.204)
n2 n3 i i
b=bmin
≤
2C
2 +
b (cid:88)max
P[E |B =b]P[B =b]. (A.205)
n2 i i
b=bmin
34PublishedasaconferencepaperatICLR2024
Remark13. NoticethatasimpleadaptationofthisargumentcanturntheadditiveprobabilityC /n2
3
intoC′/nz foranypositiveintegerz ∈ N(whereC′ dependsonz). However,tryingtoreplaceit
3 3
withC′e−C′nismoredifficult.
3
Gaussianconcentration.
Proposition 14. There areuniversal constants C ,C ,C > 0 such thatthe following holds. Let
1 2 3
n,q ∈N,andletM ∈Rq×nbesuchthatM ∼ N(0,x2). Then
ij i.i.d.
P[∥M∥>s]≤C exp(cid:32) −n(cid:26)s/x− √√ q −1(cid:27)2(cid:33) , ∀s>x(cid:8)√ q+C √ n(cid:9) (A.206)
1 C n 2
2
P[∥M∥>s]≤C exp(cid:32) −q(cid:26) s/x− √√ n −1(cid:27)2(cid:33) , ∀s>x(cid:8)√ n+C √ q(cid:9) . (A.207)
1 C q 3
3
.
Proof. DefineM = 1M,sothatM ∼ N(0,1).ByVershynin(2018,Example2.5.8,Lemma
x ij i.i.d.
3.4.2), we see that each row M has Orlicz norm ∥M ∥ ≤ C for some universal constant
i i ψ2 1
C >0.
1
ByVershynin(2018,Theorem4.6.1)wehaveforsomeotheruniversalconstantC > 0thatforall
2
t>0,
√ √ √ √
q−C2C ( n+t)≤σ (M)≤σ (M)≤ q+C2C ( n+t) (A.208)
1 2 min(n,q) 1 1 2
.
withprobabilityatleast1−2e−t2. DefiningC =C2C andnotingthat∥·∥=σ (·),wehavewith
3 1 2 1
thesameprobabilitythat
√ (cid:0)√ (cid:1)
∥M∥− q ≤C n+t . (A.209)
3
Simplifying,weobtain
√ (cid:0)√ (cid:1)
∥M∥− q ≤C n+t (A.210)
3
1 √ (cid:0)√ (cid:1)
∥M∥− q ≤C n+t (A.211)
x 3
√ (cid:0)√ (cid:1)
∥M∥−x q ≤C x n+t (A.212)
3
(cid:8)√ (cid:0)√ (cid:1)(cid:9)
∥M∥≤x q+C n+t . (A.213)
3
Defines>0by
√
. (cid:8)√ (cid:0)√ (cid:1)(cid:9) s/x− q √
s=x q+C n+t ⇐⇒ t= − n. (A.214)
3 C
3
Notethattherangeofvalidityis
(cid:8)√ √ (cid:9)
t>0 ⇐⇒ s>x q+C n . (A.215)
3
Forsinthisrange,wehave
(cid:32) (cid:26)s/x−√
q √
(cid:27)2(cid:33) (cid:32) (cid:26)s/x−√
q
(cid:27)2(cid:33)
P[∥M∥>s]≤2exp − − n =2exp −n √ −1 . (A.216)
C C n
3 3
TheotherinequalityfollowsfromapplyingthisinequalitytoM⊤.
Proposition 15. There areuniversal constants C ,C ,C > 0 such thatthe following holds. Let
1 2 3
n,q ∈N,andletM ∈Rq×nbesuchthatM ∼ N(0,x2). Then
ij i.i.d.
P(cid:2)(cid:13) (cid:13)M⊤M −qx2I(cid:13) (cid:13)>s(cid:3) (A.217)
 (cid:18) (cid:110) (cid:111)2(cid:19) √
≤C 1exp (cid:18)−n
(cid:110)C
22C3√1
nqx
√2s−1 (cid:111)2(cid:19), ifC 22C
3
nqx2 ≤s≤C 22qx2
(A.218)
C 1exp −n C2C31√
nx
s−1 , ifs≥C 22qx2.
35PublishedasaconferencepaperatICLR2024
.
Proof. DefineM = 1M,sothatM ∼ N(0,1).ByVershynin(2018,Example2.5.8,Lemma
x ij i.i.d.
3.4.2),weseethateachrowhasOrlicznorm∥M ∥ ≤C forsomeuniversalconstantC >0.
i ψ2 1 1
ByVershynin(2018,Eq.4.22)wehaveforsomeotheruniversalconstantC >0thatforallt>0,
2
√
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)1 qM⊤M −I(cid:13) (cid:13) (cid:13)≤C 12max{δ,δ2} where δ =. C 2 n √+ q t . (A.219)
withprobabilityatleast1−2e−t2. Simplifying,weobtain
(cid:13) (cid:13)
(cid:13) (cid:13)1 M⊤M −I(cid:13) (cid:13)≤C2max{δ,δ2} (A.220)
(cid:13)q (cid:13) 1
(cid:13) (cid:13)M⊤M −qI(cid:13) (cid:13)≤C2qmax{δ,δ2} (A.221)
1
(cid:13) (cid:13)(x−1M)⊤(x−1M)−qI(cid:13) (cid:13)≤C2qmax{δ,δ2} (A.222)
1
(cid:13) (cid:13)x−2M⊤M −qI(cid:13) (cid:13)≤C2qmax{δ,δ2} (A.223)
1
(cid:13) (cid:13)M⊤M −qx2I(cid:13) (cid:13)≤C2qx2·max{δ,δ2}. (A.224)
1
Nowfromsimplealgebraandthefactthatn≥1,wehave
max(cid:8) δ,δ2(cid:9)
=δ ⇐⇒
0≤t≤C−1√ q−√
n (A.225)
2
max(cid:8) δ,δ2(cid:9) =δ2 ⇐⇒ t≥C−1√ q−√ n. (A.226)
2
Nowdefines≥0by
.
s=C2qx2·max{δ,δ2}. (A.227)
1
Thusinthefirstcasewehave
. √ √ 1 √
s=C2C qx2( n+t) ⇐⇒ t= √ s− n, (A.228)
1 2 C2C qx2
1 2
andinparticularthefirstcaseholdswhen
√ √ √
max{δ,δ2}=δ ⇐⇒ 0≤t≤C−1 q− n ⇐⇒ C2C nqx2 ≤s≤C2qx2. (A.229)
2 1 2 1
Meanwhile,inthesecondcase,wehave
. √ 1 √ √
s=C2C2( n+t)2x2 ⇐⇒ t= s− n (A.230)
1 2 C C x
1 2
whereweobtainonlyonesolutiontothequadraticequationbyrequiringt≥0,andinparticularthe
secondcaseholdswhen
√ √
max{δ,δ2}=δ ⇐⇒ t≥C−1 q− n ⇐⇒ s≥C2qx2. (A.231)
2 1
Thuswehave
P[∥M⊤M −qx2I∥>s] (A.232)
 (cid:18) (cid:110) √ (cid:111)2(cid:19) √
≤2exp (cid:18)− (cid:110)C 12C21 √ √qx2s−
√
(cid:111)n
2(cid:19)
, ifC 12C 2 nqx2 ≤s≤C 12qx2
(A.233)
2exp − C1C1
2x
s− n , ifs≥C 12qx2
 (cid:18) (cid:110) (cid:111)2(cid:19) √
=2exp (cid:18)−n
(cid:110)C
12C2√1
nqx
√2s−1 (cid:111)2(cid:19), ifC 12C
2
nqx2 ≤s≤C 12qx2
(A.234)
2exp −n C1C21√
nx
s−1 , ifs≥C 12qx2.
36PublishedasaconferencepaperatICLR2024
A.3 COMPANIONTOSECTION2.4
In this section, we justify the scaling applied to ∇Rc in Section 2.4 and supply the discretization
scheme.
.
First,supposethatZℓsatisfies(2.1),andZ =Zℓ+σ W,whereW isastandardGaussianmatrix,
♮ t ♮ t
sothatZ satisfies(2.2)withnoiselevelσ > 0. Letq bethedensityofZ . Theoreticalanalysis
t t t t
from(Luetal.,2023)andempiricalanalysisfrom(Song&Ermon,2019)demonstratesthatunder
genericconditions,wehavethat
1
∥∇q (Z )∥ ∝ , (A.235)
t t 2 σ2
t
ignoringalltermsintheright-handsideexceptforthoseinvolvingσ . Ontheotherhand,fromthe
t
proofofTheorem3,weobtainthat−∇Rc(Z )hasconstant(inσ )magnitudewithhighprobability.
t t
Thus,inordertohavethembethesamemagnitude,weneedtodivide−∇Rc(Z )byσ2 tohaveit
t t
beadrop-inreplacementforthescorefunction,asalludedtoinSections2.3and2.4.
Second, we wish to explicitly state our discretization scheme given in Section 2.4. To wit, we
provide a discretization scheme that turns the structured diffusion ODE (2.15) into its discretized
analogue;theotherdiscretizationofthestructureddenoisingODE(2.11)occurssimilarly. Tobegin
with,definetheshorthandnotation
.
f(t,Y(t))=∇Rc(Y(t)|U (T −t)), (A.236)
[K]
sothatwehave
1
dY(t)= f(t,Y(t))dt. (2.11)
2t
FixL,andlet0 < t < t < ··· < t = T,suchthatt issmall. (Thesewillbespecifiedshortly
1 2 L 1
inordertosupplythediscretizationscheme.) Asuitablefirst-orderdiscretizationisgivenby
t −t
Yℓ+1 ≈Yℓ+ ℓ+1 ℓf(t ,Yℓ). (A.237)
2t ℓ
ℓ
Thusitremainstosett ,...,t suchthat
1 L
t −t
ℓ+1 ℓ =κ (A.238)
2t
ℓ
forsomeconstantκ,weobservethatwemustset
t =(1+2κ)t , (A.239)
ℓ+1 ℓ
so that the time grows exponentially in the index. The reverse process time decays exponentially
intheindex,whichmatchespracticaldiscretizationschemesforordinarydiffusionmodels(Song&
Ermon,2019). Finally,wehaveT =t =(1+2κ)Lt ,sothatt = T .
L 1 1 (1+2κ)L
37PublishedasaconferencepaperatICLR2024
B ADDITIONAL EXPERIMENT DETAILS
B.1 EXPERIMENTDETAILSANDCLARIFICATIONS
Inallsetups,asisstandardpractice(Heetal.,2022),weappendatrainableclasstokenz1 ∈ Rd
cls
aftermaskingandlinearprojection,namely,
fpre(X)=[z1 ,WpreX+Epos]. (B.1)
cls
Everything else goes through with Z having N +1 instead of N tokens, indexing the N image
tokens’intermediaterepresentationsaszℓ,etc.,andindexingtheclasstokenintermediaterepresen-
i
tationaszℓ . Attheend,thepost-processingmapis
cls
gpost(YL)=WpostYL (B.2)
1:N
whereYL ∈ Rd×N isthematrixwhosecolumnsarethecolumnsofYL correspondingtoimage
1:N
tokens,i.e.,thesecondthroughlastcolumnsofYL.Thus,theclasstokenz1 (anditsrepresentation
cls
z ,i.e.,thefirstcolumnofZ)areneithermaskedorreconstructed.
cls
Fortrainingusingmaskedautoencoding,wefollowamodifiedrecipeof(Heetal.,2022). Wemask
afixedpercentageµ ∈ [0,1]ofrandomlyselectedtokensinX;thatis,werandomlyset(1−µ)N
image tokens x to 0 ∈ RD, obtaining X. Then X is the input to the encoder. Unlike in (He
i
et al., 2022), the decoder receives no special treatment, and operates on all token representations.
Thelossiscomputedonlyonthemaskedimagepatches. RefertoHeetal.(2022)formoreMAE
implementationdetails.
Forfine-tuningusingsupervisedclassification,weusetherepresentationz oftheso-far-vestigial
cls
class token as the feature used for classification. Namely, we obtain the unnormalized log-
.
probabilities for the classes as u = WheadLN(z ), where LN is a trainable layer-norm and
cls
Whead ∈RC×disatrainableweightmatrix,whereCisthenumberofclasses. Theoutputu∈RC
istheinputtothesoftmaxcross-entropyloss.Allmodelparametersaretrainableduringfine-tuning,
whileinlinearprobingonlytheweightmatrixWheadistrainable(andinfactlearnedviafull-batch
logisticregression).
Inalltrainingsetups,weaveragethelossoverallsamplesinthebatch.
Wepre-trainCRATE-MAEonImageNet-1K(Dengetal.,2009). WeemploytheAdamWoptimizer
(Loshchilov&Hutter,2019). Weconfigurethelearningrateas3×10−5,weightdecayas0.1,and
batchsizeas4,096.
We fine-tune and linear probe our pre-trained CRATE-MAE on the following target datasets: CI-
FAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford Flowers-102 (Nilsback & Zisserman, 2008),
Oxford-IIIT-Pets (Parkhi et al., 2012). For each fine-tuning task, we employ the AdamW opti-
mizer (Loshchilov & Hutter, 2019). We configure the learning rate as 5 × 10−5, weight decay
as 0.01, and batch size as 256. For each linear probing task, we use the linear probing function-
ality in Scikit-Learn (Pedregosa et al., 2011). For each evaluation we choose several regularizers
C ∈ {100,101,102,103,104,105}, train a logistic regression model on features from the whole
dataset, and choose the logistic regression model with the best performance. All numbers are re-
portedonthetestsets.
For experiments, we use the model configurations reported in Table 1. From this table, there are
two unspecified hyperparameters, namely λ and η. In all experiments we fix η = 0.1, sincce it
only multiplies with a trainable matrix and λ. In numerical experiments we use λ = 0.5, while
figuresaregeneratedfrommodelswithhyperparameterλ=5.0,thoughthedifferenceinnumerical
performanceandfigurequalityismarginalbetweenthetwosettings(Table6).
Toallowtransferlearning,inalltrainingandevaluationssetupswefirstresizeourinputdatato224
heightandwidth. Fordataaugmentationsduringpre-trainingandfine-tuning,wealsoadoptseveral
standardtechniques: randomcropping,randomhorizontalflipping,andrandomaugmentation(with
numberoftransformationsn=2andmagnitudeoftransformationsm=14).
B.2 PYTORCH-LIKEPSEUDOCODE
38PublishedasaconferencepaperatICLR2024
Listing1: PyTorch-LikeCodeforMSSAandISTA
class ISTA:
# initialization
def __init__(self, dim, hidden_dim, dropout = 0., step_size=0.1,
lambd=0.1):
super().__init__()
self.weight = nn.Parameter(torch.Tensor(dim, dim))
with torch.no_grad():
init.kaiming_uniform_(self.weight)
self.step_size = step_size
self.lambd = lambd
# forward pass
def forward(self, x):
x1 = F.linear(x, self.weight, bias=None)
grad_1 = F.linear(x1, self.weight.t(), bias=None)
grad_2 = F.linear(x, self.weight.t(), bias=None)
grad_update = self.step_size * (grad_2 - grad_1) - self.step_size
* self.lambd
output = F.relu(x + grad_update)
return output
class MSSA:
# initialization
def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
inner_dim = dim_head * heads
project_out = not (heads == 1 and dim_head == dim)
self.heads = heads
self.scale = dim_head ** -0.5
self.attend = Softmax(dim = -1)
self.dropout = Dropout(dropout)
self.qkv = Linear(dim, inner_dim, bias=False)
self.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout))
if project_out else nn.Identity()
# forward pass
def forward(self, x):
w = rearrange(self.qkv(x), ’b n (h d) -> b h n d’, h = self.heads)
dots = matmul(w, w.transpose(-1, -2)) * self.scale
attn = self.attend(dots)
attn = self.dropout(attn)
out = matmul(attn, w)
out = rearrange(out, ’b h n d -> b n (h d)’)
return self.to_out(out)
Listing2: PyTorch-LikeCodeforCRATE-MAEEncoder
class CRATE_Encoder:
# initialization
def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout =
0.):
self.layers = []
self.depth = depth
for _ in range(depth):
self.layers.extend([LayerNorm(dim), MSSA(dim, heads, dim_head,
dropout)])
self.layers.extend([LayerNorm(dim), ISTA(dim, mlp_dim,
dropout)])
# forward pass
def forward(self, x):
for ln1, attn, ln2, ff in self.layers:
x_ = attn(ln1(x)) + x
x = ff(ln2(x_))
return x
39PublishedasaconferencepaperatICLR2024
Listing3: PyTorch-LikeCodeforCRATE-MAEDecoder
class CRATE_Decoder:
# initialization
def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout =
0.):
# define layers
self.layers = []
self.depth = depth
for _ in range(depth):
self.layers.extend([LayerNorm(dim), Linear(in_features=dim,
out_features=dim, bias=False)])
self.layers.extend([LayerNorm(dim), MSSA(dim, heads, dim_head,
dropout)])
# forward pass
def forward(self, x):
for ln1, f_linear, ln2, attn in self.layers:
x_ = f_linear(ln1(x))
x = ln2(x_) - attn(ln2(x_))
return x
B.3 VISUALIZATIONMETHODOLOGY
Inthissubsectionweformallydescribetheproceduresweusedtogeneratethevisualizationsused
to evaluate the segmentation property of CRATE-MAE in Figure 7. Much of this evaluation is the
sameasinYuetal.(2023b),whichinitiallydemonstratestheemergentsegmentationpropertiesof
white-boxarchitectures.
B.3.1 PCAVISUALIZATIONS
WerecapitulatethemethodtovisualizethepatchrepresentationsinFigure7ausingPCAfromAmir
etal.(2021);Oquabetal.(2023);Yuetal.(2023b).
WefirstselectJ imagesthatbelongtothesameclass,{X }J ,andextractthetokenrepresenta-
j j=1
tions for each image at layer ℓ, i.e., (cid:2) zℓ ,...,zℓ (cid:3) for j ∈ [J]. In particular, zℓ represents the
j,1 j,N j,i
ith tokenrepresentationattheℓth layerforthejth image. Wethencomputethefirstprincipalcom-
ponentsofZ(cid:98)ℓ ={z (cid:98)1ℓ ,1,...,z (cid:98)1ℓ ,N,...,z (cid:98)Jℓ ,1,...,z (cid:98)Jℓ ,N},andusez (cid:98)jℓ
,i
todenotetheaggregatedtoken
representation for the i-th token of X , i.e., zℓ = [(U∗zℓ )⊤,...,(U∗zℓ )⊤]⊤ ∈ R(p·K)×1.
j (cid:98)j,i 1(cid:98)j,i K(cid:98)j,i
We denote the first eigenvector of the matrix Z(cid:98)∗Z(cid:98) by u
0
and compute the projection values as
(cid:26)
x, |x|≥λ
{σ (⟨u ,zℓ ⟩)} , where σ (x) = is the hard-thresholding function. We then se-
λ 0 j,i i,j λ 0, |x|<λ
lectasubsetoftokenrepresentationsfromZ(cid:98)withσ λ(⟨u 0,z jℓ ,i⟩)>0.whichcorrespondtonon-zero
projection values after thresholding, and we denote this subset as Z(cid:98)s ⊆ Z(cid:98). This selection step is
usedtoremovethebackground(Oquabetal.,2023). Wethencomputethefirstthreerightsingular
vectors of Z(cid:98)s with the first three eigenvectors of the matrix Z(cid:98) s∗Z(cid:98)s denoted as {u 1,u 2,u 3}. We
definetheRGBtupleforeachtokenas:
[r j,i,g j,i,b j,i]=[⟨u 1,z jℓ ,i⟩,⟨u 2,z jℓ ,i⟩,⟨u 3,z jℓ ,i⟩], i∈[N], j ∈[J],z jℓ
,i
∈Z(cid:98)s. (B.3)
Next,foreachimageX wecomputeR ,G ,B ,whereR = [r ,...,r ]⊤ ∈ Rd×1 (similar
j j j j √j √j,1 j,N
forG andB ). Thenwereshapethethreematricesinto N × N andvisualizethe“principal
j j
√ √
components”ofimageX viatheRGBimage(R ,G ,B )∈R3× N× N.
j j j j
B.3.2 VISUALIZINGATTENTIONMAPS
We recapitulate the method to visualize attention maps in Abnar & Zuidema (2020); Caron et al.
(2021).
40PublishedasaconferencepaperatICLR2024
Table3: Top-1classificationaccuracyofCRATE-MAE-Basewhenpre-trainedonImageNet-1Kandfine-
tunedonclassificationforvariousdatasets. Wecompareafine-tunedmodelwhichwaspre-trainedonthe
MAEtaskwithamodeltrainedfromscratchonclassification(“randominit”)usingexactlythesameexperi-
mentalconditions.Ourresultsshowthattherepresentationlearningoccurringduringpre-trainingsubstantially
improvesperformanceondownstreamtasks.
ClassificationPerformance CIFAR10 CIFAR100 OxfordFlowers Oxford-Pets
CRATE-MAE-Base(trained) 96.8 80.3 78.5 76.7
CRATE-MAE-Base(randominit) 85.1 58.8 38.0 28.8
Table 4: Average reconstruction loss over the training and validation sets of ImageNet-1K for both
CRATE-MAE-BaseandViT-MAE-Base.WeseethattheperformanceofCRATE-MAE-Base,whileabitworse
thanViT-MAE-Base,obtainspromisingperformanceonthechallengingmaskedautoencodingtask.
ReconstructionLoss TrainingLoss ValidationLoss
CRATE-MAE-Base 0.265 0.302
ViT-MAE-Base 0.240 0.267
Table5:Top-1classificationaccuracyofCRATE-MAE-Basewhenpre-trainedonImageNet-1Kandlinear
probedforclassificationonCIFAR-10,whenpre-trainedusingdifferentmaskpercentage(i.e.,numberof
maskedtokensineachsample).ThisshowsthatCRATE-MAEmodelswith75%ofthetokensmaskedduring
training tend to have the most structured representations that are useful for downstream tasks, an empirical
conclusionthatechoesHeetal.(2022),butawiderangeofmaskpercentagesresultingoodrepresentations.
(Note:Thistableusesthehyperparametersettingλ=5.0.)
ClassificationAccuracy 25%Masked 50%Masked 75%Masked 90%Masked
CRATE-MAE-Base 69.78 75.97 75.99 73.45
Forthekth headattheℓth layeroftheencoderofCRATE-MAE,wecomputetheself-attentionmap
Aℓ ∈RN definedasfollows:
k
 Aℓ 
Aℓ = k . .,1 ∈RN, where Aℓ = exp(⟨U kℓ∗z iℓ,U kℓ∗z cℓ ls⟩) . (B.4)
k  .  k,i (cid:80)N exp(⟨Uℓ∗zℓ,Uℓ∗zℓ ⟩)
Aℓ j=1 k j k cls
k,N
wherezℓ istheℓthlayerrepresentationoftheclasstoken.
cls
√
Foreachimage,wereshapetheattentionmatrixAL−1forthepenultimatelayerL−1intoa N×
√ k
N matrix and visualize the heatmaps as shown in Figure 7b. For example, the ith row and the
jth columnelementofeachheatmapinFigure7bcorrespondstothemth componentofAℓ,where
√ k
m=(i−1)· N +j. InFigure7b,foreachimageweselectoneattentionheadkofCRATE-MAE
andvisualizetheattentionmatrixAL−1.
k
B.4 ADDITIONALEXPERIMENTS
InthissectionweperformmoreexperimentstoexplorepropertiesoftheCRATE-MAEarchitecture.
First,inTable3,wecomparethefine-tuningperformanceofanCRATE-MAE-BasemodelwithMAE
pretrainingagainstanCRATE-MAE-Basemodelwithnopretrainingatall(i.e.,randomlyinitialized).
Weapplythesamefine-tuningprocesstobothmodels,andweobserveamassivedisparityinper-
formance, where the pre-trained model succeeds while the randomly initialized model performs
poorly. Thisindicatesthattheorganizedrepresentationsofapre-trained CRATE-MAE-Basemodel
areastrongstartingpointwhenfine-tuningfordownstreamtasks.
Next,inTable4,weevaluatethereconstructionloss(measuredinmean-squarederror)of CRATE-
MAE-BaseversusViT-MAE-Base, evaluatedonthetrainingandtestsetsofImageNet-1K.Weob-
serve that while CRATE-MAE-Base performs slightly worse at masked reconstruction, the perfor-
manceisstillveryreasonable.
41PublishedasaconferencepaperatICLR2024
Table6:Top-1classificationaccuracyofCRATE-MAE-Basewhenpre-trainedonImageNet-1Kandlinear
probedforclassificationonvariousdatasets,whenpre-trainedusingdifferentλ.ThisshowsthatCRATE-
MAEmodelsperformreasonablywellatCRATE-MAE-Basescaleacrossdifferentvaluesofλ.
ClassificationAccuracy λ=0.1 λ=0.5 λ=5.0
CRATE-MAE-Base 83.33 80.87 75.99
Masked ViT-MAE CRATE-MAE Original Masked ViT-MAE CRATE-MAE Original
Figure 8: Moreinstancesofparity between CRATE-MAE-BaseandViT-MAE-Basein themaskedau-
toencodingtask. EchoingthemessageofFigure6,wefindthatCRATE-MAE-BaseandViT-MAE-Basehave
similar performance on the masked autoencoding task, even as the CRATE-MAE-Base model is significantly
moreparameter-efficient.
InTable5,wecheckthedownstreamperformanceandfeaturelearningofCRATE-MAE-Basemodels
when varying the mask size. We use test accuracy of linear probing on CIFAR10 as a proxy for
the quality of the learned features. Our results show that the performance is maximized when the
numberofmaskedtokensis75%ofthenumberoftotaltokens,i.e.,when75%oftokensaremaxed
out. ThisconfirmstheexperimentsinHeetal.(2022).
InTable6,wecheckthedownstreamperformanceandfeaturelearningofCRATE-MAE-Basemodels
whenvaryingthehyperparameterλ. WeagainusetestaccuracyoflinearprobingonCIFAR10as
aproxyforthequalityofthelearnedfeatures. Ourresultsshowthattheperformanceismaximized
whenλ=0.1,butallmodelsperformreasonablywellatCRATE-MAE-Basescale.
InFigure8weprovidemoreexamplesofthemaskedautoencodingefficacyofboth CRATE-MAE-
BaseandViT-MAE-Base.OurresultsconfirmthoseofFigure6,namelyCRATE-MAE-Baseachieves
paritywiththemuchlargerViT-MAE-Baseonthechallengingmaskedautoencodingtask.
InFigure9,weprovidemoreexamplesofthelinearityoftherepresentationswithin CRATE-MAE-
Base. Weseethatoverawidevarietyofimages,thefirstthreeprincipalcomponentsofeachclass
correlate strongly with semantically meaningful patches of the input image. These results extend
Figure7a.
42PublishedasaconferencepaperatICLR2024
Figure 9: More examples of linearized representations in CRATE-MAE-Base. Echoing the message of
Figure 7a, we find that CRATE-MAE-Base has a linear feature space. In particular, the first three principal
componentsstronglycorrelatewiththemainsemanticcontentoftheimage.
Figure10: MoreexamplesofinterpretableattentionmapsinCRATE-MAE-Base.Echoingthemessageof
Figure7b,wefindthatCRATE-MAE-Basehashuman-interpretableattentionmapswhichsemanticallysegment
theforegroundofinputimages.
InFigure10,weprovidemoreexamplesoftheinterpretabilityofselectedattentionoutputswithin
CRATE-MAE-Base. Weseethatoverawidevarietyofimages,theattentionmapsucceedsatcaptur-
ingmanysemanticsoftheinputimage. TheseresultsextendFigure7b.
InFigure11weapply(nearly)thesamemethodologyinvolvedinconstructingFigure7,aqualitative
demonstrationofthefeaturequalityoftheCRATE-MAE-Baseencoder,toevaluatethefeaturequality
oftheViT-MAE-Baseencoder. Onenecessarydifferenceisthattoevaluatetheattentionmaps,the
ViTdoesnothavetheUℓ matricesthatCRATEdoes,butinsteadhasthreesetsofmatricesQℓ ,
[K] [K]
Kℓ ,andVℓ ;thus,weconstructtheattentionmapsviathefollowingequation:
[K] [K]
exp(⟨Kℓzℓ,Qℓzℓ ⟩)
Aℓ = k i k cls . (B.5)
k,i (cid:80)N exp(⟨Kℓzℓ,Qℓzℓ ⟩)
j=1 k j k cls
43PublishedasaconferencepaperatICLR2024
(a)VisualizingPCAofViT-MAE-Base. (b)VisualizingattentionmapsofViT-MAE-Base.
Figure 11: AcomparisonofCRATEtoViT-MAEinthesettingofFigure7. Weusethevisualizations
ofPCAonthetokenrepresentationsandattentionmaps,introducedinFigure7,toqualitativelyevaluatethe
representationqualityoftheViT-MAE-Base. Bycomparingthisfigure(Figure11)andFigure7,weobserve
thatCRATE-MAE-BaseattentionmapscontainmoreclearsemanticsthanthosefromViT-MAE-Base,whileboth
CRATE-MAE-Base and ViT-MAE-Base have nearly-linear representation spaces wherein semantic concepts
correspondtothefirstthreeprincipalcomponents.
Overall, Figure 7 and Figure 11 demonstrate that, at least empirically, the attention semantics in
CRATE-MAE-BasearesignificantlybetterandclearerthanViT-MAE-Base. ThereasonthatCRATE-
MAE models have semantically meaningful attention maps may be due to our white-box design,
namelythefactthatin CRATE-MAE wehaveQℓ = Kℓ = Vℓ = Uℓ∗ ; indeed, thefactthat
[K] [K] [K] [K]
setting Qℓ = Kℓ = Vℓ = Uℓ∗ yields semantically meaningful attention maps has been
[K] [K] [K] [K]
showninotherwork, albeitinadifferentsetting(Yuetal.,2023b). ThereasonthattheViT-MAE
has semantically meaningful attention maps, albeit not as clear and worse than CRATE-MAE, may
be due to several different factors such as using the class token as a register (Darcet et al., 2023).
Nevertheless,thefeaturesinbothmodelshavelinearstructures,oratleasteachclass’representations
havethreeprincipalcomponentswhichcorrelatestronglywiththesemanticsoftheclass.
44