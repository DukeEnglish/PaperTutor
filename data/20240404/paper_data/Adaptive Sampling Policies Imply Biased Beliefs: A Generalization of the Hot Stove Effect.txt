ADAPTIVE SAMPLING POLICIES IMPLY BIASED BELIEFS:
A GENERALIZATION OF THE HOT STOVE EFFECT
JERKERDENRELL
UNIVERSITYOFWARWICK
JDENRELL@GMAIL.COM
Abstract. TheHotStoveEffectisanegativitybiasresultingfromtheadaptive
characteroflearning. Themechanismisthatlearningalgorithmsthatpursueal-
ternativeswithpositiveestimatedvalues,butavoidalternativeswithnegativees-
timatedvalues, willcorrecterrorsofoverestimationbutfailtocorrecterrorsof
underestimation. Here,wegeneralizethetheorybehindtheHotStoveEffectto
settingsinwhichnegativeestimatesdonotnecessarilyleadtoavoidancebuttoa
smallersamplesize(i.e.,alearnerselectsfewerofalternativeBifBisbelievedtobe
inferiorbutdoesnotentirelyavoidB).Weformallydemonstratethatthenegativ-
itybiasremainsinthisset-up.WealsoshowthereisanegativitybiasforBayesian
learnersinthesensethatmostsuchlearnersunderestimatetheexpectedvalueof
analternative.
Keywords:Learning;Bias;Sampling;Bayesianmodels
1. Introduction
Learningfromexperiencedoesnotnecessarilygenerateunbiasedbeliefs,partly
duetopsychologicalbiasesbutalsoduetobiasesintheinformationlearnerssam-
pleandgetexposedto.Oneimportantbiasinsamplingistheso-called"hotstove
effect"(Denrell&March,2001)whichreferstotheasymmetryinerrorcorrection
generated by adaptive learning processes. The key idea is that the tendency to
avoid alternatives with unfavorable past outcomes generates a biased set of ex-
periences. Alternativesthatareunderestimated-believedtobeworsethanwhat
theyare–areunlikelytobetriedandsampledagain,whichimpliesthaterrorsof
underestimationareunlikelytobecorrected. Alternativesthatareoverestimated
-believedtobebetterthanwhattheyare-arelikelytobetriedandsampledagain
whichimpliesthaterrorsofoverestimationarelikelytobecorrected. Thisasym-
metry in error correction generates a biased set of experiences that, in turn, can
giverisetobiasedjudgments(Denrell,2005),includingin-groupbiasandappar-
entrisk-aversebehavior(Denrell,2005,2007).
There is good experimental support for the hot-stove effect at the individual
levelandresearchersinpsychologyhavereliedonthehot-stoveeffecttoexplain
regularitiesinrisktakinginexperimentalstudies(Erev&Roth,2014)andwhypeo-
pleunderestimatethetrustworthinessofothers(Fetchenhauer&Dunning,2014).
Researchers in finance (Dittmar & Duchin, 2016) have used field data to demon-
strate that the hot stove effect can explain the risk-taking behavior of executives.
1
4202
rpA
3
]GL.sc[
1v19520.4042:viXraADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 2
The hot stove effect also has important implications for information aggregation
andonlinereviews:Ifconsumersavoidproductswithpoorreviewsandconsumers
reviewproductstheybuy,negativereviewswillbemorepersistentthanpositive
reviews,generatingbiasedaverages(Mensetal.,2018).
Pasttheoreticalworkonthehotstoveeffecthasassumedthatnegativeexperi-
encesmayleadtoavoidance,thatis,thealternativehasnotbeentriedatall(Den-
rell,2005,2007).Clearly,ifnomoreinformationisavailable,anegativeimpression
will persist. In many settings, however, a negative belief or impression may not
leadtoavoidanceofthealternative,butrathertoasmallersamplesize.Ananimal
withamorefavorableimpressionoftheenergycontentofaplantoftypeAthanof
aplantoftypeBmaysearchforplantsoftypeA.Duringthissearchforplantsof
typeA,someplantsoftypeBmaybeincidentallyfound.Theresultisthatthean-
imalsamplesmoreplantsoftypeA(becausethesearchisfocusedonsuchplants)
thanoftypeB,buttheanimaldoesnotavoidplantsoftypeBbutsimplysamples
fewer of them. Similarly, a firm may prefer to hire graduates from university A,
but may hire some, although fewer, graduates from university B if there are not
enoughgraduatesfromAthatacceptitsoffers.
Inthispaper,Igeneralizethetheorybehindthehot-stoveeffectandshowthat
itholdsevenifanegativeimpressiononlyleadstoareductioninthesamplesize,
notnecessarilytoavoidance.Specifically,Ishowthatthefinalbeliefwillbebiased
for a broad class of learning algorithms in which the sample size is a function of
thepastbelief. Ifthesamplesizeishigherandthepastbeliefwasmorepositive,
thereisanegativitybias: Thefinalbeliefwillbelowerthantheexpectedvalueof
therandomvariablethelearnerislearningabout.Thisresultalsoappliestotaking
averages:theaverageofasamplewillbebiasedifthetotalsamplesizeisafunction
of the average based on an initial subset. The bias will be eliminated in the long
runasthenumberofsamplesincreases,butintheshortrunitcanbesignificant.
I also examine whether the bias remains for a Bayesian learner. I show that
there is no bias on average for a Bayesian learner: the average belief is equal to
theexpectedvalueoftherandomvariablethelearnerislearningabout.However,
I also show that most Bayesian learners will underestimate the variable they are
learningaboutifthesamplesizeisanincreasingfunctionoftheinitialbelief.
Theseresultsimplythatalargeclassofsensibleandadaptivelearningprocesses
can be expected to generate biased beliefs, even if decision-makers process the
availableinformationinaseeminglyunbiasedway(i.e.,takingaverages).Indeed,
even rational Bayesian learners will tend to underestimate the expected value of
analternative(i.e.,mostwilldoso)ifthetotalsamplesizeishigherwhentheini-
tially observed payoffs are high. Adaptive sampling processes are common and
oftennecessarytoreducesearchcosts. Itisnotsensible,forexample,tocontinue
tosampleanalternativeafixednumberoftimesifinitialtrialsrevealthatthisal-
ternativehasamuchlowerpayoffthanotheravailablealternatives.Theresultsin
thispapershowthatevenunbiasedprocessingofinformationgeneratedbysuchADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 3
samplingpoliciescangenerateseeminglybiasedbeliefs. Adaptivesamplingpoli-
ciesthusofferanalternativeexplanationofbiasesinbeliefs,suchasatendencyto
underestimatetheextenttowhichothersaretrustworthy.
2. Illustration
Toillustratethebasicideas,weconsiderasimpletwo-periodset-up. Inperiod
one,alearnersamplesanalternativektimesandobservesthepayoffsgenerated.
Thatis,thelearnerobserveskpayoffs,x ,x ,...,x ,eachindependentlydrawn
1,1 1,2 1,k
fromthepayoffdistribution f(x),where f(x)isassumedtobeanormaldistribu-
tion with mean zero and variance σ2. Based on the observed payoff, the learner
computestheaverageobservedpayoffafterthefirstperiod: x¯ = (1/k)∑k x .
1 j=1 1,j
In the second period, the learner takes an additional sample and observes m
payoffs, x ,x ,...,x , eachindependentlydrawnfromthepayoffdistribution
2,1 2,2 2,m
f(x). Weassumethatthesizeofthissample, m,isafunctionof x¯ . Forexample,
1
the learner may take a larger sample if the observed average first-period payoff
is high (x¯ is high), than if the observed average first-period payoff is low (x¯ is
1 1
low),becausethealternativeisbelievedtobemorerewardingwhentheobserved
average first-period payoff is high compared to when it is low. To illustrate the
impact of such an adaptive sample size policy, suppose that the sample size in
period two is equal to m = h(high) whenever x¯ > c and equal to m = l(low)
1
wheneverx¯ ≤ c.
1
Afterthesecondperiod,thelearnercomputestheaverageofallthepayoffsob-
served in periods one and two: x¯ = (1/(k+m))[∑k x +∑m x ]. We are
2 j=1 1,j j=1 2,j
interestedtoseewhetherthisaverageisunbiasedornot.
The answer is that this average will be biased. To illustrate this, suppose that
the learner samples two payoffs in the first period (k = 2), samples ten more if
thefirst-periodaverageispositive(h =10)butonlysamplesonemoreifthefirst-
period average is negative (l = 1). The average of all observed payoffs after the
secondperiodwillthenbenegative.Whenσ,thestandarddeviationofthepayoff
equals 1, then E[x¯ ] = −0.141 and the proportion of negative averages is 0.587.
2
When σ = 5, E[x¯ ] = −0.705 and the proportion of negative averages is 0.583.
2
Moregenerally,
√
(1) E[x¯ ] = −√ σ k(h−l) e−c2k/2σ2 ,
2
2π(k+l)(k+h)
(seetheappendixforaderivationofthisequation). Thisequationshowsthatthe
biasdependsonhowthesamplesizevarieswith x¯ . Ifthelearnertakesalarger
1
samplewhenx¯ ishighcomparedtowhenx¯ islow(h > l),thereisanegativebias:
1 1
E[x¯ ] < 0. If the learner instead takes a larger sample when x¯ is low compared
2 1
to when x¯ is high (h < l), there is a positive bias: E[x¯ ] > 0. Equation (1) also
1 2
impliesthatthebiasisgreaterwhenthepayoffsaremorevariable,thatis,whenσ2
isgreater.Insummary,anaveragebasedonanadaptivesamplingpolicy(adaptiveADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 4
inthesensethatthesamplesizeisafunctionoftheinitialaverage)willbebiased,
andthebiasdependsonthetypeofsamplingpolicy(increasingordecreasingin
the initial average). Because learners often regulate sample sizes in response to
feedbackfrominitialsamples, thisimpliesthatlearningprocessesgenerallylead
tobiasedbeliefs,atleastifthebeliefsarebasedontheaveragepayoffsobserved.
Notethatthisoccursevenifthedecisionmakerdoesnotprocesstheinformation
in a biased way. The decision maker takes the average of the observed payoffs.
Thisaveragingprocess,whichisunbiasedwhenthesamplesizeisfixed,becomes
biasedwhenthetotalsamplesizedependsontheinitiallyobservedaverage.
3. Intuition
Tounderstandthereasonforthebias,notethatthesumofallobservationsafter
thesecondperiodisthesumoftwocomponents:thesumofallobservationsafter
the first period (∑k x ) and the sum of all observations in the second period
j=1 1,j
(∑m x .). Thesamplesizetakeninthesecondperiod(m)willaffecttherelative
j=1 2,j
weightofthesetwocomponents.Thereasonforthenegativebias,whenthelearner
takesalargersamplewhen x¯ ishighcomparedtowhen x¯ islow(h > l),isthat
1 1
thefirstcomponentwillbeweightedrelativelymorewhenx¯ islow,whichimplies
1
thatthesamplesizetakeninperiodtwoislow(m = l).
To illustrate this, suppose that the average after two observations in the first
period (k = 2) is x¯ = 1. The sum of the first two periods was equal to kx¯ =
1 1
2. Suppose c = 0, implying that the learner will take a large sample (m = h),
whenever x¯ > 0. Supposeh = 10,thatis,thesamplesizeisteninperiodtwoif
1
thefirstperiodaverageispositive.Theaveragepayoffobservedinbothperiodsis
then
2+∑1 j=0 1x 2,j 2 ∑1 j=0 1x i,j
x¯ = = + .
2 2+10 2+10 2+10
Theexpectedvalueofanobservationinthesecondperiodiszero,thatis,E[x =
2,j
0].Itfollowsthat
2 1
E[x¯ |x¯ =1] = = ,
2 2 2+10 6
whichisclosetozero.Apositivefirst-periodaveragepayofftendstoregresstothe
meanofthedistribution,whichiszero.Thereasonisthatalargesampleistakenin
thesecondperiod,whichimpliesthatthefirst-periodaveragewillnotbeweighted
much.
Suppose,incontrast,thattheaverageaftertwoobservationsinthefirstperiod
(k =2)wasx¯ = −1.Thesumofthefirsttwoperiodswasthenequaltokx¯ = −2.
1 1
When c = 0, the learner will take a small sample (m = l) in the second period
becausex¯ = −1 < c. Supposel = 1: onlyonesampleistakenifthebeliefinthe
1
firstperiodwasnegative.Theaveragepayoffobservedinbothperiodsisthen
−2+x −2 x
x¯ = 2,1 = + 2,1 .
2 2+1 2+1 2+1ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 5
Again,theexpectedvalueofanobservationinthesecondperiodiszero,i.e.E[x =
2,1
0].Itfollowsthat
−2
E[x¯ |x¯ = −1] = ,
2 2
3
whichisfurtherawayfromzerothanE[x¯ |x¯ = 1] = 1 is. Anegativefirst-period
2 2 6
averagepayofftendstoregresslesstothemeanofthedistributionthanapositive
first-periodaveragepayoff. Thereasonisthatasmallsampleistakeninthesec-
ondperiodiftheaverageinthefirstperiodisnegative. Thenegativefirst-period
averagewillbemoreweightedinthefinalaverage.
As this example shows, adaptive sampling implies that negative first-period
averages are more ’persistent’: they are given a larger weight than positive first-
periodaverages. Becausenegativeandpositivefirst-periodaveragesofthesame
magnitude are equally likely, when the payoff distribution is normal with mean
zero(thisdistributionissymmetricaroundzero),thegreaterpersistenceofnega-
tivefirst-periodaveragesexplainstheoverallnegativebias.
The impact of variance can also be understood in a similar way. If the payoff
distributionismorevariable,thefirstperiodaverageswilltendtodiffermorefrom
zero, both in the positive and negative directions. Positive first-period averages
will tend to regress more to the mean (zero) than negative averages. When the
negative first-period averages are more extreme because the variance is greater,
theresultisastrongerbias.
Ofcourse,thebiaswillbeeliminatedinthelongrun,asthenumberofsamples
increasessincetheaverageofnsamplesofarandomvariable X willconvergeto
itsexpectedvalue.Intheshortrun,however,thebiascanbesignificant.
4. GeneralTheoremaboutBiasedAverages
Thebiasgeneratedbyanadaptivesamplingsizepolicydoesnotonlyholdfor
thenormaldistributionandthespecificbinarysamplingpolicyconsideredabove
(highabovezero,lowbelow),butitholdsforanydistributionandalargeclassof
adaptivesamplingpolicies.
Theorem1:Inperiodone,alearnerobserveskpayoffs,x ,x ,...,x ,eachinde-
1,1 1,2 1,k
pendentlydrawnfromthepayoffdistribution f(x),withexpectedvalueE[x] = u.
Inperiodtwo,thelearnersamplesn(x¯ )payoffs,eachindependentlydrawnfrom
1
the payoff distribution f(x). Here, n(x¯ ) ≥ 1, and n(x¯ ) is a function of the av-
1 1
eragepayoffofthefirstperiod: x¯ = (1/k)∑k x . Let x¯ betheaveragepayoff
1 j=1 1,j 2
observed during periods one and two. Then i) E[x¯ ] < u whenever n(x¯ ) is a
2 1
strictly increasing function of x¯ , and ii) E[x¯ ] > u whenever n(x¯ ) is a strictly
1 2 1
decreasingfunctionofx¯ .
1
Proof:SeetheAppendix.
Forunimodalsymmetricdistributions,centeredontheexpectedvalueE[x] = u,
it can also be shown that a majority of the averages, after the second period, areADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 6
below u if higher averages in the first period lead to larger sample sizes in the
secondperiod:
Theorem2:Inperiodone,alearnerobserveskpayoffs,x ,x ,...,x ,eachinde-
1,1 1,2 1,k
pendentlydrawnfromthepayoffdistribution f(x)whichissymmetricaroundits
expectedvalueE[x] = u(i.e., f(x−u) = f(x+u).Inperiodtwo,thelearnersam-
ples n(x¯ ) payoffs, each independently drawn from the payoff distribution f(x).
1
Here, n(x¯ ) ≥ 1,and n(x¯ ) isafunctionoftheaveragepayoffofthefirstperiod:
1 1
x¯ = (1/k)∑k x .Letx¯ betheaveragepayoffobservedduringperiodsoneand
1 j=1 1,j 2
two.Theni)P[x¯ > u] <0.5whenevern(x¯ )isastrictlyincreasingfunctionofx¯ ,
2 1 1
andii)P[x¯ > u] >0.5whenevern(x¯ )isastrictlydecreasingfunctionofx¯ .
2 1 1
Proof:SeetheAppendix.
5. AlternativeLearningModels
Sofarwehaveassumedthatthelearnercomputestheaverageoftheobserved
payoffs, but a similar bias holds for several other learning models. Suppose, for
example,thatthelearnergivesmoreweighttothemostrecentlyobservedpayoff.
Thisresultsinasimilarbias:
Theorem3:Inperiodone,alearnerobserveskpayoffs,x ,x ,...,x ,eachinde-
1,1 1,2 1,k
pendentlydrawnfromthepayoffdistribution f(x),withexpectedvalueE[x] = u.
The learner updates his or her belief z after each observed payoff, giving more
weighttothemostrecentlyobservedpayoff.Specifically,ifthepriorbeliefwasz ,
t
thenewbeliefisz
t+1
= (1−b)z t+bx t,wherebistheweightofthemostrecently
observed payoff. The initial belief is assumed to be unbiased: z = u. In period
0
two,thelearnersamplesn(z )payoffs,eachindependentlydrawnfromthepay-
1,k
offdistribution f(x).Heren(z ) ≥1,andn(z )isafunctionofbeliefafterthek
1,k 1,k
observedpayoffsinthefirstperiod,z . Letz bethebeliefafterperiodsoneand
1,k 2
two. Theni)E[z ] < uwhenevern(z )isastrictlyincreasingfunctionofx¯ ,and
2 1,k 1
ii)E[z ] > uwhenevern(z )isastrictlydecreasingfunctionofx¯ .
2 1,k 1
Proof:SeetheAppendix.
6. Bayesianupdating
Onemightsuspectthatthebiasoccursbecausethelearnerignoresthesample
sizewhenupdating.Arationallearnershould,afterall,updatelesswhenthesam-
plesizeissmallandupdatemoreifthesamplesizeislarge,whichwouldseemto
workagainsttheabovebias.Thisleadstothequestionofwhetherthebiaspersists
ifthelearnerisaBayesianupdater,whotakesthesamplesizeintoaccountwhen
updating,andwhosebeliefistheconditionalexpectationgiventheobserveddata,
i.e. b = E[u|X]? Theansweristhatthereisnobiasonaverageinthefollowing
2
sense: Theexpectedvalueofthebeliefafterthetwoperiods,E[b ],isequaltothe
2
expectedvalueoftheprior.Forexample,ifBayesianlearnerslearnaboutthemean
ofarandomvariable,u,andthemeansaredrawnfromapriordistribution, f(u ),
i iADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 7
withexpectedvalueE[u ] = m,thenE[b ] = m.Averagingoverdifferentlearners,
i 2
whodrawdifferentvaluesofu,thusgeneratesnobias.However,thedistribution
i
of beliefs may still be “biased” (or skewed) in the following sense: a majority of
Bayesianupdaterswillhaveabeliefaftertwoperiodsbelowm.
To explain this in more detail, consider a learner who observes independent
draws from a random variable with distribution f (X) with expected value u.
i i
Thelearnerknowsthatu isdrawnfromapriordistributionwithexpectedvalue
i
m (E[u ] = m). Let b be the belief after having observed k independent draws
i 1
x ,x ,...,x from f (X)inthefirstperiod,b = E[u |x ,x ,...,x ]. Letb be
1,1 1,2 1,k i 1 i 1,1 1,2 1,k 2
thebeliefafterallobservationsinperiodsoneandtwo: b = E[u |x ,...,x ].
2 i 1,1 2,n(b1)
It is important to distinguish between two ways that the learner’s belief can
be ’biased’. Consider first a given value of u = k. We may ask if the expected
i
beliefisunbiasedinthesensethat E[b |u = k] = k,whereb isthebeliefaftera
n i n
fixedsampleofnobservations(fixedinthesensethatthenumberofobservations
is independent of the value of the observations). The average is unbiased in this
sense, butitis wellknownthatBayesianupdatingis not. Suppose, forexample,
thatu isdrawnfromanormaldistributionwithmeanzeroandvarianceone.The
i
learner observes x = u +ε where ε is drawn from a normal distribution with
i i i i
mean zero and variance one. For this setup, it is well known that the expected
valueoftheposteriorafteroneobservationisx /2(e.g.,DeGroot,1970). Suppose
i
now u = 5. The expected value of the belief after one observation, given that
i
u = 5, equals E[x /2|u = 5] = 2.5 which is lower than 5. Bayesian updating
i 1 i
canbebiasedinthesensethattheexpectedbelief,givenasetofobservationsand
u = k,isnotequaltok. Thereasonisthatthebeliefgraduallyincreasesfromthe
i
priorvalue(zero)towardstheexpectedvalue(five).
Bayesianupdatingisunbiased,however,ifweaverageoveralllearners,inthe
following sense. Let b denote the belief of a Bayesian learner, i.e., b = E[u |X],
i i i
where X is the observed data. Then E[b] = m where m = E[u ] is the expected
i i
valueoftheprior. Forexample,supposethatu isdrawnfromanormaldistribu-
i
tionwithmeanzeroandvarianceoneandthelearnerobservesx = u +ε ,where
i i i
ε isdrawnfromanormaldistributionwithmeanzeroandvarianceone.Wethen
i
haveE[b] = E[x /2] = E[(u +ε )/2] =0.Thus,E[b] = E[u ] =0.
i i i i i i
WenowshowthatBayesianupdatingremainsunbiasedinthislattersense,even
ifthesamplesizeinperiodtwodependsonthebeliefafterperiodone.Thisistrue
foranydistribution:
Theorem4:Inperiodonethelearnerobserveskindependentdrawsx ,x ,...,x ,
1,1 1,2 1,k
fromdistribution f (X)withexpectedvalueu.Thelearnerknowsthatu isdrawn
i i i
fromapriordistribution, f(u ),withexpectedvalueE[u ] = m.Inperiodtwo,the
i i
learnerobservesn(b )independentdrawsfromdistribution f (X):x ,x ,...,x .
1 i 2,1 2,2 2,n(b1)
Thesamplesizeinthesecondperiod,n(b ),isafunctionofthebeliefafterthefirst
1
period, b = E[u |x ,...,x ]. Let b be the belief after all observations, in both
1 i 1,1 1,k 2
periodsoneandtwo: b = E[u |x ,...,x ].ThenE[b ] = m.
2 i 1,1 2,n(b1) 2ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 8
ProofofTheorem4: Theproofissimple. Weconditiononthebeliefafterthefirst
period: E[b |b ]. Becauseconditionalexpectationsaremartingales(e.g.,Williams
2 1
(1991), p. 96), we have E[b |b ] = b . That is, there is no change, in expectation,
2 1 1
from the belief after period one to the belief after the information in the second
period. This is true since if information that would lead to such a change could
be anticipated in period one, it should already have been incorporated into the
belief,ofarationalagentinperiodone. FromE[b |b ] = b itfollowsthatE[b ] =
2 1 1 2
E [E[b |b ]] = E[b ]. Bythetowerpropertyofmartingales(e.g.,Williams(1991),
b1 2 1 1
p.88)wehaveE[b ] = E[E[u |x ,x ,...,x ]] = E[u ] = m.Thus,E[b ] = m.
1 i 1,1 1,2 1,k i 2
While there is no bias when averaging the beliefs of all learners (i.e. E[b ] =
2
m), most Bayesian learners may have beliefs below m if positive beliefs lead to
largersamplesizes. Toillustratethis,supposethat f (X)isanormaldistribution
i
withmeanu andvarianceσ2. Moreover,supposethatu isdrawnfromanormal
i e i
distribution with mean m and variance σ2. The learner observes k independent
u
draws x ,x ,...,x from f (X) in the first period. In period two, the learner
1,1 1,2 1,k i
observesn(b )independentdrawsfromdistribution f (X): x ,x ,...,x .
1 i 2,1 2,2 2,n(b1)
Theorem5:Ifthepayoffandthepriordistributionsarebothnormal,theni)when-
evern(b )isastrictlyincreasingfunctionofb ,mostlearnerswill,afterthesecond
1 1
period,underestimatetherandomvariabletheyarelearningabout: Pr(b < m) >
2
0.5. ii) Whenever n(b ) is a strictly decreasing function of b , most learners will,
1 1
afterthesecondperiod,overestimatetherandomvariabletheyarelearningabout:
Pr(b > m) >0.5.
2
Proof:SeetheAppendix.
Thus,evenifBayesianlearnersarelearningaboutasymmetricdistributionand
theyhaveapriorthatissymmetricaroundzero(m = 0),mostlearnerswillhave
anegativebeliefaftersampling. Thisisduetotheadaptivesamplingpolicy. The
intuitionissimilartowhythelearningpolicythattakestheaverageisbiased:neg-
ativeinitialbeliefsaremorepersistentbecausethelearnerwillthennottakemany
additional samples, and the initial few negative observations will be weighted
heavily.Notethatsuchabiaswouldnotoccurifthelearnerfollowedafixedsam-
pling policy and decided, at the outset, how many samples to take. If the sam-
ple size was fixed, the belief would be equally likely to be positive or negative,
when learning about a normally distributed payoff with a mean taken from nor-
mallydistributedpriorwithmeanzero.Notealsothatwhensamplingisadaptive,
all Bayesian learners know that only 50% of the means are negative. Still, most
Bayesianlearnersbelievethatthemeantheyobserveisnegative.
To illustrate this, suppose that payoffs are normally distributed with mean m
and variance σ2 where m is drawn from a normal prior with mean u = 0 and
e
variance σ2 = 1. Consider a learner who samples two payoffs in the first pe-
p
riod,samplestenmoreifthebeliefafterthefirstperiodispositive,butonlysam-
ples one more if the belief after the first period is negative. When σ2 = 1, the
eADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 9
0.06
0.05
0.04
0.03
0.02
0.01
0
-2 -1 0 1 2 3
Figure1. ThedistributionofbeliefsofaBayesianlearnerafterthe
secondperiodwhenσ2 =5.
e
proportion of beliefs after the second period that are negative, P(b < 0) where
2
b = E[u |x ,...,x ],is0.534.Whenσ2 =5,theproportionofbeliefsafterthe
2 i 1,1 2,n(b1) e
secondperiodthatarenegativeis0.577(basedon10millionsimulations).Figure1
showsthedistributionofbeliefsafterthesecondperiodforthecasewhereσ2 =5.
e
That a majority of Bayesian learners end up with a negative belief may seem
paradoxical since on average there is no bias: E[b ] = 0 when m = 0, i.e., the
2
average belief, averaging over all Bayesian learners, is equal to the mean of the
prior. The paradox is resolved by noting that the learners who believe that the
meanisnegativearelessconfidentinthisestimatethanthelearnerswhobelieve
thatthemeanispositiveandthereforetookalargersampleinthesecondperiod.
Thisresultsinaskeweddistributionofbeliefsafterthesecondperiod.Mostbeliefs
arebelowm = 0butthosebelowm = 0arelesslikelytobeextremebeliefsthan
beliefsabovem = 0sincebeliefsbelowm = 0arebasedonsmallersamplesizes
thanthoseabovem =0.ThiscanbeseeninFigure1.
BecauseBayesianupdatingwithnormaldistributionsreliesontheobservedav-
erage,itmayalsoseempuzzlingthatBayesianupdatingisunbiasedinthesense
that E[b ] = m (Theorem 3) while averaging is biased in the sense that E[x¯|u =
2 i
k] < k for all k (Theorem 1). Consider again the case where u is drawn from
i
a normal distribution with mean m and variance σ2. In period one, the learner
uADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 10
observes k independent draws x ,x ,...,x , x = u +ε where ε are inde-
1,1 1,2 1,k 1,j i j j
pendentlydrawnfromanormaldistributionwithmeanzeroandvarianceσ2. In
e
period two, the learner observes n(b ) independent draws from the same distri-
1
bution: x ,x ,...,x ,wheren(b )isafunctionofb = E[u |x ,...,x ].The
2,1 2,2 2,n(b1) 1 1 i 1,1 1,k
Bayesian estimate of u given the observations in both periods equals (DeGroot,
i
1970): E[u |X] = (x¯σ2+(σ2/n )m)/(σ2+(σ2/n )),wheren = k+n(b ),theto-
i u e 2 u e 2 2 1
talnumberofobservationsinbothperiods,andx¯istheaverageofallobservations
inbothperiods. Asσ2 → ∞ E[u |X] → x¯,thatis,convergestotheaverage. How
u i
canthentheBayesianestimatebeunbiasedwhiletheaverageisbiased?Thepuzzle
isresolvedbynotingthatwhenσ2growslargecomparedtoσ2,thesignal-to-noise
u e
ratio(σ2/σ2)becomeslarge. Whenσ2 → ∞thesignal-to-noiseratiogoestoinfin-
u e u
ity.Thenoisetermisthenvanishinglysmallcomparedtothevaluesofu.Because
i
thebiasintheaveragedependsonthepossibilitythatthefirst-periodobservations
canfarbelow(orabove)u,i.e.,dependonnoise,thereisonlyavanishinglysmall
i
biasintheaveragewhenσ2 → ∞,resolvingtheseemingparadox.
u
7. ImplicationsforUnderstandingLearning
Thebiasresultingfromadaptivesamplingpoliciesimpliesthatevenseemingly
unbiasedlearningalgorithms,suchasaveragingorBayesianupdating,canresult
in biased beliefs, at least in the sense that most learners underestimate (or over-
estimate)analternative. Thisoffersanalternativeexplanationofsomejudgment
biases;anexplanationthatdoesnotrequireapsychologicalbiasthatassumesbi-
ases in information processing. For example, suppose that it is observed that a
firmtendstounderestimatetheproductivityofgraduatesfromuniversities.Such
anegativitybias.canbeexplainedbyanadaptivesamplingpolicy(thefirmtends
tohirefewerpeoplefromuniversitiesithashadaworseexperiencewith)anddoes
notrequiremotivatedreasoningoracognitivebias.ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 11
Appendix
Derivationofequation1:
E[b ] = P(x¯ > c)E[b |x¯ > c]+P(x¯ < c)E[b |x¯ < c].
2 1 2 1 1 2 1
Wehave
S x¯ k
E[b |x¯ > c] = E[ 2 |x¯ > c]+E[ 1 |x¯ > c].
2 1 k+h 1 k+h 1
whereS isthesumofobservationsinperiodtwo.Duetoindependence,E[ S2 |x¯ >
2 k+h 1
c] = 1 E[S ] =0.Usingtheexpressionfortheexpectationofatruncatednormal
k+h 2
distribution(see,e.g.,Greene2000,p.952)wehave
σ ϕ(α)
E[x¯ |x¯ > c] = √e ,
1 1 k1−Φ(α)
√ √
whereα = c/(σ / k) = −c k/σ .Moreover,P(x¯ > c) =1−Φ(α).Hence,
e e 1
k σ ϕ(α)
P(x¯ > c)E[b |x¯ > c] = (1−Φ(α)) (√e )
1 2 1 k+h k1−Φ(α)
√
σ k
= e ϕ(α).
k+h
Similarly,
k σ ϕ(α)
P(x¯ < c)E[b |x¯ < c] = Φ(α) (−√e )
1 2 1 k+l kΦ(α)
√
σ k
= − e ϕ(α).
k+l
Overallweget
√
σ k
E[b ] = e [((k+l)ϕ(α)−(k+h)ϕ(α)]
2 (k+l)(k+h)
√
σ k
= − e [(h−l)ϕ(α)].
(k+l)(k+h)
Now,ϕ(α) = √1 e−α2/2.Hence,
2π
√
E[b 2] = −√ σ e k(h−l) e−c2k/2σe2 .
2π(k+l)(k+h)
ProofofTheorem1: Theaverageofallpayoffsobservedinperiodsoneandtwo
is,giventheaverageobservedinperiodone(x¯ ),is
1
kx¯
+∑n(x¯1)
x
1 j=1 2,j
x¯ = ,
2 k+n(x¯ )
1
wherekx¯ isthesumofallobservedpayoffsinthefirstperiod.Thus,
1
kx¯
+E[∑n(x¯1)
x |x¯ ]
1 j=1 2,j 1
E[x¯ |x¯ ] = .
2 1 k+n(x¯ )
1ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 12
BecauseE[∑n(x¯1) x |x¯ ] = un(x¯ ),thiscanbewrittenas
j=1 2,j 1 1
kx¯ n(x¯ )u
E[x¯ |x¯ ] = 1 + 1 .
2 1 k+n(x¯ ) k+n(x¯ )
1 1
Moreover,becauseE(x¯ ) = E (E[x¯ |x¯ ])weget
2 x¯1 2 1
kx¯ n(x¯ )u
E(x¯ ) = E ( 1 )+E ( 1 )
2 x¯1 k+n(x¯ ) x¯1 k+n(x¯ )
1 1
Let g(x¯ ) = 1 . Because E(x¯ g(x¯ )) = E(x¯ )E(g(x¯ ))+cov(x¯ ,g(x¯ )) and
1 k+n(x¯1) 1 1 1 1 1 1
E(x¯ ) = kuweget
1
kx¯
E ( 1 ) = kuE(g(x¯ ))+Cov(x¯ ,g(x¯ )).
x¯1 k+n(x¯ ) 1 1 1
1
Notethat
k n(x¯ )
kE(g(x¯ )) = E( ) =1−E( 1 )
1 k+n(x¯ ) k+n(x¯ )
1 1
Overallweget
n(x¯ )
E(x¯ ) = u[1−E( 1 ]
2 k+n(x¯ )
1
n(x¯ ))
+Cov(x¯ ,g(x¯ ))+uE( 1 )
1 1 k+n(x¯ )
1
= u+Cov(x¯ ,g(x¯ )).
1 1
Cov(x¯ ,g(x¯ )) is negative (positive) whenever g(x¯ ) is a strictly decreasing (in-
1 1 1
creasing) function of x¯ . Because g(x¯ ) = 1/(k+n(x¯ )), which is a strictly de-
1 1 1
creasingfunctionof n(x¯ ), Cov(x¯ ,g(x¯ )) isnegativewhenever n(x¯ ) isastrictly
1 1 1 1
increasingfunctionofx¯ andCov(x¯ ,g(x¯ ))ispositivewhenevern(x¯ )isastrictly
1 1 1 1
decreasingfunctionofx¯ .
1
ProofofTheorem2:
Withoutlossgeneralityweonlyconsidercaseswhenu =0.Aunimodalrandom
variable symmetric around the mode k can always be represented as x = k+ε
whereεhasaunimodaldistributionsymmetricaroundzero.
WefirstprovethefollowingLemma
Lemma 1: Let x, i = 1,...,n be independent random variables with a unimodal
i
distributionsymmetricaroundthemode0.Supposer >0.ThenP(∑k x >r) >
i=1 i
P(∑v x > r)wheneverk > v. Thatis,asumwithmoretermsismorelikelyto
i=1 i
beextremethanasumwithfewerterms.
ProofofLemma1:Considerfirstthecasewhenk =2andv =1.Lets = x +x .
2 1 2
Weneedtoshowthat P(s > r) > P(x > r), i.e., thatadding x to x increases
2 1 2 1
thechancethatthesumisabover.Supposefirstthatx =r+awherea >0.Then
1
s >rifx >r−x = −a.ThisoccurswithprobabilityP(x > −a) =1−F(−a).
2 2 1 2
Moreover, 1−F(−a) = F(a) due to symmetry around zero. Suppose next x =
1
r−a where a > 0. Then s > r if x > r−x = a. Thisoccurswithprobability
2 2 1ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 13
density
0 r-a r r+a
Figure2. IllustrationofProof
1−F(a).Altogether,P(s >r)equals
2
(cid:90) ∞
P(s >r) = [f(r+a)F(a)+ f(r−a)(1−F(a))]da.
2
a=0
Now, f(r+a)F(a)+ f(r−a)(1−F(a)) > f(r+a)F(a)+ f(r+a)(1−F(a))when
a > 0 because f(r−a) > f(r+a) when a > 0, r > u, and f(·) has a unimodal
distributioncenteredonzero.ThisisillustratedinFigure1.Next,notethat
f(r+a)F(a)+ f(r+a)(1−F(a)) = f(r+a).
Moreover,(cid:82)∞ f(r+a)da = P(x >r).Thus,P(s >r) > P(x >r).
a=0 1 2 1
The proof now follows by induction, because the sum of k independent ran-
domvariableswithaunimodaldistributionsymmetricarounduisalsounimodal
symmetriccenteredonu(Shaked&Shantikumar(2007),page173).
Let now r denote the the sum of the observed payoffs in the first period: r =
∑k x .Becausethesumofkindependentrandomvariableswithaunimodaldis-
j=1 1,j
tributionsymmetricarounduisalsounimodalsymmetriccenteredonu(Shaked
andShantikumar,1996,p.173),rhasaunimodalsymmetricdistributioncentered
onzero.
Considerafixedvalueofr.Supposer >0.Theaverageafterthesecondperiod
becomeslessthanzerowhenever
r+∑n(r)
x
j=1 2,j
x¯ = <0,
2 k+n(r)
i.e.,when∑n(r) x < −r. Heren(r)isthesamplesize,foragivenlevelofr. This
j=1 2,j
occurswithprobabilityP(∑n(r) x < −r). Becausethesumisalsosymmetrically
j=1 2,j
distributedaroundzero,thisequalsP(∑n(r) x >r).
j=1 2,j
Suppose next r < 0. The average after the second period remains below zero
whenever
r+∑n(r)
x
j=1 2,j
x¯ = <0,
2 k+n(r)ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 14
i.e.,when∑n(r) x < −r.ThisoccurswithprobabilityP(∑n(r) x < −r).
j=1 2,j j=1 2,j
Theprobabilitythattheaverageisbelowzeroafterthesecondperiodisthus
(cid:90) ∞ n(r) (cid:90) 0 n(r)
∑ ∑
0.5 f (r|r >0)P( x >r)dr+0.5 f (r|r <0)P( x < −r)dr.
r 2,j r 2,j
r=0 j=1 r=−∞ j=1
Now, f (r|r > 0) = f (r)/P(r > 0) = f (r)/0.5. Similarly, f (r|r < 0) =
r r r r
f (r)/P(r < 0) = f (r)/0.5. The probability that the average is below zero after
r r
thesecondperiodcanthusbewrittenas
(cid:90) ∞ n(r) (cid:90) 0 n(r)
∑ ∑
f (r)P( x >r)dr+ f (r)P( x < −r)dr.
r 2,j r 2,j
r=0 j=1 r=−∞ j=1
Byachangeofvariabless = −r,andnotingthat f (−r) = f (r)foradistribution
r r
symmetricaroundzero,thelasttermcanbewrittenas
(cid:90) 0 n(r) (cid:90) ∞ n(−s)
∑ ∑
f (r)P( x < −r)dr = f (s)P( x < s)ds
r 2,j r 2,j
r=−∞ j=1 s=0 j=1
Altogether,theprobabilitythattheaverageisbelowzeroafterthesecondperiod
equals
(cid:90) ∞ n(r) (cid:90) ∞ n(−r)
∑ ∑
f (r)P( x >r)dr+ f (r)P( x <r)dr.
r 2,j r 2,j
r=0 j=1 r=0 j=1
Moreover, P(∑n(−r) x < r) = 1−P(∑n(−r) x > r). The probability that the
j=1 2,j j=1 2,j
averageisbelowzeroafterthesecondperiodthusequals
(cid:90) ∞ (cid:90) ∞ n(r) (cid:90) ∞ n(−r)
∑ ∑
f (r)dr+ f (r)P( x >r)dr− f (r)P( x >r)dr.
r r 2,j r 2,j
r=0 r=0 j=1 r=0 j=1
Now, P(∑n(r) x > r) is larger (smaller) than P(∑n(−r) x > r) for all r > 0
j=1 2,j j=1 2,j
when n(x) is an increasing (decreasing) function of x. Thus, the probability that
theaverageisbelowzeroafterthesecondperiodislarger(smaller)than
(cid:90) ∞
f (r)dr =0.5.
r
r=0
ProofofTheorem3: Thebeliefafterallpayoffsobservedinbothperiodsoneand
two,z ,giventhebeliefattheendofperiodone(z ),isequalto
2,n(z 1,k) 1,k
z
1,k(1−b)n(z 1,k)+x 2,1b(1−b)n(z 1,k)−1+...+bx
2,n(z
1,k).
TheconditionalexpectedvalueE[z |z ]equals
2,n(z 1,k) 1,k
z 1,k(1−b)n(z 1,k)+E[x 2,1b(1−b)n(z 1,k)−1+...+bx
2,n(z
1,k)|z 1,k].
BecauseE[x |z ] = u,thiscanbewrittenas
2,j 1,k
E[z
2,n(z
1,k)|z 1,k] = z 1,k(1−b)n(z 1,k)+(1−(1−b)n(z 1,k))u.ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 15
Moreover,becauseE(Y) = E (E[Y|X]),E[z ]equals
X 2,n(z 1,k)
E
z
1,k[z 1,k(1−b)n(z 1,k)]+E
z
1,k[(1−(1−b)n(z 1,k))]u.
Letg(z 1,k) = (1−b)n(z 1,k).BecauseE(z 1,kg(z 1,k)) = E(z 1,k)E(g(z 1,k))+Cov(z 1,k,g(z 1,k)),
andE(z ) = u,weget
1,k
E[z ] = uE(g(z ))+Cov(z ,g(z ))+u(1−E(g(z ))
2,n(z 1,k) 1,k 1,k 1,k 1,k
= u+Cov(z ,g(z )).
1,k 1,k
Cov(z ,g(z ))isnegative(positive)wheneverg(z )isastrictlydecreasing(in-
1,k 1,k 1,k
creasing)functionofx¯ 1.Becauseg(z 1,k) = (1−b)n(z 1,k),whichisastrictlydecreas-
ing function of n(z ), Cov(z ,g(z )) is negative whenever n(z ) is a strictly
1,k 1,k 1,k 1,k
increasing function of z and Cov(z ,g(z )) is positive whenever n(z ) is a
1,k 1,k 1,k 1,k
strictlydecreasingfunctionofz .
1,k
ProofofTheorem5:Withoutlossofgeneralitywefocusonthecasewhenm =0.
Ifmdiffersfromzero,thedistributionofpayoffsisidenticaltoaconstant,m,plus
arandomvariablewithanormaldistributionwithmeanzero. LetS bethesum
1
of the observed payoffs in period one. This sum is equally likely to be positive
or negative. Suppose S = z > 0. It follows that the belief after the first period,
1
E[u|S ] = (1/k)S σ2/(σ2+σ2/k), is positive. Denote this belief by b , i.e., b =
1 1 u u e 1 1
E[u|S ].Notethatb isastrictlyincreasingfunctionofS .Thispositivebeliefturns
1 1 1
intoanegativebeliefafterperiod2whenever
1 (S +S )σ2
b =
k+n(b1) 1 2 u
<0,
2
σ2+
σe2
u k+n(b1)
i.e.,wheneverS < −S . Weseektheprobabilityofthiseventgiventheobserved
2 1
valueofS = z,i.e.,weseekPr(S < −S |S = z).
1 2 1 1
ConditionalonS = z,thesumofthepayoffsinthesecondperiod,S ,isanor-
1 2
mallydistributedrandomvariablewithmeann(z)E(u|S = z) = n(z)zσ2/(σ2+
1 u u
σ2/k)andvariancen(z)2Var(u|S = z)+σ2n(z),whereVar(u |x = z) = σ2σ2/(kσ2+
e 1 e i 1 u e u
σ2)(seeLindgren(1993,p.289).Itfollowsthat
e
−z−n(z)(z/k)σ2/(σ2+σ2/k)
Pr(S
2
< −S 1|S
1
= z) = Φ(
(cid:112)
u u e ).
n(z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
Altogether,theprobabilitythatapositivebeliefafterperiodoneturnsintoaneg-
ativebeliefafterperiodtwoequals
Pr(+ → −) =
(cid:90) z=+∞ −z−n(z)(z/k)σ2/(σ2+σ2/k)
Φ(
(cid:112)
u u e )f(z|z >0)dz.
z=0 n(z)2σ u2σ e2/(kσ u2+σ e2)+σ e2n(z)
Because f(z|z >0) = f(z)/P(z >0) = f(z)/0.5,Pr(+ → −)equals
(cid:90) z=+∞ −z−n(z)(z/k)σ2/(σ2+σ2/k)
(2) Φ(
(cid:112)
u u e )2f(z)dz,
z=0 n(z)2σ u2σ e2/(kσ u2+σ e2)+σ e2n(z)ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 16
where f(z) is the density of the sum of the payoffs in the first period which is a
normaldistributionwithmeanzero.
SupposenextS = z < 0implyingthatE[u|S ] < 0. Thisnegativebeliefturns
1 1
intoapositivebeliefafterperiod2wheneverS > −z.Followingthesamereason-
2
ingasabovePr(S > −S |S = z)equals
2 1 1
−z−n(z)(z/k)σ2/(σ2+σ2/k)
1−Φ(
(cid:112)
u u e ).
n(z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
Because1−Φ(y) = Φ(−y),thiscanbewrittenas
z+n(z)(z/k)σ2/(σ2+σ2/k)
Φ(
(cid:112)
u u e ).
n(z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
Altogether,theprobabilitythatanegativebeliefafterperiodoneturnsintoapos-
itivebeliefafterperiodtwo,Pr(− → +),equals
(cid:90) z=0 z+n(z)(z/k)σ2/(σ2+σ2/k)
Φ(
(cid:112)
u u e )2f(z)dz.
z=−∞ n(z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
Afterthevariablesubstitution,z = −y,thisintegralequals
(cid:90) y=∞ −y−n(−y)(y/k)σ2/(σ2+σ2/k)
Φ(
(cid:112)
u u e )2f(y)dy.
y=0 n(−y)2σ u2σ e2/(kσ u2+σ e2)+σ e2n(−y)
wherewehaveusedthefactthat f(−y) = f(y).
Wewishtoshowthat Pr(+ → −) > Pr(− → +), whichrequiresustoshow
that
−z−n(z)(z/k)σ2/(σ2+σ2/k)
Φ( u u e ) >
(cid:112)
n(z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
−z−n(−z)(z/k)σ2/(σ2+σ2/k)
Φ(
(cid:112)
u u e ),
n(−z)2σ2σ2/(kσ2+σ2)+σ2n(z)
u e u e e
overthepositivedomainofz.Todoso,notefirstthatthederivativeof
−z−wzσ2/(σ2+σ2/k)
g(z,w) = (cid:112) u u e ,
w2σ2σ2/(kσ2+σ2)+σ2w
u e u e e
withrespecttowis
δg(z,w) z
= (cid:114) ,
δw
2w
σe2w(σe2+σu2+σe2w)
σu2+σe2
whichispositivewheneverz >0. Henceg(z,w)isanincreasingfunctionofn(z).
Because Φ(x) is a increasing function of x and n(z) is an increasing function of
z, implying that n (z) > n (−z), it follows that Φ(g(z,n(z)) > Φ(g(z,n(−z))
2 2
implyingPr(+ → −) > Pr(− → +).
References
Denrell, J. (2005). Why most people disapprove of me: Experience sampling in
impressionformation. PsychologicalReview,112,951–978.ADAPTIVESAMPLINGPOLICIESIMPLYBIASEDBELIEFS 17
Denrell, J. (2007). Adaptive learning and risk taking. Psychological Review, 114,
177–187.
Denrell,J.,&March,J.(2001).Adaptationasinformationrestriction:Thehotstove
effect. PsychologicalReview,12,523–538.
Dittmar, A., & Duchin, R. (2016). Looking in the rear-view mirror: The effect
of managers’ professional experiences on corporate financial policy. Review of
FinancialStudies,29,565–602.
Erev,I.,&Roth,A. (2014). Maximization,learning,andeconomicbehavior. Pro-
ceedingsoftheNationalAcademyofSciences,111,10818–10825.
Fetchenhauer,D.,&Dunning,D. (2014). Whysocynical?: Asymmetricfeedback
underliesmisguidedskepticismregardingthetrustworthinessofothers.Psycho-
logicalScience,21,189–193.
Mens,G.L.,Kovács,B.,Avrahami,J.,&Kareev,Y.(2018).Howendogenouscrowd
formationunderminesthewisdomofthecrowdinonlineratings. Psychological
Science,29,1475–1490.
Shaked,M.,&Shantikumar,J.G.(2007).Stochasticorders.NewYork,NY:Springer.
Williams,D. (1991). Probabilitywithmartingales. Cambridge,UK:CambridgeUni-
versityPress.