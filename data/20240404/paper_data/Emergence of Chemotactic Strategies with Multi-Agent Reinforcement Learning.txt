EMERGENCE OF CHEMOTACTIC STRATEGIES WITH
MULTI-AGENT REINFORCEMENT LEARNING
SamuelTovey†,ChristophLohrmann†,ChristianHolm
InstituteforComputationalPhysics
UniversityofStuttgart
70569,Stuttgart,Germany
{stovey, clohrmann, holm}@icp.uni-stuttgart.de
ABSTRACT
Reinforcementlearning(RL)isaflexibleandefficientmethodforprogrammingmicro-robotsin
complexenvironments. Hereweinvestigatewhetherreinforcementlearningcanprovideinsightsinto
biologicalsystemswhentrainedtoperformchemotaxis. Namely,whetherwecanlearnabouthow
intelligentagentsprocessgiveninformationinordertoswimtowardsatarget. Werunsimulations
coveringarangeofagentshapes,sizes,andswimspeedstodetermineifthephysicalconstraints
onbiologicalswimmers,namelyBrownianmotion,leadtoregionswherereinforcementlearners’
trainingfails. WefindthattheRLagentscanperformchemotaxisassoonasitisphysicallypossible
and,insomecases,evenbeforetheactiveswimmingoverpowersthestochasticenvironment. We
studytheefficiencyoftheemergentpolicyandidentifyconvergenceinagentsizeandswimspeeds.
Finally,westudythestrategyadoptedbythereinforcementlearningalgorithmtoexplainhowthe
agentsperformtheirtasks. Tothisend,weidentifythreeemergingdominantstrategiesandseveral
rareapproachestaken. Thesestrategies,whilstproducingalmostidenticaltrajectoriesinsimulation,
aredistinctandgiveinsightintothepossiblemechanismsbehindwhichbiologicalagentsexplore
theirenvironmentandrespondtochangingconditions.
Keywords Reinforcementlearning,microrobotics,chemotaxis,activematter,biophysics
1 Introduction
Microswimmershavetheuniqueprivilegeofhavingevolvedovermillionsofyearstolearnhowtooptimallynavigate
noisy,Brownianmotion-dominatedenvironmentsinsearchofbetterlivingconditions. Mostinteractionshumansare
familiarwithoccuronlengthandtimescalesthatarenotsubjecttothisnoise. Therefore,wedonaturallyhavean
understandingofhowthesemicroswimmerscanperformthisnavigation. However,understandingtheemergenceofthis
behaviouriscriticalasscientistsstrivetoconstructtheartificialcounterpartsofbiologicalmicroswimmers. Previous
reviewshavediscussedtheemergenceandfunctionofbiologicalmicroswimmersingreatdetail[Bastos-Arrietaetal.,
2018,Elgetietal.,2015],elucidatingthemechanismsandstrategiesbehindtheirmovement. Onerecurringformof
navigationinmicroswimmersistheso-calledrun-and-tumblemotionexhibitedbyEscherichiacoli(E-coli)wherein
thebacteriawilltravelinastraightlineforsomeextendedperiodbeforespontaneouslyrotatingintoarandomnew
direction[WatariandLarson,2010,Berg,2004,Darntonetal.,2006]. Oneapplicationofthisnavigationmechanismis
bacterialchemotaxis[Hansenetal.,2007],thebiasedmovementofabacteriatowardsregionswithhigherconcentrations
ofbeneficialchemicalsorlowerconcentrationsofharmfulchemicals[WadhamsandArmitage,2004]. Bacteriaachieve
thisbiasedmotionthroughchangesinrundurationdependingonchangesintheconcentrationofthechemo-attractant
or-repellant. Learningchemotaxishasbeenthefocalpointofseveralresearchpapersaimedatreproducingorbetter
understandingbiologicalmicroswimmersthroughtheuseofreinforcementlearning(RL)[Toveyetal.,2023b,Moand
Bian,2022,Hartletal.,2021,Muin˜os-Landinetal.,2021]. Intheir2021study,Hartletal.[2021]appliedagenetic
algorithmtotheproblemoflearningshapedeformationsfornavigationinstaticanddynamicenvironments. They
†Theseauthorscontributedequally
4202
rpA
2
]hp-oib.scisyhp[
1v99910.4042:viXraEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
foundthattheneuralnetworkslearnedamovementcloselyresemblingthatofrun-and-tumblemotion. Inanother
2021study,Muin˜os-Landinetal.[2021]appliedQ-learningtolearningnavigationstrategiesinself-thermophoretic
particlesfromwhichtheyagainseetheemergenceofrun-and-tumblemotion. Theyfurtherinvestigatedtheeffects
oftemperatureonthelearningprocess,identifyingthatmodelstrainedathighertemperaturestooklongertolearntheir
emergentstrategy. Finally,ourpreviouswork[Toveyetal.,2023b]directlyaddressedtheroleoftemperatureinthe
emergentstrategyofRL-drivenmicroswimmersbystudyingchemotaxislearningbytheactor-criticreinforcement
learningalgorithm. Itwasfoundthat,whiletheefficacyofthechemotaxischangedwithdifferenttemperatures,the
samerun-and-tumblemotionarosefromthemajorityofagentstrainedatdifferenttemperatures. Whileitisclear
thatRLalgorithmscanand,infact,seeminglyoftendolearnrun-and-tumbletypemotionforchemotaxisproblems,
whatimpactthishasonourunderstandingofbiologicalmicroswimmersandevenoptimaldesignofartificialswimmers
isnotclear. Thisstudyinvestigatesnaturallimitationsonemergentchemotaxisbytrainingactor-criticRLmodels
usingprolate,oblate,andsphericalagentsofdifferentsizesandwithdifferentswimspeedsinphysicallyrealisticfluid
environmentssubjecttotranslationalandrotationalBrownianmotion. Inthisway,wehopetoidentifyhowoptimalRL
algorithmsareforthelearningtaskandtoidentify,ifanyexist,optimalsize/speedcombinationsofmicroswimmersin
theseenvironments,whichmayguideourinterpretationofbiologicalmicroswimmersaswellasadvisethedesignof
artificialones. Furthermore,byinvestigatingthedeploymentoftheRLalgorithmsclosetoconditionswhereagentswill
bedominatedbyrotationalandtranslationalBrownianmotion,wecanexploretheemergenceofdifferentnavigation
strategiesthatmaybeleveragedinthetreatmentofbiologicalorartificialswimmers,essentiallypeeringintotheminds
ofbacteriaastheynavigateenvironments.
Themanuscriptisstructuredasfollows. Wewillfirstdiscussthetheorybehindtheinvestigation,explainingthemecha-
nismandphysicallimitationsofchemotaxisinbiologicalsystems. Wethenintroducedeepactor-criticreinforcement
learninganddiscussitsmulti-agentrealization. Thesimulationandtrainingmethodsarediscussedindetailbeforeour
resultsarepresented,andabriefoutlookispresented.
2 Theory
2.1 BiologicalChemotaxis
Asmentionedbrieflyintheintroduction,chemotaxisisthebiasedmovementofbacteriatowardsfavourableregions
intheirenvironment[WadhamsandArmitage,2004]. Escherichiacoli(e. coli)performchemotaxisactivelyusing
run-and-tumblemotion. Intheirrunningphase,theybundletheirflagellatogetherandrotatethemanti-clockwise,
duringatumblephase, oneormoreflagellachangetheirrotationdirection, breakingapartthebundleandcausing
randomrotationofthebacteriabeforethebundlereformsandtranslationalswimmingisresumed[Turneretal.,2000].
Utilizingasensingmechanism,thesebacteriacanidentifyiftheyaremovingtowardsorawayfromfavourableregions
oftheirenvironment,adjustingtheirtumbleratesaccordinglytomaintaindesiredmovement[Schnitzeretal.,1990].
Inordertocapturetheessentialfeaturesofbacterialmotility,weconsiderhereactiveparticlesthatcanperformfour
distincttypesofmovement. Firstly,theycanmoveforwardalongtheirintrinsicdirectionlikebacteriaintherunphase.
Secondly,theycanactivelyrotateclockwiseorcounterclockwise,likebacteriainthetumblephase. Ourswimmerscan
choosethedirectionofrotationwhilebacteriarotatetowardsarandomneworientation. Thirdly,theswimmerscanopt
todonothing,thatis,onlymovepassivelybyBrownianmotion.
Inourinvestigations,interactionsoftheparticleswiththeirsurroundingfluidismodelledbytheover-dampedLangevin
equations
(cid:113)
r˙ =γ−1[F(t)e (Θ )−∇V(r ,{r })]+ 2k Tγ−1Rt(t), (1)
i t i i i j B t i
(cid:113)
Θ˙ =γ−1m(t)+ 2k Tγ−1Rr(t). (2)
r B r i
where, r isthe(two-dimensional)positionofparticlei, Θ theangledescribingtheparticleorientation, γ the
i i (t,r)
translational(rotational)frictioncoefficient,F andmaforceandtorquecorrespondingtotherespectivetypeofactive
motion,e=(cos(Θ),sin(Θ))T theparticleorientation,V aninteractionpotentialbetweenallparticlesinthesystem,
k theBoltzmannconstant,T thetemperatureandR(t,r)anoisetermwithzeromeanandcorrelationsaccordingto
B i
(cid:68) (cid:69)
R(t,r)(t)R(t,r)(t′) =δ δ(t−t′),where⟨·⟩denotesanensembleaverage.
i j ij
Toquantifytherelativeimportanceofactiveandpassivemotion,wedefinetranslationalandrotationalPe´cletnumbers
τdiff
Petrans,rot = trans,rot. (3)
τact
trans,rot
2EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
Here,
1 γ 2π
τdiff = = r , τact = , (4)
rot 2D 2k T rot ωact
rot B
arethetimescaleofdecorrelationoftheparticledirectorthroughrotationaldiffusionandthetimescaleforoneactive
rotation,respectively. Forthetranslationaldegreesoffreedomwehave
a2 a2γ a
τdiff = = t, τact = , (5)
trans D k T trans vact
trans B
asthetimescalefordiffusionofoneparticleradiusandthetimescaleforswimmingofoneparticleradius,respectively.
InregimeswherePetrans,rot ≫1thedynamicswillbedominatedbyactivemotionandwhenPetrans,rot ≪1,itwill
resemblepassivediffusion.
2.2 Actor-CriticReinforcementLearning
Reinforcementlearningconcernsitselfwiththeinteractionsbetweenanagentanditsenvironmentwithinwhichit
graduallylearnstoachieveadesiredtask[SuttonandBarto,2018]. Thisagentistypicallyprovidedwithasetofactions
itmayperformandusesapolicy,π(a |s )todecideattimet,basedonitscurrentstate,s ,whatthebestactions,a
t t t t
willbesuchthatitmaximisesareward,r(s ). Overthecourseofoneormanysimulations,thispolicywillbeupdated
t
sothattheagentbecomesmoreefficientataccomplishingthistaskandmaximisingitsreward,
π′ =argmax⟨r(s |π)⟩ (6)
t
π
Deepreinforcementlearningaccomplishesthistaskusingdeepneuralnetworksasthepolicy,π[Arulkumaranetal.,
2017]. Duringourinvestigations,theactor-criticapproachtodeepreinforcementlearninghasbeenadoptedduetoits
flexibilityandefficacy[Bartoetal.,1983,Grondmanetal.,2012]. Inactor-criticreinforcementlearning,theactortakes
ontheroleofthepolicy,π ,parameterizedbyθ,takingasinputthecurrentstateoftheagentandreturningoftentimesa
θ
distributionoverpossibleactionsfromwhichoneisselected. Thecriticthentakesontheroleofavaluefunction,Vπθ,
ω
theobjectiveofwhichistodescribetheexpectedreturnofanagentstartinginstates andfollowingpolicyπ. During
t
training,theactoristaskedwithmaximisingitsfinite-horizonreturnofitspolicy
(cid:42)(cid:34) T (cid:35)(cid:43)
(cid:88)
J(π θ)= logπ θ(a t|s t)·Aπθ(s t,a t) , (7)
t=0 τ
whereAπθ istheso-calledadvantage,computedby
Aπ tθ =G(s t,a t)−V ωπθ(s t), (8)
whereGisananalyticexpectedreturnsfunction. Inourstudies,asimpledecayingreturnfunction
T
G
=(cid:88) ϵt′−tr
, (9)
t t′
t′=t
withdecayfactorϵisused. J(π )ismaximisedbywayofgradientascentontheactorparameterswithupdatestaking
θ
theform
θ′ =θ+η·∇ J(π ), (10)
θ θ
withlearningrateη. Recallingthattheactoroutputisadistributionoveractions,shouldtheadvantagebenegative,i.e,
thecriticbelievesabettertrajectorycouldhavebeenchosen,thelogprobabilityoftheseactionwillbediscouraged. If
thisnumberispositive,theactorhasoutperformedtheexpectationofthecriticandthepolicyisreinforced. Thecritic
networkistraineddirectlyonthechosenexpectedreturnsfunctionviatheHuberloss
(cid:26)1 ·(ytrue−ypredicted)2 ,for|ytrue−ypredicted|≤δ,
Lδ(ytrue,ypredicted)= 2 (11)
δ·(|ytrue−ypredicted|− 1δ) ,otherwise,
2
withδ = 1inallstudies. Suchanupdateprocedureisreferredtoassimpleorvanillapolicygradient[Suttonetal.,
1999]. Whilstmoresophisticatedapproachesexist,namelyproximalpolicyoptimization[Schulmanetal.,2017],for
thisparticularstudy,thesimplerapproachsufficed.
3EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
2.3 Multi-AgentReinforcementLearning
In our simulations, we work with not one, but many agents simultaneously, moving from the general concept of
reinforcementlearningintomulti-agentreinforcementlearning(MARL)[GronauerandDiepold,2022]. Inthesecases,
eachagentsharesasingleactorandcriticnetworkandattheupdatetime,alsotheexperiencethattheyhavegathered.
Duringthesimulations,eachagentaskstheactorforanactiontotakeindividuallyandcollectsitsownreward. Atthe
timeoftheupdate,J(π )becomes,
θ
N
1 (cid:88)
J = J (π ), (12)
MARL N i θ
i
whereisumsovertheagentsinthesystemandJ issimplyEquation(7)forasingleagent. Inthisway,theexperience
i
ofeachagentisaccumulatedandupdatedtogether.
ThefieldofMARLhasaavastsetofdefinitionswithrespecttohowindividualagentsinteractandshareknowledgein
ordertoachievetheproblemtheyaretrainingon[Oliehoeketal.,2016]. Inthiswork,adecentralizedMarkovdecision
processisusedtodescribehowtheagentsinthesysteminteract. Thesystemisconsidereddecentralizedaseachagent
receivesonlylocalinformationregardingitsenvironmentandalocalrewardforitsownactions. Duringtraining,these
rewardsandlocalstatesaresummedoverandindoingso,theagentssharetheknowledgewithoneanother.
3 Methods
3.1 SwarmRL
Allreinforcementlearningandsimulationhasbeenhandledthroughtheopen-sourcesoftwarepackage,SwarmRL[Tovey
etal.,2023a]. SwarmRLisaPythonlibrarybuilttocombinemoleculardynamicsengineswithclassicalcontroland
reinforcementlearningalgorithms. AllmachinelearningusestheJAX[Bradburyetal.,2018]andFlax[Heeketal.,
2023]libraries.
3.2 ESPResSoSimulations
Inthisstudy,reinforcementlearningisappliedtotrainingmicroswimmersinaphysicallyrealisticsimulatedenvironment.
Forthisenvironment,weemploytheESPResSosimulationengine[Weiketal.,2019]. Trajectoriesoftheparticles
are simulated using the over-damped Langevin equations of motion for both position and orientation described in
Equations(1)and (2). Thefrictioncoefficientofasphericalagentwithradiusrinafluidwithdynamicviscosityµ
iscalculatedaccordingtoStokes’lawasγ =6πµrandγ =8πµr3. Interactionsbetweenthesphericalagentsare
t r
modelledwiththetwo-bodyWeeks-Chandler-Anderson(WCA)potential[Weeksetal.,1971],whichcanbeseenasan
almost-hard-sphereinteraction
 (cid:20)(cid:16) (cid:17)12 (cid:16) (cid:17)6(cid:21)
4·V σ − σ +V , r <21/6σ
V(r ij)= 0 rij rij 0 ij (13)
0, else.
Here,r =||r −r || istheEuclideandistancebetweentheparticles,andσ =2athecolloiddiameter. Wechoose
ij i j 2
theinteractionstrengthV =k T. Detailsontheanisotropicparticles’frictioncoefficientsandinteractionpotentials
0 B
canbefoundinthesupplementaryinformation.
3.3 ReinforcementLearningParameters
Inourinvestigations,theactor-criticapproachtoreinforcementlearningisutilisedwithanetworkarchitecturedisplayed
inFigure1. Atwo-layernetwork,eachwith128units,isdeployedforboththeactorandthecritic,alongwithReLU
activationfunctions. Duringthetraining,eachnetworkistrainedfor10000episodes,eachofwhichconsistsof20
applicationsofthepolicyover4000simulationtimesteps. Eachepisodewouldbe2sinreal-time. Updatesofthe
networkarehandledbytheAdamoptimizer[KingmaandBa,2017]usingalearningrateof0.002. Foreachswim
speedandagentsize,20reinforcementlearningrunswereperformedtocollectstatistics.
3.4 AgentDefinition
Thestudyconsideredthreeagentshapeswithafixedsetofactions. Agentshapesweredesignedtomimicsomeof
thosefoundinbiology: oblate,prolate,andsphericalbacteriaormicroswimmers. Figure2displaysrenderingsofthese
4EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
Figure1: Representationofactor-criticreinforcementlearningarchitectures.
Figure2: GraphicalRepresentationofthethreeagentshapesconsideredinthisstudy,thesphere(center),prolate(right),
andoblate(left). Ineachcase,thevolumeoftheagentiskeptequalforagivenradiusvalue.
5EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
agentsforradius1µmconstructedusingtheVedoPythonpackage[Musyetal.,2023]. Astheagentsaredesignedto
mimicbacterialparticles,weendowthemwiththeabilitytoperformthefouractionsdescribedinSection2.1,
 Translate: v =n·dµms−1,ω =0.0s−1
RotateCCW:
v =0µms−1,ω =10.472s−1
A= (14)
RotateCW: v =0µms−1,ω =−10.472s−1

DoNothing: v =0µms−1,ω =0.0s−1,
wheredisthecolloiddiameter,nisascalingfactorthatwevaryduringtheexperiment,andωistheangularvelocity
measuredinradianspersecond. TherotationspeedwaschosentobesimilartothatofEscherichiacoli[Bergand
Brown,1972]. Inlinewith[MurrayandJackson,1992],wearguethatagentvolumeisproportionaltoitsswimming
speed. Therefore,theactionismeasuredinbodylengths,andallagentswiththesameradiuswillswimatthesame
speed. Theagentsreceiveastatedescriptiondesignedtoresembleabacteriumsensingchangesinitssurroundings,
definedmathematicallyby
o (t)=f(||r (t)−r (t)|| )−f(||r (t−∆t)−r (t−∆t)|| ), (15)
i i s 2 i s 2
whereo istheobservablefortheithagent,f isthefieldchosentorepresentthechemicalbeingsensed,inourstudy,
i
1 ,r (t)isthepositionoftheithagentattimet,∆tistheamountoftimesincethelastactionwascomputedandˆr (t)
ris i s
denotesthepositionofthesourceofthefieldattimet. Toencouragechemotaxis,agentsarerewardedusingasimilar
function
(cid:26)
o ifo >0
r (t)= i i (16)
i 0 else.
Thisway,movementtowardsthesourceisencouraged,butmovementawayisnotexplicitlydiscouraged. Wefurther
refrainfromusinganabsolutemeasureofthefieldinthisstudyasitwouldnotresemblethenaturalsensingabilitiesof
thebacteria[BrenandEisenbach,2000]. Theadditionofsucharewardmightbeusedtoencourageagentstoform
groupsreminiscentofbiofilmsortoreplicatetheactofdigestingthesourceofthefield,bothofwhichareleftforfuture
studies.
3.5 ComputationalMethods
TraininganddeploymentofthereinforcementlearningmodelswasperformedontheUniversityofStuttgartSimTech
computecluster. EachsimulationandtrainingroutineutilisedsixthreadsofanAMDEPYC7702CPUnode,and
allsimulationswereruninparallel. Duetothesystemsizesandmachinelearningbeingperformed,noGPUswere
requiredfortheseexperiments. Trainingofeachmodelrequiredapproximatelytwenty-fourhours,andthedeployment
simulationswereapproximatelysixhours. Thesimulationsandmodelswereanalysedonthesameclusterhardware.
4 Results
Duetothesimilarityintheresultsandtheamountofanalysis,onlytheplotsforthesphericalagentanalysisareshown
inthemainmanuscript. AllotherplotsareincludedintheSI,andanydeviationsbetweenresultsarementionedhere.
4.1 ProbabilityofEmergentChemotaxis
ThisinvestigationaimstoidentifylimitsonemergentchemotaxisinRLagentsinthehopethatsuchlimitscrossover
into biology, allowing us to study natural biological processes using RL as a valid surrogate model. These limits
suggestformulatingaphasediagramwithforbiddenregionswherethisbehaviourisstrictlyprohibited. Tothisend,
allsimulationswerecollectedwherethefinal50%ofthedeploymenttrajectorywasbelow15µmfromthesourceof
thefield. Thisdistancewasdeterminedbasedonthevisualobservationthatnomodelthathadsuccessfullylearned
chemotaxiswasfartherawayfromthesourcethanthisdistance. Thesuccessfulsimulationswereusedtocomputethe
probabilityoflearningchemotaxisbyrationingthemagainstthetotalnumberofsimulationsperformedforasingle
speedandagentsize. Figure3showsthecomputedphasediagram. Thefigureplotsthesampleddatapoints;thealpha
valuecorrespondstotheprobability,withthemoretransparentpointsbeinglessprobable. Onecanidentifyaforbidden
regioninthesize-speedspace. Interestingly,itappearsthatsmaller,fastercolloidsaremorelikelytolearnaneffective
chemotaxispolicy. SupposeweconsiderthedifficultytheRLalgorithmhasintrainingapolicywiththereal-world
problemofevolvingasuitablestructureforlife. Inthatcase,theseresultssuggestatrade-offbetweenspeedandsize
whenlearninghowtoperformchemotaxis. ThemostcriticalcomponentofFigure3isthetheoreticalboundariesformed
byconsideringtheratiobetweenBrownianmotionandtheactivemotionoftheparticlesdescribedbyEquations4
and5. ThegreenlinesinFigure3correspondtothecolloidradiusandspeedvaluesforwhichthisratiois1.0for
6EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
1.0
5.0
4.5
0.8
4.0
3.5
0.6
3.0
0.4
2.5
2.0
0.2
1.5
1.0
0.0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure3: ProbabilityofsuccessfulchemotaxisemergingfromRLstudies. Rawdatafromtheexperiment. Thecolour
ofeachpointcorrespondstothenumberofRLsimulationsthatsuccessfullylearnedhowtoperformchemotaxis. The
green lines indicate the theoretical values at which translational (solid) and rotational (dashed) diffusion becomes
dominantcomparedtotheactivemotionoftheagents.
translational(solid)androtational(dashed)diffusion. ThetranslationratioformsaboundarywheretheRLagents
cannolongerlearnsuccessfulchemotaxis. Therotationaldiffusionlineappearslessstrict,particularlyforthefaster
agents,whereitappearsthatwithenoughtranslationalactivity,theagentscanovercomehavingrotationaldiffusion
dominateoveractiverotation. Thealignmentofourresultswiththetheoreticalvaluessuggeststhatitdoessoassoon
asitisphysicallypossibleforanRLagenttolearnchemotaxis. Sucharesultencouragesonetoconsiderstudying
furtherfeaturesofthemodelstounderstandhowthesefeaturesmightalsoariseintheseagents’biologicalorartificial
counterparts. Interestingly,theonsetofsuccessfulchemotaxiscantakeplacefarbelowthistheoreticallimitbutvery
rarely.
4.2 LearningEfficiency
Next,welookathowtherewardreceivedfromthereinforcementlearningprocesschangeddependingonthesizeand
speedofthecolloids. Thismeasurewillindicatehoweasyitwasforthemodeltolearnthepolicyrequiredtoperform
chemotaxis. Figure4outlinestheresultsofthisstudyinasimilarmannertoFigure3. Inthefigure,thepoint’scolour
correspondstothetotalrewardaccumulatedbytheagentsduringall10’000trainingepisodes. Inordertocomputethe
colourvaluesinFigure4,wecorrectedthesizedifferencebetweenthecolloids. Intheoriginalsimulations,anexplicit
distancetothesourceisusedintherewardcomputation. However,thisbiasestheresultssuchthatsmallercolloids,
nomatterhowsuccessfultheywere,willachieveexponentiallyhigherrewardsastheycanapproachthesourcemore
closely. Therefore,therewardsinFigure4werecomputedbyconvertingtherewardfromdistancetothenumberof
bodylengthsfromthesource. WecanseethattherewarddiagramsroughlymirrortheresultsshowninFigure3with
7
1
s
htgnel
ydob
/
deepS
miwS
sixatomehC
tnegremE
fo
ytilibaborPEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
5.0
5000
4.5
4.0 4000
3.5
3000
3.0
2.5
2000
2.0
1000
1.5
1.0
0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure4: ProbabilityofsuccessfulchemotaxisemergingfromRLstudies. Rawdatafromtheexperiment. Thecolour
ofeachpointcorrespondstothemaximumrewardachievedbytheagentsduringthe10’000episodes.
largerdiscrepanciesbetweenthelargerandsmallercolloids. Namely,therewardsachievedbysmallandfastagentsis
noticeablylargerthanthoseofthebiggeragents. Thiseffectisparticularlyevidentintheprolateandoblatesimulations
(SIFigures10and14). Itislikelyduetotheirhoppingovertheintendedtargetandinabilitytositonitasaccuratelyas
thesmalleragents.
4.3 PolicyEfficiency
Itisclearfromtheprevioussectionthatmicroswimmersofdifferentsizesandspeedsdifferintheirprobabilityof
emergent chemotaxis. However, what the differences are, if any, between their adopted strategy still needs to be
determined. Toidentifythesedifferences,thedeploymentsimulationswereanalysedtocomputethefinalequilibrium
distanceofthecolloidsaroundthesourceaswellasafterhowmanyactionupdatestheyreachedthisdistance. Figure5
displaystheresultsofthisinvestigation. Figure5(left)detailstheequilibriumdistanceoftheagentsasafunctionof
radiusforallstudiedswimspeeds. Ontheright-handside,weseeaclearemergenceofalineartrendasthelimiting
factorgettingclosertothesourcebecomesthesizeofthecolloidanditsspeed. Theleftnon-linearsideoftheplot
alsocontainsinterestingfeatures. Asidefromthespeedofone,theminimumdistancefromthesourceofthechemical
fieldisachievedatasimilarcolloidsizeforallofthedifferentspeeds,withfasteragentsabletoachieveslightlybetter
equilibriumdistanceswithsmallerbodies. InFigure5(right),weseetheaveragetimethecolloidsreachtheequilibrium
distance. For the smaller agents, as is perhaps intuitive, the faster colloids can orient and move themselves to the
sourcefasterthantheirslowercounterparts. However,thisrelationshipfadesforlargercolloidsasweseethatafter
approximately1µmradii,allcolloidsizesandspeedsconvergeatsimilartimesexceptfortheslowestatonebody
length. Thetimetominimumconvergesslightlyabove25s. Theresultsalsosuggestthatafter0.5µmradius,thereis
noconceivablebenefitand,infact,duetothelargerequilibriumdistance,perhapsevenadetrimentinbeinglarger.
8
1
s
shtgnel
ydob
/
deepS
miwS
gniniart
morf
drawer
latoTEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
10
12
Swim Speed
8 1.0
10
2.0
3.0
8 6 4.0
5.0
6 4
4 2
2
0
0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m Colloid Radius / m
Figure5: (left)Meandistancefromthesourceforeachswimspeedandcolloidsize. Aclearminimumineachplot
suggestsanoptimalsizedependentonswimspeed. (right)Rateofconvergencetothesourcefordifferentswimspeeds
andsizes. Interestingly,theconvergencerateoflargercolloidsisrelativelysimilar,suggestingsomeredundancyin
largerbodysizesandswimspeeds.
Interestingly,themostunstableequilibriumdistances,identifiedbylargevarianceinmeanvalueanddistancefrom
thesource,occurclosetoorwithintheregiondisplayedinFigure3whererotationaldiffusionoverpowerstheactive
rotationoftheagents. Thisstrongenvironmentaleffectcouldcauseinstabilityinthesemodelsastheymustrelysolely
ontheiractivetranslationtoachievechemotaxis.
4.4 EmergentPolicy
As a final investigation, we determine whether the emergent policy of the RL agent differs for changing physical
propertiesandshapes. Studyingtheparticles’trajectoryaloneisalmostimpossibleastheydonotshowlargedeviations
fromoneanother. Therefore,todoso,thetrainedmodelsaregiventestdataoveradomain,x∈(−10.0,10.0)andthe
probabilityofselectingeachactioniscomputedfromthenetworkoutputs. Thenetworkoutputwillbefournumbers
foreachconcentrationvalue;therefore,beforeperformingfurtheranalysis,theseoutputsareflattenedintoasingle
vectorforeachmodel,whichwewillrefertoastheprobabilityvectorsofthenetwork. Inordertoidentifyanystructure
inthedata,westudythetwo-componentt-distributedstochasticneighbourhoodembedding(t-SNE)[vanderMaaten
andHinton,2008]oftheprobabilityvectorsasimplementedinthesklearnPythonpackage[Pedregosaetal.,2011].
Figure6outlinestheresultsofthet-SNEforthepolicydatawithaperplexityof300andprinciple-component-analysis
(PCA)initialisation. Examiningthet-SNEplots,weseetheemergenceoffourgroups,oneofwhichisseemingly
divided into two smaller subgroups. Using this information, we perform k-means clustering [Lloyd, 1982] on the
probabilityvectorstosplitthemintofourclusters. TheprobabilityoftheoutcomeofeachpolicyislistedinTable1
alongwiththeexplainedvariancefromaPCAdecompositionoftheprobabilityvectors. Theprobabilitiesarecomputed
byexaminingthenumberofpointsclusteredintoeachclassbythek-meansalgorithm,whichweassigntoapolicy
bydirectlyexaminingtheactionprobabilitiesoftheagentsmappingintotheclass. Thediagramsusedtoperform
thismappingareincludedintheSI,whereweshowtheprobabilitiesofeachactionbeingtakenforallagentsizes,
shapes,andspeeds. Wealsoincludesmallersamplepolicydiagramstodemonstratetheactionstakenbytheagents
foreachstrategyinSIFigure7. However,asweonlyaskforfourclasses,thispercentagewillnaturallyignoremore
convolutedandrarepoliciesthatthet-SNEandK-Meanscannotsufficientlydistinguishfromothers. Wealsoperform
PCAdecompositionontheprobabilityvectorstoidentifyhowmucheachpolicyexplainsthedatadistribution. Inthis
approach,weseethat5.9%ofthedatabelongstocomponentswithasmallerthanone%impactonthevarianceof
thePCA.Whenweexaminethepoliciesinthisregion,theyaretypicallymadeupofweakcombinationsofthemore
dominantpolicieswithveryfewexceptions. Thesepoliciesmayexplainthesplittinginthemedium-sizedgroupin
Figure6. Theremainderofthesectionwilldiscusseachemergentpolicyindetail,includingsomepoliciespoorly
9
m
/ ecruoS
morf
ecnatsiD
muirbiliuqE
setunim
/
muminim
ot
emiTEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
50
Gradient Gliding
40
30 2.5
Run and Rotate, CW Run and Rotate, CCW
20 2.0
Brownian Piloting
1.5
10
1.0
0
0.5
10
20
30
40 30 20 10 0 10 20 30 40
t-SNE Dimension 1
Figure6: t-SNEembeddingofthepolicyvectorsforallsuccessfulagentsinthestudy. Fourlargegroupsareformed,
correspondingtothepolicieslearnedbytheagents. Thecolourinthesediagramscorrespondstothesizeofthestudied
agents.
PolicyName PercentageLearned(K-Means) ExplainedVariance(PCA)
RunandRotate 83.49 83.5
GradientGliding 12.88 7.1
BrownianPiloting 3.63 3.5
ExoticPolicies 0.0 5.9
Table1: Percentageandexplainedvarianceofagentswhichlearnedspecificpolicyalongwiththeexplainedvarianceof
theprinciplecomponentsforeachpolicyidentified.
capturedbytheembeddingmethods.
Run and Rotate In the vast majority of cases (83 %), the agents learned a policy strikingly similar to the
run-and-tumble approach found in nature. In these cases, upon experiencing a negative input to the network,
signifyingamovementawayfromthesourceofthegradient,theagentsrotateeitherCWorCCW.Interestingly,once
theagentschoseadirectiontorotate,theydidnotusetheotherone. Uponpositiveinputtothenetwork,i.e.,movement
towardsthesourceofthegradient,theagentschosetotranslatewithprobability1. Thispolicycanbeseenclearlyfor
thelargercolloidsinthetoptworowsofSIFigure7. CWvsCCWselectionwaseventhroughoutthesimulations,with
nopreferreddirectiondiscovered.
GradientGliding Forlargecolloids,sometooktranslateformostinputs,onlyrotatingforminimalchangesinthe
gradient,suchasthoseoccurringwhenfarawayfromthesourceormovingequipotentialaroundit. Eveninthese
cases,thestrategyisinconsistentandstillhasahighprobabilityoftranslation. Whilethisstrategymightappearstrange
initially,ithelpstoconsiderregionswherethegradientwillbesoslight. Astheagentsareinitialisedinthesimulation,
theywillsitinaregionwithminimalgradient. Uponmovingaroundthisarea,theywilllikelyrotateuntiltheyare
draggedintoaregionwherethegradientincreasesenoughforittobegintranslating. Thiswascommonamongstthe
chosenpolicies, withsomewherebetween7%and12%optingforthisapproach. Thediscrepancyinpercentage
arisesduetothespuriouspoliciesdiscussedinalatersection. WelabelthispolicyGradient Glidingasthecolloids
generallyfollowatranslationpathwithverysmalladjustmentsmadeunderlowgradientchanges. Anexampleplotof
thisstrategycanbeseeninSIFigure7,rowthree.
10
2
noisnemiD
ENS-t
m
eziS
diolloCEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
BrownianPiloting Analternativepolicy,referredtohereasBrownian Piloting,wasseenparticularlyinthecase
ofsmalleragentswhererotationalBrownianmotionovercomestheactiverotation. Theagentslearnedtodonothing
whenexperiencingnegativenetworkinputsandtotranslateiftheyseeapositiveone. Inthisway,theagentsdonot
fightagainsttheBrownianforceswhentheiractivemotioncannotovercomeit. Asthiswasonlyseeninthesmall
agents,itisclearthatsuchapolicycouldbemoreoptimal. However,itdoesdemonstratethatsmall,weakagentscan
stillsuccessfullylearntonavigatetowardsourcesofnutrition. Overall,thispolicywasadoptedin3.6%ofcases. One
canseeBrownianPilotinginthefourthrowofSIFigure7.
Exoticpolicies(EP) Aswaspreviouslymentioned,approximately5.9%oftheemergentpolicieswerenotwell
mappedintosingleclasses. However,wecanidentifyseveralso-calledexoticpoliciesbymanuallylookingattheaction
probabilities.
• EP 1: In these cases, the agents only translated when their input was negative. Otherwise, they chose to
donothing. Thiswasonlyobservedinsmallagentswhererandomfluctuationssignificantlyimpactedtheir
rotationmorethanactiveswimming.
• EP2: Inthisapproach,wesawtheagentschoosetheDo Nothingactionatalmostalltimes,exceptwhen
theinputtothenetworkwasasmallpositivegradient,atwhichpointtheywouldtranslate. Thisapproach
occurredforsmallcolloidswhereallrandomforcesoutnumberedtheiractiveswimming. Thisindicatesthat
microswimmerscansurviveincaseswheretheirswimmingisoverpowered,effectivelyusingtheirenvironment
to perform successful chemotaxis. Whilst extremely inefficient, this swim strategy preserves energy and
successfullyallowstheagentstoperformchemotaxisdespiteanalmostimpossiblecondition.
• Combinations: Wenoticedcombinationsofother,morecommonapproachesinmanyexoticpolicies. For
example,somesmallercolloidsinBrownian-dominatedregimesperformedactiverotationinbothCWand
CCWdirectionsfornegativeinputsandtranslateforpositive. Weidentifythispolicyasmoreorlessequivalent
toBrownian Pilotingastheactiverotationwillnotyieldmorethansimplysittingstill. Inothercases,
theonsetoftranslationwasdelayedoraccelerated,yieldingslightvariationsofRun and Rotate. Asthese
pointscombinedmixturesofthemoredominantpoliciesandyetdidnotoccuroften,theclusteringalgorithms
couldnotsuccessfullyseparatethemintodistinctclasses.
TheresultstellusthatinthecaseswhereactivetranslationandrotationarepossibleanddominateoverBrownian
effects,theagentsoftenlearntoperformarun-and-rotatetrajectory. IncaseswhereBrownianeffectsdominateactive
motion,theagentslearntoadapttothisenvironmentbyperformingonlyactionsthatmovethemintoanewenvironment,
byusingtheBrownianforcestotheiradvantage. Interestingly,thesepoliciesarenotwelldifferentiatedwithinthe
trajectoriesalone;onlybylookingattheneuralnetworkscanweseehowtheagentsmakedecisions. Suchaninsight
mightguideusinunderstandinghowreal-worldbacterianavigatetheirenvironments,andperhaps,howtodisrupt,asin
quorumquenching[Grandcle´mentetal.,2015],orsupport,asinquorumenhancement[Garc´ıa-Contrerasetal.,2014],
thisnavigation.
5 Conclusion
Inthisstudy,wehavetestedtheroleofsizeandswimspeedontheemergentstrategyofmicroscopicactiveagents
learningchemotaxisviamulti-agentactor-criticreinforcementlearning. Oursimulationsdemonstratedthatintelligent
agents can learn chemotactic behaviour, even in environments where Brownian random forces begin to dominate
their active motion. In such regimes, we found that the chemotaxis was not optimal in terms of their equilibrium
distancefromthesourceofthechemicalgradientorthespeedatwhichtheymadeittothesource. However,they
couldconsistentlyreachtheirtarget. Interestingly,wesawthatasthePe´cletnumbergrewaboveoneandactivemotion
dominated the Brownian forces and torques, the learned policies also converged quickly to a similar equilibrium
distanceandtime. Afterstudyingthepolicyefficiency,welookedintothestrategiesadoptedbytheagentstoperform
chemotaxis. WeidentifiedthreedominantstrategieswhichwenamedRun and Rotate,Gradient Gliding,and
Brownian Piloting. Thefirstpolicyoccurredmostoften(83.5%)butpredominantlyincaseswherethecolloids
werelargeenoughtonolongerbeinregionswhererotationalortranslationBrownianmotionovercametheiractive
swimming. Thisstrategyinvolvedtranslatingaslongastheinputtotheagentwaspositive,i.e.,movingtowardsthe
sourceandrotatingifitwasnegative. Thesecondmostcommonstrategy(7.1%emergence),alsooccurringinlarger
colloids, wastotranslateformostofthetimewhethertheinputwasnegativeorpositive, andonlywhentheinput
tothenetworkwassmallwouldtheysometimeschoosetorotate. Thisstrategymeantthatthecolloidsspentalong
timerotatingwhentheywerefarawayfromthesourcebuttranslatingwhentheyidentifiedthedirectionofthesource.
ThefinalcommonstrategyoccurredwhentherotationalandsometimestranslationalBrownianmotiondominatedthe
activeswimming. Inthesecases,theagentswouldperformtheDo Nothingactionwhiletheinputtothenetworkwas
11EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
negative,andonlywhenitwaspositivewouldtheybegintranslating. Weidentifiedfurtherpolicies,includingakind
oflazy swimmingwheretheagentsperformednoactionsexceptwhentheywerefarawayfromthesourceandthe
inputtothenetworkwasweaklypositive,aswellassomecaseswhereagentslearnedtorotateinbothCWandCCW
directionsbutofteninaregimewherethiswasnotuseful. Overall,wehaveidentifiedthatreinforcementlearning
canreplicatenaturalbehaviouroforganisms. However,itcanalsoprovideinsightintobiologicalswimmers’possible
strategiesandmayprovideapathforwardforexploitingthisknowledge. Afurtherpointofinterestwouldbetoidentify
naturalbiologicalswimmerswhohaveevolvedsuchswimmingpatternsorcanoutperformtheemergentstrategiesof
theRLagents.
6 DataandAvailability
Alldatacanbemadeavailableuponreasonablerequesttotheauthorsand,uponpublication,willbemadepublicly
availablethroughtheDaRUSservice.
7 Acknowledgements
C.HandS.TacknowledgefinancialsupportfromtheGermanFundingAgency(DeutscheForschungsgemeinschaft
DFG)underGermany’sExcellenceStrategyEXC2075-390740016,andS.TwassupportedbyanLGFstipendofthe
stateofBaden-Wu¨rttemberg. C.HandS.TacknowledgefinancialsupportfromtheGermanFundingAgency(Deutsche
Forschungsgemeinschaft DFG) under the Priority Program SPP 2363. C.H and C.L acknowledge funding by the
DeutscheForschungsgemeinschaft(DFG,GermanResearchFoundation)underProjectNumber327154368-SFB1313.
TheauthorswouldliketoacknowledgefundingfromtheDeutscheForschungsgemeinschaft(DFG,GermanResearch
Foundation)ComputeClustergrantno. 492175459.
References
K.Arulkumaran,M.P.Deisenroth,M.Brundage,andA.A.Bharath. Deepreinforcementlearning: Abriefsurvey.
IEEESignalProcessingMagazine,34(6):26–38,2017. doi: 10.1109/MSP.2017.2743240.
A.G.Barto,R.S.Sutton,andC.W.Anderson. Neuronlikeadaptiveelementsthatcansolvedifficultlearningcontrol
problems. IEEETransactionsonSystems,Man,andCybernetics,SMC-13(5):834–846,1983. doi: 10.1109/TSMC.
1983.6313077.
J.Bastos-Arrieta,A.Revilla-Guarinos,W.E.Uspal,andJ.Simmchen. Bacterialbiohybridmicroswimmers. Frontiers
inRoboticsandAI,5,2018. ISSN2296-9144. doi: 10.3389/frobt.2018.00097. URLhttps://www.frontiersin.
org/articles/10.3389/frobt.2018.00097.
H.Berg. coliinmotion2004springer. NewYork,2004.
H.C.BergandD.A.Brown.Chemotaxisinescherichiacolianalysedbythree-dimensionaltracking.Nature,239(5374):
500–504,Oct1972. ISSN1476-4687. doi: 10.1038/239500a0. URLhttps://doi.org/10.1038/239500a0.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S.Wanderman-Milne,andQ.Zhang. JAX:composabletransformationsofPython+NumPyprograms,2018. URL
http://github.com/google/jax.
A.BrenandM.Eisenbach. Howsignalsareheardduringbacterialchemotaxis: protein-proteininteractionsinsensory
signalpropagation. JBacteriol,182(24):6865–6873,Dec.2000.
N. C. Darnton, L. Turner, S. Rojevsky, and H. C. Berg. On torque and tumbling in swimming escherichia coli. J
Bacteriol,189(5):1756–1764,Dec.2006.
S.DattaandD.K.Srivastava. Stokesdragonaxiallysymmetricbodies: anewapproach. Proceedings-Mathematical
Sciences,109(4):441–452,Nov1999. ISSN0973-7685. doi: 10.1007/BF02838005. URLhttps://doi.org/10.
1007/BF02838005.
J.Elgeti,R.G.Winkler,andG.Gompper. Physicsofmicroswimmers—singleparticlemotionandcollectivebehavior:
areview. ReportsonProgressinPhysics,78(5):056601,apr2015. doi: 10.1088/0034-4885/78/5/056601. URL
https://dx.doi.org/10.1088/0034-4885/78/5/056601.
R.Garc´ıa-Contreras,L.Nun˜ez-Lo´pez,R.Jasso-Cha´vez,B.W.Kwan,J.A.Belmont,A.Rangel-Vega,T.Maeda,and
T.K.Wood. Quorumsensingenhancementofthestressresponsepromotesresistancetoquorumquenchingand
preventssocialcheating. ISMEJ,9(1):115–125,June2014.
12EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
J.G.GayandB.J.Berne. Modificationoftheoverlappotentialtomimicalinearsite–sitepotential. TheJournalof
ChemicalPhysics,74(6):3316–3319,031981. ISSN0021-9606. doi: 10.1063/1.441483. URLhttps://doi.org/
10.1063/1.441483.
C.Grandcle´ment,M.Tannie`res,S.More´ra,Y.Dessaux,andD.Faure. Quorumquenching: roleinnatureandapplied
developments. FEMSMicrobiologyReviews,40(1):86–116,102015. ISSN0168-6445. doi: 10.1093/femsre/fuv038.
URLhttps://doi.org/10.1093/femsre/fuv038.
S.GronauerandK.Diepold. Multi-agentdeepreinforcementlearning: asurvey. ArtificialIntelligenceReview,55(2):
895–943,Feb2022. ISSN1573-7462. doi: 10.1007/s10462-021-09996-w. URLhttps://doi.org/10.1007/
s10462-021-09996-w.
I.Grondman,L.Busoniu,G.A.D.Lopes,andR.Babuska. Asurveyofactor-criticreinforcementlearning: Standard
and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews),42(6):1291–1307,2012. doi: 10.1109/TSMCC.2012.2218595.
C.H.Hansen,R.G.Endres,andN.S.Wingreen. Chemotaxisinescherichiacoli: amolecularmodelforrobustprecise
adaptation. PLoSComputBiol,4(1):e1,Nov.2007.
B.Hartl,M.Hu¨bl,G.Kahl,andA.Zo¨ttl. Microswimmerslearningchemotaxiswithgeneticalgorithms. Proceedings
oftheNationalAcademyofSciences,118(19):e2019683118,2021. doi: 10.1073/pnas.2019683118. URLhttps:
//www.pnas.org/doi/abs/10.1073/pnas.2019683118.
J.Heek,A.Levskaya,A.Oliver,M.Ritter,B.Rondepierre,A.Steiner,andM.vanZee. Flax: Aneuralnetworklibrary
andecosystemforJAX,2023. URLhttp://github.com/google/flax.
D.P.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization,2017.
S. H. Koenig. Brownian motion of an ellipsoid. a correction to perrin’s results. Biopolymers, 14(11):2421–2423,
1975. doi: https://doi.org/10.1002/bip.1975.360141115. URLhttps://onlinelibrary.wiley.com/doi/abs/
10.1002/bip.1975.360141115.
S.Lloyd. Leastsquaresquantizationinpcm. IEEETransactionsonInformationTheory,28(2):129–137,1982. doi:
10.1109/TIT.1982.1056489.
C. Mo and X. Bian. Chemotaxis of sea urchin sperm cells through deep reinforcement learning, 2022. URL
https://arxiv.org/abs/2209.07407.
S. Muin˜os-Landin, A. Fischer, V. Holubec, and F. Cichos. Reinforcement learning with artificial microswimmers.
ScienceRobotics,6(52):eabd9285,2021. doi: 10.1126/scirobotics.abd9285. URLhttps://www.science.org/
doi/abs/10.1126/scirobotics.abd9285.
A.G.MurrayandG.A.Jackson. Viraldynamics: amodeloftheeffectsofsize, shape, motionandabundanceof
single-celledplanktonicorganismsandotherparticles. MarineEcologyProgressSeries,89(2/3):103–116,1992.
ISSN01718630,16161599. URLhttp://www.jstor.org/stable/24831780.
M.Musy,G.Jacquenot,G.Dalmasso,J.Lee,R.deBruin,J.Soltwedel,M.Tulldahl,Z.-Q.Zhou,RobinEnjalbert,
A.Pollack,B.Hacha,F.Claudi,C.Badger,X.Lu,A.Sol,A.Yershov,B.Sullivan,B.Lerner,D.Hrisca,D.Volpatto,
Evan, F.Matzkin, JohnsWor, mkerrinrapid,N.Schlo¨mer,RichardScottOZ,andO.Schneider. marcomusy/vedo:
2023.5.0,Nov.2023. URLhttps://doi.org/10.5281/zenodo.4587871.
F.A.Oliehoek,C.Amato,etal. AconciseintroductiontodecentralizedPOMDPs,volume1. Springer,2016.
F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Prettenhofer,R.Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
MachinelearninginPython. JournalofMachineLearningResearch,12:2825–2830,2011.
R.A.X.Persson. Note: ModificationoftheGay-Bernepotentialforimprovedaccuracyandspeed. TheJournalof
ChemicalPhysics,136(22):226101,062012. ISSN0021-9606. doi: 10.1063/1.4729745. URLhttps://doi.org/
10.1063/1.4729745.
M.Schnitzer,S.Block,H.Berg,andE.Purcell. Biologyofthechemotacticresponse(armitage,jp&lackie,jmeds)
15–34,1990.
J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimizationalgorithms,2017.
R.S.SuttonandA.G.Barto. ReinforcementLearning: AnIntroduction. TheMITPress,secondedition,2018. URL
http://incompleteideas.net/book/the-book-2nd.html.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with
functionapproximation. InS.Solla,T.Leen,andK.Mu¨ller,editors,AdvancesinNeuralInformationProcessing
Systems,volume12.MITPress,1999.
13EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
S.Tovey,C.Lohrmann,D.Zimmer,S.Koppenhoefer,andT.Merkt. SwarmRL,2023a. URLhttps://github.com/
SwarmRL/SwarmRL.
S.Tovey,D.Zimmer,C.Lohrmann,T.Merkt,S.Koppenhoefer,V.-L.Heuthe,C.Bechinger,andC.Holm. Environ-
mentaleffectsonemergentstrategyinmicro-scalemulti-agentreinforcementlearning,2023b.
L.Turner,W.S.Ryu,andH.C.Berg. Real-timeimagingoffluorescentflagellarfilaments. Journalofbacteriology,
182(10):2793–2801,2000.
L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):
2579–2605,2008. URLhttp://jmlr.org/papers/v9/vandermaaten08a.html.
G.H.WadhamsandJ.P.Armitage. Makingsenseofitall: bacterialchemotaxis. NatureReviewsMolecularCell
Biology, 5(12):1024–1037, Dec 2004. ISSN 1471-0080. doi: 10.1038/nrm1524. URL https://doi.org/10.
1038/nrm1524.
N.WatariandR.G.Larson. Thehydrodynamicsofarun-and-tumblebacteriumpropelledbypolymorphichelical
flagella. BiophysJ,98(1):12–17,Jan.2010.
J.D.Weeks,D.Chandler,andH.C.Andersen. Roleofrepulsiveforcesindeterminingtheequilibriumstructureof
simpleliquids. TheJournalofchemicalphysics,54(12):5237–5247,1971.
F. Weik, R. Weeber, K. Szuttor, K. Breitsprecher, J. de Graaf, M. Kuron, J. Landsgesell, H. Menke, D. Sean, and
C.Holm. Espresso4.0–anextensiblesoftwarepackageforsimulatingsoftmattersystems. TheEuropeanPhysical
JournalSpecialTopics,227(14):1789–1816,Mar2019. ISSN1951-6401. doi: 10.1140/epjst/e2019-800186-9. URL
https://doi.org/10.1140/epjst/e2019-800186-9.
.1 AnisotropicFrictionCoefficients
Inthecaseoftheanisotropicagents,thetranslationalfrictioncoefficientsaremorechallenging. Consideraspheroidal
agentwithaxialsemiaxis,r andequatorialsemiaxis,r . Let
ax eq
(cid:115)
(cid:18) (cid:19) (cid:18) (cid:19)
r 2 1+e
a=max(r ,r ), e= 1− eq , L=log . (17)
ax eq r 1−e
ax
ThetranslationalfrictioncannowbedefinedfollowingtheapproachtakenbyDattaandSrivastava[1999]wherefora
prolateparticle
γax =16πµae3(cid:2)(cid:0) 1+e2(cid:1) L−2e(cid:3)−1 (18)
t
istheaxialfrictioncoefficientand
γeq =32πµae3(cid:2) 2e+(3e2−1)L(cid:3)−1 (19)
t
theequatorial,andforanoblateparticle
(cid:104) (cid:105)−1
γ tax =8πµae3 e(1−e2)1 2 −(1−2e2)sin−1e (20)
(cid:104) (cid:105)−1
γ teq =16πµae3 (1+2e2)sin−1e−e(1−e2)1 2 (21)
aretheaxialandequatorialfrictioncoefficientsrespectively. RotationalfrictionfactorswerecomputeusingthePerrin
factorsKoenig[1975]. Todoso,weintroducefurthertheaspectratio
r
p= ax, (22)
r
eq
and
(cid:112)
|p2−1|
ξ = . (23)
p
Withthesetwoequations,wecanderivethePerrinS factorsforprolate
tanh−1ξ
Sprolate =2 (24)
ξ
andoblate
tan−1ξ
Soblate =2 (25)
ξ
14EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
particlesrespectively. Finally,wedefinethegenericrotationalfrictioncoefficientforanequivalentsphere
γ =8πµr r2 . (26)
sphere ax eq
Withthesedefinitions,theequatorialandaxialfrictioncoefficientsforbothprolateandoblateparticlescanbederivedas
4
1 −p2
γeq = p2 (27)
prolate/oblate r 3 (cid:20) (cid:16) (cid:17)2(cid:21)
2−Sprolate/oblate 2− 1
p
and
4 p2−1
γax = . (28)
prolate/oblate r 32p2−Sprolate/oblate
For anisotropic particles, the translational friction coefficient in Equation (1) becomes a tensorial quantity with
γ =diag(γeq,γax)inthecomovingframeofreferenceofeachparticleinwhichthesecondcoordinatepointsalong
t t t
thedirectore(t). Sinceparticlesarefixedtotwodimensions,rotationhappensonlyaroundanequatorialaxissuch
thatforanisotropicparticlesγ = γeq ineq.(2). TheESPResSo[Weiketal.,2019]simulationpackageisusedto
r r
numericallysolveEquations(1)and(2)withatime-stepδt=0.005s,theactionsthatdetermineF(t)andm(t)are
updatedeverytimeslice∆t=0.1s. Inallcases,unlessotherwisespecified,whenreferringtotimeinthisinvestigation,
werefertothetimeslice,i.e.,thenumberoftimesanactioniscomputedforeachagentinthesimulation.
.2 AnisotropicInteractionPotential
Fortheprolateandoblateagents,weusethemodifiedGay-BernepotentialforanisotropicparticlesPersson[2012],
GayandBerne[1981]
 (cid:20)(cid:16) (cid:17)12 (cid:16) (cid:17)6(cid:21)
V (u i,u j,r ij)=ϵ(u i,u j,r ij) rij−σ(ui,σ u0 j,rij)+σ0 − rij−σ(ui,σ u0 j,rij)+σ0 , r ij <4σmax(l,l−1),
0, else,
(29)
wherer isthevectordistancebetweenthecenterofmassofeachparticle,u isthedirectionvectoroftheithparticle,
ij i
andϵ(u ,u ,r )andσ(u ,u ,r )areadditionalfunctionsdependingontheorientationsoftheparticles. Weusethe
i j ij i j ij
augmentedformsofthesefunctionwhere
(cid:20) d−1−1 (cid:21)
ϵ(u ,u ,r )=ϵ 1+ (||r ·u || +||r ·u || ) (30)
i j ij 0 2 ij 1 2 ij 2 2
and
(cid:20) (cid:21)
l−1
σ(u ,u ,r )=σ 1+ (||r ·u || +||r ·u || ) , (31)
i j ij 0 2 ij 1 2 ij 2 2
wheredisratiobetweentheside-by-sidebindingenergyandtheend-to-endbindingenergyandlistheaspectration,
which,inthiswork,wassetto3fortheprolateparticlesand 1 fortheoblates. Wechooseσ =σandϵ =V .
3 0 0 0
.3 PolicyExamples
Hereweshowexamplepolicydiagramsforeachpolicydiscussedinthemanuscript.
.4 ShapeStudies
Asdiscussedinthemanuscript,weperformedthechemotaxisstudyforspherical,prolate,andoblateparticles. Herewe
displayanddiscusstheresultsnotpresentedinthemainmanuscript.
.4.1 Sphere
Whilemostofthesphereresultsarepresentedinthemainmanuscript,therawpolicyplotsarepresentedhere. Note,bl
inthefigurestandsforbodylengths. Thespherepolicydiagramsoutlinethemajorityofthepoliciesdiscussedinthe
maintext. Onthexaxis,thechangeingradientisplottedandonthey,thecolloidshape. Thecolourofthediagram
representstheprobabilityofanactionbeingtakenandeachcolumncorrespondstoasingleaction. Therowsarethe
differentswimspeedsdescendingfromonetofive. Thediagramsshowtheforbiddenregioninthechemotaxisbelow
approximately0.5µm. Afterthispoint,weseetheemergenceofnon-zeroprobabilitiesasthenetworkshavelearnedto
performchemotaxis.
15EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
.4.2 Oblate
Theoblateparticlesdemonstratedsimilarbehaviourtothesphericalcolloids,notshowinganyuniquepolicydeviations.
.4.3 Prolate
Theprolatesimulationsweresimilartoboththesphericalandoblatestudieswiththeexceptionofoneveryunique
policy. Mentionedinthemainmanuscript,wefoundanagentthatappearedtoperformnoactionsuntiltheinputto
thenetworkwassmallandpositive. Uponreceivingsuchaninput,theagentwouldtranslate. Moreinterestingly,this
occuredforaparticleofsizearound0.3µm,farbelowthetheoreticalboundariesforrandomforcestobegindominating
themotion. Thiswasalsothesmallestagentinallsimulationscapableofachievingchemotaxis.
16EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
a) Run and Rotate CW
CCW Translate CW Do Nothing
1.0 1.0 0.006
0.007
0.006 0.8 0.8 0.005
0.005 0.004 0.6 0.6
0.004
0.003
0.003 0.4 0.4
0.002
0.002
0.2 0.2
0.001 0.001
0.000 0.0 0.0 0.000
10 5 0 5 10 10 5 0 5 10 10 5 0 5 10 10 5 0 5 10
b) Run and Rotate CCW
1.0 1.0 0.10
0.004
0.8 0.8 0.08
0.003 0.6 0.6 0.06
0.4 0.4 0.002 0.04
0.2 0.2 0.001 0.02
0.0 0.0 0.000 0.00
10 5 0 5 10 10 5 0 5 10 10 5 0 5 10 10 5 0 5 10
c) Gradient Gliding
0.00175
0.175 1.000
0.020
0.975 0.00150
0.150
0.950 0.00125
0.125 0.015
0.925
0.00100
0.100
0.900
0.075 0.010 0.00075
0.875
0.050 0.850 0.005 0.00050
0.025 0.825 0.00025
0.000 0.800 0.000 0.00000
10 5 0 5 10 10 5 0 5 10 10 5 0 5 10 10 5 0 5 10
d) Brownian Piloting
0.06
1.0 1.0
0.30
0.05
0.25 0.8 0.8
0.04
0.20 0.6 0.6
0.03
0.15
0.4 0.4
0.02
0.10
0.05 0.2 0.01 0.2
0.00 0.0 0.00 0.0
10 5 0 5 10 10 5 0 5 10 10 5 0 5 10 10 5 0 5 10
Concentration Change Concentration Change Concentration Change Concentration Change
Figure7: Examplesofeachemergentpolicyfoundduringtheinvestigation. a)andb)aretherunandrotatepolicyfor
CWandCCWdirections. Youcanseethatastheinputtothenetworkbecomesnegative,theagentsdecidetorotateand
astheyaremovingtowardsthesource,theytranslate. c)TheJustrun,rarelyrotatepolicywhereformostinputs,the
agentwilltranslate,butfarawayfromthesource,whentheinputissmall,theagentmayalsodecidetorotate. d)Do
nothingwhennegativeandrunwhenpositive.
17
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborPEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
Figure8: Emergentpolicyofthesphericalmicroswimmersforallspeedsandsizes. Weseethedevelopmentofseveral
strategiesdependingonbodysize,notably,arun-and-tumbletypestrategywherecolloidswilleitherrotateortranslate
dependingonadecreaseorincreaseinconcentration,respectively. Interestingly,onceacolloidhaslearnedtorotate
eitherCWorCCW,itdoesnotchangethisdirectionatanypointduringtheruns.
18EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
1.0
5.0
4.5
0.8
4.0
3.5
0.6
3.0
0.4
2.5
2.0
0.2
1.5
1.0
0.0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure9: Theprobabilityofemergentchemotaxisfortheoblateparticlesalongwiththetheoreticalboundariesfromthe
Pe´cletnumbers.
19
1
s
htgnel
ydob
/
deepS
miwS
sixatomehC
tnegremE
fo
ytilibaborPEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
5.0 7000
4.5
6000
4.0
5000
3.5
4000
3.0
3000
2.5
2.0 2000
1.5
1000
1.0
0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure10: Rewardphasediagramcomputerfromtheoblatesimulations. Inthiscase,weseethebesttrainingemerges
againinthesmallerbutfastregionofthediagram. Whilethewidthofthecolloidsiscorrectedfor,morecomplex
geometricconditionsmaybeimpactingtheseresults.
14
10
12
Swim Speed
1.0 10 8 2.0
3.0
8 6 4.0 5.0
6 4
4
2
2
0
0
2
0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m Colloid Radius / m
Figure11: Policyefficacyfortheoblateparticlesappearsalmostidenticaltothesphericalparticles.
20
1
s
shtgnel
ydob
/
deepS
miwS
m
/ ecruoS
morf
ecnatsiD
muirbiliuqE
setunim
/ muminim
ot
emiT
gniniart
morf
drawer
latoTEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
Figure12: Emergentpolicydiagramfortheoblateparticles.
21EmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
1.0
5.0
4.5
0.8
4.0
3.5
0.6
3.0
0.4
2.5
2.0
0.2
1.5
1.0
0.0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure13: Theprobabilityofemergentchemotaxisfortheprolateparticlesalongwiththetheoreticalboundariesfrom
thePe´cletnumbers.
22
1
s
htgnel
ydob
/
deepS
miwS
sixatomehC
tnegremE
fo
ytilibaborPEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
14000
5.0
4.5 12000
4.0
10000
3.5
8000
3.0
6000
2.5
2.0 4000
1.5
2000
1.0
0
0.0 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m
Figure14: Rewardphasediagramcomputerfromtheprolatesimulations. Inthiscase,weseethebesttrainingemerges
againinthesmallerbutfastregionofthediagram. Whilethewidthofthecolloidsiscorrectedfor,morecomplex
geometricconditionsmaybeimpactingtheseresults.
14
10
12 Swim Speed
1.0
10 8 2.0
3.0
4.0
8 6 5.0
6 4
4 2
2
0
0
0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5
Colloid Radius / m Colloid Radius / m
Figure15: Policyefficacyfortheprolateparticlesappearsalmostidenticaltothesphericalparticles.
23
1
s
shtgnel
ydob
/
deepS
miwS
m
/ ecruoS
morf
ecnatsiD
muirbiliuqE
setunim
/
muminim
ot
emiT
gniniart
morf
drawer
latoTEmergenceofChemotacticStrategieswithMulti-AgentReinforcementLearning
Figure16: Emergentpolicydiagramfortheprolateparticles.
24