Retrieving Examples from Memory for Retrieval Augmented Neural
Machine Translation: A Systematic Comparison
MaximeBouthors♡,♣,JosepCrego♣,andFrançoisYvon♡
♡SorbonneUniversité,CNRS,ISIR,F-75005Paris,France
lastname@isir.upmc..fr
♣ChapsVision,4rueduPortauxVins,F-92150Suresnes,France
{mbouthors,jcrego}@chapsvision.com
Abstract jian et al. (2017) use a small set of examples to
performon-the-fly,lightweight,fine-tuning(using
Retrieval-AugmentedNeuralMachineTransla-
both source and target sides); Bulte and Tezcan
tion(RAMT)architecturesretrieveexamples
frommemorytoguidethegenerationprocess. (2019) simply concatenate the (target side of) a
While most works in this trend explore new handfulofexamplesonthesourcesideoftheen-
waystoexploittheretrievedexamples,theup- coder, leaving the rest of their autoregressive de-
streamretrievalstepismostlyunexplored. In coder unchanged; Xu et al. (2023) repurpose the
this paper, we study the effect of varying re-
edit-basedarchitectureofGuetal.(2019)tocom-
trieval methods for several translation archi-
putenewtranslationsfromexistingoneswithanon-
tectures,tobetterunderstandtheinterplaybe-
auto-regressive(NAT)decoder;finally,in-context
tweenthesetwoprocesses. Weconductexperi-
learning (ICL) in large language models (LLMs)
mentsintwolanguagepairsinamulti-domain
setting and consider several downstream ar- provides yet another way to seamlessly combine
chitecturesbasedonastandardautoregressive TMswithtextgeneration(seeMoslemetal.(2023),
model, an edit-based model, and a large lan- interalia).
guagemodelwithin-contextlearning. Ourex-
Thesestudies(andseveralothers,fullydiscussed
perimentsshowthatthechoiceoftheretrieval
in§5)notonlydifferinthewaytheyuseexamples
techniqueimpactsthetranslationscores,with
but also in the way TMs are searched, retrieved,
varianceacrossarchitectures. Wealsodiscuss
and filtered. This makes the direct comparison
the effects of increasing the number and di-
versityofexamples,whicharemostlypositive betweentheseproposalssometimesdifficulttore-
acrosstheboard. produceandanalyze. Furthermore,italsoprevents
preciselyassessingthecomputationalcomplexity
1 Introduction
ofthecompletetranslationpipeline.
Retrieval-AugmentedLanguageModelsandTrans- Inthispaper,weperformexperimentswithsev-
lationModelsaregettingalotoftraction(see(Li eral representative retrieval methods that we sys-
etal.,2022a)forarecentreview). Fortranslation tematicallycombinewithmultipleRAMTarchitec-
tasks, the use of retrieval-based techniques that tures. Indoingso,ourmaingoalisnottocompare
identifythemostrelevantsegment(s)inaTransla- thesedownstreamarchitectures,butrathertobetter
tion Memory (TM) has long been used in profes- understandtheinterplaybetweentheretrievaland
sionalComputerAidedTranslationenvironments generation tasks, to make the trade-offs between
(Bowker,2002),wheretheretrievedsegmentspro- thesestepsexplicit,andtoformulaterecommenda-
vide translators with valuable suggestions. Seg- tionsregardingfutureusesofTMsinNMT.
ments closely resembling the source sentence to Specifically,weaddressthefollowingquestions:
be translated can also be directly edited to speed
• Howmuchdoestheexampleselectionimpact
uptranslation. Suchideashavealsobeenusedin
translationperformance? Isoneretrievaltech-
Machine Translation (MT), first in the example-
niquealwaysbetterthantheothers,irrespec-
basedtradition(Nagao,1984;Somers,1999;Carl
tiveoftheMTarchitecture,orisitnecessary
etal.,2004),theninthestatistical-basedparadigm
toadapttheformertothelatter?
(KoehnandSenellart,2010),morerecentlyforneu-
ralmachinetranslation(NMT). • Do we need multiple examples? If so, what
There are several ways to take advantage of makes a good set of examples? Does the di-
translationexamplesinNMTarchitectures: Fara- versityofexampleshelp?
4202
rpA
3
]LC.sc[
1v53820.4042:viXra• By restricting retrieval to a restricted subset Pham et al., 2020; Vilar et al., 2023). Incre-
ofexamplesbasedonthesourcedomain,can mentalrankingorWeightedCoveragecanalso
weexpectbetterperformances, eventhough beusedtoenforcediversitywhenretrieving
the quality of the best retrieved match is de- multiplesamples(Chengetal.,2022;Agrawal
creased? etal.,2023;SiaandDuh,2023a).
Wenotablyfindthat(a)retrievalactuallymatters Inafinalselectionstep,onlythetop-kmostsimilar
foredit-basedandin-contextlearning;(b)existing candidatesareeventuallyretained. Dependingon
retrievalpipelinescanbesimplifiedatinference;(c) the choice of the filtering parameters, the actual
optimizingsourcecoverageand/orinstancediver- number of examples retrieved for a given query
sityishelping,especiallywhentheclosestmatch maybestrictlysmallerthank. Forsomedomains,
ispoor. retrievalmayevenreturnanemptylist.
2 RetrievalinNMTarchitectures 2.2 MeasuringRetrievalQuality
2.1 RetrievalPipeline Theeffectsofaretrievalstrategyareonlyobserved
onceatranslationmodelistrainedandevaluated.
Intheinformationretrieval(IR)framework,given
Asthisisanexcessivelycostlyprocess,weintro-
aqueryq,asetofdocumentsD isfilteredand/or
duceseveralaspectsthatdefineapriori thequality
ranked according to a retrieval score. The chal-
ofretrievedsimilarexamples. Forasingleexam-
lenge is to craft a score s(q,d) that will retrieve
ple d, this includes its semantic relatedness with
documents d that are relevant for a downstream
q orthelexicaloverlapwiththequery;d’slength
task. InMT,qisasourcesentence,andDisatrans-
also matters, as long examples may include irrel-
lationmemoryfromwhichwecanextractrelevant
evant words that can hurt translation (Xu et al.,
(source, target) pairs (d = (x,y)). The retrieval
2020),andincreasethecomputationalprocessing
processcanbedividedintothreesteps(Figure1):
cost. Now,lookingatsetsofexamples,wewould
1. Domain selection selects the corpus to re- also like them to cover most query words, while
trieveexamplesfrom,typicallybasedondo- remainingdiverseandshortonaverage.
main/genresimilarity; Toevaluatethesefacets,wecomputethefollow-
ingscoresforeachsetofsimilarexamples:
2. Filtering narrows down the set of relevant
examplesbasedonsuperficialcomparisonbe- • Coverage is the proportion of query tokens
tween the source query q and each example covered by the example tokens. It can be
d. defined in several ways (bag-of-word recall,
modifiedrecall1,n-wayalignmentscore2).
• Filtering can use a simple similarity
score to filter TM candidates based on • Relevanceistheproportionofcontributingto-
someminimalthreshold: BulteandTez- kensfromtheexampletokens(withthesame
can (2019) uses Jaccard similarity be- threeunderlyingdefinitionsascoverage). All
tweenbag-of-wordrepresentations;(Xu other words are deemed lexically irrelevant;
etal.,2020;Bouthorsetal.,2023)usean wereportanaverageoverexamples.
n-grammatchsimilarity;
• Filtering can also be controlled by the • Length is the average number of tokens of
specificationoftheindexingvocabulary, retrievedexamples.
whichtypicallyexcludesfrequentwords,
Inthenextparagraphs,wedescribehowthesequan-
therebyshorteningthelistofsimilardoc-
titiesarecontrolledduringretrieval.
uments(seeAppendixB.2).
2.3 SmoothedLongestCommonSubsequence
3. Rankingusesaretrievalscoresuchasn-gram
overlap (Xu et al., 2020), BM25 score (Gu The Levenshtein edit distance (LED) is widely
etal.,2018;Chengetal.,2022),editdistance usedastherankingfunctioninRAMT(see§2.1).
(ED)(BulteandTezcan,2019;Xuetal.,2020;
1Eachsourcetokencanonlybecoveredatmostonce.
Bouthors et al., 2023), cosine similarity be-
2Basedonanalignmentgraphbetweenexamplesandquery
tweenq andx’sembeddings(Xuetal.,2020; thatforbidsswapping,asdefinedbyBouthorsetal.(2023).Domain
Filtering Ranking
selection
Bilingual
IT Inverted S •i m Jail ca cr ait ry d score threshold alignment S •i m n-i gla rr ai mty ls ec no gr the Reranking
source Medical index • n-gram match length score • BM25 • Contrastive ...
(idf threshold) • IBM1-4
News • BM25 ... • Edit Distance ...
• LaBSE ...
in vs out vs all
Figure1: High-leveloverviewoftheretrievalpipelineinfuzzy-matching.
It counts the minimum number ∆(x,q) of word- ofremainingcandidateswiththefollowingterm:
leveloperations(deletion,insertion,replacement)
α (cid:88)
requiredtoeditxintoq andthennormalizes: LED(·,x), (2)
|M|
x∈M
∆(x,q)
LED(x,q) = 1− , (1) whereα > 0controlsthestrengthofthepenalty.
max(|x|,|q|)
2.5 IntegratingTMsinTranslation
Using LED mainly selects examples that are lex-
In our comparisons, we consider three NMT ar-
ically similar to the source. As it penalizes non-
chitectureswithvariants. Thefirst,calledNeural
matchingparts(inthenormalizer),itmaydiscard
Fuzzy Augmentation (NFA), implements the au-
longexamplescontaininggoodmatchesinasub-
toregressiveapproachofBulteandTezcan(2019),
string. Suchexamplescanstillberelevantifthey
withminorvariants. Thesecond,TM-LevT,isedit-
yieldahighcoverageofthequery.
based and mostly follows Xu et al. (2023), as re-
As an alternative to LED, setting the deletion
centlyextendedbyBouthorsetal.(2023)tohandle
costtozeroin(1)computestheLongestCommon
multiplematches. Thethirdisbasedonin-context
Subsequence(LCS)betweenxandq,whichmax-
learning (ICL) with large LMs, using the causal
imizes the coverage at the expense of relevance.
BLOOM LM (BigScience et al., 2022) and the
This also means that there is no penalization for
HuggingFace Transformer library3 to run the ex-
length,whichcanleadtolongandhard-to-exploit
periments. Weprovidefulldetailsregardingthese
examples. Weproposeasmoothedversion,namely
architecturesinAppendixC. Atahighlevel,the
δ-LCS, with a small non-zero deletion cost δ. δ-
mostimportantdistinctionisbetweentheautore-
LCSthusperformsatrade-offbetweencoverage,
gressive generative approaches of NFA and ICL,
relevance,andlength. DetailsareinAppendixA.
whichbothuseanenrichedcontextcomprisingthe
sourcesentenceandadditionalsourceand/ortarget
2.4 ControllingDiversity: Contrastive
matches,andTM-LevT,whichtriestoreuse,viaan
Retrieval
editingprocess,subpartsoftheretrievedmatches.
As is well known in the IR literature, it is unpro- Another major difference is between ICL, which
ductivetoretrievemultipleidenticalexamples. Di- inputsthesourcesideofmatchesfordecoding,and
versitycanhelpincreasecoveragewithouthurting the other two approaches, which do not need it
therelevanceofindividualexamples. ForRAMT, duringgeneration. Thisnotablyimpactsthecom-
asmallnumberofdiversitypreservingapproaches putationalcostofencodingthecontext.
have been proposed: Cheng et al. (2022) (Leven-
3 DataandMetrics
shtein distance), Agrawal et al. (2023) (n-gram
overlap) and Sia and Duh (2023b) (BM25) rely
3.1 Data
onaniterativealgorithminspiredbytheMaximal
We consider two translation directions, from En-
MarginalRelevance(MMR)criterionofGoldstein
glish to French (en-fr) and from German to En-
andCarbonell(1998). Inanutshell,thismeansthat
glish (de-en), and experiment with multiple do-
therankingscoringfunctionisiterativelyupdated
mains. This allows us to study a wide range of
to downgrade candidate examples that are either
settings, with a varying density of matches: our
too similar to already selected examples or that
datasetsincludeECB,EMEA,Europarl,GNOME,
coveralreadycoveredwords. Inourexperiments,
JRC-Acquis,KDE4,PHP,Ubuntu,OpenSubtitles,
wefollowChengetal.(2022),andafterselecting
|M|matchesinM,wepenalizetherankingscores 3https://github.com/huggingface/transformers.andKoran4 (statisticsinTables1and2). Foren-fr, data, which, however, cover fewer domains. In
these datasets reproduce the setting of (Xu et al., general,ourexperimentsaremorethouroughwith
2020). 5 Forde-en,wereusethedatapreparedby en-frdataasthislanguagepairwasusedtoselecta
KoehnandKnowles(2017)withthesplitofAha- subsetofinterestingconfigurationstobethenalso
roni and Goldberg (2020).6 The most favorable testedforde-en.
situation is to translate in a narrow domain with Asalastwordofcaution,weobservethatsome
large TMs, ensuring that multiple good matches domains are much easier to translate than others:
canbefound(e.g.JRC-AcquisandEMEA).How- JRC-Acquisisveryrepetitive,whichyieldsBLEU
ever,inanarrowdomain,theTMcansometimes scoresinthehigh70’s;NewsCommentary,onthe
be small (e.g., Ubuntu), and this is where other other hand barely achieves BLEU scores higher
relateddomainscanalsohelp. Ontheotherendof than20. Averagedresultsshouldbelookedatwith
thisspectrum,EuroparlorNews-Commentaryare care-onlyper-domainscorescantellthefullstory
thematicallyverydiverseandgoodmatchesmuch (AppendixE).
hardertofind.
ForeachdatasetD,wecomputeadensityscore 3.2 Metrics
based on the number of connected components
We report BLEU scores (Papineni et al., 2002)
(NCC) in a similarity graph Γ. Two translation computed by SacreBLEU (Post, 2018),9 as well
examplesarelinkedinΓiftheirsimilarityscore(1) asCOMET-2210 scores(Reietal.,2022)usingthe
isgreaterthan0.4:
officialimplementation. Additionally,weusethe
multi-referencesentenceBLEUscoresbetweenthe
1−NCC(Γ)
density(D) = 1− (3) target side of examples, and the translation out-
1−|D|
put11,averagedovercorpora,toevaluatethecopy
Inhigh-densitydomains,itisthuseasiertoretrieve rateofsystems,i.e.theirabilitytorecopysubparts
relevanttranslationexamples(seeTables1and2). oftheretrievedexamples.
Notethatthesedataarenotideal. First,forsome
3.3 ImplementationandParameters
domains,thecorrespondingdatamaybeincluded
in the very large corpora used to train LLMs. In We use in-house, open-source12 libraries that im-
ourexperimentswithBLOOM,whichistrainedon plementthevariousretrievalmethodsexploredin
the ROOTS corpus (Laurençon et al., 2022), this thispaper. Detailsregardingparametersettingsare
is the case for JRC-Acquis, Wikipedia, Europarl, inAppendixB.Fortranslationarchitectures,refer
TEDTalks(en-fr).7
toAppendixC.
Furthermore, the en-fr test sets have been se- Inourexperiments,wecontrastthreestrategies
lectedbasedontheexistenceofatleastoneclose for domain selection: in-domain, out-of-domain,
example in the same domain, using the standard no-selection. Regarding filtering (step 2 in Fig-
LEDtocomputesimilarities. Moreprecisely,the ure 1), we compare n-gram matching (NGM),
1,000instancesintest-0.6alwayshaveatleastone BM25, and no filter. NGM filters out examples
matchwithsimilaritygreaterthan0.6,fortest-0.4 unless they share a common n-gram g with the
the nearest match has a similarity comprised be- sourceq ofrelativelengthgreaterthanathreshold
tween0.4and0.6(detailsin(Xuetal.,2020)).8 |g|
τ (e.g. ≥ τ). AsforBM25,weonlyretainthe
|q|
This design allows us to focus on the effect of
LbestBM25candidates.
retrievalquality(mediumvshigh-scoringmatches)
Finally,regardingranking,wecomparevarious
on translation scores. It however yields absolute
definitionsoftheeditdistance(ED)(see§2.3)with
scores that do not compare with what would be
BM25.
obtainedwithafullyrandomizedselectionprocess.
For a more realistic evaluation, we use the de-en Computationalissues Inourexperimentsbelow,
wemostlyanalyzeretrievalresults. However,note
4ThesedatacanbedownloadedfromtheOPUSwebsite
(https://www.opus.nlpl.eu)(Tiedemann,2012). thateachretrievalpipelineyieldsspecifictraining
5Splitsfromhttps://github.com/jitao-xu/tm-levt.
6Thisisthetest-detestset. 9signature: nrefs:1|case:mixed|eff:no|tok:13a|
7Forthesedomains,theICLscoreshavebeendisregarded. smooth:exp|version:2.1.0;
8Asweuseourownreimplementationofeditdistances, 10Unbabel/wmt22-comet-da
wehaveobservedrarecaseswheretheseconditionswerenot 11Brevitypenalty(BP)isremoved.
exactlymet. 12https://github.com/SYSTRAN/fuzzy-matchdomain ECB EME Epp GNO JRC KDE News PHP TED Ubu Wiki all
size 195k 373k 2.0M 55k 503k 180k 151k 16k 159k 9k 803k 4.4M
avglength 29.2 16.7 26.6 9.4 28.8 10.5 26.4 14.5 17.7 5.2 19.6 18.6
density% 87.1 96.9 49.3 85.96 86.76 84.18 11.60 63.59 53.78 16.89 55.25 62.8
Table1: Numberofsamples,averagelengthoftokenizedsentences,densityfortrainingsets(en-fr).
domain it kor law med sub all 4.2.1 ArchitecturesComparison
size 223k 18k 467k 248k 500k 1.46M
Neural fuzzy augmentation We observed, in
meanlength 9.6 20.4 28.0 15.5 8.1 16.3
density% 53.2 51.4 58.6 69.5 74.2 61.4 preliminaryresults,thatNFAisinsensitivetothe
retrieval setting at training time. We only report
Table 2: Number of samples, average length of tok-
resultsforamodeltrainedonthebaselinesetting
enizedsentences,densityfortrainingsets(de-en).
NGM+LED(τ = 0.3),thenusedininferenceina
filter-freesetting.
andinferencecomputationalcosts. Domainselec-
ranker test-0.4 test-0.6 test-de
tion always speeds up the subsequent steps, with
NGM+LED1-1 55.1 64.3 -
averystrongimpactwhenthetargetdomainsare NGM+LED3-1 54.8 63.9 -
small. Regarding filtering, NGM has an algorith- NGM+LED3-2 54.8 64.2 -
mic complexity
O(ℓ¯log(cid:0) nℓ¯(cid:1)
) for a single query
NGM+LED3-3 54.9/44.6 64.3/45.7 41.6
BM25 54.7/44.6 64.2/45.6 -
using suffix array – with
ℓ¯
the average sentence BM25
c
54.7/44.6 64.2/45.6 -
lengthandntheTMsize–whereasBM25’scom- LED 54.9/44.7 64.4/45.7 41.7
δ-LCS 54.8/44.6 64.3/45.7 41.9
plexityisO(n|q|). Finally,regardingranking,ED
δ-LCS
c
54.8/44.6 64.3/45.7 41.8
calculationtakes(O(nℓ¯|q|)),soagain,linearw.r.t.
nforonequery. Table 4: Average BLEU (/COMET(×100)) scores for
en-fr (11 domains) and de-en (5 domains) using
NFAmodels. k -k inNGM+LEDdenotesamodel
4 Experiments t i
trainedwithk examples,whileinferenceusesk ;
t i
denotescontrastiveranking.
4.1 ComparingRetrievalTechniques c
Wefirstmeasurehowmuchachangeintheretrieval ResultsinTable4areveryconsistentandhardly
technique actually affects the instances that are varyacrossdomains(seeTable13inAppendixE)
eventuallyretrieved. Forthis,wecomputebag-of- andlanguagepairs. Thisisafirstimportantresult
wordcoverage,relevance,andlength(introducedin that somehow consolidates observations already
§2.2). WecompareabaselineNGMfilterusingthe performedforthismodel,whichseemstoberobust
LEDranker,asusedinBouthorsetal.(2023),with withrespecttovariationsintheretrievalstrategy.
filter-freepipelines. Thecorrespondingresultsfor
Edit-basedtechniques Regardingedit-basedap-
alltestsetsareinTable3. LEDyieldsthehighest
proaches,wetrainMulti-LevenshteinTransformer
relevance, while δ-LCS and contrastive ranking
(TM3-LevT)onthesamedataset,comparingasetof
yield a higher coverage. Overall, changing the
retrievalsettingsandbothNGMandBM25filters.
retrievaltechniquedoesimpactthesetofinstances
WereportthefollowingresultsinTable5:
thatareusedintrainingandinference.
• ThesettingusedintheoriginalTM3-LevTpa-
4.2 InteractionsBetweenRetrievaland per: NGM+LED (τ = 0.3) both at training
Translation andinferencetime.
Inthissection,welookattheinteractionsbetween
• The best-performing train and inference set-
retrieval and translation and systematically vary
tingpairs,asidentifiableinAppendixD,for
theretrievalcomponentforthethreearchitectures
NGMandBM25filtersseparately.
of §2.5. Notably, we compare two filters (NGM
andBM25)duringtrainingandalsocontrastwith • Weevaluateourbestoveralltrainingpipeline
afilter-freeversionininference. Wealsovarythe (BM25+LED)onafilter-freesetupwithvary-
editcostsandthenumberofretrievedexamples. ingEDcosts(usingδ = 0.1).filter NGM(τ =0.3) - - - - - -
ranker LED LED LED δ-LCS δ-LCS BM25 BM25
Contrast - - α=0.3 - α=0.3 - α=0.3
coverage 48.1 65.6 38.1 58.2 68.9 57.0 60.8 70.5 - 63.8 71.9 62.5 65.8 73.5 64.4 62.2 70.0 - 62.3 70.0 -
relevance 40.3 53.5 31.4 44.7 53.5 43.7 44.1 53.3 - 39.8 49.4 40.1 36.6 44.7 36.6 42.4 48.9 - 42.4 48.9 -
length 15.7 15.2 15.8 16.4 15.4 16.9 16.5 15.4 - 24.8 19.7 26.9 27.0 21.8 29.0 20.1 19.0 - 20.1 19.0 -
Table3: Retrievalscoresaveragedoverdomainsfortripletstest-0.4,test-0.6,test-de. δ =0.1
filter+ranker test-0.4 test-0.6 test-de ranker k-shot test-0.4 test-0.6 test-de
NGM+LED 43.9/26.2 56.0/47.8 29.4 LED 1 45.8/30.5 50.0/30.7 -
bestNGM+EDpair 45.5/32.8 56.9/49.7 - LED 3 47.8/35.3 51.5/35.1 33.3
bestBM25+EDpair 45.7/31.0 57.1/49.9 - LED c 3 48.3/37.5 52.0/37.3 -
LED (k=1) 44.9/29.0 55.7/46.6 - δ-LCS 3 48.1/36.0 51.6/36.3 33.7
LED (k=2) 45.2/29.1 56.7/48.5 - δ-LCS c 3 48.2/36.4 51.7/36.8 33.9
LED 45.6/31.3 57.3/50.7 33.4 BM25 1 45.8/28.7 49.6/27.1 -
δ-LCS 45.9/31.4 57.4/50.5 33.9 BM25 3 48.1/35.0 51.8/34.8 -
δ-LCS c 46.0/31.3 57.2/49.4 33.7 BM25 c 3 48.1/34.8 51.4/35.2 -
Table5: AverageBLEU(/COMET(×100))scoresacross Table6: AverageBLEU/COMET(×100)scoresaveraged
all11(en-fr)or5(de-en)domainsusingTM3-LevT. accross 7 en-fr domains (test-0.4/6) and 5 de-en
domains(test-de)forICL(k-shot)forseveralfilter-
freeretrievalsetups. denotescontrast;δ = 0.1.
c
Weobservehereastrongerimpactofretrievalon
thedownstreamscores,withalargegainoverthe
baseline(fortest-0.4anden-de). 0.1-LCSslightly whichsupposesthatthemodel-agnosticmetricsare
outperformsLEDinmostconditionsandmetrics, independentofBLEU,hasanerrorof1.2. Cover-
withaverysmalldifferencebetweenthecontrastive age, relevance, and length have respective coeffi-
andnon-contrastiveversionsoftheranking. cientsof0.13,0.09,and0.03. Thishighlightsthe
importanceofcoverageandrelevancemeasuresin
In-contextlearning WeevaluateBLOOMink-
theexplanationofBLEUperformance. AsforICL,
shottranslationfork = 1and3withtworankers:
theconstantmodelhasthesameaverageresidual
ED and BM25.13 The retrieval scope is always
error (0.28 against 0.26), with respective coeffi-
“in-domain”. We do not use any filter to ensure
cients of 0.04, 0.03, and 0.00. Thus, ICL seems
the retrieval of exactly k examples for each test
morerobusttochangesintheretrievedexamples.
instance.14 Results in Table 6 show again small
differences between retrieval techniques, with a Copyinginputtokens Thecopyrate,introduced
positiveeffectofcontrastiverankingpolicy,which in§3.2measureshowmuchthetranslationmodel
yields the best results. For this architecture, we exploits slices from the examples to produce its
also notethat retrievingat least very good match output. Pipelineswithhighcoveringexamplessys-
(e.g.test-0.6)doesnotnecessarilyimplyveryhigh tematicallyimplyahighercopyrate. Wefindthat
BLEUscores,contrarilytoNFAandTM-LevT. copy rate is correlated with higher BLEU scores
for TM3-LevT and ICL; in contrast NFA fails to
4.3 ComplementaryAnalyses
producehigherBLEUscoreswithincreasingcopy
What makes a good set of examples? We use rate.
alinearmodelandtrytopredictBLEUscoresus- Alsonotethat,eventhoughitreliesonexplicit
ingtheretrievalmetrics(coverage,relevance,and edits,TM3-LevTalwayshasalowercopyratethan
length)of§2.2forTM3-LevTandICLw.r.t.cover-
NFA or ICL; the latter notably has the highest
age, relevance, and length. First, with TM3-LevT, copy rate for test-0.6 and also the worst transla-
thelinearmodelhasanaveragesquaredresiduals tion scores, suggesting that too many irrelevant
of0.2BLEU.Ontheotherhand,aconstantmodel, tokensarekeptintheoutput.
13Wealsoconsidera“random”rankingpolicyforcontrast,
Domain Filtering Relaxing the constraint that
whichyieldscomparativelyverypoorresultsthatareabout
15BLEUpointsbelowtheothersfork = 3. Thisconfirms similarexamplesshouldberetrieved“in-domain”
thefindingsofMoslemetal.(2023,Table1,p.235)onthe increases the retrieval rate. However, it turns out
benefitsofTM-basedretrieval.
tobedetrimentalforallarchitectures: resultsare
14Insomesituationshowever,wecouldnotfind3candi-
dateswithascoregreaterthan0. inTable8,wherewecomparein-domainretrievaltest-0.4 test-0.6 test-de maturelydiscardusefulexamples,especiallywhen
NFA
usingcontrastiveranking. Toevaluatethis,weturn
BM25 47.0 62.1 -
offfilteringfortestsamplesduringinference. This
BM25 46.3 61.3 -
c
LED 43.4 60.8 38.2 simplificationofthepipelineimprovesthescores
δ-LCS 46.7 62.8 41.0 forTM3-LevT,whilethereisnoeffectforNFA(see
δ-LCS 47.5 63.8 41.3
c linesforfilteringfreeinferenceinTables4and5).
TM3-LevT
Thus,fortheformermethodatleast,atrade-offcan
NGM+LED 37.4 56.3 30.4
bemadebetweenlatencyandtranslationscores.
bestBM25+EDpair 42.4 57.8 -
LED 40.7 56.9 37.2
δ-LCS 42.8 58.3 39.5
Increasing the number of examples We vary
δ-LCS 43.0 58.4 39.4
c
ICL k-shot k, the number of TM examples retrieved, from 1
LED 1 40.5 53.5 - to3. Overall,weobserveagain(BLEU/COMET)
LED 3 51.8 64.7 45.0 when k increases. For ICL, this is already clear
LED 3 53.7 65.9 -
c from the results in Table 6 where 3-shots clearly
δ-LCS 3 54.1 66.1 47.2
δ-LCS 3 54.3 65.2 48.1 outperforms1-shot. Wegetasimilarconclusionfor
c
BM25 1 53.9 66.1 - TM3-LevTbasedontheresultsinTable5,wherewe
BM25 3 54.1 66.1 -
vary the inference procedure for a model trained
BM25 3 54.9 67.0 -
c
on (BM25+LED). The test retrieval is filter-free
Table7: Averagecopyrateofallthreemodels. LED with either exactly k = 1, 2, or 3 retrieved
examples.
withall-domainsandout-of-domainretrievals. AsforNFA,amodeltrainedonupto3instances
slightlybenefitsfrommoreexamplesbutdoesnot
domain NFA TM3-LevT ICL
compete with a model using only the one-best
In 54.9/64.4 45.6/57.3 47.8/51.5
match in training and inference. This seems to
All 52.5/62.3 43.8/55.4 45.1/49.0
Out 45.5/51.2 30.2/33.5 30.7/29.6 contradictBulteandTezcan(2019),whoclaimthe
superiorityofusingmoreexamples.
Table8: AverageBLEUscores(test-0.4/test-0.6,en-
fr) according to the domain selection strategy, using
filter-freeLEDasranker. Optimizing for coverage with δ-LCS By de-
sign, δ-LCS retrieves examples having a higher
coverage of the source than LED, which turns
The impact of the “all-domain” policy is gen-
into higher copy rates for all architectures. For
uine: whenthispolicyisenforced,themostsimilar
ICL and TM3-LevT, it yields similar BLEU gain
examples are found out-of-domain for 35.1% of
(ICL: +0.3/+0.1/+0.4; TM3-LevT: +0.3/+0.1/+0.5
our22ktestsamples. Themostimpacteddomains
onresp. test-0.4,test-0.6,test-de). Theanalysisin
areUbuntu(82.8%matchesareout-of-domain)and
AppendixDshowsaconsistentbenefitofδ-LCS
NewsCommentary(79.7%). Theper-domainanal-
forTM3-LevTwhencoupledwithfiltersattraining
ysis(AppendixE)howevershowsthatthisisdetri-
mentalforUbuntu(-6.1/-6.7BLEUfortest-0.4/test- (NGM)andinferencetime(NGMandBM25).
0.6),andneutralforNewsCommentary(-0.2/+0.2
BLEU).Asexpected,enforcingan“out-of-domain” Enforcing diversity with contrastive ranking
selectionconstraintyieldsdramaticlossesinBLEU Weobservethatusingacontrastiverankerismostly
(-15.4/-23.8BLEU). beneficial for medium-scoring similar examples
These results confirm the benefit of retrieving (test-0.4),regardlessofthearchitecture. Itcaneven
“in-domain”,evenforsmalldomains: notonlydoes bedetrimentalwhenatleastonehigh-matchingex-
itgreatlyspeedupretrieval,butitalsoyieldsbetter ampleisfound. Thisisbecausecontrastiveranking
examplesand,ultimately,highertranslationscores. generateslesssimilarexamplesthatarenotneces-
sarilyrelevant. Incomparison,fortest-0.4,increas-
SimplifyingtheRetrievalPipeline Forlargedo-
ing the diversity in retrieval seems beneficial, as
mains, removingthefilteringstepintheretrieval
it increases the coverage of the source. This sug-
pipelineconsiderablyincreasesthecomputational
gests that contrastive methods should adapt their
cost,especiallyduringtraining.15 Yet,itmaypre-
strengthparameter(αin(2))totheretrievalscores,
15Thecomplexityisquadraticw.r.t.totheTMsize. enforcingmorediversitywhenmatchesarepoor.5 RelatedWork useoflargelanguagemodels,which,providedwith
suitablepromptsandin-contextexamples,canbe
Asforothertextgenerationapplications(Lietal., turnedintoeffectivetranslationsystems. Suchap-
2022b),effortstointegratearetrievalcomponentin proacheshavebeentestedwithmostLLMs,with
NMThaveintensifiedinrecentyears. Onemotiva- thegoaltoillustratethemulti-taskingabilitiesof
tionistoincreasethetransparencyofMLmodels suchmodels. Closertoourwork,aseriesofwork
byprovidinguserswiththeretrievedexamplesthat havetriedtooptimizeLLMsperformanceforthe
were used to compute the actual output (Rudin, MTtask,systematicallystudyingtheeffectofthe
2019). For MT, this is achieved by integrating prompt change, of the number of shots, and of
fuzzymatchesretrievedfrommemoryasanaddi- thein-contextexamplesselectionprocedures(Vilar
tionalcontext. Thiscanbeperformedbyconcate- etal.,2023;Zhangetal.,2023;Hendyetal.,2023;
nating the retrieved target instance to the source Bawden and Yvon, 2023). Moslem et al. (2023)
text,anapproachthatalsoaccommodatesseveral werethefirsttocombineLLMswithTMs, using
TM matches (Bulte and Tezcan, 2019), or by the anembedding-basedretrievalsystemandcombin-
simultaneous use of their source and target sides ing(viaconcatenation)upto5TM-matchesinthe
(Pham et al., 2020; Reheman et al., 2023). More MTprompt;(Muetal.,2023)followedsuit,with
complexschemestocombineretrievedexamples adifferentLLMandatwo-stageretrievalstrategy
withthesourcesentencearein(Guetal.,2018;Xia (first 500 closest matches for a Lucene-based en-
etal.,2019;Heetal.,2021b). Therecentstudiesof gine; then using up to 9 closest matches for the
Chengetal.(2022);Agrawaletal.(2023)andSia edit-distance). (Agrawaletal.,2023)studiesaway
and Duh (2023b) handle several complementary tooptimallyselectk examplessoastomaximize
TM examples retrieved in a contrastive manner coverage,anapproachakintoour"contrastive”sce-
thataimstoenhancesourcecoverage. Guptaetal. nario–usingaBM25retrieverinafirststage,and
(2023)proposeageneralformulationintheappli- agreedyheuristicselectioninasecondstage. (Sia
cation of ICL in various tasks. Cai et al. (2021) andDuh,2023b)exploresanotherbenefitofselect-
also handle multiple matches and introduce two inggoodin-contextexamples,thatofmaintaining
novelties: (a)retrievalisperformeddirectlyinthe consistency in the generated text - for this, they
targetlanguageand(b)similarityscoresaretrain- retrieveexamplesfromamovingcontextwindow
able,whichallowstoevaluateretrievedinstances ofpasttranslations. Finally,Metal.(2023)goone
based on their usefulness in translation. Most of stepfurtherbytrainingalinearregressionmodel
theseattemptsrelyonauto-regressive(AR)decod- predictingthegoodnessofTMinstancesbasedon
ing,meaningthattheimpactofTMmatch(es)on asmallsetoffeatures.
theoutputisonlyindirect.
TheuseofTMmemorymatchwithaNATde- 6 Conclusion
coder is studied by Niwa et al. (2022); Xu et al.
(2023);Zhengetal.(2023),whoadaptLevTforthis Thispaperhasinvestigatedtheeffectofvaryingthe
specificsetting,usingonesingleretrievedinstance retrievalstrategyforthreecommonlyusedretrieval-
toinitializetheedit-baseddecoder;(Bouthorsetal., augmentedmachinetranslationarchitectures,try-
2023) extends this technique to process multiple ing to get a better understanding of the interplay
retrievedexamples. Zhangetal.(2018)explorea of these two components. While auto-regressive
different set of techniques to improve translation encoder-decoder architecture seems quite robust
usingretrievedsegmentsinsteadoffullsentences. w.r.t.changesintheretrievalstrategy,thisislessso
Generalizing nearest neighbor language models forthetwootherarchitectures,forwhichoptimiz-
(NNLMs) (He et al., 2021a) to conditional LMs, ingtheretrievalpolicycanyieldsignificantreturns.
Khandelwaletal.(2021)performk-NNMTasfol- Ourexperimentshavealsohighlightedthebenefits
lows: ateachdecodingstep,thek targetcontexts of coverage-oriented retrieval policies, based on
thatclosesttothecurrentcontextualizedrepresenta- LCS,especiallyforthenon-autoregressivemodel.
tionsareretrievedandusedtoselectthenexttoken. Finally, we have validated the use of the “in-
Thisapproachisfurtherelaboratedin(Zhengetal., domain”selectionpolicyandproposedtosimplify
2021;Mengetal.,2022)andextendedtochunks theinferencestepbyeliminatingthefilteringpro-
byMartinsetal.(2022). cess,yieldingbetterperformanceattheexpenseof
A final thread of relevant papers concerns the anincreasedlatency.In our future work, we would like to continue Linguistics: ACL2023,pages8857–8873,Toronto,
the exploration of the interplay between retrieval Canada.AssociationforComputationalLinguistics.
and translation, with the aim to jointly optimize
RoeeAharoniandYoavGoldberg.2020. Unsupervised
thesetwoprocessesratherthanhavethemdesigned domainclustersinpretrainedlanguagemodels. In
independently. Proceedingsofthe58thAnnualMeetingoftheAsso-
ciationforComputationalLinguistics,pages7747–
7 Limitations 7763, Online. Association for Computational Lin-
guistics.
In this paper, we have focused on purely trans-
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAl-
ductive techniques, meaning that all inference is
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
performedwithafrozennetwork-thislimitation MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,
certainlyneedstobereconsidered,andourresults JulienLaunay,QuentinMalartic,DanieleMazzotta,
would be stronger with additional comparisons BadreddineNoune,BaptistePannier,andGuilherme
Penedo.2023. Thefalconseriesofopenlanguage
with e.g., on-the-fly fine-tuning (Farajian et al.,
models.
2017)orlow-rankadaptationtechniques(Huetal.,
2022). DuarteM.Alves,JoséPombal,NunoM.Guerreiro,Pe-
droH.Martins,JoãoAlves,AminFarajian,BenPe-
WehavechosentouseonlyonelargeLLM,with
ters,RicardoRei,PatrickFernandes,SwetaAgrawal,
176b million parameters. This was motivated by
PierreColombo,JoséG.C.deSouza,andAndréF.T.
(a)theopennessofthemodelandthetransparency Martins.2024. Tower: Anopenmultilinguallarge
ofthetrainingdata,whichallowedustocontrolfor languagemodelfortranslation-relatedtasks.
testsamplesoccurringalsointhetraining;(b)the
RachelBawdenandFrançoisYvon.2023. Investigating
existence of multiple previous experiments with thetranslationperformanceofalargemultilingual
this model, which allowed us to get a reasonable languagemodel: thecaseofBLOOM. InProceed-
ingsofthe24thAnnualConferenceoftheEuropean
idea of its basic translation abilities. More re-
AssociationforMachineTranslation,pages157–170,
cent,smaller,andarguablybettermodels(e.g.,the
Tampere,Finland.EuropeanAssociationforMachine
LLAMA (Touvron et al., 2023) and Falcon fami- Translation.
lies)(Almazroueietal.,2023)withvariouslevels
WorkshopBigScience,:,TevenLeScao,AngelaFan,
ofmultilingualsupport(Alvesetal.,2024),would
ChristopherAkiki,ElliePavlick,SuzanaIlic´,Daniel
likely yield a more faithful picture of the current
Hesslow, Roman Castagné, Alexandra Sasha Luc-
performanceofin-contextlearningwithLLMs. cioni, François Yvon, Matthias Gallé, Jonathan
Ourdiscussionhasfocusedonmeasuresoftrans- Tow, Alexander M. Rush, Stella Biderman, Albert
Webson,PawanSasankaAmmanamanchi,Thomas
lationquality; inpracticalapplications, computa-
Wang,BenoîtSagot,NiklasMuennighoff,AlbertVil-
tionalcostsassociatedwithaspecificcombination
lanovadelMoral,OlatunjiRuwase,RachelBawden,
ofretrievalandarchitecturealsomatter. Whilewe Stas Bekman, Angelina McMillan-Major, Iz Belt-
have tried to be explicit about the complexity of agy,HuuNguyen,LucileSaulnier,SamsonTan,Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Laurençon,
eachretrievalalgorithm,wehaveleftasideissues
Yacine Jernite, Julien Launay, Margaret Mitchell,
relatedtoidentifyingtheoptimalcomputation/per-
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor
formancetradeoffs. Soroa,AlhamFikriAji,AmitAlfassy,AnnaRogers,
ArielKreisbergNitzav,CanwenXu,ChenghaoMou,
8 Acknowledgements Chris Emezue, Christopher Klamm, Colin Leong,
DanielvanStrien,DavidIfeoluwaAdelani,Dragomir
ThisworkwasperformedusingHPC/AIresources Radev, Eduardo González Ponferrada, Efrat Lev-
fromGENCI-IDRIS(Grants2022-AD011013583 kovizh, Ethan Kim, Eyal Bar Natan, Francesco
De Toni, Gérard Dupont, Germán Kruszewski, Gi-
and 2023-AD010614012). The third author was
adaPistilli,HadyElsahar,HamzaBenyamina,Hieu
alsopartlyfundedbytheFrenchNationalResearch
Tran,IanYu,IdrisAbdulmumin,IsaacJohnson,Itziar
Agency (ANR) under grant ANR-23-IAS1-0006 Gonzalez-Dios,JavierdelaRosa,JennyChim,Jesse
(TraLaLam). Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,
Joseph Tobing, Joydeep Bhattacharjee, Khalid Al-
mubarak,KimboChen,KyleLo,LeandroVonWerra,
Leon Weber, Long Phan, Loubna Ben allal, Lu-
References
dovicTanguy,MananDey,ManuelRomeroMuñoz,
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Maraim Masoud, María Grandury, Mario Šaško,
Zettlemoyer, andMarjanGhazvininejad.2023. In- Max Huang, Maximin Coavoux, Mayank Singh,
contextexamplesselectionformachinetranslation. Mike Tian-Jian Jiang, Minh Chien Vu, Moham-
In Findings of the Association for Computational madA.Jauhar,MustafaGhaleb,NishantSubramani,NoraKassner,NurulaqillaKhamis,OlivierNguyen, Qiu, Muhammed Ghauri, Mykola Burynok, Nafis
Omar Espejel, Ona de Gibert, Paulo Villegas, Pe- Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,
ter Henderson, Pierre Colombo, Priscilla Amuok, Olanrewaju Samuel, Ran An, Rasmus Kromann,
QuentinLhoest,RhezaHarliman,RishiBommasani, RyanHao,SamiraAlizadeh,SarmadShubber,Silas
Roberto Luis López, Rui Ribeiro, Salomey Osei, Wang,SouravRoy,SylvainViguier,ThanhLe,Tobi
Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Oyebade,TrieuLe,YoyoYang,ZachNguyen,Ab-
ShamsuddeenHassanMuhammad,ShanyaSharma, hinav Ramesh Kashyap, Alfredo Palasciano, Al-
ShayneLongpre,SomaiehNikpoor,StanislavSilber- ison Callahan, Anima Shukla, Antonio Miranda-
berg, Suhas Pai, Sydney Zink, Tiago Timponi Tor- Escalada,AyushSingh,BenjaminBeilharz,BoWang,
rent,TimoSchick,TristanThrush,ValentinDanchev, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin
Vassilina Nikoulina, Veronika Laippala, Violette Xu, Clémentine Fourrier, Daniel León Periñán,
Lepercq,VrindaPrabhu,ZaidAlyafeai,ZeerakTa- DanielMolano,DianYu,EnriqueManjavacas,Fabio
lat,ArunRaja,BenjaminHeinzerling,ChengleiSi, Barth, Florian Fuhrimann, Gabriel Altay, Giyased-
Davut Emre Tas¸ar, Elizabeth Salesky, Sabrina J. dinBayrak,GullyBurns,HelenaU.Vrabec,Imane
Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Bello,IshaniDash,JihyunKang,JohnGiorgi,Jonas
Santilli,AntoineChaffin,ArnaudStiegler,Debajy- Golde,JoseDavidPosada,KarthikRangasaiSivara-
otiDatta,ElizaSzczechla,GunjanChhablani,Han man,LokeshBulchandani,LuLiu,LuisaShinzato,
Wang,HarshitPandey,HendrikStrobelt,JasonAlan Madeleine Hahn de Bykhovetz, Maiko Takeuchi,
Fries,JosRozen,LeoGao,LintangSutawika,MSai- MarcPàmies,MariaACastillo,MariannaNezhurina,
fulBari,MagedS.Al-shaibani,MatteoManica,Ni- MarioSänger,MatthiasSamwald,MichaelCullan,
hal Nayak, Ryan Teehan, Samuel Albanie, Sheng MichaelWeinberg,MichielDeWolf,MinaMihalj-
Shen,SrulikBen-David,StephenH.Bach,Taewoon cic,MinnaLiu,MoritzFreidank,MyungsunKang,
Kim,TaliBers,ThibaultFevry,TrishalaNeeraj,Ur- NatashaSeelam,NathanDahlberg,NicholasMichio
mishThakker,VikasRaunak,XiangruTang,Zheng- Broad, Nikolaus Muellner, Pascale Fung, Patrick
XinYong,ZhiqingSun,ShakedBrody,YallowUri, Haller, Ramya Chandrasekhar, Renata Eisenberg,
HadarTojarieh,AdamRoberts,HyungWonChung, RobertMartin,RodrigoCanalli,RosalineSu,Ruisi
JaesungTae,JasonPhang,OfirPress,ConglongLi, Su,SamuelCahyawijaya,SamueleGarda,ShlokS
DeepakNarayanan,HatimBourfoune,JaredCasper, Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-
JeffRasley,MaxRyabinin,MayankMishra,Minjia monOtt,SineeSang-aroonsiri,SrishtiKumar,Ste-
Zhang,MohammadShoeybi,MyriamPeyrounette, fan Schweter, Sushil Bharati, Tanmay Laud, Théo
Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis
PatrickvonPlaten,PierreCornette,PierreFrançois Labrak,YashShaileshBajaj,YashVenkatraman,Yi-
Lavallée,RémiLacroix,SamyamRajbhandari,San- fanXu,YingxinXu,YuXu,ZheTan,ZhongliXie,Zi-
chitGandhi,ShadenSmith,StéphaneRequena,Suraj fanYe,MathildeBras,YounesBelkada,andThomas
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Wolf.2022. BLOOM:a176b-parameteropen-access
Singh, Anastasia Cheveleva, Anne-Laure Ligozat, multilinguallanguagemodel.
ArjunSubramonian,AurélieNévéol,CharlesLover-
ing,DanGarrette,DeepakTunuguntla,EhudReiter, Maxime Bouthors, Josep Crego, and François Yvon.
EkaterinaTaktasheva,EkaterinaVoloshina,EliBog- 2023. Towards example-based NMT with multi-
danov,GentaIndraWinata,HaileySchoelkopf,Jan- Levenshtein transformers. In Proceedings of the
ChristophKalo,JekaterinaNovikova,JessicaZosa 2023ConferenceonEmpiricalMethodsinNatural
Forde, JordanClive, JungoKasai, KenKawamura, LanguageProcessing,pages1830–1846,Singapore.
LiamHazan,MarineCarpuat,MirunaClinciu,Na- AssociationforComputationalLinguistics.
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
LynneBowker.2002. Computer-aidedtranslationtech-
Zhang,SebastianGehrmann,ShacharMirkin,Shani
nology: Apracticalintroduction. UniversityofOt-
Pais,TatianaShavrina,ThomasScialom,TianYun,
tawaPress.
TomaszLimisiewicz,VerenaRieser,VitalyProtasov,
VladislavMikhailov,YadaPruksachatkun,Yonatan
Belinkov,ZacharyBamberger,ZdeneˇkKasner,Al- BramBulteandArdaTezcan.2019. Neuralfuzzyre-
iceRueda,AmandaPestana,AmirFeizpour,Ammar pair: Integratingfuzzymatchesintoneuralmachine
Khan, Amy Faranak, Ana Santos, Anthony Hevia, translation. InProceedingsofthe57thAnnualMeet-
AntigonaUnldreaj,ArashAghagol,ArezooAbdol- ingoftheAssociationforComputationalLinguistics,
lahi,AychaTammour,AzadehHajiHosseini,Bahareh pages 1800–1809, Florence, Italy. Association for
Behroozi, Benjamin Ajibade, Bharat Saxena, Car- ComputationalLinguistics.
losMuñozFerrandis,DanishContractor,DavidLan-
sky,DavisDavid,DouweKiela,DuongA.Nguyen, Deng Cai, Yan Wang, Huayang Li, Wai Lam, and
Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fa- LemaoLiu.2021. Neuralmachinetranslationwith
timaMirza,FranklineOnoniwu, HabibRezanejad, monolingual translation memory. In Proceedings
HessieJones,IndraniBhattacharya,IreneSolaiman, of the 59th Annual Meeting of the Association for
IrinaSedenko,IsarNejadgholi,JessePassmore,Josh ComputationalLinguisticsandthe11thInternational
Seltzer,JulioBonisSanz,LiviaDutra,MaironSama- JointConferenceonNaturalLanguageProcessing
gaio,MaraimElbadri,MargotMieskes,MarissaGer- (Volume1: LongPapers),pages7307–7318,Online.
chick, Martha Akinlolu, Michael McKenna, Mike AssociationforComputationalLinguistics.MichaelCarl,AndyWay,andWalterDaelemans.2004. EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-
Recentadvancesinexample-basedmachinetransla- Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
tion. ComputationalLinguistics,30:516–520. Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
XinCheng,ShenGao,LemaoLiu,DongyanZhao,and
LearningRepresentations.
Rui Yan. 2022. Neural machine translation with
contrastive translation memories. In Proceedings
UrvashiKhandelwal,AngelaFan,DanJurafsky,Luke
ofthe2022ConferenceonEmpiricalMethodsinNat-
Zettlemoyer,andMikeLewis.2021. NearestNeigh-
uralLanguageProcessing,pages3591–3601,Abu
borMachineTranslation. InProceedingsoftheInter-
Dhabi,UnitedArabEmirates.AssociationforCom-
nationalConferenceonLearningRepresentations.
putationalLinguistics.
PhilippKoehnandRebeccaKnowles.2017. Sixchal-
M. Amin Farajian, Marco Turchi, Matteo Negri, and
lengesforneuralmachinetranslation. InProceedings
MarcelloFederico.2017. Multi-domainneuralma-
oftheFirstWorkshoponNeuralMachineTranslation,
chinetranslationthroughunsupervisedadaptation. In
pages28–39,Vancouver.AssociationforComputa-
ProceedingsoftheSecondConferenceonMachine
tionalLinguistics.
Translation,pages127–137,Copenhagen,Denmark.
AssociationforComputationalLinguistics.
PhilippKoehnandJeanSenellart.2010. Convergence
JadeGoldsteinandJaimeCarbonell.1998. Summariza- oftranslationmemoryandstatisticalmachinetransla-
tion: (1)usingMMRfordiversity-basedreranking tion. InProceedingsoftheSecondJointEM+/CNGL
and (2) evaluating summaries. In TIPSTER TEXT Workshop: Bringing MT to the User: Research on
PROGRAMPHASEIII:ProceedingsofaWorkshop IntegratingMTintheTranslationIndustry,pages21–
heldatBaltimore,Maryland,October13-15,1998, 32,Denver,Colorado,USA.AssociationforMachine
pages181–195,Baltimore,Maryland,USA.Associa- TranslationintheAmericas.
tionforComputationalLinguistics.
Hugo Laurençon, Lucile Saulnier, Thomas Wang,
JiataoGu,ChanghanWang,andJunboZhao.2019. Lev-
ChristopherAkiki,AlbertVillanovadelMoral,Teven
enshteintransformer. InAdvancesinNeuralInfor-
LeScao, LeandroVonWerra, ChenghaoMou, Ed-
mationProcessingSystems,volume32.CurranAsso-
uardoGonzálezPonferrada,HuuNguyen,etal.2022.
ciates,Inc.
TheBigScienceROOTSCorpus: A1.6TBCompos-
iteMultilingualDataset. InThirty-sixthConference
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
onNeuralInformationProcessingSystemsDatasets
tor O.K. Li. 2018. Search Engine Guided Neural
andBenchmarksTrack.
MachineTranslation. ProceedingsoftheAAAICon-
ferenceonArtificialIntelligence,32(1).
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
Shivanshu Gupta, Matt Gardner, and Sameer Singh. LemaoLiu.2022a. Asurveyonretrieval-augmented
2023. Coverage-based example selection for in- textgeneration. CoRR,abs/2202.01110.
context learning. In Findings of the Association
forComputationalLinguistics: EMNLP2023,pages Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
13924–13950,Singapore.AssociationforComputa- LemaoLiu.2022b. Asurveyonretrieval-augmented
tionalLinguistics. textgeneration.
Junxian He, Graham Neubig, and Taylor Berg-
AswanthM,RatishPuduppully,RajDabre,andAnoop
Kirkpatrick.2021a. Efficientnearestneighborlan-
Kunchukuttan.2023. CTQScorer: Combiningmul-
guagemodels. InProceedingsofthe2021Confer-
tiple features for in-context example selection for
enceonEmpiricalMethodsinNaturalLanguagePro-
machine translation. In Findings of the Associa-
cessing,pages5703–5714,OnlineandPuntaCana,
tionforComputationalLinguistics: EMNLP2023,
DominicanRepublic.AssociationforComputational
pages7736–7752,Singapore.AssociationforCom-
Linguistics.
putationalLinguistics.
Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and
PedroHenriqueMartins,ZitaMarinho,andAndréF.T.
LemaoLiu.2021b. Fastandaccurateneuralmachine
Martins. 2022. Chunk-based nearest neighbor ma-
translationwithtranslationmemory. InProceedings
chinetranslation. InProceedingsofthe2022Con-
of the 59th Annual Meeting of the Association for
ferenceonEmpiricalMethodsinNaturalLanguage
ComputationalLinguisticsandthe11thInternational
Processing, pages 4228–4245, Abu Dhabi, United
JointConferenceonNaturalLanguageProcessing
ArabEmirates.AssociationforComputationalLin-
(Volume1: LongPapers),pages3170–3180,Online.
guistics.
AssociationforComputationalLinguistics.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-
VikasRaunak,MohamedGabr,HitokazuMatsushita, aofeiSun,TianweiZhang,andJiweiLi.2022. Fast
Young Jin Kim, Mohamed Afify, and Hany Has- nearestneighbormachinetranslation. InFindingsof
san Awadalla. 2023. How good are GPT models theAssociationforComputationalLinguistics: ACL
atmachinetranslation? acomprehensiveevaluation. 2022,pages555–565,Dublin,Ireland.Association
CoRR,abs/2302.09210. forComputationalLinguistics.Yasmin Moslem, Rejwanul Haque, John D. Kelleher, CynthiaRudin.2019. Stopexplainingblackboxma-
andAndyWay.2023. Adaptivemachinetranslation chinelearningmodelsforhighstakesdecisionsand
withlargelanguagemodels. InProceedingsofthe use interpretable models instead. Nature Machine
24thAnnualConferenceoftheEuropeanAssociation Intelligence,1(5):206–215.
forMachineTranslation,pages227–237,Tampere,
Finland.EuropeanAssociationforMachineTransla- SuzannaSiaandKevinDuh.2023a. In-contextlearn-
tion. ing as maintaining coherency: A study of on-the-
flymachinetranslationusinglargelanguagemodels.
Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, InProceedingsofMachineTranslationSummitXIX,
YuchunFan,BeiLi,YinqiaoLi,TongXiao,Chun- Vol.1: ResearchTrack,pages173–185,MacauSAR,
liang Zhang, and Jingbo Zhu. 2023. Augmenting China.Asia-PacificAssociationforMachineTrans-
largelanguagemodeltranslatorsviatranslationmem- lation.
ories. InFindingsoftheAssociationforComputa-
tionalLinguistics: ACL2023,pages10287–10299, SuzannaSiaandKevinDuh.2023b. In-contextlearn-
Toronto,Canada.AssociationforComputationalLin- ingasmaintainingcoherency: Astudyofon-the-fly
guistics. machinetranslationusinglargelanguagemodels.
Makoto Nagao. 1984. A framework of a mechanical Harold Somers. 1999. Review article: Example-
translation between Japanese and English by anal- based machine translation. Machine Translation,
ogyprinciple. InArtificialandhumanintelligence. 14(2):113–157.
ElsevierSciencePublishers.B.V.
Jörg Tiedemann. 2012. Parallel data, tools and inter-
AyanaNiwa,ShoTakase,andNaoakiOkazaki.2022. faces in OPUS. In Proceedings of the Eighth In-
Nearestneighbornon-autoregressivetextgeneration. ternationalConferenceonLanguageResourcesand
CoRR,abs/2208.12496. Evaluation(LREC’12),pages2214–2218,Istanbul,
Turkey.EuropeanLanguageResourcesAssociation
KishorePapineni,SalimRoukos,ToddWard,andWei- (ELRA).
JingZhu.2002. Bleu: amethodforautomaticevalu-
ationofmachinetranslation. InProceedingsofthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier
40thAnnualMeetingoftheAssociationforCompu- Martinet,Marie-AnneLachaux,TimothéeLacroix,
tational Linguistics, pages 311–318, Philadelphia, BaptisteRozière,NamanGoyal,EricHambro,Faisal
Pennsylvania,USA.AssociationforComputational Azhar,AurelienRodriguez,ArmandJoulin,Edouard
Linguistics. Grave,andGuillaumeLample.2023. Llama: Open
andefficientfoundationlanguagemodels.
Minh Quang Pham, Jitao Xu, Josep Crego, François
Yvon,andJeanSenellart.2020. Primingneuralma- DavidVilar,MarkusFreitag,ColinCherry,JiamingLuo,
chinetranslation. InProceedingsoftheFifthConfer- VireshRatnakar,andGeorgeFoster.2023. Prompt-
enceonMachineTranslation,pages516–527,Online. ingPaLMfortranslation: Assessingstrategiesand
AssociationforComputationalLinguistics. performance. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
MattPost.2018. AcallforclarityinreportingBLEU guistics (Volume 1: Long Papers), pages 15406–
scores. InProceedingsoftheThirdConferenceon 15427,Toronto,Canada.AssociationforComputa-
MachineTranslation: ResearchPapers,pages186– tionalLinguistics.
191, Brussels, Belgium. Association for Computa-
tionalLinguistics. MengzhouXia,GuopingHuang,LemaoLiu,andShum-
ingShi.2019. Graphbasedtranslationmemoryfor
Abudurexiti Reheman, Tao Zhou, Yingfeng Luo, neuralmachinetranslation. ProceedingsoftheAAAI
DiYang,TongXiao,andJingboZhu.2023. Prompt- ConferenceonArtificialIntelligence,33(01):7297–
ingneuralmachinetranslationwithtranslationmem- 7304.
ories. ProceedingsoftheAAAIConferenceonArtifi-
cialIntelligence,37(11):13519–13527. JitaoXu,JosepCrego,andJeanSenellart.2020. Boost-
ingneuralmachinetranslationwithsimilartransla-
Ricardo Rei, José G. C. de Souza, Duarte Alves, tions. InProceedingsofthe58thAnnualMeetingof
ChrysoulaZerva,AnaCFarinha,TaisiyaGlushkova, theAssociationforComputationalLinguistics,pages
AlonLavie,LuisaCoheur,andAndréF.T.Martins. 1580–1590,Online.AssociationforComputational
2022. COMET-22: Unbabel-IST2022submission Linguistics.
for the metrics shared task. In Proceedings of the
SeventhConferenceonMachineTranslation(WMT), JitaoXu,JosepCrego,andJeanSenellart.2022. Boost-
pages578–585,AbuDhabi,UnitedArabEmirates ingneuralmachinetranslationwithsimilartransla-
(Hybrid).AssociationforComputationalLinguistics. tions. In Proceedings of the 15th Biennial Confer-
enceoftheAssociationforMachineTranslationin
StephenRobertsonandHugoZaragoza.2009. Theprob- theAmericas(Volume2: UsersandProvidersTrack
abilistic relevance framework: BM25 and beyond. and Government Track), pages 282–292, Orlando,
FoundationsandTrends®inInformationRetrieval, USA. Association for Machine Translation in the
3(4):333–389. Americas.JitaoXu,JosepCrego,andFrançoisYvon.2023. Inte- Longest Common Subsequence (LCS) of x˜ and
gratingtranslationmemoriesintonon-autoregressive x: ecs (x˜,x) = lcs(x˜,x).
d,a,r
machinetranslation. InProceedingsofthe17thCon-
Ifa+d > r:
ferenceoftheEuropeanChapteroftheAssociation
for Computational Linguistics, pages 1326–1338, 
r[|x|−|ecs (x˜,x)|]+d(|x˜|−|x|)
Dubrovnik,Croatia.AssociationforComputational 
if
|x|≤|d x˜,a |,r
Linguistics. ∆(x˜,x)=
r[|x˜|−|ecs (x˜,x)|]+a(|x|−|x˜|)
BiaoZhang,BarryHaddow,andAlexandraBirch.2023.

if
|x|>|d x˜,a |,r
Promptinglargelanguagemodelformachinetransla- (5)
tion: Acasestudy. InProceedingsofthe40thInter- Ifa+d ≤ r:
nationalConferenceonMachineLearning,ICML’23.
JMLR.org.
∆(x˜,x)=a(|x|−|lcs(x˜,x)|)+d(|x˜|−|lcs(x˜,x)|)
Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra- (6)
hamNeubig,andSatoshiNakamura.2018. Guiding ∆ismaximalwhen|ecs| = 0. Thenormaliza-
neuralmachinetranslationwithretrievedtranslation
tiontermcanbeexpressedas:
pieces. In Proceedings of the 2018 Conference of
theNorthAmericanChapteroftheAssociationfor
 a|x|+d|x˜|
C noo lm ogp iu et sa ,t Vio on lua mlL ei 1ng (Lu ois nt gic Ps: apH eu rm s)a ,n paL ga en sg 1u 3a 2g 5e –1T 3ec 3h 5-
,

(r−di )f |xa |+ +d d|≤ x˜|r
NewOrleans,Louisiana.AssociationforComputa- N (|x˜|,|x|)=
Kat nio gn jia el ZL hin eg nu gi ,st Lic os n.
gyueWang, ZhihaoWang, Binqi
d,a,r 
(r−a
ii
)
ff
|x˜
aa
|
++
+
dd
a|
>>
x|
rr aa nn dd || xx || >≤| |xx ˜˜ ||
Chen, Ming Zhang, and Zhaopeng Tu. 2023. To- (7)
wardsaunifiedtrainingforLevenshteintransformer. Choosing (d,a,r) = (0,1,1) makes sim(x˜,x)
In Proceedings of the IEEE International Confer-
computethecoverageofxwithtokensfromx˜:
ence on Acoustics, Speech and Signal Processing
(ICASSP),pages1–5.
sim(x˜,x)=1− a(|x|−|lcs(x˜,x)|) = |lcs(x˜,x)| (8)
a|x| |x|
XinZheng,ZhiruiZhang,JunliangGuo,ShujianHuang,
BoxingChen,WeihuaLuo,andJiajunChen.2021. B Fuzzy-Matchingdetailedsettings
Adaptive nearest neighbor machine translation. In
Proceedingsofthe59thAnnualMeetingoftheAsso-
Notethatthecodeusedforourexperimentsisopen
ciationforComputationalLinguisticsandthe11th
source.17 Additional details can be found in the
InternationalJointConferenceonNaturalLanguage
Processing(Volume2:ShortPapers),pages368–374, repository.
Online.AssociationforComputationalLinguistics.
B.1 NGM
A δ-LCSformulation
Forasourceq,n-grammatchingidentifiesg(q,x),
The general formulation of a similarity metric the longest common n-gram between q and any
basedontheeditdistanceis: source example x. x passes the filter if (1)
|g(q,x)| ≥ ML,anabsolutelengththresholdthat
∆(x˜,x)
sim(x˜,x) = 1− , (4) wechoosetobe3;(2)|g(q,x)| ≥ τ|q|,withτ vary-
N(|x˜|,|x|)
ingbetween0.3(followingtheworksofXuetal.
(2022);Bouthorsetal.(2023))and0.2–whichis
with N a normalizing term that upper bounds ∆.
amorepermissivefilterthatincreasesthechance
∆ is parameterized by d, a, r, corresponding re-
tohavinghigherscoringmatchesw.r.t.theranker
spectivelytothedelete(resp. insert,replace)costs
(BM25,ED).
(assumingcopieshavea0cost),generalizingthe
Ouralgorithmusesasuffixarraystructurewhich
standard ED setting (d = a = r = 1). ∆ corre-
directlyindexesthen-gramsandenablestosearch
spondstotheminimalgeneralizedcostnecessary
forn-grammatchesinthesortedarraywithaloga-
toeditx˜ intox.
rithmiccomplexity.
We define an Edit Common Subsequence
However,thedistributionofthenumberofcan-
ecs (x˜,x), which corresponds to the tokens
d,a,r
didatespassingthefilterhasahighvariance(see
copied when optimally editing x˜ into x.16 If
Table 9). In consequence, it can be ineffective to
a + d ≤ r, then the copied tokens are a
selectonlyasmallamountofrelevantcandidates
16Therecanbeinfactseveralconcurrentcommonsubse-
quences. 17Availableathttps://github.com/SYSTRAN/fuzzy-match(too permissive). On the other hand, it can often suggests that BM25 filter is better at increasing
retrieveveryfewcandidates(<3). coverage and relevance than NGM. But it has a
higherlatency. ThiscanexplainwhyBM25-filtered
τ Q1 Q2 Q3 pipelines are better at training time (as shown in
0.2 6 116 1573 AppendixD).
0.3 2 15 293
0.4 1 6 99 C Modelsdetailedsettings
0.5 1 4 54
C.1 NFA
Table9: Average1st,2nd,3rdquartilesfortheeleven WemadeourownimplementationofNFAbased
datasets of the distribution of candidates passing the ontheworkofBulteandTezcan(2019)withthe
NGMfilterw.r.t.τ.
following parameters: size of word embedding:
512;sizeofhiddenlayers: 512;sizeofinnerfeed
B.2 BM25
forwardlayer: 2,048;numberofheads: 8;number
BM25(RobertsonandZaragoza,2009)isawidely of layers: 6; batch size: 4,096 tokens. We set
usedrankerinInformationRetrieval. Itoftencom- warmup steps to 4,000 and update learning rate
peteswithstate-of-the-artneuralretrieversandis for every 8 iterations. Fine-tuning is performed
usedasabaseline. Itisafullyunsupervisedmethod continuingAdamwithlearningratedecayschedule
basedoncomputationsinspiredbythetf-idf scor- untilconvergence. Themodelsaretrainedwithone
ingfunction. BM25typicallyaimstoretrievedocu- NVIDIAP100GPU.Weuseajointvocabularyof
mentsthathavecommonrarewordswiththequery. 32Kforbothsourceandtargetsides. Atinference
Our implementation is not as optimized as in weuseabeamsizeof5.
Elasticsearch18 orApacheLucene19, itisnonethe-
C.2 TMN-LevT
lessveryefficientandintegratesseamlesslyinour
translationpipeline. We use a Multi-Levenshtein Transformer archi-
SincecomputingBM25iscomputationallylin- tecture with embeddings of dimension 512; feed-
ear in the number of sentences in the TM, naive forward layers of size 2048; number of heads 8;
implementationscansometimesbeslow. Weuse numberofencoderanddecoderlayers: 6;shared-
aninvertedindex,whichdirectlymapseachterm embeddings;dropout: 0.3.
to the set of TM segments where they occur. In- During training, we use Adam optimizer with
steadofcomputingBM25foreverysentence,we (β ,β )=(0.9,0.98);inversesqrtscheduler;learn-
1 2
onlydoitfortheunionofTMsegmentscontaining ingrate: 5e−4;labelsmoothing: 0.1;warmupup-
query terms. As common terms such as punctu- dates: 10,000; float precision: 16. We fixed the
ation, prepositions, etc. will likely map to most number of iterations at 60k. The batch size and
TMs,theyareremovedfromtheindex. numberofGPUsaresettohave,onaverage,∼ 450
Inourimplementation,atermissaidtobecom- samplesperiteration. Weperformedapretraining
monifitappearsinmorethanp%ofthesegments. of the model on a synthetic dataset as described
Wefindthatpcanbequitesmall(2%)withoutaf- by Bouthors et al. (2023). We use a joint vocab-
fectingthesetofretrievedsentencestoomuch. For ulary of 32K. For decoding, we use realignment
instance,wefindthatBM25+LCSwiththesetting anditerativerefinementwithanemptyplaceholder
L = 100,p = 2%has90.4%Jaccardsimilarity(in penaltyof3,andamaxnumberofiterationsof10
terms of the set of indices of retrieved segments) (Guetal.,2019).
withthesettingL = 100,p = 10%.
Inpreliminaryexperiments,wealsovariedthe C.3 BLOOM
value of L (10 vs 100) and found out that a low BLOOM is a family of large open-source causal
L negatively affects the model-agnostic metrics multilinguallanguagemodelstrainedinthecourse
(coverage,relevance)andsetL = 100inallexper- oftheBigScienceproject(BigScienceetal.,2022).
iments. Ourexperimentsusethelargestavailableversion,
Table10containsthemodel-agnosticscoreson comprising176bparameters. Thismodelhasbeen
the training set en-fr for filtering pipelines. It repeatedly evaluated in translation scenarios see
18https://elastic.co. e.g.(BawdenandYvon,2023). BLOOM’straining
19http:lucene.apache.org. corpus officially contains a fair share of EnglishandFrench,buthardlyanyGerman20 (Laurençon high-densityones(EMEA,KDE4)aretheonesthe
etal.,2022). Wethusconsiderthetwofollowing mostlikelytodifferfromtheaveragebest. More-
translation directions: en-fr and de-en, selecting over,low-densitydomainsseemtopreferdiversity
targetlanguagesforwhichthegenerationabilities andcoveragewithBLOOM.AsforTM3-LevT,the
arestrong. lowestandhighestdensitiesarebothmoreinclined
Followingcommonpractice,weselectasimple to choose pipelines with filters. The model may
promptoftheform“[src]: sourcesentence. =[trg]: sufferfromlow-qualityexamplesfromadifficult
targetsentence”,where“[src]”and“[trg]”arelan- low-densitydomainwhilenotnecessitatingdiver-
guage tags denoting respectively the source and sityinthecaseofdenseones.
thetargetsentences,usingeither1or3in-context
F Illustration
examplesbeforethesourcequery. Allexperiments
generatetranslationinpuregreedymode;genera-
Figure2illustratesthevariabilityacrossretrieval
tionstopseitherwhentheendofsentencetokenis
settings. Wecanobservetheirmaincharacteristics:
producedorwhenamaximumlengthof256tokens
isreached. AsisalsocustomforBLOOM,which • SinceNGMandBM25acthereasfilters,they
tendstoovergenerate,wepost-processtheoutput barelychangetheretrievedset.
toshortenexcessivelylongoutputsandmakesure
• δ-LCScoversmorewordsinthesourceatthe
thetargetisneverlongerthan1.5timesthesource
expenseoflongersentences.
(measured in chars). This post-processing only
impactsabout5to10%ofalloutputsentences.
• Thefirstretrievedsentenceforthecontrastive
ranking is always the same as for LED, but
D CrossinferenceforTM3-LevT
theothertwoareoftenmorediverse,covering
We train Multi-Levenshtein Transformer moreterms.
(TM3-LevT) with a series of retrieval settings
withNGM(Table11)andBM25filters(Table12).
Afterward, each trained model is evaluated
on all the retrieval settings of the same filter
category. Thisway,itispossibletoidentifywhich
train/inference setting pairs perform better and
whether having the same setting for training and
inferenceisrecommended. Wecan,forbothNGM
andBM25,identifyasystematicallyindependent
optimaltrainingandinferencesetting. Wefindthat
the BM25 filter is slightly better in general. The
contrastive method only helps at inference time
andformediummatches,suggestingadaptingthe
factortothefuzzyscore(ED).
0.1-LCSisoverallbetter,exceptfortrainingwith
BM25,whichsurprisinglyachievesthebestscores
with LED. Probably, having more source-similar
sentencesattraintimeisnecessarytocompensate
for the unordered nature of BM25, required by
TM3-LevT.
E Perdomainanalysis
Tables 13,14,15 contain the detailed per-domain
analysis for direction en-fr. We insist on the fact
that this corpus offers a high variability across
domains (size, density, sentence length...). Low-
densitydomains(Ubuntu,News-Commentary)and
20EventhoughtracesofGermancanbefoundinthecorpus.Ngramfilter(τ =) 0.3 0.2 0.2 0.3 0.2 - - - -
BM25filter(L=or’-’) - - - - - 100 100 100 100
Ranking(LED,LCS,δ-LCS) LED LCS δ-LCS δ-LCS δ-LCS LED LCS δ-LCS δ-LCS
Contrastfactor(α=) - - - - 0.3 - - - 0.3
coverage 27.9 42.5 38.8 35.0 39.5 32.6 48.1 43.6 44.6
relevance 23.0 23.3 26.1 23.9 24.6 26.6 28.8 29.2 27.6
length 15.4 40.6 24.0 22.0 25.5 15.2 30.1 22.4 23.5
Table10: Retrievalscoresaveragedover11domains(trainsets,en-fr).
source Mostpatientsrequiredtreatmentfortheirorthostatichypotension.
NGM+LED coverage: 3words
1 Rare:peripheralcoldness,orthostatichypotension.
2 Entacaponemayaggravatelevodopa-inducedorthostatichypotension.
3 -Stalevomayinduceorthostatichypotension.
BM25+LED coverage: 3words
1 Hypotension,orthostatichypotension.
2 -Stalevomayinduceorthostatichypotension.
3 Rare:peripheralcoldness,orthostatichypotension.
LED coverage: 3words
1 Hypotension,orthostatichypotension.
2 -Stalevomayinduceorthostatichypotension.
3 Rare:peripheralcoldness,orthostatichypotension.
δ-LCS coverage: 6words
1 Patientsreceivingaripiprazolesolutionforinjectionshouldbeobservedfororthostatichypotension.
2 Ifparenteralbenzodiazepinetherapyisdeemednecessaryinadditiontoaripiprazolesolutionforinjection,
patientsshouldbemonitoredforexcessivesedationandfororthostatichypotension(seesection4.5).
3 HypotensionVELCADEtreatmentiscommonlyassociatedwithorthostatic/posturalhypotension.
LED coverage: 6words
c
1 Hypotension,orthostatichypotension.
2 Mostpatientshadreliefofsymptomsafterstoppingtreatment.
3 Entacaponemayaggravatelevodopa-inducedorthostatichypotension.
δ-LCS coverage: 5words
c
1 Patientsreceivingaripiprazolesolutionforinjectionshouldbeobservedfororthostatichypotension.
2 Aminorityofpatientswithorthostatichypotensionexperiencedsyncopalevents.
3 Ifparenteralbenzodiazepinetherapyisdeemednecessaryinadditiontoaripiprazolesolutionforinjection,
patientsshouldbemonitoredforexcessivesedationandfororthostatichypotension(seesection4.5).
Figure2: Illustrationofthevariabilityacrosssomeretrievalsettingsandtheirrespectivecoverageforasource
sentencefromEMEA.Foreachsetting,werepresentthesource-side3best-rankedsentences.train\infer LED+ LCS− δ-LCS− δ-LCS+ δ-LCS− LED+ LCS− δ-LCS− δ-LCS+ δ-LCS−
c c
LED+ 43.9 44.1 44.8 44.8 44.8 56.0 55.6 56.2 56.3 56.1
LCS− 43.9 44.5 44.8 44.6 45.0 55.7 56.2 56.5 56.3 56.6
δ-LCS− 44.6 44.8 45.3 45.2 45.5 55.9 56.1 56.6 56.5 56.5
δ-LCS+ 44.1 44.5 45.3 45.1 45.3 56.3 56.3 56.9 56.9 56.8
δ-LCS− 43.8 44.1 44.7 44.5 44.9 55.6 55.9 56.4 56.2 56.3
c
Table11: Averagecross-inferenceBLEUscoreaccrossalldomainsfortest-0.4(left)andtest-0.6(right)withNGM
filter. τ isspecifiedwith+for0.3and−for0.2;cdenotescontrast. Bestcolumn-wise(resp. row-wise)BLEUare
underlined(resp. inbold).
train\infer LED LCS δ-LCS δ-LCS LED LCS δ-LCS δ-LCS
c c
LED 45.1 45.2 45.7 45.7 57.0 56.5 57.1 57.0
LCS 44.7 45.1 45.3 45.4 56.1 56.4 56.5 56.4
δ-LCS 44.6 45.1 45.3 45.4 56.4 56.6 56.8 56.9
δ-LCS 42.3 43.2 43.6 44.0 55.2 55.6 55.8 56.1
c
Table12: Averagecross-inferenceBLEUscoreaccrossalldomainsfortest-0.4(left)andtest-0.6(right)withBM25
filter. cdesignatescontrast. Bestcolumn-wise(resp. row-wise)BLEUareunderlined(resp. inbold).
pipeline ECB EME Epp GNO JRC KDE News PHP TED Ubu Wiki all
test-0.4
NGM+LED1-1 64.7 65.3 44.9 71.2 76.4 65.9 30.4 42.0 43.9 57.5 43.5 55.1
NGM+LED3-3 64.8 65.8 44.6 71.1 76.4 65.4 29.7 41.4 44.0 56.4 44.4 54.9
NGM+LED3-2 64.9 65.6 44.5 70.8 76.3 65.4 29.7 41.4 44.0 56.3 44.2 54.8
NGM+LED3-1 64.4 65.0 44.7 70.6 76.2 65.7 29.8 41.6 43.9 56.6 44.1 54.8
BM25 64.1 66.3 44.8 70.8 76.0 65.0 30.2 40.0 43.5 57.7 43.0 54.7
BM25 64.2 66.4 44.8 70.9 76.0 64.9 30.2 40.0 43.5 57.6 43.0 54.7
c
LED 64.8 65.4 44.8 71.1 76.5 65.4 29.6 41.0 43.8 56.7 44.5 54.9
δ-LCS 65.0 64.6 44.8 70.9 76.5 65.3 29.9 41.4 43.6 57.3 43.9 54.8
δ-LCS 65.2 63.9 45.0 70.8 76.4 65.4 29.8 41.8 43.9 57.3 43.6 54.8
c
test-0.6
NGM+LED1-1 71.0 73.3 59.1 79.9 83.7 68.1 27.6 46.4 63.7 64.4 69.8 64.3
NGM+LED3-3 70.8 73.1 59.1 80.4 83.9 69.4 27.3 45.7 64.2 64.1 69.7 64.3
NGM+LED3-2 70.8 72.3 59.1 80.4 83.7 69.2 27.2 45.6 64.1 63.8 70.1 64.2
NGM+LED3-1 70.1 72.2 59.0 79.8 83.6 68.5 27.4 44.8 64.1 63.5 69.5 63.9
BM25 71.2 72.8 59.2 81.0 83.5 67.7 27.5 44.8 64.3 64.9 69.1 64.2
BM25 71.2 72.9 59.2 81.1 83.5 67.7 27.6 44.8 64.3 64.8 68.9 64.2
c
LED 70.8 73.1 59.1 80.4 83.8 69.4 27.3 45.7 64.1 64.2 69.9 64.4
δ-LCS 71.1 73.2 59.1 80.1 83.7 68.7 27.2 46.0 64.2 64.4 69.7 64.3
δ-LCS 71.1 73.0 59.1 80.2 83.8 68.9 27.1 45.9 64.3 64.4 69.5 64.3
c
Table13: BLEUscore(en-fr): NFAtrainedonNGM+LED(τ =0.3),inferredonfilter-freeretrievalsettings.
pipeline ECB EME Epp GNO JRC KDE News PHP TED Ubu Wiki all
test-0.4
bestNGM+EDpair 57.4 57.6 35.8 62.0 68.7 55.9 22.1 29.2 31.6 45.2 35.8 45.6
bestBM25+EDpair 57.5 57.0 35.8 63.0 69.2 57.5 21.9 30.4 31.3 45.5 34.8 45.8
LED 56.7 56.9 35.6 62.4 68.8 56.4 21.5 30.6 32.3 45.9 34.4 45.6
δ-LCS 57.3 56.5 36.8 63.3 69.2 56.7 21.9 30.9 31.9 45.1 34.9 45.9
δ-LCS 57.7 56.8 36.4 62.8 69.3 56.8 21.8 30.8 31.7 46.5 35.2 46.0
c
test-0.6
bestNGM+EDpair 65.8 68.3 51.6 73.3 77.5 63.0 21.2 33.4 55.9 53.0 63.6 57.0
bestBM25+EDpair 66.7 67.8 51.2 72.6 78.4 62.5 21.4 34.9 55.9 53.7 63.6 57.2
LED 66.2 68.1 51.5 73.0 78.4 62.7 21.4 34.7 55.5 54.0 64.3 57.3
δ-LCS 66.8 68.3 51.6 73.3 78.4 62.9 21.3 35.4 56.2 53.5 63.9 57.4
δ-LCS 66.6 68.1 52.0 72.5 78.6 62.0 21.3 35.0 55.3 53.9 63.5 57.2
c
Table14: BLEUscore(en-fr): TM3-LevTwiththebesttrain/inferpipelines(top)ortrainedonBM25+LEDand
inferredonfilter-freeEDvariants.model k-shot ECB EME GNO KDE News PHP Ubu all
test-0.4
random 1 26.2 27.8 37.0 27.4 24.8 19.3 42.4 29.3
random 3 30.7 34.7 40.1 31.8 26.9 23.4 44.2 33.1
LED 1 50.7 54.4 57.6 48.0 24.2 31.3 54.5 45.8
LED 3 52.7 56.7 59.0 50.2 26.1 33.0 57.2 47.8
LED 3 53.6 57.6 59.9 51.3 26.3 33.0 56.7 48.3
c
BM25 1 48.2 57.2 57.7 47.0 25.2 31.1 53.9 45.8
BM25 3 52.2 58.9 59.1 49.7 27.0 33.2 56.7 48.1
BM25 3 52.7 58.7 59.4 49.5 27.3 32.4 56.6 48.1
c
δ-LCS 3 52.8 56.6 59.0 50.9 26.4 33.1 57.8 48.1
δ-LCS 3 52.3 57.4 58.8 51.0 26.9 33.1 58.1 48.2
c
test-0.6
random 1 26.7 26.6 33.9 25.3 21.9 18.7 43.9 28.1
random 3 32.1 34.1 36.5 29.3 24.3 23.2 47.5 32.4
LED 1 57.9 64.7 58.7 52.9 21.5 32.8 61.4 50.0
LED 3 60.7 64.6 60.3 53.8 23.4 34.8 63.2 51.5
LED 3 61.5 65.1 61.0 54.1 23.7 35.2 63.5 52.0
c
BM25 1 57.5 63.8 58.7 50.5 22.0 32.4 62.0 49.6
BM25 3 59.7 65.2 60.8 53.9 24.3 35.0 64.0 51.8
BM25 3 59.6 64.9 60.3 53.7 24.1 34.1 63.0 51.4
c
δ-LCS 3 59.6 65.3 60.8 54.5 23.3 34.8 63.0 51.6
δ-LCS 3 59.4 64.6 60.8 55.4 23.5 34.3 63.6 51.7
c
Table15: BLEUscore(en-fr): BLOOMinferredonfilter-freeretrievalsettings.