The RealHumanEval: Evaluating Large Language Models’
Abilities to Support Programmers
Hussein Mozannar∗1,2, Valerie Chen∗3, Mohammed Alsobay2, Subhro Das1,4, Sebastian
Zhao5, Dennis Wei1,4, Manish Nagireddy1,4, Prasanna Sattigeri1,4, Ameet Talwalkar3, and
David Sontag1,2
1MIT-IBM Watson AI Lab
2Massachusetts Institute of Technology
3Carnegie Mellon University
4IBM Research
5University of California, Berkeley
Abstract
Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks,
including HumanEval [Chen et al., 2021], which measure the ability of LLMs to generate complete code
that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains
onexistingbenchmarkstranslatetogainsinprogrammerproductivitywhencodingwithLLMs,including
time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that
might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do
so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers,
through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval
in which users interacted with six LLMs of varying base model performance. Despite static benchmarks
not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to
increased programmer productivity; however gaps in benchmark versus human performance are not
proportional—atrendthatholdsacrossbothformsofLLMsupport. Incontrast,wefindthatprogrammer
preferences do not correlate with their actual performance, motivating the need for better, human-centric
proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models
and the study data to facilitate efforts to improve code models.
1 Introduction
Coding benchmarks such as HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] play a key
role in evaluating the capabilities of large language models (LLMs) as programming becomes a valuable
application through products such as GitHub Copilot [Github, 2022] and ChatGPT [OpenAI, 2022a]. These
benchmarksquantifyLLMabilitiesbymeasuringhowwellamodelcancompleteentirecodingtasks, typically
∗Equalcontribution. Correspondencetomozannar@mit.eduandvchen2@andrew.cmu.edu.
1
4202
rpA
3
]ES.sc[
1v60820.4042:viXraReset Code Change Theme Show Instructions Time left: 15:02
2/7 tasks completed
Y imo pu lr e mgo ea nl t is th t eo (a) cl as ds efTo _k _en ii nz ie t_r _: (self, max_vocab_size=200): (b) Clear UC sh ea rt (c)
build_vocabulary method in the self.max_vocab_size = max_vocab_size How do I count occurrence of each
provided Tokenizer class. self.word_to_id = {} word in a list of strings
self.id_to_word = {}
A tokenizer is an object that def tokenize(self, text):
c To hn ev e or bt js e cw to ivr ed s o ft o t hn eu merical IDs. eturn text.lower().split() Chabot
build_vocabulary method is as def build_vocabulary(self, corpus): You can use the ngram function in the nltk
follows: word_freq = Counter() library to count the occurrence of each word
• The method's primary goal is for sentence in corpus: Autocomplete suggestion in a list of strings. Here's an example:
t so el fc .r we oa rt de _ tw too _ d idic t ai no dn aries: word_freq.update(sentence)
self.id_to_word. Copy
• self.word_to_id should map
each unique word in your import nltk
corpus to a unique numerical # List of strings
identifier (ID). texts = ["This is an example sentence",
• self.id_to_word is the reverse "Another example sentence"]
mapping, where each unique ID ngrams = nltk.ngrams(texts, 1)
corresponds to a word. for ng in ngrams:
The method should only consider # print(ng[0], ng[1])
the most frequent words in the print(ng[0], ng[1].count())
corpus, up to a limit specified by
max_vocab_size.
Type your message….
Output and errors will appear here
Output: Run Submit
Figure1: WeintroduceRealHumanEval,anonlineevaluationplatformforLLM-assistedcoding. Theplatform
consists of (a) a customizable task description, (b) the code editor which shows autocomplete suggestions in
grey, and (c) the chat assistant. Above the editor, users can check their task progress and the amount of
time left, reset the editor, or change the editor theme, and view study instructions. Below the editor, they
can run and submit their code. We use RealHumanEval to study whether static benchmark performance or
programmer preference judgments—based on autocomplete suggestions or chat responses—are aligned with
programmer productivity.
using metrics like pass@k. As LLMs are increasingly adopted as programmer assistants, which provide chat
responsesorautocompletesuggestions,ratherthanfullcodegenerations,morehuman-centricevaluationshave
been proposed. In particular, a predominant evaluation approach is based on human preference judgments
between pairs of LLM responses (e.g., AlpacaEval [Dubois et al., 2023], MT-Bench [Zheng et al., 2023], and
Chatbot Arena [Chiang et al., 2024]). Another form of human preference that has been used as a metric of
success, in products such as GitHub Copilot, is the acceptance rate of LLM suggestions [Bird et al., 2022]. In
this study, we ask whether the use of either static benchmarks or preference judgments is reflective of the
LLM’s downstream impact on programmer productivity.
While a set of small-scale user studies have been conducted to primarily build a qualitative understanding of
how programmers use LLM assistance, they are typically restricted to evaluations on one model, one form
of LLM support, and a limited set of tasks [Barke et al., 2022, Mozannar et al., 2022, Vaithilingam et al.,
2022, Ross et al., 2023, Liang et al., 2023, Peng et al., 2023]. As such, it is not possible to conclude from
these studies the effect of a base model’s performance and whether outcomes vary based on the form of LLM
assistance provided. To answer our primary question, in this work, we present a task-driven, human-centered
evaluation of LLMs for programming. We introduce an online evaluation platform, RealHumanEval, which
consistsofacodeeditorwhereprogrammerscansolvecodingtaskswithtwodifferentformsofLLMassistance
(Figure 1). Programmers can either ask questions to the LLM through a chat window or receive code
completion suggestions through an autocomplete system inside the editor. The interface also supports
executing and testing code given a task specification and logging telemetry which can be used to compute
2productivity metrics and human preference metrics of the LLM support, including average acceptance rates
of suggestions and the likelihood of copying code from chat responses.
Using RealHumanEval, we conduct a user study with 213 participants to understand the effect of a model’s
benchmarkperformanceandtheformofLLMassistanceondownstreamproductivitymetrics. Eachparticipant
was assigned to one of seven conditions: a control condition with no LLM support, three conditions with
autocomplete support from either CodeLlama-7b [Rozière et al., 2023], CodeLlama-34b [Rozière et al., 2023],
or GPT-3.5-turbo-instruct[Brown et al., 2020], and finally three conditions where the editor is equipped
with a chat window powered by the chat variants of the previous models. We deliberately select three model
families with increasingly higher benchmark performance and consider model pairs within each family that
havesimilarbenchmarkperformancetounderstandtheeffectofautocompleteversuschatassistance. Through
the study, we collected interactions on a total of 771 coding tasks, where 5204 autocomplete suggestions were
shown and 775 chat messages were sent.
Overall, we find that improving a model’s base performance on existing coding benchmarks leads to gains in
human productivity, particularly in the time spent completing tasks. These trends were present across both
chat and autocomplete interactions, validating the potential “generalizability” of benchmarking efforts to
more realistic contexts. However, we observe that gaps in benchmark versus human performance are not
necessarily proportional, suggesting that further gains in benchmark performance do not necessarily translate
into equivalent gains in human productivity. We also investigated whether human preference metrics, such as
theaverageacceptancerateofsuggestionsandthelikelihoodofcopyingcodefromchatresponses,alignedwith
productivitymetrics. WhilethesepreferencemetricsarereadilyavailableinrealdeploymentsofLLMsystems
compared to task completion time and thus can be attractive proxy metrics [Ziegler et al., 2022], we find that
they are only correlated with programmer perceptions of LLM helpfulness but not necessarily with actual
programmer performance. The dissimilar findings between benchmarking and human preference metrics
highlight the importance of careful evaluation to disentangle which metrics are indicative of downstream
performance. In summary, our contributions are as follows:
1. An open-source human-centric evaluation platform for code. We create and release an online
platform RealHumanEval to evaluate human coding performance with LLM assistance that supports
multiple models, forms of LLM interactions, and metrics. We use this platform to conduct a user study
with six different LLMs, along with autocomplete and chat interactions.
2. Findings on the informativeness of static benchmarks and preference metrics. While we find
that benchmark performance is correlated with downstream user productivity, we do not observe the same
with human preference metrics. Our findings emphasize the importance of studying how programmers
interact with code LLMs through user studies to identify nuances in programmer-LLM interactions. To
this end, we encourage the community to leverage and build on RealHumanEval to evaluate new LLMs.
2 Related Work
Coding Benchmarks. Benchmarks are essential for tracking the progress of LLMs, and coding benchmarks are
a key piece [Achiam et al., 2023, Laskar et al., 2023, Zan et al., 2023, Hou et al., 2023]. Moreover, the coding
ability of an LLM can be informative of its reasoning abilities [Madaan et al., 2022]; thus, performance on
coding benchmark is of broader interest. While HumanEval [Chen et al., 2021] and MBPP [Austin et al.,
32021] are the most commonly used coding benchmarks, many extensions and further benchmarks have been
proposed [Lu et al., 2021, Nijkamp et al., 2022, Zhu et al., 2022, Liu et al., 2023, Jimenez et al., 2023,
Khan et al., 2023, Yang et al., 2023, Yan et al., 2023], we highlight a few: EvalPlus extends HumanEval
with further tests per problem [Liu et al., 2023], MultiPL-E [Cassano et al., 2023] to other languages,
ReCode adds robustness checks [Wang et al., 2022], HUMANEVALPACK adds code repair and explanation
tasks [Muennighoff et al., 2023], and buggy-HumanEval introduces bugs in the reference code [Dinh et al.,
2023]. Relatedly,theDS-1000[Laietal.,2023]benchmarkevaluatesmodels’abilitiesondatascienceproblems
that require the use of external libraries. There are also more involved evaluations, including the multi-turn
program evaluation benchmark [Nijkamp et al., 2022]. SWE-bench also introduces more challenging tasks
that require the LLM to resolve GitHub issues [Jimenez et al., 2023]. Instead of relying solely on pass@k,
recent work has advocated for the consideration of syntactic similarity and human preference metrics [Dibia
et al., 2023].
Together, the above benchmarks evaluate a diverse set of LLM behaviors, ranging from code generation to
debugging. These benchmarks do not, however, include a programmer-in-the-loop, as there would be in a
real-world setup. Our approach complements this existing line of work by conducting a human evaluation of
LLMs, where programmers will put these desirable behaviors to the test in realistic scenarios.
Programmer-LLM Interaction. Our work considers both chat and autocomplete LLM support separately.
Examples of autocomplete and chat assistants include GitHub’s Copilot [Github, 2022], Replit’s Ghostwriter
[replit, 2023], Amazon CodeWhisperer [Amazon, 2022], and ChatGPT [OpenAI, 2022b]. Both setups have
been evaluated in multiple user studies: autocomplete in [Vaithilingam et al., 2022, Peng et al., 2023, Barke
et al., 2022, Prather et al., 2023, Mozannar et al., 2022, Kazemitabaar et al., 2023] and chat in [Ross et al.,
2023, Chopra et al., 2023, Gu et al., 2023, Xiao et al., 2023]. While these studies have made progress
in understanding programmer-LLM interactions, we highlight a few limitations of these studies that we
aim to address in this work: First, all studies only consider one LLM—often Copilot—and one form of
programmer-LLM interaction—either autocomplete or chat, making it difficult to compare outcomes and
metrics across models and across forms of support. Second, many studies have limited scope in terms of
the number and types of coding tasks that are considered (e.g., focusing on one minesweeper game [Prather
et al., 2023] or simple plotting tasks [Ross et al., 2023]), which differ from the breadth of tasks that have
been evaluated in benchmarks and are present in practical use cases. Furthermore, the coding tasks are often
ones that many LLMs can fully solve, limiting the extent to which programmers contribute.
3 RealHumanEval
We introduce RealHumanEval, a web-based platform to conduct human-centric evaluation of LLMs for
programming, shown in Figure 1. RealHumanEval incorporates many basic features of common code editors
and the functionality of programming interview sites such as LeetCode. Given a coding task that consists of
a natural language description, partial code (e.g., a function signature), and unit tests that evaluate the task,
RealHumanEval allows the programmer to write code with assistance from an LLM to complete the task.
The platform has a panel that displays the natural language description of a task, as shown in Figure 1(a),
alongside partial code to solve the task. Participants then write their code for the task in the code editor and
can test their code with a button that checks the code against test cases as well as run their code directly.
The editor displays any errors, if available, and whether the code passes the unit test. Once the programmer
4completes the task, a new task can be loaded into the interface.
Forms of LLM Assistance. RealHumanEval supports two forms of LLM assistance for the programmer:
autocomplete-based and chat-based. In autocomplete-based assistance, the programmer writes code in an
editor, and the LLM displays a code suggestion inline, which is greyed out as shown in Figure 1(b). The
LLM is assumed to be able to fill in code given a suffix and prefix. A suggestion, based on the current code
body in the editor, appears whenever the programmer pauses typing for more than two seconds or when
the programmer requests a suggestion by pressing a hotkey. The programmer can accept the suggestion by
pressing the tab key or reject it by either pressing escape or continuing to type.
In chat-based assistance, the programmer writes code in an editor and has access to a side chat window where
the programmer can ask questions and get responses from the LLM, as illustrated in Figure 1(c). The LLM
is assumed to be a chat model. The programmer can copy and paste code from the LLM’s responses into the
editor. Currently, the interface supports any LLM invoked via an online API. Further information on both
forms of interaction is in Appendix A.
Telemetry logging. RealHumanEval logs all user behavior, including interactions with LLM support. For
each autocomplete suggestion, we log the following tuple {(P ,S ),R ,A }n where (P ,S ) is the prefix and
i i i i i=1 i i
suffix of the code based on cursor position at the time of suggestion i, R is the LLM suggestion, and A is a
i i
binary variable indicating whether the suggestion was accepted. All the logs are stored in a dataset D . For
ac
chat-assistance, we log for each user message the following tuple {X ,M ,R ,C }n where X is the code at
i i i i i=1 i
the time of message i, M is the user message (including prior chat history), R is the response from the LLM
i i
for the message, and C is the number of times code was copied from the LLM’s response. All the logs are
i
stored in a dataset D . Moreover, every 15 seconds, the interface saves the entire code the user has written.
chat
From the telemetry logs, RealHumanEval provides multiple metrics to analyze programmer behaviors: the
number of tasks completed (completion is measured by whether the submitted code passes a set of private
test cases), time to task success (measured in seconds), acceptance rate (fraction of suggestions shown that
are accepted, for autocomplete), and number of chat code copies (counting when user copies code from LLM
response, for chat) among other metrics.
4 Study Design
Using RealHumanEval, we conducted a user study to evaluate the impact of LLM assistance on programmer
performance as a function of the LLM’s performance on static benchmarks. Moreover, we also investigate
whether human preference metrics correlate with programmer productivity metrics. The design of our study
is illustrated in Figure 2 and described below.
Overview. For the entire duration of the study, participants are randomly assigned either to a control group,
where they experienced the no LLM condition, or to the LLM-assisted group, where they experienced the
autocomplete or chat support condition. For autocomplete-based support, the window in Figure 1(c) is hidden.
For chat-based support, no autocomplete suggestions are shownin Figure 1(b). Participants are onlyassigned
to one condition to minimize context switching, given the relatively short duration of the study. The study
was conducted asynchronously through a web app; participants were told not to use any outside resources
(e.g., Google), and cannot paste any text originating outside the app into the editor. Specific instructions are
in Appendix A. The first problem was a simple task to compute the sum and product of a list for participants
5No LLM AutoComplete Chat
CodeLlama-7b CodeLlama-7b (chat)
CodeLlama-34b CodeLlama-34b (chat)
GPT-3.5-turbo GPT-3.5 (chat)
Tasks (17):
Edit /
Augment
Code
4 Algorithmic
Problems or
Data
Manipulation 9
4
Metrics:
Time to # Tasks Preference
complete task completed and usage
Figure 2: We use RealHumanEval to conduct a user study where each participant is randomly assigned one
form of LLM support (chat or autocomplete) with a specific model or no LLM support. The participant then
solves a set of coding tasks randomly chosen from 17 tasks and is evaluated on different metrics, including
time and performance.
to familiarize themselves with the interface. Participants are given 35 minutes to complete as many tasks as
possible. If 10 minutes pass and the participant has not completed the task, a button appears to provide the
option to skip the task.
Tasks. We designed 17 coding tasks for the platform that can be categorized into three categories: (a)
algorithmic problems from HumanEval (e.g., solve interview-style coding), (b) data manipulation problems
(e.g., wrangle input dataframe into desired output), and (c) editing and augmenting code tasks (e.g., fill in
provided code scaffold to achieve desired behavior). While the set of tasks does not evaluate all types of
coding problems exhaustively, they do capture tasks of varying difficulty and solutions of varying length, as
well as the use of different programming skills, leading to varying opportunities to benefit from LLM support.
We chose 17 tasks to build diversity across tasks while being able to collect enough samples per task. We
ensured that no LLM model considered in the study, in addition to GPT-4, could solve all tasks perfectly, to
ensure that programmers would not simply accept all LLM suggestions and that each task could be solved in
under 20 minutes by an experienced programmer (validated through pilots with the authors and volunteer
participants), to ensure that these were reasonable questions to consider for a user study. These 17 tasks are
distributed into five sets, where each set consists of a different mix of task types in varying orders but shares
the first two tasks. Each participant is randomly assigned to one of these sets. The LLMs are not aware of
the task descriptions unless the programmer types them in the editor or chat window; this is to simulate the
real world where the task description represents the programmer’s hidden true intent. We provide examples
of the coding tasks in Appendix B.
Conditions. For the autocomplete conditions, we chose base LLM models that naturally generate next-word
predictions, whereas the “chatty” variants of the base models are employed for the chat conditions. To
evaluate the effect of LLM capabilities, we selected three types of models that demonstrate clear gaps
6in performance on existing benchmarks (as shown in Figure 9). In total, we selected 6 LLMs for our
study: 4 from the Code Llama family [Rozière et al., 2023] (CodeLlama-7b, CodeLlama-7b-instruct,
CodeLlama-34b, CodeLlama-34b-instruct), along with two models from the GPT series [Brown et al.,
2020] (GPT-3.5-turbo and GPT-3.5-turbo-instruct). To avoid confusion, we refer to the autocom-
plete conditions by the base name of the model: CodeLlama-7b, CodeLlama-34b and GPT-3.5 (refers to
GPT-3.5-turbo-instruct);andthechatconditionsbythebasenameofthemodelwithchat: CodeLlama-7b
(chat) (refers to CodeLlama-7b-instruct), CodeLlama-34b (chat) (refers to CodeLlama-34b-instruct)
and GPT-3.5 (chat) (refers to GPT-3.5-turbo). Discussions related to the specific choices of parameters,
system prompts, and other model considerations are provided in Appendix C.
Participants. We recruited a total of 229 participants from university mailing lists and social media to
capture a range of coding experiences. We verified that participants were above 18 years of age, resided in
the United States, and correctly completed a simple Python screening question. Out of the 229 participants,
we filtered out those who did not complete any task or did not write code for a period of 15 minutes during
the study to arrive at 213 final participants. Of the 229 participants, 34% identify as Female, in terms of
occupation 79% are Computer Science Students, 13% work in Software Development and 7% work in AI,
only 34% of participants have less than 2 years of professional programming experience, 11% had never used
any form of AI for coding while 67% of participants use AI at least once a week for coding. Participants
were provided with a $15 Amazon gift card as compensation. This study was approved by institutional IRB
review.
User study metrics. To quantify the benefits of LLM assistance on the number of tasks completed and
time to task success, we report the gap between each condition where some form of LLM assistance was
provided and the control no LLM condition, which we denoted as ∆. For example, for time to task success,
if ∆<0 for LLM support, this indicates that participants took less time to complete tasks with the LLM.
In addition to the quantitative metrics, we also ask post-study questions to obtain participants’ subjective
measures of their interactions with the LLM: we ask participants to rate the helpfulness of the LLM on a
scale of [1,10], measure cognitive load via a series of questions from the NASA Task Load Index (TLX) [Hart,
2006], and ask participants to describe how the LLM support provided (if any) was helpful and how it could
be improved.
5 Results
We report results for data collected from 213 participants split across the seven conditions; since condition
assignment is random, each condition has around 25 to 35 participants (except for No LLM, which has 39
participants). Participants completed a total of 771 coding tasks (mean of 3.6 tasks per person) on average
in 334 seconds (std=238 seconds), were shown 5204 autocomplete suggestions (|D |), with an average 11.3%
ac
acceptance rate, and received 775 messages from the chat LLMs (|D |), with 29.6% of messages having at
chat
least one copy event. A more in-depth analysis of both datasets and supplementary results is in Appendix D.
In the following analyses, we conduct ordinary least squares regressions with Benjamini-Hochberg correction
and use a significance level of 0.05. We pre-registered our study design prior to data collection but not the
full analysis plan https://aspredicted.org/blind.php?x=K3P_K1J. Due to the limit on the number of
participants who completed the task within the timeframe of the study, we thus ended up with fewer number
7GPT-3.5
CodeLlama-34b
CodeLlama-7b
No LLM
100 50 0 50 100 2 1 0 1 2
in Avg Task Duration ( better) in Num Tasks Completed ( better)
(a) Difference in task completion time (in seconds)(b) Difference in number of tasks
grouped by model compared to the No LLM completedcomparedtotheNoLLM
condition. condition.
500 +12% 5
+11%
+5%
450
-7% 4 -14% -8% -11%
400 -14% -18%
-19% -18% -21%
350 3 500 +12% No LLM
300 450 CodeLlama7b
-7% CodeLlama7b (chat)
250 2 400 -14% CodeLlama34b
-19% -18% -21% CodeLlama34b (chat)
350
200 GPT-3.5
1
300 GPT-3.5 (chat)
150
250
100 0
Condition 200 Condition
(c) Average task completion time (in (d)Average15n0umberoftaskscompletedby
seconds) by condition. condition.100
Condition
Figure3: WemeasuretheeffectofLLMsupportonuserstudyperformanceonmeantaskdurationinseconds
(a,c) and number of tasks completed across model type (b,d). In (a) and (b), we compute ∆, the difference
between each model type—aggregating conditions corresponding to the same model type, e.g., Codellama7b
and Codellama7b (chat)—and the No LLM condition for each metric. In (c) and (d), we break down the same
metrics for each of the seven conditions and mark the percentage improvement over the No LLM condition.
We observe that better LLM support can improve task completion time, but not necessarily increase the
number of tasks completed. Error bars denote standard errors—the standard deviation divided by the square
root of the sample size (i.e., across participants), where each participant contributes a single data point.
of participants in the final dataset than we originally anticipated being able to collect (i.e., 30 per condition
instead of 50 per condition). As a result, we opted to pool together data from the same model class to study
both hypotheses. All other additional analyses in this work are exploratory and were not pre-registered. The
code for the data and analyses can be found at: https://github.com/clinicalml/realhumaneval.
Providing LLM assistance reduces the amount of time spent coding. To measure the productivity
gainsofLLMassistancetoprogrammers,welookattwometrics: theamountoftimespentcoding(inseconds)
andthenumberoftaskscompleted. Wefirstdistillourobservationsforeachmetricbycomparingperformance
foreachmodeltype(i.e., combiningautocompleteandchat models)againstthe No LLMcondition.1 Asshown
in Figure 3(a), we find that compared to the No LLM setting where participants spent an average of 400
seconds per task, both GPT-3.5 and CodeLlama-34b models reduce the amount of time spent per task by an
average of 78 and 64 seconds respectively (p=0.04 and p=0.12). In contrast, CodeLlama-7b models slightly
1InAppendixD,werepeatedthesameanalysescontrollingfortaskdifficultyandobservedthesametrends.
8
)s(
noitaruD
ksaT
detelpmoC
sksaT
)s(
noitaruD
ksaTGPT-3.5
Autocomplete
CodeLlama-34b
Chat
CodeLlama-7b
0 10 20 30 40 50 0 5 10 15 20 0 2 4 6 8
Chat: % Code Copied Autocomplete: % Suggest Accepted AI Helpfulness( better)
(a) Percentage of chat messages copied (b) Percentage of autocom-(c) Rating of LLM helpfulness across the six
for chat conditions. plete suggestions accepted. LLM conditions (autocomplete and chat).
Figure 4: Measuring participant preferences of different models by the amount of interaction with chat (a) or
autocomplete systems (b), with standard error. We find that preference judgments align with the reported
helpfulness of the LLM assistant post-study (c); however, these preferences do not necessarily align with their
actual task performance.
increase the average time spent on a task by 10 seconds. However, we do not observe statistical differences
across any of the conditions in the number of tasks completed, as shown in Figure 3(b), meaning no form of
LLM support allowed programmers to solve more problems than they otherwise would have on their own. We
hypothesize that there was no significant increase in the number of tasks completed given the short duration
of the user study (35 minutes) and the amount of time it takes to complete each task, though we do observe
an increase in the number of tasks attempted.
We now consider how our observations using RealHumanEval implicate the broader code LLM evaluation
landscape, specifically the use of (1) static benchmarks and (2) human preference metrics.
(1) Are LLM performance on static benchmarks informative of user productivity with LLM
assistance? We find that improvements in model-specific evaluations on benchmarks also improve human
performance on both productivity metrics in the user study (i.e., CodeLlama-7b models led to the least
number of tasks completed, while GPT-3.5 models led to the most). Interestingly, this trend holds even when
considering metrics with chat and autocomplete separately, in Figure 3(c-d). However, significant gaps in
benchmark performance result in relatively indistinguishable differences in terms of human performance.
For instance, CodeLlama-34b (chat) is 19% better over CodeLlama-7b (chat) models on HumanEval, and
participants are 22.8% (95% CI [2.8, 38.7]) faster on average to complete a task with 34b vs 7b. Yet, GPT-3.5
(chat) model outperforms CodeLlama-34b (chat) by 85% on HumanEval, and yet participants equipped
with GPT-3.5 (chat) models are only 8.3% (95% CI [-11.2, 24.6]) faster than those with CodeLlama-34b
(chat). While we do not necessarily expect performance gaps to be consistent, this finding suggests that,
after a certain point, additional gains on static benchmarks may not translate to practical utility.
(2) Do human preference metrics align with actual human performance? We also consider the
implications of human-centric proxy metrics, which include programmer preference for the LLM assistant’s
suggestionsonautocompleteandchat(e.g.,theaveragesuggestionacceptancerateandtheaveragecopies-per-
responserespectively). Wefindthattheseproxymetricsarenotalignedwiththebasemodel’sorprogrammer’s
performance when provided with LLM support. Recall that, in the user study, we found CodeLlama-34b
reduced the amount of time spent coding over CodeLlama-7b. However, as shown in Figure 4(a), we find that
suggestions from CodeLlama-34b are less likely to be accepted at 5% compared to 15% and 9% for GPT-3.5
and CodeLlama-7b (p<0.001 and p=0.19). The same ordering occurs for the percentage of chat messages
copied (27% versus 35% and 29%, though not significant). The participants’ qualitative responses, discussed
9in Section E, suggest factors that may have contributed to these preferences, including a perceived lack of
context in CodeLlama-34b suggestions and a slight increase in latency in CodeLlama-34b (chat) responses.
These results suggest that human preference metrics may be more easily affected by various external factors
and are not always indicative of downstream user performance.
5.1 Additional User Study Observations
Chatsupportisperceivedtobemorehelpfulthanautocompletesupport. Eventhoughautocomplete
and chat variants obtained similar performance on static benchmarks and participant performance in both
conditions conditioned on a model type was relatively similar, we observe that chat models are rated by
participantsinthepost-studyquestionsassignificantlymorehelpfulthanautocompletemodels(p<0.001),as
showninFigure4(c). Onasimilarnote,wefoundthatbothCodeLlama-7bandCodeLlama-34bautocomplete
models scored highest on user frustration in the TLX survey compared to their chat variants, as shown in
Table 1. We did not observe any notable differences among other TLX questions.
The benefits of LLM assistance vary by task
type. We also analyze the time spent on each task
LLM-assisted
category, comparing when participants have access False
Algorithmic Problems True
to LLM assistance versus the control condition. As
shown in Figure 5, we find suggestive evidence that
LLMassistancewasparticularlyeffectiveinreducing Data Manipulation
the time programmers needed to solve data manipu-
lation tasks, by 28.35%, and slightly less so for prob-
Edit/Augment Code
lems that required editing and augmenting existing
code, by 13.48%. In contrast, we found that LLMs 250 300 350 400 450 500
Avg task duration (s)
were unhelpful on algorithmic problems, increasing
Figure5: AveragetaskdurationwithandwithoutLLM
the amount of time spent by 11.7%. A breakdown
assistance with standard error by task category.
by individual task is in Appendix D.
6 Discussion
In this work, we introduce RealHumanEval, a human-centric evaluation platform for code LLMs, and conduct
a user study to measure programmer productivity assisted by different LLMs. The user study quantifies
productivity gains when providing users with LLM support, while highlighting nuances that arise due to the
form of LLM support and task type, and provides guidance for future evaluations using static benchmarks
and human-centric proxy metrics. We now discuss avenues for future work and the limitations of our study.
Recommendations for future work. To bridge the gap between offline and human evaluations, we
analyzed a post-study survey question on how coding assistants could be improved, which we summarize
here and described in further detail in Appendix E. A predominant response from participants is that LLMs
across both forms of interactions struggle to infer the appropriate context to provide the most useful support
from the information available, highlighting the need for benchmark development to capture settings where
LLMs need to infer intent from partial or fuzzy instructions. The results also underscore the importance of
including users in the evaluation of LLMs; as such, we recommend the community leverage and build on
10RealHumanEval to evaluate new LLMs’ coding abilities via human studies. Beyond evaluation, there are
opportunities to improve autocomplete and chat assistants to be better programming partners [Wu et al.,
2023]. For example, autocomplete systems might benefit from personalization of when participants would
benefit from suggestions and dynamically adjusting the length, while chat-based systems could be improved
to have better, more tailored dialogue experience and better integration with other aspects of the interface.
As a byproduct of our study, we collected datasets of user interactions with both forms of support that can
be leveraged as signals of user preferences and behavior patterns to improve code LLMs.
Limitations. Firstly, we acknowledge that a set of 17 coding tasks does not span the entire set of
tasks a professional programmer might encounter in their work and encourage future work to leverage
RealHumanEval to conduct further studies. Second, the coding tasks we used are of short duration, while
real-world programming tasks can take hours to months. This presents a trade-off in study design: short
tasks allow us to evaluate with more participants and models in a shorter period but give us a less clear
signalcomparedtolonger-termtasks. Third, RealHumanEvaldoesnotfullyreplicateallfunctionalityexisting
products such as GitHub Copilot may have so the study may underestimate exact productivity benefits. Such
products are complex systems comprising more than a single LLM, where many details are hidden and thus
not easily replicable. We hope our work can help others build such alternatives in a more open-source manner.
Finally, we only performed preliminary investigations into the code quality (see Appendix D.4), thus further
work is needed to understand the readability and performance of code written with and without LLMs.
Acknowledgments
WethankWayneChi,KatieCollins,HunterLang,ChristinaMa,andShannonShenforprovidingfeedbackon
the manuscript. HM is thankful for the support of the MIT-IBM Watson AI Lab. AT was supported in part
bytheNationalScienceFoundationgrantsIIS1705121, IIS1838017, IIS2046613, IIS2112471, andfundingfrom
Meta, Morgan Stanley, Amazon, and Google. Any opinions, findings and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these
funding agencies.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv preprint
arXiv:2303.08774, 2023.
Amazon. Ml-powered coding companion – amazon codewhisperer, 2022. URL https://aws.amazon.com/
codewhisperer/.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732, 2021.
Shraddha Barke, Michael B James, and Nadia Polikarpova. Grounded copilot: How programmers interact
with code-generating models. arXiv preprint arXiv:2206.15000, 2022.
11Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk,
andIdanGazit. Takingflightwithcopilot: Earlyinsightsandopportunitiesofai-poweredpair-programming
tools. Queue, 20(6):35–57, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,
Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e: a scalable and
polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering,
2023.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021.
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao
Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for
evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.
Bhavya Chopra, Ananya Singha, Anna Fariha, Sumit Gulwani, Chris Parnin, Ashish Tiwari, and Austin Z
Henley. Conversational challenges in ai-powered data science: Obstacles, needs, and design opportunities.
arXiv preprint arXiv:2310.16164, 2023.
Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han Liu, and Saleema Amershi.
Aligning offline metrics and human judgments of value for code generation models. In Findings of the
Association for Computational Linguistics: ACL 2023, pages 8516–8528, 2023.
Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis.
Largelanguagemodelsofcodefailatcompletingcodewithpotentialbugs. arXivpreprintarXiv:2306.03438,
2023.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from
human feedback. arXiv preprint arXiv:2305.14387, 2023.
Github. Github copilot - your ai pair programmer, 2022. URL https://github.com/features/copilot.
Ken Gu, Ruoxi Shang, Tim Althoff, Chenglong Wang, and Steven M Drucker. How do analysts understand
and verify ai-assisted data analyses? arXiv preprint arXiv:2309.10947, 2023.
Sandra G Hart. Nasa-task load index (nasa-tlx); 20 years later. In Proceedings of the human factors and
ergonomics society annual meeting, volume 50, pages 904–908. Sage publications Sage CA: Los Angeles,
CA, 2006.
Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy,
and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv
preprint arXiv:2308.10620, 2023.
12CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthikNarasimhan.
Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.
Majeed Kazemitabaar, Xinying Hou, Austin Henley, Barbara J Ericson, David Weintrop, and Tovi Grossman.
How novices use llm-based code generators to solve cs1 coding tasks in a self-paced learning environment.
arXiv preprint arXiv:2309.14049, 2023.
Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and
Shafiq Joty. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation,
translation and retrieval. arXiv preprint arXiv:2303.03004, 2023.
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel
Fried,SidaWang,andTaoYu. Ds-1000: Anaturalandreliablebenchmarkfordatasciencecodegeneration.
In International Conference on Machine Learning, pages 18319–18345. PMLR, 2023.
Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty,
and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of chatgpt on benchmark
datasets. arXiv preprint arXiv:2305.18486, 2023.
Jenny T Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, and Thomas
Zimmermann.Cangpt-4replicateempiricalsoftwareengineeringresearch? arXivpreprintarXiv:2310.01727,
2023.
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really
correct? rigorousevaluationoflargelanguagemodelsforcodegeneration. arXiv preprint arXiv:2305.01210,
2023.
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano,
MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU.
CodeXGLUE:Amachinelearningbenchmarkdatasetforcodeunderstandingandgeneration. InThirty-fifth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.
URL https://openreview.net/forum?id=6lE4dQXaUcb.
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are
few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pages 1384–1403, 2022.
Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. Reading between the lines: Modeling
user behavior and costs in ai-assisted programming. arXiv preprint arXiv:2210.14306, 2022.
Hussein Mozannar, Valerie Chen, Dennis Wei, Prasanna Sattigeri, Manish Nagireddy, Subhro Das, Ameet
Talwalkar, and David Sontag. Simulating iterative human-ai interaction in programming with llms. In
NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,
XiangruTang, LeandroVonWerra, andShayneLongpre. Octopack: Instructiontuningcodelargelanguage
models. arXiv preprint arXiv:2308.07124, 2023.
13Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint
arXiv:2203.13474, 2022.
OpenAI. Chatgpt: Optimizing language models for dialogue, 2022a. URL https://openai.com/blog/
chatgpt/.
OpenAI. Chatgpt: Introducing chatgpt. https://openai.com/blog/chatgpt, 2022b.
Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer productivity:
Evidence from github copilot. arXiv preprint arXiv:2302.06590, 2023.
JamesPrather,BrentN.Reeves,PaulDenny,BrettA.Becker,JuhoLeinonen,AndrewLuxton-Reilly,Garrett
Powell, James Finnie-Ansley, and Eddie Antonio Santos. “it’s weird that it knows what i want”: Usability
and interactions with copilot for novice programmers. ACM Trans. Comput.-Hum. Interact., 31(1), nov
2023. ISSN 1073-0516. doi: 10.1145/3617367. URL https://doi.org/10.1145/3617367.
replit. Meet ghostwriter, your partner in code., 2023. URL https://replit.com/site/ghostwriter.
Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. The programmer’s
assistant: Conversational interaction with a large language model for software development. In Proceedings
of the 28th International Conference on Intelligent User Interfaces, pages 491–514, 2023.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint
arXiv:2308.12950, 2023.
Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic bug fixing
performance of chatgpt. arXiv preprint arXiv:2301.08653, 2023.
Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating the
usability of code generation tools powered by large language models. In CHI Conference on Human Factors
in Computing Systems Extended Abstracts, pages 1–7, 2022.
Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson
Tan, Baishakhi Ray, Parminder Bhatia, et al. Recode: Robustness evaluation of code generation models.
arXiv preprint arXiv:2212.10264, 2022.
Tongshuang Wu, Kenneth Koedinger, et al. Is ai the better programming partner? human-human pair
programming vs. human-ai pair programming. arXiv preprint arXiv:2306.05153, 2023.
Tao Xiao, Christoph Treude, Hideaki Hata, and Kenichi Matsumoto. Devgpt: Studying developer-chatgpt
conversations. arXiv preprint arXiv:2309.03914, 2023.
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao,
Li Zhu, Shuiguang Deng, et al. Codescope: An execution-based multilingual multitask multidimensional
benchmark for evaluating llms on code understanding and generation. arXiv preprint arXiv:2311.08588,
2023.
John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and
benchmarking interactive coding with execution feedback. arXiv preprint arXiv:2306.14898, 2023.
14Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang
Lou. Large language models meet nl2code: A survey. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 7443–7464, 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685, 2023.
MingZhu,AneeshJain,KarthikSuresh,RoshanRavindran,SindhuTipirneni,andChandanK.Reddy. Xlcost:
A benchmark dataset for cross-lingual code intelligence, 2022. URL https://arxiv.org/abs/2206.08474.
Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh
Sittampalam, and Edward Aftandilian. Productivity assessment of neural code completion. In Proceedings
of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 21–29, 2022.
15A User study details
A.1 RealHumanEval interface screenshots
We show examples of the RealHumanEval web interface used in the study: autocomplete conditions (Figure 6
and Figure 7) and chat conditions Figure 8). Note that the interface is the same as that of the autocomplete
conditions for the no LLM condition, except there is no LLM to provide any inline code suggestions.
Figure 6: Screenshot of the autocomplete LLM-assistance interface in our user study.
Figure 7: Another screenshot of the autocomplete LLM-assistance interface in our user study.
16Figure 8: Screenshot of the chat LLM-assistance interface in our user study.
A.2 User Study Instructions
Before participants enter the main interface, they are provided with the following text:
After you fill out the information here, click the Start Experiment button to proceed.
Please DO NOT refresh or press back as you may lose a fraction of your progress, if needed you
can refresh while coding but you will lose your code.
Your name and email will NOT be shared with anyone or used in the study.
Note that there is a chance the interface may not have AI, that is not a bug.
By performing this task, you consent to share your study data.
In all conditions, a pop-up is displayed with the following instruction:
Welcome to the user study! You will first complete a tutorial task to make you familiar with the study.
• You will be writing code in Python only and use only standard python libraries and only numpy and
pandas.
• After the tutorial task, you will have 35 minutes total where you will try to solve as many coding tasks
as possible one at a time.
• ItisNOTallowedtouseanyoutsideresourcestosolvethecodingquestions(e.g.,Google,StackOverflow,
ChatGPT), your compensation is tied to effort only.
17A.2.1 Autocomplete Condition
You will write code in the interface above: a code editor equipped with an AI assistant that provides
suggestions inline.
• The AI automatically provides a suggestion whenever you stop typing for more than 2 seconds.
• You can accept a suggestion by pressing the key [TAB] or reject a suggestion by pressing [ESC].
• You can also request a suggestion at any time by pressing [CTRL+ENTER] (Windows) or [CMD+ENTER]
(Mac).
• You can run your code by pressing the run button and the output will be in the output box at the
bottom in grey.
• Press the submit button to evaluate your code for correctness. You can submit your code
as many times as you wish until the code is correct.
• If you cannot solve one of the tasks in 10 minutes, a button “Skip Task”, only press this button if you
absolutely cannot solve the task.
Note: please be aware that the AI assistant is not perfect and may provide incorrect suggestions. Moreover,
the AI may generate potentially offensive suggestions especially if prompted with language that is offensive.
A.2.2 Chat Condition
You will write code in the interface above: a code editor equipped with an AI assistant chatbot in the right
panel.
• The AI chatbot will respond to messages you send and incorporate previous messages in its response.
The AI does not know what the task is or the code in the editor.
• When the AI generates code in its response, there is a COPY button that will show up above the code
segment to allow you to copy.
• You can test your code by pressing the run button and the output will be in the output box at the
bottom in grey.
• Press the submit button to evaluate your code for correctness. You can submit your code
as many times as you wish until the code is correct.
• If you cannot solve one of the tasks in 10 minutes, a button “Skip Task”, only press this button if you
absolutely cannot solve the task.
Note: please be aware that the AI assistant is not perfect and may provide incorrect suggestions. Moreover,
the AI may generate potentially offensive suggestions especially if prompted with language that is offensive.
18A.2.3 No LLM Condition
You will write code in the interface above: a code editor.
• You can run your code by pressing the run button and the output will be in the output box at the
bottom in grey.
• Press the submit button to evaluate your code for correctness. You can submit your code
as many times as you wish until the code is correct.
• If you cannot solve one of the tasks in 10 minutes, a button “Skip Task”, only press this button if you
absolutely cannot solve the task.
A.2.4 Post-Study Questionnaire
• Thinking of your experience using AI tools outside of today’s session, do you think that your session
today reflects your typical usage of AI tools?
• How mentally demanding was the study? (1-20)
• How physically demanding was the study? (1-20)
• How hurried or rushed was the pace of the study? (1-20)
• How successful were you in accomplishing what you were asked to do? (1-20)
• How hard did you have to work to accomplish your level of performance? (1-20)
• How insecure, discouraged, irritated, stressed, and annoyed were you? (1-20)
• Overall, how useful/helpful was the AI assistant? (1-10)
• In which ways was the AI assistant helpful? What did it allow you to accomplish? (free-text)
• How could the AI suggestions be improved? (free-text)
• Additional comments (Optional): anything went wrong? any feedback? (free-text)
To ensure consistency in responses to scale-based questions, we labeled 1 with “low” and either 10 or 20 with
“high” depending on the question.
B Task Design
B.1 Task categories
Algorithmic coding problems: Many coding tasks require programmers to implement algorithmic
thinking and reasoning and are widely used to evaluate programmers in coding interviews. To identify
algorithmic coding problems, we sample representative problems from the HumanEval dataset Chen et al.
[2021]. Given gpt-3.5-turbo’s high performance on this type of problem, we ensure that we also include
19problems where it fails to solve the problem on its own. We evaluated each question using test cases
from the HumanEval dataset. We included the following problem ids from HumanEval: is_bored 91,
is_multiply_prime 75, encode_message 93, count_nums 108, order_by_points 145, even_odd_count 155,
sum_product 8, triple_sum_to_zero 40. In addition, we created a custom problem called event_scheduler.
All tasks with unit tests will be released.
Editing and augmenting existing code: When working with existing repositories, programmers will
oftenneedtoeditandbuildoncodethatmayhavebeenwrittenbyothers[Sobaniaetal.,2023]. Wedesigned
questions where participants are either provided with either code scaffold to fill in or with code body that
they are asked to modify the functionality of. When designing such questions, we take care to avoid common
implementations (e.g., a traditional stack and queue) that would have appeared in LLM training data. We
also constructed a set of test cases for each question. The four problem names are calculator, tokenizer, login
authenticator and retriever.
For example, here is the login authenticator problem description:
Your goal is to implement the LoginAuthenticator class, which will be used to authenticate
users of a system. The class will include the following methods:
_hash_password (Private Method): Creates a hash of a given password. Accepts a password (string)
and returns the hashed password using any hashing technique.
add_user Method: Adds a new user to the system with a username and a password. It checks if the
username already exists, hashes the password if it does not, and stores the credentials. Returns
True if successful.
remove_user Method: Removesauserfromthesystembydeletingtheirusernameentryfromself.user_credentials
if it exists. Returns True if successful.
change_password Method: Changes a user’s password after authenticating the user with their old
password. If authenticated, it hashes the new password and updates self.user_credentials.
Returns True if successful.
The programmer is given the following initial code:
class LoginAuthenticator:
def __init__(self):
# DO NOT CHANGE
self.user_credentials = {} # dictionary for username: hashed_password
def _hash_password(self, password):
# WRITE CODE HERE
return
def add_user(self, username, password):
# WRITE CODE HERE
return
20def authenticate_user(self, username, password):
# DO NOT CHANGE
#Checks if the given username and password are valid
if username not in self.user_credentials:
return False
return self.user_credentials[username] == self._hash_password(password)
def remove_user(self, username):
# WRITE CODE HERE
return
def change_password(self, username, old_password, new_password):
# WRITE CODE HERE
return
Data science tasks: Given the increased usage of data in many engineering disciplines, programmers
are often involved in data science problems. We design data science problems inspired by the DS-1000
dataset Lai et al. [2023], where participants need to perform multiple data manipulation and wrangling
operations and return a resulting Pandas dataframe. To ensure that an LLM cannot achieve perfect
accuracy on its own, we only show an example of the input and target dataframes without providing specific
instructions on each operation. The code will be evaluated based on the correctness of the dataframe in an
element-wise fashion. The four problem names are table_transform_named, table_transform_unnamed1,
table_transform_unnamed2 and t_test.
Here is for example the problem table_transform_unnamed1:
Given the input pandas DataFrame:
col1 col2 col3 col4 col5
0 6 1 5.38817 3 2
1 9 2 4.19195 5 8
2 10 8 6.8522 8 1
3 6 7 2.04452 8 7
4 1 10 8.78117 10 10
Transform this DataFrame to match the following output structure, recognizing the relationship
between the input and output DataFrames:
col1 col2 col3
0 6 2 8.38817
1 15 3 9.19195
2 25 9 14.8522
3 31 8 10.0445
4 32 11 18.7812
0 0 0 0
0 0 0 0
21Implement a function named transform_df that takes the input DataFrame and returns the
transformed DataFrame, discovering and applying the patterns between them.
The programmer is given the following initial code:
import pandas as pd
from io import StringIO
# Original dataset
data = ’’’
col1,col2,col3,col4,col5
6,1,5.3881673400335695,3,2
9,2,4.191945144032948,5,8
10,8,6.852195003967595,8,1
6,7,2.0445224973151745,8,7
1,10,8.781174363909454,10,10
’’’
# Read the dataset into a DataFrame
df = pd.read_csv(StringIO(data))
def transform_df(df):
# Your code here
print(transform_df(df))
B.2 Task organization
We created five task sets where we fixed the first task (in addition to the tutorial sum_product task) and
varied the remaining tasks randomly ensuring a split across the categories. The five sets are:
1. TaskSet1: even_odd_count,triple_sum_to_zero,table_transform_named,tokenizer,encode_message,
t_test, event_scheduler.
2. Task Set 2: even_odd_count, is_bored, login_authenticator, is_multiply_prime, count_nums, ta-
ble_transform_named, calculator.
3. TaskSet3: even_odd_count,count_nums,calculator,table_transform_unnamed2,login_authenticator,
encode_message, is_bored.
4. TaskSet4: even_odd_count,order_by_points,retriever,triple_sum_to_zero,tokenizer,event_scheduler,
encode_message.
5. Task Set 5: even_odd_count, is_multiply_prime, table_transform_unnamed1, t_test, is_bored,
order_by_points, triple_sum_to_zero.
C LLM Details
Here we provide links to model weights (where applicable) and any additional details.
2280
70
60
50
40
30
20
HumanEval MBPP
CodeLlama7b CodeLlama34b GPT-3.5
CodeLlama7b (chat) CodeLlama34b (chat) GPT-3.5 (chat)
Figure 9: Pass@1 of LLM models and their chat variants on two canonical benchmarks, HumanEval and
MBPP (results from [Rozière et al., 2023, Liu et al., 2023]), showing that CodeLlama-7b models perform
worse than CodeLlama-34b models, which are less performant than GPT-3.5 models. We investigate whether
these trends are reflected in human metrics.
CodeLlama (7b, 34b) and CodeLlama Instruct (7b, 34b). Accessed from https://api.together.
xyz/. Note that the base model variants are no longer available from this source.
GPT-3.5-turbo. Specific model version is gpt-3.5-turbo-0613. Accessed through the OpenAI API.
GPT-3.5-turbo-instruct. Accessed through the OpenAI API.
For all LLMs, we used a temperature setting of 1 to ensure varied responses.
For autocomplete LLMs, each time we query the LLM to generate a suggestion, we sample a random number
according to a normal distribution with mean 64 tokens and std 15 truncated to the range [10,120] and set
the max_token parameter to that sampled value. We used the mean value of 64 in accordance with Personal
Copilot HuggingFace implementation 2. We allow the max_token length to be random so that we have access
to future data to determine the optimal length of suggestions, this is because base LLMs are not trained
with an EOS token and thus do not know when to stop generating code. For the chat LLMs, we set the
max_token parameter to 512 tokens constant.
Why we did not select other model candidates. Of the CodeLlama models available to use at the
time of the study, we omitted CodeLlama-13b. We did not select CodeLlama-13b as its performance on
HumanEval is very similar to the 7b variant. Additionally, CodeLlama-70b had not been released when we
conducted the study. We did not include GPT-4 because of the lack of availability of the completion-based
variant via API.
C.1 Prompts used
We used the following system prompt for all chat-based LLMs:
You are an expert Python programmer, be helpful to the user and return code only in
Python.
For autocomplete-based LLMs, the first line of the prompt is always the following:
2https://huggingface.co/blog/personal-copilot
23
1@ssaP# file is main.py, ONLY CODE IN PYTHON IN THIS FILE
These prompts help to ensure that LLM responds in Python.
D Additional Results
D.1 Dataset Analysis
We post-processed both datasets to ensure they did not reveal any identifying information about participants
or contain harmful language.
Autocomplete dataset. Recall that users had the option to request suggestions via hotkey or were
provided the suggestion after some time. As shown in Figure 10, participants are much more likely to accept
suggestions if they request them. Interestingly, CodeLlama-34b suggestions seemed to be more preferred than
CodeLlama-7b when requested.
GPT-3.5
CodeLlama-34b
CodeLlama-7b
0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5
Non-requested: % Suggestion Accepted Requested: % Suggestion Accepted
Figure 10: Comparing the acceptance rate for when participants requested suggestions with when they were
automatically provided with suggestions by the autocomplete system.
Chat dataset. We analyze the 775 chat messages participants sent across the three conditions, as shown in
Figure 11. On average 2.8 messages were sent per task with a length of 100.8 characters. We note that there
is a particularly long tail in terms of words appearing in chat messages because many questions contained
implementation-specific variables. In accordance with our findings that LLMs were most useful for data
manipulation tasks, we also find that participants engaged with LLM support the most for those tasks.
24150
600
100
400
50
200
0 0
5 10 0 500 1000 1500 2000
Number of messages per task Length of Chat Message
500
Algorithmic Problems
400
300
Data Manipulation
200
Edit/Augment Code 100
0
1 2 3 4 5
Number of messages Words appearing in chat msgs
Figure 11: Analysis of the number of messages sent per task (top left), the length of chat messages (top
right), the number of messages sent per task category (lower left), and the frequency of words appearing in
chat messages (lower right).
D.2 Accounting for task difficulty
To facilitate comparisons between different sets of tasks, which may have varying difficulty, the value of each
metric is z-scored within the task set:
M −µ
Mz = i,t M,t
i,t σ
M,t
where Mz is the value of metric M achieved by participant i, z-scored within task set t; µ and σ are
i,t M,t M,t
the mean and standard deviation of metric M for task set t, across all participants. We rerun our analysis for
performance metrics and present results in Figure 12.
25
ycneuqerF ycneuqerF
ycneuqerFGPT-3.5-Turbo
CodeLlama-34b
CodeLlama-7b
Autocomplete
GPT-3.5-Turbo-Instruct Chat
No LLM
CodeLlama-34b-Instruct
CodeLlama-7b-Instruct
No LLM
1.0 0.5 0.0 0.5 1.0 2 1 0 1 2
in Avg Task Duration (z-score) in Num Tasks Completed (z-score)
Figure 12: Performance results across models, z-scored to account for potential variation in task difficulty
across sets.
D.3 Task breakdown
We show whether participants benefited from LLM assistance on an individual task level in Figure 13.
We observe that trends for individual tasks within a category are similar, indicating the importance of
understanding how programmers interact with LLMs for different types of tasks.
26Tutorial
Data
Manipulation
Edit/
Augment
Code
Algorithmic
Problems
Figure 13: Time to task completion with and without LLM assistance, reported by task and grouped by task
category, with standard error.
D.4 Code Quality Metrics
Code Comments. Code written with the assistance of the LLM will inherit some of the characteristics
of the writing style of the LLM. One instance of that is comments in the code written. We investigate the
numberofcommentswrittenbyparticipantsforthedifferenttypesofinteractionwiththeLLM:autocomplete,
chat, or no LLM. We count how many additional comments are in the code participants write compared
to the number of comments in the provided code participants complete. Participants in the autocomplete
conditionswrote0.85±0.1additionalcomments,inthechatconditionwrote0.59±0.08commentsandthose
27in the No LLM condition wrote 0.41 ± 0.13 comments. Participants writing code with autocomplete LLM
write twice as many comments as those without an LLM (p=3e−6). There are two possible explanations
for this increase: first, programmers usually prompt the LLM with inline comments to get a suggestion they
desire, and second, we often observe that code generated by LLMs is often heavily commented. This indicates
that we can potentially differentiate code written by programmers with LLM assistance by the number of
comments in the code.
D.5 TLX Results
We measure cognitive load via a series of questions from the NASA Task Load Index (TLX) Hart [2006],
summarized in Table 1.
Model Frustration Performance TemporalDemand PhysicalDemand Effort MentalDemand
GPT3.5 8.28 9.03 12.81 4.64 11.81 11.25
CodeLlama-34b 13.52 8.00 11.31 5.07 10.79 10.86
CodeLlama-7b 11.88 6.50 13.88 4.88 10.65 14.50
GPT3.5(chat) 10.09 9.28 12.19 4.94 10.88 12.09
CodeLlama-34b(chat) 11.04 8.00 13.44 5.16 11.40 12.88
CodeLlama-7b(chat) 9.54 7.43 12.57 6.75 11.93 11.82
NoLLM 9.62 7.56 13.51 5.95 11.79 12.10
Table 1: TLX scores across conditions.
D.6 Task Completion Time
Mean
Median
100
80
60
40
20
0
0 250 500 750 1000 1250 1500 1750
Time to Complete a Task (seconds)
Figure14: Histogramdepictingthedistributionoftaskcompletiontimesacrossallparticipantsandconditions.
The histogram is overlaid with dashed lines representing key statistical measures: the mean (red) and the
median (green).
28
ycneuqerFGPT-3.5
CodeLlama34b
CodeLlama7b
GPT-3.5 (chat)
CodeLlama34b (chat)
CodeLlama7b (chat)
No LLM
400 200 0 200 400 600 800
in Avg Task Duration ( better)
Figure 15: Violin plot of the difference in average task duration times (in seconds) between the No-LLM
condition and all other conditions.
No LLM
700 CodeLlama7b
CodeLlama7b (chat)
CodeLlama34b
600
CodeLlama34b (chat)
GPT-3.5
500
GPT-3.5 (chat)
400
300
200
tutorial 1 2 3 4 5
Task Index Solved
Figure 16: For each of the seven conditions, we plot the average time for participants to complete the tutorial
task, the first task they solved, the second task they solved, and so on.
29
)s(
ksaT
etelpmoC
ot
emiTExamples of helpful interactions
Actionable response to question Brainstorm test cases
User
how would I get the day from this2019-03-06 def even_odd_count(num):
where I want thenumber 6 even_count = 0
odd_count = 0
while num > 0:
Chatbot remainder = num % 10
if remainder % 2 == 0:
You can use the `day` attribute of the even_count += 1
`datetime.date` class to get the day of the month. else:
Here's an example: odd_count += 1
num = num // 10
import datetime
date = datetime.date(2019, 3, 6) returneven_count, odd_count
day = date.day print(even_odd_count(2468))
print(day) # prints 6 print(even_odd_count(1357))
print(even_odd_count(111))
Examples of unhelpful interactions
Verbose explanation, no code Incomplete, illogical generation
User
Write python code to determine whether a
numberisthe product of3prime numbers def is_multiply_prime(a):
primes =
cond= True
Chatbot iflen(str(a))%2!=0:
To determine whether a number is the product of 3 prime returnFalse
numbers, we can followthesteps below: foriinrange(len(str(a))):
1. Create a function `is_product_of_3_primes(num)` that primes= 0
accepts an integer `num`as a parameter. start= i
2. Initialize a variable `count`to 0 to keep track of while(a%int(str(a)[start
thenumber of prime factors.
3. Create a helper function `is_prime(n)` that accepts an
integer `n`and checkswhetherit isa prime number.
...
Figure 17: Examples from of helpful and unhelpful chat and autocomplete interactions from the user study.
While these examples showcase how LLM assistance can improve programmer productivity (e.g., by providing
actionable responses and generating test cases), they also highlight how programmer-LLM interactions can
be improved. We discuss design opportunities collected from post-task participant responses in Section E and
provide more examples in Appendix F.
E Design Opportunities
TounderstandthedesignopportunitiesaroundimprovingthecodingassistanceprovidedthroughRealHumanEval,
we analyzed a post-study question on how coding assistants could be improved. Answers to the question were
collected in free response format and were optional, though it was answered by the majority of participants
(174 out of the 213). We summarize participant suggestions into general comments that could apply to both
types of interactions and identify autocomplete- and chat-specific suggestions.3
Both autocomplete and chat models need improved context. A theme that spanned both types of
interactionsandmodeltypeswastheperceivedlackofcontextthattheLLMhadaboutthegeneraltaskwhen
providing either suggestions or chat responses (example shown in Figure 17). While one might expect that a
more performant model might mitigate these concerns, we do not observe a significant decrease in mentions
regarding this issue for GPT-3.5 models compared to both CodeLlama-7b and CodeLlama-34b models. In
general, it may not be obvious how to concisely specify the full “context”—recall that we intentionally
considered a set-up where the LLM is unaware of task T to mimic realistic constraints—but the development
of new interfaces to facilitate context specification and mechanisms to prompt for additional task-specific
information could improve LLM generations. Additionally, further baseline checks can be implemented to
3Weomittheobvious,blanketsuggestionforreplacingtheassistantwithabetterLLM,asmodel-onlyperformanceisoneof
theindependentvariablesinourexperimentandamoreperformantmodelwouldundoubtedlyimprovetheassistanceprovided.
30minimize concerns mentioned by participants (e.g., ensuring that the LLM responses are provided in the
correct programming language, beyond prompting-based approaches implemented in our study). We note
that issues surrounding context control have also been highlighted in prior work [Chopra et al., 2023, Barke
et al., 2022].
Autocomplete-specific suggestions. We highlight the three most commonly mentioned avenues of
improvement across all three model types. (1) Minimize suggestion frequency: Participants noted that the
frequency of suggestions appearing in the code editor could disrupt their train of thought. To address this
issue, it may be preferable to allow participants to turn off the LLM model when they are brainstorming the
next steps or to modify the LLM to detect when participants may not need as frequent suggestions based on
their current coding behavior. Moreover, we observe quantitatively that participants are between 3−10×
more likely to accept an assistant’s suggestion if they requested it, as shown in Figure 10. (2) Dynamic
suggestion length: A common issue with autocomplete interactions noted by participants was the presence of
“incomplete variable definitions or function implementations” and “fragmented code” (e.g., Figure 18 (left)).
AsthisbehaviorisaproductofthefixedlengthofLLMgenerations, autocompleteassistantscanbeimproved
by ensuring the suggestion is complete before terminating generation. (3) More concise suggestions: Finally,
participants also noted that code completions could be more concise, as “it was overwhelming” and “large
chunks of code... start deviating from the task question” (e.g., Figure 18 (right)). It is an open question to
determine the appropriate length for how much code to generate in a context-aware manner.
Chat-specific suggestions. There were three common suggestions shared across models. (1) Responses
should focus on code, rather than explanation: It is well known that chat LLMs tend to generate verbose
responses, which could be detrimental when used as programming assistants. An example of a lengthy
response is in Figure 20. In particular, participants noted the additional time required to read large blocks
of texts and suggested to “get rid of all explanations and stick to code only, unless the user specifies they
want explanations.” Additionally, when focusing on code, participants suggested that the chat assistant could
anticipatealternativeimplementations(2)Improveddialogueexperience: First,insteadofmakingassumptions
aboutpotentiallyambiguouspointsinaprogrammer’squestion(e.g., asinFigure19), aparticipantsuggested
that the LLM “could ask clarifying questions or provide multiple suggestions.” Additionally, in particular
for CodeLlama-7b, participants asked for better consistency across multiple chat messages (e.g., “It wasn’t
able to refer back to previous messages that I had sent when answering a question.”). (3) Better integration
with code editor: Currently, the burden is on the programmer to appropriately prompt the chat assistant
with questions and then to integrate chat suggestions into the code body in the editor. This onus can be
reduced by more readily incorporating “the code and the most recent error, if any, as well as the test case
that generated it in the context for the assistant” and “autocorrect code” based on its suggestions.
Why was CodeLlama-34b less preferred by users? Based on participants’ survey responses, we identify
two potential reasons that might qualitatively explain why CodeLlama-34b was less preferred for both
autocomplete and chat. For autocomplete, the lack of context was a particularly prevalent issue in responses
for CodeLlama-34b, mentioned by 54% of responses, as compared to 32% and 28% of CodeLlama-7b and
GPT-3.5 responses respectively. In particular, participants noted that the generated suggestions were often
irrelevanttothepriorcodeandinthewrongprogramminglanguage. Weshowexamplesofrejectedsuggestions
thatillustrate alackof contextfromparticipants whointeractedwith theCodeLlama-34bmodel inFigure 21.
For chat, while there were no exceptional concerns about lack of context, CodeLlama-34b had the most
mentions of latency as a point of improvement (6 mentions as compared to only 2 and 1 mentions for
31CodeLlama-7b and GPT-3.5 respectively). For example, one participant noted that “the responses are slow
so sometimes it was faster to go off of my memory even if I wasn’t sure if it would work.” Indeed, we found
that CodeLlama-34b response time (about 10 seconds) was on average twice as slow as either CodeLlama-7b
or GPT-3.5 (about 5 seconds). We note that this slight delay did not significantly impact any participant’s
performance metrics.
E.1 Opportunities to use data
Simulating programmer-LLM interaction. The data collected in our study presents an opportunity to
buildandevaluatesimulationenvironmentsthatmimichowprogrammerswritecodewithanLLM.Essentially,
the simulator could be used to more efficiently replicate the results of RealHumanEval and evaluate a wider
set of models. However, despite initial work on simulating programmer-LLM interaction [Mozannar et al.,
2023], building a useful simulator requires significant training and validation. Our dataset provides training
data for both chat and autocomplete interactions: The dataset of interactions with the chat models D
chat
allows us to simulate the queries programmers make to the chat assistant given the code they have currently
written. The dataset of interactions with the autocomplete models D can allow us to simulate finer-grain
ac
interactions with LLM suggestions such as verifying and editing suggestions, among other activities outlined
in [Mozannar et al., 2023]. To validate a proposed simulator, one should test whether it faithfully replicates
the trends observed in RealHumanEval before it can be used as an evaluation benchmark.
Optimizing suggestions from human feedback. In addition to using the human feedback data to
simulate the interaction, one can use it to fine-tune the models. For instance, the dataset of interactions with
autocomplete models D reveals which suggestions programmers accept and which they reject, which can be
ac
used to update the LLM and generate suggestions that maximize the probability of being accepted by the
programmer. Moreover, the dataset also captures how accepted suggestions were edited over time, which
can be used to generate suggestions that are more likely to persist in the programmer’s code. Finally, an
LLM that is not instruction-tuned usually requires specifying a maximum generation length parameter to
stop the generation of a code suggestion. In our autocomplete implementation, we intentionally randomized
the maximum suggestion length of the generated suggestion to be between the range [10,120] with a mean
token length of 64. This design decision can provide yet another signal about when the LLM should stop
generating code.
32F Example user interactions
import pandas as pd def even_odd_count(num):
from io import StringIO str_num = str(num)
even = 0
# Original dataset odd = 0
data = '''
col1,col2,col3,col4,col5 forkinstr_num:
6,1,5.3881673400335695,3,2 ifint(k) % 2== 0:
9,2,4.191945144032948,5,8 even+= 1
10,8,6.852195003967595,8,1 else:
6,7,2.0445224973151745,8,7 odd+= 1
1,10,8.781174363909454,10,10 return[even, odd]
'''
defpairs(lst):
# Read the dataset into a DataFrame cnt= 0
df = pd.read_csv(StringIO(data)) fori, eleinenumerate(lst):
ifnot
def transform_df(df):
# Add col3 col4 and col5
new_col = df['col3'] + df['col4'] + df['col5']
#use insert() toadd new column at position 3
df.insert(3,"col6",new_col)
# Create new column by taking the square root of col6
df["col7"] = [round((x ** (1/2)),
Figure 18: Examples of problematic autocomplete suggestions: incomplete suggestion (left) and starting new
irrelevant function (right).
User
Do you see any errors in this function?
def order_by_points(nums):
ans = []
for number in nums:
split = [int(x) for x in str(number) if x != '-']
if number < 0:
split[0] = -split[0]
ans.append((sum(split), number))
ans.sort()
return [x[1] for x in ans]
Chatbot
Yes, there is an error in the provided function. When sorting the `ans` list, it uses the
default sorting order, which is ascending based on the sum of the split digits.
However, the desired behavior is to sort in descending order based on the sum.
To fix this, you can pass a lambda function as the `key` argument to the `.sort()`
method, which specifies that the list should be sorted based on the negated sum.
User
The goals is to be sorted in ascending order based on the sum of the split digits
Chatbot
Apologies for the confusion. If the goal is to sort the list in ascending order based on
the sum of the split digits, the original code is correct. The `.sort()` method, without
any additional arguments, sorts the list in ascending order.
Figure19: Exampleofachatinteractionwherethechatassistantcouldhaveproactivelyaskedmoreclarifying
questions. Note that the chat agent response is shortened (by excluding code snippet) for brevity.
33User
Write python code to determine whether a number is the product of 3 prime numbers
Chatbot
To determine whether a number is the product of 3 prime numbers, we can follow
the steps below:
1. Create a function `is_product_of_3_primes(num)` that accepts an integer `num`
as a parameter.
2. Initialize a variable `count` to 0 to keep track of the number of prime factors.
3. Create a helper function `is_prime(n)` that accepts an integer `n` and checks
whether it is a prime number. This function will return True if `n` is prime, and False
otherwise.
4. Iterate from 2 up to the square root of `num` (inclusive) using a for loop. Check if
each number is a prime factor of `num`.
- If the current number is a prime factor, increment the `count` by 1.
- If the `count` exceeds 3, break the loop as there can't be more than 3 prime factors.
5. After the loop, check if the `count` is equal to 3.
- If it is equal to 3, return True as `num` is the product of 3 prime numbers.
- If it is not equal to 3, return False.
Here is the implementation of the above steps:
def is_product_of_3_primes(num):
count = 0
def is_prime(n):
if n < 2:
return False
for i in range(2, int(math.sqrt(n)) + 1):
if n % i == 0:
return False
return True
for i in range(2, int(math.sqrt(num)) + 1):
if num % i == 0 and is_prime(i):
count += 1
if count > 3:
break
return count == 3
You can now use the function `is_product_of_3_primes(num)` to check if a number
`num` is the product of 3 prime numbers. The function will return True if it is, and
False otherwise.
Figure 20: Example of an overly lengthy response from a chat interaction.
import pandas as pd class Tokenizer:
from io import StringIO def __init__(self, max_vocab_size=200):
self.max_vocab_size = max_vocab_size
[excluding code to define dataframe] self.word_to_id = {}
self.id_to_word = {}
def transform_df(df):
# Your code here def tokenize(self, text):
print(df["height"].astype("int"</PRE> # file is a cell, NOT CODE IN PYTHON IN THIS # do not change
FILE # Split
importpandas aspd returnself.id_to_word.get(word_id)
fromio importStringIO <POST> # End of file main.py
의외로간단
# Original dataset
Figure 21: Examples of rejected suggestions from CodeLlama-34b, which failed to consider the context of
existing code: (left) the suggested code tried to import the same packages that are already present and (right)
the suggested code trails off into irrelevant, non-Python text.
34