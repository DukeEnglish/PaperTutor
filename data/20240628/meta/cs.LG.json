[
    {
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "authors": "Vedang LadWes GurneeMax Tegmark",
        "links": "http://arxiv.org/abs/2406.19384v1",
        "entry_id": "http://arxiv.org/abs/2406.19384v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19384v1",
        "summary": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "updated": "2024-06-27 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19384v1"
    },
    {
        "title": "TabReD: A Benchmark of Tabular Machine Learning in-the-Wild",
        "authors": "Ivan RubachevNikolay KartashevYury GorishniyArtem Babenko",
        "links": "http://arxiv.org/abs/2406.19380v1",
        "entry_id": "http://arxiv.org/abs/2406.19380v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19380v1",
        "summary": "Benchmarks that closely reflect downstream application scenarios are\nessential for the streamlined adoption of new research in tabular machine\nlearning (ML). In this work, we examine existing tabular benchmarks and find\ntwo common characteristics of industry-grade tabular data that are\nunderrepresented in the datasets available to the academic community. First,\ntabular data often changes over time in real-world deployment scenarios. This\nimpacts model performance and requires time-based train and test splits for\ncorrect model evaluation. Yet, existing academic tabular datasets often lack\ntimestamp metadata to enable such evaluation. Second, a considerable portion of\ndatasets in production settings stem from extensive data acquisition and\nfeature engineering pipelines. For each specific dataset, this can have a\ndifferent impact on the absolute and relative number of predictive,\nuninformative, and correlated features, which in turn can affect model\nselection. To fill the aforementioned gaps in academic benchmarks, we introduce\nTabReD -- a collection of eight industry-grade tabular datasets covering a wide\nrange of domains from finance to food delivery services. We assess a large\nnumber of tabular ML models in the feature-rich, temporally-evolving data\nsetting facilitated by TabReD. We demonstrate that evaluation on time-based\ndata splits leads to different methods ranking, compared to evaluation on\nrandom splits more common in academic benchmarks. Furthermore, on the TabReD\ndatasets, MLP-like architectures and GBDT show the best results, while more\nsophisticated DL models are yet to prove their effectiveness.",
        "updated": "2024-06-27 17:55:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19380v1"
    },
    {
        "title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space",
        "authors": "Core Francisco ParkMaya OkawaAndrew LeeEkdeep Singh LubanaHidenori Tanaka",
        "links": "http://arxiv.org/abs/2406.19370v1",
        "entry_id": "http://arxiv.org/abs/2406.19370v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19370v1",
        "summary": "Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.",
        "updated": "2024-06-27 17:50:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19370v1"
    },
    {
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "authors": "Nigel FernandezAlexander ScarlatosSimon WoodheadAndrew Lan",
        "links": "http://arxiv.org/abs/2406.19356v1",
        "entry_id": "http://arxiv.org/abs/2406.19356v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19356v1",
        "summary": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "updated": "2024-06-27 17:37:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19356v1"
    },
    {
        "title": "Subtractive Training for Music Stem Insertion using Latent Diffusion Models",
        "authors": "Ivan Villa-RenteriaMason L. WangZachary ShahZhe LiSoohyun KimNeelesh RamachandranMert Pilanci",
        "links": "http://arxiv.org/abs/2406.19328v1",
        "entry_id": "http://arxiv.org/abs/2406.19328v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19328v1",
        "summary": "We present Subtractive Training, a simple and novel method for synthesizing\nindividual musical instrument stems given other instruments as context. This\nmethod pairs a dataset of complete music mixes with 1) a variant of the dataset\nlacking a specific stem, and 2) LLM-generated instructions describing how the\nmissing stem should be reintroduced. We then fine-tune a pretrained\ntext-to-audio diffusion model to generate the missing instrument stem, guided\nby both the existing stems and the text instruction. Our results demonstrate\nSubtractive Training's efficacy in creating authentic drum stems that\nseamlessly blend with the existing tracks. We also show that we can use the\ntext instruction to control the generation of the inserted stem in terms of\nrhythm, dynamics, and genre, allowing us to modify the style of a single\ninstrument in a full song while keeping the remaining instruments the same.\nLastly, we extend this technique to MIDI formats, successfully generating\ncompatible bass, drum, and guitar parts for incomplete arrangements.",
        "updated": "2024-06-27 16:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19328v1"
    }
]