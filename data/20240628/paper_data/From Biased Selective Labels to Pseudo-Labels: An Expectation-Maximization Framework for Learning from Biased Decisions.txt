From Biased Selective Labels to Pseudo-Labels:
An Expectation-Maximization Framework for Learning from Biased Decisions
TrentonChang1 JennaWiens1
Abstract Disparate Censorship: Data Generation Process
covariates
Selectivelabelsoccurwhenlabelobservationsare (unobserved ground confounding) X Y
subjecttoadecision-makingprocess;e.g.,diag- truth
nosesthatdependontheadministrationoflabora-
~
torytests. Westudyaclinically-inspiredselective sensitive A T Y observed
attribute label
labelproblemcalleddisparatecensorship,where testing/labeling decision
labelingbiasesvaryacrosssubgroupsandunla- Disparate Censorship Expectation-Maximization
beledindividualsareimputedas“negative”(i.e., 1 pretrain outcome (f) & labeling (g) predictors
no diagnostic test = no illness). Machine learn- T = 1? X f Y^ Y~ l( ap bre e- letr da din a to an )
ingmodelsna¨ıvelytrainedonsuchlabelscould
X A g T^ T
amplifylabelingbias. Inspiredbycausalmodels
of selective labels, we propose Disparate Cen- 2 repeat until convergence
sorshipExpectation-Maximization(DCEM),an T = 1? Y~ Y
algorithmforlearninginthepresenceofdisparate
T = 0? X f Y
censorship. WetheoreticallyanalyzehowDCEM
mitigates the effects of disparate censorship on X f Y^ Y
modelperformance. WevalidateDCEMonsyn-
theticdata,showingthatitimprovesbiasmitiga-
A g T^ x Y^~ x Y~
tion(areabetweenROCcurves)withoutsacrific- observed variable f outcome predictor forward pass
ingdiscriminativeperformance(AUC)compared pred. probability g labeling predictor backward pass
tobaselines.Weachievesimilarresultsinasepsis pseudo-labels cross-entropy loss x multiply
classificationtaskusingclinicaldata. Figure1.Top: Causalmodelofdisparatecensorship(x: covari-
ates, y: ground truth, y˜: observed label, t: testing/labeling in-
dicator, a: sensitive attribute). Shaded variables are fully ob-
served.Bottom:DisparateCensorshipExpectation-Maximization
1.Introduction (DCEM).Dashednodesareprobabilisticestimates.
Rhee&Klompas,2020)). Inthissetting,patientswithno
Selectivelabelsoccurwhenadecision-makingprocessde-
testresultaredefinedasnegative(Hartvigsenetal.,2018;
terminesaccesstogroundtruth(Lakkarajuetal.,2017). We
Teepleetal.,2020;Jehietal.,2020;McDonaldetal.,2021;
studyapracticalcaseofselectivelabels: disparatecensor-
Adamsetal.,2022;Kamranetal.,2022). However,labora-
ship(Changetal.,2022). Disparatecensorshipintroduces
torytestingdecisionsmaybebiased. Forexample,women
twochallenges: differentlabelingbiasesacrosssubgroups
areundertestedandunderdiagnosedforcardiovasculardis-
andtheassumptionthatunlabeledindividualshaveaneg-
ease (Beery, 1995; Schulman et al., 1999). ML models
ativelabel. Forexample,inhealthcare,labelsmaydepend
trainedonsuchdatamayrecommendwomenlessoftenfor
onlaboratorytestresultsonlyavailableinsomepatients.
diagnostictestingthanmen,reinforcinginequity.
PastworkhastrainedMLmodelstopredictoutcomesbased
To address this bias, one option is to train only on tested
onlaboratorytestresults(e.g.,sepsis(Seymouretal.,2016;
individuals. Such an approach may discard a large sub-
1DivisionofComputerScience&Engineering,Universityof setofthedataandmaynotgeneralizetountestedpatients.
Michigan, Ann Arbor, MI, USA. Correspondence to: Trenton Anotheroptionissemi-supervisedapproachesthatdonot
Chang<ctrenton@umich.edu>.
assumeuntestedpatientsarenegative,suchaslabelpropaga-
tion(Zhu&Ghahramani,2002;Lee,2013)orfiltering(Li
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by etal.,2020;Nguyenetal.,2020),ornoisy-labellearning
theauthor(s). methods (Blum & Stangl, 2020; Wang et al., 2021; Zhu
1
4202
nuJ
72
]GL.sc[
1v56881.6042:viXra
pets-E
pets-MFromBiasedSelectiveLabelstoPseudo-Labels
etal.,2021). However,suchmethodsdonotleveragecausal Learningunderdisparatecensorship. Weaimtolearn
modelsoflabelbias,apotentialsourceofadditionalinfor- amappingf : x y parameterizedbyθ optimizedfor
θ
→
mation. Weaimtodevelopanapproachthatleveragesall discriminativeperformance(i.e., AUC),butonlyobserve
availablesignalwhileaccountingforlabelingbiases. proxy labels y˜. The default approach for learning under
disparate censorship is to assume y = y˜and proceed us-
Inspiredbycausalmodelsofselectivelabeling(Laineetal.,
ingsupervisedlearning. However,suchanf mayencode
2020;Changetal.,2022;Guerdanetal.,2023a),wepro- θ
labeling biases: estimates of P(Y˜ X) may be inflated
pose a simple method for mitigating bias when training |
comparedtoP(Y X)forthosemorelikelytobelabeled.
modelsunderdisparatecensorship: DisparateCensorship |
Thus,biasedlabelingcouldyielddisproportionateimpacts
Expectation-Maximization(DCEM;Fig.1). First,weshow
onperformanceacrossdifferentsubgroupsofthedata.
thatDCEMregularizesmodelestimatestocounterbalance
disparatecensorship. WevalidateDCEMinasimulation Note that we can interpret the estimand of interest as the
studyandasepsisclassificationtaskonclinicaldata. We causaleffectoftestingontheobservedlabel,since,inthe
find that our method mitigates bias (area between ROC languageofdo-calculus(Pearl,2009),
curves)whilemaintainingcompetitivediscriminativeperfor-
mance(AUC),andisgenerallymorerobustthanbaselines E[Y X]=E[Y X,T =1]=E[Y˜ X,T =1]
| | |
tochangesinthedatagenerationprocess. =E[Y˜ X,do(T =1)], (1)
|
2.Preliminaries: DisparateCensorship which follows from standard causal identifiability deriva-
tions given the causal graph of Fig. 1 (Imbens & Rubin,
Weconsideradataset x(i),y˜(i),t(i),a(i) N ,withcovari- 2015). Intuitively,testinganindividual(do(T =1))reveals
atesx(i)
Rd,labelin{ g/testingdecisiont(} i)i=1
0,1 ,sensi- theiroutcome. Thus,amodeltrainedonlyontestedindi-
∈ ∈{ }
tiveattributea(i),andobservedlabely˜(i) 0,1 ,aproxy vidualscouldconsistentlyestimateP(Y X),butmaynot
∈{ } |
forgroundtruthy(i) 0,1 . Theproxylabely˜(i) =y(i) correctforlabelingbias. Wediscussotherapproachesin
∈{ }
whent(i) =1,andy˜(i) =0otherwise(i.e.,y˜(i) =y(i)t(i)). semi-supervisedlearninginSection6.
3.Methodology
What is disparate censorship? Disparate censorship
models “double standards” in label collection decisions
WeproposeDisparateCensorshipExpectation-Maximiza-
(Fig. 1, top). It is a variation of selective labeling or out-
tion(DCEM)asanapproachforlearninginthepresence
comemeasurementerror(Lakkarajuetal.,2017;Guerdan
of disparate censorship. We first build intuition for how
etal.,2023b). Disparatecensorshipuniquelyassumesthat
onecouldmitigatedisparatecensorshipbasedonthecausal
untestedindividualsareimputedasnegative.
model(Section3.1). WethenderiveDCEM(Section3.2)
Weconsiderdisparatecensorshipinthecontextofbinary andshowthatitmitigatesdisparatecensorshipviaaform
classification(Changetal.,2022)(Fig.1,top). Wejustify of regularization (Section 3.3). We consider alternative
themodelbyexample. Considerapatientinanemergency designsandtheirlimitations(Section3.4). Detailedproofs
roomwithcharacteristicsxandsensitiveattributea. This anddefinitionsareinAppendixB.
patientmayhavesomeconditiony(currentlyunobserved)
causedbyxbutnota. Aclinicianmayorderadiagnostic 3.1.Towardsmitigatingdisparatecensorship
test(settto1)todeterminey. Thedecisionisbasedonx,
Recallingthecausalmodelofdisparatecensorship,suppose
butcouldbeswayedbybiasesina.
thatwearenaivelytrainingamodelf topredicty˜. Define
θ
Tosimplify,supposethattestsareperfectlysensitive.1 Then, groupsaanda′andx . Considersome ′ sothat
∼X X ⊆X
weobservegroundtruthfortestedindividuals(t=1 =
y˜=y). Otherwise,thepatient’slabelisimputedasnegati⇒ ve P [T X,A=a]<< P [T X,A=a′] (2)
x∈X′ | x∈X′ |
(t = 0 = y˜ = 0; i.e., untested patients are presumed
⇒
healthy). However, duetobiasesintestingdecisionst, y for all x ′. Define tˆ≜ P(T X = x,A = a) (e.g.,
mayonlybeavailableinabiasedsubsetofthedata. The probabilit∈ yoX freceivingalaborato| rytest)andyˆ≜ P(Y
causalmodelofdisparatecensorship(Fig.1,top)encodes
X
=x).Byassumption,xissufficientforpredictingy(i.e.|
,
thisdecision-makingpipeline. Beyondhealthcare,disparate
asinFig.1,top),suchthattheoptimalyˆshouldbesimilar
censorshipmayarisewheneverpotentiallybiaseddecisions acrossa(within ′). However,Eq.2statesthatgroupais
affectdatalabeling. undertestedrelatiX vetogroupa′: theyhavealowertˆwithin
1Ifnot,wecandefineT toindicatewhetheralabelisconfirmed
X′. Equivalently,labelingbiasfavorsgroupa′. Thus,our
correct. This definition captures differences in test sensitivity
naivemodelwouldunderestimateyˆingroupa(lowertˆthan
acrossgroups(i.e.,spectrumbias(Mulherin&Miller,2002)). groupa′in ′)relativetogroupa′.
X
2FromBiasedSelectiveLabelstoPseudo-Labels
Algorithm1DisparateCensorshipExpectation-Maximiza- E-step. Theposteriorofy(i)giventheobserveddatais:
tion. : binarycross-entropyloss.
L Q(y(i))≜E[y(i) y˜(i),t(i),x(i),a(i)]. (5)
Input: (x(i),y˜(i),t(i),a(i)) N |
Output{ : f : [0,1] }i=1 WecanrewriteEq.5asfollows:
θ
f argminX → 1 (cid:80) (y˜(i),f (x(i))) Theorem 3.1 (E-step). The posterior distribution of y(i)
θ ← |{i:t(i)=1}| i:t(i)=1L θ
fθ giventheobserveddataisequivalentto
g tˆ(ζ i∗
)
←
ga ζr ∗g
g
(m
ζ
x(i in ),N1 a((cid:80) i))N i=1L(t(i),g ζ(x(i),a(i))) Q(y(i))=(cid:40)
y E˜( [i y)
(i) =1 x(i)]
t o( ti h) e= rw1
ise
(6)
←
whilenotconvergeddo |
Q(y(i)) t(i)y˜(i)+(1 t(i))f (x(i))//E-step
f
θ
←ar← gmin N1 (cid:80)N i=1L− (Q(y(i)θ ),f θ(x(i))) I cn ot mui pti lv ee tely l, abth ee
l
iE n- fs ot re mp au tis oe ns (y˜ rea cs alt lhe t(il )ab =el 1wh =en we y˜(h i)av =e
fθ y(i));otherwise,weusetheposteriorestimatef⇒
(x(i))as
+Q(y(i)) (y˜(i),f (x(i)) tˆ(i))//M-step θ
L θ · a smoothed label. Equivalently, the E-step imputes soft
endwhile
pseudo-labelsforunlabeleddata,i.e.,probabilisticestimates
returnf
θ yˆ(i) [0,1]. Motivatedbyapproachesthattrainapseudo-
∈
labelingmodelonlabeleddata(Arazoetal.,2020;Rizve
etal.,2021),wepre-trainf ontestedindividuals.
Tocounterbalancethisbias,onecouldincreaseyˆ(within ′) θ
X
wheregroupaismoreprevalentthangroupa′;i.e.,lower-tˆ
M-step. TheM-stepmaximizesthelog-likelihoodofEq.4
regions. Since we are interested in discriminative perfor-
givenE-stepestimatesQ(y(i))(Eq.6). Therearetwoterms
mance,thisisanalogoustodecreasingyˆwheretˆishigher,
tomodel,whichisdoneviaanestimatorfory(i)trainedus-
from which the proposed method follows. More broadly,
ingQ(y(i))andanestimatorfory˜(i). Thelatterisobtained
variablesassociatedwithlabelingbias(AcausallyaffectsT)
bycombininganestimateoft(i)withQ(y(i)). Concretely,
butnottheoutcomeofinterest(Adoesnotcausallyaffect
letyˆ(i) ≜ f (x(i)),andleth beamodelofP(Y˜ Y,T).
Y)maybeusefulformitigatinglabelingbias. θ ϕ |
Maximizingthelog-likelihoodofEq.4reducesto
Given our causal model with latent variable Y (Fig. 1,
N
top), we base our approach on expectation-maximization max (cid:88) Q(y(i))logyˆ(i)+(1 Q(y(i)))log(1 yˆ(i))
(EM)(Dempsteretal.,1977). Wecanwrite: θ − −
i=1
(cid:104)
P(Y˜,Y,T,X,A,U) +Q(y(i)) y˜(i)logh ϕ(yˆ(i),tˆ(i))
=P(Y˜ Y,T)P(Y X)P(T X,A)P(X,A,U). (3) +(1 y˜(i))log(1 h (yˆ(i),tˆ(i)))(cid:105) . (7)
| | | − − ϕ
Sinceyisnotfullyobserved,Eq.3cannotbeoptimizedvia Thisleadstothefollowingresult:
standardsupervisedobjectives. Droppingtermsthatdonot
Theorem3.2(M-step, informal). MaximizingEq.7also
involveY,wecanwritethemaximizationofEq.3as
maximizestheevidence-basedlowerboundofEq.3.
max P(Y˜ Y,T)P(Y X). (4) In practice, we set h ϕ(yˆ(i),tˆ(i)) ≜ yˆ(i)tˆ(i), a smoothed
| | version of the assumption y˜ = yt. Defining as binary
L
OptimizingEq.4proceedsviaEM.Weshowthattheresult- cross-entropyloss,wecanrewriteEq.7:
ingobjectivesalignwithreducingyˆinhigher-tˆregionsand
N
maintaindiscriminativeperformanceontestedindividuals. min (cid:88) (Q(y(i)),yˆ(i))+Q(y(i)) (y˜(i),yˆ(i)tˆ(i)). (8)
θ L L
3.2.DisparateCensorshipExpectation-Maximization i=1
Eq.8canbeinterpretedasaregularizedcross-entropyloss
Informal overview. EM alternates an expectation step
withrespecttopseudo-labelQ(y(i)). Thefirsttermpushes
(E-step), which imputes guesses for the latent variable(s)
yˆ(i) towards Q(y(i)), while the second “encourages” yˆ(i)
(i.e., Y inEq.4), andamaximizationstepthatoptimizes to be consistent with the causal model. To obtain tˆ, we
likelihoodgiventheimputedestimates(M-step,i.e.,Eq.4).
pre-train and freeze a binary classifier for t, and take the
OurE-stepimputespreliminaryestimatesofP(Y X)for probabilisticestimatesastˆ.
|
untestedindividuals. OurM-stepupdatestheestimatesto
counteractlabelingbiaseswhent(i) =0,andisequivalent
3.3.DCEMcounterbalancesdisparatecensorship
tofullsupervisionwhent(i) =1. TheE-andM-stepsalter-
nateuntilconvergence. Fig.1(bottom)showsaschematic WeshowthatDCEMimposesaformof“causalregulariza-
ofDCEM,withpseudocodeinAlgorithm1. tion”thatlowersyˆinuntestedindividuals.
3FromBiasedSelectiveLabelstoPseudo-Labels
DCEMisaformofcausalregularization. Byanalogy Proposition3.5. Eq.11isminimizedwhenyˆ(i) =y(i).
toregularizedriskminimization,consideranobjective
Proposition 3.5 states that causal regularization does not
(θ)+λr(θ), (9) change the M-step optimum from matching ground truth
L
whent(i) =1(i.e.,regularizationstrength=0). Thus,the
forλ R (regularizationstrength)andaregularizerr :
Θ
R∈ ,w+
hereΘistheparameterspaceofθ.
M-stepobjectivealignswithfully-supervisedloss.
→
Thus,theM-step(Eq.8)counterbalancesdisparatecensor-
Without loss of generality, setting λ = 1 and match-
ship by regularizing yˆ(i) towards 0 as tˆ(i) increases. For
ing terms between Eq. 9 and Eq. 8 yields r(θ) =
t(i) = 1,theM-stepoptimumstaysconstant,andDCEM
Q(y(i)) (y˜(i),yˆ(i)tˆ(i)). Whiletˆ(i)affectstheoptimization
L shouldmaintaindiscriminativeperformance.
of Eq. 8, it is not a multiplier (e.g., λ in Eq. 9). To in-
terprettheeffectoftˆ(i),weproposeadefinitionofcausal
3.4.Alternativedesignsandtheirlimitations
regularizationstrengthbasedhowtheoptimalyˆ(i)changes.2
Definition 3.3 (Causal regularization strength, informal). Weconsidertwoalternativedesignsandtheirlimitations:di-
Let yˆ(i) (Q(y(i)),tˆ(i)) be the minimizer of Eq. 8. For rectlyusingt(i)inDCEMandpropensityscoreadjustment.
OPT L
finite&convexonyˆ(i) in[0,1],thecausalregularization
strengthisR()≜ Q(y(i)) yˆ(i) (Q(y(i)),tˆ(i)). Whynotuset(i)directly? Wesubstitutetˆ(i) =t(i)into
· | − OPT | Eq.8andanalyzeonesummand(withoutlossofgenerality):
Definition3.3quantifiesthetradeoffbetweenmatchingyˆ(i)
totheE-stepestimatesandoptimizingEq.8. Whileyˆ(i)is (y(i),yˆ(i))+y(i) (y(i),yˆ(i)) t(i) =1 (12)
L L
notanoptimizationparameter,analyzingtheoptimalyˆ(i) (Q(y(i)),yˆ(i)) t(i) =0 (13)
canclarifytheinductivebiasoftheM-step. Weproceedby L
consideringhowcausalregularizationimpactsuntestedvs. BothlossesusetheE-stepestimateQ(y(i))assupervision.
testedindividuals. Whent(i) =0,theM-stepis Whent(i) = 1(Eq.12),theM-stepaddsy(i) (y(i),yˆ(i)),
L
penalizingfalsenegatives2xasheavilyasfalsepositives.
N
min (cid:88) (Q(y(i)),yˆ(i)) Q(y(i))log(1 yˆ(i)tˆ(i)). (10) This does not affect ranking metrics (e.g., AUC). When
θ L − − t(i) = 0(Eq.13), theM-stepdropscausalregularization,
i=1
andthuscannotcounterbalancedisparatecensorship. Di-
Since log(1 yˆ(i)tˆ(i))increasesinyˆ(i),theregularization
− − rectly using t(i) would only help if counterbalancing dis-
term “encourages” yˆ(i) to decrease when tˆ(i) > 0. The
paratecensorshipisunnecessaryforgoodestimation,i.e.,
regularizationtermisconstantiftˆ(i) =0,suchthattheM-
whentestedindividualsarerepresentativeofthepopulation.
stepwouldnotchangetheE-stepestimate.Thismatchesthe
intuitionthatonecannotlearnabouty(i) fromindividuals
Whynotpropensityscoreadjustment/relatedcausalap-
thatareverydifferentfromlabeledindividuals(i.e.,when
proaches? RecallthatestimatingtheeffectofT ontheob-
theoverlapassumptionincausalinferenceisviolated). The
servedlabelyieldsaconsistentestimateofP(Y X)(Eq.1,
regularizationstrengthdependsontˆ(i)asfollows: |
Section2). Indeed,tˆ(i)isanestimateofP(T X,A),i.e.,
Theorem3.4(informal). Ift(i) =0,causalregularization a propensity score, motivating the usage of c| ausal effect
strengthincreasesintˆ(i). estimatorsthatleveragetˆ(i). However,propensityscoread-
justment(e.g.,IPW(Rosenbaum&Rubin,1983)ordoubly-
Theresultimpliesthatcausalregularizationcounterbal-
robustvariations(Robinsetal.,1994;VanDerLaan&Ru-
ancesdisparatecensorship. Recallthatloweringyˆ(i) in
bin,2006;Huetal.,2022))requirean“overlap”assumption
regions where tˆ(i) is higher can mitigate bias. Equiva-
η <tˆ(i) <1 ηforsomeη =(0,1)andhaveasymptotic
lently,causalregularizationmuststrengthenastˆ(i)increases, − 2
varianceoforderO(1/(η (1 η)),whichissensitiveto
whichfollowsfromTheorem3.4. · −
extremetˆ(i)(e.g.,asinAIPW(Glynn&Quinn,2010)).
Causal regularization aligns with full supervision in However, in finite-sample settings, “sharp” testing deci-
testedindividuals. Whent(i) =1,theM-stepis sionsleadtoweakoverlap. Suchextremetˆ(i)mayarisein
threshold-baseddecisions(Djulbegovicetal.,2014;Pier-
N
min (cid:88) (y(i),yˆ(i))+y(i) (y(i),yˆ(i)tˆ(i)), (11) sonetal.,2018). Forexample,apatienteitherexhibitsor
θ L L doesnotexhibittherequisitesymptomstowarranttesting.
i=1
Thisisanalogoustoinducingcovariateshiftbetweentested
substitutingy(i)forQ(y(i))andy˜(i). Thus:
and untested individuals. In other words, “holes” in the
training data emerge when using only labeled examples.
2“Causal regularization” has been defined in the context of
causaldiscovery(Bahadorietal.,2017;Janzing,2019).Ourusage Thus,systematictestingbiascouldexacerbatemodelperfor-
isunrelated:weuseacausalmodeltoregularizeanestimator. mancegapsacrosspopulationsubgroups. Whilelowover-
4FromBiasedSelectiveLabelstoPseudo-Labels
lapstillimpactsDCEM(sinceDCEMcannotlearnwhen sionalityandP(A=0)=0.5)butallowsfullcontrolover
tˆ(i) =0),ourmethodinsteadleveragesanevidence-based yandt. AdditionalsimulationdetailsareinAppendixC.1.
lowerboundtomodelyunderdisparatecensorship. Wefur-
therdiscusspotentialimprovementsinoverlap-robustness 4.2.Clinicaldata: MIMIC-III
oftheproposedapproachinAppendixB.
Multiplesepsisdefinitions,suchasSepsis-3(Singeretal.,
2016), are based on laboratory tests (blood culture) such
4.ExperimentalSetup
thatpatientswithoutatestresultarebydefinitionnegative.
Thus,sepsisclassificationisapotentialreal-worldcaseof
We validate DCEM with synthetic data across different
disparatecensorship. Wecurateasepsisclassificationtask
data-generationprocessesonsimulatedbinaryclassification
usingtheMIMIC-IIISepsis-3cohort(Johnsonetal.,2016;
tasks(Section4.1)andinapseudo-syntheticsepsisclassifi-
2018),anelectronichealthrecorddataset.
cationtaskusingrealclinicaldata(MIMIC-III)(Johnson
etal.,2016),acrosspotentiallaboratorytestingpolicies(Sec- We aim to distinguish patients who never develop sepsis
tion4.2).Wethendiscussourchosenbaselines(Section4.3) fromthosewhodevelopsepsiswithin8hoursofaninitial
andevaluationmetrics(Section4.4). 3-hour observation period. If a patient met the Sepsis-3
criteriabetween3-11hoursofthefirstchartmeasurement,
4.1.SyntheticDatasets we set y = 1, and y = 0 if the patient never develops
sepsisduringtheirhospitalstay. Weexcludepatientswith
Bydefinition,y isnotfullyobservedunderdisparatecen-
onsettimesoutsidethisrangeandincludeonlyWhiteand
sorship. Thus, we design a simulation study in order to
Black patients to simplify the analysis of bias mitigation.
evaluatevariousmethodswithrespecttogroundtruth. The
Wechoosex R13followinganexistingsepsisprediction
datagenerationprocessfollowsfromtheassumedcausal ∈
model(Delahantyetal.,2019),andexcludepatientswhere
modelofdisparatecensorship(Fig.1,top):
allfeaturesaremissing. ThisyieldsN = 5,301patients,
a(i) Ber(0.5),x(i) (µ 1 ,0.032I ) fromwhichwecreatea60-20-20train-validation-testsplit.
a 2 2×2
∼ ∼N · This is a simplified version of a real clinical task, since
t(i) Ber(σ(30 s (x(i),a(i))))
∼ · T weexcludepatientswhodevelopsepsislaterduringtheir
y(i) Ber(σ(10 s (x(i)) c )), y˜(i) =y(i)t(i) hospitalization. Nonetheless, itishelpfulforprobingthe
Y y
∼ · −
strengthsandweaknessesoftheproposedapproach.
whereI istheidentitymatrix,ands :x,a R,s :
2×2 T Y
x R,andµ R,c Raresimulationparam→ eters. We To evaluate model performance, we assume that the ob-
a y
set→ P(A=0)=∈ 0.5and∈ induceconfoundingbetweenx(i) served y reflects ground truth, since 90% of patients
≈
anda(i) bysettingu(i) = a(i). Wedrawx(i) R2 from were tested (i.e., received a blood culture) in our cohort.
group-specificGaussians,andassumeBernoulli-∈ distributed To generate label proxies y˜, we simulate multiple poten-
t(i) and y(i) with parameters defined via s : x,a R tiallabelingbiasesviaaclinically-inspiredtestingfunction
T
ands Y :x R,respectively. Intuitively,s T (s Y)isa→ soft s T. We specify a linear s T based on qSOFA, a score for
“decisionbo→ undary”forT (Y).Inspiredbyobservationsthat triagingpatientsatriskofsepsis(Seymouretal.,2016). In-
cliniciantestingiscanberepresentedbysimplerfunctions spiredbyobservationsthatcliniciansover-weightrepresen-
thanobservedoutcomes(Mullainathan&Obermeyer,2022), tativesymptomsindiagnostictestdecisions(Mullainathan
wechooseanon-linears Y andalinears T. &Obermeyer,2022),wecreatedifferentversionsofs T via
differentweightingsofqSOFAfeatures. Weexaminek
WesimulateN = 20,000individualsfortraining,valida- ∈
1/4,1/3,1/2,1,2,3,4,5 andq 1/2,2/3,1,3/2,2 .
t
tion, and testing each (i.e., 60,000 total). We define set- { } ∈{ }
DetailsofthesepsiscohortareinAppendixC.2.
tings in terms of testing disparity q = P(T|A=0), preva-
t P(T|A=1)
lencedisparityq = P(Y|A=0), andtestingmultiplek = 4.3.Models
y P(Y|A=1)
PP (( YT= =1 1) ). Intuitively, q t controls labeling biases, q y con- Asnaivebaselines,wetestay-obsmodel(trainingony˜)
trols differences between groups, and k controls testing andtrainingongroupaonly.Weselectsimilarly-motivated
rate. We consider q 1/4,1/3,1/2,1,2,3,4 , q orapplicablebaselinesfromrelatedsettings:
t y
∈ { } ∈
1/4,1/3,1/2,1 ,andk 1/4,1/3,1/2,1,2,3,4 ,and
s{ etsimulationpar} ameters∈ to{ yieldthedesiredq ,q ,k} .3 • Noisy-labellearning:Grouppeerloss(Wangetal.,2021)
t y
(Appendix: Peer loss (Liu & Guo, 2020), truncated ℓ
q
Since s Y is unknown in practice, we replicate the main loss (Zhang & Sabuncu, 2018) and generalized Jensen-
experimentsacrossvariouss Y asarobustnesscheck. The Shannonloss(Englesson&Azizpour,2021)),
simulationmakessimplifyingassumptions(e.g.,lowdimen-
• Semi-supervisedlearning: SELF(Nguyenetal.,2020)
3Weskipsettingswhereq ,q ,kyieldinfeasibletestingrates. (Appendix: DivideMix(Lietal.,2020)),
t y
5FromBiasedSelectiveLabelstoPseudo-Labels
• Causal inference: tested-only (training on examples model) and their overlap robustness compared to DCEM
where t = 1), and DragonNet (Shi et al., 2019), using (AppendixE.3). Furthersensitivityanalysescanbefoundin
thetreatmenteffectofthesensitiveattributeontestingto AppendixE.4(smoothedtˆ(i))andE.5(E-stepinitialization).
correct disparate censorship (i.e., learn a correction for
P(Y X) P(Y˜ X,A)), 5.1.Resultsonsimulateddisparatecensorship
| − |
• Positive-unlabeledlearning: Selected-At-RandomEM Fig.2showsROCgaps(left)andAUCs(right)ofthebase-
(SAREM)(Bekkeretal.,2020). linesmostcompetitivewithourapproach(DCEM,magenta)
at q = 0.5,k = 1,q = 2. In this setting, 25% (i.e.,
Asanoracle,wecomparetotrainingony(“y-model”). We y t
k P(Y = 1))ofindividualsaretested,andthebaserate
use neural networks for all approaches. Training details, ·
ofY ingroupa=0is1/2thatofgroupa=1,butgroup
suchashyperparameters,areinAppendixD.
a=0istwiceaslikelytobetested. EachpointisanROC
gap/AUCvalueachievedunderonedecisionboundarys .
4.4.Evaluationmetrics Y
Results for the remaining baselines are in Appendix E.1.
Weconsiderbiasmitigationanddiscriminativeperformance Thetakeawaysalignwiththemainresults.
metrics with respect to y, and measure the robustness of
bothmetricstochangesinthedata-generationprocess.
DCEM mitigates bias more effectively than baselines.
DCEM achieves a median ROC gap of 0.030 (2nd-best,
Discriminativeperformance. Weusetheareaunderthe
SELF:0.034),suggestingthatitmitigatesbiasmoreeffec-
receiveroperatingcharacteristiccurve(AUC),astandard
tivelythanbaselines(Fig.2,left). Weshowsimilartrends
discriminativeperformancemetric.
for q 1, q , and k [1/3,2] (Appendix E.1). At low
t y
≥ ∈
testingrates,allmodelsmitigatebiaspoorly. Athightesting
Mitigatingbias. WeusetheROCgap(alsocalledROC
rates,thetested-onlymodelissufficient.
fairness(Vogeletal.,2021)orABROCA(Gardneretal.,
2019)),theabsoluteareabetweentheROCcurvesforeach For q t < 1 (Appendix E.1), DCEM mitigates bias com-
groupa. TheROCgapisin[0,1]. Lowervaluesindicate paredtothedefaultapproach(y-obsmodel)butnolonger
better bias mitigation. Intuitively, the ROC gap is zero dominatesthebaselines. WehypothesizethatDCEMhas
whenaclassifierwithsomefixedfalsepositiverateineach similarbiasmitigationcapabilitiesasbaselines,sincethere
groupobtainsequaltruepositiveratesacrossgroups. Under islessbiastomitigate. Recallingthatq y <1,sinceq t <1,
disparate censorship, a zero ROC gap is achievable if a testingprobability,P(Y X)andP(Y˜ X)arecorrelated.
modelperfectlypredictsyfromx. LearningtopredictY˜ wou| ldpreserveord| eringinP(Y X),
|
reducingimpactsonrankingmetrics(e.g.,ROCgap).4
Robustness. WeconsiderthemedianAUCandROCgap
overalls (syntheticdatasetting)ors (sepsisclassifica-
Y T DCEMismorerobustthanbaselinestochangesinthe
tion)andtheempiricalworst-case(AUC:min.;ROCgap:
data-generatingprocess. Fig.2(left)showsthatthemax-
max.) andrange.
imumROCgapislowerforDCEMcomparedtobaselines
(ours: 0.060 vs. 2nd-best, tested-only: 0.083). We re-
5.Experiments&Discussion port similar results for the minimum AUC (Fig. 2, right;
ours: 0.768vs. 2nd-best,tested-only: 0.623). DCEMalso
Ourexperimentsaimtosubstantiateourmainclaims:
achieves a tighter ROC gap and AUC range. Fig. 2 also
showsthatourmethodhasthetightestROCgaprange(left,
• Insyntheticdata,DCEMmitigatesbias,maintainscom- DCEM:0.048vs.tested-only,2nd-tightest:0.063)andAUC
petitivediscriminativeperformanceandimprovesrobust- range(right,DCEM:0.055vs. DragonNet: 0.199).
ness, while achieving better tradeoffs between perfor-
TheresultssuggestthatDCEMmaintainsrobustbiasmiti-
mance and bias mitigation compared to baselines (Sec-
gationanddiscriminativeperformanceacrossdifferentdata-
tion5.1).
generation processes (s ). This is expected, as DCEM
Y
• Onasepsisclassificationtask,DCEMimprovesdiscrimi-
optimizes likelihood under the disparate censorship data-
nativeperformancewhilemaintaininggoodtradeoffswith
generation process by design. In contrast, the baselines
biasmitigation,andismorerobustcomparedtobaselines
may experience selection bias or misspecification, since
(Section5.2).
theydiscarddataorassumecertainnoisestructures/variable
dependenciesthatdisparatecensorshipviolates.
Wealsoreportfullresults(AppendixE.1)andanablation
studyofDCEM(AppendixE.2). Wealsobenchmarkcausal 4Suchsettingsarerelatedtoboundary-consistentnoise; see
effect estimators (i.e., as alternatives to the tested-only Proposition1of(Menonetal.,2018).
6FromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=1,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.10
0.8
0.05
0.6
0.00
y-model(oracle) Group0only Tested-only DragonNet SAREM
y-obsmodel Group1only SELF Grouppeerloss DCEM(ours)
Figure2.ComparisonofROCgap(left)andAUC(right)ofselectedmodelsatq =0.5,k=1,q =2.Eachpointrepresentsadifferent
y t
s .Ourmethod(DCEM,magenta)mitigatesbiaswhilemaintainingcompetitiveAUCcomparedtobaselines,withatighterrangeand
Y
improvedempiricalworst-caseforbothmetrics.“-”:median,“△”:worst-caseROCgap,“▽”:worst-caseAUC.
ProportionofDCEMandtested-onlymodelsbyROCgapachieved,controllingforAUC
Allmodelswith Allmodelswith Allmodelswith Allmodelswith Allmodelswith
AUC=0.7+/-0.025 AUC=0.75+/-0.025 AUC=0.8+/-0.025 AUC=0.85+/-0.025 AUC=0.9+/-0.025
0.25 0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00 0.00
0.0 0.1 0.2 0.0 0.1 0.2 0.0 0.1 0.2 0.0 0.1 0.2 0.0 0.1 0.2
ROCgap ROCgap ROCgap ROCgap ROCgap
DCEM(n=219) DCEM(n=138) DCEM(n=260) DCEM(n=151) DCEM(n=337)
tested-only(n=157) tested-only(n=243) tested-only(n=237) tested-only(n=281) tested-only(n=412)
Figure3.RelativefrequenciesofROCgapsforDCEMvs.tested-onlymodelsatsimilarAUC(increasingtotheright),poolingmodels
acrossallk,q ,q tested.Dashedlines=meanROCgapbymodel.DCEMimprovesbiasmitigationamongmodelswithsimilarAUC.
y t
DCEM maintains competitive discriminative perfor- cardsreliablenegatives. Incontrast,theproposedapproach
mance. Fig. 2 (right) shows that DCEM outperforms incorporates reliable negatives in alignment with our as-
all baselines except for the tested-only model, which our sumptionsaboutlabelingbias,allowingittocounterbalance
approach lags by 0.028 AUC (DCEM: 0.787 vs. tested- biasedselectivelabeling. Trendsaresimilarforotherq ,q
t y
only: 0.815). Othercausalestimatorsachievesimilardis- andk [1/2,2]. Sincethetested-onlymodelisastrong
∈
criminativeperformancetothetested-onlyapproach(Ap- baseline,wenowcompareitdirectlytoDCEM.
pendixE.3).However,ourmethodimprovesonthe“default”
y-obsmodel,increasingthemedianAUCby0.130(y-obs: DCEMachievesbettertradeoffsbetweendiscriminative
0.657). SELF, which has a similar median ROC gap to performanceandbiasmitigation. Amongmodelswith
DCEM,underperformsDCEMby0.110AUC(SELF:0.677 similar AUC where AUC < 0.875, DCEM reduces ROC
vs. DCEM: 0.787). Other baselines also underperform. gaps compared to the tested-only model (Fig. 3). For ex-
This is expected, since some methods ignore label bias: ample,forAUC (0.825,0.875)(Fig.3,2ndfromright),
trainingonY˜ aloneismisspecifiedforE(Y X),sinceit DCEMimproves∈ theaverageROCgapby0.022(0.028vs.
|
incorrectlyassumesthatifT = 0,thenY = 0. Thesame 0.050),withsimilartrendsatlowerAUCs. Amongthebest-
argumentappliestoGroup0/1onlyapproaches. performingmodels(AUC>0.875;Fig.3,1stfromright),
bothmethodshavesimilarROCgaps.
Some baselines account for label noise/bias, but are mis-
specifiedunderdisparatecensorshipsincetheymakediffer- TheresultssuggestthatDCEMisnottradingdiscriminative
ent independence assumptions. Group peer loss assumes performanceforbiasmitigation. AtagivenAUC,DCEM
T X (Y,A), and SELF assumes T X Y, moreoftenyieldsmodelswithalowerROCgapthanthe
⊥⊥ | ⊥⊥ |
ignoring the dependence of biased selective labeling on tested-only model. Since the tested-only approach does
X. DragonNetaccountsforX byaddingP(T X,A = not account for label bias, it can achieve relatively high
|
1) P(T X,A = 0) to the default model’s estimates AUCwithoutmitigatingbias. Incontrast,DCEMexplicitly
(i.e− ., P(Y˜ | X)) as a “correction factor.” However, the counteractsdisparatecensorship. Asimilarcomparisonto
|
correction factor may be biased for true negatives: the SELFshowsthat,atlowROCgaps,DCEMlikewisefinds
oracle is zero, because P(Y˜ X) = P(Y X), but higher-AUCsolutionsthanSELF(AppendixE.6).
| |
P(T X,A = 1) P(T X,A = 0) = 0 in general
| − | ̸
undersystematiclabelingbias. 5.2.ResultsonsepsisclassificationinMIMIC-III
SAR-EM,whichismostsimilartotheproposedapproach,
DCEMhasbetterdiscriminativeperformancethanbase-
modelsmissingnessatrandom(i.e., T X,Y), butdis- lines. Fig.4comparestheROCgapandAUCofDCEM
̸⊥⊥
7
ycneuqerF
paGCOR
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=4
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure4.ROCgaps(left)andAUC(right)ofbaselinesandDCEMonsepsisclassificationtaskatq =1.5,k=4.Eachdotrepresentsa
t
differents .Ourmethod(DCEM,magenta)maintainscompetitiveorbetterbiasmitigationanddiscriminativeperformancecomparedto
T
baselines.“-”:median,“△”:worst-caseROCgap,“▽”:worst-caseAUC.
toselectedbaselinesattestingdisparityq =1.5,andtest- ROC gap of 0.133 (left; DragonNet: 0.144), and a mini-
t
ing rate multiplier k = 4. Each dot corresponds to one mumAUCof0.584(right;DragonNet: 0.574). Fig.4also
variation of s (laboratory testing policy). Our method showsthatDCEMachievesthetightestROCgaprange(left;
T
hasthehighestmedianAUCamongbaselines(ours: 0.620 DCEM:0.094vs. 2nd-best: 0.102)and2nd-tightestAUC
vs. DragonNet: 0.593),nearingtheoracle(y-model,0.633). range(right;DCEM:0.065vs. DragonNet: 0.018). Many
NotethatDCEMhasbetterdiscriminativeperformancethan baselinesalsoexhibitabimodalempiricalAUCdistribution
thetested-onlyapproach,suggestingthatextrapolationfrom andonlyperformwellunderspecificlabelingbehaviors.We
testedtountestedindividualsismoredifficultonthesepsis examinethesensitivityofbaselinestos byplottingAUC
T
classificationtaskthanthefullysynthetictasks. andROCgapsagainstcoefficientsofs (AppendixE.7).
T
WhileDragonNetiscompetitiveonthisdataset,itsrobust-
DCEM achieves good tradeoffs with bias mitigation.
nessandperformancecapabilitiesmaynotgeneralize(e.g.,
DCEM achieves a smaller median ROC gap compared
simulation results, Fig. 2). DCEM is the only approach
to five of eight baselines tested. Group peer loss, Drag-
testedthatachievedcompetitivediscriminativeperformance
onNet and the Group 0 approach achieve lower median
andbiasmitigationonbothdatasets. Trendsinperformance
ROCgapsof0.070,0.088and0.082,respectively(DCEM:
androbustnessaresimilarforotherk,q (AppendixE.1).
0.105). However, the Group 0 approach catastrophically t
fails(medianAUC:0.342). Modelsmayperformarbitrarily
Overall takeaways. In a simulation study of disparate
poorlyunderdisparatecensorshipiflabelingbiasessuffi-
censorship,DCEMmitigatesbiaswhileachievingsimilar
ciently“conceal”thetruedecisionboundary. Grouppeer
orbetterdiscriminativeperformancecomparedtobaselines.
loss (among many other baselines) also exhibits a much
Theproposedapproachisempiricallymorerobustthanbase-
widerAUCrangethantheproposedapproach(Grouppeer
linestochangesinthedata-generatingprocess. Onasepsis
loss: 0.182vs. DCEM:0.065),suggestingthatitsdiscrim-
classificationtask,DCEMmitigatesbiaswhileimprovingor
inative utility may be limited. DragonNet appears com-
maintainingdiscriminativeperformancecomparedtobase-
petitive (0.027 AUC lower than DCEM), but would only
linesacrossdifferentlabelingbehaviors. Thus,DCEMcan
performwellwhentheeffectofraceontestingiscloseto
potentiallymitigatebiaswithlessimpactondiscriminative
P(Y X) P(Y˜ X,A),whichisviolatediflabeling
| | − | | performancethanexistingmethods.
biases(largeeffectofraceontesting)arepresentinnegative
patients(P(Y X) P(Y˜ X,A)).
| ≈ | 6.RelatedWork
Manyapproaches,includingDCEM,obtainalowerROC
gapthantrainingony. Althoughtheoracleobtainsthehigh- Selectivelabeling/disparatecensorship. Disparatecen-
estmedianAUC,optimizingdiscriminativeperformanceon sorshipisavariationofselectivelabeling(Lakkarajuetal.,
y is not always guaranteed to mitigate bias. DCEM uses 2017; Kleinberg et al., 2018) and outcome measurement
labeling probabilities to mitigate bias via causal regular- error(Guerdanetal.,2023b). Selectivelabelingproblems
ization,whileDragonNetdirectlyusesanestimateofthe havebeenstudiedinclinicalsettings(Farahanietal.,2020;
labelingbiasasacorrectionfactor.Thus,theresultsvalidate Shanmugametal.,2024;Changetal.,2022;Mullainathan
thatthelabelingbiascanprovidesignalforbiasmitigation. &Obermeyer,2022;Balachandaretal.,2024),social/public
policy (Saxena et al., 2020; Kontokosta & Hong, 2021;
DCEMismorerobustthanmostbaselinestochangesin Lauferetal.,2022;Liu&Garg,2022;Kianietal.,2023),
s . DCEMmaintainsrobustbiasmitigationcapabilities andfinance(Bjo¨rkegren&Grissen,2020;Hendersonetal.,
T
acrosss ;i.e.,differencesinhowlabelersweighfeaturesin 2023), among other domains. For an extended literature
T
theirdecisions.Fig.4showsthatDCEMattainsamaximum reviewofselectivelabelingproblems,seeAppendixA.
8
paGCOR
CUAFromBiasedSelectiveLabelstoPseudo-Labels
Past work has trained ML models under disparate cen- dataexhibitingdisparatecensorshipcanamplifytheharm
sorship, directly encoding untested individuals as nega- of ML models to marginalized groups. We propose Dis-
tive(Henryetal.,2015;Jehietal.,2020;McDonaldetal., parateCensorshipExpectation-Maximization(DCEM),a
2021; Adams et al., 2022; Kamran et al., 2022). Previ- novelapproachtoclassification,tomitigatesuchharm. In
ousapproachesforlearningunderselectivelabelsleverage asimulationstudyandasepsisclassificationtask,DCEM
heterogeneityinhumandecisionstorecoveroutcomeesti- mitigatesbiasandmaintainscompetitivediscriminativeper-
mates(Lakkarajuetal.,2017;Kleinbergetal.,2018;Chen formance compared to baselines. Limitations of DCEM
etal.,2023),orusedomain-specificadjustments(Gholami includepotentialslowconvergence,sinceEMisiterative.
etal.,2018;Balachandaretal.,2024).WeproposeDCEM,a Modelevaluationunderdisparatecensorshipisalsoinher-
complementaryapproachformitigatingbiasunderdisparate entlydifficultduetothedifficultyofobtaininggroundtruth,
censorshipwithoutsuchrestrictions. motivating future work in dataset curation. Furthermore,
DCEMdoesnotlearnafullgenerativemodelwithallvari-
Semi-supervisedlearning. Semi-supervisedapproaches ables. Whilesuchamodelcouldtargetawiderrangeofesti-
do not assume labels for untested individuals. However, mands,itwouldalsoincreasethenumberoftermsthatneed
manycausally-motivatedmethodsdivergefromthecausal tobemodeled. Ultimately,DCEMisasteptowardsmiti-
model of disparate censorship (Madras et al., 2019; Yao gatingthedisproportionateimpactsofdisparatecensorship.
etal.,2021;Gargetal.,2023;Guerdanetal.,2023a;Gong Ourworkaimstoraiseawarenessofdisparatecensorship
et al., 2021; Kato et al., 2023; Sportisse et al., 2023) via andmotivatethestudyofbiasmitigationmethods.
different independence/causal relationships between vari-
ables. Filteringmethods(Hanetal.,2018;Lietal.,2020;
ImpactStatement
Nguyenetal.,2020;Chenetal.,2020;Zhangetal.,2021;
Zhaoetal.,2022)assumespecificmodelbehavioronnoisy Thispaperaddressesdisparatecensorship,arealisticsource
examples(e.g.,,noiseislearnedlateintraining(Arpitetal., oflabelbiasinML,andproposesamethodthatmitigatesits
2017)) or labeling bias (randomness/class-dependence), harms. Sincethegoalofthepaperisalignedwithreducing
whichdisparatecensorshipviolates. Wealsohighlighthis- inequityindecision-making,practicalusecasesofDCEM
toricalexpectation-maximizationapproachesforlearning areinherentlyhigh-stakessettings.Thus,webelievethatthe
with missing data (Ghahramani & Jordan, 1993; Ghahra- ethicalusageofDCEM(oranybiasmitigationapproach)
manietal.,1996;Ambroise&Govaert,2000),whichplace inthereal-worldrequiresprospectivemodelevaluationin
parametricassumptionsonthedata-generationprocess. We thecontextofuse(e.g.,shadowinghumandecision-makers)
use neural networks to target the estimands of interest to toassessunforeseennegativeimpacts. Ourworkprovidesa
circumventparametricassumptions. generalchoiceofbiasmitigation(areabetweenROCcurves)
anddiscriminativeperformancemetrics(AUC),whichare
Other alternatives include positive-unlabeled learning ap-
motivatedbyclinicaltaskswhereequitablyrankingindivid-
proachesthatassumelabelingdependsoncovariates(e.g.,
ualsintermsofresourceneedsisimportant. Practitioners
missingnotatrandom)(Bekkeretal.,2020;Furman´czyk
shouldensuretheirevaluationmetricsalignwithdomain-
etal.,2022;Gerychetal.,2022;Wangetal.,2024). How-
specificcriteriaforbiasmitigationandperformance.
ever,thesemethodsdonotleveragecorrectly-labeledneg-
atives,andnaively-incorporatingnegativeexampleswith-
outcausalassumptionsmaypotentiallyharmmodelperfor- Acknowledgements
mance or bias mitigation. Other methods for noisy-label
We thank (in alphabetical order) Donna Tjandra, Di-
learningmakeassumptionsincompatiblewithoursetting,
vya Shanmugan, Fahad Kamran, Jung Min Lee, Maggie
e.g. uniform noise within subgroups (Wang et al., 2021),
Makar, Meera Krishnamoorthy, Michael Ito, Sarah Jab-
almost-surelyclean&noisyexamples(Liu&Tao,2015;Pa-
bour,ShengpuTang,StephanieShepard,andWinstonChen
trinietal.,2017;Tjandra&Wiens,2023),differentvariable
forhelpfulconversationsandproofreading,andtheanony-
independence/directionalityrelationships(Wuetal.,2022),
mousreviewersfortheirconstructivefeedback. T.C.and
thatnoisy(i.e.,outofdistribution)examplesarerare(Wald
J.W.aresupportedbytheU.S.NationalHeart,Lung,and
&Saria,2023),orothernoiseconstraints(Lietal.,2021;
BloodInsituteoftheNationalInstitutesofHealth(Grant
Zhuetal.,2021). Ourapproachcomplementsexistingwork
No. 5R01HL158626-03). The views and conclusions in
byjointlymodelingselectiveandbiasedlabelingviacausal
this document are those of the authors and should not be
assumptionstailoredtoabiaseddecision-makingpipeline.
interpretedasnecessarilyrepresentingtheofficialpolicies,
eitherexpressedorimpliedoftheU.S.NationalInstitutes
7.Conclusion
ofHealth.
Whenbiasedhumandecisionsaffectobservationsofground
truth,applyingstandardsupervisedlearningtechniquesto
9FromBiasedSelectiveLabelstoPseudo-Labels
References Bjo¨rkegren,D.andGrissen,D. Behaviorrevealedinmobile
phoneusagepredictscreditrepayment. TheWorldBank
Adams, R., Henry, K. E., Sridharan, A., Soleimani, H.,
EconomicReview,34(3):618–634,2020.
Zhan,A.,Rawat,N.,Johnson,L.,Hager,D.N.,Cosgrove,
S.E.,Markowski,A.,etal. Prospective,multi-sitestudy Blum,A.andStangl,K. Recoveringfrombiaseddata: Can
of patient outcomes after implementation of the trews fairnessconstraintsimproveaccuracy? In1stSymposium
machinelearning-basedearlywarningsystemforsepsis. onFoundationsofResponsibleComputing,2020.
NatureMedicine,pp.1–6,2022.
Chang, T., Sjoding, M. W., and Wiens, J. Disparate cen-
Ambroise,C.andGovaert,G. EMalgorithmforpartially sorship&undertesting: Asourceoflabelbiasinclinical
knownlabels. InDataAnalysis,Classification,andRe- machine learning. In Proceedings of the 7th Machine
latedMethods,pp.161–166.Springer,2000. LearningforHealthcareConference,volume182ofPro-
ceedings of Machine Learning Research, pp. 343–390,
Arazo, E., Ortego, D., Albert, P., O’Connor, N. E., and Aug2022.
McGuinness,K. Pseudo-labelingandconfirmationbias
indeepsemi-supervisedlearning. In2020International Chen,J.,Li,Z.,andMao,X. Learningunderselectivela-
JointConferenceonNeuralNetworks(IJCNN),pp.1–8. belswithheterogeneousdecision-makers: Aninstrumen-
IEEE,2020. talvariableapproach. arXivpreprintarXiv:2306.07566,
2023.
Arpit,D.,Jastrzebski,S.,Ballas,N.,Krueger,D.,Bengio,
Chen,P.,Ye,J.,Chen,G.,Zhao,J.,andHeng,P.-A. Beyond
E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville,
class-conditionalassumption: Aprimaryattempttocom-
A., Bengio, Y., and Lacoste-Julien, S. A closer look
batinstance-dependentlabelnoise. InProceedingsofthe
at memorization in deep networks. In Proceedings of
AAAIConferenceonArtificialIntelligence, volume34,
the34thInternationalConferenceonMachineLearning,
2020.
volume70ofProceedingsofMachineLearningResearch,
pp.233–242,2017.
Delahanty,R.J.,Alvarez,J.,Flynn,L.M.,Sherwin,R.L.,
andJones,S.S.Developmentandevaluationofamachine
Bahadori,M.T.,Chalupka,K.,Choi,E.,Chen,R.,Stewart,
learningmodelfortheearlyidentificationofpatientsat
W.F.,andSun,J. Causalregularization. arXivpreprint
risk for sepsis. Annals of Emergency Medicine, 73(4):
arXiv:1702.02604,2017.
334–344,2019.
Balachandar, S., Garg, N., and Pierson, E. Domain con-
Dempster,A.P.,Laird,N.M.,andRubin,D.B. Maximum
straints improve risk prediction when outcome data is
likelihoodfromincompletedataviatheEMalgorithm.
missing. In12thInternationalConferenceonLearning
JournaloftheRoyalStatisticalSociety:SeriesB(method-
Representations,2024.
ological),39(1):1–22,1977.
Beery,T.A. Genderbiasinthediagnosisandtreatmentof Djulbegovic,B.,Elqayam,S.,Reljic,T.,Hozo,I.,Miladi-
coronaryarterydisease. Heart&Lung,24(6):427–435, novic,B.,Tsalatsanis,A.,Kumar,A.,Beckstead,J.,Tay-
1995. lor,S.,andCannon-Bowers,J. Howdophysiciansdecide
totreat: anempiricalevaluationofthethresholdmodel.
Bekker, J., Robberechts, P., andDavis, J. Beyondthese-
BMCMedicalInformaticsandDecisionMaking,14:1–10,
lectedcompletelyatrandomassumptionforlearningfrom
2014.
positiveandunlabeleddata. InMachineLearningand
KnowledgeDiscoveryinDatabases, pp.71–85, Cham, Englesson,E.andAzizpour,H.Generalizedjensen-shannon
2020. divergencelossforlearningwithnoisylabels.InRanzato,
M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,andVaughan,
Bergman, P., Kopko, E., and Rodriguez, J. E. A seven- J.W.(eds.),AdvancesinNeuralInformationProcessing
college experiment using algorithms to track students: Systems,volume34,pp.30284–30297,2021.
Impactsandimplicationsforequityandfairness. Techni-
calreport,NationalBureauofEconomicResearch,2021. Farahani,N.Z.,Sundaram,D.S.B.,Enayati,M.,Arunacha-
lam,S.P.,Pasupathy,K.,andArruda-Olson,A.M. Ex-
Binns,R.,Veale,M.,VanKleek,M.,andShadbolt,N. Like planatoryanalysisofamachinelearningmodeltoidentify
trainer,likebot? Inheritanceofbiasinalgorithmiccon- hypertrophiccardiomyopathypatientsfromEHRusing
tentmoderation. InSocialInformatics: 9thInternational diagnosticcodes. In2020IEEEInternationalConference
Conference,SocInfo2017,Oxford,UK,September13-15, onBioinformaticsandBiomedicine(BIBM),pp.1932–
2017,Proceedings,PartII9,pp.405–415,2017. 1937,2020.
10FromBiasedSelectiveLabelstoPseudo-Labels
Furman´czyk,K.,Mielniczuk,J.,Rejchel,W.,andTeisseyre, Han,B.,Yao,Q.,Yu,X.,Niu,G.,Xu,M.,Hu,W.,Tsang,
P. Jointestimationofposteriorprobabilityandpropensity I.W.,andSugiyama,M. Co-teaching: Robusttraining
score function for positive and unlabelled data. arXiv ofdeepneuralnetworkswithextremelynoisylabels. In
preprintarXiv:2209.07787,2022. Proceedings of the 32nd International Conference on
NeuralInformationProcessingSystems,pp.8536–8546,
Gardner,J.,Brooks,C.,andBaker,R. Evaluatingthefair-
2018.
nessofpredictivestudentmodelsthroughslicinganalysis.
In Proceedings of the 9th International Conference on Harris,C.R.,Millman,K.J.,vanderWalt,S.J.,Gommers,
LearningAnalytics&Knowledge,pp.225–234,2019. R.,Virtanen,P.,Cournapeau,D.,Wieser,E.,Taylor,J.,
Berg,S.,Smith,N.J.,Kern,R.,Picus,M.,Hoyer,S.,van
Garg, A., Nguyen, C., Felix, R., Do, T.-T., and Carneiro,
Kerkwijk,M.H.,Brett,M.,Haldane,A.,delR´ıo,J.F.,
G. Instance-dependentnoisylabellearningviagraphical
Wiebe,M.,Peterson,P.,Ge´rard-Marchant,P.,Sheppard,
modelling. InProceedingsoftheIEEE/CVFWinterCon-
K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C.,
ferenceonApplicationsofComputerVision(WACV),pp.
and Oliphant, T. E. Array programming with NumPy.
2288–2298,2023.
Nature,585(7825):357–362,September2020.
Gerych,W.,Hartvigsen,T.,Buquicchio,L.,Agu,E.,and
Hartvigsen,T.,Sen,C.,Brownell,S.,Teeple,E.,Kong,X.,
Rundensteiner,E. Recoveringthepropensityscorefrom
and Rundensteiner, E. A. Early Prediction of MRSA
biased positive unlabeled data. In Proceedings of the
InfectionsusingElectronicHealthRecords. InHEALTH-
AAAIConferenceonArtificialIntelligence, volume36,
INF,pp.156–167,2018.
pp.6694–6702,2022.
Henderson,P.,Chugg,B.,Anderson,B.,Altenburger,K.,
Ghahramani,Z.andJordan,M. Supervisedlearningfrom
Turk,A.,Guyton,J.,Goldin,J.,andHo,D.E. Integrating
incompletedataviaanEMapproach.AdvancesinNeural
rewardmaximizationandpopulationestimation: Sequen-
InformationProcessingSystems,6,1993.
tial decision-making for internal revenue service audit
Ghahramani,Z.,Hinton,G.E.,etal. TheEMalgorithmfor selection. In Proceedings of the AAAI Conference on
mixturesoffactoranalyzers. Technicalreport,Technical ArtificialIntelligence,volume37,pp.5087–5095,2023.
ReportCRG-TR-96-1,UniversityofToronto,1996.
Henry,K.E.,Hager,D.N.,Pronovost,P.J.,andSaria,S.
Gholami, S., Mc Carthy, S., Dilkina, B., Plumptre, A., Atargetedreal-timeearlywarningscore(trewscore)for
Tambe, M., Driciru, M., Wanyama, F., Rwetsiba, A., septic shock. Science Translational Medicine, 7(299):
Nsubaga,M.,Mabonga,J.,etal. Adversarymodelsac- 299ra122–299ra122,2015.
countforimperfectcrimedata: Forecastingandplanning
Hu,X.,Niu,Y.,Miao,C.,Hua,X.-S.,andZhang,H. On
againstreal-worldpoachers. InInternationalConference
non-randommissinglabelsinsemi-supervisedlearning.
onAutonomousAgentsandMultiagentSystems,2018.
In10thInternationalConferenceonLearningRepresen-
Glynn,A.N.andQuinn,K.M. Anintroductiontotheaug- tations,2022.
mentedinversepropensityweightedestimator. Political
Analysis,18(1):36–56,2010. Imbens,G.W.andRubin,D.B. Causalinferenceinstatis-
tics,social,andbiomedicalsciences. CambridgeUniver-
Gong,C.,Wang,Q.,Liu,T.,Han,B.,You,J.J.,Yang,J., sityPress,2015.
andTao,D. Instance-dependentpositiveandunlabeled
learningwithlabelingbiasestimation.IEEETransactions Janzing, D. Causal regularization. Advances in Neural
onPatternAnalysisandMachineIntelligence,44:4163– InformationProcessingSystems,32,2019.
4177,2021.
Jehi,L.,Ji,X.,Milinovich,A.,Erzurum,S.,Rubin,B.P.,
Guerdan,L.,Coston,A.,Holstein,K.,andWu,Z.S. Coun- Gordon, S., Young, J.B., andKattan, M.W. Individu-
terfactualpredictionunderoutcomemeasurementerror. alizing risk prediction for positive coronavirus disease
InProceedingsofthe2023ACMConferenceonFairness, 2019testing: resultsfrom11,672patients. Chest,158(4):
Accountability,andTransparency,pp.1584–1598,2023a. 1364–1375,2020.
Guerdan, L., Coston, A., Wu, Z. S., and Holstein, K. Johnson,A.E.,Pollard,T.J.,Shen,L.,Lehman,L.-w.H.,
Ground(less)truth: Acausalframeworkforproxylabels Feng,M.,Ghassemi,M.,Moody,B.,Szolovits,P.,An-
inhuman-algorithmdecision-making. InProceedingsof thony Celi, L., and Mark, R. G. MIMIC-III, a freely
the2023ACMConferenceonFairness,Accountability, accessible critical care database. Scientific Data, 3(1):
andTransparency,pp.688–704,2023b. 1–9,2016.
11FromBiasedSelectiveLabelstoPseudo-Labels
Johnson,A.E.,Aboab,J.,Raffa,J.D.,Pollard,T.J.,Delib- Lee, D.-H. Pseudo-label: The simple and efficient semi-
erato,R.O.,Celi,L.A.,andStone,D.J. Acomparative supervised learning method for deep neural networks.
analysisofsepsisidentificationmethodsinanelectronic In Workshop on challenges in representation learning,
database. CriticalCareMedicine,46(4):494–499,2018. ICML,volume3(2),pp.896,2013.
Kamran,F.,Tang,S.,O¨tles¸,E.,McEvoy,D.S.,Saleh,S.N.,
Li,J.,Socher,R.,andHoi,S.C.H. Dividemix: Learning
Gong, J., Li, B. Y., Dutta, S., Liu, X., Medford, R. J., with noisy labels as semi-supervised learning. In 8th
Valley,T.S.,West,L.R.,Singh,K.,Blumberg,S.,Don- InternationalConferenceonLearningRepresentations,
nelly, J. P., Shenoy, E. S., Ayanian, J. Z., Nallamothu, ICLR,2020.
B.K.,Sjoding,M.W.,andWiens,J. Earlyidentification
of patients admitted to hospital for covid-19 at risk of Li,X.,Liu,T.,Han,B.,Niu,G.,andSugiyama,M.Provably
clinicaldeterioration: modeldevelopmentandmultisite end-to-end label-noise learning without anchor points.
externalvalidationstudy. TheBMJ,376,2022. InProceedingsofthe38thInternationalConferenceon
MachineLearning,pp.6403–6413,2021.
Kato, M., Wu, S., Kureishi, K., andYasui, S. Automatic
debiasedlearningfrompositive,unlabeled,andexposure Liu, T. and Tao, D. Classification with noisy labels by
data. arXivpreprintarXiv:2303.04797,2023. importancereweighting. IEEETransactionsonPattern
AnalysisandMachineIntelligence,38(3):447–461,2015.
Kennedy,E.H. Towardsoptimaldoublyrobustestimation
of heterogeneous causal effects. Electronic Journal of Liu, Y. and Guo, H. Peer loss functions: Learning from
Statistics,17(2):3008–3049,2023. noisy labels without knowing noise rates. In Proceed-
ings of the 37th International Conference on Machine
Kiani,S.,Barton,J.,Sushinsky,J.,Heimbach,L.,andLuo,
Learning,2020.
B.Counterfactualpredictionunderselectiveconfounding.
arXivpreprintarXiv:2310.14064,2023.
Liu, Z. and Garg, N. Equity in resident crowdsourcing:
Measuringunder-reportingwithoutgroundtruthdata. In
Kingma,D.P.andBa,J. Adam:Amethodforstochasticop-
Proceedingsofthe23rdACMConferenceonEconomics
timization. In3rdInternationalConferenceonLearning
andComputation,pp.1016–1017,2022.
Representations,2015.
Madras,D.,Creager,E.,Pitassi,T.,andZemel,R. Fairness
Kleinberg,J.,Lakkaraju,H.,Leskovec,J.,Ludwig,J.,and
throughcausalawareness. ProceedingsoftheConference
Mullainathan, S. Human decisions and machine pre-
onFairness,Accountability,andTransparency,2019.
dictions. TheQuarterlyJournalofEconomics,133(1):
237–293,2018.
McDonald, S. A., Medford, R. J., Basit, M. A., Diercks,
Kontokosta, C. E. and Hong, B. Bias in smart city gov- D. B., and Courtney, D. M. Derivation with internal
ernance: Howsocio-spatialdisparitiesin311complaint validationofamultivariablepredictivemodeltopredict
behaviorimpactthefairnessofdata-drivendecisions.Sus- covid-19testresultsinemergencydepartmentpatients.
tainableCitiesandSociety,64:102503,2021. AcademicEmergencyMedicine,28(2):206–214,2021.
Laine, R., Hyttinen, A., and Mathioudakis, M. Evaluat- Menon,A.K.,VanRooyen,B.,andNatarajan,N. Learning
ing decision makers over selectively labelled data: A frombinarylabelswithinstance-dependentnoise. Ma-
causalmodellingapproach. InDiscoveryScience: 23rd chineLearning,107(8):1561–1595,2018.
InternationalConference,DS2020,Thessaloniki,Greece,
October19–21,2020,Proceedings23,pp.3–18,2020. Mulherin,S.A.andMiller,W.C.Spectrumbiasorspectrum
effect? subgroupvariationindiagnostictestevaluation.
Lakkaraju,H.,Kleinberg,J.,Leskovec,J.,Ludwig,J.,and AnnalsofInternalMedicine,137(7):598–602,2002.
Mullainathan, S. The selective labels problem: Evalu-
ating algorithmic predictions in the presence of unob- Mullainathan,S.andObermeyer,Z. Diagnosingphysician
servables. In Proceedings of the 23rd ACM SIGKDD error: Amachinelearningapproachtolow-valuehealth
InternationalConferenceonKnowledgeDiscoveryand care. TheQuarterlyJournalofEconomics,137(2):679–
DataMining,pp.275–284,2017. 727,2022.
Laufer,B.,Pierson,E.,andGarg,N. End-to-endauditing Nguyen,D.T.,Mummadi,C.K.,Ngo,T.,Nguyen,T.H.P.,
ofdecisionpipelines. InICMLWorkshoponResponsible Beggel,L.,andBrox,T. SELF:LearningtoFilterNoisy
Decision-Making in Dynamic Environments., pp. 1–7, LabelswithSelf-Ensembling. In8thInternationalCon-
2022. ferenceonLearningRepresentations,2020.
12FromBiasedSelectiveLabelstoPseudo-Labels
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Rockafellar,R.T. Convexanalysis. PrincetonUniversity
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga, Press,Princeton,N.J.,1970.
L.,Desmaison,A.,Ko¨pf,A.,Yang,E.,DeVito,Z.,Rai-
Rosenbaum, P. R. and Rubin, D. B. The central role of
son,M.,Tejani,A.,Chilamkurthy,S.,Steiner,B.,Fang,
thepropensityscoreinobservationalstudiesforcausal
L.,Bai,J.,andChintala,S. Pytorch: Animperativestyle,
effects. Biometrika,70(1):41–55,1983.
high-performancedeeplearninglibrary,2019.
Patrini,G.,Rozza,A.,KrishnaMenon,A.,Nock,R.,and Saxena, D., Badillo-Urquiola, K., Wisniewski, P. J., and
Qu,L.Makingdeepneuralnetworksrobusttolabelnoise: Guha,S. Ahuman-centeredreviewofalgorithmsused
Alosscorrectionapproach. InProceedingsoftheIEEE withintheu.s.childwelfaresystem.InProceedingsofthe
ConferenceonComputerVisionandPatternRecognition, 2020CHIConferenceonHumanFactorsinComputing
pp.1944–1952,2017. Systems,pp.1–15,2020.
Pearl,J. Causality. Cambridgeuniversitypress,2009. Schulman,K.A.,Berlin,J.A.,Harless,W.,Kerner,J.F.,
Sistrunk, S., Gersh, B. J., Dube, R., Taleghani, C. K.,
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Burke,J.E.,Williams,S.,etal. Theeffectofraceandsex
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
onphysicians’recommendationsforcardiaccatheteriza-
Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cour-
tion. NewEnglandJournalofMedicine,340(8):618–626,
napeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,E.
1999.
Scikit-learn: Machine learning in Python. Journal of
MachineLearningResearch,12:2825–2830,2011. Seymour, C. W., Liu, V. X., Iwashyna, T. J., Brunkhorst,
F. M., Rea, T. D., Scherag, A., Rubenfeld, G., Kahn,
Peng,A.,Nushi,B.,Kıcıman,E.,Inkpen,K.,Suri,S.,and
J.M.,Shankar-Hari,M.,Singer,M.,etal. Assessment
Kamar,E. Whatyouseeiswhatyouget? theimpactof
ofclinicalcriteriaforsepsis: forthethirdinternational
representationcriteriaonhumanbiasinhiring. InPro-
consensusdefinitionsforsepsisandsepticshock(sepsis-
ceedingsoftheAAAIConferenceonHumanComputation
3). JAMA,315(8):762–774,2016.
andCrowdsourcing,volume7,pp.125–134,2019.
Shalev-Shwartz,S.andBen-David,S. Understandingma-
Pierson, E., Corbett-Davies, S., andGoel, S. Fastthresh-
chinelearning: Fromtheorytoalgorithms. Cambridge
old tests for detecting discrimination. In International
UniversityPress,2014.
Conference on Artificial Intelligence and Statistics, pp.
96–105,2018. Shanmugam, D., Hou, K., and Pierson, E. Quantifying
disparitiesinintimatepartnerviolence: amachinelearn-
Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S.,
ingmethodtocorrectforunderreporting. npjWomen’s
Jenson,D.,Shoemaker,A.,Ramachandran,V.,Barghouty,
Health,2(1),2024.
P.,Phillips,C.,Shroff,R.,etal. Alarge-scaleanalysisof
racialdisparitiesinpolicestopsacrosstheunitedstates.
Shi,C.,Blei,D.,andVeitch,V. Adaptingneuralnetworks
NatureHumanBehaviour,4(7):736–745,2020.
fortheestimationoftreatmenteffects. AdvancesinNeu-
ralInformationProcessingSystems,32,2019.
Rambachan,A.andRoth,J. Biasin,biasout? Evaluating
thefolkwisdom. In1stSymposiumonFoundationsof
Singer,M.,Deutschman,C.S.,Seymour,C.W.,Shankar-
ResponsibleComputing,2020.
Hari,M.,Annane,D.,Bauer,M.,Bellomo,R.,Bernard,
Rhee, C. and Klompas, M. Sepsis trends: increasing in- G.R.,Chiche,J.-D.,Coopersmith,C.M.,etal. Thethird
cidenceanddecreasingmortality, orchangingdenomi- internationalconsensusdefinitionsforsepsisandseptic
nator? Journal of Thoracic Disease, 12(Suppl 1):S89, shock(Sepsis-3). Jama,315(8):801–810,2016.
2020.
Sportisse, A., Schmutz, H., Humbert, O., Bouveyron,
Rizve,M.N.,Duarte,K.,Rawat,Y.S.,andShah,M. Inde- C., and Mattei, P.-A. Are labels informative in semi-
fenseofpseudo-labeling: Anuncertainty-awarepseudo- supervised learning? estimating and leveraging the
labelselectionframeworkforsemi-supervisedlearning. missing-data mechanism. In Proceedings of the 40th
In9thInternationalConferenceonLearningRepresenta- InternationalConferenceonMachineLearning,2023.
tions,2021.
Su¨hr,T.,Hilgard,S.,andLakkaraju,H. Doesfairranking
Robins, J.M., Rotnitzky, A., andZhao, L.P. Estimation improveminorityoutcomes? Understandingtheinterplay
ofregressioncoefficientswhensomeregressorsarenot of human and algorithmic biases in online hiring. In
always observed. Journal of the American Statistical Proceedingsofthe2021AAAI/ACMConferenceonAI,
Association,pp.846–866,1994. Ethics,andSociety,pp.989–999,2021.
13FromBiasedSelectiveLabelstoPseudo-Labels
Teeple, E., Hartvigsen, T., Sen, C., Claypool, K. T., and Yao, Y., Liu, T., Gong, M., Han, B., Niu, G., and Zhang,
Rundensteiner,E.A.Clinicalperformanceevaluationofa K. Instance-dependentlabel-noiselearningunderastruc-
machinelearningsystemforpredictinghospital-acquired turalcausalmodel. InAdvancesinNeuralInformation
clostridiumdifficileinfection. InHEALTHINF,pp.656– ProcessingSystems,volume34,pp.4409–4420,2021.
663,2020.
Zhang, Y., Zheng, S., Wu, P., Goswami, M., and Chen,
Thepandasdevelopmentteam. pandas-dev/pandas: Pandas, C. Learningwithfeature-dependentlabelnoise: Apro-
2020. gressiveapproach. In9thInternationalConferenceon
LearningRepresentations,2021.
Tjandra,D.andWiens,J. Leveraginganalignmentsetin
tacklinginstance-dependentlabelnoise. InProceedings Zhang,Z.andSabuncu,M.R. Generalizedcrossentropy
of the Conference on Health, Inference, and Learning, lossfortrainingdeepneuralnetworkswithnoisylabels.
2023. InProceedingsofthe32ndInternationalConferenceon
NeuralInformationProcessingSystems,pp.8792–8802,
Van Der Laan, M. J. and Rubin, D. Targeted maximum
2018.
likelihood learning. The International Journal of Bio-
statistics,2(1),2006. Zhao, G., Li, G., Qin, Y., Liu, F., and Yu, Y. Centrality
andconsistency: Two-stagecleansamplesidentification
Virtanen,P.,Gommers,R.,Oliphant,T.E.,Haberland,M.,
for learning with instance-dependent noisy labels. In
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Avidan,S.,Brostow,G.,Cisse´,M.,Farinella,G.M.,and
Weckesser,W.,Bright,J.,vanderWalt,S.J.,Brett,M.,
Hassner, T. (eds.), Computer Vision – ECCV 2022, pp.
Wilson,J.,Millman,K.J.,Mayorov,N.,Nelson,A.R.J.,
21–37,2022.
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,
Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Zhu, X. and Ghahramani, Z. Learning from labeled and
Perktold,J.,Cimrman,R.,Henriksen,I.,Quintero,E.A., unlabeleddatawithlabelpropagation. Technicalreport,
Harris,C.R.,Archibald,A.M.,Ribeiro,A.H.,Pedregosa, CenterforAutomatedLearningandDiscovery,Carnegie
F.,vanMulbregt,P.,andSciPy1.0Contributors. SciPy MellonUniversity,2002.
1.0: FundamentalAlgorithmsforScientificComputing
inPython. NatureMethods,17:261–272,2020. Zhu, Z., Song, Y., and Liu, Y. Clusterability as an alter-
nativetoanchorpointswhenlearningwithnoisylabels.
Vogel, R., Bellet, A., and Cle´menc¸on, S. Learning fair InProceedingsofthe38thInternationalConferenceon
scoring functions: Bipartite ranking under ROC-based MachineLearning,pp.12912–12923,2021.
fairnessconstraints. InInternationalConferenceonArti-
ficialIntelligenceandStatistics,pp.784–792,2021.
Wager, S. Stats361: Causalinference. Technicalreport,
StanfordUniversity,2020.
Wald, Y. and Saria, S. Birds of an odd feather: Guaran-
teedout-of-distribution(OOD)novelcategorydetection.
InUncertaintyinArtificialIntelligence,pp.2179–2191,
2023.
Wang, J., Liu, Y., and Levy, C. Fair classification with
group-dependentlabelnoise. InProceedingsofthe2021
ACMConferenceonFairness,Accountability,andTrans-
parency,pp.526–536,2021.
Wang, X., Chen, H., Guo, T., andWang, Y. Pue: Biased
positive-unlabeled learning enhancement by causal in-
ference. Advances in Neural Information Processing
Systems,36,2024.
Wu,S.,Gong,M.,Han,B.,Liu,Y.,andLiu,T. Fairclas-
sification with instance-dependent label noise. In Pro-
ceedings of the First Conference on Causal Learning
andReasoning,volume177ofProceedingsofMachine
LearningResearch,pp.927–943,Apr2022.
14FromBiasedSelectiveLabelstoPseudo-Labels
A.Selectivelabelingintheliterature
WeenumeratedomainsinwhichourliteraturereviewfoundinstancesofselectivelabelproblemsintheMLmethodsand
applicationsliterature:
• Healthcare: Laboratory/diagnostictesting(Changetal.,2022;Mullainathan&Obermeyer,2022)anddiagnosis(Fara-
hanietal.,2020;Shanmugametal.,2024;Balachandaretal.,2024)
• Social&publicpolicy: Childwelfareassessment(Saxenaetal.,2020;Kianietal.,2023),urbanplanning/policy(Kon-
tokosta&Hong,2021;Lauferetal.,2022;Liu&Tao,2015),hiringpipelines(Pengetal.,2019;Su¨hretal.,2021),
studentplacement(Bergmanetal.,2021),andbiasinpolicing(Rambachan&Roth,2020;Piersonetal.,2020)
• Finance: Creditrepayment(Bjo¨rkegren&Grissen,2020)andfinancialauditing(Hendersonetal.,2023)
• Others/miscellaneous: Wildlifeconservation(Gholamietal.,2018),socialmediacontentmoderation(Binnsetal.,
2017)
Wenotethatthisisnotanexhaustivelistofallpapersintheselectivelabelingliteratureorrelatedproblemsettings. However,
thislistillustratesthebroadapplicabilityandrelevanceofourproblemsetting.
B.OmittedProofs
Forconvenience,werestatealltheoremsandpropositionshere.
B.1.Theorem3.1
Theorem(E-stepderivation). Theposteriorconditionalmeanofy(i) =1giventheobserveddata,Q(y(i))≜E[y(i) =1
|
t(i),y˜(i),a(i),x(i)],isequalto
(cid:40)
y˜(i) t(i) =1
Q(y(i))= . (14)
E[y(i) =1 x(i)] otherwise
|
Proof. Wedropsuperscripts()(i) intheproofforclarity. DenoteQ(y)asposteriordistributionofygiventheobserved
data,E[y =1 t,y˜,a,x](i.e.,· theE-stepestimate). First,wecanwrite
|
Q(y)≜E[y =1 t,y˜,a,x]=E[y =1 y˜,x,t]=P(y =1 y˜,x,t) (15)
| | |
forsimplicity,whereweusethefactthatY A (T,X)andthefactthatE[y =1 y˜,x,t]isbinary. Proceeding,wecan
⊥⊥ | |
write:
=t P(y =1 y˜,x,t=1)+(1 t) P(y =1 y˜,x,t=0) (16)
· | − · |
=ty˜+(1 t)[y˜P(y =1 y˜=1,x,t=0)+(1 y˜)P(y =1 y˜=0,x,t=0)] (17)
− | − |
=ty˜+(1 t)(1 y˜)P(y =1 y˜=0,x,t=0) (18)
− − |
P(y˜=0 y =1,x,t=0)P(y =1 x,t=0)
=ty˜+(1 t)(1 y˜) | | (19)
− − · P(y˜=0 x,t=0)
|
=ty˜+(1 t)(1 y˜)P(y =1 x). (20)
− − |
Thesecondequalityfollowssincet = 1 = y˜= y. ThethirdequalityholdssinceP(y = 1 y˜= 1,x,t = 0) = 0by
⇒ |
construction,sincey˜=ytunderdisparatecensorship. Thefinalstepfollowsfromthreefacts: (1)P(y˜=0 y =1,x,t=
|
0)=1forallx,(2)P(y˜=0 x,t=0)=1forallx,and(3)Y T X. ThisismoresuccinctlyrewrittenasE-stepis:
| ⊥⊥ |
(cid:40)
y˜ t=1
Q(y)= , (21)
E[y =1 x] otherwise(i.e.,y˜=0 t=0)
| ∧
whichiswhatwewantedtoshow.
Remark B.1. Sincey(i) isbinarybyassumption, thisresultfullydeterminestheposteriordistributionsinceE[y = 1
y˜,x,t]=1 E[y =0 y˜,x,t]. |
− |
15FromBiasedSelectiveLabelstoPseudo-Labels
B.2.Theorem3.2
Theorem(M-stepderivation). LetP(U,A,X,T,Y˜;θ)beamodelforthejointdatadistributionparameterizedbysome
arbitraryθ ΘinsomeparameterspaceΘ,whichfactorizesaccordingtothedisparatecensorshipDAG=(Fig.1). Let
Q(y) ≜ E[y∈ = 1 t,y˜,a,x]betheposteriorexpectationthaty = 1giventheobserveddata. Then(replacingrandom
|
variableswiththeirrealizedcounterparts),wehave
N N
1 (cid:88) 1 (cid:88)
max logP(u(i),a(i),x(i),t(i),y˜(i);θ) max Q(y(i))logP(y(i) x(i);θ )
θ N ≥ θ N |
Y|X
i=1 i=1
+(1 Q(y(i)))log(1 P(y(i) x(i);θ ))
Y|X
− − |
+Q(y(i))logP(y˜(i) y(i),t(i);θ ) (22)
|
Y˜|Y,T
whereθ =[θ θ ]⊤.
Y|X Y˜|Y,T
Proof. Wefirstconstructtheevidence-basedlowerbound(ELBO)oftheLHSinthetheoremstatement. First,forasingle
exampleindexedbyi,wecanwrite:
(cid:88) P(u(i),a(i),x(i),t(i),y˜(i),y(i);θ)
logP(u(i),a(i),x(i),t(i),y˜(i);θ)=log Q(y(i)) (23)
Q(y(i))
y(i)∈{0,1}
(cid:88) P(u(i),a(i),x(i),t(i),y˜(i),y(i);θ)
Q(y(i))log (24)
≥ Q(y(i))
y(i)∈{0,1}
viaJensen’sinequality. Then,wenotethat
1 (cid:88)N (cid:88) P(u(i),a(i),x(i),t(i),y˜(i),y(i);θ)
max Q(y(i))log
θ N Q(y(i))
i=1y(i)∈{0,1}
N
1 (cid:88) (cid:88)
=max Q(y(i))logP(u(i),a(i),x(i),t(i),y˜(i),y(i);θ), (25)
θ N
i=1y(i)∈{0,1}
droppingQ(y(i))logQ(y(i)),whichisconstantwithrespecttoθ,afterexpandingthelogterm. WecanthenusetheDAGto
factorizethejointdistributionofallvariables(includinglatentvariableY),whichisgivenby
P(Y˜,Y,T,X,A,U)=P(Y˜ Y,T)P(Y X)P(T X,A)P(X,A,U). (26)
| | |
Notethatweneedonlymodelthefirsttwotermsforestimationofy(i). Thefirsttwotermsdonotinvolvey(i),arenot
parameterized,andcanbedroppedfromthemaximizationproblem. Hence,weproceedtowrite
N
1 (cid:88) (cid:88)
= max Q(y(i))logP(y(i) x(i);θ )P(y˜(i) t(i),y(i);θ ) (27)
θ N |
Y˜|X
|
Y˜|Y,T
i=1y(i)∈{0,1}
whereθ =[θ θ ]⊤. Thiscanberewrittenas
Y|X Y˜|Y,T
N
1 (cid:88) (cid:88)
= max Q(y(i))logP(y(i) x(i);θ )+Q(y(i))P(y˜(i) t(i),y(i);θ ), (28)
θ N |
Y˜|X
|
Y˜|Y,T
i=1y(i)∈{0,1}
atwhichpointwenotethatitissufficienttoshowthat
(1 Q(y(i)))log(1 P(y˜(i) y(i),t(i);θ )) (29)
− − |
Y˜|Y,T
isconstantinθ. Wecanrewritetheaboveas
(1 Q(y(i)))[y˜(i)log(P(y˜(i) =1 y(i) =0,t(i);θ ))+(1 y˜(i))log(P(y˜(i) =0 y(i) =0,t(i);θ ))]. (30)
− |
Y˜|Y,T
− |
Y˜|Y,T
16FromBiasedSelectiveLabelstoPseudo-Labels
First, notethattheevent y˜(i) = 1 y(i) = 0 occurswithprobabilityzerobydefinition(recally˜(i) = y(i)t(i)). Thus,
{ | }
P(y˜(i) =1 y(i) =0,t(i);θ ))cannotchangewithrespecttoθ;wedropitfromthemaximizationproblem. Similarly,
|
Y˜|Y,T
P(y˜(i) =0 y(i) =0,t(i))=0bydefinition,so(1 Q(y(i)))log(1 P(y˜(i) y(i),t(i)))=0whichisconstantasneeded,
| − − |
fromwhichthetheoremfollows.
RemarkB.2. Inthetheoremstatement,replacingP(y(i) x(i);θ )withyˆ(i)andP(y˜(i) y(i),t(i);θ )withh (),
|
Y|X
|
Y˜|Y,T ϕ
·
assumingy(i)andy˜(i)arebinary,andwritingtheexplicitformofnegativebinarycross-entropy(e.g.,ylogy+(1 y)logyˆ)
−
recoverstheformoftheM-stepobjectiveseeninEq.7. Notethattheoptimizationproblemflipsfromamaximizationtoa
minimizationduetotherelationshipbetweenmaximizinglog-likelihoodofbinaryvariable(s)andminimizingcross-entropy
loss.
B.3.Theorem3.4
Theorem(Strengthofthecausalregularizerintˆ(i)). Foranexampleindexedbyi,Q(y(i)) [0,1),andJ(i)definedasin
∈
Eq.88,R(J(i))ismonotonicallyincreasingintˆ(i)on(0,1].
Proof. Asaproofoutline,wefirstshowtheclosed-formofyˆ(i) (Q(y(i)),tˆ(i))bysolvingthefirst-orderoptimalitycondition
OPT
ofEq.88. Then,weshowthatyˆ(i) (Q(y(i)),tˆ(i))isdecreasingintˆ(i),andattainsamaximumofQ(y(i))astˆ(i) 0+. We
OPT →
concludebyshowingthattheprecedingimpliesthatR(J(i))ismonotonicallyincreasingintˆ(i)on(0,1],asdesired.
Thefirst-orderoptimalityconditionofEq.88is
(cid:18) Q(y(i)) 1 Q(y(i))(cid:19) tˆ(i)Q(y(i))
− + =0. (31)
− yˆ(i) − 1 yˆ(i) 1 yˆ(i)tˆ(i)
− −
Byassumption(convexityof ),theminimizerisunique. Somealgebrayields
L
Q(y(i))(1 yˆ(i))(1 yˆ(i)tˆ(i))+(1 Q(y(i)))yˆ(i)(1 yˆ(i)tˆ(i))+tˆ(i)Q(y(i))yˆ(i)(1 yˆ(i)) =0 (32)
− − − − − −
(tˆ(i)+Q(y(i))tˆ(i))yˆ(i)2 (2Q(y(i))tˆ(i)+1)yˆ(i)+Q(y(i)) =0, (33)
⇐⇒ −
from which we can apply the quadratic formula. Define B(Q(y(i)),tˆ(i)) ≜ 2Q(y(i))tˆ(i) +1 and D(Q(y(i)),tˆ(i)) ≜
( B(Q(y(i)),tˆ(i)))2 4Q(y(i))(Q(y(i))tˆ(i)+tˆ(i)).5 Thequadraticformulayieldssolutions
− −
(cid:113)
B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i))
yˆ(i) (Q(y(i)),tˆ(i))= ± . (34)
OPT 2(tˆ(i)+Q(y(i))tˆ(i))
Weusethefactthatyˆ(i) mustbein[0,1]andtheconstraintsthattˆ(i) (0,1]andQ(y(i)) [0,1)todeterminewhich
∈ ∈
branch of Eq. 34 yields real solutions in [0,1]. By Lemma 1, D(Q(y(i),tˆ(i)) 0, so the solutions are real. Then, by
≥
Lemma2,
(cid:113)
B(Q(y(i)),tˆ(i))+ D(Q(y(i),tˆ(i))
1, (35)
2(tˆ(i)+Q(y(i))tˆ(i)) ≥
eliminatingthatbranch. Byelimination,
(cid:113)
B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i))
yˆ(i) (Q(y(i)),tˆ(i))= − . (36)
OPT 2(tˆ(i)+Q(y(i))tˆ(i))
5WeuseletterB(·)becauseitcorrespondstocoefficientbintheconventionalquadraticformula:
√
−b± b2−4ac
x=
2a
foraquadraticpolynomialax2+bx+c=0.WechooseletterD(·)sinceitcorrespondstothediscriminant.
17FromBiasedSelectiveLabelstoPseudo-Labels
Lemma 3 verifies that the resulting yˆ(i) (Q(y(i)),tˆ(i)) are in [0, 1], as needed. To proceed, it suffices to show that
OPT
yˆ(i) (Q(y(i)),tˆ(i))isdecreasingintˆ(i)andattainsamaximumofQ(y(i))astˆ(i) 0+.
OPT →
ApplyingLemma4,toprovethatyˆ(i) (Q(y(i)),tˆ(i))decreasesintˆ(i),itissufficienttoshow
OPT
(cid:113)
1 2tˆ(i)Q(y(i))2 D(Q(y(i)),tˆ(i)))<0 (37)
− −
(cid:113)
because1 2tˆ(i)Q(y(i))2 D(Q(y(i),tˆ(i))hasthesamesignasthederivativeofyˆ(i) (Q(y(i)),tˆ(i))forthevaluesof
− − OPT
(tˆ(i),Q(y(i)))ofinterest.
Forvaluesoftˆ(i) (0,1]andQ(y(i)) [0,1)suchthat1 2tˆ(i)Q(y(i))2 <0,Lemma1yieldsthedesiredresult. Forthe
∈ ∈ −
remainingvaluesof(tˆ(i),Q(y(i))),wecanwrite
1 4Q(y(i))2tˆ(i)+4Q(y(i))4tˆ(i)2 <D(Q(y(i)),tˆ(i)) (38)
−
1 4Q(y(i))2tˆ(i)+4Q(y(i))4tˆ(i)2 <1 4Q(y(i))2tˆ(i)+4Q(y(i))2tˆ(i)2 (39)
⇐⇒ − −
Q(y(i))2 <1 (40)
⇐⇒
whichholdsforallfeasiblevaluesofQ(y(i)) [0,1). Lastly,duetothemonotonicityofyˆ(i) (Q(y(i)),tˆ(i))intˆ(i),the
∈ OPT
followingone-sidedlimitisthemaximum:
lim yˆ(i) (Q(y(i)),tˆ(i))= max yˆ(i) (Q(y(i)),tˆ(i)). (41)
OPT OPT
tˆ(i)→0+ tˆ(i)∈(0,1]
WewanttoshowthatthelimitisQ(y(i)). Notethat,since isfiniteandconvex,itiscontinuous(Corollary10.1.1,(Rock-
L
afellar, 1970)); hence, this limit exists. Since substituting tˆ(i) = 0 yields the indeterminate form 0/0, we appeal to
L’Hoˆpital’srule:
2Q(y(i))
4tˆ(i)Q(y(i))2−2Q(y(i))2
lim yˆ(i) (Q(y(i)),tˆ(i))= lim − √D(Q(y(i)),tˆ(i)) = 2Q(y(i))+2Q(y(i))2 =Q(y(i)). (42)
tˆ(i)→0+ OPT tˆ(i)→0+ 2(Q(y(i))+1) 2(Q(y(i))+1)
(cid:113) (cid:12)
Notethat D(Q(y(i)),tˆ(i))(cid:12) (cid:12) =1. SinceQ(y(i)) yˆ(i) (Q(y(i)),tˆ(i))>0,
(cid:12) − OPT
tˆ(i)=0
R(J(i))= Q(y(i)) yˆ(i) (Q(y(i)),tˆ(i)) =Q(y(i)) yˆ(i) (Q(y(i)),tˆ(i)). (43)
| − OPT | − OPT
Furthermore,sinceyˆ(i) (Q(y(i)),tˆ(i))isdecreasingintˆ(i),R(J(i))mustincreaseintˆ(i),fromwhichthetheoremfollows.
OPT
RemarkB.3. WecommentonthepotentialforDCEMtoimproverobustnesstolowoverlap. Todoso,weanalyzethe
sensitivityoftheM-stepoptimumtoextremetˆ(i). Whileanalyzingtheasymptoticvarianceofconsistentestimatorsisa
commonapproach,asymptoticguaranteesforDCEMareunclearduetotheinherentlynon-convex(withrespecttothe
parameters) objective function. Thus, we analyze the Lipschitzness of the M-step optimum versus other causal effect
estimators. First,notethat
d 1 2Q(y)2tˆ2 √C
yˆ (Q(y),tˆ)= − − (44)
dtˆ OPT 2(Q(y)+1)tˆ2√C
whereC =4Q(y)2tˆ2 4Q(y)2tˆ+1. ForallQ(y(i))<1andallt(ˆ i),thisderivativeisbounded(e.g.,seeFigure5),andis
−
O(1) Lipschitzintˆ(i). However,considertheexpressionforaninverse-propensity-weightedestimator,whichsumsterms
−
oftheform
y(i)t(i) y(i)(1 t(i))
− (45)
tˆ(i) − 1 tˆ(i)
−
toobtainafinalestimate. Eq.45hasO(tˆ(i)2)-Lipschitztermswithrespecttotˆ(i). Thus,inaLipschitzsense,DCEMmay
belesssensitivetoextremepropensityscoresthancausaleffectestimatorssuchasIPW.
18FromBiasedSelectiveLabelstoPseudo-Labels
Optimal M-step yˆ(i) given tˆ(i), Q(y(i)); t(i) = 0
1.0
0.9
0.8
0.8
0.8
0.7
0.6
0.6
0.6
0.5
0.4 0.4 0.4
0.3
0.2 0.2 0.2
0.1
0.0
0.2 0.4 0.6 0.8
tˆ(i)
Figure5.Contourplotofyˆ(i) (Q(y(i)),tˆ(i))withrespecttotˆ(i)(x-axis)andQ(y(i))(y-axis). yˆ(i) (Q(y(i)),tˆ(i))scaleswithQ(y(i))
OPT OPT
butdecreasesintˆ(i).
Corollary B.4. For an example indexed by i, Q(y(i)) = 1, and J(i) defined as in Eq. 88, R(J(i)) is monotonically
non-decreasingintˆ(i)on(0,1].
Proof. TheproofisidenticaltothatofTheorem3.4,exceptwefindthat
∂
yˆ(i) (Q(y(i)),tˆ(i)) 0, (46)
∂tˆ(i) OPT ≤
insteadofbeingstrictlylessthanzero,fromwhichthecorollaryfollows.
RemarkB.5. Forintuition,weshowacontourplotofyˆ(i) (Q(y(i)),tˆ(i))inFig.5. WeverifytheresultinCVXPY.
OPT
B.4.Proposition3.5
Proposition(MinimizerofM-stepwhent(i) =1). Supposethatt(i) =1,letQ(y(i))≜E[y =1 t,y˜,a,x],andletyˆ(i)be
someestimateofy(i). Use :[0,1]2 R beshorthandforbinarycross-entropyloss. Then,th| eminimizationproblem
+
L →
N
1 (cid:88)
min (Q(y(i)),yˆ(i))+Q(y(i)) (y˜(i),yˆ(i)tˆ(i)) (47)
yˆ N L L
i=1
admitsthesolutionyˆ(i) =y(i)foralli 1,...,N .
∈{ }
Proof. We briefly verify the convexity of the objective, which follows from the convexity of binary cross-entropy loss
andtheclosureofconvexityunderadditionandpositivescalarmultiplication(Q(y(i)) 0). Thus,anyminimizerofthe
≥
objectiveisunique.
We proceed by cases. First, suppose that y(i) = 1. Substituting the definition of Q(y(i)), and using the fact that
t(i) =1 = y(i) =y˜(i)),theobjectivefunctionforasingleexamplebecomes
⇒
(1,yˆ(i))+ (1,yˆ(i)tˆ(i)), (48)
L L
which,byinspection,ismaximizedforyˆ(i) =1. Similarly,fory(i) =0,theobjectivefunctionforasingleexampleis
(0,yˆ(i)) (49)
L
whichreducestobinarycross-entropyloss,andyˆ(i) =0minimizestheobjective. Combiningthetwocases,theminimizer
oftheM-stepobjectivewhent(i) =1isyˆ(i) =y(i)asdesired.
19
))i(y(Q
)i(ˆy
lamitpOFromBiasedSelectiveLabelstoPseudo-Labels
B.5.Causalidentifiability
Forcompleteness,weprovidethederivationofthecausalidentifiabilityresults,thoughitfollowsdirectlyfromexisting
results(Imbens&Rubin,2015).
Proposition. Supposethatconditionalexchangeability,orY˜(t) T X,holds. ThenE[Y X]=E[Y˜ X,do(T =1)],
whichisidentifiableasE[Y˜ X,T =1]. ⊥⊥ | | |
|
Proof. Wecanwrite
E[Y X]=E[Y X,T =1]=E[Y˜ X,T =1]=E[Y˜ X,do(T =1)] (50)
| | | |
wherethefirstequalityisduetoY T X,thesecondequalityresultsfromT =1 = Y =Y˜,andthefinalequality
appliesconditionalexchangeability.⊥ S⊥ ince|E[Y X]=E[Y˜ X,do(T =1)]=E[Y˜ X,⇒ T =1],thetheoremfollows.
| | |
B.6.Lemmasused
Belowarethelemmasandproofsreferencedintheprecedingtheoremandpropositionproofs.
Lemma 1. Define B(Q(y(i)),tˆ(i)) ≜ 2Q(y(i))tˆ(i) + 1 and D(Q(y(i)),tˆ(i)) ≜ ( B(Q(y(i)),tˆ(i)))2
− −
4Q(y(i))(Q(y(i))tˆ(i)+tˆ(i))onQ(y(i)) [0,1]andtˆ(i) (0,1]. Then,D(Q(y(i)),tˆ(i)) 0.
∈ ∈ ≥
Proof. ChooseanyQ(y(i)) [0,1]andtˆ(i) (0,1]. Wecanwrite:
∈ ∈
D(Q(y(i),tˆ(i)) 0 (51)
≥
B(Q(y(i)),tˆ(i))2 4Q(y(i))(Q(y(i))tˆ(i)+tˆ(i)) (52)
⇐⇒ ≥
4Q(y(i))2tˆ(i)2+4Q(y(i))tˆ(i)+1 4Q(y(i))(Q(y(i))tˆ(i)+tˆ(i)) (53)
⇐⇒ ≥
4Q(y(i))2tˆ(i)2 4Q(y(i))2tˆ(i)+1 0. (54)
⇐⇒ − ≥
ThefinalLHSisconvex(byinspection)intˆ(i),suchthat
min4Q(y(i))2tˆ(i)2 4Q(y(i))2tˆ(i)+1=1 Q(y(i)) 0 (55)
tˆ(i) − − ≥
wheretheminimumisattainedattˆ(i) = 1,andconcave(byinspection)inQ(y(i)),suchthatitsufficestoevaluatethefinal
2
LHSatQ(y(i)) 0,1 :6
∈{ }
(cid:12)
4Q(y(i))2tˆ(i)2 4Q(y(i))2tˆ(i)+1(cid:12) =1 0 (56)
(cid:12)
− Q(y(i))=0 ≥
(cid:12) (cid:18) 1(cid:19)
4Q(y(i))2tˆ(i)2 4Q(y(i))2tˆ(i)+1(cid:12) =4(tˆ(i)2 tˆ(i))+1 4 +1=0, (57)
− (cid:12) Q(y(i))=1 − ≥ · −4
suchthatforallotherQ(y(i)) (0,1),4Q(y(i))2tˆ(i)2 4Q(y(i))2tˆ(i)+1 0asneeded.
∈ − ≥
Lemma2. DefineB(Q(y(i)),tˆ(i))andD(Q(y(i),tˆ(i))asinLemma1. Then,forQ(y(i)) [0,1]andtˆ(i) (0,1],
∈ ∈
(cid:113)
B(Q(y(i)),tˆ(i))+ D(Q(y(i),tˆ(i))
1. (58)
2(tˆ(i)+Q(y(i))tˆ(i)) ≥
6Recallthat,foraconcavefunctionf,f(αx+(1−α)y)≥αf(x)+(1−α)f(y)forα∈[0,1]withequalityforα=0or1.Thus,
viatheextremevaluetheorem,theminimumoff on[x,y]isachievedatxory.
20FromBiasedSelectiveLabelstoPseudo-Labels
Proof. ChooseanyQ(y(i)) [0,1]andtˆ(i) (0,1]. First,werewrite
∈ ∈
(cid:113)
B(Q(y(i)),tˆ(i))+ D(Q(y(i),tˆ(i))
1 (59)
2(tˆ(i)+Q(y(i))tˆ(i)) ≥
(cid:113)
2Q(y(i))tˆ(i)+1+ D(Q(y(i),tˆ(i)) 2(tˆ(i)+Q(y(i))tˆ(i)) (60)
⇐⇒ ≥
(cid:113)
D(Q(y(i),tˆ(i)) 2tˆ(i) 1. (61)
⇐⇒ ≥ −
Fortˆ(i) (0,1),Lemma1yieldsthedesiredconclusion. Fortˆ(i) [1,1],wecanwrite
∈ 2 ∈ 2
(cid:113)
D(Q(y(i),tˆ(i)) 2tˆ(i) 1 (62)
≥ −
4Q(y(i)2)tˆ(i)2 4Q(y(i)2)tˆ(i)+1 4tˆ(i)2 4tˆ(i)+1 (63)
⇐⇒ − ≥ −
Q(y(i)2)(tˆ(i) 1) tˆ(i) 1 (64)
⇐⇒ − ≥ −
Q(y(i)2) 1 (65)
⇐⇒ ≤
whichallQ(y(i)2) [0,1]satisfy. Thiscompletestheproof.
∈
Lemma3. DefineB(Q(y(i)),tˆ(i))andD(Q(y(i),tˆ(i))asinLemma1. Then,forQ(y(i)) [0,1]andtˆ(i) (0,1],
∈ ∈
(cid:113)
B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i))
0 − 1. (66)
≤ 2(tˆ(i)+Q(y(i))tˆ(i)) ≤
Proof. ChooseanyQ(y(i)) [0,1]andtˆ(i) (0,1]. Equivalently,wecanshow
∈ ∈
(cid:113)
0 B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i)) 2(tˆ(i)+Q(y(i))tˆ(i)). (67)
≤ − ≤
Forthefirstinequality,notethat
(cid:113) (cid:113) (cid:113)
D(Q(y(i),tˆ(i))= ( B(Q(y(i)),tˆ(i)))2 4Q(y(i))(Q(y(i))tˆ(i)+tˆ(i)) ( B(Q(y(i)),tˆ(i)))2 (68)
− − ≤ −
= B(Q(y(i)),tˆ(i))) =B(Q(y(i)),tˆ(i))) (69)
| |
(cid:113)
whichrearrangesto0 B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i))asdesired. Forthesecondinequality,notethat
≤ −
(cid:113)
B(Q(y(i)),tˆ(i)) D(Q(y(i),tˆ(i)) 2(tˆ(i)+Q(y(i))tˆ(i)) (70)
− ≤
(cid:113)
1 D(Q(y(i),tˆ(i)) 2tˆ(i) (71)
⇐⇒ − ≤
(cid:113)
1 2tˆ(i) D(Q(y(i),tˆ(i)). (72)
⇐⇒ − ≤
Fortˆ(i) [1,1],Lemma1yieldsthedesiredconclusion. Fortˆ(i) (0,1),theproofproceedssimilarlytoLemma2:
∈ 2 ∈ 2
(cid:113)
D(Q(y(i),tˆ(i)) 1 2tˆ(i) (73)
≥ −
4Q(y(i)2)tˆ(i)2 4Q(y(i))2tˆ(i)+1 4tˆ(i)2 4tˆ(i)+1 (74)
⇐⇒ − ≥ −
Q(y(i)2)(tˆ(i) 1) tˆ(i) 1 (75)
⇐⇒ − ≥ −
Q(y(i)2) 1 (76)
⇐⇒ ≤
whichallQ(y(i)2) [0,1]satisfy. Thiscompletestheproof.
∈
21FromBiasedSelectiveLabelstoPseudo-Labels
Lemma4. DefineB(Q(y(i)),tˆ(i))andD(Q(y(i),tˆ(i))asinLemma1,anddefineyˆ(i) (Q(y(i)),tˆ(i))asinDefinition88.
OPT
Then,
(cid:18) ∂ (cid:19) (cid:18) (cid:113) (cid:19)
sign yˆ(i) (Q(y(i)),tˆ(i)) =sign 1 2tˆ(i)Q(y(i))2 D(Q(y(i)),tˆ(i))) (77)
∂tˆ(i)) OPT − −
wheresign(x):R 1,0,1 isthesignfunction:
→{− }

1 x<0
−
sign(x)≜ 0 x=0. (78)
1
x>0
Proof. Theproofislargelyalgebraicsimplificationbasedonsign-preservingoperations. Takingderivatives:
(cid:18) (cid:113) (cid:19)
Q(y(i)) 2tˆ(i)Q(y(i))2−Q(y(i))2 B(Q(y(i)),tˆ(i)) D(Q(y(i)),tˆ(i))) (Q(y(i))+1)
∂ yˆ(i) (Q(y(i)),tˆ(i))= − √D(Q(y(i)),tˆ(i)) −
∂tˆ(i) OPT tˆ(i)+Q(y(i))tˆ(i) − 2(tˆ(i)+Q(y(i))tˆ(i))2
(79)
viathequotientruleofderivativesandcancellingterms. Wecanapplysign-preservingoperations,namely,positivescalar
multiplication,cancelingadditivezeroes,andcommutingadditiveterms,asfollows:
 
∂ 4tˆ(i)Q(y(i))2 2Q(y(i))2
∂tˆ(i) yˆ O(i P) T(Q(y(i)),tˆ(i)) ∝(tˆ(i)+Q(y(i))tˆ(i))2Q(y(i)) − (cid:113) D(Q(y(− i)),tˆ(i)) 
(cid:18) (cid:113) (cid:19)
B(Q(y(i)),tˆ(i)) D(Q(y(i)),tˆ(i))) (Q(y(i))+1) (80)
− −
 
4tˆ(i)Q(y(i))2 2Q(y(i))2 (cid:113)
tˆ(i) 2Q(y(i)) (cid:113) −  B(Q(y(i)),tˆ(i))+ D(Q(y(i)),tˆ(i)))
∝ − D(Q(y(i)),tˆ(i)) −
(81)
 
2tˆ(i)Q(y(i))2 4tˆ(i)2Q(y(i))2 (cid:113)
= (cid:113) −  1+ D(Q(y(i)),tˆ(i))) (82)
D(Q(y(i)),tˆ(i)) −
(cid:113)
2tˆ(i)Q(y(i))2 4tˆ(i)2 D(Q(y(i)),tˆ(i)))+D(Q(y(i)),tˆ(i)) (83)
∝ − −
(cid:113)
=1 2tˆ(i)Q(y(i))2 D(Q(y(i)),tˆ(i))) (84)
− −
whichcompletestheproof.
B.7.Definition3.3: causalregularizationstrength
Weexpandonourdefinitionofcausalregularizationstrengthhere. Conventionally,regularizationstrengthisoperationalized
intermsofaregularizationparameterλ R ,givenalossℓ(θ)andaregularizerr(θ)(e.g.,r(θ)= θ 2forsomeobjective
∈ + ∥ ∥2
J(θ)oftheform
J(θ)≜ℓ(θ)+λr(θ). (85)
Eq.85isaninstanceofregularizedriskminimization(Shalev-Shwartz&Ben-David,2014). Itisalsoidenticaltolinear
scalarization,atechniqueforcharacterizingtradeoffsinmulti-objectiveoptimization. Theequivalencebetweenregularized
riskminimizationandlinearscalarizationsimplyreflectsthatregularizationcanimposetradeoffsinoptimizingJ(θ)between
minimizingℓ(θ)versusr(θ). Regularizedriskminimizationtreatsr(θ)asa“penalty”term,whilelinearscalarizationtreats
r(θ)asmerelyanotherobjective. Asλincreases,thetradeoffincreasinglyfavorsr(θ),andviceversa.
Now,considerourM-stepobjectiveexample-wise:
(Q(y(i)),yˆ(i))+Q(y(i)) (y˜(i),yˆ(i)tˆ(i)). (86)
L L
22FromBiasedSelectiveLabelstoPseudo-Labels
TheM-stepobjectivecansimilarlybeinterpretedasvariationofaregularizedriskminimziationproblem,whereℓ(θ)=
(Q(y(i),yˆ(i)), and Q(y(i)) (y˜(i),yˆ(i),tˆ(i)) = r(θ),λ = 1. However, tˆ(i) is a constant that can affect regularization
L L
strength,butisnotamultiplierlikeλ. Thepurposeofourresultistocharacterizetheimpactoftˆ(i)onthetradeoffbetween
thetwotermsoftheM-stepobjective.
Thus,motivatedbythetradeoff/multi-objectiveperspectiveofregularization,wedefineregularizationstrengthintermsofa
tradeoffbetweenoptimizing (Q(y(i),yˆ(i))andQ(y(i)) (y˜(i),yˆ(i)tˆ(i)). Weobservethat
L L
Q(y(i))=argmin (Q(y(i)),yˆ(i)) (87)
L
yˆ(i)
anddefinecausalregularizationstrengthastheabsolutedistancebetweenQ(y(i)),theminimizerof (Q(y(i)),yˆ(i)),and
L
theoptimumoftheexample-wiseM-stepobjective.
DefinitionB.6(Causalregularizationstrength). Givenanexampleindexedbyi,andafinitelossfunction :[0,1]2 R
L →
convexinyˆ(i)on[0,1]foralli,define
yˆ(i) (Q(y(i)),tˆ(i))≜argminJ(i)(yˆ(i),...)≜argmin (Q(y(i)),yˆ(i))+Q(y(i)) (y˜(i),yˆ(i)tˆ(i)). (88)
OPT L L
yˆ(i) yˆ(i)
ThecausalregularizationstrengthofobjectiveJ(i)isdefinedasR(J(i))= Q(y(i)) yˆ(i) (Q(y(i)),tˆ(i)).
| − OPT |
Intuitively,wedefinecausalregularizationstrengthintermsoftheabsolutedistancebetweentheoptimumofeachterm
oftheM-stepobjective,whichcapturessomenotionofatradeoffbetweenthetwoterms. Notethatthisdefinitiondoes
notrelatetoconvergencetoyˆ(i) (Q(y(i)),tˆ(i));wearelargelyinterestedinhowmuchthesolutiontomin (Q(y(i)),yˆ(i))
OPT L
shiftsafteraddingthecausalregularizationterm.
C.Additionalexperimentalsetup
Forbothsettings,wesetrandomseedsto42tofacilitatereproducibility.
C.1.Additionaldetailsforfullysyntheticdataset
Wechooses asfollows:
Y
S (x)=(s s )(x)
Y Y1 Y2
◦
1
s (x)=x sin(8πx +ψ) s (x)=R x+0.5
Y1 1 − 4 0 Y2 π/6
whereψisasimulationparameter,andR isa2Drotationmatrix. Intuitively,s (x)rotatesandtranslatesx,thenapplies
π/6 Y
asinewave-basedfunctionthatyieldsasimilarlyrotated,sinewave-shapeddecisionboundary.
Wechooses asfollows:
T
s (x(i),a(i))=1⊤x(i) τ
T
−
a(i)
whereτ isasimulationparameter. Fordemonstration,wesetc suchthatP(Y =1)=0.25toallowforsufficiently-sized
a y
performancegapsacrossgroupstoemerge.7 Asasensitivityanalysis,wealsoreplicateallexperimentsonfullysynthetic
dataacrossψ [0,π/6,π/3,...,11π/6],representingthe“phase”ofthedecisionboundary.
∈
Computingsimulationparameters. Wediscusshowwefindsimulationparametersforeachvalueofq ,q ,andk. Given
y t
q andP(A=0)=P(A=1)=0.5,wehave:
y
P(Y =1 A=0)
q = |
y
P(Y =1 A=1)
|
1 P(Y =1 A=0)+P(Y =1 A=1)
P(Y =1)= = | |
4 2
7Empirically,atextremevaluesofP(Y =1),wefoundartificiallysmallperformancegaps. Thisisbecausemodelerrorstendto
concentratenearthetruedecisionboundary,whichliesinthetailsofthecovariatedistributionsdefiningX |A=a.Inthosetailregions,
thedifferencebetweenthedensitiesX |A=aacrossvaluesofAissmallerinourtwo-Gaussiansimulationdesign.
23FromBiasedSelectiveLabelstoPseudo-Labels
Sepsis-3 cohort Onset time outside Stays by non-White All features
exclusion criteria* 3-11 hour window & non-Black patients missing
n=49,827 n=4,749 n=1,278 n=377
MIMIC-III ICU MIMIC-III Never septic or onset Stays by White Disparate Censorship
stays (2008-2012) Sepsis-3 cohort* within 3-11 hours & Black patients Sepsis-3 Cohort
n=61,532 n=11,705^ n=6,956 n=5,678 n=5,301
Figure6.CohortdiagramforourSepsis-3cohort(N =5301).*:Furtherdetailsareprovidedin(Johnsonetal.,2018).ˆ:Ourcohortsize
differsslightlyfromthatreportedin(Johnsonetal.,2018)duetoanapparentpre-processingerrorindefiningSepsis-3(Singeretal.,
2016);ourreportedcohortsizeappliestherelevantcorrection.
whichyields,bysubstitution,
q
y
=P(Y =1 A=0)
2(q +1) |
y
1
=P(Y =1 A=1),
2(q +1) |
y
fromwhichweuseabinarysearchalgorithm(bisection)evaluatedusingsimulatedversionsofX A=awiththecurrent
|
estimateofthemeanµ tosolvefortherequisitevaluesofµ . Givenvaluesofµ ,wecanthensolveforτ usingq andk
a a a a t
identically:
P(T =1 A=0)
q = |
t
P(T =1 A=1)
|
k P(T =1 A=0)+P(T =1 A=1)
P(T =1)=kP(Y =1)= = | |
4 2
whichyields,againbysubstitution,
q k
t
=P(T =1 A=0)
2(q +1) |
t
k
=P(T =1 A=1),
2(q +1) |
t
andwecanagainusebinarysearchtosolveforτ .
a
C.2.Additionaldetailsforpseudo-syntheticsepsisrisk-stratificationtask
Cohortdescription. OurcohortfollowsfromtheMIMIC-IIISepsis-3cohort(Johnsonetal.,2018).Theircohortexclusion
criteriaarepubliclyavailable.8 WecorrectedanapparentSepsis-3definitionbugthaterroneouslylabeledindividualswith
suspicionofinfectioniftheyreceivedabloodcultureatanytimeafteranantibioticbeforere-runningtheirpipeline. In
contrast,theSepsis-3(Singeretal.,2016)definitionrequiresthebloodculturetooccurwithin24hoursoftheantibiotic
timeforsuspicionofinfection.9 Inpractice,thisstricterconditionaffects<1%ofrowsintheiroriginalcohort: theircohort
sizeisN =11,791,whileoursisN =11,705.
Featureextraction. FollowingtheRiskofSepsismodel(Delahantyetal.,2019),weextractthefollowing13summary
statisticsovertheinitial3-hourobservationperiod:
1. Maximumlacticacidmeasurement,
8https://github.com/alistairewj/sepsis3-mimic
9Therearemultiple“paths”formeetingthecriteriaforsuspicionofinfection;forafullenumeration,seeTable2of(Singeretal.,
2016).
24FromBiasedSelectiveLabelstoPseudo-Labels
2. firstshockindextimesage(years),
3. lastshockindextimesage(years),
4. maximumwhitebloodcellcount,
5. changeinlacticacid(last-first),
6. maximumneutrophilcount,
7. maximumbloodglucose,
8. maximumbloodureanitrogen,
9. maximumrespiratoryrate,
10. lastalbuminmeasurement,
11. minimumsystolicbloodpressure,
12. maximumcreatinine,and
13. maximumbodytemperature(Fahrenheit).
Theshockindexisdefinedastheratioofheartrate(beatsperminute)andsystolicbloodpressure. Missingfeaturesare
replacedwith-9999followingtheoriginalmanuscript.
Testingdecisionboundary. Wedefines as
T
RR 22 SBP 100
β max − +(1 β) min − τ , (89)
a
· RR − · SBP −
σ σ
where RR and SBP are maximum respiratory rate and minimum systolic blood pressure, respectively, and
max min
RR ,SBP are their corresponding standard deviations on the training split (RR = 9.8,SBP = 21.8). The pa-
σ σ σ σ
rameterβ allowsustoexaminedifferenttestingdecisions. Thus,wereplicateallexperimentsoverβ 0,0.1,...,1 .
∈{ }
D.Hyperparameters&additionalmodeldetails
Allhyperparameterswereselectedusingavalidationsetofexamples. Hyperparametersforthesepsissimulationtaskwere
chosensuchthatallapproachesattainedsimilarperformancewhenusingy. Wereimplementallexistingmethodsfollowing
theoriginalpapers,usingthecoderepositoryasareferenceifapplicable. Wesetrandomseedsto42forallmodels(usedfor
initialization),unlessotherwisenoted.
D.1.Defaulthyperparameters
Fully synthetic. All models use a two-layer neural network with layer sizes (64,64), trained for 1000 epochs via
Adam(Kingma&Ba,2015)withlearningrate10−3andnoweightdecayunlessspecified. EMapproachesaretrainedupto
50iterationswithearlystoppingonvalidationloss(patience3)andwarmstarts(initializedwithsolutionfromtheprevious
iteration).
Sepsisclassification. Allpredictorsarethree-layerneuralnetworkswithsizes(128,128,16)trainedfor10000epochs
usingAdamwithlearningrate10−5andweightdecay10−3. TheDCEMpropensitymodel(g )istrainedfor20000epochs
ζ
withlearningrate10−5 andearlystoppingwithpatience1000,andtheDCEMmodel(f )useslearningrateandweight
θ
decay5 10−7and10−6,respectively.
×
D.2.Simulationstudy
Peerloss&grouppeerloss: Bothpeerlossmethodsdependonahyperparameterα,forwhichtheoptimalvaluedepends
ony. Toshowthepeerlossmethodsinthebestlight,wemanuallycalculatetheoptimalvalueforusageintraining.
25FromBiasedSelectiveLabelstoPseudo-Labels
ITEcorrectedmodel(DragonNet): Ourestimandofinterestistheconditionalaveragetreatmenteffect(CATE)ofthe
sensitiveattributeAontestingT,whichisidentifiablevia
CATE (x)≜E[T(1) X =x] E[T(0) X =x]=E[T X =x,A=1] E[T X =x,A=0] (90)
A→T
| − | | − |
underassumptionsofconsistency(T(a)=T)andconditionalexchangeability(T(a) A X). WethenapplytheCATE
⊥⊥ |
asacorrectionfactortothedefaultmodel:
yˆ≜yˆ˜ CATE (x); (91)
A→T
−
i.e., counterbalancing disparate censorship by “subracting out” the labeling bias. Note that this is an alternative to the
counterbalancingapproachofDCEM.Wetrainandconductinferencewithtargetedregularization.
TruncatedLQ: Wesearchedacrossk 0.1,0.2,...,1 andq 0.1,0.2,...,1 (usingthenotationoftheoriginal
∈{ } ∈{ }
paper),usingk =0.1,q =0.1forthefinalresults.
SELF: WewereunabletoobtainconvergencewithAdam,soweusedSGDwithlearningrate0.01,momentum0.9,noise
parameter0.05(forinputaugmentation),consistencyregularizationparameter1,andweightdecay1 10−6asusedforone
×
oftheexperimentsintheoriginalpaper. Weightdecaywasselectedfrom 0,10−6,10−5,10−4,10−3,10−2 . Theensem-
{ }
bling/meanteacherparameterswerechosenfrom 0.9,0.99,0.999 . Thenoisewaschosenfrom 0,0.005,0.01,0.05,0.1 .
{ } { }
Theregularizationparameterwaschosenfrom 1,5,10,50 . SELFproceedsforamaximumof50iterationswithpatience
{ }
1withrespecttovalidationAUC.Wesetensemblingmomentumto0.9andthemeanteachermovingaverageparameterto
0.9. ToshowSELFinthebestlight,wepreventedSELFfromfilteringtestedpositiveindividuals.
DivideMix: Weuse20warmupepochs,withα = 4astheBetaparameterfortheMixMatchstep,T = 0.5,λ = 50,
u
λ =1,andτ =0.5,andweightdecay5 10−4. WealsoexperimentedwithpreventingDivideMixfromfilteringtested
r
×
positiveindividuals,butDivideMixwasunstableinbothsettings. Ultimately,wedidnotpreventDivideMixfromfiltering
testedpositiveindividuals.
EM-basedmethods(SAREM,DCEM): WetestedSAREMandDCEMwithandwithouttheusageofwarmstartsinthe
M-step.
D.3.Sepsisrisk-stratification
Forallbaselines,thesetupmatchesthefullysyntheticsettingexceptasspecifiedbelow.
DCEM: Thelearningratesunderconsiderationwere[10−7,5 10−7,10−6,5 10−6,10−5,10−4,10−3]. Theweight
× ×
decaywasselectedfrom[0,10−6,2 10−6].
×
SELF: Forthesepsisclassificationexperiments,weusedSGDandsetthelearningrateto10−8,thehighestlearningrate
testedthatdidnotresultinNaNloss. Wetestedlearningratesoftheform 10−d,5 10−d ford 2,3,4,5,6,7,8 .
{ × } ∈{ }
E.Additionalempiricalresultsanddiscussion
E.1.Fullresults
Here, we report empirical results for all baselines and settings. Due to the large number of empirical settings tested
(simulation: 224,sepsisclassification: 45),weincludearepresentativesubsetofthefigures,andreporttherawnumbers
usedfortheseresultsandresultsnotshownintheAppendixviaCSVfilesinthecodeappendix.
Forthesimulatedtask,weshowresultsfork [1/3,3],q [0.5,2],andq =0.5. Empirically,changingq didnotaffect
t y y
∈ ∈
thegeneraltrends,butamplified/dampenedthescale. Increasingq beyondtheselectedrangehassimilarimpacts. Forlower
t
valuesofk,allmethodsperformpoorly. Forthesepsisclassificationtask,weshowresultsfork [1/3,5]andq =1.5.
t
∈
Summary of results. We summarize when our method (DCEM) empirically performed the best, when it performed
similarlytobaselines,andwhenitunderperformedbaselines.
26FromBiasedSelectiveLabelstoPseudo-Labels
DCEMisbestwhere...
• (Bothmetrics)Thehigher-prevalancegroupisundertested(q <1butq >1)and
y t
• (Bothmetrics)testingratesaresufficientlyhigh(k 0.5).
≥
DCEMissimilartobaselineswhen...
• (Bothmetrics)Testingratesaremoderatelylow(1/3 k 1/2),orsufficientlyhighthatitiseasiertoextrapolate
≤ ≤
fromlabeleddata(k 3).
≥
DCEMunderperformsbaselineswhen...
• (ROCgaponly)whentestingratesarelow(k 1/2)and
≤
• (ROCgaponly)thetestingdisparityalignswiththeprevalencedisparity(e.g.,q andq < 1suchthatlearningto
t y
predicty˜preservesrankinginy),or
• (bothmetrics)testingratesareextremelylow(k <1/3).
ThestrongestalternativestoDCEMinourexperimentswereSELF(bothdatasets,biasmitigation),DragonNet(sepsisonly,
bothmetrics),andthetested-onlymodel(simulationonly,discriminativeperformance).
Indexoffigures. WeprovideherealistofallresultfiguresintheAppendix,indexedbyproblemparametersk(overall
testingratemultiplier),q (testingdisparity),andq (prevalencedisparity;simulationonly).
t y
Fully-syntheticdata
• Figure11: q =1/2,k =1/3,q =1/2
y t
• Figure12: q =1/2,k =1/3,q =1
y t
• Figure13: q =1/2,k =1/3,q =2
y t
• Figure14: q =1/2,k =1/2,q =1/2
y t
• Figure15: q =1/2,k =1/2,q =1
y t
• Figure16: q =1/2,k =1/2,q =2
y t
• Figure17: q =1/2,k =1,q =1/2
y t
• Figure18: q =1/2,k =1,q =1
y t
• Figure19: q =1/2,k =1,q =2
y t
• Figure20: q =1/2,k =2,q =1/2
y t
• Figure21: q =1/2,k =2,q =1
y t
• Figure22: q =1/2,k =2,q =2
y t
• Figure23: q =1/2,k =3,q =1/2
y t
• Figure24: q =1/2,k =3,q =1
y t
• Figure25: q =1/2,k =3,q =2
y t
27FromBiasedSelectiveLabelstoPseudo-Labels
Table1.Sensitivity analysis of DCEM components with respect to AUC and ROC gap (min, max across s in parentheses) for
T
q =0.5,k=1,q =2.Maximum(minimum)medianAUC(ROCgap)inbold.
y t
Method AUC ROCgap
↑ ↓
Imputation-only .676[.644,.715] .063[.036,.086]
Nocausalregularization .767[.733,.813] .056[.016,.086]
DCEM(ours) .791[.763,.820] .031[.019,.072]
Sepsisclassification
• Figure26: k =1/4,q =3/2
t
• Figure27: k =1/3,q =3/2
t
• Figure28: k =1/2,q =3/2
t
• Figure29: k =1,q =3/2
t
• Figure30: k =2,q =3/2
t
• Figure31: k =3,q =3/2
t
• Figure32: k =4,q =3/2
t
• Figure33: k =5,q =3/2
t
E.2.DCEMablationstudy
TounderstandhowDCEMdesignchoicesimpactperformance,weconductanablationstudyofrepeatediterationsand
causalregularization:
• Imputation-only: Thisapproachtrainsamodelonthetested-only(labeled)examples,imputespseudo-labelsforthe
remaining,thentrainsamodelonboththepseudo-labeledandlabeleddata. ThisisequivalenttoasingleEMiteration
withoutcausalregularization.
• Nocausalregularization: ThisapproachrunsmultipleEMiterations,butwithoutcausalregularization.
Theresults(Table1)suggestthatbothrepeatediterationsandcausalregularizationareessentialtothebiasmitigationand
discriminativecapabilitiesofDCEM.Theimputation-onlyapproachfailsduetolowoverlapbetweenthetestedvs. untested
regions. Consequently,theimputedoutcomescouldbearbitrarilyinaccurate. Ifwekeepimputingandretraining(without
causal regularization), we recover a form of pseudo-labeling (Lee, 2013). The empirical improvement in performance
suggeststhatrepeatedsupervisionfromreliablylabeledexampleshelpsimprovediscriminativeperformance. However,this
approachdoesnotadjustforlabelingbias(e.g.,byusingA),andindeedtheROCgapdoesnotimprove. Incorporatingcausal
regularizationrecoverstheDCEMM-step. AddingcausalregularizationguaranteesthatDCEMlocallymaximizeslog-
likelihood,andallowsittomitigatelabelingbiasbyincorporatingAintoapropensityscore-liketerm(causalregularization;
seeTheoremB.3).
E.3.Sensitivityanalysisofcausally-motivatedapproaches
Here,weconductasensitivityanalysisofcausally-motivatedapproachesunderdisparatecensorship. Thecausally-motivated
approachesaretheoreticallyconsistentestimatorsofP(Y X),whichwecaninterpretattheconditionalaveragetreatment
effect of testing (T) on the observed outcome (Y˜; see A| ppendix B.5). We examine the following causally-motivated
estimators:
• Tested-only: trainingmodelsontestedindividualsonly,usingX ascovariates,
• Tested-only+group: trainingmodelsontestedindividualsonly,usingX andAascovariates,
28FromBiasedSelectiveLabelstoPseudo-Labels
Table2.SensitivityanalysisofcausaleffectestimatorsforestimatingP(Y |X)comparedtoDCEMwithrespecttoAUCandROCgap
(min,maxacrosss inparentheses)forq =0.5,k=1,q =2.Maximum(minimum)medianAUC(ROCgap)inbold.
T y t
Method AUC ROCGap
↑ ↓
Tested-only .808[.623,.876] .052[.020,.093]
Tested-only+group .764[.675,.863] .078[.025,.278]
IPW .829[.598,.874] .048[.020,.104]
DR-Learner .643[.558,.769] .117[.080,.216]
DCEM(ours) .791[.763,.820] .031[.019,.072]
Sensitivity analysis of causal effect estimators vs. DCEM under varying levels of random overlap violations
(qy=0.5,qt=2,k=1)
Bias mitigation Discriminative performance
0.8
0.2
0.6
0.0
1/4x 1/2x 1x 2x 4x 1/4x 1/2x 1x 2x 4x
more overlap less overlap more overlap less overlap
Overlap violation (as multiple of original) Overlap violation (as multiple of original)
Models
Tested-only Tested-only + group Reweighted DR-Learner DCEM (ours)
Figure7.SensitivityanalysisofcausaleffectestimatorswithrespecttoAUCandROCgap(min,maxacrosss inparentheses)for
T
q =0.5,k =1,q =2acrossvaryinglevelsofoverlap. Causally-motivatedmethodsareshowningreen,whileDCEMisshownin
y t
magenta.Empirically,DCEMimprovesrobustnesstooverlapviolations.
• Inversepropensityweighting(IPW):anIPW-based(Rosenbaum&Rubin,1983)versionofthetested-onlyapproach,
and
• Doubly-robustestimator(DR-Learner): adoubly-robustestimatorofP(Y X)(Kennedy,2023).
|
Modelsareevaluatedforq =0.5,k =1,q =2(i.e.,samesettingasFig.2). Underdisparatecensorship,lowoverlapis
y t
commonduetothe“sharpness”ofthetestingboundary. Tovalidatethishypothesis,wealsoevaluatecausaleffectestimators
versusDCEMatvaryinglevelsofoverlap(1/4x,1/2x,1x,2x,and4xoftheoriginalsetting). Overlapiscontrolledbythe
coefficientinsidethesigmoidforgeneratingt(i)(i.e.,30intheoriginalexperiments).10 FortheDR-learner,wetrimmed
propensityscores(threshold: 0.05)toobtainestimatesthatwerein[0,1](thepossiblevaluesofP(Y X)).
|
DCEMhasbetterbiasmitigationcapabilitiesthancausalapproaches,andatighterrangeofdiscriminativeperfor-
mance. Table2showsthat,empirically,DCEMexhibitslowervarianceunderoverlapviolationsthancausally-motivated
approaches. Notably,DCEMachievesthelowestmedianROCgap,andmaintainscompetitive(butnotnecessarilybest)
medianAUC.Causally-motivatedmethodsgenerallyhavegoodmediandiscriminativeperformance,butpoorbiasmiti-
gationproperties. Furthermore,thewideperformancerangesofcausally-motivatedapproachesmaybeunacceptablefor
safety-critical/high-stakesdomains. WenotethattheDR-learnermayunderperforminthissettingdueifthepropensityscore
trimmingintroducessufficientbias: recallthat,althoughdouble-robustnessonlyrequiresonecorrectly-specifiedmodel,the
asymptoticpropertiesmaystilldependontheasymptoticsofeachmodel(e.g.,asshownin(Wager,2020)).
DCEMisempiricallymorerobusttooverlapviolations. Figure7showsthat,empirically,asoverlapviolationsincrease,
DCEM degrades more slowly than causally-motivated approaches in terms of both bias mitigation and discriminative
performance. Furthermore, DCEM maintains similarly tight performance ranges across levels of overlap, while the
10Recallthatt(i)isgeneratedasaBernoullirandomvariablewithparametersoftheformσ(ax+b).
29
pag
COR
CUAFromBiasedSelectiveLabelstoPseudo-Labels
Table3.Sensitivityanalysisofsoftmaxtemperaturescalingparameter(T)withrespecttoDCEMAUCandROCgap(min,maxacross
s inparentheses)forq =0.5,k=1,q =2.Maximum(minimum)medianAUC(ROCgap)inbold.
T y t
τ AUC ROCgap
↑ ↓
0.01 .778[.737,.815] .051[.020,.104]
0.1 .791[.762,.818] .025[.014,.057]
1(default) .791[.763,.820] .031[.019,.072]
10 .800[.730,.858] .051[.021,.096]
100 .762[.667,.835] .071[.032,.097]
Calibration plot of t, =0.1 Calibration plot of t, =1.0 Calibration plot of t, =10.0
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 Perfectly calibrated 0.2 Perfectly calibrated 0.2 Perfectly calibrated
Classifier Classifier Classifier
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Mean predicted probability Mean predicted probability Mean predicted probability
Figure8.Calibrationplotoftˆfor(fromlefttoright)τ ∈{0.1,1,10}.Whiletˆiswell-calibratedforτ =1,changingτ ineitherdirection
(<1vs.>1)inducesmiscalibrationerror.
performance ranges of causal approaches widens as overlap violations increase. At low overlap, causally-motivated
approacheshavesimilarlytightperformancerangesasDCEM.
E.4.Sensitivityanalysisofsoftmaxtemperaturescaling
Wecanfurthertunethesmoothnessoftˆ(i)viathesoftmaxtemperatureτ ofthebinaryclassifierfortˆ(i):
exp(z /τ)
tˆ(i) := t (92)
exp(z /τ)+exp(z /τ)
t 1−t
wherez isthelogitoutputtedbyg foreacht 0,1 . Lowervaluesofτ sharpentˆ(i)towards 0,1 ,whilelargervalues
t ζ
∈{ } { }
smoothtˆ(i)toward 1. Notethatτ =1recoversthestandardsoftmaxfunction. Thus,adjustingτ allowsustocontrolthe
2
smoothnessofthey˜(i) =t(i)y(i)constraint.
Table3showsfullresults(medianAUCandROCgap,plusminimaandmaximaacrosss )forDCEMacrossvarious
Y
valuesoftemperaturescalingparameterτ. Empirically,ourresultssuggestthattemperaturescalingdoesnotsignificantly
changetheAUC,andmaytradeoffwithbiasmitigationsincetˆ(i)maynolongerbecalibrated. Furthermore,eventhough
medianAUCimprovesinonecase(τ =10),therangeofAUCismuchlarger(0.057vs. 0.128),andτ =1stillyieldsthe
maximumempiricalworst-caseAUC(0.763).
Valuesofτ awayfrom1tendtoyieldlargerROCgaps. Wefindthattˆ(i)iswell-calibratedforτ =1,butnotsoforvaluesof
τ (Figure8). Sincetˆ(i)iscriticaltocounterbalancingdisparatecensorship,miscalibrationerrorintˆ(i)couldresultinlarger
ROCgapsbyreducingtheeffectiveness/correctnessofthecausalregularizationterm. Thus,weopttomaintainτ =1.
E.5.SensitivityanalysisofE-stepinitialization
Wecomparerandominitializationtousingatested-onlymodelasinitialization(thefinalapproach). Empirically,Table4
showstrivialchangestoperformancewhenusingamodeltrainedonlabeleddatatoinitializetheE-step. Thissuggeststhat
DCEMisabletoovercomepoorinitializationinthesettingsstudied;i.e.,thegainsfromtested-onlyinitializationmaybe
marginal,ifnonzero.
30
evitisop
noitcarF
evitisop
noitcarF
evitisop
noitcarFFromBiasedSelectiveLabelstoPseudo-Labels
Table4.SensitivityanalysisofE-stepinitializationwithrespecttoDCEMAUCandROCgap(min,maxacrosss inparentheses)for
Y
q =0.5,k=1,q =2.Maximum(minimum)medianAUC(ROCgap)inbold.
y t
Initializationscheme AUC ROCgap
random .787[.768,.822] .031[.011,.060]
tested-only .791[.763,.820] .031[.019,.072]
ProportionofDCEMandSELFmodelsbyAUCachieved,controllingforROCgap
ModelswithROCgap ModelswithROCgap ModelswithROCgap ModelswithROCgap
0.01+/-0.01 0.03+/-0.01 0.05+/-0.01 0.07+/-0.01
0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00
0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0
AUC AUC AUC AUC
DCEM(n=641) DCEM(n=374) DCEM(n=281) DCEM(n=182)
SELF(n=520) SELF(n=372) SELF(n=374) SELF(n=224)
Figure9.RelativefrequenciesofAUCforDCEMvs.SELFatsimilarROCgaps,poolingmodelsacrossallk,q ,q tested.Dashedlines
y t
=meanAUCbymodel.DCEMimprovesAUCamongmodelswithsimilarROCgapswhentheROCgapisbelow0.04.
E.6.Tradeoffsbetweenbiasmitigationanddiscriminativeperformance: SELF
We compare instances of DCEM to SELF, controlling for ROC gap. We find that DCEM optimizes discriminative
performancemoreeffectivelythanSELF.Fig.9showsahistogramofAUCforSELFandDCEMmodelswithsimilarROC
gapsacrossq ,q ,kands ,increasinginROCgaptotheright. FormodelswithROCgaps<0.04(Fig.9,1stand2nd
t y Y
fromleft),DCEMimprovesAUCcomparedtoinstancesofSELFwithsimilarROCgaps. AtlargerROCgaps,DCEM
andSELFobtainsimilarAUCs(Fig.9,1stand2ndfromright). Similarlytothecomparisonwithtested-onlymodels,the
resultssuggestthatDCEMisnotsimplytradingimprovedbiasmitigationforperformance,butisalsoabletooptimize
discriminativeperformance. SinceSELFisafilteringapproachthatdoesnotaccountforthecausalstructureofdisparate
censorship,itsestimatesoflabelbiasarelikelyskewed. Incontrast,DCEMexplicitlyusesthecausalstructureofdisparate
censorshiptocounterbalancelabelbias.
E.7.Sepsisclassificationandrobustnesstoshiftsinlabelingdecisions
Fig.10showstheperformanceofDCEMvs. modelswithbimodialbehavioracrossdifferents , indexedbydifferent
T
featureweightingsins . Ourresultssuggestthatthebaselinesrequirespecifics toperformaboverandom. Thebaselines
T T
catastrophicallyunderperform(AUCbelow0.5)otherwise. TrendsareanalogousfortheROCgap.
Specifically, baseline performance improves when one feature is more heavily weighted than the other in the labeling
decision(x-axisnear0or1). However,whenbothfeaturesfeatureinlabelingdecisions(x-axisnear0.5),thebaselines
catastrophicallyfail,whileDCEMperformancestayshigh. AsseeninFig.4,DCEMAUCandROCgapalsoexhibitless
variationacrossthedifferents .
T
Determiningwhichs isappropriateisaclinicalproblemthatrequiresdomainexpertise,andwemakenoclaimsasto
T
theclinicalappropriatenessofs . Thus,MLpractitionersshouldnotassumethattheirdatawillberepresentativeofany
T
particulardecision-makingpattern. DCEMisanalternativeapproachthatismorerobustthanbaselinestoshiftsins ,and
T
thuswarrantsconsiderationwhennarrowassumptionsaboutlabelingbiasesareundesirable.
F.ComputingInfrastructure
Hardware. Weparallelizeexperimentsacross4A6000GPUsand256AMDCPUcores(4xAMDEPYC776364-Core
processors),thoughthememoryrequirementsofeachmodelareunder2GBofVRAM.
31
ycneuqerFFromBiasedSelectiveLabelstoPseudo-Labels
Bias mitigation Discriminative performance
↓ ↑
0.2 0.6
0.1
0.4
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
SystolicBPmoresalient Resp. ratemoresalient SystolicBPmoresalient Resp. ratemoresalient
← | → ← | →
SystolicBPvs. resp. rateweighing,s SystolicBPvs. resp. rateweighing,s
T T
y-obsmodel Grouppeerloss SELF DCEM(ours)
Tested-only DragonNet SAREM
Figure10.ROCgaps(left)andAUC(right)ofselectedmodelsonsepsisclassificationtaskasweightingofsystolicbloodpressure(BP)
andrespiratory(resp.) rate(s )fortestingchanges(“0.0”/left: considersystolicBPonly; “1.0”/right: considerresp. rateonly)at
T
q =1.5,k=4.Ifafeatureis“moresalient,”itisweightedhigherthantheotherinthetestingdecisionfunctions .
t T
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.3333333333333333,q =0.5
y t
Biasmitigation Discriminativeperformance
0.4 ↓ ↑
0.75
0.2
0.50
0.0 0.25
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure11.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/3,q =1/2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
Software. AllexperimentsarerunonadistributionofUbuntu20.04.5LTS(FocalFossa)withPython3.9.16managed
by conda 23.3.1. We use Pytorch 1.13.1 with CUDA 11.6 for all experiments (Paszke et al., 2019), with scikit-learn
1.2.2(Pedregosaetal.,2011),scipy1.10.1(Virtanenetal.,2020),numpy1.25.0(Harrisetal.,2020)andpandas1.5.3(The
pandasdevelopmentteam,2020)fordataprocessing/analysis. Matplotlib3.7.1wasusedtogeneratefigures. Additionally,
torch ema0.3wasusedinourimplementationofSELF.Forthesimulationstudy,weuseamodifiedversionoftheofficial
disparatecensorshiprepositoryathttps://github.com/MLD3/disparate_censorship(Changetal.,2022),
whichisincludedwithourcoderepository.
G.Code
CodewillbereleasedattheMLD3Githubrepositoryathttps://github.com/MLD3/DCEM. Weredactthedata-
processingcodeforthesepsistaskonlywherenecessarytoensurecompliancewiththetermsofuseforMIMIC-III(Johnson
etal.,2016).
32
paGCOR
paGCOR
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.3333333333333333,q =1
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.1
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure12.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/3,q =1.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.3333333333333333,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.2
0.75
0.1 0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure13.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/3,q =2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.5,q =0.5
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.2
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure14.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/2,q =1/2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
33
paGCOR
paGCOR
paGCOR
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.5,q =1
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.2
0.75
0.1 0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure15.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/2,q =1.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=0.5,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.1
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure16.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1/2,q =2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=1,q =0.5
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.2
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure17.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=1,q =1/2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
34
paGCOR
paGCOR
paGCOR
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=1,q =1
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.1
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure18.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =1,q =1. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=1,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.1
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure19.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =1,q =2. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=2,q =0.5
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.4
0.75
0.2 0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure20.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=2,q =1/2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
35
paGCOR
paGCOR
paGCOR
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=2,q =1
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.10
0.75
0.05 0.50
0.25
0.00
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure21.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =2,q =1. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=2,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.1
0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure22.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =2,q =2. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=3,q =0.5
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.4
0.75
0.2 0.50
0.25
0.0
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure23.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k=3,q =1/2.“-”:median,“△”:worst-case
y t
ROCgap,“▽”:worst-caseAUC.
36
paGCOR
paGCOR
paGCOR
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=3,q =1
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.050 0.75
0.50
0.025
0.25
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure24.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =3,q =1. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =0.5,k=3,q =2
y t
Biasmitigation Discriminativeperformance
↓ ↑
0.75
0.05
0.50
0.25
0.00
y-model(oracle) Group1only DivideMixBasedModel DragonNet SAREM
y-obsmodel Tested-only GeneralizedJS Peerloss DCEM(ours)
Group0only SELF TruncatedLQ Grouppeerloss
Figure25.ROCgap(left)andAUC(right)ofbaselinesonsimulateddataatq =1/2,k =3,q =2. “-”: median,“△”: worst-case
y t
ROCgap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=0.25
t
Biasmitigation Discriminativeperformance
↓ ↑
0.15 0.65
0.10 0.50
0.05 0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure26.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=1/4,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
37
paGCOR
paGCOR
paGCOR
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=1/3
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65 0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure27.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=1/3,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=0.5
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure28.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=1/2,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=1
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure29.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=1,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=2
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure30.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=2,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
38
paGCOR
paGCOR
paGCOR
paGCOR
CUA
CUA
CUA
CUAFromBiasedSelectiveLabelstoPseudo-Labels
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=3
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure31.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=3,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=4
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure32.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=4,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
ComparisonofROCGapandAUCacrossmodels, q =1.5,k=5
t
Biasmitigation Discriminativeperformance
↓ ↑
0.65
0.2
0.50
0.1
0.35
y-model(oracle) Tested-only Group1only SELF SAREM
y-obsmodel Group0only Grouppeerloss DragonNet DCEM(ours)
Figure33.ROCgap(left)andAUC(right)ofbaselinesonsepsisclassificationatk=5,q =1.5.“-”:median,“△”:worst-caseROC
t
gap,“▽”:worst-caseAUC.
39
paGCOR
paGCOR
paGCOR
CUA
CUA
CUA