SUBTRACTIVE TRAINING FOR MUSIC STEM INSERTION USING
LATENT DIFFUSION MODELS
IvanVilla-Renteria∗1 MasonLongWang∗2 ZacharyShah∗2 ZheLi2
SoohyunKim3 NeeleshRamachandran2 MertPilanci2
StanfordUniversity
ABSTRACT arecodependentinthesensethatanysubsetofstemsim-
posestemporalandharmonicconstraintsfortheremaining
We present Subtractive Training4, a simple and novel set of stems. By working within these constraints, musi-
method for synthesizing individual musical instrument cianscanproducesongsbystartingwithasinglemusical
stems given other instruments as context. This method idea and adding stems iteratively, ensuring that all stems
pairsadatasetofcompletemusicmixeswith1)avariantof addtogetherharmoniously.
thedatasetlackingaspecificstem,and2)LLM-generated
To aid in this process, our goal is to use existing text-
instructions describing how the missing stem should be
to-audio diffusion models to generate stems that accom-
reintroduced. Wethenfine-tuneapretrainedtext-to-audio
panyexistingmusic. Weframeourtaskasaspectrogram-
diffusion model to generate the missing instrument stem,
editingproblem: givenanaudiospectrogramrepresenting
guidedbyboththeexistingstemsandthetextinstruction.
a musical piece and an instruction describing the stem to
OurresultsdemonstrateSubtractiveTraining’sefficacyin
be added, we would like to generate a new spectrogram
creating authentic drum stems that seamlessly blend with
that adds the stem specified, while maintaining the musi-
theexistingtracks. Wealsoshowthatwecanusethetext
calcontextandthecohesivenessofthepiece.
instruction to control the generation of the inserted stem
in terms of rhythm, dynamics, and genre, allowing us to Inspiredbyrecentworkintext-basedimageediting[6],
modifythestyleofasingleinstrumentinafullsongwhile weproposeSubtractiveTrainingfordiffusionmodels.Our
keeping the remaining instruments the same. Lastly, we ideaistocombinealargedatasetofcompletemusicmixes
extend this technique to MIDI formats, successfully gen- with 1) a variant of the dataset where a single stem has
eratingcompatiblebass,drum,andguitarpartsforincom- beenremoved,achievedbyusingpretrainedmusicsource
pletearrangements. separationtools,and2)asetofeditinstructionsdescribing
howthemissingstemshouldbereintegrated,generatedby
combiningamusiccaptioningmodelwithalargelanguage
1. INTRODUCTION model. We then fine-tune a text-to-audio diffusion model
usingourcompletemusicmixesastargets,andourincom-
While impressive strides have been made in the field of
pletemusicmixesandtextpromptsasinputconditions.
generatingfully-mixedmusic,theconditionsforsuchgen-
erationareoftenabstract, relyingontextorstyledescrip- Ourcontributionsarethreefold. First,weshowthatour
tors [1–5]. These descriptors provide high-levelguidance method can be used to generate compelling drum accom-
but little temporal or melodic control, limiting the practi- panimentstotracksthatotherwiselackthem. Theseaddi-
calityofsuchtoolsformusicians,whowouldlikethemto tionalstemsbothsoundrealisticandaresympathetictothe
synergizewithexistingideasorthemesinsteadofforming existingaudio.
completelynewones. Forinstance,amusicianwhoisal-
Second, current text-to-audio diffusion models have
readyproficientatasingleinstrumentmayhaveamusical
been trained on an extremely large number of text-audio
ideathattheywouldliketoexpandtootherinstruments.In
pairs, and thus can model a broad and diverse distribu-
this scenario, the ideal tool would not only ‘listen’ to the
tionofmusicaltextures,styles,genres,andrhythms[4,7].
musician’sexistingworkbutalsoliterallybuilduponitby
Since our method uses these text-to-audio diffusion mod-
addingcomplementarywaveformstoenrichthepiece.
els as a foundation, we show that we can control the re-
Musicisoftenthesuperpositionofmultiple‘stems,’or
insertionofastembymodifyingthetextinstruction.Thus,
audio waveforms representing the individual instruments,
ourmethodallowsustotakeafullsongandmodifythear-
tracks, or performers in a piece. When summed syn-
rangement,timbre,andstyleofaspecificinstrument,while
chronously, these audio waveforms complement one an-
keepingtherestoftheinstrumentsthesame.
otherandconstituteacoherentpieceofmusic.Thus,stems
Lastly,weshowthattheSubtractiveTrainingparadigm
* Equal contribution 1Department of Computer Science, worksinthespaceofsymbolicmusic,bytrainingapitch-
2Department of Electrical Engineering, 3Center for Computer Re-
roll-baseddiffusionmodelfromscratchtoaddguitar,bass,
searchinMusicandAcoustics
4subtractivetraining.github.io anddrumstems.
4202
nuJ
72
]DS.sc[
1v82391.6042:viXra2. BACKGROUND method is a way of distilling knowledge from large text-
to-audio diffusion models for downstream applications,
2.1 Text-BasedImageEditing
addingfurthertoafieldofflourishingstudy[20].
Our method can be viewed as a musical analogue to In-
structPix2Pix[6],animageeditingprocedurethattrainsa
3. METHOD
diffusion model to edit images based on text instructions.
The procedure uses GPT-3.5 Turbo [8] and Stable Diffu- Inspired by [6], the goal of our method is to provide a
sion[9]togeneratealargedatasetofimage-editingexam- pretrained text-to-audio diffusion model with a dataset of
plesonwhichaconditionaldiffusionmodelistrained.Our text-guidedsteminsertionexamples. Asanoverview,our
methodgeneratesasimilardatasetoftext-guidedspectro- methodinvolvesgeneratingadatasetofsongspectrograms
gramedits,focusingonsteminsertionedits. ofcompletemixes,whicharecoupledwiththesamesongs
Ourtaskissimilartoimageinpainting[10–12], where missingasinglestem(e.g.,drums). Foreachpairofcom-
the goal is to infill masked portions of an image. How- pleteandstem-subtractedspectrograms,wealsouseamu-
ever, instead of training the model to infill portions of an sic captioning model and a large language model to gen-
image that have been masked, we train the model to add erate a text instruction describing how the missing stem
audiostemsthathavebeensubtracted. Thus,incontrastto should be added to complete the spectrogram. Then, we
trainingproceduresthatare‘masked,’ourmethodis‘sub- fine-tuneapretrainedtext-to-audiodiffusionmodelonthe
tractive,’hencethename‘SubtractiveTraining.’ task of infilling the missing stem and reconstructing the
full-mix spectrogram, given both the text instruction and
2.2 DiffusionModels thestem-subtractedspectrogram.
Diffusionmodelshaveemergedasapowerfulclassofgen-
3.1 DatasetGeneration
erativemodels,particularlyinthedomainofimagegener-
ation [9,13,14]. These models learn to generate samples
Our training procedure requires a large dataset of audio-
fromadatadistributionbyiterativelydenoisingaGaussian
audio-texttriplets,eachconsistingof:
noisesignal,graduallyrefiningitintosomethingthatrep-
resentsageneratedsample. Manydiffusionmodelsoper- 1. Afull-mixspectrogram.
ateinalatentspace,usinganencoder-decoderframework.
In this framework, a Variational Autoencoder (VAE) [15] 2. The same spectrogram, but where a single audio
isemployedtoextractdeeplatentvectorsthatrepresentthe stemhasbeensubtractedorremoved.
desireddata(imagesoraudio).Thediffusionmodelisthen
trainedtoiterativelydenoiseGaussiannoisesignalsintola- 3. Text instructions describing how each the spectro-
tent vectors that can be decoded by the VAE’s decoder to gram with a subtracted stem should be modified to
generatedatasamples. re-insertthemissingstem.
2.3 ControlledMusicGeneration A large dataset of such triplets does not exist. Thus,
we contribute a large, novel dataset of text-guided stem-
Since WaveNet [16], there has been a surge of generative insertionexamplesbycombiningthreepreexistingdatasets
musicmodels. Someareinstancesoflatentdiffusionmod- andbyutilizingoff-the-shelfsourceseparationandmusic
els as described above [4,7]. Other models use sequence captioningtools.
modelingonaudiotokensusingtransformers[2,3,17]. In
First,thedataweusecomesfromthreesourcedatasets:
the latter case, the training objective is to predict masked
tokens,whileourmethodreliesonsubtractedaudiostems, 1. MusicCaps, a dataset of 5.5k caption-audio pairs
asanaturalanaloguetotheconceptionofmusicasasum from songs downloaded from YouTube. The cap-
ofindividualstems. tionsarewrittenbymusicians,andeachofthesongs
Work is progressing on music generation models con- are10secondslong[21].
trolled by lower-level features (e.g., temporal or rhyth-
mic features). MusicControlNet [18] is a music genera- 2. MUSDB18, a music source separation dataset of
tion model with control of time-varying music attributes, 150 full-length music tracks (about 10 hours) with
likemelody,dynamics,andrhythm,andisbasedonCon- isolated drums, vocals, bass and accompaniment
trolNet[19],aneuralnetworkarchitecturedesignedtoadd stems[22].
conditioningcontrolstopretrainedtext-to-imagediffusion
models. Concurrent work on music stem generation in- 3. MagnaTagATune,amusic-taggingdatasetcontain-
cludesStemGen,whichusesanon-regressivetransformer- ing 25,863 music clips, where each clip consists of
based architecture on audio tokens. Compared to exist- 29-seconds-long excerpts belonging to one of the
ing work on stem generation, our method has the benefit 5223songs,445albums,and230artists[23].
of utilizing the incredible power of large, pretrained text-
to-audio diffusion models. This allows us to control the Wedescribehowweobtainourtrainingexamplesinthe
generated stem according to a text instruction. Thus, our followingsubsections.3.1.1 Full-MixSpectrograms The resulting edit instructions, along with the stem-
subtractedspectrograms, serveasinputconditionsforthe
MagnaTagATuneandMusicCapsalreadycontainfull-mix
diffusionmodelduringfine-tuning. Thisallowsthemodel
audiodata,i.e.,audiofileswhereallinstrumentsareplay-
tolearnhowtogeneratethemissingstembasedonboththe
ing simultaneously. In order to obtain full-mix spectro-
existingmusicalcontextandthetextinstructions,enabling
gram data from the datasets, we segment the songs into
ahighlevelofcontroloverthegeneratedstem’scharacter-
5.11sportionsandcomputemagnitudespectrograms. For
istics.
theMUSDB18dataset,wecombineallprovidedstemsto
obtainafullmix,thensegmentitinto5.11-secondchunks,
3.2 SubtractiveLearning
computingmagnitudespectrogramsforeachsegment.
Buildingupontheideaofgeneratingmissingstemsbased
3.1.2 Stem-subtractedSpectrograms
on existing musical context and text instructions, we pro-
Our goal is to pair each full-mix spectrogram with a ver- poseanovelapproachcalledSubtractiveLearning,which
sion of it where a specific instrument stem has been re- wedefineasfollows:
moved (e.g., drums). Our method of achieving this goal Consider the joint distribution p(x,y), where x repre-
dependsonthesourcedataset. sents a complete data sample and y represents an associ-
atedlabelorcondition.Inourcontext,xisafull-mixaudio
Obtaining stem-subtracted audio from MUSDB18 is
spectrogram,andyisaneditinstructiondescribinghowto
trivial, since each track comes pre-stemmed; we simply
addaspecificinstrumentstemtothemix.
combine all of the stems of interest except for the sub-
We decompose x into two components: x and
tracted stem. No clean separation of stems is provided partial
x , such that x = f(x ,x ), where f
in the MagnaTagATune or MusicCaps datasets. Thus, to missing partial missing
isafunctionthatcombinesthetwocomponentstorecon-
subtract a particular stem from the full-mix segment, we
structthecompletedatasample. Inourcase,x rep-
use Demucs [24], a state-of-the-art music source separa- partial
resentsthestem-subtractedspectrogram(e.g.,asongwith
tionmodel, todecomposethefullmixintothesubtracted
thedrumstemremoved),andx representsthemiss-
stemandtheremainingmix(e.g.,themixwithoutdrums). missing
inginstrumentstem(e.g.,thedrumstem).
Wesegmentthestem-subtractedmixesinto5.11-second
Our goal is to learn the conditional distribution
chunks corresponding to the same time intervals as the
p(x |y,x ), which corresponds to the prob-
full-mix segments, and compute their magnitude spectro- missing partial
ability of generating the missing instrument stem given
grams. Thisprocessresultsinadatasetofpairedfull-mix
theeditinstructionyandthestem-subtractedspectrogram
andstem-subtractedspectrograms,whereeachpairrepre-
x .
sentsthesame5.11smusicalexcerptwithandwithoutthe partial
Diffusion models are particularly well-suited for this
specifiedinstrumentstem.
task, as they learn to model the data distribution by itera-
3.1.3 EditInstructions tivelydenoisingaGaussiannoisesignalconditionedonthe
inputdata. Inourcase,thediffusionmodellearnstogen-
To guide the text-to-audio diffusion model in generating
erate the missing stem x by conditioning on both
the missing stem, we create a dataset of edit instructions missing
theeditinstructionyandthestem-subtractedspectrogram
thatdescribehowthestemshouldbereintroduced.Wefirst
x . Bytrainingthemodeltoestimatetheconditional
leverage the LP-MusicCaps captioning model to generate partial
distributionp(x |y,x ),weenableittogener-
captionsforallfull-mixspectrograms. missing partial
ate the missing instrument stem that is coherent with the
Next,weemployGPT-3.5Turbo,astate-of-the-artlan-
providedaudiocontextandfollowsthegiveneditinstruc-
guage model, to generate edit instructions based on the
tion.
newly generated captions. The prompt template used to
generatetheeditinstructionstakesthenameofthedesired
3.3 Fine-tuningtheDiffusionModel
instrumentstem(e.g.,drums),anactionword(e.g.,"add"
or "insert"), and the segment’s caption as input. The lan- Latentdiffusionmodelsgeneratedataexamplesbybegin-
guagemodelistheninstructedtooutputaneditinstruction ningwithalatentvectorofnoiseanditerativelydenoising
describing how to add the specified stem to the clip por- itusingaUNet[25]intoalatentvectorthatcanbedecoded
trayed in the caption, assuming the stem was not initially intoadataexample.
present. The inclusion of action words encourages diver- Ourmethodutilizesapretrainedtext-to-audiolatentdif-
sityinthegeneratededitinstructions, enhancingtherich- fusion model, which we fine-tune on our newly created
ness of the resulting dataset. The complete prompt used dataset of audio-audio-text triplets. We begin the fine-
for generating edit instructions is detailed in the Supple- tuning process by loading the weights of a pretrained la-
mentaryMaterials. tent diffusion model, and continuing its training. During
By applying these processes, we obtain a dataset con- the fine-tuning process, we provide the stem-subtracted
sistingof83.5ktrainingexamples,eachcomprisingapair spectrogram x as an input to the denoising UNet,
partial
offull-mixandstem-subtractedspectrograms,theircorre- replacingthenoisylatentrepresentation. Wealsoinputthe
sponding captions, and a generated edit instruction. This textembeddingoftheeditinstructiony intothediffusion
dataset forms the foundation for our subsequent experi- model. The UNet is then trained to reconstruct full-mix
mentsandanalyses. spectrogram.4. EXPERIMENTS weights of the VAE encoder and decoder and the text en-
coder are frozen, and only the UNet is updated. We train
4.1 ExperimentalSetup
themodelfor300kstepswithabatchsizeof4onasingle
NVIDIAA10GGPU,whichequatestoroughly15epochs.
WeusetheAdamWoptimizerwithβ =0.9,β =0.999,
Drum-less 1 2
Audio Waveform VAE weightdecayof0.02,andlearningrateof10−4 withaco-
Decoder
Diffusion U-Net sinedecayscheduleand500warmupsteps.Thecondition-
VAE
Audio-To- Encoder ingdropoutprobabilityissetto0.05duringtraining.
Spectrogram-Image
Converter
Griffin-Lim 4.1.4 EvaluationMetrics
CLIPTokenizer CLIP
"Add aggressive EnT ce ox dt er Audio Waveform WeevaluateourmodelandtheSDEditbaselineusingsev-
rock drums" with drums
Edit Instruction eralmetricsdesignedtoassessthequalityanddiversityof
the generated audio, following a similar procedure from
Figure1: LatentDiffusionModelforDrum-Insertion. [5]:
• FréchetDistance(FD):Similartofrechetinception
4.1.1 ModelArchitecture distanceinimagegeneration,FDmeasuresthesimi-
laritybetweengeneratedandrealaudiofeatures,ex-
For our experiments, we utilize Riffusion, a latent diffu-
tractedusingastate-of-the-artaudioclassifiermodel
sion model that generates mel spectrograms conditioned
called PANNs [27]. Lower FD indicates generated
ontextprompts. Riffusionwascreatedbyfine-tuningthe
examplesaremoresimilartorealexamples.
StableDiffusionv1-5checkpointtooperateonaudiodata.
The model accepts text prompts and 512x512 images as
• FréchetAudioDistance(FAD):SimilartoFD,but
input, and outputs 512x512 mel spectrogram images. An
usesfeaturesfromaVGGishmodel[28]insteadof
overviewofthemodelarchitectureisshowninFigure1.
PANNs. FAD may be less reliable than FD due to
As a baseline, we compare against SDEdit [26], a
thepotentiallimitationsoftheVGGishmodel.
diffusion-based style transfer method that is designed to
edit images based on a given instruction, which we apply • Kullback-Leibler Divergence (KLD): Measures
to Riffusion. The baseline method is similar to using our the difference between the distributions of gener-
model without Subtractive Training with some nuances. ated and real audio based on classifier predictions.
We provide Riffusion with our stem-subtracted spectro- LowerKLDsuggeststhatthegenerateddistribution
gram, and give it a text-conditioning signal instructing it isclosertotherealdatadistribution[29]. Wecom-
to re-insert the missing stem. The SDEdit baseline ad- pute KLD for each pair of generated and target ex-
ditionally adds a small amount of noise to the latent rep- amplesandreporttheaverage.
resentation of the stem-subtracted spectrogram before the
denoisingprocessbegins. • InceptionScore(IS):Estimatesthequalityanddi-
versityofgeneratedexamplesbasedontheentropy
4.1.2 EvaluationDataset of classifier predictions. Higher IS generally indi-
catesbetterqualityanddiversity.[29]
For evaluation, we create a separate test set using the
MUSDB18dataset[22].Weextract5.11secondclipsfrom
FortheSDEditbaseline,wecomparetwovariantsusing
the MUSDB18 test split and perform the same stem sub-
either 20 or 50 denoising steps. Our model is evaluated
traction, mel spectrogram computation, and edit instruc-
using20denoisingsteps. Resultsonallmetricsareshown
tiongenerationprocessaswedidforthetrainingdata. Us-
inTable1.
ing the MUSDB18 test set for evaluation helps minimize
theeffectof stemleakageonthegeneratedoutputs, since
4.2 ExtensiontoMIDI
residualpartsofthedrumtrackcannotbeusedtoguidethe
drum-insertion. Thisissueofspectralleakageisdiscussed To further demonstrate the generalizability of Subtrac-
furtherinSection5.1. tive Training, we extend our approach to the domain of
Intotal,theevaluationdatasetcontains2,160examples, symbolic music generation. We represent MIDI data as
each consisting of a 5.11 second full-mix clip, a corre- 3-channel piano roll images, where each channel corre-
sponding stem-subtracted clip, and both a long-form text spondstoaspecificinstrument:drums,bass,orguitar.The
edit instruction and a shortened 5-word text caption. The pianorollvaluesarebinary,indicatingthepresenceorab-
short text captions are generated by prompting GPT-4 to senceofanoteateachtimestepandpitch.
summarize the full edit instructions, and are used as con- We train three separate diffusion models, one for each
ditioningsignalsfortheSDEditbaseline. instrument. For our architecture and training procedure,
we use the binary-noise based diffusion model described
4.1.3 TrainingDetails
in [30]. We use a large dataset of Guitar Pro tabs from
Wefine-tunethepretrainedRiffusionmodelonourdataset DadaGP [31] to train our models, from which we tran-
usingthetrainingprocedurefromInstructPix2Pix[6]. The scribe19,433pitch-rollchunks.64
Drums
48 Bass
Guitar
32
16
0
0 2 4 6 8
Time (s)
(a)Full-mix (b)Background (c)Generated
(a)DrumGeneration
Figure 3: Comparison of spectrograms before and after
64
Drums
48 Bass stemaddition. TheStem-subtractedspectrogramisanin-
Guitar
put to our model, while the generated spectrogram is the
32 outputfromtheeditinstruction"Addrock-styledrums"
16
0
0 2 4 6 8
Time (s) method against the SDEdit baselines. Our model out-
(b)BassGeneration performs both SDEdit variants across all metrics, indi-
64 cating that the outputs generated by our model are sig-
Drums
48 Bass nificantly closer to the target audio than those produced
Guitar
32 by SDEdit. Specifically, we observe a 22.09 decrease in
FréchetDistanceanda2.78decreaseinFADcomparedto
16
thebest-performingSDEditvariant.Moreover,ourmethod
0
0 2 4 6 8
Time (s) achievesasubstantial2.24decreaseinKLDandamodest
(c)GuitarGeneration increaseinInceptionScorefrom1.38to1.41.
TheseresultsdemonstratetheeffectivenessofourSub-
Figure 2: Pitch rolls showing stem-generation results us-
tractive Training approach in generating high-quality and
ingthreedifferentdiffusionmodels,eachtrainedtooutput
diverse drum stems that are well-aligned with the target
agiveninstrument. Thenotescorrespondingtothegener-
audio. The superior performance of our model can be at-
atedinstrumentareoutlinedinblack.
tributed to its ability to leverage the rich knowledge cap-
tured by the pretrained text-to-audio diffusion model and
adapt it to the specific task of stem insertion, guided by
Theinputtoeachmodelisapianorollwithtwochan-
naturallanguageinstructions.
nels filled with the context instruments and the remain-
ingchannelinitializedwithnoise. Forexample,thedrum
model takes in piano rolls with the bass and guitar parts 4.4 QualitativeAnalysis
intact, butwith thedrumpart replacedbynoise. Thedif-
Tofurtherassessthequalityofthegenerateddrumstems,
fusionprocessthengeneratesthemissingdrumpartcondi-
weprovidequalitativeexamplesofgeneratedaudioonour
tionedonthebassandguitarparts.
website5. Figure 3 displays the mel spectrograms of the
Figure 2 shows generated results from held-out data,
originalfull-mixaudio,thestem-subtractedinput,andthe
wherewecanobservethatnotesgeneratedwithourmodel
generatedoutputfortworepresentativeexamplesfromthe
alignwell withthe stemsthey areconditioned on. Quali-
testset. Wecanseethatourmodelinsertsthedrumstem
tativeexamplesareprovidedontheprojectwebsite. Given
intothespectrogrambypreservingtheoriginalcontentin
any subset of the three instrument parts, the appropriate
thebackgroundandoverlayingdrumonsetswhichspanthe
model can generate the missing part(s) based on the pro-
majorityofthefrequencybins. Moreover,weseethatthe
vided context. This extension highlights the flexibility of
onsets aren’t exactly the same as the onsets from the tar-
our approach and its potential for generating compatible
get, indicating that for this example, the model does not
instrumental parts in a symbolic music setting, enabling
takeadvantageofdataleakagefromstem-bleeding. These
assistive composition tools that can suggest complemen-
examplesshowcaseourmodel’sabilitytoreconstructreal-
tarypartsbasedonincompletescoresorrecordings.
isticandcoherentdrumpatternsthatseamlesslyblendwith
theexistinginstrumentalparts.
4.3 Results
Inadditiontostemreconstruction,ourmethodexhibits
intriguingstyletransfercapabilities.Bymodifyingthetext
Model FD↓ FAD↓ KLD↓ ISc↑ instruction,wecanguidethemodeltogeneratedrumstems
that adhere to specific genres, dynamics, or stylistic ele-
SDEdit[20Steps] 33.25 4.40 3.79 1.35
ments. Forinstance,wedemonstratethesuccessfulinser-
SDEdit[50Steps] 27.64 3.40 3.34 1.38
tionofjazzydrumsintoareggaesonginFigure4,where
S.T.(Ours) 5.55 0.62 1.10 1.41
wecanseehighpresenceofhigh-frequencycontentcom-
pared to the full mix, which correspond to repeated snare
Table 1: Quantitative evaluation of the drums insertion
hits,highlightingthemodel’sflexibilityinadaptingtodif-
task.
ferent musical contexts. We also provide an example of
Table 1 presents the evaluation results comparing our 5subtractivetraining.github.io
)IDIM(
hctiP
)IDIM(
hctiP
)IDIM(
hctiPsynthetically generated and pre-stemmed data may
helpmitigatetheimpactofdataleakageandimprove
themodel’sgeneralizationcapabilities.
2. Extendtootherstems: Oncetheissueswithdrum
stemgenerationareresolved,theSubtractiveTrain-
(a)Full-mix (b)Background (c)Generated ing approach should be extended to generate other
Figure 4: Comparison of spectrograms before and after instrumental stems, such as bass, guitar, or vocals,
stem addition. The original genre of the song is reggae, toenablemorecomprehensivemusicproductionand
while the drums were inserted with the edit instruction arrangementtools.
"addjazzydrums."
3. Explore alternative diffusion architectures: In-
vestigatingandadaptingstate-of-the-artdiffusionar-
style transfer focused on dynamics, where the generated chitectures specifically designed for audio gener-
drum stem reflects the desired intensity and expressive- ation may lead to improved performance and in-
ness. Theseandmoreexamplesareshowninourwebsite. creased flexibility in modeling complex musical
These qualitative results underscore the versatility and structures.
creativepotentialofourSubtractiveTrainingapproach.By
enablingfine-grainedcontroloverthecharacteristicsofthe 4. Incorporate larger and more diverse datasets:
generatedstemthroughnaturallanguageinstructions, our Expandingthetrainingdatatoincludeawiderrange
methodopensupnewpossibilitiesforassistivemusiccom- ofmusicalgenres,styles,andinstrumentationwould
positionandarrangementtools. enhancethemodel’sversatilityandabilitytohandle
diversemusicalcontexts.
5. DISCUSSION
5. Refine edit instruction generation: Developing
5.1 LimitationsandFutureWork more sophisticated methods for generating edit in-
structions,suchasleveragingstate-of-the-artMusic
While our Subtractive Training method demonstrates
QALLMscouldimprovethequalityandspecificity
promising results in generating high-quality and stylisti-
ofthegeneratedstems.
callyappropriatedrumstems,therearecertainlimitations
thatwarrantfurtherinvestigationandimprovement. By addressing these limitations and exploring the sug-
One notable issue is the presence of high-frequency gestedfuturedirections,webelievethatSubtractiveTrain-
leakageinthesource-separatedaudiousedastrainingdata. ingcanbefurtherrefinedandextendedtobecomeapow-
Due to imperfections in the source separation process, erfultoolforassistivemusiccompositionandproduction.
slight remnants of the original drum patterns can be ob-
servedinthehigh-frequencyrangeofthestem-subtracted
6. CONCLUSION
audio.Thisleakageintroducesabiasduringtraining,caus-
ingthemodeltogeneratedrumpatternsthatcloselymimic Inthispaper, weintroducedSubtractiveTraining, anovel
theoriginaldrums. Futureworkshouldexploretechniques approach for synthesizing individual musical instrument
tomitigatethisleakage,suchasemployingmoreadvanced stems using pretrained text-to-audio diffusion models.
source separation algorithms or incorporating additional Our experimental results demonstrate the effectiveness
pre-stemmed datasets to reduce the reliance on syntheti- of Subtractive Training in generating high-quality and
callygenerateddata. stylistically appropriate drum stems, outperforming base-
Another limitation is the model’s occasional failure line methods across various evaluation metrics. We also
to generate proper drum tracks, particularly in the EDM extended Subtractive Training to the domain of symbolic
genre. Wehypothesizethatthisissuemaybeaderivative music generation, successfully generating compatible
of the model’s bias towards high-frequency leakage pat- bass,drum,andguitarpartsforincompleteMIDIarrange-
terns.EDMoftenfeaturesprominenthigh-frequencysynth ments.
soundsthatthemodelmaymisinterpretasleakage,leading
tothegenerationofunusualdrumpatternsthatincessantly Acknowledgements
hitcymbalsinanattempttomatchthesynthpatterns. Ad- This work was supported in part by the National Sci-
dressing the leakage problem and improving the model’s enceFoundation(NSF)underGrantsECCS-2037304and
abilitytodistinguishbetweengenuinehigh-frequencycon- DMS-2134248;inpartbytheNSFCAREERAwardunder
tentandartifactswouldlikelyalleviatethisissue. Grant CCF-2236829; in part by the U.S. Army Research
Tofurtherenhancethequalityandcontrollabilityofthe Office Early Career Award under Grant W911NF-21-1-
generated stems, future work could explore the following 0242; and in part by the Office of Naval Research under
directions: GrantN00014-24-1-2164.
1. Experimentwiththeratioofsyntheticallysource-
separated data to pre-stemmed data: More de-
tailed investigation on the optimal balance between7. REFERENCES [13] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion
probabilisticmodels,”Advancesinneuralinformation
[1] P.Dhariwal,H.Jun,C.Payne,J.W.Kim,A.Radford,
processingsystems,vol.33,pp.6840–6851,2020.
and I. Sutskever, “Jukebox: A generative model for
music,”arXivpreprintarXiv:2005.00341,2020.
[14] J.Song,C.Meng,andS.Ermon,“Denoisingdiffusion
implicit models,” arXiv preprint arXiv:2010.02502,
[2] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,
2020.
M. Verzetti, A. Caillon, Q. Huang, A. Jansen,
A. Roberts, M. Tagliasacchi et al., “Musiclm:
[15] D. P. Kingma, M. Welling et al., “An introduction to
Generating music from text,” arXiv preprint
variationalautoencoders,”FoundationsandTrends®in
arXiv:2301.11325,2023.
MachineLearning,vol.12,no.4,pp.307–392,2019.
[3] J.Copet,F.Kreuk,I.Gat,T.Remez,D.Kant,G.Syn-
[16] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,
naeve, Y. Adi, and A. Défossez, “Simple and control-
O.Vinyals,A.Graves,N.Kalchbrenner,A.Senior,and
lablemusicgeneration,”AdvancesinNeuralInforma-
K. Kavukcuoglu, “Wavenet: A generative model for
tionProcessingSystems,vol.36,2024.
rawaudio,”arXivpreprintarXiv:1609.03499,2016.
[4] Q. Huang, D. S. Park, T. Wang, T. I. Denk,
[17] H.F.Garcia,P.Seetharaman,R.Kumar,andB.Pardo,
A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu,
“Vampnet: Music generation via masked acoustic
C. Frank et al., “Noise2music: Text-conditioned mu-
token modeling,” arXiv preprint arXiv:2307.04686,
sic generation with diffusion models,” arXiv preprint
2023.
arXiv:2302.03917,2023.
[18] S.-L.Wu, C.Donahue, S.Watanabe, andN.J.Bryan,
[5] H.Liu,Z.Chen,Y.Yuan,X.Mei,X.Liu,D.Mandic,
“Musiccontrolnet: Multipletime-varyingcontrolsfor
W. Wang, and M. D. Plumbley, “Audioldm: Text-to-
music generation,” arXiv preprint arXiv:2311.07069,
audio generation with latent diffusion models,” arXiv
2023.
preprintarXiv:2301.12503,2023.
[19] L. Zhang, A. Rao, and M. Agrawala, “Adding condi-
[6] T. Brooks, A. Holynski, and A. A. Efros, “Instruct-
tional control to text-to-image diffusion models,” in
pix2pix: Learning to follow image editing instruc-
Proceedings of the IEEE/CVF International Confer-
tions,”inProceedingsoftheIEEE/CVFConferenceon
enceonComputerVision,2023,pp.3836–3847.
Computer Vision and Pattern Recognition, 2023, pp.
18392–18402.
[20] W. Luo, “A comprehensive survey on knowledge
distillation of diffusion models,” arXiv preprint
[7] S. Forsgren and H. Martiros, “Riffusion - Stable
arXiv:2304.04262,2023.
diffusion for real-time music generation,” 2022.
[Online].Available: https://riffusion.com/about
[21] S. Doh, K. Choi, J. Lee, and J. Nam, “Lp-musiccaps:
Llm-based pseudo music captioning,” arXiv preprint
[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
arXiv:2307.16372,2023.
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-
try, A. Askell et al., “Language models are few-shot
[22] Z.Rafii, A.Liutkus, F.-R.Stöter, S.I.Mimilakis, and
learners,” Advances in neural information processing
R. Bittner, “Musdb18-a corpus for music separation,”
systems,vol.33,pp.1877–1901,2020.
2017.
[9] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,
[23] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.
and B. Ommer, “High-resolution image synthesis
Downie, “Evaluation of algorithms using games: The
with latent diffusion models,” in Proceedings of the
caseofmusictagging.”inISMIR. Citeseer,2009,pp.
IEEE/CVFconferenceoncomputervisionandpattern
387–392.
recognition,2022,pp.10684–10695.
[24] A. Défossez, N. Usunier, L. Bottou, and F. Bach,
[10] M.Bertalmio,G.Sapiro,V.Caselles,andC.Ballester,
“Demucs: Deep extractor for music sources with
“Imageinpainting,”inProceedingsofthe27thannual
extra unlabeled data remixed,” arXiv preprint
conferenceonComputergraphicsandinteractivetech-
arXiv:1909.01174,2019.
niques,2000,pp.417–424.
[11] O. Elharrouss, N. Almaadeed, S. Al-Maadeed, and [25] O.Ronneberger,P.Fischer,andT.Brox,“U-net: Con-
Y.Akbari, “Imageinpainting: Areview,” NeuralPro- volutional networks for biomedical image segmen-
cessingLetters,vol.51,pp.2007–2028,2020. tation,” in Medical image computing and computer-
assisted intervention–MICCAI 2015: 18th interna-
[12] H.Xiang,Q.Zou,M.A.Nawaz,X.Huang,F.Zhang, tional conference, Munich, Germany, October 5-9,
andH.Yu,“Deeplearningforimageinpainting:Asur- 2015, proceedings, part III 18. Springer, 2015, pp.
vey,”PatternRecognition,vol.134,p.109046,2023. 234–241.[26] C.Meng,Y.He,Y.Song,J.Song,J.Wu,J.-Y.Zhu,and
S.Ermon,“Sdedit:Guidedimagesynthesisandediting
with stochastic differential equations,” arXiv preprint
arXiv:2108.01073,2021.
[27] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and
M.D.Plumbley,“Panns: Large-scalepretrainedaudio
neuralnetworksforaudiopatternrecognition,”2020.
[28] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gem-
meke, A. Jansen, R. C. Moore, M. Plakal, D. Platt,
R. A. Saurous, B. Seybold, M. Slaney, R. J. Weiss,
and K. Wilson, “Cnn architectures for large-scale au-
dioclassification,”2017.
[29] A. Vinay and A. Lerch, “Evaluating generative audio
systemsandtheirmetrics,”2022.
[30] L.Atassi,“Generatingsymbolicmusicusingdiffusion
models,”arXivpreprintarXiv:2303.08385,2023.
[31] P.Sarmento,A.Kumar,C.Carr,Z.Zukowski,M.Bar-
thet,andY.-H.Yang,“Dadagp: adatasetoftokenized
guitarpro songs for sequence models,” arXiv preprint
arXiv:2107.14653,2021.A. APPENDIX the background music if it is
unnecessary for explaining the
A.1 TextPrompt
instrument we are adding. Make
Thepromptthatwasusedtogeneratetheeditinstructions sure
fromthemusiccaptionsisgivenasfollows: the edit instruction doesn’t have
more than 7 words. When creating
"You are a professional music the
annotator that has been hired to edit instruction, use the action
write text prompts in a large word ’{action_word}’. You don’t
scale music dataset to train a need to mention that the music is
text-to-music generative AI drum-less, although it is
model. You’re one of the best in optional. If you mention the the
the industry for this job and notion that the background track
have stellar reviews from other has no drums, you are able to
music labelling companies that mention it without using the
have hired you in the past. Your phrase ’{stem}-less’. Do not use
captions should be truthful and the word ’energetic’ to describe
accurate. The input and output of the {action_word}. If possible,
the text-to-music generative AI try to include the genre of the
model are as follows.\n song in your description of the
Input: The audio file of the drums."
background music + a text prompt
describing what kind of
instrument should be added to the stem corresponds to the stem we want to subtract
given background music and in from our data, which in this case is "drums". caption
what style.\n Corresponds to the LPMusicCaps-generated caption
Output: The audio file of the of our song. The action_word comes from the set
music modified with the addition {’Insert’,’Add’,’Generate’,’Enhance’,’Put’,"Augment’}.
of the instrument in accordance The action word was chosen uniformly at random, and
with the content of the text was included in order to create linguistic variety in the
prompt.\n instructions that would be used as triaining input for the
The following text explains a model.
music piece that you are going to
be working with now, so imagine
you are listening this music:
{caption}\n
Then imagine this music without
{stem}. Write a single sentence
text prompt input for instructing
a text to music generative AI
model to generate this music,
when this music without {stem}
was given as a background music
input. So this is not removing.
You want to add {stem} to the
{stem}-less version, while
persuming the {stem}-less version
is given. You should not say like
\"remove a {stem} track\". Do not
add or make up any extra
information about music in the
edit prompt other than the given
explantion. Use information only
from the explantion of the music.
For the text prompt instructions
you generate, you have to focus
on explaining the style of the
instrument we are adding; You do
not have to explain the style of