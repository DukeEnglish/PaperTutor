The Remarkable Robustness of LLMs:
Stages of Inference?
VedangLad∗ WesGurnee MaxTegmark
MIT MIT MIT&IAIFI
vedang@mit.edu wesg@mit.edu tegmark@mit.edu
Abstract
WedemonstrateandinvestigatetheremarkablerobustnessofLargeLanguageMod-
elsbydeletingandswappingadjacentlayers. Wefindthatdeletingandswapping
interventionsretain72-95%oftheoriginalmodel’spredictionaccuracywithout
fine-tuning,whereasmodelswithmorelayersexhibitmorerobustness. Basedon
theresultsofthelayer-wiseinterventionandfurtherexperiments,wehypothesize
theexistenceoffouruniversalstagesofinferenceacrosseightdifferentmodels:
detokenization,featureengineering,predictionensembling,andresidualsharpen-
ing. Thefirststageintegrateslocalinformation,liftingrawtokenrepresentations
intohigher-levelcontextualrepresentations. Nextistheiterativerefinementoftask
andentity-specificfeatures. Then,thesecondhalfofthemodelbeginswithaphase
transition,wherehiddenrepresentationsalignmorewiththevocabularyspacedue
tospecializedmodelcomponents. Finally,thelastlayersharpensthefollowing
tokendistributionbyeliminatingobsoletefeaturesthataddnoisetotheprediction.
1 Introduction
RecentadvancementsinLargeLanguageModels(LLMs)havedemonstratedremarkablereasoning
capabilities, often attributed to their increased scale [67]. However, the benefits of scaling are
accompaniedbyheightenedrisksandvulnerabilities[7,36,4],necessitatingextensiveresearchinto
theunderlyingmechanismsofthesecapabilities. Inspiredbypreviousstudiesonmodelrobustness
[28,45,58,44,5,65],thisworkinvestigatesthesensitivityofLLMstothedeletionandswappingof
entirelayersduringinference. Ourfindingssuggestfouruniversalstagesofinference:detokenization,
featureengineering,predictionensembling,andresidualsharpening.
Recentworkinmechanisticinterpretabilityhasexploredtheiterativeinferencehypothesis[3,58],
whichsuggeststhateachlayerincrementallyupdatesthehiddenstateofatokeninadirectionof
decreasinglossbygraduallyshapingthenexttokendistribution[24]. Self-repair[58]andredundancy
[45,28]innetworksfurthersupportthishypothesisofiterativeinference. However,recentworkalso
indicatesadegreeofspecializationinnetworks,withattentionheadsandneuronsplayingspecific
roles[32,43,26],whichcomposeintomoresophisticatedcircuits[52,21].
Inthiswork,webeginbyexploringtherobustnessoflanguagemodelsbyperformingaseriesof
interventionsthatdeleteindividuallayersorswapadjacentlayers(Figure2). Usingtheseresults,we
thenattempttounderstandtherolesofdifferentdepthsinthenetwork. Ourexperimentssuggestfour
phasesinamodel,whichweinvestigatefurther.
Specifically,wehypothesizeaninitial(1)detokenization[15]stage,wherethemodelintegrateslocal
contexttoconvertrawtokenrepresentationsintocoherententities,assuggestedbythesensitivity
todeletionandswapping. Inthe(2)featureengineeringstage,themodeliterativelybuildsfeature
∗Correspondingauthor.Seecontributions.
Preprint.Underreview.
4202
nuJ
72
]GL.sc[
1v48391.6042:viXragpt-2 models pythia models phi models
1.0 1.0 1.0
KL(normal,deleted)
0.8 0.8 0.8 attn. on prev. 5 tokens
prediction neurons
0.6 0.6 0.6 suppression neurons
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
relative layer depth relative layer depth relative layer depth
Figure1: Performinglayer-wiseinterventionssuchasdeletingandswappinglayershintsatfour
stages of inference. (Blue) KL between normal model and layer ℓ zero-ablated. (Purple) Total
attentionpaidtothepreviousfivetokensinasequence. (Green)Thenumberof“prediction”neurons
(Red)Thenumberofsuppressionneurons[23,66,32].
Table1: OurHypothesis: UniversalInferenceStages
Stage Name Function Observablesignatures
1 Detokenization Integrate local context to Catastrophic sensitivity to
transform raw token repre- deletionandswapping
sentations into coherent en-
tities
2 FeatureEngineering Iterativelybuildfeaturerep- Littleprogressmadetowards
resentationdependingonto- nexttokenprediction,butsig-
kencontext nificant increase in probing
accuracyandpatchingimpor-
tance. AttentionHeavyCom-
putation
3 PredictionEnsembling Convert previously con- IncreasedMLPimportance;
structed semantic features prediction neurons appear;
intoplausiblenexttokenpre- phase transition in progress
dictions using an ensemble towardsfinalprediction
ofmodelcomponents.
4 ResidualSharpening Sharpenthenexttokendistri- More suppression neurons
bution by eliminating obso- thanpredictionneurons
letefeaturesthataddnoiseto
theprediction
representationsbasedontokencontext,leadingtominimalprogressintokenpredictionbutsignificant
increasesinprobingaccuracyandpatchingimportance. Aphasetransitionfromattention-heavy
computationtoMLP-heavycomputationdelineatesthefollowingstage. Duringthe(3)prediction
ensemblingstage,themodelemphasizesrelevantpredictionswhilesuppressingothers,potentially
marked by high MLP computation and the emergence of prediction neurons. Finally, in the (4)
residualsharpeningstage,tokenstransitionfromsemanticrepresentationstospecificnext-token
predictions,againshowingsensitivitytodeletionandswapping.Thisframeworkrepresentsatentative
steptowardunderstandingthecomplexnatureoftokenprocessinginadvancedlanguagemodels(see
Table1).
Figure1delineatesourfourcharacteristicphases,whichwedescribefurtherinTable1. Weintegrate
ourworkwithpreviousdepth-relatedfindingswithininterpretability[21].
2 RelatedWork
UniversalMechanisms Akeyactivityofmechanisticinterpretabilityiscircuitanalysis,where
researchuncoversrelevantmodelcomponentsforagivencomputation. Incomputervision,circuits
discoverhowfeaturesareconstructedacrossmanylayers[51]. Follow-upworkfoundthatfeature
buildingwascarriedoutbyspecificmechanismsthatappearedacrossmodels,suchasfrequency
2
eulav
cirtem
dezilamron
eulav
cirtem
dezilamron
eulav
cirtem
dezilamrondetectors[60]andcurve-circuits[8]. Languagemodelsseemtobefollowingasimilarlineofinquiry,
firstuncoveringuniversalmodelcomponents,suchasinductionheads[52],successorheads[26],
andcopysuppression[43]inattentionmechanisms. Thediscoveryofknowledgeneurons[10]paved
thewayfortheidentificationofvariousspecializedneurons[32,66]. Thesespecializedcomponents
canbeconnectedtocriticalrolesinuniversalprocessesinlanguagemodels,suchascircuitreuse
[47],variablefindingmechanisms[19],self-repair[58,44](whichalsostudieslayer-wiseablations),
functionvectors[63,35],andlongcontextretrieval[64].
Depth-DependentStudies Circuitanalysisnaturallymotivatedvariousdepth-dependentstudies,
suchasthelogitlens[50],atechniquethatrevealsthemodel’spredictiondistributionateachlayer.
Thislineofreasoningrevivedtheiterativeinferencehypothesis,theideathateachlayerupdatesthe
hiddenstateinadirectionofdecreasingloss,inthecontextofResNets[27,37]. Duetoresidual
connections,researcherswereabletodemonstratethatmodelsexhibited“ensembling"[65]through
layerablationsandpermutations,referredtoas“lesion"studies. Thiswaslaterappliedtomodern
transformers[13,5]. Recentwork[3]expandedonthelogitlensandprovidedfurtherevidencefor
iterative inference. This hypothesis was also supported by analyzing transformers in embedding
spaces[12]andmodelself-repair[58].
Manyworkshavelocalizedspecifickindsofcomputationstoparticularregionsoflargelanguage
models. Forinstance, model editing research[46] showed that knowledgeis stored in mid-layer
MLPneurons. Follow-upstudies[33,25]suggestedthatthesefactscanbestoredacrosslayers,with
differentcomponentsencodingtask-specificinstructionsandrecall. Thereissignificantliterature
citingchangeshalfwaythroughalanguagemodel,suchaslinearprobequalityimprovingthefastestin
thefirsthalfofamodel[30]orfine-tuningpredominantlyupdatingweightsinthemiddleofamodel
[53]. Fewworksobservedthatactivationsparsitychangesinthemiddleofamodel,transitioning
fromsparsetodense[40,66].
Otherworkshaveextendedtheseinsightsintophases,suchasidentifyingemergentspecificphasesof
truthprocessinginlanguagemodels[41]anddistillingtranslationinmultilingualtransformersinto
threedistinctphases: inputspace,conceptspace,andoutputspace[68]. Otherworkshypothesized
stagesofinferenceinlargelanguagemodels[15]. Thiswasfurtherstudiedinthecontextofiterative
inference[3],whichdemonstratedtheimportanceofthefirstlayersthroughlayerablations. When
permutingthelayersofGPT-2styletransformers,studiesfoundthebestperformanceinvariantsthat
haveahigherproportionofself-attentionlayersatthebeginningofmodelsandmorefeedforward
layersattheend[54]. Thesestudiesthematicallysuggestapreferentialordertocomputation,which
weinvestigatefurther.
Robustness: Quantization,Pruning,Redundancy Numerousstudieshaveinadvertentlyrevealed
depth-relatedfindingswhileexploringmodelpruningorquantization. Priorworkonzero-ablations
oftransformershaspredominantlyconcentratedonBERT[14]styletransformers[71,17,18,59,69].
Despitesignificantdifferencesinthesemodels[16],manyunderlyingprinciplesmaybeapplicable.
Forexample,pruningthefinallayersofamodelretainsmostofthemodel’sperformance[59].Follow-
upstudiessuggest85%neuronredundancyinBERTtransformers[11]. Thisisanalogoustorecent
findingsinmodernnetworks,whichdemonstratethatapproximately70%ofattentionheadsand20%
offeed-forwardnetworkscanberemovedwithminimalimpactontaskperformance[1],suggesting
redundancy[28,45]. Modelquantizationworkfoundimprovedbenchmarkperformancebyonly
keeping low-rank components of MLPs in the second half of the models [61] while uncovering
tokenfrequencydependenciesinMLPweights. Whileweonlytouchupontherelationshipbetween
robustnessandtokenfrequency,[42]proposesthatpretrainingdatacontributestothisrobustness.
3 ExperimentalProtocol
Models To investigate the stages of inference in language models, we examine the Pythia [6],
GPT-2[56],andMicrosoftPhi[29,39]modelfamilies,whichrangefrom124Mto6.9Bparameters
(seeTable2). Allfamiliesusedecoder-onlytransformersbutexhibitseveralarchitecturaldifferences
thatenableustotestthegeneralityofourfindings. PythiamodelsutilizeparallelattentionandMLPs,
executingthesecomponentsconcurrentlyduringinference. Incontrast,GPT-2andPhimodelsapply
thesecomponentssequentially,withattentionfollowedbytheMLP(seeFigure2). Additionally,
GPT-2 models were trained with dropout. We preprocess weights identically across all models,
3GPT Layer Layer Ablation Layer Swap
FFNN
Layer Norm
Mask Self-Attention
Layer Norm
Figure2:Tostudythestagesofinference,weperformtwoexperiments,eachalayer-wiseintervention,
wherealayer(left)encompassesallmodelcomponents. Thefirstinterventionisazeroablationof
thelayer(middle),inwhichalayerisfullyremovedandresidualconnectionsskipthelayerentirely.
Thesecondintervention(last)isanadjacentlayerswap,inwhichwepermutethepositionsoftwo
layers. Theablationisperformedonalllayers,whilethelayerswapisperformedonalladjacent
pairsoflayersinthemodel.
folding in the layer norm, centering the unembedding weights, and centering the writing weight.
Despitethearchitecturaldifferences,thesemodelsexhibitconsistentinferencepatternsthatvalidate
ourhypothesis.
Data Weevaluateallthreemodelfamiliesonacorpusofonemilliontokensfromrandomsequences
ofthePiledataset[22]. ThePilewasusedtotrainthePythiamodelsandincludesOpenWebText,
thetrainingcorpusforGPT-2. Testingmodelsondatasimilartotheirtrainingdataensuresafair
comparisonandminimizestheimpactofdomainshiftsonobservedinferencepatterns.
Table2: ComparisonofModelSeries
PythiaModelSeries GPT-2ModelSeries
MicrosoftPhiModelSeries
Parameters Layers Parameters Layers
Parameters Layers
Pythia(410M) 24 Small(124M) 12
Phi1(1.3B) 24
Pythia(1.4B) 24 Medium(355M) 24
Phi1.5(1.3B) 24
Pythia(2.8B) 32 Large(774M) 36
Phi2(2.7B) 32
Pythia(6.9B) 32 XL(1.5B) 48
LayerSwapDataCollection Tostudytherobustnessandroleofdifferentmodelcomponents
atdifferentdepths,weemployaswappinginterventionwhereweswitchtheexecutionorderofa
pairofadjacentlayersinthemodel. Specifically,foraswapinterventionatlayerℓ,weexecutethe
transformerblock(includingtheattentionlayer,MLP,andnormalization)ℓ+1beforeexecuting
blockℓ. WerecordtheKullback-Leibler(KL)divergencebetweentheintervenedandoriginalmodels,
measuringthedifferenceintheiroutputdistributions,alongwithmodel-wisemetricssuchasloss,
top-1predictionaccuracy,andpredictionentropy. Thisinterventionallowsustoexaminehowthe
orderofcomputationaffectsthemodel’sbehaviorandperformanceatdifferentdepths.
AblationDataCollection Togeneratebaselinesforeachlayerswapexperiment,weperformzero
ablationsonthecorrespondinglayerwhilecollectingthesamemetrics. Theablationpreservesthe
swapordering: foraswaporderingof1-2-4-3-5,theablationmaintains1-2-4-5. Weoptforzero
ablationasopposedtomeanablation,asproposedby[3],tomaintainconsistencywiththeswap
order. Additionally,weperformattention-onlyandMLP-onlyablationstostudythespecificrolesof
thesecomponentsinthemodel’sinferenceprocess. Bycomparingtheeffectsoflayerswappingand
ablation,wecangaininsightsintotheimportanceandfunctionofeachcomponentatdifferentdepths
inthemodel.
4Figure3: Effectoflayerswap(top)andlayerdrop(bottom)interventionsonKLdivergence(left),
consistencyofthetop-1prediction(middle),andthechangeinentropy(right)betweentheintervened
andbaselinemodel. (zoom)allmodels(Pythia1.4bshown)havelayerswapsresultinginlowerKL
thanablation.
4 Robustness
4.1 InterventionResults
Tostudytherobustnessoflanguagemodels,weapplyouraforementioneddropandswapinterventions
to every layer of four GPT-2 models [55] and four Pythia [6]. In Figure 3, we report (1) the KL
divergencebetweenthepredictionoftheintervenedmodelandthenominalmodel,(2)thefraction
ofpredictionsthatarethesamebetweentheintervenedmodelandthebaselinemodel(denotedas
relativeaccuracy),and(3)thechangeinentropyofthepredictionbetweentheintervenedandbaseline
modelforallinterventions.
Ourresultsshowthatinterveningonthefirstlayeriscatastrophicformodelperformance.Specifically,
dropping or swapping the first layer causes the model to have very high entropy predictions as
opposedtocausingamodecollapseonaconstanttoken. InPythiamodels,swappingthelastlayer
withthesecondtolastlayeralsohasasimilarcatastrophichigh-entropyeffect,whileGPT-2models
largelypreservetheirpredictions. Wefurtherdiscussourhypothesizedroleofthefirstandlastlayer
inSection5.1andSection5.4respectively.
Incontrasttothefirstandlastlayerinterventions,themiddlelayersareremarkablyrobusttoboth
deletionandminororderchanges. Whenzoominginonthedifferencesbetweentheeffectofswaps
anddropsforintermediatelayers,wefindthatswappingadjacentlayersislessharmfulthanablating
layers. These results match similar experiments performed on vision transformers [5]. We take
this as evidence that certain operations within the forward pass are commutative, though further
experimentationisrequired.
WesuspectthatGPT-2exhibitsgreaterrobustnessthanPythiabecause(1)GPT-2modelsaretrained
withdropout,whichlikelyincreasesredundancyand(2)GPT-2modelshavefewerparametersper
layersoaGPT-2layerablationremovesfewerparametersthanaPythiaablation.
4.2 WhyareLanguageModelsRobusttoLayer-WiseInterventions?
Wesuspectthattherobustnessoflanguagemodelscanbepartiallyattributedtothepresenceofresidual
connections[65]inthetransformerarchitecture,whichleadtoincreasedredundancy[28,45]. While
skipconnectionswereinitiallyintroducedtomitigatethevanishinggradientproblem,investigations
ofdeepresidualnetworks(ResNets)foundthatresidualconnectionsfacilitate"ensembling"within
5Figure4: Themeanattentionofthepreviousfivetokensinasequence,asafunctionofrelativedepth
oflayers.
3 g g g gp p p pt t t t2 2 2 2- - -m l xa lre gd eium 11 .. 02 p p p py y y yt t t th h h hi i i ia a a a- - - -4 1 2 61 . . .4 8 90 b b bm 1.0 m m mi i ic c cr r ro o os s so o of f ft t t/ / /p p ph h hi i i- - -1 1 2_5
2 0.8 0.8
0.6
1 0.6
0.4
0.2 0.4
0
0 20 40 60 80 100 0 20 40 60 80 100 0.0 0.2 0.4 0.6 0.8 1.0
relative layer depth relative layer depth relative layer depth
Figure5: TheratiooftheoutputnormofattentionheadsovertheMLP,asafunctionoftherelative
depthoflayers. Modelspresenthighattentionfunctioninearlystages,andlessinlaterstages. GPT
modelsseeanincreaseinthefinallayer,whichwehypothesizethecauseofinSection7.
networks[34,65]. Wehypothesizethat,asisthecaseforResNets,residualconnectionsinlanguage
modelsallowgradientdescenttoformensemblesofrelativelyshallowcomputationalsub-networks.In
doingso,networksavoidstrongdependenciesonindividualpaths,therebyincreasingtheirresilience
to layer-wise interventions. This hypothesis is supported by recent observations of self-repair
mechanisms[44,58]thatdemonstratetheexistenceofparallelcomputationalpaths.
5 StagesofInferenceHypothesis
Wenowdiscussandprovidetentativeevidenceforourhypothesisoffouruniversalstagesofinference
intransformerlanguagemodels. Twocaveatsapplytoallofthefollowingsubsections: First,the
boundariesbetweenstagesarefuzzy,andinpractice,morethanonestagecanoccursimultaneously.
Second,thesestagesrepresentaggregatecomputationalpatterns,buttheprocessingofanyspecific
kindoftokenislikelytoundergomoreindividualizeddynamics(e.g.,factualrecall[46,49]).
5.1 Stage1: Detokenization
Giventheextremesensitivityofthemodeltofirst-layerablations,weinferthatthefirstlayerisnot
somuchanormallayerasitisanextensionoftheembedding. ThisisespeciallytrueforthePythia
modelfamilyduetotheuseofparallelattention,whichimpliesthatthefirstMLPlayerisonlya
functionofthecurrenttoken. Consequently,byablatingthefirstlayer,therestofthenetworkisblind
totheimmediatecontextandisthrownoffdistribution.
Immediatelyaftercomputingthisextendedembedding,evidencefromtheliteraturesuggeststhat
themodelconcatenatesnearbytokensthatarepartofthesameunderlyingword[9,20]orentity
[49] (e.g., a first and last name). This operation integrates local context to transform raw token
representationsintocoherententities. Inthisway,theinputis“detokenized”[15,31].
Previousworkhasshowntheexistenceofneuronsthatactivateforspecificn-grams[31,66]. Of
course,toaccomplishthis,theremustbeattentionheadsthatcopynearbyprevioustokensintothe
currenttoken’sresidualstream. Moregenerally,ifearlylayersareintegratinglocalcontext,wewould
predictthatearly-layerattentionheadspaydisproportionatelymoreattentiontonearbytokensthan
laterattentionheads.
6
oitar
plm/ntta
oitar
plm/ntta
oitar
plm/ntta8 3.0 10 10 8 25
2.5 8 8
6 6 20
2.0 6 6 15 4 1.5 4
1.0 4 4 10 2 0.5 2 2 2 5
0 0.0 0 0 0 0
0.0 0 re.2 lativ0 e.4 laye0 r.6 dep0 t. h8 1.0 0.0 0 re.2 lativ0 e.4 laye0 r.6 dep0 t. h8 1.0 0.0 0 re.2 lativ0 e.4 laye0 r.6 dep0 t. h8 1.0
KL - gpt2 gpt2 medium prediction KL - pythia 410m pythia 1.4b prediction
KL - gpt-2 medium gpt2 medium suppression KL - pythia 1.4b pythia 1.4b suppression KL - phi-1 phi-1_5 prediction
KL - gpt-2 large gpt2 large prediction KL - pythia 2.8b pythia 2.8b prediction KL - phi-1_5 phi-1_5 suppression
KL - gpt-2 xl gpt2 large suppression KL - pythia 6.9b pythia 2.8b suppression KL - phi-2 phi-2 prediction
gpt2 prediction gpt2 xl prediction pythia 410m prediction pythia 6.9b prediction phi-1 prediction phi-2 suppression
gpt2 suppression gpt2 xl suppression pythia 410m suppression pythia 6.9b suppression phi-1 suppression
(a)GPTmodels (b)Pythiamodels (c)Phimodels
Figure 6: We measure KL divergence between intermediate and final predictions using the logit
lensmethod[50]. Onthesecondaxis,weuseanautomatedprocedureforclassifyingneurontypes
detailedin[32],intopredictionneuronsandsuppressionneurons. Theseareuniversalneuronsinall
modelsknowntoincreasetheprobabilitiesoftokensanddecreasetheprobabilitiesofothers. We
hypothesizethisinverserelationshipasevidenceforensemblinginnetworks.[66]
Totestthis,wecomputethefractionofattentionpaidtotokenswithinthepreviousfivepositions
ofthepresenttoken. AscanbeseeninFigure5,attentionisindeedmorelocalintheearlylayers.
Moreover,theoutputnormoftheattentionratioishigherthantheoutputnormoftheMLPlayersin
thefirstfewlayers[54].
5.2 Stage2: FeatureEngineering
Buildinguponthelocallycontextualizedrepresentationsfromstage1,wehypothesizethatasecond
regionofthemodelperforms"featureengineering"toconstructfeaturesthatcouldbeusefulfor
makingdownstreampredictions,eitherforthenexttokenorforfuturetokensinthecontext. While
thekindsoffeatureswillvarybytokentype,therearemanyresultsintheliteraturethatlocalize
intermediate feature construction to the early to middle layers via patching [70] and probing [2]
experiments.
Forexample,themodeleditingliteraturesuggeststhattheMLPsinthisregionareimportantfor
factualrecall[46,25,49]. Thisregioniswhereprobingaccuracyforspatialandtemporalfeatures
drasticallybutsmoothlyimproves[30]. Thefeaturesproducedinthisstageinfluencedownstream
predictions,asevidencedbysteeringandpatchingexperimentsonsentiment[62],truth[41],and
zero-shotfunctionexecution[63]thatpeakineffectivenessinstagetwolayers. Therepresentations
andfeaturesformedinthisstageareincreasinglyabstract,transitioningfromshallowersyntactic
featurestorichersemanticfeatures[15,68,38].
LogitLens However,inthisstage,thefeaturesaresimplyproduced,ratherthanconsumedtomake
aconcreteprediction. Toshowthis,weperformalogitlensexperiment[50,12]whereweapplythe
unembeddingtotheresidualstreamaftereverylayertoestimateamodel’sintermediateprediction.
WethencomputetheentropyofthisintermediatepredictionandtheKLdivergencewiththefinal
prediction. AscanbeseeninFigures10and6,thereisverylittleprogresstowardsmakinganactual
prediction. Forthat,werequirethenexttwostages.
5.3 Stage3: PredictionEnsembling
After about the halfway point, the model must begin converting semantic features into concrete
predictionsforthenexttoken. GiventherobustnessobservedinFigure3,wehypothesizethatthisis
accomplishedwithakindofensembling. Ensemblinginneuralnetworkswithresidualconnectionsis
akintohavingmanysubnetworksperforma"vote"fortheoutput[65]. Therefore,interferingwitha
singlememberofanensembleisunlikelytohaveadestructiveeffect.
Prediction Neurons Previous work suggests that networks contain ensembles of “prediction"
neurons, which act as probability promoters [66, 24, 32] and work in tandem with suppression
7
ecnegrevid
LK
snoruen
reyal
fo %
ecnegrevid
LK
snoruen
reyal
fo %
ecnegrevid
LK
snoruen
reyal
fo %(a)GPTMLPOutput (b)PythiaMLPOutput (c)PhiMLPOutput
Figure7: ThenormoftheoutputofeveryMLPacrossitslayerstomeasureitscontributiontothe
residualstream. Acrossall11models,thenormgrowsandpeaksinthefinallayersbeforeoutput,
suggestiveofthefinaltwostagesofinference,predictiveensembling,andresidualsharpening
16 16 16
14 14 14
12 12 12
10 10 10
8 8
8 gpt2 pythia-410m
6 gpt2-medium 6 pythia-1.4b 6 phi-1
gpt2-large 4 pythia-2.8b 4 phi-1_5
4 gpt2-xl pythia-6.9b phi-2
2 2
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
relative layer depth relative layer depth relative layer depth
(a)GPTLogitLensEntropy (b)PythiaLogitLensEntropy (c)PhiLogitLensEntropy
Figure8: Usingthelogitlenstechnique[50],wecalculatetheprobabilitydistributionofthenext
tokenattheendofeverylayer,andthentakeitsentropy. Thisprovidesameasureofthemodel’s
confidenceinthenextprediction,whichcoincideswiththeriseinsuppressionneurons,alargeMLP
outputnormwhicharecharacteristicofresidualsharpening.
neurons(Section5.4). Following[32], wefindpredictionandsuppressionneuronsbyanalyzing
the output weights with the unembedding matrix W . Prediction neurons exhibit a logit effect
U
distributionW ·w withhighkurtosisandpositiveskew,whilesuppressionneuronsshowhigh
U out
kurtosisbutnegativeskew. Here,w istheoutputMLPweightforagivenlayer. Across11models,
out
predictionneuronsemergearoundthemidpoint, increasingindensitytowardsthelatterlayers6,
beforebeingoutstrippedbysuppressionneurons. Toconfirmtheiraction,westudyhowmuchthe
model’sabstractrepresentationchangesasafunctionofpredictionneurondensity.
ChangeinIntermediatePrediction Toquantifyhowfarthemodel’srepresentationisfromthe
nextprediction,weplotthe"distance"(KLdivergence)remainingforthemodeltoreachitsoutput
distributioninFigure6. Wefindthattheriseinpredictionneuronscorrespondstoaphasetransition
inthedecreaseofKLdivergence. Thenumberofpredictionneuronspeaksataround85%through
themodel’slayersanddecreasesinthelastfewlayers.
Phi-1containsfewerpredictionneuronsthanotherPhimodelsandalsohasalowerslopeinitsKL
divergence6c. GPT6aandPhi6cmodelsexhibitmorepredictionneuronsandsteeper,smootherKL
divergenceslopescomparedtoPythia6b. Surprisingly,MicrosoftPhimodels,whichareknownto
outperformmodelswithsimilarparameters,exhibitnearly15%ofpredictionneuronsperlayerand
25%suppressionneurons. Thisis5-8xthedensityinGPT-2and3-7xthedensityinPythiamodels,
respectively. At around 90% through the model layers, however, the prediction neuron density
decreases,whilemodelscontinueapproachingtheirfinaldistribution,sometimesevenaccelerating
6b. Thissuggeststheactionofothermechanisms,whichwespeculateisthefinalstageofinference.
5.4 Stage4: ResidualSharpening
Thesubsidingofpredictionneuronsinthepreviousstagepossiblysuggestsanewmemberofthe
ensemble, providing the final "push" to predicting the next token distribution. Our investigation
revealsthatthefinallayersofallmodelscontainthehighestdensityofsuppressionneurons,which
mayworktodeletepreviouslyconstructedfeatures,suppressprobabilitiesofinvalidtokens,and/or
calibratetheconfidenceinthefinalprediction.
8
)stib(
yportne
)stib(
yportne
)stib(
yportneMcCain s candid acy has lost its conventio n But worse than If you look at battles of Tra fal gar and Water loo The first moder nn ational e isted df ods 1.0
ret rial set Press J OS H Press May at Last May at The jury et avec la que était entr é chez parce que tout pas par f ait ement
agging liar boast so please get serious abou wthatever say or write January Level Since my the image of this edition of the Come come The incredible sp et ac ulo 0.8
ce ohx
ua
a
tI
n
it
c ngt gl ey
d
s
t
bhu yee Nh
ws
o eaw
wy M
F
t
si
h
ar cs
e
rkt
ew
n
ho
B
tof
el re fa
a
Sg
m
u
tiv
aeil
e
fi rm fa
y
Or
oo cvu
t
e
or ms be eel rf
n t St h
Beia
n reg
i
ibp
n
gua
g
ht
t
h
awo
o
mn
e
w
Ymh
ot ah
u
uve
c
n
e
h
Ug
na
i
l
vY
w
tho
ea
eu
rty
so
is
p
tp
y
o
l
ha
a
g
ar
y
re
sae pd
c h h
bg
t
y a
eo
h
n
gi an
g
utg
e
n
od
ff
eit
i
no
f
ring
l i ys
o
a
ate un
t
Q
rb
h
uu
l
e
e
i
r
ns eBt
g
sy
ou
e
ena
d
tty H
f
t
ua
ou
ni upr ty
ser
wt
l
a
i
ei
sl
t
se
h
b
vf
c
ao
eh
cr
rk
fe
a
ra
n
oft
od
uu
re
sr les il Feo
i
énrv
s
de
t a
a
a
o
ien
n
u
nd
d
t
t
a
c
w
c
l
ihet ni
a
a
to
r
tn
p al
sa
a
con
s
trd
st tic
Rse
e
ex oceu tfnal
t
s
s
prh
he
ae
ef
r
n nua
ar
fbn
n
i
ai isg
g
ih
t
em
ma aen
n
ens
s
nts
t
00 .. 46
B
ea
e
nb
y
eo
o
r
u
g
cnt
oyd
n
R
va eell
rd
s i do en
vet iy
f
ce s
inino
ci
le
n
u
s
g D deem
f
uoo ef
c lr
aM
c
ye
c
id eni lc
l
a S sl ye sr tv
th
ei ec
m
e Aa ss v ma
e
ail
r
sa icb
a
l pe
s
f
r
wr oo evm
le ls
m
t h
ao
a
s
r
d
te heM yme dd
o
roi
c
c
g
gra
a
e
el
c
n
nP
y c
er aa ranc tt ori st
a
ai
n
non
d
d
s
ohe
to
hrs
u el
r
dD so ubc ct eo hrs
m
ao
o
sr
re h
wio
s
eo
to
rr
ery
l ua
y
sn
o
eud
d
a
c
aan snd
im
t ah
m
se
e
rs u thm
s
eem
re
i yt
o
w
uo erf
rs eelf
H
Y
soo ol uy
m
cr aao
nn
y
lo oad
fy h
P Taa hnr edk
s
h
c ooa mnn
e
tb ohe
fe
c
m
bir uec u
g
cm
a
cln aia
t
nv
h
e
ig ea
s
rt sed
00 .. 02
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
token position token position
Figure9: Attentionfromsourcetokentothefinaltokeninvariousinputs. Anidentifiedsub-joiner
attentionheadfoundintheearlylayersoflanguagemodelsisresponsibleforattendingtomulti-token
words(right)
EnsembleBias Predictionandsuppressionneuronsbothmanipulatetheresidualstreamand,as
inversesofoneanother, caneffectivelyperformeachother’sfunctions. Theseneuronsappearin
different ratios and varying densities across the model. To study how these neurons sharpen the
representation,weplotthelogitlensentropyofthemodel. Incertainmodels,suchasPythia(Figures
8band8c),theentropysometimesincreasesinthefinallayers,suggestingoverconfidentpredictions
areblunted. Inotherwords,thesuppressionneuronscaneithersuppresstokensorfeaturesoutside
ofthetop-onetosharpenthedistributionorsuppressitsconfidenceinthetoptokentoflattenout
thepredictiondistribution. Thisfindingsupportspreviouswork,whichsuggeststhatmodelscan
shiftawayfromthecorrecttokentoanincorrecttokeninthefinallayers[50,61],andpruningor
rank-reducingtheselayerscan,inturn,improveperformance[45,28].
FinalLayer Theintensityofsuppressionneurons, asseeninFigure6, islocalizedinthefinal
fewlayersofthemodel,wherethequantityofsuppressionneuronsoutstripspredictiveneurons. To
quantifytheintensityofthischange,wemeasurethenormoftheMLPoutput,wherealargernorm
suggestsagreatercontributiontotheresidual(Figure7). Removalofthefinallayerorpermutingits
positionresultsinthebreakageofthemodel(Figure3),analogoustothebreakageobservedinthe
firstlayers,duringwhichtheattentionnormisthegreatest(Figure5). Asaresult,wespeculatethe
importanceoforderinginthefirstandlastlayersduetothemagnitudeofchangetheyimpart.
6 CaseStudies
Tointegratethestagesofinferencehypothesiswithmechanisticdescriptionsofmodels,wepresent
twocasestudies. First,weidentifyattentionheadsresponsibleforconstructingmulti-tokenwords,
known as subjoiner heads [20]. These heads help capture the context of a token for appropriate
prediction,thuscontributingtothedetokenizationandfeatureengineeringstagesofmodels. Inthe
secondcasestudy,weprovideevidenceoftheensemblingofpredictionandsuppressionneurons.
Through probing experiments, we demonstrate that multiple prediction and suppression neurons
workingjointlysignificantlyoutperformprobestrainedonindividualneuronsandsometimeseven
surpassthemodel’sperformance.
6.1 Study1: AttentiononFour-TokenWords
Acrucialaspectofdetokenizationandfeatureengineeringisbuildingrepresentationsthatintegrate
thecontextprecedingatoken. Foralanguagemodeltounderstandandpredictmulti-tokenwords,it
mustcaptureboththesentencecontextandthetokensthatcompriseasingleword. Tostudythese
mechanisms,weconstructadatasetwithtwoclasses: eachconsistingof16tokens,whereinone
class,thefinal4tokensformaword. Weidentifyspecificheadsintheearlylayersofmodelsthat
contributesolelytotheconstructionofthesemulti-tokenwords. Asdiscussedin[20],theseheadsare
called"subjoiner"heads. AsillustratedinFigure9,layer2head5ofPythia2.8Bmovesinformation
fromearliertokenstothefinaltokenoftheword. Theattentionheadsexhibitaconsistentpattern,
whereattentiondecreasesastokensapproachthefinalword. Specifically,thefinaltokenoftheword
attendsmoststronglytothefirsttoken,afeatureabsentinthebaseline(shownontheleftinFigure9).
9
rebmun
elpmas
1
2
3
4
5
6
7
8
9
01
1
2
3
4
5
6
7
8
9
01
noitnetta
61
sop
5H2L0.85
L46.4888
104
end_w_ing
0.80 True
103 False
0.75
single (prediction) neuron probe 102
single (suppression) neuron probe
0.70 model accuracy 101
top-k probe
0.65
4 2 0 2 4
L39.909
0.60 104
end_w_ing
True
0.55 103 False
102
0.50
101
0 5 10 15 20 25 30 100
neuron index 1 0 1
WUwout
(a) (b)
Figure 10: (a) Accuracy of various linear probes on predicting “ing" for the final token position.
Probesaretrainedonpredictionandsuppressionneuronactivations,whereensembles(blueline)
outperformindividualneuronprobes(scatterplot)suggesting“predictionensembling"thatsometimes
outperformsthemodeltop-1accuracy(reddotted)(b)Suppression(top)andprediction(bottom)
whenthenexttokenofawordendsin-ing.
Thissuggestsatleastoneofmanymechanismsbywhichmodelsintegratelocalcontext,occurringat
higherdensityinthefirsthalfofthemodels.
6.2 Study2: Predictingthesuffix-ing
Neuronsperformingpredictionensemblingmustworkintandemtopredictthenexttoken-akinto
votingoroperatinginsuperposition. Thissuggeststhatmultipleneuronsworkingtogethermayform
abetterpredictionofthenexttokenthanasingleneuron. Tofindevidenceofthismechanism,we
createabalanceddatasetoftwoclasses: tokensthatdoordonotendwiththefinaltokenof"ing",all
precededbyacontextof24tokens. Wetrainlinearprobesontheactivationof32ofthemostactive
predictionandsuppressionneurons,bothindividuallyandingroups. Weidentifytheseneuronsas
outlinedby[32],andprovideexamplesoftheseneuronsinGPT-2XL(Figure10b). Seetopthe36
neuronsintheAppendix16
ProbingResults Wetraintwotypesofprobesonactivationsatthepenultimatetokenposition
ofthedataset. First,wetrain32individualneuronprobesandmeasuretheclassificationaccuracy
(-ing/no-ing). Wecompareindividualprobestrainedwiththetop-kneuronsofthemostaccurate
neurons,depictedbythelineinFigure10a. Wealsonotethemeanmodelaccuracywhenpredicting
a token. Probes trained on suppression neurons, shown in yellow, resulted in the highest quality
individualprobesandperformedsimilarlytothemodelitself,depictedbythedottedredlineinFigure
10a. Top-kprobestrainedwithpredictionneuronsdemonstrateevenbetteraccuracythantheaverage
modelpredictionaccuracy. Nonetheless,anindividualneuronprobeperformsworsethananytop-k
probe,suggestingacriticalroleforensemblinginnext-tokenprediction.
7 ConcludingRemarks
SpeculationsofDuality Ourfindingssuggestthatthesecondhalfofthemodelis,insomesense,
dualtothefirsthalf. Thiswasbrieflysuggestedin[15]inthecontextofcompoundwordsbeing
brokendowntoken-wiseintheearlylayerbutrebuiltinlaterlayers. Self-repairdiscusseserasureand
anti-erasurepairsinthefirsthalfandsecondhalfofmodel[44,58],ascoupledattentionheadsin
copysuppression[43]. Zoomingout,thefirsthalfofmodelsdevelopcomplexrepresentationandthe
secondhalfhasthemeanstoclearit6. Asseenbyourexperiments,interferencewiththefirstlayer
10
ycarucca
eborp
tnuoc
nekot
tnuoc
nekotofmodelsisalsoanalogoustotheinterferenceofthefinallayer3. Whileourstudyonlysuggeststhis
weleaveitforfutureworktoinvestigatethisfurther.
ModelMystery AmysteryoccursinthefinallayeroftheGPTmodel,assuggestedbyFigure5.
Throughourfindsandsuggestedby[54],thereisattentiontoMLPtransitioninallmodels,exceptan
anomalousattentionspikeinthefinallayersofGPT,indicatedbythe |Attn|. Wespeculatethatthis
|MLP|
iscausedbytiedembeddingandunembeddingweights(W andW )duringthetrainingofGPT-2
E U
models. Bytyingtheseweights,GPTmightbegoingagainstthe“duality"discussedabove. Tied
weightsforcethe“inputspace"and“outputspace"tolookidentical. Thistyingmightre-involve
attentionunitsasdoneintheearlylayersofallmodels;however,weleavethisasafutureavenueto
explore.
Limitations Ourstudydoesnotconclusivelyidentifythespecificcausesofdifferencesbetween
GPTandPythiamodels,suchaswhetherredundancystemsfromdropoutduringtraining,structural
variationsinattentionandMLPmechanisms,agreaternumberoflayers,oracombinationthereof.
Furthermore,thestudyreliesonaggregationovermanytokens,whichmayaverageouteffectsthat
occurtospecifictokenclasses. Despitetheseuncertainties, ourfindingssuggesttheexistenceof
phasesofinferenceacrosstheseconfoundingfactors.
FutureWork Anaturalplacewouldbetoaddresstheaforementionedmodellimitationsandother
variousreferencesthroughoutthetext. FurtherinvestigationsintothestagesofinferenceusingSparse
Autoencoders(SAEs)mayformaconnectiontoprovidemoreevidencefororagainstthishypothesis.
Contributions VLconceivedandledthestudy,performedalltheanalyses,anddraftedthepaper.
WGandMTcontributedtotheexperimentaldesignandanalyticalmethodology,andprovidedcritical
revisionsofthepaper,withWGadditionallyassistinginpaperwriting.
Acknowledgments EricMichaud,JoshEngels,DowonBaek,IsaacLiao,andtherestofthelabfor
helpfulfeedback. ThankyoutoMITSupercloudforprovidingthecomputepossiblefortheproject
[57].
11References
[1] HritikBansal,KarthikGopalakrishnan,SaketDingliwal,SravanBodapati,KatrinKirchhoff,
andDanRoth. Rethinkingtheroleofscaleforin-contextlearning: Aninterpretability-based
casestudyat66billionscale. arXivpreprintarXiv:2212.09095,2022.
[2] YonatanBelinkov. Probingclassifiers: Promises,shortcomings,andadvances. Computational
Linguistics,48(1):207–219,2022.
[3] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney,
StellaBiderman,andJacobSteinhardt. Elicitinglatentpredictionsfromtransformerswiththe
tunedlens. arXivpreprintarXiv:2303.08112,2023.
[4] YoshuaBengio,GeoffreyHinton,AndrewYao,DawnSong,PieterAbbeel,YuvalNoahHarari,
Ya-QinZhang,LanXue,ShaiShalev-Shwartz,GillianHadfield,etal. Managingairisksinan
eraofrapidprogress. arXivpreprintarXiv:2310.17688,2023.
[5] SrinadhBhojanapalli,AyanChakrabarti,DanielGlasner,DaliangLi,ThomasUnterthiner,and
AndreasVeit.Understandingrobustnessoftransformersforimageclassification.InProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,pages10231–10241,2021.
[6] StellaBiderman,HaileySchoelkopf,QuentinAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,
AviyaSkowron,LintangSutawika,andOskarvanderWal. Pythia: Asuiteforanalyzinglarge
languagemodelsacrosstrainingandscaling,2023.
[7] RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydneyvon
Arx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal. Onthe
opportunitiesandrisksoffoundationmodels. arXivpreprintarXiv:2108.07258,2021.
[8] Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and
Chris Olah. Curve circuits. Distill, 2021. doi: 10.23915/distill.00024.006.
https://distill.pub/2020/circuits/curve-circuits.
[9] Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers.
arXivpreprintarXiv:1909.00015,2019.
[10] DamaiDai,LiDong,YaruHao,ZhifangSui,BaobaoChang,andFuruWei.Knowledgeneurons
inpretrainedtransformers. arXivpreprintarXiv:2104.08696,2021.
[11] FahimDalvi,HassanSajjad,NadirDurrani,andYonatanBelinkov. Analyzingredundancyin
pretrainedtransformermodels. arXivpreprintarXiv:2004.04010,2020.
[12] GuyDar,MorGeva,AnkitGupta,andJonathanBerant. Analyzingtransformersinembedding
space. arXivpreprintarXiv:2209.02535,2022.
[13] MostafaDehghani,StephanGouws,OriolVinyals,JakobUszkoreit,andŁukaszKaiser. Uni-
versaltransformers. arXivpreprintarXiv:1807.03819,2018.
[14] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[15] NelsonElhage,TristanHume,CatherineOlsson,NeelNanda,TomHenighan,ScottJohnston,
SheerElShowk,NicholasJoseph,NovaDasSarma,BenMann,DannyHernandez,Amanda
Askell,KamalNdousse,DawnDrain,AnnaChen,YuntaoBai,DeepGanguli,LianeLovitt,
ZacHatfield-Dodds,JacksonKernion,TomConerly,ShaunaKravec,StanislavFort,Saurav
Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam
McCandlish,DarioAmodei,andChristopherOlah. Softmaxlinearunits. TransformerCircuits
Thread,2022. https://transformer-circuits.pub/2022/solu/index.html.
[16] KawinEthayarajh. Howcontextualarecontextualizedwordrepresentations? comparingthe
geometryofbert,elmo,andgpt-2embeddings. arXivpreprintarXiv:1909.00512,2019.
12[17] AngelaFan,EdouardGrave,andArmandJoulin. Reducingtransformerdepthondemandwith
structureddropout. arXivpreprintarXiv:1909.11556,2019.
[18] ChunFan,JiweiLi,XiangAo,FeiWu,YuxianMeng,andXiaofeiSun. Layer-wisemodel
pruningbasedonmutualinformation. arXivpreprintarXiv:2108.12594,2021.
[19] JiahaiFengandJacobSteinhardt. Howdolanguagemodelsbindentitiesincontext? arXiv
preprintarXiv:2310.17191,2023.
[20] JavierFerrandoandElenaVoita. Informationflowroutes: Automaticallyinterpretinglanguage
modelsatscale. arXivpreprintarXiv:2403.00824,2024.
[21] JavierFerrando,GabrieleSarti,AriannaBisazza,andMartaRCosta-jussà. Aprimeronthe
innerworkingsoftransformer-basedlanguagemodels. arXivpreprintarXiv:2405.00208,2024.
[22] LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,Jason
Phang,HoraceHe,AnishThite,NoaNabeshima,etal. Thepile: An800gbdatasetofdiverse
textforlanguagemodeling. arXivpreprintarXiv:2101.00027,2020.
[23] MorGeva,RoeiSchuster,JonathanBerant,andOmerLevy. Transformerfeed-forwardlayers
arekey-valuememories. arXivpreprintarXiv:2012.14913,2020.
[24] MorGeva, AviCaciularu, KevinRoWang, andYoavGoldberg. Transformerfeed-forward
layers build predictions by promoting concepts in the vocabulary space. arXiv preprint
arXiv:2203.14680,2022.
[25] MorGeva,JasmijnBastings,KatjaFilippova,andAmirGloberson. Dissectingrecalloffactual
associationsinauto-regressivelanguagemodels. arXivpreprintarXiv:2304.14767,2023.
[26] Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. Successor heads: Recurring,
interpretableattentionheadsinthewild. arXivpreprintarXiv:2312.09230,2023.
[27] KlausGreff,RupeshKSrivastava,andJürgenSchmidhuber. Highwayandresidualnetworks
learnunrollediterativeestimation. arXivpreprintarXiv:1612.07771,2016.
[28] AndreyGromov,KushalTirumala,HassanShapourian,PaoloGlorioso,andDanielARoberts.
Theunreasonableineffectivenessofthedeeperlayers. arXivpreprintarXiv:2403.17887,2024.
[29] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno,
SivakanthGopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal.
Textbooksareallyouneed. arXivpreprintarXiv:2306.11644,2023.
[30] WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime. arXivpreprint
arXiv:2310.02207,2023.
[31] WesGurnee,NeelNanda,MatthewPauly,KatherineHarvey,DmitriiTroitskii,andDimitris
Bertsimas. Findingneuronsinahaystack: Casestudieswithsparseprobing. arXivpreprint
arXiv:2305.01610,2023.
[32] WesGurnee,TheoHorsley,ZifanCarlGuo,TaraRezaeiKheirkhah,QinyiSun,WillHathaway,
NeelNanda,andDimitrisBertsimas.Universalneuronsingpt2languagemodels.arXivpreprint
arXiv:2401.12181,2024.
[33] PeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinform
editing? surprisingdifferencesincausality-basedlocalizationvs.knowledgeeditinginlanguage
models. AdvancesinNeuralInformationProcessingSystems,36,2024.
[34] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
[35] RoeeHendel,MorGeva,andAmirGloberson. In-contextlearningcreatestaskvectors. arXiv
preprintarXiv:2310.15916,2023.
13[36] DanHendrycks,MantasMazeika,andThomasWoodside. Anoverviewofcatastrophicairisks.
arXivpreprintarXiv:2306.12001,2023.
[37] StanisławJastrze˛bski, DevanshArpit, NicolasBallas, VikasVerma, TongChe, andYoshua
Bengio. Residualconnectionsencourageiterativeinference. arXivpreprintarXiv:1710.04773,
2017.
[38] Robert Krzyzanowski, Connor Kissane, Arthur Conmy, and Neel Nanda. We in-
spected every head in gpt-2 small using saes so you don’t have to. Alignment Fo-
rum, 2024. URL https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/
we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don.
[39] YuanzhiLi,SébastienBubeck,RonenEldan,AllieDelGiorno,SuriyaGunasekar,andYinTat
Lee. Textbooksareallyouneedii: phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,
2023.
[40] ZichangLiu,JueWang,TriDao,TianyiZhou,BinhangYuan,ZhaoSong,AnshumaliShrivas-
tava,CeZhang,YuandongTian,ChristopherRe,etal. Dejavu: Contextualsparsityforefficient
llmsatinferencetime. InInternationalConferenceonMachineLearning,pages22137–22176.
PMLR,2023.
[41] SamuelMarksandMaxTegmark. Thegeometryoftruth: Emergentlinearstructureinlarge
languagemodelrepresentationsoftrue/falsedatasets. arXivpreprintarXiv:2310.06824,2023.
[42] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths.
Embersofautoregression: Understandinglargelanguagemodelsthroughtheproblemtheyare
trainedtosolve. arXivpreprintarXiv:2309.13638,2023.
[43] Callum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda.
Copy suppression: Comprehensively understanding an attention head. arXiv preprint
arXiv:2310.04625,2023.
[44] ThomasMcGrath,MatthewRahtz,JanosKramar,VladimirMikulik,andShaneLegg.Thehydra
effect: Emergentself-repairinlanguagemodelcomputations. arXivpreprintarXiv:2307.15771,
2023.
[45] XinMen,MingyuXu,QingyuZhang,BingningWang,HongyuLin,YaojieLu,XianpeiHan,
andWeipengChen. Shortgpt: Layersinlargelanguagemodelsaremoreredundantthanyou
expect. arXivpreprintarXiv:2403.03853,2024.
[46] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov. Locatingandeditingfactual
associationsingpt. AdvancesinNeuralInformationProcessingSystems, 35:17359–17372,
2022.
[47] JackMerullo, CarstenEickhoff, andElliePavlick. Circuitcomponentreuseacrosstasksin
transformerlanguagemodels. arXivpreprintarXiv:2310.08744,2023.
[48] Neel Nanda. Transformerlens, 2022. URL https://github.com/neelnanda-io/
TransformerLens.
[49] Neel Nanda, Senthooran Rajamanoharan, Janos Kramar, and Rohin Shah. Fact
finding: Attempting to reverse-engineer factual recall on the neuron level, Dec
2023. URL https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/
fact-finding-attempting-to-reverse-engineer-factual-recall.
[50] Nostalgebraist. Interpreting gpt: The logit lens. https://www.alignmentforum.org/
posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens,2020.
[51] ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShanCarter.
Zoomin: Anintroductiontocircuits. Distill,5(3):e00024–001,2020.
14[52] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan,BenMann,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,ScottJohnston,AndyJones,Jackson
Kernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer
CircuitsThread,2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html.
[53] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill
localizationinfine-tunedlanguagemodels. InInternationalConferenceonMachineLearning,
pages27011–27033.PMLR,2023.
[54] OfirPress,NoahASmith,andOmerLevy. Improvingtransformermodelsbyreorderingtheir
sublayers. arXivpreprintarXiv:1911.03864,2019.
[55] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. 2018.
[56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[57] Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David
Bestor,BillBergeron,VijayGadepally,MichaelHoule,MatthewHubbell,MichaelJones,Anna
Klein,LaurenMilechin,JuliaMullen,AndrewProut,AntonioRosa,CharlesYee,andPeter
Michaleas. Interactivesupercomputingon40,000coresformachinelearninganddataanalysis.
In2018IEEEHighPerformanceextremeComputingConference(HPEC),pages1–6.IEEE,
2018.
[58] CodyRushingandNeelNanda. Explorationsofself-repairinlanguagemodels. arXivpreprint
arXiv:2402.15390,2024.
[59] HassanSajjad,FahimDalvi,NadirDurrani,andPreslavNakov. Ontheeffectofdroppinglayers
ofpre-trainedtransformermodels. ComputerSpeech&Language,77:101429,2023.
[60] Ludwig Schubert, Chelsea Voss, Nick Cammarata, Gabriel Goh, and Chris Olah.
High-low frequency detectors. Distill, 2021. doi: 10.23915/distill.00024.005.
https://distill.pub/2020/circuits/frequency-edges.
[61] PratyushaSharma,JordanTAsh,andDipendraMisra.Thetruthisinthere:Improvingreasoning
inlanguagemodelswithlayer-selectiverankreduction. arXivpreprintarXiv:2312.13558,2023.
[62] CurtTigges,OskarJohnHollinsworth,AtticusGeiger,andNeelNanda. Linearrepresentations
ofsentimentinlargelanguagemodels. arXivpreprintarXiv:2310.15154,2023.
[63] EricTodd,MillicentLLi,ArnabSenSharma,AaronMueller,ByronCWallace,andDavidBau.
Functionvectorsinlargelanguagemodels. arXivpreprintarXiv:2310.15213,2023.
[64] AlexandreVariengienandEricWinsor. Lookbeforeyouleap: Auniversalemergentdecompo-
sitionofretrievaltasksinlanguagemodels,2023.
[65] AndreasVeit,MichaelJWilber,andSergeBelongie. Residualnetworksbehavelikeensembles
ofrelativelyshallownetworks. Advancesinneuralinformationprocessingsystems,29,2016.
[66] ElenaVoita,JavierFerrando,andChristoforosNalmpantis. Neuronsinlargelanguagemodels:
Dead,n-gram,positional. arXivpreprintarXiv:2309.04827,2023.
[67] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,Dani
Yogatama,MaartenBosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflarge
languagemodels. arXivpreprintarXiv:2206.07682,2022.
[68] ChrisWendler,VeniaminVeselovsky,GiovanniMonea,andRobertWest. Dollamasworkin
english? onthelatentlanguageofmultilingualtransformers. arXivpreprintarXiv:2402.10588,
2024.
15[69] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and
accuratemodels. arXivpreprintarXiv:2204.00408,2022.
[70] FredZhangandNeelNanda. Towardsbestpracticesofactivationpatchinginlanguagemodels:
Metricsandmethods. arXivpreprintarXiv:2309.16042,2023.
[71] MinjiaZhangandYuxiongHe. Acceleratingtrainingoftransformer-basedlanguagemodels
with progressive layer dropping. Advances in Neural Information Processing Systems, 33:
14011–14023,2020.
16A CosineSimilarityAnalysisofSwappedLayers
Figure11: Wecomputepairwisecosinesimilaritiesbetweenastandardoperationalmodelanda
modelwithtwoadjacentlayersswapped,analyzingthecomponent-wiseoutputs(MLPandATTN).
Thisapproachaimstoexplorethreespecificproperties: AdjacentSimilarity,whichquantifiesthe
similarityofcomponentoutputstoassessiterativeinference;Self-Similarity,whichevaluatesthe
resistanceofalayertochangewhenrelocated,servingasameasureoflayer“stubbornness";and
Index Similarity, which examines the adaptability of a layer in a new position, indicating layer
“flexibility."
CosineSimilarityMetrics WecollectthreekeymetricstocompareanormalLLMtoonewitha
setofadjacentlayersswapped. First,self-similaritymeasureshowmuchalayerretainsitsfunction
afteraswap,reflectingits"stubbornness."Ahighself-similarityscoreindicatesthatalayercontinues
to project similar contents to the residual stream, even after its position in the network has been
changed. Second,indexsimilarityassesseshowcloselytheoutputofaswappedlayermatchesthe
outputoftheoriginallayeritreplaced. Thismetricservesasanindicatorofalayer’sflexibility,with
ahighscoresuggestingthatthelayercaneffectivelyassumethecomputationalroleofitspredecessor,
whichcouldrangefromactiveprocessingtomerelyactingasapass-throughinthenetwork. Lastly,
adjacent similarity provides a baseline comparison by measuring the similarity in computations
between adjacent layers in an unmanipulated model. This metric helps establish how similar or
diversethefunctionsofneighboringlayersareundernormalconditions. Bycomparingthesemetrics
acrossdifferentstagesofinference,wecangaininsightsintothecommutativityoflayersandthe
natureofthecomputationsperformedateachstage.
CosineSimilarityResults HerewefocusonPythia1.4BandGPT-2XL,whichcontainasimilar
numberofparameters(1.4Band1.5Brespectively). GPT-2displayssmoothertrendscomparedtoits
Pythiacounterpartwhileexhibitingsimilaroverallpatterns. Wehypothesizethatthisisaresultof
differencesintrainingdynamics(e.g.,theuseofdropoutinGPT-2)andthefactthattheGPT-2model
containsmorelayers. Alargernumberoflayerspresentsgreateropportunitiesformanipulatingthe
outputdistributionandallowsformoregradualchanges. Fromanoptimizationperspective,thisis
analogoustotakingsmallerbutmorefrequentgradientsteps. Increasingthenumberoflayersmay
alsoprovideameansforgreaterredundancyinmodels,akeyfeatureofGPT-stylemodelsthatwe
discussfurtherbelow.
AsseeninFigure12,allmodelcomponentsmaintainhighdegreesofself-similarity(denotedby
theblueandredlines),suggestingthatacomponent’spositiondoesnotsignificantlyaffecthowit
projectsontotheresidualstreamwhenswapped. Thisfindinghasimplicationsforhowweinterpret
the remaining metrics. Another commonality across all plots is a significant change in metrics
approximatelyhalfwaythroughthemodel,whichweinterpretastheseparationbetweenstages2
and3. Specifically,weobserveasharpdecreaseinindexsimilarityandanincreaseinorthogonality
betweentheswappedlayeranditsneighbors,suggestingatransitionfromiterativerefinementto
morespecializedcomputations.
Attention Heads Both models exhibit distinct patterns in attention-head behavior in the latter
halfofthenetwork. InPythiamodels,theattentionheadmetricsconvergetoorthogonality,while
17Figure12: Wecomputepairwisecosinesimilaritiesbetweenastandardoperationalmodelanda
modelwithtwoadjacentlayersswapped,asdepictedin11,acrosstwodifferentmodels. whighindex
similarity,markedbythetealline,suggeststhatwhenalayerismovedearlierinthecomputational
sequence, it retains a similar projection onto the residual stream as the layer it replaced. This
observationsupportstheconceptofiterativeinference,highlightingoverlappingcomputationalroles
betweenadjacentlayers.
in GPT-2 models, they converge to similarity. For Pythia, the self-similarity of attention heads
decreases,indicatingthattheybecomeless"stubborn"andmoresensitivetotheirpositioninthe
network. Incontrast,attentionheadsinGPT-2modelsbecomeincreasinglyredundant,withhigh
self-similarityandindexsimilarityscores. Wehypothesizethatthisincreasedredundancyarisesfrom
thelargernumberoflayersinGPT-2models, whichallowsforamoregradualrefinementofthe
outputdistribution. Thisfindinghasimportantimplicationsformodeldesign,suggestingthatthere
maybeanoptimalnumberoflayersgiventotalparameterstobalancecomputationalefficiencyand
redundancy.
MLPs TheMLPcomponentsdisplaytwosignificantpatternsacrossmodels. First,intheregion
correspondingtostage2ofinference,weobservethattheindexsimilarity(tealline)ishigherthan
boththeadjacentsimilarityandtheself-similarityscores. Thispatternprovidesevidenceforiterative
inference,wherealayermovedearlierinthecomputationhasaprojectionontotheresidualstream
that overlaps more strongly with its previous neighbor than with its original position or its new
neighbor. ThisoverlapismorepronouncedinPythiamodelsthaninGPT-2models,possiblybecause
Pythiamodelshavefewerlayerstocompletestage2ofinference.
Second,instage3ofinference,bothmodelsdemonstrateaconvergenceofallmetricsexceptself-
similaritytowardorthogonality. Thecombinationofhighself-similarity(indicatingstubbornness)
andorthogonalitytothereplacedlayerandtheadjacentlayerssuggestsahighdegreeofspecialization
intheMLPsofstage3.
D AdditionalEmpiricalDetails
Allexperimentalcodeforfutureexperimentsisavailableat:
https://github.com/vdlad/Remarkable-Robustness-of-LLMs.
WemakeubiquitoususeofTransformerLens[48]toperformhooksandtransformermanipulations.
Forspecificity,weutilizethefollowingHuggingFacemodelnames,anddataset. Wedonotchange
theparametersofthemodelsfromwhattheyaredescribedontheHuggingFacepage.
AllexperimentsdescribedcanbeperformedonasingleNVIDIAA6000. Weutilized2NVIDIA
A6000and500GBofRAM.Toaggregatethemetricsdescribedinthepaper,werunthemodelon1
18B SingleComponentAblationsPythia1.4BandGPT-2
0.10 1.0
ablate MLP ablate MLP
ablate attention ablate attention
0.08 0.8
0.06 0.6
0.04 0.4
0.02 0.2
0.00 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
relative layer ablated relative layer ablated
Figure13: AblateGPT-2XL Figure14: AblatePythia1.4B
Figure15: AblatingtheMLPinbothmodelsincreasestheKLdivergencebetweenthenominaland
ablatedmodelsinthefinallayers,suggestingneurondependenciesinlaterlayers.
Name HuggingFaceModelName
Pythia410M EleutherAI/pythia-410m-deduped
Pythia1.4B EleutherAI/pythia-1.4b-deduped
Pythia2.8B EleutherAI/pythia-2.8b-deduped
Pythia6.9B EleutherAI/pythia-6.9b-deduped
GPT-2Small(124M) gpt2
GPT-2Medium(355M) gpt2-medium
GPT-2Large(774M) gpt2-large
GPT-2XL(1.5B) gpt2-xl
Phi1(1.3B) microsoft/Phi-1
Phi1.5(1.3B) microsoft/Phi-1.5
Phi2(2.7B) microsoft/Phi-2
ThePile EleutherAI/the_pile_deduplicated
Table3: Listofmodelsanddatasetusedintheexperiments.
milliontokensℓtimes,whereℓisthenumberoflayers. Thistakesonaverage8hourspermodel,per
layerintervention(swappingandablating). Wesavethisaggregationfordataanalysis.
Weutilizeseveralconventionalweightpreprocessingtechniquestostreamlineourcalculations[48].
LayerNormPreprocessing Following[32],beforeeachMLPcalculation,alayernormoperation
isappliedtotheresidualstream. ThisnormalizestheinputbeforetheMLP.TheTransformerLens
packagesimplifiesthisprocessbyincorporatingthelayernormintotheweightsandbiasesofthe
MLP,resultinginmatricesW andb . Inmanylayernormimplementations,trainableparameters
eff eff
γ ∈Rnandb∈Rnareincluded:
x−E(x)
LayerNorm(x)= ∗γ+b. (1)
(cid:112)
Var(x)
We"fold"thelayernormparametersintoW bytreatingthelayernormasalinearlayerandthen
in
mergingthesubsequentlayers:
W =W diag(γ) b =b +W b (2)
eff in eff in in
Additionally,wethencenterreadingweights. Thus,weadjusttheweightsW asfollows:
eff
W′ (i,:)=W (i,:)−W¯ (i,:)
eff eff eff
CenteringWritingWeights BecauseoftheLayerNormoperationineverylayer,wecanalign
weighswiththeall-onedirectionintheresidualstreamastheydonotinfluencethemodel’scalcula-
19
LK
naem
LK
naemC TopPredictionandSuppressionNeurons
Figure16: Top36predictionandsuppressionneuronsfor-ingwhichhavethegreatestmeanabsolute
difference between respective (W ·w ). This is the product between the model unembedding
U out
weightsandoutputweightsofMLP.
tions. Therefore,wemean-centerW andb bysubtractingthecolumnmeansofW :
out out out
W′ (:,i)=W (:,i)−W¯ (:,i)
out out out
20