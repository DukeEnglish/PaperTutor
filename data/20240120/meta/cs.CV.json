[
    {
        "title": "ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions",
        "authors": "Jeonghwan KimJisoo KimJeonghyeon NaHanbyul Joo",
        "links": "http://arxiv.org/abs/2401.10232v1",
        "entry_id": "http://arxiv.org/abs/2401.10232v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10232v1",
        "summary": "To enable machines to learn how humans interact with the physical world in\nour daily activities, it is crucial to provide rich data that encompasses the\n3D motion of humans as well as the motion of objects in a learnable 3D\nrepresentation. Ideally, this data should be collected in a natural setup,\ncapturing the authentic dynamic 3D signals during human-object interactions. To\naddress this challenge, we introduce the ParaHome system, designed to capture\nand parameterize dynamic 3D movements of humans and objects within a common\nhome environment. Our system consists of a multi-view setup with 70\nsynchronized RGB cameras, as well as wearable motion capture devices equipped\nwith an IMU-based body suit and hand motion capture gloves. By leveraging the\nParaHome system, we collect a novel large-scale dataset of human-object\ninteraction. Notably, our dataset offers key advancement over existing datasets\nin three main aspects: (1) capturing 3D body and dexterous hand manipulation\nmotion alongside 3D object movement within a contextual home environment during\nnatural activities; (2) encompassing human interaction with multiple objects in\nvarious episodic scenarios with corresponding descriptions in texts; (3)\nincluding articulated objects with multiple parts expressed with parameterized\narticulations. Building upon our dataset, we introduce new research tasks aimed\nat building a generative model for learning and synthesizing human-object\ninteractions in a real-world room setting.",
        "updated": "2024-01-18 18:59:58 UTC",
        "id": 1
    },
    {
        "title": "OMG-Seg: Is One Model Good Enough For All Segmentation?",
        "authors": "Xiangtai LiHaobo YuanWei LiHenghui DingSize WuWenwei ZhangYining LiKai ChenChen Change Loy",
        "links": "http://arxiv.org/abs/2401.10229v1",
        "entry_id": "http://arxiv.org/abs/2401.10229v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10229v1",
        "summary": "In this work, we address various segmentation tasks, each traditionally\ntackled by distinct or partially unified models. We propose OMG-Seg, One Model\nthat is Good enough to efficiently and effectively handle all the segmentation\ntasks, including image semantic, instance, and panoptic segmentation, as well\nas their video counterparts, open vocabulary settings, prompt-driven,\ninteractive segmentation like SAM, and video object segmentation. To our\nknowledge, this is the first model to handle all these tasks in one model and\nachieve satisfactory performance. We show that OMG-Seg, a transformer-based\nencoder-decoder architecture with task-specific queries and outputs, can\nsupport over ten distinct segmentation tasks and yet significantly reduce\ncomputational and parameter overhead across various tasks and datasets. We\nrigorously evaluate the inter-task influences and correlations during\nco-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.",
        "updated": "2024-01-18 18:59:34 UTC",
        "id": 2
    },
    {
        "title": "RAP-SAM: Towards Real-Time All-Purpose Segment Anything",
        "authors": "Shilin XuHaobo YuanQingyu ShiLu QiJingbo WangYibo YangYining LiKai ChenYunhai TongBernard GhanemXiangtai LiMing-Hsuan Yang",
        "links": "http://arxiv.org/abs/2401.10228v1",
        "entry_id": "http://arxiv.org/abs/2401.10228v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10228v1",
        "summary": "Advanced by transformer architecture, vision foundation models (VFMs) achieve\nremarkable progress in performance and generalization ability. Segment Anything\nModel (SAM) is one remarkable model that can achieve generalized segmentation.\nHowever, most VFMs cannot run in realtime, which makes it difficult to transfer\nthem into several products. On the other hand, current real-time segmentation\nmainly has one purpose, such as semantic segmentation on the driving scene. We\nargue that diverse outputs are needed for real applications. Thus, this work\nexplores a new real-time segmentation setting, named all-purpose segmentation\nin real-time, to transfer VFMs in real-time deployment. It contains three\ndifferent tasks, including interactive segmentation, panoptic segmentation, and\nvideo segmentation. We aim to use one model to achieve the above tasks in\nreal-time. We first benchmark several strong baselines. Then, we present\nReal-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an\nefficient decoupled decoder to perform prompt-driven decoding. Moreover, we\nfurther explore different training strategies and tuning methods to boost\nco-training performance further. Our code and model are available at\nhttps://github.com/xushilin1/RAP-SAM/.",
        "updated": "2024-01-18 18:59:30 UTC",
        "id": 3
    },
    {
        "title": "A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting",
        "authors": "Wouter Van GansbekeBert De Brabandere",
        "links": "http://arxiv.org/abs/2401.10227v1",
        "entry_id": "http://arxiv.org/abs/2401.10227v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10227v1",
        "summary": "Panoptic and instance segmentation networks are often trained with\nspecialized object detection modules, complex loss functions, and ad-hoc\npost-processing steps to handle the permutation-invariance of the instance\nmasks. This work builds upon Stable Diffusion and proposes a latent diffusion\napproach for panoptic segmentation, resulting in a simple architecture which\nomits these complexities. Our training process consists of two steps: (1)\ntraining a shallow autoencoder to project the segmentation masks to latent\nspace; (2) training a diffusion model to allow image-conditioned sampling in\nlatent space. The use of a generative model unlocks the exploration of mask\ncompletion or inpainting, which has applications in interactive segmentation.\nThe experimental validation yields promising results for both panoptic\nsegmentation and mask inpainting. While not setting a new state-of-the-art, our\nmodel's simplicity, generality, and mask completion capability are desirable\nproperties.",
        "updated": "2024-01-18 18:59:19 UTC",
        "id": 4
    },
    {
        "title": "Towards Language-Driven Video Inpainting via Multimodal Large Language Models",
        "authors": "Jianzong WuXiangtai LiChenyang SiShangchen ZhouJingkang YangJiangning ZhangYining LiKai ChenYunhai TongZiwei LiuChen Change Loy",
        "links": "http://arxiv.org/abs/2401.10226v1",
        "entry_id": "http://arxiv.org/abs/2401.10226v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10226v1",
        "summary": "We introduce a new task -- language-driven video inpainting, which uses\nnatural language instructions to guide the inpainting process. This approach\novercomes the limitations of traditional video inpainting methods that depend\non manually labeled binary masks, a process often tedious and labor-intensive.\nWe present the Remove Objects from Videos by Instructions (ROVI) dataset,\ncontaining 5,650 videos and 9,091 inpainting results, to support training and\nevaluation for this task. We also propose a novel diffusion-based\nlanguage-driven video inpainting framework, the first end-to-end baseline for\nthis task, integrating Multimodal Large Language Models to understand and\nexecute complex language-based inpainting requests effectively. Our\ncomprehensive results showcase the dataset's versatility and the model's\neffectiveness in various language-instructed inpainting scenarios. We will make\ndatasets, code, and models publicly available.",
        "updated": "2024-01-18 18:59:13 UTC",
        "id": 5
    }
]