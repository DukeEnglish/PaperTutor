[
    {
        "title": "Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security",
        "authors": "Alec WilsonRyan MenziesNeela MorarjiDavid FosterMarco Casassa MontEsin TurkbeylerLisa Gralewski",
        "links": "http://arxiv.org/abs/2401.10149v1",
        "entry_id": "http://arxiv.org/abs/2401.10149v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10149v1",
        "summary": "This paper demonstrates the potential for autonomous cyber defence to be\napplied on industrial control systems and provides a baseline environment to\nfurther explore Multi-Agent Reinforcement Learning's (MARL) application to this\nproblem domain. It introduces a simulation environment, IPMSRL, of a generic\nIntegrated Platform Management System (IPMS) and explores the use of MARL for\nautonomous cyber defence decision-making on generic maritime based IPMS\nOperational Technology (OT). OT cyber defensive actions are less mature than\nthey are for Enterprise IT. This is due to the relatively brittle nature of OT\ninfrastructure originating from the use of legacy systems, design-time\nengineering assumptions, and lack of full-scale modern security controls. There\nare many obstacles to be tackled across the cyber landscape due to continually\nincreasing cyber-attack sophistication and the limitations of traditional\nIT-centric cyber defence solutions. Traditional IT controls are rarely deployed\non OT infrastructure, and where they are, some threats aren't fully addressed.\nIn our experiments, a shared critic implementation of Multi Agent Proximal\nPolicy Optimisation (MAPPO) outperformed Independent Proximal Policy\nOptimisation (IPPO). MAPPO reached an optimal policy (episode outcome mean of\n1) after 800K timesteps, whereas IPPO was only able to reach an episode outcome\nmean of 0.966 after one million timesteps. Hyperparameter tuning greatly\nimproved training performance. Across one million timesteps the tuned\nhyperparameters reached an optimal policy whereas the default hyperparameters\nonly managed to win sporadically, with most simulations resulting in a draw. We\ntested a real-world constraint, attack detection alert success, and found that\nwhen alert success probability is reduced to 0.75 or 0.9, the MARL defenders\nwere still able to win in over 97.5% or 99.5% of episodes, respectively.",
        "updated": "2024-01-18 17:22:22 UTC",
        "id": 1
    },
    {
        "title": "Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data",
        "authors": "Nathan Lichtl√©Kathy JangAdit ShahEugene VinitskyJonathan W. LeeAlexandre M. Bayen",
        "links": "http://arxiv.org/abs/2401.09666v1",
        "entry_id": "http://arxiv.org/abs/2401.09666v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09666v1",
        "summary": "Designing traffic-smoothing cruise controllers that can be deployed onto\nautonomous vehicles is a key step towards improving traffic flow, reducing\ncongestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass\nthe common issue of having to carefully fine-tune a large traffic\nmicrosimulator by leveraging real-world trajectory data from the I-24 highway\nin Tennessee, replayed in a one-lane simulation. Using standard deep\nreinforcement learning methods, we train energy-reducing wave-smoothing\npolicies. As an input to the agent, we observe the speed and distance of only\nthe vehicle in front, which are local states readily available on most recent\nvehicles, as well as non-local observations about the downstream state of the\ntraffic. We show that at a low 4% autonomous vehicle penetration rate, we\nachieve significant fuel savings of over 15% on trajectories exhibiting many\nstop-and-go waves. Finally, we analyze the smoothing effect of the controllers\nand demonstrate robustness to adding lane-changing into the simulation as well\nas the removal of downstream information.",
        "updated": "2024-01-18 00:50:41 UTC",
        "id": 2
    },
    {
        "title": "Improved Consensus ADMM for Cooperative Motion Planning of Large-Scale Connected Autonomous Vehicles with Limited Communication",
        "authors": "Haichao LiuZhenmin HuangZicheng ZhuYulin LiShaojie ShenJun Ma",
        "links": "http://arxiv.org/abs/2401.09032v1",
        "entry_id": "http://arxiv.org/abs/2401.09032v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09032v1",
        "summary": "This paper investigates a cooperative motion planning problem for large-scale\nconnected autonomous vehicles (CAVs) under limited communications, which\naddresses the challenges of high communication and computing resource\nrequirements. Our proposed methodology incorporates a parallel optimization\nalgorithm with improved consensus ADMM considering a more realistic locally\nconnected topology network, and time complexity of O(N) is achieved by\nexploiting the sparsity in the dual update process. To further enhance the\ncomputational efficiency, we employ a lightweight evolution strategy for the\ndynamic connectivity graph of CAVs, and each sub-problem split from the\nconsensus ADMM only requires managing a small group of CAVs. The proposed\nmethod implemented with the receding horizon scheme is validated thoroughly,\nand comparisons with existing numerical solvers and approaches demonstrate the\nefficiency of our proposed algorithm. Also, simulations on large-scale\ncooperative driving tasks involving 80 vehicles are performed in the\nhigh-fidelity CARLA simulator, which highlights the remarkable computational\nefficiency, scalability, and effectiveness of our proposed development.\nDemonstration videos are available at\nhttps://henryhcliu.github.io/icadmm_cmp_carla.",
        "updated": "2024-01-17 07:58:48 UTC",
        "id": 3
    },
    {
        "title": "Data assimilation approach for addressing imperfections in people flow measurement techniques using particle filter",
        "authors": "Ryo MurataKenji Tanaka",
        "links": "http://arxiv.org/abs/2401.09014v1",
        "entry_id": "http://arxiv.org/abs/2401.09014v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09014v1",
        "summary": "Understanding and predicting people flow in urban areas is useful for\ndecision-making in urban planning and marketing strategies. Traditional methods\nfor understanding people flow can be divided into measurement-based approaches\nand simulation-based approaches. Measurement-based approaches have the\nadvantage of directly capturing actual people flow, but they face the challenge\nof data imperfection. On the other hand, simulations can obtain complete data\non a computer, but they only consider some of the factors determining human\nbehavior, leading to a divergence from actual people flow. Both measurement and\nsimulation methods have unresolved issues, and combining the two can\ncomplementarily overcome them. This paper proposes a method that applies data\nassimilation, a fusion technique of measurement and simulation, to agent-based\nsimulation. Data assimilation combines the advantages of both measurement and\nsimulation, contributing to the creation of an environment that can reflect\nreal people flow while acquiring richer data. The paper verifies the\neffectiveness of the proposed method in a virtual environment and demonstrates\nthe potential of data assimilation to compensate for the three types of\nimperfection in people flow measurement techniques. These findings can serve as\nguidelines for supplementing sparse measurement data in physical environments.",
        "updated": "2024-01-17 07:20:15 UTC",
        "id": 4
    },
    {
        "title": "AgentMixer: Multi-Agent Correlated Policy Factorization",
        "authors": "Zhiyuan LiWenshuai ZhaoLijun WuJoni Pajarinen",
        "links": "http://arxiv.org/abs/2401.08728v1",
        "entry_id": "http://arxiv.org/abs/2401.08728v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08728v1",
        "summary": "Centralized training with decentralized execution (CTDE) is widely employed\nto stabilize partially observable multi-agent reinforcement learning (MARL) by\nutilizing a centralized value function during training. However, existing\nmethods typically assume that agents make decisions based on their local\nobservations independently, which may not lead to a correlated joint policy\nwith sufficient coordination. Inspired by the concept of correlated\nequilibrium, we propose to introduce a \\textit{strategy modification} to\nprovide a mechanism for agents to correlate their policies. Specifically, we\npresent a novel framework, AgentMixer, which constructs the joint fully\nobservable policy as a non-linear combination of individual partially\nobservable policies. To enable decentralized execution, one can derive\nindividual policies by imitating the joint policy. Unfortunately, such\nimitation learning can lead to \\textit{asymmetric learning failure} caused by\nthe mismatch between joint policy and individual policy information. To\nmitigate this issue, we jointly train the joint policy and individual policies\nand introduce \\textit{Individual-Global-Consistency} to guarantee mode\nconsistency between the centralized and decentralized policies. We then\ntheoretically prove that AgentMixer converges to an $\\epsilon$-approximate\nCorrelated Equilibrium. The strong experimental performance on three MARL\nbenchmarks demonstrates the effectiveness of our method.",
        "updated": "2024-01-16 15:32:41 UTC",
        "id": 5
    }
]