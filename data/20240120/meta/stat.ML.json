[
    {
        "title": "Maximal-Capacity Discrete Memoryless Channel Identification",
        "authors": "Maximilian EggerRawad BitarAntonia Wachter-ZehDeniz GündüzNir Weinberger",
        "links": "http://arxiv.org/abs/2401.10204v1",
        "entry_id": "http://arxiv.org/abs/2401.10204v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10204v1",
        "summary": "The problem of identifying the channel with the highest capacity among\nseveral discrete memoryless channels (DMCs) is considered. The problem is cast\nas a pure-exploration multi-armed bandit problem, which follows the practical\nuse of training sequences to sense the communication channel statistics. A\ncapacity estimator is proposed and tight confidence bounds on the estimator\nerror are derived. Based on this capacity estimator, a gap-elimination\nalgorithm termed BestChanID is proposed, which is oblivious to the\ncapacity-achieving input distribution and is guaranteed to output the DMC with\nthe largest capacity, with a desired confidence. Furthermore, two additional\nalgorithms NaiveChanSel and MedianChanEl, that output with certain confidence a\nDMC with capacity close to the maximal, are introduced. Each of those\nalgorithms is beneficial in a different regime and can be used as a subroutine\nin BestChanID. The sample complexity of all algorithms is analyzed as a\nfunction of the desired confidence parameter, the number of channels, and the\nchannels' input and output alphabet sizes. The cost of best channel\nidentification is shown to scale quadratically with the alphabet size, and a\nfundamental lower bound for the required number of channel senses to identify\nthe best channel with a certain confidence is derived.",
        "updated": "2024-01-18 18:44:10 UTC",
        "id": 1
    },
    {
        "title": "False Discovery Rate Control for Gaussian Graphical Models via Neighborhood Screening",
        "authors": "Taulant KokaJasin MachkourMichael Muma",
        "links": "http://arxiv.org/abs/2401.09979v1",
        "entry_id": "http://arxiv.org/abs/2401.09979v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09979v1",
        "summary": "Gaussian graphical models emerge in a wide range of fields. They model the\nstatistical relationships between variables as a graph, where an edge between\ntwo variables indicates conditional dependence. Unfortunately, well-established\nestimators, such as the graphical lasso or neighborhood selection, are known to\nbe susceptible to a high prevalence of false edge detections. False detections\nmay encourage inaccurate or even incorrect scientific interpretations, with\nmajor implications in applications, such as biomedicine or healthcare. In this\npaper, we introduce a nodewise variable selection approach to graph learning\nand provably control the false discovery rate of the selected edge set at a\nself-estimated level. A novel fusion method of the individual neighborhoods\noutputs an undirected graph estimate. The proposed method is parameter-free and\ndoes not require tuning by the user. Benchmarks against competing false\ndiscovery rate controlling methods in numerical experiments considering\ndifferent graph topologies show a significant gain in performance.",
        "updated": "2024-01-18 13:46:41 UTC",
        "id": 2
    },
    {
        "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
        "authors": "Alexander TelepovArtem TsypinKuzma KhrabrovSergey YakukhnovPavel StrashnovPetr ZhilyaevEgor RumiantsevDaniel EzhovManvel AvetisianOlga PopovaArtur Kadurin",
        "links": "http://arxiv.org/abs/2401.09840v1",
        "entry_id": "http://arxiv.org/abs/2401.09840v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09840v1",
        "summary": "A rational design of new therapeutic drugs aims to find a molecular structure\nwith desired biological functionality, e.g., an ability to activate or suppress\na specific protein via binding to it. Molecular docking is a common technique\nfor evaluating protein-molecule interactions. Recently, Reinforcement Learning\n(RL) has emerged as a promising approach to generating molecules with the\ndocking score (DS) as a reward. In this work, we reproduce, scrutinize and\nimprove the recent RL model for molecule generation called FREED\n(arXiv:2110.01219). Extensive evaluation of the proposed method reveals several\nlimitations and challenges despite the outstanding results reported for three\ntarget proteins. Our contributions include fixing numerous implementation bugs\nand simplifying the model while increasing its quality, significantly extending\nexperiments, and conducting an accurate comparison with current\nstate-of-the-art methods for protein-conditioned molecule generation. We show\nthat the resulting fixed model is capable of producing molecules with superior\ndocking scores compared to alternative approaches.",
        "updated": "2024-01-18 09:54:19 UTC",
        "id": 3
    },
    {
        "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
        "authors": "Seong Jin ChoGwangsu KimJunghyun LeeJinwoo ShinChang D. Yoo",
        "links": "http://arxiv.org/abs/2401.09787v1",
        "entry_id": "http://arxiv.org/abs/2401.09787v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09787v1",
        "summary": "Active learning is a machine learning paradigm that aims to improve the\nperformance of a model by strategically selecting and querying unlabeled data.\nOne effective selection strategy is to base it on the model's predictive\nuncertainty, which can be interpreted as a measure of how informative a sample\nis. The sample's distance to the decision boundary is a natural measure of\npredictive uncertainty, but it is often intractable to compute, especially for\ncomplex decision boundaries formed in multiclass classification tasks. To\naddress this issue, this paper proposes the {\\it least disagree metric} (LDM),\ndefined as the smallest probability of disagreement of the predicted label, and\nan estimator for LDM proven to be asymptotically consistent under mild\nassumptions. The estimator is computationally efficient and can be easily\nimplemented for deep learning models using parameter perturbation. The\nLDM-based active learning is performed by querying unlabeled data with the\nsmallest LDM. Experimental results show that our LDM-based active learning\nalgorithm obtains state-of-the-art overall performance on all considered\ndatasets and deep architectures.",
        "updated": "2024-01-18 08:12:23 UTC",
        "id": 4
    },
    {
        "title": "Harnessing Density Ratios for Online Reinforcement Learning",
        "authors": "Philip AmortilaDylan J. FosterNan JiangAyush SekhariTengyang Xie",
        "links": "http://arxiv.org/abs/2401.09681v1",
        "entry_id": "http://arxiv.org/abs/2401.09681v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09681v1",
        "summary": "The theories of offline and online reinforcement learning, despite having\nevolved in parallel, have begun to show signs of the possibility for a\nunification, with algorithms and analysis techniques for one setting often\nhaving natural counterparts in the other. However, the notion of density ratio\nmodeling, an emerging paradigm in offline RL, has been largely absent from\nonline RL, perhaps for good reason: the very existence and boundedness of\ndensity ratios relies on access to an exploratory dataset with good coverage,\nbut the core challenge in online RL is to collect such a dataset without having\none to start. In this work we show -- perhaps surprisingly -- that density\nratio-based algorithms have online counterparts. Assuming only the existence of\nan exploratory distribution with good coverage, a structural condition known as\ncoverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses\ndensity ratio realizability and value function realizability to perform\nsample-efficient online exploration. GLOW addresses unbounded density ratios\nvia careful use of truncation, and combines this with optimism to guide\nexploration. GLOW is computationally inefficient; we complement it with a more\nefficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022)\nwherein online RL is augmented with additional offline data. HyGLOW is derived\nas a special case of a more general meta-algorithm that provides a provable\nblack-box reduction from hybrid RL to offline RL, which may be of independent\ninterest.",
        "updated": "2024-01-18 02:21:06 UTC",
        "id": 5
    }
]