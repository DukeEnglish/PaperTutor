[
    {
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "authors": "Zihan LiuWei PingRajarshi RoyPeng XuMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2401.10225v1",
        "entry_id": "http://arxiv.org/abs/2401.10225v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10225v1",
        "summary": "In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval in conversational QA, we fine-tune a dense retriever on a\nmulti-turn QA dataset, which provides comparable results to using the\nstate-of-the-art query rewriting model while largely reducing deployment cost.\nNotably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10\nconversational QA datasets (54.14 vs. 53.90), without relying on any synthetic\ndata from OpenAI GPT models.",
        "updated": "2024-01-18 18:59:11 UTC",
        "id": 1
    },
    {
        "title": "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer",
        "authors": "Changyao TianXizhou ZhuYuwen XiongWeiyun WangZhe ChenWenhai WangYuntao ChenLewei LuTong LuJie ZhouHongsheng LiYu QiaoJifeng Dai",
        "links": "http://arxiv.org/abs/2401.10208v1",
        "entry_id": "http://arxiv.org/abs/2401.10208v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10208v1",
        "summary": "Developing generative models for interleaved image-text data has both\nresearch and practical value. It requires models to understand the interleaved\nsequences and subsequently generate images and text. However, existing attempts\nare limited by the issue that the fixed number of visual tokens cannot\nefficiently capture image details, which is particularly problematic in the\nmulti-image scenarios. To address this, this paper presents MM-Interleaved, an\nend-to-end generative model for interleaved image-text data. It introduces a\nmulti-scale and multi-image feature synchronizer module, allowing direct access\nto fine-grained image features in the previous context during the generation\nprocess. MM-Interleaved is end-to-end pre-trained on both paired and\ninterleaved image-text corpora. It is further enhanced through a supervised\nfine-tuning phase, wherein the model improves its ability to follow complex\nmulti-modal instructions. Experiments demonstrate the versatility of\nMM-Interleaved in recognizing visual details following multi-modal instructions\nand generating consistent images following both textual and visual conditions.\nCode and models are available at\n\\url{https://github.com/OpenGVLab/MM-Interleaved}.",
        "updated": "2024-01-18 18:50:16 UTC",
        "id": 2
    },
    {
        "title": "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction",
        "authors": "Qingyun WangZixuan ZhangHongxiang LiXuan LiuJiawei HanHeng JiHuimin Zhao",
        "links": "http://arxiv.org/abs/2401.10189v1",
        "entry_id": "http://arxiv.org/abs/2401.10189v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10189v1",
        "summary": "Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.",
        "updated": "2024-01-18 18:20:15 UTC",
        "id": 3
    },
    {
        "title": "Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation",
        "authors": "Zdeněk KasnerOndřej Dušek",
        "links": "http://arxiv.org/abs/2401.10186v1",
        "entry_id": "http://arxiv.org/abs/2401.10186v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10186v1",
        "summary": "We investigate to which extent open large language models (LLMs) can generate\ncoherent and relevant text from structured data. To prevent bias from\nbenchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc\nbenchmark for five data-to-text (D2T) generation tasks, consisting of\nstructured data records in standard formats gathered from public APIs. We\nleverage reference-free evaluation metrics and LLMs' in-context learning\ncapabilities, allowing us to test the models with no human-written references.\nOur evaluation focuses on annotating semantic accuracy errors on token-level,\ncombining human annotators and a metric based on GPT-4. Our systematic\nexamination of the models' behavior across domains and tasks suggests that\nstate-of-the-art open LLMs with 7B parameters can generate fluent and coherent\ntext from various standard data formats in zero-shot settings. However, we also\nshow that semantic accuracy of the outputs remains a major issue: on our\nbenchmark, 80% of outputs of open LLMs contain a semantic error according to\nhuman annotators (91% according to GPT-4). Our code, data, and model outputs\nare available at https://d2t-llm.github.io.",
        "updated": "2024-01-18 18:15:46 UTC",
        "id": 4
    },
    {
        "title": "Spatial-Temporal Large Language Model for Traffic Prediction",
        "authors": "Chenxi LiuSun YangQianxiong XuZhishuai LiCheng LongZiyue LiRui Zhao",
        "links": "http://arxiv.org/abs/2401.10134v1",
        "entry_id": "http://arxiv.org/abs/2401.10134v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10134v1",
        "summary": "Traffic prediction, a critical component for intelligent transportation\nsystems, endeavors to foresee future traffic at specific locations using\nhistorical data. Although existing traffic prediction models often emphasize\ndeveloping complex neural network structures, their accuracy has not seen\nimprovements accordingly. Recently, Large Language Models (LLMs) have shown\noutstanding capabilities in time series analysis. Differing from existing\nmodels, LLMs progress mainly through parameter expansion and extensive\npre-training while maintaining their fundamental structures. In this paper, we\npropose a Spatial-Temporal Large Language Model (ST-LLM) for traffic\nprediction. Specifically, ST-LLM redefines the timesteps at each location as\ntokens and incorporates a spatial-temporal embedding module to learn the\nspatial location and global temporal representations of tokens. Then these\nrepresentations are fused to provide each token with unified spatial and\ntemporal information. Furthermore, we propose a novel partially frozen\nattention strategy of the LLM, which is designed to capture spatial-temporal\ndependencies for traffic prediction. Comprehensive experiments on real traffic\ndatasets offer evidence that ST-LLM outperforms state-of-the-art models.\nNotably, the ST-LLM also exhibits robust performance in both few-shot and\nzero-shot prediction scenarios.",
        "updated": "2024-01-18 17:03:59 UTC",
        "id": 5
    }
]