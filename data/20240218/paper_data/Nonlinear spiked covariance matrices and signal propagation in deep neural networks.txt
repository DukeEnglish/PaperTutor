Nonlinear spiked covariance matrices and signal propagation
in deep neural networks
Zhichao Wang∗, Denny Wu†, Zhou Fan‡
Abstract
Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by
the nonlinear feature map of a feedforward neural network. However, existing results only establish
weakconvergenceoftheempiricaleigenvaluedistribution,andfallshortofprovidingprecisequantitative
characterizations of the “spike” eigenvalues and eigenvectors that often capture the low-dimensional
signal structure of the learning problem. In this work, we characterize these signal eigenvalues and
eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case.
Usingthisgeneralresult,wegiveaquantitativedescriptionofhowspikedeigenstructureintheinputdata
propagatesthroughthehiddenlayersofaneuralnetworkwithrandomweights. Asasecondapplication,
we study a simple regime of representation learning where the weight matrix develops a rank-one signal
componentovertrainingandcharacterizethealignmentofthetargetfunctionwiththespikeeigenvector
of the CK on test data.
1 Introduction
Kernel matrices associated with the nonlinear feature map of deep neural networks (NNs) provide insight
into the optimization dynamics [JGH18, MZ20, FDP+20] and predictive performance [LBN+17, ADH+19,
OJMDF21]; consequently, properties of these kernel matrices can guide the design of network architecture
[XBSD+18,MBD+21,LNR22]andlearningalgorithms[KO20,ZNB22]. Particularemphasishasbeenplaced
on the spectral properties of kernel matrices, due to their connection with the training and test performance
of the underlying NN [BCP20, LGC+21, WHS22].
In this paper, we focus on the conjugate kernel (CK) [Nea95, CS09] defined as the Gram matrix of
the features at the penultimate (or more generally, any intermediate) NN layer. In the high-dimensional
asymptotic setting where the width of the NN and the number of training samples diverge at the same
rate, prior works employed random matrix theory to analyze the limit eigenvalue distribution of the CK
matrix at random initialization [PW17, LLC18, P´ec19, FW20]. These and related characterizations of the
CK resolvent enable precise computations of various errors for NNs with random first-layer weights, known
as random features models [MM22, TAP21, HJ22].
It is worth noting that while existing results establish the weak convergence of the empirical spectral
measure, the precise behavior of “spike” eigenvalues that are separated from the spectral bulk remains
largely unexplored. In learning applications, these spike eigenvalues and corresponding eigenvectors are
often the primary spectral features (signal) of interest, because they pertain to low-rank structure of the
underlying learning problem (e.g., class labels or the direction of the target function). For the linearly
defined spiked covariance model X = ZΣ1/2 ∈ Rn×d, whose dependence across features is induced by a
linear map Σ1/2(·) applied to Z having i.i.d. coordinates, classical work in random matrix theory provides
a quantitative description of the spike eigenvalue/eigenvector behavior [Joh01, BS06, BGN12, BKYY16]. In
this paper, we establish an analogous characterization of spiked spectral structure for the CK, motivated in
part by the following applications:
∗DepartmentofMathematics,UniversityofCaliforniaSanDiego. zhw036@ucsd.edu.
†CenterforDataScience,NewYorkUniversity,andFlatironInstitute. dennywu@nyu.edu.
‡DepartmentofStatisticsandDataScience,YaleUniversity. zhou.fan@yale.edu.
1
4202
beF
51
]LM.tats[
1v72101.2042:viXra• Structured input data. Real data often contain low-dimensional structure despite the high ambient
dimensionality [LV07, HTFF09, PZA+21], and the leading eigenvectors of the input covariance matrix
may be good predictors of the training labels. Common examples where the input features exhibit a
low-dimensional spiked structure include Gaussian mixture models [LGC+21, RGKZ21, BAGJ23] and
the block-covariance setting of [GMMM20, BES+23, MHWSE23]. Assuming that the input data X has
informative spikes eigenvectors, we ask the natural question:
How does the low-dimensional signal propagate through nonlinear layers of the NN?
When do we observe a similar spiked structure in the CK matrix?
• Spiked weight matrices in early training. ItisknownthatNNscanlearnusefulrepresentationsthat
adapttothelearningproblem,andoutperformtherandomfeaturesmodeldefinedbyrandomlyinitialized
weights [GMMM19, WLLM19, AAM22]. Recent works have shown that when the target function is low-
dimensional, the gradient update for two-layer NNs around initialization is low-rank [BES+22, DLS22,
WES+23], andhencetheupdatedweightmatrixW iswell-approximatedbyaspikedmodel. Weconsider
the following question on this pre-trained kernel model in NNs:
When gradient descent produces a spiked structure in the weight matrix, how does the feature
representation of the NN change, in terms of spectral properties of the CK?
1.1 Our Contributions
We analyze the spike eigenstructure in a general nonlinear spiked covariance model, which includes the CK
as a special case. Specifically, we characterize the BBP phase transition [BAP05] and first-order limits of
the eigenvalues and eigenvector alignments in the proportional asymptotics regime, for spike eigenvalues of
bounded size. Our work makes the following contributions:
• Signal propagation in deep random NNs. Followingthesetupof[FW20],weconsidertheCKmatrix
defined by a multi-layer fully-connected NN at random initialization, where the width of each layer grows
linearly with the sample size. Given spiked input data, we compute the magnitude of the leading CK
eigenvalues and the alignments between the corresponding CK eigenvectors with those of the input data,
across network depth.
• Feature learning in two-layer NNs. Weconsidertheearly-phasefeaturelearningsettingin[BES+22],
where the first-layer weights in a two-layer NN are optimized by gradient descent, and the learned weight
matrixexhibitsarank-onespikedstructure. Wecharacterizethespikedeigenstructureofthecorresponding
CK matrix for independent test data, and the alignment of spike eigenvectors with the test labels. This
provides a quantitative description of how gradient descent improves the NN representation.
• Spectral analysis for nonlinear spiked covariance models. We give a general analysis of the signal
eigenvalues/eigenvectors of spiked covariance matrices with arbitrary and possibly nonlinear dependence
acrossfeatures,showinga“Gaussianequivalence”withthequantitativespectralpropertiesoflinearspiked
covariance models established by [BY12]. We prove a deterministic equivalent for the Stieltjes transform
andresolventforanyspectralargumentseparatedfromthesupportofthelimitspectralmeasure,extending
recent results for spectral arguments bounded away from the positive real line [Cho22, Cho23, SCDL23].
1.2 Related Works
Eigenvalues of nonlinear random matrices. Global convergence of the empirical eigenvalue distri-
bution of nonlinear kernel matrices has been studied in both proportional and polynomial scaling regimes
[EK10,CS13,FM19,LY22,DLMY23]. Buildinguponrelatedtechniques,recentworkscharacterizedthespec-
trum of the CK matrix [PW17, LLC18, P´ec19] and the neural tangent kernel (NTK) matrix [MZ20, AP20],
with generalizations to deeper networks studied in [FW20] and [Cho23].
[BP22] gave a precise characterization of the largest eigenvalue in a one-hidden-layer CK matrix when
the input data X and weight matrix W both have i.i.d. entries, identifying possible uninformative spike
2eigenvalues when the nonlinear activation is not an odd function. [GKK+23] and [Fel23] recently character-
izedspikedeigenstructureinmodelswhereanactivationisappliedtoaspikedWignermatrixorrectangular
information-plus-noise matrix entrywise, for possibly growing spike sizes and activations having degenerate
information/Hermite coefficients.
Precise error analysis of NNs. An important application of spectral analyses of the CK matrix is
the precise computation of generalization error of random features regression, first performed for two-layer
models in proportional scaling regimes [LLC18, MM22] and later extended to deep random features models
[SCDL23, BPH23] and polynomial scaling regimes [GMMM21, XHM+22]. These risk analyses reveal a
Gaussianequivalenceprinciple,wheregeneralizationerrorcoincideswiththatofaGaussiancovariatesmodel,
andthisequivalencehasbeenextendedtoothersettingsofnonlinear(regularized)empiricalriskminimization
[HL20, GLR+21, MS22].
Going beyond random features, [BES+22] derived the precise asymptotics of representation learning in
a two-layer NN when the first-layer weights are trained by one (or finitely many) gradient descent steps; see
also [DLS22, BES+23, DKL+23]. The computation follows from an information-plus-noise characterization
of the weight matrix due to a low-rank gradient update. [MLHD23] derived a corresponding information-
plus-noisedecompositionoftheCKmatrixdefinedbytheresultingtrainedweights,inanasymptoticregime
differentfromourswherethelearningrateandspikeeigenvaluesdiverge. [BAGHJ23]examinedtheemerging
spike eigenstructure in the NN Hessian that arises during SGD training.
Eigenvalues of sample covariance matrices. Asymptotic spectral analyses of sample covariance ma-
trices have a long history in random matrix theory [MP67, Sil95, SB95, BS98], with the strongest known
results in the linearly defined model X = ZΣ1/2, see e.g. [BEK+14, KY17]. Outside of this linear setting,
[SV13] and [CT18] develop sharp bounds for the extremal eigenvalues with isotropic population covariance,
and [BX22] develop eigenvalue rigidity and Tracy-Widom fluctuation results for isotropic and log-concave
distributions.
The spiked covariance model was introduced in [Joh01]. [BAP05, BS06, Pau07] initiated the study of
spiked eigenstructure and phase transition phenomena for spiked covariance matrices with isotropic bulk
covariance. [P´ec06, BGN11, BGN12, Cap13, Cap18] studied spiked eigenstructure in related Wigner and
information-plus-noise models. Closely related to our work are the results of [BY12] that characterize spike
eigenvaluesinlinearlydefinedmodelsX =ZΣ1/2 withgeneralpopulationcovarianceΣ,andweextendthis
characterization to nonlinear settings.
2 Results for neural network models
2.1 Propagation of signal through multi-layer neural networks
Consider input features X = [x ,...,x ] ∈ Rd×n, where x ∈ Rd are independent samples. Define a
1 n i
L-hidden-layer feedforward neural network by
1
X
ℓ
= √ σ(W ℓX ℓ−1)∈Rdℓ×n for ℓ=1...,L (2.1)
d
ℓ
with weight matrices W
ℓ
∈ Rdℓ×dℓ−1, X
0
≡ X and d
0
≡ d, and a nonlinear activation function σ : R → R
applied entrywise. The Conjugate Kernel (CK) at each layer ℓ=1,...,L is given by the Gram matrix
K =X⊤X ∈Rn×n. (2.2)
ℓ ℓ ℓ
In the limit n,d ,...,d → ∞ with n/d → γ ∈ (0,∞) for each ℓ = 0,...,L, under deterministic
0 L ℓ ℓ
conditions for the input data X and for random weight matrices W ,...,W as specified below, it is
1 L
shown in [FW20] that the empirical eigenvalue distribution µ of K for each ℓ=1,...,L satisfies the weak
(cid:98)ℓ ℓ
convergence
n
1 (cid:88)
µ := δ →µ a.s. (2.3)
(cid:98)ℓ n λi(Kℓ) ℓ
i=1
3for limit measures µ ,...,µ defined as follows: Let µ be the limit eigenvalue distribution of the input
1 L 0
gram matrix K =X⊤X (c.f. Assumption 2). Then, for ℓ=1,...,L, let
0
ν =b2 ⊗µ ⊕(1−b2) (2.4)
ℓ−1 σ ℓ−1 σ
denote the law of b2x+(1−b2) when x∼µ and b :=E [σ′(ξ)], and define
σ σ ℓ−1 σ ξ∼N(0,1)
µ =ρMP⊠ν .
ℓ γℓ ℓ−1
Here, ρMP⊠ν denotes the deformed Marcenko-Pastur law describing the limit eigenvalue distribution of a
γ
sample covariance matrix with limit dimension ratio γ ∈(0,∞) and population spectral measure ν, and we
review its definition in Appendix A.
Inthissection,weprovideaprecisequantitativecharacterizationofthespikeeigenvaluesandeigenvectors
of K for each ℓ=1,...,L when the input data X has a fixed number of spike singular values of bounded
ℓ
magnitude. We assume the following conditions for the random weights, input data, and activation.
Assumption 1. The number of layers L≥1 is fixed, and n,d ,...,d →∞ such that
0 L
n/d →γ ∈(0,∞) for each ℓ=0,...,L.
ℓ ℓ
iid
The weights W ,...,W have entries [W ] ∼ N(0,1), independent of each other and of X.
1 L ℓ ij
Definition 1. A feature matrix X ∈Rd×n is τ -orthonormal if
n
(cid:12) (cid:12)∥x α∥ 2−1(cid:12) (cid:12)≤τ n, (cid:12) (cid:12)∥x β∥ 2−1(cid:12) (cid:12)≤τ n, (cid:12) (cid:12)x⊤ αx β(cid:12) (cid:12)≤τ
n
for all pairs α̸=β ∈[n], where {x }n are the columns of X.
α α=1
Assumption 2. For some τ >0 such that lim τ ·n1/3 =0, X ≡X is τ -orthonormal almost surely
n n→∞ n 0 n
for all large n. Furthermore, K = X⊤X has eigenvalues λ (K ),...,λ (K ) (not necessarily ordered by
0 1 0 n 0
magnitude) such that for some fixed r ≥0, as n,d→∞,
(a) There exists a compactly supported probability measure µ on [0,∞) such that
0
n
1 (cid:88)
δ →µ weakly a.s.
n−r λi(K0) 0
i=r+1
and for any fixed ε>0, almost surely for all large n,
λ (K )∈supp(µ )+(−ε,ε) for all i≥r+1.
i 0 0
(b) There exist distinct values λ ,...,λ >0 with λ ,...,λ ̸∈supp(µ ) such that
1 r 1 r 0
λ (K )→λ a.s. for each i=1,...,r.
i 0 i
Assumption 3. The activation σ :R→R is twice differentiable with sup |σ′(x)|,|σ′′(x)|≤λ for some
x∈R σ
λ ∈(0,∞). Under ξ ∼N(0,1), we have E[σ(ξ)]=0 and E[σ2(ξ)]=1. Furthermore,
σ
b :=E[σ′(ξ)]̸=0, E[σ′′(ξ)]=0. (2.5)
σ
Assumption1definesthelinear-widthasymptoticregime. Assumption2requiresanorthogonalitycondi-
tionfortheinputfeaturesthatissimilarto[FW20,Definition3.1],andalsocodifiesourspikedeigenstructure
assumptionfortheinputdata. We briefly commenton(2.5)inAssumption3: The condition b ̸=0ensures
σ
thatthelinearcomponentofσ(·)isnon-degenerate;ifb =0,thenspikedeigenstructuredoesnotpropagate
σ
across the NN layers in our studied regime of bounded spike magnitudes. The condition E[σ′′(ξ)] = 0 en-
sures that K does not have uninformative spike eigenvalues; otherwise, as shown in [BP22], K may have
ℓ ℓ
spike eigenvalues even when the input K has no spiked structure. We assume E[σ′′(ξ)] = 0 for clarity, to
0
avoidcharacterizingalsosuchuninformativespikesacrosslayers. Thisconditionholds,inparticular,forodd
activation functions σ(·) such as tanh.
The following theorem first extends [FW20, Theorem 3.4] by affirming that the weak convergence state-
ment (2.3) holds under the above assumptions, and furthermore, each K has no outlier eigenvalues outside
ℓ
its limit spectral support when the input K has no spike eigenvalues.
0
4Theorem 2. Suppose Assumptions 1, 2, and 3 hold. Then for each ℓ=1,...,L, (2.3) holds weakly a.s. as
n → ∞. Furthermore, if the number of spikes is r = 0 in Assumption 2, then for any fixed ε > 0, almost
surely for all large n,
K has no eigenvalues outside supp(µ )+(−ε,ε).
ℓ ℓ
The main result of this section characterizes the eigenvalues of K outside supp(µ ) when r ≥ 1. To
ℓ ℓ
describe this characterization, define for each ℓ=1,...,L the domain
T ={−1/λ:λ∈supp(ν )}
ℓ ℓ−1
where ν is defined by (2.4), and define z ,φ :(0,∞)\T →R by
ℓ−1 ℓ ℓ ℓ
1 (cid:90) λ sz′(s)
z (s)=− +γ ν (dλ), φ (s)=− ℓ . (2.6)
ℓ s ℓ 1+λs ℓ−1 ℓ z (s)
ℓ
It is known from the results of [BY12] and [YZB15, Chapter 11] that these are precisely the functions that
characterize the spike eigenvalues and eigenvectors in linear spiked covariance models. Set
1
I ={1,...,r}, s =− for i∈I ,
0 i,0 b2λ +(1−b2) 0
σ i σ
where λ and b are defined in Assumptions 2 and 3 respectively. Here, I records the indices of the spike
i σ 0
eigenvalues of the input Gram matrix K . Then define recursively for ℓ=1,...,L
0
(cid:110) (cid:111) 1
I = i∈I :z′(s )>0 , s =− for i∈I . (2.7)
ℓ ℓ−1 ℓ i,ℓ−1 i,ℓ b2z (s )+(1−b2) ℓ
σ ℓ i,ℓ−1 σ
Theconditionz′(s )>0describesthe“phasetransition”phenomenonforspikeeigenvaluesinthismodel,
ℓ i,ℓ−1
where spikes i ∈ I with z′(s ) > 0 induce spike eigenvalues in the CK matrix K of the next layer,
ℓ−1 ℓ i,ℓ−1 ℓ
while spikes with z′(s )≤0 are absorbed into the bulk spectrum of K .
ℓ i,ℓ−1 ℓ
Theorem 3. Suppose Assumptions 1, 2, and 3 hold. Then for each ℓ=1,...,L:
(a) s ∈(0,∞)\T for each i∈I , so z (s ) and I are well-defined. Furthermore, if i∈I (i.e.
i,ℓ−1 ℓ ℓ−1 ℓ i,ℓ−1 ℓ ℓ
if z′(s )>0) then z (s )>0 and φ (s )>0.
ℓ i,ℓ−1 ℓ i,ℓ−1 ℓ i,ℓ−1
(b) For any fixed and sufficiently small ε>0, almost surely for all large n, there is a 1-to-1 correspondence
between the eigenvalues of K outside supp(µ )+(−ε,ε) and {i:i∈I }. Denoting these eigenvalues
ℓ ℓ ℓ
of K
ℓ
by {λ(cid:98)i,ℓ :i∈I ℓ}, for each i∈I
ℓ
as n→∞,
λ(cid:98)i,ℓ →z ℓ(s i,ℓ−1) a.s.
(c) Letv
(cid:98)i,ℓ
beaunit-normeigenvectorofK
ℓ
correspondingtoitseigenvalueλ(cid:98)i,ℓ,andletv
j
beaunit-norm
eigenvector of K corresponding to its spike eigenvalue λ (K ). Then for each i ∈ I and j ∈ I , as
0 j 0 ℓ 0
n→∞,
ℓ
|v⊤v |2 → (cid:89) φ (s )·1{i=j} a.s.
(cid:98)i,ℓ j k i,k−1
k=1
Moreover, for each i∈I and any unit vector v ∈Rn independent of W ,...,W ,
ℓ 1 ℓ
ℓ
|v⊤v|2− (cid:89) φ (s )·|v⊤v|2 →0 a.s.
(cid:98)i,ℓ k i,k−1 i
k=1
We present the following corollary as a concrete example in which the assumptions of the theorem are
satisfied. The corollary encompasses, for instance, Gaussian mixture models with a fixed number r of
balanced classes, each class having Θ(n) samples.
50.05 0.05 0.05
0.04 0.04 0.04
0.03 0.03 0.03
0.02 0.02 0.02
0.01 0.01 0.01
0.00 0.00 0.00
0 2 4 6 0 2 4 6 0 2 4 6 8
eigenvalues eigenvalues eigenvalues
(a) Spectrum of K (b) Spectrum of K (c) Spectrum of K
0 1 2
Figure1: Spectraofthree-layerCKmatricesdefinedby(2.2)withn=5000,d =d =d =15000,andσ∝arctan.
0 1 2
InputdataisaGMMsatisfying(2.8)withr=3,θ =2.0,θ =1.18,andθ =1.0. (a)-(c)aretheoreticallypredicted
1 2 3
(red) and empirical (blue) bulk distributions and spikes of K for ℓ=0,1,2.
ℓ
Corollary 4. Suppose the input data X is itself a low-rank signal-plus-noise matrix
r
(cid:88)
X = θ a b⊤+Z ∈Rd×n (2.8)
i i i
i=1
where θ ,...,θ >0 are fixed distinct signal strengths, a ,...,a ∈Rd and b ,...,b ∈Rn are orthonormal
1 r 1 r 1 r
sets of unit vectors, and Z has i.i.d. N(0,1/d) entries. Assume that b ,...,b satisfy the ℓ -delocalization
1 r ∞
condition: for any sufficiently small ε>0 and all large n,
mr ax∥b ∥ <n−1/2+ε.
i ∞
i=1
Define φ (·) and s by (2.6) and (2.7), with the initial measures µ = ρMP and ν = b2 ⊗µ ⊕(1−b2)
ℓ i,ℓ−1 0 γ0 0 σ 0 σ
and initial spike values λ =(1+θ2)(γ +θ2)/θ2 for i∈I .
i i 0 i i 0
Then for each ℓ=1,...,L, K has a spike eigenvalue corresponding to the input signal component θ if
ℓ i
and only if θ >γ1/4 and i∈I . In this case, its corresponding unit eigenvector v satisfies, as n→∞,
i 0 ℓ (cid:98)i,ℓ
|v⊤b |2 →
(cid:89)ℓ
φ (s
)·(cid:18)
1−
γ 0(1+θ i2) (cid:19)
a.s. (2.9)
(cid:98)i,ℓ i k i,k−1 θ2(θ2+γ )
k=1 i i 0
Numerical illustration. A simple illustration of this result for a 3-component Gaussian mixture model
is provided in Figure 1. We note that I ⊆ ··· ⊆ I and φ (s ) ∈ (0,1), so the number of spike
L 0 ℓ i,ℓ−1
eigenvaluesofK inducedbyK andthealignmentofthespikeeigenvectorsofK withthetrueclasslabel
ℓ 0 ℓ
vectors {b }r are both non-increasing in the network depth, see also Figure 2. In other words, at random
i i=1
initialization, the input signal diminishes as the depth of the NN increases.
In Figure 2 we highlight two remedies to this “curse of depth” at random initialization.
• In Figure 2(a) we observe that when the width of NN becomes larger, alignment between the leading
eigenvector of K at random initialization and the signal can be preserved across a larger depth. This
ℓ
illustrates the benefit of overparameterization by increasing the network width.
• In Figure 2(b) we observe that gradient descent training on the weight matrices also restores and even
amplifies the informative signal in the CK matrix of each layer; specifically, after 50 steps of GD training
(yellow curve), the alignment between the class labels and the leading eigenvector of K may increase
ℓ
through depth. This demonstrates the benefit of gradient-based feature learning. In Section 2.2 we
precisely quantify this improved alignment due to gradient descent in a simplified two-layer setting.
6
ycneuqerf ycneuqerf ycneuqerf0.8 N=2048
N=4096 0.8
N=8192
0.6 N=10240 0.6
0.4 0.4
theoretical
t=0
0.2 0.2 t=10
t=20
t=50
0.0 0.0
2 4 6 8 0 2 4 6
depth layer depth
(a) Effect of width on alignment. (b) Effect of GD training.
Figure 2: We consider multiple-layer NNs in (2.1) with σ ∝ tanh on Gaussian mixture data (2.8) for r = 1, and
computethealignmentbetweenthelargesteigenvectoroftheCKmatrixK withgenuinesignalb (classlabels)for
ℓ 1
different layer ℓ. (a) NNs at random initialization with varying hidden widths N =2048,4096,8192,10240. (b) NNs
trainedbygradientdescentwithlearningrateη=0.1forvaryingstepsT =0,10,20,50;weusetheµ-parameterization
[YH20] to encourage feature learning. θ is 2.5 and 1.8 for (a) and (b), respectively. Dots are empirical values (over
1
10 runs) and solid curves represent theoretical predictions at random initialization from Theorem 3.
2.2 CK matrix after O(1) steps of gradient descent
TheprecedingsectionstudiedthespikeeigenstructureoftheCKinducedbylow-rankstructureintheinput
data. Here, focusing on a two-layer model, we study an alternative setting where spiked structure arises
instead in the weight matrix W from gradient descent training.
We consider an early training regime studied in [BES+22], with a width-N two-layer feedforward NN,
N
1 (cid:88) 1
f (x)= √ a σ(⟨x,w ⟩)= √ σ(x⊤W)a. (2.10)
NN i i
N N
i=1
Here x∈Rd is the input, and W =[w ,...,w ]∈Rd×N and a∈RN are the network weights. For clarity
1 N
of the subsequent discussion, we will transpose the notation for X and W from the preceding section, and
√
incorporate a 1/ d scaling into W rather than into the input data X.
Given are an input feature matrix X = [x ,...,x ]⊤ ∈ Rn×d and labels y ∈ Rn for n samples, where
1 n
y =f (x )+noise. We consider the training of first-layer weights W to minimize the mean squared error
i ∗ i
n
1 (cid:88)
L(W)= (f (x )−y )2,
2n NN i i
i=1
fixing the second-layer weight vector a. From a random initialization W ∈ Rd×N, and over T steps with
√ 0
learning rates η ,...,η scaled by N, the gradient descent (GD) updates take the form
1 T
√
W =W +η N ·G , G =−∇L(W ). (2.11)
t+1 t t+1 t t t
Of interest is the information about the label function f that is learned by W ≡W , which may be
∗ trained T
characterized by the spectral alignment of the CK matrix with the class label vector on independent test
data (X˜,y˜). This use of independent test data may be understood as a pre-training setup, also considered
previously in [BES+22, MLHD23] and studied for real-world data in [WHS22].
Itwasshownin[BES+22]thatinatrainingregimewithinitialization∥W ∥≍1suchthat|f (x )|≪1
0 NN i
for each i = 1,...,N, and with learning rates η ,...,η ≍ 1 for a fixed number T of GD steps, the weight
1 T
matrix W undergoes a change during training that is O(1) in operator norm and approximately rank-1,
T
W ≈W + ηb σX⊤ya⊤ where η =(cid:88) η .
trained 0 n t
t=1
7
2
1v,1b
tnemngila
2
v,1b
tnemngila0.12 empirical histogram n=512
theoretical prediction 0.5 n=1024
0.10
n=2048
0.4 n=4096
0.08
0.3
0.06
top eigenvector u 0.2
0.04
0.1
0.02
0.0
0.00
0 2 4 6 8 0.5 1.0 1.5 2.0 2.5 3.0
eigenvalues learning rate × steps: t
(a) Spectrum of the updated CK. (b) Eigenvector alignment of the updated CK.
Figure 3: (a) We set n=2000,d=1600,N =2400,η·t=2, and σ=σ =erf. (b) We set d=2048,N =1024,η=
∗
0.2, σ = tanh,σ = SoftPlus, and vary the sample size n and number of GD steps t; dots represent empirical
∗
simulations (over 10 runs) and solid curves are theoretical predictions from Theorem 5.
Moreover, [BES+22, Conjecture 4] conjectured that for the CK matrix
1
K = σ(X˜W )σ(X˜W )⊤ ∈Rn×n (2.12)
N trained trained
defined by the pre-trained weights and test data X˜, the resulting spike eigenvalue and the alignment of its
spike eigenvector with the test labels y˜ ∈Rn are accurately predicted by a Gaussian equivalent model. Our
main result of this section is an affirmative verification of this conjecture and precise characterization of the
spike eigenstructure of K, in the following representative setting.
Assumption 4. For a two-layer NN in (2.10) with GD training defined by (2.11), we assume that
(a) n,d,N →∞ such that N/d→γ ∈(0,∞) and N/n→γ ∈(0,∞).
0 1
(b) TrainingfeaturesX =[x ,...,x ]⊤ ∈Rn×d haveentries[X] i ∼id N(0,1),traininglabelsy ∈Rn have
1 n ij
entries y = σ (β⊤x )+ε where β ∈ Rd is a deterministic unit vector and ε i ∼id N(0,σ2), and test
i ∗ ∗ i i ∗ i ε
data (X˜,y˜) is an independent copy of (X,y).
(c) The NN activation σ : R → R and label function σ : R → R both satisfy Assumption 3, with
∗
b :=E[σ′(ξ)]̸=0 and b :=E[σ′(ξ)]̸=0.
σ σ∗ ∗
i.i.d. i.i.d.
(d) The weight initializations satisfy [W ] ∼ N(0,1/d) and a ∼ N(0,1/N).
0 ij j
(e) The number of iterations T and learning rates η ,...,η are fixed independently of n,d,N.
1 T
Under these assumptions, the following theorem characterizes the spike eigenvalue of the CK matrix and
the alignment between the corresponding eigenvector and the test labels, as a function of the learning rate
η and the number of gradient descent steps T.
t
Theorem 5. Suppose that Assumption 4 holds, and set η
=(cid:80)T
η . Define
t=1 t
(cid:113)
θ =b η· (γ /γ )(1+σ2)+b2 , θ =b b η. (2.13)
1 σ 1 0 ε σ∗ 2 σ σ∗
Let z (·) and φ (·) be defined by (2.6) with γ and ν =b2 ⊗ρMP⊕(1−b2), and set
1 1 1 0 σ γ0 σ
(1+θ2)(γ +θ2)
λ =b2 1 0 1 +1−b2.
1 σ θ2 σ
1
Then K defined by (2.12) has a spike eigenvalue if and only if θ > γ1/4 and z′(−1/λ ) > 0. In this case,
1 0 1
λ (K)→γ−1z(−1/λ ) a.s., and the leading unit eigenvector u∈Rn of K satisfies
max 1 1 (cid:98)
(cid:112) (cid:112)
1 z(−1/λ )φ(−1/λ ) θ (θ4−γ )(γ +θ2)
√ |y˜⊤u|→b b 1 1 · 2 1 0 0 1 >0 a.s. (2.14)
n (cid:98) σ σ∗ λ θ3
1 1
8
ycneuqerf y
lebal
2u,y
1 n
tnemngilaNumerical illustration. Figure 3 empirically validates the predictions of Theorem 5, for a two-layer
NN trained with a small number of GD steps. Figure 3(a) shows that one spike eigenvalue emerges over
training in the test-data CK, the location of which is accurately predicted by Theorem 5; moreover, the
leading eigenvector u aligns with the labels y˜. This is quantified in Figure 3(b), where above a phase
(cid:98)
transition threshold, the alignment ⟨u,y˜⟩2 (predicted by (2.14)) increases with the learning rate or number
(cid:98)
of GD steps; in addition, alignment also increases with the training set size n. Compared with random
initialization (η = 0), this illustrates that training improves the NN representation, and the test-data CK
contains information on the label function f .
∗
3 Analysis of a nonlinear spiked covariance model
The results of Sections 2.1 and 2.2 rest on an analysis of spiked eigenstructure in a general nonlinear spiked
covariancemodel. Wedescribetheassumptionsandstatementofthisgeneralresultinformallyhere,deferring
formal and more quantitative statements to Appendix B.
Let G = √1 [g ,...,g ]⊤ ∈ RN×n have independent rows g ,...,g ∈ Rn with mean 0 and common
N 1 N 1 N
covariance Σ ∈ Rn×n. We assume that the law of g satisfies concentration of quadratic forms g⊤Ag ,
i i j
but has otherwise arbitrary dependence across coordinates. As n,N → ∞ with n/N → γ ∈ (0,∞), the
eigenvalues of Σ satisfy
n
1 (cid:88)
λ (Σ)→λ for i=1,...,r, δ →ν weakly,
i i n−r λi(Σ)
i=r+1
for fixed spike values λ ,...,λ > 0 and a deterministic limit spectral law ν. Then the empirical spectral
1 r
law of the sample covariance matrix K =G⊤G satisfies
n
1 (cid:88)
δ →µ=ρMP⊠ν weakly a.s.
n λi(K) γ
i=1
Under these assumptions, let us define
1 (cid:90) λ z′(s)
z(s)=− +γ dν(λ), φ(s)= .
m 1+λs (−1/s)z(s)
Theorem 6 (informal).
(a) If r = 0, then all eigenvalues of K converge to supp(µ)∪{0}. More generally for r ≥ 0, the eigen-
values of K asymptotically separated from supp(µ)∪{0} are in 1-to-1 correspondence with I = {i :
z′(−1/λ i)>0}, and λ(cid:98)i(K)→z(−1/λ i).
(b) For each i ∈ I and any deterministic unit vector v ∈ Rn, (v⊤v )2 −φ(−1/λ )(v⊤v )2 → 0, where
(cid:98)i i i
v i,v
(cid:98)i
are the unit eigenvectors of Σ,K for eigenvalues λ i(Σ),λ(cid:98)i(K).
(c) Let u = √1 N(u 1,...,u N)⊤ ∈ RN be such that [u,G] ∈ RN×(n+1) has i.i.d. rows {[u j,g j]}N j=1, and
denote E[ug]=E[u g ] for all j ∈[N]. Then for each i∈I,
j j
z(−1/λ )φ(−1/λ )
(u⊤u )2− i i (E[ug]⊤v )2 →0
(cid:98)i λ2 i
i
where u
(cid:98)i
is the unit eigenvector of GG⊤ for its eigenvalue λ(cid:98)i(GG⊤)=λ(cid:98)i(K).
Statements (a–b) are known in a linear setting g = Σ1/2z when z has i.i.d. entries, see e.g. [BY12]
i i i
and [YZB15, Theorems 11.3 and 11.5]. The above theorem thus verifies an exact asymptotic equivalence
between spiked spectral phenomena in a nonlinear spiked covariance model with those of a linearly defined
(possibly Gaussian) model.
In Section 2.1, each CK matrix K has (approximately) the structure of the above matrix K over the
ℓ
randomness of W , conditional on the features X of the preceding layer, and Theorem 3 follows from
ℓ ℓ−1
Theorem 6(a,b). In Section 2.2, the CK matrix K defined by trained weights has (approximately) this
structure over the randomness of X˜, conditional on W , and Theorem 5 follows from Theorem 6(a,c).
trained
9Proof ideas. Analyses in the linearly defined model g = Σ1/2z commonly stem from block matrix
i i
inversion identities with respect to the block decompositions
(cid:18) (cid:19)
Σ= Σ r 0 , G=(cid:0) G G (cid:1)
0 Σ r 0
0
where Σ contains the spike eigenvalues of Σ, and G is independent of G . This independence does not
r r 0
hold in our setting, and we develop a different “master equation” approach.
Let λ(cid:98)1/2 be a spike singular value of G with corresponding unit singular vectors (u (cid:98),v (cid:98)). We consider the
linearized equation
0=(cid:18)
−λ(cid:98)I
G⊤(cid:19)(cid:18)
v (cid:98)
(cid:19)
. (3.1)
G −I λ(cid:98)1/2u
(cid:98)
Writing V ∈Rn×r for the r spike eigenvectors of Σ, we define a generalized resolvent
r
(cid:18) −zI−αV V⊤ G⊤(cid:19)−1
R(z,α)= r r ,
G −I
(cid:18) (cid:19)
V
add to (3.1) the quantity −α r ·V⊤v on both sides for some large α>0, and rewrite this as
0 r (cid:98)
(cid:18) (cid:19) (cid:18) (cid:19)
v V
λ(cid:98)1/(cid:98)
2u
(cid:98)
=−αR(λ(cid:98),α) 0r ·V⊤
r
v (cid:98). (3.2)
We will show that R(z,α) exists and is bounded in operator norm for any z separated from the limit bulk
spectralsupportofK andanylargeenoughα>0. Then,multiplying(3.2)by(V⊤ 0)andapplyingablock
r
matrix inversion identity,
(cid:18) V (cid:19)⊤ (cid:18) V (cid:19) (cid:16) (cid:17)−1
V⊤
r
v
(cid:98)
=−α 0r R(λ(cid:98),α) 0r ·V⊤
r
v
(cid:98)
=−αV⊤
r
G⊤G−λ(cid:98)I−αV rV⊤
r
V r·V⊤
r
v (cid:98).
As a result, spike eigenvalues λ(cid:98) are roots z =λ(cid:98) of the master equation
(cid:18) (cid:16) (cid:17)−1 (cid:19)
det I +αV⊤ G⊤G−zI−αV V⊤ V =0,
r r r r r
for any fixed and large α>0. Singular vector alignments may be characterized likewise from (3.2).
The core of the proof is an asymptotic analysis of this master equation via a deterministic equivalent
approximation
v⊤R(Γ)v :=v⊤(G⊤G−Γ)−1v ≈−v⊤(Γ+zm˜(z)Σ)−1v (3.3)
1 2 1 2 1 2
foranydeterministicunitvectorsv ,v ∈Rn andlow-rankperturbationsΓofzI,wherem˜(z)istheStieltjes
1 2
transform of the “companion” limit measure µ˜ for the eigenvalue distribution of GG⊤ ∈RN×N. We extend
results of [Cho22, SCDL23] by establishing this approximation not only for Γ = zI but also perturbations
thereof, and for spectral arguments z ∈ C\supp(µ) that may belong to the positive real line. The latter
extensionrequiresshowing, apriori, thatalleigenvaluesofK =G⊤Gfallclosetosupp(µ)intheabsenceof
spiked structure. We show this by adapting an argument of [BS98] and using a fluctuation averaging lemma
described below.
Let us conclude with a brief discussion of our proof of (3.3): From manipulations of the identity
N
1 (cid:88)
TrB =Tr(G⊤G−Γ)R(Γ)B =−TrR(Γ)BΓ+ g⊤R(Γ)Bg
N i i
i=1
for appropriately chosen matrices B ∈ Cn×n, the Sherman-Morrison (leave-one-out) formula for matrix
inversion applied to R(Γ), and the concentration of bilinear forms in g , one may show
i
N
1 1 (cid:88)
v⊤(Γ+zm˜(z)Σ)−1v ≈−v⊤R(Γ)v + · (1−E )T (3.4)
1 2 1 2 1+N−1TrΣR(Γ) N g i i
i=1
10where T =g⊤R(i)(Γ)v ·v⊤(Γ+zm(i)(Γ)Σ)−1g . Here, R(i)(Γ) and m(i)(Γ) are generalized leave-one-out
i i 2 1 K˜ i K˜
resolvents and empirical Stieltjes transforms defined by {g } , and E is the partial expectation over
only g . Under our assumptions for g , each error term
(1j −j̸= Ei
)T
hag si
mean 0 and O(1) fluctuations.
i i g i i
We develop a fluctuation averaging lemma using recursive applications of the Sherman-Morrison identity to
further resolve the dependence of R(i)(Γ) and m(i)(Γ) on fixed subsets of rows {g } , to show that the
K˜ j j̸=i
errors (1−E )T are weakly correlated across i∈[N]. Hence their average has a mean 0 and fluctuates on
g i
i
the asymptotically negligible scale of O(N−1/2), and applying this to (3.4) shows (3.3).
Acknowledgements
This research was supported in part by NSF DMS-2142476, NSF DMS-2055340 and NSF DMS-2154099.
References
[AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase prop-
erty: anecessaryandnearlysufficientconditionforsgdlearningofsparsefunctionsontwo-layer
neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR, 2022.
[ADH+19] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Interna-
tional Conference on Machine Learning, pages 322–332. PMLR, 2019.
[AP20] Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple
descent and a multi-scale theory of generalization. In International Conference on Machine
Learning, pages 74–84. PMLR, 2020.
[BAGHJ23] GerardBenArous,RezaGheissari,JiaoyangHuang,andAukoshJagannath. High-dimensional
sgd aligns with emerging outlier eigenspaces. arXiv preprint arXiv:2310.03010, 2023.
[BAGJ23] G´erard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems
for sgd: Effective dynamics and critical scaling. Communications on Pure and Applied Math-
ematics, 2023.
[BAP05] Jinho Baik, G´erard Ben Arous, and Sandrine P´ech´e. Phase transition of the largest eigenvalue
for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643–1697,
2005.
[BCP20] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning
curves in kernel regression and wide neural networks. In International Conference on Machine
Learning, pages 1024–1034. PMLR, 2020.
[BEK+14] Alex Bloemendal, L´aszl´o Erd˝os, Antti Knowles, Horng-Tzer Yau, and Jun Yin. Isotropic local
lawsforsamplecovarianceandgeneralizedWignermatrices. Electronic Journal of Probability,
19(none):1 – 53, 2014.
[BES+22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-
dimensional asymptotics of feature learning: How one gradient step improves the representa-
tion. arXiv preprint arXiv:2205.01445, 2022.
[BES+23] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the
presence of low-dimensional structure: a spiked random matrix perspective. In Thirty-seventh
Conference on Neural Information Processing Systems (NeurIPS 2023), 2023.
[BGN11] Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite,
low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494–521,
2011.
[BGN12] Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low
rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis,
111:120–135, 2012.
11[BKYY16] AlexBloemendal,AnttiKnowles,Horng-TzerYau,andJunYin. Ontheprincipalcomponents
of sample covariance matrices. Probability theory and related fields, 164(1-2):459–552, 2016.
[BP22] LucasBenigniandSandrineP´ech´e. Largesteigenvaluesoftheconjugatekernelofsingle-layered
neural networks. arXiv preprint arXiv:2201.04753, 2022.
[BPH23] DavidBosch, AshkanPanahi, andBabakHassibi. Preciseasymptoticanalysisofdeeprandom
feature models. arXiv preprint arXiv:2302.06210, 2023.
[BS98] Zhi-DongBaiandJackWSilverstein. Noeigenvaluesoutsidethesupportofthelimitingspec-
tral distribution of large-dimensional sample covariance matrices. The Annals of Probability,
26(1):316–345, 1998.
[BS06] Jinho Baik and Jack W Silverstein. Eigenvalues of large sample covariance matrices of spiked
population models. Journal of multivariate analysis, 97(6):1382–1408, 2006.
[BX22] Zhigang Bao and Xiaocong Xu. Extreme eigenvalues of log-concave ensemble. arXiv preprint
arXiv:2212.11634, 2022.
[BY12] Zhidong Bai and Jianfeng Yao. On sample eigenvalues in a generalized spiked population
model. Journal of Multivariate Analysis, 106:167–177, 2012.
[Cap13] Mireille Capitaine. Additive/multiplicative free subordination property and limiting eigenvec-
torsofspikedadditivedeformationsofwignermatricesandspikedsamplecovariancematrices.
Journal of Theoretical Probability, 26:595–648, 2013.
[Cap18] Mireille Capitaine. Limiting eigenvectors of outliers for spiked information-plus-noise type
matrices. In S´eminaire de Probabilit´es XLIX, pages 119–164. Springer, 2018.
[Cho22] Cl´ement Chouard. Quantitative deterministic equivalent of sample covariance matrices with a
general dependence structure. arXiv preprint arXiv:2211.13044, 2022.
[Cho23] Cl´ement Chouard. Deterministic equivalent of the conjugate kernel matrix associated to arti-
ficial neural networks. arXiv preprint arXiv:2306.05850, 2023.
[CS09] Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in neural
information processing systems, 22, 2009.
[CS13] Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices.
Random Matrices: Theory and Applications, 2(04):1350010, 2013.
[CT18] Djalil Chafa¨ı and Konstantin Tikhomirov. On the convergence of the extremal eigenvalues of
empirical covariance matrices with dependence. Probability Theory and Related Fields, 170(3-
4):847–889, 2018.
[DK70] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation.
iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.
[DKL+23] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning
two-layer neural networks, one (giant) step at a time. arXiv preprint arXiv:2305.18270, 2023.
[DLMY23] Sofiia Dubova, Yue M Lu, Benjamin McKenna, and Horng-Tzer Yau. Universality for the
global spectrum of random inner-product kernel matrices in the polynomial regime. arXiv
preprint arXiv:2310.18280, 2023.
[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn repre-
sentationswithgradientdescent. InConference on Learning Theory,pages5413–5452.PMLR,
2022.
[EK10] Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics,
38(1):1–50, 2010.
[EKY13] L´aszl´o Erd˝os, Antti Knowles, and Horng-Tzer Yau. Averaging fluctuations in resolvents of
random band matrices. In Annales Henri Poincar´e, volume 14, pages 1837–1926. Springer,
2013.
12[FDP+20] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M
Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss
landscape geometry and the time evolution of the neural tangent kernel. Advances in Neural
Information Processing Systems, 33:5850–5861, 2020.
[Fel23] Michael J Feldman. Spectral properties of elementwise-transformed spiked matrices. arXiv
preprint arXiv:2311.02040, 2023.
[FJ22] Zhou Fan and Iain M Johnstone. Tracy-widom at each edge of real covariance and manova
estimators.Theannalsofappliedprobability: anofficialjournaloftheInstituteofMathematical
Statistics, 32(4):2967, 2022.
[FM19] ZhouFanandAndreaMontanari. Thespectralnormofrandominner-productkernelmatrices.
Probability Theory and Related Fields, 173(1-2):27–85, 2019.
[FW20] Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for
linear-width neural networks. Advances in neural information processing systems, 33:7710–
7721, 2020.
[GKK+23] Alice Guionnet, Justin Ko, Florent Krzakala, Pierre Mergny, and Lenka Zdeborov´a. Spectral
phase transitions in non-linear wigner spiked models. arXiv preprint arXiv:2310.14055, 2023.
[GLR+21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M´ezard, and Lenka
Zdeborov´a. The gaussian equivalence of generative models for learning with shallow neural
networks. Proceedings of Machine Learning Research vol, 145:1–46, 2021.
[GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of
lazytrainingoftwo-layersneuralnetwork.AdvancesinNeuralInformationProcessingSystems,
32, 2019.
[GMMM20] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? Advances in Neural Information Processing Systems,
33:14820–14830, 2020.
[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-
layers neural networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021.
[HJ22] Hamed Hassani and Adel Javanmard. The curse of overparametrization in adversarial train-
ing: Precise analysis of robust generalization for random features regression. arXiv preprint
arXiv:2201.05149, 2022.
[HL20] HongHuandYueMLu. Universalitylawsforhigh-dimensionallearningwithrandomfeatures.
arXiv preprint arXiv:2009.07669, 2020.
[HTFF09] TrevorHastie,RobertTibshirani,JeromeHFriedman,andJeromeHFriedman. The elements
of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.
[JGH18] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalizationinneuralnetworks. InAdvances inneural information processingsystems,pages
8571–8580, 2018.
[JNG+19] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short
note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint
arXiv:1902.03736, 2019.
[Joh01] Iain M Johnstone. On the distribution of the largest eigenvalue in principal components anal-
ysis. The Annals of statistics, 29(2):295–327, 2001.
[KO20] Ryo Karakida and Kazuki Osawa. Understanding approximate fisher information for fast con-
vergence of natural gradient descent in wide neural networks. Advances in Neural Information
Processing Systems, 33, 2020.
[KY17] Antti Knowles and Jun Yin. Anisotropic local laws for random matrices. Probability Theory
and Related Fields, 169:257–352, 2017.
13[LBN+17] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165, 2017.
[LGC+21] BrunoLoureiro,CedricGerbelot,HugoCui,SebastianGoldt,FlorentKrzakala,MarcMezard,
and Lenka Zdeborov´a. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Advances in Neural Information Processing Systems, 34, 2021.
[LLC18] Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural
networks. The Annals of Applied Probability, 28(2):1190–1248, 2018.
[LNR22] Mufan Li, Mihai Nica, and Dan Roy. The neural covariance sde: Shaped infinite depth-
and-width networks at initialization. Advances in Neural Information Processing Systems,
35:10795–10808, 2022.
[LV07] John A Lee and Michel Verleysen. Nonlinear dimensionality reduction, volume 1. Springer,
2007.
[LY22] Yue M Lu and Horng-Tzer Yau. An equivalence principle for the spectrum of random inner-
product kernel matrices. arXiv preprint arXiv:2205.06308, 2022.
[MBD+21] James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard,
Jascha Sohl-Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks
without skip connections or normalization layers using deep kernel shaping. arXiv preprint
arXiv:2110.01765, 2021.
[MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A. Erdogdu. Gradient-based
feature learning under structured data. In Thirty-seventh Conference on Neural Information
Processing Systems (NeurIPS 2023), 2023.
[MLHD23] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban. A theory of non-
linear feature learning with one gradient step in two-layer neural networks. arXiv preprint
arXiv:2310.07891, 2023.
[MM22] Song Mei and Andrea Montanari. The generalization error of random features regression:
Precise asymptotics and the double descent curve. Communications on Pure and Applied
Mathematics, 75(4):667–766, 2022.
[MP67] V.A. Marˇcenko and Leonid Pastur. Distribution of eigenvalues for some sets of random matri-
ces. Math USSR Sb, 1:457–483, 01 1967.
[MS22] AndreaMontanariandBasilSaeed. Universalityofempiricalriskminimization. arXivpreprint
arXiv:2202.08832, 2022.
[MZ20] Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks:
Memorizationandgeneralizationunderlazytraining.arXivpreprintarXiv:2007.12826v1,2020.
[Nea95] Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &
Business Media, 1995.
[OJMDF21] Guillermo Ortiz-Jim´enez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can
linearizedneuralnetworksactuallysayaboutgeneralization? Advances in Neural Information
Processing Systems, 34, 2021.
[Pau07] DebashisPaul. Asymptoticsofsampleeigenstructureforalargedimensionalspikedcovariance
model. Statistica Sinica, pages 1617–1642, 2007.
[P´ec06] Sandrine P´ech´e. The largest eigenvalue of small rank perturbations of hermitian random
matrices. Probability Theory and Related Fields, 134:127–173, 2006.
[P´ec19] S P´ech´e. A note on the pennington-worah distribution. Electronic Communications in Proba-
bility, 24:1–7, 2019.
[PW17] Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In
Advances in Neural Information Processing Systems, pages 2637–2646, 2017.
14[PZA+21] Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The
intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894,
2021.
[RGKZ21] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov´a. Classifying high-
dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In
International Conference on Machine Learning, pages 8936–8947. PMLR, 2021.
[SB95] Jack W Silverstein and ZD Bai. On the empirical distribution of eigenvalues of a class of large
dimensional random matrices. Journal of Multivariate analysis, 54(2):175–192, 1995.
[SC95] Jack W Silverstein and Sang-Il Choi. Analysis of the limiting spectral distribution of large
dimensional random matrices. Journal of Multivariate Analysis, 54(2):295–309, 1995.
[SCDL23] Dominik Schr¨oder, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent
and error universality of deep random features learning. arXiv preprint arXiv:2302.00401,
2023.
[Sil95] Jack W Silverstein. Strong convergence of the empirical distribution of eigenvalues of large
dimensional random matrices. Journal of Multivariate Analysis, 55(2):331–339, 1995.
[SV13] Nikhil Srivastava and Roman Vershynin. Covariance estimation for distributions with 2 + ε
moments. The Annals of Probability, 41(5):3081–3111, 2013.
[TAP21] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate shift in high-dimensional
random feature regression. arXiv preprint arXiv:2111.08234, 2021.
[Ver10] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027, 2010.
[WES+23] Zhichao Wang, Andrew William Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang.
Spectralevolutionandinvarianceinlinear-widthneuralnetworks.InThirty-seventhConference
on Neural Information Processing Systems, 2023.
[WHS22] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models pre-
dicthowreal-worldneuralrepresentationsgeneralize. InInternational Conference on Machine
Learning, pages 23549–23588. PMLR, 2022.
[WLLM19] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization
and optimization of neural nets vs their induced kernel. In Advances in Neural Information
Processing Systems, pages 9712–9724, 2019.
[WZ21] ZhichaoWangandYizheZhu. Deformedsemicirclelawandconcentrationofnonlinearrandom
matrices for ultra-wide neural networks. arXiv preprint arXiv:2109.09304, 2021.
[XBSD+18] LechaoXiao,YasamanBahri,JaschaSohl-Dickstein,SamuelSchoenholz,andJeffreyPenning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutionalneuralnetworks. InInternational Conference on Machine Learning,pages5393–
5402. PMLR, 2018.
[XHM+22] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington. Precise learn-
ing curves and higher-order scalings for dot-product kernel regression. Advances in Neural
Information Processing Systems, 35:4558–4570, 2022.
[Yas16] Pavel Yaskov. Controlling the least eigenvalue of a random Gram matrix. Linear Algebra and
its Applications, 504:108–123, 2016.
[YH20] GregYangandEdwardJHu.Featurelearningininfinite-widthneuralnetworks.arXivpreprint
arXiv:2011.14522, 2020.
[YZB15] Jianfeng Yao, Shurong Zheng, and Zhidong Bai. Large sample covariance matrices and high-
dimensional data analysis. Cambridge UP, New York, 2015.
[ZNB22] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature
regression. Advances in Neural Information Processing Systems, 35:9813–9827, 2022.
15Organization of the Appendices
• Appendix A introduces relevant notation and background.
• Appendix B states our main results for the general nonlinear spiked covariance model
K =G⊤G,
formalizing the discussion in Section 3. These results are divided into two subsections: Appendix B.1
gives a “no outliers” statement for K and a deterministic equivalent approximation for its resolvent,
under minimal asymptotic assumptions. Appendix B.2 then states the main characterizations of spike
eigenvalues/eigenvectors in an asymptotic setting with a spiked eigenstructure.
• Appendix C develops a general fluctuation averaging lemma for the sample covariance model, and proves
the results of Appendix B.1.
• Appendix D proves the results of Appendix B.2.
• Finally, Appendix E proves the results of Section 2.1 on propagation of spiked eigenstructure through the
layers of a neural network, and Appendix F proves the results of Section 2.2 on the eigenstructure of the
CK after gradient descent training.
A Notations and background
A.1 Stochastic domination
We use the following standard notation for stochastic domination of random variables, see e.g. [EKY13,
Definition 2.4]: For random variables X ≡ X(u) and Y ≡ Y(u) ≥ 0 depending implicitly on N and a
parameter u∈U , as N →∞, we write
N
X ≺Y or X =O (Y) uniformly over u∈U
≺ N
if, for any fixed ε,D >0 and all large N,
(cid:104) (cid:105)
sup P |X(u)|>NεY(u) <N−D.
u∈UN
Throughout, “for all large N” means for all N ≥ N where N may depend on ε,D, any quantities that
0 0
are constant in the context of the statement, and convergence rates of the spike eigenvalues and empirical
spectral measures in the given assumptions.
If X =1{E} is the indicator of an event E ≡E , then 1{E}≺0 means P[E]<N−D for any fixed D >0
N
and all large N. If X and Y are both deterministic, then X ≺ Y means |X| ≤ NεY (deterministically) for
any ε>0 and all large N. For an event E ≡E , we will write
N
X =OE(Y)
≺
as shorthand for X·1{E}≺Y.
We will use the following basic properties often implicitly.
Proposition 7. Suppose X ≺Y uniformly over u∈U .
N
(a) If |U |≤NC for a constant C >0, then for any fixed ε,D >0 and all large N,
N
(cid:104) (cid:105)
P there exists u∈U with |X(u)|≥NεY(u) ≤N−D.
N
(b) If |U |≤NC for a constant C >0, then (cid:80) X(u)≺(cid:80) Y(u).
N u∈UN u∈UN
16(cid:81) (cid:81)
(c) If |U |≤C for a constant C >0, then X(u)≺ Y(u).
N u∈UN u∈UN
(d) If Y is deterministic, and E[X2] ≤ NC and Y ≥ N−C for a constant C > 0, then also E[|X|] ≺ Y
uniformly over u∈U .
N
Proof. ThefirstthreestatementsfollowfromaunionboundoverU . Forthelaststatement, foranyfixed
N
ε>0, observe that
(cid:104) (cid:105)
E|X|≤Nε/2Y +E |X|1{|X|>Nε/2Y} ≤Nε/2Y +E[X2]1/2P[|X|>Nε/2Y]1/2.
Applying E[X2]≤NC, Y ≥N−C, and P[|X|>Nε/2Y]<N−D for sufficiently large D >0 shows that the
second term is less than Nε/2Y for all large N, hence E|X|<NεY.
A.2 Deformed Marcenko-Pastur law
Foraprobabilitymeasureν supportedon[0,∞)andanaspectratioparameterγ >0,considerthedeformed
Marcenko–Pastur measure
µ=ρMP⊠ν
γ
and its “companion” probability measure
µ˜ =γµ+(1−γ)δ .
0
Here, µ and µ˜ represent the limit eigenvalue distributions of G⊤G∈Rn×n and GG⊤ ∈RN×N respectively,
when G = √1 [g ,...,g ] ∈ RN×n has i.i.d. rows with mean 0 and covariance Σ, and n,N → ∞ with
N 1 N
n/N →γ and 1 (cid:80)n δ →ν weakly.
n i=1 λi(Σ)
These measures µ,µ˜ may be defined by their Stieltjes transform
(cid:90) 1 (cid:90) 1
m(z)= dµ(x), m˜(z)= dµ˜(x)
x−z x−z
where m˜(z) = γm(z)+(1−γ)(−1/z). By the results of [MP67, SB95], for any z ∈ C+, m(z) and m˜(z)
are the unique roots in {m∈C:γm+(1−γ)(−1/z)∈C+} and C+, respectively, to the Marcenko-Pastur
equations
(cid:90) 1 1 (cid:90) λ
m(z)= dν(λ), z =− +γ dν(λ). (A.1)
λ(1−γ−γzm(z))−z m˜(z) 1+λm˜(z)
We define m(z),m˜(z) via (A.2) also on the full domains C\supp(µ) and C\supp(µ˜) respectively, where the
support sets supp(µ) and supp(µ˜) may differ only at the single point {0}.
In the setting Σ=I (and ν =δ ), the law µ=ρMP is the standard Marcenko-Pastur law, with explicit
1 γ
density function with respect to Lebesgue measure
(cid:112)
1 (λ −λ)(λ−λ ) √
dρMP(λ)= + − ·1 dλ, λ :=(1± γ)2
γ 2π γλ λ∈[λ−,λ+] ±
for γ ≤1, and an additional point mass (1−1/γ) at 0 when γ >1.
Ingeneral,µandµ˜donothaveanalyticallyexplicitdensities. However,supp(µ˜)isexplicitlycharacterized
in [SC95], and we review this characterization here: Define
T ={0}∪{−1/λ:λ∈supp(ν)}. (A.2)
For m˜ ∈C\T, define
1 (cid:90) λ
z(m˜)=− +γ dν(λ). (A.3)
m˜ 1+λm˜
In light of the second equation of (A.1), this may be understood as a formal inverse of m˜(z). From [SC95,
Theorems 4.1 and 4.2], we have the following properties.
Proposition 8. m˜(·) defines a bijection from {z ∈R\supp(µ˜)} to {m˜ ∈R\T :z′(m˜)>0}, whose inverse
function is z(·). In particular, x ∈ R does not belong to supp(µ˜) if and only if there exists m˜ ∈ R\T such
that z′(m˜)>0 and z(m˜)=x.
17A.3 Additional notation
For a probability measure µ, its support is the closed set
supp(µ)={x∈R:µ(O)>0 for any open neighborhood O ∋x}.
We write dist(x,A)=inf{|x−y|:y ∈A} and define the ε-neighborhood
supp(µ)+(−ε,ε)={x∈R:dist(x,supp(µ))<ε}.
We write δ for the probability measure given by a point mass at x ∈ R, aµ +(1−a)µ for the convex
x 0 1
combination of µ ,µ , and a⊗µ⊕b for the law of ax+b when x∼µ.
0 1
Forvectors,∥v∥≡∥v∥ istheEuclideannorm. Formatrices,∥M∥istheoperatornormsup ∥Mv∥,
2 v:∥v∥=1
∥M∥ istheFrobeniusnorm(TrM⊤M)1/2,Tristhe(unnormalized)matrixtrace,andA⊙B istheentry-
F
wise (Hadamard) product. We write diag(v) for the diagonal matrix with vector v along the main diagonal,
and I for the n×n identity matrix.
n
B Results for the nonlinear spiked covariance model
B.1 Deterministic equivalent for the resolvent
We consider the sample covariance and Gram matrix
1
K =G⊤G∈Rn×n, K(cid:102)=GG⊤ ∈RN×N, where G= √ [g ,...,g ]∈RN×n.
1 N
N
The following are our basic assumptions, where we recall that 1{E} ≺ 0 means P[E] ≤ N−D for any fixed
D >0 and all large N.
Assumption 5. The rows of G are independent and satisfy E[g ] = 0 and E[g g⊤] = Σ for all i ∈ [N],
i i i
such that:
(a) There exist constants C,c>0 such that c<n/N <C and ∥Σ∥<C.
(b) There exists a constant B >0 such that 1{∥K∥>B}≺0.
(c) Uniformly over deterministic matrices A∈Cn×n and over i̸=j ∈[N],
g⊤Ag −E[g⊤Ag ]≺∥A∥ , g⊤Ag ≺∥A∥ .
i i i i F i j F
(d) For any integer α>0, there exists a constant C =C(α)>0 such that E[∥g ∥α]≤NC.
i
Denote the finite-N dimension ratio and empirical eigenvalue distribution of Σ by
n
n 1 (cid:88)
γ = , ν = δ .
N N N n λi(Σ)
i=1
Let
µ =ρMP⊠ν , µ˜ =γ µ +(1−γ )δ .
N γN N N N N N 0
Denote the Stieltjes transforms of µ ,µ˜ by m (z),m˜ (z). These are characterized exactly as in (A.1)
N N N N
with (γ ,ν ) in place of (γ,ν).
N N
We first establish that with high probability, K and K(cid:102) have no outlier eigenvalues far from the support
set
S =supp(µ )∪{0}=supp(µ˜ )∪{0}. (B.1)
N N N
Theorem 9. Suppose Assumption 5 holds. Then for any fixed ε>0,
(cid:110) (cid:111)
1 K has an eigenvalue outside S +(−ε,ε) ≺0.
N
18In asymptotic settings where ν → ν and µ → µ weakly and Σ has no spike eigenvalues, this set S
N N N
will converge to S := supp(µ)∪{0}. In general, S may contain intervals around spike eigenvalues of K
N
thatareseparatedfromsupp(µ)∪{0}ifΣhasaspikedstructure,andthiswillbeclarifiedinthesubsequent
section.
Next,weestablishadeterministicequivalentapproximationfortheresolventofK,forspectralarguments
separated from this support set S . Let us denote by
N
1
R(z)=(K−zI)−1, m (z)= TrR(z)
K n
the resolvent and Stieltjes transform of K for z ̸∈supp(µ ). For any ε>0, define the domain
N
(cid:110) (cid:111)
U (ε)= z ∈C: |z|≤ε−1, dist(z,S )≥ε . (B.2)
N N
Theorem 10. Suppose Assumption 5 holds. Then for any fixed ε>0, uniformly over z ∈U (ε) and over
N
deterministic matrices A∈Cn×n, we have
1 (cid:104) (cid:105) 1
m (z)−m (z)≺ , Tr R(z)A−(−zm˜ (z)Σ−zI)−1A ≺ √ ∥A∥ .
K N N N N F
For spectral arguments z ∈ C\R separated from the positive real line, such a result has been shown
+
recently in [Cho22, SCDL23] (using different proof techniques). We use Theorem 9 as an input to establish
thisapproximationalsoforspectralargumentsinR \S ,assucharesult(anditsextensiontoageneralized
+ N
resolvent) is needed for our analysis of spiked eigenstructure to follow.
B.2 Spike eigenvalues and eigenvectors
Now we consider an asymptotic setting with a specific spiked structure for the population covariance matrix
Σ,havingafixednumberofspikesoutsidethesupportoftheweaklimitofitsspectrallaw. Thisassumption
is summarized as follows.
Assumption 6. Σ has eigenvalues λ (Σ),...,λ (Σ) (not necessarily ordered by magnitude) where, for a
1 n
fixed integer r ≥0, as N →∞:
(a) n/N →γ ∈(0,∞).
(b) There exists a probability measure ν with compact support in (0,∞), such that
n
1 (cid:88)
δ →ν weakly.
n−r λi(Σ)
i=r+1
Furthermore, for any fixed ε>0 and all large N,
λ (Σ)∈supp(ν)+(−ε,ε) for all i≥r+1.
i
(c) There exist distinct values λ ,...,λ >0 with λ ,...,λ ̸∈supp(ν) such that
1 r 1 r
λ (Σ)→λ for all i=1,...,r.
i i
Under this assumption, we analyze the outlier singular values of G and their corresponding singular
vectors. Let
n
n−r 1 (cid:88)
γ = , ν = δ
N,0 N N,0 n−r λi(Σ)
i=r+1
be the finite-N aspect ratio and population spectral measure corresponding to the bulk component of Σ.
Define the laws
µ =ρMP ⊠ν , µ˜ =γ µ +(1−γ )δ
N,0 γN,0 N,0 N,0 N,0 N,0 N,0 0
19and let m (z),m˜ (z) be their Stieltjes transforms. In the setting of Assumption 6, we note that µ →
N,0 N,0 N,0
µ=ρMP⊠ν and µ˜ →µ˜ =γµ+(1−γ)δ weakly as N →∞, where the Stieltjes transforms m(z),m˜(z)
γ N,0 0
of these limits µ,µ˜ are characterized by (A.1).
Denote the limit support set
S =supp(µ)∪{0}=supp(µ˜)∪{0}.
Under Assumption 6 when r = 0, i.e. Σ does not have spike eigenvalues, the following is a corollary of
Theorem 9. A similar “no outlier” statement has been shown for linearly defined sample covariance models
in [BS98].
Corollary 11. Suppose Assumptions 5 and 6 hold, where r =0. Then for any fixed ε>0,
(cid:110) (cid:111)
1 K has an eigenvalue outside S+(−ε,ε) ≺0.
We now give a more quantitative version of Theorem 6 stated informally in Section 3, which describes
the spike eigenvalues of K = G⊤G and corresponding singular vectors of G when there are possibly spike
eigenvalues in Σ. Define the domain
T ={0}∪{−1/λ:λ∈supp(ν )}.
N,0 N,0
For m˜ ∈C\T , define the functions
N,0
1 (cid:90) λ m˜z′ (m˜)
z (m˜)=− +γ dν (λ), φ (m˜)=− N,0 . (B.3)
N,0 m˜ N,0 1+λm˜ N,0 N,0 z (m˜)
N,0
We note that under Assumption 6, the domain T converges in Hausdorff distance to T as defined in
N,0
(A.2). Wewillverifyintheproof(c.f.Lemma23)thatz (m˜)→z(m˜)andz′ (m˜)→z′(m˜)foreachfixed
N,0 N,0
m˜ ∈C\T, where z(·) is as defined in (A.3). Then also φ (m˜)→φ(m˜) for the limiting function
N,0
m˜z′(m˜)
φ(m˜)=− . (B.4)
z(m˜)
Theorem 12. Suppose Assumptions 5 and 6 hold. Let
I =(cid:8) i∈{1,...,r}:z′(−1/λ )>0(cid:9) .
i
(a) For any sufficiently small constant ε > 0 and all large N, on an event E ≡ E satisfying 1{Ec} ≺ 0,
N
there is a 1-to-1 correspondence between the eigenvalues of K outside S +(−ε,ε) and {λ : i ∈ I}.
i
Denoting these eigenvalues of K by {λ(cid:98)i :i∈I}, we have
(cid:18) (cid:19)
1
λ(cid:98)i−z N,0(−1/λ i(Σ))=O ≺E √
N
for each i∈I, where z (−1/λ (Σ))→z(−1/λ )>0 as N →∞.
N,0 i i
(b) On this event E, for each i∈I, let v ∈Rn be a unit-norm eigenvector of K (i.e. right singular vector
(cid:98)i
of G) corresponding to its eigenvalue λ(cid:98)i, and let v
i
be a unit-norm eigenvector of Σ corresponding to
λ (Σ). Then, uniformly over (deterministic) unit vectors v ∈Rn,
i
(cid:113) (cid:18) 1 (cid:19)
|v⊤v |− φ (−1/λ (Σ))·|v⊤v |=OE √
(cid:98)i N,0 i i ≺
N
whereφ (−1/λ (Σ))→φ(−1/λ )>0asN →∞. Inparticular,foreachi∈I,|v⊤v |2 →φ(−1/λ )
N,0 i i i (cid:98)i i
and sup |v⊤v |2 →0 almost surely as N →∞.
j∈[n]:j̸=i j (cid:98)i
(c) Let u = √1 (u 1,...,u N)⊤ ∈ RN be a random vector such that [u,G] ∈ RN×(n+1) has independent
N
rows also satisfying Assumption 5. Denote by E[ug]∈Rn the common value of E[u g ] for all j ∈[N].
j j
On this event E, for each i∈I, let u
(cid:98)i
∈RN be a unit-norm eigenvector of K(cid:102) (i.e. left singular vector
of G) corresponding to its eigenvalue λ(cid:98)i, and let v
i
be the eigenvector of Σ as in part (b). Then
|u⊤u (cid:98)i|− (cid:112) z N,0(−1/λ i( λΣ i) () Σφ )N,0(−1/λ i(Σ)) ·(cid:12) (cid:12)E[ug]⊤v i(cid:12) (cid:12)=O ≺E(cid:18) √1 N(cid:19) .
20C Analysis of the resolvent
We prove the results of Appendix B.1. Appendix C.1 first develops a fluctuation averaging lemma for the
sample covariance model. Appendix C.2 applies this lemma within the arguments of [BS98], to prove the
“nooutliers”resultofTheorem9. AppendixC.3usesTheorem9andasecondapplicationofthefluctuation
averaging lemma to prove the deterministic equivalent approximation of Theorem 10.
C.1 Fluctuation averaging lemma
Recall the definitions
K =G⊤G, K(cid:102)=GG⊤.
For S ⊂ [N], let G(S) ∈ R(N−|S|)×n be the matrix obtained by removing the rows of G corresponding to
i∈S, and define
K(S) =G(S)⊤ G(S) = 1 (cid:88) g g⊤ ∈Rn×n.
N i i
i∈[N]\S
Then, for Γ∈Cn×n, define
1
R(S)(Γ)=(K(S)−Γ)−1, m(S)(Γ)= TrR(S)(Γ),
K n
(cid:18) 1(cid:19) 1 (cid:16) n(cid:17)(cid:18) 1(cid:19) (C.1)
m˜(S)(Γ)=γ m(S)(Γ)+(1−γ ) − = TrR(S)(Γ)+ 1− − .
K N K N z N N z
Importantly, these quantities are independent of {g : i ∈ S}. We say that R(S)(Γ) exists (and hence
i
also m(S),m˜(S) exist) when K(S) − Γ is invertible. For simplicity, we write R = R∅, R(i) = R({i}),
K K
R(Si) =R(S∪{i}), and similarly for m and m˜ .
K K
Lemma13. SupposeAssumption5holds. SupposealsothatthereareconstantsC ,c ,δ,υ >0,N-dependent
0 0
domains U ⊂ C \ {0} and D ,D ⊆ Cn×n, and N-dependent maps Φ : D × D → (N−υ,Nυ) and
Γ A N Γ A
Ψ :D →(N−υ,N1−δ), such that for any fixed L≥1, the events
N Γ
(cid:110)
E(z,Γ,A,S)= R(S)(Γ) exists, ∥R(S)(Γ)A∥ ≤Φ (Γ,A), ∥R(S)(Γ)∥ ≤Ψ (Γ),
F N F N
(cid:111)
∥(z−1Γ+m˜(S)(Γ)Σ)−1∥≤C , and |1+N−1g⊤R(S)(Γ)g |≥c for all j ∈S (C.2)
K 0 j j 0
satisfy 1{E(z,Γ,A,S)c}≺0 uniformly over z ∈U, Γ∈D , A∈D , and S ⊂[N] with |S|≤L.
Γ A
Then, denoting by E the partial expectation over only g (i.e. conditional on {g } ), also uniformly
g i i j j̸=i
over z ∈U, Γ∈D , and A∈D ,
Γ A
N (cid:18) (cid:19)
1 (cid:88) (1−E )(cid:2) g⊤R(i)(Γ)A(z−1Γ+m˜(i)(Γ)Σ)−1g (cid:3) ≺max Ψ N(Γ) ,√1 ·Φ (Γ,A). (C.3)
N g i i K i N N N
i=1
WeremarkthatapplyingAssumption5(c)andtheconditionsofE(z,Γ,A,i)separatelytoeachsummand
of the left side of (C.3) gives the naive bound
N
1 (cid:88) (1−E )[g⊤R(i)(Γ)A(z−1Γ+m˜(i)(Γ)Σ)−1g ]
N g i i K i
i=1
≺mN ax∥R(i)(Γ)A∥ ·∥(z−1Γ+m˜(i)(Γ)Σ)−1∥≺Φ (Γ,A).
F K N
i=1
The content of the lemma is to improve this by the additional factor of max(ΨN(Γ),√1 )≪1.
N N
In this work, we will apply Lemma 13 only to spectral arguments z with O(1)-separation from supp(µ )
√ N
(and matrices Γ=zI or a finite-rank perturbation thereof), in which case we will take Ψ (Γ)=C/ N for
N
a constant C > 0. For full-rank matrices A having bounded operator norm, we will take also Φ (Γ,A) =
N
21√
C/ N, whereas for finite-rank matrices A we will take Φ (Γ,A) = C. We state the result here more
N
abstractly,asitmaybeofindependentinteresttoprovelocallawsinthisnonlinearsamplecovariancemodel
for spectral arguments z that approach supp(µ ).
N
In the remainder of this section, we prove Lemma 13. Fix z ∈ U, Γ ∈ D , and A ∈ D , and write as
Γ A
shorthand
R(S) =R(S)(Γ), m˜(S) =m˜(S)(Γ), Ω(S) =(z−1Γ+m˜(S)(Γ)Σ)−1,
K K
Φ =Φ (Γ,A), Ψ =Ψ (Γ), E(S)=E(z,Γ,A,S).
N N N N
All subsequent instances of ≺ will be implicitly uniform over z ∈ U, Γ ∈ D , and A ∈ D . Define the
Γ A
quantities, for i∈S, j,k ∈S\{i}, and d≥0,
Y(S)[d]=Tr(g g⊤−Σ)R(S)AΩ(S)[ΣΩ(S)]d,
i i i
Z(S)[d]=N−1Tr(g g⊤−Σ)R(S)g g⊤R(S)AΩ(S)[ΣΩ(S)]d,
ijk i i j k
B(S) =N−1g⊤R(S)g ,
jk j k
C(S) =N−2g⊤(R(S))2g ,
jk j k
Q(S) =(1+N−1g⊤R(S)g )−1.
j j j
For each L≥1, define also the event
(cid:92)
E = E(S). (C.4)
L
S⊂[N]:|S|≤L
Lemma 14. For any fixed L,D ≥1, uniformly over S ⊂[N] with |S|≤L, and over i∈S and j,k ∈S\{i}
and d≤D,
Y(S)[d]=OE(S)(Φ ), Z(S)[d]=OE(S)(cid:0) N−1Ψ Φ (cid:1) ,
i ≺ N ijk ≺ N N
(C.5)
B(S) =OE(S)(cid:0) N−1Ψ (cid:1) for j ̸=k, C(S) =OE(S)(cid:0) N−2Ψ2 (cid:1) , Q(S) =OE(S)(1).
jk ≺ N jk ≺ N j ≺
Furthermore, for any α>0, there exists a constant C =C(α,L,D)>0 such that
E(cid:2) |Y(S)[d]|α1{E(S)}(cid:3) <NC, E(cid:2) |Z(S)[d]|α1{E(S)}(cid:3) <NC,
i ijk
(C.6)
E(cid:2) |B(S)|α1{E(S)}(cid:3) <NC, E(cid:2) |C(S)|α1{E(S)}(cid:3) <NC, E(cid:2) |Q(S)|α1{E(S)}(cid:3) <NC.
jk jk j
Proof. On the event E(S), we have by definition Q(S) ≤1/c , so the two statements for Q(S) hold immedi-
j 0 j
ately. Theremainingstatementsof(C.6)followeasilyfromHolder’sinequality,themomentboundsfor∥g ∥
i
in Assumption 5(d), the bound ∥Σ∥ < C in Assumption 5(a), and the conditions ∥R(S)A∥ ≤ Φ ≤ Nυ,
N
∥R(S)∥ ≤Ψ ≤N, and ∥Ω(S)∥≤C defining E(S).
F N 0
For the bounds for B(S) and C(S) in (C.5), note that when j ̸= k, Assumption 5(c) implies B(S) ≺
jk jk jk
N−1∥R(S)∥ and C(S) ≺N−2∥(R(S))2∥ ≤N−2∥R(S)∥2. When j =k, Assumption 5(c) implies also
F jk F F
C(S) ≺N−2|TrΣ(R(S))2|+N−2∥(R(S))2∥
jj F
≤N−2∥ΣR(S)∥ ∥R(S)∥ +N−2∥R(S)∥2 ≤N−2(∥Σ∥+1)∥R(S)∥2.
F F F F
Then these bounds in (C.5) follow from the condition ∥R(S)∥ ≤Ψ defining E(S).
F N
Finally,fortheboundsforY(S)[d]andZ(S)[d]in(C.5),observethatforanymatrixA∈Cn×nindependent
i ijk
of g , we have Tr(g g⊤ −Σ)A ≺ ∥A∥ by Assumption 5(c). Then Y(S)[d] ≺ ∥R(S)AΩ(S)[ΣΩ(S)]d∥ ≤
i i i F i F
∥R(S)A∥ ·∥Ω(S)∥d+1∥Σ∥d, so the bound for Y(S)[d] in (C.5) follows from the conditions ∥R(S)A∥ ≤Φ
F i F N
and ∥Ω(S)∥≤C defining E(S). For Z(S)[d], similarly by Assumption 5(c),
0 ijk
Z(S) ≺N−1∥R(S)g g⊤R(S)AΩ(S)[ΣΩ(S)]d∥ ≤N−1∥R(S)g ∥·∥g⊤R(S)A∥·∥Ω(S)∥d+1∥Σ∥d.
ijk j k F j k
22Applying again Assumption 5(c), we have
∥R(S)g ∥2 =g⊤(R(S))∗R(S)g ≺|TrΣ(R(S))∗R(S)|+∥(R(S))∗R(S)∥ ≺∥R(S)∥2
j 2 j j F F
and similarly ∥g⊤R(S)A∥2 ≺ ∥R(S)A∥2. Then the bound for Z(S)[d] in (C.5) follows from the conditions
k 2 F ijk
∥R(S)A∥ ≤Φ , ∥R(S)∥ ≤Ψ , and ∥Ω(S)∥≤C defining E(S).
F N F N 0
Lemma 15. Fix any L,D ≥ 1. Then there exist coefficients α(d,d′,D) ∈ R such that the following holds:
Uniformly over S ⊂[N] with |S|≤L−1, and over i∈S, j,k ∈S\{i}, l∈[N]\S, and d≤D,
Y(S)[d]=d+ (cid:88)⌈D/2⌉ α(d,d′,D)(cid:104) C(Sl)Q(Sl)(cid:105)d′−d(cid:16) Y(Sl)[d′]−Z(Sl)[d′]Q(Sl)(cid:17) +OEL(cid:0) N−DΨDΦ (cid:1) (C.7)
i ll l i ill l ≺ N N
d′=d
Z(S)[d]=d+ (cid:88)⌈D/2⌉ α(d,d′,D)(cid:104) C(Sl)Q(Sl)(cid:105)d′−d(cid:16)
Z(Sl)[d′]−Z(Sl)[d′]B(Sl)Q(Sl)
ijk ll l ijk ilk lj l
d′=d
−Z(Sl)[d′]B(Sl)Q(Sl)+Z(Sl)[d′]B(Sl)B(Sl)(Q(Sl))2(cid:17) +OEL(cid:0) N−DΨDΦ (cid:1) , (C.8)
ijl kl l ill lj kl l ≺ N N
B(S) =B(Sl)−B(Sl)B(Sl)Q(Sl), (C.9)
jk jk jl lk l
C(S) =C(Sl)−B(Sl)C(Sl)Q(Sl)−C(Sl)B(Sl)Q(Sl)+B(Sl)C(Sl)B(Sl)(Q(Sl))2, (C.10)
jk jk jl lk l jl lk l jl ll lk l
⌈D/2⌉
Q(S) = (cid:88) (cid:16) Q(Sl)(cid:17)d(cid:104) (B(Sl))2Q(Sl)(cid:105)d−1 +OEL(cid:0) N−DΨD(cid:1) . (C.11)
j j jl l ≺ N
d=1
Proof. By the Sherman-Morrison formula, on the event E where R(S) and R(Sl) both exist, we have
L
R(S) =R(Sl)−N−1R(Sl)g g⊤R(Sl)·Q(Sl). (C.12)
l l l
Applying this to each copy of R(S) defining B(S) and C(S) yields immediately (C.9) and (C.10), as well as
jk jk
the identities
(cid:16) (cid:17)
z−1Γ+m˜(S)Σ=z−1Γ+ N−1TrR(S)+(1−γ )(−1/z) Σ
N
=(z−1Γ+m˜(Sl)Σ)−C(Sl)Q(Sl)Σ,
ll l
1+B(S) =1+B(Sl)−(B(Sl))2Q(Sl).
jj jj jl l
Taking inverses and applying the expansion
⌈D/2⌉
(cid:88)
(A−∆)−1 = A−1(∆A−1)d−1+(A−∆)−1(∆A−1)⌈D/2⌉,
d=1
we obtain
⌈D/2⌉
Ω(S) = (cid:88) Ω(Sl)[C(Sl)Q(Sl)ΣΩ(Sl)]d−1+E, (C.13)
ll l
d=1
⌈D/2⌉
Q(S) = (cid:88) Q(Sl)[(B(Sl))2Q(Sl)Q(Sl)]d−1+e, (C.14)
j j jl l j
d=1
for remainder terms E ∈Cn×n and e∈C satisfying, by the bounds of Lemma 14,
∥E∥=OEL(cid:16) |C(Sl)|D/2(cid:17) =OEL(cid:0) (N−1Ψ)D(cid:1)
,
|e|=OEL(cid:16) |(B(Sl))2|D/2(cid:17) =OEL(cid:0) (N−1Ψ)D(cid:1)
.
≺ ll ≺ ≺ jl ≺
23In particular, (C.14) shows (C.11). Applying (C.13) to the definitions of Y(S)[d] and Z(S)[d], we get
i ijk
 
⌈D/2⌉
Y i(S)[d]=Tr(g ig⊤
i
−Σ)R(S)A (cid:88) Ω(Sl)[C l( lSl)Q( lSl)ΣΩ(Sl)]d′−1+E
d′=1
  d
⌈D/2⌉
·Σ
(cid:88) Ω(Sl)[C(Sl)Q(Sl)ΣΩ(Sl)]d′−1+E
,
ll l
d′=1
 
⌈D/2⌉
Z i( jS k)[d]= N1 Tr(g ig⊤
i
−Σ)R(S)g jg⊤ kR(S)A (cid:88) Ω(Sl)[C l( lSl)Q( lSl)ΣΩ(Sl)]d′−1+E
d′=1
  d
⌈D/2⌉
·Σ
(cid:88) Ω(Sl)[C(Sl)Q(Sl)ΣΩ(Sl)]d′−1+E
.
ll l
d′=1
For any matrix B ∈ Cn×n independent of g , observe that Tr(g g⊤ −Σ)R(S)AB = OE(S)(Φ ∥B∥) and
i i i ≺ N
Tr(g g⊤ −Σ)R(S)g g⊤R(S)AB = OE(S)(Ψ Φ ∥B∥) by the same arguments as those bounding Y(S)[d]
i i j k ≺ N N i
and Z(S)[d] in the proof of Lemma 14. Then, expanding the above and absorbing all terms containing E
ijk
andalltermswithcombinedpowerofC(Sl) largerthanD/2intoOE(S)(cid:0) N−DΨDΦ (cid:1) remainders,weobtain
ll ≺ N N
for some coefficients α(d,d′,D)∈R that
⌈D/2⌉
Y(S)[d]=Tr(g g⊤−Σ)R(S)A (cid:88) α(d,d′,D)[C(Sl)Q(Sl)]d′ Ω(Sl)[ΣΩ(Sl)]d+d′
i i i ll l
d′=0
+OE(S)(cid:0) N−DΨDΦ (cid:1) ,
≺ N N
⌈D/2⌉
Z(S)[d]= 1 Tr(g g⊤−Σ)R(S)g g⊤R(S)A (cid:88) α(d,d′,D)[C(Sl)Q(Sl)]d′ Ω(Sl)[ΣΩ(Sl)]d+d′
ijk N i i j k ll l
d′=0
+OE(S)(cid:0) N−DΨDΦ (cid:1) .
≺ N N
Finally, applying the Sherman-Morrison formula (C.12) to expand each copy of R(S), and re-indexing the
summations by d+d′ (cid:55)→d′, we get (C.7) and (C.8).
Lemma 16. Fix any L,D ≥ 1. Uniformly over S ⊂ [N] with |S| ≤ L and over i ∈ S, the following holds:
Denote S¯=S\{i}. Then there exists a collection of monomials M such that Y(i)[0] can be approximated
i,S i
as
(cid:18)
Y i(i)[0]= (cid:88) q {Y i(S)[d]} d≤⌊D/2⌋,{Z i( jS k)[d]} j,k∈S¯,d≤⌊D/2⌋,{B j(S k)} j̸=k∈S¯,
q∈Mi,S
(cid:19)
{C j( kS)} j,k∈S¯,{Q( jS)} j∈S¯ +O ≺EL(cid:0) N−DΨD NΦ N(cid:1) . (C.15)
Each monomial q ∈M is a product of a real-valued scalar coefficient and one or more factors of the form
i,S
Y(S)[d], Z(S)[d], B(S) with j ̸=k, C(S), Q(S) for j,k ∈S¯ and d≤⌊D/2⌋. We have q =OEL(Φ ) uniformly
i ijk jk jk j ≺ N
over q ∈M , and the number of monomials |M | is most a constant depending on L,D. Furthermore:
i,S i,S
(a) There is exactly one factor of the form Y(S)[d] or Z(S)[d] appearing in q.
i ijk
(b) The number of factors Z(S)[d], B(S), and C(S) appearing in q is no less than the number of distinct
ijk jk jk
indices of S¯ (not including i) that appear as lower indices across all factors of q.
24Proof. We arbitrarily order the indices of S¯ = S \{i} as l ,l ,...,l . Beginning with the monomial
1 2 |S|−1
Y(i)[0], iteratively for j = 1,2,...,|S|−1, we replace all factors with superscript (il ...l ) by a sum of
i 1 j−1
termswithsuperscript(il ...l ), usingtherecursions(C.7)–(C.11). Itisthendirecttocheckthatthisgives
1 j
a representation of the form (C.15), where:
• Each application of (C.7)–(C.8) replaces a factor Y(...)[d] or Z(...)[d] by terms having exactly one such
i ijk
factor. Thus, each monomial q ∈M has exactly one factor Y(S)[d] or Z(S)[d].
i,S i ijk
• The number of total applications of (C.7)–(C.11) is bounded by a constant depending on L,D, so
|M | and the scalar coefficient of each q ∈M are both bounded by constants depending on L,D.
i,S i,S
Then, by the bounds of (C.5), each q ∈ M satisfies q = OEL(Φ ), and the remainder in (C.15) is
i,S ≺ N
at most OEL(cid:0) N−DΨDΦ (cid:1) . If q has the term Y(S)[d] or Z(S)[d], then it also has combined power of
≺ N N i i
{C j( kS)} j,k∈S¯ equal to d, and hence may be absorbed into the remainder of (C.15) if d>D/2.
• Each term on the right side of (C.7)–(C.11) that contains the new lower index l has at least one more
factor of the form Z(...)[d], B(...), or C(...) than the left side. Thus, each monomial q ∈ M is such
ijk jk jk i,S
that the number of distinct lower indices of S¯ across all of its factors is no greater than the number of
its factors of the form Z(...)[d], B(...), or C(...).
ijk jk jk
Combining these observations yields the lemma.
Proof of Lemma 13. Foreachε,D >0,letusfixanevenintegerL=L(ε,D)>D/ε. Theassumptionof
thislemmaguarantees1{E(S)c}≺0uniformlyoverS ⊂[N]with|S|≤L. Sincethenumberofsuchsubsets
is at most NL, we may take a union bound (c.f. Proposition 7(a)) to obtain 1{Ec}≺0 for the intersection
L
event E of (C.4). Noting that (1−E )[g⊤R(i)AΩg ]=Y(i)[0], to prove the lemma, it suffices to show for
L g i i i i
any ε,D >0 and all sufficiently large N that
(cid:34)(cid:32) N (cid:33) (cid:18) (cid:19) (cid:35)
P 1 (cid:88) Y(i)[0] 1{E }>max Ψ N,√1 Φ ·Nε <N−D. (C.16)
N i L N N N
i=1
In anticipation of applying Markov’s inequality, we analyze
(cid:32) N (cid:33)L  N (cid:34) L (cid:35)
E  (cid:88) Y i(i)[0] 1{E L}= (cid:88) E (cid:89) Y i( lil)[0]1{E L} . (C.17)
i=1 i1,...,iL=1 l=1
(cid:124) (cid:123)(cid:122) (cid:125)
:=E[m(i1,...,iL)]
Fixanyindextuple(i ,...,i ). LettingS ={i ,...,i }bethesetofdistinctindicesinthistuple,weapply
1 L 1 L
Lemma 16 to each term Y(il)[0], with this set S and with D =L. This gives
il
L
m(i ,...,i )= (cid:88) ... (cid:88) (cid:89) q(l)·1{E }+O (cid:0) (N−1Ψ )LΦL(cid:1) , (C.18)
1 L L ≺ N N
q(1)∈M(i1,S) q(l)∈M(il,S)l=1
where each M(i ,S) is the collection of monomials arising in the approximation of Y(il)[0], and we have
l il
applied q(l) = OEL(Φ ) to bound the remainder. Observe that by (C.6) and Holder’s inequality, we have
≺ N
E[|m(i ,...,i )|2] ≤ NC and E[|(cid:81)L q(l)·1{E }|2] ≤ NC for all q(1),...,q(L) and a constant C > 0. By
1 L l=1 L
this and the given condition Ψ ,Φ ≥N−υ, we may take expectations in (C.18) using Proposition 7(d) to
N N
get
(cid:34) L (cid:35)
E[m(i ,...,i )]= (cid:88) ... (cid:88) E (cid:89) q(l)·1{E } +O (cid:0) (N−1Ψ )LΦL(cid:1) . (C.19)
1 L L ≺ N N
q(1)∈M(i1,S) q(l)∈M(il,S) l=1
25Now to bound E[(cid:81)L q(l)·1{E }], we consider separately two cases, focusing on those indices i which
l=1 L l
appearexactlyoncein(i ,...,i ). Inthefirstcase,supposethereissomesuchindexi thatdoesnotappear
1 L l
as a lower index of q(l′) for any l′ ̸=l. Fixing this set S ={i ,...,i } and index i ∈S, let us introduce
1 L l
(cid:26)
E′ = R(S) exists, ∥R(S)A∥ ≤Φ , ∥R(S)∥ ≤Ψ , ∥(z−1Γ+m˜(S)Σ)−1∥≤C ,
F N F N 0
(cid:27)
and |1+N−1g⊤R(S)g |≥c for all j ∈S\{i } .
j j 0 l
ComparingwiththedefinitionofE(S)from(C.2),observethatonlythelastconditiondefiningE′ isdifferent
(where we do not require the bound for j = i ), so that this event E′ is independent of g . Then E ⊆
l il L
E(S)⊆E′, and
(cid:34) L (cid:35) (cid:34) L (cid:35) (cid:34) L (cid:35)
(cid:89) (cid:89) (cid:89)
E q(l)·1{E } =E q(l)·1{E′} −E q(l)·1{E′}1{Ec} . (C.20)
L L
l=1 l=1 l=1
For the first term of (C.20), observe that both {q(l′) :l′ ̸=l} and E′ are independent of g , and only the one
il
factor Y(S)[d] or Z(S)[d] in q(l) depends on g . Then, noting that E [Y(S)[d]] = 0 and E [Z(S)[d]] = 0,
il iljk il g i i g i ijk
the first term of (C.20) is 0. For the second term of (C.20), observe that all statements of (C.6) continue to
hold with E(S) replaced by E′, except for the bound on Q(S). But Q(S) appears neither in {q(l′) : l′ ̸= l}
il il
nor in q(l), so we may apply Holder’s inequality to get E[|(cid:81)L q(l)|21{E′}] ≤ NC for a constant C > 0.
l=1
Then, applying Cauchy-Schwarz and 1{Ec} ≺ 0, the second term of (C.20) is bounded by N−D′ for any
L
fixed constant D′ >0 and all large N. Thus,
(cid:34) L (cid:35)
E (cid:89) q(l)·1{E } ≤N−D′ . (C.21)
L
l=1
In the second case, every index i that appears exactly once in (i ,...,i ) appears as a lower index of
l 1 L
q(l′) for some l′ ̸= l. Call the number of such indices K. Then condition (b) of Lemma 16 implies that the
total number of factors of the forms Z(S)[d], B(S) for j ̸= k, and C(S) across all monomials q(1),...,q(L) is
ijk jk jk
at least K. Then, by the bounds of Lemma 14 and Proposition 7(d), we have
(cid:34) L (cid:35)
(cid:89)
E q(l)·1{E } ≺(N−1Ψ )KΦL. (C.22)
L N N
l=1
Under the given condition Φ ,Ψ ≥N−υ, we have N−D′ ≤(N−1Ψ )KΦL for large enough D′. Then,
N N N N
combining the two cases (C.21) and (C.22) and applying this back to (C.19), we get
E[m(i ,...,i )]≺(N−1Ψ )KΦL (C.23)
1 L N N
where K is the number of indices in S ={i ,...,i } that appear exactly once in (i ,...,i ). Let J be the
1 L 1 L
number of distinct indices in S = {i ,...,i } that appear at least twice in (i ,...,i ). Then 2J +K ≤ L,
1 L 1 L
and the number of index tuples (i ,...,i ) ∈ [N]L with these values of (J,K) is at most CNJ+K, for a
1 L
constant C =C(J,K)>0. Then, applying (C.23) back to (C.17) yields
(cid:32)
N
(cid:33)L 
E  (cid:88) Y i(i)[0] 1{E L}≺ max NJ+K ·(N−1Ψ N)KΦL N
J,K≥0:2J+K≤L
i=1
√ √
= max ( N)2JΨKΦL ≤max(Ψ , N)LΦL.
N N N N
J,K≥0:2J+K≤L
Finally, by Markov’s inequality, the probability in (C.16) is at most
√
(cid:32)
N
(cid:33)L 
max(Ψ N, N)−LΦ− NLN−εL·E  (cid:88) Y i(i)[0] 1{E L}≺N−εL,
i=1
and (C.16) follows as desired under our initial choice L=L(ε,D)>D/ε.
26C.2 No eigenvalues outside the support
We now prove Theorem 9. Let m (z),m˜ (z) be the Stieltjes transform of the N-dependent deterministic
N N
measures µ ,µ˜ . For each z ∈C+, m˜ (z) is the unique root in C+ to the equation
N N N
1 (cid:90) λ
z =− +γ dν (λ), (C.24)
m˜ (z) N 1+λm˜ (z) N
N N
and m (z),m˜ (z) are related by m˜ (z)=γ m (z)+(1−γ )(−1/z). Define the discrete set
N N N N N N
T ={0}∪{−1/λ:λ∈supp(ν )}. (C.25)
N N
On the domain C\T , we may define the formal inverse of (C.24),
N
1 (cid:90) λ
z (m˜)=− +γ dν (λ), (C.26)
N m˜ N 1+λm˜ N
which is a finite-N analogue of (A.3). Let S be the deterministic support defined in (B.1), and let U (ε)
N N
be the spectral domain (B.2). The following basic properties of S and m˜ (z) are known.
N N
Proposition 17. SupposeAssumption5(a)holds, andfixanyε>0. ThenthereexistconstantsC ,c >0,
0 0
depending only on ε and the constants C,c of Assumption 5(a), such that for all x ∈ S we have |x| ≤ C,
N
and for all z =x+iη ∈U (ε) we have
N
c<|m˜ (z)|<C, cη ≤|Imm˜ (z)|≤Cη, min |1+λm˜ (z)|≥c
N N N
λ∈supp(νN)
Proof. See [FJ22, Propositions A.3, B.1, B.2].
Let m K˜(z) = N−1Tr(K(cid:102)−zI)−1 be the Stieltjes transform of the empirical eigenvalue distribution of
K(cid:102)=GG⊤. Since K(cid:102) and K =G⊤G have the same eigenvalues up to |N −n| 0’s, we have
m (z)=γ m (z)+(1−γ )(−1/z),
K˜ N K N
so in particular m coincides with m˜(∅) from (C.1). We begin with a preliminary estimate for the Stieltjes
K˜ K
transformm (z)whenImz ≥N−1/11. Similarstatementshavebeenshownin[Sil95,BS98],andweprovide
K˜
an argument here following ideas of [BS98, Section 3] for later reference.
Lemma 18. Fix any ε > 0, and suppose Assumption 5 holds. Then, uniformly over z = x+iη ∈ U (ε)
N
with Imz ≥N−1/11,
1
m (z)−m˜ (z)≺ √ .
K˜ N
Nη4
Proof. Let R(i) and m˜(i) be as defined in (C.1) with Γ=zI. Applying the Sherman-Morrison formula
K
N−1R(i)g g⊤R(i)
R=R(i)− i i , (C.27)
1+N−1g⊤R(i)g
i i
for any matrix B ∈Cn×n we have
N
1 (cid:88)
TrB =Tr(K−zI)RB =−zTrRB+ g⊤RBg
N i i
i=1
1 (cid:88)N g⊤R(i)Bg
=−zTrRB+ i i . (C.28)
N 1+N−1g⊤R(i)g
i=1 i i
ChoosingB =I in(C.28),applyingTrR=nm =Nm +(n−N)(−1/z),andrearranging,weobtain
K K˜
the identity
N
1 (cid:88) 1
m =− . (C.29)
K˜ Nz 1+N−1g⊤R(i)g
i=1 i i
27Now fix any deterministic matrix A∈Cn×n, define
1 1
d = g⊤R(i)A(I+m Σ)−1g − TrRA(I+m Σ)−1Σ,
i N i K˜ i N K˜
and choose B =A(I+m Σ)−1 in (C.28). Then, applying also the identity (C.29), we get
K˜
TrA(I+m Σ)−1
K˜
N
=−zTrRA(I+m Σ)−1−zm TrRA(I+m Σ)−1Σ+(cid:88) d i
K˜ K˜ K˜ 1+N−1g⊤R(i)g
i=1 i i
N
=−zTrRA+(cid:88) d i . (C.30)
1+N−1g⊤R(i)g
i=1 i i
We proceed to bound d , where (for later purposes) we derive estimates in terms of the Frobenius norms
i
of R,RA,R(i),R(i)A rather than their operator norms. Note that Assumption 5(c) implies, for any matrix
B ∈Cn×n independent of g ,
i
∥Bg ∥2 =g⊤B∗Bg ≺TrΣB∗B+∥B∗B∥ ≺∥B∥2. (C.31)
i i i F F
We have also, by Assumption 5(c) and the Sherman-Morrison formula (C.27),
N−1|TrRB−TrR(i)B|=N−2|1+N−1g⊤R(i)g |−1|g⊤R(i)BR(i)g |
i i i i
(cid:16) (cid:17)
≺N−2|1+N−1g⊤R(i)g |−1 |TrΣR(i)BR(i)|+∥R(i)BR(i)∥
i i F
≺N−2|1+N−1g⊤R(i)g |−1∥R(i)B∥ ∥R(i)∥ . (C.32)
i i F F
Define d =d +d +d +d where
i i,1 i,2 i,3 i,4
d =N−1g⊤R(i)A(I+m Σ)−1g −N−1g⊤R(i)A(I+m˜(i)Σ)−1g ,
i,1 i K˜ i i K i
d =N−1g⊤R(i)A(I+m˜(i)Σ)−1g −N−1TrΣR(i)A(I+m˜(i)Σ)−1,
i,2 i K i K (C.33)
d =N−1TrΣR(i)A(I+m˜(i)Σ)−1−N−1TrΣRA(I+m˜(i)Σ)−1,
i,3 K K
d =N−1TrΣRA(I+m˜(i)Σ)−1−N−1TrΣRA(I+m Σ)−1.
i,4 K K˜
ApplyingtheidentityA−1−B−1 =A−1(B−A)B−1,thedefinitionofm˜(i) in(C.1),andthebounds(C.31)
K
and (C.32) (the latter with B =I),
|d |≤N−1∥g⊤R(i)A∥∥(I+m Σ)−1∥∥(m˜(i)−m )Σ∥∥(I+m˜(i)Σ)−1∥∥g ∥
i,1 i K˜ K K˜ K i
≺N−5/2|1+N−1g⊤R(i)g |−1∥R(i)A∥ ∥R(i)∥2∥(I+m Σ)−1∥∥(I+m˜(i)Σ)−1∥. (C.34)
i i F F K˜ K
Applying Assumption 5(c),
|d |≺N−1∥R(i)A(I+m˜(i)Σ)−1∥ ≤N−1∥R(i)A∥ ∥(I+m˜(i)Σ)−1∥.
i,2 K F F K
Applying the Sherman-Morrison identity (C.27), |Truv⊤|≤∥u∥∥v∥, and (C.31),
|d |≤N−2|1+N−1g⊤R(i)g |−1∥ΣR(i)g ∥∥g⊤AR(i)(I+m˜(i)Σ)−1∥
i,3 i i i i K
≺N−2|1+N−1g⊤R(i)g |−1∥R(i)A∥ ∥R(i)∥ ∥(I+m˜(i)Σ)−1∥.
i i F F K
Finally, applying A−1 −B−1 = A−1(B −A)B−1, (C.32) (with B = I), and |TrAB| ≤ ∥A∥ ∥B∥ ≤
√ F F
N∥A∥ ∥B∥,
F
(cid:12) (cid:12)
|d |=N−1(cid:12)TrΣRA(I+m˜(i)Σ)−1(m˜(i)−m )Σ(I+m Σ)−1(cid:12)
i,4 (cid:12) K K K˜ K˜ (cid:12)
28≺N−5/2|1+N−1g⊤R(i)g |−1∥RA∥ ∥R∥2∥(I+m˜(i)Σ)−1∥∥(I+m Σ)−1∥. (C.35)
i i F F K K˜
Forthecurrentproof, weapply(C.30)andthedefinitions(C.33)withA=I. RecallingTrR=nm =
K
Nm +(n−N)(−1/z) and rearranging (C.30) with A=I, we get the identity
K˜
N
z (m )−z =− 1 · 1 (cid:88) d i (C.36)
N K˜ m
K˜
N
i=1
1+N−1g⊤
i
R(i)g
i
where z (m)=−(1/m)+N−1TrΣ(I +mΣ)−1 is the function defined in (C.26). For any z =x+iη with
N
η >0, we have
|z(1+N−1g⊤R(i)g )|≥Im[z(1+N−1g⊤R(i)g )]≥Imz =η, (C.37)
i i i i
max(∥R∥ ,∥R(i)∥ )≤N1/2max(∥R∥,∥R(i)∥)≤N1/2η−1. (C.38)
F F
Here, the second inequalities of both (C.37) and (C.38) follow from the spectral representations of R,R(i),
i.e. writing (λ ,v )n for the eigenvalues and unit eigenvectors of K(i), we have
j j j=1
   
n n
(cid:88) 1 (cid:88) z
Im[zg⊤ i R(i)g i]=Imzg⊤ i  λ −zv jv⊤ j g i= Im λ −z ·(g⊤ i v j)2
j j
j=1 j=1
n
=(cid:88) λ jImz
·(g⊤v )2 ≥0,
|λ −z|2 i j
j
j=1
(cid:13) (cid:13)
(cid:13) n (cid:13)
∥R(i)∥=(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)
j=1
λ j1 −zv jv⊤ j (cid:13) (cid:13) (cid:13) (cid:13)=m j=n a 1x|λ j −z|−1 ≤η−1,
and similarly for ∥R∥. In particular, (C.37) and (C.38) imply
(1+N−1g⊤R(i)g )−1 ≺η−1, ∥R∥ ,∥R(i)∥ ≺N1/2η−1. (C.39)
i i F F
Next, observe that if m(z)=(cid:82) 1 dµ(λ) is the Stieltjes transform of any probability measure µ supported
λ−z
on [−B,B], then for z =x+iη with η >0 and |z|≤ε−1, we have
(cid:90) η (cid:90) |λ−x|
Imm(z)= dµ(λ)≥cη, |Rem(z)|≤ dµ(λ)≤(C/η)Imm(z)
|λ−z|2 |λ−z|2
for some constants C,c > 0 depending on ε,B. Consequently, for any λ ≥ 0, either λ·|Rem(z)| < 1/2
or λ·Imm(z) ≥ 2η/C, so |1+λm(z)| ≥ max(2,2η/C). By Assumption 5(b) and Weyl’s inequality, we
have 1{∥K∥ > B} ≺ 0 and 1{∥K(i)∥ > B} ≺ 0, and on the event where ∥K∥,∥K(i)∥ ≤ B, we have that
m ,m˜(i) are Stieltjes transforms of probability measures supported on [−B,B]. Thus, this implies
K˜ K
|m |−1 ≤|Imm |−1 ≺η−1, max(∥(I+m Σ)−1∥,∥(I+m˜(i)Σ)−1∥)≺η−1. (C.40)
K˜ K˜ K˜ K
Applying these bounds (C.39) and (C.40) to (C.34)–(C.35), we get d ≺N−1η−6+N−1/2η−2 ≤2N−1/2η−2
i
for η ≥N−1/11. Then, applying these bounds (C.39) and (C.40) also to (C.36), we get
1
z (m )−z ≺ √ . (C.41)
N K˜
Nη4
√
The proof is completed by the following stability argument: When η ≥ N−1/11, we have 1/( Nη4) ≪
η =Imz, so (C.41) implies in particular that
1{z (m )∈/ C+}≺0. (C.42)
N K˜
29On the event z (m ) ∈ C+, recalling the implicit definition of m˜ : C+ → C+ by (C.24), the value
N K˜ N
m˜ (z (m )) must be the unique root u∈C+ to the equation
N N K˜
1 (cid:90) λ
z (m )=− +γ dν (λ),
N K˜ u N 1+λu N
i.e. to the equation z (m ) = z (u). This equation is satisfied by u = m ∈ C+, so we deduce that
N K˜ N K˜
m˜ (z (m ))=m . Then, applying that z ∈U (ε) and that m˜ :C+ →C+ is (4/ε2)-Lipschitz over the
N N K˜ K˜ N N
domain U (ε/2), we obtain from (C.41) that
N
(cid:16) (cid:17) (cid:16) (cid:17) 1
1{z (m )∈C+} m −m˜ (z) =1{z (m )∈C+} m˜ (z (m ))−m˜ (z) ≺ √ .
N K˜ K˜ N N K˜ N N K˜ N
Nη4
Together with (C.42), this yields the lemma.
Corollary 19. Fix any ε>0, and suppose Assumption 5 holds. Then there is a constant C >0 such that
uniformly over z ∈U (ε) with Imz ≥N−1/11,
N
√
1{∥R(z)∥ >C N}≺0.
F
Proof. Since m (z)=γ m (z)+(1−γ )(−1/z) and m˜ (z)=γ m (z)+(1−γ )(−1/z), Lemma 18
K˜ N K N N N N N
implies also
1
m (z)−m (z)≺ √ ≪η.
K N
Nη4
Observe that Imm (z) = (cid:82) η/|λ−z|2dµ (λ) ≤ ηε−2 for z ∈ U (ε), so 1{Imm (z) > (1+ε−2)η} ≺ 0.
N N N K √
Then by the identity ∥R(z)∥2 =(cid:80) 1/|z−λ (K)|2 =(n/η)Imm (z), we get 1{∥R(z)∥ >C N}≺0 for
F i i K F
a constant C =C(ε)>0, as desired.
WemaynowapplyCorollary19andthefluctuationaveragingresultofLemma13toimprovetheestimate
of Lemma 18 to the following result.
Lemma 20. Fix any ε > 0, and suppose Assumption 5 holds. Then, uniformly over z = x+iη ∈ U (ε)
N
with Imz ≥N−1/11,
1
m (z)−m˜ (z)≺ .
K˜ N N
Proof. Wederiveanimprovedestimatefor(C.36). First, combiningLemma18withtheboundsform˜ (z)
N
in Proposition 17, there are constants C ,c >0 for which
0 0
1{|m |>C }≺0, 1{|m |<c }≺0, 1{∥(I+m Σ)−1∥>C }≺0 (C.43)
K˜ 0 K˜ 0 K˜ 0
uniformly over z ∈ U (ε) with Imz ≥ N−1/11. Next, applying Assumption 5(c), we have also uniformly
N
over i∈[N],
(cid:16) (cid:17)
N−1g⊤R(i)g =N−1TrΣR(i)+O N−1∥R(i)∥
i i ≺ F
(cid:16) (cid:17) (cid:16) (cid:17)
=N−1TrΣR+O N−2|1+N−1g⊤R(i)g |−1∥R(i)∥2 +O N−1∥R(i)∥
≺ i i F ≺ F
where the second line follows from (C.32) applied with B = Σ. Applying ∥R(i)∥ ≺ N1/2 by Corollary 19
F
and the estimate |1+N−1g⊤R(i)g |−1 ≺η−1 from (C.39), this gives
i i
(cid:16) (cid:17)
1+N−1g⊤R(i)g =1+N−1TrΣR+O N−1/2 . (C.44)
i i ≺
Then, applying this and |1+N−1g⊤R(i)g |−1 ≺η−1 to (C.29),
i i
1 1 (cid:16) (cid:17)
m =− · +O N−1/2η−2 .
K˜ z 1+N−1TrΣR ≺
30Together with the first bound of (C.43) and the bound |z| ≤ ε−1 for z ∈ U (ε), this implies for a constant
N
c >0 that 1{|1+N−1TrΣR|<c }≺0, and thus 1{|1+N−1g⊤R(i)g |<c }≺0.
0 0 i i 0
Applying Corollary 19 and the above arguments now for K(S) and R(S) in place of K and R, we obtain
for any fixed L ≥ 1 and some constants C ,c > 0, uniformly over S ⊂ [N] with |S| ≤ L, over i ∈ S, and
0 0
over z ∈U (ε) with Imz ≥N−1/11,
N
1{|m˜(S)|>C }≺0, 1{|m˜(S)|<c }≺0, 1{∥(I+m˜(S)Σ)−1∥>C }≺0,
K 0 K 0 K 0
√
1{∥R(S)∥ >C N}≺0, 1{|1+N−1TrΣR(S)|<c }≺0, (C.45)
F 0
1{|1+N−1g⊤R(S)g |<c }≺0.
i i 0
(We remark that a direct application of the above arguments for K(S) yields the first three estimates of
(C.45) for the quantity N m˜(S) = 1 TrR(S)+ n (−1/z) in place of m˜(S), and the estimates for
N−|S| K N−|S| N−|S| K
m˜(S) then follow for slightly modified constants C ,c >0 because |S|≤L.)
K 0 0
Finally,applying(C.44)and(C.45)backto(C.36)and(C.34)–(C.35)withA=I,weget|d |,|d |,|d |≺
i,1 i,3 i,4
N−1, |d |≺N−1/2, and
i,2
(cid:12) (cid:12)
|z (m )−z|≺(cid:12) (cid:12) 1 (cid:88)N d i,2 (cid:12) (cid:12)+O (cid:0) N−1(cid:1)
N K˜ (cid:12) (cid:12)N
i=1
1+N−1g⊤
i
R(i)g i(cid:12)
(cid:12)
≺
(cid:12) (cid:12)
= 1 · 1 ·(cid:12) (cid:12)(cid:88)N d (cid:12) (cid:12)+O (cid:0) N−1(cid:1) .
N 1+N−1TrΣR (cid:12) i,2(cid:12) ≺
(cid:12) (cid:12)
i=1
Thestatementsof(C.45)verifytheneededassumptionsofLemma13withA=I,Γ=zI,andΦ =Ψ =
√ N N
C N. Then Lemma 13 gives
(cid:80)N
d ≺1, and hence
i=1 i,2
|z (m )−z|≺N−1.
N K˜
TheproofisthencompletedbythesamestabilityargumentasintheconclusionoftheproofofLemma18.
Proof of Theorem 9. We apply the idea of [BS98, Section 6]. Let z =x+iη, where dist(x,S )≥ε and
N
η =N−1/11. Taking imaginary part in the estimate m (z)−m˜ (z)≺N−1 of Lemma 20 and multiplying
K˜ N
by η gives
1 (cid:88)N η2 (cid:90) η2 η
− dµ˜ (λ)≺ .
N
j=1
(λ j(K(cid:102))−x)2+η2 (λ−x)2+η2 N N
√
Fix any integer P ≥1, and apply this instead at the point z =x+i pη for each p=1,...,P. Then
1 (cid:88)N η2 (cid:90) η2 η
− dµ˜ (λ)≺ for all p=1,...,P.
N
j=1
(λ j(K(cid:102))−x)2+pη2 (λ−x)2+pη2 N N
Taking successive finite differences using
(cid:32) (cid:33)
1 1 1 η2
− = ,
r−q+1 (cid:81)r (λ−x)2+pη2 (cid:81)r+1 (λ−x)2+pη2 (cid:81)r+1(λ−x)2+pη2
p=q p=q+1 p=q
we then obtain
1 (cid:88)N η2P (cid:90) η2P η
− dµ˜ (λ)≺ . (C.46)
N
j=1
(cid:81)P p=1[(λ j(K(cid:102))−x)2+pη2] (cid:81)P p=1[(λ−x)2+pη2] N N
Sincedist(x,S )≥ε,thesecondintegraltermof(C.46)isboundedbyCη2P foraconstantC :=C(ε,P)>0.
N
Thus, we get
1 (cid:88)N C (cid:88)N η2P η
N
j=11{λ j(K(cid:102))∈(x−η,x+η)}≤
N
j=1
(cid:81)P p=1[(λ j(K(cid:102))−x)2+pη2]
≺
N
+η2P
31where the first inequality holds for a constant C :=C(P)>0. Finally, recalling η =N−1/11 and taking any
P ≥6, we get η/N +η2P ≪1/N, hence
(cid:8) (cid:9)
1 there exists an eigenvalue of K(cid:102) in (x−η,x+η) ≺0.
RecallingAssumption5(b)andtakingaunionboundoverxbelongingtoaη-netof[−B,B]\(S +(−ε,ε))
N
(with cardinality at most CN1/11), we obtain
(cid:8) (cid:9)
1 there exists an eigenvalue of K(cid:102) in S
N
+(−ε,ε) ≺0.
The theorem follows from the observation that K has the same non-zero eigenvalues as K(cid:102), and all 0 eigen-
values belong by definition to S .
N
C.3 Deterministic equivalent for the resolvent
In this section, we prove Theorem 10.
Lemma 21. Suppose Assumption 5 holds. Let
n
γ(S) = , µ(S) =ρMP ⊠ν , µ˜(S) =γ(S)µ(S)+(1−γ(S))δ
N N −|S| N γ(S) N N N N N 0
N
be the analogues of γ ,µ ,µ˜ defined with the dimension N −|S| in place of N. Then for any fixed ε>0
N N N
and L≥1, all large N, and all S ⊂[N] with |S|≤L,
supp(µ˜(S))⊆supp(µ˜ )+(−ε,ε)
N N
Proof. Let T and z :C\T →C be as defined by (C.25) and (C.26). Define similarly
N N N
1 (cid:90) λ
z(S)(m˜)=− +γ(S) dν (λ), z(S) :C\T →C.
N m˜ N 1+λm˜ N N N
We recall from Proposition 8 that x∈R\supp(µ˜ ) if and only if there exists m˜ ∈R\T where z (m˜)=x
N N N
and z′ (m˜)>0; the analogous characterization holds for R\supp(µ˜(S)) and z(S)(m˜).
N N N
Nowfixanyε,L>0. ByProposition17,thereisaconstantC >0suchthatsupp(µ˜(S))⊆[−C ,C ]for
0 N 0 0
all|S|≤LandalllargeN. Consideranyx∈[−C ,C ]\(supp(µ˜ )+(−ε,ε)). Then[x−ε/2,x+ε/2]⊂R\
0 0 N
supp(µ˜ ),som˜ iswell-definedandincreasingon[x−ε/2,x+ε/2]. Define[m˜ ,m˜ ]=[m˜ (x−ε/2),m˜ (x+
N N − + N N
ε/2)]. ThenProposition8impliesthatz isincreasingon[m˜ ,m˜ ],andz ([m˜ ,m˜ ])=[x−ε/2,x+ε/2].
N − + N − +
AgainbyProposition17,thereisaconstantc>0suchthat,foranysuchx∈[−C ,C ]\(supp(µ˜ )+(−ε,ε)),
0 0 N
we have
min min |1+λm˜ (y)|>c.
N
y∈[x−ε/2,x+ε/2]λ∈supp(νN)
This then implies that there is a constant C >0 for which
|z N(S)(m˜)−z N(m˜)|=|γ N(S)−γ N|·(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) 1+λ
λm˜
dν N(λ)(cid:12) (cid:12) (cid:12) (cid:12)≤ NC <ε/2
for all m˜ ∈ [m˜ ,m˜ ], |S| ≤ L, and large N. Then z(S)(m˜ ) < z (m˜ ) + ε/2 = x and z(S)(m˜ ) >
− + N − N − N +
z (m˜ ) − ε/2 = x. [SC95, Theorem 4.3] shows that if m ,m ∈ [m˜ ,m˜ ] satisfy
z(S)′
(m ) ≥ 0 and
N + 1 2 − + N 1
z(S)′
(m )≥0, then
z(S)′
(m)>0 strictly for all m∈[m ,m ]. By this and the continuity and differentiabil-
N 2 N 1 2
ity of z(S) on [m˜ ,m˜ ], there must be a point m˜ ∈(m˜ ,m˜ ) where z(S)(m˜)=x and z(S)′ (m˜)>0 strictly.
N − + − + N N
Then Proposition 8 implies that x ∈/ supp(µ˜(S)). This holds for all x ∈ [−C ,C ]\(supp(µ˜ )+(−ε,ε)),
N 0 0 N
implying supp(µ˜(S))⊆supp(µ˜ )+(−ε,ε) as desired.
N N
The following now applies Lemma 21 and Theorem 9 to extend the estimates (C.45) previously obtained
over {z ∈U (ε):Imz ≥N−1/11} to all of U (ε).
N N
32Lemma 22. Fix any ε > 0 and L ≥ 1. Then for some constants C ,c > 0, uniformly over z ∈ U (ε),
0 0 N
S ⊂[N] with |S|≤L, and i∈S, we have
1{|m˜(S)(z)|>C }≺0, 1{|m˜(S)(z)|<c }≺0, 1{∥(I+m˜(S)(z)Σ)−1∥>C }≺0,
K 0 K 0 K 0
1{∥R(S)(z)∥>C }≺0, 1{|1+N−1TrΣR(S)(z)|<c }≺0,
0 0
1{|1+N−1g⊤R(S)(z)g |<c }≺0.
i i 0
Proof. By conjugation symmetry, it suffices to show the statements for z ∈ U (ε) with Imz ≥ 0. Denote
N
for simplicity R(S) = R(S)(z) and m˜(S) = m˜(S)(z). Let S(S) = supp(µ(S))∪{0} = supp(µ˜(S))∪{0} where
K K N N N
µ(S),µ˜(S) are as defined in Lemma 21. Then Theorem 9 applied to K(S) guarantees that
N N
1{K(S) has an eigenvalue outside S(S)+(−ε/4,ε/4)}≺0,
N
uniformly over all S ⊂ [N] with |S| ≤ L. Note that S(S)+(−ε/4,ε/4) ⊆ S +(−ε/2,ε/2) by Lemma 21.
N N
Then, applying the bound ∥R(S)∥≤1/dist(z,S(S)) and the condition z ∈U (ε), we get
N N
1{∥R(S)∥>2/ε}≺0.
The remaining statements have already been shown for z ∈ U (ε) with Imz ≥ N−1/11 in (C.45). For
N
z =x+iη whereη ∈[0,N−1/11],definez′ =x+iN−1/11. OntheeventthatK(S) hasnoeigenvaluesoutside
S +(−ε/2,ε/2), both N−1TrΣR(S)(z) and m˜(S)(z) = N−1TrR(S)(z)+γ (−1/z) are C-Lipschitz over
N K N
z ∈U (ε)foraconstantC =C(ε)>0, andN−1g⊤R(S)(z)g isCN−1∥g ∥2-LipschitzwhereN−1∥g ∥2 ≺1
N i i i i
by Assumption 5. Then
N−1TrΣR(S)(z)−N−1TrΣR(S)(z′)≺N−1/11, m˜(S)(z)−m˜(S)(z′)≺N−1/11,
K K
N−1g⊤R(S)(z)g −N−1g⊤R(S)(z′)g ≺N−1/11,
i i i i
so the remaining statements of the lemma hold also for z ∈U (ε) with Imz ∈[0,N−1/11].
N
Proof of Theorem 10. Again by conjugation symmetry, it suffices to show the result for z ∈ U (ε)
N
with Imz ≥0. Denote for simplicity R(S) =R(S)(z) and m˜(S) =m˜(S)(z). The first estimate of Lemma 22
K K
implies
√
1{∥R(S)∥ >C N}≺0, 1{∥R(S)A∥ >C∥A∥ }≺0 (C.47)
F F F
uniformly over z ∈U (ε) and A∈Cn×n. Then also, by Assumption 5(c) and (C.32) applied with B =Σ,
N
(cid:16) (cid:17)
1+N−1g⊤R(i)g =1+N−1TrΣR+O N−2|1+N−1g⊤R(i)g |−1∥R(i)∥2 +∥R(i)∥
i i ≺ i i F F
(cid:16) (cid:17)
=1+N−1TrΣR+O N−1/2 . (C.48)
≺
Letd =d +d +d +d beasdefinedin(C.33)withA=I. Then,applying(C.47),(C.48),andthe
i i,1 i,2 i,3 i,4
bounds of Lemma 22, we obtain exactly as in the proof of Lemma 20 (using again the fluctuation averaging
result of Lemma 13) that, uniformly over z ∈U (ε), we have |d |,|d |,|d |≺N−1, |d |≺N−1/2, and
N i,1 i,3 i,4 i,2
(cid:12) (cid:12)
|z (m )−z|≺ 1 · 1 ·(cid:12) (cid:12)(cid:88)N d (cid:12) (cid:12)+O (cid:0) N−1(cid:1) =O (cid:0) N−1(cid:1) .
N K˜ N 1+N−1TrΣR (cid:12) i,2(cid:12) ≺ ≺
(cid:12) (cid:12)
i=1
Fix any ι > 0. If Imz ≥ N−1+ι, then this implies 1{z (m ) ∈/ C+} ≺ 0. By the same stability argument
N K˜
as in Lemma 18, we get m (z) − m˜ (z) ≺ N−1 uniformly over z ∈ U (ε) with Imz ≥ N−1+ι. For
K˜ N N
Imz ∈[0,N−1+ι],ontheeventthatalleigenvaluesofK belongtoS +(−ε/2,ε/2),wemayapplythatboth
N
m (z) and m˜ (z) are C(ε)-Lipschitz over z ∈U (ε) to compare values at z =x+iη and z′ =x+iN−1+ι.
K˜ N N
Applying m (z′)−m˜ (z′)≺N−1, we then get for any D >0, all z ∈U (ε), some constant C >0, and all
K˜ N N
large N,
P[|m (z)−m˜ (z)|>CN−1+ι]≤N−D.
K˜ N
33Since ι > 0 is arbitrary, this shows m (z) − m˜ (z) ≺ N−1 uniformly over z ∈ U (ε). The bound
K˜ N N
m (z)−m (z)≺N−1 then follows from m (z)=γ m (z)+(1−γ )(−1/z) and m˜ (z)=γ m (z)+
K N K˜ N K N N N N
(1−γ )(−1/z).
N
For the estimate of TrRA, we apply the definition of d = d +d +d +d from (C.33) and the
i i,1 i,2 i,3 i,4
identity (C.30) now with this matrix A. Then (C.30) gives
N
Tr(cid:104)
RA−(−zI−zm
Σ)−1A(cid:105)
=
1(cid:88) d
i .
K˜ z 1+N−1g⊤R(i)g
i=1 i i
Applying (C.47), (C.48), and the bounds of Lemma 22 to (C.34)–(C.35), uniformly over z ∈ U (ε) and
N
A∈Cn×n, we have |d |,|d |,|d |≺N−3/2∥A∥ , |d |≺N−1∥A∥ , and hence
i,1 i,3 i,4 F i,2 F
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)N d
i
(cid:12)
(cid:12)≺
1 (cid:12) (cid:12)(cid:88)N
d
(cid:12)
(cid:12)+O
(cid:16)
N−1/2∥A∥
(cid:17)
.
(cid:12)
(cid:12)
i=1
1+N−1g⊤
i
R(i)g i(cid:12)
(cid:12)
1+N−1TrΣR(cid:12)
(cid:12)
i=1
i,2(cid:12)
(cid:12)
≺ F
√
Finally, applying Lemma 13 with Γ=zI, Ψ (Γ)=C N, and Φ (Γ,A)=C∥A∥ (where we may assume
N N F
without loss of generality ∥A∥ ∈(N−υ,Nυ) by scale invariance of the desired estimate with respect to A),
F
we get |(cid:80) d |≺N−1/2∥A∥ . Thus,
i i,2 F
(cid:104) (cid:105) 1
Tr RA−(−zI−zm Σ)−1A ≺ √ ∥A∥ .
K˜
N
F
D Analysis of spiked eigenstructure
We now consider the asymptotic setup of Appendix B.2 and prove Corollary 11 and Theorem 12. As all the
desired statements are invariant under conjugation of Σ by an orthogonal matrix, we may assume without
loss of generality that Σ is diagonal and of the form
(cid:18) (cid:19)
Σ 0
Σ= r , Σ =diag(λ (Σ),...,λ (Σ)), Σ =diag(λ (Σ),...,λ (Σ)).
0 Σ r 1 r 0 r+1 n
0
Denote the block decomposition of G corresponding to Σ ,Σ as
r 0
G=[G ,G ], G ∈RN×r, G ∈RN×(n−r).
r 0 r 0
We remind the reader that G and G need not be independent.
r 0
D.1 No outliers outside the limit support
We consider first the setting of r = 0, and prove Corollary 11 together with some uniform convergence
properties of m˜ and z that will be used in the later analysis.
N N
Recall the domain T and function z : C\T → C from (C.25) and (C.26), and their asymptotic
N N N
analogues T and z :C\T →C from (A.2) and (A.3).
Lemma 23. Suppose Assumption 5 holds, and Assumption 6 holds with r =0. Then, as N →∞,
(a) z (m˜) and its derivative z′ (m˜) converge uniformly over compact subsets of C\T to z(m˜) and z′(m˜).
N N
(b) For any ε>0 and all large N,
supp(µ˜ )⊆supp(µ˜)+(−ε,ε).
N
(c) m˜ (z) and its derivative m˜′ (z) converge uniformly over compact subsets of C\supp(µ˜) to m˜(z) and
N N
m˜′(z).
34Proof. For part (a), let K ⊂C\T be any fixed compact set. Then K does not intersect some sufficiently
smallopenneighborhoodofthecompactdomainT. IfAssumption6holdswithr =0, thenT iscontained
N
in this open neighborhood of T for all large N, so K ⊂ C\T , and both z and z are well-defined on K.
N N
The pointwise convergences z (m˜) → z(m˜) and z′ (m˜) → z′(m˜) on K then follow from γ → γ, the weak
N N N
convergence ν →ν, and the uniform boundedness of the functions λ(cid:55)→λ/(1+λm˜) and λ(cid:55)→λ2/(1+λm˜)2
N
on an open neighborhood of supp(ν), for m˜ ∈ K. This convergence is furthermore uniform because {z }
N
and {z′ } are both equicontinuous over K.
N
Forpart(b),consideranyx∈/ supp(µ˜)+(−ε,ε). Then[x−ε/2,x+ε/2]⊂R\supp(µ˜),som˜ iswell-defined
and increasing on [x−ε/2,x+ε/2]. Let [m˜ ,m˜ ] = [m˜(x−ε/2),m˜(x+ε/2)]. Then by Proposition 8,
− +
z′(m˜)>0 for all m˜ ∈[m˜ ,m˜ ], and z([m˜ ,m˜ ])=[x−ε/2,x+ε/2]. The uniform convergence in part (a)
− + − +
implies for all large N that z (m˜ ) < x, z (m˜ ) > x, and z′ (m˜) > 0 for all m˜ ∈ [m˜ ,m˜ ]. Then there
N − N + N − +
exists m˜ ∈ [m˜ ,m˜ ] where z (m˜) = x and z′ (m˜) > 0, implying by Proposition 8 that x ∈/ supp(µ˜ ). So
− + N N N
supp(µ˜ )⊆supp(µ˜)+(−ε,ε) as desired.
N
For part (c), let K ⊂C\supp(µ˜) be any fixed compact set. Then K does not intersect some sufficiently
small open neighborhood of the compact set supp(µ˜), so the inclusion of part (b) implies K ⊂C\supp(µ˜ )
N
for all large N, and both m˜ and m˜ are well-defined on K. The uniform convergence m˜ (z) → m˜(z) and
N N
m˜′ (z) → m˜′(z) on K then follow from the weak convergence µ˜ → µ˜, the uniform boundedness of the
N N
functions λ (cid:55)→ 1/(λ−z) and λ (cid:55)→ 1/(λ−z)2 on an open neighborhood of supp(µ˜) for z ∈ K, and the
equicontinuity of {m˜ } and {m˜′ } on K.
N N
Proof of Corollary 11. By Lemma 23(b), for any fixed ε > 0, we have S +(−ε/2,ε/2) ⊆ S +(−ε,ε)
N
for all large N. Then by Theorem 9,
1{K has an eigenvalue in R\(S+(−ε,ε))
≤1{K has an eigenvalue in R\(S +(−ε/2,ε/2))}≺0.
N
D.2 Deterministic equivalents for generalized resolvents
We next introduce two generalized resolvents for the matrix K, and extend Theorem 10 to establish deter-
ministic equivalents for these generalized resolvents.
Define the spectral domain
(cid:110) (cid:111)
U(ε)= z ∈C:|z|≤ε−1, dist(z,S)≥ε
where S is the limit support set defined in (B.2). Given z ∈U(ε) and α∈C, define a diagonal matrix
(cid:18) (cid:19) (cid:18) (cid:19)
(z+α)I 0 I
Γ:=Γ(z,α)=zI +αV V⊤ = r ∈Cn×n, V = r ∈Rn×r. (D.1)
n r r 0 zI r 0
n−r
Define the first generalized resolvent
(cid:18) −Γ G⊤ (cid:19)−1
R(z,α)= ∈C(n+N)×(n+N). (D.2)
G −I
N
This matrix inverse exists if and only if the Schur complement G⊤G−Γ=K−Γ for its lower right block
is invertible, in which case the upper-left block of R(z,α) is R(Γ) = (K −Γ)−1. The following provides a
deterministic equivalent for this block of R(z,α).
Lemma 24. Under the assumptions of Theorem 12, for any fixed ε > 0, there exist C ,α > 0 (depending
0 0
on ε) such that fixing any α∈C with |α|>α , the following hold:
0
(a) The event
(cid:110) (cid:111)
E = R(z,α) exists and ∥R(z,α)∥≤C for all z ∈U(ε)
0
satisfies 1{Ec}≺0.
35(b) Uniformly over z ∈U(ε) and deterministic unit vectors v ,v ∈Rn,
1 2
(cid:13) (cid:18) (cid:19) (cid:13)
(cid:13) (cid:13) (cid:13)(cid:0) v⊤
1
0(cid:1) R(z,α) v 02 +v⊤ 1(Γ+z·m˜ N,0(z)Σ)−1v 2(cid:13) (cid:13) (cid:13)≺ √1 N.
In the setting of Theorem 12(c), let u= √1 (u 1,...,u N)∈RN be the additional given vector for which
N
{(u ,g⊤)}N are independent vectors in Rn+1. For z ∈U(ε) and α∈C, define
j j j=1
(cid:18)E[u2] E[ug]⊤(cid:19)
Σ(cid:101) =
E[ug] Σ
∈R(n+1)×(n+1),
(cid:18) (cid:19)
z+α 0
Γ(cid:101) =Γ(cid:101)(z,α)= ∈C(n+1)×(n+1)
0 Γ
where E[u2] and E[ug] denote the common values of E[u2] and E[u g ] for j = 1,...,N. Define the second
j j j
generalized resolvent
(cid:18) (cid:19)−1  −(z+α) 0 u⊤ −1
R(cid:101)(z,α)=
−Γ(cid:101) [u,G]⊤
= 0 −Γ G⊤  ∈C(n+1+N)×(n+1+N). (D.3)
[u,G] −I
u G −I
N
We have the following deterministic equivalent for the upper-left block of R(cid:101)(z,α), which is analogous to
Lemma 24.
Lemma 25. UndertheassumptionsofTheorem12(c), foranyfixedε>0, thereexistC ,α >0(depending
0 0
on ε) such that fixing any α∈C with |α|>α , the following hold:
0
(a) The event
(cid:110) (cid:111)
E(cid:101)= R(cid:101)(z,α) exists and ∥R(cid:101)(z,α)∥≤C
0
for all z ∈U(ε)
satisfies 1{E(cid:101)c}≺0.
(b) Uniformly over z ∈U(ε) and deterministic unit vectors v ,v ∈Rn+1,
1 2
(cid:13) (cid:13) (cid:13) (cid:13)(cid:0) v⊤
1
0(cid:1) R(cid:101)(z,α)(cid:18) v 02(cid:19) +v⊤ 1(cid:16) Γ(cid:101)+z·m˜ N,0(z)Σ(cid:101)(cid:17)−1 v 2(cid:13) (cid:13) (cid:13) (cid:13)≺ √1 N.
In the remainder of this section, we prove Lemmas 24 and 25. Recall
µ =ρMP ⊠ν , µ˜ =γ µ +(1−γ )δ .
N,0 γN,0 N,0 N,0 N,0 N,0 N,0 0
Define the bulk components of the sample covariance and Gram matrices
K
0
=G⊤ 0G
0
∈R(n−r)×(n−r), K(cid:102)0 =G 0G⊤
0
∈RN×N.
Define also the N-dependent bulk spectral support and spectral domain
S =supp(µ )∪{0}=supp(µ˜ )∪{0},
N,0 N,0 N,0
U (ε)={z ∈C:|z|≤ε−1,dist(z,S )≥ε}.
N,0 N,0
Lemma 23(b) shows S ⊆ S +(−ε/2,ε/2) for any fixed ε > 0 and all large N, so also U(ε) ⊆ U (ε/2)
N,0 N,0
foralllargeN. Thus, theresultsofAppendixCappliedtoK , whichholduniformlyoverz ∈U (ε/2)for
0 N,0
any fixed ε>0, also hold uniformly over z ∈U(ε). In particular, the following is an immediate consequence
of Corollary 11 and Theorem 10, which we record here for future reference.
36Lemma 26. Suppose Assumptions 5 and 6 hold. Then for any fixed ε>0,
1{K has an eigenvalue outside S+(−ε,ε)}≺0.
0
Furthermore, uniformly over z ∈U(ε),
m −m (z)≺1/N, m −m˜ (z)≺1/N.
K0 N,0 K˜
0
N,0
We now check that for sufficiently large |α|, the generalized resolvent R(z,α) exists and has bounded
operator norm with high probability.
Proof of Lemma 24(a). Let
(cid:110) √ (cid:111)
E′ = all eigenvalues of K belong to S+(−ε/2,ε/2), and ∥G∥< B .
0
By Assumption 5(b) and Lemma 26, 1{E′c} ≺ 0, so it suffices to show E′ ⊆ E. On this event E′, for any
z ∈U(ε), we have that each eigenvalue of K is separated by at least ε/2 from z. Then,
0
(cid:18) −zI G⊤ (cid:19)−1
R (z):= n−r 0 ∈C(n−r+N)×(n−r+N)
0 G −I
0 N
exists for all z ∈ U(ε) because the Schur complement K −zI of its lower-right block is invertible.
0 n−r √
Furthermore, denoting R =(K −zI )−1, we have ∥R ∥≤2/ε and ∥G ∥≤∥G∥< B, so
0 0 n−r 0 0
∥R 0(z)∥=(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) GR R0 G RR G0G ⊤⊤ 0 −I (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)≤C 1 (D.4)
0 0 0 0 0 N
for some constant C depending only on ε,B.
1
Now write R(z,α) as defined in (D.2) in its block decomposition with blocks of sizes r and n−r+N.
Then the Schur complement of the upper left block of size r×r is given by
(cid:18) (cid:19)
S
=−(cid:0)
0
G⊤(cid:1)
R (z)
0
−(α+z)I . (D.5)
r 0 G r
r
Notice that
(cid:18) (cid:19) (cid:18) (cid:19)
SS∗ = |α+z|2I +(cid:0) 0 G⊤(cid:1) R (z) 0 (cid:0) 0 G⊤(cid:1) R (z) 0
r r 0 G r 0 G
r r
(cid:18) (cid:19) (cid:18) (cid:19)
+(α¯+z¯)(cid:0)
0
G⊤(cid:1)
R (z)
0 +(α+z)(cid:0)
0
G⊤(cid:1)
R (z)
0
r 0 G r 0 G
r r
√
where the first two terms are positive semi-definite. Therefore, applying (D.4) and ∥G ∥ ≤ ∥G∥ < B on
r
the event E′, there exist α ,c >0 depending only on ε,B, such that
0 0
λ (SS∗)≥|α+z|2−2(|α|+|z|)∥G ∥2∥R (z)∥>c
min r 0 0
for any z ∈ U(ε) and |α| > α . Consequently, under the event E′, the Schur complement S in (D.5) is
0
invertible with ∥S−1∥<c−1/2. Then R(z,α) exists, and
0
(cid:13) (cid:13) (cid:13) S (cid:18)−1
(cid:19)
−S−1 (cid:18)(cid:0) 0 (cid:19)G⊤
r
(cid:1) R 0(z) (cid:13) (cid:13)
(cid:13)
∥R(z,α)∥=(cid:13) (cid:13) (cid:13) −R 0(z) G0
r
S−1 R 0(z)+R 0(z) G0
r
S−1(cid:0) 0 G⊤
r
(cid:1) R 0(z)(cid:13) (cid:13) (cid:13)≤C 0
for a constant C >0 depending only on ε,B. This shows E′ ⊆E as desired.
0
For the matrix Γ = Γ(z,α) in (D.1), recall the definitions of R(S)(Γ) and m˜(S)(Γ) from (C.1). The
K
following provides an analogue of Lemma 22 for these quantities.
37Lemma 27. Fix any ε > 0 and L ≥ 1. Then there exist C ,c ,α > 0 such that for any fixed α ∈ C with
0 0 0
|α|>α , uniformly over S ⊂[N] with |S|≤L, over j ∈S, and over z ∈U(ε),
0
1{|m˜(S)(Γ)|>C }≺0, 1{|m˜(S)(Γ)|<c }≺0, 1{∥(z−1Γ+m˜(S)(Γ)Σ)−1∥>C }≺0,
K 0 K 0 K 0
1{∥R(S)(Γ)∥>C }≺0, 1{|1+N−1TrΣR(S)(Γ)|<c }≺0,
0 0
1{|1+N−1g⊤R(S)(Γ)g |<c }≺0.
j j 0
Proof. Suppose |α| is large enough so that Lemma 24(a) holds. Since R(Γ) is the upper-left block of
R(z,α), Lemma 24(a) applied with G(S) in place of G shows that 1{∥R(S)(Γ)∥ > C } ≺ 0 for a constant
0
C > 0, uniformly over S ⊂ [N] with |S| ≤ L and over z ∈ U(ε). For the remaining statements, let
0
G(S) ∈R(N−|S|)×(n−r) be the submatrix of G with the rows of S removed, and define
0 0
K(S) =G(S)⊤ G(S), K(cid:102)(S) =G(S)G(S)⊤ ,
0 0 0 0 0 0
1 (cid:16) 1(cid:17)
R(S) =(K(S)−zI )−1, m(S) = TrR(S), m˜(S) =γ m(S) +(1−γ ) − .
0 0 n−r K0 n−r 0 K0 N,0 K0 N,0 z
Then by Lemma 26 applied to K(S), also 1{∥R(S)∥>C }≺0 for a constant C >0.
0 0 0 0
Using these bounds, we first show the comparisons
(cid:12) (cid:12)
|m˜(S)(Γ)−m˜(S)|≺1/N, (cid:12)N−1TrΣR(S)(Γ)−N−1TrΣ R(S)(cid:12)≺1/N. (D.6)
K K0 (cid:12) 0 0 (cid:12)
For the first comparison, notice that in the decompositions into blocks of sizes r and n−r,
n−r 1 (cid:18)0 0 (cid:19)
m(S) = Tr
n K0 n 0 R(S)
0
and
1
m(S)(Γ)= TrR(S)(Γ)
K n
(cid:32) (cid:33)−1
1 G(S)⊤G(S)−(α+z)I G(S)⊤G(S)
= Tr r r r r 0
n G(S)⊤G(S) K(S)−zI
0 r 0 N−|S|
(cid:32) (cid:33)
1 (S(S))−1 −(S(S))−1G(S)⊤G(S)R(S)
= Tr r r r 0 0 ,
n −R(S)G(S)⊤G(S)(S(S))−1 R(S)+R(S)G(S)⊤G(S)(S(S))−1G(S)⊤G(S)R(S)
0 0 r r 0 0 0 r r r 0 0
where
S(S) :=G(S)⊤G(S)−(α+z)I −G(S)⊤G(S)R(S)G(S)⊤G(S)
r r r r r 0 0 0 r
is the Schur complement of the lower-right block. We have ∥(S(S))−1∥ ≤ ∥R(S)(Γ)∥ ≺ 1, ∥R(S)∥ ≺ 1, and
r 0
by Assumption 5, ∥G(S)∥≺1 and ∥G(S)∥≺1. Combining these bounds and using |TrA|≤r∥A∥ when A
0 r
has rank r (as follows from the von Neumann trace inequality),
(cid:12) (cid:12)
(cid:12) (cid:12)m(S)(Γ)− n−r m(S)(cid:12)
(cid:12)
(cid:12) K n K0(cid:12)
(cid:12) (cid:32) (cid:33)(cid:12)
(cid:12)1 (S(S))−1 −(S(S))−1G(S)⊤G(S)R(S) (cid:12)
=(cid:12) Tr r r r 0 0 (cid:12)
(cid:12) (cid:12)n −R( 0S)G( 0S)⊤G( rS)(S( rS))−1 R( 0S)G( 0S)⊤G( rS)(S( rS))−1G( rS)⊤G( 0S)R( 0S) (cid:12)
(cid:12)
1 1
≤ |Tr(S(S))−1|+ |TrR(S)G(S)⊤G(S)(S(S))−1G(S)⊤G(S)R(S)|
n r n 0 0 r r r 0 0
r r
≤ ∥(S(S))−1∥+ ∥G(S)∥2∥G(S)∥2∥R(S)∥2∥(S(S))−1∥≺1/N.
n r n r 0 0 r
Then also |m(S)(Γ)−m(S)| ≺ 1/N since |m(S)| ≤ ∥R(S)∥ ≺ 1 and (n−r)/n = 1+O (1/N). Hence
K K0 K0 0 ≺
|m˜(S)(Γ)−m˜(S)|≺1/N fromthedefinitionsm˜(S)(Γ)=γ m(S)(Γ)+(1−γ )(−1/z)andm˜(S) =γ m(S)+
K K0 K N K N K0 N,0 K0
38(1−γ )(−1/z), as |1/z|≤ε for z ∈U(ε) and γ =γ +O (1/N). The proof of the second comparison
N,0 N,0 N ≺
of (D.6) is analogous, considering in addition
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
1 0 0 1 Σ 0 r 1
Tr Σ− R(S)(Γ)= Tr r R(S)(Γ)≤ ∥Σ ∥∥R(S)(Γ)∥≺ . (D.7)
n 0 Σ 0 n 0 0 n r N
Now, Lemma 22 applied with K shows, uniformly over S ⊂[N] with |S|≤L and over z ∈U(ε),
0
1{|m˜(S)|>C }≺0, 1{|m˜(S)|<c }≺0, 1{|1+N−1TrΣ R(S)|<c }≺0,
K0 0 K0 0 0 0 0
which together with (D.6) implies
1{|m˜(S)(Γ)|>C }≺0, 1{|m˜(S)(Γ)|<c }≺0, 1{|1+N−1TrΣR(S)(Γ)|<c }≺0
K 0 K 0 0
for adjusted constants C ,c >0. Also by Assumption 5, uniformly over j ∈S,
0 0
N−1g⊤R(S)(Γ)g −N−1TrΣR(S)(Γ)≺N−1∥R(S)(Γ)∥ ≤N−1/2∥R(S)(Γ)∥≺N−1/2, (D.8)
j j F
so 1{|1+N−1g⊤R(S)(Γ)g | < c} ≺ 0 for a constant c > 0. Lastly, from the definition of Γ = Γ(z,α) in
j j
(D.1), we have
(cid:32) (cid:33)
m˜(S)(Γ)Σ +(α +1)I 0
z−1Γ+m˜(S)(Γ)Σ= K r z r .
K 0 m˜(S)(Γ)Σ +I
K 0 n−r
By (D.6) and Lemma 22, we have
1(cid:26)(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) m˜( KS)(Γ)Σ 0+I n−r(cid:17)−1(cid:13) (cid:13) (cid:13) (cid:13)>C(cid:27) ≺0 (D.9)
for some constant C > 0. We have already proved 1{|m˜(S)(Γ)| > C } ≺ 0, and applying ∥Σ ∥ ≤ C under
K 0 r
Assumption 5, we can deduce for the smallest singular value that
(cid:16) (cid:17) |α|
σ m˜(S)(Γ)Σ +(α/z+1)I ≥ −1−|m˜(S)(Γ)|∥Σ ∥≥c (D.10)
min K r r |z| K r
on the event {|m˜(S)(Γ)| ≤ C }, for any z ∈ U(ε), |α| ≥ α , and some α ,c > 0 depending on ε,C . Thus
K 0 0 0 0
also
1{∥(z−1Γ+m(S)(Γ)Σ)−1∥>C}≺0 (D.11)
K˜
for a constant C >0, showing all statements of the lemma.
Proof of Lemma 24(b). Recalling the form of R(z,α) in (D.2), the quantity we wish to approximate is
(cid:18) (cid:19)
(cid:0) v⊤ 0(cid:1) R(z,α) v 2 =v⊤R(Γ)v =v⊤(K−Γ)−1v .
1 0 1 2 1 2
Analogous to (C.28) in the proof of Lemma 18, for any matrix B ∈Cn×n, we have
1 (cid:88)N g⊤R(i)(Γ)Bg
TrB =Tr(K−Γ)R(Γ)B =−TrR(Γ)BΓ+ i i . (D.12)
N 1+N−1g⊤R(i)(Γ)g
i=1 i i
Applying the definition m (Γ) = N−1TrR(Γ)+(1−γ )(−1/z) and the identity (D.12) with B = I, we
K˜ N
obtain analogously to (C.29) that
1 1
m (Γ)=(1−γ )(−1/z)+ TrR(Γ)Γ− Tr(z−1Γ−I)R(Γ)
K˜ N Nz N
39N
1 (cid:88) 1 1
=− − Tr(z−1Γ−I)R(Γ).
Nz 1+N−1g⊤R(i)(Γ)g N
i=1 i i
Then, noting that z−1Γ−I has rank r and hence |N−1Tr(z−1Γ−I)R(Γ)| ≤ r |α|∥R(Γ)∥ ≺ N−1, this
N |z|
gives
N
m (Γ)=−
1 (cid:88) 1
+O
(cid:0) N−1(cid:1)
. (D.13)
K˜ Nz 1+N−1g⊤R(i)(Γ)g ≺
i=1 i i
Fixing the unit vectors v ,v ∈Rn, let us now choose A=v v⊤ and B =A(z−1Γ+m (Γ)·Σ)−1 in
1 2 2 1 K˜
(D.12), and define
1 1
d = g⊤R(i)(Γ)Bg − TrR(Γ)BΣ
i N i i N
= 1 g⊤R(i)(Γ)A(cid:0) z−1Γ+m (Γ)Σ(cid:1)−1 g − 1 TrR(Γ)A(cid:0) z−1Γ+m (Γ)Σ(cid:1)−1 Σ.
N i K˜ i N K˜
Then, combining (D.12) and (D.13), we get
v⊤(z−1Γ+m (Γ)Σ)−1v =TrB
1 K˜ 2
N
=−TrR(Γ)BΓ+TrR(Γ)BΣ·(cid:0) −zm (Γ)+O (cid:0) N−1(cid:1)(cid:1) +(cid:88) d i
K˜ ≺ 1+N−1g⊤R(i)(Γ)g
i=1 i i
N
=−z·v⊤R(Γ)v +(cid:88) d i +O (cid:0) N−1(cid:1) , (D.14)
1 2 1+N−1g⊤R(i)(Γ)g ≺
i=1 i i
wherethelastequalityappliesthedefinitionofBtocombinethefirsttwoterms,andappliesalso|TrR(Γ)BΣ|≤
∥(z−1Γ+m (Γ)Σ)−1ΣR(Γ)∥≺1 by Lemma 27 to obtain the O (cid:0) N−1(cid:1) remainder.
K˜ ≺
Considering a similar decomposition as in Lemma 18, we define d =d +d +d +d where
i i,1 i,2 i,3 i,4
1 1
d = g⊤R(i)(Γ)A(z−1Γ+m (Γ)Σ)−1g − g⊤R(i)(Γ)A(z−1Γ+m(i)(Γ)Σ)−1g ,
i,1 N i K˜ i N i K˜ i
1 1
d = g⊤R(i)(Γ)A(z−1Γ+m(i)(Γ)Σ)−1g − TrΣR(i)(Γ)A(z−1Γ+m(i)(Γ)Σ)−1,
i,2 N i K˜ i N K˜
1 1
d = TrΣR(i)(Γ)A(z−1Γ+m(i)(Γ)Σ)−1− TrΣR(Γ)A(z−1Γ+m(i)(Γ)Σ)−1,
i,3 N K˜ N K˜
1 1
d = TrΣR(Γ)A(z−1Γ+m(i)(Γ)Σ)−1− TrΣR(Γ)A(z−1Γ+m (Γ)Σ)−1.
i,4 N K˜ N K˜
For A=v v⊤, by the bound 1{∥R(S)(Γ)∥>C }≺0 from Lemma 27, we have for a constant C >0 that
2 1 0
(cid:110)(cid:13) (cid:13) √ (cid:111) (cid:110)(cid:13) (cid:13) (cid:111)
1 (cid:13)R(S)(Γ)(cid:13) >C N ≺0, 1 (cid:13)R(S)(Γ)A(cid:13) >C ≺0
(cid:13) (cid:13) (cid:13) (cid:13)
F F
uniformly over z ∈ U(ε). Then, employing Lemma 27 and the same bounds as (C.34)–(C.35) from the
proof of Lemma 22 (where here, the bounds for ∥R(i)A∥ ,∥RA∥ are improved by a factor of N−1/2
F F
because A is low-rank), we conclude that |d |,|d |,|d |≺N−3/2 and |d |≺N−1. Hence, applying also
i,1 i,3 i,4 i,2
1+N−1g⊤R(i)(Γ)g =1+N−1TrΣR(Γ)+O (cid:0) N−1/2(cid:1) as follows from (D.8) and the bound (C.32),
i i ≺
N N
(cid:88) d i = 1 ·(cid:88) d +O (cid:16) N−1/2(cid:17) .
1+N−1g⊤R(i)(Γ)g 1+N−1TrΣR(Γ) i,2 ≺
i=1 i i i=1
√
(cid:80)
By Lemma 13 applied with Ψ (Γ) = C N and Φ (Γ,A) = C for a constant C > 0, we have | d | ≺
N−1/2. Thus the above quantiN ty is of size O (cid:0) N−1N /2(cid:1) , so applying this back to (D.14), i i,2
≺
v⊤(Γ+zm (Γ)·Σ)−1v +v⊤R(Γ)v ≺N−1/2.
1 K˜ 2 1 2
40Finally, from (D.6) and Lemma 26 we have m (Γ) = m˜ (z)+O
(cid:0) N−1(cid:1)
, and applying this above com-
K˜ N,0 ≺
pletes the proof.
Proof of Lemma 25. TheproofissimilartoLemma24,replacingr andnthroughoutbyr+1andn+1,
G(S) by [u(S),G(S)], Σ by Σ(cid:101), and R(S)(Γ) and m˜(S)(Γ) by
r r K
R(S)(Γ(cid:101))=(cid:0) [u(S),G(S)]⊤[u(S),G(S)]−Γ(cid:101)(cid:1)−1 , m˜(S)(Γ(cid:101))= 1 TrR(S)(Γ(cid:101))+(cid:16) 1− n+1(cid:17)(cid:18) −1(cid:19) .
K N N z
The only difference here is that Σ(cid:101) is no longer diagonal, leading to the following minor modifications of the
preceding proof: The bound
  
0 0 0
1 1
TrΣ(cid:101) −0 0 0 R(S)(Γ(cid:101))≺
n+1 N
0 0 Σ
0
analogous to (D.7) follows upon noting that (with E[ug]⊤ =(cid:0)E[ug ]⊤ E[ug ]⊤(cid:1) )
r 0
 0 0 0  E[u2] E[ug ]⊤ E[ug ]⊤
r 0
Σ(cid:101) −0 0 0 =E[ug r] Σ r 0 
0 0 Σ E[ug ] 0 0
0 0
still is of low rank, with rank at most r+2. Writing as shorthand m˜(S) =m˜(S)(Γ(cid:101)), the bound
K K
1{∥(z−1Γ(cid:101)+m˜( KS)Σ(cid:101))−1∥>C 0}≺0
analogous to (D.11) follows from

m˜(S)E[u2]+ α +1 m˜(S)E[ug ]⊤ m˜(S)E[ug
]⊤−1
K z K r K 0
(z−1Γ(cid:101)+m˜(S)Σ(cid:101))−1 =  m˜(S)E[ug ] m˜(S)Σ +(α +1)I 0  ,
K  K r K r z r 
m˜(S)E[ug ] 0 m˜(S)Σ +I
K 0 K 0
the bound 1{∥m˜(S)Σ +I∥−1 > C} ≺ 0 for the lower-right block as follows from (D.9), and the bound for
K 0
the inverse of its Schur-complement
(cid:40)(cid:13)(cid:34)(cid:32) (cid:33)
(cid:13) m˜(S)E[u2]+ α +1 m˜(S)E[ug ]⊤
1 (cid:13) K z K r
(cid:13) (cid:13) m˜( KS)E[ug r] m˜( KS)Σ r+(α z +1)I r
−(cid:18) m˜( KS)E 0[ug 0]⊤(cid:19) (m˜( KS)Σ 0+I)−1(cid:16) m˜( KS)E[ug 0] 0(cid:17)(cid:35)−1(cid:13) (cid:13) (cid:13) (cid:13)>C(cid:41) ≺0
(cid:13)
whichholdsuniformlyoverz ∈U(ε)forany|α|>α sufficientlylarge,byanargumentanalogousto(D.10).
0
The remainder of the proof is identical to that of Lemma 24, and we omit the details.
D.3 Analysis of outliers
Let V r,Γ(z,α),R(z,α),R(cid:101)(z,α) be as defined in the preceding section. Consider the decomposition of
R(cid:101)(z,α) as in (D.3) into its blocks of dimensions 1, n, and N, and define
 ⊤  
1 1
1
R(cid:101)11(z,α):=0 R(cid:101)(z,α)0= −z−α+u⊤u−u⊤G(G⊤G−Γ(z,α))−1G⊤u, (D.15)
0 0
41 ⊤  
1 0
(cid:16) (cid:17)−1
R(cid:101)1V(z,α):=0 R(cid:101)(z,α)V r=−R(cid:101)11(z,α)·u⊤G G⊤G−Γ(z,α) V r, (D.16)
0 0
where the second equalities follow from block matrix inversion of the lower 2×2 blocks of R(cid:101)(z,α), followed
by block matrix inversion of the full matrix R(cid:101)(z,α). Set
(cid:18) (cid:19)
M (z,α)=I +α(cid:0) V⊤ 0(cid:1) R(z,α) V r .
K r r 0
Proposition 28. Fix any ε>0 and any α∈R sufficiently large that satisfies Lemmas 24 and 25. Then on
the event E ∩E(cid:101)of Lemmas 24 and 25, for all sufficiently large N,
(a) λ(cid:98) ∈ U(ε)∩R is an eigenvalue of G⊤G if and only if detM K(λ(cid:98),α) = 0, and its multiplicity as an
eigenvalue of G⊤G equals the dimension of kerM K(λ(cid:98),α).
(b) Letv ∈Rn beauniteigenvectorofG⊤G(i.e.rightsingularvectorofG)correspondingtoaneigenvalue
(cid:98)
λ(cid:98)∈U(ε)∩R. Then V⊤
r
v
(cid:98)
is a non-zero vector in kerM K(λ(cid:98),α), and
1
(cid:18)
V
(cid:19)⊤ (cid:18)
I
0(cid:19) (cid:18)
V
(cid:19)
α2
=v (cid:98)⊤V
r
0r R(λ(cid:98),α) 0n
0
R(λ(cid:98),α) 0r V⊤
r
v
(cid:98)
(D.17)
For any vector v ∈Rn, we have
(cid:18) (cid:19)
v⊤v (cid:98)+α(cid:0) v⊤ 0(cid:1) R(λ(cid:98),α) V 0r V⊤
r
v
(cid:98)
=0. (D.18)
(c) Let u be as in Theorem 12(c), and let uˆ ∈RN be a unit eigenvector of GG⊤ (i.e. left singular vector
of G) corresponding to the eigenvalue λ(cid:98)∈U(ε)∩R. Then
α
u⊤u
(cid:98)
= R(cid:101)1V(λ(cid:98),α)V⊤
r
v (cid:98). (D.19)
λ(cid:98)1/2R(cid:101)11(λ(cid:98),α)
Proof. For part (a), note that if λ(cid:98) is an eigenvalue of G⊤G, i.e. λ(cid:98)1/2 is a singular value of G with left and
right unit singular vectors u and v, then
(cid:98) (cid:98)
0=(cid:18) −λ(cid:98)I n G⊤ (cid:19)(cid:18) v (cid:98) (cid:19)
G −I
N
λ(cid:98)1/2u
(cid:98)
which implies, for any α∈R,
−α(cid:18) V r(cid:19) ·V⊤v =(cid:18) −λ(cid:98)I n−αV rV⊤ r G⊤ (cid:19)(cid:18) v (cid:98) (cid:19) .
0 r (cid:98) G −I
N
λ(cid:98)1/2u
(cid:98)
Fixing α∈R large enough, on the event E of Lemma 24, the generalized resolvent
R(λ(cid:98),α)=(cid:18) −λ(cid:98)I n−αV rV⊤
r
G⊤ (cid:19)−1
G −I
N
exists, and multiplying both sides by R(λ(cid:98),α) gives
(cid:18) (cid:19) (cid:18) (cid:19)
v V
λ(cid:98)1/(cid:98)
2u
(cid:98)
=−αR(λ(cid:98),α) 0r ·V⊤
r
v (cid:98). (D.20)
Then, multiplying by (V⊤
r
0) on both sides and re-arranging, we get M K(λ(cid:98),α)·V⊤
r
v
(cid:98)
=0.
42We remark that if λ(cid:98) is an eigenvalue of multiplicity k, and G has corresponding linearly independent
left singular vectors u (cid:98)1,...,u
(cid:98)k
and right singular vectors v (cid:98)1,...,v (cid:98)k, then the vectors {(v (cid:98)j,λ(cid:98)1/2u (cid:98)j)}k
j=1
on
the left side of (D.20) are linearly independent, implying that the vectors {V⊤v }k on the right side must
r (cid:98)j j=1
also be (non-zero and) linearly independent vectors in kerM K(λ(cid:98),α). Conversely, if {y j}k
j=1
are linearly
independent vectors in kerM K(λ(cid:98),α), then defining
(cid:18) (cid:19) (cid:18) (cid:19)
v V
(cid:98)j =−αR(λ(cid:98),α) r ·y
λ(cid:98)1/2u
(cid:98)j
0 j
and multiplying by (V⊤
r
0), we must have V⊤
r
v
(cid:98)j
=(−M K(λ(cid:98),α)+I)y
j
=y j. Thus the pairs (v (cid:98)j,λ(cid:98)1/2u (cid:98)j)
are linearly independent vectors satisfying (D.20), and multiplying by R(λ(cid:98),α)−1 and rearranging shows
that λ(cid:98)1/2 must be a singular value of G with multiplicity at least k, with corresponding singular vectors
{(v ,u )}k . This establishes part (a).
(cid:98)j (cid:98)j j=1
For part (b), the above argument has shown V⊤
r
v
(cid:98)
∈kerM K(λ(cid:98),α). Multiplying (D.20) on the left by
(cid:18) (cid:19)
I 0
n
0 0
and taking the squared norm (noting that λ(cid:98),α and R(λ(cid:98),α) here are real) shows (D.17). Multiplying (D.20)
on the left by (v⊤ 0) shows (D.18). For part (c), multiplying (D.20) by (0u⊤), we have
(cid:18) 0(cid:19)⊤ (cid:18) V (cid:19) (cid:16) (cid:17)−1
λ(cid:98)1/2u⊤u
(cid:98)
=−α
u
R(λ(cid:98),α) 0r ·V⊤
r
v
(cid:98)
=−αu⊤G G⊤G−Γ(λ(cid:98),α) V r·V⊤
r
v
(cid:98)
where the second equality follows from the block matrix inversion of R(λ(cid:98),α). Then, recalling the forms of
R(cid:101)11 and R(cid:101)1V from (D.15) and (D.16), this gives
u⊤u=
αR(cid:101)1V(λ(cid:98),α)
·V⊤v
(cid:98) r (cid:98)
λ(cid:98)1/2R(cid:101)11(λ(cid:98),α)
which is (D.19).
For notational convenience, let us now introduce the shorthand
ψ (z)=zm˜ (z), ψ(z)=zm˜(z).
N,0 N,0
ByLemma24(b)appliedwith(v ,v )beingthecolumnsofV ,weseethatM (z,α)iswell-approximated
1 2 r K
by the (deterministic, N-dependent) matrix
(cid:16) (cid:17)−1
M (z,α):=I −α (α+z)I +ψ (z)diag(λ (Σ),...,λ (Σ)) .
N r r N,0 1 r
ToshowTheorem12(a),wetranslatethisapproximationintoacomparisonoftherootsof0=detM (z,α)
K
and 0 = detM (z,α), where the latter are explicitly given by z (−1/λ (Σ)) for the function z (·) de-
N N,0 i N,0
fined in (B.3).
Proof of Theorem 12(a). Let us fix any ε>0 and α∈R satisfying Lemmas 24 and 25, and denote
α
f (z,α)=1−
N,i α+z+ψ (z)λ (Σ)
N,0 i
for each i∈[r]. Then detM
(z,α)=(cid:81)r
f (z,α). Define also the limiting functions
N i=1 N,i
α (cid:16) (cid:17)−1
f (z,α)=1− , M(z,α)=I −α (α+z)I +ψ(z)diag(λ ,...,λ )
i α+z+ψ(z)λ r r 1 r
i
43so detM(z,α) =
(cid:81)r
f (z,α). We first analyze the roots of 0 = detM(z,α): By the definition ψ(z) =
i=1 i
zm˜(z), observe that z ∈R\supp(µ˜) satisfies 0=detM(z,α) if and only if either z =0 or
m˜(z)=−1/λ for some i∈[r].
i
(This condition is the same for any non-zero α ∈ R.) Let T = {0} ∪ {−1/λ : λ ∈ supp(ν)} be as in
(A.2) where ν is the limit spectral law of Σ . Then −1/λ ∈ R\T for all i ∈ [r] under Assumption 6, so
0 i
Proposition 8 implies that m˜(z) = −1/λ holds for some z ∈ R\supp(µ˜) if and only if z′(−1/λ ) > 0, i.e.
i i
i∈I. If i∈I, then m˜(z)=−1/λ holds for z =z(−1/λ ), and we must have z(−1/λ )>0 strictly because
i i i
for any z ≤0, we have m˜(z)>0 (and hence m˜(z)̸=−1/λ ) by the definition m˜(z)=(cid:82) 1 dµ˜(x). Thus the
i x−z
roots of 0=detM(z,α) in R\S =R\(supp(µ˜)∪{0}) — and hence in U(ε)∩R for any sufficiently small
ε>0 — are given precisely by
z :=z(−1/λ ) for i∈I.
i i
Since m˜′(z)=(cid:82) 1 dµ˜(x)>0 for all z ∈R\S, and {λ :i∈I} are distinct by assumption, these values
(x−z)2 i
{z : i ∈ I} are simple roots of 0 = detM(z ,α). Then (detM)′(z ,α) ̸= 0 where (detM)′ denotes the
i i i
derivative in z.
Lemma 23(c) implies m˜ (z) → m˜(z) and m˜′ (z) → m˜′(z) uniformly over z ∈ U(ε). Since also
N,0 N,0
λ (Σ) → λ , we have detM (z,α) → detM(z,α) and (detM )′(z,α) → (detM)′(z,α) uniformly over
i i N N
z ∈U(ε). This, together with the above condition (detM)′(z ,α)̸=0, imply that for all large N, the roots
i
z ∈ U(ε)∩R of 0 = detM (z,α) are in 1-to-1 correspondence with, and converge to, the above roots
N,i N
z ∈U(ε)∩R of 0=detM(z,α). We note that 0=detM (z,α) if and only if either z =0 or
i N
m˜ (z)=−1/λ (Σ) for some i∈[r]. (D.21)
N,0 i
For each i∈I, we have λ (Σ) →λ where z′(−1/λ ) > 0. Recall from Lemma 23(a) that z (m˜)→ z(m˜)
i i i N,0
and z′ (m˜)→z′(m˜) uniformly over compact subsets of R\T. Then z′ (−1/λ (Σ))→z′(−1/λ ), so also
N,0 N,0 i i
z′ (−1/λ (Σ))>0foralllargeN. ThenProposition8impliesthat(D.21)holdsforz :=z (−1/λ (Σ)).
N,0 i N,i N,0 i
We have z → z = z(−1/λ ), so these must be the roots of detM (z,α) in U(ε)∩R. Thus we have
N,i i i N
shown that for any sufficiently small ε>0 and all large N, the roots z ∈U(ε)∩R of 0=detM (z,α) are
N
precisely the values
z :=z (−1/λ (Σ)) for i∈I,
N,i N,0 i
and z →z >0 for each i∈I.
N,i i
Finally, we apply Lemma 24(b) with (v ,v ) being the columns of V . On the event E of Lemma 24(a),
1 2 r
we have
∥M (z,α)∥≤C, ∥M (z,α)−M (z′,α)∥≤C|z−z′| (D.22)
K K K
for some C >0 and all z,z′ ∈U(ε/2). Also |m˜ (z)|,|m˜′ (z)|<C for a constant C >0, all z ∈U(ε), and
N,0 N,0
all large N, and thus
∥M (z,α)∥≤C, ∥M (z,α)−M (z′,α)∥≤C|z−z′| (D.23)
N N N
forsomeC >0andallz,z′ ∈U(ε/2). Then, applyingLemma24(b)andtheLipschitzboundsof(D.22)and
(D.23) to take a union bound over a sufficiently fine covering net of U(ε/2), we get
√
sup ∥M (z,α)−M (z,α)∥≺1/ N. (D.24)
N K
z∈U(ε/2)
Applying also the first bounds of (D.22) and (D.23), this gives
√
sup |detM (z,α)−detM (z,α)|≺1/ N. (D.25)
N K
z∈U(ε/2)
Since detM (z,α) and detM (z,α) are both holomorphic over z ∈ U(ε/2) on this event E, the Cauchy
N K
integral formula then implies
√
sup |(detM )′(z,α)−(detM )′(z,α)|≺1/ N.
N K
z∈U(ε)
44Inparticular,combiningwiththeuniformconvergencestatementsdetM (z,α)→detM(z,α)and(detM )′(z,α)→
N N
(detM)′(z,α) over z ∈ U(ε) as argued above, this shows that on an event E satisfying 1{Ec} ≺ 0 and for
some δ →0, we have
N
sup |detM(z,α)−detM (z,α)|,|(detM)′(z,α)−(detM )′(z,α)|<δ .
K K N
z∈U(ε)∩R
Thus, on this event E and as N → ∞, the roots λ(cid:98)i ∈ U(ε)∩R of 0 = detM K(z,α) are also in 1-to-
1 correspondence with, and converge to, the roots z ∈ U(ε)∩R of 0 = detM(z,α). Furthermore, the
i
condition (detM)′(z ,α) ̸= 0 implies that |(detM )′(z,α)| and |(detM )′(z,α)| are bounded away from
i N K
0 in a neighborhood of each such root z i, so (D.25) then implies that the corresponding roots λ(cid:98)i and z
N,i
of
0=detM (z,α) and 0=detM (z,α) satisfy
K N
√
|λ(cid:98)i−z N,i|≺1/ N.
Proposition28showsthatonthiseventE,theseroots{λ(cid:98)i :i∈I}arepreciselytheeigenvaluesofG⊤Gin
U(ε)∩R. BythedefinitionofM(z ,α),eachrootz ofdetM(z ,α)issuchthatkerM(z ,α)hasdimension
i i i i
1. Since1{E}(λ(cid:98)i−z i)→0,wehave1{E}∥M K(λ(cid:98)i,α)−M(z i,α)∥→0,sokerM K(λ(cid:98)i,α)alsohasdimension
1 on this event E for all large N. Then Proposition 28 implies that the eigenvalues {λ(cid:98)i :i∈I} of G⊤G are
simple, and thus in 1-to-1 correspondence with {λ :i∈I}. This proves part (a) of the theorem.
i
Lemma 29. Under the assumptions of Theorem 12, for any fixed ε>0, there exists α >0 such that fixing
0
any α∈C with |α|>α , uniformly over z ∈U(ε),
0
(cid:13)
(cid:13)(cid:18)
V
(cid:19)⊤ (cid:18)
I
0(cid:19) (cid:18)
V
(cid:19)
(cid:13) r R(z,α) n R(z,α) r
(cid:13) 0 0 0 0
(cid:13)
(cid:13)
−((α+z)I +ψ (z)Σ )−2(cid:0) I +ψ′ (z)Σ (cid:1)(cid:13) (cid:13)≺ √1 .
r N,0 r r N,0 r (cid:13)
(cid:13) N
Proof. Fix any α∈C satisfying Lemma 24, and denote
(cid:18) (cid:19)
f (z,α):=(cid:0) V⊤ 0(cid:1) R(z,α) V r , g (z,α):=−((α+z)I +ψ (z)Σ )−1.
N r 0 N r N,0 r
Applying Lemma 24(b) and the Lipschitz continuity statements of (D.22) and (D.23) to take a union bound
over a sufficiently fine covering net of U(ε/2), we have
√
sup ∥f (z,α)−g (z,α)∥≺1/ N.
N N
z∈U(ε/2)
√
ThenbytheCauchyintegralformula,sup ∥f′ (z,α)−g′ (z,α)∥≺1/ N wheref′ andg′ denotethe
z∈U(ε) N N N N
entrywise derivatives in z. The lemma follows, since differentiating R(z,α) in (D.2) shows
(cid:18) (cid:19) (cid:18) (cid:19)
f′ (z,α)=(cid:0) V⊤ 0(cid:1) R(z,α) I 0 R(z,α) V r
N r 0 0 0,
while g′ (z,α)=((α+z)I +ψ (z)Σ )−2(I +ψ′ (z)Σ ).
N r N,0 r r N,0 r
Proof of Theorem 12(b). Let v
(cid:98)i
be the given unit-norm eigenvector of K with eigenvalue λ(cid:98)i. Let
z =z (−1/λ (Σ))andz =z(−1/λ ). Then,fixinganyα∈RlargeenoughtosatisfyLemmas24and25,
N,i N,0 i i i
Proposition28(b)showsthatV⊤
r
v
(cid:98)i
∈kerM K(λ(cid:98)i,α). By(D.24),(D.23),andthebound|λ(cid:98)i−z N,i|≺N−1/2
of part (a) of the theorem already proven, we have
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)M K(λ(cid:98)i,α)−M N(z N,i,α)(cid:13) (cid:13)≤(cid:13) (cid:13)M K(λ(cid:98)i,α)−M N(λ(cid:98)i,α)(cid:13) (cid:13)+(cid:13) (cid:13)M N(λ(cid:98)i,α)−M N(z N,i,α)(cid:13)
(cid:13)
45≺N−1/2. (D.26)
Let v ,...,v denote the columns of V , which are the unit eigenvectors of Σ. Then, applying V⊤v ∈
1 r r r (cid:98)i
kerM K(λ(cid:98)i,α), (D.26), and the definition of M N(z,α), and noting that ψ N,0(z N,i) = z N,im˜ N,0(z N,i) =
−z /λ (Σ), we have
N,i i
(cid:13) (cid:13)2 (cid:88)r (cid:18) α (cid:19)2
(cid:13)M (z ,α)·V⊤v (cid:13) = 1− (v⊤v )2 ≺1/N.
(cid:13) N N,i r (cid:98)i(cid:13) α+z (1−λ (Σ)/λ (Σ)) j (cid:98)i
N,i j i
j=1
For each j ∈ [r]\{i}, we have that z (1−λ (Σ)/λ (Σ)) is bounded away from 0 as N → ∞ because
N,i j i
z →z >0 and λ (Σ)/λ (Σ)→λ /λ ̸=1. So this implies
N,i i j i j i
|v⊤v |2 ≺1/N for all j ∈[r]\{i}. (D.27)
j (cid:98)i
At the same time, applying Lemma 29 and |λ(cid:98)i−z N,i| ≺ N−1/2 to bound (D.17) in Proposition 28(b), we
have
1 (cid:18) V (cid:19)⊤ (cid:18) I 0(cid:19) (cid:18) V (cid:19) (cid:16) (cid:17)
=v⊤V r R(z ,α) n R(z ,α) r V⊤v +O N−1/2
α2 (cid:98)i r 0 N,i 0 0 N,i 0 r (cid:98)i ≺
(cid:16) (cid:17)−2(cid:16) (cid:17) (cid:16) (cid:17)
=v⊤V (α+z )I +ψ (z )Σ I +ψ′ (z )Σ V⊤v +O N−1/2
(cid:98)i r N,i r N,0 N,i r r N,0 N,i r r (cid:98)i ≺
1+ψ′ (z )λ (Σ)
=|v⊤v |2· N,0 N,i i
i (cid:98)i α2
+(cid:88)
|v⊤v |2·
1+ψ N′ ,0(z N,i)λ j(Σ)
+O
(cid:16) N−1/2(cid:17)
j (cid:98)i (α+z (1−λ (Σ)/λ (Σ)))2 ≺
N,i j i
j̸=i
=|v⊤v |2·
1+ψ N′ ,0(z N,i)λ i(Σ)
+O
(cid:16) N−1/2(cid:17)
,
i (cid:98)i α2 ≺
the last equality applying (D.27). Observe that
1+ψ′ (z )λ (Σ)=1+z m˜′ (z )λ (Σ)+m˜ (z )λ (Σ)
N,0 N,i i N,i N,0 N,i i N,0 N,i i
=z m˜′ (z )λ (Σ)=z λ (Σ)/z′ (−1/λ (Σ)),
N,i N,0 N,i i N,i i N,0 i
wherethelasttwoequalitiesusez =z (−1/λ (Σ))andm˜ (·)istheinversefunctionofz (·). Then,
N,i N,0 i N,0 N,0
multiplying by α2/(1+ψ′ (z )λ (Σ)) we obtain
N,0 N,i i
|v⊤v |2 =
z N′ ,0(−1/λ i(Σ))
+O
(cid:16) N−1/2(cid:17)
=φ (−1/λ (Σ))+O
(cid:16) N−1/2(cid:17)
,
i (cid:98)i z λ (Σ) ≺ N,0 i ≺
N,i i
where we recall φ from (B.3). We have φ (−1/λ (Σ)) → φ(−1/λ ) = z′(−1/λ )/(λ z ) > 0, so taking
N,0 N,0 i i i i i
a square root gives
(cid:113) (cid:16) (cid:17)
|v⊤v |= φ (−1/λ (Σ))+O N−1/2 . (D.28)
i (cid:98)i N,0 i ≺
Finally, for any unit vector v ∈ Rn, by (D.18) in Proposition 28(b), Lemma 24(b), and the bound
|λ(cid:98)i−z N,i|≺N−1/2 in part (a) of the theorem already shown, we know that
v⊤v = −α·(cid:0) v⊤ 0(cid:1) R(z ,α)(cid:18) V r(cid:19) ·V⊤v +O (cid:16) N−1/2(cid:17)
(cid:98)i N,i 0 r (cid:98)i ≺
=
−α(cid:88)r v⊤v
j
·v⊤
j
v
(cid:98)i +O
(cid:16) N−1/2(cid:17)
α+z +ψ (z )λ (Σ) ≺
N,i N,0 N,i j
j=1
=
−α(cid:88)r v⊤v
j
·v⊤
j
v
(cid:98)i +O
(cid:16) N−1/2(cid:17)
.
α+z ·(1−λ (Σ)/λ (Σ)) ≺
N,i j i
j=1
46Applying (D.27) and (D.28), only the summand with j =i contributes, and we obtain as desired
(cid:113) (cid:16) (cid:17)
|v⊤v |= φ (−1/λ (Σ))·|v⊤v |+O N−1/2 .
(cid:98)i N,0 i i ≺
Proof of Theorem 12(c). Applying Lemma 25(b) and block matrix inversion of Γ(cid:101) +ψ N,0(z)Σ(cid:101) to the
definitions of R(cid:101)11 and R(cid:101)1V in (D.15) and (D.16), we have
(cid:12) (cid:12)
(cid:12) (cid:12)R(cid:101)11(z,α)+(cid:16) z+α+ψ N,0(z)·E[u2]−ψ N,0(z)2·E[ug]⊤(Γ+ψ N,0(z)Σ)−1E[ug](cid:17)−1(cid:12) (cid:12)≺ √1 ,
(cid:12) (cid:12) N
(cid:13) (cid:13)
(cid:13) ψ (z)·E[ug]⊤(Γ+ψ (z)Σ)−1V (cid:13) 1
(cid:13)
(cid:13)
(cid:13)R(cid:101)1V(z,α)−
z+α+ψ
N,0(z)N ·, E0
[u2]−ψ
N,0(z)2·E[N u, g0
]⊤(Γ+ψ
Nr ,0(z)Σ)−1E[ug](cid:13)
(cid:13)
(cid:13)≺ √ N.
Hence,
(cid:13) (cid:13)
(cid:13) (cid:13)R(cid:101)1V(z,α)
+ψ (z)·E[ug]⊤V ·((α+z)I +ψ (z)Σ
)−1(cid:13)
(cid:13)≺
√1
.
(cid:13) N,0 r r N,0 r (cid:13)
(cid:13) R(cid:101)11(z,α) (cid:13) N
Applying this and the bound |λ(cid:98)i−z N,i|≺N−1/2 to Proposition 28(c),
u⊤u
(cid:98)i
=
λ(cid:98)1
iα /2R R(cid:101) (cid:101)1 1V 1(( λ(cid:98)λ(cid:98) ii ,, αα )) ·V⊤
r
v
(cid:98)i
=−√ zα
N,i
(cid:88) j=r
1
ψ αN +,0( zz NN ,i,i +)· ψE N[u ,0g (] z⊤ Nv ,ij )λ·v j(⊤ j Σv (cid:98) )i +O ≺(cid:16) N−1/2(cid:17)
=−√ zα (cid:88)r ψ αN +,0( zz N,i () 1· −E[ λug (] Σ⊤ )v /j λ· (v Σ⊤ j )v (cid:98) )i +O ≺(cid:16) N−1/2(cid:17) .
N,i N,i j i
j=1
Then, applying again (D.27) and (D.28), only the summand with j =i contributes, and this gives
|E[ug]⊤v |·|ψ (z )|(cid:112) φ (−1/λ (Σ)) (cid:16) (cid:17)
|u⊤u (cid:98)i|= i N,0 √N z,i N,0 i +O
≺
N−1/2 .
N,i
Recalling ψ (z )=−z /λ (Σ), this yields part (c) of the theorem.
N,0 N,i N,i i
E Proofs for propagation of spiked eigenstructure in deep NNs
We next prove Theorems 2 and 3. Appendix E.1 first establishes these results for a one-hidden-layer NN,
L = 1. We then apply this result for L = 1 inductively in Appendix E.2 to obtain these results for general
L. Appendix E.3 proves Corollary 4.
E.1 Spike analysis for one-hidden-layer CK
Consider the setup in Section 2.1 with a single hidden layer L = 1. In this setting, let us simplify notation
and denote
X =X , W =W , d=d , N =d ,
0 0 0 1
1
Y =X = √ σ(WX), K =K =Y⊤Y.
1 1
N
We denote the rows of W and columns of X respectively by
w⊤ ∈Rd for i∈[N], x ∈Rd for α∈[n].
i α
We write E for the expectation over a standard Gaussian vector w ∼N(0,I) in Rd.
w
47Note that for a sufficiently large constant B > 0 (depending on supp(ν) and λ ,...,λ ), Assumption 2
1 r
implies that the event
(cid:110) (cid:111)
E(X)= ∥X∥<B, |x⊤x |<τ and |∥x ∥ −1|<τ for all α̸=β ∈[n]
α β n α 2 n
holds almost surely for all large n. We will use throughout this section the following argument: Since
W ≡W(n)isindependentofX ≡X(n),andE(X(n))holdsforalllargenwithprobability1over{X(n)}∞ ,
n=1
to prove any almost-sure statement, it suffices to show that the statement holds with probability 1 over
{W(n)}∞ ,foranydeterministicmatrices{X(n)}∞ satisfyingE(X(n)). Thus,weassumeintheremainder
n=1 n=1
of this section that X is deterministic and satisfies E(X) for all large n, and write E,P for the expectation
and probability over only the random weight matrix W.
We will apply Theorem 12 to a centered version of Y,
1
G:=Y −EY = √ [g ,...,g ]⊤, g⊤ :=σ(w⊤X)−E [σ(w⊤X)].
1 N i i w
N
Note that these rows g⊤ are i.i.d. with mean 0 and covariance
i
Σ:=E [σ(w⊤X)⊤σ(w⊤X)]−E [σ(w⊤X)]⊤E [σ(w⊤X)]∈Rn×n.
w w w
Lemma 30. Suppose Assumptions 1, 2, and 3 hold, with L=1 and deterministic X. Then
∥E [σ(w⊤X)]∥ →0, ∥EY∥→0.
w 2
Proof. Denote ξ ∼ N(0,1). Applying E[σ(ξ)] = 0, E[σ′(ξ)ξ] = E[σ′′(ξ)] = 0, and a Taylor approximation
of σ, for any α∈[n],
E [σ(w⊤x )]= E[σ(∥x ∥ξ)]−E[σ(ξ)]
w α α
= E[σ′(ξ)ξ(∥x ∥−1)]+E[σ′′(η)ξ2(∥x ∥−1)2]=E[σ′′(η)ξ2(∥x ∥−1)2]
α α α
for some η between ξ and ∥x ∥ξ. Then, applying |σ′′(x)|≤λ and the τ -orthonormality of X under E(X),
i σ n
|E [σ(w⊤x )]|≤λ τ2.
w α σ n
√ (cid:13) (cid:13)
This gives ∥E w[σ(w⊤X)]∥
2
≤λ στ n2 n→0, so also ∥EY∥=(cid:13) (cid:13)√1 N1
N
·E w[σ(w⊤X)](cid:13) (cid:13)→0.
Next, we recall from [WZ21] an approximation of Σ by the linearized matrix
Σ :=b2X⊤X+(1−b2)I
lin σ σ n
in the operator norm.
Lemma 31. Suppose Assumptions 1, 2, and 3 hold, with L=1 and deterministic X.
∥Σ−Σ ∥→0.
lin
Consequently, ordering λ (Σ),...,λ (Σ) in the same order as λ (X⊤X),...,λ (X⊤X),
1 n 1 n
(cid:12) (cid:12)
sup(cid:12)b2λ (X⊤X)+(1−b2)−λ (Σ)(cid:12)→0. (E.1)
(cid:12) σ i σ i (cid:12)
i∈[n]
Proof. Denote ξ ∼ N(0,1). Let ζ (σ) = E[σ(ξ)h (ξ)] be the k-th Hermite coefficient of σ, where h (x) is
k k k
the k-th Hermite polynomial normalized so that E[h (ξ)2]=1. Note that by Gaussian integration by parts
k
and the assumption E[σ′′(ξ)]=0,
√
ζ (σ)=E[ξσ(ξ)]=E[σ′(ξ)]=b , 2ζ (σ)=E[(ξ2−1)σ(ξ)]=E[ξσ′(ξ)]=E[σ′′(ξ)]=0.
1 σ 2
48Then by [WZ21, Lemma 5.2] and the first statement of Lemma 30, we have
∥Σ −Σ∥≤∥Σ −E [σ(w⊤X)⊤σ(w⊤X)]∥+∥E [σ(w⊤X)⊤σ(w⊤X)]−Σ∥→0
0 0 w w
where
Σ =ζ (σ)2X⊤X+ζ (σ)2(X⊤X)⊙3+(1−ζ (σ)2−ζ (σ)2)I .
0 1 3 1 3 n
(Here, examination of the proof of [WZ21, Lemma 5.2] shows that the condition (cid:80) (∥x ∥ −1)2 ≤B2 for
α α 2
(ε,B)-orthonormality is not used when ζ (σ) = 0, and the remaining conditions of (ε,B)-orthonormality
2
hold under E(X).) The lemma then follows upon observing that under E(X),
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(X⊤X)⊙3−I (cid:13)≤ (cid:13)diag((X⊤X)⊙3−I )(cid:13)+(cid:13)offdiag(X⊤X)⊙3(cid:13)
(cid:13) n(cid:13) (cid:13) n (cid:13) (cid:13) (cid:13)
F
(cid:12) (cid:12)
≤ max(cid:12)∥x ∥6−1(cid:12)+n· max |x⊤x |3 ≤C(τ +nτ3),
(cid:12) α (cid:12) α β n n
α∈[n] α̸=β∈[n]
(cid:13) (cid:13)
so that ∥Σ −Σ ∥=ζ (σ)2(cid:13)(X⊤X)⊙3−I (cid:13)→0 when lim τ ·n1/3 =0.
lin 0 3 (cid:13) n(cid:13) n→∞ n
Theorem 12 will provide a characterization of outlier eigenvalues of K that are separated from S =
1
supp(µ )∪{0}, which is different from supp(µ ) when γ <1. For γ <1, we augment this statement with
1 1 1 1
a small-ball argument to bound the smallest eigenvalue of K, using the following result of [Yas16, Theorem
2.1].
Lemma 32 ([Yas16]). Let G = √1 [g ,...,g ]⊤ ∈ RN×n where the rows g ∈ Rn are i.i.d. and equal in
N 1 N i
law to g ∈Rn. Define
(cid:104) (cid:105)
Σ=Egg⊤, c = inf E|g⊤v|, L (δ,ι)= sup P |Πg|2 ≤δrank(Π)
g g
v∈Rn:∥v∥2=1
Π:rank(Π)≥ιn
where the latter supremum is taken over all orthogonal projections Π∈Rn×n with rank at least ι·n.
Suppose λ (Σ)≤1, c(g)≥c, and n/N ≤y for some constants c>0 and y ∈(0,1). Then there exist
max
constants s ,ι>0 depending only on (c,y) such that for any δ ∈(0,1) and s>0,
0
P[λ (G⊤G)≥(s −L(δ,ι;g)−s)δ]≥1−2e−yns2/2.
min 0
Lemma 33. Suppose Assumptions 1, 2, and 3 hold, with L=1 and deterministic X. Let G=Y −EY.
(a) If γ ≥1, then 0∈supp(µ ).
1 1
(b) If γ <1, then there is a constant c>0 such that λ (G⊤G)>c almost surely for all large n.
1 min
Proof. If γ >1 strictly, then by definition
1
1 γ −1
µ = µ˜ + 1 δ
1 γ 1 γ 0
1 1
is a mixture of µ˜ and a point mass at 0, so 0∈supp(µ ). If γ =1, then µ =µ˜ . In this case, recall from
1 1 1 1
Proposition 8 that supp(µ˜ ) is characterized by the function
1
1 (cid:90) λ
z(m˜)=− +γ dν (λ).
m˜ 1 1+λm˜ 0
When γ =1, we have for all m˜ ∈(0,∞) that
1
1 (cid:90) λ2
z(m˜)<0, z′(m˜)= − dν(λ)>0,
m˜2 (1+λm˜)2
soz(m˜)increasesfrom−∞to0overthepositivelinem˜ ∈(0,∞). Supposebycontradictionthat0∈/ supp(µ˜).
Then by Proposition 8, there must be a point m˜ ∈ R\T where z(m˜) = 0 and z′(m˜) > 0 strictly, implying
49that there is an open interval (m˜ ,m˜ ) ∋ m˜ on which z(·) increases from z(m˜ ) < 0 to z(m˜ ) > 0. We
− + − +
must have m˜ <0 by the above behavior of z(·) on (0,∞), and the range [z(m˜ ),z(m˜ )] must overlap with
− +
[z(a),z(b)] for some sufficiently large a,b∈(0,∞). But this contradicts the non-intersecting property shown
in [SC95, Theorem 4.4]. So also in this case 0∈supp(µ˜ )=supp(µ ), showing part (a).
1 1
For part (b), we apply Lemma 32. Under E(X), the condition b ̸= 0 implies c < λ (Σ ) ≤
σ 0 min lin
λ (Σ )<C for some constants C ,c >0. Hence also
max lin 0 0 0
c <λ (Σ)≤λ (Σ)<C (E.2)
0 min max 0
for all large n, by Lemma 31. We may assume without loss of generality that λ (Σ) ≤ 1 as needed in
max
Lemma 32; otherwise, the following argument may be applied to a rescaling of Σ and G.
To lower bound c in Lemma 32, observe that for any unit vector v ∈Rn we have
g
E[(g⊤v)2]=v⊤Σv >c .
0
Viewing F(w) = g⊤v = σ(w⊤X)v = (cid:80)n v σ(w⊤x ) as a function of w ∼ N(0,I), we have ∇F(w) =
α=1 α α
(cid:80)n v σ′(w⊤x )x =X(v⊙σ′(w⊤X))whereσ′(·)isappliedcoordinatewiseand⊙isthecoordinatewise
α=1 α α α
product. Then, applying |σ′(x)| ≤ λ , observe that ∥∇F(w)∥ ≤ ∥X∥·∥v ⊙σ′(w⊤X)∥ ≤ λ ∥X∥, so
σ 2 2 σ
F(w) is C-Lipschitz in w for a constant C > 0 (not depending on v) on the event E(X). This implies
by Gaussian concentration-of-measure that g⊤v is sub-gaussian, i.e. for some constants C,c > 0 and any
t>0, P[|g⊤v|≥t]≤Ce−ct2. Integrating this tail bound, for some constant t>0 sufficiently large, we have
E[(g⊤v)21 ]≤c /2, and hence
{|g⊤v|>t} 0
c c
c <E[(g⊤v)2]≤E[(g⊤v)21 ]+ 0 ≤t·E|g⊤v|+ 0.
0 {|g⊤v|≤t} 2 2
So E|g⊤v|≥c /(2t), and hence c ≥c /(2t)>c for a constant c>0.
0 g 0
Now let s ,ι>0 be the constants depending on (c,γ ) in the statement of Lemma 32. By the nonlinear
0 1
Hanson-Wright inequality of [WZ21, Eq. (3.3)], for any orthogonal projection Π ∈ Rn×n, any t > 0, and
some constant c>0, we have
P(cid:2) |g⊤Πg−Eg⊤Πg|>t(cid:3) ≤2e−cmin(t2/∥Π∥2 F,t/∥Π∥).
Here Eg⊤Πg = TrΠΣ > c rank(Π), ∥Π∥2 = rank(Π), and ∥Π∥ = 1, so applying this with t =
0 F
(c /2)rank(Π) yields
0
P[|Πg|2 ≤(c /2)rank(Π)]≤2e−c′rank(Π).
0
Then, choosing δ = c /2, we get L (δ,ι) → 0 as n → ∞. Then Lemma 32 implies λ (G⊤G) > s δ/2
0 g min 0
almost surely for all large n, as desired.
The following is the main result of this section, showing that Theorems 2 and 3 hold in this setting of
L=1.
Lemma 34. Theorems 2 and 3 hold for a single layer L=1. Furthermore, Y is Cτ -orthonormal for some
n
constant C >0, almost surely for all large n.
Proof. We condition on X as discussed at the start of this section, and apply Theorem 12 to the centered
matrixG=Y−EY = √1 [g ,...,g ]⊤. LetusverifyAssumption5forG: WehaveshownAssumption5(a)
N 1 N
in(E.2). Therowsg aresub-gaussianasshownintheaboveproofofLemma33(b),soAssumption5(b)holds
i
by [Ver10, Eq. (5.26)], and Assumption 5(d) holds by [JNG+19, Lemma 2]. The nonlinear Hanson-Wright
inequality of [WZ21, Eq. (3.3)] implies
|g⊤Ag −TrAΣ|≺∥A∥
i i F
uniformly over i ∈ [N] and deterministic matrices A ∈ Cn×n. Furthermore, it is clear from the argument
preceding [WZ21, Eq. (3.3)] that for any i ̸= j ∈ [N], the joint vector (g ,g ) ∈ R2n also satisfies Lipschitz
i j
concentration, hence
(cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12)
(cid:12) (cid:12) (cid:12)(cid:0) g⊤ i g⊤ j (cid:1) B gg i −TrB Σ 0 Σ0 (cid:12) (cid:12) (cid:12)≺∥B∥ F
j
50uniformly over i̸=j ∈[N] and deterministic matrices B ∈C2n×2n. Applying this with
(cid:18) (cid:19)
0 A
B =
A 0
verifies both statements of Assumption 5(c).
Next, we check Assumption 6 for the population covariance matrix Σ. Combining Assumption 2 and
(E.1) from Lemma 31, we have
n
1 (cid:88)
δ →ν :=b2 ⊗µ ⊕(1−b2) weakly,
n−r λi(Σ) 0 σ 0 σ
i=r+1
1
λ (Σ)→− :=b2λ +(1−b2)̸∈supp(ν ) for i=1,...,r. (E.3)
i s σ i σ 0
i,0
Here, the statement −1/s ∈/ supp(ν ) in (E.3) follows from the assumptions λ ∈/ supp(µ ) and b ̸= 0.
i,0 0 i 0 σ
This then implies by the definition of T that s ∈ R\T , as claimed in Theorem 3(a). Furthermore, for
1 i,0 1
any fixed ε>0 and all large n, Assumption 2 and (E.1) imply also that
λ (Σ)∈supp(ν )+(−ε,ε) for all i≥r+1.
i 0
Thus Assumption 6 holds for Σ as n→∞.
Then we can apply Theorems 10 and 12 for K¯ := G⊤G. The Stieltjes transform approximation in
Theorem 10 and Lemma 23(c) together imply m K¯(z) → m 1(z) almost surely for each fixed z ∈ C+, where
m (z) is the Stieltjes transform of the measure µ =ρMP⊠ν . This implies the weak convergence
1 1 γ1 0
n
1 (cid:88)
n δ λi(K¯) →µ 1 a.s. (E.4)
i=1
Theorem 12(a,b) further justifies:
• Let z (·) and I be defined by (2.6) and (2.7) with ℓ = 1. Then for any sufficiently small constant
1 1
ε > 0, almost surely for all large n, there is a 1-to-1 correspondence between the eigenvalues of K¯
outside S +(−ε,ε) and {i:i∈I }. Furthermore, for each i∈I ,
1 1 1
λ (K¯)→z (s )>0. (E.5)
i 1 i,0
almost surely as n→∞.
• Let φ (·) be defined by (2.6) with ℓ = 1. For each i ∈ I , let v (K¯) ∈ Rn be a unit-norm eigenvec-
1 1 i
tor of K¯ corresponding to λ (K¯), and for each j ∈ [r], let v (Σ) be a unit-norm eigenvector of Σ
i j
corresponding to λ (Σ). Then almost surely as n→∞, for each i∈I and j ∈[r],
j 1
(cid:113)
|v (Σ)⊤v (K¯)|→ φ (s )·1{i=j} (E.6)
j i 1 i,0
where φ (s )>0. Moreover, letting v ∈Rn be any unit vector independent of W, almost surely
1 i,0
(cid:113)
|v⊤v (K¯)|− φ (s )·|v⊤v (Σ)|→0. (E.7)
i 1 i,0 i
If γ ≥1, then Lemma 33(a) shows that supp(µ )=supp(µ )∪{0}=S . If γ <1, then Lemma 33(b)
1 1 1 1 1
shows that for any sufficiently small constant ε > 0, K¯ has no eigenvalues in [0,ε) almost surely for all
large n. Thus, in both cases, the first statement above in fact establishes a 1-to-1 correspondence between
{i:i∈I } and all eigenvalues of K¯ outside supp(µ )+(−ε,ε), almost surely for all large n.
1 1
To translate these statements to the non-centered matrix K =Y⊤Y, recall from Lemma 30 that ∥Y −
G∥ → 0 almost surely, and from Assumption 5(b) verified above that 1{∥G⊤G∥ > B′} ≺ 0 for a constant
B′ >0. Then, almost surely as n→∞,
(cid:13) (cid:13)K−K¯(cid:13)
(cid:13)→0.
51Therefore,byWeyl’sinequalityand(E.4),theempiricaleigenvaluedistributionµ ofK convergesalsotoµ
(cid:98)1 1
weakly a.s., as claimed in Theorem 2. Furthermore, by (E.5), almost surely for all large n, the eigenvalues
λ(cid:98)i,1 ofK outsidesupp(µ 1)+(−ε,ε)arealsoin1-to-1correspondencewith{i:i∈I 1},whereλ(cid:98)i,1 →z 1(s i,0)
foreachi∈I . Inparticular,ifr =0,thenalso|I |=0,soK hasnoeigenvaluesoutsidesupp(µ )+(−ε,ε).
1 1 1
This proves Theorem 2.
For each i ∈ I 1, let v
(cid:98)i,1
∈ Rn be a unit-norm eigenvector of K corresponding to λ(cid:98)i,1. Then by the
Davis-Kahan Theorem [DK70], we may choose a sign for v such that
(cid:98)i,1
√
(cid:13)
(cid:13)v (cid:98)i,1−v
i(K¯)(cid:13)
(cid:13)≤
dist(λ(cid:98)i,1,sp2∥ ecK (K¯− )K¯ \∥
{λ
i(K¯)}).
We note that λ(cid:98)i,1 →z 1(s i,0) a.s., which is distinct from the limit values {z 1(s j,0):j ∈I 1\{i}} of {λ j(K¯):
I \{i}}bybijectivityofthemapz (·)inProposition8. Furthermorez (s )fallsoutsidesupp(µ )+(−ε,ε)
1 1 1 i,0 1
forsufficientlysmallε>0,whichcontainsallothereigenvaluesofK¯. Thusdist(λ(cid:98)i,1,spec(K¯)\{λ i(K¯)})≥c
for a constant c>0 almost surely for all large n, so
(cid:13)
(cid:13)v (cid:98)i,1−v
i(K¯)(cid:13)
(cid:13)→0 a.s.
Similarly,bytheconvergence∥Σ−Σ ∥→0andtheassumptionb ̸=0,wehave∥v (Σ)−v ∥→0foreach
lin σ j j
j ∈[r], wherev istheunit-normeigenvectorofΣ correspondingtoitseigenvalueb2λ (X⊤X)+(1−b2),
j lin σ j σ
i.e. the eigenvector of X⊤X corresponding to λ (X⊤X). Then (E.6) and (E.7) imply also
j
|v⊤v |2 →φ (s )·1{i=j}, |v⊤v |2−φ (s )·|v⊤v |2 →0.
j (cid:98)i 1 i,0 (cid:98)i 1 i,0 i
This shows all claims of Theorem 3 for L=1.
Finally, on E(X), the matrix Y is Cτ -orthonormal for a constant C > 0 by [FW20, Lemma D.3(b)].
n
(The proof of [FW20, Lemma D.3(b)] again does not use the condition (cid:80) (∥x ∥2 −1)2 ≤ B2 of (ε,B)-
α α 2
orthonormalitytherein,andtheremainingconditionsof(ε,B)-orthonormalityholdunderE(X).) Thisshows
the last claim of the lemma.
E.2 Spike analysis for multiple layers
We now prove Theorem 3 by inductively applying the result for L = 1 through multiple layers. We follow
the notations of Section 2.1.
Proof of Theorem 3. Suppose inductively that Assumption 2 holds with X in place of X , and all
ℓ−1 0
conclusions of Theorem 3 hold for K . The base case of ℓ=1 follows from Lemma 34.
ℓ
Then the last statement of Lemma 34 implies that X is τ′-orthonormal almost surely for all large n,
ℓ n
for some τ′ satisfying τ′ ·n1/3 → 0. Furthermore, the conclusions of Theorem 3(b,c) for K imply that
n n ℓ
statements (a) and (b) of Assumption 2 also hold for X , in the following sense: Let r =|I |. Then
ℓ ℓ ℓ
1 (cid:88)
δ →µ weakly a.s.
n−|r ℓ| λi(X⊤ ℓXℓ) ℓ
i∈/Iℓ
For any fixed ε > 0, almost surely for all large n, λ(cid:98)i,ℓ := λ i(X⊤
ℓ
X ℓ) ∈ supp(µ ℓ)+(−ε,ε) for all i ∈/ I ℓ.
Furthermore, for each i∈I ℓ, λ(cid:98)i,ℓ →z ℓ(s i,ℓ−1)∈/ supp(µ ℓ).
Then we may apply Lemma 34 with input data X = X in place of X . This shows that for any
ℓ 0
fixed ε>0 and all large n, there is a 1-to-1 correspondence between the eigenvalues λ(cid:98)i,ℓ+1 of K
ℓ+1
outside
supp(µ ℓ+1)+(−ε,ε) and {i : i ∈ I ℓ+1}, where λ(cid:98)i,ℓ+1 → z ℓ+1(s i,ℓ) > 0 a.s., and s
i,ℓ
∈ R\T ℓ+1. Moreover,
for any unit vector v ∈Rn independent of W ,...,W ,
1 ℓ+1
|v⊤ v|2−φ (s )·|v⊤v|2 →0,
(cid:98)i,ℓ+1 ℓ+1 i,ℓ (cid:98)i,ℓ
52where also φ (s )>0. Then by the induction hypothesis for |v⊤v|2,
ℓ+1 i,ℓ (cid:98)i,ℓ
ℓ+1
|v⊤ v|2 → (cid:89) φ (s )·|v⊤v|2,
(cid:98)i,ℓ+1 k i,k−1 i
k=1
and specializing to v =v for j ∈[r] gives
j
ℓ+1
|v⊤ v |2 → (cid:89) φ (s )·1{i=j}.
(cid:98)i,ℓ+1 j k i,k−1
k=1
This verifies all conclusions of Theorem 3 for K , completing the induction.
ℓ+1
E.3 Corollary for signal-plus-noise input data
Proof of Corollary 4. It is shown in [BGN12, Section 3.1] that asymptotically as d,n → ∞ with
n/d→γ ,thedatamatrixX hasaspikesingularvaluecorrespondingtoθ ifandonlyifθ >γ1/4,inwhich
0 i i 0
case
(1+θ2)(γ +θ2) γ (1+θ2)
λ (K )→λ := i 0 i , |b⊤v |2 →1− 0 i
i 0 i θ2 i i θ2(θ2+γ )
i i i 0
where v is the unit eigenvector of the input Gram matrix K = X⊤X. Thus claims (a) and (b) of
i 0
Assumption 2 hold with r = |{i : θ > γ1/4}|, µ = ρMP being the standard Marcenko-Pastur law, and
i 0 0 γ0
λ =(1+θ2)(γ +θ2)/θ2 being the above values.
i i 0 i i
WenotethatX isn−1/2+ε-orthonormalforanyε>0almostsurelyforalllargen,bythegivencondition
maxr ∥b ∥ <n−1/2+ε and the bounds, for any α,β ∈[n],
i=1 i ∞
r
(cid:88) (cid:16) (cid:17) (cid:16) (cid:17)
∥x ∥ =∥z ∥ + O (∥a ∥ |θ ||b |)=∥z ∥ +O n−1/2+ε =1+O n−1/2+ε ,
α 2 α 2 ≺ i 2 i i,α α 2 ≺ ≺
i=1
r
(cid:88) (cid:16) (cid:16) (cid:17) (cid:17) (cid:16) (cid:17)
x⊤x =z⊤z + O |θ | |a⊤z ||b |+|a⊤z ||b | +θ2∥a ∥2|b b | =O n−1/2+ε .
α β α β ≺ i i α i,α i β i,β i i 2 i,α i,β ≺
i=1
Hence Theorem 3 applies, showing that K has an outlier eigenvalue corresponding to each input signal θ
ℓ i
if and only if θ >γ1/4 and i∈I . The statement (2.9) follows from Theorem 3(c) applied with v =b .
i 0 ℓ i
F Proofs for spiked eigenstructure of the trained CK
In this section, we prove Theorem 5. The proof is an application of Theorem 12 as in the one-layer setting
of the preceding section, but now reversing the roles of X and W. We abbreviate
1
W =W , Y = √ σ(X˜W), K =YY⊤
trained
N
where K is the CK matrix of interest. In contrast to the preceding section, the theorem requires character-
izing the left spike singular vector of Y, and we will do so using Theorem 12(c).
We first recall the following approximation of W from [BES+22].
Proposition 35. Under Assumption 4, set η
=(cid:80)T
η , and let θ ,θ be as defined in (2.13). Then
t=1 t 1 2
b η
∥W −W(cid:102)∥≺N−1/2 where W(cid:102) =W 0+ σ
n
X⊤ya⊤. (F.1)
53The largest singular value s (W) falls outside the limit of its empirical singular value distribution if and
max
only if θ >γ1/4, in which case s (W) and its unit-norm left singular vector u(W) satisfy
1 0 max
(cid:115)
(1+θ2)(γ +θ2) θ2(cid:18) γ +θ2 (cid:19)
s (W)→s := 1 0 1 , |u(W)⊤β |2 → 2 1− 0 1 a.s. (F.2)
max 1 θ2 ∗ θ2 θ2(θ2+1)
1 1 1 1
Proof. Notice that each gradient update matrix G of (2.11) takes the form
t
(cid:20)(cid:18) (cid:18) (cid:19) (cid:19) (cid:21)
1 1 1
G = X⊤ √ y− √ σ(XW )a a⊤ ⊙σ′(XW ) ,
t n N N t t
From the proof of [BES+22, Lemma 16], for each t = 1,...,T, this matrix G satisfies the same rank-one
t
approximation
(cid:13)√ b (cid:13)
(cid:13) NG − σX⊤ya⊤(cid:13)≺N−1/2.
(cid:13) t n (cid:13)
This implies (F.1) in light of (2.11), and the statements of (F.2) then follow from [BES+22, Theorem 3].
We denote the columns of W ≡W ∈Rd×N and of the initialization W ∈Rd×N by
trained 0
w ∈Rd, w ∈Rd for i∈[N]
i i,0
respectively. Fixing a large constant B >0 and small constant ε>0, define the event
(cid:110) (cid:111)
E(W)= ∥W∥<B, |w⊤w |<n−1/2+ε and |∥w ∥ −1|<n−1/2+ε for all i̸=j ∈[N] .
i j i 2
Lemma 36. Under Assumption 4, for some sufficiently large constant B > 0 and any fixed ε > 0, E(W)
holds almost surely for all large n and any fixed T ∈N.
iid
Proof. By the assumption [W ] ∼ N(0,1/d), it is immediate to check that E(W ) holds almost surely
0 ij 0
foralllarge n. To showthatE(W)holds, weapplytheapproximation (F.1). Here, underAssumption4, we
have by standard tail bounds for Gaussian vectors and matrices that
1{∥X⊤y∥>Cn}≤1{∥X∥·(λ ∥Xβ ∥+∥ε∥ )>Cn}≺0, 1{∥a∥ >C}≺0
σ∗ ∗ 2 2
for a sufficiently large constant C >0, and also ∥a∥ ≺N−1/2. Then this implies
∞
mN ax∥w i−w i,0∥
2
≤mN ax∥w (cid:101)i−w i,0∥ 2+∥W −W(cid:102)∥≺N−1/2 (F.3)
i=1 i=1
and 1{∥W −W ∥>C′}≺0 for a constant C′ >0. Then E(W) also holds almost surely for all large n, as
0
claimed.
Analogous to the argument of Appendix E.1, we may now condition on W, i.e. we assume that W is
deterministicandsatisfiesE(W)foralllargen,andwewriteEfortheexpectationoveronlytherandomness
of the new data (X˜,y˜). Defining
(cid:114)
N 1 1
G= (Y −EY)∈Rn×N, u= √ y˜ ∈Rn where Y = √ σ(X˜W), (F.4)
n n N
observe that [u,G] ∈ Rn×(N+1) has centered i.i.d. rows with respect to the randomness of (X˜,y˜). We will
write E for the expectation with respect to a standard Gaussian vector x∼N(0,I ).
x d
Lemma 37. Suppose W satisfies E(W) for all large n. Then
∥E [σ(x⊤W)]∥ →0, ∥EY∥→0, ∥Σ−Σ ∥→0
x 2 lin
where
Σ:= E [σ(x⊤W)⊤σ(x⊤W)]−E [σ(x⊤W)]⊤E [σ(x⊤W)]
x x x
Σ := b2(W⊤W)+(1−b2)I .
lin σ σ N
54Proof. The proof is the same as Lemmas 30 and 31.
Proof of Theorem 5. We condition on W satisfying E(W) for all large n, and we apply Theorem 12(c)
for [u,G]∈R(n+1)×N (exchanging n and N). It may be checked that Assumption 5 holds for [u,G] by the
same argument as in Lemma 34.
By the convergence ∥Σ−Σ ∥ → 0 in Lemma 37 and Proposition 35, if θ > γ1/4, then Assumption 6
lin 1 0
holds for Σ with r =1 and
(1+θ2)(γ +θ2)
ν =b2 ⊗ρMP⊕(1−b2), λ =b2 1 0 1 +(1−b2)∈/ supp(ν),
σ γ0 σ 1 σ θ2 σ
1
where ρMP is the standard Marcenko-Pastur limit for the empirical eigenvalue distribution of W⊤W, hence
γ0
ν is the limit empirical eigenvalue distribution of Σ, and λ is the limit of λ (Σ). If instead θ ≤ γ1/4,
1 max 1 0
then Assumption 6 holds with r =0.
Then Theorem 12(a,c) characterizes the outlier eigenvalue and eigenvector of GG⊤, showing:
• GG⊤ has a spike eigenvalue if and only if θ >γ1/4 and z′(−1/λ )>0, where z(·) is defined by (A.3)
1 0 1
with γ =γ and the measure ν given above. In this case, λ (GG⊤)→z(−1/λ ) almost surely.
1 max 1
• When θ > γ1/4 and z′(−1/λ ) > 0, letting u(G),v(Σ) be the leading unit-norm left singular vector
1 0 1
of G and leading unit-norm eigenvector of Σ, almost surely
(cid:112)
z(−1/λ )φ(−1/λ ) (cid:12) (cid:12)
|u⊤u(G)|− 1 1 ·(cid:12)E [σ (β⊤x)σ(x⊤W)]v(Σ)(cid:12)→0.
λ (cid:12) x ∗ ∗ (cid:12)
1
where φ(·) is defined by (B.4) also with γ =γ and the above measure ν.
1
By an application of Weyl’s inequality and the Davis-Kahan Theorem as in the proof of Theorem 3, this
implies for K = YY⊤ that if θ > γ1/4 and z′(−1/λ ) > 0, then its leading eigenvalue λ (K) and unit
1 0 1 max
eigenvector u satisfy
(cid:98)
λ (K)→γ−1z(−1/λ ),
max 1 1
(cid:112) z(−1/λ )φ(−1/λ ) (cid:12) (cid:12) (F.5)
|u⊤u|− 1 1 ·(cid:12)E [σ (β⊤x)σ(x⊤W)]v(W)(cid:12)→0,
(cid:98) λ (cid:12) x ∗ ∗ (cid:12)
1
where v(W) is the leading unit eigenvector of Σ , i.e. the leading right singular vector of W. If θ ≤γ1/4
lin 1 0
or z′(−1/λ )≤0, then all eigenvalues of K converge to the support of its limiting empirical eigenvalue law.
1
Finally, in the case of θ >γ1/4 and z′(−1/λ )>0, we may conclude the proof by showing
1 0 1
(cid:13) (cid:13)
(cid:13)E [σ (β⊤x)σ(x⊤W)]−b b β⊤W(cid:13) →0 a.s. (F.6)
(cid:13) x ∗ ∗ σ σ∗ ∗ (cid:13)
2
For each column i∈[N], we have from (F.3) that ∥w −w ∥ ≺N−1/2, where w ∼N(0,d−1I) and β
i i,0 2 i,0 ∗
is deterministic. Hence (w ,β ) satisfy the approximate orthonormality conditions |∥w ∥ −1| ≺ N−1/2,
i ∗ i 2
∥β ∥ −1=0, and |w⊤β |≺N−1/2. Then [FW20, Lemma D.3(a)] implies
∗ 2 i ∗
(cid:12) (cid:12)
(cid:12)E [σ (β⊤x)σ(x⊤w )]−b b β⊤w (cid:12)≺N−1.
(cid:12) x ∗ ∗ i σ σ∗ ∗ i(cid:12)
(We note that [FW20, Lemma D.3(a)] assumes σ =σ , but the proof is identical for σ ̸=σ both satisfying
∗ ∗
Assumption3.) Applyingthistoeachcoordinatei∈[N]yields(F.6). Observethatβ⊤Wv(W)=s (W)·
∗ max
β⊤u(W)wheres (W)andu(W)aretheleadingsingularvalueandleftsingularvectorofW, andrecall
∗ max
from the definitions (F.4) that u= √1 y˜. Then we can apply (F.6) and Proposition 35 to (F.5) to conclude
n
that
(cid:112) (cid:112)
1 z(−1/λ )φ(−1/λ ) θ (θ4−γ )(γ +θ2)
√ |y˜⊤u|→b b 1 1 · 2 1 0 0 1 >0 a.s.
n (cid:98) σ σ∗ λ θ3
1 1
55