[
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": "Huizhuo YuanZixiang ChenKaixuan JiQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.10210v1",
        "entry_id": "http://arxiv.org/abs/2402.10210v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10210v1",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "updated": "2024-02-15 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10210v1"
    },
    {
        "title": "Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
        "authors": "Romain IlbertAmbroise OdonnatVasilii FeofanovAladin VirmauxGiuseppe PaoloThemis PalpanasIevgen Redko",
        "links": "http://arxiv.org/abs/2402.10198v1",
        "entry_id": "http://arxiv.org/abs/2402.10198v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10198v1",
        "summary": "Transformer-based architectures achieved breakthrough performance in natural\nlanguage processing and computer vision, yet they remain inferior to simpler\nlinear baselines in multivariate long-term forecasting. To better understand\nthis phenomenon, we start by studying a toy linear forecasting problem for\nwhich we show that transformers are incapable of converging to their true\nsolution despite their high expressive power. We further identify the attention\nof transformers as being responsible for this low generalization capacity.\nBuilding upon this insight, we propose a shallow lightweight transformer model\nthat successfully escapes bad local minima when optimized with sharpness-aware\noptimization. We empirically demonstrate that this result extends to all\ncommonly used real-world multivariate time series datasets. In particular,\nSAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on\naverage, while having ~4 times fewer parameters. The code is available at\nhttps://github.com/romilbert/samformer.",
        "updated": "2024-02-15 18:55:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10198v1"
    },
    {
        "title": "Nonlinear spiked covariance matrices and signal propagation in deep neural networks",
        "authors": "Zhichao WangDenny WuZhou Fan",
        "links": "http://arxiv.org/abs/2402.10127v1",
        "entry_id": "http://arxiv.org/abs/2402.10127v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10127v1",
        "summary": "Many recent works have studied the eigenvalue spectrum of the Conjugate\nKernel (CK) defined by the nonlinear feature map of a feedforward neural\nnetwork. However, existing results only establish weak convergence of the\nempirical eigenvalue distribution, and fall short of providing precise\nquantitative characterizations of the ''spike'' eigenvalues and eigenvectors\nthat often capture the low-dimensional signal structure of the learning\nproblem. In this work, we characterize these signal eigenvalues and\neigenvectors for a nonlinear version of the spiked covariance model, including\nthe CK as a special case. Using this general result, we give a quantitative\ndescription of how spiked eigenstructure in the input data propagates through\nthe hidden layers of a neural network with random weights. As a second\napplication, we study a simple regime of representation learning where the\nweight matrix develops a rank-one signal component over training and\ncharacterize the alignment of the target function with the spike eigenvector of\nthe CK on test data.",
        "updated": "2024-02-15 17:31:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10127v1"
    },
    {
        "title": "How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage",
        "authors": "Achraf AzizeDebabrota Basu",
        "links": "http://arxiv.org/abs/2402.10065v1",
        "entry_id": "http://arxiv.org/abs/2402.10065v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10065v1",
        "summary": "We study the per-datum Membership Inference Attacks (MIAs), where an attacker\naims to infer whether a fixed target datum has been included in the input\ndataset of an algorithm and thus, violates privacy. First, we define the\nmembership leakage of a datum as the advantage of the optimal adversary\ntargeting to identify it. Then, we quantify the per-datum membership leakage\nfor the empirical mean, and show that it depends on the Mahalanobis distance\nbetween the target datum and the data-generating distribution. We further\nassess the effect of two privacy defences, i.e. adding Gaussian noise and\nsub-sampling. We quantify exactly how both of them decrease the per-datum\nmembership leakage. Our analysis builds on a novel proof technique that\ncombines an Edgeworth expansion of the likelihood ratio test and a\nLindeberg-Feller central limit theorem. Our analysis connects the existing\nlikelihood ratio and scalar product attacks, and also justifies different\ncanary selection strategies used in the privacy auditing literature. Finally,\nour experiments demonstrate the impacts of the leakage score, the sub-sampling\nratio and the noise scale on the per-datum membership leakage as indicated by\nthe theory.",
        "updated": "2024-02-15 16:30:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10065v1"
    },
    {
        "title": "How to validate average calibration for machine learning regression tasks ?",
        "authors": "Pascal Pernot",
        "links": "http://arxiv.org/abs/2402.10043v1",
        "entry_id": "http://arxiv.org/abs/2402.10043v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10043v1",
        "summary": "Average calibration of the uncertainties of machine learning regression tasks\ncan be tested in two ways. One way is to estimate the calibration error (CE) as\nthe difference between the mean absolute error (MSE) and the mean variance (MV)\nor mean squared uncertainty. The alternative is to compare the mean squared\nz-scores or scaled errors (ZMS) to 1. Both approaches might lead to different\nconclusion, as illustrated on an ensemble of datasets from the recent machine\nlearning uncertainty quantification literature. It is shown here that the CE is\nvery sensitive to the distribution of uncertainties, and notably to the\npresence of outlying uncertainties, and that it cannot be used reliably for\ncalibration testing. By contrast, the ZMS statistic does not present this\nsensitivity issue and offers the most reliable approach in this context.\nImplications for the validation of conditional calibration are discussed.",
        "updated": "2024-02-15 16:05:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10043v1"
    }
]