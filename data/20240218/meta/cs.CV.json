[
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": "Huizhuo YuanZixiang ChenKaixuan JiQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.10210v1",
        "entry_id": "http://arxiv.org/abs/2402.10210v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10210v1",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "updated": "2024-02-15 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10210v1"
    },
    {
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
        "authors": "Eliahu HorwitzJonathan KahanaYedid Hoshen",
        "links": "http://arxiv.org/abs/2402.10208v1",
        "entry_id": "http://arxiv.org/abs/2402.10208v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10208v1",
        "summary": "The dominant paradigm in generative modeling consists of two steps: i)\npre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained\nmodel with human values via fine-tuning. This practice is considered safe, as\nno current method can recover the unsafe, pre-fine-tuning model weights. In\nthis paper, we demonstrate that this assumption is often false. Concretely, we\npresent Spectral DeTuning, a method that can recover the weights of the\npre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In\ncontrast to previous attacks that attempt to recover pre-fine-tuning\ncapabilities, our method aims to recover the exact pre-fine-tuning weights. Our\napproach exploits this new vulnerability against large-scale models such as a\npersonalized Stable Diffusion and an aligned Mistral.",
        "updated": "2024-02-15 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10208v1"
    },
    {
        "title": "Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model",
        "authors": "Mariia DrozdovaVitaliy KinakhOmkar BaitOlga TaranErica LastufkaMiroslava Dessauges-ZavadskyTaras HolotyakDaniel SchaererSlava Voloshynovskiy",
        "links": "http://dx.doi.org/10.1051/0004-6361/202347948",
        "entry_id": "http://arxiv.org/abs/2402.10204v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10204v1",
        "summary": "Reconstructing sky models from dirty radio images for accurate source\nlocalization and flux estimation is crucial for studying galaxy evolution at\nhigh redshift, especially in deep fields using instruments like the Atacama\nLarge Millimetre Array (ALMA). With new projects like the Square Kilometre\nArray (SKA), there's a growing need for better source extraction methods.\nCurrent techniques, such as CLEAN and PyBDSF, often fail to detect faint\nsources, highlighting the need for more accurate methods. This study proposes\nusing stochastic neural networks to rebuild sky models directly from dirty\nimages. This method can pinpoint radio sources and measure their fluxes with\nrelated uncertainties, marking a potential improvement in radio source\ncharacterization. We tested this approach on 10164 images simulated with the\nCASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied\nconditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models\nreconstruction, then used Photutils to determine source coordinates and fluxes,\nassessing the model's performance across different water vapor levels. Our\nmethod showed excellence in source localization, achieving more than 90%\ncompleteness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed\nPyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in\nthe test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional\nDDPMs is a powerful tool for image-to-image translation, yielding accurate and\nrobust characterisation of radio sources, and outperforming existing\nmethodologies. While this study underscores its significant potential for\napplications in radio astronomy, we also acknowledge certain limitations that\naccompany its usage, suggesting directions for further refinement and research.",
        "updated": "2024-02-15 18:57:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10204v1"
    },
    {
        "title": "Is Continual Learning Ready for Real-world Challenges?",
        "authors": "Theodora KontogianniYuanwen YueSiyu TangKonrad Schindler",
        "links": "http://arxiv.org/abs/2402.10130v1",
        "entry_id": "http://arxiv.org/abs/2402.10130v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10130v1",
        "summary": "Despite continual learning's long and well-established academic history, its\napplication in real-world scenarios remains rather limited. This paper contends\nthat this gap is attributable to a misalignment between the actual challenges\nof continual learning and the evaluation protocols in use, rendering proposed\nsolutions ineffective for addressing the complexities of real-world setups. We\nvalidate our hypothesis and assess progress to date, using a new 3D semantic\nsegmentation benchmark, OCL-3DSS. We investigate various continual learning\nschemes from the literature by utilizing more realistic protocols that\nnecessitate online and continual learning for dynamic, real-world scenarios\n(eg., in robotics and 3D vision applications). The outcomes are sobering: all\nconsidered methods perform poorly, significantly deviating from the upper bound\nof joint offline training. This raises questions about the applicability of\nexisting methods in realistic settings. Our paper aims to initiate a paradigm\nshift, advocating for the adoption of continual learning methods through new\nexperimental protocols that better emulate real-world conditions to facilitate\nbreakthroughs in the field.",
        "updated": "2024-02-15 17:34:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10130v1"
    },
    {
        "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering",
        "authors": "Abdullah HamdiLuke Melas-KyriaziGuocheng QianJinjie MaiRuoshi LiuCarl VondrickBernard GhanemAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2402.10128v1",
        "entry_id": "http://arxiv.org/abs/2402.10128v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10128v1",
        "summary": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D\nreconstruction and generation. However, it may require a large number of\nGaussians, which creates a substantial memory footprint. This paper introduces\nGES (Generalized Exponential Splatting), a novel representation that employs\nGeneralized Exponential Function (GEF) to model 3D scenes, requiring far fewer\nparticles to represent a scene and thus significantly outperforming Gaussian\nSplatting methods in efficiency with a plug-and-play replacement ability for\nGaussian-based utilities. GES is validated theoretically and empirically in\nboth principled 1D setup and realistic 3D scenes.\n  It is shown to represent signals with sharp edges more accurately, which are\ntypically challenging for Gaussians due to their inherent low-pass\ncharacteristics. Our empirical analysis demonstrates that GEF outperforms\nGaussians in fitting natural-occurring signals (e.g. squares, triangles, and\nparabolic signals), thereby reducing the need for extensive splitting\noperations that increase the memory footprint of Gaussian Splatting. With the\naid of a frequency-modulated loss, GES achieves competitive performance in\nnovel-view synthesis benchmarks while requiring less than half the memory\nstorage of Gaussian Splatting and increasing the rendering speed by up to 39%.\nThe code is available on the project website https://abdullahamdi.com/ges .",
        "updated": "2024-02-15 17:32:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10128v1"
    }
]