[
    {
        "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction",
        "authors": "Yunfan JiangChen WangRuohan ZhangJiajun WuLi Fei-Fei",
        "links": "http://arxiv.org/abs/2405.10315v1",
        "entry_id": "http://arxiv.org/abs/2405.10315v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10315v1",
        "summary": "Learning in simulation and transferring the learned policy to the real world\nhas the potential to enable generalist robots. The key challenge of this\napproach is to address simulation-to-reality (sim-to-real) gaps. Previous\nmethods often require domain-specific knowledge a priori. We argue that a\nstraightforward way to obtain such knowledge is by asking humans to observe and\nassist robot policy execution in the real world. The robots can then learn from\nhumans to close various sim-to-real gaps. We propose TRANSIC, a data-driven\napproach to enable successful sim-to-real transfer based on a human-in-the-loop\nframework. TRANSIC allows humans to augment simulation policies to overcome\nvarious unmodeled sim-to-real gaps holistically through intervention and online\ncorrection. Residual policies can be learned from human corrections and\nintegrated with simulation policies for autonomous execution. We show that our\napproach can achieve successful sim-to-real transfer in complex and\ncontact-rich manipulation tasks such as furniture assembly. Through synergistic\nintegration of policies learned in simulation and from humans, TRANSIC is\neffective as a holistic approach to addressing various, often coexisting\nsim-to-real gaps. It displays attractive properties such as scaling with human\neffort. Videos and code are available at https://transic-robot.github.io/",
        "updated": "2024-05-16 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10315v1"
    },
    {
        "title": "How Far Are We From AGI",
        "authors": "Tao FengChuanyang JinJingyu LiuKunlun ZhuHaoqin TuZirui ChengGuanyu LinJiaxuan You",
        "links": "http://arxiv.org/abs/2405.10313v1",
        "entry_id": "http://arxiv.org/abs/2405.10313v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10313v1",
        "summary": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. Yet, the\nescalating demands on AI have highlighted the limitations of AI's current\nofferings, catalyzing a movement towards Artificial General Intelligence (AGI).\nAGI, distinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing works have summarized\nspecific recent advancements of AI, they lack a comprehensive discussion of\nAGI's definitions, goals, and developmental trajectories. Different from\nexisting survey papers, this paper delves into the pivotal questions of our\nproximity to AGI and the strategies necessary for its realization through\nextensive surveys, discussions, and original perspectives. We start by\narticulating the requisite capability frameworks for AGI, integrating the\ninternal, interface, and system dimensions. As the realization of AGI requires\nmore advanced capabilities and adherence to stringent constraints, we further\ndiscuss necessary AGI alignment technologies to harmonize these factors.\nNotably, we emphasize the importance of approaching AGI responsibly by first\ndefining the key levels of AGI progression, followed by the evaluation\nframework that situates the status-quo, and finally giving our roadmap of how\nto reach the pinnacle of AGI. Moreover, to give tangible insights into the\nubiquitous impact of the integration of AI, we outline existing challenges and\npotential pathways toward AGI in multiple domains. In sum, serving as a\npioneering exploration into the current state and future trajectory of AGI,\nthis paper aims to foster a collective comprehension and catalyze broader\npublic discussions among researchers and practitioners on AGI.",
        "updated": "2024-05-16 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10313v1"
    },
    {
        "title": "Stochastic Q-learning for Large Discrete Action Spaces",
        "authors": "Fares FouratiVaneet AggarwalMohamed-Slim Alouini",
        "links": "http://arxiv.org/abs/2405.10310v1",
        "entry_id": "http://arxiv.org/abs/2405.10310v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10310v1",
        "summary": "In complex environments with large discrete action spaces, effective\ndecision-making is critical in reinforcement learning (RL). Despite the\nwidespread use of value-based RL approaches like Q-learning, they come with a\ncomputational burden, necessitating the maximization of a value function over\nall actions in each iteration. This burden becomes particularly challenging\nwhen addressing large-scale problems and using deep neural networks as function\napproximators. In this paper, we present stochastic value-based RL approaches\nwhich, in each iteration, as opposed to optimizing over the entire set of $n$\nactions, only consider a variable stochastic set of a sublinear number of\nactions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic\nvalue-based RL methods include, among others, Stochastic Q-learning, StochDQN,\nand StochDDQN, all of which integrate this stochastic approach for both\nvalue-function updates and action selection. The theoretical convergence of\nStochastic Q-learning is established, while an analysis of stochastic\nmaximization is provided. Moreover, through empirical validation, we illustrate\nthat the various proposed approaches outperform the baseline methods across\ndiverse environments, including different control problems, achieving\nnear-optimal average returns in significantly reduced time.",
        "updated": "2024-05-16 17:58:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10310v1"
    },
    {
        "title": "4D Panoptic Scene Graph Generation",
        "authors": "Jingkang YangJun CenWenxuan PengShuai LiuFangzhou HongXiangtai LiKaiyang ZhouQifeng ChenZiwei Liu",
        "links": "http://arxiv.org/abs/2405.10305v1",
        "entry_id": "http://arxiv.org/abs/2405.10305v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10305v1",
        "summary": "We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.",
        "updated": "2024-05-16 17:56:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10305v1"
    },
    {
        "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
        "authors": "Yu GuiYing JinZhimei Ren",
        "links": "http://arxiv.org/abs/2405.10301v1",
        "entry_id": "http://arxiv.org/abs/2405.10301v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10301v1",
        "summary": "Before deploying outputs from foundation models in high-stakes tasks, it is\nimperative to ensure that they align with human values. For instance, in\nradiology report generation, reports generated by a vision-language model must\nalign with human evaluations before their use in medical decision-making. This\npaper presents Conformal Alignment, a general framework for identifying units\nwhose outputs meet a user-specified alignment criterion. It is guaranteed that\non average, a prescribed fraction of selected units indeed meet the alignment\ncriterion, regardless of the foundation model or the data distribution. Given\nany pre-trained model and new units with model-generated outputs, Conformal\nAlignment leverages a set of reference data with ground-truth alignment status\nto train an alignment predictor. It then selects new units whose predicted\nalignment scores surpass a data-dependent threshold, certifying their\ncorresponding outputs as trustworthy. Through applications to question\nanswering and radiology report generation, we demonstrate that our method is\nable to accurately identify units with trustworthy outputs via lightweight\ntraining over a moderate amount of reference data. En route, we investigate the\ninformativeness of various features in alignment prediction and combine them\nwith standard models to construct the alignment predictor.",
        "updated": "2024-05-16 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10301v1"
    }
]