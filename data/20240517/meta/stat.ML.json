[
    {
        "title": "Stochastic Q-learning for Large Discrete Action Spaces",
        "authors": "Fares FouratiVaneet AggarwalMohamed-Slim Alouini",
        "links": "http://arxiv.org/abs/2405.10310v1",
        "entry_id": "http://arxiv.org/abs/2405.10310v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10310v1",
        "summary": "In complex environments with large discrete action spaces, effective\ndecision-making is critical in reinforcement learning (RL). Despite the\nwidespread use of value-based RL approaches like Q-learning, they come with a\ncomputational burden, necessitating the maximization of a value function over\nall actions in each iteration. This burden becomes particularly challenging\nwhen addressing large-scale problems and using deep neural networks as function\napproximators. In this paper, we present stochastic value-based RL approaches\nwhich, in each iteration, as opposed to optimizing over the entire set of $n$\nactions, only consider a variable stochastic set of a sublinear number of\nactions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic\nvalue-based RL methods include, among others, Stochastic Q-learning, StochDQN,\nand StochDDQN, all of which integrate this stochastic approach for both\nvalue-function updates and action selection. The theoretical convergence of\nStochastic Q-learning is established, while an analysis of stochastic\nmaximization is provided. Moreover, through empirical validation, we illustrate\nthat the various proposed approaches outperform the baseline methods across\ndiverse environments, including different control problems, achieving\nnear-optimal average returns in significantly reduced time.",
        "updated": "2024-05-16 17:58:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10310v1"
    },
    {
        "title": "Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift",
        "authors": "Jiawei GeDebarghya MukherjeeJianqing Fan",
        "links": "http://arxiv.org/abs/2405.10302v1",
        "entry_id": "http://arxiv.org/abs/2405.10302v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10302v1",
        "summary": "As machine learning models are increasingly deployed in dynamic environments,\nit becomes paramount to assess and quantify uncertainties associated with\ndistribution shifts. A distribution shift occurs when the underlying\ndata-generating process changes, leading to a deviation in the model's\nperformance. The prediction interval, which captures the range of likely\noutcomes for a given prediction, serves as a crucial tool for characterizing\nuncertainties induced by their underlying distribution. In this paper, we\npropose methodologies for aggregating prediction intervals to obtain one with\nminimal width and adequate coverage on the target domain under unsupervised\ndomain shift, under which we have labeled samples from a related source domain\nand unlabeled covariates from the target domain. Our analysis encompasses\nscenarios where the source and the target domain are related via i) a bounded\ndensity ratio, and ii) a measure-preserving transformation. Our proposed\nmethodologies are computationally efficient and easy to implement. Beyond\nillustrating the performance of our method through a real-world dataset, we\nalso delve into the theoretical details. This includes establishing rigorous\ntheoretical guarantees, coupled with finite sample bounds, regarding the\ncoverage and width of our prediction intervals. Our approach excels in\npractical applications and is underpinned by a solid theoretical framework,\nensuring its reliability and effectiveness across diverse contexts.",
        "updated": "2024-05-16 17:55:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10302v1"
    },
    {
        "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
        "authors": "Yu GuiYing JinZhimei Ren",
        "links": "http://arxiv.org/abs/2405.10301v1",
        "entry_id": "http://arxiv.org/abs/2405.10301v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10301v1",
        "summary": "Before deploying outputs from foundation models in high-stakes tasks, it is\nimperative to ensure that they align with human values. For instance, in\nradiology report generation, reports generated by a vision-language model must\nalign with human evaluations before their use in medical decision-making. This\npaper presents Conformal Alignment, a general framework for identifying units\nwhose outputs meet a user-specified alignment criterion. It is guaranteed that\non average, a prescribed fraction of selected units indeed meet the alignment\ncriterion, regardless of the foundation model or the data distribution. Given\nany pre-trained model and new units with model-generated outputs, Conformal\nAlignment leverages a set of reference data with ground-truth alignment status\nto train an alignment predictor. It then selects new units whose predicted\nalignment scores surpass a data-dependent threshold, certifying their\ncorresponding outputs as trustworthy. Through applications to question\nanswering and radiology report generation, we demonstrate that our method is\nable to accurately identify units with trustworthy outputs via lightweight\ntraining over a moderate amount of reference data. En route, we investigate the\ninformativeness of various features in alignment prediction and combine them\nwith standard models to construct the alignment predictor.",
        "updated": "2024-05-16 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10301v1"
    },
    {
        "title": "Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees",
        "authors": "Feng Ruan",
        "links": "http://arxiv.org/abs/2405.10289v1",
        "entry_id": "http://arxiv.org/abs/2405.10289v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10289v1",
        "summary": "In nonsmooth, nonconvex stochastic optimization, understanding the uniform\nconvergence of subdifferential mappings is crucial for analyzing stationary\npoints of sample average approximations of risk as they approach the population\nrisk. Yet, characterizing this convergence remains a fundamental challenge.\n  This work introduces a novel perspective by connecting the uniform\nconvergence of subdifferential mappings to that of subgradient mappings as\nempirical risk converges to the population risk. We prove that, for stochastic\nweakly-convex objectives, and within any open set, a uniform bound on the\nconvergence of subgradients -- chosen arbitrarily from the corresponding\nsubdifferential sets -- translates to a uniform bound on the convergence of the\nsubdifferential sets itself, measured by the Hausdorff metric.\n  Using this technique, we derive uniform convergence rates for subdifferential\nsets of stochastic convex-composite objectives. Our results do not rely on key\ndistributional assumptions in the literature, which require the population and\nfinite sample subdifferentials to be continuous in the Hausdorff metric, yet\nstill provide tight convergence rates. These guarantees lead to new insights\ninto the nonsmooth landscapes of such objectives within finite samples.",
        "updated": "2024-05-16 17:49:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10289v1"
    },
    {
        "title": "On Partially Unitary Learning",
        "authors": "Mikhail Gennadievich BelovVladislav Gennadievich Malyshkin",
        "links": "http://arxiv.org/abs/2405.10263v1",
        "entry_id": "http://arxiv.org/abs/2405.10263v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10263v1",
        "summary": "The problem of an optimal mapping between Hilbert spaces $IN$ of\n$\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set\nof wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots\nM$, is formulated as an optimization problem maximizing the total fidelity\n$\\sum_{l=1}^{M} \\omega^{(l)}\n\\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability\npreservation constraints on $\\mathcal{U}$ (partial unitarity). Constructed\noperator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it\nis a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times\n\\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN}\n\\mathcal{U}^{\\dagger}$. An iteration algorithm finding the global maximum of\nthis optimization problem is developed and it's application to a number of\nproblems is demonstrated. A software product implementing the algorithm is\navailable from the authors.",
        "updated": "2024-05-16 17:13:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10263v1"
    }
]