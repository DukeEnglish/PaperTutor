Stochastic Q-learning for Large Discrete Action Spaces
FaresFourati1 VaneetAggarwal2 Mohamed-SlimAlouini1
Abstract advances in the field, a significant challenge lies in nav-
igating complex environments with large discrete action
Incomplexenvironmentswithlargediscreteac-
spaces(Dulac-Arnoldetal.,2015;2021). Insuchscenarios,
tion spaces, effective decision-making is criti-
standard RL algorithms suffer in terms of computational
cal in reinforcement learning (RL). Despite the
efficiency(Akkermanetal.,2023). Identifyingtheoptimal
widespread use of value-based RL approaches
actionsmightentailcyclingthroughallofthem,ingeneral,
likeQ-learning,theycomewithacomputational
multiple times within different states, which is computa-
burden,necessitatingthemaximizationofavalue
tionallyexpensiveandmaybecomeprohibitivewithlarge
functionoverallactionsineachiteration. This
discreteactionspaces(Tessleretal.,2019).
burden becomes particularly challenging when
addressinglarge-scaleproblemsandusingdeep Suchchallengesapplytovariousdomains,includingcombi-
neural networks as function approximators. In natorialoptimization(Mazyavkinaetal.,2021;Fouratietal.,
thispaper,wepresentstochasticvalue-basedRL 2023;2024b;a),naturallanguageprocessing(Heetal.,2015;
approacheswhich,ineachiteration,asopposed 2016a;b;Tessleretal.,2019),communicationsandnetwork-
to optimizing over the entire set of n actions, ing(Luongetal.,2019;Fourati&Alouini,2021),recom-
only consider a variable stochastic set of a sub- mendationsystems(Dulac-Arnoldetal.,2015),transporta-
linear number of actions, possibly as small as tion(Al-Abbasietal.,2019;Haliemetal.,2021;Lietal.,
O(log(n)). Thepresentedstochasticvalue-based 2022), and robotics (Dulac-Arnold et al., 2015; Tavakoli
RL methods include, among others, Stochastic et al., 2018; Tang & Agrawal, 2020; Seyde et al., 2021;
Q-learning,StochDQN,andStochDDQN,allof 2022;Gonzalezetal.,2023;Ireland&Montana,2024). Al-
whichintegratethisstochasticapproachforboth thoughtailoredsolutionsleveragingactionspacestructures
value-functionupdatesandactionselection. The anddimensionsmaysufficeinspecificcontexts, theirap-
theoreticalconvergenceofStochasticQ-learning plicabilityacrossdiverseproblems,possiblyunstructured,
isestablished,whileananalysisofstochasticmax- stillneedstobeexpanded. Wecomplementtheseworksby
imizationisprovided. Moreover,throughempir- proposingageneralmethodthataddressesabroadspectrum
icalvalidation,weillustratethatthevariouspro- ofproblems,accommodatingstructuredandunstructured
posedapproachesoutperformthebaselinemeth- singleandmulti-dimensionallargediscreteactionspaces.
ods across diverse environments, including dif-
Value-basedandactor-basedapproachesarebothprominent
ferentcontrolproblems,achievingnear-optimal
approachesinRL.Value-basedapproaches,whichentailthe
averagereturnsinsignificantlyreducedtime.
agentimplicitlyoptimizingitspolicybymaximizingavalue
function,demonstratesuperiorgeneralizationcapabilities
butdemandsignificantcomputationalresources,particularly
1.Introduction
incomplexsettings. Conversely, actor-basedapproaches,
Reinforcementlearning(RL),acontinuallyevolvingfield whichentailtheagentdirectlyoptimizingitspolicy,offer
ofmachinelearning,hasachievednotablesuccesses,espe- computationalefficiencybutoftenencounterchallengesin
ciallywhencombinedwithdeeplearning(Sutton&Barto, generalizingacrossmultipleandunexploredactions(Dulac-
2018; Wang et al., 2022). While there have been several Arnoldetal.,2015).Whilebothholduniqueadvantagesand
challenges,theyrepresentdistinctavenuesforaddressing
1DepartmentofComputer,ElectricalandMathematicalSci-
thecomplexitiesofdecision-makinginlargeactionspaces.
enceandEngineering,KingAbdullahUniversityofScienceand
However, comparing them falls outside the scope of this
Technology(KAUST),Thuwal,KSA.2SchoolofIndustrialEn-
gineering, Purdue University, West Lafayette, IN 47907, USA. work. Whilesomepreviousmethodshavefocusedonthe
Correspondenceto:FaresFourati<fares.fourati@kaust.edu.sa>. latter (Dulac-Arnold et al., 2015), our work concentrates
ontheformer. Specifically, weaimtoexploitthenatural
Proceedings of the 41st International Conference on Machine
generalizationinherentinvalue-basedRLapproacheswhile
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
reducingtheirper-stepcomputationalcomplexity.
theauthor(s).
1
4202
yaM
61
]GL.sc[
1v01301.5042:viXraStochasticQ-learningforLargeDiscreteActionSpaces
Q-learning,asintroducedbyWatkins&Dayan(1992),for We propose Stochastic Q-learning, Stochastic Double Q-
discrete action and state spaces, stands out as one of the learning, StochDQN, and StochDDQN, which are ob-
mostfamousexamplesofvalue-basedRLmethodsandre- tained by changing max and argmax to stochmax and
mainsoneofthemostwidelyusedonesinthefield. Asan stochargmaxintheQ-learning(Watkins&Dayan,1992),
off-policylearningmethod,itdecouplesthelearningprocess theDoubleQ-learning(Hasselt,2010),thedeepQ-network
fromtheagent’scurrentpolicy,allowingittoleveragepast (DQN)(Mnihetal.,2013)andthedoubleDQN(DDQN)
experiencesfromvarioussources,whichbecomesadvanta- (Van Hasselt et al., 2016), respectively. Furthermore, we
geousincomplexenvironments. IneachstepofQ-learning, observed that our approach works even for the on-policy
the agent updates its action value estimates based on the Sarsa(Rummery&Niranjan,1994).
observedrewardandtheestimatedvalueofthebestaction
Weconductatheoreticalanalysisoftheproposedmethod,
inthenextstate.
provingtheconvergenceofStochasticQ-learning, which
SomeapproacheshavebeenproposedtoapplyQ-learning integrates these techniques for action selection and value
tocontinuousstatespaces,leveragingdeepneuralnetworks updates,andestablishingalowerboundontheprobability
(Mnihetal.,2013;VanHasseltetal.,2016). Moreover,sev- of sampling an optimal action from a random set of size
eralimprovementshavealsobeensuggestedtoaddressits ⌈log(n)⌉andanalyzetheerrorofstochasticmaximization
inherentestimationbias(Hasselt,2010;VanHasseltetal., comparedtoexactmaximization. Furthermore,weevaluate
2016; Zhang et al., 2017; Lan et al., 2020; Wang et al., theproposedRLalgorithmsonenvironmentsfromGymna-
2021). However,despitethedifferentprogressanditsnu- sium(Brockmanetal.,2016). ForthestochasticdeepRL
merousadvantages,asignificantchallengestillneedstobe algorithms,theevaluationswereperformedoncontroltasks
solved in Q-learning-like methods when confronted with withinthemulti-jointdynamicswithcontact(MuJoCo)en-
largediscreteactionspaces. Thecomputationalcomplexity vironment(Todorovetal.,2012)withdiscretizedactions
associatedwithselectingactionsandupdatingQ-functions (Dulac-Arnoldetal.,2015;Tavakolietal.,2018;Tang&
increasesproportionallywiththeincreasingnumberofac- Agrawal, 2020). These evaluations demonstrate that the
tions,whichrenderstheconventionalapproachimpractical stochastic approaches outperform non-stochastic ones re-
as the number of actions substantially increases. Conse- gardingwalltimespeedupandsometimesrewards. Ourkey
quently, we confront a crucial question: Is it possible to contributionsaresummarizedasfollows:
mitigatethecomplexityofthedifferentQ-learningmethods
whilemaintainingagoodperformance?
• Weintroducenovelstochasticmaximizationtechniques
denotedasstochmaxandstochargmax,offeringacom-
This work proposes a novel, simple, and practical ap-
pelling alternative to conventional deterministic maxi-
proachforhandlinggeneral,possiblyunstructured,single-
mizationoperations,particularlybeneficialforhandling
dimensional or multi-dimensional, large discrete action
largediscreteactionspaces,ensuringsub-linearcomplex-
spaces. Ourapproachtargetsthecomputationalbottleneck
ityconcerningthenumberofactions.
invalue-basedmethodscausedbythesearchforamaximum
(maxandargmax)ineverylearningiteration,whichscales
• We present a suite of value-based algorithms suit-
asO(n),i.e.,linearlywiththenumberofpossibleactionsn.
able for large discrete actions, including Stochastic Q-
Throughrandomization,wecanreducethislinearper-step
learning,StochasticSarsa,StochasticDoubleQ-learning,
computationalcomplexitytologarithmic.
StochDQN, and StochDDQN, which integrate stochas-
Weintroducestochmaxandstochargmax,which,instead tic maximization within Q-learning, Sarsa, Double Q-
ofexhaustivelysearchingfortheprecisemaximumacross learning,DQN,andDDQN,respectively.
theentiresetofactions, relyonatmosttworandomsub-
sets of actions, both of sub-linear sizes, possibly each of • Weanalyzestochasticmaximizationanddemonstratethe
size⌈log(n)⌉. Thefirstsubsetisrandomlysampledfrom convergenceofStochasticQ-learning. Furthermore,we
the complete set of actions, and the second from the pre- empiricallyvalidateourapproachtotasksfromtheGym-
viouslyexploitedactions. Thesestochasticmaximization nasiumandMuJoCOenvironments,encompassingvari-
techniques amortize the computational overhead of stan- ousdimensionaldiscretizedactions.
dardmaximizationoperationsinvariousQ-learningmeth-
ods (Watkins & Dayan, 1992; Hasselt, 2010; Mnih et al., 2.RelatedWorks
2013;VanHasseltetal.,2016). Stochasticmaximization
methodssignificantlyacceleratetheagent’ssteps,including WhileRLhasshownpromiseindiversedomains,practical
actionselectionandvalue-functionupdatesinvalue-based applicationsoftengrapplewithreal-worldcomplexities. A
RLmethods,makingthempracticalforhandlingchalleng- significanthurdleariseswhendealingwithlargediscrete
ing,large-scale,real-worldproblems. actionspaces(Dulac-Arnoldetal.,2015;2021). Previous
researchhasinvestigatedstrategiestoaddressthischallenge
2StochasticQ-learningforLargeDiscreteActionSpaces
byleveragingthecombinatorialorthedimensionalstruc- neuralnetworkwastrainedtopredicttheoptimalactionin
turesintheactionspace(Heetal.,2016b;Tavakolietal., combinationwithauniformsearch. Thisapproachinvolves
2018;Tessleretal.,2019;Delarueetal.,2020;Seydeetal., theuseofanexpensiveautoregressiveproposaldistribution
2021;2022;Fouratietal.,2023;2024b;a;Akkermanetal., to generate n′ actions and samples a large number of ac-
2023; Fourati et al., 2024b;a; Ireland & Montana, 2024). tions(m),thusremainingcomputationallyexpensive,with
Forexample,Heetal.(2016b)leveragedthecombinatorial O(n′+m). In(Metzetal.,2017),sequentialDQNallows
structureoftheirlanguageproblemthroughsub-actionem- theagenttochoosesub-actionsonebyone,whichincreases
beddings. Compressedsensingwasemployedin(Tessler thenumberofstepsneededtosolveaproblemandrequires
et al., 2019) for text-based games with combinatorial ac- dstepswithalinearcomplexityofO(i)foradiscretization
tions. Delarueetal.(2020)formulatedthecombinatorial granularityi. Additionally,Tavakolietal.(2018)employsa
action decision of a vehicle routing problem as a mixed- branchingtechniquewithduellingDQNforcombinatorial
integer program. Moreover, Akkerman et al. (2023) in- controlproblems.TheirapproachhasacomplexityofO(di)
troduceddynamicneighbourhoodconstructionspecifically for actions with discretization granularity i and d dimen-
for structured combinatorial large discrete action spaces. sions, whereas our method, in a similar setting, achieves
Previous works tailored solutions for multi-dimensional O(dlog(i)). Anotherlineofworkintroducesactionelim-
spacessuchasthosein(Seydeetal.,2021;2022;Ireland ination techniques, such as the action elimination DQN
& Montana, 2024), among others, while practical in the (Zahavyetal.,2018),whichemploysanactionelimination
multi-dimensional spaces, may not be helpful for single- networkguidedbyanexternaleliminationsignalfromthe
dimensionallargeactionspaces. Whilerelyingonthestruc- environment. However,itrequiresthisdomain-specificsig-
ture of the action space is practical in some settings, not nal and can be computationally expensive (O(n′) where
allproblemswithlargeactionspacesaremulti-dimensional n′ ≤ narethenumberofremainingactions). Incontrast,
orstructured. Wecomplementtheseworksbymakingno curriculumlearning,asproposedbyFarquharetal.(2020),
assumptionsaboutthestructureoftheactionspace. initiallylimitsanagent’sactionspace,graduallyexpanding
itduringtrainingforefficientexploration. However,itsef-
Some approaches have proposed factorizing the action
fectivenessreliesonhavinganinformativerestrictedaction
spaces to reduce their size. For example, these include
space, and as the action space size grows, its complexity
factorizingintobinarysubspaces(Lagoudakis&Parr,2003;
scaleslinearlywithitssize,eventuallyreachingO(n).
Sallans&Hinton,2004;Pazis&Parr,2011;Dulac-Arnold
etal.,2012),expertdemonstration(Tennenholtz&Mannor, Inthecontextofcombinatorialbanditswithasinglestate
2019),tensorfactorization(Mahajanetal.,2021),andsym- but large discrete action spaces, previous works have ex-
bolicrepresentations(Cui&Khardon,2016). Additionally, ploitedthecombinatorialstructureofactions,whereeach
somehierarchicalandmulti-agentRLapproachesemployed actionisasubsetofmainarms. Forinstance,forsubmodu-
factorizationaswell(Zhangetal.,2020;Kimetal.,2021; larrewardfunctions,whichimplydiminishingreturnswhen
Pengetal.,2021;Endersetal.,2023). Whilesomeofthese adding arms, in (Fourati et al., 2023) and (Fourati et al.,
methodseffectivelyhandlelargeactionspacesforcertain 2024b),stochasticgreedyalgorithmsareusedtoavoidexact
problems, they necessitate the design of a representation search. Theformerevaluatesthemarginalgainsofadding
foreachdiscreteaction. Eventhen,forsomeproblems,the andremovingsub-actions(arms),whilethelatterassumes
resultingspacemaystillbelarge. monotonicrewardsandconsidersaddingthebestarmuntil
a cardinality constraint is met. For general reward func-
Methodspresentedin(VanHasselt&Wiering,2009;Dulac-
tions,Fouratietal.(2024a)proposeusingapproximation
Arnoldetal.,2015;Wangetal.,2020)combinecontinuous-
algorithms to evaluate and add sub-actions. While these
actionpolicygradientswithnearestneighboursearchtogen-
methodsarepracticalforbandits,theyexploitthecombina-
eratecontinuousactionsandidentifythenearestdiscreteac-
torialstructureoftheirproblemsandconsiderasingle-state
tions. Theseareinterestingmethodsbutrequirecontinuous-
scenario,whichisdifferentfromgeneralRLproblems.
to-discretemappingandaremainlypolicy-basedratherthan
value-basedapproaches. IntheworksofKalashnikovetal. While some approaches above are practical for handling
(2018)andQuillenetal.(2018),thecross-entropymethod specific problems with large discrete action spaces, they
(Rubinstein,1999)wasutilizedtoapproximateactionmaxi- often exploit the dimensional or combinatorial structures
mization. Thisapproachrequiresmultipleiterations(r)for inherentintheirconsideredproblems. Incontrast,wecom-
asingleactionselection.Duringeachiteration,itsamplesn′ plementtheseapproachesbyproposingasolutiontotackle
values,wheren′ <n,fitsaGaussiandistributiontom<n′ any general, potentially unstructured, single-dimensional
ofthesesamples,andsubsequentlydrawsanewbatchof or multi-dimensional, large discrete action space without
n′samplesfromthisGaussiandistribution. Asaresult,this relyingonstructureassumptions. Ourproposedsolutionis
approximationremainscostly,withacomplexityofO(rn′). general,simple,andefficient.
Additionally,intheworkofVandeWieleetal.(2020),a
3StochasticQ-learningforLargeDiscreteActionSpaces
3.ProblemDescription When representing the value function as a parameterized
function,suchasaneuralnetwork,takingonlythecurrent
In the context of a Markov decision process (MDP), we
statesasinputandoutputtingthevaluesforallactions,as
havespecificcomponents: afinitesetofactionsdenotedas
proposed in DQN (Mnih et al., 2013), the network must
A,afinitesetofstatesdenotedasS,atransitionprobability
accommodatealargenumberofoutputnodes,whichresults
distribution P : S×A×S → [0,1], a bounded reward
inincreasingmemoryoverheadandnecessitatesextensive
functionr : S×A → R,andadiscountfactorγ ∈ [0,1].
predictions and maximization over these final outputs in
Furthermore,fortimestept,wedenotethechosenaction
thelastlayer. Anotablepointaboutthisapproachisthat
as a , the current state as s , and the received reward as
t t itdoesnotexploitcontextualinformation(representation)
r ≜ r(s ,a ). Additionally, for time step t, we define a
t t t ofactions,ifavailable,whichleadstolowergeneralization
learningratefunctionα :S×A→[0,1].
t capabilityacrossactionswithsimilarfeaturesandfailsto
Thecumulativerewardanagentreceivesduringanepisode generalizeovernewactions.
inanMDPwithvariablelengthtimeT isthereturnR . It
t Previousworkshaveconsideredgeneralizationoveractions
iscalculatedasthediscountedsumofrewardsfromtime
bytakingthefeaturesofanactionaandthecurrentstates
steptuntiltheepisodeterminates: R ≜(cid:80)T γi−tr . RL
t i=t i asinputstotheQ-networkandpredictingitsvalue(Zahavy
aims to learn a policy π : S → A mapping states to ac-
etal.,2018;Metzetal.,2017;VandeWieleetal.,2020).
tionsthatmaximizetheexpectedreturnacrossallepisodes.
However,itleadstofurthercomplicationswhenthevalue
Thestate-actionvaluefunction,denotedasQπ(s,a),repre-
functionismodeledasaparameterizedfunctionwithboth
sentstheexpectedreturnwhenstartingfromagivenstates,
statesandactionaasinputs. Althoughthisapproachal-
takingactiona,andfollowingapolicyπ afterwards. The
lows for improved generalization across the action space
functionQπcanbeexpressedrecursivelyusingtheBellman
byleveragingcontextualinformationfromeachactionand
equation:
generalizingacrosssimilarones,itrequiresevaluatingthe
(cid:88) functionforeachactionwithintheactionsetA. Thisresults
Qπ(s,a)=r(s,a)+γ P(s′|s,a)Qπ(s′,π(s′)). (1)
inalinearincreaseinthenumberoffunctioncallsasthe
s′∈S
number of actions grows. This scalability issue becomes
particularly problematic when dealing with computation-
Twomaincategoriesofpoliciesarecommonlyemployedin
allyexpensivefunctionapproximators,suchasdeepneural
RLsystems: value-basedandactor-basedpolicies(Sutton
networks (Dulac-Arnold et al., 2015). Addressing these
&Barto,2018). Thisstudyprimarilyconcentratesonthe
challengesformsthemotivationbehindthiswork.
former type, where the value function directly influences
thepolicy’sdecisions. Anexampleofavalue-basedpolicy
inastatesinvolvesanε -greedyalgorithm,selectingthe 4.ProposedApproach
s
actionwiththehighestQ-functionvaluewithprobability
Toalleviatethecomputationalburdenassociatedwithmax-
(1−ε ),whereε ≥0,functionofthestates,requiringthe
s s
imizing a Q-function at each time step, especially when
useofargmaxoperation,asfollows:
dealing with large action spaces, we introduce stochastic
(cid:40)
playrandomly withproba. ϵ maximizationmethodswithsub-linearcomplexityrelative
π Q(s)= s (2) to the size of the action set A. Then, we integrate these
argmax Q(s,a) otherwise.
a∈A methodsintodifferentvalue-basedRLalgorithms.
Furthermore,duringthetraining,toupdatetheQ-function,
Q-learning(Watkins&Dayan,1992),forexample,usesthe 4.1.StochasticMaximization
followingupdaterule,whichrequiresamaxoperation:
Weintroducestochasticmaximizationasanalternativeto
Q (s ,a )=(1−α (s ,a ))Q (s ,a ) maximizationwhendealingwithlargediscreteactionspaces.
t+1 t t t t t t t t
(cid:20) (cid:21) Insteadofconductinganexhaustivesearchfortheprecise
+α (s ,a ) r +γmaxQ (s ,b) . (3) maximumacrosstheentiresetofactionsA,stochasticmaxi-
t t t t t t+1
b∈A mizationsearchesforamaximumwithinastochasticsubset
Therefore, the computational complexity of both the ac- of actions of sub-linear size relative to the total number
tion selections in Eq. (2) and the Q-function updates in ofactions. Inprinciple, anysizecanbeused, tradingoff
Eq. (3)scaleslinearlywiththecardinalitynoftheaction timecomplexityandapproximation. Wemainlyfocuson
set A, making this approach infeasible as the number of O(log(n))toillustratethepowerofthemethodinrecover-
actions increases significantly. The same complexity is- ingQ-learning,evenwithsuchasmallnumberofactions,
suesremainforotherQ-learningvariants,suchasDouble withlogarithmiccomplexity.
Q-learning(Hasselt,2010),DQN(Mnihetal.,2013),and
We consider two approaches to stochastic maximization:
DDQN(VanHasseltetal.,2016),amongseveralothers.
4StochasticQ-learningforLargeDiscreteActionSpaces
memorylessandmemory-basedapproaches. Thememory- Algorithm1StochasticQ-learning
lessonesamplesarandomsubsetofactionsR ⊆ Awith
InitializeQ(s,a)foralls∈S,a∈A
a sublinear size and seeks the maximum within this sub- foreachepisodedo
set. Ontheotherhand,thememory-basedoneexpandsthe Observestates.
randomlysampledsettoincludeafewactionsMwithasub- foreachstepofepisodedo
ChooseafromswithpolicyπS(s).
linearsizefromthelatestexploitedactionsE andusesthe Q
Takeactiona,observer,s′.
combinedsetstosearchforastochasticmaximum. Stochas-
b∗ ←stochargmax Q(s′,b).
ticmaximization,whichmaymisstheexactmaximumin ∆←r+γQ(s′,b∗)b −∈A Q(s,a).
both versions, is always upper-bounded by deterministic Q(s,a)←Q(s,a)+α(s,a)∆.
maximization,whichfindstheexactmaximum. However, s←s′.
endfor
byconstruction,ithassublinearcomplexityinthenumber
endfor
ofactions,makingitappealingwhenmaximizingoverlarge
actionspacesbecomesimpractical.
Formally,givenastates,whichmaybediscreteorcontin- 2inAppendixC,thatreplacethemaxandargmaxopera-
uous,alongwithaQ-function,arandomsubsetofactions tionsinQ-learningandDoubleQ-learningwithstochmax
R⊆A,andamemorysubsetM⊆E (emptyinthememo- andstochargmax,respectively.Furthermore,weintroduce
rylesscase),eachsubsetbeingofsublinearsize,suchasat StochasticSarsa,describedinAlgorithm3inAppendixC,
mostO(log(n))each,thestochmaxisthemaximumvalue whichreplacesthemaximizationinthegreedyactionselec-
computedfromtheunionsetC =R∪M,definedas: tion(argmax)inSarsa.
Ourproposedsolutiontakesadistinctapproachfromthe
stochmaxQ (s,k)≜maxQ (s,k). (4)
t t conventionalmethodofselectingtheactionwiththehigh-
k∈A k∈C
est Q-function value from the complete set of actions A.
Besides,thestochargmaxiscomputedasfollows: Instead,itusesstochasticmaximization,whichfindsamax-
imum within a stochastic subset C ⊆ A, constructed as
stochargmaxQ t(s,k)≜argmaxQ t(s,k). (5) explainedinSection4.1. OurstochasticpolicyπS(s),uses
Q
k∈A k∈C
anε -greedyalgorithm,inagivenstates,withaprobability
s
of(1−ε ),forε >0,isdefinedasfollows:
s s
Intheanalysisofstochasticmaximization,weexploreboth
(cid:40)
memory-basedandmemorylessmaximization. Intheanal- playrandomly withproba. ϵ
ysis and experiments, we consider the random set R to π QS(s)≜
stochargmax Q(s,a) otherwise.
s
consistof⌈log(n)⌉actions. Whenmemory-based, inour a∈A
(6)
experiments,withinagivendiscretestate,weconsiderthe
Furthermore,duringthetraining,toupdatetheQ-function,
twomostrecentlyexploitedactionsinthatstate. Forcon-
ourproposedStochasticQ-learningusesthefollowingrule:
tinuous states, where it is impossible to retain the latest
exploited actions for each state, we consider a randomly Q (s ,a )=(1−α (s ,a ))Q (s ,a )
t+1 t t t t t t t t
sampledsubsetM ⊆ E,whichincludes⌈log(n)⌉actions, (cid:20) (cid:21)
eventhoughtheywereplayedindifferentstates. Wedemon- +α (s ,a ) r +γstochmaxQ (s ,b) . (7)
t t t t t t+1
b∈A
stratethatthisapproachwassufficienttoachievegoodre-
sultsinthebenchmarksconsidered; seeSection7.3. Our WhileStochasticQ-learning,likeQ-learning,employsthe
StochasticQ-learningconvergenceanalysisconsidersmem- same values for action selection and action evaluation,
orylessstochasticmaximizationwitharandomsetRofany StochasticDoubleQ-learning,similartoDoubleQ-learning,
size. learnstwoseparateQ-functions. Foreachupdate,oneQ-
Remark4.1. BysettingC equaltoA,weessentiallyrevert functiondeterminesthepolicy,whiletheotherdetermines
to standard approaches. Consequently, our method is an thevalueofthatpolicy. Bothstochasticlearningmethods
extensionofnon-stochasticmaximization. However,inpur- removethemaximizationbottleneckfromexplorationand
suitofourobjectivetomakeRLpracticalforlargediscrete trainingupdates,makingtheseproposedalgorithmssignifi-
actionspaces,foragivenstates,inouranalysisandexperi- cantlyfasterthantheirdeterministiccounterparts.
ments,wekeeptheunionsetClimitedtoatmost2⌈log(n)⌉,
ensuringsub-linear(logarithmic)complexity. 4.3.StochasticDeepQ-network
WeintroduceStochasticDQN(StochDQN),describedin
4.2.StochasticQ-learning
Algorithm4inAppendixC,andStochasticDDQN(StochD-
WeintroduceStochasticQ-learning,describedinAlgorithm DQN)asefficientvariantsofdeepQ-networks. Thesevari-
1,andStochasticDoubleQ-learning,describedinAlgorithm antssubstitutethemaximizationstepsintheDQN(Mnih
5StochasticQ-learningforLargeDiscreteActionSpaces
et al., 2013) and DDQN (Van Hasselt et al., 2016) algo- similartothoseobtainedfromanoptimalaction. Therefore,
rithmswiththestochasticmaximizationoperations.Inthese thedifferencebetweenstochasticmaximizationandexact
modifiedapproaches,wereplacetheε -greedyexploration maximizationmightbeamoreinformativemetricthanjust
s
strategywiththesameexplorationpolicyasinEq. (6). theprobabilityoffindinganexactmaximizer. Thus,attime
stept, givenstatesandthecurrentestimatedQ-function
For StochDQN, we employ a deep neural network as a
Q ,wedefinetheestimationerrorasβ (s),asfollows:
functionapproximatortoestimatetheaction-valuefunction, t t
representedasQ(s,a;θ) ≈ Q(s,a), whereθ denotesthe β (s)≜maxQ (s,a)−stochmaxQ (s,a). (9)
t t t
weightsoftheQ-network. Thisnetworkistrainedbymin- a∈A a∈A
imizingaseriesoflossfunctionsdenotedasL (θ ),with
i i Furthermore,wedefinethesimilarityratioω (s),asfollows:
theselossfunctionschangingateachiterationiasfollows: t
ω (s)≜stochmaxQ (s,a)/maxQ (s,a). (10)
(cid:104) (cid:105) t t t
L (θ )≜E (y −Q(s,a;θ ))2 , (8) a∈A a∈A
i i s,a∼ρ(·) i i
It can be seen from the definitions that β (s) ≥ 0 and
t
wherey ≜E[r+γstochmax Q(s′,b;θ )|s,a]. In ω (s) ≤ 1. Whilesamplingtheexactmaximizerisnotal-
i b∈A i−1 t
thiscontext,y representsthetargetvalueforaniterationi, wayspossible,near-optimalactionsmayyieldnear-optimal
i
andρ(.)isaprobabilitydistributionthatcoversstatesand values,providinggoodapproximations,i.e.,β (s)≈0and
t
actions. LiketheDQNapproach,wekeeptheparameters ω (s)≈1. Ingeneral,thisdifferencedependsonthevalue
t
fixed from the previous iteration, denoted as θ when distributionovertheactions.
i−1
optimizingthelossfunctionL (θ ).
i i Whilewedonotmakeanyspecificassumptionsaboutthe
Thesetargetvaluesdependonthenetworkweights,which valuedistributioninourwork,wenotethatwithsomesim-
differ from the fixed targets typically used in supervised plifying assumptions on the value distributions over the
learning. We employ stochastic gradient descent for the actions,onecanderivemorespecializedguarantees. Forex-
training. WhileStochDQN,likeDQN,employsthesame ample,assumingthattherewardsareuniformlydistributed
valuesforactionselectionandevaluation,StochDDQN,like overtheactions,wedemonstrateinAppendixB.3thatfora
DDQN, trains two separate value functions. It does this givendiscretestates,ifthevaluesofthesampledactions
by randomly assigning each experience to update one of independentlyfollowauniformdistributionfromtheinter-
thetwovaluefunctions,resultingintwosetsofweights,θ val [Q (s,a⋆)−b (s),Q (s,a⋆)], where b (s) represents
t t t t t t
andθ′. Foreachupdate,onesetofweightsdeterminesthe therangeoftheQ (s,.)valuesovertheactionsinstates
t
policy,whiletheothersetdeterminesthevalues. attimestept,thentheexpectedvalueofβ (s),evenwith-
t
out memory, is: E[β (s)|s] ≤ bt(s) . Furthermore,
t ⌈log(n)⌉+1
5.StochasticMaximizationAnalysis weempiricallydemonstratethatfortheconsideredcontrol
problems, the difference β (s) is not large, and the ratio
t
Inthefollowing,westudystochasticmaximizationwithand ω (s)isclosetoone,asshowninSection7.4.
t
withoutmemorycomparedtoexactmaximization.
5.2.StochasticMaximizationwithMemory
5.1.MemorylessStochasticMaximization
Whilememorylessstochasticmaximizationcouldapproach
Memorylessstochasticmaximization,i.e.,C =R∪∅,does themaximumvalueorfinditwiththeprobabilityp,lower-
notalwaysyieldanoptimalmaximizer.Toreturnanoptimal bounded in Lemma 5.1, it does not converge to an exact
action,thisactionneedstoberandomlysampledfromthe maximization,asitkeepssamplingpurelyatrandom,ascan
set of actions. Finding an exact maximizer, without rely- be seen in Fig. 6 in Appendix E.2.1. However, memory-
ingonmemoryM,isarandomeventwithaprobabilityp, basedstochasticmaximization,i.e.,C =R∪MwithM≠
representingthelikelihoodofsamplingsuchanexactmaxi- ∅,canbecomeanexactmaximizationwhentheQ-function
mizer. Inthefollowinglemma,weprovidealowerbound becomesstable,aswestateintheCorollary5.3,whichwe
ontheprobabilityofdiscoveringanoptimalactionwithin proveinAppendixB.2.1,andasconfirmedinFig. 6.
auniformlyrandomlysampledsubsetC = Rof⌈log(n)⌉
Definition5.2. AQ-functionisconsideredstableforagiven
actions,whichweproveinAppendixB.1.1.
timerangeandstateswhenitsmaximizingactioninthat
Lemma5.1. Foranygivenstates,theprobabilitypofsam- stateremainsunchangedforallsubsequentstepswithinthat
plinganoptimalactionfromauniformlyrandomlychosen time,eveniftheQ-function’svaluesthemselveschange.
subsetC ofsize⌈log(n)⌉actionsisatleast ⌈log(n)⌉.
n
A straightforward example of a stable Q-function occurs
Whilefindinganexactmaximizerthroughsamplingmaynot during validation periods when no function updates are
alwaysoccur,therewardsofnear-optimalactionscanstillbe performed. However,ingeneral,astableQ-functiondoes
6StochasticQ-learningforLargeDiscreteActionSpaces
nothavetobestaticandmightstillvaryovertherounds; Thefollowingtheoremstatestheconvergenceoftheiterates
the critical characteristic is that its maximizing action re- Q of Stochastic Q-learning with memoryless stochastic
t
mainsthesameevenwhenitsvaluesareupdated. Although maximizationtotheQ∗,definedinEq.11,foranysampling
thestochmaxhassub-linearcomplexitycomparedtothe distributionP,regardlessofthecardinality.
max,withoutanyassumptionofthevaluedistributions,the Theorem 6.2. For a finite MDP, as described in Section
followingcorollaryshowsthat,onaverage,forastableQ- 3, let C be a randomly independently sampled subset of
function,afteracertainnumberofiterations,theoutputof actionsfromA,ofanycardinality,followinganydistribu-
thestochmaxmatchespreciselytheoutputofmax. tionP, exclusivelysampledforthevalueupdates, forthe
Corollary5.3. Foragivenstates,assumingatimerange StochasticQ-learning,asdescribedinAlgorithm1,given
wheretheQ-functionbecomesstableinthatstate,β (s)is bythefollowingupdaterule:
t
expectedtoconvergetozeroafter n iterations.
⌈log(n)⌉ Q (s ,a )=(1−α (s ,a ))Q (s ,a )
t+1 t t t t t t t t
(cid:20) (cid:21)
Recallingthedefinitionofthesimilarityratioω ,itfollows
t +α (s ,a ) r +γ max Q (s ,a) ,
thatω t(s)=1−β(s)/max a∈AQ t(s,a). Therefore,fora t t t t b∈C∼P t t+1
givenstates,wheretheQ-functionbecomesstable,given
givenanyinitialestimateQ ,Q convergeswithprobability
0 t
the boundedness of iterates in Q-learning, it is expected 1toQ∗,definedinEq. (11),aslongas(cid:80) α (s,a) = ∞
thatω tconvergestoone. Thisobservationwasconfirmed, and(cid:80) α2(s,a)<∞forall(s,a)∈S×At .t
evenwithcontinuousstatesandusingneuralnetworksas t t
functionapproximators,inSection7.4. Thetheorem’sresultdemonstratesthatforanycardinalityof
actions,StochasticQ-learningconvergestoQ∗,asdefined
6.StochasticQ-learningConvergence inEq. (11),whichrecoverstheconvergenceguaranteesof
Q-learningwhenthesamplingdistributionisP(A)=1.
Inthissection,weanalyzetheconvergenceoftheStochastic Remark6.3. Inprinciple,anysizecanbeused,balancing
Q-learning,describedinAlgorithm1. Thisalgorithmem- timecomplexityandapproximation. Ourempiricalexperi-
ploysthepolicyπ QS(s),asdefinedinEq.(6),withε s >0to mentsfocusedonlog(n)toillustratethemethod’sa √bilityto
guaranteethatP π[a t =a|s t =s]>0forallstate-action recoverQ-learning,evenwithafewactions. Using nwill
pairs(s,a) ∈ S×A. Thevalueupdaterule,ontheother approachthevaluefunctionofQ-learningmorecloselycom-
hand,usestheupdaterulespecifiedinEq. (7). paredtousinglog(n),albeitatthecostofhighercomplexity
thanlog(n).
Intheconvergenceanalysis,wefocusonmemorylessmax-
imization. While the stochargmax operator for action
Thetheoremshowsthatevenwithmemorylessstochastic
selectioncanbeemployedwithorwithoutmemory,weas- maximization,usingrandomlysampledO(log(n))actions,
sumeamemorylessstochmaxoperatorforvalueupdates,
the convergence is still guaranteed. However, relying on
which means that value updates are performed by maxi-
memory-basedstochasticmaximizationhelpsminimizethe
mizingoverarandomlysampledsubsetofactionsfromA,
approximationerrorinstochasticmaximization,asshown
sampledindependentlyfromboththenextstates′andthe
inCorollary5.3,andoutperformsQ-learningasshownin
setusedforthestochargmax.
theexperimentsinSection7.1.
ForastochasticvariablesubsetofactionsC ⊆A,following
Inthefollowing,weprovideasketchoftheproofaddressing
someprobabilitydistributionP:2A →[0,1],weconsider,
theextrastochasticityduetostochasticmaximization. The
withoutlossofgeneralityQ(.,∅)=0,anddefine,according
fullproofisprovidedinAppendixA.
toP,atargetQ-function,denotedasQ∗,as:
Wetackletheadditionalstochasticitydependingonthesam-
(cid:20) (cid:21)
Q∗(s,a)≜E r(s,a)+γ max Q∗(s′,b)|s,a . (11) pling distribution P, by defining an operator function Φ,
b∈C∼P whichforanyq :S×A→R,isasfollows:
Remark 6.1. The Q∗ defined above depends on the sam- (Φq)(s,a)≜ (cid:88) P(C)(cid:88) P(s′ |s,a)
pling distribution P. Therefore, it does not represent the
C∈2A s′∈S
optimal value function of the original MDP problem; in- (cid:20) (cid:21)
stead,itisoptimalundertheconditionwhereonlyarandom r(s,a)+γmaxq(s′,b) . (12)
subsetofactionsfollowingthedistributionPisavailable b∈C
to the agent at each time step. However, as the sampling Wethendemonstratethatitisacontractioninthesup-norm,
cardinalityincreases,itincreasinglybetterapproximatesthe asshowninLemma6.4,whichweproveinAppendixA.2.
optimalvaluefunctionoftheoriginalMDPandfullyrecov- Lemma 6.4. The operator Φ, defined in Eq. (12), is a
erstheoptimalQ-functionoftheoriginalproblemwhenthe contraction in the sup-norm, with a contraction factor γ,
samplingdistributionbecomesP(A)=1. i.e.,∥Φq −Φq ∥ ≤γ∥q −q ∥ .
1 2 ∞ 1 2 ∞
7StochasticQ-learningforLargeDiscreteActionSpaces
We then use the above lemma to establish the conver-
Stochastic Q-learning
genceofStochasticQ-learning. Givenanyinitialestimate 2000 Stochastic Double Q-learning
Stochastic Sarsa
Q , using the considered update rule for Stochastic Q- Q-learning
0 1500 Double Q-learning
learning,subtractingfrombothsidesQ∗(s ,a )andletting Sarsa
t t Random
∆ (s,a)≜Q (s,a)−Q∗(s,a),yields 1000
t t
∆ (s ,a )=(1−α (s ,a ))∆ (s ,a ) 500
t+1 t t t t t t t t
+α (s ,a )F (s ,a ), with
t t t t t t 0
0 100000 200000 300000 400000 500000
F (s,a)≜r(s,a)+γmaxQ (s′,b)−Q∗(s,a). (13) Steps
t t
b∈C
WithF representingthepastattimet, Figure 1: Comparison of stochastic vs. non-stochastic value-
t
basedvariantsontheFrozenLake-v1,withstepsonthex-axisand
(cid:88) (cid:88)
E[F (s,a)|F ]= P(C) P(s′ |s,a)[F (s,a)] cumulativerewardsonthey-axis.
t t t
C∈2A s′∈S
=(Φ(Q t))(s,a)−Q∗(s,a). 7.2.ExponentialWallTimeSpeedup
UsingthefactthatQ∗ =ΦQ∗andLemma6.4,
Stochastic maximization
∥E[F (s,a)|F ]∥ ≤γ∥Q −Q∗∥ =γ∥∆ ∥ . methods exhibit logarith- 0.6 S St to oc ch hD DQ DQN N
t t ∞ t ∞ t ∞ miccomplexityregarding 0.5 DQN
(14) DDQN
0.4
the number of actions. Random
Giventhatr isbounded,itsvarianceisboundedbysome
0.3
Therefore, StochDQN
constant B. Thus, as shown in Appendix A.1, for C = 0.2
max{B + γ2∥Q∗∥2 ∞,γ2}, var[F t(s,a)|F t] ≤ C(1 + and StochDDQN, which 0.1
∥∆ t∥ ∞)2. Then, by this inequality, Eq. (14), and Theo- applythesetechniquesfor 0.0
action selection and up- 22 24 26 28 210 212
rem1in(Jaakkolaetal.,1993),∆ tconvergestozerowith Number of Actions
probability1,i.e.,Q tconvergestoQ∗withprobability1. dates,haveexponentially Figure 2: Comparison of wall
faster execution times
timeinsecondsofstochasticand
thanDQNandDDQN,as non-stochasticDQNmethodson
7.Experiments
confirmedinFig.2. variousactionsetsizes.
We compare stochastic maximization to exact maximiza- Forthetimedurationofactionselectionalone,pleaserefer
tion and evaluate the proposed RL algorithms in Gymna- toAppendixE.1.Thetimeanalysisresultsshowthatthepro-
sium(Brockmanetal.,2016)andMuJoCo(Todorovetal., posedmethodsarenearlyasfastasarandomalgorithmthat
2012)environments. ThestochastictabularQ-learningap- selectsactionsrandomly. Specifically,intheexperiments
proachesaretestedonCliffWalking-v0,FrozenLake-v1,and withtheInvertedPendulum-v4,thestochasticmethodstook
ageneratedMDPenvironment. Additionally,thestochas- around 0.003 seconds per step for a set of 1000 actions,
ticdeepQ-networkapproachesaretestedoncontroltasks whilethenon-stochasticmethodstook0.18seconds,which
andcomparedagainsttheirdeterministiccounterparts, as indicates that the stochastic versions are 60 times faster
wellasagainstDDPG(Lillicrapetal.,2015),A2C(Mnih thantheirdeterministiccounterparts. Furthermore,forthe
etal.,2016),andPPO(Schulmanetal.,2017),usingStable- HalfCheetah-v4experiment, weconsidered4096actions,
Baselines implementations (Hill et al., 2018), which can whereone(D)DQNsteptakes0.6seconds,needingaround
directly handle continuous action spaces. Further details 17hourstorunfor100,000steps,whiletheStoch(D)DQN
canbefoundinAppendixD. needsaround2hourstofinishthesame100,000steps. In
other words, we can easily run for 10x more steps in the
7.1.StochasticQ-learningAverageReturn sameperiod(seconds). Thismakesthestochasticmethods
morepractical,especiallywithlargeactionspaces.
We test Stochastic Q-learning, Stochastic Double Q-
learning,andStochasticSarsainenvironmentswithdiscrete
7.3.StochasticDeepQ-networkAverageReturn
states and actions. Interestingly, as shown in Fig. 1, our
stochasticalgorithmsoutperformtheirdeterministiccounter- Fig. 3 shows the performance of various RL algorithms
parts. Furthermore,weobservethatStochasticQ-learning on the InvertedPendulum-v4 task, which has 512 actions.
outperformsallthemethodsconsideredregardingthecu- StochDQN achieves the optimal average return in fewer
mulativerewardsintheFrozenLake-v1. Moreover,inthe stepsthanDQN,withalowerper-steptimeadvantage(as
CliffWalking-v0(asshowninFig. 10),aswellasforthe shown in Section 7.2). Interestingly, while DDQN strug-
generated MDP environment with 256 actions (as shown gles,StochDDQNnearlyreachestheoptimalaveragereturn,
inFig. 12),allthestochasticandnon-stochasticmethods demonstratingtheeffectivenessofstochasticity. StochDQN
reachtheoptimalpolicyinasimilarnumberofsteps. and StochDDQN significantly outperform DDQN, A2C,
8
sdraweR
evitalumuC
)s(
noitaruD
petS
emiTStochasticQ-learningforLargeDiscreteActionSpaces
100 situationswithgenerallargediscreteactionspaces.
80 StochDQN WefocusmainlyonQ-learning-likemethodsamongvalue-
StochDDQN
DQN basedapproachesduetotheiroff-policynatureandproven
60
DDQN
successinvariousapplications. Wedemonstratethatthese
PPO
40 A2C methodscanbeappliedtolargediscreteactionspaceswhile
DDPG
20 Random achievingexponentiallylowercomplexityandmaintaining
goodperformance. Furthermore,ourproposedstochastic
0
0 5000 10000 15000 20000 25000 30000 maximization method performs well even when applied
Steps
to the on-policy Sarsa algorithm, extending its potential
Figure3: ComparisonofstochasticDQNvariantsagainstother beyond off-policymethods. Consequently, thesuggested
RLalgorithmsontheInvertedPendulum-v4,with1000actions, stochastic approach offers broader applicability to other
withstepsonthex-axisandaveragereturnsonthey-axis. value-basedapproaches,resultinginlowercomplexityand
improvedefficiencywithlargediscreteactionspaces.
andPPObyobtaininghigheraveragereturnsinfewersteps.
Similarly,Fig. 9binAppendixE.3showstheresultsforthe Whiletheprimarygoalofthisworkistoreducethecom-
HalfCheetah-v4task,whichhas4096actions. Stochastic plexityandwalltimeofQ-learning-likealgorithms,ourex-
methods, particularly StochDDQN, achieve results com- perimentsrevealedthatstochasticmethodsnotonlyachieve
parabletothenon-stochasticmethods. Notably,allDQN shortersteptimes(inseconds)butalso,insomecases,yield
methods(stochasticandnon-stochastic)outperformPPO higherrewardsandexhibitfasterconvergenceintermsof
andA2C,highlightingtheirefficiencyinsuchscenarios. thenumberofstepscomparedtoothermethods. Theseim-
provementscanbeattributedtoseveralfactors. Firstly,in-
Remark7.1. Whilecomparingthemfallsoutsidethescope
troducingmorestochasticityintothegreedychoicethrough
ofourwork,wenotethatDDQNwasproposedtomitigate
stochargmaxenhancesexploration. Secondly,Stochastic
theinherentoverestimationinDQN.However,exchanging
Q-learning specifically helps to reduce the inherent over-
overestimationforunderestimationbiasisnotalwaysben-
estimationinQ-learning-likemethods(Hasselt,2010;Lan
eficial, as our results demonstrate and as shown in other
etal.,2020;Wangetal.,2021). Thisreductionisachieved
studiessuchas(Lanetal.,2020).
usingstochmax,alowerboundtothemaxoperation.
7.4.StochasticMaximization Q-learningmethods, focusedinitiallyondiscreteactions,
can be adapted to tackle continuous problems with dis-
This section analyzes cretization techniques and stochastic maximization. Our
stochastic maximization 1.00 control experiments show that Q-network methods with
by tracking returned 0.99 discretizationachievesuperiorperformancetoalgorithms
values of ω t (Eq. (10)) 0.98 withcontinuousactions,suchasPPO,byobtaininghigher
across the steps. As 0.97 rewardsinfewersteps,whichalignswithobservationsin
shown in Fig. 4, for 0.96 previous works that highlight the potential of discretiza-
StochDQN, for the 0.95 tionforsolvingcontinuouscontrolproblems(Dulac-Arnold
InvertedPendulum-v4, etal.,2015;Tavakolietal.,2018;Tang&Agrawal,2020).
101 102 103 104
ω t approaches one Steps Notably,thelogarithmiccomplexityoftheproposedstochas-
rapidly, similarly for Figure 4: The stochmax and ticmethodsconcerningthenumberofconsideredactions
the HalfCheetah-v4, as maxratiovaluestrackedoverthe makes them well-suited for scenarios with finer-grained
showninAppendixE.2.2. stepsontheInvertedPendulum-v4. discretization,leadingtomorepracticalimplementations.
Furthermore,wetrackthereturnedvaluesofthedifference
β (Eq. (9))andshowthatitisboundedbysmallvaluesin
t 9.Conclusion
bothenvironments,asillustratedinAppendixE.2.2.
WeproposeadaptingQ-learning-likemethodstomitigate
8.Discussion thecomputationalbottleneckassociatedwiththemaxand
argmax operations in these methods. By reducing the
In this work, we focus on adapting value-based methods, maximization complexity from linear to sublinear using
whichexcelingeneralizationcomparedtoactor-basedap- stochmax and stochargmax, we pave the way for prac-
proaches(Dulac-Arnoldetal.,2015). However,thisadvan- ticalandefficientvalue-basedRLforlargediscreteaction
tagecomesatthecostoflowercomputationalefficiencydue spaces. WeprovetheconvergenceofStochasticQ-learning,
tothemaximizationoperationrequiredforactionselection analyzestochasticmaximization,andempiricallyshowthat
andvaluefunctionupdates. Therefore,ourprimarymotiva- itperformswellwithsignificantlylowcomplexity.
tionistoprovideacomputationallyefficientalternativefor
9
nruteR
egarevA
seulaVStochasticQ-learningforLargeDiscreteActionSpaces
ImpactStatement Fourati, F. and Alouini, M.-S. Artificial intelligence for
satellitecommunication: Areview. IntelligentandCon-
Thispaperpresentsworkwhosegoalistoadvancethefield
vergedNetworks,2(3):213–243,2021.
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be Fourati, F., Aggarwal, V., Quinn, C., and Alouini, M.-S.
specificallyhighlightedhere. Randomizedgreedylearningfornon-monotonestochas-
ticsubmodularmaximizationunderfull-banditfeedback.
References InInternationalConferenceonArtificialIntelligenceand
Statistics,pp.7455–7471.PMLR,2023.
Akkerman,F.,Luy,J.,vanHeeswijk,W.,andSchiffer,M.
Handlinglargediscreteactionspacesviadynamicneigh- Fourati, F., Alouini, M.-S., and Aggarwal, V. Federated
borhoodconstruction. arXivpreprintarXiv:2305.19891, combinatorial multi-agent multi-armed bandits. arXiv
2023. preprintarXiv:2405.05950,2024a.
Al-Abbasi,A.O.,Ghosh,A.,andAggarwal,V. Deeppool:
Fourati, F., Quinn, C. J., Alouini, M.-S., and Aggarwal,
Distributedmodel-freealgorithmforride-sharingusing
V. Combinatorialstochastic-greedybandit. InProceed-
deepreinforcementlearning. IEEETransactionsonIntel-
ings of the AAAI Conference on Artificial Intelligence,
ligentTransportationSystems,20(12):4714–4727,2019.
volume38,pp.12052–12060,2024b.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Gonzalez, G., Balakuntala, M., Agarwal, M., Low, T.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
Knoth, B., Kirkpatrick, A. W., McKee, J., Hager, G.,
arXivpreprintarXiv:1606.01540,2016.
Aggarwal,V.,Xue,Y.,etal. Asap: Asemi-autonomous
Cui,H.andKhardon,R. Onlinesymbolicgradient-based precisesystemfortelesurgeryduringcommunicationde-
optimization for factored action mdps. In IJCAI, pp. lays. IEEETransactionsonMedicalRoboticsandBion-
3075–3081,2016. ics,5(1):66–78,2023.
Delarue,A.,Anderson,R.,andTjandraatmadja,C. Rein-
Haliem,M.,Mani,G.,Aggarwal,V.,andBhargava,B. A
forcementlearningwithcombinatorialactions: Anappli-
distributed model-free ride-sharing approach for joint
cationtovehiclerouting.AdvancesinNeuralInformation
matching,pricing,anddispatchingusingdeepreinforce-
ProcessingSystems,33:609–620,2020.
mentlearning. IEEETransactionsonIntelligentTrans-
Dulac-Arnold, G., Denoyer, L., Preux, P., and Gallinari, portationSystems,22(12):7931–7942,2021.
P. Fast reinforcement learning with large action sets
Hasselt,H. Doubleq-learning. Advancesinneuralinforma-
using error-correcting output codes for mdp factoriza-
tionprocessingsystems,23,2010.
tion. InJointEuropeanConferenceonMachineLearning
and Knowledge Discovery in Databases, pp. 180–194.
He,J.,Chen,J.,He,X.,Gao,J.,Li,L.,Deng,L.,andOsten-
Springer,2012.
dorf,M. Deepreinforcementlearningwithanunbounded
Dulac-Arnold,G.,Evans,R.,vanHasselt,H.,Sunehag,P., actionspace. arXivpreprintarXiv:1511.04636,5,2015.
Lillicrap,T.,Hunt,J.,Mann,T.,Weber,T.,Degris,T.,and
Coppin,B. Deepreinforcementlearninginlargediscrete He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and
actionspaces. arXivpreprintarXiv:1512.07679,2015. Ostendorf,M. Deepreinforcementlearningwithanat-
urallanguageactionspace. InProceedingsofthe54th
Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Annual Meeting of the Association for Computational
Paduraru, C., Gowal, S., and Hester, T. Challenges of Linguistics (Volume 1: Long Papers), pp. 1621–1630,
real-world reinforcement learning: definitions, bench-
Berlin,Germany,August2016a.AssociationforCompu-
marks and analysis. Machine Learning, 110(9):2419–
tationalLinguistics. doi: 10.18653/v1/P16-1153. URL
2468,2021. https://aclanthology.org/P16-1153.
Enders,T.,Harrison,J.,Pavone,M.,andSchiffer,M.Hybrid
He,J.,Ostendorf,M.,He,X.,Chen,J.,Gao,J.,Li,L.,and
multi-agentdeepreinforcementlearningforautonomous
Deng,L. Deepreinforcementlearningwithacombinato-
mobilityondemandsystems. InLearningforDynamics
rialactionspaceforpredictingpopularRedditthreads. In
andControlConference,pp.1284–1296.PMLR,2023.
Proceedingsofthe2016ConferenceonEmpiricalMeth-
Farquhar,G.,Gustafson,L.,Lin,Z.,Whiteson,S.,Usunier, ods in Natural Language Processing, pp. 1838–1848,
N.,andSynnaeve,G. Growingactionspaces. InInterna- Austin,Texas,November2016b.AssociationforCompu-
tionalConferenceonMachineLearning,pp.3040–3051. tationalLinguistics. doi: 10.18653/v1/D16-1189. URL
PMLR,2020. https://aclanthology.org/D16-1189.
10StochasticQ-learningforLargeDiscreteActionSpaces
Hill,A.,Raffin,A.,Ernestus,M.,Gleave,A.,Kanervisto,A., Mazyavkina, N., Sviridov, S., Ivanov, S., and Burnaev,
Traore,R.,Dhariwal,P.,Hesse,C.,Klimov,O.,Nichol, E. Reinforcementlearningforcombinatorialoptimiza-
A., Plappert, M., Radford, A., Schulman, J., Sidor, S., tion: Asurvey. Computers&OperationsResearch,134:
andWu,Y.Stablebaselines.https://github.com/ 105400,2021.
hill-a/stable-baselines,2018.
Metz, L., Ibarz, J., Jaitly, N., and Davidson, J. Discrete
sequential prediction of continuous actions for deep rl.
Ireland,D.andMontana,G. Revalued: Regularisedensem-
arXivpreprintarXiv:1705.05035,2017.
blevalue-decompositionforfactorisablemarkovdecision
processes. arXivpreprintarXiv:2401.08850,2024. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou,I.,Wierstra,D.,andRiedmiller,M. Playing
Jaakkola, T., Jordan, M., and Singh, S. Convergence of
atari with deep reinforcement learning. arXiv preprint
stochastic iterative dynamic programming algorithms.
arXiv:1312.5602,2013.
Advances in neural information processing systems, 6,
1993. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, chronousmethodsfordeepreinforcementlearning. In
A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Internationalconferenceonmachinelearning,pp.1928–
Vanhoucke,V.,etal.Qt-opt:Scalabledeepreinforcement 1937.PMLR,2016.
learning for vision-based robotic manipulation. arXiv
Nair,V.andHinton,G.E. Rectifiedlinearunitsimprove
preprintarXiv:1806.10293,2018.
restrictedboltzmannmachines.InProceedingsofthe27th
internationalconferenceonmachinelearning(ICML-10),
Kim, M., Park, J., et al. Learning collaborative policies
pp.807–814,2010.
tosolvenp-hardroutingproblems. AdvancesinNeural
InformationProcessingSystems,34:10418–10430,2021. Pazis,J.andParr,R. Generalizedvaluefunctionsforlarge
action sets. In Proceedings of the 28th International
Lagoudakis,M.G.andParr,R. Reinforcementlearningas ConferenceonMachineLearning(ICML-11),pp.1185–
classification: Leveragingmodernclassifiers. InProceed- 1192,2011.
ings of the 20th International Conference on Machine
Learning(ICML-03),pp.424–431,2003. Peng, B., Rashid, T., Schroeder de Witt, C., Kamienny,
P.-A., Torr, P., Bo¨hmer, W., andWhiteson, S. Facmac:
Lan, Q., Pan, Y., Fyshe, A., and White, M. Maxmin q- Factored multi-agent centralised policy gradients. Ad-
learning: Controllingtheestimationbiasofq-learning. vances in Neural Information Processing Systems, 34:
arXivpreprintarXiv:2002.06487,2020. 12208–12221,2021.
Quillen,D.,Jang,E.,Nachum,O.,Finn,C.,Ibarz,J.,and
Li,S.,Wei,C.,andWang,Y. Combiningdecisionmaking
Levine,S. Deepreinforcementlearningforvision-based
and trajectory planning for lane changing using deep
roboticgrasping: Asimulatedcomparativeevaluationof
reinforcementlearning. IEEETransactionsonIntelligent
off-policymethods. In2018IEEEInternationalConfer-
TransportationSystems,23(9):16110–16136,2022.
enceonRoboticsandAutomation(ICRA),pp.6284–6291.
IEEE,2018.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous Rubinstein,R. Thecross-entropymethodforcombinatorial
controlwithdeepreinforcementlearning. arXivpreprint andcontinuousoptimization. Methodologyandcomput-
arXiv:1509.02971,2015. inginappliedprobability,1:127–190,1999.
Luong,N.C.,Hoang,D.T.,Gong,S.,Niyato,D.,Wang,P., Rummery, G. A. and Niranjan, M. On-line Q-learning
Liang,Y.-C.,andKim,D.I. Applicationsofdeeprein- usingconnectionistsystems, volume37. Universityof
forcementlearningincommunicationsandnetworking: Cambridge,DepartmentofEngineeringCambridge,UK,
Asurvey. IEEECommunicationsSurveys&Tutorials,21 1994.
(4):3133–3174,2019.
Sallans,B.andHinton,G.E. Reinforcementlearningwith
factored states and actions. The Journal of Machine
Mahajan,A.,Samvelyan,M.,Mao,L.,Makoviychuk,V.,
LearningResearch,5:1063–1088,2004.
Garg,A.,Kossaifi,J.,Whiteson,S.,Zhu,Y.,andAnand-
kumar, A. Reinforcement learning in factored action Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
spaces using tensor decompositions. arXiv preprint Klimov, O. Proximal policy optimization algorithms.
arXiv:2110.14538,2021. arXivpreprintarXiv:1707.06347,2017.
11StochasticQ-learningforLargeDiscreteActionSpaces
Seyde, T., Gilitschenski, I., Schwarting, W., Stellato, B., Wang, H., Lin, S., and Zhang, J. Adaptive ensemble q-
Riedmiller, M., Wulfmeier, M., and Rus, D. Is bang- learning: Minimizingestimationbiasviaerrorfeedback.
bang control all you need? solving continuous control AdvancesinNeuralInformationProcessingSystems,34:
withbernoullipolicies. AdvancesinNeuralInformation 24778–24790,2021.
ProcessingSystems,34:27209–27221,2021.
Wang, X., Wang, S., Liang, X., Zhao, D., Huang, J., Xu,
Seyde, T., Werner, P., Schwarting, W., Gilitschenski, I., X.,Dai,B.,andMiao,Q. Deepreinforcementlearning:
Riedmiller, M., Rus, D., and Wulfmeier, M. Solv- a survey. IEEE Transactions on Neural Networks and
ing continuous control via q-learning. arXiv preprint LearningSystems,2022.
arXiv:2210.12566,2022.
Watkins,C.J.andDayan,P. Q-learning. Machinelearning,
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An 8:279–292,1992.
introduction. MITpress,2018.
Zahavy,T.,Haroush,M.,Merlis,N.,Mankowitz,D.J.,and
Tang, Y. andAgrawal, S. Discretizingcontinuous action Mannor,S. Learnwhatnottolearn: Actionelimination
spaceforon-policyoptimization. InProceedingsofthe with deep reinforcement learning. Advances in neural
aaaiconferenceonartificialintelligence,volume34,pp. informationprocessingsystems,31,2018.
5981–5988,2020.
Zhang, T., Guo, S., Tan, T., Hu, X., and Chen, F. Gen-
Tavakoli,A.,Pardo,F.,andKormushev,P. Actionbranch- erating adjacency-constrained subgoals in hierarchical
ingarchitecturesfordeepreinforcementlearning. InPro- reinforcementlearning. AdvancesinNeuralInformation
ceedingsoftheAAAIconferenceonArtificialIntelligence, ProcessingSystems,33:21579–21590,2020.
volume32,2018.
Zhang, Z., Pan, Z., and Kochenderfer, M. J. Weighted
Tennenholtz,G.andMannor,S. Thenaturallanguageofac- doubleq-learning. InIJCAI,pp.3455–3461,2017.
tions. InInternationalConferenceonMachineLearning,
pp.6196–6205.PMLR,2019.
Tessler,C.,Zahavy,T.,Cohen,D.,Mankowitz,D.J.,and
Mannor,S. Actionassembly: Sparseimitationlearning
for text based games with combinatorial action spaces.
arXivpreprintarXiv:1905.09700,2019.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
engineformodel-basedcontrol. In2012IEEE/RSJinter-
nationalconferenceonintelligentrobotsandsystems,pp.
5026–5033.IEEE,2012.
Van de Wiele, T., Warde-Farley, D., Mnih, A., and
Mnih, V. Q-learning in enormous action spaces via
amortized approximate maximization. arXiv preprint
arXiv:2001.08116,2020.
VanHasselt,H.andWiering,M.A.Usingcontinuousaction
spacestosolvediscreteproblems. In2009International
Joint Conference on Neural Networks, pp. 1149–1156.
IEEE,2009.
VanHasselt,H.,Guez,A.,andSilver,D. Deepreinforce-
mentlearningwithdoubleq-learning. InProceedingsof
theAAAIconferenceonartificialintelligence,volume30,
2016.
Wang,G.,Shi,D.,Xue,C.,Jiang,H.,andWang,Y. Bic-
ddpg: Bidirectionally-coordinated nets for deep multi-
agent reinforcement learning. In International Confer-
enceonCollaborativeComputing: Networking,Applica-
tionsandWorksharing,pp.337–354.Springer,2020.
12StochasticQ-learningforLargeDiscreteActionSpaces
A.StochasticQ-learningConvergenceProofs
In this section, we prove Theorem 6.2, which states the convergence of Stochastic Q-learning. This algorithm uses a
stochastic policy for action selection, employing a stochargmax with or without memory, possibly dependent on the
currentstates. Forvalueupdates,itutilizesastochmaxwithoutmemory,independentofthefollowingstates′.
A.1.ProofofTheorem6.2
Proof. StochasticQ-learningemploysastochasticpolicyinagivenstates,whichusestochargmaxoperation,withor
withoutmemoryM,withprobability(1−ε ),forε >0,whichcanbesummarizedbythefollowingequation:
s s
(cid:40)
playrandomly withprobabilityϵ
πS(s)= s (15)
Q stochargmax Q(s,a) otherwise.
a∈A
Thispolicy,withε >0,ensuresthatP [a =a|s =s]>0forall(s,a)∈S×A.
s π t t
Furthermore,duringthetraining,toupdatetheQ-function,givenanyinitialestimateQ ,weconsideraStochasticQ-learning
0
whichusesstochmaxoperationasinthefollowingstochasticupdaterule:
(cid:20) (cid:21)
Q (s ,a )=(1−α (s ,a ))Q (s ,a )+α (s ,a ) r +γstochmaxQ (s ,b) . (16)
t+1 t t t t t t t t t t t t t t+1
b∈A
Forthefunctionupdates,weconsiderastochmaxwithoutmemory,whichinvolvesamaxoverarandomsubsetofaction
C sampledfromasetprobabilitydistributionPdefinedoverthecombinatorialspaceofactions,i.e.,P:2A →[0,1],which
canbeauniformdistributionovertheactionsetsofsize⌈log(n)⌉.
Hence,forarandomsubsetofactionsC,theupdateruleofStochasticQ-learningcanbewrittenas:
(cid:20) (cid:21)
Q (s ,a )=(1−α (s ,a ))Q (s ,a )+α (s ,a ) r +γmaxQ (s ,b) . (17)
t+1 t t t t t t t t t t t t t t+1
b∈C
WedefineanoptimalQ-function,denotedasQ∗,asfollows:
(cid:20) (cid:21)
Q∗(s,a)=E r(s,a)+γ stochmaxQ∗(s′,b)|s,a (18)
b∈A
(cid:20) (cid:21)
=E r(s,a)+γmaxQ∗(s′,b)|s,a . (19)
b∈C
SubtractingfrombothsidesQ∗(s ,a )andletting
t t
∆ (s,a)=Q (s,a)−Q∗(s,a), (20)
t t
yields
∆ (s ,a )=(1−α (s ,a ))∆ (s ,a )+α (s ,a )F (s ,a ), (21)
t+1 t t t t t t t t t t t t t t
with
F (s,a)=r(s,a)+γmaxQ (s′,b)−Q∗(s,a). (22)
t t
b∈C
ForthetransitionprobabilitydistributionP : S×A×S → [0,1], thesetprobabilitydistributionP : 2A → [0,1], the
rewardfunctionr :S×A→R,andthediscountfactor,γ ∈[0,1],wedefinethefollowingcontractionoperatorΦ,defined
forafunctionq :S×A→Ras
(cid:20) (cid:21)
(cid:88) (cid:88)
(Φq)(s,a)= P(C) P(s′ |s,a) r(s,a)+γmaxq(s′,b) . (23)
b∈C
C∈2A s′∈S
Therefore,withF representingthepastattimestept,
t
(cid:20) (cid:21)
(cid:88) (cid:88)
E[F (s,a)|F ]= P(C) P(s′ |s,a) r(s,a)+γmaxQ (s′,b)−Q∗(s,a)
t t t
b∈C
C∈2A s′∈S
=(Φ(Q ))(s,a)−Q∗(s,a).
t
13StochasticQ-learningforLargeDiscreteActionSpaces
UsingthefactthatQ∗ =ΦQ∗,
E[F (s,a)|F ]=(ΦQ )(s,a)−(ΦQ∗)(s,a).
t t t
ItisnowimmediatefromLemma6.4,whichweproveinAppendixA.2,that
∥E[F (s,a)|F ]∥ ≤γ∥Q −Q∗∥ =γ∥∆ ∥ . (24)
t t ∞ t ∞ t ∞
Moreover,
(cid:34)(cid:18) (cid:19)2 (cid:35)
var[F (s,a)|F ]=E r(s,a)+γmaxQ (s′,b)−Q∗(s,a)−(ΦQ )(s,a)+Q∗(s,a) |F
t t t t t
b∈C
(cid:34)(cid:18) (cid:19)2 (cid:35)
=E r(s,a)+γmaxQ (s′,b)−(ΦQ )(s,a) |F
t t t
b∈C
(cid:20) (cid:21)
=var r(s,a)+γmaxQ (s′,b)|F
t t
b∈C
(cid:20) (cid:21)
=var[r(s,a)|F ]+γ2var maxQ (s′,b)|F +2γcov(r(s,a),maxQ (s′,b)|F )
t t t t t
b∈C b∈C
(cid:20) (cid:21)
=var[r(s,a)|F ]+γ2var maxQ (s′,b)|F .
t t t
b∈C
Thelastlinefollowsfromthefactthattherandomnessofmax Q (s′,b)|F onlydependsontherandomsetC andthe
b∈C t t
nextstates′. Moreover,weconsidertherewardr(s,a)independentofthesetC andthenextstates′,bynotusingthesame
setC forboththeactionselectionandthevalueupdate.
Giventhatrisbounded,itsvarianceisboundedbysomeconstantB. Therefore,
(cid:20) (cid:21)
var[F (s,a)|F ]≤B+γ2var maxQ (s′,b)|F
t t t t
b∈C
(cid:20) (cid:21) (cid:20) (cid:21)2
=B+γ2E (maxQ (s′,b))2 |F −γ2E maxQ (s′,b)|F
t t t t
b∈C b∈C
(cid:20) (cid:21)
≤B+γ2E (maxQ (s′,b))2 |F
t t
b∈C
(cid:20) (cid:21)
≤B+γ2E (maxmaxQ (s′,b))2 |F
t t
s′∈S b∈A
≤B+γ2(maxmaxQ (s′,b))2
t
s′∈S b∈A
=B+γ2∥Q ∥2
t ∞
=B+γ2∥∆ +Q∗∥2
t ∞
≤B+γ2∥Q∗∥2 +γ2∥∆ ∥2
∞ t ∞
≤(B+γ2∥Q∗∥2 )(1+∥∆ ∥2 )+γ2(1+∥∆ ∥2 )
∞ t ∞ t ∞
≤max{B+γ2∥Q∗∥2 ,γ2}(1+∥∆ ∥2 )
∞ t ∞
≤max{B+γ2∥Q∗∥2 ,γ2}(1+∥∆ ∥ )2.
∞ t ∞
Therefore,forconstantC =max{B+γ2∥Q∗∥2 ,γ2},
∞
var[F (s,a)|F ]≤C(1+∥∆ ∥ )2. (25)
t t t ∞
Then,byEq. (24),Eq. (25),andTheorem1in(Jaakkolaetal.,1993),∆ convergestozerowithprobability1,i.e.,Q
t t
convergestoQ∗withprobability1.
14StochasticQ-learningforLargeDiscreteActionSpaces
A.2.ProofofLemma6.4
Proof. ForthetransitionprobabilitydistributionP :S×A×S →[0,1],thesetprobabilitydistributionPdefinedoverthe
combinatorialspaceofactions,i.e.,P:2A →[0,1],therewardfunctionr :S×A→R,andthediscountfactorγ ∈[0,1],
forafunctionq :S×A→R,theoperatorΦisdefinedasfollows:
(cid:20) (cid:21)
(cid:88) (cid:88)
(Φq)(s,a)= P(C) P(s′ |s,a) r(s,a)+γmaxq(s′,b) . (26)
b∈C
C∈2A s′∈S
Therefore,
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:88) (cid:20) (cid:21)(cid:12)
∥Φq −Φq ∥ =max(cid:12) P(C) P(s′ |s,a) r(s,a)+γmaxq (s′,b)−r(s,a)+γmaxq (s′,b) (cid:12)
1 2 ∞ s,a (cid:12) (cid:12) b∈C 1 b∈C 2 (cid:12) (cid:12)
C∈2A s′∈S
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:88) (cid:20) (cid:21)(cid:12)
=maxγ(cid:12) P(C) P(s′ |s,a) maxq (s′,b)−maxq (s′,b) (cid:12)
(cid:12) 1 2 (cid:12)
s,a (cid:12) b∈C b∈C (cid:12)
C∈2A s′∈S
(cid:12) (cid:12)
≤maxγ (cid:88) P(C)(cid:88) P(s′ |s,a)(cid:12) (cid:12)maxq 1(s′,b)−maxq 2(s′,b)(cid:12) (cid:12)
s,a (cid:12)b∈C b∈C (cid:12)
C∈2A s′∈S
(cid:88) (cid:88)
≤maxγ P(C) P(s′ |s,a)max|q (z,b)−q (z,b)|
1 2
s,a z,b
C∈2A s′∈S
(cid:88) (cid:88)
≤maxγ P(C) P(s′ |s,a)∥q −q ∥
s,a 1 2 ∞
C∈2A s′∈S
=γ∥q −q ∥ .
1 2 ∞
15StochasticQ-learningforLargeDiscreteActionSpaces
B.StochasticMaximization
Weanalyzetheproposedstochasticmaximizationmethodbycomparingitserrortothatofexactmaximization. First,we
considerthecasewithoutmemory,whereC =R,andthenthecasewithmemory,whereM≠ ∅. Finally,weprovidea
specializedboundforthecasewheretheactionvaluesfollowauniformdistribution.
B.1.MemorylessStochasticMaximization
Inthefollowinglemma,wegivealowerboundontheprobabilityoffindinganoptimalactionwithinauniformlysampled
subsetRof⌈log(n)⌉actions. Weprovethatforagivenstates,theprobabilitypofsamplinganoptimalactionwithinthe
uniformlyrandomlysampledsubsetRofsize⌈log(n)⌉actionsislowerboundedwithp≥ ⌈log(n)⌉.
n
B.1.1.PROOFOFLEMMA5.1
Proof. In the presence of multiple maximizers, we focus on one of them, denoted as a∗, and then the probability p of
0
samplingatleastonemaximizerislower-boundedbytheprobabilityp a∗
0
offindinga∗ 0,i.e.,
p≥p .
a∗
0
Theprobabilityp
a∗ 0
offindinga∗
0
istheprobabilityofsamplinga∗
0
withintherandomsetRofsize⌈log(n)⌉,whichisthe
fractionofallpossiblecombinationsofsize⌈log(n)⌉thatincludea∗.
0
Thisfractioncanbecalculatedas(cid:0) n−1 (cid:1) dividedbyallpossiblecombinationsofsize⌈log(n)⌉,whichis(cid:0) n (cid:1) .
⌈log(n)⌉−1 ⌈log(n)⌉
( n−1 )
Therefore,p a∗
0
= ⌈ (log( nn)⌉− )1 .
⌈log(n)⌉
Consequently,
⌈log(n)⌉
p≥ . (27)
n
B.2.StochasticMaximizationwithMemory
While stochastic maximization without memory could approach the maximum value or find it with the probability p,
lower-boundedinLemma5.1,itneverconvergestoanexactmaximization,asitkeepssamplingpurelyatrandom,ascanbe
seeninFig. 6. However,stochasticmaximizationwithmemorycanbecomeanexactmaximizationwhentheQ-function
becomesstable,whichweproveinthefollowingCorollary. Althoughthestochmaxhassub-linearcomplexitycomparedto
themax,thefollowingCorollaryshowsthat,onaverage,forastableQ-function,afteracertainnumberofiterations,the
outputofthestochmaxmatchestheoutputofmax.
DefinitionB.1. AQ-functionisconsideredstableforagivenstatesifitsbestactioninthatstateremainsunchangedforall
subsequentsteps,eveniftheQ-function’svaluesthemselveschange.
AstraightforwardexampleofastableQ-functionoccursduringvalidationperiodswhennofunctionupdatesareperformed.
However,ingeneral,astableQ-functiondoesnothavetobestaticandmightstillvaryovertherounds;thekeycharacteristic
isthatitsmaximizingactionremainsthesameevenwhenitsvaluesareupdated. Althoughthestochmaxhassub-linear
complexitycomparedtothemax,withoutanyassumptionofthevaluedistributions,thefollowingCorollaryshowsthat,on
average,forastableQ-function,afteracertainnumberofiterations,theoutputofthestochmaxmatchesexactlytheoutput
ofmax.
B.2.1.PROOFOFCOROLLARY5.3
Proof. Weformalizetheproblemasageometricdistributionwherethesuccesseventistheeventofsamplingasubsetof
size⌈log(n)⌉thatincludesatleastonemaximizer. Thegeometricdistributiongivestheprobabilitythatthefirsttimeto
sampleasubsetthatincludesanoptimalactionrequireskindependentcalls,eachwithsuccessprobabilityp. FromLemma
5.1,wehavep≥ ⌈log(n)⌉. Therefore,onanaverage,successrequires: 1 ≤ n calls.
n p ⌈log(n)⌉
16StochasticQ-learningforLargeDiscreteActionSpaces
Foragivendiscretestates,Mkeepstrackofthemostrecentbestactionfound. ForC =R∪M,
stochmaxQ(s,a)=maxQ(s,a)≥ maxQ(s,a). (28)
a∈A a∈C a∈M
Therefore,foragivenstates,onaverage,iftheQ-functionisstable,thenwithin n ,Mwillcontaintheoptimalaction
⌈log(n)⌉
a∗. Therefore,onanaverage,after n timesteps,
⌈log(n)⌉
stochmaxQ(s,a)≥ maxQ(s,a)=maxQ(s,a).
a∈A a∈M a∈A
Weknowthat,stochmax Q(s,a)≤max Q(s,a).Therefore,forastableQ-function,onanaverage,after n
a∈A a∈A ⌈log(n)⌉
timesteps,stochmax Q(s,a)becomesmax Q(s,a).
a∈A a∈A
B.3.StochasticMaximizationwithUniformlyDistributedRewards
Whiletheabovecorollaryoutlinesanupper-boundontheaveragenumberofcallsneededtodeterminetheexactoptimal
actioneventually,thefollowinglemmaoffersinsightsintotheexpectedmaximumvalueofarandomlysampledsubsetof
actions,comprising⌈log(n)⌉elementswhentheirvaluesareuniformlydistributed.
LemmaB.2. ForagivenstatesandauniformlyrandomlysampledsubsetRofsize⌈log(n)⌉actions,ifthevaluesofthe
sampledactionsfollowindependentlyauniformdistributionintheinterval[Q (s,a⋆)−b (s),Q (s,a⋆)],thentheexpected
t t t t t
valueofthemaximumQ-functionwithinthisrandomsubsetis:
(cid:20) (cid:21)
b (s)
E maxQ (s,k)|s,a⋆ =Q (s,a⋆)− t . (29)
k∈R t t t t ⌈log(n)⌉+1
Proof. ForagivenstatesweassumeauniformlyrandomlysampledsubsetRofsize⌈log(n)⌉actions,andthevaluesofthe
sampledactionsareindependentandfollowauniformdistributionintheinterval[Q (s,a⋆)−b (s),Q (s,a⋆)]. Therefore,
t t t t t
thecumulativedistributionfunction(CDF)forthevalueofanactionagiventhestatesandtheoptimalactiona∗is:
t

0 fory <Q (s,a∗)−b (s)
 t t t
G(y;s,a)= y fory ∈[Q (s,a∗)−b ,Q (s,a∗)]
t t t t
 1 fory >Q (s,a∗).
t t
Wedefinethevariablex=(y−(Q (s,a∗)−b (s)))/b (s).
t t t t

0 forx<0

F(x;s,a)= x forx∈[0,1]
 1 forx>1.
Ifweselect⌈log(n)⌉suchactions,theCDFofthemaximumoftheseactions,denotedasF isthefollowing:
max
(cid:18) (cid:19)
F (x;s,a)=P maxQ (s,a)≤x
max t
a∈R
(cid:89)
= P(Q (s,a)≤x)
t
a∈R
(cid:89)
= F(x;s,a)
a∈R
=F(x;s,a)⌈log(n)⌉.
Thesecondlinefollowsfromtheindependenceofthevalues,andthelastlinefollowsfromtheassumptionthatallactions
followthesameuniformdistribution.
TheCDFofthemaximumisthereforegivenby:

0 forx<0

F (x;s,a)= x⌈log(n)⌉ forx∈[0,1]
max
 1 forx>1.
17StochasticQ-learningforLargeDiscreteActionSpaces
Now,wecandeterminethedesiredexpectedvalueas
(cid:20) Q (s,a)−(Q (s,a∗)−b (s))(cid:21) (cid:90) ∞
E max t t t t = xdF (x;s,a)
a∈R b t(s) −∞ max
(cid:90) 1
= xdF (x;s,a)
max
0
(cid:90) 1
=[xF (x;s,a)]1− F (x;s,a)dx
max 0 max
0
(cid:90) 1
=1− x⌈log(n)⌉dx
0
1
=1− .
⌈log(n)⌉+1
(cid:82)1 (cid:82)1
Weemployedtheidentity xdµ(x)= 1−µ(x)dx,whichcanbedemonstratedthroughintegrationbyparts. Toreturn
0 0
totheoriginalscale,wecanfirstmultiplybyb andthenaddQ (s,a∗)−b (s),resultingin:
t t t t
(cid:20) (cid:21)
b (s)
E maxQ (s,a)|s,a∗ =Q (s,a∗)− t .
a∈R t t t t ⌈log(n)⌉+1
Asanexampleofthissetting,forQ (s,a⋆)=100,b =100,forasettingwithn=1000actions,⌈log(n)⌉+1=11.Hence
t t t
theE[max Q (s,k)|s,a⋆]≈91. ThisshowsthatevenwitharandomlysampledsetofactionsR,thestochmaxcan
k∈R t t
beclosetothemax. WesimulatethissettingintheexperimentsinFig. 6.
OurproposedstochasticmaximizationdoesnotsolelyrelyontherandomlysampledsubsetofactionsRbutalsoconsiders
actionsfrompreviousexperiencesthroughM. Therefore,theexpectedstochmaxshouldbehigherthantheaboveresult,
providinganupperboundontheexpectedβ asdescribedinthefollowingcorollaryofLemmaB.2.
t
CorollaryB.3. Foragivendiscretestates,ifthevaluesofthesampledactionsfollowindependentlyauniformdistribution
fromtheinterval[Q (s,a⋆)−b (s),Q (s,a⋆)],thentheexpectedvalueofβ (s)is:
t t t t t t
b (s)
E[β (s)|s]≤ t . (30)
t ⌈log(n)⌉+1
Proof. Attimestept,givenastates,andthecurrentestimatedQ-functionQ ,β (s)isdefinedasfollows:
t t
β (s)=maxQ (s,a)−stochmaxQ (s,a). (31)
t t t
a∈A a∈A
ForagivenstatesandauniformlyrandomlysampledsubsetRofsize⌈log(n)⌉actionsandasubsetofsomeprevious
playedactionsM⊂E,usingthelawoftotalexpectation,
E[β (s)|s]=E[E[β (s)|s,a⋆]|s]
t t t
(cid:20) (cid:20) (cid:21) (cid:21)
=E E maxQ (s,k)−stochmaxQ (s,k)|s,a⋆ |s
t t t
k∈A k∈A
(cid:20) (cid:20) (cid:21) (cid:21)
=E E maxQ (s,k)− max Q (s,k)|s,a⋆ |s
t t t
k∈A k∈R∪M
(cid:20) (cid:20) (cid:21) (cid:21)
≤E E maxQ (s,k)−maxQ (s,k)|s,a⋆ |s
t t t
k∈A k∈R
(cid:20) (cid:20) (cid:21) (cid:21)
=E Q (s,a∗)−E maxQ (s,k)|s,a⋆ |s .
t t t t
k∈R
18StochasticQ-learningforLargeDiscreteActionSpaces
ThereforebyLemmaB.2:
(cid:20) (cid:21)
b (s)
E[β (s)|s]≤E Q (s,a∗)−(Q (s,a∗)− t )|s
t t t t t ⌈log(n)⌉+1
(cid:20) (cid:21)
b (s)
=E t |s
⌈log(n)⌉+1
b (s)
= t .
⌈log(n)⌉+1
19StochasticQ-learningforLargeDiscreteActionSpaces
C.Pseudocodes
Algorithm2StochasticDoubleQ-learning
InitializeQA(s,a)andQB(s,a)foralls∈S,a∈A,n=|A|
foreachepisodedo
Observestates.
foreachstepofepisodedo
ChooseafromsviaQA+QB withpolicyπS (s)inEq. (6).
(QA+QB)
Takeactiona,observer,s′.
ChooseeitherUPDATE(A)orUPDATE(B),forexamplerandomly.
ifUPDATE(A)then
∆A ←r+γQB(s′,stochargmax QA(s′,b))−QA(s,a).
b∈A
QA(s,a)←QA(s,a)+α(s,a)∆A.
elseifUPDATE(B)then
∆B ←r+γQA(s′,stochargmax QB(s′,b))−QB(s,a).
b∈A
QB(s,a)←QB(s,a)+α(s,a)∆B.
endif
s←s′.
endfor
endfor
Algorithm3StochasticSarsa
InitializeQ(s,a)foralls∈S,a∈A,n=|A|
foreachepisodedo
Observestates.
ChooseafromswithpolicyπS(s)inEq. (6).
Q
foreachstepofepisodedo
Takeactiona,observer,s′.
Choosea′froms′withpolicyπS(s′)inEq. (6).
Q
Q(s,a)←Q(s,a)+α(s,a)[r+γQ(s′,a′)−Q(s,a)].
s←s′;a←a′.
endfor
endfor
D.ExperimentalDetails
D.1.Environments
Wetestourproposedalgorithmsonastandardizedsetofenvironmentsusingopen-sourcelibraries. Wecomparestochastic
maximizationtoexactmaximizationandevaluatetheproposedstochasticRLalgorithmsonGymnasiumenvironments
(Brockmanetal.,2016). StochasticQ-learningandStochasticDoubleQ-learningaretestedontheCliffWalking-v0,the
FrozenLake-v1,andageneratedMDPenvironment,whilestochasticdeepQ-learningapproachesaretestedonMuJoCo
controltasks(Todorovetal.,2012).
D.1.1.ENVIRONMENTSWITHDISCRETESTATESANDACTIONS
WegenerateanMDPenvironmentwith256actions,withrewardsfollowinganormaldistributionofmean-50andstandard
deviation of 50, with 3 states. Furthermore, while our approach is designed for large discrete action spaces, we tested
it in Gymnasium environments (Brockman et al., 2016) with only four discrete actions, such as CliffWalking-v0 and
FrozenLake-v1. CliffWalking-v0involvesnavigatingagridworldfromthestartingpointtothedestinationwithoutfalling
offacliff. FrozenLake-v1requiresmovingfromthestartingpointtothegoalwithoutsteppingintoanyholesonthefrozen
surface,whichcanbechallengingduetotheslipperynatureoftheice.
20StochasticQ-learningforLargeDiscreteActionSpaces
Algorithm4StochasticDeepQ-Network(StochDQN)
Algorithmparameters: learningrateα∈(0,1],replaybufferE,updaterateτ.
Initialize: neuralnetworkQ(s,a;θ)withrandomweightsθ,targetnetworkQˆ(s,a;θ−)withθ− =θ,setofactionsAof
sizen.
foreachepisodedo
Initializestates.
whilenotterminalstateisreacheddo
ChooseafromsusingastochasticpolicyasdefinedinEq. (15)usingQ(s,.;θ).
Takeactiona,observerewardr(s,a)andnextstates′.
Store(s,a,r(s,a),s′)inreplaybufferE.
Computetargetvaluesforthemini-batch:
(cid:40)
r if s′ isterminal
y = i i
i r +γQˆ(s′,stochargmax Qˆ(s′,a′;θ−);θ−) otherwise.
i i a′∈A i
Performagradientdescentstepontheloss:
⌈log(n)⌉
1 (cid:88)
L(θ)= (y −Q(s ,a ;θ))2.
⌈log(n)⌉ i i i
i=1
Updatethetargetnetworkweights:
θ− ←τ ·θ+(1−τ)·θ−.
UpdatetheQ-networkweightsusinggradientdescent:
θ ←θ+α∇ L(θ).
θ
s←s′.
endwhile
endfor
21StochasticQ-learningforLargeDiscreteActionSpaces
D.1.2.ENVIRONMENTSWITHCONTINUOUSSTATES: DISCRETIZINGCONTROLTASKS
We test the stochastic deep Q-learning approaches on MuJoCo (Todorov et al., 2012) for continuous states discretized
controltasks. Wediscretizeeachactiondimensionintoiequallyspacedvalues,creatingadiscreteactionspacewithn=id
d-dimensionalactions. Wemainlyfocusedontheinvertedpendulumandthehalf-cheetah. Theinvertedpenduluminvolves
acartthatcanbemovedleftorright,intendingtobalanceapoleontopusinga1Dforce,withi=512resultingin512
actions. Thehalf-cheetahisarobotwithninebodypartsaimingtomaximizeforwardspeed. Itcanapplytorqueto6joints,
resultingin6Dactionswithi=4,whichresultsin4096actions.
D.2.Algorithms
D.2.1.STOCHASTICMAXIMIZATION
Wehavetwoscenarios,onefordiscreteandtheotherforcontinuousstates. Fordiscretestates,E isadictionarywiththekeys
asthestatesinS withcorrespondingvaluesofthelatestplayedactionineverystate. Incontrast,E comprisestheactionsin
thereplaybufferforcontinuousstates. Indeed,wedonotconsiderthewholesetE either. Instead,weonlyconsiderasubset
M⊂E. Fordiscretestates,foragivenstates,Mincludesthelatesttwoexploitedactionsinstates. Forcontinuousstates,
whereitisimpossibletoretainthelastexploitedactionforeachstate,weconsiderrandomlysampledsubsetM⊂E,which
includes⌈log(n)⌉actions,eventhoughtheywereplayedindifferentstates. Intheexperimentsinvolvingcontinuousstates,
wedemonstratethatthiswassufficienttoachievegoodresults,seeSection7.3.
D.2.2.TABULARQ-LEARNINGMETHODS
WesetthetrainingparametersthesameforalltheQ-learningvariants. Wefollowsimilarhyper-parametersasin(Hasselt,
2010). Wesetthediscountfactorγ to0.95andapplyadynamicalpolynomiallearningrateαwithα (s,a)=1/z (s,a)0.8,
t t
wherez (s,a)isthenumberoftimesthepair(s,a)hasbeenvisited,initiallysettooneforallthepairs. Fortheexploration
t
(cid:112)
rate,weuseuseadecayingε,definedasε(s) = 1/ (z(s))wherez(s)isthenumberoftimesstateshasbeenvisited,
initiallysettooneforallthestates. ForDoubleQ-learningz (s,a)=zA(s,a)ifQAisupdatedandz (s,a)=zB(s,a)if
t t t t
QB isupdated,wherezA andzB storethenumberofupdatesforeachactionforthecorrespondingvaluefunction. We
t t
averagedtheresultsovertenrepetitions. ForStochasticQ-learning,wetrackadictionaryDwithkeysbeingthestates,and
valuesbeingthelatestexploitedaction. Thus,forastates,thememoryM=D(s),thusMisthelatestexploitedactionin
thesamestates.
D.2.3.DEEPQ-NETWORKMETHODS
WesetthetrainingparametersthesameforallthedeepQ-learningvariants. Wesetthediscountfactorγ to0.99andthe
learningrateαto0.001. Ourneuralnetworktakesinputofasizeequaltothesumofthedimensionsofstatesandactions
withasingleoutputneuron. Thenetworkconsistsoftwohiddenlinearlayers,eachwithasizeof64,followedbyaReLU
activationfunction(Nair&Hinton,2010). Wekeeptheexplorationrateεthesameforallstates,initializeitat1,andapply
adecayfactorof0.995, withaminimumthresholdof0.01. Forntotalnumberofactions, duringtraining, totrainthe
network,weusestochasticbatchesofsize⌈log(n)⌉uniformlysampledfromabufferofsize2⌈log(n)⌉. Weaveragedthe
resultsoverfiverepetitions. Forthestochasticmethods,weconsidertheactionsinthebatchofactionsasthememoryset
M. WechoosethebatchsizeinthiswaytokeepthecomplexityoftheStochasticQ-learningwithinO(log(n)).
D.3.ComputeandImplementation
WeimplementthedifferentQ-learningmethodsusingPython3.9,Numpy1.23.4,andPytorch2.0.1. Forproximalpolicy
optimization(PPO)(Schulmanetal.,2017),asynchronousactor-critic(A2C)(Mnihetal.,2016),anddeepdeterministic
policygradient(DDPG)(Lillicrapetal.,2015),weusetheimplementationsofStable-Baselines(Hilletal.,2018). Wetest
thetrainingtimeusingaCPU11thGenIntel(R)Core(TM)i7-1165G7@2.80GHz1.69GHz. with16.0GBRAM.
22StochasticQ-learningforLargeDiscreteActionSpaces
0.25 stoch max 0.6 StochDQN
max StochDDQN
0.20 0.5 DQN
DDQN
0.4
0.15 Random
0.3
0.10
0.2
0.05 0.1
0.00 0.0
22 24 26 28 210 212 22 24 26 28 210 212
Number of Actions Number of Actions
(a)ActionSelectionTime (b)FullStepDuration
Figure5: Comparisonresultsforthestochasticanddeterministicmethods. Thex-axisrepresentsthenumberofpossible
actions,andthey-axisrepresentsthetimestepdurationoftheagentinseconds.
100
80
60
40
max
20 stochmax w/
stochmax w/o
0 random
0 50 100 150 200 250 300
Steps
Figure6: stochmaxwith(w/)andwithout(w/o)memoryMvs. maxonuniformlydistributedactionvaluesasdescribed
inSectionB.Thex-axisandy-axisrepresentthestepsandthevalues,respectively.
E.AdditionalResults
E.1.WallTimeSpeed
Stochasticmaximizationmethodsexhibitlogarithmiccomplexityregardingthenumberofactions,asconfirmedinFig.
5a. Therefore, bothStochDQNandStochDDQN,whichapplythesetechniquesforactionselectionandupdates, have
exponentiallyfasterexecutiontimescomparedtobothDQNandDDQN,whichcanbeseeninFig5bwhichshowsthe
completestepdurationfordeepQ-learningmethods,whichincludeactionselectionandnetworkupdate. Theproposed
methodsarenearlyasfastasarandomalgorithm,whichsamplesandselectsactionsrandomlyandhasnoupdates.
E.2.StochasticMaxmization
E.2.1.STOCHASTICMAXMIZATIONVSMAXIMIZATIONWITHUNIFORMREWARDS
InthesettingdescribedinSectionB.3with5000uniformlyindependentlydistributedactionvaluesintherangeof[0,100],
asshowninFig. 6,stochmaxwithoutmemory,i.e.,M = ∅reachesaround91inaveragereturn,andkeepsfluctuating
around,whilestochmaxwithMquicklyachievestheoptimalreward.
E.2.2.STOCHASTICMAXIMIZATIONANALYSIS
Inthissection,weanalyzestochasticmaximizationbytrackingreturnedvaluesacrossrounds,ω (Eq.(10)),andβ (Eq.(9)),
t t
whichweprovidehere. Attimestept,givenastates,andthecurrentestimatedQ-functionQ ,wedefinethenon-negative
t
23
)s(
emiT
seulaV
)s(
noitaruD
petS
emiTStochasticQ-learningforLargeDiscreteActionSpaces
200 stoch max 1.00 0.5
max 0.99 0.4
150
0.98 0.3
100
0.97 0.2
50 0.96 0.1
0.95 0.0
0
0 5000 10000 15000 20000 25000 30000 0 5000 10000 15000 20000 25000 30000 0 5000 10000 15000 20000 25000 30000
Steps Steps Steps
(a)stochmaxvsmax (b)Ratioω (c)Differenceβ
t t
Figure7: Comparisonresultsforthestochasticandnon-stochasticmethodsfortheInvertedPendulumwith512actions.
350 8
stoch max
300 max 0
250 6
2
200 4
150
4
100 2
50 6
0 0
8
50
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Steps Steps Steps
(a)stochmaxvsmax (b)Ratioω (c)Differenceβ
t t
Figure8: Comparisonresultsforthestochasticandnon-stochasticmethodsfortheHalfCheetahwith4096actions.
underestimationerrorasβ (s),asfollows:
t
β (s)=maxQ (s,a)−stochmaxQ (s,a). (32)
t t t
a∈A a∈A
Furthermore,wedefinetheratioω (s),asfollows:
t
stochmax Q (s,a)
ω (s)= a∈A t . (33)
t max Q (s,a)
a∈A t
Itfollowsthat:
β (s)
ω (s)=1− t . (34)
t max Q (s,a)
a∈A t
ForDeepQ-Networks,fortheInvertedPendulum-v4,bothstochmaxandmaxreturnsimilarvalues(Fig. 7a),ω approaches
t
onerapidly(Fig. 7b)andβ remainsbelow0.5(Fig. 7c). InthecaseofHalfCheetah-v4,bothstochmaxandmaxreturn
t
similarvalues(Fig. 8a),ω quicklyconvergestoone(Fig. 8b),andβ isupperboundedbeloweight(Fig. 8c).
t t
Whilethedifferenceβ remainsbounded, thevaluesofbothstochmaxandmaxincreaseovertheroundsastheagent
t
exploresbetteroptions. Thisleadstotheratioω convergingtooneastheerrorbecomesnegligibleovertherounds,as
t
expectedaccordingtoEq. (34).
E.3.StochasticQ-networkRewardAnalysis
AsillustratedinFig. 9aandFig. 9bfortheinvertedpendulumandhalfcheetahexperiments,whichinvolve512and4096
actions,respectively,bothStochDQNandStochDDQNattaintheoptimalaveragereturninacomparablenumberofrounds
toDQNandDDQN.Additionally,StochDQNexhibitsthequickestattainmentofoptimalrewardsfortheinvertedpendulum.
Furthermore,whileDDQNdidnotperformwellontheinvertedpendulumtask,itsmodification,i.e.,StochDDQN,reached
theoptimalrewards.
24
seulaV
seulaV
seulaV
seulaV
seulaV
seulaVStochasticQ-learningforLargeDiscreteActionSpaces
100
175
80 StochDQN 150 StochDQN
StochDDQN
125 StochDDQN
DQN
60 DDQN 100 DQN
DDQN
PPO 75
40 A2C PPO
50 A2C
DDPG
20 Random 25 Random
0
0
0 5000 10000 15000 20000 25000 30000 0 20000 40000 60000 80000 100000
Steps Steps
(a)InvertedPendulum (b)HalfCheetah
Figure9: Stochasticvsnon-stochasticofdeepQ-learningvariantsonInvertedPendulumandHalfCheetah,withstepson
thex-axisandaveragereturns,smoothedoverasize100windowonthey-axis.
0
20000
40000
Stochastic Q-learning
60000 Stochastic Double Q-learning
Stochastic Sarsa
Q-learning
80000
Double Q-learning
Sarsa
100000 Random
0 2000 4000 6000 8000 10000
Steps
(a)InstantaneousRewards (b)CumulativeRewards
Figure10: Comparingstochasticandnon-stochasticQ-learningapproachesontheCliffWalking,withstepsonthex-axis,
instantaneousrewardssmoothedoverasize1000windowonthey-axisforplot(a),andcumulativerewardsonthey-axis
forplot(b).
E.4.StochasticQ-learningRewardAnalysis
WetestedStochasticQ-learning,StochasticDoubleQ-learning,andStochasticSarsainenvironmentswithbothdiscrete
statesandactions. Interestingly,asshowninFig. 11,ourstochasticalgorithmsoutperformtheirdeterministiccounterparts
intermsofcumulativerewards. Furthermore,wenoticethatStochasticQ-learningoutperformsalltheconsideredmethods
regardingthecumulativerewards. Moreover,intheCliffWalking-v0(asshowninFig. 10),aswellasforthegenerated
MDPenvironmentwith256possibleactions(asshowninFig. 12),allthestochasticandnon-stochasticalgorithmsreach
theoptimalpolicyinasimilarnumberofsteps.
25
nruteR
egarevA
nruteR
egarevA
sdraweR
evitalumuCStochasticQ-learningforLargeDiscreteActionSpaces
Stochastic Q-learning
2000 Stochastic Double Q-learning
Stochastic Sarsa
Q-learning
1500 Double Q-learning
Sarsa
Random
1000
500
0
0 100000 200000 300000 400000 500000
Steps
(a)InstantaneousRewards (b)CumulativeRewards
Figure11: Comparingstochasticandnon-stochasticQ-learningapproachesontheFrozenLake,withstepsonthex-axis,
instantaneousrewardssmoothedoverasize1000windowonthey-axisforplot(a),andcumulativerewardsonthey-axis
forplot(b).
600000
400000
200000
0 Stochastic Q-learning
Stochastic Double Q-learning
Stochastic Sarsa
200000 Q-learning
Double Q-learning
400000 Sarsa
Random
0 2000 4000 6000 8000 10000
Steps
(a)InstantaneousRewards (b)CumulativeRewards
Figure12: Comparingstochasticandnon-stochasticQ-learningapproachesonthegeneratedMDPenvironment,withsteps
onthex-axis,instantaneousrewardssmoothedoverasize1000windowonthey-axisforplot(a),andcumulativerewards
onthey-axisforplot(b).
26
sdraweR
evitalumuC
sdraweR
evitalumuC