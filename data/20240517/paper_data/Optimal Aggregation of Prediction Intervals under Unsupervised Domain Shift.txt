Optimal Aggregation of Prediction Intervals under
Unsupervised Domain Shift
JiaweiGe∗ DebarghyaMukherjee∗
OperationsResearch&FinancialEngineering DepartmentofMathematicsandStatistics
PrincetonUniversity BostonUniversity
jg5300@princeton.edu mdeb@bu.edu
JianqingFan
OperationsResearch&FinancialEngineering
PrincetonUniversity
jqfan@princeton.edu
Abstract
Asmachinelearningmodelsareincreasinglydeployedindynamicenvironments,
it becomes paramount to assess and quantify uncertainties associated with dis-
tribution shifts. A distribution shift occurs when the underlying data-generating
processchanges, leadingtoadeviationinthemodel’sperformance. Thepredic-
tioninterval,whichcapturestherangeoflikelyoutcomesforagivenprediction,
servesasacrucialtoolforcharacterizinguncertaintiesinducedbytheirunderlying
distribution. Inthispaper, weproposemethodologiesforaggregatingprediction
intervals to obtain one with minimal width and adequate coverage on the target
domain under unsupervised domain shift, under which we have labeled samples
from a related source domain and unlabeled covariates from the target domain.
Our analysis encompasses scenarios where the source and the target domain are
relatedviai)aboundeddensityratio,andii)ameasure-preservingtransformation.
Ourproposedmethodologiesarecomputationallyefficientandeasytoimplement.
Beyondillustratingtheperformanceofourmethodthroughareal-worlddataset,
wealsodelveintothetheoreticaldetails.Thisincludesestablishingrigoroustheo-
reticalguarantees,coupledwithfinitesamplebounds,regardingthecoverageand
width of our prediction intervals. Our approach excels in practical applications
and is underpinned by a solid theoretical framework, ensuring its reliability and
effectivenessacrossdiversecontexts2.
1 Introduction
Inthemoderneraofbigdataandcomplexmachinelearningmodels,extensivedatacollectedfrom
diversesourcesareoftenusedtobuildapredictivemodel. However,theassumptionofindependent
andidenticallydistributed(i.i.d.) dataisfrequentlyviolatedinpracticalscenarios. Takealgorithmic
fairness as an example: historical data often exhibit sampling biases towards certain groups, like
femalesbeingunderrepresentedincreditcarddata. Overtime,thedifferencesingroupproportions
havediminished,leadingtodistributionshifts. Consequently,modelstrainedonhistoricaldatamay
faceshifteddistributionsduringtesting,andproperadjustmentsisneeded.Distributionshifthasgar-
neredsignificantattentionfromstatisticalandmachinelearningcommunitiesundervariousnames,
∗equalcontribution
2Forthecodetoreproducetheresultsinourpaper,seehere.
Preprint.Underreview.
4202
yaM
61
]EM.tats[
1v20301.5042:viXrai.e.,transferlearning(PanandYang,2009;Weissetal.,2016),domainadaptation(Farahanietal.,
2021),domaingeneralization(Zhouetal.,2022;Wangetal.,2022),continuallearning(DeLange
et al., 2021; Mai et al., 2022), multitask learning (Zhang and Yang, 2021) etc. While numerous
methodsareavailableintheliteraturefortrainingpredictivemodelsunderdistributionshift,uncer-
taintyquantificationunderdistributionshifthasreceivedrelativelyscantattentiondespiteitscrucial
importance. Onenotableexceptionisconformalpredictionunderdistributionshift;Tibshiranietal.
(2019)proposedavariantofstandardconformalinferencemethodstoaccommodatetestdatafrom
adistinctdistributionfromthetrainingdataunderthecovariateshift. Recently,GibbsandCandes
(2021)introducedanadaptiveconformalinferenceapproachsuitableforcontinuouslychangingdis-
tributionsovertime. Additionally,quantileregressionunderdistributionshiftoffersanotheravenue
foraddressinguncertaintyquantificationunderdistributionshift(Eastwoodetal.,2022).
Althoughfewmethodsexistforconstructingpredictionintervalsunderdistributionshift,mostfocus
primarilyonensuringcoverageguaranteeratherthanminimizingintervalwidth. Thispromptsthe
immediatequestion: Canwegeneratepredictionintervalsinthetargetdomainthatprovidebothi)
coverageguaranteeandii)minimalwidth? Thispaperseekstoaddressthisquestionbyleveraging
model aggregation techniques. Suppose we have K different methods for constructing prediction
intervals in the source domain. Our proposed approach efficiently combines these methods to
produce prediction intervals in the target domain with adequate coverage and minimal width.
Whenindividualmethodsaretheelementarybasisfunctions,suchasthekernelbasis,theresulting
aggregation is indeed a construction of the prediction interval based on the basis functions. Our
methodologydrawsinspirationprimarilyfromrecentwork(Fanetal.,2023)onpredictioninterval
aggregation under the i.i.d. setting. However, a key distinction lies in our focus on unsupervised
domain adaptation, where we can access labeled samples from the source and unlabeled samples
fromthetargetdomain. Certainassumptionsregardingthesimilaritiesbetweenthesedomainsare
necessary to facilitate knowledge transfer from the source to the target domain. We explore two
types of similarities in this paper: i) covariate shift, where we assume that the distribution of the
response variable Y given X is consistent across both domains, albeit the distribution of X may
differ,andii)domainshift,whereweassumethattheconditionaldistributionofY givenX remains
unchanged up to a measure-preserving transformation. Covariate shift is a well-explored concept
intransferlearningandhasalsogarneredattentioninuncertaintyquantification. Itallowsdifferent
distributions of X while maintaining identical conditional distributions Y|X across domains. For
constructingconformalpredictionintervalswithinthisframework,seeTibshiranietal.(2019);Hu
and Lei (2023); Yang et al. (2022); Lei and Cande`s (2021) and references therein. On the other
hand, distribution shift is more general, allowing both the distribution of X and the conditional
distribution of Y|X to differ across domains. Our methods in this context draw upon domain
matching principles via transport map, as proposed in Courty et al. (2014) and further elaborated
in subsequent works like Courty et al. (2016, 2017); Redko et al. (2017), among others. The key
assumption is the existence of a measure-preserving/domain-aligning map T from the target to
the source domain, such that the conditional distribution of Y|X on the target domain matches
Y|T(X) on the source domain, i.e., conditional distributions matches upon domain alignment.
The case where the domain-aligning map is the optimal transport map has received considerable
attention in the literature, e.g., see Courty et al. (2014, 2016, 2017); Xu et al. (2020). Empirical
evidencesupportstheefficacyofdomainalignmentthroughoptimaltransportmapsacrossvarious
datasets. Forinstance,inXuetal.(2020),avariantofthismethodisappliedfordomainadaptation
in image recognition tasks, such as recognizing similarities between USPS (Hull, 1994), MNIST
(LeCun et al., 1998), and SVHN digit images (Netzer et al., 2011), as well as between different
types of images in the Office-home dataset (Venkateswara et al., 2017), including artistic and
product images. Additionally, in Courty et al. (2014), the authors explore the impact of domain
alignment via optimal transport maps on the face recognition problem, where different poses give
rise to distinct domains. However, most of these works concentrate on training predictors that
perform well on the target domain without any guarantee regarding uncertainty quantification. To
our knowledge, this is the first work to propose a method with rigorous theoretical guarantees for
constructingpredictionintervalsonthetargetdomainunderthedomain-aligningassumptionwithin
anunsuperviseddomainadaptationframework. Wenowsummarizeourcontributions.
Our Contributions: This paper introduces a novel methodology for aggregating various
predictionmethodsavailableonthesourcedomaintoconstructaunifiedpredictionintervalonthe
target domain under both covariate shift and domain shift assumptions. Our approach is simple
2and easy to implement and requires solving a convex optimization problem, which can even be
simplified toa linear programproblem in certainscenarios. We alsoestablish rigorous theoretical
guarantees,presentingfinitesampleconcentrationboundstodemonstratethatourmethodachieves
adequate coverage with a small width. Furthermore, our methodology extends beyond model
aggregation;itcanbeusedtoconstructefficientpredictionintervalsfromanyconvexcollectionof
candidatefunctions. Inthepaper,weadoptthisbroaderperspective,discussinghowtheaggregation
of prediction intervals emerges as a particular case. Lastly, we validate the effectiveness of our
approachbyanalyzingareal-worlddataset.
2 Notationsandpreliminaries
Notation ThecovariatesofthesourceandthetargetdomainsaredenotedbyX andX ,respec-
S T
tively,andX := X ∪X . ThespaceofthelabelisdenotedbyY. WeusethenotationE (resp.
S T S
E )todenotetheexpectationwithrespecttothesource(resp. target)distribution. Theexpectation
T
withrespecttosampledistributionisdenotedbyE andE . Weusep (resp. p )todenotethe
n,S n,T S T
probabilitydensityfunctionofX onthesourceandthetargetdomain,respectively. Throughoutthe
paper,weusectodenoteuniversalconstants,whichmayvaryfromlinetoline.
2.1 Problemformulation
Our setup aligns with the unsupervised domain adaption; we assume to have n i.i.d. labeled
S
samples {X ,Y }nS ∼ P (X,Y) from the source domain, and n i.i.d. unlabeled samples
S,i S,i i=1 S T
{X }nT ∼ P (X) from the target domain. Given any α > 0, ideally, we want to construct a
T,i i=1 T
validpredictionintervalwithminimalwidthonthetargetdomain:
min E [u(X)−l(X)], s.t. P (l(X)≤Y ≤u(X))≥1−α. (2.1)
T T
f∈F
Inmanypracticalcontexts,thepreferredpredictionintervaltakestheformofm(X)±g(X),where
m(X)isapredictorforY givenX (anestimatorofE [Y | X]),andg(X)gaugestheuncertainty
T
of the predictor m(X). The optimizer of (2.1) takes this simplified form when the distribution
of Y − E [Y | X] is symmetric around 0. Moreover, it offers a straightforward interpretation
T
as the pair (m,g) is a predictor and a function quantifying its uncertainty. Within the framework
of this simplified prediction interval, we need to estimate m and g. Estimating the conditional
mean function m is relatively easy and has been extensively studied; one may use any suitable
parametric/non-parametricmethod.Uponestimatingm,weneedtoestimategsothattheprediction
interval[m(X)±g(X)]hasbothadequatecoverageandminimalwidth.Thistranslatesintosolving
thefollowingoptimizationproblem:
min E [f(X)], s.t. P (cid:0) (Y −m(X))2 >f(X)(cid:1) ≤α. (2.2)
T T
f∈F
Let f be the solution of the above optimization problem. Then the optimal prediction interval is
0
(cid:112)
[m (x)± f (x)]. However,thekeychallengehereisthatwedonotobservetheresponsevariable
0 0
Y from the target, and consequently, solving (2.2) becomes infeasible. Hence, we must rely on
transferringourknowledgeacquiredfromlabeledobservationsinthesourcedomain,whichneces-
sitates making certain assumptions regarding the similarity between the two domains. Depending
on the nature of these assumptions regarding domain similarity, our findings are presented in two
sections:Section3addressescovariateshiftundertheboundeddensityratioassumption,whileSec-
tion4considersamoregeneraldistributionassumptionundermeasure-preservingtransformations.
Furthermore,aswillbeshownlater,thisproblem,thoughwell-defined,isnoteasilyimplementable.
Therefore,weproposeasurrogateconvexoptimizationprobleminthispaperandprovideitstheo-
reticalguarantees.
2.2 Complexitymeasure
The complexity of the function class F is usually quantified through the Rademacher complexity,
definedasfollows.
Definition2.1(Rademachercomplexity). LetF beafunctionclassand{X }n beasetofsamples
i i=1
drawni.i.d. fromadistributionD. TheRademachercomplexityofF isdefinedas
(cid:34) n (cid:35)
1 (cid:88)
R (F)=E sup ϵ f(X ) , (2.3)
n ϵ,D n i i
f∈F
i=1
3where{ϵ }n arei.i.d. Rademacherrandomvariablesthatequalsto±1withprobability1/2each.
i i=1
3 Covariateshiftwithboundeddensityratio
Setup and methodology In this section, we focus on the covariate shift problems, where the
marginal densities p (X) and p (X) of the covariates may vary between the source and target
S T
domains, albeit the conditional distribution Y|X remains same. Denote by m (x) = E [Y|X =
0 T
x] = E [Y|X = x], the conditional mean function. For the ease of the presentation, we assume
S
m is known. If unknown, one may use the labeled source data to estimate it using a suitable
0
parametric/non-parametricestimate(e.g.,splines,localpolynomial,ordeepneuralnetworks),sub-
sequently substituting m with mˆ in our approach. The density ratio of the source and the target
0
distribution of X is denoted by w (x) := p (x)/p (x). We henceforth assume that the density
0 T S
ratioisuniformlybounded:
Assumption3.1. ThereexistsW suchthatsup w (x)≤W.
x∈XS 0
Ifw isknown,(2.2)hasthefollowingsamplelevelcounterpart:
0
min E [f(X)], s.t. E (cid:2) w (X)1 (cid:3) ≤α, (3.1)
f∈F n,T n,S 0 (Y−m0(X))2>f(X)
which is NP-hard owing to the presence of the indicator function. However, in many practical
scenarios, itisobservedthat theshapeofthepredictionband doesnotchangemuchifwe change
the level of coverage (i.e., α); only the bands shrink/expand. Indeed, the true shape determines
the average width; if the shape is wrong, then the width of the prediction band is quite likely
to be unnecessarily large. Therefore, to obtain a prediction interval with adequate coverage and
minimalwidth,oneshouldfirstidentifytheshapeofthepredictionbandandthenshrink/expandit
appropriatelytogetthedesiredcoverage. Thismotivatesthefollowingtwostepsprocedure:
Step 1: (Shape estimation) Obtain an initial estimate fˆ via by solving (3.1) for α = 0 (to
init
capturetheshape):
min E [f(X)], s.t. f(X )≥(Y −m (X ))2 ∀1≤i≤n :w (X )>0. (3.2)
f∈F n,T i i 0 i S 0 i
Step2: (Shrinkage)Refinefˆ byscalingitdownusingλˆ(α),definedas:
init
(cid:110) (cid:111)
λˆ(α)=inf λ≥0:E [w (X)1 ]≤α . (3.3)
n,S 0 (Y−m0(X))2>λfˆ init(X)
Thefinalpredictionintervalis:
(cid:20) (cid:113) (cid:113) (cid:21)
P(cid:99)I 1−α(x)= m 0(x)− λˆ(α)fˆ init(x),m 0(x)+ λˆ(α)fˆ init(x) . (3.4)
In Step 1, we relax (3.1) by effectively setting α = 0. This relaxation aids in determining the
optimal shape while also converting (3.1) into a convex optimization problem (equation (3.2)) as
longasF isaconvexcollectionoffunctions. Furthermore,in(3.2),weonlyconsiderthosesource
observations for which w (x) > 0, as otherwise, the samples are not informative for the target
0
domain. Inpractice,w istypicallyunknown;onemayusethesourceandtargetdomaincovariates
0
toestimatew . Varioustechniquesareavailableforestimatingthedensityratio(e.g.,Ueharaetal.
0
(2016);Choietal.(2022);Qin(1998);Grettonetal.(2008)andreferencestherein). However,any
suchestimatorwˆ(x)canbenon-zeroforxwherew (x)=0duetoestimationerror. Consequently,
0
wˆ maynotbeefficientinselectinginformativesourcesamples. Tomitigatethisissue,wepropose
belowamodificationof(3.2),utilizingahingefunctionh (t):=max{0,(t/δ)+1}:
δ
min E [f(X)]
n,T
f∈F (3.5)
subjectto E [wˆ(X)h (cid:0) (Y −m (X))2−f(X)(cid:1) ]≤ϵ,
n,S δ 0
with δ and ϵ should be chosen based on sample size n and the estimation accuracy of wˆ. When
S
wˆ =w (i.e.,thedensityratioisknown),thenbychoosingϵ=0andδ →0,(3.5)recovers(3.2).As
0
h isconvex,theoptimizationproblem(3.5)isstillaconvexoptimizationproblem. Wesummarize
δ
ouralgorithminAlgorithm1.
4Algorithm1Predictionintervalswithboundeddensityratio
1: Input: m 0 (or mˆ if unknown), density ratio estimator wˆ, function class F, sample D S =
{(X ,Y )}nS andD ={X }nT ,parametersδ,ϵ,coveragelevel1−α.
S,i S,i i=1 T T,i i=1
2: Obtainfˆ initbysolving(3.5).
3: Obtaintheshrinklevelλˆ(α)bysolving(3.3)withw 0replacedbywˆ.
4: Output: P(cid:99)I 1−α(x)definedin(3.4).
Theoreticalresults Wenextpresenttheoreticalguaranteesofthepredictionintervalobtainedvia
Algorithm1. Fortechnicalconvenience,weresorttodata-splitting; wedividethesourcedatainto
twoequalparts(D andD ),useD andD tosolve(3.5),andD toobtaintheshrinklevel
S,1 S,2 S,1 T S,2
λˆ(α). Without loss of generality, we assume m ≡ 0 (otherwise, we set Y ← Y −m (X)). A
0 0
carefulinspectionofStep1revealsthatfˆ aimstoapproximateafunctionf∗definedasfollows:
init
f∗ =argmin E [f(X)] subjectto Y2 <f(X)almostsurelyontargetdomain. (3.6)
f∈F T
Inotherwords,fˆ estimatesf∗thathasminimalwidthamongallfunctionscoveringtheresponse
init
variable. This is motivated by the philosophy that the right shape leads to a smaller width. The
followingtheoremprovidesafinitesampleconcentrationboundontheapproximationerroroffˆ :
init
Theorem3.2. SupposeY2−f∗(X) ≤ B onthesourcedomainandhasadensityboundedbyL.
Alsoassume∥f∥ ≤B forallf ∈F. Thenfor
∞ F
(cid:113) (cid:16) (cid:113) (cid:17)
ϵ≥Lδ+W t + B+δ · E [|wˆ(X)−w (X)|]+(W +W′) t , (3.7)
nS δ S 0 nS
wehavewithprobabilityatleast1−3e−t:
(cid:113)
E [fˆ (X)]≤E [f∗(X)]+2R (F −f∗)+2B t
T init T nT F 2nT
whereW′ =∥wˆ∥ .
∞
The bound in the above theorem depends on the Rademacher complexity of F (the smaller, the
better),theestimationerrorofw ,andaninterplaybetweenthechoiceof(ϵ,δ). Thelowerbound
0
onϵin(3.7)dependsonbothδand1/δ. Althoughitisnotimmediatefromtheabovetheoremwhy
weneedtochooseϵtobeassmallaspossible,itwillbeapparentinoursubsequentanalysis;indeed
ifϵislargein(3.5),thenfˆ ≡ 0willbeasolutionof(3.5). Consequently,theshapewillnotbe
init
captured. Therefore,oneshouldfirstchooseδ (sayδ∗),thatminimizesthelowerbound(3.7),and
then set ϵ = ϵ∗ equal to the value of the right-hand side of (3.7) with δ = δ∗, which ensures that
ϵ∗ is optimally defined to capture the shape accurately. Once the shape is identified, we shrink it
properlyinStep2toattainthedesiredcoverageandreducethewidth. Althoughideallyλˆ(α) ≤ 1,
itisnotimmediatelyguaranteedasweuseseparatedata(D )forshrinking. Thefollowinglemma
S,2
showsthatλˆ(α)≤1foranyfixedα>0aslongasthesamplesizeislargeenough. Recallthatthe
dataweresplitintoexactlyhalfwithsizen =|D |.
S S
Lemma3.3. Undertheaforementionedchoiceof(ϵ∗,δ∗),wehavewithhighprobability:
1 (cid:88)
wˆ(X )1 ≤α,
n S/2 i {(Yi−m0(Xi))2>fˆ init(Xi)}
i∈DS,2
foralllargen ,providedthatwˆisaconsistentestimatorofw . Hence,λˆ(α)≤1.
S 0
Ourfinaltheoremforthissectionprovidesacoverageguaranteeforthepredictionintervalgivenby
Algorithm1.
Theorem3.4. Forthepredictionintervalobtainedin(3.4),withprobabilitygreaterthan1−2e−t:
(cid:12) (cid:16) (cid:17) (cid:12) (cid:113) (cid:113)
(cid:12)P Y2 >λˆ(α)fˆ (X)|D ∪D −α(cid:12)≤E [|wˆ(X)−w(X)|]+(2W+W′) t + C
(cid:12) T init S T (cid:12) S 2nS nS
forsomeconstantC >0andW′ =∥wˆ∥ .
∞
5Theorem3.4validatesthecoverageofthepredictionintervalderivedthroughAlgorithm1,achieving
thedesiredcoveragelevelastheestimateofw improvesandsamplesizeexpands. Theorems3.2
0
and3.4collectivelydemonstratetheefficacyofourmethodinmaintainingvalidityandaccurately
capturingtheoptimalshapeofthepredictionband,whichinturnleadstosmallintervalwidths.
Remark3.5. Inouroptimizationproblem,we’vesubstitutedtheindicatorlosswiththehingeloss
function to ensure convexity. However, it’s worth noting that if we know the subset of X where
S
w (x)>0beforehand,wecoulddirectlyoptimize(3.2). Thisapproachwouldbeeasytoimplement
0
andwouldn’tinvolvetuningparameters(δ,ϵ). Aspecialcaseiswhenw (x)>0forallx∈X (as
0 S
istrueinourexperiment),whichsimplifiestheconditionin(3.2)tof(X )≥(Y −m (X ))2forall
i i 0 i
1≤i≤n . However,ifthisinformationisunavailable,onecanstillemploy(3.2)byenforcingthe
S
constraintonallsourceobservations.Whilethisapproachmightresultinwiderpredictionintervals,
itiseasytoimplementanddoesn’trequiretuningparameters.
4 Domainshiftandtransportmap
Setupandmethodology Intheprevioussection,weassumeauniformboundonthedensityratio.
However,thismaynotbethecaseinreality;itispossiblethatthereexistsx∈supp(X )∩supp(Xc),
T S
whichimmediatelyimpliesthatw (x) = ∞. Inimagerecognitionproblems,ifthesourcedataare
0
imagestakenduringthedayatsomeplace, andthetargetdataareimagestakenatnight, thenthis
directly results in an unbounded density ratio (due to the change in the background color). Yet a
transportmapcouldeffectivelymodelthisshiftbyadaptingfeaturesfromthesourcetocorrespond
withthoseofthetarget,maintainingtheunderlyingpatternsorobjectrecognitioncapabilitiesacross
bothdomains. Toperformtransferlearninginthissetup,wemodelthedomainshiftviaameasure
transportmapT thatpreservestheconditionaldistribution,aselaboratedinthefollowingassump-
0
tion:
d
Assumption4.1. ThereexistsameasuretransportmapT : X → X ,i.e.,T (X ) = X ,such
0 T S 0 T S
that: P (Y |X =x)=d P (Y |X =T (x)), ∀x∈X .
T S 0 T
ThisassumptionallowstheextrapolationofsourcedomaininformationtothetargetdomainviaT ,
0
enabling the construction of prediction intervals at x ∈ X by leveraging the analogous intervals
T
at T (x) ∈ X . Inspired by this observation, we present our methodology in Algorithm 2 that
0 S
essentiallyconsistsoftwokeysteps: i)constructingapredictionintervalinthesourcedomainand
ii) transporting this interval to the target domain using the estimated transport map T . If T (or
0 0
its estimate) is not given, it must be estimated from the source and the target covariates. Various
methodsareavailableintheliterature(e.g.,Divoletal.(2022);Seguyetal.(2017);Makkuvaetal.
(2020); Deb et al. (2021)), and practitioners can pick a method at their convenience. Notably, the
processesdescribedinequations(4.1)and(4.2)followthemethodology(i.e.,(3.2)and(3.3))from
Section3forscenarioswithoutshift(i.e.,w ≡ 1),addingaslightδ toensurecoverageevenwhen
0
F iscomplex. InAlgorithm2,weassumetheconditionalmeanfunctionm onthesourcedomain
0
Algorithm2Transportmap
1: Input: conditionalmeanfunctionm 0 onthesourcedomain,transportmapestimatorTˆ 0,func-
tion class F, sample D = {(X ,Y )}nS and D = {X }nT , parameter δ, coverage
S S,i S,i i=1 T T,i i=1
level1−α.
2: Obtainfˆ initbysolving:
min 1 (cid:80)nS f(X ), s.t. f(X )≥(Y −m (X ))2∀i∈[n ]. (4.1)
f∈F nS i=1 S,i S,i S,i 0 S,i S
3: Obtaintheshrinklevel
(cid:110) (cid:111)
λˆ(α):=inf λ>0: 1 (cid:80)nS 1 ≤α . (4.2)
nS i=1 (YS,i−m0(XS,i))2≥λ(fˆ init(XS,i)+δ)
(cid:20) (cid:114) (cid:16) (cid:17)(cid:21)
4: Output: P(cid:99)I 1−α(x)= m 0◦Tˆ 0(x)± λˆ(α)· fˆ init◦Tˆ 0(x)+δ .
is known. In cases where the conditional mean function m on the source domain is unknown, it
0
6can be estimated using standard regression methods from labeled source data, after which m is
0
replacedbythisestimate,mˆ.
Remark 4.2 (Model aggregation). Suppose we have K different methods {f ,...,f } for con-
1 K
structingpredictionintervalsinthesourcedomain. Inthecontextofmodelaggregation,(4.1)then
reducesto:
1
(cid:88)nS(cid:110)(cid:88)K
(cid:111)
min α f (X )
α1,...,αK n j j S,i
S
i=1 j=1
K
(cid:88)
subjectto α f (X )≥(Y −m (X ))2∀i∈[n ],
j j S,i S,i 0 S,i S
j=1
α ≥0, ∀1≤j ≤K.
j
Inotherwords,thefunctionclassF isalinearcombinationofthecandidatemethods. Theproblem
isthensimplifiedtoalinearprogramproblem,whichcanbeimplementedefficientlyusingstandard
solvers.
Theoreticalresults Wenowpresenttheoreticalguaranteesofourmethodologytoensurethatour
method delivers what it promises: a prediction interval with adequate coverage and small width.
For technical simplicity, we split data here: divide the labeled source observation with two equal
parts (with n /2 observations in each), namely D and D . We use D to solve (4.1) and
S S,1 S,2 S,1
obtain the initial estimator fˆ , and D to solve (4.2), i.e. obtaining the shrinkage factor λˆ(α).
init S,2
Henceforth, without loss of generality, we assume m = 0 and present the theoretical guarantees
0
ofourestimator. WestartwithananalogofTheorem3.2,whichensuresthatwithhighprobability
fˆ ◦Tˆ approximatesthefunctionthathasminimalwidthamongallthefunctionsinF composed
init 0
withT thatcoversthelabelsonthetargetalmostsurely:
0
Theorem4.3. AssumethefunctionclassF isB -boundedandL -Lipschitz. Define
F F
∆=min(cid:8)E [f ◦T (X)]:f ∈F,Y2 ≤f ◦T (X)a.s. ontargetdomain(cid:9) .
T 0 0
Thenwehavewithprobability≥1−e−t:
(cid:114)
t
E [fˆ ◦Tˆ (X)]≤∆+4R (F)+L E [|Tˆ (X)−T (X)|]+4B .
T init 0 nS F T 0 0 F 2n
S
Theupperboundonthepopulationwidthoffˆ ◦Tˆ (x)consistsoffourterms: thefirsttermisthe
init 0
minimalpossiblewidththatcanbeachievedusingthefunctionsfromF,thesecondterminvolves
the Rademacher complexity of F, the third term encodes the estimation error of T , and the last
0
termisthedeviationtermthatinfluencestheprobability. Hence, themarginbetweenthewidthof
thepredictedintervalandtheminimumachievablewidthissmall,withtheconvergenceraterelying
ontheprecisionofestimatingT andthecomplexityofF,asexpected.
0
We next establish the coverage guarantee of our estimator of Algorithm 2, obtained upon suitable
truncationoffˆ . Asmentioned,theshrinkageoperationisperformedonaseparatedatasetD .
init S,2
Therefore, it is not immediate whether the shrinkage factor λˆ(α) is smaller than 1, i.e., whether
we are indeed shrinking the confidence interval (λˆ(α) > 1 is undesirable, as it will widen fˆ ,
init
increasingthewidthofthepredictionband). Thefollowinglemmashowsthatwithhighprobability,
λˆ(α)≤1.
Lemma4.4. Withprobabilitygreaterthanorequalto1−e−t,wehave:
P(λˆ(α)>1|D S,1,D T)≤e−(α− 6p pn nS S)2nS ,
where
p
nS
=P S(cid:16) Y2 ≥fˆ init(X)+δ(cid:12) (cid:12)D S,1,D T(cid:17) ≤ 4
δ
(cid:18)(cid:113) E S n[Y S4] +R nS(F)(cid:19) +(cid:113) nt
S
.
7Here p is the conditional probability of a test observation Y falling outside
(cid:113)
nS
(cid:113)
[− fˆ (X)+δ, fˆ (X)+δ], which is small as evident from the above lemma. In par-
init init
ticular, for model aggregation, if F is the linear combination of K functions, then p is of the
order(cid:112) K/n . Hence,thefinalpredictionintervalisguaranteedtobeacompressedfn oS rmoffˆ
S init
with an overwhelmingly high probability. We present our last theorem of this section, confirming
that the prediction interval derived from Algorithm 2 achieves the intended coverage level with a
highprobability:
Theorem4.5. UnderthesamesetupofTheorem4.3, alongwiththeassumptionthatf (y | x)is
S
uniformlyboundedbyG,wehavewithprobabilitygreaterthan1−cn−10that
S
(cid:12) (cid:16) (cid:16) (cid:17) (cid:17) (cid:12)
(cid:12)P Y2 ≥λˆ(α) fˆ ◦Tˆ (X)+δ |D ∪D −α(cid:12)
(cid:12) T init 0 S T (cid:12)
(cid:114)
logn (cid:104)(cid:12) (cid:12)(cid:105)
≤C S +GL ·E (cid:12)Tˆ (X)−T (X)(cid:12) .
n F T (cid:12) 0 0 (cid:12)
S
As for Theorem 4.3, the bound obtained in Theorem 4.5 also depends on two crucial terms:
RademachercomplexityofF andestimationerrorofT . Therefore, thekeytakeawayofourthe-
0
oretical analysis is that the prediction interval obtained from Algorithm 2 asymptotically achieves
nominalcoverageguaranteeandminimalwidth. Furthermore,theapproximationerrorintrinsically
depends on the Rademacher complexity of the underlying function class and the precision in esti-
matingT .
0
Remark 4.6 (Measure preserving transformation). In our approach, T is employed to maintain
0
measuretransformation,althoughitmaynotnecessarilybeanoptimaltransportmap. Yet,estimat-
ingT canbechallenginginmanypracticalscenarios. Insuchcases,simplertransformationslike
0
linearorquadraticadjustmentsareoftenutilizedtoalignthefirstfewmomentsofthedistributions.
Various methods provide such simple solutions, including, but not limited to, CORAL (Sun et al.,
2017)andADDA(Tzengetal.,2017).
5 Application
Inthissection, weillustratetheeffectivenessofourmethodusingtheairfoildatasetfromtheUCI
Machine Learning Repository (Dua and Graff, 2019). This dataset includes 1503 observations,
featuringaresponsevariableY (scaledsoundpressurelevel)andafive-dimensionalcovariateX(log
of frequency, angle of attack, chord length, free-stream velocity, log of suction side displacement
thickness). Weassessandcomparetheperformanceofourpredictionintervalsintermsofcoverage
and width with those generated by the weighted split conformal prediction method described in
Tibshiranietal.(2019).
We use the same data-generating process described in Tibshirani et al. (2019) to facilitate a direct
comparison. We have run experiments 200 times; each time, we randomly partitioned the data
intotwopartsD andD ,whereD contains75%ofthedata,andD contains25%of
train test train test
the data. Following Tibshirani et al. (2019), we shift the distribution of the covariates of D by
test
weightedsamplingwithreplacement,wheretheweightsareproportionalto
w(x)=exp(xTβ), where β =(−1,0,0,0,1).
These reweighted observations in D , which we call D , act as observations from the target
test shift
domain. Clearly,byourdatagenerationmechanismw (x) = f (x)/f (x) = cexp(x⊤β),where
0 T S
c is the normalizing constant. The source and target domains share the same support under this
configuration. Asourmethodologyisdevelopedforunsuperviseddomainadaptation,wedonotuse
thelabelinformationofD todevelopthetargetdomain’spredictioninterval.
shift
Densityratioestimation Weusetheprobabilisticclassificationtechniquetoestimatethedensity
basedonthesourceandthetargetcovariates.LetX ,...,X bethecovariatesindatasetD and
1 n1 train
X ,...,X bethecovariatesindatasetD . Thedensityratioestimationproceedsintwo
n1+1 n1+n2 shift
steps: (1)logisticregressionisappliedtothefeature-classpairs{(X ,C )}n , whereC = 0for
i i i=1 i
i=1,...,n andC =1fori=n +1,...,n +n ,yieldinganestimateofP(C =1|X =x),
1 i 1 1 2
denoted as pˆ(x); (2) the density ratio estimator is then defined as wˆ(x) = n1 · pˆ(x) . Further
n2 1−pˆ(x)
explanationsareprovidedinAppendixB.
8Implementation of our method and results As the mean function m (x) = E[Y | X = x]
0
(whichisthesameonthesourceandthetargetdomain)isunknown,wefirstestimateitvialinear
regression,whichhenceforthwillbedenotedbymˆ(x). Toconstructapredictioninterval,wecon-
siderthemodelaggregationapproach,i.e.,thefunctionclassF isdefinedasthelinearcombination
ofthefollowingsixestimates:
(1) Estimator1(f ):Aneuralnetworkbasedestimatorwithdepth=1,width=10thatestimates
1
the0.85quantilefunctionof(Y −mˆ(X))2 |X =x.
(2) Estimator 2(f ): A fully connected feed forward neural network with depth=2 and
2
width=50thatestimatesthe0.95quantilefunctionof(Y −mˆ(X))2 |X =x.
(3) Estimator3(f ): Aquantileregressionforestestimatingthe0.9quantilefunctionof(Y −
3
mˆ(X))2 |X =x.
(4) Estimator4(f ): Agradientboostingmodelestimatingthe0.9quantilefunctionof(Y −
4
mˆ(X))2 |X =x.
(5) Estimator5(f ): AnestimateofE[(Y −mˆ(X))2 |X =x]usingrandomforest.
5
(6) Estimator6(f ): Theconstantfunction1.
6
Here, the quantile estimators are obtained by minimizing the corresponding check loss. The im-
plementation of our method is summarized as follows: (1) We divide the training data D into
train
twohalvesD ∪D . WeutilizedatasetD toderiveameanestimatorandsixaforementionedesti-
1 2 1
mates. WealsoemploythecovariatesfromD andD tocomputeadensityratioestimator. (2)
1 shift
We further split D into two equal parts D and D . D , along with covariates from D ,
2 2,1 2,2 2,1 shift
isusedtofindtheoptimalaggregationofthesixestimatestocapturetheshape, i.e., forobtaining
fˆ . The second part D is used to shrink the interval to achieve 1−α = 0.95 coverage, i.e.
init 2,2
to estimate λˆ(α). (3) We evaluate the effectiveness of our approach in terms of the coverage and
averagebandwidthontheD dataset.
shift
Wenowpresentthehistogramsofthecoverageandtheaveragebandwidthofourmethod,andamore
generalversionofweightedconformalpredictioninTibshiranietal.(2019)over200experiments
(see Appendix B for details), which show that our method consistently yields a shorter prediction
(a)Coverageofourmethod (b)Bandwidthofourmethod
(c)Coverageofweightedconformal (d)Bandwidthofweightedconformal
intervalthantheweightedconformalpredictionwhilemaintainingcoverage.Over200experiments,
theaveragecoverageachievedbyourmethodwas0.964029(SD=0.04),whiletheweightedcon-
formalpredictionmethodachievedanaveragecoverageof0.9535(SD=0.036). Additionally,the
averagewidthofthepredictionintervalsforourmethodwas13.654(SD=2.22),comparedto20.53
(SD = 4.13) for the weighted conformal prediction. Regarding the performance of intervals over
995%coverage, ourmethodachievedthisin72.5%ofcaseswithanaveragewidthof14.35(SD=
2.22).Incontrast,theweightedconformalpredictionmethoddidsoin57%ofcaseswithanaverage
widthof21.4(SD=4.39). BoxplotsarepresentedinAppendixBforfurthercomparison.
6 Conclusion
This paper focuses on unsupervised domain shift problems, where we have labeled samples from
thesourcedomainandunlabeledsamplesfromthetargetdomain. Weintroducemethodologiesfor
constructingpredictionintervalsonthetargetdomainthataredesignedtoensureadequatecoverage
whileminimizingwidth.Ouranalysisincludesscenariosinwhichthesourceandtargetdomainsare
relatedeitherthroughaboundeddensityratioorameasure-preservingtransformation.Ourproposed
methodologies are computationally efficient and easy to implement. We further establish rigorous
finite sample theoretical guarantees regarding the coverage and width of our prediction intervals.
Finally,wedemonstratethepracticaleffectivenessofourmethodologythroughitsapplicationtothe
airfoildataset.
References
Choi, K., Meng, C., Song, Y., and Ermon, S. (2022). Density ratio estimation via infinitesimal
classification. InInternationalConferenceonArtificialIntelligenceandStatistics, pages2552–
2573.PMLR.
Courty, N., Flamary, R., Habrard, A., and Rakotomamonjy, A. (2017). Joint distribution optimal
transportationfordomainadaptation. Advancesinneuralinformationprocessingsystems,30.
Courty,N.,Flamary,R.,andTuia,D.(2014).Domainadaptationwithregularizedoptimaltransport.
In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML
PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14, pages 274–289.
Springer.
Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. (2016). Optimal transport for domain
adaptation. IEEEtransactionsonpatternanalysisandmachineintelligence,39(9):1853–1865.
De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and
Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks.
IEEEtransactionsonpatternanalysisandmachineintelligence,44(7):3366–3385.
Deb,N.,Ghosal,P.,andSen,B.(2021). Ratesofestimationofoptimaltransportmapsusingplug-
in estimators via barycentric projections. Advances in Neural Information Processing Systems,
34:29736–29753.
Divol,V.,Niles-Weed,J.,andPooladian,A.-A.(2022). Optimaltransportmapestimationingeneral
functionspaces. arXivpreprintarXiv:2212.03722.
Dua,D.andGraff,C.(2019). Ucimachinelearningrepository. https://archive.ics.uci.edu.
Eastwood, C., Robey, A., Singh, S., Von Ku¨gelgen, J., Hassani, H., Pappas, G. J., and Scho¨lkopf,
B. (2022). Probable domain generalization via quantile risk minimization. Advances in Neural
InformationProcessingSystems,35:17340–17358.
Fan,J.,Ge,J.,andMukherjee,D.(2023). Utopia: Universallytrainableoptimalpredictionintervals
aggregation. arXivpreprintarXiv:2306.16549.
Farahani, A., Voghoei, S., Rasheed, K., and Arabnia, H. R. (2021). A brief review of domain
adaptation. Advances in data science and information engineering: proceedings from ICDATA
2020andIKE2020,pages877–894.
Gibbs, I.andCandes, E.(2021). Adaptiveconformalinferenceunderdistributionshift. Advances
inNeuralInformationProcessingSystems,34:1660–1672.
Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K., andScho¨lkopf, B.(2008). Co-
variateshiftbykernelmeanmatching.
10Hu, X. and Lei, J. (2023). A two-sample conditional distribution test using conformal prediction
andweightedranksum. JournaloftheAmericanStatisticalAssociation,pages1–19.
Hull, J. J. (1994). A database for handwritten text recognition research. IEEE Transactions on
patternanalysisandmachineintelligence,16(5):550–554.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to
documentrecognition. ProceedingsoftheIEEE,86(11):2278–2324.
Lei,J.,G’Sell,M.,Rinaldo,A.,Tibshirani,R.J.,andWasserman,L.(2018). Distribution-freepre-
dictiveinferenceforregression. JournaloftheAmericanStatisticalAssociation,113(523):1094–
1111.
Lei, L.andCande`s, E.J.(2021). Conformalinferenceofcounterfactualsandindividualtreatment
effects.JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,83(5):911–938.
Mai,Z.,Li,R.,Jeong,J.,Quispe,D.,Kim,H.,andSanner,S.(2022). Onlinecontinuallearningin
imageclassification: Anempiricalsurvey. Neurocomputing,469:28–51.
Makkuva,A.,Taghvaei,A.,Oh,S.,andLee,J.(2020). Optimaltransportmappingviainputconvex
neuralnetworks. InInternationalConferenceonMachineLearning,pages6672–6681.PMLR.
Maurer, A. (2016). A vector-contraction inequality for rademacher complexities. In Algorithmic
Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016,
Proceedings27,pages3–17.Springer.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. (2011). Reading digits
in natural images with unsupervised feature learning. In NIPS workshop on deep learning and
unsupervisedfeaturelearning,volume2011,page7.Granada,Spain.
Pan,S.J.andYang,Q.(2009). Asurveyontransferlearning. IEEETransactionsonknowledgeand
dataengineering,22(10):1345–1359.
Qin, J. (1998). Inferences for case-control and semiparametric two-sample density ratio models.
Biometrika,85(3):619–630.
Redko, I., Habrard, A., and Sebban, M. (2017). Theoretical analysis of domain adaptation with
optimal transport. In Machine Learning and Knowledge Discovery in Databases: European
Conference,ECMLPKDD2017,Skopje,Macedonia,September18–22,2017,Proceedings,Part
II10,pages737–753.Springer.
Seguy, V., Damodaran, B. B., Flamary, R., Courty, N., Rolet, A., and Blondel, M. (2017). Large-
scaleoptimaltransportandmappingestimation. arXivpreprintarXiv:1711.02283.
Sun,B.,Feng,J.,andSaenko,K.(2017).Correlationalignmentforunsuperviseddomainadaptation.
Domainadaptationincomputervisionapplications,pages153–171.
Tibshirani,R.J.,FoygelBarber,R.,Candes,E.,andRamdas,A.(2019).Conformalpredictionunder
covariateshift. Advancesinneuralinformationprocessingsystems,32.
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain
adaptation. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages7167–7176.
Uehara,M.,Sato,I.,Suzuki,M.,Nakayama,K.,andMatsuo,Y.(2016). Generativeadversarialnets
fromadensityratioestimationperspective. arXivpreprintarXiv:1610.02920.
Venkateswara,H.,Eusebio,J.,Chakraborty,S.,andPanchanathan,S.(2017).Deephashingnetwork
forunsuperviseddomainadaptation. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pages5018–5027.
Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., and Philip, S. Y.
(2022). Generalizingtounseendomains: Asurveyondomaingeneralization. IEEEtransactions
onknowledgeanddataengineering,35(8):8052–8072.
11Weiss,K.,Khoshgoftaar,T.M.,andWang,D.(2016). Asurveyoftransferlearning. JournalofBig
data,3:1–40.
Xu, R., Liu, P., Wang, L., Chen, C., andWang, J.(2020). Reliableweightedoptimaltransportfor
unsuperviseddomainadaptation.InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages4394–4403.
Yang, Y., Kuchibhotla, A.K., andTchetgen, E.T.(2022). Doublyrobustcalibrationofprediction
setsundercovariateshift. arXivpreprintarXiv:2203.01761.
Zhang,Y.andYang,Q.(2021). Asurveyonmulti-tasklearning. IEEETransactionsonKnowledge
andDataEngineering,34(12):5586–5609.
Zhou, K., Liu, Z., Qiao, Y., Xiang, T., and Loy, C. C. (2022). Domain generalization: A survey.
IEEETransactionsonPatternAnalysisandMachineIntelligence,45(4):4396–4415.
12A Proofs
A.1 ProofofTheorem3.2
First,weshowthatforourchoiceof(ϵ,δ),asdepictedinTheorem3.2,f∗ isafeasiblesolutionof
equation(3.5). Considerw insteadofwˆ. Bydefinitionoff∗,
0
P (Y2 ≤f∗(X))=1 ⇐⇒ E (cid:2) w (X)1 (cid:3) =0 ⇐⇒ w (X)1 =0 a.s. onsource.
T S 0 Y2>f∗(X) 0 Y2>f∗(X)
Thisimplies:
1 (cid:88) w (X )h (cid:0) Y2−f⋆(X )(cid:1)
n /2 0 i δ i i
S
i∈DS,1
= 1 (cid:88) w (X )h (cid:0) Y2−f⋆(X )(cid:1) 1
n S/2 0 i δ i i Y i2≤f⋆(Xi)
i∈DS,1
= 1 (cid:88) w (X )h (cid:0) Y2−f⋆(X )(cid:1) 1
n S/2 0 i δ i i f⋆(Xi)−δ≤Y i2≤f⋆(Xi)
i∈DS,1
1 (cid:88)
≤ w (X )1 ,
n S/2 0 i f⋆(Xi)−δ≤Y i2≤f⋆(Xi)
i∈DS,1
where the first equality follows from the fact that w (X)1 = 0 a.s. on the source do-
0 Y2>f⋆(X)
main, the second equality follows from the fact that h (t)1 = 0 for all t, and the last in-
δ t<−δ
equality follows from the fact that h (Y2 − f⋆(X )) ≤ 1 when Y2 − f⋆(X ) ≤ 0. Since
δ i i i i
w (X)1 ≤ W, by Hoeffding’s inequality, we have with probability at least
0 f⋆(X)−δ≤Y2≤f⋆(X)
1−e−t:
(cid:114)
1 (cid:88) w (X )h (cid:0) Y2−f⋆(X )(cid:1) ≤E (cid:2) w (X)1 (cid:3) +W t
n /2 0 i δ i i S 0 f⋆(X)−δ≤Y2≤f⋆(X) n
S S
i∈DS,1
(cid:114)
=P (cid:0) f⋆(X)−δ ≤Y2 ≤f⋆(X)(cid:1) +W t
T n
S
(cid:114)
t
≤Lδ+W ,
n
S
where L is upper bound on the density of Y2 −f∗(X). Call this event Ω that the above bound
1
holds. Atthiseventwehave:
1 (cid:88) wˆ(X )h (cid:0) Y2−f⋆(X )(cid:1)
n /2 i δ i i
S
i∈DS,1
= 1 (cid:88) w (X )h (cid:0) Y2−f⋆(X )(cid:1) + 1 (cid:88) (wˆ(X )−w (X ))h (cid:0) Y2−f⋆(X )(cid:1)
n /2 0 i δ i i n /2 i 0 i δ i i
S S
i∈DS,1 i∈DS,1
(cid:114)
t B+δ 2
n (cid:88)S/2
≤Lδ+W + · |wˆ(X )−w (X )|,
n δ n i 0 i
S S
i=1
wherethelastinequalityfollowsfromthefactthath (t)≤(B+δ)/δift≤B.Finally,toboundthe
δ
lastsummand, weagainapplyHoeffding’sinequality. As∥wˆ∥ ≤ W′, wehavewithprobability
∞
greaterthanorequalto1−e−t:
1
n (cid:88)S/2 (cid:114)
t
|wˆ(X )−w (X )|≤E [|wˆ(X)−w (X)|]+(W +W′) .
n /2 i 0 i S 0 n
S S
i=1
IfwedenotetheeventΩ wheretheaboveinequalityholds,thenontheeventΩ ∩Ω ,wehave:
2 1 2
1 (cid:88) wˆ(X )h (cid:0) Y2−f⋆(X )(cid:1)
n /2 i δ i i
S
i
(cid:114) (cid:18) (cid:114) (cid:19)
t B+δ t
≤Lδ+W + · E [|wˆ(X)−w (X)|]+(W +W′) ≤ϵ.
n δ S 0 n
S S
13Furthermore,
P(Ω ∩Ω )≥P(Ω )+P(Ω )−1≥1−2e−t.
1 2 1 2
Therefore,weconcludethatwithprobability≥1−2e−t,f∗isafeasiblesolution.
We now proof Theorem 2.2 on the event Ω ∩Ω , when f∗ is a feasible solution. Then we have,
1 2
P (fˆ (X))≤P (f∗(X))onthisevent,bytheoptimalityoffˆ inequation(3.5). Thenwe
n,T init n,T init
have:
E [fˆ (X)]=P (fˆ (X))+(P −P )(fˆ (X))
T init nT init T nT init
≤P (f∗(X))+(P −P )(fˆ (X))
nT T nT init
=E [f∗(X)]+(P −P )(f∗(X)−fˆ (X))
T nT T init
≤E [f∗(X)]+sup|(P −P )(f∗(X)−f(X))|
T nT T
f∈F
Finallyasf −f∗isupperboundedbyF′ =B +∥f∗∥ (asf isuniformlyupperboundedbyF).
F ∞
Therefore,byMcdiarmid’sinequality,wewithhavewithprobability1−et:
(cid:34) (cid:35) (cid:114)
t
sup|(P −P )(f∗(X)−f(X))|≤E sup|(P −P )(f∗(X)−f(X))| +F′ .
nT T T nT T 2n
f∈F f∈F T
CallthiseventΩ . Furthermore,bystandardsymmetrization:
3
(cid:34) (cid:35)
E sup|(P −P )(f∗(X)−f(X))| ≤2R (F −f∗),
T nT T nT
f∈F
whereR (F −f∗)istheRademachercomplexityofF −f∗. Therefore,on∩3 Ω ,wehave:
nT i=1 i
(cid:114)
t
E [fˆ (X)]≤E [f∗(X)]+2R (F −f∗)+F′ ,
T init T nT 2n
T
andP(∩3 Ω )≥1−3e−t. Thiscompletestheproof.
i=1 i
A.2 ProofofLemma3.3
We prove the lemma into two steps; first we show that fˆ satisfies P (Y2 > fˆ (X)) ≤ τ
init T init
with high probability for some small τ. Next we argue that, on D , we have
S,2
(2/n ) · (cid:80) wˆ(X )1(Y2 ≥ fˆ (X )) ≤ τˇ with high probability for some small τˇ.
S i∈DS,2 i i init i
Thenaslongasτˇ≤α,weconcludetheproofofthelemma.
Step1: Notethat,byfeasibility,fˆ satisfies:
init
1 (cid:88) wˆ(X )h (Y2−fˆ (X ))≤ϵ.
n /2 i δ i init i
S
i∈DS,1
Thisimplies:
(cid:104) (cid:16) (cid:17)(cid:105)
E h Y2−fˆ (X)
T δ init
(cid:104) (cid:16) (cid:17)(cid:105)
=E w (X)h Y2−fˆ (X)
S 0 δ init
= 1 (cid:88) w (X )h (Y2−fˆ (X ))+(cid:0)P −P (cid:1) w (X)h (Y2−fˆ (X))
n /2 0 i δ S init i S nS/2 0 δ init
S
i∈DS,1
= 1 (cid:88) wˆ(X )h (Y2−fˆ (X ))+ 1 (cid:88) (w (X )−wˆ(X ))h (Y2−fˆ (X ))
n /2 i δ i init i n /2 0 i i δ i init i
S S
i∈DS,1 i∈DS,1
+(cid:0)P −P (cid:1) w (X)h (Y2−fˆ (X))
S nS/2 0 δ init
≤ϵ+ B δ+δ ∥wˆ−w 0∥ L1(P n1,S)+ fsu ∈Fp(cid:12) (cid:12)(cid:0)P S −P nS/2(cid:1) w 0(X)h δ(Y2−f(X))(cid:12) (cid:12)
14Now, as h (Y2 −f(X)) ≤ (B +δ)/δ and w ≤ W, we have by Mcdiarmid’s inequality, with
δ 0
probability≥1−e−t:
sup(cid:12) (cid:12)(cid:0)P S −P nS/2(cid:1) w 0(X)h δ(Y2−f(X))(cid:12) (cid:12)
f∈F
(cid:34) (cid:35) (cid:114)
≤E S sup(cid:12) (cid:12)(cid:0)P S −P nS/2(cid:1) w 0(X)h δ(Y2−f(X))(cid:12) (cid:12) +WB δ+δ nt
f∈F S
(cid:114)
B+δ t
≤2R (w h ◦f)+W .
nS/2,F 0 δ δ n
S
Meanwhile,asintheproofofTheorem3.2,withprobability≥1−e−t:
(cid:114)
t
∥wˆ−w 0∥ L1(P n1,S) ≤E S[|wˆ(X)−w(X)|]+(W +W′) n S.
Choosingt=10logn weobtainthatwithprobability≥1−2n−10:
S S
(cid:16) (cid:16) (cid:17)(cid:17)
E h Y2−fˆ (X )
T δ T init T
(cid:32) (cid:114) (cid:33)
B+δ 10logn
≤ϵ+ E [|wˆ(X)−w (X)|]+(W +W′) S
δ S 0 n
S
(cid:114)
B+δ 10logn
+2R (w h ◦f)+W S
nS/2,F 0 δ δ n
S
(cid:32) (cid:114) (cid:33)
B+δ 10logn
≤ϵ+ E [|wˆ(X)−w (X)|]+(2W +W′) S +2R (w h ◦f).
δ S 0 n nS/2,F 0 δ
S
We next bound the Rademacher complexity of R (w h ◦f). By symmetrization, we have
nS/2,F 0 δ
withζ ,...ζ i.i.d. Rademacher(1/2):
1 nS/2
(cid:34) (cid:12) (cid:12)(cid:35)
(cid:12) 1 (cid:88) (cid:12)
R (w h ◦f)=2E sup(cid:12) ζ w (X )h (Y2−f(X ))(cid:12)
nS/2,F 0 δ S (cid:12)n /2 i 0 i δ i i (cid:12)
f∈F(cid:12) S (cid:12)
i
(cid:34) (cid:12) (cid:12)(cid:35)
=2E sup(cid:12) (cid:12) 1 (cid:88) ζ ϕ(cid:0) w (X ),Y2−f(X )(cid:1)(cid:12) (cid:12) [ϕ(x,y)=xh (y)]
S (cid:12)n /2 i 0 i i i (cid:12) δ
f∈F(cid:12) S (cid:12)
i
We first show that ϕ : R2 → R is a Lipschitz function on its domain. The first argument of ϕ is
w (x)whichlieswithin[−W,W].ThesecondargumentofϕisY2−f(X)(onthesourcedomain),
0
whichisboundedbyB. Therefore,h (Y2−f(X))isboundedaboveby(B+δ)/δ. Thederivative
δ
ofh is0forx≤−δandδforx≥−δ. Hence,wehavethefollowing:
δ
(cid:114)
(B+δ)2 W2 B+W +δ
∥∇ϕ(x,y)∥=∥(h (y) xh′(y))∥≤ + ≤ .
δ δ δ2 δ2 δ
Wenextapplyvector-valuedLeduox-Talagrandcontractioninequalityonthefunctionϕ(equation
(1)ofMaurer(2016)),toobtainthefollowingboundontheRademachercomplexity:
(cid:34) (cid:12) (cid:12)(cid:35)
2E sup(cid:12) (cid:12) 1 (cid:88) ζ ϕ(cid:0) w (X ),Y2−f(X )(cid:1)(cid:12) (cid:12)
S (cid:12)n /2 i 0 i i i (cid:12)
f∈F(cid:12) S (cid:12)
i
≤2√ 2(cid:18) B+W +δ(cid:19)
E
(cid:34) sup(cid:12) (cid:12)
(cid:12)
1 (cid:88)(cid:0)
ζ w (X )+ζ (Y2−f(X
))(cid:1)(cid:12) (cid:12) (cid:12)(cid:35)
δ S (cid:12)n /2 i1 0 i i2 i i (cid:12)
f∈F(cid:12) S (cid:12)
i
≤2√ 2(cid:18) B+W δ +δ(cid:19) E S(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n S1 /2(cid:88)
i
ζ i1w 0(X i)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:35) + E S (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n S1 /2 i∈(cid:88) DS,1ζ i,2Y i2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) R nS/2(F) 
≤2√ 2(cid:18) B+W +δ(cid:19)(cid:34) ∥w 0∥ L2(PXS) +(cid:115) E S[Y4]
+R
(F)(cid:35)
δ (cid:112) n S/2 n S/2 nS/2
15Usingthis,weobtainthefollowing:
(cid:16) (cid:16) (cid:17)(cid:17)
E h Y2−fˆ (X)
T δ init
(cid:32) (cid:115) (cid:33)
B+δ 5log(n /2)
≤ϵ+ E [|wˆ(X)−w (X)|]+(2W +W′) S
δ S 0 n /2
S
+4√ 2(cid:18) B+W +δ(cid:19)(cid:34) ∥w 0∥ L2(PX√S)+(cid:112)E S[Y4]
+ R
(F)(cid:35)
δ n nS/2
S
√ (cid:18) B+W +δ(cid:19)(cid:34) (cid:115) 5log(n /2)
≤ϵ+4 2 E[|wˆ(X )−w(X )|]+(2W +W′) S
δ S S n /2
S
∥w ∥
+(cid:112)E
[Y4]
(cid:35)
+
0 L2(PXS) S
+ R (F)
(cid:112)
n /2
nS/2
S
√ (cid:18) B+W +δ(cid:19)(cid:34) (cid:115) 5log(n /2) W +(cid:112)E [Y4] (cid:35)
≤ϵ+4 2 E[|wˆ(X )−w(X )|]+(2W +W′) S + S + R (F)
δ S S n S/2 (cid:112) n S/2 nS/2
Choosing
(cid:115) (cid:32) (cid:115) (cid:33)
5log(n /2) B+δ 5log(n /2)
ϵ=Lδ+W S + · E [|wˆ(X)−w (X)|]+(W +W′) S ,
n /2 δ S 0 n /2
S S
weobtain
(cid:16) (cid:16) (cid:17)(cid:17)
E h Y2−fˆ (X)
T δ init
(cid:32) (cid:114) (cid:33)
B+W +δ 5logn
≲Lδ+ · (E [|wˆ(X)−w (X)|]+(W +W′) S +R (F)
δ S 0 n nS/2
S
(cid:118)
(cid:117) (cid:32) (cid:114) (cid:33)
≲(cid:117) (cid:116)L(B+W) (E [|wˆ(X)−w (X)|]+(W +W′) 5logn S +R (F)
S 0 n nS/2
S
(cid:114)
5logn
+(E [|wˆ(X)−w (X)|]+(W +W′) S +R (F)
S 0 n nS/2
S
(bychoosingδtobalancetheterms)
≜τ
CalltheaboveeventΩ . ThiscompletestheproofofStep1.
1
Step2: ComingbacktoD ,wehave:
S,2
1 (cid:88) 1 (cid:88) 1 (cid:88)
wˆ(X )1 ≤ |wˆ(X )−w (X )|+ w (X )1
n S/2 S,i Y i2>fˆ init(Xi) n S/2 i 0 i n S/2 0 i Y i2>fˆ init(Xi)
i∈DS,2 i∈DS,2 i∈DS,2
Furthermore,byHoeffding’sinequality,wehavewithprobability≥1−e−t:
(cid:114)
1 (cid:88) (cid:104) (cid:105) t
w (X )1 ≤E w (X)1 +W
n S/2 0 i Y i2>fˆ init(Xi) S 0 Y2>fˆ init(X) n
S
i∈D2
(cid:114)
(cid:104) (cid:16) (cid:17)(cid:105) t
≤E w (X)h Y2−fˆ (X) +W
S 0 δ init n
S
(cid:114)
(cid:16) (cid:16) (cid:17)(cid:17) t
=E h Y2−fˆ (X) +W
T δ init n
S
Meanwhile,withprobability≥1−e−t:
(cid:114)
1 (cid:88) t
|wˆ(X )−w (X )|≤E [|wˆ(X)−w (X)|]+(W +W′) .
n /2 i 0 i S 0 n
S S
i∈DS,2
16Therefore,witht=10logn ,wehavewithprobability≥1−2n−10:
S S
(cid:114)
1 (cid:88) wˆ(X )1 ≤E [|wˆ(X)−w (X)|]+(W +W′) 10logn S
n S/2 i Y i2>fˆ init(Xi) S 0 n
S
i∈DS,2
(cid:114)
(cid:16) (cid:16) (cid:17)(cid:17) 10logn
+E h Y2−fˆ (X) +W S .
T δ init n
S
CallthiseventΩ . Therefore,onΩ ∩Ω wehave:
2 1 2
(cid:114)
1 (cid:88) wˆ(X )1 ≤E [|wˆ(X)−w (X)|]+(2W +W′) 10logn S +τ ≜τ˜.
n S/2 i Y i2>fˆ init(Xi) S 0 n
S
i∈DS,2
This completes the proof of Step 2. For any fixed α > 0, we have τ˜ ≤ α as long as n is large
S
enoughandE [|wˆ(X)−w (X)|]issmallenough,andasaconsequenceλˆ(α)≤1.Thiscompletes
S 0
theproof.
A.3 ProofofTheorem3.4
Recallthatweconstructthepredictionintervalsusingdatasplitting; fromthefirstpartofthedata
(namely D ), we estimate fˆ and use the second part of the data (namely D ) to estimate λˆ(α).
1 init 2
ConditionalonD ,defineafunctionclassG ≡G(fˆ)as:
1
(cid:110) (cid:111)
G = g (x,y)=w (x)1 :λ≥0 .
λ 0 y2−λfˆ init(x)≥0
AsG onlydependsonascalarparameterλ(asw andfˆ arefixedconditionallyonD ,D ),it
0 init S,1 T
isaVCclassoffunctionwithVC-dim≤2.
(cid:16) (cid:17)
P Y2 ≥λˆ(α)fˆ (X)
T init
(cid:104) (cid:105)
=E w (X)1
S 0 Y2−λˆ(α)fˆ init(X)≥0
1 (cid:88)
= w (X )1 +(P −P )w (X)1
n S/2 0 i Y i2−λˆ(α)fˆ init(Xi) S nS/2 0 Y2≥λˆ(α)fˆ init(X)
i∈DS,2
1 (cid:88) 1 (cid:88)
= wˆ(X )1 + (w (X )−wˆ(X ))1
n S/2 i Y i2−λˆ(α)fˆ init(Xi)≥0 n S/2 0 i i Y i2−λˆ(α)fˆ init(Xi)≥0
i∈DS,2 i∈DS,2
+(P −P )w (X)1 (A.1)
S nS/2 0 Y2−λˆ(α)fˆ init(X)≥0
Now,bythedefinitionofλˆ(α)(seeStep2),wehave:
1 1 (cid:88)
α− ≤ wˆ(X )1 ≤α.
n S/2 n S/2 i Y i2−λˆ(α)fˆ init(Xi)≥0
i∈DS,2
WeuseasimilartechniquetocontrolthesecondsummandasintheproofofTheorem3.2. Byusing
thefactthattheindicatorfunctionislessthanone,wehave:
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)n S1 /2 i∈(cid:88) DS,2(w 0(X i)−wˆ(X i))1 Y i2−λˆ(α)fˆ init(Xi)≥0(cid:12) (cid:12) (cid:12) (cid:12)≤ n S1 /2 i∈(cid:88) DS,2|wˆ(X i)−w 0(X i)|.
ApplyingHoeffding’sinequality(withthefactthat∥wˆ∥ ≤ W′ and∥w ∥ ≤ W),wehavewith
∞ 0 ∞
probabilitygreaterthanorequalto1−e−t:
(cid:114)
1 (cid:88) t
|wˆ(X )−w (X )|≤E [|wˆ(X)−w(X)|]+(W +W′) .
n /2 i 0 i S n
S S
i∈DS,2
Tocontrolthethirdsummandof(A.1),notethat,conditionalonD andD (i.e.,assumingfˆ
S,1 T init
fixed), and using the fact that ∥g∥ ≤ ∥w ∥ ≤ W for all g ∈ G, we have by Mcdiarmid’s
∞ 0 ∞
17inequalitywithprobabilitygreaterthanorequalto1−e−t:
(cid:20) (cid:21) (cid:114)
sup(cid:12) (cid:12)(P
S
−P nS/2)g(X,Y)(cid:12) (cid:12)≤E
S
sup(cid:12) (cid:12)(P
S
−P nS/2)g(X,Y)(cid:12) (cid:12)|D S,1,D
T
+W nt
g∈G g∈G S
(cid:114)
t
≤2R (G |D ,D )+W .
nS/2 S,1 T n
S
NowconditionalonD ,D ,G isaVCclassoffunctionwithVCdimension≤2. Therefore,
S,1 T
(cid:114)
C
R (G |D ,D )≤
nS/2 S,1 T n
S
forsomeconstantC >0. Thus,wehave
(cid:114) (cid:114)
sup(cid:12) (cid:12)(P
S
−P nS/2)g(X,Y)(cid:12) (cid:12)≤ nC +W nt .
g∈G S S
Combiningthebounds,wehave,withprobability≥1−2e−t:
(cid:12) (cid:16) (cid:17) (cid:12)
(cid:12)P Y2 >λˆ(α)fˆ (X) −α(cid:12)
(cid:12) T init (cid:12)
(cid:114) (cid:114)
1 t C
≤ +E [|wˆ(X)−w (X)|]+(2W +W′) + .
n /2 S 0 n n
S S S
Thiscompletestheproof.
A.4 ProofofTheorem4.3
Westartwiththefollowingdecomposition:
E [fˆ ◦Tˆ (X)]=E [fˆ ◦T (X)]+E [fˆ ◦Tˆ (X)−fˆ ◦T (X)]
T init 0 T init 0 T init 0 init 0
=E [fˆ (X)]+E [fˆ ◦Tˆ (X)−fˆ ◦T (X)]
S init T init 0 init 0
≤E [fˆ (X)]+L E [|Tˆ (X)−T (X)|]
S init F T 0 0
wherethesecondequationfollowsfromthefactthatwhenX ∼P ,thenT (X)∼P ,andthelast
T 0 S
linefollowsfromthefactf ∈ F isL Lipschitz. AsimilarargumentasintheproofofTheorem
F
3.5(Fanetal.,2023)yields:
(cid:114)
t
E [fˆ (X)]≤∆+4R (F)+4B .
S init nS F 2n
S
withprobability≥1−e−t. Wethenfinishtheproofs.
A.5 ProofofLemma4.4
Bythedefinitionofλˆ(α),wehave
 
(cid:110) λˆ(α)≥1(cid:111) =⇒  1 (cid:88) 1(cid:16) Y2 ≥fˆ (X )+δ(cid:17) >α .
n /2 i init i
 S 
i∈DS,2
NowbyanapplicationofChernoffboundforbinomialdistribution,wehave:
 
P  n1
/2
(cid:88) 1(cid:16) Y i2 ≥fˆ init(X i)+δ(cid:17) >α|D S,1,D T≤e−(α− 6p pn nS S)2nS .
S
i∈DS,2
Hence,wehavethefollowing:
P(λˆ(α)>1|D S,1,D T)≤e−(α− 6p pn nS S)2nS .
18Wenextestablishthehighprobabilityboundonp . Wedefineafunctionℓ (x)whichis1when
nS δ
x≤−δ,0whenx≥0and−x/δwhen−δ ≤x≤0.
(cid:104) (cid:105) (cid:104) (cid:105)
p =E 1 ≤E ℓ (fˆ (X)−Y2)
nS S Y2≥fˆ init(X)+δ S δ init
= 1 (cid:88) ℓ (fˆ (X )−Y2)+(cid:0)P −P (cid:1) ℓ (fˆ (X)−Y2)
n /2 δ init i i nS/2 S δ init
S
i∈DS,1
≤ sup(cid:0)P −P (cid:1) ℓ (f(X)−Y2)
nS/2 S δ
f∈F
(cid:115) 
4 E [Y4] (cid:114) t
≤
δ
 S
n
+R nS/2(F)+
n
.
S S
wherethefirstinequalityusedℓ (x)≥1(x≤−δ),secondinequalityusesthefactthatsampleaver-
δ
ageofℓ overD is0bythedefinitionoffˆ ,thirdinequalityusesLeduox-Talagrandcontraction
δ S,1 init
inequalityobservingthatℓ is1/δ-Lipschitz. Thiscompletestheproof.
δ
A.6 ProofofTheorem4.5
(cid:16) (cid:17)
P Y2 ≥λˆ(α)(fˆ ◦Tˆ (X)+δ)
T init 0
(cid:16) (cid:17)
=P Y2 ≥λˆ(α)(fˆ ◦T (X)+δ)
T init 0
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)(cid:12)
+(cid:12)P Y2 ≥λˆ(α)(fˆ ◦Tˆ (X)+δ) −P Y2 ≥λˆ(α)(fˆ ◦T (X)+δ) (cid:12)
(cid:12) T init 0 T init 0 (cid:12)
≜T +T . (A.2)
1 2
Westartwithanalyzingthefirstterm:
(cid:16) (cid:17)
T =P Y2 ≥λˆ(α)(fˆ ◦T (X)+δ)
1 T init 0
(cid:90) (cid:90)
= 1 f (y |X =x)p (x)dydx
y2≥λˆ(α)(fˆ init(T0(x))+δ) T T T
XT Y
(cid:90) (cid:90)
= 1 f (y |X =T (x))p (x)dydx
y2≥λˆ(α)(fˆ init(T0(x))+δ) S S 0 T
XT Y
(cid:90) (cid:90)
= 1 f (y |X =z)p (T−1(z))|∇T−1(z)|dydx
y2≥λˆ(α)(fˆ init(z)+δ) S S T 0 0
XS Y
(cid:90) (cid:90)
= 1 f (y |X =z)p (z)dydx
y2≥λˆ(α)(fˆ init(z)+δ) S S S
XS Y
=P (Y2 ≥λˆ(α)(fˆ (X)+δ)).
S init
Therefore,weneedahighprobabilityupperboundonP (Y2 ≥ λˆ(α)(fˆ (X)+δ) | D ∪D ).
S init S T
Towardsthatend,westartwiththefollowingexpansion:
(cid:16) (cid:17)
P Y2 ≥λˆ(α)(fˆ (X)+δ)|D ∪D
S init S T
= 1 (cid:88) 1 +(cid:0)P −P (cid:1)1 (A.3)
n S/2 Y i2≥λˆ(α)(fˆ init(Xi)+δ) nS/2 S Y2≥λˆ(α)(fˆ init(X)+δ)
i∈DS,2
Now,notethat,bythedefinitionofλˆ(α),wehave:
1 1 (cid:88)
α− ≤ 1 ≤α.
n S/2 n S/2 Y i2≥λˆ(α)(fˆ init(Xi)+δ)
i∈DS,2
Toboundthesecondtermin(A.3),weuse:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:0)P −P (cid:1)1 (cid:12)≤sup(cid:12)(cid:0)P −P (cid:1)1 (cid:12):=Z .
(cid:12) nS/2 S Y2≥λˆ(α)(fˆ init(X)+δ)(cid:12) λ≥0(cid:12) nS/2 S Y2≥λ(fˆ init(X)+δ)(cid:12) n
19To bound the supremum we use standard techniques from the empirical process theory. Define a
(cid:110) (cid:111)
collectionoffunctionsG = 1 :λ≥0 . Notethat,hereweconditiononD ,so
Y2≥λ(fˆ init(X)+δ) S,1
wetreatfˆ asaconstantfunction. Fornotationalsimplicity,suppose
init
Ψ
n
=E S(cid:20) s λu ≥p 0(cid:12) (cid:12) (cid:12)(cid:0)P nS/2−P S(cid:1)1
Y2≥λ(fˆ
init(X)+δ)(cid:12) (cid:12) (cid:12)|D S,1(cid:21) =E S(cid:20) s gu ∈p G(cid:12) (cid:12)(cid:0)P nS/2−P S(cid:1) g(X,Y)(cid:12) (cid:12)|D S,1(cid:21) .
AsthefunctionsinG areuniformlyboundedby1(andconsequently, E[g2(X,Y)] ≤ 1), wehave
byTalagrand’sconcentrationinequalityofthesupremaoftheempiricalprocess:
(cid:32) (cid:114) (cid:33)
1+4Ψ 4t
P Z ≥Ψ + 2t n + |D ≤e−t. (A.4)
n n n 3n S,1
S S
Therefore,weneedanupperboundonΨ toobtainahighprobabilityupperboundonZ . Towards
n n
that end, observe that G is a VC class with VC-dim less than or equal to 2 (as it is an indicator
functionofacollectionoffunctionswithoneparameter). Hence,wehave,bysymmetrizationand
Dudley’smetricentropybound:
 (cid:12) (cid:12) 
(cid:12) (cid:12)
Ψ n ≤2E Ssup(cid:12) (cid:12) (cid:12)n1 /2 (cid:88) ϵ ig(X i,Y i)(cid:12) (cid:12) (cid:12)|D S,1≤ √C n .
g∈G(cid:12) S i∈DS,2 (cid:12) S
Therefore,goingbackto(A.4),wehavewithprobability≥1−e−t
(cid:115)
C C C √ 4t
Z ≤ √ + 1 + 2 t+ .
n n
S
n
S
n3/2 3n
S
S
Hence,wehave:
(cid:114)
(cid:12) (cid:16) (cid:17) (cid:12) t
(cid:12)P Y2 ≥λˆ(α)(fˆ (X)+δ)|D ∪D −α(cid:12)≲
(cid:12) S init S T (cid:12) n
S
withprobability≥1−e−t. ThiscompletestheproofofT . ToobtainaboundonT ,notethat:
1 2
T
2
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)(cid:12)
=(cid:12)P Y2 ≥λˆ(α)(fˆ ◦Tˆ (X)+δ) −P Y2 ≥λˆ(α)(fˆ ◦T (X)+δ) (cid:12)
(cid:12) T init 0 T init 0 (cid:12)
=(cid:12) (cid:12) (cid:12)(cid:90) (cid:16) P T(Y2 ≤λˆ(α)(fˆ init(Tˆ 0(x))+δ)|X
T
=x)
(cid:12)
XT
(cid:17) (cid:12)
−P (Y2 ≤λˆ(α)(fˆ (T (x))+δ)|X =x) p (x)dx(cid:12)
T init 0 T T (cid:12)
=(cid:12) (cid:12) (cid:12) (cid:12)(cid:90)
XT
(cid:16) F
Y
T2|XT=x(λˆ(α)(fˆ init(Tˆ 0(x))+δ))−F
Y
T2|XT=x(λˆ(α)(fˆ init(T 0(x))+δ)(cid:17) p T(x)dx(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:90) (cid:12) (cid:12)
≤G λˆ(α)(cid:12)fˆ (T (x))−fˆ (Tˆ (x))(cid:12) p (x)dx
(cid:12) init 0 init 0 (cid:12) T
XT
≤GL E [|T (X)−Tˆ (X)|].
F T 0 0
Here, the penultimate inequality uses the fact that the conditional distribution of Y2 given X is
T T
Lipschitz(asthedensityofY2 givenX isbounded),andthelastinequalityusesthefactthatfˆ
T T init
isLipschitzaswehaveassumedallfunctionsinF areLipschitz.
B Detailsoftheexperiment
B.1 Densityratioestimationviaprobabilisticclassification
Suppose we observe {X ,...,X } from a distribution P (with density p) and
1 n1
{X ,...,X } from another distribution Q (with density q). We are interested in es-
n1+1 n1+n2
timating w (x) = q(x)/p(x), where we assume Q is absolutely continuous with respect to P
0
20(otherwise, the density ratio can be unbounded with positive probability). Define, n +n mane
1 2
binary random variables {C ,...,C } such that C = 0 for 1 ≤ i ≤ n and C = 1 for
1 n1+n2 i 1 i
n +1 ≤ i ≤ n +n . ConsidertheaugmenteddatasetD = {(X ,C )} . Wecanthink
th1 atthisdataseti1 sgene2 ratedfromamixturedistributionρp(X)+(1i −ρi )q(1 x≤ )i≤ wn h1 e+ rn e2 ρ=P(C =1).
Forthismixturedistribution,theposteriordistributionofC givenX is:
P(X =x|C =1)P(C =1)
P(C =1|X =x)=
P(X =x|C =1)P(C =1)+P(X =x|C =0)P(C =0)
ρq(x)
=
ρq(x)+(1−ρ)p(x)
(ρ/(1−ρ))w (x)
= 0
(ρ/(1−ρ))w (x)+1
0
Thisimplies:
1−ρ P(C =1|X =x)
w (x)= .
0 ρ 1−P(C =1|X =x)
Now,fromthedata,wecanestimateρˆ=n /(n +n )andP(C =1|X =x)byanyclassification
2 1 2
technique (e.g., using logistic regression, boosting, random forest, deep neural networks etc). Let
gˆ(x)beonesuchclassifier. Thenwecanestimatew (x)by(n /n )(gˆ(x)/(1−gˆ(x))).
0 1 2
B.2 Generalweightedconformalprediction
Theweightedconformalpredictionmethod,aspresentedinTibshiranietal.(2019),consistsoftwo
mainsteps:
1. Splitthesourcedataintoparts;estimatetheconditionalmeanfunctionE[Y |X =x],say
µˆ(x)usingthefirstpartofthesourcedata.
2. Use thesecond partof the sourcedata andthe target datato constructweight w(X ) and
i
thescorefunctionS(x,y)=|y−µˆ(x)|toconstructtheconfidenceinterval.
InSection5,wehaveimplementedageneralizedversionofit,wherewemodifythescorefunction
asfollows:
(cid:112)
1. Weestimatetheconditionalstandarddeviationfunction var(Y |X =x)alongwiththe
conditionalmeanfunctionfromthefirstpartofthedata. Callitσˆ(x).
2. Weusethemodifiedscorefunctions(x,y)=|y−µˆ(x)|/σˆ(x).
TherestofthemethodisthesameasTibshiranietal.(2019). Thisadditionalestimatedconditional
variancefunctionallowsmoreexpressivityandflexibilitytotheconformalpredictionband,asob-
servedinSection5.2ofLeietal.(2018),asthiscapturesthelocalheterogeneityoftheconditional
distributionofY givenX.
B.3 Boxplotstocomparecoverageandbandwidth
Inthissubsection,wepresenttwoboxplotstocomparethevariationincoverageandaveragewidth
ofthepredictionbandsbetweenourmethodandthegeneralizedweightedconformalprediction(as
describedintheprevioussubsection).
(a)AverageBandwidth (b)Coverage
21The boxplots immediately show that our methods yield similar coverage (even with lesser vari-
ability)withsignificantlyloweraveragewidththanthegeneralizedweightedconformalprediction
method.
22