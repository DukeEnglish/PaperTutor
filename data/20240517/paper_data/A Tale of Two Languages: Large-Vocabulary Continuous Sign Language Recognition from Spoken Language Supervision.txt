ATaleofTwoLanguages: Large-VocabularyContinuous
SignLanguageRecognitionfromSpokenLanguageSupervision
CharlesRaude1,2* PrajwalKR2∗ LilianeMomeni2∗ HannahBull1 SamuelAlbanie3
AndrewZisserman2 GülVarol1,2
1LIGM,ÉcoledesPonts,UnivGustaveEiffel,CNRS,France
2VisualGeometryGroup,UniversityofOxford,UK
3DepartmentofEngineering,UniversityofCambridge,UK
https://imagine.enpc.fr/~varolg/cslr2/
CVPRA2b4st rTaecatser Figure (v2) Input: Sign language video clip
CSLR2.
Inthiswork,ourgoalsaretwofold:large-vocabularycon-
tinuoussignlanguagerecognition(CSLR),andsignlanguage Task 1: CSLR
retrieval.Tothisend,weintroduceamulti-taskTransformer forest tree wake early see animal see wow
model,CSLR2,thatisabletoingestasigningsequenceand dicW tioo nrd ary Task 2: Sentence retrieval
outputinajointembeddingspacebetweensignedlanguage 1. Forests are incredible habitats and, if you're up early enough, there's a good
chance you'll see some of the animals.
and spoken language text. To enable CSLR evaluation in 2. And so conifers are actually amongst our best habitats for many types of animal.
thelarge-vocabularysetting,weintroducenewdatasetan- Gallery of 3. What is the point in bringing back an animal whose habitat is destroyed?
sentences
notationsthathavebeenmanuallycollected.Theseprovide Figure1. CSLR2model:Weillustrateourmulti-taskmodelthat
continuoussign-levelannotationsforsixhoursoftestvideos,
Top ranked spoken language sentences performsbothCSLRandsentenceRetrieval,thankstoitsjointem-
andwillbemadepubliclyavailable.Wedemonstratethatby
beddingspacebetweensignedlanguageandspokenlanguagetext.
acarefulchoiceoflPoresdisctefd usignn sceqtuieoncnes,trainingthemodelforboth
theCSLRandretrievfaorelsttasksistrmeeutuallqyuicbkenleoofikcialinantimearlms see wow
Ourgoalinthispaperistwo-fold: first,toenablelarge-
ofperformance–retrievalimprovesCSLRperformanceby
vocabularycontinuoussignlanguagerecognition(CSLR)
providingcontext,whileCSLRimprovesretrievalwithmore
–providingtimealignedanddensewordpredictionsforeach
fine-grained supervision. We further show the benefits of
signwithinasigningsequence.Thisisanessentialfirststep
leveragingweakandnoisysupervisionfromlarge-vocabulary
towardstranslation, asEnglishsentence-levelannotations
datasetssuchasBOBSL,namelysign-levelpseudo-labels,
have been shown to be difficult to use directly as targets
andEnglishsubtitles.Ourmodelsignificantlyoutperforms
forsignlanguagetranslation[4,14,57].Oursecondgoalis
thepreviousstateoftheartonbothtasks.
sentenceretrieval,i.e.givenasigningvideo,toretrievethe
mostsimilarsentencetextorviceversa(seeFig.1).Thisis
importantasindexingsignlanguagevideostomakethem
1.Introduction
searchablehasbeenhighlightedasausefulapplicationfor
deaforhardofhearing[7].Also,videotosubtitleretrievalcan
Recognisingcontinuousandlarge-vocabularysignlanguage
beseenasaproxyfortranslation–itisreminiscentofthepre-
isavitalsteptowardsenablingreal-worldtechnologiesthat
deeplearningstyleofmachinetranslationwheresentences
enhancecommunicationandaccessibilityforthedeaforhard
werebrokendownintophrasesandtranslationproceeded
ofhearing. Withtheavailabilityofdatathatdepictscontin-
byalookupofpairedphrasesinthetwolanguages[34].
uous signing from a large vocabulary of signs [4, 14, 23],
the computer vision field has recently gained momentum There are several challenges to achieving these goals,
towards this direction, building on previous research that primarily due to the lack of suitable data for training and
hadlargelyfocusedonrestrictedsettingssuchasrecognising evaluation.ForCSLR,ideally,eachindividualsignwithina
singlesignsinisolation[31,36]orsignscoveringrelatively continuousvideoshouldbeassociatedtoasymboliccategory.
smallvocabularies[35,69]. However,currenttrainingsupervisionsourcesarerestricted
bytheirweakorsparsenature. Forinstance,inthelargest
*Equalcontribution. datasetBOBSL[4],theavailableannotationsareeither(i)
1
4202
yaM
61
]VC.sc[
1v66201.5042:viXraatsentence-level,weaklyassociatingtheentiresigningvideo segmented #sentences hours vocab. #glosses source
toanEnglishsentence,ratherthanbreakingthevideointo PHOENIX-2014[35] ✗ 006K 0011 1K 065K* TV
train CSL-Daily[69] ✗ 018K 0021 2K 134K* lab
individualsign-wordcorrespondences,or(ii)atsign-level, BOBSL[4] ✗ 993K 1220 72K* ..5.5M* TV
but sparse with gaps in the temporal timeline (despite the PHOENIX-2014[35] ✗ 0629 1.0 0.5K* 07.1K* TV
test CSL-Daily[69] ✗ 1176 1.4 1.3K* 09.0K* lab
densificationeffortsin[43]toscaleupthenumberandvocab- BOBSLCSLR-TEST ✓ 4518 6.0 5.1K* 32.4K* TV
ularyofannotations).Also,thereisnoevaluationbenchmark
withcontinuousgroundtruthsignannotationsfortheBOBSL Table 1. Recent CSLR training and evaluation sets: Our
dataset, so it is not possible to assess and compare the
manually-curatedBOBSLCSLR-TESTsetislargerinnumberof
annotatedsignsandvocabulary,comparedtootherCSLRtestsets
performanceoflarge-vocabularyCSLRalgorithmsatscale.
fromtheliterature.Inaddition,italsocomeswithsignsegmentation
Inthispaper,weintroduceasimpleTransformerencoder
annotations.*NotethattheBOBSLtraininghasdifferentvocabulary
model[58]thatingestsasigningvideosequenceandoutputs
setsofvaryingsizes: 72Kwordsspannedbysubtitlesand25K
tokensinajointembeddingspacebetweensignedandspo-
wordsspannedby5.5Mautomaticannotationsgeneratedin[43].
ken1languages.TheoutputspaceenablesboththeCSLRand
sentenceretrievaltasks. TheTransformerarchitectureout- onlyseekstoassignacategory(typicallyalsoexpressedasa
putsCSLRpredictionsbyleveragingtemporalcontext,and word)toashortvideosegmenttrimmedaroundasinglesign
aretrievalembeddingthroughpooling.Thejointembedding withoutcontext. BesidesCSLR,severaltasksthatrequire
languagespacemayalsohelptoovercomeyetanotherchal- ingesting a continuous video stream exist. These include
lengeofsignlanguagerecognition:polysemywherethesame sign spotting [2, 42, 43, 57], sign tokenization [48, 49],
wordmaycorrespondtoseveralsignvariants,andconversely translation[11–13,63,68],subtitlealignment[10],subtitle
thesamesignmaycorrespondtoseveraldifferentwords. segmentation[9],text-basedretrieval[17,24],fingerspelling
We train our model on both tasks by leveraging noisy detection [44], active signer detection and diarization [3].
supervisionfromthelarge-scaleBOBSLdataset.Specifically, Ourworkisrelatedtosomeoftheseworksinthattheyalso
weuseanindividualsignpredictortogeneratecontinuous operate on a large-vocabulary setting [2, 10, 42, 43, 57];
pseudo-labels (for training CSLR) and available weakly- however, they do not tackle CSLR, mainly due to lack of
aligned sentence-level annotations (for training sentence continuoussignannotations.While[24]addressesretrieval,
retrieval). Weshowthattrainingforbothtasksismutually theirmethodisnotsuitableforCSLR–ourworkdiffersin
beneficial–inthatincludingCSLRimprovestheretrieval thatweperformbothtasksjointly.
performance,andincludingretrievalimprovestheCSLRper- State-of-the-art CSLR methods have so far focused
formance.ToenableCSLRevaluation,wemanuallycollect on PHOENIX-2014 [35] or CSL-Daily [69] benchmarks,
newsign-levelannotationsthatarecontinuousonthetimeline. where the performances are saturated. These methods
Sincewefocusonthelarge-vocabularysetting,wecollect typicallyconsiderafully-supervisedsetting,andtrainwith
annotationsontheBOBSLtestset.WehopeournewCSLR RNN-based[19,29,66],orTransformer-based[13]models.
benchmarkwillfacilitatefurtherexplorationinthisfield. Duetolackofsignsegmentationannotation(i.e.,thestartand
Insummary,ourcontributionsarethefollowing: (i)We endtimesofsignsareunknown),manyworksusetheCTC
demonstratetheadvantagesofasinglemodel,CSLR2,that
loss[13,16,30,61,70].Ourworkdiffersfromtheseprevious
istrainedjointlyforbothCSLRandsignlanguagesentence worksonseveralfronts. Weconsideraweakly-supervised
retrievalwithweaksupervision.(ii)Thankstoourjointem- setting,wherethetrainingvideosarenotannotatedforCSLR
beddingspacebetweenspokenandsignedlanguages,weare purposes,butareaccompaniedwithweakly-alignedspoken
thefirsttoperformsignrecognitionviavideo-to-textretrieval. languagetranslationsentences. Wealsostudythebenefits
(iii)Webuildabenchmarkofsubstantialsizeforevaluating of joint training with CSLR and retrieval objectives. In a
large-vocabularyCSLRbycollectingcontinuoussign-level similarspirit,theworksof[13,70]jointlytrainCSLRwith
annotations for 6 hours of video. (iv) We significantly sentence-level objectives (translation in [13], margin loss
outperformstrongbaselinesonournewCSLRandretrieval forgloss-sequencetextretrievalin[70]),butinsignificantly
benchmarks,andcarefullyablateeachofourcomponents. different settings (e.g., 8× smaller vocabulary, and with
Wemakeourcodeanddataavailableforresearch. manually-annotatedCSLRlabelsfortraining).
Signlanguageretrieval.Earlyworksfocusedonquery-by-
2.RelatedWork
example[5,67],wherethegoalistoretrieveindividualsign
instancesforgivensignexamples.Thereleaseofcontinuous
Webrieflydiscussrelevantworksthatoperateon(i)contin-
sign video datasets, like BOBSL [4], How2Sign [23],
uoussignlanguagevideostreams,(ii)signlanguageretrieval,
and CSL-Daily [69] with (approximately) aligned spoken
and(iii)CSLRbenchmarks.
languagesubtitles,hasshiftedtheinteresttowardsspoken
Ingesting continuous sign language video streams. In
languagetosignlanguageretrieval(andvice-versa). The
therecentyears,thecommunityhasstartedtomovebeyond
firstworkinthisdirectionistherecentmethodof[24],which
isolatedsignlanguagerecognition(ISLR)[31,36],which
focusesonimprovingthevideobackbonethatissubsequently
1Werefertothewrittenformofspokenlanguage,notthespeechaudio. used for a simple retrieval model using a contrastive
2margin loss. CiCo [17] also focuses on improving video encoderT basedonapretrainedT5model[47]. Given
enc
representations,specifically,bydesigningadomain-aware rawRGBvideoframepixelsV forasigningsentence,we
backbone.Incontrast,ourmainemphasisison(i)theuseof obtainasequenceofisolatedsignvideoembeddingsfrom
weakly-superviseddata,and(ii)thejointtrainingwithCSLR. VSign as{vˆ }=VSign(V). Inpractice,suchasequenceis
enc f enc
Ourworknaturallyderiveslessonsfromthelargenumber obtainedbyfeeding16consecutiveframestothesignvideo
ofeffortsintheparenttaskofsignlanguageretrieval,i.e., encoderinaslidingwindowfashion,withastrideof2frames.
video-textretrieval[6,27,38,39,41,54,65].Workssuchas These are then fed through the sentence video encoder
CoCa[64]andJSFusion[65]haveshownthatjointlytraining VSent tohave context-aware signvideo embeddings {v },
enc f
withacross-modalretrievalobjectivecanhelpinothertasks aswellasasinglesentencevideoembeddingV, denoted
suchascaptioningandquestion-answering.Ourapproachis ({v },V)=VSent({vˆ }). Similarly, forthetextside, we
f enc f
inthesameveinastheseworks:weshowthatjointlytraining embed the sentence T into T = T (T). Additionally,
enc
forretrievalandCSLRimprovesperformanceforbothtasks. we define sign-level text embeddings for each sign in
CSLRbenchmarks.Earlyworkswithcontinuoussigning the sentence as {t }W = {T (t )}W , where W is
w w=1 enc w w=1
videos provided very small vocabularies in the order of the number of signs in the sentence. In practice, we get
severalhundreds(104signsinPurdueRVL-SLLL[62]and these embeddings by independently feeding the word(s)
BOSTON104[22]ASLdatasets,178inCCSL[29],310in correspondingtoeachsigntothetextencoder.
GSL[1],455intheSIGNUMDGSdataset[59],and524in CSLR inference. Sign-level recognition predictions are
theKETIKSLdataset[33]). BSLCorpus[50]representsa obtainedbyusingthesequenceofsignvideoembeddings
large-vocabularycollection; however, itismainlycurated in v that lies in the same space as spoken language. To
f
forlinguisticsstudies,andhasnotbeenusedforCSLR. associate each feature frame f to a word (or phrase), we
RelativelylargecollectionsmadeitpossibletotrainCSLR perform nearest neighbour classification by using a large
methodsbasedonneuralnetworks(seeTab.1).Mostwidely textgalleryofsigncategorynames,asillustratedinFig.2c.
usedRWTH-PHOENIX-Weather2014[35]datasetcontains In our experiments, we observe superior performance of
around 11 hours of videos sourced from weather forecast suchretrieval-basedclassificationoverthemoretraditional
onTV.CSL-Daily[69]provides20Kvideoswithglossand cross-entropyclassification[60],withtheadvantagethatitis
translationannotationsfromdailylifetopics,coveringa2K potentiallynotlimitedtoaclosedvocabulary.Inordertogo
signvocabulary,and23hoursoflabrecordings.InTab.1,we fromper-featureclassificationtocontinuoussignpredictions,
provideseveralstatisticstocompareagainstournewCSLR weperformapost-processingstrategydetailedinSec.3.3.
benchmark,mainlyontheirevaluationsets(bottom).While Wenotethatthissamepost-processingstrategyisusedfor
beinglarger,wealsoprovidesignsegmentationannotations. obtainingoursign-levelpseudo-labelsfortraining.
Recentlyreleasedlarge-vocabularycontinuousdatasets Retrievalinference.Forsign-video-to-text(V2T)retrieval,
(suchasBOBSL[4],How2Sign[23],Content4All[14],and thevideosentenceembeddingVismatchedtoagalleryof
OpenASL[52])donotprovidesign-levelglossannotations textsentenceembeddings, rankingtextsentencesbytheir
duetotheprohibitivecostsofdenselylabelingwithinthe cosinesimilarities.Symmetrically,text-to-sign-video(T2V)
open-vocabularysetting.Inthiswork,weleverageanisolated retrievalisperformedinasimilarmanner,asshowninFig.2b.
sign recognition model to generate continuous pseudo-
3.2.Trainingwithsentence-andsign-levellosses
labels for training CSLR, and available weakly-aligned
sentence-levelsupervisionforretrieval. We train the Transformer-based model, that operates on
sentence-levelsignlanguagevideos,toperformtwotasks,
3.JointSpaceforSignedandSpokenLanguages namely, CSLRandsignlanguageRetrieval(CSLR2). As
illustratedinFig.2a,weemploytworetrievallosses: (i)a
We start by describing the model design that goes from
sentence-level objective, supervised with weakly-aligned
rawsignlanguagevideopixelstoajointembeddingspace
subtitles, and (ii) a sign-level objective, supervised with
withspokenlanguagetext(Sec.3.1). Wethenpresentthe
pseudo-labelsobtainedfromastrongISLRmodel[44].We
lossesofourjointtrainingframeworkwithsentence-level
nextformulateeachobjectiveindividuallybeforeintroducing
andsign-levelobjectives(Sec.3.2). Next,wedetailoursu-
ourjointframeworkthatleveragesbothsentence-leveland
pervisionwhichconsistsof(noisy)sign-levelpseudo-labels
sign-levelinformation.
andweakly-alignedsubtitles(Sec.3.3).Finally,weprovide
Sentence-levelobjective:signlanguagesentenceretrieval
modelimplementationdetails(Sec.3.4).
(SentRet). Weexplorethetaskofretrievalasameansto
obtainsupervisionsignalfromthesubtitles. Followingthe
3.1.Modeloverviewandinference
successofvision-languagemodelsbuildingacross-modal
Ourmodel,showninFig.2,consistsofthreemaincompo- embeddingbetweenimagesandtext[37,46],weemploya
nents: (i)asignvideoencoderVSign basedonapretrained standardcontrastiveloss,andmapsignlanguagevideosto
enc
Video-Swin[40],(ii)asentencevideoencoderVSentbased spokenlanguagetextspace.
enc
onarandomlyinitialisedTransformerencoder,and(iii)atext Signlanguagesentenceretrievalismadeoftwosymmetric
3CVPR24 Method fig (v2++) alternative with symmetry
Sentence Retrieval Sign Retrieval
. video 1
T
video 2 Sentence video Sign video Sentence text Sign text …
embedding embeddings embedding embedding
𝒯enc
video N V v 1 v 2 v 3 v 4 … vf T t1 t2 … tw Sentence text query
… (b) T2V retrieval inference
Sentence Video Encoder 𝒱enS cent 𝒯enc 𝒯enc
.
v̂1 v̂2 v̂3 v̂4 v̂f
Sign … word 1
𝒱enc
… Over a century earlier. hundred years earlier v 1 v 2 … vf w …ord 2
Sent word K 𝒱enc
Video V time Subtitle T Sign pseudo-labels {t w} time 𝒱enS cign …
Test video
(a) Joint training withℒ +ℒ (c) CSLR inference
SentRet SignRet
Figure2.Methodoverview:(a)Weshowasimplifiedviewforourmodelarchitecturewhichconsistsofbothvideoandtextstreams.Onthe
videoside,featuresareextractedfromasigningvideoclipV byrunningVSigninaslidingwindowfashionandpassedthroughaTransformer
enc
modelVSent.AvideoembeddingVandsignvideoembeddings{v }aresubsequentlyextracted.Onthetextside,weinputanEnglishsubtitle
enc f
sentenceT andsignpseudo-labels{t }tothetextencoderT andobtainsentenceandsigntextembeddings(T,{t }),respectively.While
w enc w
weillustrateonlyonetripletdatapoint(V,T,{t }),inpractice,weoperateonaminibatchoftriplets,andemploytwocontrastivelossesto
w
jointlytrainonsentenceretrievalL andsignretrievalL .(b)Fortext-to-videoretrievalinference,wesimplyextractasentencetext
SentRet SignRet
embeddinggivenatextquery,andrankthesentencevideoembeddingscorrespondingtogalleryvideosaccordingtotheircosinesimilarities.
(c)ForCSLRinference,eachsignvideoembeddingismatchedtothetop-rankedwordfromalargevocabularyofsize8K.Apost-processing
strategyisappliedonframe-levelpredictionstoproducefinaloutputs.Forvisibility,weomitlinearlayerswhichprojectembeddingsinto
thelearntjoint-space.SeeSec.3.1foradetaileddescriptionofthearchitectureandinferenceprocedure.
tasks,thatis,V2TandT2Vretrievals.Fortheformer,given ingthesimilaritybetweennegativepairs,HN-NCEservesasa
aquerysigningvideoV,thegoalistorankagalleryoftext proxyfortheretrievalbyrankingthatweperformatinference.
samples(heresubtitles)suchthatthecontentofV matches Sign-levelobjective:signclassificationviasignretrieval
thecontentinthehighestrankedtexts. Symmetrically, in (SignRet). Givenacontinuoussigningvideo, thegoalof
thelatter,givenatextqueryT,thegoalistorankagallery CSLRistorecogniseasequenceofindividualsigns. The
ofsigningvideos. continuousvideoisencodedintoasequenceofcontext-aware
Formally, given a dataset D = {(V ,T )}N of video- sign video embeddings {v }F , with F the number of
i i i=1 f f=1
subtitle pairs, the goal is to learn two encoders ϕ ,ϕ videoframes.Again,sincetheseembeddingsareinthesame
V T
mapping each signing video V and subtitle T into a joint jointspaceasthetextembeddings,acontrastivelosscanbe
embedding space. In the following, V = ϕ (V ) and usedasaproxyforsignretrieval.
i V i
T =ϕ (T )denotethevideoandtextembeddings,respec- Similarlytothesentence-levelretrieval,weusetheHN-
i T i
tively. Theencodersaretrainedusingarecentlyproposed NCEcontrastiveformulationdefinedinEq.(1)forthesign
Hard-Negative variant of InfoNCE [56], HN-NCE [45], retrieval(SignRet)loss.However,insteadofthefullsentence
that re-weighs the contribution of each element in the video-textembeddingpair(V,T),wemapindividualsign
computationofthecontrastiveloss. Let{(V ,T )}B be video-wordembeddingpairs(v,t)(seeFig.2c).
i i i=1
abatchofencodedvideo-subtitlepairsandS =VTT be Overallloss.Ourmodelistrainedjointlyusingaweighted
ij i j
thesimilaritybetweenthepair(i,j).Forthesakeofvisibility, sumofthetworetrievalterms:
weonlydetailtheequationsforV2T:
L=λ L +λ L
SentRet SentRet SignRet SignRet
1(cid:88)B eSii/τ
L =− log , withL ,L ,i.e.twocontrastivelossesforsentence
HN-NCE,V2T B
i=1
α·eSii/τ+(cid:80) j̸=iw iV j2T·eSij/τ andsigS nen rt eR te rt ievS ai lg ,n rR ee st
pectively.Thetrainingdetailsincluding
(1) thebatchsize,learningrate,andotherhyperparameterscan
withw iV j2T weightsdefinedas befoundinthesupplementarymaterials.
(B−1)·eβSij/τ 3.3.Sourcesofsupervision
wV2T= , (2)
ij (cid:80) eβSik/τ Leveraging weak and noisy text labels for the CSLR and
k̸=i
retrievaltrainingconstitutesoneofthekeycontributionsof
wherethetemperatureτ>0,α∈(0,1],andβ≥0arehyper- thiswork. Next,wepresentourtwosourcesoftextsuper-
parameters.Bytrainingtomaximisethesimilaritybetween vision,namely,sign-levelpseudo-labelsandsentence-level
correctpairsofvideoandsubtitleembeddings,whileminimis- weakly-alignedsubtitles.
4
from
a search
gallery
from
a large
vocabulary
Sentence
video
embeddings
Sign
text
embeddingsSign-levelpseudo-labels.Westartwith(V,T)video-subtitle embeddings {vˆ } and outputs context-aware sign video
f
pairsthatdonotcontainsign-levelannotations. Inorderto embeddings {v }. We obtain a single sentence video
f
obtainsign-levelsupervisiontotrainforCSLR,weperform embedding by simply max-pooling over the temporal
sign-levelpseudo-labelling.Specifically,weapplyanISLR dimension, i.e. V = MaxPool ({v }F ), with F video
f f f=1
modelinaslidingwindowfashionwithastrideof2frames, features,andexperimentallyvalidatethischoice.Werestrict
andperformpost-processingofsignpredictionsasanattempt ourtrainingtovideoclipsshorterthan20seconds. The
to reduce noise. Our post-processing strategy consists of parametersoftheTransformerencoderarelearntusingthe
3steps: (i)wefirstcombineconfidencescoresofsynonym sentenceretrievallossL betweensentencetextT and
SentRet
categoriesfortheTop-5predictionsfromtheISLRmodel video V embeddings, and the sign retrieval loss L
SignRet
(usingthesynonymlistdefinedin[43]); (ii)wethenfilter betweensigntextembeddingst andthecorrespondingsign
w
outlowconfidencepredictions(belowathresholdvalueof videoembeddingsv ,asdescribedinSec.3.2.
f
θ=0.6);(iii)finally,weremovenon-consecutivepredictions Textencoder(T ).Weusetheencoderpartofapre-trained
enc
– since each sign spans several video frames, we expect T5 [47] (t5-large), and keep its weights frozen. Note
repetitionsfromtheISLRmodel(wekeeppredictionswith wedonotuseitsdecoder.Theoutputtextembeddingshave
atleastm=6repetitions). dimensionality1024.
Inpractice,foreachsubtitle,wedefineasentence-level Projectionheads. Weadditionallylearnprojectionlayers,
video(onaverage3.4seconds)bytrimmingtheepisode-level mainly to reduce the joint embedding dimensionality to
video (∼1h duration) using the subtitle timestamps. The 256beforecontrastivelosscomputations. Specifically,we
sentence-levelvideoisfurtherbrokendownintoframe-sign haveatotalof4projectionheads:twoforreducingthetext
correspondences based on pseudo-label timestamps. The dimensionality(1024→256)withaseparateprojectionfor
sign-levellossisthenonlycomputedonframesassociated sign categories and sentences, two for reducing the video
toapseudo-labelafterpost-processing. embeddingdimensionality(768→256)separatelyforsign
Weakly-aligned subtitles. The source of our large-scale andsentencevideoembeddings.
video-subtitle pairs is from sign language interpreted TV
shows,wherethetimingsoftheaccompanyingsubtitlescor- 4.ANewCSLREvaluationBenchmark
respondtotheaudiotrack,butnotnecessarilytosigning[4].
Forbettersign-video-to-textalignments,weuseautomatic Inthissection,wedescribethenewcontinuoussignannota-
signing-alignedsubtitlesfrom[10](describedin[4])totrain tionsthatwecollectedforevaluatingCSLR.Wefirstdescribe
ourmodels. Werestrictourtrainingtosubtitlesspanning whattheCSLR-TESTis,andthenhowitwasannotated.
1-20seconds,resultingin689Kvideo-subtitletrainingpairs. CSLR-TEST.Thecontinuousannotationsareprovidedfor
asubsetoftheSENT-TESTpartitionofBOBSL[4]. SENT-
3.4.Implementationdetails TESTisa31hoursubsetoftheBOBSLtestsetwheretheBSL
signingsequenceshavebeenmanuallyalignedtemporally
Inthefollowing,wedetaileachcomponentofourmodel.
withtheircorrespondingEnglishsubtitlesentences.
Signvideoencoder(VSign).Similarto[44],oursignvideo
enc TheCSLR-TESTannotationsconsistofatimealigned
featuresareobtainedbytrainingaVideo-Swinmodel[40],
sequence ofsign ‘glosses’2, where each signis annotated
onISLR.Thenetworkingestsashortvideoclip(16frames,
<1second)andoutputsasinglevectorvˆ∈Rd (d=768), withitstemporalinterval,thetypeofthesign,anditsword
equivalentifthatexists.Inadditiontolexicalsigns(i.e.signs
followedbyaclassificationheadtorecogniseisolatedsigns.
thathaveanEnglishwordequivalent),awiderangeofsign
Specifically,wefinetunetheVideo-Swin-Tinyarchitecture,
typessuchasfingerspelling,pointing,depictingorno-signing
pretrainedonKinetics-400[15],usingautomaticannotations
areannotated.Thesearemarkedwithspecialcharacterssuch
releasedin[43].Theseannotationsprovideindividualsignla-
as*FSand*Pforfingerspellingandpointing,respectively.
belsalongwithtimestampsofwheretheyoccurinthevideo.
Note,thatthereexistsnouniversallyacceptedwritingsys-
Note that the annotations have been automatically
temforsignlanguagestoday[26],thoughattemptshavebeen
obtainedwiththehelpofsubtitles(byexploitingcuessuchas
madewithdescriptivelanguagessuchasHamNoSys[28]and
mouthing),andcanthusbenoisy.OncetrainedforISLR,we
SignWriting[55].Also,carefulglossingthatislinguistically
freezetheparametersofthisrelativelyexpensivebackbone,
consistent (e.g., enumerating each sign variant [51]) is a
and extract isolated sign video embeddings vˆ in a sliding
tediousprocesswhichhindersscalingup.Forthesereasons,
windowmannerwithastrideof2frames.NoteweuseRGB-
wemakeacompromisewhenannotatingforCSLRanduse
basedembeddings,insteadofbodykeypointestimates,dueto
Englishwordsfortheglosses, assigning‘any’reasonable
theirmorecompetitiveperformanceinthelarge-vocabulary
Englishwordforasignsegment,butprioritisingwordsinthe
setting,wheresigndifferencesaresubtleandnuanced[4].
surroundingsubtitle. Forexample,ifthe‘natural’English
Sentencevideoencoder(VSent). WeadoptaTransformer
enc word for the sign is ‘laugh’ but a synonym word such as
encoderarchitecture,similartoBERT[20],with6encoder
layers, 8 attention heads and 768 hidden dimensionality. 2Weabusetheglossterminology,despiteoursign-levelannotationsnot
It ingests the sign sentence video as isolated sign video beingcarefullinguisticglosses,butratherfree-formsign-leveltranslations.
5CVPR24
Beneath the shadow of the Severn Bridge is a jetty that once bustled with travellers. You know, the America's cup is a design cup as much as it is a sailing race.
I’m catching a ferry back to another coast connected by this water. I worked in the finance industry, so I just want to do something a little bit more…
Figure3.AnnotationexamplesfromtheCSLR-TESTdataset:AswellasassigningtheEnglishword(s)correspondingtoasign(i.e.‘gloss’),
theannotatorsindicatethetypeofsignwhenappropriate.Forexample,‘*P’forpointing,‘*FS’forfingerspelling,‘*G’forgesturesign.
‘giggle’isinthesubtitle, thentheglosswouldbe‘giggle’ (Sec.5.4),andillustratequalitativeresults(Sec.5.5).
(with‘laugh’alsoprovidedasamoregeneraltranslation).
However, oneshouldkeepinmindthatassociatingwords 5.1.Dataandevaluationprotocol
tosignsisalossyanderror-proneprocessinanycase.
BOBSL [4] consists of about 1500 hours of video data
Fig.3showsexamplegroundtruthglossannotationsfor
accompanied with approximately-aligned subtitles. A
CSLR-TEST.Intotal,wecuratethesecontinuouslabelsfor
200-hoursubsetisreservedfortesting. Wereuseexisting
6hoursofvideo,comprising32.4Kindividualsignsfroma
vocabularyofapproximately5.1Kglosses.TheCSLR-TEST
manually-alignedvalidationandtestsets(SENT-VAL [4],
annotations evenly cover all 35 episodes in SENT-TEST.
SENT-TEST[4])foroursentenceretrievalevaluation(20,870
and1,973alignedsentencesrespectively). Weperformour
AdditionalstatisticsforthedatasetaregiveninTab.1. We
retrievalablationsonthevalidationset,andreportthefinal
note that we also annotate a small subset of the BOBSL
modelonbothevaluationsets.ForCSLRevaluation,weuse
trainingandvalidationsubtitle-alignedsplits.Allannotations
willbepubliclyreleased.
ourmanuallyannotatedtestset(CSLR-TEST)asdescribed
inSec.4,whichcorrespondsto4950unseentestsubtitles.
Dataset annotation. The annotation procedure uses a
For retrieval evaluation, we report both T2V and V2T
webbasedannotationtoolthatisbuiltfromtheVIAvideo
performancesusingstandardretrievalmetrics,namelyrecall
annotation software [25]. Annotators are provided with
atrankk(R@k)fork∈{1,5}.ForCSLRevaluation,given
a video sequence with 10 time aligned subtitle sentences.
Annotatorsenterfree-formtextforeachsigntoken,taking
a video sequence defined by CSLR-TEST, we compute
ourpredictedglosssequence(afterpost-processingtheraw
intoaccountthecontextbywatchingthefullvideoarounda
per-frameoutputswiththeoptimalθandmheuristics–see
givensubtitle.Thesubtitleisalsodisplayedonthevideoand
Sec.3.3forpost-processingstrategy)andcompareagainstits
theannotatorsareencouragedtoprioritiseassigningwords
correspondingground-truthglosssequenceusingseveralmet-
that appear in the corresponding subtitle. The annotators
rics.Wenotethatwefilteroutsigntypesandsignsthatarenot
additionallyassignsigntypeswhenappropriate(seeFig.3)
associatedtolexicalwordsfromtheground-truthsequence.
andtemporallyaligneachglosstothedurationofthesign.
Inaddition,ifseveralannotationwordsareassociatedtoone
Wefacilitatefasterannotationiterationsbyincorporating
sign(e.g.‘giggle’and‘laugh’),predictionsareconsidered
several strategies. We adopt a semi-automatic labeling
tobecorrectifoneofthesewordsispredictedcorrectly.
techniquewhereweinitialisethesignboundariesbyusing
anautomaticsignsegmentationmethod[48].Annotatorsare AsinotherCSLRbenchmarks[35,69],wereportword
thusgivenaninitialsetofsignintervals,andareinstructed error rate (WER) as our main performance measure. We
torefinethesesignboundaries(oradd/removesignintervals) alsomonitormIoU(meanintersectionoverunion)between
ifnecessary. Wefurthershowadropdownmenuforeach predicted and ground truth sequences’ words, without
sign and prioritise at the top of the list words from the considering any temporal aspect. In all metrics, similar
correspondingsubtitle. TwonativeBSLusersworkedon to[43],wedonotpenaliseoutputwordsiftheyaresynonyms.
thistaskforoveroneyear.Furtherdetailsontheannotation Inordertoassessthemodel’sabilitytocorrectlypredict
procedureareprovidedinthesupplementarymaterial. signsegmentsattherighttemporallocation,wealsoreport
theF1score.Wedefineasegmenttobecorrectlypredicted
5.Experiments if(i)thepredictedsignmatches,uptosynonyms,theground-
truthgloss,and(ii)theIoUbetweenthepredictedsegment’s
Inthissection,wefirstpresentevaluationprotocolsusedin boundariesandthegroundtruthglosssegment’sboundaries
ourexperiments(Sec.5.1)anddescribebaselines(Sec.5.2). ishigherthanagiventhreshold.WecomputetheF1scoreas
Next, we provide ablations to assess the contribution of theharmonicmeanofprecisionandrecall,basedonthisdef-
important components (Sec. 5.3). We then report CSLR initionofcorrectsegmentdetection.WereporttheF1score
andretrievalperformance,comparingtothestateoftheart atdifferentthresholdsvalues,namely,F1@{0.1,0.25,0.5}.
6SentRet Sign-levelloss WER↓ mIOU↑ F1@{0.1,0.25,0.5}↑ SentRet Sign-level T2V V2T
Pool. loss loss R@1↑ R@5↑ R@10↑ R@1↑ R@5↑ R@10↑
ISLRBASELINE 71.7 30.1 40.0 37.9 27.2
cls InfoNCE ✗ 38.9 62.1 69.1 39.2 61.0 68.1
✗ CTC 75.0 27.8 - - - cls HN-NCE ✗ 48.9 68.3 73.9 46.5 67.2 72.8
✗ ✗ SigC nE Ret 7 72 1. .9 2 2 38 0. .6 4 3 47 0. .9 7 3 37 9. .3 9 2 39 1. .1 0 m ma ax x HIn Nfo -N NC CE E ✗ ✗ 4 53 0. .4 5 6 64 9. .9 5 7 71 5. .0 1 4 42 9. .7 7 6 64 9. .8 7 7 70 4. .7 7
✓ CE 71.0 30.4 39.7 38.9 30.9 max HN-NCE CE 50.0 69.1 74.4 48.7 68.7 74.3
✓ SignRet 65.5 35.5 47.1 46.0 37.1 max HN-NCE SignRet 51.7 69.9 75.4 50.2 69.1 74.7
Table2.CSLRablationsonCSLR-TEST:OnlyusingCTC,cross Table 3. Retrieval ablations on SENT-VAL: We experiment
entropy(CE),orSignRetdoesnotperformwell,remainingbelow with the choice of the contrastive sentence retrieval (SentRet)
orcomparabletotheISLRbaseline.Weobservebestresultswhen loss(standardInfoNCEvs.HN-NCE),thevisualencoderpooling
incorporatingjointsentenceretrievaltraining. (clsvsmax),theadditionandchoiceofsign-levellosses(cross
entropyCEvs.contrastivesignretrievalSignRet). Thelasttwo
5.2.Baselines rowscorrespondtothejointmodelsevaluatedforCSLRinTab.2.
Subtitle-basedautomaticannotationsCSLRbaseline.The
firstbaselineforCSLRisobtainedusingspottingmethods thanthestrongISLRbaseline(i.e.withVideo-Swin)forCTC
thatsearchforsignscorrespondingtowordsinthespokenlan- andCE,andcomparableforSignRet.Inthefinaltworows,
guagesubtitles.Theinitialsetofsparsesignannotationsalong weobservecleargainsbycombiningtheSentRetlosswith
withtimestampswasreleasedwiththeBOBSLdataset[4], either(i)ourSignRetlossor(ii)thestandardCEloss.While
usingsignspottingmethodsfrom[2,42,57],followedby thejointtrainingwiththeCElossbringsaperformanceboost,
denserspottingsin[43].Weevaluatetheseexistingsequences from72.9to71.0WER,itdoesnotsignificantlysurpassthe
of spottings, in particular the ones corresponding to our competitiveISLRbaseline.OurmodelCSLR2,whichjointly
CSLR-TESTsubtitles.Inpractice,wefiltertheseautomatic trainssentenceandsignretrieval,bringsamajorimprovement
annotationsusingthesamesetsofthresholdsasin[4,43],re- byreducingtheWERby5.7points,from71.2to65.5.
spectively(seesupplementarymaterialfordetails).Notethat Retrieval components. In Tab. 3, we compare design
thesespottingsmakeuseoftheweakly-alignedsubtitles(and choicesfortheretrievaltask: (i)choiceofthecontrastive
cannotgobeyondwordsinthesubtitles),andthereforecannot loss function – InfoNCE vs. HN-NCE; (ii) choice of
beusedasatrueCSLRmethod.Theyarealsopointannota- poolingthetemporalfeatures–usingaclstoken[20,21]
tions,withoutprecisetemporalextent,thusweomitF1scores. vs. max-pooling. We observe a clear boost in all metrics
ISLRbaselinesforCSLR.ThissetofbaselinesusesISLR by using HN-NCE with our weakly-aligned data, which
models in a sliding window fashion to obtain continuous givesmoreweighttothehard-negativeswhencomputingthe
frame-levelpredictions. Weaggregatetheslidingwindow contrastiveloss:thereisaminimumimprovementof+7R@1
outputsbyperformingthepost-processingstrategydescribed forT2Vcomparisons.Wefurtherobservethatmax-pooling
inSec.3.3(foroptimalθandmparameters,tunedonunseen thevisualTransformerencoderoutputs,insteadofusinga
manuallyannotatedCSLRsequencesfromthevalidationset). learnableclstoken,consistentlygivesbetterresults. The
We use the I3D [15] and Video-Swin [40] models trained joint training of retrieval and CSLR, with SignRet, also
fromspottingannotationsof[4,43].Webuildonpriorworks improves retrieval performance: R@1 for T2V increases
forthesebaselines:weusetheI3Dweightsreleasedin[4]and from50.5to51.7. Moreimportantly,jointtrainingenables
trainaVideo-Swin-Tinymodelinasimilarfashionasin[44]. asingle,strongmodelwhichcanperformbothtasks.
InfoNCE retrieval baseline. Our baseline for sentence
retrievalisthestandardcontrastivetraining[56]employed 5.4.Comparisontothestateoftheart
by many strong vision-language models [37, 46]. We
Wecomparetothecurrentstate-of-the-artapproaches,both
train this vanilla model, without the CSLR objective, on
forlarge-vocabularyCSLRandsentenceretrieval,inTab.4.
the automatically-aligned subtitles from [10]. Sentence
CSLR performance. First, in terms of the baselines, it
embeddings,obtainedbyfeedingsubtitletextintoT ,are
enc canbeseenfromTab.4, thatourpost-processingstrategy
comparedagainstalearnableclstokenonthevideoside
significantly strengthens the original ISLR I3D-2K [4]
whichpoolsthevideoembeddingsasdonein[21,46].
performancebyremovingsignificantnoise(with/without†)
–wereducetheWERbymorethanafactorof5(453.5vs
5.3.Ablationstudy
82.5). Also, our post-processing strategy combined with
CSLRcomponents.InTab.2,weexperimentwiththechoice the8KvocabularyISLRmodels,deliversmodelsofhigher
oftrainingobjectivesforCSLRperformance. Inparticular, performance(bymorethan10WER)thanallsubtitle-based
wetrainwiththestandardCTCorcross-entropy(CE)losses, spottingsmethods,eventhoughtheISLRmodelsdonothave
aswellasoursignretrieval(SignRet)lossalone.Withoutan access to the subtitles. Second, our joint model, CSLR2,
additionalsentenceretrievalloss,i.e.ifweonlyoptimisefora outperforms all CSLR baselines by a significant margin
sign-levelobjective,weobservethattheperformanceisworse onallmetrics. Indeed, CSLR2 surpassesthebestsubtitle
7CVRP24 Qualitative main (v3) - manual text -
manual color for synonyms
understand ≠ idea
mountain ≠ home
WER: 25.0 WER: 31.3 WER: 40.0
imagine = clear
island = town
hit = make
that it = what down = low
worry = concern Around the world, zoos like London Zoo dedicated a lot of their
l ai lk toe g= e ts ha em r e = whole juT sh t e s tm raa igjo hr ti ft oy r o wf a s rw d e wa at ti es r . time and money to saving some of world's most endangered species. theA irn vd e j ru ys ct ota mk pe lm icae t t eh dr o bu og dh ie s.
1960 = sixty
e ex vp idla ei nn c e= =s t wor iy tness WER: 42.9 WER: 50.0 WER: 50.0
lunch = eat
seven = six
We were worried that it might happen again, falling on
a primary school, someone’s home, or a playground. And, for the first time in a very long time, It is a normal, inevitable part
I have three functioning cooling units. of the process of evolution.
decrease / few
rocket / upstairs WER: 58.3 WER: 71.4 WER: 75.0
often / regular most / especially
OK, this is primitive United Kingdom, we’re going to have Since we began exploring space in the 1960s,
Scotland at one end, the Isle of Wight on the other end. nearly 7,000 satellites have been sent into orbit. With the lost child safe, the camp
team can prepare for the guests’ lunch.
Figure4.QualitativeCSLRresults:Wecompareourmodel’spredictions(Pred)againstthegroundtruth(GT),providingexamplesfromsev-
eralerrorranges(sortedbyWER).Thesubtitlesdisplayedbeloweachexamplearenotusedbythemodel.Whileweobservethatourmodelcor-
rectlypredictsalargeportionofsigns,handlingbothEnglishsynonymsaswellassignlanguagepolysemy(twovisuallysimilarsignswithdiffer-
entmeanings)makestheCSLRtaskchallenging.Synonymsaredepictedwiththesamecolorcoding,e.g.‘earth’and‘world’in3rdrow,middle.
CSLR CSLR-TEST 5.5.Qualitativeanalysis
Model WER↓ mIOU↑ F1@{0.1,0.25,0.5}↑
Subtitle-basedspotting[4] 93.8 07.1 - - -
Subtitle-basedspotting[43] 85.5 18.1 - - -
ISLRI3D-2K[4] 453.5 08.6 10.6 08.5 05.4
InFig.4,weshowseveralqualitativeexamplesofourCSLR
ISLRI3D-2K[4]† 82.5 18.1 25.7 23.8 16.0
ISLRI3D-8K[43]† 74.7 27.0 36.7 35.4 26.7 predictions (Pred rows) against the corresponding ground
ISLRSwin-8K[44]† 71.7 30.1 40.0 37.9 27.2 truth(GTrows)onCSLR-TEST. Notethatwedisplaythe
CSLR2(OURS)† 65.5 35.5 47.1 46.0 37.1 corresponding ground truth subtitles below each example
togivecontexttothereader,buttheyarenotgivenasinput
Retrieval SENT-VAL(2K) SENT-TEST(20K)
to the model. We illustrate examples from several error
T2V V2T T2V V2T
Model R@1↑ R@1↑ R@1↑ R@5↑ R@1↑ R@5↑ ranges, sortingthembyWERpersample(reportedatthe
InfoNCE 38.9 39.2 19.5 35.1 18.9 33.8 top).Thesetimelinesshowthatourmodelisabletopredicta
CSLR2(OURS) 51.7 50.2 29.4 45.2 28.1 44.9 largeproportionoftheannotations,inthecorrectorder,with
Table4. Comparisontothestateoftheart: Ourjointmodel approximatesignsegmentation. Forinstance,eventhough
significantlyoutperformsbothCSLR(top)andretrieval(bottom) thebottom-rightexampleinFig.4hasahigherrorrateof75
baselines. For CSLR, note that the automatic spotting annota- WER,ourpredictionscorrectlyidentify4outofthe8ground
tions[4,43]haveaccesstothesubtitlesatinference(unlikeourfully
truthwords,andcatchthemeaningofthesentence.
automatedapproach).WealsocomparetorawISLRoutputsfrom
signclassificationmodelsfrom[4,43,44]withvariousbackbones
(I3DorSwin)andwithvariousvocabularies(2Kor8Kcategories).
Ouroptimalfilteringandpost-processingstrategyatinferenceis However, we also observe several challenges: (i) our
denotedwith†(see.3.3).WenotethatfortheISLRI3D-2Kbaseline model has difficulty predicting several words for a single
without†,westillremoveconsecutiverepetitions. sign,asthe8Ktrainingvocabularyofpseudo-labelsprimarily
comprisesofindividualwords(e.g.thephrase‘longtime’is
associatedtoasinglesigninthe2ndrow-middle-butour
modelpredictstwoseparatesigns‘long’and‘time’,leading
spottingmethodby20WERandthestrongestISLRbaseline
toanextrainsertion)(ii)ourmodelperformanceissensitiveto
by6.2WER.Pleaserefertothesupplementarymaterialfor
thesynonymslist,whichmustbecarefullyconstructedtonot
abreakdownofperformancebasedondifferentsigntypes.
unfairlypenalisepredictions(e.g.inthetopleftexample,‘reg-
Retrievalperformance.AsTab.4shows,ourjointCSLR2 ular’iscountedasasubstitutionsince‘often’isnotpresentin
model outperforms a standard InfoNCE [56] baseline for ‘regular”ssynonymslist)(iii)ourmodelstillstruggleswithvi-
retrievalonallreportedmetrics,withgainsinR@1forboth suallysimilarsignsinBSLwhichcorrespondtodifferentEn-
T2VandV2Tofalmost10points.Onthemorechallenging glishwords(e.g.‘upstairs’and‘rocket’signsarevisuallysimi-
SENT-TESTgalleryof20kvideo-subtitlepairs,ourCSLR2 lar,bothsignedbypointingupwards,3rdrow-middle);(iv)fi-
modelachievesaTop-5accuracyof45.2%forT2Vretrieval. nally,ourmodelismorelikelytofailinrecognisingnamesof
Weobservethatforcaseswherethetargetsentenceisnot placesandpeopleastheseareoftenfingerspelledinBSLand
theTop-1,thetop-retrievedresultsusuallyexhibitsemantic maythereforenotbeinour8Ksignvocabularyofpseudo-
similaritieswiththecorrectsentence,withmultiplecommon labels(e.g.‘islewright’ispredictedas‘america’inthebottom
words(seethequalitativeexamplesinthesupp.mat.). left).Futuredirectionsincludeaddressingsuchlimitations.
8
TG
derP
TG
derP
TG
derP
TG
derP
TG
derP
TG
derP
TG
derP
TG
derP
TG
derP6.Conclusion Vogler,andMeredithRingelMorris. Signlanguagerecog-
nition, generation, and translation: An interdisciplinary
Inthiswork,wedemonstratethatjointlytrainingforCSLR
perspective.InACMSIGACCESS,2019.1
andsignlanguageretrievalismutuallybeneficial.Wecollect
[8] PatrickBuehler,MarkEveringham,DanielP.Huttenlocher,
alarge-vocabularyCSLRbenchmark,consistingof6hours andAndrewZisserman. Longtermarmandhandtracking
ofcontinuoussign-levelannotations. Byleveragingweak forcontinuoussignlanguageTVbroadcasts.InProc.BMVC,
supervision, we train a single model which outperforms 2008.14
strong baselines on both our new CSLR benchmark and [9] Hannah Bull, Michèle Gouiffès, and Annelies Braffort.
existingretrievalbenchmarks. Whileourapproachshows Automaticsegmentationofsignlanguageintosubtitle-units.
substantialimprovements,futureworkincludesincreasing InECCVW,2020.2
the vocabulary size beyond 8K and modeling non-lexical [10] Hannah Bull, Triantafyllos Afouras, Gül Varol, Samuel
signingclassessuchaspointingandgesture-basedsigns. Albanie,LilianeMomeni,andAndrewZisserman.Aligning
subtitlesinsignlanguagevideos.InProc.ICCV,2021.2,5,
Societalimpact.Thetwosignlanguageunderstandingtasks
7,17,18
weaddresscanhavepositiveimplicationsbybridgingthe
[11] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller,
gapbetweenspokenandsignlanguages. Thesetaskscan
HermannNey,andRichardBowden. Neuralsignlanguage
enablemoreseamlesscommunication,contentcreationand
translation.InCVPR,2018.2
consumptionbybreakingdownthelanguagebarriersthatare
[12] NecatiCihanCamgoz, OscarKoller, SimonHadfield, and
prevalenttoday.Atthesametime,theabilitytoautomatically
Richard Bowden. Multi-channel transformers for multi-
search a large volume of signing videos can lead to risks articulatorysignlanguagetranslation.InECCVW,2020.
suchassurveillanceofsigners.Webelievethatthepositives [13] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield,
outweighthenegatives. and Richard Bowden. Sign language transformers: Joint
end-to-end sign language recognition and translation. In
Acknowledgements. ThisworkwasgrantedaccesstotheHPC
CVPR,2020.2
resourcesofIDRISundertheallocation2023-AD011013569made
[14] NecatiCihanCamgoz,BenSaunders,GuillaumeRochette,
byGENCI.TheauthorswouldliketoacknowledgetheANRproject
MarcoGiovanelli,GiacomoInches,RobinNachtrab-Ribback,
CorVisANR-21-CE23-0003-01.
andRichardBowden.Content4allopenresearchsignlanguage
translationdatasets.arXiv,2021.1,3
References
[15] Joao Carreira and Andrew Zisserman. Quo vadis, action
[1] Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, recognition?anewmodelandthekineticsdataset.InCVPR,
Andreas Stergioulas, Georgios Th Papadopoulos, Vassia 2017.5,7,12
Zacharopoulou, George J Xydopoulos, Klimnis Atzakas, [16] KaLeongCheng,ZhaoyangYang,QifengChen,andYu-Wing
DimitrisPapazachariou,andPetrosDaras.Acomprehensive Tai. Fully convolutional networks for continuous sign
studyonsignlanguagerecognitionmethods.arXiv,2020.3 languagerecognition.InECCV,2020.2
[2] SamuelAlbanie,GülVarol,LilianeMomeni,Triantafyllos [17] YitingCheng,FangyunWei,BaoJianmin,DongChen,and
Afouras,JoonSonChung,NeilFox,andAndrewZisserman. Wen Qiang Zhang. CiCo: Domain-aware sign language
BSL-1K:Scalingupco-articulatedsignlanguagerecognition retrievalviacross-lingualcontrastivelearning.InCVPR,2023.
usingmouthingcues.InProc.ECCV,2020.2,7 2,3
[3] SamuelAlbanie,GülVarol,LilianeMomeni,Triantafyllos [18] KearsyCormier,AdamSchembri,andBencieWoll.Pronouns
Afouras, Andrew Brown, Chuhan Zhang, Ernesto Coto, andpointinginsignlanguages.Lingua,137:230–247,2013.13
NecatiCihanCamgö;z,BenSaunders,AbhishekDutta,Neil [19] RunpengCui,HuLiu,andChangshuiZhang.Adeepneural
Fox,RichardBowden,BencieWoll,andAndrewZisserman. framework for continuous sign language recognition by
Signerdiarisationinthewild.InTechnicalReport,2021.2 iterativetraining.IEEETransactionsonMultimedia,2019.2
[4] Samuel Albanie, Gül Varol, Liliane Momeni, Hannah [20] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Toutanova.Bert:Pre-trainingofdeepbidirectionaltransform-
BencieWoll,RobCooper,AndrewMcParland,andAndrew ersforlanguageunderstanding.InNAACL,2018.5,7
Zisserman. BOBSL:BBC-OxfordBritishSignLanguage [21] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk
dataset.arXiv,2021.1,2,3,5,6,7,8,14,16 Weissenborn,XiaohuaZhai,ThomasUnterthiner,Mostafa
[5] Vassilis Athitsos, Carol Neidle, Stan Sclaroff, Joan Nash, Dehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,
AlexandraStefan,AshwinThangali,HaijingWang,andQuan JakobUszkoreit,andNeilHoulsby.Animageisworth16x16
Yuan. Largelexiconproject:Americansignlanguagevideo words:Transformersforimagerecognitionatscale.InICLR,
corpusandsignlanguageindexing/retrievalalgorithms. In 2021.7
LREC,2010.2 [22] Philippe Dreuw, Jens Forster, Thomas Deselaers, and
[6] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. HermannNey.Efficientapproximationstomodel-basedjoint
Frozenintime:Ajointvideoandimageencoderforend-to-end trackingandrecognitionofcontinuoussignlanguage.InIEEE
retrieval.InProc.ICCV,2021.3 International Conference on Automatic Face and Gesture
[7] DanielleBragg,OscarKoller,MaryBellard,LarwanBerke, Recognition,2008.3
PatrickBoudreault,AnneliesBraffort,NaomiCaselli,Matt [23] Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti
Huenerfauth, Hernisa Kacorri, Tessa Verhoef, Christian Ghadiyaram,KennethDeHaan,FlorianMetze,JordiTorres,
9andXavierGiro-iNieto. How2Sign: ALarge-scaleMulti- [41] HuaishaoLuo, LeiJi, MingZhong, YangChen, WenLei,
modalDatasetforContinuousAmericanSignLanguage. In NanDuan,andTianruiLi. Clip4clip: Anempiricalstudy
CVPR,2021.1,2,3 of clip for end to end video clip retrieval and captioning.
[24] AmandaDuarte,SamuelAlbanie,XavierGiró-iNieto,and Neurocomputing,508:293–304,2022.3
Gül Varol. Sign language video retrieval with free-form [42] LilianeMomeni,GülVarol,SamuelAlbanie,Triantafyllos
textualqueries.InCVPR,2022.2 Afouras,andAndrewZisserman. Watch,readandlookup:
[25] AbhishekDuttaandAndrewZisserman.Theviaannotation Learningtospotsignsfrommultiplesupervisors. InProc.
softwareforimages,audioandvideo.InProc.ACMM,2019. ACCV,2020.2,7
6,12 [43] LilianeMomeni,HannahBull,KRPrajwal,SamuelAlbanie,
[26] MichaelFilhol. Elicitationandcorpusofspontaneoussign Gül Varol, and Andrew Zisserman. Automatic dense
languagediscourserepresentationdiagrams.InLREC,2020.5 annotationoflarge-vocabularysignlanguagevideosa.InProc.
[27] ValentinGabeur,ChenSun,KarteekAlahari,andCordelia ECCV,2022.2,5,6,7,8,14,16
Schmid. Multi-modal transformer for video retrieval. In
[44] KRPrajwal,HannahBull,LilianeMomeni,SamuelAlbanie,
ECCV,2020.3
Gül Varol, and Andrew Zisserman. Weakly-supervised
[28] ThomasHanke.HamNoSys-representingsignlanguagedata
fingerspellingrecognitioninbritishsignlanguagevideos.In
inlanguageresourcesandlanguageprocessingcontexts. In
Proc.BMVC,2022.2,3,5,7,8,14,16
LRECWorkshopproceedings:Representationandprocessing
[45] FilipRadenovic,AbhimanyuDubey,AbhishekKadian,Todor
ofsignlanguages,2004.5
Mihaylov,SimonVandenhende,YashPatel,YiWen,Vignesh
[29] JieHuang,WengangZhou,QilinZhang,HouqiangLi,and
Ramanathan,andDhruvMahajan.Filtering,distillation,and
WeipingLi.Video-basedsignlanguagerecognitionwithout
hard negatives for vision-language pre-training. In arXiv,
temporalsegmentation.InAAAI,2018.2,3
2023.4
[30] PeiqiJiao,YuecongMin,YananLi,XiaotaoWang,LeiLei,
[46] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,
andXilinChen. CoSign: Exploringco-occurrencesignals
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
inskeleton-basedcontinuoussignlanguagerecognition. In
Askell,PamelaMishkin,JackClark,GretchenKrueger,and
ICCV,2023.2
Ilya Sutskever. Learning transferable visual models from
[31] Hamid Reza Vaezi Joze and Oscar Koller. MS-ASL: A
naturallanguagesupervision.InICML,2021.3,7
large-scale data set and benchmark for understanding
AmericanSignLanguage.InBMVC,2019.1,2 [47] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
[32] DiederikPKingmaandJimmyBa. Adam: Amethodfor SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
stochasticoptimization.arXiv,2014.12 PeterJLiu. Exploringthelimitsoftransferlearningwith
[33] Sang-KiKo,ChangJoKim,HyedongJung,andChoongsang aunifiedtext-to-texttransformer. TheJournalofMachine
Cho. Neural sign language translation based on human LearningResearch,21(1):5485–5551,2020.3,5,13,16,17
keypointestimation.Appl.Sci.,2019.3 [48] KatrinRenz,NicolajStache,SamuelAlbanie,andGülVarol.
[34] PhilippKoehn,FranzJ.Och,andDanielMarcu. Statistical Signsegmentationwithtemporalconvolutionalnetworks.In
phrase-basedtranslation.InProceedingsofthe2003Human ICASSP,2021.2,6,12
Language Technology Conference of the North American [49] KatrinRenz,NicolajStache,NeilFox,GülVarol,andSamuel
Chapter of the Association for Computational Linguistics, Albanie. Sign segmentation with changepoint-modulated
2003.1 pseudo-labelling.InCVPRW.IEEE,2021.2,12,13
[35] OscarKoller,JensForster,andHermannNey.Continuoussign [50] Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally
languagerecognition: Towardslargevocabularystatistical Reynolds, and Kearsy Cormier. Building the British sign
recognitionsystemshandlingmultiplesigners. Computer languagecorpus.LanguageDocumentation&Conservation,
VisionandImageUnderstanding,141:108–125,2015.1,2,3,6 7:136–154,2013.3
[36] DongxuLi,CristianRodriguezOpazo,XinYu,andHongdong
[51] AdamSchembri,JordanFenlon,RamasRentelis,andKearsy
Li. Word-leveldeepsignlanguagerecognitionfromvideo:
Cormier. BritishSignLanguageCorpusProject: Acorpus
Anewlarge-scaledatasetandmethodscomparison.InWACV,
ofdigitalvideodataandannotationsofBritishSignLanguage
2019.1,2
2008-2017(ThirdEdition),2017.5,13
[37] JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi.
[52] BowenShi,DianeBrentari,GregShakhnarovich,andKaren
BLIP:Bootstrappinglanguage-imagepre-trainingforunified
Livescu.Open-domainsignlanguagetranslationlearnedfrom
vision-language understanding and generation. In ICML,
onlinevideo.InEMNLP,2022.3
2022.3,7
[53] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan
[38] SongLiu,HaoqiFan,ShengshengQian,YiruChen,Wenkui
Liu.MPNet:Maskedandpermutedpre-trainingforlanguage
Ding,andZhongyuanWang. Hit: Hierarchicaltransformer
understanding.NeurIPS,2020.16,17
withmomentumcontrastforvideo-textretrieval. InICCV,
2021.3 [54] ChenSun,AustinMyers,CarlVondrick,KevinMurphy,and
[39] YangLiu,SamuelAlbanie,ArshaNagrani,andAndrewZisser- CordeliaSchmid. Videobert: Ajointmodelforvideoand
man.Usewhatyouhave:Videoretrievalusingrepresentations
languagerepresentationlearning.InICCV,2019.3
fromcollaborativeexperts.InProc.BMVC,2019.3 [55] ValerieSutton.Lessonsinsignwriting,1990.SignWriting.5
[40] ZeLiu,JiaNing,YueCao,YixuanWei,ZhengZhang,Stephen [56] AäronvandenOord,YazheLi,andOriolVinyals. Repre-
Lin,andHanHu. Videoswintransformer. InCVPR,2022. sentationlearningwithcontrastivepredictivecoding. arXiv
3,5,7,12 preprintarXiv:1807.03748,2018.4,7,8
10[57] GülVarol,LilianeMomeni,SamuelAlbanie,Triantafyllos
Afouras,andAndrewZisserman.Readandattend:Temporal
localisationinsignlanguagevideos. InProc.CVPR,2021.
1,2,7
[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanN.Gomez,ŁukaszKaiser,andIllia
Polosukhin.Attentionisallyouneed.InNeurIPS,2017.2
[59] U.vonAgris,M.Knorr,andK.Kraiss. Thesignificanceof
facialfeaturesforautomaticsignlanguagerecognition.In8th
IEEEInternationalConferenceonAutomaticFaceGesture
Recognition,2008.3
[60] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Ac-
tionCLIP: A new paradigm for video action recognition.
arXiv:2109.08472,2021.3
[61] FangyunWeiandYutongChen.Improvingcontinuoussignlan-
guagerecognitionwithcross-lingualsigns.InICCV,2023.2
[62] R.B.WilburandA.C.Kak. PurdueRVL-SLLLAmerican
signlanguagedatabase.TechnicalReport,2006.3
[63] AoxiongYin,TianyunZhong,LiTang,WeikeJin,TaoJin,
andZhouZhao.Glossattentionforgloss-freesignlanguage
translation.InCVPR,2023.2
[64] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mojtaba
Seyedhosseini,andYonghuiWu.Coca:Contrastivecaptioners
areimage-textfoundationmodels.arXiv,2022.3
[65] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint
sequence fusion model for video question answering and
retrieval.InECCV,2018.3
[66] HuaiwenZhang,ZihangGuo,YangYang,XinLiu,andDeHu.
C2ST:Cross-modalcontextualizedsequencetransductionfor
continuoussignlanguagerecognition.InICCV,2023.2
[67] ShilinZhangandBoZhang.Usingrevisedstringeditdistance
tosignlanguagevideoretrieval.In2010SecondInternational
Conference on Computational Intelligence and Natural
Computing,pages45–49.IEEE,2010.2
[68] BenjiaZhou,ZhigangChen,AlbertClapés,JunWan,Yanyan
Liang,SergioEscalera,ZhenLei,andDuZhang.Gloss-free
signlanguagetranslation: Improvingfromvisual-language
pretraining.InICCV,2023.2
[69] Hao Zhou, Wen gang Zhou, Weizhen Qi, Junfu Pu, and
HouqiangLi.Improvingsignlanguagetranslationwithmono-
lingualdatabysignback-translation.InCVPR,2020.1,2,3,6
[70] RonglaiZuoandBrianMak.C2SLR:Consistency-enhanced
continuoussignlanguagerecognition.InCVPR,2022.2
11APPENDIX pre-trained weights3. The input videos are of resolution
224×224,withthepixelvaluesnormalizedbytheImageNet
meanandstandarddeviationasalsodonein[40]. Weonly
A.Implementationdetails 12 perform two data augmentations: (i) random horizontal
flipping,(ii)randomcolorjitterbyscalingtheoriginalpixel
B.CSLR-TEST 12 valuesbyarandomfactorbetween[0.8,1.2]. Westartwith
B.1.Annotationtool . . . . . . . . . . . . . . . . 12 theKinetics[15]pretrainedweightsandtrainforISLRwith
B.2.Datasetstatistics . . . . . . . . . . . . . . . 13 abatchsizeof256distributedacross4NVIDIAA40GPUs
overaperiodof3days.WeusetheAdamoptimizerwiththe
C.Experiments 14 defaultparameters,exceptforthelearningrate.Thelearning
rateislinearlywarmeduptoamaximumof1×10−3over
C.1.CSLRbaselines . . . . . . . . . . . . . . . . 14
8000iterations,followedbyacosinedecayschedule. We
C.2.OracleCSLRperformance . . . . . . . . . . 15
stoptrainingwhenthemodel’svalidationaccuracy(Top-1
C.3.CSLRperformancebreakdownandanalysis . 15
andTop-5)doesnotimprovefor2epochs.
D.Ablations 16
B.CSLR-TEST
D.1.Textencoder . . . . . . . . . . . . . . . . . 16
D.2.RetrievalonSENT-TEST . . . . . . . . . . . 16 Inthissection,weprovidefurtherdetailsontheannotation
D.3.SynonymsforCSLR . . . . . . . . . . . . . 16 collection procedure for CSLR-TEST (B.1) and dataset
D.4.WeightingoflossesonCSLRperformance . . 17 statistics(B.2).
D.5.Subtitlealignment . . . . . . . . . . . . . . 17
B.1.Annotationtool
E.QualitativeExamples 18 In Fig. A.1, we show two screenshots of the web-based
E.1.CSLR . . . . . . . . . . . . . . . . . . . . . 18 annotation tool used for collecting CSLR-TEST. It is
E.2.Retrieval . . . . . . . . . . . . . . . . . . . 18 based on the VIA video annotation software [25, 48, 49].
Specifically,westartfromVIA-SLA4andextenditforour
A.Implementationdetails purposes(byaddingthesigning-alignedsubtitle,adjusting
thedropdownvocabulary,andremovingtheTop-1signclass
We provide additional implementation details to what is predictionfromasignrecognitionmodel).
describedinSec.3ofthemainpaper. Annotatorsareprovidedwithavideosequencewith20
CSLR2trainingdetails.Weperformthecontrastivetraining signing-alignedsubtitlesentencesspanningroughly2-3min-
withbatchsize512andlearningratelr=5×10−5 using utes(thistakesabout1hourtoannotatedensely).Thesubti-
Adam[32]optimizer. TheHN-NCElossusestemperature tlesaredisplayedatthetopofthevideo.Annotatorsaregiven
τ = 0.07, α = 1.0 and β SignRet = 0.5, β SentRet = 1.0. We aninitialsetofsignintervals(shownatthebottomofthetool
useλ SignRet=0.0075,λ SentRet=0.90toweighthedifferent withacoloredtimeline)fromanautomaticsignsegmentation
terms in our joint loss. Training takes less than 20 hours method[49],butareinstructedtorefinethesesignboundaries
on4NVIDIA32GBV100GPUs. Wechoosethemodel (orremove/addsignintervals)ifnecessary. Foreachsign
checkpointthatperformsbestforT2VR@1onamanually interval,theannotatorsenterfree-formtextwhichbestcorre-
alignedsubsetofvalidationsubtitles. spondstothemeaningofthesign,bytakingintoaccountcon-
Data augmentation is performed on both the video and textofthesurroundingvideo.Theannotatorsareencouraged
textside.Onthetextside,randomwordsfromthesubtitles toprioritiseassigningwordsthatappearinthecorresponding
aredroppedusingtwoprobabilities. First,wechoosewith subtitle.Forexample,inFig.A.1a,thesigning-alignedsub-
probabilityp =0.8whetherwordswillbedroppedin titleis‘Manji,areyoulyingintheway?’andthehighlighted
sub,drop
agivensubtitle.Second,insubtitlesselectedbytheprevious grayintervalisannotatedwiththesubtitleword‘lying’.We
step,wedropeachwordwithprobabilityp =0.4.On furthershowadropdownmenuforeachsign(withthefullEn-
word,drop
thevideoside,weperformdroppinginasimilarfashionto glishvocabularyofthesubtitlewordsintheBOBSLdataset,
droppingwordsinsubtitles,anddropfeaturesinsequences togetherwithasearchfunctionality),withthesubtitlewords
withtwoprobabilities: p =0.8andp =0.5. listedatthetopofthelist,toenablefasterannotation(see
seq,drop frame,drop
Wealsotemporallyshiftsubtitleswitharandomoffset,which Fig.A.1b).BesidesassigningtheEnglishword(s)correspond-
isuniformlysampledintherange±0.5seconds. ingtoasign,theannotatorsindicatethetypeofsignwhen
ModelsummaryisdisplayedinTab.A.1(pleasealsorefer appropriate.Forexample,asshownontheleftofFig.A.1a,
toFig.2ofthemainpaper).
3https://github.com/SwinTransformer/Video-Swin-
Video-Swin-Tinytraininghyper-parameters.Wetrainthe
Transformer
videobackbonetoperformisolatedsignlanguagerecognition 4https://github.com/RenzKa/VIA_sign-language-
onshortvideoclipsof16frames,startingfromKinetics-400 annotation
12Notation Description Input Output
VSign Video-Swin-Tiny(acrossT clips) T×16×224×224×3 T×768
enc
VSent Transformer Encoder with 6 encoder layers, 8 attention B×F×768 B×F×768
enc
headsand768hiddendimensions
T Pre-trainedT5[47]("t5-large")encoder B×W B×1024(sentence-level)
enc
B×W×1024(word-level)
FC Twolinearprojectionsfrom768-dspaceto256-dspace B×F×7680 B×F×256
Twolinearprojectionsfrom1024-dspaceto256-dspace B×F×1024 B×F×256
TableA.1. Modelsummary:F isthemaximumnumberofframesforallvideosequencesinabatchofsizeB. Zero-paddingisapplied
toinputs.W isthemaximumnumberoftokensobtainedaftertokenisationofBsentences.Zero-paddingisalsoappliedhere.
(b)Zoomondropdownmenubeforeanyentry
(a)Finalsignannotations
FigureA.1.Annotationtool:WeshowtwoscreenshotsofthewebbasedannotationtoolusedforcollectingCSLR-TEST.(a)Theannotators
use(i)theright-handsideofthetooltoviewthesigningvideoandalignedEnglishsubtitle,(ii)thecoloredtimelineatthebottomofthetool
toadjust/add/removesignintervals,(iii)theleft-handsidetowritefree-formtexttoassignEnglishword(s)tosignintervals,aswellasthesign
type(forexample,*Pforpointing)whenappropriateand(iv)adropdownmenuforeachsignwiththevocabularyspannedbysubtitlewords
inBOBSL.Wenotethatonthebottomcoloredtimeline,somesignintervalsappearasifunlabelledsimplybecausethereisnotenoughspaceto
fittheannotationtext(howevertheseareshownonthelefthandsideofthetool).Thesegmentsarefirstinitialisedfromautomaticsignsegmen-
tation[49]andarerefinedbyannotators.(b)Whennoannotationisenteredyet,subtitlewordsaredisplayedatthetopofthedropdownmenu.
thethirdsignisannotatedas‘you*P’meaningthesignfor work.Wehighlightthatunknown,i.e.unrecognisablesigns
‘you’isapointingsign,andthefifthsignisannotatedas‘hey denotedby*U,arealsofilteredoutfortheCSLRevaluation
*G’meaningthesignfor‘hey’isagesturesign.Thefulllist sincetheyarenotassociatedtoalexicalword. InFig.A.2,
ofsigntypesannotatedisgiveninthefollowingsection. weshowseveralexamplespersigntype.
Wearealsoawarethatour*Ppointingannotations’def-
B.2.Datasetstatistics
initiondiffersfromstandardpointingasseeninthelinguistic
BOBSL-CSLR. In Tab. A.2, we show statistics for our literature(e.g.BSL-Corpus[51]).Annotatorsareencouraged
collectedCSLR-TESTdata.Wealsocollectannotationsina to mark lexical signs that contain the pointing handshape
similarwayforasmallproportionofthetrainandvalidation (such as the sign for ‘rocket’ which is signed with index
splits. fingerpointingupwards)evenwhentheyarenotpronominal
Signtypes. AsmentionedinSec.4ofthemainpaper, in reference[18]. Thiscanpotentiallybebeneficialinfuture
additiontolexicalsignsthathaveanEnglishwordequivalent, worktorecognisetheroleofpointingincontext.Ourpointing
awiderangeofsigntypesareannotated.Thisismainlymoti- annotationsarethereforeasupersetofthelinguisticdefinition.
vatedbysignsthatcannoteasilybeassociatedtolexicalwords: Weapproximatethelinguisticpointingannotationscontain-
pointingsignsorfingerspelledsignsaresuchexamples. inglocativepointingorpronominalreference[18](shown
InTab.A.3,wepresentthedifferentsigntypeannotations inparenthesisinTableA.2)bycountingpointingannotations
collected and their occurrence in CSLR-TEST. We note (i)withoutanylexicalwordassociatedtoitor(ii)annotations
that for the CSLR evaluation, we only use signs that are associated to words in the following list: ‘me’, ‘this’, ‘I’,
associatedtowords(orphrases),filteringoutthenon-lexical ‘here’,‘in’,‘that’,‘you’. Thesewordsweredeterminedby
specialsigntypessincetheseareoutsidethescopeofthis inspectingthemostfrequentwordsassignedto*Psigns.We
13#sentences #hours #subtitle subtitle #glosses gloss #signtype
#swords svocab. vocab. #aa.annot.
all 7134 8.9 81.3K 9.0K 47.7K 7.1K 17.1K
train 1463 1.7 15.0K 3.1K 08.7K 2.6K 03.4K
val 1153 1.3 13.4K 2.7K 06.6K 2.1K 02.0K
test 4518 6.0 52.9K 6.9K 32.4K 5.1K 11.7K
TableA.2.BOBSL-CSLR:WeshowstatisticsforourcollectedCSLR-TESTdata(numberofsubtitlesentences,durationinhours,number
andvocabularyofsubtitlewords,numberandvocabularyofannotatedglosses,numberofannotatedsigntypes).Wealsocollectannotations
inasimilarwayforasmallproportionofthetrainandvalidationsplits.
Typeofpartially-lexical Description Shortform #occurences
ornon-lexicalsigns inCSLR-TEST
Namesign countriesorpeoplethat *N 0075
haveawell-knownBSLsign
Facialexpressionsign meaningfulfacialexpression *FE 0252
Slangsign signcorrespondingtoslangword *S 0258
Timelinesign signswhichrelatetotime(present,past, *T 0324
future,calendarunits,actioncontinuity)
Unknownsign unrecognisablesigns *U 0680
Gestures meaningfulbodyposes *G 876
thatnon-signersalsodo
Fingerspelling signinglettersoftheBSL *FS 1340
alphabettospellaword
Depictingsigns signswhichdescribeshape, *D 2569
size,movementorhandling
Pointing signwithindexfinger *P (2477*P1)4179
orflathand
Total 0011K
TableA.3. Signtypes:WepresentthedifferentsigntypeannotationscollectedandtheiroccurrenceinCSLR-TEST. Wealsoprovidea
briefdescriptionofeachsigntypeandtheirannotationshortform.SeeFig.A.2forvisualexamples.Noteweaddinparenthesis,thesubset
ofpointingannotationsreferredtoas*P1inSectionB.2,toapproximatethelinguisticpointingsuchaspronominalreferencing.
refertothissubsetof*Psignsas*P1,andtherestas*P2. C.Experiments
Annotation duration. In Fig. A.3, we show three plots
onthedurationofCSLR-TESTannotations. First,theleft C.1.CSLRbaselines
plotshowsthehistogramofdurations: weseethatalarge
portionoftheannotationslastbetween0.4and0.6seconds,
whichisinaccordancetotheestimatesofpreviousworks[8]. InTab.A.4,weprovideamoredetailedversionofTab.4of
Second,themiddleplotshowsthedurationdistributionper themainpaper. Inparticular,wegiveresultswith/without
signtype;weseethatfingerspelledsigns(*FS)lastlonger our post-processing filtering † on ISLR-based methods
than other sign types, potentially because fingerspelling from [4, 43, 44], and show the impact of thresholds on
involvesmultiplehandshapes(oneperletter)toexpressa spotting-based methods from [4, 43] that use subtitles.
word.Ontheotherhand,pointingsigns(*P1)seemtohave We see that without the post-processing heuristics, the
shorter duration than others. Third, the right plot shows ISLR-basedmethodsandourCSLR-basedmethoddonot
theproportionofsigntypesperduration, andweseethat obtainreasonableresults,mainlyduetothenoisefrommany
purelylexicalsignsdominateotherannotationtypes.Wealso false positives. Our model performs the best both in the
observethattheproportionoffingerspelledanddepicting settings(i)withand(ii)withoutpost-processing,amongst
signsincreaseswithlongerduration. alltheapproachesthatdonotusesubtitle-cues.
14Sign Type Illustrative Examples Sign Type Illustrative Examples
Star signs
(Standalone
images) Pointing sign No sign
(*P) (*NS)
index (self/*P) index (*P) index (*P) Three examples of no signing (*NS)
Depicting sign Timeline sign
(*D) (*T)
Depicting handling of steering wheel (drive/*D) (ago/*T) (years/*T) (continue/*T)
Fingerspelling Facial expression sign
(*FS) (*FE)
(A/Allen/*FS) (M/Mummy/*FS) (R/Roller/*FS) (*FE) (good/*FE) (yes/*FE)
Gestures Name sign
(*G) (*N)
(have/*G) (whoo-hoo/*G) (*G) (Britain/*N) (France/*N) (Scotland/*N)
FigureA.2.Signtypeexamples:ForeachsigntypeannotationinTab.A.3,weshow3illustrativesamples.
FigureA.3. StatisticsonCSLR-TESTannotations’duration:Weshow(left)thehistogramofdurationsforallannotations;(middle)
thekerneldensityestimationofdurationforeachannotationtype;and(right)thedistributionoverannotationtypesfordifferentdurations.
Ldenotespurelylexicalsigns.Seetextforinterpretation.
C.2.OracleCSLRperformance C.3.CSLRperformancebreakdownandanalysis
We conduct an oracle experiment to determine an upper- In Tab A.6, we report the CSLR performance breakdown
bound for: “What is the best performance we can obtain by excluding specific sign types from both ground truth
if we remove all the false positives predictions from our and prediction timelines, i.e. removing corresponding
modelafterpost-processing?". Todothis,wecomparethe frames. We further examine the oracle performance by
post-processedpredictionsofourbestmodelwiththewords assumingweperfectlyrecognisethewordsthatcorrespond
intheground-truthsequenceandremovetheonesthatare to certain sign types, such as pointing and fingerspelling.
falsepositives.InTab.A.5,weseethatwecanobtainalarge Specifically,weinvestigatetheimpactof(i)all*annotations,
improvementinscores,buttheWERisstillrelativelyhigh. (ii) pointing *P annotations, and (iii) fingerspelling *FS
Thisindicatesthatourmodelshouldrecognisemoresigns annotations.Weseethatinanidealscenariowherepointing
inthesequencesforittolowertheWERevenmore. or fingerspelled annotations are perfectly predicted, the
15WER↓ mIOU↑
Subtitle-basedauto.annots.[4][M D A ] 115.8 13.0
.5 .7 .0
Subtitle-basedauto.annots.[4][M D ] 093.8 07.1
.8 .8
Subtitle-basedauto.annots.[43][M∗D∗A P EN] 090.7 20.0
.8 .8 .0 .5
Subtitle-basedauto.annots.[43][M∗D∗A P ] 085.5 18.1
.8 .8 .0 .5
Subtitle-basedauto.annots.[43][M∗D∗A P ] 085.9 15.3
.7 .9 .4 .2
ISLRI3D-2K[4] 453.5 08.6
ISLRI3D-2K[4]† 082.5 18.1
ISLRI3D-8K[43] 343.6 14.9
ISLRI3D-8K[43]† 074.7 27.0
ISLRSwin-8K[44] 293.9 17.6
ISLRSwin-8K[44]† 071.7 30.1
CSLR2(Ours) 201.4 22.4
CSLR2(Ours)† 065.5 35.5
TableA.4. AnalysisonCSLRbaselines:†denotestheoptimalsubsetoftheannotationsthatgivesthehighestperformance. Thevalues
without†areobtainedusingtherawmodelpredictionsandcollapsingcontinuousrepetitions.Weobservethatperformingpost-processing
isessentialtoobtainreasonableresults.OurmodelgivesaclearimprovementoverthecurrentbestCSLRmethod.
D.Ablations
WER↓ mIOU↑
CSLR2(Ours) 65.5 35.5 In this section, we provide complementary ablations to
what is done in Sec. 5. of the main paper. We first look
Oracle 61.0 42.3
at the impact of changing the frozen text backbone, from
TableA.5.Oracleanalysis:Wecomputeoracleresultsbyremoving pre-trained T5 model to pre-trained MPNet [53], on both
all false positives (i.e. words in the post-processed predicted
CSLR and retrieval (Sec. D.1). We then show additional
sequencethatarenotintheground-truthsequence). Weseethat
retrievalresultsobtainedon20kunseenSENT-TESTsubtitles
despiteremovingfalsepositives,theWERisstillrelativelyhigh,
(Sec. D.2). Next, we show the impact of evaluating with
implyingthatweeithermisswordsintheground-truthsequence
synonymsattesttimeonCSLRperformance(Sec.D.3).We
orwepredictthemattheincorrectlocation.
thenprovideablationson:(i)lossweights(Sec.D.4),(ii)and
subtitlealignment(Sec.D.5).
D.1.Textencoder
Filter Oracle
InTab.A.7,weexperimentwithreplacingthefrozenT5[47]
SignType WER↓ mIOU↑ WER↓ mIOU↑
textencoderwithMPNet[53].WefirstseethatoptingforT5
CSLR2(Ours) 65.5 35.5 - - insteadofMPNetyieldsperformancegainsonallreported
metrics.Moreover,weobservethatourjointtraininghelps,
*FS 64.9 36.5 54.4 48.0
forbothtextencoders,improvingover(i)thestrongISLR
*P 65.0 35.7 56.0 47.3
BaselineforCSLR,aswellas(ii)thesoleretrievaltraining
*P/*FS 64.2 36.9 50.7 50.6
overall.
all* 64.3 37.0 49.3 51.7
TableA.6.CSLRperformancebreakdown:Weexperimentwith
D.2.RetrievalonSENT-TEST
(i)filteringoutcertainannotationtypesfrombothgroundtruthand
InTab.3,ofthemainpaper,weprovideablationsbymea-
predictiontimelines(Filtercolumn),and(ii)replacingpredictions
suringperformanceon2KSENT-VALsentences.InTab.A.8,
withgroundtruthlabelsfortheseannotations(Oraclecolumn).We
wereportthesameablationson20KSENT-TESTsentences.
observethatpointingandfingerspelling,inparticular,contribute
Thesameconclusionsholdonthismorechallenging(larger)
toasignificantWERreduction(-14.8).
retrievalbenchmark.
D.3.SynonymsforCSLR
Wehaveshownthroughaseriesofqualitativeexamplesthe
overallCSLRperformanceissubstantiallyimproved,leading importanceofhavingamorecomprehensivesynonymslist
toasignificantWERreductionof−14.8(65.5vs50.7),and in the evaluation process. Here, we look at the impact of
evenmoreifall*signtypesarecorrectlyrecognised(49.3). consideringsynonymsascorrectwhenevaluatingforCSLR
16CSLR T2V V2T
TextBackbone SentRet SignRet WER↓ mIoU↑ F1@{0.1,0.25,0.5}↑ R@1↑ R@5↑ R@10↑ R@1↑ R@5↑ R@10↑
ISLRBaseline 71.7 30.1 40.0 37.9 27.2 - - - - - -
MPNet ✓ ✗ - - - - - 44.7 64.9 70.6 43.0 64.4 69.9
T5 ✓ ✗ - - - - - 50.5 69.5 75.1 49.7 69.7 74.7
MPNet ✓ ✓ 66.3 34.9 46.4 45.3 36.3 46.1 66.2 72.2 44.3 65.1 71.8
T5 ✓ ✓ 65.5 35.5 47.1 46.0 37.1 51.7 69.9 75.4 50.2 69.1 74.7
TableA.7. Impactofthetextencoder:Weexperimentwiththechoiceofthetextencoderinourmodel. Morespecifically,wecompare
T5(t5-large)[47]andMPNet(all-mpnet-v2)[53].WeobservegainsonallreportedmetricsbyoptingforT5overMPnet.
T2V V2T
Pool TextEncoder SentRet Sign-levelloss R@1↑ R@5↑ R@10↑ R@50↑ MedR↓ R@1↑ R@5↑ R@10↑ R@50↑ MedR↓
cls MPNet InfoNCE ✗ 16.1 29.5 35.8 51.1 45 14.5 27.8 34.1 50.1 50
cls T5-large InfoNCE ✗ 19.5 35.1 41.8 58.1 24 18.9 33.8 40.7 57.4 25
max T5-large InfoNCE ✗ 21.9 36.9 43.6 59.0 20 20.7 36.0 42.8 58.8 21
cls T5-large HN-NCE ✗ 25.8 42.1 48.9 63.0 12 27.2 43.0 49.2 63.7 11
max T5-large HN-NCE ✗ 28.6 44.2 50.5 64.8 10 27.5 43.6 50.3 64.5 10
max T5-large HN-NCE CE 28.5 44.4 50.8 64.8 10 27.9 43.9 50.4 64.5 10
max T5-large HN-NCE SignRet 29.4 45.2 51.5 64.9 09 28.1 44.9 51.0 64.9 09
TableA.8. RetrievalablationsonSENT-TEST: WeinvestigateseveraldesignchoicessimilartoTab.3ofthemainpaper,butthistime
onthelargergalleryofSENT-TEST(insteadofSENT-VAL).PleaserefertoSec.3.2ofthemainpaperforadescriptionofthisexperiment
alongwiththeinterpretationoftheresults.
CSLR
Model Synonyms WER↓ mIOU↑ F1@{0.1,0.25,0.5}↑
ISLRBASELINE ✗ 78.0 22.6 32.6 30.9 21.6
ISLRBASELINE ✓ 71.7 30.1 40.0 37.9 27.2
CSLR2(Ours) ✗ 73.3 26.5 37.8 36.8 29.4
CSLR2(Ours) ✓ 65.5 35.5 47.1 46.0 37.1
TableA.9. Impactofsynonymsattesttime: Weobservemajor
gains on all reported metrics when considering synonyms as
correctforCSLRevaluation.Thisshowsthathavingapreciseand
exhaustivelistofsynonymsisimportantforCSLR.
in terms of performance. In Tab. A.9, we observe major
gainsonallreportedmetrics,whenconsideringsynonyms
ascorrect,despiteoursynonymslistbeingfarfromperfect FigureA.4. WeightingoflossesonCSLRperformance: We
asexplainedinourqualitativeanalysisinthemainpaper. lookattheimpactofchangingλ SignRetforfixedλ SentRet=0.9. We
observethatWERdecreaseswithlowerλ values.
SignRet
D.4.WeightingoflossesonCSLRperformance
WestudytheimpactoflossweightingonCSLRperformance. D.5.Subtitlealignment
Morespecifically, inFig.A.4, weplotWERperformance
obtainedbyjointlytrainingwithourtwolosses(SentRetand
InTab.A.10,wemeasuretheimpactofusingaudio-aligned
SignRet)againstvaryinglossweightsforthesign-levelloss,
subtitlesinsteadofautomaticsigning-alignedsubtitles[10].
fixingtheweightgiventothesentence-levelloss. Surpris-
We observe a notable gain in retrieval performance using
ingly,weobservethatgivinglessweighttotheCSLR-relevant
automaticsigning-alignedsubtitles(R@1forbothT2Vand
SignRetlossactuallybringsimprovementtoCSLR.
V2T almost doubles). This is expected as audio-aligned
We see that WER decreases with lower λ values. subtitlesarenotnecessarilyalignedwiththesignedcontentin
SignRet
A potential explanation could be that the SentRet loss videos,butinsteadwiththeaudiosoundtrackaccompanying
regularizes the training, as we observed SignRet alone is theoriginalTVbroadcasts.WealsoobserveadropinCSLR
pronetooverfitting. performancewhenusingaudio-alignedsubtitles.
17GT Orig vs Eval
CSLR-TEST T2VSENT-VAL V2TSENT-VAL
Subtitles WER↓ mIOU↑ F1@{0.1,0.25,0.5}↑ R@1↑ R@5↑ R@1↑ R@5↑
Audio-aligned 68.0 33.5 44.5 43.3 33.5 25.2 43.8 27.7 44.2
Signing-aligned[10] 65.5 35.5 47.1 46.0 37.1 51.7 69.9 50.2 69.1
TableA.10. Subtitlealignment: Usingautomaticsigning-alignedsubtitles[10]insteadofaudio-alignedsubtitlesdrasticallyimproves
retrievalperformance,andmoderatelyimprovesCSLRperformance.
GT (orig)
GT (eval)
GT (orig)
GT (eval)
GT (orig)
GT (eval)
Effect of Pred Post-Processing
GT (orig)
GT (eval)
FigureA.5.Groundtruthprocessing:Weprovideexamplesoftheoriginalgroundtruthannotationsascollectedbyannotators(GTorig),
andtheversionweuseatevaluationtime(GTeveal).
WER: 40.0 WER: 45.5 WER: 50.0
GT
Pred
Pred (raw)
Portsmouth is the military heart of this coast,
But these altars were dug up 100 metres away from the temples. and just offshore rests a powerful relic of war. Oh, God, looks who's back.
WER: 54.6 WER: 64.3 WER: 72.7
GT
Pred
Pred (raw)
HayI lf in y go Iu s lp ar ne dfe , r o t no e b oe f o Bn r it to ap in o 'sf bth ese t w wa it ne dr, s uh re fa ind g f o sr p ots. cT lh ae wy s ,a wre h d ice hca tp ho ed ys u: st eh e fy o rh sa cv ae v t ee nn g l ie ng gs a a nn dd p h ra ov tee c t th ine gs e t hla er mg se e f lr vo en st . Today, he's tranquillising an elephant to fit a tracking collar.
WER: 75.0 WER: 80.0 WER: 100.0
GT
Pred
Pred (raw)
Get all the zips redone and replace all the netting, In this museum is the largest collection of Roman altars
which has been ... needed doing for ages. in Britain, 18 in total and all found here at Maryport. Past Bournemouth's boltholes and beaches, to reach Sunderland
FigureA.6.Effectofpost-processingthepredictions:Weshowthatprocessingtheframe-levelsignrecognitionsintosegment-levelones
(asexplainedinSec.3.3ofthemainpaper)effectivelycleansupalargeportionofthenoise.Foreachsample,toprowcorrespondstothe
groundtruth(GT),middlerowtopost-processedCSLRpredictionsfromCSLR2model,andbottomtounfilteredrawpredictions.
E.QualitativeExamples E.2.Retrieval
E.1.CSLR Weshowqualitativeresultsofretrievalperformancebothfor
V2TandT2VsettingsinTab.A.11andTab.A.12,respectively.
Ground truth processing. We show in Fig A.5 several Foreaseofvisualisation,wereportthesubtitlescorrespond-
sequences illustrating how we obtain the ground truth for ingtothesignvideoV.Wenotethattheretrievedsubtitle(in
evaluation,inparticularwiththeremovalof*annotations theV2Tcase)orretrievedvideosubtitles(intheT2Vcase)
andkeepingonlythewordsassociatedtoeachannotation. areveryoftenintheTop-5andthatothertopretrievedresults
Effect of post-processing the predictions. We show areoftensemanticallysimilartothecorrectresult(asshown
in Fig A.6 qualitative results with and without the post- bysimilarwordsinblue),highlightingthestrongcapabilities
processingproceduredescribedinSec.3.3ofthemainpaper. ofourjointsign-video-to-textembeddingspace.
Theseexamplesparticularlyhighlighttheimportanceofthe
post-processingweproposeinourmethod.
Additionalresults.WeshowqualitativeexamplesofCSLR
performanceinFig.A.7.Foreachsigningvideosample,we
showthegroundtruthandpredictedglosses,alongwiththe
Englishsubtitle.WealsoshowtheWERandIOUpersample.
We observe that our model is able to correctly predict an
importantproportionofthegroundtruthannotations.
18WER: 25.0 WER: 28.6 WER: 33.3
Qualitative Examples
Manual subtitles
But how do you get the pests off when it comes to harvest time? I wouldn’t have even seen beauty here, but Well, it’s seawater, it mimics the conditions that they live in.
now with looking at them in this light…
WER: 40.0 WER: 45.5 WER: 46.2
The remote Zagros Mountains in western Iran - You can hear the road behind me, you can hear cars whizzing behind So far, the same as all cheese is made, and after a few minutes, a
home to an animal found nowhere else on earth. me, so it just goes to show you, these aren’t difficult animals to see. remarkable change takes place.
WER: 50.0 WER: 50.0 WER: 50.0
The problem is ever escalating because the price of ivory,
Tell me what the differences are between them and… there’s no sign of the price of ivory decreasing yet. But all their hard work is beginning to pay off.
WER: 65.0 WER: 65.0 WER: 70.0
Once the team have identified which chemicals cause the Now, rural buses are a lifeline for many people, but, as And he believed the more confined your sphere of
distinctive Parkinson’s smell, they can develop a test to Tom’s been finding out, it’s claimed that more and more observation, the more perfect would be your remarks.
find those compounds rather than relying on Joy’s nose. villages are being cut off by the loss of local services.
WER: 72.7 WER: 75.0 WER: 80.0
Slowed down by 40 times, it’s possible to fully appreciate the flexibility
of th lie fs ee i sh i tg rah n-s sp foe re md eh du n inte tors a, a b es aig uh tit f uth l a dt ii ss p e lax yh i ol fa r pa rt ein cg is ia on nd ai nm dp gre rass ci ev .e in They’re all Polynesian tree snails, from French Polynesia. I st t’ es pa m oua tz si in dg e t to h et ih ri n hk o uth sea t a n t o nt i gs ho t l ao nn dg sa eg eo s, ka in ey so ln ike e c to hu isld .
WER: 80.0 WER: 85.7 WER: 85.7
It might seem like a glum future for humans The first effect of the eruptioin was to make the Earth much colder. Its main prey would have been things like
but perhaps not for the planet as a whole. this animal here - the herbivores of the time.
WER: 90.0 WER: 92.9 WER: 150.0
Witnesses in the town said they saw strange objects,
These guests are from America and Saba wants like a cluster of fire, which then dispersed and fell. The good news is the law has been in place for six months now.
to make sure they see the Koitagor lion pride.
FigureA.7. AdditionalqualitativeCSLRresults. Weprovideadditionalexamplesforourmodel’sCSLRpredictions(bottomrowper
sample)onCSLR-TEST,aswellasthethegroundtruth(toprowpersample). AsinFig.4ofthemainpaper,wedisplayWERforeach
example,alongwiththecorrespondingsubtitletoprovidecontextforthereader.
19QueryVideo(illustratedbyitscorrespondingsubtitle) RetrievedSentences
She was deeply involved in farming, and her impassioned Shewasdeeplyinvolvedinfarming,andherimpassioned
campaigningcertainlyruffledafewfeathers campaigningcertainlyruffledafewfeathers.(0.74)
Toencouragethemtodojustthat,farmershavebeencampaign-
inghard(0.72)
Shecelebratedandchampionedrurallife.(0.68)
AndlikeBeatrixPotter,youhavearealinterestinpreserving
thecountryside,especiallyfarms.(0.67)
Visitourwebsiteandtelluswhytheydeservetobeournext
farminghero.(0.66)
LikegettingdresseduponaFridaynight? LikegettingdresseduponaFridaynight?(0.73)
Butthehomecomingcelebrationwon’tbeginuntilmuchlater
thisevening.(0.69)
Thisisthefestivitiesofthenight.(0.67)
Fridaynightin.....withjusttheBBCTwoofus.(0.67)
Itreallydoeslookfurioustonight.(0.66)
But,actually,surprisinglytheyemitfewer It’salljustpurecalciumcarbonate.(0.68)
greenhousegases,uptoabout3.5. Intheend,itcamedowntojustthree.(0.66)
So,welookatalloftheseandbringthemdownintooneunit,
whichwecallkilogramsofcarbondioxideequivalent,which
letsusmakecomparisonsbetweendifferentfoodchains.(0.66)
TheyleftfromNorthWalesonthe3rdofJuly,andtheyheaded
downtheIrishSeaandthenacrosstheBristolChannel,around
Land’sEnd,andtheyrendezvousedwiththeGermantugthe
GladiatorofftheBelgiumcoast.(0.65)
But,actually,surprisinglytheyemitfewergreenhousegases,
uptoabout3.5.(0.65)
And in the course of the afternoon, we’d start singing our Andinthecourseoftheafternoon,we’dstartsingingour
chorusesfromSundayschool. chorusesfromSundayschool.(0.83)
Onewouldstartandthenanothergroupwouldjoininandbefore
longyou’dhavethewholebeachsinging.(0.71)
HalfanhouronSundaymornings.(0.70)
Allthethingsyouimagineyou’ddoonaBritishholidayonthe
beach.(0.67)
Iremember,inIreland,IstoppedonthebanksofGalwayBay
andhadalittlesing.(0.67)
Now,I’vegotnice...marinana...marinarameatballs. Yeah,Imean,you’veprobablygot20mmoffatthere,andwith
theMangalitzathefatissocreamy.(0.68)
Well,yes,ifyou’reafanofmussels,then,Ithinkyou’regoing
toenjoythis.(0.68)
Now,I’vegotnice...marinana...marinarameatballs.(0.67)
Ifyoulookthroughthere,youcanseeMarum,notverywell,
but...(0.66)
I’mgoingwiththechickenpestopasta.(0.66)
TableA.11.QualitativeV2TretrievalexamplesobtainedontheunseenSENT-TESTsentences.Toeasevisibility,wereportthesubtitles
correspondingtothequerysignvideo,alongwiththeirtheTop-5retrievedsentences,aswellastheirsimilarities(intherange0-1).Wordsinthe
retrievedsentencethatareidenticalorsemanticallysimilartothoseinthequeryareshowninblue.Notethetablecontinuesonthenextpage.
20QueryVideo(illustratedbyitscorrespondingsubtitle) RetrievedSentences
Butkeepingsalmoncontainedinlargenumberslikethisdoesn’t Itdoesmakeitdifficult,butthecurrentoptimisesthewelfare
comewithoutcomplications. ofthesalmon.(0.74)
Butkeepingsalmoncontainedinlargenumberslikethis
doesn’tcomewithoutcomplications.(0.72)
We’re pioneers when it comes to farming in rough waters,
becauseit’sprobablythefarmingareawhereyoufarmsalmon
withthestrongestcurrentsintheworld.(0.68)
Thebiggestthreattofarmedsalmonaroundtheworldissealice,
amarineparasite.(0.68)
We’vegotalittlesalmonparrhere,perfect.(0.67)
Butit’sariskworthtaking. Thisissuchariskyoperation!(0.70)
Youknow,it’sexceedinglydangerous.(0.69)
Butit’sariskworthtaking.(0.67)
It’sgoingtobeahugegamble.(0.67)
Itisgoingtobeahugegamble.(0.67)
Wearelookingforwardtomeetherandit’sexcitingbecause I’msoexcitedtomeetnewpeopleeverytimebecauseitgives
we’vebeenworkingreallyhardtogetthecampready. meanopportunitytoshowthemwhatwehavehere.(0.76)
Wearelookingforwardtomeetherandit’sexcitingbecause
we’vebeenworkingreallyhardtogetthecampready.(0.76)
I’mparticularlyexcitedtobeherebecauseit’snotmyfirstvisit
toHeartwoodForest.(0.74)
I’vebeencomingtowatchgatheringsmoreyearsthanIcare
toremember,andI’mstilljustasexcited,andwhenyousee
awholegroupofthembreaktheskyline,gallopingintowards
you,allidentical,it’sfantastic.(0.73)
Ijustfeellikeeverything’skickingoff,andit’squiteexciting.
(0.71)
Inthewild,theyhavetoavoidbeingeatenbypredators. Theyspendmostoftheirlifetryingtoavoidbeingeatenby
variousthings.(0.71)
Someofthesepredators,theyrelyonwhattheycanfind.(0.71)
They’lleatsomeofthem,butthey’llscaremostaway,andthat’s
whyweenduplikethis.(0.70)
Theygettheanimals,thenthey’renotsurewheretograzethem,
thentheythinkit’sourdutytoprovidethemwithsomegrazing.
(0.70)
Inthewild, theyhavetoavoidbeingeatenbypredators.
(0.69)
Enjoythegrassandhappycalving! Andwhentheyarenotrollingtheirballsdownthehillourcattle
aregrazingit.(0.72)
Enjoythegrassandhappycalving!(0.70)
They’resoonbackontrackandheadingtowardstheirsummer
grazing.(0.69)
It’sgoodtoseethemlikethis-thewayawhiterhinomustgraze.
(0.69)
OK,forgrazing,youmean?(0.68)
21QuerySentence RetrievedVideos(illustratedbytheircorrespondingsubtitles)
Well,I’mascientist. Yeah,therereallyis.(0.78)
I’veinvitedatopscientistfromExeterUniversitytothefarm.
(0.74)
So,it’sanastonishingdiscovery,there’snodoubtofthat.(0.73)
Well,I’mascientist.(0.73)
Thereisnomiraculousscienceatworkhere.(0.72)
She was deeply involved in farming, and her impassioned Shewasdeeplyinvolvedinfarming,andherimpassioned
campaigningcertainlyruffledafewfeathers. campaigningcertainlyruffledafewfeathers.(0.74)
Ihavealwaysthoughtitsomewhatoddthatthelady,whohas
aperfectlycompetenthusband,shouldinsistonmanagingevery
detailoffarmsandwoodlandproblemsherself.(0.66)
It’sdrivingmeabsolutelycrazy.(0.66)
Toencouragethemtodojustthat,farmershavebeencampaign-
inghard.(0.65)
Yes,hecansolvecivilengineeringproblems,miningquarrying
problems,butthisisfarming.(0.65)
Theyhadchosenthepatchofsky,theSouthernHole,precisely Soasfaraswecantell,thecosmicmicrowavebackgroundlooks
becauseitwasrelativelyclearofgalacticdust. muchthesameanywherethatwecanobserveitonthesky,and
sothebestplacetoobserveit,tolookforveryfaintsignals,is
whereourowngalaxyandtheemissionfromourgalaxyisthe
faintest.(0.69)
It’s really important, because the telescopes are searching
for exceedingly faint signals, signals that might arise from
gravitationalwavesintheearlyuniverse.(0.68)
Theyhadchosenthepatchofsky,theSouthernHole,pre-
ciselybecauseitwasrelativelyclearofgalacticdust.(0.67)
Johnandhisteamhavechosenaparticularpatchofskytosearch
forevidenceofgravitationalwaves.(0.67)
Infact,itwasimpossiblyeasytoattractthebirds.(0.66)
Now,I’vegotnice...marinana...marinara I’mgoingmeatballsinmarinara.(0.70)
meatballs. Yeah,mushroomy,yeah(0.69)
It’snotmushroomcoloured,it’snude!(0.68)
I’mgoingwiththechickenpestopasta.(0.68)
Now,I’vegotnice...marinana...marinarameatballs.(0.67)
Now,makingbreadisfairlyeasy,becauseyouneedveryfew Now,makingbreadisfairlyeasy,becauseyouneedveryfew
ingredients. ingredients.(0.78)
It’sthischangewhichmakesbreadpossible.(0.77)
Expertsdecidedtheycouldreinventbread.(0.76)
Whydoessupermarketbreadstaysoft,whilehomebakedbread
goesstale?(0.74)
Forhomebakedbreadyouonlyneedflour, waterandyeast.
(0.74)
TableA.12.QualitativeT2VretrievalexamplesobtainedontheunseenSENT-TESTsentences.Forvisibility,wereportthequerysentence
alongwiththesubtitlescorrespondingtotheretrievedsignvideos,aswellastheirsimilarities(intherange0-1).Wordsintheretrievedsubtitle
thatareidenticalorsemanticallysimilartothoseinthequeryareshowninblue.Notethetablecontinuesonthenextpage.
22QuerySentences RetrievedVideos(illustratedbytheircorrespondingsubtitles)
HisloveoftheOrientinspiredhimtotransformBatsford,tearing HisloveoftheOrientinspiredhimtotransformBatsford,
outformalbedsinfavourofwildplantingandexotictrees. tearingoutformalbedsinfavourofwildplantingandexotic
trees.(0.67)
Theorientalplantsandwaterfeatureherewerethebrainchild
ofVictoriandiplomatLordRedesdale.(0.66)
LordRedesdaleconvertedtoBuddhismandhiddenamongst
thetreesareBuddhist-themedbronzes,aJapanesebridgeand
apeacepavilionthatreflectedhisloveoftheculture.(0.66)
I’mrootedtothegroundatBatsfordArboretum,hometoawide
varietyofunusualtreespeciesfromaroundtheworld.(0.66)
Whenyouwanderaroundafairlikethis,onethatissofocusedon
gamehunting,youdon’tnaturallythinkaboutwildlifeconserva-
tion,butI’vebeentoldthatthetwocangohand-in-hand.(0.64)
Anarchipelagoof18islands,200milesnorthwestofShetland, An archipelago of 18 islands, 200 miles north west of
theFaroeIslandsareaself-governingnationwithintheKingdom Shetland, the Faroe Islands are a self-governing nation
ofDenmark. withintheKingdomofDenmark.(0.75)
TheIsleofMull...(0.75)
We’reontheFalklandIslands...(0.72)
Yes,we’vegotLittleEyejustdownthere,MiddleEyeinfront
ofus,andHilbrejustthere(0.71)
Easterislandwasoncecoveredinforest.(0.71)
We’regrowingtomatoesalmostallyearroundnow. We’regrowingtomatoesalmostallyearroundnow.(0.81)
Togetthebestperformancefromthefruitandtheplant,weneed
togivetheplanteverythingthatitneeds.(0.78)
Becauseyouneedextraheattogrowtomatoesoutofseason,
itmakesthemmoreexpensive.(0.78)
JulytoOctoberwerealwaysthemonthstoeattomatoesbut
nowvastheatedgreenhousesmeanwecangrowthembetween
FebruaryandNovember.(0.76)
So,whydidyouchoosetomatoesasthemainmediumforthis
gene?(0.75)
It’s size and quality suggests Maryport was a cult centre, PeoplecametoLymeRegistogofossilhuntingwithMary
drawingpeoplefromfarandwideto Anning.(0.69)
worship. It’ssizeandqualitysuggestsMaryportwasacultcentre,
drawingpeoplefromfarandwidetoworship.(0.68)
This custom is controversial and draws objections from the
widerworld.(0.66)
Butinsidetheearthwormthisactivityismagnifiedtolevelsthat
aretrulymind-blowing.(0.66)
Aspagans,theRomansworshippedmanygodsandspirits.(0.65)
Notjusttheskeleton,butallthesofttissue,youknow,allthe Notjusttheskeleton,butallthesofttissue,youknow,allthe
musclesandthebrainandthefur. musclesandthebrainandthefur.(0.80)
Weveryrarelygetsofttissuepreservationofextinctanimals.
(0.70)
We’reconcernedthattheremightbeaboneinfection.(0.69)
Itcaninsertthatintotheaphidandit’llsuckallthegoodness
out.(0.68)
They’re covered in scratches and scars, they’re covered in
notches,they’recoveredintoothrakes.(0.68)
23