Analogist: Out-of-the-box Visual In-Context Learning with Image
Diffusion Model
ZHENGGU,CityUniversityofHongKongandStateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
SHIYUANYANG,CityUniversityofHongKongandTianjinUniversity,China
JINGLIAO∗,CityUniversityofHongKong,China
JINGHUO∗,StateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
YANGGAO,StateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
ImageColorization ImageDeblurring ImageDenoising Low-lightEnhancement
𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′
𝑨′ 𝑨′ 𝑨′ 𝑨′
ImageEditing ImageTranslation StyleTransfer MotionTransfer
𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′
𝑨′ 𝑨′ 𝑨′ 𝑨′
Skeleton-to-imageGeneration Mask-to-imageGeneration ImageInpainting Object Multiplication
𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′ 𝑨 𝑩 𝑩′
𝑨′ 𝑨′ 𝑨′ 𝑨′
Fig.1. Examplesofin-contextvisualgenerationbyourmethodusingapretrainedStableDiffusionInpaintingmodelaredemonstrated.Withanexampleimage
pair𝐴and𝐴′,illustratingavisualtransformation,andaqueryimage𝐵,ourmethodenhancesthemodel’scapacityforvisualin-contextcomprehension,pro-
ducingareasonableoutput𝐵′thatfollowsthesamevisualpattern.Sourceimages:ImageNet[Dengetal.2009],LOL[Chenetal.2018],InstructPix2Pix[Brooks
etal.2023],TongYiQianWenAPP,UBC-Fashion[Zablotskaiaetal.2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016],DALLE-3[Betkeretal.2023].
VisualIn-ContextLearning(ICL)hasemergedasapromisingresearcharea bytextprompts.Ourmethodisout-of-the-boxanddoesnotrequirefine-
duetoitscapabilitytoaccomplishvarioustaskswithlimitedexamplepairs tuningoroptimization.Itisalsogenericandflexible,enablingawiderange
throughanalogicalreasoning.However,training-basedvisualICLhaslim- ofvisualtaskstobeperformedinanin-contextmanner.Extensiveexperi-
itationsinitsabilitytogeneralizetounseentasksandrequiresthecollec- mentsdemonstratethesuperiorityofourmethodoverexistingapproaches,
tionofadiversetaskdataset.Ontheotherhand,existingmethodsinthe bothqualitativelyandquantitatively.Ourprojectwebpageisavailableat
inference-basedvisualICLcategorysolelyrelyontextualprompts,which https://analogist2d.github.io.
failtocapturefine-grainedcontextualinformationfromgivenexamplesand
CCSConcepts:•Computingmethodologies→Imageprocessing.
canbetime-consumingwhenconvertingfromimagestotextprompts.To
addressthesechallenges,weproposeAnalogist,anovelinference-based AdditionalKeyWordsandPhrases:VisualIn-ContextLearning,Diffusion
visualICLapproachthatexploitsbothvisualandtextualpromptingtech- Models,ImageTransformation
niquesusingatext-to-imagediffusionmodelpretrainedforimageinpainting.
Forvisualprompting,weproposeaself-attentioncloning(SAC)methodto 1 INTRODUCTION
guidethefine-grainedstructural-levelanalogybetweenimageexamples.
Asoneofthemostpopularresearchtopicsintherecentfieldof
Fortextualprompting,weleverageGPT-4V’svisualreasoningcapabilityto
naturallanguageprocessing(NLP),in-contextlearning(ICL)rep-
efficientlygeneratetextpromptsandintroduceacross-attentionmasking
resentsaparadigmwhereinlargelanguagemodels(LLMs)acquire
(CAM)operationtoenhancetheaccuracyofsemantic-levelanalogyguided
theabilitytolearntasksbasedonalimitedsetofdemonstrative
∗JingLiaoandJingHuoaretheco-correspondingauthors. examples[Dongetal.2022].Unlikesupervisedlearning,ICLdirectly
generatespredictionsusingpretrainedLLMs[Brownetal.2020].
Authors’addresses:ZhengGu,guzheng@smail.nju.edu.cn,CityUniversityofHong Thisparadigmoffersaninterpretableinterfaceforinteractingwith
KongandStateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China;
ShiyuanYang,s.y.yang@my.cityu.edu.hk,CityUniversityofHongKongandTianjin LLMsthroughlanguagedemonstrations,mirroringhumandecision-
University,China;JingLiao,jingliao@cityu.edu.hk,CityUniversityofHongKong, makingbylearningthroughanalogiesandsimilarexperiences.ICL
China;JingHuo,huojing@nju.edu.cn,StateKeyLabforNovelSoftwareTechnology,
significantlylowerscomputationalcostsforadaptingmodelstonew
NanjingUniversity,China;YangGao,gaoy@nju.edu.cn,StateKeyLabforNovelSoft-
wareTechnology,NanjingUniversity,China. tasks,makinglanguage-model-as-a-servicefeasibleandenabling
4202
yaM
61
]VC.sc[
1v61301.5042:viXra2 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
practicalapplicationsinlarge-scale,real-worldtaskssuchasma- 2023]thatrelyontime-consumingtextualinversionoptimization,
chinetranslation[Xuetal.2023],informationextraction[Heetal. weproposeutilizingGPT-4V’svisualreasoningcapabilitytoana-
2023],andcomplexityreasoning[Weietal.2022]. lyzethesemantictransformationbetween𝐴and𝐴′ andapplyit
FollowingthesuccessofNLP,researchinvisualIn-ContextLearn- analogicallyto𝐵togenerateatextualdescriptionof𝐵′.Thisisfa-
inghasentereditsembryonicstageofexploration[Baietal.2023; cilitatedbyourwell-designedgraphicalandtextualinstructionsfed
Yangetal.2023a].Specifically,whenthedemonstrationisapairof intoGPT-4V.Furthermore,weintroduceacross-attentionmasking
images𝐴and𝐴′,visualin-contextlearningcanbeconsideredasan (CAM)operationtorestricttheinteractionbetweentextandimage
imageanalogyproblem[Hertzmannetal.2001].Thisinvolvesanalo- tothe𝐵′regiononly,whichensuresthatthetextualpromptmore
gizingtheobservedtransformationfrom𝐴to𝐴′ andapplyingit accuratelyguidesthegenerationof𝐵′.
ontoaqueryimage𝐵,resultingin𝐵′.Thisanalogycapabilityholds Withbothsemantic-level(coarse-grained)andstructural-level
significantpotentialincomputergraphicsandvisiontasks[Caoetal. (fine-grained)contextualinformationrespectivelyprovidedbytex-
2023;Parmaretal.2023;Šubrtováetal.2023].Forexample,asshown tualandvisualpromptingtechniques,ourapproachiscapableof
inFigure1,withjustasinglepairofexampleswithouttrainingona performingawiderangeofvisualtasksinanin-contextmanner,
largedataset,thepretrainedmodelcanperformtasksrangingfrom asillustratedinFigure1.Ourapproachisanout-of-the-boxsolu-
low-leveltaskssuchascolorization,deblurring,denoising,etc.,to tionthatonlyrequiresoneforwardstepofapretraineddiffusion
high-leveltaskssuchasimageediting,imagetranslation,motion model,withouttheneedforfine-tuningoroptimization.Extensive
transfer,etc.VisualICLalsoofferssignificantpotentialinenhancing experimentsandcomparisonsacrossdifferenttaskshaveconfirmed
creativeworkflows.Designerscanleverageamodeltolearndesign thatourmethodoutperformsexistingtraining-basedandinference-
ideassuchascolorthemes,typography,andvisualmotifsfroman basedvisualICLmethods,bothqualitativelyandquantitatively.Our
examplepairandadaptthemanalogouslytodifferentcontents. methodisprimarilydesignedforapplicationswheretheinput𝐴and
ExistingvisualICLworksfallintotwocategories:training-based 𝐴′arespatiallyaligned.Nonetheless,weshowthatitholdspromise
andinference-based.Training-basedmethodstrainthegenerative forapplicationsinmisalignedscenariosaswell.Insummary,our
modelondiversein-contexttasks[Najdenkoskaetal.2023;Wang contributionscanbesummarizedasfollows:
etal.2023a].TheICLcapabilitiesprimarilyexhibittaskssimilarto
• We introduce Analogist, an out-of-the-box approach for
theirtrainingtasksandhavelimitationswhenappliedtounseen
visualin-contextlearningthatutilizesapretraineddiffusion
tasks.Moreover,collectingandorganizingthedataintoin-context
inpainting model along with effective visual and textual
taskformatislaborious.Inference-basedmethodsconductICLvia
promptingtechniques.
appropriatepromptingthemodelduringinference,possessingbetter
• Invisualprompting,weproposeaSelf-AttentionCloning
generalizability.However,existingmethods[Nguyenetal.2023;
(SAC)methodthateffectivelyguidestheimageinpainting
Šubrtováetal.2023]convertthegivenimagesintotextualprompts,
modeltoexploitfine-grainedcontextualinformationinthe
fallingshortintwoaspects.First,thetextualpromptingiscoarse-
2×2gridvisualprompt.
grainedandcannotcoverthedetailedinformationpresentedinthe
• In textual prompting, we propose to efficiently generate
imageexamples.Second,textualinversionfromimagesrequires
textualpromptsusingGPT-4Vandenhancetheaccuracyof
iterativeoptimization,whichisstilltime-consuming.
textualguidancebyintroducingaCross-AttentionMasking
Inthiswork,weproposeAnalogist,anovelinference-basedvisual
(CAM)operation.
ICLapproach,toaddresstheaforementionedchallenges.Weintro-
ducebothvisualandtextualpromptingtechniquesonapretrained
2 RELATEDWORK
text-to-imagediffusionmodel.
2.1 VisualIn-contextLearning
Firstly,weintroduceanovelvisualpromptingtechniquetoover-
comethecoarse-granularityissueintextualprompting.Inspired InspiredbythetaxonomyinDongetal.[2022],wecategorizecur-
byMAEVQGAN[Baretal.2022],weformulatetheICLtaskasan rentvisualin-contextlearningintotwogroups,training-basedand
imageinpaintingtaskbyarrangingtheexemplaryimagepair𝐴and
inference-based, based on the criterion of whether the model is
𝐴′,thequeryimage𝐵,andtheunknownimage𝐵′ina2×2grid.
trainedonin-contexttasks.
Then,weutilizeapretraineddiffusioninpaintingmodeltofillin
theregionof𝐵′.Toguidetheinpaintingprocesswithfine-grained Training-basedMethods. Training-basedmethodstrain(orfine-
visualcontextualinformation,weproposeaself-attentioncloning tune)themodelondiversein-contexttasks.Painter[Wangetal.
(SAC)method.Thismethodclonestheself-attentionmapsbetween 2023b]usespairedinputandoutputimagesasvisualpromptsto
𝐴and𝐵totheself-attentionmapsbetween𝐴′and𝐵′duringthe trainaVisionTransformer[Dosovitskiyetal.2020],whichenables
forwardpropagationofthediffusioninpaintingmodel.Sincethe themodeltolearnandperformawiderangeofvisiontasks.The
self-attentionmapsrepresentsimilaritybetweenpixels,theSAC follow-upworkSegGPT[Wangetal.2023c]extendsthein-context
methodeffectivelyhelpslearnstructural-levelrelationshipsbetween learningcapabilitiesofPainterspecificallyforpreciseandadapt-
𝐴and𝐵,whicharethenappliedto𝐴′togenerate𝐵′analogically. ablesegmentationacrossvariousdomains.Morerecently,several
Inadditiontovisualpromptingofferingstructural-levelguidance, workprogressivelyexhibitstheICLabilityofstate-of-the-artdif-
weincorporatetextualpromptingtooffersemantic-levelguidance fusionmodels[Rombachetal.2022].PromptDiffusion[Wangetal.
byprovidingappropriatetextpromptstotheinpaintingmodel.How- 2023a] introduces ControlNet [Zhang et al. 2023] to tune a pre-
ever,unlikepreviousmethods[Nguyenetal.2023;Šubrtováetal. trainedStableDiffusiononsixmanuallydesignedvision-languageAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 3
tasks.Theproposedmethodisabletogeneralizetosimilar,contex- Ourworkaimstotackletheproblemofimageanalogiesinthe
tuallyrelatedunseentasks.However,itposeschallengeforusersto paradigmofvisualin-contextlearning.Differentfromtraditional
offerdetailedandprecisetextdescriptions.ImageBrush[SUNetal. texture synthesis approaches [Hertzmann et al. 2001; Liao et al.
2023]introducesanovelframeworkforimagemanipulationusing 2017],theanalogyisachievedbypromptingapre-trainedtext-to-
in-contextvisualinstructions,ratherthannaturallanguage.Anaddi- imagediffusionmodelandcanbeappliedtomoreapplicationssuch
tionalpromptencoderisintroducedtotranslatethevisualchanges aslow-leveltasks,manipulationtasks,andvisiontasks.
depictedintheexampleimagesintotextfeaturestoguidethein-
2.3 Prompt-basedImageEditing
paintingmodel.ImageBrushisbuiltonadiffusion-basedinpainting
modelandtrainedonseveralvisiondatasets.Theabovetraining- Recentmultimodalapproacheshavedemonstratedsuperiortext-
basedmethodsnecessitatetheconstructionofhigh-qualityanddi- imagefeaturealignmentcapabilities[Lietal.2022;Radfordetal.
versetasks,makingthepipelinelaboriousandinflexible.Meanwhile, 2021],leadingtoaseriesofworksonprompt-basedimageediting.
thetesttasksshouldideallybearsomesimilaritytothetraining PreviousGAN-basedmethodsperformmanipulationinthelatent
tasks,suggestingopportunitiesforimprovinggeneralizability. spaceviaGANinversion[Baykaletal.2023;Patashniketal.2021;
Xiaetal.2022].Morerecentmethodsutilizetext-to-imagediffusion
Inference-basedMethods. Insteadoftuningthemodelparameters, modelstoattainleadingoutcomes[Brooksetal.2023;Caoetal.2023;
inference-basedmethodsinspirethemodel’sunderstandingonthe Parmaretal.2023].However,thesemethodsstruggletodoimage
givendemonstrationsduringinferencetime.Amongthem,MAEVQ- analogytasksincetheytaketextualdescriptionsasinput,which
GAN[Baretal.2022]innovativelyproposesavisualprompting isnotsufficientlyintuitiveandaccuratetodepictdetailsrelatedto
formatofinpaintingthemissingpatchina2×2grid-likeimage. theimagestructure.Incontrast,ourworktakesapairofimages
Themodelispre-trainedonfiguresfromcomputervisionpapers asdemonstrationinput,utilizesself-attentiontoprovidestructure-
whicharetypicallyinaregulargridpatternandemergeswithICLca- relatedinformation,andautomaticallyacquiresthecorresponding
pability.However,thegenerationeffectsarenotentirelysatisfactory textualdescriptionthroughGPT-4V.
duetolimitationsindatasetsizeandmodelcapacityincomparison
3 PRELIMINARY
withthelatestdiffusionmodels.VISII[Nguyenetal.2023]considers
thedemonstrationasimagesbeforeandafterimageediting.This SinceourapproachutilizesapretrainedStableDiffusioninpainting
approachestimatestheeditinginstructionbasedonapretrained model,webrieflyreviewlatentStableDiffusioninSection3.1as
text-basedimageeditingmodel[Brooksetal.2023],producingre- wellastheStableDiffusioninpaintingmodelinSection3.2.
sultswithhigherquality.However,reverse-engineeringthetextual
descriptionofthedifferencesbetweentwoimagesthroughoptimiza- 3.1 LatentDiffusionModels.
tionremainstime-consuming.What’smore,bytransferringvisual
DenoisingDiffusionProbabilisticModels(DDPM)[Hoetal.2020]
informationtocoarse-grainedtext,thegenerationprocessismerely
areaclassofgenerativemodelsthatgraduallyconvertrandomnoise
drivenbytextualdescriptions.Theroleofvisualpromptingisnot
intostructureddatathroughaseriesofreversediffusionstepsbased
fullyleveraged,leadingtoinaccuratecontextualunderstanding.
onaMarkovchain.LatentDiffusionModels(LDM)likeStableDif-
Ourworkfallsintothecategoryofinference-basedmethodsand,
fusion(SD)[Rombachetal.2022]enhancesDDPMbyemployingan
notably,eliminatestheneedforadditionaloptimizationsteps.In- encoder𝐸tomaphigh-dimensionaldata𝑥 intolower-dimensional
steadofsolelyrelyingontextualprompts,ourapproachleverages latentspace𝑧 = 𝐸(𝑥).ThegenerationofStableDiffusioncanbe
bothtextualandvisualprompting.Thisallowsustorespectivelyun- guidedbyanadditionaltextembedding𝑐(𝑦)encodedbyCLIP[Rad-
derstandsemantic-levelandstructural-levelcontextualinformation fordetal.2021]andatextprompt𝑦.Duringtraining,anUNetmodel,
forvisualICL.Besides,ourmethodutilizesGPT-4Vtogettextual parameterizedby𝜃,isoptimizedtoeliminatethenoise𝜖introduced
promptsinsteadoftextualinversion. into𝑧 𝑡:
2.2 ImageAnalogies L=E 𝑧∼𝐸(𝑥),𝑦,𝜖∼N(0,1),𝑡 (cid:2) ∥𝜖−𝜖 𝜃(𝑧 𝑡,𝑡,𝑐(𝑦))∥2 2(cid:3). (1)
Definedby𝐴:𝐴′ ::𝐵:𝐵′,thegoalofimageanalogies[Hertzmann
Duringinference,arandomlysampledlatent𝑧
𝑇
∼N(0,1)ispro-
etal.2001]istofindan“analogous”image𝐵′thatrelatesto𝐵inthe gressivelydenoisedthroughthemodeltoproduceacleanlatent
samewayas𝐴′relatesto𝐴.Suchideacanbeextendedinvarious representation𝑧 0by
waysofimagesynthesis[Diamantietal.2015;Jamriškaetal.2019; 1 (cid:20) 1−𝛼 𝑡 (cid:21)
Liaoetal.2017;Yuanetal.2024].Recently,DIA[Šubrtováetal. 𝑧 𝑡−1= √ 𝛼
𝑡
𝑧 𝑡 − 1−√ 𝛼¯𝑡𝜖 𝜃 (𝑧 𝑡,𝑡,𝑐(𝑦)) , (2)
2023]investigatestheimageanalogiestaskwithDiffusionmodel.
ThismethodestimatestheCLIPfeaturesofthegivenimages.The where𝛼¯𝑡 =(cid:206) 𝑖𝑡 =1𝛼 𝑡.Subsequently,thecleanlatentisfedintothe
CLIPfeaturesareinjectedintoapretrainedtext-to-imagediffusion decodertoobtainthegeneratedimage𝐷(𝑧 0).
modeltoprovidein-contextguidance.DIAiscapableofexecuting
3.2 StableDiffusionInpaintingModel
example-basedimageeditingthatencompassescomplex,higher-
levelcontextualorstructuralrelationships.However,sincethegoal WeapplyourmethodoverthepretrainedStableDiffusioninpainting
ofCLIPistoalignimageandtextspaces,theestimatedfeaturesare model,whichisfine-tunedtoboastsanadditionalfeatureofimage
highlevelandstruggletocapturedetailedimageinformation. inpainting. The forward process of the inpainting pipeline is as4 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
Visual Prompting Self-Attention Cloning (SAC) Cross-Attention Masking (CAM) Textual Prompting
𝑨 𝑨′ 𝑨′
𝑨 𝑨′ ❌ 𝑨 𝑨′
addmarks
❌ add arrows 𝑩 𝑩′
𝐸(𝐼) Close-up of a
𝑩 Input 𝐼 tiger’s face
concat Please help me with the image analogy
𝑨 𝑨′ ❌ task: take an image A and its transfor-
𝑩 clone result 𝑩′ mation A’, and provide any image B to
produce an output B’ that is analogous
: Self-attention Relations ✔ to A’. Or, more succinctly: A : A’ :: B :
B’. You should give me the text prompt
of image B’ with no more than 5 words.
Mask
𝑩 𝐼′ 𝑩 Stable Diffusion Inpainting
Based on the provided image transforma-
𝑨 𝑨′ tion from A to A’, where a domestic cat
Encoder in image A is transformed into a tiger in
image A’, the analogy for image B which
fD orD wI aM rd ？ concat 𝑥 " SAC CAM SAC CAM 𝑥 "#$ 𝑥 % f i cme ha aat ru g ar e ce ts eB r’a i s f td e icao stm u tore i s n tt hgic e a cc a a ttt i ig ns eh r Bo .u wld it hr e ss iu mlt i lain r
Thus, the text prompt for image B’ would
UNet 𝑩 Output 𝑩′ be: “Close-up of a tiger's face”
𝐸(𝐼′) 𝑥
!
Fig.2. OverviewoftheproposedAnalogist.Avisualdemonstrationisdefinedbyanexamplepair𝐴(womanholdingacat)and𝐴′(thesamewomanholdinga
tiger).Givenanewimage𝐵(anothercat),weformatthesethreeimagesintoa2×2gridandtacklethisproblembyfillthemissingimageviaapretrained
StableDiffusioninpaintingmodel.WeemployGPT-4Vtoprovideapropertextdescription(i.e.,“close-upofatiger’sface”)tofurtherguidetheinpainting
process.Duringtheprocessofmodelinference,Self-AttentionCloning(SAC)andCross-AttentionMasking(CAM)areintroducedtoencouragethemodel
concentrateonthevisualandtextualprompts,thusenhanceitsin-contextlearningcapacities.Sourceimage:InstructPix2Pix[Brooksetal.2023].
follows:
1 (cid:20) 1−𝛼 𝑡 (cid:21)
𝑧 𝑡−1= √ 𝛼
𝑡
𝑧 𝑡 − 1−√ 𝛼¯𝑡𝜖 𝜃 (𝑧 𝑡,𝑡,𝑐(𝑦),𝐸(𝐼 𝑚),𝑀) , (3)
TheUNetisupdatedtoincludefiveextrainputchannels–four
dedicatedtotheencodedmasked-image𝐸(𝐼 𝑚)andoneforthemask
𝑀itself.Thesetwoextrainputsareconcatedwith𝑧 𝑡 tofedintothe
UNettopredictthenoiseateachtimestep.
4 METHOD 𝑨 𝑩
ThegoalofICListoencouragepretrainedmodeltolearntasks Fig.3. Visualizationoftheattentionrelationships.Givenananchorpoint
givenonlyafewexamplesintheformofdemonstration[Dong onimage𝐴(showninred,green,andbluecolors),wecalculatetheatten-
et al. 2022]. Specific to the image domain, the demonstration is tionvaluesbetweenthispointandallregionsofimage𝐵.Soucreimage:
definedasanexampleimagepair𝐴and𝐴′,where𝐴′istheresult InstructPix2Pix[Brooksetal.2023].
obtainedbyapplyingacertainvisualeffectortransformationto
𝐴.Givenanewqueryimage𝐵,themodelisexpectedtoapplythe
sameeffectto𝐵,thuscreatinganewimage𝐵′,sothat𝐴:𝐴′ ::𝐵: 4.1 VisualPrompting
𝐵′[Hertzmannetal.2001].Thisprocessdemonstratesthemodel’s
Tointroducefine-grainedstructural-levelvisualguidanceinthein-
understanding and replication of visual transformations from a
contextinferenceprocess,weconstructavisualpromptintheform
givendemonstrationtoanewcontext,exhibitingtheICLability.
ofa2×2grid-likeimageforthepretrainedinpaintingmodel,and
AsillustratedinFigure2,toaddressthisissue,weapproachit
providevisualcontextualinformationbycloningtheself-attention
frombothvisualstructural-level(Section4.1)andtextualsemantic-
associationsbetweenthegivenimages.
level(Section4.2)perspectives.Forvisualprompting(redregion
inFigure2),weformulatetheinputimagesintoa2x2gridimage, 4.1.1 2×2-gridPrompting. Imageinpaintingmodelsfillinunknown
utilizingapretraineddiffusioninpaintingmodeltofillinthemissing areasofanimagebasedonitsknownregions,whichnaturallyaligns
regioninSection4.1.1.Tointroducemorefine-grainedvisualinfor- withtheconceptofICL.AsshowninFigure2,totakeadvantage
mation,weproposeSelf-AttentionCloning(SAC)inSection4.1.2. ofthisproperty,wefirstrearrangetheinputimages𝐴,𝐴′,and𝐵
Fortextualprompting(blueregioninFigure2),GPT-4Viselabo- intoasingle2×2grid-likeimage,denotedas𝐼.Image𝐵ispasted
ratedtoprovidesemantic-levelguidancetothegenerationprocess tothebottomrightcornerofthegridimage,gettingimage𝐼′.We
inSection4.2.1.Tofostersemanticcorrespondencebetweenthe extractthefeaturesofthepastedimage,𝐸(𝐼′),andaddnoisetoitvia
inpaintedimageandthetextprompt,weproposeCross-Attention diffusionforwardprocess,gettingtheinitial𝑥 𝑇.Toalignwiththe
Masking(CAM)inSection4.2.2. interfaceofthepretrainedmodel,amaskimage𝑀issimultaneously
Encoder
redoceDAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 5
ℎ𝑤×𝑐 Self-AttentionCloning(SAC) ℎ𝑤×𝑐 Cross-AttentionMasking(CAM)
𝑄 ℳ 𝐴",𝐵" ≔ℳ(𝐴,𝐵)(𝑠 𝑄 ℳ 𝐴 ≔0; ℳ 𝐴" ≔0; ℳ (𝐵)≔0
! ! ! ! !
ℎ𝑤×ℎ𝑤 ℎ𝑤×𝐿
ℎ𝑤 ℎ𝑤 ℎ𝑤 ℎ𝑤 ℎ𝑤 ℎ𝑤 ℎ𝑤
ℎ𝑤×𝑐 × × 𝐿×𝑐 ×𝐿 ×𝐿 ×𝐿
4 4 4 4 4 4 4
𝐾 𝑠 Softmax 𝐾 0 0 0
ℳ!(𝐴,𝐵) ℳ!(𝐴′,𝐵′) ℳ! 𝐴 ℳ! (𝐴′) ℳ! (𝐵)
ℎ𝑤×𝑐 ℳ ! 𝐿×𝑐 ℳ !
𝑉 𝑍 𝑉 𝑍
: Assignment operation : Assignment operation
Fig.4. Detailedillustrationofself-attentioncloning(SAC).Thesubself- Fig.5. Detailedillustrationofcross-attentionmasking(CAM).Thesub
attentionmapM𝑠(𝐴′,𝐵′)issetasthevalueofM𝑠(𝐴,𝐵),denotingcloning cross-attentionmapbetweentextembeddingandregions𝐴,𝐴′,and𝐵are
therelationbetween𝐴and𝐵tothatof𝐴′and𝐵′. settozero,makingthesemanticguidancemorefocusedonregion𝐵′.
generated.Inthismask,thebottomrightregionisentirelyones, where𝑠isacoefficientusedtobalancethedegreeofpreservingthe
whiletheremainingregionsarezeros.Ateachtimestep𝑡,thelatent structureofimage𝐵andthedegreeofapplyingtransformations.
𝑥 𝑡 ∈R𝑏×4×ℎ×𝑤 isconcatenatedwiththefeature𝐸(𝐼) ∈R𝑏×4×ℎ×𝑤 Weperformtheself-attentioncloningoperationbeforesoftmaxto
andmask𝑀 ∈R𝑏×1×ℎ×𝑤 ,constructingtheinputoftheUNet.By preventtheoriginalself-attentionresultsbeingexcessivelyaffected.
establishingsucha2×2-gridprompt,weencouragethemodelto
fillinthecontentofunknownarea(𝐵′)basedonthecontextual 4.2 TextualPrompting
regions(𝐴,𝐴′,and𝐵)intheimage.
Cloningself-attentioneffectivelymanagesbasicin-contextvisual
guidance,yetthediffusionmodel’scelebratedtext-to-imagefeature
4.1.2 Self-AttentionCloning. Thekeyofin-contextlearningisto
remainsunderutilizedtoprovidesemantic-levelguidance.Toad-
recognizetaskinstructionfromthegivendemonstration.Previous
dressthis,weutilizeGPT-4V’svisualreasoningabilities[Yangetal.
inference-basedworkextractthevisualinstructionsthroughcross-
2023a]toprovidesemanticguidancetotheinpaintingmodel.
attentioninjection,whichcouldonlyprovidescoarseandimprecise
guidance.Differently,weintroducefine-grainedstructural-aware 4.2.1 GPT-4VPrompting. WepromptGPT-4Vtogenerateacoher-
contextualinformationviaself-attention. enttextdescriptiontoaidtheinpaintingprocess.Consideringthe
OurmotivationcomesfromanobservationthattheDiffusion consistencyoftheentirepipeline,wefeedthewhole2𝑥2grid-like
modelaccuratelyconstructsassociationsbetweendifferentpositions imagedirectlyintoGPT-4Vwithapre-designedproblemDescrip-
intheknownareasthroughself-attention.Weshowthevisualiza- tion,asdepictedinFigure2.Weemploytwocarefully-designed
tionofself-attentionrelationsinFigure3.Wecalculatetheattention graphicalinstructionstomakeiteasierforGPT-4Vtounderstand
valuesbetweenkeysemanticpositions(e.g.,theeyes,mouth,and thetask.Firstly,inspiredby[Yangetal.2023b],weplacealetter
flowerinthefirstrowandthespire,building,andthebackground mark(𝐴,𝐴′,𝐵,𝐵′)inthetop-leftcornerofeachgridcell.Secondly,
grasslandinthesecondrow)in𝐴andallregionsin𝐵.Theresults weaddprominentarrowmarkers(→)between𝐴and𝐴′,aswell
demonstratethatthevisualassociationsbetweenimagescanbe asbetween𝐵and𝐵′,toindicatetherelationshipbetweenthetwo
accuratelyidentifiedthroughself-attention,whichcouldbemore images.Theseapproachesintroducestructured,easilyidentifiable
accuratethanabstractsemantictextpromptsasguidance.Basedon referencepoints,facilitatingmoreeffectiveandaccurateresponses
thisobservation,weproposetouseself-attentionasastructural- toqueriesinvolvingvisualcontent.Then,GPT-4Visaskedtoper-
levelpriortoguidethein-contextgenerationprocedurebymodu- formananalogyandoutputthetextdescriptionfor𝐵′.Finally,we
latingself-attentioninUNet.WeshowanexampleinFigure2of useGPT-4V’sanswerasthesemantic-levelpositivetextpromptto
translatingacatintoatiger.Therelativepositionalrelationship reinforcethemodel’sICLcapabilities.Wealsoemploynegativetext
ofthetigerin𝐵′andthetigerin𝐴′shouldbeconsistentwiththe prompts(i.e.,“Messy,Disordered,Chaotic,Cluttered,Haphazard,
relativepositionalrelationshipofthetwocatsin𝐵and𝐴. Unkempt,Scattered,Disheveled,Tangled,Random”)topreventthe
Wepresentdetailedillustrationoftheproposedself-attention diffusionmodelfromgeneratingirregularandillogicalresults.These
cloning(SAC)inFigure4.Denotetheimagefeaturebeforeself- twopromptsworkcooperativelytoinjectsemantic-levelguidance
attentionas𝐹 𝑖 ∈Rℎ×𝑤×𝑐 .Theself-attentionmapM𝑠 ∈Rℎ𝑤×ℎ𝑤 intothemodel.
recordsthesimilarityofeachpositionontheentireimagewith
otherpositions,whichalsoincludesthesimilaritiesbetween𝐴and 4.2.2 Cross-AttentionMasking. Notethatthepromptobtainedfrom
𝐵,aswellasbetween𝐴′and𝐵′.Weextractthesubself-attention
GPT-4Visspecificallytailoredfor𝐵′,yetthetextualguidanceim-
mapM𝑠(𝐴,𝐵) ∈ Rℎ 4𝑤×ℎ 4𝑤 andassignitsvaluetoM𝑠(𝐴′,𝐵′) ∈ pactstheentireimagethroughcross-attentionintheUNet.Toad-
dress this issue, we propose cross-attention masking (CAM): in
Rℎ 4𝑤×ℎ 4𝑤
: cross-attentionlayers,werestrictthetextinteractsonlywiththe
M𝑠(𝐴′,𝐵′):=M𝑠(𝐴,𝐵)·𝑠, (4) regioncorrespondingto𝐵′.Specifically,supposethecross-attention
xamtfoS6 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
𝑨 𝑨′ 𝑩 GPT-4VPrompt MAEVQGAN PromptDiffusion DIA VISII Analogist
Black
maracas to
wooden
Red chair
detailed
painting
Lemonwith
black
background
Illuminated
closet interior
Woman with
drawn
mustache
Old man's
skull
Abstract
mountain
watercolor
painting
Person in
dress standing
Classroom
desk and
chair
Soccer ball in
forest
Fig.6. Comparisonwithotherbaselinemethods,eachrowindicatesonetask,giventheinputimagepair𝐴,𝐴′andqueryimage𝐵.SinceMAEVQGAN[Bar
etal.2022]doesnottaketextasinputandDIA[Šubrtováetal.2023]andVISII[Nguyenetal.2023]estimatethetextpromptsbyextraoptimization,thetext
promptsgeneratedbyGPT-4VpromptingareonlyusedbyPromptDiffusion[Wangetal.2023a]andAnalogist.Sourceimages:ImageNet[Dengetal.2009],
LOL[Chenetal.2018],InstructPix2Pix[Brooksetal.2023],UBC-Fashion[Zablotskaiaetal.2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016].
noitaziroloC
rulbeD
esioneD
tnemecnahnE
gnitidE
noitalsnarT
refsnarTelytS
egami-ot-notelekS
egami-ot-ksaM
gnitniapnIAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 7
ImageColorization Deblur
“Dog in color” “Colorized lakeside house” “Eagle perched clearly” “Mannequin wearing trench
Denoise ImageEditing coat”
“Dog with longer coat” “BlurryImageSharpened” “Hippoinsunsetpainting” “Smallcabinbyriver”
ImageTranslation StyleTransfer
“Motorcycle silhouette at sunset” “Monochrome close-up cat” “Mill sketch style” “Caricatured man smiling”
Mask-to-imageGeneration ImageInpainting
“Realistic room corner” “Colored shapes to kitchen” “Duckenteringwater” “Jeepmovedforward”
Fig.7. ExamplesofresultsgeneratedbytheproposedAnalogistondifferenttasks.Ineachexample,theimage𝐴and𝐴′areshowninthefirstcolumn,the
image𝐵andgeneratedimage𝐵′isshowninthesecondandthirdcolumn.ThetextpromptgeneratedviaGPT-4Visshownbeloweachexample.Source
ImageNet[Dengetal.2009],InstructPix2Pix[Brooksetal.2023],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016].
mapasM𝑐 ∈ Rℎ𝑤×𝐿 ,where𝐿denotesthelengthoftextembed- missingareasspecifiedbyamask.TheUNetarchitecturecontains
ding.Werepurposetheindicesofdifferentregionsidentifiedinthe 16blocks,eachconsistsofonecross-attentionandoneself-attention.
previousSACprocessandsettheattentionvaluesbetweenthetext WeperformSACandCAMfromlayer3to10atalltimestepsinthe
andregionsotherthan𝐵′(i.e.,𝐴,𝐴′,and𝐵)tozero: diffusionprocess.Thescaleforclassifier-freeguidanceissetat15.
M𝑐(𝐴):=0;M𝑐(𝐴′):=0;M𝑐(𝐵):=0. (5)
Thecoefficientforself-attentioncloning𝑠 =1.3inallexperiments
exceptforskeleton-to-imagewhere𝑠 = 1.4.Allexperimentsare
AsillustratedinFigure5,weutilizetheattentionmappost-softmax, conductedonanRTX3090GPU.
aswearecompletelyobstructingtherelationshipbetweenthetext
andregionsoutsideof𝐵′.
5.2 EvaluationSetup
AsfortheattentionmapindexinginSACandCAM,duetothe
Dataset. Weemploythefollowingthreemajorcategories,totaling
fixedpositionsofeachimage,weareabletopre-calculatethein-
tentaskstoevaluatetheeffectivenessoftheproposedmethodquan-
dicesrequiredforextractingthenecessarysub-attentionmaps(e.g.,
M𝑠(𝐴,𝐵) and M𝑐(𝐴)) from the entire attention map. This pre- titatively:low-leveltasks,manipulationtasks,andmorechallenging
visiontasks.
determinationstreamlinestheentirepipeline,enhancingitssim-
plicityandefficiency. • Low-level tasks. We test out method on four low-level
tasks,i.e.,imagecolorization,imagedeblurring,imagede-
5 EXPERIMENTS
noising,andimageenhancement.Forthefirstthreetasks,we
5.1 ImplementationDetails samplein-the-wildimagesfromImageNet[Dengetal.2009]
and apply corresponding transformations (i.e., grayscale,
WeimplementourworkinPyTorch[Paszkeetal.2019].Theinput
images𝐴,𝐴′,𝐵areresizedto256×256andspatiallycombinedto gaussianblurry,addingnoise).Forimageenhancement,we
usetheLOLdataset[Chenetal.2018],whichconsistsof
forma512×512grid-likeimage.WeusedapubliclyavailableStable
Diffusioninpaintingmodel1.ThemodelisinitializedwithSD1.2 low/normal-lightimagepairs.Wecollect100samplesfor
eachlow-leveltask.
andtrainedoninpaintingtask,thereforecapableofinpaintingthe
• Manipulationtasks.Weselectthreekindofimagema-
1https://huggingface.co/runwayml/stable-diffusion-inpainting nipulationtasks(i.e.,imageediting,imagetranslation,and8 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
𝑨 𝑨′ 𝑩 ImageBrush Analogist nothavepairedtextdescriptions,weinputthesametextprompts
asoursthatobtainedfromGPT-4VintoPromptDiffusion[Wang
etal.2023a]toensureafaircomparison.
EvaluationMetrics. Weevaluatethemodel’sICLcapacityviathe
CLIPdirectionsimilaritybetweenthedemonstrationandthepro-
ducedresults.WeutilizetheImageEncoderfromCLIPtoextractthe
imagefeaturesof𝐴,𝐴′,𝐵,andthegenerated𝐵′.Then,wecalculate
thecosinesimilaritybetweenthedirectionalchangesfrom𝐴to𝐴′
andfrom𝐵to𝐵′.Thehigherthesimilarity,themoreconsistentthe
inferred𝐵′iswiththetransformationeffectsappliedto𝐴.Dueto
thegenerationdiversityofdiffusionmodels,wedonotcompare
pixel-levelmetricslikeSSIMandPSNR.Instead,wecalculateFID
betweenthegenerated𝐵′imagesandthegroundtruthimages.In
ordertoobtainmoreaccurateresult,wemergeallthedataineach
majorcategorytocalculatetheFIDvaluesforcomparison.
5.3 QualitativeResults
Figure6presentscomparisonofourmethodwiththebaselineson
allofthetentasks.ForMAEVQGAN[Baretal.2022],duetothelack
ofspecificstructuringoftrainingdataintotheformoftasksand
theabsenceoftextualguidance,thequalityofthegeneratedoutput
isrelativelypoor,especiallyforhigh-leveltaskslikemanipulation.
ForPromptDiffusion[Wangetal.2023a],thebiasintrainingtask
(i.e.,image-to-HED,HED-to-image)significantlyimpactstheICL
Fig.8. ComparisonwithImageBrush[SUNetal.2023].TheresultofIm-
generalizabilityofthemodel.Asshownintheexampleofdeblurand
ageBrushinthefirstthreetasksarefromtheoriginalpaperandtheresult
translation,theresultstendtoproducelinedrawingssimilarwith
ofthelastthreetasksareprovidedbytheauthorsofImageBrush.Source
images:InstructPix2Pix[Brooksetal.2023],UBC-Fashion[Zablotskaiaetal. edgedetectionresults.Fortheothertwoinference-basedmethods
2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016]. DIA[Šubrtováetal.2023]andVISII[Nguyenetal.2023],theycon-
ductin-contextlearningthroughtheestimatedtextsolely,making
itdifficultprovidesufficientlyaccuratepromptinformationtogen-
styletransfer)fromtheCLIP-filteredsubsetprocessedby eratethecorrectresults.Ourmethodtakesintoaccountguidanceat
InstructPix2Pix[Brooksetal.2023].Sincethedatasetiscon- boththevisualandsemanticlevels,whichcanproduceaccurateand
structedforgeneralimageediting,wesplitthesamplesinto reasonablein-contextoutputs.NoticethatGPT-4Vpromptingmay
threetasksbasedonthekeywords.Instructionscontaining strugglewithvisiontasks,givingcoarsedescriptions.Forexample,
“add”,“remove”areconsideredasimageeditingtasks,those “personindressstanding”intheskeleton-to-imageexampledoes
with“make,turn,change”areimagetranslationtasks.Each notgivethedetaileddescriptionthatwhatposethewomanshould
manipulationtaskcontains200samples. bestandingin.However,thankstotheproposedSACoperation,
• Visiontasks.Weselectthreemorechallengingvisiontasks thesestructure-awarein-contextinformationcanbestillcaptured
forevaluation:skeleton-to-imagegenerationfromUBC-Fas- andutilizedtoproducethecorrectresults.Figure7showsfurther
hion[Zablotskaiaetal.2019],mask-to-imagegeneration resultsofAnalogistonthesetasks,demonstratingtheICLcapabil-
fromScanNet[Daietal.2017],andimageinpaintingfrom itiesofourproposedmethod.Morerandomlyselectedresultsare
DAVISdataset[Perazzietal.2016].Eachtaskcontains200 showninsupplementarymaterials.
samples. Additionally,weconductedacomparisonwithImageBrush[SUN
etal.2023].SinceImageBrushhasnotreleasedthecode,thecompar-
Bydevelopingthesethreemajorcategories,wecanevaluateifthe
isonismadeintherangeoftrainingtasksofImageBrush.Asshown
pretrainedmodeliscapableofunderstanding,processing,andutiliz-
inFigure8,itisworthnotingthatourmethodismoreeffectiveat
ingvisualinformationacrossvariouslevels,whilealsoevaluating
preservingthedetailsinImage𝐵.Especiallyinmanipulationtasks,
itsabilitytogeneralizeeffectivelyacrossthesetasks.
thecoloroftheaurora,thecontourstructureoftheanimals,and
Baselinemethods. Wetakefourmethods,MAEVQGAN[Baretal. thetextureontheclothingarebetterpreserved.Thisisbecauseour
2022],PromptDiffusion[Wangetal.2023a],DIA[Šubrtováetal. proposedvisualandtextualpromptingcontainmoredetailedin-
2023]andVISII[Nguyenetal.2023]asourbaseline.Allbaseline contextinformation.Onthethreevisiontasks,weachievecompeti-
methodsutilizetheofficialimplementationsandcheckpointspro- tiveresultswithImageBrush.Notethatourmodelisnotfine-tuned
vided.SincePromptDiffusion[Wangetal.2023a]requirestextas specificallyforthesetasks,whichdemonstrateoursuperiorityof
partofitsinput,butmostofthetestdatasets(suchaslow-level)do in-contextgeneralizabilityasaninference-basedmethod.
gnitidE
noitalsnarT
refsnarTelytS
egami-ot-notelekS
egami-ot-ksaM
gnitniapnI
revo
aroruA“
dliw
detaminA“
eutats
eznorB“
ni
namoW“
gniteem
lanigirO“
derotseR“
”sniatnuom
ywons
”sdinac
”tceffe
”sserd
tnagele
”enecs
moor
”egami
ognimalfAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 9
Table1. QuantitativecomparisonondifferentcategoryoftaskswithpreviousICLapproaches.WereportthecosinesimilaritybetweenCLIPdirectionfrom𝐴
to𝐴′andfrom𝐵to𝐵′.Highersimilarityrepresentsmorecontextuallyappropriategeneratedresults.Thebestresultsarehighlighted.
Category Task MAEVQGAN PromptDiffusion DIA VISII Analogist
Colorization 0.0558 0.1283 0.0066 0.1061 0.1797
Deblur -0.0961 0.0251 -0.1337 0.0081 0.0608
Lowleveltasks
Denoise -0.0389 0.1612 0.1212 0.1098 0.2391
Enhancement 0.1120 0.1551 -0.1443 0.2181 0.2251
ImageEditing 0.1600 0.1768 0.0922 0.2181 0.1800
Manipulationtasks ImageTranslation 0.2526 0.2426 0.1617 0.2965 0.3136
StyleTransfer 0.2274 0.2336 0.1515 0.2687 0.2455
Skeleton-to-image 0.4452 0.6150 0.2874 0.5201 0.7334
Visiontasks Mask-to-image 0.4467 0.3984 0.1590 0.3071 0.5531
Inpainting -0.0357 0.0014 -0.0511 0.0619 0.1013
Average 0.1529 0.2137 0.0650 0.2104 0.2832
Table2. ComparisonofFIDbetweenthegenerated𝐵′sandtheground-
isbecauseVISIIleveragesanInstructPix2Pix[Brooksetal.2023]
truthimages.Thebestresultsarehighlighted.Ourmethodoutperforms
modelwhichispretrainedonthesamedataset,makingitmore
previousmethodsintermsofallthethreetaskcategories.
familiarwithgeneratingdataofsimilarquality.
Method Low-level Manipulation Vision UserStudy. Weconductauserstudytoevaluatetheperceptual
performanceofourmethod.Theuserstudyconsistedof50ques-
MAEVQGAN 181.48 143.19 169.74
tions,with42participantsinvolved,containingallofthe10kind
PromptDiffusion 180.39 111.79 159.02
oftasks.Ineachquestion,first,wepresentedtheparticipantswith
DIA 173.10 103.39 191.51 images𝐴and𝐴′,askingthemtoanalyzethechangesbetweenthem.
VISII 140.39 88.36 138.44 Then,weprovidedimage𝐵andtaskedthemwithpredictingthe
Analogist 114.15 85.67 96.67 expectedtransformationof𝐵 followingthesamepattern.Subse-
quently,wedisplayedtheoutputsgeneratedbydifferentmethods
Table3. Userstudyresults.Ineachtask,wereporttheaveragepercentageof forthistask,andtheparticipantswererequiredtoselecttheone
selectedresultbytheusers.Thebestresultsarehighlighted.Ourapproach theydeemedmostconsistentwiththeidentifiedpatternandofthe
garneredthehighestnumberofselections.
highestgenerativequality.Wereporttheaverageselectionresult
forthethreemajortasks:low-leveltasks,manipulationtasks,and
Method Low-level Manipulation Vision visiontasksinTable3.Ourproposedmethodexhibitedthehighest
rateofbeingchosenamongallofthethreetasks.
MAEVQGAN 3.51% 3.45% 0.87%
PromptDiffusion 5.33% 14.99% 9.09% 5.5 AblationStudy
DIA 4.88% 3.32% 0.43%
VISII 20.18% 18.30% 15.58% Effectivenessofproposedcomponents. Toevaluatetheeffective-
Analogist 66.10% 59.95% 74.03% nessoftheproposedcomponents,weconductaseriesofablation
studies.TheablationresultsarepresentedinFigure9.(a)Thebase-
line model of pretrained inpainting model generates rough and
5.4 QuantitativeComparisons low-qualityresults.(b)Bypasting𝐵tothebottomrightcornerof
thegridimage,theoutputsaremorestructurallyconsistentwith
CLIPDirection. WecomputethefollowingCLIPdirectionsimilar-
ity,𝑐𝑜𝑠[(E(𝐵′)−E(𝐵)),(E(𝐴′)−E(𝐴))],toevaluatehowfaithfully
𝐵.(c)Addingnegativepromptshelpstostabilizethegeneration
processandavoidmessyresults.(d-1)Crucially,whenoperating
thetransformationsprovidedbythemodeladheretothetransfor-
self-attentioncloningbyM𝑠(𝐵,𝐵′):=M𝑠(𝐴,𝐴′),themodelretains
mationscontainedinthegivenexamples.Theresultsareshownin
theinformationfrom𝐵,butisunabletoextractaccuratecontext
inTable1.NotethatVISII[Nguyenetal.2023]achievesacceptable
from𝐴′ toinferthesametransformationresult.(d-2)Whenex-
resultsinmanipulationtaskssincethemodelitutilizesispretrained
ecuting SAC by M𝑠(𝐴′,𝐵′) := M𝑠(𝐴,𝐵), the model is required
onthisip2pdataset[Brooksetal.2023].Overall,ourmethoddemon-
tokeepthestructuralrelationbetween𝐴and𝐵 consistent,after
stratessuperiorICLcapabilitiesacrossallthesetasks.
theyhavebeentransformedinto𝐴′and𝐵′.Thus,weuse(d-2)in-
Fréchetinceptiondistance(FID). WecalculateFIDbetweengen- steadof(d-1).(e)WhenaddingtextualpromptsfromGPT-4Vinthe
eratedimagesandgroundtruthontheentiremajorcategory.The wholegridimage,themodelrarelyfocusesthetextguidanceonthe
resultsareshowninTable2.TheproposedAnalogistoutperforms targetinpaintingarea𝐵′.(f)Finally,withtheproposedCAM,our
allbaselinesacrossthethreemajortasks.NoticethatVISII[Nguyen fullapproachnotonlymaintainedrespectablegenerationquality
etal.2023]outperformsotherbaselinesonmanipulationtasks.This butalsosuccessfullyidentifiedthenecessaryvisualediting(adding10 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
Cat with sunglasses
Cubist art transformation
Cartoonish mosque illustration
Input (a) (b) (c) (d-1) (d-2) (e) (f)
Fig.9. Ablationontheproposedcomponents.Aninput2×2imagegridisinpaintedby:(a)pretrainedSDInpaintingmodelwithrandomnoiseasinput,
(b)initializing𝐵′ asnoised𝐵,(c)addingnegativeprompt,(d-1)addingself-attentioncloning(SAC)byM𝑠(𝐵,𝐵′) := M𝑠(𝐴,𝐴′),(d-2)addingSACby
M𝑠(𝐴′,𝐵′):=M𝑠(𝐴,𝐵),(e)addingGPT-4Vpromptingwithoutcross-attentionmasking(CAM),and(f)addingCAM(thefullapproach).Sourceimages:The
1𝑠𝑡 rowaregeneratedbyDALLE-3[Betkeretal.2023]andallothersarefromInstructPix2Pix[Brooksetal.2023].
Withoutgraphicalinstructions
Pleasehelpmewiththeimageanalogytask:takean
imageAand its transformation A’, and provideany
imageBtoproduceanoutputB’thatisanalogoustoA’. 𝑠=0.5 𝑠=1.0 𝒔=𝟏.𝟑 𝑠=1.5 𝑠=1.8
Or,moresuccinctly:A:A’::B:B’.Youshouldgiveme
thetextpromptofimageB’withnomorethan5words.
Toproceedwiththeimageanalogytask,lneedtounderstandthetransformation
f trr ao nm sfoim rmag ae tioA nt ho asim bea eg ne aA p. pC lieo dul fd roy mou imp al gea es Aec tola ir mify ago er Ad ?es Tc hr ii sbe ww illh ha et lpki mnd eo tof Input Only Only BothE&D BothE&D BothE&D
Encoder Decoder Deep Mid Shallow
createananalogoustransformationforimageBtogenerateimageB’.
Addinggraphicalinstructions Fig.11. Ablationonhyper-parameters.Inthefirstrow,lowercoefficient𝑠
producesresultsmorelike𝐵,whilehigher𝑠transfersmorefeatureof𝐴′.In
𝑨 𝑨′ Pleasehelpmewiththeimageanalogytask:takean
imageAand its transformation A’, and provideany
thesecondrow,performingSACandCAMatmiddlelayers(16×16)ofthe
imageBtoproduceanoutputB’thatisanalogoustoA’. UNetachievesbalancebetweenstructurepreservingandtransformation
𝑩 𝑩′ Or,moresuccinctly:A:A’::B:B’.Youshouldgiveme applying.Sourceimages:InstructPix2Pix[Brooksetal.2023].
thetextpromptofimageB’withnomorethan5words.
TocreateananalogoustransformationfromBtoB‘,similartoAtoA’,we
observethatAisawintryforestscene,andA‘addsanEastAsianpagodawith
redaccentsandaslightlywarmerambiance,whilemaintainingthesnowytheme.
GiventhatBisalsoasnowyforestscene,tomakeananalogoustransformationto GPT-4VPrompting. Weablateonthedesignedgraphicalinstruc-
B’,wewouldintroduceanelementthatisculturallysignificantorarchitecturally tionsthatusedtohintGPT-4VinFigure10.Withoutaddingthe
distinctwhileadjustingtheambianceinasimilarway.
ThetextpromptforB‘wouldbe:“Pagodainsnowyforest” visualmarksonthegridimage,GPT-4Vmaynotknowthecor-
respondingrelationshipofthegivenimages,thereforeisunable
Fig.10. AblationonthegraphicalinstructionsinGPT-4Vprompting.By tocorrectlyanalyzethecontentaccordingtotheinstructions.By
addingmarksandarrows,theidentityandrelationofthetaskbecomes explicitlymarkingthepositionsofimages(𝐴,𝐴′,𝐵,and𝐵′)onthe
moreobvious,makingiteasierforGPT-4Vtoproducepropertextprompt. constructedgridimage,GPT-4Vconvenientlyunderstandstheinfor-
Sourceimages:InstructPix2Pix[Brooksetal.2023].
mationcontainedinthepictures.Meanwhile,theintroducedarrows
from𝐴to𝐴′and𝐵to𝐵′successfullydemonstratethetransforma-
tionrelations,makingitmoreacceptableforGPT-4Vtoproduce
theidealresponseofaddinga“pagodainthesnowyforest”.This
sunglasses),effects(applyingacubiststyle),andtransformations textpromptwillintroducesemanticcontextualinformationforthe
(changingchurchintomosque)fortheICLtask. pretrainedmodeltounderstandthetask.NotethatourmethodisAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 11
PhototoCaricature SketchtoPortrait
𝑨 𝑨′ 𝑨 𝑨′
𝑨 𝑨𝟏" 𝑨𝟐" 𝑨𝟑" 𝑨𝟒"
𝑩 𝑩
𝑩 𝑩𝟏" 𝑩𝟐" 𝑩𝟑" 𝑩𝟒"
Fig.12. Giventhesameimage𝐴and𝐵inthefirstcolumn,anddifferent “Smilingwomanexaggeratedfeatures” “Color realistic woman's portrait”
𝐴′s,ourmethodisabletorecognizethecontextualrelationbetween𝐴and NormaltoRGB IcontoImage
𝐴′andproducetheoutput𝐵′imagesaccordingly.Sourceimage:𝐴and 𝑨 𝑨′ 𝑨 𝑨′
𝐵arefromImageBrush[SUNetal.2023].{𝐴′,𝐴′,𝐴′,𝐴′}aregenerated
1 2 3 4
usingMasaCtrl[Caoetal.2023].
Table4. ComparisonofinferencetimetakentoperformoneICLtaskfor
differentmethods.Comparedtoexistingmethods,ourmethoddoesnot 𝑩 𝑩
requiretrainingonaspecifictaskandadditionaloptimization.
Method Inferencetime
MAEVQGAN[Baretal.2022] 0.4s
“RGBshiftfacialimage” “Detailedcrabrendering”
PromptDiffusion[Wangetal.2023a] 4s
DIA[Šubrtováetal.2023] 258s
Fig.13. Examplesofapplicationfortaskswhere𝐴and𝐴′arealigned.The
VISII[Nguyenetal.2023] 685s textpromptsgeneratedbyGPT-4Visshownbeloweachexample.utput
Analogist(ours) 4s imagesarehighlighted.Sourceimage:Photo-to-caricatureimagesarefrom
CariMe[Guetal.2021].Sketch-to-portraitimagesarefromDeepFace-
Drawing[Chenetal.2020].Normal-to-RGBimagesarefromTrevithicket
al.[2024].IconimagesarefromIconShop[Wuetal.2023].
genericandsupportsothervision-languagemodels[Zhuetal.2023]
aswell.
Hyper-parameters. Wepresentablationontheparametersensitiv-
ityofourproposedmethodinFigure11.AsfortheSACcoefficient𝑠, experimentasshowninFigure12.Giventhesameimage𝐴asan
utilizingasmaller𝑠value(𝑠 =0.5)resultsinanoutputmoreclosely imageofwolves,wefirsttranslate𝐴intodifferentexampleoutputs
resemblingtheoriginalImage𝐵,whereasalargervalue(𝑠 =1.3) (cid:8)𝐴′,𝐴′,𝐴′,𝐴′(cid:9)
usingMasaCtrl[Caoetal.2023],obtainingdifferent
tendstoimbuetheresultwithcharacteristicsof𝐴′.However,ex- ani1 mal2 slik3 eli4
on,tiger,dog,andpanda.WeconstructdifferentICL
cessivelylargecoefficients(𝑠 =1.8)leadstoanoverlyunbalanced tasks,keepingtheimage𝐴and𝐵 beingthesame,whilevarying
attention map, which in turn reduces the quality of generation. theimage𝐴′s.Ourmethodisabletorecognizethetranslationfrom
WealsoablatetheselectionofUNetlayersinwhichweperform 𝐴to𝐴′accordinglyandgeneratethecorrespondinganimalsin𝐵′,
SACandCAM.Theresultsindicatethatitisnecessarytoperform demonstratingtheICLcapacityofourAnalogist.
operationssimultaneouslyinboththeencoderandthedecoder.Fur-
thermore,iftheoperationsareperformedatashallowlevel(high InferenceRuntime. Inthissection,wecomparetheexecutiontime
resolution),theoutcomeismerelyasimplereplicationofsomecol- fordifferentICLmethodsperformedonce.Ourexperimentiscon-
orsandcoarsetextures,leadingtopoorquality.Iftheoperations ductedonanRTX3090GPU,andwecalculatedthetimetakento
areperformedatadeeperlevel(lowresolution),theexcessivecom- generateoneimage.TheresultisshowninTab4.MAEVQGAN[Bar
pressionofinformationleadstothegeneratedresultbeingsimilar etal.2022]istheleasttime-consuming,taking0.4seconds,since
totheoriginalimage𝐵.Inourexperiments,weperformSACand
itisgeneratingveryfewtokenswithouttheneedofiterativelyde-
CAMatamiddleleveloftheUNetlayers. noising.OurmethodAnalogisttakesabout4second,thesameas
PromptDiffusion[Wangetal.2023a],whichistypicallythestandard
5.6 Analysis
samplingtimeforDiffusionmodels,butdoesnotrequirespecific
DifferentIn-contextexamples. Amodelwithcontextualreasoning fine-tuning.Asforthepreviousinference-baesdmethodsDIA[Šubr-
abilitiesshouldbeabletoproducedifferentresultsbasedondifferent továetal.2023]andVISII[Nguyenetal.2023],ittakesratherlong
in-contextexamples,whengiventhesameinput.Toverifythat time(i.e.,258secondsand685seconds)forthesetwomethodsto
ourapproachhassuchcapabilities,weconductedthefollowing estimatetheCLIPfeatureandeditinginstructionrespectively.12 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
ShapeChange Resize
𝑨′
𝑩
MAEVQGAN Analogist MAEVQGAN Analogist
Fig.14. Illustrationofthepipelinefortasksinwhich𝐴isalignedwith
Shape&ColorChange NumberExtrapolation
𝐵 insteadof𝐴′.Weswapthepositionsof𝐴′ and𝐵 inthegridimage.
Throughthisway,wesimplifytheproblemintoalignedtasks.Sourceimages:
generatedbyDALLE-3[Betkeretal.2023].
MotionTransfer ObjectMultiplication
𝑨 𝑨′ 𝑨 𝑨′
MAEVQGAN Analogist MAEVQGAN Analogist
LetterExtrapolation Letter&StyleExtrapolation
𝑩 𝑩
MAEVQGAN Analogist MAEVQGAN Analogist
Fig.16. Examplesofapplicationfortaskswhere𝐴,𝐴′and𝐵areallmis-
“high contrast animecharacter” “goldbricksstacked”
aligned.WetestourmethodwithoutSAC,onlyCAMisapplied.Output
MotionEditing&StyleTransfer ObjectMultiplication&Editing
imagesarehighlighted.Sourceimages:MAEVQGAN[Baretal.2022].
𝑨 𝑨′ 𝑨 𝑨′
forphoto-to-caricatureandicon-to-image.However,ourmethodis
stillrobusttotheseminorissuessinceweareprovidingin-context
informationfrombothstructuralandsemanticlevels.
𝑩 𝑩
6.2 𝐴and𝐵arealigned
Wemakeitpossibletoaddresstaskswhere𝐴 isalignedwith𝐵
insteadof𝐴′.WegiveanexampleofobjectmultiplicationinFig-
ure14,where𝐴containsonebrickand𝐴′containsabrickstack.
“stylized corgi in grass” “watermelon jack-o'-lantern”
Thisproblemcannotbedonethroughouroriginalpipeline.To
tacklethisproblem,weswapthepositionsof𝐴′and𝐵inthegrid
Fig.15. Examplesofapplicationfortaskswhere𝐴and𝐵arealigned.The
textpromptsofGPT-4Vareshownbeloweachexample.Outputimages
image,constructinganewgridimagewhere𝐴′containsonebrick
arehighlighted.Sourceimages:Theexampleimagesofthefirstmotion
and𝐵containsastackofbricks.Inthisway,wesimplifythetask
transfercasearefromChangetal.[2023].Theotherthreeexampleimages
intoonewhere𝐴and𝐴′arealignedagain,i.e.,changingthetask
aregeneratedbyDALLE-3[Betkeretal.2023]. ofturningonebrickintobrickstackintothetaskofchangingbricks
intogoldenbricks.Thisstrategycanbeappliedtotaskslikemo-
tiontransferandimageanalogywhere𝐴and𝐴′aremisalignedin
6 APPLICATION
figure15.Wealsodemonstrateourmethod’sabilityofaddressing
Inthissection,weextendAnalogisttothreecategoriesofapplica- taskswithmultipletranslationslikebothmotioneditingandstyle
tions:(a)𝐴and𝐴′arealigned,(b)𝐴and𝐵arealigned,and(c)𝐴, transfer,andobjectmultiplicationwithediting.
𝐴′,and𝐵areallmisaligned.For(b)and(c),wemakeadjustments
6.3 𝐴,𝐴′,and𝐵areallmisaligned
toourmethodaccordingly.
We extend our method on tasks where𝐴,𝐴′, and𝐵 are all mis-
6.1 𝐴and𝐴′ arealigned
alignedinFigure16,suchaschangingacircletoasquare,resizing
Undertheconditionthat𝐴and𝐴′arealigned,weshowexample abigcircletoasmallerone,extrapolatingnewcontentofnumbers
of applications in Figure 13, e.g., photo-to-caricature, sketch-to- andletters.WetestourmethodwithoutSACtopreventincorrect
portrait,normal-to-RGB,andicon-to-imagetasks.Theresultsshow structureguidance.Analogistproducesreasonableresultsandout-
thatourmethodisabletogeneratereasonableresultsonthesetasks. performsMAEVQGAN.Itshouldbepointedoutthatthequalityof
Noticethatthereareslightstructuralchangesbetween𝐴and𝐴′ longsequencelettergenerationstillhaveroomtoimproveduetoAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel • 13
Secondly,themodelstruggleswithproducingdatathatitseldom
Addingapolar bear Sketchofelephant
seesduringthetrainingstage.AsshowninFigure17(b),whenasked
toproduceunnaturalimageslikenormalmapandline-drawing
icons,themodelfailstogenerateaccurateresultssincemostofits
trainingdataarenaturalRGBimages.Ontheotherhand,itexplains
ourmethod’smediocreperformanceonvisiontaskscomparedto
ImageBrush[SUNetal.2023].Webelievethiscouldpotentiallybe
:“Mountainlakeattwilight” :“Lionsketch in savanna” achievedbydemandingamorepowerfulpretrainedbasemodel.
Finally,theproposedself-attentioncloningmaystrugglewith
(a)ExampleofinaccuratepromptbyGPT-4V.Theexpectedrightpromptisshownabove scenarioinwhich𝐴,𝐴′,and𝐵areallmisalignedasshowninFig-
theimagewiththecriticalwordsmarkedgreen.ThepromptgivenbyGPT-4Visshown
belowwiththewrongwordsinred. ure17(c).Thestructural-levelinformationisnotapplicableinthis
case.Onepossiblesolutionistorelyonsemantic-levelinformation
toproducethetransformationasdiscussedinSection6.3.
8 CONCLUSION
Addressingthelimitationsofinaccurateinstructionandtedious
optimizationofexistinginference-basedmethods,weintroduced
“Thermal image filter” “Outlined glasses icon” Analogist,anovelapproachforvisualIn-ContextLearning(ICL)
combining visual and textual prompting. The proposed method
(b)Failureexamplesofgeneratingunnaturalimagesonwhichthemodelisrarelyseen
duringthepretrainingstage,forexample,normalmapsandabstracticons. utilizesatext-to-imagediffusionmodelpretrainedforimagein-
painting,makingitanout-of-the-boxsolutionforawiderangeof
visualtasks.WeinnovatewithSelf-AttentionCloning(SAC)forvi-
sualprompting,enablingfine-grainedstructural-levelanalogy,and
leverageGPT-4V’svisualreasoningforefficienttextualprompting,
supplementedbyCross-AttentionMasking(CAM)forenhanced
semantic-levelanalogyaccuracy.Ourapproach,withouttheneed
forextratrainingoroptimization,demonstratessuperiorperfor-
“TwoOranges”,Analogist “TwoOranges”,AnalogistwithoutSAC
manceinbothqualitativeandquantitativemeasures,showcasing
(c)Exampleof𝐴,𝐴′,and𝐵areallmisaligned,whereSACisnotapplicable. robustICLcapabilities.
ACKNOWLEDGMENTS
Fig.17. Exampleoffailurecases.(a)GPT-4Vfailstoaccuratelydeducethe
correcttextualpromptfromthegivengridimageswhenthetransformation
ThisworkwassupportedinpartbytheNationalNaturalScience
(addingapolarbear)orcategory(elephant,insteadoflion)isambiguous.
FoundationofChinaunderGrant62276128,Grant62192783inpart
(b)Themodelfailstogenerateunnaturalimageslikenormalmapsoricons
bytheCollaborativeInnovationCenterofNovelSoftwareTech-
eventhoughgiventherighttextprompt.(c)TheproposedSACstruggles
nologyandIndustrialization,andaGRFgrantfromtheResearch
withtaskswhere𝐴,𝐴′,and𝐵areallmisaligned.Sourceimage:Trevithick
etal.[2024],IconShop[Wuetal.2023],andDALLE-3[Betkeretal.2023]. GrantsCouncil(RGC)oftheHongKongSpecialAdministrative
Region,China[ProjectNo.CityU11216122].
REFERENCES
notorioustendencyofdiffusionmodelstostrugglewithgenerating YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,
high-quality text. Nevertheless, we believe these results demon- JitendraMalik,andAlexeiAEfros.2023. SequentialModelingEnablesScalable
LearningforLargeVisionModels.arXivpreprintarXiv:2312.00785(2023).
stratethepre-trainedgenerativemodelshaveamplepotentialof
AmirBar,YossiGandelsman,TrevorDarrell,AmirGloberson,andAlexeiEfros.2022.
in-contextabilitytobefurthertapped. Visualpromptingviaimageinpainting.AdvancesinNeuralInformationProcessing
Systems35(2022),25005–25017.
7 LIMITATION AhmetCanberkBaykal,AbdulBasitAnees,DuyguCeylan,ErkutErdem,AykutErdem,
andDenizYuret.2023.CLIP-guidedStyleGANInversionforText-drivenRealImage
Editing.ACMTransactionsonGraphics42,5(2023),1–18.
Althoughourapproachenhancesin-contextlearningabilities,it’s
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,
importanttoconsidertwopossiblelimitations.Firstly,theinpainting JuntangZhuang,JoyceLee,YufeiGuo,etal.2023. Improvingimagegeneration
modelmightbemisledbyincorrecttextdescriptions.InFigure17(a), withbettercaptions.ComputerScience.https://cdn.openai.com/papers/dall-e-3.pdf
whenthetransformationfrom𝐴to𝐴′isminor(i.e.,theaddedobject 2(2023),3.
TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023.Instructpix2pix:Learning
inthefirstcaseissmallandeasilyoverlooked),GPT-4Vfailsto tofollowimageeditinginstructions.InProceedingsoftheIEEE/CVFConferenceon
recognizeit.Thesecondcaseshowsanstyletransfertaskofdrawing ComputerVisionandPatternRecognition.18392–18402.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,Prafulla
“asketchofelephant”.However,GPT-4Vrecognizestheobjectas Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.
alioninsteadofanelephant,leadingtoinaccurateguidance.The 2020. Languagemodelsarefew-shotlearners. AdvancesinNeuralInformation
ProcessingSystems33(2020),1877–1901.
potentialsolutioncouldbeleavinganinterfaceforuserstomonitor
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang
andcustomizethetextpromptsinrealtime. Zheng.2023. Masactrl:Tuning-freemutualself-attentioncontrolforconsistent14 • ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
imagesynthesisandediting.InProceedingsoftheIEEE/CVFInternationalConference theIEEE/CVFConferenceonComputerVisionandPatternRecognition.10684–10695.
onComputerVision.22560–22570. AdélaŠubrtová,MichalLukáč,JanČech,DavidFutschik,EliShechtman,andDaniel
DiChang,YichunShi,QuankaiGao,JessicaFu,HongyiXu,GuoxianSong,QingYan, Sy`kora.2023. DiffusionImageAnalogies.InACMSIGGRAPH2023Conference
XiaoYang,andMohammadSoleymani.2023.MagicDance:RealisticHumanDance Proceedings.1–10.
VideoGenerationwithMotions&FacialExpressionsTransfer. arXivpreprint YashengSUN,YifanYang,HouwenPeng,YifeiShen,YuqingYang,HanHu,LiliQiu,
arXiv:2311.12052(2023). andHidekiKoike.2023. ImageBrush:LearningVisualIn-ContextInstructions
Shu-YuChen,WanchaoSu,LinGao,ShihongXia,andHongboFu.2020.DeepFace- forExemplar-BasedImageManipulation.InThirty-seventhConferenceonNeural
Drawing:Deepgenerationoffaceimagesfromsketches. ACMTransactionson InformationProcessingSystems. https://openreview.net/forum?id=EmOIP3t9nk
Graphics39,4(2020),72–1. AlexTrevithick,MatthewChan,TowakiTakikawa,UmarIqbal,ShaliniDeMello,
WeiChen,WangWenjing,YangWenhan,andLiuJiaying.2018.DeepRetinexDecom- ManmohanChandraker,RaviRamamoorthi,andKokiNagano.2024.WhatYouSee
positionforLow-LightEnhancement.InBritishMachineVisionConference.British isWhatYouGAN:RenderingEveryPixelforHigh-FidelityGeometryin3DGANs.
MachineVisionAssociation. arXivpreprintarXiv:2401.02411(2024).
AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,ThomasFunkhouser,and XinlongWang,WenWang,YueCao,ChunhuaShen,andTiejunHuang.2023b.Images
MatthiasNießner.2017. Scannet:Richly-annotated3dreconstructionsofindoor speakinimages:Ageneralistpainterforin-contextvisuallearning.InProceedings
scenes.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog- oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6830–6839.
nition.5828–5839. XinlongWang,XiaosongZhang,YueCao,WenWang,ChunhuaShen,andTiejun
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet:A Huang.2023c.SegGPT:TowardsSegmentingEverythinginContext.InProceedings
large-scalehierarchicalimagedatabase.In2009IEEEConferenceonComputerVision oftheIEEE/CVFInternationalConferenceonComputerVision.1130–1140.
andPatternRecognition.Ieee,248–255. ZhendongWang,YifanJiang,YadongLu,yelongshen,PengchengHe,WeizhuChen,
OlgaDiamanti,ConnellyBarnes,SylvainParis,EliShechtman,andOlgaSorkine- ZhangyangWang,andMingyuanZhou.2023a.In-ContextLearningUnlockedfor
Hornung.2015.SynthesisofComplexImageAppearancefromLimitedExemplars. DiffusionModels.InThirty-seventhConferenceonNeuralInformationProcessing
ACMTransactionsonGraphics(Mar2015),1–14. https://doi.org/10.1145/2699641 Systems. https://openreview.net/forum?id=6BZS2EAkns
QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun, JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocV
JingjingXu,andZhifangSui.2022.Asurveyforin-contextlearning.arXivpreprint Le,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoninginlarge
arXiv:2301.00234(2022). languagemodels. AdvancesinNeuralInformationProcessingSystems35(2022),
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xiaohua 24824–24837.
Zhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold, RonghuanWu,WanchaoSu,KedeMa,andJingLiao.2023. IconShop:Text-Guided
SylvainGelly,etal.2020.Animageisworth16x16words:Transformersforimage VectorIconSynthesiswithAutoregressiveTransformers. ACMTransactionson
recognitionatscale.arXivpreprintarXiv:2010.11929(2020). Graphics42,6(2023),1–14.
ZhengGu,ChuanqiDong,JingHuo,WenbinLi,andYangGao.2021.CariMe:Unpaired WeihaoXia,YulunZhang,YujiuYang,Jing-HaoXue,BoleiZhou,andMing-Hsuan
caricaturegenerationwithmultipleexaggerations.IEEETransactionsonMultimedia Yang.2022. Ganinversion:Asurvey. IEEETransactionsonPatternAnalysisand
24(2021),2673–2686. MachineIntelligence45,3(2022),3121–3138.
JiabangHe,LeiWang,YiHu,NingLiu,HuiLiu,XingXu,andHengTaoShen.2023.ICL- CanwenXu,YichongXu,ShuohangWang,YangLiu,ChenguangZhu,andJulian
D3IE:In-ContextLearningwithDiverseDemonstrationsUpdatingforDocument McAuley.2023.Smallmodelsarevaluableplug-insforlargelanguagemodels.arXiv
InformationExtraction.InProceedingsoftheIEEE/CVFInternationalConferenceon preprintarXiv:2305.08848(2023).
ComputerVision.19485–19494. JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao.2023b.
AaronHertzmann,CharlesE.Jacobs,NuriaOliver,BrianCurless,andDavidH.Salesin. Set-of-markpromptingunleashesextraordinaryvisualgroundingingpt-4v.arXiv
2001. Imageanalogies.InProceedingsofthe28thannualconferenceonComputer preprintarXiv:2310.11441(2023).
graphicsandinteractivetechniques. https://doi.org/10.1145/383259.383295 ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,
JonathanHo,AjayJain,andPieterAbbeel.2020. Denoisingdiffusionprobabilistic andLijuanWang.2023a.Thedawnoflmms:Preliminaryexplorationswithgpt-4v
models.AdvancesinNeuralInformationProcessingSystems33(2020),6840–6851. (ision).arXivpreprintarXiv:2309.174219,1(2023).
OndřejJamriška,ŠárkaSochorová,OndřejTexler,MichalLukáč,JakubFišer,Jingwan LiangYuan,DingkunYan,SuguruSaito,andIsseiFujishiro.2024. DiffMat:Latent
Lu,EliShechtman,andDanielSýkora.2019. Stylizingvideobyexample. ACM diffusionmodelsforimage-guidedmaterialgeneration.VisualInformatics(2024).
TransactionsonGraphics(Aug2019),1–11. https://doi.org/10.1145/3306346.3323006 PolinaZablotskaia,AliaksandrSiarohin,BoZhao,andLeonidSigal.2019. Dwnet:
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022. Blip:Bootstrapping Densewarp-basednetworkforpose-guidedhumanvideogeneration.arXivpreprint
language-imagepre-trainingforunifiedvision-languageunderstandingandgenera- arXiv:1910.09139(2019).
tion.InInternationalConferenceonMachineLearning.PMLR,12888–12900. LvminZhang,AnyiRao,andManeeshAgrawala.2023. Addingconditionalcontrol
JingLiao,YuanYao,LuYuan,GangHua,andSingBingKang.2017.Visualatribute totext-to-imagediffusionmodels.InProceedingsoftheIEEE/CVFInternational
transferthroughdeepimageanalogy.ACMTransactionsonGraphics36,4(2017), ConferenceonComputerVision.3836–3847.
120. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
IvonaNajdenkoska,AnimeshSinha,AbhimanyuDubey,DhruvMahajan,Vignesh MiniGPT-4:EnhancingVision-LanguageUnderstandingwithAdvancedLargeLan-
Ramanathan,andFilipRadenovic.2023.ContextDiffusion:In-ContextAwareImage guageModels.InTheTwelfthInternationalConferenceonLearningRepresentations.
Generation.arXivpreprintarXiv:2312.03584(2023).
ThaoNguyen,YuhengLi,UtkarshOjha,andYongJaeLee.2023.VisualInstructionIn-
version:ImageEditingviaImagePrompting.InThirty-seventhConferenceonNeural
InformationProcessingSystems. https://openreview.net/forum?id=l9BsCh8ikK
GauravParmar,KrishnaKumarSingh,RichardZhang,YijunLi,JingwanLu,andJun-
YanZhu.2023. Zero-shotimage-to-imagetranslation.InACMSIGGRAPH2023
ConferenceProceedings.1–11.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesin
NeuralInformationProcessingSystems32(2019).
OrPatashnik,ZongzeWu,EliShechtman,DanielCohen-Or,andDaniLischinski.
2021.Styleclip:Text-drivenmanipulationofstyleganimagery.InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision.2085–2094.
FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,LucVanGool,MarkusGross,and
AlexanderSorkine-Hornung.2016.Abenchmarkdatasetandevaluationmethodol-
ogyforvideoobjectsegmentation.InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition.724–732.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021.
Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInterna-
tionalConferenceonMachineLearning.PMLR,8748–8763.
RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.
2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.InProceedingsof