TRANSIC: Sim-to-Real Policy Transfer
by Learning from Online Correction
YunfanJiang1,ChenWang1,RuohanZhang1,2,JiajunWu1,2,LiFei-Fei1,2
1DepartmentofComputerScience 2InstituteforHuman-CenteredAI(HAI)
StanfordUniversity
Abstract: Learning in simulation and transferring the learned policy to the real
worldhasthepotentialtoenablegeneralistrobots. Thekeychallengeofthisap-
proach is to address simulation-to-reality (sim-to-real) gaps. Previous methods
often require domain-specific knowledge a priori. We argue that a straightfor-
ward way to obtain such knowledge is by asking humans to observe and assist
robotpolicyexecutionintherealworld. Therobotscanthenlearnfromhumans
toclosevarioussim-to-realgaps.WeproposeTRANSIC,adata-drivenapproachto
enable successful sim-to-real transfer based on a human-in-the-loop framework.
TRANSIC allows humans to augment simulation policies to overcome various
unmodeled sim-to-real gaps holistically through intervention and online correc-
tion. Residual policies can be learned from human corrections and integrated
with simulation policies for autonomous execution. We show that our approach
canachievesuccessfulsim-to-realtransferincomplexandcontact-richmanipula-
tiontaskssuchasfurnitureassembly. Throughsynergisticintegrationofpolicies
learned in simulation and from humans, TRANSIC is effective as a holistic ap-
proachtoaddressingvarious,oftencoexistingsim-to-realgaps. Itdisplaysattrac-
tivepropertiessuchasscalingwithhumaneffort. Videosandcodeareavailable
attransic-robot.github.io.
Keywords: Sim-to-RealTransfer,Human-in-the-Loop,RobotManipulation
1 Introduction
Learning in simulation is a potential approach to the realization of generalist robots capable of
solving sophisticated decision-making tasks [1, 2]. Learning to solve these tasks requires a large
amount of training data [3–5]. Providing unlimited training supervision [6] through state-of-the-
art simulation [7–11] could alleviate the burden of collecting data in the real world with physical
robots [12, 13]. Therefore, it is crucial to seamlessly transfer and deploy robot control policies
acquiredinsimulation,usuallythroughreinforcementlearning(RL),toreal-worldhardware. Suc-
cessfuldemonstrationsofthissimulation-to-reality(sim-to-real)approachhavebeenshownindex-
terous in-hand manipulation [14–18], quadruped locomotion [19–22], biped locomotion [23–28],
andquadrotorflight[29,30].
Nevertheless, replicating similar success in manipulation tasks with robotic arms remains surpris-
ingly challenging, with only a few cases in simple non-prehensile manipulation (such as pulling,
pushing,andpivotingobjects)[31–34],industryassemblyunderrestrictedsettings[35–39],drawer
opening [40], and peg swinging [40]. The difficulty mainly stems from the unavoidable sim-to-
real gaps [11, 41], including but not limited to perception gap [19, 42–44], embodiment mis-
match [19, 45, 46], controller inaccuracy [47–49], and dynamics realism [50]. Traditionally, re-
searchers tackle these sim-to-real gaps and the transferring problem through system identifica-
tion[19,31,51,52],domainrandomization[14,53–55],real-worldadaptation[56,57],andsimu-
latoraugmentation[58–60]. Manyoftheseapproachesrequireexplicit,domain-specificknowledge
and expertise on tasks or simulators. Although for a particular simulation-reality pair, there may
4202
yaM
61
]OR.sc[
1v51301.5042:viXra(a)SimulationPolicy (b)DirectDeployment
(c)Human-in-the-LoopCorrection (d)SuccessfulTransferwithLearnedResidualPolicy
Figure1: TRANSICforsim-to-realtransferincontact-richroboticmanipulationtasks. a)and
b) Na¨ıvely deploying policies trained in simulation usually fails due to various sim-to-real gaps.
Here,therobotattemptstofirstalignthelightbulbwiththebaseandtheninsertandscrewthelight
bulbintothebase. c)Ahumanoperatormonitorsrobotbehaviors,intervenes,andprovidesonline
correctionthroughteleoperationwhennecessary.Humandataarecollectedtotrainaresidualpolicy
totacklevarioussim-to-realgapsinaholisticmanner. d)Thesimulationpolicyandresidualpolicy
are integrated together during test time to achieve successful sim-to-real transfer for contact-rich
tasks,suchasscrewingalightbulbintothebase.
exist specific inductive biases that can be hand-crafted post hoc to close the sim-to-real gap [19],
such knowledge is often not available a priori. Identifying its effects on task completion is also
intractable.
Wearguethatastraightforwardandfeasiblewayforhumanstoobtainsuchknowledgeistoobserve
andassistpolicyexecutionintherealworld. Ifhumanscanassisttherobottosuccessfullyaccom-
plishthetasksintherealworld,sim-to-realgapsareeffectivelyaddressed. Thisnaturallyleadsto
a generally applicable paradigm that can cover different priors across simulations and realities —
human-in-the-looplearning[61–63]andshared-autonomy[64,65].
Our key insight is that the human-in-the-loop framework is promising for addressing the sim-to-
real gaps as a whole, in which humans directly assist the physical robots during policy execution
by providing online correction signals. The knowledge required to close sim-to-real gaps can be
learnedfromhumansignals. Tothisend,wepresentTRANSIC(transferringpoliciessim-to-realby
learningfromonlinecorrection,Fig.1),adata-drivenapproachtoenablesuccessfultransferringof
robotmanipulationpoliciestrainedwithRLinsimulationtotherealworld. InTRANSIC,oncethe
baserobotpoliciesareacquiredfromsimulationtraining,theyaredeployedontorealrobotswhere
human operators monitor the execution. When the robot makes mistakes or gets stuck, humans
interruptandassistrobotpoliciesthroughteleoperation. Suchhumaninterventiondataarecollected
totrainaresidualpolicy,afterwhichthebasepolicyandtheresidualpolicyarecombinedtosolve
contact-rich manipulation tasks, such as furniture assembly. With the synergetic integration with
previous approaches, since humans can successfully assist the robot trained in silico to complete
real-world tasks, sim-to-real gaps are implicitly handled and addressed by humans in a domain-
agnosticmanner. Additionally,humansupervisionnaturallyguaranteessafedeployment.
To summarize, the key contribution of our work is a novel, holistic human-in-the-loop method
calledTRANSICtotacklesim-to-realtransferofpoliciesformanipulationtasks. Throughextensive
evaluation, we show that our method leads to more effective sim-to-real transfer compared to
traditionalmethods[51,53]andrequireslessreal-robotdatacomparedtotheprevalentimitation
learningandofflineRLalgorithms[66–69]. Wedemonstratethatsuccessfulsim-to-realtransferof
short-horizonskillscansolvelong-horizon,contact-richmanipulationtasksinourdailyactivities,
suchasfurnitureassembly. Videosandcodeareavailableattransic-robot.github.io.
22 Preliminaries
2.1 ProblemFormulation
We formulate a robot manipulation task as an infinite-horizon discrete-time Partially Observable
Markov Decision Process (POMDP) := ( , ,Ω, , ,R,γ,ρ ), where is the state space,
0
M S O A T S
is the observation space, and is the action space. At time step t, a robot observes o
t
O A ∈ O
emittedfromobservationfunctionΩ(o s ,a ): ,executesanactiona ,andreceives
t t t−1 t
a scalar reward r from the reward func| tion R(s ,aS )× :A→O R. The environment proceeds
t t t
S ×A →
to the next state governed by the transition function (s s ,a ) : . The robot
t+1 t t
T | S × A → S
learns a parameterized policy π ( o) : ∆ to maximize the expected discounted return
θ
:=E [(cid:80)∞ γtr ]overindu·| cedtraO jec→ torydA istributionτ :=(s ,o ,a ,r ,...) p ,where
J τ∼pπθ t=0 t 0 0 0 0 ∼ πθ
s ρ issampledfromtheinitialstatedistribution. Additionally,γ [0,1)isadiscountfactor.
0 0
∼ ∈
Inthiswork,wemodelsimulationandrealenvironmentsastwodifferentPOMDPs.
2.2 Intervention-BasedPolicyLearning
We adopt an intervention-based learning framework [66, 67, 70] where a human operator can in-
tervene and take control during the execution of the robot base policy π . Denote the human
B
policy as π , the following combined policy is deployed during data collection: πdeployed =
H
1HπH+(cid:0) 1 1H(cid:1) πB,where1H isabinaryfunctionindicatinghumaninterventions. Introducing
−
atrajectorydistributionq(τ)thatconsistsoftwoobservation-actiondistributionsgeneratedbythe
robot ρB and human operator ρH, the original RL objective leads to the maximization of a varia-
tionallowerboundonlogarithmicreturn[67,71]: (θ,q)=E [logR(τ)+logp logq(τ)],
J
q(τ) πθ
−
where p is the induced trajectory distribution. While the human operator optimizes this
πθ
objective through intervention and correction, the robot learner maximizes it through θ =
argmax E [logπ (ao)]. Various intervention-based policy learning methods have
θ∈Θ (o,a)∼q(τ) θ
|
beenderivedbyweightingobservation-actionpairsdifferently. Forexample,HG-Dagger[66]com-
pletelyignoresrobotdata B andonlytrainsonhumandata H thatcontaininterventionsamples.
D D
This is equivalent to q(τ) ρH. Intervention Weighted Regression (IWR) [67] balances the data
∝
distributionbyemphasizinghumanintervention: q(τ) αρH +ρB withα = B / H . Non-
∝ |D | |D |
intervention-basedmethodssuchastraditionalbehaviorcloning(BC)[72]onlylearnon H with
D
fullhumandemonstrationsinsteadofintervention. Thiseffectivelysetsq(τ) ρH.
∝
3 TRANSIC: Sim-to-RealPolicyTransferbyLearningfromOnline
Correction
AnoverviewofTRANSICisshowninFig.2.Atahighlevel,aftertrainingthebasepolicyinsimula-
tion,wedeployitontherealrobotwhilemonitoredbyahumanoperator. Thehumaninterruptsthe
autonomousexecutionwhennecessaryandprovidesonlinecorrectionthroughteleoperation. Such
interventionandonlinecorrectionarecollectedtotrainaresidualpolicy,afterwhichbothbaseand
residualpoliciesaredeployedtocompletecontact-richmanipulationtasks. Inthissection,wefirst
elaborateonthesimulationtrainingphasewithseveralimportantdesignchoicesthatreducesim-to-
realgapsbeforetransfer. Wethenintroduceresidualpolicieslearnedfromhumaninterventionand
onlinecorrection. Subsequently,wepresentanintegratedframeworkfordeployingthebasepolicy
alongsidethelearnedresidualpolicyduringtesting. Finally,weprovideimplementationdetails.
3.1 LearningBasePoliciesinSimulationwithRL
PolicyLearningwith3DRepresentation Objectgeometrymattersforcontact-richmanipulation.
Forexample,arobotshouldideallyinsertalightbulbintothelampbasewiththethreadfacingdown.
Toretainsuch3Dinformationandfacilitatesim-to-realtransfer, weproposetousepointcloudas
themainvisualmodality. TypicalRGBobservationusedinvisuomotorpolicytraining[73]suffers
fromseveraldrawbacksthathindersuccessfultransfer,suchasthevulnerabilitytodifferentcamera
3Proprioception Privileged Teacher Policy
 Ope Cra ot nio trn oa ll l eS rpace
 Base Policy Hu Dm aa tan C-i on l- lt eh ce t- iL oo nop
 Correction Dataset
(Trained with RL)
Object Poses
Human Operator
Correct

or not?
Action Space

Distillation Residual Policy Training
Synthetic Point Cloud Student
 Residual Policy Successful Transfer
Policy
Proprioception Base Policy
Joint Position
 Integrated

Controller Deployment
(a)SimulationPolicyTrainingthroughActionSpaceDistillation (b)ResidualPolicyLearningfromHumanCorrection
Figure2: TRANSIC methodoverview. a)Basepoliciesarefirsttrainedinsimulationthroughac-
tion space distillation with demonstrations generated by RL teacher policies. Base policies take
point cloud as input to reduce perception gap. b) After acquiring base policies, they are first de-
ployedwhereahumanoperatormonitorstheexecution. Thehumanintervenesandcorrectsthrough
teleoperationwhennecessary. Suchinterventionandcorrectiondataarecollectedtolearnresidual
policies. Finally,bothresidualpoliciesandbasepoliciesareintegratedduringtesttimetoachieve
successfultransfer.
poses[74]anddiscrepanciesbetweensyntheticandrealimages[43].Well-calibratedpointcloudob-
servationcanbypasstheseissuesandhasbeensuccessfullydemonstrated[15,75].Duringthesimu-
lationRLtrainingphase,wesynthesizepointcloudobservationsforhigherthroughput. Concretely,
given the synthetic point cloud of the m-th object P(m) RK×3, we transform it into the global
framethroughP(m) =P(m)(cid:0) R(m)(cid:1)⊺ +(cid:0) p(m)(cid:1)⊺ .HereR∈ (m) R3×3andp(m) R3×1denotethe
g
∈ ∈
object’sorientationandtranslationintheglobalframe. Further,thepointcloudrepresentationofa
sceneSwithM objectsisaggregatedasPS =(cid:83)M P(m)andsubsequentlyusedaspolicyinput.
m=1 g
Action Space Distillation A suitable action abstraction is critical for efficient learning [47, 48]
aswellassim-to-realtransfer[49]. Ahigh-levelcontrollersuchastheoperationalspacecontroller
(OSC) [76] facilitates RL exploration [47] but may hinder sim-to-real transfer because it requires
accurate modeling of robot parameters, such as joint friction, mass, and inertia [77]; on the
other hand, a low-level action space such as the joint position ensures consistent deployment in
simulationandrealhardware,butrenderstrial-and-errorRLimpractical. Wedrawinspirationfrom
the teacher-student framework [16, 78–80] and propose to first train the teacher policy πteacher
through RL with OSC and then distill successful trajectories into the student policy πstudent with
jointpositioncontrol. Specifically,werolloutπteacher andrecordtherobot’sjointpositionatevery
simulated time interval δt to construct a dataset teacher = τ(n) N . We then relabel actions
D { }n=1
from the end-effector’s poses to joint positions. Such a relabeled dataset is ready to train student
policies through behavior cloning. We name this approach as action space distillation and find it
crucial to overcome the sim-to-real controller gap. Furthermore, teacher policies directly receive
privileged observations for ease of learning, while student policies learn on synthetic point-cloud
inputs to match real-world measurements. The student policy parameterized by θ, πstudent, is
θ
trainedbyminimizingthefollowinglossfunction:
student = E (cid:2) logπstudent(cid:3) +βE (cid:2) ϕ(Preal) ϕ(Psim) 2(cid:3) , (1)
L − Dteacher θ Dpcd ∥ − ∥
where ϕ() denotes the point cloud encoder of πstudent and pcd = (Preal,Psim)(i) N is
· θ D { }i=1
a separate dataset that contains N pairs of matched point clouds in simulation and reality for
regularizationpurpose. WeempiricallyjustifytheimportanceofsuchregularizationinSec.4.5.
3.2 LearningResidualPoliciesfromOnlineCorrection
Human-in-the-Loop Data Collection Once the student policy is obtained from simulation, it
isdirectlyusedasthebasepolicyπB tobootstrapthedatacollection. Na¨ıvelydeployingthebase
4policy on real robots usually results in inferior performance and unsafe motion due to various
sim-to-realgaps.InTRANSIC,thebasepolicyisinsteaddeployedinawaythatisfullysynchronized
with the human operator. Concretely, at time step t, once aB πB is deployed, the execution
t ∼
is paused and a human operator needs to decide whether intervention is necessary, indicated as
1H. Interventionisnotnecessaryformosttaskexecutionwhentherobotisapproachingobjectsof
t
interest. However,whentherobottendstobehaveabnormallyduetounaddressedsim-to-realgaps,
the human operator intervenes and takes full control through teleoperation to correct robot errors.
Inthesecases,therobot’spre-andpost-interventionstates,aswellasinterventionindicator1H,are
t
collectedtoconstructtheonlinecorrectiondataset H H (cid:0)1H,qpre,qpost(cid:1) . Thisprocedure
D ←D ∪ t t t
isillustratedinAlgorithm1.
HumanCorrection asResidualPolicies Properlymodeling humancorrection canbechalleng-
ing. Thisisbecausehumansusuallysolvetasksnotpurelybasedoncurrentobservation,hencethe
non-Markoviandecisionprocess[68]. Therefore,directlyfine-tuningthebasepolicyπB onhuman
correction dataset H leads to large motions and even model collapse (Sec. 4). Inspired by prior
D
work onlearning residuals tocompensate forunknown dynamics andnoisy observations [81–83],
we propose to incorporate human correction behaviors with residual policies. Concretely, at the
timeofintervention,aresidualpolicyπR parameterizedbyψ learnstopredicthumanintervention
ψ
as the difference between post- and pre-intervention robot states: aR = qpost qpre, where
⊖ ⊖
denotes generalized subtraction. For continuous variables such as joint position, it computes the
numerical difference; for binary variables such as opening and closing the gripper, it computes
exclusivenor. Theresidualpolicyisthentrainedtomaximizethelikelihoodofhumancorrection:
(cid:104) (cid:105)
residual = E logπR(aR ) .
L − DH ψ |·
3.3 AnIntegratedDeploymentFramework
Whileintheoriginalformulationoftheresidualpolicy,residualactionsarealwaysapplied[82]. In
practice, we find that learning a gating function to predict whether to apply the residual policy or
notleadstosmootherandmorehuman-likebehaviors. Wecallthislearnedgatedresidualpolicy.
Denotethegatingfunctionasg (). Itcanbetrainedthroughclassificationon H andsharesthe
ψ
· D
same feature encoder with πR. The policy effectively being deployed to autonomously complete
tasksisanintegrationofbasepolicyπB andresidualpolicyπR,gatedbyg:πdeployed =πB gπR.
⊕
Ajointpositioncontrollerisusedduringdeployment.
3.4 ImplementationDetails
We use Isaac Gym [10] as the simulation backend. Proximal policy optimization (PPO [84]) is
usedtotrainteacherpoliciesfromscratch. Wedesigntask-specificrewardfunctionsandcurricula
whennecessarytofacilitateRLtraining.Weapplyexhaustivedomainrandomizationduringteacher
policy training and proper data augmentation during student policy distillation. Student policies
are parameterized as Gaussian Mixture Models (GMMs [68]). We have also experimented with
otherstate-of-the-artpolicymodels,suchasDiffusionPolicy[85],butdidnotobservebetterperfor-
mances.SeetheAppendixSec.Aformoredetailsaboutthesimulationtrainingphaseandadditional
comparisons. During the human-in-the-loop data collection phase, we use a 3Dconnexion Space-
Mouse as the teleoperation interface. Residual policies use state-of-the-art point cloud encoders,
such as PointNet [86] and Perceiver [87, 88], and GMM as the action head. We follow the best
practicestotrainresidualpolicies,includingusinglearningratewarm-upandcosineannealing[89].
MoretraininghyperparametersareprovidedintheAppendixSec.B.4.
4 Experiments
Weseektoanswerthefollowingresearchquestionswithourexperiments:
5(a)Stabilize (b)ReachandGrasp
(c)Insert (d)Screw
Figure3:Fourtasksbenchmarkedinthiswork.Theyarefundamentalskillsrequiredtoassemble
a square table from FurnitureBench [90]. We randomize objects’ initial poses during evaluation.
a)Therobotpushesthesquaretabletoptotherightcornerofthewallsuchthatitremainsstablein
followingassemblysteps. b)Therobotreachesandgraspsthetableleg. Itneedstoproperlyadjust
theendeffector’sorientationtoavoidinfeasiblegraspingposes. c)Therobotinsertsthepre-grasped
tablelegtothefarrightassemblyholeofthetabletop.d)Therobot’send-effectorisinitializedclose
toaninsertedtableleganditscrewsthetablelegclockwiseintothetabletop.
1: Does TRANSIC lead to better transfer performance compared to traditional sim-to-real
Q
methods(Sec.4.2)?
2: CanTRANSICbetterintegratehumancorrectionintothepolicylearnedinsimulationthan
Q
existinginteractiveimitationlearning(IL)approaches(Sec.4.2)?
3: Does TRANSIC requirelessreal-worlddatatoachievegoodperformancecomparedtoal-
Q
gorithmsthatonlylearnfromreal-robottrajectories(Sec.4.2)?
4: HoweffectivecanTRANSICaddressdifferenttypesofsim-to-realgaps(Sec.4.3)?
Q
5: HowdoesTRANSICscalewithhumaneffort(Sec.4.4)?
Q
6: Does TRANSIC exhibitintriguingproperties,suchasgeneralizationtounseenobjects,ef-
Q
fective gating, policy robustness, consistency in learned visual features, ability to solve
long-horizonmanipulationtasks,andotheremergentbehaviors(Sec.4.5)?
4.1 ExperimentSettings
Tasks We consider complex contact-rich manipulation tasks that require high precision in
FurnitureBench [90]. These tasks are challenging and ideal for testing sim-to-real transfer, since
perception,embodiment,controller,anddynamicsgapsallneedtobeaddressedtoaccomplishthe
taskssuccessfully. Specifically,wedividetheassemblyofasquaretableintofourindependenttasks
(Fig.3): Stabilize, ReachandGrasp, Insert, andScrew. Wecollect20, 100, 90, and17real-robot
trajectorieswithhumancorrection,respectively. Tofurthertestthegeneralizationtounseenobjects
from a new category, we experiment with a lamp (Fig. 6b). All experiments are conducted on a
tabletopsettingwithamountedFrankaEmika3robot. SeetheAppendixSec.B.1forthedetailed
systemsetup.
Baselines and Evaluation Protocol We compare with the following three groups of baselines.
1) Traditional sim-to-real methods: This group includes direct deployment of simulation policy
trained with domain randomization and data augmentation [53], denoted as “DR. & Data Aug.”.
Italsocoversthereal-worldfine-tuningparadigm,wheresimulationpoliciesarefurtherfine-tuned
with real-robot data through BC (denoted as “BC Fine-Tune”) and the state-of-the-art offline RL
method (implicit Q-learning [69], denoted as “IQL Fine-Tune”). To estimate the performance
lowerbound,wealsoincludethebaselinewithoutanydataaugmentationorreal-worldfine-tuning,
denoted as “Direct Transfer”. 2) Interactive IL: This group represents the state-of-the-art inter-
active imitation learning methods, including HG-Dagger [66] and IWR [67]. 3) Learning from
680 DirectTransfer IQLFine-Tune BC
DR.&DataAug. HG-Dagger BC-RNN
60 BCFine-Tune IWR IQL
40
20
0
Transic TraditionalSim-to-Real InteractiveIL RealDataOnly
Figure 4: Average success rates over four benchmarked tasks. TRANSIC significantly outper-
formsthreebaselinegroups. Resultsaresuccessratesaveragedoverfourtasks.
Table1: Successratespertasks. TRANSICoutperformsallbaselinemethodsonallfourtasks.
Direct DR.&Data BC IQL
Tasks TRANSIC
Transfer Aug.[53] Fine-Tune Fine-Tune
HG-Dagger[66] IWR[67] BC[72] BC-RNN[68] IQL[69]
Stabilize 100% 10% 35% 55% 0% 65% 65% 40% 40% 5%
ReachandGrasp 95% 35% 60% 35% 0% 30% 40% 25% 0% 5%
Insert 45% 0% 15% 15% 25% 35% 40% 10% 5% 0%
Screw 85% 0% 35% 50% 65% 40% 40% 15% 25% 0%
real-robotdataonly: ThisgroupincludesBC[72],BC-RNN[68],andIQL[69]. Theyaretrained
onreal-robotdemonstrationsonly. WefollowLiuetal.[70]tolabelrewardforIQL.Allevaluations
consist of 20 trials starting with different objects and robot poses. We make our best efforts to
ensure the same initial settings when evaluating different methods. See Appendix Sec. C for the
detailedevaluationprotocol.
4.2 QuantitativeComparisononFourAssemblyTasks
AsshowninFig.4andTable1,TRANSICachievesthebestperformanceonaverageandonallfour
taskswithsignificantmargins. Wenowcomparethemindetailanddiscussthemainfindings.
TRANSICiseffectiveforsim-to-realtransfer( 1).Itsuccessfullyachievesahighaveragesuccess
Q
rateof81%acrossfourtasksonrealrobots. ForthetaskStabilize, itcanachievea100%success
rate while the best baseline method (BC Fine-Tune) only succeeds half the time. For challenging
tasksthatrequirepreciseandcontact-richmanipulation,suchasInsertandScrew,TRANSICresults
inpromisingoutcomes(45%and85%successrates,respectively)whiletheDirectTransferbaseline
neversucceeds.
What are the reasons for successful transfer? We observe that adding real-world human correc-
tion data does not guarantee improvement. For example, among traditional sim-to-real methods,
the best baseline BC Fine-Tune outperforms DR. & Data Aug. by 7% but IQL Fine-Tune leads
toworseperformance. Incontrast, TRANSIC effectivelyuseshumancorrectiondata,whichboosts
averageperformanceby124%.Presumably,BCFine-Tune’smarginalimprovementisduetothedo-
maindifferencebetweensimulationandreality. Thissignificantdistinctioncannotbeeasilybridged
through na¨ıve fine-tuning. Overall, TRANSIC not only achieves the best transfer performance, but
alsoimprovessimulationpoliciesthemostamongvarioussim-to-realapproaches.
TRANSICcanbetterincorporatehumancorrectionintotheoriginallearnedpolicy( 2). Due
Q
to the differences between human and robot behaviors [68], using real-world data to directly fine-
tuneapolicythathasbeenlargelytrainedonmachine-generatedtrajectoriescanleadtoundesired
performance.Specifically,TRANSICoutperformsinteractiveILmethodsincludingHG-Dagger[66]
and IWR [67] by 75% on average. While both of them weigh the intervention data higher during
training(Sec.2.2),wefindthattheytendtoerasetheoriginalpolicyandleadtocatastrophicforget-
7
)%(
etaR
sseccuSting. Therefore, in state space where no human intervention exists, they behave suboptimally and
hence suffer from out-of-distribution issues due to compounding error. In contrast, by incorporat-
inghumancorrectionwithaseparateresidualpolicyandintegratingbothbaseandresidualpolicies
throughgating,TRANSICcombinesthebestpropertiesofbothpoliciesduringdeployment. Itrelies
onthesimulationpolicyforrobustexecutionmostofthetime;whenthebasepolicyislikelytofail,
itautomaticallyappliestheresidualpolicytopreventfailuresandcorrectmistakes.
TRANSICrequiressignificantlyless
real-world data ( 3). It only re-
Perception
Q
quires dozens of real-robot trajecto-
riestoachievesuperiorperformance. TRANSIC
However, methods such as BC-RNN IWR 80
TRANSICZero-Shot Asset
and IQL trained on such a limited
IWRZero-Shot 60
number of data suffer from overfit-
40
ting and model collapse. TRANSIC
achieves 3.6 better performance 20
×
thanthem.Infact,astaskcomplexity
increases,wemayneedexponentially Embodiment
more real-world data to train these
models, which solely rely on real-
world demonstration data, accord-
ing to the previous literature [68].
Thisresulthighlightstheimportance Controller
of training in simulation first, and
then leveraging sim-to-real transfer
forrobotlearningpractitioners. Dynamics
Summary we show that in sim- Figure5: Robustnesstodifferentsim-to-realgaps. Num-
to-real transfer, a good base pol- bers are averaged success rates (%). Polar bars represent
icy learned from the simulation can performances after training with data collected specifically
be combined with limited real-world toaddressaparticulargap. Dashedlinesarezero-shotper-
data to achieve success ( 3). How- formances. Shadedcirclesshowaverageperformances.
Q
ever,effectivelyutilizinghumancor-
rectiondatatoaddressthesim-to-real
gapischallenging( 1),especiallywhenwewanttopreventcatastrophicforgettingofthebasepol-
Q
icy( 2).
Q
4.3 EffectivenessinAddressingDifferentSim-to-RealGaps( 4)
Q
WhileTRANSICisaholisticapproachtoaddressmultiplesim-to-realgapssimultaneously,weshed
lightonitsabilitytocloseeachindividualgap. Todoso,wecreatefivedifferentsimulation-reality
pairs.Foreachofthem,weintentionallycreatelargegapsbetweenthesimulationandtherealworld.
These gaps are applied to the real-world setting and they include perception error, underactuated
controller,embodimentmismatch,dynamicsdifference,andobjectassetmismatch. Notethatthese
areartificialsettingsforacontrolledstudy. SeetheAppendixSec.C.2fordetailedsetups.
As shown in Fig. 5, TRANSIC achieves an average success rate of 77% across five different
simulation-reality pairs with deliberately exacerbated sim-to-real gaps. This indicates its remark-
ableabilitytoclosetheseindividualgaps. Incontrast,thebestbaselinemethod,IWR,onlyachieves
an average success rate of 18%. We attribute this effectiveness in addressing different sim-to-real
gaps to the residual policy design. Zeng et al. [83] echos our finding that residual learning is an
effective tool to compensate for domain factors that cannot be explicitly modeled. Furthermore,
training with data specifically collected from a particular setting generally increases TRANSIC’s
performance. However, thisisnotthecaseforIWR,wherefine-tuningonnewdatacanevenlead
to worse performance. These results show that TRANSIC is better not only in addressing multiple
sim-to-realgapsasawhole,butalsoinhandlingindividualtypesofgapsofverydifferentnature.
890
TRANSIC TRANSIC IWR
IWR 80
70 60
40
50
20
30 0
0 25 50 75 100 SquareTable Lamp
CorrectionDatasetSize(%) (Seen) (Unseen)
(a) (b)
Figure 6: a) Scalability with human correction data. Numbers are success rates averaged over
fourtaskswithdifferentamountofhumancorrectiondata.Per-taskresultsareshowninTableA.XI.
b)Generalizationtounseenobjectsfromanewcategory. Successratesareaveragedovertasks
ReachandGraspandScrew.
4.4 ScalabilitywithHumanEffort( 5)
Q
Scalingwithhumaneffortisadesiredpropertyforhuman-in-the-looprobotlearningmethods[70].
We show that TRANSIC has better human data scalability than the best baseline IWR in Fig. 6a
andTableA.XI.Ifweincreasethesizeofthecorrectiondatasetfrom25%to75%ofthefulldataset
size,TRANSICachievesarelativeimprovementof42%intheaveragesuccessrate.Incontrast,IWR
onlyachieves23%relativeimprovement.Additionally,fortasksotherthanInsert,IWRperformance
plateaus at an early stage and even starts to decrease as more human data becomes available. We
hypothesizethatIWRsuffersfrom catastrophicforgettingandstrugglestoproperlymodel thebe-
havioralmodesofhumansandtrainedrobots. Ontheotherhand, TRANSIC bypassestheseissues
bylearninggatedresidualpoliciesonlyfromhumancorrection.
4.5 IntriguingPropertiesandEmergentBehaviors( 6)
Q
Finally,weexaminefurtherTRANSICanddiscussseveralemergentcapabilities.
Generalization to Unseen Objects We show that a robot trained with TRANSIC can zero-shot
generalize to new objects from a novel category. As shown in Fig. 6b, TRANSIC can achieve an
average success rate of 75% when zero-shot evaluated on assembling a lamp. However, IWR can
only succeed once every three attempts. This evidence suggests that TRANSIC is not overfitted to
aparticularobject,instead,ithaslearnedreusableskillsforcategory-levelobjectgeneralization.
Effects of Different Gating Mechanisms We introduce the learned gated residual policy in
Sec.3.3wherethegatingmechanismcontrolswhentoapplyresidualactions. Toassessthequality
of learned gating, we compare its performance with an actual human operator performing gating.
ResultsareshowninTable2. Itisevidentthatthelearnedgatingmechanismonlyincursnegligible
performancedropscomparedtohumangating. ThissuggeststhatTRANSICcanreliablyoperatein
afullyautonomoussettingoncethegatingmechanismislearned.
PolicyRobustness Weinvestigatethepolicyrobustnessagainst1)pointcloudobservationswith
inferior quality by removing two cameras, and 2) suboptimal correction data with noise injection.
SeeAppendixSec.C.4fordetailedexperimentsetups. ResultsareshowninTable2. Wehighlight
that TRANSIC is robust to partial point cloud inputs caused by the reduced number of cameras.
Weattributethistotheheavypointclouddownsamplingemployedduringtraining. Fishmanetal.
[91] echos our finding that policies trained with downsampled synthetic point cloud inputs can
9
)%(
etaR
sseccuS
)%(
etaR
sseccuSTable2: Resultsofablationstudies. Westudytheeffectsofdifferentgatingmechanisms(learned
gatingvshumangating),policyrobustnessagainstreducedcamerasandsuboptimalcorrectiondata,
andtheimportanceofvisualencoderregularization.
Reach
Average Stabilize Insert Screw
andGrasp
Original 81% 100% 95% 45% 85%
w/HumanGating 82% 100% 95% 50% 85%
ReducedCameras 80% 95% 90% 40% 95%
NoisyCorrection 76% 85% 80% 45% 95%
w/oRegularization 55% 85% 55% 20% 60%
generalizetopartialpointcloudobservationsobtainedintherealworldwithouttheneedforshape
completion. Meanwhile, when the correction data used to learn residual policies are suboptimal,
TRANSIConlyshowsarelativedecreaseof6%intheaveragesuccessrate. Weattributethistothe
advantageofourintegrateddeployment—whentheresidualpolicybehavessuboptimally,thebase
policycouldstillcompensatefortheerrorinsubsequentsteps.
ImportanceofPointCloudEncoderRegularization Tolearnconsistentvisualfeaturesbetween
the simulation and reality, we propose to regularize the point cloud encoder during the distillation
stageasinEq.1.AsshowninTable2,theperformancesignificantlydecreaseswithoutsuchregular-
ization,especiallyfortasksthatrequirefine-grainedvisualfeatures. Withoutit,simulationpolicies
wouldoverfittosyntheticpointcloudobservationsandhencearenotidealforsim-to-realtransfer.
QualitativeAnalysisandEmergentBehaviors Wefirstexaminethedistributionofthecollected
human correction dataset. During the human-in-the-loop data collection, the probability of
interveningandcorrectingisreasonablylow(P 0.20).Thisisconsistentwithourintuition
correction
≈
that, with agood basepolicy, interventionsare notnecessary formost ofthe time. However, they
become critical when the robot tends to behave abnormally due to unaddressed sim-to-real gaps.
Moreover, as highlighted in Fig. A.8, interventions happen at different times across tasks. This
fact renders heuristics-based methods [92] for deciding when to intervene difficult, and further
necessitatesourlearnedresidualpolicy.
Surprisingly,TRANSICshowsseveralrepresentativebehaviorsthatresemblehumans. Forinstance,
they include error recovery, unsticking, safety-aware actions, and failure prevention as shown in
Fig.7.
SolvingLong-HorizonManipulationTasks Finally,wedemonstratethatsuccessfulsim-to-real
transferofindividualskillscanbeeffectivelychainedtogethertoenablelong-horizoncontact-rich
manipulation(Fig.8). Seevideosontransic-robot.github.ioforarobotassemblingasquare
tableandalampusingTRANSIC.
5 RelatedWork
Robot Learning via Sim-to-Real Transfer Physics-based simulations [7–11, 50, 93–95] have
becomeadrivingforce[1,2]fordevelopingroboticskillsintabletopmanipulation[96–99],mobile
manipulation [100–103], fluid and deformable object manipulation [104–107], dexterous in-hand
manipulation[14–18],locomotionwithvariousrobotmorphology[19–27,108],objecttossing[83],
acrobaticflight[29,30],etc. However,thedomaingapbetweenthesimulatorsandtherealityisnot
negligible[11]. Successfulsim-to-realtransferincludeslocomotion[19–28],in-handre-orientation
for dexterous hands where objects are initially placed near the robot [14–18], and non-prehensile
manipulationlimitedtosimpletasks[31–40]. Inthiswork,wetacklemorechallengingsim-to-real
transfer forcomplex manipulation tasksand successfullydemonstrate that ourapproach can solve
sophisticatedcontact-richmanipulationtasks. Moreimportantly,itrequiressignificantlyfewerreal-
10(a)ErrorRecovery (b)Unsticking
(c)Safety-awareActions (d)FailurePrevention
Figure7: EmergentbehaviorslearnedbyTRANSIC. a)Errorrecovery. Left: Therobottriesto
insertthetablelegbutthedirectioniswrong;Right: TRANSICraisestheendeffectorandmovesto
thecorrectinsertionposition. b)Unsticking. Left: Therobothoversforawhileandneverreaches
thelightbulb;Right: TRANSIChelpstherobotgetunstuckandmovetothebulb. c)Safety-aware
actions. Left: When pushing the tabletop, the gripper is too low and bends. This might damage
therobot;Right: TRANSICcompensatesforthecommandthatcausestheendeffectortomovetoo
low. d)Failureprevention. Left: Thelightbulbwillfallandbreakaftergripperopening; Right:
TRANSICadjuststhebulbtoastableposetopreventfailure.
robotdatacomparedtotheprevalentimitationlearningandofflineRLapproaches[68,69,72]. This
makessolutionsthatarebasedonsimulatorsandsim-to-realtransfermoreappealingtoroboticists.
Sim-to-Real Gaps in Manipulation Tasks Despite the complex manipulation skills recently
learnedwithRLinsimulation[109],directlydeployinglearnedcontrolpoliciestophysicalrobots
often fails. The sim-to-real gaps [11, 41, 45, 110] that contribute to this performance discrepancy
can be coarsely categorized as follows: a) perception gap [19, 42–44], where synthetic sensory
observations differ from those measured in the real world; b) embodiment mismatch [19, 45, 46],
wheretherobotmodelsusedinsimulationdonotmatchthereal-worldhardwareprecisely;c)con-
troller inaccuracy [47–49], meaning that the results of deploying the same high-level commands
(suchasinconfigurationspace[111]andtaskspace[112])differinsimulationandrealhardware;
and d) poor physical realism [50], where physical interactions such as contact and collision are
poorlysimulated[113].
Althoughthesegapsmaynotbefullybridged,traditionalmethodstoaddressthemincludesystem
identification [19, 31, 51, 52], domain randomization [14, 53–55], real-world adaptation [56], and
simulatoraugmentation[58–60]. However,systemidentificationismostlyengineeredonacase-by-
casebasis. Domainrandomizationsuffersfromtheinabilitytoidentifyandrandomizeallphysical
parameters. Methods with real-world adaptation, usually through meta-learning [114], incur po-
tentialsafetyconcernsduringtheadaptationphase. Mostoftheseapproachesalsorelyonexplicit
and domain-specific knowledge about tasks and the simulator a priori. For instance, to perform
systemidentificationforclosingtheembodimentgapforaquadruped,Tanetal.[19]disassembles
thephysicalrobotandcarefullycalibratesparametersincludingsize, mass, andinertia. Kimetal.
[33] reports that collaborative robots, such as the commonly used Franka Emika robot, have intri-
cate joint friction that is hard to identify and randomized in typical physics simulators. To make
a simulator more akin to the real world, Chebotar et al. [40] deploys trained virtual robots multi-
11(a)Assemblealamp.
(b)Assembleasquaretable.
Figure 8: We demonstrate that individual skills learned with TRANSIC can be effectively
chained together to enable long-horizon contact-rich manipulation. a) The robot assembles
alamp(160secondsin1 speed). b)TherobotassemblesasquaretablefromFurnitureBench[90]
×
(550secondsin1 speed). Videosareavailableontransic-robot.github.io.
×
pletimestorefinethedistributionsofsimulationparameters. Thisprocedurenotonlyintroducesa
significantreal-worldsamplingeffort,butalsoincurspotentialsafetyconcernsduetodeployingsub-
optimalpolicies. Incontrast,ourmethodleverageshumaninterventiondatatoimplicitlyovercome
thetransferringprobleminadomain-agnosticwayandalsoleadstosaferdeployment.
Human-in-The-LoopRobotLearning Human-in-the-loopmachinelearningisaprevalentframe-
worktoinjecthumanknowledgeintoautonomoussystems[62,115,116]. Variousformsofhuman
feedback exist [63], ranging from passive judgement, such as preference [117–126] and evalua-
tion[127–132],toactiveinvolvement,includingintervention[133–135]andcorrection[136,137].
Theyarewidelyadoptedinsolutionsforsequentialdecision-makingtasks. Forinstance,interactive
imitation learning [66, 67, 70, 138] leverages human intervention and correction to help na¨ıve
imitatorsaddressdatamismatchandcompoundingerror. InthecontextofRL,rewardfunctionscan
bederivedtobetteralignagentbehaviorswithhumanpreferences[120,123,124,127]. Noticeably,
12recent trend focuses on continually improving robots’ capability by iteratively updating and
deployingpolicieswithhumanfeedback[70],combiningactivehumaninvolvementwithRL[137],
and autonomously generating corrective intervention data [139]. Our work further extends this
trend by showing that sim-to-real gaps can be effectively eliminated by using human intervention
andcorrectionsignals.
Insharedautonomy,robotsandhumanssharethecontrolauthoritytoachieveacommongoal[64,
65,140–142]. Thiscontrolparadigmhasbeenlargelystudiedinassistiveroboticsandhuman-robot
collaboration[143–145].Inthiswork,weprovideanovelperspectivebyemployingitinsim-to-real
transferofrobotcontrolpoliciesanddemonstratingitsimportanceinattainingeffectivetransfer.
6 ConclusionandLimitations
Inthiswork,wepresentTRANSIC,aholistichuman-in-the-loopmethodtotacklesim-to-realtrans-
ferofpoliciesforcontact-richmanipulationtasks. Weshowthatinsim-to-realtransfer,agoodbase
policylearnedfromthesimulationcanbecombinedwithlimitedreal-worlddatatoachievesuccess.
However, effectivelyutilizinghumancorrectiondatatoaddressthesim-to-realgapischallenging,
especially when we want to prevent catastrophic forgetting of the base policy. TRANSIC success-
fullyaddressesthesechallengesbylearningagatedresidualpolicyfromhumancorrectiondata. We
showthatTRANSICiseffectiveasaholisticapproachtoaddressdifferenttypesofsim-to-realgaps
whenpresentedsimultaneously;itisalsoeffectiveasanapproachtoaddressindividualgapsofvery
differentnatures. Itdisplaysattractiveproperties,suchasscalingwithhumaneffort.
WhileTRANSICachievesremarkablesim-to-realtransferresults,itstillhasseverallimitationswhich
shouldbeaddressedbyfollow-upstudies. 1)Thecurrenttasksarelimitedtothetabletopscenario
with a soft parallel-jaw gripper. However, we notice that with the development of teleoperation
devicesformorecomplicatedrobots,suchasbimanualarms[146],dexteroushands[147],mobile
manipulators [148, 149], and humanoids [150], TRANSIC can also be potentially applied to these
settings. 2)Duringthecorrectiondatacollectionphase,thehumanoperatorstillmanuallydecides
when to intervene. This effort might be reduced by leveraging automatic failure detection tech-
niques [151, 152]. 3) TRANSIC requires simulation policies with reasonable performances in the
first place, which is challenging to learn by itself. Nevertheless, TRANSIC is compatible with and
canbenefitfromrecentadvancesinthisdirection[6,85,153].
Acknowledgments
We are grateful to Josiah Wong, Chengshu (Eric) Li, Weiyu Liu, Wenlong Huang, Stephen Tian,
SanjanaSrivastava,andtheSVLPAIRgroupfortheirhelpfulfeedbackandinsightfuldiscussions.
ThisworkisinpartsupportedbytheStanfordInstituteforHuman-CenteredAI(HAI),ONRMURI
N00014-22-1-2740,ONRMURIN00014-21-1-2801,andSchmidtSciences. RuohanZhangispar-
tiallysupportedbyWuTsaiHumanPerformanceAllianceFellowship.
13References
[1] H.Choi,C.Crump,C.Duriez,A.Elmquist,G.Hager,D.Han,F.Hearl,J.Hodgins,A.Jain,
F. Leve, C. Li, F. Meier, D. Negrut, L. Righetti, A. Rodriguez, J. Tan, and J. Trinkle. On
the use of simulation in robotics: Opportunities, challenges, and suggestions for moving
forward. Proceedings of the National Academy of Sciences, 118(1):e1907856118, 2021.
doi:10.1073/pnas.1907856118. URLhttps://www.pnas.org/doi/abs/10.1073/pnas.
1907856118.
[2] C. K. Liu and D. Negrut. The role of physics-based simulators in robotics. An-
nual Review of Control, Robotics, and Autonomous Systems, 4(1):35–58, 2021.
doi:10.1146/annurev-control-072220-093055. URL https://doi.org/10.1146/
annurev-control-072220-093055.
[3] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learn-
ing. In Proceedings of the Twenty-First International Conference on Machine Learning,
ICML ’04, page 1, New York, NY, USA, 2004. Association for Computing Machinery.
ISBN 1581138385. doi:10.1145/1015330.1015430. URL https://doi.org/10.1145/
1015330.1015430.
[4] H. D. III, J. Langford, and D. Marcu. Search-based structured prediction. arXiv preprint
arXiv: Arxiv-0907.0786,2009.
[5] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models
for decision making: Problems, methods, and opportunities. arXiv preprint arXiv: Arxiv-
2303.04129,2023.
[6] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox.
Mimicgen:Adatagenerationsystemforscalablerobotlearningusinghumandemonstrations.
arXivpreprintarXiv: Arxiv-2310.17596,2023.
[7] E.Todorov,T.Erez,andY.Tassa. Mujoco: Aphysicsengineformodel-basedcontrol. 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033,
2012. doi:10.1109/IROS.2012.6386109.
[8] E.CoumansandY.Bai.Pybullet,apythonmoduleforphysicssimulationforgames,robotics
andmachinelearning. http://pybullet.org,2016–2021.
[9] Y. Zhu, J. Wong, A. Mandlekar, R. Mart´ın-Mart´ın, A. Joshi, S. Nasiriany, and Y. Zhu. ro-
bosuite: Amodularsimulationframeworkandbenchmarkforrobotlearning. arXivpreprint
arXiv: Arxiv-2009.12293,2020.
[10] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller,
N. Rudin, A. Allshire, A. Handa, and G. State. Isaac gym: High performance gpu-based
physicssimulationforrobotlearning. arXivpreprintarXiv: Arxiv-2108.10470,2021.
[11] C.Li,C.Gokmen,G.Levine,R.Mart´ın-Mart´ın,S.Srivastava,C.Wang,J.Wong,R.Zhang,
M.Lingelbach, J.Sun, M.Anvari, M.Hwang, M.Sharma, A.Aydin, D.Bansal, S.Hunter,
K.-Y.Kim,A.Lou,C.R.Matthews,I.Villa-Renteria,J.H.Tang,C.Tang,F.Xia,S.Savarese,
H. Gweon, K. Liu, J. Wu, and L. Fei-Fei. BEHAVIOR-1k: A benchmark for embodied AI
with1,000everydayactivitiesandrealisticsimulation. In6thAnnualConferenceonRobot
Learning,2022. URLhttps://openreview.net/forum?id=_8DoIe8G3t.
[12] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,
K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth,
N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu,
U.Malla,D.Manjunath,I.Mordatch,O.Nachum,C.Parada,J.Peralta,E.Perez,K.Pertsch,
J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke,
14A.Stone,C.Tan,H.Tran,V.Vanhoucke,S.Vega,Q.Vuong,F.Xia,T.Xiao,P.Xu,S.Xu,
T. Yu, and B. Zitkovich. Rt-1: Robotics transformer for real-world control at scale. arXiv
preprintarXiv: Arxiv-2212.06817,2022.
[13] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou,
A.Gupta,A.Raju,A.Laurens,C.Fantacci,V.Dalibard,M.Zambelli,M.Martins,R.Pevce-
viciute,M.Blokzijl,M.Denil,N.Batchelor,T.Lampe,E.Parisotto,K.Z˙ołna,S.Reed,S.G.
Colmenarejo,J.Scholz,A.Abdolmaleki,O.Groth,J.-B.Regli,O.Sushkov,T.Rotho¨rl,J.E.
Chen, Y. Aytar, D. Barker, J. Ortiz, M. Riedmiller, J. T. Springenberg, R. Hadsell, F. Nori,
and N. Heess. Robocat: A self-improving generalist agent for robotic manipulation. arXiv
preprintarXiv: Arxiv-2306.11706,2023.
[14] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron,
A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welin-
der,L.Weng,Q.Yuan,W.Zaremba,andL.Zhang. Solvingrubik’scubewitharobothand.
arXivpreprintarXiv: Arxiv-1910.07113,2019.
[15] T.Chen,M.Tippur,S.Wu,V.Kumar,E.Adelson,andP.Agrawal. Visualdexterity: In-hand
reorientationofnovelandcomplexobjectshapes. ScienceRobotics, 8(84):eadc9244,2023.
doi:10.1126/scirobotics.adc9244. URLhttps://www.science.org/doi/abs/10.1126/
scirobotics.adc9244.
[16] Y. Chen, C. Wang, L. Fei-Fei, and C. K. Liu. Sequential dexterity: Chaining dexterous
policiesforlong-horizonmanipulation. arXivpreprintarXiv: Arxiv-2309.00987,2023.
[17] H.Qi,A.Kumar,R.Calandra,Y.Ma,andJ.Malik. In-handobjectrotationviarapidmotor
adaptation. arXivpreprintarXiv: Arxiv-2210.04887,2022.
[18] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y. Ma, R. Calandra, and J. Malik. General in-hand
objectrotationwithvisionandtouch. arXivpreprintarXiv: Arxiv-2309.09979,2023.
[19] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke.
Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv: Arxiv-
1804.10332,2018.
[20] A.Kumar,Z.Fu,D.Pathak,andJ.Malik. RMA:rapidmotoradaptationforleggedrobots.
InD.A.Shell,M.Toussaint,andM.A.Hsieh,editors,Robotics: ScienceandSystemsXVII,
Virtual Event, July 12-16, 2021, 2021. doi:10.15607/RSS.2021.XVII.011. URL https:
//doi.org/10.15607/RSS.2021.XVII.011.
[21] Z.Zhuang,Z.Fu,J.Wang,C.Atkeson,S.Schwertfeger,C.Finn,andH.Zhao.Robotparkour
learning. arXivpreprintarXiv: Arxiv-2309.05665,2023.
[22] R.Yang, G.Yang, andX.Wang. Neuralvolumetricmemoryforvisuallocomotioncontrol.
arXivpreprintarXiv: Arxiv-2304.01201,2023.
[23] H. Benbrahim and J. A. Franklin. Biped dynamic walking using reinforcement learn-
ing. Robotics and Autonomous Systems, 22(3):283–302, 1997. ISSN 0921-8890. doi:
https://doi.org/10.1016/S0921-8890(97)00043-2. URL https://www.sciencedirect.
com/science/article/pii/S0921889097000432. RobotLearning: TheNewWave.
[24] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid. Reinforcement learning-based cascade
motionpolicydesignforrobust3dbipedallocomotion.IEEEAccess,10:20135–20148,2022.
doi:10.1109/ACCESS.2022.3151771.
[25] L. Krishna, G. A. Castillo, U. A. Mishra, A. Hereid, and S. Kolathaya. Linear policies are
sufficient to realize robust bipedal walking on challenging terrains. arXiv preprint arXiv:
Arxiv-2109.12665,2021.
15[26] J. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst. Blind bipedal stair traversal via
sim-to-realreinforcementlearning. arXivpreprintarXiv: Arxiv-2105.08328,2021.
[27] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath. Real-world hu-
manoid locomotion with reinforcement learning. arXiv preprint arXiv: Arxiv-2303.03381,
2023.
[28] Z.Li,X.B.Peng,P.Abbeel,S.Levine,G.Berseth,andK.Sreenath. Reinforcementlearning
forversatile, dynamic, androbustbipedallocomotioncontrol. arXivpreprintarXiv: Arxiv-
2401.16889,2024.
[29] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Mu¨ller, V. Koltun, and D. Scaramuzza.
Champion-leveldroneracingusingdeepreinforcementlearning. Nature,2023. doi:10.1038/
s41586-023-06419-4. URLhttps://doi.org/10.1038/s41586-023-06419-4.
[30] Y. Song, A. Romero, M. Mu¨ller, V. Koltun, and D. Scaramuzza. Reaching the limit in au-
tonomous racing: Optimal control versus reinforcement learning. Science Robotics, 8(82):
eadg1462, 2023. doi:10.1126/scirobotics.adg1462. URL https://www.science.org/
doi/abs/10.1126/scirobotics.adg1462.
[31] V.Lim,H.Huang,L.Y.Chen,J.Wang,J.Ichnowski,D.Seita,M.Laskey,andK.Goldberg.
Planarrobotcastingwithreal2sim2realself-supervisedlearning.arXivpreprintarXiv:Arxiv-
2111.04814,2021.
[32] W.ZhouandD.Held.Learningtograsptheungraspablewithemergentextrinsicdexterity.In
K.Liu,D.Kulic,andJ.Ichnowski,editors,ConferenceonRobotLearning,CoRL2022,14-18
December2022, Auckland, NewZealand, volume205ofProceedingsofMachineLearning
Research,pages150–160.PMLR,2022. URLhttps://proceedings.mlr.press/v205/
zhou23a.html.
[33] M. Kim, J. Han, J. Kim, and B. Kim. Pre- and post-contact policy decomposition for non-
prehensile manipulation with zero-shot sim-to-real transfer. arXiv preprint arXiv: Arxiv-
2309.02754,2023.
[34] X.Zhang,S.Jain,B.Huang,M.Tomizuka,andD.Romeres.Learninggeneralizablepivoting
skills. InIEEEInternationalConferenceonRoboticsandAutomation,ICRA2023,London,
UK,May29-June2,2023,pages5865–5871.IEEE,2023. doi:10.1109/ICRA48891.2023.
10161271. URLhttps://doi.org/10.1109/ICRA48891.2023.10161271.
[35] S. Kozlovsky, E. Newman, and M. Zacksenhouse. Reinforcement learning of impedance
policiesforpeg-in-holetasks: Roleofasymmetricmatrices. IEEERoboticsandAutomation
Letters,7(4):10898–10905,2022. doi:10.1109/LRA.2022.3191070.
[36] D. Son, H. Yang, and D. Lee. Sim-to-real transfer of bolting tasks with tight tolerance. In
2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
9056–9063,2020. doi:10.1109/IROS45743.2020.9341644.
[37] B. Tang, M. A. Lin, I. Akinola, A. Handa, G. S. Sukhatme, F. Ramos, D. Fox, and Y. S.
Narang. Industreal: Transferring contact-rich assembly tasks from simulation to reality. In
K.E.Bekris,K.Hauser,S.L.Herbert,andJ.Yu,editors,Robotics:ScienceandSystemsXIX,
Daegu,RepublicofKorea,July10-14,2023,2023. doi:10.15607/RSS.2023.XIX.039. URL
https://doi.org/10.15607/RSS.2023.XIX.039.
[38] X. Zhang, C. Wang, L. Sun, Z. Wu, X. Zhu, and M. Tomizuka. Efficient sim-to-real trans-
fer of contact-rich manipulation skills with online admittance residual learning. In 7th An-
nual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id=
gFXVysXh48K.
16[39] X.Zhang,M.Tomizuka,andH.Li. Bridgingthesim-to-realgapwithdynamiccompliance
tuningforindustrialinsertion. arXivpreprintarXiv: Arxiv-2311.07499,2023.
[40] Y.Chebotar,A.Handa,V.Makoviychuk,M.Macklin,J.Issac,N.Ratliff,andD.Fox.Closing
thesim-to-realloop: Adaptingsimulationrandomizationwithrealworldexperience. arXiv
preprintarXiv: Arxiv-1810.05687,2018.
[41] N. Jakobi, P. Husbands, and I. Harvey. Noise and the reality gap: The use of simulation in
evolutionaryrobotics.InF.Mora´n,A.Moreno,J.J.Merelo,andP.Chaco´n,editors,Advances
inArtificialLife,pages704–720,Berlin,Heidelberg,1995.SpringerBerlinHeidelberg.ISBN
978-3-540-49286-3.
[42] E.Tzeng,C.Devin,J.Hoffman,C.Finn,X.Peng,S.Levine,K.Saenko,andT.Darrell. To-
wardsadaptingdeepvisuomotorrepresentationsfromsimulatedtorealenvironments. ArXiv,
abs/1511.07111,2015. URLhttps://api.semanticscholar.org/CorpusID:1541419.
[43] K.Bousmalis,A.Irpan,P.Wohlhart,Y.Bai,M.Kelcey,M.Kalakrishnan,L.Downs,J.Ibarz,
P.Pastor,K.Konolige,S.Levine,andV.Vanhoucke.Usingsimulationanddomainadaptation
to improve efficiency of deep robotic grasping. arXiv preprint arXiv: Arxiv-1709.07857,
2017.
[44] D.Ho,K.Rao,Z.Xu,E.Jang,M.Khansari,andY.Bai.Retinagan:Anobject-awareapproach
tosim-to-realtransfer. arXivpreprintarXiv: Arxiv-2011.03148,2020.
[45] S. Koos, J.-B. Mouret, and S. Doncieux. Crossing the reality gap in evolutionary robotics
by promoting transferable controllers. In Proceedings of the 12th Annual Conference on
GeneticandEvolutionaryComputation, GECCO’10, page119–126, NewYork, NY,USA,
2010.AssociationforComputingMachinery. ISBN9781450300728. doi:10.1145/1830483.
1830505. URLhttps://doi.org/10.1145/1830483.1830505.
[46] Z.Xie,G.Berseth,P.Clary,J.Hurst,andM.vandePanne. Feedbackcontrolforcassiewith
deepreinforcementlearning. arXivpreprintarXiv: Arxiv-1803.05580,2018.
[47] R. Mart´ın-Mart´ın, M. A. Lee, R. Gardner, S. Savarese, J. Bohg, and A. Garg. Variable
impedance control in end-effector space: An action space for reinforcement learning in
contact-richtasks. arXivpreprintarXiv: Arxiv-1906.08880,2019.
[48] J.Wong,V.Makoviychuk,A.Anandkumar,andY.Zhu.Oscar:Data-drivenoperationalspace
controlforadaptiveandrobustrobotmanipulation. arXivpreprintarXiv: Arxiv-2110.00704,
2021.
[49] E.Aljalbout,F.Frank,M.Karl,andP.vanderSmagt. Ontheroleoftheactionspaceinrobot
manipulation learning and sim-to-real transfer. arXiv preprint arXiv: Arxiv-2312.03673,
2023.
[50] Y. S. Narang, K. Storey, I. Akinola, M. Macklin, P. Reist, L. Wawrzyniak, Y. Guo,
A´.Morava´nszky, G.State, M.Lu, A.Handa, andD.Fox. Factory: Fastcontactforrobotic
assembly. InK.Hauser,D.A.Shell,andS.Huang,editors,Robotics: ScienceandSystems
XVIII,NewYorkCity,NY,USA,June27-July1,2022,2022. doi:10.15607/RSS.2022.XVIII.
035. URLhttps://doi.org/10.15607/RSS.2022.XVIII.035.
[51] L. Ljung. System Identification, pages 163–173. Birkha¨user Boston, Boston, MA, 1998.
ISBN978-1-4612-1768-8. doi:10.1007/978-1-4612-1768-8 11. URLhttps://doi.org/
10.1007/978-1-4612-1768-8_11.
[52] P.ChangandT.Padir. Sim2real2sim: Bridgingthegapbetweensimulationandreal-worldin
flexibleobjectmanipulation. arXivpreprintarXiv: Arxiv-2002.02538,2020.
17[53] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic
controlwithdynamicsrandomization. arXivpreprintarXiv: Arxiv-1710.06537,2017.
[54] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk,
K. V. Wyk, A. Zhurkevich, B. Sundaralingam, and Y. S. Narang. Dextreme: Transfer of
agilein-handmanipulationfromsimulationtoreality. InIEEEInternationalConferenceon
Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 5977–
5984.IEEE,2023. doi:10.1109/ICRA48891.2023.10160216. URLhttps://doi.org/10.
1109/ICRA48891.2023.10160216.
[55] J.Wang,Y.Qin,K.Kuang,Y.Korkmaz,A.Gurumoorthy,H.Su,andX.Wang. Cyberdemo:
Augmenting simulated human demonstration for real-world dexterous manipulation. arXiv
preprintarXiv: Arxiv-2402.14795,2024.
[56] G.Schoettler,A.Nair,J.A.Ojea,S.Levine,andE.Solowjow. Meta-reinforcementlearning
forroboticindustrialinsertiontasks. arXivpreprintarXiv: Arxiv-2004.14404,2020.
[57] Y.Zhang,L.Ke,A.Deshpande,A.Gupta,andS.Srinivasa. Cherry-PickingwithReinforce-
mentLearning. InProceedingsofRobotics:ScienceandSystems,Daegu,RepublicofKorea,
July2023. doi:10.15607/RSS.2023.XIX.021.
[58] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. D. Ratliff, and D. Fox.
Closing the sim-to-real loop: Adapting simulation randomization with real world experi-
ence. InInternationalConferenceonRoboticsandAutomation,ICRA2019,Montreal,QC,
Canada,May20-24,2019,pages8973–8979.IEEE,2019.doi:10.1109/ICRA.2019.8793789.
URLhttps://doi.org/10.1109/ICRA.2019.8793789.
[59] J.P.HannaandP.Stone. Groundedactiontransformationforrobotlearninginsimulation. In
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page
4931–4932.AAAIPress,2017.
[60] E.Heiden,D.Millard,E.Coumans,andG.S.Sukhatme. Augmentingdifferentiablesimula-
torswithneuralnetworkstoclosethesim2realgap. arXivpreprintarXiv:Arxiv-2007.06045,
2020.
[61] C.A.CruzandT.Igarashi. Asurveyoninteractivereinforcementlearning:Designprinciples
andopenchallenges. arXivpreprintarXiv: Arxiv-2105.12949,2021.
[62] Y. Cui, P. Koppol, H. Admoni, S. Niekum, R. Simmons, A. Steinfeld, and T. Fitzgerald.
Understandingtherelationshipbetweeninteractionsandoutcomesinhuman-in-the-loopma-
chinelearning. InZ.-H.Zhou, editor, ProceedingsoftheThirtiethInternationalJointCon-
ference on Artificial Intelligence, IJCAI-21, pages 4382–4391. International Joint Confer-
ences on Artificial Intelligence Organization, 8 2021. doi:10.24963/ijcai.2021/599. URL
https://doi.org/10.24963/ijcai.2021/599. SurveyTrack.
[63] R.Zhang, F.Torabi, L.Guan, D.H.Ballard, andP.Stone. Leveraginghumanguidancefor
deepreinforcementlearningtasks. arXivpreprintarXiv: Arxiv-1909.09906,2019.
[64] S.Javdani, S.S.Srinivasa, andJ.A.Bagnell. Sharedautonomyviahindsightoptimization.
arXivpreprintarXiv: Arxiv-1503.07619,2015.
[65] S.Reddy, A.D.Dragan, andS.Levine. Sharedautonomyviadeepreinforcementlearning.
arXivpreprintarXiv: Arxiv-1802.01744,2018.
[66] M.Kelly,C.Sidrane,K.Driggs-Campbell,andM.J.Kochenderfer. Hg-dagger: Interactive
imitationlearningwithhumanexperts. arXivpreprintarXiv: Arxiv-1810.02890,2018.
[67] A.Mandlekar,D.Xu,R.Mart´ın-Mart´ın,Y.Zhu,L.Fei-Fei,andS.Savarese. Human-in-the-
loopimitationlearningusingremoteteleoperation. arXivpreprintarXiv: Arxiv-2012.06733,
2020.
18[68] A.Mandlekar,D.Xu,J.Wong,S.Nasiriany,C.Wang,R.Kulkarni,L.Fei-Fei,S.Savarese,
Y.Zhu,andR.Mart´ın-Mart´ın. Whatmattersinlearningfromofflinehumandemonstrations
forrobotmanipulation. arXivpreprintarXiv: Arxiv-2108.03298,2021.
[69] I.Kostrikov,A.Nair,andS.Levine. Offlinereinforcementlearningwithimplicitq-learning.
arXivpreprintarXiv: Arxiv-2110.06169,2021.
[70] H.Liu, S.Nasiriany, L.Zhang, Z.Bao, andY.Zhu. Robotlearningonthejob: Human-in-
the-loopautonomyandlearningduringdeployment.arXivpreprintarXiv:Arxiv-2211.08416,
2022.
[71] A.Abdolmaleki,J.T.Springenberg,Y.Tassa,R.Munos,N.Heess,andM.Riedmiller. Max-
imumaposterioripolicyoptimisation. arXivpreprintarXiv: Arxiv-1806.06920,2018.
[72] D.A.Pomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. InD.Touretzky,
editor, Advances in Neural Information Processing Systems, volume 1. Morgan-Kaufmann,
1988. URL https://proceedings.neurips.cc/paper_files/paper/1988/file/
812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf.
[73] Y.Zhu,Z.Wang,J.Merel,A.Rusu,T.Erez,S.Cabi,S.Tunyasuvunakool,J.Krama´r,R.Had-
sell,N.deFreitas,andN.Heess.Reinforcementandimitationlearningfordiversevisuomotor
skills. arXivpreprintarXiv: Arxiv-1802.09564,2018.
[74] A. Xie, L. Lee, T. Xiao, and C. Finn. Decomposing the generalization gap in imitation
learningforvisualroboticmanipulation. arXivpreprintarXiv: Arxiv-2307.03659,2023.
[75] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang. Dexpoint: Generalizable point cloud
reinforcementlearningforsim-to-realdexterousmanipulation. arXivpreprintarXiv: Arxiv-
2211.09423,2022.
[76] O. Khatib. A unified approach for motion and force control of robot manipulators: The
operationalspaceformulation.IEEEJournalonRoboticsandAutomation,3(1):43–53,1987.
doi:10.1109/JRA.1987.1087068.
[77] J. Nakanishi, R. Cory, M. Mistry, J. Peters, and S. Schaal. Operational space control: A
theoretical and empirical comparison. The International Journal of Robotics Research, 27
(6):737–757, 2008. doi:10.1177/0278364908091463. URL https://doi.org/10.1177/
0278364908091463.
[78] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu,
V.Mnih,K.Kavukcuoglu,andR.Hadsell. Policydistillation. arXivpreprintarXiv: Arxiv-
1511.06295,2015.
[79] T.Chen,M.Tippur,S.Wu,V.Kumar,E.Adelson,andP.Agrawal. Visualdexterity: In-hand
dexterousmanipulationfromdepth. arXivpreprintarXiv: Arxiv-2211.11744,2022.
[80] T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. In
A.Faust,D.Hsu,andG.Neumann,editors,ConferenceonRobotLearning,8-11November
2021,London,UK,volume164ofProceedingsofMachineLearningResearch,pages297–
307.PMLR,2021. URLhttps://proceedings.mlr.press/v164/chen22a.html.
[81] T.Johannink,S.Bahl,A.Nair,J.Luo,A.Kumar,M.Loskyll,J.A.Ojea,E.Solowjow,and
S. Levine. Residual reinforcement learning for robot control. arXiv preprint arXiv: Arxiv-
1812.03201,2018.
[82] T.Silver,K.Allen,J.Tenenbaum,andL.Kaelbling. Residualpolicylearning. arXivpreprint
arXiv: Arxiv-1812.06298,2018.
[83] A.Zeng, S.Song, J.Lee, A.Rodriguez, andT.Funkhouser. Tossingbot: Learningtothrow
arbitraryobjectswithresidualphysics. arXivpreprintarXiv: Arxiv-1903.11239,2019.
19[84] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimiza-
tionalgorithms. arXivpreprintarXiv: Arxiv-1707.06347,2017.
[85] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotor policy learning via action diffusion. In K. E. Bekris, K. Hauser, S. L. Herbert,
andJ.Yu,editors,Robotics:ScienceandSystemsXIX,Daegu,RepublicofKorea,July10-14,
2023, 2023. doi:10.15607/RSS.2023.XIX.026. URL https://doi.org/10.15607/RSS.
2023.XIX.026.
[86] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d
classificationandsegmentation. arXivpreprintarXiv: Arxiv-1612.00593,2016.
[87] A.Jaegle,F.Gimeno,A.Brock,A.Zisserman,O.Vinyals,andJ.Carreira.Perceiver:General
perceptionwithiterativeattention. arXivpreprintarXiv: Arxiv-2103.03206,2021.
[88] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y. W. Teh. Set transformer: A frame-
workforattention-basedpermutation-invariantneuralnetworks. arXivpreprintarXiv:Arxiv-
1810.00825,2018.
[89] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprintarXiv: Arxiv-1608.03983,2016.
[90] M.Heo,Y.Lee,D.Lee,andJ.J.Lim. Furniturebench: Reproduciblereal-worldbenchmark
forlong-horizoncomplexmanipulation. InK.E.Bekris,K.Hauser,S.L.Herbert,andJ.Yu,
editors, Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023,
2023. doi:10.15607/RSS.2023.XIX.041. URLhttps://doi.org/10.15607/RSS.2023.
XIX.041.
[91] A.Fishman,A.Murali,C.Eppner,B.Peele,B.Boots,andD.Fox.Motionpolicynetworks.In
K.Liu,D.Kulic,andJ.Ichnowski,editors,ConferenceonRobotLearning,CoRL2022,14-18
December2022, Auckland, NewZealand, volume205ofProceedingsofMachineLearning
Research,pages967–977.PMLR,2022. URLhttps://proceedings.mlr.press/v205/
fishman23a.html.
[92] R. Hoque, A. Balakrishna, E. Novoseller, A. Wilcox, D. S. Brown, and K. Goldberg.
Thriftydagger: Budget-awarenoveltyandriskgatingforinteractiveimitationlearning. arXiv
preprintarXiv: Arxiv-2109.08273,2021.
[93] R.TedrakeandtheDrakeDevelopmentTeam. Drake: Model-baseddesignandverification
forrobotics,2019. URLhttps://drake.mit.edu.
[94] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang,
L. Yi, A. X. Chang, L. J. Guibas, and H. Su. Sapien: A simulated part-based interactive
environment. arXivpreprintarXiv: Arxiv-2003.08515,2020.
[95] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, P. P. Tehrani, R. Singh,
Y. Guo, H. Mazhar, A. Mandlekar, B. Babich, G. State, M. Hutter, and A. Garg. Orbit:
Aunifiedsimulationframeworkforinteractiverobotlearningenvironments. arXivpreprint
arXiv: Arxiv-2301.04195,2023.
[96] A.Zeng,P.Florence,J.Tompson,S.Welker,J.Chien,M.Attarian,T.Armstrong,I.Krasin,
D.Duong,A.Wahid,V.Sindhwani,andJ.Lee.Transporternetworks:Rearrangingthevisual
worldforroboticmanipulation. arXivpreprintarXiv: Arxiv-2010.14406,2020.
[97] M.Shridhar,L.Manuelli,andD.Fox. Cliport: Whatandwherepathwaysforroboticmanip-
ulation. arXivpreprintarXiv: Arxiv-2109.12098,2021.
[98] Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,
and L. Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint
arXiv: Arxiv-2210.03094,2022.
20[99] M.Shridhar,L.Manuelli,andD.Fox. Perceiver-actor: Amulti-tasktransformerforrobotic
manipulation. arXivpreprintarXiv: Arxiv-2209.05451,2022.
[100] D.Batra,A.X.Chang,S.Chernova,A.J.Davison,J.Deng,V.Koltun,S.Levine,J.Malik,
I.Mordatch,R.Mottaghi,M.Savva,andH.Su. Rearrangement: Achallengeforembodied
ai. arXivpreprintarXiv: Arxiv-2011.01975,2020.
[101] J. Gu, D. S. Chaplot, H. Su, and J. Malik. Multi-skill mobile manipulation for object rear-
rangement. arXivpreprintarXiv: Arxiv-2209.02778,2022.
[102] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna, T. Gervet, T.-Y. Yang,
V. Jain, A. W. Clegg, J. Turner, Z. Kira, M. Savva, A. Chang, D. S. Chaplot, D. Batra,
R. Mottaghi, Y. Bisk, and C. Paxton. Homerobot: Open-vocabulary mobile manipulation.
arXivpreprintarXiv: Arxiv-2306.11565,2023.
[103] K. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K.-H. Zeng, K. P. Singh, Y. Kim,
W. Han, A. Herrasti, R. Krishna, D. Schwenk, E. VanderBilt, and A. Kembhavi. Imitating
shortestpathsinsimulationenableseffectivenavigationandmanipulationintherealworld.
arXivpreprintarXiv: Arxiv-2312.02976,2023.
[104] Y. Wu, W. Yan, T. Kurutach, L. Pinto, and P. Abbeel. Learning to manipulate deformable
objectswithoutdemonstrations. arXivpreprintarXiv: Arxiv-1910.13439,2019.
[105] H. Ha and S. Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for
clothunfolding. arXivpreprintarXiv: Arxiv-2105.03655,2021.
[106] D. Seita, Y. Wang, S. J. Shetty, E. Y. Li, Z. Erickson, and D. Held. Toolflownet: Robotic
manipulation with tools via predicting tool flow from point clouds. arXiv preprint arXiv:
Arxiv-2211.09006,2022.
[107] X.Lin,Z.Huang,Y.Li,J.B.Tenenbaum,D.Held,andC.Gan. Diffskill: Skillabstraction
from differentiable physics for deformable object manipulations with tools. arXiv preprint
arXiv: Arxiv-2203.17275,2022.
[108] Y. Ji, Z. Li, Y. Sun, X. B. Peng, S. Levine, G. Berseth, and K. Sreenath. Hierarchical re-
inforcement learning for precise soccer shooting skills using a quadrupedal robot. arXiv
preprintarXiv: Arxiv-2208.01160,2022.
[109] Y.J.Ma, W.Liang, G.Wang, D.-A.Huang, O.Bastani, D.Jayaraman, Y.Zhu, L.Fan, and
A. Anandkumar. Eureka: Human-level reward design via coding large language models.
arXivpreprintarXiv: Arxiv-2310.12931,2023.
[110] A. Boeing and T. Bra¨unl. Leveraging multiple simulators for crossing the reality gap. In
2012 12th International Conference on Control Automation Robotics & Vision (ICARCV),
pages1113–1119,2012. doi:10.1109/ICARCV.2012.6485313.
[111] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter.
Learningagileanddynamicmotorskillsforleggedrobots.ScienceRobotics,4(26):eaau5872,
2019. doi:10.1126/scirobotics.aau5872. URLhttps://www.science.org/doi/abs/10.
1126/scirobotics.aau5872.
[112] J. Luo, E. Solowjow, C. Wen, J. A. Ojea, A. M. Agogino, A. Tamar, and P. Abbeel. Rein-
forcementlearningonvariableimpedancecontrollerforhigh-precisionroboticassembly. In
InternationalConferenceonRoboticsandAutomation,ICRA2019,Montreal,QC,Canada,
May 20-24, 2019, pages 3080–3087. IEEE, 2019. doi:10.1109/ICRA.2019.8793506. URL
https://doi.org/10.1109/ICRA.2019.8793506.
[113] P. C. Horak and J. C. Trinkle. On the similarities and differences among contact models in
robotsimulation. IEEERoboticsandAutomationLetters,4(2):493–499,2019. doi:10.1109/
LRA.2019.2891085.
21[114] C.Finn,P.Abbeel,andS.Levine. Model-agnosticmeta-learningforfastadaptationofdeep
networks. arXivpreprintarXiv: Arxiv-1703.03400,2017.
[115] J. A. Fails and D. R. Olsen. Interactive machine learning. In Proceedings of the 8th In-
ternationalConferenceonIntelligentUserInterfaces, IUI’03, page39–45,NewYork, NY,
USA,2003.AssociationforComputingMachinery.ISBN1581135866.doi:10.1145/604045.
604056. URLhttps://doi.org/10.1145/604045.604056.
[116] S. Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role
of humans in interactive machine learning. AI Magazine, 35(4):105–120, Dec. 2014.
doi:10.1609/aimag.v35i4.2513.URLhttps://ojs.aaai.org/aimagazine/index.php/
aimagazine/article/view/2513.
[117] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits prob-
lem. JournalofComputerandSystemSciences,78(5):1538–1556,2012. ISSN0022-0000.
doi:https://doi.org/10.1016/j.jcss.2011.12.028. URLhttps://www.sciencedirect.com/
science/article/pii/S0022000012000281. JCSS Special Issue: Cloud Computing
2011.
[118] A.Jain,B.Wojcik,T.Joachims,andA.Saxena. Learningtrajectorypreferencesformanipu-
latorsviaiterativeimprovement. arXivpreprintarXiv: Arxiv-1306.6294,2013.
[119] P.Christiano,J.Leike,T.B.Brown,M.Martic,S.Legg,andD.Amodei.Deepreinforcement
learningfromhumanpreferences. arXivpreprintarXiv: Arxiv-1706.03741,2017.
[120] E.Bıyık,D.P.Losey,M.Palan,N.C.Landolfi,G.Shevchuk,andD.Sadigh.Learningreward
functionsfromdiversesourcesofhumanfeedback:Optimallyintegratingdemonstrationsand
preferences. arXivpreprintarXiv: Arxiv-2006.14091,2020.
[121] K.Lee,L.Smith,andP.Abbeel. Pebble: Feedback-efficientinteractivereinforcementlearn-
ing via relabeling experience and unsupervised pre-training. arXiv preprint arXiv: Arxiv-
2106.05091,2021.
[122] X.Wang,K.Lee,K.Hakhamaneshi,P.Abbeel,andM.Laskin. Skillpreferences: Learning
to extract and execute robotic skills from human feedback. arXiv preprint arXiv: Arxiv-
2108.05382,2021.
[123] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.L.Wainwright,P.Mishkin,C.Zhang,S.Agar-
wal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,
P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow in-
structionswithhumanfeedback. arXivpreprintarXiv: Arxiv-2203.02155,2022.
[124] V. Myers, E. Bıyık, and D. Sadigh. Active reward learning from online preferences. arXiv
preprintarXiv: Arxiv-2302.13507,2023.
[125] R.Rafailov,A.Sharma,E.Mitchell,S.Ermon,C.D.Manning,andC.Finn.Directpreference
optimization: Yourlanguagemodelissecretlyarewardmodel. arXivpreprintarXiv: Arxiv-
2305.18290,2023.
[126] J.Hejna,R.Rafailov,H.Sikchi,C.Finn,S.Niekum,W.B.Knox,andD.Sadigh. Contrastive
preferencelearning: Learningfromhumanfeedbackwithoutrl. arXivpreprintarXiv: Arxiv-
2310.13639,2023.
[127] W. B. Knox and P. Stone. Reinforcement learning from human reward: Discounting in
episodic tasks. In 2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot
andHumanInteractiveCommunication,pages878–885,2012. doi:10.1109/ROMAN.2012.
6343862.
22[128] B.D.Argall,E.L.Sauser,andA.G.Billard.Tactileguidanceforpolicyrefinementandreuse.
In2010IEEE9thInternationalConferenceonDevelopmentandLearning,pages7–12,2010.
doi:10.1109/DEVLRN.2010.5578872.
[129] T. Fitzgerald, E. Short, A. Goel, and A. Thomaz. Human-guided trajectory adaptation for
tool transfer. In Proceedings of the 18th International Conference on Autonomous Agents
and MultiAgent Systems, AAMAS ’19, page 1350–1358, Richland, SC, 2019. International
FoundationforAutonomousAgentsandMultiagentSystems. ISBN9781450363099.
[130] A. V. Bajcsy, D. P. Losey, M. K. O’Malley, and A. D. Dragan. Learning robot objectives
from physical human interaction. In Conference on Robot Learning, 2017. URL https:
//api.semanticscholar.org/CorpusID:28406224.
[131] A.Najar,O.Sigaud,andM.Chetouani. Interactivelyshapingrobotbehaviourwithunlabeled
humaninstructions. arXivpreprintarXiv: Arxiv-1902.01670,2019.
[132] N.Wilde,E.Bıyık,D.Sadigh,andS.L.Smith. Learningrewardfunctionsfromscalefeed-
back. arXivpreprintarXiv: Arxiv-2110.00284,2021.
[133] J.ZhangandK.Cho. Query-efficientimitationlearningforend-to-endautonomousdriving.
arXivpreprintarXiv: Arxiv-1605.06450,2016.
[134] W. Saunders, G. Sastry, A. Stuhlmueller, and O. Evans. Trial without error: Towards safe
reinforcement learning via human intervention. arXiv preprint arXiv: Arxiv-1707.05173,
2017.
[135] Z. Wang, X. Xiao, B. Liu, G. Warnell, and P. Stone. Appli: Adaptive planner parameter
learningfrominterventions. arXivpreprintarXiv: Arxiv-2011.00400,2020.
[136] C. Celemin and J. R. del Solar. An interactive framework for learning continuous actions
policies based on corrective feedback. Journal of Intelligent & Robotic Systems, 95:77–
97,2018.doi:10.1007/s10846-018-0839-z.URLhttp://link.springer.com/article/
10.1007/s10846-018-0839-z/fulltext.html.
[137] Z. Peng, W. Mo, C. Duan, Q. Li, and B. Zhou. Learning from active human involvement
throughproxyvaluepropagation. InThirty-seventhConferenceonNeuralInformationPro-
cessingSystems,2023. URLhttps://openreview.net/forum?id=q8SukwaEBy.
[138] C. Celemin, R. Pe´rez-Dattari, E. Chisari, G. Franzese, L. de Souza Rosa, R. Prakash,
Z.Ajanovic´,M.Ferraz,A.Valada,andJ.Kober. Interactiveimitationlearninginrobotics: A
survey. arXivpreprintarXiv: Arxiv-2211.00600,2022.
[139] R. Hoque, A. Mandlekar, C. R. Garrett, K. Goldberg, and D. Fox. Interventional data gen-
eration for robust and data-efficient robot imitation learning. In First Workshop on Out-of-
DistributionGeneralizationinRoboticsatCoRL2023,2023. URLhttps://openreview.
net/forum?id=ckFRoOaA3n.
[140] J. Crandall and M. Goodrich. Characterizing efficiency of human robot interaction: a case
study of shared-control teleoperation. In IEEE/RSJ International Conference on Intelligent
Robots and Systems, volume 2, pages 1290–1295 vol.2, 2002. doi:10.1109/IRDS.2002.
1043932.
[141] A. D. Dragan and S. S. Srinivasa. A policy-blending formalism for shared control.
The International Journal of Robotics Research, 32(7):790–805, 2013. doi:10.1177/
0278364913490324. URLhttps://doi.org/10.1177/0278364913490324.
[142] D.Gopinath,S.Jain,andB.D.Argall. Human-in-the-loopoptimizationofsharedautonomy
in assistive robotics. IEEE Robotics and Automation Letters, 2(1):247–254, 2017. doi:10.
1109/LRA.2016.2593928.
23[143] A. D. Dragan and S. S. Srinivasa. Formalizing assistive teleoperation, volume 376. MIT
Press,July,2012.
[144] H.J.Jeon,D.P.Losey,andD.Sadigh. Sharedautonomywithlearnedlatentactions. arXiv
preprintarXiv: Arxiv-2005.03210,2020.
[145] Y.Cui,S.Karamcheti,R.Palleti,N.Shivakumar,P.Liang,andD.Sadigh. ”no,totheright”
–onlinelanguagecorrectionsforroboticmanipulationviasharedautonomy. arXivpreprint
arXiv: Arxiv-2301.02555,2023.
[146] P. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel. Gello: A general, low-cost, and intuitive
teleoperation framework for robot manipulators. arXiv preprint arXiv: Arxiv-2309.13037,
2023.
[147] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. Liu. Dexcap: Scalable and
portable mocap data collection system for dexterous manipulation. arXiv preprint arXiv:
Arxiv-2403.07788,2024.
[148] Z.Fu,T.Z.Zhao,andC.Finn. Mobilealoha: Learningbimanualmobilemanipulationwith
low-costwhole-bodyteleoperation. arXivpreprintarXiv: Arxiv-2401.02117,2024.
[149] S.Dass,W.Ai,Y.Jiang,S.Singh,J.Hu,R.Zhang,P.Stone,B.Abbatematteo,andR.Mart´ın-
Mart´ın. Telemoma: Amodularandversatileteleoperationsystemformobilemanipulation.
arXivpreprintarXiv: Arxiv-2403.07869,2024.
[150] T. He, Z. Luo, W. Xiao, C. Zhang, K. Kitani, C. Liu, and G. Shi. Learning human-to-
humanoid real-time whole-body teleoperation. arXiv preprint arXiv: Arxiv-2403.04436,
2024.
[151] H.Liu,S.Dass,R.Mart´ın-Mart´ın,andY.Zhu. Model-basedruntimemonitoringwithinter-
activeimitationlearning. arXivpreprintarXiv: Arxiv-2310.17552,2023.
[152] Z.Liu,A.Bahety,andS.Song. Reflect: Summarizingrobotexperiencesforfailureexplana-
tionandcorrection. arXivpreprintarXiv: Arxiv-2306.15724,2023.
[153] L.Ankile,A.Simeonov,I.Shenfeld,andP.Agrawal.Juicer:Data-efficientimitationlearning
forroboticassembly. arXivpreprintarXiv: Arxiv-2404.03729,2024.
[154] M. N. Mistry and L. Righetti. Operational space control of constrained and underactuated
systems. InRobotics:ScienceandSystems,2011. URLhttps://api.semanticscholar.
org/CorpusID:10392712.
[155] L.Fan, G.Wang, Y.Jiang, A.Mandlekar, Y.Yang, H.Zhu, A.Tang, D.-A.Huang, Y.Zhu,
and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale
knowledge. arXivpreprintarXiv: Arxiv-2206.08853,2022.
[156] D.-A.Clevert,T.Unterthiner,andS.Hochreiter. Fastandaccuratedeepnetworklearningby
exponentiallinearunits(elus). arXivpreprintarXiv: Arxiv-1511.07289,2015.
[157] D. Makoviichuk and V. Makoviychuk. rl-games: A high-performance framework for rein-
forcementlearning. https://github.com/Denys88/rl_games,May2021.
[158] J.Schulman, P.Moritz, S.Levine, M.Jordan, andP.Abbeel. High-dimensionalcontinuous
control using generalized advantage estimation. arXiv preprint arXiv: Arxiv-1506.02438,
2015.
[159] D.P.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:
Arxiv-1412.6980,2014.
24[160] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):
1735–1780, nov 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735.
[161] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:
Arxiv-1606.08415,2016.
[162] Y.Zhu,A.Joshi,P.Stone,andY.Zhu.Viola:Imitationlearningforvision-basedmanipulation
withobjectproposalpriors. 6thAnnualConferenceonRobotLearning,2022.
[163] C.R.Qi, L.Yi, H.Su, andL.J.Guibas. Pointnet++: Deephierarchicalfeaturelearningon
pointsetsinametricspace. arXivpreprintarXiv: Arxiv-1706.02413,2017.
[164] L. X. Shi, Y. Jiang, J. Grigsby, L. J. Fan, and Y. Zhu. Cross-episodic curriculum for trans-
formeragents. arXivpreprintarXiv: Arxiv-2310.08549,2023.
25A SimulationTrainingDetails
Inthissection,weprovidedetailsaboutsimulationtraining,includingtheusedsimulatorbackend,
taskdesigns,reinforcementlearning(RL)trainingofteacherpolicy,andstudentpolicydistillation.
A.1 TheSimulator
WeuseIsaacGymPreview4[10]asthesimulatorbackend. NVIDIAPhysX1isusedasthephysics
engine to provide realistic and precise simulation. Simulation settings are listed in Table A.I. The
robotmodelisfromFrankaROSpackage2. WeborrowfurnituremodelsfromFurnitureBench[90]
tocreatevarioustasksthatrequirecomplexandcontact-richmanipulation.
TableA.I:Simulationsettings.
Hyperparameter Value
SimulationFrequency 60Hz
ControlFrequency 60Hz
NumSubsteps 2
NumPositionIterations 8
NumVelocityIterations 1
A.2 TaskImplementations
Weimplementfourtasksbasedonthefurnituremodelsquare table: Stabilize,ReachandGrasp,
Insert,andScrew. AnoverviewofsimulatedtasksisshowninFigA.1. Weelaborateontheirinitial
conditions,successcriteria,rewardfunctions,andothernecessaryinformation.
A.2.1 Stabilize
In this task, the robot needs to push the square tabletop to the right corner of the wall such that it
is supported and remains stable in following assembly steps. The robot is initialized such that its
gripperlocatesataneutralposition. Thetabletopisinitializedatthecoordinate(0.54,0.00)relative
totherobotbase. Wethenrandomlytranslateitwithdisplacementsdrawnfrom ( 0.015,0.015)
U −
alongxandydirections(thedistanceunitismeterhereafter). WealsoapplyrandomZrotationwith
valuesdrawnfrom ( 15°,15°). Fourtablelegsareinitializedinthescene. Thetaskissuccessful
U −
onlywhenthefollowingthreeconditionsaremet:
1) Thesquaretabletopcontactsthefrontandrightwalls;
2) Thesquaretabletopiswithinapre-definedregion;
3) Notablelegisinthepre-definedregion.
Weusethefollowingrewardfunction:
r =w 1 w q˙ w a , (A.1)
t success success q˙ t action t
− ∥ ∥− ∥ ∥
where w is the success reward, 1 indicates the success according to aforementioned
success success
conditions,w penalizeslargejointvelocities,q˙ isthejointvelocity,w penalizeslargeaction
q˙ t action
commands,anda representstheactioncommandattimestept.Wesetw =10,w =10−5,
t success q˙
andw =10−5. Theepisodelengthis100. Oneepisodeterminatesuponsuccessortimeout.
action
A.2.2 ReachandGrasp
In this task, the robot needs to reach and grasp a table leg that is randomly spawned in the valid
workspaceregion. Thetaskissuccessfuloncetherobotgraspsthetablelegandliftsitforacertain
1https://developer.nvidia.com/physx-sdk
2https://github.com/frankaemika/franka_ros
26(a)Stabilize (b)ReachandGrasp
(c)Insert (d)Screw
FigureA.1: Visualizationofsimulatedtasks.
height. The object’s irregular shape limits certain grasping poses. For example, the end-effector
needs to be near orthogonal to the table leg in the xy plane and far away from the screw thread.
Therefore,wedesignacurriculumovertheobjectgeometrytowarmuptheRLlearning.Itgradually
adjusts the object geometry from a cube, to a cuboid, and finally the table leg. In all curriculum
stages,therewardfunctionis
r =w d+w 1 +w 1 . (A.2)
t distance lifted lifted success success
Here,w istheweightfordistancereward,w istherewardforthelegbeinglifted,and
distance lifted
w isthesuccessweight. disthedistancetothetablelegandiscalculatedas
success
(cid:18) (cid:19)
10
d=1 tanh (d +d +d +d ) , (A.3)
− 4 eef leftfinger rightfinger orthogonal
where d is the distance between the end-effector and the table leg, d is the distance
eef leftfinger
betweentheleftgrippertiptothetableleg,d isthedistancebetweentherightgrippertip
rightfinger
to the table leg, and d is the difference between the current and the orthogonal grasping
orthogonal
orientations. Wesetw = 0.1,w = 1.0,andw = 200.0. Theepisodelengthis
distance lifted success
50. Oneepisodeterminatesuponsuccessortimeout.
A.2.3 Insert
In this task, the robot needs to insert a pre-grasped table leg into the far right assembly hole of
the tabletop, while the tabletop is already stabilized. The tabletop is initialized at the coordinate
(0.53,0.05) relative to the robot base. We then randomly translate it with displacements sam-
pled from ( 0.02,0.02) along x and y directions. We also apply random Z rotation with values
U −
drawnfrom ( 45°,45°). Wefurtherrandomizetherobot’sposebyaddingnoisessampledfrom
U −
( 0.25,0.25) to joint positions. The task is successful when the table leg remains vertical and
U −
is close to the correct assembly position within a small threshold. We design curricula over the
randomizationstrengthtofacilitatethelearning. Thefollowingrewardfunctionisused:
r =w d+w 1 , (A.4)
t distance success success
27where w is the weight for distance-based reward, d is the distance between the table leg
distance
and target assembly position, w is the success weight, and 1 indicates task success.
success success
The distance d consists of an Euclidean distance d and an orientation distance d to
position vertical
encouragetherobottokeepthetablelegvertical.
(cid:18) (cid:19)
10
d=1 tanh d +d (A.5)
− 2 position vertical
Wesetw = 1.0andw = 100.0. Theepisodelengthis100. Oneepisodeterminates
distance success
uponsuccessortimeout.
A.2.4 Screw
Inthistask,therobotisinitializedsuchthatitsend-effectorisclosetoaninsertedtableleg. Itneeds
to screw the table leg clockwise into the tabletop. We design curricula over the action space: at
theearlystage,therobotonlycontrolstheend-effector’sorientation;atthelatterstage,itgradually
takes full control. We slightly randomize object and robot poses during initialization. The reward
functionis
r =(1 1 )(w d +w 1 ) w d . (A.6)
t failure screw screw success success deviation deviation
− −
Here, 1 indicates the task failure, w is the screwing reward weight, d measures
failure screw screw
thescrewedangle,w isthesuccessweight,and1 indicatesthetasksuccess. Thetask
success success
is considered as successful when the leg has been screwed 180° into the tabletop. It is considered
as failed when the table leg tilts more than 10° from the vertical pose. We set w = 0.1,
screw
w =100.0,andw =10−2. Theepisodelengthis200. Oneepisodeterminatesupon
success deviation
success,failure,ortimeout.
A.3 TeacherPolicyTraining
A.3.1 ModelDetails
ObservationSpace Besidesproprioceptiveobservations,teacherpoliciesalsoreceiveprivileged
observations to facilitate the learning. They include objects’ states (poses and velocities), end-
effector’s velocity, contact forces, gripper left and right fingers’ positions, gripper center position,
andjointvelocities. FullobservationsaresummarizedinTableA.II.
TableA.II:Theobservationspaceforteacherpolicies.
Name Dimension Name Dimension
Proprioceptive Privileged
CosJ ino ein Jt oP ino tsi Pti oo sn
ition
77 EndO -Eb fj fe ec ct ts oS rt Va ete los
city
Nobjec 6ts×13
SineJointPosition 7 C Lo efn tta ac nt dF Ro ir gc hes
t
Nobjects×3
End-EffectorPosition 3 6
Fingers’Positions
End-EffectorRotation 4 GripperCenterPosition 3
GripperWidth 1 JointVelocity 7
Controller and Action Space An operational space controller (OSC) [76] is used in teacher
policytrainingtoimprovesampleefficiency. WefollowMistryandRighetti[154]toaddnullspace
controltorquestopreventlargechangesinjointconfiguration. Theactionspaceisthusthechange
ofend-effector’spose. Wefurtheraddabinaryactiontocontrolgripper’sopeningandclosing. For-
mally,itcanbeexpressedas =(δx,δy,δz,δr,δp,δy,1 ),where(δx,δy,δz) R3
teacher gripper
is the translation change, (δrA ,δp,δy) R3 is the rotation change, and 1 0,1 i∈ s the
gripper
∈ ∈ { }
gripperaction.
Model Architecture We use feed-forward policies [155] in RL training. It consists of MLP
encoders to encode proprioceptive and privileged vector observations, and unimodal Gaussian
distributionsastheactionhead. ModelhyperparametersarelistedinTableA.III.
28TableA.III:ModelhyperparametersforRLteacherpolicies.
Hyperparameter Value Hyperparameter Value
Obs.EncoderHiddenDepth 1 Obs.EncoderActivation ReLU
Obs.EncoderHiddenDim 256 ActionHeadHiddenLayers [256,128,64]
Obs.EncoderOutputDim 256 ActionHeadActivation ELU[156]
A.3.2 DomainRandomization
WeapplydomainrandomizationduringRLtrainingtolearnmorerobustteacherpolicies. Parame-
tersaresummarizedinTableA.IV.
TableA.IV:DomainrandomizationusedinRLtraining.
Parameter Type Distribution
Robot
Mass Scaling (0.5,1.5)
U
Friction Scaling (0.7,1.3)
U
JointLowerLimit Scaling log (1.00,1.01)
U
JointUpperLimit Scaling log (1.00,1.01)
U
JointStiffness Scaling log (1.00,1.01)
U
JointDamping Scaling log (1.00,1.01)
U
Simulation
Gravity Additive (0.0,0.4)
U
Objects
Mass Scaling (0.5,1.5)
U
Friction Scaling (0.5,1.5)
U
RollingFriction Scaling (0.5,1.5)
U
TorsionFriction Scaling (0.5,1.5)
U
Restitution Additive (0.0,1.0)
U
Compliance Additive (0.0,1.0)
U
A.3.3 RLTrainingDetails
We use the model-free RL algorithm Proximal Policy Optimization (PPO) [84] to learn teacher
policies. HyperparametersarelistedinTableA.V.WecustomizetheframeworkfromMakoviichuk
andMakoviychuk[157]touseasourtrainingframework.
TableA.V:HyperparametersusedinPPOtraining.
Hyperparameter Value Hyperparameter Value
LearningRate 5 10-4 CriticWeight 4
×
DiscountFactor 0.99 GAE[158]λ 0.95
EntropyWeight 0 PPOϵ 0.2
Optimizer Adam[159] Horizon 32
A.4 StudentPolicyDistillation
A.4.1 DataGeneration
Weusetrainedteacherpoliciesasoraclestogeneratedataforstudentpoliciestraining. Concretely,
werollouteachteacherpolicytogenerate10,000successfultrajectoriesforeachtask. Weexclude
trajectoriesthatareshorterthan20steps.
29A.4.2 ObservationSpace
Student policies receive observations that can be obtained in the real world. They are point-
cloud and proprioceptive observations. We synthesize point clouds from objects’ 6D poses to im-
prove the training throughput. Concretely, given the groundtruth point cloud of the m-th object
P(m) RK×3,wetransformitintotheglobalframethroughP(m) = P(m)(cid:0) R(m)(cid:1)⊺ +(cid:0) p(m)(cid:1)⊺ .
g
∈
Here R(m) R3×3 and p(m) R3×1 denote the object’s orientation and translation in the
∈ ∈
global frame. Further, the point-cloud representation of a scene S with M objects is aggregated
as PS = (cid:83)M P(m). For the robot, we only include point clouds for its two fingers and ignore
m=1 g
otherparts. Tofacilitatepoliciestodifferentiategripperfingersfromthescene, weextendtheco-
ordinatedimensiontoincludeasemanticlabel 0,1 thatindicatesgripperfingersornot. This
∈ { }
informationcanbeobtainedonrealrobotsthroughforwardkinematics. Afullpointcloudisthen
downsampledto768points. TableA.VIliststheobservationspace.
FigureA.2: Visualizationofpairedpointcloudsinsimulation(red)andreality(blue).
TableA.VI:Theobservationspaceforstudentpolicies.
Name Dimension
PointCloud 768 4
×
Proprioceptive
JointPosition 7
CosineJointPosition 7
SineJointPosition 7
End-EffectorPosition 3
End-EffectorRotation 4
GripperWidth 1
A.4.3 ActionSpaceDistillation
To reduce the controller sim-to-real gap before transfer, we train student policies to output in the
configurationspace.Toachievethat,werelabelactionsaˆintrajectoriesgeneratedbyteacherpolicies
fromend-effector’sdeltaposestoabsolutejointpositions. Thisisequivalenttosetaˆ = q for
t t+1
all time steps. Therefore, the action space for student policies is = (q,1 ), where
student gripper
q R7 is the joint position within the valid range. In simulatiA on, student policies’ actions are
∈
deployedwithajointpositioncontroller.
A.4.4 ModelArchitecture
Weusefeed-forwardpoliciesfortasksReachandGraspandInsertandrecurrentpoliciesfortasks
StabilizeandScrewaswefindtheyachievethebestdistillationresults.PointNets[86]areusedtoen-
30Left
Right
Cameras
Cameras
Wrist

Camera
Fixed
Wall
FigureA.3:Systemsetup. OursystemconsistsofaFrankaEmika3robotmountedonthetabletop,
four fixed cameras and one wrist camera (positioned at the rear side of the end-effector) for point
cloudreconstruction,anda3d-printedthree-sidedwallgluedontotabletoptoprovideexternalsup-
port.
codepointclouds. Recallthateachpointinthepointcloudalsocontainsasemanticlabelindicating
thegripperornot. Weconcatenatepointcoordinateswiththesesemanticlabels’vectorembeddings
before passing into the PointNet encoder. We use Gaussian Mixture Models (GMM) [68] as the
actionhead. DetailedmodelhyperparametersarelistedinTableA.VII.
TableA.VII:Modelhyperparametersforstudentpolicies.
Hyperparameter Value Hyperparameter Value
PointCloud RNN
PointNetHiddenDim 256 RNNType LSTM[160]
PointNetHiddenDepth 2 RNNNumLayers 2
PointNetOutputDim 256 RNNHiddenDim 512
PointNetActivation GELU[161] RNNHorizon 5
GripperSemanticEmbdDim 128 GMMActionHead
FeatureFusion HiddenDim 128
MLPHiddenDim 512 HiddenDepth 3
MLPHiddenDepth 1 NumModes 5
MLPActivation ReLU Activation ReLU
A.4.5 DataAugmentation
Weapplystrongdataaugmentationduringdistillation. Forpoint-cloudobservations,randomtrans-
lationandrandomjitterareindependentlyappliedwithaprobabilityP = 0.4. Wealsoadd
pcdaug
Gaussiannoisestoproprioceptiveobservations.AugmentationparametersarelistedinTableA.VIII.
A.4.6 TrainingDetails
To regularize point-cloud features, we separately collect a dataset containing 59 pairs of matched
pointcloudsinsimulationandreality. OnepairfromthemisvisualizedinFigA.2. Studentpolicies
aretrainedbyminimizingEq.1,wherewesetβ = 10−3. WeusetheAdamoptimizer[159]with
a learning rate of 10−4 during training. We periodically roll out student policies in simulation for
1,000episodes. Wethenselectthecheckpointthatcorrespondstothehighestsuccessratetouseas
thebasepolicyinthereal-worldlearningstage.
31TableA.VIII:Dataaugmentationusedindistillation.
Hyperparameter Value
PointCloud
AugmentationProbability 0.4
RandomTranslationDistribution ( 0.04,0.04)
U −
RandomJitteringRatio 0.1
RandomJitteringDistribution (0,0.01)
N
RandomJitteringLow -0.015
RandomJitteringHigh 0.015
Proprioception
Prop. NoiseDistribution (0,0.1)
N
Prop. NoiseLow -0.3
Prop. NoiseHigh 0.3
FigureA.4: Asoftgripperthatcanbendforbettergrasping.
B Real-WorldLearningDetails
Inthissection,weprovidedetailsaboutreal-worldlearning,includingthehardwaresetup,human-
in-the-loopdatacollection,andresidualpolicytraining.
B.1 HardwareSetup
As shown in Fig. A.3, our system consists of a Franka Emika 3 robot mounted on the tabletop.
We use four fixed cameras and one wrist camera for point cloud reconstruction. They are three
RealSenseD435andtwoRealSenseD415. Thereisalsoa3d-printedthree-sidedwallgluedontop
ofthetabletoprovideexternalsupport. Weuseasoftgripperforbettergrasping(FigA.4). Weuse
ajointpositioncontrollerfromtheDeoxyslibrary[162]tocontrolourrobotat1000Hz.
B.2 ObtainingPointCloudsfromMulti-ViewCameras
Weusemulti-viewcamerasforpointcloudreconstructiontoavoidocclusions. Specifically,wefirst
calibrateallcamerastoobtaintheirposesintherobotbaseframe. Wethentransformcapturedpoint
clouds in camera frames to the robot base frame and concatenate them together. We further per-
form cropping based on coordinates and remove statistical and radius outliers. To identify points
belongingtothegrippersothatwecanaddgrippersemanticlabels(Sec.A.4.2),wecomputeposes
fortwogripperfingersthroughforwardkinematics. Wethenremovemeasuredpointscorrespond-
ingtogripperfingersthroughK-nearestneighbor, givenfingers’posesandsyntheticpointclouds.
Subsequently,weaddsemanticlabelstopointsbelongingtothesceneandsyntheticgripper’spoint
clouds. Finally, weuniformlydown-samplewithoutreplacement. Weopttonotusefarthestpoint
sampling[163]duetoitsslowspeed. OneexampleisshowninFig.A.5.
32FigureA.5:Visualizationofreal-worldpoint-cloudobservations.Weobtainthemby1)cropping
pointcloudsfusedfrommulti-viewcamerasbasedoncoordinates,2)removingstatisticalandradius
outliers, 3) removing points corresponding to gripper fingers and replacing with synthetic point
cloudsthroughforwardkinematics,4)uniformlysamplingwithoutreplacement,and5)appending
semanticlabelstoindicategripperfingers(red)andthescene(blue).
B.3 Human-in-the-LoopDataCollection
ThisdatacollectionprocedureisillustratedinAlgorithm1. AsshowninFig.A.6,weusea3Dcon-
nexionSpaceMouseastheteleoperationdevice. WedesignaspecificUI(Fig.A.7)tofacilitatethe
synchronizeddatacollection. Here,thehumanoperatorwillbeaskedtointerveneornot. Theoper-
atoranswersthroughkeyboard. Iftheoperatordoesnotintervene,thebasepolicy’snextactionwill
be deployed. If the operator decides to intervene, the SpaceMouse is then activated to teleoperate
therobot. Afterthecorrection, theoperatorcanexittheinterventionmodebypressingonebutton
on the SpaceMouse. We use this system and interface to collect 20, 100, 90, and 17 trajectories
withcorrectionfortasksStabilize,ReachandGrasp,Insert,andScrew,respectively. Weuse90%
ofthemastrainingdataandtheremainingasheld-outvalidationsets. Wevisualizethecumulative
distributionfunctionofhumancorrectioninFigureA.8.
SpaceMouse
FigureA.6: Realworkspacesetupforhuman-in-the-loopdatacollection. Thehumanoperator
provides online correction through a 3Dconnexion SpaceMouse while monitoring the robot’s exe-
cution.
B.4 ResidualPolicyTraining
B.4.1 ModelArchitecture
The residual policy takes the same observations as the base policy (Table A.VI). Furthermore, to
effectively predict residual actions, it is also conditioned on base policy’s outputs. Its action head
outputs eight-dim vectors, while the first seven dimensions correspond to residual joint positions
and the last dimension determines whether to negate base policy’s gripper action or not. Besides,
aseparateinterventionheadpredictswhethertheresidualactionshouldbeappliedornot(learned
gatedresidualpolicy,Sec.3.3).
33Algorithm1:HumanInterventionandOnlineCorrectionDataCollection
input :BasepolicyπB,humanpolicyπH,real-worldenvironment
output :Humancorrectiondataset H E
initialize: H D
D ←∅
o .reset()
←E
whilenot .terminateddo
E
▷ deploy the base policy for one step
aB aB πB(o)
← ∼
onext .deploy(aB)
←E
▷ human decides intervention or not
1H πH.intervene(o,onext)
←
if1H then
qpre .robot state
←E
▷ deploy human correction
aH aH πH(o,onext)
← ∼
onext .deploy(aH)
←E
qpost .robot state
←E
▷ update dataset
H H (cid:0) qpre,qpost,1H,o(cid:1)
D ←D ∪
end
▷ update observation for the next step
o onext
end ←
FortasksStabilizeandInsert, weuseaPointNet[86]asthepoint-cloudencoder. FortasksReach
and Grasp and Screw, we use a Perceiver [87, 88] as the point-cloud encoder. Residual policies
areinstantiatedasfeed-forwardpoliciesinalltasks. WeuseGMMastheactionheadandasimple
two-wayclassifierastheinterventionhead. ModelhyperparametersaresummarizedinTableA.IX.
TableA.IX:Modelhyperparametersforresidualpolicies.
Hyperparameter Value Hyperparameter Value
PointNet FeatureFusion
PointNetHiddenDim 256 MLPHiddenDim 512
PointNetHiddenDepth 2 MLPHiddenDepth 1
PointNetOutputDim 256 MLPActivation ReLU
PointNetActivation GELU GMMActionHead
GripperSemanticEmbdDim 128 HiddenDim 128
Perceiver HiddenDepth 3
PerceiverHiddenDim 256 NumModes 5
PerceiverNumberofHeads 8 Activation ReLU
PerceiverNumberofQueries 8 InterventionHead
GripperSemanticEmbdDim 128 HiddenDim 128
BasePolicyActionConditioning HiddenDepth 3
BasePolicyGripperActionEmbdDim 64 Activation ReLU
B.4.2 TrainingDetails
Totrainthelearnedgatedresidualpolicy,wefirstonlylearnthefeatureencoderandtheactionhead.
We then freeze the entire model and only learn the intervention head. We opt for this two-stage
trainingsincewefindthattrainingbothactionandinterventionheadsatthesametimewillresultin
sub-optimalresidualactionprediction.Wefollowthebestpracticeforpolicytraining[98,155,164],
including using learning rate warm-up and cosine annealing [89]. Training hyperparameters are
listedinTableA.X.
34(...)
system: need human intervention? (y/n)
user: n
(deploying the next action)
system: need human intervention? (y/n)
user: y
(correction through teleopeartion)
system: exiting human intervention...
(...)
FigureA.7: TheUIforsynchronizedhuman-in-the-loopdatacollection.
Stabilize
1.0 ReachandGrasp
Insert
0.8
Screw
0.6
0.4
0.2
0.0
0 50 100
Task Progress (%)
FigureA.8: Cumulativedistributionfunction(CDF)ofhumancorrection. Shadedregionsrep-
resent standard deviation. Human correction happens at different times across tasks. This fact
necessitatesTRANSIC’slearnedgatingmechanism.
C ExperimentSettingsandEvaluationDetails
Inthissection,weprovidedetailsaboutourexperimentsettingsandevaluationprotocols.
C.1 MainExperiments(Sec.4.2)
We evaluate all methods on four tasks for 20 trials. Each trail starts with different objects and
robotposes. Wemakeourbesteffortstoensurethesameinitialsettingswhenevaluatingdifferent
methods. Specifically,wetakepicturesforthese20differentinitialconfigurationsandrefertothem
when resetting a new trial. See Figs. A.14, A.15, A.16, A.17 for initial configurations of tasks
Stabilize,ReachandGrasp,Insert,andScrew,respectively.
C.2 ExperimentswithDifferentSim-to-RealGaps(Sec.4.3)
C.2.1 ExperimentSetup
Weelaborateonhowdifferentsim-to-realgapsarecreated.
PerceptionError Thisisdonebyapplyingrandomjitterto25%pointsfrompointclouds,which
correspondstoaddingnoiseinobservationspace . Wetestthissim-to-realgaponthetaskReach
O
and Grasp. As visualized in Fig. A.9, with probability P = 0.6, we apply random jitter to 25%
points from the point-cloud observation. The jittering noise is sampled independently from the
distribution (0,0.03). Weclipthenoisetobewithinthe 0.03range.
N ±
Underactuated Controller This is done by making the joint position controller less accurate,
whichcorrespondstomismatchedactionspace . WetestthisgaponthetaskInsert. Weemulate
A
35
P
fo
FDC
noitcerrocTableA.X:Hyperparametersusedinresidualpolicytraining.
Hyperparameter Value
LearningRate 10−4
WeightDecay 0
LearningRateWarmUpSteps 1,000
LearningRateCosineDecaySteps 100,000
MinimalLearningRate 10−6
Optimizer Adam
(a) (b)
FigureA.9:Visualizationofintroducedperceptionerror.a)Theoriginalpoint-cloudobservation.
b)Theerroneouspoint-cloudobservationwithrandomjitter.
an underactuated controller through early stopping. Concretely, at every time a new joint position
goal q is set, we record the distance to the goal in configuration space d = q q
goal q goal
∥ − ∥
and sample a factor Γ (0.80,0.95). The controller will stop reaching the desired goal once it
∼ U
achievesΓprogress,i.e.,stopearlywhen q q (1 Γ)d . Fig.A.10visualizestheeffect.
goal q
∥ − ∥≤ −
EmbodimentMismatch Thisisdonebychangingtherobotgrippertobeshorterlengthasdemon-
strated in Fig. A.11, which corresponds to discrepancy in state space and transition function .
S T
WetestthisgaponthetaskScrew.Wenoticethatthe9cmlengthdifferenceincursasignificantgap.
Dynamics Difference This is done by changing object surfaces and increasing friction, which
correspondstodifferenttransitionfunction . WetestthisgaponthetaskStabilize. Concretely,we
T
36
0.2
0.0 18
0.2
− 0
0.2 0.0 0.2
−
End-EffectorXPosition(m)
Reference NormalController UnderactuatedController
FigureA.10: Visualizationofthetrajectoryrealizedbyanunderactuatedcontroller. Theplot
displays the end-effector’s position in the XY plane. It shows a reference circular movement, a
trajectorytrackedbythenormalcontroller,andatrajectorytrackedbytheunderactuatedcontroller.
36
)m(noitisoPYrotceffE-dnE
petSemiTyrotcejarT5 cm
14 cm
FigureA.11: Twodifferentgripperfingersusedtocreateembodimentmismatch. Policiesare
trainedwiththelongerfingerandtestedontheshorterfinger.
(a) (b)
FigureA.12: Twosquaretabletopsusedtocreatedynamicsdifference. a)Theoriginalsurface
issmooth. b)Weattachfrictiontapestochangethedynamics.
attachfrictiontapestothesquaretabletop’ssurfacetoincreasefriction,hencechangethedynamics
(Fig.A.12).
Object Assert Mismatch As shown in Fig. A.13, this is done by replacing the table leg with a
lightbulb,whichcorrespondstochangeinemittingfunctionΩ. WetestthisgaponthetaskReach
andGrasp.
(a) (b)
FigureA.13: Twoobjectsusedtocreateassetmismatch. a)Policiesaretrainedwiththetableleg.
b)Wetestpolicieswithanunseenlightbulb.
37Table A.XI: Quantitative results for scalability with human correction dataset size on four
tasks.
CorrectionDatasetSize(%)
Method
0 25 50 75 100
Stabilize
TRANSIC 80% 80% 100% 100%
35%
IWR[67] 70% 75% 80% 65%
ReachandGrasp
TRANSIC 65% 80% 90% 95%
60%
IWR[67] 60% 65% 40% 40%
Insert
TRANSIC 20% 35% 40% 45%
5%
IWR[67] 5% 15% 30% 40%
Screw
TRANSIC 50% 65% 75% 85%
35%
IWR[67] 20% 40% 40% 40%
C.2.2 Evaluation
We conduct 20 trails with different initial configurations. Initial conditions for first four experi-
ments are the same as main experiments (Figs. A.14, A.15, A.16, A.17). Fig. A.18 shows initial
configurationsfortheexperimentObjectAssetMismatch.
C.3 DataScalabilityExperiments(Sec.4.4)
In Table A.XI, we show quantitative results for scalability with human correction dataset size on
fourtasks.
C.4 PolicyRobustnessExperiments(Sec.4.5)
C.4.1 RemovingCameras
We remove two cameras and only keep three. Note that this is the same number of cameras as in
FurnitureBench[90]. FortasksotherthanInsert,wekeepthewristcamera,therightfrontcamera,
andtheleftrearcamera. ForthetaskInsert,wekeeptwofrontcamerasandtheleftrearcamera.
C.4.2 SuboptimalCorrectionData
We simulate suboptimal correction data by injecting noise into residual actions aR. This noise is
of large magnitude, which follows the normal distribution with zero mean and standard deviation
correspondingto5%ofthelargestresidualactioninthedataset.
D AdditionalExperimentResults
D.1 DistillingSimulationBasePolicywithDiffusionPolicy
We experiment with learning simulation base policies (Sec. 3.1) with the Diffusion Policy [85].
Concretely, when performing action space distillation to learn student policies, we replace the
Gaussian Mixture Model (GMM) action head with the Diffusion Policy. Proper data augmenta-
tion (Table A.VIII) is also applied to robustify learned policies. Hyperparameters are provided in
TableA.XII.
38TableA.XII:DiffusionPolicyhyperparameters.
Hyperparameter Value Hyperparameter Value
Architecture UNet To 2
UNetHiddenDims [64,128] Ta 8
UNetKernelSize 5 Tp 16
UNetGroupNormNumGroups 8 NumDenoisingSteps(Train) 100
DiffusionStepEmbdDim 128 NumDenoisingSteps(Eval) 16
ThecomparisonbetweenGMMsontherealrobotisshowninTable.A.XIII. Wehighlighttwofind-
ings. First,thesignificantdomaindifferencebetweensimulationandrealitygenerallyexistsregard-
less of different policy modeling methods. Second, since the Diffusion Policy plans and executes
a future trajectory, it is more vulnerable to simulation-to-reality gaps due to planning inaccuracy
andtheconsequentcompoundingerror. Onlyexecutingthefirstactionfromtheplannedtrajectory
and re-planning at every step may help, but the inference latency renders the real-time execution
infeasible.
TableA.XIII:Thereal-robotperformancedifferencebetweenGMMandDiffusionPolicy. The
policyerrorcausedbysimulation-to-realitygapswillbeamplifiedbytheDiffusionPolicybecause
itplansandexecutesafuturetrajectory.
Reach
Average Stablize Insert Screw
andGrasp
GMM 33.7% 35% 60% 5% 35%
DiffusionPolicy 22.5% 35% 50% 5% 0%
39FigureA.14: InitialsettingsforevaluatingthetaskStabilize.
FigureA.15: InitialsettingsforevaluatingthetaskReachandGrasp.
FigureA.16: InitialsettingsforevaluatingthetaskInsert.
FigureA.17: InitialsettingsforevaluatingthetaskScrew.
FigureA.18: InitialsettingsfortheexperimentObjectAssetMismatch.
40