Societal Adaptation to Advanced AI
JamieBernardi*a,GabrielMukobi*b,HilaryGreaves*c,LennartHeima,d,MarkusAnderljung*†a
aCentrefortheGovernanceofAI bStanfordUniversity cUniversityofOxford dRAND
Abstract ever-smaller actors’ development and deployment
Existing strategies for managing risks from advanced AI activities would bebothdifficultandundesirable(Sastryet
systems often focus on affecting what AI systems are al. 2024). Moreover, capability-modifying interventions
developed and how they diffuse. However, this approach already fail to comprehensively address risk, and in
becomes less feasible as the number of developers of additionrestrictbeneficialaswellasharmfulapplications.
advanced AI grows, and impedes beneficial use-cases as Capability-modifying interventions should therefore be
wellasharmfulones.Inresponse,weurgeacomplementary
complemented by adaptation to advanced AI: adjusting
approach: increasing societal adaptation to advanced AI,
other aspects of society to reduce the expected negative
that is,reducing theexpectednegativeimpactsfromagiven
impacts downstream of capability diffusion, holding fixed
level of diffusion of a givenAI capability. Weintroduce a
which AI capabilities are created and how they diffuse.1
conceptual framework which helps identify adaptive
interventions that avoid, defend against and remedy While a large portionoftheeffortsaimedataddressingthe
potentially harmful uses of AI systems, illustrated with risks of AI systems with relatively modest capabilities
examplesin electionmanipulation, cyberterrorism,andloss focus on adaptation measures, efforts to address the risks
of control to AI decision-makers. We discuss a three-step from more advanced AI systems tend to predominantly
cyclethat societycan implementto adapt toAI. Increasing focus on capability-modifying interventions. We urge an
society’s abilitytoimplement thiscyclebuildsitsresilience
increased focus on adaptation to advanced AI as a crucial
to advanced AI. We conclude with concrete
complementtocapability-modification.
recommendations for governments, industry, and
This paper motivates the need for societal adaptation to
third-parties.
advanced AI (Section 2) and introduces a framework for
conceptualising such adaptation (Section 3). Weapplythis
1. Introduction
framework to three examples of AI risk (Section 4). We
explore the structures society needs to successfully adapt,
The diffusion of advanced AI—AI systems that approach
introducing resilience as the capacity to adapt(Section5).
and exceed human capabilities—brings both benefits and
Section 6 offers recommendations for government,
risks, necessitating careful governance. Many existing
industry,academia,andnonprofits.
approaches to managing AI risks focus on identifying
potentially harmful capabilities of AIsystems(Shevlaneet
al. 2023) and modifying how those capabilities are
1 An analogous distinction between capability modification and
developed and made available. Examples include adaptationisalreadywell-recognisedineffortstoaddressclimatechange.
Climatemitigation,likecapabilitymodification,tacklesriskatitssource:
monitoring inputs and outputs to block harmful prompts
here,reducingnetCO emissionsinordertopreventconsequentclimate
2
and responses (OpenAI 2023), regulating deployment change.Climateadaptationisamatterofadjustingsocietytoreducethe
impact ofclimatechangethatnonethelessdoesoccur(IPCC2014).The
(Anderljung et al. 2023a), or employing training methods
OECD (2023) estimates that 27% of total climate finance is spent on
to generate safer outputs (Bai et al. 2022). We refer to adaptation.
Ingeneral,adaptation(toadvancedAI,andgenerally)includesseizing
interventions of these types as capability-modifying
opportunitiestoincreasebenefitsgained,aswellasavoidingdownsides.
interventions. However,inthispaperwewillfocusonadaptationtoavoiddownsides.
As the cost of developing advanced AI decreases,
*Equalcontribution.Orderoffirstthreeauthorsrandomised.Authors
however, it becomes less feasible for risk management to arefreetolistthemselvesfirstintheauthororderintheirCVs.
†Seniorauthor.
rely solely on capability-modifying interventions (Pilz,
Correspondenceto:contact@jamiebernardi.com,
Heim and Brown 2023). Government oversight of gmukobi@cs.stanford.edu,hilary.greaves@philosophy.ox.ac.uk,
lennart.heim@governance.ai,markus.anderljung@governance.ai2. The Need for Societal AI Adaptation But beyond a certain point, it underminesthefeasibilityof
AI governance approaches that solely rely on
New technologies often introduce novel risks. These risks capability-modification (Scharre 2024). If capability-
can arise from the intentional misuse of thetechnology,or focused interventions focused on an absolute level of
as an unintendedconsequence.Overtime,societytypically capability, they would affect a growing number of small
adapts to the risks. This trajectory can be observed in the actors, someday potentially including individual citizens.
historical rise and fall of pedestrian road collisions (U.K. This would be both impractical and undesirable.
DfT 2022) and numbers of smokers (Ritchie and Roser Capability-modifyingapproachesfocusedonrelativerather
2013)intheUnitedKingdom,forexample. than absolute performance may remain more feasible.
Though adaptation does to some extent arise Nonetheless, such approaches would have to be
spontaneously, it usually benefits from deliberate planning accompaniedbyadaptivemeasures.
and effort. Pedestrian fatalities have decreased in part due
2.1.2SafeguardsAreNotFailsafe
to speed limits (Jepson et al. 2022) and road safety
Models are often deployed with capability-modifying
campaigns2, not only increased pedestrian caution.
safeguards, such as fine-tuning (Bai et al. 2022) or input
Adaptation can be reactive, responding to harm as it
and output filtering (OpenAI 2023). But relying solely on
manifests, or proactive by anticipating potential risks
such safeguards is insufficient for managing risks, for the
(IPCC2001).
followingreasons.
In this section, we motivate the need for adaptation to
Some proportion of developers will deploy models
complement capability-modifyingapproachestorisksfrom
without safeguards, e.g. because such safeguards can
advanced AI. We suggest that capability-modifying
affect product quality. For instance, Mistral releases some
approaches will become less feasible and less effective
of itsmodelweightswithoutsafeguardsto“empowerusers
over time (Section 2.1), and we explain how adaptation
to test and refine moderation3.” As the number of actors
mayalsoaidbeneficialdiffusionofadvancedAI
developingmodelsincreases(Section2.1.1),sotoowillthe
(Section2.2).
diversity of decisions developers make regarding
safeguards. Even if some countries mandate safeguards,
2.1Capability-ModifyingApproachesWill
other,morepermissive,regimeswilllikelyremain.
BecomeLessEffectiveOverTime
Some safeguards can be cheaply removed by
2.1.1Increaseddiffusionmakescapability-modifying small-scale actors. Even if models are initially deployed
interventionslessfeasible with safeguards, it can be cheap for small teams with
Pilz, Heim and Brown (2024) observe that the cost of accesstomodelweightstointentionallyremovesafeguards
training an AI system to a given level of performance has (Yang et al. 2023; Gade et al. 2023). Additionally, even
been decreasing over the last decade, due to efficiency withoutaccesstoweights,techniqueslikejailbreaking(e.g.
improvements in training algorithms and in hardware Anil et al. 2024) can circumvent many existing
performance,andthatthesetrendsarelikelytocontinue.In safeguards.4
2020, training OpenAI’s GPT-3 was estimated to cost at Model leakage and theft. Even if model weights are
least $4.6 million in computing costs (Li 2020); twoyears secured to prevent safeguard tampering, models (and their
later, Mosaic claimed to offer the same performance for a dangerous capabilities) could still be leaked or stolen via
tenth of the cost (Venigalla and Li 2022). Rahman, Owen, informationsecurityfailures(Nevoetal.2023).5
and You (2024) estimate that 56 models have now been
trained using more compute than GPT-3, by 29
organisations. In sum, we should expect that over time,
more actors will have the resources required to train 3docs.mistral.ai/getting-started/open_weight_models.Accessed:
advancedandpotentiallyriskyAIsystems. 2024-05-13.
4 Thereare,however,othersafeguardsthataremoredifficulttoundo,
Increased access to developing advanced AI
such as unlearning (Li et al. 2024)ormodelfingerprintsthatcouldaid
technologies enables significant benefits (Section 2.2.2). traceability(Lukas,ZhangandKerschbaum2019).
5Whilstweightswerenotstoleninthiscase,Meta’sLlamawasleaked
online one week after it was made available to researchers on-request
2https://www.think.gov.uk/.Accessed2024-05-13. (Vincent2023).Figure1:AsimplifiedcausalpathwaytoanAIsystemcausingnegativeimpactsandhowvarioustypesofinterventioncan
reducethem.Thefocusofthispaperisonthelatterthreeinterventions:adaptationinterventions.
While AI safeguard failures appear to have relatively open-weight models limit safeguarding options (Section
limited impacts today, we should be prepared for greater 2.1.2) and have caused tangible harms already, suchasthe
potential impact in the case of future, more advanced production of DeepFakes depicting non-consensual
systems. Such preparations will require complementing intimate imagery (Lakatos 2023) and AI generated Child
capability-modifyinginterventionswithadaptiveones. Sexual Abuse Material (CSAM) (Internet Watch
Foundation2023).
2.2AdaptiveApproachesAidBeneficialDiffusion Without societal adaptation, the primary approaches for
Societal adaptation to advanced AI may be not only avoiding unacceptable levels of harm from open
necessary, but also beneficial in other ways, through deployment involve restricting openness. An adaptive
promoting the diffusion of AI capabilities and the open approach offers more promise of realising the benefits of
developmentofAIsystems. opennesswhilesimultaneouslyreducingitsharms.
2.2.1AdaptationCanEnableBeneficialUse
While capability-modifying interventions can reduce risk, 3. A Framework for AI Adaptation
they will often be blunt instruments, since they inhibit
beneficial use-cases as well as harmful ones, resultingina In the previous section,wearguedthataddressingtherisks
Use-Misuse Tradeoff (Anderljung and Hazell 2023; from advanced AIisnotonlyamatterofinterveningonAI
Weidinger et al. 2023). For example, restricting an AI capabilities, but also ensuring society’s adaptation:
system’s knowledge of virology through techniques like reducing the expected negative impactsfromadvancedAI,
unlearning (Lietal.2024)orfiltering-outAPIrequestsand holding fixed whichAIcapabilitiesaredevelopedandhow
model responses related to that capability (OpenAI 2023) theydiffuse.
could reduce thehypothesisedriskofenablingbioterrorists In this section, we offer a framework to guide thinking
(Nelson and Rose2023),butmayalsohinderstudents’and about such adaptation. The framework lays out the
scientists’ ability to learn and to combat diseases. To the structureofacausalchainleadingtonegativeimpactsfrom
extent that society is able to adapt, we would be better AI,6 and offers a categorisation of interventions that could
positioned to harness the benefits from such dual-use
reducesuchimpacts.7
capabilitieswithoutincurringunacceptablerisks.
2.2.2AdaptationCanEnableOpenDevelopment
Open development of AI systems, particularly the open 6 A threat model is a modelofaparticularpossiblecausalpathway.
Section 3 lays out the abstract structure; Section 4 discusses three
release of model weights, can be both beneficial and
examplethreatmodels.
harmful (Seger et al. 2023; Kapoor et al. 2024). Benefits 7The“use,”“initialharm,”“impact”distinctionweuseissimilar,but
not identical, to distinctions often used in legal scholarship between a
include stimulating innovation (Langenkamp and Yue
“wrong” (an inappropriate action taken by some party), “injury” (a
2022), distributing decision-making power, mitigating harmful event), and “damage” (the magnitude of impact of an injury)
(Nolan 2013), and in risk management between “cause”, “event” and
market concentration(VipraandKorinek2023;U.K.CMA
“consequence”(Waycott2018).
2023), and facilitating external scrutiny of models In reality, in any givencasethereareahugenumberofcausalsteps
leadingtoharm,whichcouldbemappedontothisframeworkinvarious
(Bucknall and Trager 2023). On the other hand,
equallyvalidways.3.1TheCausalChaintoNegativeImpactsofAI and a range of impacts such access might have (e.g.
stealing importantdataorharmingcitizensinacyberattack
Negative impacts from AI systems follow the causal
onphysicalinfrastructure).
pathwayillustratedinFigure1:
Development:AnAIcapabilityorsystemisdeveloped.
3.2InterventionstoReduceNegativeImpacts
Diffusion: The capability or system becomes available
tovarioususers. To reduce negative impacts, policymakers caninterveneat
Use:8 The AI system is used in a way that could cause differentpointsalongthiscausalchain.
harm. This harm could be actively intended (“misuse”), 3.2.1Capability-ModifyingInterventions
such as a cybercriminal using a new general-purpose Capability-modifying interventions intervene at points
model to automate the generation of spear phishing immediately preceding the “development” and “diffusion”
messages, aiming to access sensitive information on a steps:
company’s systems. It could also be that AI is used in a Development interventions. Society can affect which
way that has a concerning likelihood of causing AI capabilities are developed. For example, companies
unintentionalharm(“accident”). could refrain from developing systems that have certain
Initial harm: The use of the AI system results in some potentially harmful capabilities, or make systems that are
proximateharmfulevent.9Inthecaseofmisuse,thiscanbe more resistant to jailbreaking, have higher chances of
thought of as the initial success of the malign use of AI: refusing potentially harmful requests, or have outputs that
success in the first step of the actor’s plan.10 (In our canbemoreeasilyidentifiableasAI-generated.
example, the “initial harm” occurs if the cybercriminal Diffusion interventions. Society can affect which AI
succeeds in gaining access to the sensitive information in systems are made available, to whom, and with what
question.) degrees of access. For example, companies can employ
Impact: The initial harm results in further negative “stagedrelease”:graduallymakingthesystemmorewidely
impact. This impact could be measured in terms of e.g. available (Solaiman 2023). They could make potentially
lives lost, economic opportunities lost, or damage to risky models available only via an API, allowing them to
national security.11 (In our example, the cybercriminal implement secure safeguards, such as watermarking or
might sell sensitive information from an arms content provenance tags (Shevlane 2022). They could
manufacturer’s systems to a state actor that either enforce terms of service policies, removing access from
reproduces a weapon or learns how to exploit its customerswhousethesysteminprohibitedways.
weaknesses,therebyleadingtoadditionalliveslost.)
3.2.2AdaptationInterventions
To apply the frameworkinpractice,itisoftenbesttofix
Adaptation interventions, the primary focus of this paper,
aspecificuse,harm,orimpactofinterest,andthenidentify
intervene at later stages in the causal chain. Such
the other steps accordingly. For example, if focussing on
interventionsimmediatelyprecedethe“use”,“initialharm”
the harm of unauthorised access to sensitive computer
or “impact” stages of that chain. (Occasionally, a specific
systems, one might consider a range of uses thatmaylead
intervention can affect multiple points along the causal
to such breaches (e.g. spear phishing or insider threats),
chain.)
Avoidance interventions. Society can reduce the
8Amoregeneraltermwouldbe“operation”oftheAIsystem:insome
expected extent of the potentially harmful use of AI,
casesoflossofcontroloverAI,thereneednotbea“user.”
9Moreprecisely,wemightdefine“initialharm”asroughlyan“event making the problematic actions in question more difficult
thatwould,bydefault,leavesomepartyworseofforhavesomerightof
to engage in, or more costly compared to relevant
theirsviolated”.Theclause“bydefault”leavesopenthatremedialaction
might prevent thepartyinquestionfromactuallyexperiencingnegative alternatives.12 One can make it more difficult for a given
impacts at the end of the day. It is also consistent withourusagethat
instance of potentially harmful AI activity to occur by
“harm” occurs in cases in which thatharmcausallyleadstomorethan
adequatecompensation,sothattheneteventualeffectispositive. limiting the user’s or the AI system’s access to key
10Inanaccidentcase,whatcountsasthe“initialharm”inagivencase
resourcesthatarerequiredfortheactivityinquestion,orto
is(still)moreopentostipulation(cf.footnote7).
11 In the examples we’ll consider, the impact will most often be
negative, but it could be made zero or even positivegivensufficiently
12Thesetworoutestoavoidancecorrespondtothedistinctionbetween
effectiveadaptation.Forexample,peoplelosingtheirjobsduetoAIcould
“deterrencebydenial”and“deterrencebypunishment”thatiscommonly
receive financial compensationthatexceededtheiremploymentincome,
therebymakingthem(atleastfinancially)betteroff. drawnintheliteratureonmilitarystrategy(Mazarr2018).Risk ThreatModel ExampleAdaptations
Use:AImisusetocreatesyntheticelection Avoidance:Criminalisingelectioninterference,
manipulationmedia. requiringidentityverificationonsocialmedia.
Defence:Publicawarenesscampaigns,content
Harm:Votersmisled,holdingfalseelectionbeliefs.
provenancetechniques,AIcontentdetectiontools.
ElectionManipulation
Impact:Disenfranchisement,misrepresentation, Remedy:Transparentinvestigationsintoelectoral
withGenerativeAI(4.1)
lowerelectionintegrity,politicalinstability. integrity,rerunningelectionsinextremecases.
Use:AIaidsnon-stateactorsinlaunching Avoidance:Internationaljusticeagreements,
cyberattacksoncriticalinfrastructure. enhanceddetectioncapabilitiesforcyberintrusions.
Harm:Criticalinfrastructuretakenofflineor Defence:AI-enhancedcyberdefence,information-
damaged,datatheft. sharingnetworks.
CyberterrorismAttacks
onCritical Impact:Lossoflife,economicdamage,national Remedy:Compensationschemes,redundancyin
Infrastructure(4.2) securitythreats. criticalinfrastructure,rapidrepairplans.
Use:IncreasedrelianceonAIindecision-making,not Avoidance:Regulationrequiringtestingbefore
necessarilyinvolvingmaliciousintent. automationinhigh-stakesdecision-making.
Harm:High-stakesdecisionsmadewithouteffective Defence:Human-in-the-looprequirements,rigorous
humanoversight. auditing,whistleblowerprotections.
LossofControltoAI
Impact:Harmfuldecisions,potentiallycatastrophic Remedy:DisempoweringharmfulAI
Decision-Makers(4.3)
lossofcontroloversociety,existentialrisk. decision-makers,sharedincidentreporting.
Table1:ExamplesofadaptingtoAIrisks.EachisdescribedinmoredepthinSection4.Images:Flaticon.com
key actuators that are required for completion of the negative impact downstream of that. In our spearphishing
intended action. (In the spear phishing example we example, this might go via reducing the extent to which
introduced in Section 3.1: relevant companies could make national securityisunderminedasaresultofthesaleofthe
it harder for cybercriminals to access the names and proprietary informationtoaforeignactor.Forexample,the
contact details of their staff.) One can make potentially company could include some false and misleading
harmful uses of AI more ex ante costly by building documents on its servers. Governments could reduce
institutions that create credible threats of punishment for incentives for staff with relevant implicit knowledge to
harmfuluse.13 work for the foreign actor, on the grounds that implicit
Defence interventions. Holding fixed that the knowledge is often required to complement information
potentially harmful use of AI occurs, society can reduce containedindocuments.
the expected extent of the corresponding initial harm. In
our spear phishing example, “defence” is a matter of
4. Examples of Adapting to AI Risks
reducing the chance that thespearphishingemailssucceed
in giving the cybercriminal access to the sensitive To illustrate the practical application of the framework
information. For example, companies could provide described in Section 3, we discuss three examples of AI
anti-phishing training to their staff, andimplementtoolsto threats and corresponding adaptations shown in Table 1:
warn staff ofsuspectedphishingemails.Theycouldensure election manipulation, cybersecurity, and gradual loss of
that only a very small number of staff members have control to AI decision-makers. For each example, we
access to particularly sensitive information, and then only describe a concretethreatmodelforhowharmmightoccur
withapprovalfromotheremployees. and list some possible adaptationinterventionscategorised
Remedial interventions. Holding fixed that the initial by our frameworkofAvoidance,Defence,andRemedy.We
harm occurs, society can reduce or eliminate the expected do not make claims about the likelihood or importance of
these AI threats, or the merits of the particular adaptive
13 The main means of creating such acrediblethreatisofcourseto
interventionswesuggest;thegoalisrathertoillustratehow
actuallyissueafter-the-factpenalties.Superficially,thismakesitlookas
though systems ofpenaltyactlaterinthecausalchain;buttheircrucial
disincentiveeffectactsatthe“avoidance”point.the framework presented in the previous section mightaid 4.1.2AdaptationExamples
brainstormingofpossibleadaptiveinterventions. Avoidance: Governments can deter election interference
4.1ElectionManipulationwithGenerativeAI by criminalising it (Lerner 2023), subject to requirements
of free speech (Toney 2024). Social media platforms can
4.1.1ThreatModel
require some "proof of humanity" for creation of user
Generative AI systems can create high-quality text (Jones
accounts, making it more challenging for bot accounts to
and Bergen 2024), video (Brooks et al. 2024), and audio
spreaddisinformation(Shoemaker2024).
media.14 This synthetic media is often difficult to
Defence: Public awareness campaigns can empower
distinguish from authentic content(Cookeetal.2024),and
individuals to critically assess AI-generated content.
frontier languagemodelsarealreadyaboutaspersuasiveas
Contentprovenancetechniquesthroughoutthelifetimeofa
humans (Goldstein et al. 2024; Durmus et al. 2024).
piece of media, e.g. when a photo is captured and edited,
Further, the capabilities for generating high-quality
can help to verify genuine content (Srinivasan 2024;
synthetic media are already quite diffuse, including access
Earnshaw, Dupras and MacCormack 2023). AI content
to proprietary15 as well as open access LLMs16, andimage
detection tools can enable platforms to take appropriate
generators.17
actions such as removal, labelling, or adding scalable
Use: Generative AI systems might be used maliciously
counter-disinformation such as Community Notes (Wojcik
to manipulate democratic elections.Forexample,synthetic
etal.2022).
media could impersonate political figures for defamatory
Remedy: In extreme circumstances, given robust
political content (Meyer 2023). This disinformation could
evidence of election manipulation, governments could
be micro-targeted to individual voters for greater efficacy
rerun elections, as has been done in Germany (Martin,
(Salvietal.2024),especiallyifpeopleincreasinglyuseand
Hallam, and Hubenko 2024), India (Agarwala 2024),
trustpersonalisedAIcompanions(Roose2024).
Malawi (Kell 2020)andSerbia(Gec2024),thoughcaution
Initial Harm: We take the initial harm to be: voters
is required (Huefner 2007). Impartial and transparent
holding false election-relevant views that they otherwise
investigations into the integrityoftheelectoralprocesscan
would not hold. For example, they could believe that the
build public trust to avoid secondary harms from a
impersonations are genuine and that the fake news stories
disgruntledpublic.
are true (West 2023), believe incorrect information about
when and where they can vote (Swenson and Weissert
4.2AI-EnabledCyberterrorismAttackson
2024), or be manipulated into weighing the merits of
CriticalInfrastructure
candidates in a different way than their authentic selves
would. 4.2.1ThreatModel
Impact: A misledandmanipulatedelectoratenegatively Increasingly capable large language models could lower
impacts the validity and efficacy of democratic elections, the barriers to cyberattacks by rapidly finding and
diverginganelection’soutcomefromthevaluesandwillof exploiting vulnerabilities (Li et al. 2024; Fang et al.2024;
the voters. AI-enabled election manipulation could also thoughseealsoRohlf2024).
undermine public trust in elections, which can in turn can Use: Future advanced AI systems could aid small
lead to political instability. Furthermore, asocietywhereit non-state actors, such as terrorist groups, to carry out
is widely believed that AI-generated and authenticcontent cyberattacks oncriticalinfrastructurenecessaryforsocietal
are indistinguishable is vulnerable to the “liar’sdividend,” security, safety, and stability (Newman 2024). These
where public figures may dismiss real incriminating non-state actors may be more willing to carry out such
evidence as fake (Chesney andCitron2019;Schiff,Schiff, attacks,becauseofhavinglowaccountabilityand/orfearof
andBueno2023). retaliationcomparedtonation-states.
Initial Harm: Such attacks could take critical
infrastructure offline or cause lasting damage to it.
14https://elevenlabs.io.Accessed:2024-05-08.
State-level cyberattacks unaided by AI have already been
15https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free.
Accessed:2024-05-13. used to disable electrical grids in Ukrainian cities (Finkle
16https://llama.meta.com/llama3.Accessed:2024-05-13.
2016) and undermine nuclear infrastructure in Iran
17https://www.midjourney.com.Accessed:2024-05-13.
https://stability.ai/news/stable-diffusion-3.Accessed:2024-05-13.(Kushner 2013). Cyberattackers targeting digital Use: Unlike misuse cases, the widespread use of AI
infrastructure have been used to steallargesumsofmoney decision-makers may arise without any individual
and exfiltrate sensitive information from governments intending harm. If AI systems seem more efficient or
(CSIS2024). effective than human decision-makers, simple cost-benefit
Impact: Critical infrastructure, by definition, is vital to analyses may pressure institutions to rely more on AI
societal needs. Damage to systems such as healthcare, (Hendrycks 2023). For example,AIdecision-makerscould
energy, or communications could lead to enormous lossof at some point replace board members in corporations
life, economic damage, national security threats, or (Pugh 2019),policymakersingovernments(Samuel2019),
provocationstowardinternationalconflict. and commanders in militaries (Clarke and Whittlestone
4.2.2AdaptationExamples 2022). Furthermore, this reliance on AI decision-makers
Avoidance: Robust international agreements against could compound: increasingly capable AI systems may
cyberterrorism could facilitate global cooperation in produce work outputs and audit trails that areincreasingly
detecting, tracking, and prosecuting cyberterrorists (Peters difficult for humans alone to supervise, leadingtoreliance
and Jordan 2020). Enhancing state abilities todetectcyber onAIauditors(Christiano2021).
intrusions with access to critical infrastructure systems Initial Harm: High-stakes decisions come to be made
could preemptively identify and neutralise threats (CISA by AI alone on a large scale, without humans either inthe
2013), especially including advanced persistent threats decision loop or in a position to effectively oversee
(CISA2024). decisions.
Defence: Defensive AI capabilities can augment Impact: WhileAIdecision-makerscouldcertainlybring
traditional cyber defence, for example by detecting and many benefits (Koster et al. 2022), they could also cause
patching security vulnerabilities (Lohn and Jackson2022). harm by sometimes making much worse decisions than
Better information-sharing networks enhance the abilityto would be made by humans, even if they seem better on
detect diffuse or stealthy cyberterrorism and rapidly average. Simple machine learning predictors may already
mitigateitsimpacts(Johnsonetal.2016). exhibit algorithmic bias inhigh-stakesapplicationssuchas
Remedy: Appropriate compensation schemes can criminal justice (Angwin and Larson 2016). Military
reduce harm by spreading the costs associated with decisions made by AIcouldescalateinternationalconflicts
cyberattacks. Decoupled and redundant critical (Rivera et al. 2024) or could lead to high rates of civilian
infrastructure, such as backup power for hospitals casualties (as alleged by Abraham 2024), especially if
(Davoudi 2015), can ensure continuity of service. Cities “automation bias” causes humans to defer more to AI
can prepare to rapidly restore attacked infrastructure—for (Cummings 2012). Beyond bad decisions, overreliance on
example, via planning and drills for rebooting the power AI decision-makerscouldalsoleadtohumanenfeeblement
gridorrepairingcompromiseddigitalsystems. (Árvai 2024). Ultimately, if AI decision-makers are
misaligned to human-compatible goals, loss of control to
4.3LossofControltoAIDecision-Makers AI could constitute an existential catastrophe (Hendrycks,
Mazeika,andWoodside2023;Carlsmith2022).
4.3.1ThreatModel
AI developers are increasingly building highly capable 4.3.2AdaptationExamples
general-purpose AI systems that can carry out tasks Avoidance: Regulation could limit decision-making
without human supervision. OpenAI’s Charter explicitly automation in certain high-stakes industriesorgovernment
commitstoattemptingthedevelopmentofartificialgeneral roles until these systems have been proved trustworthy
intelligence (AGI),definedas“highlyautonomoussystems (Coy 2024), similar to the existing regime of requiring
that outperform humans at most economically valuable trialsfornewpharmaceuticals(U.S.FDA2017).
work” (OpenAI 2018). As these systems increase in Defence: Human-in-the-loop requirements can require
capability and seemorewidespreaduse,eventuallythereis human oversight for certain high-stakes decisions, such as
a riskof“valueerosion”andlosingcontrolofsocietytoAI was proposed in the U.S. Block Nuclear Launch by
decision-makers(Assadi2023;Dafoe2018). Autonomous Artificial Intelligence Act of 2023 (U.S.
Senate 2023), or ensure that AI decision-makers areaugmenting and not strictly replacing humans (Acemoglu technological change, the potential scale of impacts, and
and Restrepo 2019). Society could rigorously audit (in some cases) the indirectness of causal pathways from
high-stakesprovisionalAIdecisionsbeforeactingonthem, AIusetonegativeimpacts.
and red team these auditing mechanisms. Whistleblower We will saythatasocietywithastrongcapacitytoadapt
protections could encourage people to report issues in AI effectively is resilient to advanced AI.18 Inthissection,we
decision-making (Katyal 2018; Bloch-Wehba 2024). describe each component of the adaptive cycleandoutline
Lastly, society could invest considerable resources in possible initiatives for building society’s capacity to
ensuring that AI systems do in fact act in accordancewith executeit.
our wishes, even where humansareincapableofproviding
effectivesupervision(Bowmanetal.2022). 5.1Identify,Forecast,andAssessRisks
Remedy: Governmentagenciescould“bust”harmfulAI The planning of appropriate adaptations begins with a
decision-makers in critical roles, such as corporate threat model: a mapping of the particular causal pathway
executives, disempowering them similar to the way in by which a given AI system might lead to negative
which antitrust agencies bust corporate decisions that impacts. Such threat models should take into account the
undermine consumer welfare. Shared incident reporting interests and views of all relevantstakeholders(Watkinset
mechanisms could help institutions piece together diffuse al.2021;LazarandNelson2023).
patternsoffailure(McGregor2021). Early availability of information helps make threat
models more accurate, and provides relevant actors with
5. Resilience: The Capacity to Adapt more time to identify and implement adaptive responses.
For example, in anticipation ofthediffusionofAI-enabled
To ensure society adapts toadvancedAI,certainstructures vulnerability detection capabilities, DARPA and ARPA-H
and processes are required—specifically, continual investedinhardeninginfrastructureagainstcyberattacks19.
implementationofthethree-stepcycleshowninFigure2: Adaptation-relevant information can be gathered at
1.Identify, forecast, and assess risks introduced or various points along the causal pathway to negative
exacerbatedbyadvancedAIsystems. impactsfromanAIsystem:
2.Identify and assess possible adaptive responses to Pre-development information. Before an AI system is
addressthoserisks. developed, someinformationcanbegatheredtopredictthe
3.Implement appropriate adaptive responses and measure type and extent of likely capabilities (Kolt et al. 2024;
theireffectiveness. Toner et al. 2023). Reporting on compute usage (Sevilla,
Ho and Besiroglu 2023; Heim et al. 2024)andregistration
of large training runs (Hadfield, Cuéllar and O’Reilly
2023; White House 2023) can indicate in advance where
novel capabilities are most likely to arise. Documenting
datasets can help to predict unwanted model behaviours
suchasbias(Gebruetal.2021).
Pre-deployment information. Before an AI system is
deployed, labs and external parties can produce and share
relevant information (Anderljung et al. 2023b; Mitchell et
Figure2:Thethree-stepadaptationcyclethatmustbe
al. 2019), e.g. by evaluating models for dangerous
implementedtosuccessfullyadapttoadvancedAI.
capabilities (Shevlane et al. 2023) and examining human
Resilienceissociety’scapacitytoperformthisloop.
interactions with the system (Weidinger et al. 2023). AI
developers can publish safety cases that assess whether
Similar cycles describe the adaptation process in other
contexts, for example climate change (European
Environment Agency 2024). However, the challenge of
implementing this cycle effectively is especially acute in 18“Capacitytoadapt”isonestandardmeaningoftheterm“resilience,”
the case of advanced AI. This is due to the pace of thoughthereareothers.
19https://aicyberchallenge.com/.Accessed2024-05-08.deploying the system would impose unacceptable risks beneficial activity. As in the case of identifying possible
(Clymeretal.2024). pathways to harm,therearetheoreticalapproachestothose
Integration and usage information. Afterdeployment, calculations (e.g. modelling) and empirical approaches
information can be gatheredonwhereandhowAIsystems (e.g.controlledtrialsornaturalexperiments).
are being integrated, which may help society to predict
where and how harmful use is mostlikelytooccur(Javadi 5.3ImplementAdaptationsandMeasure
et al. 2021; Bonney et al.2024)andunderstanditssocietal Effectiveness
impact. Staged release protocols (Solaiman 2023) could
Evenwhensomegroupsinsocietyarewell-informedabout
provide opportunities for monitoring use under limited
risks and appropriate adaptive responses, adaptive
release. Companies deploying advanced AI could be
interventions may not be put into practice, for at least the
required toreporttheirownaggregateusagestatistics(Kolt
three reasons below. It took decades for society to
et al. 2024), and application providers could implement
implement measures to reduce smoking after its negative
identifiers, real-time monitoring, and activity logging for
consequenceswerewidelyunderstood.
AI agents (Chan et al. 2024). Experimentscanbesetupto
Shared awareness and understanding. Successful
collect information on usage in plausibly representative
implementation requires shared awareness and
samples(Zhaoetal.2024).
understanding of appropriate adaptive interventions across
Incident information. This helps us recognise initial
society, including at least government, the private sector,
harms and negative impacts as they occur (OECD AIM20;
academia and non-profit organisations, as well as (often)
McGregor 2021).OnechallengeindetectingharmfromAI
the general public.Effectivecommunicationbetweenthese
systems is that the causal roles of AI are often quite
sectors is therefore vital for ensuring identified adaptive
indirect, diffuse and unpredictable: far more so than for
responsesareintegratedintoplanning.
climate change or smoking, where the causal mechanisms
Institutional capacity. Successful implementation also
are simpler and better understood. Watermarking or AI
depends on the existence of appropriate institutions for
content identification tools (Fernandez et al. 2023; U.K.
resolving collective action problems,andonorganisations’
DSIT 2023) could help to track where advanced AI
technical, financial and institutional capacities for
systemshavebeenused.
monitoring and responding to AI risk. The rapid pace of
development makes adaptation particularly challenging in
5.2IdentifyandEvaluatePossibleAdaptive
the case of advanced AI, potentially raising challenges
Responses
fasterthansocietyisequippedtoimplementsolutions.This
Once a given threat model is sufficiently well-evidenced, could be especially problematic if some risks are
society must identify plausible adaptive responses, and path-dependent, threatening permanent and irreversible
evaluate these to make an informed choice of which to damage when a pathway to harm proceeds unchecked,
implement. even temporarily. For example, a major disruption to the
To identify plausible responses, society can invest in labour market could leadtomassdissatisfaction,economic
research to identify ways in which a given threat model difficulty, and resulting societal instability that would be
might effectively be blocked (via “avoidance”, “defence” harder to address than adapting to the initial stages of
or “remedy”, in terms of the framework we offered in labourautomation(KlinovaandKorinek2021).
Section 3).21 Sometimes, this might require identifying International coordination. Appropriate international
possible new technologies, not yet developed, whose institutions and coordination may be required foreffective
availability would enhance adaptation (consider the collective action. To illustrate, difficulty in coordinating
invention of airbags in response to theriskofcarcrashes). with AISafetyInstitutes(whichaimtoevaluatefrontierAI
To evaluate proposed adaptive interventions, researchers systems) in multiple jurisdictions has been cited as one
must take into account cost-effectiveness, and impact on underlying reason the U.K. AI Safety Institute has not
received frontier model access prior to deployment
20https://oecd.ai/en/incidents.Accessed:2024-05-08. (Manancourt, Volpicelli and Chatterjee 2024). This has
21OurdiscussioninSection4illustratesinoutlinewhatthismightlook
likeforthreeexamples,butforapropertreatment,vastlymoredetailand
carefulanalysisisrequired.resulted in frontier model release without any public body • Improve AI Literacy: Educators, governments and
conductingpre-deploymentevaluation. journalists should continually make the general public,
Once new adaptations have been implemented, their industry leaders, and key decision-makers aware of what
effectiveness should be monitored to assess whether the advanced AI systems are capable of and their
interventionshouldbescaled,changed,ordropped.22 correspondingimpacts(LongandMagerko2020).
• Sanction Known Harmful Uses: Governments may
need to criminalise certain harmful uses of advanced AI
6. Recommendations
systems.
To ensure society identifies, prioritises, and implements
6.3StrategicRecommendations
adaptations to AI, we highlight the following nine
recommendations for decision-makers across policy, • Use Defensive AI: Governments should incentivize AI
industry,academicsandnon-profits. companies to develop and provide access to AIsystemsto
defend against AI-caused threats (Buterin 2023). Such
6.1Understanding-BasedRecommendations efforts may be bolstered by the fact that widely available
• Measure and Predict AI Risks: Governments should AI systems may lag behind the capability of frontier
fund academics and auditors that measure and predict AI systems (Pilz, Heim and Brown 2023), which could
capabilities and corresponding risks,andbuildframeworks differentially be put to defensive uses. For example, in
to ensure robust oversight of frontier AI companies (Ee cybersecurity, frontier systems that identify and fix
2023). Frontier AI companies should carry out vulnerabilities faster than widely diffused systems can
pre-deployment evaluations in collaboration with exploit them could improve the offence-defence balance
governments and third parties, reportingbothdevelopment
(LohnandJackson2022;AICyberChallenge202423).
plans and deployment risks. They should make • Secure International Cooperation: Governments
considerable investments to improve best practice in risk should facilitate international cooperation to increase
identification and mitigation. Academics should work to adaptation (Ho et al. 2023). For example, the various AI
improvethescienceofriskandcapabilitiesassessment. Safety Institutes and potentially the EU AI Office could
• BuildanExternalScrutinyEcosystem:High-stakesAI coordinatetoconductpre-deploymenttestingoffrontierAI
developmentanddeploymentdecisionsshouldbeinformed systems to identify emerging risks and share information,
by third-party assessments. Policymakers have an thereby providing states with the time and knowledge to
important role in ensuring such access is granted and that betteradapt.
such external scrutiny is both informative and in fact • InvestinAdaptation:Governments,philanthropistsand
informs important decisions (Raji et al. 2022; Anderljung private entities should allocate sufficient funds for timely
etal.2023b). societal adaptation to advanced AI. This could take many
• Establish Incident Reporting Mechanisms: forms, such as funds for third-party organisations to build
Governments should establish incident reporting systems resilience(Microsoft2024),fundstoexistinginstitutionsto
and requirements (Walker, Schiff and Schiff 2024), along execute adaptation, or funds to establish new institutions
with whistleblower protections. Non-profit organisations focused on adaptation-specific needs such as red-teaming
canimplementpilotsofsuchprograms(McGregor2021). societyforAIvulnerabilitiesorapplyingdefensiveAI.
6.2ImplementationRecommendations
7. Conclusion
• Employ staged release: AI companies should employ
staged release protocols for their frontier systems, thereby As increasingly advanced AI systems are developed and
giving society more time to implement adaptations widely diffused, society will need not only
(Shevlane2022;Solaiman2023;Segeretal.2023). capability-modifying interventions, but also adaptation
interventions to manage the accompanying risks. This is
because capability-modifying interventions (i)becomeless
22 Conceptually, this is a return to the first step in the adaptation
process: identify, forecast,andassess(remaining)societalrisksfromAI
systems. 23https://aicyberchallenge.com.Accessed:2024-05-11.feasible over time as it becomes possible for smaller and harms via systemic effects and from loss of control.
smaller actors to train advanced AI systems, (ii) are not However, the underlying concept of “defence” appears to
failsafe,and(iii)inhibitbeneficialaswellasharmfuluses. besimilaroridenticaltoourconceptof“adaptation”.
This paperpresentsaframeworkforconceptualisingand Similarly, Krier (2024) discusses using frontier models
identifying the range of possible adaptive interventions in to “improvesocietaldefenses”againstattacksthatcouldbe
response to a given threat. Avoidance interventions allow facilitated by advanced AI in the hands of bad actors; in
that the AI capability in question has diffused, but inhibit this connection, he mentions the possibility of adaptive
dangerous uses of that capability. Defence interventions initiatives, including enhanced cybersecurity and closing
block or reduce the severity of initial harms along the legal loopholes. Krier’s focus is still narrower than that of
pathway to negative impact, after dangerous use takes Kapoor etal.sincehefocusesspecificallyonusingfrontier
place. Remedial interventions intervene causally models to improve defences, but again the underlying
downstream of the initial harm, to diminish total negative concept of improving defences seems similar to our
impacts. We illustrated how this framework might aid concept of adaptation (or perhaps, in Krier’s case,
brainstorming by applying it to three examples: election specificallytothe“defence”componentthereof).
manipulation,cyberterrorism,andlossofcontrol. In addition, informally we are aware of several groups
While some adaptation will happen by default, we considering various nearby concerns under the heading of
expect sufficient adaptation will require deliberate action, “AI resilience”, though we have not yet seen any
foresight, and considerable investment. To adapt correspondingsustaineddiscussioninprint.
effectively, society will need tocontinually(i)identifyand
assess risks, (ii) identify andevaluatepossibleadaptations,
Appendix B: Adverse Impacts
and (iii) implement and measure the effectiveness of
selected adaptations. We should increase society’s Statement
resilience to advanced AI, by increasing its capacity to
This paper aims to positively affect how society responds
executethiscycle.
to the opportunities and risks presented by advanced AI,
via an increased and more well-targeted focus on
Appendix A: Related Work
adaptation.
Our primary concern is that this work might be
Inthispaperwehaveurgedtheimportanceof:
misconstrued as a call to de-emphasise
• Implementing adaptation to advanced AI, defined as
capability-modifying interventions. While we have argued
reducing the expected negative impactsfromadvancedAI,
that capability-modifying approaches have limitations in
holding fixed which AI capabilities exist and the extentto
the long run, we believe they continue to be a crucially
whichtheyhaveproliferated;togetherwith
important component of risk management as frontier AI
• Building resilience to advanced AI, defined as the
capabilities continue to be developed and deployed. Our
capacitytoadapt.
argument is that we should also invest seriously in
Some authors have flagged theimportanceofsomething
adaptationmeasures.
very similar to what we call “adaptation” using adifferent
In particular, like capability-modifying interventions,
term, viz. “defense”. For example, Kapoor et al. (2024)
adaptation interventions are not failsafe guarantees ofzero
suggest that “assuming that risks existforthemisuse…in
harm. Successful implementation of adaptation measures
question, misuse analyses should clarify how society (or
should not give free reign to AI developers to make risky
specific entities or jurisdictions) defends against these
deploymentdecisions.
risks. … [N]ew defenses can be implemented or existing
defenses can be modified to addresstheincreaseinoverall
risk.” In onerespect,Kapooretal.’sscopeisnarrowerthan Acknowledgements
ours: they focus on defence against risks arising
We would like to thank the following for productive
specifically from misuse of an advanced AIsystembybad
conversation and comments on previous drafts of the
actors, whereas we urge consideration also of unintended
paper: Shahar Avin, Mauricio Baker, Matthew Bradbury,Ben Garfinkel, Josh Goldstein, Lujain Ibrahim, Nitarshan Assadi, G. 2023. Will HumanityChoose ItsFuture?Philarchive
Rajkumar, Ben Robinson, Girish Sastry, Toby Shevlane, preprint.https://philpapers.org/rec/ASSWHC.
Merlin Stein and Jess Whittlestone, as well as participants Bai, Y.; Jones,A.;Ndousse,K.;Askell,A.;Chen,A.;DasSarma,
N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; Joseph, N.;
in the Centre for the Governance of AI 2024 Winter
Kadavath, S.;Kernion, J.;Conerly, T.; El-Showk,S.;Elhage,N.;
Fellowship.
Hatfield-Dodds, Z.; Hernandez, D.; Hume, T.; Johnston, S.;
Kravec, S.; Lovitt, L.; Nanda, N.; Olsson, C.; Amodei, D.;
Brown, T.; Clark, J.; McCandlish, S.; Olah, C.; Mann, B.; and
References
Kaplan, J. 2022.Traininga Helpful andHarmlessAssistantwith
Reinforcement Learning from Human Feedback.
Abraham, Y. 2024. ‘Lavender’: The AI Machine Directing
arXiv:2204.05862.
Israel’sBombingSpreeinGaza.+972Magazine.https://www.97
Bloch-Wehba, H. 2024. The Promise and Perils of Tech
2mag.com/lavender-ai-israeli-army-gaza/.Accessed:2024-05-08.
Whistleblowing.NorthwesternUniversityLaw Review,Vol.118,
Acemoglu, D., Restrepo, P. 2019. Automation and New Tasks:
No. 6, pp. 1503-1562. Texas A&M University School of Law
How Technology Displaces and Reinstates Labor. InJournalof
LegalStudiesResearchPaperNo.23-13.
EconomicPerspectives,33(2):3-30.
Bonney,K.;Breaux, C.; Buffington,C.;Dinlersoz,E.;Foster,L.;
Agarwala, T. 2024.Re-run Vote Concludes Peacefully inIndia's
Goldschlag,N.;Haltiwanger,J.;Kroff,Z.;and Savage,K.2024.
RestiveManipurState.Reuters.https://www.reuters.com/world/
Tracking Firm Use of AI in Real Time: A Snapshot from the
india/re-run-vote-concludes-peacefully-indias-restive-manipur-sta
Business Trends and Outlook Survey. CES Working Paper No.
te-2024-04-22/.Accessed:2024-05-08.
24-16.U.S.CensusBureau,March2024.https://www.census.gov
Anderljung,M., and Hazell,J. 2023. ProtectingSociety fromAI /library/working-papers/2024/adrm/CES-WP-24-16.html.
Misuse: When are Restrictions on Capabilities Warranted? Accessed:2024-05-08.
arXiv:2303.09377.
Bowman,S.; Hyun,J.; Perez,E.; Chen,E.;Pettit,C.;Heiner,S.;
Anderljung,M.;Barnhart,J.;Korinek,A.;Leung,J.;O'Keefe,C.; Lukošiūtė, K.; Askell, A.; Jones, A.; Chen, A.; Goldie, A.;
Whittlestone, J.; Avin, S.; Brundage, M.; Bullock, J.B.; Mirhoseini, A.; McKinnon, C.; Olah, C.; Amodei, D.;Amodei,
Cass-Beggs, D.; Chang,B.;Collins, T.;Fist, T.; Hadfield,G.K.; D.; Drain, D.; Li, D.; Tran-Johnson, E.; Kernion, J.; Kerr, J.;
Hayes,A.;Ho, L.;Hooker, S.;Horvitz,E.; Kolt,N.;Schuett,J.; Mueller, J.; Ladish, J.; Landau, J.D.; Ndousse, K.; Lovitt, L.;
Shavit, Y.; Siddarth, D.; Trager, R.F.; and Wolf, K.J. 2023a. Elhage,N.;Schiefer,N.;Joseph, N.;Mercado,N.;Dassarma,N.;
Frontier AI Regulation: Managing Emerging Risks to Public Larson, R.;McCandlish, S.;Kundu, S.;Johnston, S.;Kravec,S.;
Safety.arXiv:2307.03718. Showk, S.E.; Fort, S.; Telleen-Lawton, T.; Brown, T.B.;
Anderljung,M.;Smith,E.T.;O'Brien,J.;Soder,L.;Bucknall,B.; Henighan, T.; Hume, T.; Bai,Y.; Hatfield-Dodds, Z.;Mann,B.;
Bluemke, E.;Schuett, J.;Trager,R.;Strahm,L.;andChowdhury, and Kaplan, J. 2022.MeasuringProgressonScalable Oversight
R. 2023b. Towards Publicly Accountable Frontier LLMs: forLargeLanguageModels.arXiv:2211.03540.
Building an External Scrutiny Ecosystem under the ASPIRE Bucknall, B. S., and Trager, R. F. 2023. Structured access for
Framework.arXiv:2311.14711. third-party research on frontier AI models: Investigating
Angwin, J., andLarson, J. 2016.Biasin CriminalRiskScoresIs researchers’ model access requirements. University of Oxford,
Mathematically Inevitable, Researchers Say. ProPublica. OxfordMartinSchool.
https://propublica.org/article/bias-in-criminal-risk-scores-is-math Brooks,T.;Peebles,B.;Holmes,C.;DePue,W.;Guo,Y.;Jing,L.;
ematically-inevitable-researchers-say.Accessed:2024-05-08. Schnurr, D.; Taylor, J.;Luhman,T.; Luhman,E.;Ng, C.;Wang,
Anil,C.; Durmus, E.;Sharma,M.;Benton,J.;Kundu,S.;Batson, R.; and Ramesh, A. 2024. Video generation models as world
J.; Rimsky,N.;Tong,M.;Mu,J.;Ford,D.;Mosconi,F.;Agrawal, simulators.OpenAI.
R.; Schaeffer,R.; Bashkansky,N.;Svenningsen,S.;Lambert,M.; Buterin,V.2023.MyTechno-Optimism.https://vitalik.eth.limo/
Radhakrishnan,A.;Denison,C.;Hubinger,E.J.;Bai,Y.;Bricken, general/2023/11/27/techno_optimism.html.Accessed:
T.; Maxwell,T.;Schiefer,N.;Sully,J.; Tamkin, A.; Lanham,T.; 2024-05-08.
Nguyen, K.; Korbak, T.; Perez, E.; Grosse, R.; Duvenaud, D.;
Carlsmith, J. 2022. Is Power-Seeking AI an Existential Risk?
Kaplan, J.; Ganguli, D.; and Bowman, S. R. 2024. Many Shot
arXiv:2206.13353.
Jail-Breaking.https://cdn.sanity.io/files/4zrzovbb/website/
Chan, A.; Ezell, C.; Kaufmann, M.; Wei, K.; Hammond, L.;
af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf. Accessed:
Bradley, H.; Bluemke, E.; Rajkumar, N.;Krueger,D.;Kolt, N.;
2024-05-08.
Heim, L.; and Anderljung, M. 2024. Visibility into AI Agents.
Árvai, J. 2024.TheHiddenRisk of LettingAI Decide–Losing
arXiv:2401.13138.
theSkillstoChooseforOurselves.https://theconversation.com/
Chesney, R., and Citron, D. K. 2019. Deep Fakes: A Looming
the-hidden-risk-of-letting-ai-decide-losing-the-skills-to-choose-fo
Challenge forPrivacy,Democracy, andNationalSecurity.In107
r-ourselves-227311.Accessed:2024-05-08.
CaliforniaLawReview1753.doi.org/10.15779/Z38RV0D15J.Christiano, P.2021.Another (Outer)AlignmentFailureStory.AI Earnshaw, N.; Dupras, J.; and MacCormack, B. 2023. Fighting
AlignmentForum.https://www.alignmentforum.org/posts/AyNH Misinformation with Authenticated C2PA Provenance Metadata.
oTWWAJ5eb99ji/another-outer-alignment-failure-story. In Proceedings of the 2023 NAB Broadcast Engineering and
Accessed:2024-05-08. Information Technology (BEIT) Conference. National
CISA (Cybersecurity and Infrastructure Security Agency)2013. AssociationofBroadcasters,Washington,DC,USA.
Targeted Cyber Intrusion Detection and Mitigation Strategies European Environment Agency2024..AdaptationSupport Tool.
UpdateB.https://www.cisa.gov/news-events/news/targeted- https://climate-adapt.eea.europa.eu/en/knowledge/tools/adaptatio
cyber-intrusion-detection-and-mitigation-strategies-update-b. n-support-tool.Accessed:2024-05-08.
Accessed:2024-05-08. Fang,R.; Bindu,R.; Gupta, A.;andKang,D.2024.LLMAgents
CISA (Cybersecurity and Infrastructure Security Agency)2024. can Autonomously Exploit One-day Vulnerabilities.
Nation-StateCyberActors.https://www.cisa.gov/topics/cyber- arXiv:2404.08144.
threats-and-advisories/nation-state-cyber-actors. Accessed: Fernandez,P.; Couairon,G.;J'egou,H.;Douze,M.;andFuron,T.
2024-05-08. 2023. The Stable Signature: Rooting Watermarks in Latent
Clarke, S., and Whittlestone, J.2022. A Surveyofthe Potential Diffusion Models. 2023 Institute of Electrical and Electronics
Long-term Impacts of AI: How AI Could Lead to Long-term Engineers / Computer Vision Foundation International
ChangesinScience,Cooperation, Power,EpistemicsandValues. ConferenceonComputerVision(ICCV),22409-22420.
In Proceedings ofthe 2022Associationfor the Advancementof Finkle, J. 2016.UkraineCybersecurity:SandwormTeamandthe
Artificial Intelligence / Association for Computing Machinery PowerGridAttack.Reuters.https://www.reuters.com/article/us-
Conference on AI, Ethics, and Society (AIES '22). Association ukraine-cybersecurity-sandworm-idUSKBN0UM00N20160108.
for Computing Machinery, New York, NY, USA, 192–202. Accessed:2024-05-08.
https://doi.org/10.1145/3514094.3534131.
Gade, P.; Lermen, S.; Rogers-Smith, C.; and Ladish, J. 2023.
Clymer,J.;Gabrieli,N.;Krueger,D.;andLarsen,T.2024.Safety BadLlama: Cheaply Removing Safety Fine-Tuning from Llama
Cases: How to Justify the Safety of Advanced AI Systems. 2-Chat13B.arXiv:2311.00117
arXiv:2403.10462.
Gec, J. 2024. Serbia Election: Belgrade Opposition. AP News.
Cooke, D.; Edwards, A.; Barkoff, S.; and Kelly, K. 2024. As https://apnews.com/article/serbia-election-belgrade-opposition-ad
Good AsA CoinToss:HumandetectionofAI-generatedimages, f6f5b21e020d27bda986a366c64a8c.Accessed:2024-05-08.
videos,audio,andaudiovisualstimuli.arXiv:2403.16760.
Gebru, T.; Morgenstern, J.; Vecchione, B.; Vaughan, J. W.;
Coy, P. 2024. Will A.I. Take All Our Jobs? This Economist Wallach, H.; Daumé III,H.;and Crawford,K. 2018.Datasheets
SuggestsMaybeNot.TheNewYorkTimes.https://www.nytimes. for Datasets. Communications of the ACM 64, 12 (December
com/2024/03/22/opinion/ai-jobs-comparative-advantage.html. 2021),86–92.doi.org/10.1145/3458723.
Accessed:2024-05-08.
Goldstein, J.A.;Chao, J.;Grossman, S.;Stamos,A.;andTomz,
CSIS (Center for Strategic and International Studies) 2023. M. 2024. How persuasive is AI-generated propaganda?
SignificantCyberIncidents.https://www.csis.org/programs/ ProceedingsoftheNationalAcademyofSciencesNexus,Volume
strategic-technologies-program/significant-cyber-incidents. 3,Issue2.doi.org/10.1093/pnasnexus/pgae034.
Accessed:2024-05-08.
Hadfield, G.; Cuéllar, M.-F.; and O'Reilly, T.2023. It’s Timeto
Cummings, M. L. 2012. Automation Bias in Intelligent Time Create a National Registry for Large AI Models. Carnegie
Critical Decision Support Systems. In Proceedings of the 1st EndowmentforInternationalPeace.https://carnegieendowment.
American Institute of Aeronautics and Astronautics (AIAA) org/posts/2023/07/its-time-to-create-a-national-registry-for-large-
IntelligentSystems Technical Conference, Chicago,USA.AIAA ai-models.Accessed:2024-05-08.
Paper2004-6313.doi.org/10.2514/6.2004-6313.
Heim, L.; Fist, T.; Egan, J.; Huang, S.; Zekany, S.; Trager,R.;
Dafoe,A. 2018.AIGovernance:AResearchAgenda.University Osborne,M. A.;andZilberman,N.2024.GoverningThroughthe
ofOxford,FutureofHumanityInstitute. Cloud: The Intermediary Role of Compute Providers in AI
Davoudi, V. 2015. Emergency and StandbyPower in Hospitals. Regulation.UniversityofOxford,OxfordMartinSchool.
Consulting-SpecifyingEngineer,October15,2015.https://www. Hendrycks,D. 2023.Natural Selection FavorsAIsoverHumans.
csemag.com/articles/emergency-and-standby-power-in-hospitals/. arXiv:2303.16200.
Accessed:2024-05-08.
Hendrycks, D.; Mazeika, M.; and Woodside, T. 2023. An
Durmus, E.; Lovitt, L.; Tamkin, A.; Ritchie, S.; Clark, J.; and OverviewofCatastrophicAIRisks.arXiv:2306.12001.
Ganguli, D. 2024. Measuring the Persuasiveness of Language
Ho, L.; Barnhart, J.; Trager, R.; Bengio, Y.; Brundage, M.;
Models.Anthropic.
Carnegie,A.;Chowdhury,R.;Dafoe,A.;Hadfield,G.;Levi,M.;
Ee, S. 2023. Adapting Cybersecurity Frameworks to Manage and Snidal, D.2023.International Institutions forAdvanced AI.
FrontierAI Risks:a Defense-in-Depth Approach.InstituteforAI arXiv:2307.04699v2.
PolicyandStrategy(IAPS).Huefner, S.2007.Remedying ElectionWrongs. Harvard Journal Association for Computing Machinery, New York, NY, USA,
on Legislation, Vol. 44, 2007, Ohio State Public Law Working 645–651.doi.org/10.1145/3461702.3462619.
PaperNo.82. Koster, R.; Balaguer, J.; Tacchetti, A.; Weinstein, A.; Zhu, T.;
Internet Watch Foundation (IWF). 2023. How AI is Being Hauser, O.;Williams, D.;Campbell-Gillingham,L.;Thacker, P.;
AbusedtoCreateChildSexualAbuseImagery.https://www.iwf. Botvinick, M.; and Summerfield, C. 2022. Human-Centred
org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abuse Mechanism Design with Democratic AI. Nature Human
d-to-create-child-sexual-abuse-imagery/.Accessed:2024-05-08. Behaviour,6:1398-1407.doi.org/10.1038/s41562-022-01383-x.
IPCC 2001. Annex B: Glossary of Terms. In: Climate Change Kolt, N.; Mazeika, M.; Barnhart, J.; Brass, A.; Esvelt, K.;
2001: Impacts, Adaptation, and Vulnerability. Contribution of Hadfield, G. K.;Heim,L.;Rodriguez,M.; Sandbrink, J.B.;and
Working Group II to the Third Assessment Report of the Woodside, T. 2024. Responsible Reporting for Frontier AI
IntergovernmentalPanelonClimateChange. Development.arXiv:2404.02675.
IPCC 2014: AnnexII: Glossary,editedbyMach, K.J.; Planton, Krier, S. 2024. Models on the Frontline: AI's Defensive Role.
S.; and von Stechow, C. In: Climate Change 2014: Synthesis https://www.aipolicyperspectives.com/p/models-on-the-frontline-
Report.Contribution of WorkingGroupsI, IIandIII totheFifth ais-defensive.Accessed:2024-05-08.
Assessment Report of the Intergovernmental Panel on Climate Kushner, D. 2013. The Making of Arduino. IEEE Spectrum,
Change, edited by Pachauri, R. K.; and Meyer, L. A. IPCC, 50(3),pp.48-53.doi.org/10.1109/MSPEC.2013.6471059.
Geneva,Switzerland,pp.117-130.
Lakatos,S.2023.ARevealingPicture.Graphika.
Javadi, S. A.; Norval, C.; Cloete, R.; and Singh, J. 2021. https://graphika.com/reports/a-revealing-picture.Accessed:
Monitoring AI Services for Misuse. In Proceedingsofthe 2021 2024-05-08.
Association for the Advancement of Artificial Intelligence /
Langenkamp, M., and Yue, D. N. 2022. How Open Source
Associationfor ComputingMachinery ConferenceonAI,Ethics,
Machine Learning Software Shapes AI. In Proceedings of the
andSociety,597-607.doi.org/10.1145/3461702.3462566.
2022Associationfor theAdvancementofArtificialIntelligence/
Jepson, R.; Baker, G.; Cleland, C.; Cope, A.;Craig, N.;Foster, Associationfor ComputingMachinery ConferenceonAI,Ethics,
C.; Hunter, R.; Kee F.; Kelly, M. P.; Kelly, P.; Milton, K.; and Society. Association for Computing Machinery, NewYork,
Nightingale, G.; Turner, K.; Williams, A. J.;andWoodcock, K. NY,USA,385–395.doi.org/10.1145/3514094.3534167.
2022. Developing and implementing 20-mph speed limits in
Lazar S.andNelsonA.2023.AIsafetyonwhoseterms?Science.
Edinburgh and Belfast: mixed-methods study. SouthamptonUK:
2023Jul14;381(6654):138.doi.org/10.1126/science.adi8982.
National Institute for Health and Care Research; Public Health
Research,No.10.9.doi.org/10.3310/XAZI9445. Lerner,K.2023.Top StateOfficials PushtoMakeSpreadofUS
ElectionMisinformationIllegal.TheGuardian.https://www.
Johnson, C.; Badger, M.; Waltermire, D.; Snyder, J.; and
theguardian.com/us-news/2023/feb/23/us-spreading-election-misi
Skorupka, C. 2016.Guide toCyber ThreatInformationSharing.
nformation-illegal.Accessed:2024-05-08.
NIST Special Publication 800-150. National Institute of
StandardsandTechnology.https://csrc.nist.gov/publications/ Li, C. 2020. Demystifying GPT-3. Lambda Labs Blog.
detail/sp/800-150/final.Accessed:2024-05-08. https://lambdalabs.com/blog/demystifying-gpt-3. Accessed:
2024-05-08.
Jones, C.R.,and Bergen, B.K. 2024. People cannotdistinguish
GPT-4fromahumaninaTuringtest.arXiv:2405.08007. Li, N.; Pan,A.;Gopal,A.;Yue, S.; Berrios,D.;Gatti,A.;Li,J.
D.; Dombrowski, A.-K.; Goel, S.; Phan, L.; Mukobi, G.;
Kapoor, S.; Bommasani, R.; Klyman, K.; Longpre, S.;
Helm-Burger, N.;Lababidi, R.;Justen,L.;Liu, A.B.;Chen,M.;
Ramaswami,A.;Cihon,P.;Hopkins,A.;Bankston,K.;Biderman,
Barrass,I.;Zhang,O.;Zhu,X.;Tamirisa,R.;Bharathi,B.;Khoja,
S.; Bogen,M.;Chowdhury,R.;Engler,A.;Henderson,P.;Jernite,
A.;Zhao,Z.; Herbert-Voss,A.;Breuer,C. B.;Zou,A.;Mazeika,
Y.; Lazar, S.; Maffulli, S.; Pineau, J.; Skowron, A.; Song, D.;
M.;Oswal, P.;Liu, W.;Hunt,A.A.;Tienken-Harder,J.;Shih,K.
Storchan, V.; Ho, D.E.; Liang,P.; andNarayanan, A.2024.On
Y.; Talley, K.; Guan, J.; Kaplan, R.;Steneker,I.; Campbell,D.;
the Societal Impact of Open Foundation Models.
Jokubaitis, B.; Levinson, A.; Wang, J.; Qian,W.;Karmakar, K.
arXiv:2403.07918.
K.;Basart,S.;Fitz,S.;Levine,M.;Kumaraguru,P.;Tupakula,U.;
Katyal,S.K.2018.PrivateAccountabilityintheAgeofArtificial Varadharajan, V.; Shoshitaishvili,Y.;Ba,J.;Esvelt,K.M.;Wang,
Intelligence.UCLALawReview,66:54. A.;and Hendrycks,D.2024.TheWMDPBenchmark:Measuring
Kell, F. 2020. Malawi's Re-Run Election is Lesson for African and Reducing Malicious Use With Unlearning.
Opposition.ChathamHouse.https://www.chathamhouse.org/ arXiv:2403.03218.
2020/07/malawis-re-run-election-lesson-african-opposition. Lohn, A. J., and Jackson, K. A. 2022. Will AI Make Cyber
Accessed:2024-05-08. SwordsorShields?CenterforSecurityandEmergingTechnology
Klinova,K., and Korinek,A. 2021. AIand SharedProsperity.In (CSET).doi.org/10.51593/2022CA002.
Proceedings of the 2021 Association for the Advancement of Long, D., and Magerko, B. 2020. What is AI Literacy?
Artificial Intelligence / Association for Computing Machinery Competencies and DesignConsiderations. InProceedingsof the
Conference Conference on AI, Ethics, and Society (AIES '21). 2020CHI ConferenceonHuman Factorsin ComputingSystems(CHI'20).AssociationforComputingMachinery,NewYork,NY, OpenAI 2023. GPT-4 Technical Report: System Card.
USA,1–16.doi.org/10.1145/3313831.3376727. arXiv:2303.08774.
Lukas, N.; Zhang, Y.; and Kerschbaum, F. 2019. Deep Neural Peters, A., and Jordan, A. 2020. Countering the Cyber
Network Fingerprinting by Conferrable Adversarial Examples. EnforcementGap:StrengtheningGlobalCapacityonCybercrime.
arXiv:1912.00888. InJournalofNationalSecurity,Law&Policy.Volume10No.3.
Manancourt, V.; Volpicelli, G.; and Chatterjee, M. 2024. Rishi Pilz, K.; Heim, L.; and Brown, N. 2023. Increased Compute
Sunak promised to make AI safe. Big Tech's not playing ball. Efficiency and the Diffusion of AI Capabilities.
POLITICO.https://www.politico.eu/article/rishi-sunak-ai-testing arXiv:2311.15377v2.
-tech-ai-safety-institute/.Accessed:2024-05-08. Pugh, W. 2019. Artificial Intelligence on the Corporate Board?
Martin, N.; Hallam, M.; and Hubenko, D. 2024. Berlin Stages Slate.https://slate.com/technology/2019/03/artificial-intelligence-
Partial Rerun of2021German FederalElection. Deutsche Welle corporate-board-algorithm.html.Accessed:2024-05-08.
(DW).https://www.dw.com/en/berlin-stages-partial-rerun-of- Ritchie, H., and Roser, M. 2013. Smoking. Our World inData.
2021-german-federal-election/a-68225111Accessed:2024-05-08. https://ourworldindata.org/smoking.Accessed:2024-05-13.
Mazarr, M. J. 2018. Understanding Deterrence. RAND Rahman, R.; Owen, D.; and You, J. 2024. Tracking
Corporation.doi.org/10.7249/PE295. Compute-IntensiveAIModels.https://epochai.org/blog/tracking-
McGregor,S.2021.Preventing RepeatedRealWorldAIFailures compute-intensive-ai-models.Accessed:2024-05-13.
by Cataloging Incidents: The AI Incident Database. In Raji, I. D.; Xu, P.; Honigsberg, C.; and Ho, D. 2022. Outsider
Proceedings of the Innovative Applications of Artificial Oversight: Designing a Third Party Audit Ecosystem for AI
Intelligence Technical Track on AI Best Practices, Challenge Governance. In Proceedings of the 2022 Association for the
Problems,TrainingAIUsers,Vol.35No.17.doi.org/10.1609/ Advancement of Artificial Intelligence / Association for
aaai.v35i17.17817. Computing Machinery Conference on AI, Ethics, and Society
Meyer, D. 2023. Turkey'sDeep Fake-Influenced ElectionSpells (AIES '22). Association for Computing Machinery, New York,
Trouble.Fortune.https://fortune.com/europe/2023/05/15/turkeys NY,USA,557–571.doi.org/10.1145/3514094.3534181.
-deepfake-influenced-election-spells-trouble/. Accessed: Rivera, J. P.; Mukobi, G.; Reuel, A.; Lamparth, M.; Smith,C.;
2024-05-08. and Schneider,J. 2024.Escalation RisksfromLanguage Models
Microsoft 2024. Microsoft and OpenAI launch Societal inMilitaryandDiplomaticDecision-Making.arXiv:2401.03408.
ResilienceFund.https://blogs.microsoft.com/on-the-issues/2024/ Rohlf,C. 2024.No,LLMAgents cannotAutonomouslyExploit
05/07/societal-resilience-fund-open-ai/.Accessed:2024-05-14. One-day Vulnerabilities. secure.dev/auto_agents_1_day.html.
Mitchell, M.; Wu, S.; Zaldivar, A.; Barnes, P.; Vasserman, L.; Accessed:2024-05-13.
Hutchinson, B.; Spitzer, E.; Raji, I. D.; and Gebru, T. 2019. Roose, K. 2024. Meet My A.I. Friends. The New York Times.
Model Cards for Model Reporting. In Proceedings of the https://www.nytimes.com/2024/05/09/technology/meet-my-ai-frie
ConferenceonFairness, Accountability,andTransparency(FAT* nds.html.Accessed:2024-05-13.
'19). Association for Computing Machinery, New York, NY,
Samuel, S. 2019. A Quarter of Europeans Want AI to Replace
USA,220–229.doi.org/10.1145/3287560.3287596.
Politicians. That’s a Terrible Idea. Vox.
Nelson, C.,and Rose,S.2023.Report Launch:ExaminingRisks https://www.vox.com/future-perfect/2019/3/27/18283992/ai-repla
at the Intersection of AI and Bio. Centre for Long-Term ce-politicians-europe-survey.Accessed:2024-05-08.
Resilience.
Salvi, F.; HortaRibeiro, M.;Gallotti, R.;andWest,R. 2024.On
Nevo,S.;Lahav,D.;Karpur,A.;Alstott,J.;andMatheny,J.2023. theConversational Persuasiveness ofLargeLanguageModels:A
Securing Artificial Intelligence Model Weights. RAND RandomizedControlledTrial.arXiv:2403.14380.
Corporation.doi.org/10.7249/WRA2849-1.
Sastry,G.;Heim,L.;Belfield,H.;Anderljung,M.;Brundage,M.;
Newman,S. 2024.Cybersecurityand AI:TheEvolvingSecurity Hazell, J.; O'Keefe, C.; Hadfield, G.K.;Ngo, R.;Pilz, K.;Gor,
Landscape.CenterforAISafety.https://www.safe.ai/blog/ G.; Bluemke, E.; Shoker, S.; Egan, J.; Trager, R. F.; Avin, S.;
cybersecurity-and-ai-the-evolving-security-landscape. Accessed: Weller, A.; Bengio, Y.; and Coyle, D. 2024. Computing Power
2024-05-08. andtheGovernanceofArtificialIntelligence.arXiv:2402.08797
Nolan, D. 2013. Damage in the English Law of Negligence. 4 Scharre, P. 2024. Future-Proofing Frontier AI Regulation:
JournalofEuropeanTortLaw259-281. Projecting FutureCompute forFrontier AI Models.Center fora
OECD (Organisation for Economic Co-operation and NewAmericanSecurity.
Development) 2023. Climate Finance and the USD 100Billion Schiff, K. J.; Schiff, D. S.; and Bueno, N. 2023. The Liar’s
Goal. Dividend: Can Politicians Claim Misinformation to Evade
OpenAI 2018. OpenAI Charter. https://openai.com/charter/. Accountability?SocArXivpreprint.doi.org/10.31235/osf.io/
Accessed:2024-05-08. x43ph.Seger,E.;Dreksler, N.;Moulange,R.;Dardaman,E.;Schuett,J.; U.S. FDA (Food and Drug Administration) 2017. FDA's Drug
Wei, K.;Winter,C.;Arnold,M.;ÓhÉigeartaigh,S.;Korinek,A.; ReviewProcess:EnsuringDrugsAreSafeandEffective.https://
Anderljung, M.; Bucknall, B.; Chan, A.; Stafford, E.; Koessler, www.fda.gov/drugs/information-consumers-and-patients-drugs/fd
L.; Ovadya,A.;Garfinkel,B.;Bluemke,E.;Aird,M.;Levermore, as-drug-review-process-ensuring-drugs-are-safe-and-effective.
P.; Hazell,J.; andGuptaA.2023.OpenSourcingHighlyCapable Accessed:2024-05-08.
FoundationModels.arXiv:2311.09227. U.S. Senate. 2023. Block Nuclear Launch by Autonomous
Sevilla, J.; Ho, A.; and Besiroglu, T. 2023.PleaseReport Your Artificial Intelligence Act of 2023. S.1394, 118th Congress.
Compute. Communications oftheACM,Volume 66,Issue5,pp. https://www.congress.gov/bill/118th-congress/senate-bill/1394.
30-32.doi.org/10.1145/3563035. Accessed:2024-05-08.
Shevlane, T. 2022.Structuredaccess: anemerging paradigmfor Venigalla, A., and Li, L. 2022. GPT-3 Quality for 500k.
safeAIdeployment.arXiv:2201.05159. https://www.databricks.com/blog/gpt-3-quality-for-500k.
Shevlane, T.; Farquhar, S.; Garfinkel, B.; Phuong, M.; Accessed:2024-05-08.
Whittlestone, J.; Leung, J.; Kokotajlo, D.; Marchal, N.; Vincent,J. 2023.Meta’s powerfulAI languagemodelhasleaked
Anderljung, M.; Kolt, N.; Ho, L.; Siddarth, D.; Avin, S.; online—whathappensnow?TheVerge.https://www.theverge.
Hawkins, W.; Kim, B.;Gabriel, I.; Bolina, V.;Clark, J.;Bengio, com/2023/3/8/23629362/meta-ai-language-model-llama-leak-onli
Y.; Christiano, P.; and Dafoe, A. 2023. Model Evaluation for ne-misuse.Accessed2024-05-13.
ExtremeRisks.arXiv:2305.15324. Vipra, J., and Korinek, A. 2023. Market concentration
Shoemaker,P.2024.Why Proof ofHumanity IsMore Important implications of foundation models: The Invisible Hand of
ThanEver.Identity.com.https://www.identity.com/why-proof-of- ChatGPT.Brookings.doi.org/10.48550/arXiv.2311.01550.
humanity-is-more-important-than-ever/.Accessed:2024-05-08. Walker, C. P.; Schiff, D. S.;and Schiff, K.J. 2024.MergingAI
Solaiman, I. 2023. The Gradient of Generative AI Release: Incidents Research with Political Misinformation Research:
MethodsandConsiderations.arXiv:2302.04844. Introducing the Political Deepfakes Incidents Database.
Srinivasan, S. 2024. Detecting AI fingerprints: A guide to Proceedings of the AAAI Conference onArtificialIntelligence,
watermarkingandbeyond.Brookings. 38(21),23053-23058.doi.org/10.1609/aaai.v38i21.30349.
Swenson, A., and Weissert, W. 2024. New Hampshire Watkins, E.A.;Moss, E.;Metcalf,J.; Singh,R.andElish,M.C.
investigating fake Biden robocall meant to discourage voters 2021.Governing AlgorithmicSystemswithImpactAssessments:
aheadofprimary.APNews.https://apnews.com/article/new- Six Observations.In Proceedings of the 2021Associationforthe
hampshire-primary-biden-ai-deepfake-robocall-f3469ceb6dd6130 Advancement of Artificial Intelligence / Association for
79092287994663db5.Accessed:2024-05-13. Computing Machinery Conference on AI, Ethics, and Society
(AIES '21). Association for Computing Machinery, New York,
Toner,H.;Ji, J.;Bansemer,J.;Lim,L.;Painter,C.;Corley,C.D.;
NY,USA,1010–1022.doi.org/10.1145/3461702.3462580.
Whittlestone,J.; Botvinick,M.;Rodriguez,M.;andKumar,R.S.
S. 2023. Skating to Where thePuck IsGoing: Anticipatingand Waycott, A. 2018. Managing Risks - Cause, Event and
Managing Risks from Frontier AI Systems. Center for Security Consequence.https://www.fenwick.com.au/blog/2018/08/29/man
andEmergingTechnology.doi.org/10.51593/2023CA004. aging-risks-cause-event-and-consequence/.Accessed2024-05-13.
Toney, D.2024.PressStatement:ACLUofGeorgiaOpposesBill Weidinger,L.;Rauh,M.;Marchal,N.;Manzini,A.;Hendricks,L.
Criminalizing'Deep Fakes'AboutElectionCandidates.American A.;Mateos-Garcia,J.;Bergman, S.; Kay,J.;Griffin,C.;Bariach,
CivilLibertiesUnionofGeorgia.https://www.acluga. B.; Gabriel, I.; Rieser, V.; and Isaac, W. 2023. Sociotechnical
org/en/press-releases/press-statement-aclu-georgia-opposes-bill-c SafetyEvaluationofGenerativeAISystems.arXiv:2310.11986.
riminalizing-deep-fakes-about-election.Accessed:2024-05-08. West, D. M. 2023. How AI will transform the 2024 elections.
U.K. CMA (Competition and Markets Authority) 2023. AI Brookings.https://www.brookings.edu/articles/how-ai-will-
FoundationModelsInitialReport.https://assets.publishing. transform-the-2024-elections/.Accessed:2024-05-13.
service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_. White House 2023. Executive Order on the Safe, Secure, and
pdfAccessed:2024-05-08. Trustworthy Development and Use of Artificial Intelligence.
U.K. DSIT (Department forScience, Innovation& Technology) https://www.whitehouse.gov/briefing-room/presidential-actions/2
2023.EmergingProcessesforFrontierAISafety.https://www. 023/10/30/executive-order-on-the-safe-secure-and-trustworthy-de
gov.uk/government/publications/emerging-processes-for-frontier- velopment-and-use-of-artificial-intelligence/. Accessed:
ai-safety/emerging-processes-for-frontier-ai-safety. Accessed: 2024-05-08.
2024-05-08. Wojcik, S.; Hilgard, S.; Judd, N.; Mocanu, D.; Ragain, S.;
U.K. DfT (Department for Transport). 2022. Reported Road Hunzaker, M. B.; Coleman, K.;and Baxter, J.2022. Birdwatch:
CasualtiesGreatBritain:PedestrianFactsheet2021.https://www. Crowd Wisdom and Bridging Algorithms can Inform
gov.uk/government/statistics/reported-road-casualties-great-britai Understanding and Reduce the Spread of Misinformation.
n-pedestrian-factsheet-2021/reported-road-casualties-great-britain arXiv:2210.15723.
-pedestrian-factsheet-2021.Accessed:2024-05-13.Yang, X.;Wang,X.;Zhang,Q.;Petzold,L.; Wang,W. Y.;Zhao,
X.;andLin,D.2023.ShadowAlignment:TheEaseofSubverting
Safely-AlignedLanguageModels.arXiv:2310.02949.
Zhao,W.; Ren,X.;Hessel,J.; Cardie,C.;Choi,Y.;andDeng,Y.
2024.WILDCHAT:1MChatGPTInteractionLogsintheWild.In
Proceedings of the International Conference on Learning
Representations.doi.org/10.48550/arXiv.2405.01470.