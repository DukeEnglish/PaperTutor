Fine-Tuning Large Vision-Language Models as
Decision-Making Agents via Reinforcement Learning
YuexiangZhai1∗ HaoBai2† ZipengLin1† JiayiPan1† ShengbangTong3† YifeiZhou1†
AlaneSuhr1 SainingXie3 YannLeCun3 YiMa1 SergeyLevine1
1UCBerkeley 2UIUC 3NYU
Batch Observations & Rewards
Environments
+
You are a blackjack player. You are observing the current game
state, you can choose between ['stand', 'hit'].
concat Please first describe the current state, then output the action.
Pretrained
 RL
Vision Language Model Fine-Tuning
"thoughts": "I have 20 points in total. The dealer has at least 10
points. My total points are large, I should stand."

"action": "stand"
Actions
Figure1: Methodoverview.WeproposeaframeworkfortraininglargeVision-LanguageModels(VLM)with
ReinforcementLearning(RL).Ateachtimestep,theVLMtakesthecurrentobservationandapredesigned
promptasinputandoutputsanutterancecontainingachainofthoughtreasoningandatextaction. Thetext
actionisparsedintotheenvironmentforgeneratingtaskrewards.Finally,weapplyRLwiththetaskrewardto
fine-tunetheentireVLM.
Abstract
Largevision-languagemodels(VLMs)fine-tunedonspecializedvisualinstruction-
followingdatahaveexhibitedimpressivelanguagereasoningcapabilitiesacross
various scenarios. However, this fine-tuning paradigm may not be able to effi-
cientlylearnoptimaldecision-makingagentsinmulti-stepgoal-directedtasksfrom
interactiveenvironments. Toaddressthischallenge,weproposeanalgorithmic
frameworkthatfine-tunesVLMswithreinforcementlearning(RL).Specifically,
ourframeworkprovidesataskdescriptionandthenpromptstheVLMtogener-
ate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore
intermediate reasoning steps that lead to the final text-based action. Next, the
open-endedtextoutputisparsedintoanexecutableactiontointeractwiththeenvi-
ronmenttoobtaingoal-directedtaskrewards. Finally,ourframeworkusesthese
taskrewardstofine-tunetheentireVLMwithRL.Empirically,wedemonstrate
thatourproposedframeworkenhancesthedecision-makingcapabilitiesofVLM
∗ProjectLead,email:simonzhai@berkeley.edu.Projectpage:https://rl4vlm.github.io/
†Equalcontribution,listedinalphabeticalorder,seeAppendixAforlistofcontributions.
Preprint.
4202
yaM
61
]IA.sc[
1v29201.5042:viXraagentsacrossvarioustasks,enabling7bmodelstooutperformcommercialmodels
suchasGPT4-VorGemini. Furthermore,wefindthatCoTreasoningisacrucial
componentforperformanceimprovement,asremovingtheCoTreasoningresults
inasignificantdecreaseintheoverallperformanceofourmethod.
1 Introduction
Largevision-languagemodels(VLMs)[7,43,17]demonstrateremarkablecapabilitiesasgeneral-
purposeagentsinsolvingvarioustasksthroughlanguagereasoning. Inparticular,fine-tuningVLMs
withspecializedvisualinstructionfollowingdataappearstobeakeytechniqueforimprovingthe
capabilitiesofVLMs[33,83,32,29]. However,visualinstructiontuningmaynotbeoptimalfor
trainingdecision-makingagentsinmulti-stepinteractiveenvironmentsrequiringvisualrecognition
andlanguageunderstanding,asvisualinstructiontuningmainlyperformssupervisedlearningonpre-
collecteddatasetswithoutinteractingwiththeenvironments[21]. Consequently,ifthepre-collected
datasetslacksufficientdiversitytocoverawiderangeofdecision-makingscenarios,visualinstruction
tuningmayfailtoimprovetheVLMagent’sdecision-makingcapabilities.
To unleash the learning capabilities of VLM agents in multi-step goal-directed decision-making
environments,reinforcementlearning(RL),amethodthathasproveneffectiveintrainingmulti-step
interactiveagents[40,58,6,68],naturallyoffersaparadigmthatsupportsthispurpose. However,
whileRLhasbeenwidelyadoptedfortrainingpurelytext-basedtasksforlargelanguagemodels
(LLMs)[59,49,1,82],end-to-endVLMfine-tuningwithRLforgoal-directedmulti-steptaskshas
notyetbeenstudied,tothebestofourknowledge.
Our main contribution in this paper is an algorithmic framework that directly fine-tunes VLMs
withRLformulti-stepgoal-directeddecision-makingtasksrequiringvision-languageunderstanding.
In our framework, the VLM first receives a task description prompt, which guides it to generate
task-specific chain-of-thought (CoT) reasoning [74, 72] (blue parts in Figure 1), followed by a
text-basedaction(redpartsinFigure1). TheCoTreasoningisdesignedforefficientexplorations
bypromptingtheVLMstogenerateintermediatereasoningthatleadstothefinaltext-basedaction.
Ourframeworkthenparsesthetext-basedactionsintoexecutableactionsfortheenvironment,which
generatespotentiallygoal-directedrewardsandthenextstateforRLtraining.
ToevaluatetheeffectivenessofourmethodinenhancingaVLM’sdecision-makingcapabilities,we
adopta7bmodel[34]asthebackboneVLMandapplyourmethodtofivedecision-makingtasks.
Thesetaskscomefromtwodomains:anoriginaldomain,whichevaluatestheVLM’sdecision-making
capabilitiesrequiringfine-grainedvisualrecognitionandlanguagereasoning,andanembodiedAI
domain[57]focusingontestingtasksdemandingvisualsemanticreasoningcapabilities. Empirical
resultsshowthatourmethodenhancesthedecision-makingcapabilitiesofVLMsinbothdomains,
enabling7bmodelstosurpasstheperformanceofcommercialmodelssuchasGPT4-V[43]and
Gemini [17]. Moreover, our experiments reveal that CoT reasoning is crucial for performance
improvementinourRLtraining. Specifically,wetestourmethodonthesametaskswithouttheCoT
reasoningandobserveasignificantdropinoverallperformanceinbothdomains.
2 RelatedWork
TrainingLLMsorVLMswithRL. RLhasbeenwidelyadoptedfortrainingLLMsandVLMs
[84,60,69,44,9,49,42,17,61,59,1,19,82]. Somestudies[84,60,44,9,42,17,61]focuson
applyingRLfromhumanfeedback(RLHF),whichinvolveslearningrewardmodelsfromhuman
feedback before deploying RL. Other research [49, 59, 1, 19, 82] focuses on deploying RL with
task-specific reward functions without using human preference data. Our paper is similar to the
latter [49, 59, 1, 19, 82] which applies RL to train LLMs on customized reward functions from
differentenvironments.Therearetwomajordifferencesbetweenourpaperandpriorworks[49,59,1,
19,82]. Firstly,ourmethodincorporatesvisualinputs,broadeningitsapplicabilitytoawiderrangeof
tasksthatrequirevision-languageunderstandingormultimodalreasoning[28,37]. Secondly,while
previousworksdonotexplorehowCoTreasoningaffectsRLtrainingonlargemodelsingeneral,we
identifyCoTreasoningasacrucialcomponentforenhancingRLtraining. Weempiricallyobserve
thatincorporatingCoTreasoningsignificantlyimprovestheoverallperformanceofRLtrainingon
alltesteddomains.
2AdoptingLLMsandVLMsasdecision-makingagents. Manypriorworkshavestudiedvarious
methods of using frozen LLMs and VLMs for decision-making. One line of work studies the
promptingtechniques[74,13,78,77,73,30,75,46,70,47,23]forenhancingthedecision-making
capabilitiesoflargefoundationmodels,seeDongetal.[13],Yangetal.[76]foradetailedsurveyfor
otherpromptingbasedmethods.Ourworkdiffersfromallprompting-basedmethodssincewedirectly
useRLtofine-tunetheentireVLMasdecision-makingagents. Otherstudies[41,63,4,51,10]
integrate frozen VLMs ot LLMs into their training pipeline for processing task descriptions or
featureextraction,withoutusingtext-basedactions. focusesonintegratingdifferentcomponents
fromVLMsfordownstreamRLtraining. Forexample,somestudiesusetheVLMsorCLIPvision
encoder[45,41,63]asrewardmodelsfortraining,whichdiffersfromourmethodsinceweadopt
rewardsfromtheenvironments. Otherstudies[41,63,10]integratefrozenVLMs/LLMsintotheir
trainingpipelineforprocessingtaskdescriptions[41,63,45]orfeatureextraction[10],withoutusing
text-basedactions. Ourpaperdiffersfromtheseworks[41,63,10]intwomajoraspects. Froma
technicalperspective,wefocusonamorechallengingparadigmbydirectlyfine-tuningtheentire
VLMwith RL,whereas previous methods[41, 63, 10] onlytrainadditional MLPor transformer
layerstoconnectthefrozenLLM/VLMwiththeactionspace. Moreimportantly,ourmethoddirectly
interacts with the environments using open-ended text, enabling it to utilize the CoT reasoning
capabilityofVLMsformoreefficientexplorationsfordecision-making.
Evaluating VLMs as decision-making agents. For the evaluation, previous studieshave thor-
oughly examined the fundamental evaluations of VLMs in non-interactive tasks [3, 36, 79, 31,
64,80,15]. Ourfocus, however, isonevaluatingaVLM’sdecision-makingcapabilitiesininter-
activeenvironmentsthatrequirebothvisualrecognitionandlanguagereasoning. Representative
interactiveenvironmentsincludepurelytext-basedenvironments[12,27,71]orembodiedAIen-
vironments[39,57,55,14]. WeadopttheALFWorld[57]embodiedenvironmentforevaluating
ourmethod’sabilitytoimproveVLM’svisualsemanticreasoningcapabilities. Inadditiontothe
ALFWorldembodiedAIenvironment, wealsodesignanoriginal“gym-like”[8]environmentto
testVLM’sdecision-makingcapabilitiesintasksthatrequirefine-grainedvisualrecognitionand
languagereasoning.
CoT prompting. Recent developments in prompting for LLMs have demonstrated the crucial
roleofCoTinenhancingcomplexreasoningcapabilities[74,25,16,72,81,78]. Weietal.[74]
show that CoT reasoning can significantly boost LLMs’ performance across different reasoning
tasks by showing that adding simple exemplar-based prompts, leading to better performance on
benchmarkssuchastheGSM8K[11]. Afollow-upstudy[72]proposesanovelself-consistency
decodingstrategythatexploresmultiplereasoningpaths,demonstratingsubstantialgainsinarithmetic
andcommonsensereasoningtasks.Otherworks[25,81,16]haveshownthataddingpromptstobreak
complextasksintosubtasksandsolvethemstep-by-stepsignificantlyimprovesLLM’sreasoning
capability. OurworkdiffersfromtheseCoTpromptingstudiesasweaimtoprovideanalgorithmic
frameworkthatcantrainVLMswithRL,wheretheCoTpromptingappearsasakeycomponentof
theframework. Incontrast,priorworksfocusonimprovingthereasoningcapabilitiesofLLMswith
increasinglysophisticatedpromptingoffrozenmodels.
3 Preliminaries
StandardRLterminologies. WefollowthestandardnotationsfromclassicRLliterature[62,2].
Specifically, weuseM={S,A,P,r,γ}todenoteanMDP,whereS denotesthestatespace, A
denotestheactionspace,P denotesthetransitiondynamics,r : S ×A → Rdenotesthereward
functionandγ ∈ [0,1]denotesthediscountfactor. Ourgoalistolearnapolicyπ :S →Athat
(cid:104) (cid:105)
maximizestheoveralldiscountedreturnmax E (cid:80)T γtr(s ,a ) ,whereT isthemaximum
π∈Π π t=0 t t
numberofstepsperepisode. Withoutlossofgenerality,weuseπ(a|s)∈[0,1]todenoteprobability
ofπchoosingaats.
AdaptingtheRLformalismtoVLMs. Weuse V todenotethediscreteandfinitevocabulary
(token) space, and we use Vm,Vn to represent the input and output text space, where m and n
representthemaximumtokenlengthoftheinputandoutputsequence. WeadapttheRLformalism
toVLMsbytreatingthecombinationofthevisionandlanguageinputstoVLMsasthestatespace,
3S = O×Vm, where O is the space of all RGB images. We view each utterance [1, 82] of the
languageoutputsfromVLMsastheactionspaceVn. Therefore,theinputandoutputofaVLM
policywithparameterθcanbewrittenasπ :O×Vm →Vn. Forexample,intheBlackjacktask
θ
showninFigure1,eachstatesconsistsofanRGBimageowiththecardsofthedealerandtheplayer,
aswellasaninputpromptvinwithmaximumtokenlengthm,andthetextoutputvout =π (o,vin)
θ
(withamaximumtokenn)willlaterbeparsedasanactiontointeractwiththeenvironment. Similar
tothestandardRLsetting,weuseπ (vout|o,vin)∈[0,1]todenotetheprobabilityofaVLMpolicy
θ
π outputtingvoutwithinputimageoandpromptvin.
θ
4 TrainingVLMswithRL
ComparedtoclassicMLP-basedpolicynetworks[52–54,18],anaturaladvantageofVLMpoliciesis
thattheycanleverageCoTreasoningforefficientexplorationbyperformingintermediatereasoning
stepsthatleadtothefinaldecision. However,trainingaVLMpolicyπ withRLpresentsadditional
θ
challenges. First,theVLMpolicyπ (o,vin)directlygeneratesopen-endedtextratherthanvectorized
θ
actionsinclassicpolicygradient-basedRLmethods[52–54,18],complicatingdirectinteractionwith
theenvironment. Evenwithaparsingmechanismf : Vn → Athatmapsopen-endedtextvout to
alegalactionaforinteractionwiththeenvironment,itremainsunclearhowtoestimatetheaction
probabilityπ (a|o,vin)fromthetextgenerationprocess.
θ
Figure2presentsanoverviewofourframework,leveragingtheCoTreasoningandaddressingthetwo
aforementionedchallenges. Wedesignatask-specificpromptvinthatrequirestheVLMtogeneratea
formattedoutputvout,includingtheCoTreasoning. Next,weadoptapost-processingfunctionf
toparseopen-endedtextintoalegalactiona thatcandirectlyinteractwiththeenvironment. To
t
computeπ (a|o,vin),wedevelopamethodtoestimateitsvaluebasedontheprobabilityofeach
θ
outputtokeninvout.
PPO Replay buffer
Task descriptio  concat
Legal action spac  compute
Desired output  VLM log-likelihood of
CoT reasonin 
env.step
Text action
“thought”: “...”,
legal action environment
“action”: “look”
Figure 2: AdiagramoftheproposedRLfine-tuningframeworkforVLM.Attimestept,thestates
t
containsaninputpromptvinandavisualobservationo . TheVLMtakess =[o ,vin]asinputandoutputs
t t t t t
open-endedtextvoutcontainingtheCoTreasoning,keywords"action":"a ",andthelog-likelihoodofvout.
t t t
Wefirstapplyapost-processingfunctionf onvout, toobtainalegalactiona whichcaninteractwiththe
t t
environment.Then,weinputa totheenvironmentforobtainingrewardr(s ,a )andthenextobservationo .
t t t t+1
Afterward,wedeviseamethodtocomputeanumericalvalueofπ (a |o ,vin).Finally,weuser(s ,a )and
θ t t t t t
π (a |o ,vin)fortheRLtraining.
θ t t t
TheremainingSectionisstructuredasfollows. First,wedescribetheformatofourinputpromptvin
t
andthedesiredoutputvout(Section4.1).Next,wepresentthepost-processingfunctionf(Section4.2).
t
Then,weintroduceamethodtocomputeanumericalvalueofπ (a |o ,vin)(Section4.3). Finally,
θ t t t
weconcludeourframeworkinAlgorithm1(Section4.4).
4.1 PromptDesignforDomain-SpecificOutputs
ForeachtaskM,ourinputpromptvincontainsadescriptionofthetask,thelegalactionspaceofthe
t
currentobservation,andthedesiredoutputformat(includingtheCoTreasoning). Ourdesiredoutput
vout, contains a CoT reasoning followed by the keywords "action" : "a " for post-processing.
t t
Figure 3 provides an example of our input prompt vin and the desired formatted output vout. In
t t
particular,wedefineafunctionhwhichconstructsvinfromthecurrentobservationo :vin =h(o ),to
t t t t
4accommodatefortasksthatmaycontainobservation-dependentinformation.3 Weprovideadditional
examplesofvinandvoutinAppendixB.
CoTpromptvinfortaskM
t
YouaretryingtosolveataskM.{Descriptionofthetask}.Youareobservingthecurrentstatusof
thetask.TheactionspaceofMis{textversionofalllegalactionsa∈A}.Yourresponseshouldbe
avalidjsonfileinthefollowingformat:
{
"thoughts":"{firstdescribethecurrentstatusofthetask,thenthinkcarefullyaboutwhichactionto
choose}",
"action":{Chooseanaction"a∈A"}
}
Formattedtextoutputvout
t
{
"thoughts":"IamsolvingtaskT,giventhecurrentstatusofthetask,Ishouldchoosea ",
t
"action":"a "
t
}
Figure3: Atemplateofourinputpromptandoutputtext.ThebluepartrepresentstheCoTreasoningand
theredpartisthetext-basedaction.
4.2 Post-ProcessingOpen-EndedTextforLegalActions
Ourpost-processingmechanisminvolvesbothvinandf. Intheinputpromptvin,wedirectlyaskthe
t t
VLMtooutputatext-basedactionintheformatof"action":"a "(seeFigure1andFigure2for
t
examples). Afterobtainingvout,ourpost-processingfunctionf directlysearchesforthetext-based
t
keywords"action":"a "fromvout,andmapsittoalegalactiona ,eitherinsymbolicorintext
t t t
dependingonthetaskofinterest. ForthecaseshowninFigure1,f willmapvout tothesymbolic
t
operatorthatrepresentstheaction"stand"intheBlackjacktask(tobeintroducedinSection5.1),
sincetheBlackjacktasktakessymbolicactionsasinput. Asforthealfworld[57]environment
showninFigure2,f willmapvout tothetext"look",becausethealfworldenvironmenttakes
t
text-basedactionsasinputs.
However,VLMsarenotalwaysguaranteedtogenerateavoutthatcontainsthekeywords"action":
t
"a ",evenwhenweexplicitlyrequestaformattedoutputfromvin. TocontinuetheRLtrainingwhen
t t
vout doesnotcontainanylegalaction,weperformrandomexplorationbyselectingalegalaction
t
a ∈Auniformlyatrandom. Mathematically,f isdefinedasfollows:
t
(cid:26) a, if"action":"a"∈vout,
f(vout)= (4.1)
Unif(A), otherwise.
4.3 EstimatingActionProbabilitiesofVLMPolicies
To estimate the action probability logπ (a |o ,vin) (or equivalently logπ (a |o ,vin)) for pol-
θ t t t θ t t t
icy gradient-based methods [54], a naïve calculation is directly using logπ (vout|o ,vin) as
θ t t t
logπ (a |o ,vin),bysummingthelog-likelihoodofalltokensinvout. Thisisbecause
θ t t t t
P(o ,vin,vout)
logπ (vout|o ,vin)=log t t t
θ t t t P(o ,vin)
t t
=log(cid:34) P(o t,vi tn,v [:n]) ...P(o t,vi tn,v [:2])P(o t,vi tn,v [:1])(cid:35) =(cid:88)n log(cid:34) P(o t,vi tn,v [:i]) (cid:35)
.
(4.2)
P(o ,vin,v ) P(o ,vin,v ) P(o ,vin) P(o ,vin,v )
t t [:n−1] t t [:1] t t i=1 t t [:i−1]
In the equation above, we use v to denote the output token vout for simplicity, and we use v
t [:i]
to denote the first i tokens in vout, and we slightly abuse our notion by using P(o ,vin,v ) to
t t t [:0]
denote P(o ,vin) in the log summation. Hence, a natural way to compute a numerical value for
t t
logπ (a |o
,vin)is(cid:80)n log(cid:104) P(ot,vi tn,v[:i]) (cid:105)
.
θ t t t i=1 P(ot,vi tn,v[:i−1])
3E.g., the alfworld environment (to be introduced in Section 5.2) contains an observation-dependent
admissibleactionspace.
5However, the naïve calculation logπ (a |o ,vin) ← (cid:80)n log(cid:104) P(ot,vi tn,v[:i]) (cid:105) may not be ideal
θ t t t i=1 P(ot,vi tn,v[:i−1])
for computing π (a |o ,vin) since our formatted output vout also contains CoT reasoning. This
θ t t t t
is because in vout = [vtht,vact], the CoT reasoning tokens vtht are generally much longer than
t t t t
the action tokens vact (see the blue and red parts in Figure 3 for examples, and Table 3 for a
t
relative scaling of their sum log-likelihood). Hence the naïve computation logπ (a |o ,vin) ←
θ t t t
logπ (vtht|o ,vin)+logπ (vact|o ,vin,vtht)willmakelogπ (a |o ,vin)largelydeterminedbythe
θ t t t θ t t t t θ t t t
CoTtokenslogπ (vtht|o ,vin),whichispracticallyundesirablebecauseourpipelineonlyrelieson
θ t t t
vactfordecision-making.
t
As shown in Table 1, logπ (vtht|o ,vin) typically has
θ t t t log NL BJ EZP P24 ALF
a much larger magnitude than logP(vact|o ,vin,vtht)
t t t t
across all tasks we have tested (in terms of absolute vtht -3.4 -2.2 -9.0 -37.6 -20.3
t
value). Hence, to mitigate the effect of the CoT to- vact 0.0 0.0 0.0 0.0 -0.4
t
kens, we adopt a scaling factor λ ∈ [0,1] to scale
downlogπ (vtht|o ,vin)toobtainaregularizedversion Table 1: The absolute values of sum log
oflogπ
(aθ
|o
,t vint ),wt
hichresultsin
probabilityofvt thtismuchlargerthanva tct.
θ t t t Eachnumberisaveragedamong1000samples
on our evaluation tasks to be introduced in
logπ θ(a t|o t,vi tn) Section5.
(4.3)
←λlogπ (vtht|o ,vin)+logπ(vact|o ,vin,vtht).
θ t t t t t t t
Empirically,weobservethatscalingfactorλcouldlargelyaffectthefinalperformance. Aswewill
showinSection6.2,choosinganextremeλvalue(closeto1or0)willdegradeoverallperformance.
Allofourexperimentsadoptλ∈[0.2,0.5].
4.4 FormalImplementation
Puttingthepromptconstructionfunctionh(Section4.1),thepost-processingfunctionf (Section4.2),
andthecomputationofπ (a |o ,vin)(Section4.3)together,weconcludeourmethodinAlgorithm1.
θ t t t
Algorithm1TrainingVLMwithRL
1: Input: Anenvironmentenv,aninitialVLMwithparametersθ 0.
2: Input: Apost-processingfunctionf,aCoTreasoningscalingfactorλ.
3: Input: ReplaybuffersizeB,maximumepisodelengthT.
4: fork =0,...,K−1do
5: t=0 ▷ResetRLtimestep
6: o t =env.reset() ▷Resettheinitialstate
7: vi tn =h(o t) ▷Generatevi tnfromo t,hisdefinedinSection4.1
8: B k =∅ ▷Initializeanon-policyreplaybuffer
9: while|B k|≤Bdo
10: vo tut =π θk(o t,vi tn) ▷Generatetextoutput
11: a t =f(vo tut) ▷Obtainalegalactionfromvo tut,f isdefinedinEquation4.1
12: π θk(a t|o t,vi tn)=λlogπ θk(vt tht|vi tn)+logπ θk(va tct|o t,vi tn,vt tht) ▷Equation4.2
13: r t,o t+1 =env.step(a t)
14: B k =B k∪{(o t,a t,r t,vo tut,π θk(a t|o t,vi tn))} ▷AdddatatothebufferB k
15: t=t+1
16: ift=T then
17: t=0 ▷ResetRLtimestepifthemaximumstepisreached
18: o 0 =env.reset() ▷Resetenvironment
19: endif
20: vi tn =h(o t) ▷Preparethenextvi tn
21: endwhile
22: RunPPO[54]withdataB k toobtainθ k+1
23: endfor
24: Output: θ K.
65 EvaluationTasks
How does our method improve a VLM’s decision-making capabilities in tasks that require fine-
grainedvision-languagereasoningorsemanticunderstanding? Tostudythisquestion,weadopttwo
differentdomains: gym_cardsandalfworld[57]. Ouroriginalgym_cardsdomainisa“gym-like”
environment[8]containingfourtasksdesignedtotestthedecision-makingcapabilitiesofVLMs.
These tasks require fine-grained visual-language reasoning, specifically focusing on recognizing
numbersforarithmeticreasoning. Inaddition,wealsoadoptalfworld[57],whichassessesthe
decision-making capabilities of VLMs in an embodied AI setting that demands visual semantic
understanding. WepresentsomeexamplesofthevisualobservationsofeachtaskinFigure4. Wedo
notincludestandardimage-basedAtaribenchmarks[5,38]duetolimitedcomputationresources.4
(a)NumberLine (b)EZPoints (c)Points24 (d)Blackjack (e)alfworld
Figure4: Examplesofobservationofourevaluationtasks.(a)-(d)arefromouroriginalgym_cardsdomain.
(a)-(c)aredeterministictaskswithincreasingdifficulties;(d)isastochastictask.
5.1 GymCards
Our gym_cards domain is designed to evaluate a VLM’s decision-making capabilities requiring
fine-grainedvisionrecognitionandlanguagereasoning. Moreprecisely, tasksinthegym_cards
domainrequiretheVLMtorecognizethenumbers(potentiallyfromcards)andutilizethenumbers
forlanguagereasoning. AsdepictedinFigure4,thefirstthreetasks—NumberLine,EZPoints,and
Points24—aredeterministic,anddevelopedtoassesstheVLMs’abilitytoidentifyandprocessnum-
bersormathematicaloperatorsateachtimestep. Thesetasksincreaseincomplexity: NumberLine
requiresrecognitionoftwonumbersinanimage,EZPointsinvolvesidentifyingnumbersfromtwo
cards,andPoints24extendstorecognizingfourcards. TheBlackjacktaskchallengestheVLM
furtherbyrequiringtheagenttoreasonbasedonvisualinformationandadapttostochasticoutcomes.
Thissubsectionoutlinesthegoalsofeachtask,andweleavethedetaileddescriptionsoftheirstate
spaces,actionspaces,andrewardfunctionstoAppendixB.1.
NumberLine. Inthistask,thegoalistomoveanumbertothetargetonasyntheticnumber
line. Ateachstates ,thevisualobservationo containstwolinesoftext: “Target: x”and“Current:
t t
y ”. Theagentneedstomovethecurrentnumbery tothetargetnumberx,byoutputtingtextvout
t t t
that interacts with the discrete action space {"+", "−"}. Mapping the vout to "+" or "−" will
t
increaseordecreasethecurrentnumberby1,respectively.
EZPoints. Inthistask, thegoalistooutputaformulausingthenumbersinthecardsthat
evaluatesto12. Ateachstates ,theagentobservesanimageoftwocardsandatextversionof
t
(potentiallyincomplete)“formula”belowthecards. Thegoalistouseallnumbersinthecards(only
once)tocompute12. Theactionspacecontainsnaturalnumbersin[1,10],aswellasoperatorin
{"+","∗","="}. Ateachstates ,onlyoperatorsandnumbersthatappearinthecardsarelegal
t
actions,and“J”,“Q”,and“K”aretreatedas“10”. Inparticular,iftheoutputtextvoutismappedto
t
alegalactiona atstates ,thetextversionofa willbeappendedtothe“formula”inthecurrent
t t t
imageofs resultings ,otherwises willremainthesameass .
t t+1 t+1 t
Points24. In this task, the goal is to output a formula using the numbers in the cards that
evaluatesto24. ThePoints24taskisaharderversionofEZPointsasitcontains4cards,hence
4Image-basedAtaritasksgenerallytakeatleast2millionenvironmentstepstoreachareasonableperfor-
mance[22],whileourmethodneedsroughly30hoursforrunning15kenvironmentstepsduetothemodelsize
ofthebackboneVLMs.
7requiringtheVLMstogeneratealongerformula. TherulesofPoints24aresimilartoEZPoints,
despitetwominordifferences: thePoints24taskrequirestheVLMtocomputeatargetnumberof
24,anditsactionspacecontainsmoreoperators{"+","−","∗","/","="}.
Blackjack. Inthistask,thegoalistowinthecurrentblackjackgame. Ateachstates ,thevisual
t
observationo consistsoftwocards(oneface-down)fromthedealerandallcardsfromtheplayer.
t
Theagent’sgoalinthistaskistowinthecurrentgame,byoutputtingtextvoutthatcanbemappedto
t
{"stand","hit"}. Theagentwillreceiveonemorecardifvoutismappedto"hit",andthegame
t
willterminateifvoutismappedto"stand".
t
5.2 ALFWorld
Whilethegym_cardsdomainisdesignedtoassesstheVLM’sarithmeticreasoningrequiringfine-
grainedvisualrecognition,thealfworldenvironmentaimsattestingVLM’sdecision-makingtasks
requiringvisualsemanticunderstanding
ALFWorld. The ALFWorld embodied environment [57] is combines a text-based interactive
environment [12] with a large vision-language instruction following dataset [56]. It contains 6
differenttypesofgoal-conditionedtasks(“Pick&Place”,“ExamineinLight”,“Clean&Place”,
“Heat&Place”,“Cool&Place”,and“PickTwo&Place”),andtheagent’sgoalistonavigateinthe
environmentviatext-basedactions(e.g.,"gotoshelf1","examinesidetable1"). Unlike
ouroriginalgym_cardsenvironment,whereallstatessharethesameactionspace,thealfworld
environmentcontainsastate-dependentadmissibleactionspace–someactionsareonlyavailableat
certainstates. Forexample,iftheagent’sgoalisto“putsomepillowsonarmchair”,thentheagent
canonlyputapillowafterpickingupapillow. Hence,toincorporatethestate-dependentadmissible
action set, our prompt of alfworld asks the VLM to choose among an admissible action. See
Figure2foranexampleofthevisualobservationofalfworld. Weleavethedetaileddescriptionsof
thealfworld(statespace,actionspace,rewardfunctions,andtheCoTprompt)toAppendixB.2.
6 ExperimentalResults
Thefirstpartofourexperimentexamineshowourmethodimprovesthedecision-makingcapabilities
of VLMs (Section 6.1). The second part investigates the role of CoT reasoning in our method
(Section6.2). DetailsofourexperimentalsetupareprovidedinAppendixC.
6.1 ImprovingVLMDecision-MakingCapabilities
Doesourmethodimprovethedecision-makingcapabilitiesofVLMagentsacrossvariousdomains?
Toinvestigatethis,weassesshowourmethodimprovesarithmetictasksrequiringfine-grainedvisual
recognitioninthegym_cardsdomainandvisualsemanticreasoninginthealfworlddomain. The
gym_cardsexperimentsincludedeterministictasks(NumberLine,EZPoints,andPoints24,each
withincreasingdifficulty)andastochastictask(Blackjack). Inthealfworlddomain,weevaluate
overallperformanceanddetailedtask-specificperformanceasdiscussedinSection5.2.Weinstantiate
ourmethodontopofthellava-v1.6-mistral-7b[34]modelandcompareitagainstcommercialmodels
(GPT4-VandGemini),asupervisedfine-tunedversionofthellava-v1.6-mistral-7bmodel(LLaVA-
sft),5andavanillaRLimplementationusingaCNN-basedpolicynetwork(CNN+RL).6Thefinal
resultsandlearningcurvesarepresentedinTable2andFigure5. Detailsoftheexperimentalsetup
areprovidedinAppendixC.
Enhancingdecision-makingcapabilitiesofVLMagentsacrossvarioustasks. Asillustrated
in Table 2 and Figure 5, our method demonstrates consistent improvement across various tasks,
5ToensuretheRLtrainingstartsfromamodelwithreasonableinstructionfollowingcapabilities[44],our
RLtrainingforVLMstartsfromtheLLaVA-sftcheckpointofeachtask,weleavethedetailedtrainingpipeline
ofourmethodtoAppendixC.1.
6TheCNN-basedmethodadoptsthesameCLIPvisionencoderasLLaVA-7b.Additionally,fortasksthat
requiretextinputs(e.g.,alfworld),weadopttheRoBERTa-base[35]modeltoencodethetextfeatureand
concatenatethetextandCLIPvisualfeaturesfordownstreamRLtraining.DetailsofourCNN-basedmodelare
providedtoAppendixC.2.
8gym_cards alfworld
NL EZP P24 BJ Avg. Exp.Data Pick Look Clean Heat Cool Pick2 Avg.
BUTLERg - - - - - ✓ 33.0 17.0 26.0 70.0 76.0 12.0 22.0
BUTLER - - - - - ✓ 46.0 22.0 39.0 74.0 100.0 24.0 37.0
CNN+RL 87.1 0 0 38.8 31.5 ✗ 0 0 0 0 0 0 0
GPT4-V 65.5 10.5 0 25.5 25.4 ✗ 38.2 12.1 18.8 6.7 17.8 14.6 19.4
Gemini 82.5 2.0 0 30.0 28.6 ✗ 34.6 16.7 0 0 0 12.0 13.5
LLaVA-sft 24.8 23.0 2.6 23.1 18.4 ✗ 39.2 0 14.4 11.1 0 28.6 17.7
Ours 89.4 50.0 2.3 40.2 45.5 ✗ 47.4 14.7 10.4 14.4 18.8 18.0 21.7
Table2: Averageepisodesuccessrates(%)ofdifferentmethodsongym_cardsandalfworld. Forall
RL-basedmethods(CNN+RLandourmethod),wepresentthepeaknumbers(first15kenvironmentstepsforthe
gym_cardsand5kenvironmentstepsforalfworld)fromeachtrainingcurvefromFigure5.Weaveragethe
performanceofall4tasksongym_cardswithequalweight.Duetothenatureofthealfworldenvironment,
whereeachsubtaskdoesnotappearwithequalprobability,theaverageperformanceonalfworldisaweighted
averageamongalltypesoftasks.WemarktheBUTLER andBUTLERagent[57]ingraysincetheyrequire
g
expertdata,whiletheremainingmethodsdonotrequireexpertdata.AsdiscussedbyShridharetal.[57],the
performancediscrepancybetweenBUTLER andBUTLERhappensduetodifferentdecodingstrategiesin
g
evaluationstrategies: BUTLER usesgreedydecoding,whichmayrepeatfailedactions,whereasBUTLER
g
employsbeamsearchduringevaluation.
gym_cards/Numberline gym_cards/EZPoints gym_cards/Blackjack ALFWorld
50 45 30
80 40 40
Ours 20
60 CNN+RL 30 35
40 G GP emT4 in-V i 12 00 30 10
LLaVA-sft 25
20 0 0
0 5k 10k 15k 0 5k 10k 15k 0 5k 10k 15k 1k 2k 3k 4k 5k
env steps env steps env steps env steps
Figure5: Episodesuccessrates(%)ofdifferentmethodsongym_cardsandalfworldduringtraining.
Lefttoright:gym_cards/Numberline,gym_cards/EZPoints,gym_cards/Blackjack,andalfworld(all).
ThecurvesofPoints24arenotincludedbecausenoneofthetestedmethodsachievereasonableperformance.
includingdeterministic(NumberLineandEZPoints)7orstochastic(Blackjack)arithmetictasks
and visual semantic reasoning task (alfworld). Specifically, our method improves the average
performancefromtheinitialLLaVA-sftmodelby27.1%onarithmetictasks(18.4%→45.5%)and
4.0% on visual semantic decision-making task (17.7% → 21.7%). In addition, our method also
achievesthebestperformanceamongallcomparativemethods,surpassingthesecond-bestmethod
by14.0%(CNN+RL)ongym_cardsand2.3%(GPT4-V)onalfworld.
6.2 UnderstandingtheRoleoftheCoTReasoning
InSection6.1,wehavedemonstratedthatourmethodimprovesthearithmeticandvisualsemantic
reasoningcapabilitiesofVLMagents. Conceptually,ourmethodcanbeviewedasanaugmented
versionofthestandardCNN-basedRL,wherethetextoutput[vtht,vact](fromFigure3)serveasthe
textactionvact,augmentedbyCoTreasoningvtht. Thisraisesanimportantquestion: Howdoesthe
CoTreasoningvthtinfluencetheoverallperformanceofourmethod? ToassesstheimpactofCoT
reasoningonourmethod’sperformance,weconducttwosetsofablationexperiments. Thefirstset
(presentedinTable3andFigure6)evaluatesourmethodwithouttheCoTreasoning,andthesecond
part(showninFigure7)examinesvariousscalinghyperparametersλforthelog-likelihoodofCoT
tokens,asdefinedinEquation4.3.
ThecrucialroleofCoTreasoninginperformanceimprovement. AspresentedinTable3and
Figure6,theperformanceofourmethodsignificantlydecreaseswithouttheCoTreasoning.8 Besides
the improvement in the final performance, CoT reasoning appears to be a crucial component for
deterministicarithmetictasks(BlackjackandEZPoints),asourmethodfailstoimprovethesetwo
taskswithouttheCoTreasoning.
7AlthoughPoints24sharessimilarruleswithEZPoints,itrequirestheVLMtorecognizeallfourcards
andgeneratemuchlongerequations. MostfailurecasesinPoints24arecausedbyeitherinaccuratevisual
perceptionorflawedlanguagereasoning.WeprovidesomeexamplesofthesefailuresinAppendixC.5.
8ExceptfortheBlackjacktask,wherethepeakperformancewithoutCoTisslightlybetter(+0.2%).
9gym_cards alfworld
CoT NL EZP P24 BJ Avg. Pick Examine Clean Heat Cool Pick2 Avg.
✓ 89.4 50.0 2.3 40.2 45.5 47.4 14.7 10.4 14.4 18.8 18.0 21.7
✗ 26.9 29.9 0 40.4 24.3 40.5 12.0 2.8 8.5 14.4 17.7 16.3
Diff.(✓-✗) +62.5 +20.1 +2.3 -0.2 +21.2 +6.9 +2.7 +7.6 +5.9 +4.4 +0.3 +5.4
Table3: Episodesuccessrates(%)ofourmethodwithandwithoutCoTreasoning. Wereportthebest
resultsfromFigure6(first15kenvironmentstepsforthegym_cardsand5kenvironmentstepsforalfworld).
gym_cards/Numberline gym_cards/EZPoints gym_cards/Blackjack ALFWorld
50 40
80 25
60 w/ CoT 40 35 20
40 w/o CoT 30 30 15
20 25 10
20
0 5k 10k 15k 0 5k 10k 15k 0 5k 10k 15k 1k 2k 3k 4k 5k
env steps env steps env steps env steps
Figure 6: Training cuvres of our method without and without the CoT reasoning. Left to right:
gym_cards/Numberline,gym_cards/EZPoints,gym_cards/Blackjack,andalfworld(all).Thecurves
ofPoints24arenotincludedbecausenoneofthetestedmethodsachievereasonableperformance.
The importance of moderate scaling factors λ. As dis-
cussedinSection4.3,integratingCoTreasoningintoourframe- 80
workinvolvestuninganadditionalhyperparameter,λ∈[0,1] 0.1
0.3
(proposedinEquation4.3). Toidentifyanoptimalrangefor 60
0.5
λ,weconductexperimentsassessingtheimpactofvariousλ. 0.7
OurresultsinFigure7indicatethatamoderateλ(between0.3 40 0.9
and0.5)facilitateseffectivetrainingontheNumberLinetask.
20
Conversely,ourmethodunderperformswhenλissettoolarge
(≥0.7)ortoosmall(≤0.1),andweempiricallyfindthatanop- 0 2k 4k 6k 8k 10k
env steps
timalλtypicallyfallswithin0.2to0.5. Thisisbecausealarge
λresultsinanestimateoflogπ (a |o ,vin)beingoverlyinflu- Figure7: Episodesuccessrates(%)
θ t t t
encedbylogπ (vtht|o ,vin),whileasmallλvaluecausesπ to ofourmethodunderdifferentλon
bepredominanθ tlyt affet ctedt bylogπ (vact|o ,vin,vtht),therθ eby NumberLine.
θ t t t t
reducingtheeffectoftheCoTreasoninginRLtraining.
7 Conclusions,Limitations,andFutureDirections
Inthispaper,weintroduceanalgorithmicframeworkthatdirectlyfine-tunesVLMsusingRL,with
thehelpoftheVLM’sCoTreasoningcapability. Empiricalresultsdemonstratethatourmethodcan
enhancethedecision-makingabilitiesofVLMsacrossdiversedomainsthatrequirefine-grainedvisual
recognitionorvisualsemanticunderstanding. Inaddition,wedemonstratethatCoTreasoningisa
crucialcomponentforenablingRLtraining,allowing7bVLMstooutperformestablishedcommercial
modelssuchasGPT-4VandGeminionmosttasks. WhileourresultssuggestthatCoTreasoningis
crucialtotheperformanceimprovementofVLMtrainingwithRL,wehavenotextensivelyexplored
theeffectsofdifferentpromptingtechniquesinthiswork,whichwillbeaninterestingfuturedirection.
Anotherlimitationofthisworkisthatourtrainingparadigmonlyimprovestheperformanceofone
task, it will be interesting to extend our method in the future so that it improves multiple tasks
simultaneously.
8 Acknowledgement
WewouldliketothankWilliamChen,KuanFang,AviralKumar,QiyangLi,FangchenLiu,Oier
Mees,SeohongPark,KarlPertsch,HaozhiQi,Chun-HsiaoYeh,andAndreaZanettefortheearly
discussionsandsuggestionsontheproject. ThisresearchwassupportedbyNSFRIIIS-2246811,
AFOSRFA9550-22-1-0273,thejointSimonsFoundation-NSFDMSgrant#2031899,theONRgrant
N00014-22-1-2102,TsinghuaBerkeleyShenzhenInstitute(TBSI)ResearchFund,andtheHong
KongCenterforConstructionRoboticsLimited(HKCRC)Award052245. Wewouldalsoliketo
thankHyperbolicLabsforthecomputingsupport.
10References
[1] MarwaAbdulhai,IsadoraWhite,CharlieSnell,CharlesSun,JoeyHong,YuexiangZhai,Kelvin
Xu,andSergeyLevine. Lmrlgym: Benchmarksformulti-turnreinforcementlearningwith
languagemodels. arXivpreprintarXiv:2311.18232,2023.
[2] AlekhAgarwal,NanJiang,ShamMKakade,andWenSun. Reinforcementlearning: Theory
andalgorithms. CSDept.,UWSeattle,Seattle,WA,USA,Tech.Rep,32,2019.
[3] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE
internationalconferenceoncomputervision,pages2425–2433,2015.
[4] KateBaumli,SatinderBaveja,FeryalBehbahani,HarrisChan,GheorgheComanici,Sebastian
Flennerhag,MaximeGazeau,KristianHolsheimer,DanHorgan,MichaelLaskin,etal. Vision-
languagemodelsasasourceofrewards. arXivpreprintarXiv:2312.09187,2023.
[5] M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling. Thearcadelearningenvironment: An
evaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:253–279,
jun2013.
[6] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,PrzemysławDe˛biak,Christy
Dennison,DavidFarhi,QuirinFischer,ShariqHashme,ChrisHesse,etal. Dota2withlarge
scaledeepreinforcementlearning. arXivpreprintarXiv:1912.06680,2019.
[7] RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydneyvon
Arx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal. Onthe
opportunitiesandrisksoffoundationmodels. arXivpreprintarXiv:2108.07258,2021.
[8] GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,
andWojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
[9] LouisCastricato,AlexHavrilla,ShahbulandMatiana,DuyV.Phung,AmanTiwari,Jonathan
Tow, and Maksym Zhuravinsky. trlX: A scalable framework for RLHF, June 2023. URL
https://github.com/CarperAI/trlx.
[10] WilliamChen,OierMees,AviralKumar,andSergeyLevine. Vision-languagemodelsprovide
promptablerepresentationsforreinforcementlearning. arXivpreprintarXiv:2402.02651,2024.
[11] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[12] Marc-AlexandreCôté,AkosKádár,XingdiYuan,BenKybartas,TavianBarnes,EmeryFine,
James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A
learningenvironmentfortext-basedgames. InComputerGames: 7thWorkshop,CGW2018,
HeldinConjunctionwiththe27thInternationalConferenceonArtificialIntelligence,IJCAI
2018,Stockholm,Sweden,July13,2018,RevisedSelectedPapers7,pages41–75.Springer,
2019.
[13] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,Jingjing
Xu,andZhifangSui. Asurveyforin-contextlearning. arXivpreprintarXiv:2301.00234,2022.
[14] LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,Andrew
Tang, De-AnHuang, YukeZhu, andAnimaAnandkumar. Minedojo: Buildingopen-ended
embodiedagentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessing
Systems,35:18343–18362,2022.
[15] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,
WeiLin,JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkfor
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
[16] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based
promptingformulti-stepreasoning. InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.net/forum?id=yf1icZHC-l9.
[17] DeepMindGoogle. Introducinggemini: ourlargestandmostcapableaimodel,2023. URL
https://blog.google/technology/ai/google-gemini-ai/.
11[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policymaximumentropydeepreinforcementlearningwithastochasticactor. InInternational
conferenceonmachinelearning,pages1861–1870.PMLR,2018.
[19] Joey Hong, Sergey Levine, and Anca Dragan. Zero-shot goal-directed dialogue via rl on
imaginedconversations. arXivpreprintarXiv:2311.05584,2023.
[20] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
InternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=nZeVKeeFYf9.
[21] JiaxingHuang,JingyiZhang,KaiJiang,HanQiu,andShijianLu. Visualinstructiontuning
towardsgeneral-purposemultimodalmodel: Asurvey. arXivpreprintarXiv:2312.16602,2023.
[22] ShengyiHuang,QuentinGallouédec,FlorianFelten,AntoninRaffin,RousslanFernandJulien
Dossa,YanxiaoZhao,RyanSullivan,ViktorMakoviychuk,DenysMakoviichuk,MohamadH
Danesh, et al. Open rl benchmark: Comprehensive tracked experiments for reinforcement
learning. arXivpreprintarXiv:2402.03046,2024.
[23] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Vox-
poser: Composable 3d value maps for robotic manipulation with language models. In 7th
AnnualConferenceonRobotLearning,2023. URLhttps://openreview.net/forum?id=
9_8LF30mOC.
[24] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[25] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Large
languagemodelsarezero-shotreasoners. Advancesinneuralinformationprocessingsystems,
35:22199–22213,2022.
[26] Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail,2018.
[27] HeinrichKüttler,NantasNardelli,AlexanderMiller,RobertaRaileanu,MarcoSelvatici,Edward
Grefenstette,andTimRocktäschel. Thenethacklearningenvironment. AdvancesinNeural
InformationProcessingSystems,33:7671–7684,2020.
[28] Chunyuan Li. Large multimodal models: Notes on cvpr 2023 tutorial. arXiv preprint
arXiv:2306.14895,2023.
[29] ChunyuanLi,ZheGan,ZhengyuanYang,JianweiYang,LinjieLi,LijuanWang,andJianfeng
Gao. Multimodalfoundationmodels: Fromspecialiststogeneral-purposeassistants. arXiv
preprintarXiv:2309.10020,1(2):2,2023.
[30] HunterLightman,VineetKosaraju,YuraBurda,HarriEdwards,BowenBaker,TeddyLee,Jan
Leike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let’sverifystepbystep. arXivpreprint
arXiv:2305.20050,2023.
[31] FuxiaoLiu,TianruiGuan,ZongxiaLi,LichangChen,YaserYacoob,DineshManocha,and
TianyiZhou. Hallusionbench: Youseewhatyouthink? oryouthinkwhatyousee? animage-
contextreasoningbenchmarkchallengingforgpt-4v(ision),llava-1.5,andothermulti-modality
models. arXivpreprintarXiv:2310.14566,2023.
[32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[33] HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstructiontuning. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=w0H2xGHlkw.
[34] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,2024.
[35] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019.
12[36] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[37] ChaochaoLu,ChenQian,GuodongZheng,HongxingFan,HongzhiGao,JieZhang,JingShao,
JingyiDeng,JinlanFu,KexinHuang,etal. Fromgpt-4togeminiandbeyond: Assessingthe
landscapeofmllmsongeneralizability,trustworthinessandcausalitythroughfourmodalities.
arXivpreprintarXiv:2401.15071,2024.
[38] MarlosC.Machado,MarcG.Bellemare,ErikTalvitie,JoelVeness,MatthewJ.Hausknecht,
andMichaelBowling. Revisitingthearcadelearningenvironment: Evaluationprotocolsand
open problems for general agents. Journal of Artificial Intelligence Research, 61:523–562,
2018.
[39] ManolisSavva*,AbhishekKadian*,OleksandrMaksymets*,YiliZhao,ErikWijmans,Bhavana
Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.
Habitat: APlatformforEmbodiedAIResearch. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision(ICCV),2019.
[40] VolodymyrMnih, KorayKavukcuoglu, DavidSilver, AndreiARusu, JoelVeness, MarcG
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-levelcontrolthroughdeepreinforcementlearning. nature,518(7540):529–533,2015.
[41] YaoMu,QinglongZhang,MengkangHu,WenhaiWang,MingyuDing,JunJin,BinWang,
JifengDai,YuQiao,andPingLuo. Embodiedgpt: Vision-languagepre-trainingviaembodied
chainofthought. arXivpreprintarXiv:2305.15021,2023.
[42] OpenAI. Gpt-4,2023. URLhttps://openai.com/research/gpt-4.
[43] OpenAI. Gpt-4v,2023. URLhttps://openai.com/research/gpt-4v-system-card.
[44] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. AdvancesinNeuralInformationProcessingSystems,
35:27730–27744,2022.
[45] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Au-
tonomousevaluationandrefinementofdigitalagents. arXivpreprintarXiv:2404.06474,2024.
[46] LiangmingPan,MichaelSaxon,WendaXu,DeepakNathani,XinyiWang,andWilliamYang
Wang. Automaticallycorrectinglargelanguagemodels: Surveyingthelandscapeofdiverse
self-correctionstrategies. arXivpreprintarXiv:2308.03188,2023.
[47] JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,and
MichaelSBernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceed-
ingsofthe36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pages
1–22,2023.
[48] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[49] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa,
ChristianBauckhage,HannanehHajishirzi,andYejinChoi. Isreinforcementlearning(not)for
naturallanguageprocessing: Benchmarks,baselines,andbuildingblocksfornaturallanguage
policyoptimization. InTheEleventhInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=8aHzds2uUyB.
[50] JeffRasley, SamyamRajbhandari, OlatunjiRuwase, andYuxiongHe. Deepspeed: System
optimizations enable training deep learning models with over 100 billion parameters. In
Proceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&
DataMining,pages3505–3506,2020.
[51] JuanRocamonde,VictorianoMontesinos,ElvisNava,EthanPerez,andDavidLindner. Vision-
language models are zero-shot reward models for reinforcement learning. In The Twelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=N0I2RtD8je.
13[52] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
regionpolicyoptimization. InInternationalconferenceonmachinelearning,pages1889–1897.
PMLR,2015.
[53] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensionalcontinuouscontrolusinggeneralizedadvantageestimation. InProceedingsofthe
InternationalConferenceonLearningRepresentations(ICLR),2016.
[54] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[55] BokuiShen,FeiXia,ChengshuLi,RobertoMartín-Martín,LinxiFan,GuanzhiWang,Claudia
Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A
simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages7520–7527.IEEE,
2021.
[56] MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,RoozbehMot-
taghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded
instructionsforeverydaytasks. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10740–10749,2020.
[57] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and
MatthewHausknecht. {ALFW}orld: Aligningtextandembodiedenvironmentsforinteractive
learning. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=0IOX0YcCdTn.
[58] DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-
che,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mas-
teringthegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,
2016.
[59] CharlieVictorSnell,IlyaKostrikov,YiSu,SherryYang,andSergeyLevine. OfflineRLfor
naturallanguagegenerationwithimplicitlanguageqlearning. InTheEleventhInternational
ConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?
id=aBH_DydEvoH.
[60] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,ChelseaVoss,Alec
Radford,DarioAmodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems,33:3008–3021,2020.
[61] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,Chuang
Gan,Liang-YanGui,Yu-XiongWang,YimingYang,etal. Aligninglargemultimodalmodels
withfactuallyaugmentedrlhf. arXivpreprintarXiv:2309.14525,2023.
[62] RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,
2018.
[63] AndrewSzot,MaxSchwarzer,HarshAgrawal,BogdanMazoure,WalterTalbott,Katherine
Metcalf,NatalieMackraz,DevonHjelm,andAlexanderToshev. Largelanguagemodelsas
generalizablepoliciesforembodiedtasks. arXivpreprintarXiv:2310.17722,2023.
[64] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyeswide
shut? exploringthevisualshortcomingsofmultimodalllms. arXivpreprintarXiv:2401.06209,
2024.
[65] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[66] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[67] MarkTowers,JordanK.Terry,ArielKwiatkowski,JohnU.Balis,GianlucadeCola,Tristan
Deleu, Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-
Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G.
Younis. Gymnasium,March2023. URLhttps://zenodo.org/record/8127025.
14[68] OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,Jun-
youngChung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal. Grandmaster
levelinstarcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
[69] LeandrovonWerra,YounesBelkada,LewisTunstall,EdwardBeeching,TristanThrush,Nathan
Lambert,andShengyiHuang. Trl: Transformerreinforcementlearning. https://github.
com/huggingface/trl,2020.
[70] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023.
[71] RuoyaoWang,PeterAlexanderJansen,Marc-AlexandreCôté,andPrithvirajAmmanabrolu.Sci-
enceworld:Isyouragentsmarterthana5thgrader? InConferenceonEmpiricalMethodsinNat-
uralLanguageProcessing,2022. URLhttps://api.semanticscholar.org/CorpusID:
247451124.
[72] XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,EdH.Chi,SharanNarang,Aakanksha
Chowdhery,andDennyZhou.Self-consistencyimproveschainofthoughtreasoninginlanguage
models. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=1PL1NIMMrw.
[73] ZihaoWang,ShaofeiCai,GuanzhouChen,AnjiLiu,XiaojianMa,andYitaoLiang. Describe,
explain,planandselect: Interactiveplanningwithlargelanguagemodelsenablesopen-world
multi-taskagents. arXivpreprintarXiv:2302.01560,2023.
[74] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
AdvancesinNeuralInformationProcessingSystems,35:24824–24837,2022.
[75] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,
JunzheWang, SenjieJin, EnyuZhou, etal. Theriseandpotentialoflargelanguagemodel
basedagents: Asurvey. arXivpreprintarXiv:2309.07864,2023.
[76] SherryYang,OfirNachum,YilunDu,JasonWei,PieterAbbeel,andDaleSchuurmans. Foun-
dation models for decision making: Problems, methods, and opportunities. arXiv preprint
arXiv:2303.04129,2023.
[77] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasLGriffiths,YuanCao,andKarthik
Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv
preprintarXiv:2305.10601,2023.
[78] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and
Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=WE_vluYUL-X.
[79] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities.
arXivpreprintarXiv:2308.02490,2023.
[80] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma.
Investigatingthecatastrophicforgettinginmultimodallargelanguagemodelfine-tuning. In
ConferenceonParsimonyandLearning,pages202–227.PMLR,2024.
[81] DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchu-
urmans,ClaireCui,OlivierBousquet,QuocVLe,andEdH.Chi. Least-to-mostprompting
enables complex reasoning in large language models. In The Eleventh International Con-
ferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?id=
WZH7099tgfM.
[82] YifeiZhou,AndreaZanette,JiayiPan,SergeyLevine,andAviralKumar. Archer: Training
languagemodelagentsviahierarchicalmulti-turnrl. arXivpreprintarXiv:2402.19446,2024.
[83] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
[84] DanielMZiegler,NisanStiennon,JeffreyWu,TomBBrown,AlecRadford,DarioAmodei,
PaulChristiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences.
arXivpreprintarXiv:1909.08593,2019.
15A Contributions
• YXZ:proposed,led,andmanagedtheproject;integratedallcodebases;ranallablationsfor
methoddevelopment;babysatallexperiments;implementedthepost-processingfunctionf;
proposedandimplementedthescalingfactorλforactiontokens;beautifiedthegym_cards
environment;maintainedallcodebases;wrotethemajorpartofthepaper.
• HB:setuptheinfrastructureandinitialexperimentsforsupervisedfine-tuningbeforeRL
training; setupmajorinfrastructuresforparametersweeping; maintainedallcodebases;
partiallywrotethepaper.
• ZL: set up the alfworld environment; set up major infrastructures for data collection;
maintainedallcodebases;partiallywrotethepaper.
• JP:proposedtheCoTideaforend-to-endRLtraining;optimizedtheRLtrainingframework
withquantizationandenableddistributedtraining;implementedtheinitialversionofthe
gym_cardsenvironment;partiallywrotethepaper.
• ST:maintainedtheusageofLLaVArepo[33,32,34];implementedthequeriesforGPT4-V
andGemini;partiallywrotethepaper.
• YFZ: implementedtheinitialversionofRLtrainingonLLaVA;partiallywrotethepaper.
• AS,SX,YL,YM,SL:providedsuggestionsfortheproject. AS,SX,SLalsoprovided
feedbacksonwriting. YM,SLinspiredYXZtoinitiatetheentireproject.
B AdditionalDetailsoftheEvaluationTasks
B.1 GymCards
B.1.1 NumberLine
Stateandactionspace. IntheNumberLinetask,thevisualobservationateachstates containstwo
t
linesoftext: “Target: x”and“Current: y ”,wherex,y arebothintegerssuchthatx,y ∈[0,n ],
t t t max
wheren isanenvironmentinputvariablethatcontrolsthemaximumpositionofthenumbers.
max
Thegoalistomovethecurrentnumbery tothetargetnumberx,bysequentiallychoosingactions
t
fromthediscreteactionspace{"+","−"}. Wesetn =5forallexperimentsinthiswork,but
max
n canbesettoanypositiveintegers. Choosing"+"or"−"willincreaseordecreasethecurrent
max
numbery by1,respectively,andtheagentwillstayattheboundaryifittakesanactionthatattempts
t
tocrosstheboundary(e.g.,takinga ="+"wheny =n ora ="−"whenx =0). Seean
t t max t t
exampleofthestateactiontransitioninFigure8.
−→ "action": "+" −→
Figure8: AnexampleofthetransitioninNumberLine.
Reward functions and the CoT prompts. An episode in NumberLine ends when the current
numberequalsthetargetnumberorthemaximumstepT =2n isreached. Theagentreceives
max
a terminal reward of r(s ,a ) = 1 when y = x. The agent also receives a reward penalty of
t t t+1
r(s ,a )=−1upontakinganincorrectactionthatdoesnotresultinacloserpositiontothetarget
t t
(|x−y |≥|x−y |),otherwisetheagentreceivesrewardr(s ,a )=0. Intheexampleprovided
t t+1 t t
above (Figure 8), the agent receives a reward r = 0, since it moves closer to the target, but not
reachingthetargetyet. FortheNumberLinetask,weadoptthefollowingCoTpromptinFigure9,
and for the case without CoT reasoning (discussed in Section 6.2), we use the same prompt but
withouttheblueCoTreasoningparts.
16CoTpromptvinfortaskNumberLine
t
Youareplayingagamecallednumberline.Youwillseeatargetnumberandacurrentnumberinthe
image.Andyourgoalistomovethecurrentnumberclosertothetargetbychoosingeitheradding
orsubtractingonetothecurrentnumber.Yourresponseshouldbeavalidjsonfileinthefollowing
format:
{
"currentnumber":"x",
"targetnumber":"x",
"thoughts":{firstreadoutthecurrentandtargetnumber,thenthinkcarefullyaboutwhichactionto
choose},
"action":"-"or"+"
}
Figure9: Task-specificCoTpromptinputvinforNumberLine.ThebluepartrepresentstheCoTreasoning
t
andtheredpartisthetext-basedaction.
B.1.2 EZPoints
Stateandactionspace. IntheEZPointstask,theagentwillobserveanimageoftwocardsand
a text version of “formula” below the cards, at each state s . The goal is to use the cards in the
t
imagetocomputeatargetnumberof12andweview{"J","Q","K"}as"10". Theactionspaceof
EZPointsis{"1","2",...,"10","+","∗","="}andeachnumberinthecardscanonlybeused
once. Anyactionattemptingtoeitherselectanumbernotshowninthecardsoruseacardmorethan
onceareillegal. Ats ,ifalegalactiona istaken,theactionwillbeappendedtothetext“formula”
t t
ins andbecomesthenextstates . Ontheotherhand,whenanillegalactionistaken,s will
t t+1 t+1
remainthesameass . AllimagesgeneratedfromtheEZPointsenvironmentareguaranteedtohave
t
aviablesolutionforcomputing12.
−→ "action": "+" −→
Figure10: AnexampleofthetransitioninEZPoints.
Reward functions and the CoT prompts. An episode terminates when "=" is taken or the
maximumstepT = 5isreached. Theagentreceivesarewardofr = −1upontakinganillegal
action,andr =0whiletakingalegalaction. When"="istaken,theagentwillreceiveapositive
rewardr =10iftheformulaequals12,andr =−1otherwise. FortheEZPointstask,weadoptthe
followingCoTpromptinFigure11,andforthecasewithoutCoTreasoning(discussedinSection6.2),
weusethesamepromptbutwithouttheblueCoTreasoningpartsandthebrownpartinFigure11is
thetextversionofthecurrentformuladirectlyextractedfromthecurrentstates .
t
17CoTpromptvinforEZPoints
t
Youareanexpertcardgameplayer.Youareobservingtwocardsintheimage.Youareobservingthe
currentformula:'5'.Youcanchoosebetween['1','2','3','4','5','6','7','8','9','10',
'+','*','='].Thenumberoroperatoryouchoosewillbeappendedtothecurrentformula.Note
that'J','Q',and'K'countas'10'.Yourgoalistooutputaformulathatevaluatesto12,andeach
numbercanonlybeusedonce.Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"cards":[x,y],
"currentformula":'5',
"thoughts": {Firstcheckwhetherthecurrentformula'z'iscomplete. Ifthecurrentformula'z'
iscomplete,output'='.Otherwiseconsiderwhichnumberoroperatorshouldbeappendedtothe
currentformulatomakeitequal12.}
"action":"{number}"or"{operator}"
}
Figure11: Task-specificCoTpromptinputvinforEZPointsgiventheobservationinFigure10.Theblue
t
partrepresentstheCoTreasoning,theredpartisthetext-basedaction,andthebrownpartisthestate-dependent
textfromtheformulaintheimage.
B.1.3 Points24
Stateandactionspace. SimilartoEZPoints,thegoalofPoints24isalsotogenerateaformula
tocomputethetargetnumberof24,usingallfourcards. Points24hasaslightlylargeractionspace:
{"1","2",...,"10","+","−","∗","/","(",")","="}andtwomorecards. Eachnumberinthe
cardscanonlybeusedonce. SimilartoEZPoints,anyactionattemptingtoeitherselectanumber
notshowninthecardsoruseacardmorethanonceareillegal. Ats ,ifalegalactiona istaken,
t t
theactionwillbeappendedtothetext“formula”ins andbecomesthenextstates . Whenan
t t+1
illegalactionistaken,s willremainthesameass . DifferentfromEZPointswhereallimages
t+1 t
areguaranteedtohaveaviablesolutionforcomputing12,theimagesgeneratedbyPoints24donot
alwayshaveaviablesolutionto24.
−→ "action": "+" −→
Figure12: AnexampleofthetransitioninPoints24.
RewardfunctionsandtheCoTprompts. Therewardfunctionsandterminationconditionsof
Points24 are the same as those in EZPoints. An episode terminates when "=" is taken or the
maximumstepT = 20isreached. Theagentreceivesarewardofr = −1upontakinganillegal
action,andr =0whiletakinglegalactions. When"="istaken,theagentwillreceiveapositive
reward r = 10 when the formula equals 24, and r = −1 otherwise. For the Points24 task, we
adoptthefollowingCoTpromptinFigure13,andforthecasewithoutCoTreasoning(discussed
inSection6.2),weusethesamepromptbutwithouttheblueCoTreasoningpartsandthebrown
partinFigure13isthetextversionofthecurrentformuladirectlyextractedfromthecurrentstates .
t
Wealsoprovideanadditionalfeaturethatallowsustoview{"J", "Q", "K"}as{"11", "12",
"13"},insteadof{"10"}.
18CoTpromptvinforPoints24
t
Youareanexpert24pointscardgameplayer.Youareobservingthesefourcardsintheimage.You
areobservingthecurrentformula: '(2'. Youcanchoosebetween['1','2','3','4','5','6',
'7','8','9','10','+','-','*','/','(',')','='].Thenumberoroperatoryouchoosewillbe
appendedtothecurrentformula.Notethat'J','Q',and'K'countas'10'.Yourgoalistooutput
aformulathatevaluatesto24,andeachnumbercanonlybeusedonce.Yourresponseshouldbea
validjsonfileinthefollowingformat:
{
"cards":[x,y,z,w],
"currentformula":'(2'
"thoughts": {Firstcheckwhetherthecurrentformulaequals24. Ifthecurrentformulaequals24,
output'='.Otherwiseconsiderwhichnumberoroperatorshouldbeappendedtothecurrentformula
tomakeitequal24.}
"action":"{number}"or"{operator}"
}
Figure13: Task-specificCoTpromptinputvinforPoints24giventheobservationinFigure12.Theblue
t
partrepresentstheCoTreasoningandtheredpartisthetext-basedaction,brownpartisthestate-dependenttext
thatdirectlyobtainedfromtheformulaintheimage.
B.1.4 Blackjack
Stateandactionspace. FortheBlackjacktask,thevisualobservationatstates consistsoftwo
t
cards(oneface-down)fromthedealerandallcardsfromtheplayer. Theagent’sgoalinthistaskisto
winthecurrentgame,bychoosingactionsin{"stand","hit"}. Theagentwillreceiveanewcard
uponchoosing"hit". SeeFigure14foranexampletransition.
−→ "action": "hit" −→
Figure14: AnexampleofthetransitioninBlackjack.
RewardfunctionsandtheCoTprompts. Thegameterminateswhentheplayerchooses"stand"
orbusts(totalpointsexceed21). WeadoptthesamerewardfunctionastheBlackjack-v1taskin
Gymnasiym[67],wherer(s ,a )=1,0,−1uponwin,draw,andloss,respectively. Wealsoprovide
t t
asimilarfeatureasGymnasium[67],wherethe“blackjack”winning(theagentwinwithan"A"
anda"10", "J", "Q"or"K")rewardroftheplayerwillbecome1.5. Intheexampleprovidedin
Figure14,thegamehasnotterminatedaftertakingtheaction"hit",hencetheagentwillnotreceive
anyrewards,eventhoughithastotalpointsof21. FortheBlackjacktask,weadoptthefollowing
CoTpromptinFigure15,andforthecasewithoutCoTreasoning(discussedinSection6.2),weuse
thesamepromptbutwithouttheblueCoTreasoningparts.
CoTpromptvinforBlackjack
t
Youareablackjackplayer. Youareobservingthecurrentgamestate, youcanchoosebetween
['stand','hit'].Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"thoughts": "{firstdescribeyourtotalpointsandthedealer’stotalpointsthenthinkaboutwhich
actiontochoose}",
"action":"stand"or"hit"
}
Figure15: Task-specificCoTpromptinputvinforBlackjack.ThebluepartrepresentstheCoTreasoning
t
andtheredpartisthetext-basedaction.
19B.2 ALFWorld
State and action space. Inherited from Text World [12], at each state s of alfworld, the
t
agent will observe an RGB image and text-based description. The action space of alfworld
can be summarized these following format [57]: (1) goto {recep}; (2) take {obj} from
{recep};(3)put {obj} in/on {recep};(4)open {recep};(5)close {recep};(6)toggle
{obj}{recep}; (7) clean {obj} with {recep}; (8) heat {obj} with {recep}; (9) cool
{obj} with {recep},where{obj}and{recep}standsforobjectsandreceptacles. SeeFigure16
foranexampleofthestateactiontransitioninthealfworldenvironment.
"action":
−→ "gotocabinet2" −→
Youarriveatloc0.Thecabinet1is Youarriveatloc2. Thecabinet2
open.Onthecabinet1,youseeapan isopen. Onthecabinet2,yousee
1,akettle1,awinebottle1,aapple ahouseplant1,apot1,abread1,a
1,astoveknob1,astoveknob2,a kettle1,abowl1,asoapbottle1,and
stoveknob3,astoveknob4,aknife1, aknife2.
asaltshaker1,andabread1.
Figure16: Anexampleofthetransitioninalfworld.
RewardfunctionsandtheCoTprompts. Eachstates∈S ofalfworldhasasetofadmissible
actionsA (s),afinalgoalg ,andsubgoalsg . Sincethegoalofalfworldistocompletethe
adm task sub
language-basedgoal-conditionedtasks,werewardtheagentuponreachingsubgoalsandcompleting
thetask,whilepenalizingtheagentupontakinginadmissibleactions. Tosummarize,wedefinethe
rewardfunctionofalfworldasr(s ,a ,s |g )=50∗1{s =g }+1{s =g }−
t t t+1 task t+1 task t+1 sub
1{a ∈/ A (s )}. Forthealfworldtask,weadoptthefollowingCoTpromptinFigure17,andfor
t adm t
thecasewithoutCoTreasoning(discussedinSection6.2),weusethesamepromptbutwithoutthe
blueCoTreasoningpartsandthebrownpartinFigure17isthetextdescriptionofthetaskdirectly
extractedfromthecurrentstates .
t
20CoTpromptvinforalfworld
t
YourareanexpertintheALFREDEmbodiedEnvironment.Youarealsogiventhefollowingtext
descriptionofthecurrentscene:['Youarriveatloc0.Thecabinet1isopen.Onthecabinet1,you
seeapan1,akettle1,awinebottle1,aapple1,astoveknob1,astoveknob2,astoveknob3,a
stoveknob4,aknife1,asaltshaker1,andabread1.']. Yourtaskistoputacoolmugincabinet.
Youradmissibleactionsofthecurrentsituationare:['gotocountertop1','gotocabinet2','goto
countertop2','gotostoveburner1','gotodrawer1','gotodrawer2','gotodrawer3','goto
stoveburner2','gotostoveburner3','gotostoveburner4','gotodrawer4','gotocabinet3',
'gotocabinet4','gotomicrowave1','gotocabinet5','gotocabinet6','gotocabinet7','go
tosink1','gotosinkbasin1','gotofridge1','gototoaster1','gotocoffeemachine1','goto
cabinet8','gotodrawer5','gotodrawer6','gotodrawer7','gotodrawer8','gotoshelf
1','gotoshelf2','gotocountertop3','gotoshelf3','gotodrawer9','gotogarbagecan1',
'opencabinet1','closecabinet1','takepan1fromcabinet1','takekettle1fromcabinet1',
'takewinebottle1fromcabinet1','takeapple1fromcabinet1','takestoveknob1fromcabinet
1','takestoveknob2fromcabinet1','takestoveknob3fromcabinet1','takestoveknob4from
cabinet1','takeknife1fromcabinet1','takesaltshaker1fromcabinet1','takebread1from
cabinet1','inventory','look','examinecabinet1'].Yourresponseshouldbeavalidjsonfilein
thefollowingformat:
{
"thoughts": "firstdescribewhatdoyouseeintheimageusingthetextdescription,thencarefully
thinkaboutwhichactiontocompletethetask.",
"action":"anadmissibleaction"
}
Figure17: Task-specificCoTpromptinputvinforalfworldgiventheobservationinFigure16.Theblue
t
partrepresentstheCoTreasoningandtheredpartisthetext-basedaction,brownpartisthestate-dependenttext
thatdirectlyobtainedfromthetextdescriptionandtheadmissibleactionsofthecurrentstate.
C AdditionalDetailsontheExperiments
WeprovideadditionaldetailedoftheexperimentalresultsinSection6here. Detailsofourexperimen-
talpipelineisprovidedinSectionC.1,includingpreparingtheinitialSFTcheckpointsandtheRL
training. SectionC.2containsdetailssetupofallcomparativemethods. Welisttask-specifictraining
detailsinSectionC.3. WeprovideadditionalexperimentalresultsinSectionC.4. SectionC.5lists
severalfailureexamplesofthePoints24tasks.
C.1 ExperimentalPipeline
OurexperimentsadoptasimilarpipelineasRLHF[44],wherewefirstapplysupervisedfine-tuning
(SFT)tothebackbonellava-v1.6-mistral-7bmodel,beforeRLtraining. AsoutlinedbyOuyangetal.
[44],theRLHFtrainingprocedureconsistsofthreedistinctstages:SFT,learningrewardmodelsfrom
humanpreferencedata,andapplyingRLwiththelearnedrewardmodels. Ourpipelineisanalogous
toRLHFbutwithoutrequiringthecollectionofhumanpreferencedataforlearningrewardmodels,
aswecandirectlycollectrewardsfromtheenvironment.9 Consequently,ourexperimentalpipeline
onlycontainstwostages: SFTandRL,whichwewillexplainbelow.
Supervised fine-tuning. For the original gym_cards environment, we manually construct
instruction-following data for all tasks following the format specified in Figure 3 of Section 4.1.
Asforalfworld,weuseGPT4-V[43]tocollectinstructionfollowingdataforSFT.Foralltasks,
we prepare two versions of the instruction-following data, one with CoT and one without. We
leave the details of the CoT prompts for each task, and the details of each fine-tuning dataset in
Appendix D. After constructing the instruction-following data (with and without CoT), we fine-
tunellava-v1.6-mistral-7bfor1epochonthecollecteddataforeachtaskandreporttheresultsfor
LLaVA-sft.
RLtraining. Foreachtask,westartourRLtrainingfromtheLLaVA-sftcheckpoint. TheLLaVA
model [33] consists of three jointly trainable components, a CLIP vision encoder [48], an LLM
9WeadoptthesamepipelinefortheevaluationwithoutCoTreasoning(discussedinSection6.2)while
changingthedataforSFTaswellasvin(seemoredetailsonourSFTdataandvininAppendixD)
21backbone[65,66,24],andanMLPprojectorthatconnectsvisualfeaturesandthewordembeddings,
andwedirectlyapplyPPO[54]totrainallthreecomponents.Duetocomputationresourcelimitations,
we instantiate our experiments via LoRA [20], with the LoRA configuration of r = 128,α =
256,dropout=0.05,foralltrainablecomponents. FortheCoTcoefficientλ,wesetλ=0.5inthe
gym_cardsdomainandλ=0.2inalfworld.
C.2 ExperimentalSetupforComparativeMethods
GPT4-VandGemini. AllofourexperimentalresultsonGPT4-V[43]andGemini[17]aretested
onMarch15,2024,usingthesamepromptforourRLtraining(seedetailedpromptsinAppendixD).
Forgym_cards,thenumbersfrombothGPT4-VandGeminiareaveragedamongthesamenumber
ofepisodes: 200episodesfordeterministictasks(NumberLine,EZPointsandPoints24);1000
episodesforstochastictask(Blackjack). Asforalfworld,wereporttheperformanceofGPT4-V
onall1000episodeswecollected, seeAppendixD.5forourdatacollectiononalfworldusing
GPT4-V.Duetothefinancialbudget,wereporttheresultsofGeminiusing100episodes.
LLaVA-sft. ForeachnumberofLLaVA-sft,wefirstcollecttheinstruction-followingdatasetfor
eachtaskandthenfine-tuneLLaVA-1.6-7bfor1epochonthecollecteddatausingtheofficialLLaVA
fine-tuningscript.10 DetailsofourdatacollectionprocessisprovidedinAppendixD.Wealsouse
thesameLLaVA-sftcheckpointasinitializationsforthedownstreamRLtraining.
CNN-basedRL. SincetheLLaVA-7bmodeladoptsaCLIPViT-L/14visionencoderwhichismore
powerfulthanvanillaCNNembeddings,weinstantiateourCNN-basedmethodusingthefeaturefrom
thesameCLIPViT-L/14forafaircomparison. Fortasks(EZPoints,Points24,andalfworld,
seeourdetailedpromptinAppendixD)thatrequiretextinputs,weadopttheRoBERTa-base[35]
modeltoencodethetextfeatureandconcatenatethetextandCLIPvisualfeaturesfordownstream
RLtraining. AfterobtainingtheCLIP(potentiallyconcatenatedwithtext)features,weadopt2MLP
layersfollowedbyafullyconnectedlayertomaptheclipfeaturesintotheactionspace. Weadopt
thePPO[54]implementationfromKostrikov[26]asthebackboneRLalgorithm. Inaddition,we
adoptaCosineAnnealingLRlearningratescheduler,withtheinitiallearningrateof3e−4,the
finallearningrateof1e−8,andthemaximumlearningratestepof25. Theremainingtaskspecific
hyperparametersarethesameastheVLMcaseinSectionC.3.
C.3 GeneralSetupforEnd-to-EndRLTraining
Allexperimentsareconductedonan8A100sDGXmachine(80G),whilethemaximumVRAM
requirementis<40G.WeadoptDeepSpeedzero2[50]formulti-gputraining. Duringourtraining
for the VLM, we directly train all trainable components (vision encoder, LLM, and the MLP
projector). Weadoptanopen-sourceimplementation[26]forthePPO.InspiredbyvonWerraetal.
[69],Castricatoetal.[9],weapplya3-layerMLPasthevaluehead,ontopoftheoutputhiddenstates
layerbeforetheoutputtokens,toestimatethevaluefunctionVπθ. Afterobtainingthevalueestimate
V ,weadoptthegeneralizedadvantageestimator(GAE)[53]toestimatethereturnfunctionRˆ(s)
ϕ
andtheadvantagefunctionAˆπθ ofπ θ. Inaddition,weadoptaCosineAnnealingLRlearningrate
scheduler,withtheinitiallearningrateof1e−5,thefinallearningrateof1e−9,andthemaximum
learning rate step of 25. For all experiments in the gym_cards and alfworld environment, we
setthescalinghyperparameterλ = 0.5,0,2, respectively. Thelearningratedecayhappensafter
everyPPOupdate,whichconsistsof4epochsofgradientupdateswithPPO.Thenumberofdatafor
on-policytrainingandbatchsizeistask-dependent,welistthembelow.
Numberline and Blackjack. For NumberLine and Blackjack, our VLM training curves in
Figure5use4GPUs. Ourimplementationnaturallyenablesdifferentrandomseedsondifferent
GPUs, hence our VLM curves are averaged among 4 seeds. For one PPO update on each GPU,
we collect 512 transitions, with a batch size of 128 per GPU (batch size = 512 in total). The
episodereturnandsuccessrateareaveragedwithNumberLine,Blackjackareaveragedamong200
and1000episodes,respectively. WeaveragedthereturnofBlackjackonmoreepisodesbecause
BlackjackcontainsstochasticwhileNumberLineisadeterministictask.Weadoptthesamenumber
10https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh. We start
fromthellava-v1.6-mistral-7binsteadofthev1.5checkpointinthescript.
22oftransitionsandbatchsizefortheon-policytrainingintheCNN-basedmethodonbothtasks. The
CNN-basedmethodsareaveragedamong4randomseedsaswell.
EZPointsandPoints24. ForEZPointsandPoints24,ourVLMtrainingcurvesinFigure5use
4GPUs. OurimplementationnaturallyenablesdifferentrandomseedsondifferentGPUs,hence
ourVLMcurvesareaveragedamong4seeds. ForonePPOupdateoneachGPU,wecollect1024
transitions,withabatchsizeof128perGPU(batchsize=512intotal). Weuse1024transitions
becausetheepisodesofEZPointsandPoints24usuallyhavelongerhorizonsthanNumberLine
andBlackjack. TheepisodereturnandsuccessrateareaveragedwithEZPointsandPoints24
areaveragedamong200. Weadoptthesamenumberoftransitionsandbatchsizefortheon-policy
trainingintheCNN-basedmethodonbothtasks. TheCNN-basedmethodsareaveragedamong4
randomseedsaswell.
ALFWorld. For the alfworld environment, each run of our VLM training curves in Figure 5
andFigure19areconductedononeGPU,andeachcurveisaveragedamong4seeds. Wedonot
conductmulti-GPUtrainingforalfworldbecausetheon-policysamplingtimehasahugevariance
ondifferentGPUs,whichwilllargelyincreasethesynchronizationtimeacrossdifferentGPUs. For
eachPPOupdate,wecollect1024transitions,andwithabatchsizeof256. Theepisodesuccessrates
areaveragedamong200episodes. Weadoptthesamenumberoftransitionsandbatchsizeforthe
on-policytrainingintheCNN-basedmethodonbothtasks. TheCNN-basedmethodsareaveraged
among4randomseedsaswell.
C.4 AdditionalExperimentalResults
Weprovidesomeadditionalexperimentalresultsontheepisodereturnsonthegym_cardsandthe
task-specifictrainingcurvesforalfworldhere.
EpisodeSuccessRate(%) EpisodeReturn
NL EZP P24 BJ NL EZP P24 BJ
CNN+RL 87.1 0 0 38.8 0.79 -1.02 -1.12 -0.17
GPT4-V 65.5 10.5 0 25.5 -0.59 -1.30 -4.39 -0.44
Gemini 82.5 2.0 0 30.0 0.74 -2.57 -2.68 -0.35
LLaVA-sft 24.8 23.0 2.6 23.1 -2.30 -0.50 -13.52 -0.50
Ours 89.4 50.0 2.3 40.2 0.87 4.46 -11.84 -0.13
Table4: Averageepisodesuccessratesandreturnsofdifferentmethodsongym_cards.ForallRL-based
methods(CNNandourmethod),wereportthebestresultsineachtrainingcurvefromFigure18.
gym_cards/Numberline gym_cards/EZPoints gym_cards/Blackjack
1.0
4.0
-0.1
0.0 2.0
-0.2
Ours
-1.0 CNN+RL 0.0 -0.3
GPT4-V
-2.0 Gemini -2.0 -0.4
-3.0 LLaVA-sft -4.0 -0.5
0 5k 10k 15k 0 5k 10k 15k 0 5k 10k 15k
env steps env steps env steps
Figure18: Episodereturnsofdifferentmethodsongym_cards.AnextendedversionofFigure5containing
episodesuccessratesandreturns.
23Pick Look Clean
30
60 20
15 20
40
10
20 10
5
0 0 0
1k 2k 3k 4k 5k 1k 2k 3k 4k 5k 1k 2k 3k 4k 5k
env steps env steps env steps
Cool Heat Pick2
100 50
40 Ours
80 CNN+RL 40
30
60 GPT4-V 30
20 Gemini
40 LLaVA-sft 20
10 20 10
0 0 0
1k 2k 3k 4k 5k 1k 2k 3k 4k 5k 1k 2k 3k 4k 5k
env steps env steps env steps
Figure19: Episodesuccessratesourmethodondifferenttypesoftasksfromalfworld.Thecurvescome
fromthesamerunasexperimentsfromthealfworldfigureofFigure5.
C.5 FailureExamplesofPoints24
GroundTruthCards: [2,8,5,J]
ErrorType: Failtorecognizeallnumbers
{
"cards":[10,10,5,9],
"formula":"10+",
"thoughts":"'10+'isanincompleteformula,since'10+10-5+9=24',Ishouldappend'10'tothe
currentformula",
"action":"10"
}
Figure20: FailureexampleofPoints24duetoincorrectperception. Inthisexample,theVLMfailsto
recognizethefournumbersfromthecardscorrectlyasitonlyrecognizes2numberscorrectly.Thereasoning
partiscorrect,asitindeedgeneratesaformulathatequals24.
GroundTruthCards: [J,K,5,6]
ErrorType: Failtorecognizeallnumbers
{
"cards":[10,10,5,7],
"formula":"10+10+5+",
"thoughts":"'10+10+5'isanincompleteformula,since'10+10+5+7=24',Ishouldappend'7'to
thecurrentformula",
"action":"7"
}
Figure21: FailureexampleofPoints24duetoincorrectperceptionandreasoning.Inthisexample,the
VLMfailstorecognizethefournumbersfromthecardscorrectlyasitonlyrecognizes3numberscorrectly.The
reasoningpartisalsoincorrect,10+10+5+7doesnotequal24.
24D DetailsonSupervisedFine-tuningData
Inthissection,weprovidedetaileddescriptionsofthesupervisedfine-tuningdataforalltasks.
D.1 NumberLine
IntheNumberLinetask,wecollect20kinstruction-followingexpertdatasamplesusingatasksolver.
SeeexampleswithandwithoutCoTinFigure22and23,respectively.
Inputs:
Youareplayingagamecallednumberline. Youwillseeatargetnumber
andacurrentnumberintheimage. Andyourgoalistomovethecurrent
numberclosertothetargetbychoosingeitheraddingorsubtractingone
tothecurrentnumber. Yourresponseshouldbeavalidjsonfileinthe
followingformat:
{
"currentnumber": "x",
"targetnumber": "x",
"thoughts": {first read out the current and target number, then think
carefullyaboutwhichactiontochoose},
"action": "-"or"+"
}
Responses:
{
"currentnumber": "0",
"targetnumber": "3",
"thoughts": "Thecurrentnumberis0,whichissmallerthanthetargetnumberis3. Tomovethe
currentnumberclosertothetarget,Ishouldincreasethecurrentnumberbychoosing+.",
"action": "+"
}
Figure22: Anexampleofthesupervisedfine-tuningdataforNumberLinewithCoT.
Inputs:
Youareplayingagamecallednumberline. Youwillseeatargetnumber
andacurrentnumberintheimage. Andyourgoalistomovethecurrent
numberclosertothetargetbychoosingeitheraddingorsubtractingone
tothecurrentnumber. Yourresponseshouldbeavalidjsonfileinthe
followingformat:
{
"action": "-"or"+"
}
Responses:
{
"action": "+"
}
Figure23: Anexampleofthesupervisedfine-tuningdataforNumberLinewithoutCoT.
25D.2 Blackjack
In the Blackjack task, we collect 50k instruction-following expert data samples following an
open-sourcedsolver.11 SeeexampleswithandwithoutCoTinFigure24and25,respectively.
Inputs:
Youareablackjackplayer. Youareobservingthecurrentgamestate,you
canchoosebetween['stand','hit']. Yourresponseshouldbeavalid
jsonfileinthefollowingformat:
{
"thoughts": "{firstdescribeyourtotalpointsandthedealer’stotalpoints
thenthinkaboutwhichactiontochoose}",
"action": "stand"or"hit"
}
Responses:
{
"thoughts": "Ihave13pointsandthedealerhas8points. IthinkIshouldhit.",
"action": "hit"
}
Figure24: Anexampleofthesupervisedfine-tuningdataforBlackjackwithCoT.
Inputs:
Youareablackjackplayer. Youareobservingthecurrentgamestate,you
canchoosebetween['stand','hit']. Yourresponseshouldbeavalid
jsonfileinthefollowingformat:
{
"action": "stand"or"hit"
}
Responses:
{
"action": "hit"
}
Figure25: Anexampleofthesupervisedfine-tuningdataforBlackjackwithoutCoT.
11https://www.cs.cmu.edu/~chengwen/blackjack/blackjack.html
26D.3 EZPoints
IntheEZPointstask,wedirectlycollect50kinstruction-followingexpertdatasamplesusingatask
solver. SeeexampleswithandwithoutCoTinFigure26and27,respectively.
Inputs:
Youareanexpertcardgameplayer. Youareobservingtwocardsinthe
image. You are observing the current formula: '5'. You can choose
between['1','2','3','4','5','6','7','8','9','10','+','*',
'=']. Thenumberoroperatoryouchoosewillbeappendedtothecurrent
formula. Notethat'J','Q',and'K'countas'10'. Yourgoalisto
outputaformulathatevaluatesto12,andeachnumbercanonlybeused
once. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"cards": [x,y],
"currentformula": '5',
"thoughts": {Firstcheckwhetherthecurrentformula'z'iscomplete. Ifthecurrentformula'z'is
complete, output '='. Otherwise consider which number or operator should be appended to the
currentformulatomakeitequal12.}
"action": "{number}"or"{operator}"
}
Responses:
{
"cards": [7,5],
"formula": "5",
"thoughts": "'5'isanincompleteformula, since'5+7=12', Ishouldappend'+'tothecurrent
formula","action": "+"
}"
Figure26: Anexampleofthesupervisedfine-tuningdataforEZPointswithCoT.
Inputs:
Youareanexpertcardgameplayer. Youareobservingtwocardsinthe
image. You are observing the current formula: '5'. You can choose
between['1','2','3','4','5','6','7','8','9','10','+','*',
'=']. Thenumberoroperatoryouchoosewillbeappendedtothecurrent
formula. Notethat'J','Q',and'K'countas'10'. Yourgoalisto
outputaformulathatevaluatesto12,andeachnumbercanonlybeused
once. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"action": "{number}"or"{operator}"
}
Responses:
{
"action": "+"
}"
Figure27: Anexampleofthesupervisedfine-tuningdataforBlackjackwithoutCoT.
27D.4 Points24
InthePoints24task,wedirectlycollect50kinstruction-followingexpertdatasamplesusingatask
solver.SeeexampleswithandwithoutCoTinFigure28and29,respectively.Notethatthesupervised
fine-tuningdatahasaslightlydifferentdistributionasthestatesinthePoints24environment,since
theexpertdataalwayshaveaviablesolutionto24,whilenotallstatesfromPoints24hasaviable
solutionto24.
Inputs:
Youareanexpertcardgameplayer. Youareobservingtwocardsinthe
image. You are observing the current formula: '5'. You can choose
between['1','2','3','4','5','6','7','8','9','10','+','*',
'=']. Thenumberoroperatoryouchoosewillbeappendedtothecurrent
formula. Notethat'J','Q',and'K'countas'10'. Yourgoalisto
outputaformulathatevaluatesto12,andeachnumbercanonlybeused
once. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"cards": [x,y],
"currentformula": '(2',
"thoughts": {Firstcheckwhetherthecurrentformula'z'iscomplete. Ifthecurrentformula'z'is
complete, output '='. Otherwise consider which number or operator should be appended to the
currentformulatomakeitequal12.}
"action": "{number}"or"{operator}"
}
Responses:
{
"cards": [1,2,10,1],
"formula": "(2",
"thoughts": "'(2'isanincompleteformula,since'(2+10)*(1+1)=24',Ishouldappend'+'tothe
currentformula",
"action": "+"
}"
Figure28: Anexampleofthesupervisedfine-tuningdataforPoints24withCoT.
Inputs:
Youareanexpertcardgameplayer. Youareobservingtwocardsinthe
image. You are observing the current formula: '5'. You can choose
between['1','2','3','4','5','6','7','8','9','10','+','*',
'=']. Thenumberoroperatoryouchoosewillbeappendedtothecurrent
formula. Notethat'J','Q',and'K'countas'10'. Yourgoalisto
outputaformulathatevaluatesto12,andeachnumbercanonlybeused
once. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"action": "{number}"or"{operator}"
}
Responses:
{
"action": "+"
}"
Figure29: Anexampleofthesupervisedfine-tuningdataforPoints24withoutCoT.
28D.5 ALFWorld
Thedatacollectionprocedureofthealfworldembodiedenvironmentisslightlydifferentthanthe
gym_cards,aswedonothavesolverstogeneratetheinstruction-followingdata,evenwiththeexpert
textaction. Therefore,wetakeastepbackanddirectlyuseourpromptaspresentedinFigure17(or
Figure30)tocollect1kepisodes(around45k)instruction-followingdatafromGPT4-V[43]withthe
CoTreasoningfortheinstruction-followingfine-tuningwithCoT.AsforthecasewithoutCoT,we
directlycleanthecollectedCoTbyremovingtheCoTreasonings. Seeexampleswithandwithout
CoTinFigure30and31,respectively.
Inputs:
Your are an expert in the ALFRED Embodied Environment. You are
also given the following text description of the current scene: ['You
arriveatloc0. Thecabinet1isopen. Onthecabinet1,youseeapan
1,akettle1,awinebottle1,aapple1,astoveknob1,astoveknob2,a
stoveknob3,astoveknob4,aknife1,asaltshaker1,andabread1.'].
Yourtaskistoputacoolmugincabinet. Youradmissibleactionsofthe
currentsituationare: ['gotocountertop1', 'gotocabinet2', 'goto
countertop2','gotostoveburner1','gotodrawer1','gotodrawer
2','gotodrawer3','gotostoveburner2','gotostoveburner3','go
tostoveburner4','gotodrawer4','gotocabinet3','gotocabinet4','gotomicrowave1','go
tocabinet5','gotocabinet6','gotocabinet7','gotosink1','gotosinkbasin1','gotofridge
1','gototoaster1','gotocoffeemachine1','gotocabinet8','gotodrawer5','gotodrawer
6','gotodrawer7','gotodrawer8','gotoshelf1','gotoshelf2','gotocountertop3','go
toshelf3','gotodrawer9','gotogarbagecan1','opencabinet1','closecabinet1','takepan
1fromcabinet1','takekettle1fromcabinet1','takewinebottle1fromcabinet1','takeapple
1fromcabinet1','takestoveknob1fromcabinet1','takestoveknob2fromcabinet1','take
stoveknob3fromcabinet1', 'takestoveknob4fromcabinet1', 'takeknife1fromcabinet1',
'takesaltshaker1fromcabinet1','takebread1fromcabinet1','inventory','look','examine
cabinet1']. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"thoughts": "firstdescribewhatdoyouseeintheimageusingthetextdescription,thencarefully
thinkaboutwhichactiontocompletethetask. ",
"action": "anadmissibleaction"
}
Responses:
{
"thoughts": "Basedonthetextdescription,Iamatloc0withanopencabinet1infrontofme. Items
suchasapan,kettle,winebottle,apple,severalstoveknobs,aknife,saltshaker,andbreadarevisible
onthecabinet1. Tofindacoolmugtoputinthecabinet,Imayneedtofirstlocateamug,which
mightbeinoneofthecabinetsordrawers. Sincethetaskdoesnotinvolveanyitemscurrentlyin
cabinet1,andthemugisnotmentionedinthedescription,mybestcourseofactionwouldbetostart
bysearchingtheothercabinetsordrawersforthemug.",
"action": "gotocabinet2"
}"
Figure30: Anexampleofthesupervisedfine-tuningdataforalfworldwithCoT.
29Inputs:
Your are an expert in the ALFRED Embodied Environment. You are
also given the following text description of the current scene: ['You
arriveatloc0. Thecabinet1isopen. Onthecabinet1,youseeapan
1,akettle1,awinebottle1,aapple1,astoveknob1,astoveknob2,a
stoveknob3,astoveknob4,aknife1,asaltshaker1,andabread1.'].
Yourtaskistoputacoolmugincabinet. Youradmissibleactionsofthe
currentsituationare: ['gotocountertop1', 'gotocabinet2', 'goto
countertop2','gotostoveburner1','gotodrawer1','gotodrawer
2','gotodrawer3','gotostoveburner2','gotostoveburner3','go
tostoveburner4','gotodrawer4','gotocabinet3','gotocabinet4','gotomicrowave1','go
tocabinet5','gotocabinet6','gotocabinet7','gotosink1','gotosinkbasin1','gotofridge
1','gototoaster1','gotocoffeemachine1','gotocabinet8','gotodrawer5','gotodrawer
6','gotodrawer7','gotodrawer8','gotoshelf1','gotoshelf2','gotocountertop3','go
toshelf3','gotodrawer9','gotogarbagecan1','opencabinet1','closecabinet1','takepan
1fromcabinet1','takekettle1fromcabinet1','takewinebottle1fromcabinet1','takeapple
1fromcabinet1','takestoveknob1fromcabinet1','takestoveknob2fromcabinet1','take
stoveknob3fromcabinet1', 'takestoveknob4fromcabinet1', 'takeknife1fromcabinet1',
'takesaltshaker1fromcabinet1','takebread1fromcabinet1','inventory','look','examine
cabinet1']. Yourresponseshouldbeavalidjsonfileinthefollowingformat:
{
"action": "anadmissibleaction"
}
Responses:
{
"action": "gotocabinet2"
}"
Figure31: Anexampleofthesupervisedfine-tuningdataforalfworldwithoutCoT.
30