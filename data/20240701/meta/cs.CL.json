[
    {
        "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
        "authors": "Sukmin YunHaokun LinRusiru ThusharaMohammad Qazim BhatYongxin WangZutao JiangMingkai DengJinhong WangTianhua TaoJunbo LiHaonan LiPreslav NakovTimothy BaldwinZhengzhong LiuEric P. XingXiaodan LiangZhiqiang Shen",
        "links": "http://arxiv.org/abs/2406.20098v1",
        "entry_id": "http://arxiv.org/abs/2406.20098v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20098v1",
        "summary": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.",
        "updated": "2024-06-28 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20098v1"
    },
    {
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "authors": "Xiang LiCristina MataJongwoo ParkKumara KahatapitiyaYoo Sung JangJinghuan ShangKanchana RanasingheRyan BurgertMu CaiYong Jae LeeMichael S. Ryoo",
        "links": "http://arxiv.org/abs/2406.20095v1",
        "entry_id": "http://arxiv.org/abs/2406.20095v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20095v1",
        "summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.",
        "updated": "2024-06-28 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20095v1"
    },
    {
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas",
        "authors": "Xin ChanXiaoyang WangDian YuHaitao MiDong Yu",
        "links": "http://arxiv.org/abs/2406.20094v1",
        "entry_id": "http://arxiv.org/abs/2406.20094v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20094v1",
        "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.",
        "updated": "2024-06-28 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20094v1"
    },
    {
        "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
        "authors": "Tianyi QiuYang ZhangXuchuan HuangJasmine Xinze LiJiaming JiYaodong Yang",
        "links": "http://arxiv.org/abs/2406.20087v1",
        "entry_id": "http://arxiv.org/abs/2406.20087v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20087v1",
        "summary": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
        "updated": "2024-06-28 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20087v1"
    },
    {
        "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
        "authors": "Sheridan FeuchtDavid AtkinsonByron WallaceDavid Bau",
        "links": "http://arxiv.org/abs/2406.20086v1",
        "entry_id": "http://arxiv.org/abs/2406.20086v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20086v1",
        "summary": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.",
        "updated": "2024-06-28 17:54:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20086v1"
    }
]