[
    {
        "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
        "authors": "Sukmin YunHaokun LinRusiru ThusharaMohammad Qazim BhatYongxin WangZutao JiangMingkai DengJinhong WangTianhua TaoJunbo LiHaonan LiPreslav NakovTimothy BaldwinZhengzhong LiuEric P. XingXiaodan LiangZhiqiang Shen",
        "links": "http://arxiv.org/abs/2406.20098v1",
        "entry_id": "http://arxiv.org/abs/2406.20098v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20098v1",
        "summary": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.",
        "updated": "2024-06-28 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20098v1"
    },
    {
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "authors": "Xiang LiCristina MataJongwoo ParkKumara KahatapitiyaYoo Sung JangJinghuan ShangKanchana RanasingheRyan BurgertMu CaiYong Jae LeeMichael S. Ryoo",
        "links": "http://arxiv.org/abs/2406.20095v1",
        "entry_id": "http://arxiv.org/abs/2406.20095v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20095v1",
        "summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.",
        "updated": "2024-06-28 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20095v1"
    },
    {
        "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
        "authors": "Tianyi QiuYang ZhangXuchuan HuangJasmine Xinze LiJiaming JiYaodong Yang",
        "links": "http://arxiv.org/abs/2406.20087v1",
        "entry_id": "http://arxiv.org/abs/2406.20087v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20087v1",
        "summary": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
        "updated": "2024-06-28 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20087v1"
    },
    {
        "title": "AI for Extreme Event Modeling and Understanding: Methodologies and Challenges",
        "authors": "Gustau Camps-VallsMiguel-Ángel Fernández-TorresKai-Hendrik CohrsAdrian HöhlAndrea CastellettiAytac PacalClaire RobinFrancesco MartinuzziIoannis PapoutsisIoannis PrapasJorge Pérez-AracilKatja WeigelMaria Gonzalez-CalabuigMarkus ReichsteinMartin RabelMatteo GiulianiMiguel MahechaOana-Iuliana PopescuOscar J. Pellicer-ValeroSaid OualaSancho Salcedo-SanzSebastian SippelSpyros KondylatosTamara HappéTristan Williams",
        "links": "http://arxiv.org/abs/2406.20080v1",
        "entry_id": "http://arxiv.org/abs/2406.20080v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20080v1",
        "summary": "In recent years, artificial intelligence (AI) has deeply impacted various\nfields, including Earth system sciences. Here, AI improved weather forecasting,\nmodel emulation, parameter estimation, and the prediction of extreme events.\nHowever, the latter comes with specific challenges, such as developing accurate\npredictors from noisy, heterogeneous and limited annotated data. This paper\nreviews how AI is being used to analyze extreme events (like floods, droughts,\nwildfires and heatwaves), highlighting the importance of creating accurate,\ntransparent, and reliable AI models. We discuss the hurdles of dealing with\nlimited data, integrating information in real-time, deploying models, and\nmaking them understandable, all crucial for gaining the trust of stakeholders\nand meeting regulatory needs. We provide an overview of how AI can help\nidentify and explain extreme events more effectively, improving disaster\nresponse and communication. We emphasize the need for collaboration across\ndifferent fields to create AI solutions that are practical, understandable, and\ntrustworthy for analyzing and predicting extreme events. Such collaborative\nefforts aim to enhance disaster readiness and disaster risk reduction.",
        "updated": "2024-06-28 17:45:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20080v1"
    },
    {
        "title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
        "authors": "Anisha GunjalGreg Durrett",
        "links": "http://arxiv.org/abs/2406.20079v1",
        "entry_id": "http://arxiv.org/abs/2406.20079v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20079v1",
        "summary": "Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.",
        "updated": "2024-06-28 17:43:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20079v1"
    }
]