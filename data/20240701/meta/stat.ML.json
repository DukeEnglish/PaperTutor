[
    {
        "title": "Minimax And Adaptive Transfer Learning for Nonparametric Classification under Distributed Differential Privacy Constraints",
        "authors": "Arnab AuddyT. Tony CaiAbhinav Chakraborty",
        "links": "http://arxiv.org/abs/2406.20088v1",
        "entry_id": "http://arxiv.org/abs/2406.20088v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20088v1",
        "summary": "This paper considers minimax and adaptive transfer learning for nonparametric\nclassification under the posterior drift model with distributed differential\nprivacy constraints. Our study is conducted within a heterogeneous framework,\nencompassing diverse sample sizes, varying privacy parameters, and data\nheterogeneity across different servers. We first establish the minimax\nmisclassification rate, precisely characterizing the effects of privacy\nconstraints, source samples, and target samples on classification accuracy. The\nresults reveal interesting phase transition phenomena and highlight the\nintricate trade-offs between preserving privacy and achieving classification\naccuracy. We then develop a data-driven adaptive classifier that achieves the\noptimal rate within a logarithmic factor across a large collection of parameter\nspaces while satisfying the same set of differential privacy constraints.\nSimulation studies and real-world data applications further elucidate the\ntheoretical analysis with numerical results.",
        "updated": "2024-06-28 17:55:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20088v1"
    },
    {
        "title": "Cost-aware Bayesian optimization via the Pandora's Box Gittins index",
        "authors": "Qian XieRaul AstudilloPeter FrazierZiv ScullyAlexander Terenin",
        "links": "http://arxiv.org/abs/2406.20062v1",
        "entry_id": "http://arxiv.org/abs/2406.20062v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20062v1",
        "summary": "Bayesian optimization is a technique for efficiently optimizing unknown\nfunctions in a black-box manner. To handle practical settings where gathering\ndata requires use of finite resources, it is desirable to explicitly\nincorporate function evaluation costs into Bayesian optimization policies. To\nunderstand how to do so, we develop a previously-unexplored connection between\ncost-aware Bayesian optimization and the Pandora's Box problem, a decision\nproblem from economics. The Pandora's Box problem admits a Bayesian-optimal\nsolution based on an expression called the Gittins index, which can be\nreinterpreted as an acquisition function. We study the use of this acquisition\nfunction for cost-aware Bayesian optimization, and demonstrate empirically that\nit performs well, particularly in medium-high dimensions. We further show that\nthis performance carries over to classical Bayesian optimization without\nexplicit evaluation costs. Our work constitutes a first step towards\nintegrating techniques from Gittins index theory into Bayesian optimization.",
        "updated": "2024-06-28 17:20:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20062v1"
    },
    {
        "title": "Electrostatics-based particle sampling and approximate inference",
        "authors": "Yongchao Huang",
        "links": "http://arxiv.org/abs/2406.20044v1",
        "entry_id": "http://arxiv.org/abs/2406.20044v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20044v1",
        "summary": "A new particle-based sampling and approximate inference method, based on\nelectrostatics and Newton mechanics principles, is introduced with theoretical\nground, algorithm design and experimental validation. This method simulates an\ninteracting particle system (IPS) where particles, i.e. the freely-moving\nnegative charges and spatially-fixed positive charges with magnitudes\nproportional to the target distribution, interact with each other via\nattraction and repulsion induced by the resulting electric fields described by\nPoisson's equation. The IPS evolves towards a steady-state where the\ndistribution of negative charges conforms to the target distribution. This\nphysics-inspired method offers deterministic, gradient-free sampling and\ninference, achieving comparable performance as other particle-based and MCMC\nmethods in benchmark tasks of inferring complex densities, Bayesian logistic\nregression and dynamical system identification. A discrete-time, discrete-space\nalgorithmic design, readily extendable to continuous time and space, is\nprovided for usage in more general inference problems occurring in\nprobabilistic machine learning scenarios such as Bayesian inference, generative\nmodelling, and beyond.",
        "updated": "2024-06-28 16:53:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20044v1"
    },
    {
        "title": "The Computational Curse of Big Data for Bayesian Additive Regression Trees: A Hitting Time Analysis",
        "authors": "Yan Shuo TanOmer RonenTheo SaarinenBin Yu",
        "links": "http://arxiv.org/abs/2406.19958v1",
        "entry_id": "http://arxiv.org/abs/2406.19958v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19958v1",
        "summary": "Bayesian Additive Regression Trees (BART) is a popular Bayesian\nnon-parametric regression model that is commonly used in causal inference and\nbeyond. Its strong predictive performance is supported by theoretical\nguarantees that its posterior distribution concentrates around the true\nregression function at optimal rates under various data generative settings and\nfor appropriate prior choices. In this paper, we show that the BART sampler\noften converges slowly, confirming empirical observations by other researchers.\nAssuming discrete covariates, we show that, while the BART posterior\nconcentrates on a set comprising all optimal tree structures (smallest bias and\ncomplexity), the Markov chain's hitting time for this set increases with $n$\n(training sample size), under several common data generative settings. As $n$\nincreases, the approximate BART posterior thus becomes increasingly different\nfrom the exact posterior (for the same number of MCMC samples), contrasting\nwith earlier concentration results on the exact posterior. This contrast is\nhighlighted by our simulations showing worsening frequentist undercoverage for\napproximate posterior intervals and a growing ratio between the MSE of the\napproximate posterior and that obtainable by artificially improving convergence\nvia averaging multiple sampler chains. Finally, based on our theoretical\ninsights, possibilities are discussed to improve the BART sampler convergence\nperformance.",
        "updated": "2024-06-28 14:45:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19958v1"
    },
    {
        "title": "Kolmogorov-Smirnov GAN",
        "authors": "Maciej FalkiewiczNaoya TakeishiAlexandros Kalousis",
        "links": "http://arxiv.org/abs/2406.19948v1",
        "entry_id": "http://arxiv.org/abs/2406.19948v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19948v1",
        "summary": "We propose a novel deep generative model, the Kolmogorov-Smirnov Generative\nAdversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the\nlearning process as a minimization of the Kolmogorov-Smirnov (KS) distance,\ngeneralized to handle multivariate distributions. This distance is calculated\nusing the quantile function, which acts as the critic in the adversarial\ntraining process. We formally demonstrate that minimizing the KS distance leads\nto the trained approximate distribution aligning with the target distribution.\nWe propose an efficient implementation and evaluate its effectiveness through\nexperiments. The results show that KSGAN performs on par with existing\nadversarial methods, exhibiting stability during training, resistance to mode\ndropping and collapse, and tolerance to variations in hyperparameter settings.\nAdditionally, we review the literature on the Generalized KS test and discuss\nthe connections between KSGAN and existing adversarial generative models.",
        "updated": "2024-06-28 14:30:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19948v1"
    }
]