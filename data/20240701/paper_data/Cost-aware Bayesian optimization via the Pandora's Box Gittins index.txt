Cost-aware Bayesian optimization
via the Pandora’s Box Gittins index
QianXie1 RaulAstudillo2 PeterFrazier1 ZivScully1 AlexanderTerenin1
1CornellUniversity 2Caltech
Abstract
Bayesianoptimizationisatechniqueforefficientlyoptimizingunknownfunctions
inablack-boxmanner. Tohandlepracticalsettingswheregatheringdatarequires
useoffiniteresources,itisdesirabletoexplicitlyincorporatefunctionevaluation
costsintoBayesianoptimizationpolicies. Tounderstandhowtodoso,wedevelop
apreviously-unexploredconnectionbetweencost-awareBayesianoptimizationand
thePandora’sBoxproblem,adecisionproblemfromeconomics. ThePandora’s
BoxproblemadmitsaBayesian-optimalsolutionbasedonanexpressioncalledthe
Gittinsindex,whichcanbereinterpretedasanacquisitionfunction. Westudythe
useofthisacquisitionfunctionforcost-awareBayesianoptimization,anddemon-
strateempiricallythatitperformswell,particularlyinmedium-highdimensions.
WefurthershowthatthisperformancecarriesovertoclassicalBayesianoptimiza-
tionwithoutexplicitevaluationcosts. Ourworkconstitutesafirststeptowards
integratingtechniquesfromGittinsindextheoryintoBayesianoptimization.
1 Introduction
Bayesianoptimizationisaframeworkforoptimizingfunctionswhoseevaluationistime-consuming
or expensive. It is widely used for hyperparameter tuning of machine learning algorithms [33],
robotcontrol[26],materialdesign[40],andotherareas. Bayesianoptimizationworksbyforminga
probabilisticmodelfortheobjectivefunction,andthenchooseswheretosampleviaanacquisition
functionthatbalancestheexplore-exploittrade-offsarisingfromuncertaintyinthismodel.
Westudycost-awareBayesianoptimization,whereonemustpayacosttoacquireanothersample
and this cost may vary with where the function is evaluated. Costs are an important factor in
practicalscenarios. Forinstance,inhyperparametertuningusingGPUsrentedfromacloudprovider,
traininganeuralnetworkfortwiceasmanyepochsmaycarrytwicethefinancialcost.
Despite its practical relevance, cost-aware Bayesian optimization is less-studied than standard
Bayesianoptimization,wherebudgetsareframedintermsofthenumberoffunctionevaluationsand
costsarenotconsidered. Existingtheoretically-principledcost-awareapproaches[39,22,24,3,6]
relyonmulti-steplookaheadcomputationsthatarecomputationallyexpensiveandcanbenumerically
brittle,limitingtheirapplicability. Otherapproacheslackatheoreticalfoundationandriskhaving
poorperformanceoncertainproblems. Forexample,oneofthemostpopularcost-awareacquisition
functionsusedinpractice,expectedimprovementperunitcost[33],hasrecentlybeentheoretically
shown by Astudillo et al. [3] to perform arbitrarily-worse than the optimal policy. Thus, in the
cost-awaresetting,thereisaneedfortheoretically-principledandcomputationally-straightforward
acquisitionfunctionswithgoodempiricalperformance.
Inthiswork,wedevelopsuchanapproach. Todoso,weintroduceanovellinkbetweencost-aware
Bayesianoptimizationandadiscrete-spacedecisionproblemfromeconomicscalledthePandora’s
Preprint.Underreview.
Codeavailableat: HTTPS://GITHUB.COM/QIANJANEXIE/PANDORABAYESOPT.
4202
nuJ
82
]GL.sc[
1v26002.6042:viXraBox problem [38, 11, 32, 29]. The Pandora’s Box problem admits an explicit Bayesian-optimal
solution. Weshowhowthissolutioncanbeusedtodevelopanovelacquisitionfunctionclassfor
twocost-awareBayesianoptimizationsettings: (i)expectedbudget-constrainedcost-awareBayesian
optimization,wherethereisaconstraintontheexpectedcostofthesamplestaken,and(ii)cost-per-
samplecost-awareBayesianoptimizationwherethecostsincurredaresubtractedfromtheobjective
functionvalue. Theresultingacquisitionfunctionsarecloselyconnectedtoexpectedimprovement
variants,butincorporatecostsinadifferent,non-multiplicativeway.
Weevaluatetheproposedacquisitionfunction,termedthePandora’sBoxGittinsindex(PBGI),ona
comprehensivesetofexperimentstounderstanditsstrengthsandweaknesses. Onbothsufficiently-
easylow-dimensionalproblemsandtoo-difficulthigh-dimensionalones,performanceiscomparable
tobaselines. Onmostmedium-hardproblemsofmoderatedimension,however,theproposedacqui-
sitionfunctionoutperformbaselines,intheworstcaseapproximatelymatchingtheirperformance.
Surprisingly,wefindthisperformancecarriesovertotheclassicalsettingwithuniformcosts. We
alsodiscusslimitations,includingbehavioronunimodalproblemswherebaselinesarestronger.
The Pandora’s Box Gittins index is a version of the Gittins index [18], a general framework for
derivingoptimalpoliciesforavarietyofbandit-likedecisionproblems[37,12,20]whichiswidely-
usedinqueueingtheoryandrelatedareas[19,1,31]. Ourworkthusopensanovelangleofattackfor
designingacquisitionfunctionsspecializedtospecificpracticalsettingsofinterest.
Contributions. Inthiswork,we(i)connectthePandora’sBoxproblemwithavariantofcost-aware
Bayesianoptimizationoveradiscretesearchspace. Usingthisconnection,we(ii)exploretheuse
of Gittins indices, which are Bayesian-optimal for the Pandora’s Box problem, as an acquisition
functionforgeneralcost-awareBayesianoptimizationwheredataisincorporatedviatheposterior
distribution. We(iii)demonstratetheresultingacquisitionfunctionhasstrongempiricalperformance
onavarietyofproblemsofmoderate-to-highdimension,includingtheheterogeneous-costproblems
itwasdesignedfor,aswellasclassicalcost-unawareproblems.
2 Cost-awareBayesianoptimization
Inblack-boxoptimization,weareinterestedinfindingtheglobaloptimumofanunknown(potentially
stochastic) function f : X R defined on some compact domain, using pointwise function
→
evaluations of f at locations x X that we select sequentially. We are interested in policies
t
∈
achievingasmallsimpleregret—seeGarnett[16],Sec. 10.1—namely
Esupf(x) E max f(x ) (1)
t
x ∈X − 1 ≤t ≤T
wheretheexpectationistakenwithrespecttoallrandomnessinthefunctionandprocedure.Obtaining
anewfunctionevaluationatapointxcarriesacostc(x) R .Weconsidertwosettingsthatintegrate
+
∈
costsintotheproblemindifferentways:
(a) Intheexpectedbudget-constrainedsetting,thereisabudgetB R ,andthealgorithmisnot
+
∈
allowedtoexceedthisbudgetinexpectation.
(b) Inthecost-per-samplesetting,ateachtimethealgorithmmustchoosewhethertopayacostand
obtainanewfunctionevaluation,ortostopandreturnsomepreviously-observedpoint. Inthis
setting,weaddthetotalsumofcostsatterminationtimetotheregret.
Note that the cost function c : X R can be constant, which we term uniform costs. In this
+
→
case,(a)reducestostandardblack-boxoptimizationwithafinitetimehorizon,and(b)reducestoa
variantofstopping-awareBayesianoptimization. Thesearenottheonlypossiblesettings: onecan
alsoconsideralmost-surebudgetconstraintsandothervariants. Sinceweareinterestedprimarily
intheroleofcostsratherthanstoppingtimesinthiswork,wemostlyworkwithbudgetconstraints
throughoutthispaper,butwillusethecost-per-samplesettingasaconceptualframeworkwithwhich
tostudythebudget-constrainedsetting.
2.1 Probabilisticmodelsandacquisitionfunctions
Bayesian optimization algorithms for solving various black-box optimization problems work by
(i)buildingaprobabilisticmodeloff—thatis,aprobabilitydistributionwhichquantifieswhatis
2knownaboutf giventhedatapoints(x ,y )T seensofar,wherey =f(x )arepreviousfunction
t t t=1 t t
evaluations,then(ii)usingthemodelanditsuncertaintytodecidewheretoevaluatetheunknown
functionnext. Foranintroduction,seeFrazier[15]andGarnett[16]. Followingstandardpractice,we
workwithGaussianprocessmodels[30]. Letf y ,..,y betherespectiveposteriordistribution.
1 T
|
Todecidewheretoevaluatef next,oneusesthemodeltodefinea(potentiallyrandom)acquisition
functionα :X R,whichquantifieshowpromisingaparticularlocationisgivenwhatisknown
t
→
sofar. Wethenevaluatef at
x =argmaxα (x), (2)
t+1 t
x X
∈
obtaininganadditionaldatapointthatisusedtoreduceuncertaintyandfurtherimprovethemodel.
2.2 Expectedimprovementperunitcost
Themostpopularcost-awareacquisitionfunctionisexpectedimprovementperunitcost(EIPC)[33],
definedvia
EI (x;max y )
α tEIPC(x)= f |y1,..,yt
c(x)
1 ≤τ ≤t τ EI ψ(x;y)=Emax(0,ψ(x) −y) (3)
wherewehavewrittenαEIPC()intermsofthegeneralexpectedimprovementfunctionEI ,defined
withrespecttosomerant domf· unctionψ : X R,andacomparatorpointy. Withthisψ notation,
→
EIPCcanbeinterpretedastheratiooftheexpectedimprovement,withrespecttothecurrentposterior
andusingthebestpointseensofarasthecomparator,tothecost.
Intheuniform-costcase,wherec(x)=C R forallx,thisacquisitionfunctionreducestotheclas-
+
sicalexpectedimprovement(EI)acqusition∈ function,namelyαEI(x)=EI (x;max y ).
Inturn,expectedimprovementcanbederivedbyconsideringt thesetupwhf e|y r1 e,. t. h,y et unknow1 n≤fτ u≤nct tioτ
n
f israndomlysampledfromthemodel’sprior. Ifweimaginethattheoptimizationprocesscontinues
foronemoretimestepandstopsafterthat,onecanshowthatmaximizingexpectedimprovementis
theoptimalstrategyinexpectation.
SinceEIPCreducestoexpectedimprovementintheuniform-costcasewherec(x)=C,itfollows
thatitchoosesthesamepointswhetherC =0.0001orC =1000000. Thisissomewhatpeculiar:
onemightexpectthatacost-awareacquisitionfunctionshouldbemorerisk-averseifcostsarehigh,
andviceversaiftheyarelow. Thus,EIPCisperhapsbestsuitedtosettingswhereheterogeneityis
themainfactoratplay. However,eventhere,Astudilloetal.[3]showthereexistreasonableproblems
whereEIPCperformsarbitrarilyworsethantheoptimalpolicyinanapproximation-ratiosense.
In spite of this rather negative theoretical outlook, EIPC has been shown to work well on many
practicalproblems,iscomputationallyefficientandreliable,andisinwidespreaduse. Wetherefore
ask: canonedevelopatechnically-principledandcomputationally-straightforwardalternativewith
at-least-comparableempiricalperformance?
3 ThePandora’sBoxGittinsindexforBayesianoptimization
Todevelopacost-awareacquisitionfunction,westudyasimplifieddecisionproblemthatcaptures
keydifficultiesofthemainproblembutistractableenoughtoyieldanalyticinsights. Ananalogous
strategyisusedclassicallytoderiveexpectedimprovement,byexactlysolvingasimplifiedone-step
decision problem. We study a different simplified decision problem, which can also be solved
exactly,butwherethesimplificationisspatialratherthantemporalinnature. Specifically,weconnect
Bayesian optimization with the Pandora’s Box problem from economics. To do so, we describe
Pandora’sBoxinSection3.1anditssolutioninSection3.2,showingalongthewayhowtheseideas
canbereinterpretedfromtheviewofBayesianoptimization. WeillustratethisinFigure1. Then,in
Section3.3,weusePandora’sBoxtoderiveanovelclassofcost-awareacquisitionfunctions.
3.1 ThePandora’sBoxproblem
The Pandora’s Box problem [38, 18] is a sequential decision-making problem. It begins with a
finite set of boxes, which we collect into a set and label X = 1,..,N . Each box has a hidden
{ }
reward,denotedbyf(x),andaninspectioncost,denotedbyc(x). Therewardsaregivenbymutually
independentrandomvariableswhosedistributionsareknownandvarybetweendifferentboxes.
3Pandora’sBox BayesianOptimization
addcorrelationsand f(x)
f(x)
takecontinuumlimit
c=2 c=1 c(x)
α⋆: Bayesian-optimal incorporatef |y 1,..,y t α tPBGI: acquisition function
acquisition function proposed in this work
Figure1: Anillustrationofthiswork’skeyidea. Weviewcost-awareBayesianoptimizationasan
extensionofthePandora’sBoxproblem,andderivethecost-awareacquisitionfunctionαPBGIby
t
incorporatingtheposteriorintotheBayesian-optimalPandora’sBoxacquisitionfunctionα⋆.
Thedecision-makingprocessstartswithasetofclosedboxes,andproceedsindiscretetimesteps. At
timet,onecanchoosetodooneoftwothings:
1. Openaboxx . Thisincurscostc(x ),butrevealstheexactvaluef(x )oftherewardinsidethe
t t t
box,whichisdrawnusingthebox’srespectiverewarddistribution.
2. Stopopeningnewboxes,andtaketherewardfromthebestopenedbox. Thisendsthedecision-
makingprocess,andyieldsaterminalrewardequaltothemaximumvalueamongtheboxes
openedsofar,withtheconventionthatatleastoneboxmustbeopened.
Thepolicy’sgoalistomaximizetheexpectednetutility,whichistherewardofthebestopenbox
minustheexpectedtotalcostsofallboxesopenedsofar,andiswritten
T
(cid:88)
E max f(x ) E c(x ) (4)
t t
1 t T −
≤ ≤ t=1
whereT isarandomvariablethatdenotesthenumberofopenedboxes,indicatingthatthepolicy
terminatesattimeT +1.
Ifwesubtracttheobjective(4)fromEsup f(x),whichisconstantwithrespecttothepolicy,
x X
weobtainthesumofthesimpleregretobjec∈tivedefinedinSection2andexpectedtotalcosts. The
Pandora’sBoxproblemisthereforeequivalenttoaspecialcaseofcost-awareblack-boxoptimization,
specificallythecost-per-samplevariantofSection2,where(a)thedomainX isafiniteset,and(b)
theobjectivefunctionf israndom,withindependentf(x)andf(x)forx=x. Wewillreturnto
′ ′
̸
thispointinthesequel,butfirststudythePandora’sBoxprobleminmoredetail.
3.2 OptimallysolvingPandora’sBox
The Pandora’s Box problem gives rise to an explore-exploit tradeoff: a policy must balance the
opportunitygainedfromlearninghevalueoftherewardcontainedinsidetheboxwiththecostof
opening it. Since the reward distributions are known, this tradeoff is captured within a Markov
decisionprocess(MDP).BygeneralMDPtheory,thereexistsanoptimalpolicydescribingwhich
box,ifany,oneshouldopenforagivenconfiguration—wecallsuchapolicyBayesian-optimal.
ThisMDPcanbesolvedexplicitly,witharemarkablysimplesolution,firstderivedbyWeitzman[38].
Westartbyassociatingwitheachboxx X anumberα⋆(x)knownastheGittinsindex[18].Define
∈
α⋆(x)=g whereg solves EI (x;g)=c(x) (5)
f
whereEI (x;y),previouslydefinedin(3)ofSection2,istheexpectedimprovementofxrelativeto
f
y—thesameexpressionwhichappearedintheexpectedimprovementacquisitionfunctionvariants
αEI and αEIPC. Note that, unlike in those cases, α⋆ is not time-dependent due to the lack of
t t
correlationsorconditioning. SinceEI (x;g)isstrictlydecreasinging,(5)admitsauniquesolution
f
foreveryvalueofc(x).
Tounderstandwhatα⋆(x)represents,considerasingleclosedboxx,andsupposethereisasecond,
openboxwithrewardf . Isopeningboxxbetterthantakingtherewardf fromtheopenbox? This
∗ ∗
amountstowhethertheexpectedimprovementfromopeningxbalancesouttheopeningcostc(x):
onecanshowthatopeningxisbetterifandonlyifEI (x;f )>c(x). Thevalueα⋆(x)tellsushow
f ∗
4PriorDistribution CostFunction LogRegret
1 10
100
0 5
10 1
−
1 0
−
5 0 5 5 0 5 0 10 20 30 40
− −
Expectedimprovementperunitcost Pandora’sBoxGittinsindex
Figure 2: A Bayesian optimization problem with heterogeneous costs on which EIPC has poor
performance,inspiredbyAstudilloetal.[3],SectionA.ThedomainisX =[ 500,500],whichwe
−
visualizeonthesubinterval[ 5,5]. Left: illustrationofthenon-uniformpriorvariance,whichis
−
givenbyaMatérn-5/2kernelscaledbyanarrowbumpfunction. Center: thecostfunction,whichisa
narrowbump-shapedfunction. Right: regretcurvesforEIPCandPBGI.
largedoesthealternativerewardf needtobe,forstoppingandtakingittobeatleastasgoodas
∗
openingboxx—akindoffairpricewhichmakesdifferentboxesdirectlycomparabletooneanother.
Ifwedecidewhichboxtoopenviatheaforementionedfairprices,weobtaintheGittinsindexpolicy,
which proceeds as follows. At each time t, let f = max f(x ) be the maximum reward
t∗ 1 τ t τ
amongallopenboxes,andletx betheboxofmaximumGittin≤si≤ndexvalueα⋆(x)amongunopened
∗t
boxes,withtiesbrokenaccordingtosomearbitraryordering. Withthisnotation,usingatie-breaking
rulethatstopsasearlyaspossible—butnotingthatothertie-breakingrules,includingstoppingas
lateaspossible,orstoppingwithsomeprobability,arealsovalid—weget:
• Iff <α⋆(x ),thepolicyopensboxx .
t∗ ∗t ∗
• Iff α⋆(x ),thepolicystopsandreceivesterminalrewardf .
t∗
≥
∗t t∗
Itturnsoutthatopeningboxesaccordingtotheorderdeterminedbytheirfairprices,inthesense
above,isnotonlyagoodidea,butisoutrightBayesian-optimal. Westatethisformallyasfollows.
Theorem1(Weitzman[38]). LetX beafiniteset,letf :X Rbeafinite-meanrandomfunction
forwhichf(x)isindependentoff(x)forx=x,andletc→ :X R ,withoutlossofgenerality,
′ ′ +
̸ →
bedeterministic. Then,forthecost-per-sampleproblem,thepolicydefinedbymaximizingtheGittins
indexacquisitionfunctionα⋆withitsassociatedstoppingruleisBayesian-optimal.
In the language of Bayesian optimization, this means that not only is there an explicit Bayesian-
optimalpolicyforthePandora’sBoxsetting,butthispolicyalsotakestheformofmaximizingan
acquisitionfunction. Thisgivesanexplicitsolutionforthecost-per-samplesetting,therebyshowing
Pandora’s Box fits our original goal of finding a simplified decision problem that sheds insights
oncost-awareBayesianoptimization. Foranalternativeproof,seeKleinbergetal.[23],Theorem
1. Using Lagrangian relaxation, we show that the obtained solution carries over to the expected
budget-constrainedsetting.
Theorem2. Considertheexpectedbudget-constrainedproblem,withtheassumptionsofTheorem1.
(cid:80)
Assumetheproblemisfeasibleandtheconstraintisactive,namelymin c(x)<B < c(x).
x X x X
Thenthereexistsaλ > 0andatie-breakingrulesuchthatthepolic∈ydefinedbymaximi∈zingthe
Gittinsindexacquisitionfunctionα⋆(),definedusingcostsλc(x),isBayesian-optimal.
·
AproofisgiveninAppendixB.ThisresultextendsaspecialcaseofAminianetal.[2],Theorem1.
Comparedtothatwork,weconsideronlyPandora’sBox,butallowgeneralrewarddistributions—
includingthosewithinfinitesupport,suchasGaussianrewards. Theoptimalλdependsonthebudget
constraintBimplicitlyviaaconvexoptimizationproblemgiveninAppendixB.Inbudget-constrained
problems,wethereforeviewλasahyperparameter,whichcontrolsthedegreetowhichthealgorithm
isrisk-aversevs. risk-seeking—preciselywhatwearguedwasmissingfromEIPCinSection2.
3.3 Anacquisitionfunctionclassforcost-awareBayesianoptimization
Toadaptα⋆totheBayesianoptimizationsetting,weneedtohandletwodifferences: (i)X doesnot
needtobediscrete,and(ii)ageneralprobabilisticmodelisusedforf. SinceTheorem1ostensibly
requiresf(x)tobeindependentoff(x)forallx=x,thekeyquestionishowtoincorporatedata
′ ′
̸
5Expected PBGI PBGI LogRegret
Improvement λ=100 λ=10 −5 5 100 λ=10−2
4 ·
λ=10−3
2
λ=10−4
0 100
λ=10−5
3 0 3 3 0 3 3 0 3 0 50 100 150 200 Dynamicλ
− − −
Mean CumulativeCost
Figure3: Left: contourplotsshowinghowEI(left)andPBGI(center-left,center-right)dependon
theposteriormeanandstandarddeviationatagivenpoint(lightercolorsindicatehighervalues). We
seethatPBGIvalueshighstandarddeviationmorethanEI.Right: PBGIperformanceacrossvalues
ofλ. Weseethatlargeλ-valuesdecreaseregretsooner,buteventuallyloseouttosmallerλ-values.
andspatialcorrelationsintoα⋆. Weproposetodosointhesimplestandmostobviousway: namely,
ateachtimet,weplugtheposteriordistributionf y ,..,y inplaceoff. Thisyieldsthreevariants,
1 t
|
dependingontheprecisecost-awaresettingoneisinterestedin:
1. Budget-constrained: definePandora’sBoxGittinsindex(PBGI)acquisitionfunction
αPBGI(x)=g whereg solves EI (x;g)=λc(x) (6)
f |y1,..,yt
andλisahyperparameterthatshouldbetunedtomatchtheevaluationbudget.
2. Cost-per-sample: we can directly apply αPBGI in this setting as well, but now λ is instead
interpreted as unit-conversion factor which ensures costs and rewards have the same units,
andthePandora’sBoxstoppingruleisusedfordecidingwhentoterminatetheoptimization
procedureandreturnthebestobservedvalue.
3. Cost-awareanytime—thatis, withoutanexplicitbudgetorcost-basedstoppingrule: define
the Pandora’s Box Gittins index with dynamic decay αPBGI-D(x) analogously to αPBGI(x),
butwhereλisreplacedwithatime-dependentλ setusingthePandora’sBoxstoppingrule.
t
Specifically,firstsetλ toaninitialvalue,thenatalltimesτ wherethePandora’sBoxstopping
1
ruletriggers,setλ = βλ ,whereβ < 1isthedecayparameter,otherwisesetλ = λ .
τ+1 τ t+1 t
Theadvantageofthisvariantisthatonecanpotentiallyavoidtuningλ.
Tounderstandthisacquisitionfunctionclass,onecanthinkofitviathefollowingapproximation: for
thegeneralcost-awareBayesianoptimizationproblem,we(a)correctlyincorporateobserveddata
intothepriortoobtaintheposterior,butthen(b)picknewsamplesaccordingtotherulethatwould
havebeenBayesian-optimaliftheposteriorhadnocorrelations. Saiddifferently,αPBGIarisesfrom
exactlysolvingasimplifieddynamicprogram,wherethesimplificationisofaspatialnature,rather
thantheusualtemporallookahead. Onecanthereforeexpectthisacquisitionfunctiontoworkbestin
situationswherecorrelationsarenotthedecisivefactorfordeterminingperformance.
Inwhatproblemsdoesthishappen? Instationarykernels, correlationsencodelocaldependence.
Therefore,onecanexpectαPBGI tobeapproximately-optimalinsettingswherethekeydecisions
involveschoosingbetweendifferentfar-awaydatapoints. Onecanintuitivelyexpectthistooccur
moreofteninhigh-dimensionalproblems,wherethevolumeofthesearchspaceislargeandmost
pointsarefarawayfromeachother. Wewillexaminethispointempiricallyinthesequel.
Computation. TocomputeαPBGI efficiently,notethaty EI (x;y)ismonotone. Asaresult,
ψ
(cid:55)→
the value g can be computed efficiently via bisection search. In Appendix B.2, we show that (i)
its gradient can be computed straightforwardly via an explicit analytical expression without any
additionaloptimization,and(ii)theresultingcomputationalcostsaremuchclosertothoseofexpected
improvementthanthoseofexpensivemulti-step-lookahead-basedapproaches.
Qualitativebehaviorandcomparisons. Comparedtocost-unawareacquisitionfunctionssuchas
expectedimprovement,thePandora’sBoxGittinsindexcanbemorerisk-averseifcostsarelargeor
morerisk-seekingifcostsaresmall. Inheterogeneous-costbudget-constrainedsettings,thistradeoff
is mediated by λ, and the obtained decisions can differ significantly from those of widely-used
baselinessuchasexpectedimprovementperunitcost. Inparticular,PBGIcanmakequalitatively
differentdecisionsonproblemswherethereisahigh-variancepointwithalargecost,amongaset
6
.veD
.dtSd=8 d=16 d=32
5 100 7 100 9 100
· · ·
100 100 8 100
·
0 50 100 150 200 0 100 200 300 400 0 200 400 600 800
5 100 7 100 101
· ·
2 100 2 100 3 100
· · ·
0 100 200 300 400 0 200 400 600 800 0 200 400 600 800
CumulativeCost
EI TS UCB KG MSEI
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure4: Bayesianregretcurves,shownusingmedians,aswellasquartilestoindicateexperiment
variability. Weseeinthecost-awaresettingthatbothPBGIvariantsexhibitcomparableperformance
tobaselinesford=8,anddecisivelyoutperformbaselinesind=16andd=32. Thisbehavioris
roughly-mirroredintheuniform-costsetting,withtwonotabledistinctions: (a)UCBalsoexhibits
strongperformanceford=16matchingPBGIandPBGI-D,and(b)allmethodsperformcomparably
torandomsearchford=32underuniformcosts.
ofmanylow-variancelow-costpoints. InFigure2,weadapttheconstructionofAstudilloetal.[3],
SectionAintoaone-dimensionalBayesianoptimizationproblemwithanon-stationaryprior,and
observethatEIPCindeedhassubstantiallyworseperformancethanPBGI.
ThePBGIacquisitionfunctionsdependsonf y ,..,y throughitsmeanandstandarddeviation
1 t
|
ateachpoint. WeplotthisinFigure3. ThisshowsforlargeλthatPBGIcanresembleexpected
improvement,whereasforsmallλitisnearlylinear,similartotheupperconfidencebound(UCB)
acquisitionfunctionwhosedependenceisexactlylinear. Forsmallλ,onecanthusviewPBGIas
givingawaytoautomaticallytuneUCB’sconfidenceparameterinacarefulwaydependingonc(x).
4 Experiments
WenowempiricallyevaluatetheGittins-index-basedacquisitionfunctiononcost-awareproblems.
Wealsoevaluateonthesameproblemswithaspatially-constantcostfunction,asettingweterm
uniformcosts—thisfacilitatescomparisonswithclassical,cost-unawarebaselines. Inbothcases,
mirroringpracticalsettings,weworkwithadeterministic,algorithm-independentevaluationbudget.
We implement all methods in BoTorch [4] using Matérn Gaussian processes with smoothness
ν =5/2andlengthscaleκ=10 1. Toensurethatourresultsarenotsensitivetotheseandother
−
hyperparameterchoices,allexperimentswererepeatedwithalternativesgiveninAppendixD.Each
experimentwasrepeatedfor16seedstoassessvariability. ExperimentaldetailsareinAppendixC.
WeevaluatebothPBGIvariantsofSection3.3,namelyαPBGI withλ=10 4,andαPBGI-D with
−
β =1/2. Toensurethatperformancedifferencesarenotprimarilyduetotuning,wedeliberatelyuse
thesameλ-and-β-valuesonallproblems,eventhoughper-problemtuningcouldbeadvantageous.
Forcost-awareproblems,wecomparewithexpectedimprovementperunitcost(EIPC)andbudgeted
multi-step expected improvement (BMSEI), which was proposed by Astudillo et al. [3] and has
state-of-the-artcost-awareperformance. Tounderstandwhathappensifwesimplyignorethecost
function,wealsocompareagainstordinary(thatis,uniform-cost)expectedimprovement(EIPC-U).
For uniform-cost (that is, cost-unaware) problems, we compare with expected improvement (EI),
Thompsonsampling(TS),upperconfidencebound(UCB)[34],knowledgegradient(KG)[14],and
multi-stepexpectedimprovement(MSEI)[22]. Thesewerechosenbecausetheyarestandard,and
becauseacquisitionfunctionoptimizationsucceedsforthemonourproblems,reducingconfounding.
7
tergeRgoL
Uniform-cost
Cost-awareAckley Levy Rosenbrock
102
100 105
10 1 101 104
− 103
0 40 80 120 160 0 40 80 120 160 0 40 80 120 160
4 100 102
·
105
100
101 104
3 10 1
−
·
0 100 200 300 400 0 100 200 300 400 0 100 200 300 400
CumulativeCost
EI TS UCB KG MSEI
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure5: Syntheticbenchmarkregretcurves,shownusingmedians,aswellasquartilestoassess
variability. Allobjectivefunctionsaredefinedwithdimensiond = 16. Weseeinthecost-aware
settingthatPBGIandPBGI-Dperformstrongestontheheavily-multimodalAckleyfunction,match-
ingthenon-myopicBMSEIbaseline. OntheLevyfunction,PBGI-Dinsteadmatches,andforsome
costbudgetsoutperforms,theEIPCbaseline,significantlyoutperformingBMSEI.Incontrast,onthe
unimodalRosenbrockfunction,PBGIandPBGI-Donlyperformbestforsmallcostbudgets,and
eventuallyendupmatchingBMSEI,andforlargecostbudgets,endupoutperformedbyEIPC,whose
myopicbehaviortakesadvantageofunimodality. Uniform-costresultsaresimilar: PBGIperforms
wellonAckleyandLevy,butisoutperformedbymostbaselinesandPBGI-DonRosenbrock.
4.1 Bayesianregret
Forourfirstexperiment,weexaminehowwelltheproposedacquisitionfunctionsperformonrandom
functionssampledfromtheprior. Toquantifytheeffectofproblemdifficulty,wevarythedimension
ofthedomainX =[0,1]d,andconsiderd 8,16,32 . Results,intermsofempiricalregretcurves
∈{ }
andtheirassociatedquartiles,areshowninFigure4. Additionalresultsford=4,whichshowboth
PBGIvariantsandallbaselinesachievingsimilarperformance,areinAppendixC.
Inthelow-dimensionalcaseofd=8,mostuniform-costandcost-awareapproachesachievesimilar
performance. Onceweincreasedimensiontod=16,weseebiggerdifferences: here,bothPBGI
variantsachieveamodestimprovementcomparedtoexpectedimprovementperunitcost. Strikingly,
bothPBGIvariantsarealsocompetitiveintheuniform-costsetting—inspiteofbeingdesignedfor
cost-awareproblems. Thiscanbeexplainedviathecurseofdimensionality: asdimensionincreases,
theproblembeginstolookmoreliketheuncorrelatedPandora’sBoxproblemwhereusingGittins
indexisBayesian-optimal. Eventually,however,theproblembecomestoodifficultformeaningful
progresstobemadewithinourcomputationalbudget,asseenfortheuniform-costproblemwith
d=32,wherenomethodoutperformsrandomsearch.
4.2 Syntheticbenchmarks
Next,weconsiderstandardsyntheticglobaloptimizationbenchmarkfunctions. Torepresentavariety
ofgeometricproperties,weexaminetheAckley,Levy,andRosenbrockfunctions. Avisualizatonof
thetwo-dimensionalversionsofthesefunctionsisgiveninAppendixA.
Figure 5 presents results for d = 16. Additional results for d = 4,8 showing that PBGI and all
baselines perform similar, are in Appendix C. We see that the behavior of different acquisition
functionsvariesaccordingtothethefunction.OntheAckleyfunction,PBGIandPBGI-Doutperform
mostbaselines,exceptforthenon-myopicBMSEIpolicyinthecost-awaresetting. Incontrast,on
theLevyfunction,PBGI-DonlyoutperformstheEIPCbaselineonsmall-enoughcosthorizons,and
PBGIisworsethanPBGI-Doncost-awareproblems: thesamealsoholdsfortheBMSEIbaseline,
indicatingthatusingmulti-steplookaheadactuallyreducesperformancehere—wewillreturntothis
8
tergeRgoL
Uniform-cost
Cost-awarePestControl LunarLander RobotPushing
14 300 111/4
− −
15 200 111/2
− −
16 100 113/4
− −
17 0 12
− −
0 25 50 75 100 0 50 100 150 200 0 50 100 150 200
300
15 11
− 200 −
16 100 111/2
− −
17 0 12
− −
0 200 400 600 800 0 1600 3200 0 200 400 600 800
CumulativeCost
EI TS UCB KG MSEI
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure6: Empiricalbenchmarkregretcurves,shownusingmedians,aswellasquartilestoshow
variability. We see in both the cost-aware and uniform-cost settings that PBGI exhibits stronger
performanceonthePestControlandLunarLanderproblems,whilePBGI-DtogetherwiththeEI
andEIPCbaselinesperformlystronglyontheRobotPushingproblem. Notethatinthecost-aware
variantofRobotPushing,thenon-myopicBMSEIbaselineperformspoorly,potentiallymirroringthe
behaviorpreviouslyseenontheunimodalRosenbrockfunctioninFigure5.
momentarilyinthecontextoftheRosenbrockfunction. WeconcludethatPBGIcaninprincipleoffer
strongerperformancethanPBGI-D,aslongasλisnot-too-suboptimal,whilePBGI-Dtendstobe
less-performantbutismorerobusttothishyperparameterchoice.
Wealsoexamineperformanceontheunimodal,banana-shapedRosenbrock-function. Here,both
expectedimprovementvariantsperformthestrongest,matchingPBGI-Dandmulti-steplookahead
baselines, and outperforming PBGI.This can intuitively be explained bythe one-step optimality
of expected improvement, which better-exploits the unimodal objective, while PBGI and multi-
step-basedacquisitionfunctionsaremoreconservative. WeconcludethatPBGI-Dmaybeabetter
choiceinsettingswherethereisapotentialmismatchbetweentheobjectiveandthepriorintermsof
unimodality.
4.3 Empiricalobjectives
Finally,webenchmarkPGBIpoliciesonthreeempiricalglobaloptimizationproblemsmotivated
by applied challenges: Pest Control where d = 25 [28], Lunar Lander where d = 12 [13], and
Robot Pushing where d = 14 [36]. Detailed descriptions of these problems and associated cost
functionsareinAppendixC.Notethat, forLunarLanderandRobotPushing, thecostfunctions
usedarenotautomatically-differentiable. Toavoidthischallengeandillustratehowouracquisition
functioncanbeusedwhenthecostfunctionisunknown,weapplyunknown-costPBGIandbaseline
variants,wherethecostsaremodeledusingasecondindependentlog-Gaussianprocess: detailson
thisunknown-costPBGIvariant,includingitsanalyticform,aregiveninAppendixB.3.
FromFigure6,weseethatthePBGIoutperformsbaselinesonPestControlandLunarLander,inboth
thecost-awareanduniform-costsettings.Ontheotherhand,PBGIperformspoorlyonRobotPushing,
whereinsteadexpectedimprovementandPBGI-Dperformbest,andthenon-myopicBMSEIbaseline
performs poorly. This mirrors behavior previously seen on the unimodal Rosenbrock function,
fromwhichwesuspectunimodality-likebehaviormaybeatplayhereaswell. Notealsothatthe
performancegapbetweenPBGIandUCBissubstantiallybiggerherethanontheBayesianregretor
syntheticproblems:thismaybeinpartbecausewetuneUCBusingthescheduleofSrinivasetal.[34],
whichisexplicitlydesignedfortheBayesianregretsetting,andmaybeless-idealforotherobjectives.
Incomparison,PBGI’stuningworksreasonablywellonallthreeproblemclassessimultaneously.
9
eulaVdevresbOtseB
Uniform-cost
Cost-aware5 Conclusion
Inthispaper,weintroducedanewacquisitionfunctionclassforcost-awareBayesianoptimization,
thePandora’sBoxGittinsindex,basedonanunexploredconnectionbetweenBayesianoptimization
andthePandora’sBoxproblemfromeconomics. Weobservedpromisingperformancefromtwo
variantsofthisacquisitionfunctionclassonbothcost-awareproblemswhicharethefocusofthis
work,and,additionally,onclassicaluniform-costproblems. Performancegainstendedtobelargest
onhigher-dimensionalandmulti-modalproblems. Ourworkconstitutesafirststeptowardintegrating
ideas from Gittins index theory, including insights from generalizations of Pandora’s Box, into
Bayesianoptimization.
Acknowledgments
WethankJamesT.Wilsonforhissuggestions,whichhelpedusimprovethepaper’spresentation. AT
isgratefultoAnastasiosAngelopoulosforhostinghimatUCBerkeleyonNovember17th,2022:
thoughthevisitandplannedtalkthatdayhadtobecancelledlast-minuteduetolaborstrikes,the
cancellationultimatelyresultedinanunexpectedchainofeventsthat,monthslater,ledtoATand
ZSexchangingideasaboutGaussianprocessesandGittinsindextheory,whichultimatelyledtothis
work. PFwaspartiallysupportedbyAFSORFA9550-20-1-0351. ZSwassupportedbytheNSF
undergrantnumbersCMMI-2307008,DMS-2023528,andDMS-2022448. ATwassupportedby
CornellUniversity,jointlyviatheCenterforDataScienceforEnterpriseandSociety,theCollegeof
Engineering,andtheAnnS.BowersCollegeofComputingandInformationScience.
References
[1] S.Aalto,U.Ayesta,andR.Righter.OntheGittinsIndexintheM/G/1Queue.Queueing
Systems,2009.Citedonpage2.
[2] M.R.Aminian,V.Manshadi,andR.Niazadeh.Markoviansearchwithsociallyawarecon-
straints.ManagementScience,2024.Citedonpages5,21.
[3] R.Astudillo,D.Jiang,M.Balandat,E.Bakshy,andP.Frazier.Multi-stepbudgetedbayesian
optimization with unknown evaluation costs. Advances in Neural Information Processing
Systems,2021.Citedonpages1,3,5,7,13,15,24,26.
[4] M. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy.
BoTorch:AframeworkforefficientMonte-CarloBayesianoptimization.AdvancesinNeural
InformationProcessingSystems,2020.Citedonpages7,24.
[5] T. Bas¸ar and P. Bernhard. H -optimal control and related minimax design problems: a
∞
dynamicgameapproach.Springer,2008.Citedonpage23.
[6] S.Belakaria,J.R.Doppa,N.Fusi,andR.Sheth.Bayesianoptimizationoveriterativelearners
withstructuredresponses:Abudget-awareplanningapproach.InArtificialIntelligenceand
Statistics,2023.Citedonpage1.
[7] D.P.Bertsekas.NonlinearProgramming.AthenaScientific,1999.Citedonpage23.
[8] J.F.BonnansandA.Shapiro.Perturbationanalysisofoptimizationproblems.Springer,2013.
Citedonpage23.
[9] S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
Citedonpage22.
[10] J.M.Danskin.Thetheoryofmax-minanditsapplicationtoweaponsallocationproblems.
Springer,1967.Citedonpage23.
[11] L.Doval.WhetherorNottoOpenPandora’sBox.JournalofEconomicTheory,2018.Cited
onpage2.
[12] I. Dumitriu, P. Tetali, and P. Winkler. On Playing Golf with Two Balls. SIAM Journal on
DiscreteMathematics,2003.Citedonpage2.
[13] D.Eriksson,M.Pearce,J.Gardner,R.D.Turner,andM.Poloczek.Scalableglobaloptimiza-
tionvialocalBayesianoptimization.AdvancesinNeuralInformationProcessingSystems,
2019.Citedonpages9,25.
[14] P.Frazier,W.Powell,andS.Dayanik.Theknowledge-gradientpolicyforcorrelatednormal
beliefs.INFORMSJournalonComputing,2009.Citedonpage7.
10[15] P.I.Frazier.BayesianOptimization.InRecentAdvancesinOptimizationandModelingof
ContemporaryProblems.2018.Citedonpage3.
[16] R.Garnett.BayesianOptimization.2023.Citedonpages2,3.
[17] J.C.Gittins.Banditprocessesanddynamicallocationindices.JournaloftheRoyalStatistical
SocietySeriesB:StatisticalMethodology,1979.Citedonpage14.
[18] J. C. Gittins, K. D. Glazebrook, and R. R. Weber. Multi-Armed Bandit Allocation Indices.
Wiley,2011.Citedonpages2–4.
[19] K.D.GlazebrookandJ.Niño-Mora.ParallelSchedulingofMulticlassM/M/mQueues:Ap-
proximateandHeavy-TrafficOptimizationofAchievablePerformance.OperationsResearch,
2001.Citedonpage2.
[20] A.Gupta,H.Jiang,Z.Scully,andS.Singla.TheMarkovianPriceofInformation.InInteger
ProgrammingandCombinatorialOptimization,2019.Citedonpage2.
[21] J. M. Hernández-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive entropy search
for efficient global optimization of black-box functions. Advances in Neural Information
ProcessingSystems,2014.Citedonpage24.
[22] S. Jiang, D. Jiang, M. Balandat, B. Karrer, J. Gardner, and R. Garnett. Efficient nonmy-
opic bayesian optimization via one-shot multi-step trees. Advances in Neural Information
ProcessingSystems,2020.Citedonpages1,7,24.
[23] R.Kleinberg,B.Waggoner,andE.G.Weyl.Descendingpriceoptimallycoordinatessearch.
InEconomicsandComputation,2016.Citedonpages5,18,19.
[24] E.H.Lee,D.Eriksson,V.Perrone,andM.Seeger.Anonmyopicapproachtocost-constrained
Bayesianoptimization.InUncertaintyinArtificialIntelligence,2021.Citedonpage1.
[25] Y.L.Li,T.G.Rudner,andA.G.Wilson.AstudyofBayesianneuralnetworksurrogatesfor
Bayesianoptimization.InInternationalConferenceonLearningRepresentations,2024.Cited
onpage25.
[26] R.Martinez-Cantin.Bayesianoptimizationwithadaptivekernelsforrobotcontrol.InInterna-
tionalConferenceonRoboticsandAutomation,2017.Citedonpage1.
[27] P.MilgromandI.Segal.EnvelopeTheoremsforArbitraryChoiceSets.Econometrica,2002.
Citedonpages21,22.
[28] C.Oh,J.Tomczak,E.Gavves,andM.Welling.Combinatorialbayesianoptimizationusing
thegraphcartesianproduct.AdvancesinNeuralInformationProcessingSystems,2019.Cited
onpages9,25.
[29] W.OlszewskiandR.Weber.AMoreGeneralPandoraRule?JournalofEconomicTheory,
2015.Citedonpage2.
[30] C.E.RasmussenandC.K.Williams.GaussianProcessesforMachineLearning.MITPress,
2006.Citedonpages3,26.
[31] Z. Scully, I. Grosof, and M. Harchol-Balter. The Gittins Policy is Nearly Optimal in the
M/G/k under Extremely General Conditions. Measurement and Analysis of Computing
Systems,2020.Citedonpage2.
[32] S.Singla.ThePriceofInformationinCombinatorialOptimization.InSymposiumonDiscrete
Algorithms,2018.Citedonpage2.
[33] J.Snoek,H.Larochelle,andR.P.Adams.PracticalBayesianoptimizationofmachinelearning
algorithms.AdvancesinNeuralInformationProcessingSystems,2012.Citedonpages1,3.
[34] N.Srinivas,A.Krause,S.M.Kakade,andM.Seeger.Gaussianprocessoptimizationinthe
banditsetting:Noregretandexperimentaldesign.InInternationalConferenceonMachine
Learning,2010.Citedonpages7,9,24.
[35] A.Terenin.GaussianProcessesandStatisticalDecision-makinginNon-EuclideanSpaces.
PhDthesis,ImperialCollegeLondon,2022.Citedonpage13.
[36] Z. Wang and S. Jegelka. Max-value entropy search for efficient Bayesian optimization. In
InternationalConferenceonMachineLearning,2017.Citedonpages9,25.
[37] R.R.Weber.OntheGittinsIndexforMultiarmedBandits.TheAnnalsofAppliedProbability,
1992.Citedonpage2.
[38] M.L.Weitzman.Optimalsearchforthebestalternative.Econometrica,1979.Citedonpages2–
5,14,15,18.
11[39] X.YueandR.A.Kontar.Whynon-myopicBayesianoptimizationispromisingandhowfar
shouldwelook-ahead?Astudyviarollout.InArtificialIntelligenceandStatistics,2020.Cited
onpage1.
[40] Y.Zhang,D.W.Apley,andW.Chen.Bayesianoptimizationformaterialsdesignwithmixed
quantitativeandqualitativevariables.ScientificReports,2020.Citedonpage1.
12(a)Ackley (b)Levy (c)Rosenbrock
Figure7: Anillustrationofthetwo-dimensionalAckley,Levy,andRosenbrockfunctions.1 Fromthe
visual,onecanseethatthesefunctionsdifferintermsofmultimodalityandridge-regionswithinthe
optimizationlandscape.
A Illustrations
Hereweprovideasetofadditionalillustrationstoaidunderstandingofourresults.
A.1 Visualizationofsyntheticbenchmarkfunctions
TobetterunderstandhowthebehaviorofBayesianoptimizationalgorithmsonthethreedifferent
syntheticbenchmarkfunctionsmightbeaffectedbytheirgeometricshape,Figure7providesavisual
illustrationoftheirtwo-dimensionalvariants.1 Thisallowsustovisuallyseethemultimodalityofthe
Ackleyfunction,multimodalityandridge-likeregionsintheLevyfunction,andunimodalityofthe
Rosenbrockfunction. Whileweusehigher-dimensionalversionsoftheseinourexperiments,this
illustrationprovidessomeintuitionforwhattheresultingtheoptimizationlandscapemightlooklike,
helpingcontextualizeresults.
A.2 EI/EIPC-Uperformancecounterexample
InSection3.3,weshowedaBayesianoptimizationproblemonwhichexpectedimprovementper
unitcost(EIPC)haspoorperformance. Here,weshowthatthisproblemcanbemodifiedsothat
ordinaryexpectedimprovement(EI),whichignoresthecostfunction,alsohaspoorperformance—a
somewhatobvious,butnonethelessimportantsanitycheckthatwemaketoensurethatcostsplaya
sufficiently-importantroleinproblemsofthisclasstomerittheirconsideration. Thisisshownin
Figure8. Itisnothardtoconstructaless-visualization-friendlyvariantoftheseproblemsonwhich
bothexpectedimprovementperunitcostandordinaryexpectedimprovementperformpoorly,by
consideringcostfunctionswhichareappropriately-weightedsumsofbumpfunctions.
PriorDistribution CostFunction LogRegret
1 10 4 100
·
0 5 100
1 0
−
5 0 5 5 0 5 0 10 20 30 40
− −
Expectedimprovement(cost-unaware) Pandora’sBoxGittinsindex
Figure8:ABayesianoptimizationproblemwithheterogeneouscostsonwhichexpectedimprovement,
which ignores the cost function, has poor performance. Like the EIPC example of Figure 2, the
constructionalsomirrorsAstudilloetal.[3],SectionA.ThedomainisX =[ 500,500],whichwe
−
visualizeonthesubinterval[ 5,5]. Left: illustrationofthenon-uniformpriorvariance,whichis
−
givenbyaMatérn-5/2kernelscaledbyanarrowbumpfunction. Center: thecostfunction,whichisa
narrowbump-shapedfunction. Right: regretcurves.
1ThisvisualoriginallyappearedinTerenin[35],andisreproducedherewithpermission.
13B Theoryandcalculations
Below,weprovideadditionalideastohelpunderstandthePandora’sBoxproblemandacquisition
functionthatresultsfromitsconsiderations.
B.1 AdditionalintuitiononPandora’sBox
Inwhatfollows,wesketchaviewpointfromwhichonecanseethekeyideabehindwhyTheorem1
holds. RatherthanconsideringthefullPandora’sBoxproblemwithageneralsetofopenandclosed
boxes,considerfirstthecasewherethereisexactlyoneclosedandoneopenbox. Toslightlysimplify
notation,letf denotetherandomrewardinsidetheclosedbox,letcdenotethecostofopeningthe
closedbox,assumeddeterministic,andletgdenotethevisiblerewardoftheopenbox. Ourpossible
actionsareasfollows:
1. Opentheclosedbox. Inthiscase,wepayacostofc,butsubsequentlygettochoosebetween
taking the realized value f, or instead taking g from the box that was originally open. In
expectation,thetotalvalueobtainedbytakingthisactionisE(max(f,g)) c.
−
2. Taketherewardfromtheopenbox. Thetotalvalueobtainedisg.
WecanthereforeanalyticallysolvefortheoptimalpolicyofthisrespectiveMarkovdecisionprocess:
we open the closed box if E(max(f,g)) c g, and take the reward from the open box if
E(max(f,g)) c g,withbothactionso− ptim≥ alinthecaseofequality. Asconsequence,ifg is
− ≤
suchthatbothactionsareoptimal,thesamevalueisobtainednomatterwhetheronechoosestoopen
theboxornot. Rewritingtheprecedingexpressionsslightly,thisoccurswhen
Emax(f g,0)=c (7)
−
whereinthecaseofmultipleboxestheleft-hand-sidebecomestheexpectedimprovementfunction.
TheinsightofWeitzman[38]—andindeedofGittins[17]inamuchmoregeneralsetting—isthat
onecanmodifythePandora’sBoxMarkovdecisionprocessbyreplacingclosedboxeswithopen
boxeswhosevaluegsatisfies(7)withoutchangingtheoptimalpolicy. Asaconsequence,theoptimal
policyispreciselytheGittinsindexpolicyofTheorem1.
Asafinalpoint,notethattheassumptionthatcisdeterministicismadewithoutlossofgenerality: if
cisinsteadstochasticbuthasfiniteexpectation,thesamereasoningapplies,butwiththevaluecin
(7)replacedwithitsexpectedvalue. WewillmakeuseofthisinAppendixB.3.
B.2 GradientofthePBGIacquisitionfunction
Gradient-basedmethodsincludingmulti-startstochasticgradientdescent,BFGS,andL-BFGS-B
effectivelyoptimizeanalyticalacquisitionfunctions,suchasEIandUCB.Thesemethodsarealso
applicableforoptimizingthePBGIacquisitionfunction. Tofacilitatethis,weprovidethegradient
formulaforthePBGIacquisitionfunction. Inwhatfollows,mirroringtheprecedingandfollowing
sections,ifcostsarestochasticthenc(x)shouldbereplacedwithitsrespectivemean.
Proposition3(GradientofPBGI). Letµ(x)andσ(x)bethemeanandstandarddeviationofthe
posterior Gaussian process (f y ,..,y )(x). With this notation, the gradient of the acquisition
1 t
functionαPBGI(x)isgivenby |
(cid:16) (cid:17)
ϕ µ(x) −αPBGI(x) σ(x) λ c(x)
αPBGI(x)= µ(x)+ σ(x (cid:16)) ∇ − (cid:17) ∇ , (8)
∇ ∇ Φ µ(x) −αPBGI(x)
σ(x)
whereϕandΦdenotethedensityandcumulativedistributionfunctionofastandardnormaldistribu-
tion,respectively.
Proof. Recallthatwhenψ(x) N(µ(x),σ(x))isGaussian,theexpectedimprovementwithrespect
∼
tothecomparatoryisgivenas
(cid:18) (cid:19) (cid:18) (cid:19)
µ(x) y µ(x) y
EI (x;y)=(µ(x) y)Φ − +σ(x)ϕ − . (9)
ψ
− σ(x) σ(x)
14Next,notebydefinitionofαPBGI,wehave
EI (x;αPBGI(x))=λc(x). (10)
f |y1,..,yt
Differentiatingthiswithrespecttoxonbothsidesgives
EI (x;αPBGI(x))=λ c(x). (11)
∇
f |y1,..,yt
∇
Applyingtheproductandchainruletotheleft-hand-sidegives
(cid:18)
µ(x)
αPBGI(x)(cid:19)
EI (x;αPBGI(x))=( µ(x) αPBGI(x))Φ − (12)
∇ f |y1,..,yt ∇ −∇ σ(x)
(cid:18)
µ(x)
αPBGI(x)(cid:19) (cid:18)
µ(x)
αPBGI(x)(cid:19)
+(µ(x) αPBGI(x))ϕ − − (13)
− σ(x) ∇ σ(x)
(cid:18)
µ(x)
αPBGI(x)(cid:19)
+ σ(x)ϕ − (14)
∇ σ(x)
(cid:18)
µ(x)
αPBGI(x)(cid:19) (cid:18)
µ(x)
αPBGI(x)(cid:19)
+σ(x)ϕ ′ − − . (15)
σ(x) ∇ σ(x)
RecalltheidentityforthederivativeoftheGaussiandensity,namely
ϕ ′(x)= xϕ(x). (16)
−
Applyingthisidentityto(15)gives
(cid:18)
µ(x)
αPBGI(x)(cid:19) (cid:18)
µ(x)
αPBGI(x)(cid:19)
σ(x)ϕ ′ − − (17)
σ(x) ∇ σ(x)
(cid:18)
µ(x)
αPBGI(x)(cid:19) (cid:18)
µ(x)
αPBGI(x)(cid:19)
= (µ(x) αPBGI(x))ϕ − − (18)
− − σ(x) ∇ σ(x)
whichisequaltothenegationof(13),hence(13)and(15)cancel: weget
(cid:18)
µ(x)
αPBGI(x)(cid:19) (cid:18)
µ(x)
αPBGI(x)(cid:19)
( µ(x) αPBGI(x))Φ − + σ(x)ϕ − =λ c(x). (19)
∇ −∇ σ(x) ∇ σ(x) ∇
Rearrangingthisgivestheexpressionintheclaim.
B.3 Closed-formexpressionforthePBGIunknown-costvariant
In the Lunar Lander and Robot Pushing empirical examples of Section 4, the cost function does
notadmitananalytic,automatically-differentiableform,andcanonlybeevaluatedinablack-box
manner. Tohandlethis,wemodelthelogarithmofthecostsasaGaussianprocess,andcondition
thisprocessonthecostsobservedatlocationsevaluatedsofar. Thismirrorshowunknowncosts
arehandledinotheracquisitionfunctions,suchasthebudgetedmulti-stepexpectedimprovement
acquisitionfunctionofAstudilloetal.[3].
FromtheviewpointofthePandora’sBoxproblem,stochasticcostsmakelittledifference: following
thediscussioninAppendixB.1,theoptimalityresultsofWeitzman[38]continuetoholdevenifcosts
arestochastic,solongasthecostsintheformulaforα⋆arereplacedwithexpectedcosts. Mirroring
this,ifwepluginthemeanofalog-normalrandomvariableintothedefinitionofαPBGI,weobtain
thefollowingacquisitionfunction.
Definition4. Letc(x)belog-normalforallx. Foradataset(x ,y ),..,(x ,y ),letµ andσ
1 1 t t lnc lnc
betheposteriormeanandposteriorstandarddeviationofthelog-costs. Definetheunknown-cost
Pandora’s Box Gittins index acquisition functionby
(cid:18)
σ
(x)2(cid:19)
αPBGI-U(x)=g whereg solves EI (x;g)=λexp µ (x)+ lnc . (20)
f |y1,..,yt lnc 2
Theinterpretationofλ,namelyasahyperparameterthatdeterminestheexpectedbudgetthealgorithm
willusebeforereachingitsrespectivestoppingtime,isthesameasintheknown-costsetting. One
candefinecost-per-sampleandanytimevariantsinthesamemanneraswell.
15B.4 Relationshipbetweenexpectedbudget-constrainedandcost-per-sampleproblems
WenowproveTheorem2. Inwhatfollows,recallthattheassumptionsofTheorem1are(i)X is
discrete,(ii)E f(x) < forallx,(iii)f(x)andf(x)areindependentforx = x,and(iv)the
′ ′
| | ∞ ̸
budgetsatisfiesB > min c(x). Further, Theorem1wasstatedfordeterministiccosts: more
x X
generally,weallowforstoch∈asticcostssatisfying0<Ec(x)< ,andinsuchcasesinsteaddefine
α⋆usingtheexpectedcosts. ∞
WebeginbydefiningtheMarkovdecisionprocess(MDP)understudyandstatingthepropertiesofit
thatwewilluse. Define:
1. States: let = ∂ betheunionoftwodisjointsets,namelytheset ofnon-terminal
◦ ◦
S S ∪ S S
statesandset∂ ofterminalstates. Thesearedescribedbelow:
S
(a) Non-terminalstates: let consistofallfinitesequencesoflength X takingvaluesin
◦
R ⊠ , where numberS s represent the reward inside an open box,| an| d ⊠ represents a
∪{ }
closedbox,alongwithterminalstatesdescribedbelow.
(b) Terminalstates: let∂ =Rrepresenttherewardoftheboxchosenbythelearner.
S
2. Actions: let =X,whereactionsrepresenteitheropeningaclosedbox,ortakingareward
A
fromanopenbox.
3. Costs: definethenon-terminalcostfunctionc: Rbyc(s,a)=c(a).
S×A→
4. Rewards: definetheterminalrewardfunction∂r :∂ Rby∂r(s)=s.
S →
5. Transitionkernel: defineaMarkovtransitionkernelsuchthatforastatesandactiona:
(a) Ifboxiisclosed,thatis,s =⊠,andacorrespondstoopeningthisclosedbox,thenthe
i
MDPtransitionstoanewstates,wheres representsarandomdrawofthevalueinboxi
′ ′i
andallothercomponentss forj =iremainsthesameass ;
′j
̸
j
(b) Ifboxiisopen,thatis,s R,andacorrespondstotakingtherewards fromthisopen
i i
∈
box,thentheMDPdeterministicallytransitionstoaterminalstates =s .
′ i
Thisdefinesaclassoftime-homogeneousundiscountedMarkovdecisionprocesses,parameterized
bytherewarddistributionf,ofboundedexpectedvalue,andcostfunctionc. WeconsiderMarkov
policies, which are probability kernels mapping states to probability measures over actions. For
this class of MDPs, all such policies are guaranteed to terminate in finite time, because at most
onecanopenalltheboxesbeforebeingforcedtoselectoneandtherebyenteraterminalstate. As
consequence,bystandardMDPtheory:
1. ThevaluefunctionV(π,c) : Riswell-defined,wherethesuperscriptsdenotethepolicy
S →
andthecostwithrespecttowhichtheMDPisdefined.
2. TheoptimalvaluefunctionV( ,c) : Risalsowell-defined.
∗
S →
3. Thereexistsanoptimalpolicyπ( ,c)whichachievestheoptimalvalueV( ,c).
∗ ∗
4. ThemapRX Rdefinedbyc V(π,c)isaffineforallπ.
| |
→ (cid:55)→
5. ThemapRX Rdefinedbyc V( ,c)isconvex,sinceitisasupremumofaffinefunctions.
| | ∗
→ (cid:55)→
Weimmediatelynotethatthisformulationextendstocovertwovariationsofinterest:
1. Stochastic costs. One can handle this by replacing c with a probability kernel. In this case,
lettingmbethemeancosts,wehaveV(π,c) =V(π,m). Usingthis,wehenceforthworkwith
deterministiccostswithoutlossofgenerality.
2. Simpleregretasaterminalreward. OnecanalsoconsideranMDPwithastochasticterminal
rewardwhichsubtractsthebest-in-hindsighttermsup f(x)fromtheterminalrewards∂r
x X
givenabove—thisgivesanobjectiveequaltosimplereg∈retuptoaminussign. Sincethisonly
changestherewardsuptoarandomconstantwhichisindependentofthechosenactions,by
linearityofexpectation,thevaluefunctionofthismodifiedMDPisequaltothoseofourMDP
uptoaconstant. Wethereforeomitthistermwithoutlossofgenerality.
Theexpectedbudget-constrainedsettingdoesnotdirectlyincorporatecostsintotheMDPitself: more
precisely,ittakesc(s,a)=0withintheMDPformulation,whichmeansthevaluefunctionofthis
16modifiedMDPisV(π,0). Instead,costsareincorporatedasaconstraintsetonthepolicyclassone
considers. Withthisnotation,thevaluesoftheoptimizationproblemsforBayesian-optimalpolicyin
theexpectedbudget-constrainedandcost-per-sampleproblemsare
V e( b∗ c,c) = sup V(π,0) V( ∗,c) = supV(π,c) (21)
π ∈Π( Bc) π ∈Π
whereΠisthesetofallMarkovpolicies,andthefeasiblesetfortheexpectedbudget-constrained
problemis
 
 T (cid:88)(π) 
Π(c) = π Π:E c(x ) B (22)
B  ∈ t ≤ 
t=1
andT(π)ispolicyπ’sstoppingtime,thatis
T(π) =inf t 1:s ∂S (23)
t
{ ≥ ∈ }
whichisboundedaboveby X < . Thisdefinestheoptimizationproblemsunderstudy. Define
| | ∞
thesetofmaximizers
(cid:110) (cid:111)
Π( ∗,c) = π Π:V(π,c) =V( ∗,c) (24)
∈
whichisnon-empty,since,assaidabove,byMDPtheorythesupremumdefiningV( ,c)isachieved.
∗
Definealsothesetoffeasiblepolicieswhichsatisfytheconstraintsinatightmanner,namely
 
 T (cid:88)(π) 
Π(c) = π Π(c) :E c(x )=B . (25)
B,eq  ∈ B t 
t=1
Wearenowreadytoprovetheclaiminquestion. Forthis,weemployaLagrangiandualityargument:
looselyspeaking,thiswillrevealthecosts-per-sampleλtobetheLagrangemultipliersassociated
withtheexpectedbudgetconstraint. Weprovethemainclaimviaaseriesoflemmas,startingfrom
handlingtheLagrange-multiplier-partoftheargument.
Lemma5. Definethefunction
:[0, ) R (λ)=V( ∗,λc)+λB. (26)
A ∞ → A
Supposethattheinfimumof isachieved,anddenoteitbyλ [0, ). Supposefurtherthatthere
A
∗B
∈ ∞
existsanoptimalpolicyforthecost-per-sampleMDPforwhichtheexpectedbudgetconstraintis
tight,namelyπ
(cid:98)
∈Π( ∗,λ∗ Bc) ∩Π( Bc ,) eq. Thenwehave
V(π(cid:98),c) =V( ∗,c).
(27)
ebc ebc
Proof. Westartbyexpressingtheexpectedbudget-constrainedoptimizationprobleminanuncon-
strainedformviaLagrangemultipliers,obtaining
  
T(π)
(cid:88)
V e( b∗ c,c) =
π
∈su Πp
(
Bc)V(π,0) = πsu ∈p Πλin ≥f 0V(π,0) −λE
t=1
c(x t) −B (28)
  
T(π)
(cid:88)
= inf supV(π,0) λE c(x t) B (29)
λ 0π Π − −
≥ ∈ t=1
(cid:16) (cid:17)
= inf V( ∗,λc)+λB = inf (λ) (30)
λ 0 λ 0A
≥ ≥
wherethesecondlinefollowsfromtheLemma10sincetherespectiveLagrangianequalsV(π,λc)up
toaconstant,andthethirdlinefollowsbydefinitionofV( ,λc)isbydefinitiontheterminalreward
∗
minusthecumulativecosts.
17( ,λ∗c)
N Vo ( ∗w ,λ, ∗ Bsu cp )p +os λe ∗Bth Be .in Ufi sm inu gm tho if s,A anis da tc hh eie fv ae cd t, than atd tl he etλ p∗B oli∈ cy[0 π (cid:98), ∞ by) ab se sua mny ptm ioin nim aci hz ie er v. eT sh te hn eV se ub∗ pc reB mum=
oversup
π
∈ΠV(π,λ∗ Bc)andsatisfiesπ
(cid:98)
∈Π( Bc) ,weget
sup V(π,0) = supV(π,λ∗ Bc)+λ ∗BB = sup V(π,λ∗ Bc)+λ ∗BB. (31)
π ∈Π( Bc) π ∈Π π ∈Π( Bc)
Thus,theoptimizationobjectives,definingtheexpectedbudget-constrainedproblemandthecost-per-
sampleproblemwithcostsλ c,areequaluptoaconstant. Therefore,theirminimizersetscoincide,
∗B
andtheclaimfollows.
Lemma5revealsthataslongastheoptimizationproblemsarisingfromLagrangemultipliersare
achieved,andtheresultingpolicyπisfeasible,theexpectedbudget-constrainedproblemwilladmit
(cid:98)
thesameoptimumasitsassociatedcost-per-sampleproblem.ByMDPtheory,weknowthesupremum
overΠisachieved: wenowshowtheinfimuminvolvingλisachievedaswell,whichessentially
amountstorulingoutarbitrarily-largeλvalues.
Lemma6. Themap isconvex. Moreover,theinfimuminf (λ)isachieved.
λ 0
A ≥ A
Proof. First,notethatconvexityfollowsstraightforwardlyfromconvexityofc V( ,c) andthe
∗
(cid:55)→
factthatthesumofconvexfunctionsisconvex. Next,sincethefeasiblesetisλ [0, )andthe
∈ ∞
objectiveisconvex,eithertheinfimumisachieved,ortheobjectiveisnon-increasing. Weprovethe
latterpropertycannothold. Forthis,firstconsiderthepolicyπ thatdeterministicallyopenssome
′
boxx X whosecostisc(x)<B—notethatourassumptionsguaranteetheexistenceofatleast
′ ′
onesuc∈
hbox—andselectsthevalueinit. Thereforeifwedefine
(λ)=V(π′,λc)+λB,wehave
′
A
′(λ)= V(π′,λc)+λB V( ∗,λc)+λB = (λ). (32)
A ≤ A
Atthesametime,wehaveV(π′,λc) =Ef(x) λc(x)therefore
′ ′
−
′(λ)=Ef(x ′) λ(c(x ′) B) (33)
A − −
negative
which means the map is affine and strictly increasing with respect to λ. Thus, the map is
′
A A
lower-boundedbyastrictlyincreasingaffinefunction,andthereforecannotbenon-increasing. We
concludethattheinfimumisachieved.
Thenextpartistoshowthattheoptimalcost-per-samplepolicyisfeasiblefortheexpectedbudget-
constrainedproblem. Thentodoso,weneedtoverifyacertainconvergencecriterion,givenbelow.
Inthefollowing,notethatλ—whichcanbenegative—isneverusedinthedefinitionofanyoptimal
′
policy,onlytocomputethevalueofagivenpolicy,whichstillmakessenseevenwithnegativecosts.
Lemma7. Foranymonotonesequenceλ λwhereλ >0andλ>0,andanyλ R,there
n n ′
existpoliciesπ
n∗
∈Π( ∗,λnc)andπ
∗
∈Π( ∗,λc→)suchthat ∈
V(π n∗,λ′c) V(π∗,λ′c). (34)
→
Proof. First,notethat
T(πn∗)
V(π n∗,λ′c) =V(π n∗,λc)+(λ ′ λ n)E (cid:88) c(x t). (35)
−
t=1
(a) (b)
Usingthis,bypassinglimitsthroughtherespectivesumsandproducts,itsufficestoproveconvergence
of(a)and(b)separately,withthesamesequencechoiceπ inbothcases. ByWeitzman[38]—see
n∗
Kleinberg et al. [23], Theorem 1, for an alternative proof—any policies π ,π which maximize
n∗ ∗
therespectiveGittinsindicesα ,α areoptimal: wewillthereforechooseπ ,π fromthissetof
n∗ ∗ n∗ ∗
policies. Thischoiceisnotuniqueduetothepossibilityofties, bothintermsofwhichboxesto
open, andwhentostop: wewillshowthattie-breakingchoicesdonotaffectconvergenceof(a),
andwillmakeasuitablechoiceoftie-breakingrules,dependingonthesequenceλ intheclaim’s
n
assumptions,toproveconvergenceof(b).
18Part I: convergence of (a). Define κ (x) = min(f(x),α (x)), and define κ analogously. By
n n∗
Kleinbergetal.[23],Theorem1,wehave
V(π∗,λnc) =Emaxκ (x) (36)
n
x X
∈
andanalogouslyforλ, α , andκ. Wenowarguethatα α monotonicallypointwise. Since
∗ n∗
→
∗
λ λconvergesmonotonically,consider
n
→
EI (x,g)=λ c(x). (37)
f n
RecallthatEI (x,g)iscontinuousandstrictlydecreasinging: hence,itsinverseingexistsandis
f
alsostrictlydecreasingandcontinuous. Weconcludethatα α monotonicallypointwise. From
this,convergenceoftherespectiveexpectationsEmax
n∗
κ→
(x),∗
andthereforeconvergenceof(a),
x X n
followsbytheMonotoneConvergenceTheorem. ∈
PartII:convergenceof(b). Wewillneedanidentityinvolvingtheorderinwhichboxesareopened:
forthis,wefirstprovethatonceatie-breakingruleischosen,forlargeenoughn,π andπ open
n∗ ∗
boxesinthesameorder. Sincethenumberofboxesisfinite,andα α monotonicallypointwise
n∗
→
∗
from(a): itfollowsfornlargeenoughthat,iftherearenotiesinα (x),thenπ opensboxesinthe
∗ n∗
sameorderasπ . Iftherearetiesinα ,wechooseatie-breakingrulesothatπ andπ openboxes
∗ ∗ n∗ ∗
inthesameorder. Itfollowsthat,oncethischoiceismade,fornlargeenough,allpoliciesopen
boxesinthesameorderx ,..,x ,withdifferentπ andπ possiblystoppingatdifferenttimes.
1 X n ∗
| |
Theidentityweseekwilldifferdependingonhowtie-breakingrulesregardingwhentostopare
handled: recallthattiescanoccurwhenopeningtheboxx X attimet,wehaveα (x ) = f ,
t
∈
∗ t t∗
where f is the best observed value up to time t. We adopt the following tie-breaking rule for
t∗
stopping,dependingonwhetherornotλ isanincreasingordecreasingsequence:
n
• Ifλ isincreasing: letπ =π bethepolicythatopensthebestclosedboxintheeventofatie.
n ∗ +∗
• Ifλ isdecreasing: letπ =π bethepolicythatstopsintheeventofatie.
n ∗ ∗
−
Note also that if λ is both increasing and decreasing, it is constant, and the claim we want to
n
proveholds: therefore,wesupposeitisnot,whichmakesthechoicebetweenπ andπ uniquely
+∗ ∗
determined. Wemakethesametie-breakingchoicesforπ ,lettingπ andπ bethe−respective
n∗ n∗,+ n∗,
policies. Letx ,..,x denoteboxesintheordertheyareopened. Thentheexpe−ctedtotalcostsare
1 X
| |
T(π+∗)
X X
E(cid:88) c(x t)=(i) (cid:88)| | P(T(π +∗) ≥i)c(x i)( =ii)(cid:88)| | P(f j∗ ≤α ∗(x j),1 ≤j ≤i)c(x i) (38)
t=1 i=1 i=1
X X i 1
( =iii)(cid:88)| | P(f(x j) α ∗(x i),1 j i)c(x i)( =iv)(cid:88)| | (cid:89)− P(f(x j) α ∗(x i))c(x i) (39)
≤ ≤ ≤ ≤
i=1 i=1j=1
where(i)followsbywritingtheprobabilityasanexpectationofanappropriateindicator,(ii)follows
bynotingthatunderthepolicyπ +∗,theeventT(π +∗) ≥ioccursifandonlyifateachtimej =1,..,i,
thebestobservedvaluef isnothigherthantheGittinsindexofx ,(iii)followsbyexpandingthe
j∗ j
maximumwhichdefinesf andusingmonotonicityofα (x )inj tosimplifytheresultingevents,
j∗ ∗ j
and(iv)followsbyindependenceoff(x j)andf(x j′)forj =j ′. Usingthis,itsufficestoshow
̸
P(f(x j) ≤α n∗(x i)) →P(f(x j) ≤α ∗(x i)) ifπ ∗ =π +∗, and (40)
P(f(x j)<α n∗(x i)) →P(f(x j)<α ∗(x i)) ifπ ∗ =π −∗. (41)
Theformerequalsthecumulativedistributionfunctionoff(x ),whichisright-continuous,evaluated
j
at α (x ). The latter is similar but is instead left-continuous. For π , since λ is increasing,
n∗ i n∗,+ n
wehavethatα (x)isdecreasing,andtheclaimfollowsbyright-continuity. Forπ sinceλ is
n n∗, n
decreasing,wehavethatα (x)isincreasing,andtheclaimfollowsbyleft-continuity.−
n
WearenowreadytoprovethekeypropertyneededtoapplyLemma5.
Lemma8. Ifλ
∗B
>0,thenthereexistsaπ
(cid:98)
∈Π( ∗,λ∗ Bc) ∩Π( Bc ,) eq.
19Proof. BytheEnvelopeTheorem—specifically,Lemma13—wehaveforλ [0, )that
∈ ∞
(cid:16) (cid:17)
∂
v
(λ)=∂
v
V( ∗,λc)+λB (42)
A
   
T(π)
(cid:88)
=∂ vsupV(π,0) λE c(x t) B (43)
− −
π Π
∈ t=1
  (cid:12)
T(π) (cid:12)
= max ∂ vV(π,0)
λE(cid:88)
c(x t)
B(cid:12)
(cid:12) (44)
π′ Π(∗,λc) − − (cid:12)
∈ t=1 (cid:12) π=π′
T(π)
(cid:88)
=vB min vE c(x ) (45)
t
−π Π(∗,λc)
∈ t=1
whereallGâteauxderivatives—seeDefinition11—aretakenwithrespecttoλandexistbyconvexity,
andthepointwiseconvergenceconditionofLemma13followsbyfirstnotingthat
 
T(π)
(cid:88)
V(π,0) λE c(x t) B=V(π,λc)+λB (46)
− −
t=1
andapplyingLemma7. Bythefirst-orderoptimalityconditions,wehave
∂
v
A(λ ∗B) ≥0. (47)
Combiningthiswiththeabove,weconclude
T(π)
(cid:88)
min vE c(x ) vB. (48)
t
π Π(∗,λc) ≤
∈ t=1
Sinceλ >0,thisexpressionholdsforvequalto 1: pluggingthisin,weget
∗B
±
T(π) T(π)
(cid:88) (cid:88)
min E c(x ) B B max E c(x ). (49)
t t
π ∈Π(∗,λ∗ Bc) t=1 ≤ ≤ π ∈Π(∗,λ∗ Bc) t=1
Now,letπ beapolicyfromtheminimizerset,andletπ beapolicyfromthemaximizerset.Define
(cid:98) (cid:98)+
athirdpol−icyπ whichrandomizesbetweenthetwo,choosingπ withprobabilityαandπ with
(cid:98)α (cid:98) (cid:98)+
probability1 α. Sinceπ andπ areoptimal,theyachievethe−samevalue,sobyconvexityπ
(cid:98)+ (cid:98) (cid:98)α
alsoachieves− thesamevalue: there− fore,π
(cid:98)α
Π( ∗,λBc). Ontheotherhand,theexpectedtotalcosts
∈
ofπ areaconvexcombinationoftheexpectedcostsofπ andπ : sincetheformerlower-bounds
(cid:98)α (cid:98) (cid:98)+
Bandthelatterupper-boundsB,thereexistsanα [0,1−]forwhichthecostsofπ areexactlyB.
α
∈
Takingπ =π forthisvalueofα,weobtainπ Π(c) andtheclaimfollows.
(cid:98) (cid:98)α (cid:98) ∈ B,eq
Tocompletetheproof,weshowtheminimizersetof doesnotcontainzero.
A
Lemma9. Letλ [0, )beanyminimizerof ,thenλ >0.
∗B
∈ ∞ A
∗B
Proof. From the derivative calculation used in proof of Lemma 8, plugging in v = 1 into the
respectiveGâteauxderivative,weobtain
T(π)
d (cid:88)
(λ)=B min E c(x ). (50)
t
dλ A −π Π(∗,λc)
∈ t=1
If λ = 0, then the costs are zero: thus, any policy that opens every box, then selects the highest
rewardamongopenedboxes,isoptimal. Therefore,wehaveT(π) = X ,andeveryoptimalpolicy’s
| |
costisequaltothetotalcostofalltheboxes. Thisgives
d (cid:88)
(0)=B c(x)<0 (51)
dλ A −
x X
∈
wheretheinequalityfollowsbytheassumptionthattheexpectedbudgetconstraintisactive. The
claimfollows.
20Withtheseresultsathand,wearenowreadytoprovethemainclaim.
Theorem2. Considertheexpectedbudget-constrainedproblem,withtheassumptionsofTheorem1.
(cid:80)
Assumetheproblemisfeasibleandtheconstraintisactive,namelymin c(x)<B < c(x).
x X x X
Thenthereexistsaλ > 0andatie-breakingrulesuchthatthepolic∈ydefinedbymaximi∈zingthe
Gittinsindexacquisitionfunctionα⋆(),definedusingcostsλc(x),isBayesian-optimal.
·
Proof. CombineLemma5withLemmas6,8and9.
WeconcludebycomparingthisclaimandproofwiththeresultsofAminianetal.[2]:
1. Theobjectiveoftheirexpectedbudget-constrainedproblemismoregeneral: whilewefocuson
minimizingsimpleregret,Aminianetal.[2]studyhowtomaximizeutility,whichisdefined
inabroadercontext—oneexamplebeingthedifferencebetweenthebestobservedvalueand
cumulativecosts,whichwestudyhere. Ourexpectedbudgetconstraintissimilarlyaspecial
case, which, in their terminology, takes all weights on indicators denoting inspection to be
identical,andtakesallselectionweightstobezero.
2. Wedonotassumethatf(x)hasfinitesupportforallx. Thisresultsinasignificanttechnical
difference: wemustapplyasharpenvelopetheoremwithanexplicitsupremumtoconclude
that the expected budget constraint is satisfied. More-straightforward results such as those
presentedbyMilgromandSegal[27],ortheirsubdifferential-formulatedanaloguesasfound
in the proof of Aminian et al. [2], Proposition 1 (iii), would not suffice: due to an explicit
counterexample involving upside-down-absolute-value functions, without suitable structure,
onecannotconcludethatthesupremumisachievedandtheresultinginequalityistight. Asan
alternativetoourapproach,onecouldinsteadappealtofinitenessoftheclassofdeterministic
policies—whichAminianetal.[2]assume,leadingtopiecewise-linearvaluefunctions. We
avoidtheseassumptions,leadingtoamoretechnicalbutmoregeneralargument.
Weconcludebynotingthatonecanalsoconsidervariantsoftheproblemswehavestudiedunderan
almost-surebudgetconstraint,forinstance
   
 T (cid:88)(π) 
Π( Bc ,)
as
= π ∈Π:P 
t=1
c(x t) ≤B=1

V a( s∗ b, cc) =
π
∈s Πu
(
Bp
c ,)
asV(π,0). (52)
Forthisproblem,thisoptimalvalueisupper-boundedbytheoptimalvalueoftheexpectedbudget-
constrainedoptimization—specifically,V( ∗,c) V( ∗,c) ,sinceΠ(c) Π(c)
.
asbc ≤ ebc B,as ⊆ B
B.5 Auxillaryresultsfromoptimizationtheory
Belowwestateandprovethreeresultsfromoptimizationtheory: aLagrangeMultiplierTheoremfor
optimizationproblemswithinequalityconstraints,andtwoEnvelopeTheorems. Theseresultsare
notnew,butaredifficulttofindatthelevelofgeneralityweneedthemat: mostreferencesbeginby
assumingthedomainofoptimizationisRd,or,ifnotthat,thatitisaBanachspace,whereasweneed
resultsthatholdwhenthedomainofoptimizationisanarbitraryset,withoutalinearstructureora
topology. Inlightofthis,wehavefounditeasiertosimplyprovetheclaimsweneed.
Lemma 10. Let X be an arbitrary set, let f : X R and let g : X R. Then defining
→ →
(x,λ)=f(x) λ(g(x) y)forλ 0,wehave
L − − ≥
sup f(x)= sup inf (x,λ). (53)
x X x Xλ 0L
g(x∈) y ∈ ≥
≤
Moreover,supposethereexistx ,λ whichsatisfy (x ,λ )=inf sup (x,λ)andg(x )=
y. Thenwehave
∗ ∗ L ∗ ∗ λ ≥0 x ∈XL ∗
sup inf (x,λ)= inf sup (x,λ). (54)
x Xλ 0L λ 0x XL
∈ ≥ ≥ ∈
Proof. First,weshowthatforallx,wehave
(cid:26)
f(x) g(x) y
inf (x,λ)= ≤ (55)
λ 0L g(x)>y.
≥ −∞
21Toshowthis,supposefirstthatg(x) y. Thenλ(g(x) y) 0,henceitsnegationisnon-negative,
≤ − ≤
and minimized at λ = 0. Now, suppose the converse. Then λ(g(x) y) > 0, so its negation is
−
negative,andtheobjectivecanbemadearbitrarilycloseto byscalingλ. Usingthis,write
−∞
 
sup inf (x,λ)=max sup inf (x,λ), sup inf (x,λ) (56)
x Xλ 0L  x X λ 0L x X λ 0L 
∈ ≥ g(x∈) y ≥ g(x∈)>y ≥
≤
 
=max sup f(x), = sup f(x). (57)
 −∞
x X x X
g(x∈) y g(x∈) y
≤ ≤
Wenowarguethat,undertheclaim’sadditionalassumption,onecanswaptheorderofthesupremum
andinfimum. Letx ,λ beapairforwhichtheconstraintsaretight. Then
∗ ∗
(i) (ii)
inf sup (x,λ)= sup (x,λ ∗)= sup (x,λ ∗) (58)
λ 0 x X L x X L x X L
≥ g(x∈) y g(x∈) y g(x∈)=y
≤ ≤
(iii) (iv)
= sup inf (x,λ) sup inf (x,λ) (59)
x X λ 0L ≤ x X λ 0L
g(x∈)=y ≥ g(x∈) y ≥
≤
where(i)followsbydefinitionofλ , (ii)followsbythefactthatx achievesthesupremumand
∗ ∗
satisfiesg(x ) = y,(iii)followsbythefactthat,whenrestrictedtotheset x X : g(x) = y ,
∗
{ ∈ }
(x,λ)isconstantinλforallx-values, hencetheinfimumistakenoverconstantfunctions, (iv)
L
followsbymakingthefeasiblesetlarger. Combiningthiswiththesup-infinequality[9]
sup inf (x,λ) inf sup (x,λ) (60)
x X λ 0L ≤λ 0 x X L
g(x∈) y ≥ ≥ g(x∈) y
≤ ≤
givestheclaim.
Itiseasytoseethatthisargumentholdsevenifoneletsy takevaluesinaninfinite-dimensional
vector space, as long as λ takes values within a convex cone which is suitably paired with the
aforementionedvectorspace,butwewillnotneedthislevelofgenerality. Theargumentsweemploy
will be cleanest if we work with directional derivatives in the sense of Gâteaux, as opposed to
left-derivativesandright-derivativesofreal-valuedfunctions,whichinoursettingareequivalentbut
requiremoremanagementofminussigns. Forthis,weadoptthefollowingnotation.
Definition11. Let beatopologicalvectorspace,letX ,andletf :X Rbeafunction.
TheGâteaux derXivative∂ f(x) Rofafunctionf at⊆ apX ointx X inthe→ directionv ,if
v
∈ ∈ ∈X
itexists,isdefinedas
f(x+εv) f(x)
∂ f(x)= lim − . (61)
v
ε 0+ ε
→
NotethatwerequirenopropertiesofourGâteauxderivatives: inparticular,∂ f(x)canbenon-linear
v
inv,thoughonecaneasilyseethatitwillalwayssatisfy∂ f(x)=α∂ f(x)forα 0. Iff isa
αv v
≥
convexfunction,onecanshowbymonotonicityoffinitedifferencesthat∂ f(x)alwaysexists—in
v
thenon-extended-valuedsensedefinedabove—ontherelativeinterioroftheeffectivedomainoff.
OurargumentsneedanappropriateEnvelopeTheorem. Thefirststatementweneedisessentially
equivalent to Milgrom and Segal [27], Theorem 1, which is formulated in the language of left-
derivativesandright-derivatives: MilgromandSegal[27]stateinafootnotethattheirclaimalso
holdsinageneralnormedvectorspace,providedoneworkswithdirectionaldifferentiation. Infact,
theclaimisevenmoregeneralthanthat,anddoesnotrequireanorm. Sincewefindthisvarianttobe
particularlyclean,elegant,andinstructive,weprovetheclaiminfullgeneralitybelow.
Lemma12. LetΘbeasubsetofatopologicalvectorspace, andletX beanarbitraryset. Let
f :X Θ Rbeboundedaboveinitsfirstargument,anddefine tobe
× → V
(θ)= supf(x,θ) (62)
V
x X
∈
22Supposethat,foreveryθ Θ,thesupremumisachieved,andletX (θ)bethemaximizerset. For
∗
∈
anyv,supposethattheGâteauxderivatives∂ (θ)and∂ f(x,θ)existandarefinite-valued,with
v v
V
theconventionthattheGâteauxderivativeoff istakeninitssecondargument. Then
sup ∂ vf(x ∗,θ) ∂
v
(θ). (63)
≤ V
x∗ X∗(θ)
∈
Proof. Fixθ Θ,andletx (θ) X (θ)beanarbitrarymaximizer. Notethat,forallθ Θand
∗ ∗ ′
∈ ∈ ∈
allx (θ ) X (θ ),byoptimalitywehave
∗ ′ ∗ ′
∈
f(x ∗(θ),θ ′) f(x ∗(θ ′),θ ′)= (θ ′) (64)
≤ V
with equality for θ = θ. Letting ε > 0 be sufficiently small, taking θ = θ +εv, subtracting
′ ′
f(x (θ),θ)= (θ)frombothsides,anddividingbyε,weget
∗
V
f(x (θ),θ+εv) f(x (θ),θ) (θ+εv) (θ)
∗ ∗
− V −V (65)
ε ≤ ε
thustakinglimitsasε 0gives
→
∂ vf(x ∗(θ),θ) ∂
v
(θ). (66)
≤ V
Thisholdsforallchoicesx (θ) X (θ),andtheclaimfollows.
∗ ∗
∈
Forourargumentstogothrough,weneedasharperversionofthisresult,withtheinequalityreplaced
withanequality. ResultslikethisgobackatleasttoDanskin[10],andareprovenbyBonnansand
Shapiro[8],Proposition4.12,Bas¸arandBernhard[5],Theorem10.1,andBertsekas[7]: however,
all of their claims require topological assumptions on the domain of optimization. Moreover, it
is easy to see—for instance by considering an envelope made up of upside-down absolute value
functions, where (θ) is constant but f(x,θ) is not—that the inequality cannot be tight without
V
similarassumptions. Below,weshowthatbeingaffineinθandacertainconvergencecriterionsuffice,
evenifX isanarbitraryset. Theargumentissimilartothatoftheaforementionedreferences.
Lemma13. WiththeassumptionsandnotationsinLemma12,supposefurtherthatf(x,θ)isaffine
inθforallx,andthatforanymonotoneθ θthereexistx (θ ) X (θ )andx (θ) X (θ)
n ∗ n ∗ n ∗ ∗
→ ∈ ∈
suchthatf(x (θ ),v) f(x (θ),v). Thenwehave
∗ n ∗
→
sup ∂ vf(x ∗,θ)= max ∂ vf(x ∗,θ)=∂ vf(x ∗(θ),θ)=∂
v
(θ). (67)
x∗ ∈X∗(θ) x∗ ∈X∗(θ) V
Proof. Foranyε,notethat
(θ+εv) (θ) f(x (θ+εv),θ+εv) f(x (θ),θ)
∗ ∗
V −V = − (68)
ε ε
f(x (θ+εv),θ+εv) f(x (θ+εv),θ)
∗ ∗
= − (69)
ε
negative
f(x (θ+εv),θ) f(x (θ),θ)
∗ ∗
+ − (70)
ε
f(x (θ+εv),θ+εv) f(x (θ+εv),θ)
∗ ∗
− (71)
≤ ε
=f(x ∗(θ+εv),v) (72)
where the respective term is negative because f(x (θ +εv),θ) f(x (θ),θ), which holds by
∗ ∗
≤
optimalityofx (θ). Takinglimitsgives
∗
∂
v
(θ) f(x ∗(θ),v)=∂ vf(x ∗(θ),θ) (73)
V ≤
wherethefinalequalityfollowsfromtheexpressionfortheGâteauxderivativeofalinearfunction.
CombiningthisexpressionwiththeLemma12showsthattherespectivesupremumisachieved,and
givestheclaim.
23C Experimentalsetup
WeimplementallexperimentsinBOTORCH. Followingstandardpractice,weinitializeeachopti-
mizationalgorithmwith2(d+1)valuesdrawnusingaquasirandomSobolsequence,wheredisthe
dimensionofthedomain. AllcomputationswererunonCPU,withindividualexperimentsranin
parallelonvariousnodesoftheCornellG2cluster,eachallocatedupto4GBofmemory. Exceptions
includeKG,MSEI,andBMSEIforhigherdimensions,whichrequiredsubstantiallymorememory,
upto32GB.Mostindividualrunstookseveralminutesatmost,withexceptionofthemore-expensive
KG,MSEIandBMSEIbaselines: moreinformationwithadirectruntimecomparisonisgivenin
AppendixD.
Gaussianprocessmodels. IntheBayesianregretexperiments,weuseMatérnkernelswithidentical
fixed hyperparameters—namely smoothness 5/2 and length scale 10 1—for both the Gaussian
−
process prior used tosample the objective function, and the Gaussian process usedfor Bayesian
optimization. Tomaintainconsistency,wedonotstandardizedata. Forthesyntheticandempirical
experiments,weuseMatérnkernelswithsmoothness5/2andlengthscalesestimatedinadvancevia
maximummarginallikelihood,andstandardizedatatobezeromeanandunitvariance,following
BoTorchdefaults. Intheunknown-costexperiments,wemodeltheobjectiveandthelogarithmofthe
costfunctionusingindependentGaussianprocesses. Additionalexperimentalresultswhichshowthe
effectofvaryingthesehyperparameterchoices,aregiveninAppendixD.
Acquisitionfunctionoptimization. Thisisdoneasfollows. Webeginbycomputingacquisition
valuesat200dpointsspreadacrossthedomainX,wheredisthedimensionofX. Forallacquisition
functionsexceptMSEIandBMSEI,whichuseamodificationdescribedbelow,theinitial200dpoints
aregeneratedusingaSobolsequencedesign. Fromthese,10dpointsareselectedaccordingtothe
initializationheuristicusedbyBoTorch,detailedinBalandatetal.[4],AppendixF.1. Wethenuse
multi-startL-BFGS-Btooptimizetheacquisitionfunctionfromeachselectedpoint. Thepointwith
highestacquisitionvalueamongthe10doptimizedpointsischosenasthenextevaluationpoint.
WenowdetailthemodifiedstrategyusedforMSEIandBMSEI:here,theinitial200dpointsare
selectedusingthewarm-startinitializationstrategydescribedinJiangetal.[22],AppendixDand
Astudilloetal.[3],AppendixF.Thisstrategyusestheoptimalsolutionfromthepreviousiteration,
targeting the branch that originates from the tree’s root and whose fantasy sample most closely
matchestheactualobservedvalueofthepreviouslysuggestedcandidateonthetruefunction. This
modificationfavorsMSEIandBMSEI,slightlydisadvantagingPBGIandotherbaselines.
Acquisitionfunctionhyperparameters. ForPBGI,weuseλ=10 4andatotalof100iterations
−
ofbisectionsearchwithoutanyearlystoppingorotherperformanceandreliabilityoptimizations.
ForUCB,wefollowthescheduleinSrinivasetal.[34],Theorem1givenbyβ =2log(dt2π2/6δ),
t
wheredisthenumberofdimensions. Wealsoadoptthechoiceofδ =0.1andascale-downfactorof
5,asusedinthatwork’sexperiments. ForMSEIandBMSEI,weuse4lookaheadsteps,eachwitha
batchsizeof1andasinglefantasypoint.
Omittedbaselines. WeomitMSEIfromtheBayesianregretplotsford=16withκ=10 1and
−
ford=32withalllengthscalechoicesbecausewewereunabletogetittoworkreliablyinthese
settings: theimplementationofJiangetal.[22]resultsinfrequentcrashesduetorunningoutof
memoryandrelatedissueswhenusedonhigher-difficultyproblems. WealsoomitKGfromd=32
andBMSEIfromthecost-awarePestControlandRobotPushingexperimentsforthesamereasons.
In addition to the baselines mentioned in Section 4, we also implemented the predictive entropy
search(PES)acquisitionfunctionofHernández-Lobatoetal.[21],butcouldnotgetitscomputations
torunreliablyinanautomatic-differentiation-basedenvironmentwithoutresultinginNaNgradients.
Hernández-Lobatoetal.[21]documentthisbehavior,andsuggestusingfinite-differencinginsitua-
tionswhereitoccurs: however,frominitialexamination,wefoundthistodecreaseperformanceon
higher-dimensionalproblems. Wethereforeoptedtorestrictourselvestoautomatically-differentiable
baselinesandomitPES,toensurethatperformancedifferencesseencanreliablybeattributedtothe
acquisitionfunctionsused,andnottogradientcomputation.
Objectivefunctions: Bayesianregret. ForBayesianregret,thisisstraightforward: theobjectiveis
simplyadrawfromaFourierfeatureapproximationoftherespectiveGaussianprocessprior,drawn
24insuchawaythatdifferentbaselineswiththesamerandomnumberseedsharethesameobjective,
butobjectivesfordifferentseedsaredifferentdrawsfromthesameprior. Weuseatotalof1024
Fourierfeatures.
Objectivefunctions: syntheticbenchmarks. Thesyntheticbenchmarksweuseareasfollows.
Ackley: thisis
 (cid:118) (cid:117) d  (cid:32) d (cid:33)
(cid:117)1(cid:88) 1(cid:88)
f A(x 1,..,x d)=20 −20exp −0.2(cid:116)
d
x2 i −exp
d
cos(2πx i) +e (74)
i=1 i=1
withsearchdomainX =[ 1,1]d.
−
Levy: thisis
d 1
f (x ,..,x )=s
+(cid:88)−
(w
1)2(cid:0)
1+10sin(πw
+1)2(cid:1)
+(w
1)2(cid:0)
1+sin(2πw
)2(cid:1)
(75)
L 1 d 1 i i d d
− −
i=1
withw
i
=1+ xi 4−1 ands
1
=sin(πw 1)2,andsearchdomainX =[ −10,10]d.
Rosenbrock: thisis
d 1
f (x ,..,x )=(cid:88)− (cid:0) 100(x x2)2+(x 1)2(cid:1) (76)
R 1 d i+1 − i i −
i=1
withsearchdomainX =[ 5,10]d. Specifically,wetesteddimensiond=4,8,16inoursynthetic
−
benchmarkexperiments. Inthecost-awareBayesianregretandsyntheticbenchmarkexperiments,we
usethecostfunction
c(x)=20 S(x) +1 (77)
∥ ∥1
whereS isanaffinemapusedtostandardizetheinputdomain: specifically,S(x)=Ax+bwhereA
isadiagonalmatrixandbisavector,bothchosensothattheimageofX underS is[0,1]d.
Objectivefunctions: empirical. Wenowdetailtheempiricalobjectivefunctions.
PestControl(d=25). Thepestcontrolproblem,asdescribedinOhetal.[28]aimstominimizethe
spreadofpestsaswellasthecostsofpreventiontreatment. WeadopttheexperimentsetupfromLi
etal.[25],framingthisasacategoricaloptimizationproblemwith25variables,eachrepresentinga
stageofinterventionwith5valuesreflectingdifferenttreatments. Theobjectivefunctioncombines
thespreadofpestsandthecostsofprevention. Inourcost-awareexperiment,weusethecostof
preventionasthecostfunction. Thiscanbecomputedinanautomatically-differentiablemanner,thus
thisproblemisaknown-costproblem.
LunarLander(d = 12). FollowingthesetupinErikssonetal.[13],weconsiderareinforcement
learningproblemoptimizingacontrollerforthelunarlanderasimplementedinOpenAIGym,which
includes12continuousinputdimensionsforenginethrottleadjustments. Thestatespacecapturesthe
lander’sposition,angle,timederivatives,andlegcontactstatus. Thecontroller’sactionsallowfor
directionalboosterfiringsorinaction. Theobjectiveistomaximizetheaveragefinalrewardover
50randomlygeneratedenvironments. Forcost-awareexperimentation,wechoosethecosttobethe
averagenumberofsimulationtimesteps,assumingbatchprocessingingroupsof16.Thisassumption
isbasedontheimplementationfoundinthecodeassociatedwithLietal.[25].Notethatthisobjective
involvesthenumberofactualsimulationstepsused,andisthereforenotautomatically-differentiable:
andwethusconsiderthisproblemtobeanunknown-costproblem.
Robot Pushing (d = 14). In this work, we adapt one of the three versions of the robot pushing
problemdesignedbyWangandJegelka[36],wheretworobotsworktopushtwoobjectstotheir
specifiedtargets. Theproblem’scomplexityiscapturedthrough14controlparameters,including
eachrobot’sinitialplacementandmotionsettings. Theobjectiveminimizesthesumofthedistances
from the final position of each object to its respective target. In our cost-aware experiments, we
testbothknown-costandunknown-costvariants. Theknown-costvariantisthemaximumofthe
tworobots’operationaldurationandtheunknown-costvariantsvariantisthesumoftheirtraversal
distancesrepresentingthetotalenergyuse,similartothecostfunctionusedforenergy-awarerobot
25d=4 d=8 d=16
300 600 1,200
200 400 800
100 200 400
0 0 0
0 10 20 30 40 0 20 40 60 80 0 40 80 120 160
CumulativeCost
EI TS KG MSEI PBGI
Figure9: RuntimecomparisonofPBGIagainstbaselinesforcomputingtheacquisitionfunctionon
theAckleybenchmarkacrossdifferentdimensions(d=4,8,16). WeseethatruntimeofPBGIis
slightlyslowerthanEIandTS,butsignificantlyfasterthanKGandMSEI.
pushingbenchmarkofAstudilloetal.[3],butwithonemodification: weusethedistancetraversed
bytherobotarmsinsteadofthedistancetheobjectsbeingmoved. Tounderstandtheeffectofthis
difference,weincludetheresultsforboththeunknown-costandknown-costversionsinFigure10.
D Additionalexperimentalresults
Here,weprovideadditionalexperimentalresultstobetterunderstandperformancedifferencesand
otheraspectsofpolicybehavior,includingtheeffectofvariousproblemhyperparameters.
D.1 Runtimecomparison
Here,weprovidearuntimecomparisonbetweenPBGIandvariousbaselines,includinginexpensive
baselinessuchasEIandTS,andexpensiveonessuchasMSEI.WedosointheAckleysynthetic
benchmarksetting,usingthesamehyperparametersettingsasthemainexperiments. Wemeasurethe
timetocomputeandoptimizetheacquisitionfunction.
ResultscanbeseeninFigure9. WeseethatPBGIisslightlyslowerthanEIandTS,butsignificantly
fasterthaneitherKGorMSEI,thoughtheruntimeofthelatterdecreasessubstantiallyasitaccumu-
latesmoredata. Overall,weconcludethatPBGI’sruntimeisclosertothatofclassicalacquisition
functionsthansophisticatedlookahead-basedvariants.
D.2 Effectofunknowncosts
TheRobotPushingempiricalbenchmarkinvolvestwocostfunctions: aknown-costvariantrepresent-
ingtotaloperationalduration,andanknown-costvariantrepresentingtotaldistancetraversed,aproxy
forenergyusesimilartothevariantconsideredbyAstudilloetal.[3]. Onecanthereforeask: how
differentistheresultingalgorithmbehaviorinthesetwosettings? Figure10showsthis: itreveals
thatfortheknown-costvariant,EIPCandPBGI-Dperformsimilarly,whereasfortheunknown-cost
variant,PBGI-Dachievesthebestperformanceonallexcepttheshortesttimehorizons,whereEIPC
isinsteadcompetitive. Otherbaselines,mostnotablyBMSEI,substantiallyunderperformEIPCand
PBGI-D,behavingsimilarlyinbothsettings.
D.3 Kernelandproblemhyperparameters
Choiceofkernel. TocheckwhetherourresultsaresensitivetothekernelusedfortheGaussian
processmodel,wereplicatedtheBayesianregretexperimentswithMatérnkernelswithsmoothness
parametersν = 3/2,5/2,aswellasthesquaredexponentialkernel,whichisthelimitofMatérn
kernelsasν [30].
→∞
Similar to the original results of Figure 4, we can clearly see from Figure 11 and Figure 12 that
behaviorsplitsintothreeregimes:
26
emitnuRKnownCosts UnknownCosts
10.5 EIPC
−
EIPC-U
11
− BMSEI
11.5 PBGI
−
PBGI-D
12
− RS
0 150 300 450 600 0 200 400 600 800
CumulativeCost
Figure10: ExperimentalresultsfortheRobotPushingempiricalbenchmark,withtheknown-cost
variant(left)andunknown-costvariant(right). Weseethatperformanceoverallissimilar,withEIPC
andPBGI-Dperformingstrongest. Theirrelativeperformanceissimilarintheknown-costvariant,
whereasintheunknown-costvariantPBGI-DoutperformsEIPConsufficiently-largehorizons,and
vice-versaonsufficientlysmallhorizons.
1. Easy: dsufficiently-small,mostpoliciesachievesimilarperformance.
2. Medium-hard: dmoderate-to-large,bothPBGIvariantsperformbetterthanbaselines.
3. Veryhard: dsufficientlylarge,nopolicyoutperformsrandomsearch.
We also see that d = 32 lands in the very-hard regime for the uniform-cost case but not for the
cost-awarecase: intuitively,thisoccursbecausecostscanreducetheeffectivevolumeofthesearch
space,sincehigh-costregionswithoutpromisingpointscanbeexcludedfromsearch.
Thisbehaviorisconsistentamongdifferentkernels,butwheretheexactthresholdatwhichregimes
switchdiffers. Inparticular,forthesquaredexponentialkernel,theseparationbetweenthemedium
and the hard regime appears earlier than for the other variants: all policies there have similar
performancetorandomsearchwhend=16.
Choiceoflengthscale. TocheckwhetherourresultsaresensitivetotheGaussianprocessmodel’s
lengthscale,wecompareκ=10 1withκ=5 10 1andκ=100. FromFigure13andFigure14,
− −
·
whichshowuniform-costandcost-awareresults,respectively,wecanseethatPBGIvariantsalso
havemuchbetterperformancesasthedimensionincreasesinbothscenarios. Sinceκ = 100 and
κ=5 10 1resultineasierproblemsthanκ=10 1,intheuniform-costcased=32landsinto
− −
×
themedium-hardregimeratherthanthevery-hardregime.
Syntheticbenchmarkdimension. Tobetterunderstandtheeffectofproblemdimensioninsettings
outsideofBayesianregret,werepeatthesyntheticbenchmarkexperimentswithd=4,d=8and
d=16. ResultsinFigure16. Sinced=4andd=8areeasiertosolve,herePBGIvariantsperform
comparablytobaselines.
27
eulaVdevresbOtseBMatérn-3/2 Matérn-5/2 Sq. Exp.
4 100
·
100
10 1
−
0 25 50 75 100 0 25 50 75 100 0 20 40 60 80
5 100
·
100
5 10 1
−
·
0 50 100 150 200 0 50 100 150 200 0 40 80 120 160
7 100
·
100
0 100 200 300 400 0 100 200 300 400 0 80 160 240 320
101
7 100
·
0 200 400 600 800 0 200 400 600 800 0 160 320 480 640
CumulativeCost
EI TS UCB KG
MSEI PBGI PBGI-D RS
Figure11: ComparisonofBayesianregretacrossGaussianprocesspriorswithdifferentkernelsover
differentdimensions,intheuniform-costsetting. Alllengthscalesareκ=10 1. Weseethatoverall
−
behavior is similar, but the precise thresholds at which each example switches between the easy,
medium-hard,andveryhardregimesdiffer.
28
tergeRgoL
d=4
d=8
d=16
d=32Matérn-3/2 Matérn-5/2 Sq. Exp.
4 100
·
100
0 25 50 75 100 0 25 50 75 100 0 50 100 150 200
5 100
·
2 100
·
0 50 100 150 200 0 50 100 150 200 0 40 80 120 160
7 100
·
3 100
·
0 100 200 300 400 0 100 200 300 400 0 80 160 240 320
101
3 100
·
0 200 400 600 800 0 200 400 600 800 0 160 320 480 640
CumulativeCost
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure12: ComparisonofBayesianregretacrossGaussianprocesspriorswithdifferentkernelsover
differentdimensions,inthecost-awaresetting. Alllengthscalesareκ=10 1. Weseethatoverall
−
behavior is similar, but the precise thresholds at which each example switches between the easy,
medium-hard,andveryhardregimesdiffer.
29
tergeRgoL
d=4
d=8
d=16
d=32κ=10 1 κ=5 10 1 κ=100
− −
·
4 100 100 100
· 10 1 10 −1
100 11 00 −− 32 11 00 −− 32
10− 4 10 −4
3 10 1 − 10 5
− −
·
0 25 50 75 100 0 15 30 45 60 0 10 20 30 40
5 100
· 100
100
10 1
10 1 −
−
10 2
10 2 −
−
100 10 3
10 3 −
−
0 50 100 150 200 0 30 60 90 120 0 20 40 60 80
7 100 7 100 5 100
· · ·
100
100 100 2 10 1
−
·
0 100 200 300 400 0 60 120 180 240 0 40 80 120 160
101 9 100 8 100
· ·
7 100 100 100
·
0 200 400 600 800 0 120 240 360 480 0 80 160 240 320
CumulativeCost
EI TS UCB KG
MSEI PBGI PBGI-D RS
Figure 13: Comparison of Bayesian regret across different length scales and dimensions, with a
Matérn-5/2kernel,intheuniform-costsetting. Weseesimilaroverallbehavior,buteachexample
switchesbetweentheeasy,medium-hard,andveryhardregimesatdifferentprecisethresholds.
30
tergeRgoL
d=4
d=8
d=16
d=32κ=10 1 κ=5 10 1 κ=100
− −
·
4 100
· 100 100
10 −1 10 −1
10 2 10 −2
100 −
0 50 100 150 200 0 40 80 120 160 0 25 50 75 100
5 100 4 100
· ·
100
100
100 5 10 1 10 1
− −
·
0 100 200 300 400 0 80 160 240 320 0 50 100 150 200
7 100 6 100 4 100
· · ·
2 100 2 100 100
· ·
0 200 400 600 800 0 160 320 480 640 0 100 200 300 400
101 9 100 8 100
· ·
3 100 3 100 3 100
· · ·
0 200 400 600 800 0 200 400 600 800 0 200 400 600 800
CumulativeCost
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure 14: Comparison of Bayesian regret across different length scales and dimensions, with a
Matérn-5/2 kernel, in the cost-aware setting. We see similar overall behavior, but each example
switchesbetweentheeasy,medium-hard,andveryhardregimesatdifferentprecisethresholds.
31
tergeRgoL
d=4
d=8
d=16
d=32d=4 d=8 d=16
100
10 1
−
10 2
−
102
101
100
10 1
−
105
104
103
102
0 10 20 30 40 0 20 40 60 80 0 40 80 120 160
NumberofFunctionEvaluations
EI TS UCB KG
MSEI PBGI PBGI-D RS
Figure15: Comparisonofregretforsyntheticbenchmarkfunctionsunderdifferentdimensions,inthe
uniform-costsetting. Weseethatallmethodsperformsimilarlyford=4,withdifferencesbetween
themost-competitivemethodsemergingasdimensionincreasestod=8andd=16.
32
tergeRgoL
Ackley
Levy
Rosenbrockd=4 d=8 d=16
4 100
·
100
2 10 1
−
·
102
101
100
105
104
103
0 25 50 75 100 0 50 100 150 200 0 100 200 300 400
CumulativeCost
EIPC EIPC-U BMSEI PBGI PBGI-D RS
Figure16: Comparisonofregretforsyntheticbenchmarkfunctionsunderdifferentdimensions,in
thecost-awaresetting. Weseethatallmethodsperformsimilarlyford=4,withdifferencesbetween
themost-competitivemethodsemergingasdimensionincreasestod=8andd=16.
33
tergeRgoL
Ackley
Levy
Rosenbrock