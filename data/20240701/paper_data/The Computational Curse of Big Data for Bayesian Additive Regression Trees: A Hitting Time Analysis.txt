The Computational Curse of Big Data for Bayesian Additive
Regression Trees: A Hitting Time Analysis
YanShuoTan1,OmerRonen*2,TheoSaarinen*2,andBinYu2,3,4
1DepartmentofStatisticsandDataScience,NationalUniversityofSingapore
2DepartmentofStatistics,UCBerkeley
3DepartmentofElectricalEngineeringandComputerSciences,UCBerkeley
4CenterforComputationalBiology,UCBerkeley
July1,2024
Abstract
Bayesian Additive Regression Trees (BART) is a popular Bayesian non-parametric regression model that is
commonly used in causal inference and beyond. Its strong predictive performance is supported by theoretical
guaranteesthatitsposteriordistributionconcentratesaroundthetrueregressionfunctionatoptimalratesundervarious
datagenerativesettingsandforappropriatepriorchoices.Inthispaper,weshowthattheBARTsampleroftenconverges
slowly,confirmingempiricalobservationsbyotherresearchers.Assumingdiscretecovariates,weshowthat,whilethe
BARTposteriorconcentratesonasetcomprisingalloptimaltreestructures(smallestbiasandcomplexity),theMarkov
chain’shittingtimeforthissetincreaseswithn(trainingsamplesize),underseveralcommondatagenerativesettings.
Asnincreases,theapproximateBARTposteriorthusbecomesincreasinglydifferentfromtheexactposterior(forthe
samenumberofMCMCsamples),contrastingwithearlierconcentrationresultsontheexactposterior.Thiscontrastis
highlightedbyoursimulationsshowingworseningfrequentistundercoverageforapproximateposteriorintervalsanda
growingratiobetweentheMSEoftheapproximateposteriorandthatobtainablebyartificiallyimprovingconvergence
viaaveragingmultiplesamplerchains.Finally,basedonourtheoreticalinsights,possibilitiesarediscussedtoimprove
theBARTsamplerconvergenceperformance.
1 Introduction
1.1 TheRiseofBART
Decision tree models such as CART (Breiman et al., 1984) are piecewise constant regression models obtained by
recursivelypartitioningthecovariatespacealongcoordinateaxes. TheyandtheirensemblessuchasRandomForests
(RFs)(Breiman,2001)andGradientBoostedTrees(GBTs)(Friedman,2001;ChenandGuestrin,2016)haveprovedto
beenormouslysuccessfulbecauseoftheirstrongpredictiveperformance(CaruanaandNiculescu-Mizil,2006;Caruana
et al., 2008; Fernández-Delgadoet al.,2014). Indeed, RFs andGBTs regularly outperformevendeep learningon
medium-sizedtabulardatasets(Grinsztajnetal.,2022). Nonetheless,thesetree-basedmethodsstillsufferfromseveral
notableproblems: Theyaredefinedviaalgorithmsratherthanviastatisticalmodels,soitisoftendifficulttoquantify
theuncertaintyoftheirpredictions; theyusegreedysplittingcriteria,sothereisnoguaranteefortheoptimalityof
thefittedmodel;RFsinparticulargrowtheirtreesindependentlyofeachother,thereforemakingthemstatistically
inefficientwhenfittedtodatawithadditivestructure(Tanetal.,2022a).
Toaddresstheseissues,Chipmanetal.(1998)proposedaBayesianadaptationofCART(BCART)andlateran
ensembleofBayesianCARTtrees,whichtheycalledBayesianAdditiveRegressionTrees(BART)(Chipmanetal.,
*Equalcontribution,alphabeticalordering
1
4202
nuJ
82
]LM.tats[
1v85991.6042:viXra2010). TheseareBayesiannon-parametricregressionmodels,whichputaprioronthespaceofregressionfunctions,
assumealikelihoodfortheobserveddata,andcombinethesetoobtainaposterior. InthecaseofBayesianCARTand
BART,priorsandposteriorsaresupportedonthesubspaceoffunctionsthatcanberealizedbydecisiontrees(ortheir
ensembles). SimilartoGaussianprocess(GP)regression,theposteriordistributioncanbeusedtoprovideposterior
predictivecredibleintervals. Ontheotherhand,unlikeGPregression,thereisnoclosedformformulafortheBART
posteriorandonehastosamplefromitapproximatelyviaaMarkovchainMonteCarlo(MCMC)algorithm.
BARThasbeenshownempiricallytoenjoystrongpredictiveperformancethatissometimesevensuperiortothatof
RFsandGBTs,especiallyafterhyperparameteroptimization(Hilletal.,2020). Naturally,ithasbecomeincreasingly
popularindiversefieldsrangingfromthesocialsciences(GreenandKern,2010;Yeageretal.,2019)tobiostatistics
(Wendlingetal.,2018;Starlingetal.,2020)andhasbeenparticularlyenthusedcausalinferenceresearchers(Hill,
2011;GreenandKern,2012;Kernetal.,2016;Dorieetal.,2019;Hahnetal.,2019). ExtendingandimprovingBART
methodologyremainsahighlyactiveareaofresearch,withmanyvariantsofthealgorithmproposedoverthelastfew
years(seeforinstanceLinero(2018);Pratola(2016);Pratolaetal.(2020);LuoandPratola(2023),aswellasthesurvey
Hilletal.(2020)andthereferencestherein.)
ThestrongpredictiveperformanceofBARTissupportedbyaburgeoningbodyoftheoreticalevidenceregarding
the BART posterior. Most significantly, researchers have shown that the BART posterior concentrates around the
trueregressionfunctionusedtogeneratetheresponsedataasn,thenumberoftrainingsamples,increases,withthis
concentrationhappeningatoptimalratesundervariousassumptionsonthesmoothnessandsparsityoftheregression
functionandforappropriatepriorchoices(RocˇkováandSaha,2019;RocˇkováandvanderPas,2020;LineroandYang,
2018;JeongandRockova,2020;RockovaandRousseau,2021;CastilloandRocˇková,2021). Achievingtheseoptimal
ratesdoesnotrequireanyoracleknowledgeorhyperparametertuning—insteadBARTautomaticallyadaptstothelevel
ofsmoothnessandsparsity,withtheformerevenhappeningatalocallevel(RockovaandRousseau,2021).
1.2 ObservedPoorMixingofBARTanditsSignificance
WhilethereisevidencethattheBARTposteriorenjoysfavorablepropertiesunderavarietyofsettings,thefactthat
wecanonlysampleapproximatelyfromtheposteriorviaMCMCcreatesagapinourunderstandingofhowandwhy
BARTworks. Specifically,ifthesamplerchaindoesnotconvergeefficientlytotheposteriordistribution,thatis,if
itdoesnotmixwell, theoutputofBARTalgorithmmaynotenjoythesamedesirableinferentialpropertiesasthe
BARTposterior. MostpopularBARTimplementationsuseremarkablysimilarsamplersbasedontheoriginaldesign
ofChipmanetal.(2010),whichusesaBayesianbackfittingapproachtoupdateonetreeatatimeviaproposedlocal
changestothetreenodescoupledwithaMetropolis-Hastingsfilter. Unfortunately,asdescribedbyHilletal.(2020),
“whilethisalgorithmisofteneffective,itdoesnotalwaysmixwell.” Indeed,poormixingforthissamplerhasbeen
empiricallydocumentedbymultiplesources(Chipmanetal.,1998;Carnegie,2019).
TheliteraturecontainsvarioussuggestionsonhowtoimprovethemixingtimefortheBARTsampler. Theseinclude
parallelization(Pratolaetal.,2014),modifyingtheMCMCproposalmoves(Wuetal.,2007;Pratola,2016;Kimand
Rockova,2023),warmstartsfromgreedilyconstructedtreeensembles(HeandHahn,2021),orrunningmultiplechains
(Carnegie,2019). Despitethisinterest,therehasbeenminimaltheoreticalworkdonetoquantifythemixingtimeandto
understandwhyandunderwhatsettingsslowmixingoccurs.
1.3 MainContributions
Inthispaper,weshowtheoretically,assumingdiscretecovariates,thattheBARTsampleroftenconvergesslowlytoits
posterior,confirmingtheempiricalobservationsofHilletal.(2020)andotherresearchers. Infact,theconvergence
unexpectedlybecomesworseasnincreases,incontrastwiththeposterior’sconcentrationtothetrueregressionfunction
becomingbetter.
Tostateourresultsmoreformally,notethataregressiontreeisparameterizedviaitstreestructure(whichfeatures
aresplitonandatwhichthresholds)anditsleafparameters(thefunctionvalueoneachleaf). BARTcombinesthese
parametersovermultipletreestoparameterizeatreeensemble(seeSection2.2.). Sincetheleafparameterscanbe
sampledinclosedformconditionallyontreestructures,theoriginalBARTsampler,underaslightmodification,canbe
thoughtofasaMarkovchainonthespaceoftreestructures.
2DataGeneratingProcess AllowedMoveset MultipleTrees LowerBound
Additive Full Yes Squareroot
Additive “Grow”and“Prune” Yes Polynomial
ContainsPureInteraction “Grow”and“Prune” Yes Squareroot
RootDependence “Grow”and“Prune” No Exponential
Table1: AsummaryoftheHPDRhittingtimelowerboundsprovidedbyourpaperandtheirdependenceonthetraining
samplesizen(lastcolumn). Thefirsttwolowerboundsapplytoadditivegenerativemodels. Thethirdappliestoa
settingwherethegenerativeregressionfunctioncontainsapureinteraction(definedinSection5.2.) Thefourthapplies
tothesettingwherethegenerativefunctionhas“rootdependence”,whichrepresentsaformofasymmetricdependence
onthefeatures(definedinSection5.3.) AlllowerboundsapartfromthefourthallowBARTtousemultipletrees.
We show that the BART posterior concentrates on a set comprising all optimal tree structures (i.e. those with
thesmallestbiasandcomplexity),whichalsoformsahighestposteriordensityregion(HPDR).Ontheotherhand,
theBARTsampler’shittingtimeforthissetincreaseswithnunderfourcommondatagenerativesettings. Inother
words,thesamplerrequiresmoreandmorestepstoreachanyoptimaltreestructure. Notethatthisisafrequentist
analysisandrequirestheassumptionofagenerativemodelforthedatathatcanandwillbedifferentfromtheBayesian
parameterization. OurhittingtimelowerboundsaresummarizedintheTable1.
WecomplementourtheoreticalanalysiswithacomprehensivesimulationstudyofBARTinvolvingawiderangeof
data-generatingprocesseswithcontinuouscovariates. Fromnowon,weusethetermapproximateposteriortorefer
tothedistributionobtainedfrom1000MCMCsamplesafteragenerousburn-in(5000iterationsasopposedtothe
defaultof100). Tocreateaproxyfortheexactposterior,wecombinesamplesfrommultiple(5)samplerchains,which
isknowntoimprovemixing(Carnegie,2019).1 Wecomparetheperformanceoftheapproximateandmulti-chain
approximateposteriorsviatwometrics: (i)theRMSEoftheirposteriormeanfunctionfromthetrueregressionfunction
onaheld-outtestset,(ii)theempiricalcoverageoftheirpointwisecredibleintervalsforthetrueregressionfunction. In
bothcases,therelativeperformancegapbetweentheapproximateandmulti-chainapproximateposteriors(measuredas
aratio)increasesasnincreases. ThesetwofindingsprovidefurtherevidencethattheapproximateBARTposterior
becomesincreasinglyandmeaningfullydifferentfromtheexactposteriorasnincreases. Wealsoperformtwoother
experimentstovalidatethisclaiminthesettingofourtheoreticallowerbounds. Ourtheoryandsimulationsthusecho
existingadvicethatBARTusersshouldnotblindlytakeitsposteriorcredibleintervalsatfacevalueandshouldrun
multiplechainswheneverfeasible.
Lastly, our theoretical results and their proof strategies yield insights on why the BART sampler has trouble
converging,whichleadsustosuggestpossiblewaystoimproveitsperformance. Mostimportantly,ourproofwill
showthatamajorreasonwhyhittingtimesgrowwithtrainingsamplesizeisbecausethe“temperature”oftheBART
sampleris inverselyproportional tothe trainingsample size. Thereisno needfor temperatureand samplesizeto
beintrinsicallytiedinthismanner,andweconjecturethatbuildinginmoreflexibletemperaturecontrol,suchasvia
simulatedtempering,mayhelptoacceleratemixing.
1.4 PriorWorkonMixingforBART
KimandRockova(2023)andourpriorwork(Ronenetal.,2022)soughttoanalyzemixingtimesfortheBayesian
CARTsampler. Bothmadethesurprisingdiscoverythatitsmixingtimecangrowexponentiallyinthetrainingsample
size,whentheonlyallowedmovesaregrowingnewleavesandpruningexistingones. KimandRockova(2023)studied
thisinaone-dimensionalsettingandfurthershowedthat,withamoreaggressivemoveset,BayesianCARTconstrained
todyadicsplitshasamixingtimeupperboundthatislinearinthesamplesize. Ronenetal.(2022)’sproofstrategywas
toshowthatthisMarkovchainhasabottleneckstate—thetrivialtreecomprisingasinglenode. Thisbottleneckarises
becausewhenBayesianCARTmakesawrongfirstsplitfollowedbyotherinformativesplits,theonlywaytoreverse
thewrongfirstsplitinvolvespruningtheinformativesplits,whichbecomesincreasinglydifficultasthetrainingsample
sizeincreases.
1Carnegie(2019)investigatedtheuseof1,4,and10chains.Takingreferencefromthis,weused5chainsbutdidnotinvestigatetheeffectof
increasingthenumberofchainsbeyondthisvalue.WebelievethattheoptimalnumbercouldpossiblyvarywiththesamplesizeandDGP.
3Whenwesubmittedourpriorworktoapeerreview,somereviewersrightlypointedoutthatourmixingtimelower
boundsmaybemisleadingtopractitioners. Thisisbecausedifferenttreestructurescouldrealizethesamepartitionof
thecovariatespaceandhenceimplementthesameregressionfunction. Inotherwords,theBayesianCARTmodelis
notidentifiableattheleveloftreestructures,whichmeansthatfailuretomixinthisspaceoftreestructuresmaybe
benign(RednerandWalker,1984). Itdoesnotreflectafailuretomixatthelevelofregressionfunctions,letaloneany
degradationintheinferentialpropertiesofthealgorithm’soutput.
Figure1: TheBayesianCARTmodelisnotidentifiableattheleveloftreestructures. Thetwotreestructuresshownon
theleftbothrealizethesamepartitionofthecovariatespaceandeventhesameregressionfunction,whichisshownon
theright.
Afurtherlimitationofourprioranalysisisthatmixingtimeisdefinedastheworstcasetimetoconvergenceaswe
varyoverallpossibleinitialstates. SinceBayesianCARTisalwaysinitializedatthetrivialtree,alowerboundonthis
worstcasequantitymaybeunreasonablypessimistic.
Theseissueswithrelevanceareresolvedbyconsideringhittingtimes.
2 Data Generation Models for BART and for Frequentist Analysis
2.1 GenerativeModelforFrequentistAnalysis
We assume a d-dimensional discrete covariate space X = {1,2,...,b}d and a regression function f∗: X → R.
SupposethatweobserveatrainingdatasetD comprisingnindependentandidenticallydistributedtuples(x ,y )with
n i i
x ∼ν andy =f∗(x )+ϵ fori=1,2...,n. Here,ν isameasureonX withfullsupport,whileϵisasub-Gaussian
i i i i
randomvariable. Fornotationalconvenience,wewilluseXtodenotethen×dmatrixformedbystackingthecovariate
vectorsasrows. Wewillalsousey,f∗,andϵtodenotethen-dimensionalvectorsformedbystackingtheresponsesy ,
i
thefunctionvaluesf∗(x )andthenoisecomponentsϵ respectively. Similarly,wewilldenoteallvectorsandmatrices
i i
withboldfacenotation,withsubscriptsreferencingtheindexofthevector. Vectorcoordinateswillbedenotedusing
regularfont. WewilldenoteprobabilitiesandexpectationswithrespecttoD usingP andE respectively.
n n n
2.2 BayesianModelSpecificationforBART
WefirstdescribetheversionofBARTthatweanalyzeinourpaper,beforediscussingitsdifferenceswiththeversion
describedbyChipmanetal.(2010)andwhichisstillmostcommonlyusedinpractice.
4Regressiontrees. Abinaryaxis-alignedregressiontreeisparameterizedbyatuple(T,µ). Here,Treferstothetree
structure,whichspecifiesthetopologyofthetreeasarootedbinarytreeplanargraphand,givenanorderingofthe
graph’svertices(e.g. viabreadth-firstsearch),specifiesthesplittingruleforeachinternalnodej. Notethatthesplitting
rulecomprisesafeaturev andathresholdt . TheleavesL ,L ,...,L ofTthuscorrespondtorectangularregions
j j 1 2 b
ofthecovariatespacethattogetherformapartitionofthespace. Weletµ∈Rbbeavectorofleafparameters,onefor
eachleafofT. Together,(T,µ)specifyapiecewiseconstantfunctiongthatoutputs
g(x;T,µ)=µ ,
l(x)
wherel(x)istheindexoftheleafcontainingx.
Sum-of-treesmodel. GivenobserveddataD ,theBARTmodelpositsy =f(x )+e fori=1,2...,n,where
n i i i
e ,e ,...,e ∼ N(0,σ2),andf isasumoftheoutputsofmtrees:
1 2 n i.i.d.
f(x)=g(x;T ,µ )+g(x;T ,µ )+···+g(x;T ,µ ).
1 1 2 2 m m
Wedenotetheorderedtuple(T ,T ,...,T )byEandcallitatreestructureensemble(TSE).Weshallabusenotation
1 2 m
anduseµtorefertotheconcatenationofµ ,µ ,...,µ . Notethat, whenconditionedonEandX, thisisjusta
1 2 m
Bayesianlinearregressionmodel. Toseethis,letΨdenotethen×bmatrixwhosecolumnsaretheindicatorvectors
overthetrainingsetofeachleafinE. Wethenhave
y=Ψµ+e. (1)
Priors. WeassumeafixedpriordistributionponΩ ,thespaceofTSEswithmtrees.2 ConditionedonaTSEE,
TSE,m
theconditionalpriordistributionontheleafparametersisanisotropicGaussian,i.e. p(µ|E)∼N(0,(σ2/λ)I ),where
b
bisthetotalnumberofleavesinalltreesinE. Bothσ2andλareassumedtobefixedhyperparameters,withσ2taking
thesamevalueasthatusedinthevarianceoftheadditivenoisee ,whileλisamodulationparameterthatshouldbeset
i
toapproximatelythereciprocalofthesignal-to-noiseratio.
Differenceswithin-practiceBART. Chipmanetal.(1998)proposedapriorontreestructuresdefinedintermsofa
stochasticprocess. Startingfromasinglerootnode,theprocessrecursivelysplitseachnodeatdepthdwithprobability
α(1+d)−β,whereαandβ arehyperparameterswithdefaultvaluesα=0.95andβ =2respectively. Featuresand
thresholds for splits are selected uniformly at random. Chipman et al. (2010) extended this to a prior on TSEs by
independence,i.e. p(E) =
(cid:81)m
p(T ). Afterrescalingtheresponsevariabletoliebetween−0.5and0.5,theleaf
j=1 j √
parameterstandarddeviationissettobeσ =0.5/k m,wherekisafurtherhyperparameterwithdefaultvaluek =2.
µ
Finally,aninverse-χ2 hyperpriorisplacedonthenoisevarianceσ2 andiscalibratedtotheobserveddata. Thislast
assumptionistheonlywayinwhichtheBARTmodelwestudyinthispaperdepartsfromthatinChipmanetal.(2010).
Wemakethischangeforanalyticaltractabilityandbelieveittobeminor,sincesimulationsshowthattheposterioron
σ2quicklyconvergestoafixedvalueandourtheoreticalguaranteesholdforanyfixedchoiceofσ2.
3 Sampling from BART via MCMC
3.1 TheIn-PracticeBARTSampler
ThesamplerproposedbyChipmanetal.(2010)canbedescribedasa“Metropolis-within-GibbsMCMCsampler”(Hill
etal.,2020). Moreprecisely,foreachouterloopofthealgorithm,ititeratesoverthetreeindicesj =1,2,...,mand
updatesthej-thpair(T ,µ )usinganapproximatedrawfromtheconditionaldistributionp(T ,µ |E ,µ ,y,σ2),
j j j j −j −j
whereE andµ refertotheconcantenationofcurrenttreestructuresandleafparametervectorsrespectively,each
−j −j
2Becausetheemphasisinourtheoryisonthedependenceofhittingtimesontrainingsamplesizeinlargesamples,thespecificformoftheprior
holdsnobearingonourresults.
5withthej-thindexomitted.3 Asafinalstepintheloop,itupdatesσ2usingadrawfromitsfullconditionaldistribution
p(σ2|E,µ,y).
To describe how to sample (approximately) from p(T ,µ |E ,µ ,y,σ2), we first describe Chipman et al.
j j −j −j
(1998)’salgorithmforBayesianCART,i.e. whentheensemblecomprisesasingletree. Inthiscase,wefirstfactorize
theposteriorintoaconditionalposterioronleafparametersandamarginalposteriorontreestructures:4
p(T,µ|y,σ2)=p(µ|T,y,σ2)p(T|y,σ2). (2)
Thefirstmultiplicandontheright,p(µ|T,y,σ2),isamultivariateGaussian(withdiagonalcovariance)andcanbe
sampledfromdirectly. Thesecondmultiplicandisproportionaltop(y|T,σ2)p(T),whichistheproductbetweena
marginallikelihoodofaBayesianlinearregressionmodelandtheprior. Themarginallikelihoodcanbecomputedusing
standardtechniques,buttheposteriorcannotbesampleddirectly. Assuch,aMetropolis-Hastingssamplerisusedwith
thefollowingtypesofproposedmoves:
1. Pickaleafinthetreeandsplitit(grow);
2. Picktwoadjacentleavesandcollapsethembackintoasingleleaf(prune);
3. Pickaninteriornodeandchangethesplittingrule(change);
4. Pickapairofparent-childnodesthatarebothinternalandswaptheirsplittingrules,unlessbothchildrenofthe
parenthavethesamesplittingrules,inwhichcase,swapthesplittingruleoftheparentwiththatofbothchildren
(swap).
Notethatallselectionsintheseproposedmoves(ofnodes,splittingrules,etc.) aremadeuniformlyatrandomfromall
availablechoices.5 Theproposedmovetypesarechosenwithprobabilitiesπ ,π ,π ,andπ respectively. LetQ(−,−)
g p c s
denotethetransitionkerneloftheproposal,i.e. Q(T,T∗)istheprobabilityoftreestructureT∗beingproposedgiven
currenttreestructureT. WithTandT∗ thusdefined,theMetropolis-Hastingsalgorithmacceptstheproposalwith
probability
(cid:26) Q(T∗,T)p(T∗|y,σ2) (cid:27)
α(T,T∗):=min ,1 .
Q(T,T∗)p(T|y,σ2)
Asimplereparameterizationtrickisusedtoadaptthissamplertothecasewhentheensemblehasmultipletrees.
BecauseoftheindependenceofthepriorsondifferenttreesandtheGaussianlikelihood, theconditionalposterior
p(T ,µ |E ,µ ,σ2,y)canberewrittenintermsoftheresidualvector
j j −j −j
(cid:88)
r :=y− g(X;T ,µ ).
−j k k
k̸=j
Specifically,wehave
p(T ,µ |E ,µ ,σ2,y)=p(T ,µ |r ,σ2),
j j −j −j j j −j
wheretheright-handsideisthesingle-treeposterior. AsingleMetropolis-Hastingsupdatestepasdescribedaboveis
performedtodrawanapproximatesample(T ,µ )fromtheconditionalposterior.
j j
3.2 TheAnalyzedBARTSampler
TheBARTsamplerdescribedaboveisdifficulttoanalyzebecausethedeterministicGibbsouterloopmakesitatime-
varyingMarkovchain. Moresignificantly,itisconvenientinanalyzingBayesianCARTtocollapsetheMarkovchain
statespacebymarginalizingouttheleafparameters. Thecollapsedstatespaceissimplythespaceoftreestructures,
whichisdiscreteandfinite. However,weareunabletodothisforBARTingeneralbecauseoftheconditioningonthe
3NotethattheconditionaldistributionisofcoursealsoconditionalontheobservedcovariatedataX.However,sincethisisalwaysconditioned
upon,weomititfromournotationtoavoidclutter.
4Chipmanetal.(1998)’sformulationoftheBayesianCARTsamplermarginalizesoutσ2insteadofconditioningonit.Asthisisnolongerdone
forBART,weomitdiscussingittoavoidconfusingreaders.
5Splitsthatresultinemptyleavesarenotallowed.
6residualsfromothertreesintheinnerloop. Bothofthesedifficultiesmakeitimpossibletoapplystandardtechniquesin
Markovchaintheory.
Toovercomethis,weproposeanadaptationofthesamplerthatbringsitclosertoBayesianCART.First,weimitate
(2)andfactorizetheposteriorintoaconditionalposterioronleafparametersandamarginalposteriorontreeensemble
structures:
p(E,µ|y)=p(µ|E,y)p(E|y).
TheconditionalposterioronleafparametersisstillamultivariateGaussianandcanbesampledfromdirectly,while
themarginalposteriorontreeensemblestructuresremainsproportionaltotheproductofthemarginallikelihoodofa
Bayesianlinearregressionmodelandtheprior:p(E|y)∝p(y|E)p(E)(see(1).)Tosamplefromthismarginalposterior,
werunMetropolis-HastingsMCMCsimilarlytobefore. However,insteadofcyclingdeterministicallythroughthe
trees in an inner loop as before, we pick a tree index uniformly at random. We propose an updated tree using the
sametransitionkernelQ(−,−),butwritetheacceptanceprobabilityintermsofthefullmarginalposteriorinstead
ofconditioningontheresidualsfromothertrees. Forfurtherclarity,thealgorithmissummarizedinpseudocodeas
Algorithm1.
WedenotethetransitionkernelofthesamplerusingP(−,−),and,toavoidconfusionwithrandomnessarisingfrom
samplingthetrainingset,wewilldenoteallprobabilitiesandexpectationswithrespecttothealgorithmicrandomness
usingP andE respectively.
Algorithm1BARTsampler.
1: BART(D n:data,m:no.oftrees,σ2:guessfornoisevariance,λ:guessforreciprocalSNR,π:proposalprobabilities,p TSE:
TSEprior,t :no.ofsampleriterations)
max
2: InitializeT 1,T 2,...,T mastrivialtrees.
3: fort=1,2,...,t max
4: Samplek∼Unif({1,2,...,m}).
5: ProposeT∗ ∼Q(T k,T∗).
6: Setα(T k,T∗)=min(cid:110) QQ (( TT k∗, ,T Tk ∗)) pp (( TT 11 ,, .. .. .. ,, TT kk −− 11 ,, TT k∗, ,T Tk k+ +1 1, ,. .. .. ., ,T Tm m| |y y) ),1(cid:111) .
7: SetT k =T∗withprobabilityα(T,T∗).
4 BIC for BART and Posterior Concentration on Optimal TSEs
ThegoalofthissectionistofirstshowhowtoquantifythebiasandcomplexityofaTSEseparatelyandthenjointly
viaBIC.Wewillthenshowthat,asafunctionofTSEs,theposteriorprobabilityp(E|y)concentratesonthesetof
TSEswithzerobiasandthelowestpossiblecomplexity,andarethereforeminimizersofBIC.Assuch,asarguedin
theintroduction,thehighestposteriordensityregioncontainsallofthemostdesirableTSEs. Thisimpliesthatlower
boundsonthehittingtimesofthisregionreflectcomputationaldrawbacksofpracticalconsequence.
4.1 MeasuringBiasandComplexityforTSEs
WefirstdiscusshowtoquantifythebiasandcomplexityofaTSE.
Partitions. AcellC isarectangularregionofX,i.e.
C ={x∈X : a <x ≤b fori=1,...,d},
i i i
withlowerandupperlimitsa andb respectivelyincoordinateifori = 1,2,...,d. Apartitionisacollectionof
i i
disjointcellsC ,...,C whoseunionisthewholespaceX. EverytreestructureTinducesapartitionP viaitsleaves.
1 b
NotonlyisP asufficientstatisticforT,italsocompletelycharacterizesthebiasandcomplexityoftheresultingdata
modelconditionedonT. Indeed,thisdatamodelisjustBayesianlinearregressionontheindicatorfunctionsonthe
leavesofT. Sincethefunctionsareorthogonal,thedegreesoffreedomoftheregressionisequaltothesizeofthe
partition.
7Partitionensemblemodels(PEMs). ATSEEinducesanensembleofpartitionsP ,P ,...,P . Indeedthedata
1 2 m
modelconditionedonEisstillaBayesianlinearregressionontheindicatorfunctionsofallleavesinE. However,these
indicatorsarenolongerorthogonal,whichmeansthatdifferentensembles,withpossiblydifferentnumbersofleaves,
cangiverisetothesamesubspaceofregressors,makingEnotidentifiablefromdata. Toavoidthisissue,wedirectly
considerthesubspaceoftheregressionfunction. Formally,letV⊂L2(Xd,ν)bethesubspacespannedbyindicators
ofthecellsinP forj =1,2,...,m. Wecallthisthepartitionensemblemodel(PEM)associatedtotheTSEEand
j
indicatethisassociationviathemappingV=F(E).
Measuringbias. LetΠ denoteorthogonalprojectionontoF(E)inL2(ν). Wedefinethesquared(mangitudeofthe)
E
biasofEwithrespecttoaregressionfunctionf as
(cid:90)
Bias2(E;f):= (f −Π [f])2dν. (3)
E
ThisispreciselythesquaredbiasofBayesianlinearregressiononEifweignoretheregularizationeffectfromtheleaf
parameterpriors,whichisinconsequentialinlargesamplesizes.
Measuringcomplexity. WhenconditionedonaTSEEandignoringregularizationfromleafparameterpriors,the
degreesoffreedomoftheresultingBayesianlinearregressionmodelisjustthedimensionofF(E). Wedenotethisby
df(E)anduseitasameasureofcomplexityofE. Notethatthisdefinitiondoesnotdependonthecovariatedistribution
ν (seeLemmaJ.2intheappendix.)
Function dimension and optimal sets. Excessive complexity leads to overfitting and is hence undesirable. To
quantifytheexcess,wefirstdefinethem-ensembledimensionofaregressionfunctionf as
dim (f):=min{df(E): f ∈F(E)andE∈Ω }. (4)
m TSE,m
Inlargesamplesizesn,whichisthesettingweareconcernedwith,theTSEsthatresultinthesmallestMSEmustbe
bias-free. WehencedefinethesetofoptimalTSEsinΩ tobetheminimizersof(4). Moregenerally,wedefinea
TSE,m
seriesofnestedsetswithincreasinglevelsofsuboptimalitytolerancevia:
OPT (f,k):={E∈Ω : f ∈F(E)anddf(E)≤dim (f)+k}.
m TSE,m m
4.2 BICandBARTPosteriorConcentration
TheBayesianinformationcriterion(BIC)(Schwarz,1978)ofaTSEEisgivenby
BIC(E)=
yT(I−P E)y +df(E)logn+log(cid:0) 2πσ2(cid:1)
n.
σ2
Here,P referstoprojectionontoF(E)withrespecttotheempiricalnorm∥·∥ (realizedasamatrix.) Ignoringthe
E n
effectofthenoisevector fornow,weseethatthefirstterm,dividedbythesamplesizen,isanestimateforthesquared
bias. Meanwhile,thesecondtermdirectlymeasuresthemodelcomplexity. Hence,BICquantifiesthequalityofa
TSEbyaccountingforbothbiasandcomplexity. Indeed,underourdatagenerativemodel(Section2.1)wehavethe
followingconcentrationlemma:
Proposition4.1(ConcentrationofBICdifferences). ConsidertwoTSEsEandE′anddenotethedifferenceintheir
BICvaluesas∆BIC(E,E′)=BIC(E)−BIC(E′). Thenforany0<δ <1,withprobabilityatleast1−δwithrespect
toP ,wehave
n
∆BIC(E,E′)= n (cid:0) Bias2(E;f∗)−Bias2(E′;f∗)(cid:1) +O(cid:16)(cid:112) nlog(1/δ)+log(1/δ)(cid:17) . (5)
σ2
Iffurthermore,bothTSEshavethesamebias,i.e. Π [f∗]=Π [f∗],thenwehave
E E′
∆BIC(E,E′)=logn(df(E)−df(E′))+O(log(1/δ)). (6)
8Fromthisproposition,wealsoseethatOPT (f∗,k)fork = 0,1,2,...arejustsublevelsetsofBICwhennis
m
largeenough. WenextshowthatBICiscloselyconnectedtothelogmarginallikelihoodforTSEsasfollows:
Proposition4.2(LogmarginallikelihoodandBIC). ConsideraTSEE. Thenforany0<δ <1,thereisaminimal
samplesizeN sothatforalln ≥ N,withprobabilityatleast1−δ withrespecttoP ,thelogmarginallikelihood
n
satisfies
BIC(E)
logp(y|E)=− +O(1).
2
Consequently,thelogmarginalposterioralsosatisfies
BIC(E)
logp(E|y)=− −logp(y)+O(1).
2
ThisalmostlinearrelationshipimpliesthatOPT (f,k)fork =0,1,2,...arealsosuperlevelsetsofthemarginal
m
posterior. Inotherwords,theyformhighestposteriordensityregions(HPDR).Asadvertised,thesewillbethetarget
setsofourhittingtimeanalysis. Finally,combiningtheprevioustwopropositionsgivesthefollowingresultonposterior
concentration.
Proposition4.3(BARTposteriorconcentration). Forany0<δ,ϵ<1,thereisaminimalsamplesizeN sothatforall
n≥N,withprobabilityatleast1−δwithrespecttoP ,themarginalposteriormeasureonΩ satisfies
n TSE,m
p(OPT (f∗,0))|y)>1−ϵ.
m
5 Hitting Time Lower Bounds for BART MCMC
OurprimaryfindingsconsistoflowerboundsonHPDRhittingtimesforBART,exploredacrossfourdistinctsettings
forBARTandthedatageneratingprocess(DGP).Asdiscussedintheprevioussection,theseregionsalsocomprise
sublevelsetsofBIC.Wefirstdefinehittingtimesinageneralsetting.
Hittingtimes. Let(X )beadiscretetimeMarkovchainonafinitestatespaceΩ. LetA⊂Ωbeasubset. Thehitting
t
timeofAisdefinedas:
τ :=min{t≥0: X ∈A}.
A t
Notethatthisisarandomvariableandthatit,inprinciple,dependsontheinitialstateX . Inouranalysis,theinitial
0
stateisalwayschosentobeanensembleoftrivialtreesandsowillnotbereferencedinthenotationtoavoidclutter.
5.1 SquareRootandPolynomialLowerBoundsforAdditiveModels
OurfirsttwolowerboundsareforthesettingwheretheDGPisanadditivemodel.
Theorem5.1(Lowerboundforadditivemodel). Letf∗beanadditivefunction,i.e.
f∗(x)=f (x )+f (x )+···+f (x )
1 1 2 2 m′ m′
withm′ ≥2. Supposex ,x ,...,x areindependent. Supposem≤m′,andwemakearbitrarychoicesfortheother
1 2 m′
BARThyperparametersσ2,λ,π,p . Thenforany0 < δ < 1,thereisaminimalsamplesizeN sothatforall
TSE
n≥N,withprobabilityatleast1−δwithrespecttoP ,theMarkovchaininducedbyBART(D ,m,σ2,λ,π,p ,−)
n n E
satisfies
E(cid:8)
τ
(cid:9) =Ω(cid:16) n1/2(cid:17)
,
OPTm(f∗,(qmax−2)(qmin−2)−2)
whereq =max dim (f ),q =min dim (f ).
max 1≤i≤m′ 1 i min 1≤i≤m′ 1 i
Iffurthermore,m<m′andwedisallow“change”and“swap”moves,i.e. π =π =0,thenwehave6
c s
E(cid:8)
τ
OPTm(f∗,qmax−qmin−1)(cid:9) =Ω(cid:16) nqmin/2−1(cid:17)
.
6RecallthatOPTm(f∗,0)⊂OPTm(f∗,k)fork≥0.SinceOPTm(f∗,k)=∅fornegativek,thesestatementscanbeinterpretedasbeing
meaninglessunlessqmaxandq minsatisfytherelevantconstraints.
9Additivemodelsarenaturalgeneralizationsoflinearmodelsthathavebeenwidelystudiedinstatisticsandmachine
learning(HastieandTibshirani,1986;Hastieetal.,2009). Whenfittedtorealworlddatasets,theyoftenenjoygood
predictionaccuracy. Hence,weviewanadditivegenerativemodeltobeanaturalclassoffunctionsforourstudy.
Furthermore,severalworksderivingconsistencyguaranteesforfrequentistgreedydecisiontreesandrandomforests
haveusedadditivemodelsasagenerativefunctionclass,astheassumptionofadditivityhelpstocircumventsome
ofthepracticalandtheoreticaldifficultiesarisingfromgreedysplitting(Scornetetal.,2015;Klusowski,2021). On
the other hand, Tan et al. (2022a) showed generalization lower bounds for decision trees in this setting, with the
recommendationthatmodelswithmultipletreesfitadditivemodelsbetterthanthosecomprisingasingletree(seealso
Tanetal.(2022b).) Indeed,giventheassumptionsofTheorem5.1,wehavedim (f∗) > dim (f∗)foranyl < m,
l m
whiledim (f∗)=dim (f∗)foranyl≥m(seePropositionF.7.) Inotherwords,aminimumBICvalueisachievable
l m
ifandonlyifthenumberoftreesintheBARTmodelislargerthanorequaltothenumberofcomponentsintheadditive
model.
However,Theorem5.1tellsusthatevenwhentheBARTmodeliscorrectlyspecifiedandcontainsanefficient
representationoff∗,i.e. whenm′ =m,theBARTsamplermayfailtoreachaTSEimplementingsucharepresentation
withinareasonabletime. Thisaddsanotherperspectivetorecentargumentsthattheremaybevalueinoverparameteri-
zation(seeforinstanceBartlettetal.(2020).) Allowingformoretreesthanthenumberofadditivecomponentsmay
empowerthesamplerwithmorefreedomofnavigationtoavoidpotentialcomputationalbottlenecks. Weconfirmthis
conjectureempiricallyinoursimulationssection.
5.2 SquareRootLowerBoundforPureInteractions
Ournextlowerboundisforthesettingwhenthedatageneratingprocesshasapureinteraction,whichwedefineas
follows. Letx andx betwofeatures, i.e. componentsofx ∼ ν. Wesaythattheyforma pureinteractionwith
i j
respecttoaregressionfunctionf∗iftheyarejointlydependentwiththeresponsey,butareseparatelyconditionally
independentofyforanyconditioningsetofindicesI unlessitincludestheother’sindex. Mathematically,wecanwrite
thisasfollows:
• (x ,x )̸⊥⊥y;
i j
• x ⊥⊥y|x andx ⊥⊥y|x foranyI ⊂{1,2,...,d}suchthati,j ∈/ I.
i I j I
Acanonicalexampleofapureinteractionistheexclusive-or(XOR)functionoverbinaryfeatureswithauniform
distribution, i.e. f(x) = x x forX = {−1,1}d. Thisfunctioniswell-knowntobedifficulttolearnusingeither
1 2
CART(SyrgkanisandZampetakis,2020;MazumderandWang,2024)orneuralnetworks(Abbeetal.,2022). Assuch,
itisperhapsunsurprisingthattheBARTsampleralsoexperiencesdifficultiesinthissetting.
Theorem5.2(Lowerboundforpureinteraction). Letf∗containapureinteraction. Supposewedisallow“change”
moves,i.e. π =0,andwemakearbitrarychoicesforallotherBARThyperparametersm,σ2,λ,π ,π ,π ,p .
c g p s TSE
Thenforany0<δ <1,thereisaminimalsamplesizeN sothatforalln≥N,withprobabilityatleast1−δwith
respecttoP ,theMarkovchaininducedbyBART(D ,m,σ2,λ,π,p ,−)satisfies
n n E
E(cid:8)
τ
(cid:9) =Ω(cid:16) n1/2(cid:17)
,
OPTm(f∗,∞)
whereOPT (f∗,∞):=∪∞ OPT (f∗,k).
m k=0 m
NotethatthesuboptimalitygapforTheorem5.2ismuchwiderthanthatforTheorem5.1. Indeed,itimpliesthat
theonlyTSEsthatarereachablewithino(cid:0) n1/2(cid:1)
iterationsofthesamplerhavenonzerobias. Assuch,theMSEof
theBARTsampleroutputdoesnotevenconvergetozerowiththetrainingsamplesize,unlessweallowforΩ(n1/2)
iterationsofthesampler.
5.3 ExponentialLowerBoundforBayesianCART
OurfinalhittingtimelowerboundshowsthattheHPDRhittingtimeforBayesianCARTcanbeexponentialinthe
trainingsamplesize. ThiscomplementsandimprovesourresultsinRonenetal.(2022),whichprovidedanexponential
10lowerboundformixingtimeforBayesianCART.Whilethepreviousresultreliedonextremelyweakassumptions,the
improvedversionrequiresanewassumptionontheasymmetryoftheregressionfunctionf∗intermsofitsdependence
ondifferentfeatures. Specifically,wesaythataregressionfunctionf∗hasrootdependenceifthereexistsafeaturex
i
andthresholdtsuchthat:
• Corr2(y,1{x ≤t})>0;
i
• (i,t)doesnotoccurasarootsplitonanytreestructureT∈OPT (f,0).
1
An example of such a function is the “staircase” function f∗(x) = (cid:80)s (cid:81)j 1{x >1}. Any feature x for
j=1 k=1 k i
2 ≤ i ≤ ssatisfiestheabovetwoproperties. Ontheotherhand,additivefunctionsonindependentfeaturesdonot
satisfytheseproperties.
Theorem5.3(LowerboundforBayesianCARTwithrootdependence). Supposef∗hasrootdependence. Suppose
m=1andthatwedisallow“change”and“swap”moves,i.e. π =π =0. Supposewemakearbitrarychoicesforall
c s
otherBARThyperparametersσ2,λ,π ,π ,p . ThentheMarkovchaininducedbyBART(D ,1,σ2,λ,π,p ,−)
g p TSE n E
satisfies
(cid:40) logE(cid:8) τ (cid:9)(cid:41) 1 (cid:32) (cid:90) (cid:18)(cid:90) (cid:19)2(cid:33)
liminfE OPT1(f∗,0) ≥ (f∗)2dν− f∗dν .
n→∞ n n 2σ2
Remark5.4. OurhittingtimelowerboundsdirectlyimplymixingtimelowerboundsinthespaceofPEMs,which,
as argued in Section 4, are identifiable from data. Since the bias and degrees of freedom of a TSE E is defined in
termsofitsassociatedPEMF(E),OPT m(f∗,k)isthepreimageunderF ofasetO(cid:103)PT m(f∗,k)inthespaceofPEMs.
Hence,ahittingtimelowerboundforOPT m(f∗,k)issimultaneouslyahittingtimelowerboundforO(cid:103)PT m(f∗,k)
whenconsideringtheinducedMarkovchainonthespaceofPEMs. Itiseasytoseethatthisisalowerboundforthe
mixingtime.
Remark 5.5. For the sake of narrative clarity, we have not tried to optimize the suboptimality gaps (i.e. k in
OPT (f∗,k))inourlowerbounds. Notealsothatwehavenotattemptedtoinvestigatethedependenceofourlower
m
boundsonotherdataoralgorithmichyperparameters. TheseofcourseinfluencetheminimumsamplesizeN aswellas
thehiddenconstantfactorintheBig-Omeganotationofourlowerbounds.
6 Hitting Time Lower Bounds via Barrier Sets
Inthissection,webrieflyoutlinetheproofstrategyweusetoderiveourhittingtimelowerbounds. Ourfirstmoveisto
makeuseofthestandardinterpretationofasymmetricMarkovchainasarandomwalkonanetwork,whosevertices
comprisethestatesoftheMarkovchainandwhoseedgescomprisepairsofstateswithpositivetransitionprobability.
Wenextnoticethathittingtimesarecloselyrelatedtoescapeprobabilities,whichcanbeinterpretedasvoltagesonthe
network. Voltagescanthenbecalculatedusingstandardnetworksimplificationtechniques. Puttingtheseingredients
togethercreatesthefollowingrecipeforderivinghittingtimelowerbounds:
Proposition6.1(Recipeforhittingtimelowerbounds). LetE ,E ,E ,...denotetheMarkovchaininducedbyarun
0 1 2
ofBART(D ,m,σ2,λ,π,p ,−)foranyfixeddatasetD andanychoiceofhyperparameters. LetE ∈Ω be
n E n bad TSE,m
aTSEsuchthat,forsome0<δ <1,withprobabilityatleast1−δwithrespecttoP ,
n
(cid:8) (cid:9)
P τ <τ =Ω(1).
Ebad OPTm(f∗,k)
LetB ⊂Ω beasubsetsuchthateverypathfromE toOPT (f∗,k)intersectsB. Thenwithprobabilityatleast
TSE,m bad m
1−2δwithrespecttoP ,thehittingtimeofOPT (f∗,k)satisfies
n m
(cid:18) (cid:18) (cid:19)(cid:19)
(cid:8) (cid:9) 1
E τ =Ω exp min∆BIC(E,E ) .
OPTm(f∗,k) 2 E∈B bad
11Figure 2: Visual illustration of Proposition 6.1. The chain is initialized at E and with positive probability hits a
0
suboptimalTSEE beforeOPT (f∗,k). Thiscausestochaintogetstuck,asitcanonlyreachOPT (f∗,k)by
bad m m
passingthroughan“impassable”barriersetB.
Inotherwords,ourhittingtimelowerboundsfollowimmediatelyonceweareabletoidentify(i)asuboptimal
TSE E that is reachable by the sampler before it hits OPT (f∗,k) and (ii) a barrier B that separates E from
bad m bad
OPT (f∗,k),i.e. isavertexcutset,andwhichhashigherBIC,thereforeactingasanimpassable“barrier”. Wenow
m
brieflydescribeourchoicesforeachofthethreedifferentlowerboundsettingswehaveconsideredinthispaper.
Additivemodels. Forsimplicity,weonlydiscussthecaseofm′ =m(whenthenumberofcomponentsisequalto
thenumberoftrees.) Fori=1,2,...,m,denoteq =dim (f ),andlet0=ξ <ξ <···<ξ =bdenotethe
i 1 i i,0 i,1 i,qi
knotsoff ,i.e. thevaluesforwhichf (ξ )̸=f (ξ +1),togetherwiththeendpoints.7 Withoutlossofgenerality,
j i i,j i i,j
assumethatf ,...,f areorderedindescendingorderoftheir1-ensembledimension,i.e. q ≥q ≥···≥q .
1 m 1 2 m
WenowdefineE anda“badset”A. Tothisend,wedefineacollectionofpartitionmodelsV ,V ,...,V
bad 1 2 m
(spans of indicators in a single partition) as follows. First, for i = 3,4,...,m, j = 1,2,...,q , define the cells
i
L :={x: ξ <x ≤ξ },andsetV =span(cid:0)(cid:8) 1 : j =1,2,...,q (cid:9)(cid:1) ,i.e. foreachi,V containssplitsonly
i,j i,j−1 i i,j i Li,j i i
onfeatureiandonlyattheknotsoff . Tointroduceinefficiency,wedefineeachofV andV tohavesplitsonboth
i 1 2
features1and2. ThisconstructionisdemonstratedinFigure3. Theformaldetailsarefairlyinvolvedandwillbe
deferredtotheappendix. WedefineAvia
A:={(T ,T ,...,T ): F(T )=V fori=1,2,...,m},
1 2 m i i
andsetBtobetheouterboundaryofA,i.e.
B ={E∈Ω : EhasanedgetoAandE∈/ A}.
TSE,m
Finally,wepickE tobeaparticularelementofAwhosepreciseconstructionwillbedetailedintheAppendixH.
bad
Therein,wewillalsoshowthat
• EhaszerobiasforallE∈A;
• A∩OPT (f∗,(q −2)(q −2)−2)=∅;
m max min
• min ∆BIC(E,E )≥logn−O(1).
E∈B bad
7dim1(fi)issimplythenumberofconstantpiecesoffioralternatively,onelargerthanthenumberofknotsoffi.
12Figure3: VisualillustrationoftheconstructionforE usedintheproofofTheorem5.1. Theleftpaneldisplays
bad
thefunctionf (x )+f (x )togetherwithallknotsoff andf . WedefineE sothatitsfirsttwotreesT andT
1 1 2 2 1 2 bad 1 2
inducethepartitionsV andV respectively. Thesecombineforatotalof13leaves. Ontheotherhand,anoptimal
1 2
TSE will instead make use of V∗ and V∗, which combine for a total of only 8 leaves. Nonetheless, we still have
1 2
f +f ∈V∗+V∗.
1 2 1 2
Pureinteractions. Wefirstdefinethenotionofreachabilityasfollows: GivenE,E′ ∈Ω ,wesaythatE≿E′
TSE,m
if E and E′ are connected by an edge and if either Bias2(E;f∗) > Bias2(E′;f∗) or Bias2(E;f∗) = Bias2(E′;f∗)
anddf(E)≥df(E′). Notethat,becauseweallowonly“grow”and“prune”moves,foradjacentEandE′,F(E)and
F(E′)arenestedsubspaces,sothatBias2(E;f∗)=Bias2(E′;f∗)ifandonlyifΠ [f∗]=Π [f∗]. WesaythatEis
E E′
reachablefromE′,denotedE⪰E′,ifthereisasequenceofTSEsE=E0,E1,...,Ek =E′suchthatEi ≿Ei+1for
i=0,1,...,k−1.
Withoutlossofgenerality,let(x ,x )beapureinteractionforf∗. LetE beanyTSEsuchthat
1 2 bad
• E isreachablefromE ;
bad ∅
• TheredoesnotexistE∈Ω suchthatEisreachablefromE butE isnotreachablefromE.
TSE,m bad bad
NotethatsuchaTSEexistsbecauseΩ isfiniteand⪰isapartialorderingonthisspace. WesetAtobethe
TSE,m
equivalenceclassofE under⪰andsetB tobetheouterboundaryofA. Wewillshowintheappendixthatno
bad
TSE in A makes a split on either x or x , which implies that A∩OPT (f∗,∞) = ∅. We will also show that
1 2 m
min ∆BIC(E,E )≥logn−O(1).
E∈B bad
BayesianCART. Withoutlossofgenerality,letx bethefeaturethatgivesf∗ rootdependence. Byassumption,
1
thereisathresholdtsuchthatsplittingthetrivialtreeonx attgivesadecreaseinsquaredbias. Weset
1
A={T∈Ω : Thasrootsplitonx att},
TSE,1 1
and
T =argmin{BIC(E): E∈A}.
bad
Note that the outer boundary of A is a singleton set comprising the trivial tree T . By assumption, we have A∩
∅
OPT (f∗,0)=∅. Wewillshowintheappendixthat
1
n (cid:32) (cid:90) (cid:18)(cid:90) (cid:19)2(cid:33)
∆BIC(T ,T )≥ (f∗)2dν− f∗dν −o(n).
∅ bad σ2
13Fromtheseconstructions,wealsoseethatthereasonwhyhittingtimesgrowwithtrainingsamplesizeisbecause
thebarriersetsbecomeincreasinglydifficulttopassthrough. Heuristically,wecansaythatthisisbecausetheintrinsic
“temperature”oftheBARTsamplerisinverselyproportionaltothetrainingsamplesize.
7 Theoretical Limitations and Future Work
Inthissection,wedetailthelimitationsofourtheoreticalresults,whichnaturallysuggestdirectionsforfuturework.
Differencefromin-practiceBART. ThethreedifferencesbetweentheversionofBARTweanalyzedandBARTas
usedinpracticearethat:
• Weassumeafixednoiseparameterσ2insteadofputtingaprioronit;
• Ateachiteration,wepickarandomtreetoupdateinsteadofcyclingdeterministicallythroughthetrees;
• WechangetherejectionprobabilityintheMetropolisfiltertobeintermsofthemarginalposteriorsonTSEs
insteadofbeingconditionalposteriorsontheupdatedtree.
WebelievethatthesedifferencestoberelativelyminorandthattheymayevenimprovethemixingpropertiesofBART,
albeitatthecostofhighercomputationalcomplexityperiteration.8
FailuretoaddressMSEandcoveragedirectly. LetE ,E ,E ,...denotetheMarkovchaininducedbyarunof
0 1 2
BART.Forj = 0,1,2,...,leth beadrawfromtheconditionalposterioronregressionfunctions,p(f|E ,y). The
j j
BARTalgorithmreturnsthecollection
{h ,h ,...h }, (7)
tburn-in+1 tburn-in+2 tmax
wheret isthenumberofburn-initerationstobediscarded. Thefinalfittedfunctionisthemeanofthiscollection,
burn-in
whileapproximatecrediblepredictionintervalsarederivedfromquantiles. Whileourhittingtimelowerboundsimply
thattheMarkovchainonTSEsfailstoconvergeandhencethatthep(f|E ,y)’sareseparatelysuboptimal,thisdoes
j
notprecludethemeanof(7)havinggoodMSEperformance. Italsodoesnotaddresscoverageoftheapproximate
credibleintervals.
Restrictiontodiscretecovariates. Weassumedthatourcovariatedistributionwasdiscrete,i.e. X ={1,2,...,b}d.
Manyrealdatasets,ofcourse,containcontinuousfeatures. Inthiscase,in-practiceBARTcomputesagridofquantile
valuesforeachcontinuousfeature,andselectsasplitthresholdonlyfromamongthesevalues. Althoughthiseffectively
makes the covariate space discrete, it also means that the space varies with the training sample size n. Our proof
reliesheavilyontheuniformconcentrationofallnodeandsplit-basedquantitiesacrossafinitesetandsodoesnot
automaticallygeneralizetothissetting.
FailuretoaddressdependenceonotherDGPparameters. Wedidnotanalyzethedependenceofourhittingtime
lowerboundsond,thedimensionofthefeaturespace,b,thenumberofcategoriesforeachdiscretefeature,s,the
sparsityofthetrueregressionfunction,andν,thecovariatedistribution.
Failuretoaddressdependenceonotheralgorithmicparameters. Wedidnotanalyzethedependenceofourhitting
timelowerboundsonp ,theprioronΩ ,themoveprobabilitiesπ ,π ,π ,π ,andthevarianceparametersσ2
E TSE,m g p c s
andλ.
8Computingthemarginalposteriorinvolvessolvingad-dimensionallinearregressionwhereascomputingtheconditionalposteriorinvolves
solvingaunivariatelinearregression.
14Asymptotic nature of results. The hitting time lower bounds hold only when the training sample size is larger
thanaminimumnumberN. ThisnumberdependsontheDGPandalgorithmicparameters,andiftracked,canbe
exponentiallylargeinsomeofthem.
8 Simulations
Inthissection,wedescribetheresultsofasimulationstudydesignedtobridgesomeofthetheoreticallimitations
raisedintheprevioussection. Specifically,wedirectlystudytheRMSEoftheposteriormean,withrespecttothetrue
regressionfunction,andtheempiricalcoverageofpointwiseposteriorcredibleintervalsforthetrueregressionfunction.
WedothisfortheoutputoftheoriginalBARTalgorithm,investigatinghowtheyvaryaccordingtovariousDGPand
algorithmicparameters. Ourexperimentsshowthefolowing:
• Whenthedataisgeneratedfromanadditivemodel,RMSEandcoverageforBARTimprovewiththenumberof
trees,evenwhenthenumberoftreesislargerthanthenumberofadditivecomponents;
• WhenusingBayesianCART,therootsplitisoftenchosensuboptimallyandyetisrarelyreversed.Theprobability
thatthisrootsplitisreverseddecreasesasthetrainingsamplesizeincreases;
• AcrossawidevarietyofrealandsyntheticDGPs,RMSEandcoverageimprovewhenaveragingtheresultsof
multipleBARTsamplerchains. Therelativeimprovementgapbecomesincreasinglypronouncedasthetraining
samplesizeincreases.
Thefirstandsecondresultsvalidateourhittingtimelowerbounds(Theorem5.1and5.3respectively)andsuggest
thattheyholdfortheoriginalBARTalgorithmandforreasonabletrainingsamplesizes. Sinceaveragingresultsfrom
multiplechainsshouldnotmakeadifferenceifeachchainiswell-mixed,thethirdresultsuggeststhatthetendencyof
mixingandhittingtimestoincreasewiththenumberoftrainingsamplesisafairlygeneralphenomenonthatholds
acrossawidevarietyofDGPs.
Code availability. All the code necessary to reproduce the experiments in this section is publicly available at
(cid:135)github.com/theo-s/bart-hitting-time-simsThecomputinginfrastructureusedwasaLinuxclustermanagedbyDepart-
mentofStatisticsatUCBerkeley. Mostrunsofthesimulationusedasingle24-corenodewith128GBofRAM,while
thelargerdatasetsrequiredalarge-memorynodewith792GBRAMand96cores.
Algorithmsettingsandhyperparameters. WeusethedbartsRpackage(Dorie,2022)withalmostallhyperpa-
rameterskeptattheirdefaultvalues. Inparticular,π =π =0.25,π =0.1,π =0.4,andthenumberofposterior
g p c s
samplesisndpost=1000. Theonlyexceptionisthatweincreasethenumberofburn-initerationsfrom100to5000
(nskip=5000),inordertohighlightthatmixingdoesnotoccurwithinareasonablenumberofiterations. Theresponses
arecenteredandscaledtohavevarianceone. Thepriorforthenoisevarianceiscalibratedusingtheresidualsofalinear
modelfitonallofthepredictors.9 Unlessotherwisenotedintheexperimentdescription,thenumberoftreeswaskept
equalto200.
Data. For each experiment, we generate a training dataset D comprising n i.i.d. tuples (x ,y ), where y =
n i i i
f∗(x )+ϵ ,ϵ∼N(0,η2). f∗,η2,andthecovariatedistributionwillbevariedfromexperimenttoexperiment.
i i
Evaluation. Both RMSE and empirical coverage are calculated over an independent test set consisting of 1000
pointsdrawnfromthesamedistribution. Theresultsareaveragedover100experimentalreplicates,witherrorbars
representing±1.96SE.Tocomputeempiricalcoverage,wecounttheproportionofthetestsetpointswhosecredible
intervalforthefunctionvaluecontainsthegroundtruth.
9Thisisstandardformostsoftwareimplementationsofthealgorithm.
158.1 Experiment1: UnderanAdditiveModel,MoreTreesImprovesPerformance
OurfirstexperimentstudieshowRMSEandempiricalcoveragedependsontheinteractionofthenumberoftrees
specifiedintheBARTmodelandthenumberofcomponentsinanadditiveDGP.Wechosetheregressionfunctiontobe
oneofthetwoformsdescribedbelow. Wevariedthenumberoftreesinthegrid{1,2,...,10}andthetrainingsample
sizeinthegrid{10K,20K,50K,100K}.
DGP. Weletx ∈R10,x ∼N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j. Weletη2 =2,andtakef∗ to
i i ii ij
beanadditivefunctionwith5componentstakingonethefollowingtwofunctionalforms:
1. Linear: f∗(x)=0.2x −x +0.6x −0.9x +0.85x ;
1 2 3 4 5
2. Smooth: f∗(x)=0.2x2−x +0.6cos(x )−0.9|x |1/2+0.85sin(x ).
1 2 3 4 5
Results: TheresultsaredisplayedinFigure4andshowthattheRMSEforBARTdecreaseswhiletheempirical
coverageincreasesasthenumberoftreesincreases,evenwhenthenumberoftreesislargerthanthenumberofadditive
components. Thistrendholdsoverbothfunctionalformsandisconsistentacrossdifferentchoicesoftrainingsample
sizes. Asdiscussedinprevioussections,theBARTposterioralreadyachievesthemaximumgoodnessoffit(i.e. BICis
minimized)whenthenumberoftreesisequaltothenumberofadditivecomponents,implyingthatfurtherimprovement
in RMSE and empirical coverage as we increase the number of trees arises purely from better mixing. This thus
corroboratesTheorem5.1. Furthermore,thedecreaseinRMSE(andincreaseinempiricalcoverage)hasalargerslope
forlargertrainingsamplesizes,indicatingthattheimprovementinmixingismorepronouncedinthesesettings.
Linear Smooth
1.00 1.00
n
0.75 0.75
10K
0.50 0.50 20K
50K
0.25 0.25
100K
0.00 0.00
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Trees Trees
Linear Smooth
100% 100%
n
90% 90%
10K
20K
80% 80%
50K
100K
70% 70%
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Trees Trees
Figure4: Whenfittedtoanadditivemodel,RMSEforBARTdecreasesandcoverageincreasesasthenumberoftrees
increases,evenwhenthenumberoftreesislargerthanthenumberofadditivecomponents(i.e. m>5.) Thissuggests
thatmoretreesleadstobettermixingandtherebycorroboratesTheorem5.1. Thistrendholdsoverdifferentfunctional
formsandseveralchoicesoftrainingsamplesizes. BothRMSEandcoveragearecalculatedonanindependenttestset
andareaveragedover100experimentalreplicates,witherrorbarsrepresenting±1.96SE.
16
ESMR
egarevoC
laciripmE8.2 Experiment2: RootSplitGetsStuckonSuboptimalFeature
Inthesecondexperiment,wefitBayesianCART(BARTwithntree=1)toDGPsinwhichtheoptimalrootsplitis
knownandstudytherootsplitbehaviorofthesampleracrossitsiterations. Wechosetheregressionfunctiontobeone
ofthetwoformsdescribedbelowandvariedthetrainingsamplesizein{100,10K}. Foreachofthesechoices,weran
thesamplerfor500iterationsandcounted(i)thepercentageofiterationsforwhichtherootsplitwascorrectaswellas
(ii)thenumberoftotalchangestotherootsplitacrossthe500iterations.10 Werepeatedthisprocess100times.
DGP. Weletx ∈R10,x ∼N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j. Wesetη =1andtakef⋆tobe
i i ii ij
apiecewiseconstantfunctionwithasingleoptimaltreestructure,takingoneofthefollowingforms(seealsoFigure5):

12, ifx >.1∧x >.7
9, ifx1 >.1∧x2
≤.7
1. DGP1: f∗(x)= 1 2
63 ,, ii ff xx
1
≤≤. .11 ∧∧ xx
3
≤>− −.. 22
1 3

12, ifx >.1∧x >.7∧x >.1
39
,, ii ff xx
11
>> .. 11 ∧∧ xx
22
> ≤..7 7∧ ∧x
x44
≤ >. .1
1orx ≤.1∧x >−.2∧x >.2
2. DGP2: f∗(x)= 1 2 5 1 3 6 .
4 12,
,,
i iif ffx
xx1
1
>
≤≤
...1 11∧ ∧∧x
xx2
3
≤ ≤≤. −−7 ..∧
22
∧∧x
5
xx≤
7
≤>.1 ..o 11r 55x
1
≤.1∧x
3
>−.2∧x
6
≤.2
1 3 7
Inbothcasestheoptimalfirstsplitisonx .
1
Figure5: ThetworegressionfunctionsusedinExperiment2toexplorerootsplitbehaviorofBayesianCART.
Results. Thedistributionsofbothquantitiesacrossthe100replicatesaredisplayedinFigure6. Theyshowthat,
asthetrainingsamplesizeincreases,thepercentageofcorrectfirstsplitsdoesnotconvergeto1,butinsteadseems
todevelopabimodaldistribution. Moreover,thenumberofchangestotherootsplitdecreases. Thissuggeststhat
thesamplerbecomesmorelikelytogetstuckinasetcomprisingtreeswithanincorrectrootsplit,whichagreeswith
Theorem5.3. WhatisespeciallynotableisthatwhileTheorem5.3requiresarestrictiontoonly“grow”and“prune”
moves,theseresultsdonothavesucharestriction.
10Wedidnotdiscardanyburn-insamplesforthisinvestigation.
17DGP 1 DGP 2
1.25
3
1.00
Sample Size
2 0.75
100
0.50
10000
1
0.25
0 0.00
0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
Percent Correct Initial Splits Percent Correct Initial Splits
0.08 0.125
0.100
0.06
Sample Size
0.075
0.04 100
0.050 10000
0.02
0.025
0.00 0.000
0 50 100 150 0 50 100 150
Changes to Initial Split Changes to Initial Split
Figure6: WhentheDGPhasauniqueoptimalrootsplit,asthetrainingsamplesizeincreases,theBayesianCART
sampler becomes more likely to get stuck in a set comprising trees with incorrect root splits. For each of 100
experimentalreplicates,weinitializeasampleratthetrivialtreeandrunitfor500iterations. Wethencalculate(i)
thepercentageofsampleriterationswithacorrectrootsplitand(ii)thetotalnumberofchangestotherootsplit. The
topandbottompanelsplotthedistributionofthefirstandsecondquantitiesrespectively. Asthetrainingsamplesize
increases,thedistributionfor(i)seemstobecomebimodalaround0and1. Meanwhile,(ii)becomesmoreconcentrated
around0.
8.3 Experiment3: MultipleBARTChainsImprovesMixing
Inthefinalexperiment,weinvestigatetheeffectoftrainingsamplesizeonthemixingperformanceofBARTaswefititto
avarietyofDGPs,describedbelow.Wevariedthenumberoftrainingsamplesinthegrid{1K,10K,20K,50K,100K}
and,foreachdataset,variedthenumberofchainsfortheBARTsamplerin{1,2,5,10}. Foreachnumberofchains,
wedividedafixedbudgetof1000posteriorsamplesevenlyamongstthechainsandobtainedRMSEandempirical
coveragevaluesbyaveragingtheposteriorsamplesthusobtained. Notethatwestillusethesamenumberofburn-in
iterationsforeachchain.
DGP. WestudythefollowingDGPs:
1. Low (Lei and Candès, 2021): f∗(x) = g(x )g(x ), g(x) = 2 . We let x ∈ R10, x ∼
1 2 1+exp{−12(x−0.5)} i i
N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j.
ii ij
2. High (Lei and Candès, 2021): f∗(x) = g(x )g(x ), g(x) = 2 . We let x ∈ R100, x ∼
1 2 1+exp{−12(x−0.5)} i i
N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j.
ii ij
3. LocalSparseSpiky(Behretal.,2021):f∗(x)=2·1{x <0,x >0)}−3·1 +.8·1{x <1.5,x <1}.
1 3 (x5>0,x6>1 3 5
Weletx ∈R10,x ∼N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j
i i ii ij

βTx, ifx <−.4
 1 20
4. PiecewiseLinear(Künzeletal.,2019):f∗(x)= βTx, ifx <.4 ,whereβ ,β ,β ∼Unif(cid:0) [−15,15]20(cid:1) .
2 20 1 2 3
βTx,
otherwise
3
18
ytisned
ytisnedWeletx ∈R20,x ∼N(0,Σ),wherewesetΣ =1,Σ =0.01fori̸=j.
i i ii ij

1 ifi=j

5. Sum: f∗(x)=(cid:80)10 x . Weletx ∈R20,x ∼N(0,Σ),wherewesetΣ = 0.01 ifi=j+10.
j=1 j i i ij
0
otherwise
6. Tree: f∗(x)=T(x),whereTisadecisiontreefunctionfittedtoastandardGaussianresponsevector,usingthe
CARTalgorithmwithmaximaldepthof7. Weletx ∈R10,x ∼N(0,Σ),wherewesetΣ =1,Σ =0.01
i i ii ij
fori̸=j
WesetηsuchthatTreeandPiecewisehavesignaltonoiseratiosof1,HighandLowhavesignaltonoiseratiosof.5,
andLocalSparseSpikyandSumhavesignaltonoiseratiosof.75.
Results. TheresultsaredisplayedinFigure7. NotethatinsteadofplottingRMSE,wehavechosentoplotrelative
RMSE,whichmeasurestheratiobetweentheRMSEobtainedfrommultiplechainsandthatobtainedforasinglechain
foragivendatasetting. TheresultsshowthatbothRMSEandempiricalcoverageimproves,oftenquitesignificantly,
asweaddmultiplechains. Thismeansthatdifferentchainsgiverisetosignificantlydifferentdistributions,implying
thattheBARTsamplerhasnotmixedevenafterthelargenumberofburn-initerations. Furthermore,asthenumber
of training samples increases, the relative performance gap between a single chain and multiple chains increases.
OurresultsthereforeprovideevidencethatthetendencyofHPDRhittingtimestogrowwithtrainingsamplesizeis
consistentacrossawiderrangeofDGPsthanwasstudiedtheoretically.
Realdatasimulations. Wealsoperformedasimilarexperimentwithanumberofbenchmarkdatasets. Resultsand
furtherdescriptionforthisexperimentareprovidedinAppendixK.
19Sum High Low
100% 100% 100%
Chains
90% 90% 90%
1
80% 80% 80% 2
5
70% 70% 70%
10
60% 60% 60%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Piecewise Linear Tree Local Sparse Spiky
100% 100% 100%
Chains
90% 90% 90%
1
80% 80% 80% 2
5
70% 70% 70%
10
60% 60% 60%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Sum High Low
100% 100% 100%
97.5% 97.5% Chains
90% 1
95%
95% 2
80% 92.5% 5
92.5%
10
90%
90%
70%
87.5%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Piecewise Linear Tree Local Sparse Spiky
98%
95% Chains
75% 96%
1
92.5%
50% 94% 2
5
25% 90% 92% 10
87.5% 90%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Figure7: TheRMSE(toppanel)andempiricalcoverage(bottompanel)ofBARTimproveifweaverageposterior
samplesfrommultiplesamplerchains,givenafixedtotalbudgetofposteriorsamples. Thisimprovementmeansthatthe
BARTsamplerhasnotmixedeverafter5000burn-initerations. Furthermore,therelativeperformancegapsincreases
withthenumberoftrainingsamples,providingevidencethatthetendencyofHPDRhittingtimetogrowwithtraining
samplesizeisconsistentacrossawiderangeofDGPs. BothRMSEandcoveragearecalculatedonanindependenttest
setandareaveragedover100experimentalreplicates,witherrorbarsrepresenting±1.96SE.
20
ESMR
evitaleR
ESMR
evitaleR
egarevoC
laciripmE
egarevoC
laciripmE9 Discussion
ItiswidelyacceptedthatChipmanetal.(2010)’sBARTsampleroftenhasissueswithmixingandthatthereismuch
roomforcomputationalimprovement. Inthispaper,wevastlyimproveuponourearlierwork(Ronenetal.,2022)to
providetheoreticalcomputationallowerboundsforaslightlymodifiedversionoftheBARTsampler. Oursisthefirst
worktoanalyzeBARTratherthanBayesianCART.Furthermore,wecreateanewframeworkforanalysisbystudying
hittingtimesofHPDRsinsteadofmixingtimes,whichresolvesissuesofidentifiabilityandleadstomoremeaningful
computationallowerbounds. Wederivetheselowerboundsunderfourdifferent,fairlyrealisticalgorithmicanddata
generativesettingsandshowinallcasesthattheygrowwiththetrainingsamplesize. Wecomplementourtheoretical
resultswithasimulationstudythatvalidatesourresultsandalsosuggeststhatourcentralthesis,thatBARTmixingand
hittingtimesincreasewiththenumberoftrainingsamples,isafairlygeneralphenomenonthatholdsacrossawide
varietyofDGPs. WealsoarguethatthisisduetotheunnecessarilycoarsewayinwhichtheBARTsamplerrelates
temperatureandtrainingsamplesize.
OurresultsgivetheoreticalandempiricalsupporttosomeofthechoicesthatBARTpractitionersoftenalready
make. Specifically,theysupporttheuseofmoretreesintheBARTensemble,andtheysupporttheuseofmultiple
BARTsamplerchains. Howtoselecttheoptimalnumberoftreesandchainsisanintriguingandimportantquestionand
willbelefttofuturework. Inaddition,ourresultsadvocateforthedesignofbetterBARTsamplersandsuggestpossible
approachesforimprovement. First,webelievethatthereisgreatpotentialinexploringvariousformsoftemperature
control. Thiscouldtaketheformofsimulatedannealing(VanLaarhovenetal.,1987),orsimulatedtempering(Marinari
andParisi,1992),whichhaspreviouslybeenexploredforBayesianCART(AngelopoulosandCussens,2005),but
hasyettobeadaptedtoBART.Second,webelievethattheproposaldistributionshouldfavormorepromisingsplit
directions,insteadofbeinguniformatrandom. ThereisnowavastliteratureonhowusinggradientandevenHessian
informationcanhelptoaccelerateMCMCincontinuousstatespaces(seeforinstanceNealetal.(2011)),andthereis
recentworkinextendingthistodiscretespaces(Zanella,2020). Third,webelieveinsteadofconstrainingtheproposal
distributionto“local”moves,itcouldbenefitfromincorporatingmovesthataltertreestructuresmoredrastically. This
hasbeenexploredsomewhatbyKimandRockova(2023).
Acknowledgements
WewouldliketothankAntonioLineroandJungguemKimforinsightfuldiscussions. Wewouldalsoliketothankthe
StatisticalComputingFacility(SCF)oftheDepartmentofStatisticsatUniversityofCalifornia,Berkeleyforcomputing
support,andinparticular,JacobSteinhardtforaccesstohisgroup’slarge-memorynodeswithintheSCFLinuxcluster.
WegratefullyacknowledgepartialsupportfromNSFTRIPODSGrant1740855,DMS-1613002,1953191,2015341,
2209975,20241842,IIS1741340,ONRgrantN00014-17-1-2176,NSFgrant2023505onCollaborativeResearch:
Foundations of Data Science Institute (FODSI), NSF grant MC2378 to the Institute for Artificial CyberThreat In-
telligenceandOperatioN(ACTION),theNSFandtheSimonsFoundationfortheCollaborationontheTheoretical
Foundations of Deep Learning through awards DMS-2031883 and 814639, and a Weill Neurohub grant. YT was
partiallysupportedbyNUSStart-upGrantA-8000448-00-00.
21References
Abbe,E.,Adsera,E.B.,andMisiakiewicz,T.(2022). Themerged-staircaseproperty: anecessaryandnearlysufficient
conditionforsgdlearningofsparsefunctionsontwo-layerneuralnetworks. InConferenceonLearningTheory,
pages4782–4887.PMLR.
Agarwal,A.,Tan,Y.S.,Ronen,O.,Singh,C.,andYu,B.(2022). Hierarchicalshrinkage: Improvingtheaccuracyand
interpretabilityoftree-basedmodels. InInternationalConferenceonMachineLearning,pages111–135.PMLR.
Angelopoulos,N.andCussens,J.(2005). TemperingforBayesianCART. InProceedingsofthe22ndInternational
ConferenceonMachineLearning,pages17–24.
Bartlett,P.L.,Long,P.M.,Lugosi,G.,andTsigler,A.(2020). Benignoverfittinginlinearregression. Proceedingsof
theNationalAcademyofSciences,117(48):30063–30070.
Behr,M.,Wang,Y.,Li,X.,andYu,B.(2021). Provablebooleaninteractionrecoveryfromtreeensembleobtainedvia
randomforests. arXivpreprintarXiv:2102.11800.
Breiman,L.(2001). Randomforests. MachineLearning,45(1):5–32.
Breiman, L., Friedman, J., Olshen, R., andStone, C.J.(1984). Classificationandregressiontrees. Chapmanand
Hall/CRC.
Carnegie,N.B.(2019). Comment: ContributionsofmodelfeaturestoBARTcausalinferenceperformanceusingACIC
2016competitiondata. StatisticalScience,34(1):90–93.
Caruana,R.,Karampatziakis,N.,andYessenalina,A.(2008). Anempiricalevaluationofsupervisedlearninginhigh
dimensions. InProceedingsofthe25thInternationalConferenceonMachineLearning,pages96–103.
Caruana,R.andNiculescu-Mizil,A.(2006).Anempiricalcomparisonofsupervisedlearningalgorithms.InProceedings
ofthe23rdInternationalConferenceonMachineLearning,pages161–168.
Castillo,I.andRocˇková,V.(2021).UncertaintyquantificationforBayesianCART.TheAnnalsofStatistics,49(6):3482–
3509. Publisher: InstituteofMathematicalStatistics.
Chen,T.andGuestrin,C.(2016). XGBoost: Ascalabletreeboostingsystem. InProceedingsofthe22ndacmsigkdd
internationalconferenceonknowledgediscoveryanddatamining,pages785–794.
Chipman,H.A.,George,E.I.,andMcCulloch,R.E.(1998). BayesianCARTmodelsearch. JournaloftheAmerican
StatisticalAssociation,93(443):935–948.
Chipman,H.A.,George,E.I.,andMcCulloch,R.E.(2010). BART:Bayesianadditiveregressiontrees. TheAnnalsof
AppliedStatistics,4(1):266–298.
Dorie,V.(2022). dbarts: DiscreteBayesianadditiveregressiontreessampler. Rpackageversion0.9-22.
Dorie,V.,Hill,J.,Shalit,U.,Scott,M.,andCervone,D.(2019). Automatedversusdo-it-yourselfmethodsforcausal
inference: Lessonslearnedfromadataanalysiscompetition. StatisticalScience,34(1):43–68.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004). Least angle regression. The Annals of Statistics,
32(2):407–499.
Fernández-Delgado,M.,Cernadas,E.,Barro,S.,andAmorim,D.(2014). Doweneedhundredsofclassifierstosolve
realworldclassificationproblems? TheJournalofMachineLearningResearch,15(1):3133–3181.
Friedman,J.H.(2001). Greedyfunctionapproximation: agradientboostingmachine. AnnalsofStatistics,29(5):1189–
1232.
22Green, D. P. and Kern, H. L. (2010). Modeling heterogeneous treatment effects in large-scale experiments using
Bayesianadditiveregressiontrees. InTheAnnualSummerMeetingoftheSocietyofPoliticalMethodology,pages
100–110.
Green,D.P.andKern,H.L.(2012). ModelingheterogeneoustreatmenteffectsinsurveyexperimentswithBayesian
additiveregressiontrees. PublicOpinionQuarterly,76(3):491–511.
Grinsztajn,L.,Oyallon,E.,andVaroquaux,G.(2022). Whydotree-basedmodelsstilloutperformdeeplearningon
typicaltabulardata? AdvancesinNeuralInformationProcessingSystems,35:507–520.
Hahn,P.R.,Dorie,V.,andMurray,J.S.(2019). Atlanticcausalinferenceconference(acic)dataanalysischallenge
2017. arXivpreprintarXiv:1905.09515.
Hastie,T.andTibshirani,R.(1986). Generalizedadditivemodels. StatisticalScience,1(3):297–318.
Hastie,T.,Tibshirani,R.,Friedman,J.H.,andFriedman,J.H.(2009). TheElementsofStatisticalLearning: Data
Mining,Inference,andPrediction,volume2. Springer.
He,J.andHahn,P.R.(2021). Stochastictreeensemblesforregularizednonlinearregression. JournaloftheAmerican
StatisticalAssociation,pages1–20.
Hill,J.,Linero,A.,andMurray,J.(2020). Bayesianadditiveregressiontrees: Areviewandlookforward. Annual
ReviewofStatisticsandItsApplication,7(1).
Hill,J.L.(2011). Bayesiannonparametricmodelingforcausalinference. JournalofComputationalandGraphical
Statistics,20(1):217–240.
Jeong,S.andRockova,V.(2020).TheartofBART:OnflexibilityofBayesianforests.arXivpreprintarXiv:2008.06620,
3(69):146.
Kern,H.L.,Stuart,E.A.,Hill,J.,andGreen,D.P.(2016). Assessingmethodsforgeneralizingexperimentalimpact
estimatestotargetpopulations. JournalofResearchonEducationalEffectiveness,9(1):103–127.
Kim,J.andRockova,V.(2023). OnmixingratesforBayesianCART. arXivpreprintarXiv:2306.00126.
Klusowski,J.M.(2021). Universalconsistencyofdecisiontreesinhighdimensions. arXivpreprintarXiv:2104.13881.
Künzel,S.R.,Sekhon,J.S.,Bickel,P.J.,andYu,B.(2019). Metalearnersforestimatingheterogeneoustreatment
effectsusingmachinelearning. ProceedingsoftheNationalAcademyofSciences,116(10):4156–4165.
Lei,L.andCandès,E.J.(2021). Conformalinferenceofcounterfactualsandindividualtreatmenteffects. Journalof
theRoyalStatisticalSociety: SeriesB(StatisticalMethodology).
Levin,D.A.,Peres,Y.,andWilmer,E.L.(2006). Markovchainsandmixingtimes. AmericanMathematicalSociety.
Linero,A.R.(2018). Bayesianregressiontreesforhigh-dimensionalpredictionandvariableselection. Journalofthe
AmericanStatisticalAssociation,113(522):626–636.
Linero,A.R.andYang,Y.(2018). Bayesianregressiontreeensemblesthatadapttosmoothnessandsparsity. Journal
oftheRoyalStatisticalSociety: SeriesB(StatisticalMethodology),80(5):1087–1110.
Luo,H.andPratola,M.T.(2023). ShardedBayesianadditiveregressiontrees. arXivpreprintarXiv:2306.00361.
Marinari,E.andParisi,G.(1992). Simulatedtempering: anewmontecarloscheme. EurophysicsLetters,19(6):451.
Mazumder, R. and Wang, H. (2024). On the convergence of CART under sufficient impurity decrease condition.
AdvancesinNeuralInformationProcessingSystems,36.
23Nash,W.J.,Sellers,T.L.,Talbot,S.R.,Cawthorn,A.J.,andFord,W.B.(1994). Thepopulationbiologyofabalone
(haliotis species) in tasmania. i. blacklip abalone (h. rubra) from the north coast and islands of bass strait. Sea
FisheriesDivision,TechnicalReport,48:p411.
Neal,R.M.etal.(2011). MCMCusingHamiltoniandynamics. HandbookofMarkovChainMonteCarlo,2(11):2.
Pace,K.andBarry,R.(1997). Sparsespatialautoregressions. Statistics&ProbabilityLetters,33(3):291–297.
Pratola,M.T.(2016).Efficientmetropolis–hastingsproposalmechanismsforBayesianregressiontreemodels.Bayesian
analysis,11(3):885–911.
Pratola, M. T., Chipman, H. A., Gattiker, J. R., Higdon, D. M., McCulloch, R., and Rust, W. N. (2014). Parallel
BayesianAdditiveRegressionTrees. JournalofComputationalandGraphicalStatistics,23(3):830–852. Publisher:
Taylor&Francis_eprint: https://doi.org/10.1080/10618600.2013.841584.
Pratola,M.T.,Chipman,H.A.,George,E.I.,andMcCulloch,R.E.(2020). HeteroscedasticBARTviamultiplicative
regressiontrees. JournalofComputationalandGraphicalStatistics,29(2):405–417.
Redner,R.A.andWalker,H.F.(1984). Mixturedensities,maximumlikelihoodandtheEMalgorithm. SIAMReview,
26(2):195–239.
Rockova,V.andRousseau,J.(2021). IdealBayesianspatialadaptation. arXivpreprintarXiv:2105.12793.
Rocˇková,V.andSaha,E.(2019). OntheoryforBART. InThe22ndInternationalConferenceonArtificialIntelligence
andStatistics,pages2839–2848.PMLR.
Rocˇková,V.andvanderPas,S.(2020). PosteriorconcentrationforBayesianregressiontreesandforests. TheAnnals
ofStatistics,48(4):2108–2131.
Romano,J.D.,Le,T.T.,LaCava,W.,Gregg,J.T.,Goldberg,D.J.,Chakraborty,P.,Ray,N.L.,Himmelstein,D.,Fu,
W.,andMoore,J.H.(2021). PMLBv1.0: anopen-sourcedatasetcollectionforbenchmarkingmachinelearning
methods. Bioinformatics,38(3):878–880.
Ronen,O.,Saarinen,T.,Tan,Y.S.,Duncan,J.,andYu,B.(2022). Amixingtimelowerboundforasimplifiedversion
ofBART. arXivpreprintarXiv:2210.09352.
Schwarz,G.(1978). Estimatingthedimensionofamodel. TheAnnalsofStatistics,6(2):461–464.
Scornet,E.,Biau,G.,andVert,J.-P.(2015). Consistencyofrandomforests. AnnalsofStatistics,43(4):1716–1741.
Starling,J.E.,Murray,J.S.,Carvalho,C.M.,Bukowski,R.K.,andScott,J.G.(2020). BARTwithtargetedsmoothing:
Ananalysisofpatient-specificstillbirthrisk. TheAnnalsofAppliedStatistics,14(1):28–50.
Syrgkanis, V.andZampetakis, M.(2020). Estimationandinferencewithtreesandforestsinhighdimensions. In
ConferenceonLearningTheory,pages3453–3454.PMLR.
Tan,Y.S.,Agarwal,A.,andYu,B.(2022a). Acautionarytaleonfittingdecisiontreestodatafromadditivemodels:
generalizationlowerbounds. InInternationalConferenceonArtificialIntelligenceandStatistics,pages9663–9685.
PMLR.
Tan,Y.S.,Singh,C.,Nasseri,K.,Agarwal,A.,andYu,B.(2022b). Fastinterpretablegreedy-treesums(FIGS). arXiv
preprintarXiv:2201.11931.
VanLaarhoven,P.J.,Aarts,E.H.,vanLaarhoven,P.J.,andAarts,E.H.(1987). SimulatedAnnealing. Springer.
Vershynin,R.(2018). High-DimensionalProbability: AnIntroductionwithApplicationsinDataScience,volume47.
CambridgeUniversityPress.
24Wainwright,M.J.(2019).High-DimensionalStatistics:ANon-AsymptoticViewpoint,volume48.CambridgeUniversity
Press.
Wendling, T., Jung, K., Callahan, A., Schuler, A., Shah, N. H., and Gallego, B. (2018). Comparing methods for
estimation of heterogeneous treatment effects using observational data from health care databases. Statistics in
medicine,37(23):3309–3324.
Wu,Y.,Tjelmeland,H.,andWest,M.(2007). BayesianCART:Priorspecificationandposteriorsimulation. Journalof
ComputationalandGraphicalStatistics,16(1):44–66.
Yeager,D.S.,Hanselman,P.,Walton,G.M.,Murray,J.S.,Crosnoe,R.,Muller,C.,Tipton,E.,Schneider,B.,Hulleman,
C.S.,Hinojosa,C.P.,etal.(2019). Anationalexperimentrevealswhereagrowthmindsetimprovesachievement.
Nature,573(7774):364–369.
Zanella, G. (2020). Informed proposals for local MCMC in discrete spaces. Journal of the American Statistical
Association,115(530):852–865.
25A Literature summary for BART posterior concentration results
Welisthereallresultsonposteriorconcentrationthatweareawareof:
• Posteriorconcentrationattheoptimaln−α/(2α+p)rateforBCARTandBARTwhentheregressionfunctionis
Hölderα-smooth,0<α≤1(RocˇkováandSaha,2019).
• Posteriorconcentrationattheoptimaln−α/(2α+s)rateforBCARTandBARTwhentheregressionfunctionis
Hölderα-smooth,0<α≤1,ands-sparse(RocˇkováandvanderPas,2020).
• Posteriorconcentrationatthealmostoptimaln−α/(2α+1)lognrateforBARTwhentheregressionfunctionisan
additivesumofunivariatecomponents,eachofwhichisHölderα-smooth,0<α≤1(RocˇkováandvanderPas,
2020).
• Extensionsoftheaboveresultstoα>1,providedBARTismodifiedtoallowfor“soft”splits(LineroandYang,
2018).
• Posteriorconcentrationattheoptimalrate(uptologfactors)forBARTwhentheregressionfunctionispiecewise
heterogeneousanisotropicHöldersmooth(JeongandRockova,2020;RockovaandRousseau,2021).
B Proofs for Section 4.2
Wefirstintroducesomeadditionalnotationthatwillbeusedthroughouttherestoftheappendix. WedenoteD =
f
∥f∗∥ andletD denotethesub-Gaussianparameterofthenoiserandomvariableϵ(Wainwright,2019). C willbe
∞ ϵ
usedtodenoteauniversalconstant(i.e. notdependingonanyparameters)thatmaychangefromlinetoline. For
simplicity,manyofourresultsarewrittenusingbig-Onotation,whichwillindicateleadingorderdependenceonthe
samplesizenaswellastheerrorprobabilityδ.
B.1 MainProofs
LemmaB.1(Concentrationofempiricalriskdifference). LetEandE′betwopartitionensemblemodels. Thenforany
0<δ <1,withprobabilityatleast1−δ,theriskdifferencebetweenthemsatisfies
(cid:12) (cid:12) (cid:12)yT(P E−P E′)y−n(cid:90) (Π Ef∗)2−(Π E′f∗)2dν(cid:12) (cid:12) (cid:12)=O(cid:16)(cid:112) nlog(1/δ)(cid:17) . (8)
(cid:12) (cid:12)
Iffurthermore,Π f∗ =Π f∗,thentheaboveboundcanbeimprovedto
E E′
(cid:12) (cid:12)yT(P E−P E′)y(cid:12) (cid:12)=O(log(1/δ)). (9)
ProofofProposition4.1 Since
yT(P −P )y
∆BIC(E,E′)= E E′ +(df(E)−df(E′))logn,
σ2
thedesiredconcentrationfollowsimmediatelyfromLemmaB.1.
LemmaB.2(Logmarginallikelihoodformula). LetEbeatreeensemblestructure(TSE),andletΨbeann×bmatrix
whosecolumnscomprisetheindicatorsoftheleavesinE. Thelogmarginallikelihoodsatisfies
−2logp(y|X,E)=nlog(cid:0) 2πσ2(cid:1) +logdet(cid:16) λ−1ΨTΨ+I(cid:17)
1 (cid:18) (cid:18) (cid:16) (cid:17)−1 (cid:19) (cid:19)
+ ∥Ψµˆ −y∥2+µˆT ΨT I−Ψ ΨTΨ+λI ΨT Ψµˆ , (10)
σ2 LS 2 LS LS
26where
µˆ :=argmin∥Ψµ−y∥2
LS 2
µ
isthesolutiontotheleastsquaresproblem.
LemmaB.3(Concentrationoflog-determinent). WiththenotationofLemmaB.2,denoteΣˆ := 1ΨTΨandΣ :=
n
E(cid:110) Σˆ(cid:111) . Thenforany0<δ <1,forn≥max(cid:8) 64m3/2s−2 log(2df(E)/δ),4λs−1 (cid:9) ,withprobabilityatleast1−δ,
min min
wehave
(cid:12) (cid:12)
(cid:12) df(E) (cid:12) (cid:32)(cid:114) (cid:33)
(cid:12)
(cid:12)
(cid:12)logdet(cid:16) λ−1ΨTΨ+I(cid:17)
−df(E)log(n)−
(cid:88)
log(s
i/λ)(cid:12)
(cid:12) (cid:12)=O
log( n1/δ)
, (11)
(cid:12) i=1 (cid:12)
wheres ,s ,...,s denotethevaluesofthenonzeroeigenvaluesofΣ.
1 2 df(E)
Lemma B.4 (Concentration of error term). With the notation of Lemma B.3, for any 0 < δ < 1, when n ≥
16m3/2s−2 log(2df(E)/δ),wheres istheminimumnonzeroeigenvalueofΣ,withprobabilityatleast1−δ,we
min min
have
(cid:18) (cid:16) (cid:17)−1 (cid:19)
µˆT ΨT I−Ψ ΨTΨ+λI ΨT Ψµˆ =O(1). (12)
LS LS
ProofofProposition4.2. Startingwithequation(10)fromLemmaB.2,pluginequations(11)and(12)fromLemma
B.2andLemmaB.3respectively. NoticethatΨµˆ =P y,sothat
LS E
∥Ψµˆ −y∥2 =∥(I−P )y∥2 =yT(I−P )y.
LS 2 E 2 E
Thiscompletestheproof.
ProofofProposition4.3. LetE∗ ∈OPT (f∗,0). WewillshowthatforanyE̸∈OPT (f∗,0),thereissomeN
m m E
suchthat
ϵp(E∗|y)
p(E|y)≤ (13)
|Ω |
TSE,m
withprobabilityatleast1−δ/|Ω |foralln≥N . Ifthisistrue,setN =max N andtaken≥N.
TSE,m E E∈/OPTm(f∗,0) E
Ontheintersectionofalltheseevents,wehave
(cid:88) (cid:88) ϵp(E∗|y)
p(OPT (f∗,0)c|y)= p(E|y)≤ ≤ϵ.
m |Ω |
TSE,m
E∈/OPTm(f∗,0) E∈/OPTm(f∗,0)
Toprove(13),fixE. UsingProposition4.2andProposition4.1,wegetaprobability1−δ/|Ω |eventonwhich
TSE,m
logp(E∗|y)−logp(E|y)
(cid:40) n (cid:0) Bias2(E;f∗)(cid:1) +O(cid:16)(cid:112) nlog(1/δ)+log(1/δ)(cid:17) ifBias2(E;f∗)̸=0
= 2σ2
logn(df(E)−df(E∗))+O(log(1/δ)) otherwise.
2
Ineithercase,weget p(E|y) →0asn→∞.
p(E∗|y)
B.2 FurtherDetails
Proof of Lemma B.1. Recall that Π refers to orthogonal projection onto F(E) in L2(ν), while P refers to
E E
orthogonal projection onto F(E) with respect to the empirical norm ∥·∥ . With this in mind, decompose y =
n
27ϵ+(f∗(x)−Π f∗(x))+Π f∗(x)andwritethisinvectorformasy =ϵ+(f∗−f∗)+f∗. SinceΠ f∗ ∈F(E),
E E E E E
wehaveP f∗ =f∗. WecanthenthereforeexpandthequadraticformyTP yasfollows:
E E E E
yTP y=ϵTP ϵ+(f∗−f∗)TP (f∗−f∗)+2ϵTP (f∗−f∗)+(f∗)TP f∗+2ϵTP f∗+2(f∗−f∗)TP f∗
E E E E E E E E E E E E E E E
=ϵTP ϵ+(f∗−f∗)TP (f∗−f∗)+2ϵTP (f∗−f∗)+(f∗)Tf∗+2ϵTf∗+2(f∗−f∗)Tf∗. (14)
E E E E E E E E E E E
Notethatϵ,(f∗(x)−Π f∗(x)),andΠ f∗(x)areuncorrelatedrandomvariables,withϵbeingalsoindependentofthe
E E
othertwovariables. Thisimpliesthatthethird,fifthandsixthtermsin(14)havezeromean. Ontheotherhand,because
offinitesamplefluctuations,(f∗−f∗)andf∗,arenotnecessarilyorthogonalasvectors.
E E
Toboundtheexpectationof(14),firstobservethatf∗−Π f∗andΠ f∗areboundedrandomvariablesandthus
E E
havebothstandarddeviationandsub-GaussiannormboundedbyD (Wainwright,2019). Wethencompute
f
E(cid:8) ϵTP ϵ+(f∗−f∗)TP (f∗−f∗)+2ϵTP (f∗−f∗)+(f∗)Tf∗+2ϵTf∗+2(f∗−f∗)Tf∗(cid:9)
E E E E E E E E E E E
= E(cid:8) ϵTP ϵ(cid:9) +E(cid:8) (f∗−f∗)TP (f∗−f∗)(cid:9) +E(cid:8) (f∗)Tf∗(cid:9)
E E E E E E
(cid:90)
= tr{P }(Var{ϵ}+Var{f∗−f∗})+n (Π f∗)2dν
E E E
(cid:90)
≤ C(D +D )df(E)+n (Π f∗)2dν. (15)
ϵ f E
Next,weboundthefluctuationsofeachtermin(14)separately. UsingtheHanson-Wrightinequality(Wainwright,
2019;Vershynin,2018),weget1−δprobabilityeventsoverwhich
(cid:12) (cid:12)ϵTP Eϵ−E(cid:8) ϵTP Eϵ(cid:9)(cid:12) (cid:12)≤CD ϵmax(cid:110) log(1/δ),(cid:112) df(E)log(1/δ)(cid:111) , (16)
and
(cid:12) (cid:12)(f∗−f E∗)TP E(f∗−f E∗)−E(cid:8) (f∗−f E∗)TP E(f∗−f E∗)(cid:9)(cid:12) (cid:12)≤CD fmax(cid:110) log(1/δ),(cid:112) df(E)log(1/δ)(cid:111) . (17)
UsingHoeffding’sinequality(Wainwright,2019),wehavefurther1−δprobabilityeventsoverwhich
(cid:12) (cid:90) (cid:12)
(cid:12) (cid:12) (cid:12)(f E∗)Tf E∗−n (Π Ef∗)2dν(cid:12) (cid:12) (cid:12)≤CD f2(cid:112) nlog(1/δ), (18)
(cid:12) (cid:12)ϵTf E∗(cid:12) (cid:12)≤CD fD ϵ(cid:112) nlog(1/δ), (19)
(cid:12) (cid:12)(f∗−f∗)Tf∗(cid:12) (cid:12)≤CD2(cid:112)
nlog(1/δ). (20)
(cid:12) E E(cid:12) f
Forthethirdtermin(14),weuseCauchy-SchwarzfollowedbyYoung’sinequalitytoget
2(cid:12) (cid:12)ϵTP E(f∗−f E∗)(cid:12) (cid:12)≤2(cid:0) ϵTP Eϵ(cid:1)1/2(cid:0) (f∗−f E∗)TP E(f∗−f E∗)(cid:1)1/2
≤ϵTP ϵ+(f∗−f∗)TP (f∗−f∗). (21)
E E E E
Conditioningonalltheeventsguaranteeing(16)to(20)andpluggingintheseboundstogetherwith(15)into(14),
weget
(cid:12) (cid:90) (cid:12)
(cid:12) (cid:12)yTP Ey−n (Π Ef∗)2dν(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:16) (cid:112) (cid:112) (cid:17)
≤ C (D +D )df(E)+(D +D )log(1/δ)+D (D +D ) nlog(1/δ)+(D +D ) df(E)log(1/δ)
ϵ f f ϵ f f ϵ f ϵ
(cid:16)(cid:112) (cid:17)
= O nlog(1/δ)+log(1/δ) .
RepeatingthesameargumentforE′andadjustingδsothattheintersectionofalleventsconditionedonhasprobability
atleast1−δcompletestheproofof(8).
Toprove(9),observethatundertheadditionalassumption,wecancanceltermsin(14)toget
yT(P −P )y=ϵT(P −P )ϵ+(f∗−f∗)T(P −P )(f∗−f∗)+2ϵT(P −P )(f∗−f∗).
E E′ E E′ E E E′ E E E′ E
Applying(21)followedby(16)and(17)completestheproof.
28ProofofLemmaB.2. Recallthatthefullloglikelihoodsatisfies
(cid:32) (cid:33)
p(y|X,E,µ)=(cid:0) 2πσ2(cid:1)−n/2 exp
−∥Ψµ−y∥2
2 ,
2σ2
whileconditionedonE,theprioronµsatisfies
p(µ|E)=(cid:0) 2πσ2λ−1(cid:1)−b/2
exp(cid:18) −λ∥µ∥2 2(cid:19)
,
2σ2
wherebisthenumberofcolumnsinΨ. Hence
p(y|X,E,µ)p(µ|E)=(cid:0) 2πσ2(cid:1)−n/2(cid:0) 2πσ2λ−1(cid:1)−b/2 exp(cid:18)
−
1 (cid:16) ∥Ψµ−y∥2+λ∥µ∥2(cid:17)(cid:19)
. (22)
2σ2 2 2
Considertheorthogonaldecomposition
∥Ψµ−y∥2 =∥Ψµˆ −y∥2+∥Ψ(µ−µˆ )∥2.
2 LS 2 LS 2
Wenextaddthesecondtermontherighttotheexponentinthepriorandcompletethesquare:
∥Ψ(µ−µˆ )∥2+λ∥µ∥2
LS 2 2
(cid:16) (cid:17)
= µT ΨTΨ+λI µ−2(Ψµˆ )Tµ+µˆT ΨTΨµˆ
LS LS LS
(cid:16) (cid:17) (cid:16) (cid:17)−1
= (µ−µ )T ΨTΨ+λI (µ−µ )−µˆT ΨTΨ ΨTΨ+λI ΨTΨµˆ +µˆT ΨTΨµˆ . (23)
0 0 LS LS LS LS
where
(cid:16) (cid:17)−1
µ = ΨTΨ+λI ΨTΨµˆ .
0 LS
Theconstanttermin(23)is
(cid:16) (cid:17)−1
−µˆT ΨTΨ ΨTΨ+λI ΨTΨµˆ +µˆT ΨTΨµˆ
LS LS LS LS
(cid:18) (cid:16) (cid:17)−1 (cid:19)
= µˆT ΨT I−Ψ ΨTΨ+λI ΨT Ψµˆ . (24)
LS LS
Plugging(24)backinto(22)andintegrating,weget
(cid:90)
p(y|X,E)= p(y|X,E,µ)p(µ|E)dµ
 (cid:18) (cid:16) (cid:17)−1 (cid:19) 
∥Ψµˆ −y∥2+µˆT ΨT I−Ψ ΨTΨ+λI ΨT Ψµˆ
LS 2 LS LS
=(cid:0) 2πσ2(cid:1)−n/2 exp
−


 2σ2 
(cid:90)  (cid:0) µT −µ (cid:1)T(cid:16) ΨTΨ+λI(cid:17)(cid:0) µT −µ (cid:1)
·
(cid:0) 2πσ2λ−1(cid:1)−b/2
exp−
0 0
dµ. (25)
2σ2
Byachangeofvariables,theintegralcanbecomputedas
(cid:90) (cid:0) 2πσ2λ−1(cid:1)−b/2 exp −(cid:0) µT −µ 0(cid:1)T(cid:16) λ−1ΨTΨ+I(cid:17)(cid:0) µT −µ 0(cid:1) dµ=det(cid:16) λ−1ΨTΨ+I(cid:17)−1/2
.
2σ2λ−1
Pluggingthisbackinto(25)andtakinglogarithmsyields(10).
29Proof of Lemma B.3. Let sˆ ,sˆ ,...,sˆ be the eigenvalues of Σˆ. Using Lemma B.5, we have sˆ = 0 for any
1 2 b i
i>df(E). Wemaythereforecompute
df(E)
(cid:16) (cid:17) (cid:88)
logdet λ−1ΨTΨ+I = log(nλ−1sˆ +1)
i
i=1
df(E) (cid:18) (cid:19) df(E)
=
(cid:88)
log
sˆ i+λ/n
+df(E)logn+
(cid:88)
log(s /λ).
s i
i
i=1 i=1
Itremainstoboundthefirstterm. Tothisend, wefirstconditiononthe1−δ probabilityeventguaranteedby
LemmaB.6. Then,weobservethat
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12)
(cid:12)
(cid:12)sˆ i+ sλ/n −1(cid:12)
(cid:12) (cid:12)≤
s1
|sˆ i−s i|+
nλ
i i
1 (cid:18)(cid:13) (cid:13) λ(cid:19)
≤ (cid:13)Σˆ −Σ(cid:13)+
s (cid:13) (cid:13) n
min
(cid:32) (cid:40)(cid:114) √ (cid:41) (cid:33)
1 4m3/2log(2df(E)/δ) 4 mlog(2df(E)/δ) λ
≤ max , + . (26)
s n n n
min
Here,thesecondinequalitymakesuseofWeyl’sinequality.
Recalltheelementaryinequality
|logx|≤2|x−1|
for0<x<1/2. Usingthistogetherwith(26),weget
df(E) (cid:18) (cid:19) (cid:32)(cid:114) (cid:33)
(cid:88)
log
sˆ i+λ/n
=O
log(1/δ)
s n
i
i=1
whenn≥max(cid:8) 64m3/2s−2 log(2df(E)/δ),4λs−1 (cid:9) .
min min
ProofofLemmaB.4. RecallthatΨµˆ =P y. Wethusrewriteandboundtheerrortermas
LS E
(cid:18) (cid:16) (cid:17)−1 (cid:19)
µˆT ΨT I−Ψ ΨTΨ+λI ΨT Ψµˆ =yTP MP y
LS LS E E
≤∥P y∥2∥M∥, (27)
E 2
where
(cid:16) (cid:17)−1
M=P −Ψ ΨTΨ+λI ΨT.
E
UsingsimilarargumentsasintheproofofLemmaB.1,webound
∥P y∥2 =(f∗)Tf∗+ϵTP ϵ
E 2 E E E
(cid:16) (cid:112) (cid:17) (cid:16) (cid:112) (cid:17)
≤D2 n+ nlog(1/δ) +D2 n+ df(E)log(1/δ) . (28)
f ϵ
Meanwhile,thenonzeroeigenvaluesofMareoftheform
sˆ λ
1− i =
sˆ +λ/n nsˆ +λ
i i
30fori=1,2,...,df(E). Thesecanbefurtherboundedas
λ λ
≤
nsˆ +λ n(s −|sˆ −s |)
i i i i
λ
≤ (cid:16) (cid:13) (cid:13)(cid:17). (29)
n s −(cid:13)Σˆ −Σ(cid:13)
min (cid:13) (cid:13)
Takingn≥16m3/2s−2 log(2df(E)/δ)andconditioningonthe1−δprobabilityeventguaranteedbyLemmaB.6,we
canfurtherbound(29m )i bn
yλ/2ns
,whichgives∥M∥=O(cid:0) n−1(cid:1)
. Combiningthiswith(28)andpluggingthemback
min
into(27)finishestheproof.
LemmaB.5(Rankofempiricalcovariancematrix). WiththenotationofLemmaB.3,wehaverank(Σˆ)≤rank(Σ).
Proof. Letl ,l ,...,l denotetherowsofΨ,notingthattheyarei.i.d. randomvectors. NotethatifΣv=0forsome
1 2 n
v,thisimpliesthatCov(cid:8) vTl(cid:9) =vTΣv=0,andsovTl≡0asarandomvariable. Inparticular,wehavevTl =0
i
fori=1,2...,n,andwealsogetΣˆv=0. Assuch,thenullspaceforΣiscontainedwithinthenullspaceforΣˆ. The
conclusionfollows.
LemmaB.6(Concentrationofempiricalcovariancematrix). WiththenotationofLemmaB.3,forany0<δ <1,with
probabilityatleast1−δ,wehave
(cid:13) (cid:13) (cid:40)(cid:114) √ (cid:41)
(cid:13) (cid:13)1 ΨTΨ−Σ(cid:13)
(cid:13)≤max
4m3/2log(2df(E)/δ) ,4 mlog(2df(E)/δ)
.
(cid:13)n (cid:13) n n
Proof. Letl ,l ,...,l denotetherowsofΨasbefore. Sinceeachpointcanonlybecontainedinasingleleafon
1 2 n √
eachtree,wehave∥l ∥ = m,whileΣalsosatisfies∥Σ∥≤m. UsingCorollary6.2011inWainwright(2019),we
j 2
thereforehave
(cid:13) (cid:13) (cid:13)1 ΨTΨ−Σ(cid:13) (cid:13) (cid:13)≤2df(E)exp(cid:18)
− √
nt2 (cid:19)
(cid:13)n (cid:13) 2 m(m+t)
foranyt>0. Rearrangingthisequationcompletestheproof.
C Background on Markov Chains
Forthewholeofthissection,letX ,X ,...beanirreducibleandaperiodicdiscretetimeMarkovchainonafinitestate
0 1
spaceΩ,withstationarydistributionπ.
C.1 NetworksandVoltages
Harmonicfunctions. LetP bethetransitionkernelof(X ). Wecallafunctionh: Ω→RharmonicforP atastate
t
xif
(cid:88)
h(x)= P(x,y)h(y). (30)
y∈Ω
LemmaC.1(Uniquenessofharmonicextensions,Proposition9.1. inLevinetal.(2006)). LetA ⊂ Ωbeasubset
of the state space. Let h : A → R be a function defined on A. The function h: Ω → R defined by h(x) :=
A
E{h (X )|X =x}istheuniqueextensionofh suchthath(x)=h (x)forallx∈AandhisharmonicforP at
A τA 0 A A
allx∈Ω\A.
11WhileCorollary6.20isstatedwiththeassumptionthattherowshavemeanzero,theproofinWainwright(2019)illustratesthatthisis
unnecessary.
31Thevaluesofaharmonicfunctioncanbecomputedbysolvingthesystemoflinearequationsgivenby(30)foreach
x∈Ω\A. Thisishardtododirectlybyhandforcomplicatedstatespaces,butwhentheMarkovchainissymmetric,i.e.
thestationarydistributionsatisfiesπ(x)P(x,y)=π(y)P(y,x)foranyx,y,wecanuseseveraloperationstosimplify
thestatespacewhilepreservingthevaluesoftheharmonicfunctionontheremainingstatespace. Indeed,harmonic
functionsareequivalenttovoltagesonelectricalcircuits, anditiswell-knownhowtosimplifycircuitsinorderto
calculatevoltages:
1. (Gluing)Pointsonthecircuitwiththesamevoltagecanbejoined.
2. (Series law) Two resistors in series with resistances r and r can be merged with the new resistor having
1 2
resistancer +r .
1 2
3. (Parallellaw)Tworesistorsinparallelwithresistancesr andr canbemergedwiththenewresistorhaving
1 2
resistance1/(1/r +1/r ).
1 2
Wemakethisconnectionrigorousbyintroducingthefollowingdefinitionsandviathesubsequentlemmas.
Networks,conductance,resistance. Anetwork(Ω,c)isatuplecomprisingafinitestatespaceΩandasymmetric
functionc: Ω×Ω→R calledtheconductance. Theresistancefunctionisdefinedasr(x,y)= 1 ,andcantake
+ c(x,y)
thevalueofpositiveinfinity. Wesaythat{x,y}isanedgeinthenetworkifc(x,y)>0. Anynetwork(Ω,c)hasan
associatedMarkovchainwhosetransitionprobabilitiesaredefinedbyP(x,y)= c(x,y) .
(cid:80) c(x,z)
z∈Ω
Voltageandcurrentflow. WesaythatafunctionisharmoniconthenetworkifitisharmonicwithrespecttoP.Given
a,z ∈Ω,avoltageW betweenaandzisafunctionthatisharmoniconΩ\{a,z}. ThecurrentflowI: Ω×Ω→R
associatedwithW isdefinedasI(x,y)=c(x,y)(W(x)−W(y)). Thestrengthofthecurrentflowisdefinedas
(cid:88)
∥I∥:= c(a,y)(W(a)−W(y)).
y∈Ω
Effectiveresistanceandeffectiveconductance. Theeffectiveresistancebetweenaandzisdefinedas
W(a)−W(z)
R(a↔z):= ,
∥I∥
notingthatthisisindependentofthechoiceofW bytheuniquenesspropertyinLemmaC.1. Theeffectiveconductance
betweenaandzisdefinedasC(a↔z):=1/R(a↔z).
LemmaC.2(Networksimplificationrules). Consideranetwork(Ω,c). Definethefollowingoperationsthateach
producesamodifiednetwork(Ω′,c′).
1. (Gluing)Givenu,v ∈Ω,defineΩ′ :=Ω\{v}and

c(x,y) x,y ̸=u,
c(x,u)+c(x,v)
x̸=u,y =u,
c′(x,y)=
c c( (u u, ,y v)) ++ cc (( uv, ,y y)
)+c(u,x)
x x= =u y, =y ̸= u.u
2. (Parallelandserieslaws)Givenu,v,w ∈Ωwithc(v,x)=0forallx∈/ {u,w},defineΩ′ :=Ω\{v}and
(cid:40)
c(u,w)+ c(u,v)c(v,w) (x,y)=(u,w)or(w,u),
c′(x,y)= c(u,v)+c(v,w)
c(x,y) otherwise.
ConsiderafunctionhthatisharmoniconΩ\A. Thefollowinghold:
321. Ifwegluestatesu,v ∈Ωsuchthath(u)=h(v),thenhremainsharmoniconΩ′\Awithrespecttothemodified
transitionmatrixP′.
2. Ifweapplytheparallelandserieslawstou,v,w ∈ Ωwithv ∈/ A, thenhremainsharmoniconΩ′\Awith
respecttothemodifiedtransitionmatrixP′.
Furthermore,ifhisavoltagebetweentwostatesa,z ∈Ω,thenapplyingtheoperationsdoesnotchangetheeffective
conductanceandresistancebetweenthem.
Proof. Thefirststatementisobviousasthemeanvalueequationforharmonicfunctionscanberepeatedalmostverbatim.
Forthesecondstatement,wejusthavetocheckthemeanvalueequationforhatu. Thisisequivalenttotheequation
(cid:88)
c′(u,x)(h(x)−h(u))=0. (31)
x∈Ω′
Firstnotethatundertheoriginalnetwork,ourassumptiononc(v,x)andthemeanvaluepropertyatvgives
c(v,w)(h(w)−h(v))=c(u,v)(h(v)−h(u)). (32)
Next,wecompute
(cid:18) (cid:19)
c(u,v)c(v,w)
c′(u,w)(h(w)−h(u))= c(u,w)+ (h(w)−h(u))
c(u,v)+c(v,w)
c(u,v)c(v,w)
=c(u,w)(h(w)−h(u))+ ((h(w)−h(v))+(h(v)−h(u))
c(u,v)+c(v,w)
c(u,v)2+c(u,v)c(v,w)
=c(u,w)(h(w)−h(u))+ (h(v)−h(u))
c(u,v)+c(v,w)
=c(u,w)(h(w)−h(u))+c(v,w)(h(v)−h(u)), (33)
wherethethirdequalityfollowsfrom(32). Wethereforehave
(cid:88) (cid:88)
c′(u,x)(h(x)−h(u))= c(u,x)(h(x)−h(u)),
x∈Ω′ x∈Ω
andthemeanvalueequationatufortheoriginalnetworkimplies(31).
Finally,toconcludeinvarianceofeffectiveconductance,weobservethatitisisdefinedintermsofvoltagesand
currentflows. Wehavealreadyshownthatvoltagesareunchanged,sowejustneedtoarguethatthestrengthofthe
currentflowissimilarlyunchanged. Thisisimmediatewhenevera∈/ {u,w}. Whena=u,thisfollowsfrom(33).
LemmaC.3(Rayleigh’sMonotonicityLaw,Theorem9.12inLevinetal.(2006)). Givenanetworkwithtworesistance
functionsr,r′: Ω×Ω→R ∪{∞},supposethatr ≤r′pointwise. Thenwehave
+
R(a↔z;r)≤R(a↔z;r′).
C.2 HittingPrecedenceProbabilities
Hittingprecedenceprobabilities. LetA,B ⊂Ωbedisjointsubsets. ThehittingprecedenceprobabilityofArelative
toBisdefinedasthefollowingfunctiononΩ:
HPP(x;A,B):=P{τ <τ |X =x}.
A B 0
LemmaC.4. HPP(−;A,B)isaharmonicfunctiononΩ\(A∪B).
33Proof. Writeh (z)=1{z ∈A}. Itiseasytoseethat
A∪B
(cid:40)
1 τ <τ
h (X )= A B
A∪B τA∪B
0 τ >τ .
A B
Hence,
E{h (X )|X =x}=P{τ <τ |X =x}=HPP(x;A,B).
A∪B τA∪B 0 A B 0
ByLemmaC.1,thelefthandsideisaharmonicfunctiononΩ\(A∪B).
Lemma C.5 (HPP from a bottleneck state). Given a network (Ω,c) with states a,x,z and such that c(u,z) = 0
forallu ∈/ {x,z}. Letx = x ,x ,...,x = abeanysequenceofstatessuchthatthereissomeρ > 0forwhich
0 1 k
c(x ,x )≥ρ−1c(x,z)fori=1,...,k. Then
i−1 i
1
P{τ <τ |X =x}≥ .
a z 0 kρ+1
Proof. Let(Ω,c′)bethemodifiednetworkinwhichweset
(cid:40)
c(u,v) {u,v}∈{{x ,x }: i=1,...,k}∪{{x,z}}
c′(u,v)= i−1 i
0 otherwise.
Thenwehave
r(x,z)
P{τ <τ |X =x}=
a z 0 R(a↔x;r)+r(x,z)
r(x,z)
≥ , (34)
R(a↔x;r′)+r(x,z)
wheretheequalityusesLemmaC.6andtheinequalityusesLemmaC.3. Next,usingtheserieslawfromLemmaC.2,
wehave
k
(cid:88)
R(a↔x;r′)= r(x ,x )
i−1 i
i=1
≤kρr(x,z).
Pluggingthisinto(34)andcancellingr(x,z)inthenumeratoranddenominatorcompletestheproof.
LemmaC.6. Givenanetwork(Ω,c)withstatesa,x,zandsuchthatc(u,z)=0forallu∈/ {x,z}. Thenwehave
C(a↔x)
P{τ <τ |X =x}= .
a z 0 C(a↔x)+c(x,z)
Proof. Leth(y):=P{τ <τ |X =y},andnotethathisavoltagebetweenaandz. Assuch,wehave
a z 0
h(a)−h(z) 1
R(a↔z)= = .
∥I∥ ∥I∥
Ontheotherhand,hisalsoavoltagebetweenaandxonthereducedstatespaceΩ\{z},whichgives
h(a)−h(x)
R(a↔x)= .
∥I∥
Finally,bytheserieslaw,wehave
R(a↔z)=R(a↔x)+r(x,z).
34Puttingeverythingtogether,weget
P{τ <τ |X =x}=1−(h(a)−h(x))
a z 0
=1−R(a↔x)∥I∥
R(a↔x)
=1−
R(a↔z)
r(x,z)
=
R(a↔x)+r(x,z)
C(a↔x)
= ,
C(a↔x)+c(x,z)
aswewanted.
D Proof of Theorem 5.2
WewillfirstpresenttheproofsforTheorem5.2andTheorem5.3becausetheyarerelativelysimplecomparedtothatof
Theorem5.1. Forconvenience,werepeattherelevantconstructionsanddefinitionshere.
Reachability. Given E,E′ ∈ Ω , we say that E ≿ E′ if E and E′ are connected by an edge, and if either
TSE,m
Bias2(E;f∗)>Bias2(E′;f∗)orBias2(E;f∗)=Bias2(E′;f∗)anddf(E)≥df(E′). Notethat,becauseweallowonly
“grow” and “prune” moves, for adjacent E and E′, F(E) and F(E′) are nested subspaces, so that Bias2(E;f∗) =
Bias2(E′;f∗)ifandonlyifΠ [f∗]=Π [f∗]. WesaythatEisreachablefromE′,denotedE⪰E′,ifthereisa
F(E) F(E′)
sequenceofTSEsE=E0,E1,...,Ek =E′suchthatEi ≿Ei+1fori=0,1,...,k−1.
Set-up. Withoutlossofgenerality,let(x ,x )beapureinteractionforf∗. LetE beanyTSEsuchthat
1 2 bad
• E isreachablefromE ;
bad ∅
• TheredoesnotexistE∈Ω suchthatEisreachablefromE butE isnotreachablefromE.
TSE,m bad bad
NotethatsuchaTSEexistsbecauseΩ isfiniteand⪰isapartialorderingonthisspace. Bydefinition,thereexists
TSE,m
asequenceofTSEsE = E0,E1,...,Ek = E suchthatEi ⪰ Ei+1 fori = 0,1,...,k−1. WesetAtobethe
∅ bad
equivalenceclassofE under⪰andsetBtobetheouterboundaryofA. Fornlargeenough,usingProposition4.2
bad
andProposition4.1,thereisa1−δ/2eventoverwhich,fori=1,2,...,k,
logp(Ei|y)−logp(Ei−1|y)
(cid:40) n (cid:0) Bias2(Ei−1;f∗)−Bias2(Ei;f∗)(cid:1) +O(cid:16)(cid:112) nlog(k/δ)(cid:17) ifBias2(Ei−1;f∗)>Bias2(Ei;f∗)
= 2σ2
logn(cid:0) df(Ei−1)−df(Ei)(cid:1)
+O(log)(k/δ) otherwise.
2
Ineithercase,weget
p(Ei|y)
=Ω(1). (35)
p(Ei−1|y)
UsingProposition4.1again,thereisafurther1−δ/2probabilityeventoverwhich∆BIC(E,E )satisfieseither(5)
bad
or(6)simultaneouslyforallE∈B(afterdividingtheδthatappearsintheformulasby|B|). Conditiononthesetwo
events.
35Hittingprecedenceprobabilitylowerbound. UsingLemmaD.1andLemmaD.2,weseethatEi ∈/ OPT (f∗,∞)
m
fori=0,1,...,k. Wethereforehave
P(cid:8)
τ <τ
(cid:9) ≥P(cid:8)
E
=Eifori=1,2,...,k(cid:9)
Ebad OPTm(f∗,∞) i
k
(cid:89)
= P(Ei−1,Ei). (36)
i=1
ItsufficestoshowthatP(Ei−1,Ei)isboundedfrombelowbyaconstant. Toseethis,wenotethat
(cid:26) Q(Ei,Ei−1)p(Ei|y) (cid:27)
P(Ei−1,Ei)=Q(Ei−1,Ei)min ,1
Q(Ei−1,Ei)p(Ei−1|y)
(cid:18) (cid:26) p(Ei|y) (cid:27) (cid:19)
=Ω min ,1
p(Ei−1|y)
=Ω(1), (37)
wherethefirsttwoinequalitiesfollowbecausetheproposaldistributionsdonotdependonthetrainingsamplesizen,
whilethelastequalityfollowsfromequation(35).
BIC lower bound. Consider E′ ∈ B. By definition of B, there exists E ∈ A such that E and E′ are connected
by an edge, but E ̸≿ E′. This implies that either Bias2(E;f∗) < Bias2(E′;f∗) or Bias2(E;f∗) = Bias2(E′;f∗)
anddf(E;f∗) < df(E′;f∗). SinceEandE aremutuallyreachable,wehaveBias2(E;f∗) = Bias2(E ;f∗)and
bad bad
df(E)=df(E ). WethereforeconcludethatE′eitherhasalargersquaredbiasorlargerdegreesoffreedomcompared
bad
toE . Applyingequations(5)and(6)andtakingtheminimumsamplesizeN largeenoughgives
bad
∆BIC(E,E )≥logn+O(log(|B|/δ)). (38)
bad
Conclusion. ApplyingProposition6.1withequations(37)and(38),wegeta1−2δprobabilityeventoverwhich
E(cid:8)
τ
(cid:9) =Ω(cid:16) n1/2(cid:17)
. (39)
OPTm(f∗,∞)
LemmaD.1. Fori=0,1...,k,notreeinEicontainsasplitoneitherx orx .
1 2
Proof. Supposeotherwise. Bychangingthelabelingofx andx ifnecessary,thereexists
1 2
i:=min{1≤j ≤l: E containssplitonx }. (40)
j 1
Sinceonly“grow”and“prune”movesareallowed,E isobtainedfromE viaa“grow”movethatsplitsaleafnode
i i−1
Linto:
L :={x∈L: x ≤t}, L :={x∈L: x >t}.
L i R i
Definethefunctionψ :=1 −1 . Thenthespanof{1 ,1 }isthesameasthatof{1 ,ψ},whichimpliesthat
LL LR LL LR L
F(E )=span{F(E ),ψ}. (41)
i i−1
Furthermore,wehaveψ ∈/ F(E )sinceallfunctionsinF(E )donotdependonx . Thisimpliesthatdf(E )=
i−1 i−1 1 i
df(E ) + 1. On the other hand, since x ⊥⊥ y | x ∈ L, we have ψ ⊥ y, which means that Bias2(E ;f∗) =
i−1 i i
Bias2(E ;f∗). Assuch,wehaveE ̸≿E ,whichgivesacontradiction.
i−1 i−1 i
LemmaD.2. ForanyE∈Ω ,ifEdoesnotcontainsplitsonbothx andx ,thenBias2(E;f∗)>0.
TSE,m 1 2
Proof. Since(x ,x )̸⊥⊥y,thereexistvalues(a ,a ),(b ,b )and(c ,c ,...,c )suchthatf∗(a ,a ,c ,...,c )̸=
1 2 1 2 1 2 3 4 d 1 2 3 d
f∗(b ,b ,c ,...,c ). On the other hand, all functions in F(E) are constant with respect to x and x , so that
1 2 3 d 1 2
f∗ ∈/ F(E).
36E Proof of Theorem 5.3
Set-up. Forconvenience,werepeattherelevantconstructionhere. Withoutlossofgenerality,letx bethefeature
1
thatgivesf∗rootdependence. Byassumption,thereisathresholdtsuchthatsplittingthetrivialtreeonx attgivesa
1
decreaseinsquaredbias. Weset
A={T∈Ω : Thasrootsplitonx att},
TSE,1 1
and
T =argmin{BIC(E): E∈A}.
bad
Note that the outer boundary of A is a singleton set comprising the trivial tree T . By assumption, we have A∩
∅
OPT (f∗,0)=∅.
1
WecontinuethisconstructionbydenotingT = T andlettingT beatreestructurecomprisingasingleroot
0 ∅ 1
splitat(x ,t). ByaddingthesplitsinT iteratively,wegetasequenceT ,T ,...,T ofnestedtreestructureswith
1 bad 1 2 k
T = T . By Proposition 4.1 and Proposition 4.2, for any training sample size n large enough, there is a 1−δ
k bad
probabilityeventwithrespecttoP suchthat
n
logp(T |y)−logp(T |y)= n (cid:0) Bias2(T ;f∗)−Bias2(T ;f∗)(cid:1) +O(cid:16)(cid:112) nlog(2k/δ)(cid:17) (42)
j 0 2σ2 0 j
forj =1,2,...,k. Conditiononthisevent.
Wenotethefollowing:
(cid:90) (cid:18)(cid:90) (cid:19)2
Bias2(T ;f∗)= (f∗)2dν− f∗dν (43)
0
Bias2(T ;f∗)=0 (44)
k
Bias2(T ;f∗)≤Bias2(T ;f∗)<Bias2(T ;f∗) forj =1,2,...,k. (45)
j 1 0
Here,thelaststatementfollowsfromthefactthat
Bias2(T ;f∗)−Bias2(T ;f∗)
0 1 =Corr2(f∗(x),1{x ≤t})>0,
Bias2(T ;f∗) 1
0
andbecauseT isarefinementofT foreachj =1,2,...,k.
j 1
Hittingprecedenceprobabilitylowerbound. WedefineaconductancefunctiononΩ via
TSE,1
c(T,T′)=p(T|y)P(T,T′),
whereP isthetransitionkerneloftheBARTsampler. ItisclearthattheMarkovchainforBayesianCARTisequivalent
totheMarkovchainassociatedwiththenetwork(Ω ,c). Byignoringquantitiesthatdonotdependonthetraining
TSE,1
samplesizen,weget
c(T,T′)=min{p(T|y)Q(T,T′),p(T′|y)Q(T′,T)}
≍min{p(T|y),p(T′|y)}.
ConsiderthesetC :=Ω\(A∪{T }). ThenOPT (f∗,0)⊂C byassumption,andwehaveτ ≤τ . This
∅ 1 C OPT1(f∗,0)
meansthatitsufficestoconsider
P{τ <τ }=HPP(T ;C,T )
Tbad C ∅ bad
andtobounditfrombelow. TocalculatevaluesfortheharmonicfunctionHPP(−;C,T ),weuseLemmaC.2toglue
bad
allstatesinC togetherwithoutchangingthevaluesthefunctionvalues. Wewillabusenotationanddenotethenew
gluedstateusingC whilecontinuingtousectodenotetheconductancefunctiononthenewstatespace.
37NoticethattheonlyedgefromC connectstoT . Assuch,wecancompute
∅
(cid:88)
c(T ,C)≤ min{p(T |y)Q(T ,T),p(T|y)Q(T,T )}
∅ ∅ ∅ ∅
T∼T∅
(cid:88)
≤p(T |y) Q(T ,T)
∅ ∅
T∼T∅
≤p(T |y).
∅
Thisimplies,usingequations(42)and(45),thatwecanboundtheratiosofconductancesforj =1,2...,kas:
c(T ,T )
log j j−1 ≳min{logp(T |y)−logp(T |y),logp(T |y)−logp(T |y)}
c(T ,C) j 0 j−1 0
0
(cid:40) n (cid:0) Bias2(T ;f∗)−Bias2(T ;f∗)(cid:1) +O(cid:16)(cid:112) nlog(2k/δ)(cid:17) j ≥2
≥ 2σ2 0 j−1 (46)
0 j =1.
By (45), there is some minimum training sample size N so that for all n ≥ N, the right hand side side of (46) is
nonnegative. Inthiscase,theassumptionsofLemmaC.5hold,andweget
P{τ <τ }=Ω(1) (47)
Tbad C
asdesired.
BIClowerbound. Fromequations(43)and(44),wehave
∆BIC(T ,T )= n (cid:0) Bias2(T ;f∗)−Bias2(T ;f∗)(cid:1) +O(cid:16)(cid:112) nlog(2k/δ)(cid:17)
∅ bad σ2 ∅ bad
n (cid:32) (cid:90) (cid:18)(cid:90) (cid:19)2(cid:33) (cid:16)(cid:112) (cid:17)
= (f∗)2dν− f∗dν +O nlog(2k/δ) . (48)
σ2
Conclusion. ApplyingProposition6.1withequations(47)and(48),wegeta1−2δprobabilityeventoverwhich
E(cid:8)
τ
(cid:9) =Ω(cid:32) exp(cid:32) n (cid:32) (cid:90) (f∗)2dν−(cid:18)(cid:90) f∗dν(cid:19)2(cid:33) +O(cid:16)(cid:112) nlog(2k/δ)(cid:17)(cid:33)(cid:33)
.
OPT1(f∗,0) 2σ2
Takinglogarithms,dividingbynandapplyingMarkov’sinequalitygives
(cid:40) logE(cid:8) τ (cid:9)(cid:41) (1−δ)(1−O(n−1/2)(cid:32) (cid:90) (cid:18)(cid:90) (cid:19)2(cid:33)
E OPT1(f∗,0) ≥ (f∗)2dν− f∗dν .
n n 2σ2
Lettingδ →0andtakingn→∞finishestheproof.
F Splitting Rules, Local Decision Stumps and Coverage
BeforeprovingTheorem5.1,wefirstintroducesomerequiredmachinery.
Localdecisionstumpbasis. LetTbeatreestructure. Let(v ,τ ),j =1,...,ldenotethesplits(orsplittingrules)
j j
onT(thelabelsofitsinternalnodes). Everynodeonthetreecorrespondstorectangularregiont⊂X thatisobtained
byrecursivelypartitioningthecovariatespaceusingthesplitsfurtherupthetree. Iftisaninternalnode,ithastwo
childrennodesdenotedt andt definedby
L R
t :={x∈t : x ≤τ}
L v
38t :={x∈t : x >τ}
R v
where(v,τ)isthesplitont. Foreachinternalnode(t ,v ,τ),definealocaldecisionstumpfunction
j j
ν(t )1{x∈t }−ν(t )1{x∈t }
ψ (x):= R L L R , (49)
j (cid:112)
ν(t )ν(t )
L R
wheret andt denotethechildrenoft. Itiseasytocheck(seeforinstanceAgarwaletal.(2022))thatψ ,ψ ,...,ψ
L R 1 2 l
are orthogonal, and that, together with the constant function ψ ≡ 1, form a basis for F(T). Using this basis
0
makes it more convenient to analyze the difference between F(T) and F(T′) when T′ is obtained from T via a
“grow” move. Indeed, let ψ denote the local decision stump corresponding to the new split. We then have
l+1
F(T′)=F(T)⊕span(ψ ).
l+1
Now consider a TSE E = (T ,T ,...,T ). We may also write a basis for F(E) by concatenating the bases
1 2 m
{ψ ,ψ ,...,ψ }foreachtreeT ,togetherwiththeconstantfunction. IfE′ isobtainedfromTSEviaa“grow”
i,1 i,2 i,li i
move,welikewisehavethepropertyF(E′)=F(E)⊕span(ψ′),whereψ′isthelocaldecisionstumpcorrespondingto
thenewsplit. Ontheotherhand,thebasisfunctionsfromdifferenttreesneednotbeorthogonaltoeachother. Toregain
orthogonality,weusethefollowinglemma:
LemmaF.1(Conditionsfororthogonality). Supposeν =ν ×ν ×···×ν isaproductmeasureonX. LetT and
1 2 d 1
T betwotrees,andletI ,I ⊂ {1,2,...,d}betwodisjointsubsetsofindicessuchthatT andT containsplits
2 1 2 1 2
onlyonfeaturesinI andI respectively. Thenthelocaldecisionstumpsforbothtrees,{ψ ,ψ ,...,ψ }and
1 2 1,1 1,2 1,l1
{ψ ,ψ ,...,ψ },areorthogonaltoeachother.
2,1 2,2 2,l2
Proof. Consider two stumps from different trees: ψ and ψ . Under the assumption of a product measure,
1,k1 2,k2
{x : i∈I }isindependentof{x : i∈I }. Sinceψ isafunctionofthefirstsetofvariablesandψ isafunction
i 1 i 2 1,k1 2,k2
ofthesecondset,theyarethusindependentofeachother. Wethereforehave
(cid:90) (cid:90) (cid:90)
ψ ψ dν = ψ dν ψ dν =0,
1,k1 2,k2 1,k1 I1 2,k2 I2
wherethesecondequalityfollowsfromthefactthatalllocaldecisionstumpshavemeanzero.
Lemma F.2 (Existence of informative split). For any finite contiguous subset of integers I, let g: I → R be non-
constant. Letν beanymeasureonI. Thenthereexistsasplitatathresholdtwithassociateddecisionstumpψsuch
thattisaknotforgand
(cid:18)(cid:90) (cid:19)2
ϕgdν >0.
Assuch,thereisasequenceofrecursivesplitswithassociatedlocaldecisionstumpsψ ,ψ ,...,ψ ,whereq isthe
1 2 q
numberofknotsofg,suchthat
• g ∈span(ψ ,ψ ,...,ψ );
1 2 q
(cid:0)(cid:82) (cid:1)2
• ψ gdν >0fori=1,2,...,q;
i
• ψ splitsonaknotofgfori=1,2,...,q.
i
Proof. Leti ,i ,...,i denotetheknotsofg,andletψ˜ denotethedecisionstumpfunctionscorrespondingtoasplit
1 2 k j
atthresholdx=i forj =1,2,...,k,usingtheformula(49). Letψ˜ ≡1denotetheconstantfunctionasusual. Then
j 0
(cid:16) (cid:17)
itiseasytoseethatg ∈span ψ˜ ,ψ˜ ,...,ψ˜ . Assuch,ifg ⊥ψ˜ fork >1,theng ∈span(ψ˜ ),i.e. gisaconstant
0 1 k k 0
function. Toconcludethesecondstatement, weapplythefirstpartrecursivelytoleavesobtainedbymakingeach
split.
LemmaF.3(Formulafordecreaseinbias). SupposeE′ isobtainedfromEviaa“grow”move. Letψ bethelocal
decisionstumpassociatedwiththenewsplit. Letϕ:= ψ−ΠF(E)[ψ] . Thenforanyregressionfunctionf,wehave
∥ψ−ΠF(E)[ψ]∥
L2(ν)
(cid:18)(cid:90) (cid:19)2
Bias2(E′;f)=Bias2(E;f)− ϕfdν .
39Proof. Wehave
F(E′)=F(E)⊕span(ψ′)=F(E)⊕span(ϕ),
withthelastexpressioncomprisinganorthogonaldecomposition. Assuch,wehave
Bias2(E;f)−Bias2(E′;f)=(cid:13)
(cid:13)f −Π
F(E)[f](cid:13) (cid:13)2 L2(ν)−(cid:13)
(cid:13)f −Π F(E)[f]−Π
span(ϕ)[f](cid:13) (cid:13)2
L2(ν)
(cid:13) (cid:13)2
=(cid:13)Π span(ϕ)[f](cid:13)
L2(ν)
(cid:18)(cid:90) (cid:19)2
= ϕfdν
aswewanted.
Coverage. Wefirstintroducesomeusefulnotation. Givenacoordinateindexi,letx−i ∈ {1,2,...,b}d−1,x ∈
i
{1,2,...,b}. Combining these, we let (x−i,x ) ∈ {1,2,...,b}d have i-th coordinate equal to x and all other
i i
coordinatesgivenbyx−i. Alsousee ∈Rdtodenotethei-thcoordinatevector. Givenareal-valuedfunctionf defined
i
on{1,2,...,b}d,wesaythatx∈{1,2,...,b}disajumplocationforf withrespecttofeatureiiff(x)̸=f(x+e ).
i
LetVbeaPEM.Foreachfeaturei=1,2,...,dandt=1,2,...,b,thecoverageofVofthesplit(i,t)isdefinedas
(cid:110) (cid:111)
coverage(i,t;V):= x−i ∈{1,2,...,b}d−1: ∃f ∈V,(x−i,t)isajumplocationforf withrespecttofeaturei .
(50)
Ifcoverage(i,t;V)={1,2,...,b}d−1,wesaythatVhasfullcoverageofthesplit(i,t). Notethatgivenacollection
ofPEMsV ,V ,...,V ,wehave
1 2 m
m
(cid:91)
coverage(i,t;V +V ,...,V )= coverage(i,t;V ).
1 2 m j
j=1
LemmaF.4(Zerobiasrequiresfullcoverageofallknots). Supposef∗ ∈V,wheref∗(x)=f (x )+f (x )+···+
1 1 2 2
f (x )forsomeunivariatefunctionsf ,f ,...,f . Thenforanyfeature1≤i≤m′andanyknottoff ,i.e. a
m′ m′ 1 2 m′ i
valueforwhichf (t)̸=f (t+1),Vhasfullcoverageofthesplit(i,t).
i i
Proof. Foranyx−i ∈{1,2,...,b}d−1,wehave
f∗((x−i,t+1))−f∗((x−i,t)=f (t+1)−f (t)̸=0,
i i
sothat(x−i,t)isajumplocationforf∗withrespecttofeaturei. Hence,iff∗ ∈V,itprovidesthedesiredfunctionin
thedefinition(50).
LemmaF.5(Fullcoverageimpliesinclusionofgridcells). SupposeV = F(T)forasingletreeT. SupposeVhas
fullcoverageofsplit(i,ξ ),(i,ξ ,...,(i,ξ )fori=1,2,...,k. Foranychoiceof1≤j ≤q ,i=1,2,...,k,
i,1 i,2 i,qi i i
denotethecell
(cid:8) (cid:9)
C := x: ξ <x ≤ξ fori=1,2,...,k .
i,ji−1 i i,ji
Wethenhave1 ∈V.
C
Proof. Letx∈C beanypoint,andletL(x)betheleafofTcontainingx. WeclaimthatL(x)⊂C. Supposenot,then
thereexistsacoordinatedirectioniinwhichL(x)exceedsC. Byreorderingifnecessary,wemaythusassumethat
(x−i,ξ +1)∈L(x). Foranyf ∈V,wemaywritef =a 1 +(cid:80)L a 1 ,whereL ,L ,...,L areleaves
i,ji 0 L(x) l=1 l Ll 1 2 L
inT. Wethenhave
f(x−i,ξ +1)=a =f(x−i,ξ ),
i,ji 0 i,ji
whichcontradictsourassumptionthatVhasfullcoverageofthesplitat(i,ξ ). Wethushave
i,ji
(cid:91) (cid:91)
C = L(x)= L .
l
x∈C Ll⊂C
Sincetherighthandsideisunionoveracollectionofdisjointsets,wehave1 =(cid:80) 1 ∈V.
C Ll⊂C Ll
40LemmaF.6(Sufficientconditionforlackofcoverage). LetTbeatreestructure. Considerasplit(i,t),andlettbe
anynodeinT. Supposethefollowinghold:
• (x−i,t)∈tforsomex−i ∈{1,2...,b}d−1;
• Noancestorordescendantoft,includingtitself,usesthesplittingrule(i,t);
Thenifwelett−idenotetheprojectionoftontoallbutthei-thcoordinate,wehave
coverage(i,t;F(T))∩t−i =∅.
Proof. Let z−i ∈ t−i be any point. By the first assumption, t is within the bounds of t along direction i, so
(z−i,t) ∈ t. LetL betheleafnodecontaining(z−i,t). Bythesecondassumption, (z−i,t+1) ∈ L , otherwise
0 0
someparentofLwouldhavemadethesplit(i,t). Toshowthatz−i ∈/ coverage(i,t;F(T)),itsufficestoshowthat
f(z−i,t+1)=f(z−i,t)forallf ∈F(T). Wemaywritef =(cid:80)k a 1 whereL ,L ,...,L areotherleavesin
j=0 j Lj 1 2 j
T. Wethenhavef(z−i,t+1)=a =f(z−1,t)aswewanted.
0
PropositionF.7(Dimensionofadditivefunctions). Supposef∗ ∈V,wheref∗(x)=f (x )+f (x )+···+f (x )
1 1 2 2 m m
forsomeunivariatefunctionsf ,f ,...,f . Thenforanyl < mwehavedim (f∗) > dim (f∗), whileforany
1 2 m′ l m
l≥m,wehavedim (f∗)=dim (f∗).
l m
Proof. LetV = F(E)forsomeEwithltrees,andsupposef∗ ∈ V. Fori = 1,2,...,m,let(i,ξ ),(i,ξ ),...,
i,1 i,2
(i,ξ )denotetheknotsoff ,whereq =dim (f )WeclaimthatEmustcontainanodewithsplittingrule(i,ξ )
i,qi−1 i i 1 i i,j
foreverychoiceofiandj. Ifnot,thenVdoesnotcover(i,ξ ),contradictingLemmaF.4.
i,j
ConstructEviaasequenceofgrowmovesinarbitraryorder,therebyderivingasequenceE =E0,E1,...,Ek.
∅
Considerthefirsttimet forwhichasplitusingrule(i,ξ )isaddedtotheTSE.Letψ denotethelocaldecision
i,j i,j i,j
stump associated with the split. Since F(Eti,j−1) has zero coverage of (i,ξ i,j), we have df(E i,j) = df(E i,j)+1.
Addingthisupoverallsplitsgivesthelowerbounddf(V)≥(cid:80)m
q −m. Byputtingallsplitsonfeatureionthei-th
i=1 i
treefori=1,2,...,m,weseethatthislowerboundisachievablewheneverl≥m,therebygiving
m
(cid:88)
dim (f∗)= q −m=dim (f∗).
l i m
i=1
Whenl<m,thenbythepigeonholeprinciple,thereexistssplitsondifferentfeatures(i,s)and(j,t)thatoccuron
thesametree. Weclaimthatthisimpliesthateitherthereexiststwolinearlyindependentlocaldecisionstumpsϕandψ
splittingon(i,s)orthesameappliesto(j,t). Supposenot,thentheuniquelocaldecisionstumpsplittingon(i,s)must
notdependonanyotherfeature,whilethesameappliestothatsplittingon(j,t). However,becausenodesdependon
thefeatureusedintherootsplit,thiscannotbesimultaneouslytrueiftheybothcorrespondtosplitsonthesametree.
Byrepeatingtheargumentabove,wethereforegetdf(E)≥(cid:80)m
q −m+1. SincethisholdsforanyE,wehave
i=1 i
m
(cid:88)
dim (f∗)≥ q −m+1>dim (f∗)
l i m
i=1
aswewanted.
G Proof of Theorem 5.1 Part 1
Set-up. Fori=1,2,...,m′,denoteq =dim (f ),andlet0=ξ <ξ <···<ξ =bdenotetheknotsoff ,
i 1 i i,0 i,1 i,qi j
i.e. thevaluesforwhichf (ξ )̸=f (ξ +1),togetherwiththeendpoints.12 Withoutlossofgenerality,assumethat
i i,j i i,j
f ,...,f areorderedindescendingorderoftheir1-ensembledimension,i.e. q ≥q ≥···≥q .
1 m′ 1 2 m′
WenowdefineE anda“badset”A. Tothisend,wedefineacollectionofpartitionmodelsV ,V ,...,V
bad 1 2 m
(spansofindicatorsinasinglepartition)asfollows. First,fori = 3,4,...,m−1,j = 1,2,...,q ,definethecells
i
12dim1(fi)issimplythenumberofconstantpiecesoffioralternatively,onelargerthanthenumberofknotsoffi.
41L :={x: ξ <x ≤ξ },andsetV =span(cid:0)(cid:8) 1 : j =1,2,...,q (cid:9)(cid:1) ,i.e. foreachi,V containssplitsonly
i,j i,j−1 i i,j i Li,j i i
onfeaturei,andonlyattheknotsoff . Thisalsoimpliesthatf ∈V . Next,fortheremainingcomponentfunctions,
i i i
i=m,m+1,...,m′,j =1,2,...,q ,definethecells
i i
L
:=(cid:8)
x: ξ <x ≤ξ
fori=m,m+1,...,m′(cid:9)
, (51)
m,jm,jm+1,...,j m′ i,ji−1 i i,ji
and set V to be the span of their indicators. Observe that V comprises a grid contains splits only on features
m m
m,m+1,...,m′,andonlyattheknotsoff ,f ,...,f respectively. Thisalsoimpliesf ,f ,...,f ∈
m m+1 m′ m m+1 m′
V .
m
Finally, to introduce inefficiency, we define each of V and V to have splits on both features 1 and 2. This
1 2
construction is fairly involved and will be detailed in full in the next section. For now, it suffices to assume that
f +f ∈V +V ,whichimpliesthatf∗ ∈V +V +···+V . DefineAvia
1 2 1 2 1 2 m
A:={(T ,T ,...,T ): F(T )=V fori=1,2,...,m}.
1 2 m i i
ItisclearthatforanyE∈A,wehaveF(E)=V +V +···+V ,sothatAcomprisesacollectionofTSEswith
1 2 m
zerobias. WewillsetE tobeaparticularelementofA. WesetBtobetheouterboundaryofA.
bad
Hitting precedence probability lower bound. Step 1: Construction of path. We construct a path in Ω
TSE,m
comprisingE =E0,E1,...,Es =E suchthatEi+1isobtainedfromEiviaa“grow”movefori=0,1,...,s−1.
∅ bad
To do this, we first apply Lemma F.2 to obtain, for each feature j = 1,2,...,m′, a sequence of recursive splits
ψ ,ψ ,...,ψ suchthat
j,1 j,2 j,qj−1
• f ∈span(ψ ,ψ ,...,ψ );
j j,1 j,2 j,qj−1
(cid:0)(cid:82) (cid:1)2
• f ψ dν >0forl=1,2,...,q −1;
j j,l j j
• ψ splitsonaknotoff forl=1,2,...,q −1.
j,l j j
Notethatforconvenience,wehaveidentifiedthesplitswiththeirassociatedlocaldecisionstumps. Webreakupthe
pathintomsegments,0 = s
0
< s
1
< ··· < s
m
= k,withthej-thsegmentcomprisingEsj−1+1,Esj−1+2,...,Esj,
possessingthefollowingdesiredproperties:
• Duringthissegment,thej-thtreeisgrownfromtheemptytreeT toitsfinalstateT∗,whilenoothertreesare
∅ j
modified;
• F(T∗)=V ;
j j
• Bias2(Ei−1;f∗)>Bias2(Ei;f∗)fori=s +1,s +2,...,s .
j−1 j−1 j
Forj =3,4,...,m−1,thesplitsψ ,ψ ,...,ψ immediatelyyieldasequenceoftreesT =T0,T1,...,Tqj−1 =
j,1 j,2 j,qj−1 ∅ j j j
T∗suchthateachTl isobtainedfromTl−1viaaddingthesplitψ . Next,assumingcorrectnessoftheconstruction
j j j,l
uptoEsj−1,ψ
j,l
isorthogonaltoF(Esj−1)andalsototheprevioussplitsψ j,1,ψ j,2,...,ψ j,l−1,byLemmaF.1. This
allowsustoapplyLemmaF.3toget
(cid:18)(cid:90) (cid:19)2
Bias2(Esj−1+i;f)=Bias2(Esj−1+i−1;f)− ψ f∗dν .
i
Wethennoticethatbytheindependenceofdifferentfeatures,
(cid:18)(cid:90) (cid:19)2 (cid:18)(cid:90) (cid:19)2
ψ f∗dν = ψ f dν >0.
i i j
ToconstructT∗ ,fix0 ≤ p < m′−m,andassumethatwehaveconstructedatreeT∗ suchthattheleavesof
m m,p
T comprisethecollection(seeequation(51)):
m,p
(cid:8) (cid:9)
L : 1≤j ≤q ,i=0,1,2,...,p
m,jm,jm+1,...,jm+p m+i m+i
42WenowconstructasequenceT∗ =T0 ,T1 ,...,Tr =T∗ byloopingovertheleaves,andforeachleaf
m,p m,p m,p m,p m,p+1
L=L ,iterativelyaddingthesplitsψ˜ ,ψ˜ ,...,ψ˜ ,whereψ˜ =ν(L)−1/2ψ 1 for
m,jm,jm+1,...,jm+p 1 2 qm+p+1−1 l m+p+1,l L
l=1,2,...,q −1. Notethattheseareorthonormal. Whenaddingthesplitψ˜,thedecreaseinsquaredbiascan
m+p+1 l
thusbecomputedas
(cid:18)(cid:90) (cid:19)2 (cid:18)(cid:90) (cid:19)2
Bias2(Ti−1)−Bias2(Ti )= ψ˜f∗dν =ν(L) ψ f dν >0. (52)
m,p m,p l m+p+1,l j
WeliftthesequenceoftreestructurestoasequenceofTSEs,usingLemmaF.1andLemmaF.3asbeforetotranslate
(52)tobeintermsofthesequenceofTSEs.
ItremainstodefineV andV andconstructT∗andT∗. Leta betheindexoftheknotξ formingthethreshold
1 2 1 2 i i,ai
forψ fori = 1,2. Fork = 0,1,...,q −1,defineb = k fork < a andb = k+1fork ≥ a . Similarly,for
i,qi 1 k i k i
l=0,1,...,q −1,definec =lforl<a andc =l+1fork ≥a . SetV tobethespanofindicatorsofthecells
2 l 2 l 2 1
(cid:8) (cid:9)
L := x: ξ <x ≤ξ andξ <x ≤ξ
1,k,l 1,bk−1 1 1,bk 2,cl−1 2 2,cl
fork =1,2,...,q −1andl=1,2. Next,defined =0,d =a ,d =q ,e =0,e =a ,e =q . WesetV to
1 0 1 1 2 1 1 1 2 2 2 2
bethespanofindicatorsofthecells
(cid:8) (cid:9)
L := x: ξ <x ≤ξ andξ <x ≤ξ
2,k,l 1,dk−1 1 1,dk 2,el−1 2 2,el
fork,l=1,2. WeconstructT∗similarlytoT∗ ,throughrecursivepartitioningonx andx ,butwithoutmakingthe
1 m 1 2
finalsplitonbothfeatures. Usingthesameargumentasbefore,weobtainthedesiredsequenceoftrees. Finally,to
constructT∗,wesimplymaketheomittedsplitsonthenewtree. Moreprecisely,wedefinethesplits
2
ν{x >ξ }1{x ≤ξ }−ν{x ≤ξ }1{x >ξ }
ψ = 1 1,a1 1 1,a1 1 1,a1 1 1,a1
1 (cid:112)
ν{x ≤ξ }ν{x >ξ }
1 1,a1 1 1,a1
andψ =ν{x ≤ξ }1/2ϕ1{x≤ξ },ψ =ν{x >ξ }1/2ϕ1{x>ξ },where
2 1 1,a1 1,a1 3 1 1,a1 1,a1
ν{x >ξ }1{x ≤ξ }−ν{x ≤ξ }1{x >ξ }
ϕ= 2 2,a2 2 2,a2 2 2,a2 2 2,a2 .
(cid:112)
ν{x ≤ξ }ν{x >ξ }
2 2,a2 2 2,a2
Itisclearthatthese,togetherwiththeconstantfunction,spanV . Furthermore,itiseasytoseethatwehave
2
ψ 1−ΠV 1[ψ 1]
=ψ ,
∥ψ 1−ΠV 1[ψ 1]∥
L2(ν)
1,q1
ψ 2−ΠV 1⊕span(ψ1)[ψ 2]
=ν{x ≤ξ }−1/2ψ 1{x≤ξ },
∥ψ 2−ΠV 1⊕span(ψ1)[ψ 2]∥
L2(ν)
1 1,a1 2,q2 1,a1
ψ 3−ΠV 1⊕span(ψ1,ψ2)[ψ 3]
=ν{x >ξ }−1/2ψ 1{x>ξ }.
∥ψ 3−ΠV 1⊕span(ψ1,ψ2)[ψ 3]∥
L2(ν)
1 1,a1 2,q2 1,a1
Byconstruction,wehave
(cid:18)(cid:90) (cid:19)2 (cid:18)(cid:90) (cid:19)2
ψ f dν , ψ f dν >0.
1,q1 1 1 2,q2 2 2
ApplyingLemmaF.3thenshowsthatthedesiredpropertyforthe2ndsegmentofTSEsissatisfied. Itisclearfromthe
constructionthatψ ,ψ ,...,ψ ∈V andthatψ ∈V forj =1,2. Thisimpliesthatf +f ∈V +V as
j1 j2 jqj−1 1 jqj 2 1 2 1 2
desired.
Step2: Disjointnessfromoptimalset. Wehavealreadyproved,inthisconstruction,thatevery“grow”moveaddsa
splitthatdecreasesbias,andthereforemustbelinearlyindependentfromtheexistingPEM.Inotherwords,wehave
df(Ei+1) = df(Ei)fori = 0,1,...,k−1. Expandingthisgivesdf(E ) = k+1 = (cid:80)m (s −s )+1. Since
bad j=1 j j−1
eachsplitincrementsthenumberofleafnodesbyone,eachs −s +1isequaltothenumberofcellsinV ,which
j j−1 j
43wenowcomputeasfollows. Forj =3,4,...,m−1,wehaves −s =dim (f )−1,whileforj =m,wehave
j j−1 1 j
s −s =
(cid:81)m′
dim (f )−1. Likewise,wehaves = (dim (f )−1)(dim (f )−1)−1ands −s = 3.
m m−1 l=m 1 l 1 1 1 1 2 2 1
Puttingthesetogethergives
m−1 m′
(cid:88) (cid:89)
df(E )= dim (f )+ dim (f )+(dim (f )−1)(dim (f )−1)+2−m.
bad 1 j 1 l 1 1 1 2
j=3 l=m
Letusnowshowthatthisissuboptimal,byconstructingaTSEE withzerobiasbutfewerdegreesoffreedom.
good
Todoso,wesimplysetE =(T′T′,T∗,...,T∗ ),whereT∗hasthesamestructureasinT forj =3,4,...,m.
good 1 2 3 m j bad
Ontheotherhand,wedefineT′ usingψ ,ψ ,...,ψ andT′ usingψ ,ψ ,...,ψ . Byassumption,we
1 1,1 1,2 1,q1 2 2,1 2,2 2,q2
havef ∈F(T′)forj =1,2,whichyieldsunbiasedness. Thedegreesoffreedomisgivenby
j j
m−1 m′
(cid:88) (cid:89)
df(E )= dim (f )+ dim (f )−m. (53)
good 1 j 1 l
j=1 l=m
Takingthedifferencegives
df(E )−df(E )=(dim (f )−1)(dim (f )−1)+2−dim (f )−dim (f )
bad good 1 1 1 2 1 1 1 2
=(dim (f )−2)(dim (f )−2)−1,
1 1 1 2
whichisstrictlylargerthan0ifdim (f ),dim (f )>3. CombiningthiswiththefactthatBias2(Ei)>0foralli<s,
1 1 1 2
wethereforehaveEi ∈/ OPT (f∗,k)foralli=0,1,...,s,withk =(dim (f )−2)(dim (f )−2)−2.
m 1 1 1 2
Step3: Conclusion. UsingProposition4.1andProposition4.2,fornlargeenough,thereisa1−δ/2eventover
which
logp(Ei|y)−logp(Ei−1|y)= n (cid:0) Bias2(Ei−1;f∗)−Bias2(Ei;f∗)(cid:1) +O(cid:16)(cid:112) nlog(s/δ)(cid:17) . (54)
2σ2
Conditioningonthisset,weget
p(Ei|y)
=Ω(1)fori=1,2,...,sforallnlargeenough. Wemaythenrepeatthe
p(Ei−1|y)
calculationsintheproofofTheorem5.2,specificallyequations(36)and(37),toget
P(cid:8)
τ <τ
(cid:9) ≥P(cid:8)
E
=Eifori=1,2,...,s(cid:9)
=Ω(1).
Ebad OPTm(f∗,k) i
BIClowerbound. ConsiderE′ =(T′,T′,...,T′ )∈B. BydefinitionofB,thereexistsE=(T ,T ,...,T )∈
1 2 m 1 2 m
AsuchthatE′isobtainedfromEviaa“grow”,“prune”,“change”,or“swap”move. Wenowconsidereachtypeof
moveandshowthateitherBias2(E′;f∗)>0ordf(E′)>df(E)=df(E ). Toprovethis,wemakeuseofafewkey
bad
observations.
• Sinceamoveonlyaffectsonetreewithindexi ,wehaveT′ =T andF(T′)=V foralli̸=i ;
0 i i i i 0
• Everysplit(i,ξ )necessaryforF(E′)tobeunbiased(seeLemmaF.4)hasfullcoverageinasingletreeandhas
i,j
zerocoverageinallothertrees;
• Sincenomoveisallowedtoresultinanemptyleafnode,iftisaninternalnodeinT orT′ withasplit(i,ξ),
i0 i0
noancenstorordescendentoftmakesthesamesplit(i,ξ).
Forconvenience,wedenoteV =V +···V +V +···+V .
−i0 1 i0−1 i0+1 m
Case1: “grow”move. Letψdenotethelocaldecisionstumpassociatedwiththenewsplit,andletLdenotethe
leafthatissplit. Asshownearlier,wehaveψ ⊥V . Next,foranyboundarypoint(x−j,ξ)ofLindirectionj (that
i0
isnotanaboundarypointoftheentirespaceX),ψhasajumplocationat(x−j,ξ)withrespecttofeaturej. Onthe
otherhand,byourconstruction,thismeansthat(j,ξ)isasplitfullycoveredbyV andhaszerocoverageinV . By
i0 −i0
LemmaF.4,thismeansψ ∈/ V . Together,thisimpliesthatψ ∈/ F(E)anddf(E′)=df(E)+1asdesired.
−i0
Case2: “prune”move. Supposetheprunedsplitis(k,t)andoccursonaleafL. LetL−k denotetheprojectionof
theleafontoallbutthek-thcoordinate. ApplyingthethirdobservationabovetogetherwithLemmaF.6gives
coverage(k,t;F(E′))∩L−k =coverage(k,t;F(T′ ))∩L−k =∅.
i0
44LemmaF.4thenimpliesthatBias2(E′;f∗)>0.
Case 3: “change” move. Suppose the “changed” split occurs on a node t and is with respect to a feature k at
thresholdξ . Thensincenodescendantoftmakesthesamesplit,ifwelett−k denotetheprojectionoftontoallbut
k,j
thek-thcoordinate,thenasbefore,wehave
coverage(k,t;F(E′))∩t−k =coverage(k,t;F(T′ ))∩t−k =∅,
i0
andBias2(E′;f∗)>0.
Case4: “swap”move. Theswapmovecaneitherbeperformedonapairofparent-childnodes,oronaparent
withbothofitschildren,ifbothchildrenhavethesamesplittingrule. Inthelattercase,F(T′)=F(T),contradicting
ourassumption thatE′ ∈/ A. Intheformercase, let t denotethe parent, and lett and t denoteits twochildren.
L R
Supposewithoutlossofgeneralitythatthesplittingrules(j,s)and(k,t),oftandt respectively,aretobeswapped.
L
ThenbyconstructionofT ,tmustbeaknotforf . Bythethirdobservation,noancestoroft usesthesplittingrule
i0 k L
(k,t),whichimpliesthatadescendentoft mustspliton(k,t). However,thiswouldmeanthatthisswapmoveisnot
R
allowed,givingacontradiction.
Finally,usingProposition4.1andProposition4.2,fornlargeenough,thereisa1−δ/2eventoverwhich
∆BIC(E′,E)
(cid:40)
n
Bias2(E′;f∗)+O(cid:16)(cid:112) nlog(|B|/δ)(cid:17)
ifBias2(E′;f∗)>0
= σ2 (55)
logn(df(E′)−df(E ))+O(log(|B|/δ)) otherwise.
bad
Conditionfurtheronthisevent.
Conclusion. ApplyingProposition6.1togetherwithequations(54)and(55),whiletakingnlargeenough,wegeta
1−2δprobabilityeventoverwhich
E(cid:8)
τ
(cid:9) =Ω(cid:16) n1/2(cid:17)
,
OPTm(f∗,k
wherek =(dim (f )−2)(dim (f )−2)−1.
1 1 1 2
H Proof of Theorem 5.1 Part 2
Set-up. WefirstusethesamedefinitionofE =(T ,T ,...,T )asintheprevioussection(seetheparagraph
good 1 2 m
immediatelyprecedingequation(53)). TodefineE =(T∗,T∗,...,T∗ ),westartwithE andsimplyswapthe
bad 1 2 m good
rolesoff andf . Moreprecisely,weletT∗ =T forj =2,3,...,m−1. WedefineT∗ ascomprisingthelocal
1 m′ j j 1
decisionstumpsϕ ,ϕ ,...,ϕ ,whichweredefinedintheproofofthehittingprecedenceprobabilitylower
boundinthepreviom u′ s,1 secm tio′, n2 . Definem V′,q m =′ F(T∗)forj =1,2,...,m−1. WethendefineV tobeagridonfeatures
j j m
{1,m,m+1,...,m′−1},orinotherwords,F(T∗ )isthespanofindicatorsofthecells
m
L
:=(cid:8)
x: ξ <x ≤ξ
fori=1,m,m+1,...,m′−1(cid:9)
, (56)
m,jm,jm+1,...,j m′ i,ji−1 i i,ji
as we vary i = 1,m,m+1,...,m′ −1 and j = 1,2,...,q . We construct T∗ such that F(T∗ ), in the manner
i i m m
described in the proof of the hitting precedence probability lower bound in the previous section. Here, recall that
dim (f )>dim (f ),whichwillintroducesuboptimalityintoE .
1 1 1 m bad
Noticethatf∗ ∈V:=V +V +···V viathesameargumentasintheprevioussection. Next,wedefinetheset
1 2 m
Avia
A:={(E′,E′,...,E′): F(E )⊇V forj =1,2,...,m}.
1 j j
WedefineBtobetheouterboundaryofA.
45Hittingprecedenceprobabilitylowerbound. Thisfollowstheproofofthehittingprecedenceprobabilitylower
boundintheprevioussectionalmostexactly. First,usingthesameconstruction,thereispathinΩ comprising
TSE,m
E =E0,E1,...,Es =E suchthatEi+1isobtainedfromEiviaa“grow”moveand
∅ bad
Bias2(Ei;f∗)>Bias2(Ei−1;f∗)
fori=0,1,...,s−1.
Next,wecompute
m−1 m′−1
(cid:88) (cid:89)
df(E )= dim (f )+dim (f )+dim (f ) dim (f )−m.
bad 1 j 1 m′ 1 1 1 j
j=2 j=m
Takingthedifferencebetweenthisand(53)gives
m′−1 m′
(cid:89) (cid:89)
df(E )−df(E )=dim (f )−dim (f )+dim (f ) dim (f )− dim (f )
bad good 1 m′ 1 1 1 1 1 j 1 j
j=m j=m
m′−1
(cid:89)
=(dim (f )−dim (f )) dim (f ), (57)
1 1 1 m′ 1 j
j=m
whichisstrictlylargerthan0ifdim (f )>dim (f ). CombiningthiswiththefactthatBias2(Ei)>0foralli<s,
1 1 1 m′
wethereforehaveEi ∈/ OPT (f∗,k−1)wherekisthequantityontherighthandsideof(57).
m
Finally,usingProposition4.1andProposition4.2,fornlargeenough,thereisa1−δ/2eventoverwhich
logp(Ei|y)−logp(Ei−1|y)= n (cid:0) Bias2(Ei−1;f∗)−Bias2(Ei;f∗)(cid:1) +O(cid:16)(cid:112) nlog(s/δ)(cid:17) . (58)
2σ2
Conditioningonthisset,weget
p(Ei|y)
=Ω(1)fori=1,2,...,sforallnlargeenough. Wemaythenrepeatthe
p(Ei−1|y)
calculationsintheproofofTheorem5.2,specificallyequations(36)and(37),toget
P(cid:8)
τ <τ
(cid:9) ≥P(cid:8)
E
=Eifori=1,2,...,s(cid:9)
=Ω(1). (59)
Ebad OPTm(f∗,k) i
BIClowerbound. ConsiderE′ =(T′,T′,...,T′ )∈B. BydefinitionofB,thereexistsE=(T ,T ,...,T )∈
1 2 m 1 2 m
AsuchthatE′isobtainedfromEviaa“prune”move(a“grow”movewillnowbreakthedefiningconstraintofA.) We
nowshowthateitherBias2(E′;f∗)>0ordf(E′)≥df(E )+min dim (f )−2.
bad 1≤i≤m′ 1 i
LetT bethetreethatispruned,andsupposedtheprunesplitis(j,t),occurringonanodet. SinceF(T′ )̸⊇V ,
i0 i0 i0
(j,t)mustbeonafeaturesplitoninT∗ andonaknotξ forf . Furthermore,becauseonly“grow”and“prune”
movesareallowed,andthisisthefirstti i0 meF(T′ )̸⊇V j, ,k T∗ muj stbeasubtreeofT ,andtisaninternalnodeof
i0 i0 i0 i0
T∗ . UsingLemmaF.6,wehavecoverage(j,t;F(T′ ))∩t−j =∅.
i0 i0
Suppose that Bias2(E′;f∗) = 0, i.e. f∗ ∈ F(E′). Let I ,I ,...,I be the subsets of feature indices split on
1 2 m
in trees T∗,T∗,...,T∗ respectively. We claim that there exists some tree T , i ̸= i such that for all choices of
1 2 m i 0
coordinatesx intheindexsetI ,thereexistsachoiceofcoordinatesz intheindexset{1,2,...,b}\(I ∪{j})
Ii i −Ii∪{j} i
suchthat(x ,z ) ∈ coverage(j,t;F(T )). ThisclaimisprovedasLemmaH.1below. Assumingitfornow,
Ii −Ii∪{j} i
letL ,L ,...,L betheleavesofT∗. ConsideronesuchleafL . SinceL doesnotdependonthefeaturesinI ,
1 2 L i l l i
wemaypick(x ,z )∈coverage(j,t;F(T ))suchthat(x ,z )∈L . ByLemmaF.6,L inT mustcontaina
Ii −Ii i Ii −Ii l l i
descendentthatusesthesplittingrule(j,t). Inparticular,L issplitinT ,andthissplitcanberepresentedbyalocal
l i
decisionstumpψ . Inthismanner,weobtainψ ,ψ ,...,ψ . DenoteU = span(ψ ,ψ ,...,ψ ). Weclaimthat
l 1 2 L 1 2 L−1
U∩V=∅. Toseethis,takeanyf =(cid:80)L−1a 1 ∈Uandassumethatf ̸=0. Firstnotethatf ⊥V byorthogonality
l=1 l Ll i
oflocaldecisionstumpfeaturesfromasingletree. Furthermore,f hasjumplocationswithrespecttofeaturesinI .
i
ButnofunctioninV dependsonfeaturesinI ,whichmeansthatf ∈/ V . Thisgivesf ∈/ V.
−i i −i
46Letψ′denotethelocaldecisionstumpassociatedtotheprunedsplit,andletV′denoteitsorthogonalcomplement
inV. WehaveV′ ⊂F(E′)andhavefurthershownthatU⊂F(E′). Wethereforehave
df(E′)=dim(F(E′))
≥dim(V′)+dim(U)
=df(E )−1+L−1
bad
≥df(E )+ min dim (f )−2.
bad 1 i
1≤i≤m′
Here,thefirstinequalityfollowsfromthetrivialintersectionofV′andU.
UsingProposition4.1andProposition4.2,thereisa1−δ/2eventoverwhich
∆BIC(E′,E)


n
Bias2(E′;f∗)+O(cid:16)(cid:112) nlog(|B|/δ)(cid:17)
ifBias2(E′;f∗)>0
σ2
= (cid:16)(cid:112) (cid:17) (60)
logn(df(E′)−df(E bad))+O log(|B|/δ) otherwise.
Conditionfurtheronthisevent.
Conclusion. ApplyingProposition6.1togetherwithequations(59)and(60),whiletakingnlargeenough,wegeta
1−2δprobabilityeventoverwhich
E(cid:8)
τ
(cid:9) =Ω(cid:16) na/2−1(cid:17)
,
OPTm(f∗,k)
wherek =max dim (f )−min dim (f )−1anda=min dim (f ).
1≤i≤m′ 1 i 1≤i≤m′ 1 i 1≤i≤m′ 1 i
LemmaH.1(Existenceoftreecoveringacylinder). ThereexistssometreeT , i ̸= i suchthatforallchoicesof
i 0
coordinatesx intheindexsetI ,thereexistsachoiceofcoordinatesz intheindexset{1,2,...,b}\(I ∪{j})
Ii i −Ii∪{j} i
suchthat(x ,z )∈coverage(j,t;F(T )).
Ii −Ii∪{j} i
Proof. Let r ,r ,...,r be a permutation of {1,2,...,m}, with r = i (this implies that j ∈ I ). For k =
1 2 m 1 0 r1
2,3,...,m−1,setJ =∪ I ,andsetVk =F(T )+F(T )+···+F(T ).
k i≥k ri rk rk+1 rm
Wemakethefollowingobservation: Forsomek,supposethereexistsacylindersetC ⊂coverage(j,t;Vk)that
k
doesnotdependonanyfeatureinJ . Supposethatthereexistssomex intheprojectionofC tocoordinatesinI
k Irk rk
suchthat(x ,z )∈/ coverage(j,t;F(T ))forallchoicesofz . ThentakingtheintersectionofC and
Ii −Ii∪{j} rk −Ii∪{j} k
{x Irk}×{1,2,...,b}−Irk∪{j} givesacylindersetC
k+1
thatdoesnotdependonanyfeatureinJ
k+1
andsuchthat
C ⊂coverage(j,t;Vk+1).
k+1
Now,sincet−j ∩coverage(j,t;F(T′ ))=∅,wehavet−j ⊂coverage(j,t;V2). Sincet−j isaninternalnodeof
r1
T∗ ,itisacylindersetthatdoesnotdependonanyfeatureinJ . Byapplyingtheaboveobservationinductivelyon
r1 2
k =2,3,...,m−1,weobtainthestatementofthelemma.
I Proof of Proposition 6.1
Proposition6.1willfollowalmostimmediatelyfromthefollowingmoregeneralresult.
PropositionI.1(Generalstatementforrecipe). LetX ,X ,...beanirreducibleandaperiodicdiscretetimeMarkov
0 1
chainonafinitestatespaceΩ,withstationarydistributionπ. Letx∈ΩbeastateandC ⊂Ωbeasubsetsuchthat
theirhittingtimesfromaninitialstatex ∈Ωsatisfy
0
P{τ <τ |X =x }≥c
x C 0 0
forsomeconstantc. LetB ⊂ΩbeasubsetsuchthateverypathfromxtoC intersectsB. ThenthehittingtimeofC
satisfies
cπ(x)
E{τ |X =x }≥ .
C 0 0 π(B)
47Proof. Byconditioningontheevent{τ <τ },wecalculate
x C
E{τ |X =x }≥E{τ 1{τ <τ }|X =x }
C 0 0 C x C 0 0
=E{τ |X =x ,τ <τ }P{τ <τ |X =x }.
C 0 0 x C x C 0 0
Thesecondmultiplicandontherightislowerboundedbycbyassumption,sowejustneedtoboundthefirstone. Using
thestrongMarkovproperty,wefirstlowerboundthisas:
E{τ |X =x ,τ <τ }=E{τ |X =x}+E{τ |τ <τ }
C 0 0 x C C 0 x x C
≥E{τ |X =x}. (61)
C 0
LetZ denotethenumberoftimes(X )returnstoxbeforehittingB. Then,assumingthatX =x,wehavethe
t 0
inequalities
τ ≥τ ≥Z+1.
C B
NotethatZ+1isageometricrandomvariablewithsuccessprobability
p=(cid:8) τ+ >τ |X =x(cid:9) ,
x B 0
whereτ+isthefirstreturntimetox,i.e.
x
min{t>0: X =x}.
t
Wethereforecontinue(61)toget
1
E{τ |X =x}≥ . (62)
C 0 P(cid:8) τ+ >τ |X =x(cid:9)
x B 0
WenextwritethisprobabilityintermsofanotherrandomvariableW,whichwedefinetobethenumberofvisitsto
statesinBbeforereturningtox,whenthechainisstartedatX =x. Wethenhave
0
P(cid:8) τ+ >τ |X =x(cid:9) =P{W ≥1}≤E{W}. (63)
x B 0
Toboundthisexpectation,foreachy ∈B,letW denotethenumberofvisitstoybeforereturningtox,andobserve
y
(cid:80)
that W = W . Let π denote the unique stationary distribution of (X ). Using Lemma I.2, we then have
y∈B y t
E{W }≤π(y)/π(x). Addinguptheseinequalities,weget
y
(cid:88) π(y) π(B)
E{W}= E{W }≤ = . (64)
y π(x) π(x)
y∈B
Combiningequations(62),(63),and(64)completestheproof.
ProofofProposition6.1. ItisclearthattheMarkovchaininducedbyarunoftheBARTsamplerisirreducibleand
aperiodic,withstationarydistributiongivenbythemarginalposteriorp(E|y). WehenceusePropositionI.1toget
(cid:18) (cid:19)
E(cid:8)
τ
(cid:9)
=Ω
p(E bad|y)
.
OPTm(f∗,k)
p(B|y)
Wenowcompute
p(E |y) 1 p(E |y)
bad ≥ min bad
p(B|y) |B| E∈B p(E|y)
(cid:18) (cid:19)
1
= exp min{logp(E |y)−logp(E|y)}
|B| E∈B bad
1 (cid:18) 1 (cid:16)(cid:112) (cid:17)(cid:19)
≥ exp min∆BIC(E,E )−O log(|B|/δ)
|B| 2 E∈B bad
(cid:18) (cid:18) (cid:19)(cid:19)
1
=Ω exp min∆BIC(E,E ) ,
2 E∈B bad
wherethesecondinequalityfollowsfromProposition4.2andholdswithprobabilityatleast1−δ.
48LemmaI.2(Boundingnumberofvisits). LetX ,X ,...beanirreducibleandaperiodicdiscretetimeMarkovchain
0 1
onafinitestatespaceΩ,withstationarydistributionπ. Foranytwostatesx,y ∈Ω,wehave
π(y)
E{numberofvisitstoybeforereturningtox|X =x}= . (65)
0 π(x)
Proof. Fixxanddenotethequantityontheleftsideofequation(65)byπ˜(y). Wemaythenrewriteequations(1.25)
and(1.26)ofLevinetal.(2006)inournotationasfollows:
π˜(y)
π(y)= ,
E(cid:8) τ+|X =x(cid:9)
x 0
1
π(x)= ,
E(cid:8) τ+|X =x(cid:9)
x 0
whereτ+isthefirstreturntimetox. Takingtheratioofthetwoequationscompletestheproof.
x
J Invariance of PEM Dimension to Change of Measure
LemmaJ.1(Characterizationofsubspacedimension). Letv ,v ,...,v bevectorsinaninnerproductspace. LetG
1 2 n
betheGrammatrixofthesevectors,inotherwords,its(i,j)entrysatisfiesG =⟨v ,v ⟩for1≤i,j ≤n. Thenthe
ij i j
dimensionofthesubspacespannedbyv ,v ,...,v isequaltothenumberofnonzeroeigenvaluesofG.
1 2 n
Proof. Byrestrictingtoalinearlyindependentset,itsufficestoshowthatv ,v ,...,v arelinearlyindependentifand
1 2 n
onlyifGisinvertible. Fortheforwarddirection,supposeGisnotinvertible,thenthereexistsavectorofcoefficients
α=(α ,...,α )suchthatαTGα=0. Butinthatcase,wehave
1 n
(cid:42) n n (cid:43)
(cid:88) (cid:88)
0=αTGα= α v , α v .
i i i i
i=1 i=1
Bydefinitionoftheinnerproduct,thismeansthat(cid:80)n
α v = 0,contradictinglinearindependence. Thereverse
i=1 i i
directionissimilar.
LemmaJ.2(Invarianceofdimensiontocovariatedistribution). Letν andν′betwomeasuresonacompactcovariate
spaceX thatareabsolutelycontinuouswithrespecttoeachother. Letv ,v ,...,v ∈L2(X,ν). Thenv ,v ,...,v
1 2 n 1 2 n
spanthesamesubspaceinbothL2(X,ν)andL2(X,ν′).
Proof. Byrestrictingtoalinearlyindependentset,itsufficestoshowthatv ,v ,...,v arelinearlyindependentin
1 2 n
L2(X,ν)ifandonlyiftheyarelinearlyindependentinL2(X,ν′). Bydefinitionofabsolutecontinuity,thereexistsa
constantc>0suchthat
(cid:90) (cid:90) (cid:90)
c−1 f(x)dν(x)≤ f(x)dν′(x)≤c f(x)dν(x)
X X X
foranyf ∈LetGandG′betheirGrammatricesinL2(X,ν)andL2(X,ν′)respectively. Letα=(α ,...,α )be
1 n
anyvectorofcoefficients. Thenwehave
(cid:90) (cid:32) n (cid:33)2
(cid:88)
αTGα= α v (x) dν(x)
i i
X i=1
(cid:90) (cid:32) n (cid:33)2
(cid:88)
≤c α v (x) dν′(x)
i i
X i=1
=cαTG′α.
Similarly,wegetαTG′α≤cαTGα.
Thisshowsthatwheneverthecovariatedistributionν hasfullsupport,itisequivalenttotheuniformmeasure.
49K Real Data Simulations
WefurtherinvestigatedtheeffectoftrainingsamplesizeonthemixingperformanceofBARTaswefitittoanumber
ofrealworldregressiondatasets. Westudiedseveraldatasetsusedintherandomforestpaperalongwiththreeofthe
largestnon-redundantdatasetsfromthePMLBbenchmark. DetailsonthedatasetsareprovidedbelowinTable2.
Name Samples Features
Breasttumor(Romanoetal.,2021) 116640 9
Californiahousing(PaceandBarry,1997) 20640 8
Echomonths(Romanoetal.,2021) 17496 9
Satelliteimage(Romanoetal.,2021) 6435 36
Abalone(Nashetal.,1994) 4177 8
Diabetes(Efronetal.,2004) 442 10
Table2: Realworlddatasetsstudied.
Furthermore,aheuristicestimateofthesignal-to-noiseratiopresentineachdatasetcanbeobtainedbyinspecting
thesimulationresultsinTanetal.(2022b). CaliforniahousingandSatelliteimagehaverelativelyhighSNR(theR2of
randomforestislargerthan0.8),whereasallotherdatasetshaverelativelylowSNR(theR2ofrandomforestislower
than0.6.)
Foreachdatasetwesetaside15%oftheoveralldatasetasatestset. Theremainingdataisusedasthetrainingdata
setandissubsampledtocreateavarietyofsamplesizes. Foreachsubsampleproportion,wesamplearandomsetof
thetrainingdataandfittheBARTalgorithmonthissubsetofthetrainingdatawith1,2,and5chains. Thisisrepeated
for100MonteCarloiterationsforeachsamplesizeandtheRMSEonthetestsetisevaluatedeachtime. Theremainder
ofthesimulationset-upisthesameasthatofExperiment3,asdescribedinSection8.
Results. TheresultsaredisplayedinFigure8. NotethatinsteadofplottingRMSE,wehavechosentoplotrelative
RMSE,whichmeasurestheratiobetweentheRMSEobtainedfrommultiplechainsandthatobtainedforasingle
chainforagivendatasetting. Sinceweareworkingwithrealdatasets,errorismeasuredwithrespecttotheobserved
responses,ratherthanatrueregressionfunction. Inalmostalloftherealdatasets,weseethatincreasingthenumberof
chainsintheBARTalgorithmconsistentlydecreasestheRMSEofthepredictions,andtheperformancegapgrows
withthetrainingsamplesize. ThisprovidesfurtherevidencethatthepoormixingperformanceofBARTappliesto
realworlddatasets. Ontheotherhand,theeffectseemstobemuchlesssignificantcomparedtothatforthesimulated
datasetsstudiedinExperiment3. ThisisunsurprisingastheRMSEforrealdatasetsincorporatesandhenceisinflated
byaleatoricuncertaintyintheresponses. Notably,inthedatasetswithhigherSNR(CaliforniahousingandSatellite
image),theperformancegapremainssignificant.
50Diabetes CA Housing Satellite
100% 100% 100%
Chains
98% 98% 98% 1
2
96% 96% 96% 5
94% 94% 94%
25% 50% 75% 100% 25% 50% 75% 100% 25% 50% 75% 100%
n n n
Echo Months Breast Tumor Abalone
100% 100% 100%
Chains
98% 98% 98% 1
2
96% 96% 96% 5
94% 94% 94%
25% 50% 75% 100% 10% 20% 30% 40% 50% 25% 50% 75% 100%
n n n
Figure8: RMSEforthepredictionsfromBARTrunwithseveraldifferentnumbersofchainsrelativetotheRMSE
obtainedfromrunningBARTwithasinglechain. TheXaxisdisplaysthepercentageofthetrainingdatathatissampled
foreachMonteCarloreplication. TheRMSEiscalculatedoveranindependenttestsetconsistingof15%oftheoverall
data.
L Additional results for Experiment 3
Inordertofurtherexploretheeffectofthenumberofchainsonmixingperformance,wealsorepeatedExperiment3
using20chains. TheresultsareshowninFigure9
51
ESMR
evitaleR
ESMR
evitaleRSum High Low
100% 100% 100%
Chains
90% 90% 90% 1
2
80% 80% 80%
5
70% 70% 70% 10
20
60% 60% 60%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Piecewise Linear Tree Local Sparse Spiky
100% 100% 100%
Chains
90% 90% 90% 1
2
80% 80% 80%
5
70% 70% 70% 10
20
60% 60% 60%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Sum High Low
100% 100% 100%
Chains
97.5% 97.5%
1
90%
95% 2
95%
5
92.5%
80%
92.5% 10
90%
20
90%
70%
87.5%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Piecewise Linear Tree Local Sparse Spiky
98%
Chains
75% 95% 96% 1
2
50% 92.5% 94% 5
10
25% 90% 92%
20
87.5% 90%
0K 25K 50K 75K 100K 0K 25K 50K 75K 100K 0K 25K 50K 75K 100K
n n n
Figure9: TheRMSE(toppanel)andempiricalcoverage(bottompanel)ofBARTimproveifweaverageposterior
samplesfrommultiplesamplerchains,givenafixedtotalbudgetofposteriorsamples. Therelativeperformancegaps
increaseswiththenumberoftrainingsamples,providingevidencethatthetendencyofHPDRhittingtimetogrow
withtrainingsamplesizeisconsistentacrossawiderangeofDGPs. Thebiggestimprovementseemstooccurwhen
increasingthenumberofchainsfrom1to2andthereseemstobediminishingreturnsthereafter. BothRMSEand
coveragearecalculatedonanindependenttestsetandareaveragedover100experimentalreplicates,witherrorbars
representing±1.96SE.
52
ESMR
evitaleR
ESMR
evitaleR
egarevoC
laciripmE
egarevoC
laciripmE