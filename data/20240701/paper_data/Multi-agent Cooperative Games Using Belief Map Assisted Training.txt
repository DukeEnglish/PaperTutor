Multi-agent Cooperative Games Using Belief Map Assisted
Training
ChenLuoa,†,QinweiHuanga,†,AlexB.Wub,SimonKhanc,HaiLid andQinruQiua;*
aDepartmentofElectricalEngineering&ComputerScience,SyracuseUniversity
bFayetteville-ManliusHighSchool
cAirForceResearchLaboratory
dDepartmentofElectricalEngineering&ComputerScience,DukeUniversity
†EqualContribution
Abstract. Inamulti-agentsystem,agentssharetheirlocalobser- incomplexity[8][14].Itwillalsoincreasethevulnerabilityofthe
vationstogainglobalsituationalawarenessfordecisionmakingand systemtosingle-pointfailures[15][17].Toovercometheseissues,
collaborationusingamessagepassingsystem.Whentosendames- distributedcontrolandoptimizationareintroduced,whereeachagent
sage,howtoencodeamessage,andhowtoleveragethereceived makesitsowndecisionsbasedonlocalinformation.However,thisap-
messagesdirectlyaffecttheeffectivenessofthecollaborationamong proachalsohaslimitation,asagentsonlyhavepartialobservationsof
agents. When training a multi-agent cooperative game using rein- theirimmediatesurroundings,andmaynotbeabletomakeglobally
forcementlearning(RL),themessagepassingsystemneedstobe optimaldecisions.
optimized together with the agent policies. This consequently in- Messageexchangesamongagentscanprovideglobalinformation
creasesthemodel’scomplexityandposessignificantchallengesto andhelptheagentsmoveoutoflocaloptima.However,excessive
theconvergenceandperformanceoflearning.Toaddressthisissue, communicationcanconsumecommunicationenergy,bandwidth,and
weproposetheBelief-mapAssistedMulti-agentSystem(BAMS), processingpower.Sendingredundantmessagesinconsecutivecycles,
whichleveragesaneuro-symbolicbeliefmaptoenhancetraining.The orbyagentsclosetoeachother,islikelytowasteresources.Addition-
beliefmapdecodestheagent’shiddenstatetoprovideasymbolic ally,frequentlycommunicatingeverypieceofobservedinformation
representationoftheagent’sunderstandingoftheenvironmentand canbewastefulandalsounderminethereceiver’sdecision-making
otheragents’status.Thesimplicityofsymbolicrepresentationallows ability.Furthermore,tosavecommunicationenergyandimprovese-
thegatheringandcomparisonofthegroundtruthinformationwith curity,thehigh-dimensionalobservationshouldbeencodedintoa
thebelief,whichprovidesanadditionalchanneloffeedbackforthe low-dimensionalmessagethatcanonlybedecodedbytheagents.
learning.Comparedtothesporadicanddelayedfeedbackcoming Therefore,whentocommunicate,whattocommunicateandhowto
fromtherewardinRL,thefeedbackfromthebeliefmapismorecon- encode/decodethemessagearevariablesthatneedtobeoptimized.
sistentandreliable.AgentsusingBAMScanlearnamoreeffective Reinforcementlearning(RL),suchastheactor-criticmodel,iscom-
messagepassingnetworktobetterunderstandeachother,resultingin monlyusedtooptimizemulti-agentgames.Manuallydesignmessage
betterperformanceinthegame.WeevaluateBAMS’sperformance passingsystemusuallydoesnotworkwellwiththeRLduetothe
inacooperativepredatorandpreygamewithvaryinglevelsofmap lackofpriorknowledgeofthefeaturesthatareneededbythepolicy
complexityandcompareittopreviousmulti-agentmessagepassing network.Atypicalapproach[3][23]istotrainthemessagepassing
models.ThesimulationresultsshowedthatBAMSreducedtraining network together with the policy network so that they can evolve
epochsby66%,andagentswhoapplytheBAMSmodelcompleted simultaneously.
thegamewith34.62%fewerstepsonaverage. Trainingadeepneuralnetworkusingreinforcementlearningis
timeconsumingbecausetheonlyfeedbackforthetrainingisdelayed,
sparse,andindirectintheformofrewards.Trainingamulti-agent
1 Introduction
reinforcementlearning(MARL)model[1][25]isevenmorechalleng-
ingduetothefactthatagents’decisionsarenotvisibletooneanother.
Amulti-agentcooperativegameinvolvesmultipleautonomoussys-
Thislackofvisibilityreducesthepredictabilityoftheenvironment
temscollaboratingwitheachothertoachieveacommongoaland
andmakesitnon-stationary.Whenatrainablemessagepassingnet-
maximizetheoverallutilityofthesystem.Thesegamescanbeused
workisusedtoconnectagents,thingsbecomeevenmorecomplicated.
tomodelvariousapplications,suchasrescuemissionswheremultiple
The additional trainable variables in the message network signifi-
robotsaredeployedtosearchformissingpersons,militaryopera-
cantlyincreasethemodel’scomplexity,prolongthetrainingtime,and
tionswheremultipleUAVssurveyalargearea,andscientificexpe-
escalatethechanceofoverfitting.
ditionswhereroversexploreunknownterraintogether.However,as
Inthiswork,weacceleratetheMARLbyintroducinganotherfeed-
thenumberofagentsincreases,centralizedmonitoring,controlling,
backchannelthathelpstolearnamoreefficientmessagepassing
andoptimizationbecomesinfeasibleduetotheexponentialgrowth
networkandamoreeffectiverepresentationoftheenvironment.This
∗CorrespondingAuthor.Email:qiqiu@syr.edu. consequentlyleadstobetterpolicyandfasterconvergence.Inour
4202
nuJ
72
]AM.sc[
1v77491.6042:viXrabelief-mapassistedmulti-agentsystem(BAMS)1,eachagentissup- AccordingtotheDEC-POMDPmodel,eachagentitakesanaction
plemented with a map decoder, which transforms its hidden state a i basedonitslocalobservationo i.Whenallagentsappliedtheir
into a belief map, a neuro-symbolic representation of the agent’s actions[a 0,a 1,...,a N]totheenvironment,theenvironmentmoves
knowledgeoftheglobalenvironment.Thissymbolicrepresentation to a new state s′ and returns a joint reward r. The MARL trains
issimple,makingiteasytoobtainitscorrespondingground-truth policies π i(a i|o i) : O i → A i, ∀i, that maximizes the expected
value.Bycomparingthebeliefmapwiththeground-truthmap,the
discountedrewardE[(cid:80)∞ γtrt],whereγisthediscountfactor.
t=0
systemreceivesanadditionalfeedbackthatsupervisesthetraining Sharingobservationimprovestheperformanceandhelpsagents
process.Duringexecution,thebeliefmapprovidesawaytointerpret learnabetteractionpolicy.Efficientcommunicationallowsagentsto
the agent’s hidden state, which can further be used to explain the obtainmoreinformationabouttheglobalenvironmentandreduces
agent’sbehavior. thenegativeimpactofpartialobservations.Previousresearchmodels
Toimprovecoordinationamongtheagentsandincreasetheeffi- a multi-agent communication system as a message passing graph
ciencyofmessageretrieval,ourmessagepassingsystemincorporates neuralnetwork[11][13],whereeachnodeinthegraphrepresentsan
gatingandattentionmechanisms.Theattentionmodelenablesagents agentandeachedgemodelsacommunicationpathwayequippedwith
todifferentiateimportantandirrelevantmessages,whilethegating messageencodinganddecoding.Differentgraphtopologieshavebeen
removestheredundancyandsavescommunicationpowerandband- studied[21],andrecentworksfocusonimprovingtheefficiencyand
width. reducingthecostofthecommunicationusinggatedmessagepassing
WeassessedtheperformanceoftheBAMSmodelusingamulti- [16],attention[5],schedulecommunication[10]andevent/memory
agentpredator-preygamewithandwithoutobstacles.Centralized drivenprocessing[7][19][22].
traininganddistributedexecutionareadoptedintheexperiments.The Thefirststudyonlearnablecommunication,knownasRIALand
experimentalresultsindicatethatBAMSoutperformsexistingmodels, DIAL[4],developedamessagepassingnetworkthatgeneratesmes-
provingtobemorefittingforlarge-scaleenvironmentswithcomplex sagegenerationbasedontheagent’slocalobservation,action,and
landscapesandprovidingmorerobustperformance. receivedmessages.Themessageencoderisamulti-layerperceptron
Thekeycontributionsofthispaperaresummarizedasfollows: trainedtogetherwiththepolicynetworkusingreinforcementlearning.
CommNet[24]includesacentralizedcommunicationchannelinto
• Weproposedabelief-mapassistedtrainingmechanismthatcom- thenetwork,whichenhances[4]bymaintainingalocalhiddenstate
plementsreinforcementlearningwithsupervisedinformationto ineachagentusingarecurrentneuralnetwork(RNN).Thehidden
acceleratetrainingconvergence. stateisdeterminedbythesequenceoflocalobservationsandreceived
• Weproposedabelief-mapdecodertoreconstructaneuro-symbolic messagesandissentasthecommunicationmessagetootheragents.
mapfromtheenvironmentembeddingtoprovideadditionalfeed- Whenmultiplemessagesarereceived,theagentconsolidatesthem
backduringthetraining.Themaptransformsthehiddenstateof usingtheiraverage.
agentsintoahuman-readableformat,whichsignificantlyimproves Messagegating[9][23]hasbeenproposedasabinaryactionto
theinterpretabilityoftheagent’sdecision-makingprocess. dynamicallyblockorunblockmessagetransmission,therebyimprov-
• AgentstrainedusingBAMSmodelcommunicatemoreeffectively, ingcommunicationefficiencyandconservingpowerandbandwidth.
catchingthepreyfasterandbeinglesssusceptibletonoisesfrom IC3Net[23],anextensionofCommNet[24],utilizeslongshort-term
redundantmessagesasthenumberofagentsincreases. memory(LSTM)[6]togeneratehiddenstates.Gated-ACML[16]
• Simulationresultsshowthatagentswiththeseenhancementscanbe performsmessagepruningbeforetransmission.Forbothapproaches,
trainedeffectivelyforoperationinlargeandcomplexenvironments, communicationgatingisoptimizedbythepolicynetworkusingrein-
reducingtrainingtimebyanaverageof66%andimprovingoverall forcementlearning.
performanceby34.62%. Otherstudies[9][12][18]haveemployedattentionmodeltopriori-
tizereceivedmessagessothatagentscanselectusefulfeatures.ATOC
Therestofthepaperisorganizedasfollows.Section2introduces [9]appliesattentiontodeterminewhichagenttocommunicatewith,
previousworksrelatedtocommunicationinamulti-agentreinforce- anddynamicallychangesnetworkstructureaccordinglybygenerated
mentlearningsystem.Section3givesthedetailsofourproposed adirectedgraph.G2ANet[12]combinesahardattentionandasoft
methodincludingthebelievemapdecoderandattentionmodel.The attentionastwostageattentionmodeltoprocessdifferentincoming
experimentalresultsaregiveninSection4followedbytheconclu- messagefromdifferentagents.MAGIC[18]usesamulti-layoutgraph
sionsinSection5. attentionnetworkamongagents.However,itperformscentralized
communicationandmessageprocessing.Allmessagesaresenttoa
communicationhubwheretheyareconsolidatedusingtheattention
2 MotivationsandPreviousWork
modelandthenbroadcastedtoallagents.
TarMAC[3]utilizesbothgatingandattentiontoenhancethecom-
Weconsiderafullycooperativemulti-agentgameasadecentralized
municationefficiency.However,uponcarefulexaminationofitscode,
partiallyobservableMarkovDecisionProcess(DEC-POMDP)[2].
wefoundthatanimplementationerrorintheSoftMaxfunctionleads
DEC-POMDPisdefinedasatuple⟨N,S,P,R,O,A,Z,γ⟩,where
tounintendedmessageleakage.Ifallagentsgatetheirtransmissions,
Ndenotesthenumberofagents;Sisafinitestatespace;P(s′|s,a):
thereceivermaystillreceivethismessage.Inotherwords,anagent
S ×A×S → [0,1] stands for the state transition probabilities;
icanonlygateitsmessagetoanotheragentj ifatleastoneother
A=[A ...A ]isafinitesetofactions,whereA representsthe
1 N i agentk,1≤ k≤ N,k̸= iorj,decidedtosenditsmessagetojin
set of local actions a that agent i can take; O = [O ...O ] is
i 1 N thesamecycle.Asaresult,agentsmustsynchronizewitheachother
a finite set of observations controlled by the observation function
regardinggatingdecisionsduringeachcycletodeterminewhetherto
Z : S×A → O;R : S×A → Ristherewardfunction;and
transmitmessages.
γ ∈[0,1]isthediscountfactor.
Alltheworksmentionedabovetrainthemessagepassingnetwork
1ThecodeisavailableatGithub with the policy network using the game rewards as the feedback.Figure1:ArchitectureofBAMSmodel
Thisapproachtendstohaveaslowconvergenceandtheagentsdo tailoredtothespecificapplication.Theagentthenupdateitshidden
notunderstandeachotherwell.Inthiswork,weproposedabelief- state,whichismaintainedbyanLSTM,usingbothlocalobservations
mapassistedtrainingmethod(BAMS)thatsignificantlyimproves andthereceivedmessagesasthefollowing:
the training speed and quality for large and complex games. The
agentstrainedusingBAMScommunicatesmoreefficientlywithfewer ht i+1,st i+1 =lstm i(E i(ot i),ct i,ht i,st i), (1)
messagesandbetterattentions.
whereht andst arehiddenstateandcellstateattimetofagenti,
i i
andctistheaggregatedfeatureextractedfromthereceivedmessages
3 ProposedMethod usingi theattentionmodel.E (ot)istheencodedobservation.
i i
Inthissection,wepresentthestructureandtrainingofbelief-map Basedonthehiddenstate,theagentchoosesactionsusingapolicy
assistedmulti-agentsystem(BAMS).DetailsoftheBAMSareillus-
networkp i().Thepolicynetworkfollowstheactor-criticmodeland
tratedinFigure1.Foreachagenti,themodelcomprisesfivemajor comprisesanactornetworkθ i(ht i)andacriticnetworkV i(ht i).The
components: θ i(ht i)isaone-layerfullyconnectednetworkwithaninputofht i.Its
outputhastwocomponentsatandgt,
i i
• ObservationEncoderE ():Theobservationencoderextractskey
i at,gt =θ (ht). (2)
features from the agent’s local observation, which will later be i i i i
combinedwithreceivedmessagesandbeusedtoupdatethehidden
Thevectoratrepresentstheprobabilitiesofthegameactionsavailable
states. i
totheagent,i.e.,themovementthattheagentcanmaketocomplete
• MessageAttentionModuleA ():Theattentionmoduleassigns
i thegame.Thevariablegt,whichiseither1or0,representstheprob-
weightstodifferentmessagestoselectrelevantinformation. i
abilityofthebinarycommunicationaction,i.e.,blockingorpassing.
• HiddenStateGeneratorlstm ():Thehiddenstategeneratorisa
i
At each step, the action was sampled according to the probability
LongShort-TermMemory(LSTM)thatfusesthelocalobservation
distribution.
andreceivedmessagesintoafeaturevectorh .
i
• PolicyNetworkp ():Thepolicynetworkisanactor-criticmodel
i
that selects the best action for the local agent to maximize the 3.2 MessagePassingModel
overallsystemutility.InBAMS,theactionconsistsoftwoparts,a
discretemovementactiona i,whichdecideshowagentmovesto Agentscommunicatetheirconnectedneighborsbysendingmessages.
completethegame;andabinarycommunicationactiong i,which FollowingtheapproachusedinTarMACandIC3Net,weemploy
decideswhethertheagentshouldbroadcastitshiddenstate.The the hidden state as the communication message. The hidden state
outgoingmessagemm i istheproductofg i andh i asshownin contains all the information that an agent requires to make local
Figure1. decisions.However,notalltheinformationisusefultotheagent’s
• MapDecoderD i():Thedecoderreconstructsaneuro-symbolic neighbors.Furthermore,someoftheinformationmayoverlapwith
beliefmapoftheenvironmentbasedonthehiddenstateofthe previousmessagesfromthesameagentormessagessentbyanearby
localagent.Thebeliefmaprepresentsagent’sknowledgeofthe agent.Toimprovetheefficiencyofthecommunicationnetwork,the
globalenvironment.Itwillbecomparedwiththegroundtruthto sendersmustreducethenumberofredundantmessagestheysendand
provideadditionalfeedbacktoassistthetraining. thereceiversmustbeabletoextractusefulinformationrelevantto
theirowndecisionmaking.
Weimplementthemessagegatingatthesenderside.Theoutgoing
3.1 HiddenStateGenerationandPolicyNetwork
messagemmt ofagentjiscalculatedastheproductofht andthe
j j
Ateachtimestep,everyBAMSagentcollectsobservationsfromits binarygateactiongt.
j
localsensor.Thelocalobservationforagentiattimetisdenoted
asot i.Typically,therepresentationofot i isdesignedmanuallyand mmt j =ht j×g jt. (3)After receiving messages mmt(j ̸= i) from neighbor j, agent i beliefmap(bt)andthegroundtruthmapGt.MeanSquaredError
j i i
aggregatesthemessagesusinganattentionmodel,whichistrained (MSE)isusedtocalculatetheloss,L = (cid:80) MSE(Gt−bt).
map t i i
tomaximizetherewardfromthegameandminimizethelossofthe Duringtraining,thecentralcontrollertracksthemovementandstatus
belief-mapconstruction.Consideringthecommunicationdelay,agent ofallagentstogenerategroundtruthmap.Themaplossisobtained
iusesgatedmessagemmt−1sendbyagentjinprevioustimestep ineverytimestept.Minimizingthemaplosscanhelpallagentscon-
j
astheinputofthekeyandvaluenetworkstogeneratekt andvtfor vergetoaneffectivecommunicationprotocolandefficientmessage
j j
timet.Thequeryqtoftheattentionmodelisgeneratedbasedonthe processing.
i
agent’slocalhiddenstateatcurrenttimestep(ht). TheRLlossistheerrorofthecriticnetwork,
i
k jt =key(mmt j−1) (4) L RL =(cid:88) t∥(r(ht i,at i)+γVˆ(ht i+1)−Vˆ(ht i)∥2 (9)
v jt =value(mmt j−1) (5) wherer(ht i,at i)istherewardoftheentiresystem,andVˆ()isthe
qt =query(ht) (6) valueestimationofthecriticmodel.Theactornetworkisupdated
i i usingpolicygradient:
(cid:34) (qtT kt) (qtT kt) (qtT kt)(cid:35)
α it =softmax (cid:112)i
(d
k1
)
... (cid:112)i
(d
kj )... (cid:112)i
(d
k1
)
(7) ∇ θJ(θ)=(cid:88) t∇ θlog(p θ(at i|ht i)[r(ht i,at i)+γVˆ(ht i+1)−Vˆ(ht i)]
(10)
ct =(cid:88)N αtvt (8) wherep θ()isthepredictionoftheactornetwork.TheBAMSmodel
i i j
j=1 isupdatedusingtheaveragegradientofallagents.
wherekey(),value()andquery()arenetworkswithonefullycon-
nectedlinearlayer,d isthedimensionsofhiddenstate.ct isthe
k i 4 Experiments
aggregatedfeaturevectorthatwillbeusedtoupdatethehiddenstate
inEquation(1). Forourexperimentsandevaluations,weutilizedaclassicgrid-based
predator-preygame[13].ItinvolvesNpredators(agents)withlimited
vision(v)toexploreanenvironmentofsizem×mtocaptureeithera
3.3 MapDecoder
staticpreyoramovingprey.ThevalueofN rangesfrom3to10,and
Insteadofrelyingsolelyontherewardfromtheenvironment,addi- mrangesfrom7to20,representinggameswithvaryingcomplexity.
tionalchannelsoffeedbackinformationcouldbeaddedtoexpedite Theenvironmentisfurtherdividedinto2categories,withobstacles
thetrainingprocess.InthisworkweassisttheRLbyusingadecoded andwithoutobstacles.
beliefmap.Astheaggregationofpastobservationsandincoming
messages,anagent’shiddenstaterepresentsitsknowledgeoftheenvi-
4.1 ExperimentSetting
ronment.Themoreaccuratethisknowledgeis,thebetterdecisionan
agentcanmake.However,anagent’shiddenstateisafeaturevector WetrainedournetworkusingRMSprop[20]withalearningrateof
thatisnotinterpretable.ThebasicideaofBAMSistodecodethe 0.001andsmoothingconstant0.97.Theentropyregularizationisused
hiddenstateintoaneuro-symbolicmapthatishumaninterpretable, withcoefficient0.01.ThehiddenstatesizeforLSTMis64.Forthe
allowingfortheconstructionofthegroundtruthversionofthemap. attentionmodel,thekey(kt)andquery(qt)haveadimensionof16
j j
Bycomparingthedecodedmapwithgroundtruthmap,weprovide andthevalue(vt)hasadimensionof64.
j
additionalfeedbacktoassistthetrainingoftheentiresystem. Theagentshavelimitedobservationcapabilities.Specifically,each
ThemapdecoderD (ht)canbeviewedastheinverseprocessof agentisonlyabletoobserveobjectswithina3×3or5×5area
i i
theobservationencoderE (ot).TheencoderE (ot)usesaConvolu- centeredarounditself.Ateachtimestep,anagentcanchoosefrom5
i i i i
tionalNeuralNetwork(CNN)toextracttheinformation.Therefore, possibleactions:up,down,left,right,andstay.Additionally,allagents
weselectedtransposedCNNtodecodethemap.Boththeobservations (predators)haveamaximumsteplimitation,whichvariesaccording
anddecodedmapsarem×mgriddedplanes,wheremisthesizeof tothesizeoftheenvironment.Priortoreachingtheprey,anagent
theenvironment.ThestatusofeachgridlocationiscodedasasizeM willreceiveapenaltyr =-0.05duringeachtimestep.Once
searching
vector,whereM representsthenumberofpossiblestatesofthegrid. anagentreachestheprey,itwillremainthereandreceivesnofurther
Forexample,inthepredator-preygame,agridcanhave4possible penalty.Thegameisconsideredascompletewhenallagentsreach
statesthatindicatewhetherithasbeenobserved,iscurrentlyoccupied thepreywithinthemaximumnumberofsteps.Thenumberofsteps
byapredator,occupiedbyaprey,oroccupiedbyanobstacle.These takentocompletethegameservesastheperformancemetric.
4statesarenotnecessarilyexclusive;henceeachgridisencodedasa WeconductedacomparisonofBAMSwith2baselinemodels:Tar-
multi-hotvectorwithasizeofM.Overall,bothmapshavedimension MACandIC3Net.I3CNetemploysmessagegatingwhileTarMAC
M×m×m.Theobservationmaponlycontainsinformationfromthe employsbothmessagegatingandattention.Tothebestofourknowl-
localagent,whilethebeliefmapshouldincorporatetheinformation edge,thesemodelshavestate-of-the-artperformancewhileemploying
fromallagents. decentralizedcommunicationanddecision-making.ForTarMAC,re-
gardlessofthegatingaction,themessagewillbesent,togetherwith
thegatingaction.SowealsoimplementedavariationofBAMSthat
3.4 LossFunction
removesthebeliefmapdecoderandconductsthetrainingwithoutthe
Inthiswork,weapplycentralizedtraininganddistributedexecution. useofadditionalfeedback.ThisreducedversionofBAMSisreferred
AllcomponentsintheBAMSaretrainedtogether. toasBAMS-R.
Thetraininglossforeachagentcomprisestwocomponents,the Inadditiontotheaforementionedmodels,weimplementedaheuris-
L andtheL ,Loss=αL +βL ,whereαandβaretwo ticrule-basedalgorithm.Thealgorithmdirectstheagentstoexplore
map RL map RL
hyperparameters.Themaplosscomesfromcomparingthedecoded themapfromlefttoright,andtoptobottom.afterfinishesexploringTable1:AvgSteps&CommRateforSimpleEnvironments
N=3,m=7,MaxSteps=20 N=5m=12,MaxSteps=40 N=10,m=20,MaxSteps=80
Avgsteps Commrate Avgsteps Commrate Avgsteps Commrate
Heuristic 14.56 - 33.24 - 68.90 -
IC3Net 12.48 0.60 32.90 0.39 73.82 0.60
TarMAC 8.79 0.99 22.59 0.91 60.72 0.76
BAMS-R 12.39 0.32 29.80 0.04 71.76 0.35
BAMS 8.17 21.64 56.46
Figure2:Leftfigureshowstheexampleof4trajectoriesexhibiting
keeptheborderwithintheirobservationrange.Rightfigureshown
theheuristictrajectory.
arow/column,anagentwillmovetothenextrow/columnbeyondits
Figure3:Theconvergencecurveoffourmethods’averagesteptaken
previousobservationrange.Whenitreachesthemap’sedge,itwill
inrandomseedsundersimple12×12environment.
turnaroundandexploreintheoppositedirection.Onceanagenthas
sightedtheprey,itwilltransmittheprey’slocationtoallotheragents,
whowillthentaketheshortestpathtocapturetheprey.Anexample
oftheheuristictrajectoryisshownastherightfigureofFigure2.
ofstepsrequiredtocompletethegame.Asthemapsizeincreases,the
4.2 ExperimentalResultsforSimpleEnvironment communicationrateoftheBAMSagentsreducesasthepossibilities
ofencounteringnewevents,suchasobservinganotheragent,themap
Thefirstexperimentiscarriedoutonsimpleenvironmentwithoutany edge,ortheprey,decreases.Inotherwords,theagentsspendmostof
obstacles.Wediscoveredthatagentsdevelopedsignificantlevelsof thetimemovingstraightahead.
intelligenceandmutualunderstanding,allowingthemtocompletethe Figure3comparestheconvergencespeedofBAMSwithBAMS-R,
gamewithminimumcommunications.Forexample,allagentslearned TarmacandIC3Net.Theresultsindicatethat,IC3Nethasthefastest
toexplorethemapbymovinginacounterclockwisecircle.Insteadof convergenceduetoitsrelativelysimplerarchitecturethatdoesnot
exploringtheentiremap,agentscirclealocalregionbasedontheir employ an attention mechanism in message processing. However,
initialposition.Additionally,theagentstendtokeeptheborderwithin forthesamereason,italsohastheworstperformance.Onaverage,
theirobservationrangewhilealsostayingasfarfromitaspossible. BAMSimprovestheconvergenceby66%comparedBAMS-R.This
Thesebehaviorsallowtheagentstoobservethemaximumareawhile improvementcanbeattributedtotheadditionalfeedbackfromthe
travelingtheminimumdistance.Figure2presentsanexampleof4 belief map, which provides a more consistent relationship among
trajectoriesexhibitingsuchbehavior. hidden state, action, and reward, resulting in faster learning with
Table1comparestheBAMSwithfourreferencealgorithmsfor feweriterations.EvenTarMACsendsoutmessageseverytimestep,
gameswithdifferentsizeswhenagentshave3×3vision.Thecolumn ourBAMSstillbeattheconvergenceofTarmac.Itshouldbenoted
“commrate”showstheaveragepercentageoftimesanagenttransmits thatthisfeedbackisonlyavailableduringthetrainingasnoground
itshiddenstate.TheresultsindicatethatBAMStakesfewerstepson truthmapisavailableduringtheexecution.Nevertheless,thedecoded
averagetocompletethegamethantheotheralgorithms.Specifically, beliefmapcanprovideavisualizationoftheagent’shiddenstateand
comparedtoIC3Netandtheheuristicalgorithm,BAMScompletesthe hencecanbeusedtointerpretatetheagent’sdecision-makingprocess.
gamewithapproximately30%fewerstepsonaverage.Comparedto Figure 4(b) and Figure 4(d) depict an example of the decoded
TarMAC,BAMScompletesthegamewith6%fewersteps.However, believemapforfiveagentsatthebeginningofthegameandattime
itshouldbenotedthatagentsusingTarMACtransmittheirhidden step3ofthegame,respectively.Thegriddedmapshowsthelocation
statemuchmorefrequently.Moreover,asmentionedinSection3, oftheagentsandtheprey,andthelocationwheretheagentsends
agentsinTarMACmustsynchronizewitheachotherabouttheirgating outamessage.The3channelsofthedecodedmapindicatethebelief
decisionateverytimestep,whichincurssignificantoverhead.The of the agents’ location, the prey’s location and the explored area.
comparisonbetweenBAMSandBAMS-Rdemonstratesthattheuse Atthebeginningofthegame,allagentsonlyhaveaccessoftheir
ofbelief-mapassistedtrainingleadstoa27%reductioninthenumber local information. Interestingly, we found that the agents learned(a)Step1GroundTruthMap (b)Step1DecodedMap
(c)Step3GroundTruthMap (d)Step3DecodedMap
Figure4:VisualizationinSimple12×12environmentofStep1andStep3.Leftgridfigures(a)(c)isgroundtruthmapshowsthetrajectoryof
agents.Squarerepresentsagentandstarrepresentsprey.Circlerepresentsthestartinglocationofagent,andtheWi-Fiiconrepresentsthatagent
sentoutamessageonthatstep.Rightheatmapfigures(b)(d)givethevisualizedbeliefmapofagents.Brightergridsindicatehigherpossibility
thatthegridsaretakenbyagents,prey,orexplored.
Table2:ScalabilityAnalysisofModelwithVaryingNumbersofAgents
2 5 7 10 15
Avgsteps Commrate Avgsteps Commrate Avgsteps Commrate Avgsteps Commrate Avgsteps Commrate
Heuristic 33.36 - 27.40 - 25.94 - 20.47 - 15.56 -
IC3Net 29.34 0.43 32.90 0.39 33.13 0.42 34.91 0.42 35.74 0.39
TarMAC 23.03 0.96 22.59 0.91 23.67 0.81 24.55 0.75 24.79 0.63
BAMS-R 28.53 0.04 29.80 0.04 32.55 0.04 33.77 0.05 34.82 0.06
BAMS 23.04 0.31 21.64 0.27 19.32 0.28 18.76 0.29 18.88 0.30
to be optimistic, as each agent believed that the prey was located thoughtheBlueandYellowagentsdidnotsendoutanymessages,
nearby. The Gray agent reached the prey in step 2 and both Gray theotheragentsstillslightlyhighlightedtheleftandbottomsidesof
andGreenagentssentoutmessagesinsteps1and2.Therefore,at theirexploredareamaps,asiftheyanticipatedsomeoneexploring
step3,allagentsupdatedtheirbeliefmaptoreflectthemessages thisarea.Thissuggestsatypeofmutualunderstandingwithoutdirect
theyreceived.Intheirpreylocationmap,theareasaroundlocation communication.
(5,9)ishighlighted,whichreflectsthecorrectpreylocationthatthey Totesttherobustnessofthepolicies,wetraintheBAMS,BAMS-R
learnedfromtheGrayagent.Intheirexploredareamap,therightside andIC3Netmodelinanenvironmentwith5agentsandtestthemin
andcenterareaofthemaparehighlighted,indicatingtheareathathas differentenvironmentswithagentnumbersvaryingfrom2to15.The
beenexploredbytheGrayandGreenagents.TheGreenandOrange resultsarereportedinTable2,wherewealsolistedtheperformanceof
agentsobservedeachotherinstep3,resultinginthehighlighting theheuristicalgorithmasareference.Asweexpected,forBAMS,the
of each other’s location in their location map. Interestingly, even averagenumberofstepsneededtocompletethegamereducesasthenumberofagentsincreases.However,forIC3NetandBAMS-R,the 4.4 ExperimentalResultsforMovingPrey
trendgoesintheoppositedirection.Asthenumberofagentsincreases,
Wecreatedadynamicpreyenvironmentwherethepreyisableto
duetotheincreasednumberofmessages,theagentshavedifficulty
moveinordertoevadecapturebytheagents.Thepreyhasthesame
extractingusefulinformation,resultinginanincreasednumberof
observationandactionspaceastheagents.Whenoneormoreagents
stepstocompletethegame.ThisexperimentdemonstratesthatBAMS
areobserved,thepreywillmovetothenearbygridthathasthefarthest
helpstotrainaneffectivemessagepassingframework,allowingagents
Euclideandistancefromtheobservedpredators.Inthecaseofatie,
toperformbetterinthegame.
thedirectionofmovementischosenrandomly.Thegameendswhen
atleastoneagentsuccessfullycapturestheprey.
4.3 ExperimentalResultsforComplexEnvironment Table3:AvgSteps&CommRateforComplexEnvironments
No.ofobstacles 10 20 30
Avgsteps Commrate Avgsteps Commrate Avgsteps Commrate
Inthesecondexperiment,wetestedourapproachunderacomplex IC3Net 45.39 0.53 48.56 0.54 49.37 0.57
environmentwithobstacles.Eachgridintheenvironmentisencoded BAMS-R 39.43 0.062 44.78 0.073 46.92 0.076
BAMS 31.80 0.056 36.51 0.065 41.42 0.054
asmulti-hotvectorofsize4,whichrepresentswhetherthegridis
occupied by a predator, a prey, or an obstacle, and whether it has
been observed. We fixed the environment size to be 12×12. In
Table4:AvgSteps&CommRateforMovingPreyEnvironments
eachrandomlygeneratedtrainingenvironment,thereare20randomly
placedobstacles
VisionSize 3×3 5×5
Avgsteps Commrate Avgsteps Commrate
BAMS 32.47 0.94 35.64 0.96
BAMS(pre-trained) 14.76 0.01 17.42 0.01
WefurthertrainedtheBAMSmodelinadynamicpreyenvironment
withamapsizeof12andobservationrangeof3×3and5×5.Two
trainingstrategiesweretested.Inthefirstapproach,theBAMSmodel
wastrainedfromscratchinamovingpreyenvironment.Andinthe
2nd approach,wepre-trainedtheBAMSmodelinanenvironment
withfixedprey,andthenfine-tuneditinamovingpreyenvironment.
(a)Simple20×20 (b)Complex12×12 FromTable4wecanseethatBAMSwithpre-trainingoutperforms
the one without pre-training, reducing the average steps by more
Figure5:AverageStepTakenComparison than 50%. We also observed that BAMS without pre-train had a
much higher communication rate. A possible explanation for the
performancediscrepancyisthatthegamewithmovingpreyhastwo
differentgoals:locatingthepreyandcatchingtheprey.Theformer
hasarelativelystableenvironment,whilethelatterhasaconstantly
Figure5(b)comparestheconvergencespeedofBAMS,Tarmac,
changingenvironmentasthepreyisescaping.Itisdifficulttolearnan
IC3NetandBAMS-R.WecanseethatBAMSonceagainhasthe
effectivecommunicationstrategyandanenvironmentrepresentation
fastestconvergencespeedcomparedtotheothermodels,completing
forbothgoalsinoneround.
thegamewith3fewerstepsthantheothertwomodelsinaverage.In
comparisontoFigure5(a),theperformanceofIC3Net,whichdoes
not employ attention to the received messages, deteriorates much 5 Conclusion
fasterthanTarmac,BAMS-RandBAMS.Thismeanseffectivemes-
This paper proposes a novel training approach called belief map
sagepassingnetworkbecomesincreasinglyimportantinacomplex
assistedtrainingtoimprovetheconvergenceandefficiencyofmulti-
environment.
agentcooperativegameswithdistributeddecision-making.Toover-
Wealsoobservedthatastheenvironmentbecomesmorecomplex,
come the issue of partial observation, attention-based inter-agent
theperformanceofthosemodelsoscillatesmoresignificantly.This
communicationisadopted.Theagentsaretrainedtolearnwhento
canbeseeninFigure5(a)andFigure5(b)whentheenvironmentsize
gatethemessagetosavebandwidthandavoidinterferencewithir-
is20orwhenobstaclesareincluded.Thereasonforthisisthatin
relevantinformation.WecomparedourapproachwithIC3Netand
randomlygeneratedlargeandcomplexenvironments,thedifficulty
TarMACinbothsimpleandcomplexpredator-preyenvironments.
levelofthegamecanvarysignificantly.Factorssuchastheinitial
Theexperimentalresultsshowthatourattention-basedbeliefmapcan
location of the agents and distribution of obstacles can affect the
helptheagentslearnabetterrepresentationoftheenvironment’shid-
numberofstepsneededtocompletethegame.
denstateandprocessmessageseffectively,leadingtowiserdecisions.
Wefurthertestedthemodelusingtestingenvironmentswith10,20
Additionally,thebeliefmapassistedtrainingimprovesconvergence
and30obstacles.Wefoundthateventhoughthenetworkistrained
speedandreducestheaveragenumberofstepsneededtocomplete
with20obstacles,itwasabletohandledifferentenvironments.The
thegame.
performanceofthethreedeeplearningmodelsinacomplexenvi-
ronmentisshowninTable3.InaverageBAMSreducesthenumber
of steps by 23.6% and 16.5% compared to IC3Net and BAMS-R,
respectively.Acknowledgements ComputerandKnowledgeEngineering(ICCKE),pp.171–176.IEEE,
(2016).
ThisresearchispartiallysupportedbytheAirForceOfficeofScien- [18] YaruNiu,RohanRPaleja,andMatthewCGombolay,‘Multi-agent
graph-attentioncommunicationandteaming.’,inAAMAS,pp.964–973,
tificResearch(AFOSR),undercontractFA9550-24-1-0078andNSF
(2021).
underawardCNS-2148253.
[19] EmanuelePesceandGiovanniMontana,‘Improvingcoordinationin
ThepaperwasreceivedandapprovedforpublicreleasebyAFRL small-scalemulti-agentdeepreinforcementlearningthroughmemory-
onMay16th2023,casenumberAFRL-2023-2374.AnyOpinions, drivencommunication’,MachineLearning,109(9),1727–1747,(2020).
findings,andconclusionsorrecommendationsexpressedinthismate- [20] SebastianRuder,‘Anoverviewofgradientdescentoptimizationalgo-
rithms’,arXivpreprintarXiv:1609.04747,(2016).
rialarethoseoftheauthorsanddonotnecessarilyreflecttheviewsof
[21] JunjieSheng,XiangfengWang,BoJin,JunchiYan,WenhaoLi,Tsung-
AFRLoritscontractors. HuiChang,JunWang,andHongyuanZha,‘Learningstructuredcom-
municationformulti-agentreinforcementlearning’,AutonomousAgents
andMulti-AgentSystems,36(2), 50,(2022).
References [22] DavidSimões,NunoLau,andLuísPauloReis,‘Multiagentdeeplearn-
ingwithcooperativecommunication’,JournalofArtificialIntelligence
[1] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,Prze- andSoftComputingResearch,10,(2020).
mysławDe˛biak,ChristyDennison,DavidFarhi,QuirinFischer,Shariq [23] AmanpreetSingh,TusharJain,andSainbayarSukhbaatar,‘Learning
Hashme,ChrisHesse,etal.,‘Dota2withlargescaledeepreinforcement whentocommunicateatscaleinmultiagentcooperativeandcompetitive
learning’,arXivpreprintarXiv:1912.06680,(2019). tasks’,arXivpreprintarXiv:1812.09755,(2018).
[2] DanielSBernstein,RobertGivan,NeilImmerman,andShlomoZil- [24] SainbayarSukhbaatar,RobFergus,etal.,‘Learningmultiagentcom-
berstein,‘Thecomplexityofdecentralizedcontrolofmarkovdecision munication with backpropagation’, Advances in neural information
processes’,Mathematicsofoperationsresearch,27(4),819–840,(2002). processingsystems,29,(2016).
[3] AbhishekDas,ThéophileGervet,JoshuaRomoff,DhruvBatra,Devi [25] OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMath-
Parikh,MikeRabbat,andJoellePineau,‘Tarmac:Targetedmulti-agent ieu,AndrewDudzik,JunyoungChung,DavidHChoi,RichardPowell,
communication’,inInternationalConferenceonMachineLearning,pp. TimoEwalds,PetkoGeorgiev,etal.,‘Grandmasterlevelinstarcraftii
1538–1546.PMLR,(2019). usingmulti-agentreinforcementlearning’,Nature,575(7782),350–354,
[4] JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShi- (2019).
monWhiteson,‘Learningtocommunicatewithdeepmulti-agentrein-
forcementlearning’,Advancesinneuralinformationprocessingsystems,
29,(2016).
[5] MingyangGeng,KeleXu,XingZhou,BoDing,HuaiminWang,andLei
Zhang,‘Learningtocooperateviaanattention-basedcommunication
neuralnetworkindecentralizedmulti-robotexploration’,Entropy,21(3),
294,(2019).
[6] SeppHochreiterandJürgenSchmidhuber,‘Longshort-termmemory’,
Neuralcomputation,9(8),1735–1780,(1997).
[7] GuangzhengHu,YuanhengZhu,DongbinZhao,MengchenZhao,and
Jianye Hao, ‘Event-triggered communication network with limited-
bandwidthconstraintformulti-agentreinforcementlearning’,IEEE
TransactionsonNeuralNetworksandLearningSystems,(2021).
[8] RuHuang,XiaoliChu,JieZhang,andYuHenHu,‘Energy-efficient
monitoringinsoftwaredefinedwirelesssensornetworksusingreinforce-
mentlearning:Aprototype’,InternationalJournalofDistributedSensor
Networks,2015,(2015).
[9] JiechuanJiangandZongqingLu,‘Learningattentionalcommunication
formulti-agentcooperation’,Advancesinneuralinformationprocessing
systems,31,(2018).
[10] DaewooKim,SangwooMoon,DavidHostallero,WanJuKang,Taey-
oung Lee, Kyunghwan Son, and Yung Yi, ‘Learning to schedule
communicationinmulti-agentreinforcementlearning’,arXivpreprint
arXiv:1902.01554,(2019).
[11] QingbiaoLi,FernandoGama,AlejandroRibeiro,andAmandaProrok,
‘Graphneuralnetworksfordecentralizedmulti-robotpathplanning’,
in2020IEEE/RSJInternationalConferenceonIntelligentRobotsand
Systems(IROS),pp.11785–11792.IEEE,(2020).
[12] YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,and
YangGao. Multi-agentgameabstractionviagraphattentionneural
network,2019.
[13] YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,and
YangGao,‘Multi-agentgameabstractionviagraphattentionneuralnet-
work’,inProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume34,pp.7211–7218,(2020).
[14] RyanLowe,YiIWu,AvivTamar,JeanHarb,OpenAIPieterAbbeel,
and Igor Mordatch, ‘Multi-agent actor-critic for mixed cooperative-
competitiveenvironments’,Advancesinneuralinformationprocessing
systems,30,(2017).
[15] GarySLynch,Singlepointoffailure:The10essentiallawsofsupply
chainriskmanagement,JohnWileyandSons,2009.
[16] HangyuMao,ZhengchaoZhang,ZhenXiao,ZhiboGong,andYanNi,
‘Learningagentcommunicationunderlimitedbandwidthbymessage
pruning’,inProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume34,pp.5142–5149,(2020).
[17] MiladMoradi,‘Acentralizedreinforcementlearningmethodformulti-
agentjobschedulingingrid’,in20166thInternationalConferenceon