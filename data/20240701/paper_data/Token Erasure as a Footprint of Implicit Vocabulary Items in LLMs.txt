Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs
SheridanFeucht DavidAtkinson ByronC.Wallace DavidBau
NortheasternUniversity
{feucht.s, atkinson.da, b.wallace, d.bau}@northeastern.edu
Abstract
flat nin.th.s
LLMs process text as sequences of tokens
thatroughlycorrespondtowords,whereless
Mon.k’s compos.itions and impro.vis.ations
common words are represented by multiple feature dis.son.ances and angular mel.od.ic
tokens. However, individual tokens are of- tw.ists, often using flat nin.th.s, flat
fifth.s, unexpected chrom.atic notes together,
tensemanticallyunrelatedtothemeaningsof low bass notes and st.ride, and fast whole
thewords/conceptstheycomprise. Forexam- tone runs, combining a highly per.cuss.ive
attack with ab.rupt, dram.atic use of switched
ple, Llama-2-7b’s tokenizer splits the word
key releases, sil.ences, and hes.itations.
“northeastern” into the tokens [_n, ort, he,
astern],noneofwhichcorrespondtoseman- score tokens 0.315 stride
ticallymeaningfulunitslike“north”or“east.” 0.582 dramatic 0.234 melodic
Similarly,theoverallmeaningsofnamedenti- 0.555 twists 0.203 silences
0.415 low bass 0.183 s, tieslike“NeilYoung”andmulti-wordexpres-
0.339 flat ninths, 0.028 together,
sionslike“breakaleg”cannotbedirectlyin-
0.321 Monk’ 0.016 , and fast whole
ferred from their constituent tokens. Mecha-
nistically,howdoLLMsconvertsucharbitrary
Figure1: Weobserve“erasure”oftoken-levelinforma-
groupsoftokensintousefulhigher-levelrep-
tioninlaterlayersofLLMsformulti-tokenwordsand
resentations? In this work, we find that last
entities (top). We hypothesize that this is a result of
token representations of named entities and
a process that converts token embeddings into useful
multi-tokenwordsexhibitapronounced“era-
lexicalrepresentations,andintroduceanewmethodfor
sure”effect,whereinformationaboutprevious
enumeratingtheselexicalitems(bottom).
andcurrenttokensisrapidlyforgotteninearly
layers. Using this observation, we propose a or to a piece of a larger word (mon in “salmon”).
methodto“readout”theimplicitvocabulary The vocabulary of tokens available to a model is
of an autoregressive LLM by examining dif- typicallydeterminedbeforetrainingwithbyte-pair
ferencesintokenrepresentationsacrosslayers, encoding(Sennrichetal.,2016),whichisbasedon
andpresentresultsofthismethodforLlama-2-
aspecificdatasetandcanleadtounintuitiveresults.
7bandLlama-3-8B.Toourknowledge,thisis
Forexample,Llama-2-7b’s(Touvronetal.,2023)
thefirstattempttoprobetheimplicitvocabu-
tokenizerbreakstheword“northeastern”intothe
laryofanLLM.1
tokens[_n,ort,he,astern],noneofwhichcorre-
1 Introduction spondtosemanticallymeaningfulunitslike“north”
or “east.” Capitalization also creates unexpected
Despite their widespread use, the specific mech-
issues: forexample,theword“Hawaii”issplitinto
anisms by which LLMs are able to “understand”
twotokensifthefirstletteriscapitalized[_Hawai,
andgeneratecoherenttextarenotwellunderstood.
i],butfourifthefirstletterislowercase[_ha,w,ai,
One mystery is the process by which groups of
i]. Inspiteofthesechallenges,largemodelsareap-
subwordtokensareconvertedintomeaningfulrep-
parentlyableto“understand”suchidiosyncraticto-
resentations,aprocessdescribedbyElhageetal.,
kenizationsofmulti-tokenwordswithfewobserv-
2022andGurneeetal.,2023asdetokenization.
ableeffectsondownstreamperformance(Gutiérrez
Current language models process text as a se-
etal.,2023),unlesstheseweaknessesaredirectly
riesoftokensdrawnfromasettokenvocabulary:
targeted(Wangetal.,2024;Batsurenetal.,2024).
Onetokencancorrespondtoasingleword(_fish),
Howisthispossible?
1Codeanddataavailableatfootprints.baulab.info Wehypothesizethatduringpretraining,LLMs
1
4202
nuJ
82
]LC.sc[
1v68002.6042:viXradevelop an implicit vocabulary that maps from to a number of unrelated concepts (“Space Jam,”
groupsofarbitrarytokenstosemanticallymeaning- “SpaceStation”).2
ful units. These lexical units may be multi-token Other work in interpretability has also started
words (“northeastern”), named entities (“Neil to uncover evidence of models encoding lexical
Young”), or idiomatic multi-word expressions items. Elhage et al. (2022) observe neurons in
(“breakaleg”)andcanbeunderstoodas“item[s] early layers that fire on the last tokens of multi-
that function as single unit[s] of meaning” in a token words, names of famous people, generic
model’s vocabulary (Simpson, 2011). Lexical nouns, compound words, and LaTeX commands.
itemsarealsonon-compositional: Justasthemean- They also find late-layer neurons that seem to be
ingof“breakaleg”cannotbepredictedfromthein- relevant to retokenization, i.e., conversion from
dividualmeaningsof“break”and“leg,”themean- internal representations back into tokens. For ex-
ing of “patrolling” cannot be predicted from its ample,aretokenizationneuronmightfireon_st
constituent tokens pat and rolling. This arbi- and promote rag in order to facilitate the output
trarinessnecessitatessomekindofstoragesystem, oftheword“straggler.” Gurneeetal.(2023)also
implicitorotherwise(Murphy,2010). find examples of polysemantic neurons in Pythia
How exactly do LLMs deal with these cases models(MallenandBelrose,2023)thatactivatefor
mechanistically? Inthispaper,webegintoanswer anumberofmulti-tokenconstructionslike“apple
thisquestionbyinvestigatingtoken-levelinforma- developer,”“Bloom.ington,”and“research.gate.”
tionstoredinLLMrepresentations.
3 LinearProbingofHiddenStates
• We find that last token positions of multi-
3.1 Method
tokenwordsandnamedentities“erase”token-
level information in early layers for both Iflasttokenpositionsaresoimportant(Section2),
Llama-2-7b(Touvronetal.,2023)andLlama- then what do these representations encode? Per-
3-8b(Meta,2024). haps the last hidden state directly stores informa-
tionaboutothersubjecttokens(e.g.,_Warsmight
• Wedevelopaheuristicforscoringthe“lexical-
contain some encoding for _Star in its hidden
ity”ofagivensequenceoftokens,anduseit
state). Totestthishypothesis,weinvestigatehid-
to“readout”alistofanLLM’slexicalitems
denstatesforbothLlama-2-7bandLlama-3-8b,as
givenalargedatasetofnaturaltext.
theyhavesignificantlydifferenttokenvocabulary
sizes(32kand128ktokens,respectively). Wetrain
Weinterpretthiserasureeffectasa“footprint”
(ℓ) (ℓ)
linearprobesp totakeahiddenstateh atlayer
ofamechanisminearlylayersthatorchestratesthe i t
ℓ and token position t and predict the value of a
formationofmeaningfullexicalitems.
nearbytokent+i. (e.g.,aprobetrainedtopredict
2 Background theprevioustokenforlayer5hiddenstateswould
(5)
bedenotedbyp ).
−1
Previousworkhasshownthatknowledgeabouta
Wetrainprobesforalllayerindexes0 ≤ ℓ < 32
multi-tokenentityisoftenstoredinthelasttokenof
andoffsetsi ∈ {−3,−2,−1,0,1}. Wealsotrain
thatentity. Forexample,Mengetal.(2022)found
probesinthesamemannerontheembeddinglayer
thatfactualinformationaboutasubjectlike“The
(ℓ = −1) and on the final outputs of the network
SpaceNeedle”wouldbeconcentratedintherepre-
beforethelanguagemodellinghead(ℓ = 32). We
sentationforle. Gevaetal.(2023)findevidence
trainedprobesonarandomsampleof428ktokens
forasubjectenrichmentstageduringfactualrecall,
fromthePile(Gaoetal.,2020)usingAdamWfor
where information about an entity is collected at
16epochswithabatchsizeof4andalearningrate
itslasttokeninearlylayers,whichisalsoseenin
of 0.1. Hyperparameters were selected based on
otherworkonfactualrecallusingthesamedataset
validationperformanceonaseparatePilesample
(Katz et al., 2024), and corroborated by research
(279k tokens) after a random sweep. Each probe
on athlete → sport lookups (Nanda et al., 2023).
takes6-8hourstotrainonanRTX-A6000.
Thisphenomenonmaybeduetotheautoregressive
natureofdecodertransformermodels: modelscan- 2Thisisnotahard-and-fastrule;itdependsonentityfre-
quencyandcontextcues.Forexample,ifamodelsees_The,
notenrich“Space”withinformationaboutSeattle
_E,andiff,itmayalreadyknowthatthesetokensreferto
untilafter“Needle”isseen,as“Space”couldrefer “TheEiffelTower”withoutneedingtoseeelandTower.
2the movie Star Wars the movie Star Wars
Figure2: TestaccuracyonCOUNTERFACTsubjectlasttokensversusothertokensinthedatasetforprobestrained
onLlama-2-7bhiddenstates(n=5063). irepresentsthepositionbeingpredicted(e.g.,i=−1isprevioustoken
prediction;i=1isnext-tokenprediction). Weobservean“erasure”effectinlastsubjecttokensthatisnotpresent
forothertypesoftokens: theselastsubjecttokensconsistently“forget”aboutprecedingtokensandthemselves.
AppendixAshowsLlama-3-8bresultsandin-distributionperformanceonPiletokens.
3.2 COUNTERFACTSubjects imbalancesinprobetrainingdata, butthisseems
nottobethecaseeither(AppendixB).
AftertrainingprobesinSection3.1,wetestthem
ontheCOUNTERFACTdataset(Mengetal.,2022),
which consists of prompts about subjects that re- 3.3 Multi-TokenWords
quirefactualknowledgetocompletecorrectly(e.g.
“Mount Passel is in Antarctica”). We filter the Intuitively,theprocessofconvertingamulti-token
datasettoincludeonlypromptsthatthemodelan- sequence like [_n, ort, he, astern] into a mean-
swerscorrectly,yielding5,063examplesforLlama- ingful representation of the word “northeastern”
2-7band5,495examplesforLlama-3-8b. Toaug- resembles the process of converting [_E, iff, el,
ment this dataset, we also sampled and filtered Tower] into “Eiffel Tower.” We hypothesize that
down[album/movie/series→creator]pairsfrom models treat multi-token words in the same way
Wikidata(Vrandecˇic´ andKrötzsch,2014)andem- that they treat multi-token entities, and test our
beddedtheminpromptsinthesamemanner,yield- probesfromSection3.1onmulti-tokenwords. Af-
ingatotalof12,135correctly-answeredprompts tersampling500articles(∼256ktokens)fromthe
forLlama-2-7band13,995forLlama-3-8b. 20220301.ensplitoftheWikipediadump(Foun-
Figure2showsprobetestresultson COUNTER- dation, 2022), we split by white-space to naively
FACTlastsubjecttokens(right)versuseveryother identifywordboundaries. Aspredicted,weseethe
typeoftokeninthedataset(left). Weseeastriking same“erasing”patternformulti-tokenwordsthat
“erasure”effectforlasttokensofCOUNTERFACT we do for multi-token entities (Appendix A, Fig-
subjects, where these hidden states consistently ure11). Thissuggeststhattheymaybeprocessed
“forgetabout”precedingandcurrenttokens. Sub- inasimilarmannerinearlylayers.
ject tokens that are not in the last position (e.g.,
_Star) do not exhibit this pattern (Appendix A,
4 BuildingaVocabulary
Figure 13). This striking drop in token accuracy
isreminiscentofthesubjectenrichmentstagede-
scribedbyGevaetal.(2023),suggestingthatthe After examination of probe behavior for multi-
tokens_Starand_Warsmaybeoverwritteninthe tokenwordsandentities,wehypothesizethatthis
processofrepresentingtheconceptofStarWars. “erasure”effectisaresultoftheimplicitformation
We also observe the same phenomenon when oflexicalrepresentationsinearlylayers. Tochar-
testing on named entities identified by spaCy in acterizethisphenomenon,weproposeanerasure
Wikipedia articles (Appendix A, Figure 12), sug- scoreψ toidentifytokensequencesthatfollowthe
gestingthatthiseffectisnotanartifactoftheshort patternobservedinSection3. Wethenintroduce
templatesfoundintheCOUNTERFACTdataset. Ad- anapproachto“readingout”alistofimplicitvo-
ditionally,weconsiderwhetherthisresultisdueto cabularyentriesforagivenmodelusingthisscore.
34.1 AnErasureScore MTW MTE
We first define an erasure score ψ , which is a
p,q llama data prec. recall prec. recall
heuristicdesignedaroundtheintuitionthatthelast
wiki 0.306 0.016 0.143 0.016
tokenrepresentationofalexicalitemshouldexhibit
2-7b
pile 0.296 0.017 0.080 0.018
astrong“erasing”effectforprobepredictionsfrom
layer1tolayerL.3 Thescorequantifies“erasing” wiki 0.044 0.001 0.010 0.000
3-8b
behaviorattokenpositionswithinagivensequence pile 0.023 0.001 0.012 0.001
at indices p through q, and penalizes any erasure
oftokensoutsideoftheseboundaries. Equation1 Table1: Precisionandrecallforaggregatedresultsof
definesthescoreψ forasequences oflength Algorithm1runonLlama-2-7bandLlama-3-8b,using
p,q p,q
n = q−p+1as: eitherWikipediaorPiledocuments(|D|=500). MTW
referstoallmulti-tokenwordsinthedatasetwhensplit
(cid:32) q −1 (cid:33) bywhitespace;MTEreferstoallspaCynamedentities.
1 (cid:88) (cid:88)
δ(q,0)+ 1 (t,i)·δ(t,i)
within
1+2n
t=pi=−2
(1) multi-tokensequencethatappearsmorethanonce
whereδ(t,i)denotesthechangeinprobabilityof acrossalldocuments. Asthisprocessisverydata-
the predicted token t+i from layer 1 to layer L, dependent,wecomparethetop50resultsforPile
basedonprobesp(ℓ) fromSection3.1. andWikipediatextinAppendixE.
i
With this approach, we are able to recover
(1) (L)
δ(t,i) = P (t+i|h )−P (t+i|h ) (2) ∼1800 sequences for Llama-2-7b and ∼900 for
p(1) t p(L) t
i i
Llama-3-8busingthesamefivehundredWikipedia
Ift+iliesoutsidetheboundariesofs,wedecrease articles. Althoughrecallisquitelow(Table1),we
ψ p,q. Otherwise,alargedropbetweenlayersδ(t,i) findthat44.9%ofsequencesrecoveredforLlama-
increasesthevalueofψ p,q. 2-7bonWikipediatextareeithermulti-tokenwords
ormulti-tokenentities(29.68%forPiletext). For
(cid:40)
−1ift+i < p
1 (t,i) = (3) Llama-3-8b, only 5% and 3% of sequences are
within
1else MTWs or MTEs. However, looking at examples
ofLlama-3-8bsequencesinAppendixE,wecan
Weprovidefurtherexplanationoftheintuitionbe-
observe other interesting cases, like multi-token
hindthisapproachinAppendixC.
expressions (“gold medalists,” “by per capita in-
4.2 SegmentingDocuments come,” “thank you for your understanding”) and
LaTeXcommands(assimilarlyobservedbyElhage
Wedevelopanalgorithmbuiltaroundourerasure
etal.(2022)). BecauseLlama-3-8b’stokenvocab-
score ψ that breaks any given document d ∈ D
ulary is four times larger than Llama-2-7b’s, its
intohigh-scoring,non-overlappingsegmentscov-
implicitvocabularyalsoseemstoconsistofmore
eringallofd(Algorithm1,AppendixD).Figure1
multi-wordexpressionsandchunksofcoderather
showsthetop-scoringsequencess calculatedin
p,q
thanmulti-tokenwords(AppendixE,Table6).
thismannerfromaWikipediaexcerptaboutThelo-
nious Monk, where unigram scores are excluded
5 Conclusion
for clarity. Not all multi-token words are scored
highly via our approach, but the highest-scoring Inthiswork,wepresentpreliminaryevidencefor
sequencesareplausiblelexicalitemsthatarenon- theexistenceofanimplicitvocabularythatallows
compositionalinnature(“dram.atic”,“sil.ences”, modelstoconvertfrombyte-pairencodedtokens
“tw.ists”). Weshareexamplesofcompletesegmen- tousefullexicalitems. Wepositthatthe“erasure”
tationsinAppendixD. effect we observe for Llama-2-7b and Llama-3-
8B is a result of model processes that deal with
4.3 ModelVocabularies
multi-tokenexpressions,andusethisinsighttopro-
Finally, we propose a method to “read out” the
pose a new method for “reading out” an LLM’s
implicitvocabularyofamodelMgivenadataset
implicit vocabulary. This is a first step towards
D. Foreachdocumentd ∈ D,wesegmentdusing
understandingtheformationoflexicalrepresenta-
Algorithm1. Wethenaveragescoresψ forevery
tionsinLLMs,andmayserveasausefultoolfor
3ForbothLlama-2-7bandLlama-3-8bwesetL=9. elucidationofwordsthatagivenmodel“knows.”
4Limitations References
Evaluation of implicit vocabulary-building meth- Khuyagbaatar Batsuren, Ekaterina Vylomova, Verna
Dankers, Tsetsuukhei Delgerbaatar, Omri Uzan,
ods (Section 4) is challenging due to the lack of
Yuval Pinter, and Gábor Bella. 2024. Evaluat-
aknownground-truth. Ourapproachismotivated
ing subword tokenization: Alien subword compo-
bythedesiretounderstandtheinnerworkingsof sition and oov generalization challenge. Preprint,
themodelbeingstudied,butwehavenoauthorita- arXiv:2404.13292.
tivereferencethatdistinguishesbetweensituations
Nelson Elhage, Tristan Hume, Catherine Olsson,
whereagivensequencegetsahighψvaluebecause
NeelNanda,TomHenighan,ScottJohnston,Sheer
itistrulytreatedasalexicalunitbythemodel,or ElShowk, Nicholas Joseph, Nova DasSarma, Ben
where it may be due to an error in our methodol- Mann, Danny Hernandez, Amanda Askell, Kamal
Ndousse,AndyJones,DawnDrain,AnnaChen,Yun-
ogy. To quantify our results, we have compared
tao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-
the extracted vocbulary to sequences that we as-
Dodds, Jackson Kernion, Tom Conerly, Shauna
sumetobelikelylexicalitems: multi-tokenwords Kravec, Stanislav Fort, Saurav Kadavath, Josh Ja-
and spaCy named entities. However, this likely cobson,EliTran-Johnson,JaredKaplan,JackClark,
TomBrown,SamMcCandlish,DarioAmodei,and
doesnotcoverallcasesforwhich“tokengrouping”
Christopher Olah. 2022. Softmax linear units.
occursinLLMs.
Transformer Circuits Thread. Https://transformer-
Anotherlimitationofthisworkisthatwehave circuits.pub/2022/solu/index.html.
restrictedouranalysistoknownentities. Thereis
WikimediaFoundation.2022. Wikimediadownloads.
alsothequestionofwhathappensforintermediate
casessuchasplausible-soundingfictionaltownsor LeoGao,StellaBiderman,SidBlack,LaurenceGold-
names of people who are not famous. If ψ corre- ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
lateswithsequencepresenceintrainingdata,these
Presser, and Connor Leahy. 2020. The Pile: An
results could be useful for understanding how fa-
800gbdatasetofdiversetextforlanguagemodeling.
miliaranLLMiswithagivenwordorentity. arXivpreprintarXiv:2101.00027.
Finally,ourmeasurementshavebeenrunonlyon
MorGeva,JasmijnBastings,KatjaFilippova,andAmir
theLlamafamilyofmodelsanddonotyetextend
Globerson.2023. Dissectingrecalloffactualasso-
tonon-Llamamodelsofcomparablesize,orLlama
ciationsinauto-regressivelanguagemodels. ArXiv,
modelsoflargersizes. abs/2304.14767.
EthicsStatement Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.
In this work, we restrict our analysis to English 2023. Findingneuronsinahaystack: Casestudies
withsparseprobing. Preprint,arXiv:2305.01610.
words,duetoourbiasesasnativespeakersofEn-
glish. Wehopethatthisworkcanalsoprovidevalu-
Bernal Jiménez Gutiérrez, Huan Sun, and Yu Su.
able insights for other languages, especially low- 2023. Biomedicallanguagemodelsarerobusttosub-
resource languages, where understanding “what optimaltokenization. Preprint,arXiv:2306.17649.
wordsanLLMknows”maybeespeciallyuseful.
Shahar Katz, Yonatan Belinkov, Mor Geva, and Lior
Wolf. 2024. Backward lens: Projecting language
Acknowledgments
modelgradientsintothevocabularyspace. Preprint,
arXiv:2402.12865.
We thank David Smith, Bilal Chughtai, Chantal
Shaib,AtticusGeiger,andAdrianChangforhelp- AlexMallenandNoraBelrose.2023. Elicitinglatent
fuldiscussionandfeedbackthroughoutthecourse knowledgefromquirkylanguagemodels. Preprint,
arXiv:2312.01037.
of this project. This work was supported in part
byOpenPhilanthropy,andbytheNationalScience KevinMeng,DavidBau,AlexAndonian,andYonatan
Foundation(NSF)grantIIS-1901117. Belinkov.2022. Locatingandeditingfactualasso-
Experiments were implemented using the ciations in gpt. In Neural Information Processing
Systems.
nnsightlibrary;manywererunontheCenterfor
AISafetyComputeCluster. Anyopinions,findings, Meta.2024. Introducingmetallama3: Themostcapa-
andconclusionsorrecommendationsexpressedin bleopenlyavailablellmtodate.
thismaterialarethoseoftheauthor(s)anddonot
M. Lynne Murphy. 2010. Lexical Meaning. Cam-
necessarilyreflecttheviewsofthesponsors.
bridgeTextbooksinLinguistics.CambridgeUniver-
sityPress.
5NeelNanda,SenthooranRajamanoharan,JánosKrámar,
andRohinShah.2023. Factfinding: Attemptingto
reverse-engineerfactualrecallontheneuronlevel.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neuralmachinetranslationofrarewordswith
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages1715–1725,
Berlin,Germany.AssociationforComputationalLin-
guistics.
James Simpson. 2011. The Routledge handbook of
appliedlinguistics. Taylor&Francis. Figure3: OveralltestaccuracyonunseenPiletokens
(n = 273k)forprobestrainedonLlama-2-7bhidden
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- states. Nexttokenpredictionbecomesmoreaccurate
bert, Amjad Almahairi, Yasmine Babaei, Nikolay throughoutmodellayersascurrentandprevioustoken
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti accuracydecreases.
Bhosale,DanBikel,LukasBlecher,CristianCanton
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Multi-TokenEntityAccuracy Figure10shows
CynthiaGao,VedanujGoswami,NamanGoyal,An-
resultsforprobestestedonthelasttokenpositions
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa, ofmulti-tokenentitiesidentifiedbyspaCy,using
IsabelKloumann,ArtemKorenev,PunitSinghKoura, the same dataset as A.1. We use spaCy’s named
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
entityrecognitionpipelinetoidentifynamedenti-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
ties. Becausedigits0-9areaddedtoLlama-2-7b’s
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- vocabulary,wefilteroutallclassesrelatingtonum-
stein,RashiRungta,KalyanSaladi,AlanSchelten, bers (PERCENT, DATE, CARDINAL, TIME, ORDINAL,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
MONEY,QUANTITY),withthethoughtthatthesese-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
quences may be treated differently at the detok-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan, enizationstage.
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas A.2 Llama-2-7bResults
Scialom.2023. Llama2: Openfoundationandfine-
Multi-TokenWordAccuracy Figure11shows
tunedchatmodels. Preprint,arXiv:2307.09288.
results from Llama-2-7b probes tested on multi-
DennyVrandecˇic´ andMarkusKrötzsch.2014. Wiki- tokenwordsfromWikipediaasdescribedinSec-
data: afreecollaborativeknowledgebase. Commun.
tion3.3.
ACM,57(10):78–85.
Multi-TokenEntityAccuracy Figure12shows
DixuanWang,YandaLi,JunyuanJiang,ZepengDing,
Guochao Jiang, Jiaqing Liang, and Deqing Yang. resultsforLlama-2-7bprobestestedonmulti-token
2024. Tokenization matters! degrading large lan- entities from Wikipedia, using the same dataset
guagemodelsthroughchallengingtheirtokenization.
from Section 3.3 and also filtering out number-
Preprint,arXiv:2405.17067.
basedentityclassesasinSectionA.1.
A LinearProbingonHiddenStates
A.3 Llama-2-7bExtras
A.1 Llama-3-8bResults
PileAccuracy WhileFigure2showstestaccu-
COUNTERFACT Accuracy We share results racyoflinearprobesonmodelhiddenstates,Fig-
analogoustoFigure2butforLlama-3-8b,which ure 3 shows in-distribution test accuracy on Pile
shows a similar “erasure” pattern (Figure 8). tokens. We can observe a smoother trajectory of
ProbesaretestedonlyonpromptsthatLlama-3-8b gradual“forgetting”ofpreviousandcurrenttoken-
answerscorrectly. levelinformationthroughoutlayers.
Multi-TokenWordAccuracy Figure9showsre- Comparison of Token Positions Figure 13
sultsforprobestestedonthelasttokenpositionsof showsthebreakdownofprobeperformanceondif-
multi-tokenwordsfromWikipedia(where“words” ferenttypesofsubjecttokens: firstsubjecttokens,
aredeterminedbywhitespaceseparation). middlesubjecttokens,andlastsubjecttokens. We
6seethattheobserveddropinpreviousandcurrent MTW MTE
tokenrepresentationobservedinlastsubjecttokens
L prec. recall prec. recall
stillexists,butisnotasdrasticforfirstandmiddle
subjecttokens. 5 0.307 0.002 0.143 0.002
9 0.306 0.016 0.143 0.016
ComparisonofSubjectLengths Wealsoshow
13 0.328 0.003 0.169 0.003
previous token representation broken down by
17 0.330 0.003 0.180 0.003
COUNTERFACTsubjectlengthforlasttokenrepre-
21 0.319 0.003 0.172 0.003
sentationsinFigure14. Unigramsubjectsrepresent
previous token information at a rate even higher
Table2: PrecisionandrecallfordifferentvaluesofL
thannon-subjecttokens. Forbigramsandtrigrams, for Algorithm 1 applied to Llama-2-7b on Wikipedia
weseeapatternsimilartoFigure2. text. RecallseemstobebestforL=9,withprecision
improvingbyafewpointsinmid-latelayers.
B AccountingforPossibleTraining
Imbalance
Lforthesepositions, wetakethisasevidenceof
Oneexplanationfortheobserveddropinaccuracy tokenclumping.
forCOUNTERFACTentitiesacrosslayersisthatour However,wealsoobserve(fromFigure13and
probes have simply not been exposed to as many manualinspection)thatprobepredictionsfortoken
entity tokens during training. We do not believe positions that lie outside the boundaries of a pre-
this is the case for Llama-2-7b for two reasons: sumed lexical item are maintained across layers,
(1)Ifthiseffectwasduetoprobesbeinglesssen- or even promoted. This is clear from the i = −1
sitive to tokens found in multi-token entities, we case in the leftmost plot for Figure 13, and was
wouldalsoseeasignificantdropforfirstandmid- alsoaconsistentpatternwhenweexaminedprobe
dletokens,whichdoesnotoccur(Figure13). (2) behavior over a number of example documents.
We measure the frequency of all test n-grams in Giventhisfact,weincludetheindicatorfunction
theoriginalPiledatausedtotrainourprobes,and 1 (t,i) to decrease ψ in cases where erasure
within
findthatbothsubjectandnon-subjectn-gramsare is happening outside the bounds of the given se-
foundintheprobetrainingdatasetatsimilarrates, quence.
withthemediannumberofoccurrencesinthetest
C.2 ChoiceofL
set for both types of sequences being zero. After
removing the few non-subject sequences that do WechooseL = 9basedonprobe“erasure”behav-
appearoftenintheprobetrainingset,westillsee ior for Llama-2-7b and Llama-3-8b, particularly
thesame“erasure”effect. Figure 2. We also present a short ablation exper-
iment for L ∈ {5,9,13,17,21} with results in
C Intuitionforψ Table2,whichshowsthatothervaluesofLafter
“thedrop”areroughlyequivalenttoL = 9.
C.1 ExplanationofEquation1
D DocumentSegmentation
Our first assumption when designing ψ is that if
an LLM is processing a sequence of tokens cor-
WeshowfulldocumentsegmentationsusingAlgo-
responding to a lexical item (e.g., [Cal, g, ary]),
rithm1forshortexcerptsfromthesameWikipedia
thelasttokenaryinthatsequenceshould“forget
article in Figure 4 and Figure 5. Figure 6 and
itself” between layers 1 and L = 9, according to
Figure7showmoresegmentationsforaPiledocu-
thepatternweobserveinSection3. Wemeasure
ment.
thisdropusingprobabilityscoresfromprobeout-
putsforasingleexample. InplainEnglish,thefirst E ModelVocabularies
term δ(q,0) in Equation 1 represents how much
Tables3through6showthetop50highest-scoring
P(t = ary) drops between layer 1 and layer L
multi-tokensequencesforLlama-2-7bandLlama-
whenprobingthehiddenrepresentationforary.
3-8bacrosseitherfivehundredWikipediaarticles
Inthedoublesummationterm,wetakeintoac-
orfivehundredPilesamples. Entrieswerefiltered
countprobepredictionsfortokensone(i = −1)or
toshowonlysequencesthatappearmorethanonce.
two(i = −2)positionsbeforethecurrenttoken. If
thereisadropinprobabilitybetweenlayer1and
7Algorithm1DocumentSegmentation
Require: documentd ∈ D oflengthl
1: forn = 1toldo ▷allngramlengths
2: forp = 0tol−ndo
3: forq = p+n−1tol−1do
4: assignscoreψ p,q tosequences p,q
5: endfor
6: endfor Figure 5: Full segmentation of a document from
Wikipedia via Algorithm 1 on Llama-3-8b. Borders
7: endfor
indicate segmentation, with bolded letters indicating
8: sortsindescendingorderofψ
multi-token segments. Darker blue cells have higher
9: segms ← ∅
scores,yellowcellshavenegativescores. Thehighest-
10: fors p,q insortedsdo scoringsequenceinthisdocumentis“.AftertheGames
11: if∀s x,y ∈ segms,(x > q∨y < p)then shecommented"”(ψ =0.443).
12: segms ← segms∪{s p,q}
13: endif
14: endfor
15: returnsegms ▷non-overlappingsegments
Figure 6: Full segmentation of a document from the
PileviaAlgorithm1onLlama-2-7b. Bordersindicate
segmentation,withboldedlettersindicatingmulti-token
segments. Darker blue cells have higher scores, yel-
low cells have negative scores. The highest-scoring
sequenceinthisdocumentis“submodel”(ψ =0.559).
Figure 4: Full segmentation of a document from
Wikipedia via Algorithm 1 on Llama-2-7b. Borders
indicate segmentation, with bolded letters indicating
multi-token segments. Darker blue cells have higher
scores,yellowcellshavenegativescores. Thehighest- Figure 7: Full segmentation of a document from the
scoringsequenceinthisdocumentis“AustralianInsti- PileviaAlgorithm1onLlama-3-8b. Bordersindicate
tute”(ψ =0.579). segmentation,withboldedlettersindicatingmulti-token
segments. Darker blue cells have higher scores, yel-
low cells have negative scores. The highest-scoring
sequenceinthisdocumentis“rereallybrave:” (ψ =
0.634).
8the movie Star Wars the movie Star Wars
Figure8: TestaccuracyonCOUNTERFACTsubjectlasttokensversusothertokensinthedatasetforprobestrained
onLlama-3-8b(n=5495). irepresentsthepositionbeingpredicted(e.g.,i=−1isprevioustokenprediction;
i=1isnext-tokenprediction). Weobservean“erasure”effectsimilartoFigure2.
the inter.mitt.ent the inter.mitt.ent
Figure9: TestaccuracyofprobesonlasttokensofWikipediamulti-tokenwordsforprobestrainedonLlama-3-8b
(n=91935;right). Testaccuracyonallothertokensshownontheleft. SimilarlytoFigure2,weseeanerasing
effectthatisnotpresentforothertypesoftokens.
by Bar.ack Ob.ama by Bar.ack Ob.ama
Figure10: TestaccuracyofprobesonlasttokensofWikipediamulti-tokenentitiesforprobestrainedonLlama-3-
8b(n=36723;right). Testaccuracyonallothertokensshownontheleft. EntitiesareidentifiedviaspaCynamed
entityrecognition,excludingentitytypesthatincludedigits.
9the inter.mitt.ent the inter.mitt.ent
Figure11: TestaccuracyofprobesonlasttokensofWikipediamulti-tokenwordsforLlama-2-7b(n=80606,
right). Testaccuracyonallothertokensshownontheleft. SimilarlytoFigure2,weseeanerasingeffectthatisnot
presentforothertypesoftokens.
by Bar.ack Ob.ama by Bar.ack Ob.ama
Figure12: TestaccuracyofprobesonlasttokensofWikipediamulti-tokenentitiesforLlama-2-7b(n=36723;
right). Testaccuracyonallothertokensshownontheleft. EntitiesareidentifiedviaspaCynamedentityrecognition,
excludingentitytypesthatincludedigits.
10_E iff el
)
Figure13: BreakdownforSection3probestestedonCOUNTERFACTfirstsubjecttokens,middlesubjecttokens,
and last subject tokens. We observe an “erasing” effect only for last subject tokens. Because BOS tokens are
recoverablebyi=−1probesathighrates,andsince55%ofpromptstestedonhadsubjectsatthebeginning,we
filterexamplesforwhichBOStokensarelabelsfromtheleftmostplot.
Figure 14: Probe test results for COUNTERFACT subject last tokens broken down for unigrams, bigrams, and
trigrams. Unigramsubjectsstoreprevioustokeninformationatratesnear100%,evenexcludingBOStokens.
11TokenSequence n ct ψ TokenSequence n ct ψ
Gottsche 3 2 0.685220 1992births 7 2 0.573139
berth 3 2 0.680793 19th-century 7 3 0.568861
carries 3 2 0.647844 dehydrogen 5 2 0.553029
Eurocop 3 2 0.644104 Swahili 4 4 0.539052
franchises 3 2 0.642707 ChuckLiddell 6 2 0.537169
0Women 3 2 0.639162 itspopulationwas 5 5 0.534977
rape 3 2 0.632567 bypercapitaincome 6 3 0.518991
Rebell 3 3 0.614295 arebrownish 4 2 0.515703
intermittently 4 2 0.613479 atewomen’sfootball 7 4 0.509384
ennState 4 3 0.607535 Almeida 4 5 0.507277
NorthDakota 4 10 0.600616 ofNewSouthWales 5 3 0.503120
Sride 3 2 0.600013 2015deaths 8 2 0.503074
fiction 2 2 0.599339 Pittsburgh 3 3 0.503070
Sox 3 3 0.599043 21st-century 7 4 0.499362
Bazz 3 2 0.598242 (NSW 4 9 0.497107
erect 3 2 0.597915 ageoftheUnitedKingdom 6 3 0.487303
borough 3 3 0.596054 Presidential 3 2 0.485317
encompasses 5 2 0.592084 Landmark 3 2 0.484965
northernmost 3 2 0.591607 Alistair 4 2 0.484930
Madras 3 2 0.590394 Tauri 3 8 0.482449
hull 3 2 0.586968 2km 4 2 0.479984
iron 2 2 0.586959 20th-century 7 3 0.475703
Galaxy 3 2 0.585879 EastBay 3 2 0.475156
beganoperations 3 2 0.584680 gamegoesinextratime,ifthescored 10 2 0.472323
Redding 3 2 0.584244 SãoPaulo 3 2 0.470874
gloss 3 2 0.576740 AtlanticCity 3 2 0.470726
cello 3 2 0.573732 Chaluk 3 2 0.467165
Gators 3 5 0.573675 FrankLloyd 3 2 0.462585
senator 3 2 0.572947 mayreferto: 6 4 0.462234
restructuring 4 2 0.570552 goldmedalists 4 2 0.458494
supervised 3 3 0.570421 ,2ndBaron 6 2 0.456996
Mediterranean 4 2 0.567790 people) 4 4 0.454926
Madera 3 2 0.567563 seriesaired 4 2 0.453057
sequel 3 2 0.563626 Srib 3 2 0.451708
scarp 3 3 0.561548 withblackish 4 2 0.450033
Sout 3 2 0.560640 WorldCupplayers 4 2 0.448979
SouthDivision 3 2 0.558720 mainrole 3 2 0.448569
rectangular 3 2 0.557339 Bos 4 2 0.448425
Danny 3 2 0.556836 Asenath 4 2 0.448259
Examiner 4 2 0.555797 RoyalNavy 3 3 0.445617
Kuwait 4 4 0.554636 2. Bundesligaplayers 7 2 0.445210
Bogue 3 6 0.552219 Externallinks 3 69 0.444921
Lancaster 3 3 0.552166 anunincorpor 6 2 0.443527
Leuven 4 3 0.548806 Gast 2 4 0.437695
thePark 3 2 0.548687 Pfor 3 2 0.432194
firstBaron 3 2 0.547447 ElisiodeMed 5 2 0.431518
fights 3 2 0.547171 "(2007)"Jad 12 2 0.429412
Carpio 3 2 0.547116 Elkh 3 2 0.428984
CzechRepublic 3 2 0.546651 Früh 3 2 0.427781
Survive 4 2 0.546255 orderoftheNK 5 2 0.424037
Table3:Llama-2-7bWikipediaresults(1808sequences Table4: Llama-3-8bWikipediaresults(892sequences
total).nisthenumberoftokensinthesequence,and‘ct’12 total).nisthenumberoftokensinthesequence,and‘ct’
representsoccurrencesofthissegment. ψisaveraged representsoccurrencesofthissegment. ψisaveraged
overalloccurrences. overalloccurrences.TokenSequence n ct ψ TokenSequence n ct ψ
lowercase 3 2 0.736012 </td>\n<td> 9 2 0.627583
storm 2 4 0.716379 {d}x 5 3 0.599395
excursion 4 2 0.713134 *\n 4 3 0.587016
====... (72‘equals’signs) 8 2 0.712982 _{n=1}{ˆ\in 7 4 0.585434
Mom 3 2 0.706778 </td>\n<td 8 2 0.573310
acre 3 2 0.629213 -2-2007-061 12 3 0.551581
Subject 3 2 0.607172 reticulum 4 3 0.549337
ninth 3 2 0.606669 INSURANCE 5 2 0.548263
processingelements 3 2 0.599549 32;\ninternalstatic 8 2 0.547893
CVC 3 2 0.596735 ;\ninternalstatic 6 9 0.540374
VPN 3 3 0.596052 : At 4 2 0.538609
Regul 3 2 0.591968 (2,9,’ 6 4 0.537495
bore 2 2 0.590212 Respondent 4 2 0.534509
$\dot{G 5 2 0.589714 \t\t}\n\n\t 7 3 0.530669
Rates 3 2 0.589637 (3,0,’ 6 4 0.529493
INSURANCE 5 2 0.584323 _{n-1}\ar 7 2 0.527303
Commercial 4 2 0.581543 thankyouforyourunderstanding 6 2 0.513979
Barney 3 3 0.574872 hydroxyl 4 2 0.510059
PTA 3 2 0.571932 >\n*/\private$ 9 2 0.510054
penetrated 4 2 0.570164 inmukaan 5 2 0.506333
MG 3 2 0.569830 {w}{ˆB}_{ 6 2 0.505970
Leigh 3 2 0.567894 /2\Z 5 2 0.501998
jail 3 3 0.567225 ’);\nINSERTINTO 6 10 0.501055
TNS 3 2 0.567003 7-f131 7 2 0.496881
peptides 4 2 0.565775 0,1L> 8 2 0.495809
JohnArena 3 2 0.565648 /0S 5 2 0.492042
Disease 4 2 0.564662 5Audi 4 2 0.491043
welfare 4 4 0.564364 allthatapply 4 3 0.490469
wildtype 3 2 0.560699 ": true,\n 6 2 0.486807
uws 3 3 0.557799 4,\n 5 2 0.485315
ongrel 4 3 0.554208 toasDSP 5 2 0.484967
liquidcry 3 3 0.553408 **B**]{}\ 6 2 0.483484
princess 3 2 0.551672 ;\ninternal 5 3 0.479777
Denmark 3 2 0.548702 100%used 6 2 0.475673
birthday 3 2 0.548504 ","x": 5 3 0.474701
atedmes 4 2 0.548171 2.7 4 2 0.473720
"ENOENT 5 2 0.547169 </td>\n 6 2 0.473578
third-party 4 2 0.546949 "code=" 4 4 0.473514
aliens 3 2 0.546507 e2d-d 6 2 0.473418
Durban 3 4 0.545848 isunderconversion 4 5 0.473355
Bouncy 4 3 0.545826 {int|sys 5 3 0.471213
CHO 3 2 0.542762 ();\n}\n\nprivatebooleanisAny 12 2 0.470941
unjust 3 2 0.538813 (2,8,’ 6 4 0.470214
thesemotivational 4 3 0.537485 trachea 4 2 0.469154
DLS 3 4 0.535933 useinanautomobile 6 2 0.467788
\n& 3 2 0.534510 atorg.apache.c 7 5 0.467637
uneven 3 2 0.533137 worldaroundus 4 2 0.464469
watt 3 2 0.532243 2\left(1+x 8 2 0.463555
’She 3 2 0.531300 orCommodore 5 3 0.463106
HP 3 3 0.529555 11-117 7 2 0.459824
Table5:Llama-2-7bPileresults(1658sequencestotal). Table6: Llama-3-8bPileresults(819sequencestotal).
n is the number of tokens in the sequence, and ‘ct’13 n is the number of tokens in the sequence, and ‘ct’
representsoccurrencesofthissegment. ψisaveraged representsoccurrencesofthissegment. ψisaveraged
overalloccurrences. overalloccurrences.