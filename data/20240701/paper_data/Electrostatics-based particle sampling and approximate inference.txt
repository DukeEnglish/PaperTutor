Electrostatics-based particle sampling and
approximate inference
Yongchao Huang ∗
May 2024
Abstract
A new particle-based sampling and approximate inference method,
based on electrostatics and Newton mechanics principles, is introduced
with theoretical ground, algorithm design and experimental validation.
This method simulates an interacting particle system (IPS) where parti-
cles, i.e. the freely-moving negative charges and spatially-fixed positive
charges with magnitudes proportional to the target distribution, inter-
act with each other via attraction and repulsion induced by the resulting
electric fields described by Poisson’s equation. The IPS evolves towards
asteady-statewherethedistributionofnegativechargesconformstothe
target distribution. This physics-inspired method offers deterministic,
gradient-free sampling and inference, achieving comparable performance
asotherparticle-basedandMCMCmethodsinbenchmarktasksofinfer-
ringcomplexdensities,Bayesianlogisticregressionanddynamicalsystem
identification. A discrete-time, discrete-space algorithmic design, readily
extendable to continuous time and space, is provided for usage in more
generalinferenceproblemsoccurringinprobabilisticmachinelearningsce-
narios such as Bayesian inference, generative modelling, and beyond.
Keywords— Electrostatics; interacting particle system; sampling; approximate
inference.
1 Introduction
Probabilisticapproaches[19,31]areusedinmanylearninganddecision-makingtasks,
e.g. Bayesian inference[62, 16] and generative modelling[42, 8], and many of them
involve estimating the distribution of some quantities of interest. For example, gen-
erative adversarial network (GAN[20]), variational autoencoder (VAE[33]), and more
recently diffusion models[58, 25], either explicitly or implicitly learn some distribu-
tionsofunderlyingdataorvariables. However,inmanycasesthetargetdensityisnot
straightforwardlyapproachable,e.g. theposteriorinaBayesianmodelcanbeanalyt-
icallyintractable[43]duetoe.g. complex,high-dimensionalintegration1,unavailable
∗Authoremail: yongchao.huang@abdn.ac.uk
1Bayesian neural networks (BNNs [18]), for example, issue high-dimensional parameters
whoseposteriordistributionsareingeneralintractable.
1
4202
nuJ
82
]IA.sc[
1v44002.6042:viXralikelihoods or large data sets [39]. In such cases, one may resort to approximate in-
ference methods such as Markov chain Monte Carlo (MCMC[37, 38]) or variational
inference(VI)methods[2,14]. Thesemethodsperforminferenceviasamplingoropti-
misation routines.
Sampling from a prescribed distribution, either fully or partially known, is not
easy. Ifweknowthefullprobabilitydensityfunction(pdf),severalstatisticalmethods
are available, varying in the resulting sample quality though, for producing (quasi)
samples. These include inverse transform sampling [9], Box-Muller transform [4],
Latin hypercube sampling (LHS) [40], rejection sampling [45], importance sampling
[32], Quasi-Monte Carlo Methods (QMC) [46], etc. The most flexible approaches,
particularly for sampling from partially known densities, are based on Markov chains
[13]. Forexample,MCMCmethodssuchasMetropolis–Hastings(MH)sampling[41],
Gibbs sampling [17], Langevin Monte Carlo (LMC) [53], Hamiltonian Monte Carlo
(HMC)[11],slicesampling[44],etc. VImethodssuchasmean-fieldVI[31],stochastic
VI [27], black-box VI [49], variational autoencoders (VAEs) [33], VI with normalizing
flow[50],byitsnaturearedesignedforinferenceandwidelyusedinmanyapplications
[31, 54]; some particle-based VI (ParVI) methods such as Stein variational gradient
descent (SVGD) [36], Sequential Monte Carlo (SMC) [10], particle-based energetic
VI (EVI) [61], etc, also generate samples for inference. MCMC and VI methods
together form the paradigm of approximate inference in the Bayesian context; they
differ in that, MCMC draws and uses samples for inference while VI in general casts
the inference problem into optimisation. Of these sampling algorithms, some utilise
the original pdf or up to a constant (i.e. gradient-free), e.g. slice sampling and
the MH method, while others, e.g. HMC, LMC and most VI methods, utilise the
gradient information to guide sampling (i.e. gradient-based). Using only gradients
enablessamplingandinferencefromanun-normalised,intractabledensitieswhichare
frequentlyencounteredinBayesianinference;however,italsoincreasescomputational
burden.
WhileMCMCisflexible,canhandlemulti-modaldistributionsandprovidesexact
samplesinthelimit,itiscomputationallyintensiveandconvergeslowlyduetounde-
sirableautocorrelationbetweensamples[34],theymayalsorequirecarefultuningand
can suffer from convergence issues. It is often impractical for very large datasets or
models with complex structures. VI is deterministic, faster and more scalable, but it
canstrugglewithoptimizationchallengesandintroducesapproximationbias,andmay
be limited by the expressiveness of the variational family. Both methods, if not all,
requireestimatingthegradientsforsamplingoroptimisation,whichcanbeexpensive.
Stochastic gradient estimation can be used but extra care should be taken to reduce
variance.
ParVI methods enjoy the key strengths of sampling and VI methods: being fast,
accurate, deterministic2, capable of handling multi-modal and sequential inference.
ParVImethodsevolveafinitesetofparticleswhoseterminalconfigurationuponcon-
vergence approximates the target distribution. The particles are deterministically
updated using gradients of an explicit or implicit variational objective (e.g. KL-
divergence, ELBO, kernelised Stein discrepancy), yielding flexible and accurate ap-
proximation[34]. ComparedtoMCMCandotherVImethods,ParVIoffersappealing
time-accuracy trade-off [56]: the non-parametric nature of ParVI gives greater flex-
ibility than classical VIs, while the interaction between finite particles, along with
deterministic optimisation, makes them more efficient than MCMC methods. While
2Stochasticgradientscanbeintroducedthough,seee.g. [63].
2most currently available ParVI methods require use of gradients, gradient-free ParVI
methods have not been explored much.
Here we introduce a physics-based ParVI method, EParVI, which simulates an
electrostatics-based interacting particle system (IPS) for sampling and approximate
inference. Itisdeterministicandgradient-free,andhastraitsofnon-parametricflexi-
bility,expressiveness,andapproximationaccuracy,aswellassimplicity. Unlikeclassic
VI methods, it only requires the target density to be queryable up to a constant.
Electrostatic halftoning In physics, electrostatics studies the electromagnetic
phenomena and properties of stationary or slow-moving electric charges, i.e. when a
staticequilibrium3 hasbeenestablished. Digitalimagehalftoning,ontheotherhand,
concerns about the distribution of colors [15]. Image halftoning algorithms have been
developed for sampling problems which often occur in rendering, relighting, printing,
objectplacement,andimagevisualisation[57]. Electrostatics-basedmethodsarefirst
introduced to image processing [57, 21] in which electrostatic halftoning, a similar
approachtoours,wasdevisedtosampleimagepixelsbasedonitsgreyvalues. Digital
images are represented by discrete pixels on a computer (though may not be visually
visible)andrenderedtofiniteresolutionintaskssuchasdisplayandprinting[23]. This
requires (re)creating the illusion of e.g. a grey-scale image by distributing the black
dots on a white background (i.e. digital halftoning 4 [23]), or resampling the image
toadapttomachineresolution,whilstyieldingeitherenergeticallyorvisuallyoptimal
results[57]. Toaccuratelyandflexiblyrepresenttheoriginalimage,Schmaltzetal. [57]
proposed the electrostatic halftoning method which follows the physical principles of
electrostaticstoassignandmovenegativecharges,byinteractingwithpositivecharges
representingimageintensities,toformthedistributionofgreyvalues. Thismethodis
based on the intuitive assumption that, in a region of constant intensity, black points
(representedbynegativecharges)inthesampledimagesshouldbeequallydistributed,
while in other regions, through a process of particle interaction and evolution, their
distribution will be proportional to the image grey values.
We therefore note the similarity between (standard and quasi) Monte Carlo sam-
plinganddigitalhalftoning: whilebothmethodsaimtoresembleatargetgeometry5,
theyalsosharethesameproblemofsampleclumpiness[23]andemptyspots[47],i.e.
undesirableclumpingofsamples(dotsinanimage)whichdegradessamplingefficiency
or image quality (uneven dot placement that accompanies random dot distributions
introduces undesirable texture).
2 Related work
Using electrostatics principles to model interactions in a particle system is less ex-
posed but not new. In classic Newton mechanics, the interactions between M bodies
is described the Newton’s third law F = −F ,i ̸= j,i,j ∈ {1,2,...,M}, and each
i j
individual force follows the Newton’s second law F=ma where m is mass and a ac-
celeration. Gravitationalforce,forexample,canbespecifiedaspertheinverse-square
3When there are no moving charges, the particle system is said to have achieved a static
equilibrium,i.e. thevectorsumofallforcesactingupontheparticleiszero.
4Weonlydiscussinthecontextofdigitalhalftoningwhichrendersgray-scaleimages(i.e.
puttingblackdotsonawhitesurface),andmainlyconcernaboutrenderingspeedandquality.
5Inaninferencetask,MCmethodstrytoinferthetargetdensity;digitalhalftoningcreates
apatternofblackdotstoreplicatethedistributionofimageintensity.
3law: F=Gm1m2 where r is the distance between the two interacting objects, and G
r2
the gravitational constant. These laws can be derived in a variational form via the
energyperspective,i.e. theMaxwellequations(asetofcoupledPDEsdescribingelec-
tromagneticlaws),ortheGausslaw(whichinelectrostaticsrelatesthedistributionof
electric charge to the resulting electric field), the Laplace equation (see Appendix.A)
and the Poisson’s equation (generalisation of Laplace’s equation) in more specific do-
mains such as mathematics and physics. Regarding a set of charges as particles and
modelling their interactions using already-known electrostatic principles requires an
alignmentbetweenthethebehaviorsoftheparticlesystemweaimtomodelandthat
of the charges. In particle-based variational inference (ParVI), the SVGD method
[36] for example, particles interact and dynamically move towards forming the target
geometry; in the charge system, positive and negative charges attract and repel each
other and achieve certain distribution at equilibrium. Although we don’t intend to
binarily classify particles in ParVI (which is essential in electrostatics), the similarity
between both inspires the design of new algorithms at their intersection, by taking
advantages of both worlds.
Astraightforwardapplicationoftheinteractingelectrostaticparticlesystememerged
indigitalimageprocessing[47,23,57]. Inspiredbyprinciplesofelectrostatics,Schmaltz
etal. [57]proposedtheelectrostatic halftoning methodforimagedithering,stippling,
screening and sampling. This physics-based image processing method is simple, in-
tuitive and flexible: inkblots are modelled as small, massless particles with negative
unit charge, while the underlying image is modelled as a positive charge density, the
attracting forces attract negative charges move proportionally towards dark image
regions. Eventually,theparticlesystemreachesanequilibriumstatethatformsanac-
curate halftonerepresentationof the originalimage. This method can dealwith both
continuousanddiscretespaces,andworkswellonbothcolorandgray-scaleimages. It
also enables arbitrary number of additional samples to be added to an existing set of
samples. However, their vanilla implementation has a runtime complexity of O(M2)
in the number of particles M, making it impractical for real-world applications (e.g.
high-resolution image halftoning). Gwosdek et al. [21] proposed a GPU-parallel algo-
rithm to improve the computational efficiency of the electrostatic halftoning method.
Theysimplifiedthepairwiseforcecalculationbyevaluatingthenear-fieldandfar-field
interactions using different strategies, e.g. a novel nearest-neighbour identification
scheme was designed, as well as a GPU-based, non-equispaced fast Fourier transform
(NFFT)algorithmdevisedforevaluatinginteractionsbetweendistantparticles,which
reduces the runtime complexity to O(MlogM).
Albeit similarities exist between MC sampling and halftoning patterns, the inter-
sectionbetweenMCandhalftoningtechniqueshasnotreceivedmuchattentioninthe
past [47, 23]. Ohbuchi et al. [47] applied the QMC method, which employs deter-
ministic low-discrepancy sequences (LDSs, a screen pixel sampling method), to solve
theglobalilluminationproblemin3Dcomputergraphics. TheyusedQMCsequences
to improve rendering of surfaces and shadows in a 3D scene, and showed that, QMC
integral with LDSs converges significantly faster than MC integral and yielded more
accurate results. This QMC-LDS method also allows for incremental sampling.
Inspiredbyhalftoning,Hanson[23]designedaQuasi-MonteCarlo(QMC)method
forgeneratingrandompointssetsforevaluating2Dintegrals,whichreducesthenum-
ber of function evaluations and yields more accurate estimates than standard QMC
methods. Although computation is cheap, however, this method is inapplicable to
dimensions larger than four or five as it relies on the calculation of the dimension-
restricted mean-squared error. This haltoning-inspired QMC method can also be ap-
4plicable to sensitivity analysis of computer models. They also proposed a potential-
field approach, which considers the repulsion between particles as well as between
particles and the boundaries, to obtain 2D random, quasi-uniform integration points.
3 Main contributions
We re-invent the idea of electrostatic halftoning [57] for modelling the interacting
particlesystem(IPS),andapplyittosampleandinferprobabilitydensities. Although
thephysicsissimple,andtheideaofmodellingIPSusingelectrostaticsalreadyexists,
this work is novel in following three aspects: (1) it is for the first time principles
of electrostatics and Newton mechanics are introduced to variational inference; (2)
simplified IPS modelling; (3) we extend its theoretical applicability into arbitrary
dimensions. The repulsion-attraction framework is different from previous work (e.g.
[57]) in that, we generalise the detailed physics to any dimension, and it is more
flexible to set the parameters, e.g. charge magnitudes with an annealing scheme, by
relaxingtheequilibriumconditionforacceleratingconvergence. Wealsointroducetwo
new particle updating rules, rather than assuming linear relation between force and
displacement,foryieldingmoreaccurateapproximation. Othertrickssuchasparticle
filtering, noise perturbation and probabilistic move are also introduced.
The paper is organised as follows: Section.4 illustrates the methodology; exper-
imental validation follows in Section.5; discussion, conclusion and future work are
presented in Section.6 and Section.7. Interested readers are referred to Appendicies
for more details.
4 Electrostatics-based sampling
Thebasicideaissimpleandrootedinelementaryelectrostatics: ifweplaceaelectric
charge, either negative or positive, in a free space without friction 6, it generates
an electric field which induces attracting or repulsive forces on other charges. If a
set of charges of equal value and same sign, all negative for example, are placed, at
steady-state these charges will distribute homogeneously and pairwise distances are
maximized by repulsion. This uniform density generation motivates the design of
MC-style integration algorithms [23], and forms a basic assumption for electrostatic
halftoning [57]. If we further place a set of positive charges at fixed locations (e.g.
evenly-spacedgrids),withpre-assignedmagnitudesspecified(e.g. proportionalto)by
adesireddistribution(e.g. distributionofgreyvaluesinanimage)overthespace,the
initial steady state breaks and a new equilibrium will be established due to repulsion
and attraction. Distribution of the negative charges at the new equilibrium, as we
shall see later, is exactly the desired distribution. Based on this mechanism, we can
assignasetofpositivechargesatfixedlocationswithdensityproportionaltoatarget
distribution, and evolve a set of free, negative charges to form the target geometry.
The new steady-state distribution of the negative charges is termed the energetically
idealparticledistribution [57]. Atthenewequilibrium,thesenegativechargesserveas
samplesdrawnfromthetargetdistributionandcanbeusedtoproduce,e.g. summary
statistics and predictive uncertainties. See Fig.1 for an illustration.
6Werestrictthediscussionwithinafreespace(i.e. avacuum),whichsimplifiesthephysics
withoutlosinggenerality.
5Figure 1: Illustrative diagram of particle distributions. Red darkness indicates
the density value of two-Gaussian mixture, arrows denote example forces. Pos-
itive charges are fixed at grid points with magnitude proportional to density
value, negative charges can move freely.
The central concept used in our electrostatics-based particle sampling and infer-
encealgorithmistheinteractionbetweencharges(particles)whichisgovernedbythe
interactive forces. In the following, we first describe the design principles for particle
interactions, i.e. the designation of repulsive and attracting forces. Based on these
design, we devise an algorithm for simulating this evolving particle system towards
the target distribution. Some background knowledge, along with the detailed deriva-
tion process and primary results, e.g. 1D, 2D and 3D electric fields and forces, are
presented in Appendix.A.
Repulsion Tomathematicallyformalisethisidea,wefirstdefinethesetofcharges
(interchangeably used with particles) {q |i = 1,2,...,Mneg}, where Mneg it the car-
i
dinality of this set, in d-dimensional space, and each is associated with position x .
i
Considerasinglepointchargeq ,letE betheelectricfieldinducedbyq . Inelectro-
i qi i
statics, a electric field E is proportional to the electric displacement field and equals
the negative gradient of the electric potential φ; the divergence of E, which implies
theinfinitesimalelectricflux,islinkedtothedistributionofasourceviathePoisson’s
equation for electrostatics (see Eq.18, Eq.22, Eq.23 in Appendix.A). We assume no
external source or sink in this charge particle system, which simplifies the Poisson’s
equation to Laplace’s equation; solving Laplace’s equation gives the potential field
φ, and therefore the electric field E and force F induced by a point charge (see Ap-
pendix.A for details). In d-dimensional space, the electric field E (r), at a distance
qi
of r from a inducing point charge q , is derived as:
i
q Γ(d)
E (r)= i 2 e (cc.Eq56)
i 2πd/2ε rd−1
0
in which r is the distance from the point charge q , ε is the the vacuum permittivity
i 0
constant,Γ(·)theGamma function. eisanunit vector indicatingthedirectionofthe
field.
Imagine we have another negative, point charge q locating at a distance of r
j i,j
fromq . Todistinguish,wecallq thesource point andq thefield point. Theelectric
i i j
force exerted by q on q , as per Coulomb’s law, is:
i j
q q Γ(d)
Frep(r )=q E (r )= i j 2 e (cc.Eq57)
i→j i,j j i i,j 2πd/2ε rd−1 i→j
0 i,j
wheretheunitvectore =(x −x )/r givestheout-pointingdirection. Thearrow
i→j j i i,j
6sign above implies ’exerting effect on’, read from the left object to the right object.
According to Newton’s third law, if we treat q as a source point and q a field point,
j i
q experiences the same amount of force with reverse direction e =−e .
i j→i i→j
Now we look at the particle set {q |i=1,2,...,Mneg}, and consider the combined
i
repulsiveforcethatparticlej receivesfromallotherparticles,whichcanbecalculated
as:
Frep(x )=
M (cid:88)neg q iq jΓ(d 2)
e =
M (cid:88)neg q iq jΓ(d 2) x j−x
i (Eq57b)
j j
i=1,i̸=j
2πd/2ε 0r id ,− j1 i→j
i=1,i̸=j
2πd/2ε 0 ∥x j−x i∥d 2/2
where∥·∥ denotestheL -normofavector. Whencharges(particles)achievesteady-
2 2
state with uniform distribution, for each particle j ∈ {1,2,...,Mneg}, the combined
repulsive force Frep(x ) becomes zero, as all induced electric fields acting on the par-
j j
ticle cancel.
Onecanconvenientlyre-writethefirsthalfofEq.Eq57binmatrixform,bydenot-
ing the row vector q
j
=[q j,q j,··· ,q j]T, column vector q
1:Mneg
=[q 1,q 2,··· ,q Mneg],
diagonal matrix Λ=diag[1/rd−1,1/rd−1,...,1/rd−1], as follows:
1,j 2,j i,j
Γ(d)
Frep(x )= 2 q Λq (Eq57c)
j j 2πd/2ε j 1:Mneg
0
in which Λ can be viewed as some kind of isotropic covariance matrix produced by a
kernel(similartoaRBFkernelbutnotexactly),witheachdiagonalelementmeasuring
a pairwise correlation or inverse-distance between the two point charges q and q .
j i
Attraction We have devised the repulsive forces between particles of the same
sign(specifically,allnegativeinoursettings),which,underarbitraryinitialpositions,
should generate a uniform distribution of the particles over the free space at equilib-
rium. Toproduceanuneventargetdistributionp(x),weneedtopullparticlestowards
(neighbourhood) high density region using attracting forces. This can be achieved by
introducing a continuous field of positive charges whose magnitudes distribute as, or
proportionalto,thetargetdensity. Theinducednewelectricfieldsshallattractnega-
tive charges towards high probability (indicated by large positive charge magnitudes)
regions. Representingacontinuousdensityoverthespacerequiresaninfinitenumber
of positive charges to be allocated; realistically, we can mesh the space into equal-
distance grids, and place large Mpos positive charges fixed at these grid points. As
onecanimagine,thenumberofpositivecharges,thereforethemeshgranularity,shall
impact the smoothness of the inferred density.
As such, the overall attracting force, resulted from a positive point charge and
acting on a negative charge q , is [57]:
j
Fa jttr(x j)=− i′=M (cid:88) 1p ,io ′s
̸=j
q 2p π(x d/i′ 2) εq 0j rΓ id ′( − ,jd 2 1) e i′→j =− i′=M (cid:88) 1p ,io ′s
̸=j
qp( 2x πi′ d) /q 2j εΓ 0(d 2) ∥x jx j −− x ix ′∥i′ d 2/2
(Eq57d)
wherewehavesetthepositivechargeq =qp(x )withqbeingaconstantscalingfactor
j j
specifying the relative strength of attracting forces. A negative sign is put ahead as
it’s attracting force.
Ifweignorethecontributingforcesfromthoselocatedfaraway,inregionswithcon-
stant probability (and therefore uniform positive charge magnitudes), the attracting
7forcescancelandrepulsionstillyieldstheenergeticallyidealpositionswhichmaximize
pairwise distances. In regions with uneven probability distribution, attracting force
dominates and binds particles onto locations with higher probability. If mesh grid is
settobeinfinitelysmall,Mposgrowstoinfinitelylargeandthedistributionofpositive
charge magnitude becomes continuous, Eq.Eq57d then becomes an integral.
Evolvingtheparticlesystem Summationoftherepulsiveandattractingforces
gives the overall force F = Frep +Fattr, acting on a single free-moving negative
j j j
charge j; this combined force drives the negative charge to move as per Newton’s
second law. At equilibrium (steady-state), the repulsive force equals the attractive
force (in magnitude) and F vanishes. As we intend to evolve the particle system
j
overtime,weintroducethetimedimensionanddenotethetime-dependentquantities
withsuperscriptt. Further,avoidingcomplexphysics,weassumevariationofparticle
displacementst isproportionaltothecorrespondingoverallactingforceFt,i.e. δst ∝
qj j j
Ft, which gives the following position updating rule [57] for the particle j:
j
xt+1 =xt +τFt
j j j
=xt +τ[ M (cid:88)neg q iq jΓ(d 2) x j−x i − M (cid:88)pos qp(x i′)q jΓ(d 2) x j−x i′ ]
j i=1,i̸=j 2πd/2ε 0 ∥x j−x i∥d 2/2 i′=1,i′̸=j 2πd/2ε 0 ∥x j−x i′∥d 2/2
(1)
where τ is a constant similar to the compliance constant in a material 7; it is sug-
gested to be small to well approximate a linear relation between force and displace-
ment. Note the index i ∈ {1,2,...Mneg} indexes the negative charge group, while
i′ ∈ {1,2,...Mpos} is for the positive charge group, and if computation allows, the
positive charge group can be made continuous over the space.
Assuming a linear relation between force and displacement violates Newton’s sec-
ondlawF=mawhereFistheoverallactingforce,mthemassandatheacceleration;
however, this linearisation simplifies and accelerates calculation. Theoretically, to re-
cover Newton’s second law, the proportionality constant τ should be non-linear and
time-variant. Here we take another approach to (approximately) preserve this physi-
callawusingfinitedifference. Weknowthefollowingcentraldifferenceapproximation
holds (we drop the index j for now):
d2x xt+1−2xt+xt−1
a= ≈ (2)
dt2 ∆t2
asperNewton’ssecondlaw,wehavea=F/m,andifwesetm=1,weobtainF=a.
We directly have:
xt+1 ≈a∆t2+2xt −xt−1 =F ∆t2+2xt −xt−1 =xt +F ∆t2+∆xt−1 (1b)
j j j j j j j j j
with ∆xt = F ∆t2+∆xt−1. We can also design a damped version with a discount
j j j
factor τ′:
xt+1 =xt +τ′∆xt =xt +τ′[Ft∆t2+∆xt−1] (1c)
j j j j j j
7The compliance is the reciprocal of elastic modulus - both describe the stress-strain be-
haviourofamaterial.
8where Ft remains the same as in Eq.1. Both Eq.1b and Eq.1c require ∆t2 to be
j
sufficientlysmalltobeaccurate. Thenewlyintroducedhyper-parameter∆t2 balances
therelativeimportanceofthecontributionfromcurrentstepforceFt andthehistorical
j
displacement ∆xt−1 carried over from previous step. This momentum-like updates,
j
hopefully, will give higher fidelity in suggesting next particle position. Note the time
discretisation size ∆t, along with the step size τ′, plays a role in determining the
accuracy and speed of convergence.
Nowwehave3rules,i.e. Eq.1,Eq.1bandEq.1c,forupdatingthenegativecharge
positionsinanyiteration. Werefertothelinearmappingbetweenforceanddisplace-
ment in Eq.1 as the simple Euler method, the non-linear approximations in Eq.1b as
the Verlet integration or leapfrog method, and Eq.1c as the damped Verlet integration
method. Simple Euler method may require very small time steps to converge accu-
rately, which leads to slow simulation. The Verlet integration method is commonly
used in molecular dynamics. It takes into account the previous position, potentially
leading to smoother trajectories 8; however, it requires more storage as well. The
damped Verlet integration provides additional damping but also has more parameter
to tune.
Eq.1s define a set of discrete-time, discrete-space formulas for updating particle
positions. In order to estimate next particle position, we only need to record the
historical positions {xt,t = 1,2,...,k} and calculate the current time overall force
Ft. Wekeepupdatingparticles’positionsuntilconverge(indicatedbyeitherstaticor
j
small movements). When attaining equilibrium, we have Ft = 0 and ∆xt−1 = 0 for
j j
all j ∈{1,2,...,Mneg}. The charge system then becomes electrically neutral. As the
number of negative particles attracted to a region is proportional to the magnitudes
of nearby positive charges, particle clustering at the steady state shall align with
the distribution of the magnitudes and therefore the target distribution. A vanilla
implementation of the EParVI algorithm is described in Algo.1.
The EParVI algorithm is particularly advantageous for sampling and inferring a
partiallyknowndensityp(x)=p′(x)/Z, e.g. anenergymodelp(x)=e−f(x)/Z where
thenormalisingconstant9Z isunknown: wecanusethisalgorithmtoeffectivelydraw
samples from p(x) and make inference about its statistics (e.g. multi-modal means
and variances), once the numerator is queryable; we only need to set the positive
charge magnitudes qp(x) proportional to the queryable term and possibly normalise
them. Therefore,inthiscontextwedon’tdistinguishthecasewherethetargetdensity
is known up to a constant - we use the same, overloaded notation p(x) for denoting
either the queryable term p′(x) or the full density.
In our experiments, for simplicity, we use equally-charged unit negative charges,
and make the maximum magnitude for positive charges to be unity as well, i.e. q =
i
q = 1. Note in this setting, the absolute value of q or q is not relevant; they are
i
absorbedintothehyper-parametersτ and/or∆t2. However,onedoeshavetheoption
tosetthepositivechargeconstantq tobemultiplesofthenegativechargemagnitude
q toaddresstherelativeimportanceofattractingforce. Also,iftheforceistoolarge,
i
thenegativechargecanjumptoandgettrappedinalow-densityregion(asdiscussed
in Section.6), this can be avoided by normalising the force in each iteration. Besides,
extra force terms can be added to Ft, e.g. a dragging force controlling the negative
j
8It also provides better energy conservation and handles oscillatory systems more effec-
tively.
9Z is also called evidence or marginal distribution in Bayesian inference. It is generally
intractableinhigh-dimensionsduetocomplexintegral.
9Algorithm 1: EParVI
• Inputs: a queryable target density p(x), particle dimension m, initial
proposal distribution p0(x), total number of iterations T, total number
of negative Mneg and positive Mpos =Mm charges (where M is the
number of positive charges along each dimension), Euler proportionality
constant τ, max positive charge magnitude q.
• Outputs: Particles {x ∈Rm}Mneg whose empirical distribution
i i=1
approximates the target density p(x).
1. Initialise particles.
(1) Mesh the space into Mpos =Mm equidistant grids. Query (and
optionally normalise) the density values p(x ) at all grid points
i
x ,i∈{1,2,...,Mpos}.
i
(2) Assign a positive charge with magnitude qp(x ) to each grid point x .
i i
(2) Draw Mneg m-dimensional negative charges x0 ={x0}Mneg from the
j j=1
initial proposal distribution p0(x).
2. Update negative charge positions.
For each iteration t=0,1,2,...,T −1, repeat until converge:
(1) Calculate the overall force acting on each negative charge j (see
Eq.Eq57b and Eq.Eq57d): Ft =Frep,t+Fattr,t
j j j
(2) Update negative charge positions (Eq.1 as an example): xt+1 =xt +τFt
j j j
3. Return the final configuration of negative charges {xT}Mneg and
j j=1
their histogram and/or KDE estimate for each dimension.
charge to move within certain grids [57], which is similar to regularisation used in
manyobjectives. Also,themovext+1 =xt+τ∆xt canbemadeprobabilistically,i.e.
j j j
we can insert an MH step to assess if this move is worthwhile. These extensions are
discussed in Section.6.
Computational complexity EParVI avoids the expensive computation of gra-
dients;however,italsoperformsmanytargetfunctionevaluations. Magnitudesofthe
positivechargesatgridpointscanbepre-calculatedandcachedforuseinalliterations,
then in each iteration, we only calculate the pairwise distance between negative and
positivechargesandquerythe correspondingmagnitudesof positivecharges. In each
iteration, computing the pairwise forces takes order O(m(Mneg +Mpos)2) FLOPs,
where the number of positive charges Mpos = Mm, with M being the number of
equidistantgridpointsineachdimension,growsexponentiallywiththedimensionm.
TheoverallcomputationalcostoverT iterationsisthereforeO(Tm(Mneg+Mpos)2).
Majorcomputationaleffortscanbesavedbyawiseroutine,asproposedin[21],which
reduces the efforts for computing the pairwise forces to (Mneg +Mpos)log(Mneg +
Mpos). Ofcourse,onecanreducesthetotalnumberofpositivechargesMposbydesign-
ing an irregular grid for assigning the positive charges. As discussed in Appendix.B,
once can also save some computational efforts by performing neighbourhood search
which identifies near-field particle sets via e.g. k-d-tree search, drops contributions
10from distant particles and reduces the computation, as well as using neural approxi-
mation of the force field.
5 Experiments
5.1 Toy examples via EParVI
Gaussiandensities Twoexamples,aunimodalGaussianandabimodalGaussian
mixture (mixing proportions 0.7/0.3), are presented in Fig.2. Fig.2(a) starts from a
uniformpriorwhichpartiallyoverlapswiththetarget,convergesfastandtheinferred
shape(thirdfromleft)alignswellwithgroundtruth(pinkcontours). Fig.2(b)identifies
thetwomodesandtheirmixingproportionsverywell. UnlikemanyMCMCalgorithms
whichencourageparticlestomovefromlowtohighdensityregions(eitherfollowingthe
steepestgradientdirectionoraprobabilisticMHstep),weobservethat,theinteracting
particles following the deterministic EParVI algorithm don’t restrict themselves to
follow the gradient.
Figure 2: Two toy 2D Gaussian densities. Upper: unimodal, lower: bimodal.
Left to right: iteration 0, 5, 60 and selected particle trajectories (dot: start,
square: end). Pinkcontoursaregroundtruthdensity;bluedotsdenotenegative
charges, red dots denote positive charges. See Appendix.D for details.
Other densities We also test the EParVI algorithm on three other toy densities,
i.e. a moon-shaped density, double-banana, a wave density, which are commonly
used in other VI benchmark tests [22, 6, 35, 61]. The results are shown in Fig.3.
Withdifferentinitialisation,theEParVIalgorithmisabletotransportparticlestothe
target regions while maintaining diversity. Dispersion of particles is secured by the
repulsive forces, which helps prevent model collapse.
5.2 Neal’s funnel
Neal’s funnel distribution, as written in Eq.3b, has been challenging to infer due
to its highly anisotropic shape and exponentially increasing variance - the funnel’s
neck is particularly sharp, making it a benchmark for testing many MCMC and VI
methods[44].
p(x ,x )=p(x |x )p(x ) (3)
1 2 1 2 2
11Figure 3: Three other toy 2D densities. Left to right: iteration 0, 40, 60. See
Appendix.D for details.
where p(x )=N(x |0,σ2),p(x |x )=N(x |0,exp(x /2)). The joint pdf is therefore:
2 2 1 2 1 2
1 (cid:18) x2 (cid:19) 1 (cid:18) x2 (cid:19)
p(v,x)= √ exp − 2 · exp − 1 (3b)
2π·σ2 2·σ2 (cid:112) 2πexp(x 2/2) 2exp(x 2/2)
We test 6 methods, i.e. MH, HMC, LMC, SVGD EVI and EParVI, on the 2D
differentiable geometry with σ = 3, compare their inference quality, as measured by
two metrics, i.e. the squared maximum mean discrepancy MMD2 [1, 61] and the
average negative log-likelihood (avg NLL), as well as their runtimes in each iteration.
WhileMHandHMCproducesamplessequentially,Langevin,SVGD,EVIandEParVI
generate samples iteratively and are started with the same initialisation.
Results presented in Fig.4 show that, with a medium number of particles (i.e.
400), MH and LMC are the fastest in terms of speed, SVGD and EParVI are among
the slowest due to large number of iterations between particles. EVI is fastest among
these particle methods. HMC, being fast and accurate, is considered as the golden
methodforsamplingcomplexgeometries[26]; wethereforeusetheHMCsamplesasa
reference for evaluating MMD2. Key observations are: while all samples show some
similarity to the HMC samples, EParVI samples notably yield the smallest average
NLL,whichimpliestheyarebetterquasisamplesfollowingthetargetdistribution,as
also evidenced by the upper row figures.
5.3 Bayesian Logistic Regression
We also apply the proposed EParVI, along with other 4 counterparts, i.e. logis-
tic regression via maximum likelihood (MLE-LR), HMC, SVGD and EVI, to infer
the coefficients of a low-dimensional Bayesian logistic regression (BLR) model, us-
ing the Iris dataset {(x ,y )}N which has N = 150 observations with binary la-
i i i=1
bels y , and each instance x consists of 4 features. The logistic regression model is
i i
p(y = 1|x ,ω) = 1 . We use standard Gaussian prior for regression coef-
i i 1+exp(−ωTxi)
ficients, i.e. ω ∼N(0,I). The posterior distribution of coefficients ω is then (details
please refer to Appendix.F):
12Figure 4: Inference of the Neal’s funnel using six methods. Upper left to right:
LMC,SVGD,EVI,EParVI,andMHsamples. Lowerlefttoright: corresponding
MMD2, NLL values over runtime and HMC samples. MH and HMC take
seconds to run. See Appendix.E for details.
p(ω|y,X)∝(cid:89)N (cid:20)
1
(cid:21)yi(cid:20)
1−
1
(cid:21)1−yi exp(cid:18)
−
1
ωTω(cid:19)
1+exp(−ωTx ) 1+exp(−ωTx ) 2α
i i
i=1
(4)
which is used as the target for our sampling methods.
The initial and final samples are visualised for the two dimensions (ω ,ω ) in
1 2
Fig.5. All 4 methods yield reasonably well inference quality - all terminal particles
clusteraroundthegroundtruthpointwithsomedispersionrepresentinguncertainties.
The EParVI particles, in particular, have reduced uncertainty as compared to other
methods. A comparison of the statistics are made in Table.1, in which we observe
HMC yields results closest to the MLE-LR estimates. Particle methods give less
accurate estimates yet fast speeds such as SVGD and EVI. EParVI achieves smallest
NLLbutlessimpressivemeanestimates;italsodiffersfromtheHMCsamplesdueto
less iterations and the coarse granularity used in meshing.
Method ωˆ ωˆ ωˆ ωˆ NLL/MMD2 runtime
1 2 3 4
MLE-LR -0.64 1.89 -1.75 -1.67 - <1 second
HMC -0.69 1.98 -1.84 -1.86 - <30 seconds
SVGD -0.76 1.87 -1.74 -1.67 12.83 / 2.41 <100 seconds
EVI -0.78 1.81 -1.63 -1.56 11.39 / 10.14 <100 seconds
EParVI -0.64 1.62 -1.50 -1.42 11.32 / 23.25 ∼104 seconds
Table 1: Comparison of inference methods for the BLR problem. Sample mean
estimates of coefficients are presented for all but MLE-LR. With these inferred
coefficients,thetestaccuracyforallcasesis100%. Theruntimeisestimatedon
Colab TPU v2. NB:theEParVIruntimecanbemassivelyimprovedbycaching
the magnitudes of positive charges for all time steps.
13Figure 5: Inference of BLR model. Upper left to right: HMC and SVGD
samples. Lower left to right: EVI and EParVI samples. MH and HMC take
seconds to run. See Appendix.F for details.
5.4 Inference of the Lotka-Volterra dynamical system
Figure 6: Temporal evolution of the LV system.
HerewelookattheLotka-Volterra(LV)ecologicalsystemwhosedynamicscanbe
represented by the following two ODEs:
dx
=ax−bxy
dt
(5)
dy
=cxy−dy
dt
where x and y are the numbers of hare and lynx pelts. We are interested in inferring
the for inferring the 4-dimensional parameter θ = (a,b,c,d) based on 21 observed
real-world data as plotted in Fig.6. This can be formulated as a Bayesian inference
problem with the posterior:
p(X′,Y′|a,b,c,d)p(a)p(b)p(c)p(d)
p(θ|X′,Y′)=p(a,b,c,d|X′,Y′)= (6)
Z
whereZisthenormalisingconstant. Theobservedtrajectories(X′,Y′)={(x′,y′)}21 ,
i i i=1
and independence of parameters is assumed. The priors are specified as:
14a,d∼U(0.001,1.0), b,c∼U(0.001,0.05) (7)
and likelihood:
logx′(t )∼N(logx(t ),0.252), logy′(t )∼N(logy(t ),0.252) (8)
i i i i
where we have for simplicity assumed known 10, homogeneous state noise level σ =
0.25, and known initial conditions x = 33.956,y = 5.933. Assuming independence
0 0
between the states writes the likelihood as:
N N
p(X′,Y′|a,b,c,d)=p(X′|a,b,c,d)p(Y′|a,b,c,d)=(cid:89) p(x′|a,b,c,d)(cid:89) p(y′|a,b,c,d)
i i
i=1 i=1
=(cid:89)N √1 1 exp[−(logx′ i−logx i)2 ]√1 1 exp[−(logy i′−logy i)2
] (9)
2πσx′ 2σ2 2πσy′ 2σ2
i=1 i i
wherex(t )andy(t )arethesolutionstoEq.5,x′(t )andy′(t )aretheobserveddata
i i i i
in Fig.6.
Substituting the priors (Eq.7) and likelihood (Eq.9) into the posterior (Eq.6), we
obtaintheprobabilisticmodelp(θ|X′,Y′),whichisthetargetofourinferencepractice.
Tomakethelearningmorenumericallystable11,wesetthepositivechargemagnitudes
12 q = qlogp(θ |X′,Y′) where q is a scaling factor which specifies the maximum
j j
magnitude, as shown in Eq.64 (Appendix.G).
Before applying EParVI, we do a quick check of the probabilistic model. We
evaluate p(θ|X′,Y′) over all mesh grid points, and compare its maximum, i.e. the
maximum a posterior (MAP) estimate, with reference values. The MAP estimate is
θˆMAP =argmax p(θ |X′,Y′)≡argmax logp(θ |X′,Y′)=[0.5a 39,0.0b 27,0.0c 24,0.7d 95]
j j j j
which is very close to the ground truth value from literature [5]. This confirms that,
withthecurrentmeshinggranularity,thepeakofthepositivechargemagnitudealigns
with ground truth. The whole approximate landscape of p(θ|X′,Y′) is visualised in
Fig.7, and the marginal distributions in Fig.12. We observe that, the log posterior
surface is relatively flat, which could potentially impact the identifiability of modes.
Then we run 5 methods, i.e. MAP, HMC, LMC, SVGD and EParVI, to infer
the posterior p(θ|X′,Y′), using similar model settings 13, and compare their perfor-
mances. ForEParVI,wefirstsimulatetheLVsystematagridofmeshpoints(Fig.7),
obtaining(X,Y)andsubstituting(X,Y)and(X′,Y′)intothelogposteriortoobtain
10Theseknownvaluesarereferencedfrom[5]
11ThechainproductintheposteriorEq.6leadstosmallabsolutevaluesofthetarget,and
thusrequireslargepositivemagnitudeq.
12Note that, it is crucial for the charge magnitudes qi, where i ∈ {1,2,...Mneg}, and q i′,
where i′ ∈ {1,2,...Mpos}, to remain positive. While we use qi=1.0 in all experiments; the
magnitudesofpositivechargesq i′,however,canbenegativeinscenariose.g. q i′ =logp(x i′).
Therefore,anoffsetismade,i.e. q i′ =logp(x i′)−min[logp(x i′)]toensurepositivity. Further,
atgridpointswhichresultsinnegativevaluesof(x(t),y(t)),wesetq i′=0.
13The same priors are used in all six methods; HMC, however, for convenience of imple-
mentation(mainlyduetothenegativityofsometrajectoryvaluesatimpossibleθ locations,
which results in error when taking log), a different likelihood is used: x′(ti) ∼ N(x(ti),σ2)
andy′(ti)∼N(y(ti),σ2).
15Figure 7: Log posterior contour plots. Left: snapshot at c = 0.024,d = 0.8;
right: snapshot at a=0.55,b=0.028.
the positive charge magnitudes at these mesh points. These magnitudes are cached
for later query. Then we randomly initialise θ = (a,b,c,d), i.e. the coordinates of
negative charges, and calculate the pairwise distances between each negative charge
and all other charges, further the repulsive and attracting forces (by querying the
positive charge magnitudes) acting on each negative charge. The overall force is then
substituted into the updating rule to give the displacement for each negative charge.
WeruntheEParVIalgorithmtillstabilityusing400negativeand640000positivepar-
ticles, the initial and final configurations of the particles, along with their empirical
marginaldistributions,arepresentedinFig.8(moredetailsseeFig.13inAppendix.G).
It is seen that, the inferred geometries for b,c are narrow and peaky, while those for
a,d are flat and widely distributed; final particles sit in the high probability regions
of the posterior, which is consistent with the posterior snapshots in Fig.7 - particles
alongthedimensionsbandcaremoerconcentratedthanindimensionsaandd. The
relatively wide spread of the posterior distributions of a and d may be due to the
fact that [55], realizations of the LV system are more sensitive to the interaction pa-
rameters b and c, while less sensitive to a and b. The posterior-based predictions are
presented in Fig.9, from which we observe loosely good match between the empirical
posteriormode-basedpredictionsandrealtrajectories,withsomeliftsandshifts. The
predictions made using all final particles, which represent uncertainties, exhibit large
variations.
A comparison of the inference results is made in Table.2. We observe that, the
EParVIestimationsareclosetothereferencevalues,particularlyindimensionsband
c. It, however, also occupies the longest runtime. EParVI outperforms SVGD and
LMC; the optimal choice, balancing accuracy and efficiency, however is HMC.
6 Discussions
EParVI is built on the principles that charges repel and attract each other, in re-
gions of uniform density, charges are distributed uniformly; in regions where density
varies, the clustering of negative charges follows the density distribution. The repul-
siveforcesbetweennegativechargesensureparticlediversityandavoidmodecollapse.
These principles ensure the correctness and diversity of the inferred geometry. From
16Figure 8: EPaVI: initial and final particle distributions.
Figure9: EParVIposterior-basedtrajectorypredictions. Left: predictionsmade
using mean posterior, right: predictions made using posterior mode. In both
figures, light blue and pink lines represent the predictions for hare and lynx,
respectively, made by all final particles.
Method\Quantity a b c d runtime(s)1
EParVI(mean/mode) 0.487/0.5490.026/0.026 0.024/0.0250.529/0.645 221400
SVGD (mean/mode) 1.017/1.029 0.076/0.073 0.045/0.032 1.025/0.978 8662
LMC (mean/mode) 0.482/0.556 0.028/0.019 0.021/0.021 0.555/0.651 24178
HMC (mean/mode) 0.566/0.570 0.028/0.0270.023/0.023 0.763/0.759366
MAP (EParVI mesh) 0.539 0.027 0.024 0.795 -
Reference2 0.55 0.028 0.024 0.80 -
1These methods can differ in numbers of samples and iterations, see Appendix.G.
Table 2: Inference results of the LV system coefficients θ = [a,b,c,d] using six
methods.
17an optimisation point of view, each update reduces global force imbalance by reposi-
tioningtheparticlesinaccordancewiththeforcefield[57];fromanenergyperspective,
we are implicitly minimizing the overall electric potential which serves as the varia-
tional objective. Energy is minimized when forces are in equilibrium (i.e. system is
electrically neutral)[59]. More detailed discussions are delegated to Appendix.H.
Computation in high-dimensions EParVI suffers from curse of dimensional-
ity. Thequalityofsamplingandinferenceresultsreliesonthegranularityofmeshing.
Finer mesh grid is needed for complex geometries (e.g. with large curvature). How-
ever,thecomputationalcomplexityincreasesexponentiallywithdimensionm. Efforts
can be saved by, e.g. pre-calculating the probability matrix at grid points, as well as
pairwiseforcesbetweencells,andstoringthemforlaterquery. FastFouriertransform
techniques [21] have also been developed. It would be beneficial to leverage the fast
inferencecapabilityofatrainedneuralnetworktoapproximatetheforcefieldaswell.
Local optima This can lead to ambiguous distribution of the negative charges;
adding perturbations to the particle in each iteration may help, e.g. projecting the
particleontotheclosesthorizontalorverticallineconnectingthegridpoints[57]. We
proposeinjectingsomerandomnoisetothemoveproposal,e.g. everyksteps,toavoid
getting stuck in local optima:
xt+1 =xt +τ′∆xt +ϵt, every k steps (10)
j j j
where ϵt ∼ N(0,σ2). This is equivalent to saying xt+1 ∼ N(xt +τ′∆xt,ϵt). The
j j j j j
proposedmoveτ′∆xt =τFt fortheEulerscheme,and∆xt =F ∆t2+∆xt−1 forthe
j j j j j
Verlet and damped Verlet schemes.
Probabilistic move If, in an iteration, the particle takes a large step and jumps
toalowdensityregion,itmaygettrappedinthatregionduetosmalldragforce(e.g.
theextraregularisationforcesoutweighingtheattractingforce[57]). Wethussuggest
takingaprobabilisticmovetoavoidoccasionallylargemove,andassessthequalityof
each move via an MH step to decide if this move is to be executed:
(cid:40)
xt +τ∆xt, with probability p
xt+1 = j j accept (11)
j xt, else
j
where
p(xt +τ∆xt)
p =min{1, j j } (12)
accept p(xt)
j
inwhichtheratiodoesn’trequireafullyknowdensity. Usingthisacceptance-rejection
criteria, we can then decide if a move proposal τ∆xt should be accepted or not. The
j
useofanMHstep,however,istwo-side: whileitpreventstheparticleofconsideration
from taking dangerous move in the current iteration, it may break the strategy for
promoting global force equilibrium. It also induces extra cost.
EParVI diagnostics First,weneedtoensurepositivityofthechargemagnitudes
q i, where i∈{1,2,...Mneg}, and q i′, where i′ ∈{1,2,...Mpos}. Negative magnitudes
can result, e.g. in scenarios where q i′ = logp(x i′) (e.g. the LV dynamical system
inference problem). Therefore, adjustment (offset) is needed. Further, there may be
18other constraints imposed by the problem itself, e.g. positive trajectories in the LV
dynamical system inference problem, which requires special treatment (e.g. q i′=0)
of the positive charge magnitudes at grid points yielding negative (x(t),y(t)) values.
In each step, we monitor the number of free-moving negative charges which are sit-
ting within the initialisation regime; large number of negative charges falling out of
this region may indicate potentially wrong specification of the probabilistic model, or
erroneous calculation of the positive charge magnitudes (e.g. negative values appear
in magnitudes), or simply the repulsive forces is way bigger than the attracting force
(i.e. too small positive charge magnitudes). Correctness of the probabilistic model
can be checked via other sampling methods such as MCMC, or by comparing the
MAP estimate with point ground truth values (if ground truth values are available).
Theimbalancebetweenrepulsiveandattractingforcescanbecheckedbyrunningthe
simulation for several trial steps, monitoring both values, and increasing/decreasing
the charge magnitudes accordingly.
7 Conclusion and future work
Conclusion We propose a variational sampling and inference method, EParVI,
which adheres to physical laws. EParVI simulates an interacting particle system in
which positive particles are fixed at grid points and negative particles are free to
move. With positive charge magnitudes proportional to the target distribution, neg-
ative particles evolve towards a steady-state approximating the target distribution;
particles repositioning follows a discretised Newton’s second law, and the force acts
in accordance with the electric fields described by the Poisson’s equation. It features
physics-based, gradient-free, deterministic dynamics sampling, with traits of simplic-
ity, non-parametric flexibility, expressiveness and high approximation accuracy; as a
mesh-based method, it also suffers from curse of dimensionality.
ExperimentalresultsconfirmtheeffectivenessofEParVI.Itexhibitssuperiorper-
formance in inferring low-dimensional, complex densities; it also achieves comparable
performance as other MCMC and ParVI methods in real-world inference tasks such
as Bayesian logistic regression and dynamical system identification. This is the first
time an inference problem is fully formulated as a physical simulation process.
Future work There are theoretical aspects which can be explored. For example,
theconvergenceoftheIPStowardsequilibriumwiththeoreticalguarantees. Thiscan
be routed to using the variational potential energy objective. Also, improving the
computational efficiency is crucial for its applicability. Further, this technique can be
potentiallyusedforoptimisation. However,samplingdiffersfromoptimisationinthat,
we can query the target function (i.e. the density, possibly partially known) as many
times as needed; in optimisation, however, one typically avoids to perform functional
evaluations too many times as it’s expensive.
Codeavailability Fullcodesareavailableat: https://github.com/YongchaoHuang/
EParVI
19References
[1] Mikol(cid:32)aj Arbel et al. “Maximum mean discrepancy gradient flow”. In: Ad-
vances in Neural Information Processing Systems. 2019, pp. 6484–6494.
[2] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. “Variational Infer-
ence:AReviewforStatisticians”.In:https://arxiv.org/pdf/1601.00670.pdf
(2016).
[3] S.L.BoreandF.Paesani.“RealisticPhaseDiagramofWaterfrom“First
Principles” Data-Driven Quantum Simulations”. In: Nature Communica-
tions 14.1 (2023), p. 3349.
[4] George E. P. Box and Mervin E. Muller. “A Note on the Generation of
Random Normal Deviates”. In: The Annals of Mathematical Statistics
29.2 (1958), pp. 610–611.
[5] Bob Carpenter. Predator-Prey Population Dynamics: the Lotka-Volterra
modelinStan.2022.url:https://mc-stan.org/users/documentation/
case-studies/lotka-volterra-predator-prey.html.
[6] Changyou Chen et al. A Unified Particle-Optimization Framework for
Scalable Bayesian Sampling. 2018. arXiv: 1805.11659 [stat.ML].
[7] DanielCristofaro-Gardiner.The Laplace and the Poisson equations in the
whole space. Lecture Notes, Carnegie Mellon University. 2017.
[8] Deep Learning - Foundations and Concepts. Springer International Pub-
lishing, 2023. doi: 10.1007/978-3-031-45468-4. url: https://link.
springer.com/book/10.1007/978-3-031-45468-4.
[9] LucDevroye.Non-UniformRandomVariateGeneration.NewYork:Springer-
Verlag, 1986.
[10] Arnaud Doucet, Nando de Freitas, and Neil Gordon. Sequential Monte
Carlo Methods in Practice. New York: Springer, 2001.
[11] Simon Duane et al. “Hybrid Monte Carlo”. In: Physics Letters B 195.2
(1987), pp. 216–222. doi: 10.1016/0370-2693(87)91197-X.
[12] Timothy T. Duignan. “The Potential of Neural Network Potentials”. In:
ACS Physical Chemistry Au 0.0(0),null.doi:10.1021/acsphyschemau.
4c00004. url: https://doi.org/10.1021/acsphyschemau.4c00004.
[13] Joseph J.K. O Ruanaidh; William J. Fitzgerald. “Numerical Bayesian
Methods Applied to Signal Processing”. In: 1st. Springer, 1996, p. 244.
[14] C. W. Fox and S. J. Roberts. “A tutorial on variational Bayesian infer-
ence”. In: Artificial Intelligence Review 38 (2012), pp. 85–95. doi: 10.
1007/s10462-011-9236-8. url: https://doi.org/10.1007/s10462-
011-9236-8.
[15] “Dithering”.In:Encyclopedia of Multimedia.Ed.byBorkoFurht.Boston,
MA: Springer US, 2008, pp. 198–199. isbn: 978-0-387-78414-4. doi: 10.
1007/978-0-387-78414-4_18. url: https://doi.org/10.1007/978-
0-387-78414-4_18.
20[16] Andrew Gelman et al. Bayesian Data Analysis. 3rd ed. New York: Chap-
man and Hall/CRC, 2013, p. 675. doi: 10.1201/b16018. url: https:
//doi.org/10.1201/b16018.
[17] Stuart Geman and Donald Geman. “Stochastic Relaxation, Gibbs Distri-
butions,andtheBayesianRestorationofImages”.In:IEEE Transactions
on Pattern Analysis and Machine Intelligence PAMI-6.6 (1984), pp. 721–
741. doi: 10.1109/TPAMI.1984.4767596.
[18] Zoubin Ghahramani. A history of Bayesian neural networks. http://
bayesiandeeplearning.org/2016/slides/nips16bayesdeep.pdf.2016.
[19] Zoubin Ghahramani. “Probabilistic machine learning and artificial in-
telligence”. In: Nature 521.7553 (2015), pp. 452–459. doi: 10.1038/
nature14541.
[20] IanGoodfellowetal.“GenerativeAdversarialNets”.In:Advancesinneu-
ral information processing systems. 2014, pp. 2672–2680.
[21] PascalGwosdeketal.“Fastelectrostatichalftoning”.In:Journal of Real-
Time Image Processing 9.2 (2014), pp. 379–392. doi: 10.1007/s11554-
011-0236-3.
[22] Heikki Haario, Eero Saksman, and Johanna Tamminen. “Adaptive pro-
posal distribution for random walk Metropolis algorithm”. In: Computa-
tional Statistics 14.3 (1999), pp. 375–396.
[23] KennethM.Hanson.“HalftoningandQuasi-MonteCarlo”.In:Sensitivity
Analysis of Model Output. Ed. by Kenneth M. Hanson and Francois M.
Hemez. Los Alamos Research Library. 2005, pp. 430–442. url: http:
//library.lanl.gov/ccw/samo2004/.
[24] RussellHerman.IntroductiontoPartialDifferentialEquations.University
of North Carolina Wilmington. LibreTexts, 2020.
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising Diffusion Proba-
bilistic Models”. In: arXiv preprint arxiv:2006.11239 (2020).
[26] Matthew D. Hoffman and Andrew Gelman. “The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte Carlo”. In: Jour-
nal of Machine Learning Research 15 (2014). Submitted 11/11; Revised
10/13; Published 4/14, pp. 1593–1623.
[27] MatthewD.Hoffmanetal.“StochasticVariationalInference”.In:Journal
of Machine Learning Research 14.1 (2013), pp. 1303–1347.
[28] MatthewD.HomanandAndrewGelman.“TheNo-U-turnsampler:adap-
tively setting path lengths in Hamiltonian Monte Carlo”. In: J. Mach.
Learn. Res. 15.1 (Jan. 2014), pp. 1593–1623. issn: 1532-4435.
[29] Robert Hunt. Mathematical Methods II (Natural Sciences Tripos, Part
IB) - Poisson’s Equation. https://www.damtp.cam.ac.uk/user/reh10/
lectures/nst-mmii-chapter2.pdf. Lecture notes, Department of Ap-
plied Mathematics and Theoretical Physics, University of Cambridge.
2002.
21[30] Aapo Hyv¨arinen. “Estimation of Non-Normalized Statistical Models by
Score Matching”. In: Journal of Machine Learning Research 6 (2005),
pp. 695–709.
[31] Michael I. Jordan et al. “An Introduction to Variational Methods for
Graphical Models”. In: Machine Learning 37 (1999), pp. 183–233.
[32] HermanKahnandTheodoreE.Harris.“EstimationofParticleTransmis-
sion by Random Sampling”. In: Monte Carlo Method. Vol. 12. Applied
Mathematics Series. National Bureau of Standards, 1949, pp. 27–30.
[33] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes.
2013. arXiv: 1312.6114 [stat.ML].
[34] Chang Liu et al. “Understanding and Accelerating Particle-Based Vari-
ational Inference”. In: Proceedings of the 36th International Conference
on Machine Learning (ICML). 2019. url: http://proceedings.mlr.
press/v97/liu19d/liu19d.pdf.
[35] Chang Liu et al. “Understanding and accelerating particle-based varia-
tionalinference”.In:InternationalConferenceonMachineLearning.2019,
pp. 4082–4092.
[36] Qiang Liu and Dilin Wang. “Stein Variational Gradient Descent: A Gen-
eralPurposeBayesianInferenceAlgorithm”.In:AdvancesinNeuralInfor-
mation Processing Systems 29 (NIPS).2016,pp.2378–2386.url:https:
//arxiv.org/abs/1608.04471.
[37] D. J. C. MacKay. “Introduction to Monte Carlo Methods”. In: Learning
in Graphical Models. Ed. by M. I. Jordan. Kluwer Academic Press, 1998,
pp. 175–204.
[38] David J. C. MacKay. Information Theory, Inference & Learning Algo-
rithms. USA: Cambridge University Press, 2002. isbn: 0521642981.
[39] GaelM.Martin,DavidT.Frazier,andChristianP.Robert.“Approximat-
ing Bayes in the 21st Century”. In: Statistical Science 39.1 (Feb. 2024),
pp. 20–45. doi: 10.1214/22-STS875.
[40] M. D. McKay, W. J. Conover, and R. J. Beckman. “A Comparison of
Three Methods for Selecting Values of Input Variables in the Analysis of
OutputfromaComputerCode”.In:Technometrics 21.2(1979),pp.239–
245.
[41] Nicholas Metropolis et al. “Equation of State Calculations by Fast Com-
putingMachines”.In:TheJournalofChemicalPhysics 21.6(1953),pp.1087–
1092. doi: 10.1063/1.1699114.
[42] Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT
Press, 2022. url: probml.ai.
[43] Iain Murray, Zoubin Ghahramani, and David J. C. MacKay. “MCMC for
doubly-intractable distributions”. In: Proceedings of the Twenty-Second
Conference on Uncertainty in Artificial Intelligence. UAI’06. Cambridge,
MA, USA: AUAI Press, 2006, pp. 359–366. isbn: 0974903922.
22[44] Radford M. Neal. “Slice sampling”. In: The Annals of Statistics 31.3
(2003), pp. 705–767. doi: 10.1214/aos/1056562461.
[45] JohnvonNeumann. “Various techniques usedin connectionwith random
digits”. In: Applied Math Series, Notes by G. E. Forsythe, in National
Bureau of Standards 12 (1951), pp. 36–38.
[46] HaraldNiederreiter.RandomNumberGenerationandQuasi-MonteCarlo
Methods.Vol.63.CBMS-NSFRegionalConferenceSeriesinAppliedMath-
ematics. Philadelphia: SIAM, 1992.
[47] Ryutaroh Ohbuchi and Masaki Aono. Quasi-Monte Carlo rendering with
adaptive sampling. http://www.kki.yamanashi.ac.jp/~ohbuchi/
online_pubs/eg96_html/eg96.htm. 1996.
[48] Abril-Pla Oriol et al. “PyMC: A Modern and Comprehensive Probabilis-
tic Programming Framework in Python”. In: PeerJ Computer Science 9
(2023), e1516. doi: 10.7717/peerj-cs.1516.
[49] Rajesh Ranganath, Sean Gerrish, and David M. Blei. “Black Box Varia-
tional Inference”. In: Proceedings of the 17th International Conference on
Artificial Intelligence and Statistics (AISTATS). 2014, pp. 814–822. url:
http://proceedings.mlr.press/v33/ranganath14.pdf.
[50] Danilo Jimenez Rezende and Shakir Mohamed. “Variational Inference
with Normalizing Flows”. In: Proceedings of the 32nd International Con-
ference on Machine Learning (ICML). 2015, pp. 1530–1538. url: https:
//arxiv.org/abs/1505.05770.
[51] DaniloJimenezRezendeandShakirMohamed.Variational Inference with
Normalizing Flows. 2016. arXiv: 1505.05770 [stat.ML].
[52] DaniloJimenezRezendeandShakirMohamed.Variational Inference with
Normalizing Flows. 2016. arXiv: 1505.05770 [stat.ML]. url: https:
//arxiv.org/abs/1505.05770.
[53] GarethO.RobertsandRichardL.Tweedie.“ExponentialConvergenceof
LangevinDistributionsandTheirDiscreteApproximations”.In:Bernoulli
2.4 (1996), pp. 341–363. doi: 10.2307/3318418.
[54] Stephen J Roberts and Will D Penny. “Variational Bayes for generalized
autoregressive models”. In: IEEE Transactions on Signal Processing 50.9
(2002), pp. 2245–2257. doi: 10.1109/TSP.2002.802091.
[55] TetsuyaKaji;VeronikaRoˇckov´a.“Metropolis-HastingsviaClassification”.
In: https://arxiv.org/pdf/2103.04177.pdf (2021).
[56] Ardavan Saeedi et al. “Variational Particle Approximations”. In: Journal
of Machine Learning Research 18 (2017), pp. 1–29.
[57] ChristianSchmaltzetal.“ElectrostaticHalftoning”.In:ComputerGraph-
icsForum 29.8(2010),pp.2313–2327.doi:10.1111/j.1467-8659.2010.
01716.x.
23[58] Yang Song et al. “Score-Based Generative Modeling through Stochastic
Differential Equations”. In: CoRR abs/2011.13456 (2020). url: https:
//arxiv.org/abs/2011.13456.
[59] T.Teuberetal.“DitheringbyDifferencesofConvexFunctions”.In:SIAM
Journal on Imaging Sciences 4.1 (2011), pp. 79–108. doi: 10.1137/
100790197. url: https://doi.org/10.1137/100790197.
[60] DilinWangetal.Stein Variational Gradient Descent With Matrix-Valued
Kernels. 2019. arXiv: 1910.12794 [stat.ML].
[61] Yiwei Wang et al. “Particle-based Energetic Variational Inference”. In:
Statistics and Computing 31.34 (2021). doi: 10.1007/s11222-021-
10009-7. url: https://doi.org/10.1007/s11222-021-10009-7.
[62] Robert L. Winkler. An Introduction to Bayesian Inference and Decision.
Holt McDougal, 1972.
[63] YifanYang,ChangLiu,andZhengzeZhang.“Particle-basedOnlineBayesian
Sampling”.In:ArXiv abs/2302.14796(2023).url:https://api.semanticscholar.
org/CorpusID:257232350.
A A hypothetical treatment of Coulomb’s law
in d-dimension
Hereweaimtoderivetheforcesbetweentwochargesinlowandhighdimensions. We
first present the well-known force formula in 3D (Section.A.1), then move to general
d dimensions (Section.A.2), solve the 3D Poisson’s equation (Section.A.3), and solve
the Poisson’s equation in d dimensions (Section.A.4).
A.1 Electric forces in 3D
In three-dimensions, the electric force between two charges q and q is calculated as
i j
per Coulomb’s law:
q q
F=k i je (13)
r2
where r is the distance between the two charges, k = 10−7c2 = 1 is the Coulomb
4πε0
constant (alsocalledtheelectricforceconstantorelectrostaticconstant),withcbeing
the light speed in a vacuum, ε the vacuum permittivity introduced later. The unit
0
vector e implies the direction of the forces - they point in inverse directions as per
Newton’s third law. Theforceisrepulsiveforlikeandtheattractingforunlikeelectric
charges. Note that, Eq.13 only holds for three dimensions in which both Coulomb’s
law and Newton’s law of gravity takes the form of distance inverse-squared.
The electric field, e.g. induced by the charge q , is defined as:
i
q
E =k ie (14)
i r2
which conveniently simplifies Eq.13 to F=q E .
1 2
Superposition principle applies to the electromagnetic field. The overall electric
field and force acting on a particle, induced by others, can be calculated by summing
24over all the contributions due to individual source particles. Consider a collection of
M particles each with charge q and distribute in a 3D space. The electric field at
i
arbitrary point (other than the point itself) can be expressed as:
M
E(r)= 1 (cid:88) q ie (15)
4πε r2 i
0 i=1 i
where r is the distance between a source point and the point of consideration. e is
i i
a unit vector indicating the direction of the electric field.
The electric field generated by a distribution of charges, as given by the charge
density ρ(r), can be obtained by replacing the sum by integral:
1 (cid:90)(cid:90)(cid:90) ρ(r )
E(r)= i e d3r (16)
4πε r2 i
0 i
which accounts for all contributions from the small charges ρ(r )d3r contained in the
i
infinitesimally small volume element d3r≈dxdydz.
All these equations are only valid for three dimensions; ideally, we would like to
generalizeCoulomb’slawford-dimension. Inthefollowing,weusePoisson’sequation
(and Laplace’s equation) for deriving the electric force in d dimensions.
A.2 The Poisson and Laplace equations for electrostatics
We start with the Gauss’s law for electricity in differential form:
∇·D=ρ (17)
where ∇· is the divergence operator 14. D is the electric displacement field, ρ is the
free-chargedensity(chargesfromoutside). Gauss’slawisoneofMaxwell’sequations;
it relates the distribution of electric charge to the resulting electric field.
Assuming linear, isotropic and homogeneous medium, the following constitutive
equation holds:
D=εE (18)
whereEistheelectricfield. εistheabsolutepermittivity15ofthemedium. Often,the
relativepermittivity,whichistheratiooftheabsolutepermittivityεandthevacuum
permittivity ε , i.e. ε = ε/ε , is used. The vacuum permittivity 16 is the absolute
0 r 0
dielectric permittivity of classical vacuum; it is referred to as the permittivity of free
space ortheelectric constant whosevalueε ≈8.8541878128×10−12F/m(faradsper
0
meter).
14Inthree-dimensionalCartesiancoordinates,forexample,thedivergenceofacontinuously
differentiable vector field D = Dxi+Dyj+Dzk is defined as the scalar-valued function:
divD=∇·D=( ∂∂ x, ∂∂ y, ∂∂ z)·(Dx,Dy,Dz)= ∂ ∂D xx + ∂ ∂D yy + ∂ ∂D zz. Thedivergenceoperator
incylindricalandsphericalcoordinatescanbesimilarlyderived.
15Inelectromagnetism,theabsolutepermittivityεisameasureoftheelectricpolarizability
of a dielectric material. A material with high permittivity polarizes more in response to an
applied electric field than a material with low permittivity, thereby storing more energy in
thematerial.
16It measures how dense of an electric field is ”permitted” to form in response to electric
charges,andrelatestheunitsforelectricchargetomechanicalquantitiessuchasforcegiven
byCoulomb’slaw.
25The electrostatic force F on a charge q′, induced by the electric field E, is the
product:
F=q′E (19)
SubstitutingEq.18intoGauss’slawinEq.17,andassumeconstantpermittivityε
of the medium, we have:
ρ
∇·E= (20)
ε
In the presence of constant magnetic field or no magnetic field (i.e. if the field is
irrotational), we have:
∇×E=0 (21)
where ∇× is the curl operator. As the curl of any gradient is zero, we can write the
electric field E as the gradient of a scalar function φ (the electric potential):
E=−∇φ (22)
where ∇ here denotes gradient, and φ is a real or complex-valued function on a man-
ifold. Eq.22 states that, the gradient of the electric potential gives the electric field.
The negative sign reflects that the φ is the electric potential energy per unit charge.
Substituting Eq.22 into Eq.20 gives:
ρ
∇·E=−∇2φ= (23)
ε
where ∇·∇ = ∇2 = ∆ is the Laplace operator 17. Eq.23 relates the divergence of
the electric field ∇·E to the charge density ρ through the negative Laplacian of the
potential φ, which further leads to the Poisson’s equation for electrostatics:
ρ
∆φ=∇2φ=− (24)
ε
SolvingthePoisson’sequationEq.24givestheelectricpotentialφ,giventhemass
densitydistributionofchargesρ(x);however,inordertosolvethePoisson’sequation,
we need to know the charge density distribution ρ. Different configurations of ρ gives
different solutions. For example, if the charge density ρ = 0 (i.e. in a region where
therearenochargesorcurrents),whichgivestheLaplace’sequation (thehomogeneous
Poisson’s equation):
∇2φ=0 (25)
a function satisfying the Laplace equation is called harmonic [7].
AsidenoteonPoisson’sandLaplaceequations. Poisson’sequation, initsgeneral
formasinEq.27,canbeusedtodescribethediffusionprocessofachemicalsoluteor
heat transfer, i.e. ∂φ(x,t)/∂t = κ∇2φ = κf(x,t), where κ is termed the diffusivity.
The RHS f(x,t) represents external sources (for example, in Eq.24 we have f(x,t)=
−ρ/ε)whichinjectinputsintothesystem(e.g. solutegeneratedbyachemicalreaction,
17Laplace operator is a second-order differential operator in the n-dimensional Euclidean
space, defined as the divergence (∇·) of the gradient (∇φ), where φ is a twice-differentiable
function. Under Cartesian coordinates, the Laplacian of φ is thus the sum of all unmixed
second partial derivatives: ∆φ(x) = (cid:80)d ∂2φ(x). Similar expressions can be derived in
i=1 ∂x2
i
othercoordinates(e.g. cylindricalandsphericalcoordinates).
26or heat provided by external sources). The Laplace equation, as a special case of the
Poisson’sequation,canbederivedasasteady-stateconditionforthediffusionprocess
(e.g. concentration of chemical solute, heat transfer). The Poisson’s equation doesn’t
have a unique solution, unless extra boundary condition is imposed. For example,
if the electrostatic potential φ(x) is specified on the boundary of a region, then its
solutionisuniquelydetermined. SomeapplicationscenariosofthePoisson’sequation,
solutions, as well as uniqueness proofs, can be found in e.g. [29].
We formerly define φ:Rd →R, and its Laplacian, ∆φ:Rd →R, i.e.
d
∆φ(x):=(cid:88) ∂2 φ(x) (26)
xixi
i=1
wherex istheith coordinateofx∈Rd. IncraftingEq.26,wehaveassumedthatthe
i
Laplacian extends the same form into d-dimension.
A.3 The 3D Poisson’s equation
Withoutlosinggenerality18,westartwithsolvinga3DPoisson’sequation generalised
from Eq.24:
∆φ(x)=∇2φ(x)=f(x) (27)
where φ : R3 → R and f : R3 → R (the support can be restricted to subset of R3
though). The RHS f(x) is called the source and it is often zero either everywhere or
everywherebutsomespecificregionorpoints. Forexample,f(x)=c δ3(x),wherec
0 0
is a constant, represents a point source.
We shall apply Fourier transform to Eq.27; before that, we re-cap the 3D Fourier
transform:
(cid:90)
φˆ(k)= φ(x)e−ikxd3x (28)
where d3x=dx dx dx , k=k i+k j+k k is a three dimensional wavenumber.
1 2 3 x1 x2 x3
The inverse Fourier transform is
1 (cid:90)
φ(x)= φˆ(k)eikxd3k (29)
(2π)3
where d3k=dk dk dk .
x1 x2 x3
Assumingthatφ(x)anditsgradientsvanishforlargex,weapplyFouriertransform
to the generalised 3D Poisson’s equation (Eq.27):
F[∇2φ(x)]=−(k2 +k2 +k2 )φˆ(k)=F[f(x)]=fˆ(k) (30)
x1 x2 x3
by defining k2 =k2 +k2 +k2 , we have:
x1 x2 x3
k2φˆ(k)=fˆ(k) (31)
which gives
1
φˆ(k)= fˆ(k) (32)
k2
18Thissubsectionmainlyfollows[24].
27Thesolutiontothegeneralised3DPoisson’sequation(Eq.27)canbeobtainedby
performing inverse Fourier transform:
1 (cid:90) eikx
φ(x)= fˆ(k) d3k (33)
(2π)3 k2
Asasidenote,onecanalsoapplyinverseFouriertransformtoEq.30,asapathway
forevaluatingtheLaplacian∇2φ(x),whichfallsintothemethodofspectralderivative.
As an example demonstrating the use of Eq.33 to calculate the potential φ(x),
we set f(x) = −qδ(x), which represents a single point charge q in free space (i.e. a
ε
vacuum). The Fourier transform of f(x) is:
(cid:90) q q
fˆ(k)= [− δ(x)]e−ikxd3x=− (34)
ε ε
R3
as the Fourier transform of the delta function δ(x) is 1 over all space.
Substituting fˆ(k) into Eq.33, we obtain:
q (cid:90) 1
φ(x)=− eikxd3k (35)
(2π)3ε k2
Weshallfocusonevaluatingthisterm(cid:82) 1 eikxd3k. Itismoreconvenienttoswitch
k2
to the spherical coordinates with dk =k2sinθdkdθdϕ. As such, we have:
(cid:90) eikx (cid:90) +∞(cid:90) π(cid:90) 2π eikrcosθ
d3k= k2sinθdkdθdϕ (36)
k2 k2
0 0 0
where r=|x| is the radius. We re-arrange the integration sequence in Eq.36 as:
(cid:90) eikx (cid:90) +∞(cid:90) 2π (cid:90) π (cid:90) +∞ 2sin(kr) 2π2
d3k= dϕ eikrcosθsinθdθdk=2π dk=
k2 kr r
0 0 0 0
(37)
wherewehavemadeuseoftheintegralofsincfunctionoverarealline:
(cid:82)+∞sinc(x)dx=
−∞
π. Finally, substituting this result into Eq.35 gives:
q 2π2 q
φ(x)=− =− (38)
(2π)3ε r 4πεr
whichgivestheelectricfieldE(r)=−∇ φ(r)= q ,andtheresultingforceF(r)=
r 4πεr2
q′E(r)= qq′ ,whichisconsistentwiththeresultderivedlaterbyintegration(Eq.60).
4πεr2
A.4 Solution to the d-dimensional Poisson’s equation
Here we look at the generalised Poisson’s equation in d-dimensional space. We can
repeatthedirectandinverseFouriertransformsprocessasinthe3Dcase,whichgives
the solution to the d-dimensional, generalised Poisson’s equation:
1 (cid:90) eikx
φ(x)= fˆ(k) ddk (39)
(2π)d k2
where k is a d-dimensional wavenumber, k2 =(cid:80)d k2 , and ddk=dk dk ...dk .
i=1 xi x1 x2 xd
We can also obtain the derivative:
1 (cid:90) eikx
∇ φ(x)= (ik)fˆ(k) ddk (40)
x (2π)d k2
28TheproblemwithEq.39(aswellasEq.40)istheintractabilityoftheinverseFourier
integration (which involves multiple integral in spherical coordinates), particularly
when fˆ(k) is complex. Here we take another approach, i.e. using direct integration
to solve the Poisson’s equation in a general d-dimensional space with a point charge.
Similar to the 3D example, a single point charge at x can be represented by ρ(x) =
qδ(x), where q is the total charge. The Poisson’s equation for electrostatics (Eq.24),
correspondingly, can be written as:
q
∆φ=∇2φ=− δ(x) (41)
ε
the solution to this single point charge Poisson’s equation is the potential field φ(x),
andthereforetheelectricfieldE(x)aspertheirrelationEq.22(andfurthertheforce),
caused by the given electric charge qδ(x).
In Cartesian coordinates (x ,x ,...,x ), Eq.41 becomes:
1 2 d
(cid:88)d ∂2 q
φ(x)=− δ(x) (42)
∂x2 ε
i=1 i
Due to the problem’s spherical symmetry in d dimensions, it is natural to switch
to hyper-spherical coordinates. The Laplacian for a radially symmetric function φ(r)
in hyper-spherical coordinates reduces to:
1 d dφ
L[φ(r)]= (rd−1 ) (43)
rd−1dr dr
substituting the Laplacian L(φ(r) into the RHS of the Poisson’s equation (Eq.42) in
in hyper-spherical coordinates, we have:
1 d dφ q
(rd−1 )=− δ(r) (44)
rd−1dr dr ε
Away from the origin (i.e. r̸=0), we have δ(r)=0, which simplifies Eq.44 to:
d dφ
(rd−1 )=0 (45)
dr dr
integrating it gives:
dφ
rd−1 =C (46)
dr
where C is an integration constant. Solving for φ, we get:
C
φ(r)= +D (47)
(2−d)rd−2
againDisanotherintegrationconstant. WesetD=0,assumingthepotentialvanishes
at infinity. To determine the constant C, we can integrate Eq.41. First, we integrate
the LHS and use the divergence theorem 19:
19The divergence theorem states that the surface integral of a vector field over a closed
surface (’the flux through the surface) equals the volume integral of the divergence over the
(cid:82) (cid:82)
region enclosed by the surface, i.e. ∇·FdV = F ·nˆdS, where S is the closed surface
V S
surrounding the volume V, and nˆ is a unit vector directed along the outward normal to S.
Itconvertsbetweenthesurfaceintegralandthevolumeintegral,andgeneralisestoarbitrary
dimension. Physically,itmeansthesumofallsourcesinaregiongivesthenetfluxoutofthe
region.
29(cid:90) (cid:90)
∇·(∇φ)dV = ∇φrˆ·dS (48)
V S
which converts the volume integral into a surface integral, with S being the closed
surface surrounding the volume V and rˆa unit vector pointing outward normal to S.
(cid:82) (cid:82)
The quantity on the RHS, i.e. ∇φrˆ·dS = ErˆdS is the electric flux over the
S S
surface S (the negative sign from Eq.22 has been indicated by the outward pointing
unit vector). Due to the spherical symmetry, the integral in Eq.48 is the product of
the surface area of the d−1 dimensional hypersphere 20 and ∇φ, i.e.
(cid:90)
∇φrˆ·dS =S rd−1×∇φ (49)
d−1
S
in which S is the the surface area of the d−1 dimensional unit hypersphere, and
d−1
the surface area of the d−1-sphere with radius r is scaled by rd−1. S can be
d−1
obtained21 byusingthetrickofad-dimensionalGaussianintegral,whichrelatesthis
surface area to a known quantity. We know the following:
(cid:90) +∞ (cid:90) +∞
e−x2 ddx=[ e−x2 dx]d =πd/2 (50)
−∞ −∞
wherewehavemadeuseoftheknownEuler–Poissonintegral(cid:82)+∞e−x2 dx=√
π. This
−∞
integral can also be related to the surface area of the d−1 dimensional unit sphere.
Writing the LHS of Eq.50 in spherical coordinates:
(cid:90) +∞ (cid:90) +∞
e−x2 ddx=S rd−1e−r2 dr (51)
d−1
−∞ 0
the integral (cid:82)+∞rd−1e−r2 dr can be evaluated 22 using change of variable, i.e. sub-
0 √
stituting u = r2 and du = 2rdr, dr = du/2r = du/(2 u) into it, which yields
(cid:82)+∞rd−1e−r2 dr=(cid:82)+∞(√ u)d−1e−u √1 du= 1(cid:82)+∞ud 2−1e−udu,whichisintheform
0 0 2 u 2 0
of the Gamma function Γ(d). Therefore, we have Eq.51 reduces to:
2
(cid:90) +∞ e−x2 ddx= 1 S Γ(d ) (52)
2 d−1 2
−∞
Equating Eq.50 and Eq.52, we arrive at:
20Asphereinad-dimensionalspace,enclosingad-dimensionalball(calledd-ball,thed-1-
sphereistheboundaryofthed-ball),hasdimensiond-1becauseithasd-1degreesoffreedom
due to the fixed radius. For example, the sphere defined by x2+x2+x2+x2+ = r2 in
1 2 3 4
4-dimensionalspaceiscalleda3-sphere(S3).
21Alternatively,onecanderivethesurfaceareabynotingthat,thevolumeofad-ballwith
radius r is: V (r) = rdV (1) = rd πd/2 (one can simply verify this in 2D and 3D. NB:
d d Γ(d/2+1)
if r is fixed, its volume converges to zero as dimension increases, i.e. the volume of a high-
dimensional unit ball is concentrated near its surface due to curse of dimensionality), then
thesurfaceareaofthed-ballcanbederivedasthederivativeofthevolumew.r.t. theradius
r: S (r)= d V (r)=drd−1 πd/2 . Therefore, the surface area of the d-ball with unit
d−1 dr d Γ(d/2+1)
radiusis: S =d
πd/2
=
2πd/2
,whichisthesameasEq.54.
d−1 Γ(d/2+1) Γ(d/2)
22Note that, as we are evaluating a definite integral, when d−1 is an odd integer, this
integral is zero. We can relax this by replacing it with indefinite integral, which gives the
sameresult.
301 d
S Γ( )=πd/2 (53)
2 d−1 2
which gives:
2πd/2
S = (54)
d−1 Γ(d)
2
NowwegobacktoEq.49. Wehavenow(cid:82) ∇φrˆ·dS =S rd−1×∇φ= 2πd/2 ×
S d−1 Γ(d)
2
φ′ = 2πd/2rd−1 ×Cr1−d = 2πd/2 ×C as per Eq.46 (or Eq.47). The integration of
Γ(d) Γ(d)
2 2
the RHS of Eq.41 is just −q/ε due to the delta function. Therefore, we have the
integration constant C = −qΓ(d)/(2πd/2ε). Finally, the solution to the single point
2
charge Poisson’s equation (i.e. Eq.47) becomes:
−qΓ(d)/(2πd/2ε) qΓ(d)
φ(r)= 2 = 2 (55)
(2−d)rd−2 2(d−2)πd/2εrd−2
After obtaining the potential φ, we can get the electric field E as per Eq.22.
dφ qΓ(d)
E =− = 2 (56)
q dr 2πd/2εrd−1
and the electric force 23 exerted on another charge q′, by the single point charge q at
a distance r from the point charge q:
qq′Γ(d)
F q→q′ =q′E q = 2πd/2εr2 d−1 (57)
As we are concerning free space, ε=ε which is the vacuum permittivity.
0
Notethat,fornotationalsimplicity,wehavedroppedalldirectionnotationsinour
electricpotential(Eq.55),field(Eq.56),force(Eq.57)expressionsandtheirderivations,
andfocusontheirmagnitudes. Themindfulcanaddaunit vector e q,q′,whichpoints
away from a positive charge and points to a negative charge, to these formula to
indicate the directions of these quantities. In a Cartesian coordinate, for example, if
q is atpositionx q, and q′ at position x q′, the directionalunit vector is thene q→q′ :=
(x q′ −x q)/∥x q′ −x q)∥ d =(x q′ −x q))/r q,q′ where r i,j =∥x q′ −x q)∥ d, and ∥·∥ d is the
distance measure in d-dimensional space. By defining the direction of e q→q′, we have
by default assumed repulsive to be positive. Substituting the expression of r into e.g.
the force formula gives:
F q→q′(x i,x j)= 2πq dq /′ 2Γ ε( rd 2 d) −1e q→q′ = 2πq dq /′ 2Γ ε( rd 2 d) −1x q′ −
r
x q
(57b)
= qq′Γ(d 2)x q′ −x q = qq′Γ(d 2) x q′ −x q
2πd/2ε rd 2πd/2ε ∥x q′ −x q∥d 2/2
where∥·∥ denotestheL norm. A2Ddiagramforillustratingtherepulsiveforceand
2 2
the distance is presented Fig.10.
Eq.57extendstheCoulomb’slawforanydimensions. Forexample,in1Dwehave:
√
qq′ π qq′
F q,q′ =
2π1/2ε rd−1
=
2ε
(58)
0 0
23Ifweareonlyconcerningabouttheelectricforce,thereisnoneedtocalculatethepotential
-allweneedisthepotentialderivative∇xφ(x).
31Figure10: Diagramofrepulsiveforceanddistanceina2DCartesioncoordinate.
and for 2D:
qq′
F q,q′ =q′E q = 2πε r (59)
0
and for 3D:
√
qq′ π/2 qq′
F q,q′ =q′E q = 2π3/2ε r2 = 4πε r2 (60)
0 0
which is the same as Eq.13 and Eq.38 given:
1
k= (61)
4πε
0
B A note on complexity
Most computational efforts lie in computing the interacting force between particles.
At each iteration, in order to calculate the two forces for a single negative par-
ticle j, we have to calculate all the pairwise distances 1/rd−1 and 1/rd−1 where
i,j i′,j
i ∈ {1,2,...,Mneg} and i′ ∈ {1,2,...,Mpos}, as shown in Figure.11. That is, we
have to evaluate the following distance matrix in each iteration:
32R (Mneg+Mpos)×(Mneg+Mpos) =
q 1 q 2 ··· q Mneg ··· q Mpos
 
q 1 0 r 1,2 ··· r 1,Mneg ··· r 1,Mpos
 
 
q 2 r 2,1 0 ··· r 2,Mneg ··· r 2,Mpos 
 
 
···   . . . . . . ... . . . ... . . .    (62)
 
q Mneg  r Mneg,1 r Mneg,2 ··· 0 ··· r Mneg,Mpos  
 
 
···

. .
.
. .
.
... . .
.
... . .
.


 
 
q Mpos r Mpos,1 r Mpos,2 ··· r Mpos,Mneg ··· 0
whichcanbepre-calculatedorcalculatedwhenneeded. Calculatingthewholematrix
R takes order O((Mneg +Mpos)2). The computational efforts will be huge if either
Mneg or Mpos is large. Of course, half of the computation can be saved by making
use of the symmetry r =r .
i,j j,i
Figure 11: Pairwise distances.
Let’stakeacloserlookattherepulsiveforceinEq.Eq57cbyexpandingitinmatrix
form:
Γ(d)
Frep(x )= 2 q Λq
j j 2πd/2ε j 1:Mneg
0
1/rd−1 0 ··· 0  q 
1,j 1
= 2πΓ d( /d 2 2) ε
0
(cid:0) q j q j ··· q j(cid:1)  

0 . .
.
1/r . . .2d ,− j1 · .· ..· 0 . .
.
     

q . . .2   

0 0 ··· 1/r id ,− j1 q Mneg
(cc.Eq57c)
similar matrix expansion can be written with the attracting force in Eq.Eq57d for
particle j.
WhencalculatingFrep(x )andFattr(x )fortheparticlej,weneedtoretrievethe
j j j j
relevant pairwise distance vector, i.e. the row r or column r , from the distance
j,· ·,j
matrix R in Eq.62. However, if necessary (i.e. if either Mneg or Mpos is large), we
can take a neighbourhood search to identify a neighbourhood set and calculate only
33the pairwise distances between particles within the neighbourhood set. This shrinks
the distance vector for particle j.
C Mapping concepts in electrostatics and prob-
ability
Quantitiesinelectrostaticscanbemappedtotheconceptsinprobabilitydistributions
as follows:
Output Type Electrostatics Probability Used in this work
Scalar-valued φ(x) logp(x) -
∇φ(x)
Vector-valued E=−∇φ(x) ∇logp(x) F ∝ p(x) or logp(x)
F=q′E=−q′∇φ(x)
Scalar-valued ∆φ(x)=∇·∇φ(x) tr(∇2logp(x)) -
Table 3: Correspondence of concepts in electrostatics and probability.
For example, if we assume φ(x) = k∇logp(x), then F = −qE = −q∇φ(x) =
−qk∇logp(x), which is similar to the driving force term k(·,·)∇logp(x) in SVGD.
Therefore, in order to calculate the force F, we only need to gain knowledge about
∇φ(x), which says, equivalently in probability, that we only need to know ∇logp(x),
thegradientfieldcanbeconvenientlyestimatedbytechniquessuchasscore matching
[30].
D Toy examples: more details
Gaussian densities The two target Gaussian densities are as follows:
Unimodal:
p(x)=N(µ,Σ)
with µ=(0.5,0.5) and Σ=0.05I.
Bimodal:
p(x)=0.7N(µ ,Σ )+0.3N(µ ,Σ )
1 1 2 2
(cid:20) (cid:21) (cid:20) (cid:21)
1 −0.5 1 0.5
with µ =(0,0), Σ = , µ =(4,4), Σ = .
1 1 −0.5 1 2 2 0.5 1
In inferring the unimodal Gaussian, we draw negative charges from the uniform
prior: p0(x) = U(0,0.5) × U(0,0.5); In inferring the bimodal Gaussian, we draw
negative charges from the uniform prior: p0(x)=U(−3,7)×U(−3,7).
Other three densities The other 4 un-normalised target densities used are:
Moon-shared [22, 61]:
ρ(x)∝exp(cid:26) −x2
1 −
1(cid:0)
10x
+3x2−3(cid:1)2(cid:27)
2 2 2 1
34Double-banana [51, 35]:
ρ(x)∝exp(cid:110) −2(cid:0) (x2+x2−3)2(cid:1) +log(cid:16) e−2(x1−2)2 +e−2(x2+2)2(cid:17)(cid:111)
1 2
Wave density [51, 6, 61]:
ρ(x)∝exp(cid:40) −1(cid:34)
x
2−sin(cid:0)π 2x1(cid:1)(cid:35)2(cid:41)
2 0.4
Implementation Themeshgridforall3otherdensitiesiswithin[−3,3]×[−3,3];
forthebi-modalGaussianitis[−3,7]×[−3,7];foruni-modalGaussianitis[0,1]×[0,1].
The 400 negative charges are initialised with 2D standard Gaussian for the moon-
shaped case, and uniformly for all other cases (uniform within the whole mesh grid,
exceptfortheuni-modalGaussiancasewheretheinitialisationareais[0,0.5]×[0,0.5]).
Euler updating rule (Eq.1) is used and Mpos = 50×50,Mneg = 400,σ = 0
noise
applied. The overall forces across negative charges are normalised in each iteration
before calculating the displacement. The charge magnitudes qneg =q =1.0. Permit-
tivityoffreespaceε =8.854e−12. Eulerproportionalityconstantτ =0.1. Particles
0
are evolved for T =100 iterations. For all three densities, at equilibrium, we discard
the particles outside the grid area.
E Neal’s funnel: implementation details
The MMD2 metric is defined as [1, 61]:
N M N M
MMD2 = 1 (cid:88) k(x ,x )+ 1 (cid:88) k(y ,y )− 2 (cid:88)(cid:88) k(x ,y ) (63)
N2 i j M2 i j NM i j
i,j=1 i,j=1 i=1j=1
with the polynomial kernel
k(x,y)=(cid:16)
x⊤y
+1(cid:17)3
.
3
HMC ThePyMC PythonAPI[48]isusedtoimplementtheHMCmethod. Intotal,
1chainwith4000samplesisdrawn,with1000burn-upsamplesdiscarded. These4000
samples are then thinned evenly to produce 400 samples.
MH Initialisedfromnearzero,weuseastandardnormaldistributionastheproposal
distribution, the simplified acceptance probability is defined as the ratio of the new
sample to current position.
LMC We initialise 400 samples uniformly within [−7,3]×[−7,3], and evolve them
through10000iterations;ineachiteration,eachparticleisevolvedthroughtheLangevin
dynamics [60]:
√
x =x +ϵ ∇logp(x )+ 2ϵ z
t+1 t t t t t
ϵ =a(b+t)−c
t
z ∼N(0,I)
t
35with a=0.01,b=1,c=0.55 and 10000 iterations for each sample. t is the iteration
number.
We also records the step time, mean negative log-likelihood (NLL), and squared
maximum mean discrepancy squared (MMD2) in each iteration.
SVGD SVGD inference updates particle positions as per [36]:
xl+1 =xl+ϵϕ∗ (xl)
i i q,p i
and
M
ϕ∗(xl)= 1 (cid:88) [k(xl,xl)∇ logp(xl)+∇ k(xl,xl)]
i M i j x j xj i j
j=1
We referred to the implementation by [61], in which a RBF kernel exp(∥x−x′∥/2h2)
is used with the hyper-parameter length-scale h=median pairwise distance. The step
size used is 0.01. The same 400 uniformly initialised samples (as used in LMC) are
evolved,withtotalnumberofiterations100,000. Wealsorecordsthesteptime,mean
NLL, and MMD2 in each iteration.
EVI We refer to the implementation by [61], in which the RBF kernel with length-
scale h = 0.1 is used. The step size used is 1.0×10−7. The same 400 uniformly
initialised samples (as used in LMC) are evolved, with total number of iterations
2,500,000. We also records the step time, mean NLL, and MMD2 in each iteration.
EParVI The 400 negative charges are initialised uniformly, the same as that of
LMC,SVGDandEVI.Eulerupdatingrule(Eq.1)isusedandMpos =10,000,Mneg =
400,σ = 0 applied. The overall forces across negative charges are normalised in
noise
each iteration before calculating the displacement. The charge magnitudes qneg =
q = 1.0. Permittivity of free space ε = 8.854e−12. Euler proportionality constant
0
τ =0.1. Particles are evolved for T =100 iterations. The MMD2 and NLL metrics
are evaluated based on particles within the mesh grid.
F Bayesian logistic regression: more details
Posterior The likelihood can be written as:
(cid:89)N (cid:20)
1
(cid:21)yi(cid:20)
1
(cid:21)1−yi
p(y|x,ω)= 1−
1+exp(−ωTx ) 1+exp(−ωTx )
i i
i=1
combining with the prior:
(cid:18) (cid:19)
1 1
p(ω)=N(ω;0,αI)= exp − ωTω
(2πα)d/2 2α
where d = 4 is the number of features (not considering intercept), we obtain the
coefficients ω posterior:
p(ω|y,x)∝(cid:89)N (cid:20)
1
(cid:21)yi(cid:20)
1−
1
(cid:21)1−yi ·exp(cid:18)
−
1
ωTω(cid:19)
1+exp(−ωTx ) 1+exp(−ωTx ) 2α
i i
t=1
36Taking log we have:
logp(ω|y,x)=(cid:88)N (cid:104)
y logσ(ωTx )+(1−y )log(1−σ(ωTx
))(cid:105)
−
1
ωTω+constant
i i i i 2α
t=1
where σ(x) = 1 is the logistic function. One can then conveniently take
1+exp(−x)
gradient w.r.t ω to use gradient-based sampling algorithms such as HMC.
Gradient of posterior The gradient of logσ(ωTx ) w.r.t. ω is:
i
∇ logσ(ωTx )=(1−σ(ωTx ))x
ω i i i
The gradient of log(1−σ(ωTx )) w.r.t. ω is:
i
∇ log(1−σ(ωTx ))=−σ(ωTx )x
ω i i i
Combining these, the gradient of the likelihood term is:
(cid:88)N (cid:104)
y (1−σ(ωTx ))x −(1−y )σ(ωTx )x
(cid:105)
i i i i i i
i=1
which simplifies to:
(cid:88)N (cid:104)
(y −σ(ωTx ))x
(cid:105)
i i i
i=1
On the other hand, the gradient of the prior term w.r.t. ω is:
1
− ω
α
Combining the gradients of the likelihood and the prior, we get:
∇
logp(ω|y,X)=(cid:88)N (cid:104)
(y −σ(ωTx ))x
(cid:105)
−
1
ω
ω i i i α
t=1
In matrix form, this can be written as:
1
∇ logp(ω|y,X)=XT(y−σ(Xω))− ω
ω α
where σ(Xω) is the vector of sigmoid values for each row of X.
Implementation TheIrisdatasetcontains150observations,eachinstancewith4
features. Thewholedatasetis70%:30%splitintotrainingandtestsets,withtraining
contains 105 instances and test has 45 instances. Each observation is represented by
the feature-label pair (x ,y ). Features are normalised before being used.
i i
TheSVGD,EVIandEParVImethodsusethesamenumberofparticles(i.e. 400),
and share the same initialisation. Specifically:
HMC In total, 1 chain with 2000 samples is drawn. After discarding the 1000
burn-up samples, the first 400 samples are used.
37SVGD A RBF kernel exp(∥x−x′∥/2h2) is used with the hyper-parameter length-
scale h=median pairwise distance. The step size used is 0.01. Total number of itera-
tions is 200,000.
EVI ARBFkernelwithlength-scaleh=0.1isused. Thestepsizeusedis1×10−7.
Total number of iterations is 2.5×108.
EParVI Euler updating rule (Eq.1) is used and Mpos =124,Mneg =400,σ =
noise
0applied. Notetheregionspans[−3,3]×[−3,3],andthegranularity(i.e. resolution)
is therefore 0.5. The overall forces across negative charges are normalised in each
iteration before calculating the displacement. The charge magnitudes qneg =1.0 and
q=1.0×107. The relatively large value of q is due to small chain product of the un-
normalised likelihood in Eq.4; to make it on the same scale as the repulsive force, by
trial and error we need to set a large value for q. a strategy to avoid small (absolute)
values of the likelihood is to take log, as we did in inferring the LV dynamical system
parameters in Section.5.4. Euler proportionality constant τ = 0.1. Particles are
evolved for T =60 iterations.
Implementation tricks For complex geometries, more steps and more charges
arerequired. Updatingtheparticlepositionsusingdifferentrulesmayleadtodifferent
results. Interestingly, in our experiments we find that, if we update the negative
positions according to Eq.1c, in the limit all negative charges are absorbed to high
probabilitygridpoints. Also,ifthe∆tusedintheVerletordampedVerletmethodsis
too large, then the particle system is unstable and cannot converge to a steady-state.
Onecanaddrandomnoisetoperturbthenegativechargepositionstoavoidover-
lappingatgridpointsandtoavoidgettingtrappedinlocaloptima. Onecanchooseto
discardparticlesfallingoutofthegrids(e.g. thosegettrappedinlow-densityregions)
attheendorsequentially. Thisalsosavescomputationalefforts. WithoutanMHstep,
charges can take large, risky steps and jump outside the region. [23] also addressed
that, appropriate conditions need to be specified at the boundary of the region. In
ourexperiments,wedesignedanannealingschemeforreducingqpos overiterationsto
promote within-mode diversity.
It is generally suggested to pre-evaluate the relative magnitudes of repulsive and
attracting forces and scale qpos properly to ensure they are on the same scale; other-
wise, e.g. when repulsive forces is of order larger than attracting force, the density
center will never attract the particles. Effect of the ratio q/q is important: a small
i
ratio can make the repulsive force dominate and lead to explosion; a large value can
accelerate convergence, but also be too restrictive - the final positions of negative
charges overlap and diversity reduces.
G LV dynamical system: more details
Posterior The logarithm of the posterior (Eq.6), with priors written in Eq.7 and
likelihood in Eq.9, is:
38logp(θ|X′,Y′)=logp(a,b,c,d|X′,Y′)=logp(a)+logp(b)+logp(c)+logp(d)+logp(X′,Y′|a,b,c,d)=
(cid:88) log( 1 )−2Nlog(√ 2πσ)−(cid:88)N (logx′+logy′)− 1 (cid:88)N [(logx′−logx )2+(logy′−logy )2]
max −min i i 2σ2 i i i i
i i
i=a,b,c,d i=1 i=1
(64)
where min =min =min =min =0.001,max =max =max =max =1 as
a b c d a b c d
specified by Eq.7, and noise level σ = 0.25. Note that, though not explicitly shown,
the ODE solutions (x ,y ) are functions of θ=(a,b,c,d).
i i
To make all log posteriors positive, we use the following offset log posterior as
target for EParVI:
p(θ|X′,Y′)
logp(θ|X′,Y′)−logp(θ |X′,Y′)=log
0 p(θ |X′,Y′)
0
where p(θ |X′,Y′) = min logp(θ|X′,Y′), G is the set of all grid points (locations
0 θ∈G
of positive charges). Alternatively, p(θ |X′,Y′) can be chosen to be a value smaller
0
than the log posterior evaluated at any point in the space of consideration.
Byqueryingthelogposteriorformula,weobtainthelogposteriorvaluesat810000
equidistant points (30 along each dimension). Marginalising out two dimensions, we
arrive at the theoretical, 2D marginal log posteriors as presented in Fig.12.
Figure12: Marginallogposteriors. Left: marginalisingoutc,d;right: marginal-
ising out a,b.
As the score ∇ logp(θ|X′,Y′) is used in gradient-based samplers such as LMC
θ
and HMC, we derive it as follows 24:
24Moredetailscanbefoundintheauthor’scomingbook”Someempiricalobservationson
Steinvariationalgradientdescent”.
39∂logp(a,b,c,d|X′,Y′) 1 logX′−logX ∂X logY′−logY ∂Y
= [( )T +( )T ]
∂a σ2 X ∂a Y ∂a
∂logp(a,b,c,d|X′,Y′) 1 logX′−logX ∂X logY′−logY ∂Y
= [( )T +( )T ]
∂b σ2 X ∂b Y ∂b
(65)
∂logp(a,b,c,d|X′,Y′) 1 logX′−logX ∂X logY′−logY ∂Y
= [( )T +( )T ]
∂c σ2 X ∂c Y ∂c
∂logp(a,b,c,d|X′,Y′) 1 logX′−logX ∂X logY′−logY ∂Y
= [( )T +( )T ]
∂d σ2 X ∂d Y ∂d
where the posterior with uniform priors in Eq.64 is used. The parameter sensitivities
(∂X,∂X,∂X,∂X) can be solved by differentiating the LV dynamics in Eq.5.
∂a ∂b ∂c ∂d
Implementationandresults Implementationdetailsandsupplementaryresults
are listed as follows:
EParVI The uniform prior space spans [0.001,1]×[0.001,0.05]×[0.001,0.05]×
[0.001,1]. Each dimension has 40,20,20,40 grid points, respectively. Total number of
positivechargesMpos =40×20×20×40=640000. Totalnumberofnegativecharges
Mneg = 400. Positive charge magnitudes q = qlogp(θ |X′,Y′) are pre-calculated
j j
before all iterations, with maximum magnitude q = 10−5. Noise level σ = 0.
noise
Euler proportionality constant τ = 0.01. Particles are evolved for T = 82 iterations.
Each iteration takes ∼ 2700 seconds to run. Overall forces across negative charges
arenormalisedineachiterationbeforeEulerupdates. Totalruntimefor82iterations
is ∼ 221400 seconds. Particle positions marginally change after 50 iterations. The
empirical,marginaldistributionforeachdimension,calculatedfromthe400particles,
is pictured in Fig.13.
(a) a (b) b
(c) c (d) d
Figure 13: Empirical marginal distributions for a,b,c,d.
40LMC We use similar LMC implementation as described in Appendix.E, with a =
1e−8,b = 1,c = 0.55. 400 x are initialised uniformly (same initial samples used
0
in EParVI), and use Langevin dynamics to trace each sample through 10000 steps.
Total runtime is 24178 seconds. The final positions of the 400 samples are presented
inFig.14. Thefinalshapesaresomewhatconsistentwiththetheoreticalsnapshotsin
Fig.7, with much dispersion.
Figure 14: EPaVI: initial and final particle distributions.
HMC TheNo-U-Turnsampler(NUTS)[28],anadvancedHMCimplementation,is
used with PyMC3 library [48]. The author crafted the specific ODEOp and solver.
Twochains,eachwith1900samples,aredrawnwithtargetacceptancerate0.8. Total
runtime is 366 seconds. The first chain is used, with 1500 burn-in samples discarded.
Initial conditions used are x =33.956,y =5.933. Note that, the likelihood used for
0 0
HMC is x′(t ) ∼ N(x(t ),(e0.25)2) and y′(t ) ∼ N(y(t ),(e0.25)2), which is different
i i i i
from that of EParVI specified in Eq.8.
The two chains are presented in Fig.15, with left showing the sample histogram
based KDE density and on the right sample trajectories. We observe that the two
chainsagreeontheestimations,andthesamples,althoughsmallnumbers,concentrate
around the the means and modes. We extract one chain (yellow one in Fig.15) and
take a closer look in Fig.16 at the sample estimates, we observe that, given our HMC
model, the results are very close to the reference values (note the small horizontal
scales). TheseempiricallyestimatedvaluesaresummarisedinthethirdrowofTable.2.
WethenmakepredictionsusingtheempiricallyestimatedposteriormeansandMAPs
based on this single chain, as shown in Fig.17. Again, we observe good match with
the real world recorded data.
41Figure 15: HMC samples. In the figure, α=a, β =b, δ =c,
γ =d.
(a) a
(b) b
(c) c
(d) d
Figure16: Lotka-Volterra systemidentification: HMCinferenceofθ =[a,b,c,d]
using the real-world data as shown in Fig.6. Red dashed vertical line is the
reference value from [5].
42Figure 17: HMC inferred posterior predictives. Left: MAP-based predictions.
Right: mean-based predictions.
SVGD Total number of initial particles: M=2000, overall number of iterations:
L=100, kernel: RBF (2h2=med2/logM where med is the median distance between
particles),learningrate: ϵ=0.001/∥ϕ∗(zl)∥ ,whereiistheparticleindex,l=1,2,...,L
i 2
is the iteration number. Total runtime time: 8662 seconds. Valid number of particles
after 100 iterations: 1454.
EVI EVI is not used due to our intractability of negative states encountered in
simulating the LV dynamics when calculating the gradients.
H Further discussions
Here we discuss more aspects and prospects related to the method we proposed.
Hyperparameters Hyper-parameters used in the SPH method include: number
of positive charges (number of mesh grid points) Mpos, number of negative charges
Mneg, negative charge magnitude (normally set to be 1.0), maximum positive charge
magnitudeq,totalnumberofiterationsT,positionupdatingparametersτ,τ′ and∆t,
noise level σ, and others (e.g. annealing parameters).
EParVI doesn’t have many parameters to tune - most of them can be empirically
specified. NumberofpositivechargesMpos dependsonthegranularitytobeachieved,
it relies on dimension of the problem m, and it has great impact on the inference
accuracy. It is set to be Mpos =Mm if regular, equidistant mesh grid is used. Mneg
canimpacttheinferencequalityaswell,ingeneralitcanbemuchsmallerthanMpos,
and there is no hard requirement on it. the maximum positive charge magnitude q
canbeconvenientlychosenbytrialanderror,onecanchooseittomakerepulsiveand
attracting forces are on the same order in a trial step. T should be large enough to
ensure a transient or steady state which is close to the target (one can monitor some
metricssuchasiteration-wisedensityorclusteringmetrictodecidewhentoterminate).
Small values for ∆t, τ and τ′ are required; adaptive schemes can be designated, i.e.
inregionswheretheattractingforceislarge(i.e. highdensityregimes),smallermove
is expected. One can also use different values for different dimensions (otherwise
one dimension may converge fast while other dimensions not). For example, if the
parameters to be inferred are of different scales, e.g. a,d ∈ [0,1] while b,c ∈ [0,0.05]
in the LV system, we can use different Euler proportionality constant τ and different
stepsizes∆tanddampingconstantτ′ fora,dandb,c. Thenoiselevelσ canperturb
43particlepositionstoavoidgettingtrappedinlocaloptimalconfiguration(Eq.10);itis
normally set to be small as compared to the move. There are other implementation
tricks, as listed in Appendix.F, to facilitate the inference procedure.
Effect of mesh grid Itisreportedby[57]that,hexagonalstructureistheoptimal
structurein2Dforachievingequilibrium;however,thiscannotbewellrepresentedon
the rectangular grid. If the rectangular grid is to be used, unpleasant artefacts such
as multiple particles cluster at the same grid position can be resulted. This can be
solved by constraining the particles to concentrate on moving towards nearest grid
point, which can be realised by adding an extra, distance-inverse force component to
theforceactingoneachparticle(Eq.1s)whichregularizeslargemovementfromnearby
grid points.
A temporal graph network perspective Dynamic movement of a group of
interactingparticlescanbeviewedastheevolutionofaconnectedgraphnetworkover
time. Onecanusenetworkanalysistechniquestoidentifytheimportantcontributing
edges for each node at at any cross-section of time.
Insightsfrommoleculardynamics Highlyaccurateandefficientfirst-principle
based molecular simulations provide insights as well as a wealth of training data for
modellingIPS[12]. Forexample,accuratequantumchemistryalgorithms,e.g. density
functional theory (DFT), have made it possible to build fast and highly accurate
models of water [3]. Neural network potentials (NNPs[12]), for example, learn the
interaction between molecules based on quantum chemistry data and is capable of
predictingtheatomicpotentialfieldinamolecularsystem. Itcancapturethecomplex
interactionpatternsbetweenmoleculeswithhighcomputationalefficiency(O(M))and
accuracy. These insights can be brought to create analogy when devising a particle
system.
Sample clustering for multi-modal inference In the post-sampling stage,
one may be interested in empirically estimating the statistics of p(x) for use of e.g.
posteriorpredictive inBayesianmodelling. Summarisingthestatisticsofadistribution
isnoteasy,particularlywhenit’shigh-dimensionalandmulti-modal. Withthe(quasi)
samples drawn, we can perform e.g. clustering to group the samples and summarise
the statistics in each cluster. This can be achieved using available clustering and
partition algorithms such as k-d-tree.
Learningp(x)orlogp(x) Insomecases,directlylearningp(x)canbenumerically
unstable,especiallywhenp(x)isverysmallorlarge(e.g. chainproductsoflikelihoods).
Working with logp(x) helps to mitigate these issues because the log-transformation
compresses the range of values, leading to more stable numerical computation. Also,
the gradients of the log-probability ∇logp(x) are more stable and easier to handle
numerically than the gradients of p(x) itself, and therefore used commonly in ParVI
(e.g. SVGD). However, for ParVI methods, if one decide to performance inference on
logp(x),thenadensityestimatorisneededtorecoverp(x)=elogp(x). Thisisthecase
with Section.5.4.
Change of variables Forvector-valuedrandomvariablex′ andxwholiveinthe
same dimension Rm, the change of variables principle applies:
44∂x ∂f (x′)
p(x)=p′(x′)/|detJ(x′)|=p′(x′)/|det( )|=p′(x′)/|det[ θ ]| with x=f(x′)
∂x′ ∂x′
(66)
where
|det[∂fθ(x′)]|
is the absolute value of the determinant of the Jacobian matrix
∂x′
Jf (x ),i = 1,2,...M. If a density is transformed multiple times, the chain rule of
θ i
productcanbeusedtoarriveatthefinaltransformedpdf,yieldinganormalizingflow
25. TheEParVIinferencealgorithmforchangeofvariablesispresentedinAlgo.2. Note
that, It requires computing the determinant of the Jacobian matrix of the transform,
which can be computationally expensive for high dimensions.
Algorithm 2: EParVI inference for change of variables
• Inputs: target probability density function p(x) with x∈Rm;
probability density function p′(x′) with x′ ∈Rm. Number of particles M.
• Outputs: Particles {x ∈Rm}M whose empirical distribution
i i=1
approximates the target density p(x).
1. Generate samples in low dimensions. Use EParVI to generate sample
particles x′ ∼p′(x′), i=1,2,...M.
i
2. Neural normalizing flow training. Initialize a neural network f :
θ
Rm →Rm which accepts x′ and outputs x.
(1) For each particle i, compute x = f (x′) and individual loss
i θ i
L =−logp(x )−log|detJf (x′)|.
i i θ i
(2) Compute mean loss 1 (cid:80)M L . Back-propagating the error to update
M i=1 i
network weights.
2. Inference and original sample reconstruction. Mapping x′ to x .
i i
(1) For each particle x′,i=1,2,...,M, project it using the trained neural
i
network: x =f (x′).
i θ i
(2) Perform density estimation (e.g. KDE) to obtain pˆ′(x′).
i
(3) Estimate target density pˆ(x )=pˆ′(x′)/|detJf (x′)|.
i i θ i
3. Return x ,i=1,2,...,M and the estimated density pˆ(x).
i
Generalise to high dimensions Eq.66 requires a bijective and differentiable
map x=f (x′). If we have the low dimensional samples x′ ∈Rm′ , e.g. produced by
θ i
EParVI,andwewanttoprojectthemintohigherdimensionalsamplesx ∈Rm,with
i
m′ <m,i=1,2,...M. Thatis,wefirstgeneratesamplesx′ ∼p′(x′)inlowdimensions,
which is feasible using EParVI, and then project the low-dimensional samples into
high dimensions via x = f (x′), obtaining x ∼ p(x) and preserving the probability
θ
structure. Thechangeofvariablesprinciplecannolongerbeappliedinthiscase(the
Jacobianmatrixisnon-square). Instead,weemployaneuralnetworktorepresentthis
25Normalizing flows [52] allow for learning a complex transformation between probability
distributions.
45non-linearmappingx=f (x′),withthetrainingobjective26 L(x′)=−logp(x )−L′,
θ i i i
whereL′ isameasureofthecontributionoftheprojectedsamplex tothediversityof
i i
p(x)intheprojectedspaceRm,ensuringthemappingproperlycapturesthestructure
of the target pdf p(x). The algorithm is presented in Algo.3.
Algorithm 3: Projected EParVI inference
• Inputs: target probability density function p(x) with x∈Rm;
probability density function p′(x′) with x′ ∈Rm′. Number of particles
M.
• Outputs: Particles {x ∈Rm}M whose empirical distribution
i i=1
approximates the target density p(x).
1. Generate samples in low dimensions. Use EParVI to generate sample
particles x′ ∼p′(x′), i=1,2,...M.
i
2. Neural network training. Initialize a neural network f : Rm′ →Rm
θ
which accepts x′ and outputs x.
(1) For each particle i, compute x = f (x′) and individual loss
i θ i
Li=−logp(x )−L′.
i i
(2) Compute mean loss 1 (cid:80)M L . Back-propagating the error to update
M i=1 i
network weights.
2. Inference and original sample reconstruction. Mapping x′ ∈Rm′ to
i
x ∈Rm.
i
(1) For each particle x′,i=1,2,...,M, project it using the trained neural
i
network: x =f (x′).
i θ i
(2) Perform density estimation (e.g. KDE) to obtain pˆ(x).
3. Return x ,i=1,2,...,M and the estimated density pˆ(x).
i
Algo.3 provides a way to estimate high-dimensional distributions by working in
a lower-dimensional space, which can be more computationally efficient. Quality of
the final estimation depends on the expressiveness of the neural network, design of
the loss function and the training process. The diversity loss L′ can be designed
i
using strategies for example: (1) Product of singular values of the Jacobian. This
gives a measure of the ’volume change’: L′ = (cid:80)m′ logσ [Jf (x′)], where σ are
i j=1 j θ i j
the singular values of the Jacobian Jf . This approach accounts for expansion in
θ
all directions, and it is computationally expensive (routinely, SVD is performed to
obtain the sigular values). One can use only the largest singular value which gives
an upper bound on the expansion. (2) Project the Jacobian onto an orthonormal
basis in the higher-dimensional space: L′ = log|det[Jf (x′i)TJf (x′)]|1/2. (3) Use
i θ θ i
the trace of the Gram matrix of the Jacobian as a measure of magnitude: L′ =
i
logTr[Jf (x′i)TJf (x′)]. (4)UsetheFrobeniusnormoftheJacobianasasimplescalar
θ θ i
measure: L′ = log|Jf (x′)| . Each of these loss designs provides a way to capture
i θ i F
the transformation’s properties, by quantifying the ’expansion’ or ’contraction’ of the
transformation without requiring a square Jacobian.
26Note that, regularization term can be added to promote sample diversity and smooth
mapping.
46