Web2Code: A Large-scale Webpage-to-Code Dataset
and Evaluation Framework for Multimodal LLMs
SukminYun∗,1,4,HaokunLin∗,1,RusiruThushara∗,1,MohammadQazimBhat∗,1,
YongxinWang∗,1,ZutaoJiang1,MingkaiDeng2,JinhongWang1,TianhuaTao1,3,
JunboLi1,HaonanLi1,PreslavNakov1,TimothyBaldwin1,ZhengzhongLiu1,5,
EricP.Xing1,2,5,XiaodanLiang1,6,ZhiqiangShen1
1MBZUAI,2CMU,3UIUC,4HYUERICA,5Petuum,6SYSU
https://mbzuai-llm.github.io/webpage2code/
Abstract
Multimodal large language models (MLLMs) have shown impressive success
acrossmodalitiessuchasimage,video,andaudioinavarietyofunderstandingand
generationtasks. However,currentMLLMsaresurprisinglypooratunderstanding
webpagescreenshotsandgeneratingtheircorrespondingHTMLcode. Toaddress
thisproblem,weproposeWeb2Code,abenchmarkconsistingofanewlarge-scale
webpage-to-codedatasetforinstructiontuningandanevaluationframeworkfor
thewebpageunderstandingandHTMLcodetranslationabilitiesofMLLMs. For
datasetconstruction,weleveragepretrainedLLMstoenhanceexistingwebpage-
to-code datasets as well as generate a diverse pool of new webpages rendered
intoimages. Specifically,theinputsarewebpageimagesandinstructions,while
theresponsesarethewebpage’sHTMLcode. Wefurtherincludediversenatural
languageQApairsaboutthewebpagecontentintheresponsestoenableamore
comprehensiveunderstandingofthewebcontent. Toevaluatemodelperformance
inthesetasks,wedevelopanevaluationframeworkfortestingMLLMs’abilitiesin
webpageunderstandingandweb-to-codegeneration. Extensiveexperimentsshow
thatourproposeddatasetisbeneficialnotonlytoourproposedtasksbutalsointhe
generalvisualdomain,whilepreviousdatasetsresultinworseperformance. We
hopeourworkwillcontributetothedevelopmentofgeneralMLLMssuitablefor
web-basedcontentgenerationandtaskautomation. Ourdataandcodeareavailable
athttps://github.com/MBZUAI-LLM/web2code.
1 Introduction
Multimodal large language models (MLLMs) have achieved explosive growth in the past few
years. Leveragingtherichcommonsenseknowledgeinlargelanguagemodels(LLMs),MLLMsare
remarkablysuccessfulatprocessingandreasoningaboutvariousmodalitiessuchasimage[2,35],
video[60,53],andaudio[40]inabroadrangeoftaskssuchasrecognition[52],reasoning[59],and
question-answering[39],allusinglanguageastheintermediaterepresentation. However,existing
MLLMsaresurprisinglypooratunderstandingwebpagescreenshotsandgeneratingtheHTMLcode
to express their latent states. For instance, given the instruction “Parse the HTML code for this
webpage”,thewell-knownLLaVA-1.5[33]generatesgeneric,palecodethatfailstopreservemostof
theoriginalwebpage’sfeatures(seeFigure1),whichhampersitsutilityinapplicationssuchasUI
prototyping,automationagents,andaccessibility(e.g.,notingavailablebuttonsandoptionsgiven
webpagescreenshot).
∗EqualContribution.
Preprint.Underreview.
4202
nuJ
82
]VC.sc[
1v89002.6042:viXra(1)Originalwebpages
(2)TrainedonLLaVAdatasetonly
(3)Usingourwebpagedataset
Figure 1: Our motivation for constructing the Web2Code dataset stems from the limitations of
previousmodels,suchasLLaVA[33],whicharetrainedongeneraldatasetsandstruggletogenerate
high-qualitywebpages,asinthesecondrow. Ourdatasetaimstosignificantlyenhancethequalityof
webpagegenerationasinthirdrowwhilemaintainingastronglevelofgeneralmultimodalability.
The essential ingredients behind the progress in MLLMs are arguably large-scale instruction
datasets [9, 63] and evaluation benchmarks [16, 58] – the former for aligning multimodal inputs
withthemassiveknowledgeinLLMs[27,35],andthelatterforstandardizedcomparisonwhich
facilitates model development. However, existing instruction datasets and benchmarks typically
focusongeneralsettings(e.g.,visualQAandreasoning)andpayinsufficientattentiontowebpage
understandingandwebpage-to-codegeneration,whichrequiresauniquecombinationofcapabilities
such as optical character recognition (OCR), spatial reasoning, and long-text generation, among
others. Whilepreviousworkhasdevelopeddatasetsforthesetasks[4,22], theylackinstruction
informationandareunsuitableforintegrationwithgeneral-purposeMLLMs. Ontheotherhand,
popularbenchmarks[36,25]evaluatesomeoftherequiredcapabilitiesinisolation,butnotfullyin
combinationforvisualparsingandreasoningoverwebpages.
Tofillthisgap,weproposeanewinstructiontuningdatasetandevaluationsuitenamedWeb2Code.
Web2Code contains a total of 1179.7k webpage based instruction-response pairs. The responses
consistofnotonlytheHTMLcode,butalsothestructuredquestionsandanswersaboutthewebpage,
whichassistamodelinbetterunderstandingitsinformation. Fordatasetcollection,weuseGPT-3.5
andGPT-4tocleanexistingdata(e.g.WebSRC[10])aswellastogeneratecompletelynewwebpages
inHTMLcode. ToevaluatetheMLLM’ssuccessatwebpageunderstandingandHTMLparsing,we
proposetheWebpageUnderstandingBenchmark(WUB)andWebpageCodeGenerationBenchmark
(WCGB),twotasksthattestthemodel’sabilitiestoanswerquestionsaboutawebpageandgenerate
itsHTMLcode,respectively. Forthelattertask,wefindthattraditionaltextsimilaritymetricsare
insufficientforevaluatingthefidelityofthegeneratedcode,andinsteadproposetorendertheoutput
HTMLbacktoawebpagescreenshot,anduseGPT-4V[42]toevaluatethequalityoftheresulting
webpage[61].
Todemonstratetheutilityofourdataset,wetrainLLaVA-styleMLLMswithourdatasetincluded
intheinstructionfinetuningstage. Quantitativeresultsshowthatfinetuningonourdatasetnotonly
clearlyimprovestheimage-to-HTML-codetranslationabilityoftheMLLM,butalsoleadstoslight
improvementsinthemodel’sperceptionandreasoningabilitiesingeneralbenchmarks. Incontrast,
including previous datasets without our data conversion leads to degraded general performance,
suggestingourdatasetissuitableforinclusioninMLLMinstruction-tuningforadditionalcapabilities
withoutimpactingexistingones.
2 RelatedWork
MLLM Dataset. At present, there is a substantial amount of large-scale visual instruction data,
primarilygeneratedusingGPT.SVIT[63]andLRV-Instruction[32]arebothgeneratedbyGPT4
2basedonmanualpromptstoadjusttheinstructiondata,includingrichhigh-qualitydialoguequestion
and answer, complex reasoning question and answer, reference question and answer, and image
detaildescriptiontaskdatasets;similarly,ShareGPT4V[9],LLaVAR[62],LVIS-Instruct4V[55]use
GPT-4V[42]togeneratemillionsofhigh-qualityimage-textpairs,aimingtoenhancetheperception,
reasoningandplanningcapabilitiesofMLLM.CommonlyusedimagedatasourcesincludeLAION
[48],CC[49],SBU[46],COCO[31],VG[21],VQAv2[18].
MLLM.InstructionTuningMLLMhasmadegreatprogressinrecentyears. ThestructureofMLLM
usuallycontainsavisualencoder,vision-languagemappingmoduleandLLM.LLaVA-v1.5[33]only
usesMLPasthevision-languagemappingmoduleandthesuccessfulapplicationofinstructiontuning
onMLLMhasinspiredpeople.Thecommunityhasexploredvariousfeasiblestructures,whichcanbe
dividedintoattentionstructuresBLIP2[27],InstructBLIP[12],Qwen-VL[3],ShareGPT4V[9]and
non-attentionstructuresLLaVA[33],Shikra[8]accordingtothevision-languagemappingmodule.
Atthesametime,variousopensourceandmorepowerfulLLMs,suchasVicuna1.5[11],InternLM2
[5]alsohelpMLLMachievericherandmoreextensiveinstructionfollowingcapabilities. Qwen-VL
[3],OtterHD[24],mPLUG-Owl[56],InternLM-XComposer2-4KHD[13]increasetheresolutionof
images,whileLLaVA-NeXT[34],Mini-Gemini[28],MM1[41]splittheinputimageintoseveral
imagecrops. Inaddition,BRAVE[19],MoVA[64],DeepSeek-VL[38],OmniFusion[17]apply
supplementaryvisionencoderstoobtainabundantvisualfeatures, e.g. DINOv2[43], SAM[20].
Furthermore, more computer vision models are utilized for different tasks, which include image
segmentation,detectionandOCR,inMOAI[23],CuMo[26]andSpatialVLM[7]. Subsequently,
MoE[30]wasappliedtoMLLMtoexpandthescaleoftrainingdataatthesamecomputingscale.
CodeStudy. TherearevariouscodestudiesrelatedtoLLM.Sarkeretal.[47]focusedongenerating
codefunctionsthroughformulahints,tryingtoimprovetherobustnessofthesyntax,andsystem-
aticallytestedtherobustnessofthesyntax. Fromtheperspectiveofsecurity. Finkmanetal.[15]
said that these code assistance tools may inadvertently disclose the developer’s proprietary code
to the code assistant service provider in the process of helping development, thus they proposed
a complementary method to reduce the risk of code leakage while providing effective advice to
developers. Inadditiontocodeleakage,thecodegeneratedbyLLMhasalsocausedconcernsin
industriesandotherfields. Toaddressthisissue,Yeetal.[57]proposedanewzero-shotsynthetic
codedetectorbasedonthesimilaritybetweencodeanditsrewrittenvariants. Intheevaluationwork
oncodegeneration,Duetal.[14]proposedanewcomputationalefficiencybenchmarkMercuryand
anewmetricBeyondfortheefficiencyevaluationofcode. Theyexperimentallyshowedthatdirect
preference optimization can be used as a robust baseline for improving computational efficiency
comparedtosupervisedfine-tuning,whichpavesapromisingpathforfutureexplorationofefficient
codegeneration.
3 DatasetConstruction
Overview. OurWeb2Codeinstructiontuningdatasetconstructionandinstructiongenerationprocess
involvesfourkeycomponents: (1)Creationofnewwebpageimage-codepairdata: Wegenerated
high-qualityHTMLwebpage-codepairsfollowingtheCodeAlpacaprompt[6]usingGPT-3.5and
convertthemintoinstruction-followingdata. (2)Refinementofexistingwebpagecodegeneration
data: WetransformexistingdatasetsincludingWebSight[22]andPix2Code[4]intoaninstruction-
followingdataformatsimilartoLLaVAdata[33],sotheycanbeusedasinstruction-followingdata
totrainMLLMs. (3)Creationofanewtextquestion-answerpairdata: Wegeneratedanewquestion-
answerpairdatasetutilizingournewGPT-3.5generateddatafrom(1)forwebpageunderstanding.
(4)Refinementofexistingwebpageunderstandingdata: WerefinetheWebSRC[10]question-answer
datatoimproveitsqualityusingtheGPT-4. Eachcomponentiselaboratedindetailasfollows:
DWCG: Creation of new webpage image-code pair data for code generation. To augment
ourdatasetwithhigh-qualitydata,weemployedGPT-3.5togenerate60KHTMLpagesfollowing
theguidelinesandpromptsinCodeAlpaca[6]. UsingSeleniumWebDriver,wethencreatedweb
imagescreenshotsfromthegeneratedHTMLcode. Thesewebimage-codepairsweresubsequently
convertedintoaninstruction-followingdataformatsimilartotheLLaVAdataformat[33],enabling
theiruseintrainingMultimodalLargeLanguageModels(MLLMs). Theexampleoftheinstructionis
showninFigure16. ThegenerationofinstructionwasdoneintwostagesusingpromptsfedtoGPT-4:
(a)Duringstage1,thepromptshowninFigure12resultedinthecreationofgenericinstructions. (b)
Thiswasfollowedbyaugmentingtheinstructionfrom(a)withtheGPTgeneratedinstructionsusing
3Q Describe the procedure for someone interested in Q Identify the main color used on the 'Add to Cart'
volunteering. buttons.
A An interested individual can volunteer by filling A The'AddtoCart'buttonshaveabackgroundcolorof
out the form provided in the 'Volunteer' section of the #7cb342,whichisashadeofgreen.
website. This form asks for the person's full name,
email, phone number, and area of interest. Additionally, Q What items are currently in the cart, and can you
the person must check a box agreeing to the terms and statetheirprices?
conditionsbeforesubmittingtheform.
A There are two items in the cart: 'Trendy Sneakers'
Q Whatisthemaingoaloftheorganization,according pricedat$49.99and'StylishHandbag'pricedat$69.99.
totheirmissionstatement?
Q Calculatethetotalcostifauserdecidestoremove
A The main goal of the organization, according to the'StylishHandbag'fromthecart.
their mission statement, is to provide quality education
and opportunities to underprivileged communities, A If the 'Stylish Handbag', which costs $69.99, is
empowering individuals to achieve their full potential removedfromthecart,thetotalcostwouldbetheprice
andbreakthecycleofpoverty. ofthe'TrendySneakers',whichis$49.99.
Figure2: Qualitativeexampleofgeneratedquestion-answerpairdataset. Questionscoverdiverse
aspectsofthewebpageunderstanding.
Question: Where will I be working if I choose
Question: Where will I be working if I choose Unlimited Commission -Work from Home?
Unlimited Commission -Work from Home? Answer: Western Cape
Answer: Western Cape
Question: What is the location of Unlimited Commission
Question: What is the location of Unlimited Commission -Work from Home?
-Work from Home? Answer: The Unlimited Commission -Work from Home job is
Answer: Western Cape located inWestern Cape.
Question: Where is the job Sales Representative Question: Where is the job Sales Representative
located? located?
Answer: Johannesburg Answer: The Sales Representative job is based in
Johannesburg.
Question: Where will I be working if I choose Sales
Representative? Question: Where will I be working if I choose Sales
Answer:Johannesburg Representative?
Answer:Johannesburg
Question: Where is the job Work from home -Call
centre agent / lead warmer located? Question: Where is the job Work from home -Call centre
Answer: Gauteng agent / lead warmer located?
Answer: The job is located inGauteng.
Question: What is the location of Work from home -
Call centre agent / lead warmer? Question: What is the location of Work from home -
Answer: Gauteng Call centre agent / lead warmer?
Answer: Gauteng
Question: When was the job Sales Representative
posted? Question: When was the job Sales Representative
Answer: 21 Oct 2020 posted?
Answer: The Sales Representative job was posted on 21
Question: When was the job offer Sales Representative Oct 2020.
posted on the internet?
Answer: 21 Oct 2020 Question: When was the job offer Sales Representative
posted on the internet?
Answer: 21 Oct 2020
Figure3: WebSRCdatarefinementforimprovedQuality. Left: Beforerefinement;Right: After
refinement,thequalityhasbeenimprovedandduplicationshavebeenexcluded.
thepromptshowninFigure13toincludestylisticinformation. Thisallowedthemodeltolearntwo
styles: ModernandBootstrapstyleasshowninFigure21andFigure22,respectively.
DWCGR: Refinementofexistingwebpagecodegenerationdata. Toenhancethecapabilityofour
modelinthetaskofHTMLcodegeneration,weleveragethePix2code[4]andWebSight[22]datasets.
TomitigatethedetrimentalimpactonmodelperformancefromrandomlettersinPix2Codedata,
wereplacetheserandomletterswithmeaningfultextusingGPT-4,therebyrefiningthewebpages
intodiversewebpagesencompassingproductlandingpages,personalportfolios,blogs,andother
categories. Wethenvisuallyrenderedeachsamplebytakingscreenshotsofthebrowserviewofeach
webpage. Further,weconvertallthesedataintoLLaVAinstructionfollowingdataformatusingthe
samestrategyasusedforDWCG.WenotethatDWCGandWebSightwebpagesfollowModernstyle
whilePix2CodefollowBootstrapstyle.
DWU:Creationofanewquestion-answerpairdataforwebpageunderstanding. Forthepurpose
offine-tuningourmodelsthroughaninstruction-followingmanner,weutilizedthecapabilitiesof
GPT-4togeneratewebpagecode-basedquestion-answerpairs. Wegenerated10question-answer
pairsusingGPT-4forasubsetof24.35Kwebpagedata,resultinginatotalof243.5Kquestion-answer
4datapoints. Thisincludes,asetof230Kquestion-answerpairsforGPT-3.5basedwebpages,aset
of13.5KnewlygeneratedquestionanswerpairsforrefinedPix2Codeimages. Thesepairswere
meticulouslycraftedtoalignwithourimage-basedevaluationcriteria,ensuringthateachquestion
probesspecificaspectsofthevisualandcontentqualityreflectedinthegeneratedwebimages. This
strategyenhancesthemodel’sperformancebyintegratinganuancedunderstandingoftheevaluation
parameters into its learning process. Figure 2 shows a qualitative example from our generated
question-answerpairs. Thepromptusedforquestion-answergenerationisshowninFigure11.
DWUR: Refinement of existing webpage understanding data. To increase our instruction-
following dataset with high-quality instruction-following examples for webpages, we integrated
theWebSRCdatasetintoourtrainingregime. Priortoinclusion,wemeticulouslyfilteredtheexisting
question-and-answerpairsfromtheWebSRCdatasettoensurerelevanceandquality. Thisinvolves
duplication removal and quality optimization as shown in Figure 3. Specifically, we found that
WebSRCdatacontainsseveralquestionsrelatedtothesameanswer. Tothisend,wefirstremoved
thoseduplicatesandthenemployedGPT-4toassessandenhancethequalityofanswers. Thisprocess
notonlyrefinedthedatasetinto51.5Khigh-qualityinstructiondatabutalsoensuredthatthemodel’s
trainingwasinfluencedbyhigh-fidelity,instructionallysounddata,therebyimprovingitsabilityto
followcomplexweb-basedinstructions.
3.1 StatisticsandAnalysis
Figure4showsthewordcloudoftheanswersetofourquestion-answerdataset. Thewordcloud
highlightsthemostfrequentlyoccurringterms,with"section,""color","button",and"website"being
themostprominent,indicatingastrongemphasisonstructuralanddesignelementsinthedata. This
reflectsthedetailedfocusonthelayoutandvisualaspectsofthedataset.
Figure5illustratesthedistributionofthemostcommonHTMLtagsinourGPT-3.5generatedHTML
data. Thedistributionshowsahighfrequencyofessentialstructuraltagssuchas<div>,<p>,<meta>,
<img>,and<a>,indicatingthatthegeneratedpagesincludeadiverserangeofelementsnecessaryfor
richandvariedwebcontent. Thesignificantpresenceof<h2>,<input>,<html>,<head>,and<body>
tagsfurtherreinforcesthecompletenessandstructuralintegrityofthegeneratedHTMLdocuments.
ToestimatethedifficultylevelsofourHTML-basedwebpagedataset,weprovideseveralquantitative
measures and compare them with recent and similar existing datasets, namely WebSight [22],
Design2Code[50],andPix2Code[4](SeeTable1).
Design2Codeisprimarilyusedfortestingandhasasmallsizeof484examples,limitingitsversatility
and robustness. In contrast, our dataset, intended for both training and testing, is significantly
larger(884.7Kexamples)andmorecomplex,makingitmoresuitablefordevelopingrobustmodels.
Overall,ourbenchmarkexamplesaremorechallengingandcoverabroaderspectrumofcomplexities
comparedtoprioreffortssuchasWebSight.
Dataset WebSight[22] Design2Code[50] Pix2Code[4] DWCG(ours) DWCGR(ours)
Instruction - - - ✓ ✓
Source Synthetic Real-World Synthetic Synthetic Synthetic
Size 823K 484 1.7K 60K 824.7K
AvgLength(tokens) 647±216 31216±23902 658.7±98.0 471.8±162.3 652.85±157.0
AvgTagCount 19±8 158±100 51.6±8.0 28.1±10.6 35.3±9.0
AvgDOMDepth 5±1 13±5 8.0±0.0 5.3±1.0 6.5±1.0
AvgUniqueTags 10±3 22±6 17.0±0.0 13.6±2.7 13.5±2.5
Table 1: Comparison of dataset statistics among webpage code generation datasets: WebSight,
Design2Code,Pix2Code,ourDWCG,andourDWCGR. DWCGisanewlygeneratedGPT-3.5-based
dataset,whileDWCGRistherefineddatasetthatutilizesWebSightandPix2Codedatasets.
3.2 Distribution
Dataset DWU DWUR
Ourinstruction-followingdatasetcontains1,179.7Kinstruction
data points. This includes 884.7K website image-code pairs Instruction ✓ ✓
Size 243.5K 51.5K
and295Kquestion-answerpairs.
The295Kquestion-answerpairsconsistof243.5KGPT-4based Table2: DistributionofDWUand
question-answerpairs(DWUData)and51.5Kquestion-answer DWUR datasets. Bothdatasetsin-
pairs from WebSRC image-based data, as shown in Table 2. cludehigh-qualityquestion-answer
Our evaluation dataset comprises 1,198 webpage screenshot pairsforwebpageunderstanding.
5250K
200K
150K
100K
50K
0K
div p meta img a h i2 nput html head body title h3 style lih1 lab bel utto on ption for hem ader
HTML Tags
Figure4: WordCloudfortheanswersetofthe Figure5: Thedistributionofmostcommon20
GPT4basedDWUdataset. tagsinGPT-3.5basedHTMLdata.
images,sourcedfromdiverseorigins,includingWebSight,Pix2Code,GPT-3.5baseddata,andmanual
processes,ensuringabroadrepresentationofwebcontent. Additionally,weutilize5,990"yes"/
"no"question-answerpairsgeneratedfromtheGPT-4VisionAPIforourWebpageUnderstanding
Benchmark,asshowninSection4.1.
4 ANewEvaluationFrameworkforWebpage
Ourproposedevaluationframeworkincludestwoschemes: (1)WebpageUnderstandingBenchmark
(WUB):Anofflineevaluationusing"yes"/"no"questions.(2)WebpageCodeGenerationBenchmark
(WCGB):Anonlineevaluation(usingGPT-4Vision)basedonimagesimilarity.
4.1 EvaluationMetricforHTMLCodeGeneration
Intherealmofassessingcodequality,particularlyintermsoffinalvisualappealandoverallfunction-
ality,existingmethodsthatrelyoncodesimilaritymetricsfallshort. Thesetraditionalapproaches
often lack the precision and reliability needed for nuanced evaluations of code effectiveness. To
addresstheseshortcomings,wehavedevelopedanovelapproach: regeneratingthewebpageusing
themodel’spredictedHTMLcodeandcapturingscreenshotsofthesegeneratedwebpages. This
process,automatedusingtheSeleniumWebDriverextensioninPython,shiftsthefocusfromtheless
reliablecodesimilarityassessmentstoamoreaccurateandvisuallyorientedmethod. Bycomparing
imagesofthegeneratedwebpages,wecanmoreeffectivelyevaluatetheaestheticandfunctional
aspectsofthecode,offeringamorecomprehensiveunderstandingofitsquality.
Question: Please Generated GT webpage Question: ….
generate the HTML webpage Answer: Yes / No
code for the given
Question: ….
image.
Answer: Yes / No
GT webpage
MLLM
MLLM
Question id:
Answer: Yes / No
GPT4 Vision
Question id:
Answer: Yes / No
• Visual Structure and Alignment
• Textual and Content Consistency
GPT based Q&A based
Evaluation • Color and Aesthetic Design • Accuracy Score Evaluation
• User Interface and Interactivity
Webpage Code Generation Benchmark (WCGB) Webpage Understanding Benchmark (WUB)
Figure6: Evaluationbenchmarkforwebpagegenerationandwebpageunderstanding. Left: WCGB
utilizesGPT4Visionbasedonlineevaluationforimagelevelcomparison;Right: WUBemploysan
offlineevaluationbasedonquestion-answerpairs.
Weproposetwobenchmarksforassessingwebpageunderstandingandcodegenerationcapabilities.
WUB:Thisbenchmarkcomprises5,990high-qualityquestion-answerpairsgeneratedfromGPT-4
Vision API (See prompt 15), based on 1,198 webpage screenshot images, where each answer is
6
tnuoCeither "yes" or "no". These images are sourced from diverse data origins, including WebSight,
Pix2Code,GPT-3.5,andmanualprocesses,ensuringabroadrepresentationofwebcontent. Figure
10showsaqualitativesampledataweusedforWUB.Wetestthesepairsonvariousmultimodal
imageunderstandingmodelsbycomparingthepredictedanswerstothegroundtruth,withthefinal
accuracyscoreservingastheevaluationmetricasdepictedontherightsideofFigure6. Qualitative
dataexamplesinourWUBbenchmarkareshowninFigure10.
WCGB:UtilizingthesameimagesastheWUB,thisbenchmarkevaluatesamultimodalmodeltasked
withgeneratingHTMLcodefromwebpageimagesbasedonspecificinstructions. Unliketraditional
code-levelevaluations,thisbenchmarkassessesthegeneratedwebpage’sfidelityattheimagelevel.
WeconvertthepredictedHTMLcodesbackintoimagesusingSeleniumWebDrivertoallowadirect
visualcomparisonwiththegroundtruthimages. Theevaluation,depictedontheleftsideofFigure6,
considers10differentaspects,whicharefurthercategorizedintofourevaluationmatricesusingthe
GPT-4VisionAPI.Thisimage-levelevaluationprovidesamoreaccuratemeasureofthemodel’s
codegenerationcapabilities,acknowledgingthatidenticalwebpagescanbeconstructedfromvarying
codes. ThepromptusedforevaluationisshowninFigure14. Thisframeworkconsistsof10distinct
criteria,whichwegroupintofourcategories,eachencompassingspecificcriteriathatarescoredona
0-10scale,asshowninSectionDofAppendix.
4.2 QuantitativeEvaluationforHTMLCodeGenerationofMLLMs
WehaveevaluatedthetrainedmodelsusingvariousdataconfigurationsandbackbonesonourWUB
and WCGB benchmarks. The performance of the models on the code generation benchmark is
presentedinTable3,whiletheresultsforwebpageunderstandingareshowninTable4.
LLMBackbone DWCG DWU DWCGR DWUR VSA↑ CAD↑ TCC↑ UII↑ Overall↑
- - - - 3.832 3.678 3.411 3.992 3.728
✓ - - - 7.812 7.899 8.138 8.112 7.990
CystalCoder-7B[37] ✓ ✓ - - 8.010 8.102 8.266 8.124 8.126
✓ ✓ ✓ ✓ 8.023 8.071 8.321 8.334 8.187
- - - - 4.714 4.572 4.865 5.147 4.825
CrystalChat-7B[37] ✓ ✓ - - 7.900 8.001 8.204 8.215 8.080
✓ ✓ ✓ ✓ 8.384 8.287 8.417 8.488 8.394
- - - - 3.042 3.250 3.333 3.167 3.198
Vicuna1.5-7B[11] ✓ ✓ ✓ ✓ 7.876 7.687 7.267 7.563 7.598
LLaMA3-8B[1] ✓ ✓ ✓ ✓ 8.522 8.564 8.421 8.611 8.530
Table3: TheperformanceofdifferentLLMbackbonesundervariousdataconfigurationsonour
WebpageCodeGenerationBenchmark(WCGB)."VSA"denotesVisualStructureandAlignment,
"CAD"representsColorandAestheticDesign,"TCC"representsTextualandContentConsistency,
and"UII"denotesUserInterfaceandInteractivity.
LLMBackbone DWCG DWU DWCGR DWUR Accuracy(%)
- - - - 73.54
✓ - - - 71.81
CrystalCoder-7B[37]
✓ ✓ - - 73.74
✓ ✓ ✓ ✓ 73.61
- - - - 73.94
CrystalChat-7B[37] ✓ ✓ - - 73.48
✓ ✓ ✓ ✓ 74.14
- - - - 71.12
Vicuna1.5-7B[11]
✓ ✓ ✓ ✓ 71.23
LLaMA3-8B[1] ✓ ✓ ✓ ✓ 74.84
Table 4: The accuracy of webpage understanding under various data configurations and LLM
backbones. Allmodelsareinstruction-tunedandevaluatedonourWUBbenchmark. Wenotethat
thegeneraldomaindata(i.e.,LLaVA)isincludedinalldataconfigurationasdefault.
Tobespecific,ourdatasetcomponentshaveanorthogonalcontributiontotheoverallimprovements
onboththeWUBandtheWCGBbenchmarks. Table3demonstratesimprovementsinwebpage
7codegenerationqualitywhenaddingDWU,DWCG+DWU,andDWCGR+DWUR. Forexample,
the results based on instruction-tuned CrystalChat show step-wise improvements on the WCGB
benchmark when adding DWCG + DWU, and DWCGR + DWUR (4.825→8.080→8.530 on the
overallmetric). Interestingly,theinstruction-tunedCrystalChat-7Bwiththegeneraldomaindataonly
showspoorWCGBperformanceswhileitshowsonparWUBperformancecomparedtotheuseof
furtherwebpagedatasets. SimilartrendsarealsofoundintheVicuna1.5-7Bbackbone,whileadding
theproposeddatasetshowssignificantimprovements,achievinganoverallscoreof7.598.
Similarly,Table4furtherdemonstratestheeffectivenessoftheproposeddatasetonthewebpage
comprehensioncapabilities. Forexample,thefourrowsoftheinstruction-tunedCrystalCoder-7B
showthatnotonlyusingDWCGwithDWUcanpreventdegradationinlanguageunderstanding,
butalsothefullcomponentsevenenhancethewebpagecomprehensioncapabilities. Overall,we
foundtheproposeddatasetcanenhancebothwebpageunderstandingcapabilityandwebpagecode
generationabilitiesundervariousLLMbackbones,andLLaMA3-8Barchivesthebestperformance
amongallonbothwebpagecodegenerationandwebpageunderstanding.
4.3 VisualizationsforQualitativeEvaluation
AsshowninFigure7,wecomparetheresultsbetweentheoriginalimagewhichisthereal-world
webpage sample, the rendered image generated by using LLM backbones of Vicuna1.5-7B and
CrystalChat-7B,respectively. CrystalChat-7Bisacode-enhancedLLMandourvisualizationdemon-
stratesthatitachievesthebetterqualityofgenerationthanVicuna1.5-7Beventheperformanceis
slightlyworseongeneralmultimodaldomain,aspresentedinTable6. Moreover,asinFigure8,
our rendered webpage from the model trained on our web dataset closely resembles the original
image,indicatingthepositiveimpactoftheweb2codedataset. Wefurthervisualizeourgenerationin
Figure9whentheinputisahand-drawnwebpagetoexaminetheadaptationabilityofourmodel.
(a)Original (b)OurVicuna1.5-7B (c)OurCrystalChat-7B
Figure 7: Visualization comparison using different backbones. Using the code-enhanced LLM
backboneCrystalChat-7BachievesbetterqualityofgenerationthanVicuna1.5-7B.
(a)Original (b)GT-CodetoImage (c)Ours
Figure8: Visualizationcomparisonbetweenground-truthcodegeneratedimageandourresult. The
styleandlayoutofthegeneratedwebpageimagearesimilartotheground-truthimage.
HanHd adnradw dnra wwenb wpaegbepage CrysCtarylCsthaaltCohuattpouuttput
(a)Handdrawnwebpageandourgeneration (b)Handdrawnwebpageandourgeneration
Figure9: VisualizationofourCrystalChat-7Bgenerationwhentheinputisahand-drawnwebpage.
85 GeneralEvaluationofMLLMsUsingWeb2Code
SetupandOverview. OurmodeltrainingframeworkmainlyfollowsthedesignofLLaVA-1.5[33]
where we leverage the capabilities of both a pre-trained visual encoder, an LLM and a projector
toconnectvisualfeaturesintothewordembeddingspace. Themodelconsistsof(1)apre-trained
CLIPViT-L/14[44]visualencoderwitharesolutionof336×336andapatchsizeof14,whichhas
good feature representation already aligned with the text embedding space. (2) As for the LLM
backbones,weleverageCrystalChat[37]asthebasemodelandcompareitwithotherlatestLLM
backboneslikeVicuna1.5[11],LLaMA2[54],LLaMA3[1]andCrystalCoder[37]2. Trainingdetails
andhyperparametersarepresentedintheAppendixA.
GeneralEvaluationMetricsforMLLMs. MME[16]servesasanextensiveevaluativebenchmark,
aimingtoassesstheperceptualandcognitivecapabilityofMLLMswithin14sub-tasks. Addition-
ally, we also evaluate the performance of our models on text-oriented visual question-answering
tasksemployingadiversesetofbenchmarkdatasetsincludingScienceQA[39]andTextVQA[51].
Furthermore,Weassessourmodels’abilitytowardanti-hallucinationthroughPOPE[29].
LLMBackbone DWU DWCG DWUR DWCGR MME-P MME-C POPE SciQA TextVQA
- - - - 1456.53 308.21 86.86 67.77 57.84
✓ - - - 1438.51 292.14 87.10 68.27 58.15
CrystalChat-7B[37] ✓ ✓ - - 1478.82 297.14 86.13 67.92 57.41
✓ ✓ ✓ ✓ 1449.54 279.64 86.53 68.32 57.86
Table5: ComponentanalysisonCrystalChat-7Bbackboneundervariousdataconfigurations. We
notethatthegeneraldomaindata(i.e.,LLaVA)isincludedinalldataconfigurationasdefault.
EffectsofWeb2CodeonGeneralDomain.Here,wefirstperforminstructiontuningusingWeb2Code
onvariousLLMbackbonesandthenweevaluatethoseMLLMsonthegeneraldomainofvisual
languageunderstanding. Throughoutextensiveexperimentsundervariousdataconfigurations,we
observed that the proposed dataset Web2Code can be incorporated with the conventional visual
languageinstructiontuningdatasetofLLaVA[33]withoutharmingperformancesonthegeneral
domain. Table5summarizestheresults3. Specifically,bothproposedWebUnderstandingdata(DWU
orDWUR)andWebCodeGenerationdata(DWCGorDWCGR)donothurtorevencanbebeneficial
tothevisuallanguageunderstanding. Forexample,weobservedthataddingDWUtoCrystalChat
achievescomparableorevenbetterperformancesonPOPE(86.86→87.10),SciQA(67.77→68.27),
andTextVQA(57.84→58.15). Somewhatsurprisingly,wefurtherfoundthataddingDWCGcaneven
improvevisuallanguageunderstanding. Forexample,thesecondandthirdrowsofCrystalChatshow
+40.31and+5.00pointshigherimprovementsinMME-PandMME-Cbenchmarks,respectively.
Moreover,addingrefineddataDWURandDWCGRarestilleffectiveinthevisuallanguagedomain,
byachievingcomparable(orevenbetter)performancesonoverallbenchmarks. Forexample,the
lastrowindicatesthataddingDWURandDWCGRpreservescomparableperformancesonoverall
benchmarksandevenachieves+0.4higherpointsontheSciQAbenchmark.
6 Conclusion
WehavepresentedWeb2Code,abenchmarkthatconsistsofahigh-quality,large-scalewebpage-to-
codeinstructiontuningdatasetcontaining1179.7kentriesandanevaluationsuiteforthewebpage
understandingandwebpage-to-HTMLtranslationabilitiesofMLLMs. Throughextensiveexperi-
ments,wehavedemonstratedthatourproposeddatasetisclearlyeffectiveatenhancingtheseabilities
ofMLLMsaswellasgeneralvisualproficiency,whileexistingdatasetsleadtoinferiorperformance.
Wehopeourworkwillattractthecommunity’sattentionandfacilitateprogresstowardfoundation
modelsservingasvirtualassistantsforcontentgenerationandtaskautomation.
LimitationsandEthicsStatement. TheWeb2Codeprojectprovidesacomprehensivedatasetand
evaluationframeworkforfine-grainedmultimodallargelanguagemodels. Thiscansignificantly
enhance the capabilities of LLMs in understanding and generating web code from instructions,
leading to advancements in web development automation, improved coding assistance tools and
2CrystalCoder[37]andCrystalChat[37]areanopen-sourcecodeLLMpre-trainedandinstruction-tuned
models,respectively.Theyaretrainedonpubliclyavailablelanguageandcodedatasets.
3Weobservethattheconventionalvisuallanguagedomaindata(i.e.,LLaVA)isacrucialcomponentfor
visuallanguageunderstanding,i.e.,instruction-tunedMLLMswithoutthegeneraldomaindataareweak.
9platforms. Byenablingmoreaccurateandcontext-awarecodegeneration,itcanboostproductivity
fordevelopersandmakecodingmoreaccessibletobeginners. However,theprimarylimitationsof
theWeb2CodeincludethepotentialforbiaseddatasetthatmaynotcoverallpossibleHTMLcoding
scenarios,potentiallyleadingtogapsinmodelperformance,somewebpagesthatincludehumans
may be privacy sensitive. Ensuring high-quality annotations and comprehensive coverage of all
possibleHTMLandcodestructuresischallenging. Also,handlingcomplex,real-worldHTMLand
codescenariosmightstillbebeyondthecurrentcapabilitiesofthemodelstrainedonthisdataset.
Moreover,theproposedevaluationframeworkmaynotcaptureallaspectsofthecodegeneration
quality,suchascodeefficiency,readability,oradherencetobestpractices.
Acknowledgements
WearethankfultoVeselinStoyanovfordiscussionsonmultimodallargelanguagemodels. Wealso
thankthesupportprovidedbytheMBZUAIIT/corporateserviceteams(IanMathews,JohnMurphy,
PadmaPavani,TapasSen,WalidOmari)andCIAIengineeringteam(GuoweiHe,YunXu,YuePeng)
fororganizingHighPerformanceComputingresourcesandservices. Z.S.andE.X.wouldliketo
thanktheMBZUAI-WISJointProgramforAIResearch. Z.S.alsowouldliketothanktheGoogle
Researchawardgrantforthesupport.
References
[1] AI@Meta. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/
blob/main/MODEL_CARD.md.
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. NeurIPS,2022.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,
localization,textreading,andbeyond,2023.
[4] TonyBeltramelli. pix2code: Generatingcodefromagraphicaluserinterfacescreenshot. arXiv
preprintarXiv:1705.07962,2017.
[5] ZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,Zehui
Chen,ZhiChen,PeiChu,etal. Internlm2technicalreport. arXivpreprintarXiv:2403.17297,
2024.
[6] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation.
https://github.com/sahil280114/codealpaca,2023.
[7] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa
Sadigh,LeonidasGuibas,andFeiXia. Spatialvlm: Endowingvision-languagemodelswith
spatialreasoningcapabilities. arXivpreprintarXiv:2401.12168,2024. URLhttps://arxiv.
org/abs/2401.12168.
[8] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
Unleashingmultimodalllm’sreferentialdialoguemagic,2023.
[9] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua
Lin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprint
arXiv:2311.12793,2023.
[10] XingyuChen,ZihanZhao,LuChen,DanyangZhang,JiabaoJi,AoLuo,YuxuanXiong,and
KaiYu. Websrc: Adatasetforweb-basedstructuralreadingcomprehension. arXivpreprint
arXiv:2101.09465,2021.
[11] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL
https://lmsys.org/blog/2023-03-30-vicuna/.
10[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, BoyangLi, PascaleNFung, andStevenHoi. Instructblip: Towardsgeneral-purpose
vision-languagemodelswithinstructiontuning. NeurIPS,36,2024.
[13] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,andetalBinWang.Internlm-xcomposer2-
4khd: Apioneeringlargevision-languagemodelhandlingresolutionsfrom336pixelsto4khd,
2024.
[14] MingzheDu,AnhTuanLuu,BinJi,andSee-KiongNg. Mercury: Anefficiencybenchmarkfor
llmcodesynthesis. arXivpreprintarXiv:2402.07844,2024.
[15] AmitFinkman,EdenBar-Kochva,AvishagShapira,DuduMimran,YuvalElovici,andAsaf
Shabtai.Codecloak:Amethodforevaluatingandmitigatingcodeleakagebyllmcodeassistants.
arXivpreprintarXiv:2404.09066,2024.
[16] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels,2023.
[17] ElizavetaGoncharova,AntonRazzhigaev,MatveyMikhalchuk,MaximKurkin,IrinaAbdul-
laeva,MatveySkripkin,IvanOseledets,DenisDimitrov,andAndreyKuznetsov. Omnifusion
technicalreport,2024.
[18] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthev
invqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering,2017.
[19] Og˘uzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and
FedericoTombari. Brave: Broadeningthevisualencodingofvision-languagemodels,2024.
[20] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDollár,andRossGirshick.
Segmentanything,2023.
[21] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,Stephanie
Chen,YannisKalantidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,andFei-FeiLi.
Visualgenome: Connectinglanguageandvisionusingcrowdsourceddenseimageannotations,
2016.
[22] HugoLaurençon,LéoTronchon,andVictorSanh. Unlockingtheconversionofwebscreenshots
intohtmlcodewiththewebsightdataset. arXivpreprintarXiv:2403.09029,2024.
[23] Byung-KwanLee,BeomchanPark,ChaeWonKim,andYongManRo. Moai: Mixtureofall
intelligenceforlargelanguageandvisionmodels,2024.
[24] BoLi,PeiyuanZhang,JingkangYang,YuanhanZhang,FanyiPu,andZiweiLiu. Otterhd: A
high-resolutionmulti-modalitymodel,2023.
[25] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125,2023.
[26] JiachenLi,XinyaoWang,SijieZhu,Chia-WenKuo,LuXu,FanChen,JiteshJain,Humphrey
Shi,andLongyinWen. Cumo: Scalingmultimodalllmwithco-upcycledmixture-of-experts,
2024.
[27] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pages19730–19742.PMLR,2023.
[28] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels,2024.
[29] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
objecthallucinationinlargevision-languagemodels,2023.
11[30] BinLin, ZhenyuTang, YangYe, JiaxiCui, BinZhu, PengJin, JunwuZhang, MunanNing,
andLiYuan. Moe-llava: Mixtureofexpertsforlargevision-languagemodels. arXivpreprint
arXiv:2401.15947,2024.
[31] Tsung-YiLin,MichaelMaire,SergeBelongie,LubomirBourdev,RossGirshick,JamesHays,
PietroPerona,DevaRamanan,C.LawrenceZitnick,andPiotrDollár.Microsoftcoco:Common
objectsincontext,2015.
[32] FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Mitigat-
inghallucinationinlargemulti-modalmodelsviarobustinstructiontuning. InTheTwelfth
InternationalConferenceonLearningRepresentations,2023.
[33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[34] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae
Lee. Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024. URLhttps:
//llava-vl.github.io/blog/2024-01-30-llava-next/.
[35] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[36] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[37] ZhengzhongLiu,AurickQiao,WillieNeiswanger,HongyiWang,BowenTan,TianhuaTao,
JunboLi,YuqiWang,SuqiSun,OmkarPangarkar,RichardFan,YiGu,VictorMiller,Yonghao
Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen,
Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim
Baldwin,andEricP.Xing. Llm360: Towardsfullytransparentopen-sourcellms,2023.
[38] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,Tongzheng
Ren,ZhuoshuLi,HaoYang,YaofengSun,ChengqiDeng,HanweiXu,ZhendaXie,andChong
Ruan. Deepseek-vl: Towardsreal-worldvision-languageunderstanding,2024.
[39] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. InThe36thConferenceonNeuralInformationProcess-
ingSystems(NeurIPS),2022.
[40] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du,
ShumingShi, andZhaopengTu. Macaw-llm: Multi-modallanguagemodelingwithimage,
audio,video,andtextintegration. arXivpreprintarXiv:2306.09093,2023.
[41] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,Philipp
Dufter,andetal. Mm1: Methods,analysis&insightsfrommultimodalllmpre-training,2024.
[42] OpenAI. Gpt-4technicalreport,2024.
[43] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil
Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell
Howes,Po-YaoHuang,HuXu,VasuSharma,Shang-WenLi,WojciechGaluba,MikeRabbat,
Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal,
PatrickLabatut,ArmandJoulin,andPiotrBojanowski. Dinov2: Learningrobustvisualfeatures
withoutsupervision,2023.
[44] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
12[45] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi,JingyuLiu,TalRemez,JérémyRapin,etal. Codellama: Openfoundationmodels
forcode. arXivpreprintarXiv:2308.12950,2023.
[46] BabakSalehandAhmedElgammal. Large-scaleclassificationoffine-artpaintings: Learning
therightmetricontherightfeature. arXivpreprintarXiv:1505.00855,2015.
[47] LaboniSarker,MaraDowning,AchintyaDesai,andTevfikBultan. Syntacticrobustnessfor
llm-basedcodegeneration. arXivpreprintarXiv:2404.01535,2024.
[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis,AarushKatta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m: Open
datasetofclip-filtered400millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
[49] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions: A
cleaned,hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsof
the56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),pages2556–2565,2018.
[50] ChengleiSi,YanzheZhang,ZhengyuanYang,RuiboLiu,andDiyiYang. Design2code: How
fararewefromautomatingfront-endengineering? arXivpreprintarXiv:2403.03163,2024.
[51] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps: adataset
forimagecaptioningwithreadingcomprehension,2020.
[52] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages8317–8326,2019.
[53] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,
YongmingRao,JingjingLiu,TiejunHuang,etal. Generativemultimodalmodelsarein-context
learners. arXivpreprintarXiv:2312.13286,2023.
[54] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[55] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,ZuxuanWu,andYu-GangJiang. Toseeisto
believe: Promptinggpt-4vforbettervisualinstructiontuning. arXivpreprintarXiv:2311.07574,
2023.
[56] QinghaoYe,HaiyangXu,JiaboYe,MingYan,AnwenHu,HaoweiLiu,QiQian,JiZhang,Fei
Huang,andJingrenZhou. mplug-owl2: Revolutionizingmulti-modallargelanguagemodel
withmodalitycollaboration,2023.
[57] TongYe,YangkaiDu,TengfeiMa,LingfeiWu,XuhongZhang,ShoulingJi,andWenhaiWang.
Uncoveringllm-generatedcode: Azero-shotsyntheticcodedetectorviacoderewriting. arXiv
preprintarXiv:2405.16133,2024.
[58] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,etal.Mmmu:Amassivemulti-disciplinemultimodal
understandingandreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[59] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:
Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,pages6720–6731,2019.
[60] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual
languagemodelforvideounderstanding.InYansongFengandElsLefever,editors,Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations,2023.
[61] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng
Yan,WilliamYangWang,andLindaRuthPetzold. Gpt-4v(ision)asageneralistevaluatorfor
vision-languagetasks. arXivpreprintarXiv:2311.01361,2023.
13[62] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTongSun.
Llavar: Enhancedvisualinstructiontuningfortext-richimageunderstanding. arXivpreprint
arXiv:2306.17107,2023.
[63] BoZhao,BoyaWu,andTiejunHuang.Svit:Scalingupvisualinstructiontuning.arXivpreprint
arXiv:2307.04087,2023.
[64] ZhuofanZong,BingqiMa,DazhongShen,GuangluSong,HaoShao,DongzhiJiang,Hongsheng
Li,andYuLiu. Mova:Adaptingmixtureofvisionexpertstomultimodalcontext. arXivpreprint
arXiv:2404.13046,2024.
14Appendix
A TrainingDetailsandHyperparameters
Wefollowtheinstruction-tuningprotocolofLLaVA-1.5[33]. Inthepretrainingstep,weemploythe
captiondatatooptimizetheprojector,whilekeepingthevisionencoderandLLMfrozen. Meanwhile,
weoptimizetheprojectorandLLMintheinstructiontuningstep. Duringthepretrainingphase,we
utilizeabatchsizeof256,whilefortheinstructiontuningphase,weemployabatchsizeof128. The
learningrateissetat1e−3duringpretrainingandadjustedto2e−5forinstructiontuning,withboth
phasesincorporatingacosinedecayschedule. Wealsoapplyalearningratewarmupwithadecay
factorof0.03,andnoweightdecayisused. Bothpretrainingandinstructiontuningareconductedfor
oneepocheach,consistentlyusingtheAdamWoptimizer.
B MoreEffectsofWeb2CodeonGeneralDomain
Here,wefirstperforminstructiontuningusingWeb2CodeonvariousLLMbackbonesandthenwe
evaluatethoseMLLMsonthegeneraldomainofvisuallanguageunderstanding.
ComparisonondifferentLLMbackbones. Wecomparethegeneraldomainabilitiesofvarious
LLMbackbonesunderthesamedataconfigurationofLLaVA+DWU+DWCG;Table6summarizes
theresultsofinstruction-tunedMLLMs.Specifically,wefoundthatinstruction-tunedCrystalChat-7B,
Vicuna1.5-7B,andLLaMA2-7Bshowsuperiorperformancesinthegeneraldomaincomparedto
CrystalCoderandCodeLlama. Forexample,CrystalChatshows+132.89pointshigherthanCodeL-
lamainMME-P(i.e. perceptiondomain). Somewhatsurprisingly,instruction-tunedCrystalChat
showedthestrongestperformanceonTextVQA,whichrequiresvisualreasoningbasedontextin
images.
LLMBackbone MME-P MME-C POPE SciQA TextVQA
CodeLlama-7B[45] 1345.93 258.92 85.28 61.87 55.23
CrystalCoder-7B[37] 1351.22 274.64 86.05 61.63 50.11
CrystalChat-7B[37] 1478.82 297.14 86.14 67.92 57.41
Vicuna1.5-7B[11] 1488.26 268.21 87.05 69.31 56.40
LLaMA2-7B[54] 1448.58 304.29 86.87 67.87 55.62
Table6: ComparisonofdifferentLLMbackbonesonvisuallanguageunderstandingbenchmarks. All
modelsareinstruction-tunedonthegeneraldomaindata(i.e. LLaVA)withDWUandDWCG.
C QualitativeDataExamplesinWUBBenchmark
ThequalitativedataexamplesinourWUBbenchmarkareshowninFigure10. Itcoversdifferent
aspectsofwebpageunderstandingbasedon"yes"/"no"question-answerpairs.
D WCGBCriteria
OurproposedWCGBframeworkconsistsof10distinctcriteria,whichwegroupintofourcategories,
eachencompassingspecificcriteriathatarescoredona0-10scale:
1. VisualStructureandAlignment
• LayoutConsistency: Measuresthearrangementofstructuralwebpageelementslikehead-
ers,footers,andsidebars.
• ElementAlignment: Assessesthealignmentofimages,buttons,andtextboxes.
• ProportionalAccuracy:Checksforconsistencyinsizesandaspectratiosofvisualelements.
• VisualHarmony: Examinestheoverallbalanceandharmonyindesign.
2. ColorandAestheticDesign
• ColorSchemeandAestheticMatch: Focusesonthesimilarityincolorschemes,including
huesandsaturation.
15Q Does the visual structure of the webpage
indicate that the flight information is meant for
simultaneous display of both departures and
Q Can users find information about specific arrivals? Output YES or NO. A NO
match timings for the tournament directly on this
page? Output YES or NO. A NO Q Are there interactive elements visible that
would allow a user to update the flight status
Q Does the webpage employ a monochromatic color information on the webpage? Output YES or NO.
scheme for its design? Output YES or NO. A NO A NO
Q Is there a consistent font style used Q Do the flight numbers follow a consistent
throughout the different sections of the webpage? naming convention that assists users in
Output YES or NO. A YES recognizing the airline's code and flight
sequence? Output YES or NO. A YES
Figure10: QualitativedataexamplesinourWUBbenchmark. Itcoversdifferentaspectsofwebpage
understandingbasedon"yes"/"no"question-answerpairs.
• AestheticResemblance: Looksattheoverallaestheticappealandstyle(modern,minimal-
istic,traditional,etc.).
3. TextualandContentConsistency
• FontCharacteristicsandConsistency: Assessesuniformityinfonttype,size,style,and
weight.
• TextualContentMatch: Evaluatesthematchinwordsandsentences.
• NumericandSpecialCharacterAccuracy: Checksforconsistencyinnumbers,dates,and
specialcharacters.
4. UserInterfaceandInteractivity
• UserInterfaceConsistency: Assessesthesimilarityindesignlanguageandappearanceof
UIelementslikemenus,buttons,andforms.
16E PromptTemplates
E.1 PromptUsedtoQuestionandAnswerGenerationforDWUData
System: You are an advanced AI model who can identify html code and interpret the compiled webpage.
User: You are asked to come up with a set of 10 diverse website understanding task instructions with the
corresponding source codes. These task instructions will be given to a GPT model and we will evaluate the GPT
model for completing the instructions.
You should output 10 questions and the answers for them, for each html code.
Your output should be formatted as follows. Don't include any other text, connective phases other than the
formatted text.
output a comma separated list of 10 instruction pairs in following format
[{"Q": "<question>" , "A": "<answer>"}, {"Q": "<question>" , "A": "<answer>"}]
Here are the requirements:
1. Try not to repeat the verb for each instruction to maximize diversity.
2. The languages used for the instruction should be diverse. For example, you should combine questions with
imperative instructions.
3. The instructions should be in English.
4. The instructions should be at least 1 to 2 sentences long. Either an imperative sentence or a question is
permitted.
5. Instructions should be clear and precise.
6. Instructions can either be simple queries like 'what text is displayed on the button?', to more complex
tasks requiring analysis, such as 'what would be the total cost of ordering items A and B from the
menu?'.
7. Avoid direct html code related instructions. i.e. Don't include questions related to html tags, font
sizes, and etc. Instead include real webpage image related instructions that a human can ask after
looking into the webpage. For example, What is the main event that this website is allowing users to
register for?, State the variety of dishes featured in the desserts section of the menu.
8. The Answer should be an appropriate response to the instruction and the input.
9. If the answer isn't immediately clear from the content, the output should detail the thought process. For
instance, if the task is to determine the age of someone born in 1983, the response should explain:
'Considering the current year is 2024, the age would be 2023 minus 1984, which equals 40.’
10.Try to make questions covering the following aspects i.e. Visual Structure and Alignment: Layout
Consistency: Evaluates the match in the placement of headers, footers, and sidebars. Element Alignment:
Assesses alignment of images, buttons, and text boxes. Proportional Accuracy: Checks for consistency in
sizes and aspect ratios of visual elements. Visual Harmony: Examines the overall balance and harmony in
design. Color and Aesthetic Design: Color Scheme and Aesthetic Match: Focuses on the similarity in color
schemes, including hues and saturation. Aesthetic Resemblance: Looks at the overall aesthetic appeal and
style (modern, minimalistic, traditional, etc.). Textual and Content Consistency: Font Characteristics
and Consistency: Assesses uniformity in font type, size, style, and weight. Textual Content Match:
Evaluates the match in words and sentences. Numeric and Special Character Accuracy: Checks for
consistency in numbers, dates, and special characters. User Interface and Interactivity: User Interface
Consistency: Assesses the similarity in design language and appearance of UI elements like menus,
buttons, and forms.
11.Answers should be more informative and descriptive. Don't output single word or numbers for the outputs,
make them descriptive with one or two sentences.
12.Don't shrink the output. Need everything expanded. Avoid outputs like (...)
Here's the html code.
<!DOCTYPE html>
<html>
<head>
<title>Charity Walk Registration</title>
</head>
<body>
<h1>Welcome to the Charity Walk Registration Site</h1> ...
Figure11: PromptusedtogenerateQuestionAnswerpairsusingGPT4forDWUdata.
17E.2 PromptsUsedForInstructiongenerationofDWCGandDWCGR
User:
Here are some instructions that are being fed to a generative AI model as instruction tuning during the
training of a Vision language model.
"In the provided webpage screenshot, generate HTML to replicate the layout and styling of the webpage",
"Given the web application interface shown, write HTML code to implement the interactive features visible in
the image",
"Analyze the webpage structure in the screenshot and provide HTML and CSS code to design a responsive webpage
with similar layout and components",
"Examine the provided webpage screenshot and generate HTML code to implement the responsive layout",
"Given the webpage screenshot, provide the HTML code that represents its structure",
"Create HTML code for the webpage depicted in the image provided",
"Craft HTML code to replicate the visual design and structure of the webpage captured in the provided
screenshot",
"Create HTML and CSS code to imitate the appearance and layout of the web interface shown in the provided
image",
"Generate HTML and CSS code snippets to mirror the layout and styling of the website visible in the given
screenshot”
Note:
1.Try not to repeat the verb for each instruction to maximize diversity.
2.The languages used for the instruction should be diverse. For example, you should combine questions with
imperative instructions.
3.The instructions should be in English.
4.The instructions should be at least 1 to 2 sentences long. Either an imperative sentence or a question is
permitted.
5.Instructions should be clear and precise.
6.Please provide 150 examples.
Figure 12: Prompt used to generate instructions for DWCG using GPT4, feeding input as Seed
instructionsandoutputasGPTgeneratedinstructionsshowninFigure16.
User:
Here are some instructions that are being fed to a generative AI model as instruction tuning during the
training of a Vision language model.
“Please provide the code in material design style.",
"Share the code adhering to the principles of material design.",
"Ensure the code follows the guidelines of material design aesthetics.",
"Provide the code in a style consistent with material design principles.",
"Code should reflect the design language of material design.",
"Maintain a coding style aligned with material design aesthetics.",
"Present the code in accordance with material design styling.”
Note:
1.Try not to repeat the verb for each instruction to maximize diversity.
2.The languages used for the instruction should be diverse. For example, you should combine questions with
imperative instructions.
3.The instructions should be in English.
4.The instruction should be clear.
5.Please provide 40 examples.
Figure13: PromptusedtogeneratewebpagestyleinstructionforDWCGusingGPT4,feedinginput
asSeedinstructionsandoutputasGPTgeneratedwebpagestyleinstructionsshowninFigure16.
18E.3 PromptUsedForGPT4-VisionEvaluationinWCGBbenchmark
System: You are an advanced AI model equipped with OCR and image understanding capabilities, capable of
analyzing visual elements in detail.
User: Your task is to assess two webpage images and output a score between 0 and 10 for each of the following
questions.
If the answer to a question is a definite YES, output a score of 10, signifying perfect similarity.
Conversely, a definite NO should yield a score of 0, indicating no similarity.
For answers that fall in between, assign a score accordingly, where a higher number indicates a greater
degree of similarity. Only provide the numerical score for each question, without any additional text.
Example contexts are provided for clarity. Examples provides the idea, but you can output any number in 0-10
range accordingly.
Only output a comma separated list containing 10 numbers. DO NOT give score of 10 for any category unless
otherwise the two images are identical.
Layout Consistency (Score: 0-10): Does the placement of headers, footers, and sidebars match in both
webpages? (e.g., A score of 10 for identical layouts, 5 for similar but not exact placements, and 0 for
completely different layouts.)
Element Alignment (Score: 0-10): Are elements like images, buttons, and text boxes aligned similarly on both
pages? (e.g., A score of 10 for perfectly aligned elements, 6 for slight misalignments, and 0 for major
misalignments.)
Proportional Accuracy (Score: 0-10): Do the sizes and aspect ratios of images, buttons, and text boxes appear
consistent across both pages? (e.g., A score of 10 for exact proportions, 4 for noticeable size differences,
and 0 for drastic inconsistencies.)
Visual Harmony (Score: 0-10): Do both webpages exhibit a similar level of visual harmony and balance in their
design? (e.g., A score of 10 for harmonious designs, 5 for some dissonance, and 0 for clashing designs.)
Color Scheme and Aesthetic Match (Score: 0-10): How closely do the color schemes of the two webpages align in
terms of background and text colors? Evaluate the similarity in hues, saturation, and overall color
aesthetics. (e.g., A score of 10 for perfectly matching color schemes, including identical hues and
saturation levels, 6 for similar color palettes with minor variations, and 0 for starkly different color
schemes that create entirely different visual impacts.)
Aesthetic Resemblance (Score: 0-10): Is the overall aesthetic appeal (modern, minimalistic, traditional,
etc.) similar on both pages? (e.g., A score of 10 for identical aesthetics, 4 for somewhat similar but
distinguishable styles, and 0 for completely different aesthetics.)
Font Characteristics and Consistency (Score: 0-10): Assess the degree of consistency in font attributes
across both webpages. This includes not only the font type and size but also the nuances of font style
(italic, bold) and weight (light, regular, bold). (e.g., A score of 10 for complete uniformity in font type,
size, style, and weight across both pages, 5 for consistency in font type and size but variations in style or
weight, and 0 for wide disparities in font type, size, style, or weight, leading to a distinctly different
textual appearance.)
Textual Content Match (Score: 0-10): Do the words and sentences match between the two webpages? (e.g., A
score of 10 for identical text, 5 for some similar paragraphs or sections, and 0 for completely different
textual content.)
Numeric and Special Character Accuracy (Score: 0-10): Are numbers, dates, and special characters (like email
addresses) consistent between the two pages? (e.g., A score of 10 for exact matches, 6 for minor
discrepancies, and 0 for major differences.)
User Interface Consistency (Score: 0-10): Do the user interface elements (like menus, buttons, and forms) on
both pages share a similar design language and appearance? (e.g., A score of 10 for identical UI elements, 6
for slight design variations, and 0 for completely different UI designs.)
<GROUND TRUTH IMAGE>
<PREDICTED IMAGE>
Figure14: ThepromptutilizedforevaluationinWCGBwiththeemploymentofGPT4-Vision.
19E.4 PromptUsedForQAGenerationforWUBbenchmark
User: Generate 5 QA pairs for assessing an AI model's comprehension and response accuracy in relation to a
given webpage screenshot.
The answers should be evenly balanced between YES and NO. These QA pairs will be used as a benchmark to
evaluate another model's understanding of the webpage's visual and textual elements.
Ensure the output is formatted as QA pairs in the following dictionary format: '{ 'Q': \"question\", 'A':
\"answer\"}’.
The QA pairs should cover the following aspects:
- Advanced Logical Reasoning and Textual Understanding: The questions should challenge the AI's ability to
understand complex text and infer meanings or implications that are not explicitly stated.
- Visual Structure and Alignment Evaluation: Questions should test the AI's ability to interpret and
analyze the webpage's layout, including the positioning and relationship of various visual elements.
- Color and Aesthetic Design Analysis: Include questions that assess the AI's understanding of the
webpage's color scheme and overall design aesthetics, and how they contribute to the page's purpose or
message.
- Consistency in Text and Content: Formulate questions that evaluate the AI's recognition of textual
consistency and coherence across different sections of the webpage.
- User Interface and Interactivity Insights: Develop questions to assess the AI's comprehension of the
webpage's interactive elements and user interface design, including navigational features and response
mechanisms.
Remember, the goal is to create questions that will test another AI model's ability to analyze and
interpret a webpage screenshot in a comprehensive manner. Do not include additional text other than the
output dictionary entry.
Figure 15: The prompt employed to generate "yes" / "no" Question Answer pairs for the WUB
benchmarkthroughtheutilizationofGPT4-Vision.
F Datasamples
Figure16: Examplesofwebpagetocodegenerationinstructiontuningdata. Thesewebimage-code
pairswereconvertedintoaninstruction-followingdataformatclosetotheLLaVAdataformat.
20Figure17: ExamplesofwebpagetocodegenerationinstructiontuningdatainDWCG.Theseweb
image-codepairswereconvertedintoaninstruction-followingdataformatclosetotheLLaVAdata
format.
Figure18: ExamplesofwebpagetocodegenerationinstructiontuningdatainDWCGR. Therefined
instructiontuningdatasetforwebpagecodegeneration,byutilizingGPT-4.
Figure19:ExamplesofwebpageunderstandingdatainDWU.Forawebpage,thereisasetofdiverse
question-answerpairsaboutwebunderstanding.
21Figure20:ExamplesofwebpageunderstandingdatainDWUR.Therefinedinstructiontuningdataset
forwebunderstandingtasks.
Figure21: ExamplesofModernstyleswebpages.
Figure22: ExamplesofBootstrapstyleswebpages.
22