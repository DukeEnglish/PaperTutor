BESTOW:EFFICIENTANDSTREAMABLESPEECHLANGUAGEMODELWITHTHE
BESTOFTWOWORLDSINGPTANDT5
ZhehuaiChen,HeHuang,OleksiiHrinchuk,KrishnaC.Puvvada,NithinRaoKoluguri,PiotrZ˙elasko,
JagadeeshBalam,BorisGinsburg
NVIDIA,SantaClara,CA,USA
ABSTRACT spokeQ-formermodulebeforeprependingspeechpromptsto
betterbringspeech,audio,andmusicfeaturestoLLMspace.
Incorporating speech understanding capabilities into pre-
Nevertheless, there are several potential drawbacks in
trained large-language models has become a vital research
this popular design: i) Efficiency problem raised from the
direction (SpeechLLM). The previous architectures can be
interactionbetweenself-attentionandthelongerspeechem-
categorized as: i) GPT-style, prepend speech prompts to the
beddings than text, which will be elaborated in Section 4.4.
textpromptsasasequenceofLLMinputslikeadecoder-only
Workarounds like massive downsampling of speech embed-
model; ii) T5-style, introduce speech cross-attention to each
dings usually come with information loss and cannot com-
layer of the pretrained LLMs. We propose BESTOW archi-
pletely avoid. ii) As the speech embeddings of the whole
tecture to bring the BESt features from TwO Worlds into a
utterancearetreatedaspromptandalwaysprependedbefore-
singlemodelthatishighlyefficientandhasstrongmultitask
hand,itdisablesmanystreamingapplicationsinspeech,e.g.
capabilities. Moreover, there is no clear streaming solution
streamingASRandsimultaneousspeechtranslation(SST).
for either style, especially considering the solution should
Inthiswork,weproposeanalternativemodularandmul-
generalize to speech multitask. We reformulate streamable
titask SpeechLLM design that is both streamable and effi-
SpeechLLM as a read-write policy problem and unifies the
cient. Ourmaincontributionsaresummarizedasfollows:
offline and streaming research with BESTOW architecture.
Hencewedemonstratethefirstopen-sourceSpeechLLMso-
• To the best of our knowledge, this is the first open
lutionthatenablesStreamingandMultitaskatscale(beyond
SpeechLLMsolutionthatenablesstreamingandmulti-
ASR) at the same time. This streamable solution achieves
taskatscale(beyondASR)atthesametime.Moreover,
very strong performance on a wide range of speech tasks
thesolutionisend-to-endoptimizableandallowsLLM
(ASR,AST,SQA,unseenDynamicSuperb). Itisend-to-end
knowledgetransfertospeech.
optimizable, with lower training/inference cost, and demon-
stratesLLMknowledgetransferabilitytospeech.
• Propose a different backbone architecture from the
popularSpeech-LLaMAvariantsthatisbasedoncross-
1. INTRODUCTION attention and read-write policy. The novel backbone
unifies the offline and streaming modes and achieves
With the huge success of large language models (LLMs) [1, state-of-the-art on several large-scale and multitask
2], researchers start to explore the possibilities of extending speech-to-textbenchmarks(ASR,AST,SQA,Dynam-
the capabilities of LLMs with multi-modal understanding icSuperb),withlowertraining/inferencecost.
skills,andmanyworkshavebeenproposedtosupportimage
andaudiounderstanding[3,4,5,6,7]. Moreover,wescalethemodelto87Khoursofspeechdata
andthetrainingcanfinishinoneday.Wewillreleasethecode
This work focuses on leveraging speech encoder and
LLM(SpeechLLM)tobuildaspeechfoundationalmodelfor andcheckpointsofthismultitaskspeechfoundationalmodel
topromoteofflineandstreamingSpeechLLMresearch.
manyspeech-and-audiototextapplications(STT).Onepop-
ularframeworkinthedirectionisSpeech-LLaMA[8,9]and
its extensions, which updates the input of LLM by prepend-
2. RELATEDWORK
ing speech prompts to the text prompts while keeping the
rest of LLM unchanged or LoRA finetuned. This design
2.1. SpeechFoundationalModel
shows good modularity which allows knowledge transfer
from LLMs to speech and results in strong ASR and AST MotivatedbythesuccessoffoundationmodelsinNLP[1,2],
performance[10,11]. Themodulardesignalsoshowsstrong recent speech foundational model research has been shifted
in-context learning ability [12]. [7] further introduces a be- towards developing universal pretrained models capable of
4202
nuJ
82
]LC.sc[
1v45991.6042:viXrativations and resultant designs from our work. E.g. [26, 27]
introduces cross attention before the above concatenation to
bridgethegapbetweenthespeechencoderandLLM,whose
computationisthesameormore.
2.3. StreamingSpeechModels
InstreamingASR,Effortshavebeenmadetoenablestream-
ingintoTransformer[28,29]. Thesemethodseitheruselim-
itedcontextwithofflinemodelsortrainmodelsinastreaming
manner. ThemethodsusuallybuildontopofTransducer[30]
whichrarelybenefitsfrompretrainedLLM.
Fig. 1. Reformulate streamable SpeechLLM as a read-write Insimultaneousspeechtranslation,researchershavebeen
policy problem previously used in simultaneous translation, lookingatfixedandadaptiveread-writepolicies. Wait-k[31]
i.e. theLLMagentsshouldstartreplyingwhenevertheythink and variants are the most longstanding fixed policy. Typical
theyhavegottenenoughinformation. adaptivepoliciestrytolearnthepolicywithabespokemodel
fromthetrainingdata,e.g. EMMA[32]andITST[33].
handling multilingual speech and audio tasks. Recent ad-
vances include but are not limited to: i) Large scale multi-
lingualself-supervisedlearningandsemi-supervisedlearning
3. WHYSTREAMINGSPEECHLLMISHARD
toleverageunlabledspeech,e.g. XLSR[13]andUSM[14].
ii) Large scale multitask supervised training, e.g. Whisper While making SpeechLLMs operate in real-time and proac-
variants[15,16]andSeamlessM4T[17]. iii)Morepowerful tivelyrespondisimportantforhuman-machineinterface,the
andefficientspeechencoderdesign,e.g. Conformerandvari- topicisunder-exploredbesidessomeveryrecentworks.
ants[18,19]. iv)Multilingualmultitaskspeechbenchmarks,
AsSpeechLLMusuallystartsofflinetextpredictionafter
e.g. XTREME[20]andML-SUPERB[21].
it accepts a complete speech segment as the speech prompt,
thesestreamingworksproposetoupdatethepromptformatto
2.2. SpeechLLM takeinterleavedblock-wisespeechfeaturesandtext[34,35].
Recently,researchersstartedtolookatcombiningpretrained [36,37]alsobelongstothiscategoryexceptfocusingontext
speechmodelswithlargelanguagemodelstoformanewtype machine translation with an ASR model as cascaded speech
ofspeechfoundationalmodel,SpeechLLM,whichcanbecat- frontend. There are several limitations in this line of ap-
egorized through speech embedding types. SpeechGPT [5] proaches: i) Prompt format mismatch between text pre-
andAudioPaLM[22]quantizespeechintodiscretetokens,so train and offline/streaming modes of SpeechLLM. The up-
speech tokens and text tokens can be combined into a sin- datedinterleavedformatwithinjectedBLANK[34]orWAIT[37]
glevocabulary. Nevertheless,thisapproachislimitedbythe tokens circumvents the LLM textual knowledge transfer to
quality and diversity of the speech quantization model espe- speech.Previousresearchcannotshowextrawinsfromlever-
ciallyforSTTtasks. Anothermorepopularmethodistofeed aging pretrained LLMs and didn’t demonstrate abilities be-
speechintopretrainedspeechencoder,wherethespeechfea- yondASR[34,35]. ii)Notend-to-endoptimizableblocked
turesarethenprojectedintothewordembeddingspaceofthe bytheintroducedalignmentstage.Thisalignmentpreprocess
LLM[12,23,11,7,6]. Ourworkbelongstothiscategory. design not only results in hardness in generalizing to multi-
After extracting speech features, the previous architec- task, e.g. AST, SQA, but also is bounded by the alignment
tures to provide speech information to LLMs can be further errors in e.g. CTC [34] and translation [37], especially con-
categorizedintotwobranches:i)GPT-stylemodels[12,7,11] sideringcertainwordcancrossspeechsegments. iii)Higher
directly concatenate the speech prompts with text prompts inferencecost,stemsfromeitherlongertextpredictionlength
and use the combined sequence as input to the LLM like a inthenewpromptformat[34]orrequiringbeamsearch[38].
decoder-onlyGPTmodel;ii)Flamingoanditsextension[23, AnewstreamableSpeechLLMsolutionwillbeproposed
3,24]areanotherbranchofworks,wherecross-modalcross- which keeps the same LLM prompt format and unifies the
attentionisaddedtoeachlayer/blockofthepretrainedGPT- learningframeworkofofflineandstreamingmodes. Several
basedLLMwithsharedtextquery. Theresultantarchitecture uniqueadvantagesthatwillbedemonstrated:i)LLMknowl-
issimilartotheT5architecture[25]. edgetransferii)multitasksupportiii)end-to-endoptimiz-
Some recent works look at introducing cross-attention able iv) lower training/inference cost. Lastly, we believe
layersintothefirstbranchofmethods,butwithdifferentmo- thisisthefirstopen-sourcestreamableSpeechLLMsolution.Fig. 2. (a) SALM architecture [12] (as an example of Speech-LLaMA). (b) the proposed BESTOW architecture, which uses
cross-attentiontoextracttask-relevantfeaturesusingthetextasqueriesandspeechfeaturesaskeys/values. Comparedwiththe
former,theproposedmodelhaslowerruntimecomplexity,whileachievingthestate-of-the-artperformanceonmultipletasks.
4. BESTOW:ASTREAMABLESPEECHLLM Withthisnewdesign, thespeechfeaturesextractedfrom
thespeechencoderserveasthekeysandvaluesforthecross-
4.1. Unifiedoffline-streamingframework
attention mechanism, while the input text (text prompt and
previoustokens)isfirstlyembeddedbytheLLMsinputem-
The goal is to design a unified framework for offline and
bedding matrix and later used as the queries. To make the
streaming SpeechLLM so as to maximize the LLM knowl-
query considering both the current step and the context, we
edge transfer from pretrain and instruction tuning. Offline
furtherinjectcausalself-attentionlayersbetweentheLLMin-
and streaming modes should share most of the architectures
putembeddingsandthecross-attentionlayers. Intheablation
andend-to-endoptimizable.
studysection,wealsoconsideranalternativedesignofRNNs
AsFigure1,weproposetoformulatethestreamingprob-
for the same goal. To preserve the original textual knowl-
lem of SpeechLLM as the read-write policy problem previ-
edge,weincludearesidualconnectionthataddstextprompts
ouslydefinedinsimultaneousspeechtranslation[31]. Atev-
directly to the output and the combined speech and text em-
ery LLM step, the model decides whether to wait for more
beddingsserveasthefinalinputtothepretrainedLLM.The
speechincomingfeatures(READ)ortopredictatargetword
resultantdesignisessentiallyonecross-attentiontransformer
(WRITE).ThesolutionstillkeepsthepromptformatofLLM
layerproposedin[39]whichcanberepeatedforXtimes.We
unchanged and experiment results will show its benefit on
empirically found in the ablation study section that X = 2
LLM knowledge transfer. The prerequisite of this solution
is sufficient. Besides, the residual design above ensures the
istodecoupleREADandWRITEoperationsfromLLMand
model can fall back to the original textual LLM by learning
makeitmodeledbyastandalonemodule,thecross-attention
toignorethecross-attentionoutputsinnon-speechsteps.
feature extractor proposed in Section 4.2. Empirical result
The proposed model can be trained using next-token-
will show this module is on par with Speech-LLaMA archi-
prediction as in training other GPT models [1]. During
tecture. After that, streaming is straightforward and end-to-
inference, LLM can take text prompts and generate output
endoptimizable,discussedinSection4.3. Lastlywediscuss
tokens in a step-by-step fashion. The only difference is that
additionalefficiencybenefitofthisarchitectureinSection4.4.
eachpredictedtokenisfedbacktoboththeinputofLLMand
thecross-attentionmoduleabove.
4.2. Introducespeechmodalitywithcross-attention
We propose a new mechanism on how text prompts inter- 4.3. FromOfflinetoStreaming
act with speech prompts in the LLM input side, which will With the above cross-attention speech feature extractor, the
showuniqueadvantagesinstreamingandefficiency. Differ- speech context length required to make prediction at each
entfrommajority[8,9,12,7]ofpreviousworksthatsimply decoder step is independent with LLM backbone and com-
concatenatespeechpromptswithtextpromptsasinputtothe pletely decided by the cross-attention module with a read-
LLM(Figure2(a)),weinjectatrainabletransformer-likeself- write policy in Figure 1. This characteristic enables the
attentionandcross-attentionlayersbeforefeedingintoLLMs streamingdesigninsimplytwosteps.
tolettextpromptattendtodifferentregionsofspeechprompt The first step is to design a read-write policy for han-
toextractfeaturesthataremorerelevanttothecurrentLLM dling the streaming speech input. Our framework converts
stepasillustratedinFigure2(b). streamable SpeechLLM to a similar read-write problem assimultaneous translation where previous research in fixed Whisper-LLaMA [24] also add cross-attention on pretrained
and adaptive policies can be reused. We will take the most GPT-based LLMs, the cross-attention is added to each layer
popular and longstanding fixed policy, wait-k [31], as an oftheLLM,whichintroducesalargenumberofparameters.
example in the following while integrating more adaptive In contrast, we show with only two layers of cross-attention
policies[32]willbeinterestingfuturetopics. Wefirstdecide before feeding into LLMs, we can achieve state-of-the-art
a fixed pre-decision ratio of L which represents a step size performance in ASR and AST. Moreover, the modality-
of(L∗P ∗10ms)whereP isthedownsamplingratioofthe specific design in the proposed method is only introduced
speech encoder. After taking text context prompt without to the input of LLMs, which allows isolating the LLM pa-
cross-attention, LLM starts to predict the first subword unit rameter updates to parameter-efficient-finetune methods like
by cross-attending to the first (K ∗ L) speech embedding LoRA[40]asdemonstratedintheexperiment.
steps. Afterthat,LLMpredictonenextsubwordforeveryL
incomingspeechembeddingsinastreamingfashion.
5. EXPERIMENTS
The second step is to make the speech encoder work in
streamingmode.Thiscanbedonebytwoapproaches:i)keep
5.1. DatasetsandSettings
thebidirectionalencoderintheinferencetime,recomputeall
the available encoder states after getting each new speech Weinclude31Khoursofpublicdataspeechas[47]and54K
block, and provide that to the above read-write policy (de- hours of extra in-house data for speech recognition (ASR)
noted as BESTOW-S-bidi) ii) retrain the model with a unidi- andspeechtranslation(AST),whichincludes67.4Khoursin
rectionalencoder(denotedasBESTOW-S-unid). Forthefor- English, 6.1K hours in German, 6.6K hours in Spanish, and
mer,weintroduceafixedrightcontextof13framesinthein- 5.1K hours in French. In order to support multitask speech
ferencetimetocompensatethetraining/inferencemismatch. and audio understanding, we further add 2K hours from the
Forthelatter,weadaptthecacheawarestreamingmodel[29] speechsynthesizedversion(releasedin[44])ofMSMACRO
to update the FastConformer pre-encoder layers. We utilize textQA[48]andthetrainingsetofDynamicSuperb[46].
causalconvolutionswiththefollowingleftandrightcontext
We evaluate ASR and AST performance through WER
windowsinframes: [[70,13],[70,6],[70,1],[70,0]],whichcan
and BLEU on public benchmarks elaborated in the next
bechosenfromintheinferencetime.
section. SpeechQA (SQA) and general speech understand-
We initialize the streamable SpeechLLM (BESTOW-S) ing ability are assessed through the test splits of the above
from the offline SpeechLLM (BESTOW) and continue train- datasetsrespectively. Wealsoreportnon-computation-aware
ingonthesamedata. Toimprovethegeneralization,wetrain LAAL[49]asthelatencymetricofthestreamableBESTOW-S.
BESTOW-SwitharandomK rangesothatininference,any
We implement the model with PyTorch using NeMo
K intherangecanbeusedtoallowlatency-qualitytradeoff.
Tookit [50], and the model is trained on 128 A100 (80G)
GPUs for 60k steps in one day, with a batch duration of
360 sec per GPU. The speech encoder is initialized from
4.4. BESTOWv.s. GPT-styleandT5-style
the Canary-1B model [47], while the LLM is initialized
Besidesthestreamablecapabilityabove,comparedwithGPT- fromtheTinyLlama-1.1B-chatmodel[51]. Weusea2-layer
style SpeechLLMs (or Speech-LLaMA) [12, 7, 6], the pro- cross-attention Transformer module, where effect of its lay-
posedarchitectureisalsocomputationallymoreefficient. Let ers is studied in Section 5.3 with separate experiments on
L andL denotesthelengthsoftexttokensandspeechfea- LibriSpeech [41]. We train all parameters in the model by
t a
tures respectively, then computational complexity of GPT- default, which is about 1.8 billion. LoRA [40] with frozen
stylemodelsistotheorderof(L +L )2 =L2+L2+2L L , LLM backbone is also explored in an ablation study (1024
t a t a a t
duetothequadraticcomplexityforself-attentionmechanism. dimensions). We use distributed fused Adam optimizer and
Meanwhile,withourcross-attentionbetweentextandspeech cosineannealing,withlearningrate1e−4andweightdecay
prompts,weareabletoreducethecomplexitytoL L +L2. of1e−3. Gradientclippingof1.0isapplied. Code,configs
t a t
Given that the length of text tokens is usually much shorter andcheckpointswillbepubliclyavailable.
than that of speech features (in which case L a ≫ L t), in We follow Section 4.3 to turn above offline BESTOW to
theory we can enjoy a speed up of L a times, which means streamable BESTOW-S. We set L = 4, P = 8, K = 10 by
thelongerthespeechthegreaterthespeedup.Thisspeedupis defaultforalltasksexceptK = 6inASRtaskasitrequires
crucialespeciallyconsideringverywideanddeepLMswhere less speech context. K is sampled from 3 to 12 in training.
thecomputationfromspeechencoderandcrossattentioncan Varying L to result in different step size is studied in Sec-
be omitted from LLM forward and backward computation. tion5.4. TostabilizethetrainingofBESTOW-S-unid,wefirst
Tocompensateforthat,allpreviousresearchhastointroduce pretraintheFastConformercacheawareencoders[29]using
significantdownsamplingonthespeechfeaturestoreducethe wav2vec2onthelibrilight[52]. Intheinferencetime,weal-
featurelength,whichpotentiallyincursinformationloss. waysuse13framesastherightcontextofthespeechencoder
Although the T5-style models like Flamingo [3, 23] and ofBESTOW-S-unidandBESTOW-S-bidi.Table1. CompareBESTOW,asaMultitaskandStreamableSpeechLLM,withothermultitaskSpeechLLMs. ForASR,weuse
Librispeechtest-other(LS)[41]andGigaspeech(Giga)[42]andreportnon-computation-awareLAALandWERaslatencyand
quality metrics. For AST, we use CoVoST [43] and report LAAL and BLEU. We report numbers on speech synthesized MS-
MACRO(SQA)[44]andDynamicSuperb[45](follow[46]toreport6categories)whereROUGE-LandAccuracyarereported.
Model ASR↓ AST↑ SQA↑ DynamicSuperb(unseen)↑
LAAL↓ LS Giga LAAL↓ en-de de-en es-en fr-en CNT SPK SEM DEG PRL AUD
OfflinemultitaskSpeechLLM
QWEN-audio[11] N/A 4.2 10.2 N/A 25,1 33.9 39.7 38.5 39.7 N/A
SALMONN[7] N/A 4.9 10.0 N/A 18.6 N/A N/A N/A 48.8 N/A
Whisper-LLM[46] N/A N/A N/A N/A N/A N/A N/A N/A N/A 8.7 60.6 20.9 59.0 6.6 15.9
BESTOW N/A 3.2 9.9 N/A 39.0 39.0 41.1 41.0 59.8 100.0 81.0 75.0 43.5 46.0 2.0
StreamablemultitaskSpeechLLM
BESTOW-S-bidi 3.0 3.5 10.4 3.9 39.2 37.9 40.1 40.3 52.3 94.0 59.5 76.5 39.0 47.0 1.0
BESTOW-S-unid 2.9 4.3 10.8 3.9 36.6 34.9 37.7 37.6 47.7 97.5 79.5 76.5 76.5 39.0 2.5
We compare the proposed cross attention based Speech- unidirectionalsolutionsareimportantnextsteps.
LLMarchitecturewithSALM[12]trainedonthesamedatain
Section5.3,whichisanopen-sourcespeechlanguagemodel 5.3. CanBESTOWservewellasaofflineSpeechLLM?
conditioning a LLM on speech and text prompts to predict
textual outputs for different speech tasks. SALM simply Inthissection,wetakeastepbackandconfirmwhetherBE-
prepends speech prompts to the text prompts as the LLM STOWasaoffline/streamingunifiedarchitecturecanperform
inputswhichisthetypicaldesignofSpeech-LLaMA[8,9]. well on offline scenario. We compare the offline BESTOW
withthestate-of-the-artASRmodelsonMCV-16.1[53]and
ASTmodelsonFLEURS[55]testsets,andshowtheresults
5.2. BESTOW-Senablesstreamingandmultitask
inTable2. WeusetheWhisper[15],SeamlessM4T[17]and
Canary[47]asbaselines3,byusingtheirofficialcheckpoints
Table1comparestheproposedBESTOW,asaMultitaskand
and rerunning the models on the same test sets. All models
Streamable SpeechLLM, with other SpeechLLMs. To our
usebeamsearchwithbeamsize5.
best knowledge, BESTOW-S is the only open SpeechLLM
thatsupportsbothstreamingandmultitask1. BothBESTOW BESTOW performs the best in the multidomain Open
ASRLeaderboard[54]andthefourseenlanguagesofmulti-
and BESTOW-S significantly outperform other models on
lingualASRbenchmark. Moreover,itachievesthefirstplace
ASRandASTtasks. TheBESTOWmodelscanalsosupport
ofoneASTlanguagepairandsecondplacesofanotherfour
SQA and unseen tasks from DynamicSuperb and perform
pairswith2%ofthetrainingamountofSeamlessM4T-large-
reasonablywelleveninstreamingmode(BESTOW-S)witha
3.9secondsLAALlagging. AsacontrastfortheSQAtask2, v2[17].Notably,whileBESTOWisbuiltontopofthespeech
foundationalmodelCanary-1b,itsignificantlyadvancesASR
we built a strong cascaded baseline by first transcribing the
andASTperformancesonalltasksinthelasttworows. This
speech with 110M NGC ASR pretrained Fast Conformer-
demonstrates that BESTOW architecture can leverage LLM
large, followed by feeding hypotheses to a LoRA finetuned
textual knowledge to improve speech task performance and
TinyLlama 1B LM with the textual MSMARCO dataset
minimizethedataamounttobuildgoodsystems.
whichleadsto56.2ROUGE-L.BESTOWperformsdecently
Table 3 conducts another apple-to-apple comparison be-
(59.8and52.3)comparedtothecascadedbaseline.
tween the proposed cross-attention speech feature extractor
In short, BESTOW-S enables streaming and multitask at
(X-Attn) in Section 4.2 and one of the most popular of-
the same time with competitive performance compared to
fline methods to connect speech models with LLM, Speech-
offline BESTOW on each task. The latency is tunable and
LLaMA [8, 9] types of works (Prepend). The latter branch
can be less on tasks like ASR which require less context.
of methods share an architecture of prepending speech em-
Section 5.4 will elaborate on the latency-quality tradeoff.
beddingstothetextembeddingsbeforefeedingtoadecoder-
BESTOW-S-bidi with bidirectional encoder and recomputa-
only LLM. We leverage the open-source SALM implemen-
tionintheinferencetimeisgenerallybetterthanBESTOW-
tation [12] to build a model with the same training data and
S-unid, consistent with previous research in SST [32, 31].
similarsizeforcomparison,whichusestwoConformerlayers
Optimizing the computation-aware lagging of the bidirec-
with4Xsubsamplingontopofthepretrainedspeechencoder
tionalencoderandclosingthegapbetweenbidirectionaland
as modality adapter layers. The speech encoder is always
1Weacknowledgethatpreviousstreamingworks[34]and[35]report7.4 jointly trained. Prepend performs similar as X-Attn on ASR
and7.9onLibrispeechofASRtask(ournumberis3.3)butnotincluded and AST tasks while the proposed X-Attn shows clear speed
becauseofonlysupportingASRandnotusingorbenefitingfromLLM.
2GivennocommonSQAbenchmark,whileotherSpeechLLMworksdo 3AcknowledgingstrongresultsfromGemini[2]andSLM[10],wecannot
nottrainontheSQAdataset[44]weused,itmaynotbefairtocompare. comparetothemduetomissingper-languagenumbers.Table2. Comparisonwithstate-of-the-artmodelsonspeechrecognition(ASR)andspeech-to-texttranslation(AST).ForASR,
weuseMCV-16.1[53]testsets,andprocessboththepredictionsandgroundtruthusingWhisperNormalizer[15]toreportWER.
WealsoreportaverageWERonlarge-scaleOpenASRLeaderboard(HF-Lead)[54]. ForAST,weuseFLEURS[55]testsets
andtheirnativeannotationswithpunctuationandcapitalizationtoreportBLEU.
Data ASR(WER↓) AST(BLEU↑)
Model
hour HF-Lead En De Es Fr En→De En→Es En→Fr De→En Es→En Fr→En
SeamlessM4T-medium(1.2B) 4M N/A 10.25 9.32 7.25 11.07 28.30 21.05 37.36 33.39 21.68 31.02
SeamlessM4T-large-v2(2.3B) 4M N/A 7.47 5.82 4.82 7.75 33.17 23.72 43.05 37.06 25.41 30.94
Whisper-large-v3(1.5B) 5M 7.16 9.92 6.17 4.94 11.18 N/A N/A N/A 33.40 22.70 33.70
Canary-1b(1B) 85K 6.67 7.97 4.61 3.99 6.53 32.15 22.66 40.77 33.99 21.80 30.95
BESTOW(1.8B) 87K 6.50 7.31 4.16 3.77 6.18 31.98 23.08 41.90 35.75 23.86 35.10
Table 3. Efficiency and Accuracy Comparison of Speech- Table4.Ablationstudyonthecross-attentionarchitecture.A
LLMArchitecture.Thetrainingspeedandmemoryconsump- smallerencoderistrainedwhileLLMisfrozenwith256-dim
tionaremeasuredbysteps-per-secondandmemoryconsump- LoRA.WERisreportedonLibrispeechtestothersplit.
tionpercentageofaA100(80G)GPU.ASRandASTresults Trainable LS
Arch. Details
arereportedonMCV-16.1andFLEURSrespectively. parameters (WER)
Speed↑ Mem.↓ ASR↓ AST↑ 8*(Self-Attn+X-Attn) 276M 5.6
(step/s) (%) en de en-de de-en 2*(Self-Attn+X-Attn) 175M 5.6
Prepend 0.50 69 7.2 4.1 32.1 36.0 X-Attn 1*(Self-Attn+X-Attn) 158M 5.8
X-Attn 0.59 58 7.3 4.1 32.0 35.8 1*X-Attn 154M 6.1
+LoRA 0.60 N/A 7.0 4.1 31.6 36.2 2*RNN+1*X-Attn 203M 5.7
and memory improvements as explained in Section 4.4. We
alsocontrastLoRAfinetunewithfullfinetuneLLMbackbone
inthelastrow. TheASRandASTresultsaresimilarwhilein
practicethisLoRAbasedBESTOWmodeltakesmuchlonger
timetoconverge(170kv.s. 60ksteps).
Table 4 details the values from each introduced com-
ponent in the cross-attention speech feature extractor. The
speechencoderhereisalwaysinitializedwithasmaller110M
NGC ASR pretrained Fast Conformer-large and trained. In
Fig. 3. Latency-Quality tradeoff curves of streaming ASR
this study, we found two layers of transformer-like self-
withBESTOW-S-bidi.
attention (Self-Attn) and cross-attention (X-Attn) layers are
sufficient to get the optimal performance. Removing self-
attention in the layer results in significant degradation. As
describedinSection4.2,theself-attentioniscrucialtomodel
thehistoryofprevioustexttokenssoastobethequeryforthe
lattercross-attention. Toconfirmthis, wealsotriedanalter-
nativedesigninthelastrowbyusingtwolayersofRNNsto
modelthehistory,similartoLASarchitectureforend-to-end
ASR[56],whichcaneffectivelybringbacktheWER.
5.4. Streaminglatency-qualitytradeoff
Fig. 4. Latency-Quality tradeoff curves of simultaneous
speechtranslationwithBESTOW-S-bidi.
To complete the streaming story of BESTOW-S, we give
latency-qualitytradeoffcurvesforstreamingASRandsimul-
taneous speech translation in Figure 3 and 4 by varying K
6. CONCLUSIONS
in the inference time and reporting non-computation-aware
laggings. The trend is in line with state-of-the-art bespoke Inthiswork,weproposethefirstopenSpeechLLMsolution
systemsinASR[29]andAST[32]respectively. Optimizing thatenablesStreamingandMultitaskatscale(beyondASR)
computation-awarelaggingwillbeinterestingfuturetopic. at the same time. The solution is based on a different back-bone architecture from the popular Speech-LLaMA variants [13] A. Conneau et al., “Unsupervised cross-lingual rep-
that is based on cross-attention and read-write policy. The resentation learning for speech recognition,” arXiv
novel backbone unifies the offline and streaming modes and preprintarXiv:2006.13979,2020.
achieves state-of-the-art on several large-scale and multitask
[14] Y.Zhangetal.,“Googleusm:Scalingautomaticspeech
speech-to-text benchmarks, with lower training/inference
recognition beyond 100 languages,” arXiv preprint
cost. We will release the code and checkpoints to promote
arXiv:2303.01037,2023.
next-generationSpeechLLMusingthisbackbonedesign.
[15] A. Radford, J. W. Kim, T. Xu, G. Brockman,
7. REFERENCES C. McLeavey, and I. Sutskever, “Robust speech recog-
nitionvialarge-scaleweaksupervision,”2022.
[1] T. Brown et al., “Language models are few-shot learn-
ers,” Advances in neural information processing sys- [16] Y. Peng et al., “Owsm v3. 1: Better and faster open
tems,vol.33,pp.1877–1901,2020. whisper-stylespeechmodelsbasedone-branchformer,”
arXivpreprintarXiv:2401.16658,2024.
[2] G. Team, “Gemini: a family of highly capable multi-
modalmodels,”arXivpreprintarXiv:2312.11805,2023. [17] L. Barrault et al., “Seamless: Multilingual expres-
sive and streaming speech translation,” arXiv preprint
[3] J.-B.Alayracetal.,“Flamingo:avisuallanguagemodel
arXiv:2312.05187,2023.
forfew-shotlearning,”AdvancesinNeuralInformation
ProcessingSystems,vol.35,pp.23716–23736,2022. [18] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang,
J.Yu, W.Han, S.Wang, Z.Zhang, Y.Wuetal., “Con-
[4] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstruction
former: Convolution-augmentedtransformerforspeech
tuning,”Advancesinneuralinformationprocessingsys-
recognition,”inInterspeech,2020.
tems,vol.36,2024.
[19] D. Rekesh et al., “Fast conformer with linearly scal-
[5] D. Zhang et al., “Speechgpt: Empowering large lan-
ableattentionforefficientspeechrecognition,”in2023
guagemodelswithintrinsiccross-modalconversational
IEEE Automatic Speech Recognition and Understand-
abilities,”arXivpreprintarXiv:2305.11000,2023.
ingWorkshop(ASRU). IEEE,2023,pp.1–8.
[6] Y. Gong, A. H. Liu, H. Luo, L. Karlinsky, and
[20] A. Conneau et al., “Xtreme-s: Evaluating cross-
J. Glass, “Joint audio and speech understanding,” in
lingual speech representations,” arXiv preprint
2023 IEEE Automatic Speech Recognition and Under-
arXiv:2203.10752,2022.
standingWorkshop(ASRU). IEEE,2023,pp.1–8.
[21] J. Shi, D. Berrebbi, W. Chen, H.-L. Chung, E.-P.
[7] C. Tang et al., “Salmonn: Towards generic hearing
Hu, W. P. Huang, X. Chang, S.-W. Li, A. Mohamed,
abilities for large language models,” arXiv preprint
H.-y. Lee et al., “Ml-superb: Multilingual speech
arXiv:2310.13289,2023.
universal performance benchmark,” arXiv preprint
[8] J.Wuetal., “Ondecoder-onlyarchitectureforspeech- arXiv:2305.10615,2023.
to-text and large language model integration,” in 2023
[22] P. K. Rubenstein et al., “Audiopalm: A large lan-
IEEE Automatic Speech Recognition and Understand-
guage model that can speak and listen,” arXiv preprint
ingWorkshop(ASRU). IEEE,2023,pp.1–8.
arXiv:2306.12925,2023.
[9] Y. Fathullah, C. Wu, E. Lakomkin et al., “Prompting
[23] Z. Kong, A. Goel, R. Badlani, W. Ping, R. Valle, and
large language models with speech recognition abili-
B.Catanzaro,“Audioflamingo:Anovelaudiolanguage
ties,”arXiv:2307.11795,2023.
model with few-shot learning and dialogue abilities,”
[10] M. Wang et al., “Slm: Bridge the thin gap between arXivpreprintarXiv:2402.01831,2024.
speechandtextfoundationmodels,”in2023IEEEAuto-
[24] S. Radhakrishnan et al., “Whispering llama: A cross-
maticSpeechRecognitionandUnderstandingWorkshop
modalgenerativeerrorcorrectionframeworkforspeech
(ASRU). IEEE,2023,pp.1–8.
recognition,”arXivpreprintarXiv:2310.06434,2023.
[11] J. Bai et al., “Qwen technical report,” arXiv preprint
[25] C. Raffel et al., “Exploring the limits of transfer
arXiv:2309.16609,2023.
learningwithaunifiedtext-to-texttransformer,”Journal
[12] Z. Chen et al., “Salm: Speech-augmented language of Machine Learning Research, vol. 21, no. 140, pp.
model with in-context learning for speech recognition 1–67, 2020. [Online]. Available: http://jmlr.org/papers/
andtranslation,”inICASSP. IEEE,2024. v21/20-074.html[26] W.Yuetal.,“Connectingspeechencoderandlargelan- [40] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li,
guagemodelforasr,”arXivpreprintarXiv:2309.13963, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank
2023. adaptation of large language models,” arXiv preprint
arXiv:2106.09685,2021.
[27] A. S. Hussain, S. Liu, C. Sun, and Y. Shan, “m2ugen:
Multi-modal music understanding and generation with [41] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur,
the power of large language models,” arXiv preprint “Librispeech: anasrcorpusbasedonpublicdomainau-
arXiv:2311.11255,2023. diobooks,”inICASSP. IEEE,2015,pp.5206–5210.
[28] Q.Zhangetal.,“Transformertransducer: Astreamable [42] G.Chenetal.,“Gigaspeech:Anevolving,multi-domain
speech recognition model with transformer encoders asr corpus with 10,000 hours of transcribed audio,”
andrnn-tloss,”inICASSP. IEEE,2020. arXivpreprintarXiv:2106.06909,2021.
[29] V. Noroozi, S. Majumdar, A. Kumar, J. Balam, and [43] C.Wang,A.Wu,andJ.Pino,“Covost2andmassively
B. Ginsburg, “Stateful fastconformer with cache-based multilingual speech-to-text translation,” arXiv preprint
inference for streaming automatic speech recognition,” arXiv:2007.10310,2020.
arXivpreprintarXiv:2312.17279,2023.
[44] V.Noroozi,Z.Chenetal.,“Instructiondatageneration
[30] A.Graves,“Sequencetransductionwithrecurrentneural andunsupervised adaptationfor speechlanguage mod-
networks,”arXivpreprintarXiv:1211.3711,2012. els,”inInterspeech,2024.
[31] X. Ma, J. Pino, and P. Koehn, “Simulmt to simulst: [45] “Dynamic-superb leaderboard,” https://github.com/
Adapting simultaneous text translation to end-to- cyhuang-tw/dlhlp-dynamic-superb-leaderboard/blob/
end simultaneous speech translation,” arXiv preprint main/leaderboard.md.
arXiv:2011.02048,2020.
[46] C.-y. Huang et al., “Dynamic-superb: Towards
a dynamic, collaborative, and comprehensive
[32] X. Ma, A. Sun, S. Ouyang, H. Inaguma, and
instruction-tuning benchmark for speech,” arXiv
P. Tomasello, “Efficient monotonic multihead atten-
preprintarXiv:2309.09510,2023.
tion,”arXivpreprintarXiv:2312.04515,2023.
[47] “Canary-1b model,” https://huggingface.co/nvidia/
[33] S. Zhang and Y. Feng, “Information-transport-based
canary-1b,accessed: 2024-03-11.
policy for simultaneous translation,” arXiv preprint
arXiv:2210.12357,2022.
[48] P. Bajaj et al., “Ms marco: A human generated ma-
chine reading comprehension dataset,” arXiv preprint
[34] F.Seide,M.Doulaty,Y.Shi,Y.Gaur,J.Jia,andC.Wu,
arXiv:1611.09268,2016.
“Speechreallm–real-timestreamingspeechrecognition
with multimodal llms by teaching the flow of time,” [49] S. Papi, M. Gaido, M. Negri, and M. Turchi, “Over-
arXivpreprintarXiv:2406.09569,2024.
generation cannot be rewarded: Length-adaptive aver-
agelaggingforsimultaneousspeechtranslation,”arXiv
[35] E. Tsunoo, H. Futami, Y. Kashiwagi, S. Arora, and
preprintarXiv:2206.05807,2022.
S. Watanabe, “Decoder-only architecture for streaming
end-to-endspeechrecognition,”2024. [50] O. Kuchaiev, J. Li, H. Nguyen et al., “NeMo: a
toolkit for building ai applications using neural mod-
[36] V. Agostinelli, M. Wild, M. Raffel, K. A. Fuad, and
ules,”arXiv:1909.09577,2019.
L.Chen,“Simul-llm: Aframeworkforexploringhigh-
quality simultaneous translation with large language [51] P.Zhang,G.Zeng,T.Wang,andW.Lu,“Tinyllama:An
models,”arXivpreprintarXiv:2312.04691,2023. open-sourcesmalllanguagemodel,”2024.
[37] R. Koshkin, K. Sudoh, and S. Nakamura, “Transllama: [52] J. Kahn et al., “Libri-light: A benchmark for asr with
Llm-based simultaneous translation system,” arXiv limitedornosupervision,”inICASSP,2020,pp.7669–
preprintarXiv:2402.04636,2024. 7673,https://github.com/facebookresearch/libri-light.
[38] M. Wang, J. Zhao, T.-T. Vu, F. Shiri, E. Shareghi, [53] R. Ardila et al., “Common voice: A massively-
and G. Haffari, “Simultaneous machine transla- multilingualspeechcorpus,”inLREC,2020.
tion with large language models,” arXiv preprint
[54] V. Srivastav et al., “Open automatic speech
arXiv:2309.06706,2023.
recognition leaderboard,” https://huggingface.co/
[39] A.Vaswanietal.,“Attentionisallyouneed,”Advances spaces/huggingface.co/spaces/open-asr-leaderboard/
inneuralinformationprocessingsystems,vol.30,2017. leaderboard,2023.[55] A.Conneauetal.,“Fleurs:Few-shotlearningevaluation
ofuniversalrepresentationsofspeech,”inSLT. IEEE,
2023,pp.798–805.
[56] W.Chanetal., “Listen, attendandspell: Aneuralnet-
workforlargevocabularyconversationalspeechrecog-
nition,”inICASSP. IEEE,2016,pp.4960–4964.