Interactive Topic Models with Optimal Transport
GarimaDhanania* ShesheraMysore*† ChauMinhPham
MohitIyyer HamedZamani AndrewMcCallum
UniversityofMassachusettsAmherst, MA,USA
{smysore,zamani,mccallum}@cs.umass.com {gdhanania,ctpham,miyyer}@umass.edu
Abstract modeling. Acrossthesemodels,avaluablefeature
forpractitionersistheirabilitytorepresentlatent
Topicmodelsarewidelyusedtoanalyzedoc-
topicswithinterpretabledescriptorssuchasword,
umentcollections. Whiletheyarevaluable
sentence,ordocumentsassignedtolatenttopics.
fordiscoveringlatenttopicsinacorpuswhen
Whilelatenttopicsarevaluableforseveralforms analystsareunfamiliarwiththecorpus,ana-
lystsalsocommonlystartwithanunderstand- ofanalysis(Robertsetal.,2013;Hoyleetal.,2019),
ingofthecontentpresentinacorpus. This topic models fall short of practitioners’ expecta-
maybethroughcategoriesobtainedfroman tions when analysts don’t wish to be biased by
initialpassoverthecorpusoradesiretoan- machinegeneratedtopics,wishtohavetopicscap-
alyzethecorpusthroughapredefinedsetof
turetheirownunderstandingofthecorpus,orana-
categoriesderivedfromahighleveltheoret-
lyzeacorpusinatop-downmannerthroughcate-
icalframework(e.g. politicalideology). In
goriesdefinedbyananalyticalframework(Hong
thesescenariosanalystsdesireatopicmodel-
ingapproachwhichincorporatestheirunder- etal.,2022;CarlsenandRalund,2022;Jasimetal.,
standingofthecorpuswhilesupportingvari- 2021). Oneapproachtoaddresssuchconcernsmay
ousformsofinteractionwiththemodel. In involvelabelingatrainingsetofdocumentswith
thiswork,wepresentEDTM,asanapproach categories,followedbyclassificationorsupervised
for label name supervised topic modeling.
clusteringtolabelthewholecollection(Finleyand
EDTMmodelstopicmodelingasanassign-
Joachims,2005). However,theseprovetobetime
ment problem while leveraging LM/LLM
consumingduetotheneedtolabeldataandbrittle
baseddocument-topicaffinitiesandusingop-
whenthesetofcategoriesevolveaspractitioners’
timaltransportformakinggloballycoherent
topic-assignments. Inexperiments,weshow understandingofthecorpuschanges.
theefficacyofourframeworkcomparedto Analternativeexploredinalargebodyofwork
few-shotLLMclassifiers,andtopicmodels
onprobabilistictopicmodelsinvolvessupervising
based on clustering and LDA. Further, we
latenttopics. Thisworkhasexploreduseofdocu-
showEDTM’sabilitytoincorporatevarious
mentmetadata(Cardetal.,2018),seedwordsper
formsofanalystfeedbackandwhileremain-
topic(Jagarlamudietal.,2012),andconstraintsto
ingrobusttonoisyanalystinputs.
superviseandrefinelatenttopics(Huetal.,2011).
However,generativetopicmodelsprovechalleng-
1 Introduction
ing to scale to large corpora (Lund et al., 2017),
Topic models have had a long history of devel- are influenced by document lengths (Hong and
opment and use for exploring document collec- Davison,2010),andarelimitedintheirabilityto
tionsandrepresentingdocumentsandcollections leverageperformantpre-trainedlanguagemodels
for downstream tasks (Boyd-Graber et al., 2017). (Hoyle et al., 2022). While an emerging body of
Whiletopicmodelsaremostcommonlyassociated workhaseffectivelyleveragedpre-trainedlanguage
with probabilistic generative models (Blei et al., models for unsupervised topic modeling (Pham
2003),otherapproachesleveragingmatrixfactor- et al., 2024; Wang et al., 2023; Thompson and
ization (Lund, 2019) and clustering embeddings Mimno, 2020) exploration of contemporary pre-
frompre-trainedlanguagemodels(Thompsonand trainedmodelsforinteractivetopicmodelinghas
Mimno, 2020) have also been explored for topic beenlimited. Ourworkfillsthisgap.
Inthiswork,weproposeaframeworkforlabel
*Equalcontribution.
†CurrentlyaPostdoctoralResearcheratMicrosoft. name supervised topic modeling. We show label
4202
nuJ
82
]LC.sc[
1v82991.6042:viXra1 Document-topic scoring 2 Topic assignment tipletopicsinLornotopicsatall. Thelattercase
analyst at no
alpi yc
s t
a1
t no
alpi yc
s t
a2
t no
alpi yc
s t
3
topic
4
analyst at no
alpi yc
s t
a1
t no
alpi yc
s t
a2
t no
alpi yc
s t
3
topic
4
r ce ap pr te us re en tt hs ea ss ce ele nc at riv ioe wto hp eic reas as nig aln ym ste sn it ni tn ut ie tn iod ne sd oto
f
thetopicsLareincorrectorincomplete,requiring
thatsomedocumentsortopicsremainunassigned.
C Q Weassumedocumentsd ∈ Dtobeshortsentences
LM/LLM Complete
scores or partial ormulti-sentenceparagraphs.
assignments
Further,weareinterestedinsupportingdifferent
formsofinteractiveanalysisandexplorationofD-
Figure1: Interactivetopicmodelingwith EDTM
thisisdonethroughdifferentformsoftopiclabels
consistsoftwosteps,document-topicscoringfor
l. Topiclabelsmayberepresentedasnaturallan-
analyst provided topic names using LM/LLM bi-
guagenamesfortopics,examplewordsorlonger
encodersandcross-encodersfollowedbycomputa-
descriptionsoftopics,orexampledocumentsrep-
tionofpartialorcompletetopicassignmentsusing
resentingatopic. Thevariousformsoftopiclabels
optimaltransport. Analysttopicnamesmaytake
combinedwith selectivetopic assignmentallows
onvariousformssuchaslabelnames,descriptions,
forrichwaysofinteractionwithatopicmodel.
ordocuments,tosupportrichformsofinteraction. 5
3 ProposedApproach
namesasaflexibleformofinteractionwithtopic
We propose a flexible framework for interactive
models – allowing analysts to specify topics as
topic modeling, EDTM, consisting of two com-
shortlabelnamesakintoclasslabels,longerlabel
ponents: adocument-topicscoringleveragingex-
descriptions,aswellasseeddocuments. Wemodel
pressive LM/LLMs, and document-topic assign-
topicmodelingasanassignmentproblem,requir-
ment leveraging optimal transport assignment al-
ingagloballycoherentassignmentofdocuments
gorithms. Weexplorescorescomputedusingpre-
totopics,andleveragethewellexploredmachinery
trainedLMbi-encodermodelsaswellasexpressive
ofoptimaltransport(Peyréetal.,2019)forinter-
LLM crossencoders allowing analysts to specify
active topic modeling. Use of optimal transport
theirunderstandingoftopicsinavarietyofintuitive
offersseveraladvantages: assignmentalgorithms
naturallanguageforms. Fordocument-topicassign-
capableofleveragingGPUcomputation,anability
ment, optimal transport algorithms allow assign-
to leverage document-topic similarities obtained
mentdecisionstobemadeinagloballycoherent
from pre-trained language models, and a mature
mannerandnaturallyincorporatedocument-topic
bodyofworkonassignmentalgorithmswhichmay
affinitiesfromexpressiveLLMscoringfunctions.
beappliedforvarioustopicmodelingapplications.
Further,weleveragepartialassignmentalgorithms
Werefertoourapproachtointeractivetopicmod-
(Benamou et al., 2015) that selectively exclude
eling as an editable topic model, EDTM (Figure
high cost document-topic assignments, to ensure
1). Inexperiments,weshowEDTMtoinducehigh
thattopic-assignmentsremainrobusttopotentially
quality topics compared to a range of baselines,
noisy or incomplete labels provided by users – a
supportvariousformsofinteractionfromanalysts,
likelyoccurrenceininteractivetopicmodeling. Fi-
andinducerobusttopicsevenwhenpresentedwith
nally,sincecomputationoftheD×Lassignment
noisy analyst supervision. Code and datasets for
remain expensive for large corpora with optimal
ourworkwillbereleaseduponacceptance.
transport, we compute approximate assignments
throughbatchedcomputationofassignments(Fa-
2 ProblemDefinition
trasetal.,2020). Wediscusstheassignmentalgo-
We consider a label supervised topic modeling rithmsandscoringfunctionsusedin EDTM next.
problemwherewearepresentedwithacollection
3.1 OptimalTransportTopicAssignment
of documents D that an analyst is interested in
studying and a set of topic labels l ∈ L that the Optimaltransportmaybeseenasawaytocompute
analystwouldliketoorganizeD byorbelievesis aglobalminimumcostalignmentbetweensetsof
containedinD. Weareinterestedinpredictinga points given the cost of aligning individual pairs
partitioningofthedocumentsinD intothelabels of points. Additionally, OT problems associate
Lwhereeachdocumentcouldbeassignedtomul- thesetswithprobabilitydistributionswherevalidalignmentssatisfyingcapacityconstraintsspecified Algorithm1CompletetopicassignmentinEDTM
overthesedistributions. SolutionstotheOTprob- 1: Input: D,L,f
dist
lem result in sparse and soft alignments between 2: C←f (D,L) ▷Computepairwisecosts
dist
thetwosetswhengivenmeaningfulpairwisecosts 3: Q∗ ←zeros(|D|,|L|) ▷Initializeplanwithzeros
from most scoring models, this makes them well 4: forepocheofE do
5: forbatchbofBdo
suitedtotopicmodelingwhileleveragingexpres-
siveLM/LLMmodels. 6: xb D,x L ←uniform(b),uniform(L)
7: Q∗b ←argmin⟨Cb,Qb⟩−1/λH(Qb)
Specifically, we assume the source and target
Qb∈Sc
pointstobedocumentandlabelsets,D andL,of 8: Q∗[b,:]←Q∗b ▷Updateplanforbatch
size n and m respectively. We assume them to 9: endfor
be distributed according to distributions x and 10: endfor
D
x , with pairwise costs C ∈ Rn×m. Since the 11: Q∗ ←Q∗/B·E ▷Normalizeplan
L + 12: {l∗}|D| ←argmax Q∗
distributionsareunknown,wetreatx andx to i=1 l
D L
be uniform distributions. The solution to the OT
problem is a soft assignment, the transport plan
we leverage algorithms of Cuturi (2013) and Be-
Q∗, which converts x into x by transporting
D L namou et al. (2015) implemented in off the shelf
probabilitymassfromx tox whileminimizing
D L solvers.1 Topicassignmentsaredescribedfurther
anaggregatecostW,referredtoastheWasserstein
in§3.3andAlgorithm1.
Distance. Capacity constraints on Q result in a
constrainedlinearoptimizationproblem: 3.2 DocumentTopicScoring
To allow users to provide rich natural language
W = min⟨C,Q⟩ (1)
Q∈S topic targets L, we leverage LM/LLM models
to compute document-topic costs C[d,l] in Eq
We examine two kinds of capacity constraints
(1). Here, we leverage off-the-shelf BERT based
onQresultingindifferentfeasiblesetsS –acom-
bi-encodersandexpressiveT5-XLcrossencoders
plete assignment which assigns all source points
trained for predicting query-document relevance
to target points and a partial assignment which
in IR tasks (Nogueira et al., 2020; Lin et al.,
doesnotrequireallsourceandtargetpointstobe
2020). With a bi-encoder we compute pairwise
assigned. This allows potentially noisy topic as-
costsasL2distancesbetweendocumentandlabel
signments to be ignored. Complete assignment
embeddings: L2(f (d),f (l)). From crossen-
ensuresthatthecolumnsandrowsofQmarginal- BE BE
coders,f ,wefirstobtaintheprobabilityofrele-
ize respectively to x and x , resulting in S = CE
D L c
vanceP (rel = 1|d,l). Thenweobtaincostsas:
{Q ∈ Rn×m|Q1 = x ,QT1 = x }. Par- CE
+ m D n L 1−P /max P . The normalization per input
tial assignment requires only a fraction p < 1 of CE l CE
densuresthatcostsremaincomparableacrossall
the source or target points to be assigned result-
inputs with a minimum value of 0 across all doc-
ing in: S = {Q ∈ Rn×m|Q1 ≤ x ,QT1 ≤
p + m D n uments. Wenotethatwhilecomputationofcosts
x ,1TQT1 = p}. Themassp,topreserveinQ∗
L
with crossencoders is an expensive operation, re-
maybespecifiedbyanalystsandcapturesthefrac-
cent work (Yadav et al., 2022) leveraging matrix
tionofhighqualitytopicassignmentsthatcanbe
factorization to efficiently compute crossencoder
madeforD andL.
basedscoresforlargecorporapromiseareadypath-
In practice, for complete as well as partial as-
waytowardscalingourapproachtolargercorpora
signmentswesolveanentropyregularizedvariant
–weleaveexplorationofthistofuturework. We
of Eq (1): W = min⟨C,Q⟩−1/λH(Q). While
refer to bi-encoder and crossencoder variants as
exactsolutionstoEq(1)requireO(n3)computa-
tions,entropyregularizationallowsbothproblems
EDTM
BE
and EDTM CE. We provide further im-
plementationdetailsin§4.1.
to be solved using iterative methods with an em-
piricalcomplexityofO(n2)andsolverscapableof
3.3 HardeningTopicAssignments
leveragingGPUcomputation. Thisallowsscaling
Hereweoutlinethefinalprocedureforobtaining
tolargerdatasets. Tofurtherspeeduptopicassign-
hard topic assignments for documents from Q∗.
ments for large D, we compute Q in batches of
Both partial and complete topic assignments in
sourcepoints,withbatchessized< |D|. Tosolve
complete and partial batch assignment problems 1OTsolvers:https://pythonot.github.ioQ∗ may assign documents to a variable number pairedwith1of15high-leveland114finer-grained
oftopics,howeverweonlyretainthetoptopicas- labels(Merityetal.,2018).
signmentperdocumentfortomakecomparisons BillscontainsUScongressbillsummariesfrom
tobaselinemethodsthatmakesingletopicassign- January2009toJanuary2017manuallypairedwith
ments, representing the majority of recent base- 1of21high-leveland45finer-grainedlabels(Adler
linesthatwecompareto. Weleaveexplorationof andWilkerson,2018;Hoyleetal.,2022).
multi-topicassignmentstofuturework. Complete
Bookgenome contains book descriptions from
assignmentsaremadeas: argmax Q∗. Forpartial
l Goodreads scored by crowd workers against 727
assignment,wefirstmarginalizeQ∗ overthetopic
user-generatedtags(Kotkovetal.,2022). Ofthese
labels as: x∗ = (cid:80) Q[·,l]. Then, we only make
D l weretainonlythemaxscoringtagperbook,and
assignmentsforthefractionpofdocumentswhich
exclude tags which are used fewer than 5 times.
havethehighestvaluesinx∗ (addingastepafter
D This results in 226 tags which contain a mix of
Line11inAlgorithm1). Recallthatprepresents
high level and finer grained tags, which serve as
the fraction of mass conserved in making partial
thegoldlabelsinourevaluationsof EDTM (§5).
assignmentsbytheoptimaltransportsolutionofEq
Evaluation metrics. We evaluate EDTM us-
1. In practice, points which don’t receive assign-
ingextrinsicclusteringevaluationmetricsbycom-
mentsrepresenthigh-costtopicassignmentswhich
paringpredictedclustersC′,togoldclustersC in-
areunlikelytoreceiveaccurateassignments.
ducedbypredictedandgoldlabelassignmentsre-
spectively. Wereportthesetoverlapbasedmetric
4 Experiments
P (Zhao, 2005; Amigó et al., 2009) and the mu-
1
We experiment with EDTM in a variety of En- tualinformation(MI),I(C,C′)(Meila˘,2007). P 1
glish datasets commonly used to evaluate topic- istheharmonicmeanofclusterpurityandinverse
models. We compare EDTM against methods purity,andisboundedbetween0and1. Whilepu-
for few-shot classification leveraging LM/LLMs, rityandinversepuritymaybetriviallymaximized
clusteringmethods,andLDAbasedtopicmodels. byoverorunder-segmentingD into1or|D|clus-
We evaluate EDTM using extrinsic metrics com- tersrespectively,theharmonicmeantradesthese
monlyusedtoevaluateclusteringsincewepropose quantitiesoff. Notably,P 1 capturestheuserexperi-
EDTMasamodelfordataexploration. Further,we enceindataexplorationsetups–thecoherenceof
evaluate EDTM invariousinteractivesetupslever- clustersexperiencedbyauserwhileaccountingfor
aging varying human interaction and noise. We over-segmentation. WhileP 1 representsacluster
outlineourprimaryexperimentalsetupnext, and levelmetric,MIrepresentsainstancelevelmetric–
detailinteractionsetupsintherespectivesections. tellingusthereductioninuncertaintyofthelabel
forapointaccordingtoC ifweknowitslabeling
4.1 ExperimentalSetup accordingtoC′.
Inourevaluationswedon’temploytheNormal-
Datasets. Weusefourdatasetsofshort-tomedium-
izedMutualInformation,NMI:I(C,C′)/(H(C)+
length texts, each accompanied by a correspond-
H(C′))commonlyusedinpriorworksinceNMI
ing gold label. These datasets feature a diverse
resultsinhighermetricsforimbalancedclusterings
range of domains, including Wikipedia articles,
i.eclusteringswithlowentropy(H(C)orH(C′)),
CongressionalBillssummaries,Twitterposts,and
eveniflowerinmutualinformationI(C,C′). No-
Goodreadsbookdescriptions. Asaresult,thefour
tably, all our datasets represent a realistic imbal-
datasets exhibit varying types and numbers of la-
ancedlabelingraisingthechancesofinflatedNMI.
bels (|L|). Our datasets and their topic labels are
Further,wedon’trelyontheAdjustedRandIndex
summarizedinTable1.
(ARI)sinceitevaluatesifpairsofpointsbelongto
TwittercontainsshorttweetsfromSep2019to
the same cluster in C and C′ – while meaningful,
Aug 2021 paired with 1 of 6 high level labels as-
pairwise relationships are less aligned with ana-
signedbycrowdworkers(Antypasetal.,2022).2
lystworkflowsofclusterexplorationwheretopic
WikicontainsWikipediaarticlesdesignatedto
modelsarecommonlyemployed.
be“Goodarticles”basedonadherencetoeditorial
standards by Wikipedia editors. The articles are
Baselines. WecompareEDTMagainstvarious
few-shot classification and clustering/topic mod-
2HFDatasets:cardiffnlp/tweet_topic_single eling approaches. Clustering approaches range:Dataset |D| |L| |d| Labelnameandfrequency
Twitter 4373 6 28 pop culture,1705;sports and gaming,1528;daily life,647
Wiki 8024 15 2888 Media and drama,1118;Warfare,1112;Music,1007
Bills 15242 21 215 Health,1755;Public Lands,1355;Domestic Commerce,1295
Bookgenome 9177 226 170 fiction,548;adventure,352;suspense,301
Table1: Summaryofthedatasetsusedforexperiments: numberofdocuments(|D|),numberoflabels
(|L|),andaveragelengthofdocuments(|d|). Wealsopresentthetopthreelabelsandtheirfrequencies,
illustratingthetopicnamesandlabelskewspresentinourdatasets.
LDA: Representsawidelyusedapproachtotopic While clustering approaches remain agnostic
modeling representing documents as mixtures of to analyst provided label names, zero- and few-
latent topics, in turn represented as mixtures of shotclassificationapproachesleverageanalystpro-
the corpus vocabulary. Document-topic distribu- videdlabelnames,thesebaselinesrange: GPT3.5-
tions are used for topic assignment. We use the Turbo: This approach uses few-shot prompting
MALLETimplementationofLDAwithGibbssam- to assign one of the L labels to each input docu-
pling (McCallum, 2002). We set |V| = 15,000, mentwithGPT3.5-Turbo. NearestNeighbor(NN):
α = 1.0, β = 0.1, and run LDA for 2,000 it- Thisnearestneighbormodelpredictsthelabelmost
erations with optimization at every 10 intervals. similar to the input text using a similarity metric
BertTopic: Representsanapproachtotopicmod- identicaltoEDTM (§3.2),i.elabelsarepredicted
elingimplementedinthewidelyusedBERTTopic as argmin C. We differentiate Bi-Encoder and
l
package(Grootendorst,2022). Topicmodelingis crossencoderapproachesasNN andNN . Note
BE CE
performed by embedding input texts with a pre- thatNNmayseenasthegreedyversionofEDTM,
trainedBiEncoder3,reducingdimensionalitywith makinglabelassignmentsgreedilyforeachd ∈ D
UMAP,andclusteringresultingembeddingsusing andmaybeseenasmostsimilartoEDTM.Forla-
HDBSCAN.KMeans: Astandardclustering-based belassignmentwithTopicGPTandGPT-3.5-Turbo,
approach(MacQueenetal.,1967;Lloyd,1982)to wetruncatetheinputdocumentandourtopiclistif
topic modeling, embedding inputs using the pre- theircombinationexceedtheLLMcontextlength
trained bi-encoder f
BE
used in EDTM then per- of 4,096 tokens.5 Given the large label space in
formingKMeansclustering. Thishasshowntobe Bookgenome,truncationofthetopiclistisneces-
an effective approach to topic modeling (Thomp- sary. In such cases, we include only a top set of
sonandMimno,2020)–notablyhowever,weclus- candidatelabels,whichareselectedbasedontheir
ter document embeddings rather than contextual- cosine similarity with the input text as computed
izedwordembeddings. TopicGPT: ALLMbased byapretrainedBi-Encoder.6
topicmodelingapproach,whichinvolvesprompt- Implementation Details. For computing
ingGPT-4togeneratetopicsbasedonasmallsub- document-topiccostsCinEDTM,forf BEweusea
set of D, and then using GPT-3.5-Turbo for as- 110MparameterBERT-likeBi-Encoderpre-trained
signing the generated topics to all documents in for dense retrieval on weakly supervised query-
D (Pham et al., 2024). For LDA, BertTopic, and documentpairsconstructedfromweb-forums. For
KMeans we set the number of clusters to be |L|. f ,weleveragetheMonoT5crossencoderbased
CE
The number of clusters in TopicGPT vary across on T5-XL and trained for query-document rele-
datasets4 since we cannot fix the number of top- vanceontheMS-MARCOdataset(Nogueiraetal.,
ics generated by GPT-4. Note here, that while 2020). OwingtothesizeofBookgenome,welever-
we report the performance of TopicGPT we pre- ageaT5-Largecrossencodertokeepexperiments
clude extensive comparison to it given its use of feasible.7 While,theaveragenumberoftokensin
twohighlyperformantcommercialLLMs,instead input texts across our datasets of Table 1 are 28,
we treat it as an upper bound in performance of
existingapproaches. 5Wetruncateeitherthetopiclistorthedocumentifeither
componentexceedsapproximatelyhalfoftheinputcontext
length(around1,700tokens).
3HFModel:all-MiniLM-L6-v2 6HFModel:all-MiniLM-L6-v2
4k=15forTwitter,31forWiki,79forBills,and482for 7HFModels; f : multi-qa-mpnet-base-cos-v1,f :
BE CE
Bookgenome. monot5-3b-msmarco-10kBookgenome Bills Wiki Twitter
|L|: 226 |L|: 21 |L|: 15 |L|: 6
Method P MI P MI P MI P MI
1 1 1 1
TopicGPT 0.18 1.97 0.57 1.66 0.73 1.79 0.75 0.70
LDA 0.17 1.37 0.56 1.30 0.73 1.62 0.49 0.29
BertTopic 0.15 1.04 0.39 0.93 0.52 1.17 0.53 0.31
KMeans 0.16 1.77 0.46 1.27 0.55 1.37 0.55 0.58
GPT-3.5-Turbo 0.25 1.92 0.51 1.21 0.71 1.59 0.55 0.21
NN 0.17 1.74 0.52 1.19 0.57 1.10 0.64 0.57
BE
EDTM
BE
0.17 1.84 0.54 1.22 0.57 1.11 0.62 0.55
NN 0.20 1.76 0.58 1.34 0.61 1.25 0.71 0.68
CE
EDTM
CE
0.20 1.77 0.58 1.35 0.65 1.37 0.72 0.70
Table2: ClusterqualitywithEDTM.Underlineforabaselinerepresentsthebestevaluationmetric,and
underlineforNNandEDTM indicatebetterormatchedperformancetothebestbaseline. Boldindicates
the better performing model between NN and EDTM. We compare most closely to NN given that it
representsthemostsimilarapproachto EDTM aswenotein§4.1.
2888,215,and170–weretainthefirst450tokens in§4.1,duetoitslimitedcontextlength,GPT-3.5-
tomeetlengthlimitationsofLM/LLMs. Further, Turbo re-ranks labels, this differs from the three
in inputing label text to f we format labels as otherdatasets–relyingonafirststageretrievalof
BE
questions,e.g.forWiki,“IsthisaWikipediaarticle labelsusingabi-encoderfollowedbyLLMbased
about LABEL?” to mimic the structure of training assignment and likely explains its strong perfor-
dataforf . Next,forcomputingcompleteassign- mance. Thismayalsorepresentameaningfulstrat-
BE
ments with optimal transport, we set the entropy egyforlabelsupervisedtopicmodelingwithlarge
regularizerλ = 1,computeQinbatchesbofsize Lthatmaybeexploredinfuturework.
500andaveragedover3epochs. Inexperimentswe
distinguishbi-encoderandcrossencoderversions EDTM performance. We begin by examin-
as EDTM
BE
and EDTM CE. ing EDTM compared with NN. Here, note that
EDTM
BE
matches or outperforms NN
BE
in three
4.2 MainResults
of four datasets and EDTM
CE
matches or outper-
Table2compares EDTM againstbaselinecluster- forms NN
CE
in all four datasets – indicating the
ing as well as zero- and few-shot prediction ap- value of joint assignment of texts to labels over
proacheson4datasetsvaryingincharacteristics. thegreedyassignmentofNN.Next,wenotethat
Baselineperformance. Webeginbyexamining use of a crossencoder improves upon the results
theperformanceofbaselinemodels. First,wenote ofabi-encoderinbothNNandEDTM indicating
thatLDAshowsstrongperformancecomparedto thevalueofmoreexpressivetextsimilaritymodels.
other clustering models, KMeans and BertTopic. Finally,wecomparetheperformanceofEDTM
BE
Next,LDAseessignificantlylowerperformanceon and EDTM
CE
with the best baseline approaches.
shorterTwittertextsandseesperformancenearing Hereweseethat EDTM resultsinimprovements
thatofTopicGPTonthesignificantlylongertexts comparedtoclusteringapproachesinBookgenome,
in Wiki – this trend mirrors prior results of poor Bills, and Twitter. This may be attributed to the
performance on short texts (Hong and Davison, effective use of label name supervision absent in
2010). Next,weconsidertheperformanceofGPT- clustering approaches. Further, EDTM methods
3.5-Turbo. First,wenotethatGPT-3.5-Turbosees alsooutperformGPT-3.5-Turbobasedassignment
consistentlylowerperformancecomparedtoTop- in Bills and Twitter data indicating their value in
icGPTin3of4datasets–indicatinganinabilityof domainslikelytobemissingfromLLMpretrain-
GPT-3.5-Turbotomakehighqualityassignments ing data. Finally, EDTM
CE
also approaches the
with analyst provided labels L. However, it sees performanceofTopicGPTinBills,Bookgenome,
strongerperformanceinBookgenome. Aswenote and Twitter indicating its ability to induce highBills Wiki Bookgenome Bills Wiki Twitter
|L|:226 |L|:21 |L|:15 |L|:6
|L|: 21 |L|: 15
Method P1 MI P1 MI P1 MI P1 MI
Method P 1 MI P 1 MI NNBE 0.23 1.91 0.54 1.25 0.53 0.97 0.64 0.46
EDTMBE 0.19 1.94 0.54 1.26 0.56 1.08 0.56 0.41
GPT-3.5-Turbo 0.51 1.21 0.71 1.59
SeededLDA 0.48 1.22 0.65 1.71 Table 4: Cluster quality with topic labels repre-
sentedwithaveragedembeddingsofhighprecision
NN 0.55 1.27 0.65 1.32
BE
seeddocumentsretrievedusingthetargetlabelwith
EDTM
BE
0.58 1.31 0.67 1.36
aretrievalmodel.
NN 0.63 1.44 0.72 1.56
CE
EDTM
CE
0.65 1.48 0.74 1.62
Results. InTable3,firstwenotethatbothNN
Table3: Clusterqualitywithfinergrainedsupervi-
andEDTM outperformGPT-3.5-TurboandSeed-
sionprovidedperlabelintheformofseedwords.
edLDA in Bills, and NN and EDTM outperform
BoldindicatesbetterperformancebetweenNNand
baselinesinWikiwithcrossencoders. Further,com-
EDTM,andunderlineindicatesbetterperformance
paringtoTable2,wenotethatadditionoflabelde-
comparedtoabaseline.
scriptionsconsistentlyimprovedperformancefor
NNand EDTM.ThisindicatesNNand EDTM’s
ability to incorporate rich natural language topic
qualityclustersatparwithlargescaleLLMswhile
labelsfromanalysts. Finally, wenotethatacross
adheringtoanalystprovidedtopiclabels.
Bills and Wiki, EDTM consistently outperforms
5 InteractionExperiments NNwithbi-encoderandcrossencodertextsimilari-
ties–indicatingjointassignmentstobenefitfrom
InTables3,4,and5wepresentresultsinvarious improvedsimilarities/costestimates.
interactive scenarios demonstrating respectively,
the ability of EDTM to incorporate finer grained 5.2 Seeddocumentsastopiclabels
labeldescriptions,seeddocuments,andmakehigh Setup. Here, we simulate a scenario where an-
qualitytopicassignmentsinthepresenceofincom- alysts use topic names in L to perform a search
pletetopicnames. Wemakecompleteassignments over the corpus D, verifies their correctness, and
forlabeldescriptionsandseeddocumentsandpar- uses the verified sample documents as topic tar-
tialassignmentsforincompletetopicnames. For gets. This setup also mirrors a common sce-
eachinteractiveevaluationwefirstdescribetheex- nario where seed documents serve as queries
perimental setup and follow with a discussion of for corpus exploration (Wang and Diakopoulos,
experimentalresults. 2021). Here, we compare NN and EDTM alone
given that few-shot classification with GPT-3.5-
5.1 Seedwordsastopiclabels
Turbo runs into context length limitations in us-
Setup. Inthisexperimentwesimulateascenario ing document examples for our datasets. Fur-
whereananalystauthorslongerformtopicdescrip- ther,weonlyexperimentwithbi-encodervariants
tionsinsteadoftopicnamesalone. Welimitexper- given that crossencoders remain limited by con-
imentstotheBillsandWikidataset,andusetheir text length limitations for larger number of seed
finergrainedtopiclabelstogeneratedescriptions documents. For NN
BE
and EDTM
BE
we com-
foreachtopic. Weformatthesefinergrainedtopics putecostsCbetweendocumentsdandtopiclabels
into a natural language description for the target l, using the top five verified retrievals per label
labelsetL,forexample: “Isthisawikipediaarticle as: L2[f (d),mean f (dk)]. We choose
BE k=1...5 BE l
about Media and drama or Television or Film or k = 5 to represent a reasonable effort to verify
Actors?”. ForNNand EDTM,thesericherlabels label-documentcorrectnessbyananalyst.
areusedtocomputedocumenttopiccostsCthat Results. In Table 4, we note that EDTM
BE
areusedfortopicassignmentwithbi-encodersor matchesoroutperformsNN inBills,Wiki,and
BE
crossencoders. Here, we also compare to Seed- Bookgenome(MI)indicatingitsabilitytoincorpo-
edLDA (Jagarlamudi et al., 2012), an LDA topic rateseeddocumentsupervision. Wealsonote,com-
modelincorporatinguserprovidedseedwordsinto paring to Table 2 that while NN
BE
and EDTM
BE
inducedtopics. sees improvement from using seed documents inBookgenome Bills Wiki Twitter coder costs, EDTM CE sees stronger performance
|L|:226 |L|:21 |L|:15 |L|:6
than NN. Note here that in the presence of miss-
Method P1 MI P1 MI P1 MI P1 MI ing labels in L, both NN and EDTM could
NNBE 0.18 1.76 0.51 1.19 0.58 1.13 0.65 0.54
makealternativetopicassignmentsfordocuments
EDTMBE 0.17 1.76 0.51 1.17 0.56 1.09 0.64 0.52
NNCE 0.21 1.79 0.55 1.27 0.62 1.29 0.71 0.65 whichcouldhavereceivedl e whileleavingafrac-
EDTMCE 0.20 1.76 0.55 1.31 0.64 1.33 0.73 0.65 tion p of documents unassigned. For example,
when Language and literature topic name
Table 5: Cluster quality with a topic label omit-
tedfromthelabelsetL,followingwhichNNand
is excluded from L in Wiki, EDTM
CE
most fre-
quentlymakesassignmentsto: Media and drama,
EDTM make partial assignments i.e witholding
History, Philosophy and religion, and oth-
predictionsforcertaininputs. Thissetupsimulat-
ers. Manual examination revealed these to often
ingascenariowhereanalystsmaynotlistalltopic
be reasonable. However, since we only consider
labels due to insufficient knowledge of a corpus.
singletopicassignmentsinourevaluationsevalu-
Reported numbers are averaged over metrics ob-
atingalternativetopicassignmentsormulti-topic
tainedfromexcluding3highfrequencytopiclabels
assignments,moregenerally,remainsfuturework.
oneatatime.
Nevertheless, these results indicate the ability of
EDTMtomakehighqualitytopicassignmentsde-
spitereceivingincompletetopicsetsLindicating
Bookgenome and Bills, they see drops in perfor-
anabilitytohandletheerrorslikelyininteracting
manceindatasetswithfewerlabels,WikiandTwit-
withusers.
ter. This follows from the finer grained labels of
BookgenomeandBillsbeingbetterrepresentedus-
6 RelatedWork
ing high precision seed documents. However, in
WikiandTwitter,seeddocumentsareonlylikelyto
Webeginbydiscussingpriorworkoninteractive
representcertainaspectsofthehigherlevellabels.
and supervised topic modeling. Then we discus
Theresultinglowerqualitydocument-topicsimilar-
priorworkleveragingoptimaltransportandlarge
ities result in especially degraded assignments in
languagemodelsfortopicmodeling.
EDTM
BE
onTwitter. However,inthepresenceof
Interactive topic modeling. Topic modeling
finergrainedlabelsintheremainingthreedatasets
hasfocusedonlearninghumaninterpretabletopics
EDTM resultsinhighqualitytopicassignments.
givenonlyacorpusofdocuments. Avastbodyof
workhasexploredprobabilisticgenerativemodels
5.3 Partialassignmentoftopics
fortopicmodeling,representingtopicsasdistribu-
Setup. Todemonstratethevalueofmakingpartial tionsoverwordtypesanddocumentsasmixtures
assignmentswesimulateascenariowherealabel of latent topics (Boyd-Graber et al., 2017; Lund,
ismissingfromthetargetlabelsetL,e.g.duetoan 2019). Wefirstexaminethelineofworkwhichhas
analysthavinginsufficientknowledgeofthecorpus soughttoincorporatesupervisionfromusersinto
topics. Thiscreatesascenariowhereaperformant thesemodels. Supervisionfromusershasbeenof
modelshouldnotmaketopicassignmentsforsome three broad types: (1) document labels (e.g. sen-
inputs. Specifically,weexcludeonelabel,l from timent)andmetadata(eg.dates)pairedwitheach
e
Lselectedatrandomfromthemostfrequent5la- document,(2)userspecifiedseed-wordsavailable
bels and make a partial assignment with NN or atthecorpuslevel,and(3)throughconstraints(e.g.
EDTM.Werepeatthisprocedurewith3different “must-link”)overlatenttopics. Whileworkon(1)
l labelsand reportaveraged performanceover3 and(2)havenothistoricallybeenconsideredinter-
e
different clusterings. In each case, p is set to the activetopicmodeling,weconsiderthemassuch.
proportionofdocumentswithl e inthegoldlabel- Incorporationofdocumentlevellabelsandmeta-
ing. Formakingpartialassignmentswefollowthe data has been explored through generation of la-
procedure outlined in §3.3. In computing evalu- bels conditioned on latent topics (Mcauliffe and
ation metrics we exclude input texts which don’t Blei, 2007, STM), conditioning document gener-
receiveaclusterassignmentfromNNand EDTM. ation on observed metadata (Card et al., 2018),
Results. InTable5wenotethatwithbi-encoder or as priors influencing document-topic distribu-
costs, NN slightly outperforms or matches the tions(MimnoandMcCallum,2008,DMR).While
performance of EDTM. However, with crossen- these approaches aim to influence latent topicswith labels/metadata, Ramage et al. (2009, La- orderlogicrules(Pachecoetal.,2023). Incontrast
beledLDA)soughttoensureone-to-onecorrespon- withtheseapproachesEDTMoffersamorenatural
dence between latent topics and document multi- formofinteractionwithtopicmodelsthroughlong
labels, learning a word-label assignment. In en- form natural language interactions by leveraging
suringacorrespondencebetweentopicsandlabels LM/LLMsformodelingtextsimilarities. Workof
LabeledLDAbearsresemblanceto EDTM.How- Meng et al. (2020) learns a discriminative model
ever,incontrastwiththislineofwork,EDTMdoes forretrievingwordsrepresentativeofatopicgiven
notassumeknowledgeofdocumentspairedwith onlyatopicname,bootstrappedfrompre-trained
labelsormetadata. word-embeddings. Whilemirroring EDTM inits
useoftopicnamesitremainslimitedtoexploring
Inassumingcorpusleveltopiclabelsourwork
greedyassignmentssimilartoournearestneighbor
resemblespriorworkthatseekstoincorporateuser
(NN)baseline. Finally,whilealargebodyofwork,
provided seed words for topics. Here, work of
(Jagarlamudi et al., 2012; Churchill et al., 2022)
including EDTM,hasexploreduseofsupervision
toguidelatenttopicstowarduserprovidedtopics,
incorporatestopic-seedwordsintothegeneration
ThompsonandMimno(2018)highlightthevalue
ofdocumentwordswithamixtureofseededand
ofbiasingtopicsawayfromuserlabelsanddiscov-
latent topic-word distributions or through modi-
eringmorenoveltopicalstructure(Ramageetal.,
fiedsamplingschemes. Ontheotherhand,Haran-
2011) – we leave exploration of such considera-
dizadehetal.(2021)incorporateseedinformation
intoanembeddedtopicmodelbyregularizingthe
tionsin EDTM tofuturework.
topic-wordmatrix. Relatedly,Akashetal.(2022)
LLMtopicmodels. Theadventofhighlyperfor-
incorporateweaksupervisionintotopic-wordand
mantLM/LLMshasleadrecentworktoexploreuse
document-topicmatricesfromaLabeledLDAand
ofthesemodelsfortopicmodelingandtextcluster-
zero-shotclassificationmodelrespectively. EDTM
ing. Thedominantlineofworkherehasexplored
extendstopicmodelsbasedonseedwordstomore
unsupervised topic modeling. These approaches
verboseformsoftopicssupervisionsuchasdescrip-
commonlyconsistoftwostages,topicgeneration
tionsanddocumentsthroughitsuseofLM/LLM
withanLLMfollowedbyatopicassignmenttothe
similarityfunctions,whilealsoshowingpartialas-
generatedtopics. Aconsiderabledesignspacehas
signmentstoresultinaccuratetopicassignments
beenexploredforbothstages. Phametal.(2024,
inthepresenceofnoiseinuserprovidedtopics.
TopicGPT) leverages GPT-4 for topic generation
Finally, a third line of work prior work has
and GPT-3.5-Turbo for topic assignment, we in-
explored interaction with topic models through
clude TopicGPT in our experiments. Other two
“must-link”and“cannot-link”constraintsavailable
stageapproacheshavealsoexplorediterativegen-
a-priori(Andrzejewskietal.,2009)orsuppliedin-
eration of topic taxonomies rather than flat topic
teractivelyonceamodeloftopicshasbeenlearned
lists(Lametal.,2024;Wanetal.,2024). Fortopic
Huetal.(2011)withtreestructuredpriorsforword-
assignment,Lametal.(2024)exploremulti-choice
topic distributions. Interaction through such con-
assignment with LLMs while Wan et al. (2024)
straints provides a complementary form of inter-
trainsmallclassifiersonLLMassignmentstospeed
action than ours – we leave exploration of such
up test time topic assignment. Notably, these ap-
constraintsinto EDTM tofuturework.
proaches omit joint assignment of documents to
Whilealargebodyofworkhasexploredincor- topics as in EDTM. In this regard, Wang et al.
porationofvariousformsofinteractionintogener- (2023,GoalEx)bearsresemblancetoEDTM,lever-
ativetopicmodelssuchinteractionshavealsobeen agingintegerlinearprogramsforglobaldocument-
exploredfortopicmodelinginotherframeworks. topicassignmentfollowingLLMbasedtopicgener-
The Anchor algorithm for topic modeling based ationandinitialgreedyassignment. Differingfrom
onnon-negativematrixfactorization(Lund,2019) GoalEx, EDTM’suseofoptimaltransportforas-
wasextendedtoincorporatedocumentmetadataas signmentallowsricherdocument-topiccoststobe
wellasseedwordsupervisionfromusers(Nguyen usedandfractionalassignmentstobemaderather
etal.,2015;Lundetal.,2017). PachecoandGold- thansingletopicassignmentsofGoalEx–however,
wasser (2021) introduce a probabilistic program- both forms of joint assignment may be valuable
mingframeworkforrelationallearninganddemon- in various applications. Also similar in its use of
strateitsuseforinteractivetopicmodelingviafirst jointassignmenttotopicnamesistheworkofFeietal.(2022)–leveraginggreedyclassificationwith 7 Conclusion
pretrained embedding models followed by joint
InthispaperinintroduceEDTM,alabelsupervised
document-topicassignmentwithGaussianMixture
topic model. EDTM leverages optimal transport
Models. However use of GMM’s precludes use
basedassignmentalgorithmstomakegloballyco-
ofblack-boxsimilarityfunctionssuchascrossen-
herenttopicassignmentsfordocumentsbasedon
coderspossibletousein EDTM.
pre-trainedLM/LLMbaseddocument-topicaffini-
Besides leveraging the zero/few shot classifi- ties. Theproposedmethodresultsinhighquality
cation ability of LLMs for topic modeling as in topics compared to a range of baseline methods
EDTM,asmallerbodyofworkhasalsoleveraged basedonpretrainedLM/LLMs,LDAtopicmodels,
LLMs for refining text clustering. Viswanathan and clustering methods. EDTM is also shown to
et al. (2023) explore using LLMs to augment in- incorporatenumerousdifferentformsofinteraction
puttextstoclusteringalgorithms,generateoracle from analysts while remaining robust to noise in
constraintsforconstrainedclusteringmethods,and analysts’input.
iterativelyre-assignlowconfidenceclusterassign-
8 Acknowledgments
ments. Ontheotherhand,Zhangetal.(2023)ex-
ploreactivelytrainingtextsimilaritymodelsbased
This work was supported in part by awards IIS-
onLLMsimilaritiesanddeterminingclustergranu-
2202506andIIS-2312949fromtheNationalSci-
laritieswithLLMs. LeveragingLLMsforactively
ence Foundation (NSF), the Center for Intelli-
refiningclusteringpresentsanunderexploredline
gentInformationRetrieval,Lowe’s,IBMResearch
offuturework.
AI through the AI Horizons Network, and Chan
Zuckerberg Initiative under the project Scientific
Optimaltransportfortopicmodels. Asmall
Knowledge Base Construction. Any opinions,
bodyofworkhasleveragedoptimaltransportfor
findingsandconclusionsorrecommendationsex-
topicmodeling. Zhaoetal.(2020)learnencoders
pressedinthismaterialarethoseoftheauthorsand
fortransformingbagofworddocumentrepresen-
donotnecessarilyreflectthoseofthesponsor.
tations to topic distributions by minimizing OT
distancesbetweenthetwodistributions–building
onembeddedtopicmodels(Diengetal.,2020). On
References
the other hand, Huynh et al. (2020) model topic
modeling as a dictionary learning problem, rep- EScottAdlerandJohnWilkerson.2018. Congres-
resenting documents as mixtures of latent topics sionalbillsproject: 1995-2018.
whicharelearnedbyminimizingtheWasserstain
Pritom Saha Akash, Jie Huang, and Kevin Chen-
distancetobagofworddocumentrepresentations.
Chuan Chang. 2022. Coordinated topic mod-
While these approaches must learn latent topics,
eling. In Proceedings of the 2022 Conference
weexploreoptimaltransportforsupervisedorin-
onEmpiricalMethodsinNaturalLanguagePro-
teractivetopicmodeling. Whilethissimplifiesthe
cessing,pages9831–9843,AbuDhabi,United
problem to an assignment problem, it introduces
ArabEmirates.AssociationforComputational
complexitiessuchasdealingwithnoiseinlabels–
Linguistics.
tothebestofourknowledegenopriorworkshave
exploredoptimaltransportinthissetupandEDTM
EnriqueAmigó,JulioGonzalo,JavierArtiles,and
representsinitialworkinthisspace.
FelisaVerdejo.2009. Acomparisonofextrinsic
clustering evaluation metrics based on formal
Finally,Mysoreetal.(2023)leveragethelabel-
constraints. Informationretrieval,12:461–486.
nameinteractionsof EDTM fordevelopingacon-
trollable recommendation model that represents
David Andrzejewski, Xiaojin Zhu, and Mark
historical user documents using a model similar
Craven.2009. Incorporatingdomainknowledge
to EDTM BE–incontrast,thisworkexploresvari-
into topic modeling via dirichlet forest priors.
ousotherformsofinteraction,partialassignments,
InProceedingsofthe26thannualinternational
andcrossencodersimilaritiesforinteractivetopic
conferenceonmachinelearning,pages25–32.
modeling. However,futureworkmayexploreother
downstreamapplicationswhileleveragingaEDTM DimosthenisAntypas,AsahiUshio,JoseCamacho-
torepresentacollectionoftexts. Collados, Vitor Silva, Leonardo Neves, andFrancesco Barbieri. 2022. Twitter topic clas- withminibatchwasserstein: asymptoticandgra-
sification. In Proceedings of the 29th Interna- dient properties. In Proceedings of the Twenty
tionalConferenceonComputationalLinguistics, ThirdInternationalConferenceonArtificialIn-
pages 3386–3400, Gyeongju, Republic of Ko- telligenceandStatistics,volume108ofProceed-
rea.InternationalCommitteeonComputational ingsofMachineLearningResearch,pages2131–
Linguistics. 2141.PMLR.
Jean-David Benamou, Guillaume Carlier, Marco YuFei,ZhaoMeng,PingNie,RogerWattenhofer,
Cuturi,LucaNenna,andGabrielPeyré.2015. It- andMrinmayaSachan.2022. Beyondprompt-
erativebregmanprojectionsforregularizedtrans- ing: Makingpre-trainedlanguagemodelsbetter
portationproblems. SIAMJournalonScientific zero-shotlearnersbyclusteringrepresentations.
Computing,37(2):A1111–A1138. InProceedingsofthe2022ConferenceonEm-
pirical Methods in Natural Language Process-
DavidMBlei,AndrewYNg,andMichaelIJordan. ing,pages8560–8579,AbuDhabi,UnitedArab
2003. Latent dirichlet allocation. Journal of Emirates. Association for Computational Lin-
machineLearningresearch,3(Jan):993–1022. guistics.
Jordan Boyd-Graber, Yuening Hu, and David ThomasFinleyandThorstenJoachims.2005. Su-
Mimno. 2017. Applications of topic models. pervised clustering with support vector ma-
Foundations and Trends® in Information Re- chines. InProceedingsofthe22ndInternational
trieval,11(2-3):143–296. Conference on Machine Learning, ICML ’05,
page 217–224, New York, NY, USA. Associa-
Dallas Card, Chenhao Tan, and Noah A. Smith.
tionforComputingMachinery.
2018. Neuralmodelsfordocumentswithmeta-
data. InProceedingsofthe56thAnnualMeeting Maarten Grootendorst. 2022. Bertopic: Neural
of the Association for Computational Linguis- topic modeling with a class-based tf-idf proce-
tics(Volume1: LongPapers),pages2031–2040, dure. arXivpreprintarXiv:2203.05794.
Melbourne,Australia.AssociationforComputa-
BaharehHarandizadeh,J.HunterPriniski,andFred
tionalLinguistics.
Morstatter.2021. Keywordassistedembedded
Hjalmar Bang Carlsen and Snorre Ralund. 2022. topicmodel. CoRR,abs/2112.03101.
Computationalgroundedtheoryrevisited: From
LiangjieHongandBrianD.Davison.2010. Empir-
computer-ledtocomputer-assistedtextanalysis.
icalstudyof topicmodelingintwitter. InPro-
BigData&Society,9(1):20539517221080146.
ceedingsoftheFirstWorkshoponSocialMedia
RobertChurchill,LisaSingh,RebeccaRyan,and Analytics, SOMA ’10, page 80–88, New York,
PamelaDavis-Kean.2022. Aguidedtopic-noise NY,USA.AssociationforComputingMachin-
model for short texts. In Proceedings of the ery.
ACM Web Conference 2022, WWW ’22, page
Matt-Heun Hong, Lauren A. Marsh, Jessica L.
2870–2878,NewYork,NY,USA.Association
Feuston, Janet Ruppert, Jed R. Brubaker, and
forComputingMachinery.
DanielleAlbersSzafir.2022. Scholastic: Graph-
Marco Cuturi. 2013. Sinkhorn distances: Light- ical human-ai collaboration for inductive and
speed computation of optimal transport. Ad- interpretivetextanalysis. InProceedingsofthe
vancesinneuralinformationprocessingsystems, 35thAnnualACMSymposiumonUserInterface
26:2292–2300. SoftwareandTechnology,UIST’22,NewYork,
NY,USA.AssociationforComputingMachin-
AdjiB.Dieng,FranciscoJ.R.Ruiz,andDavidM. ery.
Blei. 2020. Topic modeling in embedding
spaces. TransactionsoftheAssociationforCom- Alexander Miserlis Hoyle, Pranav Goel, Rupak
putationalLinguistics,8:439–453. Sarkar, and Philip Resnik. 2022. Are neural
topicmodelsbroken? InFindingsoftheAssoci-
KilianFatras,YounesZine,RémiFlamary,Remi ation for Computational Linguistics: EMNLP
Gribonval,andNicolasCourty.2020. Learning 2022, pages 5321–5344, Abu Dhabi, UnitedArabEmirates.AssociationforComputational high-levelconceptsusinglloom. arXivpreprint
Linguistics. arXiv:2404.12259.
AlexanderMiserlisHoyle,LawrenceWolf-Sonkin, JimmyLin,RodrigoNogueira,andAndrewYates.
HannaWallach,IsabelleAugenstein,andRyan 2020. Pretrainedtransformersfortextranking:
Cotterell.2019. Unsuperviseddiscoveryofgen- Bert and beyond. corr abs/2010.06467 (2020).
deredlanguagethroughlatent-variablemodeling. arXivpreprintarXiv:2010.06467.
In Proceedings of the 57th Annual Meeting of
theAssociationforComputationalLinguistics, StuartLloyd.1982. Leastsquaresquantizationin
pages 1706–1716, Florence, Italy. Association pcm. IEEEtransactionsoninformationtheory,
forComputationalLinguistics. 28(2):129–137.
Yuening Hu, Jordan Boyd-Graber, and Brianna Jeffrey Lund. 2019. Fine-grained Topic Models
Satinoff. 2011. Interactive topic modeling. In Using Anchor Words. Brigham Young Univer-
Proceedingsofthe49thAnnualMeetingofthe sity.
AssociationforComputationalLinguistics: Hu-
JeffreyLund,ConnorCook,KevinSeppi,andJor-
man Language Technologies, pages 248–257,
danBoyd-Graber.2017. Tandemanchoring: a
Portland,Oregon,USA.AssociationforCompu-
multiwordanchorapproachforinteractivetopic
tationalLinguistics.
modeling. In Proceedings of the 55th Annual
Viet Huynh, He Zhao, and Dinh Phung. 2020. Meeting of the Association for Computational
Otlda: Ageometry-awareoptimaltransportap- Linguistics(Volume1: LongPapers),pages896–
proachfortopicmodeling. InAdvancesinNeu- 905,Vancouver,Canada.AssociationforCom-
ralInformationProcessingSystems,volume33, putationalLinguistics.
pages18573–18582.CurranAssociates,Inc.
JamesMacQueenetal.1967. Somemethodsfor
Jagadeesh Jagarlamudi, Hal Daumé III, and classificationandanalysisofmultivariateobser-
RaghavendraUdupa.2012. Incorporatinglexi- vations. In Proceedings of the fifth Berkeley
calpriorsintotopicmodels. InProceedingsof symposiumonmathematicalstatisticsandprob-
the13thConferenceoftheEuropeanChapterof ability,volume1,pages281–297.Oakland,CA,
theAssociationforComputationalLinguistics, USA.
pages 204–213, Avignon, France. Association
forComputationalLinguistics. Jon Mcauliffe and David Blei. 2007. Supervised
topic models. Advances in neural information
Mahmood Jasim, Enamul Hoque, Ali Sarvghad,
processingsystems,20.
and Narges Mahyar. 2021. Communitypulse:
Facilitatingcommunityinputanalysisbysurfac- AndrewKachitesMcCallum.2002. Mallet: Ama-
inghiddeninsights,reflections,andpriorities. In chinelearningforlanguagetoolkit. http://mallet.
Proceedingsofthe2021ACMDesigningInterac- cs.umass.edu.
tiveSystemsConference,DIS’21,page846–863,
NewYork,NY,USA.AssociationforComput- MarinaMeila˘.2007. Comparingclusterings—an
ingMachinery. information based distance. Journal of Multi-
variateAnalysis,98(5):873–895.
Denis Kotkov, Alan Medlar, Alexandr Maslov,
Umesh Raj Satyal, Mats Neovius, and Dorota YuMeng,JiaxinHuang,GuangyuanWang,Zihan
Glowacka. 2022. The tag genome dataset for Wang,ChaoZhang,YuZhang,andJiaweiHan.
books. InProceedingsofthe2022Conference 2020. Discriminativetopicminingviacategory-
onHumanInformationInteractionandRetrieval, nameguidedtextembedding. InProceedingsof
CHIIR’22,page353–357,NewYork,NY,USA. TheWebConference2020,pages2121–2132.
AssociationforComputingMachinery.
StephenMerity,NitishShirishKeskar,andRichard
MichelleSLam,JaniceTeoh,JamesLanday,Jef- Socher. 2018. Regularizing and optimizing
freyHeer,andMichaelSBernstein.2024. Con- LSTMlanguagemodels. InInternationalCon-
ceptinduction: Analyzingunstructuredtextwith ferenceonLearningRepresentations.David M Mimno and Andrew McCallum. 2008. ofthe2009ConferenceonEmpiricalMethodsin
Topicmodelsconditionedonarbitraryfeatures NaturalLanguageProcessing,pages248–256,
with dirichlet-multinomial regression. In UAI, Singapore.AssociationforComputationalLin-
volume24,pages411–418.Citeseer. guistics.
Sheshera Mysore, Mahmood Jasim, Andrew Mc- DanielRamage,ChristopherD.Manning,andSu-
callum,andHamedZamani.2023. Editableuser sanDumais.2011. Partiallylabeledtopicmod-
profilesforcontrollabletextrecommendations. elsforinterpretabletextmining. InProceedings
InProceedingsofthe46thInternationalACMSI- ofthe17thACMSIGKDDInternationalConfer-
GIRConferenceonResearchandDevelopment enceonKnowledgeDiscoveryandDataMining,
inInformationRetrieval. KDD’11,page457–465,NewYork,NY,USA.
AssociationforComputingMachinery.
ThangNguyen,JordanBoyd-Graber,JeffreyLund,
Kevin Seppi, and Eric Ringger. 2015. Is your Margaret E Roberts, Brandon M Stewart, Dustin
anchorgoingupordown? fastandaccuratesu- Tingley, Edoardo M Airoldi, et al. 2013. The
pervised topic models. In Proceedings of the structuraltopicmodelandappliedsocialscience.
2015ConferenceoftheNorthAmericanChapter In Advances in neural information processing
of the Association for Computational Linguis- systems workshop on topic models: computa-
tics: HumanLanguageTechnologies,pages746– tion, application, and evaluation. Harrahs and
755,Denver,Colorado.AssociationforCompu- Harveys,LakeTahoe.
tationalLinguistics.
LaureThompsonandDavidMimno.2018. Author-
RodrigoNogueira,ZhiyingJiang,RonakPradeep, less topic models: Biasing models away from
andJimmyLin.2020. Documentrankingwith known structure. In Proceedings of the 27th
a pretrained sequence-to-sequence model. In InternationalConferenceonComputationalLin-
Findingsof the Associationfor Computational guistics,pages3903–3914,SantaFe,NewMex-
Linguistics: EMNLP2020,pages708–718,On- ico, USA. Association for Computational Lin-
line.AssociationforComputationalLinguistics. guistics.
MariaLeonorPachecoandDanGoldwasser.2021. LaureThompsonandDavidMimno.2020. Topic
ModelingContentandContextwithDeepRela- modelingwithcontextualizedwordrepresenta-
tionalLearning. TransactionsoftheAssociation tionclusters. arXivpreprintarXiv:2010.12626.
forComputationalLinguistics,9:100–119.
Vijay Viswanathan, Kiril Gashteovski, Carolin
MariaLeonorPacheco,TunazzinaIslam,LyleUn- Lawrence,TongshuangWu,andGrahamNeubig.
gar, MingYin, andDanGoldwasser.2023. In- 2023. Largelanguagemodelsenablefew-shot
teractiveconceptlearningforuncoveringlatent clustering. arXivpreprintarXiv:2307.00524.
themesinlargetextcollections. InFindingsof
MengtingWan,TaraSafavi,SujayKumarJauhar,
theAssociationforComputationalLinguistics:
YujinKim,ScottCounts,JenniferNeville,Sid-
ACL2023,pages5059–5080,Toronto,Canada.
dharth Suri, Chirag Shah, Ryen W White,
AssociationforComputationalLinguistics.
Longqi Yang, et al. 2024. Tnt-llm: Text min-
Gabriel Peyré, Marco Cuturi, et al. 2019. Com- ingatscalewithlargelanguagemodels. arXiv
putationaloptimaltransport: Withapplications preprintarXiv:2403.12173.
to data science. Foundations and Trends® in
Yixue Wang and Nicholas Diakopoulos. 2021.
MachineLearning,11(5-6):355–607.
Journalistic source discovery: Supporting the
ChauMinhPham,AlexanderHoyle,SimengSun, identificationofnewssourcesinusergenerated
PhilipResnik,andMohitIyyer.2024. Topicgpt: content. InProceedingsofthe2021CHIConfer-
Aprompt-basedtopicmodelingframework. enceonHumanFactorsinComputingSystems,
CHI’21,NewYork,NY,USA.Associationfor
Daniel Ramage, David Hall, Ramesh Nallapati,
ComputingMachinery.
and Christopher D. Manning. 2009. Labeled
LDA:Asupervisedtopicmodelforcreditattri- ZihanWang,JingboShang,andRuiqiZhong.2023.
butioninmulti-labeledcorpora. InProceedings Goal-drivenexplainableclusteringvialanguagedescriptions. InProceedingsofthe2023Confer-
enceonEmpiricalMethodsinNaturalLanguage
Processing,pages10626–10649,Singapore.As-
sociationforComputationalLinguistics.
Nishant Yadav, Nicholas Monath, Rico Angell,
Manzil Zaheer, and Andrew McCallum. 2022.
Efficient nearest neighbor search for cross-
encoder models using matrix factorization. In
Proceedingsofthe2022ConferenceonEmpir-
icalMethodsinNaturalLanguageProcessing,
pages2171–2194,AbuDhabi,UnitedArabEmi-
rates. Association for Computational Linguis-
tics.
Yuwei Zhang, Zihan Wang, and Jingbo Shang.
2023. Clusterllm: Large language models as
a guide for text clustering. arXiv preprint
arXiv:2305.14871.
HeZhao,DinhPhung,VietHuynh,TrungLe,and
Wray Buntine. 2020. Neural topic model via
optimaltransport. InInternationalConference
onLearningRepresentations.
Ying Zhao. 2005. Criterion Functions for Doc-
ument Clustering. Ph.D. thesis, University of
Minnesota,USA. AAI3180039.