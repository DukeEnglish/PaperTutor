GIST: Gibbs self-tuning for locally
adaptive Hamiltonian Monte Carlo
NawafBou-Rabee* BobCarpenter† MiloMarsden‡
April24,2024
Abstract
WepresentanovelandflexibleframeworkforlocalizedtuningofHamiltonianMonteCarlo
samplersbysamplingthealgorithm’stuningparametersconditionallybasedontheposition
andmomentumateachstep. Foradaptivelysamplingpathlengths,weshowthatrandomized
HamiltonianMonteCarlo,theNo-U-TurnSampler,andtheApogee-to-ApogeePathSamplerall
fitwithinthisunifiedframeworkasspecialcases. Theframeworkisillustratedwithasimple
alternativetotheNo-U-TurnSamplerforlocallyadaptingpathlengths.
1 Introduction
TuningtheparametersofMarkovchainMonteCarlo(MCMC)algorithmsiscriticaltotheirperfor-
mance,butnotoriouslydifficultinpractice. ThischallengeisparticularlyacuteforHamiltonian
MonteCarlo(HMC),wherethetuningofpathlength(integrationtime)[31,15,4,34],steplength
(timediscretizationparameter)[2,5,6],andpre-conditioner(massmatrix)[25,33,52]frequently
presentsacomplextrade-offbetweencomputationalcostandmixing. Thesuccessfulself-tuningof
pathlengthprovidedbytheNo-U-TurnSampler(NUTS)hasledtoitswidespreadadoptionasthe
defaultsamplerinprobabilisticprogramminglanguages[18,48,22,24,45].
InNUTS,thepathlengthisadaptivelysampledaccordingtoU-turnavoidingconditions,which
roughlyspeakingstoptheunderlyingHamiltoniantrajectorywheneveritdoublesback[31,4]. The
mainideaistonumericallyintegrateHamilton’sequationsforwardandbackwardintimeuntil
a U-turn occurs.1 A point along this path is then randomly selected such that: (i) the resulting
samplerhasthecorrectinvariantdistribution;and(ii)pointsfarfromthestartingpointaremore
likelytobeselected.
Canthealgorithm’sothertuningparametersbesimilarlyself-tuned? Motivatedbythebenefitsof
NUTS,thispaperaddressesthisquestionbypresentinganewframeworkforadaptivelysampling
*DepartmentofMathematicalSciences,RutgersUniversity,nawaf.bourabee@rutgers.edu
†CenterforComputationalMathematics,FlatironInstitute,bcarpenter@flatironinstitute
.org
‡DepartmentofMathematics,StanfordUniversity,mmarsden@stanford.edu
1AU-turnmustoccurinaproperdensity,butinpractice,acapsuchas1024iterationsisimposed.
1
4202
rpA
32
]OC.tats[
1v35251.4042:viXraHMCtuningparameterssuchaspathlength,steplength,pre-conditioner,etc.2 Thisframework
istermedGibbsself-tuningHMCorGISTforshort. Inthecontextofadaptivelysamplingpath
lengths, we demonstrate that GIST includes as special cases almost all locally adaptive HMC
samplers in current use including randomized HMC [15, 9], NUTS [31, 4], and the Apogee-to-
ApogeePathSampler[20]. Owingtoitssimplicityandgenerality,GISTcanbenaturallyextended
toadaptingthealgorithm’sremainingtuningparametersincludingstepsizeandmassmatrix.
Tobesure,theprimarycontributionofthispaperistheGISTsampler,whichadmitsarelatively
simpleproofofcorrectness. Moreover,wedemonstratetheutilityoftheGISTsamplingframework
asatheoreticaltoolbyunifyingtheproofsofcorrectnessforexistinglocallyadaptiveHMCsamplers
andasapracticaltoolbydevelopinganovelalternativeforlocallyadaptingpathlengths.
Theremainderofthepaperisorganizedasfollows. Section2presentstheGISTsamplingframe-
workforself-tuningHMCsamplers. Inthecontextofadaptivelysamplingpathlengths,Section3
considers some fundamental special cases of GIST samplers including randomized HMC (Sec-
tion 3.1), NUTS (Section 3.4), and the Apogee-to-Apogee Path Sampler (Section 3.5). Section 4
comparesGISTsamplersforpath-lengthadaptationinthecontinuous-timecontextappliedtoa
truncation of an infinite-dimensional Gaussian target measure. Section 5 considers path-length
adaptationinthediscrete-timecontext,andpresentsseveralconcreteproposalsforsamplingthe
numberofleapfrogstepsbasedonthecurrentpositionandmomentumgivenafixedstepsizeand
massmatrix. Section6describesanovelsamplerforadaptingpathlengthsalongwithexperimental
evaluationandanempiricalcomparisontoNUTS.
2 A framework for self-tuning Hamiltonian Monte Carlo
HamiltonianMonteCarlo(HMC)isaclassofMCMCmethodsforsamplingabsolutelycontinuous
probabilitytargetdistributionsoftheform
µ(dθ,dρ) ∝ e−H(θ,ρ) m(dθdρ) , (1)
whichhaveadensityrelativetoLebesguemeasure m onphasespaceR2d,withposition θ ∈ Rd
andmomentumρ ∈ Rd.Forsimplicity,weassumethecaseofaseparableHamiltonianfunction
withunitmassmatrix,i.e.,
1
H(θ,ρ) = U(θ)+ ρ⊤ ρ ,
2
foracontinuouslydifferentiablepotentialenergyfunctionU : Rd → R suchthat(cid:82) e−U(θ)dθ < ∞ .
Thenon-normalizedtargetdensityofinterestise−U(θ) formodelparametersθ.
AdefiningingredientofanyHMCalgorithmisareversible,volume-preservingmap F(α) : R2d →
R2d
whereαencapsulatesthealgorithmtuningparameters. Thismapistypicallyobtainedfrom
discretizingHamilton’sequationsfortheHamiltonianfunction H(θ,ρ),i.e.,
d d
θ = ρ , ρ = −∇U(θ ) .
t t t t
dt dt
2TheapproachpresentedherecouldalsobeappliedtoMetropolissamplersotherthanHMC.
2GIST(θ)
θ ∈ Rd position
ρ ∈ Rd momentum
α ∈ A algorithmtuningparameter
(INITIALIZE)
θ = θ
0
(GIBBS)
ρ ∼ normal(0,I ) (completemomentumrefreshment)
0 d×d
α ∼ p(· | θ ,ρ ) (sampletuningparameter)
0 0
(METROPOLIS-WITHIN-GIBBS)
(θ∗,ρ∗) = F(α)(θ ,ρ ) (computeproposal)
0 0
u ∼ uniform([0,1])
p(π(θ ,ρ )(α)|S ◦F(α)(θ ,ρ ))
ifu < e−∆H(θ0,ρ0) 0 0 0 0
p(α|θ ,ρ )
0 0
returnθ∗ (accept)
else
returnθ (reject)
0
Figure1: GISTSampling. TheGISTsamplerdiffersfromstandardHMCinthesamplingofthetuning
parameterandsubsequentadjustmentoftheacceptanceprobability. Hereweusetheshorthand∆H(θ,ρ) :=
H◦F(α)(θ,ρ)−H(θ,ρ)andI denotesthed×didentitymatrix.
d×d
Reversibilityandvolume-preservationensurethatthemapF(α)isMetropolisadjustable[10]. Note
thatS ◦F(α)isavolume-preservinginvolutionwhereS : R2d → R2d isthemomentumflipmap
definedbyS(θ,ρ) = (θ,−ρ)forall(θ,ρ) ∈ R2d [29]. Wesupposethatthetuningparameterαtakes
valuesinasetAwhere(A,B,η)isameasurespacewithσ-algebraB andbackgroundmeasureη.
To“selftune”theparameterα,thestatespaceR2d isenlargedtoaproductspaceR2d×A.
Onthis
enlargedspace,anenlargedtargetmeasureisdefinedbyspecifyingaconditionaldistributionof
thetuningparameterαgiventhepositionandmomentum(θ,ρ),i.e.,
µ (dθ,dρ,dα) ∝ e−H(θ,ρ) p(α|θ,ρ)(m⊗η)(dθdρdα) . (2)
e
Notethatthe(θ,ρ)-marginalofµ isthedesiredtargetmeasureµ. Explicitformsoftheconditional
e
distribution p(α|θ,ρ)arespecifiedinsubsequentsectionsforthecaseofpathlengthadaptivity.
Foranypositionandmomentum(θ,ρ) ∈ R2d,letπ(θ,ρ) : A → Abeameasurablemapactingon
thetuningparameterα ∈ Asuchthat
(cid:16) (cid:17)
G : (θ,ρ,α) (cid:55)→ S◦F(α)(θ,ρ), π(θ,ρ)(α) (3)
3isa(m⊗η)-preservinginvolutiononR2d×A. Thisclearlyholdswhenthemapπ(θ,ρ)istakento
betheidentitymaponA.
Intermsofthisnotation,Figure1describesatransitionstepofthe“GIST”(Gibbsself-tuningHMC)
sampler. ThefollowingtheoremindicatesthattheGISTsampleriscorrect.
Theorem1. TheGISTsamplerinFigure1generatesaMarkovChainθ ,θ ,... onRd thatisreversible
0 1
withrespecttoe−U(θ)m(dθ).
Adetailed,self-containedproofofTheorem1isprovidedinAppendixA.ByinterpretingtheGIST
sampler as a Gibbs sampler, the proof shows that each update in Figure 1 leaves invariant the
enlargedtarget(2). Inparticular,theaccept/rejectstepisinterpretedasaMetropolis-within-Gibbs
stepontheenlargedspaceR2d×Awithtargetmeasuredefinedin(2),proposalgivenbyGdefined
in(3),andcorrespondingacceptanceprobability
(cid:18) (cid:19)
a (θ ,ρ ,α) = 1∧
e−∆H(θ 0, ρ 0) p(π(θ 0,ρ 0)(α)|S ◦F(α)(θ 0,ρ 0))
, (4)
e 0 0 p(α|θ ,ρ )
0 0
whereweusedtheshorthand ∆H(θ,ρ) := H◦F(α)(θ,ρ)−H(θ,ρ) for (θ,ρ,α) ∈ R2d×A. Note,
thisMetropolis-within-Gibbsstepiscorrectbecauseweassumethatthemap G onR2d×A isa
(m⊗η)-preservinginvolution;foradetailedjustificationseeLemma2inAppendixA.
Remark1. JustastheMarkovchainforstandardHMCcanbeformulatedoverphasespacevariables
(θ,ρ) ∈ R2d, the Markov chain for GIST can also be formulated over triples (θ,ρ,α) ∈ R2d ×A.
However, in the case treated here of full momentum and tuning parameter refreshment, the
sequenceofθvaluesbyitselfformsaMarkovchainandincludingtheothervariablesissuperfluous.
Remark2. InadditiontoadaptivelysamplingHMCtuningparameters,theGISTsamplercanalso
beappliedtorandomizethetimeintegratorfortheHamiltonianflow,asin[13,12]. Inthiscase,
the tuning parameter would specify a particular time integrator within a parametric family of
timeintegratorsthatareeachreversibleandvolume-preserving. Incertainrepresentativemodels,
randomizedtimeintegratorshaveprovablybettercomplexityforHamiltonianMCMCthanthe
frequentlyusedleapfrogintegrator[49,30,17,13,16,14].
3 Adaptively sampling path lengths
TomaketheGISTsamplingframeworkmoreconcreteanddemonstrateitsbreadth,thissection
shows that several fundamental HMC samplers that locally adapt path lengths fit within this
unifiedframework.
3.1 SamplingpathlengthsinrandomizedHMC
InrandomizedformsofHMC,thepathlengthorthesteplengthisgeneratedrandomlyateach
iterationindependentlyofthecurrentpositionandmomentum[37]; using, e.g., anexponential
distributionoverpathlength[15,9],anempiricallylearneddistributionoverpathlength[53],ora
uniformdistributionoverpathandsteplengths[37,43]. Thesecanallbeanalyzedasinstancesof
theGISTsamplergiveninFigure1.
4Asanexample,consider F(α)(θ,ρ) = φ (θ,ρ)wherewehaveintroducedtheexactHamiltonian
α
flow φ : R2d → R2d attimeα ≥ 0. TakeA = [0,∞)andηtobetheLebesguemeasureonR . Define
α
p(α | θ,ρ) = λe−λα whereλ > 0;thatis,αisanexponentialrandomvariablewithparameterλ. In
thiscase,themapGin(3)isgivenexplicitlybyG : (θ,ρ,α) (cid:55)→ (S ◦F(α)(θ,ρ), α),forwhich a ≡ 1
e
(theproposalisalwaysaccepted),andFigure1reducestoadrawfromrandomizedHMCatthe
firstjumptime[15,9].
TheotherrandomizedformsofHMCthatgeneraterandomlythetuningparameterindependently
ofthecurrentpositionand/ormomentumcanbeanalyzedinexactlythesameway. InSection4,
in the setting of a truncation of an infinite-dimensional Gaussian measure, the performance of
randomizedHMCiscomparedwithaGISTsamplerthatwedescribenext.
3.2 AdaptingpathlengthsinexactHMC
Consider again F(α) = φ where φ : R2d → R2d is the exact Hamiltonian flow map at time
α α
α ≥ 0. TakeA = [0,∞)andη tobetheLebesguemeasureonR . Letτ(θ,ρ) : R2d → (0,∞)beany
measurablefunction. Define
1
p(α | θ,ρ) = 1 (α) .
τ(θ,ρ) [0,τ(θ,ρ)]
Thatis,conditionedonthepositionandmomentum(θ,ρ),αisauniformrandomvariableoverthe
interval[0,τ(θ,ρ)]. Here1 denotesthestandardindicatorfunctionofaset A. Asashorthand,let
A
τ = τ(θ ,ρ ) and τ = τ(S ◦ φ (θ ,ρ )), i.e., thefunction τ evaluatedatthecurrentstateofthe
1 0 0 2 α 0 0
chain(θ ,ρ )andtheproposedstatebutwithmomentumreversedS ◦φ (θ ,ρ ). Inthiscase,the
0 0 α 0 0
mapGin(3)isgivenexplicitlyby
G : (θ,ρ,α) (cid:55)→ (S ◦F(α)(θ,ρ), α) ,
andthecorrespondingacceptanceprobabilityin(4)reducesto
(cid:18) (cid:19)
τ
a (θ ,ρ ,α) = 1∧ 11 ,
e 0 0
τ
{τ2≥α}
2
because ∆H(θ ,ρ ) = 0 for the exact Hamiltonian flow. The indicator in this Metropolis ratio
0 0
indicatesthat p(α | S ◦φ (θ ,ρ )) ̸= 0.
α 0 0
Conditions avoiding U-turns in the exact Hamiltonian flow can be used to specify the function
τ(θ,ρ). There are several ways to characterize such U-turn conditions. For example, here is a
conditionbasedonwhentheanglebetweentheinitialvelocityρ ∈ Rd andthevelocityρ ∈ Rd at
0 t
timet ≥ 0firstexceedsπ/2
τ(θ ,ρ ) := inf{t > 0 : ρ ·ρ ≤ 0} , (5)
0 0 0 t
wherewehaveintroduced(θ ,ρ ) := φ (θ ,ρ )fort ≥ 0. Anotherconditionisbasedonwhenthe
t t t 0 0
squareddistanceΓ(t) := |θ −θ |2 betweentheinitialconfigurationθ ∈ Rd andtheconfiguration
0 t 0
θ ∈ Rd attimet ≥ 0firstdecreases
t
τ(θ ,ρ ) := inf{t > 0 : Γ′(t) < 0} . (6)
0 0
Thesecontinuous-timeU-turnconditionshavediscrete-timeanalogs,whicharediscussedinthe
nextexampleandSection5.
53.3 AdaptingpathlengthsinnumericalHMC
Fix h > 0. Let Φ : R2d → R2d denote one step of the leapfrog integrator with step length h.
h
ConsiderF(α) = Φα whereαisthenumberofleapfrogsteps. TakeA = N andletηbethecounting
h
measure. Similarlytotheabove,letτ : R2d → N beameasurablefunctionanddefine
1
p(α | θ,ρ) = 1 (α) .
τ(θ,ρ) {0,...,τ(θ,ρ)−1}
Similarlytotheabove,themapGin(3)isgivenexplicitlyby
G : (θ,ρ,α) (cid:55)→ (S ◦F(α)(θ,ρ), α) ,
andthecorrespondingacceptanceprobabilityin(4)simplifiesto
(cid:18) (cid:19)
a (θ ,ρ ,α) = 1∧
e−∆H(θ0,ρ0)τ
11 ,
e 0 0
τ
{τ2≥α}
2
whereτ = τ(θ ,ρ )andτ = τ(S ◦Φα(θ ,ρ )).
1 0 0 2 h 0 0
3.4 AdaptingpathlengthsinNUTS
HereweshowthatNUTSisaspecialcaseoftheGISTsamplerpresentedinFigure1.
In order to write NUTS as a GIST sampler, we first explain NUTS more precisely following the
terminology and framework introduced in [4, 23]. Given the current position and momentum
(θ ,ρ ) ∈ R2d,theideabehindNUTSistofirstrandomlygenerateafinitesubsetofintegers J ⊂ Z ,
0 0
and in turn, sample the next state of the chain from among the leapfrog iterates {Φi(θ ,ρ )} .
h 0 0 i∈J
Followingthenotationof[23],letP (J | θ ,ρ )betheprobabilityofselectingJandletQ (i | J,θ ,ρ )
h 0 0 h 0 0
betheprobabilityofselectingthei-thleapfrogiterateΦi(θ ,ρ ). In[23],thesearereferredtoasthe
h 0 0
“orbit”and“index”selectionkernels,respectively.
Moreprecisely,therandomproceduretogeneratetheset J canbedescribedbyinduction. First,
take J = {0}. Then,given J ,wefirstcheckwhethertheleapfrogiterates{Φi(θ ,ρ )} satisfy
0 k h 0 0 i∈J k
theU-turncondition:
ρ+·(θ+−θ−) < 0 and ρ−·(θ−−θ+) < 0
where(θ+,ρ+) = Φm haxJ k(θ 0,ρ 0)and(θ−,ρ−) = Φm hinJ k(θ 0,ρ 0). Ifthisconditionismet,weoutput
J = J . IfinsteadnoU-turnisencountered,thensample I uniformlyfrom{J +|J |,J −|J |}.
k k+1 k k k k
Thissetisthenextproposedextensiontotheset J .
k
Thisproposedextensionwilleitherbeaccepted,yielding J = J ∪ I ,orrejected,returning
k+1 k k+1
J = J . Toensurethecorrectnessandefficiencyofthealgorithmthefollowingsymmetrycondition
k
is imposed: P (J | θ ,ρ ) = P (J −i | Φi(θ ,ρ )) for all i ∈ J. This condition ensures that the
h 0 0 h h 0 0
probabilityofobtainingaparticularsetofleapfrogiterates{Φi(θ ,ρ )} isuniformwithrespect
h 0 0 i∈J
tothestartingpointΦi(θ ,ρ )withinthissetforalli ∈ J.
h 0 0
Imposingthisconstraintwillrequirethatsomeproposedextensions I arerejected. Forevery
k+1
i ∈ J , corresponds an alternate sequence of extensions I′,...,I′ which would have produced
k 1 k
the set J −i had the orbit generating procedure instead been started at Φi(θ ,ρ ). To enforce
k h 0 0
6P (J | θ ,ρ ) = P (J−i | Φi(θ ,ρ )),itissufficienttorequirethat J satisfiestheNo-Intermediate-
h 0 0 h h 0 0 k
U-Turn property. This property states that at no stage in this alternate sequence of extensions
I′,...,I′ wouldtheU-turnconditionhavebeentriggered.
1 k
Toensurethatevery J satisfiestheNo-Intermediate-U-Turnproperty,wesimplyneedtorejectany
k
proposal I where J = J ∪I doesnotsatisfytheNo-Intermediate-U-Turnproperty. While
k+1 k+1 k k+1
naivelythisappearstoinvolvecheckingoverexponentiallymanysequencesofextensions,this
stepcanbeimplementedefficiently. Indeed,overdifferentchoicesofi ∈ J manyofthealternate
k
sequenceswillrefertothesamesetofleapfrogiterates. SincetheU-turnconditiondependsonly
ontheleapfrogiterates,wethereforeneedonlycheckthatallpossibleintermediatesetsofleapfrog
iterateshavenoU-turns. Asthenumberofpossibleintermediatesetsofleapfrogiteratesisrelatively
small,thisobservationgreatlyreducesthecomplexityofcheckingtheNo-Intermediate-U-Turn
property. SeeRemark3formoredetailsaboutthecomputationalcomplexityoftheorbitgeneration
procedure.
Havinggenerated J usingthisrandomizedorbitselectionprocedure,NUTSthengeneratesi ∈ J
accordingtotheindexselectionkernelQ (i | θ ,ρ ,J)andreturnsΦi(θ ,ρ )asthenextstate. There
h 0 0 h 0 0
aretwoindexselectionkernelscommonlyusedinpractice. ThefirstisgivenbytheBoltzmann
weight of the corresponding leapfrog iterate, i.e., Q h(i | θ 0,ρ 0,J) ∝ e−H(Φi h(θ0,ρ0)). The second is
biasedprogressivesampling,asdetailedin[4].
WearenowinpositiontodescribeNUTSasaGISTsampler. LetA = P ×Z whereP = {J ⊂ Z |
F F
|J| < ∞},letη bethecountingmeasureonP ×Z ,andwritethetuningparameterincomponents
F
asα = (J,i) ∈ A. Forall(θ,ρ,α) ∈ R2d×A,define
p(J,i | θ,ρ) = P (J | θ,ρ)Q (i | θ,ρ,J) .
h h
AsinSection3.3,let F(α)(θ,ρ) = Φi(θ,ρ). ThemapGgivenin(3)canbewrittenexplicitlyas
h
G(θ,ρ,J,i) = (S ◦Φi(θ,ρ),−(J−i),i) .
h
The resulting GIST sampler in Figure 1 corresponds to NUTS. Notably, the Metropolis-within-
GibbsstepforNUTSisalwaysaccepted. Indeed,since {Φi(S(θ,ρ))} = {Φ−i(S(θ,ρ))} =
h i∈−J h i∈J
{S(Φi(θ,ρ))} ashortcouplingconstructionshows P (−J | S(θ,ρ)) = P (J | θ,ρ). Combining
h i∈J h h
this with P (J−i | Φi(θ,ρ)) = P (J | θ,ρ) gives P (−(J−i) | S ◦Φi(θ,ρ)) = P (J | θ,ρ). Since
h h h h h h
theindexselectionkernelsdescribedabovesatisfythefollowingdetailedbalancecondition
e−H(Φi h(θ0,ρ0)) Q h(i | S ◦Φi h(θ 0,ρ 0),−(J−i)) = e−H(θ0,ρ0) Q h(i | θ 0,ρ 0,J) ,
itfollowsthattheacceptanceprobabilityinFigure 1isalwaysequalto1. Fortheindexselection
kernel based on the Boltzmann weights this detailed balance condition can be seen by a direct
computation,andforbiasedprogressivesamplingthispropertyfollowsfromProposition6in[23].
Remark 3 (Computational Cost of Orbit Selection). As noted above, one can efficiently check
whether J satisfies the No-Intermediate-U-Turn condition by checking whether all possible
k+1
intermediatesetsofleapfrogiteratesarefreeofU-turns. Ifwedefinerandomvariables β ,...,β
1 k+1
as β = 1 and B = ∑k+12l−1β thenonecanprovebyinductionthat J = {−B,...,2k−
l I l=J l−|J l| l=1 l k+1
1− B} = [2k]−(B+1). The possible sets of intermediate leapfrog iterates corresponding to
7J thencorrespondexactlytothesets{Φi(θ,ρ)} ,forl = 1,...,kandm =
k+1 h i∈[m·2l:(m+1)2l−1]−(B+1)
0,1,...,2k+1−l −1,where[m : n] = {m,m+1,...,n−1,n}.
SincetheU-turnconditioninvolvesonlytheendpointsofthesetofleapfrogiterates,thecostof
checkingwhetherornotagivensetofleapfrogiteratessatisfiestheU-turnconditionisconstant
in the size of the set. Therefore, when generating an orbit starting from (θ ,ρ ) the total cost of
0 0
checkingwhether J = I ∪J satisfiestheNo-Intermediate-U-Turnconditionisupperbounded
k+1 k+1 k
bythenumberofsetsweneedtocheck.
Sinceeachchoiceofl abovecorrespondsto2k−l setsthetotalcostforcheckingthat J satisfiesthe
k+1
No-Intermediate-U-Turnpropertyisupperboundedby∑k 2k−l ≤ 2k+1. Consequently,returning
l=1
aset J with |J| = M = 2k hascomputationalcostO(M). Inpractice,byinductionweneedonly
checkthoseintermediatesetsofleapfrogiterateswhichcorrespondtosubsetsof I whichgives
k+1
furtherspeedup.
Comparethistothesimplerstrategyofextending J onepointatatimebysampling I uniformly
k k+1
from{min(J )−1,max(J )+1}. Forsuchastrategy,thecostofappending I isagainoforder
k k k+1
|J |andthereforetoreturnaset J with|J| = MhascomputationalcostO(M2). Reducingthecost
k
ofproducinglargeorbitsmotivatesthestrategyofhavingtheorbits J doubleinsizewitheach
k
iteration. FormoredetailsonthecomputationalcostoftheorbitselectionprocedureinNUTSas
wellasotheraspectsoftheimplementation,seeAppendixAin[4].
3.5 AdaptingpathlengthsintheApogee-to-ApogeePathSampler
TheApogee-to-ApogeePathSampler(AAPS)introducedin[20]isaspecialcaseoftheGISTsampler
presentedinFigure1. TheideabehindAAPSistoreplacetheNo-U-Turnconditionforadapting
thenumberofleapfrogstepswithanalternativeconditionbasedonpartitioningthediscretepath
{Φi h(θ,ρ)} i∈Z into‘segments’{S i(θ,ρ)} i∈Z. Thei-thsegmentS i(θ,ρ)consistsofpointsalongthe
discretepathoriginatingfromthecurrentpoint(θ,ρ)thatliebetweenconsecutivelocalmaxima
(termed“apogees”)oftheunderlyingpotentialenergyU(x).
In order to write AAPS as a GIST sampler, we first explain AAPS more precisely starting with
thepartitioningmentionedabove. Ourdescriptionfollows[20]. Proceedingbothforwardsand
backwardsintimefrom(θ,ρ)vialeapfrogsteps,consecutiveoutputsbelongtothesamesegment
ifthepotentialU iseitherinstantaneouslyincreasingatbothpointsorinstantaneouslydecreasing
atbothpoints. Otherwise,anapogeeoccursbetweenthispairofpoints,andhence,anewsegment
isdefined. Iteratingthisprocedureproducesatwo-sidedsequenceofsegments:
...,S −1(θ,ρ),S 0(θ,ρ),S 1(θ,ρ),...
where S (θ,ρ) is the initial segment. Following the notation of [20], let S ((θ,ρ),(θ′,ρ′)) be
0 #
the index of the segment started at (θ,ρ) containing the leapfrog iterate (θ′,ρ′). For instance,
S ((θ,ρ),(θ,ρ)) = 0 since the starting point is always in the initial segment S (θ,ρ) and
# 0
S ((θ,ρ),(θ′,ρ′)) = k for every (θ′,ρ′) ∈ S (θ,ρ) and k ∈ Z . Additionally, let S (θ,ρ) =
# k a:b
(cid:83) S (θ,ρ)for a,b ∈ Z suchthat a ≤ b.
i=a,...,b i
Apositiveinteger K andauser-definedweightfunction w : R4d → [0,∞)arerequiredinAAPS.
Thelatterassignsweightstopointswithinselectedsetsofsegments. Fromthecurrentpoint(θ,ρ),
8AAPS uniformly randomly chooses among the K+1 sets of segments: S 0:K,S −1:K−1,...,S −K:0.
ThesesetsofsegmentsareallpossibleunionsofK+1consecutivesegmentswhoseunioncontains
thecurrentpoint. HavingselectedsuchasetofsegmentsS −c:K−c,AAPSthenrandomlyselectsa
leapfrogiterate(θ′,ρ′) ∈ S −c:K−c asaproposalwithweightw((θ,ρ),(θ′,ρ′)). Finally,thisproposal
isMetropolizedwithMetropolis-Hastingsacceptanceprobability
(cid:32) w((θ′,ρ′),(θ,ρ)) ∑ w((θ,ρ),(θ˜,ρ˜)) (cid:33)
1∧
e−∆H(θ,ρ) (θ˜,ρ˜)
. (7)
w((θ,ρ),(θ′,ρ′)) ∑ w((θ′,ρ′),(θ˜,ρ˜))
(θ˜,ρ˜)
Theweightfunctioncanbechosentopreferentiallyselectproposalsaccordingtovariousdesiderata.
For instance, one might take w((θ,ρ), (θ′,ρ′)) = e−H(θ′,ρ′) to select points which will always be
accepted under the Metropolis-Hastings step, or w((θ,ρ),(θ′,ρ′)) = e−H(θ′,ρ′)||θ −θ′||2 to bias
toward proposals which are farther from the current point. In addition to these, several other
choicesofweightfunctionsaresuggestedin[20].
WearenowinpositiontowriteAAPSasaGISTsampler. LetA = Z2,letηbethecountingmeasure
onA,andwritethealgorithmtuningparameterincomponentsasα = (c,i) ∈ A. Thecomponent
crepresentsthechoiceofsetofsegmentsS
−c:K−c
whilethecomponentirepresentsthechoiceof
leapfrogiterateΦi(θ,ρ)withinthissetofsegments.
Define
h
p(c,i | θ,ρ) =
1 w((θ,ρ),Φi h(θ,ρ))1 S−c:K−c(θ,ρ)(Φi h(θ,ρ))
,
K+1 ∑ w((θ,ρ),Φj(θ,ρ))1 (Φj(θ,ρ))
j h S−c:K−c(θ,ρ) h
wherec ∈ {0,...,K}. ThemapGin(3)isgivenexplicitlyby
(cid:16) (cid:17)
G : (θ,ρ,c,i) (cid:55)→ S ◦Φi(θ,ρ), c+S ((θ,ρ), S ◦Φi(θ,ρ)), i ,
h # h
and the corresponding acceptance probability in (4) simplifies to (7). Thus, the GIST sampler
in Figure 1 with the above specification of the conditional distribution of the algorithm tuning
parameterα = (c,i)giventhestate(θ,ρ)correspondtoAAPS.
4 Truncation of an infinite-dimensional Gaussian
Thissectiondemonstratesthattheexactself-tunedHMCalgorithmdescribedinSection3.2can
achieveasimilarmean-squaredjumpdistance(MSJD)asrandomizedHMCfromSection3.1on
atargetdistributionthatcanbeinterpretedasatruncationofaninfinite-dimensionalGaussian
measure[3,11]. Thisisaworst-caseexamplesincethecorrespondingHamiltoniandynamicsis
highlyoscillatory[44]. Moreprecisely,thetargetdistributionisad-dimensionalcenterednormal
distributionwithcovariancematrixgivenby
i
Σ = diag(σ ,...,σ ) , where σ = fori ∈ {1,...,d} . (8)
1 d i
d
Thisexamplehastraditionallybeenusedtoillustratetheimportanceofpathlengthrandomization
to avoid slow mixing due to periodicities or near-periodicities in the underlying Hamiltonian
dynamics [43, 15, 9]. Here we use this worst-case example to demonstrate the efficacy of the
self-tunedHMCalgorithmsbasedontheU-turnconditionsdefinedby(5)and(6).
9Forthismodel,theexactsolutiontothecorrespondingHamiltoniandynamics(θ ,ρ )attimet ≥ 0
t t
frominitialcondition(θ ,ρ )isgivenincomponentsby
0 0
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
t t 1 t t
θi = cos θi +σ sin ρi and ρi = − sin θi +cos ρi,
t σ 0 i σ 0 t σ σ 0 σ 0
i i i i i
for i ∈ {1,...,d}. Using these solutions, the evaluation of the U-turn path lengths in (5) or (6)
reducestofindingthefirstpositiverootofascalarfunctionoftime,whichitselfcruciallyrelieson
agoodinitialization.
Tobeprecise,weprovideacompletedescriptionoftheinitializationprocedureforthecaseofthe
U-turnpathlengthgivenin(5);asimilarinitializationisusedfor(6)andthereforeomitted. Our
goalistofindthefirstpositiverootof f,i.e.,
τ = inf{t ≥ 0 : f(t) = 0} .
wherewehavedefinedthefollowingfunctionoftime
d
∑
f(t) = f (t) ,
i
i=1
where
(cid:18) (cid:19) (cid:18) (cid:19)
1 t t
f (t) = − sin θiρi +cos (ρi)2 .
i σ σ 0 0 σ 0
i i i
Thefirstpositiverootof f canbecomputedanalytically,i.e.,
i
(cid:32) (cid:33)
ρi
τ = arctan 0 +k⋆ π ,
i θi
0
where
(cid:40) (cid:32) (cid:33) (cid:41)
ρi
k⋆ = min k ∈ Z : arctan 0 +kπ ≥ 0 .
θi
0
The mean of {τ}d is used to seed the root solver for finding τ. Using the above root finding
i i=1
proceduretoevaluateτ pertransitionstep,thenumericallyestimatedmean-acceptanceprobability
(mean a ),mean-squaredjumpdistance(MSJD),andmeanpathlength(meanτ)ford = 1000and
e
using105 transitionstepsissummarizedinthefollowingtable.
mean a MSD meanτ
e
randomizedHMC 100% 429.81 1.00
exactself-tunedHMC(5) 97.4% 174.85 0.44
exactself-tunedHMC(6) 94.4% 573.15 1.16
Randomized HMC is operated using the optimal choice of mean path length for maximizing
expectedsquarejumpdistanceinthisexample,whichcorrespondstothestandarddeviationofthe
leastconstrainedcoordinate[15,Section4]. Theself-tunedHMCalgorithmbasedon(6)slightly
outperformed exact randomized HMC in terms of MSD, while the one based on (5) performed
worse. Remarkably, neither degenerated in high dimension. This high-dimensional example
demonstrates both the efficacy of the U-turn conditions and the leniency of the corresponding
acceptanceprobabilityintheself-tunedHMCalgorithm.
102 2
1 1
0 0
-1 -1
-2 -2
-2 -1 0 1 2 -2 -1 0 1 2
Figure2: Self-TuningPathLengthsforExactHMC.Thisfigureplotsforwardandbackwardtrajectories
oftheGISTsamplerusing(5)inthephasespaceoftheleastconstrainedcoordinateofthe d-dimensional
Gaussianmeasurewithcovariancematrixgivenin(8)with d = 103. Theleftpanelillustratesarejected
proposal(duetoα > τ )andtherightpanelillustratesanacceptedproposal.
2
Remark4. Thesenumericalfindingsareofindependentinterest,sincetheymotivateusingGIST
samplersbasedonU-turnavoidingconditionstosamplefromperturbedGaussianmeasureson
Hilbertspaces. Thisclassoftargetmeasuresisrelevanttoseveralimportantapplicationsincluding
Path Integral Molecular Dynamics [19, 39, 28, 36], Transition Path Sampling [47, 46, 7, 40], and
Bayesian statistical inverse problems for Hilbert spaces [32, 50, 21, 8]. Since the corresponding
Hamiltonian dynamics is potentially highly oscillatory in high modes [44], in actual numerical
implementations,preconditioning[10]orstronglystableintegrators[35]arenecessarytobeableto
choosethestepsizeindependentlyofthedimension.
5 Path length sampling to avoid U-turns
WenowturntoanextendedexampleofanovelU-turnavoidingsamplerbasedonGibbsselftuning.
We will focus on locally adapting the number of steps. Thus for each HMC step, the self-tuned
HMC algorithm will generate the number of steps probabilistically according to a distribution
p(L | θ,ρ,ϵ,Σ). We consider several such distributions based on the no U-turn condition in the
nextsectionandevaluatetheminthefollowingsections.
Figure3providespseudocodeforageneralalgorithmadaptingthenumberofleapfrogsteps. The
ratiooftheproposaljointdensitytothestaringpointjointdensitycanbefactoredas
p(θ∗ ,ρ∗ ,L∗ | ϵ,Σ) p(θ∗) p(ρ∗ | Σ) p(L∗ | θ∗ ,ρ∗ ,ϵ,Σ)
(cid:16) (cid:17) = (cid:16) (cid:17) · (cid:16) (cid:17) · (cid:16) (cid:17),
p θ(0) ,ρ(0) ,L∗ | ϵ,Σ p θ(0) p ρ(0) | Σ p L∗ | θ(0) ,ρ(0) ,ϵ,Σ
where p(θ) = p(θ | y) is the target probability density function, p(ρ | Σ) = normal(0,Σ) is the
momentumprobabilitydensityfunction,and p(L | θ,ρ,ϵ,Σ)istheconditionalstepsizeprobability
massfunction.
11GIST(θ,ρ,ϵ,Σ)
θ ∈Rd initialposition
ρ ∈Rd initialmomentum(unused—discardedinGibbsupdate)
ϵ ∈ (0,∞) stepsize
Σ ∈Rd×d symmetric,positivedefinitemassmatrix
p(θ) targetdensity(logdensityevaluation&gradient)
p(L | θ,ρ,ϵ,Σ) conditionalstepsdistribution(sampler&logdensityevaluation)
(INITIALIZE)
θ(0) = θ
(GIBBS)
ρ(0) ∼normal(0,Σ) (Gibbssamplemomentum)
L ∼ p(L | θ(0), ρ(0), ϵ,Σ) (Gibbssamplenumberofsteps)
(METROPOLIS-WITHIN-GIBBS)
forℓfrom0toL−1(inclusive): (Lleapfrogsteps)
ρ(ℓ+1/2) = ρ(ℓ)+ ϵ ·∇logp(θ(ℓ)) (halfstepmomentum)
2
θ(ℓ+1) = θ(ℓ)+ϵ·Σ−1·ρ(ℓ+1/2) (fullstepposition)
ρ(ℓ+1) = ρ(ℓ+1/2)+ ϵ ·∇logp(θ(ℓ+1)) (halfstepmomentum)
2
θ∗,ρ∗ = θ(L),−ρ(L) (proposalflipsmomentum)
u ∼uniform([0,1]) (sampleacceptanceprobability)
p(θ∗ , ρ∗ , L | ϵ,Σ)
ifu < (Metropolisacceptcondition)
p(θ(0) , ρ(0) , L | ϵ,Σ)
returnθ∗, ρ∗, L (accept)
else
return θ(0), ρ(0), L (reject)
Figure3: GISTsamplingforpath-lengthself-tuning. TheGISTsamplerforpath-lengthself-tuning
differsfromstandardHMCinsamplingthenumberofstepseachiterationandthenadjustingtheacceptance
probabilitytoensuredetailedbalance. Note,theGibbsstepsforrefreshmentofbothmomentumandnumber
ofstepsareexactdrawsfromthecorrespondingconditionaldistributions.
5.1 StepdistributionsavoidingU-turns
Tomakeoursamplerconcrete,aspecificdistributionoverthenumberofstepsmustbedefined. We
evaluateafewrelatedchoices,allofwhicharemotivatedbytheobservationdrivingtheNoU-Turn
Sampler(NUTS)[31],whichisthatit’swastefultoevaluatetheHamiltoniandynamicsbeyondthe
pointatwhichthetrajectoryhasmadeaU-turnandisheadingbacktowardthestartingposition.
Figure4illustratestheU-turncondition,whichismadepreciseinEquation(9).
Let
U(θ,ρ,ϵ,Σ)
be the maximum number of leapfrog steps that can be taken before a U-turn,
startingfrom(θ,ρ)andusingstepsizeϵandmassmatrixΣ
. ThepointjustbeforeaU-turncanbe
12θ(3)
θ(1)
α
θ∗
θ(2)
θ(0)
Figure 4: U-turn condition. A Hamiltonian trajectory of positions (not momenta) in two dimensions,
consistingofthreeleapfrogstepsplusapotentialfourthstep. Thedottedlineconnectstheinitialposition
θ(0) tothecurrentpositionθ(3). Thedashedlineconnectsthecurrentpositiontothenextpotentialposition
θ∗ andrunsinthedirectionofthecurrentmomentumρ(3). Thetrajectoryisextendedonesteptoθ∗ ifthe
nextstepmovesawayfromtheinitialposition,whichrequirestheabsolutevalueoftheangleαbetweenthe
dottedlineandthedashedlinetobegreaterthan90◦,whichariseswhen(θ(3)−θ(0))⊤·(θ∗−θ(3)) > 0,or
equivalently,when(θ(3)−θ(0))·ρ(3) > 0.
L−N ··· 0 ··· L ··· M
(θ,ρ) (θ∗,ρ∗)
M = U(θ,ρ,ϵ,Σ)
N = U(θ∗,ρ∗,ϵ,Σ)
Figure5: Self-tunedstepsproposal. Startingfromtheinitialpositionandmomentum(θ,ρ),thealgorithm
makes MforwardleapfrogstepsuntilaU-turn. ItthenGibbssamplesanumberofsteps Lbetween0and M
forwhichtheleapfrogintegratorproducestheproposal(θ∗,ρ∗). Then N backwardleapfrogstepsaretaken
until a U-turn. If L−N > 0, the proposal is rejected. The algorithm takes M+N−L unique leapfrog
steps.
definedforthediscretestepsoftheleapfrogintegratorby
(cid:16) (cid:17) (cid:16) (cid:17)⊤
U θ(0) , ρ(0) , ϵ,Σ = argmin θ(n)−θ(0) ·ρ(n) < 0, (9)
n∈N
where θ(n),ρ(n) isthelocationinphasespaceafter n leapfrogstepsfrom θ(0),ρ(0) withstepsize ϵ
Σ
andpositive-definitemassmatrix .
Figure5showsasinglestepofthealgorithm. Thenumberofstepsissampledconditionallybased
onthenumberofstepspossiblebeforeaU-turn. Topreservedetailedbalance,thetrajectoryfrom
the selected point backward in time is evaluated until it makes a U-turn and the probability of
selecting the initial state (i.e., the same number of steps) is used to balance the selection. This
requiressamplingzeroormorestepsbackwardintimebeforetheinitialposition.
135.2 Conditionaldistributionofsteps
In this section, we consider a few closely related approaches to generating the number of steps
giventhenumberofstepstoaU-turn.
5.2.1 Stepsgenerateduniformly
Themostobviouschoiceforaconditionaldistributionoverthenumberofstepsisuniformbetween
1andthenumberofstepsbeforeaU-turn,
p(L | θ,ρ,ϵ,Σ) = uniform(L | 1, U(θ,ρ,ϵ,Σ)), (10)
wheretheboundsarereadinclusively.
5.2.2 Stepsgenerateduniformlyfromlaterstates
OneofthetechniquesNUTSusestotakesuchlongjumpsonaverageistobiastheselectionofa
candidatetowardtheendoftheHamiltoniantrajectory[31]. Tocreateasimpleapproximationto
this,theuniformdistributioncanberestrictedtothelatterpartofthetrajectorybychangingthe
lowerboundfrom1tosomethinggreater. Anynumberbetween0and M = U(θ,ρ,ϵ,Σ)isvalidas
alower bound. The choice of1correspondstothe uniformdistributionof theprevioussection.
Wewillalsoevaluatelowerboundsof ⌊1 ·M⌋ and ⌊3 ·M⌋, where ⌊x⌋ isthefloorof x, whichis
2 4
largestintegerlessthanorequalto x. Withsmallerintervals,theproposedtrajectorylengthswill
belonger,butthereversibilitybalanceconditionwillreducetheacceptanceprobability.
5.2.3 Binomialstepgeneration
Non-uniformdistributionsmayalsobeused. Forexample,abinomialdistributionforthenumber
ofstepscouldbedefinedforafixedψ ∈ (0,1)as
p(L | θ,ρ,ϵ,Σ) = binomial(L | ψ, U(θ,ρ,ϵ,Σ)). (11)
The expected value of L starting from (θ,ρ) is thus ψ·U(θ,ρ,ϵ,Σ). We evaluated the binomial
methodanditwindedupgeneratingproposalsthatweretooconcentratedandthushardtobalance
andaccept,sowedonotincluderesultshere. Wedidnotevaluateamoredispersedbeta-binomial
distributionduetothecostofthebetaandgammafunctionsrequiredfornormalization.
6 Empirical evaluation
Inthissection,weevaluatetheperformanceofourvariousproposalsforstepsizeadaptationand
comparetheirperformancetothestate-of-the-artNoU-TurnSamplerascodedinStan[31,4].
6.1 Modelsevaluated
The models considered are multivariate standard normal, ill-conditioned multivariate normal,
correlated multivariate normal, eight schools meta-analysis, item-response theory 2 parameter
14theta theta**2 theta theta**2
1 1
0.1 0.1
0.01 0.01
0.001 0.001
1e2 1e4 1e6 1e2 1e4 1e6 1e2 1e4 1e6 1e2 1e4 1e6
iteration iteration
Figure6: Learningcurvevalidation. Theabsoluteerrorinparameterandparametersquaredestimatesfor
i.i.d. draws(left)andtheuniformself-tuningalgorithm(right). Ineachplot,thelefthalfshowserrorfor
parameterestimatesandtherighthalfshowserrorforsquaredparameterestimates(averagedoverthe100
identicaldimensions). Forreference,thedottedlineisthestandarderrorderivedfromindependentdraws,
√ √ √
whichatiterationnis1/ nforparameterestimatesand 2/ nforsquaredparameterestimates.
logistic model, mixed effects Poisson regression, normal mixture, hidden Markov model, one-
compartmentpharmacokinetic/pharmacodynamicmodel(PK/PD),Lotka-Volterrapopulation
dynamics,autoregressivetimeseries(AR),autoregressivemovingaveragetimeseries(ARMA),
andgeneralizedautoregressiveconditionalheteroskedasticitytimeseries(GARCH).Allbutthe
normalmodelsaredrawnfromtheposteriordbpackage[38],butreparameterizedaccordingto
StanbestpracticessothatNUTScansamplethem.
6.2 Learningcurve
Asasimplevalidationthatthesampleristargetingthecorrectdistribution,Figure6plotsalearning
curve(expectedabsoluteerror)versusiterationfortheuniformsamplerwiththefullpathforthe
√
eight schools model. The plot shows that error decreases as expected at a rate of 1/ n and the
efficiencyisgreaterthanthatofindependentdrawsfortheparametersandslightlyworseforthe
parameterssquaredinthissimplecase.
6.3 Effectofstepsizeandpathfraction
InFigure7,theperformanceoftheuniformself-tuningsamplerisshownfortwostepsizes,0.36
(blue lines) and 0.18 (red lines), across a range of lower bound fractions (x axis) for uniform
sampling. The step size 0.36 is what NUTS adapted for an 80% average Metropolis acceptance
probability(Stan’sdefault);0.18isthestepsizeforroughly95%Metropolisacceptance. Halving
thestepsizeroughlydoublesthenumberofleapfrogstepstaken,asshownintheupperleftofthe
plot. Theremainingplotsshowthatperformanceisbetterwithasmallerstepsize.
Meansquarejumpdistance(MSJD)isalsoshowninFigure7;itisdefinedfor Mstepsofsampling
by
MSJD =
1 ∑M (cid:12) (cid:12)(cid:12) (cid:12)θ(m+1)−θ(m)(cid:12) (cid:12)(cid:12) (cid:12)2
. (12)
(cid:12)(cid:12) (cid:12)(cid:12)
M 2
m=1
15
]|rre|[E ]|rre|[ELeapfrog Steps MSJD No Return
0.6
2500 1500
0.4
2000 1200
1500 0.2
900
1000 0.0 Step Size
0.18
RMSE (param sq) RMSE (param) Reject
0.16 0.36
0.6
1.0 0.12
0.4
0.08
0.5 0.2
0.04
0 1/4 1/2 3/4 1 0 1/4 1/2 3/4 1 0 1/4 1/2 3/4 1
Lower Bound Fraction
Figure7: Performanceversusstepsizeandlowerboundfraction. Theplotsrepresentaverages(over
200 repetitions) of results for 100 iterations of uniform self-tuning HMC starting from a draw from the
500-dimensionalstandardnormaltargetdistribution. The x-axisrepresentsthefractionofthenumberof
stepstoaU-turntouseasthelowerboundinuniformlydrawinganumberofleapfrogsteps. Linecolor
indicatesstepsizes0.36(blue)and0.18(red). Thetitlesofthesubplotsdescribethevaluesonthey-axis. The
label"NoReturn"(topright)isforthefractionofnon-reversibleproposalsduetoaU-turnbeforereturning
tothestartingpoint,andtheproportionofrejections(lowerright)includesthoseduetonon-reversibility.
TheMSJDisareasonableestimateoftheexpectedsquaredjumpdistance,
(cid:20)(cid:12)(cid:12) (cid:12)(cid:12)2(cid:21)
ESJD = E (cid:12)(cid:12)θ(t+1)−θ(t)(cid:12)(cid:12) . (13)
(cid:12)(cid:12) (cid:12)(cid:12)
2
Asthepathfractiongoesup,theMSJDgoesupuntiltheno-returnrejectionrateovertakesitandit
beginstodecrease.
Therejectionrateisbrokendownintototalrejectionrate,andthenthenumberofrejectionsdueto
notbeingabletoreturntotheoriginbeforehittingaU-turn(seeFigure5). Smallerstepsizesdoa
betterjobatpreservingtheHamiltonianandthushavelowerrejectionrates.
Rootmeansquareerror(RMSE)isdefinedby
(cid:118)
(cid:117)
RMSE = (cid:117) (cid:116) 1 ∑M (cid:12) (cid:12)(cid:12) (cid:12)θ(m)−θ(cid:12) (cid:12)(cid:12) (cid:12)2 , (14)
M 2
m=1
whereθisthevalueoftheparameterandtheθ(m)areMCMCdraws. Ifthedrawswereindependent,
theexpectedRMSEforparametersis0.1(thestandarddeviation,1,dividedbythesquarerootof
√ √
thenumberofi.i.d. draws, 100). TheexpectedRMSEforparameterssquaredis 2/10because
standard normal variates squared follow a chi-squared distribution with 1 degree of freedom,
√
the standard deviation of which is 2. The RMSE for parameters is below that of independent
16draws,goingdowntoroughlyhalfatlowerboundfraction0.6. Notsurprisingly,becauseMSJD
tracksinverselag-oneautocorrelation,thebestRMSEforparametersisachievedwheretheMSJDis
maximized. TheRMSEforparameterssquaredismazimizedatarounda0.35lower-boundfraction,
thoughvaluesareveryclosebetween0and1/2. Withasinglelowerboundfraction,atradeoff
mustbemade,asitmustbeforHMCandNUTS.Hereafractionof0.6appearsreasonable. The
RMSEislowerforthesmallerstepsizes,butdoublingthenumberofiterationswiththelargerstep
√
sizewouldreduceexpectedRMSEbyafactorof1/ 2(about70%).
6.4 Evaluationsformultiplemodels
Werunallofourreferencemodelsfor10,000warmupand40,000iterationsofNUTSusingStan’s
defaults and use the sampling draws to define reference means for parameters and parameters
squared. TheresultsfortheuniformsamplerareshowninFigure8. Therearefourplots,which
showRMSEforparameters,RMSEforsquaredparameters,MSJD,andleapfrogsteps.
We evaluate NUTS (in red) and three settings of uniform sampling, S(hort), which uses
uniform(1,L) sampling, where L is the number of steps to a U-turn, M(edium), which uses
uniform(⌊0.3· L⌋,L), and L(ong), which uses uniform(⌊0.5· L⌋,L), and XL (extra long), which
uses uniform(⌊0.7·L,L) sampling. Two step sizes are considered, (1) the step size adapted by
NUTSwithitsdefaultacceptancetarget(80%),and(1/2),whichishalfofthatstepsize,whichis
approximatelywhatNUTSwouldadaptforatargetof95%stepsize,whichisrecommendedfor
thenumericalstabilityoftheleapfrogintegratorinmoredifficultproblems.
Startingwithleapfrogsteps(thebottomplot),itisclearthattheshorterexpectedpathsrequiremore
stepsforself-tuningHMC,becausetheyarelikelytohavetogofurtherpasttheoriginbackwardin
timebeforehittingaU-turn. Thereisnevermorethanafactoroftwodifferenceamongsystemsin
termsofleapfrogsteps. ThestepsrequiredforNUTSandST-HMCaresimilar;forsomemodels,
ST-HMCrequiresfewerstepsthanNUTSandforothersitrequiresmoresteps. Forthethreenormal
cases,ST-HMCrequiresfewerstepsforthestandardnormalandcorrelatednormal,butaboutthe
samenumberfortheill-conditionednormal.
Smallerstepsizesleadtoroughlytwiceasmanyleapfrogsteps,butnotquitethatincomplicated
models(ormodelsthatrunupagainstthe1024maximumsteplimitimposedonbothNUTSand
ST-HMC).
ThesecondplotfromthebottomshowstheMSJDforallsamplers,whichisinverselyrelatedto
autocorrelationatlag1. Thecleartrendisthatlargerstepsleadtolowererror,butthisisnotalways
borne out. For instance, in the standard normal, the ST-HMC sampler takes longer steps and
improvestheestimateofparameters,butNUTShaslowererrorestimatingparameterssquared.
Thefirst twoplotsshow RMSEforparameters andparameterssquared. All plots havea y-axis
startingat0sothatrelativeperformancemaybeassessedvisually. Manyofthemorecomplicated
modelsareneckandneckinRMSEforparametersandparameterssquared,withNUTSconsistently
slightlyoutperformingST-HMC.
In conclusion, NUTS slightly outperforms ST-HMC overall. This is highly encouraging in that
thisisarelativelysimplefirstattemptatself-tunedHMC,whereasNUTSisrepresentedbyStan’s
17000 ... 000 123
staerpK 1
0000 .... 0000 0011 4826
satremp a1
0000 .... 1234
corsrt-enpo r1mal
123
eighstt-espc h1ools
00000 ..... 00011 25702 50505
sgtaerpc h1
0000 .... 0000 0112 5050
glmmst-eppo 1isson
000 ... 000 246
shtmepm 1
0112 .... 5050
ills-nteoprm 1al
000 ... 011 505
sirtte-2pp 1l
000 ... 246
lotksat-evpo l1terra
00 .. 01 50
nsotermp a1l
000 ... 000 001
482normsatel-pm 1ixture
11 4826
sptekpp d1
0.00 0.000 0.0 0 0.000 0.000 0.00 0.0 0.00 0.0 0.00 0.000 0
0.025
steapr K1/2 staerpm 1a/2 cosrtre-npo 1rm/2al eigshtte-ps c1h/o2ols stgeapr c1h/2
0.0100
glmsmte-pp o1i/s2son sthemp m1/2
2.0
ilslt-neopr 1m/2al stiretp-2 1p/l2 lotkstae-vpo 1lt/e2rra
0.125
snteoprm 1a/2l normstaelp-m 1i/x2ture
8
stpekpp 1d/2
0.020 0.009 0.3 2 0.15 0.0075 0.06 1.5 0.075 0.3 0.100 0.010 6
000 ... 000 011 505 00 .. 00 00 36 00 .. 12
1
00 .. 01 50 00 .. 00 00 25 50 00 .. 00 24 01 .. 50 00 .. 00 25 50 00 .. 12 000 ... 000 257 505
0.005
24
0.000 0.000 0.0 0 0.00 0.0000 0.00 0.0 0.000 0.0 0.000 0.000 0
Sampler
0000 .... 0000 0112 5050
staerpK 1
0000 .... 0000 0000 2468
satremp a1
000 ... 246
corsrt-enpo r1mal
257 505
eighstt-espc h1ools
00000 ..... 12345
sgtaerpc h1
000 ... 000 011 505
glmmst-eppo 1isson
0000 .... 1234
shtmepm 1
12345 00000
ills-nteoprm 1al
000 ... 246
sirtte-2pp 1l
1234 0000
lotksat-evpo l1terra
000 ... 246
nsotermp a1l
000 ... 000 257 505
normsatel-pm 1ixture
11 505 000 000
sptekpp d1
0.000 0.000 0.0 0 0.0 0.000 0.0 0 0.0 0 0.0 0.000 0
0000 .... 0000 0112 5050
steapr K1/2
0000 .... 0000 0001 3692
staerpm 1a/2
000 ... 246
cosrtre-npo 1rm/2al
123 000
eigshtte-ps c1h/o2ols
000 ... 246
stgeapr c1h/2
000 ... 000 011 505
glmsmte-pp o1i/s2son
000 ... 246
sthemp m1/2
246 000
ilslt-neopr 1m/2al
000 ... 246
stiretp-2 1p/l2
123 000
lotkstae-vpo 1lt/e2rra
0000 .... 1234
snteoprm 1a/2l
0000 .... 0000 2468
normstaelp-m 1i/x2ture
246 000 000
stpekpp 1d/2
0.000 0.000 0.0 0 0.0 0.000 0.0 0 0.0 0 0.0 0.00 0
Sampler
00 .. 01 50
staerpK 1
000 ... 000 011 505
satremp a1
246 000 000
corsrt-enpo r1mal
246 000 000
eighstt-espc h1ools
00011 ..... 25702 50505
sgtaerpc h1
0000 .... 1234
glmmst-eppo 1isson
000 ... 123
shtmepm 1
246 000 000 000 000
ills-nteoprm 1al
369 000
sirtte-2pp 1l
12345 00000
lotksat-evpo l1terra
11 505 000 000
nsotermp a1l
00000 ..... 00000 01122
50505normsatel-pm 1ixture
11 505 000 000
sptekpp d1
0.00 0.000 0 0 0.00 0.0 0.0 0 0 0 0 0.000 0
0000 .... 0001 3692
steapr K1/2
000 ... 000 011 505
staerpm 1a/2
123456 000000 000000
cosrtre-npo 1rm/2al
2468 0000 0000
eigshtte-ps c1h/o2ols
000 ... 257 505
stgeapr c1h/2
0000 .... 1234
glmsmte-pp o1i/s2son
000 ... 011 505
sthemp m1/2
246 000 000 000 000
ilslt-neopr 1m/2al
369 000
stiretp-2 1p/l2
246 000
lotkstae-vpo 1lt/e2rra
11 505 000 000
snteoprm 1a/2l
0000 .... 0000 0112
5050normstaelp-m 1i/x2ture
12345 00000 00000
stpekpp 1d/2
0.00 0.000 0 0 0.00 0.0 0.00 0 0 0 0 0.000 0
Sampler
123 000 000 000
staerpK 1
1 50 00 00
satremp a1
1 50 00 00 00
corsrt-enpo r1mal
1 482 000 000
eighstt-espc h1ools
112 5050 0000 0000
sgtaerpc h1
12345 00000 00000 00000
00000glmmst-eppo 1isson
12345 00000 00000 00000
shtmepm 1
246 000 000 000 000
ills-nteoprm 1al
246 000 000 000
sirtte-2pp 1l
123 000 000 000
lotksat-evpo l1terra
1 482 000 000
nsotermp a1l
11 4826 0000
0000normsatel-pm 1ixture
112 5050 0000 0000
sptekpp d1
0 0 0 0 0 0 0 0 0 0 0 0 0
12345 00000 00000 00000
steapr K1/2
1122 50505 00000 00000
staerpm 1a/2
112 5050 0000 0000 0000
cosrtre-npo 1rm/2al
12 00 00 00
eigshtte-ps c1h/o2ols
123 000 000 000
stgeapr c1h/2
1 2570 5050 0000 0000
0000glmsmte-pp o1i/s2son
369 000 000 000
sthemp m1/2
1 2570 5050 0000 0000 0000
ilslt-neopr 1m/2al
11 25702 50505 00000 00000
stiretp-2 1p/l2
2468 0000 0000 0000
lotkstae-vpo 1lt/e2rra
123 000 000 000
snteoprm 1a/2l
123 000 000
000normstaelp-m 1i/x2ture
1234 0000 0000 0000
stpekpp 1d/2
0 0 0 0 0 0 0 0 0 0 0 0 0
Sampler
Figure8: EmpiricalEvaluationsvs.NUTS.Fromtoptobottom: rootmeansquareerrorofparameters,
rootmeansquareerrorofparameterssquared(lowerisbetter),meansquarejumpdistances(higherisbetter),
andnumberofleapfrogsteps(smallerismoreefficient). Eachcolumnisforadifferentmodelandtherows
are forruns atNUTS default adaptedstep size(targeting0.8average Metropolis acceptance)andhalf of
thatrate(whichtargetsapproximatelya0.95Metropolisacceptance,whichincreasesstabilityoftheleapfrog
integrator). TheredresultisNUTSandtheblueresultsarefortheself-tuninguniformsampleroverintervals
with0.0(S),0.3(M),0.5(L),and0.7(XL)pathfraction.
currentimplementation,whichhasbeenimplementedrobustlyandimprovedconsiderablyover
thefirstNUTSpaper(andfirstversioninStan)[4,31].
18
)marap(
ESMR
)qs marap(
ESMR
DJSM
spetS gorfpaeL
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
ML SS L- X-T -U U -UU UT TTN S TSS S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
S LX-U -UT TS S
STUN
STUN
STUN
STUN
STUN
STUN
STUN
STUN
L-UTS
L-UTS
L-UTS
L-UTS
L-UTS
L-UTS
L-UTS
L-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
LS -T UU TN S
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
M-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
MLS -T -U UU T TN S S
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
S-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS
LX-UTS6.5 Open-sourcecodeandreproduciblity
Theresultsandplotscanbereproducedfromourcodedistribution,whichisavailablefromGitHub
underapermissiveopen-sourcelicense.3
7 Other Related Prior work
TheGISTsamplermaybeviewedasadynamicversionoftheempiricalHMCsampler[53]. This
empiricalHMCsamplerlearnsanempiricaldistributionoftheunderlyingpathlengthstoU-turns
duringawarmupphaseandthenfixesthisempiricaldistribution,whichisthenusedtosample
pathlengthsateachstepduringsampling. LikerandomizedHMCinSection3.1, theempirical
HMCsamplerisaninstanceoftheGISTsamplerwheretheconditionaldistributionofpathlengths
doesnotdependonthecurrentpositionand/ormomentum.
Both NUTS and the Apogee-to-Apogee Path Sampler reviewed in Section 3 can be formulated
within the dynamic HMC framework introduced in [23]. Dynamic HMC can be viewed as an
instanceofageneralizedHit-and-Runsamplerinthesenseof[1]. Ontheotherhand,asdiscussed
inSection2,theGISTsamplerisaGibbssampler(withaMetropolis-within-Gibbsstep)aimedat
theenlargedtargetmeasure(2). AkeybenefitofincorporatingaMetropolis-within-Gibbsstepin
GISTisthatitrelaxestherestrictivesymmetryconditionsrequiredforcorrectnessofdynamicHMC
inpractice,andhence,offerssignificantflexibilitytoadaptotheralgorithmtuningparameters.
The autoMALA sampler introduced a self-tuning version of the Metropolis-adjusted Langevin
(MALA) sampler [33, 6]. It uses a forward and reverse non-deterministic scheme to choose
adaptationparametersinawaythatsatisfiesdetailedbalance. MALAisequivalenttoone-step
HamiltonianMonteCarlo,butissimplerinnotneedingtoevolvemomentum. TheGISTsampler
canbeviewedasaprobabilisticgeneralizationoftheautoMALAadaptationselectioncriteria.
Forstepsizeandmassmatrixtuning,twoapproacheshavebeenpopularinpractice. Inthefirst,
anadaptationphaseisusedtoestimatealgorithmtuningparameters. Thesetuningparametersare
thenfixedsothattheresultingchainisMarkovian. ThisisthestrategyusedbyNUTSforstepsize
andmassmatrixadaptationforHMC[31]. Inthesecondapproach,theadaptationphaseisnever
turnedoff,buttheamountofadaptationisdecreasedsothatasymptoticallytheresultsarevalid.
ThisisthestrategyusedbydelayedrejectionMetropolis(DRAM)[27].
AnotherapproachforstepsizeadaptivityistherecentlyintroduceddelayedrejectionHMCsampler
of [42]. The delayed rejection algorithm [41, 26] is a generalization of Metropolis-Hastings to a
sequenceofproposalmoves. Thisproposalsequencecanbetunedtostartwithlargerscalemoves
and then scaled down for subsequent proposals [27]. In the same vein, the delayed rejection
HMCsamplerautomaticallytriessmallerstepsizesifproposalswithlargerstepsizesarerejected,
allowingittosamplefromtargetdensitieswithvaryingscale. Likeotherdelayedrejectionmethods,
itgenerates“ghostpoints”usingreversedproposals,whicharethenusedaspartoftheacceptance
probabilitytoensuredetailedbalance[26].
3ThecodeisdistributedundertheMITLicenseathttps://github.com/bob-carpenter/adaptive-hmc.
19Acknowledgements
We would like to thank Edward Roualdes, Tore Kleppe, Andreas Eberle, Sam Livingstone, and
ChiragModiforfeedbackontheGibbsself-tuningidea.
N.Bou-RabeehasbeenpartiallysupportedbyNSFGrantNo.DMS-2111224.
References
[1] H.AndersenandP.Diaconis.“Hitandrunasaunifyingdevice”.In:J.Soc.Fr.Stat.&Rev.
Stat.Appl.148(Jan.2007)(cit.onp.19).
[2] A.Beskos,N.S.Pillai,G.O.Roberts,J.M.Sanz-Serna,andA.M.Stuart.“Optimaltuningof
theHybridMonteCarloalgorithm”.In:Bernoulli19(2013),pp.1501–1534(cit.onp.1).
[3] A.Beskos,F.J.Pinski,J.M.Sanz-Serna,andA.M.Stuart.“HybridMonteCarloonHilbert
spaces”.In:StochasticProcessesandtheirApplications121.10(2011),pp.2201–2230(cit.onp.9).
[4] M.Betancourt.“AconceptualintroductiontoHamiltonianMonteCarlo”.In:arXivpreprint
1701.02434(2017)(cit.onpp.1,2,6–8,14,18).
[5] M.Betancourt,S.Byrne,andM.Girolami.“OptimizingtheintegratorstepsizeforHamilto-
nianMonteCarlo”.In:arXivpreprint1411.6669(2014)(cit.onp.1).
[6] M.Biron-Lattes,N.Surjanovic,S.Syed,T.Campbell,andA.Bouchard-Côté.“autoMALA:
LocallyadaptiveMetropolis-adjustedLangevinalgorithm”.In:27thInternationalConference
onArtificialIntelligenceandStatistics.Vol.PMLR238.2024(cit.onpp.1,19).
[7] P.G.Bolhuis,D.Chandler,C.Dellago,andP.L.Geissler.“Transitionpathsampling:Throwing
ropesoverroughmountainpasses,inthedark”.In:AnnualReviewofPhysicalChemistry53.1
(2002),pp.291–318.ISSN:0066-426X(cit.onp.11).
[8] J.Borggaard,N.Glatt-Holtz,andJ.Krometis.“ABayesianapproachtoestimatingbackground
flowsfromapassivescalar”.In:arXivpreprint1808.01084(2018)(cit.onp.11).
[9] N. Bou-Rabee and A. Eberle. “Couplings for Andersen dynamics in high dimension”. In:
Ann.Inst.H.PoincaréProbab.Statist58.2(2022),pp.916–944(cit.onpp.2,4,5,9).
[10] N. Bou-Rabee and J. M. Sanz-Serna. “Geometric integrators and the Hamiltonian Monte
CarloMethod”.In:ActaNumerica27(2018),pp.113–206(cit.onpp.3,11).
[11] N.Bou-RabeeandA.Eberle.“Two-scalecouplingforpreconditionedHamiltonianMonte
Carlo in infinite dimensions”. In: Stochastics and Partial Differential Equations: Analysis and
Computations9.1(2021),pp.207–242(cit.onp.9).
[12] N. Bou-Rabee and T. S. Kleppe. “Randomized Runge-Kutta-Nyström”. In: arXiv preprint
2310.07399(2023)(cit.onp.4).
[13] N. Bou-Rabee and M. Marsden. “Unadjusted Hamiltonian MCMC with stratified Monte
Carlotimeintegration”.In:arXivpreprint2211.11003(2022)(cit.onp.4).
[14] N. Bou-Rabee and S. Oberdörster. “Mixing of Metropolis-Adjusted Markov Chains via
couplings:Thehighacceptanceregime”.In:arXivpreprint2308.04634(2023)(cit.onp.4).
[15] N.Bou-RabeeandJ.M.Sanz-Serna.“RandomizedHamiltonianMonteCarlo”.In:Ann.Appl.
Probab.27.4(2017),pp.2159–2194(cit.onpp.1,2,4,5,9,10).
[16] N.Bou-RabeeandK.Schuh.“NonlinearHamiltonianMonteCarloanditsparticleapproxi-
mation”.In:arXivpreprint2308.11491(2023)(cit.onp.4).
20[17] Y. Cao, J. Lu, and L. Wang. “Complexity of randomized algorithms for underdamped
Langevindynamics”.In:CommunicationsinMathematicalSciences19.7(2021),pp.1827–1853
(cit.onp.4).
[18] B.Carpenter,A.Gelman,M.Hoffman,D.Lee,B.Goodrich,M.Betancourt,M.A.Brubaker,
J. Guo, P. Li, and A. Riddell. “Stan: A probabilistic programming language”. In: Journal of
StatisticalSoftware20(2016),pp.1–37(cit.onp.1).
[19] D.ChandlerandP.G.Wolynes.“Exploitingtheisomorphismbetweenquantumtheoryand
classicalstatisticalmechanicsofpolyatomicfluids”.In:TheJournalofChemicalPhysics74.7
(1981),pp.4078–4095.ISSN:0021-9606(cit.onp.11).
[20] S. U. Chris Sherlock and M. Ludkin. “The apogee to apogee path sampler”. In: Journal of
ComputationalandGraphicalStatistics32.4(2023),pp.1436–1446(cit.onpp.2,8,9).
[21] M.DashtiandA.M.Stuart.“TheBayesianapproachtoinverseproblems”.In:Handbookof
UncertaintyQuantification(2017),pp.311–428(cit.onp.11).
[22] P.deValpine,D.Turek,C.Paciorek,C.Anderson-Bergman,D.TempleLang,andR.Bodik.
“Programmingwithmodels:writingstatisticalalgorithmsforgeneralmodelstructureswith
NIMBLE”.In:JournalofComputationalandGraphicalStatistics26(2017),pp.403–417(cit.on
p.1).
[23] A. Durmus, S. Gruffaz, M. Kailas, E. Saksman, and M. Vihola. “On the convergence of
dynamicimplementationsofHamiltonianMonteCarloandnoU-turnsamplers”.In:arXiv
preprint2307.03460(2023)(cit.onpp.6,7,19).
[24] H.Ge,K.Xu,andZ.Ghahramani.“Turing:Alanguageforflexibleprobabilisticinference”.In:
InternationalConferenceonArtificialIntelligenceandStatistics,(AISTATS).2018,pp.1682–1690
(cit.onp.1).
[25] M.GirolamiandB.Calderhead.“RiemannmanifoldLangevinandHamiltonianMonteCarlo
methods”.In:JRStatistSocB73(2011),pp.123–214(cit.onp.1).
[26] P. J. Green and A. Mira. “Delayed rejection in reversible jump Metropolis–Hastings”. In:
Biometrika88.4(2001),pp.1035–1053(cit.onp.19).
[27] H. Haario, M. Laine, A. Mira, and E. Saksman. “DRAM: efficient adaptive MCMC”. In:
Statisticsandcomputing16(2006),pp.339–354(cit.onp.19).
[28] S.Habershon,D.E.Manolopoulos,T.E.Markland,andT.F.Miller.“Ring-polymermolecular
dynamics:Quantumeffectsinchemicaldynamicsfromclassicaltrajectoriesinanextended
phasespace”.In:AnnualReviewofPhysicalChemistry64.1(2013),pp.387–413.ISSN:0066-426X
(cit.onp.11).
[29] E. Hairer, C. Lubich, and G. Wanner. Geometric numerical integration. en. 2nd ed. Springer
SeriesinComputationalMathematics.Berlin,Germany:Springer,Feb.2010(cit.onp.3).
[30] Y. He, K. Balasubramanian, and M. A. Erdogdu. “On the ergodicity, bias and asymptotic
normality of randomized midpoint sampling method”. In: Advances in Neural Information
Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin.
Vol.33.2020,pp.7366–7376(cit.onp.4).
[31] M.D.HoffmanandA.Gelman.“Theno-U-turnsampler:Adaptivelysettingpathlengthsin
HamiltonianMonteCarlo”.In:JournalofMachineLearningResearch15.1(2014),pp.1593–1623
(cit.onpp.1,2,12,14,18,19).
21[32] J.KaipioandE.Somersalo.StatisticalandComputationalInverseProblems.Vol.160.Applied
MathematicalSciences.SpringerScience&BusinessMedia,2005(cit.onp.11).
[33] T.S.Kleppe.“AdaptivestepsizeselectionforHessian-basedmanifoldLangevinsamplers”.
In:ScandinavianJournalofStatistics43.3(2016),pp.788–805(cit.onpp.1,19).
[34] T.S.Kleppe.“Connectingthedots:NumericalrandomizedHamiltonianMonteCarlowith
state-dependenteventrates”.In:JournalofComputationalandGraphicalStatistics31.4(2022),
pp.1238–1253(cit.onp.1).
[35] R. Korol, J. L. Rosa-Raíces, N. Bou-Rabee, and T. F. Miller. “Dimension-free path-integral
molecular dynamics without preconditioning”. In: The Journal of Chemical Physics 152.10
(2020),p.104102(cit.onp.11).
[36] J.Lu,Y.Lu,andZ.Zhou.“ContinuumlimitandpreconditionedLangevinsamplingofthe
pathintegralmoleculardynamics”.In:JournalofComputationalPhysics423(2020),p.109788
(cit.onp.11).
[37] P. B. Mackenzie. “An improved hybrid Monte Carlo method”. In: Physics Letters B 226.3
(1989),pp.369–371(cit.onp.4).
[38] M.Magnusson,P.Bürkner,andA.Vehtari.posteriordb:asetofposteriorsforBayesianinference
andprobabilisticprogramming.https://github.com/stan-dev/posteriordb.Version0.5.
2023(cit.onp.15).
[39] T. F. Miller and D. E. Manolopoulos. “Quantum diffusion in liquid para-hydrogen from
ring-polymermoleculardynamics”.In:TheJournalofChemicalPhysics122.18(2005),p.184503.
ISSN:0021-9606(cit.onp.11).
[40] T. F. Miller III and C. Predescu. “Sampling diffusive transition paths”. In: The Journal of
ChemicalPhysics126.14(2007),p.144102.ISSN:0021-9606(cit.onp.11).
[41] A. Mira. “On Metropolis-Hastings algorithms with delayed rejection”. In: Metron 59.3-4
(2001),pp.231–241(cit.onp.19).
[42] C. Modi, A. Barnett, and B. Carpenter. “Delayed rejection Hamiltonian Monte Carlo for
samplingmultiscaledistributions”.In:BayesianAnalysis1.1(2023),pp.1–28(cit.onp.19).
[43] R.M.Neal.“MCMCusingHamiltoniandynamics”.In:HandbookofMarkovChainMonteCarlo
2(2011),pp.113–162(cit.onpp.4,9).
[44] L. R. Petzold, L. O. Jay, and J. Yen. “Numerical solution of highly oscillatory ordinary
differentialequations”.In:ActaNumer.6(1997),pp.437–483.ISSN:0962-4929(cit.onpp.9,
11).
[45] D.Phan,N.Pradhan,andM.Jankowiak.“Composableeffectsforflexibleandaccelerated
probabilisticprogramminginNumPyro”.In:arXivpreprint1912.11554(2019)(cit.onp.1).
[46] F.J.PinskiandA.M.Stuart.“Transitionpathsinmoleculesatfinitetemperature”.In:The
JournalofChemicalPhysics132.18(2010),p.184104(cit.onp.11).
[47] M.G.ReznikoffandE.Vanden-Eijnden.“Invariantmeasuresofstochasticpartialdifferential
equationsandconditioneddiffusions”.In:ComptesRendusMathematique340.4(2005),pp.305–
308(cit.onp.11).
[48] J.Salvatier,T.V.Wiecki,andC.Fonnesbeck.“ProbabilisticprogramminginPythonusing
PyMC3”.In:PeerJComputerScience2(2016),e55(cit.onp.1).
[49] R. Shen and Y. T. Lee. “The randomized midpoint method for log-concave sampling”. In:
AdvancesinNeuralInformationProcessingSystems32(2019)(cit.onp.4).
22[50] A.M.Stuart.“Inverseproblems:aBayesianperspective”.In:ActaNumerica19(2010),pp.451–
559(cit.onp.11).
[51] L.Tierney.“AnoteonMetropolis-Hastingskernelsforgeneralstatespaces”.In:TheAnnals
ofAppliedProbability8.1(1998),pp.1–9(cit.onp.23).
[52] P. A. Whalley, D. Paulin, and B. Leimkuhler. “Randomized time Riemannian manifold
HamiltonianMonteCarlo”.In:StatisticsandComputing34.1(2024),p.48(cit.onp.1).
[53] C. Wu, J. Stoehr, and C. P. Robert. “Faster Hamiltonian Monte Carlo by learning leapfrog
scale”.In:arXivpreprint1810.04449(2018)(cit.onpp.4,19).
A Proof of correctness
ThecorrectnessoftheGISTsamplergiveninFigure1followsfromtheobservationthattheGIST
sampler is a Gibbs sampler which interleaves Gibbs resampling of the velocity variable ρ and
tuningparameterαwithaMetropolis-within-Gibbsstepthatusesaproposalgivenbyameasure-
preservinginvolution. Infact,thisideaisalsotheessenceofthestandardHMCsampler,andthe
GISTsamplerisanaturalextensionofthisideatotheselectionoftuningparametersaswell.
Theproofofcorrectnessreliesonthefollowinggenerallemma. Thislemmaisnotnew;see,e.g.,
specialcase2ofTheorem2in[51]. However,acompleteproofisgivenforthereader’sconvenience.
Lemma 2. Suppose that (S ,F,λ) is a measure space, ν is a probability measure with strictly positive
density γ with respect to λ, and F : S → S is a measurable involution which preserves λ. Given a state
Z ∈ S,defineaMarkovtransitionkernelbythefollowingprocedure:
• DrawU ∼ uniform([0,1]).
(cid:16) (cid:17)
• Set Z′ = F(Z)ifU ≤ min 1, γ(F(Z)) and Z′ = Zotherwise.
γ(Z)
Then,thistransitionstepisreversiblewithrespecttoν.
ProofofLemma2. Let Z ∼ ν. Theproofshowsthat
P(Z ∈ A,Z′ ∈ B) = P(Z′ ∈ A,Z ∈ B) ,
(cid:16) (cid:17)
forallmeasurablesets A,B ∈ F. Let β(x,F(x)) = min 1, γ(F(x)) forall x ∈ S . Forall x ∈ S and
γ(x)
foranymeasureableset A ∈ F,thetransitionkernelassociatedtothetransitionstepdescribedin
Lemma2isgivenby
p(x,A) = β(x,F(x))δ (A)+(1−β(x,F(x)))δ (A) .
F(x) x
Hence,
(cid:90)
P(Z ∈ A,Z′ ∈ B) = p(x,B)ν(dx)
A
(cid:90)
= [β(x,F(x))1 (F(x))+(1−β(x,F(x)))1 (x)]ν(dx) (15)
B B
A
(cid:90)
= [β(x,F(x))1 (F(x))1 (x)+(1−β(x,F(x)))1 (x)1 (x)]ν(dx) . (16)
B A A B
S
23Thesecondtermin(16)isalreadysymmetricinthesets A,B—soweturnourattentiontothefirst
term. Forthisterm,weusetheelementaryidentity amin(1, b) = bmin(1, a)validforall a,b ̸= 0,
a b
asfollows
(cid:90)
β(x,F(x))1 (F(x))1 (x)ν(dx)
B A
S
(cid:90)
= β(x,F(x))1 (F(x))1 (x)γ(x)λ(dx)
B A
S
(cid:90) (cid:18) γ(F(x))(cid:19)
= 1 (F(x))1 (x)γ(x)min 1, λ(dx)
S B A γ(x)
(cid:90) (cid:18) γ(x) (cid:19)
= 1 (F(x))1 (x)γ(F(x))min 1, λ(dx) .
B A γ(F(x))
X
But,sincethemap Fpreservesλandisaninvolutionwehave,bychangeofvariablesunder F,
(cid:90) (cid:18) γ(x) (cid:19)
1 (F(x))1 (x)γ(F(x))min 1, λ(dx)
S B A γ(F(x))
(cid:90) (cid:18) γ(F(x)) (cid:19)
= 1 (F(F(x)))1 (F(x))γ(F(F(x)))min 1, λ(dx)
S B A γ(F(F(x)))
(cid:90) (cid:18) γ(F(x))(cid:19)
= 1 (x)1 (F(x))γ(x)min 1, λ(dx)
S B A γ(x)
(cid:90) (cid:90)
= β(x,F(x))1 (x)1 (F(x))ν(dx) = β(x,F(x))1 (F(x))ν(dx) . (17)
B A A
S B
Inserting(17)into(16),andcomparingwith(15),weobservethat
P(Z ∈ A,Z′ ∈ B) = P(Z ∈ B,Z′ ∈ A)
asrequired.
WithLemma2inhand,wearenowinpositiontoprovecorrectnessoftheGISTsampler.
ProofofTheorem1. Itisclearthattheiteratesθ ,θ ,... generatedbytheGISTsamplerinFigure1
0 1
formaMarkovchain,soweneedonlyestablishreversiblity. InthenotationofFigure 1,suppose
θ = θ ∼ µ . Afterselectingρ ∼ normal(0,I )andα ∼ p(· | θ ,ρ ),then(θ ,ρ ,α) ∼ µ .
0 θ 0 d×d 0 0 0 0 e
Letu ∼ uniform([0,1])andset
 (cid:16) (cid:17)
 p π(θ ,ρ )(α) | S◦F(α)(θ ,ρ )
 0 0 0 0
(θ∗ ,ρ∗ ,α∗) =  G(θ 0,ρ 0,α) ifu ≤ e−∆H(θ0,ρ0) p(α | θ ,ρ ) ,
0 0

 (θ ,ρ ,α) else .
0 0
SinceGisameasure-preservinginvolutionbyassumption,byLemma2wehave
P((θ ,ρ ,α) ∈ A,(θ∗ ,ρ∗ ,α∗) ∈ B) = P((θ∗ ,ρ∗ ,α∗) ∈ A,(θ ,ρ ,α) ∈ B) (18)
0 0 0 0
foranymeasurable A,B ⊂ R2d×A. Inparticular,forBorelsets A˜,B˜ ⊂ Rd,
P(θ ∈ A˜,θ∗ ∈ B˜) = P(θ∗ ∈ A˜,θ ∈ B˜)
24by taking A = A˜ ×Rd ×A and B = B˜ ×Rd ×A in (18). Hence, the GIST sampler produces a
MarkovChainthatisreversiblewithrespecttoe−U(x)m(dθ).
Asacorollary,Theorem1impliescorrectnessoftheGISTsamplerspresentedinSections3.1and
3.2,aswellastheGISTsamplerinSection3.3.
Corollary3. Ifπ(θ,ρ)(α) = αforall(θ,ρ,α) ∈ R2d×A,thecorrespondingGISTsampleriscorrect.
Proof. We need only verify the map G : R2d × A → R2d × A defined by G(θ,ρ,α) = (S ◦
F(α)(θ,ρ),α) is a measure-preserving involution. As S ◦ F(α) is an involution G is automati-
callyaninvolutiononR2d×A. AdditionallyS ◦F(α)preservesLebesguemeasuremonR2d for
everyfixedα.
Hence,byFubini’stheorem,Gpreserves(m⊗η)onR2d×A.
Thus,iteratingthetransitionstepinFigure1producesaMarkovchainreversiblewithrespecttoµ
θ
byTheorem1.
ThenexttwocorollariesofTheorem1showthecorrectnessoftheGISTsamplersinSection3.4and
Section3.5.
Corollary4. TheNo-U-TurnSamplerpresentedinSection3.4iscorrect.
Proof. Inthiscase,themapGisoftheformG(θ,ρ,J,i) = (S◦Φi(θ,ρ),−(J−i),i). Thisisclearly
h
aninvolution. Moreover,forθ,ρfixedthemap(J,i) → (−(J−i),i)isabijectionandthuspreserves
thecountingmeasureonP ×Z . ByFubini’stheoremGthenpreserves(m⊗η)onR2d×P ×Z .
F F
Thus,byTheorem1,wegetcorrectnessofNUTSaspresentedinSection3.4.
Corollary5. TheApogee-to-ApogeePathSamplerpresentedinSection3.5iscorrect.
(cid:16) (cid:17)
Proof. Here,wetakeG(θ,ρ,c,i) = S ◦Φi(θ,ρ),c+S ((θ,ρ),S ◦Φi(θ,ρ)),i . Sincebydefinition
h # h
S ((θ,ρ),(θ′,ρ′)) = −S ((θ′,ρ′),(θ,ρ)),weobservebydirectcomputationthatGisaninvolution.
# #
Forfixed(θ,ρ,i),themapc (cid:55)→ c+S ((θ,ρ),S◦Φi(θ,ρ))isabijectionandhencepreservescounting
# h
measure. Additionally,forifixed,θ,ρ (cid:55)→ S ◦Φi(θ,ρ)preservesLebesguemeasure.
h
ApplyingFubini’stheoremthenimpliesthatthemapGismeasurepreserving. Hence,byTheorem
1,theApogee-to-ApogeePathSamplerpresentedinSection3.5iscorrect.
Remark 5. In the description above, the proposal G(θ ,ρ ,α) has phase space state given by S ◦
0 0
F(α)(θ ,ρ ) whereas the proposal in the original algorithm is F(α)(θ ,ρ ). This difference is
0 0 0 0
inconsequential since the acceptance probabilities are the same and the map S affects only the
velocity variable. Since this variable is immediately discarded in the next step of the chain, we
disregardthisdifference. Equivalently,onecouldadaptthedescriptionabovesothatS isapplied
tothephasespacecoordinatesimmediatelyafterMetropolizingtheproposal. SinceS preserves
theextendedtargetdistributionµ ,thewholeprocedurewillstillpreservetheextendedtarget.
e
25A.1 ReductiontostandardMetropolis-Hastings
It is worth noting that the above yields the original Metropolis-Hastings algorithm as a special
case. Let (X,F,λ) be a measure space with σ-finite measure λ. Suppose µ(dx) = p (x)λ(dx) is
0
anabsolutelycontinuoustargetprobabilitymeasureand P(x,dy) = p(x,y)λ(dy)isanabsolutely
continuoustransitionkernelwithstrictlypositivedensities. Analogouslyto(2),definethefollowing
enlargedtargetmeasureon X×X:
µ (dxdy) = γ(x,y)λ(dxdy) = p (x)p(x,y)(λ⊗λ)(dxdy) .
e 0
Notethatthemarginalofµ inthefirstcomponentgivesthedesiredtarget.
e
On X×X,andfromastatewithfirstcomponent x ∈ X,firstgenerateasamplefromthesecond
componentyconditionalonthefirst,i.e.,
y ∼ p(x,·) .
Then,applytheMetropolisproceduretotheinvolutiveproposalϕ : (x,y) (cid:55)→ (y,x)whichclearly
preservesλ⊗λ. AccordingtoLemma2,thecorrespondingacceptanceprobabilityisgivenby
(cid:18) (cid:19) (cid:18) (cid:19)
γ(ϕ(x,y)) p (y)p(y,x)
a (x,y) = 1∧ = 1∧ 0 ,
e γ(x,y) p (x)p(x,y)
0
whichwerecognizeasthestandardMetropolis-Hastingsacceptanceprobability.
B Test models
In this section, we briefly describe the models used for evaluations. Greek letters are used for
parameters,romanlettersforconstants,predictorsandmodeleddata,anditalicsforindexes. Where
nototherwisespecified,parametershaveweaklyinformativepriorsconcentratedontheirrough
scale.
Standardnormal A500-dimensionalstandardnormal,withα ∼normal(0,I),whereIisa500×500identity
matrix.
Correlatednormal A250-dimensionalcorrelatednormalα ∼normal(0,S),wherethecovarianceisthatof
aunitscalefirst-orderrandomwalk,S =r|i−j|,withcorrelationr=0.9.
i,j
Ill-conditionednormal A 250-dimensional ill-conditioned normal, with α ∼ normal(0,diag-matrix(σ)),
(cid:104) (cid:105)⊤
whereσ = 1 2 ···250 .
250 250 250
Poissongeneralizedlinearmixedmodel Theexpectedcountforobservationiisλ =exp(α+β ·t +β ·
i 1 i 2
t2+β ·t3+ε ),withahierarchicalprior ε ∼ normal(0,σ),andalikelihood y ∼ Poisson(λ ),for
i 3 i i i i i
i <40.
Eight-schoolsmeta-analysis Observationsofmeaneffectsy andstandarderrorsσ inschooljareusedto
j j
estimateaschooleffectθ withlikelihoody ∼normal(θ ,σ)andanon-centeredparameterizationof
j j j j
thepriorθ ∼normal(µ,τ).
j
OrderKautoregressive AK-thorderautoregressivemodelwithlikelihoody
t
∼normal(α+[y t−1y t−2···y t−K]·
β,σ).Thepriordoesnotrestrictcoefficientvectors β toensurestationary. Ahistoryof K = 5and
T =200timepointsisusedintheevaluations.
26Autoregressivemovingaverage(order1,1) Anautoregressivetimeserieswithfirstelementy ∼normal(µ+
1
ϕ·µ,σ),andsubsequentelementsy
t
∼normal(µ+ϕ·y t−1+θ·ϵ t−1,σ)fort >1.
Generalizedautoregressiveconditionalheteroskedasticity An autoregressive time series model with
(cid:113)
stochasticvolatility,withy
t
∼normal(µ,σ t),whereσ
1
= sisgivenasdataandσ
t
= α 0+α 1·(y t−1−µ)2+β 1·σ t2 −1.
HiddenMarkovmodel AhiddenMarkovmodel(HMM)withnormalobservations. Thedatagenerating
processisMarkovianinhiddenstate,z ∼ categorical(ϕ ),andthennormalinobservation,y ∼
t zt−1 t
normal(µ ,σ ).Intheimplementation,theforwardalgorithmisusedtomarginalizethez tocalculate
zt zt t
thelikelihood. Thevectorµisconstrainedtoascendingorderforidentifiability.
Hierarchicalitem-responsetheory2parameterlogistic Fortestitemi. astudent j’scorrectnessisgener-
ated by y ∼ bernoulli(logit−1(α ·(θ −β ))), for ability θ, difficulty β, and discrimination α. For
i,j i j i
identifiability, β has a standard hierarchical prior, β ∼ normal(µβ,σβ), whereas the prior for α is
i
centered,α ∼normal(0,σα),andthepriorforθisfixedtounitscale,θ ∼normal(0,1).
i
Normalmixture A normal mixture model with z ∼ bernoulli(θ), and y ∼ normal(µ ,σ ), with µ
n n zn zn
constrainedtoascendingorderforidentifiability. Theresponsiblityparameterz ismarginalizedout.
n
Lotka-Volterrapopulationdynamics Alognormalmodelofpopulationtimeseriesforprey(y )andpreda-
t,1
tor(y ).Thepopulationdynamicsismodeledbyasystemofordinarydifferentialequations, du(t) =
t,2 dt
(α−β·v(t))·u(t)and dv(t) = (−γ+δ·u(t))·v(t)withunknownstartingpoint(u(0),v(0))and
dt
discreteobservationsmodeledby y ∼ lognormal(logu(t),σ ) and y ∼ lognormal(logv(t),σ ),
t,1 1 t,2 2
whereu(t)andv(t)aresolutionstotheODE.
Pharmacometric/pharmacodynamicmodel Aone-compartmentPK/PDeliminationmodelwithMichaelis-
Mentindynamicsdefinedbyadifferentialequationforconcentrationwithdosing(D)of dC = ν ·
dt V
C(t) −exp(−δ·t)·D· δ,withlognormallikelihoodfordiscreteobservationsy ∼lognormmal(logC(t),σ),
µ+C(t) V t
withallparametersconstrainedtobepositive.
27