1
PHLP: Sole Persistent Homology for Link
Prediction - Interpretable Feature Extraction
Junwon You, Eunwoo Heo, Jae-Hun Jung
Abstract— Link prediction (LP), inferring the connectivity GNN-based Method Proposed Method
between nodes, is a significant research area in graph data,
where a link represents essential information on relationships
between nodes. Although graph neural network (GNN)-based
models have achieved high performance in LP, understanding
why they perform well is challenging because most comprise
complex neural networks. We employ persistent homology (PH),
a topological data analysis method that helps analyze the topo-
logicalinformationofgraphs,toexplainthereasonsforthehigh
performance. We propose a novel method that employs PH for Extracted feature vector Extracted feature vector
LP (PHLP) focusing on how the presence or absence of target Classifier Classifier
linksinfluencestheoveralltopology.ThePHLPutilizestheangle
hop subgraph and new node labeling called degree double radius Fig. 1. Difference between the GNN-based and proposed methods. (Left)
node labeling (Degree DRNL), distinguishing the information of TheGNN-basedmethodextractsfeaturevectorsthroughoptimization(dashed
graphsbetterthanDRNL.Usingonlyaclassifier,PHLPperforms area), making it difficult to interpret what these vectors represent. (Right)
similarly to state-of-the-art (SOTA) models on most benchmark The proposed method extracts feature vectors through the designed analysis
datasets. Incorporating the outputs calculated using PHLP into process,resultingininterpretablevectors.
the existing GNN-based SOTA models improves performance
across all benchmark datasets. To the best of our knowledge,
PHLP is the first method of applying PH to LP without GNNs. to heuristic [3], [14]–[18] and embedding methods [19]–[22],
The proposed approach, employing PH while not relying on GNN-based models have achieved significant score improve-
neural networks, enables the identification of crucial factors for ments in capturing intricate relationships within graphs [23]–
improving performance.
[28].
Index Terms—Graph analysis, link prediction, persistent ho- However, GNN-based methods are comprised of neural
mology, topological data analysis.
networks, making it challenging to understand the reasons
for their performance. To explore these reasons, we employ
I. INTRODUCTION persistent homology (PH), a mathematical tool in topological
GRAPH data pervade numerous domains such as social data analysis (TDA) that enables the inference of topolog-
ical information regarding the manifold approximating the
networks, biological systems, recommendation engines,
data [29], [30] by quantifying the persistence of topological
and e-commerce networks [1], [2]. The graph is well-suited
features across multiple scales. Various research has had
for modeling complex real-world relationships.
successfuloutcomesinapplyingPHtographclassificationand
Predictingmissingorpotentialconnectionswithinagraphis
node classification tasks [31]–[40]. In contrast, relatively few
essentialformanyapplications,unlockingvaluableinsightand
studies have explored using PH for LP. The topological loop-
facilitating intelligent decision-making. The ability to predict
counting (TLC) GNN [27] is a notable example that uses PH.
futurenetworkinteractionscanbeappliedtodiversedomains,
The TLC-GNN injects topological information into a GNN,
including friend recommendations on social networks [3]–[5],
and experiments were conducted on benchmark data where
knowledgegraphcompletion[6],[7],identificationofpotential
node attributes are available.
drug-protein interactions in bioinformatics [8], [9], prediction
In this context, as illustrated in Fig. 1, we present a novel
proteininteractions[9]–[11],andoptimizationofsupplychain
approachtoLP,calledPHLP,whichcalculatesthetopological
logistics [12], [13].
information of a graph. To use the topological information of
The link prediction (LP) problem has been categorized
subgraphsforLP,wemeasurehowthetopologicalinformation
into three major paradigms: heuristic methods, embedding
changes depending on the existence of the target link, as
methods, and graph neural network (GNN)-based methods,
illustrated in Fig. 2. To extract topological information from
whichareexploredindetailinSectionII.Recently,compared
various perspectives, we utilize angle hop subgraphs for each
JunwonYouandEunwooHeoareco-firstauthors. target node. Additionally, we propose new node labeling
called degree double radius node labeling (Degree DRNL),
Junwon You, Eunwoo Heo, and J.-H. Jung are with the Department of
which incorporates degree information for each node, using
Mathematics, Pohang University of Science and Technology (POSTECH),
Pohang,SouthKorea.J.-H.JungisalsowiththeGraduateSchoolofArtificial DRNL [24].
Intelligence,POSTECH,Pohang,SouthKorea. The contributions are summarized as follows:
Preprint.Underreview. • We develop an explainable LP method, PHLP, that em-
4202
rpA
32
]GL.sc[
1v52251.4042:viXra2
subgragh PI
Fig.2. Topologicalfeaturesinsubgraphswithandwithoutatargetlink(u,v).Thediagramillustratesthetopologicalinformationextractionprocessforthe
subgraphN,asdescribedinSectionIII-D.Thepresence(top)orabsence(bottom)ofthetargetlinkchangesthetopologicalstructureofthegraph.Toprow:
Whenthetargetlinkisconnected,threefeatures(C1,C2,andC3)aredetectedshowninthepersistenceimage(PI)intherightcolumn.ThePIrepresentsthe
topologicalfeaturesofthesubgraphN (SectionIII-E).Bottomrow:Whenthetargetlinkisabsent,onlytwofeatures(C2 andC3)aredetectedasdepicted
inthecorrespondingPI.
ploys the topological information for LP through PH Embedding methods are advantageous due to their applica-
without relying on neural networks, as illustrated in bility regardless of the data characteristics using optimization.
Fig. 1. Noderepresentationscaptureglobalpropertiesandlong-range
• We demonstrate that the proposed method, even with a effects through the learning process. However, these methods
simple classifier such as a multilayer perceptron (MLP), often require significantly large dimensions to express basic
can achieve LP performance close to that of state-of-the- heuristics,resultinginlowerperformancethanheuristicmeth-
art (SOTA) models. This method surpassed the SOTA ods[41].Moreover,inembeddingmethods,Ribeiroetal.[42]
performance for the Power dataset. explainedthattwonodeswithsimilarneighborhoodstructures
• Werevealthatmerelyincorporatingvectorscomputedby may have vastly different embedded vectors, especially when
PHLP into existing LP models, including SOTA models, theyarefarapartinthegraph,leadingtoincorrectpredictions.
can improve their performance. GNN-Based Methods. The GNN has become a pivotal ap-
• Tothebestofourknowledge,theproposedmethodusing proach to LP due to its ability to grasp graph-structured
PH without a GNN is the first to achieve performance data.Byeffectivelyincorporatinglocalandglobalinformation
close to that of SOTA models. throughmessagepassingandgraphaggregationlayers,GNNs
enhance LP performance. The model by Zhang et el. [24]
II. RELATEDWORK uses subgraphs as the primary structural units to learn and
A. Link Prediction predictconnections,resultinginsignificantimprovement.This
paradigmshiftledtoresearchfocusingonrefiningandadvanc-
Heuristic Methods. Heuristic-based approaches to LP com-
ingsubgraphmethodsinthecontextofGNNs[25],[26],[28].
pute the predefined structural features within the observed
Followingthistrend,Panetal.[28]proposedWalkPool(WP),
nodes and edges of the graph. Classic methods, such as com-
anewpoolingmechanismthatusesattentiontojointlyencode
monneighbors[3],Adamic-Adar[3],Jaccardcoefficient[14],
node representations and graph topology into learned topo-
and preferential attachment [15], rely on simple heuristics
logical features. However, despite their superior performance,
that capture certain aspects of node relationships. Zhou et
GNN-based methods pose a challenge in comprehending the
al. [16] proposed a local random walk method, whereas Jeh
underlying mechanisms driving their predictions. Within this
and Widom [18] developed SimRank to quantify similarity
context,wedevelopthePHLP,basedonPH,withperformance
based on the structural context. Although heuristic methods
comparable to GNN-based models.
provideapreliminaryunderstandingofLP,theyarelimitedby
their inability to capture complex relationships within graphs.
B. Persistent Homology on Graph Data
Furthermore, heuristic methods are effective only when the
defined heuristics align with the graph structure; therefore, In recent years, PH, a method of analyzing the topological
applying heuristic methods across all graph datasets can be features of data, has been widely used to analyze graph data.
challenging. It has demonstrated its effectiveness in graph classification
Embedding Methods. Embedding methods map nodes from tasks by analyzing the topology of graphs [31]–[38] and has
thegraphintoalow-dimensionalvectorspacewheregeometric been applied to node classification tasks [31], [39], [40].
relationships mirror the graph structure. Koren et al. [19] However, its suitability for LP tasks has been limited, and
demonstrated the power of matrix factorization for collabora- research on applying PH for LP has progressed slowly. Yan
tive filtering. Perozzi et al. [20] introduced DeepWalk, using et al. [27] proposed an intriguing approach by integrating PH
random walks to generate node sequences and employing the with GNNs. While their model demonstrates the potential of
skip-gram model to produce embeddings. Tang et al. [22] PHforcapturingtopologicalfeaturesofgraphdata,itrelieson
developedlarge-scaleinformationnetworkembedding(LINE), GNN structures. Additionally, the TLC-GNN requires further
which preserves local and global structures. Grover and research on datasets without node attributes.
Leskovec [21] further advanced this approach with Node2Vec Although PH has demonstrated success in graph and node
(N2V), proposing a flexible notion of the neighborhood to classification tasks, its filtration technique, tailored to analyz-
capture diverse node relationships. ing the entire graph structure, might not be optimal for LP3
(a) -PHLP with Classifier (b) MA - PHLP with Classifier
PHLP
MA-PHLP
-PHLP
-PHLP
: Node labeling & : Edge-weight function
CONCAT
CONCAT -PHLP
: MLP Classifier
Fig.3. Overallstructureofpersistenthomologyforlinkprediction(PHLP)andmultianglePHLP(MA-PHLP).(a)PHLPcalculatesthetopologicalinformation
basedontheexistenceoftargetlinksinanglehopsubgraphsforeachtargetnode.(b)Withaclassifier,MA-PHLPintegratestopologicalinformationacross
variousanglestoperformLP.
as the role of each node in LP differs from that in graph C. Filtration of the Subgraph
or node classification tasks. To address this challenge and
For a given subgraph, the Rips filtration [43]–[45] is em-
advanceresearchinLP,wedevelopafiltrationmethodtailored
ployed to calculate the topology using PH. To apply the
explicitly to LP tasks.
Rips filtration, we define an edge-weight function using node
III. METHOD labeling that reflects the topology of the given graph.
A. Outline of the Proposed Methods
We propose (a) PHLP and (b) multiangle PHLP (MA-
PHLP)asdescribedinFig.3.ThePHLPmethodanalyzesthe
topological structure of the graph, focusing on target links.
First,PHLPsamplesa(k,l)-anglehopsubgraphforthegiven
targetnodes(SectionIII-B).Then,PHLPcomputespersistence
images (PIs; Section III-E) for cases with and without the (a)DRNL
target link. To calculate PIs, we introduce the node labeling
and define the edge-weight function (Section III-C). Through
PHLP,eachtargetnodeistransformedintoavectorcomprising
PIs. In addition, LP is performed using the calculated vectors
with a classifier (Section III-F). To reflect diverse topological
information, we also propose MA-PHLP, which analyzes data
from various angles (Section III-G).
(b)DegreeDRNL
B. Extracting Angle Hop Subgraph
Fig.4. Nodelabelingongraphs.(a)Nodelabelvalueswithoutconsidering
GivenagraphG=(V,E)andtwonodesu,v ∈V,ak-hop thegraphstructurecannotdistinguishbetweenG1 andG2 usingDRNL.(b)
enclosing subgraph for (u,v) is defined as Nk = (V′,E′)
Applying Degree DRNL allows G1 and G2 to be distinguished solely by
u,v nodelabelvalues.
such that
V′ ={z ∈V |d(u,z)≤k or d(z,v)≤k},
E′ ={(z,w)∈E |z ∈V′ and w ∈V′},
where d(z,w) is the minimum number of edges in any
path from z to w in G. We define a (k,l)-angle hop en-
closing subgraph, where the term “angle” signifies viewing
the subgraph from multiple perspectives. The (k,l)-angle hop
subgraph is a generalization of the k-hop subgraph. Given a
graph G=(V,E) and two nodes u,v ∈V, a (k,l)-angle hop (a)DRNL (b)DegreeDRNL
enclosing subgraph for (u,v) is defined as N u(k ,v,l) =(V′,E′) Fig.5. Persistenceimages(PIs)fortwonodelabelingmethodsforthegraphs
such that inFig.4.(a)DRNLexhibitsidenticalzero-dimensionalPIsforG1andG2,(b)
DegreeDRNLproducesdistinctoutcomes,effectivelydistinguishingbetween
V′ ={z ∈V |d(u,z)≤k or d(z,v)≤l}, thetwo.
E′ ={(z,w)∈E |z ∈V′ and w ∈V′}.
Degree DRNL. Zhang et al. [24] introduced DRNL, which
Thus, the angle hop can generate subgraphs in various forms, computes the distance from any node to two fixed nodes. For
providing flexibility to adapt to various graph characteristics. any subgraph N = (V′,E′) of G and two nodes a,b ∈ V′,4
theDRNLf(a,b) :V′ →Nbasedon(a,b)ofGforanyvertex E. Persistence Image
drnl
w in V′, is defined as
We convert the persistence diagram into a PI [46]. For a
f(a,b)(w)=1+min(d(w,a),d(w,b))+q (q +r −1), given persistence diagram D, consider a linear transform L :
drnl w w w R2 → R2 defined by L(x,y) = (x,y −x). The image set
where q ∈ Z and r ∈ {0,1} are integers representing of D under this transformation is denoted as L(D). For each
w w
the quotient and remainder, respectively, such that d(w,a)+ point (b,d′) in L(D), a weight function ϕ (b,d′) : R2 → R is
d(w,b)=2q +r . We call these two nodes, a and b, center defined that assigns a weight to each point in the persistence
w w
nodes. These center nodes do not need to be the target nodes diagram.Acommonchoiceforϕ (b,d′)istheGaussianfunction
used when extracting the subgraph. centered at (b,d′). The nonnegative function is defined as h:
However, DRNL encounters limitations when the graph R2 →R, as h(x,y)=1/log(1+|y|). The function h is zero
is transformed into node-label information. As depicted in along the horizontal x-axis, and is continuous and piecewise
Fig. 4a, DRNL assigns the same node labels to different differentiable, satisfying the conditions presented in [46]. The
graphs, resulting in identical zero-dimensional PIs (Fig. 5a, persistence surface ρ D :R2 →R is defined as
Section III-E). To incorporate the local topology of each node (cid:88)
ρ (z)= h(b,d′)ϕ (z).
withtheeffectsofDRNL,weintroducedDegreeDRNL.Fora D (b,d′)
givensubgraphN =(V′,E′)ofGandcenternodesa,b∈V′, (b,d′)∈L(D)
the Degree DRNL f(a,b) : V′ → R based on (a,b), for all The continuous surface ρ D is discretized into a finite-
degdrnl
vertex w in V′, is defined as dimensional representation over a predefined grid. This grid
consists of n cells, each corresponding to a specific region
M −deg(w)
f(a,b) (w)=f(a,b)(w)+ , in the plane. The PI is defined as an array of values I(ρ D) p
degdrnl drnl M for each cell p. Each I(ρ ) in this array is computed by
D p
where M denotes the maximum degree of nodes in N. The integrating the persistence surface ρ over the area of cell p:
D
(M−deg(w))/M term above assigns larger values for lower (cid:90)(cid:90)
degrees of w. When M = deg(w), the value of Degree I(ρ D) p = ρ Ddydx.
DRNL matches the original DRNL, ensuring that the edges p
connected to nodes with higher degrees are assigned smaller
F. Predicting the Existence of the Target Link
values, promoting their earlier emergence in the filtration.
Forthegiventargetnodes(u,v),wesamplethe(k,l)-angle
Fig. 4b demonstrates various node labels obtained using De-
gree DRNL, resulting in PIs that can be distinguished from hopsubgraphN u(k ,v,l),denotedasN−(SectionIII-B),assuming
each other (Fig. 5b). that the target link does not exist during this process. On this
Edge-weight function. For a given subgraph N = (V′,E′), subgraph, we extract topological features by calculating PH
f : V′ → N denotes any node labeling function. The edge- anditsvectorization(i.e.,thePI,asdescribedinSectionsIII-D
weight function W : E′ → R, for any edge (w,z) in E′, is and III-E). The vectorization is calculated for each dimension
defined as and concatenated. If k ̸=l, for symmetry, we repeat the same
process with the (l,k)-angle hop subgraph once and consider
min(f(w),f(z))
W(w,z)=max(f(w),f(z))+ . the average of the two vectors, denoting this vector as x−.
max(f(w),f(z))
To observe the difference in topological features, we consider
The min/max term in the definition of W refines values a subgraph N+ obtained by connecting the target link to
further, enhancing the discriminative power by reducing the N−. For this graph, x+ denotes the vector obtained using
occurrence of identical edge weights. this method.
To predict the existence of the target link with the vectors
x− andx+,weemployanMLPclassifierΦ:R2(d+1)n2 →R
D. Persistent Homology
where n represents the resolution of the PI, and d denotes the
Given an edge-weighted subgraph N = (V′,E′,W), we maximaldimensionofPH.Themodelpredictstheexistenceof
constructaRipsfiltrationandcomputeitsPH.First,wecreate alinkbetweentwotargetnodeswiththefollowingprobability:
asequenceofsubgraphs{N ϵ} ϵ∈R,whereeachN
ϵ
=(V′,E ϵ′)
z =σ(Φ(x)),
and E′ = {e ∈ E | W(e) ≤ ϵ}. Second, we convert each uv
ϵ
subgraph N into the Rips complex K = {τ ∈ X | (w,z) ∈ where x is the concatenation of x− and x+, and σ is the
ϵ ϵ
E′ for any two vertices w,z ∈ τ}, where X is the power set activation function. For the training dataset X ⊆ V × V,
ϵ
of V′. In K , a simplex τ is formed when the vertices in comprising positive and negative links corresponding to the
ϵ
τ are pairwise connected by edges in N . Then, the Rips elements of E and (V ×V)\E, respectively, we define the
ϵ
filtration is obtained as K (cid:44)→ K (cid:44)→ ··· (cid:44)→ K = X for loss function as follows:
ϵ1 ϵ2 ϵm
ϵ 1 ≤ ϵ 2 ≤ ··· ≤ ϵ m. Third, we compute the p-dimensional (cid:88)
BCE(z ,y ),
homologygroupH (K )foreachcomplexK andtrackhow uv uv
p ϵ ϵ
these groups change as ϵ increases. The persistence diagram (u,v)∈X
D [45] comprises persistence pairs (b,d) representing the ϵ where BCE(·,·) represents the binary cross-entropy loss and
valuesatwhichahomologicalfeatureappearsbanddisappears y denotes the label of the target link (u,v), which is 0 for
uv
d, respectively, in the filtration. negative links or 1 for positive links.5
G. Multiangle PHLP heuristic methods include the Adamic-Adar (AA) [3], Katz
index (Katz) [48], PageRank (PR) [49], Weisfeiler-Lehman
The MA-PHLP maximizes the advantages of PHLP by
graph kernel (WLK) [50], and Weisfeiler-Lehman neural
examining data from various angles through the extraction of
machine (WLNM) [51]. For the embedding-based methods,
subgraphsbasedonahyperparameter,themaximumhop(max
we applied N2V [21], spectral clustering (SPC) [52], matrix
hop, denoted as H). The types of angles are elements of all
combinations of k and l within the set {(k,l) ∈ Z2|0 ≤ l ≤ factorization(MF)[19],andLINE[22].Moreover,SEAL[24]
and WP [28] represent the GNN-based methods.
k ≤ H,k > 0}. If we define the prediction probability of a
Datasets.Inlinewithpreviousstudies[24]and[28],weeval-
PHLP for each type of angle hop as z for i = 1,2,...,N,
i
then MA-PHLP predicts the likelihood of the link existence
with the following probability: TABLEI
STATISTICSOFTHEDATASETS
N
(cid:88)
p= α iz i, Dataset #Nodes #Edges Avg.nodedeg. Density
i=1 USAir 332 2126 12.81 3.86e-2
where α = (α ,...,α ) ∈ RN is a trainable parameter. We NS 1589 2742 3.45 2.17e-3
1 N PB 1222 16714 27.36 2.24e-2
apply the softmax function to the parameter α to ensure that Yeast 2375 11693 9.85 4.15e-3
the sum of all elements equals 1. Moreover, MA-PHLP is C.ele 297 2148 14.46 4.87e-2
Power 4941 6594 2.67 5.40e-4
trained using the binary cross-entropy loss.
Router 5022 6258 2.49 4.96e-4
E.coli 1805 15660 16.24 9.61e-3
H. Hybrid Method
uatetheperformanceofourMA-PHLPontheeightdatasetsin
The proposed approach easily integrates with existing sub-
TableIwithoutnodeattributes:USAir[53],NS[54],PB[55],
graphmethods.SubgraphmethodstreattheLPtaskasabinary
Yeast [56], C. elegans (C. ele) [57], Power [57], Router [58],
classification problem comprising two components: a feature
and E. coli [59]. The detailed statistics for each dataset are
extractorF andclassifierP.VectorswithPHinformationcal-
summarized in Table I.
culated using the proposed methods are incorporated through
Implementation Details. All edges in the datasets were split
concatenationbeforetheclassifier.Thedetailedprocessofthe
into training, validation, and testing datasets with proportions
hybrid method is outlined as follows:
of0.85,0.05,and0.1,respectively,ensuringafaircomparison
1) SubgraphExtraction:ForthegivengraphGandtarget
with previous studies. The max hop M was set to 3 for most
nodes (u,v), k-hop subgraph Nk is extracted.
u,v datasets (Table II). However, for the E. coli dataset, it was
2) Feature Extraction: Existing methods extract features
reduced to 2 when employing one-dimensional homology due
Z =F(Nk ) from the subgraph.
u,v to memory constraints. Conversely, for the Power dataset, the
3) Persistent Image Calculation: The methods described
max hop was set to 7 because it does not demand heavy
in Sections III-C, III-D, and III-E are applied to Nk ,
u,v memory and computation time. The sigmoid function was
where I denotes the PI vector. An MLP Φ : Rm →
employed for the activation function of the PHLP classifier.
Rn transforms the PI into a format similar to Z. For
Tables III and IV present the results of the hybrid methods
the hybrid method of MA-PHLP, Nk is replaced with
u,v using SEAL [24] and WP [28], respectively. For these exper-
multiangle subgraphs, concatenating their PI vectors.
iments, a two-layer MLP was used for the MLP Φ in Step
4) Classification:Next,α Z andα Φ(I)areconcatenated,
1 2 3 of Section III-H. We set the k-hops following the original
where α and α are trainable parameters. The softmax
1 2 methods, SEAL and WP, and the max hops M of MA-PHLP
function is applied to the parameter α = (α ,α ),
1 2 were set as the k, except for the Power dataset. Forthe Power
ensuring that the sum of elements equals 1, denoted
dataset, we set the k-hop to 1-hop and max hop M to 7,
by J. This concatenated vector is classified using the
respectively, which is discussed in detail in Section IV-D.
existing method’s classifier, P(J).
B. Results
IV. EXPERIMENTS
Results of MA-PHLP. Table II presents the AUC scores
This section evaluates the performance of MA-PHLP. The
for each model on the benchmark datasets. Bold marks the
experimentswerealsoconductedusingonlyzero-dimensional
best results, and underline indicates the second-best results.
homology (MA-PHLP (dim0)). We used the area under the
The results of AA, Katz, WLK, WLNM, N2V, SPC, MF,
curve (AUC) [47] as an evaluation metric. We repeated all
LINE,andSEALarecopiedfromSEAL[24]forcomparison.
experiments 10 times and reported the mean and standard
The MA-PHLP demonstrates high performance across most
deviation of the AUC values.
datasets, achieving competitive scores. The proposed model
outperforms several baselines, falling between the SEAL and
A. Experimental Settings
WPmodelsintermsoftheAUCscore.Notably,forthePower
Baselines. To evaluate the effectiveness of PHLP, we com- dataset,MA-PHLPachievesthehighestAUCscore,indicating
pared the proposed model with five heuristic methods, four its effectiveness in capturing link patterns.
embedding-based methods, and two GNN-based models. The Results of Hybrid Methods. Simply concatenating the PI6
TABLEII
LINKPREDICTIONPERFORMANCEMEASUREDBYTHEAUCONBENCHMARKDATASETS(90%OBSERVEDLINKS)
Dataset USAir NS PB Yeast C.ele Power Router E.coli
AA 95.06±1.03 94.45±0.93 92.36±0.34 89.43±0.62 86.95±1.40 58.79±0.88 56.43±0.51 95.36±0.34
Katz 92.88±1.42 94.85±1.10 92.92±0.35 92.24±0.61 86.34±1.89 65.39±1.59 38.62±1.35 93.50±0.44
PR 94.67±1.08 94.89±1.08 93.54±0.41 92.76±0.55 90.32±1.49 66.00±1.59 38.76±1.39 95.57±0.44
WLK 96.63±0.73 98.57±0.51 93.83±0.59 95.86±0.54 89.72±1.67 82.41±3.43 87.42±2.08 96.94±0.29
WLNM 95.95±1.10 98.61±0.49 93.49±0.47 95.62±0.52 86.18±1.72 84.76±0.98 94.41±0.88 97.21±0.27
N2V 91.44±1.78 91.52±1.28 85.79±0.78 93.67±0.46 84.11±1.27 76.22±0.92 65.46±0.86 90.82±1.49
SPC 74.22±3.11 89.94±2.39 83.96±0.86 93.25±0.40 51.90±2.57 91.78±0.61 68.79±2.42 94.92±0.32
MF 94.08±0.80 74.55±4.34 94.30±0.53 90.28±0.69 85.90±1.74 50.63±1.10 78.03±1.63 93.76±0.56
LINE 81.47±10.71 80.63±1.90 76.95±2.76 87.45±3.33 69.21±3.14 55.63±1.47 67.15±2.10 82.38±2.19
SEAL 97.10±0.87 98.25±0.61 95.07±0.39 97.60±0.33 89.54±1.23 86.21±2.89 95.07±1.63 97.57±0.30
WP 98.20±0.57 99.12±0.45 95.42±0.25 98.21±0.17 93.30±0.91 92.11±0.76 97.15±0.29 98.54±0.19
MA-PHLP 97.10±0.69 98.88±0.45 95.10±0.26 97.98±0.22 90.33±1.16 93.05±0.45 96.30±0.43 97.64±0.20
MA-PHLP(dim0) 97.10±0.73 98.78±0.65 95.06±0.28 97.98±0.23 89.88±1.22 93.37±0.41 96.37±0.43 97.72±0.17
TABLEIII TABLEV
AUCSCORESFORSEALWITHANDWITHOUTTDAFEATURES AUCSCORESFORMA-PHLP(DIM0)BYNODELABELING
Dataset SEAL MA-PHLP+SEAL Dataset DRNL DegreeDRNL
USAir 97.10±0.87 97.41±0.62 USAir 96.73±0.64 97.10±0.73
NS 98.25±0.61 98.97±0.30 NS 98.35±0.58 98.78±0.65
PB 95.07±0.39 95.14±0.39 PB 94.49±0.27 95.06±0.28
Yeast 97.60±0.33 97.93±0.18 Yeast 97.42±0.27 97.98±0.23
C.ele 89.54±1.23 89.61±1.12 C.ele 88.97±1.37 89.88±1.22
Power 86.21±2.89 95.53±0.33 Power 88.51±0.81 92.77±0.47
Router 95.07±1.63 96.15±1.26 Router 96.21±0.53 96.37±0.43
E.coli 97.57±0.30 97.93±0.34 E.coli 97.15±0.18 97.72±0.17
vector calculated using PHLP with the final output of the
AUC scores when used with Degree DRNL than with DRNL.
SEAL model increases AUC scores for all datasets, as listed
The substantial improvement observed in the Power dataset is
in Table III. This outcome suggests that when the SEAL
noteworthy, where Degree DRNL yields an increase of over
model lacks topological information for inference, the vectors
4 points in the AUC score. These experiments demonstrate
calculated using PHLP can serve as additional inputs.
the importance of incorporating degree information into node
labeling, revealing its efficacy in enhancing the performance
TABLEIV of MA-PHLP.
AUCSCORESFORWALKPOOL(WP)
WITHANDWITHOUTTDAFEATURES
TABLEVI
Dataset WP MA-PHLP+WP
AUCSCORESFORMA-PHLP(DIM0)WITHVARIOUS(k,l)-ANGLEHOPS
USAir 98.20±0.57 98.27±0.53
Dataset (1,0) (1,1)
NS 99.12±0.45 99.24±0.32
PB 95.42±0.25 95.58±0.32 USAir 96.15±0.83 95.87±0.83
Yeast 98.21±0.17 98.25±0.18 NS 98.28±0.55 98.66±0.66
C.ele 93.30±0.91 93.32±0.71 PB 93.95±0.34 94.46±0.36
Power 92.11±0.76 96.09±0.38 Yeast 95.52±0.32 97.31±0.20
Router 97.15±0.29 97.18±0.24 C.ele 86.18±2.12 87.57±1.20
E.coli 98.54±0.19 98.57±0.20 Power 73.39±0.99 77.83±1.44
Router 92.09±0.57 93.25±0.47
E.coli 96.94±0.24 96.95±0.28
Similarly,weattemptedtohybridizePHLPwiththecurrent
SOTA model, WP. As presented in Table IV, a slight increase Dataset (2,0) (2,1) (2,2)
in AUC scores is observed for all datasets. The Power dataset USAir 96.69±0.92 96.74±0.84 96.85±0.83
NS 98.72±0.51 98.59±0.65 98.56±0.47
demonstrates significant improvement.
PB 94.78±0.30 94.73±0.30 94.82±0.24
Yeast 97.71±0.18 97.66±0.27 97.58±0.28
C. Ablation Study C.ele 88.86±1.48 89.16±1.31 89.08±1.07
Power 80.27±1.07 83.90±1.29 86.12±0.86
Effects of Degree DRNL. To assess the proposed Degree Router 95.65±0.44 95.71±0.39 94.51±0.69
DRNL regarding the influence of incorporating degree in- E.coli 97.26±0.16 97.29±0.24 97.41±0.21
formation on model performance, we conducted experiments
usingDRNLandDegreeDRNLandcomparedtheresults.We Angles of PHLP. Table VI presents the performance of
used MA-PHLP (dim0) for the experiments. Table V presents PHLP (dim 0) concerning various (k,l)-angle hop subgraphs.
theAUCscoresofMA-PHLP(dim0)withDRNLandDegree Section III-B proposed angle hop subgraphs as an alternative
DRNL. Across all datasets, MA-PHLP (dim0) yields higher to traditional k-hop subgraphs to capture information from7
various perspectives. Moreover, MA-PHLP is proposed to we conducted experiments on the Power dataset. Table VIII
aggregate information from multiple angles. To investigate presents the AUC scores for varying hop (SEAL or WP) and
performancewhenextractinginformationfromspecificangles, max hop (MA-PHLP). For each target node, while the SEAL
weconductedexperimentsusingPHLPatdifferentangles.We and WP extract a k-hop subgraph, the MA-PHLP calculates
used only zero-dimensional PIs for the experiments. Overall, the PIs based on a subgraph with max hop M. When the
the results demonstrate that the performance is favorable for parameter M is 1 or 2, the AUC scores are not robust to
cases corresponding to the k-hop subgraph (where k and l k, showing large variations; however, when M is 3, although
are the same). However, some datasets perform better when MA-PHLP+SEALstillexhibitsvariationsupto2,MA-PHLP
k and l differ, highlighting the importance of varying angles +WPshowsonlyminorvariations.AsM exceeds3,theAUC
to achieve the best performance. Therefore, using MA-PHLP scoresofMA-PHLP+SEALandMA-PHLP+WParerobust
is recommended to maximize performance consistently across tok,exhibitinglittlesensitivity(maximum0.84)tovariations.
datasets. This suggests that setting both the hop and the max hop to
Comparison with TLC-GNN. To demonstrate that the pro- identical values may be permissible without further searching
posed method extracts superior topological information com- for optimal hyperparameters.
pared to the conventional TLC-GNN approach, we conducted
the same experiments. The TLC-GNN was constructed by V. ANALYSIS
augmenting the graph convolutional network (GCN) model A. Analysis of the PHLP
with PI information. We replaced the PI component of the
Figs. 6 and 7 visualize concatenated PIs to illustrate how
TLC-GNN model with the PI vector produced by MA-PHLP,
MA-PHLP (dim0) extracts topological features for LP. We
resulting in the MA-PHLP-GNN. The zero-dimensional PH
let Z ⊆ R2×k×r2 be a set of vectors calculated by MA-
was employed in this study for fair comparison because
PHLP, where k is the number of angles, and r denotes the PI
TLC-GNN used only zero-dimensional PH. Additionally, we
resolution. For (z ,z )∈Z, z ∈Rk×r2 is the concatenation
conducted experiments where the PH vectors were replaced 1 2 1
of PIs for all angles with a target link, and z ∈ Rk×r2 is
with zero vectors, denoted as GCN. Table VII presents the 2
the concatenation for cases without a target link. We consider
experimental results.
a function h : Rk×r2 → R defined as h(⃗v ,...,⃗v ) =
1 k
1 (cid:80)k ∥⃗v ∥ , where ⃗v ∈ Rr2 are PIs, and ∥·∥ denotes the
TABLEVII k i=1 i 1 i 1
L -norm. For visualization, we transform Z into points in R2
COMPARISONOFAUCSCORESWITHTLC-GNN 1
using the function G, defined as G(z ,z ) = (h(z ),h(z ))
1 2 1 2
Dataset GCN TLC-GNN MA-PHLP-GNN for each (z ,z )∈Z.
1 2
We plot distributions of points separately for positive and
Cora 92.20±0.83 93.16±0.56 93.14±0.93
CiteSeer 86.52±1.29 87.38±0.97 92.08±0.53 negative links, considering both DRNL and Degree DRNL.
PubMed 96.63±0.15 96.30±0.25 98.07±0.07 ThedistributionsoftheNSandYeastdatasetsbetweenpositive
and negative links display significant differences, supporting
The TLC-GNN is employed when the given data includes the highest performance in Table V. In contrast, the distribu-
node attributes. Hence, we conducted experiments using the tions for the C. ele and Power datasets are the most similar
following widely used benchmark datasets with node at- when using Degree DRNL, correlating with the lowest scores
tributes: Cora [60], CiteSeer [61], and PubMed [62]. The in Table V.
MA-PHLP-GNNoutperformedtheTLC-GNNsignificantlyon
the CiteSeer and PubMed datasets while achieving similar
B. Analysis of the Power Dataset
performance on the Cora dataset. The TLC-GNN does not
In most LP models, including the SOTA models SEAL and
exhibit performance improvement for the PubMed dataset
WP, the Power dataset tends to have the lowest AUC scores
despite adding topological information. However, the pro-
among the datasets. In Table II, the Power dataset is at the
posed MA-PHLP-GNN demonstrates substantial performance
bottomintermsofscoresacrossmodels(e.g.,WLK,WLNM,
enhancement. Although the proposed model is developed for
MF, LINE, SEAL, and WP). However, the proposed model
datasets without node attributes, it exhibits effective perfor-
achieves the highest AUC scores on the Power dataset among
mance on datasets with node attributes through hybridization
baseline models, prompting an analysis of the reasons for this
with the existing methods: SEAL+PHLP, WP+PHLP, and
performance.
MA-PHLP-GNN.Theseexperimentsverifytheversatilityand
In Fig. 7, for DRNL, the Power dataset exhibits horizontal
effectiveness of this approach across diverse datasets.
lines, indicating that the values h(z ) have a limited range of
2
outcomes for vectors z in cases without the target link; thus,
2
D. The hops and max hops of the hybrid methods
the set of values h(z ) with the same value should be spread
2
Determining the hyperparameters such as “hop” and “max out. This observation implies that, for numerous subgraphs
hop” is crucial for the performance of the hybrid method. We the calculation of PIs yields similar outcomes despite the
conductedexperimentstoexploretheeffectsofdifferentcom- differences in their topological structures, posing a challenge
binations of these parameters. Given that the hybrid methods in distinguishing between them.
(e.g., MA-PHLP + SEAL and MA-PHLP + WP) exhibited To address this problem, we applied Degree DRNL, which
the highest performance improvement on the Power dataset, incorporates degree information. The points in Fig. 7 are8
Fig.6. VisualizationofvectorscalculatedusingMA-PHLP(dim0).Foreachdataset,thefirstandsecondcolumnsdepicttheprojectionsofpersistenceimages
(PIs) when double radius node labeling (DRNL) is applied for node labeling, and the third and fourth columns represent the values obtained when Degree
DRNLisapplied.Thefirstandthirdcolumnsplotthevaluesproducedfrompositiveedges(i.e.,targetnodeslabeled1),andthesecondandfourthcolumns
plotthevaluesproducedfromnegativeedges(i.e.,targetnodeslabeled0).9
TABLEVIII
AUCSCORESONTHEPOWERDATASETVARYINGk-HOPANDMAXHOPM OFTHEHYBRIDMETHODS
MA-PHLP(withmaxhopM)
M 1 2 3 4 5 6 7
k notrobusttok robusttok
1 86.66±0.56 90.22±0.79 92.63±0.54 94.50±0.41 95.12±0.40 95.46±0.38 95.53±0.33
2 91.40±0.88 90.20±0.80 92.50±0.59 94.39±0.39 95.00±0.46 95.31±0.40 95.39±0.36
3 93.21±0.64 92.79±0.60 92.57±0.58 94.22±0.43 94.86±0.42 95.21±0.45 95.19±0.44
4 94.51±0.58 94.23±0.34 94.21±0.41 94.31±0.40 94.80±0.37 95.10±0.33 95.27±0.36
5 94.73±0.56 94.45±0.44 94.61±0.51 94.80±0.53 94.91±0.54 95.13±0.51 95.19±0.46
6 94.58±0.94 94.81±0.32 94.87±0.42 95.06±0.50 95.11±0.46 95.25±0.45 95.25±0.46
7 93.97±0.73 94.22±0.35 94.43±0.44 94.78±0.45 94.92±0.39 94.99±0.52 94.98±0.39
k notrobusttok robusttok
1 87.53±0.73 91.48±0.64 93.55±0.48 94.84±0.43 95.53±0.46 95.88±0.31 96.09±0.38
2 92.51±0.58 91.59±0.77 93.49±0.58 94.83±0.53 95.56±0.59 95.88±0.38 96.06±0.45
3 94.04±0.46 93.07±0.67 93.61±0.52 94.86±0.54 95.61±0.60 95.86±0.40 96.00±0.52
4 93.55±0.71 92.61±0.76 93.68±0.55 94.85±0.55 95.59±0.58 95.87±0.38 96.03±0.45
5 93.40±0.70 92.64±0.69 93.66±0.53 94.84±0.54 95.55±0.59 95.85±0.39 96.04±0.52
6 93.34±0.75 92.66±0.72 93.64±0.55 94.91±0.57 95.55±0.58 95.85±0.44 95.98±0.55
7 93.30±0.73 92.61±0.69 93.65±0.56 94.87±0.56 95.56±0.58 95.90±0.39 96.01±0.52
Fig.7. VisualizationofvectorscalculatedusingMA-PHLP(dim0).
distributed without horizontal lines, leading to the highest datasets. Embedding methods also display low performance.
score increase, as listed in Table V. In contrast, the GNN-based methods demonstrate improved
performanceusingsubgraphsandthenetworklearningability.
The performance of heuristic methods, such as AA, Katz,
However, the performance for the Power dataset is signifi-
andPR,tendtobesimilartorandomguessingondatasetswith
cantly lower than that for the Router dataset.
low density, particularly in the cases of the Power and Router
)poh-khtiw(LAES
)poh-khtiw(PW10
TABLEIX demonstrate that the proposed PHLP method achieves com-
AVERAGENUMBEROFNODESINSUBGRAPHS petitive performance across benchmark datasets, even SOTA
FORTHEPOWERANDROUTERDATASETS
performance, especially on the Power dataset. Additionally,
when integrated with existing GNN-based methods, PHLP
Power Router
improves performance across all datasets. By analyzing the
positive negative positive negative
topological information of the given graphs, PHLP addresses
1-hop 8.03 9.12 5.11 6.72
the limitations of GNN-based methods and enhances overall
2-hop 22.26 24.85 29.21 13.94
3-hop 43.11 49.50 120.35 55.22 performance.Asdemonstrated,PHLPprovidesexplainableal-
4-hop 71.72 82.16 411.87 176.34 gorithmswithoutrelyingoncomplexdeeplearningtechniques,
5-hop 99.28 116.75 740.80 411.35
providing insight into the factors that significantly influence
6-hop 136.23 158.27 1272.42 852.13
7-hop 182.22 210.35 1835.46 1498.58 performance for the LP problem of graph data.
REFERENCES
To bridge this gap, we analyzed subgraphs with node
labeling. The number of nodes within the selected subgraphs [1] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,”
IEEETransactionsonKnowledgeandDataEngineering,vol.34,no.1,
between positive and negative links was significantly different
pp.249–270,2020.
on the Router dataset but not the Power dataset (Table IX). [2] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A
Thisdifferenceisattributedtothepresenceofthehubnodesin comprehensivesurveyongraphneuralnetworks,”IEEEtransactionson
neuralnetworksandlearningsystems,vol.32,no.1,pp.4–24,2020.
the Router dataset, which are connected to numerous nodes.
[3] L.A.AdamicandE.Adar,“Friendsandneighborsontheweb,”Social
Thus, the subgraphs corresponding to positive links tend to networks,vol.25,no.3,pp.211–230,2003.
have more nodes than those corresponding to negative links. [4] L. Yao, L. Wang, L. Pan, and K. Yao, “Link prediction based on
common-neighbors for dynamic social network,” Procedia Computer
Science,vol.83,pp.82–89,2016.
TABLEX [5] M.Fire,L.Tenenboim,O.Lesser,R.Puzis,L.Rokach,andY.Elovici,
COMPARISONOFMODELSBYMAXHOPSETTINGS “Link prediction in social networks using computationally efficient
ONTHEPOWERANDROUTERDATASETS topological features,” in 2011 IEEE third international conference on
privacy, security, risk and trust and 2011 IEEE third international
Model MA-PHLP MA-PHLP WP MA-PHLP+WP conferenceonsocialcomputing. IEEE,2011,pp.73–80.
[6] S.M.KazemiandD.Poole,“Simpleembeddingforlinkpredictionin
Center target random - random
knowledgegraphs,”Advancesinneuralinformationprocessingsystems,
1-hop 78.05±1.20 85.66±0.86 80.24±0.95 87.53±0.73 vol.31,2018.
2-hop 86.34±1.04 90.52±0.73 89.40±1.00 91.59±0.77
3-hop 89.65±0.64 91.90±0.58 92.11±0.77 93.61±0.52 [7] M.Nayyeri,G.M.Cil,S.Vahdati,F.Osborne,A.Kravchenko,S.An-
4-hop 91.38±0.53 92.67±0.55 91.67±0.80 94.85±0.55 gioni, A. Salatino, D. R. Recupero, E. Motta, and J. Lehmann, “Link
5-hop 92.27±0.40 93.06±0.44 91.39±0.78 95.55±0.59 prediction of weighted triples for knowledge graph completion within
6-hop 92.77±0.47 93.16±0.49 91.55±0.83 95.85±0.44 thescholarlydomain,”IeeeAccess,vol.9,pp.116002–116014,2021.
7-hop 93.06±0.43 93.37±0.41 91.50±0.89 96.01±0.52
[8] Z. Stanfield, M. Cos¸kun, and M. Koyutu¨rk, “Drug response prediction
1-hop 93.12±0.45 93.40±0.46 94.48±0.36 94.83±0.41 asalinkpredictionproblem,”Scientificreports,vol.7,no.1,p.40321,
2-hop 95.96±0.40 95.70±0.45 97.15±0.27 97.22±0.23
2017.
3-hop 96.38±0.41 96.11±0.43 97.28±0.24 97.42±0.27
4-hop 96.45±0.40 96.22±0.43 OOM1 OOM [9] E.Nasiri,K.Berahmand,M.Rostami,andM.Dabiri,“Anovellinkpre-
5-hop 96.46±0.42 96.24±0.48 OOM OOM dictionalgorithmforprotein-proteininteractionnetworksbyattributed
6-hop 96.44±0.45 96.23±0.47 OOM OOM graph embedding,” Computers in Biology and Medicine, vol. 137, p.
7-hop 96.43±0.45 96.19±0.49 OOM OOM 104772,2021.
[10] C.LeiandJ.Ruan,“Anovellinkpredictionalgorithmforreconstructing
However, the Power dataset does not have hub nodes, protein–proteininteractionnetworksbytopologicalsimilarity,”Bioinfor-
matics,vol.29,no.3,pp.355–364,2013.
and the number of nodes in the subgraph of positive links
[11] I. A. Kova´cs, K. Luck, K. Spirohn, Y. Wang, C. Pollis, S. Schlabach,
remains small. We randomly changed the center nodes (a,b) W. Bian, D.-K. Kim, N. Kishore, T. Hao et al., “Network-based
for node labeling f(a,b) increasing the performance, as listed prediction of protein interactions,” Nature communications, vol. 10,
degdrnl no.1,p.1240,2019.
in Table X. This outcome highlights that setting target nodes
[12] N.Brockmann,E.ElsonKosasih,andA.Brintrup,“Supplychainlink
as the center nodes may not effectively analyze the topolog- predictiononuncertainknowledgegraph,”ACMSIGKDDExplorations
ical structure in the case of small graphs. Furthermore, the Newsletter,vol.24,no.2,pp.124–130,2022.
[13] A. Brintrup, P. Wichmann, P. Woodall, D. McFarlane, E. Nicks, and
performance for the Power dataset continues to increase with
W.Krechel,“Predictinghiddenlinksinsupplynetworks,”Complexity,
increasing hops (Table X), achieving an AUC score of 95.87, vol.2018,pp.1–12,2018.
which is significantly better than that of 92.11 for WP. [14] L. Lu¨ and T. Zhou, “Link prediction in complex networks: A survey,”
Physica A: statistical mechanics and its applications, vol. 390, no. 6,
pp.1150–1170,2011.
VI. CONCLUSION [15] A.-L. Baraba´si and R. Albert, “Emergence of scaling in random net-
works,”science,vol.286,no.5439,pp.509–512,1999.
This paper proposes PHLP, an explainable method that [16] T. Zhou, L. Lu¨, and Y.-C. Zhang, “Predicting missing links via local
applies PH to analyze the topological structure of graphs information,”TheEuropeanPhysicalJournalB,vol.71,pp.623–630,
2009.
to overcome the limitations of GNN-based methods for LP.
[17] S. Brin and L. Page, “Reprint of: The anatomy of a large-scale
By employing the proposed methods, such as angle hop hypertextual web search engine,” Computer networks, vol. 56, no. 18,
subgraphs and Degree DRNL, PHLP improves the analysis of pp.3825–3833,2012.
[18] G. Jeh and J. Widom, “Simrank: a measure of structural-context
the topological structure of graphs. The experimental results
similarity,” in Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining, 2002, pp. 538–
1OOMdenotes“outofGPUmemory”. 543.
rewoP
retuoR11
[19] Y.Koren,R.Bell,andC.Volinsky,“Matrixfactorizationtechniquesfor [45] Edelsbrunner, Letscher, and Zomorodian, “Topological persistence and
recommendersystems,”Computer,vol.42,no.8,pp.30–37,2009. simplification,” Discrete & computational geometry, vol. 28, pp. 511–
[20] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning 533,2002.
of social representations,” in Proceedings of the 20th ACM SIGKDD [46] H.Adams,T.Emerson,M.Kirby,R.Neville,C.Peterson,P.Shipman,
international conference on Knowledge discovery and data mining, S.Chepushtanova,E.Hanson,F.Motta,andL.Ziegelmeier,“Persistence
2014,pp.701–710. images:Astablevectorrepresentationofpersistenthomology,”Journal
[21] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for ofMachineLearningResearch,vol.18,2017.
networks,” in Proceedings of the 22nd ACM SIGKDD international [47] A.P.Bradley,“Theuseoftheareaundertheroccurveintheevaluation
conference on Knowledge discovery and data mining, 2016, pp. 855– ofmachinelearningalgorithms,”Patternrecognition,vol.30,no.7,pp.
864. 1145–1159,1997.
[22] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: [48] L. Katz, “A new status index derived from sociometric analysis,”
Large-scaleinformationnetworkembedding,”inProceedingsofthe24th Psychometrika,vol.18,no.1,pp.39–43,1953.
internationalconferenceonworldwideweb,2015,pp.1067–1077. [49] S. Brin and L. Page, “The anatomy of a large-scale hypertextual web
[23] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” arXiv searchengine,”ComputernetworksandISDNsystems,vol.30,no.1-7,
preprintarXiv:1611.07308,2016. pp.107–117,1998.
[50] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn,
[24] M. Zhang and Y. Chen, “Link prediction based on graph neural net-
and K. M. Borgwardt, “Weisfeiler-lehman graph kernels.” Journal of
works,” Advances in neural information processing systems, vol. 31,
MachineLearningResearch,vol.12,no.9,2011.
2018.
[51] M. Zhang and Y. Chen, “Weisfeiler-lehman neural machine for link
[25] S.Yun,S.Kim,J.Lee,J.Kang,andH.J.Kim,“Neo-gnns:Neighbor-
prediction,” in Proceedings of the 23rd ACM SIGKDD international
hoodoverlap-awaregraphneuralnetworksforlinkprediction,”Advances
conference on knowledge discovery and data mining, 2017, pp. 575–
inNeuralInformationProcessingSystems,vol.34,pp.13683–13694,
583.
2021.
[52] L.Tangand H.Liu,“Leveragingsocial medianetworksforclassifica-
[26] C. Mavromatis and G. Karypis, “Graph infoclust: Leveraging cluster-
tion,” Data Mining and Knowledge Discovery, vol. 23, pp. 447–478,
levelnodeinformationforunsupervisedgraphrepresentationlearning,”
2011.
arXivpreprintarXiv:2009.06946,2020.
[53] V.BatageljandA.Mrvar,“Pajekdatasets,”http://vlado.fmf.uni-lj.si/pub/
[27] Z. Yan, T. Ma, L. Gao, Z. Tang, and C. Chen, “Link prediction with
networks/data/,2006.
persistent homology: An interactive view,” in International conference
[54] M. E. Newman, “Finding community structure in networks using the
onmachinelearning. PMLR,2021,pp.11659–11669.
eigenvectorsofmatrices,”PhysicalreviewE,vol.74,no.3,p.036104,
[28] L. Pan, C. Shi, and I. Dokmanic´, “Neural link prediction with walk 2006.
pooling,”arXivpreprintarXiv:2110.04375,2021. [55] R. Ackland et al., “Mapping the us political blogosphere: Are con-
[29] S. Huber, “Persistent homology in data science,” in Data Science– servative bloggers more prominent?” in BlogTalk Downunder 2005
Analytics and Applications: Proceedings of the 3rdInternational Data Conference,Sydney. BlogTalkDownunder2005Conference,Sydney,
ScienceConference–iDSC2020. Springer,2021,pp.81–88. 2005.
[30] T. K. Dey and Y. Wang, Computational topology for data analysis. [56] C.VonMering,R.Krause,B.Snel,M.Cornell,S.G.Oliver,S.Fields,
CambridgeUniversityPress,2022. andP.Bork,“Comparativeassessmentoflarge-scaledatasetsofprotein–
[31] M. Horn, E. De Brouwer, M. Moor, Y. Moreau, B. Rieck, and proteininteractions,”Nature,vol.417,no.6887,pp.399–403,2002.
K. Borgwardt, “Topological graph neural networks,” arXiv preprint [57] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-
arXiv:2102.07835,2021. world’networks,”nature,vol.393,no.6684,pp.440–442,1998.
[32] X. Ye, F. Sun, and S. Xiang, “Treph: A plug-in topological layer for [58] N. Spring, R. Mahajan, and D. Wetherall, “Measuring isp topologies
graphneuralnetworks,”Entropy,vol.25,no.2,p.331,2023. with rocketfuel,” ACM SIGCOMM Computer Communication Review,
[33] M.Carrie`re,F.Chazal,Y.Ike,T.Lacombe,M.Royer,andY.Umeda, vol.32,no.4,pp.133–145,2002.
“Perslay: A neural network layer for persistence diagrams and new [59] M. Zhang, Z. Cui, S. Jiang, and Y. Chen, “Beyond link prediction:
graphtopologicalsignatures,”inInternationalConferenceonArtificial Predictinghyperlinksinadjacencyspace,”inProceedingsoftheAAAI
IntelligenceandStatistics. PMLR,2020,pp.2786–2796. ConferenceonArtificialIntelligence,vol.32,no.1,2018.
[34] F.M.Taiwo,U.Islambekov,andC.G.Akcora,“Explainingthepower [60] A.K.McCallum,K.Nigam,J.Rennie,andK.Seymore,“Automating
oftopologicaldataanalysisingraphmachinelearning,”arXivpreprint theconstructionofinternetportalswithmachinelearning,”Information
arXiv:2401.04250,2024. Retrieval,vol.3,pp.127–163,2000.
[35] T. Wen, E. Chen, and Y. Chen, “Tensor-view topological graph neural [61] C.L.Giles,K.D.Bollacker,andS.Lawrence,“Citeseer:Anautomatic
network,”arXivpreprintarXiv:2401.12007,2024. citationindexingsystem,”inProceedingsofthethirdACMconference
onDigitallibraries,1998,pp.89–98.
[36] J.Immonen,A.Souza,andV.Garg,“Goingbeyondpersistenthomology
[62] G.Namata,B.London,L.Getoor,B.Huang,andU.Edu,“Query-driven
usingpersistenthomology,”AdvancesinNeuralInformationProcessing
active surveying for collective classification,” in 10th international
Systems,vol.36,2024.
workshoponminingandlearningwithgraphs,vol.8,2012,p.1.
[37] C. Ying, X. Zhao, and T. Yu, “Boosting graph pooling with persistent
homology,”arXivpreprintarXiv:2402.16346,2024.
[38] Q. Zhao and Y. Wang, “Learning metrics for persistence-based sum-
maries and applications for graph classification,” Advances in Neural
InformationProcessingSystems,vol.32,2019.
[39] Y. Chen, B. Coskunuzer, and Y. Gel, “Topological relational learning
ongraphs,”Advancesinneuralinformationprocessingsystems,vol.34,
pp.27029–27042,2021.
[40] Q. Zhao, Z. Ye, C. Chen, and Y. Wang, “Persistence enhanced graph
neural network,” in International Conference on Artificial Intelligence
andStatistics. PMLR,2020,pp.2896–2906.
[41] M. Nickel, X. Jiang, and V. Tresp, “Reducing the rank in relational
factorization models by including observable patterns,” Advances in
NeuralInformationProcessingSystems,vol.27,2014.
[42] L.F.Ribeiro,P.H.Saverese,andD.R.Figueiredo,“struc2vec:Learning
noderepresentationsfromstructuralidentity,”inProceedingsofthe23rd
ACM SIGKDD international conference on knowledge discovery and
datamining,2017,pp.385–394.
[43] L. Vietoris, “U¨ber den ho¨heren zusammenhang kompakter ra¨ume und
eine klasse von zusammenhangstreuen abbildungen,” Mathematische
Annalen,vol.97,no.1,pp.454–472,1927.
[44] M.Gromov,“Hyperbolicgroups,”inEssaysingrouptheory. Springer,
1987,pp.75–263.