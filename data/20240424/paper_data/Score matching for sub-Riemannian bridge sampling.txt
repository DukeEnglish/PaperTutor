SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE
SAMPLING
ERLENDGRONG,KARENHABERMANN,STEFANSOMMER
Abstract. Simulation of conditioned diffusion processes is an essential tool
in inference for stochastic processes, data imputation, generative modelling,
and geometric statistics. Whilst simulating diffusion bridge processes is al-
ready difficult on Euclidean spaces, when considering diffusion processes on
Riemannian manifolds the geometry brings in further complications. In even
highergenerality, advancingfromRiemanniantosub-Riemanniangeometries
introduceshypoellipticity,andthepossibilityoffindingappropriateexplicitap-
proximationsforthescoreofthediffusionprocessisremoved. Wehandlethese
challenges and construct a method for bridge simulation on sub-Riemannian
manifolds by demonstrating how recent progress in machine learning can be
modifiedtoallowfortrainingofscoreapproximatorsonsub-Riemannianman-
ifolds. Sincegradientsdependentonthehorizontaldistribution,wegeneralise
the usual notion of denoising loss to work with non-holonomic frames using
astochasticTaylorexpansion,andwedemonstratetheresultingschemeboth
explicitly on the Heisenberg group and more generally using adapted coor-
dinates. We perform numerical experiments exemplifying samples from the
bridgeprocessontheHeisenberggroupandtheconcentrationofthisprocess
forsmalltime.
1. Introduction
The problem of simulating bridges for diffusion processes has been studied ex-
tensively in the literature, such as in [6, 16, 53, 11, 58, 10], and effective simulation
schemesfordiffusionbridgeprocesseshavebecomeparamountfordataimputation
and in parameter inference for stochastic processes as they provide a stochastic
method for approximating the intractable likelihood or when sampling from a pos-
teriori distribution of parameters. In the parameter estimation from given sets
of time-discrete observations, bridge simulations feature in the data imputation
performed when missing data is simulated through diffusion bridge processes that
connecttheobserveddata,seee.g. GolightlyandWilkinson[23],Papaspiliopoulos,
Roberts and Stramer [54], and van der Meulen and Schauer [65].
Bridge sampling further plays a prominent role in geometric statistics and has
found applications in areas such as medical image analysis and shape analysis,
see [2]. As discussed in [59], simulated diffusion bridge processes have been used to
approximate the heat kernel of a diffusion process on a Riemannian manifold and
to estimate the underlying metric structure.
2020 Mathematics Subject Classification. 58J65,53C17,62R30.
Key words and phrases. Bridgeprocesses,sub-Riemannianmanifolds,scorematching,bridge
sampling.
ThefirstauthorissupportedbythegrantGeoProCofromtheTrondMohnFoundation-Grant
TMS2021STG02(GeoProCo). Thethirdauthorissupportedbyaresearchgrant(VIL40582)from
VILLUMFONDENandtheNovoNordiskFoundationgrantNNF18OC0052000.
1
4202
rpA
32
]RP.htam[
1v85251.4042:viXra2 E.GRONG,K.HABERMANN,S.SOMMER
Most previous work on diffusion bridge simulations focuses on elliptic diffusion
processes on Euclidean spaces where the diffusivity matrix is uniformly invertible.
The simulations introduced by Bierkens, van der Meulen and Schauer [10] apply
both to elliptic diffusion processes and to those hypoelliptic diffusion processes
whose hypoellipticity arises from an interaction of the drift term with the diffusiv-
ity, such as for Langevin dynamics. Although the recent papers [39, 13, 15] have
addressed bridge simulations on manifolds, these schemes do not apply directly to
simulate bridge processes associated with diffusion processes on sub-Riemannian
manifolds, that is, those hypoelliptic diffusion processes where the hypoellipticity
is induced by the diffusivity term itself, without any need to interact with the drift
term. Phrased differently, as far as the authors are aware, it has not been known
how to effectively simulate bridge processes on sub-Riemannian manifolds. Such
spacesaredescribedasatriple(M,E,g),whereM isasmoothmanifold,E ⊆TM
isabracket-genertingsubbundlethatiscalledhorizontalbundleandgisasmoothly
varyinginnerproductonE. ThesubbundleE representsthedirectionsinwhichwe
allowthestochasticprocessonM todiffuseandthebracket-generatingassumption
on E ensures that the induced diffusion process is hypoelliptic. Sub-Riemannian
structuresnaturallyappearinallsciences, e.g. inthestudyofconstrainedphysical
systems such as the motion of robot arms or the orbital dynamics of satellites.
Motivated by the apparent gap in existing bridge simulation schemes, the im-
mersiveness of sub-Riemannian geometry, and the importance of bridge simulation
for data imputation and parameter inference, we here develop a method for bridge
simulation on sub-Riemannian manifolds.
A central object in the study of diffusion bridge processes is the score of the
density of the corresponding unconditioned diffusion process. To introduce the
score associated with a diffusion process on a manifold M, we need a gradient ∇E
on M. On a Riemannian manifold, ∇E is simply taken to be the usual gradient,
whilst on a sub-Riemannian manifold (M,E,g), the horizontal gradient ∇E will
be defined with respect to the subbundle E. For a time-homogeneous diffusion
process on M starting from x ∈ M and having transition probability density
0
p : M ×M →R for t>0, the score S (x ,·): M →E is given by, for y ∈M,
t t 0
(1.1) S (x ,y)=∇y,Elogp (x ,y).
t 0 t 0
Notably, the score is generally intractable due to its dependency on the transition
probability densities.
Bridge simulation schemes often employ guiding terms that approximate the
intractable score of the diffusion process. As we do not have such approximations
available explicitly in the sub-Riemannian context, we employ the approach of [32]
building on score matching techniques [36, 66] to learn the score parameterised by
a neural network and use this score in the simulation of the conditioned diffusion
processes. The guiding terms used in previous works on Riemannian manifolds
include radial vector fields, see [39, 13], or exploit the heat kernel on comparison
manifolds as in [15]. Neither of these options are computationally tractable in the
sub-Riemannian context. For training the neural network score approximation, we
show how the score divergence as well as a denoising term can be used for the loss
function. BothapproachesareusedintheEuclideansetting,butwewillseethatthe
sub-Riemannian geometry influences the terms resulting in non-trivial differences
between the Euclidean and geometric losses, see Figure 1.SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 3
V Xti V Xti+1
E
Xti
X
t E
Xti+1
Figure 1. In a sub-Riemannian geometry, the increments of a
horizontalstochasticprocess(X ) lieinthehorizontalbundleE.
t t≥0
When approximating the score for short time intervals by taking
derivatives of the density of the steps, a common approximation
takes the increment at the ith step to be normally distributed
in the distribution E at X (sketched dark grey). However,
Xti ti
this distribution is not differentiable with horizontal derivatives
in the distribution E at step X . The hypoellipticity of
Xti+1 ti+1
(X ) implies that the distribution has a vertical component in
t t≥0
V (sketched light grey). We rely on Taylor expansion for the
Xti
stochastic integral to approximate this component, which we then
exploit to derive a sub-Riemannian denoising loss for use in the
training of neural network score approximators.
1.1. Related work. In Euclidean space, simulation of conditioned diffusion pro-
cesses has been treated extensively in the literature in works including [17, 63, 6,
16, 7, 24, 45, 53, 11, 67, 58, 10, 9, 46]. The use of explicit score approximations
has extended bridge simulations to Riemannian manifolds, see [38, 39, 37, 13, 15].
Insteadofusingexplicitscoreapproximations,thework[32]hasdemonstratedhow
scorematchingin[36,66]canbeexploitedtotrainaneuralnetworktoapproximate
the score and use this in the simulation. Score matching and generative modelling
on manifolds has been the focus of [35, 12]. In this paper, we build on elements
of these ideas to show how a neural network can learn an approximated score on
sub-Riemannian manifolds, which subsequently can be used for bridge sampling.
1.2. Paper outline. In Section 2, we discuss the needed background material for
thepaper,and,inSection3,weprovidefurtherdetailsonbridgeprocessesandtheir
timereversalsinthesub-Riemanniansettinglinkingtheirgeneratorsandthescore.
We apply this in Section 4 to construct the neural network score approximator
using divergence loss, followed by a derivation of the corresponding denoising loss
in the sub-Riemannian context in Section 5. Combined together, the stochastic4 E.GRONG,K.HABERMANN,S.SOMMER
differential equation for the conditioned process and the score approximation allow
for numerical simulations of the bridge processes, which we use in Section 6 to
exemplify score simulations and bridge sampling on the Heisenberg group.
2. Diffusion processes on sub-Riemannian manifolds
Weherediscussrelevantbackgroundmaterialonsub-Riemanniangeometryand
diffusion processes on sub-Riemannian manifolds needed in the remainder of the
paper.
2.1. Sub-Riemannian manifolds. LetM beaconnectedmanifold. Weconsider
a fixed subbundle E ⊆ TM that we denote the horizontal bundle. We refer to the
elements of E as horizontal vectors and we use X(E) to denote the space of vector
fields that take values in E. Let g = ⟨·,·⟩ be a smoothly varying inner product
g
defined only on E. The triple (M,E,g) is called a sub-Riemannian manifold, and
the smoothly varying tensor g is called a sub-Riemannian metric. Such an inner
product gives rise to a sharp map
♯: T∗M →E defined by α(v)=⟨♯α,v⟩ , α∈T∗M,v ∈E.
g
Note that ♯ is not invertible when E ̸= TM. A sub-Riemannian metric g can also
be described in terms of its cometric g∗. This cometric g∗ = ⟨·,·⟩ is defined for
g∗
any pair of covectors α, β ∈T∗M for x∈M by
x
⟨α,β⟩ =⟨♯α,♯β⟩ .
g∗ g
Whilst the sub-Riemannian cometric g∗ is defined on all of T∗M, it is degenerate
whenever E ̸= TM because it vanishes on ker♯ = Ann(E), which consists of all
covectors vanishing on E.
We say that an absolutely continuous curve γ: [a,b] → M is horizontal if it is
thecasethatγ˙(t)∈E foralmosteveryt∈[a,b]. Suchacurvehasawell-defined
γ(t)
length given by
(cid:90) b (cid:90) b
length(γ)= ∥γ˙(t)∥ dt:= ⟨γ˙(t),γ˙(t)⟩1/2dt.
g g
a a
Furthermore, the metric defines a distance d : M ×M →R by setting d (x,y) for
g g
x,y ∈ M to be the infimum over the lengths of all those horizontal curves which
connect x to y. In order to guarantee the property that d (x,y) is finite for all
g
x,y ∈ M, we assume for the remainder of the paper that E is bracket-generating,
that is, we suppose that all directional derivatives can be recreated from E. To be
more precise, let us define, for any x∈M,
E¯ =span{[σ ,[σ ,...,[σ ,σ ]···]](x):σ ∈X(E), j ∈{1,...,l}, l≥1},
x 1 2 l−1 l j
where we interpret the bracket for the case l =1 simply as σ . The vector bundle
1
E isthensaidtobebracket-generatingifE¯ =TM. TheChow–Rashevski˘ıtheorem
in [14, 56] tells us that the bracket-generating assumption is a sufficient condition
not only for d to be finite, but furthermore for its metric topology to have the
g
same open sets as the original topology of the manifold M.
Observe that we can consider a Riemannian manifold (M,g) as a special case of
a sub-Riemannian manifold (M,E,g) with E =TM.SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 5
2.2. Sub-Laplacians and diffusion processes. We consider a sub-Riemannian
manifold (M,E,g). For a function f ∈ C∞(M), define ∇Ef = ♯df ∈ X(E), the
horizontal gradient of f. In the special case where E = TM, we simply write
∇Ef = ∇f. Assuming that we also have a smooth volume density dµ on M, we
define a corresponding second order operator ∆=∆ by
E,g,dµ
∆f =div (∇Ef), f ∈C∞(M).
dµ
The operator ∆ is the sub-Laplacian of (M,E,g,dµ). It is the unique second
order operator on M satisfying, for any pair of compactly supported functions
f ,f ∈C∞(M),
1 2 0
1
⟨∇Ef ,∇Ef ⟩ = (∆(f f )−f ∆f −f ∆f ),
1 2 g 2 1 2 1 2 2 1
(cid:90) (cid:90) (cid:90)
⟨∇Ef ,∇Ef ⟩ dµ=− f ∆f dµ=− f ∆f dµ.
1 2 g 1 2 2 1
M M M
If rankE = k and (σ ,...,σ ) is a local orthonormal frame for E with respect to
1 k
the sub-Riemannian metric g, then
k
(cid:88)
∆= (σ2+(div σ )σ ).
j dµ j j
j=1
If the subbundle E is a proper subbundle of the tangent bundle TM, then the
operator ∆ is not elliptic. However, if E is bracket-generating, then both the
operator ∆ and its heat operator ∂ − 1∆ are hypoelliptic, see H¨ormander [33].
t 2
For the special case where E = TM and (M,g) is a Riemannian manifold, the
operator ∆ is called the Witten–Laplacian. If dµ is the Riemannian volume form
g
of the Riemanian metric g and ∆ is the Laplace–Beltrami operator corresponding
g
to dµ ,thenthereexistsafunctionϕonM suchthatdµ=eϕdµ ,whichisrelated
g g
to ∆ by
∆=∆ +∇ϕ,
g
where ∇ϕ is defined with respect to the metric g.
For the remainder of the paper, we consider an operator L of the form
L=∆+2Z,
where ∆ is the sub-Laplacian of (M,E,g,dµ) and Z is a smooth vector field on
M. For such a second order partial differential operator L and x ∈ M, there
0
exists a unique semi-martingale (X ) = (Xx0) on M starting from x ,
t ζ>t≥0 t ζ>t≥0 0
with infinitesimal generator 1L and defined up to some maximal explosion time ζ,
2
see e.g. Elworthy [18], E´mery [19] and Hsu [34]. Throughout the paper, we will
assume that the process does in fact not explode, that is, ζ(x )=∞ almost surely.
0
For details on some criteria for non-explosion, see e.g. Bichteler and Jacod [8] or
Norris [52] as well as Grigor’yan [25] and [27] for geometric conditions.
Remark 2.1 (Choice of measure). Consider L = ∆ + 2Z for ∆ = ∆
dµ dµ E,g,dµ
defined with respect to a sub-Riemannian structure (E,g) and a smooth volume
density dµ on a manifold M. We observe that if dν = eϕdµ is another smooth
measure on M then ∆ =∆ satisfies
dν E,g,dν
∆ =∆ +∇Eϕ,
dν dµ6 E.GRONG,K.HABERMANN,S.SOMMER
and hence, we can write
L=∆ −∇Eϕ+2Z =:∆ +2Z˜.
dν dν
It follows that we are free to change our definition of dµ as long as we update the
vector field Z accordingly.
Conversely, if we have L = ∆+2Z where 2Z = ∇Eϕ is a horizontal gradient
then L=∆ is the sub-Laplacian with respect to some measure dµ˜ on M.
E,g,dµ˜
2.3. Local description of the operators. The following discussion concerning
the local description of Brownian motion on a Riemannian manifold provides a
motivation for studying the more general case of sub-Riemannian manifolds. Let
(W ) be a standard Brownian motion on Rd and consider the unique strong
t t≥0
solution (X ) , assumed to exist for all times, to the Stratonovich stochastic
t t≥0
differential equation
dX =σ(X )◦dW +σ (X )dt,
t t t 0 t
whereσ : Rd →Rdisasmoothvectorfieldandσ: Rd →Rd×d,x(cid:55)→σ(x)=(σi(x))
0 j
a smooth function. If σσ⊤(x) = ((cid:80)d σiσj(x)) is positive definite for all x ∈ Rd,
l=1 l l
then we can define a Riemannian metric g on Rd by setting g = (g ) = (σσ⊤)−1.
ij
The infinitesimal generator 1L of the stochastic process (X ) is then in terms
2 t t≥0
of the Laplace–Beltrami operator ∆ given by
g
 
d d d
(cid:88) (cid:88) (cid:88)
L= σ liσ lj∂ xi∂
xj
+ 2σ 0l + σ ji(∂ xiσ jl)∂
xl
=:∆ g+2Z,
i,j,l=1 l=1 i,j=1
where ∆ =(cid:80)d σiσi∂ ∂ −(cid:80)d gijΓl ∂ ,
g i,j,l=1 l l xi xj i,j,l=1 ij xl
 
d d d
2Z =(cid:88) 2Zl∂
xl
=(cid:88) 2σ 0l + (cid:88) (cid:0) σ ji(∂ xiσ jl)+gijΓl ij(cid:1) ∂
xl
l=1 l=1 i,j=1
andΓl aretheChristoffelsymbolswithrespecttotheRiemannianmetricg onRd.
ij
Ifσσ⊤ ismerelypositivesemi-definiteofconstantrank,thenwecannotinvertit.
However, we can still use g∗ = σσ⊤ as a cometric for a sub-Riemannian structure
(E,g). If we consider g∗ as a matrix, which is the same as identifying it with ♯,
then E will be the image of g∗.
2.4. Densities and conditional probability. Let∆bethesub-Laplacianofthe
sub-Riemannian manifold (M,E,g) equipped with the smooth volume density dµ
andletZ beasmoothvectorfieldonM. Define(X ) tobetheuniquestochastic
t t≥0
process starting from x ∈M and having generator 1L= 1∆+Z. Recall that we
0 2 2
assume this stochastic process does not explode. For T >0 given, let P=Px0,T be
the measure on the path space C([0,T],M) induced by (X ) . For s,t∈[0,T]
t t∈[0,T]
with s<t, we further let P denote the push-forward measure on M ×M of the
s,t
map (x ) (cid:55)→(x ,x ). Similarly, we set P to be the push-forward measure of
r r∈[0,T] s t t
the evaluation (x ) (cid:55)→ x . Since E is assumed to be bracket-generating, it
r r∈[0,T] t
follows by [33, 62] that P and P have smooth densities with respect to dµ⊗dµ
s,t t
anddµ,respectively. Foranyx,y ∈M andforanys,t∈[0,T]withs<t,wewrite
dP (x,y)=p (x)p (x,y)(dµ⊗dµ)(x,y) and dP (x)=p (x)dµ(x),
s,t s s,t t tSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 7
where p : M ×M →R is the transition probability density such that
s,t
(cid:90)
p (y)= p (x)p (x,y)dµ(x) and p (y)=p (x ,y).
t s s,t t 0,t 0
M
Furthermore, since L is a time-homogeneous operator, we know that
p (x,y)=p (x,y)=:p (x,y).
s,t 0,t−s t−s
FromourassumptionthatE isbracket-generating, itfurtherfollowsfrom[62]that
p (y) > 0 and p (x,y) > 0 for all x,y ∈ M and all s,t ∈ [0,T] with s < t. By
t s,t
definition, for any open set U ⊆M and for s,t∈(0,T] with s<t, we have
(cid:90) (cid:90)
P(X ∈U)= p (y)dµ(y) and P(X ∈U|X =x)= p (x,y)dµ(y).
t t t s s,t
U U
Let L∗ denote the adjoint of the operator L with respect to the smooth volume
density dµ, which is given by
L∗ =∆−2Z−2div Z.
dµ
Fromthedefinitionofthegenerator,wehavetherelation∂ E[f(X )]= 1E[Lf(X )]
t t 2 t
for f ∈C∞(M), which in turn implies that, for x,y ∈M and t∈[0,T],
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
∂ − L∗ p (y)=0, ∂ − Ly,∗ p (x,y)=0
t 2 t t 2 s,t
as well as
(cid:18) (cid:19)
1
(2.1) ∂ + Lx p (x,y)=0,
s 2 s,t
subject to initial conditions given by p (y)=δ (y) and p (x,y)=δ (y). For the
0 x0 t,t x
details, see e.g. Haussmann and Pardoux [31, p. 1191].
Remark 2.2. Ifwedefinep∗ (x,y)=p (y,x)andP∗f(x)=(cid:82) f(y)p∗(x,y)dµ(y)
s,t s,t t M t
then by the above equations, we have (∂ − 1L∗)P f = 0 and P f = f. It follows
t 2 t 0
that as long as div Z = 0 there then exists a stochastic process (X∗) with
dµ t t≥0
generator 1L∗ and such that
2
p∗ (x,y)=p∗ (x,y)=p (y,x)
s,t t−s t−s
are its transition probability densities.
3. Bridge processes and time-reversed processes
We determine the generator for Riemannian and sub-Riemannian time-reversed
diffusion bridge processes, with our main objective being to demonstrate how this
generator is related to the score defined in (1.1).
3.1. Bridge processes. As before, let (X ) on (M,E,g,dµ) be the stochastic
t t≥0
process with generator 1L = 1∆ + Z and initial value x ∈ M. For T > 0
2 2 0
and x ∈ M, the diffusion bridge process (Y ) on (M,E,g,dµ) obtained by
T t t∈[0,T]
conditioning (X ) on X = x can formally be defined using disintegration
t t∈[0,T] T T
ofmeasuretechniques,seee.g.[3]and[28]. Ifthereexistsauniquepathofminimal
energy connecting the starting point x to the final point x then in small time, as
0 T
shown in [4], the laws of the diffusion bridge processes concentrate near the unique
path of minimal energy. This result has been extended in [51] with x and x
0 T
being connected by multiple minimal paths, subject to the additional assumption
thatnoneofthemcontainsasegmentwhichisso-calledabnormal. Weremarkthat8 E.GRONG,K.HABERMANN,S.SOMMER
this concentration property for the diffusion bridge processes in small time heavily
relies on the underlying geometric structure and, as demonstrated in [29], may fail
for more general diffusion bridge processes.
Using the formulation of the Doob h-transform, see e.g. [57, Chapter 7.5], an
expression for the generator of the diffusion bridge process (Y ) in terms of
t t∈[0,T]
the logarithmic derivative of the transition probability densities p = p and
t,T T−t
the generator 1L of the unconditioned process (X ) can be obtained.
2 t t∈[0,T]
Lemma3.1. Thediffusionbridgeprocess(Y ) hastheinfinitesimalgenerator
t t∈[0,T]
1
L+∇x,Elogp (·,x ).
2 T−t T
Proof. Fort∈[0,T),letthefunctionh : M →Rbedefinedbyh (x)=p (x,x ).
t t t,T T
According to (2.1), we know that
(cid:18) (cid:19)
1
(3.1) ∂ + L h =0,
t 2 t
and by the Chapman–Kolmogorov equation, we further have, for s∈(0,T −t),
(cid:90)
(3.2) h (x)= p (x,y)h (y)dµ(y).
t t,t+s t+s
M
For any open set U ⊆M, we obtain
(cid:90) p (x,y)p (y,x )
P(X ∈U|X =x,X =x )= t,t+s t+s,T T dµ(y).
t+s t T T p (x,x )
U t,T T
Hence, the diffusion bridge process (Y ) on (M,E,g,dµ) has the same law as
t t∈[0,T]
the Markov process (Xh) starting from Xh = x whose transition kernel ph
t t∈[0,T] 0 0
is given by, for x,y ∈M,
h (y)
(3.3) ph (x,y)=p (x,y) t+s .
t,t+s t,t+s h (x)
t
It follows that, with ph(y)=ph (x ,y), we have
t 0,t 0
(cid:90)
P(Y ∈U)= ph(y)dµ(y).
t t
U
This is indeed all well-defined due to h (x)=p (x,x )>0 for all x∈M and all
t t,T T
t∈[0,T)and,bythespace-timeregularityproperty(3.2),furtherdefinesagenuine
transition probability density and thus a Markov process. The expression (3.3)
implies that, for f ∈C∞(M),
0
E[f(X )h (X )|X =x]
E[f(Xh )|Xh =x]= t+s t+s t+s t .
t+s t h (x)
t
Usingthisformula,wecanfindthegenerator 1LhfortheMarkovprocess(Xh) ,
2 t t t∈[0,T]
and hence for (Y ) , by first deducing that
t t∈[0,T]
1 E[f(Xh )|Xh =x]−f(x)
Lhf(x)=lim t+s t
2 t s↓0 s
E[f(X )h (X )|X =x]−f(x)h (x)
=lim t+s t+s t+s t t
s↓0 sh t(x)
(∂ + 1L)(fh )(x)
= t 2 t .
h (x)
tSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 9
By applying (3.1) and using 1L= 1∆+Z, we conclude that
2 2
(∂ + 1L)(fh ) 1 ⟨∇Ef,∇Eh ⟩
t 2 t = Lf + t ,
h 2 h
t t
whichestablishes 1Lh = 1L+∇Elogh . Duetoh (x)=p (x,x )=p (x,x ),
2 t 2 t t t,T T T−t T
the claimed result follows. □
3.2. Time-reversals. Weconsiderageneralprocess(Y ) on(M,E,g,dµ)with
t t≥0
a time-dependent generator 1L = 1∆+Z for a time-dependent vector field Z
2 t 2 t t
and with initial value x ∈ M. We shall again assume non-explosion. For T > 0,
0
we denote the associated measure induced on the path space C([0,T],M) by Q.
Let us further define q on M for t∈[0,T] through, for any open set U ⊆M,
t
(cid:90)
(3.4) Q(Y ∈U)= q (y)dµ(y).
t t
U
We are now interested in looking at the time-reversed process (Y¯) given
t t∈[0,T]
by Y¯ =Y . We observe the following about its infinitesimal generator.
t T−t
Lemma 3.2. The time-reversed processes (Y¯) defined by setting Y¯ = Y
t t∈[0,T] t T−t
for t∈[0,T] has infinitesimal generator
1 1
L¯ = ∆−Z +∇Elogq .
2 t 2 T−t T−t
Furthermore, we have that ∇Eq is in L2(E;g,dµ) for t∈[0,T].
t
Toproduceourresult,wemodifytheproofof[31,Theorem2.1]foracoordinate
independent formulation.
Proof. In order to prove that the time-reversed process (Y¯) has infinitesimal
t t∈[0,T]
generator 1L¯ , it suffices to show that, for s,t ∈ [0,T] with s < t and for any
2 t
compactly supported function f ∈C∞(M), the mapping
0
1(cid:90) t
t(cid:55)→f(Y¯)−f(Y¯ )− (L¯ f)(Y¯ )dr
t s 2 r r
s
defines a martingale. As argued in [31], the above is equivalent to the statement
that, for all f,ϕ∈C∞(M),
0
(cid:20)(cid:18) 1(cid:90) t (cid:19) (cid:21)
E f(Y¯)−f(Y¯ )− (L¯ f)(Y¯ )dr ϕ(Y¯ ) =0,
t s 2 r r s
s
whichafterapplyingY¯ =Y andchangingvariablesinthetimeintegralamounts
t T−t
to
(cid:34)(cid:32) (cid:33) (cid:35)
1(cid:90) T−s
E f(Y )−f(Y )− (L¯ f)(Y )dr ϕ(Y ) =0.
T−t T−s 2 T−r r T−s
T−t
SinceT−t<T−sfors<t,itthereforesufficestoestablishthat,foralls,t∈[0,T]
with s<t and for all f,ϕ∈C∞(M),
0
(cid:20)(cid:18) 1(cid:90) t (cid:19) (cid:21)
E f(Y )−f(Y )+ (L¯ f)(Y )dr ϕ(Y ) =0.
t s 2 T−r r t
s
Let Q with, for x,y ∈M,
s,t
dQ (x,y)=q (x)q (x,y)(dµ⊗dµ)(x,y)
s,t s s,t10 E.GRONG,K.HABERMANN,S.SOMMER
denote the push-forward measure of the map (x ) (cid:55)→(x ,x ). We observe
r r∈[0,T] s t
(cid:90)
E[f(Y )ϕ(Y )]= f(x)ϕ(y)dQ (x,y)
s t s,t
M×M
(cid:90) (cid:18)(cid:90) (cid:19)
= f(x)q (x) q (x,y)ϕ(y)dµ(y) dµ(x)
s s,t
M M
(cid:90)
=: f(x)q (x)Φ (x)dµ(x)=E[f(Y )Φ (Y )],
s s,t s s,t s
M
where Φ (x)=(cid:82) q (x,y)ϕ(y)dµ(y)=E[ϕ(Y )|Y =x]. By using the properties
s,t M s,t t s
that (∂ + 1L )Φ =0 and (∂ − 1L∗)q =0, we further obtain
r 2 r r,t r 2 r r
E[f(Y )Φ (Y )]−E[f(Y )Φ (Y )]
t t,t t s s,t s
(cid:90) t(cid:90)
= f(x)(q (x)∂ Φ (x)+Φ (x)∂ q (x))dµ(x)dr
r r r,t r,t r r
s M
1(cid:90) t(cid:90)
= f(x)(−q (x)L Φ (x)+Φ (x)L∗q (x))dµ(x)dr
2 r r r,t r,t r r
s M
1(cid:90) t(cid:90)
= Φ (x)(−L∗(fq )(x)+f(x)L∗q (x))dµ(x)dr.
2 r,t r r r r
s M
Since the adjoint L∗ of the operator L =∆+2Z with respect to dµ is given by
t t t
L∗ =∆−2Z −2div Z ,
t t dµ t
we have
1 1
L∗(fq )− fL∗q
2 t t 2 t t
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
= ∆−Z −div Z (fq )−f ∆−Z −div Z q
2 t dµ t t 2 t dµ t t
(cid:18) (cid:19)
1 1
=q ∆−Z f +⟨∇Ef,∇Eq ⟩= q L¯ f.
t 2 t t 2 t T−t
Hence, it follows that
E[f(Y )ϕ(Y )]−E[f(Y )ϕ(Y )]=E[f(Y )Φ (Y )]−E[f(Y )Φ (Y )]
t t s t t t,t t s s,t s
1(cid:90) t(cid:90)
=− q (x)Φ (x)(L¯ f)(x)dµ(x)dr
2 r r,t T−r
s M
1(cid:90) t(cid:90)
=− q (x)q (x,y)ϕ(y)(L¯ f)(x)(dµ⊗dµ)(x,y)dr
2 r r,t T−r
s M×M
1 (cid:20) (cid:90) t (cid:21)
=− E ϕ(Y ) (L¯ f)(Y )dr ,
2 t T−r r
s
as required. □
By combining Lemma 3.1 and Lemma 3.2, we can determine the infinitesimal
generator of the stochastic process obtained by time-reversing the diffusion bridge
process corresponding to (X ) with infinitesimal generator 1L = 1∆+Z and
t t≥0 2 2
starting point x ∈ M conditioned on X = x for T > 0 and x ∈ M. Recall
0 T T T
that we use p (x,y) = p (x,y) for x,y ∈ M to denote the transition probability
t 0,t
density of the semi-martingale (X ) .
t t≥0SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 11
Corollary 3.3. For(Y ) thediffusionbridgeprocessobtainedbyconditioning
t t∈[0,T]
(X ) with infinitesimal generator 1L= 1∆+Z and starting point x ∈M on
t t∈[0,T] 2 2 0
X =x , wedefinethetime-reversedprocess(Y¯) byY¯ =Y fort∈[0,T].
T T t t∈[0,T] t T−t
Then the infinitesimal generator of (Y¯) is given by
t t∈[0,T]
1 1
L¯ = ∆−Z+S (x ,·).
2 t 2 T−t 0
Proof. By Lemma 3.1, the infinitesimal generator of (Y ) takes the form
t t∈[0,T]
1
∆+Z+∇x,Elogp (·,x ).
2 T−t T
Lemma 3.2 then implies that the time-reversed process (Y¯) has infinitesimal
t t∈[0,T]
generator
1 1
(3.5) L¯ = ∆−Z−∇x,Elogp (·,x )+∇Elogq ,
2 t 2 t T T−t
whereq isstilldefinedby (3.4). Itremainstoshowthatthelineartermreducesto
t
the claimed expression. Since (Y ) is the diffusion bridge process associated
t t∈[0,T]
with (X ) from x to x in time T >0, we have, for y ∈M,
t t≥0 0 T
p (x ,y)p (y,x ) p (x ,y)p (y,x )
q (y)= 0,T−t 0 T−t,T T = T−t 0 t T .
T−t p (x ,x ) p (x ,x )
0,T 0 T T 0 T
It follows that
∇Elogq =∇y,Elogp (x ,·)+∇x,Elogp (·,x ).
T−t T−t 0 t T
This combined with (3.5) and the definition (1.1) establishes
1 1 1
L¯ = ∆−Z+∇y,Elogp (x ,·)= ∆−Z+S (x ,·),
2 t 2 T−t 0 2 T−t 0
as required. □
Remark 3.4 (Generative models). We use the reversed generator from Lemma 3.2
for simulating bridge processes. However, we could have also applied it for genera-
tive modelling as is often the focus in the machine learning literature on diffusion
models, i.e., given a stochastic process (Y ) with some final distribution Y ,
t t∈[0,T] T
reversing the process enables sampling from the initial distribution for Y . Some
0
examples of generative modelling on manifolds include [12, 35].
4. Approximating the score with neural networks
Wenowbuildthesetupforapproximatingthescoreusingneuralnetworksinthe
sub-Riemanniansetting. Theideasfollowtheonespresentedin[32],wherediffusion
processesonRd withuniformlypositivedefinitediffusivitymatricesareconsidered.
The complexities arising in our context correctly handle the sub-Riemannian ge-
ometry, particularly the hypoellipticity of the sub-Riemannian diffusion processes.
Inthissection,wediscusslossfunctionsinvolvingthedivergenceofthescoreand
a basic Euler–Maruyama sampling scheme leading to the score learning algorithm.
We also give some details on the sampling scheme, for further use in Section 5,
where we discuss the alternative denoising loss.
As previously, let (E,g) be a bracket-generating sub-Riemannian structure on a
manifold M and let ∆ be the sub-Laplacian with respect to the volume form dµ.
Weconsiderthestochasticprocess(X ) =(Xx0) withinfinitesimalgenerator
t t≥0 t t≥012 E.GRONG,K.HABERMANN,S.SOMMER
1L = 1∆+Z started from the initial value x ∈ M and assumed to not explode.
2 2 0
Let p (x ,·) for t>0 denote the density of (X ) with respect to dµ whose score
t 0 t t≥0
S (x ,·) is given by (1.1).
t 0
Recall that according to Lemma 3.1, the bridge process associated with (X )
t t≥0
from x to x ∈M in time T >0 has generator 1L+∇x,Elogp (·,x ). Since,
0 T 2 T−t T
if Z = 0, the operator L∗ = L = ∆ is symmetric, it follows from Remark 2.2 that
thenthegeneratorofthebridgeprocesscanbeexpressedas 1L+S (x ,·). This
2 T−t T
wouldallowustosimulatebridgeprocessesbyapproximatingthescoreS (x ,·)
T−t T
from the target point x . However, in general, we have p∗(y,x) = p (x,y) with
T t t
p∗(y,·) a fundamental solution of ∂ − 1L∗ where the adjoint operator
t t 2
L∗ =∆−2Z−2div Z
dµ
with respect to dµ may have a non-zero zeroth-order term. To avoid this problem,
weinsteadconsiderthereversedbridgeprocess(Y¯) asinCorollary3.3which
t t∈[0,T]
has the generator
1 1
L¯ = ∆−Z+S (x ,·).
2 t 2 T−t 0
Thus, we will first need to determine the score from the starting point x and then
0
simulate bridges backwards from the endpoint x .
T
We henceforth write S (·)=S (x ,·).
t t 0
4.1. Score representation and loss function. For estimating the score S, we
letSθ beatime-dependentvectorfieldinE givenastheoutputofaneuralnetwork
whose network parameters are denoted by θ. If (σ ,...,σ ) is frame for E, we can
1 k
consider this problem as searching for functions, for j ∈{1,...,k},
Sθ,j: [0,T]×M →R
(t,y)(cid:55)→Sθ,j(y)
t
such that Sθ =(cid:80)k Sθ,jσ . Further details of the network architecture are given
j=1 j
in Section 6.
When modifying the neural network weights to make the network match the
score, we work with the squared L2-distance as the loss function
(cid:90) T (cid:90)
E(θ)= (cid:13) (cid:13)S tθ(y)−∇y,Elogp t(x 0,y)(cid:13) (cid:13)2
g
p t(x 0,y)dµ(y)dt
0 M
=(cid:90) T Ex0(cid:104)(cid:13) (cid:13)S tθ(X t)−∇y,Elogp t(x 0,X t)(cid:13) (cid:13)2 g(cid:105) dt.
0
For short-time approximations and for being able to compute the loss at every
step, we will subdivide the energy E(θ) into shorter time-intervals. Observe that,
for s∈[0,t),
(cid:90) (cid:90)
⟨Sθ(y),∇y,Elogp (x ,y)⟩ p (x ,y)dµ(y)= ⟨Sθ(y),∇y,Ep (x ,y)⟩ dµ(y)
t t 0 g t 0 t t 0 g
M M
(cid:90)
= ⟨Sθ(y),∇y,Elogp (z,y)⟩ p (x ,z)p (z,y)(dµ⊗dµ)(z,y)
t t−s g s 0 t−s
M×M
(cid:90)
= ⟨Sθ(y),∇y,Elogp (z,y)⟩ dP (z,y)=Ex0(cid:2) ⟨Sθ(X ),S (X ,X )⟩ (cid:3) .
t t−s g s,t t t t−s s t g
M×MSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 13
From these computations, we deduce that for e (θ) defined by
s,t
(4.1) e (θ)=Ex0(cid:2) ⟨Sθ(X ),Sθ(X )−2S (X ,X )⟩ (cid:3) ,
s,t t t t t t−s s t g
the expression
Ex0(cid:104)(cid:13) (cid:13)S tθ(X t)−∇y,Elogp t(x 0,X t)(cid:13) (cid:13)2 g(cid:105) −e s,t(θ)
is constant with respect to θ. This implies in particular that, for any subdivision
0 = t < t < ··· < t = T of the interval [0,T], there exists a constant C such
0 1 n
that
n−1 (cid:90) t
(cid:88)
E(θ)=E (θ)+C = E (θ)+C, where E (θ):= e (θ)dr.
0,T ti,ti+1 s,t s,r
i=0 s
It follows that letting our neural network minimise relative to E(θ) is equivalent to
minimisingwithrespecttoE (θ). NotethatthedefinitionforE (θ)stillcontains
0,T s,t
the expression of the true score, which is usually intractable. In the subsequent
sections, we discuss how we can consider minima of the loss functions without
knowing the true score explicitly.
4.2. Lossfunctionwithdivergence. Byexploitinganintegration-by-partstrick,
the loss function can be computed without explicitly knowing the true score. We
provetheresulthereforthesub-Riemanniancasethathasbeenpreviouslyderived
for the Euclidean setting in [36] and for Riemannian manifolds in [12], based on
earlier work in [60, 61].
Theorem 4.1. For any s,t ∈ [0,T] with s < t and for e (θ) defined as in (4.1),
s,t
we have
e s,t(θ)=(cid:90) (cid:16)(cid:13) (cid:13)S tθ(y)(cid:13) (cid:13)2 g+2(div dµS tθ)(y)(cid:17) dP t(y)
(4.2) M
=Ex0(cid:104)(cid:13) (cid:13)S tθ(X t)(cid:13) (cid:13)2 g+2(div dµS tθ)(X t)(cid:105) =:e( t2)(θ).
We remark that this rewritten expression for e(2)(θ) does not depend explicitly
t
on the score itself. Since then E (θ)=(cid:82)t e(2)(θ)dr, we can use the formulation of
s,t s r
Theorem 4.1 to minimise E (θ) without knowing the score. In computations, the
0,T
expectations are approximated by averaging over samples X(1),...,X(K) from the
stochastic process X =(X ) through, with δ = T,
t t∈[0,T] n
(4.3) E (θ)≈
δ (cid:88)K (cid:88)n (cid:18)(cid:13) (cid:13)Sθ(X(l))(cid:13) (cid:13)2
+2(div
Sθ)(X(l))(cid:19)
.
0,T K (cid:13) iδ iδ (cid:13) g dµ iδ iδ
l=1 i=1
Proof of Theorem 4.1. Recall that on the manifold M without boundary we have,
for f ∈C∞(M) and a smooth vector field V on M,
(cid:90) (cid:90)
(4.4) df(V)dµ=− fdiv (V)dµ.
dµ
M M
We apply (4.4) to deduce
(cid:90)
⟨Sθ(y),∇y,Elogp (z,y)⟩ dP (z,y)
t t−s g s,t
M×M
(cid:90)
= p (x ,z)(dyp (z,y)(Sθ(y)))(dµ⊗dµ)(z,y)
s 0 t−s t
M×M14 E.GRONG,K.HABERMANN,S.SOMMER
(cid:90)
=− p (x ,z)p (z,y)(div Sθ)(y)(dµ⊗dµ)(z,y)
s 0 t−s dµ t
M×M
(cid:90)
=− (div Sθ)(y)dP (y).
dµ t t
M
The claimed result that e (θ)=e(2)(θ) then follows. □
s,t t
4.2.1. Expressioninlocalcoordinates. Supposewechoosealocalcoordinatesystem
(x1,...,xd) on M and let dx = dx1...dxd denote the Euclidean volume measure
inthesecoordinates. Considerasecondorderoperator L=∆+2Z onM where∆
is a sub-Laplacian of a sub-Riemannian structure (E,g). By Remark 2.1, we know
that with the appropriate choice for Z we can assume that ∆ = ∆ is the
E,g,dx
sub-Laplacian with respect to dx. Let (σ ,...,σ ) be a local orthonormal frame
1 k
for (E,g), which is described by a d×k matrix (σi) such that σ =(cid:80)d σi∂ for
j j i=1 j xi
j ∈{1,...,k}. With Z =(cid:80)d Zi∂ , we have
i=1 xi
k d k (cid:32) d (cid:33) d
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
L=∆+2Z = σlσi∂ ∂ + ∂ σl σi∂
j j xl xi xl j j xi
j=1i,l=1 j=1 l=1 i=1
k d d
(cid:88) (cid:88) (cid:88)
+ σl∂ σi∂ +2 Zi∂ .
j xl j xi xi
j=1i,l=1 i=1
Let (W ) be a standard Brownian motion on Rk and set
t t≥0
k k (cid:32) d (cid:33) d d
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
σ = (div σ )σ +2Z = ∂ σl σi∂ +2 Zi∂
0 dx j j xl j j xi xi
j=1 j=1 l=1 i=1 i=1
as well as τ =σ + 1(cid:80)k (∇¯ σ ), where ∇¯ denotes the flat connection given by
0 0 2 j=1 σj j
d d
(4.5) ∇¯ V =(cid:88) (V Vi)∂ = (cid:88) Vl(∂ Vi)∂ .
V1 2 1 2 xi 1 xl 2 xi
i=1 i,l=1
Thenthestochasticprocess(X ) withgenerator 1Listheuniquestrongsolution
t t≥0 2
to the Stratonovich stochastic differential equation
k
(cid:88)
(4.6) dX = σ (X )◦dWj +σ (X )dt, X =x ,
t j t t 0 t 0 0
j=1
or to the Itˆo stochastic differential equation
k
(cid:88)
(4.7) dX = σ (X )dWj +τ (X )dt, X =x .
t j t t 0 t 0 0
j=1
Writing the output of the neural network as Sθ =(cid:80)k Sθ,jσ , we obtain
j=1 j
 
(cid:90) k
e( t2)(θ)= (cid:88) S tθ,j(y)2 p t(x 0,y)dy
M j=1
 
(cid:90) (cid:88)k (cid:88)d (cid:16) (cid:17)
+2  S tθ,j(y)(∂ xiσ ji)(y)+σ ji(y)(∂ xiS tθ,j)(y) p t(x 0,y)dy.
M j=1i=1SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 15
If we now have sample paths X(1), ..., X(K) and discretise the interval [0,T] with
δ = T, then (4.3) gives rise to
n
K n k
E (θ)≈ δ (cid:88)(cid:88)(cid:88) Sθ,j(X(l))2
0,T K iδ iδ
l=1 i=1j=1
K n k d
+ δ (cid:88)(cid:88)(cid:88) (cid:88) (cid:16) Sθ,j(X(l))(∂ σm)(X(l))+σm(X(l))(∂ Sθ,j)(X(l))(cid:17) .
K iδ iδ xm j iδ j iδ xm iδ iδ
l=1 i=1j=1m=1
4.3. Time integration. We now explicitly describe an Euler–Maruyama simula-
tion scheme for finding paths of the stochastic process (X ) = (Xx0)
t t∈[0,T] t t∈[0,T]
with generator 1L= 1∆+Z. Later on, we will further exploit this approximation
2 2
for the denoising version of the loss function.
In order to describe the method, we assume that M = Rd topologically. If the
manifoldM hasadifferenttopology, wecaneitherworkinlocalcoordinatesoruse
geometric Euler–Maruyama methods. For instance, see [55, 50] for some examples
of methods for matrix Lie groups.
Assumethat(X ) can,atleastlocally,beconsideredasthesolutionof (4.7)
t t∈[0,T]
inacoordinatesystem(x1,...,xd). Forafixedn∈N,wesetδ = T andt =iδ for
n i
i ∈ {0,1,2,...,n}. The Euler–Maruyama approximation (Xˆ ) = (Xˆn)
t t∈[0,T] t t∈[0,T]
is then defined by, for i∈{0,1,2,...,n−1} and t∈(0,δ],
k
(4.8) Xˆ =Xˆ +(cid:88) σ (Xˆ )(Wj −Wj)+τ (Xˆ )t, Xˆ =x .
ti+t ti j ti ti+t ti 0 ti 0 0
j=1
The Euler–Maruyama approximation (Xˆn) converges strongly to (X )
t t∈[0,T] t t∈[0,T]
as n → ∞ under appropriate growth conditions on the vector fields τ ,σ ,...,σ ,
0 1 k
see [41, Theorem 4.5.3 and Theorem 9.6.2] for details.
4.4. Score estimation. Together the loss e (θ) given by (4.2) and the sampling
s,t
scheme(4.8)providethetoolsneededtotrainthescoreapproximation. Theexplicit
algorithm is listed in Algorithm 1 below.
Algorithm 1 Estimating the score using the divergence loss
Require:
• Initial set-up: Initial point x , final point x .
0 T
• Diffusion vector fields: Vector fields σ ,...,σ through the function with
1 k
values in d×k matrix (σi).
j
• Learning setup: Learning rate ϵ>0, number of iterations N ∈N.
1: Initialisation: Initialise weights θ randomly.
2: for i=1 to N do
3: Forward pass: Draw batch of K ∈ N Euclidean Wiener process samples
W(1),...,W(K) and integrate (4.8) to obtain sample paths X(1),...,X(K).
4: Compute Loss: Evaluate the approximation (4.3) of E 0,T(θ) using samples
X(1),...,X(K).
5: Backward pass: Compute the gradient ∇ θE 0,T(θ).
6: Update Parameters: Set θ ←θ−ϵ∇ θE 0,T(θ).
7: end for16 E.GRONG,K.HABERMANN,S.SOMMER
5. Approximating loss functions for short time steps
Even though the loss function in E (θ) = (cid:82)t e(2)(θ)dr defined by (4.2) has no
s,t s r
explicit dependency on the score, using the term (div Sθ)p can in practice be
dµ t t
problematic because the integral often is approximated with finite samples at the
steps of the training, and the training might therefore at each step minimise the
divergence at the sample points only, resulting in unstable convergence. Moreover,
the computational expense of finding derivatives of the neural network and taking
gradients of those for the optimisation can be a problem if the dimension of M is
high. These issues have also been observed in the Euclidean case, and a common
way to deal with these problems is the denoising loss, see [36, 66]. In this section,
we show how this approach can be generalised to the sub-Riemannian setting.
Withthenotationinthispaper,thedenoisinglossarisesfrom(4.1)usingexplicit
approximations of the true score for s and t close, e.g. for steps from t to t .
i i+1
Such steps can be approximated, in the Euclidean setting, by normal distributions
and, in the Riemannian situation, by heat kernel approximations or tangent space
normal distributions as in [12]. In the sub-Riemannian context, we take horizontal
derivatives of the score, but we also need to take account of the change of the
distributionbetweentangentspacesandthehypoellipticityofthediffusionprocess.
We therefore look at short-time approximations for the steps of diffusion processes
on sub-Riemannian manifolds.
5.1. Short-time score approximations with Euler steps. We choose a local
coordinate system (x1,...,xd) such that we can identify M with Rd. As in Sec-
tion4.3, letusconsidertheapproximation(Xˆ ) givenasthesolutionto(4.8)
t t∈[0,T]
from the Euler–Maruyama integration. The equation is stated using the global
orthonormal frame (σ ,...,σ ) represented by the matrix σ = (σi). We write
1 k j
Σ = σσ⊤ and observe, for t ∈ (0,δ), that Xˆ ∼ N(tτ (x ),tΣ(x )) which is sup-
t 0 0 0
ported on tτ (x )+E . It follows that the probability measure Pˆ = Pˆn of Xˆ is
0 0 x0 t t t
notevenabsolutelycontinuouswithrespecttoP whenevert∈(0,δ)andE ̸=TM.
t
If we write dPˆ =pˆ dµ then pˆ cannot be described as a continuous function.
t t t
Wefurtherencountertheproblemthatingeneralthetangentspaceatthepoint
y ∈ tτ (x )+E is not included in the horizontal distribution at y, i.e., we gen-
0 0 x0
erally have T (tτ (x ) + E ) ̸⊆ E . In particular, it becomes problematic to
y 0 0 x0 y
define a horizontal gradient. We instead consider the following solution. We take
Sθ = (cid:80)k Sθ,jσ with associated vector representation Sθ = (Sθ,1,...,Sθ,k) and
j=1 j
observe that
e (θ)=Ex0(cid:2) ⟨Sθ (X ),Sθ (X )−2S (X ,X )⟩ (cid:3)
ti,ti+t ti+t ti+t ti+t ti+t t ti ti+t g
(cid:88)k (cid:90) (cid:16) (cid:17)
= Sθ,j (y)2−2Sθ,j (y)dylogp (z,y)(σ (y)) dP (z,y).
ti+t ti+t t j ti,ti+t
j=1 M×M
Asaresultofthediscussedproblemswiththehorizontalgradientintheapproxima-
tion scheme due to only having access to derivatives of pˆ(z,·) in the E directions,
t z
we evaluate the differential above at the initial point instead of the target point
and use the approximation
eˆ
(θ):=(cid:88)k (cid:90) (cid:16)
Sθ,j (y)2−2Sθ,j (y)dylogpˆ(z,y)(σ
(z))(cid:17)
dPˆ (z,y),
ti,ti+t ti+t ti+t t j ti,ti+t
j=1 M×MSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 17
wherePˆ istheprobabilitymeasureonM×M of(Xˆ ,Xˆ ). Fort∈(0,δ),wehave
s,t s t
1 (cid:18) ⟨y−z−tτ (z),y−z−tτ (z)⟩ (cid:19)
0 0 g(z)
pˆ(z,y)=δ (y) exp − ,
t tτ0(z)+Ez (cid:112) (2πt)k(cid:81) λ 2t
i i
(cid:81)
where λ is the product of all non-zero eigenvalues of Σ(z). These functions
i i
have well-defined derivatives in the directions of E which yields
z
1
dylogpˆ(z,y)(σ (z))=− ⟨σ (z),y−z−tτ (z)⟩ .
t j t j 0 g(z)
Recalling now that Xˆ −Xˆ =σ(Xˆ )(W −W )+τ (Xˆ )t, we obtain
ti+t ti ti ti+t ti 0 ti
k (cid:20) (cid:21)
eˆ (θ)=(cid:88) Ex0 Sθ,j (Xˆ )2+ 2 ⟨(Sθ,j σ )(Xˆ ),Xˆ −Xˆ −tτ (Xˆ )⟩
ti,ti+t ti+t ti+t t ti+t j ti ti+t ti 0 ti g(Xti)
j=1
(cid:20)(cid:28) (cid:29) (cid:21)
2
=Ex0 Sθ (Xˆ ),Sθ (Xˆ )+ (W −W ) .
ti+t ti+t ti+t ti+t t ti+t ti
Rk
In summary, if we approximate the diffusion process (X ) with (Xˆ )
t t∈[0,T] t t∈[0,T]
which relies on the Euler–Maruyama scheme, then we can use as a loss function
Eˆ (θ) = (cid:80)n−1(cid:82)ti+1eˆ (θ)ds that tries to predict each step. However, since
0,T i=0 ti ti,s
each Euler step merely has positive probability on a proper subspace, we can only
predict the gradient of logpˆ(Xˆ ,Xˆ ) at the initial point Xˆ . We again use a
t ti ti+t ti
time-discretisation with n intervals and write δ = T. We generate K ×n random
n
vectors ∆ W(l) for i ∈ {1,...,n} as well as l ∈ {1,...,K} that are drawn from a
i
N(0,δI)-distribution in Rk. We define first X(l) =x and then iteratively
0 0
k
Xˆ(l) =Xˆ(l)+(cid:88) σ (Xˆ(l))∆ W(l),j +δτ (X(l))
i+1 i j i i+1 0 i
j=1
and compute the loss function by
K n
(5.1) Eˆ (θ)≈
1 (cid:88)(cid:88)(cid:68)
Sθ (Xˆ(l)),δSθ (Xˆ(l))+2∆
W(l)(cid:69)
0,T K iδ i iδ i i Rk
l=1 i=1
= Kδ (cid:88)K (cid:88)n (cid:13) (cid:13) (cid:13) (cid:13)Sθ iδ(Xˆ i(l))+ 1 δ∆ iW(l)(cid:13) (cid:13) (cid:13) (cid:13)2 +C.
l=1 i=1
Rk
WecanusethelossEˆ (θ)totrainaneuralnetworkbyapplyingAlgorithm1with
0,T
the loss E (θ) replaced by Eˆ (θ).
0,T 0,T
5.2. Taylor expansion and approximations of stochastic integrals. We im-
prove on the method in Section 5.1 by considering a more complicated approxima-
tion scheme for each step. Contrary to what is the case for the Euler–Maruyama
approximation, each step in our new approximation scheme gives us densities with
respecttoLebesguemeasurethataresmoothandpositive. Asatoolinthisnewap-
proximation,weusethestochasticTaylorexpansion. Wewilldescribetheresulting
approach in Section 5.5.
WefirstlookattheStratonovichTaylorexpansionforastochasticprocessfound
in[40,Chapter5andChapter10.7]. Let(X ) betheuniquestrongsolutionofthe
t t≥0
Stratonovich stochastic differential equation (4.6) with vector fields σ ,σ ,...,σ .
0 1 k
We use the convention W0 = t and write Wj = Wj −Wj. For a multi-index
t s,t t s18 E.GRONG,K.HABERMANN,S.SOMMER
α=(α ,...,α )∈{0,...,k}l forl∈N ,wesetl(α)=landweletn(α)denotethe
1 l 0
number of zeros in the multi-index α. We further define the differential operators
σ =σ σ ···σ
α α1 α2 αl
as well as the stochastic processes given by
(cid:90)
Jα = ◦dWα1 ◦···◦dWαl.
s,t t1 tl
s<t1<···<tl<t
Let (x1,...,xd) be a choice of local coordinate system for the manifold M and
define a function σ =(σi) with values in Rd such that
α α i
σi =σ xi,
α α
i.e.,thedifferentialoperatorσ appliedtothei-thcoordinatefunction. Forfurther
α
reference, we state below [40, Theorem 10.7.1 and Corollary 10.7.2].
Lemma 5.1 (Stratonovich Stochastic Taylor expansion). Let ⌊·⌋ denote the floor
function, i.e., ⌊a⌋ = max n∈Z,n≤an. Let A
γ
be the collection of all multi-indices α
satisfying l(α)+n(α) ≤ 2γ, and set A+ = {(β ,α) : α ∈ A ,β ∈ {0,1,...,k}}.
γ 1 γ 1
We say that a vector-valued function f satisfies (H ) if almost surely
α
(cid:90) T
(cid:13)
(cid:13)(cid:90)
(cid:13) (cid:13)2−δ0,αl
(H ) (cid:13) f(X )dWα1··· dWαl−1(cid:13) ds<∞.
α
0
(cid:13)
(cid:13) 0<t1<···<tl−1<s
t1 t1 tl−1 (cid:13)
(cid:13)
Let integers γ and n be given such that γ ≥ 0 and n > T. Set δ = T and t = iδ
n i
for i∈{0,1,...,n}, and define Xˆi iteratively by Xˆ0 =0 as well as
Xˆi+1 = (cid:88) Jα ·σ (Xˆi),
ti,ti+1 α
l(α)+n(α)≤2γ
which we extend to (Xˆ ) through
t t∈[0,T]
Xˆ = (cid:88) Jα ·σ (Xˆi), for t∈[0,δ].
ti+t ti,ti+t α
l(α)+n(α)≤2γ
If we assume that we have constants C ,C such that both
1 2
• ∥σ (x)−σ (y)∥ ≤C ∥x−y∥ for any α∈A , and
α α Rd 1 Rd γ
• σ is C1,1, satisfies condition (H ) and furthermore the inequality
α α
∥σ (x)∥ ≤C (1+∥x∥ )
α Rd 2 Rd
for any α∈A ∪A+,
γ γ
then we have the following bound, for some constant C,
(cid:18) (cid:19)
Ex0 sup ∥X t−Xˆ t∥
Rd
≤Cδγ.
0≤t≤T
LetusnowconsidertheresultofLemma5.1fortheparticularcasewhereγ =1.
For t∈[t ,t ], we obtain
i i+1
k k (cid:90) t
Xˆ =Xˆ +(t−t )σ (Xˆ )+(cid:88) Wj σ (Xˆ )+ (cid:88) Wj ◦dWlσ (Xˆ )
t ti i 0 ti ti,t j ti ti,s s (j,l) ti
j=1 j,l=1 ti
k
=Xˆ +(t−t )σ (Xˆ )+(cid:88) Wj σ (Xˆ )
ti i 0 ti ti,t j ti
j=1SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 19
k
+ 1 (cid:88) Wj Wl σ (Xˆ )+ (cid:88) Aj,l (σ −σ )(Xˆ ),
2 ti,t ti,t (j,l) ti ti,t (j,l) (l,j) ti
j,l=1 1≤j<l≤k
where Aj,l = 1(cid:82)t (Wj ◦dWl−Wl ◦dWj) is the so-called L´evy area.
s,t 2 s s,r r s,r r
We continue by discussing several methods for approximating this L´evy area.
One option, see e.g. [41, 47, 49] exploits the Fourier expansion of the involved
Brownian motions and eliminates a dependency to give, for j,l∈{1,...,k},
∞ ∞
(cid:88) (cid:16) (cid:17) (cid:88)
(5.2) Aj,l = a Wj −a Wl +π m(a b −a b )
t,t+h l,m t,t+h j,m t,t+h j,m l,m l,m j,m
m=1 m=1
in terms of independent Gaussian random variables with Wj ∼N(0,h) and
t,t+h
2 (cid:90) h(cid:16) s (cid:17) (cid:18) 2πms(cid:19) (cid:18) h (cid:19)
a = Wj − Wj cos ds∼N 0, ,
j,m h s h h h 2π2m2
0
2 (cid:90) h(cid:16) s (cid:17) (cid:18) 2πms(cid:19) (cid:18) h (cid:19)
b = Wj − Wj sin ds∼N 0, .
j,m h s h h h 2π2m2
0
We can then approximate the L´evy area by truncating the series in (5.2).
A second option which only requires the simulation of independent Gaussian
random variables makes use of the polynomial decomposition of Brownian motion
developed in [21, 30] and results in the expansion, see [20, 42],
∞
1(cid:16) (cid:17) 1 (cid:88)
(5.3) Aj,l = c Wj −c Wl + (c c −c c ),
t,t+h 2 l,1 t,t+h j,1 t,t+h 2 j,m l,m+1 j,m+1 l,m
m=1
with independent Gaussian random variables given by Wj ∼N(0,h) as well as
t,t+h
(cid:18) (cid:19)
h
c ∼N 0, .
j,m 2m+1
Truncatingtheseriesin(5.3)thenagainyieldsanapproximationfortheL´evyarea.
If we have a discretisation 0 = t < ··· < t = T with δ = t −t then we
0 n i+1 i
can obtain the bound in Lemma 5.1 with approximated L´evy area under appro-
priate bounds. If Aˆj,l is an approximation for the L´evy area Aj,l such that
t,t+h t,t+h
Ex0(sup ti≤t≤ti+1|Aj ti,l ,t−Aˆj ti,l ,t|) ≤ C 0δ3 on each interval [t i,t i+1], and if we replace
Aj,l by Aˆj,l in the definition of (Xˆ ) , then by [40, Corollary 10.7.3], we still
ti,t ti,t t t∈[0,T]
have
(cid:18) (cid:19)
Ex0 sup ∥X t−Xˆ t∥
Rd
≤Cδ.
0≤t≤T
Remark 5.2. Since we are working with local coordinates, we can identify vectors
v = (cid:80)d vi∂ with the point (v1,...,vd). Under this correspondence, we can
i=1 xi
identify σ with σ and σ with ∇¯ σ for ∇¯ defined by (4.5). It follows that Xˆ
j j (j,l) σj l t
for t∈[t ,t ] can be expressed as
i i+1
k
Xˆ =Xˆ +(t−t )σ (Xˆ )+(cid:88) Wj σ (Xˆ )
t ti i 0 ti ti,t j ti
j=1
k
+ 1 (cid:88) Wj Wl ∇¯ σ (Xˆ )+ (cid:88) Aj,l [σ ,σ ](Xˆ ).
2 ti,t ti,t σj l ti ti,t j l ti
j,l=1 1≤j<l≤k20 E.GRONG,K.HABERMANN,S.SOMMER
5.3. Heisenberg group. Let us consider the space M = R2k+1 with coordinates
q =(x,y,z) for x,y ∈Rk and z ∈R. We equip M with a multiplication rule such
that if q =(x,y,z) and q˜=(x˜,y˜,z˜), then
(cid:18) (cid:19)
1
ℓ (q)=q˜·q = x˜+x,y˜+y,z˜+z+ (⟨x˜,y⟩ −⟨x,y˜⟩ ) .
q˜ 2 Rk Rk
Here, ℓ denotes the left translation with respect to q˜, i.e., multiplication by q˜
q˜
on the left. Observe that this multiplication is not abelian. With respect to this
group structure, we have the identity element 0 = (0,0,0) and inverses given by
(x,y,z)−1 = (−x,−y,−z). We further define the sub-Riemanian structure (E,g)
on M where E is the rank 2k subbundle spanned by the vector fields
yj xj
σ =∂ − ∂ , τ =∂ + ∂ , j ∈{1,...,k},
j xj 2 z j yj 2 z
andthesub-Riemannianmetricgisdefineduniquelybyrequiring(σ ,τ ,...,σ ,τ )
1 1 k k
to be an orthonormal frame for E. The subbundle E is bracket-generating because
[σ ,τ ] = δ ∂ . We additionally observe that the above vector fields are left-
i j i,j z
invariant, meaning that, for any smooth function φ: M → R and for q ∈ M, we
have
σ (φ◦ℓ )=(σ φ)◦ℓ , τ (φ◦ℓ )=(τ φ)◦ℓ .
j q j q j q j q
As a consequence, if d denotes the sub-Riemannian distance on (M,E,g) and we
g
write f(q):=d (0,q) then
g
d (q˜,q)=d (0,q˜−1·q)=f(q˜−1·q).
g g
IfwechoosedµtobetheusualLebesguemeasureonR2k+1 thenthecorresponding
sub-Laplacian ∆=∆ on M =R2k+1 is given by
E,g,dµ
k k
(cid:88) (cid:88)
∆= σ2+ τ2.
j j
j=1 j=1
This sub-Laplacian ∆ is also left-invariant, i.e., for a smooth function φ: M → R
and q ∈M, we have ∆(φ◦ℓ )=(∆φ)◦ℓ . We now consider the stochastic process
q q
(X ) =(X0) onM withinfinitesimalgenerator 1∆andstartingfrom0∈M.
t t≥0 t t≥0 2
Alternatively, (X ) can be characterised as the unique strong solution to the
t t≥0
Stratonovich stochastic differential equation
k k
(cid:88) (cid:88)
(5.4) dX = σ (X )◦dWj + τ (X )◦dWk+j,
t j t t j t t
j=1 j=1
subject to X = 0 and where (W1,...,W2k) is a standard Brownian motion
0 t t t≥0
onR2k. Thestochasticdifferentialequation(5.4)subjecttoX =0hastheexplicit
0
solution existing for all times t≥0 and defined by
 
1(cid:88)k (cid:90) t
X
t
=W t1,...,W tk,W tk+1,...,W t2k,
2
(W sj ◦dW sk+j −W sk+j ◦dW sj)
j=1 0
 
k
(cid:88)
=W t1,...,W t2k, A 0j, ,k t+j .
j=1SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 21
If we take another initial point q ∈ M, then the solution (Xq) to (5.4) subject
t t≥0
to Xq =q is given by Xq =ℓ (X ) for t≥0. Furthermore, we have
0 t q t
 
k
(cid:88)
X
ti+t
=X
ti
·X
ti,ti+t
with X
ti,ti+t
=W t1 i,ti+t,...,W t2 ik ,ti+t, Aj ti,k ,t+ i+j t.
j=1
From the explicit expression above, we see that the stochastic process (X )
t t≥0
andtheassociateddensityp (0,q)=p (q)canbesimulatedusingapproximationsof
t t
theL´evyareadescribedinSection5.2. Wefurthernotethatsincethesub-Laplacian
∆ is left-invariant, we have p (q˜,q)=(p ◦ℓ )(q).
t t q˜−1
Letusnowlookatanexplicitformulaforp . Byadaptingtheexpressionderived
t
in [22] to our normalisation for the vector fields used to define the sub-Riemannian
structure (E,g), we obtain
4 (cid:90) ∞ (cid:18) 2λ (cid:19)k (cid:18) 4iλz ∥x∥2 +∥y∥2 (cid:19)
p (q)= exp −2λcoth(2λ) Rk Rk dλ.
t (2πt)k+1 sinh2λ t 2t
−∞
As further shown in [22], we then have the approximation, as t↓0,
d (0,q)2 f(q)2
logp (q)≈− g =− ,
t 2t 2t
and consequently
f(q˜−1·q)2 d (q˜,q)2
logp (q˜,q)=logp (q˜−1·q)≈− =− g .
t t 2t 2t
Unfortunately, computing the distance f explicitly is very expensive. However, it
is well known, see e.g. [1], that
(cid:113) (cid:112)
f(x,y,0)= ∥x∥2 +∥y∥2 and f(0,0,z)=2 π|z|.
Rk Rk
Define fˆ: M →[0,∞) by
fˆ(x,y,z)2 =f(x,y,0)2+f(0,0,z)2 =∥x∥2 +∥y∥2 +4π|z|.
Rk Rk
Following the proof of [26, Example 4.1], we also have f(x,y,0) ≤ f(x,y,z) and
f(0,0,z)≤2f(x,y,z). Hence, we obtain that
1 1
fˆ(x,y,z)2 ≤ max{f(x,y,0)2,f(0,0,z)2}
8 4
≤f(x,y,z)2 ≤(f(x,y,0)+f(0,0,z))2 ≤2fˆ(x,y,z)2.
If we further consider dˆ: M ×M →R given by dˆ(q˜,q)=fˆ(q˜−1·q), then dˆand d
g
are Lipschitz equivalent left-invariant metrics agreeing on the distance from 0 to
points on the z-axis and to points in the (x,y)-plane. We therefore introduce the
following score approximation
1 1
Sˆ(q˜,q):=− ∇q,Edˆ(q˜,q)2 =− ∇q,Efˆ(q˜−1·q)2.
t 2t 2t
We further note that
∇E(cid:16) fˆ2◦ℓ (cid:17) (q)=∇q,Efˆ(cid:0) q˜−1·q(cid:1)2
q˜−1
k (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) 1
= 2(xj −x˜j)−2π(yj −y˜j)sgn z−z˜− (⟨x˜,y⟩ −⟨x,y˜⟩ ) σ (q)
2 Rk Rk j
j=122 E.GRONG,K.HABERMANN,S.SOMMER
k (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) 1
+ 2(yj −y˜j)+2π(xj −x˜j)sgn z−z˜− (⟨x˜,y⟩ −⟨x,y˜⟩ ) τ (q).
2 Rk Rk j
j=1
ItfollowsthatifwritingSˆ(q˜,q)=(cid:80)k Sˆj(q˜,q)σ (q)+(cid:80)k Sˆk+j(q˜,q)τ (q)then,
t j=1 t j j=1 t j
for l∈{1,...,2k},
Sˆl(q˜,q)=Sˆl(0,q˜−1·q)=:Sˆl(q˜−1·q),
t t t
which are given by, for j ∈{1,...,k},
−tSˆj(q)=xj −πyjsgn(z) and −tSˆk+j(q)=yj +πxjsgn(z).
t t
Wenowcontinuewiththefollowingprocedure. Supposewehaveadiscretisation
0=t <t <···<t =T with t −t =δ = T. For each t∈(0,δ], we define
0 1 n i+1 i n
Xˆ =Xˆ ·Xˆ ,
ti+t ti ti,ti+t
with
 
k
Xˆ
ti,ti+t
=W t1 i,ti+t,...,W t2 ik ,ti+t,(cid:88) Aˆj ti,k ,t+ i+j t.
j=1
Letting Pˆ denote the probability measure on M ×M associated with (Xˆ ,Xˆ ),
s,t s t
we then define the approximation eˆ (θ) to e (θ) by
s,t s,t
(cid:90)
eˆ (θ)= ⟨Sθ(y),Sθ(y)−2Sˆ (z,y)⟩2 dP (z,y).
s,t t t t−s g(y) s,t
M×M
We write Sθ = (cid:80)k Sθ,jσ +(cid:80)k Sθ,k+jτ . In terms of Sˆ = (Sˆ1,...,Sˆ2k) as
t j=1 t j j=1 t j t t t
well as Sθ =(Sθ,1,...,Sθ,2k), we obtain
t t t
(cid:104)(cid:68) (cid:69) (cid:105)
eˆ (θ)=E Sθ (Xˆ ),Sθ (Xˆ )−2Sˆ (Xˆ )
ti,ti+t ti+t ti+t ti+t ti+t t ti,ti+t
R2k
(cid:104)(cid:68) (cid:69) (cid:105)
=E Sθ (ℓ Xˆ ),Sθ (ℓ Xˆ )−2Sˆ (Xˆ ) .
ti+t Xˆ
ti
ti,ti+t ti+t Xˆ
ti
ti,ti+t t ti,ti+t
R2k
We remark that
(cid:18) (cid:19) (cid:18) (cid:19)
1 x π −y 1
−Sˆ (q)= + sgn(z) =:− Sˆ(q).
t t y t x t
Using these observation in summary, we describe how Eˆ (θ) is presented with
0,T
adiscretisationof[0,T]forδ = T. WegenerateK·n·2k randomsamples∆ W(l),j
n i
from the distribution N(0,δ), and further K ·K ·n·2k numbers ∆ c(l) sampled
2 i j,m
from N(0, δ ). Here 1 ≤ i ≤ n, 1 ≤ j ≤ 2k, 1 ≤ l ≤ K and 1 ≤ m ≤ K . We
2m+1 2
then determine
1(cid:16) (cid:17)
∆ A(l),j,k+j = ∆ c(l) ∆ W(l),j −∆ c(l)∆ W(l),k+j
i 2 i k+j,1 i i j,1 i
+
1K (cid:88)2−1
(cid:16)
∆ c(l) ∆ c(l) −∆ c(l) ∆ c(l)
(cid:17)
2 i j,m i k+j,m+1 i j,m+1 i k+j,m
m=1
and set
k
∆ A(l) =(cid:88) ∆ A(l),j,k+j, ∆ Xˆ(l) =(∆ W(l),1,...,∆ W(l),2k,∆ A(l)).
i i i i i i
j=1SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 23
We further define Xˆ(l) =x and iteratively
0 0
Xˆ(l) =Xˆ(l)·∆ Xˆ(l).
i+1 i i+1
We compute the loss function by
K n
(5.5) Eˆ (θ)≈
1 (cid:88)(cid:88)(cid:68)
Sθ (Xˆ(l)),δSθ (Xˆ(l))−2Sˆ(∆
Xˆ(l))(cid:69)
0,T K iδ i iδ i i R2k
l=1 i=1
= Kδ (cid:88)K (cid:88)n (cid:13) (cid:13) (cid:13) (cid:13)Sθ iδ(Xˆ i(l))− 1 δSˆ(∆ iXˆ(l))(cid:13) (cid:13) (cid:13) (cid:13)2 +C
l=1 i=1
R2k
and can now apply Algorithm 1 with the loss E (θ) replaced by Eˆ (θ).
0,T 0,T
5.4. General case and adapted coordinates. Despite us using very particular
properties of the Heisenberg group in Section 5.3, it is possible to perform local
approximations for a general sub-Riemannian manifold in a similar manner.
Let (M,E,g) be a sub-Riemannian manifold where the manifold M has dimen-
sion d. Let p be the Dirichlet heat kernel for an operator 1L= 1∆+Z. We will
t 2 2
make use of small-time asymptotics, see [43, 44], saying that, for any x ,x ∈ M,
1 2
the transition density p satisfies the limit
t
(5.6) lim2tlogp (x ,x )=−d (x ,x )2.
t 1 2 g 1 2
t→0
In order to use this result, we need an approximation of the distance d .
g
Fortheremainingresultsinthissection,wefollowMontgomery[48,Chapter2.4].
We consider adapted coordinates, which are also called privileged coordinates in [5].
For a given point x ∈ M, we define a collection of integers k ,k ,...,k which
0 1 2 s
satisfy k = k ≤ k ≤ ··· ≤ k = d such that k is the rank of E, k is the
1 2 s 1 2
rank of the space spanned by vector fields in E along with their first order Lie
brackets at x , k is the rank of the space which also includes their second order
0 3
Lie brackets at x , and so on. Due to the bracket-generating assumption on E,
0
there exists some minimal s ∈ N such that k = d, which is the step of E. The
s
vector (k ,...,k ) is then called the growth vector of E at x . For instance, in
1 s 0
the (2k+1)-dimensional Heisenberg group, the growth vector is (2k,2k+1) at all
points. With the convention that k =0, we further define weights ν ,...,ν such
0 1 d
that ν for i∈{1,...,d} is the maximal number satisfying
i
k ≤i.
νi−1
In the Heisenberg group, we have (ν ,...,ν ,ν )=(1,...,1,2).
1 2k 2k+1
A coordinate system (y1,...,yd) centered at x is then called adapted at x if,
0 0
for any l≤ν −1 and for any selection of vector fields V ,...,V in E,
i 1 l
(cid:0)
V ...V
yi(cid:1)
(x )=0.
1 l 0
Note that the condition for ν = 1 is already satisfied for all coordinates because
i
y(x )=0 by assumption. Relative to an adapted coordinate system, we define the
0
box metric d by, for x and x in the domain of the coordinate system y,
box 1 2
(cid:110) (cid:111)
d (x ,x )= max |yi(x )−yi(x )|1/νi .
box 1 2 1 2
1≤i≤d
The ball box theorem says, for all x in the domain of y and for constants c and C,
cd (x ,x)≤d (x ,x)≤Cd (x ,x).
box 0 g 0 box 024 E.GRONG,K.HABERMANN,S.SOMMER
In order to have a smoother metric, we instead consider dˆdefined by
d
dˆ(x ,x )2 =(cid:88) |yi(x )−yi(x )|2/νi.
1 2 1 2
i=1
Thepreviousinequalitiesimplythattherearepositiveconstantsc˜andC˜ suchthat
(5.7) c˜dˆ(x ,x)2 ≤d (x ,x)2 ≤C˜dˆ(x ,x)2.
0 g 0 0
Motivated by (5.6) and (5.7), we introduce the approximation
dˆ(x ,x)2
logp (x ,x)≈− 0 ,
t 0 2t
leading to our final approximated score
1
Sˆ(x ,x):=− ∇x,Edˆ(x ,x)2.
t 0 2t 0
We conclude that if (σ ,...,σ ) is an orthonormal frame for E with respect to
1 k
g and we write σ = (cid:80)d σi∂ for j ∈ {1,...,k}, then our approximation of the
j i=1 j yi
score is given by
(5.8) Sˆ(x
,x)=−1
∇x,Edˆ(x ,x)2
=−1(cid:88)k (cid:32) (cid:88)d σi(x)yi|yi|2/νi−2(cid:33)
σ (x).
t 0 2t 0 t j ν j
i
j=1 i=1
Remark 5.3. Note that, for any selection of strictly positive constants c1,...,cd,
the metric dˆc defined by
d
dˆc(x ,x )2 =(cid:88) ci|yi(x )−yi(x )|2/νi
1 2 1 2
i=1
willalsosatisfy(5.7). Hence,foreachcase,wecanscalethismetrictomakeitmatch
the true sub-Riemannian metric close to x for each coordinate, as we did for the
0
HeisenberggroupinSection5.3. Arecommendedmethodforfindingagoodchoice
of c1,...,cd is to compute explicit lengths along each coordinate axis according to
the sub-Riemannian metric of the nilpotent homogeneous approximation at x , as
0
described in [5, Section 5.3].
5.5. Simulationofastepinthegeneralgeometry. Forsimulatingthestochas-
tic process (X ) in the general sub-Riemannian setting, we use the stochastic
t t∈[0,T]
Taylor expansion discussed in Section 5.2 and the adapted coordinates reviewed in
Section 5.4. For the sake of simplicity, we reduce our considerations to the case
where the bracket-generating distribution E has step 2, that is, where the tangent
bundle TM is spanned by E and its first order Lie brackets.
As before, let (σ ,...,σ ) be an orthonormal frame for E, let (W ) be a
1 k t t≥0
standard Brownian motion on Rk and assume that (X ) is the unique strong
t t∈[0,T]
solution to the Stratonovich stochastic differential equation
k
(cid:88)
dX = σ (X )◦dWj +σ (X )dt, X =x .
t j t t 0 t 0 0
j=1
Additionally consider vector fields σ ,...,σ on M such that (σ ,...,σ ) is
k+1 d 1 d
a frame for the full tangent bundle TM. Let (x1,...,xd) be any choice of local
coordinate system for M around the point x = (x1,...,xd) ∈ M. We introduce
0 0 0SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 25
a matrix-valued function σ = (σi) such that σ = (cid:80)d σi∂ for l ∈ {1,...,d},
l l i=1 l xi
and we denote its inverse by σ−1 = (σi ). We then define a new local coordinate
−l
system (y1,...,yd) for M centred at x by setting
0
d
(cid:88)
(5.9) yi = σi (x )(xl−xl).
−l 0 0
l=1
This yields an adapted coordinate system at x . Letting (Y ) defined by
0 t t∈[0,T]
Y =y(X )betheprocessinthisadaptedcoordinatesystemandtakingtheTaylor
t t
expansion of order 1, we obtain that, for t ∈ [0,δ] and i ∈ {1,...,k} as well as
m∈{k+1,...,d},
d k (cid:18)(cid:90) t (cid:19) d
Yi ≈Wi+(cid:88) (cid:88) Wj1 ◦dWj2 σi (x )(∇¯ σ )l(x )+(cid:88) tσi (x )σl(x )
t t s s −l 0 σj1 j2 0 −l 0 0 0
l=1j1,j2=1 0 l=1
as well as
d k (cid:18)(cid:90) t (cid:19) d
Ym ≈(cid:88) (cid:88) Wj1 ◦dWj2 σm(x )(∇¯ σ )l(x )+(cid:88) tσm(x )σl(x ).
t s s −l 0 σj1 j2 0 −l 0 0 0
l=1j1,j2=1 0 l=1
Returning to our original coordinate system (x1,...,xd) and identifying vectors
in the basis (∂ ,...,∂ ) with points in the domain of (x1,...,xd), we get the
x1 xd
approximation
k k (cid:18)(cid:90) t (cid:19)
X −x ≈(cid:88) Wjσ (x )+ (cid:88) Wj1 ◦dWj2 (∇¯ σ )(x )+tσ (x ).
t 0 t j 0 s s σj1 j2 0 0 0
j=1 j1,j2=1 0
We now have the following beneficial proposition.
Proposition 5.4. Suppose that span{σ ,[σ ,σ ] : j,l ∈ {1,...,k}} equals the full
j j l
tangentspaceateverypointonM. Foragivenpointx ∈M,wedefineastochastic
0
process (Xˆ ) by Xˆ =x and
t t≥0 0 0
k k (cid:90) t
(5.10) Xˆ −x =(cid:88) σ (x )Wj + (cid:88) (∇¯ σ )(x ) Wj1 ◦dWj2 +σ (x )t.
t 0 j 0 t σj1 j2 0 s s 0 0
j=1 j1,j2=1 0
Then Xˆ for t>0 has a positive smooth density with respect to Lebesgue measure.
t
We remark that by using the identity (cid:82)t Wj1 ◦dWj2 = 1Wj1Wj2 +Aj1,j2 we
0 s s 2 t t 0,t
can rewrite the expression (5.10) as
k k
Xˆ −x =(cid:88) σ (x )Wj + 1 (cid:88) (∇¯ σ )(x )Wj1Wj2
t 0 j 0 t 2 σj1 j2 0 t t
j=1 j1,j2=1
(cid:88)
+ [σ ,σ ](x )Aj1,j2 +σ (x )t.
j1 j2 0 0,t 0 0
1≤j1<j2≤k
Proof of Proposition 5.4. We consider the space Rk ×Rd with coordinates (w,x)
and we introduce the vector fields, for j ∈{1,...,k},
d k d
σˆ (w,x)=∂ +(cid:88) σi(x )∂ + (cid:88) (cid:88) wj1(∇¯ σ )i(x )∂
j wj j 0 xi σj1 j 0 xi
i=1 j1=1i=126 E.GRONG,K.HABERMANN,S.SOMMER
k
=∂ +σ (x )+ (cid:88) wj1(∇¯ σ )(x ).
wj j 0 σj1 j 0
j1=1
We notice that, for j,l∈{1,...,k} and at any point (w,x)∈Rk×Rd,
d
(cid:88)
[σˆ ,σˆ ](w,x)= [σ ,σ ]i(x )∂ .
j l j l 0 xi
i=1
LetusnowconsiderthesetO ⊆Rk×Rdofpointsthatcanbereachedfrom(0,x )
0
bymovingtangentiallytothesubbundleEˆ =span{σˆ :j ∈{1,...,k}},andfurther
j
define the constant vector space R = span{(cid:80)d [σ ,σ ]i(x )∂ } . By
i=1 j1 j2 0 xi 1≤j1<j2≤k
the Orbit theorem [64], the set O is a manifold of dimension k +rankR, whose
tangent space is given by
T O =Eˆ ⊕R , (w,x)∈O.
(w,x) (w,x) (w,x)
Let π: Rk ×Rd → Rd be the projection on the second factor. By our assumption
on the brackets, we have π (Eˆ ⊕R )=T Rd for any (w,x)∈Rk×Rd. It
∗ (w,x) (w,x) x
follows that π| : O → Rd is a surjective submersion with each fiber π−1(x) being
O
an embedded manifold.
Weobservethatπ maps(kerπ )⊥ bijectivelytoT Rd. Hence,wecan
∗,(w,x) ∗,(w,x) x
define a vector field σˆ on Rk×Rd determined uniquely by being in (kerπ )⊥ and
0 ∗
satisfying π σˆ (w,x) = σ (x ), where we identify the vector σ(x ) and the
∗,(w,x) 0 0 0 0
corresponding constant vector field.
Define a stochastic process (Φ ) on Rk×Rd as the unique strong solution to
t t≥0
k
(cid:88)
dΦ = σˆ (Φ )◦dWj +σˆ (Φ )dt, Φ =(0,x ).
t j t t 0 t 0 0
j=1
SinceEˆ| isbracket-generatingasasubbundleofTO,wededucefrom[33]and[62]
O
thatΦ fort>0hasapositivesmoothdensityφ withrespecttoLebesguemeasure
t t
dµO on O. Furthermore, by definition we have π(Φ ) = Xˆ for t ≥ 0. Hence, the
t t
density pˆ of Xˆ given by pˆ(x) = (cid:82) φ (w,x)dµO| is the average of φ
t t t π−1(x) t π−1(x) t
on every fiber, which is then also smooth and positive for t>0. □
5.6. General summary. To summarise the above results, assume that (E,g) is
a step 2 sub-Riemannian structure with an orthonormal frame (σ ,...,σ ) for E,
1 k
extended with further vector fields σ ,...,σ to a frame for the tangent bundle
k+1 d
TM, which in local coordinates (x1,...,xd) is represented as a matrix (σi) with
j
inverse (σi ). Let us divide [0,T] into n intervals of length δ = T and set t = iδ
−j n i
for i ∈ {0,...,n}. We then define an approximation (Xˆ ) to (X ) by
t t∈[0,T] t t∈[0,T]
Xˆ =x as well as, for t∈(0,δ] and i∈{0,...,n−1},
0 0
k k
Xˆ =Xˆ +(cid:88) σ (Xˆ )Wj + 1 (cid:88) (∇¯ σ )(Xˆ )Wj1 Wj2
ti+t ti j ti ti,ti+t 2 σj1 j2 ti ti,ti+t ti,ti+t
j=1 j1,j2=1
+ (cid:88) [σ ,σ ](Xˆ )Aˆj1,j2 +σ (Xˆ )t,
j1 j2 ti ti,ti+t 0 ti
1≤j1<j2≤k
where Aˆj1,j2 is a truncation of (5.2) or (5.3), respectively. We apply this model
ti,ti+t
to sample from the density pˆ(x ,·) of Xˆ .
t 0 tSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 27
Moreover, we need a corresponding loss function for the score. For this, we use
the adapted coordinates defined in (5.9) for the approximation (5.8) of the score,
that is, with
k d
fˆ(x)2 =dˆ(x ,x)2 =(cid:88) |yi|2+ (cid:88) |yi|
0
i=1 i=k+1
(cid:88)k (cid:32) (cid:88)d (cid:33)2 (cid:88)d (cid:12) (cid:12)(cid:88)d (cid:12) (cid:12)
= σi (x )(xl−xl) + (cid:12) σi (x )(xl−xl)(cid:12),
−l 0 0 (cid:12) −l 0 0 (cid:12)
(cid:12) (cid:12)
i=1 l=1 i=k+1 l=1
we take Sˆ(x ,x)=−1∇Efˆ(x)2. Writing
t 0 2t
k k
Sˆ(x ,x)=−1 (cid:88) σ (fˆ2)(x)σ (x)=(cid:88) Sˆj(x ,x)σ (x),
t 0 2t j j t 0 j
j=1 j=1
we then have
k d (cid:32) d (cid:33)
Sˆj(x ,x)=−1(cid:88)(cid:88) σl2(x)σi (x ) (cid:88) σi (x )(xl−xl)
t 0 t j −l2 0 −l 0 0
i=1l2=1 l=1
d d (cid:32) d (cid:33)
1 (cid:88) (cid:88) (cid:88)
− σl2(x)σi (x )sgn σi (x )(xl−xl) .
2t j −l2 0 −l 0 0
i=k+1l2=1 l=1
Weremarkthattheexpressionabovediffersfrom(5.8)inthattherethecoordinate
expressions σi for the vector fields are defined relative to the adapted coordinate
j
system (y1,...,yd), whereas here these are relative to a given coordinate system
(x1,...,xd). A particular advantage of this formulation is that we do not need to
change coordinate system if we change x .
0
WenowdefinethelossfunctionEˆ (θ)fortheneuralnetworkSθ =(cid:80)k Sθ,jσ
0,T j=1 j
using Sθ =(Sθ,1,...,Sθ,k) and Sˆ =(Sˆ1,...,Sˆk) by
(cid:90) T (cid:104)(cid:68) (cid:69) (cid:105)
Eˆ (θ)= Ex0 Sθ(Xˆ ),Sθ(Xˆ )−2Sˆ (x ,Xˆ ) dt
0,T t t t t t 0 t
0
Rk
(cid:90) T (cid:20)(cid:13) (cid:13)2 (cid:21)
= Ex0 (cid:13)Sθ(Xˆ )−Sˆ (x ,Xˆ )(cid:13) dt+C.
(cid:13) t t t 0 t (cid:13)
0
Rk
6. Experiments
We now exemplify the bridge simulation code for the Heisenberg group whose
sub-Riemannian structure is described in Section 5.3. We use an Euler–Heun to
simulate the unconditioned Brownian motion, and we employ this to train score
approximators using the denoising loss (5.5) and the divergence loss (4.3). Code
for representing the Heisenberg geometry, geometric computations, and training of
the score approximations is available in the Jax Geometry library1.
Theneuralnetusedisa3-layerdensenetworkwith15nodesineachlayerandex-
ponentiallinearunits(ELU)activationfunctions. Forthelossfunctionsdepending
on the divergence, it is important that the activation functions are differentiable.
In contrast, with the denoising loss, the ELU can be replaced with e.g. rectified
1https://github.com/computationalevolutionarymorphometry/jaxgeometry/28 E.GRONG,K.HABERMANN,S.SOMMER
linear units (ReLU). The nets are trained for 2500 epochs with batch size of 64
sample paths and 8 batches per epoch.
In Figure 2, we sample a single sample path (blue) starting at x =(0.5,0,0.8)
0
and conditioned on hitting x =(0,0,0) at T =0.1. For comparison, the minimis-
T
ing geodesic (red) between the two points x and x is included in the figure. We
0 T
additionally plot the norm of the (x,y)-component and the z-component, respec-
tively, as a function of t∈[0,0.1], particularly to illustrate the progression of the z
coordinate from its starting value towards 0.
Figure 2. Left: Sample path from bridge starting at (0.5,0,0.8)
conditioned on hitting (0,0,0) at T = 0.1 (blue curve). Geodesic
between the same points corresponding to the limit T → 0 (red
curve). Right: Norm of (x,y)-component (red) and z-component
(blue) for the sample path as a function of t∈[0,0.1].
Since we know from short-time asymptotics established in [4] for diffusion pro-
cesses on sub-Riemannian manifolds that sample paths should concentrate around
ageodesicifwereducetheconditioningtimetowards0,weplotinFigure3median
curves and quartiles with conditioning for T = 0.1, T = 0.2, T = 0.5 and T = 1,
respectively. The concentration effect is clearly visible.
Finally, Figure 4 shows plots of the estimated score fields in the unit cube in R3
aswellasevaluatedonlyonthe(x,y)-sliceoftheunitcubesatisfyingz =0.2. The
figure shows estimated score fields with the denoising loss (5.5) (top row) and the
divergence loss (4.3) (bottow row). For values close to the origin, the estimated
scores appear visually close. Our experience is that training with the divergence
loss is in practice less stable than using the denoising loss which is exemplified in
the figure when the score is evaluated for higher values of z where the stochastic
process has lower probability density and the training therefore is performed with
fewer samples.
References
[1] A.Agrachev,D.Barilari,andU.Boscain.AComprehensiveIntroductiontosub-Riemannian
Geometry,volume181ofCambridgeStudiesinAdvancedMathematics.CambridgeUniversity
Press,Cambridge,2020.FromtheHamiltonianviewpoint,withanappendixbyIgorZelenko.
[2] A.Arnaudon,F.vanderMeulen,M.Schauer,andS.Sommer.Diffusionbridgesforstochastic
Hamiltoniansystemsandshapeevolutions.SIAM J. Imaging Sci.,15(1):293–323,2022.SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 29
Figure 3. Mean (x,y)-component and z-component norms over
100 samples with quartiles for T = 0.1, T = 0.2, T = 0.5 and
T =1, respectively.
[3] I. Bailleul, L. Mesnager, and J. Norris. Small-time fluctuations for the bridge of a sub-
Riemanniandiffusion.Ann. Sci. E´c. Norm. Sup´er. (4),54(3):549–586,2021.
[4] I. Bailleul and J. Norris. Diffusion in small time in incomplete sub-Riemannian manifolds.
Anal. PDE,15(1):63–84,2022.
[5] A.Bella¨ıche.Thetangentspaceinsub-Riemanniangeometry.InSub-Riemanniangeometry,
pages1–78.Springer,1996.
[6] A.Beskos,O.Papaspiliopoulos,G.O.Roberts,andP.Fearnhead.Exactandcomputationally
efficient likelihood-based estimation for discretely observed diffusion processes. J. R. Stat.
Soc.Ser.BStat.Methodol.,68(3):333–382,2006.Withdiscussionsandareplybytheauthors.
[7] A.Beskos,G.Roberts,A.Stuart,andJ.Voss.MCMCmethodsfordiffusionbridges.Stoch.
Dyn.,8(3):319–350,2008.
[8] K. Bichteler and J. Jacod. Calcul de Malliavin pour les diffusions avec sauts: existence
d’une densit´e dans le cas unidimensionnel. In S´eminaire de Probabilit´es, XVII, volume 986
ofLecture Notes in Math.,pages132–157.Springer,Berlin,1983.
[9] J.Bierkens,S.Grazzi,F.vanderMeulen,andM.Schauer.ApiecewisedeterministicMonte
Carlomethodfordiffusionbridges.Stat. Comput.,31(3):PaperNo.37,21,2021.
[10] J. Bierkens, F. van der Meulen, and M. Schauer. Simulation of elliptic and hypo-elliptic
conditionaldiffusions.Adv. in Appl. Probab.,52(1):173–212,2020.
[11] M.Bladt,S.Finch,andM.Sørensen.Simulationofmultivariatediffusionbridges.J.R.Stat.
Soc. Ser. B. Stat. Methodol.,78(2):343–369,2016.
[12] V.D.Bortoli,E.Mathieu,M.J.Hutchinson,J.Thornton,Y.W.Teh,andA.Doucet.Rie-
mannian Score-Based Generative Modelling. In Advances in Neural Information Processing
Systems,May2022.30 E.GRONG,K.HABERMANN,S.SOMMER
Figure 4. Left: Estimated score field for x,y,z ∈ [−1,1]. Right:
Estimated score for x,y ∈ [−1,1] and z = 0.2. Top row: Training
with denoising loss (5.5). Bottom row: Training with divergence
loss (4.3). Arrow lengths scaled for visualisation, and arrows col-
ored according to lengths.
[13] M. N. Bui, Y. Pokern, and P. Dellaportas. Inference for partially observed Riemannian
Ornstein-Uhlenbeckdiffusionsofcovariancematrices.Bernoulli,29(4):2961–2986,2023.
[14] W.-L. Chow. U¨ber Systeme von linearen partiellen Differentialgleichungen erster Ordnung.
Math. Ann.,117:98–105,1939.
[15] M.Corstanje,F.vanderMeulen,M.Schauer,andS.Sommer.Simulatingconditioneddiffu-
sionsonmanifolds,2024.arXiv:2403.05409.
[16] B.DelyonandY.Hu.Simulationofconditioneddiffusionandapplicationtoparameteresti-
mation.Stochastic Process. Appl.,116(11):1660–1675,2006.
[17] G.B.DurhamandA.R.Gallant.Numericaltechniquesformaximumlikelihoodestimation
of continuous-time diffusion processes. J. Bus. Econom. Statist., 20(3):297–338, 2002. With
commentsandareplybytheauthors.
[18] K.D.Elworthy.StochasticDifferentialEquationsonManifolds,volume70ofLondonMath-
ematical Society Lecture Note Series. Cambridge University Press, Cambridge-New York,
1982.
[19] M. E´mery. Stochastic Calculus in Manifolds. Universitext. Springer-Verlag, Berlin, 1989.
WithanappendixbyP.-A.Meyer.SCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 31
[20] J.FosterandK.Habermann.BrownianbridgeexpansionsforL´evyareaapproximationsand
particular values of the Riemann zeta function. Combin. Probab. Comput., 32(3):370–397,
2023.
[21] J.Foster,T.Lyons,andH.Oberhauser.AnoptimalpolynomialapproximationofBrownian
motion.SIAM J. Numer. Anal.,58(3):1393–1421,2020.
[22] B.Gaveau.Principedemoindreaction,propagationdelachaleuretestim´eessouselliptiques
surcertainsgroupesnilpotents.Acta Math.,139(1-2):95–153,1977.
[23] A. Golightly and D. J. Wilkinson. Bayesian sequential inference for nonlinear multivariate
diffusions.Stat. Comput.,16(4):323–338,2006.
[24] A.GolightlyandD.J.Wilkinson.LearningandInferenceinComputationalSystemsBiology,
chapterMarkovchainMonteCarloalgorithmsforSDEparameterestimation,pages253–276.
MITPress,2010.
[25] A.Grigor’yan.StochasticcompletenessofsymmetricMarkovprocessesandvolumegrowth.
Rend. Semin. Mat. Univ. Politec. Torino,71(2):227–237,2013.
[26] E. Grong, T. Nilssen, and A. Schmeding. Geometric rough paths on infinite dimensional
spaces.J. Differential Equations,340:151–178,2022.
[27] E. Grong and A. Thalmaier. Stochastic completeness and gradient representations for sub-
Riemannianmanifolds.Potential Anal.,51(2):219–254,2019.
[28] K.Habermann.Small-timefluctuationsforsub-Riemanniandiffusionloops.Probab. Theory
Related Fields,171(3-4):617–652,2018.
[29] K.Habermann.Small-timefluctuationsforthebridgeinamodelclassofhypoellipticdiffu-
sionsofweakH¨ormandertype.Electron. J. Probab.,24:PaperNo.11,19,2019.
[30] K.Habermann.AsemicirclelawanddecorrelationphenomenaforiteratedKolmogorovloops.
J. Lond. Math. Soc. (2),103(2):558–586,2021.
[31] U. G. Haussmann and E. Pardoux. Time reversal of diffusions. Ann. Probab., 14(4):1188–
1205,1986.
[32] J.Heng,V.DeBortoli,A.Doucet,andJ.Thornton.Simulatingdiffusionbridgeswithscore
matching,2022.arXiv:2111.07243.
[33] L. H¨ormander. Hypoelliptic second order differential equations. Acta Math., 119:147–171,
1967.
[34] E.P.Hsu.StochasticAnalysisonManifolds,volume38ofGraduateStudiesinMathematics.
AmericanMathematicalSociety,Providence,RI,2002.
[35] C.-W.Huang,M.Aghajohari,J.Bose,P.Panangaden,andA.Courville.RiemannianDiffu-
sionModels.InAdvances in Neural Information Processing Systems,May2022.
[36] A.Hyv¨arinen.Estimationofnon-normalizedstatisticalmodelsbyscorematching.J. Mach.
Learn. Res.,6:695–709,2005.
[37] M. H. Jensen, S. Joshi, and S. Sommer. Discrete-Time Observations of Brownian Motion
on Lie groups and Homogeneous Spaces: Sampling and Metric Estimation. Algorithms,
15(8):290,2022.
[38] M. H. Jensen, A. Mallasto, and S. Sommer. Simulation of conditioned diffusions on the
flat torus. In Geometric Science of Information: 4th International Conference, GSI 2019,
Toulouse, France, August 27–29, 2019, Proceedings 4,pages685–694.Springer,2019.
[39] M. H. Jensen and S. Sommer. Simulation of conditioned semimartingales on Riemannian
manifolds,2021.arXiv:2105.13190.
[40] P. E. Kloeden and E. Platen. Numerical solution of stochastic differential equations, vol-
ume23ofApplications of Mathematics.Springer-Verlag,Berlin,1992.
[41] P.E.Kloeden, E.Platen, andI.W.Wright.Theapproximationofmultiplestochasticinte-
grals.Stochastic Anal. Appl.,10(4):431–441,1992.
[42] D. F. Kuznetsov. A method of expansion and approximation of repeated stochastic
StratonovichintegralsbasedonmultipleFourierseriesonfullorthonormalsystems.[InRus-
sian].Electronic Journal “Differential Equations and Control Processes”,1:18–77,1997.
[43] R.L´eandre.Majorationentempspetitdeladensit´ed’unediffusiond´eg´en´er´ee.Probab.Theory
Related Fields,74(2):289–294,1987.
[44] R. L´eandre. Minoration en temps petit de la densit´e d’une diffusion d´eg´en´er´ee. J. Funct.
Anal.,74(2):399–414,1987.
[45] M.Lin,R.Chen,andP.Mykland.OngeneratingMonteCarlosamplesofcontinuousdiffusion
bridges.J. Amer. Statist. Assoc.,105(490):820–838,2010.32 E.GRONG,K.HABERMANN,S.SOMMER
[46] M.Mider,M.Schauer,andF.vanderMeulen.Continuous-discretesmoothingofdiffusions.
Electron. J. Stat.,15(2):4295–4342,2021.
[47] G. N. Milstein. Numerical integration of stochastic differential equations, volume 313.
SpringerScience&BusinessMedia,2013.
[48] R.Montgomery.A tour of subriemannian geometries, their geodesics and applications,vol-
ume 91 of Mathematical Surveys and Monographs. American Mathematical Society, Provi-
dence,RI,2002.
[49] J. Mrongowius and A. Ro¨ß ler. On the approximation and simulation of iterated stochastic
integralsandthecorrespondingL´evyareasintermsofamultidimensionalBrownianmotion.
Stoch. Anal. Appl.,40(3):397–425,2022.
[50] M.Muniz,M.Ehrhardt,M.Gu¨nther,andR.Winkler.Higherstrongordermethodsforlinear
ItˆoSDEsonmatrixLiegroups.BIT,62(4):1095–1119,2022.
[51] R.W.NeelandL.Sacchelli.Uniform,localizedasymptoticsforsub-Riemannianheatkernels
anddiffusions,2020.arXiv:2012.12888.
[52] J. Norris. Simplified Malliavin calculus. In S´eminaire de Probabilit´es, XX, volume 1204 of
Lecture Notes in Math.,pages101–130.Springer,Berlin,1986.
[53] O. Papaspiliopoulos and G. Roberts. Importance sampling techniques for estimation of dif-
fusion models. In Statistical methods for stochastic differential equations, volume 124 of
Monogr. Statist. Appl. Probab.,pages311–340.CRCPress,BocaRaton,FL,2012.
[54] O. Papaspiliopoulos, G. O. Roberts, and O. Stramer. Data augmentation for diffusions. J.
Comput. Graph. Statist.,22(3):665–688,2013.
[55] M. J. Piggott and V. Solo. Geometric Euler–Maruyama schemes for stochastic differential
equationsinSO(n)andSE(n).SIAM J. Numer. Anal.,54(4):2490–2516,2016.
[56] P. K. Rashevski˘ı. On the connectability of two arbitrary points of a totally nonholonomic
spacebyanadmissiblecurve.Uchen.Zap.Mosk.Ped.Inst.Ser.Fiz.-Mat.Nauk,3(2):83–94,
1938.
[57] S.S¨arkk¨aandA.Solin.Appliedstochasticdifferentialequations,volume10.CambridgeUni-
versityPress,2019.
[58] M. Schauer, F. van der Meulen, and H. van Zanten. Guided proposals for simulating multi-
dimensionaldiffusionbridges.Bernoulli,23(4A):2917–2950,2017.
[59] S.Sommer,A.Arnaudon,L.Kuhnel,andS.Joshi.BridgeSimulationandMetricEstimation
onLandmarkManifolds.InGraphsinBiomedicalImageAnalysis,ComputationalAnatomy
and Imaging Genetics,pages79–91.Springer,2017.
[60] Y.SongandS.Ermon.Generativemodelingbyestimatinggradientsofthedatadistribution.
InAdvances in Neural Information Processing Systems,2019.
[61] Y. Song and S. Ermon. Improved techniques for training score-based generative models. In
Advances in Neural Information Processing Systems,2020.
[62] D. W. Stroock and S. R. S. Varadhan. On the support of diffusion processes with applica-
tionstothestrongmaximumprinciple.InProceedings of the Sixth Berkeley Symposium on
Mathematical Statistics and Probability, Vol. III: Probability theory, pages 333–359. Univ.
CaliforniaPress,Berkeley,California,1972.
[63] A.M.Stuart,J.Voss,andP.Wiberg.FastcommunicationconditionalpathsamplingofSDEs
andtheLangevinMCMCmethod.Commun. Math. Sci.,2(4):685–697,2004.
[64] H. J. Sussmann. Orbits of families of vector fields and integrability of distributions. Trans.
Amer. Math. Soc.,180:171–188,1973.
[65] F. van der Meulen and M. Schauer. Bayesian estimation of discretely observed multi-
dimensional diffusion processes using guided proposals. Electron. J. Stat., 11(1):2358–2396,
2017.
[66] P.Vincent.Aconnectionbetweenscorematchinganddenoisingautoencoders.NeuralCom-
put.,23(7):1661–1674,2011.
[67] G. A. Whitaker, A. Golightly, R. J. Boys, and C. Sherlock. Improved bridge constructs for
stochasticdifferentialequations.Stat. Comput.,27(4):885–900,2017.
Erlend Grong, University of Bergen, Department of Mathematics, P.O. Box 7803,
5020 Bergen, Norway.
Email address: erlend.grong@uib.noSCORE MATCHING FOR SUB-RIEMANNIAN BRIDGE SAMPLING 33
KarenHabermann,DepartmentofStatistics,UniversityofWarwick,Coventry,CV4
7AL, United Kingdom.
Email address: karen.habermann@warwick.ac.uk
Stefan Sommer, Department of Computer Science, University of Copenhagen, Uni-
versitetsparken 5, DK-2100 Copenhagen E, Denmark.
Email address: sommer@di.ku.dk