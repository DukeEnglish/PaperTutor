XFT: Unlocking the Power of Code Instruction Tuning
by Simply Merging Upcycled Mixture-of-Experts
YifengDing,JiaweiLiu,YuxiangWei,TerryYueZhuo,LingmingZhang
UniversityofIllinoisUrbana-Champaign
{yifeng6, lingming}@illinois.edu
Abstract Dense Dense Dense
LLM LLM LLM
We introduce XFT, a simple yet powerful
trainingscheme,bysimplymergingupcycled Expert … Expert
Mixture-of-Experts(MoE)tounleashtheper-
Mixture-of-Experts
formancelimitofinstruction-tunedcodeLarge Expert … Expert
Language Models (LLMs). While vanilla Dense Mixture-of-Experts Dense
sparse upcycling fails to improve instruction LLM LLM
tuning,XFTintroducesasharedexpertmecha- SFT Sparse Upcycling X FT (Ours)
nismwithanovelroutingweightnormalization Fine-Tuning Learned Merging
strategy into sparse upcycling, which signif- Figure1:OverviewofSFT,sparseupcycling,andXFT.
icantly boosts instruction tuning. After fine-
tiondatasetofinstruction-outputpairs,wherethe
tuning the upcycled MoE model, XFT intro-
ducesalearnablemodelmergingmechanism instruction reflects human intents in natural lan-
tocompiletheupcycledMoEbacktoadense guageandtheoutputincludesexplainedcodesnip-
model,achievingupcycledMoE-levelperfor- pets that correspond to the intent; and (ii) super-
mance with only dense-model compute. By vised fine-tuning of pre-trained LLM on the in-
applying XFT to a 1.3B model, we create a
struction dataset. In the realm of code, multiple
newstate-of-the-arttinycodeLLM(<3B)with
instruction-tuning methods have been proposed
67.1and64.6pass@1onHumanEvalandHu-
to curate high-quality instruction datasets. For
manEval+respectively. Withthesamedataand
example, Code Evol-Instruct (Luo et al., 2023)
modelarchitecture,XFTimprovessupervised
fine-tuning (SFT) by 13% on HumanEval+, uses ChatGPT to obtain complex synthetic code
alongwithconsistentimprovementsfrom2% instructions with heuristic prompts, while OSS-
to13%onMBPP+,MultiPL-E,andDS-1000, INSTRUCT(Weietal.,2023)promptsChatGPTto
demonstrating its generalizability. XFT is generatenewcodingproblemsbydrawinginspira-
fully orthogonal to existing techniques such
tionfromopensourcecodesnippets. Whileexist-
as Evol-Instruct and OSS-INSTRUCT, open-
ingworkfocusesonthedataperspectivesofinstruc-
ing a new dimension for improving code in-
tiontuning,theyallfollowthestandardSFT,leav-
structiontuning. Codesareavailableathttps:
//github.com/ise-uiuc/xft. ingroomforexploringadvancedtrainingschemes.
Wearguethatpriorworkslargelyoverlookthe
1 Introduction
possibility of improving the code instruction tun-
Programsynthesis(orcodegeneration)isalong- ing by advancing the training schemes. Figure 1
standingproblemexploredsincetheearlydaysof depicts the supervised fine-tuning (SFT), which
computer science (Manna and Waldinger, 1971). directlystartswiththepre-trainedweightsandar-
Recently, instruction tuning of code Large Lan- chitectureforfine-tuning. Themodelisdensehere
guage Models (LLMs) has been used to improve becauseallparametersareactivatedtocomputethe
many coding tasks (Chaudhary, 2023; Luo et al., nexttoken(assumingitisadecoder-onlyLLM).In
2023;Weietal.,2023),suchastext-to-codegener- contrasttofine-tuningadensemodel,followingthe
ation(Chenetal.,2021;Austinetal.,2021),code scalinglaws(Kaplanetal.,2020)(i.e.,moreparam-
completion(Cassanoetal.,2022),anddatascience eters, better performance), sparse upcycling (Ko-
engineering(Laietal.,2022). matsuzaki et al., 2023) is proposed to efficiently
A typical instruction tuning flow involves two upgradethemodelsizesbyupcyclingadenseLLM
steps(Zhangetal.,2023): (i)curatinganinstruc- toasparselyactivatedMixture-of-Experts(MoE)
1
4202
rpA
32
]LC.sc[
1v74251.4042:viXramodel. An MoE model is efficient because the • Dimension: We open a new dimension of im-
generationofthenexttokenonlyinvolvesasubset proving instruction tuning of code LLMs by
of parameters (i.e., experts) and thus is sparsely advancingitstrainingscheme,usingenhanced
activated. Forexample,Mixtral-8x7B(Jiangetal., sparseupcyclingandlearnablemodelmerging
2024),comparedtoadense7Bmodel,usesapprox- mechanism, which neither changes the final
imately 8× parameters and 2× computation, i.e., modelstructurenorrequiresmoretrainingdata.
only2outof8expertsaredynamicallyselectedto • Technique: We present XFT, a new training
compute the next token. However, there are two scheme for code instruction tuning. XFT in-
keylimitationswhenusingsparseupcyclinginin- volvestwosteps: upcyclingandmerging. Apre-
struction tuning: (i) Slow scaling: Komatsuzaki traineddenseLLMisfirstupcycledintoanMoE
etal.(2023)showthatsparseupcyclingimproves withthesharedexpertsettingandthenfine-tuned
the dense SFT marginally at the early phase, re- on the instruction dataset. To avoid the perfor-
quiring orders of magnitude of extra compute to mancedegradationcausedbythescalemismatch
achieve decent improvement; and (ii) Inference issue, we propose a novel routing weight nor-
cost: though MoE is more efficient than directly malization strategy. In addition, we introduce
scaling the size of dense LLMs, MoE is still ex- the first learnable mechanism for merging the
pensive, especially at inference, as it introduces upcycledMoEintoadensemodel,eliminating
significantlymoreparameters(i.e.,memory)and, additionalinferenceoverheadwhilepreserving
moreimportantly,computesduringinference,com- orevenimprovingtheMoEperformance.
paredtoitsdensecounterparts.
• Results: With only 1.3B parameters, XFT
In this paper, we propose XFT: by simply
achieves67.1pass@1onHumanEvaland64.6
mergingupcycledMoEmodels, wepushtheper-
pass@1onHumanEval+,whichisthenewstate-
formance limit of instruction-tuned code LLMs.
of-the-artfortinycodeLLMs(<3B).Compared
While vanilla sparse upcycling fails to improve
withnormalsupervisedfine-tuning(SFT),XFT
instruction tuning efficiently (Komatsuzaki et al.,
achieves 13% improvement on HumanEval+!
2023),XFTaddressesthischallengebyisolating
XFT also achieves a consistent improvement
oneexpertasthesharedexpertamongalltheother
from2%to13%onMBPP,MultiPL-E,andDS-
expertsineachMoElayer,inspiredbyDeepSeek-
1000overSFT,demonstratingitsgeneralization.
MoE (Dai et al., 2024) and MoCLE (Gou et al.,
2024). XFTalsoincludesanovelroutingweight 2 RelatedWork
normalizationstrategytoeliminatescalemismatch
betweentheupcycledMoElayerwiththeshared 2.1 Mixture-of-Experts
expertandtheoriginaldenselayer,whichwilloth- Mixture-of-Experts(MoE)canefficientlyscaleup
erwiseleadtoperformancedegradation(Wuetal., modelsizeswithonlysub-linearincreasesincom-
2022). After the upcycled MoE model finishes putation (Shazeer et al., 2017). Compared with
theSFTphase,motivatedbyModelSoups(Worts- thestandardTransformer,MoEreplaceseachFeed-
man et al., 2022), XFT uses a learnable model ForwardNetwork(FFN)layerwithanMoElayer,
merging mechanism to output a dense model by whichusesN (i.e.,multiple)expertnetworksthat
merging all the expert networks in the upcycled arestructurallyequivalenttotheoriginalFFNlayer
MoE, i.e., the final dense model is of the same and uses a router that directs each input token to
modelstructureandsizeastheoriginalpre-trained K outofN expertnetworks. Formally,forthel-th
model,achievingsimilarperformancewithoutpay- MoElayer,outputhiddenstatehl ofthet-thinput
t
ing extra inference cost as the sparse upcycling. tokeniscomputedasfollows(Daietal.,2024):
With only 1.3B parameters, XFT achieves 67.1
pass@1 on HumanEval and 64.6 pass@1 on Hu- N
(cid:88)
manEval+, which is the new state-of-the-art for hl t = (g i,tFFN i(ul t))+ul t
tinycodeLLMs(<3B).ComparedwithSFT,XFT i=1
(cid:40)
achieves13%improvementonHumanEval+. Sur- s s ∈ Topk(s ,K)
i,t i,t t
g = (1)
prisingly,ourmodelmergingmechanismcanpre- i,t 0 otherwise
serveorevenfurtherboostthegeneralperformance
s = {s | 1 ≤ i ≤ N}
oftheupcycledMoEwitharound1/8×parameters! t i,t
Weconcludeourcontributionasfollows: s = Softmax (ulT el)
i,t i t i
2whereN referstothetotalnumberofexperts,g 2024) proposes to integrate adapters into MoE
i,t
referstothegatevalueforthei-thexpert,FFN (·) that are upcycled from dense models. Different
i
refers to the i-th expert, ul refers to the hidden fromtheseworks,XFTfocusesonfullfine-tuning,
t
statesofthet-thtokenwhichistheinputofthel-th which generally performs better than parameter-
MoElayer,s referstotheaffinityscorebetween efficientfine-tuning(Chenetal.,2022).
i,t
the i-th expert and the t-th token, Topk(S,K)
refers to a function computing K largest scores 2.3 WeightAveraging
over S, and el refers to the centroid of the i-th
i Weight averaging is a commonly used technique
expert in the l-th MoE layer. By definition, each
toimprovetheperformanceofdeeplearningmod-
tokenwillonlybeassignedtoandcomputedinthe
els. Forexample, ModelSoups(Wortsmanetal.,
topK expertsamongalltheN experts.
2022)averagestheweightsofmultiplemodelsthat
Recently, many works have been proposed to
areinitializedfromthesamepre-trainedmodelbut
scalemodelsizeswithMoEarchitecture(Lepikhin
finetunedwithdifferenthyperparameterconfigura-
et al., 2020; Du et al., 2022; Fedus et al., 2022;
tionstoimprovetheaccuracyandrobustnessofthe
Jiang et al., 2024; Xue et al., 2024). While most
model. However,onlyafewworkshavebeenpro-
MoE models are trained from scratch, sparse up-
posedtomergeexpertnetworksofanMoElayer
cycling(Komatsuzakietal.,2023)isproposedto
toanormalFFNlayerusingweightaveraging. For
initialize MoE models based on the pre-trained
example,OneS(Xueetal.,2022)proposesseveral
densemodel,whichcanefficientlyreducethecom-
simpleweightaveragingmethodstomergeexpert
putationalcostsoftrainingMoEmodels,compared
networks of a BERT-based MoE model. Closely
with training MoE models from scratch. Specif-
related to our work, Experts Weights Averaging
ically, sparse upcycling constructs a new MoE
(EWA)(Huangetal.,2023)proposestoconvertan
modelbyinitializingeachexpertofeachMoElayer
MoE model to a dense model with two steps: (i)
as a copy of the original FFN layer in the dense
DuringMoEtraining,EWAconductsweightedav-
model,whiledirectlycopyingtheremaininglayers
eragingofalltheexpertweightsaftereachweight
fromthedensemodeltothenewMoEmodel.
update of MoE, which is based on a manually-
craftedhyperparameterβ;(ii)Aftertraining,EWA
2.2 InstructionTuning
convertseachMoElayerintoanFFNlayerbyuni-
Instruction tuning is designed to improve the
formlyaveragingtheexperts.
instruction-following ability of LLMs by fine-
Different from all the aforementioned existing
tuning them on the instruction datasets in a su-
works,XFTisthefirstworkproposingalearnable
pervisedfashion(Weietal.,2022). Thequalityof
mechanismtomergeexpertnetworksintheupcy-
the instruction dataset is significant for the effec-
cledMoEmodel. Furthermore,whilethetraining
tivenessofinstructiontuningandresearchershave
scheme of EWA is deeply coupled to a specific
proposedmultiplemethodstoimprovedataqual-
MoE architecture, XFT can be easily adapted to
ity. For example, SELF-INSTRUCT (Wang et al.,
differentMoEarchitecturesbyonlyadjustingthe
2023)synthesizeshigh-qualityinstructiondataby
final merging process. In addition, unlike EWA,
prompting a foundation LLM with specially de-
XFTdoesnotintroduceanyhyperparametersinto
signed prompts. To improve SELF-INSTRUCT,
thetrainingofthelargeMoEmodels,significantly
Evol-Instruct(Xuetal.,2023)improvesthecom-
reducingthecomputationalresourcesforhyperpa-
plexityanddiversityoftheinstructiondatasetby
rametersearching. OurempiricalresultsinSection
promptingChatGPTwithheuristicprompts. OSS-
4alsoshowcasetheclearadvantageofXFT.
INSTRUCT (Weietal.,2023)queriesChatGPTto
generateinstruction-outputpairsbygettinginspira-
3 XFT
tionfromreal-worldcodesnippets.
Recently, some parameter-efficient fine-tuning We describe the details of XFT in this section.
techniques have been proposed to use MoE for There are two steps in our framework: upcycling
better instruction tuning. For example, Lo- (Section3.1)andmerging(Section3.2). Duringup-
RAMoE(Douetal.,2023)andMoCLE(Gouetal., cycling,weconstructanMixture-of-Experts(MoE)
2024) propose MoE-like modules that are con- model from the pre-trained dense model, namely
structed with Low-Rank Adaptations (LoRA) to MoE ,whichisthenfine-tunedoncodinginstruc-
DS
improveinstructiontuning,whilePESC(Wuetal., tion data. For merging, we propose a learnable
3Original Dense Block
Layer Layer
Attention MLP
Norm Norm
Upcycling MLP layers
MoE Shared Instruction
MLP Tuning
Upcycled MoE Block
MLP 1
Normalized
Layer Layer MLP 2
Attention Router Weighted
Norm Norm
Sum
MLP E
Merging with learned weights
Merged Dense Block
Before Training
Layer Layer
Norm Attention Norm MLP (merged) Post Training
Figure2: OverviewofXFT.
modelmergingmethodtoconverttheinstruction- scalemismatchproblem(Wuetal.,2022).
tuned MoE back to a normal dense model by
DS
3.1.1 SharedExpertforUpcycling
mergingeachMoElayerintoanFFNlayerthrough
During upcycling, we isolate one shared expert
weightaveragingwhiledirectlycopyingotherre-
among all the other normal experts in each MoE
maining layers. Consequently, we can obtain
layer,wherethesharedexpertwillbedeterministi-
XFT thathasthesamemodelarchitectureand
DS
callyassignedtohandleallthetokenswhileother
sizeastheoriginalpre-traineddensemodel,which
normalexpertsareassignedbytherouter. Bydoing
eliminates all the additional inference overhead
so, the upcycled MoE model can achieve a clear
brought by the original sparse upcycling, while
performanceboostininstructiontuning,wherethe
preservingorevenimprovingtheperformanceof
sharedexpertcanlearngeneralknowledgeacross
MoE . OurframeworkisillustratedinFigure2.
DS
the whole instruction dataset while other normal
3.1 Upcycling expertslearnspecificknowledgeamongdifferent
Inspiredbysparseupcycling(Komatsuzakietal., instructionsassignedbytherouter. Formally,the
2023),weconvertthepre-traineddenseLLMtoa
outputhiddenstatehl
t
ofthel-thMoElayerwhen
newMoEbyinitializingeachexpertofeachMoE
processingthet-thtokencanbeexpressedas:
layer as a copy of the original FFN layer in the
N
dense model, while directly copying the remain- hl = (cid:88) (g FFN (ul))+ul
t i,t i t t
inglayersfromthedensemodeltothenewMoE
i=1
model. However,theperformancegainbroughtby 
1−s i = 1
 tmax
sparseupcyclingisnegligiblewithaverylimited 
g = Softmax (s )·s s ∈ S
extratrainingbudget(Komatsuzakietal.,2023)– i,t i i,t tmax i,t tK

whichisexactlythesituationwearefacingduring 0 otherwise
instruction tuning. Intuitively, it is because each S = Topk({s | 1 ≤ i ≤ N},K −1)
tK i,t
expert in the upcycled MoE model is trained on
s = max({s | 1 ≤ i ≤ N})
tmax i,t
fewerinstructiondatathantheoriginaldensemodel
(cid:40)
doesbecausetraditionalroutersusedinsparseup- −∞ i = 1
s =
cyclingwillassigndifferenttokenstodifferentex- i,t Softmax (ulT el) i ≥ 2
i t i
pertsandthusreducetheamountofdataeachex- (2)
pertistrainedon(Gouetal.,2024). Consequently,
inspiredbyDeepSeekMoE(Daietal.,2024)and wherehl referstotheoutputhiddenstateofthel-th
t
MoCLE (Gou et al., 2024), XFT introduces the MoElayerwhenprocessingthet-thtoken,N refers
sharedexpertsettingintosparseupcyclingtotackle tothetotalnumberofexperts,g referstothegate
i,t
thischallenge. Wefurtherproposeanovelrouting valueforthei-thexpert,FFN (·)referstothei-th
i
weightnormalizationstrategyforXFTtoavoidthe expert, ul refers to the output hidden state of the
t
potential performance degradation caused by the l-thattentionlayerwhenprocessingthet-thtoken
4
…andalsotheinputofthel-thMoElayer,s refers Formallyspeaking,giventheweightsofN ex-
i,t
to the affinity score between the i-th expert and perts at the l-th layer Wl,Wl,··· ,Wl , the pro-
1 2 N
thet-thtoken,s referstothemaximumaffinity cess of merging each MoE layer to an FFN layer
tmax
scoreamongalltheexpertsbesidesthesharedex- canbestatedasbelow:
pert,Topk(S,K)referstoafunctioncomputingK
N
largestscoresoverS,S referstoasetofK −1 (cid:88)
tK Wl = αlWl (3)
largestaffinityscoresamongalltheexpertsbesides i i
i=1
the shared expert, and el refers to the centroid of
i
thei-thexpertinthel-thMoElayer. whereWl denotesthemergedparameterofallN
FFN 1ischosenasthesharedexpertineachMoE expertsandαl denotesthelearnablemixingcoeffi-
i
layer and each token will be assigned to top K cientofexpertWl. Weconsideraneuralnetwork
i
experts including one shared expert and K − 1 f(x;θ)withinputxandparametersθ. ForlossL
othernormalexperts. Comparedwiththeoriginal andinstructiondataset{(x ,y )}m ,suchmixing
i i i=1
sparseupcycling,therearetwomajordifferences: coefficientsαofalltheLlayerscanbelearnedvia:
• Weighted Shared Expert. Following Mo-
m N
(cid:88) (cid:88)
CLE(Gouetal.,2024),withthetoken-to-expert argmin L(f(x ;θ ,( αlWl) ),y ) (4)
j o i i 1:L i
α
affinityscores i,t,wegetthemaximumaffinity j=1 i=1
score s and use its complement 1−s
tmax tmax
where θ refers to all the remaining layers of
astheroutingweightofthesharedexpert. o
MoE other than MoE layers and α is parame-
• Routing Weight Normalization. Although DS
terizedastheoutputofasoftmax,sothateachαl
the shared expert setting is also used in recent i
ispositiveand(cid:80)N αl = 1.
works (Dai et al., 2024; Gou et al., 2024), we i=1 i
WhilethelearningprocessdefinedinEq. (4)is
cannotdirectlyfollowtheirroutingstrategybe-
the most intuitive way of learning α, our experi-
causetheycannothandleascalemismatchprob-
mentinSection5.2showsthat,duetotheshared
lem that is unique for sparse upcycling. The
expertsetting,ittendstosimplyincreasethemix-
scale mismatch problem is that differences be-
ingcoefficientofthesharedexpertateachlayeras
tween the scale of the output of the upcycled
muchaspossibletodecreasetheloss. Itisnothelp-
MoElayerandtheoriginalFFNlayercancause
fulbecause,althoughthesharedexperthaslearned
performancedegradation(Wuetal.,2022). To
general knowledge across the whole instruction
handlethisproblem,weneedtomakesurethe
datasetandneedsarelativelylargemixingcoeffi-
sum of g equals 1, so that the output of the
i,t
cient,westillneedtokeepthescaleofthemixing
MoElayermatchesthatoftheFFNlayerinscale.
coefficientofothernormalexpertsatacertainlevel
Todoso,wenormalizetheaffinityscoresoftop
also to keep some specific knowledge learned by
K −1 normal experts with Softmax and scale
othernormalexpertsinthemergedparameterWl.
theirsumtos tomakesurethatthesumof
tmax
Tosolvethisissue,weintroduceasharedexpert
the g of top K experts, including one shared
i,t
rate λ to fix the mixing coefficient of the shared
expertandK −1normalexperts,equals1.
expertandlearnthemixingcoefficientsofthere-
3.2 Merging maining normal experts which sums to 1 − λ in
eachlayer. Bydoingso,wecaneasilycontrolthe
Weproposealearnablemodelmergingmethodto
scaleofthemixingcoefficientofthesharedexpert,
convertthelargeMoEmodel,namelyMoE ,back
DS
whilestillbeingabletolearntheoptimallayer-wise
toadensemodelXFT . Bydoingso,weexpect
DS
mixingcoefficientsofothernormalexperts. Let’s
XFT to keep the boosted performance gained
DS sayWl isthesharedexpertofthel-thlayer,then
duringupcyclingwhilekeepingitsmodelsizethe 1
Eq. (3)andEq. (4)canbereformulatedasbelow:
sameastheoriginaldensemodeltoavoidanyad-
ditional inference overhead. Inspired by Model
N
(cid:88)
Soups(Wortsmanetal.,2022),wechoosetomerge Wl = λWl + αlWl (5)
1 i i
MoE by learning the mixing coefficients that
DS i=2
canbeusedtoaveragetheparametersofallexperts m
(cid:88)
in each MoE layer to obtain a normal FFN layer, argmin L(f(x j;θ o,Wl 1:L),y i) (6)
α
whiledirectlycopyingotherremaininglayers. j=1
5In practice, we uniformly initialize the mix- Table 1 shows the pass@1 results of different
ing coefficients α of all the normal experts as LLMs. XFTachieves67.1pass@1onHumanEval
1−λ, which is then trained on the same instruc- and 64.6 pass@1 on HumanEval+, which makes
N−1
tiondatasetasupcycling. itthenewstate-of-the-artsmallcodeLLM(<3B).
We can also observe that XFT has a clear im-
DS
4 MainEvaluation provement over the SFT on both benchmarks,
DS
with 13% and 2% improvement on HumanEval+
4.1 ExperimentalSetup
andMBPP+respectively,whileEWA evenper-
DS
Training. We use DeepSeek-Coder-Base forms worse than SFT DS on MBPP(+). XFT DS
1.3B (Guo et al., 2024) as the main base also outperforms EWA DS on both benchmarks.
codeLLM.evol-codealpaca-v1,anopen-source Surprisingly, XFT DS even surpasses MoE DS on
Evol-Instruct (Luo et al., 2023) dataset contain- HumanEvalandHumanEval+,despiteonlyusing
ing 110K samples, is used as our instruction around 1/8× parameters and around 1/6× compu-
dataset. MoE , our MoE model upcycled from tations,whichshowcasestheeffectivenessofour
DS
thebasemodel,isimplementedfollowingLlama- simplelearnablemergingtechnique. AppendixA.4
MoE(LLaMA-MoETeam,2023). Itisconstructed furtherdemonstratesthestatisticalsignificanceof
with 8 experts in one expert layer and the top 6 theimprovementsbroughtbyXFT.
experts1 areactivatedforeachtoken,includingone
4.3 MultilingualCodeGeneration
sharedexpert. Assuch,wedenotethemodelsizeof
MoE as8×1.3B.Otherhyperparametersettings WeuseMultiPL-E(Cassanoetal.,2022),amulti-
DS
are detailed in Appendix A.1. We finally obtain programmingbenchmarkthatsupports18program-
XFT byusingthelearnedmixingcoefficientsto ming languages in addition to Python, to evalu-
DS
mergeMoElayersinsideMoE asnormalFFN atethemultilingualabilityandgeneralizabilityof
DS
layers. Note that XFT is the final instruction- XFT. Among these, we choose 6 representative
DS
tunedLLMweproduce,whileMoE isonlyan programmingfortheirdistinctlanguagefeatures:
DS
intermediateproductofXFTframework. Java, JavaScript, C++, PHP, Swift, and Rust, fol-
Baselines. To study the effectiveness of XFT, lowing Wei et al. (2023). Table 2 shows, among
all 1.3B models, XFT achieves the best aver-
webuildabaselinemodel,namelySFT ,bydi- DS
DS
age multilingual performance and performs the
rectlyperformingSFTforDeepSeek-Coder-Base
1.3Bonevol-codealpaca-v1. TocompareXFT best on 5 (out of 6) individual programming lan-
guages, overall largely improving SFT which
withEWA(Huangetal.,2023),wealsoimplement DS
uses standard SFT. Notably, the overall perfor-
a baseline EWA and instruction-tune it using
DS
manceofEWA isonparwithSFT ,indicating
thesamehyperparametersettingasSFT ,which DS DS
DS
thatEWA maynotimproveSFTonmultilingual
isdescribedinAppendixA.1. Moreimplementa- DS
coding. AppendixA.5furtherstudieswhethereach
tion details of EWA can be seen in Appendix
DS
expert in MoE specializes differently in these
A.2. Furthermore, we incorporatemultiplesmall DS
programminglanguages.
open-sourcemodels(<3B)asourbaselines,includ-
ingDeepSeek-Coder-Base1.3B,DeepSeek-Coder-
4.4 CodeGenerationforDataScience
Instruct 1.3B (Guo et al., 2024), Phi-2 2.7B, and
TheDS-1000dataset(Laietal.,2022)isacollec-
STABLE-CODE3B(Pinnaparajuetal.,2024).
tionof1000realisticdatasciencecodingproblems
4.2 PythonText-to-CodeGeneration ranging from 7 popular data science libraries in
Python, including Matplotlib (plt), NumPy (np),
HumanEval(Chenetal.,2021)andMBPP(Austin
Pandas (pd), SciPy (scp), Scikit-Learn (sk), Py-
etal.,2021)benchmarksarethetwomostwidely-
Torch(py),andTensorFlow(tf). WeevaluateXFT
usedcollectionsofPythoncodegenerationtasks.
onDS-1000tounderstanditseffectivenessforprac-
We further employ HumanEval+ and MBPP+,
ticaldatascienceengineering. Wefollowtheeval-
which use more tests automatically generated by
uation setting of prior works (Guo et al., 2024;
EvalPlus(Liuetal.,2023)formorerigorousevalu-
Weietal.,2023). InTable3,XFT achievesthe
DS
ation. WeleavethedetailsinAppendixA.3.
bestoverallperformanceamongalltheevaluated
1.3Bmodels. Specifically,XFT consistentlysur-
16isthebest-performingnumberofactivatedexpertsper DS
ourHumanEval+experimentsusingtop{2,4,6}experts. passesSFT DS amongallthesevenstudiedlibraries
6Instruction Dataset Benchmark
Model Size
Dataset Size
HumanEval(+) MBPP(+)
GPT-3.5(May2023) - Private - 73.2(66.5) -
STABLE-CODE 3B - - 28.7(25.6) 53.6(44.1)
DeepSeek-Coder-Base 1.3B - - 28.7(25.6) 55.6(46.9)
Phi-2 2.7B - - 48.8(45.1) 62.7(52.9)
DeepSeek-Coder-Instruct 1.3B Private 2B 65.2(59.8) 63.9(53.1)
SFT 1.3B Evol-Instruct 0.3B 61.6(57.3) 59.6(49.1)
DS
EWA 1.3B Evol-Instruct 0.3B 67.1(63.4) 58.9(48.4)
DS
MoE 8×1.3B Evol-Instruct 0.3B 65.2(62.2) 60.4(50.1)
DS
XFT 1.3B Evol-Instruct 0.3B 67.1(64.6) 60.4(50.1)
DS
Table1: Pass@1resultsofdifferentLLMsonHumanEval(+)andMBPP(+)computedwithgreedydecoding,
followingthesettingofpriorworks(Weietal.,2023;Liuetal.,2023). Wereporttheresultsconsistentlyfrom
theEvalPlus(Liuetal.,2023)Leaderboard. Notethatnumbersinboldrefertothehighestscoresamongall1.3B
modelsfine-tunedonpublicdatasets,whichisthesameforalltheothertables.
ProgrammingLanguage
Model Size Average
C++ PHP Java JS Swift Rust
DeepSeek-Coder-Base 1.3B 28.1 22.9 27.2 28.7 10.9 18.0 22.6
SFT 1.3B 40.4 38.5 40.2 46.2 16.4 27.7 34.9
DS
EWA 1.3B 39.4 38.4 37.3 45.2 20.9 28.6 35.0
DS
MoE 8×1.3B 42.2 42.2 35.4 49.8 24.7 30.6 37.5
DS
XFT 1.3B 42.7 41.5 36.0 49.7 25.3 32.1 37.9
DS
Table 2: Pass@1 results on MultiPL-E (Cassano et al., 2022) following the same hyperparameter settings as
prior works (Wei et al., 2023; Luo et al., 2023): temperature = 0.2, top_p = 0.95, max_length = 512, and
num_samples=50. Allmodelsareevaluatedusingbigcode-evaluation-harness(BenAllaletal.,2022).
andalsooutperformsEWA ingeneral. an ablation by excluding it from XFT. Table 4
DS
showsthat,afterremovingroutingweightnormal-
5 AblationStudy
ization, the performance substantially decreases,
despitebeingstillbetterthantheoriginalsparseup-
5.1 EffectofSharedExpertwithRouting
cyclingthatdoesnotusethesharedexpertsetting.
WeightNormalization
We demonstrate the importance of the shared ex-
5.2 EffectofMergingStrategy
pert of XFT by comparing its performance with
the sparse upcycling (Komatsuzaki et al., 2023) Inthissection,wedemonstratetheeffectivenessof
baseline that does not employ any shared expert. ourlearnablemergingtechniquebycomparingit
AsshowninTable4,theperformanceoftheorig- with (1) directly merging experts with initialized
inalsparseupcycling(withthe"-SharedExpert" mixingcoefficients,and(2)thelearnablemerging
label) drops greatly compared with MoE . No- technique without the shared rate setting, which
DS
tably, the sparse upcycling model even performs is the same setting as the learned soup in Model
worse than SFT on HumanEval+, showing its Soups(Wortsmanetal.,2022)andisdescribedin
DS
ineffectivenessforinstructiontuning. Eq. (3)andEq. (4). Specifically,weinitializethe
Whilethesharedexpertsettingisalsoemployed learnablemixingcoefficientofthesharedexpertas
inmostrecentworks(Daietal.,2024;Gouetal., 0.75 and that of the other 7 normal experts as 1
28
2024), their routing strategy will cause perfor- forfaircomparison. AsshowninTable5,trained
mancedegradationduetothescalemismatchbe- mixingcoefficientsoutperformtheinitializedmix-
tweentheoutputsoftheupcycledMoElayerand ingcoefficientsformerging. Furthermore,remov-
the original FFN layer. To understand the impor- ingthesharedratesettingwilllargelydegradethe
tanceofroutingweightnormalization,weconduct performance of XFT on both HumanEval and
DS
7DataScienceLibrary
Model Size Overall
np pd plt py scp tf sk
DeepSeek-Coder-Base 1.3B 25.1 5.8 34.5 12.7 9.8 11.1 12.7 16.4
SFT 1.3B 30.9 17.0 40.5 32.7 18.3 21.1 24.4 25.9
DS
EWA 1.3B 32.9 19.4 41.8 25.7 17.7 22.2 33.0 27.8
DS
MoE 8×1.3B 33.2 21.3 38.4 41.8 21.8 23.5 37.5 30.0
DS
XFT 1.3B 32.9 20.2 38.9 41.4 21.1 16.9 37.5 29.3
DS
Table3: Pass@1resultsonDS-1000(completionformat)withtemperature=0.2,top_p=0.5,max_length=
1024,andnum_samples=40,followingthesamehyperparametersettingusedinpriorworks(Weietal.,2023).
Model HumanEval HumanEval+ Model λ HumanEval HumenEval+
SFT 61.6 57.3 SFT - 61.6 57.3
DS DS
MoE 65.2 62.2
DS 0.00 62.8 59.8
MoE 0.25 64.6 61.0
DS 63.4 59.1
-Normalization XFT
DS
0.50 65.9 62.8
0.75 67.1 64.6
MoE
DS 61.6 56.7 1.00 63.4 60.4
-SharedExpert
Table6: Ablationovertheeffectofthesharedexpert
Table4: AblationoverthedesignofMoE ."-Normal-
DS
rateλinourlearnablemergingtechnique.XFTcancon-
ization"removestheroutingweightnormalizationfrom
sistentlyoutperformthenormalSFTbaselineregardless
therouter,makingitthesamedesignasMoCLE(Gou
ofthesharedexpertrate,whileλ=0.75istheoptimal
etal.,2024). "-SharedExpert"removesthesharedex-
settinginourexperiments.
pert setting, making MoE the same architecture as
DS
originalsparseupcycling(Komatsuzakietal.,2023).
MoEmodel. AsshowninTable6,therearemainly
threeinterestingobservations:
Model HumanEval HumanEval+
MoE 65.2 62.2
DS • The performance of the final merged dense
XFT (INIT) 66.5 64.0
DS modelimprovesgraduallywhenthesharedex-
XFT 67.1 64.6
DS pert rate grows from 0.00 to 0.75, indicating
XFT thatgeneralknowledgelearnedbytheshared
DS 66.5 64.0
-SharedExpertRate expertisimportantforbetterperformance.
• The performance of the final merged dense
Table5: AblationoverthedesignofXFT . "(INIT)"
DS
referstodirectlyusingtheinitializedmixingcoefficients model drops significantly when the shared ex-
tomergeexpertswithouttraining. "-SharedRate"re- pertrategrowsfrom0.75to1.00,showingthat
movesthesharedratesettingfromXFT DS,whichisthe specificknowledgelearnedbyotherexpertsis
sameasthelearnedsoup(Wortsmanetal.,2022). alsoirreplaceableandignoringthemwilllead
toasignificantperformancedrop.
HumanEval+,demonstratingitsimportance.
• Allthefinalmergeddensemodelsconsistently
Wefurtherstudytheeffectofthesharedexpert
outperformthenormalSFTbaselineregard-
rateλontheperformanceofthefinalmergeddense
lessoftheirsharedexpertrate,furtherdemon-
model. Weevenlychoosefivesharedexpertrates,
stratingtheeffectivenessofXFT.
including 0.00, 0.25, 0.50, 0.75, and1.00, to per-
form the learnable merging process and evaluate
5.3 EffectofBaseCodeLLM
eachmergeddensemodelaccordingly. Notethat
0.75 is the default shared expert rate used in our Inthissection, wedemonstratethattheeffective-
mainexperiments. Ifthesharedexpertrateis0.00, nessofXFTisnotdependentonthechoiceofbase
it means that the shared expert is ignored when codeLLMs. Toshowthis,weconductanablation
constructingthemergeddensemodelfromtheup- experiment by applying XFT to STABLE-CODE
cycledMoEmodel;ifthesharedexpertrateis1.00, 3B(Pinnaparajuetal.,2024),whosearchitecture
itmeansthatthefinaldensemodelisbuiltbysim- isdifferentfromDeepSeek-Coder-Base1.3B(Guo
plyextractingthesharedexpertfromtheupcycled etal.,2024), andseewhetheritcanstillimprove
8Model HumanEval HumanEval+ Model HumanEval HumanEval+
SFT 62.2 56.1 SFT
STABLE DS 61.6 57.3
w/samesteps
MoE 64.0 59.1
STABLE
SFT
XFT STABLE 68.3 62.2 DS 62.2 57.3
w/samebudget
Table 7: Ablation over the effect of the base model
XFT 67.1 64.6
byreplacingDeepSeek-Coder-Base1.3BwithSTABLE- DS
CODE3B.XFTcanconsistentlyimprovetheinstruction
Table8: Experimentsontheeffectoftrainingoverhead.
tuningperformanceofdifferentbasecodeLLMs.
For our two SFT baselines, "w/ same steps" refers to
oneSFTbaselineusingthesametrainingstepsasXFT
its performance. Hyperparameter settings are de-
while "w/ same budget" refers to the other SFT base-
tailed in Appendix A.6. As is shown in Table
lineusingthesametrainingbudgetasXFT.XFTcan
7,XFT significantlyimprovesSFT
STABLE STABLE consistentlyoutperformbothSFTbaselinestoalarge
by10%onHumanEvaland11%onHumanEval+ extent,furtherdemonstratingtheabilityofXFTtoun-
respectively. Furthermore, XFT consis- lockthepowerofcodeinstructiontuning.
STABLE
tentlybooststheperformanceofMoE while
STABLE inTable9,overall,XFT improvesSFT by5%
onlyusing1/4×parametersand1/2×computations. TL TL
onMMLU,demonstratingthegeneralizableeffec-
TheseresultsshowthattheeffectivenessofXFT
tivenessofXFTforgeneralinstructiontuning.
does not depend on any specific choice of base
codeLLMs,demonstratingthegeneralizabilityof
6.3 PreliminaryTheoreticalExplanation
XFTacrossdifferentmodelarchitectures.
Weprovideapreliminarytheoreticalexplanationof
6 Discussion XFTbyconsideringasimplifiedvariantofit. Let’s
startbyanalyzingthetwomajorstepsofXFT:
6.1 TrainingOverheadAnalysis
ComparedwithSFT,XFTwillinevitablyintroduce • Step 1: Upcycling. According to the scaling
additionaloverheadinthetrainingprocessbecause law (Kaplan et al., 2020), the upcycled MoE
XFTneedstofine-tunetheupcycledMoEmodel model performs better than the normal SFT
whilethenormalSFTtechniqueonlyneedstofine- densemodelduetomoretrainableparameters.
tunetheoriginaldensemodel. Tobetterunderstand • Step2: Merging. Weconsiderasimplifiedvari-
theeffectofsuchoverhead,weconductanexperi- ant of XFT, where the upcycled MoE model
mentwhereweusethesametrainingbudget(i.e., (e.g.,MoE )canbeviewedastheensembling
DS
thesameGPUhours)insteadofthesametraining of two dense models and the merged dense
steps for the normal SFT baseline. As shown in model(e.g.,XFT )canbeviewedasthemerg-
DS
Table8,althoughsharingthesametrainingbudget ingofthesametwodensemodels;seeAppendix
asXFT DS,theperformanceofSFT DSisstillsignif- A.8formoredetails. Assuch,wecandirectlyap-
icantlyworsethanthatofXFT DS,demonstrating plythetheoreticalanalyzingprocessinSection
the ability of XFT to unlock the power of code 4of(Wortsmanetal.,2022)toanalyzetheper-
instructiontuningusingthesametrainingbudget. formancedifferencebetweentheupcycledMoE
model and the merged dense model, which is
6.2 GeneralizabilityforGeneralTasks
initiallydesignedtoanalyzetheperformancedif-
In this section, we demonstrate that XFT can ferencebetweenmodelensemblingandmodel
improve the performance of LLMs on general merging. Accordingto(Wortsmanetal.,2022),
tasks across different domains by applying XFT the convexity of the loss can help the merged
to general instruction tuning. We use TinyLlama densemodelachieveasimilarexpectedlossas
1.1B (Zhang et al., 2024) as the base model and thatoftheupcycledMoEmodel.
use evol-instruct-70k (Xu et al., 2023) as the
trainingdatasetforgeneralinstructiontuning. Fol- Overall, the Upcycling step improves the per-
lowingexistingwork(Zhangetal.,2024),weuse formance with more trainable parameters, while
MMLU (Hendrycks et al., 2021) with the 5-shot the Merging step maintains the upcycled MoE-
setting as our evaluation benchmark to showcase levelperformancewithonlydense-modelcompute.
thegeneralperformanceofLLMs. Hyperparameter Consequently,weprovideapreliminarytheoretical
settings are detailed in Appendix A.7. As shown explanationfortheeffectivenessofXFT.
9Discipline
Model Overall
Humanities SocialScience STEM Other
SFT 25.38 23.30 24.20 26.78 24.97
TL
MoE 23.85 26.32 27.40 28.03 26.11
TL
XFT 23.91 26.49 27.72 28.29 26.30
TL
Table9: ExperimentsonthegeneralizableeffectivenessofXFTforgeneraltasksinMMLUbenchmark(Hendrycks
etal.,2021). ItshowsthatXFTcanimprovethegeneralinstructiontuningperformanceofLLMs.
7 Conclusion References
This paper introduces XFT to unlock the power JacobAustin,AugustusOdena,MaxwellNye,Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
ofcodeinstructiontuningbysimplymergingup-
Jiang, Carrie Cai, Michael Terry, Quoc Le, and
cycled MoE. Similar to SFT, XFT starts with a CharlesSutton.2021. Programsynthesiswithlarge
denseLLMandproducesafine-tuneddenseLLM languagemodels.
withtheexactsizeandmodelstructure. Yet,XFT
Loubna Ben Allal, Niklas Muennighoff, Lo-
improvesSFTbyupcyclingthepre-traineddense
gesh Kumar Umapathi, Ben Lipkin, and
LLMtoanMoEmodelforfine-tuning,afterwhich Leandro von Werra. 2022. A framework
we compile the MoE model back to an efficient for the evaluation of code generation mod-
denseLLMwithalearnablemergingmechanism. els. https://github.com/bigcode-project/
bigcode-evaluation-harness.
As such, we unleash the performance limit of in-
struction tuning without any additional inference FedericoCassano,JohnGouwar,DanielNguyen,Syd-
overhead. Usingthesamedataset,XFTimproves neyNguyen,LunaPhipps-Costin,DonaldPinckney,
Ming-HoYee,YangtianZi,CarolynJaneAnderson,
SFT on a variety of benchmarks, including Hu-
MollyQFeldman,ArjunGuha,MichaelGreenberg,
manEval(+),MBPP(+),MultiPL-E,andDS-1000,
and Abhinav Jangda. 2022. Multipl-e: A scalable
from2%to13%. ByapplyingXFTtoDeepSeek- andextensibleapproachtobenchmarkingneuralcode
Coder-Base1.3B,wecreatethenextstate-of-the- generation.
artsmall(<3B)LLMforcode. Theultimatedense
SahilChaudhary.2023. Codealpaca: Aninstruction-
LLMproducedbyXFTpreservesorevenoutper-
followingllamamodelforcodegeneration. https:
formsthefullupcycledMoEwhichuses8×param- //github.com/sahil280114/codealpaca.
etersasmuchasourfinaldenseLLM.XFTisfully
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and
orthogonaltotheexistinginstructiontunerssuch
Shangsong Liang. 2022. Revisiting parameter-
as Evol-Instruct and OSS-INSTRUCT, opening a
efficienttuning: Arewereallythereyet?
newdimensiontomaximalcodeinstructiontuning.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Limitations Yuan,HenriquePondedeOliveiraPinto,JaredKa-
plan, HarriEdwards, YuriBurda, NicholasJoseph,
WhileXFThasproventobeeffectivethroughex- Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger,MichaelPetrov,HeidyKhlaaf,GirishSas-
tensiveexperimentsinthepaper,weapplyourtech-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
niquetoLLMswithnomorethan3Bparameters NickRyder,MikhailPavlov,AletheaPower,Lukasz
due to resource constraints. This limitation hin- Kaiser, Mohammad Bavarian, Clemens Winter,
dersourabilitytoshowcasetheimpactofXFTon Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
largermodels. Inaddition,tobalancethegeneral
beth Barnes, Ariel Herbert-Voss, William Hebgen
knowledge in the shared expert and the specific
Guss,AlexNichol,AlexPaino,NikolasTezak,Jie
knowledgeinothernormalexperts,weintroducea Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain,
hyperparameterλinthemergingprocessofXFT, William Saunders, Christopher Hesse, Andrew N.
Carr,JanLeike,JoshAchiam,VedantMisra,Evan
whichmightslightlyincreasetheeffortsforhyper-
Morikawa, Alec Radford, Matthew Knight, Miles
parametersearch. Itwouldbeinterestingtoexplore
Brundage,MiraMurati,KatieMayer,PeterWelinder,
otherhyperparameter-freetechniquestotacklethis BobMcGrew,DarioAmodei,SamMcCandlish,Ilya
challengeinthefuture. Furthermore, whileXFT Sutskever,andWojciechZaremba.2021. Evaluating
largelanguagemodelstrainedoncode.
hasbeenempiricallyprovenpowerful,itwouldbe
interestingtoprovideatheoreticalexplanationfor
Damai Dai, Chengqi Deng, Chenggang Zhao, R. X.
itsstrongperformance. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding
10Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Sophia Yang, Szymon Antoniak, Teven Le Scao,
PanpanHuang,FuliLuo,ChongRuan,ZhifangSui, Théophile Gervet, Thibaut Lavril, Thomas Wang,
andWenfengLiang.2024. Deepseekmoe: Towards TimothéeLacroix,andWilliamElSayed.2024. Mix-
ultimateexpertspecializationinmixture-of-experts tralofexperts.
languagemodels.
JaredKaplan,SamMcCandlish,TomHenighan,TomB.
ShihanDou,EnyuZhou,YanLiu,SongyangGao,Jun Brown,BenjaminChess,RewonChild,ScottGray,
Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao AlecRadford,JeffreyWu,andDarioAmodei.2020.
Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Scalinglawsforneurallanguagemodels.
Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang.
2023. Loramoe: Revolutionizingmixtureofexperts AranKomatsuzaki,JoanPuigcerver,JamesLee-Thorp,
formaintainingworldknowledgeinlanguagemodel CarlosRiquelmeRuiz,BasilMustafa,JoshuaAinslie,
alignment. YiTay,MostafaDehghani,andNeilHoulsby.2023.
Sparseupcycling: Trainingmixture-of-expertsfrom
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi densecheckpoints.
Reichart.2018. Thehitchhiker’sguidetotestingsta-
tisticalsignificanceinnaturallanguageprocessing. YuhangLai,ChengxiLi,YimingWang,TianyiZhang,
In Proceedings of the 56th Annual Meeting of the RuiqiZhong,LukeZettlemoyer,ScottWentauYih,
AssociationforComputationalLinguistics(Volume DanielFried,SidaWang,andTaoYu.2022. Ds-1000:
1: LongPapers),pages1383–1392,Melbourne,Aus- A natural and reliable benchmark for data science
tralia.AssociationforComputationalLinguistics. codegeneration.
NanDu,YanpingHuang,AndrewM.Dai,SimonTong, Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, DehaoChen,OrhanFirat,YanpingHuang,Maxim
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
Zoph,LiamFedus,MaartenBosma,ZongweiZhou, Gshard: Scalinggiantmodelswithconditionalcom-
TaoWang,YuEmmaWang,KellieWebster,Marie putationandautomaticsharding.
Pellat, Kevin Robinson, Kathleen Meier-Hellstern,
Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, JiaweiLiu,ChunqiuStevenXia,YuyaoWang,andLing-
Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. mingZhang.2023. Isyourcodegeneratedbychat-
Glam: Efficient scaling of language models with GPTreallycorrect? rigorousevaluationoflargelan-
mixture-of-experts. guagemodelsforcodegeneration. InThirty-seventh
ConferenceonNeuralInformationProcessingSys-
WilliamFedus,BarretZoph,andNoamShazeer.2022. tems.
Switch transformers: Scaling to trillion parameter
modelswithsimpleandefficientsparsity. LLaMA-MoE Team. 2023. Llama-moe: Building
mixture-of-experts from llama with continual pre-
YunhaoGou,ZhiliLiu,KaiChen,LanqingHong,Hang
training.
Xu,AoxueLi,Dit-YanYeung,JamesT.Kwok,and
YuZhang.2024. Mixtureofcluster-conditionallora AntonLozhkov,RaymondLi,LoubnaBenAllal,Fed-
expertsforvision-languageinstructiontuning. ericoCassano, JoelLamy-Poirier, NouamaneTazi,
AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,
DayaGuo,QihaoZhu,DejianYang,ZhendaXie,Kai
Tianyang Liu, Max Tian, Denis Kocetkov, Arthur
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Zucker, Younes Belkada, Zijian Wang, Qian Liu,
Y.Wu,Y.K.Li,FuliLuo,YingfeiXiong,andWen-
DmitryAbulkhanov,IndraneilPaul,ZhuangLi,Wen-
fengLiang.2024. Deepseek-coder: Whenthelarge
DingLi,MeganRisdal,JiaLi,JianZhu,TerryYue
language model meets programming – the rise of
Zhuo,EvgeniiZheltonozhskii,NiiOsaeOsaeDade,
codeintelligence.
WenhaoYu,LucasKrauß,NamanJain,YixuanSu,
XuanliHe,MananDey,EdoardoAbati,YekunChai,
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Niklas Muennighoff, Xiangru Tang, Muhtasham
MantasMazeika,DawnSong,andJacobSteinhardt.
Oblokulov,ChristopherAkiki,MarcMarone,Cheng-
2021. Measuringmassivemultitasklanguageunder-
haoMou, MayankMishra, AlexGu, BinyuanHui,
standing.
Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas
Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Patry,CanwenXu,JulianMcAuley,HanHu,Torsten
TaoChen,TongHe,andWanliOuyang.2023. Ex- Scholak,SebastienPaquet,JenniferRobinson,Car-
perts weights averaging: A new general training olynJaneAnderson,NicolasChapados,MostofaPat-
schemeforvisiontransformers. wary,NimaTajbakhsh,YacineJernite,CarlosMuñoz
Ferrandis,LingmingZhang,SeanHughes,Thomas
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Wolf, Arjun Guha, Leandro von Werra, and Harm
Roux, Arthur Mensch, Blanche Savary, Chris deVries.2024. Starcoder2andthestackv2: The
Bamford, Devendra Singh Chaplot, Diego de las nextgeneration.
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam- Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- uboGeng,WenxiangHu,ChongyangTao,JingMa,
AnneLachaux,PierreStock,SandeepSubramanian, QingweiLin,andDaxinJiang.2023. Wizardcoder:
11Empoweringcodelargelanguagemodelswithevol- Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zang-
instruct. wei Zheng, Wangchunshu Zhou, and Yang You.
2024. Openmoe: An early effort on open
ZoharMannaandRichardJWaldinger.1971. Toward mixture-of-expertslanguagemodels. arXivpreprint
automatic program synthesis. Communications of arXiv:2402.01739.
theACM,14(3):151–165.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung,
Wei Lu. 2024. Tinyllama: An open-source small
JonathanTow,JamesBaicoianu,,andNathanCooper.
languagemodel.
2024. Stablecode3b.
NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz, ShengyuZhang,LinfengDong,XiaoyaLi,SenZhang,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff XiaofeiSun,ShuheWang,JiweiLi,RunyiHu,Tian-
Dean. 2017. Outrageously large neural networks: weiZhang,FeiWu,andGuoyinWang.2023. Instruc-
Thesparsely-gatedmixture-of-expertslayer. tiontuningforlargelanguagemodels: Asurvey.
Anders Søgaard, Anders Johannsen, Barbara Plank,
A Appendixfor"XFT:Unlockingthe
Dirk Hovy, and Hector Martínez Alonso. 2014.
What’sinap-valueinNLP? InProceedingsofthe PowerofCodeInstructionTuningby
Eighteenth Conference on Computational Natural SimplyMergingUpcycled
LanguageLearning,pages1–10,AnnArbor,Michi-
Mixture-of-Experts"
gan.AssociationforComputationalLinguistics.
A.1 HyperparameterSettings
YizhongWang,YeganehKordi,SwaroopMishra,Alisa
Liu,NoahA.Smith,DanielKhashabi,andHannaneh
We use a batch size of 64 and a learning rare of
Hajishirzi. 2023. Self-instruct: Aligning language
5e-5 with a linear scheduler to fine-tune MoE
modelswithself-generatedinstructions. DS
for 4 epochs with 500 warmup steps, following
JasonWei, MaartenBosma, VincentY.Zhao, Kelvin the implementation of previous work (Wei et al.,
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
2023). Wefurtheruseabatchsizeof64,ashared
drew M. Dai, and Quoc V. Le. 2022. Finetuned
expert rate λ of 0.75, and a learning rare of 1e-5
languagemodelsarezero-shotlearners.
with a linear schedule to fine-tune the learnable
YuxiangWei,ZheWang,JiaweiLiu,YifengDing,and mixing coefficients for each of the experts in the
LingmingZhang.2023. Magicoder: Sourcecodeis
instruction-tunedMoE ontheinstructiondataset
allyouneed. arXivpreprintarXiv:2312.02120. DS
for1epochwith125warmupsteps. Detailedly,we
Frank. Wilcoxon. 1945. Individual comparisons by use Softmax to keep the sum of the mixing coef-
rankingmethods. Biometrics,1:196–202. ficientsoftheother7normalexpertsas0.25. For
SFT andEWA ,weusethesamehyperparam-
Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak DS DS
Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, etersettingasXFT,wherethebatchsizeis64and
Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, thelearningrateis5e-5withalinearscheduler. Be-
YairCarmon,SimonKornblith,andLudwigSchmidt. causeXFTistrainedfor4epochsduringupcycling
2022. Modelsoups: averagingweightsofmultiple
and1epochduringmerging,forafaircomparison,
fine-tunedmodelsimprovesaccuracywithoutincreas-
inginferencetime. wetrainSFT DS andEWA DS for5(=4+1)epochs
with625warmupsteps.
Haoyuan Wu, Haisheng Zheng, and Bei Yu. 2024.
Parameter-efficient sparsity crafting from dense to
A.2 ImplementationdetailsofEWA
mixture-of-expertsforinstructiontuningongeneral
tasks.
BecauseEWA(Huangetal.,2023)doesnotrelease
their implementation, we implemented EWA by
LemengWu,MengchenLiu,YinpengChen,Dongdong
Chen, Xiyang Dai, and Lu Yuan. 2022. Residual ourselves,includingconstantscheduleandlinear
mixtureofexperts. schedule. Weuseasharerateβ of0.3,following
theoriginalsettingofEWA.WhileEWAwiththe
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
constantscheduleachievesreasonableperformance
PuZhao,JiazhanFeng,ChongyangTao,andDaxin
Jiang.2023. Wizardlm: Empoweringlargelanguage inourevaluation,thetraininglossofEWAwiththe
modelstofollowcomplexinstructions. linearschedulebecomesveryunstable,asisshown
in Figure 3, and thus cannot achieve reasonable
FuzhaoXue,XiaoxinHe,XiaozheRen,YuxuanLou,
performance. Asaresult,wereporttheresultsof
andYangYou.2022. Onestudentknowsallexperts
know: Fromsparsetodense. EWAwiththeconstantscheduleinSection4.
1216 Model HumanEval HumanEval+
Constant schedule
Linear schedule
14 XFT vs. EWA 2.6e-18 8.0e-23
DS DS
XFT vs. SFT 9.6e-30 3.7e-33
12 DS DS
Table11: p-valuesforXFT vs. EWA andXFT
10 DS DS DS
vs. SFT in200experimentsonHumanEval(+)com- DS
8
putedwithsampling. Resultsshowthatimprovements
6 broughtbyXFTarestatisticallysignificant.
4 Following prior work (Liu et al., 2023), we re-
2 peatthisexperiment200timesforthreetechniques:
XFT ,EWA ,andSFT . EWA isincluded
0 DS DS DS DS
0 2000 4000 6000 8000 because it is the best-performing baseline in our
Training steps
mainexperiment. Wefirstcomputetheiraverage
Figure 3: Training loss curve of EWA with constant
pass@1performanceinthese200experiments. As
scheduleandlinearschedule.
is shown in Table 10, XFT outperforms both
DS
Model HumanEval HumanEval+ EWA andSFT clearly.
DS DS
Furthermore,weusetheWilcoxonsigned-rank
SFT 61.6 57.2
DS
test(Wilcoxon,1945;Droretal.,2018),awidely
EWA 62.7 58.8
DS
usedstatisticaltest,tocheckiftheimprovements
XFT 64.5 60.9
DS
brought by XFT are statistically significant. As
Table10: Averagepass@1resultsof200experiments
shown in Table 11, the p-values for both XFT
DS
on HumanEval (+) computed with sampling. XFT
vs. EWA and XFT vs. SFT are much
clearlyoutperformsbothEWA andSFT . DS DS DS
DS DS
smaller than both 0.0025 (the significance level
A.3 DetailsofHumanEvalandMBPP recommended for NLP work by (Søgaard et al.,
2014)) and 0.05 (the most common significance
In these benchmarks, each task consists of a task
level),demonstratingthestatisticalsignificanceof
description in English, which is sent to LLMs as
theimprovementsbroughtbyXFT.
theprompt,andLLMsareexpectedtogeneratethe
correspondingcodetosatisfytherequirementsin A.5 AnalysisonExpertSpecialization
thedescription. Whilethesebenchmarksprovide
Inspiredbyrecentworks(Jiangetal.,2024;Xue
ahandfuloftestcasestovalidatethecorrectness
et al., 2024), we analyze whether each expert in
of the generated code, these tests are often insuf-
MoE has different specializations in different
ficientformorerigorousevaluation. Assuch,Hu- DS
programminglanguagesbyvisualizingtherouting
manEval+andMBPP+proposedbyEvalPlus(Liu
decisionofthetokensfromdifferentprogramming
etal.,2023)areusuallyusedtoevaluatethecorrect-
languagesintheMultiPL-Ebenchmark(including
nessofthegeneratedcode,whichprovides80×/35×
Python). FortheMultiPL-Ebenchmark,wecollect
moretestscomparedwiththeoriginalbenchmarks.
theroutingdecisionwhenconductingexperiments
inSection4.3. ForPython,wecollecttherouting
A.4 StatisticalSignificanceAnalysis
decision by reruning HumanEval experiment fol-
Inthissection,weshowthatimprovementsbrought lowingthesamesettingasSection4.3. Following
byXFTarestatisticallysignificant. Inourmainex- Mixtral (Jiang et al., 2024), we get the visualiza-
periments,wefollowpriorworks(Weietal.,2023; tion results from layers 0, 11, and 23 in MoE ,
DS
Lozhkov et al., 2024) to conduct experiments on wherelayer0andlayer23arethefirstandthelast
HumanEval(+)usinggreedydecoding. Todemon- layersofMoE . AsisshowninFigure4,wedo
DS
strate the statistical significance of our improve- notobserveobviouspatternsintheassignmentof
ments,wechangeoursettingfromgreedydecoding expertsbasedontheprogramminglanguage,which
tosampling. Indetail,toconductoneexperiment is in line with the observation reported by recent
onHumanEval(+),themodelwillsampleoneso- works(Jiangetal.,2024;Xueetal.,2024). 1681
lutionforeachprobleminHumanEval(+)withtop
p=0.95andtemperature=0.8,whichisthesame
A.6 TrainingSettingsforSTABLE-CODE3B
settingusedinpriorworks(Liuetal.,2023;Chen We use evol-codealpaca-v1 as the training
etal.,2021). dataset. SinceSTABLE-CODE3Bisthebasemodel,
13
ssol
gniniarTlayer: 0
0.20
0.15
0.10
0.05
0.00
layer: 11
0.20
0.15
0.10
0.05
0.00
layer: 23
0.20
0.15
0.10
0.05
0.00
1 2 3 4 5 6 7
Expert ID
Python C++ PHP Java JS Swift Rust
Figure 4: Proportion of tokens assigned to each expert on different programming languages from MultiPL-E
(includingPython)forlayers0,11,and23. Thesharedexpert0isexcludedfromthechartbecauseallthetokensare
alwaysassignedtoit. Thegrayverticallinemarks 1,whichistheproportionexpectedwiththeuniformsampling.
7
weupcycleanewMoEmodelfromthebasemodel, ofparametersforMoE canbewrittenas8×1.1B.
TL
namely MoE . Due to limited computa- Weuseabatchsizeof64andalearningrateof5e-5
STABLE
tionalresources,weconstructMoE with4 with a linear scheduler to fine-tune MoE for 4
STABLE TL
expertsinoneexpertlayer,wherethetop2experts epochswith240warmupsteps. ToobtainXFT ,
TL
areactivatedforeachtoken,includingoneshared welearnmixingcoefficientstomergeMoElayers
expert. Consequently,thesizeofMoE can inside MoE by fine-tuning them with a batch
STABLE TL
bedescribedas4×3B.Weuseabatchsizeof64 size of 64, a shared expert rate λ of 0.85, and a
and a learning rate of 5e-5 with a linear sched- learning rate of 2e-5 with a linear schedule for 1
uler to fine-tune MoE for 4 epochs with epoch with 60 warmup steps. For a fair compari-
STABLE
500 warmup steps. Similar to XFT , we ob- son,wefine-tuneabaselinemodelSFT for5(=
DS TL
tainXFT bylearningmixingcoefficients 4 + 1) epochs with a batch size of 64, a learning
STABLE
tomergeMoElayersinsideMoE asnormal rateof5e-5,and300warmupsteps.
STABLE
FFNlayers,whichisfine-tunedwithabatchsize
A.8 TheoraticalExplanationDetails
of64,asharedexpertrateλof0.85,andalearning
rateof1e-5withalinearschedulefor1epochwith WeconsiderasimplifiedvariantofXFTasbelow:
125 warmup steps. Our baseline model, namely
• The original dense model is a one-layer trans-
SFT , is fine-tuned for 5 (= 4 + 1) epochs
STABLE formermodel,whichcontainsoneattentionlayer
withabatchsizeof64,alearningrateof5e-5,and
connectedwithonefeed-forwardnetwork(FFN)
625warmupstepsforafaircomparison.
layer. Assuch,theupcycledMoEmodelisalso
aone-layertransformermodel, containingone
A.7 TrainingSettingsforTinyLlama1.1B
attentionlayerconnectedwithanMoElayer.
UsingTinyLlama1.1Basthebasemodel,weup- • TheupcycledMoEmodelonlyhastwoexperts
cycleanewMoEmodel,namelyMoE TL,fromthe (e 1 and e 2), both of which are always selected
basemodel. FollowingthesettingforMoE ,we forprocessingtheinputtokens.
DS
constructMoE with8expertsinoneexpertlayer, • The router in the MoE model assigns constant
TL
wherethetop6expertsareactivatedforeachtoken, weights to each expert, regardless of the input
includingonesharedexpert. Assuch,thenumber token. Consequently, the output of the MoE
14
noitroporp
noitceleSlayer for the t-th token h can be represented
t
as (1 − α)e (u ) + αe (u ), where 1 − α is
1 t 2 t
therouterweightassignedtoe ,αistherouter
1
weightassignedtoe ,andu istheinputofthe
2 t
MoElayerforthet-thtoken.
• We simplify the process of merging the MoE
model back to a dense model as W = (1−
eα
α)W +αW ,whereW referstotheweight
e1 e2 e
of e and e refers to the weight of the FFN in
α
themergeddensemodel.
Inthissimplifiedscenario,ifwedenotef(x;θ)
as the output of the model θ for the input x, the
outputofthissimplifiedMoEmodelforinputto-
ken x can be represented as f(x;θ ). Interest-
MoE
ingly, if we define two new dense models θ and
1
θ ,whereθ andθ usethesameattentionlayeras
2 1 2
thisMoEmodelwhileusinge ande astheFFN
1 2
layerseparately,f(x;θ )canberepresentedas
MoE
(1 − α)f(x;θ ) + αf(x;θ )! Consequently, the
1 2
computationprocessofthissimplifiedMoEmodel
can be viewed as ensembling the outputs of two
densemodelsθ andθ . Meanwhile,theprocessof
1 2
mergingtheupcycledMoEmodelbacktoadense
model in this simplified XFT can be represented
asθ = (1−α)θ +αθ ,whichisthemergingof
α 1 2
thesametwodensemodelsθ andθ .
1 2
15