Automatic Layout Planning for Visually-Rich Documents with
Instruction-Following Models
WanrongZhu¶,JenniferHealey§,RuiyiZhang§,WilliamYangWang¶,TongSun§
¶UCSantaBarbara,§AdobeResearch
{wanrongzhu,william}@cs.ucsb.edu,{jehealey,ruizhang,tsun}@adobe.com
Abstract
How can I design a flyer with the following components?
The flyer should have a width of 128 and a height of 128.
Recentadvancementsininstruction-following
modelshavemadeuserinteractionswithmod-
elsmoreuser-friendlyandefficient, broaden-
ingtheirapplicability. Ingraphicdesign,non-
professionalusersoftenstruggletocreatevisu-
allyappealinglayoutsduetolimitedskillsand
You can arrange the components like this:
resources. Inthiswork,weintroduceanovel
component#0 {left: 0; top: 0; width: 128; height: 128; layer: 0;}
multimodal instruction-following framework component#1 {left: 22; top: 8; width: 84; height: 84; layer: 2;}
component#2 {left: 20; top: 96; width: 88; height: 30; layer: 1;}
for layout planning, allowing users to easily component#3 {left: 0; top: 0; width: 128; height: 128; layer: 3;}
arrange visual elements into tailored layouts
byspecifyingcanvassizeanddesignpurpose, Figure1: Anexampleofamodelconductingautomatic
such as for book covers, posters, brochures, layoutplanningfollowinghuman-providedinstructions
or menus. We developed three layout rea- andarrangingvisualcontentsfordesignpurpose.
soningtaskstotrainthemodelinunderstand-
ing and executing layout instructions. Ex-
periments on two benchmarks show that our
potentially limiting creative expression. Existing
methodnotonlysimplifiesthedesignprocess
researchonautomaticlayoutplanning(Hsuetal.,
fornon-professionalsbutalsosurpassestheper-
formance of few-shot GPT-4V models, with 2023;Yamaguchi,2021;Inoueetal.,2023)often
mIoU higher by 12% on Crello (Yamaguchi, requires detailed annotations and poses addition
2021). Thisprogresshighlightsthepotentialof constraints on fixed canvas ratios, thereby dimin-
multimodalinstruction-followingmodelstoau- ishinguser-friendlinessandadaptability.
tomateandsimplifythedesignprocess,provid-
Recentadvancementsinlargelanguagemodels
inganapproachablesolutionforawiderange
ofdesigntasksonvisually-richdocuments. (LLMs) have showcased their remarkable ability
tofollowhumaninstructionsandexecutespecified
1 Introduction
tasks (Brownetal.,2020;Ouyangetal.,2022;Ope-
The creation of visually-rich documents (e.g., nAI,2023a),introducinganewlevelofflexibility
posters,brochures,bookcovers,digitaladvertise- andcontrolinhuman-computerinteraction. Along-
ments, etc) using available visual components, side these developments, we have witnessed the
poses a significant challenge for both profession- emergenceofinstruction-tunedmultimodalmod-
als and amateurs in the design field. Central to els (Ye et al., 2023; Li et al., 2023a,b; Awadalla
this challenge is the task of arranging these com- etal.,2023;OpenAI,2023b),extendingthecapabil-
ponents in an efficient and aesthetically pleasing itiesofLLMstounderstandandprocessinforma-
manner, a process known to be both tedious and tionacrossbothtextualandvisualdomains. This
time-consuming. ExistingtoolkitssuchasAdobe progressionnaturallyraisesthequestionofthepo-
Express1, Canva2, and PicsArt3, usually provide tentialapplicationofinstruction-followingmodels
fixed templates to users. These templates, while inthecomplexdomainofmultimodallayoutplan-
useful, often fail to fully accommodate the var- ning. However,employingthesemodelsforlayout
ied and evolving design needs of users, thereby planningpresentssignificantchallenges,asthetask
requiresintricatereasoningabilities,includingbut
1https://www.adobe.com/express/
2https://www.canva.com/ not limited to, cross-referencing multiple images
3https://picsart.com/ andperformingnumericalcalculations.
4202
rpA
32
]VC.sc[
1v17251.4042:viXraCoordinates Predicting Layout Recovering Layout Planning
The first figure is the background canvas of a design
The first figure is a design template with a width of 128 The first figure is a Facebook AD with a width of 128 poster with a width of 128 and a height of 128. The
and a height of 128; and it composes of various and a height of 128; and it composes of various following images are a few text components or logos
components. Predict the bounding box coordinates of components as listed in the following images. Predict to be added to the poster. Predict the bounding box
the component as specified in the second image. the bounding box coordinates of each component. coordinates of each component so that it would not
occlude the main object.
Multimodal Instruction-Following Model
Prediction: Prediction: Prediction:
component#0 {left: 0; top: 14; width: 25; height: 90; layer: 0;} component#0 {left: 10; top: 84; width: 81; height: 18; layer: 3;} component#0 {left: 4; top: 17; width: 79; height: 12;}
component#1 {left: 0; top: 0; width: 128; height: 77; layer: 0;} component#1 {left: 26; top: 31; width: 34; height: 7;}
component#2 {left: 0; top: 77; width: 96; height: 50; layer: 1;} component#2 {left: 29; top: 7; width: 27; height: 7;}
component#3 {left: 96; top: 77; width: 31; height: 50; layer: 2;} rendered result ➡
(a) (b) (c)
Figure2: Exampleinputsandoutputsofthethreelayoutreasoningtasks. (a)and(b)areexamplesfromCrello(Ya-
maguchi,2021),while(c)isanexamplefromPosterLayout(Hsuetal.,2023).
In this study, we propose DocLap, aiming to scenarios a (e.g., posters, Instagram posts, book
address the challenge of visually-rich document covers)withdefineddimensionsw (width)andh
layoutplanningusinginstruction-followingmod- (height). Thecanvasmayeitherbeblankorhavea
els. To equip these models with the necessary predefinedbackground.
knowledge beyond their primary focus on natu-
Instruction-FollowingFormat Toofferamore
ral language processing, we have devised three
adaptable solution and enhance user experience,
instruction-followingtasksfocusingonlayoutrea-
weapproachthisvisually-richlayoutplanningtask
soning. We evaluated our instruction-tuned Do-
inaninstruction-followingmanner(Yeetal.,2023;
cLap modelacrosstwobenchmarkdatasets,and
Lietal.,2023a,b;Awadallaetal.,2023;OpenAI,
thefindingsrevealthatourapproachnotonlysuc-
2023b). Themodel,inadditiontoreceivingthese-
ceedsinthisnovelapplicationbutalsooutperforms
quenceofdesigncomponentsi ,i ,...i ,willalso
thebaselineestablishedbyfew-shotGPT-4(V).Our 1 2 n
be given instructions I detailing the application
maincontributionsare:
scenariosaandthecanvassizepw,hq. Itistasked
• Weproposeanovelmethodforsolvingthelayout
with predicting the layout of each component in
planning task using instruction-following mod-
a structured format (Feng et al., 2023; Lin et al.,
els,openingnewavenuesforresearchindesign
2023). WeadoptCSStoencapsulatelayoutproper-
automation.
tiesincludingtop,left,width,height,and
• Wedevelopaninstructiondatasetfeaturingthree
anotherpropertylayerthatmanagesthestacking
layout reasoning tasks, aiming to enrich the re-
orderofpotentiallyoverlappingelements.
sourcesavailableforfutureresearch.
• Throughexperimentsontwobenchmarkdatasets,
Instruction-FollowingFormat Thetaskoflay-
we validate the feasibility of our approach and
outplanningencompasseschallengessuchasfol-
demonstrateitscompetitiveperformanceagainst
lowing instructions, cross-modal understanding,
few-shotGPT-4(V)models.
andnumericalreasoning. Toequipthemodelwith
essentialknowledge,wedesignedthreeinterrelated
2 Instruction-GuidedLayoutPlanning
tasks, as illustrated in Figure 2: (a) Coordinates
forVisually-RichDocuments
Predicting, where the model predicts the coordi-
TaskDefinition Visually-richdocumentsconsist natesofaspecificcomponentwithinagivendesign
ofdiversedesignelementsdistributedacrossacan- template; (b) Layout Recovering, which involves
vas. To maintain the integrity of original text de- predicting the coordinates of each component in
signs,textcontentisconvertedintoimagesinour a template given a sequence of components; and
setup. Thelayoutplanningtaskinvolvesarranging (c)LayoutPlanning,wherethemodelarrangesa
thesedesigncomponents,providedasasequence sequenceofcomponentsonacanvasbypredicting
ofimagesi ,i ,...i ,wherenrepresentsthecom- their coordinates. During preprocessing, compo-
1 2 n
ponentcount,ontoacanvasforspecificapplication nents smaller than 5% of the canvas size are ex-Express Crello PosterLayout Model mIoU Left Top Width Height
CoordinatesPredicting 581k 57k 26k #1 CanvasVAE 42.39 29.31 30.97 27.58 29.99
Train LayoutRecovering 160k 18k 9k #2 FlexDM 50.08 34.98 34.03 30.04 33.08
LayoutPlanning 160k 18k 9k
#3 GPT-40-shot 30.75 24.36 24.07 13.63 15.11
Val DesignLayout - 1493 591 #4 GPT-41-shot 29.97 26.09 23.71 13.94 13.33
#5 GPT-4V0-shot 28.81 19.96 18.09 10.45 10.08
Table1:Numberofexamplescontainedineachtraining
#6 GPT-4V1-shot 35.17 22.77 20.90 13.16 14.11
orvalidationtasksforthedatasetsusedinthisstudy.
#7 DocLap(Ours) 43.75 33.46 35.61 19.18 22.79
Table2:AutomaticevaluationresultsonCrelloshowing
cluded,andalltemplatesareresizedtoensurethe mIoUandtheaccuracyforleft,top,widthandheight.
longestedgedoesnotexceed128. Whileallthree
taskscontributetomodeltraining,onlytheLayout Model Occ.Ó Uti.Ò Rea.Ó
Planningtaskisevaluatedduringinference.
#1 DS-GAN 21.57 23.92 20.16
#2 GPT-40-shot 50.61 43.09 25.87
Model DocLapextendsmPLUG-Owl(Yeetal.,
#3 GPT-41-shot 47.92 38.00 25.34
2023), a multimodal framework integrating an #4 GPT-4V0-shot 36.67 33.26 24.39
#5 GPT-4V1-shot 36.39 20.24 26.03
LLM,avisualencoder,andavisualabstractormod-
#6 DocLap(Ours) 23.01 22.46 21.00
ule. Specifically,itemploysLlama-7bv1(Touvron
etal.,2023)astheLLMandCLIPViT-L/14(Rad- Table3: EvaluationresultsonPosterLayout. Occ.: oc-
clusionrate;Uti.: utilityrate;Rea.: unreadability.
fordetal.,2021)asthevisualencoder. Thevisual
abstractormoduleconvertsCLIP’svisualfeatures
into64tokensthatmatchthedimensionalityoftext
comparativeevaluationswithtext-onlyversionsof
embeddings, allowing for the simultaneous pro-
GPT-4andGPT-4V(OpenAI,2023a,b,c;gpt,2023)
cessingofmultiplevisualinputs. Weextendedthe
acrossbothtasks. Forthetext-onlyGPT-4evalua-
Llamav1vocabularywithnumericaltokensrang-
tions,visualcomponentsarenotdirectlysupplied.
ingfrom0to128. Theembeddingsoftheextended
Instead, we employ BLIP-2 (Li et al., 2023c) to
tokensarerandomlyinitialized,andthentunedin
generatetextualdescriptionsofeachcomponent.
furtherinstructiontuning.
Metrics ForCrelloevaluation,wemeasuremean
3 ExperimentalSetup Intersection-over-Union(mIoU)betweenpredicted
andactualboundingboxes,alongwithaccuracyin
Datasets Weconductexperimentsonlayoutplan-
width, height, left, and top dimensions following
ningforvisually-richdocumentswiththefollowing
FlexDM (Inoue et al., 2023). Accuracy is quan-
twobenchmarks: (1)Crello(Yamaguchi,2021)is
tified by assigning a score of 1 if the predicted
builtupondesigntemplatescollectedfromonline
valuefallsintothesame64-binquantizedrangeas
service. This task begins with an empty canvas,
thegroundtruth;otherwise,itscores0. Inassess-
challenging the model to organize the layouts of
ingPosterLayout,wefollowDS-GAN(Hsuetal.,
the provided visual components. (2) PosterLay-
2023)andemploycontent-awaremetrics, includ-
out(Hsuetal.,2023)startsfromnon-emptycanvas
ing(1)occlusionrateÓ,indicatingthepercentage
(background image for posters), and requires the
of primary objects obscured by design elements;
model to strategically place text, labels, and lo-
(2)utilityrateÒ,reflectingtheextenttowhichde-
gos. Ourtrainingdataissupplementedwithdesign
signcomponentscovernon-primaryobjectareas;
templates from Adobe Express. Detailed dataset
and (3) unreadabilityÓ, measuring the uniformity
statistics are available in Table 1. To ensure fair
ofareaswheretext-containingelementsareplaced.
comparison,validationexamplesarelimitedtono
more than 4 images, aligning with the input con- 4 Results&Analysis
straintsofGPT-4Vatthetimeofoursubmission.
Quantitative Results Table 2 shows the auto-
Illustrative examples from both datasets are pre-
maticevaluationresultsonCrellodataset. Thefirst
sentedinFigure2.
twolinesareresultsfrommodelsthataretrained
Baselines ForCrello,wecomparewithCanvas- withsupervisedlearning. Line#3-#6showfew-
VAE(Yamaguchi,2021)andFlexDM(Inoueetal., shotGPT-4(V)results,inwhichwenoticethatGPT-
2023). For PosterLayout, we compare with DS- 4Vsurpassestext-onlyGPT-4,andthatproviding
GAN(Hsuetal.,2023). Additionally,weinclude demonstrativeexamplesleadstobetterresultscom-FlexDM GPT4V Ours Ground-truth DS-GAN GPT4V Ours Ground-truth
(a)
(b)
Figure3: (a)mIoUvariationwiththenumberofvisual
components in design templates. (b) IoU correlation
withtherelativesizeofasinglevisualcomponent. Both Figure4: Qualitativecomparisonsforlayoutplanning
plotspertaintoCrello. resultsonCrello. GPT-4Vw/1-shotlearning.
DS-GAN GPT4V Ours Ground-truth
paredtozero-shotprompting. OurDocLap’sper-
formanFcleex(D#M7)surpGaPsTs4tVhefew-sOhuorstGPT-G4ro(uVnd)-otrnuth
both mIoU and aspect accuracies, but still falls
behindabitcomparedtoFlexDM(#2).
Table3presentsthePosterLayoutevaluationre-
sults,whichrevealsatrade-offbetweenocclusion
rateandutilityrateacrossmodels. GPT-4(V)mod-
els(#2-#5)exhibithighocclusionandutilityrates,
indicatingapropensityforpredictinglargerbound-
ingboxes. OurDocLap showsareducedocclusion
rate,accompaniedbyadecreaseinutilityrate. Re-
gardingunreadability,DocLap outperformsGPT-
4(V),thoughDS-GAN(#1)achievesthehighest Figure5: Qualitativecomparisonsforlayoutplanning
resultsonPosterLayout. GPT-4Vw/1-shotlearning.
performance, underscoring the efficacy of super-
visedmodelsinthiscontext.
tiveelements,haveadegreeofpositionalflexibility,
Effectsof#Component Figure3(a)revealsthat
allowingformultiplevalidplacements.
alllistedmodelsexhibithighmIoUfortemplates
withasinglecomponent. FlexDM’smIoUshows DemonstrativeExamples Figure4showsexam-
slightfluctuations,stabilizingaround50%. Incon- ples from Crello while Figure 5 shows examples
trast,mIoUforDocLap andGPT-4(V)decreases fromPosterLayout.
asthenumberofcomponentsincreases,indicating
that more complex scenarios involving more vi- 5 Conclusion
sualcomponentsmightposechallengestocurrent
Thisstudydemonstratesthepotentialofinstruction-
instruction-followingmodels.
followingmodelsinaddressingtheintricatetaskof
EffectsofComponentSize Figure3(b)demon- layoutplanningforvisuallyrichdocuments. The
stratesalinearcorrelationbetweentherelativesize positiveoutcomesobservedfromourexperiments
of a single visual component and the IoU of the ontwodistinctbenchmarksaffirmtheviabilityand
modelpredictionwiththegroundtruthforallmod- effectiveness of our methodology. This research
elsassessed. Thissuggeststhatsmallervisualcom- pavesthewayforfutureexplorationsintotheap-
ponentsposeagreaterchallengeforpreciseplace- plication of instruction-following models across
ment in accordance with the ground truth during variousdomains,highlightingtheirpotentialtorev-
layout planning. Typically, these small compo- olutionizetasksthatrequireanuancedunderstand-
nents, such as logos, small text boxes, or decora- ingofbothlanguageandvisualelements.Limitations traininglargeautoregressivevision-languagemodels.
ArXiv,abs/2308.01390.
This study, while pioneering in its approach to
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
simplifying the graphic design process through
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
instruction-following models, acknowledges sev-
Neelakantan,PranavShyam,GirishSastry,Amanda
eral limitations. First, the performance of our Askell, Sandhini Agarwal, Ariel Herbert-Voss,
model,DocLap,andGPT-4(V)diminishesasthe Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
complexity of the layout increases, particularly
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
withtheadditionofmorevisualcomponents. This
teusz Litwin, Scott Gray, Benjamin Chess, Jack
suggestsaneedforimprovedmodelrobustnessand Clark, ChristopherBerner, SamMcCandlish, Alec
adaptabilityinhandlingmoreintricatedesignsce- Radford, Ilya Sutskever, and Dario Amodei. 2020.
narios. Additionally,theevaluationmetrics,such Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
as mIoU and the binary accuracy measurement
volume 33, pages 1877–1901. Curran Associates,
forlayoutattributes,maynotfullycapturethenu-
Inc.
ances of aesthetic and functional design quality.
WeixiFeng,WanrongZhu,Tsu-JuiFu,VarunJampani,
Therelianceonthesemetricsmightoverlookthe
Arjun Reddy Akula, Xuehai He, S Basu, Xin Eric
subjectiveandcontext-specificnatureofeffective
Wang,andWilliamYangWang.2023. LayoutGPT:
design,indicatingapotentialareafordeveloping Compositionalvisualplanningandgenerationwith
morecomprehensiveevaluationframeworks. largelanguagemodels. InThirty-seventhConference
onNeuralInformationProcessingSystems.
EthicsStatement
Hsiao-AnHsu,XiangtengHe,YuxinPeng,Hao-Song
Kong,andQingZhang.2023. Posterlayout: Anew
Ourworkoninstruction-followingmodelsforlay-
benchmark and approach for content-aware visual-
outplanning,whileinnovative,introducespotential textualpresentationlayout. 2023IEEE/CVFConfer-
risksincludingover-relianceonautomation,which ence on Computer Vision and Pattern Recognition
mayimpedethedevelopmentofdesignskillsand (CVPR),pages6018–6026.
creativity. Importantly,ourmodeldoesnotgener-
NaotoInoue,KotaroKikuchi,EdgarSimo-Serra,Mayu
atenewvisualcontent;allpredictionsarebasedon Otani, and Kota Yamaguchi. 2023. Towards flexi-
existing components provided by users. The out- blemulti-modaldocumentmodels. 2023IEEE/CVF
ConferenceonComputerVisionandPatternRecog-
putsaresolelylayoutsintextformats,mitigating
nition(CVPR),pages14287–14296.
risksrelatedtocopyrightinfringementandoriginal-
ity. However,therelianceonautomatedtoolscould BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,
Fanyi Pu, Jingkang Yang, C. Li, and Ziwei Liu.
leadtoahomogenizationofdesignaestheticsand
2023a. Mimic-it: Multi-modal in-context instruc-
potentiallyamplifybiasespresentintheinputdata.
tiontuning. arXivpreprintarXiv:2306.05425.
Addressingthesechallengesrequirescarefulcon-
BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,
siderationoftheethicalimplicationsofautomated
Jingkang Yang, and Ziwei Liu. 2023b. Otter: A
designtoolsandthepromotionofresponsibleus-
multi-modalmodelwithin-contextinstructiontuning.
agetocomplementhumancreativity. Notedhere arXivpreprintarXiv:2305.03726.
thatweutilizeChatGPTtopolishthewritingand
JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.
ensureclarityandconcisenessinthepresentation
Hoi.2023c. Blip-2: Bootstrappinglanguage-image
ofourresearch,withoutalteringthefundamental pre-training with frozen image encoders and large
natureoftheworkoritsimplications. language models. In International Conference on
MachineLearning.
JiaweiLin,JiaqiGuo,ShizhaoSun,ZijiangJamesYang,
References
Jian-Guang Lou, and Dongmei Zhang. 2023. Lay-
outprompter: Awakenthedesignabilityoflargelan-
2023. Chatgpt can now see, hear, and
guagemodels. InThirty-seventhConferenceonNeu-
speak. https://openai.com/blog/
ralInformationProcessingSystems.
chatgpt-can-now-see-hear-and-speak.
OpenAI.2023a. Gpt-4technicalreport.
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-
sel,YusufHanafy,WanrongZhu,KalyaniMarathe,
OpenAI.2023b. Gpt-4v(ision)systemcard.
YonatanBitton,SamirYitzhakGadre,ShioriSagawa,
JeniaJitsev,SimonKornblith,PangWeiKoh,Gabriel OpenAI. 2023c. Gpt-4v(ision) technical work
Ilharco, Mitchell Wortsman, and Ludwig Schmidt. and authors. https://cdn.openai.com/
2023. Openflamingo:Anopen-sourceframeworkfor contributions/gpt-4v.pdf.LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,John
Schulman,JacobHilton,FraserKelton,LukeMiller,
Maddie Simens, Amanda Askell, Peter Welinder,
PaulFChristiano,JanLeike,andRyanLowe.2022.
Traininglanguagemodelstofollowinstructionswith
humanfeedback. InAdvancesinNeuralInformation
ProcessingSystems,volume35,pages27730–27744.
CurranAssociates,Inc.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
GretchenKrueger,andIlyaSutskever.2021. Learn-
ingtransferablevisualmodelsfromnaturallanguage
supervision. InInternationalConferenceonMachine
Learning.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample.2023. Llama: Open
and efficient foundation language models. ArXiv,
abs/2302.13971.
KotaYamaguchi.2021. Canvasvae: Learningtogener-
atevectorgraphicdocuments. 2021IEEE/CVFIn-
ternationalConferenceonComputerVision(ICCV),
pages5461–5469.
QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,Ming
Yan,YiZhou,JunyanWang,AnwenHu,Pengcheng
Shi,YayaShi,ChenliangLi,YuanhongXu,Hehong
Chen,JunfengTian,QiangQi,JiZhang,andFeiyan
Huang.2023. mplug-owl: Modularizationempowers
large language models with multimodality. ArXiv,
abs/2304.14178.