Title: Evaluating Physician-AI Interaction for Cancer Management: Paving the Path towards
Precision Oncology
Zeshan Hussain, PhD1*†, Barbara D. Lam, MD2,3†, Fernando A. Acosta-Perez4, Irbaz Bin Riaz,
MD5, Maia Jacobs, PhD6, Andrew J. Yee, MD7, and David Sontag, PhD1
1Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology
2Division of Hematology and Oncology, Department of Medicine, Beth Israel Deaconess Medical
Center
3Division of Clinical Informatics, Department of Medicine, Beth Israel Deaconess Medical Center
4Department of Industrial and Systems Engineering, University of Wisconsin-Madison
5Division of Hematology and Oncology, Department of Medicine, Mayo Clinic, Arizona
6Department of Computer Science, McCormick School of Engineering, Northwestern University
7Division of Hematology and Oncology, Department of Medicine, Massachusetts General Hospital
†Co-first authors.
1Abstract
Purpose: Evaluate how clinicians approach clinical decision-making when given findings from
both randomized controlled trials (RCTs) and machine learning (ML) models.
Methods: We designed a clinical decision support system (CDSS) that displays survival curves
and adverse event information from a synthetic RCT and ML model for 12 patients with multiple
myeloma. We conducted an interventional study in a simulated setting to evaluate how clinicians
synthesized the available data to make treatment decisions. Participants were invited to participate
in a follow-up interview to discuss their choices in an open-ended format.
Results: The cohort contained 32 physicians. When ML model results were concordant with RCT
results, physicians had increased confidence in treatment choice compared to when they were
given RCT results alone. When ML model results were discordant with RCT results, the majority
of physicians followed the ML model recommendation in their treatment selection. Perceived
reliability of the ML model was consistently higher after physicians were provided with data on
how it was trained and validated. Follow-up interviews revealed four major themes: (1) variability
in what variables participants used for decision-making, (2) perceived advantages to an ML model
over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and
(4) perception that this type of study is an important thought exercise for clinicians.
Conclusion: ML-based CDSSs have the potential to change treatment decisions in cancer
management. Meticulous development and validation of these systems as well as clinician training
are required before deployment.
21 Introduction
Cancer treatment has changed dramatically over the past decade. Advances in molecular
sequencing and innovative therapies such as immune checkpoint inhibitors, bispecific antibodies,
and CAR T-cells have rapidly evolved the treatment landscape [1–3]. Historically, the field was
driven by landmark clinical trials [4–11] but now, the highly personalized characterization of each
patient’s disease coupled with the explosion in treatment options has made it increasingly difficult
to capture every patient scenario in a clinical trial setting.
Multiple myeloma (MM) is often highlighted as an area where the rapid development of new
therapies has outpaced our ability to conduct clinical trials [12, 13]. The FDA approved over a
dozen new agents for MM in the last 10 years [14] and registry studies have demonstrated that
approximately 40% of patients with MM do not meet inclusion criteria for the phase 3 trials that
led to their own treatments [13]. While randomized controlled trials (RCTs) are considered to
produce the highest caliber of evidence in medicine [15], the reality is that the gap between clinical
trial and real-world settings will likely worsen in the field of MM, where clinicians treat with
multi-drug combinations and patients need multiple lines of therapy [12].
Clinical decision support systems (CDSSs) offer an alternate path to precision oncology. Modern
CDSSs can leverage machine learning (ML) models trained on vast amounts of data to provide
personalized treatment recommendations. While several CDSSs for cancer have been proposed
and published in the literature [16–20], few make it to the bedside due to challenges in clinical
validation and implementation [21]. Clinicians believe ML has a role to play in clinical care [22–
24] and studies have explored how clinicians use ML recommendations for treatment selection
3[25], how clinicians perceive ML recommendations in terms of trust, interpretability, and
diagnostic accuracy [26], and how ML recommendations influence decisions [27]. However, little
work has been done to understand how clinicians interact with ML-based systems in conjunction
with current standard-of-care evidence from RCTs.
In oncology, clinicians need to synthesize all available evidence to maximize patient survival and
quality of life while minimizing adverse events. As ML systems proliferate, clinicians will be
forced to reconcile findings from RCTs and ML models. If an ML model produces similar
outcomes as an RCT, a clinician may feel more confident in their decision. Inevitably, however,
an ML model will produce outcomes that contradict RCT data. Understanding how clinicians
navigate these complex scenarios can better prepare us for the implementation of ML-based
CDSSs in real-world clinical settings.
In this study, we design a CDSS that displays survival and adverse event data from a synthetic
RCT and ML model and evaluate how clinicians integrate the available data to make treatment
decisions for twelve patients diagnosed with MM.
2 Methods
2.1 Study Design
Participants reviewed survival curves and adverse event outcomes from an RCT and an ML model
for 12 patient scenarios (A-L) using a web-based CDSS we created (Appendix Figure 1) and
answered questions about treatment choice, confidence, and perceived reliability of the ML model.
This study was deemed IRB-exempt by the Massachusetts Institute of Technology.
42.2 Patient Scenarios and RCT/ML Combinations
Each scenario was based on the same clinical vignette of an MM patient with four changing
variables: chronic kidney disease (or not), chronic obstructive pulmonary disease (or not),
cytogenetic risk profile (normal versus high-risk), and Eastern Cooperative Oncology Group status
(0 versus 3). These variables determined whether the patient was well represented in the RCT or
ML model and influenced the ML model’s predictions (Appendix Figure 2). Different
combinations of RCT and ML results were created and presented to participants in a randomized
fashion (Figure 1a).
2.3 Tiered Information Approach
Each patient scenario had three to four “tiers” of information (Figure 1b). Tier 1 provided RCT
outcomes only. Tier 2 added ML model outcomes. Tier 3 added information about how the ML
model was trained, i.e., whether the patient was represented in the training cohort and external
validation results (Appendix Figure 3a). Three patient scenarios (C, E, I) included tier 4 data,
which described whether leveraging causal inference tools to replicate results of the RCT in the
observational dataset was successful or not (Appendix Figure 3b).
5Tier I: Randomized Controlled Trial (R CT ) / Population Level Data
Tier II : Machine Learning ( ML ) / Patient Level Data
Tier III : some information on the Machine Learning model
Tier IV : more information on the Machine Learning model
Figure 1a (top): Combinations of RCT and ML outcomes presented to study participants. We
excluded scenarios in which the patient 1) met RCT inclusion criteria and 2) the ML model was
not trained on any similar patients in an attempt to shorten the study and remove less compelling
scenarios. RCT results always show survival benefit with the red pill and similar adverse events
6compared to the blue pill. Figure 1b (bottom): For each patient scenario, participants were
presented with increasing tiers of data.
2.4 Data Collection
The study was piloted with two Hematology Oncology physicians for clarity, length, and clinical
relevance (final survey in Appendix File 4). Between January and April 2023, we recruited
physicians in Internal Medicine and Hematology/Oncology from academic institutions via email
to participate in our study. Participants were offered a gift card as incentive. Cases were presented
in a randomized fashion. At each tier of information, participants were asked to select a treatment
(“red pill” or “blue pill”), rate their confidence in treatment choice on a Likert scale from 1-10
and, when ML data was available, rate their perceived reliability of the model on a Likert scale
from 1-10.
All participants were invited to participate in a semi-structured exit interview. They were asked to
“think aloud” as they worked through a single scenario (interview protocol in Appendix File 5,
COREQ checklist in Appendix File 8). At least one researcher (BDL, ZH) was present during the
interview and took field notes.
2.5 Data Analysis
We used descriptive statistics to analyze respondent characteristics. For each scenario, we ran two-
sample paired t-tests to compare the changes in confidence and reliability between tier 2 and 1,
and tier 3 and 2. We utilized Shapiro-Wilk tests to assess normality of the confidences and the
reliability measurements. A power analysis for the paired t-tests with continuous outcomes
revealed a requirement of 34 physicians to detect a mean difference of 1 with a standard deviation
7of 2 (given the range of outcomes is 1-10). All p-values are reported after adjustment via the
Holm-Bonferroni correction. We also ran McNemar’s tests with Holm-Bonferroni correction to
assess the difference in proportions of blue pill selection at different tiers in order to characterize
the extent of treatment switching. Two researchers (BDL, ZH) analyzed the exit interviews for
themes based on field notes, and the end-of-study free-text responses using Braun and Clarke’s
methods for thematic analysis [28].
3 Results
A total of 284 physicians were invited to the study and 32 participated, for a response rate of
11.3%. Half were internal medicine residents and half were hematology and oncology fellows and
attendings (Table 1).
8Demographic N = 32, n(%)
Age
21-30 y.o. 16 (50%)
31-40 y.o. 16 (50%)
Sex
Male 23 (72%)
Female 9 (28%)
Race
White Caucasian 22 (69%)
Asian 7 (22%)
Hispanic 2 (6%)
Black or African American 1 (3%)
Clinical Role
Internal Medicine Resident 16 (50%)
Hematology Oncology Fellow 13 (41%)
Hematology Oncology Attending 3 (9%)
Clinical Specialty*
General Medicine 15 (47%)
Hematology 11 (34%)
Oncology 10 (31%)
Other 2 (6%)
*Participants could select more than one, thus totals
may sum to greater than 100%
Table 1: Study cohort
3.1 Quantitative Analysis
Patient meets inclusion criteria for RCT and ML model (scenarios A, B, C, D)
After evaluating all available data for scenario A, where ML model results were concordant with
RCT results, participants had the highest treatment confidence across all scenarios at 7.84 (+/-
1.18). In scenario B where the ML model showed benefit with red pill but worse adverse events,
the majority of participants chose to treat with red pill but reported a decrease in confidence (p =
9.05). In scenario C where the ML model showed no benefit with red pill and similar adverse events,
the majority of participants switched to blue pill (p<0.001). In scenario D where the ML model
showed no benefit with red pill and worse adverse events, the majority of participants switched to
blue pill (p<0.001) (Figure 2). Across scenarios B, C, and D, confidence decreased after seeing
ML results (B: p = 0.05, C: p = 0.05, D: p = 0.36), but increased when participants learned that the
model was trained on patients like theirs (B: p = 0.002, C: p = 0.17, D: p = 0.06).
Figure 2: In scenarios A-D, the patient meets inclusion criteria for the RCT and is well represented
in the training data used for the ML model. Tier 1 provided RCT results alone, tier 2 added ML
model results, and tier 3 added information on the quality of the ML model.
10Patient does not meet inclusion criteria for RCT but ML model was trained on data that
represents the patient well (scenarios E, F, G, H)
In scenario E where ML model results were concordant with RCT results, confidence and
perceived reliability of the model increased as participants saw more tiers of evidence. In scenario
F where the ML model showed benefit with red pill but worse adverse events, about half of the
participants chose to treat with blue pill instead. In scenarios G and H where the ML model showed
no benefit with red pill, the majority of participants switched to treat with blue pill upon seeing
ML model results (p = 0.002, p = <0.001). Across these four scenarios, confidence and perceived
reliability of the model increased when participants learned that the ML model had been trained
on patients like theirs (Figure 3).
11Figure 3: In scenarios E-H, the patient does not meet inclusion criteria for the RCT but is well
represented in the training data used for the ML model. Tier 1 provided RCT results alone, tier 2
added ML model results, and tier 3 added information on the quality of the ML model.
Patient does not meet inclusion criteria for RCT and ML model was not trained on data that
represents the patient well (scenarios I, J, K, L)
In scenario I where the RCT and ML model results were concordant but neither represented the
patient well, most participants chose to treat with red pill. In scenario J, the ML model showed
worse adverse events with blue pill and most participants maintained their choice of treating with
red pill. In scenario K, the ML model showed no benefit with red pill and similar adverse events;
the majority of participants switched treatment to blue pill (p <0.001), and maintained their
treatment choice even after learning the ML model had not been trained on patients like theirs. In
scenario L, the ML model showed no benefit with red pill and worse adverse events with blue pill,
and about half of the participants chose to treat with red pill. Overall, confidence and perceived
reliability of the model decreased across all four scenarios when participants learned that the ML
model had not been trained on patients like theirs (Figure 4).
12Figure 4: In scenarios I-L, the patient does not meet inclusion criteria for the RCT and is not well
represented in the training data used for the ML model. Tier 1 provided RCT results alone, tier 2
added ML model results, and tier 3 added information on the quality of the ML model.
3.2 Replication Experiment
Participants were presented with replication experiment results (tier 4 data) in scenarios C, E, I.
When the replication experiment was successful (ML model results matched RCT results on a
matched observational cohort), confidence in treatment and perceived reliability of the model
tended to increase. Both tended to decrease when replication failed. These changes were
statistically significant in scenario E, where the patient was not RCT-eligible but was well
represented in the model training data (Appendix Figure 6).
133.3 Qualitative Analysis of Open-ended Questions
When asked how they evaluate adverse event data, the majority of participants described looking
for specific organ side effects and some discussed evaluating that in the context of their patient’s
comorbidities. One participant reported that the total number of adverse events was important in
decision-making. When asked how they approach treatment selection when their patients do not
meet inclusion criteria, the majority reported that adverse events are weighed more heavily.
Several described working to balance the benefits of survival outcomes with adverse event data.
When asked how ML results impact clinical decision-making, the majority of participants reported
that they find personalized data helpful and that they would incorporate it into their decision-
making. “[RCT] results are much harder to generalize for patients who don’t nicely meet the
inclusion criteria... with support from ML datasets, we could feel more confident in making more
personalized treatment plans and decisions with our patients,” one participant wrote. Several
reported that concordant results between the RCT and ML model increased confidence: “When
they were in agreement, it was a nice confirmation. When there was discrepancy, [ML data] was
easy to throw out.” A few reported that ML model estimates are not helpful with one participant
stating, “They may bias towards increasing confidence but [they] rarely changed my mind.”
Participants were also asked to describe the replication procedure (tier 4) in their own words.
Several were not sure or provided an incorrect definition, but the majority were able to do so,
stating, “Confirming the ML model was reliable” or “Verifying reproducibility.” Several also
correctly described the procedure as “simulating” an RCT using observational data.
3.4 Exit Interview; Qualitative Analysis of Scenario K
14Six of the 32 participants (18.8%) elected to participate. Three were Internal Medicine residents,
two were Hematology and Oncology fellows and one was an attending, a sampling representative
of the larger participant group. Four major themes were identified in analysis: (1) there is
variability in what clinical factors participants use in their decision-making, (2) there are perceived
advantages to an ML model over RCT data, (3) uncertainty around decision-making when
participants learned that the ML model had not been trained on similar patients, and (4) the
perception that these types of studies are important thought exercises (Appendix File 7).
Participants described evaluating performance status and its reversibility, comorbidities and their
effects on drug metabolism, as well as adverse events and their perceived impact on quality of life.
They described advantages of an ML model over an RCT, including expressivity, training on real-
world data, improved external validity, and the ability to give individualized recommendations.
When shown the finding that most of the 32 participants had chosen treatment blue despite
learning that the ML model had not been trained on patients like theirs, none of the participants
expressed surprise. Several shared that they also felt conflicted in their treatment choice—“There’s
no good, right answer”—and some reported that physicians may be less likely to identify
weaknesses in an ML model because they are not trained to critically appraise them. One
participant noted, “ML models in general do not seem to do as well extrapolating to things outside
their training sets,” and explained that they would rather a human extrapolate RCT findings. In
contrast, another participant appreciated being told the lack of support in the ML training data: “I
thought of it as a disclaimer and not a big deal since [the model] had good performance,” and went
on to say, “It gives me more confidence because it’s being honest about it.” All participants
reported that the simulated interventional study was a helpful thought exercise. In the words of
15one participant: “It forced me to think through data, how it applies to my patient, and how to
maximize benefit.”
4 Discussion
In our study evaluating how clinicians couple RCT with ML data to make treatment decisions,
three key themes emerged. First, confidence was highest when RCT and ML findings were
concordant. Second, clinicians favored treatments that led to better survival outcomes, whether
that benefit was suggested by an RCT or an ML model. Third, clinicians were likely to follow ML
estimates even before information about how the model was trained or validated was given.
Analysis of reported confidence and free-text responses showed that clinicians are most confident
when the RCT and ML model represented their patient well and the outcomes matched. When
findings were discordant, the majority of participants switched to the treatment supported by the
ML model. Clinicians may prioritize survival outcomes over other types of information, consistent
with prior reports in the cancer literature [29]. In our study, this preference took place whether the
benefit was estimated by the RCT or the ML model, and before participants received information
on how the model was trained and validated. In scenarios where the patient did not meet inclusion
criteria for the RCT but was represented in the ML training data—a clinical situation that will be
increasingly common in the future—participants were more likely to adhere to ML model
estimates. Interestingly, in scenarios where the patient did not meet inclusion criteria for the RCT
and was not represented in the ML training data, the majority of participants continued to rely on
ML-generated data to make treatment decisions.
16In contrast, in our qualitative assessment of scenario K, participants tended to favor the RCT-
supported treatment when they were told the ML model was not trained on patients like theirs.
This contradiction to what we found in the study suggests there may be potential biases at play.
Participants may make different decisions when being evaluated by a researcher. Our study was
voluntary, and clinicians who elected to participate may be more likely to view ML favorably.
Regardless, our contradictory findings raise questions of how clinicians will interact with ML
models when they are not being observed.
Our results suggest clinicians may incorporate ML estimates into decision-making before
evaluating the quality of the model. This highlights the importance of filtering which models make
it to the bedside. Some have called for using RCTs to evaluate models [30], but this approach is
limited by cost and time [31, 32]. Similar to what has happened with the proliferation of cancer-
directed therapies, the development of ML models will likely outpace our ability to conduct robust
trials. As validated and unvalidated models enter the clinical realm, it becomes increasingly
important to train clinicians in how to critically appraise them. National organizations have called
for increasing clinician education on artificial intelligence (AI) [33]. Case studies of challenging
scenarios may help clinicians develop a mental model for how they would manage AI in clinical
practice.
Model developers can also work to improve clinician interpretation. In scenarios where
participants were presented with a causal inference approach to validating the model, confidence
in treatment dropped significantly when the replication procedure failed. This approach may be
useful for contextualizing ML data for clinicians [34, 35]. Other strategies for model safety include
17conveying uncertainty and improving explainability so that users can better weigh decision-
making [36, 43]. However, one study showed that an explainable model may make it more difficult
for users to detect errors, perhaps due to information overload [37]. Methods to improve
explainability can hide biases [38] and may be interpreted differently across users, as demonstrated
by our exit interviews. How ML model results are displayed may also influence users. We designed
a CDSS where the ML model provided outcomes without making recommendations based on prior
work showing that models that give prescriptive advice rather than descriptive evidence are more
likely to bias users [27]. Further research exploring how user interfaces can influence clinicians
and how clinicians can inform interface design (e.g. [41,42]) is needed.
There are limitations to our study. We had a low response rate, though not dissimilar to other
reported web-based study response rates among clinicians [39]. All participants were younger than
40 years old and 91% were in training via residency or fellowship. Younger clinicians may be
more likely to view ML favorably [40,44], though challenges in interpreting ML outputs were
likely to occur regardless of training level. Further qualitative studies that include a more
heterogeneous group of clinicians can help us better understand how RCT and ML data are used
in clinical decision-making.
Acknowledgments
Disclosures: The authors have no conflicts of interest to disclose.
Funding: This research was generously supported by an ASPIRE award from The Mark
Foundation for Cancer Research. ZH was additionally supported by the National Institutes of
Health under Award Number F30CA268631.
18Data access, responsibility, and analysis: ZH and BDL had full access to all the data in the study
and take responsibility for the integrity of the data and the accuracy of the data analysis.
Data sharing: Data is available upon request.
References
1. Esfahani K, Roudaia L, Buhlaiga N, Del Rincon S, Papneja N, and Miller W. A review of
cancer immunotherapy: from the past, to the present, to the future. Current Oncology
2020;27:87–97.
2. Sterner RC and Sterner RM. CAR-T cell therapy: current limitations and potential strategies.
Blood cancer journal 2021;11:69.
3. Ma J, Mo Y, Tang M, et al. Bispecific antibodies: from research to clinical application.
Frontiers in Immunology 2021:1555.
4. Gamboa AC and Maithel SK. The landmark series: gallbladder cancer. Annals of Surgical
Oncology 2020;27:2846–58.
5. Fisher CS, Margenthaler JA, Hunt KK, and Schwartz T. The landmark series: axillary
management in breast cancer. Annals of surgical oncology 2020;27:724–9.
6. Kumar SK, Callander NS, Alsina M, et al. Multiple myeloma, version 3.2017, NCCN clinical
practice guidelines in oncology. Journal of the National Comprehensive Cancer Network
2017;15:230–69.
7. Kumar SK, Dispenzieri A, Lacy MQ, et al. Continued improvement in survival in multiple
myeloma: changes in early mortality and outcomes in older patients. Leukemia
2014;28:1122–8.
198. Durie BG, Hoering A, Abidi MH, et al. Bortezomib with lenalidomide and dexamethasone
versus lenalidomide and dexamethasone alone in patients with newly diagnosed myeloma
without intent for immediate autologous stem-cell transplant (SWOG S0777): a randomized,
open-label, phase 3 trial. The Lancet 2017;389:519–27.
9. Attal M, Harousseau JL, Stoppa AM, et al. A prospective, randomized trial of autologous bone
marrow transplantation and chemotherapy in multiple myeloma. New England Journal of
Medicine 1996;335:91–7.
10. Child JA, Morgan GJ, Davies FE, et al. High-dose chemotherapy with hematopoietic stem-
cell rescue for multiple myeloma. New England Journal of Medicine 2003;348:1875–83.
11. Attal M, Lauwers-Cances V, Hulin C, et al. Lenalidomide, bortezomib, and dexamethasone
with transplantation for myeloma. New England Journal of Medicine 2017;376:1311–20.
12. Rajkumar SV. Multiple myeloma: 2022 update on diagnosis, risk stratification, and
management. American journal of hematology 2022;97:1086–107.
13. Terpos E, Mikhael J, Hajek R, et al. Management of patients with multiple myeloma beyond
the clinical-trial setting: understanding the balance between efficacy, safety and tolerability,
and quality of life. Blood cancer journal 2021;11:40.
14. FDA U et al. Oncology (cancer)/hematologic malignancies approval notifications. 2021.
15. Burns PB, Rohrich RJ, and Chung KC. The levels of evidence and their role in evidence-
based medicine. Plastic and reconstructive surgery 2011;128:305.
16. Allegra A, Tonacci A, Sciaccotta R, et al. Machine learning and deep learning applications in
multiple myeloma diagnosis, prognosis, and treatment selection. Cancers 2022;14:606.
2017. Cammarota G, Ianiro G, Ahern A, et al. Gut microbiome, big data and machine learning to
promote precision medicine for cancer. Nature reviews gastroenterology & hepatology
2020;17:635– 48.
18. Ozer ME, Sarica PO, and Arga KY. New machine learning applications to accelerate
personalized medicine in breast cancer: rise of the support vector machines. Omics: a journal
of integrative biology 2020;24:241–6.
19. Zhang S, Bamakan SMH, Qu Q, and Li S. Learning for personalized medicine: a
comprehensive review from a deep learning perspective. IEEE reviews in biomedical
engineering 2018;12:194– 208.
20. Qureshi R, Basit SA, Shamsi JA, et al. Machine learning based personalized drug response
prediction for lung cancer patients. Scientific Reports 2022;12:18935.
21. Topol EJ. High-performance medicine: the convergence of human and artificial intelligence.
Nature medicine 2019;25:44–56.
22. Emani S, Rui A, Rocha HAL, et al. Physicians’ Perceptions of and Satisfaction With Artificial
Intelligence in Cancer Treatment: A Clinical Decision Support System Experience and
Implications for Low-Middle–Income Countries. JMIR cancer 2022;8:e31461.
23. Parikh RB, Manz CR, Nelson MN, et al. Clinician perspectives on machine learning
prognostic algorithms in the routine care of patients with cancer: a qualitative study.
Supportive Care in Cancer 2022;30:4363–72.
24. Scheetz J, Rothschild P, McGuinness M, et al. A survey of clinicians on the use of artificial
intelligence in ophthalmology, dermatology, radiology and radiation oncology. Scientific
reports 2021;11:1–10.
2125. Jacobs M, Pradier MF, McCoy Jr TH, Perlis RH, Doshi-Velez F, and Gajos KZ. How
machinelearning recommendations influence clinician treatment selections: the example of
antidepressant selection. Translational psychiatry 2021;11:108.
26. Gaube S, Suresh H, Raue M, et al. Do as AI say: susceptibility in deployment of clinical
decision-aids. NPJ digital medicine 2021;4:31.
27. Adam H, Balagopalan A, Alsentzer E, Christia F, and Ghassemi M. Mitigating the impact of
biased artificial intelligence in emergency decision-making. Communications Medicine
2022;2:149.
28. Braun V and Clarke V. Using thematic analysis in psychology. Qualitative research in
psychology 2006;3:77–101.
29. Valentı V, Ramos J, Pérez C, et al. Increased survival time or better quality of life? Tradeoff
between benefits and adverse events in the systemic treatment of cancer. Clinical and
Translational Oncology 2020;22:935–42.
30. Kelly CJ, Karthikesalingam A, Suleyman M, Corrado G, and King D. Key challenges for
delivering clinical impact with artificial intelligence. BMC medicine 2019;17:1–9.
31. Lam TY, Cheung MF, Munro YL, Lim KM, Shung D, and Sung JJ. Randomized Controlled
Trials of Artificial Intelligence in Clinical Practice: Systematic Review. Journal of Medical
Internet Research 2022;24:e37188.
32. Plana D, Shung DL, Grimshaw AA, Saraf A, Sung JJ, and Kann BH. Randomized clinical
trials of machine learning interventions in health care: a systematic review. JAMA Network
Open 2022;5:e2233946–e2233946.
2233. Lomis K, Jeffries P, Palatta A, et al. Artificial intelligence for health professions educators.
NAM perspectives 2021;2021.
34. Hussain Z, Oberst M, Shih MC, and Sontag D. Falsification before Extrapolation in Causal
Effect Estimation. Arxiv preprint arXiv:2209.13708 2022.
35. Hussain Z, Shih MC, Oberst M, Demirel I, and Sontag D. Falsification of Internal and
External Validity in Observational Studies via Conditional Moment Restrictions. In:
International Conference on Artificial Intelligence and Statistics. PMLR. 2023:5869–98.
36. Doshi-Velez F and Kim B. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608 2017.
37. Poursabzi-Sangdeh F, Goldstein DG, Hofman JM, Wortman Vaughan JW, and Wallach H.
Manipulating and measuring model interpretability. In: Proceedings of the 2021 CHI
conference on human factors in computing systems. 2021:1–52.
38. Balagopalan A, Zhang H, Hamidieh K, Hartvigsen T, Rudzicz F, and Ghassemi M. The road
to explainability is paved with bias: Measuring the fairness of explanations. In: 2022 ACM
Conference on Fairness, Accountability, and Transparency. 2022:1194–206.
39. Barnhart BJ, Reddy SG, and Arnold GK. Remind me again: physician response to web
surveys:
the effect of email reminders across 11 opinion survey efforts at the American Board of
Internal Medicine from 2017 to 2019. Evaluation & the Health Professions 2021;44:245–59.
40. Oh S, Kim JH, Choi SW, Lee HJ, Hong J, and Kwon SH. Physician confidence in artificial
intelligence: an online mobile survey. Journal of medical Internet research 2019;21:e12422.\
2341. Staes, Catherine J., et al. "Design of an interface to communicate artificial intelligence-based
prognosis for patients with advanced solid tumors: a user-centered approach." Journal of the
American Medical Informatics Association 31.1 (2024): 174-187.
42. Buçinca, Zana, Maja Barbara Malaya, and Krzysztof Z. Gajos. "To trust or to think: cognitive
forcing functions can reduce overreliance on AI in AI-assisted decision-
making." Proceedings of the ACM on Human-Computer Interaction 5.CSCW1 (2021): 1-21.
43. Radakovich, Nathan, Matthew Nagy, and Aziz Nazha. "Machine learning in haematological
malignancies." The Lancet Haema tology 7.7 (2020): e541-e550.
44. Tschandl, Philipp, et al. "Human–computer collaboration for skin cancer recognition." Nature
Medicine 26.8 (2020): 1229-1234.
24Appendix
Appendix Figure 1: Clinical decision support system created for the study
Participants used a web-based clinical decision support system (CDSS) created for the study called
the “Multiple Myeloma Decision Support Tool” (MM-DST) to view all randomized controlled
trial (RCT) and machine learning (ML) outcomes. The tool allowed participants to select one of
the twelve patient scenarios and evaluate the RCT and ML survival curves and adverse event
outcomes. All data was synthetic in nature so as not to bias participants who may have opinions
about existing RCTs and ML models. Survival outcomes were presented as progression free
survival curves. Adverse event data were presented as the frequency of different symptoms.
Cytopenias, upper respiratory tract infections, and infusion-related reactions varied most
significantly.
25Appendix Figure 2: Changing clinical variables in the patient scenarios
Variable Value Output
ECOG 0 Patient meets inclusion criteria of RCT
3 Patient does not meet inclusion criteria of RCT
History of No ML model was trained on patients similar to current patient
CKD
Yes ML model was not trained on any patient similar to current patient
Cytogenetic Normal Risk ML model shows benefit with red pill
Risk
High Risk ML model shows no benefit with red pill
History of No ML model shows similar adverse events to RCT
COPD
Yes ML model shows worse adverse events with red pill*
A 62-year-old man with a history of [COPD and/or CKD] and nonalcoholic fatty liver disease (NAFLD)
presents to his liver doctor for routine check up and is found to have mildly elevated liver function tests
(LFTs). As part of his workup, quantitative immunoglobulins are sent and are found to be low. Next week,
his LFTs have normalized and further labs show:
WBC: 4.6 K/uL (within normal range)
Hemoglobin: 10.1 g/dL (abnormal)
Platelet count: 203 K/uL (within normal range)
Creatinine: [1.0 mg/dL (within normal range) / 1.5 mg/dL (abnormal but at his baseline)]
Albumin: 4.4 g/dL (within normal range)
Calcium: 9.5 mg/dL (within normal range)
Serum protein electrophoresis: A monoclonal IgG Kappa is present based on immunofixation. An
abnormal band is present in the gamma region, roughly 3,574 mg/dL (3.5 g/dL) of total protein (abnormal).
Free Kappa: 60.2 mg/L (abnormal)
Free Lambda: 25.8 mg/L (within normal range)
Free Kappa/Lambda ratio: 2.33 (abnormal)
A PET scan shows a lytic lesion in the right tibia measuring 2.0cm that is FDG avid, concerning for
multiple myeloma.
A bone marrow biopsy shows plasma cells occupying 70% of the bone marrow core. Further
characterization with cytogenetics and FISH show a [normal risk / high risk] profile. He is a Stage II based
on the International Staging System (ISS).
The patient is referred to your hematology clinic. He reports [no symptoms and is completely independent
at home (ECOG 0) / leg pain and needs significant help at home (ECOG 3)]. He is eager to get started on
treatment. What medication do you plan to start for treatment?
*Note that scenarios J & L show worse adverse events with the blue pill.
26Appendix Figure 3: Tier 3 and 4 information on the machine learning data
A
B
Tier 4C Tier 4NC
Additional information about the ML models was displayed in the Qualtrics survey. Appendix
Figure 3a: All patient scenarios included tier 3 data, which described how the ML model was
trained and validated, and specifically whether the patient was well represented in the training
data. Appendix Figure 3b: Three scenarios also included tier 4 data, which described a replication
experiment in which the ML model was tested on an observational cohort that matched the RCT
cohort.
27Appendix File 4: Study Questions
Welcome to the Multiple Myeloma Decision Support Tool (MM-DST) user study!
This form allows you to enter your medication recommendations for each patient. For each
scenario, you will enter your choice of medication as well as your confidence after each “tier” of
information is made available to you.
Before beginning, please watch the following five minute tutorial video:
https://youtu.be/xguWpxUQmis
Your participation is voluntary and your answers are anonymous. This study meets the criteria for
IRB exemption as determined by MIT (E-4559, Decision Support Tool for Treatment Selection in
Multiple Myeloma). If you have questions about your rights participating in research or would like
to speak with someone independent from the research team, please contact MIT COUHES at
couhes@mit.edu. For further questions about the study, please contact the PI at
dsontag@csail.mit.edu.
To begin, we will ask a few demographic questions for post-study analysis. Your answers will
be anonymous.
What is your age range?
• 21-30 years old
• 31-40 years old
• 41-50 years old
• 51-60 years old
• 61 years or older
What gender do you identify with?
• Male
• Female
• Other
What is your race/ethnicity? (Check all that apply)
• American Indian or Alaska Native
• Asian
• Black or African American
• Native Hawaiian or Pacific Islander
• White
• Hispanic
• Other
What is your level of training?
• Resident
• Fellow
28• Attending (out of training <10 years)
• Attending (out of training ≥10 years)
What is your specialty? (Select all that apply)
• General Medicine (Hospitalist or PCP)
• Hematology
• Oncology
• Other
For the following questions, 1 is “not very comfortable” and 5 is “very comfortable”
How comfortable are you with managing the care of a multiple myeloma patient?
1 2 3 4 5
How comfortable are you interpreting adverse event information found in Randomized Controlled
Trials (RCTs)?
1 2 3 4 5
How comfortable are you with machine learning?
1 2 3 4 5
How comfortable are you with causal inference?
1 2 3 4 5
End of study questions
Did you understand what the survival curves show? (1 is “no, not really” and 10 is “yes,
completely”)
1 2 3 4 5 6 7 8 9 10
To what extent did the adverse event bar plots inform your treatment selection? (1 is “not at all”
and 10 is “very much so”)
1 2 3 4 5 6 7 8 9 10
What adverse events, if any, were important for making a treatment decision?
Rate the following statements from 1 (no effect on decision-making) to 10 (high effect on decision-
making). As a reminder of each of these components, a screenshot is included below.
To what extent did the "Data" portion of the machine learning model details affect your treatment
decision-making and/or your confidences?
1 2 3 4 5 6 7 8 9 10
29To what extent did the "Modeling and Validation" portion of the machine learning model details
affect your treatment decision-making and/or your confidences?
1 2 3 4 5 6 7 8 9 10
To what extent did the "Replication of RCT" portion of the machine learning model details affect
your treatment decision-making and/or your confidences?
1 2 3 4 5 6 7 8 9 10
In your own words, what was the replication procedure doing, at a high level?
What was your approach for treatment selection when the study results were not applicable to your
patient (e.g. you got a statistical warning akin to the screenshot below)?
Overall, how do you think machine learning-driven, personalized-level estimates of your patient's
outcomes impact clinical decision making?
30Appendix File 5: Interview protocol for exit interview and single scenario analysis
Introduce yourself, state your affiliation and role on the research team.
Thank you for agreeing to take part in this interview. Your participation is voluntary and you may
stop the interview at any time. I expect this discussion to last about 15 minutes. We are using this
time to get more information about a specific scenario in the study you completed. There are no
right or wrong answers. We want to understand how participants are thinking through this scenario.
As you work through the scenario, please “think aloud” so that I can understand your thought
process. We’ll use the term “RCT” to refer to randomized controlled trials and the term “ML” to
refer to machine learning.
Question prompts:
For Tier 1 (RCT data)
1. How do you interpret the data from this RCT?
If participant mentions that their patient does not meet inclusion criteria,
1a. How do you think about data from RCTs when your patient does not meet
inclusion criteria?
2. What factors are you weighing when choosing a treatment option?
3. How are you weighing the side effects?
4. Why are you choosing that confidence level?
For Tiers 2 and 3 (ML data)
1. How do you interpret the data from this ML model?
If participant mentions that their patient does not meet inclusion criteria,
2a. How do you think about data from ML models when your patient does not meet
inclusion criteria?
2. What factors are you weighing when choosing a treatment option?
3. How are you weighing the side effects?
4. Why are you choosing that confidence level?
5. Why are you choosing that level of perceived reliability?
Finally:
1. How do you compare the RCT results to the ML results?
312. We found that the majority of participants choose to switch to the blue pill after seeing the
ML data and context (show them Scenario K results document). Why do you think that
is?
Helpful probes:
• Can you talk more about that?
• Help me understand what you mean.
• Can you give an example?
32Appendix Figure 6: Experimental results for all scenarios
Full results for all scenarios are shown below. The statistical tests in the “red pill” row are
McNemar’s tests done to assess for significant change in proportion of treatment blue selections.
All other tests to detect statistically significant changes in confidence and perceived reliability are
two-sample paired t-tests. P-values are shown adjusted for multiple hypothesis testing via the
Holm-Bonferonni correction. Results of the Shapiro-Wilks tests (done before conducting the 2-
sample t-tests) for normality were all non-significant after adjusting for multiple hypothesis testing
via the Bonferroni correction.
33Table 2: Results for scenarios A & B
34Table 3: Results for scenarios C & D
35Table 4: Results for scenarios E & F
36Table 5: Results for scenarios G & H
37Table 6: Results for scenarios I & J
38Table 7: Results for scenarios K & L
39Appendix File 7: Exit interview and “think aloud” session of scenario K results
Participant with their treatment selection with (confidence in selection / perceived reliability of
ML model) on a Likert scale of 1-10 for each tier of information provided. Representative quotes
are included.
Participant Tier 1 RCT data only Tier 2 ML data added Tier 3 ML context added
Fellow Red (4/NA) Blue (5/7) Red (4/8)
“From a high-risk “I have some trust in the “If the ML model showed
standpoint we want to be [ML] data” blue was superior to red I’d
aggressive with treatment” have been even more
conflicted”
Resident Red (6/NA) Blue (7/6) Red (5/3)
“When looking at RCTs “It would be an equivalent “If the RCT had excluded
that don’t include your PFS with potentially lower CKD patients, I would have
patient, it has to be a risk adverse events so I feel gone with what the ML
benefit discussion” more confident” model recommended
because it’s an
individualized
recommendation”
Resident Red (6/NA) Blue (3/5) Red (5/3)
“I was biased towards the “Theoretically the model is “People pay attention to,
more aggressive treatment, factoring in all these hey, this ML model
the red treatment to start” patient-level things” performed really well…
people don’t really think
about ML or the specific
issues with ML. People in
medicine are not trained to
do that”
Fellow Blue (7/NA) Blue (9/9) Blue (8/9)
“I discounted some of these “I want to say my “Knowing the stuff that
side effects that were confidence is high because went into the model was
highest—the hematologic [the model] fits with what I disclosed is helpful”
events—because I can deal want it to say”
with those”
Resident Red (3/NA) Blue (6/5) Blue (6/7)
“Seems like a high-risk “I’m assuming the model is “Even though the training
patient in a low-risk taking something specific dataset wasn’t
situation” about the patient into representative, it still
account… so these results performed well”
might be more applicable”
Attending Red (6/NA) Blue (8/8) Red (6/4)
“ECOG 3 is likely related to “I take it at face value, “Real-life decision making
their disease, so I want to be assume it’s a perfect model, is so complex and messy”
aggressive with treatment” my confidence is high”
NA = not applicable; RCT = randomized controlled trial; ML = machine learning
40Appendix File 8: COREQ Checklist for exit interview and single scenario analysis
Topic Item Question Answer
Domain 1: Research team and reflexivity
Interviewer/facilitator 1 Which author/s conducted the ZH and BDL
interview or focus group?
Credentials 2 What were the researcher’s ZH was a PhD candidate and BDL
credentials? has a medical degree
Occupation 3 What was their occupation at the ZH was a PhD candidate and BDL
time of the study? was a hematology oncology fellow
Gender 4 Was the researcher male or ZH is male and BDL is female
female?
Experience and 5 What experience or training did BDL is trained in qualitative
training the researcher have? methods and ZH was trained by
BDL
Relationship 6 Was a relationship established BDL had worked with one of the
established prior to study commencement? participating clinicians in the past
Participant knowledge 7 What did the participants know Participants who worked with BDL
of the interviewer about the researcher? in the past may have known BDL
was interested in oncology
informatics research
Interviewer 8 What characteristics were No information about the researchers
characteristics reported about the were provided during the focus
interviewer/facilitator? groups
Domain 2: Study design
Methodological 9 What methodological orientation Braun and Clarke’s methods for
orientation and Theory was stated to underpin the study? thematic analysis
Sampling 10 How were participants selected? Participants were recruited from the
simulated interventional study
Method of approach 11 How were the participants Participants were recruited via e-
approached? mail
Sample size 12 How many participants were in 6 participants
the study?
Non-participation 13 How many people refused to 26 of 32 study participants (81.3%)
participate or dropped out? did not respond to requests to
41participate in the follow-up
interview
Setting of data 14 Where was the data collected? Participants were allowed to
collection participate remotely
Presence of non- 15 Was anyone else present besides No
participants the participants and researchers?
Description of sample 16 What are the important Key demographics are provided in
characteristics of the sample? the results section
Interview guide 17 Were questions, prompts, guides The interview guide is provided in
provided by the authors? Was it Supplementary File 5. It was not
pilot tested? pilot tested.
Repeat interviews 18 Were repeat interviews carried No
out?
Audio/visual 19 Did the research use audio or No, only field notes were recorded
recording visual recording to collect the
data?
Field notes 20 Were field notes made during Yes, both researchers took field
and/of after the interview or focus notes
group?
Duration 21 What was the duration of the All interviews lasted between 15 and
interviews or focus group? 30 minutes
Data saturation 22 Was data saturation discussed? We were unable to reach data
saturation
Transcripts returned 23 Were transcripts returned to No
participants for comment and/or
correction?
Domain 3: Analysis and findings
Number of data coders 24 How many data coders coded the Two researchers coded the data
data?
Description of the 25 Did authors provide a description All codes were reported as themes in
coding tree of the coding tree? the results section given the small
sample size
Derivation of themes 26 Were themes identified in Potential themes were identified
advance or derived from the data? from the study results and used to
create the interview guide. All final
themes were derived directly from
the data.
42Software 27 What software, if applicable, was Not applicable
used to manage the data?
Participant checking 28 Did participants provide feedback No
on the findings?
Quotations presented 29 Were participant quotations Yes and further details are available
presented to illustrate the in Supplementary File 7
themes/findings? Was each
quotation identified?
Data and findings 30 Was there consistency between Yes
consistent the data presented and the
findings?
Clarity of major 31 Were major themes clearly Yes, major themes are discussed in
themes presented in the findings? the results section of the manuscript
Clarity of minor 32 Is there a description of diverse Yes, some minor themes are
themes cases or discussion of minor discussed in the results section of the
themes? manuscript
Developed from: Tong A, Sainsbury P, Craig J. Consolidated criteria for reporting qualitative research
(COREQ): a 32-item checklist for interviews and focus groups. International Journal for Quality in Health
Care. 2007. Volume 19, Number 6: pp. 349-357.
`
43