TOP-Nav: Legged Navigation Integrating Terrain,
Obstacle and Proprioception Estimation
Junli Ren∗, Yikai Liu∗, Yingru Dai, Guijin Wang
Abstract—Legged navigation is typically examined within
open-world, off-road, and challenging environments. In these
scenarios, estimating external disturbances requires a complex
synthesis of multi-modal information. This underlines a major
limitation in existing works that primarily focus on avoiding
obstacles. In this work, we propose TOP-Nav, a novel legged
navigation framework that integrates a comprehensive path
planner with Terrain awareness, Obstacle avoidance and close-
loopProprioception.TOP-Navunderscoresthesynergiesbetween
vision and proprioception in both path and motion planning.
Within the path planner, we present and integrate a terrain
estimator that enables the robot to select waypoints on terrains
withhigher traversabilitywhile effectivelyavoiding obstacles.In
the motion planning level, we not only implement a locomotion
controllertotrackthenavigationcommands,butalsoconstructa
proprioceptionadvisortoprovidemotionevaluationsforthepath
planner.Basedontheclose-loopmotionfeedback,wemakeonline
correctionsforthevision-basedterrainandobstacleestimations.
Consequently,TOP-Navachievesopen-worldnavigationthatthe
robotcanhandleterrainsordisturbancesbeyondthedistribution
ofpriorknowledgeandovercomesconstraintsimposedbyvisual
conditions. Building upon extensive experiments conducted in
both simulation and real-world environments, TOP-Nav demon-
stratessuperiorperformanceinopen-worldnavigationcompared
to existing methods.
I. INTRODUCTION
Fig.1. ExampledeploymentscenariosforTOP-Navinbothsimulationand
Imagineanoutdoorhikingscenariowheregaitstabilityand
the real world. Besides obstacle avoidance, the robot plans an optimized
efficient navigation are both critical. The target environment direction on terrains with better traversability. For novel terrains, the robot
is often marked by intricate obstacles and hazardous terrains incorporatesproprioceptionhistoryfrompreviouslytraversedterraintoinfer
thetraversability.
that cannot be fully observed through onboard vision. This
challenge underscores the need for a comprehensive path
planner integrating multi-modal observations. To achieve this, Aneffectivesolutiontoovercometheselimitationsisequip-
we humans employ experienced guidance and alternative per- ping the robot with terrain awareness. A traversable path can
ception modalities such as trekking poles and GPS devices. beplannedbasedontherobot’spreferencesonterrains[9,15]
This example reveals a requirement of numerous trials and a with an appropriate distribution of contact heights and forces
substantial foundation of prior knowledge to accomplish the [8].Unlikeobstacleestimation,thedistinctterrainfeaturesare
challenge navigation task. typically encoded as semantic information, where traditional
Although recent advancements in legged locomotion have methods collect sufficient data and train segmentation or
allowed the robots to navigate various terrains based on a classificationmodels[16]tolearnthesefeatures.Nevertheless,
simulation-learned strong controller [5, 20, 21, 40, 42], the compiling an exhaustive catalogue of all conceivable terrains
complexity of currently insimulable real-world factors makes alongwiththeircorrespondingwalkingpreferencesisimprac-
it impossible for the robot to traverse all potential terrains tical [11]. Compounding the issue, the dynamic real-world
encountered in reality. As a result, integrating the locomotion conditions, such as lighting, humidity, and temperature, may
controller with only an open-loop path planner often restricts introduce inaccuracies in the correspondence between images
legged navigation to limited scenarios [4, 18, 23, 34]. and walking preferences, especially when relying exclusively
on vision in this context [39].
∗ EqualContribution. The mentioned challenges can be attributed to the reliance
CorrespondingAuthor:GuijinWang(wangguijin@tsinghua.edu.cn)
on vision and the ignorance of motion states in path plan-
All authors are with Department of Electronic Engineering, Tsinghua
University,Beijing100084,China. ning. To address this, we complement the vision-only terrain
4202
rpA
32
]OR.sc[
1v65251.4042:viXraestimator with online corrections from motion evaluations. arobustperceptionmodule[19],makingitdifficultandcostly
We construct a proprioception advisor to not only convey to transfer them to different hardwares.
information about the traversability cost of novel terrains Toprovideacomprehensivetaskobservationandreducethe
but also alert the robot to unexpected disturbances, such as reliance on vision systems, recent researches have introduced
invisible obstacles. proprioception to improve task planning. A majority of these
By integrating the Terrain estimator, Obstacle estimator, works learn proprioception representations along with visual
and Proprioception advisor, we formulate TOP-Nav: a hi- features in simulation, and then implement the cross-modal
erarchical path and motion planning framework designed to features through end-to-end [38], hybrid [17] or decoupled
navigate a quadruped robot through diverse and challenging methods [41]. Despite the effectiveness demonstrated in these
terrainsproficiently.TOP-Navmaintainsfourreal-timerobot- works, the high-dimensional representation space presents
centric costmaps corresponding to: 1) goal approaching; 2) challenges for adaptation to novel scenarios and sim-to-real
terraintraversibility;3)obstacleoccupancy;and4)propriocep- transfer. Alternatively, Fu et al. [14] introduced a hierar-
tion advice. The costmaps are synthesized with dynamically chical navigation framework that derives evaluation scores
weighted factors to ensure a balanced consideration of safety from motion states, yet it overlooks the integration of visual
and efficiency. We evaluate TOP-Nav both in simulation observations.
and on a physical robot, with a comparative analysis against To mitigate these limitations, we propose a novel approach
existingapproachesthataddresssubsetsofthefactors(terrain, within TOP-Nav by maintaining a series of lightweight cost
obstacle, proprioception). In simulation, we construct an en- maps derived from multi-modal observations. This integration
vironment featuring diverse terrains, including slopes, steps, achieves a dynamic balance between vision and propriocep-
and random textures, along with various obstacles. For real- tion. Furthermore, we leverage the learning-based locomotion
worldexperiments,wedeployourapproachindiverseoff-road controllertoderivemotionevaluationsfromthevaluefunction,
navigationscenariosencompassingcommonfieldterrains.Our offering an efficient solution without additional training.
main contributions are summarized below:
B. Terrain Traversability Estimation
• A comprehensive legged navigation framework Integrat-
Terraintraversabilityisdeterminedbyfactorssuchasterrain
ing multi-modal observations throughout a task and mo-
geometry, texture, and physical properties [12]. Estimating
tion planner;
these features could be achieved by identifying the semantic
• Aterrainestimatortrainedfrompreviouslycollecteddata
classwithapredefinedstatictraversabilityscore[9,16,31,35].
toinformtherobotofavision-basedterraintraversability;
These solutions exhibit a notable dependency on large-scale
• Compensating proprioception to offer online corrections
datasets [29] or limited to structured environments like urban
forthevision-basedestimationofbothterraintraversabil-
scenarios [1, 6].
ity and obstacles;
In off-road navigation, the motion states involved in the
• Successfulimplementationandquantitativevalidationsof
dynamic interactions between the robot and the environment
theproposedframeworkinbothsimulationandhardware.
provide valuable metrics for assessing terrain traversability
II. RELATEDWORK [10]. These insights have inspired methods that eliminate the
need for manual annotation by autonomously deriving ter-
The pivotal feature of TOP-Nav lies in its integration of
raintraversabilityfromproprioceptionthroughself-supervised
the terrain estimator and proprioception advisor. To elucidate
learning [3, 7, 22, 24, 28, 39]. Nevertheless, the performance
this aspect, we present an overview of the relevant literature.
ofthesestudiesiscontingentuponthequalityofthecollected
datasets [12]. To emphasize the challenges in unconstrained
A. Vision and Legged Proprioception Integration
navigation, researchers have proposed various approaches to
Recentadvancementsinleggedlocomotionhaveshowcased handling novel observations. For instance, Frey et al. [11]
a synergistic mechanism for processing vision and propri- updated the traversability estimation network online with
oception within the context of “How to Walk” [2]. While anomalies into consideration. Karnan et al. [25] performs
heightmaps serve as crucial observations for a controller to nearest-neighbor search in the proprioception space to align
generate dynamic motions across various terrains [26, 27], visually novel terrains with existing traversability.
proprioception observations can be employed to reconstruct Drawing inspiration from those works estimating
the heightmaps in visually degraded environments [30]. Dif- traversability for novel terrains, we propose a prior-
ferent from the mentioned efforts focus on the depth channel, knowledge informed terrain estimator that employs the
we propose a data-efficient solution to extract semantic infor- proprioception advisor as online corrections. Our method
mation from the proprioceptive feedback. diverges from previous approaches primarily in two key
Theinsightsofintegratingvisionandleggedproprioception aspects: 1) We employ an estimated value function from
are further explored in guiding the robot in “Where to Walk”. reinforcement learning to assess terrain traversability,
From the perspective of vision adied navigation, researchers providing a comprehensive evaluation of robot-terrain
have explored both hierarchical [4, 23] and end-to-end [32] interactions. 2) By incorporating the proprioception based
pipelines.Thesemethodsrequiresubstantialeffortstodevelop terrain traversability estimation as online corrections tovision-based predictions, we achieve a more data-efficient allowing TOP-Nav to handle open-world task without being
approach for estimating traversability on unknown terrains constrained by visual conditions or prior knowledge.
compared to online training. To elucidate the task configuration, the robot is given a
point goal with its location p and has to approach the goal
III. BACKGROUND goal
within a limited time. The location of the robot is denoted as
The proposed proprioception advisor leverages recent (p ,r ).Forexternalobservation,weutilizeabio-channel
base base
progress in legged locomotion to provide online motion eval- perception module for both the task and motion planner. This
uationsforthepathplanner.Thissectionprovidesanessential moduleincludesdepth(I )andRGB(I )channels.Thepath
d rgb
background of this. planner computes the desired velocity command (v ,∆ )
lin yaw
Learning-basedleggedlocomotioncontrollershasbeenwell for the robot, which is tracked by the motion planner along
developed through reinforcement learning, which is generally with the appropraite joint position signal.
achievedbyupdatingthepolicyπwithintheasymmetricactor-
critic training: B. Integrated Path Planner
(cid:40) a =π (op,oi,oe,oh) A key feature within the path planner of TOP-Nav is the
π t actor t t t t , (1) integrated cost map M which offers a comprehensive esti-
1 c =π (op,oi,oe,oh) C
t critic t t t t mation that encompasses terrain traversability M , obstacles
T
thelocomotionpolicyreceivesprivilegedobservationop,pro- M O, proprioception advice M
P
and the goal approaching
t
prioception observation oi, scanned dots external observation map M G. This integration addresses both visible and unex-
t
oeandhistoricalobservationohrespectively.π ismodelled pected external disturbances against the robot.
t t actor
as a Gaussian policy and infers the optimized actions a to Tokeepabalancebetweennavigationefficiencyandsafety,
t
compute the joint positions q des. c t stands for the estimated we formulate the combination of M C with dynamic scaling
value function from π , which is updated through: factors as follows:
critic
M =M +M +α M +α M , (4)
Lcritic =(c −ctarg)2, (2) C P O T T G G
t t t k k
α = T1 , α = G1 +k , (5)
where
T
T 1+e−kT2(d−d0) G 1+e−kG2(t−t0) G0
(cid:88)
ct targ = γi−tR(s i), (3) here M
i
are 2-d matrix with a spatial resolution of 0.15m,
i=t d 0,t 0,k T,k G are hyperparameters, t denotes current time
s denotes the robot state, R(s ) represents the rewards consuming and d denotes the distance from the point goal
i i
accrued at timestep i, which commonly includes guiding the at the current step. We accord the highest priority to M P
robot to track a given velocity commands with stable gaits and M O since they represent non-traversable locations where
and attitude. A substantial efforts in reward engineering has the robot cannot pass through. The terrain traversability scale
enabled the value function ct targ to evaluate a comprehensive α T decreases as the robot approaches the target. This design
set of interactions between the robot and its environment. is made considering that, as the robot nears the target (d de-
Therefore, when deploying the learning-based controller, c creases),takingadetourtoavoidachallengingyettraversable
t
can be utilized to provide closed-loop feedback for the task terrainwouldbeaninefficientbehaviour.Conversely,thegoal
planner. approachingscaleα G increaseswiththedurationtofthetask.
Thecompletetrainingparadigmwillinvolveasecondstage, With the integrated map M C, we select the optimal way-
trainingadepthencoderandastudentnetworktoreproducing point p way based on the lowest combined cost.
op and oe from real world accessible observations I (depth
t t d (x,y)=p
image), oi and oh. The motion controller will track the 1 (cid:88)
t t p =argmin M (x,y), (6)
planned ∆ and v and the control action is represented way ∥p −p∥ C
yaw lin p∈E base
by the 12 desired joint position q .
(x,y)=pbase
des
Here E denotes the set of points on the edge of M . For
IV. METHOD C
each point p in E, we calculate the average combined cost
A. System Overview along the path from the robot location p to the edge point
base
As illustrated in Fig. 2, the proposed TOP-Nav pipeline p.Theoptimalwaypointp isthenchosenasthepointwith
way
connects a path and motion planner to tackle the task of the lowest path cost.
legged navigation. To efficiently generate a collision-free path Afterdeterminingtheoptimizedlocaltargetp ,thetarget
way
and stable motion states, we highlight a terrain estimator direction is calculated based on the relative position ∆ =
yaw
and a proprioception advisor inside the pipeline. In addition arctan(p −p )−r ,herer denotestheyawdirection
way base base base
to combining these modules on the costmap (Fig. 2), the oftherobotbase.Thelinearvelocityisconstrainedtomitigate
proprioception advisor provides online corrections for the the impact of high angular variation with v
lin
= v 0e−k∆yaw,
terrain estimation. Such integration informs the robot of the thereby preventing abrupt and substantial turning at high
traversability of unknown terrains and invisible obstacles, speeds.Fig. 2. TOP-Nav: the hierarchical path and motion planning framework to tackle the problem of legged navigation with a point goal. The path planner
synthesizesrelativedistance,terraintraversability,obstacleoccupancy,andmotionevaluationsintoacombinedcostmap,fromwhichitcomputeswaypoints
based on the overall cost considerations. The motion evaluations are extracted from a learning-based motion planner, which receives depth observation as
input.
Obstacle Estimation and Localization: In simulation, the
robot has access to the ground truth location. For each point
p = (x,y), M (x,y) = ∥p −p∥. In real world, we set
G goal
a target direction r to compute the goal map M (x,y)=
goal G
∥r −arctan(p−p )∥. We construct the obstacle estima-
goal base
tion based on the depth channel I . The perceived obstacles
d
are converted into point clouds, and for each point in M ,
O
wecomputethesigneddistanceM totheclosestobstacles
SDF
within distance d , the cost of obstacles therefore can be
max
computed as:
max(0,d −M (x,y))
M (x,y)= max SDF . (7)
O d
max
C. Proprioception Advisor
We design the proprioception advisor to identify motion
abnormalities that may arise from unexpected external dis-
turbances. A straightforward approach is to use the velocity
tracking error [11], but its effectiveness can be hindered by
the noisy velocity estimation on real hardware. Considering
the interactions between the robot and its environment, the
ideal metrics should be both observable and comprehensive.
Toaddressthis,weutilizethevaluefunctionc estimatedfrom
t
π , which synthesizes multiple metrics and provides more
critic
stable results by incorporating historical observations.
Theoriginalπ requiresprivilegedobservations,wetrain
critic
another value function estimation network using observable
motionstatesfortheproprioceptionadvisortobedeployedon
hardware. The estimated motion evaluation c is normalized
t
using a sigmoid function. As demonstarted in Fig. 3, the
Fig. 3. We demonstrate the variation of the normalized ct when the robot
traversesondifferenttypesofterrain.Thetraversalonbushrevealsunstable
motion evaluation will decline sharply when the robot en- posturesandafailuretotrackvelocitycommands,resultinginadecreasein
counters a locomotion failure, potentially due to transitioning ct.Conversely,navigatingonpavementexhibitsastablect.
ontochallengingterrain.Implementationdetailsforthereward
design and normalization are provided in the appendix.Within the path planner, we first directly utilize the motion or human demonstrations [24]. We leverage experience from
evaluations to enhance the robot’s awareness of unforeseen these efforts to establish the terrain traversability (Fig. 4) as
disturbances.Thisinvolvesestimatingcollisions(M )beyond the operator preference. As we demonstrated in Fig. 3, such
P
the obstacle estimator and integrating them into the combined prior knowledge-based terrain traversability aligns with the
cost map (M ). Previous works generate fix-sized virtual proprioception evaluations.
C
obstalces when proprioception feedback reaches a certain
threshold [14], the design overlooks the variation of motion
evaluations within each interval. We propose an continuously
varied proprioception cost along the y-axis in M .
P
1−Norm(c )
M (x,y)= t , (8)
P ekP(∥ybase−y∥)
here k is hyperparameter. Since a lower motion evaluation
P
indicatesapotentialchallengingterrainoraninvisibleobstacle
in the current direction of the robot, we allocate c to the
t
centriod column in M and decrease it towards the edges.
P
D. Vision-based Terrain Estimation
The proposed terrain estimator incorporates terrain
traversability into the path planner. Firstly, we construct
a mapping from visual observations to reference terrain
traversabilities using prior knowledge.
Giventhatterrainslikestairs,slopes,ordiscretesmallsteps
canbeeffectivelyidentifiedfromdepthobservations,towhich
ourlocomotioncontrollercanrespondappropriately,ourtask-
level terrain estimation module focuses on semantic features Fig. 4. The proposed terrain estimator incorporates a visual estimator and
online corrections. The visual estimator infers the terrain name T from the
such as color and texture. These features are highlighted by
BEVmapwithitsuncertaintyU,andthecorrespondingterraintraversability
the RGB channel, specifically pertaining to terrains such as MTO for T is interpreted based on prior knowledge. The online correc-
grass, snow, and cement. To correspond the RGB observation tions involve recording the motion evaluations MTP by the proprioception
advisor and retrieving them from the experience buffer when encountering
into real-world coordinates, we apply the perspective trans-
similarobservations.Inthefigure,(x b,y b),(xo,yo),and(xr,yr)represent
formation on I rgb to map the pixels to M BEV, which has the locations of the robot base, observation point, and historical position,
corresponding coordinates as M . In subsequent processing, respectively.
T
we discretize M into patches (Fig. 4) and assign the same
BEV
difficulty within each patch.
E. Online Terrain traversability Corrections
Toaccomplishvision-basedterrainestimation,therobotfirst
identifies the observed terrain from M . We achieve this
BEV Through the seamless integration of vision and proprio-
by training a terrain classification network π from offline
terrain ception, we provide online corrections for the vision-based
collected data D . Considering that such inference falls
terrain traversabilityestimationM withoutadditionaltraining(Fig.
short when faced with unfamiliar terrains, we calculate the TO
4). This enables the robot with terrain awareness beyond the
predictive entropy to approximate the uncertainty U of the
limitations of the collected data D .
inferenceatcurrentobservation[36].Sincetheapproximation terrain
requires Monte Carlo simulation, we perform K stochastic We address the mechanism by which the robot recalls the
forward inferences with different data augmentations at each traversability of a terrain once it has been seen and tra-
timestep, with Pk denoting the predicted probability for label versed within the navigation process. With the robot location
i
i at trail k. We estimate the expectation as the average of (x ,y ),werecordadurationof1sproprioceptionadvice
base base
the predicted probability P =(cid:80) Pk/K. Then the predicted to indicate the terrain traversability at the robot location:
i k i
explicit terrain names T and corresponding uncertainty U for
the current patch can be calculated as: T (x ,y )=1−Norm(c ). (10)
P base base t−k:t
(cid:88)
U =− P logP and T =argmax(P ). (9)
i i i
i
i Besides T P(x base,y base), we can access a latent feature
L(x ,y )extractedbytheclassifierπ whenthesame
base base terrain
For safe navigation, terrain traversability signifies the patch was observed a few steps ago, we record both T and
P
robot’spreferenceorstabilityintraversingaparticularterrain, L at location (x ,y ) into an experienced list P . Now
base base e
which has been explored from either self-supervision [11] given a new observation at location (x ,y ), We can find a
o otraversedandseenpatch(x ,y )thatlooksclosestto(x ,y ) prioritizes a light-weight, real-time updated path planner and
r r o o
based on cosine similarity: maintainsconsistencywiththeheightmapusedinlocomotion.
The environments include randomly generated obstacles and
S =argmaxcos(L(x ,y ),L(x ,y )), (11)
(o,r) r r o o terrains. Each cell is equipped with at least one wall obstacle,
r∈Pe
two column obstacles and the remaining space is divided into
Note that we also have the historical traversability 1m×1m sections. For terrain traversability assignments, we
T (x ,y ) infered by the proprioception advisor at (x ,y ), uniformlypartitiontheobstacle-freespaceintodifficultylevels
P r r r r
we can compute a proprioception adapted traversability with [0,0.25,0.5,0.75,1]andgenerateterrainwithvariousheights
the similarity S to access the historical proprioception of steps and the intensity of irregular terrain corresponding
(o,r)
correction M (x ,y ): to the difficulty levels. This difficulty serves as ground-truth
TP o o
walkingpreferencesinsimulation.Thesimulationexperiments
T (x ,y )
M (x ,y )= P r r , (12) are conducted 25 times in each of the 64 navigation cells.
TP o o 1+e−kT3(S(o,r)−S0)
Real-World Settings: For real-world evaluations, we conduct
here k and S are hyperparameters.
T3 0 outdoor navigation tasks in 5 different scenarios involving
Due to the delay in estimation from the proprioception challenge obstacles and various terrains, with each scenario
history, we intend for M TP to be utilized when encountering replicated 5 times for each method under investigation. To
novel terrains where M TO becomes unreliable. Therefore, we obtain a more comprehensive observation in the real world,
normalize the visual uncertainty U(x o,y o) into confidence weconfigurethecostmapM
C
withdimensionsof(3m,3m).
C(x o,y o) to adjust the contribution of M TO and M TP: The location of robot is at pm baa sp e =(0m,1.5m) in real world
M .Thecriteriaforsuccessincludereachingthegoalwithin
M −M C
M T(x o,y o)=( 1+eT −O kT4(C−T CP
0)
+M TP)(x o,y o), (13) the specified time constraints, consistent with the simulation
setting.
here k and C are hyperparameters.
T4 0
Metrics: We evaluate TOP-Nav with the following metrics:
By integrating the adapted M as the terrain traversability
T
into the path planner, our system demonstrates rapid adapt-
ability to different terrain and visual conditions. • SR (Success Rate): The percentage of successful exper-
iments. We define a success experiment as approaching
the point goal into 0.5m within 20 seconds
V. EVALUATIONS
• TD (Terrain Difficulty): The percentage of average tra-
A. Experimental Setup versed terrain costs, TD provides the ground truth tra-
versed terrain costs within each episode.
We assess TOP-Nav in both simulation and the real world. • UT (Unstable Time): The percentage of unstable motion
Considering the sim2real gap primarily lies in the RGB states (|roll|>0.15 or |pitch|>0.15) in each episode.
semantic information, we assume the robot has access to • VFT (VelocityTrackingFailure):Theaveragepercentage
ground-truth terrain traversability in simulation. Therefore, of velocity tracking failures (∥v −v ∥>0.2) in each
lin act
our simulation experiments were designed to emphasize the episode.
following evaluations: • AEC(AverageEnergyConsumption):Theaverageenergy
consumption (τq˙) [13] in each successful episode.
• The improvements of Terrain awareness in navigating
challenge environments.
Among them, UT, VFT and AEC demonstrate walking states
• The improvements of the proposed Proprioception advi-
whilecompletingnavigationtasksinchallengingterrains.,TD
sor in navigating the challenge environments.
provides a reference terrain traversable cost, measuring the
• The improvements in locomotion by selecting waypoints
improvements in terrain-aware path planning for gait stability.
with higher traversability.
Wecalculatethevarianceacrossdifferentnavigationscenarios
Relatively, in addition to the evaluations mentioned above, to assess the robustness of the proposed method.
our real-world experiments specifically focus on assessing the
Baselines: Beyond ablation study, we compare TOP-Nav
enhancements provided by the proposed terrain estimator.
againststate-of-the-artleggednavigationframeworksandseg-
Simulation Settings: Our simulation experiments are con- mentation based terrain estimators. VP-Nav [14] integrates
ducted within Nvidia Isaac Gym. We create a grid of 8×8 vision and proprioception to develop a collision detector and
independent navigation cells. Each cell is 5m×5m in size, a fall predictor within the navigation pipeline. GA-Nav [16]
featuring a robot assigned to a point goal navigation task. achievesterrainsegmentationrelyingsolelyonvision.Sterling
The robot and point goal is randomly generated, with a [24] learns terrain traversability in a self-supervised manner
minimum initial distance of 5m within the cell. M is byassigningtraversabilitytoterrainsbasedonthosethatshare
C
configured with dimensions (1.2m,1.5m) centered around similarproprioceptionrepresentations.However,thisapproach
the robot‘s location at pmap =(0.45m,0.75m). This design requires prior training for encountered terrains.
baseFig.5. Weestablishaparallelnavigationevaluationenvironmentinsimulation,featuring8×8independentnavigationcells.Eachcellconsistsofrandomly
generatedchallengingterrainswithdistincttraversedifficulty,whichismarkedbytheirregularityandcomplexityoftheterrain.Theproposedterrainawareness
navigationframeworkplansanoptimalpathtonavigatechallengingterrains.Therobotdemonstratesthecapabilitytorecoverfromunexpectedobstaclesor
irregularterrainswiththeproprioceptionadvisor.
B. Simulation Evaluations and wo/Proprioception, as well as the comparison between
wo/Terrain and Obstacle-Only. Such ablations also indicate
Improvements with Terrain Awareness: We illustrate a thattheterraintraversabilityintegrationandtheproprioception
comprehensive terrain and obstacle map within one nav- advisor each contribute to performance improvement. Mean-
cell in Fig. 5, exemplifying that the robot equipped with while, even without terrain awareness, the proposed system
terrain awareness can plan a path with higher traversability. outperforms VP-Nav by 2.5%, signifying an improvement
As demonstrated in Table I, when integrated with terrain in our integration of the proprioception advisor compared to
awareness, TOP-Nav surpasses the VP-Nav baseline by ap- existing methods.
proximately 8% in success rate (SR). We observe that TOP-
Advancements in Locomotion: The metrics of UT and VFT
Nav and the wo/Proprioception methods achieve a TD of
evaluate the locomotion states. We observe that TOP-Nav
nearly 20%, which is half of the TD achieved in methods
achieves the lowest UT and VFT, indicating that selecting
without terrain awareness. These results indicate that the
simpler terrains contributes to locomotion stability. In the
proposed path planner empowers the robot to select terrains
absence of the proprioception advisor, wo/Proprioception and
withhighertraversability,leadingtoasignificantimprovement
Obstacle-Only exhibit a significant increase in VFT due to
in the success rate of navigation.
the robot getting stuck by unexpected obstacles. We evaluate
ImprovementswithProprioceptionAdvisor:Thesimulation energy consumption with the assumption that when traversing
environments are cluttered with various obstacles and difficult simplerterrain,therobotshouldexhibitmorenaturalgaits.As
terrains, leading to potential locomotion failures such as get- a result, TOP-Nav and wo/Proprioception exhibit a 10% re-
tingstuckonirregularterrainsurfacesorcollidingwithunseen duction in energy consumption compared to methods without
obstacles during directional changes (Fig. 5). We demonstrate terrain awareness.
that the proposed proprioception advisor could address those
challenges: In contrast to methods without proprioception,
our approach exhibits an approximate 11% enhancement in
SR. This is evident in the comparison between TOP-NavTABLEI
SIMULATIONRESULTSWITHCOMPARISONEXPERIMENTSANDABLATIONSTUDY
Method SR (%) ↑ TD(%) ↓ UT(%) ↓ VFT(%) ↓ AEC ↓
VP-Nav 65.62±2.38 47.18±0.79 36.18±0.54 26.09±0.42 101.80±49.61
wo/Terrain 68.00±2.57 45.83±0.70 34.92±0.59 25.06±0.46 101.70±46.55
wo/Proprioception 62.75±1.64 20.47±0.17 32.87±0.48 26.66±0.57 88.17±15.31
Obstacle-Only 52.81±2.30 47.79±0.82 41.45±0.79 39.13±0.99 96.82±48.03
TOP-Nav 73.62±1.21 22.65±0.19 27.21±0.24 18.14±0.16 92.08±24.39
C. Real World Evaluations TABLEII
EVALUATIONRESULTSONSCENE1,2,3.
AsshowninFig.6,TOP-Navisevaluatedindifferentenvi- Method SR ↑ UT(%) ↓ AEC ↓ Time(%) ↓
ronments featured with diverse terrains and obstacles. Among GA-Nav 13/15 19.80 74.42 68.76
them, the terrains encountered in scenes 1-3 were provided in VP-Nav 11/15 23.02 71.45 80.12
the terrain classifier, we assess the improvements introduced Obstacle-Only 12/15 22.75 71.09 79.72
by the vision-based terrain estimator in these experiments. TOP-Nav 15/15 7.79 59.07 65.03
scenes4-5aredesignedtoassesstheeffectivenessoftheonline
correctionmodulewhenencounteringnovelterrains.Inscene
4, the terrain classifier does not include high-cost gravel. In
TABLEIII
scene5,therobotencountersterrainswithnopriorknowledge, EVALUATIONRESULTSONSCENE4(NOVELTERRAINS).
includingaslipperydetergentsurface.Theevaluationsinscene
6 demonstrate that with the proposed proprioception advisor, Method SR ↑ UT(%) ↓ AEC ↓ Time(%) ↓
the robot is able to avoid invisible obstacles. GA-Nav 5/5 7.98 62.57 67.42
VP-Nav 4/5 26.43 70.30 77.90
Due to significant velocity estimation errors in the real
Obstacle-Only 5/5 42.33 72.60 70.64
robot, we exclude VFT in the real-world experiments. The
TOP-Nav 5/5 14.30 57.46 62.72
Time metric measures the proportion of time the robot takes
to complete the navigation task. ST in scenario 5 assesses the
timetakenbytherobotfromencounteringatransparentobject
to completely avoiding the obstacle.
Improvements with the Online Correction: To evaluate the
Improvements with the Visual Estimator: The evaluation efficacy of the online corrections for providing the robot with
results for scenes 1-3 are presented in Table II. TOP-Nav traversability on novel terrains, we conduct experiments in
successfully completes all 15 trials with superior walking scene 4 using a terrain classifier trained without gravel. The
stabilityandminimalenergyconsumption.Theseresultsaffirm degraded classifier exhibits an accuracy of 96.30% on the
the successful deployment of the proposed system on real paved road with an average confidence of 0.87, whereas the
hardware, allowing the robot to select waypoints on terrains inference on gravel has an average confidence of 0.43. We
with better traversability and thus execute more natural gaits. provide a navigation example in Fig. 6 and divide it into four
Incontrast,GA-Nav,trainedonRUGD[37],demonstratesno- phases. In phase 1, despite M indicating that the robot is
P
tablelimitationswhenconfrontedwithopen-worldnavigation. traversing a challenging terrain, the goal map M guides the
G
WeobservethattheimprovementsofTOP-NavoverGA-Nav robot to continue moving forward until this information is
mainlystemfrom:1)Inscene1,2,GA-Navfailstoidentifythe incorporated into M . In phase 2, an online correction is
TP
slabbed road as a low-cost terrain from the bush and gravel. applied as the robot encounters similar gravel terrain, leading
2) In scene 3, GA-Nav incorrectly identifies the lower part to an increased cost brought by M in the current direction.
TP
of the obstacle as a cement floor, which could be addressed Consequently,therobotturnsrighttotaponthepavedroad.In
by our integration of obstacles and terrain estimation. Never- phase3,therobotavoidsanobservedobstacle.Inphase4,the
theless, GA-Nav surpasses VP-Nav and Obstacle-Only, which historical proprioception continues to indicate the presence of
navigates without terrain awareness. With Obstacle-Only, the high-costgravelterrainontheleft.Thequantitativeresultsare
robot gets stuck by the stones or Grassroots, leading to a provided in Table III. Despite GA-Nav possessing complete
higher time consumption. While VP-Nav effectively alerts the prior knowledge and correctly identifying the paved road in
robot to locomotion failures that could be brought by such scene 4, our online corrections only lead to a 7% increase in
terrains, the absence of terrain awareness prevents the robot UT, mainly due to the forward movement in phase 1. VP-
fromsuccessfullynavigatingoutofthesechallengingterrains, Nav allows the robot to exit the gravel terrain with the safety
resulting in inferior locomotion performance. advisor, but it can not retain this information. As a result, theFig.6. Inscene1-3,thegreenlinerepresentsthetrajectoryofTOP-Nav,whiletheredlinerepresentsObstalce-Only.Ourdemonstrationsforscene4,5
aredividedintophasesthatweprovidedetailsofthecostmapintegrationinthefigure.
TABLEIV of the robot, and the target direction is set in this facing
EVALUATIONRESULTSONSCENE5(NOVELSLIPPYTERRAINS). direction. To mitigate the disturbances brought by reflections
from detergents, the obstacle cost M is not considered in
O
Method SR ↑ UT(%) ↓ AEC ↓ ST(s) ↓
this experiment. As demonstrated in Fig. 6, the experiment
VP-Nav 3/5 2.64 102.04 63.05
could be divided into three phases. In phase 1, the robot
Obstacle-Only 2/5 3.88 119.77 66.14 moving forward along the goal direction, but the slippery
Sterling 2/5 3.85 113.75 76.52 terrain causes a decline in motion evaluations, resulting in
TOP-Nav 5/5 1.19 83.40 67.62 a higher M . Simultaneously, the traversability cost of the
P
slippery tarpaulin is recorded along with the visual features.
In phase 2, the robot begins to turn aside to avoid the high-
costcentralarea.Inphase3,thecostof M guidestherobot
G
target orientation prompts the robot to return to the gravel,
toreturntotheoriginaldirection,whereitcannavigateonthe
causing increased time consumption and instability.
wooden floor with better traversability to complete the task.
Evaluations on Challenge Novel Terrains: In Scene 5, The results presented in Table IV validate the effectiveness of
we present a more challenging scenario where the robot ourmethodinmaintainingstabilityandimprovingthesuccess
encounters terrains without any prior information. A black rate by avoiding challenging terrain. Among the comparisons,
tarpaulin covered with slippery detergent is laid out in front Obstacle-OnlykeepsmovingforwardontheslippysurfaceandTABLEV on these vision-only estimations. To validate the proposed
EVALUATIONRESULTSONSCENE6(INVISIBLEOBSTACLES). system, we conducted extensive experiments in simulated
environmentsfeaturingdiverseterrainsandobstacles.Further-
Method SR ↑ UT(%) ↓ AEC ↓ ST(s) ↓
more, the navigation system was successfully deployed on
VP-Nav 5/5 1.45 42.97 3.30
a real robot, and quantitative experiments were conducted.
Obstacle-Only 0/5 / / +∞ These evaluation results underscore the success of our system
TOP-Nav 5/5 0.79 34.63 1.98 in achieving open-world navigation, surpassing limitations
posed by terrain, obstacles, or prior knowledge. Compared to
existing approaches that couple vision and proprioception or
navigate with terrain awareness, our system excels not only
exhibit the worst motion performance. We include Sterling in
in achieving a higher success rate in challenge navigation,
this comparison to demonstrate the importance of online cor-
but also in demonstrating more stable gaits and lower energy
rectionswhenincorporatingproprioceptionintoterrainestima-
consumption. In the future, we aim to extend the application
tion. Without online corrections, Sterling behaves similarly to
of the proposed system to longer-distance navigation tasks
Obstacle-Only,unabletoadapttonovelterrainsandcontinuing
forward.Comparedwiththeresultinscene4,wedemonstrate through precise localization from radar. Furthermore, we plan
to enhance the task planner with data-sufficient models such
that TOP-Nav can handle novel terrains effectively without
as Transformer.
relying on operator preferences. This showcases the ability of
the system to perform open-world navigation.
REFERENCES
[1] Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars
Mescheder, Andreas Geiger, and Carsten Rother. Aug-
mentedrealitymeetscomputervision:Efficientdatagen-
eration for urban driving scenes. International Journal
of Computer Vision, 126:961–972, 2018.
[2] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and
Deepak Pathak. Legged locomotion in challenging ter-
rains using egocentric vision. In Conference on Robot
Learning, pages 403–415. PMLR, 2023.
[3] Xiaoyi Cai, Siddharth Ancha, Lakshay Sharma, Philip R
Osteen, Bernadette Bucher, Stephen Phillips, Jiuguang
Wang, Michael Everett, Nicholas Roy, and Jonathan P
How. Evora: Deep evidential traversability learn-
ing for risk-aware off-road autonomy. arXiv preprint
Fig.7. Inscene6,therobotavoidsunseenobstacleswiththeproprioception arXiv:2311.06234, 2023.
advisor.
[4] Ken Caluwaerts, Atil Iscen, J Chase Kew, Wenhao Yu,
Invisible Obstacles: The proposed proprioception advisor Tingnan Zhang, Daniel Freeman, Kuang-Huei Lee, Lisa
provides close-loop feedback to alert the robot to invisible Lee, Stefano Saliceti, Vincent Zhuang, et al. Bark-
obstacles. We assess this capability in scene 6, where a glass our: Benchmarking animal-level agility with quadruped
wallislocatedalongtheplannedpath.Inphase1,theobstacle robots. arXiv preprint arXiv:2305.14654, 2023.
estimator is oblivious to the presence of the glass wall, and [5] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak
the robot continues moving forward. In phase 2, the robot Pathak. Extreme parkour with legged robots. arXiv
collides with the glass wall, causing a rapid increase in M preprint arXiv:2309.14341, 2023.
P
alongthedirectionofmovement.Asaresult,therobotadjusts [6] Marius Cordts, Mohamed Omran, Sebastian Ramos,
its waypoint to steer clear of the high-cost area, successfully Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
avoiding the glass wall. The quantitative results are provided Uwe Franke, Stefan Roth, and Bernt Schiele. The
in Table V. cityscapes dataset for semantic urban scene understand-
ing. InProceedingsoftheIEEEconferenceoncomputer
CONCLUSION vision and pattern recognition, pages 3213–3223, 2016.
[7] Mohamed Elnoor, Adarsh Jagan Sathyamoorthy, Kasun
We present TOP-Nav, a legged navigation system that Weerakoon, and Dinesh Manocha. Pronav: Proprio-
achieves closed-loop integration of visual and proprioception ceptive traversability estimation for autonomous legged
at both task and motion planning levels. Specifically, not robotnavigationinoutdoorenvironments. arXivpreprint
only have we incorporated vision-based obstacle and terrain arXiv:2307.09754, 2023.
estimation into the navigation framework, but we have also [8] GianErni,JonasFrey,TakahiroMiki,MatiasMattamala,
utilized the proprioception advisor to make online corrections andMarcoHutter. Mem:Multi-modalelevationmappingfor robotics and learning. In 2023 IEEE/RSJ Inter- Hutter. Learning agile and dynamic motor skills for
national Conference on Intelligent Robots and Systems legged robots. Science Robotics, 4(26):eaau5872, 2019.
(IROS), pages 11011–11018. IEEE, 2023. [21] Fabian Jenelten, Junzhe He, Farbod Farshidian, and
[9] Parker Ewen, Adam Li, Yuxin Chen, Steven Hong, and Marco Hutter. Dtc: Deep tracking control. Science
RamVasudevan.Thesemapsaremadeforwalking:Real- Robotics, 9(86):eadh5401, 2024.
time terrain property estimation for mobile robots. IEEE [22] Sanghun Jung, JoonHo Lee, Xiangyun Meng, Byron
RoboticsandAutomationLetters,7(3):7083–7090,2022. Boots, and Alexander Lambert. V-strong: Visual self-
[10] David D Fan, Kyohei Otsu, Yuki Kubo, Anushri supervisedtraversabilitylearningforoff-roadnavigation.
Dixit, Joel Burdick, and Ali-Akbar Agha-Mohammadi. arXiv preprint arXiv:2312.16016, 2023.
Step: Stochastic traversability evaluation and planning [23] Simar Kareer, Naoki Yokoyama, Dhruv Batra, Sehoon
for risk-aware off-road navigation. arXiv preprint Ha, and Joanne Truong. Vinl: Visual navigation and
arXiv:2103.02828, 2021. locomotion over obstacles. In 2023 IEEE International
[11] Jonas Frey, Matias Mattamala, Nived Chebrolu, Ce- Conference on Robotics and Automation (ICRA), pages
sar Cadena, Maurice Fallon, and Marco Hutter. Fast 2018–2024. IEEE, 2023.
traversabilityestimationforwildvisualnavigation. arXiv [24] Haresh Karnan, Elvin Yang, Daniel Farkash, Garrett
preprint arXiv:2305.08510, 2023. Warnell, Joydeep Biswas, and Peter Stone. Sterling:
[12] Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Self-supervised terrain representation learning from un-
Atha, Julian Nubert, Curtis Padgett, Marco Hutter, and constrained robot experience. In Conference on Robot
Patrick Spieler. Roadrunner–learning traversability esti- Learning, pages 2393–2413. PMLR, 2023.
mation for autonomous off-road driving. arXiv preprint [25] Haresh Karnan, Elvin Yang, Garrett Warnell, Joydeep
arXiv:2402.19341, 2024. Biswas, and Peter Stone. Wait, that feels familiar:
[13] Zipeng Fu, Ashish Kumar, Jitendra Malik, and Deepak Learningtoextrapolatehumanpreferencesforpreference
Pathak. Minimizing energy consumption leads to the aligned path planning. arXiv preprint arXiv:2309.09912,
emergence of gaits in legged robots. arXiv preprint 2023.
arXiv:2111.01674, 2021. [26] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra
[14] Zipeng Fu, Ashish Kumar, Ananye Agarwal, Haozhi Qi, Malik. Rma: Rapid motor adaptation for legged robots.
Jitendra Malik, and Deepak Pathak. Coupling vision arXiv preprint arXiv:2107.04034, 2021.
and proprioception for navigation of legged robots. In [27] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen,
Proceedings of the IEEE/CVF Conference on Computer VladlenKoltun,andMarcoHutter.Learningquadrupedal
Vision and Pattern Recognition, pages 17273–17283, locomotion over challenging terrain. Science Robotics, 5
2022. (47):eabc5986, 2020.
[15] LuGan,YoungjiKim,JessyWGrizzle,JeffreyMWalls, [28] Gabriel B Margolis, Xiang Fu, Yandong Ji, and Pulkit
Ayoung Kim, Ryan M Eustice, and Maani Ghaffari. Agrawal. Learning to see physical properties with active
Multitask learning for scalable and dense multilayer sensingmotorpolicies.arXivpreprintarXiv:2311.01405,
bayesianmapinference. IEEETransactionsonRobotics, 2023.
39(1):699–717, 2022. [29] Xiangyun Meng, Nathan Hatch, Alexander Lam-
[16] Tianrui Guan, Divya Kothandaraman, Rohan Chandra, bert, Anqi Li, Nolan Wagener, Matthew Schmittle,
AdarshJaganSathyamoorthy,KasunWeerakoon,andDi- JoonHo Lee, Wentao Yuan, Zoey Chen, Samuel Deng,
neshManocha.Ga-nav:Efficientterrainsegmentationfor et al. Terrainnet: Visual modeling of complex terrain
robot navigation in unstructured outdoor environments. for high-speed, off-road navigation. arXiv preprint
IEEE Robotics andAutomation Letters, 7(3):8138–8145, arXiv:2303.15771, 2023.
2022. [30] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz
[17] Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Wellhausen,VladlenKoltun,andMarcoHutter. Learning
Changliu Liu, and Guanya Shi. Agile but safe: Learn- robust perceptive locomotion for quadrupedal robots in
ing collision-free high-speed legged locomotion. arXiv the wild. Science Robotics, 7(62):eabk2822, 2022.
preprint arXiv:2401.17583, 2024. [31] Pascal Roth, Julian Nubert, Fan Yang, Mayank Mittal,
[18] David Hoeller, Lorenz Wellhausen, Farbod Farshidian, and Marco Hutter. Viplanner: Visual semantic im-
and Marco Hutter. Learning a state representation and perative learning for local navigation. arXiv preprint
navigationinclutteredanddynamicenvironments. IEEE arXiv:2310.00982, 2023.
RoboticsandAutomationLetters,6(3):5081–5088,2021. [32] NikitaRudin,DavidHoeller,MarkoBjelonic,andMarco
[19] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco Hutter. Advancedskillsbylearninglocomotionandlocal
Hutter. Anymal parkour: Learning agile navigation for navigation end-to-end. In 2022 IEEE/RSJ International
quadrupedal robots. arXiv preprint arXiv:2306.14874, Conference on Intelligent Robots and Systems (IROS),
2023. pages 2497–2503. IEEE, 2022.
[20] JeminHwangbo,JoonhoLee,AlexeyDosovitskiy,Dario [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Radford,andOlegKlimov. Proximalpolicyoptimizationalgorithms. arXiv preprint arXiv:1707.06347, 2017.
[34] Joanne Truong, Denis Yarats, Tianyu Li, Franziska
Meier, Sonia Chernova, Dhruv Batra, and Akshara Rai.
Learningnavigationskillsforleggedrobotswithlearned
robot embeddings. In 2021 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 484–491. IEEE, 2021.
[35] Kasi Viswanath, Kartikeya Singh, Peng Jiang, PB Sujit,
and Srikanth Saripalli. Offseg: A semantic segmentation
framework for off-road driving. In 2021 IEEE 17th
international conference on automation science and en-
gineering (CASE), pages 354–359. IEEE, 2021.
[36] Guotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest,
Se´bastien Ourselin, and Tom Vercauteren. Aleatoric
uncertainty estimation with test-time augmentation for
medical image segmentation with convolutional neural
networks. Neurocomputing, 338:34–45, 2019.
[37] Maggie Wigness, Sungmin Eum, John G Rogers, David
Han,andHeesungKwon. Arugddatasetforautonomous
navigation and visual perception in unstructured outdoor
environments. In 2019 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS), pages
5000–5007. IEEE, 2019.
[38] Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe
Xu, and Xiaolong Wang. Learning vision-guided
quadrupedal locomotion end-to-end with cross-modal
transformers. arXiv preprint arXiv:2107.03996, 2021.
[39] Xinjie Yao, Ji Zhang, and Jean Oh. Rca: Ride comfort-
aware visual navigation via self-supervised learning. In
2022 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 7847–7852. IEEE,
2022.
[40] Chong Zhang, Nikita Rudin, David Hoeller, and Marco
Hutter.Learningagilelocomotiononriskyterrains.arXiv
preprint arXiv:2311.10484, 2023.
[41] Chong Zhang, Jin Jin, Jonas Frey, Nikita Rudin, Ma-
tias Eduardo Mattamala Aravena, Cesar Cadena, and
MarcoHutter.Resilientleggedlocalnavigation:Learning
to traverse with compromised perception end-to-end.
In 41st IEEE Conference on Robotics and Automation
(ICRA 2024), 2024.
[42] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christo-
pher Atkeson, Soeren Schwertfeger, Chelsea Finn, and
Hang Zhao. Robot parkour learning. arXiv preprint
arXiv:2309.05665, 2023.VI. IMPLEMENTATIONDETAILS TABLEVII
HYPERPARAMETERSINTHEPATHPLANNER
A. Motion Planner Details
Parameter d t k k k k k
0 0 T1 T2 G0 G1 G2
The motion planner is implemented following [5]. Both Default Value 0.5 0.5 1.0 2.0 0.1 0.4 −10
the actor and critic networks within the locomotion policy π
havehiddenlayersizesof[512,256,128].Theproprioception
observation op t includes angular velocity (3), Orientation (2), historical observation oh, updated through the MSE loss with
velocity commands (3), joint positions (12), joint velocities ctarg. The estimated valt ue c is normalized through sigmoid
t t
(12), and the last action (12). We store the last 10 steps of
function:
op into the history observation oh. The depth encoder learns
t t
the exteroception latent features from onboard observation I
d 1
using a conv-GRU structure. π is updated using PPO [33] Norm(c )= (14)
t 1+e2∗(2.2−ct)
for 20K iterations, the batch size is 160000 divided into 4
mini-batches.Thedepthencoderisoptimizedfor5kiterations. In practice, and we set a minimum threshold c th for the
The rewards obtained at each step are the sum of the reward normalized critic value be considered in the path planner, in
functions listed in Table VI. simulation c th = 0.8, while in the real world c th = 0.5,
Norm(c ) larger than c will be set to 1.
t th
TABLEVI M is calculated with k = 0.3, we demonstrate an
REWARDFUNCTIONS P P
example of M after the robot collides with a wall (with
P
Target Velocity Tracking1 min(v ,v ) vision and obstacle detection disabled in this trial) in Fig 8.
act lin
Target Direction Tracking e−|∆yaw| In this scenario, the normalized c t returns 0.02, M P has a
Orientation Penalty −(roll2+pitch2) width of 0.75m along the moving direction of the robot with
costs greater than 0.5.
Hip Joint Position Penalty −∥q ∥2
hip
z-direction velocity Penalty −|v |2
z
Collision Penalty2 −(cid:80) (Fi ≥0.1N)
i∈(calf,thigh) c
Torques Variation Penalty −∥τ −τ ∥
t t−1
1 vact istheprojectioncomponentofvact inthetargetdirection.
2 Fi isthecontactforceofcalfandthighindices.
c
B. Path Planner Details
For reference, we provide the default values of the hyper-
parameters used in the path planner in Table VII. Parameters Fig.8. WedemonstrateanexampleofMP whentherobotcollideswithan
d ,t adjust the weights of M and M as the robot obstacle.
0 0 T G
approaches the target and as time progresses. k keeps a
G0
Generally, the proprioception advisor engages when loco-
minimum weight for the robot approaching the target. The
motionstatesarecompromised.Toenablerecoveryfrombeing
listedvaluesremainconsistentinsimulation;however,consid-
obstructed by the unexpected terrains, we have designed the
ering the various conditions in the real world, we make slight
following recovery strategy in simulation: when Norm(c )
changestothesevaluesindifferentreal-worldexperimentsfor t
falls below a specific threshold (0.5), we initially assign a
bettervalidationofthecontributionofthiswork.Forinstance,
backwardvelocitycommandof0.5m/sfor0.5seconds.Next,
in outdoor navigation experiments, we assign a higher value
toaddresspotentialterrain-relatedconstraintsthatmayprevent
to k to expand the scale of the terrain estimator, which is
T1
the robot from reaching planned waypoints at its default
effective for evaluating the accuracy of the proposed terrain
velocity, we reduce the frequency of the path planner to 0.5
estimator.
Hzuntiltherobotreachesthewaypointormakesadirectional
Thedefaultcommandedvelcoityv 0 issetto0.5m/sinboth change of 0.3 radians.
simulationandtherealworld.Themaximumdistanced we
max
considered for computing the obstacle map M O is 0.3m. D. Terrain Estimator Details
C. Proprioception Advisor Details For prior data collection of the terrain estimator, we teleop-
eratetherobottowalkfor5minutesoneachoftheconcerned
The proprioception advisor is trained with the locomotion terrains. We capture the motion evaluations c as well as
t
policy π, with an exact same network architechture as π . the first-view observations during these demonstrations. We
critic
The input only includes proprioception observation op and convert the RGB observations into BEV maps and dividing
tthem into patches. The collected data is labelled based on
the corresponding terrain from which it was acquired. The
classification network is implemented using the MobileNet
backbone and trained on a Nvidia GTX 2080 Ti for 6 hours.
Theproposedpipelineprovidesalight-weightinferencemodel
of 8 MB to be deployed for onboard computation. During
deployment, at each step, we perform K = 8 forward
inferences of the terrain estimator to predict the vision-based
terrain traversability and its uncertainty.
Based on the collected motion evaluations, we compute the
average T observed during walks on each terrain. These
P
walking experiences provide reference values for assigning
the terrain costs defined by the operator. Table VIII presents
the validation accuracy of the terrain classifier, along with the
reference terrain cost and the average collected T for each
P
terrain.Sincetheterrainsencounteredintherealworlddidnot
Fig.9. t-SNEvisualizationofthefeaturesinferredbytheterrainclassifier
exhibit significant differences in motion evaluations, we did
not strictly set the reference cost based on linear correlation.
TABLEVIII
TERRAINCOSTANDCLASSIFYACCURACY
Name Bush Brick Snow Gravel
Accuracy (%) 89.58 92.28 83.20 83.47
T 0.481 0.523 0.445 0.507
P
Cost 0.7 0.6 0.9 0.7
Name Slab Cement Paved Grass
Accuracy 90.67 82.87 93.81 88.36
T 0.581 0.580 0.580 0.572
P
Cost 0.0 0.0 0.0 0.2
Fig.10. Weillustratethedistributionofsamplesacrossdifferentconfidence
levels, with the training of the terrain classifier, the confidence values for
The hyperparameters used for online corrections are fine-
known terrains predominantly clustered beyond 0.9, while samples of novel
tuned based on the distribution of the learned latent features terrainsremaineddispersedacrosstheconfidenceaxis.
L from the pre-collected data. Figure 9 depicts a t-SNE
visualization of the features inferred by the terrain classifier,
E. Hardware Details
involving data both within and outside of D . We observe
terrain
that π can learn distinctive features for specific terrains,
terrain We implement TOP-Nav on the Unitree-Go1 and Unitree-
showcasing unique clustering in the latent space, even for
Go2 quadruped robot, with a weight of approximately 12kg
previously unseen terrain types. The average cosine similarity
and dimensions of 645mm×280mm. The robot is equipped
S withineachterrainclassis0.95,wesetthemiddlepoint
(o,r) with 12 brushless motors, each capable of producing a torque
S to be 0.85 and k to be 15. This allows the terrain cost
0 T3 of 35.5Nm. The perception module is equipped with a Re-
derived from historical experience to decrease rapidly when
alSense D435i camera, offering simultaneous depth and RGB
the similarity falls below 0.8.
channels. All computations are processed onboard with an
To demonstrate the effectiveness of computing confidence NVIDIA Jetson NX asynchronously, i.e. the motion planner
indecidingwhethertheobservedterrainsisoutsideofD , operates at a fixed frequency of 50Hz and receives the latest
terrain
we illustrate the distribution of samples across different con- depthlatentℓinferencefromtheperceptionmodule.Thepath
fidence levels in Fig 10. After 200 steps of training, the planner operates at a frequency of 3Hz.
confidence values for known terrains predominantly clustered
beyond 0.9, while samples of novel terrains remained dis- VII. ADDITIONALEXPERIMENTS
persed across the confidence axis. The confidence is used to
adjust the weight of online correct ions in terrain estimation. In this section, we provide additional experiments to justify
We set k =20 and C =0.9 in the experiments. the effectiveness of the key components in TOP-Nav.
T4 0Fig.11. Weconductaseriesofstraightnavigationtasksinsimulation.Eachrobot’swaypointissetinastraightline,coveringterrainsofvariousdifficulties
[0.25,0.5,0.75,1]withtypescoveringslopes,discretestepsandirregularsurfaces.
A. Evaluations on Proprioception Advisor As shown in Table IX, c demonstrates a noticeable de-
ref
cline as terrain difficulty increases, yet it does not exhibit a
In this section, we discuss the relationship between the
significantadvantagecomparedtotheproposedproprioception
proposed proprioception advisor and terrain difficulties, along
advisor.
with ground truth reward terms such as velocity tracking and
On the other hand, the same terrain may exhibit different
attitude stability.
traversability for robots with varying locomotion capabili-
We conduct a series of straight navigation tasks in simula-
ties. Therefore, the evaluation of walking states represents
tion, as demonstrated in Fig 11. Each robot’s waypoint is set
another important metric for assessing terrain traversability.
in a straight line, covering terrains of various difficulties D
l We demonstrate the correlation between the predicted motion
and types: evaluations c and the ground truth motion states ctarget in Fig
t ref
12. The perason correlation coefficient is 0.914, indicating
• Irregular Surface: The height of each point in the terrain
a strong linear correlation between the estimated c and the
is randomly changed within the range of (0.01+0.04∗ t
actual motion states of the robot.
D ,0.07+0.04∗D )m.
l l
• Discrete Steps: This terrain type consists of 12∗D l mall
steps within a 1m×1m area, where each small step has
dimensions of (0.1,0.1,0.08)m.
• Slope&Holes:Randomlyswitchesbetweenupperslopes
and holes, where the maximum height/depth of each
slope/hole is set as 1.2m∗D .
l
We conduct 30k steps of navigation for each robot and
compute the average motion evaluations estimated by the
proprioceptionadvisor.TheresultsareshowninTableIX.We
observe that as terrain ruggedness, irregularity, and the pres-
enceofobstaclesincrease,thereisacorrespondingdecreasein
themotionevaluations.Althoughthiscorrelationisnotstrictly
linear, the results provide supportive for us to estimate terrain
traversability based on the proprioception advisor.
For comparison, we train an independent motion evaluation
function π , which is decoupled from the training process Fig. 12. We demonstrate the correlation between the predicted motion
deq
of the motion controller. π is updated using the MSE loss
evaluations ct and the ground truth motion states, the estimated ct and the
deq actualmotionstatesoftherobothasastronglinearcorrelation.
between the ground truth motion states and the network pre-
diction c ref. The ground truth walking states include velocity Building upon the discussion above, we validate that con-
tracking, orientation penalty, and energy consumption: structing the proprioception advisor with critic output enables
ctarget =min(v ,v )−(roll2+pitch2)−0.001∗τq˙, (15) efficient motion evaluation, including information on both the
ref act lin terrain difficulty and walking states of the robot. Moreover,
we train π for 10k iterations, ctarget is normalized using a the proposed advisor can be implemented without the need
deq ref
sigmoid function. foradditionaltrainingorsensors,makingitamoreappropriateTABLEIX
EVALUATIONOFTHEPROPRIOCEPTIONADVISORONDIFFERENTTYPESOFTERRAINS.
Method Terrain Difficulty Slope & Holes Discrete Steps Irregular Surface Flat Ground
0.25 0.7560 0.7964 0.7001
0.50 0.3316 0.3633 0.2589
Prop Advisor 0.8600
0.75 0.1893 0.1614 0.1058
1.0 0.0573 0.0837 0.0347
0.25 0.8649 0.9836 0.9811
0.50 0.3076 0.7403 0.8141
Decoupled Training 0.9900
0.75 0.2312 0.2323 0.4831
1.0 0.0770 0.3230 0.2563
TABLEX
SIMULATIONRESULTSWITHVELOCITYCOMPARISONS
Velocity (m/s) SR (%) ↑ TD(%) ↓ UT(%) ↓ VFT(%) ↓ AEC ↓
0.25 49.69±3.43 26.22±0.33 26.56±0.41 25.95±0.45 65.55±352.19
0.5 73.62±1.21 22.65±0.19 27.21±0.24 18.14±0.16 92.08±24.39
0.75 73.91±2.46 22.22±0.26 22.53±0.37 16.79±0.28 80.32±46.82
1.0 72.03±2.80 21.65±0.23 41.92±0.49 21.30±0.44 123.53±39.61
approachtoprovidemotionevaluationsforrobotpathplanning C. Long Distance Navigation
compared to existing methods.
To further investigate the open-world navigation ability of
the proposed system, we conduct a longer outdoor navigation
experimentcoveringatotaldistanceof190meters,asdepicted
in Fig. 13.
B. Different Velocities
We conducted quantitative experiments in simulation with
different velocities (0.25,0.5,0.75,1.0) (m/s), and the results
demonstrate that proper velocity design does affects the per-
formance of the navigation system. As demonstrated in Table
X, a slower velocity command (0.25) prevents the robot from
reaching the goal on time, resulting in a lower success rate.
Moreover,itcanbeconcludedthatlowerthespeedwouldhelp
decrease the energy consumption.
Ontheotherhand,settingatoolargevelocitycommandwill
introduce locomotion instability, as indicated by the results
with a velocity of 1m/s. However, conducting a quantitative
analysis of the influence of speed is challenging in complex
terrains. For instance, in slopes, robots with higher velocities
may navigate more effectively, whereas terrains with discrete
steps could lead to locomotion failures at faster speeds. Con-
sequently, experiments with velocity commands of 1m/s still Fig. 13. We demonstrate the open-world navigation ability of TOP-Nav in
along-distancenavigationtaskcovering190m
achieve a high success rate.
The performances vary between 0.5m/s and 0.75m/s is We conducted the experiment in an off-road scenario with
not significant; therefore, we consider this range to be an grass and gravel on both sides of the path. The robot was
appropriate commanded velocity range for deployment. In initially given a target direction straight ahead, relying on the
real-world experiments, we demonstrate the robot’s velocity terrain and obstacle estimator to keep navigating on the paved
set at 0.75m/s in the supplementary video. road. The path traversed by the robot is depicted in Fig. 13.Weobservethatinsuchscenarios,thechallengingterrainshelp
guidetherobotinchangingitsmovingdirection,ensuringthat
the robot stays on the paved road automatically. In contrast,
on the open flat terrain, we had to provide teleoperation to
make the robot change its direction and move onto another
narrowing path.
Since we do not have localization enabled in the real-
world experiment, human teleoperation was used three times
tochangethetargetmovingdirection.Thislimitationwasdue
to the onboard vision system’s inability to perform accurate
online localization, which we plan to address by integrating a
mounted lidar in future work.