CT-GLIP: 3D Grounded Language-Image
Pretraining with CT Scans and Radiology
Reports for Full-Body Scenarios
Jingyang Lin⋆ 1,2((cid:66)), Yingda Xia1((cid:66)), Jianpeng Zhang1,3, Ke Yan1,3,
Le Lu1, Jiebo Luo2, and Ling Zhang1
1DAMO Academy, Alibaba Group 2University of Rochester
3 Hupan Lab, 310023, Hangzhou, China
Abstract. MedicalVision-LanguagePretraining(Med-VLP)establishes
a connection between visual content from medical images and the rel-
evant textual descriptions. Existing Med-VLP methods primarily focus
on 2D images depicting a single body part, notably chest X-rays. In
this paper, we extend the scope of Med-VLP to encompass 3D images,
specificallytargetingfull-bodyscenarios,byusingamultimodaldataset
ofCTimagesandreports.Comparedwiththe2Dcounterpart,3DVLP
is required to effectively capture essential semantics from significantly
sparser representation in 3D imaging. In this paper, we introduce CT-
GLIP (Grounded Language-Image Pretraining with CT scans), a novel
method that constructs organ-level image-text pairs to enhance multi-
modal contrastive learning, aligning grounded visual features with pre-
cise diagnostic text. Additionally, we developed an abnormality dictio-
narytoaugmentcontrastivelearningwithdiversenegativesamples.Our
method, trained on a multimodal CT dataset comprising 44,011 organ-
level vision-text pairs from 17,702 patients across 104 organs, demon-
strates it can identify organs and abnormalities in a zero-shot manner
usingnaturallanguages.TheperformanceofCT-GLIPisvalidatedona
separate test set of 1,130 patients, focusing on the 16 most frequent ab-
normalities across 7 organs. The experimental results show our model’s
superiorperformanceoverthestandardCLIPframeworkacrosszero-shot
and fine-tuning scenarios, using both CNN and ViT architectures.
Keywords: MedicalVision-LanguagePretraining·CT·GroundedCon-
trastive Learning.
1 Introduction
Vision-languagepretraining(VLP)hasbecomeafundamentaltrainingparadigm
invision-language(VL)research,toenabletheVLframeworktolearnuniversal
and transferable vision-language representations in a weakly-supervised man-
ner[5,9,17].RecentattemptsinMedicalVLP(Med-VLP)[4,7,12,14,23,25,27,28]
⋆ Work was done during an internship at Alibaba DAMO Academy. Corresponding
authors((cid:66)): jlin81@ur.rochester.edu, yingda.xia@alibaba-inc.com.
4202
rpA
32
]VC.sc[
1v27251.4042:viXra2 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
Component Fine-grained “no evidentabnormality
RadiologyReport Extraction Radiology Descriptions in kidney” Text
… { “ r i g h “t o l lu dn ng o m duid led l ie n l uo pb pe e”: r { “right kidney stone” Encoder
Old nodule in upper
middle lobe of right lung”,
middle lobe of right LLMs },
l Cu hn og l. e cystolithiasis “ g a l “lb chla od led ce yr’ s: t{ olithiasis may”, Or Pg oa on l- il ne gvel T 1 T 2
may. }
G inr lo eu ftn ld u- ng gla ls os w n eo r d lou ble e . Manually “ l e f t “ glu rn og u nlo dw -ge lar slo sb ne o’: dule in EV ni cs oio dn er V 1 0.1 0.9
… Check left lung lower lobe”, Right Kidney
} Input CT Scans Segmentation
(a) Radiology report preprocessing (b) Zero-shot abnormality detection
Fig.1: Overview of (a) the procedure of radiology report preprocessing and (b)
the illustration of zero-shot abnormality detection. We parse radiology reports
with assistance of LLMs and manually check to extract respective descriptions
for each organ. After organ-level alignment, we evaluate zero-shot abnormality
detection via a similarity comparison between the visual feature and a pair of
positiveandnegativetextsforeachtargetedabnormality,e.g.,rightkidneystone.
have demonstrated the effectiveness of the VLP paradigm in medical imaging.
Byleveraginglarge-scalemedicalimage-reportpaireddataderivedfromroutine
clinical practice, these methods decrease the dependency on expensive anno-
tated data and alleviate the workload on radiologists. However, previous works
mainly focus on 2D medical images of single body parts (i.e., chest) due to data
scarcity.ThislimitationrestrictstheapplicationofMed-VLPinbroadermedical
contexts, particularly in dealing with 3D medical images that are not only more
complex but also constitute the primary workload in radiology departments,
offering a richer, more detailed view of patient anatomy and abnormalities.
To this end, our goal is to expand Med-VLP to incorporate 3D images, cov-
ering full body parts. Compared with 2D VLP, the sparse representations of 3D
images complicate the alignment of textual descriptions with the corresponding
visual concepts. Therefore, we seek to design an efficient 3D Med-VLP training
paradigm for the scenario of full-body 3D imaging.
Inthispaper,wepresentaninnovativemethod,namedCT-GLIP(Grounded
Language-ImagePretrainingwithCTscans),toreorganizegrounded(i.e.organ-
level) vision-text pairs for multimodal contrastive learning, while reducing the
complexity of 3D vision-text alignment from both grounded visual and textual
ends. For 3D images, we use Totalsegmentator [24] to generate segmentation
masks to identify the location of 104 organs. To the radiology report, we adopt
LLaMA-2 [20] and manually check to break down the original report into sev-
eral diagnostic descriptions for each organ, as shown in Fig. 1a. The simplified
groundedvisualandtextualcomponentsenableefficientlyassociatingtheorgan-
levelvisualconceptswithconcisediagnosticdescriptions.Technically,CT-GLIP
consistsoftwoobjectivesrespectivelyfororgan-textandabnormality-textalign-
ments. Organ-text alignment aims to understand basic medical visual concepts.
Meanwhile, abnormality-text alignment associates the abnormal visual compo-CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans 3
nentswiththecorrespondingtextdescriptions,facilitatingzero-shotabnormality
detection as shown in Fig. 1b. Furthermore, to mitigate the limitations of using
small mini-batch sizes in large-scale 3D models for contrastive learning, which
benefits from a larger number of diverse contrastive pairs [3,6,16,18,21,22], we
havedevelopedanabnormalitydictionarycontainingavarietyofabnormaltext
descriptions. This dictionary substantially increases the availability of diverse
negative pairs, thereby improving the effectiveness of contrastive learning.
Our study curates a multimodal CT image-report dataset with 44,011 pairs
from 17,702 patients covering 104 organs, and develops validation and test
datasets with 643 and 1,130 patients, respectively, targeting 16 common ab-
normalities across 7 organs. In Section 4, the proposed CT-GLIP outperforms
whole image-report alignment [17] in 3D imaging. It achieves notable zero-shot
performance for organ classification and abnormality detection, and enhances
tumor segmentation and detection for both CNN and ViT-based models.
The main contributions of CT-GLIP are summarized as follows: (1) we pro-
poseanovelmechanismtoreorganizegroundedvision-textpairsforefficient3D
Med-VLP;(2)webuildanabnormalitydictionarythatscalesupthenumberand
diversity of contrastive pairs; (3) empirical results on both zero-shot and fine-
tuningsettingsdemonstratethesuperiorityofgroundedimage-reportalignment
mechanism over whole image-report alignment in 3D imaging scenarios.
2 Related Work
GeneralVLP.Vision-languagepretrainingaimstodevelopmultimodalfounda-
tional models that enhance performance across a variety of tasks involving both
vision and language. CLIP [17] and ALIGN [9] represent significant milestones
in the VLP field. These models have highlighted the critical role of language
supervision in enhancing both computer vision and natural language processing
tasks. BLIP [10] strives to unify vision-language understanding and generation
during pretraining. GLIP [11] seeks to learn object-level, language-aware, and
semanticallyrichvisualrepresentationsthroughlocalvision-languagealignment.
Medical VLP. Medical image-report paired data, derived from routine clini-
cal practice, promotes the development of Med-VLP. ConVIRT [4] adapts the
CLIP methodology to medical imaging, enabling the matching of chest X-ray
images with their corresponding reports. Building upon this, MedCLIP [23] re-
fines the approach by processing images and texts separately, which effectively
scales the available training data at a low cost. CheXzero [19] further advances
the field by creating a system capable of detecting pathologies in a zero-shot
fashion. MedKLIP [25] incorporates additional medical knowledge to enhance
the joint analysis of images and language. LoVT [15] and GLoRIA [7] introduce
localized vision-language alignment, sharing similar motivations with our work.
However, our research distinguishes itself by focusing on 3D medical imaging,
which presents challenges with its significantly sparser representation. In addi-
tion,weemphasizethecriticalimportanceofnotonlygroundedvisualrepresen-
tation but also high-quality, organ-level text descriptions.4 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
Normal organs: {spleen, pancreas, liver, stomach,lung, …}
look up
Abnormal Descriptions Normal Template Abnormality Dictionary
“chronic pancreatitis” { “spleen”: {
no evident abnormality in {organ}
“right kidney stone” “splenomegaly”,
“calcification in spleen”,
…},
Text “pancreas’: {
Encoder “pancreatitis”,
“pancreatic cancer”,
Organ-level …},
Pooling t 1 t 2 t 3 … t M t M+1 t M+2 … t M+B “liver”: {
Multi-organ “fatty liver”,
Segmentation z 1 … … “hepatic cysts”,
…},
z 2 … … “lung’: { “lung cancer”,
z 3 … … “fibrous lesion in lung”,
... … … … … … … … … … … “ …p },leural calcification”,
…
Input CT Scans z M … … }
Fig.2: Overview of the abnormality-text alignment procedure. For each input
CT scan, we first segment all available organs using TotalSegmentator [24]. We
thenextractthe imagefeaturesusing a3Dvisionencoderandapply organ-level
pooling on each segmented organ mask to obtain a set of organ-level features
{z }M . The text input consists of M′ real abnormal descriptions, M − M′
i i=1
normal descriptions generated by the normal template (to increase the diversity
of the normal descriptions), and B additional abnormal descriptions (in grey)
fromtheabnormaldictionarylookedupbythenormalorgans.Wethengettext
features{t }M+B usinganexperttextencoder.Themodelisrequiredtopredict
i i=1
the correct vision-language pairs, where the red boxes denote the ground truth.
3 Methodology
In this section, we delve into the design of CT-GLIP for 3D grounded language-
imagepretraining.Ourpretrainingmethodologyconsistsoforgan-textalignment
and abnormality-text alignment, promoting the capabilities of zero-shot organ
classificationandabnormalitydetection.Furthermore,theproposedabnormality
dictionary significantly increases the scale and diversity of contrastive pairs,
optimizing the effectiveness of contrastive learning.
Problem Formulation. The basic motivation for multimodal contrastive loss
is to learn visual concepts from text by training an image encoder f and a text
encoder g to maximize the similarity between corresponding image and text
featureembeddingswhileminimizingitfornon-correspondingpairs.Forabatch
of N image-text pairs (V , T ), we first get the i-th normalized visual feature
n n
v =f(V )andthei-thnormalizedtextualfeaturet =f(T ).Then,thelossfor
i i i i
a single pair is shown below:
exp(vTt )/τ exp(vTt )/τ
L (v,t)=−log i i −log i i , (1)
i (cid:80)N exp(vTt )/τ (cid:80)N exp(vTt )/τ
k=1 i k k=1 k i
where τ is a temperature parameter. The total loss is L= 1 (cid:80)N L .
N i=1 i
noisiV redocnECT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans 5
Organ-Text Alignment. The motivation for organ-text alignment is to learn
the medical visual concepts from the supervision contained in the medical ex-
pertlanguagemodel,whichenablesourmodeltounderstandbasicmedicalvisual
concepts. Following the previous work [23], we adopt BioClinicalBERT [1] as an
experttextencodertogeneratetextembeddings.Organ-textalignmentrequires
organ-level visual embeddings and corresponding textual embeddings. Specifi-
cally, given a CT image V , the vision encoder projects the CT image into the
i
representation space and produces a feature map v . Based on the multi-organ
i
segmentation, we apply organ-level average pooling on each segmented organ
mask to obtain a set of organ-level visual features {z }M , where the M refers
ij j=1
to the number of organs in the given CT image. For each organ, we generate
its organ description T by integrating the specified organ into a predefined
ij
template, like “this is a {organ} in the CT scan”. We then feed the organ
descriptionsintotheexperttextencodertoproduceorgan-leveltextembeddings
{t }M . After that, our training objective L is to align organ-text features,
ij j=1 OT
as follows:
L =
1 (cid:88)M (cid:32)
−log
exp(zT ijt ij)/τ
−log
exp(zT ijt ij)/τ (cid:33)
. (2)
OTi M (cid:80)M exp(zTt )/τ (cid:80)M exp(zTt )/τ
j=1 k=1 ij ik k=1 ik ij
Furthermore, to better utilize the given pseudo-segmentation label y˜, we intro-
duceanadditionalsegmentationheadtopredictmulti-organsegmentation.The
segmentation objective L is a mixture of cross-entropy loss and dice loss.
segm
Abnormality-Text Alignment. The goal of abnormality-text alignment is to
integrate the knowledge of abnormality into the multimodal model. The train-
ing procedure of abnormality-text alignment is illustrated in Fig. 2. Similar to
organ-text alignment, we first extract organ-level visual features embeddings
{z }M from the given CT image V . Different from organ-text alignment, we
ij j=1 i
organize M diagnostic descriptions, including M′ organ-level real diagnostic de-
scriptions for abnormal organs and M −M′ generated descriptions with pre-
defined templates (e.g., “no evident abnormality in {organ}”) for normal
organs. Furthermore, to scale up the number and diversity of contrastive pairs
for abnormality-text alignment [6,16], we introduce an abnormality dictionary
storing diverse text descriptions of abnormalities for 104 organs. In particular,
for each normal organ, we look up T abnormal descriptions from the abnormal-
ity dictionary and integrate B =(M −M′)×T abnormal descriptions in total.
TheseB abnormaldescriptionsprovideadditionalnegativepairsformultimodal
contrastive learning to distinguish normal and abnormal organs. After that, all
M +B text descriptions are fed into the medical expert language model, pro-
ducing the text embedding {t }M+B. Given grounded paired vision and text
ij j=1
embeddings, the training objective of abnormality-text alignment is shown:
L =
1 (cid:88)M (cid:32)
−log
exp(zT ijt ij)/τ
−log
exp(zT ijt ij)/τ (cid:33)
, (3)
ATi M +B (cid:80)M+Bexp(zTt )/τ (cid:80)M exp(zTt )/τ
j=1 k=1 ij ik k=1 ik ij
wheretemperatureτ is0.07forbothorgan-textandabnormality-textalignment.6 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
Overall objective. The overall objective of our organ-level vision-language
alignment is calculated as the integration of organ-text contrastive loss L ,
OT
abnormality-text contrastive loss L , and auxiliary cross-entropy loss L
AT segm
(dice loss supervised by pseudo organ segmentation masks):
L=λ 1L OT+λ 2L AT+λ 3Lsegm, (4)
where the weights λ , λ , and λ are set to 0.5, 0.5, and 1.0, respectively.
1 2 3
4 Experiments
Dataset for pretraining.FortheproposedCT-GLIP,wecollectamultimodal
dataset of CT images and reports containing 17,702 consecutive patients with
44,011 organ-level vision-text pairs.
Pretraining details.Forthevisionpart,weemploybothrepresentativeCNN-
based and ViT-based vision encoders, particularly nnUNet [8] and MiT [26]. To
keepthelow-levelsemantics,wefeedthefeaturemapwiththehighestresolution
into the organ-level average pooling. On top of organ-level average pooling, an
additional two-layer MLP (hidden layer 768-d with ReLU) is added. For the
language part, we adopt BioClinicalBERT [1] as an expert text encoder. We
keep the expert text encoder frozen [13] to avoid catastrophic forgetting by CT-
specificdomaindata.Thebatchsizeis8,distributedon4V100GPUs.Wetrain
the CT-GLIP for 20 epochs since the training loss has converged at that point.
We adopt the Adam optimizer with the weight decay of 3e-5, an initial learning
rate of 1e-3, and a final learning rate of 1e-6 upon the cosine decay rule.
4.1 Zero-shot Evaluation
Dataset for zero-shot evaluation. To assess the zero-shot capabilities, we
furtherbuildadditionaldatasets,including643patientsforvalidationand1,130
patients for testing, specifically focusing on the 16 most frequent abnormalities.
Pleaserefertothesupplementarymaterialformoredetailsabouttheevaluation
datasets and zero-shot inference for organ-text and abnormality-text alignment.
Baseline model. We adopt vanilla CLIP [17] as our baseline model, which
employs standard image-level contrastive pairs.
Impact of abnormality-text (AT) alignment.Inzero-shotabnormalityde-
tection,theATalignmentgreatlyenhancesperformancecomparedtothevanilla
CLIPacrossdifferentarchitecturesinTable1.Theresultsshowthatimage-level
contrastive learning can hardly learn useful information from the 3D CT-report
pairs. It highlights that CT-GLIP can efficiently facilitate effective VL align-
ment over the sparse representations of 3D images. In particular, with nnUNet
backbones, there is a 15.0% increase in the F1 score and a 16.4% rise in AUC.
The boosts on MiT backbones are even more pronounced, with improvements
of 15.6% in F1 score and 19.5% in AUC.
Impact of organ-text (OT) alignment. Table 1 shows that OT alignment
equipsourmodelwithastrongcapabilityofzero-shotorganclassification.Inpar-
ticular,theperformanceontop-1accuracyreaches86.9%and85.4%fornnUNetCT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans 7
Table 1: The performance of zero-shot organ classification and pathology detec-
tion. OT align indicates organ-text alignment, AT align refers to abnormality-
textalignmentandA-Dictdenotesabnormalitydictionary.Top-1accuracy,PPV
(Positive Predictive Value), Sensitivity, F1 score, and AUC are shown in %.
Zero-shot Zero-shot
Method OrganClassification AbnormalityDetection
Top-1Acc↑ PPV↑Sensitivity↑ F1↑ AUC↑
CNN-basedarchitecture:nnUNet
VanillaCLIP[17] 0.00 32.75 35.19 33.93 52.23
+ATalign 0.03 35.24 70.66 47.02 66.00
CT-GLIP +ATalign+OTalign 0.00 39.07 64.11 48.60 66.76
+ATalign+OTalign+A-Dict 86.92 39.24 72.85 49.02 68.63
ViT-basedarchitecture:MiT
VanillaCLIP[17] 0.00 34.01 40.43 36.94 52.37
+ATalign 0.07 37.65 74.24 49.96 69.27
CT-GLIP +ATalign+OTalign 0.05 38.24 77.43 51.19 70.12
+ATalign+OTalign+A-Dict 85.46 39.47 78.59 52.55 71.90
and MiT, respectively. Moreover, Table 1 presents OT alignment further im-
proves performance on zero-shot abnormality detection, demonstrating that the
capabilityofbasicvisualconceptunderstandingservesasafoundationforbetter
abnormality detection. In particular, for both nnUNet and MiT, OT alignment
achieves more than 2% boosts in F1 score and AUC.
Impact of abnormality dictionary. The abnormality dictionary aims to ex-
pand diverse negative samples since large-scale negative samples benefit con-
trastive learning [16]. The scale of the abnormality dictionary is 512 because
the larger scale will no longer benefit the performance. Based on AT and OT
alignment, the abnormality dictionary improves performance on zero-shot ab-
normality detection for both nnUNet and MiT.
4.2 Fine-Tuning Evaluation on Cancer Screening
Dataset and evaluation. For the evaluation of the proposed CT-GLIP in
a downstream fine-tuning context, we prepare an in-house dataset encompass-
ing700non-contrastCTscansof700patients,specificallytargetingsevenofthe
mostprevalenttypesofcancer,includinglung,breast,liver,esophagus,stomach,
colorectum, and pancreas cancer, 100 patients for each type. This dataset is de-
signed to validate the adaptability and performance of our pre-trained model in
thesegmentationanddetectionofthesetypesofcanceronnoncontrastCTscans,
whichisanemergingandchallengingclinicaltask[2].Sevenboard-certifiedradi-
ologistsmanuallyannotatedthepixel-levelmaskofthetumors,allconfirmedby
histopathology. We randomly split the dataset into 448, 112, and 140 cases for
training, validation, and test set, respectively. The performance is evaluated by
theDicescorefortumorsegmentation.Asforthepatient-leveldetectionofeach
type of tumor (the presence or absence of each tumor), we use the 3D volume
of the respective tumor as the score for the computation of AUC [29].8 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
Table 2: The performance of downstream fine-tuning on the task of cancer
screeningofpancreas(Pan),breast(Bre),stomach(Sto),colorectum(Col),lung,
esophagus (Eso),and liver (liv). Tumorsegmentationis evaluated via DSC(%),
and the performance of cancer screening is evaluated via AUC (%).
Metric Method Pan Bre Sto Col Lung Eso Liv Mean
CNN-based architecture:nnUNet
Scratch 38.77 13.20 20.36 24.18 40.45 52.94 19.25 29.88
VanillaCLIP[17] 50.58 19.28 19.98 24.31 47.15 56.45 16.43 33.45
CT-GLIP(ours) 52.42 20.59 23.13 26.16 48.89 53.25 18.44 34.70
DSC↑
ViT-based architecture:MiT
Scratch 35.18 19.13 0.00 18.56 11.12 40.32 34.41 22.68
VanillaCLIP[17] 36.02 19.96 17.47 28.78 33.39 50.19 28.75 30.65
CT-GLIP(ours) 39.85 22.84 27.23 34.15 42.32 46.60 37.39 35.77
CNN-based architecture:nnUNet
Scratch 92.19 70.23 83.48 90.72 74.91 100.00 63.31 82.12
VanillaCLIP[17] 96.69 80.97 83.17 86.99 92.05 100.00 71.39 87.32
CT-GLIP(ours) 97.73 81.49 90.64 90.54 92.43 93.70 80.35 89.55
AUC↑
ViT-based architecture:MiT
Scratch 97.63 77.91 50.00 90.26 74.05 98.74 77.99 80.94
VanillaCLIP[17] 90.10 85.83 78.40 95.65 78.45 99.55 81.03 87.00
CT-GLIP(ours) 91.48 81.31 87.79 95.03 85.76 96.35 82.46 88.60
Fine-tuning strategy. We employ the same two backbone architectures, i.e.,
nnUNet [8] and the MiT [26] network. For the nnUNet backbone, we use the
originaltrainingscheduleandtheself-configuredarchitecture,onlywithourpre-
trained model as initialization. The batch size is 8 and we train all experiments
for 125k iterations. For the MiT backbone, we add an UNet-style decoder for
the segmentation task, fix the MiT encoder for the initial 25k iterations, and
tune the whole encoder-decoder network for another 100k iterations. The opti-
mizer for finetuning MiT is RAdam with an initial learning rate of 0.001 and a
polynomial learning rate decay.
Results. For both backbones, our CT-GLIP outperforms the baseline models
trained from scratch and those fine-tuned with vanilla CLIP training, as shown
in Table 2. For example, CT-GLIP outperforms the model trained from scratch
andfine-tunedfromvanillaCLIPby4.8%and1.3%inmeantumorsegmentation
dice score, and 7.4% and 2.2% in cancer detection AUC score for the nnUNet
backbone, respectively. For the MiT backbones, the respective improvements
are 13.1% and 5.1% for tumor segmentation, and 7.7% and 1.6% for cancer de-
tection. Generally, either pretrained with CLIP or CT-GLIP can improve the
performance by a large margin, illustrating the importance of pre-learned rep-
resentation for this clinically significant task. Our superiority over vanilla CLIP
further illustrates the efficacy of our method in leveraging visual-textual associ-
ations for enhanced tumor-related image representations.CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans 9
5 Conclusion
In this study, we have expanded VLP into 3D medical imaging, particularly
full-body CT scans, by generating grounded (organ-level) image-text pairs and
enhancing learning pair diversity with an abnormality dictionary. Our proposed
CT-GLIP overcomes sparse data challenges and shows promise in zero-shot
recognition of organs and abnormalities, with implications for improving the
downstreamtaskofmulti-cancerscreening.Thisresearchestablishesnewbench-
marks for evaluating 3D VLP’s potential in medical diagnostics.
References
1. Alsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D., Naumann, T., Mc-
Dermott, M.: Publicly available clinical bert embeddings. In: NAACL. pp. 72–78
(2019)
2. Cao,K.,Xia,Y.,Yao,J.,Han,X.,Lambert,L.,Zhang,T.,Tang,W.,Jin,G.,Jiang,
H.,Fang,X.,etal.:Large-scalepancreaticcancerdetectionvianon-contrastctand
deep learning. Nature Medicine pp. 3033–3043 (2023)
3. Caron,M.,Misra,I.,Mairal,J.,Goyal,P.,Bojanowski,P.,Joulin,A.:Unsupervised
learning of visual features by contrasting cluster assignments. In: NeurIPS. pp.
9912–9924 (2020)
4. Chauhan, G., Liao, R., Wells, W., Andreas, J., Wang, X., Berkowitz, S., Horng,
S., Szolovits, P., Golland, P.: Joint modeling of chest radiographs and radiology
reports for pulmonary edema assessment. In: MICCAI. pp. 529–539 (2020)
5. Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J., et al.: Vision-language pre-
training:Basics,recentadvances,andfuturetrends.FoundationsandTrends®in
Computer Graphics and Vision pp. 163–352 (2022)
6. He,K.,Fan,H.,Wu,Y.,Xie,S.,Girshick,R.:Momentumcontrastforunsupervised
visual representation learning. In: CVPR. pp. 9729–9738 (2020)
7. Huang,S.C.,Shen,L.,Lungren,M.P.,Yeung,S.:Gloria:Amultimodalglobal-local
representationlearningframeworkforlabel-efficientmedicalimagerecognition.In:
ICCV. pp. 3942–3951 (2021)
8. Isensee,F.,Jaeger,P.F.,Kohl,S.,Wasserthal,J.,Koehler,G.,Norajitra,T.,Wirk-
ert, S., Maier-Hein, K.H.: nnu-net: a self-configuring method for deep learning-
based biomedical image segmentation. Nature Methods pp. 1–9 (2020)
9. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision. In: ICML. pp. 4904–4916 (2021)
10. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML. pp. 12888–
12900 (2022)
11. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L.,
Zhang,L.,Hwang,J.N.,etal.:Groundedlanguage-imagepre-training.In:CVPR.
pp. 10965–10975 (2022)
12. Lin, W., Zhao, Z., Zhang, X., Wu, C., Zhang, Y., Wang, Y., Xie, W.: Pmc-clip:
Contrastivelanguage-imagepre-trainingusingbiomedicaldocuments.In:MICCAI.
pp. 525–536 (2023)10 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
13. Liu, C., Cheng, S., Chen, C., Qiao, M., Zhang, W., Shah, A., Bai, W., Arcucci,
R.: M-flag: Medical vision-language pre-training with frozen language models and
latent space geometry optimization. In: MICCAI (2023)
14. Lu, M.Y., Chen, B., Zhang, A., Williamson, D.F., Chen, R.J., Ding, T., Le, L.P.,
Chuang, Y.S., Mahmood, F.: Visual language pretrained multiple instance zero-
shot transfer for histopathology images. In: CVPR. pp. 19764–19775 (2023)
15. Mu¨ller,P.,Kaissis,G.,Zou,C.,Rueckert,D.:Jointlearningoflocalizedrepresen-
tations from medical images and reports. In: ECCV. pp. 685–701 (2022)
16. Oord,A.v.d.,Li,Y.,Vinyals,O.:Representationlearningwithcontrastivepredic-
tive coding. arXiv:1807.03748 (2018)
17. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML. pp. 8748–8763 (2021)
18. Tian, Y., Krishnan, D., Isola, P.: Contrastive multiview coding. In: ECCV. pp.
776–794 (2020)
19. Tiu,E.,Talius,E.,Patel,P.,Langlotz,C.P.,Ng,A.Y.,Rajpurkar,P.:Expert-level
detection of pathologies from unannotated chest x-ray images via self-supervised
learning. Nature Biomedical Engineering pp. 1399–1406 (2022)
20. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv:2307.09288 (2023)
21. Wang, Y., Lin, J., Cai, Q., Pan, Y., Yao, T., Chao, H., Mei, T.: A low rank
promoting prior for unsupervised contrastive learning. IEEE TPAMI pp. 2667–
2681 (2022)
22. Wang, Y., Lin, J., Zou, J., Pan, Y., Yao, T., Mei, T.: Improving self-supervised
learningwithautomatedunsupervisedoutlierarbitration.In:NeurIPS.pp.27617–
27630 (2021)
23. Wang, Z., Wu, Z., Agarwal, D., Sun, J.: Medclip: Contrastive learning from un-
paired medical images and text. In: EMNLP. pp. 3876–3887 (2022)
24. Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W.,
Heye,T.,Boll,D.T.,Cyriac,J.,Yang,S.,etal.:Totalsegmentator:Robustsegmen-
tation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence
(2023)
25. Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Medklip: Medical knowledge
enhanced language-image pre-training. In: ICCV. pp. 21372–21383 (2023)
26. Xie, Y., Zhang, J., Xia, Y., Wu, Q.: Unimiss: Universal medical self-supervised
learning via breaking dimensionality barrier. In: ECCV. pp. 558–575 (2022)
27. You,K.,Gu,J.,Ham,J.,Park,B.,Kim,J.,Hong,E.K.,Baek,W.,Roh,B.:Cxr-
clip:Towardlargescalechestx-raylanguage-imagepre-training.In:MICCAI.pp.
101–111 (2023)
28. Zhang,Y.,Jiang,H.,Miura,Y.,Manning,C.D.,Langlotz,C.P.:Contrastivelearn-
ingofmedicalvisualrepresentationsfrompairedimagesandtext.In:MLHC.pp.
2–25 (2022)
29. Zhu, Z., Xia, Y., Xie, L., Fishman, E.K., Yuille, A.L.: Multi-scale coarse-to-fine
segmentation for screening pancreatic ductal adenocarcinoma. In: MICCAI. pp.
3–12 (2019)CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans 11
A Supplementary Material
spleen pancreas
0.7% 0.9%
aorta gallbladder
0.6% 2.4%
others kidney
14.1% 6.7%
liver
12.0%
lung
62.6%
(a) 7 most common organs (b) Word cloud for all radiology reports
Fig.3:Overviewof(a)theratioof7mostcommonorgansand(b)theillustration
of the word cloud for the radiology reports in our dataset.
A.1 Details about Zero-shot Evaluation
16 representative abnormalities over 7 common organs. The evaluation
of zero-shot abnormality detection requires the model to identify whether the
givenorganisabnormal.AsshowninFig.3a,wefirstselectthe7mostfrequent
organs,includingthespleen,pancreas,aorta,gallbladder,kidney,liver,andlung.
Then, we select 1-3 most common abnormalities from these 7 organs. Figure 3b
illustrates the word cloud of the radiology report over our dataset. In Table 3,
we present 16 abnormalities from 7 frequent organs. Figure 4 shows the AUC of
zero-shot abnormality detection over 16 abnormalities on MiT backbones. The
result demonstrates the superiority of our proposed CT-GLIP.
Table 3: The 16 representative abnormalities across 7 organs.
Organ Abnormalities
Spleen splenomegaly, spleen calcification
Pancreas acute pancreatitis, chronic pancreatitis, pancreatic duct stones
Aorta arteriosclerosis of aorta
Kidney kidney stone, renal cyst
Liver fatty liver, hepatic cyst, hepatic calcification
Lung old lesions in lung, pulmonary nodules, pulmonary fibrous lesion12 CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans
Vanilla CLIP
0.5
CT-GLIP: + AO align
CT-GLIP: + AO align + A-Dict
CT-GLIP: + AO align + OT align + A-Dict
0.0
0 2 4 6 8 10 12 14
Abnormality ID
Fig.4: The AUC over 16 representative abnormalities on MiT backbones.
spleen right kid ln ee fty kidn gey allbladd …er urinary bladder
Template
this is a {organ} in the ct scans
Text
Encoder
Organ-level
Multi-organ Pooling t 1 t 2 t 3 t 4 t 5 … t C
Segmentation
z 1 …
z 2 …
z 3 …
... … … … … … … … …
Input CT Scans z M …
Fig.5: Illustration of zero-shot organ classification in CT-GLIP.
Inference for zero-shot organ classification. Fig. 5 illustrates the infer-
ence process for zero-shot organ classification. In particular, we first generate
organdescriptionsforall104organsbyagiventemplate.Wethenconvertthese
descriptions into text embeddings using expert text encoder. Meanwhile, the
corresponding CT scans and multi-organ segmentation are fed into the 3D im-
age encoder to produce organ-level visual embeddings. The class label whose
text embedding is closest to the image embedding is then predicted as the most
likely class for the organ. This approach allows CT-GLIP to perform 104-way
organ classification tasks on CT scans using just organ descriptions of possible
outcomes,enablingaccurateandflexibleclassificationwithoutdirecttrainingon
the task’s specific classes. The results of zero-shot organ classification on top-1
accuracy are shown in Table 1.
Inferenceforzero-shotabnormalitydetection.InFig.1b,weillustratethe
inference process of zero-shot abnormality detection. For each test CT image,
we provide a pair of normal and abnormal text descriptions with corresponding
organ segmentation. We assess the similarity between the organ-level grounded
visual features and both normal and abnormal textual embeddings for each tar-
geted abnormality. The prediction is made based on the higher similarity score.
Clearly,zero-shotabnormalitydetectionoperatesasabinaryclassificationtask.
CUA
noisiV redocnE