ID-Animator: Zero-Shot Identity-Preserving Human
Video Generation
XuanhuaHe1,2∗,QuandeLiu3†,ShengjuQian3,XinWang3,
TaoHu1,2,KeCao1,2,KeyuYan1,2,ManZhou1,JieZhang2†
1University of Science and Technology of China
2Hefei Institute of Physical Science, Chinese Academy of Sciences
3LightSpeed Studios, Tencent
https://id-animator.github.io/
Figure 1: Given simply one facial image, our ID-Animator is able to produce a wide range of
personalizedvideosthatnotonlypreservetheidentityofinputimage,butfurtheralignwiththegiven
textprompt,allwithinasingleforwardpasswithoutfurthertuning.
Abstract
Generatinghighfidelityhumanvideowithspecifiedidentitieshasattractedsignifi-
cantattentioninthecontentgenerationcommunity. However,existingtechniques
struggletostrikeabalancebetweentrainingefficiencyandidentitypreservation,ei-
therrequiringtediouscase-by-casefinetuningorusuallymissingtheidentitydetails
invideogenerationprocess. Inthisstudy,wepresentID-Animator,azero-shot
human-videogenerationapproachthatcanperformpersonalizedvideogeneration
givensinglereferencefacialimagewithoutfurthertraining. ID-Animatorinherits
existingdiffusion-basedvideogenerationbackboneswithafaceadaptertoencode
theID-relevantembeddingsfromlearnablefaciallatentqueries. Tofacilitatethe
extractionofidentityinformationinvideogeneration,weintroduceanID-oriented
datasetconstructionpipeline,whichincorporatesdecoupledhumanattributeand
∗InterninTencent †Correspondingauthors
Preprint.Underreview.
4202
rpA
32
]VC.sc[
1v57251.4042:viXraactioncaptioningtechniquefromaconstructedfacialimagepool. Basedonthis
pipeline,arandomfacereferencetrainingmethodisfurtherdevisedtoprecisely
capturetheID-relevantembeddingsfromreferenceimages,thusimprovingthe
fidelityandgeneralizationcapacityofourmodelforID-specificvideogeneration.
Extensive experiments demonstrate the superiority of ID-Animator to generate
personalizedhumanvideosoverpreviousmodels. Moreover,ourmethodishighly
compatiblewithpopularpre-trainedT2Vmodelslikeanimatediffandvariouscom-
munitybackbonemodels,showinghighextendabilityinreal-worldapplicationsfor
videogenerationwhereidentitypreservationishighlydesired.Ourcodesandcheck-
pointswillbereleasedathttps://github.com/ID-Animator/ID-Animator.
1 Introduction
Personalizedorcustomizedgenerationistocreateimagesconsistentinstyle,subject,orcharacter
IDbasedononeormorereferenceimages. Intherealmofimagegeneration,considerablestrides
havebeenmadeincraftingthisidentity-specificcontent,particularlyinthedomainofhumanimage
synthesis[20,27,33,29]. Recently,text-drivenvideogeneration[10,28,30]hasgatheredsubstantial
interest within the research community. These methods enable the creation of videos based on
user-specified textual prompts. However, the quest for generating high-fidelity, identity-specific
humanvideosremainsanareatoexplore. Thegenerationofidentity-specifichumanvideosholds
profound significance, particularly within the film industry, where characters must authentically
executeactions. Previousapproachestocustomizationprimarilyemphasizedspecifiedpostures[16],
styles[22],andactionsequences[32],oftenemployingadditionalcontroltoensuregeneratedvideos
metuserrequirements. However,thesemethodslargelyoverlookspecificidentitycontrol. Some
techniquesinvolvedmodelfine-tuningthroughmethodslikeLoRA[15]andtexturalinversion[7]
toachieveID-specificcontrol[23],butattheexpenseofsubstantialtrainingcostsandnecessitating
separatetrainingweightsforeachID.Othersreliedonimagepromptstoguidethemodelingenerating
videosfeaturingparticularsubjects, yetencounteredchallengessuchasintricatedatasetpipeline
constructionandlimitedIDvariations[18].
Furthermore,thedirectintegrationofimagecustomizationmodules[36]intothevideogeneration
modelresultedinpoorquality,suchasstaticmotionandineffectiveinstructionfollowing. Asshown
inFigure2,theuseofsuccessfulimagecustomizationmethods,IP-Adapter,ledtoclearfailuresin
followingtextualdescriptionsandidentitypreservation,aswellassubtlemotiondynamics.
ThefieldofID-specifiedvideogenerationcurrentlyconfrontsseveralnotablechallenges:
1. High training costs: Many ID-specified methods need large training costs, often due
to the customization modules with large parameter counts and lack of prior knowledge,
consequently imposing significant training overheads. These training costs hinder the
widespreadadoptionandscalabilityofID-specifiedvideogenerationtechniques.
2. Scarcityofhigh-qualitytext-videopaireddatasets: Unliketheimagegenerationcom-
munity,wheredatasetslikeLAION-face[39]arereadilyavailable,thevideogeneration
communitylackssufficienthigh-qualitytext-videopaireddatasets. Existingdatasets,such
asCelebV-text[37], featurecaptionsannotatedwithfixedtemplatesthatconcentrateon
emotionchangeswhileignoringhumanattributesandactions,makingthemunsuitablefor
ID-preserving video generation tasks. This scarcity hampers research progress, forcing
manyendeavorstoresorttocollectingprivatedatasets.
3. InfluenceofID-irrelevantfeaturesfromreferenceimagesonvideogenerationquality:
ThepresenceofID-irrelevantfeaturesinreferenceimagescanadverselyaffectthequality
ofgeneratedvideos. Reducingtheinfluenceofsuchfeaturesposesachallenge,demanding
novelsolutionstoensurefidelityinID-specifiedvideogeneration.
Solutions To tackle the first issue, we propose an efficient ID-specific video generation frame-
work,namedID-Animator,whichiscomposedofapre-trainedtext-to-videodiffusionmodelanda
lightweightfaceadaptermodule. Withthisdesign,ourmodulecancompletetrainingwithinadayon
asingleA100GPUandcangenerate21framesofvideoonasingle3090GPU.Toaddressthesecond
issue,webuildanID-orienteddatasetconstructionpipeline. Byleveragingexistingpubliclyavailable
datasets,weintroducetheconceptofdecoupledcaptions,whichinvolvesgeneratingcaptionsfor
2Figure 2: Comparison between the proposed ID-Animator and previous approaches. Directly
integratingimagecustomizationmodulesintothevideogenerationmodelledtopoorqualityresults.
humanactions,humanattributes,andaunifiedhumandescription. Additionally,weutilizefacial
recognition,cropping,andothertechniquestocreatecorrespondingreferenceimages. Trainedwith
there-writtencaptions,ourID-Animatorsignificantlyenhancesitseffectivenessinfollowinginstruc-
tions. Inresponsetothethirdissue,wedeviseanoveltrainingmethodforusingrandomfaceimages
asreferences. Byrandomlysamplingfacesfromthefacepool,wedecoupleID-independentimage
contentfromID-relatedfacialfeatures,allowingtheadaptertofocusonID-relatedcharacteristics.
Through the aforementioned designs, our model can achieve ID-specific video generation in a
lightweightmanner. Itseamlesslyintegratesintoexistingcommunitymodels[5],showcasingrobust
generalizationandID-preservingcapabilities.
Ourcontributioncanbesummarizedasfollows:
• WeproposeID-Animator,anovelframeworkthatcangenerateidentity-specificvideosgiven
any reference ficial image without model tuning. It inherits pre-trained video diffusion
modelswithalightweightfaceadaptertoencodetheID-relevantembeddingsfromlearnable
faciallatentqueries.Tothebestofourknowledge,thisisthefirstendeavortowardsachieving
zero-shotID-specifichumanvideogeneration.
• WedevelopanID-orienteddatasetconstructionpipelinetomitigatethemissingoftraining
datasetinpresonalizedvideogeneration. Overpubliclyavailabledatasources,wepresent
decoupled captioning of human videos, which extracts textual descriptions for human
attributesandactionrespectivelytoattaincomprehensivehumancaptions. Besides,afacial
imagepoolisconstructedoverthisdatasettofacilitatetheextractionoffacialembeddings.
• Overthispipeline,wefurtherdevisearandomreferencetrainingstrategyforID-Animator
topreciselyextracttheidentity-relevantfeaturesanddiminishtheinfluenceofID-irrelevant
informationinherentinthereferencefacialimage,thereforeimprovingtheidentityfidelity
andgenerationabilityinreal-worldapplicationsforpersonalizedhumanvideogeneration.
2 RelatedWork
2.1 VideoGeneration
Videogenerationhasbeenakeyareaofinterestinresearchforalongtime. Earlyendeavorsinthe
taskutilizedmodelslikegenerativeadversarialnetworks[14,4,17]andvectorquantizedvariational
autoencodergeneratevideo[19,8,24]. However,duetotheinherentmodelability,thisvideolacks
motionanddetailsandisunabletoachievegoodresults. Withtheriseofthediffusionmodel[11],
notablythelatentdiffusionmodel[25]anditssuccessinimagegeneration,researchershaveextended
thediffusionmodel’sapplicabilitytovideogeneration[13,12,1]. Thistechniquecanbeclassified
intotwoparts: image-to-videoandtext-to-videogeneration. Theformeressentiallytransformsa
givenimageintoadynamicvideo,whereasthelattergeneratesvideoonlyfollowingtextinstructions,
3withoutanyimageasinput. Leading-edgemethods,exemplifiedbytheseworks,includeAnimate
Diffusion[10],Dynamicrafter[31],Modelscope[28],AnimateAnything[6],andStableVideo[1],
amongothers. Thesetechniquesgenerallyexploitpre-trainedtext-to-imagemodelsandintersperse
them with diverse forms of temporal mixing layers. Although these techniques are pushing the
boundariesofproducingvisuallyappealingvideos,thereisstillagapinprovidinguser-specificvideo
creationusingreferenceimages,likeportraits.
2.2 IDPreservingImageGeneration
The impressive generative abilities of diffusion models have attracted recent research endeavors
investigatingtheirpersonalizedgenerationpotential. Currentmethodswithinthisdomaincanbe
dividedintotwocategories,basedonthenecessityoffine-tuningduringthetestingphase. Asubset
of these methods requires the fine-tuning of the diffusion model leveraging ID-specific datasets
duringthetestingphase,representativetechniquessuchasDreamBooth[26],textualinversion[7],
andLoRA[15]. WhilethesemethodsexhibitacceptableIDpreservationabilities,theynecessitate
individualmodeltrainingforeachuniqueID,thusposingasignificantchallengerelatedtotraining
costsanddatasetcollection,subsequentlyhinderingtheirpracticalapplicability. Thelatestfocusof
researchinthisdomainhasshiftedtowardstraining-freemethodsthatbypassadditionalfine-tuningor
inversionprocessesintestingphase. Duringtheinferencephase,itispossibletocreateahigh-quality
ID-preservingimagewithjustareferenceimageasthecondition. MethodslikeFace0[27]replace
thefinalthreetokensoftextembeddingwithfaceembeddingwithinCLIP’sfeaturespace,utilizing
thisnewembeddingasconditionalforimagegeneration. PhotoMaker[20],ontheotherhand,takes
a similar approach by stacking multiple images to reduce the influence of ID-irrelevant features.
Similarly,IP-Adapter[36]decoupledreferenceimagefeaturesandtextfeaturestofacilitatecross
attention,resultinginbetterinstructionfollowing.Concurrently,InstantID[29]combinedthefeatures
ofIP-AdapterandControlNet[38],utilizingbothglobalstructuralattributesandthefine-grained
featuresofreferenceimagesforthegenerationofID-preservingimages. Althoughthesemethods
haveyieldedpromisingresults,thedomainofvideogenerationstillremainsrelativelyunderexplored.
2.3 SubjectDrivenVideoGeneration
Researchonsubject-drivenvideogenerationisstillinitsearlystages,withtwonotableworksbeing
VideoBooth[18]andMagicMe[23]. VideoBooth[18]strivestogeneratevideosthatmaintainhigh
consistencywiththeinputsubjectbyutilizingthesubject’sclipfeatureandlatentembeddingobtained
through a VAE encoder. This approach offers more fine-grained information than ID-preserving
generation methods; however, its limitation remains as the subjects required to be present in the
trainingdata,suchascats,dogs,andvehicles,whichresultsinarestrictedrangeofapplicablesubjects.
MagicMe[23],ontheotherhand,ismorecloselyrelatedtotheID-preservinggenerationtask. It
learnsID-relatedrepresentationsbygeneratinguniqueprompttokensforeachID.However,this
methodrequiresseparatetrainingforeachID,makingitunabletoachievezero-shottraining-free
capabilities. Thislimitationposesachallengeforitspracticalapplication. Ourproposedmethod
distinguishes itself from these two approaches by being applicable to any human image without
necessitatingretrainingduringinference.
3 Method
3.1 Overview
Given a reference ID image, ID-Animator endeavors to produce high-fidelity ID-specific human
videos. Figure3demonstratesourmethods,featuringthreepivotalconstituents: adatasetreconstruc-
tionpipeline,theID-Animatorframework,andtherandomreferencestrategyemployedduringthe
trainingprocessofID-Animator.
3.2 ID-Animator
AsdepictedatthebottomofFigure3,ourID-Animatorframeworkcomprisestwocomponents: the
backbonetext-to-videomodel,whichiscompatiblewithdiverseT2Vmodels,andthefaceadapter,
whichissubjecttotrainingforefficiency.
4Figure3: AnOverviewofOurProposedFramework: TheID-Animator; DatasetReconstruction
Pipeline,andRandomReferenceTraining.
PretrainedTexttoVideoDiffusionModelThepre-trainedtext-to-videodiffusionmodelexhibits
strong video generation prowess, yet it lacks efficacy in the realm of ID-specific human video
generation. Thus,ourobjectiveistoharnesstheexistingcapabilitiesoftheT2Vmodelandtailoritto
theID-specifichumanvideogenerationdomain. Specifically,weemployAnimateDiff[16]asour
foundationalT2Vmodel.
FaceAdapterTheadventofimagepromptinghassubstantiallybolsteredthegenerativeabilityof
diffusionmodels,particularlywhenthedesiredcontentischallengingtodescribepreciselyintext.
IP-Adapter[36]proposedanovelmethod,enablingimagepromptingcapabilitiesonparwithtext
prompts,withoutnecessitatinganymodificationtotheoriginaldiffusionmodel.Ourapproachmirrors
thedecouplingofimageandtextfeaturesincross-attention. Thisprocedurecanbemathematically
expressedas:
Z =Attention(Q,Kt,Vt)+λ·Attention(Q,Ki,Vi) (1)
new
whereQ,Kt,andVtdenotethequery,key,andvaluematricesfortextcross-attention,respectively,
whileKiandVicorrespondtoimagecross-attention. ProvidedthequeryfeaturesZ andtheimage
featuresc ,Q=ZW ,Ki =c Wi,andVi =c Wi. OnlyWi andWi aretrainableweights.
i q i k i v k v
InspiredbyIP-Adapter,welimitourmodificationstothecross-attentionlayerinthevideogeneration
model,leavingthetemporalattentionlayerunchangedtopreservetheoriginalgenerativecapacity
ofthemodel. Alightweightfaceadaptermoduleisdesigned, encompassingahandfulofsimple
query-basedimageencoderandthecross-attentionmodulewithtrainablecross-attentionprojection
weights,asshowninFigure3. Theimagefeaturec isderivedfromtheclipfeatureofthereference
i
image,andisfurtherrefinedbythequery-basedimageencoder. Theotherweightsincrossattention
5Figure4: ExamplesoftheoriginalCeleve-CaptionandourHumanAttributeCaption,HumanAction
CaptionandtheUnifiedHumanCaption.
moduleareinitializedfromtheoriginaldiffusionmodel,ofwhichtheprojectionweightsWi and
K
Wi areinitializedusingtheweightsoftheIP-Adapter,facilitatingtheacquisitionofpreliminary
V
imagepromptingcapabilitiesandreducingtheoveralltrainingcosts. Throughasimplifiedandrapid
trainingprocess,wecanattainavideogenerationmodelwithidentitypreservationcapability.
3.3 ID-OrientedHumanDatasetReconstruction
Contrarytoidentity-preservationimagegenerationtasks,videogenerationtaskscurrentlysufferfrom
alackofidentity-orienteddatasets. ThedatasetmostrelevanttoourworkistheCelebV-HQ[37]
dataset,comprising35,666videoclipsthatencompass15,653identitiesand83manuallylabeled
facialattributescoveringappearance,action,andemotion. However,theircaptionsarederivedfrom
manuallysettemplates,primarilyfocusingonfacialappearanceandhumanemotionwhileneglecting
thecomprehensiveenvironment,humanaction,anddetailedattributesofvideo. Additionally,its
stylesignificantlydeviatesfromtheuserinstructions,renderingitunsuitableforcontemporaryvideo
generationmodels,anditlacksfaciallabels,suchasmasksandboundingboxes. Consequently,we
finditnecessarytoreconstructthisdatasetintoanidentity-orientedhumandataset. Ourpipeline
incorporatescaptionrewritingandfacedetection,coupledwithcropping.
3.3.1 DecoupledHumanVideoCaptionGeneration
ToenhancetheinstructionfollowingabilityofID-Animator,wedesignacomprehensiverestructuring
ofthecaptionswithintheCelebV-HQdataset. Toproducehigh-qualityhumanvideos,itiscrucialfor
thecaptiontocomprehensivelyencapsulatethesemanticinformationandintricatedetailspresent
withinthevideo. Consequently,thecaptionmustincorporatedetailedattributesoftheindividualas
wellastheactionstheyareperforminginthevideo. Inlightofthis,weemployanovelrewriting
technique that decouples the caption into two distinct components: human attributes and human
actions. Subsequently,weleveragealanguagemodeltoamalgamatetheseelementsintoacohesive
andcomprehensivecaption,asillustratedatthetopofFigure3.
HumanAttributeCaptionAsapreliminarystep, wefocusoncraftinganattributecaptionthat
aimstovividlydepicttheindividual’spreferencesandthesurroundingcontext. Toachievethis,we
employtheShareGPT4V[2]modelforcaptiongeneration. Recognizedasaleadingtoolwithinthe
imagecaptioningdomain,ShareGPT4VistrainedusingadatasetgeneratedbyGPT4,enablingitto
providedetaileddescriptionsofimages. Wechoosethemedianframeofthevideoastheinputfor
ShareGPT4V.Thisapproachallowsustogeneratedetailedcharacterdescriptionsthatincorporatea
wealthofattributeinformation.
HumanActionCaptionOurobjectiveistocreatehumanvideoswithaccurateandrichmotions,
where a mere human attribute caption is insufficient for our needs. We require a caption that
emphasizestheoveralldynamismandactionsinherentinthevideo. Toaddressthisrequirement,we
introducetheconceptofahumanactioncaption,whichstrivestodepicttheactionpresentwithin
thevideo. Thesecaptionsarespecificallydesignedtoconcentrateonthesemanticcontentacrossthe
entirevideo,facilitatingacomprehensiveunderstandingoftheindividual’sactionscapturedtherein.
Toachievethisgoal,weleveragetheVideo-LLava[21]model,whichhasbeentrainedonvideodata
andexcelsatfocusingontheoveralldynamism. ByemployingVideo-LLava,weensurethatour
captionseffectivelyconveythedynamicnatureoftheactionstakingplaceinthevideo.
6UnifiedHumanCaptionThelimitationsofrelyingsolelyonhumanattributecaptionsandhuman
actioncaptionsaredemonstratedinFigure4. Humanattributecaptionfailstoencompasstheoverall
actionoftheindividual,whilehumanactioncaptionneglectsthedetailedcharacteristicsofthesubject.
Toaddressthis,wedesignedaunifiedhumancaptionthatamalgamatesthebenefitsofbothcaption
types,usingthiscomprehensivecaptiontotrainourmodel.
WeemployaLargeLanguageModeltofacilitatethisintegration, capitalizingonitscapacityfor
human-like expression and its prowess in generating high-quality captions. The GPT-3.5 API is
utilizedinthisprocess. AsdepictedinFigure4,therewrittencaptioneffectivelyencapsulatesthe
videoscene,aligningmorecloselywithhumaninstructions.
Figure4alsoillustratestheshortcomingsoftheCelebV-caption,whichdeviatesfromthehuman
instructiondistributionandevenincludesincorrectinformation(e.g.,ayoungboyiserroneously
annotated as a woman). Our method disentangles the video content into attributes and actions,
yieldingmorecomprehensiveresults.
3.3.2 RandomFaceExtractionforFacePoolConstruction
Incontrasttopreviousmethods[36,23,18],ourapproachdoesnotdirectlyutilizeaframefromthe
videoasareferenceimage. Instead,weopttoextractthefacialregionfromthevideo,usingthisas
theidentityreferenceimage. ThisstrategyeffectivelyreducestheinfluenceofID-irrelevantfeatures
onvideogeneration. Simultaneously,ourtechniquediffersfromtheimagereconstructiontraining
strategy employed in the ID preservation image generation works [29, 27, 20], which typically
reconstructsareferenceimageusingthesameimageascondition. Hence,weadoptamorestochastic
approachfortrainingwithrandomrizedfaceextraction.
AsdepictedatthebottomofFigure3,weemployshufflingonvideosqeuencesandextractfacial
regionfromfiverandomlyselectedframes. Ininstanceswhereaframecontainsmorethanoneface,
itisdiscardedandadditionalframesareselectedforre-extraction. Theextractedfacialimagesare
subsequently stored in the face pool. This stochastic approach of facial extraction enables us to
disentangleidentityinformationfromthesemanticcontentofthevideo.
3.4 RandomReferenceTrainingForDiminishingID-IrrelevantFeatures
Prior to presenting our approach, we begin by revising the training methods of existing identity
preservationimagegenerationmodels,therebyhighlightingthedistinctionsbetweenourproposed
methodandpreviousresearch.
InthetrainingphaseoftheDiffusionmodel,theobjectiveistoestimatethenoiseϵatthecurrent
timesteptfromanoisylatentrepresentationz . Thisnoisylatentz isderivedfromthecleanlatent
t t
zcombinedwiththenoisecomponentassociatedwiththecurrenttimestept,i.e.,z =f(z,t). This
t
optimizationprocedurecanbeexpressedbythefollowingfunction:
L=E [||ϵ−ϵ (z ,t)||2] (2)
zt,t,ϵN(0,1) θ t 2
Generally speaking, the clean latent z can be either the image itself or its embedding within the
featurespace. Specifically,withinthecontextofthelatentdiffusionmodel,z originatesfromthe
encodingofimageI obtainedviatheVAEencoder. Consequently,thisprocesscanbeviewedasthe
reconstructionofzfromagivennoisyencodingz . Incorporatingconditionssuchasatextcondition
t
C andanimageconditionC ,thisprocesscanbemathematicallyexpressedas:
i
L=E [||ϵ−ϵ (z ,t,C,C )||2] (3)
zt,t,C,Ci,ϵN(0,1) θ t i 2
Incurrentidentitypreservationimagegenerationmodels,theimageconditionC andthereconstruc-
i
tiontargetZ typicallyoriginatefromthesameimageI. Forinstance,Face0[27],InstantID[29],
andFaceStudio[33]utilizeimageI asthetargetlatentZ,withthefacialregionofI servingasC .
i
Conversely,PhotoMaker[20],Anydoor[3],andIP-AdapterdirectlyemploythefeatureofimageI as
C . Inthelearningphaseofimagereconstruction,thisapproachprovidesoverlystrongconditionsfor
i
thediffusionmodel,whichnotonlyconcentratesonfacialfeaturesbutalsoencompassesextraneous
featuressuchasthebackground,characters,andadornments(e.g.,hats,jewelry,glasses). Thismay
resultintheneglectofdomain-invariantidentityfeatures. Whendirectlyapplyingthistechniqueto
videos,weareessentiallyusingthefirstframeasaguidetorecreatethevideosequence. Thisstrong
conditioningcancausethemodeltodevolveintoanimage-to-videomodel,wherethevideocontent
7becomesheavilydependentonthesemanticinformationofthereferenceimage,ratherthanfocusing
onitsfacialembedding.
However,characteridentityshouldexhibitdomaininvariance,implyingthatgivenimagesofthesame
individualfromvariousanglesandattire,videogenerationoutcomesshouldbesimilar. Therefore,
drawinginspirationfromtheMonteCarloconcept,wedesignedarandomreferencetrainingmethod-
ology. This approach employs weakly correlated images with the current video sequence as the
conditionC ,effectivelydecouplingthegeneratedcontentfromthereferenceimages. Specifically,
j
duringtraining,werandomlyselectareferenceimagefromthepreviouslyextractedfacepool,as
depictedinFigure3. ByemployingthisMonteCarlotechnique,thefeaturesfromdiversereference
imagesareaveraged,reducingtheinfluenceofidentity-invariantfeatures. Thistransformationof
themappingfrom(C,C )− > Z to(C,C )− > Z notonlydiminishestheimpactofextraneous
i j
featuresbutalsobooststhemodel’scapacitytofollowuserinstructions.
Figure5: Thecomparisonbetweenourmethodsandpreviousmethodsonthreecelebritiesimages.
Figure6: Thecomparisonbetweenourmethodsandpreviousmethodsonthreeordinaryindividuals
images
4 Experiment
4.1 Implementationdetails
Weemployetheopen-sourceAnimateDiff[10]asourtext-to-videogenerationmodel. Ourtraining
datasetisprocessedbyclippingto16frames,centercropping,andresizingto512x512pixels. During
training, only the parameters of the face adapter are updated, while the pre-trained text-to-video
modelremainsfrozen. OurexperimentsarecarriedoutonasingleNVIDIAA100GPU(80GB)with
abatchsizeof2.WeloadthepretrainedweightsofIP-Adapterandsetthelearningrateto1e-4forour
trainableadapter. Furthermore,toenhancethegenerationperformanceusingclassifier-freeguidance,
8Figure7: Fromtoptobottom,ourmodelshowcasesitsabilitytorecontextualizevariouselementsin
anreferenceimage,includinghumanhair,clothing,background,actions,age,andgender.
weapplieda20%probabilityofutilizingnull-textembeddingstoreplacetheoriginalupdatedtext
embedding. WeutilizeasubsetoftheCelebVdatasetasourprimarydataset,comprising15kvideos,
andconstructouridentity-orienteddatasetbasedonthisfoundation. Followingthefilteringofvideos
containingmultiplefaces,thefinaldatasetemployedfortrainingcontains13kvideos.
4.2 QualitativeComparison
Weofferaqualitativecomparisonbetweenourapproachandthewell-knownmethodsinthedomain
of ID preserving image generation, specifically, the IP-Adapter-Plus-Face [34] and IP-Adapter-
FaceID-Portrait [35]. We choose three images of celebrities and three of ordinary individuals as
testcases,withtheimagesofthelatterbeingsourcedfromunuseddataintheCelebVdataset. We
randomlygeneratedsixpromptsfromLLM,maintainingconsistencywithhumanlanguagestyle,
thusallowingustoassessthemodel’sabilitytofollowinstructions.
As depicted in Figure 5, it is evident that our approach yields the most desirable outcomes. The
facegeneratedbyIP-Adapter-Plus-Facedemonstratesacertainlevelofdeformation,whereastheIP-
Adapter-FaceID-Portraitmodelisdeficientinfacialstructuralinformation,resultinginadiminished
similaritybetweenthegeneratedoutputsandthereferenceimage.
TheresultspresentedinFigure6furtherunderscorethesuperiorityofourapproach,showcasingthe
mostpronouncedmotion,thehighestfacialsimilarity,andthecapabilityofinstructionfollowing.
9Figure8: Thefigureillustratesourmodel’scapabilitytoblenddistinctidentitiesandcreateidentity-
specificvideos.
4.3 Application
Inthissection,Weshowcasethepotentialapplicationsofourmodel,encompassingrecontextualiza-
tion,alterationofageorgender,IDmixing,andintegrationwithControlNetorcommunitymodels[5]
togeneratehighlycustomizedvideos.
4.3.1 Recontextualization
Givenareferenceimage,ourmodeliscapableofgeneratingIDfidelityvideosandchangingcontextual
information. Thecontextualinformationofcharacterscanbetailoredthroughtext,encompassing
attributessuchasfeatures,hair,clothing,creatingnovelcharacterbackgrounds,andenablingthem
toexecutespecificactions. AsillustratedinFigure7,wesupplyreferenceimagesandtext,andthe
outcomesexhibittherobusteditingandinstruction-followingcapacitiesofourmodel.
Asdepictedinthefigure7,fromtoptobottom,weexhibitthemodel’sproficiencyinalteringcharacter
hair,clothes,background,executingparticularactions,andchangingageorgender.
4.3.2 IdentityMixing
ThepotentialofourmodeltoamalgamatedifferentIDsisshowcasedinthefigure8. Throughthe
blendingofembeddingsfromtwodistinctIDsinvaryingproportions,wehaveeffectivelycombined
featuresfrombothIDsinthegeneratedvideo. Thisexperimentsubstantiatestheproficiencyofour
faceadapterinlearningfacialrepresentations.
10Figure9: OurmodelcancombinewithControlNettogenerateID-specificvideos.
Figure10: Fromthetoptobottom,wevisualizetheinferenceresultswithLyrielandRaemuximodel
weights.
4.3.3 CombinationwithControlNet
Furthermore,ourmodeldemonstratesexcellentcompatibilitywithexistingfine-grainedcondition
modules,suchasControlNet[38]. WeoptedforSparseControlNet[9],trainedforAnimateDiff,as
anadditionalconditiontointegratewithourmodel. AsillustratedinFigure9,wecansupplyeither
singleframecontrolimagesormulti-framecontrolimages. Whenasingleframecontrolimageis
provided,thegeneratedresultadeptlyfusesthecontrolimagewiththefacereferenceimage. Incases
wheremultiplecontrolimagesarepresented,thegeneratedvideosequencecloselyadherestothe
sequence provided by the multiple images. This experiment highlights the robust generalization
capabilitiesofourmethod,whichcanbeseamlesslyintegratedwithexistingmodels.
4.3.4 InferencewithCommunityModels
We assessed the performance of our model using the Civitai community model, and our model
continues to function effectively with these weights, despite never having been trained on them.
TheselectedmodelsincludeLyrielandRaemumxi. AsdepictedinFigure10,thefirstrowpresents
theresultsobtainedwiththeLyrielmodel,whilethesecondrowshowcasestheoutcomesachieved
usingtheRaemuximodel. Ourmethodconsistentlyexhibitsreliablefacialpreservationandmotion
generationcapabilities.
5 Conclusion
Inthisresearch,ourprimarygoalistoachieveID-specificcontentgenerationintext-to-video(T2V)
models. Tothisend, weintroduceaID-AnimatorframeworktodriveT2Vmodelsingenerating
ID-specific human videos using ID images. We facilitate the training of our ID-Animator by
constructinganID-orienteddatasetbasedonpubliclyavailableresources,incorporatingdecoupled
captiongenerationandfacepoolconstruction.Moreover,wedeveloparandomfacereferencetraining
methodtominimizeID-irrelevantcontentinreferenceimages,therebydirectingtheadapter’sfocus
11towardsID-relatedfeatures. OurextensiveexperimentsdemonstratethatourID-Animatorgenerates
stablevideoswithsuperiorIDfidelitycomparedtopreviousmodels.
References
[1] A.Blattmann,T.Dockhorn,S.Kulal,D.Mendelevitch,M.Kilian,D.Lorenz,Y.Levi,Z.English,
V.Voleti,A.Letts,etal. Stablevideodiffusion: Scalinglatentvideodiffusionmodelstolarge
datasets. arXivpreprintarXiv:2311.15127,2023. 3,4
[2] L.Chen,J.Li,X.Dong,P.Zhang,C.He,J.Wang,F.Zhao,andD.Lin. Sharegpt4v: Improving
largemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,2023. 6
[3] X.Chen,L.Huang,Y.Liu,Y.Shen,D.Zhao,andH.Zhao. Anydoor: Zero-shotobject-level
imagecustomization. arXivpreprintarXiv:2307.09481,2023. 7
[4] M.Chu,Y.Xie,J.Mayer,L.Leal-Taixé,andN.Thuerey. Learningtemporalcoherencevia
self-supervisionforgan-basedvideogeneration. ACMTransactionsonGraphics(TOG),39(4):
75–1,2020. 3
[5] Civitai. Civitai. https://civitai.com/. Accessed: April21,2024. 3,10
[6] Z.Dai,Z.Zhang,Y.Yao,B.Qiu,S.Zhu,L.Qin,andW.Wang. Animateanything: Fine-grained
opendomainimageanimationwithmotionguidance. arXive-prints,pagesarXiv–2311,2023.
4
[7] R.Gal,Y.Alaluf,Y.Atzmon,O.Patashnik,A.H.Bermano,G.Chechik,andD.Cohen-Or. An
imageisworthoneword: Personalizingtext-to-imagegenerationusingtextualinversion. arXiv
preprintarXiv:2208.01618,2022. 2,4
[8] S.Ge,T.Hayes,H.Yang,X.Yin,G.Pang,D.Jacobs,J.-B.Huang,andD.Parikh. Longvideo
generationwithtime-agnosticvqganandtime-sensitivetransformer. InEuropeanConference
onComputerVision,pages102–118.Springer,2022. 3
[9] Y.Guo,C.Yang,A.Rao,M.Agrawala,D.Lin,andB.Dai. Sparsectrl: Addingsparsecontrols
totext-to-videodiffusionmodels. arXivpreprintarXiv:2311.16933,2023. 11
[10] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai. Animatediff: Animate
your personalized text-to-image diffusion models without specific tuning. arXiv preprint
arXiv:2307.04725,2023. 2,4,8
[11] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems,33:6840–6851,2020. 3
[12] J.Ho,W.Chan,C.Saharia,J.Whang,R.Gao,A.Gritsenko,D.P.Kingma,B.Poole,M.Norouzi,
D.J.Fleet,etal. Imagenvideo: Highdefinitionvideogenerationwithdiffusionmodels. arXiv
preprintarXiv:2210.02303,2022. 3
[13] J.Ho,T.Salimans,A.Gritsenko,W.Chan,M.Norouzi,andD.J.Fleet. Videodiffusionmodels.
AdvancesinNeuralInformationProcessingSystems,35:8633–8646,2022. 3
[14] F.-T.Hong,L.Zhang,L.Shen,andD.Xu. Depth-awaregenerativeadversarialnetworkfor
talkingheadvideogeneration. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages3397–3406,2022. 3
[15] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021. 2,4
[16] L. Hu, X. Gao, P. Zhang, K. Sun, B. Zhang, and L. Bo. Animate anyone: Consistent and
controllableimage-to-videosynthesisforcharacteranimation.arXivpreprintarXiv:2311.17117,
2023. 2,5
[17] Y.Hu,C.Luo,andZ.Chen. Makeitmove: controllableimage-to-videogenerationwithtext
descriptions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18219–18228,2022. 3
12[18] Y.Jiang,T.Wu,S.Yang,C.Si,D.Lin,Y.Qiao,C.C.Loy,andZ.Liu. Videobooth: Diffusion-
based video generation with image prompts. arXiv preprint arXiv:2312.00777, 2023. 2, 4,
7
[19] Y.Jiang,S.Yang,T.L.Koh,W.Wu,C.C.Loy,andZ.Liu. Text2performer: Text-drivenhuman
video generation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision,pages22747–22757,2023. 3
[20] Z.Li,M.Cao,X.Wang,Z.Qi,M.-M.Cheng,andY.Shan. Photomaker: Customizingrealistic
humanphotosviastackedidembedding. arXivpreprintarXiv:2312.04461,2023. 2,4,7
[21] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023. 6
[22] G. Liu, M. Xia, Y. Zhang, H. Chen, J. Xing, X. Wang, Y. Yang, and Y. Shan. Stylecrafter:
Enhancingstylizedtext-to-videogenerationwithstyleadapter.arXivpreprintarXiv:2312.00330,
2023. 2
[23] Z.Ma,D.Zhou,C.-H.Yeh,X.-S.Wang,X.Li,H.Yang,Z.Dong,K.Keutzer,andJ.Feng.
Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368,
2024. 2,4,7
[24] S.Qian,H.Chang,Y.Li,Z.Zhang,J.Jia,andH.Zhang. Strait: Non-autoregressivegeneration
withstratifiedimagetransformer. arXivpreprintarXiv:2303.00750,2023. 3
[25] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684–10695,2022. 3
[26] N.Ruiz,Y.Li,V.Jampani,Y.Pritch,M.Rubinstein,andK.Aberman. Dreambooth:Finetuning
text-to-imagediffusionmodelsforsubject-drivengeneration. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages22500–22510,2023. 4
[27] D.Valevski,D.Lumen,Y.Matias,andY.Leviathan. Face0: Instantaneouslyconditioninga
text-to-imagemodelonaface. InSIGGRAPHAsia2023ConferencePapers,pages1–10,2023.
2,4,7
[28] J. Wang, H. Yuan, D. Chen, Y. Zhang, X. Wang, and S. Zhang. Modelscope text-to-video
technicalreport. arXivpreprintarXiv:2308.06571,2023. 2,4
[29] Q. Wang, X. Bai, H. Wang, Z. Qin, and A. Chen. Instantid: Zero-shot identity-preserving
generationinseconds. arXivpreprintarXiv:2401.07519,2024. 2,4,7
[30] J.Z.Wu,Y.Ge,X.Wang,S.W.Lei,Y.Gu,Y.Shi,W.Hsu,Y.Shan,X.Qie,andM.Z.Shou.
Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages7623–7633,
2023. 2
[31] J. Xing, M. Xia, Y. Zhang, H. Chen, X. Wang, T.-T. Wong, and Y. Shan. Dynamicrafter:
Animatingopen-domainimageswithvideodiffusionpriors. arXivpreprintarXiv:2310.12190,
2023. 4
[32] Z.Xu,K.Wei,X.Yang,andC.Deng. Doyouguyswanttodance: Zero-shotcompositional
humandancegenerationwithmultiplepersons. arXivpreprintarXiv:2401.13363,2024. 2
[33] Y.Yan,C.Zhang,R.Wang,Y.Zhou,G.Zhang,P.Cheng,G.Yu,andB.Fu. Facestudio: Put
yourfaceeverywhereinseconds. arXivpreprintarXiv:2312.02663,2023. 2,7
[34] H.Ye. IP-AdapterPlusFace. https://huggingface.co/h94/IP-Adapter/blob/main/
models/ip-adapter-plus-face_sd15.bin,2024. Accessedon: 2024-04-19. 9
[35] H. Ye. IP-Adapter FaceID Portrait V11 SD15. https://huggingface.co/h94/
IP-Adapter-FaceID/blob/main/ip-adapter-faceid-portrait-v11_sd15.bin,
2024. Accessedon: 2024-04-19. 9
13[36] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023. 2,4,5,7
[37] J. Yu, H. Zhu, L. Jiang, C. C. Loy, W. Cai, and W. Wu. Celebv-text: A large-scale facial
text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages14805–14814,2023. 2,6
[38] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages
3836–3847,2023. 4,11
[39] Y.Zheng, H.Yang, T.Zhang, J.Bao, D.Chen, Y.Huang, L.Yuan, D.Chen, M.Zeng, and
F.Wen. Generalfacialrepresentationlearninginavisual-linguisticmanner. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages18697–18709,
2022. 2
14