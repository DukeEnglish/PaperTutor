Aligning LLM Agents by Learning Latent Preference
from User Edits
GeGao AlexeyTaymanov EduardoSalinas PaulMineiro DipendraMisra
♣∗ ♢∗ ♢ ♢ ♢
DepartmentofComputerScience,CornellUniversity MicrosoftResearchNewYork
♣ ♢
ggao@cs.cornell.edu {ataymano, edus, pmineiro, dimisra}@microsoft.com
Abstract
Westudyinteractivelearningoflanguageagentsbasedonusereditsmadetothe
agent’soutput. Inatypicalsettingsuchaswritingassistants,theuserinteractswith
alanguageagenttogeneratearesponsegivenacontext,andmayoptionallyedit
theagentresponsetopersonalizeitbasedontheirlatentpreference,inaddition
toimprovingthecorrectness. Theeditfeedbackisnaturallygenerated,makingit
asuitablecandidateforimprovingtheagent’salignmentwiththeuser’spreference,
andforreducingthecostofusereditsovertime. Weproposealearningframe-
work,PRELUDE,toconductPREferenceLearningfromUser’sDirectEditsby
inferringadescriptionoftheuser’slatentpreferencebasedonhistoriceditdata
andusingittodefineapromptpolicythatdrivesfutureresponsegeneration. This
avoidsfine-tuningtheagent,whichiscostly,challengingtoscalewiththenumber
ofusers,andmayevendegradeitsperformanceonothertasks. Furthermore,learn-
ingdescriptivepreferenceimprovesinterpretability,allowingtheusertoviewand
modifythelearnedpreference. However,userpreferencecanbecomplex,subtle,
and vary based on context, making it challenging to learn. To address this, we
proposeasimpleyeteffectivealgorithmnamedCIPHER(ConsolidatesInduced
PreferencesbasedonHistoricalEditswithRetrieval). CIPHERleveragesalarge
languagemodel(LLM)toinfertheuserpreferenceforagivencontextbasedon
useredits. Inthefuture,CIPHERretrievesinferredpreferencesfromthek-closest
contextsinthehistory,andformsanaggregatepreferenceforresponsegeneration.
Weintroducetwointeractiveenvironments–summarizationandemailwriting,for
evaluationusingaGPT-4simulateduser. Wecomparewithalgorithmsthatdirectly
retrieveusereditsbutdonotlearndescriptivepreference,andalgorithmsthatlearn
context-agnosticpreference. Onbothtasks,CIPHERoutperformsbaselines by
achievingthelowesteditdistancecost. Meanwhile,CIPHERhasalowercomputa-
tionalexpense,asusinglearnedpreferenceresultsinashorterpromptthandirectly
using user edits. Our further analysis reports that the user preference learned
byCIPHERshowssignificantsimilaritytothegroundtruthlatentpreference.1
1 Introduction
Language agents based on large language models (LLMs) have been developed for a variety of
applications(Dohmke,2022;Brynjolfssonetal.,2023),followingrecentbreakthroughsinimproving
LLMs(Achiametal.,2023;Ouyangetal.,2022b;Teametal.,2023). However,despitetheirimpres-
sivezero-shotperformance,LLMsstillneedtoadaptandpersonalizetoagivenuserandtask(Mysore
etal.,2023;Lietal.,2023). Inmanyapplications,anaturalfeedbackforLLM-basedagentsisuser
edits,whereauserqueriestheagentandeditstheagent’sresponsebeforetheirownfinaluse. In
∗Equalcontribution.
1Ourcodeanddataarepubliclyavailableathttps://github.com/gao-g/prelude.
Preprint.
4202
rpA
32
]LC.sc[
1v96251.4042:viXraInteractive Learning from User Edits
Round 1 Step 1: User (and x the world) provides a Article: {user-provided article} x t
t context t to the LLM agent. Please summarize the above article. <latexit sha1_base64="fOBpdrE9QlvaWNMd4DFWhYg2sCA=">AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm+4uLxZcaYw/rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY/CWz2JmRuQYch9Tok20s19X/eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i++9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4+Rf+TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ==</latexit>
<latexit sha1_base64="fOBpdrE9QlvaWNMd4DFWhYg2sCA=">AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm+4uLxZcaYw/rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY/CWz2JmRuQYch9Tok20s19X/eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i++9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4+Rf+TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ==</latexit>
<latexit sha1_base64="A0YNLl7TgtZmToQd/LF2/q+Bj78=">AAAB6HicdVDLSgNBEJyNrxhfUY9eBoPgaZmNRpNb0IvHBMwDkiXMTmaTMbMPZnqFsOQLvHhQxKuf5M2/cZKsoKIFDUVVN91dXiyFBkI+rNzK6tr6Rn6zsLW9s7tX3D9o6yhRjLdYJCPV9ajmUoS8BQIk78aK08CTvONNrud+554rLaLwFqYxdwM6CoUvGAUjNWFQLBG7QpzahYOJTRYwpFwhtSrBTqaUUIbGoPjeH0YsCXgITFKtew6JwU2pAsEknxX6ieYxZRM64j1DQxpw7aaLQ2f4xChD7EfKVAh4oX6fSGmg9TTwTGdAYax/e3PxL6+XgF91UxHGCfCQLRf5icQQ4fnXeCgUZyCnhlCmhLkVszFVlIHJpmBC+PoU/0/aZds5s8vN81L9Kosjj47QMTpFDrpEdXSDGqiFGOLoAT2hZ+vOerRerNdla87KZg7RD1hvnyQAjSo=</latexit>
Farming, a part of agriculture, involves growing crops and
rearing animals for food and raw materials. It began thousands
2 Step 2: LLM Agent generates a response o f y e a r s ago, likely in the Fertile Crescent, and led to the y
y t given the context x t . Neolithic Revolution as people transitioned from nomadic <latexit sha1_base64="ix6PfFjCx5pvhchPC5u71ke6Vac=">AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw+KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO/VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix+q3NxP/8nqJ9mtuxsM40Syki0V+IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC+PoU/U/aFds5tSs3Z+XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT/BsCevRerFeF61LVj5zAD9gvX0CtoqOFg==</latexit> t
<latexit sha1_base64="ix6PfFjCx5pvhchPC5u71ke6Vac=">AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw+KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO/VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix+q3NxP/8nqJ9mtuxsM40Syki0V+IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC+PoU/U/aFds5tSs3Z+XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT/BsCevRerFeF61LVj5zAD9gvX0CtoqOFg==</latexit> <latexit sha1_base64="fOBpdrE9QlvaWNMd4DFWhYg2sCA=">AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm+4uLxZcaYw/rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY/CWz2JmRuQYch9Tok20s19X/eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i++9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4+Rf+TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ==</latexit> hunting to settled farming. This allowed for a significant increase
in human population.
- Farming, as a part of agriculture, involves growing crops
mcM u inmi icn mu oim l sia zti t ez ive e 3 S t e p 3 : U y <latexit sha1_base64="xey+nIxyXLaQVpdktF5dXCISDcs=">AAAB63icdVBNSwMxEM3Wr1q/qh69BIvoaclWq+2t6MVjBVsL7VKyabYNTbJLkhXK0r/gxYMiXv1D3vw3ZtsVVPTBwOO9GWbmBTFn2iD04RSWlldW14rrpY3Nre2d8u5eR0eJIrRNIh6pboA15UzStmGG026sKBYBp3fB5Crz7+6p0iySt2YaU1/gkWQhI9hk0vR4YAblCnJryGucexC5aA5LqjXUqCPo5UoF5GgNyu/9YUQSQaUhHGvd81Bs/BQrwwins1I/0TTGZIJHtGepxIJqP53fOoNHVhnCMFK2pIFz9ftEioXWUxHYToHNWP/2MvEvr5eYsO6nTMaJoZIsFoUJhyaC2eNwyBQlhk8twUQxeyskY6wwMTaekg3h61P4P+lUXe/Urd6cVZqXeRxFcAAOwQnwwAVogmvQAm1AwBg8gCfw7Ajn0XlxXhetBSef2Qc/4Lx9Ahdkjkc=</latexit> s t0 e br ee fd oi rt es uth se in r ge s itp .onse y <latexit sha1_base64="ix6PfFjCx5pvhchPC5u71ke6Vac=">AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw+KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO/VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix+q3NxP/8nqJ9mtuxsM40Syki0V+IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC+PoU/U/aFds5tSs3Z+XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT/BsCevRerFeF61LVj5zAD9gvX0CtoqOFg==</latexit> t to c - C
-
u O Tr e l rt r asi iv g nca i sent ini tao it otn ,e
n
ld ea
a
an I st dd b i pna e egn g
o
i tm a pon la
e
tl t h h tr e re o
a
a Nu nr s sein a io tg n il oi d tf nho s ei r co df fo R fyo re oed v ma o a r l s un
n
td oa io g mr noa
a
w ,
d
l i ikm ce a hlyt ue nir n tia intl hs ge.
t
F oe rtile y <latexit sha1_base64="xey+nIxyXLaQVpdktF5dXCISDcs=">AAAB63icdVBNSwMxEM3Wr1q/qh69BIvoaclWq+2t6MVjBVsL7VKyabYNTbJLkhXK0r/gxYMiXv1D3vw3ZtsVVPTBwOO9GWbmBTFn2iD04RSWlldW14rrpY3Nre2d8u5eR0eJIrRNIh6pboA15UzStmGG026sKBYBp3fB5Crz7+6p0iySt2YaU1/gkWQhI9hk0vR4YAblCnJryGucexC5aA5LqjXUqCPo5UoF5GgNyu/9YUQSQaUhHGvd81Bs/BQrwwins1I/0TTGZIJHtGepxIJqP53fOoNHVhnCMFK2pIFz9ftEioXWUxHYToHNWP/2MvEvr5eYsO6nTMaJoZIsFoUJhyaC2eNwyBQlhk8twUQxeyskY6wwMTaekg3h61P4P+lUXe/Urd6cVZqXeRxFcAAOwQnwwAVogmvQAm1AwBg8gCfw7Ajn0XlxXhetBSef2Qc/4Lx9Ahdkjkc=</latexit> t0
T settled farming. resulted in significant human population increase
<latexit sha1_base64="YuFQWylcNpMQgh4JYusIx/EE/f0=">AAACHHicdZBNSyNBEIZ7/Dbrulk9emkMC+Jh6Inr10EQ9+JRwaiQiUNPp2Iau3uG7prFOOSHePGvePHgIl48CP4bOzGCLrsvNLw8VUV1vWmupEPGXoKx8YnJqemZ2cqXua/z36rfF45dVlgBDZGpzJ6m3IGSBhooUcFpboHrVMFJevFrUD/5DdbJzBxhL4eW5udGdqTg6FFSXYt1ml2WWhqp5RX0aRxX4q7LuYDVkoWbQnvkCp2UuBP1z46oSDCp1li4zqLtjYiykA3lTX2dbW8xGo1IjYx0kFSf4nYmCg0GheLONSOWY6vkFqVQ0K/EhQO/8YKfQ9NbwzW4Vjk8rk9/eNKmncz6Z5AO6ceJkmvnejr1nZpj1/1dG8B/1ZoFdrZapTR5gWDE26JOoShmdJAUbUsLAlXPGy6s9H+losstF+jzrPgQ3i+l/zfH9TBaC+uHP2u7e6M4ZsgSWSYrJCKbZJfskwPSIIJck1tyT/4EN8Fd8BA8vrWOBaOZRfJJwfMrcLGhkg==</latexit>
Xt=1ct
Agent incurs a cost c
<latexit sha1_base64="CTH5mNcE52uui2J/dLNhhi0z8zI=">AAACEHicdVDJSgNBEO1xN25Rj14ag6ggoSeuOQhBPXiMYFRIwtDTqWhjz0J3jTgM+QQv/ooXD4p49ejNv7GzCCr6oODxXhVV9fxYSYOMfThDwyOjY+MTk7mp6ZnZufz8wpmJEi2gJiIV6QufG1AyhBpKVHARa+CBr+Dcvz7s+uc3oI2MwlNMY2gG/DKUbSk4WsnLrwoP6T5tHIFC7mUNhFvUQQYtiZ3OWurhBk1XPVz38gVW3GZuecelrMh6sKS0zcp7jLoDpUAGqHr590YrEkkAIQrFjam7LMZmxjVKoaCTayQGYi6u+SXULQ15AKaZ9R7q0BWrtGg70rZCpD31+0TGA2PSwLedAccr89vrin959QTbe81MhnGCEIr+onaiKEa0mw5tSQ0CVWoJF1raW6m44poLtBnmbAhfn9L/yVmp6G4WSydbhcrBII4JskSWyRpxyS6pkGNSJTUiyB15IE/k2bl3Hp0X57XfOuQMZhbJDzhvnyjGnK0=</latexit>
t=  edit(y t,y t0)
Figure1: Illustrationofinteractivelearningfromuseredits. Colorcodingineditsisforvisualization
only–ouragenttakestheplainrevisedtextasfeedback.
contrast,typicalfeedbackusedforfine-tuning,suchasthecomparison-basedpreferencefeedback
inRLHF,isexplicitlycollectedbyprovidingannotatorswithmodelresponsesandaskingthemto
rank(Ziegleretal.,2019;Stiennonetal.,2020;Nakanoetal.,2021;Ouyangetal.,2022a,interalia),
makingsuchfeedbackanexpensivechoiceforimprovingalignment. Motivatedbythisobservation,
wefocusoninteractivelearningofLLM-basedlanguageagentsusingusereditsasfeedback.
ConsiderthescenarioinFigure1whereauserinteractswithanLLM-basedwritingassistant(agent)
tocompletetheirtask. Theinteractionstartswiththeuser(andtheworld)providingacontextto
the agent. This context may include a query prompt provided by the user, along with additional
informationprovidedbytheworld,suchasthecontentonthescreen,currenttime,andtheuser’s
calendarinformation. Theagentgeneratesatextualresponsetotheusergiventhecontext.
Inthebeginning,theagent’sresponsemaynotbeoptimalfortheuser,asitisnotpersonalizedtothis
user’sindividualneedsandpreference. Asmostusersarenotfamiliarwithpromptengineering,and
LLMsareoftenabletogenerateanacceptableresponseforthetask,therefore,usersmayfinditthe
mostconvenienttosimplyedittheresponsewhenitisnotidealtosuittheirneeds,ratherthantrying
differentpromptstogetnewresponses. TheexampleinFigure1illustratesthattheuserdirectlyedits
thesummarygeneratedbytheagenttosatisfytheirpreferenceonbulletpointformat. Ittakestime
andeffortsfortheusertomakeedits.Wecanmeasuresuchcostusingavarietyofmetrics,suchasthe
editdistancebetweentheagent-generatedresponseandtheuser-revisedtext. Zeroeditfromtheuser
isalsoausefulfeedback,reflectingthattheagent’sresponsesatisfiesthisuser’sneeds. Oneimportant
featureofoursettingisthateverynaturaluseoftheagentyieldsaneditfeedbackforlearning. Since
there is no distinction between training and testing in this setting, we care about minimizing the
user’seffortsacrossallroundsofinteractionwiththeagent. Insummary,ourgoalistolearnfrom
theimplicitfeedbackinuseredithistorytominimizethecumulativecostoftheuser’sefforts.
We conjecture that user edits are driven by user’s hidden preference which can be described in
naturallanguage. Thesepreferencedescriptionsaredifferentfromthenotionofcomparison-based
preferenceusedinRLHF.Inthispaper,weusethewordpreferencetomeanpreferencedescriptions.
Forinstance,preferenceoftheuserinFigure1canbedescribedasbulletpoints. Inpractice,user
preferencecanbecompound,suchaspreferringbulletpoint,informal,withemojisatthesametime,
and also context-dependent, e.g., informal tone when writing an email to a family member, and
formaltonewhenwritingtoacolleague. Inmorecomplexsettings,userpreferencecanevolvewith
time(non-stationary),ordependoninformationunavailableinthecontext(partiallyobserved). Such
userpreferencemaynotbefullyderivablefromthecontext,andtheusermaynotevenbefullyaware
ofalltheirpreference. Theseconsiderationsimplythatuserpreferenceislatenttothelanguageagent.
Iftheagentcouldlearnthelatentpreferencecorrectly,itcansignificantlyimproveitsperformance
bygeneratingsatisfactoryresponsesaccordingly. Furthermore,preferencelearnedbytheagentcan
beshowntotheusertoenhanceinterpretability,andcanevenbemodifiedbytheusertoimprove
correctness. Motivatedbythis,weproposealearningframework,PRELUDE(PREferenceLearning
fromUser’sDirectEdits),whereweseektolearnatextualdescriptionoftheuserpreferencefora
givencontextusingthehistoryofuseredits.
2Inatypicalreal-worldscenariosuchaswritingassistants,onehastopotentiallyupdatetheLLM-
basedagentforeveryuser. Efficientapproaches,therefore,mustscalewiththenumberofusers. This
makesapproachesthatperformafullfine-tuningoftheLLMusedbytheagentveryhardtoscale.
Furthermore,LLMstypicallyundergoevaluationonavarietyofmetricsbeforebeingreleased,and
thusfine-tuningthemoftenresultsinbreakingthegeneralizationguaranteesofferedbythesetests.
Forexample,fine-tuningGPT-4formillionsofuserscanquicklyturnveryexpensive. Approaches
suchasaddingLORAandAdapterlayersandonlyupdatingthem,orusingfederatedlearning,can
reducetheexpensetosomeextent,whilethelossofgeneralizablealignmentremainsasaconcern. In
thiswork,wefocusonleveragingafrozen,black-boxLLM,andinsteadlearningapromptpolicy
thatcaninfertextualdescriptionofuser’spreferenceforagivencontext,andthenuseittodirectly
drivetheresponsegeneration.
WeintroduceasimpleyeteffectivealgorithmCIPHER(ConsolidatesInducedPreferencesbasedon
HistoricalEditswithRetrieval)underthePRELUDEframework. Foragivencontext,CIPHERfirst
retrievesthek-closestcontextsfromhistory,andaggregatesinferredpreferencesforthesekcontexts.
Itreliesonthisaggregatepreferencetogeneratearesponseforthegivencontext. Iftheuserperforms
no edits, then it saves this aggregate preference as the correct preference for the given context.
Otherwise,itqueriestheLLMtoinferaplausiblepreferencethatexplainstheseusereditsmadetothe
agentresponse,andsavesthisinferredpreferenceasthecorrectpreferenceforthegivencontext. A
keyadvantageofCIPHERisthatittypicallyleadstosignificantlyshorterpromptscomparedtoother
retrievalmethodsthatusetheentiredocumentsorcontext,asinferredpreferencesaremuchshorter
thanretrieveddocumentsorcontexts. Thisresultsinasignificantreductioninthecomputational
expenseofqueryingtheLLM.
Weintroducetwointeractiveenvironmentsforevaluation,inspiredbywritingassistantapplications.
Inthefirstenvironment, weevaluatetheagent’sabilitytosummarizeagivendocument(articles
fromdifferentsources). Inthesecondenvironment,weevaluatetheagent’sabilitytocomposean
emailusingcontentfromagivendocument(notesforvariouspurpose). Inbothtasks,wesimulatea
GPT-4userthatcangenerateeditsbasedonapre-designedlatentpreference. Weusedocumentsfrom
severalexistingdomainsasouruser-providedcontext,andvarytheGPT-4user’spreferencebasedon
thedomain,inordertocapturethereal-worldcontext-dependentnatureofhumanuser’spreference.
WeevaluateCIPHERagainstseveralbaselines, includingapproachesthatlearncontext-agnostic
userpreferences,andretrieval-basedapproachesthatdonotlearnpreferencesbutdirectlyusepast
usereditsforgeneration. Weshowthatforbothtasks,CIPHERachievesthelowestusereditcost
comparedtobaselines,andsignificantlyreducesthecumulativecostcomparedtousingthefrozen
base agent. Additionally, CIPHER results in a lower LLM query cost than other retrieval-based
baselines. Finally,wequalitativelyandquantitativelyanalyzepreferenceslearnedbyouragents,and
findthattheyshowsignificantsimilaritytothegroundtruthlatentpreferencesinoursetup.
2 InteractiveLearningfromUserEditsandthePRELUDEFramework
WefirstdescribeLLMagentsandthegenerallearningframeworkfromuseredits. Wethendescribe
ourspecializedPRELUDEframeworkforlearningdescriptiveuserpreference,anddiscussassociated
learningchallenges.
LLMandLanguageAgents. Weassumeaccesstoalanguageagentthatinternallyreliesonan
LLM.Wemakenoassumptionaboutthelanguageagentexceptthatitcantakeinputx asapieceof
t
contentandanadditionalprompt(whichcanbein-contextlearningexamplesorlearnedpreferences)
andgeneratearesponsey . ThelanguageagentmaysimplyperformgreedydecodingontheLLM,
t
ormayperformcomplexplanningusingthegivenLLMtogeneratearesponse.
Protocol1InteractiveLearningfromUserEdits.
1: fort=1,2,··· ,T do
2: Userandtheworldprovideacontextx
t
3: Agentgeneratesaresponsey giventhecontextx
t t
4: Usereditstheresponsetoy
t′
5: Agentreceivesacostofc =∆ (y ,y )
t edit t t′
6: Evaluatetheagentandlearningalgorithmon T c
t=1 t
(cid:80)
3InteractiveLearningfromUserEdits. Inanapplicationsuchasawritingassistant,auserinteracts
withthelanguageagentoverT rounds. Protocol1showssuchlearningprotocol. Inthetth round,
theuserandtheworldprovideacontextx ∈X whereX isthespaceofallpossiblecontexts. This
t
contextwillincludetheuserpromptintext,alongwithadditionalinformationprovidedbytheuser
ortheworld,andmayincludemultimodaldataaswellsuchasimages. Giventhecontextx ,the
t
languageagentgeneratesaresponsey ∈Y intext,whereY isthespaceofalltexts. Theuseredits
t
theresponsey toy . Iftheuserdoesnotperformanyedits,wetreatthisassettingy = y . The
t t′ t′ t
agentreceivesacostofc =∆ (y ,y )forthisround,whichmeasurestheuser’seffortsonmaking
t edit t t′
edits. Thegoaloftheagentistominimizethesumofcostsacrossallrounds T c .
t=1 t
Inourexperiments,weuse∆ asLevenshteineditdistance(Levenshtein,1965)inthetokenspace
edit (cid:80)
whichcomputestheminimumnumberoftotaltokenaddition,tokendeletion,andtokensubstitution
necessary to convert y to y . In general, a higher edit distance implies that the user has made
t t′
moreeditsandspentmoreefforts. Wenotethatourframeworkisgeneralenoughtoaccommodate
situationswheretheusertriesdifferentpromptswiththesamedemand. Wetreateachcalltothe
languageagentasadifferentroundwithadifferentcontext(asthecontextincludestheuserprompt).
PRELUDEFramework. WedescribeourPRELUDEframeworkinProtocol2whichisaspecial-
izationofthegenerallearningsetupdescribedaboveinProtocol1. InPRELUDE,inthetthround,
theagentinfersthepreferenceoftheuserasf ,andusesittogeneratearesponse. Weassumethat
t
inthisroundandforthegivencontextx ,theuserhasalatentpreferencef⋆thatdrivestheuserto
t t
performalledits. Furthermore,weassumethatiftheagentwasabletoinferthislatentpreference
(f =f⋆),thenitwillleadtominimalpossibleedits.2 Toremovethedependenceonperformance
t t
duetothechoiceofthebaseLLMagent,wecomparewithanoracleagentthathasaccesstof⋆at
t
thestartofeachround. WeassumethattheLLMremainsfrozenacrossallmethodsinthiswork.
Protocol2PRELUDE:PREferenceLearningfromUser’sDirectEdits
1: fort=1,2,··· ,T do
2: Userpresentsatextcontextx
t
3: Agentinfersapreferencef tusingthehistory{(x ℓ,y ℓ,y ℓ′)}t ℓ− =1 1andcontextx t
4: Agentusesf andx togeneratearesponsey
t t t
5: Usereditstheresponsetoy usingtheirlatentpreferencef⋆
t′ t
6: Agentincursacostc =∆(y ,y )
t t t′
7: Return T c
t=1 t
(cid:80)
ChallengesofLearningUserPreference. Learninguserpreferencefromeditsischallenging. In
practice,userpreferencearemultifacetedandcomplex.Furthermore,user’spreferencecanalsosignif-
icantlyvarybasedonthecontext.Thefeedbackintheformofusereditsemergesnaturallybutisinher-
entlyimplicit,lackingdirectexpressionsoftheactualpreferenceandcarryingsubtletiesthatmaylead
todiverseinterpretations. Thecombinationofpreferencevariabilityandtheimplicitnatureoffeed-
backposesconsiderablechallengesforagentsinaccuratelylearningandintegratingthesepreferences.
3 LearningUserPreferencethroughRetrievalandAggregation
In this section, we present our method, CIPHER (Consolidates Induced Preferences based on
HistoricalEditswithRetrieval),thatlearnsuserpreferencebasedonuseredits.
Algorithm1showsCIPHERwhichimplementsthePRELUDEframework. CIPHERmaintainsa
preferencehistoryD
t
={(x ℓ,f˜ ℓ)} ℓt −=1 1ofpastcontextsx ℓalongwithapreferencef˜ ℓinferredbythe
agent. CIPHERassumesaccesstoacontextrepresentationfunctionϕ : X → Rd thatcanmapa
contexttoavectorrepresentation. Foragivenroundtwithcontextx ,theagentfirstretrievesthe
t
k-closestcontextsfromtheinteractionhistoryD . Weusecosinesimilarityforcomputingproximity,
t
althoughothermetricssuchasEuclideandistance,orHammingdistancewhenϕoutputsabinary
vector,canbeused. Giventheretrievedcontextsandtheirinferredpreferences{(x ,f˜ )}k ,we
zi zi i=1
2Theeditcostinpracticemaynotalwaysbe0,asthelanguageagentcouldbeincapableofadeptlyusingthe
correctpreference,ortheusermayperformeditsthatareinconsistentwiththeirpreference.
4querytheunderlyingLLMtosummarizetheinferredpreferences{f˜ }k intoasinglepreference
zi i=1
f . Inthebeginning,whent≤k,weretrieveallthepasttcontexts. Inparticular,fort=1wehave
t
f asanemptystringastheagenthasnopriorknowledgeofthisuser’spreference.3
1
Theagentusestheinferredpreferencef togeneratetheresponse. Thisisdonebyconcatenatingthe
t
contextx withanagentpromptsuchas“Thisuserhasapreferenceof<f >whichmustbeused
t t
whengeneratingtheresponse”,where<f >indicateswhereweinserttheinferredpreferencef . We
t t
listtheactualtemplateusedinourexperimentsinTable7inAppendixA.
Giventheusereditsy ,iftheusereditsareminimal,i.e.,∆ (y ,y )≤δforahyperparameterδ,
t′ edit t t′
thenwesettheinferredpreferenceforthisroundasf˜ = f asusingf forgeneratingaresponse
t t t
resulted in minimal edits. However, if ∆ (y ,y ) > δ, then we query the LLM a third time to
edit t t′
generatetheinferredpreferencef˜ thatexplainswhytheusereditedy toy . WecallthistheLatent
t t t′
PreferenceInduction(LPI)step. Inbothcases,weappend(x ,f )tothepreferencehistory.
t t
NotethatwecannotquerytheLLMfortheinferredpreferenceinthefirstcasewheretheuseredit
costc issmall,i.e.,c ≤δ. Inthiscase,queryingtheLLMtoinferthepreferencetoexplaintheedits
t t
iny giveny ,willresultintheLLMoutputtingthattheagenthasnopreference. Thisisincorrectas
t′ t
itmerelyshowsthatthepreferencef usedtogeneratey wassufficientlygoodtoincludemostof
t t
thetrueuserpreferencef⋆.
t
ComputationalCostofCIPHER. Inagivenround,CIPHERaddsamaximumof3LLMcallson
topofthecostofcallingtheunderlyinginferencealgorithmoftheagentinline6. CIPHERfurther
reducesthememorystoragebyonlystoringtherepresentationofcontextsinthepreferencestring
instead of the input itself. Finally, CIPHER only adds a small prompt to the context x , before
t
callingtheagent’sinferencealgorithm. Thisonlyslightlyincreasesthelengthoftheprompt,thereby,
reducingthequerycostassociatedwithLLMsthatscaleswiththenumberofinputtokens.
Algorithm 1 CIPHER(ϕ,k,δ). A context representation function ϕ : X → Rd, the retrieval
hyperparameterk,andtolerancehyperparameterδ ≥0.
1: D =∅
2: fort=1,2,··· ,T do
3: User(andtheworld)presentsacontextx
t
4: Retrievethetop-kexamples{ϕ(x ),f˜ }k inDwithmaximumcosinesimilaritytoϕ(x )
zi zi i=1 t
5: Ifk >1,thenquerytheLLMtoaggregatethesepreferences{f˜ }k intof ,elsef =f˜
zi i=1 t t z1
6: Agentgeneratesatextresponsey basedonx andf
t t t
7: Usereditstheresponsetoy usingtheirlatentpreferencef⋆
t′ t
8: Agentincursacostc =∆ (y ,y )
t edit t t′
9: ifc ≤δthen
t
10: f˜ =f
t t
11: else
12: QuerytheLLMtogenerateapreferencef˜ thatbestexplainsusereditsin(y ,y )
t t t′
13: D ←D∪{(ϕ(x ),f˜)}
t t
14: Return T c
t=1 t
(cid:80)
4 Experiment
In this section, we first introduce two interactive tasks for evaluating agents that learn from user
edits. ThesetaskscanbeusedmorebroadlyevenoutsidethePRELUDEframework,andcanbeof
independentinterest. WethendescribeourbaselinesandprovideimplementationdetailsofCIPHER.
Finally, we provide quantitative results in terms of user edit cost and qualitative analysis of the
learnedpreferences.
3Inpractice,onecaninitializewithapubliclyavailablepreferencehistory.
5Table1: Latentuserpreferencedesign,specifictothedocumentsource.
DocSource LatentUserPreference Scenario
Summarization
Newsarticle targeted to young children, storytelling, short introduceapoliticalnewstokids
(Seeetal.,2017) sentences,playfullanguage,interactive,positive
Redditpost secondpersonnarrative,brief,showemotions, forcharacterdevelopmentincre-
(Stiennonetal.,2020) invokepersonalreflection,immersive ativewriting
Wikipediapage bulletpoints,parallelstructure,brief takenotesforkeyknowledge
(Foundation,2022)
Paperabstract tweetstyle,simpleEnglish,inquisitive,skillful promoteapapertoinvokemore
(Clementetal.,2019) foreshadowing,withemojis attentionandinterests
Moviereview questionansweringstyle,direct,concise quicklygetmainopinions
(Maasetal.,2011)
EmailWriting
Personalproblem informal,conversational,short,noclosing sharelifewithfriends
(Stiennonetal.,2020)
Paperreview casualtone,positive,clear,calltoaction peerreviewtocolleague
(Huaetal.,2019)
Papertweet engaging,personalized,professionaltone,thank- networkingemailsforresearchers
(Bar,2022) fulclosing
Papersummary structured,straighttothepoints,respectful,pro- milestonereporttosuperiors
(Kershaw & Koeling, fessionalgreetingandclosing
2020)
4.1 TwoInteractiveWritingAssistantEnvironmentsforLearningfromUserEdits
Task. WeintroducetwotasksinspiredbytheuseofLLMsaswritingassistants(Mysoreetal.,2023;
Shenetal.,2023;Wangetal.,2023). Inthefirsttask,weevaluatetheagent’sabilitytosummarizea
givendocument.Weusedocumentsfrom5existingsourceslistedinTable1.4 Thesesourcesrepresent
adiversecategoryofdocumentsthatawritingassistantwouldtypicallyencounter,includingnews
articlesthatareformalandconcise,moviereviewsthatareinformal,andpaperabstractsthataretech-
nical.Inthesecondtask,weevaluatetheagent’sabilitytocomposeanemailgivennotes.Forthistask,
weusenotesfromfourdifferentsourcesincludingavarietyoftaskssuchaswritingemailstofriends,
describingreportstomanagers,andwritingreviewsforcolleagues.Inanygivenround,theuserispro-
videdacontextthatisadocumentfromoneofthedocumentsourcesforthegiventask. Importantly,
theagentisunawareofthesourceofthegivendocumentwhichaswediscusslater,willdetermine
theuserpreference. Forbothtasks,werunanexperimentforT =200rounds,withanequalnumber
ofrandomlysampleddocumentsfromeachdocumentsource. Wemixdocumentsfromdifferent
sourcesandshufflethemtoremoveanytemporalcorrelationindocumentsourceacrossrounds.
Two-StageGPT-4SimulatedUser. Wesimulateauserthatcaneditagivenresponse. Wedefinea
setoflatentuserpreferencesfortheuserthatvarybasedonthedocumentsource.Table1liststhe
preferenceandthecorrespondingdocumentsource. Thiscapturesthecontext-dependentnatureof
userpreferencesasthedocumentsourceinfluencesthetypeofcontext. Forexample,thePersonal
problemdocumentsourcecontainsdocumentspertainingtodiscussionswithafriend,andauser
mayhaveadifferentpreferencewhenwritinganemailtoafriendcomparedtowritinganemailtoa
colleague. Inreal-worldsettings,thecontextdependenceoftheuserpreferencecanbemorecomplex
thanjustthedocumentsource. Weassumethatouruserisawareofthedocumentsourced ofagiven
t
contextx . Thisimplies,thatwecanexpressthetrueuserpreferenceforx asf⋆ =F(d )whereF
t t t t
mapsagivendocumentsourcetotheuserpreference. Recallthattheagentinourlearningsetupis
neverprovidedthedocumentsourceofanycontext.
WemodelouruserusingGPT-4withatwo-stageapproach.Givenanagentresponsey andthecontext
t
x ,wefirstqueryGPT-4tocheckifthegivenresponsesatisfiesthepreferenceinf⋆. Iftheanswer
t t
isyes,thentheuserpreformsnoeditsandreturnsy =y . Iftheanswerisno,thenweuseGPT-4
t′ t
togeneratetheeditedresponsey giveny andf⋆. WeusepromptingtoconditionGPT-4onthese
t′ t t
latentpreferences. WeprovideexamplesofeditsmadebyourGPT-4userinTable5inAppendixA.
4Table4inAppendixprovideslinkstoeachsourcedataset,usedasuser-providedcontextinourtasks.
6Wefoundthatourtwo-stageGPT-4usercangeneratehigh-qualityedits,consistentwithobservations
inpriorworkthatLLM-writtenfeedbackishigh-qualityandusefultolearnfrom(Baietal.,2022;
Saundersetal.,2022). Weadoptedatwo-stageprocesssincewefoundthatusingGPT-4todirectly
edittheresponsey alwaysresultedineditsevenwhentheresponsesatisfiedthepreferencef⋆. We
t t
evaluatedseveraldifferentpromptsformodelingourtwo-stageGPT-4useruntilwefoundaprompt
suchthatanoracleGPT-4agentwithaccesstof⋆achievesaminimalusercost.
t
EvaluationMetric. Weproposethreemetricsforevaluatingagentslearningfromuseredits. Our
mainmetricisthecumulativeusereditcost T c overT rounds. Inanygivenround,wecompute
t=1 t
theusereditcostc =∆ (y ,y )usingLevenshteineditdistancebetweenagentresponsey and
t edit t t′ t
usereditsy . Tocomputetheeditdistance,(cid:80)weperformBPEtokenizationusingTiktokentokenizer,
t′
and compute the edit distance in the token space. In general, one can learn a metric that better
capturesthecognitiveloadassociatedwithauseredit. However,Levenshteineditdistanceprovidesa
clean,transparentmetricthatiseasytointerpret. Additionally,itdoesn’thaveconcernssharedby
learnedmetricssuchaserroneousevaluationswhenapplyingthemetrictoexamplesnotcoveredby
themetric’strainingdistribution.
For CIPHER and any other method in the PRELUDE framework, we additionally evaluate the
accuracyoftheinferreduserpreferencef usedtogeneratetheresponsey . Formally,givenacontext
t t
x containingadocumentfromsourced ,weevaluateiftheinferredpreferencef isclosertothe
t t t
truepreferencef⋆ =F(d )thanpreferenceF(d)ofanyotherdocumentsourced̸=d . Lettherebe
t t t
N documentsourcesforagiventaskandweindexd∈{1,2,··· ,N}. Thenwecomputethismetric
as 1 T 1{d =argmax BERTScore(f ,F(d))},whereBERTScore(Zhang*etal.,2020)
T t=1 t d [N] t
isapopulartextsimilarityme∈ tric.5
(cid:80)
Finally,weevaluatethetokenexpenseassociatedwithqueryingtheLLMacrossallmethods. We
computethetotalnumberoftokensbothgeneratedbyorprovidedasinputtotheLLMacrossall
rounds. ThisisatypicalmetricusedbypopularLLMproviderstochargetheircustomers.
4.2 DetailsofCIPHERandComparisonSystems
WeuseGPT-4asourbaseLLMforCIPHERandallbaselines. Wedonotperformfine-tuningofthe
GPT-4anddonotaddanyadditionalparameterstothemodel. Weuseaprompt-basedGPT-4agent
forallmethodsthatusesasinglepromptwithgreedydecodingtogeneratetheresponse. Ourmain
methodCIPHERandthebaselines,canbeextendedtomorecomplexlanguageagentsthatperform
multiplestepsofreasoningontopofthebaseLLMbeforegeneratingaresponse.
CIPHERDetails. WeuseasimpleagentthatusesGPT-4withaprompttemplatetogeneratethe
responsey giventhecontextx andpreferencef . WelisttemplatesinTable7inAppendixA. We
t t t
experiment with MPNET (Song et al., 2020) and BERT (Devlin et al., 2019) as our two context
representationfunctionsϕ,andusecosinesimilarityforretrieval. Weexperimentwithtwodifferent
valuesofthenumberofretrievedexamplesk ∈{1,5}.
Baselines. WeevaluateCIPHERagainstbaselinesthateitherperformnolearning,orlearncontext-
agnosticpreferencesandagainstmethodsthatdonotlearnpreferencesbutdirectlyusepastuseredits
forgeneratingaresponse.
1. Nolearning: Theagentperformsnolearningbasedoninteractionwiththeuser. Ineach
step,theagentgeneratesaresponsey giventhecontextx .
t t
2. Explore-then-exploit (E-then-e) LPI: This baseline is based on the classic explore-then-
exploit strategy in interactive learning (Garivier et al., 2016). The agent first generates
responsesforthefirstT roundswithoutperforminganylearning(explorationstage). It
e
theninfersasingleuserpreferencef˜ usingtheusereditsinthefirstT roundsusingthe
e e
LPIstepsimilartoline12inCIPHER(Algorithm1). Itthenusesthelearnedpreferenceto
generatetheresponseforallremainingrounds(exploitationstep).
3. Continual LPI: This method is similar to explore-then-exploit except that it never stops
exploring. Inanygivenroundt, itusesthedataofallpastedits{(y i,y i′)}t i=−11 tolearna
5Weusethemicrosoft/deberta-xlarge-mnlitoimplementBERTScore.
7preferencef byperformingtheLPIstep. Itthengeneratesaresponseusingthispreference.
t
Incontrast,toexplore-then-exploitapproach,ContinualLPIcanavoidoverfittingtothefirst
T rounds,butbothapproacheslearnpreferencesthatareindependentofx .
e t
4. ICL-edit: Thisisastandardretrieval-basedin-contextlearning(ICL)baseline(Brownetal.,
2020). Inagivenroundt,theagentfirstretrievestheclosestk examples{(y ,y )}k
zℓ z′ ℓ ℓ=1
tothegivencontextx usingtherepresentationfunctionϕ. ItthencreatesanICLprompt
t
containingthesek exampleswherey ispresentedastheinput,andy ispresentedas
zℓ z′
ℓ
thedesiredoutput. Theagentthenusesthecontextx andtheICLprompttogeneratethe
t
response. Thisapproachdoesn’tinferpreferencesbutmustinsteadusetheusereditdata
directlytoaligntothegivenuserpreference. However, unlikeexplore-then-exploitLPI
andContinualLPI,thisapproachcanperformcontext-dependentlearningasthegenerated
responseattendsonboththegivencontextx andthehistoricaldata.
t
BaselineHyperparameters. Forexplore-then-exploitLPIandcontinualLPIbaselines,wesetthe
numberofexplorationT as5. ForICL-editbaselines,weexperimentwithdifferentk valuesfor
e
retrieval,andreportourbestresultswithk =5.
OracleMethod. Weadditionallyrunanoraclepreferencemethodtoprovideanapproximated
upper bound on performance. In each round t, we let the GPT-4 agent generate a response by
conditioning on the ground-truth latent preference f⋆ and the context x . This method can test
t t
whetheroursetupiswell-defined,e.g.,inapoorlydesignedsetup,theuseralwayseditstheagent
responsenomatterwhattheagentgeneratesincludingprovidingusereditsbacktotheuser,andthus
nomethodcaneffectivelyminimizethecostovertimeinthiscase. Iftheoraclemethodachievesa
zerooraminimalusereditcost,thenlearningtheoptimalpreferenceleadstosuccess.
4.3 MainResultandDiscussion.
MainResults. Table2reportstheperformanceofbaselinesandourmethodsonsummarization
andemailwritingtasksonthreemetrics: editdistancewhichmeasurescumulativeusereditcost,
accuracywhichmeasuresmeanpreferenceclassificationaccuracy,andexpensemeasuringthetotal
BPEtokencostofqueryingLLM.6 Wereportthemeanandstandarddeviationacross3different
randomseeds.7
Table2: Performanceofbaselinesandourmethodsintermsofcumulativeeditdistancecostand
classificationaccuracy. µ denotesthemeanµandstandarddeviationσacross3runsoverdifferent
σ
seeds. ExpensecolumnshowsbudgetastheaveragenumberofinputandoutputBPEtokensacross3
runs(unitis·105). Weuse-kinmethodnamestodenotethatweusekretrievedexamples. Numbers
inboldarethebestperformanceineachcolumnexcludingoraclepreferencemethod,underlinefor
thesecondbest,anddottedunderlineforthethirdbest.
Method Summarization EmailWriting
EditDistance↓ Accuracy↑ Expense↓ EditDistance↓ Accuracy↑ Expense↓
OraclePreference 6,573 1.000 1.67 1,851 1.000 1.62
1,451 243
NoLearning 48,269 - 1.50 31,103 - 1.65
957 900
E-then-eLPI 65,218 0.218 1.99 24,562 0.263 1.73
17,466 0.003 1,022 0.003
ContinualLPI 57,915 0.233 8.89 26,852 0.243 8.63
2,210 0.010 1,464 0.019
ICL-edit-5-MPNET 38,560 - 8.00 32,405 - 12.12
1,044 1,307
ICL-edit-5-BERT 39,734 - 7.96 30,949 - 11.55
1,929 3,250
CIPHER-1-MPNET 33,926
4,000
0.520
0.022
2.74 1..0.,.7.8..1.1,711 .0...4.3..5.0.084 1.94
CIPHER-5-MPNET 32,974
195
.0...4.7..8.0.010 3.00 10,058
1,709
0.467
0.081
2.09
CIPHER-1-BERT 37,637 0.565 2.81 12,634 0.487 1.99
3,025 0.053 4,868 0.125
CIPHER-5-BERT 3..5.,.8.1..1.3,384 .0...4.7..8.0.028 3.03 8,391
3,038
0.363
0.075
2.22
6Table9inAppendixshowsthebreakdownofexpenseintermsofinputandoutput.
7Werandomizethecontextsamplingfromsourcedatasets,soexperimentsondifferentseedscontaindifferent
setsofinputcontexts.Onthesameseed,experimentsacrossdifferentmethodsarestrictlycomparable,asboth
thesetofinputcontextsandtheorderofinputcontextseenarethesameinourimplementation.
8Figure2: Learningcurvesofdifferentmethodsbasedoncumulativecostovertime(averageacross3
seeds). Inthelegend,-kmeanswithtopkretrievedexamples,-BforBERT,and-MforMPNET.
·104 Summarization ·104 EmailWriting
7 3.5
Oracle Oracle
NoLearning NoLearning
6 E-then-e 3 E-then-e
Continual Continual
ICL-edit-B ICL-edit-B
5 2.5
ICL-edit-M ICL-edit-M
CIPHER-1-B CIPHER-1-B
CIPHER-5-B CIPHER-5-B
4 2
CIPHER-1-M CIPHER-1-M
CIPHER-5-M CIPHER-5-M
3 1.5
2 1
1 0.5
0 40 80 120 160 200 0 40 80 120 160 200
Round Round
DiscussionofMainResult. Weobservethatnotperforminglearningresultsinahigheditcost,
whereasusingtheOraclepreferencesachievesasignificantlysmallereditcost. Thisshowsthatour
environmentsaresoundandwell-conditioned. E-then-eLPIandContinualLPIlearncontext-agnostic
preferenceswhichcannotcapturethecontext-dependentpreferencesintheenvironmentsandend
up doing poorly. For the summarization task, they end up with a higher edit distance than even
performingnolearning. Oneexplanationisthatusingcontext-agnosticpreferencescanpushthe
modeltospecializetoagivenpreferencemuchmorethanthebasemodel,resultinginmoreedits
whenthatpreferenceisincorrect. Weseethisinpreferenceaccuracywhichislowforbothofthese
baselines,andlowerforthesummarizationtaskthantheemailwritingtaskwheretheyoutperformno
learningbaselines. Further,ContinualLPIhasahigherexpensecostduetoconstantlyqueryingthe
LLMtoinfertheuserpreference.
ICL-editbaselinesperformsignificantlybetteronthesummarizationtask. However,usingalistof
usereditsinthepromptresultsinahighertokenexpensecost,astheresponsesandtheireditscanbe
significantlylonginpractice. Further,theICL-editbaselinesprovidenointerpretableexplanationfor
theirresponseorforexplaininguserbehavior.
Finally,CIPHERachievesthesmallesteditdistancecostreducingeditsby31%inthesummarization
taskand73%intheemailwritingtask. Weobservethatretrievingk =5preferencesandaggregating
themachieveslowereditdistance,however,thechoiceofidealrepresentationϕseemstask-dependent.
Further,CIPHERachievesthehighestpreferenceaccuracyshowingthatCIPHERcanlearnpref-
erencesthatcorrelatemorewiththegroundtruthpreferencethanpreferencesofotherdocument
sources. Notethattheperformanceofarandompreferenceclassifierisonly20%forsummarization
and25%foremailwriting. Further,CIPHERachievesasmallercostthanICL-editandContinual
LPI baselines, as it doesn’t use long user edits in the prompt for generating a response. Overall,
CIPHERprovidesacheap,moreeffective,andinterpretablemethodthanourbaselines.
4.4 MoreAnalysis
LearningCurves. WeplotmeancumulativeusereditcostsoverroundsinFigure2.Thecumulative
usereditcostsinFigure2showthattheangleofthelearningcurvesdecreasesforCIPHERafteran
initialnumberofrounds,showingthatlearninghelpsdecreasetherateatwhichusereditsareaccu-
mulated. Incontrast,theangleofthelearningcurvefortheno-learningbaselineremainsunchanged.
9
tsoCevitalumuC tsoCevitalumuCFigure3: Normalizedcostandpercentageofzero-costexamplesofCIPHERovertime,binnedper
20roundstoshowthetrend(averageacross3seeds). Inthelegend,-kmeanswithtopkretrieved
examples,-BforBERT,and-MforMPNET.
CIPHER-5-B CIPHER-1-M CIPHER-5-M Oracle
Summarization EmailWriting
0.6
0.4
0.4
0.2
0.2
0 0
40 80 120 160 200 40 80 120 160 200
Round Round
0.6
0.8
0.6
0.4
0.4
0.2 0.2
0
40 80 120 160 200 40 80 120 160 200
Round Round
Evaluating Normalized Edit Cost. The cumulative user edit cost measures the total effort of
theuserbutissusceptibletooutlierexamples,astheeditdistanceforagivenroundispotentially
unbounded. Therefore,wealsocomputeanormalizededitdistance∆ (y ,y )/|y |bydividing
edit t t′ t
theeditdistancebymax{|y |,|y |},i.e. themaxlengthoftheagentoutputoruserrevisedtext. As
t t′
Levenshteindistance∆ (y ,y )isupperboundedbymax{|y |,|y |}, therefore, thenormalized
edit t t′ t t′
costisatmost1. Figure3reportsnormalizedcostoverroundsforthetop3methods. Wenoticethat
forallvariantsofCIPHERforthesummarizationtask,andforCIPHER-5-Mfortheemailwriting
task,thenormalizedcostdecreasesnotablyastrainingprogressesindicatinglearning. Asthecostis
normalizedbytheresponselength,evenasmalldecreasecanleadtoasignificantreductioninthe
numberoftokensedited.
EvaluatingFractionofEditedResponse. RecallthatthefirststageofourGPT-4userchecksif
theagentresponsesatisfiesthelatentuserpreferencef⋆. Ifitdoes,thentheuserperformsnoedits.
Otherwise,inthesecondstage,theusereditstheresponse. Tomeasurehowmanytimestheagent
responseisn’tedited,wealsoplotthepercentageofexampleswithzeroeditcostper20roundsbin
inFigure3. Wenoticeasmallincreaseinthenumberofexampleswithzeroeditcost. Thisindicates
thatgainscomefromreducingeditsacrossallexamples,andnotjustbyincreasingthenumberof
examplesthatavoidgettingeditedinstage1ofouruser.
QualitativeAnalysisofLearnedPreferences. Wequalitativelyanalyzethelearnedpreferences
for CIPHER to understand the quality of learned preferences. We present our analysis on the
summarizationtask,whereourmethodshavealargergapwiththeoracleperformancecompared
totheemailwritingtask. Table3lists3learnedpreferencesperdocumentsourceforCIPHER-5-
MPNET whicharerandomlysampledatthebeginning,middle,andendoftheinteractionhistory.
Weseethatoveralltheagentcanlearnareasonabledescriptionofthelatentpreference. Forexample,
itcanlearnbulletpointspreferenceforWikipediaarticles,andsecondpersonnarrativeforReddit
posts,andQAstyleforMoviereviews. CIPHERcanpicksomepreferencesfairlyearlysuchasbullet
points for Wikipedia and emojis for Paper abstract, whereas some are learned only later such as
StructuredQ&AforMoviereviews. ThisshowsusingCIPHERcanquicklylearnusefulpreferences,
butfurtherinteractioncontinuestohelp.
Failure Cases. CIPHER notably reduces the edit cost and learns useful preference, however,
significantgapstotheoraclemethodremain,especiallyinthesummarizationtask. Wemanually
analyzefailurecasesonsummarizationtaskwiththebestperformingmethodCIPHER-5-MPNET.
Table10intheAppendixreportsthesummaryandexampleofourfindings,categorizedaspreference
10
tsoc-oreZ%
tsoCdezilamroN
niBrep
.xE
tsoc-oreZ%
tsoCdezilamroN
niBrep
.xETable3: ExamplesoflearnedpreferencesonsummarizationtaskwithCIPHER-5-MPNET,grouped
basedonthedocumentsourceandcorrespondinglatentpreference. Werandomlysample3examples
pertypeatthebeginning,middle,andendoftheinteractionhistory.
LatentUserPreference (Round)LearnedPreference
News article. targeted to (22)Fairytalenarrativestyle,informalandconversationaltone,useofrhetorical
youngchildren,storytelling, questions,simplifiedlanguage.
shortsentences,playfullan- (115)Simplified,childlikestorytellingwithplayfullanguageandimagery
guage,interactive,positive (192)Simplifiedandplayfulstorytellinglanguage
Redditpost:secondperson (14)Conciseandcoherentstorytelling
narrative,brief,showemo- (102)Theuserprefersasecond-personnarrativeandamoredirect,personaltone
tions,invokepersonalreflec- (194)Poeticanddescriptivelanguage,narrativeperspectiveshifttosecondperson
tion,immersive
Wikipedia page. bullet (19)Concise,Bullet-Pointed,StructuredSummarieswithaNarrativeQ&AStyle
points, parallel structure, (124)Conciseandfactualwritingstyle,bullet-pointformatting
brief (197)Conciseandstreamlinedformatting,withbulletpointsandclearsubhead-
ingsforeasyscanning
Paperabstract.tweetstyle, (20)Concise,conversationalsummarieswithbulletpointsandemojis.
simpleEnglish,inquisitive, (111)Concise,conversational,whimsicalbullet-pointsummarieswithemojis.
skillfulforeshadowing,with
emojis (193)Concise,conversational,andwhimsicalbullet-pointsummarieswithemojis.
Moviereview.questionan- (12)Theuserprefersastraightforward, clear, andconcisewritingstylewith
sweringstyle factualformatting.
(123) The user prefers a clear and concise question and answer format with
straightforwardlanguage.
(199)Concise,StructuredQ&AwithWhimsicalClarity
inferencefromoutput-revisionpair,consolidationofinferredpreferences,andretrieval.8 Inbrief,
themostcommontypeoffailureisonthepreferenceinferencestepgiventheagentoutputanduser
revision. Forexample,theagentoftenmissestheexactkeywordforbrief orshortsentences,and
sometimesstruggleswithinferringthesecond-personnarrativeaspect.
5 RelatedWork
Wedescriberelatedworkinthisareagroupedbymainthemesinthiswork.
LearningfromFeedback. Besidespair-wisecomparisonfeedbackfromannotatorsusedinRein-
forcementLearningfromHumanFeedback(RLHF)research(Ziegleretal.,2019;Stiennonetal.,
2020;Nakanoetal.,2021;Ouyangetal.,2022a,interalia),priorworkhasalsostudiedfree-form
textfeedbackprovidedbyannotators (Fernandesetal.,2023),suchasonthetaskofdialog(We-
ston, 2016; Li et al., 2016; Hancock et al., 2019; Xu et al., 2022; Petrak et al., 2023), question
answering(Lietal.,2022;Malaviyaetal.,2023),summarization(Saundersetal.,2022),andgeneral
decision making (Cheng et al., 2023). This feedback, tailored to each example, is often utilized
torankcandidateoutputs,therebyimprovingtaskperformance. Someworkstudieslearningfrom
textfeedbacktogenerateoutputsdirectly(Scheureretal.,2023;Baietal.,2022;Shietal.,2022),
by generating multiple refinements of the original output based on the feedback and fine-tuning
the original model to maximize the likelihood of the best refinement. In grounded settings such
asinstruction-basednavigation,onelineofworkhasalsousedhindsightfeedbackthatexplicitly
providesatextinstructionforthegeneratedtrajectory,totrainpolicies(Nguyenetal.,2021;Misra
et al., 2024). Moving beyond the conventional focus on text feedback that explicitly articulates
humanintent,weinvestigatefeedbackintheformofdirecteditsontheoriginalmodeloutput. Such
revisionsbyusersoccurnaturallyduringmodeldeploymentinpractice. Additionally,weexamine
thelearningofuserpreferencesthroughhistoricalinteractions,aimingtosurpasstheconstraintsof
example-specificfeedback.
8WeprovideadditionalanalysisontheaccuracyofretrievalinTable11.
11LanguageAgentsandPersonalization. LLMshaveenabledthedevelopmentoflanguageagents
foravarietyoftasksfromwritingassistants(Leeetal.,2024),codingassistants(Dohmke,2022),
andcustomerserviceassistants(Brynjolfssonetal.,2023). SincetheseLLM-basedassistantsare
often used by individuals, a natural question has arisen on how to personalize these agents for
eachuser. Straightforwardapproachesforfine-tuningLLMsincludessupervisedlearning,online
DPO(Guoetal.,2024),learning-to-search(Changetal.,2023),andreinforcementlearning(Ouyang
etal.,2022b). Theseapproachescanbedirectlyappliedtooursetting. Forexample,onecanuse
(y ,y )inProtocol1asthepreferencedatawherey ispreferredovery ,orusey astheground
t t′ t′ t t′
truthforsupervisedlearning. However,fine-tuningisexpensiveandhardtoscalewiththenumber
ofusers. Therefore,alineofworkhasexploredimprovingthealignmentoffrozenLLMsbyprompt
engineering,suchaslearningapersonalizedretrievalmodel(Mysoreetal.,2023),learningaprompt
policygivenarewardfunction(Dengetal.,2022),ormoregenerally,learningtorewritetheentire
prompt (Li et al., 2023). We focus on learning a prompt policy by learning from user edits, and
specifically,usingthemtoextracttexturaldescriptionsofuserpreference.
EditsandRevisions. Manypriorworkoneditingmodeloutputfocusesonerrorcorrection,such
asfixingsourcecode(Yinetal.,2018;Chenetal.,2018;Reidetal.,2023)andimprovingthefactual
consistencyofmodelsummaries(Caoetal.,2020;Liuetal.,2022;Balachandranetal.,2022). A
line of work has explored understanding human edits based on edit history of Wikipedia (Botha
etal.,2018;Faltingsetal.,2020;Rajagopaletal.,2022;Reid&Neubig,2022;Labanetal.,2023),
or revisions of academic writings (Mita et al., 2022; Du et al., 2022; D’Arcy et al., 2023). Prior
workexplorespredictingtextrevisionswitheditintents(Brodyetal.,2020;Kimetal.,2022;Chong
etal.,2023),andmodelingeditswithvariousapproaches,includinglatentvectors(Guuetal.,2017;
Marrese-Tayloretal.,2020,2023),structuredtrees(Yaoetal.,2021),discretediffusionprocess(Reid
et al., 2023), or a series of singular edit operations (Stahlberg & Kumar, 2020; Mallinson et al.,
2020;Agrawal&Carpuat,2022;Zhangetal.,2022;Liuetal.,2023). However,thesemethodologies
predominantlytargetgenericimprovementsinmodelperformance,overlookingtheintricaciesof
individual user satisfaction and preference. Our research takes a distinct direction, focusing on
understandingeditsacrossavarietyofexamplestostudyuser-levelpreferences,withapracticalgoal
ofaligningtheagenttoindividualpreferences.
6 Conclusion
We study aligning LLM-based agents using user edits that arise naturally in applications such
as writing assistants. We conjecture that user edits are driven by a latent user preference that
canbecapturedbytextualdescriptions. WeintroducethePRELUDEframeworkthatfocuseson
learningdescriptionsofuserpreferencesfromusereditdataandthengeneratinganagentresponse
accordingly. Weproposeasimpleyeteffectiveretrieval-basedalgorithmCIPHERthatinfersuser
preferencebyqueryingtheLLM,retrievesrelevantexamplesinthehistory,andaggregatesinduced
preferencesinretrievedexamplestogeneratearesponseforthegivencontext. Weintroducetwo
interactiveenvironmentswithaGPT-4simulatedusertostudylearningfromedits,whichcanbe
ofindependentinterest. Inthiswork,wefocusonaligninganLLMagentwithafrozenLLM,in
part,duetothechallengeofscalingfine-tuningbasedapproacheswiththenumberofusers. However,
forsettingswherecomputationalcostisnotabarrier,applyingfine-tuningapproacheswouldbean
interestingfutureworkdirection. Anotherpromisingfutureworkdirectionistolearnuserpreference
basedondifferentlevelsofedits–words,sentences,paragraphs–togenerateasatisfactoryresponse.
Acknowledgments
GaowasaresearchinterninMSRNYC,andlaterwaspartiallysupportedbyNSFproject#1901030.
Allcontentrepresentstheopinionoftheauthors,whichisnotnecessarilysharedorendorsedbytheir
respectiveemployersand/orsponsors. WethankMSRNYCresearchcommunity,JonathanD.Chang,
DanielD.Lee,ClaireCardie,andSashaRushforhelpfuldiscussionsandsupport.
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
12SwetaAgrawalandMarineCarpuat. Animitationlearningcurriculumfortexteditingwithnon-
autoregressivemodels. ArXiv,abs/2203.09486,2022.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
AnnaChen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,CarolChen,CatherineOlsson,
ChristopherOlah,DannyHernandez,DawnDrain,DeepGanguli,DustinLi,EliTran-Johnson,
EthanPerez,JamieKerr,JaredMueller,JeffreyLadish,JoshuaLandau,KamalNdousse,Kamile
Lukosuite, LianeLovitt, MichaelSellitto, NelsonElhage, NicholasSchiefer, NoemiMercado,
NovaDasSarma, RobertLasenby, RobinLarson, SamRinger, ScottJohnston, ShaunaKravec,
SheerElShowk,StanislavFort,TameraLanham,TimothyTelleen-Lawton,TomConerly,Tom
Henighan,TristanHume,SamuelR.Bowman,ZacHatfield-Dodds,BenMann,DarioAmodei,
NicholasJoseph,SamMcCandlish,TomBrown,andJaredKaplan.Constitutionalai:Harmlessness
fromaifeedback,2022.
VidhishaBalachandran,HannanehHajishirzi,WilliamCohen,andYuliaTsvetkov. Correctingdiverse
factualerrorsinabstractivesummarizationviapost-editingandlanguagemodelinfilling. ArXiv,
abs/2210.12378,2022.
NitsanBar. Papertweet. https://github.com/bnitsan/PaperTweet/,2022.
JanA.Botha,ManaalFaruqui,JohnAlex,JasonBaldridge,andDipanjanDas. Learningtosplitand
rephrasefromwikipediaedithistory. ArXiv,abs/1808.09468,2018.
ShakedBrody,UriAlon,andEranYahav. Astructuralmodelforcontextualcodechanges. Proceed-
ingsoftheACMonProgrammingLanguages,4:1–28,2020.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
ErikBrynjolfsson,DanielleLi,andLindseyRRaymond. Generativeaiatwork. Technicalreport,
NationalBureauofEconomicResearch,2023.
Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. Factual error correction for
abstractivesummarizationmodels. ArXiv,abs/2010.08712,2020.
JonathanDChang,KianteBrantley,RajkumarRamamurthy,DipendraMisra,andWenSun.Learning
togeneratebetterthanyourllm. arXivpreprintarXiv:2306.11816,2023.
Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and
Monperrus Martin. Sequencer: Sequence-to-sequence learning for end-to-end program repair.
IEEETransactionsonSoftwareEngineering,47:1943–1959,2018.
Ching-AnCheng,AndreyKolobov,DipendraMisra,AllenNie,andAdithSwaminathan. Llf-bench:
Benchmarkforinteractivelearningfromlanguagefeedback. arXivpreprintarXiv:2312.06853,
2023.
RuiningChong,CunliangKong,LiuWu,ZhenghaoLiu,ZiyeJin,LinerYang,YangeFan,Hanghang
Fan,andErhongYang. Leveragingprefixtransferformulti-intenttextrevision. AnnualMeetingof
theAssociationforComputationalLinguistics,2023.
ColinB.Clement,MatthewBierbaum,KevinP.O’Keeffe,andAlexanderA.Alemi. Ontheuseof
arxivasadataset,2019.
MikeD’Arcy, AlexisRoss, ErinBransom, BaileyKuehl, JonathanBragg, TomHope, andDoug
Downey. Aries: A corpus of scientific paper edits made in response to peer reviews. ArXiv,
abs/2306.12587,2023.
MingkaiDeng,JianyuWang,Cheng-PingHsieh,YihanWang,HanGuo,TianminShu,MengSong,
EricPXing, andZhitingHu. Rlprompt: Optimizingdiscretetextpromptswithreinforcement
learning. arXivpreprintarXiv:2205.12548,2022.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. NorthAmericanChapteroftheAssociation
forComputationalLinguistics,2019.
13ThomasDohmke. Githubcopilotisgenerallyavailabletoalldevelopers. https://github.blog/
2022-06-21-github-copilot-is-generally-available-to-all-developers/, 2022.
Accessed: April-20-2024.
WanyuDu, VipulRaheja, DhruvKumar, ZaeMyungKim, MelissaLopez, andDongyeopKang.
Understandingiterativerevisionfromhuman-writtentext. ArXiv,abs/2203.03802,2022.
FelixFaltings,MichelGalley,GeroldHintz,ChrisBrockett,ChrisQuirk,JianfengGao,andBill
Dolan. Texteditingbycommand. ArXiv,abs/2010.12826,2020.
PatrickFernandes,AmanMadaan,EmmyLiu,AntónioFarinhas,PedroHenriqueMartins,Amanda
Bertsch,JoséG.C.deSouza,ShuyanZhou,TongshuangSherryWu,GrahamNeubig,andAndré
F.T.Martins. Bridgingthegap: Asurveyonintegrating(human)feedbackfornaturallanguage
generation. ArXiv,abs/2305.00955,2023.
WikimediaFoundation. Wikimediadownloads. https://dumps.wikimedia.org,2022.
AurélienGarivier,TorLattimore,andEmilieKaufmann. Onexplore-then-commitstrategies. Ad-
vancesinNeuralInformationProcessingSystems,29,2016.
ShangminGuo,BiaoZhang,TianlinLiu,TianqiLiu,MishaKhalman,FelipeLlinares,Alexandre
Rame,ThomasMesnard,YaoZhao,BilalPiot,etal. Directlanguagemodelalignmentfromonline
aifeedback. arXivpreprintarXiv:2402.04792,2024.
Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editingprototypes. TransactionsoftheAssociationforComputationalLinguistics,6:437–450,
2017.
Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré, and Jason Weston. Learning from
dialogue after deployment: Feed yourself, chatbot! Annual Meeting of the Association for
ComputationalLinguistics,2019.
XinyuHua,MitkoNikolov,NikhilBadugu,andLuWang. Argumentminingforunderstandingpeer
reviews. Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),
June2019.
DanielJamesKershawandR.Koeling. Elsevieroacc-bycorpus. ArXiv,abs/2008.00774,2020. doi:
https://doi.org/10.48550/arXiv.2008.00774. URLhttps://elsevier.digitalcommonsdata.
com/datasets/zm33cdndxs.
ZaeMyungKim,WanyuDu,VipulRaheja,DhruvKumar,andDongyeopKang. Improvingiterative
textrevisionbylearningwheretoeditfromotherrevisiontasks. ArXiv,abs/2212.01350,2022.
PhilippeLaban,JesseVig,WojciechKryscinski,ShafiqR.Joty,CaimingXiong,andChien-Sheng
Wu. Swipe: Adatasetfordocument-levelsimplificationofwikipediapages. AnnualMeetingof
theAssociationforComputationalLinguistics,2023.
MinaLee,KatyIlonkaGero,JohnJoonYoungChung,SimonBuckinghamShum,VipulRaheja,
HuaShen,SubhashiniVenugopalan,ThiemoWambsganss,DavidZhou,EmadA.Alghamdi,Tal
August,AvinashBhat,MadihaZahrahChoksi,SenjutiDutta,JinL.C.Guo,Md.NaimulHoque,
YewonKim,SeyedParsaNeshaei,AgniaSergeyuk,AntonetteShibani,DishaShrivastava,Lila
Shroff,JessiStark,S.Sterman,SitongWang,AntoineBosselut,DanielBuschek,JosephChee
Chang,SherolChen,MaxKreminski,JoonsukPark,RoyPea,EugeniaH.Rho,ShannonZejiang
Shen, and Pao Siangliulue. A design space for intelligent and interactive writing assistants.
ConferenceonHumanFactorsinComputingSystems,abs/2403.14117,2024.
Vladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals.
Sovietphysics.Doklady,10:707–710,1965.
ChengLi,MingyangZhang,QiaozhuMei,WeizeKong,andMichaelBendersky. Automaticprompt
rewritingforpersonalizedtextgeneration. arXivpreprintarXiv:2310.00152,2023.
14JiweiLi,AlexanderH.Miller,SumitChopra,Marc’AurelioRanzato,andJasonWeston. Dialogue
learningwithhuman-in-the-loop. ArXiv,abs/1611.09823,2016.
Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung, and Siva Reddy. Using in-
teractive feedback to improve the accuracy and explainability of question answering systems
post-deployment. ArXiv,abs/2204.03025,2022.
Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu, and Soroush Vosoughi. Second
thoughtsarebest: Learningtore-alignwithhumanvaluesfromtextedits. ArXiv,abs/2301.00355,
2023.
YixinLiu,BudhadityaDeb,MilagroTeruel,AaronLHalfaker,DragomirR.Radev,andAhmedHas-
sanAwadallah. Onimprovingsummarizationfactualconsistencyfromnaturallanguagefeedback.
AnnualMeetingoftheAssociationforComputationalLinguistics,2022.
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher
Potts. Learningwordvectorsforsentimentanalysis. Proceedingsofthe49thAnnualMeetingof
theAssociationforComputationalLinguistics: HumanLanguageTechnologies,June2011.
ChaitanyaMalaviya,SubinLee,DanRoth,andMarkYatskar. Pachinko: Patchinginterpretableqa
modelsthroughnaturallanguagefeedback. ArXiv,abs/2311.09558,2023.
Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido. Felix: Flexible text
editingthroughtaggingandinsertion. ArXiv,abs/2003.10687,2020.
EdisonMarrese-Taylor,MachelReid,andYutakaMatsuo. Variationalinferenceforlearningrepre-
sentationsofnaturallanguageedits. ArXiv,abs/2004.09143,2020.
EdisonMarrese-Taylor,MachelReid,andAlfredoSolano. Editawarerepresentationlearningvia
levenshteinprediction. TheFourthWorkshoponInsightsfromNegativeResultsinNLP,2023.
DipendraMisra,AldoPacchiano,andRobertESchapire. Provableinteractivelearningwithhindsight
instructionfeedback. arXivpreprintarXiv:2404.09123,2024.
MasatoMita,KeisukeSakaguchi,MasatoHagiwara,TomoyaMizumoto,JunSuzuki,andKentaro
Inui. Towardsautomateddocumentrevision: Grammaticalerrorcorrection, fluencyedits, and
beyond. ArXiv,abs/2205.11484,2022.
ShesheraMysore,ZhuoranLu,MengtingWan,LongqiYang,SteveMenezes,TinaBaghaee,Em-
manuelBarajasGonzalez,JenniferNeville,andTaraSafavi. Pearl: Personalizinglargelanguage
modelwritingassistantswithgeneration-calibratedretrievers. arXivpreprintarXiv:2311.09180,
2023.
ReiichiroNakano,JacobHilton,S.ArunBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,TynaEloundou,
GretchenKrueger,KevinButton,MatthewKnight,BenjaminChess,andJohnSchulman. Webgpt:
Browser-assistedquestion-answeringwithhumanfeedback. ArXiv,2021.
KhanhXNguyen,DipendraMisra,RobertSchapire,MiroslavDudík,andPatrickShafto. Interactive
learningfromactivitydescription. InternationalConferenceonMachineLearning,pp.8096–8108,
2021.
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,
LukeE.Miller, MaddieSimens, AmandaAskell, PeterWelinder, PaulFrancisChristiano, Jan
Leike,andRyanJ.Lowe. Traininglanguagemodelstofollowinstructionswithhumanfeedback.
ArXiv,2022a.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022b.
15DominicPetrak,NafiseSadatMoosavi,YeTian,NikolaiRozanov,andIrynaGurevych.Learningfrom
free-texthumanfeedback-collectnewdatasetsorextendexistingones? ArXiv,abs/2310.15758,
2023.
DheerajRajagopal,XuchaoZhang,MichaelGamon,SujayKumarJauhar,DiyiYang,andEduardH.
Hovy. Onedocument,manyrevisions: Adatasetforclassificationanddescriptionofeditintents.
InternationalConferenceonLanguageResourcesandEvaluation,2022.
MachelReidandGrahamNeubig. Learningtomodeleditingprocesses. ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,2022.
Machel Reid, Vincent J. Hellendoorn, and Graham Neubig. Diffuser: Diffusion via edit-based
reconstruction. InternationalConferenceonLearningRepresentations,2023.
WilliamSaunders, CatherineYeh, JeffWu, Steven Bills, OuyangLong, JonathanWard, andJan
Leike. Self-critiquingmodelsforassistinghumanevaluators. ArXiv,abs/2206.05802,2022.
J’er’emyScheurer,JonAnderCampos,TomaszKorbak,JunShernChan,AngelicaChen,Kyunghyun
Cho, and Ethan Perez. Training language models with language feedback at scale. ArXiv,
abs/2303.16755,2023.
AbigailSee,PeterJ.Liu,andChristopherD.Manning. Gettothepoint: Summarizationwithpointer-
generatornetworks. Proceedingsofthe55thAnnualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),July2017.
ZejiangShen,TalAugust,PaoSiangliulue,KyleLo,JonathanBragg,JeffHammerbacher,Doug
Downey,JosephCheeChang,andDavidSontag. Beyondsummarization: Designingaisupportfor
real-worldexpositorywritingtasks. arXivpreprintarXiv:2304.02623,2023.
WeiyanShi,EmilyDinan,KurtShuster,JasonWeston,andJingXu. Whenlifegivesyoulemons,
makecherryade:Convertingfeedbackfrombadresponsesintogoodlabels.ArXiv,abs/2210.15893,
2022.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted
pre-trainingforlanguageunderstanding. ArXiv,abs/2004.09297,2020.
FelixStahlbergandShankarKumar. Seq2edits: Sequencetransductionusingspan-leveleditopera-
tions. ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2020.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec
Radford,DarioAmodei,andPaulChristiano.Learningtosummarizefromhumanfeedback.ArXiv,
abs/2009.01325,2020.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
SitongWang,LydiaBChilton,andJeffreyVNickerson. Writingwithgenerativeai: Multi-modal
andmulti-dimensionaltoolsforjournalists. TheSecondWorkshoponIntelligentandInteractive
WritingAssistantsatACMCHI,2023.
JasonWeston. Dialog-basedlanguagelearning. ArXiv,abs/1604.06045,2016.
JingXu,MeganUng,MojtabaKomeili,KushalArora,Y-LanBoureau,andJasonWeston. Learn-
ingnewskillsafterdeployment: Improvingopen-domaininternet-drivendialoguewithhuman
feedback. AnnualMeetingoftheAssociationforComputationalLinguistics,2022.
ZiyuYao,FrankF.Xu,PengchengYin,HuanSun,andGrahamNeubig. Learningstructuraleditsvia
incrementaltreetransformations. ArXiv,abs/2101.12087,2021.
PengchengYin,GrahamNeubig,MiltiadisAllamanis,MarcBrockschmidt,andAlexanderL.Gaunt.
Learningtorepresentedits. ArXiv,abs/1810.13337,2018.
16Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Miloš Gligoric´. Coditt5:
Pretrainingfor sourcecode andnaturallanguage editing. Proceedingsof the37thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering,2022.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:
Evaluatingtextgenerationwithbert. InternationalConferenceonLearningRepresentations,2020.
Daniel M. Ziegler, Nisan Stiennon, JeffWu, TomB. Brown, Alec Radford, Dario Amodei, Paul
Christiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences. ArXiv,
2019.
17Appendix
A AdditionalDetails
DatasetExamples. Welistlinkstodatasetsourcesforouruser-providedcontextinTable4.
GPT-4User’sEdits WelistexamplesofOURGPT-4user’seditswithdifferentlatentpreference
onsummarizationinTable5.
GPT-4UserTemplate. PrompttemplatesusedbyourGPT-4userareprovidedinTable6.
CIPHERTemplates. PrompttemplatesusedbyCIPHERareprovidedinTable7.
ICL-editTemplates. PrompttemplatesusedbyICL-editbaselineareprovidedinTable8.
B AdditionalAnalysis
DetailedExpenseAnalysis. WelistadetailedcomputationalexpenseofdifferentmethodsinTa-
ble9.
FailureCases. WesummarizeourfailurecaseanalysisofCIPHERonsummarizationinTable10.
RetrievalAccuracy. Wecalculateretrievalaccuracyfor CIPHERasthefractionofallretrieved
contexts that are of the same document type as the currently given context across all seeds and
timesteps. WereporttheresultsinTable11. Wefindthattheretrievalaccuracyishigheronthe
summarizationtaskthanonemailwriting. andusingMPNETtypicallyperformsbetterthanusing
Berttoencodecontext.
Table4: Linktoeachsourcedataset,fromwhichwerandomlysampleexamplesastheuser-provided
contextinourtasks.
DataSource LinkandExample
CNNDailyMail(Seeetal.,2017) https://huggingface.co/datasets/cnn_dailymail
SLF5K(Stiennonetal.,2020) https://huggingface.co/datasets/JeremyAlain/SLF5K
Wikidump(Foundation,2022) https://huggingface.co/datasets/wikipedia
Arxiv(Clementetal.,2019) https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers
IMDb(Maasetal.,2011) https://huggingface.co/datasets/imdb
PaperTweet(Bar,2022) https://huggingface.co/datasets/nitsanb/paper_tweet
ElsevierOACC-By(Kershaw& https://huggingface.co/datasets/orieg/elsevier-oa-cc-by
Koeling,2020)
18Table5: Examplesofusereditswithdifferentlatentpreferenceonsummarization. Theagentoutput
columnshowstheagentgenerationwithoutconditioningonanypreference. Thelatentpreferencefor
theusereditsisinboldandparenthesis. Wetruncatesomeexamplesduetolimitedspace.
AgentOutput UserEdits
LeonFenner,thesuspectedgunmanina$2million (targetedtoyoungchildren,storytelling,short
diamondheistatWatchStandardJewelryinNew sentences, playful language, interactive, posi-
YorkCity,wasarrestedinMaryland.Therobbery tive)
occurredonNovember11andresultedinthein- Onceuponatimeinthebig,bustlingcityofNew
juryofthestoreowner’selderlyfather. Fenner’s York,duringaparadefilledwithmusicandmarch-
accomplice,RonduFrisby,wasafriendofthestore ingonVeteran’sDay,somethingsneakyhappened
ownerandwasapprehendedaweekaftertheheist. at a place called Watch Standard Jewelry. This
Therobberytookplaceduringthecity’sVeteran’s wasn’tjustanystore;itwasatreasurechestfullof
DayParade,leadingtoablockshutdown.Fenner sparklingdiamondsandshinywatchesthatcould
andFrisbystolenumerousjewelsand20luxury makeyoureyestwinklelikestars.
watches,butwerecaughtonsurveillancecameras. Now, two not-so-nice men, Leon Fenner and
Fennerissettoappearbeforeafederalcourtin his buddy Rondu Frisby, thought they could be
Maryland. sneakyandtakethesetreasureswithoutasking.On
November11,whileeveryonewaswatchingthe
parade,theytiptoedintothestore.Butohno!They
accidentallyhurtthestoreowner’sdearolddad,
whichwasn’tverykindatall.[TRUNCATED]
The article describes a situation where a neigh- (secondpersonnarrative,brief,showemotions,
bor’s cat, one of seven rescued by a "cat lady," invokepersonalreflection,immersive)
hasstartedfrequentingtheauthor’sfrontyardafter Imagineyourselfinaquaintneighborhood,where
leavingitsoriginalhomeduetothepresenceof thegentlepurrsofafelinefriendhavebecomea
othercats. Overthepastfewmonths,thecathas familiarpartofyourdailyroutine.Thisisn’tjust
becomemoreconsistentinitsvisits,meowingat any cat—it’s one of seven, a charming escapee
theauthorandtheirfamilymembersforattention. from the home of a local ""cat lady,"" seeking
The cat allows itself to be petted and shows af- solaceinthetranquilityofyourfrontyard. Over
fectionatebehavior,suchasrubbingagainstlegs recentmonths,thiswhiskeredwandererhasturned
andheadbuttinghands,untilitisfed.Aftereating, your doorstep into its personal stage, meowing
thecattypicallyleavesbutreturnsintheevening. forattentionwithapersistencethattugsatyour
Theauthorisseekingadviceonhowtohandlethe heartstrings.
situation,expressingconcernaboutfeedingthecat Eachencounterwiththisaffectionatecreatureis
becauseitisnotlegallytheirsandtherecouldbe adanceofmutualadmiration. Youfindyourself
potentialownershipandlegalissues. captivated by its soft fur and the way it eagerly
rubsagainstyourlegs,headbuttingyourhandsina
silentpleaforaffection.[TRUNCATED]
ThePuertoPlataProvinceisanorthernprovincein (bulletpoints,parallelstructure,brief)
theDominicanRepublic,knownforitsbeautiful -Location:NorthernDominicanRepublic
beachesandasapopulartouristdestinationsince -Capital:SanFelipedePuertoPlata
thelate1990s.ItscapitalisSanFelipedePuerto -Founding:1502byEuropeans
Plata,namedafterthesilvercloudsColumbussaw -Historicalsite:LaIsabela(1493,firstEuropean
onamountainnearbyin1493.ThecityofPuerto townintheAmericas)
Platawasfoundedin1502. -1605:DestructionbyGovernorAntoniodeOso-
Historically,thefirstEuropeantownintheAmeri- riotostopillegaltrade
cas,LaIsabela,wasestablishedbyColumbusnear -Status:BecameMaritimeDistrictin1865,recog-
PuertoPlatain1493. In1605,thegovernorAn- nizedasprovincein1907
toniodeOsoriodestroyedPuertoPlataandother -Expansion:Addedmoremunicipalities,latestin
townstocurbillegaltrade,leadingtotheformation 2007
ofnewsettlements. - Borders: Atlantic Ocean, Espaillat, Santiago,
PuertoPlatabecameaMaritimeDistrictin1865 Valverde,MonteCristiprovinces
andwaslaterrecognizedasaprovinceinthe1907 [TRUNCATED]
constitution. Over time, it expanded to include
moremunicipalitiesandmunicipaldistricts,with
thelatestadditionsin2007.[TRUNCATED]
19Thearticlediscussesthechallengeofdetectingand (tweetstyle,simpleEnglish,inquisitive,skillful
classifyingstutteredspeech,whichaffectsmany foreshadowing,withemojis)
people but has been subject to limited research. Everstruggledwithstutteringorknowsomeone
Theauthorspresentanewmodelthatdiffersfrom who does? Research on detecting stutters
most existing methods, which typically use lan- hasn’tbeenhuge,butguesswhat?Anewstudyis
guage models for stutter identification. Instead,
changingthegame! Insteadofjustlookingat
theirmodelisbasedonacousticfeatures,which
words,thissmartmodellistenstohowthingsare
allowsittoidentifyvariouskindsofstutteringdis-
fluencies without relying on speech recognition said. It’slikeadetectiveforsounds,usingcool
technology.Theproposedmodelcombinesadeep techcalleddeeplearningandmemorynetworksto
residualnetworkwithbidirectionallongshort-term spotdifferentstuttertypes. Andit’scrushing
memory(LSTM)layerstoclassifydifferentstutter it,beatingoldmethodsby27%!Missedstutters?
types. Ithasbeenshowntosignificantlyoutper-
Downtojust10%! Thiscouldbeabigdealfor
formcurrentleadingmethods,reducingtheaver-
agemissrateto10.03%,whichisa27%improve- speechtherapy! #Stuttering#SpeechTech#
mentoverthestate-of-the-art. Innovation
Thearticlediscussesamoviethatbeginspromis- (questionansweringstyle,direct,concise)
inglybutlaterbecomesconfusingduetoapoorly Whatisthegeneralsentimentofthearticleabout
depictedportrayalofancientblackmagicrituals. themovie?
Themaleleaddeliversastrongperformancede- Thearticlesuggeststhatthemoviestartsoffwell
spitehischaracter’sunfavorableoutcomeinthe buteventuallybecomesmuddled.Thedepictionof
climax. The movie is compared to a mix of ancientblackmagicritualsiscriticizedforbeing
"Boomerang"and"Extremities." confusing.
Howdoesthearticledescribethemalelead’sper-
formance?
Themalelead’sperformanceisdescribedasstrong,
eventhoughhischaracterdoesnothaveafavorable
outcomeintheclimax.
Whatfilmsisthemoviecomparedtointhearticle?
The movie is compared to ""Boomerang"" and
""Extremities""inthearticle.
Table6: PrompttemplatesfortheAIuser. Thefirststepistoprompttheuserforyes/noanswer
regardingsatisfaction. Iftheanswerisno,thesecondstepistoasktheuseredittheagentoutput
accordingtothelatentpreference. Iftheanswerisyes,theagentoutputreceives0edits.
Summarization EmailWriting
Step1 Article:{user-providedarticle} Notes:{user-providednotes}
Summary:{agent-generatedsummary} Email:{agent-generatedemail}
Is the above summary of the above article Istheaboveemailbasedontheabovenotesgood
goodforpersonwhowouldlovetousethe forauserwhowantsthefollowingstyle:{latent
followingstyle: {latentuserpreference}? userpreference}?Pleaseansweryesorno.
Pleaseansweryesorno.
Step2 Summary:{agent-generatedsummary} Email:{agent-generatedemail}
Pleaserevisetheabovesummaryofanarticle Assumethatyouprefer{latentuserpreference}.
tomeetyourstyle:{latentuserpreference}. Pleaserevisetheaboveemailtomeetyourstyle.
20Table7: PrompttemplatesforCIPHER.
Summarization EmailWriting
Task prompt Article:{user-providedarticle} Notes:{user-providednotes}
conditioned Assumethatyouneedtosummarizetheabove Thesenotesarewrittenbyauserwhoprefers
on inferred article for a user, who prefers the following thefollowingstyleofemails: {inferreduser
preference style:{inferreduserpreference}.Pleasewrite preference}.Pleasewriteashortemailbased
(line6inAl- asummaryoftheabovearticletoaddressthose ontheabovenotestoaddressthosespecified
gorithm1) specifiedpreferences. preferences.
Prompttoin- Original summary of an article: {agent- Originalemail:{agent-generatedemail}
feruserpref- generatedsummary} Revisedemail:{userrevision}
erencebased Revisedsummarybyauser:{userrevision} Basedontheeditsandrevisionbythisuseron
onrevision Basedontheeditsandrevisionbythisuseron theoriginalemailintheaboveexamples,what
(line12inAl- theoriginalsummaryintheaboveexamples, doyoufindaboutthisuser’sgenericpreference
gorithm1) whatdoyoufindaboutthisuser’sgenericpref- intermsofwritingstyleandformatting?Please
erenceintermsofwritingstyleandformatting? answerinashortphraseandonlyrecommend
Pleaseanswerinashortphraseandonlyrecom- thosepreferencesthatarewidelyused.
mendthosepreferencesthatarewidelyused.
Prompt to Listofuserpreferencessuccessfullybeingused Listofuserpreferencessuccessfullybeingused
consolidate togeneratesummariesofsimilardocuments: togenerateemailsofasimilarkind:
inferred -{inferredpreferenceinaretrievedexample} -{inferredpreferenceinaretrievedexample}
preferences -{inferredpreferenceinaretrievedexample} -{inferredpreferenceinaretrievedexample}
fromhistory ... ...
(line5inAl- Basedonthetheaboveexamples,pleasecome Basedonthetheaboveexamples,pleasecome
gorithm1) upwithshortphrasewiththemostrepresented upwithshortphrasewiththemostrepresented
summarizationpreferencesoftheuser. writingpreferencesofthisuser.
Table8: PrompttemplatesfortheICL-editbaseline.
Summarization EmailWriting
Prompt with Original summary of an article: {agent- Original summary of an article: {agent-
retrieved generatedsummaryinaretrievedexample} generatedsummaryinaretrievedexample}
user edit Revisedsummarybyauser:{userrevisionin Revisedsummarybyauser:{userrevisionin
examples aretrievedexample} aretrievedexample}
Original summary of an article: {agent- Original summary of an article: {agent-
generatedsummaryinaretrievedexample} generatedsummaryinaretrievedexample}
Revisedsummarybyauser:{userrevisionin Revisedsummarybyauser:{userrevisionin
aretrievedexample} aretrievedexample}
... ...
Article:{user-providedarticle} Notes:{user-providednotes}
Basedontheeditsandrevisionbythisuseron Basedontheeditsandrevisionbythisuseron
theoriginalsummaryintheaboveexamples, theoriginalemailintheaboveexamples,Please
Pleasesummarizetheabovearticle: writeanemailbasedontheabovenotesforthis
user:
Table9: Expenseofdifferentmethods: numberofBPEtokensintermsofinput,outputandtotal.
Eachnumberistheaverageacross3runs(unitis·105).
Method Summarization EmailWriting
Input Output Total Input Output Total
OraclePreference 1.14 0.53 1.67 0.91 0.71 1.62
NoLearning 1.06 0.44 1.50 0.85 0.80 1.65
E-then-eLPI 1.16 0.83 1.99 0.94 0.79 1.73
ContinualLPI 8.14 0.75 8.89 7.89 0.73 8.63
ICL-edit-5-MPNET 7.35 0.65 8.00 11.05 1.06 12.12
ICL-edit-5-BERT 7.32 0.64 7.96 10.51 1.03 11.55
CIPHER-1-MPNET 2.02 0.72 2.74 1.21 0.73 1.94
CIPHER-5-MPNET 2.27 0.73 3.00 1.44 0.64 2.09
CIPHER-1-BERT 2.10 0.71 2.81 1.27 0.73 1.99
CIPHER-5-BERT 2.32 0.71 3.03 1.48 0.73 2.22
21Table10: SummaryoffailurecasesonsummarizationtaskwithCIPHER-5-MPNET.
TypeofFailures Summary Examples
Preference inference 1)Nottotallywrongbutin- Thedominantmissingaspectisbrief orshortsen-
based on an output- sufficient, i.e. the inferred tencesacrossdifferentcontext,althoughtheagent
revisionpair(f ) preference only captures a can infer keywords such as concise. For news
t
(themostcommonfail- fewaspectsofuser’slatent articlecontext,theagenttendstoinfertheprefer-
uretype) preference. This is most encekeywordwhimsical.Theagenthasdifficulty
common for news articles toinfersubtleaspects,includinginvokepersonal
andRedditposts,forwhich reflection,immersive,positive,parallelstructure,
theusershowsnuancedpref- inquisitive,andskillfulforeshadowing.
erenceforseveralaspects.
2) Sometimes fail to in- Theagentoftencouldnotinfersecond-personnar-
fersomeimportantaspects, rative. For question answering style, the agent
even though the user edits occasionallyonlylearnsconsistentformat.
clearly show such prefer-
ence.
Consolidation of Overall, this step can cap- Whenbothspecificphrasesecond-personnarra-
induced preferences turethemajoritypreference tiveandgeneralphrasenarrativeornarrationoc-
fromretrievedinterac- relatively well, although it curinretrievedexamples,theagentoftenchooses
tions(f˜) tendstoresultinamoregen- togiveafinalpreferencenotincludingthesecond-
t
eralpreferencecomparedto personperspectiveaspect.
theretrievedones.
Retrievalofhistorical Theretrievalpartingeneral The agent sometimes retrieves wrong examples
examples relevant to worksreasonablywell,with forWikipediacontextwhenitstopicveryrelates
thegivencontext more than half of the re- toothercontext,e.g. wronglyretrievingpastex-
trievedexamplebeingtruly amplesonnewsarticlesandmoviereviewswhen
relevant to the given con- thetopicinthegivenWikipediacontextrelatesto
text. Note that one incor- thesedomains.
rect retrieved example typ-
ically does not affect the
performance,asweinstruct
the agent to only use the
mostrepresentedpreference
keywordsamongallfivere-
trievedexamples.
Table11: Wereportretrievalaccuracyasthepercentageoftotalretrieveddocumentrepresentations
acrossalltimestepsandseedsthatareofthesamedocumentsourcetypeasthecontextdocumentfor
whichtheywereretrieved. Weuse3seeds. Weretrieve600examplesfork =1and2970examples
fork =5.
Method Summarization EmailWriting
CIPHER-1-B 72.00 25.83
CIPHER-1-M 82.00 26.33
CIPHER-5-B 65.79 26.57
CIPHER-5-M 76.33 25.45
22