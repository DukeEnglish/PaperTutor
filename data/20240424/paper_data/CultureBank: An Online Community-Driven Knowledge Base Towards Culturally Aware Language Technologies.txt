Preprint. Underreview.
CultureBank: An Online Community-Driven Knowledge
Base Towards Culturally Aware Language Technologies
WeiyanShi1,RyanLi1,YutongZhang1,CalebZiems1,ChunhuaYu1,
RayaHoresh2,Roge´rioAbreudePaula2,DiyiYang1
StanfordUniversity1,IBMResearch2
{weiyans,lansong,yutongz7,cziems,syu03}@stanford.edu
rhoresh@us.ibm.com, ropaula@br.ibm.com, diyiy@stanford.edu
Abstract
Toenhancelanguagemodels’culturalawareness,wedesignageneraliz-
ablepipelinetoconstructculturalknowledgebasesfromdifferentonline
communitiesonamassivescale. Withthepipeline,weconstructCulture-
Bank,aknowledgebasebuiltuponusers’self-narrativeswith12Kcultural
descriptorssourcedfromTikTokand11KfromReddit. Unlikeprevious
culturalknowledgeresources,CultureBankcontainsdiverseviewsoncul-
tural descriptors to allow flexible interpretation of cultural knowledge,
andcontextualizedculturalscenariostohelpgroundedevaluation. With
CultureBank,weevaluatedifferentLLMs’culturalawareness,andidentify
areasforimprovement. Wealsofine-tunealanguagemodelonCultureBank:
experimentsshowthatitachievesbetterperformancesontwodownstream
cultural tasks in a zero-shot setting. Finally, we offer recommendations
basedonourfindingsforfutureculturallyawarelanguagetechnologies.1 2
1 Introduction
Figure 1: Overview. Our goal is culturally-aware language technologies. To do so, we
developapipelineandconstructCultureBankwithstructuredculturaldescriptors. Each
descriptorcomeswithagroundedscenario,persona,andquestiontohelpevaluateLLMs.
Wefine-tuneamodelonCultureBankandimproveitsperformanceontwoculturaltasks.
“Globally, people express pride, celebrate, and respect cultural diversity, while
acknowledgingandworkingtowardsreducingculturalbias”
—CultureBank
Large Language Models (LLMs) have become instrumental in various applications to
interactwithdiverseuserpopulations,suchasinrecommendersystems(Lietal.,2023;Fan
etal.,2023)andcustomerservice(Pandya&Holia,2023). However,thesemodelsoften
1WereleasetheCultureBankdataset,code,andmodelsatgithub.com/SALT-NLP/CultureBank.
2Ourprojectpageisatculturebank.github.io.
1
4202
rpA
32
]LC.sc[
1v83251.4042:viXraPreprint. Underreview.
mirrorWestern-centricperspectives(Santurkaretal.,2023;Durmusetal.,2023b),asthey
arepredominantlytrainedondatathatreflectthesevaluesandbehaviors. Suchcultural
biascanleadtounintendedconsequences(Ryanetal.,2024),e.g.,reinforcingstereotypes,
alienatingnon-Westernusers,hinderingglobaldeploymentandsoon.Therefore,itbecomes
increasinglyimportanttodeveloplanguagetechnologiesthatareawareofdiversecultures.
To enhance LLMs’ culture3 awareness, existing studies have developed cultural knowl-
edgedatabasestorepresentculture-relatedknowledgeandnorms,buttheyhaveseveral
limitations. (1)TheyoftenrelyonformalknowledgesourceslikeWikipediaandonline
articles(Nguyenetal.,2023;Fungetal.,2024),whichmisstherich,evolvingandlong-tailed
culturalnuancesexperiencedbylocalcommunities. (2)Secondly,thesemethodstendto
presentculturalknowledgeinanassertivemanner(Nguyenetal.,2023;Fungetal.,2024;
Yinetal.,2022),failingtocapturethefactthatculturalpracticesandvaluescanvaryamong
individualswithinthesameculturalgroup. (3)Besides,theirevaluationmethodsoftenrely
onclassificationtasksandquestionanswering(Naousetal.,2023;AfinaPutrietal.,2024;
Shafayatetal.,2024),whichisverydifferentfromhowLLMsaredeployedintherealworld
andhencecannotreflecttheirculturalawarenessinpractice.
Totacklethesechallenges,weutilizeonlinecommunitieswherepeoplesharetheircultural
experiences,anddevelopabottom-upapproachtoprocessnoisyself-narrativesonamas-
sivescale. Usingthispipeline,wedevelopCultureBank,aculturalknowledgebasewith12K
culturaldescriptorssourcedfromTikTok(Figure1showsoneexample). Besides,toaddress
thelimitationonassertiveness,wegatherdiverseviewsonsimilarculturalpractices,and
calculateanagreementleveltoenableinclusiveculturalunderstanding. Moreover,tofacili-
tatecontextualizedevaluationonLLMs’culturalawareness,weprovidearelatedsituation
grounded in real-world settings for each cultural descriptor (e.g., travel consultation in
Figure1). Thenweevaluatestate-of-the-artLLMs’culturalawarenessonCultureBank,and
theresultsshowroomforimprovement. Additionally,wedemonstratethattrainingLLMs
onCultureBankenhancestheirperformanceondownstreamculture-relatedtasks. Wealso
show that our pipeline can be easily generalized to Reddit, another online community,
illustratingitstransferabilityandpotentialforfutureexpansions.
Tosummarize,wemakethefollowingcontributions.
• Ageneralframeworktocollectculturalknowledgefromonlinecommunities(§4)
• CultureBank,anopen-sourceculturalknowledgebasewith12Kculturaldescriptors
fromTikTokand11KfromReddit(§5and§8).
• Grounded evaluation on existing LLMs’ cultural awareness (§6) and a more
culturally-awarelanguagemodelfine-tunedonCultureBank(§7)
2 RelatedWork
Cultural knowledge bases. There have been many cultural knowledge base efforts in
differentdomains(Leeetal.,2023;Kimetal.,2024;Jinetal.,2023;Fungetal.,2024). With
traditionalethnographicmethods,socialscientistsrecordedculturalknowledgethrough
existing historical accounts, ethnographic data, and cultural documents. For instance,
behavioral scientists compiled a collection of cultural materials, and released an online
databasenamedeHRAF(theHumanRelationsAreaFiles)(eHR).Incomputersciencestud-
ies,researchersemploycomputationalmethodstoautomaticallyconstructdatasets(Penta
etal.,2011)fromlargesourcesorcuratedatafromcrowdsourceworkers(Leeetal.,2023).
Nguyenetal.(2023)builtapipelinetoextractassertiveculturalcommonsenseknowledge
fromC4(Raffeletal.,2020),alargecollectionofInternetdata,andFungetal.(2024)used
Wikipediaandnavigatedtorelatedonlinedocumentstoextractculturalknowledge. Data
fromthesesourcesaremuchcleanercomparedtoonlinecommunities,andoftenfocusmore
onnormativeculturalindicators. Sincecultureishighlyheterogeneous,wealsoneedde-
scriptiveculturalexpressionsfromsourceslikeonlinecommunities. StereoKG (Deshpande
3Weacknowledgethatcultureisabroadconcept,andpriorworkhasattemptedtooperationalize
cultureviadifferentproxies. Weuseculturetorefertotheknowledgesharedbyarelativelylarge
groupofpeoplefromdifferentbackgroundsabouttheirsharedbeliefs,practicesandbehaviors.
2Preprint. Underreview.
etal.,2022)usedRedditandTwittertoextractculturalstereotypesfor5religiousgroupsand
5nationalities,butduetothelackofproperfiltering,theresultsarenoisy. Asanimportant
complementtoexistingdatasources,ourworkproposesapipelinetoprocesshighlynoisy
onlinecommunitiesdataonalargescale,andshowthatitcanbeeasilygeneralizedacross
differentplatforms,toprovidevaluabledescriptiveculturalknowledge.
Cultural-awarenessinlanguagemodels. Previousworkshavestudiedculturaldimensions
inlanguagemodels(Gutie´rrezetal.,2016;Ramezani&Xu,2023;Jiangetal.,2020;Adewole
etal.,2021;Yaoetal.,2023;Lietal.,2024;Caoetal.,2023;Liuetal.,2021;Ha¨mmerletal.,
2022;Huang&Yang,2023;Wangetal.,2023;Ko¨ksaletal.,2023). Ontheevaluationside,
priorstudieshavemeasuredsubjectiveglobalopinionsfromLLMs (Durmusetal.,2023a;
Santurkaretal.,2023),andprobedculturalvaluedifferencesinthesemodels (Aroraetal.,
2022;Yinetal.,2022;Robertsetal.,2023). Onthemodelside,CultureLLM(Lietal.,2024)
proposed a cost-effective method to integrate cultural differences into language models
withaugmenteddata. Thisworkproposesagroundedwaytoevaluateculturalawareness
tomatchreal-worldusecases,andfine-tuneamoreculturallyawarelanguagemodelwith
descriptiveculturalbehaviorsconstructedfromonlinecommunities.
3 CultureBankTaxonomy
Prior efforts on cultural knowledge base (Nguyen et al., 2023) often represent cultural
knowledgeinfree-textsentences. Butfree-textcontentsononlinecommunitiesareoften
noisy,andsuchanunstructuredrepresentationhindersfurthercomputationaloperation
suchassearchandfilter. Therefore,wedevelopataxonomy(showninTable1)formore
structured cultural knowledge representation, based on the taxonomy of social factors
(Hovy&Yang,2021),andthetaxonomyofsocialnorms(Ziemsetal.,2023;Goffmanetal.,
2002). It has the following fields: (1) cultural group, (2) context, (3) goal, (4) actor, (5)
recipient,(6)relation,(7)actor’sbehavior,(8)recipient’sbehavior,(9)otherdescription,
(10)topic,and(11)agreement. Forallthesefields,weprovidein-contextexamplesandlet
themodelextractanyrelatedinformationwithoutconstraint. Thisallowsdiversityand
inclusivityinthedata: forinstance,examplesforculturalgroupincludetypicalcultural
groupsbycountriessuchas“American”,aswellasmorefine-grainedonesbyregionsor
ethnicitygroupssuchas“Californian”and“AsianAmerican”,andmorebroadsocialgroups
suchas“internationalstudents”whichcanbeoverlookedbefore(Barth,2010;Stenou,2002).
Field Definition Example
Culturalgroup groupsofpeoplewithsimilarcultural American,Californian,AsianAmerican,inter-
backgrounds nationalstudent
Context settingsthebehaviortakesplace inFrance,inpublic,4thofJulycelebrations
Goal whatthebehavioraimstoachieve toadapttodifferentcultures,tocelebrate
Actor whoexhibitthebehavior people,customers,drivers
Recipient recipientoftheaction kids,servicestaff,passengers
Relation relationbetweentheactorandthere- parentstochildren,actortoaudience,among
cipient friends
Actor’sbehavior behavioroftheactor dresscasually,tiptoexpressgratitude
Recipient’sbehavior behavioroftherecipient respondwiththanks,acceptcardpayments
Otherdescription anythingthatcannotfitintotheother Bangkokisknownforitschaotictraffic
fields
Topic topic educationandtechnology,culturalexchange
Agreement agreementlevel,%ofpeoplewhoagree anone-decimalfloatbetween0and1,like0.6
Table1: Fields,definitionsandexamplesintheCultureBanktaxonomy.
4 ConstructionPipeline
Centeringontheproposedtaxonomy,weproposeabottom-uppipelinetoconstructcultural
descriptorsfromonlinecommunities. Figure2givesanoverviewofthepipelinewhich
3Preprint. Underreview.
hasthreeparts: (1)descriptorextraction,(2)descriptorclustering,and(3)descriptorpost-
processing. SeeSectionCformoreimplementationdetails.
1. Descriptor Extraction 2. Descriptor Clustering 3. Post-processing
2.1 Clustering 3.1 Agreement Calculator
Comments on Online Communities
0 1
“I was traveling in Japan and left a tip as Agreement
1 1
thanks, but the owner returned it!”
Agreement Level = 0.7
1.1 Cultural Relevance Classifier
3.2 Content Moderation
1.2 Cultural Descriptor Extractor
2.2 Cluster Summarizer Perspective API
Cultural Group: Japan Context: In Japan
Score < 0.2
Goal: Express gratitude Agreement: 0 Classifier
Actor: Customers Recipient: Service staff
HumanAnnotation
Actor's Behavior: Attempt to tip Representative
Summary
Topic: Social Norms and Etiquette PII Info
Human Evaluation Human Evaluation Human Evaluation
Figure2:CultureBankconstructionpipeline.Startingfromcommentsononlinecommunities,
wewill(1)selectculture-relatedcommentsandextractmentionedculturaldescriptors,then
(2)clusterthesedescriptorsandsummarizetheclusters,andfinally(3)post-processthemto
getagreementvalueandremovebadcontents. Eachstepisvalidatedbyhumanevaluation.
4.1 Descriptorextraction
Inthefirstpartofthepipeline,weextractculturaldescriptorsfromself-narrativedatalike
commentsandpostsononlinecommunities,andorganizethemintoourtaxonomy.
Culturerelevanceclassifier. Giventhelargeamountsofnoisydata,thefirststepistoget
theculturally-relevantportion. Todoso,weannotateasubsetwith280trainingexamples,
and trained a distill-bert-based (Sanh et al., 2019) cultural relevance classifier. Then the
classifierisappliedontheentiredatasettogetthesubsetrelatedtoculture. Theclassifier
achievesanaccuracyof79%onaheldouttestsetwith100examples.
Culturaldescriptorextractor. Afterobtainingtheculturalsubset,weemployLlama-2-70B
(Touvronetal.,2023),oneofthebestopen-sourceLLMsatthetasktime,toextractvalues
foreachfieldinourtaxonomy,byconditioningonthedefinitionoffieldsandin-context
examples. Listing1showsthepromptused. Humanevaluationshowsthatthisextractor
achievesanaccuracyof82%acrossfieldsinthetaxonomyonatestsetwith240examples.
4.2 Descriptorclustering
Aftertheextractionstep,wehavemanyculturaldescriptors,butthesameculturalbehavior
canbeexpressedinmanydifferentways,forinstance,“Japanesedonottipservicestaff”,or
“InJapan,peopledonotgivetips”. Sonaturallyinthesecondpart,weneedtofirstcluster
theextracteddescriptors,andsummarizeeachclusterafterwards.
Clustering. Fortheclusteringstep,weconcatenatetheextractedfields,encodetheconcate-
natedcontentswithSentenceBert(Reimers&Gurevych,2019),andperformHierarchical
AgglomerativeClustering(HAC)clustering. Weusetheclustersizeasthesupportvalue
andremoveclusterswithlessthan5datapointstoensureenoughsupportingevidences.
Theclusteringparametersarechosenbasedontheperformanceonavalidationset,andthe
clusteredresultsachieveanaverageSilhouettescoreof0.14withintheclusters.
Clustersummarizer. Afterclustering,eachclustercontainsmultipleculturaldescriptors,so
thenextstepistosummarizeandgeneratearepresentativedescriptorforeachcluster. We
useMixtral-8X7B(Jiangetal.,2024),astate-of-the-artopen-sourcelanguagemodelatthetask
time,tosummarizeeachcluster. Sincetheclusterscontainnoisyopinions,thevanillamodel
oftenfailstooutputacomprehensivesummarizationwithin-contextexamples. Toachievea
4Preprint. Underreview.
betterperformance,weaskGPT-4togenerate1Khigh-qualitysummarizations,anddistilled
thosesamplestotrainourownMixtralsummarizer. Listing2showsthepromptusedfor
thesummarizer. Humanevaluationshowstheclustersummarizerachievesafidelityscore
of89.7%andcoherencescoreof96.6%. Definitionsofthesemetricsareavailablein§C.2.
4.3 Post-processing
Thefinalstepistopost-processtheclustereddata.
Agreement calculator. People may have different opinions regarding the same cultural
behaviors,soinsteadofassertivestatements,weprovideagreementlevelsforeachcultural
descriptorinourCultureBank. Eachclusternowcontains≥5datapoints,andeachpointis
associatedwithanagreementscoreof0or1,sowecomputetheaverageoftheseagreement
scoresastheagreementlevel. Besides,theclustersizecanalsoreflecttheagreementlevel.
Contentmoderation. Finally,onlineplatformscancontaincontroversialcontents. Sothe
laststepiscontentmoderation. Todoso, wefirstusetheperspectiveAPI4,amachine-
learning-basedcontentmoderationtool,andfilteroutcontentswithscoresabove0.2for
everycategory(toxicity,profanity,insult,identityattack,threat,severetoxicity). Formore
nuancedcontroversialcontents,weannotate800examplesandtrainadistill-bert-based
classifier(testacc=0.77on117examples),andemployalistofkeywordstofurtheridentify
them. Next,wemanuallylabeltheseidentifiedcontents,andremovebadones. Finally,we
usethePresidioAnalyzer5todetectandremovePersonalIdentifiableInformation(PII).
5 CultureBankDataset
TikTokisapopularsocialmediaplatformwithusersfromdiverseculturalbackgrounds,
soweapplyourpipelineondatafromTikToktoconstructourCultureBank dataset. We
obtainTikTokdataviatheirofficialresearchAPI6andcollectatotalof34Kpostsand720K
Englishcommentsfrom2019/05to2023/08withthehashtags“#culturaldifference”and
“#cultureshock”. Table2showsCultureBankbasicstatisticsafterconstruction: forTiktok,
thereare12Kculturaldescriptors,730culturalgroups,and36topics. Table9showsthe
topicdistribution. Table7showstherunningtimeandthedatavolumeaftereachstep.
Statistics TikTok Reddit Metrics TikTok Reddit
#culturaldescriptors 11,754 11,236 Well-formatted 98.5% 95.5%
#culturalgroups 730 1,850 Traceable 93.3% 94.0%
#culturaltopics 36 36 Meaningful 84.5% 85.0%
Table2: CultureBankbasicstatistics. Table3: AnnotatedCultureBankquality.
Toassessthedatasetqualityquantitatively,weselectarandomsubsetwith200samples,
andfourhumanannotatorsannotatedthemfortheir(1)format(ifthedescriptoriswell-
formatted),(2)traceability(ifitispossibletotracetheculturalknowledgeontheInternet)
and (3) meaningfulness (if the descriptor provides meaningful cultural insights rather
than generic ones). We evaluate them on traceability instead of factualness, since these
descriptorsareself-reportedandmaybenuanced,soitisdifficulttofact-checkthem;as
longasthereisrelatedinformationonline,weconsiderittraceableandmeaningfultobe
included. TheannotatorsachievedaKappascoreof0.8. Table3showsthatCultureBankhas
well-formatted,traceableandmeaningfulculturaldescriptorswithmoderatenoiselevels.
Table6showsqualitativeexamplesinCultureBank. Itpresentsinterestingfeatures,such
as: (1)cross-culturebehaviors: e.g.,AmericansinFranceexperiencecultureshockintermsof
electricitybillsanddrivinghabits;(2)linguisticsvariations: e.g.,Americansuse“chickpeas”
or“garbanzobeans”interchangeably;(3)diverseethnicgroups:e.g.,ItalianAmericansidentify
themselvesasItalianAmericanwithvaryingconnectiontoItalyheritage;(4)recentcultural
4https://perspectiveapi.com/
5https://microsoft.github.io/presidio/analyzer/
6https://developers.tiktok.com/products/research-api/
5Preprint. Underreview.
information: e.g.,Chinesepeopleheavilyrelyonmobilepayment;and(5)culturalnuances
hardtoobtainfromformalsourceslikeWikipedia:e.g.,inSouthAfrica,somepeopleexpress
frustrationoverhavingtocalculatepricesandtaxesseparatelywhileothersdonotthinkso.
Forthefollowingevaluation(§6)andfine-tuning(§7)steps,wesplitCultureBank-TikTokby
culturaldescriptorsinto9402train,1183validation,and1169testsamples.
6 EvaluatingLLMs’CultureAwareness
WithCultureBank-TikTok,weevaluateLLMs’culturalawareness. PriorworkasksLLMsto
answerculturaltrue/falsequestions(Fungetal.,2024).ButLLMsareusedincontextualized
settingslikeadialogueagent. Soweproposeagroundedevaluation,thatgroundscultural
knowledgeinareal-worldscenario,totestLLMs’abilitytointegrateculturalknowledge
intotheirresponses. Wealsoperformclassification-baseddirectevaluationin§D.2.
Groundeddatageneration. ForeachdescriptorinCultureBank,wefirstuseaMixtral-8x7B
modelfine-tunedonGPT-4-generatedexamplestogeneratearelevantconsultingscenario,
aclientpersona,andagroundedevaluationquestion. Thenweemployaself-refinement
methodtoimprovethemodelgenerationbasedontwoquality-controlmetricsatinference
time. Figure3showsageneratedexample(Forthe”NotippinginJapan”descriptor,the
grounded question is ”what gesture says ’thanks you’ in Japan?”). Human annotation
shows86%questionsaregroundedontheoriginaldescriptor. See§D.3formoredetails.
Groundedevaluation. AsshowninFigure3,wepresentthegeneratedgroundedquestion
totheLLMforananswer. Giventheanswer,weperform(1)automaticevaluationthatuses
GPT-4tojudgeiftheanswerentailstheoriginalculturaldescriptor(entailmentscore);and
(2)humanevaluationwheretwoexpertscompareanswersfromtwoLLMsandselectthe
moreculturally-awareone(winrate).
Figure3: Workflowofgroundedevaluation. WepresentthegroundedquestiontoanLLM
andgetananswer. Giventheanswer,weperformautomaticevaluationandhumanevaluation.
Win, Tie, and Lose Rates by Model
Model High Mid Low All
Mixtral-SFT (Ours) vs Mixtral-Vanilla
Llama-2-7B-chat 71.2 66.0 61.2 62.5
34% 48% 18%
Llama-2-70B-chat 74.9 66.2 64.2 65.1
Mistral-7B-Instruct 72.9 67.2 63.4 64.5 Mixtral-DPO (Ours) vs Mixtral-Vanilla
Mixtral-8x7B-Instruct 73.9 67.4 66.3 66.9 36% 50% 14%
GPT-3.5 71.4 66.4 61.8 62.6
Mixtral-SFT (Ours) vs GPT4
GPT-4 75.8 67.9 65.0 66.1
46% 38% 16%
Llama2-7B-SFT(Ours) 75.7 67.1 63.8 64.7
Mixtral-DPO (Ours) vs GPT4
Mixtral-8X7B-SFT(Ours) 73.3 70.3 66.6 67.5
20% 76% 4%
Mixtral-8x7B-DPO(Ours) 72.4 70.5 68.1 68.7
Table4: AutomaticevaluationonLLMs’cul- 0% 20% 40% 60% 80% 100%
Win Rate Tie Rate Loss Rate
tural awareness, evaluated by knowledge
entailmentscoresonourgroundedevalua- Figure4:Humanevaluationonwinratesbe-
tionbenchmarkbysupport. Highsupport: tweendifferentLLMs(50examplesperpair)
clustersize>50(70examples).Mid:cluster
evaluatedbyhumansoncultural-awareness
sizebetween20and50(175examples).Low: ingroundedconsultingscenarios. Thetwo
clustersize≤20(924examples).
annotatorsachievedaKappascoreof0.87.
6Preprint. Underreview.
Evaluatedmodels. Weevaluateopen-source(Llama-2,Mixtral),close-source(GPTfamilies
(Achiametal.,2023)),andourownfine-tunedmodels(See§7). SeeTable10forthemodel
versiondetails. Alltheresultsareonthetestset.
Automaticentailmentresults. Table4showstheaverageentailmentscoreofeachmodel
splitbytheclustersize(levelofsupport). Mixtral-8X7BandGPT-4arethebestbutstillhas
arelativelylowoverallscoreof66.9and66.1,suggestingroomforimprovements. Larger
modelshaveslightlybetterperformancethantheirsmallerversions. Formorelong-tailed
culturaldescriptorswithfewersupports,theperformancedropsasexpected.
Human-evaluatedwinrate. Figure4showsthehumanevaluationresults. Comparedto
the base Mixtral model, Mixtral-DPO is strictly more culturally aware 36% of the time,
andequallygood49%ofthetime;comparedtoGPT-4,Mixtral-SFTwins46%ofthetime
andties38%ofthetime. ThistrendalsoalignswiththeautomaticevaluationinTable4,
indicatingthattheautomaticentailmentevaluationmakessense. Qualitatively(Table14),
wefindthatourfine-tunedmodelsgenerateshorter,andmoreculturallyspecificanswers
tailoredtotheuser’sinquiry(e.g.,“inFrance,youmightseekoutartisanalcheeseshops”),
whereasstandardRLHF-edmodelslikeGPT-4oftengivegenerictemplatedanswers(e.g.,
“1. Researchlocalspecialties,2. Portioncontrol...”).
§D.3.3hasmoredetailsonthehumanevaluation,models’winrates,andqualitativeanalysis.
7 Fine-tuningaMoreCulturallyAwareLanguageModel
Ourultimategoalistodevelopmoreculturally-awarelanguagetechnologies. Sowetrain
onourCultureBankdatasettoseeifsucharesourcecanimproveLLMs’culturalawareness.
Trainingprocess. Thetraininghastwosteps. First,wetrainamodelonthe9402cultural
descriptorsinthetrainingsetviasupervisedfine-tuning(SFT).Inthesecondstep,weselect
a2Ksubsetwherethemodelperformspoorly,andtrainonthegroundedquestionsand
answersaugmentedbygoldenculturaldescriptorsviaSFT(“model-SFT(Ours)”inthe
tables)orDPO(Rafailovetal.,2024)(“model-DPO(Ours)”). See§Eformoredetails.
Results. Table4showstheresultsonthetestset. Thetestsetmostlycontainsout-of-domain
culturaldescriptors,butthefine-tunedmodelscanstillachieveabetterperformancethan
theirvanillaversions,suggestingthatCultureBankcanimprovemodels’culturalawareness.
7.1 Zero-shottransferabilityondownstreamtasks
Ideally,amoreculturally-awarelanguagemodelcanachieveabetterperformanceondiffer-
entculture-relatedtasks. So,wealsoevaluatethefine-tunedmodelsontwodownstream
tasks,toseeifourCultureBankcanhelpotherculturaltasksinazero-shotfashion.
Downstreamtasks. Wechoosetwotasks: (1)GlobalOpinionQA(Durmusetal.,2023a),
whichhasquestionsfromworldvaluesurvey(Inglehartetal.,2000)tomeasurehowsimilar
LLM-generatedresponsesaretowardsdifferentcountries;(2)CultureNLI(Huang&Yang,
2023),whichcontainspremise-hypothesispairslabeledbytwoculturalgroups,American
andIndian. ThedetailedpromptsandevaluationsettingsareinAppendixE.2.
Results. Table5showstheresults. Onbothdatasets,themodelsfine-tunedonCultureBank
achievesabetterperformancethanthevanillacounterparts(e.g.,inGlobalOpinionQA,79.5
VS.81.8forMixtral-SFT;inCultureNLI,59.9VS.61.5intheUSand60.8VS.61.3inIndia
forMixtral-SFT).TheseresultssuggestthatCultureBankcanbeusedtoimprovemodels’
culturalawarenessindownstreamtaskseveninazero-shotsetting.
8 GeneralizingtoReddit
Therearedifferentonlinecommunities,soitisimportanttotestifourpipelinecanbetrans-
ferredtootherplatforms. SoweapplyourpipelineonReddit,anotheronlinecommunity,
withthefollowingcustomization. Table8showstherunningtimeanddatavolume.
7Preprint. Underreview.
GlobalOpinionQA CultureNLI
Model(zero-shot)
AvgSim(↑) Skew(↓) US(↑) IN(↑)
Llama-2-7B-chat 83.6 2.2 39.2 39.5
Llama-2-70B-chat 83.6 2.2 69.7 68.9
Mistral-7B-Instruct 79.3 3.2 42.5 43.8
Mixtral-8x7B-Instruct 79.5 2.7 59.9 60.8
GPT-3.5 - - 75.0 73.0
GPT-4 - - 80.0 72.0
Llama2-7B-SFT(Ours) 85.4 1.5 39.2 39.6
Mixtral-8X7B-SFT(Ours) 81.8 2.8 61.5 61.3
Mixtral-8x7B-DPO(Ours) 80.5 2.6 56.3 55.4
Table5: Zero-shotculturalawarenessonGlobalOpinionQAandCultureNLI.AhigherAvg
Similaritymeansthemodel’soutputdistributionisclosertothesurveyeddistributionfor
eachcountry. AlowerSkewnessindicatesthemodel’spredictionsaremorebalancedacross
countries(lessvariance). USandINshowtheF1scoreonUSandIndia. InGlobalOpin-
ionQA,GPTs’resultsareNAbecausewedonothaveaccesstotheirlogitdistributions.
• CultureRelevanceClassifier: similartoTikTok,wefirstsearchfortagslike”#cul-
turaldifference”onReddit,buttagsarenotusedfrequentlyonReddit,sowealso
directlysearchforculturalkeywordsonbothsubmissionsandcommentstoiden-
tifyculturalcontents. Seedetailin§F.AsRedditcommentsaremuchlongerthan
TikTokcomments,theTikTok-basedculturalrelevanceclassifierdoesnotworkwell.
Soweannotate1Kexamples,andtrainanewculturalrelevanceclassifierforReddit
to get the culturally-relevant portion from the keyword-curated subset. Finally,
weobtain2.6Mculturalcomments. Consideringthecomputationcost,wetakea
randomsubsetof528Kculturalcommentsforthefollowingprocessingsteps.
• DescriptorExtractor: toachieveabetterperformance,insteadofusingfew-shot
Llama-2extractor,wefine-tuneaMixtral-basedextractoron1KGPT-4-generatedex-
tractionexamplestoextractstructuredculturaldescriptorsfromRedditcomments.
Table2showsthebasicstatistics: CultureBank-Redditcontains11Kculturaldescriptorsand
2Kculturalgroups. HumanannotationinTable3showsthatitalsocontainshigh-quality
data,suggestingthatourpipelinecanbeeasilygeneralizedtoadifferentplatform.
9 RecommendationforCulturallyawareLanguageTechnologies
InformedbyresultsonCultureBankconstructionandanalysis,culturalawarenessevaluation,
andfine-tuning,weoutlineinsightstowardsfutureculturally-awarelanguagetechnologies.
9.1 Culturalknowledgedata
Weshowthatfine-tuningonCultureBankcanimprovethecultural-awarenessonvarious
downstreamtasks,soitremaincriticaltokeepdevelopingculturalknowledgedatabases.
Datasource. Priorworkoftenreliesonformaldatasourcesandcollapsedifferentsources
together: e.g.,Fungetal.(2024)startedfromWikiPediaandcontinuedtoscrapeanyrelated
websitestoconstructculturalknowledgebases. Butdifferentdatasourcescovervarious
aspectsofculture: officialdocumentsliketextbooksprovidefactualculturalknowledge,
whileonlinecommunitieslikesocialmediaofferinsightsoneverydayculturalpractices. So
weshouldinvitediversedatasourcestocapturethefullspectrumofcultureinthefuture.
Besides,differentdatasourceshostdifferentpopulations: Table9showsthattopic-wise,
8Preprint. Underreview.
CultureBank-Redditcontainsmorecontentsoncommunityandidentify,whileCultureBank-
TikTokismoreaboutdailylifelikesocialnormsandetiquette. Sofuturedatasetsshould
keepdatasourceasanimportantattributetoallowfurtheranalysis.
Datacontents. Cultureismultifaceted,soitisalsoimportanttofactorinvariousdimen-
sionsinthedatacontent. Hereisanexamplelistofattributestoconsider.
• Cross-culturebehavior. Inaglobalizedworld, itiscrucialtounderstandcross-
culturebehaviorstofacilitateeffectivecommunication(Watkins,2012). OurCulture-
Bankcontainssomecross-culturebehaviorsbutweneedmoreeffortsonit.
• Perspectives. Itisalsoimportanttotrackthroughwhoselenswearelookingat
a certain culture behavior, because different perspectives may lead to different
understandingofthesameculturalpractice(Iyengaretal.,1999;Brewer,1999).
• Time. Culturechangesovertime. InCultureBank,wereleasethetimerangeassoci-
atedwiththeculturaldescriptors. Futuredataeffortsshouldalsoconsiderthetime
factortoenabletemporalanalysis.
• Multilingual. Cultureandlanguagearedeeplyintertwined. Butmanyexisting
culturalknowledgebasesstillrelyonEnglish. Tocapturetheculturalnuances,in
thefuture,weshoulddevelopmultilingualmulticulturalknowledgebanks.
• Multimodality. Culturalknowledgegoesmuchbeyondtextinformation. Soitis
essentialtoincludedifferentmodalitiestocapturethefullspectrumofculture,from
non-verbalcommunicationcues,toritualsandarts,andsooninthefuture.
Dataanalysis. Intermsofdataanalysis,futureresearchshouldconsidertemporalchange
rather than focusing on static data, as culture is evolving over time. For instance, we
performpreliminarytemporalanalysisin§G.1andfindtherearemorediscussionsaround
studyingabroad,LGBTQ+rights,andtechnologyovertheyears. Besides,existingresearch
stillcategorizesculturebycountry,butweneedtoattendtomorefine-grainedcultural
groups(ethnicity,generation,regions,ethnolinguistics,immigrants,socioeconomics,etc),to
fullyunderstandculturaldiversity. Moreover,thestudyofculturaladaptationbecomes
increasinglyimportant,asitrevealshowculturechangesinresponsetoglobalinfluences.
Thesefocusareas–temporaldynamics,culturalgroupdiversity,andadaptationprocesses–
offersacomprehensiveunderstandingofthefluidnatureofcultureinaglobalizedworld.
9.2 Culturalawarenessevaluation
Wehighlighttwofindingsinevaluation.First,inourevaluation,humansalsofinditdifficult
todecidewhichmodelresponseismoreculturallyaware,partlybecausetheyarenotfrom
thepresentedculturalgroup. Aswespendmoreeffortonculturaldataresources,itisalso
increasinglyimportanttoinvolveglobalannotatorstoenablemoreaccurateevaluation.
Secondly,asshowninourfindings,directandgroundedevaluationsgivedifferentresults.
Soduringevaluation,itisimportanttobemoregroundedontheendapplications.
9.3 Trainingculturally-awarelanguagetechnologies
Werealizethatwhenfine-tuningmodelsforculturalawareness,trainingonlyonthecultural
knowledgeorthegroundedQAtaskscouldbeinsufficient. Taketrainingaculturally-aware
conversationalassistantasanexample. First,itrequiresappropriateculturaldatagrounded
in multi-turn conversational settings. In addition, it requires a well-designed training
paradigm to attend to the cultural nuances potentially implicit in the dialogue context.
It also needs a solid evaluation method to rate the culture awareness of the generated
responses,tohelpthemodelimproveandevolve. Suchamodelneedstohaveaholistic
viewoftheuserculturalbackground,apersonalizedrecognitionofindividualdifferences,
andaninclusivemindfornewculturalconceptsandpractices.
9Preprint. Underreview.
10 Conclusion
Toconclude,ourstudyintroducesageneralizablepipelineforcreatingculturalknowledge
bases from online communities. Using the pipeline, we develop CultureBank, a cultural
knowledge database with 12K cultural descriptors sourced from TikTok, and 11K from
Reddit. CultureBank features agreement levels for nuanced cultural interpretation and
contextualizedscenariosforgroundedevaluation. WithCultureBank,weassessthecultural
awareness of various LLMs, showing room for improvement. Further, fine-tuning an
LLMwithCultureBankleadstobetterperformanceondownstreamculturaltasks,which
showcasesthepotentialofCultureBank. Finally,drawingfromourfindings,weclosethe
paperbypresentinginsightstowardsfutureculturally-awarelanguagetechnologies.
EthicalStatement
In this work, we construct a cultural knowledge base from online communities. Given
thelargesizeofthedataset,weacknowledgethatstereotypes,controversial,andnegative
content may still exist in our dataset, despite our rigorous efforts to filter the data and
minimizetheimpactofsuchcontent. Wewanttoemphasizethattheculturaldescriptors
inCultureBankarenotintendedtoreflect,norshouldtheybeinterpretedasreflecting,the
personal views or opinions of the authors or the online platforms. We call for a better
approach for content moderation in the future and hope that researchers will use our
data with a discerning perspective, and always consider the broader implications of its
applicationandthepotentialforreinforcingharmfulbiases.
Wealsorecognizetheresponsibilitythatcomeswithhandlingculturaldata,especiallyfrom
diverseandbroadcommunitieslikethoseonTikTokandReddit. Inourmethod,wehave
strivednotonlyfortechnologicalinnovationbutalsoforaconsciousapproachthatrespects
thedignity,privacy,andculturalsensitivitiesofindividualsandgroupsrepresentedinthe
data. Thisincludesanonymizingdatawherepossible,ensuringcompliancewithplatform
termsofservice,andengagingwithethicalguidelinesthatgovernresearchinsocialsciences
andhumanities.
Inconclusion,whileweacknowledgethelimitationsandchallengesinherentinourwork,
webelieveinitspotentialtocontributepositivelytothefieldofculturally-awarelanguage
technology. Weencouragethecommunitytojoinusintheseefforts,topromotecultural
diversity,inclusivity,andsensitivity. Wediscusslimitationsofthisworkin§A.
Acknowledgement
WethankfeedbackfromChunchenXu,EmilyGoodwin,JingHuang,andmembersfrom
theSALTlabatStanfordUniversity. WealsothankTikTokforprovidingtheresearchAPI.
Stanfordprocessedtherawdatainternally. IBMprovideshigh-levelfeedbackandisnot
involvedinthedataprocessing.
References
URLhttps://ehrafworldcultures.yale.edu/.
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
Sodiq Adewole, Erfaneh Gharavi, Benjamin Shpringer, Martin Bolger, Vaibhav Sharma,
SungMingYang,andDonaldE.Brown.Dialogue-basedsimulationforculturalawareness
training,2021.
RifkiAfinaPutri,FaizGhifariHaznitrama,DeaAdhista,andAliceOh. Canllmgenerate
culturallyrelevantcommonsenseqadata? casestudyinindonesianandsundanese. arXiv
e-prints,pp.arXiv–2402,2024.
10Preprint. Underreview.
ArnavArora,Lucie-Aime´eKaffee,andIsabelleAugenstein. Probingpre-trainedlanguage
modelsforcross-culturaldifferencesinvalues. arXivpreprintarXiv:2203.13722,2022.
FredrikBarth. Introductiontoethnicgroupsandboundaries: Thesocialorganizationof
culturaldifference. Selectedstudiesininternationalmigrationandimmigrantincorporation,1:
407,2010.
MarilynnBBrewer. Thepsychologyofprejudice: Ingrouploveandoutgrouphate? Journal
ofsocialissues,55(3):429–444,1999.
Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich.
Assessingcross-culturalalignmentbetweenchatgptandhumansocieties: Anempirical
study,2023.
AwanteeDeshpande,DanaRuiter,MariusMosbach,andDietrichKlakow. Stereokg: Data-
driven knowledge graph construction for cultural knowledge and stereotypes. arXiv
preprintarXiv:2205.14036,2022.
EsinDurmus,KarinaNyugen,ThomasI.Liao,NicholasSchiefer,AmandaAskell,Anton
Bakhtin, CarolChen, ZacHatfield-Dodds, DannyHernandez, NicholasJoseph, Liane
Lovitt,SamMcCandlish,OrowaSikder,AlexTamkin,JanelThamkul,JaredKaplan,Jack
Clark, andDeepGanguli. Towardsmeasuringtherepresentationofsubjectiveglobal
opinionsinlanguagemodels,2023a.
EsinDurmus,KarinaNyugen,ThomasILiao,NicholasSchiefer,AmandaAskell,Anton
Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al.
Towardsmeasuringtherepresentationofsubjectiveglobalopinionsinlanguagemodels.
arXivpreprintarXiv:2306.16388,2023b.
WenqiFan,ZihuaiZhao,JiatongLi,YunqingLiu,XiaoweiMei,YiqiWang,JiliangTang,and
QingLi. Recommendersystemsintheeraoflargelanguagemodels(llms). arXivpreprint
arXiv:2307.02046,2023.
Yi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, and Heng Ji. Massively multi-cultural
knowledgeacquisitionandlmbenchmarking. arXivpreprintarXiv:2402.09369,2024.
ErvingGoffmanetal. Thepresentationofselfineverydaylife.1959. GardenCity,NY,259,
2002.
E Dario Gutie´rrez, Ekaterina Shutova, Patricia Lichtenstein, Gerard De Melo, and Luca
Gilardi. Detectingcross-culturaldifferencesusingamultilingualtopicmodel. Transactions
oftheAssociationforComputationalLinguistics,4:47–60,2016.
DirkHovyandDiyiYang. Theimportanceofmodelingsocialfactorsoflanguage: Theory
andpractice. InKristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-
Tur,IzBeltagy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou
(eds.),Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTechnologies,pp.588–602,Online,June2021.
AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.naacl-main.49. URL
https://aclanthology.org/2021.naacl-main.49.
JingHuangandDiyiYang. Culturallyawarenaturallanguageinference. InFindingsofthe
AssociationforComputationalLinguistics: EMNLP2023,pp.7591–7609,2023.
KatharinaHa¨mmerl,Bjo¨rnDeiseroth,PatrickSchramowski,JindrˇichLibovicky´,Alexander
Fraser,andKristianKersting. Domultilinguallanguagemodelscapturedifferingmoral
norms?,2022.
RonaldInglehart,MiguelBasanez,JaimeDiez-Medrano,LoekHalman,andRuudLuijkx.
Worldvaluessurveysandeuropeanvaluessurveys,1981-1984,1990-1993,and1995-1997.
AnnArbor-Michigan,InstituteforSocialResearch,ICPSRversion,2000.
11Preprint. Underreview.
SheenaSIyengar,MarkRLepper,andLeeRoss. Independencefromwhom? interdepen-
dencewithwhom? culturalperspectivesoningroupsversusoutgroups. Culturaldivides:
Understandingandovercominggroupconflict,pp.273–301,1999.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,Florian
Bressand,etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
ZhengbaoJiang,AntoniosAnastasopoulos,JunAraki,HaiboDing,andGrahamNeubig.
X-FACTR:Multilingualfactualknowledgeretrievalfrompretrainedlanguagemodels.
InBonnieWebber,TrevorCohn,YulanHe,andYangLiu(eds.),Proceedingsofthe2020
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.5943–5959,
Online,November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2020.emnlp-main.479. URLhttps://aclanthology.org/2020.emnlp-main.479.
JihoJin,JiseonKim,NayeonLee,HaneulYoo,AliceOh,andHwaranLee. Kobbq: Korean
biasbenchmarkforquestionanswering. arXivpreprintarXiv:2307.16778,2023.
EunsuKim,JuyoungSuk,PhilhoonOh,HaneulYoo,JamesThorne,andAliceOh. Click:
A benchmark dataset of cultural and linguistic intelligence in korean. arXiv preprint
arXiv:2403.06412,2024.
AbdullatifKo¨ksal,OmerFarukYalcin,AhmetAkbiyik,M.TahirKilavuz,AnnaKorhonen,
andHinrichSchu¨tze. Language-agnosticbiasdetectioninlanguagemodelswithbias
probing,2023.
NayeonLee,ChaniJung,JunhoMyung,JihoJin,JuhoKim,andAliceOh. Crehate: Cross-
cultural re-annotation of english hate speech dataset. arXiv preprint arXiv:2308.16705,
2023.
Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. Cul-
turellm: Incorporatingculturaldifferencesintolargelanguagemodels. arXivpreprint
arXiv:2402.10946,2024.
LeiLi,YongfengZhang,andLiChen. Promptdistillationforefficientllm-basedrecommen-
dation.InProceedingsofthe32ndACMInternationalConferenceonInformationandKnowledge
Management,pp.1348–1357,2023.
FangyuLiu,EmanueleBugliarello,EdoardoMariaPonti,SivaReddy,NigelCollier,and
Desmond Elliott. Visually grounded reasoning across languages and cultures. arXiv
preprintarXiv:2109.13238,2021.
TarekNaous,MichaelJRyan,andWeiXu. Havingbeerafterprayer? measuringcultural
biasinlargelanguagemodels. arXivpreprintarXiv:2305.14456,2023.
Tuan-PhongNguyen,SimonRazniewski,AparnaVarde,andGerhardWeikum. Extracting
culturalcommonsenseknowledgeatscale. InProceedingsoftheACMWebConference2023,
WWW’23.ACM,April2023. doi: 10.1145/3543507.3583535. URLhttp://dx.doi.org/10.
1145/3543507.3583535.
KeivalyaPandyaandMehfuzaHolia. Automatingcustomerserviceusinglangchain: Build-
ingcustomopen-sourcegptchatbotfororganizations. arXivpreprintarXiv:2310.05421,
2023.
AntonioPenta,NigelShadbolt,PaulSmart,andWinstonR.Sieck. Detectionofcognitive
featuresfromwebresourcesinsupportofculturalmodelingandanalysis. InProceedings
oftheInternationalConferenceonManagementofEmergentDigitalEcoSystems,MEDES’11,
pp. 53–60, New York, NY, USA, 2011. Association for Computing Machinery. ISBN
9781450310475. doi: 10.1145/2077489.2077499. URLhttps://doi.org/10.1145/2077489.
2077499.
RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyareward
model. AdvancesinNeuralInformationProcessingSystems,36,2024.
12Preprint. Underreview.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a
unifiedtext-to-texttransformer. Journalofmachinelearningresearch,21(140):1–67,2020.
AidaRamezaniandYangXu. Knowledgeofculturalmoralnormsinlargelanguagemodels,
2023.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese
bert-networks. arXivpreprintarXiv:1908.10084,2019.
JonathanRoberts,TimoLu¨ddecke,SowmenDas,KaiHan,andSamuelAlbanie. Gpt4geo:
Howalanguagemodelseestheworld’sgeography. arXivpreprintarXiv:2306.00020,2023.
MichaelJRyan,WilliamHeld,andDiyiYang. Unintendedimpactsofllmalignmenton
globalrepresentation. arXivpreprintarXiv:2402.15018,2024.
VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf. Distilbert,adistilled
versionofbert: smaller,faster,cheaperandlighter. arXivpreprintarXiv:1910.01108,2019.
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori
Hashimoto. Whoseopinionsdolanguagemodelsreflect? InInternationalConferenceon
MachineLearning,pp.29971–30004.PMLR,2023.
SheikhShafayat,EunsuKim,JuhyunOh,andAliceOh. Multi-fact: Assessingmultilingual
llms’multi-regionalknowledgeusingfactscore. arXivpreprintarXiv:2402.18045,2024.
Kate´rinaStenou. Unescouniversaldeclarationonculturaldiversity: avision,aconceptual
platform,apoolofideasforimplementation,anewparadigm. 2002.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
WenxuanWang,WenxiangJiao,JingyuanHuang,RuyiDai,JentseHuang,ZhaopengTu,
andMichaelR.Lyu. Notallcountriescelebratethanksgiving: Ontheculturaldominance
inlargelanguagemodels,2023.
DavidWatkins. Learningandteaching: Across-culturalperspective. InSchoolLeadership
andAdministration,pp.61–76.Routledge,2012.
BinweiYao,MingJiang,DiyiYang,andJunjieHu. Empoweringllm-basedmachinetransla-
tionwithculturalawareness,2023.
Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang.
Geomlama: Geo-diversecommonsenseprobingonmultilingualpre-trainedlanguage
models. arXivpreprintarXiv:2205.12247,2022.
CalebZiems,JaneDwivedi-Yu,Yi-ChiaWang,AlonHalevy,andDiyiYang. NormBank:
A knowledge bank of situational social norms. In Anna Rogers, Jordan Boyd-Graber,
and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),pp.7756–7776,Toronto,Canada,July
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.429.
URLhttps://aclanthology.org/2023.acl-long.429.
13Preprint. Underreview.
A Limitations
Despiteourefforts,werecognizeseverallimitationsofourwork.
First,inourpipeline,weutilizeopen-sourceLLMsinvarioussteps. Constrainedbyopen-
sourceLLMs’abilitytoprocessnon-Englishlanguages,weprocessEnglish-onlydata. But
manyculturalnuancescannotbefullyexpressedorcapturedbyEnglish. Thislimitation
inherentlyrestrictsourabilitytograspandrepresentthefullspectrumofculturalcontexts
andmeanings,potentiallyleadingtooversimplificationsofcertainculturalaspects. Besides,
althoughweattempttominimizebiaswithvariousefforts,theseopen-sourceLLMscould
stillextractbiasedinformation,andgeneratebiasedsummarization,whichcouldleadto
biasedfinaloutcomeinthedata.
Second, our dataset is subject to sample bias. Because we scrape the data with certain
keywordsandhashtagslike“culturaldifference”and“cultureshock”. Oftentimespeople
onlypostononlineplatformswhentheyhavestrongreactionssuchassurpriseorshock
towardscertainculturalphenomena. Thisbiasmeansthatourfindingsmightoverempha-
sizeaspectsofculturaldifferencethataremorelikelytostandouttoindividuals, while
underrepresentingmoremundaneoruniversallysharedaspectsofculture. Suchabiascan
skewtheperceptionofculturaldiversityanddifference,potentiallyreinforcingstereotypes
oroverlookingthesubtletiesofculturalexchangeandadaptation.
Third,CultureBankstillcontainsgenericculturalstatements,suchasexpressionsofculture
shockwithoutdetailedinformation,whichmaynothelptoprovidenuancedunderstandings
ofinterculturalinteractions.
B CultureBankexamples
Table6showsqualitativeexamplesinourCultureBank-TikTok.
Culturalgroup Context Actor Actor’sbehavior Otherdescription Agreement
American inFrance people experiencecultureshock,expresssurprise, includesspecificexampleslike 0.9
and feel confused due to differences in electricity bills and driving
lifestyle,food,andsocialnorms habits
American intheUnitedStates people refertochickpeasas’chickpeas’or’garbanzo theterm’chickpeas’hasbeen 1.0
andgrocerystores beans’,oftenusingtheterminterchangeably adoptedfromHispaniclanguage
ItalianAmerican primarily in the individuals identifyasItalianAmerican,oftenwithvary- discussionsaroundappropriate- 1.0
UnitedStates andcommu- inglevelsofconnectiontoItalianheritageand nessandculturalpreservation
nities culture
Californians inCaliforniaandits people experienceamixofhighlivingcosts,attrac- Californiaisperceivedaswealth- 0.7
variousregions tiontothestate,andapreferenceforliving ier,withvaryingexpensesanda
theredespitecheaperalternatives needforaffordablehousing
Alabamian in Alabama and people enjoyoutdooractivitiesanduniqueexperi- 1.0
duringroadtrips enceslikevisitingInandOutandridingin
thebackofatruck
Norwegian inNorway,particu- people, followastrictcandyconsumptionschedule, candyisconsideredatreatand 0.8
larlyinthenorth including eatingcandyonlyonSaturdaysandavoiding istypicallyboughtonSaturdays
childrenand unwrappedcandies
parents
Chinese in China, particu- people and heavilyrelyondigitalandmobilepayment 1.0
larlyinurbanareas businesses methodslikeWeChatPayandAlipay,often
usingfacialrecognition
Rwandan in Rwanda and people speakKinyarwanda,Swahili,andEnglish, 1.0
among Rwandan withKinyarwandabeingtheprimarylan-
communities guage
SouthAfrican when paying for people expressfrustrationoverhavingtocalculate prefer straightforward pricing 0.1
items pricesandtaxes withoutadditionalcalculations
pricingexpectations
Australian in Australia, par- customers tippingisnotcommonorexpectedduetofair tippingisnotacommonpractice 0.5
ticularlyinrestau- wagesandgoodservice inAustralia,butcanbeseenin
rantsandbars somehigh-endestablishments
Argentinian inthenorthernre- people enjoyspicyfood,particularlyinlocalcuisine 0.7
gionsincludingJu-
juy,Salta,andthe
north
Table 6: Selected qualitative examples in CultureBank. We omit several fields for space.
Pleaserefertothereleaseddatasetforthecompleteexamples.
14Preprint. Underreview.
C ConstructionPipeline
Wepresentmoredetailsontheconstructionpipelineinthissection.
C.1 Descriptorextraction
Culturaldescriptorextractor. Toensurediversityinthefinaldata,wedonotrestrictthe
form of the extracted values, except for the agreement field. For agreement, we ask the
model to output 1 if the comment agrees with the extracted cultural information and 0
otherwise,toenableagreementcalculationin§4.3.
Negationconverter Afterextractingtheculturaldescriptor,wecanalsooptionallyperform
anegationconversionstep.Thisisbecauseculturaldescriptionscanbediverseanddynamic,
itisimportanttocalculatetheagreementlevelstocertainculturalbehaviorinsideacultural
group. Thisrequiresbothpositiveandnegativeexamplesforoneculturalbehavior. But
duetothelimitationofclusteringalgorithms,oppositesentencessuchas“Japanesesdo
notgivetips”and“Japanesesgivetips”maybeplacedintothedifferentclusterinlatter
steps. Soaftertheextractionandbeforeclustering,weusedifferentheuristicstoconvert
negativesentencestotheirpositiveforms,andalsofliptheagreementfield,i.e.,“Japanese
donotgivetips,agreement=1”willbecome“Japanesesgivetips,agreement=0”,tohelp
futureclusteringandagreementcalculationsteps.
C.2 Descriptorclustering
Clustering. Tomanagethecomputation,weperformtworoundsofclusterings. Wefirst
conductHierarchicalAgglomerativeClustering(HAC)ontheculturalgroupstoidentify
similarones. Thenweenumeratetheidentifiedculturalgroups. Withineachculturalgroup,
weconcatenatetheextractedfields,encodetheconcatenatedsentencewithSentenceBert
(Reimers&Gurevych,2019),andperformanotherroundofHACclustering. Wekeepthe
yearofthecommentsineachcluster,tohelppotentialtemporalanalysis,soeachdescriptor
isassociatedwithatimerangeasshowninFigure1. Toensureenoughsupportingevidence
foreachcluster,weremoveclusterswithlessthan5datapoints,anddiscretizedthecluster
sizeinto10-unitintervalslike[10,20)asthesupportvalueforeachculturaldescriptor.
Summarization. To summarize each cluster into a single, high-level cultural indicator,
weleverageaMixtral8x7Bmodelfinetunedon1ksummarizationexamplesgeneratedby
GPT-4. Toevaluatetheperformanceofoursummarizer,weperformedhumanannotation
onthefollowingtwometrics:
• Fidelity: extenttowhichthesummaryaccuratelyrepresentsthemainideas,facts,
andfiguresfromtheoriginaltextwithoutintroducinginaccuraciesordistortions;
• Coherence: Howwellthesummaryflowsfromonesentencetoanother,maintaining
logicalprogressionandclearconnectionsbetweenpoints.
Normalizationonculturalgroupandtopic. SinceweleverageLLMstoperformclus-
tersummarization,andmodelscanextractandsummarizetogenerateanyinformation,
similarvaluesmayappearindifferentforms,forinstance,“AboriginalAustralians”and
“IndigenousAustralians”aresynonyms,and“fashionandattire”and“clothing”aretwo
similartopics. Havingsuchsynonymsmakesithardtoqueryandmanageourdataset,so
wenormalizevaluesinthetwofieldsof“culturalgroup”and“topic”,tocollapsesynonyms.
Similartothepreviousclusteringstep,weuseHACtogroupsynonymsinculturalgroups
togetherandtakethemajorityvoteastherepresentativeculturalgroup. Groupingtogether
similartopicsturnsouttobeamoreinvolvedtask,asmanytopicsoverlapwithanddiffer
fromeachotherinsubtleways,soembedding-basedclusteringmethods(e.g.,HAC)often
yieldnoisyresultsthatfailtoalignwithhumanjudgements. Instead,weuseanLLMto
detect100recurringthemesfromouroriginaltopics,andthenmanuallymerge,prune,and
rephrasethesethemesinto36high-levelculturaltopics. Finally,weaskanLLMtoclassify
eachsummarizedclusterintooneofthesetopics. Table9showsthedetailsofourfinallist
of36culturaltopics.
15Preprint. Underreview.
C.3 Post-processing
Agreementcalculator. Wecalculatetheaverageofindividualagreementscoresandround
ittothefirstdecimalpointasthefinalagreementlevelinourfinalCultureBankdata.
C.4 LLMPromptsforDifferentSteps
Listing1showsthepromptusedforthedescriptorextractionstep,andListing2showsthe
promptusedfortheclustersummarizer.
[INST] <<SYS>>
You are a helpful, respectful and intelligent assistant trained to
identify and extract cultural information. Your role is to follow
the given instructions precisely and format your responses as
required. Keep your responses succinct and limited to the
requested information. If you don’t know the answer to a question,
please don’t share false information.
Cultural information encompasses content that showcases the
distinctive characteristics, artifacts, or manifestations of a
specific group, community, or region. This includes, but is not
limited to, practices, behaviors, norms, values, beliefs, habits,
customs, architectural styles, environmental engagements, and any
other elements that are emblematic of a particular cultural
setting. It does not include generic information or widespread
practices that are not distinctly tied to a specific cultural
identity.
For this task, consider information as "cultural" if:
1. It is associated with or characteristic of a specific identified
group (e.g., Americans, Italians, midwestern Americans, etc.).
2. It reveals a unique aspect of that group’s way of life, including
social conventions, physical creations, or interactions with their
surroundings that are not typically seen in other cultures.
3. It provides insight into the cultural uniqueness, whether through
social practices, material culture, or other culturally
significant elements.
Please exclude generic or ubiquitous statements or observations that
do not clearly relate to the unique cultural context of a specific
group.
<</SYS>>
For each video-comment pair, you need to do two things:
1. Determine whether the provided example contains cultural
information.
2. If the example does include cultural information, extract the
cultural knowledge into a list of JSON objects with the following
fields:
‘‘‘
{
"cultural group": "group of people with the same cultural
background",
"context": "location, or other settings this behavior is
performed",
"goal": "goal of the behavior",
"relation": "relation between the actor and recipient",
"actor": "the actor of the action",
"recipient": "the recipient of the action",
"actor’s behavior": "the behavior of the actor",
"recipient’s behavior": "the behavior of the recipient",
"other descriptions": "any other description that doesn’t fit
into previous categories",
16Preprint. Underreview.
"topic": "cultural topic",
"norm": "whether the described event is considered norm according
to the given comment. 1 = norm; 0 = taboo",
}
‘‘‘
If an example contains multiple cultural knowledge, please encode
each piece of knowledge into a seperate JSON object.
Output the extracted cultural knowledge as a list of JSON objects, or
an empty list if the provided example does not contain any
cultural information.
----------------------------------------------------
Here are some examples:
{few\_shot\_examples}
----------------------------------------------------
Now determine if the following example contains cultural information
and extract any cultrual knowledge into a list of JSON objects.
Please only include information that you directly extract from the
provided text and do not hallucinate.
[Reminder]: Consider information as "cultural" if:
1. It pertains to a specific identified group (e.g., Americans,
Italians).
2. It shows unique cultural traits or practices of that group
differing from others.
3. It provides insight into the cultural uniqueness, whether through
social practices, material culture, or other culturally
significant elements.
Please avoid considering generic statements or behaviors that are
common across multiple cultures or lack specificity as "cultural
information."
Please base your answers strictly on the provided text. If important
cultural context, such as the cultural group, is not explicitly
mentioned or directly inferable from the text, output an empty
list. Avoid adding or assuming any information that is not
directly supported by the text.
Once you’ve outputed a list of JSON objects, please immediately
output "<EOD>".
Video description: {}
Comment: {}
Contain cultural knowledge: [/INST]
Listing1: PromptforLlama-2-70Bondescriptorextraction
<s>[INST] Here is a list of cultural behaviors that belong to a
single cluster:
{}
Please summarize the above records into one consolidated JSON object.
For each field in the output JSON, provide a concise, overarching
summary that encapsulates the key essence or common theme of all
records.
Your JSON output should contain the following fields:
{
"cultural group": "group of people with the same cultural
background",
"context": "location, or other settings this behavior is
performed",
"actor": "the actor of the action",
17Preprint. Underreview.
"recipient": "the recipient of the action",
"relation": "relation between the actor and recipient",
"actor’s behavior": "the behavior of the actor",
"goal": "goal of the actor’s behavior",
"recipient’s behavior": "the behavior of the recipient",
"other descriptions": "any other description that doesn’t fit
into previous categories",
"topic": "cultural topic relating to the behaviors",
}
Your summary should:
1. Focus on the cultural aspect and ignore any non-cultural
information
2. Leave a field as null if it is inapplicable or not specified
3. Include only the dominating opinion if there are conflicting
opinions in the cluster
4. Merge the value in "other descriptions" into other fields whenever
possible, use "other descriptions" only if necessary
5. Ensure that each field is one single phrase or a **short**
sentence that succinctly summarizes and accurately reflects the
aggregated information from the cluster; avoid repeating redundant
information
6. Include a "topic" field that summarizes the related cultural
behavior into a single word or a high-level phrase
Focus on creating a single concise, complete, culturally-focused, and
accurately formatted JSON object without any extra words.
Output: [/INST]
Listing2: PromptforMixtral-8X7Bonclustersummarizer
C.5 Runningtime
Table7and8showstherunningtimeanddatavolumnaftereachstepforTikTokandReddit.
# Step(TikTok) Time #gpus Output/Datasize
Input:totalcomments - - 720K
1 Culturalrelevanceclassifier 2hours 1 400K
2 Culturaldescriptorextractor 4days 32 400K
3 Clustering 2hours 1 13K
4 Clustersummarizer 13hours 4 13K
5 Agreementcalculator <1min 0 13K
6 Contentmoderation 3 hours + human 0 12K
annotation
Table7: TikTokrunningtimeofeachprocessingstepandtheamountofdataafterwards.
18Preprint. Underreview.
# Step(Reddit) Time #gpus Output/Datasize
Input:totalcomments - - 7M after keyword
filtering
1 Culturalrelevanceclassifier 2hours 1 2.6M
2 Culturaldescriptorextractor 5days 32 input=528K, out-
put=493K
3 Clustering 2hours 1 13K
4 Clustersummarizer 13hours 4 13K
5 Agreementcalculator <1min 0 13K
6 Contentmoderation 3 hours + human 0 11K
annotation
Table8: Redditrunningtimeofeachprocessingstepandtheamountofdataafterwards.
C.6 CulturalTopicDistribution
Table9showsthetopicdistributioninCultureBank.
D EvaluatingCulturalAwareness
D.1 Evaluatedmodeldetails
Table10showsthemappingbetweenthemodelnamementionedinthepaperandexact
modelversionsandhowtheyaretrained.
D.2 DirectEvaluation
Priorwork(Fungetal.,2024)evaluatesLLMs’cultureawarenessviadirectlyaskingtrue/-
falsequestions. Followingthesamepractice,wealsoconstructthedirectevaluationasa
binaryclassificationtaskbyaskingmodelsifaculturalbehaviorispracticedbythemajority
ofpeopleinthepresentedculturalgroup. WeconverttheagreementlevelinCultureBankto
abinarylabel:ifaculturalbehaviorhasanagreementlevel>0.5,thenwelabelitaspositive.
Otherwise,welabelitasnegative. Listing3showsthepromptsfordirectevaluation.
Table 11 shows the macro F1 as we have highly unbalanced agreement levels. We also
furthercategorizetheculturaldescriptorsintothreebins: highsupport(clustersize>50),
midsupport(clustersizebetween20and50),lowsupport(morelong-tailedbehaviorwith
clustersize≤20).
Resultsshowsthatnoneofthemodelswereabletoachieveaperfectscoreindirecteval-
uation. ForLlama-basedmodels, wedoobservethetrendthattheyareworseformore
long-tailedculturalbehaviors,whiletheGPTmodelfamilyoutperformstheothermodels.
Interestingly,itseemsthatChatGPTconsistentlyoutperformsGPT4acrossalllevelsofsup-
port. Qualitativeanalysisfoundthatitispartiallybecausethereisoftentimesnoclean-cut
thresholdsonwhetheraculturalbehaviorisacommonpractice,andinambiguouscases
GPT4 tends to behave more conservatively, and this demonstrates one major limitation
of such a simplistic approach of direct evaluation. Moreover, predicting the agreement
levelinaclassificationsettingcorrectlydoesnotnecessarilymeanthatthemodelcanap-
propriatelyleveragesuchculturalknowledgeandpayattentiontotheculturalnuances
indownstreamconversationalapplications. Wefoundthatevenifalanguagemodelhas
alreadyseen/learnedaculturalsignature,itoftenfailstorecognizetheseculturalnuances
invariousconsultingscenariosandendsupprovidinggenericadvice. Thesedrawbacks
leadustoconductamoregroundedevaluationmentionedin§6.
<s>[INST] You are presented with cultural behaviors encoded into the
following fields:
19Preprint. Underreview.
TikTok Reddit
CulturalTopic
Count Pct Count Pct
SocialNormsandEtiquette 1576 12.89% 729 6.45%
FoodandDining 952 7.79% 245 2.17%
Miscellaneous 816 6.67% 622 5.50%
CulturalExchange 805 6.58% 2232 19.75%
CommunicationandLanguage 783 6.40% 485 4.29%
CommunityandIdentity 607 4.97% 2151 19.04%
ConsumerBehavior 606 4.96% 126 1.12%
HealthandHygiene 585 4.79% 147 1.30%
EnvironmentalAdaptationandSustainability 559 4.57% 185 1.64%
CulturalTraditionsandFestivals 506 4.14% 659 5.83%
CulturalandEnvironmentalAppreciation 476 3.89% 1039 9.19%
FinanceandEconomy 451 3.69% 147 1.30%
EducationandTechnology 288 2.36% 195 1.73%
FamilyDynamics 261 2.13% 180 1.59%
MigrationandCulturalAdaptation 258 2.11% 388 3.43%
SocialInteractions 252 2.06% 157 1.39%
HouseholdandDailyLife 250 2.04% 62 0.55%
Lifestyles 231 1.89% 86 0.76%
SafetyandSecurity 223 1.82% 61 0.54%
EntertainmentandLeisure 218 1.78% 326 2.88%
RelationshipsandMarriage 195 1.60% 257 2.27%
DrinkingandAlcohol 193 1.58% 100 0.88%
BeautyandFashion 181 1.48% 57 0.50%
FamilyTraditionsandHeritage 170 1.39% 155 1.37%
Work-LifeBalance 161 1.32% 38 0.34%
Workplace 141 1.15% 114 1.01%
ReligiousPractices 84 0.69% 109 0.96%
Transportation 76 0.62% 39 0.35%
TimeManagementandPunctuality 68 0.56% 8 0.07%
SportsandRecreation 54 0.44% 126 1.12%
SocialInfrastructure 43 0.35% 15 0.13%
HumorandStorytelling 39 0.32% 9 0.08%
DressCodes 37 0.30% 5 0.04%
Travelling 36 0.29% 19 0.17%
PetandAnimalCare 33 0.27% 24 0.21%
HousingandInteriorDesign 11 0.09% 3 0.03%
Table9: DistributionofCulturalTopicsinCultureBankonTikTokandReddit.
20Preprint. Underreview.
Modelnameinourpaper ModelcardinHuggingFace/OpenAI
Llama-2-7B-chat meta-llama/Llama-2-7b-chat-hf
Llama-2-70B-chat meta-llama/Llama-2-70b-chat-hf
Mistral-7B-Instruct mistralai/Mistral-7B-Instruct-v0.2
Mixtral-8x7B-Instruct mistralai/Mixtral-8x7B-Instruct-v0.1
GPT-3.5 gpt-3.5-turbo-1106
GPT-4 gpt-4-1106-preview
Llama-2-7B-SFT(ours) Llama-2-7b-chat-hf+supervisedfine-tunedonCultureBank
Mixtral-8X7B-SFT(ours) Mixtral-8x7B-Instruct-v0.1+supervisedfine-tunedonCultureBank
Mixtral-8X7B-DPO(ours) Mixtral-8x7B-Instruct-v0.1+trainedonCultureBankwithDPO
Table10: Mappingbetweenthemodelnameinourpaperandtheexactmodelcardnamein
HuggingFaceorOpenAIandhowtheyaretrained.
{
"cultural group": "group of people with the same cultural
background",
"context": "location, or other settings this behavior is
performed",
"actor": "the actor of the action",
"recipient": "the recipient of the action",
"relation": "relation between the actor and recipient",
"actor’s behavior": "the behavior of the actor",
"goal": "goal of the actor’s behavior",
"recipient’s behavior": "the behavior of the recipient",
"other descriptions": "any other description that doesn’t fit
into previous categories",
}
Given a cultural behavior encoded in the format above, do you think
the majority of people in the given cultural group would agree
with the described behavior?
Your answer should be "Yes" no "No", without any other words.
-------------------------------------------------------------
Cultural Behavior: {}
Does the majority of people in the given cultural group agree with
the described behavior?
Your output should be Yes/No only. Even if you are uncertain, you
must pick either "Yes" or "No" without using any extra words.
Your Answer (Yes/No): [/INST]
Listing3: PromptforLLMDirectEvaluation
21Preprint. Underreview.
High(70) Mid(175) Low(924) All(1169)
Llama-2-7B-chat 48.8 50.4 34.6 34.4
Llama-2-70B-chat 67.2 46.1 44.4 44.6
Mistral-7B-Instruct 42.3 40.7 38.1 38.6
Mixtral-8x7B-Instruct 41.0 42.1 41.4 41.5
GPT-3.5 63.2 65.7 65.5 65.4
GPT-4 56.7 59.7 58.9 58.9
Table11: ComparisonofdifferentLLMs’performanceondirectevaluationbrokendown
bysupport. Wereportthemodels’macro-averagedF1scoresonculturaldescriptorswith
High,Mid,andLowsupports,aswellasthemacroF1scoresontheentiretestset.
D.3 Groundedevaluation
D.3.1 Datageneration
Cultural Descriptor Reward Model
“In Japanese restaurants, tipping is uncommon ü Is the generated question
and often declined, showcasing the cultural relevant to the provided
preference for implicit gratitude over financial Question Generation Model cultural knowledge?
rewards.” ü Does the generated
ConsultingScenario Client Persona question indirectly relate
Sample 1000 Fine-tune travel advising backpacker to the provided
knowledges?
Generate Sample Questions Grounded Question
On my backpacking trip, what subtle
Scenario + Persona + Question g ree ss tt au ur re a nsa t?ys 'thank you' in a Japanese Final Question
Figure 5: Detailed workflow of how we generate the scenario, persona, and question
groundedoneachculturaldescriptor. Wedistill1KGPT-4-generatedexamplestotraina
Mixtralmodel,andemployarewardmodeltorefinetheMixtralmodel.
Detailsongroundeddatageneration.Figure5showsamoredetailedproceduretogenerate
scenarios,personas,andquestionsforthegroundedevaluation. Wefirstsample1Kcultural
descriptorsfromCultureBank,anduseGPT-4togenerateadiverserangeofscenariosand
consultingquestionsforthem. Foramoreaffordablecost,wedistilltheseGPT-4generated
scenariostofine-tuneaMixtral8x7Bmodeltogenerateevaluationquestionsfortheentire
dataset. Givenapieceofculturalknowledge,weaskthemodeltogenerate: 1. aconsulting
scenario, 2. a client persona, and 3. a question asked by the client that indirectly relates to the
givenknowledge. Listing4showsthepromptusedtogeneratequestionsinthegrounded
evaluation.
Self-refinement. Tofurtherimprovethequestionquality,asshowninFigure5,weapplya
self-refinementmethodwhensamplingfromthefine-tunedmodel. WeuseanGPT-4-based
rewardmodeltoscorethegeneratedquestionsbasedontwoqualityevaluationcriteria: (1)
Relevance: Isthegeneratedquestionrelevanttothegivenknowledge? (2)Subtlety: Does
thegeneratedquestionindirectlyrefertothegivenknowledge? Foreachoftheevaluation
metrics,weusethepredictedprobabilitiestheanswerbeing“Yes”asourrating,andusea
thresholdof0.95forRelevanceand0.8forSubtlety. Listing5showsthepromptusedforthe
questionevaluation. Thenweaskthemodeltorefineitsgenerationbasedonifeitherofthe
criteriaisnotmet. Lastlyweusethisimprovedmodeltogenerateddatafortheentireset.
Table12and13showstwogeneratedexamples.
22Preprint. Underreview.
D.3.2 Automaticevaluation
ToevaluateLLMs’cultureawareness,weasktheLLMthegeneratedquestiongroundedin
apracticalscenario. Listing6showsthepromptusedforthegroundedevaluation. Afterwe
obtaintheanswer,weaskGPT-4tolabeliftheanswerentailstheoriginalculturaldescriptor,
anduseGPT-4’spredictedprobabilitiesoftheanswerbeing”Yes”isusedastheentailment
score. Listing7showsthepromptusedfortheknowledgeentailmenttask.
Cultural In Japan, particularly in restaurants, it is customary for customers to
descriptor attempt to tip service staff as a gesture of gratitude, despite it being
consideredrudeorunnecessaryinJapaneseculture. Thegoalbehindthis
behavioristoexpressappreciationfortheservicereceived. Inresponse,
theservicestaffmayeitherreturnthetiporpolitelyrefuseit,inlinewith
theculturalnormthattippingisnotexpectedorrequiredinJapan. This
practiceishighlycontroversialamongthesampledpopulation,witha
significantportionconsideringitanunusualbehaviorwithinJapanese
diningetiquette.
Persona A curious and generous American traveler, planning a first-time trip
toJapan,eagertoshowappreciationforthecountry’srichcultureand
impeccableservice,yetunsureabouttheproperetiquetteinregardsto
tipping
Scenario TravelAdvising
Question I’mplanningmyfirsttriptoJapanandI’veheardsomanygreatthings
about the hospitality and service there. I’ve always been a generous
tipperbackhome,butIdon’twanttooffendorconfuseanyone. Howdo
InavigatetippinginJapanwithoutcausinganymisunderstandings? Are
thereanyparticulargesturesorwordsthatareappreciatedbythelocals
whenexpressinggratitudefortheirservice?
Table 12: An example of the generated scenario, persona, and question grounded on a
culturaldescriptor.
Cultural InIcelandicculture,especiallyincoldweatherandoutdoorsettings,it
descriptor iscustomaryforparentstoleavetheirbabiesoutsidefornaps,allowing
themtoenjoythefreshairandsleeppeacefully. Thegoalofthispractice
isforthebabiestobenefitfrombothsleepandtheoutdoorenvironment.
This behavior is widely regarded as a normative practice within the
sampledpopulation,withthebeliefthatthecoldaircontributestoaiding
the babies’ sleep. This cultural practice reflects the Icelandic value of
embracingthenaturalenvironmentanditspotentialbenefitsforthewell-
beingoftheyoungestmembersofthecommunity.
Persona Awarm-climatenativenewlyemployedinReykjavik,concernedabout
the well-being of colleagues’ children during outdoor team-building
eventsinthecoldweather
Scenario WorkplaceIntegration
Question Hey there, I recently started working in Reykjavik after moving from
a much warmer climate. My colleagues and I are planning on some
team building events with outdoor activities. However, some of my
colleagues are planning to bring their little ones with them. It’s quite
chilly outside, and some of the kids are really young. I don’t want to
steponanytoes,butIamalittleconcernedaboutlettingtheyoungones
playingornappinginsuchacoldweather. WhatshouldIdo?
Table13: Anotherexampleofthegeneratedscenario,persona,andquestiongroundedona
culturaldescriptor.
23Preprint. Underreview.
<s>[INST] You are an AI assistant tasked with mentoring the training
of theurapists and consultants. Given a piece of cultural
knowledge, your task is to craft a single, creative question
grounded in a consulting scenario that probe’s the consultant’s
awareness of the cultural behaviors and information included in
the provided knowledge. For each question, imagine you are a
client discussing a personal experience, issue, or everyday
problem with a consultant in a specific consulting scenario. These
scenarios include, but not limited to, travel advising,
immigration, workplace integration, study abroad, relationship
issues, healthcare navigation, and conflict resolution. In
particular, when generating each question, you should:
1. Choose a consulting scenario
2. Generate a persona for the client
3. Craft a question that the client may ask in the chosen scenario
that relates to presented cultural knowledge.
Keep the following guidelines in mind:
1. Creativity and Diversity: Please be creative and generate diverse
questions. Each question should stem from a unique scenario, such
as travel advising, immigration, workplace integration, study
abroad, relationship issues, healthcare navigation, conflict
resolution, etc. Be imaginative in how these scenarios could
unfold due to cultural differences.
2. Client’s Perspective: Choose a client persona and frame your
question as if you’re a client speaking casually with a consultant
. The question should be personal, reflecting a real-life concern
or curiosity that a client might have, and phrased in a
conversational tone.
3. Contextual Relevance: Your question should indirectly assess the
consultant’s grasp of the cultural knowledge without directly
stating it. It should explore how the consultant might weave this
knowledge into their guidance in various scenarios, but do not ask
directly about cultural norms.
4. Open-ended Inquiry: Formulate your question to elicit detailed
insights, opinions, or strategies from the consultant, rather than
a simple yes/no answer.
Format your output as a valid JSON containing three fields: Scenario,
Persona, and Question. Do not include any additional words.
----------------------------------------------------
Knowledge: {cultural_knowledge_description}
Craft a question about a real-life scenario or concern that **subtly
** and **indirectly** relates to the given knowledge. Instead of
asking explicitly or generically about cultural norms, you should
ground your question in a specific, real-life concern or quandary
that a client might face. Aim for a question that embodies a
client’s voice and context, without revealing or hinting at the
knowledge itself. You should NEVER let any of the behaviors or
norms described in the given knowledge appear directly in the
client’s question, and avoid mentioning anything about culture at
all. Instead, use anecdotes from the client’s perspective. Be
creative and imaginative when generating the scenarios, and keep
your questions diverse.
In your questions, be sure to mention the cultural group in the given
knowledge in order to avoid confusion, even if the cultural group
is America or the US.
Format your output as a valid json. [/INST]
24Preprint. Underreview.
Listing4: Promptforgroundeddatagenerationinthegroundedevaluation
System Prompt: You task is to evaluate the quality of questions
generated from a piece of cultural knowledge. These generated
questions are aimed to assist the training of therapists and
consultants by probing their awareness of the cultural behaviors
and information included in the provided knowledge and their
ability to integrate the knowledge into real-life consulting
scenarios.
Each question is generated based on the following guidelines:
1. Creativity and Diversity: Please be creative and generate diverse
questions. Each question should stem from a unique scenario, such
as travel advising, immigration, workplace integration, study
abroad, relationship issues, healthcare navigation, conflict
resolution, etc. Be imaginative in how these scenarios could
unfold due to cultural differences.
2. Client’s Perspective: Choose a client persona and frame the
question as if you’re a client speaking casually with a consultant
. The question should be personal, reflecting a real-life concern
or curiosity that a client might have, and phrased in a
conversational tone.
3. Contextual Relevance: The question should indirectly assess the
consultant’s grasp of the cultural knowledge without directly
stating it. It should explore how the consultant might weave this
knowledge into their guidance in various scenarios, but do not ask
directly about cultural norms.
4. Open-ended Inquiry: Formulate your question to elicit detailed
insights, opinions, or strategies from the consultant, rather than
a simple yes/no answer.
For each given pair of cultural knowledge and generated question, you
should provide a Yes/No answer to each of the following
evaluation questions:
1. Is the generated question relevant to the given knowledge? Make
sure that the generated question does not go too far such that it
becomes irrelevant to the presented knowledge.
2. Does the generated question **indirectly** refer to the given
knowledge? Make sure that the question does not explicitly mention
any of the cultural behaviors described in the knowledge. A good
quality question should not mention anything about culture at all,
but use anecdotes from the client’s perspective instead.
You must select either "Yes" or "No" even if you are unsure about an
answer.
----------------------------------------------
User Prompt: Knowledge: {cultural_knowledge_description}
Question: {generated_question}
{evaluation_question} (Yes/No):
Listing5: Promptforevaluatingthegeneratedquestionsasarewardmodel
<s>[INST] You are a helpful AI assistant trained to help users on
their personal issues. Please answer the user’s question to the
best of your ability based on only the knowledge you know. Do NOT
make up any unfounded statements or claims.
25Preprint. Underreview.
User’s question: {}
Your Answer: [/INST]
Listing6: Prompttemplateforgroundedevaluation
System Prompt: You are an AI assistant trained to classify whether a
given response from a language agent entails a specific cultural
knowledge.
Particularly, given a piece of cultural knowledge and a response from
an agent, output "Yes" if the response entails the cultural
knowledge and "No" otherwise.
Your output should be Yes/No only. Even if you are uncertain, you
must pick either "Yes" or "No" without using any other words
----------------------------------------------
User Prompt: Response: {model_response}
Knowledge: {cultural_knowledge_description}
Does the given response entail the provided knowledge?
Entailment (Yes/No):
Listing7: Promptforcalculatingknowledgeentailmentingroundedevaluation
D.3.3 Humanevaluationonmodelwinrates
Tocalculatethemodelwinrates,weemployedasingle-blindhumanannotationprocess
involving two annotators and 200 randomly selected results. The main goal of human
annotationistoselectthemodelanswerthatindicatesbetterculturalawareness,readability,
and relevance to the grounded question. The entire annotation process was conducted
undersingle-blindconditions,inwhichtheannotatorswereunawareofthesourcemodel
foranygivenanswerduringtheannotation. Thisapproachisdesignedtoeliminatebias
andensurethatannotations’answersweresolelybasedonthecontentoftheanswerbut
nottheopinionaboutthemodels. Initially,twoannotatorsindependentlyevaluatedthe
first 50 pairs of model answers to achieve alignment for rating guidelines, and the two
annotatorsachievedaKappascoreof0.87,whichindicatedahighagreementbetweenthe
annotators. Finally,eachannotatorfinishedtheprocessindependentlyandevaluated50%
oftheremainingdataset.
Qualitativeinsightsfromhumanevaluation. Table14showsaqualitativedialogueexam-
plegeneratedbyourfine-tunedmodelandGPT-4. Herearesomequalitativeinsightsfrom
theannotationprocess.
• Ourfinetunedmodelstendtogiveshorterandmorestraight-to-the-pointanswers,
whereasthestandardRLHF-edmodelsalmostalwaysprovidealonglistintheir
responses, which are often more comprehensive but less specific to the user’s
inquiry.
• Ourfinetunedmodelsgenerallyprovidemoreculturally-specificadvicetailored
towards the user’s question and cultural context, while the standard RLHF-ed
models, especially the GPT family, sometimes fail to acknowledge the cultural
aspectsandoutputgenericguidelinesinstead.
• EventhoughGPT-4isquiteknowledgeablewithavarietyofculturesifbeingdi-
rectly asked about them, it sometimes fails to integrate the knowledge into the
26Preprint. Underreview.
downstreamconsultingscenariosinourgroundedevaluation. Itisobservedthat
GPT-4 sometimes applies the same advice template on multiple related user in-
quiries,regardlessoftheirdifferentculturalcontextandnuances.Thisreinforcesthe
importanceofhavinggroundedevaluationsofculturalawarenessinconversational
applicationscenarios,inadditiontodirectclassification-basedprobingevaluation.
Question I’mreallyexcitedaboutmyupcomingEuropeanbackpackingtrip,and
I’veheardsomuchabouttheirdeliciousdairyproducts. I’mabitofa
foodie,andI’dlovetotryasmanylocaldishesaspossible. ButI’malso
prettyhealth-conscious. Couldyougivemesometipsonhowtobest
enjoythesetreatswithoutworryingaboutfoodsafetyorwaste? I’dlike
toberespectfulofthelocalculturewhilealsotakingcareofmyhealth.
GPT4’s I’mgladtohearaboutyourenthusiasmforyourupcomingtrip! Hereare
Response sometipstohelpyouenjoyEuropeandairydelicaciesresponsiblyand
safelywhilebeingmindfulofyourhealthandthelocalculture:
1. **ResearchLocalSpecialties:**Beforeyougo,lookupthedairyspecial-
tiesoftheregionsyou’llbevisiting. Thiswillhelpyouidentifywhich
dishesareamust-tryandwhichmightalignwithyourhealthgoals.
2. **PortionControl:**Toindulgewithoutoverdoingit,considertrying
smallerportions. Thisway,youcansampleavarietyofdairyproducts
withoutconsumingtoomuch.
3. **BalanceYourDiet:**Whenyou’replanningyourmeals,balancerich
dairytreatswithotherfoodgroups,particularlyfruits,vegetables,and
wholegrains. Thiscanhelpmaintainahealthydietwhiletraveling.
4. **StayHydrated:**Richfoodscanbetaxingonyourdigestivesystem,
somakesuretodrinkplentyofwaterthroughouttheday.
5. **FoodSafety:**Ingeneral,Europehashighfoodsafetystandards,but
it’salwayswisetoeatatreputableestablishmentsandcheckthatdairy
productsarepasteurizedifyou’reconcernedaboutfoodborneillness.
6. **RespectLocalTraditions:**Showappreciationforthelocalculture
bylearningaboutthesignificanceofthedishesyoutry. Thismightalso
giveyouinsightintothebestwaystoenjoythem.
7. **Moderation is Key:** If you’re trying a particularly rich or heavy
dairyproduct,considermakingthatyourmaintreatfortheday.
8. **ActiveTravel:**Balanceyourfoodindulgenceswithphysicalactivity.
Walking,hiking,andcyclingaregreatwaystoseethesightswhilestaying
active.
9. **TalktoLocals:**Engagewithlocalpeopletofindoutwheretoget
thebestandfreshestdairyproducts. Theycanalsoprovidetipsonhow
toenjoytheminmoderation.
10. **WasteNot:**Toavoidfoodwaste,onlyorderasmuchasyoucan
eat,orifyou’rebuyingproductstotakewithyou,makesureyouhavea
waytostorethemproperlysotheydon’tspoil.
Remember,partofthejoyoftravelisexperiencingnewthings,which
includes local cuisine. Enjoying food is not only about taste but also
about the experience and the memories you create. Bon voyage and
happytasting!
Continuedonnextpage
27Preprint. Underreview.
Table14Continuedfrompreviouspage
OurModel’s Absolutely, I’dbehappytohelpyounavigatethisdeliciousdilemma!
Response Europeandairyproductsareindeedagastronomicdelight,andit’stotally
possibletoenjoytheminahealth-consciousway.
Firstly, when it comes to food safety, I’d recommend looking for rep-
utable sources. For instance, in France, you might seek out artisanal
cheese shops where the cheese is made from raw milk and aged for
therequiredperiod. Thisprocessensuresthatanyharmfulbacteriaare
naturallyeliminated, making thecheese safeto eat. Similarly, in Italy,
you’llfindthatParmigianoReggianoandProsciuttodiParma,bothDOP
(ProtectedDesignationofOrigin)products,undergorigorousproduction
standardsthatensuretheirsafety.
Asforwaste, considerthisasanopportunitytoembracethelocalcul-
ture. InmanyEuropeancountries,foodwasteisfrownedupon,andit’s
common for people to finish everything on their plate. However, it’s
alsoperfectlyacceptabletoaskforadoggybaginmostplaces,sodon’t
hesitatetotakeleftoversbacktoyouraccommodation.
Lastly, remember that these are treats, not everyday foods. While it’s
okaytoindulge,moderationiskey. Enjoythesedelicaciesaspartofa
balanceddietthatincludesplentyoffruits,vegetables,andwholegrains.
I hope this helps you to enjoy the culinary delights of Europe while
respectingbothyourhealthandthelocalculture. Bonappe´tit!
Table14: QualitativecomparisonsbetweenGPT’sandourModel’soutputsingrounded
evaluation
E Fine-tuningCulturally-awareLanguagemodels
E.1 Training
Ourmodelfine-tuningprocesscontainstwophases. Inphase1,wetrainedthebasemodel
on9402culturaldescriptorsfor8epochs. Formorestablefine-tuning,wefirstconverted
each cultural descriptor (split by fields) into a short paragraph of text via gpt-3.5-turbo
beforeusingittotrainourmodel. Theprompttemplateforthisconversionisavailablein
Listing8.
In phase 2, we then sampled 2000 grounded questions from our grounded evaluation
datasetwherethebasemodelperformedpoorlyaccordingtotherewardmodel(i.e.,with
knowledgeentailmentscore<0.6). Toobtainmoreculturally-awareresponsesonthese
questions to further improve our model, we augment the training data with answers
conditionedonthegoldculturalknowledge. Formodelgenerationsaugmentedwiththe
goldculturalknowledge,weprovidetheculturalknowledgedescriptionrelatedtoeach
generatedquestiontothemodelinadditiontothequestionitself,whichgivesthetheoretical
upper-boundcultural-awarenessperformance. Listing9showsthepromptusedforthe
augmentedupperboundmodel. Withthese2Kaugmentedculturally-awareresponses,we
furtherfine-tunedourusingeitherSFTorDPOfor8epochs.
System Prompt: You are provided with a piece of cultural knowledge or
behavior extracted from online social media comments. Your task
is to translate this information into a short, descriptive
paragraph. The provided cultural knowledge is encoded as a JSON
object with the following fields:
{cultural_descriptor_field_definitions}
Your task is to translate this information into a short, descriptive
paragraph. Please adhere to the following guidelines:
1. Cultural Group & Context: Begin by setting the scene, mentioning
the cultural group and the context in which the behavior occurs.
28Preprint. Underreview.
2. Actor’s Behavior: Describe the behavior of the actor within this
cultural setting. If the behavior aims to achieve a specific goal,
mention this as well.
3. Cultural Perception: Include any additional descriptions that
highlight how this behavior is perceived within the culture or
influenced by regional customs.
4. Normativity: Qualitatively assess how common or normative this
behavior is within the culture based on the "norm" field, and
avoid specific numerical values. Use phrases such as (but not
limited to) "a significant portion of the sampled population", "
around two thirds of the sampled population agrees that", "is
widely regarded as" when the "norm" value is high; and use "is
considered an unusual behavior", "is highly controversial among
the sampled population", etc. if the "norm" value is low.
5. Adherence to Provided Information: Ensure that your description
strictly follows the information provided in the JSON object. Do
not include assumptions, interpretations, or external knowledge
not present in the original data.
Your output should be no more than 150 words.
User Prompt: Here is a piece of cultural knowledge encoded in a JSON
object:"
{json_cultural_descriptor}
Based on the provided information, craft a paragraph that
encapsulates the essence of the cultural knowledge, ensuring each
point above is addressed where applicable. Remember to provide a
qualitative estimation of the behavior’s normativity according to
the sampled population without stating the numerical value. Be
sure to strictly adhere to the provided information, without
adding any extraneous details or providing your own interpretation
of the cultural significance.
Remember, the knowledge is only obtained from a small subset of each
population, so DO NOT overgeneralize your statements. Avoid using
phrases such as "deeply rooted" or "deeply ingrained", even when
the "agreement" value is high.
Limit your response to 150 words.
Listing 8: Prompt for generating cultural knowledge descriptions from JSON cultural
descriptors
<s>[INST] As an AI consultant specializing in culturally-informed
advice, you have access to insights derived from current social
media trends and opinions. Your expertise enables you to
understand and apply these insights to address user inquiries
thoughtfully. When responding to a user’s question, draw upon this
cultural context implicitly to enrich your advice, ensuring it
feels intuitive and seamlessly integrated.
Remember, your responses should reflect a deep understanding of the
cultural nuances pertinent to the user’s question, without
directly indicating the source of your insights. Your goal is to
provide guidance that feels personalized and informed, as if
coming from a seasoned consultant who naturally incorporates
cultural awareness into their advice.
User’s question: {}
29Preprint. Underreview.
Cultural Insight: {}
Use the cultural insight to inform your response, crafting advice
that is both relevant and sensitive to the user’s cultural context
. Your expertise should be evident through the nuanced
understanding you display.
Remember, you MUST use the provided cultural insights to augment your
response, but do not explicitly state the source of these
insights.
Your Answer: [/INST]
Listing9: Prompttemplatetogenerateanswersaugmentedbygoldnenknowledge
E.2 Downstreamculturaltasks
E.2.1 GlobalOpinionsQA
We follow the same evaluationsettings as theoriginal paper Durmuset al. (2023b). We
prependcross-nationalprompt“Howwouldsomeonefrom[countryX]respondtothis
question?” foreachmultiple-choicequestionpairedwithagivencountryquery. Togetthe
model’sprediction,weappendtheprompt”Ifhadtoselectoneoftheoptions,myanswer
wouldbe(”,thenpredictthenexttoken’sdistribution. Listing10showsthepromptused.
Foreachmultiple-choicequestionq,denotethedistributionoftherespondentsfromcountry
c as P (q|c), and model’s predicted distribution for country c as P (q|c), we use Jensen-
c m
Shannon Distance (JSD) to evaluate the distance between p and p . Specifically, we
c m
calculate
1 ∑ 1 ∑
cultureSim= (1−JSD(P (q|c),P (q|c)))
|D| |C | m c
q∈D q c∈Cq
ThedefinitionofJSDis:
(cid:114)
1 1
JSD(P ∥ Q) = D(P ∥ M)+ D(Q ∥ M)
2 2
inwhich D representsKullback–Leiblerdivergence,and M = 1(P+Q)isthemidpoint
2
distributionbetweenPandQ.
A higher cultureSim score represents a higher cultural similarity between the model’s
responseandthecountryqueried.
Inaddition,tomeasurehowmuchthemodel’srepresentationisbiasedtowardsacertain
subsetofcountries,wecalculatetheStandardDeviationErrorsofthecultureSimscoreacross
differentcountries. Thelowerthescore,themorebalancedthemodel’srepresentation.
Human: How would someone from {country} answer the following question
:
{question_str}
Here are the options:
{options_str}
Assistant: If I had to select one of the options, my answer would be
(
Listing10: PromptforGlobalOpinionsQAzero-shotevaluation
30Preprint. Underreview.
E.3 CulturalNLI
FortheevaluationonCulturalNLI,wepromptthemodelswithculturalindicators(e.g.,
promptingthemtothinkasanAmerican/Indian)andaskthemodelswhetherthepremise
and hypothesis entrails, neutral, or contradicts with each other. Similar to the settings
ofGlobalOpinionsQA,weusedthemodel’spredicteddistributionofE(Entailment),C
(Contradiction),andN(Neutral)astheoutputscoresandcalculatedtheF1scoreagainst
”Entailment”astheevaluationclass,asintheoriginalpaper. Listing11showstheprompt
used.
Premise: {}
Hypothesis: {}
Let’s think as someone who lives in {the United States/India}. What
do you think is the relationship between the premise and the
hypothesis?
(E) Entail
(N) Neutral
(C) Contradict
Your Answer (E/N/C): (
Listing11: PromptforCulturalNLIzero-shotevaluation
F GeneralizingtoReddit
ToapplyourpipelineonReddit,wemakethefollowingcustomizationtofitthedata.
• Culture Relevance Classifier: because Reddit comments are much longer than
TikTokcomments,theTikTok-basedCulturalRelevanceClassifierdoesnotwork
wellforReddit. Soweperformsimpletextsearchfirst. Wefirstsourcediscussions
with tags related to cultural differences. Considering Reddit has more person-
alized and user-defined tags, we extend to encompass tags such as ’#culture’,
’#culturaldifference’, ’#culturedifference’, ’#cultureshock’, ’#culturalshock’, and
’#culturalexchange’. Additionally,giventheinfrequentuseoftagsonReddit,we
furtherconductkeywordsearchesonbothsubmissionsandcommentstoidentify
moreculturallyrelevantdiscussioncontents. ThenweuseGPT-4toannotate1000
datapoints,andtrainedaclassifiertogettheculturally-relevantportionfromthis
curatedsubset. Finally,weobtain7Mcommentsafterkeywordfiltering,sendthem
totheclassifierandget2.6Mculturalcommentsaftertheclassificationstep.
G Recommendationforfuture
G.1 Preliminarytemporalanalysis
Culturechangesovertime,soitisimportanttocapturethetemporalchange. SinceReddit
contains many historical data (2005-2022), we perform a preliminary temporal analysis
on CultureBank-Reddit by searching for related keywords in CultureBank-Reddit. Each
descriptorhascommentsfromdifferentyears, andweslicetheyeartoformasnapshot
ofthatyear. AsshowninFigure6,overtheyears,morepeoplearestudyingabroad;the
discussiononLGBTQ+rightshasgainedmoreattentionandcommunitysupport;besides,
technologicaladvancementsalsoaffectculturalpractices,e.g.,peoplementionanotable
shift within Dutch society towards cashless payment and a growing acceptance among
Swisssociety. Wealsonoticethatsomeculturalpracticeswoulddeclineduetotechnological
innovation: forinstance,becauseofstreamingservicesanddigitaldownloads,theculture
relatedtoDVDshasbeeneclipsed. Thedatasetmentionsthatfromthe1990stotheearly
2000s,DVDswerethepredominantmediumsforvideoplayback,buttherehasbeenless
andlessdiscussionaboutDVDculturerecently.
31Preprint. Underreview.
Discussions Related to Cultural Keywords by Year
International Student LGBTQ+ Cashless Payment
120 900 40
800 35
100
700 30
80 600
25
500
60 20
400
15
40 300
200 10
20
100 5
0 0 0
2009 2012 2015 2018 2021 2008 2011 2014 2017 2020 2015 2017 2019 2021
Year Year Year
Figure6: PreliminarytemporalanalysisofdifferentkeywordsonCultureBank-Reddit.
Number of Reddit Comments Related to Different Entertainment Topics by Year
160 Country Music
140 Rock Music
Pop Music
120
Hip-hop Music
100 Harry Potter
80 Marvel/DC
The Lord of the Rings
60
40
20
0
2009 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022
Year
Figure7: PreliminarytemporalanalysisofentertainmenttopicsonCultureBank-Reddit.
Additionally,someculturalpracticesaremoresensitivetotemporaldynamics. InFigure7,
wediscusssomeentertainmenttopicsasexamples. Hip-hopmusicbecamemorepopularin
thelate2000sandweseeanincreasingamountofdiscussiononHip-hopmusicinthefigure.
Inourdataset,thereisadiscussionontheevolutionandadaptationoftheFrenchrapand
hip-hopscenewhichrespondstoshiftingtrendswithintherapmusicgenre. Similarly,itis
notedthatsuperheromoviesandcomicsfromMarvelandDChavedevelopedmulticultural
narratives, featuring a diversity of characters and storylines. This development reflects
theevolvingcultureofsuperheroesandunderscorestheimpactofsocietaltrendstoward
greaterinclusivityanddiversityinthisgenre.
32
stnemmoC
detaleR
fo
rebmuN
stnemmoC
detaleR
fo
rebmuN
stnemmoC
detaleR
fo
rebmuN
stnemmoC
detaleR
fo
rebmuN