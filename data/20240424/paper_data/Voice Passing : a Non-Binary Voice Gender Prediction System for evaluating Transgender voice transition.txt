Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating
Transgender voice transition
DavidDoukhan1,SimonDevauchelle1,LucileGirard-Monneron2,M´ıaCha´vezRuz3,V.Chaddouk3,
IsabelleWagner2,AlbertRilliard4,5
1InstitutNationaldel’Audiovisuel(INA),France;2HoˆpitalTenon,AP-HP,France;3Independent;
4Universite´ ParisSaclay,CNRS,LISN,France;5UniversidadeFederaldoRiodeJaneiro,Brazil
{ddoukhan, sdevauchelle}@ina.fr, lucile.monneron@aphp.fr, mia.chavezruz@gmail.com,
vancdk@pm.me, isabelle.wagner@aphp.fr, rilliard@lisn.fr
Abstract tice and gender perception. A program allowing transgender
personstotraintheirvoiceandmeasuretheirprogress,andal-
Thispaperpresentsasoftwareallowingtodescribevoicesusing
lowingvoicetherapiststoevaluateanddeveloptheirtechniques
acontinuousVoiceFemininityPercentage(VFP).Thissystem
withatooladaptedtotheirdailyneeds. Thistoolthus(i)shall
isintendedfortransgenderspeakersduringtheirvoicetransition
take into account the complex characteristics of a voice (not
andforvoicetherapistssupportingtheminthisprocess.Acor-
onlyF );(ii)shallreturnaproportionofmasculinity/femininity
0
pus of 41 French cis- and transgender speakers wasrecorded.
sotransgenderpersonsmayadapttheoutputtotheirownwant.
Aperceptualevaluationallowed57participantstoestimatethe
Todevelopthisservice,machinelearning(ML)algorithmswere
VFPforeachvoice. Binarygenderclassificationmodelswere trained to evaluate the voices’ gender, and their outputs were
trainedonexternalgender-balanceddataandusedonoverlap-
tuned to the perceptual evaluation of a corpus of individual
ping windows to obtain average gender prediction estimates,
voices by naive French listeners. This perceptual evaluation
whichwerecalibratedtopredictVFPandobtainedhigherac-
andthesetupofthesealgorithms,withtheirperformanceeval-
curacy than F or vocal track length-based models. Training
0 uation,isthetopicofthispaper. Thisstudyfocusesongender
data speaking style and DNN architecture were shown to im-
perceptionwithintheFrenchcultureandlanguage.
pactVFPestimation. Accuracyofthemodelswasaffectedby
speakers’age.Thishighlightstheimportanceofstyle,age,and
2. RelatedWork
theconceptionofgenderasbinaryornot,tobuildadequatesta-
tisticalrepresentationsofculturalconcepts.
Earlier gender prediction systems were based on LPC anal-
IndexTerms: Transgendervoice,Genderperception,Speaker
ysis [11], MFCC gender-dependent HMM phone recog-
genderclassification,CNN,X-Vector
nizer[12],orMelbandsandpitchestimationHMM[13]. This
task was defined as a binary classification problem associated
1. Introduction
with high accuracy estimates (> 95%), often considered as
Careoftransgenderpeopleoffers,amongotherthings,support solved. However, the reported performances were not nec-
forthemodificationofthegenderperceivedthroughtheirvoice, essarily comparable since accuracy depends on e.g., corpora,
a main component of identity - particularly of gender iden- sampleduration,speechtranscript,speakingstyle,speakerage,
tity [1]. During female-to-male transitions, the lowering of a and language. Recent studies, using pre-trained Transformer-
voice’sfundamentalfrequency(F )isrelativelyeasytoobtain basedacousticfeatures[14]orConvolutionalNeuralNetworks
0
bytakingtestosterone, whichproducesalengtheningofvocal (CNN)trainedonMelbands[15,16],reportedaccuracymet-
foldsandamasculinizationofvoice. Conversely,raisingone’s ricsabove90%onfixed-lengthspeechsamples(2seconds,680
vocal pitch so the voice is perceived as female is more com- ms, 30 seconds), but also gender classification biases defined
plex,astakingfemalehormonesdoesnotinfluencethephona- asaccuracydifferencesbetweenfemaleandmalespeakers.For
torystructureafterpubertyformales[2,3]. However,voice’s transgendervoices,classificationsystemsusedthreegendercat-
F isnottheonlycriterionforidentifyingvoicegender. Voice egories: a male, female, and transgender system was fitted to
0
quality(andtypicallyvocaltractresonances)isalsoanimpor- recordings of cis- and transgender (male & female) speakers,
tantdeterminant,whileprosody,speechrhythm,andvocabulary withanaccuracyof83%[17]. TheTrans-VoiceApp,usedfor
intervene as secondary cues [4, 5]. Beyond personal training, transgender auto-evaluation, has a decision function based on
transgender persons are offered care by voice therapists and, aMulti-LayerPerceptrontrainedwithabinarygenderandar-
ifrequired, twotypesofvocalsurgerythataimaraisedvoice bitrarythresholdstoobtainmasculine,feminine,andandrogy-
pitch: cricothyropexyandWendlerglottoplasty[6,7]Arecur- nousvoicecategories.Itsoutputwascomparedtospeakerjudg-
rentquestionishowtoevaluatethisworkanditsoutcome?(i.e., mentsontheirownspeech,withanaccuracyof88%[18].
howdoweestimatethegenderthat’llbeperceivedfromavoice Categorical systems make it difficult to monitor the
inagivenlanguageandculture?)Mostavailablesoftware(e.g., speaker’sprogressduringtheirtransition.Ourworkingassump-
EvaF[8]orVoiceUp[9])proposingtheevaluationofvoicemas- tionistofavorsystemsproducingcontinuousgenderestimates
culinity or femininity for non-expert essentially use voice F . fitted to human perception of gender. An LDA system based
0
Whilethismeasurementalonedoesnotcapturegenderpercep- on 29 acoustic features was trained on cisgender voices an-
tion (an extra-high F may correspond to a falsetto voice, or notated on a continuous scale [19, 20]. While not addressing
0
averylowF toapartiallaryngectomy...),letalonenotbeing transgendervoices,thisworkrequiredexcerptsofatleast7sec-
0
tunedtoculturalvariation[10]. ondstoobtainpredictionscorrelatedwithperceptionandfound
Thispaperdescribesthesetupandevaluationofatooltry- thatmeanF ,thirdandfourthformants,andvocaltractlength
0
ingtoclosethegapbetweentherealityofvoicetherapyprac- (VTL)werethemostcorrelatedfeatureswithperceivedgender.
4202
rpA
32
]SA.ssee[
1v67151.4042:viXra3. Cis-andtransgendervoicescorpus 3.3. Perceptualevaluationresults
3.1. Recordingandanalysis Table1presentstheproportionofQ1answersbyspeaker’scate-
gories.Forcisgenderspeakers,anegligibleamountoferrorsor
41 speakers were recorded reading the French version of The
IDKanswerswereobserved(resp.0.4and0.2%)togetherwith
NorthWindandtheSun. Theywere8cisgendermales(CM),
shorteraverageRT:3.4and3.7seconds,versus6.2secondsfor
12 cisgender females (CF), and 21 transgender females (TF),
TFspeakers. Participantstendtoattributeabinarygendercat-
withagevaryingbetween20and69yearsold(mean:39). The
egorytotransgendervoices,withanotablebutmodestincrease
TF speakers had transgender voice therapy supervised by one
(5%)ofIDKanswers.Meangenderjudgmentsare0.539forfe-
author of this study; none of them received surgical process-
malelistenersand0.565formalelisteners.Wilcoxonranksum
ing of the vocal apparatus. All speakers signed an informed
testshowsnosignificantdifferenceswithprobability>=95%
consentformdetailingtheaimsoftheresearchprojecttoallow
betweenthese2groups(W=268.5,p-value=0.1548).
theirvoicestobeusedforresearchpurposesonly. Recordings
weremadeeitherathospitalTenonAP-HPinaquietroomfor
Table 1: Proportion of Q1 answer categories and Reaction
transgenderandsomecisgenderspeakersorattheLISNlabo-
Times(RT)byspeakercategory(CF,CM,TF).
ratoryforsomecisgendervoices. Recordingsweremadeusing
amicrophoneatabout30cmfromthespeaker’smouth,witha
NaconmicrophoneatthehospitaloraZoomH4nrecorderwith CF CM TF
itsdefaultmicrophonesatthelab.Thereadingshadanaverage PerceivedasFemale(%) 99.6 0 47.6
durationof39seconds,varyingbetween30and51seconds. PerceivedasMale(%) 0.4 99.8 47.4
F 0wasestimatedfollowingrecommendationsin[21],com- IDK(%) 0 0.2 5.0
bining REAPER’s voicing estimation [22] with FCN-F0’s F
0
AverageRT(s) 3.4 3.7 6.2
estimation [23]. F was expressed in semitones (ST) relative
0
StandarddeviationRT(s) 4.1 4.3 5.8
to 1 Hz. Estimation of VTL was made using the first four
formants(measuredonthevocalicpartofthereadings),using
[24]’sequationandrecommendationsforformantsestimation, From this perceptual evaluation, we defined a perceived
using Praat’s Burg algorithm [25] (i.e., estimating 6 formants ”VoiceFemininityPercentage”(VFP)index, derivedfromQ1
witha5.5kHzfrequencythreshold). answers,anddefinedasthenumberofFemaleanswersplushalf
of the IDK answers, divided by the total number of answers.
3.2. Genderperceptiontest Figure1showsthemeanRTforeachspeaker’svoice,accord-
ingtotheirVFP.WhileCMandCFspeakershaveVFPcloseto
AperceptiontestwasconductedusingPsyToolkit[26,27],an
0and100,TFVFPsarespreadbetween0and100. Asecond-
online interface allowing the realization of in-browser exper-
orderpolynomialgivesareasonablefitoftheRT,asafunction
iments. A link to the online interface was sent to multiple
ofvoiceVFP,withamaximalRTcenteredaround50%VFP.
French-speaking research mailing lists and social media. 57
participants were enrolled in this perceptual evaluation. They
wereaskedtoprovidetheirgender(35female,20male,2other
orconfidential)andagerange(18in18-35yearsold,25in36-
50,9in51-65,4over65,1confidential). CF
8 CM
Participantshadtoreadtheinstructionsandtoacceptpar- TF
ticipatinginthestudy.Instructionsdescribedhowthisresearch 7
aimsatinvestigatingwhyandhowvoicesareperceivedaspro-
ducedbyfemalesormalesandthattheparticipantsweresup- 6
posed to evaluate how the voice they were about to listen to
5
couldhavebeenproducedbyafemaleoramale,ofagivenage.
Participantswerenottoldthatthevoicesmightcontaintrans- 4
gendervoicesinordertoavoidinfluencingtheirdecisions.The
41recordingswerepresentedinarandomordertoeachpartic- 3
ipant,whohadtoanswertwoquestionsintuitivelyandrapidly
0 20 40 60 80 100
without having to listen to the whole speech sample. Partici- Voice Femininity Percentage (%)
pantshadthepossibilitytoanswer”Idon’tknow”(IDK).The
questionswereQ1: Whatisthevoice’sgender? (answers: Fe- Figure1:ByspeakerplotofthemeanVFPbyaverageRT,with
male,Male,IDK);andQ2: Howoldisthespeaker? (answers: a2ndorderpolynomialfitpredictingRTfromVFP
20-35,36-50,51-65,over65,IDK).
Q1answerbuttonsappearedatthebeginningofthestim-
uluspresentation. ParticipantshadtoanswerQ1tobeableto 4. Genderpredictionmodels
answer Q2. They were not able to replay stimuli nor change
their answers. Once the two answers were recorded, the next Thecompletevoicegenderevaluationsystemisbasedonthree
stimuluswaspresentedafterashortpause.Theevaluationtook componentsthattakeasinputawavfilesampledat16kHz.The
about6minutestocomplete,excludingthetimerequiredtoread firststepisbasedontheinaSpeechSegmenterVoiceAc-
theinstructionsandprovidedemographicdata.Thequestionre- tivityDetectorusedtodiscardnon-speechsegments[28].Then,
latedtospeakerage(Q2)wasaimedatdistractingparticipants a2DCNNgenderclassificationmodelisappliedusingaslid-
toavoidafocusongender. Foreachquestion,theanswersand ingwindow. Ateachwindowstep,itproducesabinarygender
theassociatedreactiontimes(RT)wererecorded. Theanswers predictionthatisaveragedoverthecompleterecording.Lastly,
relatedtothespeaker’sagearenotusedinthisstudy. anIsotonicregressioncalibrationprocedure[29,30]transforms
)s(
emiT
noitcaeR
noitazirogetaC
redneGthis average score to the VFP obtained on the Trans- and cis- verification, diarization) [36]. VBX extractor is based on
gendercorpususinganon-linearincreasingmapping. a Resnet101 architecture pre-trained on Vox2 corpus, us-
ing 64-dimensional Mel filterbank coefficients to obtain 256-
4.1. Binaryspeakergendertrainingcorpora dimensionalX-vectors.Thisdeepmodel(347layers)hasarel-
ativelylargeamountofparameters(45M)sinceitwastrained
Table 2 presents the 4 corpora used to train or evaluate bi-
for complex tasks. We build on top of this extractor several
narygenderclassificationmodels. Voxceleb2(Vox2)contains
Multi-LayerPerceptron(MLP),withanumberoflayersvary-
celebrity voices obtained from Youtube in various recording
ingbetween1and4andthenumberofneuronsperlayerinset
conditions [31]. Despite its size, it mostly features English
{32,64,128,256,512}.
speakers,whichmaybesub-optimalfortraininggenderdetec-
tionsystemstargetingFrenchvoices. INA’sspeakerdictionary
4.3. TrainingStrategy
(INA1)anddiachronicspeakercorpus(INA2)containvoicesof
celebritiesobtainedfromFrenchaudiovisualarchives[32,33].
We defined a DNN training strategy aimed at obtaining mod-
INA1isbasedonspeechbroadcastonFrenchTVnewsbetween
els with minimal gender, corpus, and speaker biases. Male
2007 and 2013. INA2 subset contains TV and radio speech
speakers were randomly excluded from corpora so to obtain
broadcastin2015-2016, balancedacross4ageranges(20-35,
balanced subsets containing the same amount of unique male
36-50,51-65,65+)and2genders(female,male). TheFrench
and female speakers. To mix training corpora, we discarded
setofCommonVoicecorpus(CVFr)containsalargenumber
speakers from the largest in order to obtain subsets with the
of anonymous volunteer speakers reading short sentences in
same amount of unique speakers per corpus. Speech record-
French and recorded using their own devices [34]. CVFr has
ings were then grouped by unique speaker identifier and split
interestingpropertieswithrespecttoourfinalusecase,where
intomutuallyexclusivetraininganddevelopmentsetsusingra-
individualsmayusevariable-qualityrecordingdevices.
tios of 80 and 20%, so a speaker from the train set is absent
from the dev set. For each epoch, a 1515 ms speech excerpt
Table 2: Corpora used for training gender classification sys-
was randomly drawn (position, recording condition) for each
tems,describedbynumberofuniquefemale(#F)andmale(#M)
speaker, resulting in a sample number equal to the number of
speakers, durationinhours(Dur), mainlanguage(Lang)and
unique speakers, balanced across genders and corpora. Mod-
availability(Av)
els were then trained using an early stopping procedure with
patience set at 50 epochs, monitoring the estimate defined as
Corpus #F #M Dur Lang Av thegloballossplustheabsolutevalueofthelossdifferencebe-
tweenmaleandfemalespeakersobtainedonthedevelopment
Vox2[31] 2311 3682 2460 English Public
set.Eachmodelwastrainedusing3randominitializations,and
INA1[32] 494 1790 123 French Request
objectivefunctionconvergencewasobtainedwithinamaximal
INA2[33] 122 165 39 French Request
amount of 160 epochs. 1500 TpCNN and 200 Xvector-based
CVFr[34] 758 3070 478 French Public
models were trained using NVIDIA 2080 Ti GPUs, requiring
850hoursofcomputationtime(30minutes/model).
4.2. 2DCNNspeakergenderclassification
5. Results
Twotypesof2DCNNarchitectureswereinvestigatedforbuild-
ing the classification models. Both operate on Mel-scaled fil- Evaluations were realized in a cross-corpus configuration.
terbankcoefficientsobtainedfrom25mswindowswithastep Vox2,INA1,andCVFrcorporawereusedtotrainMLmodelsin
size of 10 ms. 2D CNN inputs are defined as patches of di- singleandmixedcorpusconfigurations(French=INA1+CvFr
mensionsT*N,withNthenumberofMelbandsextractedfrom andAll=Vox2+INA1+CvFr).INA2wasusedfortestingmod-
eachanalysiswindow,andT = 150beingthetimedimension els on the binary gender classification (BGC) task and to ob-
(thenumberofsignalwindowsrequiredtocreateaninputpatch, tain estimates of accuracy per gender and age category. The
correspondingto1515msspeechexcerpts). Trans-andCisgendervoiceCorpus(TCC)wasusedfortesting
WedefinedTemporalPoolingCNNarchitectures(TpCnn) the Voice Femininity Percentage (VFP) prediction. Our pro-
inspired by [15] using N=24 Mel-scaled filter bank inputs. posalsarecomparedto4baselines:F0andVTLcorresponding
ThesearchitecturesarebasedonNCONVconvolutionalblocks, to median F 0 or VTL, F0VTL is a linear SVM fit on median
atemporalpoolinglayer(<maxpool,T,1),NDENSEdense F 0andVTLfeatures,ISSisagenderclassificationmodelpro-
layers, and a sigmoid activation. Convolutional blocks are videdintheopen-sourceprojectinaSpeechSegmenterand
composed of valid K1*K2 kernels with NFILT filters fol- pre-trainedonFrenchdata[15]. Thesebaselineswereusedin
lowed by batch normalization and RELU activation. Fre- pipelines,includingVADandisotoniccalibration.
quency (<maxpool,1,2>), Time (<maxpool,2,1>), or Table 3 presents the best VFP prediction models. VFP
Time-frequency(<maxpool,2,2>)invariancepoolingstrat- results are reported separately for cis- (CIS) and transgender
egywereinsertedbetweenconvolutionsblocks. Denselayers (TF) speakers using the coefficient of determination (R2) ob-
containNNneuronsandhaveDropoutratesof0.2.Theparam- served between model predictions and perceptual estimates.
eterspaceofarchitectureswasexploredwithNCONVvarying Eachmodelisassociatedwith(i)abinarygenderclassification
from2to5,NDENSEvaryingfrom0to4,NFILTandNNin (BGC)performancemetricsdescribedastheharmonicmeanof
set {32,64,128,256,512}, K1 and K2 in set {3,5,7,9} and theaccuracyobtainedformaleandfemalespeakers(Hacc)and
varyingpoolingstrategies. (ii)aGenderBias(GB)definedasthedifferencebetweenthe
We also defined several X-vector based architectures us- accuracyformaleminusforfemalespeakers(GB >0ifmale
ing VBX open-source extractor [35]. X-vectors are 1- accuracyishigherthanfemaleaccuracy,else<0;GBcloseto
dimensional speaker embeddings obtained with DNN archi- 0isbetter). WhileF -andVTL-basedmodelsallowedobtain-
0
tectures, generallyusedforspeaker-relatedtasks(recognition, ingreasonableVFPresultsforcisgenderspeakers(R2 =0.94),theirabilitytopredicttransgenderVFPislower(R2 = 0.53), 6. Conclusion
illustratingthelimitation oftheF andVTLfeatures forpre-
0
Wepresentedanoriginalapproachforestimatingacontinuous
dicting transgender voices’ perceived femininity (or gender).
WhileshowingbetterabilitiestoestimateTFVFP(R2 =0.79), ratioofperceivedgenderfromvoice,definedasaVoiceFemi-
ninityPercentage,andfittedtotheperceptualresultsofagroup
theISSbaselinewasassociatedwithlowerscoresthanourpro-
of French speakers. This approach differs from [18], as we
posalsandalargegenderbias(GB =+4.6).
havechosentobaseourestimatesonexternallistenerjudgments
Forall training setconfigurations, TpCNNobtainedlower rather than on speakers’ own judgments, as the former better
scoresthanX-vectorarchitectures.ReportedTpCNNresultsare fits our aim: reflecting the gender perceived by the interlocu-
limited to their best training set configuration using all avail- tors. Unlike [19,20],weaskedperceptualtestparticipantsto
able training data (TF VFP R2 = 0.86); their lowest results providebinarygenderjudgmentsbecausegenderismostlyper-
were obtained while trained on CvFr (R2 = 0.76). X-vector ceivedasabinarycharacteristicintheFrenchsociety(asshown
models obtained CIS VFP R2 > 0.99 for all training config- bythebarelyusedIDKoption)–butweconsideredthepropor-
urations, corresponding to almost perfect VFP estimation for tionoffemaleanswers,thatallowedusworkingonacontinuous
cisgenderspeakers.BestTFVFPwasobtainedwithamodelus- dimension.Whileweconsideredthisperceptiontaskmorenat-
ingfour512neuronhiddenlayersonthetopoftheX-vector uralthanaskingforcontinuousgenderjudgments,itrequireda
extractor, trained in a single corpus configuration using CvFr significantgroupofparticipants; resultingincostlyperceptual
(R2 = 0.94).It was associated with the lowest reported BGC gender estimations that we considered necessary with respect
gender bias (GB = 0.1) but also with the lowest BGC har- toourfinalusecase–havingamodelthatreflectshowavoice
monic accuracy (Hacc = 94.2). Best BGC results were ob- wouldbeperceivedinasocialinteractionsetting.
tainedwithdifferentsettings:asingle512-neuronhiddenlayer
We implemented several machine learning models in
MLPtrainedwithalltheavailabledata(Hacc = 98.1). This
chargeofreproducingtheseperceptualjudgmentsandobtained
bestBGC-performingmodelresultedinalowerbutfairTFVFP convincingresultsforcisgender(R2 > 0.99)andtransgender
prediction (R2 = 0.92). These two best-performing models voices(R2 = 0.94), whichwereshowntobemuchmoreac-
(TFVFPandBGC)wereassociatedwithBGCharmonicaccu-
curatethanpredictionsbasedonF and/orVTLestimatesonly.
0
racydecreasingwiththespeaker’sage,asillustratedintable4.
BestresultswereobtainedusingVBXX-vectorfeatures(pre-
trained with English data) [35] with an MLP trained in a sin-
glecorpusconfigurationusingCvFr[34]. Thisresultsuggests
Table3: BestVFPpredictionmodelsobtained. HaccandGB
thatthebestperformanceswerelinkedtospeakingstylesimilar-
aretheharmonicaccuracyandthegenderbiasobtainedonthe
itybetweenCvFrandtheevaluationmaterial(non-professional
binarygenderclassificationtask. VFPR2 isreportedforcis-
readspeech)ratherthantothetrainingdatalanguage(noma-
(CIS)andtransgender(TF)speakers.
jordifferencesbetweenINA1andVox2)orthesheersizeofthe
dataset(CvFrissmallerthanVox2).Additionalworkwouldbe
Model Training BGC VFPR2 necessarytoestimatethepotentialimpactoftrainingdatalan-
corpus Hacc GB CIS TF guageifstyleiscontrolledfor,usingX-vectorextractorstrained
onFrenchdata. Thisresultalsosuggeststhatthetrainedmod-
F0 TCC 0.8923 0.4886
elsthatobtainedfairbutnotthebestresultsonourevaluation
VTL TCC 0.6961 0.0586
taskmaybebettersuitedtotheanalysisofspontaneousspeech,
F0VTL TCC 0.9407 0.5303
which was not represented in our evaluation material. Addi-
ISS INA1 93.5 +4.6 0.985 0.792 tionalworkisnecessarytoconstituteaspontaneousspeechcor-
pususingsimilargenderperceptualevaluationprotocols.Other
TpCNN All 94.8 +2.1 0.9978 0.8586
factors,andtypicallythespeaker’sage,hadamajoreffectonall
X-vector Vox2 96.0 +5.8 0.9997 0.9181 modelsevaluationmetrics,andtypicallygenderbias: thesere-
INA1 97.3 +1.3 0.9995 0.9149 sultsmayreflectliteraturedescribingtheevolutionofvoicewith
CvFr 94.2 +0.1 0.9987 0.9420 ageduringadulthood[37,38],withdecreasedF infemalevs.
0
French 97.6 +2.4 0.9998 0.9147 anincreaseformale. Thisillustratestheimportanceofmodels
All 98.1 +1.5 0.9997 0.9153 fittedtothevoiceofspeakerswithvariedcharacteristics.
Resultsdescribedinthisstudyarecurrentlylimitedtoread
speechinFrench. OngoingworkconsistsinbuildingHuman-
Machine Interfaces to investigate if these theoretical results
matchend-usersexpectationsandallowtoprovideconstructive
Table4: BinaryGenderHarmonicAccuracy(Hacc)andGen-
voice-passingfeedbacktobeusedinadditionorinsteadofF
0
derBias(GB)describedbygenderandagecategoriesofthe
estimates.BestperformingBGCmodelspresentedinthisstudy
bestbinarygenderclassificationmodel(X-vector All)and
havebeenintegratedtoinaSpeechSegmenteropen-source
thebestVFPpredictionmodel(X-vector CvFr)
project[39].DiscussionswithFrenchregulatoryauthoritiesare
necessarytodefinehowfittedcalibrationmodules(BGCtoVFP
Model 20-35 36-50 51-65 over65 mapping) could be disseminated while preventing non-ethical
usesrelatedtothecharacterizationofnon-prototypicalvoices.
X-vectorAll Hacc 99.3 98.6 98.2 96.0
GB -1.0 -0.6 +3.1 +4.3
7. Acknowledgements
X-vectorCvFr Hacc 96.2 95.6 94.3 90.3
GB -2.1 -3.2 +2.5 +2.7 This work has been partially funded by the French National
ResearchAgency(projectGenderEqualityMonitor-ANR-19-
CE38-0012).8. References
[20] F.Chen,R.Togneri,M.Maybery,andD.W.Tan,“Acousticchar-
acterizationandmachinepredictionofperceivedmasculinityand
[1] M.L.GrayandM.S.Courey,“Transgendervoiceandcommuni-
femininityinadults,”SpeechCommunication,vol.147,pp.22–
cation,”OtolaryngologicClinicsofNorthAmerica,vol.52,no.4,
40,2023.
pp.713–722,2019.
[21] R.Vaysse,C.Aste´sano,andJ.Farinas,“Performanceanalysisof
[2] C.Fugain, Lapuberte´, lamueetlatransidentite´. IsBergues: variousfundamentalfrequencyestimationalgorithmsinthecon-
OrthoEdition,2019. textofpathologicalspeech,”TheJournaloftheAcousticalSociety
ofAmerica,vol.152,no.5,pp.3091–3101,2022.
[3] J.G.Schmidt,B.N.G.d.Goulart,M.E.K.Y.Dorfman,G.Kuhl,
and L. M. Paniagua, “Voice challenge in transgender women: [22] D.Talkin,“REAPER:Robustepochandpitchestimator,”2015.
trans women self-perception of voice handicap as compared to [Online].Available:https://github.com/google/REAPER
genderperceptionofna¨ıvelisteners,”RevistaCEFAC,vol.20,pp. [23] L. Ardaillon and A. Roebel, “Fully-Convolutional Network for
79–86,2018. PitchEstimationofSpeechSignals,”inProc.Interspeech2019,
2019,pp.2005–2009.
[4] T.MurryandS.Singh,“Multidimensionalanalysisofmaleand
femalevoices,”ThejournaloftheAcousticalsocietyofAmerica, [24] A.C.LammertandS.S.Narayanan,“Onshort-timeestimationof
vol.68,no.5,pp.1294–1300,1980. vocaltractlengthfromformantfrequencies,”PLOSONE,vol.10,
no.7,p.e0132193,Jul2015.
[5] J.M.HillenbrandandM.J.Clark,“Theroleoff0andformantfre-
[25] P. Boersma and D. Weenink, “Praat: doing phonetics by
quenciesindistinguishingthevoicesofmenandwomen,”Atten-
computer [computer program]. version 6.2.08,” Feb 2022.
tion,Perception,&Psychophysics,vol.71,pp.1150–1166,2009.
[Online].Available:http://www.praat.org/
[6] J.VanBorsel,E.VanEynde,G.DeCuypere,andK.Bonte,“Fem- [26] G.Stoet,“PsyToolkit:Asoftwarepackageforprogrammingpsy-
inineaftercricothyroidapproximation?”JournalofVoice,vol.22, chologicalexperimentsusinglinux,”BehaviorResearchMethods,
no.3,p.379–384,May2008. vol.42,no.4,pp.1096–1104,Nov.2010.
[7] N. S. Mastronikolis, M. Remacle, M. Biagini, D. Kiagiadaki, [27] ——, “PsyToolkit,” TeachingofPsychology, vol.44, no.1, pp.
andG.Lawson,“Wendlerglottoplasty:Aneffectivepitchraising 24–31,Nov.2016.
surgeryinmale-to-femaletranssexuals,”JournalofVoice,vol.27, [28] D.Doukhan,E.Lechapt,M.Evrard,andJ.Carrive,“Ina’smirex
no.4,p.516–522,Jul2013. 2018musicandspeechdetectionsystem,”MusicInformationRe-
[8] VoxPop, LLC.EvaF:Voicetrainingtools&lessons.[Online]. trievalEvaluationeXchange(MIREX2018),2018.
Available:https://www.evaf.app [29] F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
[9] Speechtools Ltd. Christella VoiceUp : Trans woman voice
J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,
training. [Online]. Available: http://www.christellaantoni.co.uk/
andE.Duchesnay, “Scikit-learn: MachinelearninginPython,”
transgender-voice/voiceupapp
JournalofMachineLearningResearch,vol.12,pp.2825–2830,
[10] R.vanBezooijen,“Socioculturalaspectsofpitchdifferencesbe- 2011.
tweenjapaneseanddutchwomen,”LanguageandSpeech,vol.38, [30] N.Chakravarti, “Isotonicmedianregression: alinearprogram-
no.3,p.253–265,Jul1995. ming approach,” Mathematics of operations research, vol. 14,
no.2,pp.303–308,1989.
[11] D.Childers,K.Wu,K.Bae,andD.Hicks,“Automaticrecognition
ofgenderbyvoice,”inICASSP-88.,InternationalConferenceon [31] J.S.Chung, A.Nagrani, andA.Zisserman, “Voxceleb2: Deep
Acoustics,Speech,andSignalProcessing,1988,pp.603–606. speakerrecognition,”inINTERSPEECH,2018.
[32] F.SalmonandF.Vallet,“Aneffortlesswaytocreatelarge-scale
[12] L. Lamel and J.-L. Gauvain, “A phone-based approach to non-
datasetsforfamousspeakers.”inLREC,2014,pp.348–352.
linguisticspeechfeatureidentification,”ComputerSpeech&Lan-
guage,vol.9,no.1,1995. [33] R.Uro,D.Doukhan,A.Rilliard,L.Larcher,A.-C.Adgharoua-
mane, M. Tahon, and A. Laurent, “A semi-automatic approach
[13] E. Parris and M. Carey, “Language independent gender identi- tocreatelargegender-andage-balancedspeakercorpora:Useful-
fication,” in1996IEEEInternationalConferenceonAcoustics, ness of speaker diarization & identification,” in 13th Language
Speech, andSignalProcessingConferenceProceedings, vol.2, ResourcesandEvaluationConference,2022,pp.3271–3280.
1996,pp.685–688vol.2.
[34] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,
[14] M. Lebourdais, M. Tahon, A. Laurent, S. Meignier, and J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber,
A.Larcher,“Overlapsandgenderanalysisinthecontextofbroad- “Common voice: A massively-multilingual speech corpus,” in
castmedia,”inProceedingsoftheThirteenthLanguageResources Proceedingsofthe12thConferenceonLanguageResourcesand
andEvaluationConference,2022,pp.3264–3270. Evaluation(LREC2020),2020,pp.4211–4215.
[15] D.Doukhan, J.Carrive, F.Vallet, A.Larcher, andS.Meignier, [35] F.Landini,J.Profant,M.Diez,andL.Burget,“Bayesianhmm
“Anopen-sourcespeakergenderdetectionframeworkformon- clustering of x-vector sequences (vbx) in speaker diarization:
itoringgenderequality,”in2018IEEEinternationalconference theory, implementation and analysis on standard tasks,” 2020.
onacoustics, speechandsignalprocessing(ICASSP). IEEE, [Online].Available:https://arxiv.org/abs/2012.14952
2018,pp.5214–5218. [36] D.Snyder,D.Garcia-Romero,G.Sell,D.Povey,andS.Khudan-
pur,“X-vectors:Robustdnnembeddingsforspeakerrecognition,”
[16] Y.Bensoussan,J.Pinto,M.Crowson,P.R.Walden,F.Rudzicz,
in2018IEEEinternationalconferenceonacoustics,speechand
andM.JohnsIII,“Deeplearningforvoicegenderidentification:
signalprocessing(ICASSP). IEEE,2018,pp.5329–5333.
proof-of-conceptforgender-affirmingvoicecare,”TheLaryngo-
scope,vol.131,no.5,pp.E1611–E1615,2021. [37] R.T.Sataloff, K.M.Kost, andS.E.Linville, Chapter13.The
EffectsofAgeontheVoice,secondeditioned. SanDiego,CA:
[17] G.Yasmin,A.K.Das,J.Nayak,S.Vimal,andS.Dutta,“Arough PluralPublishing,Inc,2017,p.221–240.
settheoryanddeeplearning-basedpredictivesystemforgender
[38] A. Yamauchi, H. Yokonishi, H. Imagawa, K.-I. Sakakibara,
recognitionusingaudiospeech,”SoftComputing,pp.1–24,2022.
T. Nito, N. Tayama, and T. Yamasoba, “Quantitative analysis
[18] J. Williams and P. Paudel, “Application of deep feedforward of digital videokymography: A preliminary study on age- and
neuralnetworkintransgendervocalanalysis,”St.OlafCollege, gender-relateddifferenceofvocalfoldvibrationinnormalspeak-
Northfield,Minnesota,U.S.A.,Tech.Rep.,2022. ers,”JournalofVoice,vol.29,no.1,p.109–119,Jan2015.
[39] D. Doukhan, “inaSpeechSegmenter : a cnn-based audio
[19] F.Chen,R.Togneri,M.Maybery,andD.Tan,“Anobjectivevoice
segmentation toolkit,” 2018. [Online]. Available: https://github.
gender scoring system and identification of the salient acoustic
com/ina-foss/inaSpeechSegmenter
measures.”inINTERSPEECH,2020,pp.1848–1852.