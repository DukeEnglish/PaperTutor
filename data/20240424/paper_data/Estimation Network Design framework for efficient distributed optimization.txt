Estimation Network Design framework for efficient distributed optimization
Mattia Bianchi and Sergio Grammatico
Abstract—Distributeddecisionproblemsfeaturesagroupof later by Alghunaim et al. [12], [13]: in this approach, each
agentsthatcanonlycommunicateoverapeer-to-peernetwork, component of the optimization variable is estimated by a
without a central memory. In applications such as network
suitablychosenclusterofagentsonly.Unfortunately,thedual
controlanddataranking,eachagentisonlyaffectedbyasmall
formulationisonlyeffectiveoverundirectedcommunication
portionofthedecisionvector:thissparsityistypicallyignored
indistributedalgorithms,whileitcouldbeleveragedtoimprove networks. Other works [14], [15] rely on the concept of
efficiencyandscalability.Toaddressthisissue,ourrecentpaper locality, which result in improved efficiency, but at the cost
[1] introduces Estimation Network Design (END), a graph of accuracy; further, any structure beyond distance on the
theoreticallanguagefortheanalysisanddesignofdistributedit-
communication graph is ignored.
erations.ENDalgorithmscanbetunedtoexploitthesparsityof
To deal with these challenges, in our recent work [1], we
specific problem instances, reducing communication overhead
andminimizingredundancy,yetwithoutrequiringcase-by-case introducedEND,agraph-theoreticlanguagetodescribehow
convergenceanalysis.Inthispaper,weshowcasetheflexilityof the estimates of any variable of interest (e.g., optimization
ENDinthecontextofdistributedoptimization.Inparticular,we vector,dualmultipliers,costgradient)areallocatedandcom-
study the sparsity-aware version of many established methods,
bined among the agents in a generic distributed algorithm.
including ADMM, AugDGM and Push-Sum DGD. Simulations
END allows assigning the estimate of each component of on an estimation problem in sensor networks demonstrate
that END algorithms can boost convergence speed and greatly the variable of interest to a subset of the agents, according
reduce the communication and memory cost. tothesparsitystructureofagivenproblem–withoutresort-
ing to a case-by-case convergence analysis. Leveraging the
I. INTRODUCTION
problemsparsityisespeciallyconvenientinrepeatedortime-
Modernbigdataoptimizationproblemsinnetworkcontrol
varying problems (e.g., distributed estimation and model
and machine learning are typically partially separable [2] –
predictive control (MPC) [11]), where the one time-cost of
i.e., the cost function is the sum of N individual costs, each
efficientlyassigningtheestimatesyieldsimproved(iterative)
dependingonlyonasmallportionoftheoveralloptimization
online performance. Although [1] focuses on distributed
variable. This structure is widely exploited in parallel algo-
Nash equilibrium problems, the END framework is flexible
rithms[2],[3]–wheremultipledistinctprocessorssharethe
and applicable to virtually any distributed decision problem.
computation cost, but having access to a common memory.
Contributions: In this paper, we apply and tailor the END
Yet, this is not the case for distributed scenarios – where
framework [1] to distributed optimization problems, thus
the processors (or agents) are constrained to communicate
unifying and generalizing several recent approaches. For the
uniquely with some neighbors over a communication net-
case of dual algorithms, our setup retrieves the formulation
work. In fact, most distributed optimization methods entails
in [11], [13] (see Proposition 1). Here we present a novel
the agents reaching consensus on the entire optimization
sparsity-aware ADMM, but one can obtain the END version
vector [4], [5], even when each agent eventually only uses a
ofvirtuallyanydualalgorithm(SectionIII-A).Further,com-
fewcomponentsofthesolution,asinresourceallocationand
paredto[11],[13],ourframeworkhasbroaderapplicability:
network control [6]. This may result in prohibitive memory
and communication requirements, and in poor scalability if (i) it can be used for primal methods. To illustrate, we
the decision vector grows with the network size. present the END version of the ABC method [5], en-
Efficient solutions are known for partitioned problems, compassingmanyestablishedalgorithms.Asanexample,
where the local cost functions (or constraints) only directly we derive a gradient-tracking iteration where each agent
couple each agent to its neighbors [6]–[10]. Notably, how- only needs to estimate a fraction of the cost gradient –
ever,thisrequiresthatthecommunicationgraphmatchesthe the END counterpart of AugDGM [16] (Section III-B);
interferencegraph(describingthecouplingamongtheagents (ii) it works on directed graphs. Specifically, we present the
in the cost or constraints), which is usually not the case for sparsity-aware version of the Push-Sum DGD algorithm
wireless and ad-hoc networks. [17], that is guaranteed to converge over time-varying
Remarkably,generalpartially-separableproblemsweread- and column stochastic graphs (Section III-C).
dressedviadistributeddualmethods,byMotaetal.[11]and
Itwillbeclearfromourargumentsthat,thankstoourpower-
fulstackednotation,theanalysisofENDalgorithmspresents
M. Bianchi is with the Automatic Control Laboratory (IfA), ETH
Zu¨rich, Switzerland (mbianch@ethz.ch). S. Grammatico is with the little complication compared to their sparsity-unaware coun-
DelftCenterforSystemsandControl(DCSC),TUDelft,TheNetherlands terparts. Nonetheless, the impact in terms of flexibility and
(s.grammatico@tudelft.nl).ThisworkissupportedbyNWOunder
performance is remarkable. We illustrate numerically this
researchprojectOMEGA(613.001.702),bytheERCunderresearchproject
COSMOS(802348)andbyETHZu¨richfunds. point on an estimation problem for wireless sensor network,
4202
rpA
32
]CO.htam[
1v37251.4042:viXrawhereweobservethatENDcandecreasethecommunication 4) agiveninterference graphGI =(P,I,EI),EI ⊆P×I,
cost by more than 90% (Section IV). specifying which components of y are indispensable for
each agent: p∈NI(i) means that agent i needs (an esti-
A. Background and notation
mateof)y toperformsomeessentiallocalcomputation;1
p
1) Basicnotation: Nisthesetofnaturalnumbers,includ- 5) abipartiteestimategraphGE =(P,I,EE),EE ⊆P×I.
ing0.R(R ≥0)isthesetof(nonnegative)realnumbers.0 q ∈ Since agents might be unable to access y, each agent
Rq (1 q ∈Rq)isavectorwithallelementsequalto0(1);I q ∈ estimates some of the components y p’s, as specified by
Rq×q is an identity matrix; the subscripts may be omitted theestimategraph:agentikeepsanestimatey
i,p
∈Rnyp
whenthereisnoambiguity.e idenotesavectorofappropriate of y
p
if and only if p∈NE(i);
dimensionwithi-thelementequalto1andallotherelements 6) P directed design graphs {GD} , GD =(NE(p),ED).
p p∈P p p
equal to 0. For a matrix A∈Rp×q, [A] i,j is the element on G pD describes how the agents that estimate y p exchange
row i and column j; null(A) := {x ∈ Rq | Ax = 0 n} and their estimates: agent i can receive y
j,p
from agent j if
range(A) := {v ∈ Rp | v = Ax,x ∈ Rq}. If A = A⊤ ∈ and only if i∈ND(j);
p
Rq×q, λ (A)=:λ (A)≤···≤λ (A)=:λ (A) denote
min 1 q max
Specifically, in this paper, we apply the END framework to
its eigenvalues. diag(A ,...,A ) is the block diagonal
1 N
the distributed optimization problems
matrix with A ,...,A on its diagonal. Given N vectors
1 N
x ,...,x ,col(x ,...,x ):=[x⊤...x⊤]⊤.⊗denotesthe (cid:80)
K1 roneckeN
r
produc1
t.
GivenN
a
positiv1
e
definN
ite matrix Rq×q ∋
ym ∈Ri nn
y
i∈If i(y), (1)
Q≻0, ⟨x|y⟩ =x⊤Qy os the Q-weighted inner product,
∥·∥
istheassoQ
ciatednorm;weomitthesubscriptsifQ=I.
where f
i
: Rny → R is a private cost function of agent i.
Q We choose the variable of interest in the END framework to
Given a function ψ : Rq → R := R∪{∞}, its set-valued
coincidewiththeoptimizationvariable2.Hence,wepartition
subdifferential operator is denoted by ∂ψ :Rq ⇒Rq :x(cid:55)→
the optimization variable as y =col((y ) ).
{v ∈Rq |ψ(z)≥ψ(x)+⟨v |z−x⟩,∀z ∈Rq}. p p∈P
The common approach [17], [18] to solve (1) over a
2) Graph theory: A (directed) graph G =(V,E) consists
communication network GC is to assign to each agent i∈I
of a nonempty set of vertices (or nodes) V ={1,2,...,V}
and a set of edges E ⊆ V × V. We denote by N(v) :=
a copy y˜
i
:= col((y i,p) p∈P) ∈ Rny of the whole decision
variable and to let the agents exchange their estimates with
{u | (u,v) ∈ E} and N(v) := {u | (v,u) ∈ E} the set
every neighbor over GC; in END notation, we write this as3
of in-neighbors (or simply neighbors) and out-neighbors of
vertex v ∈ V, respectively. A path from v 1 ∈ V to v N ∈ V EE =P ×I, G pD =GC (∀p∈P). (2)
of length T is a sequence of vertices (v ,v ,...,v ) such
1 2 T
that (v ,v ) ∈ E for all t = 1,...,T −1. G is strongly Yet, in several applications, like network control and data
t t+1
connected if there exist a path from u to v, for all u,v ∈ ranking [2], each cost function f i depends only on some of
V; in case G is undirected, namely if (u,v) ∈ E whenever the components of y, as specified by an interference graph
(v,u)∈E,wesimplysaythatG isconnected.Therestriction GI: f i depends on y p if and only if p ∈ NI(i) ⊆ P. With
of the graph G to a set of vertices VA ⊆ V is defined as some abuse of notation, we highlight this fact by writing
G| :=(VA,E∩(VA×VA)).WealsowriteG =(VA,VB,E)
VA f (y)=f ((y ) ). (3)
to highlight that G is bipartite, namely V = VA ∪VB and i i p p∈NI(i)
E ⊆ VA × VB. We may associate to G a weight matrix Clearly, the standard choice (2) for the graphs GE and
W ∈ RV×V compliant with G, namely w u,v := [W] u,v > {G pD} p∈P does not take advantage of the structure in (3). In
0 if (v,u) ∈ E, w u,v = 0 otherwise. G is unweighted if fact,agentionlyneeds(y p) p∈NI(i) toevaluate(thegradient
w u,v = 1 if (v,u) ∈ E. Given two graphs GA = (VA,EA) of) its local cost f i; storing a copy of the whole vector y
and GB =(VB,EB), we write GA ⊆GB if GA is a subgraph could be unnecessary and inefficient – especially if GI is
ofGB,i.e.,ifVA ⊆VB andEA ⊆EB;wedefineGA(cid:83) GB := sparse and n is large.
y
(VA∪VB,EA∪EB). A time-varying graph (Gk) k∈N, Gk =
(V,Ek)isQ-stronglyconnectedif(cid:83)(k+1)Q−1Gt isstrongly A. Problem-dependent design, unified analysis
t=kQ
connected for all k ∈N. From an algorithm design perspective, the graphs GC
and GI shall be considered fixed as part of the problem
II. ENDFORDISTRIBUTEDOPTIMIZATION
formulation. In contrast, the graphs GE and {GD} are
p p∈P
We first recall the general END framework [1], that de-
design choices, although with some constraints.
scribestheinformationstructureinanydistributedalgorithm. Standing Assumption 1: GE and GD are chosen such that
p
It is characterized by: GI ⊆GE and GD ⊆GC for all p∈P. □
p
1) a set of agents I :={1,2,...,N};
2) a given (directed) communication graph GC =(I,EC), 1Foreaseofnotation,assumeNI(p)̸=∅forallp∈P.
over which the agents can exchange information: agent i 2Except for Section III-D, where we instead select the dual variable
can receive data from agent j if and only if j ∈NC(i); as variable of interest; see also [1] for different possible choices (e.g.,
aggregativevalues)invariationalproblems.
3) a variable of interest y ∈ Rny, partitioned as y =
3In the following, we refer to (2) as the “standard” choice, as it is the
col((y p) p∈P), P :={1,...,P}, y
p
∈Rnyp; mostwidelystudiedscenario.WithGc=G pD wealsoimplyWC=W pD.In particular, GE ⊆GI means that each agent estimates at
least the components of y which are indispensable for local
computation. Moreover, since the estimates are exchanged
over GD and communication can only happen over GC, it
p
must hold that GD ⊆ GC. In addition, we will always need
p
some level of connectedness for each graph GD, to ensure
p
that the agents can reach consensus on the estimates of y ,
p
as for instance in the following condition.
Assumption 1: For each p∈P, GD is undirected and
p
connected. □
Designing GE and {GD} to satisfy Standing Assump-
p p∈P (a)
tion 1 and Assumption 1 is not difficult if GC is itself
undirected and connected: one trivial choice is (2). Yet,
one wishes to also consider efficiency specifications (e.g., in
terms of memory allocation, communication or bandwidth)
by imposing extra (soft) constraints on GE and {GD} .
p p∈P
We present a simple instance in Figure 1 and refer to
[1, App. A] for more examples. Such an optimal design
(b)
is in general computationally expensive. Nonetheless, the
Fig. 1: (a) A simple example of END design from [1]. On the
performance advantages in terms of algorithm execution can
left, the given communication and interference graphs, with I =
be well worth the (one-time) cost of an efficient algorithm {1,2,3,4,5} and P = {1,2}. On the right a possible choice for
design, especially in repeated problems [11], [19] (where the design graphs and the corresponding estimate graphs.
the same distributed problem is solved multiple times for (b)WefocusonthedesignofG 1D.Thegivenefficiencyspecification
is to minimize the number of copies of y (i.e., the number of
different values of some parameters/measurements). 1
nodes in GD), but provided that GD is connected and Standing
Further, while this design procedure is very problem and 1 1
Assumption 1 is met. Note that agent 2 has to estimate y (i.e.,
1
goal dependent, it does not affect the analysis of END algo- 1∈NE(2)),eventhoughagent2isnotdirectlyaffectedbyy (i.e.,
1
rithms. By simply postulating some connectedness property, 1 ∈/ NI(2)): otherwise, the information could not travel between
as in Assumption 1, we can unify the convergence analysis nodes1and3,whicharenotcommunicationneighbors.Ingeneral,
a solution to this design problem can be obtained by solving an
of standard algorithms (that use (2)) with that of methods
Unweighted Steiner Tree problem [20], for which distributed off-
specifically devised for problems with unique sparsity.
the-shelf algorithms are available [20].
B. END Notation
WenextintroducethestackedENDnotation,crucialinour
(cid:12) (cid:12) Sometimes it is useful to define agent-wise quantities,
analysis. For all p∈P, let N
p
:= (cid:12)NE(p)(cid:12) be the number
indicated with a tilde. Let
of copies of y . Recalling that that y is the estimate of y
p i,p p
kept by agent i, we define: y˜
i
:=col((y i,p) p∈NE(i)), y˜ :=col((y˜ i) i∈I)∈Rny, (10)
y
p
:=col((y i,p) i∈NE(p))∈RNpnyp, ∀p∈P; (4) where y˜
i
collects all the estimates kept by agent i.
y :=col((y p) p∈P)∈Rny, (5)
III. DISTRIBUTEDOPTIMIZATIONALGORITHMS
(cid:80)
where n := N n . Note that y collects all the Inthissection,weleveragetheENDframeworktoextend
y p∈P p yp p
copies of y kept by different agents. We denote several distributed optimization algorithms by exploiting
p
partial coupling. We recall the cost-coupled problem in (1):
WD :=diag((WD⊗I ) ), (6)
p nyp p∈P
(cid:80)
min f(y):= f ((y ) ), (11)
where W pD is the weight matrix of G pD. Let y∈Rny i∈I i p p∈NI(i)
C p :={y p ∈RNpnyp |y p =1 Np ⊗v,v ∈Rnyp}, (7) w thh ee or pe tf imi ii zs aa tiop nriv va at re iac bo ls et yfu =nc cti oo ln ((yof )agen )ti a, sa tn hd evw ae ric ah bo leos oe
f
p p∈P
be the consensus space for y p (where all the estimates of y p interest in the END; with some usual overloading, we write
(cid:81)
are equal); C := C be the overall consensus space;
p∈P p
f (y)=f ((y ) )=f ((y ) ). (12)
i i p p∈NE(i) i p p∈NI(i)
C(y):=col((1 ⊗y ) ). (8)
Np p p∈P
Let Y⋆ be the solution set of (11), assumed to be nonempty.
For each p∈P, for each i∈NE(p), we denote by
A. END dual methods
(cid:80)
i := 1 (9)
p j∈NE(p),j≤i Under Standing Assumption 1, we can recast (11) by
the position of i in the ordered set of nodes NE(p). For all introducing local estimates and consensus constraints.The
i ∈ NE(p), we denote by R
i,p
∈ Rnyp×Npnyp the matrix following redormulation is not novel, and in fact it was
that selects y from y , i.e., y =R y . employed for the dual methods in [11]–[13].
i,p p i,p i,p pProposition 1: Let Assumption 1 hold. Then, problem where z
i,p
∈ Rnyp is a local variable kept by agent i; for
(11) is equivalent to: all p∈P, A p,B p,C
p
are matrices in RNp×Np; γ > 0 is a
step size; and we recall the notation in (9). Note that if the
(cid:40) min (cid:80) f (y˜ )=f ((y ) )
y˜∈Rny i∈I i i i i,p p∈NE(i)
(13)
matrices A p,B p,C p’s are compliant with the corresponding
s.t. y =y ∀p∈P,∀(i,j)∈ED. graphs G pD’s (e.g., A p =B p =C p =W pD), then the iteration
i,p j,p p □ (15) is distributed. We can rewrite (15) in stacked form as
If GD = GC for all p∈P, then (13) reverts to the
p yk+1 =Ayk−γB∇ f(yk)−zk (16a)
formulation used in standard dual methods [21]–[23]: these y
algorithms require each agent to store a copy of the whole zk+1 =zk+Cyk+1, (16b)
optimization vector. Instead, choosing a sparse GE can con-
where A := diag((A ⊗ I ) ), B := diag((B ⊗
v de un eie tontl iy tsre sd tru uc ce tus rt ehe (in .eu .m
,
sb ee pr ac ro abn ls etra ci on st ts sin an( d13 c) o. uR pe lg ina grd cle os ns -,
I nyp) p∈P), C :=
diag((p
C
p
⊗n Iy np ypp )∈ pP
∈P) belong to
Rny×p
ny,
z := col((z ) ) with z := col((z ) ), and
straints compliant with GD, hence with the communication p p∈P p i,p i∈NE(p)
p f(y):=(cid:80) f (y˜ ). If GD =GC for all p, and A , B , C
graph), the problem in (13) can be immediately solved via i∈I i i p p p p
are independent of p, then (16) retrieves the ABC algorithm
several established Lagrangian-based algorithms (provided
[5, Eq. 3].
that the functions f ’s are sufficiently well-behaved). In
i
We next charachterize the asymptotic behavior of (16) for
practice, this allows one to extend most (virtually all) the
appropriatelychosenA,B,C(alltheproofsareinappendix).
existing dual methods to the END framework.
We recall the notation in (7)-(8).
Example 1 (END ADMM): Let Assumption 1 hold, and
Theorem 1: Let D := diag((D ⊗ I ) ), for some
assume that f
i
is proper closed convex, for all i ∈ I. p nyp p∈P
Applying the alternating direction method of multipliers {D p ∈ RNp×Np} p∈P. Assume that f i is L-smooth and
(ADMM) in [22] to (13)4 results in the iteration convex for each i∈I, and that:
(a) A=BD and B≽0, D≻0;
(cid:110) (cid:16)
y˜k+1 =argmin f (y˜ )+(cid:80) (cid:80) ∥y ∥2 (b) (∀y ∈C) Dy =y, By =y;
i i i p∈NE(i) j∈ND(i) i,p
y˜ i p (cid:17)(cid:111) (c) C≽0, null(C)=C;
−⟨z ,y ⟩
i,j,p i,p (d) B and C c √ommu √te: BC=CB;
(14a) (e) I− 1C− BD B≽0.
2
zk+1 =(1−α)zk −αzk +2αyk+1, (14b) Let y⋆ ∈Y⋆, y⋆ :=C(y⋆), and consider the merit function
i,j,p i,j,p j,i,p j,p
M(y):=max{∥Π y∥∥∇ f(y⋆)∥,|f(y)−f(y⋆)|}. (17)
where z i,j,p is an auxiliary variable kept by agent i, for ⊥ y
αeac ∈h (0i ,1∈
),
yI, p co∈ nveN rgE es(i) to, yj ⋆,∈ whN ereD p( yi) ⋆. =Th ce on l(, (yf ⋆o )r any
)
Then, for any y0 ∈ Rny, z0 = 0 ny, γ ∈ (λmin L(D)), the
is a
solutioni o,p
f (11), for all
ip
∈ I and p ∈
NE(i)p
.
p N∈ oP
te
sequence (yk) k∈N generated by (16) satisfies
that performing the update (14b) requires agent i to receive M(cid:0) yk (cid:1) ≤O(1), (18)
avg k
data from its neighbor j ∈ ND(i) (while (14b) requires no
communication). If G pD =GC fp or all p∈P, then the method for all k ∈N, where yk avg := k1 (cid:80)k t=1yt. □
retrieves the standard ADMM for consensus optimization It is shown in [5] that many celebrated schemes for con-
[22, Eq. (13)]. Yet, in general (14) requires the agents to sensusoptimizationcanberetrievedasparticularinstancesof
store and exchange less (auxiliary) variables. □ the ABC algorithm, by suitably choosing the matrices A, B,
WhileProposition1wouldholdevenifthegraphsGD’sare C [5, Tab. 2]: EXTRA [24], NEXT [18], DIGing [4], NIDS
p
only strongly connected, distributed algorithms to efficiently [25], and others. Theorem 1 allows the extension of each of
solve (13) typically require undirected communication. these methods to the END framework. We only discuss an
example below; for the other schemes, the analysis can be
B. END ABC algorithm carried out analogously, see also [5, §III.A].
Example 2 (END AugDGM): The following gradient-
Inthissubsection,weproposeanENDversionoftheABC
tracking algorithm is the END version of [16, Alg. 1]:
algorithm, recently developed in [5]. For differentiable costs
(∀i∈I)(p∈NE(i))
f ’s, let us consider the iteration: (∀i∈I)(∀p∈NE(i))
i
(cid:88)
(cid:88) yk+1 = [WD] (yk −γvk )
yk+1 =−zk + [A ] yk −γ[B ] ∇ f (y˜k) i,p p ip,jp j,p j,p
i,p i,p p ip,jp j,p p ip,jp yp j j
j∈ND(i)
j∈NE(p) p
(cid:88)
(15a) vk+1 = [WD] (vk +∇ f (y˜k+1)−∇ f (y˜k)),
i,p p ip,jp j,p yp j yp j
(cid:88)
zk+1 =zk + [C ] yk+1, (15b) j∈ND(i)
i,p i,p p ip,jp j,p p
j∈NE(p) or, in stacked form,
4Afterdecouplingtheconstraintsin(13)byintroducingauxiliarybridge yk+1 =WD(yk−γvk) (20a)
variables as {y i,p = h (i,j),p,h (i,j),p = h (j,i),p,h (j,i),p = y j,p}; the vk+1 =WD(vk+∇ f(yk+1)−∇ f(yk)); (20b)
approachisstandardandwereferto[22]foracompletederivation. y ywe impose y(0) = 0, v(0) = WD∇ f(y0). Here, v (instead of one overall), but does not store and exchange the
y i,p
represents an estimate of ∇ yp(cid:80) j∈If j(y)/N
p
kept by agent variables z
i,p
∈Rnyp for p∈/ NE(i).
i. Note that agent i only estimates and exchanges the Assumption 2: For all k ∈N and p∈P, it holds that:
components of the cost gradient (and of the optimization
(i) Self-loops: for all i∈NE(p), (i,i)∈ED,k;
variable) specified by NE(i), instead of the whole vector p
(ii) Column-stochasticity: 1⊤ WD,k =1⊤ ;
as in [16, Alg. 1] – the two algorithms coincide only if Np p Np
(iii) Finite weights: [WD,k] ≥ν >0, ∀(i,j)∈ED,k. □
W pD = WC for all p∈P. By eliminating the v variable p ip,jp p
Assumption 3: There exists an integer Q > 0 such that,
in (20), we obtain
yk+2 =2WDyk+1−(WD)2 yk for Ea xl al mp p∈ leP 3, (( CG hpD o, ok s) ik n∈ gN tii ms eQ -v-s at rr yo in ng gly dec so in gn ne gct re ad p.
hs):
A□
s-
(21)
−γ(WD)2 (∇ yf(yk+1)−∇ yf(yk)). sume GC ⊆ (cid:83)( t=k+ kQ1)Q−1GC,k, for all k ∈N and some
strongly connected graph GC. Choose some graphs (GD)
Instead, eliminating z from (16) we get p p∈P
that satisfy Standing Assumption 1 and such that each GD
p
yk+2 =(I−C+A)yk+1−Ayk is strongly connected. Then, Assumption 3 holds by settihg
−γB(∇ yf(yk+1)−∇ yf(yk)). (22) G pD,k =G pD(cid:84) GC,k, for all p∈P and all k ∈N. □
Theorem 2: Let Assumptions 2 and 3 hold. Assume that,
whichretrieves(21)forA=B=(WD)2 ,C=(I−WD)2
.5 for all i ∈ I, f is convex and there is L > 0 such that
i
This choice satisfies the conditions in Theorem 1, with D= ∥g i∥≤L, for all y ∈Rny and g i ∈∂ yf i(y). Let (γk) k∈N be
I, under Assumption 1 and doubly stochasticity.6 apositivenon-increasingsequencesuchthat(cid:80)∞ γk =∞,
k=0
Corollary 1: Let Assumption 1 hold; assume that (cid:80)∞ k=0(γk)2 <∞.Then,thesequence(yk) k∈N generatedby
WD1 = 1 , WD = WD⊤ , for all p∈P, and that f is (23) converges to C(y⋆), for some y⋆ ∈Y⋆. □
p Np Np p p i
L-smoothandconvex,foralli∈I.Then,foranyγ ∈(0, 1)
L
the rate (18) holds for (20). □ D. Constraint-coupled distributed optimization
Theorem 1 requires a recovery procedure (i.e., (18) holds
Finally, we study a different, constraint-coupled problem:
for the running average only), as e.g. in [26], but pointwise
convergence could be shown for several special cases of
 (cid:80)
min f (x ) (24a)
(16), see e.g. [16]. We note that Theorem 1 enhances cus-  xi∈Rnxi,i∈I i∈I i i
tomizability with respect to [5, Th. 24], even in the standard (cid:80)
scenario (2) (the sparsity-oblivious case), by allowing for  s.t. i∈NI(p)A p,ix i−a p,i =0, ∀p∈P (24b)
non-identical blocks A ’s, B ’s, C ’s – corresponding to
p p p for a given interference graph GI =(P,I,EI), where f and
integrating different methods for the components of y. i
{A
p,i
∈Rnyp×nxi,a
p,i
∈Rnyp} p∈NI(i)areprivatedatakept
C. END Push-sum DGD by agent i; and the constraints (24b) are not compliant with
Techniquestosolveoptimizationproblemsoverswitching the communication graph GC, namely NI(p) ̸⊆ NC(i) for
or directed graphs also find their counterpart in the END any i. Differently from what we did with the cost-coupled
framework.Asanexample,herewegeneralizethepush-sum problem in (11), here we choose as the variable of interest
subgradient algorithm in [17, Eq. (1)]. the dual variable associated with the constraints in (24b),
Let the agents communicate over a time-varying network y = col((y p) p∈P) ∈ Rny. Typical distributed methods to
(GC,k) k∈N, GC,k = (I,EC,k). Given a fixed estimate graph solve (24) require each agent to store a copy of the entire
GE ⊇ GI, for each p∈P we consider a time-dependent
dualvariable(andpossiblyofothervariablesinRny,e.g.,an
designgraph(G pD,k) k∈N,G pD,k =(NE(p),E pD,k)⊆GC,k (note estimate of the constraint violation) [27], [28]. END primal-
that the set of nodes is fixed in GD,k). For all i ∈ I and dual or dual methods can improve efficiency by exploiting
p
p∈NE(i), agent i performs the following updates: the sparsity of GI. For instance, (a simplified version of) the
algorithm in [1, Eq. (31)] can be directly used to solve (24).
qk+1 =(cid:80) [WD,k] qk (23a)
i,p j∈NE(p) p ip,jp j,p Alternatively, let us consider the dual of (24):
wk+1 :=(cid:80) [WD,k] zk (23b)
i,p j∈NE(p) p ip,jp j,p max (cid:80) φ ((y ) ), (25)
wk+1 y∈Rny i∈I i p p∈NI(i)
gk+1 ∈∂ f (y˜k+1), yk+1 := i,p (23c)
i,p yp i i i,p q ik ,+ p1 φ i(y) := min xi∈Rnxi f i(x i)+(cid:80) p∈NI(i)⟨y p,A p,ix i−a p,i⟩;
zk+1 =wk+1−γkgk+1, (23d) note that (25) is in the form (11). In fact, (25) was
i,p i,p i,p
solved in [12] via the reformulation (13); this approach
initialized at z0 i,p ∈ Rnyp, q i0 ,p = 1. With respect to [17, hasthedisadvantageofrequiringundirectedcommunication.
Eq. (1)], agent i keeps one scalar q i,p for each p ∈ NE(i) Nonetheless, (25) can also be solved over directed (time-
varying) networks, e.g., via the iteration in (23).7
5Infact,thesequence(yk)generatedby(16)coincidewiththatgenerated
by(20)forthegiveninitialization.
b nl
uo6
lc
lN
(k
Iot
s
−e
tru
Wth ca
t
pDut
r
)et 2he
i
=np r(r a6o n)p
.
ge er
F
(ti
o
1e rs
Ni
po
n
)f
st
aaW
nn
dcpD e’ n,s
uu
le
ln
(a
d
Is eil
r
−y
t
Whtr ean Dss
t
)l
a
2a tt ee =dt Co
co
.W ndiD tiod nu s,e ct lo eat rh lye
t wh
ie7 thI lf
o
xce
⋆
iaa (lc y˜dh
iu
)f ali ∈fi us
an
rc
c
go
t
min ov
n
ie nx
φ
xiiw ∈ci Rath
nn
xc
b
io em
fc
ip
o
(a
m
xc ipt
)u
+d to
e
(cid:80)dm aa psi ∈n g,
Nk
iw
, Ip
(h ie )=r ⟨e yAt ih
,p
pe
,
,ixs Au
⋆ i
pb
(
,g
y
i˜r
xk
ia id
)
−i −en aat ps
p
,i,o ⟩if
.,102
100
10-2
100 103 106
10-1
Fig. 2: Distribution of sources (red) and sensors (blue). Sensors
in the red circle receive signal from source p. Sensors in the blue 10-2
circle can receive data by (but not necessarily send to) sensor i.
10-3
IV. ILLUSTRATIVEAPPLICATION
In this section we study numerically a regression problem
10-4
with sparse measurements [2], [13], arising from distributed 0.05 0.1 0.15 0.2 0.25
estimation in wireless and ad-hoc sensor networks. Let us
consider some sensors {1,2,...,N}=:I and some sources Fig. 3: Linear regression via algorithm (23), for different values
{1,2,...,P} =: P, spatially distributed on a plane in the of the minimum sensor communication radius r cmin and stopping
criterionV(y)≤10−2 (bottom),andthetrajectoriesobtainedwith
square[0,1]×[0,1],asillustratedinFigure2.Eachsourcep
rmin =0.1 (top). A larger rmin induces a denser graph GC.
emits a signal y¯ ∈R, sensed by all the sensors in a radius c c
p
r >0; in turn, each sensor i measures
s
uniformly drawing entries in [0,1] and then normalizing
h :=H col((y¯ ) )+w , (26)
i i p p∈NI(i) i the rows to unitary norm, we draw each element of w
i
where h
i
∈ Rnhi, H
i
is a known output matrix, w
i
is the from an unbiased normal distribution with variance 0.1;
measurement noise. Sensor i can send information to all the each signal y¯ is uniformly randomly chosen in [0,1];
p
peers in a radius ri (e.g., proportional to the sensor specific the step size is set as γk = k−0.51 in (23).9 The ad-
c
power); this induces a directed communication network vancement is evaluated via the merit function V(y) :=
GC = (I,EC) among the sensors, which we assume to be max{∥diag(( 1 I) )Π y∥∥∇ f(y⋆)∥,|f(Π y)−f(y⋆)|},
Np p∈P ⊥ y ∥
strongly connected. where y⋆ = C(y⋆) and y⋆ solves (27). Figure 3 shows the
1) Linear regression: In our first simulation, the sensors’ results for different values of rmin. For rmin = 0.1, the
c c
goal is to collaboratively solve the least square problem customized method is 15 times faster then the standard one.
Increasing rmin only marginally reduces the per-iteration
(cid:88)(cid:13) (cid:13)2 c
ym ∈Rin
P
i∈I(cid:13) (cid:13)h i−H icol((y p) p∈NI(i))(cid:13)
(cid:13)
, (27) c ao lrm eam dyun fic oa rti ro cmn inco =st 0o .f 25t ,he thecu gs rto apm hiz Ged
C|
Nm Ie (t ph )od is. sI tn ronfa gc lt y,
where NI(i) is the set of sources positioned less than r connected for all p∈P, so GE = GI can be chosen (in
s
away from sensor i. Problem (27) is in the form (11). We other terms, each agent only estimates and exchanges the
seekasolutionviaalgorithm(23)(withfixedcommunication components of y that directly affect its local cost, while it
graph), comparing the performance for two choices of the also has to estimate other components for smaller r cmin). In
design graphs: this situation, the customized method achieves a reduction
of the communication cost (where sending a variable to all
• Standard: G pD’s are chosen as in (2): with this choice, (23)
the neighbors on GC has a cost of 1, in a broadcast fashion)
boils down to the standard Push-sum DGD [17].
of over 99.9%.
• Customized: each G pD is designed to exploit the sparsity
2) LASSO: Next,weassumethatonly30%ofthesources
in (27). In particular, we aim at optimizing the memory
emits a signal at a given instant (the vector y¯ is sparse).
allocation for the estimates, by minimizing the number of
The sensors collaboratively solve the following problem,
nodesinGD,providedthatGD mustbestronglyconnected
p p regularized to promote sparsity,
(and Standing Assumption 1 must be satisfied). Design-
i Sn tg ronsu gc lyh Ca oG nnpD ecc to er dre Ss tp eo inn ed rs St uo bg(a rap pp hro Px ri om ba lete mly [) 29so ]l (v win hg erea ym ∈Rin
P
∥y∥ 1+(cid:88)(cid:13) (cid:13) (cid:13)h i−H icol(y p) p∈NI(i)(cid:13) (cid:13) (cid:13)2 ,
i∈I
GD is a subgraph of GC)8.
p where ∥·∥ is the ℓ norm. By defining f ((y ) ) =
We set N = 100, P = 20, and randomly generate ∥h −H co1 l((y ) 1 )∥2+(cid:80) i 1 p |p y∈N |,I w(i e) re-
sensor/sources positions as in Figure 2. We choose r = i i p p∈NI(i) p∈NI(i) |NI(p)| p
s
0.2, and draw each ri uniformly in [rmin,rmin +0.1]. For trieve the form (11). We set N = 10, P = 20, r cmin =
c c c
all i ∈ I, we fix n = 10, we generate H by first
hi i 9Although the bounded subgradient assumption in Theorem 2 fails,
boundedness of the sequences generated by (23), and hence convergence,
8Weusealltheavailableedges,i.e.,G pD=GC| NE(p). canbeestablishedbasedoncoercivityofthecostfunction.100
for all k ≥1. Hence, we rewrite (16) as
yk =Byk, zk =γBzk (28a)
10-1 yk+1 =Dyk−γ(∇ yf(yk)+zk) (28b)
zk+1 =zk+ 1Cyk+1 (28c)
γ
forallk ≥1.LetΦ(y,z):=f(y)+⟨y,z⟩;theformin(28)
10-2
can be exploited to prove the following lemma.
Lemma 1: Let (yk,yk,zk) be a sequence generated by
0.2 0.3 0.4 0.5 0.6 0.7 0.8 (28). Then, for all y ∈C, z ∈C , it holds that:
⊥
Φ(yk+1,z)−Φ(y,z)≤ 1 h(y,z),
Fig. 4: LASSO via algorithm (23), and different source ranges r. avg 2k
s
where h(y,z) := 1∥y0 −y∥2 +γ∥B−Π ∥∥ ∥z∥2 and λ :=
γ D λ
min{(λ (C )) }. □
2 p p∈P
0.1, n hi = 1 for all i, generate random positions for Proof:Theproofisanalogoustothatof[5,Lemma23],
the sensors and sources, and choose the other parameters and omitted here. Note that [5] uses a matrix notation (i.e.,
as above, for both the standard and customized methods. y ∈RI×n),whileweneedastackednotation(asthevectors
Figure 4 compares the results for different values of r s. For (y i)
i∈I
are not homogeneous in size). Nonetheless, (28)
larger r s, the interference graph GI is denser, and the gap matches[5,Eq.(33)],whichallowsustorepeatallthesteps
between customized and standard method decreases: in fact, in [5, Lem. 23] (with the only precaution of replacing J,
f No or nr es th= el0 es.8 s,th we ht ew no Ga Ilg io sri st ph am rss ec ,o ti hn eci cd ue s, ta os mG izI ei ds c ao lgm op ril te ht me. Fsp oa rn( a1 llm z), λ ∈2( CC) i (n so[5 t] haw tit ⟨h zΠ ,y∥ ⋆, ⟩C, =λ) 0.
), setting y = y⋆
⊥
saves up to 99% of the communication cost. in Lemma 1, together with the definition of Φ, yields
In conclusion, while requiring some initial computational f(yk )−f(y⋆)+⟨yk ,z⟩≤ 1 h(y⋆,z).Furtherchoosing
avg avg 2k
effort to choose the design graphs G pD, the sparsity-aware z =2 Π ⊥yk avg ∥z⋆∥, with z⋆ :=−∇ f(y⋆), leads to
method results in substantial efficiency improvement – es- ∥Π ⊥yk avg∥ y
pecially if the estimation problem is solved repeatedly, e.g., f(yk )−f(y⋆)+2∥z⋆∥(cid:13) (cid:13)Π yk (cid:13) (cid:13)≤ 1 h(y⋆,2z⋆). (29)
avg ⊥ avg 2k
each time new signals are received from the sources.
By convexity and since z⋆ ∈C (by optimality conditions),
⊥
it holds that f(yk ) − f(y⋆) ≥ −⟨yk − y⋆,z⋆⟩ =
avg avg
V. CONCLUSION −⟨Π yk ,z⋆⟩≥−∥Π yk ∥∥z⋆∥;thelatterinequalityand
⊥ avg ⊥ avg
(29) imply M(yk )≤ 1 h(y⋆,2z⋆). ■
We have shown that the END framework [1] can be avg 2k
B. Proof of Theorem 2
applied to a variety of distributed optimization problems, to
enhance efficiency by accounting for the intrinsic sparsity Note that, for each p∈P, (23) is the standard perturbed
in the agents coupling. Besides revisiting dual methods, we push-sum protocol [17, Eq. (4)], with perturbation term
derived the END (i.e., sparsity aware) version of the very −γkgk+1. Therefore, since gk+1 is uniformly bounded by
i,p i,p
generalABCmethodandofPush-sumDGD;andweshowed assumptionandbythechoiceof(γk) k∈N,wecanapply[17,
how to efficiently tackle constraint-coupled problems with Lem. 1] to infer that, for all i∈I, p∈NE(i)
sparse constraints, even over directed graphs. Our simula-
lim ∥yk −z¯k∥=0, (30)
tionsshowthatENDalgorithmscansubstantiallyreducethe k→∞ i,p p
computational and communication cost, while introducing (cid:80)∞ γk∥yk −z¯k∥=0, (31)
k=0 i,p p
l ti ht etl ie
r
sc po am rsp il ti yc -a ut nio an wain reth ce ouc no tn ev rpe arg rte sn .ce proof with respect to where z¯k
p
:= N1
p
(cid:80) i∈NE(p)zk
i,p
∈ Rnyp, for all k ∈N. Let
As sparsity-aware END algorithms require some initial
us also define z¯k := col((z¯k p) p∈P) ∈ Rny. By (23) and
Assumption 2(ii), it follows that
design effort for the allocation of the estimates, their use is
particularly recommended for problems with special struc- z¯k+1 =z¯k−γk 1 (cid:80) gk+1. (32)
p p Np i∈NE(p) i,p
ture [1, App. A.4], or repeated/time-varying problems like
Wenextshowthatlim z¯k =y⋆ ∈Y⋆;then,thetheorem
distributedestimationandMPC.Futureworkshouldfocuson k→∞
follows by (30). The main complication with respect to the
computationallyefficientanddistributedmethodstoperform
proof of [17, Th. 1] is that we need a modification of [17,
theallocationoftheestimatesonline,thusavoidingtheneed
Lem. 8] to cope with the non-homogeneity of the estimates.
for any a priori design.
Lemma 2: For all y⋆ ∈Y⋆, for all k ∈N, it holds that
∥z¯k+1−y⋆∥2 ≤∥z¯k−y⋆∥2 −2γk(f(z¯k)−f(y⋆))
A. Proof of Theorem 1 D D
(cid:88) (cid:88)
+4Lγk ∥z¯k−y˜k+1∥
We adapt the proof of [5, Th. 24]. We note that z0 = p i,p
i∈Ip∈NE(i)
0 ∈range(B); by the conditions (a) and (d), the update in
(1n 6y
),andaninductionargument,wehaveyk,zk ∈range(B),
+(γk)2NL2,where D:=diag((N pI np) p∈P). □ [9] P. Giselsson, M. D. Doan, T. Keviczky, B. D. Schutter,
Proof: By (32), we have and A. Rantzer, “Accelerated gradient methods and dual
decomposition in distributed model predictive control,”
∥z¯k+1−y⋆∥2 =∥z¯k−y⋆∥2 Automatica, vol. 49, no. 3, pp. 829–833, 2013. [Online]. Available:
D D https://www.sciencedirect.com/science/article/pii/S0005109813000101
−2γk(cid:88)(cid:68)
z¯k−y⋆,(cid:80)
gk+1(cid:69)
[10] E. Dall’Anese, H. Zhu, and G. B. Giannakis, “Distributed optimal
p p i∈NE(p) i,p powerflowforsmartmicrogrids,”IEEETransactionsonSmartGrid,
p∈P vol.4,no.3,pp.1464–1475,2013.
+(γk)2(cid:88) N1
p
(cid:13) (cid:13) (cid:13)(cid:80) i∈NE(p)gk i,+ p1(cid:13) (cid:13) (cid:13)2 . [11] oJ. pF ti. mM izo at ta io, nJ. wM it. hX loa cv aie lr, doP m.M ain. sA :g Au pia pr l, ica an td ioM ns. iP nu Msch Pe Cl, a“ nD dis ntr ei tb wu ote rkd
p∈P flows,” IEEE Transactions on Automatic Control, vol. 60, no. 7, pp.
(33) 2004–2009,2015.
[12] S. A. Alghunaim, K. Yuan, and A. H. Sayed, “A proximal diffusion
The third addend on the right-hand side of (33) is bounded strategy for multiagent optimization with sparse affine constraints,”
IEEETransactionsonAutomaticControl,vol.65,no.11,pp.4554–
above by (γk)2NL2. For the second addend, we have
4567,2020.
(cid:88)(cid:68)
z¯k−y⋆,(cid:80)
gk+1(cid:69) [13] sS t. ocA h. asA til cgh ou pn ta imim iza an tid onA ,”. IH E. ES Eay Te rd a, ns“ aD ci ts it or nib sut oe nd Aco uu top mle ad tim cu Clt oia ng tre on lt
,
p p i∈NE(p) i,p
vol.65,no.1,pp.175–190,2020.
p∈P
[14] P. Rebeschini and S. Tatikonda, “Locality in network optimization,”
=(cid:88) (cid:88) (cid:10) (z¯k−yk+1)+(yk+1−y⋆),gk+1(cid:11)
IEEETransactionsonControlofNetworkSystems,vol.6,2019.
p i,p i,p p i,p
[15] R.Brown,F.Rossi,K.Solovey,M.Tsao,M.T.Wolf,andM.Pavone,
i∈Ip∈NE(i) “On local computation for network-structured convex optimization
(a)(cid:88) in multi-agent systems,” IEEE Transactions on Control of Network
≥ −L∥col((z¯k) )−y˜k+1∥+f (y˜k+1)−f (y⋆) Systems,2021.
p p∈NE(i) i i i i
[16] J.Xu,S.Zhu,Y.C.Soh,andL.Xie,“Augmenteddistributedgradient
i∈I
methods for multi-agent optimization under uncoordinated constant
( ≥b)(cid:88) −2L∥col((z¯k) )−y˜k+1∥+f (z¯k+1)−f (y⋆), stepsizes,” in 2015 54th IEEE Conference on Decision and Control
p p∈NE(i) i i i (CDC),2015,pp.2055–2060.
i∈I [17] A. Nedic´ and A. Olshevsky, “Distributed optimization over time-
varying directed graphs,” IEEE Transactions on Automatic Control,
wherein(a)weusedthatgk+1 ∈∂ f (y˜k+1)andconvexity vol.60,no.3,pp.601–615,2015.
of f , and (b) follows by ai dding any˜ di si ubi tracting (inside the [18] P. D. Lorenzo and G. Scutari, “NEXT: In-network nonconvex opti-
i mization,” IEEE Transactions on Signal and Information Processing
sum) f ((z¯k+1) ) = f (z¯k+1) and by L-Lipschitz
i p p∈NE(i) i overNetworks,vol.2,no.2,pp.120–136.
continuityoff .Theresultfollowsbysubstitutingthebound [19] M. Bianchi and S. Grammatico, “Fully distributed Nash equilibrium
i
back into (33). seeking over time-varying communication networks with linear con-
vergencerate,”IEEEControlSystemsLetters,vol.5,no.2,pp.499–
We finally note that, due to (31) and the choice of (γ k) k∈N,
504,2021.
the inequality in Lemma 2 satisfies all the conditions of [17, [20] J.C.ParinyaandFakcharoenphol,“Simpledistributedalgorithmsfor
Lem. 7], in the norm ∥·∥ ; hence we can conclude that approximatingminimumSteinertrees,”L.Wang,Ed. SpringerBerlin
D Heidelberg,2005,pp.380–389.
z¯k →y⋆, for some y⋆ ∈Y⋆. ■
[21] S.Boyd,N.Parikh,E.Chu,B.Peleato,andJ.Eckstein,“Distributed
optimization and statistical learning via the alternating direction
REFERENCES methodofmultipliers,”FoundationsandTrendsinMachineLearning,
vol.3,2010.
[1] M. Bianchi and S. Grammatico, “The END: Estimation Network [22] N. Bastianello, R. Carli, L. Schenato, and M. Todescato, “Asyn-
Design for games under partial-decision information,” IEEE chronous distributed optimization over lossy networks via relaxed
TransactionsonControlofNetworkSystems,Acceptedforpublication. ADMM: Stability and linear convergence,” IEEE Transactions on
[Online].Available:https://arxiv.org/abs/2208.11377 AutomaticControl,vol.66,no.6,pp.2620–2635.
[2] I.NecoaraandD.Clipici,“Parallelrandomcoordinatedescentmethod [23] C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic´, “A dual approach
forcompositeminimization:Convergenceanalysisanderrorbounds,” for optimal algorithms in distributed optimization over networks,”
SIAMJournalonOptimization,vol.26,no.1,pp.197–226,2016. OptimizationMethodsandSoftware,vol.36,pp.1–37,2021.
[3] P. Richta´rik and M. Taka´cˇ, “Distributed coordinate descent method [24] W.Shi,Q.Ling,G.Wu,andW.Yin,“EXTRA:Anexactfirst-order
for learning with big data,” Journal of Machine Learning Research, algorithmfordecentralizedconsensusoptimization,”SIAMJournalon
vol.17,2016. Optimization,vol.25,no.2,pp.944–966,2015.
[4] A.Nedic´,A.Olshevsky,,andW.Shi,“Achievinggeometricconver- [25] Z.Li,W.Shi,andM.Yan,“Adecentralizedproximal-gradientmethod
gence for distributed optimization over time-varying graphs,” SIAM withnetworkindependentstep-sizesandseparatedconvergencerates,”
JournalonOptimization,vol.27,no.4,pp.2597–2633,2017. IEEE Transactions on Signal Processing, vol. 67, no. 17, pp. 4494–
[5] J. Xu, Y. Tian, Y. Sun, and G. Scutari, “Distributed algorithms for 4506.
composite optimization: Unified framework and convergence analy- [26] G. Qu and N. Li, “Harnessing smoothness to accelerate distributed
sis,”IEEETransactionsonSignalProcessing,vol.69,pp.3555–3570, optimization,” IEEE Transactions on Control of Network Systems,
2021. vol.5,pp.159–166,2018.
[6] I.Notarnicola,R.Carli,andG.Notarstefano,“Distributedpartitioned [27] A. Falsone, I. Notarnicola, G. Notarstefano, and M. Prandini,
big-data optimization via asynchronous dual decomposition,” IEEE “Tracking-ADMM for distributed constraint-coupled optimization,”
TransactionsonControlofNetworkSystems,vol.5,no.4,pp.1910– Automatica,vol.117,p.108962,2020.
1919,2018. [28] X. Li, G. Feng, and L. Xie, “Distributed proximal algorithms for
[7] T.Erseghe,“Adistributedandscalableprocessingmethodbasedupon multiagent optimization with coupled inequality constraints,” IEEE
ADMM,”IEEESignalProcessingLetters,vol.19,no.9,pp.563–566, Transactions on Automatic Control, vol. 66, no. 3, pp. 1223–1230,
2012. 2021.
[8] M. Todescato, N. Bof, G. Cavraro, R. Carli, and L. Schenato, [29] M. Charikar, C. Chekuri, T. yat Cheung, Z. Dai, A. Goel, S. Guha,
“Partition-basedmulti-agentoptimizationinthepresenceoflossyand andM.Li,“ApproximationalgorithmsfordirectedSteinerproblems,”
asynchronouscommunication,”Automatica,vol.111,p.108648,2020. JournalofAlgorithms,vol.33,pp.73–91,1999.