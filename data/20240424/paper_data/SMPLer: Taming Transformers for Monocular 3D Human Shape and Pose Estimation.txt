IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 1
SMPLer: Taming Transformers for Monocular 3D
Human Shape and Pose Estimation
Xiangyu Xu, Lijuan Liu, Shuicheng Yan
Abstract‚ÄîExistingTransformersformonocular3Dhumanshapeandposeestimationtypicallyhaveaquadraticcomputationand
memorycomplexitywithrespecttothefeaturelength,whichhinderstheexploitationoffine-grainedinformationinhigh-resolution
featuresthatisbeneficialforaccuratereconstruction.Inthiswork,weproposeanSMPL-basedTransformerframework(SMPLer)to
addressthisissue.SMPLerincorporatestwokeyingredients:adecoupledattentionoperationandanSMPL-basedtarget
representation,whichalloweffectiveutilizationofhigh-resolutionfeaturesintheTransformer.Inaddition,basedonthesetwodesigns,
wealsointroduceseveralnovelmodulesincludingamulti-scaleattentionandajoint-awareattentiontofurtherboostthereconstruction
performance.ExtensiveexperimentsdemonstratetheeffectivenessofSMPLeragainstexisting3Dhumanshapeandposeestimation
methodsbothquantitativelyandqualitatively.Notably,theproposedalgorithmachievesanMPJPEof45.2mmontheHuman3.6M
dataset,improvinguponMeshGraphormerbymorethan10%withfewerthanone-thirdoftheparameters.Codeandpretrained
modelsareavailableathttps://github.com/xuxy09/SMPLer.
IndexTerms‚Äî3Dhumanshapeandpose,Transformer,attention,multi-scale,SMPL,joint-aware
‚ú¶
1 INTRODUCTION
MONOCULAR3Dhumanshapeandposeestimationisa resolution image features in Transformers, which possess
fundamental task in computer vision, which aims to abundantfine-grainedfeatures[30],[31],[32]thatarebene-
recover the unclothed human body shape as well as its 3D ficialforaccurate3Dhumanshapeandposerecovery.
posefromasingleinputimage[3],[4],[5],[6],[7].Ithasbeen In this work, we propose two strategies to improve the
widely used in many applications including visual track- Transformer framework to better exploit higher-resolution
ing [8], [9], virtual/augmented reality [10], [11], [12], [13], image features for high-quality 3D body shape and pose
[14],[15],[16],[17],[18],motiongeneration[19],[20],image reconstruction.Oneofthemisattentiondecoupling.Wenotice
manipulation [21], [22], and neural radiance field [23], [24], that different from the original ViT [29] where the image
[25].Differentfrommulti-view3Dreconstructionwherethe features are learned by attention operations, the 3D hu-
solution is well restricted by geometrical constraints [26], man Transformers [1], [2] usually rely on Convolutional
this task is particularly challenging due to the inherent Neural Networks (CNNs) to extract these features, and
depthambiguityofasingle2Dimage,andusuallyrequires the attention operations are mainly used to aggregate the
strong prior knowledge learned from large amounts of image features to improve the target embeddings. Thus, it
data to generate plausible results. Thus, the state-of-the-art is less important to model the feature-feature and feature-
algorithms [1], [2] use Transformers [27] for this task due targetcorrelationsinFigure1(a),astheydonothavedirect
to their powerful capabilities of grasping knowledge and effects on the target. We therefore propose to decouple the
learningrepresentationsfromdata[28],[29]. full attention operation into a target-feature attention and
Existing Transformers for monocular 3D human shape a target-target attention, which are cascaded together to
and pose estimation [1], [2] generally follow the ViT avoidthequadraticcomplexity.AsshowninFigure1(b),the
style [29] to design the network. As shown in Figure 1(a), decoupled attention only has a computation and memory
thetargetembeddingsarefirstconcatenatedwiththeinput complexity of O(l Fl T +l T2) which is linear with respect to
features and then processed by a full attention layer that thefeaturelengthl F.
models all pairwise dependencies including target-target, The other strategy we propose for improving the previ-
target-feature,feature-target,andfeature-feature.Whilethis ous 3D human Transformer is SMPL-based target representa-
designhasachievedimpressiveresults,itleadstoquadratic tion. Existing Transformers for 3D human shape and pose
computation and memory complexity with respect to the estimation mostly adopt a vertex-based target representa-
lengthoftheimagefeature,i.e.O((l F +l T)2),wherel F and tion [1], [2], where the embedding length l T is often quite
l T arethenumbersoffeatureandtargettokens,respectively large,equalingthenumberofverticesonabodymesh.Even
(orsimplythelengthsoftheimagefeatureF andthetarget with the proposed attention decoupling strategy, a large l T
embedding T). Such complexity is prohibitive for a large canstillbringaconsiderablecostofcomputationandmem-
l F, hindering the existing methods from employing high- ory,whichhinderstheusageofhigh-resolutionfeatures.To
addressthisissue,weintroduceanewtargetrepresentation
basedontheparametricbodymodelSMPL[33],withwhich
‚Ä¢ X. Xu is with Xi‚Äôan Jiaotong University, Xi‚Äôan, China. E-mail: xuxi-
angyu2014@gmail.com. weonlyneedtolearnasmallnumberoftargetembeddings
‚Ä¢ L.LiuiswithSeaAILab,Singapore.E-mail:liulj@sea.com. thataccountforthehumanbodyshapeaswellas3Dbody
‚Ä¢ S.YaniswithSkyworkAI,Singapore.E-mail:shuicheng.yan@gmail.com.
partrotations.AsillustratedinFigure1(c),theSMPL-based
4202
rpA
32
]VC.sc[
1v67251.4042:viXraIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 2
Target: ùëá Feature: ùêπ
Feature: ùêπ Target: ùëá
Feature: ùêπ Target:
Target: ùëá
Target: ùëá Target:
Feature: ùêπ
(a) Full-Vertex (b) Decoupled-Vertex (c) Decoupled-SMPL
Fig. 1. Two key designs of the proposed Transformer. The sub-caption ‚ÄúA-B‚Äù denotes the attention form ‚ÄúA‚Äù and the target representation
‚ÄúB‚Äù, respectively. The vertical and horizontal lines around the rectangles represent query and key in the attention operation. Red indicates
source image features, blue indicates target output representation, and the colors within the rectangles represent the interactions between
them.(a)ExistingTransformersfor3Dhumanreconstruction[1],[2]typicallyadoptaViT-stylefullattentionoperationandavertex-basedtarget
representation,hinderingtheutilizationofhigh-resolutionimagefeatures.Incontrast,weproposeadecoupledattention(b)andanSMPL-based
targetrepresentation(c),whicheffectivelyaddresstheaboveproblemandimprovereconstructionperformance.lT,lT,andlF arethelengthsof
thevertex-basedembedding,SMPL-basedembedding,andimagefeatures,respectively.Theareaofeachrectangledenotesthecomputationand
memorycomplexityoftheattentionoperation.PleaserefertoSection3.1formathematicalexplanations.
representationT furtherlessensthecomputationandmem- 2.1 3DHumanShapeandPoseEstimation
ory burden compared to the vertex-based representation
Recentyearshavewitnessedsignificantprogressinthefield
withl T ‚â™l T. ofmonocular3Dhumanshapeandposeestimation[1],[2],
Combining the above two strategies leads to a much [3], [5], [6], [7], [35], [36], [37], [38], [39], [40], [41], [42],
more concise attention learning process, which not only [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53],
allows the utilization of high-resolution features in the [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64],
Transformer, but also motivates us to explore more new [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75],
designsforbetter3Dhumanshapeandposeestimation.In [76]. Due to the intrinsic ambiguity of 2D-to-3D mapping,
particular,enabledbytheliftedefficiencyofourmodel,we this problem is highly ill-posed and requires strong prior
introduce a multi-scale attention operation that effectively knowledge learned from large datasets to regularize the
exploitsthemulti-scaleinformationinasimpleandunified solutions.Indeed,theprogressof3Dhumanshapeandpose
framework. Further, as the proposed target representation estimation largely relies on the development of powerful
explicitlydescribes3Drelativerotationsbetweenbodyparts data-drivenmodels.
whicharemostlylocal,wefurtherproposeajoint-awareat- As a pioneer work, SMPLify [3] learns a Gaussian
tention module that emphasizes local regions around body Mixture model from CMU marker data [77] to encourage
jointstobetterinferthearticulationstatusofthe3Dhuman. plausible 3D poses, which however needs to conduct in-
Tosummarize,wemakethefollowingcontributions: ference in a time-consuming optimization process. To ad-
dress this low efficiency issue, more recent methods use
‚Ä¢ By introducing the attention decoupling and SMPL- deep learning models to directly regress the 3D human
basedtargetrepresentation,weproposeanewTrans- meshinanend-to-endfashion,whichhaveshownpowerful
former framework, SMPLer, that can exploit a large capabilities in absorbing prior knowledge and discovering
number of feature tokens for accurate and efficient informativepatternsfromalargeamountofdata.Typically,
3Dhumanshapeandposeestimation. GraphCMR [7] trains Graph Neural Networks (GNNs) for
‚Ä¢ Based on the two key designs above, we further de- 3Dhumanshapeandposeestimation,whichdirectlylearns
velopmulti-scaleattentionandjoint-awareattention thevertexlocationonhumanmeshandispronetooutliers
moduleswhichsignificantlyimprovethereconstruc- andlargeerrorsunderchallengingposesandclutteredback-
tionresults. grounds.Incontrast,someothermethods,suchasSPIN[6]
‚Ä¢ Extensive experiments demonstrate that SMPLer and RSC-Net [72], circumvent this issue by training deep
performs favorably against the baseline methods CNNstoestimatetheunderlyingSMPLparameters,which
with better efficiency. In particular, compared to the havedemonstratedrobustperformanceforin-the-wilddata,
state-of-the-art method [1], the proposed algorithm inspiring numerous new applications [8], [10], [11], [19],
lowers the MPJPE error by more than 10% on Hu- [78].
man3.6M [34] with fewer than one-third of its pa- Nevertheless, recent research in the machine learning
rameters, showing the effectiveness of the proposed community [29], [79] reveals shortcomings of CNNs that
algorithm. theyhavedifficultiesinmodelinglong-rangedependencies,
which limits their capabilities for higher-quality represen-
tation learning. To overcome the above issue, METRO [2]
2 RELATED WORK and Mesh Graphormer [1] introduce Transformers to this
task, significantly improving the reconstruction accuracy.
We first review the previous methods for 3D human pose However, these Transformer-based methods generally fol-
and shape estimation and then discuss recent advances in low the architecture of ViT [29] without considering the
visionTransformers. characteristics of 3D human shape and pose, and adoptIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 3
a straightforward vertex representation, which hinders the work [27], we also use layer normalization [85] and MLP
network from exploiting high-resolution features for better in the attention operation, which are omitted in Eq. 1 for
performance.Incontrast,weproposeadecoupledattention brevity.
formulation that is more suitable for accurate 3D human Essentially, Eq. 1 models dependencies between pairs
reconstruction with a large number of feature tokens. To of tokens from Q and K with dot product. The output
better realize the concept of attention decoupling, we de- h(Q,K,V) ‚àà RlQ√ód can be seen as a new query embed-
velop a new Transformer framework that can effectively ding enhanced by aggregating information from V, and
exploit multi-scale (high and low) and multi-scope (global the aggregation weights are decided by the dependencies
and local) information for better results. Moreover, we between Q and K. Noticeably, when query, key and value
show that the parametric SMPL model can be combined arethesame,Eq.1iscalledself-attentionwhichwedenote
with Transformers by introducing an SMPL-based target as h (Q) = h(Q,Q,Q). When only key and value are the
self
representation, which naturally addresses issues of vertex samewhilequeryisdifferent,theoperationbecomescross-
regression. attention,denotedash (Q,K)=h(Q,K,K).
cross
2.2 Transformers 3.1.1 FullAttention
With its remarkable capability of representation learning, Existing Transformers for 3D human reconstruction [1], [2]
the Transformer has become a dominant architecture in basicallyfollowaViT-stylestructure[29],whichadoptsthe
natural language processing[27], [28]. Recently, it hasbeen full attention formulation as shown in Figure 1(a). Mathe-
introducedincomputervision,andtheapplicationsinclude matically,thefullattentioncanbewrittenas:
objectdetection[80],imageclassification[29],imagerestora-
tion[81],videoprocessing[82],andgenerativemodels[83], h (T ‚à•F), (2)
self
[84]. Most of these works employ an attention operation
that has quadratic complexity regarding the feature length where the self-attention h self(Q) is used, and the Q is a
and thus is not suitable for our goal of exploiting higher- concatenation of the target embedding T and the image
resolution features for more accurate 3D human shape and feature F along the token dimension, denoted by T ‚à•F ‚àà
poseestimation. R(lT+lF)√ód. The target embedding represents the variable
In this work, we demonstrate the effectiveness of a of interest, which corresponds to the class token in ViT
decoupled attention mechanism that has linear complexity and the 3D human representation in this work (see more
with respect to the feature length. We also introduce a explanationsinSection3.2).
compacttargetrepresentationbasedonaparametrichuman As the image feature F in Eq. 2 is involved in both
model. Furthermore, we provide new insights in design- query and key1, the full attention leads to a quadratic
ing multi-scale and joint-aware attention operations for 3D computation and memory cost with respect to the length
human reconstruction, and the proposed network achieves of the image feature l F, i.e. O((l F +l T)2),. Consequently,
better performance than the state-of-the-art Transformers previous works [1], [2] only use low-resolution features in
withimprovedefficiency. the Transformer where l F is small. In particular, suppose
different resolution features from a CNN backbone are
3 METHODOLOGY
denoted as F = {F 1,¬∑¬∑¬∑ ,F S} (see Figure 2) where S is
the number of scales2, Mesh Graphormer [1] only uses F S
In this work, we propose a new SMPL-based Transformer in the attention operation, and straightforwardly including
framework (SMPLer) for 3D human shape and pose esti- higher-resolution features in Eq. 2, e.g., F 1, would be com-
mation, which can exploit a large number of image feature putationallyprohibitive.
tokens based on an efficient decoupled attention and a
compact target representation. An overview is shown in
3.1.2 DecoupledAttention
Figure2.
Wenoticethattheattentionoperationservesdifferentroles
in ViT [29] and the Transformers for 3D human shape and
3.1 EfficientAttentionFormulation
pose estimation [1], [2]: the original ViT relies solely on
The attention operation [27] is the central component of
attention to learn expressive image features, while the 3D
Transformers,whichcanbeformulatedas:
humanTransformerusesadeepCNNforfeatureextraction
(cid:32) (cid:33)
(QW )(KW )‚ä§ (Figure 2), and the attention is mainly used to aggregate
h(Q,K,V)=f soft q‚àö k (VW v), (1) the image features for improving the target embeddings.
d
Therefore, modeling feature-feature dependencies is less
wheref softisthesoftmaxfunctionalongtherowdimension. important here as it does not have a direct effect on the
Q ‚àà RlQ√ód, and K,V ‚àà RlK√ód are the input of this oper- target, implying that it is possible to avoid the quadratic
ation,representingquery,key,andvalue,respectively.l Q is complexitybypruningthefullattention.
thelengthofthequeryQ,andl K isthelengthofKandV.d Motivatedbythisobservation,weproposeadecoupled
correspondstothechanneldimension.W q,W k,W v ‚ààRd√ód attention that bypasses the feature-feature computations as
represent learnable linear projection weights. We use a shown in Figure 1(b). It is composed of a target-feature
multi-headattention[27]inourimplementation,whereeach
head follows the formulation in Eq. 1, and different heads 1.ThiscanbeseenbyreplacingQ,K,V inEq.1withT ‚à•F.
employ different linear projection weights. Similar to prior 2.Theresolutiondecreasesfromscale1toS.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 4
CNN Transformer
Features
Input Output Reconstructed mesh
Target
Fig.2.Overviewoftheproposedframework.Givenamonocularinputimage,wefirstuseaCNNbackbone[31]toextractimagefeaturesF,which
are fed into the Transformer to reconstruct the 3D human body. The main ingredients of this framework are 1) an efficient decoupled attention
moduleintheTransformer(Section3.1),and2)acompacttargetrepresentationT basedonparametrichumanmodel(Section3.2).Moredetailed
descriptionsoftheTransformerarchitectureareprovidedinFigure3.
Refined output
MatMul/Add
Fusion
A
Reconstructed mesh
Residual
Transformer Block Linear
Linear
Transformer
Unit Fusion
Current output
Fusion Linear
Transformer
PE
Unit
Transformer Block PE Positional encoding
Linear PE A
-Reg
Element-wise add
Features
Features Global Pooling + MLP
-Reg
2D joint regression
(a) Hierarchical Transformer (b) The -th Transformer Block
Fig.3.HierarchicalarchitectureofourTransformer.(a)showsanoverviewofthehierarchicalarchitecturewhichcorrespondstothe‚ÄúTransformer‚Äùin
Figure2.WiththeimagefeaturesF,weprogressivelyrefinetheinitialestimationP0withBTransformerBlocks(Eq.13).In(b),eachTransformer
BlockconsistsofU TransformerUnits,andeachUnitisformulatedash inEq.12.Themodule‚ÄúJ-Reg‚Äùrepresents2Djointregressionfromthe
final
3Destimationresults,correspondingtoEq.4-6.
cross-attention and a target-target self-attention, which can heavycomputationandmemorycostinattentionoperations
bewrittenas: even after mesh downsampling. To address this issue, we
devise a more compact target representation based on a
h (h (T,F)). (3)
self cross parametrichumanbodymodelSMPL.
Notably, Eq. 3 has a computation and memory cost of
O(l Fl T + l T2), which is linear with respect to the feature 3.2.1 ParametricHumanModel
lengthl F.
SMPL [33] is a flexible and expressive human body model
that has been widely used for 3D human shape and pose
3.2 CompactTargetRepresentation
modeling. It is parameterized by a set of pose parameters
While the attention decoupling strategy effectively relaxes Œ∏ ‚àà RH√ó3 and a compact shape vector Œ≤ ‚àà R1√ó10. The
the computation burden, a large l T may still hinder the body pose Œ∏ is defined by a skeleton rig with H = 24
utilization of high-resolution features in Eq. 3. Existing 3D joints including the body root. The i-th row of Œ∏ (denoted
human Transformers [1], [2] usually recover the 3D body by Œ∏ i) is the rotation of the i-th body part in the axis-
mesh by regressing the vertex locations Y ‚àà RN√ó3, which angle form whose skew symmetric matrix lies in the Lie
leadstoaredundanttargetrepresentationT ‚ààRN√ód,where algebra space so(3) [86]. The body shape is represented by
the i-th row of T is the embedding of the i-th vertex. alow-dimensionalspacelearnedwithprincipalcomponent
The length of this target embedding is often quite large analysis [87] from a training set of 3D human scans, and Œ≤
(N = 6890 by default for the SMPL mesh), resulting in isthecoefficientsoftheprincipalbasisvectors.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 5
WithŒ∏andŒ≤,wecanobtainthe3Dbodymesh:
Features
Y ‚ààRN√ó3 =f (Œ∏,Œ≤), (4)
SMPL
Average
wheref istheSMPLfunction[33]thatgivesthevertices
SMPL
Y onapre-definedtrianglemesh.The3DbodyjointsJ can
be obtained from the vertices via linear mapping using a Scale-1 Scale-2 Scale-S
pretrainedmatrixM‚ààRH√óN: ......
J ‚ààRH√ó3 =MY. (5)
Target
With the 3D human joints, we can further obtain the Fig. 4. Jointly exploiting multi-scale features in the attention operation
(seeEq.8formoreexplanations).
2D joints using weak perspective projection. Denoting the
camera parameters as C ‚àà R3 that represents the scaling
A
A
factor and the 2D principal-point translation in the projec- A Element-wise add
tionprocess,the2Djointscanbeobtainedby:
J ‚ààRH√ó2 =Œ† C(J), (6) Ave (r sa trg ie
d
ep =o 2o )ling
A
whereŒ† C istheweakperspectiveprojectionfunction[26]. A
Learnable parameters
Features Positional encoding Encoded features
3.2.2 SMPL-BasedTargetRepresentation
Fig. 5. Pooling-based multi-scale positional encoding. We learn the
Inspired by the compactness and expressiveness of SMPL, positionalencodingonlyforthehighest-resolutionfeature,andtheen-
wereplacetheoriginalvertex-basedrepresentationwithan codings for other scales are generated by average pooling, such that
similar spatial locations across different scales have similar positional
SMPL-based representation. Since the variables of interest
embeddings.
are{Œ∏ i}H i=1,Œ≤,C asintroducedinEq.4and6,wedesignthe
newtargetrepresentationasT ‚àà R(H+2)√ód,wherethefirst
used as the K of the cross-attention h (Q,K) in Eq. 3.
cross
H rows of T correspond to the H body part rotations ofŒ∏, Theresultingmulti-scaleattentioncanbewrittenas3:
andtheremainingtworowsdescribethebodyshapeŒ≤ and
cameraparameterC. hÀú ms(T,F)=h cross(T,F 1‚à•F 2‚à•¬∑¬∑¬∑‚à•F S). (7)
This new target representation conveys several advan-
However,thisstrategyusesthesameprojectionweightsfor
tages.First,thelengthofthisrepresentationismuchshorter
different scale features and thereby models target-feature
thanthevertex-basedone(H+2‚â™N),therebyfacilitating
dependencies in the same subspace [27], which ignores the
the efficient design of our model as shown in Figure 1(c).
characteristicsofdifferentscalesandislessflexibleforfully
Second, our target representation is able to restrict the
exploitingtheabundantmulti-scaleinformation.
solution to the SMPL body space, which naturally ensures
To address this issue, we introduce an improved multi-
smoothmeshes, whereasthe vertex-basedrepresentationis
scalestrategythatcanbewrittenas
prone to outliers and may lead to spiky body surfaces as
shown in Figure 8. Third, the SMPL-based representation 1 (cid:88)S
explicitly models the 3D rotation of body parts and thus h ms(T,F)= S h cross(T,F i), (8)
is more readily usable in many applications, e.g., driving i=1
a virtual avatar. In contrast, the vertices cannot be directly whereweemploydifferentprojectionweightsforeachscale,
used and have to be converted into rotations first (often in and the output is an average of all scales as illustrated
an iterative optimization manner), which is sub-optimal in in Figure 4. Whereas conceptually simple, Eq. 8 effectively
terms of both efficiency and accuracy (see more details in incorporates multi-scale image features into the attention
Section4.2). computation,whichdifferssharplyfromexistingworks[1],
[2]thatonlyrelyonsingle-scalelow-resolutionfeatures.
3.3 Multi-ScaleAttention
3.3.2 Multi-ScaleFeaturePositionalEncoding
The above strategies, i.e. attention decoupling and SMPL- AstheattentionoperationitselfinEq.1ispositionagnostic,
based target representation, enable us to explore different positional encoding is often used in Transformers to inject
resolution features in the Transformer framework, which the position information of the input. Similar to ViT [29],
inspires a multi-scale attention design for high-quality 3D we use the learnable positional encoding for thetarget and
humanshapeandposeestimation. feature, which generally takes the form of x+œï, where œï
is a set of learnable parameters representing the position
3.3.1 CombiningMulti-ScaleFeatures informationofatokenx.
In particular, the positional encoding of image features
Our insight is that different resolution features are comple-
mentary to each other and should be collaboratively used can be written as œï i ‚àà RlFi√ód (i = 1,2,¬∑¬∑¬∑ ,S), which is
for3Dhumanshapeandposeestimation.Astraightforward
directlyaddedtofeatureF i.Straightforwardly,wecanlearn
way to combine those features is to take each scale as a
3.Wefocusonimprovingthecross-attentionpartofthedecoupled
subsetoftokensandconcatenateallthetokensintoasingle
attention(Eq.3).Theself-attentionpartiskeptunchangedandomitted
feature embedding. Then the concatenated feature can be inthefollowingsectionsforbrevity.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 6
:
Features Self-attention
Average
Joints Joint-aware attention Multi-scale attention
Fig. 7. Combining the joint-aware and multi-scale attention. Note that
Fig. 6. Illustration of the joint-aware attention that aggregates local
onlythefirstHrowsofT areaveragedinthe‚ÄúAverage‚Äùoperation(see
featuresaroundhumanjoints.SeemoredetailsinSec.3.4.
Eq.11formoredetails).
positionalencodingœï i forallscales,whichnotonlyresults joint-aware attention on lower-resolution features becomes
in excessive parameters, but also ignores the relationship similar to the global attention in Eq. 8. Similar to the Swin
between the locations across different scales, e.g., similar Transformer[79],weincorporatearelativepositionalencod-
spatial locations at different scales should have similar ing Œ∑ ‚àà R1√ór2 in the softmax function, which is bilinearly
positional embeddings. To address this issue, we adjust
sampledfromalearnabletensorŒ∑Àú‚ààR(r+1)√ó(r+1)according
our strategy to only learn the positional embedding for to the distance between J i and pixels in N(J i). The local
the highest scale, i.e., œï 1, and the embeddings for the other attention module has a computation and memory cost of
scalesareproducedbyaggregatingœï 1: O(Hr2),whichisalmostnegligiblecomparedtothenormal
attentionthatattendstotheimagefeaturesglobally.
œï i =f p( o2 oi‚àí l 1)(œï 1), (9) Eventually,wecombinethemulti-scaleattention(Eq.8)
and the joint-aware attention (Eq. 10) by simply taking
where
f(2i‚àí1)
is the average pooling with a stride and the average as shown in Figure 7. Denoting the combined
pool
windowsizeof2i‚àí1.Inrealimplementation,weiteratively attention as h (T,F) ‚àà R(H+2)√ód, the i-th row of the
co
applyastride-2poolinglayertoœï 1 (seeFigure5): outputcanbewrittenas:
œï =f(2)(œï ),i=2,¬∑¬∑¬∑ ,S, Ô£±1
i pool i‚àí1
h co(T
i,F)=Ô£≤ 2(h ja(T i,F)+h ms(T i,F)), i‚â§H
. (11)
which is equivalent to Eq. 9 but requires slightly fewer Ô£≥h (T ,F), i>H
ms i
computations.
Note that h ja(T i,F) is only defined for the H body parts,
i.e., i = 1,¬∑¬∑¬∑ ,H, and thereby we directly use the multi-
3.4 Joint-AwareAttention
scaleattentionforthebodyshapeŒ≤ andcameraC without
In addition to the multi-scale approach, the SMPL-based averaging,whichcorrespondtoi=H +1,H+2inEq.11.
targetrepresentationT alsomotivatesthedesignofajoint-
Following the attention decoupling in Eq. 3, the final
aware attention. Recall that the first H rows of T describe
formulationofourattentionmodulecanbewrittenas
the relative rotations of the H body parts (Section 3.2.2).
As shown in Figure 6, the local articulation status around h (T,F)=h (h (T,F)), (12)
final self co
humanjointsstronglyimpliestherelativerotationbetween
neighboring body parts. Thus, it can be beneficial to ade- whichisbrieflyillustratedinFigure7.
quately focus on the local image features around joints in
theattentionoperationtoimprovethefirstH rowsofT for
3.5 HierarchicalArchitecture
betterestimationofhumanpose.
AsshowninEq.10,animportantissueofourcurrentdesign
Tothisend,wedeviseajoint-awareattentionoperation,
which modifies the cross attention h (Q,K) by restrict- is that the joint-aware attention relies on the 2D joints J,
cross
ing K to a local neighborhood of the human joints. This which is supposed to be an output of our algorithm (see
Eq. 6). In other words, we need J to reconstruct the 3D
operationcanbewrittenas:
human and meanwhile need the 3D human to regress J,
(cid:32) (T W )(FN(Ji)W )‚ä§ (cid:33) which essentially leads to a chicken-and-egg problem. To
h (T ,F)=f i q ‚àö1 k +Œ∑ (FN(Ji)W ),
ja i soft d 1 v circumvent this problem, we propose a hierarchical archi-
tecture(Figure3)toiterativelyrefinethe2Djointestimation
(10)
and3Dreconstructionresults.
whereT i isthei-throwofT,i = 1,¬∑¬∑¬∑ ,H.N(J i)denotes We denote the output of the b-th stage in Figure 3(a)
an image patch around the i-th joint J i with size r√ór as as Pb = {Rb ,¬∑¬∑¬∑ ,Rb ,Œ≤b,Cb}, where Rb ‚àà SO(3) is
showninFigure6,andFN(Ji) ‚ààRr2√ód representsthelocal the rotation mŒ∏ a1 trix of tŒ∏ hH e i-th body part, corŒ∏ ri esponding to
1
features sampled at N(J i) from F 1. Eq. 10 has a similar Œ∏ i (the i-th row of Œ∏). Then the refinement process can be
form to Eq. 1, where T i and F 1N(Ji) serve as the Q and writtenas:
K, respectively. It is noted that we only use the highest- Tb =fb (Tb‚àí1,Pb‚àí1,F),
resolution feature F 1 for the local attention here, as N(J i) Pb =fTB (Tb,Pb‚àí1), b=1,2,¬∑¬∑¬∑ ,B, (13)
cancoveralargeareaonsmallerfeaturemapssuchthatthe fusionIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 7
TABLE1
Quantitativecomparisonagainstthestate-of-the-artmethodsonHuman3.6Mand3DPWdatasets.MPVErepresentsmeanper-vertexerror.
Numbersinboldindicatethebestineachcolumn.
Human3.6M 3DPW
Method Parameters(M)
MPJPE‚Üì PA-MPJPE‚Üì MPVE‚Üì MPJPE‚Üì PA-MPJPE‚Üì
HMR[4] ‚àí 88.0 56.8 ‚àí ‚àí 81.3
GraphCMR[7] 40.7 ‚àí 50.1 ‚àí ‚àí 70.2
SPIN[6] ‚àí ‚àí 41.1 116.4 ‚àí 59.2
RSC-Net[72] 28.0 67.2 45.7 112.1 96.6 59.1
FrankMocap[74] ‚àí ‚àí ‚àí ‚àí 94.3 60.0
VIBE[5] 42.7 65.6 41.4 99.1 82.9 51.9
Pose2Mesh[46] 72.8 64.9 47.0 ‚àí 89.2 58.9
I2LMeshNet[48] 135.7 55.7 41.1 ‚àí 93.2 57.7
PARE[62] ‚àí ‚àí ‚àí 88.6 74.5 46.5
METRO[2] 231.8 54.0 36.7 88.2 77.1 47.9
MeshGraphormer[1] 215.7 51.2 34.5 87.7 74.7 45.6
SMPLer 35.6 47.0 32.8 84.7 75.7 45.2
SMPLer-L 70.2 45.2 32.4 82.0 73.7 43.4
where fb is the b-th Transformer Block, which improves Inaddition,weincludearotationregularizationterm:
TB
the target embedding Tb‚àí1 with image features F and the
c gu enrr ee rn at te3 sD thees rt eim fina eti don estP imb‚àí a1 ti. of nfu Psio bn bi as sea df ou nsio thn el ia my per rot vh ea dt ‚Ñì rotation =w R¬∑ H1 (cid:88)H ‚à•R Œ∏i ‚àíRÀÜ Œ∏i‚à• 1, (14)
target embedding Tb. As illustrated on the rightmost of i=1
Figure 3, in the fusion layer we first use a linear layer to where we encourage the predicted rotation R Œ∏i to be close
mapTb intoasetofresiduals: to the ground-truth rotation matrix RÀÜ Œ∏i. w R is the weight
of the loss. Eventually, our training loss is a combination
‚àÜPb ={‚àÜRb ,¬∑¬∑¬∑ ,‚àÜRb ,‚àÜŒ≤b,‚àÜCb},
Œ∏1 Œ∏H of ‚Ñì
basic
and ‚Ñì rotation. We do not restrict the body shape Œ≤ as
and then add the residuals to the current estimation Pb‚àí1. weempiricallyfindnobenefitsfromit.Notethat‚Ñì rotationcan
Similarto[88],weapplytheGram‚ÄìSchmidtorthogonaliza- onlybeusedinourTransformerwhichusesanSMPL-based
tion to the rotation residuals such that ‚àÜRb ‚àà SO(3). We targetrepresentation;itisincontrasttoexisting3Dhuman
use matrix multiplication (MatMul in FigurŒ∏ ei 3) instead of reconstruction Transformers that do not directly consider
additiontoadjusttherotationparameters. rotations.
As shown in Figure 3(a), the hierarchical architecture is
composed of B Transformer Blocks, and each Transformer
3.7 Discussions
Block is composed of U Transformer Units in Figure 3(b).
TheTransformerUnitrepresentstheaforementioneddecou- Decoupled attention. While a similar idea to our atten-
pledattentionh illustratedinFigure7.Tobootstrapthis tion decoupling has been touched upon in [90] for video
final
refinement process, we apply a global pooling layer and classification, our work marks the pioneering exploration
an MLP to the CNN features to obtain an initial coarse of it in monocular 3D human shape and pose estimation,
estimation P0 similar to HMR [4]. For the initial target addressing the intrincs limitations of the full attention for-
embeddingT0,astraightforwardchoiceistousealearned mation of the existing works. While we present the idea in
feature embedding as in DETR [80]. Nevertheless, we em- a simplified manner in Figure 1 for clarity, the innovation
pirically find this choice makes the training less stable. of our decoupled attention goes beyond a basic conceptual
Instead, we employ a heuristic strategy by combining the introduction. Unlike the approach in [90] that relies solely
globally-pooledimagefeatureandlinearly-transformedP0, onsingle-scalelow-resolutionfeatures,weproposeamulti-
i.e.,T0 =f global(F S)+f linear(P0),wheref
global
istheglobal scale decoupled attention framework (Section 3.3). This
averagepoolingfunction. unique design allows the model to exploit both coarse and
granular visual information, substantially improving the
performance of 3D human shape and pose estimation. No-
3.6 LossFunction
tably, this multi-scale approach is enabled by the enhanced
For training the proposed Transformer, we adopt the loss efficiencyofourattentiondecouplingstrategy,whichallows
functionsofMeshGraphormer[1]whichrestricttherecon- incorporatinghigher-resolutionfeatureswithoutprohibitive
structedhumanintermsofverticesandbodyjoints: computationalcosts.
‚Ñì =w ¬∑‚à•Y ‚àíYÀÜ‚à• +w ¬∑‚à•J ‚àíJÀÜ‚à•2+w ¬∑‚à•J ‚àíJÀÜ‚à•2, In addition, we emphasize that effectively combining
basic Y 1 J 2 J 2
multi-scale features is not a straightforward endeavor,
where Y,J,J are the predicted vertices, 3D joints, and 2D which requires dedicated algorithmic design and meticu-
jointsthatarecomputedwithEq.4-6usingthefinaloutput lousengineeringefforts.Particularly,insteadofdirectlycon-
of our Transformer, i.e., PB in Figure 3(a). YÀÜ,JÀÜ,JÀÜ indicate catenating all scale features, our approach assigns different
the corresponding ground truth. w Y,w J,w J are hyper- projectionweightsforeachscaletomodeltarget-featurede-
parametersforbalancingdifferentterms.Following[1],we pendencyinascale-awaremanner(Section3.3.1).Moreover,
useL 1 lossforverticesandMSElossforhumanjoints. we devise a pooling-based multi-scale positional encodingIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 8
Input METRO Mesh Graphormer SMPLer SMPLer-L
Fig.8.VisualcomparisonsagainsttheSOTAmethods.ThetoptworowsarefromtheHuman3.6Mdataset[34],andthebottomtworowsarefrom
the3DPWdataset[89].
tobetterrepresentspatialinformationacrossvaryingscales, for the Transformer and set the feature sampling range of
asdetailedinSection3.3.2. the joint-aware attention as r = 8. It corresponds to a
SMPL-based target representation. While SMPL has been
32√ó32regionintheinputimage,whichisadequatelylarge
to encompass the vicinity of the joints. We set the number
usedasoutputinpriormethods[4],[41],[60],[71],ourwork
of feature scales S = 4. We use HRNet [31] as the CNN
marksthefirsttimethattheSMPL-basedtargetrepresenta-
backbone. We present two variants of the proposed Trans-
tionisusedinaTransformerframework.
former: a base model SMPLer and a larger one SMPLer-L.
We emphasize the contribution of our SMPL-based tar-
The two models use the same architecture, and the only
getrepresentationisrootedinitsdistinctadvantages.First,
differenceisthechanneldimensionofthebackbone,where
itconsiderablyreducesthecomputationalcostasillustrated
we increase the channels by half for SMPLer-L. We use
in Figure 1. Second, it ensures consistent, smooth SMPL
a batch size of 200 and train the model for 160 epochs.
mesh, avoiding the spiky outliers on human surfaces pro-
The loss weights in ‚Ñì and ‚Ñì are empirically set as
duced by vertex-based representations (Figure 8). Third, it basic rotation
explicitlymodelsbodypartrotations,facilitatingitsapplica-
w
Y
=100,w
J
=1000,w
J
=100,w
R
=50.
tionsindrivingvirtualavatars(Figure11).Furthermore,the
SPML-based target representation allows the development Dataset. Similar to [1], [2], we extensively train our
of the joint-aware attention and motivates the hierarchical modelbycombiningseveralhumandatasets,includingHu-
architecture of our Transformer, both of which are novel man3.6M [34], MuCo-3DHP [92], UP-3D [37], COCO [93],
designs absent from previous works [4], [41], [60], [71] MPII [94]. Similar to [1], [2], we use the pseudo 3D mesh
and the concurrent work [91], significantly improving the trainingdatafrom[46],[48]forHuman3.6M.Wefollowthe
reconstructionresults. general setting where subjects S1, S5, S6, S7, and S8 are
used for training, and subjects S9 and S11 for testing. We
present all results using the P2 protocol [4], [6]. We fine-
4 EXPERIMENTS
tunethemodelswiththe3DPW[89]trainingdatafor3DPW
4.1 ImplementationDetails experiments.
Forthenetworkstructure,wesetthenumberofTransformer
Blocks as B = 3 and the number of Transformer Units in Metrics. We mainly use mean per-joint position error
eachblockasU =2bydefault.Weusefourattentionheads (MPJPE) and Procrustes-aligned mean per-joint positionIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 9
Input Output Alternate views Input Output Alternate views
Fig.9.QualitativeresultsofSMPLer.Foreachexample,thefirstcolumnshowstheinputimage,thesecondcolumnshowstheoutputincamera
view,andtheremainingcolumnsshowthepredictedmeshfromalternateviewpoints.
error(PA-MPJPE)forevaluation.MPJPEisdefinedas:
Projected
H1 (cid:88)H
‚à•J i‚àíJÀÜ i‚à• 2, (15) Vertex-based
i=1
where J i is the i-th row of J, i.e., the coordinate of the i-
th human joint, and
JÀÜ
represents the ground truth. Eq. 15
GT
directly measures the joint-to-joint error, which can be in- Ours
fluencedbyglobalfactors,includingscaling,globalrotation
andtranslation.PA-MPJPEisdefinedas:
Fig.10.IllustratingthemanifoldofthedesiredSMPLhumanmeshes.
sm ,Rin
,tH1 (cid:88)H
‚à•sJ iR+t‚àíJÀÜ i‚à• 2, (16)
T mh oe vep sr ao lp oo ns ge td hista mrg ae nt ifore ldp ,re ws he iln eta thti eon exg isu tia nr gan Tt re ae ns sfoo ru mr em rse mth ao yd pra olw dua cy es
i=1 resultsoffthedesiredspace,resultinginlessaccurateestimationand
s.t. s‚ààR,R‚ààSO(3),t‚ààR1√ó3, inconvenienceinmanyapplications,e.g.,controllingvirtualavatars.Asa
straightforwardremedyforavatarcontrol,onecanprojecttheundesired
resultstotheSMPLmanifoldwithiterativeoptimization,whichhowever
where s,R,t represent scaling, global rotation and transla- isoftentime-consumingandpronetoaccumulativeerrors.Notethatthe
tion, respectively. Instead of directly computing the joint- realSMPLmanifoldisembeddedina20670-dimensionalspace(6890
to-joint error, Eq. 16 first aligns the prediction J to the 3D vertices), and here we simply show the concept in 3D for ease of
groundtruthJÀÜ
withascaledrigidtransformation(aclosed-
visualization.
form solution for the alignment is given by Procrustes
analysis[95]).Thus,PA-MPJPEisabletoexcludetheglobal respectively, while using only 16.5% and 32.5% of its pa-
factors and focus on the intrinsic human body shape and rameters, clearly demonstrating the effectiveness of our
pose, emphasizing the measurement of relative position algorithm.
betweenadjacentbodyparts. Qualitative evaluation. For a more intuitive understand-
ing of the results, we also provide visual comparisons in
Figure 8. As a typical example, in the first row of Figure 8,
4.2 ComparisonwiththeStateoftheArts
existingapproachescannotwellposethehumanlegsasthey
Quantitative evaluation. We compare against the state- onlyuselow-resolutionfeaturesintheTransformerandthus
of-the-art 3D human shape and pose estimation methods, are prone to errors under self-occlusions and challenging
including CNN-based [4], [5], [6], [48], [62], [72], [74], poses. In contrast, the proposed SMPLer is able to better
GNN-based [7], [46]), and Transformer-based [1], [2]. As exploittheabundantimagefeaturesinamulti-scale(coarse
shown in Table 1, the proposed Transformer performs fa- andfine)andmulti-scope(globalandlocal)manner,which
vorably against the baseline approaches on both the Hu- effectivelyimprovestheestimationperformanceincomplex
man3.6M and 3DPW datasets. Notably, compared to Mesh scenarios. On the other hand, since existing Transformers
Graphormer [1], the proposed SMPLer and SMPLer-L low- mostly rely on a vertex-based target representation, their
ers the MPJPE error on Human3.6M by 8.2% and 11.7% results do not always lie on the SMPL mesh manifold asIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 10
illustrated in Figure 10. This leads to spiky outliers on the
human surface as shown in the second row of Figure 8
or even distorted meshes in the fourth row of Figure 8.
By contrast, our method introduces an SMPL-based target
representation, which naturally guarantees the solutions to
lieonthesmoothhumanmeshspace(Figure10)andthereby
achieveshigher-qualityresultsasshowninFigure8.
Inaddition,weshowmorequalitativeresultsofSMPLer Reference Virtual avatar Mesh Graphormer SMPLer
withalternativeviewsinFigure9,wheretheproposednet- Fig.11.ThankstotheSMPL-basedtargetrepresentation,theproposed
work performs robustly under diverse poses and complex methodcanbewellappliedtocontrolvirtualavatars,whiletheexisting
vertex-basedTransformersareerror-prone.Thevirtualcharacterisfrom
backgrounds.Inparticular,theresultsshowthatourmodel
Mixamo[96].
is able to accurately recover global rotation relative to the
camera coordinate frame, which is also validated by the
high-resolutionfeaturesforaccurate3Dhumanreconstruc-
improvementsoverMPJPEandMPVEmetrics.
tion.Atthecoreofthisframeworkareadecoupledattention
Controlling virtual avatars. An important application of
module and an SMPL-based target representation as intro-
3D human shape and pose estimation is to control virtual
duced in Section 3.1 and 3.2. To analyze the effectiveness
avatars,e.g.,inMetaverse.AsourSMPL-basedtargetrepre-
ofthesedesigns,westudydifferentchoicesoftheattention
sentation explicitly models the 3D rotations of body parts,
operation (full vs. decoupled) and the target representation
the proposed SMPLer can be easily used to drive virtual
(vertex-basedvs.SMPL-based)inTable2.
humans.AnexampleisshowninFigure11.Incontrast,the
As shown by Table 2(b), the straightforward approach
previousTransformerstakeverticesastheoutputwhichare
of using a full attention and a vertex-based representation
not directly applicable here and have to be converted into
leads to prohibitive memory and computation cost for ex-
rotations first for this task. More specifically, the predicted
ploiting multi-resolution image features F. With limited
vertices need to be fitted to the SMPL model (Eq. 4) in
computational resources, we cannot train this model with
an iterative optimization manner, which corresponds to
a proper batch size. While the decoupled attention can
projectingtheresultsontotheSMPLmanifoldinFigure10.
wellreducethemodelcomplexity(Table2(c)),thememory
Compared to the proposed solution which is essentially
footprint is still large due to the high dimensionality of
one-step, the two-step approach (vertices ‚Üí rotations) not
the vertex-based target representation. In contrast, the pro-
onlyleadstoinefficiencyissuesduetothetime-consuming
posed Transformer combines the decoupled attention and
fittingprocess,butalsosuffersfromlowaccuracycausedby
SMPL-basedrepresentation(Table2(d)),whichsignificantly
accumulativeerrors.
lessensthememoryandcomputationalburden,andthereby
To quantitatively evaluate the rotation accuracy, we in-
achieves more effective utilization of multi-scale features.
troduce a new metric, called mean per-body-part rotation
Note that the feature dimensionality is not a computa-
error(MPRE),whichisdefinedas:
tional bottleneck for our network anymore, as it only leads
MPRE=
180(cid:88)H arccos(trace(R Œ∏iRÀÜ Œ∏‚ä§ i)‚àí1
), (17)
t ro esoa lum tia or ngi fn ea al tuc ro em sp asut sa ht oio wn no bv yer Th ae ba ld e2f (o dr )e am ndpl (o ey ).ing high-
œÄH 2
i=1 Effectiveness of the multi-scale attention. As introduced
whereRRÀÜ‚ä§ representstherotationmatrixbetweenthepre- in Section 3.3, we propose an attention operation h ms for
dicted rotation R and the ground truth RÀÜ (this can be seen better exploiting multi-scale image information. As shown
from R = (RRÀÜ‚ä§)RÀÜ ). Essentially, Eq. 17 describes the rota- in Table 3, using single-scale feature for 3D human shape
tionangleœâ (indegree)betweenthepredictionandground
andposeestimation,eitherthelow-resolution(onlyscaleS)
or the high-resolution one (only scale 1), leads to inferior
truth,asthethreeeigenvaluesofarotationmatrixare1and
e¬±iœâ,andthustrace(RRÀÜ‚ä§)=1+eiœâ+e‚àíiœâ =1+2cos(œâ). resultscomparedtoourfullmodel.
On the other hand, unifying multi-scale features in
As the ground-truth rotations of Human3.6M are not
Transformer is not a trivial task. As introduced in Sec-
available, we evaluate the rotation accuracy on 3DPW. The
tion3.3,insteadofsimplyconcatenatingdifferentresolution
proposed SMPLer achieves an MPRE of 9.9, significantly
features into a single feature vector
(hÀú
in Eq. 7), we sep-
outperformingthe57.0ofMeshGraphormer,whichdemon- ms
aratelyprocesseachscalewithdifferentprojectionweights,
stratestheadvantageoftheproposedmethodincontrolling
i.e., h in Eq. 8. As shown in Table 3, the straightforward
virtual avatars. The visual comparison in Figure 11 further ms
concatenation method cannot fully exploit the multi-scale
verifiestheimprovement.
information due to the undesirable restriction of using the
sameprojectionweightsfordifferentscales,resultinginless
4.3 AnalysisandDiscussions
accurate 3D human estimation compared to the proposed
WeconductacomprehensiveablationstudyonHuman3.6M approach.
toinvestigatethecapabilityofourmethod.Wealsoprovide In addition, we apply multi-scale feature positional en-
more analysis and discussions about the model efficiency codingtosupplementpositioninformationfortheattention
as well as the attention mechanism. We use SMPLer as our operation. As shown in Table 4, the model without feature
defaultmodelinthefollowingsections. positionalencoding(w/oFPE)suffersfromamajorperfor-
Decoupled attention and SMPL-based representation.We mance drop, especially for MPJPE. Furthermore, instead of
proposeanewTransformerframeworkthatisabletoexploit directly learning the feature positional embedding for allIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 11
KL Divergence between Attention and Uniform Distribution
Concentrated
Input Block 1 Block 2 Block 3
Fig.12.OutputofBlock1,2,and3ofSMPLer.Thereconstructionresult
isprogressivelyrefinedinthehierarchicalarchitecture.
Diffused
High-resolution Low-resolution
Fig. 14. Attention distributions of different scales averaged on Hu-
man3.6M.WeusetheKLdivergencebetweentheattentionmapanda
(a) Input (b) Recovered mesh (c) Self-attention (d) Joint-aware attention uniformdistributiontoquantitativelymeasurethespatialdiffusenessof
theattention.Ahigherdivergencefromtheuniformdistributionindicates
lowerdiffusenessinspace;inotherwords,theattentioninthatscaleis
moreconcentratedatfine-grainedfeatures.
tend to be more concentrated on fine-grained features of
specificbodyparts.Inparticular,theproposedTransformer
(e) Multi-scale: Scale-1 (f) Multi-scale: Scale-2 (g) Multi-scale: Scale-3 (h) Multi-scale: Scale-4 relies on the foot and head poses to help infer body part
Fig. 13. Visualization of the attention learned by SMPLer. The query rotationaroundtheknee.Bycontrast,forlowerresolutions,
jointistherightknee(theyellowpointin(c)).Fortheself-attentionin(c), e.g., in Figure 13(h), the attention gets more diffused in a
brighter color indicates stronger interactions. For (d)-(h), redder color
wider space, indicating that the multi-scale attention can
indicateslargerattentionresponse.
exploit global information, including the background, to
scales (all-scale FPE), we propose a pooling-based strategy decidetheglobalrotationandscaleofthebody.
Eq. 9 as illustrated in Figure 5 to better handle the spatial To further analyze the attention of different scales, we
relation across scales. Compared to ‚Äúall-scale FPE‚Äù in Ta- alsoprovidequantitativeresultsinFigure14.WeusetheKL
ble 4, this strategy further improves the 3D human shape divergencebetweentheattentionmapandauniformdistri-
andposeestimationresults. butiontomeasurehowmuchtheattentiondistributiondif-
fusesinspace.AsshowninFigure14,thehigher-resolution
Effectiveness of the joint-aware attention. Motivated by
attentionmoduleshavehigherdivergencefromtheuniform
the SMPL-based target representation, we propose a joint-
aware attention h in Eq. 10 to better exploit local features distribution, indicating that their attention is less diffused
ja
andmoreconcentratedatfine-grainedfeatures.Thisverifies
around human joints. As shown in Table 5, the model
without h suffers from a significant performance drop, thecomplementarityoflow-andhigh-resolutionfeaturesas
ja
discussedinSection3.3.
showingtheimportanceofthismoduleininferringrelative
bodypartrotations.Further,weincludearelativepositional In addition, we provide a visualization of the learned
encodinginEq.10tomodelthespatialrelationbetweentar- joint-aware attention in Figure 13(d), which shows intrigu-
getembeddingandimagefeatures,whichslightlyimproves ing local patterns where the information around the target
theperformanceasshowninTable5(‚Äúw/oŒ∑‚Äù). human joint is emphasized for better 3D human recon-
struction. Specifically, it has higher weights on the upper
Analysis of the hierarchical architecture.Weapplyahier-
and lower sides of the knee, which captures the kinetic
archical architecture in Figure 3 that is composed of multi-
status around the joint and helps infer the relative rotation
ple Transformer Blocks and multiple Transformer Units to
betweenthethighandcalf.
progressively refine the 3D human estimation results. We
Lastly, we employ a self-attention layer h for atten-
visualize the refinement process in Figure 12, where the self
tion decoupling (Figure 7), which models the target-target
reconstructionbecomesmoreaccurateaftermoreBlocks.
dependencies in Figure 1(c). We visualize this layer by
Moreover, we investigate the effect of different settings
showing the interactions between one query joint and all
of the hierarchical architecture. As shown in Table 6, the
otherjoints,wheretheh focusesonneighboringjointson
performance gain of adding more Blocks and Units is sig- self
thekinetictree[33]toproducemorereasonablepredictions.
nificant at the start but converges quickly after B > 1 and
U >1.Therefore,werefrainfromaddingevenmoreblocks Running speed. As shown in Table 7, the inference of
and take B = 3,U = 2 as the default setting for a better SMPLerandSMPLer-Lrunsat96.0and73.9framespersec-
tradeoffbetweenperformanceandcomputationcost. ond (fps) respectively, which is effectively real-time. While
exploiting high-dimensional multi-resolution features, SM-
Attention visualization. For a more comprehensive study
PLer has a nearly three-times running speed and one-fifth
oftheproposedattentionmodules,wevisualizethelearned
GFlops of the baseline Transformers [1], [2] that are solely
attention maps in Figure 13. First, we show the multi-scale
attention h in Figure 13(e)-(h), where the different scales based on low-dimensional features, showing the efficiency
ms
oftheproposedalgorithm.
correspond to the cross attention modules in Figure 4. The
attention maps for higher resolutions, e.g., in Figure 13(e), Relationship with prior works. The proposed SMPLer isIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 12
TABLE2
Comparisonacrossdifferentchoicesoftheattentionoperationandthetargetrepresentation.TheGPUmemoryandGFlopsaremeasuredfora
batchsizeof1.DuetotheimmenseGPUmemoryrequirement,wearenotabletotrain(b)and(c)withaproperbatchsize.(d)correspondstoour
defaultsettingofSMPLer.
Attention Target Feature Memory(G) GFlops MPJPE PA-MPJPE
(a) Full Vertex-based FS 0.49 8.1 51.9 36.0
(b) Full Vertex-based {F1,¬∑¬∑¬∑,FS} 7.3 26.0 ‚àí ‚àí
(c) Decoupled Vertex-based {F1,¬∑¬∑¬∑,FS} 1.15 11.0 ‚àí ‚àí
(d) Decoupled SMPL-based {F1,¬∑¬∑¬∑,FS} 0.44 8.7 47.0 32.8
(e) Decoupled SMPL-based FS 0.37 7.8 48.3 34.0
TABLE3 TABLE5
Effectivenessofthemulti-scaleattention. Effectivenessofthejoint-awareattention.
Method MPJPE PA-MPJPE Method MPJPE PA-MPJPE
onlyscaleS 48.3 34.0 w/oh ja 51.4 34.5
onlyscale1 49.5 33.7 w/oŒ∑ 48.7 33.3
concatenation 48.5 33.9 fullmodel 47.0 32.8
fullmodel 47.0 32.8
TABLE6
TABLE4 EffectofdifferentnumberofTransformerBlocksandUnitsinthe
Effectivenessofthefeaturepositionalencoding(FPE). hierarchicalarchitecture.B=3,U =2isthedefaultsetting.
Method MPJPE PA-MPJPE Method MPJPE PA-MPJPE
w/oFPE 50.4 33.7 B=1,U =2 48.6 34.4
all-scaleFPE 49.0 33.6 B=2,U =2 47.4 33.1
fullmodel 47.0 32.8 B=3,U =2 47.0 32.8
B=3,U =1 48.5 33.9
B=3,U =3 47.1 32.8
based on two fundamental designs: the attention decou-
pling and the SMPL-based target representation. These de-
These two strategies also motivate the design of the multi-
signs substantially distinguish our algorithm from existing
scale attention and joint-aware attention modules, which
Transformers [1], [2], which are based on full attention and
canbepotentiallyextendedtootherareaswheremulti-scale
vertex-based representation and thereby can only use low-
and multi-scope information is valuable, such as motion
resolutionfeaturesintheattentionoperation.Moreover,the
estimation [97] and image restoration [98]. Meanwhile, the
proposed framework also clearly differs from the existing
proposed algorithm can also be applied to other 3D recon-
SMPL-based approaches, such as HMR [4], SPIN [6], and
struction problems, such as 3D animal reconstruction [99]
RSC-Net [72] which are solely based on CNNs and cannot
and 3D hand reconstruction [100] by replacing SMPL with
exploit the powerful learning capabilities of Transformers
otherparametricmodels,e.g.,SMAL[101]andMANO[102].
for 3D human shape and pose estimation. As a result, they
Nevertheless,thisisbeyondthescopeofthisworkandwill
sufferfromalargeperformancegapcomparedtothestate-
beaninterestingdirectionforfutureresearch.
of-the-artsasevidencedinTable1.
Similar to existing Transformers, the proposed SMPLer
Thetwobasicdesignsnaturallyleadtothedevelopment
is still a hybrid structure that consists of CNN layers in
ofseveralnovelmodules,includingthemulti-scaleattention
the backbone. To incorporate attention-based backbones in
and the joint-aware attention. While these modules are
SMPLer such as [79], [103] will be another direction worth
conceptually simple, to have them work properly in our
exploringinfuturework.
framework is by no means trivial and requires meticulous
algorithmic designs, such as the approach to fuse multi-
scale features, the pooling-based positional encoding, the REFERENCES
relative positional encoding in the joint-aware attention,
[1] K.Lin,L.Wang,andZ.Liu,‚ÄúMeshgraphormer,‚ÄùinICCV,2021.
andthehierarchicalarchitecture.Eventually,withtheabove 1,2,3,4,5,7,8,9,11,12
designs,SMPLerachievessignificantimprovementoverthe [2] ‚Äî‚Äî, ‚ÄúEnd-to-end human pose and mesh reconstruction with
state-of-the-artmethods[1],[2]withbetterefficiency. transformers,‚ÄùinCVPR,2021. 1,2,3,4,5,7,8,9,11,12
[3] F.Bogo,A.Kanazawa,C.Lassner,P.Gehler,J.Romero,andM.J.
Black, ‚ÄúKeep it smpl: Automatic estimation of 3d human pose
andshapefromasingleimage,‚ÄùinECCV,2016. 1,2
5 CONCLUSIONS [4] A.Kanazawa,M.J.Black,D.W.Jacobs,andJ.Malik,‚ÄúEnd-to-end
recoveryofhumanshapeandpose,‚ÄùinCVPR,2018. 1,7,8,9,12
We have developed a new Transformer framework for [5] M.Kocabas,N.Athanasiou,andM.J.Black,‚ÄúVibe:Videoinfer-
high-quality 3D human shape and pose estimation from enceforhumanbodyposeandshapeestimation,‚ÄùinCVPR,2020.
a single image. At the core of this work are the decou- 1,2,7,9
[6] N.Kolotouros,G.Pavlakos,M.J.Black,andK.Daniilidis,‚ÄúLearn-
pled attention and the SMPL-based target representation
ingtoreconstruct3Dhumanposeandshapeviamodel-fittingin
thatallowefficientutilizationofhigh-dimensionalfeatures. theloop,‚ÄùinICCV,2019. 1,2,7,8,9,12IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 13
TABLE7 [29] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,
Runningspeedofdifferentmethodsmeasuredonthesamecomputer T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly
withanIntelXeonE5-2620v3CPUandanNVIDIATeslaM40GPU. etal.,‚ÄúAnimageisworth16x16words:Transformersforimage
TheGFlopsarecalculatedforasingleinputimagewithsize224√ó224. recognitionatscale,‚ÄùinICLR,2021. 1,2,3,5
[30] B. Bosquet, M. Mucientes, and V. M. Brea, ‚ÄúStdnet: Exploiting
high resolution feature maps for small object detection,‚Äù Engi-
Method Speed(fps)‚Üë GFLops‚Üì neeringApplicationsofArtificialIntelligence,vol.91,p.103615,2020.
METRO 33.5 50.5 1
MeshGraphormer 34.6 45.4 [31] K. Sun, B. Xiao, D. Liu, and J. Wang, ‚ÄúDeep high-resolution
SMPLer 96.0 8.7 representation learning for human pose estimation,‚Äù in CVPR,
SMPLer-L 73.9 16.5 2019. 1,4,8
[32] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional net-
worksforsemanticsegmentation,‚ÄùinCVPR,2015. 1
[33] M.Loper,N.Mahmood,J.Romero,G.Pons-Moll,andM.J.Black,
[7] N. Kolotouros, G. Pavlakos, and K. Daniilidis, ‚ÄúConvolutional ‚ÄúSmpl:Askinnedmulti-personlinearmodel,‚ÄùACMTransactions
meshregressionforsingle-imagehumanshapereconstruction,‚Äù onGraphics(SIGGRAPHAsia),vol.34,no.6,p.248,2015. 1,4,5,
inCVPR,2019. 1,2,7,9 11
[8] J.Rajasegaran,G.Pavlakos,A.Kanazawa,andJ.Malik,‚ÄúTracking [34] C.Ionescu,D.Papava,V.Olaru,andC.Sminchisescu,‚ÄúHuman3.
peoplewith3drepresentations,‚ÄùinNeurIPS,2021. 1,2 6m:Largescaledatasetsandpredictivemethodsfor3dhuman
[9] ‚Äî‚Äî,‚ÄúTrackingpeoplebypredicting3dappearance,locationand sensing in natural environments,‚Äù IEEE Transactions on Pattern
pose,‚ÄùinCVPR,2022. 1 Analysis and Machine Intelligence, vol. 36, no. 7, pp. 1325‚Äì1339,
[10] J.Wang,Y.Zhong,Y.Li,C.Zhang,andY.Wei,‚ÄúRe-identification 2013. 2,8
supervisedtexturegeneration,‚ÄùinCVPR,2019. 1,2 [35] H.-Y. Tung, H.-W. Tung, E. Yumer, and K. Fragkiadaki, ‚ÄúSelf-
[11] X.XuandC.C.Loy,‚Äú3dhumantextureestimationfromasingle supervisedlearningofmotioncapture,‚ÄùinNeurIPS,2017. 2
imagewithtransformers,‚ÄùinICCV,2021. 1,2
[36] G. Pavlakos, L. Zhu, X. Zhou, and K. Daniilidis, ‚ÄúLearning to
[12] Z.Zheng,T.Yu,Y.Liu,andQ.Dai,‚ÄúPaMIR:Parametricmodel-
estimate3dhumanposeandshapefromasinglecolorimage,‚Äù
conditioned implicit representation for image-based human re- inCVPR,2018. 2
construction,‚ÄùIEEETransactionsonPatternAnalysisandMachine
[37] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, and P. V.
Intelligence,2021. 1
Gehler,‚ÄúUnitethepeople:Closingtheloopbetween3dand2d
[13] A. Mir, T. Alldieck, and G. Pons-Moll, ‚ÄúLearning to transfer
humanrepresentations,‚ÄùinCVPR,2017. 2,8
texturefromclothingimagesto3Dhumans,‚ÄùinCVPR,2020. 1
[38] M. Omran, C. Lassner, G. Pons-Moll, P. Gehler, and B. Schiele,
[14] Q.Ma,J.Yang,S.Tang,andM.J.Black,‚ÄúThepowerofpointsfor
‚ÄúNeural body fitting: Unifying deep learning and model based
modelinghumansinclothing,‚ÄùinICCV,2021. 1
humanposeandshapeestimation,‚Äùin3DV,2018. 2
[15] T.Alldieck,M.Magnor,B.L.Bhatnagar,C.Theobalt,andG.Pons-
[39] R. A. Guler and I. Kokkinos, ‚ÄúHolopose: Holistic 3d human
Moll,‚ÄúLearningtoreconstructpeopleinclothingfromasingle
reconstructionin-the-wild,‚ÄùinCVPR,2019. 2
rgbcamera,‚ÄùinCVPR,2019. 1
[16] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, [40] Y.Xu,S.-C.Zhu,andT.Tung,‚ÄúDenserac:Joint3dposeandshape
‚ÄúVideo based reconstruction of 3d people models,‚Äù in CVPR, estimationbydenserender-and-compare,‚ÄùinICCV,2019. 2
2018. 1 [41] W.Jiang,N.Kolotouros,G.Pavlakos,X.Zhou,andK.Daniilidis,
[17] T. Alldieck, G. Pons-Moll, C. Theobalt, and M. Magnor, ‚ÄúCoherentreconstructionofmultiplehumansfromasingleim-
‚ÄúTex2shape: Detailed full human body geometry from a single age,‚ÄùinCVPR,2020. 2,8
image,‚ÄùinICCV,2019. 1 [42] T.Zhang,B.Huang,andY.Wang,‚ÄúObject-occludedhumanshape
[18] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, andposeestimationfromasinglecolorimage,‚ÄùinCVPR,2020.2
‚ÄúDetailedhumanavatarsfrommonocularvideo,‚Äùin3DV,2018. [43] W.Zeng,W.Ouyang,P.Luo,W.Liu,andX.Wang,‚Äú3dhuman
1 meshregressionwithdensecorrespondence,‚ÄùinCVPR,2020. 2
[19] R.Li,S.Yang,D.A.Ross,andA.Kanazawa,‚ÄúAichoreographer: [44] A.Zanfir,E.G.Bazavan,H.Xu,W.T.Freeman,R.Sukthankar,
Music conditioned 3d dance generation with aist++,‚Äù in ICCV, and C. Sminchisescu, ‚ÄúWeakly supervised 3d human pose and
2021. 1,2 shapereconstructionwithnormalizingflows,‚ÄùinECCV,2020. 2
[20] F. Hong, M. Zhang, L. Pan, Z. Cai, L. Yang, and Z. Liu, [45] J.Song,X.Chen,andO.Hilliges,‚ÄúHumanbodymodelfittingby
‚ÄúAvatarclip: Zero-shot text-driven generation and animation of learnedgradientdescent,‚ÄùinECCV,2020. 2
3davatars,‚ÄùACMTransactionsonGraphics(SIGGRAPH),vol.41, [46] H.Choi,G.Moon,andK.M.Lee,‚ÄúPose2mesh:Graphconvolu-
no.4,pp.1‚Äì19,2022. 1 tionalnetworkfor3dhumanposeandmeshrecoveryfroma2d
[21] S.Sanyal,A.Vorobiov,T.Bolkart,M.Loper,B.Mohler,L.S.Davis, humanpose,‚ÄùinECCV,2020. 2,7,8,9
J.Romero,andM.J.Black,‚ÄúLearningrealistichumanreposing
[47] G.Georgakis,R.Li,S.Karanam,T.Chen,J.KosÀáecka¬¥,andZ.Wu,
usingcyclicself-supervisionwith3dshape,pose,andappearance
‚ÄúHierarchicalkinematichumanmeshrecovery,‚ÄùinECCV,2020.
consistency,‚ÄùinICCV,2021. 1
2
[22] A. Grigorev, K. Iskakov, A. Ianina, R. Bashirov, I. Zakharkin,
[48] G.MoonandK.M.Lee,‚ÄúI2l-meshnet:Image-to-lixelprediction
A.Vakhitov,andV.Lempitsky,‚ÄúStylepeople:Agenerativemodel
networkforaccurate3dhumanposeandmeshestimationfrom
offullbodyhumanavatars,‚ÄùinCVPR,2021. 1
asinglergbimage,‚ÄùinECCV,2020. 2,7,8,9
[23] S.Peng,Y.Zhang,Y.Xu,Q.Wang,Q.Shuai,H.Bao,andX.Zhou,
[49] H.Zhang,J.Cao,G.Lu,W.Ouyang,andZ.Sun,‚ÄúLearning3d
‚ÄúNeural body: Implicit neural representations with structured
humanshapeandposefromdensebodyparts,‚ÄùIEEETransactions
latent codes for novel view synthesis of dynamic humans,‚Äù in
onPatternAnalysisandMachineIntelligence,2020. 2
CVPR,2021. 1
[50] G. Moon, H. Choi, and K. M. Lee, ‚ÄúAccurate 3d hand pose
[24] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, ‚ÄúNeural human
estimationforwhole-body3dhumanmeshestimation,‚ÄùinCVPR
performer: Learning generalizable radiance fields for human
Workshops,2022. 2
performancerendering,‚ÄùinNeurIPS,2021. 1
[25] M. Chen, J. Zhang, X. Xu, L. Liu, Y. Cai, J. Feng, and S. Yan, [51] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, and C. Lu, ‚ÄúHybrik:
‚ÄúGeometry-guided progressive nerf for generalizable and effi- A hybrid analytical-neural inverse kinematics solution for 3d
cientneuralhumanrendering,‚ÄùinECCV,2022. 1 humanposeandshapeestimation,‚ÄùinCVPR,2021. 2
[26] R.HartleyandA.Zisserman,Multipleviewgeometryincomputer [52] A.Sengupta,I.Budvytis,andR.Cipolla,‚ÄúProbabilistic3dhuman
vision. Cambridgeuniversitypress,2003. 1,5 shapeandposeestimationfrommultipleunconstrainedimages
[27] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N. inthewild,‚ÄùinCVPR,2021. 2
Gomez,L.Kaiser,andI.Polosukhin,‚ÄúAttentionisallyouneed,‚Äù [53] I.AkhterandM.J.Black,‚ÄúPose-conditionedjointanglelimitsfor
inNeurIPS,2017. 1,3,5 3dhumanposereconstruction,‚ÄùinCVPR,2015. 2
[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre- [54] A.Zanfir,E.G.Bazavan,M.Zanfir,W.T.Freeman,R.Sukthankar,
trainingofdeepbidirectionaltransformersforlanguageunder- andC.Sminchisescu,‚ÄúNeuraldescentforvisual3dhumanpose
standing,‚ÄùinNAACL,2019. 1,3 andshape,‚ÄùinCVPR,2021. 2IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 14
[55] H.Joo,N.Neverova,andA.Vedaldi,‚ÄúExemplarfine-tuningfor [81] H.Chen,Y.Wang,T.Guo,C.Xu,Y.Deng,Z.Liu,S.Ma,C.Xu,
3d human model fitting towards in-the-wild 3d human pose C.Xu,andW.Gao,‚ÄúPre-trainedimageprocessingtransformer,‚Äù
estimation,‚Äùin3DV,2021. 2 inCVPR,2021. 3
[56] N. Kolotouros, G. Pavlakos, D. Jayaraman, and K. Daniilidis, [82] Z. Shi, X. Xu, X. Liu, J. Chen, and M.-H. Yang, ‚ÄúVideo frame
‚ÄúProbabilistic modeling for human mesh recovery,‚Äù in ICCV, interpolationtransformer,‚ÄùinCVPR,2022. 3
2021. 2 [83] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, ‚ÄúSelf-
[57] S. K. Dwivedi, N. Athanasiou, M. Kocabas, and M. J. Black, attentiongenerativeadversarialnetworks,‚ÄùinICML,2019. 3
‚ÄúLearning to regress bodies from images using differentiable [84] Y.Jiang,S.Chang,andZ.Wang,‚ÄúTransgan:Twopuretransform-
semanticrendering,‚ÄùinICCV,2021. 2 erscanmakeonestronggan,andthatcanscaleup,‚ÄùinNeurIPS,
2021. 3
[58] Y.Sun,Q.Bao,W.Liu,Y.Fu,M.J.Black,andT.Mei,‚ÄúMonocular,
one-stage,regressionofmultiple3dpeople,‚ÄùinICCV,2021. 2 [85] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù
arXiv:1607.06450,2016. 3
[59] M.Zanfir,A.Zanfir,E.G.Bazavan,W.T.Freeman,R.Sukthankar,
[86] J.S.Dai,‚ÄúEuler‚Äìrodriguesformulavariations,quaternionconju-
and C. Sminchisescu, ‚ÄúThundr: Transformer-based 3d human
gationandintrinsicconnections,‚ÄùMechanismandMachineTheory,
reconstructionwithmarkers,‚ÄùinICCV,2021. 2
vol.92,pp.144‚Äì152,2015. 4
[60] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, and
[87] I. T. Jolliffe and J. Cadima, ‚ÄúPrincipal component analysis: a
Z. Sun, ‚ÄúPymaf: 3d human pose and shape regression with
reviewandrecentdevelopments,‚ÄùPhilosophicalTransactionsofthe
pyramidal mesh alignment feedback loop,‚Äù in ICCV, 2021. 2,
Royal Society A: Mathematical, Physical and Engineering Sciences,
8
vol.374,no.2065,p.20150202,2016. 4
[61] M.Kocabas,C.-H.P.Huang,J.Tesch,L.Mu¬®ller,O.Hilliges,and [88] Y.Zhou,C.Barnes,J.Lu,J.Yang,andH.Li,‚ÄúOnthecontinuity
M.J.Black,‚ÄúSpec:Seeingpeopleinthewildwithanestimated ofrotationrepresentationsinneuralnetworks,‚ÄùinCVPR,2019.7
camera,‚ÄùinICCV,2021. 2 [89] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and
[62] M.Kocabas,C.-H.P.Huang,O.Hilliges,andM.J.Black,‚ÄúPare: G.Pons-Moll,‚ÄúRecoveringaccurate3dhumanposeinthewild
Partattentionregressorfor3dhumanbodyestimation,‚ÄùinICCV, usingimusandamovingcamera,‚ÄùinECCV,2018. 8
2021. 2,7,9 [90] C. Zhang, A. Gupta, and A. Zisserman, ‚ÄúTemporal query net-
[63] A.Kanazawa,J.Y.Zhang,P.Felsen,andJ.Malik,‚ÄúLearning3d worksforfine-grainedvideounderstanding,‚ÄùinCVPR,2021. 7
humandynamicsfromvideo,‚ÄùinCVPR,2019. 2 [91] C.Zheng,X.Liu,G.-J.Qi,andC.Chen,‚ÄúPotter:Poolingattention
[64] A. Arnab, C. Doersch, and A. Zisserman, ‚ÄúExploiting temporal transformerforefficienthumanmeshrecovery,‚ÄùinCVPR,2023.
context for 3d human pose estimation in the wild,‚Äù in CVPR, 8
2019. 2 [92] D.Mehta,O.Sotnychenko,F.Mueller,W.Xu,S.Sridhar,G.Pons-
[65] Y.Sun,Y.Ye,W.Liu,W.Gao,Y.Fu,andT.Mei,‚ÄúHumanmesh Moll,andC.Theobalt,‚ÄúSingle-shotmulti-person3dposeestima-
recovery from monocular images via a skeleton-disentangled tionfrommonocularrgb,‚Äùin3DV,2018. 8
representation,‚ÄùinICCV,2019. 2 [93] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
[66] C.DoerschandA.Zisserman,‚ÄúSim2realtransferlearningfor3d P.Dolla¬¥r,andC.L.Zitnick,‚ÄúMicrosoftcoco:Commonobjectsin
humanposeestimation:motiontotherescue,‚ÄùinNeurIPS,2019. context,‚ÄùinECCV,2014. 8
2 [94] M.Andriluka,L.Pishchulin,P.Gehler,andB.Schiele,‚Äú2dhuman
poseestimation:Newbenchmarkandstateoftheartanalysis,‚Äù
[67] Z.Luo,S.A.Golestaneh,andK.M.Kitani,‚Äú3dhumanmotion
inCVPR,2014. 8
estimation via motion compression and refinement,‚Äù in ACCV,
[95] P. H. Scho¬®nemann, ‚ÄúA generalized solution of the orthogonal
2020. 2
procrustesproblem,‚ÄùPsychometrika,vol.31,no.1,pp.1‚Äì10,1966.
[68] H. Choi, G. Moon, J. Y. Chang, and K. M. Lee, ‚ÄúBeyond static
9
featuresfortemporallyconsistent3dhumanposeandshapefrom
[96] ‚ÄúMixamo,‚Äùhttps://www.mixamo.com/. 10
avideo,‚ÄùinCVPR,2021. 2
[97] D.Sun,X.Yang,M.-Y.Liu,andJ.Kautz,‚ÄúPwc-net:Cnnsforop-
[69] G.-H.LeeandS.-W.Lee,‚ÄúUncertainty-awarehumanmeshrecov-
ticalflowusingpyramid,warping,andcostvolume,‚ÄùinCVPR,
ery from video by learning part-based 3d dynamics,‚Äù in ICCV,
2018. 12
2021. 2
[98] S. Nah, T. H. Kim, and K. M. Lee, ‚ÄúDeep multi-scale convolu-
[70] Z. Wan, Z. Li, M. Tian, J. Liu, S. Yi, and H. Li, ‚ÄúEncoder- tionalneuralnetworkfordynamicscenedeblurring,‚ÄùinCVPR,
decoderwithmulti-levelattentionfor3dhumanshapeandpose 2017. 12
estimation,‚ÄùinICCV,2021. 2 [99] B. Biggs, O. Boyne, J. Charles, A. Fitzgibbon, and R. Cipolla,
[71] X.Xu,H.Chen,F.Moreno-Noguer,L.A.Jeni,andF.DelaTorre, ‚ÄúWho left the dogs out?: 3D animal reconstruction with expec-
‚Äú3Dhumanshapeandposefromasinglelow-resolutionimage tationmaximizationintheloop,‚ÄùinECCV,2020. 12
withself-supervisedlearning,‚ÄùinECCV,2020. 2,8 [100] G.Pavlakos,V.Choutas,N.Ghorbani,T.Bolkart,A.A.Osman,
[72] ‚Äî‚Äî, ‚Äú3D human pose, shape and texture from low-resolution D.Tzionas,andM.J.Black,‚ÄúExpressivebodycapture:3dhands,
images and videos,‚Äù IEEE Transactions on Pattern Analysis and face,andbodyfromasingleimage,‚ÄùinCVPR,2019. 12
MachineIntelligence,2021. 2,7,9,12 [101] S.Zuffi,A.Kanazawa,D.Jacobs,andM.J.Black,‚Äú3Dmenagerie:
[73] F. Moreno-Noguer, ‚Äú3d human pose estimation from a single Modelingthe3Dshapeandposeofanimals,‚ÄùinCVPR,2017. 12
imageviadistancematrixregression,‚ÄùinCVPR,2017. 2 [102] J.Romero,D.Tzionas,andM.J.Black,‚ÄúEmbodiedhands:Model-
[74] Y.Rong,T.Shiratori,andH.Joo,‚ÄúFrankmocap:Amonocular3d ingandcapturinghandsandbodiestogether,‚ÄùACMTransactions
whole-bodyposeestimationsystemviaregressionandintegra- onGraphics(SIGGRAPHAsia),vol.36,no.6,2017. 12
tion,‚ÄùinICCVWorkshops,2021. 2,7,9 [103] Y.Yuan,R.Fu,L.Huang,W.Lin,C.Zhang,X.Chen,andJ.Wang,
‚ÄúHrformer:High-resolutiontransformerfordenseprediction,‚Äùin
[75] A. Davydov, A. Remizova, V. Constantin, S. Honari, M. Salz-
mann,andP.Fua,‚ÄúAdversarialparametricposeprior,‚ÄùinCVPR, NeurIPS,2021. 12
2022. 2
[76] G. Tiwari, D. Antic, J. E. Lenssen, N. Sarafianos, T. Tung, and
G.Pons-Moll,‚ÄúPose-ndf:Modelinghumanposemanifoldswith
neuraldistancefields,‚ÄùinECCV,2022. 2
[77] ‚ÄúCMUgraphicslabmotioncapturedatabase,‚Äùhttp://mocap.cs.
cmu.edu/,2010. 2
[78] L. Liu, X. Xu, Z. Lin, J. Liang, and S. Yan, ‚ÄúTowards garment
sewingpatternreconstructionfromasingleimage,‚ÄùACMTrans-
actionsonGraphics(SIGGRAPHAsia),2023. 2
[79] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo,
‚ÄúSwintransformer:Hierarchicalvisiontransformerusingshifted
windows,‚ÄùinICCV,2021. 2,6,12
[80] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S.Zagoruyko,‚ÄúEnd-to-endobjectdetectionwithtransformers,‚Äù
inECCV,2020. 3,7