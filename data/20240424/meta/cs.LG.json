[
    {
        "title": "SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation",
        "authors": "Xiangyu XuLijuan LiuShuicheng Yan",
        "links": "http://arxiv.org/abs/2404.15276v1",
        "entry_id": "http://arxiv.org/abs/2404.15276v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15276v1",
        "summary": "Existing Transformers for monocular 3D human shape and pose estimation\ntypically have a quadratic computation and memory complexity with respect to\nthe feature length, which hinders the exploitation of fine-grained information\nin high-resolution features that is beneficial for accurate reconstruction. In\nthis work, we propose an SMPL-based Transformer framework (SMPLer) to address\nthis issue. SMPLer incorporates two key ingredients: a decoupled attention\noperation and an SMPL-based target representation, which allow effective\nutilization of high-resolution features in the Transformer. In addition, based\non these two designs, we also introduce several novel modules including a\nmulti-scale attention and a joint-aware attention to further boost the\nreconstruction performance. Extensive experiments demonstrate the effectiveness\nof SMPLer against existing 3D human shape and pose estimation methods both\nquantitatively and qualitatively. Notably, the proposed algorithm achieves an\nMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by\nmore than 10% with fewer than one-third of the parameters. Code and pretrained\nmodels are available at https://github.com/xuxy09/SMPLer.",
        "updated": "2024-04-23 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15276v1"
    },
    {
        "title": "Metric-guided Image Reconstruction Bounds via Conformal Prediction",
        "authors": "Matt Y CheungTucker J NethertonLaurence E CourtAshok VeeraraghavanGuha Balakrishnan",
        "links": "http://arxiv.org/abs/2404.15274v1",
        "entry_id": "http://arxiv.org/abs/2404.15274v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15274v1",
        "summary": "Recent advancements in machine learning have led to novel imaging systems and\nalgorithms that address ill-posed problems. Assessing their trustworthiness and\nunderstanding how to deploy them safely at test time remains an important and\nopen problem. We propose a method that leverages conformal prediction to\nretrieve upper/lower bounds and statistical inliers/outliers of reconstructions\nbased on the prediction intervals of downstream metrics. We apply our method to\nsparse-view CT for downstream radiotherapy planning and show 1) that\nmetric-guided bounds have valid coverage for downstream metrics while\nconventional pixel-wise bounds do not and 2) anatomical differences of\nupper/lower bounds between metric-guided and pixel-wise methods. Our work paves\nthe way for more meaningful reconstruction bounds. Code available at\nhttps://github.com/matthewyccheung/conformal-metric",
        "updated": "2024-04-23 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15274v1"
    },
    {
        "title": "Estimation Network Design framework for efficient distributed optimization",
        "authors": "Mattia BianchiSergio Grammatico",
        "links": "http://arxiv.org/abs/2404.15273v1",
        "entry_id": "http://arxiv.org/abs/2404.15273v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15273v1",
        "summary": "Distributed decision problems features a group of agents that can only\ncommunicate over a peer-to-peer network, without a central memory. In\napplications such as network control and data ranking, each agent is only\naffected by a small portion of the decision vector: this sparsity is typically\nignored in distributed algorithms, while it could be leveraged to improve\nefficiency and scalability. To address this issue, our recent paper introduces\nEstimation Network Design (END), a graph theoretical language for the analysis\nand design of distributed iterations. END algorithms can be tuned to exploit\nthe sparsity of specific problem instances, reducing communication overhead and\nminimizing redundancy, yet without requiring case-by-case convergence analysis.\nIn this paper, we showcase the flexility of END in the context of distributed\noptimization. In particular, we study the sparsity-aware version of many\nestablished methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an\nestimation problem in sensor networks demonstrate that END algorithms can boost\nconvergence speed and greatly reduce the communication and memory cost.",
        "updated": "2024-04-23 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15273v1"
    },
    {
        "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
        "authors": "Ge GaoAlexey TaymanovEduardo SalinasPaul MineiroDipendra Misra",
        "links": "http://arxiv.org/abs/2404.15269v1",
        "entry_id": "http://arxiv.org/abs/2404.15269v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15269v1",
        "summary": "We study interactive learning of language agents based on user edits made to\nthe agent's output. In a typical setting such as writing assistants, the user\ninteracts with a language agent to generate a response given a context, and may\noptionally edit the agent response to personalize it based on their latent\npreference, in addition to improving the correctness. The edit feedback is\nnaturally generated, making it a suitable candidate for improving the agent's\nalignment with the user's preference, and for reducing the cost of user edits\nover time. We propose a learning framework, PRELUDE that infers a description\nof the user's latent preference based on historic edit data and using it to\ndefine a prompt policy that drives future response generation. This avoids\nfine-tuning the agent, which is costly, challenging to scale with the number of\nusers, and may even degrade its performance on other tasks. Furthermore,\nlearning descriptive preference improves interpretability, allowing the user to\nview and modify the learned preference. However, user preference can be complex\nand vary based on context, making it challenging to learn. To address this, we\npropose a simple yet effective algorithm named CIPHER that leverages a large\nlanguage model (LLM) to infer the user preference for a given context based on\nuser edits. In the future, CIPHER retrieves inferred preferences from the\nk-closest contexts in the history, and forms an aggregate preference for\nresponse generation. We introduce two interactive environments -- summarization\nand email writing, for evaluation using a GPT-4 simulated user. We compare with\nalgorithms that directly retrieve user edits but do not learn descriptive\npreference, and algorithms that learn context-agnostic preference. On both\ntasks, CIPHER achieves the lowest edit distance cost and learns preferences\nthat show significant similarity to the ground truth preferences",
        "updated": "2024-04-23 17:57:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15269v1"
    },
    {
        "title": "All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs",
        "authors": "Sawyer RobertsonZhengchao WanAlexander Cloninger",
        "links": "http://arxiv.org/abs/2404.15261v1",
        "entry_id": "http://arxiv.org/abs/2404.15261v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15261v1",
        "summary": "The fields of effective resistance and optimal transport on graphs are filled\nwith rich connections to combinatorics, geometry, machine learning, and beyond.\nIn this article we put forth a bold claim: that the two fields should be\nunderstood as one and the same, up to a choice of $p$. We make this claim\nprecise by introducing the parameterized family of $p$-Beckmann distances for\nprobability measures on graphs and relate them sharply to certain Wasserstein\ndistances. Then, we break open a suite of results including explicit\nconnections to optimal stopping times and random walks on graphs, graph Sobolev\nspaces, and a Benamou-Brenier type formula for $2$-Beckmann distance. We\nfurther explore empirical implications in the world of unsupervised learning\nfor graph data and propose further study of the usage of these metrics where\nWasserstein distance may produce computational bottlenecks.",
        "updated": "2024-04-23 17:50:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15261v1"
    }
]