[
    {
        "title": "SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation",
        "authors": "Xiangyu XuLijuan LiuShuicheng Yan",
        "links": "http://arxiv.org/abs/2404.15276v1",
        "entry_id": "http://arxiv.org/abs/2404.15276v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15276v1",
        "summary": "Existing Transformers for monocular 3D human shape and pose estimation\ntypically have a quadratic computation and memory complexity with respect to\nthe feature length, which hinders the exploitation of fine-grained information\nin high-resolution features that is beneficial for accurate reconstruction. In\nthis work, we propose an SMPL-based Transformer framework (SMPLer) to address\nthis issue. SMPLer incorporates two key ingredients: a decoupled attention\noperation and an SMPL-based target representation, which allow effective\nutilization of high-resolution features in the Transformer. In addition, based\non these two designs, we also introduce several novel modules including a\nmulti-scale attention and a joint-aware attention to further boost the\nreconstruction performance. Extensive experiments demonstrate the effectiveness\nof SMPLer against existing 3D human shape and pose estimation methods both\nquantitatively and qualitatively. Notably, the proposed algorithm achieves an\nMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by\nmore than 10% with fewer than one-third of the parameters. Code and pretrained\nmodels are available at https://github.com/xuxy09/SMPLer.",
        "updated": "2024-04-23 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15276v1"
    },
    {
        "title": "ID-Animator: Zero-Shot Identity-Preserving Human Video Generation",
        "authors": "Xuanhua HeQuande LiuShengju QianXin WangTao HuKe CaoKeyu YanMan ZhouJie Zhang",
        "links": "http://arxiv.org/abs/2404.15275v1",
        "entry_id": "http://arxiv.org/abs/2404.15275v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15275v1",
        "summary": "Generating high fidelity human video with specified identities has attracted\nsignificant attention in the content generation community. However, existing\ntechniques struggle to strike a balance between training efficiency and\nidentity preservation, either requiring tedious case-by-case finetuning or\nusually missing the identity details in video generation process. In this\nstudy, we present ID-Animator, a zero-shot human-video generation approach that\ncan perform personalized video generation given single reference facial image\nwithout further training. ID-Animator inherits existing diffusion-based video\ngeneration backbones with a face adapter to encode the ID-relevant embeddings\nfrom learnable facial latent queries. To facilitate the extraction of identity\ninformation in video generation, we introduce an ID-oriented dataset\nconstruction pipeline, which incorporates decoupled human attribute and action\ncaptioning technique from a constructed facial image pool. Based on this\npipeline, a random face reference training method is further devised to\nprecisely capture the ID-relevant embeddings from reference images, thus\nimproving the fidelity and generalization capacity of our model for ID-specific\nvideo generation. Extensive experiments demonstrate the superiority of\nID-Animator to generate personalized human videos over previous models.\nMoreover, our method is highly compatible with popular pre-trained T2V models\nlike animatediff and various community backbone models, showing high\nextendability in real-world applications for video generation where identity\npreservation is highly desired. Our codes and checkpoints will be released at\nhttps://github.com/ID-Animator/ID-Animator.",
        "updated": "2024-04-23 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15275v1"
    },
    {
        "title": "Metric-guided Image Reconstruction Bounds via Conformal Prediction",
        "authors": "Matt Y CheungTucker J NethertonLaurence E CourtAshok VeeraraghavanGuha Balakrishnan",
        "links": "http://arxiv.org/abs/2404.15274v1",
        "entry_id": "http://arxiv.org/abs/2404.15274v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15274v1",
        "summary": "Recent advancements in machine learning have led to novel imaging systems and\nalgorithms that address ill-posed problems. Assessing their trustworthiness and\nunderstanding how to deploy them safely at test time remains an important and\nopen problem. We propose a method that leverages conformal prediction to\nretrieve upper/lower bounds and statistical inliers/outliers of reconstructions\nbased on the prediction intervals of downstream metrics. We apply our method to\nsparse-view CT for downstream radiotherapy planning and show 1) that\nmetric-guided bounds have valid coverage for downstream metrics while\nconventional pixel-wise bounds do not and 2) anatomical differences of\nupper/lower bounds between metric-guided and pixel-wise methods. Our work paves\nthe way for more meaningful reconstruction bounds. Code available at\nhttps://github.com/matthewyccheung/conformal-metric",
        "updated": "2024-04-23 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15274v1"
    },
    {
        "title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios",
        "authors": "Jingyang LinYingda XiaJianpeng ZhangKe YanLe LuJiebo LuoLing Zhang",
        "links": "http://arxiv.org/abs/2404.15272v1",
        "entry_id": "http://arxiv.org/abs/2404.15272v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15272v1",
        "summary": "Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse negative samples. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.",
        "updated": "2024-04-23 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15272v1"
    },
    {
        "title": "Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models",
        "authors": "Wanrong ZhuJennifer HealeyRuiyi ZhangWilliam Yang WangTong Sun",
        "links": "http://arxiv.org/abs/2404.15271v1",
        "entry_id": "http://arxiv.org/abs/2404.15271v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15271v1",
        "summary": "Recent advancements in instruction-following models have made user\ninteractions with models more user-friendly and efficient, broadening their\napplicability. In graphic design, non-professional users often struggle to\ncreate visually appealing layouts due to limited skills and resources. In this\nwork, we introduce a novel multimodal instruction-following framework for\nlayout planning, allowing users to easily arrange visual elements into tailored\nlayouts by specifying canvas size and design purpose, such as for book covers,\nposters, brochures, or menus. We developed three layout reasoning tasks to\ntrain the model in understanding and executing layout instructions. Experiments\non two benchmarks show that our method not only simplifies the design process\nfor non-professionals but also surpasses the performance of few-shot GPT-4V\nmodels, with mIoU higher by 12% on Crello. This progress highlights the\npotential of multimodal instruction-following models to automate and simplify\nthe design process, providing an approachable solution for a wide range of\ndesign tasks on visually-rich documents.",
        "updated": "2024-04-23 17:58:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15271v1"
    }
]