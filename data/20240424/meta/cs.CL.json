[
    {
        "title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios",
        "authors": "Jingyang LinYingda XiaJianpeng ZhangKe YanLe LuJiebo LuoLing Zhang",
        "links": "http://arxiv.org/abs/2404.15272v1",
        "entry_id": "http://arxiv.org/abs/2404.15272v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15272v1",
        "summary": "Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse negative samples. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.",
        "updated": "2024-04-23 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15272v1"
    },
    {
        "title": "Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models",
        "authors": "Wanrong ZhuJennifer HealeyRuiyi ZhangWilliam Yang WangTong Sun",
        "links": "http://arxiv.org/abs/2404.15271v1",
        "entry_id": "http://arxiv.org/abs/2404.15271v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15271v1",
        "summary": "Recent advancements in instruction-following models have made user\ninteractions with models more user-friendly and efficient, broadening their\napplicability. In graphic design, non-professional users often struggle to\ncreate visually appealing layouts due to limited skills and resources. In this\nwork, we introduce a novel multimodal instruction-following framework for\nlayout planning, allowing users to easily arrange visual elements into tailored\nlayouts by specifying canvas size and design purpose, such as for book covers,\nposters, brochures, or menus. We developed three layout reasoning tasks to\ntrain the model in understanding and executing layout instructions. Experiments\non two benchmarks show that our method not only simplifies the design process\nfor non-professionals but also surpasses the performance of few-shot GPT-4V\nmodels, with mIoU higher by 12% on Crello. This progress highlights the\npotential of multimodal instruction-following models to automate and simplify\nthe design process, providing an approachable solution for a wide range of\ndesign tasks on visually-rich documents.",
        "updated": "2024-04-23 17:58:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15271v1"
    },
    {
        "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
        "authors": "Ge GaoAlexey TaymanovEduardo SalinasPaul MineiroDipendra Misra",
        "links": "http://arxiv.org/abs/2404.15269v1",
        "entry_id": "http://arxiv.org/abs/2404.15269v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15269v1",
        "summary": "We study interactive learning of language agents based on user edits made to\nthe agent's output. In a typical setting such as writing assistants, the user\ninteracts with a language agent to generate a response given a context, and may\noptionally edit the agent response to personalize it based on their latent\npreference, in addition to improving the correctness. The edit feedback is\nnaturally generated, making it a suitable candidate for improving the agent's\nalignment with the user's preference, and for reducing the cost of user edits\nover time. We propose a learning framework, PRELUDE that infers a description\nof the user's latent preference based on historic edit data and using it to\ndefine a prompt policy that drives future response generation. This avoids\nfine-tuning the agent, which is costly, challenging to scale with the number of\nusers, and may even degrade its performance on other tasks. Furthermore,\nlearning descriptive preference improves interpretability, allowing the user to\nview and modify the learned preference. However, user preference can be complex\nand vary based on context, making it challenging to learn. To address this, we\npropose a simple yet effective algorithm named CIPHER that leverages a large\nlanguage model (LLM) to infer the user preference for a given context based on\nuser edits. In the future, CIPHER retrieves inferred preferences from the\nk-closest contexts in the history, and forms an aggregate preference for\nresponse generation. We introduce two interactive environments -- summarization\nand email writing, for evaluation using a GPT-4 simulated user. We compare with\nalgorithms that directly retrieve user edits but do not learn descriptive\npreference, and algorithms that learn context-agnostic preference. On both\ntasks, CIPHER achieves the lowest edit distance cost and learns preferences\nthat show significant similarity to the ground truth preferences",
        "updated": "2024-04-23 17:57:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15269v1"
    },
    {
        "title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
        "authors": "Yifeng DingJiawei LiuYuxiang WeiTerry Yue ZhuoLingming Zhang",
        "links": "http://arxiv.org/abs/2404.15247v1",
        "entry_id": "http://arxiv.org/abs/2404.15247v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15247v1",
        "summary": "We introduce XFT, a simple yet powerful training scheme, by simply merging\nupcycled Mixture-of-Experts (MoE) to unleash the performance limit of\ninstruction-tuned code Large Language Models (LLMs). While vanilla sparse\nupcycling fails to improve instruction tuning, XFT introduces a shared expert\nmechanism with a novel routing weight normalization strategy into sparse\nupcycling, which significantly boosts instruction tuning. After fine-tuning the\nupcycled MoE model, XFT introduces a learnable model merging mechanism to\ncompile the upcycled MoE model back to a dense model, achieving upcycled\nMoE-level performance with only dense-model compute. By applying XFT to a 1.3B\nmodel, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6\npass@1 on HumanEval and HumanEval+ respectively. With the same data and model\narchitecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,\nalong with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and\nDS-1000, demonstrating its generalizability. XFT is fully orthogonal to\nexisting techniques such as Evol-Instruct and OSS-Instruct, opening a new\ndimension for improving code instruction tuning. Codes are available at\nhttps://github.com/ise-uiuc/xft .",
        "updated": "2024-04-23 17:32:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15247v1"
    },
    {
        "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
        "authors": "Weiyan ShiRyan LiYutong ZhangCaleb ZiemsChunhua yuRaya HoreshRogério Abreu de PaulaDiyi Yang",
        "links": "http://arxiv.org/abs/2404.15238v1",
        "entry_id": "http://arxiv.org/abs/2404.15238v1",
        "pdf_url": "http://arxiv.org/pdf/2404.15238v1",
        "summary": "To enhance language models' cultural awareness, we design a generalizable\npipeline to construct cultural knowledge bases from different online\ncommunities on a massive scale. With the pipeline, we construct CultureBank, a\nknowledge base built upon users' self-narratives with 12K cultural descriptors\nsourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge\nresources, CultureBank contains diverse views on cultural descriptors to allow\nflexible interpretation of cultural knowledge, and contextualized cultural\nscenarios to help grounded evaluation. With CultureBank, we evaluate different\nLLMs' cultural awareness, and identify areas for improvement. We also fine-tune\na language model on CultureBank: experiments show that it achieves better\nperformances on two downstream cultural tasks in a zero-shot setting. Finally,\nwe offer recommendations based on our findings for future culturally aware\nlanguage technologies. The project page is https://culturebank.github.io . The\ncode and model is at https://github.com/SALT-NLP/CultureBank . The released\nCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .",
        "updated": "2024-04-23 17:16:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.15238v1"
    }
]