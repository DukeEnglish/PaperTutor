[
    {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": "Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2407.06192v1",
        "entry_id": "http://arxiv.org/abs/2407.06192v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06192v1",
        "summary": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.",
        "updated": "2024-07-08 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06192v1"
    },
    {
        "title": "Vision-Language Models under Cultural and Inclusive Considerations",
        "authors": "Antonia KaramolegkouPhillip RustYong CaoRuixiang CuiAnders SøgaardDaniel Hershcovich",
        "links": "http://arxiv.org/abs/2407.06177v1",
        "entry_id": "http://arxiv.org/abs/2407.06177v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06177v1",
        "summary": "Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.",
        "updated": "2024-07-08 17:50:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06177v1"
    },
    {
        "title": "On Speeding Up Language Model Evaluation",
        "authors": "Jin Peng ZhouChristian K. BelardiRuihan WuTravis ZhangCarla P. GomesWen SunKilian Q. Weinberger",
        "links": "http://arxiv.org/abs/2407.06172v1",
        "entry_id": "http://arxiv.org/abs/2407.06172v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06172v1",
        "summary": "Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.",
        "updated": "2024-07-08 17:48:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06172v1"
    },
    {
        "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study",
        "authors": "Shihan DouHaoxiang JiaShenxi WuHuiyuan ZhengWeikang ZhouMuling WuMingxu ChaiJessica FanCaishuang HuangYunbo TaoYan LiuEnyu ZhouMing ZhangYuhao ZhouYueming WuRui ZhengMing WenRongxiang WengJingang WangXunliang CaiTao GuiXipeng QiuQi ZhangXuanjing Huang",
        "links": "http://arxiv.org/abs/2407.06153v1",
        "entry_id": "http://arxiv.org/abs/2407.06153v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06153v1",
        "summary": "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.",
        "updated": "2024-07-08 17:27:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06153v1"
    },
    {
        "title": "Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks",
        "authors": "Lukas NetzJan ReimarBernhard Rumpe",
        "links": "http://arxiv.org/abs/2407.06146v1",
        "entry_id": "http://arxiv.org/abs/2407.06146v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06146v1",
        "summary": "We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.",
        "updated": "2024-07-08 17:19:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06146v1"
    }
]