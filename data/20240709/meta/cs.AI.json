[
    {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": "Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2407.06192v1",
        "entry_id": "http://arxiv.org/abs/2407.06192v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06192v1",
        "summary": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.",
        "updated": "2024-07-08 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06192v1"
    },
    {
        "title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision",
        "authors": "Orr ZoharXiaohan WangYonatan BittonIdan SzpektorSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2407.06189v1",
        "entry_id": "http://arxiv.org/abs/2407.06189v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06189v1",
        "summary": "The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.",
        "updated": "2024-07-08 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06189v1"
    },
    {
        "title": "Vision-Language Models under Cultural and Inclusive Considerations",
        "authors": "Antonia KaramolegkouPhillip RustYong CaoRuixiang CuiAnders SøgaardDaniel Hershcovich",
        "links": "http://arxiv.org/abs/2407.06177v1",
        "entry_id": "http://arxiv.org/abs/2407.06177v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06177v1",
        "summary": "Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.",
        "updated": "2024-07-08 17:50:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06177v1"
    },
    {
        "title": "On Speeding Up Language Model Evaluation",
        "authors": "Jin Peng ZhouChristian K. BelardiRuihan WuTravis ZhangCarla P. GomesWen SunKilian Q. Weinberger",
        "links": "http://arxiv.org/abs/2407.06172v1",
        "entry_id": "http://arxiv.org/abs/2407.06172v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06172v1",
        "summary": "Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.",
        "updated": "2024-07-08 17:48:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06172v1"
    },
    {
        "title": "Uni-ELF: A Multi-Level Representation Learning Framework for Electrolyte Formulation Design",
        "authors": "Boshen ZengSian ChenXinxin LiuChanghong ChenBin DengXiaoxu WangZhifeng GaoYuzhi ZhangWeinan ELinfeng Zhang",
        "links": "http://arxiv.org/abs/2407.06152v1",
        "entry_id": "http://arxiv.org/abs/2407.06152v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06152v1",
        "summary": "Advancements in lithium battery technology heavily rely on the design and\nengineering of electrolytes. However, current schemes for molecular design and\nrecipe optimization of electrolytes lack an effective\ncomputational-experimental closed loop and often fall short in accurately\npredicting diverse electrolyte formulation properties. In this work, we\nintroduce Uni-ELF, a novel multi-level representation learning framework to\nadvance electrolyte design. Our approach involves two-stage pretraining:\nreconstructing three-dimensional molecular structures at the molecular level\nusing the Uni-Mol model, and predicting statistical structural properties\n(e.g., radial distribution functions) from molecular dynamics simulations at\nthe mixture level. Through this comprehensive pretraining, Uni-ELF is able to\ncapture intricate molecular and mixture-level information, which significantly\nenhances its predictive capability. As a result, Uni-ELF substantially\noutperforms state-of-the-art methods in predicting both molecular properties\n(e.g., melting point, boiling point, synthesizability) and formulation\nproperties (e.g., conductivity, Coulombic efficiency). Moreover, Uni-ELF can be\nseamlessly integrated into an automatic experimental design workflow. We\nbelieve this innovative framework will pave the way for automated AI-based\nelectrolyte design and engineering.",
        "updated": "2024-07-08 17:26:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06152v1"
    }
]