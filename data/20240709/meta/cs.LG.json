[
    {
        "title": "4D Contrastive Superflows are Dense 3D Representation Learners",
        "authors": "Xiang XuLingdong KongHui ShuaiWenwei ZhangLiang PanKai ChenZiwei LiuQingshan Liu",
        "links": "http://arxiv.org/abs/2407.06190v1",
        "entry_id": "http://arxiv.org/abs/2407.06190v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06190v1",
        "summary": "In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.",
        "updated": "2024-07-08 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06190v1"
    },
    {
        "title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
        "authors": "Vincent RouletAtish AgarwalaJean-Bastien GrillGrzegorz SwirszczMathieu BlondelFabian Pedregosa",
        "links": "http://arxiv.org/abs/2407.06183v1",
        "entry_id": "http://arxiv.org/abs/2407.06183v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06183v1",
        "summary": "Curvature information -- particularly, the largest eigenvalue of the loss\nHessian, known as the sharpness -- often forms the basis for learning rate\ntuners. However, recent work has shown that the curvature information undergoes\ncomplex dynamics during training, going from a phase of increasing sharpness to\neventual stabilization. We analyze the closed-loop feedback effect between\nlearning rate tuning and curvature. We find that classical learning rate tuners\nmay yield greater one-step loss reduction, yet they ultimately underperform in\nthe long term when compared to constant learning rates in the full batch\nregime. These models break the stabilization of the sharpness, which we explain\nusing a simplified model of the joint dynamics of the learning rate and the\ncurvature. To further investigate these effects, we introduce a new learning\nrate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes\nlong term curvature stabilization over instantaneous progress on the objective.\nIn the full batch regime, CDAT shows behavior akin to prefixed warm-up\nschedules on deep learning objectives, outperforming tuned constant learning\nrates. In the mini batch regime, we observe that stochasticity introduces\nconfounding effects that explain the previous success of some learning rate\ntuners at appropriate batch sizes. Our findings highlight the critical role of\nunderstanding the joint dynamics of the learning rate and curvature, beyond\ngreedy minimization, to diagnose failures and design effective adaptive\nlearning rate tuners.",
        "updated": "2024-07-08 17:56:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06183v1"
    },
    {
        "title": "Transfer Learning with Self-Supervised Vision Transformers for Snake Identification",
        "authors": "Anthony MiyaguchiMurilo GustineliAustin FischerRyan Lundqvist",
        "links": "http://arxiv.org/abs/2407.06178v1",
        "entry_id": "http://arxiv.org/abs/2407.06178v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06178v1",
        "summary": "We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.",
        "updated": "2024-07-08 17:52:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06178v1"
    },
    {
        "title": "Potential Based Diffusion Motion Planning",
        "authors": "Yunhao LuoChen SunJoshua B. TenenbaumYilun Du",
        "links": "http://arxiv.org/abs/2407.06169v1",
        "entry_id": "http://arxiv.org/abs/2407.06169v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06169v1",
        "summary": "Effective motion planning in high dimensional spaces is a long-standing open\nproblem in robotics. One class of traditional motion planning algorithms\ncorresponds to potential-based motion planning. An advantage of potential based\nmotion planning is composability -- different motion constraints can be easily\ncombined by adding corresponding potentials. However, constructing motion paths\nfrom potentials requires solving a global optimization across configuration\nspace potential landscape, which is often prone to local minima. We propose a\nnew approach towards learning potential based motion planning, where we train a\nneural network to capture and learn an easily optimizable potentials over\nmotion planning trajectories. We illustrate the effectiveness of such approach,\nsignificantly outperforming both classical and recent learned motion planning\napproaches and avoiding issues with local minima. We further illustrate its\ninherent composability, enabling us to generalize to a multitude of different\nmotion constraints.",
        "updated": "2024-07-08 17:48:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06169v1"
    },
    {
        "title": "DεpS: Delayed ε-Shrinking for Faster Once-For-All Training",
        "authors": "Aditya AnnavajjalaAlind KhareAnimesh AgrawalIgor FedorovHugo LatapieMyungjin LeeAlexey Tumanov",
        "links": "http://arxiv.org/abs/2407.06167v1",
        "entry_id": "http://arxiv.org/abs/2407.06167v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06167v1",
        "summary": "CNNs are increasingly deployed across different hardware, dynamic\nenvironments, and low-power embedded devices. This has led to the design and\ntraining of CNN architectures with the goal of maximizing accuracy subject to\nsuch variable deployment constraints. As the number of deployment scenarios\ngrows, there is a need to find scalable solutions to design and train\nspecialized CNNs. Once-for-all training has emerged as a scalable approach that\njointly co-trains many models (subnets) at once with a constant training cost\nand finds specialized CNNs later. The scalability is achieved by training the\nfull model and simultaneously reducing it to smaller subnets that share model\nweights (weight-shared shrinking). However, existing once-for-all training\napproaches incur huge training costs reaching 1200 GPU hours. We argue this is\nbecause they either start the process of shrinking the full model too early or\ntoo late. Hence, we propose Delayed $\\epsilon$-Shrinking (D$\\epsilon$pS) that\nstarts the process of shrinking the full model when it is partially trained\n(~50%) which leads to training cost improvement and better in-place knowledge\ndistillation to smaller models. The proposed approach also consists of novel\nheuristics that dynamically adjust subnet learning rates incrementally (E),\nleading to improved weight-shared knowledge distillation from larger to smaller\nsubnets as well. As a result, DEpS outperforms state-of-the-art once-for-all\ntraining techniques across different datasets including CIFAR10/100,\nImageNet-100, and ImageNet-1k on accuracy and cost. It achieves 1.83% higher\nImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and\n2.5x drop in training cost (GPU*hrs)",
        "updated": "2024-07-08 17:45:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06167v1"
    }
]