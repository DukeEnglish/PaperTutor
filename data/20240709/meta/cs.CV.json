[
    {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": "Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2407.06192v1",
        "entry_id": "http://arxiv.org/abs/2407.06192v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06192v1",
        "summary": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.",
        "updated": "2024-07-08 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06192v1"
    },
    {
        "title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images",
        "authors": "Zhangyang QiYunhan YangMengchen ZhangLong XingXiaoyang WuTong WuDahua LinXihui LiuJiaqi WangHengshuang Zhao",
        "links": "http://arxiv.org/abs/2407.06191v1",
        "entry_id": "http://arxiv.org/abs/2407.06191v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06191v1",
        "summary": "Recent advances in 3D AIGC have shown promise in directly creating 3D objects\nfrom text and images, offering significant cost savings in animation and\nproduct design. However, detailed edit and customization of 3D assets remains a\nlong-standing challenge. Specifically, 3D Generation methods lack the ability\nto follow finely detailed instructions as precisely as their 2D image creation\ncounterparts. Imagine you can get a toy through 3D AIGC but with undesired\naccessories and dressing. To tackle this challenge, we propose a novel pipeline\ncalled Tailor3D, which swiftly creates customized 3D assets from editable\ndual-side images. We aim to emulate a tailor's ability to locally change\nobjects or perform overall style transfer. Unlike creating 3D assets from\nmultiple views, using dual-side images eliminates conflicts on overlapping\nareas that occur when editing individual views. Specifically, it begins by\nediting the front view, then generates the back view of the object through\nmulti-view diffusion. Afterward, it proceeds to edit the back views. Finally, a\nDual-sided LRM is proposed to seamlessly stitch together the front and back 3D\nfeatures, akin to a tailor sewing together the front and back of a garment. The\nDual-sided LRM rectifies imperfect consistencies between the front and back\nviews, enhancing editing capabilities and reducing memory burdens while\nseamlessly integrating them into a unified 3D representation with the LoRA\nTriplane Transformer. Experimental results demonstrate Tailor3D's effectiveness\nacross various 3D generation and editing tasks, including 3D generative fill\nand style transfer. It provides a user-friendly, efficient solution for editing\n3D assets, with each editing step taking only seconds to complete.",
        "updated": "2024-07-08 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06191v1"
    },
    {
        "title": "4D Contrastive Superflows are Dense 3D Representation Learners",
        "authors": "Xiang XuLingdong KongHui ShuaiWenwei ZhangLiang PanKai ChenZiwei LiuQingshan Liu",
        "links": "http://arxiv.org/abs/2407.06190v1",
        "entry_id": "http://arxiv.org/abs/2407.06190v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06190v1",
        "summary": "In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.",
        "updated": "2024-07-08 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06190v1"
    },
    {
        "title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision",
        "authors": "Orr ZoharXiaohan WangYonatan BittonIdan SzpektorSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2407.06189v1",
        "entry_id": "http://arxiv.org/abs/2407.06189v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06189v1",
        "summary": "The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.",
        "updated": "2024-07-08 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06189v1"
    },
    {
        "title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation",
        "authors": "Xinying GuoMingyuan ZhangHaozhe XieChenyang GuZiwei Liu",
        "links": "http://arxiv.org/abs/2407.06188v1",
        "entry_id": "http://arxiv.org/abs/2407.06188v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06188v1",
        "summary": "Crowd Motion Generation is essential in entertainment industries such as\nanimation and games as well as in strategic fields like urban simulation and\nplanning. This new task requires an intricate integration of control and\ngeneration to realistically synthesize crowd dynamics under specific spatial\nand semantic constraints, whose challenges are yet to be fully explored. On the\none hand, existing human motion generation models typically focus on individual\nbehaviors, neglecting the complexities of collective behaviors. On the other\nhand, recent methods for multi-person motion generation depend heavily on\npre-defined scenarios and are limited to a fixed, small number of inter-person\ninteractions, thus hampering their practicality. To overcome these challenges,\nwe introduce CrowdMoGen, a zero-shot text-driven framework that harnesses the\npower of Large Language Model (LLM) to incorporate the collective intelligence\ninto the motion generation framework as guidance, thereby enabling\ngeneralizable planning and generation of crowd motions without paired training\ndata. Our framework consists of two key components: 1) Crowd Scene Planner that\nlearns to coordinate motions and dynamics according to specific scene contexts\nor introduced perturbations, and 2) Collective Motion Generator that\nefficiently synthesizes the required collective motions based on the holistic\nplans. Extensive quantitative and qualitative experiments have validated the\neffectiveness of our framework, which not only fills a critical gap by\nproviding scalable and generalizable solutions for Crowd Motion Generation task\nbut also achieves high levels of realism and flexibility.",
        "updated": "2024-07-08 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06188v1"
    }
]