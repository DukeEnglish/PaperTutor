preprint
VIDEO-STAR: SELF-TRAINING ENABLES VIDEO IN-
STRUCTION TUNING WITH ANY SUPERVISION
OrrZohar1†,XiaohanWang1,YonatanBitton2,IdanSzpektor2&SerenaYeung-Levy1†
1StanfordUniversity,2GoogleResearch
†{orrzohar,syyeung}@stanford.edu
Projectpage: https://orrzohar.github.io/projects/video-star/
ABSTRACT
The performance of Large Vision Language Models (LVLMs) is dependent on
the size and quality of their training datasets. Existing video instruction tuning
datasets lack diversity as they are derived by prompting large language models
with video captions to generate question-answer pairs, and are therefore mostly
descriptive. Meanwhile, manylabeledvideodatasetswithdiverselabelsandsu-
pervisionexist-however,wefindthattheirintegrationintoLVLMsisnon-trivial.
Herein,wepresentVideoSelf-TrainingwithaugmentedReasoning(Video-STaR),
thefirstvideoself-trainingapproach. Video-STaRallowstheutilizationofanyla-
beledvideodatasetforvideoinstructiontuning. InVideo-STaR,anLVLMcycles
betweeninstructiongenerationandfinetuning,whichweshow(I)improvesgen-
eralvideounderstandingand(II)adaptsLVLMstonoveldownstreamtaskswith
existingsupervision. Duringgeneration,anLVLMispromptedtoproposeanan-
swer. The answers are then filtered only to those that contain the original video
labels,andtheLVLMisthenre-trainedonthegenerateddataset. Byonlytraining
on generated answers that contain the correct video labels, Video-STaR utilizes
theseexistingvideolabelsasweaksupervisionforvideoinstructiontuning. Our
resultsdemonstratethatVideo-STaR-enhancedLVLMsexhibitimprovedperfor-
mance in (I) general video QA, where TempCompass performance improved by
10%, and (II) on downstream tasks, where Video-STaR improved Kinetics700-
QAaccuracyby20%andactionqualityassessmentonFineDivingby15%.
1 INTRODUCTION
TheadventoflargeVision-LanguageModels(LVLMs)markedasignificantmilestoneinartificial
intelligence. Thesemodelsaimtocreateversatilesystemscapableofunderstandingandexecuting
vision-and-languagetasksalignedwithhumanintentions. EarlyadvancementsinLVLMs,asexem-
plifiedbyworkssuchasBLIP(Lietal.,2022;2023a)andLLaVA(Liuetal.,2023b;a),havebeen
drivenbythedisseminationofpre-trainedlargelanguagemodels(LLMs)(e.g., LLaMA(Touvron
etal.,2023a;b))andpre-trainedvision/vision-languagemodels(e.g.,CLIP(Radfordetal.,2021)).
LVLMsconnectthetwomodeltypesviavisual-languagealignmentandinstructiontuning.
Weietal.(2022);Liuetal.(2023a);Karamchetietal.(2024)demonstratedtheimportanceofvisual
instructiontuningontheresultingLVLM’sperformance. However,whilemuchprogresshasbeen
madeinimage-LVLMs,video-LVLMsstillfacechallengesduetotherelativedifficultyingenerating
qualityvideoinstructiontuningdatasets.Eventhoughvideoscontainmorecomplexscenedynamics
andtemporalinformation,indicatinganeedforlargerandmorediversetrainingdatasetscompared
toimages,thelargestvideoinstructiondataset,VideoInstruct-100K(VI-100K)(Maazetal.,2023),
comprises 100K video-text pairs but only 13K unique videos. This is small compared to image
instructiondatasetslikeLLaVA-1.5-Instruct(Liuetal.,2023a),whichhas∼665K imagequestion
pairsand∼350K uniqueimages. Suchlimitationinvideoinstructiontuningleadstoperformance
saturation(Lietal.,2023b;Maazetal.,2023;Jinetal.,2023;Wangetal.,2023;Linetal.,2023).
Furthermore,duetovideoinstructiontuningdatasetconstruction-mainlypromptinglargelanguage
modelstoproducequestion-answerpairs-thesevideodatasetsoftendegradetosimplisticquestions,
promptingforvideocaptions—75%ofVI-100K’squestionsareofthistype(seeApp. Fig.10).
Combining different sources of supervision has the potential to generate more diverse video in-
1
4202
luJ
8
]VC.sc[
1v98160.7042:viXrapreprint
Input:Video with ANY label/labels Instruction tuning Output:Instruction Tuning
Question Answer Question: What
CoT is the action …?
Text BBOX TAL AR/AQA Answer: We first see the man bending…
Answer Generation Label Verification
First, we see the man First, we see the man bending down Label
What is the action bending down and and lifting a book. Then, … Finally, Bend Down
sequence in the video? lifting a book… the man can be seen reading a book. Read Book
Figure1: Video-STaROverview. Video-STaRcanutilizeanylabeledvideodataset,includingAR
(ActionRecognition),AQA(ActionQualityAssessment),andTAL(TemporalActionLocalization)
–fromwhichitgeneratesvideoinstructiontuningdata(video,question,answertriplets). Internally,
Video-STaR cycles between: (I) Answer Generation, where an LVLM is prompted to generate
candidateanswersforthequestions. (II)LabelVerificationwheregeneratedanswersarefilteredto
onlythosethatcontainthevideolabels. And(III)InstructionTuning,whereamodelisretrained
onanswersthatpassverification. Thesecyclescontinueuntilperformanceplateaus.
struction tuning datasets, enhancing video understanding. Such supervision exists, as the broader
computervisioncommunityhasdevelopedanextensivecollectionofvideobenchmarkstailoredfor
diverse tasks such as action recognition (Smaira et al., 2020; Soomro et al., 2012), action quality
assessment(Xuetal.,2022b;Zhangetal.,2023c),amongothers(Heilbronetal.,2015;Wuetal.,
2021;Grunde-McLaughlinetal.,2021;Graumanetal.,2023;Luoetal.,2022).
Beyond improving overall LVLM performance, adapting LVLMs to novel or out-of-domain tasks
is also crucial. While LVLMs have many novel and impactful applications, many remain out of
reach—suchasanalyzingradiologyimages(Senkaiahliyanetal.,2023),meteorologicaldata(Law-
son et al., 2024), traffic analysis (Zhou & Knoll, 2024), judging sporting events (e.g., gymnastics,
Olympicdiving),andassistinginsurgicalprocedures,amongothers(Zhengetal.,2024;Jiangetal.,
2024; Deng et al., 2024). These tasks require expert, in-domain knowledge that LVLMs lack,
necessitating adaptation through instruction tuning. However, collecting video instruction tuning
datasets is complex and requires extensive manual effort. For instance, training an ‘AI judge’ to
judgeOlympicdivingwouldtraditionallyinvolvecollectingdetailedexpertcritiquesofeachdive.
Ontheotherhand, thesetasksoftenincludeauxiliaryannotationsthatcouldbeleveraged, suchas
surgicaloutcomesinmedicalproceduresorjudgingscoresinOlympicevents.
Toaddressthesechallenges,wetakeinspirationfromLLM’scapabilityforself-improvement(a.k.a
self-training) (Huang et al., 2022; Zelikman et al., 2022), which involves training a model on its
generateddataandfilteringtoexcludelow-qualityoutputs. Amodel’sperformanceisimprovedby
cyclingbetweengeneration,filtering,andtraining. Forexample,Zelikmanetal.(2022)introduced
Self-TaughtReasonerswhichgeneratechain-of-thoughtbypromptinganLLMtogenerateanswers
andrationalizeresponses,retainingonlycorrectlyansweredquestionsforfurthertraining. Herein,
we explore self-training in LVLMs and introduce Video Self-Training with augmented Reasoning
(Video-STaR,seeFig.1). Video-STaRenablestheincorporationofanylabeledvideodatasetofany
formatortaskbypromptinganLVLMwiththevideoandaquestiontogenerateanswers(Fig.2,2.1)
containingthevideocontent’slabel. Ifthemodelcannotcorrectlyanswerthequestion,weprovide
thevideolabelandaskittorationalizeit(Fig.2,2.2).Wethenutilizethelabelsasweaksupervision,
rejecting answers that do not contain the correct label (Fig. 2, 2.3). By facilitating the use of any
supervisionforvideoinstructiontuning,Video-STaRenablesthecreationofdiversedatasets.
Our experimental setup initializes Video-STaR with Video-LLaVA (Lin et al., 2023), focusing on
assessing its impact on video question-answering (VQA) performance. After a few Video-STaR
training cycles, we compare the performance of Video-STaR to other state-of-the-art LVLMs and
strong baselines to gauge the effectiveness of the Video-STaR framework. Our findings demon-
stratenotableenhancementsinaccuracyandreasoningcapabilities,highlightingVideo-STaR’srole
in overcoming the constraints posed by conventional video instruction tuning datasets. We show
thattheintegrationofVideo-STaRnotonlyboostsVideo-LLaVA’sperformanceonstandardzero-
shotVQAbenchmarksbutalsosignificantlyimprovesitsadaptabilitytovariousdownstreamvideo
understandingtasks. ThisunderscoresVideo-STaR’scapacitytoadvanceLVLMtrainingwhileim-
provingoverallperformanceandversatility.
2preprint
Ourcontributionscanbesummarizedasfollows:
1. WeintroduceVideoSelf-TrainingwithaugmentedReasoning(Video-STaR),thefirstvideo
self-training Large Video-Language Model method. Using self-training, Video-STaR en-
ablestheuseofanylabeledvideodatasetforvideoinstructiontuning.
2. Video-STaRimproveszero-shotvideoquestionansweringperformanceonvariousbench-
markdatasets,asevidencedbyincreasedaccuracyonTempCompass(Liuetal.,2024)from
45.7to50.3(+10%)andonMSVD-QA(Maazetal.,2023)from69.7to71.3(+2.3%).
3. WedemonstratethatVideo-STaRcanadaptLVLMstodiversevideotasks,notablyenhanc-
ingactionqualityassessmentonFineDiving(Xuetal.,2022b),whereaccuracyrosefrom
17.6to20.2(+15%)andinactionrecognitiononKinetics700(Smairaetal.,2020),where
accuracyincreasedfrom50.0to59.9(+20%).
4. UtilizingVideo-STaR,wecreatealarge,1M videoinstructiontuningdataset-VSTaR-1M,
sourcedfromdiversedatasetsandtasks,andshowthatitbenefitsLVLMperformance.
2 VIDEO SELF-TRAINING WITH AUGMENTED REASONING (VIDEO-STAR)
Given a dataset of videos v and their corresponding labels l : D = {(v ,l )}d , Video-STaR’s
i i i=1
objectiveistocreatequestionq answerapairstoinstruction-tunethepre-trainedmodelM onthe
datasetDˆ ={(v ,q ,a )}df ,producingtheinstruction-tunedmodelMˆ. Notethatvideosneednot
i i i i=1
be from the same task, and may contain multiple labels. We start by prompting a large language
modelwithataskdescriptionT andvideolabelsLtogeneratecandidatequestionsq:
Y =A video is labeled {L} for the task of {T}. What questions could you ask
T,L
someoneaboutthevideothatshouldcontainthevideolabelsintheresponse?
Video-STaRperformsgeneration-trainingcycles,whereincycleitheinstruction-tunedmodelMˆi⋆
isproduced,whiletheinstruction-tunedmodelfromthepreviouscycleMˆ(i−1)⋆isutilizedfortrain-
ingdatageneration. WeinitializetheprocesswithMˆ0⋆,anexistinginstruction-tunedmodel.
Topreparethetrainingdataincyclei,answersaregeneratedeitherdirectlyviaAnswerGeneration
orthroughbackwardrationalizationviaLabelRationalization. InAnswerGeneration, Mˆ(i−1)⋆ is
prompted with questions (Sec. 2.1). Candidate answers are then filtered using the original video
labels(Sec.2.3). VideosrejectedduringdirectAnswerGenerationarerationalized,whereMˆ(i−1)⋆
is provided both a video v and labels l , and then prompted with the question again (Sec. 2.2).
i i
Candidate answers are filtered again, creating the instruction tuning dataset in cycle i, Dˆ of size
i
d . Apre-trainedmodelM isthenfinetunedonD ,producingMˆi⋆. Thenextcyclegeneratesdata
i i
usingMˆi⋆,untiltheperformanceplateaus(seeFig.2).
2.1 ANSWERGENERATION
Each Video-STaR cycle begins in direct Answer Generation. In this phase, Mˆ(i−1)⋆ is prompted
withthevideo-questionpairtoprovideanansweralongwithadetailedrationale:
Y =Question: {Q}. Rationalize your answer step-by-step; how can one arrive at
Q
thisconclusion?
When prompted with the question q on a particular video, Mˆ(i−1)⋆ is expected to generate an
i
answer a that contains the label ˆl and the rationale r (a = r ∪ ˆl , see Fig. 2). We observe
i i i i i i
that answers containing the correct labels are of higher quality and suffer less from hallucination.
Therefore,wefilterthegeneratedanswerstoincludeonlythosethatcontainthecorrectlabel(ˆl =
i
l )utilizingaverifier(Sec.2.3). ForanexampleofAnswerGeneration,seeFig.3.
i
2.2 LABELRATIONALIZATION
Answergenerationhastwomaindrawbacks:(i)insomeapplications,especiallyonchallenging/out-
of-domaintasks,initialanswergenerationyieldislow,resultinginalmostnotrainingsamplesafter
filtering (e.g., FineDiving, see Fig. 3); (ii) improvement plateaus as the model fails to solve new
problemsinthetrainingset,anditisonlytrainedonexamplesitanswerscorrectly.
3preprint
(2.1) Answer Generation (2.3) Label (2.2) Label Rationalization
Filtering
Video Rationali-
Video Rationale
Label Question zation
Question Label incorrect
Label Label
Label
Label
correct
correct
Repeat (2) Instruction Tuning Label
incorrect
Video (2.3) Label
Answer
Question Filtering
Video-STaR
Figure2: VideoSelf-TrainingwithaugmentedReasoning. (2.1)Weinitializebypromptingan
LVLM to generate an answer for a particular video. (2.3) We then filter the generated answers to
those only containing the original video labels. (2.2) The videos whose generated answer did not
containtheground-truthlabelsarethensenttolabelrationalization,wheregiventhevideo,question,
and label - the model is expected to rationalize the label. (2.3) The generated answers are filtered
againtothoseonlycontainingtheground-truthlabels,and(2)theLVLMisinstruction-tunedfrom
thepre-trainedcheckpointontheresultingdataset. Thecycleisthenrepeated.
Inspired by Zelikman et al. (2022), for videos whose Mˆ(i−1)⋆ generated answers did not contain
the ground-truth labels, we introduce label rationalization as part of Video-STaR. Concretely, we
provideMˆ(i−1)⋆thevideo,question,andvideolabelandinstructthemodeltorationalizethelabel:
Y =Question: {Q}. Answer: {L}.
Q,L
Canyourationalizetheanswerstep-by-step? Howcanonearriveatthiscon-
clusion?
Giventhecorrectlabel,themodelcanreasonbackwardandmoreeasilygeneratearationaleleading
tothecorrectanswer. However, labelrationalizationismorepronetohallucinations, soweprefer
directanswergenerationanduserationalizationsonlyifanswergenerationfails. Foranexampleof
LabelRationalization,seeFig.3,top. Thegeneratedanswersarethenfilteredagain,keepingonly
thosethatcontainthecorrectlabel(ˆl = l )utilizingaverifier(Sec.2.3). LabelRationalizationis
i i
onlyutilizedintrainingcycles;onlydirectanswersareusedtoproducethefinalmodelMˆ⋆.
2.3 LABELVERIFICATION
Video-STaR aims to utilize the labels as weak supervision in instruction tuning data generation.
Gold labels are a grounding aspect of our datasets and represent some ground-truth knowledge.
Therefore,weassumethatanswersthatcontaintheground-truthlabelsintheirresponsesarehigher
qualitythanthosethatdon’t. Whilewewouldliketovalidatetheexistenceofthedifferentlabelsin
thegeneratedtext,thiscanbenon-trivial.
Tothisend, weintroducetheParser-Verifier. TheParser, P extractsthepredictedlabelsfromthe
generatedtext(lˆ =P(a )),usingamixtureofnamedentityrecognitionandRegex. Regexisused
i i
toidentifyeasilyidentifiablestringpatterns,suchasboundingboxesandtimeranges,whilenamed
entityrecognitionisusedformorenuancedentities,suchastimestamps. TheVerfier,V compares
theextractedlabelswiththegoldonesusingtheappropriatemetrics(V(l ,lˆ) → R). Forexample,
i i
IoU for bounding boxes/temporal action localization, and BERT (Devlin et al., 2018) embedding
similarityforsentenceordering. Eachvideohasbetween1-3associatedlabels. Tobeclassifiedas
correct,thepredictedlabelsmustbewithina5%marginoferrorfromthegold.
3 VIDEO-STAR GENERATED DATASET - VSTAR-1M
Inthissection,wedetailthedifferentsourcedatasetsutilizedinourstudy(Sec.3.1)andanalyzethe
generatedVideo-STaRDataset,VSTaR-1M(Sec.3.2).
4preprint
Source Videos Labels Avg.Dur. SourceTask
Kinetics700(2020) 650K 700 5.1 ActionRecognition
STAR VideoReasoning,Temporal
benchmark 22K 207 28.6 ActionLocalization,
(2021) BoundingBox
FineDiving ActionQualityAssessment,
3K 1065 3.2
(2022b) ActionSequence
Table 1: Source Dataset Summary. Video datasets used as source datasets for instruction tuning
datageneration,theirsourcetasks,andthenumberofuniquelabels.
Label Rationalization
[Input Instruction]Please assess the performer's skill level and the action's toughness in the video on a scale of 1-100.
Answer: Action Sequence: Inward, 3.5 Soms.Tuck, Entry. Action difficulty: 3.2. Overall Score: 65.6.
Can you explain step-by-step how one can arrive at this conclusion?
The performer in the video executes a series of flips and twists in the air, showcasing their athleticism and skill. The first
flip is an inward somersault... The second flip is a 3.5 somersault,… The third flip is a tuck… The fourth flip is an entry,
which is the final maneuver before the performer enters the water. Overall, the performer demonstrates a high level of
Cycle 0 skill and athleticism throughout the routine, making it a challenging and impressive display of acrobatics.
First, we need to identify the action sequence performed by the diver… inward somersault, followed by a 3.5 somersault
tuck, and then an entry into the water. Next, we need to assess the difficulty of each action in the sequence. … In this
case, the difficulty of each action is as follows: Inward: 3.5, 3.5 Soms.Tuck: 3.2, Entry: 3.0… In this case, the average
Cycle 1 difficulty score is 3.2… we can conclude that the diver's action sequence is of a high skill level and is quite tough, with
an overall difficultyscore of 65.6.
Answer Generation
[Input Instruction]Please assess the performer's skill level and the action's toughness in the video on a scale of 1-100..
Can you explain step-by-step how one can arrive at this conclusion?
First, we need to identify the action sequence performed by the diver. In this case, the diver performs an inward
somersault, followed by a 3.5 somersault tuck, and then an entry into the water... The action difficulty score for ainward
to a 3.5 somersault tuck is 3.2…
Cycle 2 we can conclude that the diver's action sequence is of a high skill level and is quite tough, with an overall score of 64.68
[Label]65.6
Figure3: QualitativeImprovementofDataGenerationoverCyclesonFineDiving. Weinitial-
izethemodelwithVideo-LLaVA(Cycle0),wherethemodelcannotgenerateananswer(→|×)or
rationalizethelabelcorrectly(| → ×). Inthesecondcycle(Cycle1),themodelstillcannotgener-
ateananswer(→ |×)butcanrationalizethevideolabel(✓| →),whichisselectedforinstruction
tuning. Finally,inthethirdcycle(Cycle2),themodeldirectlygeneratesacorrectanswer(✓| →),
whichisselectedforvisualinstructiontuning. Wehighlightingreencorrectanswers,inredwrong
answers,andinyellow-hallucinations.
3.1 SOURCEDATASETS
Inselectingsourcedatasets,weselecteddatasetsthatcontaindiversevideocontentandlabeltypes,
pleaseseeTab.2.3. TheseincludeKinetics700(Smairaetal.,2020),whichhasactionrecognition
annotationsandisparticularlylargeanddiverse. FineDiving(Xuetal.,2022b)isanactionquality
assessment dataset of Olympic diving events and has both an overall score and action sequence
annotations. Finally, STAR-benchmark (Wu et al., 2021), a video reasoning dataset, also contains
bounding box and temporal action localization annotations. Tab. 2.3 contains the relevant dataset
statistics,e.g.,thenumberofvideosandlabelsperdataset.
3.2 GENERATEDDATASETANALYSIS
QuantitativeAnalysis. ThroughtheapplicationofVideo-STaR,significantdatasetaugmentation
wasachievedovertwocyclesof, illustratedinFig.4. ThisfiguredisplaystheAnswerGeneration
andLabelRationalizationyieldacrossthedatasetssource. Notably,theinitialapplicationofVideo-
5
rewsnA
gninuteniF
noitareneGpreprint
Kinetics700 STAR-Benchmark FineDiving
Video
Onascalefrom1-100,howwould
Whattypeofactivityisbeingper- Whichobjectwasputdownbythe
Question you evaluate the execution of the
formedinthisvideo? person?
actionsinthevideo?
In the video, the person is seen
Inthevideo,amanisseenholding First, we identify the diver’s ac-
picking up a bag and putting it
alargeaxeandswingingitaround tion sequence: inward somersault
downonthefloor. Afterthat, he
while a crowd watches. He then ··· entryintothewater. Evaluat-
picksupabookandputsitdown
Generated proceedstohitablockofwoodwith ingeachmovement’sdifficulty,we
onthefloor.Basedonthisinforma-
Answer theaxe. Basedontheseobserva- assign··· inwardsomersault: 2.5,
tion,wecanconcludethattheper-
tions,itcanbeconcludedthatthe ···. Byaveragingthesescores···
sonputdownthebagandpickedup
activityischoppingwood. theoverallscoreis63.0outof100.
thebook.
Label Smashing Thebag. OverallScore63.0
Table 2: Examples of Generated Data. Examples of the video, question, Video-STaR generated
answer,andground-truthlabelfromeachsourcedataset.
Kinetics700 STAR-Benchmark FineDiving
100
Answer Generation
75 Label Rationalization
Answer Generation
50
Label Rationalization
25 Answer Generation
Label Rationalization
0
0 1 2 0 1 2 0 1 2
Cycle Cycle Cycle
Figure 4: Dataset Yield vs. Cycles. Percentage of the videos converted to instruction tuning by
theAnswerGenerationandLabelRationalizationperdataset. Ascanbeseen,ondifficultdatasets,
suchasFineDiving, novideosareconvertedbyAnswerGenerationinthefirstcycle. Byutilizing
LabelRationalization,themodelisabletoimprovetoeventuallygenerateanswerscorrectly.
LLaVAondatasetslikeKinetics700andSTAR-BenchmarkshowedsignificantAnswerGeneration
success rates. However, the FineDiving dataset presented a notable challenge, with Answer Gen-
eration having no answers generated directly, underscoring the complexity of the dataset and the
criticalroleofLabelRationalization. Bytheendofthesecondcycle,asubstantialnumberofhigh-
qualityinstanceshadbeenproduced,showcasingboththeeffectivenessofVideo-STaRinconverting
labeledvideodatasetsintovideoinstructiontuningdataset,asevidencedinFig.4.
Qualitative Analysis. See Tab. 3.2 for examples of generated question-answer pairs. From Ki-
netics700,weextractedaninstanceshowcasingavideolabeled‘smashing’. Video-STaRcorrectly
identified a more fine-grained label, ‘chopping wood’. In the FineDiving dataset, a clip depicting
a complex dive was accompanied by the question ‘On a scale from 1-100···’ The model’s output
textprovidedabreakdownofthedive’scomponents,leadingtoascore(label),aswouldbedesired
from an LVLM visual assistant. Finally, in the STAR benchmark, questions are already provided;
therefore,weutilizedthemdirectly.
In3,weshowthequalitativeimprovementofthegenerateddataoverVideo-STaRcycles.Inthefirst
cycle (Cycle 0), Video-LLaVA failed at Answer Generation and Label Rationalization. After one
Video-STaRcycle(Cycle1),Video-STaRstillfailedatAnswerGenerationbutsucceededinLabel
Rationalization. AfterthefinalVideo-STaRcycle(Cycle2),Video-STaRmanagedtogeneratethe
answerwithoutrequiringthelabelviaAnswerGeneration.
4 EXPERIMENTS
We experimented with Video-STaR and evaluated its enhanced video understanding capabilities.
In Sec. 4.3, we evaluate how Video-STaR adapts Large Vision-Language Models (LVLMs) to the
source datasets and how these capabilities are transferred zero-shot to similar benchmarks. In
Sec.4.2,weevaluatethevideoquestion-answeringcapabilitiesonvideobenchmarkdatasets.
6preprint
Action Direction Speed Event AttributeChange Avg.
Fine Coarse Obj. Cam. Abs. Rel. Order Color Size Both Other
Random 39.7 40.1 39.8 39.0 40.8 42.0 41.5 40.4 39.9 38.9 39.4 40.5
mPLUG-Owl(2023) 48.8 66.1 38.7 36.8 42.2 38.4 42.0 41.7 44.7 41.9 39.9 44.4
Video-LLaVA(2023) 63.4 93.5 36.1 34.8 42.7 26.5 39.1 52.6 37.1 43.3 33.3 45.7
Video-LLaVA+ 62.1 93.0 35.0 32.6 41.1 38.7 36.4 59.0 40.2 36.7 44.4 47.2
Vid-LLaVAGemini 30.7 30.1 37.8 40.0 41.8 42.4 21.5 50.4 49.9 38.0 37.4 38.2
Video-STaR 68.6 94.1 35.8 38.0 38.7 37.6 37.1 53.8 48.5 45.0 55.6 50.3(+10%)
Gemini-1.5(2024) 94.8 98.4 43.6 42.4 65.3 48.7 55.6 79.5 59.8 70.0 66.7 66.0
Table3:Comparisonwithstate-of-the-artmethodsonTempCompass.TempCompass(Liuetal.,
2024)assessesthetemporalunderstandingcapabilitiesofvideolanguagemodelsacrossfivedimen-
sionsVideo-STaRimprovesVideo-LLaVAperformanceonTempCompassby10%.
Dataset MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
Methods
size AccuracyScore AccuracyScore AccuracyScore Accuracy Score
VideoChat(2023b) 4K 56.3 2.8 45.0 2.5 34.4 2.3 - 2.2
Video-LLaMA(2023a) 4K 51.6 2.5 29.6 1.8 - - 12.4 1.1
Video-ChatGPT(2023) 100K 64.9 3.3 49.3 2.8 51.4 3.0 35.2 2.7
Video-LLaVA∗(2023) 100K 69.7 3.9 57.4 3.5 46.5 3.3 43.2 3.4
Video-LLaVA+ 650K 67.8 3.8 56.0 3.4 46.5 3.3 42.2 3.3
Vid-LLaVAGemini 2k 67.2 3.9 55.9 3.5 44.5 3.2 39.6 3,3
Video-STaR 550K 71.3 4.0 58.2 3.5 47.3 3.3 43.2 3.3
Gemini-1.5-pro(2024) - 71.6 3.9 52.6 3.2 45.0 3.1 56.7 -
Table4: Zero-shotVideoQAbenchmarks. Ascanbeseen,manymodelsareapproachingGemini
performance-indicatingthatLVLMsmaybeoperatingnearthenoiselevelonthesebenchmarks.
4.1 EXPERIMENTALSETTING
Implementation Details. We initialize from the Video-LLaVA (Lin et al., 2023) model, which
utilizestheVicuna-7Bv1.5(Chiangetal.,2023). WeranthreeVideo-STaRcycles,andeachcycle
was initialized with the pre-trained Video-LLaVA weights. We train for one epoch using a 128
batchsize,AdamWoptimizer,andacosinelearningrateschedule. Thelearningrateis2e−5with
a0.03warmupratio. IncombinationwiththegeneratedVideo-STaRinstructiontuningdataset,we
additionally utilized the VideoInstruct-100K (Maaz et al., 2023) and the LLaVA v1.5 instruction
tuningdatasets(Liuetal.,2023a). Additionaldetailsareavailableintheappendix.
Baselines. BesidescomparingtoVideo-LLaVA,wealsowantedtoevaluatetheeffectofutilizing
additional data and naively adapting the source datasets. Therefore, we utilized simple templates
togeneratequestion-answerpairsfromthevideolabelsandtrainedVideo-LLaVAontheresulting
dataset. We will reference this baseline as Video-LLaVA+. Another baseline for adapting Large
Vision-LanguageModelstonoveltasksismodeldistillation,whereastrongervideomodel-inthis
work, Gemini 1.5 pro-vision - is utilized to label/annotate a small set of videos (500 from each
dataset)andusedtofinetunethemodels. WewillreferencethisbaselineasVid-LLaVAGemini.
Evaluation Details. We evaluate on the following benchmarks; the Zero-shot question-answer
(QA) benchmarks: MSVD-QA, MSRVTT-QA, TGIF-QA, and ActivityNet-QA (Xu et al., 2017;
2016;Jangetal.,2017;Heilbronetal.,2015). TempCompass(Liuetal.,2024),amultiple-choice
fine-grainedQAbenchmark. Adaptedtaskperformanceisevaluatedbyconvertingsourcedatasets
usingsimpletemplatesandapplyingthesameevaluationprotocolasMaazetal.(Maazetal.,2023),
producingKinetics700-QA,STAR-benchmark-QA,andFineDiving-QA.Thisprotocolreportstwo
metrics: accuracy (the percentage of correctly answered questions) and the average score (where
ChatGPTrateseachresponseonascaleof1-5andcalculatesthemeanofthesescores). Allevalu-
ationsutilizethesameGPTmodel(Wu,2024)(“gpt-3.5-turbo”)toensureconsistentcomparisons.
Duetocostconsiderations,1000videoswererandomlyselectedfromeachdatasetforGeminieval-
uation. ThereportedvaluesareusedonActivitlyNet-QA.
7preprint
Kinetics700-QA STAR-bench-QA FineDiving-QA
Methods
Accuracy Score Accuracy Score Accuracy Score
Video-LLaVA 50.0 3.2 24.9 2.6 17.6 2.2
Video-LLaVA+ 49.5 3.2 28.8 2.8 19.1 2.2
Vid-LLaVAGemini 41.9 2.9 22.3 2.6 16.3 2.1
Video-STaR 59.9(+20%) 3.5(+10%) 33.0(+33%) 2.9(+12%) 20.2(+15%) 2.3(+5%)
Table 5: Adapted Dataset Performance. Performance metrics on test sets of Kinetics700, Fine-
Diving, and STAR-benchmark datasets via converting them to QA following Maaz et al. (2023).
Video-STaR shows significant improvement over Video-LLaVA and Video-LLaVA+, showing the
potentialofVideo-STaRforLVLMadaptationtonewtasks.
4.2 QUANTITATIVEEVALUATIONONZERO-SHOTBENCHMARKS
To evaluate Video-STaR’s effect on general video question answering, we evaluated its effect on
Video-LLaVA’sperformanceonTempCompass,seeTab.3. OnTempCompass,Video-STaRoutper-
formed Video-LLaVA across the board– by ∼ 10%. To see if this performance boost is simply a
factoroftrainingonalargerdataset,wealsoevaluatedVideo-LLaVA+. Video-LLaVA+wastrained
onevenalargervideodatasetbynaivelyutilizingvideolabels,andyieldsamoremodestimprove-
ment of 3%, showing the utility of Video-STaR. TempCompass is also a fine-grained dataset that
would be sensitive to hallucinations, indicating that Video-STaR is not more prone to hallucina-
tionscomparedtoexistingmethods. Gemini1.5proscoredanimpressive66.0onTempCompass,
showingthereisstillmuchroomforimprovementonthisbenchmark.
We then continued and evaluated Video-STaR’s effect on zero-shot video QA performance on the
MSVD-QA,MSRVTT-QA,TGIF-QAandActivityNet-QAbenchmarks. AscanbeseeninTab.4,
Video-STaR achieves performance improvements where, for instance, on the MSVD-QA dataset,
Video-STaR attains the highest accuracy of 71.3% vs Video-LLaVA’s 69.7. On MSRVTT-QA,
Video-STaRleadswithanaccuracyof58.2%andmaintainsacompetitiveedgeinotherdatasetslike
TGIF-QAandActivityNet-QA.SeeingtherelativelysmallperformancegainscomparedtoTemp-
Compass, we additionally evaluated Gemini 1.5 pro-vision on 1000 video subsets of each dataset
andfoundthatitsperformanceisonparwithexistingopen-sourcemodels. Webelievethisshows
thatwearenearthe‘noise’limitofthesebenchmarks. Ourqualitativeanalysisindicatedthatmany
ofthequestionsselectedas‘wrong’areactuallyduetothebenchmarkdesign—overlygeneralques-
tionswithmultiplecorrectanswers. Concurrentwork(Wu,2024)hassimilarlyconcludedthatthe
ChatGPT-3.5versionutilizedinevaluationcanleadtovariationsof±10inaccuracy.
4.3 QUANTITATIVEEVALUATIONONADAPTEDDATASETS
Besides improving general visual question-answering performance, Video-STaR can also adapt
LargeVision-Languagemodelstonoveltakes. Todemonstratethis,weconvertedthetestsets(not
includedintraining)ofthesourcedatasets–Kinetics700,STAR-benchmark,andFineDiving. The
results of these evaluations are reported in Tab. 5. Adapting LVLMs with easier-to-collect labels
canbehelpfulinvariousapplications, leadingtoamoreversatile, multi-domaincapableassistant.
When evaluating Video-STaR’s impact on LVLM performance on the diverse source datasets, we
foundthatitsignificantlyimprovesmodelperformance,particularlyoncomplextasks.Forinstance,
on Kinetics700, known for its extensive action categories, Video-STaR enhanced Video-LLaVA’s
performance accuracy by an average of 20% (as can be seen in Tab. 5), showcasing its ability to
developgeneralizedmodelsadeptacrossmultipledomains. Interestingly,Video-LLaVA+’sperfor-
mancedidnotimprovecomparedtoVideo-LLaVA,andinsomecases,evenworsened,showingthat
onecannotdirectlyutilizelabeleddatasetsforLVLMadaptation.
Action Quality Assessment (AQA) is a complex video task requiring detailed action understand-
ing,whereVideo-STaRsignificantlyenhancedLVLMperformanceontheFineDivingdataset. Our
results show a notable improvement from 17.6 to 20.2 in score prediction accuracy, highlighting
Video-STaR’seffectivenessinrefiningLVLM’stemporalreasoning. However,Video-STaRallows
LVLMs to not only rate a particular dive but also explain the rationale behind each assessment.
Thisrationaleisinvaluableformanyapplications,effectivelyprovidingpotentialuserfeedbackfor
8preprint
Kinetics700-QA STAR-bench-QA FineDiving-QA
Ablations
Accuracy Score Accuracy Score Accuracy Score
Video-STaR 59.9 3.5 33.0 2.9 20.2 2.3
-Rationalization 59.8 3.5 26.6 2.7 12.8 2.0
-Generation 50.0 3.2 24.9 2.6 17.6 2.2
Table6: AblationsonAdaptedDatasets. PerformancemetricsontestsetsofKinetics700,STAR-
benchmark, and FineDiving datasets. Label Rationalization impacts mostly the difficult datasets,
suchasFineDiving,whoseinitialAnswerGenerationyieldsarelow.
MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
Ablations
Accuracy Score Accuracy Score Accuracy Score Accuracy Score
Video-STaR 71.3 4.0 58.2 3.5 46.8 3.3 42.2 3.3
-Rationalization 70.6 3.9 57.5 3.5 47.7 3.4 42.2 3.3
-Generation 69.7 3.9 57.4 3.5 46.5 3.3 43.2 3.4
Table7:AblationsonZero-ShotBenchmarks.Insimplerbenchmarks,AnswerGenerationproved
morecriticalforzero-shotgeneralizationthanLabelRationalization.
improvement. This advancement enables novel applications, from sports coaching to automated
feedback systems, by offering evaluations and constructive feedback. The ability to interpret and
improveactionqualityunderscoresthepotentialofVideo-STaR,underscoringthepotentialofuti-
lizingLVLMsasintelligentandinformativevisualassistants. Formore,pleaseseeApp. Sec.A.2.
4.4 ABLATIONS
In our ablation studies, we evaluated the impact of removing Label Rationalization and Answer
Generation from Video-STaR, focusing on adapted datasets (Kinetics700, FineDiving, STAR-
benchmark)andzero-shotbenchmarks(MSVD-QA,MSRVTT-QA,TGIF-QA,ActivityNet-QA).
Adapted Datasets For adapted datasets (Tab. 4.3), excluding Label Rationalization led to a sig-
nificant performance drop in FineDiving, from 20.2 to 12.8 in accuracy, highlighting its critical
roleincomplexreasoningtasks. Thisislikelyduetothelackofconversionofanyexamplesfrom
thedata. However,theremovalofAnswerGenerationresultedinamorepronouncedanduniform
decline across all datasets. For example, Kinetics700’s accuracy was reduced from 59.9 to 50.0,
underscoringitsfoundationalroleingeneratingcontext-relevantresponses.
Zero-shot benchmarks In zero-shot benchmarks (Tab. 4.3), the removal of Label Rationaliza-
tion had a mixed impact, slightly affecting MSVD-QA where accuracy decreased from 71.3 to
70.6. TheeliminationofAnswerGenerationconsistentlyloweredperformance,suchasadecrease
in MSRVTT-QA accuracy from 58.2 to 57.4. ActivityNet-QA performance improved, probably
because 100K-Instruct utilizes ActivityNet for instruction tuning. Therefore, the introduction of
additionalvideosdecreasesperformance.
5 RELATED WORKS
5.1 LARGEVISION-LANGUAGEMODELS
Initial LVLMs, such as LLaVA (Liu et al., 2023b;a) and BLIP-2 (Li et al., 2023a), demonstrated
thepotentialofmergingimageinputswithlargelanguagemodels. MethodslikemPLUG-Owl(Ye
etal.,2023)andFlamingo(Alayracetal.,2022)furtherallowedformultipleimageinputswithout
architectural changes. Li et al. (2023b) and Zhang et al. (2024) led the transition to video un-
derstanding,integratingvideo/imageencodersandLLMswhiletrainingonsmallvideoinstruction
tuningdatasets. Jinetal.(2023)introducedChat-UniVi,aunifiedmodelemployingdynamicvisual
tokensforbothimagesandvideos,optimizingvisualtokenusageandhigherframecountsampling.
LLaMA-VID(Lietal.,2023c)showedthatthetokencountcanbefurtherreducedbypoolingthe
tokens selectively via the text prompt using Q-Former. Recently, Video-LLaVA (Lin et al., 2023)
9preprint
usedmodality-specificencodersforvideoandimageinputstoleverageLanguageBindencodersas
theyareconstructivelyalignedduringpretrainingandutilizedasharedprojection.
Maaz et al. (2023) expanded the field with the first large video instruction tuning dataset,
VideoInstruct-100K. VideoInstruct-100K wasgenerated fromActivityNet(Heilbron etal., 2015)
by prompting chatGPT with the video captions, generating question-answer pairs. While driving
much of the performance improvement in the field (Jin et al., 2023; Wang et al., 2023; Lin et al.,
2023; Li et al., 2023c), upon examination of VideoInstruct-100K, it is evident that it suffers from
qualityissues. Thequestionsoftendegradeintodefactopromptsforavideocaption(seeFig.10)
andrarelyrequiremanyspatiotemporalcapabilities,whichmaylimitLVLMperformance.
5.2 LARGELANGUAGEMODELSANDSELF-TRAINING
The advent of GPT (Radford et al., 2018; Brown et al., 2020) marked significant milestones in
naturallanguageprocessing,showcasingLLMs’powerinunderstandingandgeneratinghuman-like
text. Open-sourceLLMslikeLLaMA(Touvronetal.,2023a;b)andtheirinstruction-tunedvariants
like Alpaca and Vicuna (Taori et al., 2023; Chiang et al., 2023) further tailored these models for
nuancedhuman-AIinteractions. However,evenLLMshavefounditchallengingtoscaleannotated
datasets for training, prompting work on self-training and self-improvement (Singh et al., 2023;
Huang et al., 2022; Ho et al., 2023; Marasovic´ et al., 2022; Hosseini et al., 2024). In this line
ofwork,LLMscyclebetweeninstruction-tuningdatagenerationandinstructiontuning,iteratively
improving LLM performance over cycles. For instance, Zelikman et al. (2022) introduced the the
Self-TaughtReasonersmethod,usedrationalizationtogeneratechain-of-thought(CoT)reasoning,
filteringpoorrationalizationstoretaincorrectlyansweredquestions. Otherself-trainingapproaches
include expectation-maximization-based approaches (Singh et al., 2023), which alternate between
data generation and improvement between training cycles. Alternatively, majority voting has also
beenutilizedtogenerateanswersandrationaleforunlabeledquestions(Huangetal.,2022). These
methodsshowtheeffectivenessofiterativeself-training. Inourwork,weaimtointroduceaweakly
supervisedself-trainingapproachforvideoinstructiontuning, leveragingvideosupervisionthatis
ofteneasiertocollectandexistsinmanylargeanddiversedatasets.
6 CONCLUSIONS
In conclusion, Video Self-Taught Reasoners (Video-STaR) presents a novel approach to enhance
LargeVision-LanguageModels(LVLMs)byenablingtheuseofdiverselabeledvideodatasetsfor
visualinstructiontuning. Thismethodaddressescriticaldatadiversityandqualitychallenges,lead-
ing to significant performance improvements across various video understanding tasks. Our ex-
perimentsdemonstrateVideo-STaR’seffectivenessinbothsourcedatasetadaptationandzero-shot
generalization,showcasingitspotentialinadvancingLVLMcapabilitiesforcomplexvideocontent.
ThepromisingresultsofVideo-STaRopennewresearchavenues,particularlyinexpandingLVLM
knowledgebasesusingreadilyavailableimageandvideodatasets. Futureworkcouldexploread-
vanced self-training techniques and integration with emerging LVLM architectures, focusing on
long-form video understanding to further boost LVLM understanding. Additional work is also
neededtoreducehallucinations,perhapsbyusinggroundedVLMsasauxiliaryinput.
Acknowledgements. WethankGoogleResearchforprovidingfinancialsupportthroughaStanford
HAI–Googlecollaboration. ThisworkwaspartiallysupportedbytheNationalScienceFoundation
underGrantNo. 2026498. WethanktheKnight-HennessyScholarsFoundationforfundingOZ.
7 LIMITATIONS
While Video-STaR introduces a novel approach to visual instruction tuning, it is not without its
limitations. Firstly, the methodology can be computationally intensive due to the cycling of both
generatingandrationalizingquestion-answerandinstructiontuning. Secondly,theassumptionthat
alllabelsnecessitatearationalemaynotalwaysholdtrue. Certainlabelsmightbestraightforward
enough not to require elaborate rationalization, potentially leading to unnecessary computational
overhead.Lastly,hallucinations,especiallyinlabelrationalizationcanbefurtherreducedbyperhaps
implementingadditionalverifiers.
10preprint
REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc,ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,Serkan
Cabi,TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,Sebastian
Borgeaud, AndrewBrock, AidaNematzadeh, SahandSharifzadeh, MikolajBinkowski, Ricardo
Barreira,OriolVinyals,AndrewZisserman,andKarenSimonyan. Flamingo: avisuallanguage
modelforfew-shotlearning,2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,
BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R.Hadsell,M.F.Balcan,andH.Lin(eds.),AdvancesinNeuralInformationProcessingSystems,
volume33,pp.1877–1901.CurranAssociates,Inc.,2020.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Adam Cohen. seatgeek/fuzzywuzzy : Release 0.18.0, February 2020. URL https://pypi.
org/project/fuzzywuzzy/.
JiawenDeng,KiyanHeybati,andMatthewShammas-Toma. Whenvisionmeetsreality: Exploring
theclinicalapplicabilityofgpt-4withvision,2024.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
KristenGrauman,AndrewWestbury,LorenzoTorresani,KrisKitani,JitendraMalik,Triantafyllos
Afouras, KumarAshutosh, VijayBaiyya, SiddhantBansal, BikramBoote, EugeneByrne, Zach
Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria
Escobar,CristhianForigua,AbrhamGebreselasie,SanjayHaresh,JingHuang,MdMohaiminul
Islam,SuyogJain,RawalKhirodkar,DevanshKukreja,KevinJLiang,Jia-WeiLiu,SagnikMa-
jumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ra-
gusa,SanthoshKumarRamakrishnan,LuigiSeminara,ArjunSomayazulu,YaleSong,ShanSu,
Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke
Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo,
AnushKumar,RobertKuo,SachLakhavani,MiaoLiu,MiLuo,ZhengyiLuo,BrighidMeredith,
AustinMiller,OluwatumininuOguntola,XiaqingPan,PennyPeng,ShramanPramanick,Merey
Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Souther-
land, MasatoshiTateno, HuiyuWang, YuchenWang, TakumaYagi, MingfeiYan, XitongYang,
ZechengYu,ShengxinCindyZha,ChenZhao,ZiweiZhao,ZhifanZhu,JeffZhuo,PabloArbe-
laez,GedasBertasius,DavidCrandall,DimaDamen,JakobEngel,GiovanniMariaFarinella,An-
toninoFurnari,BernardGhanem,JudyHoffman,C.V.Jawahar,RichardNewcombe,HyunSoo
Park, JamesM.Rehg, YoichiSato, ManolisSavva, JianboShi, MikeZhengShou, andMichael
Wray.Ego-exo4d:Understandingskilledhumanactivityfromfirst-andthird-personperspectives.
arXivpreprintarXiv:2311.18259,2023.
Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark
for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,2021.
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet:
Alarge-scalevideobenchmarkforhumanactivityunderstanding. In2015IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR),pp.961–970,2015.doi:10.1109/CVPR.2015.
7298698.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.
arXivpreprintarXiv:2212.10071,2023.
11preprint
ArianHosseini,XingdiYuan,NikolayMalkin,AaronCourville,AlessandroSordoni,andRishabh
Agarwal. V-star: Trainingverifiersforself-taughtreasoners,2024.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei
Han. Large language models can self-improve. ArXiv, abs/2210.11610, 2022. URL https:
//api.semanticscholar.org/CorpusID:253080328.
YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,andGunheeKim. Tgif-qa: Towardspatio-
temporal reasoning in visual question answering. In Proceedings of the IEEE conference on
computervisionandpatternrecognition,pp.2758–2766,2017.
Yixing Jiang, Jesutofunmi A Omiye, Cyril Zakka, Michael Moor, Haiwen Gui, Shayan Alipour,
SeyedShahabeddinMousavi,JonathanHChen,PranavRajpurkar,andRoxanaDaneshjou. Eval-
uatinggeneralvision-languagemodelsforclinicalmedicine. medRxiv,pp.2024–04,2024.
PengJin, RyuichiTakanobu, CaiwanZhang, XiaochunCao, andLiYuan. Chat-univi: Unifiedvi-
sualrepresentationempowerslargelanguagemodelswithimageandvideounderstanding. arXiv
preprintarXiv:2311.08046,2023.
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa
Sadigh. Prismaticvlms: Investigatingthedesignspaceofvisually-conditionedlanguagemodels.
arXivpreprintarXiv:2402.07865,2024.
John R Lawson, Montgomery L Flora, Kevin H Goebbert, Seth N Lyman, Corey K Potvin,
David M Schultz, Adam J Stepanek, and Joseph E Trujillo-Falco´n. Pixels and predictions: Po-
tentialofgpt-4vinmeteorologicalimageryanalysisandforecastcommunication. arXivpreprint
arXiv:2404.15166,2024.
VladimirI.Levenshtein. Binarycodescapableofcorrectingdeletions,insertions,andreversals. So-
vietphysics.Doklady, 10:707–710, 1965. URLhttps://api.semanticscholar.org/
CorpusID:60827152.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
training for unified vision-language understanding and generation. In International Conference
onMachineLearning,pp.12888–12900.PMLR,2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InAndreasKrause,Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.),
Proceedings of the 40th International Conference on Machine Learning, volume 202 of Pro-
ceedings of Machine Learning Research, pp. 19730–19742. PMLR, 23–29 Jul 2023a. URL
https://proceedings.mlr.press/v202/li23q.html.
Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,
and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355,
2023b.
YanweiLi,ChengyaoWang,andJiayaJia.Llama-vid:Animageisworth2tokensinlargelanguage
models,2023c.
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united
visualrepresentationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023a.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprintarXiv:2304.08485,2023b.
Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun,
and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:
2403.00476,2024.
12preprint
ZelunLuo,ZaneDurante,LindenLi,WanzeXie,RuochenLiu,EmilyJin,ZhuoyiHuang,LunYu
Li, Jiajun Wu, Juan Carlos Niebles, et al. Moma-lrg: language-refined graphs for multi-object
multi-actoractivityparsing. AdvancesinNeuralInformationProcessingSystems,35:5282–5298,
2022.
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:
Towards detailed video understanding via large vision and language models. arXiv preprint
arXiv:2306.05424,2023.
Ana Marasovic´, Iz Beltagy, Doug Downey, and Matthew E. Peters. Few-shot self-rationalization
withnaturallanguageprompts. arXivpreprintarXiv:2111.08284,2022.
AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguageunder-
standingbygenerativepre-training. 2018.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
Ilya Sutskever. Learning transferable visual models from natural language supervision. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp.
8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
radford21a.html.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024.
Senthujan Senkaiahliyan, Augustin Toma, Jun Ma, An-Wen Chan, Andrew Ha, Kevin R An,
Hrishikesh Suresh, Barry Rubin, and Bo Wang. Gpt-4v (ision) unsuitable for clinical care and
education: aclinician-evaluatedassessment.medrxiv. PreprintpostedonlineonNovember, 16,
2023.
Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Pe-
terJ.Liu,JamesHarrison,JaehoonLee,KelvinXu,AaronParisi,AbhishekKumar,AlexAlemi,
Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi,
Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron,
Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L.
Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini
Bansal,EthanDyer,BehnamNeyshabur,JaschaSohl-Dickstein,andNoahFiedel.Beyondhuman
data: Scalingself-trainingforproblem-solvingwithlanguagemodels,2023.
LucasSmaira,Joa˜oCarreira,EricNoland,EllenClancy,AmyWu,andAndrewZisserman. Ashort
noteonthekinetics-700-2020humanactiondataset,2020. URLhttps://deepmind.com/
research/open-source/kinetics.
KhurramSoomro,AmirRoshanZamir,andMubarakShah.Ucf101:Adatasetof101humanactions
classesfromvideosinthewild,2012.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca,2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
languagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
13preprint
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXivpreprintarXiv:2307.09288,2023b.
YizhouWang,RuiyiZhang,HaoliangWang,UttaranBhattacharya,YunFu,andGangWu. Vaquita:
Enhancing alignment in llm-assisted video understanding. arXiv preprint arXiv:2312.02310,
2023.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
AndrewM.Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. InInterna-
tional Conference on Learning Representations, 2022. URL https://openreview.net/
forum?id=gEZrGCozdqR.
Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. STAR: A bench-
mark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Infor-
mation Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https:
//openreview.net/forum?id=EfgNF5-ZAjM.
WenhaoWu.Freeva:Offlinemllmastraining-freevideoassistant.arXivpreprintarXiv:2405.07798,
2024.
AngchiXu,Ling-AnZeng,andWei-ShiZheng. Likertscoringwithgradedecouplingforlong-term
actionassessment. In2022IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pp.3222–3231,2022a. doi: 10.1109/CVPR52688.2022.00323.
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.
Video question answering via gradually refined attention over appearance and motion. In ACM
Multimedia,2017.
JinglinXu,YongmingRao,XuminYu,GuangyiChen,JieZhou,andJiwenLu. Finediving: Afine-
graineddatasetforprocedure-awareactionqualityassessment. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pp.2949–2958,June2022b.
JunXu,TaoMei,TingYao,andYongRui. Msr-vtt: Alargevideodescriptiondatasetforbridging
video and language. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pp.5288–5296,2016.
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei
Huang,andJingrenZhou. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwith
modalitycollaboration,2023.
XuminYu, YongmingRao, WenliangZhao, JiwenLu, andJieZhou. Group-awarecontrastivere-
gressionforactionqualityassessment.InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision(ICCV),pp.7919–7928,October2021.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with
reasoning. InAliceH.Oh, AlekhAgarwal, DanielleBelgrave, andKyunghyunCho(eds.), Ad-
vancesinNeuralInformationProcessingSystems,2022.URLhttps://openreview.net/
forum?id=_3ELRdg2sgI.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023a.
RenruiZhang,JiamingHan,ChrisLiu,AojunZhou,PanLu,HongshengLi,PengGao,andYuQiao.
LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=d4UiXAHN2W.
14preprint
Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, and Yansong Tang.
Logo: A long-form video dataset for group action quality assessment. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2405–2414,
June2023b.
Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, and Yansong Tang.
Logo: A long-form video dataset for group action quality assessment. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2405–2414,
June2023c.
ZiqiangZheng,YiweiChen,JipengZhang,Tuan-AnhVu,HuiminZeng,YueHimWongTim,and
Sai-KitYeung.Exploringboundaryofgpt-4vonmarineanalysis:Apreliminarycasestudy.arXiv
preprintarXiv:2401.02147,2024.
XingchengZhouandAloisCKnoll. Gpt-4vastrafficassistant:Anin-depthlookatvisionlanguage
modeloncomplextrafficevents. arXivpreprintarXiv:2402.02205,2024.
15preprint
A APPENDIX
InSec.A.1,weprovideadditionalimplementationdetailsandcomputeusedindevelopingVideo-
STaR. In Sec. A.2, we introduce explainable action quality assessment and provide good and bad
examplesofVideo-STaRontheFineDivingtestdataset. Finally, weprovideadditionalqualitative
AnswerGenerationandLabelRationalizationexamplesinSec.A.3andA.4.
A.1 IMPLEMENTATIONDETAILS
Video-STaR utilized the Video-LLaVA model, which integrated the Vicuna-7B v1.5 for language
processing and ViT-L/14 video and image encoders from LanguageBind for visual encoding. The
system’stokenizer,adaptedfromLLaMA,hasavocabularysizeofaround32,000classesandadi-
mensionalityof4096. TwocyclesofVideo-STaRwereexecuted,eachinitialedwiththepre-trained
Video-LLaVAmodel(beforeinstructiontuning). Thetrainingdatawasaugmentedbyincorporating
VideoInstruct-100KandLLaVAv1.5’svisualinstructiondatasets.
Fourclustersof10NVIDIATitanRTXGPUswereemployedfor64hours. Thestructuredprompts
forthesetaskswereasfollows:
• AnswerGeneration
Question: {Q}.
Can you explain step-by-step how one can arrive at
this
conclusion?
• LabelRationalization
Question: {Q}
Answer: {L}.
Can you explain step-by-step how one can arrive at
this
conclusion?
Thesepromptsguidedthemodelinproducingdetailedanswersandrationalizations,enhancingthe
depthandutilityofthegeneratedinstruction-tuningdataset.Answercorrectnesswasevaluatedusing
templatematchingwithLevenshteinDistance-basedLevenshtein(1965)fuzzylogicCohen(2020),
consideringananswercorrectifallkeywordsfromthelabelwerepresentinthegeneratedresponse
withaminimumsimilarityscoreof80%.Forexample,ifinKineticstheactionlabelis‘eatingapple
pie’,wewouldonlyconsiderageneratedanswercorrectof‘eating’,‘apple’,‘pie’allappearedwith
asimilarityscoreof80.
A.2 EXPLAINABLEACTIONQUALITYASSESSMENT
Explainable Action Quality Assessment (AQA) is critical for detailed analysis of performances in
precision sports, such as competitive diving, where execution and complexity significantly impact
scores.UnlikepreviousAQAworks,whichonlyprovideascoreXuetal.(2022b;a);Yuetal.(2021);
Zhangetal.(2023b)Video-STaRnotonlygeneratesscoresbutalsooffersdetailedjustificationsakin
toexpertanalysis.
Fig. 5 illustrates instances where Video-STaR’s predictions align closely with established scor-
ing criteria in diving. For example, the model breaks down a dive sequence, Reverse→3.5
Soms.Tuck→Entry, with an assigned difficulty of 3.5, into its components. It then logically as-
signsscorestoeachelement,suchas1formorestraightforwardmaneuversand4formorecomplex
twists,culminatinginanoveralldifficultyscoreof3.3,closetotheGT3.5.Thefinalpredictedscore
was79.2,closetotheGTscoreof85.78.
Video-STaR’sproficiencyextendstodiveswithvaryinglevelsofperformance. Itcandiscernrela-
tivelycomplexdiveswithground-truthscoresof74.8and54.6,whichVideo-STaRscoredas76.5
and47.0,respectively. Inbothcases,themodelbreaksdowntheactionsintosub-actionsandrates
themintermsofdifficultyandexecution. Whileitmanagestoratethedivesthemselveswell,inone
instance,themodelerroneouslycalculatedtheaveragefor2.5,3.4,3.4,3.4as3.4.
16preprint
[Question]Could you appraise the structure and performance of the sequence in this video on a scale of 1 to 100?
Sure, I'd be happy to explain the reasoning behind the final label. The first step is to evaluate the action sequence. In this case, the
sequence is Back->3.5 Soms.Tuck->Entry. The first action, Back, is a basic backward somersault, which is a 1 on the difficulty scale.
The second action, 3.5 Soms.Tuck, is a more advanced move that involves a series of flips and twists, which is a 4 on the difficulty
scale. Finally, the third action, Entry, is the most difficult part of the sequence, as it involves diving into the water from ahigh
platform. This is a 5 on the difficulty scale.
Next, we need to evaluate the action difficulty. The difficulty scale ranges from 1 to 10, with 1 being the easiest and 10 beingthe most
difficult. In this case, the difficulty scale is 3.3.
Finally, we need to combine the action sequence and action difficulty to arrive at an overall score. In this case, the overall score is
79.2.
Overall, the conclusion is that the sequence in this video is quite impressive, with a high level of difficulty and execution.
[Label] :Overall Score 85.78.
[Additional Labels]: Action Sequence: Reverse->3.5 Soms.Tuck->Entry. Action Difficulty: 3.5.
[Question]How would you rate the complexity and accuracy of the performance in this video on a scale from 1 to 100?
Sure, I'd be happy to explain the reasoning behind the score. First, we need to identify the action sequence performed by thediver. In
this case, the diver starts by diving forward, then performs a 2.5 somersaults pike, followed by two twists, and finally, an entry.
Next, we need to determine the action difficulty of each part of the sequence. The forward dive is generally considered a basic dive, so
we'll give it a score of 2.5. The 2.5 somersaults pike is a more advanced move, so we'll give it a score of 3.4. The two twists are also a
more advanced move, so we'll give them a score of 3.4. Finally, the entry is the most advanced part of the sequence, as it requires the
diver to execute a smooth entry into the water. We'll give it a score of 3.4.
Finally, we need to calculate the overall score for the action sequence. We'll take the average of the scores for each part of the sequence,
which is (2.5 + 3.4 + 3.4 + 3.4) / 4 = 3.4.
So, the final label for this action sequence is Overall Score 76.5.
Answer: Reasoning: Action Sequence: Forward->2.5 Soms.Pike->2 Twists->Entry, action difficulty: 3.4
Final Label: Overall Score 76.5.
[Label] :Overall Score 74.8.
[Additional Labels]: Action Sequence: Forward->2.5 Soms.Pike->2 Twists->Entry. Action Difficulty: 3.4.
[Question]How would you rate the complexity and accuracy of the performance in this video on a scale from 1 to 100?
Sure, I'd be happy to explain the reasoning behind the score. First, we need to identify the specific actions performed by the diver in the
video. We can see that the diver performs a backflip, followed by a 2.5 somersaults, and then a 1.5 twists before entering the water.
Next, we need to evaluate the precision and complexity of each action. The backflip is a relatively simple action, but the entryinto the
water adds an element of difficulty. The 2.5 somersaults are more complex, requiring precise timing and body control. The 1.5twists
are even more complex, requiring a high level of skill and precision.
Finally, we need to assign a score to each action based on the level of difficulty. For the backflip, we can assign a score of 2.5. For the
2.5 somersaults, we can assign a score of 3.0. For the 1.5 twists, we can assign a score of 4.0.
Adding up the scores for each action, we get a total score of 9.0.
Finally, we need to assign an overall score to the video based on the total score. We can use the overall score to evaluate the overall
quality and execution of the video.
In this case, we can assign an overall score of 47.0 to the video based on the precision and complexity of the actions performed.
Answer: Reasoning: Action Sequence: Back->2.5 Soms.Pike->1.5 Twists->Entry, action difficulty: 3.0
Final Label: Overall Score 47.0
[Label] : Overall Score 54.6.
[Additional Labels]: Action Sequence: Back->2.5 Soms.Tuck->Entry. Action Difficulty: 2.8.
Figure 5: Action Quality Assessments by Video-STaR on the FineDiving Test Set. Different
divingsequenceswithcorrespondingVideo-STaRevaluations,fromahighscoreof85.78forcom-
plexsequences(top)to74.8forintermediate(middle),andalowerscoreof54.6forbasicsequences
(bottom),showcasingVideo-STaR’sproficiencyinscoringdiveswithvaryingdegreesofdifficulty
andexecutionquality.
ChallengesinmaintainingconsistentAQAaccuracyaredepictedinFig.6,showcasinginstancesof
either the model not following instructions or estimating the score incorrectly. In Fig. 6, top, the
modeldidnotproduceascoreandresortedtoexplaininghowascoremightbederived. Themodel
occasionallyproducesaninaccuratescore,andeithergrosslyover(Fig.6,middle)orunder(Fig.6,
17preprint
[Question]How would you evaluate the magnitude of complexity and performance in the displayed action sequence?
The difficulty of the action sequence is evaluated based on the number of movements, the level of difficulty of each movement, and the
level of precision required to execute the sequence. The sequence is then scored based on this evaluation.
[Label] :Overall Score 52.5.
[Additional Labels]: Action Sequence: Back->2.5 Soms.Pike->Entry. Action Difficulty: 3.0.
[Question]How would you rate the complexity and accuracy of the performance in this video on a scale from 1 to 100?
Sure, I'd be happy to explain the reasoning behind this conclusion. First, we need to identify the action sequence performed by the
diver. In this case, the diver performs an inward somersault, followed by a 3.5 somersault tuck, and then an entry into the water.
Next, we need to assign a difficulty score to each action in the sequence. In this case, we'll give the inward somersault a score of 3.5,
the 3.5 somersault tuck a score of 3.2, and the entry a score of 3.5.
Finally, we need to calculate the overall score for the action sequence. To do this, we'll take the average of the difficultyscores for each
action in the sequence. In this case, the average score is 3.2.
Based on this reasoning, we can conclude that the overall score for the action sequence is 72.0.
Answer:Reasoning: Action Sequence: Inward->3.5 Soms.Tuck->Entry, action difficulty: 3.2
Final Label: Overall Score 72.0.
[Label] :Overall Score 35.0.
[Additional Labels]: Action Sequence: Back->2.5 Soms.Tuck->Entry. Action Difficulty: 2.8.
[Question]How well would you grade the action sequence based on its difficulty and the execution quality?
Sure, I'd be happy to explain the reasoning behind the grading. First, we need to identify the action sequence performed by the diver. In
this case, the diver performs a reverse dive, followed by a 3.5 somersaults, and then a tuck before entering the water.
Next, we need to determine the action difficulty of the sequence. The action difficulty is a measure of how difficult it is to execute the
sequence. In this case, the action difficulty is 3.5.
Finally, we need to determine the overall score for the action sequence. The overall score is based on the difficulty of the action
sequence and the quality of its execution. In this case, the overall score is 34.5.
Overall, the conclusion is that the diver's action sequence is quite impressive, with a high level of difficulty and execution quality.
[Label]Overall Score 67.65.
[Additional Labels]: Action Sequence: Back->3.5 Soms.Tuck->Entry. Action Difficulty: 3.3.
Figure6: ChallengesinActionQualityAssessmentbyVideo-STaRontheFineDivingTestSet.
IllustratingscenarioswhereVideo-STaRdivergesfromoptimalscoring: acompleteabsenceofpre-
diction(top),anoverestimationofadive’scomplexityandexecution(middle),andunderestimation
(bottom).
bottom)estimatesthescore. Forexample,itattributedascoreof72.0toasequencedeemedtohave
alowerdifficultylevelof35.0. Thesediscrepanciesunderscorethenecessityforongoingimprove-
mentsinthemodel’sgraspofnuancedscoringcriteriatoenhancereliabilityinAQApredictions.
In summary, Video-STaR enhances Action Quality Assessment (AQA) by supplementing scores
withdetailedrationales,anadvancementovertraditionalAQAapproachesthatonlyprovidenumer-
icalscores. Althoughiteffectivelydissectsthecomponentsofdivingperformances,indicatingboth
complexityandexecutionasseeninFig.5, itfaceschallengesinmaintainingconsistentaccuracy,
particularlyinaligningscoreswithestablishedbenchmarks,asevidencedinFig.6. Theseinstances
highlighttheneedforadeeperunderstandingofeachmovement’sdifficultyandexecutionquality
toensurethemodel’sscoringalignswithprofessionaljudgingstandards. Additionally,theyempha-
sizetheongoingrequirementtoimprovethemodel’saccuracyandgrounditsrationalizationsinthe
verifiableaspectsoftheperformancesitevaluates.
18preprint
Kinetics700
[Input Instruction]What specific action is being performed in this video?
Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, a man is pretending to be a statue on a small table. He is holding a sign that says"I'm not a statue, I'm a performer." The man's
posture and facial expressions indicate that he is fully engaged in the act of pretending to be a statue. Additionally, the factthat he is holding a
sign suggests that he is trying to convey a message or entertain passersby. Overall, the video showcases a unique and creative performance that is
sure to captivate and amuse viewers.
[Label] pretending to be a statue.
[Input Instruction]What actions are being performed in this video?
Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, a man is seen walking towards the camera while holding a phone. He then proceeds to text while walking. Later, he is shown
driving a car while texting. The video also shows a man in a red shirt walking towards the camera, and another man in a red shirt walking towards
a car. Additionally, a man in a red shirt is seen walking towards a car with a camera.
[Label] texting.
STAR-benchmark
[Input Instruction]Which object was eaten by the person?
Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, the person is seen holding a medicine in their hand and then proceeds to eat it. The video does not provide any information
about the type of medicine or its appearance. However, we can assume that the person is taking the medicine to alleviate somesort of discomfort
or illness.
[Label] : The medicine.
[Input Instruction]Which object was sat at by the person?
Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, the person is seen sitting at a table with a plate of food. The camera zooms in on the plate of food, and we can see that it is a
white plate with a slice of cake on it. The person then proceeds to eat the cake using his hands. From this, we can conclude that the person is
sitting at a table with a plate of food, specifically a slice of cake.
[Label]The table.
FineDiving
[Input Instruction]Examine this video, andallocate a score between 1 and 100 for the complexity and efficiency of the performance.
Can you explain step-by-step how one can arrive at this conclusion?
Sure, I'd be happy to explain the reasoning behind the score. First, we need to identify the action sequence performed by thediver. In this case,
the diver performs an inward somersault, followed by a 2.5 somersault, and then a pike. This sequence is then followed by an entry into the water.
Next, we need to evaluate the action difficulty of each movement. The inward somersault is a basic movement, so we'll give ita score of 2.5. The
2.5 somersault is a more complex movement, so we'll give it a score of 3.0. The pike is a more advanced movement, so we'll give it a score of 4.0.
Finally, the entry into the water is a basic movement, so we'll give it a score of 2.5.
Finally, we need to combine these scores to arrive at an overall score for the action sequence. We'll add up the scores for eachmovement and
divide by the total number of movements to get an average score. In this case, the average score is 3.0.
Based on this reasoning, we can conclude that the diver's performance is of medium complexity and efficiency, with a score of63.0 out of 100.
Action Sequence: Inward->2.5 Soms.Pike->Entry, action difficulty: 3.0
Final Label: Overall Score 63.0
[Label] : Overall Score 63.0.
[Additional Labels]: Action Sequence: Inward->2.5 Soms.Pike->Entry. Action Difficulty: 3.0.
Figure 7: Answer Generation Across Datasets by Video-STaR. Interpreting actions on Kinet-
ics700(top),detailingactionsequencesinSTAR-benchmark(middle),andevaluatingdivingaction
qualityinFineDiving(bottom).
19preprint
STAR-benchmark
[Input Instruction]Question: What did the person do while they were lying on the sofa/couch?
Answer: Lied on the sofa/couch. Can you explain step-by-step how one can arrive at this conclusion?
Sure, in the video, the person is seen lying on the sofa/couch. They then sit up and grab a blanket. After that, they stand up and grab a
shirt. They then proceed to put the shirt on the sofa/couch. Finally, they grab the blanket again and put it on the sofa/couch. Based on
this information, it can be concluded that the person was likely getting ready to change their clothes or clean the sofa/couch.
[Label] : Lied on the sofa/couch.
[Input Instruction] Question: What did the person do with the box?
Answer: Closed. Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, the person is seen sitting in a chair and opening a box. After opening the box, they proceed to close it.
[Label]Closed.
FineDiving
[Input Instruction]Question: Can you rank the level of complexity and execution in this video from 1 to 100?
Answer: Action Sequence: Reverse->3.5 Soms.Tuck->Entry, action difficulty: 3.5. Final Label: Overall Score 71.4.
Can you explain step-by-step how one can arrive at this conclusion?
Sure, I'd be happy to explain the reasoning behind the final label. First, we need to identify the action sequence performed by the divers
in the video. The action sequence in this video is Reverse->3.5 Soms.Tuck->Entry.
Next, we need to determine the action difficulty of each movement. The action difficulty is a measure of how difficult it is to execute
each movement. In this case, the action difficulty is 3.5.
Finally, we need to calculate the overall score based on the action sequence and action difficulty. The overall score is calculated by
multiplying the action sequence score by the action difficulty score. In this case, the overall score is 71.4.
So, the conclusion is that the divers in the video performed a complex action sequence with a high level of execution, earning an
overall score of 71.4 out of 100.
[Label] : Overall Score 71.4.
[Additional Labels]: Action Sequence: Reverse->3.5 Soms.Tuck->Entry. Action Difficulty: 3.5.
Figure 8: Successful Label Rationalizations by Video-STaR. The model accurately infers a
preparatory behavior from a person’s interaction with household items (top), correctly interprets
box-handling actions (middle), and provides a nuanced breakdown of a complex diving sequence,
assigninganoverallscoreof71.4(bottom),exemplifyingpreciseactionunderstandingonFineDiv-
ingandSTAR-benchmark.
A.3 QUALITATIVEANALYSISOFANSWERGENERATION
Fig.7presentsourmodel’scapabilitiesingeneratinganswersandrationalizingactionsacrossvaried
videocontexts,demonstratingitsadeptnessininterpretingcomplexscenes.
Kinetics700. In the first example from Kinetics700 (Fig. 7, Kinetics700, top), the model effec-
tivelyidentifiesaman’sactofpretendingtobeastatueandfurtherdiscernstheperformance’ssubtle
aspects, such as the engagement level and the humor conveyed through a sign. In another Kinet-
ics700example(Fig.7,Kinetics700,bottom),themodelprocessesascenewithmultipleconcurrent
actions. Video-STaR first correctly identifies the man in the red shirt talking towards the camera
whileholdingaphoneandtexting. Itcorrectlyidentifiesthenextscene,whereanothermanistex-
tingwhiledriving. Thisprecisionintemporallylocatingdifferentactionsinthevideosisinvaluable
forvisualinstructiontuningandcouldpotentiallyenhancemodels’Thecapabilitytoanalyzescenes
withmultiplefocalpointsisanessentialfeatureforcomprehensivevideounderstanding.
20preprint
STAR-benchmark
STAR-benchmark
[Input Instruction]Question: Which object was put down by the person?
Answer: The bag. Can you explain step-by-step how one can arrive at this conclusion?
Sure, in the video, the person is seen sitting on a red sofa and talking on the phone. He then stands up and walks towards a table where
he takes a dish and puts it on the table. He then proceeds to wash the dish and places it on the rack to dry.
[Label] : Took.
[Input Instruction] Question: Which object was opened by the person?
Answer: The closet/cabinet. Can you explain step-by-step how one can arrive at this conclusion?
Yes, in the video, the person is seen opening a closet/cabinet. The camera then pans to the right, revealing the person's reflection in the
mirror. The person is seen bending down and picking up a pair of shoes. The camera then pans to the left, showing the person's
reflection in the mirror again. The person then puts on the shoes and walks away. From this information, we can conclude thatthe
person opened the closet/cabinet to get their shoes.
[Label]The closet/cabinet.
FineDiving
[Input Instruction]Question: Could you appraise the structure and performance of the sequence in this video on a scale of 1 to 10?
Answer: Reasoning: Action Sequence: Inward->3.5 Soms.Tuck->Entry, action difficulty: 3.2
Final Label: Overall Score 60.8.
Can you explain step-by-step how one can arrive at this conclusion?
Sure, I'd be happy to explain the reasoning behind the final label. The first step is to evaluate the action sequence. In this case, the
sequence is Inward->3.5 Soms.Tuck->Entry. The difficulty of this sequence is 3.2. The second step is to evaluate the overall score. The
overall score is based on a combination of factors, including the difficulty of the action sequence, the execution of the moves,and the
overall style and artistry of the performance. Based on the information provided, the overall score is 60.8.
Overall, the conclusion is that the video showcases a skilled diver performing a series of flips and twists with precision and grace,
earning an overall score of 60.8 out of 100.
[Label] : Overall Score 60.8.
[Additional Labels]: Action Sequence: Inward->3.5 Soms.Tuck->Entry. Action Difficulty: 3.2.
Figure9: ChallengesinLabelRationalizationbyVideo-STaR.Instancesofrationalizationerrors
areshown: anincorrectinferenceaboutdishwashing(top)andfabricateddetailsaboutclosetinter-
actions(middle). Arationalizationforadivingsequence(bottom)isaccuratebutdemonstratesthe
model’svulnerabilitytohallucinationsincomplexactionsequenceswithinFineDivingandSTAR-
benchmarkdatasets.
STAR-benchmark. In the first STAR-benchmark example (Fig. 7, STAR- benchmark, top), the
modelusesinferentialreasoningtodeducethepurposebehindapersonconsumingmedicinedespite
the absence of explicit details about the medicine. This instance showcases the model’s ability
to apply logical assumptions to fill in informational gaps, a valuable trait for interpreting actions
withoutfullydetailedcontext. Inthenextexample(Fig.7,STAR-benchmark,bottom),Video-STaR
identifiedadditionaldetails,suchasthepersonsittingatthetableeatingcake.
FineDiving. in Fig. 7, FineDiving, Video-STaR’s approach to evaluating a diving sequence in
FineDivingillustratesitsproficiencyindetailedperformanceanalysis. Bydeconstructingthedive
intoindividualelementsandassessingeachfordifficulty,themodelmirrorstheevaluativeprocesses
of human judges, providing a comprehensive performance score. This depth of analysis, which
21preprint
Figure 10: VideoInstruct-100K Qualitative Evaluation. Evaluation of VideoInstruct-100K’s
question distribution. A wide gap can be seen between ‘grounding’ questions, which contains
‘where/when’ (bottom) and the top-50 most common questions (top). when analyzing the top-50
mostcommonquestions(top),itiscleartheyareallpromptingforvideocaptions.
includesacritiqueofthedive’scomplexityandexecution,underscoresthemodel’sutilityincontexts
requiringnuancedassessment,suchasathleticperformanceevaluation.
TheseexamplesfromFig.7showhowtheproposedAnswerGenerationiscapableofcreatingvalu-
able and informative video question-answer pairs that can be utilized in instruction tuning, high-
lighting its potential applicability in various domains that demand a deep understanding of video
scenes.
A.4 QUALITATIVEEXAMPLESOFLABELRATIONALIZATIONS
LabelrationalizationinVideo-STaRisinitiatedwhendirectanswergenerationbytheLargeMulti-
modalModel(LMM)fails.Althoughthismethodprovesbeneficialincertainsituations,itoccasion-
allyleadstothegenerationofhallucinatedcontent—incorrectdetailsorinferencesnotsupportedby
thevideo.
STAR-benchmark. Illustrated in Fig. 8, effective label rationalizations provide added context,
enriching the model’s responses. A notable example from the STAR benchmark demonstrates the
model’scapacitytobuilduponabasicaction,likelyingonacouch,byinferringadditionalactivities,
suchasdonningashirtandtidyingup,hintingattheindividual’ssubsequentactions. Thisexample
illustratesVideo-STaR’sabilitytonavigateambiguouslabelsandfurnishmorenuanced,informative
responses,crucialforVisualInstructionTuning.
However, Label Rationalization is also more prone to hallucinations. Fig. 9 exposes the model’s
tendency for hallucinations, mainly where it introduces actions and details not evidenced in the
video. InFig.9, STAR-benchmark, top; theLMMhallucinatedthataftertakingthedish, theman
washeditandplaceditonadryrack,whichdidnotoccurinthevideo. InFig.9,STAR-benchmark,
bottom; themodelhallucinatedthatthecamerapannedtotheright, thatitcouldseethereflection
ofthemaninthemirror,andthathetookoutshoesfromthecloset—noneofwhichoccurinthe
video.
FineDiving. Labelrationalizationprovedespeciallyusefulinchallenging,domain-expertdatasets
such as the Olympic diving scoring dataset - FineDiving. Answer Generation initially had zero
yield,andLabelRationalizationallowedthemodeltostartlearningaboutthischallengingtask. For
22preprint
example, in Olympic diving events, to get an overall score for the dive, one removes the top and
bottom 2 scores (out of a total of 7), then multiplies this score with the dive difficulty. In Fig. 8,
FineDiving, the LMM correctly deduced that the execution score is multiplied by the difficulty.
Withnosupervision,themodelcorrectlydeducedtherules,deducingthatthefinalexecutionscore
isobtainedbymultiplyingtheexecutionscoreandtheactiondifficulty.
Fig.9,FineDiving,LabelRationalizationfailedtogenerateananswerwithsufficientdepth. Rather
thanprovidingadditionalinsights,themodelresortedtore-iteratingthelabels.
In summary, Label Rationalization produces shorter, less informative answers than Answer Gen-
eration and is more prone to hallucinations. It is primarily utilized so complex datasets, such as
FineDiving,canbelearned,especiallyincaseswithinitialzeroyield.
A.5 EVALUATIONOFVIDEOINSTRUCTIONTUNINGDATASETS
WeperformedaqualitativeevaluationofVideoInstruct-100K andfoundthatthebroadmajorityof
thequestionsessentiallyprompttheLargeVision-Languagemodelforvideocaptions-seeFig.10.
Ascanbeseen,∼75%ofVideoInstruct-100K’squestionsareofthistype.
23