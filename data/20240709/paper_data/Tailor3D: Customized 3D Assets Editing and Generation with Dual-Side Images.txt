Tailor3D: Customized 3D Assets Editing and
Generation with Dual-Side Images
ZhangyangQi1,2∗ YunhanYang1∗ MengchenZhang2 LongXing2 XiaoyangWu1
TongWu2,3 DahuaLin2,3 XihuiLiu1 JiaqiWang2† HengshuangZhao1†
1TheUniversityofHongKong 2ShanghaiAILab 3TheChineseUniversityofHongKong
https://tailor3d-2024.github.io/ *equalcontribution †correspondingauthor
Input Image / Text
Original Text Original Text Original Text Original Text
Super cool + grey + traditional + astronaut
clothes Chinese style clothes + blue
little boy clothes
Original Image Original Image Original Image Original Image
law letter red flag chick basketball
+golden
+ pillow + blanket ball
+ cat
+
2D Edit and Customize z 3D Reconstruction
z y
Image Editing
Triplane x y
Multi-View Transformer x
Diffusion z
Fused Triplane
Image Editing Front-View y Feature
Camera Embed
x
Figure1:ResultsandPipeline.Weshowourmethodfor3Dstylecustomization,aswellasgeometry
andtextureediting. Ourpipelineinvolveseditingimagesandgeneratingthe3DobjectusingDual-
sidedLRM,witheachstepcompletedinjust5seconds,allowingforrapid3Dobjectcustomization.
Abstract
Recentadvancesin3DAIGChaveshownpromiseindirectlycreating3Dobjects
fromtextandimages,offeringsignificantcostsavingsinanimationandproduct
design. However, detailed edit and customization of 3D assets remains a long-
standingchallenge. Specifically,3DGenerationmethodslacktheabilitytofollow
finelydetailedinstructionsaspreciselyastheir2Dimagecreationcounterparts.
Imagineyoucangetatoythrough3DAIGCbutwithundesiredaccessoriesand
dressing. Totacklethischallenge,weproposeanovelpipelinecalledTailor3D,
whichswiftlycreatescustomized3Dassetsfromeditabledual-sideimages. We
aimtoemulateatailor’sabilitytolocallychangeobjectsorperformoverallstyle
transfer. Unlikecreating3Dassetsfrommultipleviews,usingdual-sideimages
eliminatesconflictsonoverlappingareasthatoccurwheneditingindividualviews.
4202
luJ
8
]VC.sc[
1v19160.7042:viXra
ARoL
tniopweiV
noitnettA-ssorCSpecifically,itbeginsbyeditingthefrontview,thengeneratesthebackviewof
theobjectthroughmulti-viewdiffusion. Afterward,itproceedstoedittheback
views. Finally,aDual-sidedLRMisproposedtoseamlesslystitchtogetherthe
front and back 3D features, akin to a tailor sewing together the front and back
ofagarment. TheDual-sidedLRMrectifiesimperfectconsistenciesbetweenthe
frontandbackviews,enhancingeditingcapabilitiesandreducingmemoryburdens
whileseamlesslyintegratingthemintoaunified3DrepresentationwiththeLoRA
TriplaneTransformer. ExperimentalresultsdemonstrateTailor3D’seffectiveness
acrossvarious3Dgenerationandeditingtasks,including3Dgenerativefilland
styletransfer. Itprovidesauser-friendly,efficientsolutionforediting3Dassets,
witheacheditingsteptakingonlysecondstocomplete.
1 Introduction
Inrecentyears,technologieslikeStableDiffusion[1]andControlNet[2]haverevolutionized2D
AI-generatedcontent(AIGC),makingtasksliketext-to-imagesynthesis,imageediting,andstyle
transfermoreaccessibleandefficient. Concurrently,thepotentialof3DAIGChasbeenrecognized,
allowingforthedirectgenerationof3Dobjectsbyintegratingtextandimages,significantlyreducing
costs. Earlyoptimization-basedmethods[3–5],whereeachobjectneedstobeindividuallyoptimized,
usedmulti-viewstablediffusion[6–8]whichmeansgeneratingimagesofanobjectfrommultiple
perspectivesbyinputtinganimagefromoneperspective—toproducefine-grainedobjectsbutwere
slow, taking minutes to hours. However, feed-forward methods leveraging large-scale 3D asset
datasets[9]andTransformermodelsnowenablethecreationofhigh-quality3Dobjectsinseconds.
Despite progress in generation, advancements in 3D customization and editing, such as adding
patternsorchangingstylesof3Dobjects,arestillscarce.
InFeed-ForwardMethods,althoughLRM[10]cangeneratehigh-quality3Dobjectsfromasingle
view, it often lacks comprehensive details from other perspectives. In contrast, techniques like
Instant3D [11] and LGM [12] use multi-view diffusion [13, 14] to generate images from four
perspectives (front, back, left, and right) before reconstruction. While increasing the number of
perspectivescancapturemorevisualinformation,italsobringssomechallenges: managingmultiple
viewssimultaneouslyincreasesthecomplexityofeditingtasks. Forinstance,ifwewanttochange
thecolorofaspecificpartoftheobject,itisdifficulttopreciselycorrespondthechangesacrossall
fourimages. Tobalancetherichnessofvisualinformationandtheeaseofediting,werecommend
prioritizingthefrontandbackviews. Theseviewstypicallycontaincomprehensiveinformationabout
theobjectandhaveminimaloverlap,allowingthemtobeeditedindependently,thussimplifying
operationsandimprovingefficiency.
Weproposeanefficientanduser-friendly3Drapideditingframework,Tailor3D,whichintroduces
a novel 3D editing way by leveraging advanced 2D image editing techniques. This framework
delegatesthegenerationandeditingtasksto2Dimageeditingtechnologiesandgenerates3Dobjects
throughrapid3Dreconstruction,allowinguserstoiterativelyrefinethedesired3Dobjectsthrougha
combinationof2Deditingand3Dreconstructionsteps. TheprocessisshowninFigure1: Assume
theusershaveafront-viewimageofadog.First,theyeditthefrontviewusingimageeditingmethods
togeneratespaceglassesandadashboardseamlesslyintothescene. Next,employingmulti-view
diffusiontechnology,theycangenerateabackview. Thentheyedittheback-viewimagewiththe
imageeditingmethodsagaintoaddthebackpack. Finally,theeditedfrontandbackimagesareinput
intoaDual-sidedLRMmodeltogeneratea3Dmodelofthespacedog. Theentireprocessallowsfor
step-by-stepeditingandcompleteseachstepwithinseconds,providinggreatconvenienceforrapidly
editingtherequired3Dobjects. Thisstep-by-stepmethodprovidesmoreprecisecontrolthanend-
to-endediting,enablingspecificadjustmentstoimagetexturesbeforereconstruction. Additionally,
separatelyeditingfrontandbackviewsallowsformoredetailedcustomizationofthefinal3Dobject.
OurproposedDual-sidedLRM,usedinthefinalstepofTailor3D,generates3Dobjectsbyreceiving
frontandbackimages. AsshowninthelowerpartofFigure1,Havinginformationfrombothsides
allowsforamorecomprehensiveunderstandingoftheobject,butitmayleadtoViewinconsistency,
referringtodifferencesingeometry,color,andbrightnessinimagestakenfromvariousanglesand
conditions,whichcanaffectthequalityofreconstruction. WeextendsLRM’scapabilityfromsingle-
viewtodual-viewinput,effectivelyhandlinginconsistenciesbetweenviews. WeintroducetheLoRA
TriplaneTransformer[15],whichfine-tunestheLRMmodelwithminimalmemoryconsumption
2onasmalldatasetof20Kimagestogeneratetriplanefeaturesforbothfrontandbackviews. This
approachefficientlyproducesaccuratetriplanefeatures,providingasolidfoundationforsubsequent
featurefusion. Insteadofmerelystitching2Dimagefeatures,wecombinethe3Dtriplanefeaturesof
bothviewswithin3Dspace. ByapplyingViewpointCross-Attentiononthetriplane,wemergethese
featuresswiftly,enhancingthequalityofthefinal3Dobject. Additionally,weusedataaugmentation
duringtrainingtofurtherimprovethemodel’srobustness. Experimentalresultsdemonstratethatit
excelsinvarious3Deditingtasks,includinggeometricfill,texturesynthesis,andstyletransfer.
Ourcontributionscanbesummarizedasfollows:
• WeproposeTailor3D,arapid3Deditingpipeline. Bycombining2Dimageeditingandrapid3D
reconstructiontechniques,itsignificantlyenhancestheefficiencyof3Dobjectediting.
• OurDual-sidedLRM,combinedwiththeLoRATriplaneTransformer,efficientlyhandlesinconsis-
tenciesbetweenfrontandbackviews,improvingtheoverallreconstructionquality.
• Tailor3Dexcelsinvarious3Deditingandcustomizationtasks,particularlyinlocal3Dgenerative
fill,overallstyletransfer,andstylefusionforobjects,showcasingimmensepracticalutility.
2 RelatedWork
Multi-view Diffusion for Objects. Utilizing a single front-view image, multi-view diffusion
demonstratesremarkablecapabilitiesinsynthesizingimagesfromalternateviewpointsoftheobject
[6,16,17,7,18,13,14]. Thesesynthesizedimagesarepivotalforsubsequentstagesof3Dobject
reconstruction to generate a mesh. Early efforts in this domain faced hurdles, particularly with
small-scale training data and the imperative to ensure generalization performance [19–24]. The
improvement journey began with Zero-1-to-3 [6] refining Stable Diffusion [1] with the extrinsic
cameraparameters,markingasignificantstepingeneralizedmulti-viewdiffusion. However,geo-
metricconsistencyremainedachallenge. SyncDreamer[7]builtuponZero-1-to-3,introducinga
3D-awarefeatureattentionmechanismforenhancedsynchronization,yielding16highlycoherent
multi-viewimages. Recentlargemodelspreferusingfeweroverlappingcanonicalviews(e.g.,front,
back,left,right)asinputs. Thistrendhasledtotheemergenceoffixed-camera-parametermulti-view
diffusion,simplifyingtrainingandenhancingmulti-viewconsistency. Forexample,MVDream[13]
andImageDream[14]efficientlygeneratethesefourviews,whilezero123++[16]extendsthistosix
fixedviews. Tailor3Dimprovespracticalutilitybygeneratingonlythebackimagefromthefront,
effectivelyaddressingimperfectconsistenciesindiverseinputscenarios.
Large Model for 3D Reconstruction and Generation. Early 3D generation methods initially
focusedonoptimizingindividualobjectsseparately. SDS-basedapproaches[4,3,25–30,5,31,32]
utilized multi-view images from Zero-1-to-3 for this purpose. Subsequently, Diffusion + Recon-
structionmethods[33–36]expandedonSyncDreamertooptimizehigher-consistencymulti-view
images. WiththeLargeReconstructionModel(LRM)scalingupindataandmodelsize,itrapidly
generateshigh-qualityNeRFfromsingleimagesinunder5s. Thisledtoashiftwhere2Dmethods
handledgenerationtasks,andLRMmanaged3Dreconstruction. Consequently,3Dstablediffusion
methodswithfewerviews,likeMVDream[13],becamepreferred. Forinstance,Instant3D[11]uses
2Dstablediffusionforfour-viewgenerationfollowedbyLRM-likereconstruction. Similarly,LGM
[12]andGRM[37]useGaussianSplattingforreconstruction. Forextensive3Dediting,wereduce
perspectivestofrontandback,requiringlowerconsistency.
3DObjectEditing. In3Dobjectdomain,"customizedediting"involvesshapealterations,pattern
addition,andtextureapplicationunderusercontrol. Traditionalmethodsincludeexplicitgeometric
representationediting,suchasmeshdeformation[38–40],proxy-drivendeformation[41–46],and
data-drivendeformation[47,46],whichutilizepriorshapesforrealisticoutcomes. Overtime,editing
hasmovedtowardsimplicitradiancefields[48–50],especiallyonNeRFs[51–53]. Earlierworks
focusedonspecificobjectsorscenes,lackinggeneralization[54].Inthe3D-AIGCera,3Deditinghas
evolvedtowards2Dimageediting,reconstructedtogeneratenew3Dobjects[55,56]. MVEdit[56]
denoisesmulti-viewimagesandoutputshigh-qualitytexturedmeshes. However,itsinferenceprocess
takes2-5minutes,lackingreal-timeediting. Incontrast,Tailor3Dusesdual-sideLRMtoprocess
inputsfrombothobjectsides,completingeacheditingstepwithinseconds,enablinginteractive3D
objectediting.
3Step 1: FV and BV Image Encoder FV Triplane Embeds
BV Triplane
FV RGB FV Image Embeds
Image Feature
Learnable
Modulation
Frozen
FV = Front-View Image LoR A Cross-Attention
BV = Back-View Encoder
Forward firstly Modulation
Forward secondly
BV RGB BV Image
Image Feature LoRA Self-Attention
Step 3: Fuse Double Side Feature z Modulation
z y MLP
x
y FV Tri-Feature z y
z
x Rotate
Fused Tri-Feature y x
NeRF &
NVS x
BV Tri-Feature Reversed BV Tri-Feature
Figure2: ModelArchitectureofDual-sidedLRM.Westartwithfrontandbackviewimages. Then,
usingLoRATriplaneTransformer,weobtainfrontandbacktriplanes. Finally,we‘tailor’thetwo
triplanefeaturesthroughrotationandViewpointCross-Attentiontoobtainthe3Dobject.
3 Methodology
Inthissection,wepresentthepipelineandmodelarchitectureofTailor3D.Firstly,weintroducethe
LargeReconstructionModel(LRM)andmulti-viewdiffusioninSection3.1. Next,inSection3.2,
weoutlineTailor3D’sprocess,illustrating2Deditingandrapidreconstructioninto3Dobjects. In
Section3.3,wedelveintotheDual-sidedLRM,accommodatinginputsfromimperfectconsistent
frontandbackviews. WeexplainhowtheLoRATriplaneTransformerreducesmemoryusageand
ViewpointCross-Attentiontofuse3DTriplanesfromfrontandbackviews.
3.1 Preliminaries
LargeReconstructionModel(LRM). LRMenablesdirectsingle-viewto3Dreconstruction. The
inputimageI isencodedbyanimageencoder,producingpatch-wisefeaturetokensF ∈RN×dE,
whereN isthenumberofimagefeaturepatchesandd isthedimensionoftheimageencoder. Initial
E
learnablepositionalembeddingsforthetriplanearedefinedasfinit andengageincross-attention
withtheimagefeaturesF. TheyaremodulatedbythecorrespondingcameraextrinsicparametersE
togeneratethetriplanefeaturemapT.
T =(T xy,T yz,T xz)= TRI-FORMER(finit, F, E). (1)
Here, finit ∈ (3×32×32)×d , where d is the hidden dimension of the transformer decoder.
D D
TRI-FORMER incorporates self-attention, cross-attention, and modulation. The resultant triplane
feature map T ∈ (3×64×64)×d comprises three planes: T , T , and T . Resolution
T XY YZ XZ
increasesfrom32×32to64×64viadeconvolutionallayers. Finally,itundergoesMLPnerf forcolor
anddensityderivationinNeRFrendering.
2DandMulti-viewDiffusion. Thediffusionmodeliterativelydenoisespurenoisex ∼N(0,I)
T
overT stepstoyieldcleandatax ,optimizingtowardsthegradientdirectionofthelogprobability
0
distributionofthedata,∇ logp(x ). Atstept,giventhenoisyinputx ,aneuralnetworkϵ with
xt t t ϕ
parametersϕpredictsthenoiseϵ.
L (ϕ,x)=E [∥ϵ (x ,t)−ϵ∥2]. (2)
diff t,ϵ ϕ t 2
Multi-viewdiffusiongeneratesimagesfromspecificobjectsbasedoncurrentanddesiredviewpoints.
ByprovidingcurrentimageI,extrinsiccameraparametersE ∈4×4,alongsidedesiredparameters
4
PLM
FReN&
vnoceD
tniopweiV
noitnettA-ssorC enalpirT
aroL
:2
petS
debmE
aremaC
VF
remrofsnarTTriplane
Embeddings Image Feature Triplane Embeddings Linear Q LoRA
Linear K LoRA
Linear V LoRA
Linear Q Linear K Linear V Linear input
Linear Input LoRA
Input (Concat Q, K, V)
Q K V Linear Outpur LoRA
Q K V
Matmul + Softmax
Matmul + Softmax
Matmul + Softmax
Matmul + Softmax
Linear Output
Linear Output
Output Output
(a) LoRA Cross-Attention (b) LoRA Self-Attention (c) LoRA details
in Tri-Former in Tri-Former in Tri-Former
Figure3: LoRATriplaneTransformer. (a)ForCross-Attention, weusetheLoRAstructureto
replacetheconnectionlayersofqkvandoutput. (b)ForSelf-Attention,wereplacetheconnection
layersofinputandoutput. DetailsoftheLoRAareshownin(c).
cameraE ,multi-viewdiffusiongeneratestheimageI forthedesiredviewpoint. Inourpipeline,
o o
weutilizemulti-viewdiffusiontogeneratethebackimagebasedonthefront.
3.2 ThePipelineofTailor3D
ThissectionoutlinesTailor3D’spipeline,asshowninthelowerpartofFigure1. Itbeginswitha
front-facingimageI ofanobject. Initially,imageeditingandstyletransferareappliedtocreateI′.
f f
Next,multi-viewdiffusionmethodslikeZero-1-to-3[6]generatethecorrespondingbackimageI ,
b
whichistheneditedtogetI′. Finally,bothI′ andI′ areinputintoDual-sidedLRMtoobtainthe
b f b
final3Dobject. Tailor3Doffersvariouschoicesandpotentialvariations. OriginalimagesI andI
f b
canbedirectlyinputintoDual-sidedLRMforrapidreconstructionofthe3Dobject. Additionally,
thebackimageI canbegeneratednotonlythroughZero-1-to-3butalsothroughphotographyor
b
directprovision. Wewillfurtherelaborateondownstreamtasksintheexperimentalsection. The
flexibilityofTailor3Darisesfromimprovedchoicesateachstepandtherobustnessofourmodel,
Dual-sidedLRM,inhandlingimperfectconsistencybetweenfrontandbackimageinputs.
3.3 Dual-sidedLRM:HowtoAcceptImperfectConsistentViews
InSection3.2,ourfocusisonacquiringtheeditedfrontimageI′ andbackimageI′ foranobject.
f b
However,theseimagesmayexhibitimperfectconsistency: Theymightnotdirectlyfacetheobject,
and their relationship can vary. Therefore, we need a reconstruction model capable of handling
imperfectlyconsistentinputimagesfrombothviewstogenerate3Dobjects. Weselecttwoviews
insteadoffourtoreduceinconsistencypressureoneditingandreconstruction. Weexplicitlymerge
twotriplanefeaturesinthe3Ddomain,aimingtoresolvetheinconsistencyissueintuitively.
LoRATriplaneTransformer. Whenemployingpre-trainedLRMparameters[10],ourgoalisto
minimizememoryusage. InLRM,thesingleviewfeatureF′ isprocessedbyatriplanetransformer
f
servingasadecodertogeneratetriplaneNeRFfeaturesT . Thiscomponentfacilitatesmapping
f
fromasingleviewto3D,enablingthemodeltounderstanddiverseobjectshapesandinferobject
informationeffectively. Tominimizememoryusage,weintegratetheLoRAstructureintothetriplane
transformer, as depicted in Figure 3. For self-attention, where qkv is generated by shared linear
layers,wereplaceallinputandoutputlinearlayerswithLoRAstructures[15]. Forcross-attention,
whereqkvisgeneratedbydifferentlinearlayers,wereplaceallqkvandoutputlinearlayerswith
LoRAstructures. Specificdetailsareasfollows:
hi =Wix+∆Wi x=Wix+Bi Ai x. (3)
0 tp 0 tp tp
Here,idenotesthei-thTransformerlayer. Forself-attention,tprepresentsthelinearprojectionfor
inputandoutput. Forcross-attention,tpdenotesthelinearprojectionsforq,k,v,andoutput.
5AsshowninFigure2,LRMgeneratesthetriplanefeatureT forthefrontviewfromfeaturesF′
f f
andcameraparametersE . Similarly,forthebackviewfeaturesF′,weusethecameraparameters
f b
E ofthefrontviewtoobtainthetriplanefeatureTf forthebackviewthroughtheLoRAtriplane
f b
transformer,asexpressedbythefollowingequation:
T f/Tf
b
= TRI-FORMERLoRA(finit, F′ f/F′ b, E f). (4)
HereTf,thetriplanefeatureforthebackviewobtainedusingthefrontview’scameraparameters,
b
cannotbedirectlymergedwithT . Wewilladdressthisandtheinconsistencybetweenthefrontand
f
backviewanglesinthenextsection.
FuseDoubleSideFeature. TomergethetwotriplanefeaturesT andTf,wefirsthorizontally
f b
flip Tf by 180 degrees around the z-axis to obtain T . Due to inconsistency between the front
b b
andbackviews,directalignmentoradditionofthetriplanefeaturesisn’tfeasible. Leveragingthe
triplane representation, we apply Viewpoint Cross-Attention to each plane individually. We use
T asthequeryandT asthekeyandvaluetoincorporatemissinginformationfromthebackside.
f b
Weadoptawindow-basedattentionstructure,withawindowsizesetto7,significantlyreducing
memoryconsumption. ThisyieldsthefinalT ,encapsulatinginformationfrombothviews. Data
fb
augmentationfurtherbolstersrobustnesstoinconsistency,withbackviewimagesundergoingscaling,
rotation,andtranslation,eachwitha10%probability.
Finally,theTriplane-NeRFformulationutilizesMLPnerf toderiveNeRFcoloranddensityparame-
tersforvolumerendering. SupervisionincludesV views,comprisingthefront,backand(V −2)
randomlychosensideviews. Foraspecificviewv,thelossfunctionforsynthesizingtheprediction
xˆ andthegroundtruthxGT fornewviewcompositionisformulatedasfollows:
v v
L(x)=
1(cid:88)V (cid:16)
λ L (xˆ ,xGT)+λ L (xˆ ,xGT)+λ L (xˆ
,xGT)(cid:17)
. (5)
V 1 MSE v v 2 LPIPS v v 3 TV v v
v=1
L denotesthenormalizedpixel-wiseL2loss,L isperceptualimagepatchsimilarity. L
MSE LPIPS TV
isthetotalvariationlosstopreventnoiseintheimage. Weightcoefficientsλ ,λ ,λ areapplied.
1 2 3
4 Experiments
Thissectionexplorestheexperimentalaspects. WebeginwithinsightsintotheGobjaverse-LVIS[57,
58]datasetinSection4.1. InSection4.2,wedelveintovariousimplementationdetails,including
modelarchitectureparameters,cameraadjustments,andtraining/testingprocesses. InSection4.3,we
presentexperimentalresults. WeshowcaseTailor3D’sversatilityacrossdifferenttasksandconduct
ablationstudiesonkeymodulesliketheLoRATriplaneTransformerandfusiontechniques.
4.1 Dataset: Gobjaverse-LVIS
LRMpre-trainedweights[10,59]aretrainedontheObjaverse[9]andMVImgNet[60]datasets,
containing 730K objects, normalized to a cube of size [−1,1]3 and rendered from 32 random
viewpointsataresolutionof512×512pixels. Forfine-tuning,theGobjaverse-LVIS[57,58]dataset
comprises22Khigh-quality3Drenderedobjects,selectedfromG-bufferObjaverseandLVISdatasets.
Gobjaverseincludes280K3Dobjectscapturedfromvariousviewpoints. Duringtraining,matching
frontandbackviewswithidenticalelevationareused. Renderingsupervisionincludesfixedfront
andbackviewpoints,alongwith(V −2)randomlyselectedsideviewsfornewviewsynthesis. The
combinedGobjaverse-LVISdatasetconsistsof22Kobjects,ensuringhigherquality.
4.2 ImplementationDetails
Weusethenetworkarchitecturefromthepre-trainedLRMmodel. Theimageencoderisbasedon
DINOv2’sViT-B/16model[61],operatingataresolutionof384×384. Theimagefeatureshavea
dimensionalityof768. Thetriplanetransformerdecoderconsistsof16layerswith16transformer
heads,featuringpositionalembeddingsofdimensionality1024andtriplaneswithdimensionality
80. MLPnerf comprises10layers. WesettheLoRArankto4fortheLoRATriplaneTransformer.
Duringneuralrendering,wesample128pointsalongeachrayandproduceimagesataresolution
6of128×128. Forcameranormalization,wealignwithLRMstandards,positioningthecameraat
[0,−2,0]relativetotheobjectcenter. Thisensurestheobject’sz-axisisupward,andthefrontview
corresponds to the negative y-axis. External rendering parameters are normalized relative to the
referenceview. Wetrainfor10epochson8A100GPUswithabatchsizeof16, takingabout6
hours. The loss function coefficients are λ = λ = λ = 1.0. We use the AdamW optimizer
1 2 3
withalearningrateof3×10−4 andacosineschedule. Duringinference,wequeryaresolutionof
384×384×384pointsfromthereconstructedtriplane-NeRF,completingitinlessthan5seconds.
4.3 ExperimentResults
InSection4.3.1,weshowcasedTailor3D’scapabilitiesin3Dgeneration,coveringgeometricobject
fill,texturesynthesis,andstyletransfer. InSection4.3.2,wecomparedourapproachwithexisting
techniques. InSection4.3.3,weperformedablationexperimentstovalidateeachmoduleofTailor3D.
4.3.1 Tailor3DApplications
Weshowcaseitsversatilityin3DGenerativeGeometry/PatternFill,encompassinglocalgeometric
shapeandtexturepatternfilling. Wehighlightitsstyletransferandfusioncapabilities,allowingfor
operationslikestyletransferandblendingtwostylesontooneobject. Tailor3Denablesuserstoedit
boththefrontandbackofobjects,expandingeditingpossibilitiesforcustomized3Dobjects.
3D Generative Geometry / Pattern Fill. Here, we showcase Tailor3D’s local 3D object filling
ability,asdepictedinFigure4. Demonstratingstep-by-stepobjectfillingandeditingthroughtext
orimageprompts. InRow2,startingfromarmor,wegenerateamedievalgeneralbyaddingthe
head,hands,andcloakprogressively. Row3illustratesadditionalobjectmanipulation,includingthe
additionofamailbox,balloons,aflowerbush,andabasketballhoop.
3DStyleTransferandFusion. Tailor3Dalsodemonstratesitstransferandfusioncapabilitiesfor
variousstyles. Unlikepreviousapproaches,Tailor3DensuresIPintegritywhileofferingflexibilityin
specifyingstylesthroughimagesortextguidance. Notably,itleveragesMidjourneyfor2Dimage
generationandediting. Additionally,Tailor3Denablestheinfusionofdifferentstylesontoboththe
frontandbackofobjects,showcasingtheeffectivenessoftheDual-sidedLRM’smergingability.
4.3.2 ComparetoExisting3DImage-to-3DGenerationMethods
WecompareourapproachwithWonder3D[36],TriplaneGaussian[62],andLGM[12]onatestsetof
100imagesgeneratedbystablediffusion[1]. QualitativeresultsinFigure5demonstrateTailor3D’s
capabilitytoenhancebacksideinformationwithDual-sidedLRM.Wonder3DandTriplaneGaussian
strugglewithcomplexobjects,exhibitingloweroverallquality. LGM,usingGaussianrepresentation,
suffersfromghostingeffectsandlacksdetailinfeaturesliketreeleaves. Quantitativeresultsare
providedinTable1alongsidegenerationtimes,highlightingthepracticalvalueofourmethod.
4.3.3 AblationStudy
WeperformanablationstudyontheDual-sidedLRM,focusingonthreeaspects: thefusionof3D
featuresfrombothsides,therankoftheLoRATransformer,andtheextrinsiccameraparametersof
frontandbackimages. ResultsarepresentedinTable2,usingthesametestsetasinSection4.3.2.
TheWaytoFuseDoubleSideFeature. WeusetheViewpointCross-Attentiontofusefeaturesfrom
boththefrontandbacksides. Additionally,weexperimentwithmultiplelayersof2Dconvolutional
layers and direct addition to merge Triplane features from both sides. Our results indicate that
employingtheViewpointCross-Attentionproducesthebestresults.
TheRankofLoRATriplaneTransformer. Weconductablationexperimentsontherankofthe
LoRATriplaneTransformer,settingtherankto2,4,and8,respectively. Ourexperimentalresults
indicatethatarankof4achievesthebestperformance.
ExtrinsicCameraParameters. WeapplythesamefrontcameraparametersE tobothfrontand
f
back images, rotating only the back triplane. Additionally, we experiment with separate camera
parametersforfrontandbackimages,withoutrotation,byutilizingboththefrontandbackcamera
extrinsics,denotedasE andE ,respectively. Theresultssuggestthatusingfrontextrinsicsalone
f b
yieldsaccurateoutcomes,astheLRMstructuresolelyacceptsfrontcameraparameters.
73D Generative Geometry / Pattern Fill
Front and Step 1 Step 2 Step 3 Step 4
Back Images
+ blue
+love heart diamond + bouquet
+ bow tie +
+ head + hand + red cloak
+ leg
+
+ flower
+ balcony + baloon bush
+ mailbox +basketball
hoop
3D Style Transfer and Fusion
Front and Back
Images or Text Style 1 Style 2 Style 3 Style 4
Original Image Original Image Original Image Original Image
LEGO Original Text Original Text Original Text Original Text
Model + astronaut so+ r co el rd e r + cl og tr he ee sn + red ninja
Original Text Original Text Original Text Original Text
+ r we hd i ta end + + +
Fusion
Figure4: 3DGenerativeFilland3DStyleTransfer. ItincludesbothGeometryFillandPatternFill,
allowingustoaddormodifylocalgeometricstructuresortexturepatternsof3Dobjects. Guidance
canbeprovidedthroughtextorimagesasprompts. Additionally,weofferstyleimagesortextual
guidancetotransform3Dobjectsintodesiredstyles. EnsuringthemaintenanceofIPintegrityduring
disguiseaddssignificantpracticalvalueto3Dtasks.
Inourablationstudy,wefoundthatViewpointCross-Attentionismoreeffectivethanconvolutional
networksformerging3Dfeatures. Arankof4fortheLoRATriplaneTransformeryieldsoptimal
results,whiletheLRMframeworkonlyacceptsfront-facingcameraparameters.
5 LimitationandConclusion
In this paper, we introduce Tailor3D, which swiftly creates customized 3D assets using editable
dual-sideimages,akintoatailor’sapproach. Byleveraging2Dimageeditingtechniquesandrapid
3D reconstruction, Tailor3D allows users to iteratively refine objects. Our Dual-sided LRM and
LoRATriplaneTransformeractas’tailors,’seamlesslystitchingtogetherfrontandbackviewsto
8Images Input Wonder3D TriplaneGaussian LGM Tailor3D (Ours)
Figure5: ComparetoExisting3DGeneration. Wecomparesingleimage-to-3Dmethods. Won-
der3DandTriplaneGaussianhavelowerresolutions,whileLGMoftenshowsghostingeffectswith
complextextures. Ourmethod,however,achievessuperiorexperimentalresults.
Comparewithothers. CommonMetrics UserStudy↑(0to100score)
Methods InF.Time. LPIPS↓ SSIM↑ PSNR↑ Geometry Texture Overall
TriplaneGaussian[62] 20s 0.2811 0.5635 14.89 56.3 54.5 62.3
Wonder3D[36] 3min 0.2709 0.6485 16.23 73.3 76.3 79.2
LGM[12] 5s 0.2473 0.8423 19.02 79.3 85.2 83.2
Tailor3D(Ours) 5s 0.2345 0.8525 19.34 82.3 84.2 86.3
Table1: ComparisonwithExisting3DGenerationMethods. Wecomparesingleimage-to-3D
methods,includingcommonmetricsanduserstudies. Resultsindicatethatoursoutperformsothers.
(a)WaytoFuseDoubleSides. (b)LoRATransformerRank. (c)TwoCameraExtrinsics.
FuseWay Score SSIM↑ LPIPS↓ Rank Score SSIM↑ LPIPS↓ CamExt.∗ Score SSIM↑ LPIPS↓
Add 76.3 0.7377 0.2938 2 79.2 0.7623 0.2877 E b+E b 60.5 0.6288 0.3944
Conv2D 84.2 0.8239 0.2443 4 86.3 0.8525 0.2345 E f +E b 33.4 0.3523 0.5653
VP-CA† 86.3 0.8525 0.2345 8 82.2 0.7902 0.2535 E f +E f 86.3 0.8525 0.2345
Table2: AbalationStudy. Weconductedablationregardingthefusionmethodforbothsides,the
rank of the LoRA Triplane Transformer, and the extrinsic camera parameters. †: VP-CA means
ViewpointCross-Attention. ∗:Thefirstisthefront-viewextrinsicandthesecondisforthebackview.
handleinconsistenciesandimprovereconstructionquality. ExperimentalresultsvalidateTailor3D’s
effectivenessintaskslike3Dgenerativefillandstylecustomization. Itoffersauser-friendly,cost-
efficient solution for rapid 3D editing, applicable in animation, game development, and beyond,
streamliningproductionanddemocratizingcontentcreation.
Limitation and Future Direction. However, relying solely on front and back views for object
reconstruction may encounter challenges with objects of certain thicknesses. Additionally, the
generated3Dobjectmeshesmayhavelowerresolutions,andtheadditionofgeometricfeaturesmay
notsignificantlyalterthemesh. Wewillfurtherinvestigatemethodstoaddressthegenerationand
reconstructionofobjectswiththickersideprofilesinfuturework,aimingtoenhancethequalityand
resolutionofthemeshes.
9References
[1] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022. 2,3,7
[2] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InICCV,2023. 2
[3] JialeXu,XintaoWang,WeihaoCheng,Yan-PeiCao,YingShan,XiaohuQie,andShenghuaGao.Dream3d:
Zero-shottext-to-3dsynthesisusing3dshapepriorandtext-to-imagediffusionmodels. InCVPR,2023. 2,
3
[4] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
InICLR,2023. 3
[5] ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJunZhu. Prolificdreamer:
High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation. InNeurIPS,2023. 2,3
[6] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.
Zero-1-to-3:Zero-shotoneimageto3dobject. InCVPR,2024. 2,3,5
[7] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.
Syncdreamer:Generatingmultiview-consistentimagesfromasingle-viewimage. InICLR,2024. 3,20
[8] ZeyiSun,TongWu,PanZhang,YuhangZang,XiaoyiDong,YuanjunXiong,DahuaLin,andJiaqiWang.
Bootstrap3d:Improving3dcontentcreationwithsyntheticdata. arxiv:2406.00093,2024. 2
[9] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,LudwigSchmidt,
KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse:Auniverseofannotated3dobjects. In
CVPR,2023. 2,6,19
[10] YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,DifanLiu,FengLiu,KalyanSunkavalli,Trung
Bui,andHaoTan. Lrm:Largereconstructionmodelforsingleimageto3d. InICLR,2024. 2,5,6,14
[11] JiahaoLi,HaoTan,KaiZhang,ZexiangXu,FujunLuan,YinghaoXu,YicongHong,KalyanSunkavalli,
Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large
reconstructionmodel. InICLR,2024. 2,3,14,15
[12] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. Lgm: Large
multi-viewgaussianmodelforhigh-resolution3dcontentcreation. arXiv:2402.05054,2024. 2,3,7,9,14,
16,19
[13] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view
diffusionfor3dgeneration. arXiv:2308.16512,2023. 2,3,16
[14] PengWangandYichunShi. Imagedream:Image-promptmulti-viewdiffusionfor3dgeneration. arxiv,
2023. 2,3
[15] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InICLR,2022. 2,5
[16] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,LinghaoChen,Chong
Zeng,andHaoSu. Zero123++:asingleimagetoconsistentmulti-viewdiffusionbasemodel. arxiv,2023.
3
[17] XinKong,ShikunLiu,XiaoyangLyu,MarwanTaher,XiaojuanQi,andAndrewJDavison. Eschernet:A
generativemodelforscalableviewsynthesis. InCVPR,2024. 3,19
[18] ShitaoTang,JiachengChen,DilinWang,ChengzhouTang,FuyangZhang,YuchenFan,VikasChandra,
YasutakaFurukawa,andRakeshRanjan. Mvdiffusion++:Adensehigh-resolutionmulti-viewdiffusion
modelforsingleorsparse-view3dobjectreconstruction. arXiv:2402.12712,2024. 3
[19] DanielWatson,WilliamChan,RicardoMartin-Brualla,JonathanHo,AndreaTagliasacchi,andMohammad
Norouzi. Novelviewsynthesiswithdiffusionmodels. InICLR,2023. 3
[20] ZhizhuoZhouandShubhamTulsiani. Sparsefusion:Distillingview-conditioneddiffusionfor3drecon-
struction. InCVPR,2023.
10[21] EricR.Chan,KokiNagano,MatthewA.Chan,AlexanderW.Bergman,JeongJoonPark,AxelLevy,
MiikaAittala,ShaliniDeMello,TeroKarras,andGordonWetzstein. GeNVS:Generativenovelview
synthesiswith3D-awarediffusionmodels. InCVPR,2023.
[22] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion: (0-)image-
conditioned3dgenerativemodelsfrom2ddata. InCVPR,2023.
[23] TongWu,GuandaoYang,ZhibingLi,KaiZhang,ZiweiLiu,LeonidasGuibas,DahuaLin,andGordon
Wetzstein. Gpt-4v(ision)isahuman-alignedevaluatorfortext-to-3dgeneration. InCVPR,2024.
[24] YeFang,ZeyiSun,TongWu,JiaqiWang,ZiweiLiu,GordonWetzstein,andDahuaLin. Make-it-real:
Unleashinglargemultimodalmodelforpainting3dobjectswithrealisticmaterials. arXiv:2404.16829,
2024. 3
[25] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,KarstenKreis,
SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3d:High-resolutiontext-to-3dcontentcreation. In
CVPR,2023. 3
[26] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,andAndreaVedaldi. Realfusion:360degreconstruc-
tionofanyobjectfromasingleimage. InCVPR,2023.
[27] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian
chaining:Liftingpretrained2ddiffusionmodelsfor3dgeneration. InCVPR,2023.
[28] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,NatanielRuiz,BenMildenhall,ShiranZada,
KfirAberman,MichaelRubinstein,JonathanBarron,etal. Dreambooth3d: Subject-driventext-to-3d
generation. InCVPR,2023.
[29] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3d:Disentanglinggeometryandappearance
forhigh-qualitytext-to-3dcontentcreation. InCVPR,2023.
[30] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi,LizhuangMa,andDongChen. Make-it-3d:
High-fidelity3dcreationfromasingleimagewithdiffusionprior. InCVPR,2023. 3
[31] JosephZhuandPeiyeZhuang. Hifa:High-fidelitytext-to-3dwithadvanceddiffusionguidance. InICLR,
2024. 3
[32] YixunLiang,XinYang,JiantaoLin,HaodongLi,XiaogangXu,andYingcongChen. Luciddreamer:
Towardshigh-fidelitytext-to-3dgenerationviaintervalscorematching. arXiv:2311.11284,2023. 3
[33] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,MukundVarmaT,ZexiangXu,andHaoSu.One-2-3-45:
Anysingleimageto3dmeshin45secondswithoutper-shapeoptimization. InNeurIPS,2024. 3,14
[34] MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei,HanshengChen,Chong
Zeng,JiayuanGu,andHaoSu. One-2-3-45++:Fastsingleimageto3dobjectswithconsistentmulti-view
generationand3ddiffusion. arXiv:2311.07885,2023.
[35] ChengChen,XiaofengYang,FanYang,ChengzengFeng,ZhoujieFu,Chuan-ShengFoo,GuoshengLin,
andFayaoLiu. Sculpt3d: Multi-viewconsistenttext-to-3dgenerationwithsparse3dprior. InCVPR,
2024.
[36] XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,YuexinMa,Song-Hai
Zhang,MarcHabermann,ChristianTheobalt,etal. Wonder3d:Singleimageto3dusingcross-domain
diffusion. InCVPR,2024. 3,7,9,14,19
[37] YinghaoXu,ZifanShi,WangYifan,HanshengChen,CeyuanYang,SidaPeng,YujunShen,andGordon
Wetzstein. Grm: Largegaussianreconstructionmodelforefficient3dreconstructionandgeneration.
arXiv:2403.14621,2024. 3
[38] Yu-JieYuan,Yu-KunLai,TongWu,LinGao,andLigangLiu. Arevisitofshapeeditingtechniques:from
thegeometrictotheneuralviewpoint. JournalofComputerScienceandTechnology,2021. 3
[39] OlgaSorkine. Laplacianmeshprocessing. Eurographics(StateoftheArtReports),2005.
[40] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry
processing,2007. 3
[41] AlecJacobson,IlyaBaran,LadislavKavan,JovanPopovic´,andOlgaSorkine. Fastautomaticskinning
transformations. ACMTransactionsonGraphics(TOG),2012. 3
11[42] ThalmannMagnenat,RichardLaperrière,andDanielThalmann. Joint-dependentlocaldeformationsfor
handanimationandobjectgrasping. InProceedingsofGraphicsInterface’88,1988.
[43] ThomasWSederbergandScottRParry.Free-formdeformationofsolidgeometricmodels.InProceedings
ofthe13thannualconferenceonComputergraphicsandinteractivetechniques,1986.
[44] WangYifan,NoamAigerman,VladimirGKim,SiddharthaChaudhuri,andOlgaSorkine-Hornung.Neural
cagesfordetail-preserving3ddeformations. InCVPR,2020.
[45] RobertWSumner,MatthiasZwicker,CraigGotsman,andJovanPopovic´. Mesh-basedinversekinematics.
ACMtransactionsongraphics(TOG),2005.
[46] LinGao,Yu-KunLai,DunLiang,Shu-YuChen,andShihongXia. Efficientandflexibledeformation
representationfordata-drivensurfacemodeling. ACMTransactionsonGraphics(TOG),2016. 3
[47] LinGao,Yu-KunLai,JieYang,Ling-XiaoZhang,ShihongXia,andLeifKobbelt. Sparsedatadriven
meshdeformation. IEEEtransactionsonvisualizationandcomputergraphics,2019. 3
[48] LijuanLiu,YouyiZheng,DiTang,YiYuan,ChangjieFan,andKunZhou. Neuroskinning:Automaticskin
bindingforproductioncharacterswithdeepgraphnetworks. ACMTransactionsonGraphics(ToG),2019.
3
[49] QingyangTan,LinGao,Yu-KunLai,andShihongXia. Variationalautoencodersfordeforming3dmesh
models. InCVPR,2018.
[50] ZhanXu,YangZhou,EvangelosKalogerakis,ChrisLandreth,andKaranSingh. Rignet:Neuralrigging
forarticulatedcharacters. IEEETransactionsonVisualizationandComputerGraphics.,2021. 3
[51] StevenLiu,XiumingZhang,ZhoutongZhang,RichardZhang,Jun-YanZhu,andBryanRussell. Editing
conditionalradiancefields. InCVPR,2021. 3
[52] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and
ZhaopengCui. Learningobject-compositionalneuralradiancefieldforeditablescenerendering. InCVPR,
2021.
[53] Yu-JieYuan,Yang-TianSun,Yu-KunLai,YuewenMa,RongfeiJia,andLinGao. Nerf-editing:geometry
editingofneuralradiancefields. InCVPR,2022. 3
[54] ZhangyangQi,YeFang,ZeyiSun,XiaoyangWu,TongWu,JiaqiWang,DahuaLin,andHengshuang
Zhao. Gpt4point:Aunifiedframeworkforpoint-languageunderstandingandgeneration. InCVPR,2024.
3
[55] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei. Control3d:Towardscontrollabletext-to-3d
generation. InACMMultimedia,2023. 3
[56] HanshengChen,RuoxiShi,YulinLiu,BokuiShen,JiayuanGu,GordonWetzstein,HaoSu,andLeonidas
Guibas. Generic3ddiffusionadapterusingcontrolledmulti-viewediting. arXiv:2403.12032,2024. 3
[57] LingtengQiu,GuanyingChen,XiaodongGu,Qizuo,MutianXu,YushuangWu,WeihaoYuan,Zilong
Dong,LiefengBo,andXiaoguangHan. Richdreamer:Ageneralizablenormal-depthdiffusionmodelfor
detailrichnessintext-to-3d. arXiv:2311.16918,2023. 6,14,16
[58] AgrimGupta,PiotrDollár,andRossGirshick. Lvis:Adatasetforlargevocabularyinstancesegmentation.
InCVPR,2019. 6
[59] ZexinHeandTengfeiWang. Openlrm: Open-sourcelargereconstructionmodels. https://github.
com/3DTopia/OpenLRM,2023. 6,15,16
[60] XianggangYu,MutianXu,YidanZhang,HaolinLiu,ChongjieYe,YushuangWu,ZizhengYan,Chenming
Zhu,ZhangyangXiong,TianyouLiang,GuanyingChen,ShuguangCui,andXiaoguangHan. Mvimgnet:
Alarge-scaledatasetofmulti-viewimages. InCVPR,2023. 6
[61] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,RussellHowes,Po-YaoHuang,
Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas,
GabrielSynnaeve,IshanMisra,HerveJegou,JulienMairal,PatrickLabatut,ArmandJoulin,andPiotr
Bojanowski. Dinov2: Learningrobustvisualfeatureswithoutsupervision. TransactionsonMachine
LearningResearch(TMLR),2023. 6
12[62] Zi-XinZou,ZhipengYu,Yuan-ChenGuo,YangguangLi,DingLiang,Yan-PeiCao,andSong-HaiZhang.
Triplanemeetsgaussiansplatting:Fastandgeneralizablesingle-view3dreconstructionwithtransformers.
arXiv:2311.16918,2023. 7,9,14,19
[63] PengWang,HaoTan,SaiBi,YinghaoXu,FujunLuan,KalyanSunkavalli,WenpingWang,ZexiangXu,
andKaiZhang. Pf-lrm:Pose-freelargereconstructionmodelforjointposeandshapeprediction. InICLR,
2024. 14,15
[64] YinghaoXu,HaoTan,FujunLuan,SaiBi,PengWang,JiahaoLi,ZifanShi,KalyanSunkavalli,Gor-
donWetzstein, ZexiangXu, andKaiZhang. Dmv3d: Denoisingmulti-viewdiffusionusing3dlarge
reconstructionmodel. InICLR,2024. 14,15
[65] YunhanYang,YukunHuang,XiaoyangWu,Yuan-ChenGuo,Song-HaiZhang,HengshuangZhao,Tong
He,andXihuiLiu. Dreamcomposer: Controllable3dobjectgenerationviamulti-viewconditions. In
CVPR,2024. 19
[66] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
ThomasB.McHugh, andVincentVanhoucke. Googlescannedobjects: Ahigh-qualitydatasetof3d
scannedhouseholditems. arXiv:2204.11918,2022. 19
13A AdditionalIntroduction
WefirstintroduceadditionalbackgroundinformationinthesupplementarymaterialsinAppendixB.
Wefirstdivided3DReconstructionintothreecategoriesandintroducedtheLRM[10]family. In
Appendix C, we presented additional details regarding the methodology and implementation of
experiments. WeemphasizethedifferencesbetweenourtrainingconfigurationandtheoriginalLRM
andprovidefurtherinsightsintotheGobjaverse[57]dataset. InAppendixD,weprimarilyshowcase
theadditionalexperimentalcontentwehavesupplemented. Wefirstpresentadditionalexamplesof
Tailor,followedbycomparisonswithmoremulti-viewreconstructionmethods. InAppendixE,we
discussthebroadersocialimpactofourprocess.
B AdditionalRelatedWork
Thissectioncategorizes3Dreconstructionintosingle-viewreconstruction,multi-viewreconstruction,
andtherecentlypopularnormal-viewreconstruction. Wethendelveintothebenefitsofemploying
double-sidedinformationforcanonical-viewreconstructioninappendixB.1. Followingthat, we
introducearticlesfromtheLRMfamily[10,11,63,64]inappendixB.2,discussingvariousvariants
ofthisuniversalreconstructionframework.
B.1 Single,MultiandCanonical-viewReconstruction
Firstly,wedelineateseveraltypesofreconstruction. Single-viewreconstructioninvolvesgenerating
a 3D mesh of an object from a single viewpoint image (typically the front view). On the other
hand, multi-view reconstruction typically involves multiple viewpoint images of an object along
with corresponding camera extrinsic (often 20-100 views), aiming to reconstruct a 3D object. A
landmark method in this domain is NeRF, which utilizes MLPs for novel view synthesis or 3D
reconstruction. However, NeRF-based methods suffer from the need for individual optimization
for each object, resulting in long reconstruction times, sometimes reaching 1-2 hours. Early 3D
generationmethodswhichusemulti-viewdiffusionforgeneratingmultipleviewsofanobjectand
subsequentreconstruction[33,36],alsofacelongreconstructiontimes.
ThedevelopmenttrajectoryofNeRFinvolvestheneedforincreasinglyfewerviewpointsforrecon-
struction,fewercameraparameters,andfasterreconstructionspeeds. However,thesemethodsstill
requireindividualoptimizationforeachobject. Incontrast,LRMservesasauniversalreconstruction
model. As the model and dataset sizes reach a particular scale, reconstruction models become
universal,eliminatingtheneedforindividualoptimizationofobjectstobereconstructed. Within
thisuniversalframeworkemergesareconstructionmethodknownascanonical-viewreconstruction,
which uses fixed faces for reconstruction, typically the front, back, left, and right faces, referred
toas4-canonical-viewreconstruction. Instant3D[11], TriplaneGaussian[62], andLGM[12]all
employthisreconstructionmethod. However,thechallengewithusingthefront,back,left,andright
facesliesineffectiveediting,asitisdifficulttoeditallfourfacessimultaneously. Tailor3Dadopts
Dual-Canonical-viewReconstruction,utilizingonlythefrontandbackfaceswithfeweroverlaps,
facilitatinguserediting. Here,weemphasizethatmulti-viewreconstructionrequiresoptimization
forindividualobjects,whereascanonical-viewreconstructionisbuiltuponageneralreconstruction
framework.
B.2 IntroductiontoLRMFamily
Asmentionedearlier,early3Dgenerationmethodsutilizedmulti-viewdiffusiontogenerateadditional
viewpointsfromasingleimageandoptimizedthemulti-viewreconstructionofa3Dobjectbased
ontheseviewswhichneedseveralminutes. TheLRMfamily,servingasaseriesofFeed-Forward
Methods,directlygenerates3Dmesheswithouttheneedforsynthesizingmultipleviewpointimages
ortrainingandadaptingtomodelslikeNeRFwithinonlyseveralseconds. Itrepresentsauniversal
reconstructionframework. AsillustratedinFigure6,LRMisauniversalframeworkforsingle-view
reconstruction. Thatis,asingleimagecandirectlygeneratea3Dmesh. Thefundamentalconcept
involvespredefiningthefeaturemapofTriplaneNeRFandthenperformingcross-attentionwith2D
imagesandtheircorrespondingcameraparameters. Theresultingfeaturemapcandirectlyprovide
novelviewsofimagesoreventheentire3DmeshintheformatofTriplaneNeRF.
14FV Triplane
z Feature
Self- MLP
Attention
y
FV Image FV Image x
Feature
Mod AC ttr eo ns ts io- n Mod AtS tee nlf t- ion Mod MLP
FV Position Embeds FV Camera Embed
FV Camera Embed
Mod
Self- FV Image Feature
MLP
Attention
Concat
ModBV Camera
Embed
Self- MLP BV Image Attention Feature Fused Triplane
Feature
FV & BV Image z
Cross- Self- MLP y
Fused Position Attention Attention
Embeds x
z Fused Triplane
Tailor3D: (From Single-view to Normal-view) Feature z
FV & BV
Image Feature y
y
x x
Self- MLP z y
Attention
Viewpoint x
Cross-Attention
FV & BV Image
Mod AC ttr eo ns ts io- n Mod AtS tee nlf t- ion Mod MLP
FV & BV Position Embeds FV Camera Embed
Figure6: ModelarchitecturesofLRM,Instant3DandTailor3D.
Buildinguponthisfoundation,Instant3D[11]addressesnormal4-canonical-viewreconstruction. It
involvestwostages: first,utilizinga2Ddiffusionmodeltoobtainfront,back,left,andrightimages
ofanobjectfromtextprompts; second,reconstructingthe3Dobjectfromthesefourviewpoints.
PF-LRM[63]focusesonpose-freesparsemulti-viewreconstruction,enablingthegenerationofa3D
objectfromthreeimagestakenfromarbitraryviewpointswithoutcorrespondingcameraextrinsics.
However,itsframeworkcomplexityarisesfromthesupervisioninvolvingPnPandvariousgeometric
theories. DMV3D[64],anextensionofInstant3D,introducesadenoisingprocess,resultingina
denoisedmulti-viewdiffusionframework. Unfortunately,thesemethodshavenotbeenopen-sourced
yet,withonlytheOpenLRM[59]codebaseprovidingtheinferencecodeforLRM.
LRMandInstant3Dcanberegardedasmethodscorrespondingtosingle-viewand4-canonical-view
reconstruction, respectively. However, theirhandlingofcameraparametersdiffers. Asshownin
fig.6,LRMadjustscameraparameterswithtriplanefeaturesinthetriplanetransformerdecoder. In
practice,theexternalcameraparametersarefixed,meaningthecameraispositionedat[0,−2m,0]
andorientedtolookdirectlyattheobjectalongthepositivey-axis. Hence,LRMcanonlyacceptthe
cameraparametersofthefrontview,asdemonstratedinTable1c. Incontrast,Instant3Dplacesthe
modulationofthecamerawithintheimageencoder. Afterobtainingimagefeaturesfromfourviews,
thesefeaturesareconcatenatedandpassedthroughthetriplanetransformerdecoder. Thisapproach
involvesmergingthefeaturesfrommultipleviewpointsatthe2Dimagefeaturelevel. However,this
15
weiv-elgniS(
:MRL
-lamroN(
:D3tnatsnI
)noitcurtsnoceR
)noitcurtsnoceR
weiv
egamI
desuF
erutaeF(a) Objaverse (a) Gobjaverse
Figure7: RenderingperspectivesinObjaverseandGobjaverse.
approachisnotanaturaltransitionfromsingle-viewtocanonical-viewreconstruction. Wechooseto
utilizeViewpointCross-Attentiontofusethe3Dtriplanefeaturesofthefrontandbackviews. This
allowsustoeasilyextendsingle-viewreconstructiontodual(4)-canonical-viewreconstructionusing
onlythepre-trainedweightsfromthesingle-viewreconstruction. Furthermore, onlytrainingthe
ViewpointCross-Attentionisnecessarytominimizecosts.
C AdditionalMethodology
Inthissection, wedelveintothetrainingandexperimentalaspects. InappendixC.1, weoutline
our training setup, leveraging the LRM model from the OpenLRM codebase [59], and delineate
thevariationsinparameterquantitiescomparedtotheoriginalLRM.InappendixC.2,weoffera
detailedoverviewoftheviewpointrenderingintheGobjaversedataset[57]utilizedinourstudy. We
achievedsatisfactoryresultswitharelativelysmalldatasetsizebyutilizingmeticulouslycrafted
artificialrenderingdataboastinghigh-qualitytexturesandexcellentconsistency(22K).
C.1 TrainingSettings
Here, we focus on describing our training details. First, we utilized the OpenLRM codebase as
the basis for our LRM implementation. The original resolution is 512, but we used 256. The
dimensionalityofthetriplanefeaturemap,whichwasinitially80,wasreducedto40. Othermodel
parametersremainunchanged,suchasthedimensionalityofcameraembeddings(1024)andtriplane
transformer(1024). Weused96renderingsamplerays. Fortrainingparameters,thelearningratewas
setto3e−4,withaweightdecayof0.05. Weemployedacosinescheduler. Thetotalbatchsizewas
setto16(across8A100GPUs),andwetrainedforatotalof20epochs.
C.2 Dataset: Gobjaverse
WeutilizedtheGobjaversedataset[57],anenhancedversionoftheObjaversedatasetwithhigher-
qualityrendering. UnlikeObjaverse,whichrendersasingleobjectwithrandomlypositionedcameras
spherically,Gobjaverseperformsorbitrenderingaroundanobject,capturingtwoorbitsshownin
Figure7. Inthehigher-elevationorbit,24viewsatequalintervalsarerepresentedincyan. Inthe
lower-elevationorbit,12viewsatequalintervalsarerepresentedinred. Additionally,twoviews
capturedfromthetopandbottomarerepresentedinpurple.
We excluded the two views captured from the top and bottom during our training process. This
allowedourtrainingdatatoprovideinputfromboththefrontandbacksidesoftheobjects. Itisworth
notingthattheoppositedirectionsareonlyalongthex-axisandy-axis. Inthez-axisdirection,they
havethesameelevationangleratherthanbeingutterlysymmetricacrossthecenter. Thisapproach
differsfrommethodslikeInstant3DandLGM[12],whichusetechniquessimilartoMVDream[13]
togenerate4viewsofanobjectusing2Ddiffusion. Gobjaverseoffershigherconsistency,resulting
inhigherdataquality,whichfacilitatesthefusionoffeaturesfromthefrontandbackdirections.
16A book with A teddy
a leather bear with a
cover red bow
A book left Four ripe
on a park apples in
bench a basket
A boat A chair
floating on made from
calm water polished
oak
A bicycle An origami
leaning crane made
against a wall from a map
A book
A mug filled
bound in
with steaming
mysterious
coffee
symbols
Three
A dog
vibrant
creating
balloons
sand art on
tied
a beach
together
A cat with An orange
two different tabby cat
colored eyes shaped cookie
jar
A quartet of
mugs that A thorny
sing in rose
harmony
Figure8: Testset: 1003DAssetsfromStableDiffusion(1).
C.3 Testset: 100ImagesfromStableDiffusion
Ourquantitativetestsetandaportionofthequalitativetestsetconsistof100objectsgeneratedby
StableDiffusion,withthebackgroundremoved. Here,wepresentpartialexamplesusingtwoimages,
whiletheremainingqualitativeexamplesmaycomefromtheuseofMidjourneyforgeneration. Our
testsetcoversvariousobjectsandmicro-scenessuchasanimals,humans,plants,andlandscapes,
enablingacomprehensiveassessmentofthequalityofthegenerationmodels. Additionally,allour
modelscomplywithcopyrightandrelatedregulations.
17Floating
bonsai tree,
A torn hat roots in
mid-air
A crying
A soft sofa
sofa
A brick A sleeping
house cat
A twisted A shouting
tower leaf
A rubbery A wooden
cactus bicycle
A rough rock A dancing
elephant
A pen leaking A pair of
blue ink worn-out
shoes
A lamp
casting a A rusty boat
warm glow
Figure9: Testset: 1003DAssetsfromStableDiffusion(2).
18Input Mesh Input Mesh
Figure10: ComparewithDreamcomposer. Here,wepresentacomparisonwiththemulti-view
DreamComposer[65]. Inthiscomparison,weprovideTailor3Dwithground-truthRGBimagesfor
thebackside. ItcanbeobservedthatTailor3Dexhibitsmoredetailedtexturefeaturesandavoids
defectssuchasholes.
Input Mesh Input Mesh Input Mesh
Front RGB & Back Condition Back RGB Ours (Mesh) EscherNet(Mesh)
Figure11: Comparewithhmulti-viewinputmodelEscherNet[17]. Ourcreatedmeshexcels
beyondothermethods,deliveringsuperiorspeedandquality.
D AdditionalExperiments
Inthissection,wesupplementourexperiments. InappendixD.1,wecompareourmethod’seffective-
nesswithmorerecentmulti-viewreconstructiontechniques. InappendixD.2,similartoFigure4,We
presentadditionalexamplesofTailor3D,showcasingourabilitytocustomizeandeditobjects.
D.1 ComparisonwithMoreMulti-viewReconstructions
Inthemainpaper,wecomparedearlier3DgenerationmethodslikeWonder3D[36],TriplaneGaus-
sian [62], and LGM [12], most of which were focused on image-to-3D generation. Conversely,
approacheslikeDreamcomposer[65]andEscherNet[17]aimedtocomplementadditionalview-
points. It’sworthnotingherethetestsetisfromGSO30[66]andObjaverse[9]datasetsinsteadof
the100SDtestsetusedinthemainpaper. DreamcomposerandEscherNetareoptimization-based
methods,thusrequiringseveralminutestogenerate3Dresults. Incontrast,Tailor3Donlyneeds5
secondstoproducesuperior3Dreconstructionresults.
19
MRL
nepO
ynnaC
lamroN
htpeD
Composer
Composer
Ours
Ours
Dream
DreamComparisonwithDreamcomposer. DreamComposerisbuiltonSyncDreamer[7],allowingitto
acceptinputsfrommultipleviewpointsandfillinmissinginformationforallsidesexcepttheback.
Inourexperimentalresults(seefig.10), weadjustedthebackinputtobetheRGBimageofthe
ground-truthbacksideforcomparisonpurposes. Thatis,weprovidedTailor3DandDreamcomposer
withpicturesofthefrontandbackoftheobject,whichcouldhavebeenmoreperfectlyconsistent. We
foundthatTailorcangeneratesuperiormeshresultscomparedtoDreamComposer. DreamComposer
tendstoexhibitmoredefectsinitsreconstructions.
ComparisonwithEscherNet. EscherNetisamulti-viewconditionaldiffusionmodelforviewpoint
synthesis. It learns implicit and generative 3D representations combined with Camera Position
Encoding(CaPE).EscherNetcangeneratemoreconsistentimagesandhashigherreconstruction
quality. Inthisexperiment,weprovidedEscherNetwith16viewpoints,whileourTailor3Dhadonly
thefrontandbackviewpoints. Eveninthisscenario,ourapproachstillhasasignificantadvantage
andobtainsbettermeshresults. Thisfurtherdemonstratesthatourmethodusingonlytwoviewsfor
reconstructioncanachievebetterresults.
D.2 MoreExamples
Here, we showcase more qualitative examples, including 3D style transfer, style fusion, and 3D
generativefill. Wedemonstratethemodel’sabilitytotransformoverallstylesaswellasperform
localized editing. These examples are visually stunning, showcasing the potential for industrial
applications.
E BroadImpacts
Inthissection,weemphasizeTailor3D’sbroadsocietalimpact. Ourmethodispractical,asdemon-
stratedbyourqualitativeexperimentalresults.
AcademicImpact: Tailor3Dintroducesanovelapproachto3Dgeneration,startingfromasingle
imageortextandusingmulti-viewdiffusiontogenerateCanonicalviews. Bydelegatingeditingto
2Dimagesandreconstructionto3D,ourmethodprovidesadirectionforfutureexplorationin3D
generationwithintheacademiccommunity.
IndustrialImpact:Tailor3Disapracticalpaper,asevidencedbyourqualitativeexperimentalresults.
Ourfine-grainedoperationsresultinhighlyeditableandapplicableoutcomes. Ourmotivationis
rootedinconsideringuserinputandrequirements. Furthermore,eachstepofourmethodoperatesat
asub-secondlevel,makingitapplicabletovariousindustrialscenarios.
SocialImpact: Tailor3Dcanbeappliedinanimationproduction,3Dgamedevelopment,andother
fields,significantlyreducingproductioncostsandsparkingawaveofcreativecontentcreation. This
democratizationofcreationallowssocietytoenjoythefruitsofAIdevelopmentbetter.
20Front and Back Style 1 Style 2 Style 3 Style 4
Images or Text
Original Text Original Text Original Text Original Text
+ Captain + Super Man + Spider Man + Iron Man
A little boy America
A Victorian- Original Text Original Text Original Text Original Text
style doll + golden + red + blue + purple
dress dress dress dress
Original Image
Original Image Original Image Original Image
+ space
clothes + orange + brown + cooker
dress dress clothes
Original Image Original Image Original Image Original Image
Front View Front View Front View Front View
Fusion + purple + red + green +
+ yellow + red
+ flag + flower maple leaf maple leaf
Rotate
+ bow tie + glasses + star + flower
Rotate
Figure12: MoreExamplesaboutTailor3D.
21