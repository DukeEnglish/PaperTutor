Evaluating the Semantic Profiling Abilities of LLMs
for Natural Language Utterances in Data Visualization
HannahK.Bako* ArshnoorBhutani† XinyiLiu‡ KwesiA.Cobbina§
UniversityofMaryland UniversityofMaryland UniversityofTexasatAustin UniversityofMaryland
ZhichengLiu¶
UniversityofMayland
ABSTRACT be difficult to handle due to uncertainties such as ambiguities [7]
and under-specification [22]. Furthermore, it is necessary to ad-
Automaticallygeneratingdatavisualizationsinresponsetohuman
dressissuessuchasdatapreparationandtaskidentificationinvisu-
utterancesondatasetsnecessitatesadeepsemanticunderstanding
alizationsystemswithnaturallanguageinterfaces.
of the data utterance, including implicit and explicit references
to data attributes, visualization tasks, and necessary data prepa- Large Language Models (LLMs) are promising in providing a
ration steps. Natural Language Interfaces (NLIs) for data visu- foundation for natural language interfaces tailored to data visual-
alization have explored ways to infer such information, yet chal- ization, due to their ability to interpret and generate textual data.
lenges persist due to inherent uncertainty in human speech. Re- While a few tools have utilized them for visualization genera-
cent advances in Large Language Models (LLMs) provide an av- tion [26, 9, 29, 5], they tend to focus on low-level applications
enue to address these challenges, but their ability to extract the ofLLMs,suchasgeneratingcodefordatatransformations[29]or
relevant semantic information remains unexplored. In this study, simplyintegratingthemaspartofapipeline[5]. Itisstillunclear
we evaluate four publicly available LLMs (GPT-4, Gemini-Pro, howwellLLMsperformatextractinginformationcrucialtovisu-
Llama3,andMixtral),investigatingtheirabilitytocomprehendut- alizationgenerationfromutteranceswithouthumaninterference.
teranceseveninthepresenceofuncertaintyandidentifytherele- Inthiswork,weembarkonanevaluationofthecapabilitiesof
vantdatacontextandvisualtasks. OurfindingsrevealthatLLMs LLMsinthesemanticprofilingofnaturallanguageutterancesfor
are sensitive to uncertainties in utterances. Despite this sensitiv- the purpose of data visualization generation. In line with other
ity, they are able to extract the relevant data context. However, work,weusetheterm“utterance”torefertoquestionsorinstruc-
LLMs struggle with inferring visualization tasks. Based on these tionspeopleusetoelicitresponsesfromanNLIorLLM[23]. By
results,wehighlightfutureresearchdirectionsonusingLLMsfor semanticprofiling,wedonotevaluatevisualizationsgeneratedby
visualization generation. Our supplementary materials have been LLMs but instead focus on the following dimensions: 1) clarity
sharedonGitHub: https://github.com/hdi-umd/Semantic_ analysis, which determines if an utterance is ambiguous, under-
Profiling_LLM_Evaluation. specified, or asking for missing data, 2) data attribute and trans-
formationidentification,whichidentifiesrelevantdatacolumnsand
Index Terms: Human-centered computing—Visualization— anynecessarytransformstopreparethedataintoausableformat,
Empiricalstudiesinvisualization; and3)taskclassification,whichseekstouncoveruserintent.
Tosupportourresearchgoal,wecollatedacorpusof500data-
1 INTRODUCTION
relatedutterancesbasedonanevaluationoftwoNLdatasets(NLV-
Designinganeffectivedatavisualizationrequiresmultipleconsid- Corpus[23]andQuda[6]). Weanalyzedutteranceswiththefol-
erations,suchasidentifyingrelevantdataattributes,preparingthe lowingannotations: 1)uncertaintiessuchasambiguitiesandmiss-
datasetintherightformatthroughdatawranglingandtransforma- ing data references, 2) required data attributes and data transfor-
tion, identifying the analytic tasks or communication goals, and mations, and3)visualizationtasks. Wethenpresentasystematic
choosing appropriate visual encoding strategies. Over the years, analysisofthecapabilitiesoffourpubliclyavailableLLMs(GPT-
visualizationresearchershaveprimarilyfocusedondifferentways 4,Llamma3,Mixtral,andGemini)acrossthethreedimensionsof
toautomaticallyidentifyappropriatevisualencodings [31,16,33], semantic profiling. Our results show that LLMs make inferences
buthavelargelyoverlookedimportantaspectssuchasautomating at a different level of abstraction than humans, causing them to
task identification and data preparation. Only recently have re- behyper-sensitivetouncertaintiesinutterances. Wealsofindthat
searchersstartedtoaddresstheseoverlookedissues[32,19,28]. LLMsperformreasonablyatidentifyingtherelevantdatacolumns
Among these efforts, natural language interfaces (NLI) have anddatatransformationsexpressedinutterancesbutarenotableto
emerged as a popular interaction paradigm for visualization gen- properlyinfervisualizationtasks.Wehighlightourobservationson
eration. To users, it is easier to articulate their visualization in- thecurrentstrengthsandchallengesofLLMsandpresentadiscus-
tentsthroughnaturallanguagethanusingprogrammingconstructs siononconsiderationsforusingLLMsinvisualizationgeneration.
or complex graphical user interfaces; to system builders, natural
languageutterancesprovidevaluableinformationonuserintentthat 2 RELATEDWORK
couldbehardtocapture.However,naturallanguageutterancescan
Natural Language Interfaces for Visualization Generation.
*e-mail:hbako@cs.umd.edu There has been extensive research on natural language interfaces
†e-mail:arshnoor@terpmail.umd.edu (NLI)datingasfarbackas2001whenCoxetal. proposedtheuse
‡e-mail:xinyi.liu@utexas.edu ofnaturallanguageasaninputmediumforthegenerationofdata
§e-mail:kcobbina@cs.umd.edu visualizations [4]. Since then, a plethora of NLIs have been cre-
¶e-mail:zcliu@cs.umd.edu ated[7,25,12,10,14,19].TheseNLIsusetechniques,suchaslexi-
caltokenizationorsemanticparsing,toinferandtranslaterepresen-
tationsofdataattributesandtasksinutterancesintovisualizations.
However,whenusers’utterancesareunder-specified,inferringthe
correct data and task representation becomes challenging. Tools
4202
luJ
8
]IA.sc[
1v92160.7042:viXrasuch as DataTone circumvent this limitation by allowing users to 4 GENERATINGGROUNDTRUTHSANDLLMRESPONSES
resolveambiguitythroughGUIwidgets. Similarly,Eviza[21]and
4.1 ManuallyAnnotatingUtterances
Evizeon[10]provideuserswiththeabilitytointeractwithgener-
atedvisualizationsandrefinedesignsviafollow-uputterances. Threeoftheauthorsperformedmanualannotationofutterancesin
ourcorpus.Theleadannotatorhas5yearsofvisualizationresearch
Recentresearchhasprogressedtowardsfacilitatingvisualization
experience,whiletheremainingtwoannotatorshaveatleast2years
code generation based on NL input [32, 19] , generating NL ex-
ofexperiencecreatingvisualizations. Toannotateourcorpusofut-
planations for visualizations [13] and recommending input utter-
terances,theleadauthordraftedaninitialcodebookfromaneval-
ances[24]. Together, theseworksdemonstratethecapabilitiesof
uationofrelevanttaxonomiesforvisualtasksanddatatransforma-
NLIsforvisualization. However,NLIsstillstrugglewithresolv-
tions[2,18]. Fiverandomutteranceswerethenselectedfromthe
ingunder-specificationsinutteranceswithouthumanintervention.
corpus, and three of the authors independently examined and an-
LargeLanguageModelsforDataVisualization. Technological notatedthem. Theauthorsmetinasubsequentmeetingtodiscuss
advanceshavegivenrisetoimprovementsinNLIs,suchastheuse their codes. The codebook was then updated based on this dis-
of BERT to translate user intent expressed in NL into a domain- cussion. Thethreeauthorsmanuallyannotatedtheremaining495
specific language for visualizations [3]. More recently, we have utterancesoverthecourseof12weeks,holdingweeklymeetingsto
seen an uptick in the applications of Large Language Models for discussandresolveconflicts.Here,wedescribetheseannotations.
visualizationgeneration. OnesuchtoolisChartLlama[9],which
Uncertainties. We identified utterances that could lead to multi-
usesafine-tunedopen-sourceLLMtrainedonsyntheticbenchmark
pleinterpretationsorcouldn’tbeansweredbasedontheprovided
datasetgeneratedfromGPT-4[20]to enhancechartgenerationand
dataset.Weannotatedambiguitiesandunder-specificationbyhigh-
comprehension. SometoolsdeveloppipelinestopromptLLMfor
lightingconfusingwords,explainingtheirlackofclarity,andsug-
relevantcodeforvisualizationimplementations[26,5,17], while
gestingresolutions.Forinstance,theutterance“inwhatmannerare
othersuseLLMstofacilitatedatatransformations[29].
goodairqualityrecordsdispersedthroughoutthemonitoredregion
There have also been works that evaluate the capabilities of
?” was labeled ambiguous because the reference dataset had air
LLMs for different visualization contexts . Li et al. evalu-
qualityreadingsgeneratedatdifferenttimesforeachregion.There-
atepromptingstrategiesforgeneratingvisualizationsbasedonthe
fore,thegoodairqualityreadingscouldbesplitintodifferenttime
nvbenchdataset[15]. Va´zquezalsoevaluatesLLMsacross3axes:
periods (per hour of the day, per date) or even aggregated across
thevarietyofgeneratedcharttypes,supportedlibraries,anddesign
theentiredataset. Weprovidedaresolutiontocalculatesummary
refinement[27]. However, theseevaluationsdonotpresentresults
statisticsandgenerateyearlytrendsforgoodairquality.
for multiple LLMs and focus on the visual artifacts produced by
Whileannotatingthe500utterancesinourcorpus,wefound18
theseLLMs. Ourworkbuildsonthisthreadofresearchbyeval-
utterancesthatrequestedinformationunavailableinthedataset.For
uatingthestrengthsandlimitationsofdifferentLLMsininferring
instance,onthedatasetshowinglifeexpectancybystatesintheUS,
thesemanticinformationneededtocreatevisualizations.
oneoftheutterancesasked“showmetheGDPrankingofEuropean
countries”.Thisdatasetdidnotcontainanyinformationaboutany
countries. As such, it is not possible to answer such a question.
3 COLLATINGNATURALLANGUAGEUTTERANCES
Since these utterances were obtained from other studies, it is un-
TofacilitatetheevaluationofLLMs’capabilitiesforextractingrel- clearhowtheseutterancescametobe. Whilewedidnotprovide
evant data and visual contexts, we need a set of data-related user annotationsfortherelevantdataandvisualcontextfortheseutter-
utterancestoprovideaspromptstoLLMs. Theseutterancesneed ances,westillchosetoincludethemwhenpromptingLLMsaswe
toreflectthelevelofuncertaintyfoundinhumanspeech. Tothis arestillinterestedinevaluatingtheirabilitytoidentifyandresolve
end,wesourcedutterancesfromtwopubliclyavailablecorpora: suchuncertaintiesinutterances.
• NLVCorpus: Thisdatasetpresents893utterancescollected Data Attributes and Transformations. For each utterance, we
fromanonlinesurvey,where102respondentswereaskedto identified the relevant data column[s] needed to correctly answer
describeutterancestheywouldinputtoananalyticalsystem the utterance. Some utterances require data transformations to
togenerateaspecificvisualization[23]. generate a new data table that can be used to answer the ques-
• Quda: Thisdatasetutilizesinterviewswithexpertdataana- tion. We initially captured the operations needed to transform
lyststogenerateacorpusof920utterances[6]. Theseutter- the data table, such as fold, unstack, and group. However, to
anceswererefinedandparaphrasedviaacrowdsourcedstudy properly evaluate if these operations are accurate, we need to
togenerateafinaldatasetof14,035diverseutterances. evaluate the actual data tables that are generated from these op-
erations. As such, we opted to capture the relevant pandas
We performed a systematic examination of utterances from each
code that would be used to perform data transformations. Us-
dataset and filtered out utterances if they contained SQL pseudo
ing the previous example utterance on the air quality dataset,
code,e.g.,“group(region)—Foreachregion,groupby(shipsta-
thedatatransformationneededtogeneratetherelevantdatatable
tus)—Foreach(region,shipstatus),calculatethesumofprofit”.
was res=df.groupby([′Generated′,′Station′]).apply(lambdax:
Forouranalysis,wewereinterestedinexamininghowwellLLMs
x[x[′Air Quality′].lower()== ′good′])
inferthenecessaryaspectsofthesemanticprofileandnotexplicit
visualizationdescriptions.Consequently,wealsofilteredoututter- Visualization Tasks. The visual task[s] were classified based
ancesthatspecifiedvisualizationtypesormappingofdatatovisual on the inferred intent of the utterance. The taxonomy for these
elements,e.g.,“givemeascatterplotofimdbratingasxaxisand tasks was adopted from published works by Amar et al. [2]
rottentomatoesratingasyaxis”. and Munzner [18] and include: Retrieve Value, Filter, Compute
This selection process was first applied to the NLVCorpus Derived Value, Find Extremum, Sort, Determine Range, Characterize
dataset, which yielded a total of 134 utterances across 3 unique Distribution, Find Anomalies, Cluster, Correlate, summarize,
datasets. Wethenappliedthesameinclusioncriteriatoasubsetof Compare, Dependency, Similarity, and Trend.
theQudadatasettoproducetheremaining309utterancesacross32
4.2 GeneratingLLMOutputs
datasets.Wealsoincluded54utterancesacross2datasetscollected
fromaclassroomactivity conductedinanundergraduateleveldata Weevaluatedtwoproprietaryandtwoopen-sourceLLMs.
visualizationclassataUS-basedUniversity.Ourfinalcorpuscon- ProprietaryLLMs. WeevaluatedOpenAI’sGPT4-Turbo [20]
sistsof500diverseutterancesacross37uniquedatasets. andGoogle’sGemini-Pro [8]. GPT4-Turbohasatrainingdatacutoff of December 2023 and Gemini-Pro’s training data cutoff LLM and Human Visual Task Annotations
is described as “early 2023” 1. We utilized the Application Pro-
Mixtral
gramming Interfaces (APIs) for both of these models to generate
Llama3
responsesforthe500utterancesinourcorpus.
GPT4
Open Source LLMs. We evaluated two open-source LLMs,
Llama3 , and Mixtral , on the Llama factory code base [34]. Gemini−Pro
Llama3[1]has70billionparametersandacontextlengthof8,000 0% 25% 50% 75% 100%
% of responses
tokens,withaknowledgecutoffofDecember2023.Mixtral-8x7B-
Instruct [11] is configured with 46.7 billion parameters and simi- LLM + HM HM Only LLM Only
larlyhasaknowledgecutoffinDecember2023. Figure 1: Overview of the overlap in uncertainty annotations be-
tweentheLLMsandHuman(HM)annotations.
4.2.1 PromptDesign
Weexploreddifferentpromptingstrategies(One-shotvs.Few-shot) analysisshouldconsiderotherfactorsthatmightinfluencehappi-
to elicit responses from LLMs. We decided to use a few-shot ness,orifitshouldbeisolatedtojusthappinessandfreedom.” To
prompting as it is more suited for complex tasks and allows the thehumanannotators,thiswassimplyacaseofshowingthecorre-
model to learn requirements from provided examples [30]. The lationbetweenthetwoattributes; hence,therewasnouncertainty
prompt provided to each model contained similar instructions to annotationforthisutterance.Similarly,fortheutterance“Compare
thoseusedbyourhumanannotatorsinSec.4.1. Forthedatatrans- thenumberoftallbuildingsinHongKongwithTaiwan”,Gemini-
formationcode, wealsoinstructedtheLLMsnottoincludecode Pro classifiedthisasuncertainbecause“Itisunclearwhatmetric
for plots or complex analyses. We also included three utterance- should be used to quantify the tallness of a building. Should the
dataset-outputsamples,whichwerenotincludedinourevaluation number of stories be used or the height in meters or feet?”. Our
corpus. Wechosetoincludethefirst10rowsofthedatasettopro- humanannotatorsinferredthattheheightofthebuildingwouldbe
videanoverviewoftheinputdataschema. Wealsoincludedthe themeasureusedtoanswerthisutterance.
correspondinggroundtruthannotationsforoursampleutterances
UncertaintiesnotfoundbyLLMs.Ofthe96utterancesforwhich
to help the model gain an understanding of the expected output.
human annotators found uncertainty, some were not identified by
Duetospaceconsiderations,thefullprompthasbeenprovidedin
LLMs ( : 14, : 32, : 34, : 35). A majority of these uncer-
supplementarymaterials2.
tainties were as a result of either missing or conflicting data be-
ingreferencedintheutterance. Anexampleistheutterance“How
4.2.2 ChallengesRetrievingResponses.
canthepopulationofAshleybeillustratedtoshowthedistribution
We expected to receive a total of 2000 LLM responses (500 per across five years?” Our annotations labeled this as uncertain be-
LLM).However, weencounteredsomeissueselicitingresponses causethedatasetonlycontainsinformationfrom2000to2002,so
fromtheLLMs.SomeofourqueriesusingtheAPIsofproprietary itisimpossibletoanswerthisusingthedataset.NoneoftheLLMs
modelsreturnednullresponses( : 9, : 2). Fortheopen-source labeledthisutteranceasuncertain.
models, 42 of the responses did not return the JSON annotations
andinsteadreturneda text-based answertotheutterance( :20, 5.2 IdentifyingRelevantDataContext
:22). Both models also occasionally failed to correctly format
For each data column identified in LLM-generated responses, we
theJSONresponsescorrectly,wrappingkeyswith‘/,‘‘@,‘or‘<.‘
examined if they were also identified by human annotators. We
WronglyformattedJSONresponseswereresolvedmanually. The
defined three levels of agreement between LLMs and human an-
finalsetcontains1947validannotationsfromtheLLMs( : 491,
notations: 1) total agreement, where LLMs identify all relevant
:498, :481, :477).
datacolumns;2)partialagreement,whereLLMsidentifysomeof
thedatacolumns;and3)totaldisagreement,whereLLMsidentify
5 ANALYSISANDRESULTS
noneofthedatacolumns.
We analyzed the LLMs responses across three dimensions of se- Summary Statistics. Of the 1947 responses returned by LLMs,
manticprofiling:clarityanalysis(i.e.,comprehensionofutterances we filtered out 53 responses that were related to the utterances
inthepresenceofuncertainty),properidentificationoftherelevant for which our human annotators did not generate codes for data
datacontext,andproperinferenceofthevisualizationtask. columns (see Sec. 4.1). We also eliminated an additional 13 re-
sponses where the LLMs did not generate data column values,
5.1 Identifyinguncertainty bringingthetotalresponsesevaluatedfordatacolumnsto1881.
SummaryStatistics: Ofthe500utterancesinourcorpus,thehu- LLMsareabletocorrectlyinferrelevantdatacolumnsformost
manannotationsfounduncertaintyin96oftheutterances. Atotal utterances. As shown in Fig. 2a , 57.5% of the valid annota-
of813uncertaintieswerefoundacrossallLLMs( : 268, : 192, tions generated by LLMs had a total agreement with the human
: 180, : 173). Ofthese813uncertainties,only25.1%(n=204) annotations ( :312, :241, :273, :255). 34.24% had partial
overlappedwithhumanannotations( :74, :46, :44, :40). agreement( :140, :180, :157, :167) between LLMs and hu-
man annotations, while 8.29% had complete disagreement in the
DifferencesinuncertaintiesclassifiedbyLLMsandhumanan-
relevantdatacolumnsidentified( :32, :48, :37, :39). Weob-
notators. We observe that all LLMs identified a higher propor-
servedthat43.6%ofthesecompletedisagreementcaseshaduncer-
tion of uncertainty in the utterances than those identified by the
taintiesidentifiedbyeitherhumanannotatorsorLLMs.
human annotators (see Fig.1). When we examine some of these
uncertaintiesidentifiedbytheLLMs,wefindthattheydescribeun-
5.2.1 Datatransformations
certainty on how to perform analysis or missing context for data
columnvalues. Forinstance, fortheutterance“Canweconclude For each response generated by an LLM, we executed both the
that higher happiness comes from higher freedom?”, GPT-4 re- LLM-produced and human-annotated transformations, extracted
turnedthefollowingambiguity: “Thequerydoesnotspecifyifthe theresultingdatatablesfrombothexecutionsandcomparedtheir
underlying data schemas (i.e., attribute types) to verify the accu-
1AccordingtoGoogleAIdocumentation racyofthetransformationspresentedbyLLMs. Forexample,for
2SupplementaryMaterials theutterance“Whatistherelationship, ifany, betweenwindandLLM and Human Visual Task Annotations LLMs vs Human Annotations Schema Matches LLM and Human Visual Task Annotations
Mixtral mixtral Mixtral
Llama3 llama Llama3
GPT4 gpt GPT4
Gemini−Pro gemini Gemini−Pro
0% 25% % of re50 s% ponses 75% 100% 0% 25% % of re50 s% ponses 75% 100% 0% 25% % of re50 s% ponses 75% 100%
Total agreement Partial agreement Total disagreement matching_df mismatching_df Total agreement Partial agreement Total disagreement
(a)AgreementbetweenLLMandhumanannotations (b)Dataschemamatchesbetweendatatablesreturned (c)AgreementbetweenLLMandhumanannotations
forrelevantdatacolumns. byLLMsgeneratedcodeandhumanannotations. forvisualizationtasks.
Figure2:OverviewofoverlappingannotationsbetweenLLMsandhumansfordataattributes,transformationsandvisualtasks.
pressure?”, both the data transforms provided by Llama3 and tion.Ourresultsposeinterestinginsightsforfutureresearch.
humanannotationsreturnedadatatablewiththefollowingschema Using uncertainties to facilitate deeper data exploration and
{wind:int64,pressure:int64}. Since the data tables have the analysis. Our findings show that LLMs found a higher number
samenumberandtypesofattributes,thisisapositivematch. ofuncertaintiesinutterancescomparedtoourhumanannotators.It
While evaluating the data transformations, we found 31 in- ispossiblethathumansandLLMsidentifyuncertaintiesatdifferent
stances where the code for data transformations violated instruc- levelsofabstraction,ashumansareabletointerpretcontextmore
tions on not returning code for visualization plots or performing deeplyandmakebetterinferences.Onesuchinstanceofthisdiffer-
complex analyses which were excluded from our analyses ( :1, encecanbeseenintheinferenceinthe“tallestbuilding”example
:0, :15, :15). Furthermore,wefoundthat385ofthetransfor- providedinSec.5.1. Asaresult,LLMsmightbemoresensitiveto
mationsraisederrorsofvariouskinds( :59, :96, :119, :111) uncertaintiesinutterances. However,thismaynotbealimitation
orreturnedrawvaluesandnotdatatables( :66, :90, :57, :52). astheirsensitivitytouncertaintiescanbeleveragedtoposeques-
Sincethehumanannotationprioritizeddatatablesastheoutputof tions to analysts and help them think deeply about their analysis
datatransformations,weexcludesuchresponsesinouranalyses. questionsorapproach. FacilitatingsuchinteractionsinNLIsisan
DatatransformationsproducedbyLLMsdonotalwaysmatch interestingresearchdirection.
those generated by human annotators. The final set for our Improvingprogramming-basedresponsestoutterances.Weob-
analysis on data transformation is 1238 responses ( :360, :290, servedthatLLMsarealsocapableofinferringtheappropriatedata
:292, :296).48.1%oftheseresponsesproduceddatatableswith columns andtransformations forover half ofthe utterances. Yet,
schemasthatmatchthoseproducedbythehumanannotations(see formanyofthedatatransformations,wefoundanumberofissues
Fig. 2b). For the remaining 51.9% where the data did not match withinthecodereturnedbyLLMs. Thisissueisknownandtools
whatwasproducedbythecodeannotatedbyhumans,ourevalua- circumventthisbypromptingformultiplecodescriptsandfiltering
tionfocusesonmatchesbetweendataschemas.Assuch,wecannot outerroneousscripts[5,29]. Whiletheseerroneousresponsescan
verifyiftheresultingdatatablesprovidemeaningfulanswerstothe improveviafeedbackandfine-tuningprompts,thereisaneedfor
utteranceoriftheyweretheresultofincorrectdatatransformations. furtherresearchonhowtoimprovethegenerationofrelevantcode
forvisualizationcontexts.
5.3 InferringVisualizationTasks
Improvingvisualizationtaskinferencetofacilitateexploration.
Similartotheanalysisfordatacolumns,weidentifythreelevelsof We also found that LLMs struggle to correctly infer appropriate
agreementbetweenhumanandLLMannotationsforvisualtasks. visualization tasks from utterances. Nevertheless, there is a need
Summary Statistics. Of the 1947 responses returned by LLMs, toinvestigatewaystoimproveLLMs’abilitytoinfervisualization
visualizationtaskswereidentifiedin1940responses( :490, :494, tasksproperly.Thisisimportantasthesetasksofteninformvisual-
:479, :477). izationdesignchoices,suchasusingbarchartsforcomparisonor
Higher proportion of disagreements between human annota- violinplotstocharacterizedistributions[2,18,19]. Properinfer-
tions and LLMs for visual task classifications. We observed enceofvisualcontextscanalsofacilitateabreadth-wiseexploration
thehighest levelofdisagreementbetweenLLMsandhumanan- ofdatasimilartotheVoyagersystem[31]. Forinstance,ifauser
notations in the visual task classifications. 50.4% of the visual is working on the movies dataset and an LLM can infer they are
taskswereintotaldisagreement,asseeninFig.2c( :205, :253, trying to find anomalies in the IMDB ratings, it can recommend
:224, :296). There was total agreement in 33.43% of the re- potentiallyinterestingutterancesbasedontherelevanttasks,such
sponses( :208, :169, :169, :103)whiletheremaining16.17% ascomparingIMDBratingsacrosscreativetasksorfindingcorre-
hadpartialagreementforthevisualtask( :81, :68, :86, :78). lationsbetweenIMBDandRottenTomatoratings.
Whenweexamineaportionofthecaseswithtotaldisagreement,
we observe that some of the issues are a result of conflicting in- 7 CONCLUSION
terpretations. For instance, for the utterance “What is the main
We evaluated the capabilities of four publicly available LLMs
factor depending on different status (wind, time, pressure, etc)?”
(GPT-4 ,Gemini ,Llama3 andMixtral )atcorrectlyinferring
Gemini classifiedthisas“correlation”whereasthehumananno-
thesemanticprofilesofnaturallanguageutterancesfordatavisu-
tations classified the utterance as “dependency” since correlation
alization generation. Our findings reveal important strengths of
cannotbecalculatedbetweencategoricalandnumericalattributes.
LLMs at identifying uncertainties in utterances and inferring rel-
WealsoseeinstanceswhereLLMsmixdatatransformationswith
evant data columns. We also highlight the current limitations of
visual tasks, e.g., for the utterance “What was the average bud-
LLMsforgeneratingdatatransformationcodeandinferringvisu-
get for each content rating and creative type, as multiple column
alizationtasks. Basedonourfindings,wepresentfutureresearch
charts?” Mixtral classifiedtheutteranceas“aggregation, cate-
directionsontheuseofLLMsforvisualizationgeneration.
gorization&relationship”.
6 DISCUSSIONANDFUTUREWORK ACKNOWLEDGMENTS
We evaluated the capabilities of four publicly available LLMs in ThankstotheHuman-DataInteractionGroupfortheirfeedbackand
semanticprofilingofnaturallanguageutterancesfordatavisualiza- support.ThisworkwassupportedbyNSFgrantIIS-2239130.REFERENCES languagemodels,2023.2
[18] T.Munzner.Visualizationanalysisanddesign.CRCpress,2014.doi:
[1] AI@Meta.Llama3modelcard.2024.3 10.1201/b175112,4
[2] R.Amar,J.Eagan,andJ.Stasko. Low-levelcomponentsofanalytic [19] A.Narechania, A.Srinivasan, andJ.Stasko. Nl4dv: Atoolkitfor
activityininformationvisualization. InProceedingsoftheProceed- generatinganalyticspecificationsfordatavisualizationfromnatural
ingsofthe2005IEEESymposiumonInformationVisualization,IN- languagequeries. IEEETransactionsonVisualizationandComputer
FOVIS’05,p.15.IEEEComputerSociety,USA,2005.doi:10.1109/ Graphics,27(2):369–379,2021.doi: 10.1109/TVCG.2020.3030378
INFOVIS.2005.242,4 1,2,4
[3] Q.Chen,S.Pailoor,C.Barnaby,A.Criswell,C.Wang,G.Durrett, [20] OpenAI.Gpt-4technicalreport.ArXiv,abs/2303.08774,2023.2
andI.Dillig. Type-directedsynthesisofvisualizationsfromnatural [21] V.Setlur,S.E.Battersby,M.Tory,R.Gossweiler,andA.X.Chang.
languagequeries. Proc.ACMProgram.Lang.,6(OOPSLA2),article Eviza:Anaturallanguageinterfaceforvisualanalysis.InProc.,UIST
no.144,28pages,oct2022.doi:10.1145/35633072 ’16, 13 pages, p. 365–377. Association for Computing Machinery,
[4] K.Cox,R.E.Grinter,S.L.Hibino,L.J.Jagadeesan,andD.Man- NewYork,NY,USA,2016.doi:10.1145/2984511.29845882
tilla. Amulti-modalnaturallanguageinterfacetoaninformationvi- [22] V.Setlur,M.Tory,andA.Djalali. Inferencingunderspecifiednatural
sualizationenvironment.InternationalJournalofSpeechTechnology, language utterances in visual analysis. In Proceedings of the 24th
4:297–314,2001.doi:10.1023/A:10113689264791
InternationalConferenceonIntelligentUserInterfaces,IUI’19,12
[5] V.Dibia.LIDA:Atoolforautomaticgenerationofgrammar-agnostic pages,p.40–51.AssociationforComputingMachinery,NewYork,
visualizations and infographics using large language models. In NY,USA,2019.doi:10.1145/3301275.33022701
D. Bollegala, R. Huang, and A. Ritter, eds., Proceedings of the [23] A.Srinivasan,N.Nyapathy,B.Lee,S.M.Drucker,andJ.Stasko.Col-
61stAnnualMeetingoftheAssociationforComputationalLinguis- lectingandcharacterizingnaturallanguageutterancesforspecifying
tics (Volume 3: System Demonstrations), pp. 113–126. Association datavisualizations. InProc.,CHI’21,articleno.464,10pages.As-
forComputationalLinguistics,Toronto,Canada,July2023.doi: 10. sociationforComputingMachinery,NewYork,NY,USA,2021.doi:
18653/v1/2023.acl-demo.111,2,4 10.1145/3411764.34454001,2
[6] S.Fu,K.Xiong,X.Ge,S.Tang,W.Chen,andY.Wu.Quda:Natural [24] A. Srinivasan and V. Setlur. Snowy: Recommending utterances
languagequeriesforvisualdataanalytics,2020.1,2 for conversational visual analysis. In Proc., UIST ’21, 17 pages,
[7] T.Gao,M.Dontcheva,E.Adar,Z.Liu,andK.G.Karahalios. Data- p.864–880.AssociationforComputingMachinery,NewYork,NY,
tone: Managingambiguityinnaturallanguageinterfacesfordatavi- USA,2021.doi:10.1145/3472749.34747922
sualization. InProc.,UIST’15,12pages,p.489–500.Association [25] Y. Sun, J. Leigh, A. Johnson, and S. Lee. Articulate: A semi-
forComputingMachinery,NewYork,NY,USA,2015.doi:10.1145/ automatedmodelfortranslatingnaturallanguagequeriesintomean-
2807442.28074781 ingfulvisualizations. InSmartGraphics: 10thInternationalSympo-
siumonSmartGraphics,Banff,Canada,June24-26,2010Proceed-
[8] Google. Geminiapidocsandreference — googleaifordevelopers.
2
ings10,pp.184–195.Springer,2010.doi:10.1007/978-3-642-13544
[9] Y.Han, C.Zhang, X.Chen, X.Yang, Z.Wang, G.Yu, B.Fu, and -6181
H.Zhang.Chartllama:Amultimodalllmforchartunderstandingand [26] Y. Tian, W. Cui, D. Deng, X. Yi, Y. Yang, H. Zhang, and Y. Wu.
generation,2023.1,2 Chartgpt:Leveragingllmstogeneratechartsfromabstractnaturallan-
[10] E.Hoque,V.Setlur,M.Tory,andI.Dykeman. Applyingpragmatics guage. IEEETransactionsonVisualizationandComputerGraphics,
principlesforinteractionwithvisualanalytics.IEEETransactionson pp.1–15,2024.doi:10.1109/TVCG.2024.33686211,2
VisualizationandComputerGraphics,24(1):309–318,2018.doi:10. [27] P.-P.Va´zquez.Arellmsreadyforvisualization?,2024.2
1109/TVCG.2017.27446841,2 [28] C.Wang,Y.Feng,R.Bodik,I.Dillig,A.Cheung,andA.J.Ko.Falx:
[11] A.Q.Jiang,A.Sablayrolles,A.Roux,A.Mensch,B.Savary,C.Bam- Synthesis-poweredvisualizationauthoring. InProc.,CHI’21,article
ford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, no.106,15pages.AssociationforComputingMachinery,NewYork,
G.Lengyel, G.Bour, G.Lample, L.R.Lavaud, L.Saulnier, M.-A. NY,USA,2021.doi:10.1145/3411764.34452491
Lachaux,P.Stock,S.Subramanian,S.Yang,S.Antoniak,T.L.Scao, [29] C.Wang, J.Thompson, andB.Lee. Dataformulator: Ai-powered
T.Gervet,T.Lavril,T.Wang,T.Lacroix,andW.E.Sayed.Mixtralof concept-drivenvisualizationauthoring. IEEETransactionsonVisu-
experts,2024.3 alizationandComputerGraphics,30(1):1128–1138,2024.doi: 10.
[12] J.-F.KasselandM.Rohs.Valletto:Amultimodalinterfaceforubiqui- 1109/TVCG.2023.33265851,2,4
tousvisualanalytics. InExtendedAbstractsofthe2018CHIConfer- [30] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia,
enceonHumanFactorsinComputingSystems,CHIEA’18,6pages, E.H.Chi,Q.V.Le,andD.Zhou. Chainofthoughtpromptingelicits
p.1–6.AssociationforComputingMachinery,NewYork,NY,USA, reasoninginlargelanguagemodels.InA.H.Oh,A.Agarwal,D.Bel-
2018.doi:10.1145/3170427.31884451
grave,andK.Cho,eds.,AdvancesinNeuralInformationProcessing
[13] D.H.Kim,E.Hoque,andM.Agrawala. Answeringquestionsabout
Systems,2022.3
chartsandgeneratingvisualexplanations.InProc.,CHI’20,13pages, [31] K.Wongsuphasawat,D.Moritz,A.Anand,J.Mackinlay,B.Howe,
p.1–13.AssociationforComputingMachinery,NewYork,NY,USA, andJ.Heer. Voyager: Exploratoryanalysisviafacetedbrowsingof
2020.doi:10.1145/3313831.33764672 visualizationrecommendations. IEEETransactionsonVisualization
[14] A.Kumar,J.Aurisano,B.DiEugenio,A.Johnson,A.Gonzalez,and
andComputerGraphics,22(1):649–658,2016.doi:10.1109/TVCG.
J.Leigh. Towardsadialoguesystemthatsupportsrichvisualizations 2015.24671911,4
ofdata. InR.Fernandez,W.Minker,G.Carenini,R.Higashinaka, [32] Z.Wu, V.Le, A.Tiwari, S.Gulwani, A.Radhakrishna, I.Radicˇek,
R.Artstein,andA.Gainer,eds.,Proceedingsofthe17thAnnualMeet- G.Soares,X.Wang,Z.Li,andT.Xie.Nl2viz:naturallanguagetovi-
ingoftheSpecialInterestGrouponDiscourseandDialogue,pp.304– sualizationviaconstrainedsyntax-guidedsynthesis.InProceedingsof
the30thACMJointEuropeanSoftwareEngineeringConferenceand
309.AssociationforComputationalLinguistics,LosAngeles,Sept.
2016.doi:10.18653/v1/W16-36391
SymposiumontheFoundationsofSoftwareEngineering,ESEC/FSE
[15] G.Li,X.Wang,G.Aodeng,S.Zheng,Y.Zhang,C.Ou,S.Wang,and 2022,12pages,p.972–983.AssociationforComputingMachinery,
C.H.Liu. Visualizationgenerationwithlargelanguagemodels: An NewYork,NY,USA,2022.doi:10.1145/3540250.35491401,2
evaluation,2024.2 [33] J.Zhao, M.Fan, andM. Feng. Chartseer: Interactive steeringex-
[16] J.Mackinlay,P.Hanrahan,andC.Stolte. Showme: Automaticpre- ploratory visual analysis with machine intelligence. IEEE Trans-
sentationforvisualanalysis. IEEETransactionsonVisualizationand actionsonVisualizationandComputerGraphics,28(3):1500–1513,
Computer Graphics, 13(6):1137–1144, 2007. doi: 10.1109/TVCG. 2022.doi:10.1109/TVCG.2020.30187241
2007.705941 [34] Y.Zheng,R.Zhang,J.Zhang,Y.Ye,Z.Luo,andY.Ma.Llamafactory:
[17] P.MaddiganandT.Susnjak. Chat2vis: Fine-tuningdatavisualisa-
Unifiedefficientfine-tuningof100+languagemodels.arXivpreprint
tions using multilingual natural language text and pre-trained large
arXiv:2403.13372,2024.3