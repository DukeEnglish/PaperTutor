DϵpS: Delayed ϵ-Shrinking for Faster
Once-For-All Training
Aditya Annavajjala∗1, Alind Khare∗1, Animesh Agrawal1, Igor Fedorov3, Hugo
Latapie2, Myungjin Lee2, and Alexey Tumanov1
1 Georgia Institute of Technology, Atlanta, USA
2 Cisco Research, USA
3 Meta, USA
Abstract. CNNs are increasingly deployed across different hardware,
dynamicenvironments,andlow-powerembeddeddevices.Thishasledto
thedesignandtrainingofCNNarchitectureswiththegoalofmaximizing
accuracysubjecttosuchvariabledeploymentconstraints.Asthenumber
of deployment scenarios grows, there is a need to find scalable solutions
todesignandtrainspecializedCNNs.Once-for-alltraininghasemerged
as a scalable approach that jointly co-trains many models (subnets) at
oncewithaconstanttrainingcostandfindsspecializedCNNslater.The
scalability is achieved by training the full model and simultaneously re-
ducing it to smaller subnets that share model weights (weight-shared
shrinking).However,existingonce-for-alltrainingapproachesincurhuge
training costs reaching 1200 GPU hours. We argue this is because they
either start the process of shrinking the full model too early or too late.
Hence,weproposeDelayedE-Shrinking(DϵpS)thatstartstheprocessof
shrinkingthefullmodelwhenitispartially trained(∼50%)whichleads
to training cost improvement and better in-place knowledge distillation
to smaller models. The proposed approach also consists of novel heuris-
tics that dynamically adjust subnet learning rates incrementally (E),
leading to improved weight-shared knowledge distillation from larger to
smaller subnets as well. As a result, DϵpS outperforms state-of-the-art
once-for-all training techniques across different datasets including CI-
FAR10/100, ImageNet-100, and ImageNet-1k on accuracy and cost. It
achieves1.83%higherImageNet-1ktop1accuracyorthesameaccuracy
with1.3xreductioninFLOPsand2.5xdropintrainingcost(GPU*hrs).
1 Introduction
CNNsarepervasiveinnumerousapplicationsincludingsmartcameras[2],smart
surveillance [6], self-driving cars [26], search engines [12], and social media [1].
Asaresult,theyareincreasinglydeployedacrossdiversehardwarerangingfrom
server-grade GPUs like V100 [19] to edge-GPUs like Nvidia Jetson [18] and
dynamic environments like Autonomous Vehicles [8] that operate under strict
latency or power budget constraints. As the diversity in deployment scenarios
∗ Authors contributed equally to this research.
4202
luJ
8
]VC.sc[
1v76160.7042:viXra2 Annavajjala et al.
grows, efficient deployment of CNNs on a myriad of deployment constraints
becomes challenging. It calls for developing techniques that find appropriate
CNNs suited for different deployment conditions.
Neural Architecture Search (NAS) [4,32] has emerged as a successful tech-
niquethatfindsCNNarchitecturesspecializedforadeploymenttarget.Itsearches
for appropriate CNN architecture and trains it with the goal of maximizing ac-
curacy subject to deployment constraints. However, state-of-the-art NAS tech-
niques remain prohibitively expensive, requiring many GPU hours due to the
costly operation of the search and training of specialized CNNs. The problem
is exacerbated when NAS is employed to satisfy multiple deployment targets,
as it must be run repeatedly for each deployment target. This makes the cost
of NAS linear in the number of deployment targets considered (O(k)), which is
prohibitively expensive and doesn’t scale with the growing number of deploy-
ment targets. Therefore, there is a need to develop scalable NAS solutions able
to satisfy multiple deployment targets efficiently.
One such technique is Once-for-all training [3,29,40]—a step towards making
NAS computationally feasible to satisfy multiple deployment targets by decou-
pling training from search. It achieves this decoupling by co-training a family
of models (weight-shared subnets with varied shapes and sizes) embedded in-
side a supernet once, incurring a constant training cost. After the supernet is
trained,NAScanbeperformedforanyspecificdeploymenttargetbysimplyex-
tractingaspecializedsubnetfromthesupernetwithoutretraining(once-for-all).
This achieves O(1) training cost
w.r.t. the number of deployment tar-
Child Models
gets and, therefore, makes NAS scal- Distillation Full model (Subnets)
T sec/epoch ~2T sec/epoch ~3T sec/epoch Total
able. However, the efficiency of this Time
once-for-all training remains limited Train Full Model
as it incurs a significant training cost (2Tn + 2Tn + 6Tn) = 10Tn
~3T sec/epoch
(∼1200GPUhoursin[3]).Thisispri-
marily due to (a) the large number of (3T*2n) = 6Tn
training epochs required to overcome T sec/epoch ~3T
sec/epoch
training interference (OFA [3] in Fig. Train Full
1),and(b)thehighaveragetimeper- Pm aro tid ae lll y (T*n + 3T*n) = 4Tn
epoch caused by shrinking—defined n 2n 3n # Epochs 5n
as sampling and adding smaller sub-
Fig.1: DϵpS reduces training time com-
nets to the training schedule— per
pared to existing approaches like OFA [3]
minibatch (BigNAS [40] in Fig. 1).
& BigNAS [40].
Thus, in order to make once-for-all
training more efficient, we must reduce its training time without sacrificing
state-of-the-artaccuracyacrossthewholeoperatinglatency/FLOPrangeofthe
supernet.
We propose DϵpS, a technique that increases the scalability of once-for-all
training. It consists of three key components designed to meet their respective
goals — Full Model warmup (FM-Warmup) provides better supernet initial-
ization, E-Shrinking keeps the accuracy of the full model (largest subnet that
A FO
SSpANgiB
DDϵpS: Delayed ϵ-Shrinking 3
contains all the supernet parameters) on par with OFA and BigNAS, and IKD-
Warmup boosts the accuracy of small subnets with effective knowledge distilla-
tioninonce-for-alltraining.Particularly,withbettersupernetinitialization,FM-
Warmup (DϵpS in Fig. 1) reduces both the total number of epochs (compared
to OFA) and average time per-epoch (compared to BigNAS). In FM-Warmup,
thesupernetisinitializedwiththepartiallytrainedfullmodel(∼50%)andthen
subnet sampling (shrinking) is started to train the model family. The partial
full model training ensures a lower time per epoch initially. Then, E-Shrinking
ensures smooth optimization of the full model. It incrementally warms up the
learning rate of subnets using parameter E when the shrinking starts, while
keeping the learning rate of the full model higher. Lastly, IKD-Warmup enables
knowledge distillation from multiple partially trained full models (that are pro-
gressively better) to smaller subnets. The three components, when combined,
reduce the training time of once-for-all training and outperform state-of-the-art
w.r.t. accuracy of subnets across different datasets and neural network architec-
tures. We summarize the contributions of our work as follows:
• FM-Warmup provides better initialization to the weight shared supernet by
training the full model only partially and delaying model shrinking. This leads
to reduced time per epoch and lower training cost.
• E-Shrinkingensuressmoothandfastoptimizationofthefullmodelbywarm-
ing up the learning rate of smaller subnets. This enables it to reach optimal
accuracy quickly.
• IKD-Warmup provides rich knowledge transfer to subnets, enabling them to
quickly learn good representations.
We extensively evaluate DϵpS against existing once-for-all training baselines
[3,29,40] on CIFAR10/100 [21], ImageNet-100 [34] as well as ImageNet-1k [7]
datasets.DϵpSoutperformsallbaselinesacrossalldatasetsbothw.r.t.accuracy
(of subnets) and training cost. It achieves 1.83% ImageNet-1k top1 accuracy
improvement or the same accuracy with 1.3x FLOPs reduction while reducing
trainingcostbyupto1.8xw.r.t.OFAand2.5xw.r.t.BigNAS(indollarsorGPU
hours).Wealsoprovideadetailedablationstudytodemonstratethebenefitsof
DϵpS components in isolation.
2 Background
Formulation. Let W denote the supernet’s weights, the objective of once-for-
o
all training is given by —
(cid:88)
min L(S(W ,a)) (1)
o
Wo
a∈A
where S(W ,a) denotes weights of subnet a selected from the supernet’s weight
o
W and A represents the set of all possible neural architectures (subnets). The
o
goal of once-for-all training is to find optimal supernet weights that minimize
the loss (L) of all the neural architectures in A on a given dataset.4 Annavajjala et al.
Challenges. However, optimizing (1) is non-trivial. On one hand, enumerat-
ing gradients of all subnets to optimize the overall objective is computationally
infeasible. This is due to the large number of subnets optimized in once-for-all
training(|A|≈1019 subnetsin[3]).Ontheotherhand,anaiveapproximationof
objective(1)tomakeitcomputationallyfeasibleleadstointerference (sampling
a few subnets in each update step). Interference occurs when smaller subnets
affect the performance of the larger subnets [3,40]. Hence, interference causes
sub-optimal accuracy of the larger subnets. Existing once-for-all training tech-
niques mitigate interference by increasing the training time significantly (Fig.
1). For instance, OFA [3] mitigates interference by first training the full model
(largestsubnet)andthenprogressivelyincreasingthesizeof|A|.Thisleadstoa
large number of training epochs and ≈1200 GPU hours to perform once-for-all
training. Therefore, the following challenges remain in once-for-all training —
(C1) training supernet at a lesser training cost than SOTA, and (C2) mitigat-
ing interference. We divide challenge C2 into two sub-challenges — matching
existing once-for-all training techniques [3,40] w.r.t. accuracy of (C2a) the full
model (largest subnet), and (C2b) child models (smaller subnets).
3 Related Work
Efficient NN-Architectures in Deep Learning. Efficient deep neural net-
works(NNs)achievehighaccuracyatlowFLOPs.Theseneuralnetsareeasyto
deploy as they increase hardware efficiency by operating at low FLOPs. Devel-
oping such networks is an active research area. Several efficient neural networks
includeMobileNets[15],SqueezeNets [17],EfficientNets[33],and TinyNets[10].
Neural network compression. Neural network compression reduces the size
and computation of neural networks for efficient deployment. The compression
occursafterthenetworkistrained.Hence,theperformanceofcompressionmeth-
ods is bounded by the accuracy of the trained neural network. Neural network
compression can be broadly divided into two categories — network pruning
and quantization. Network pruning removes unimportant units [11,25,30] or
channels [22,23,31]. Network quantization converts the representation of neural
weights and activations to low bits [16,20,37].
Hardware aware NAS. Neural architecture search (NAS) automates the de-
signofefficientNNarchitectures.NAStypicallyinvolvessearchingforandtrain-
ingNNarchitecturesthataremoreaccuratethanmanuallydesignedNNs[27,42].
Recently, NAS methods are becoming hardware-aware [4,32,38] i.e. they find
NN architectures suited for deployment at target hardware. These methods in-
corporate deployment constraints of hardware or latency in their search. Then,
theyfindandtrainefficientNNsthatmeettheconstraints.However,theseNAS
methods only satisfy a single deployment target. They need to run repeatedly
for each deployment target that doesn’t scale well.
Once-For-All Training. Once-for-all training is a scalable NAS method that
satisfies multiple deployment targets. It co-trains models (subnets) that vary
in shape and size embedded inside a single supernet (weight-shared). NAS isDϵpS: Delayed ϵ-Shrinking 5
80 80
DpS initialization (30%)
90 70 70 D D p pS S i in ni it ti ia al li iz za at ti io on n ( (5 70 0% %) )
60 OFA initialization
80 60
50
70 50 40
40 30
60
20
30
50 DpS initialization (50%) DpS initialization (65%) 10
OFA initialization 20 OFA initialization 0
40 60 80 100120140160180 40 60 80 100120140160180 200 300 400 500
FLOPs (M) FLOPs (M) FLOPs (M)
(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet-1k
Fig.2: Supernetwork initialization.DϵpSprovidesbetterinitializationforthesu-
pernetwork for smaller subnets compared to OFA due to FMWarmup. This validates
thehypothesisthatthesupernetweightsbecomespecializedifthefullmodelistrained
tocompletion(OFA),resultinginpooreraccuracyofsubnetworkswithincreasedtrain-
ing of the full model.
performed later by extracting specialized subnets from the trained supernet for
targethardware.Someoftheproposedonce-for-alltrainingmethodsareOFA[3],
BigNAS [40], and CompOFA [29]. OFA performs Progressive Shrinking (PS)
for once-for-all training that trains the full model first and then progressively
introduces smaller subnets into the training by dividing the training procedure
intomultipletrainingjobs(phases).ComparedtoOFA,DϵpSperformsonce-for-
all training as a single training job and starts shrinking from a partially trained
full model to reduce the training cost. BigNAS starts the process of shrinking
earlyandsamplesmultiplesubnetsateveryminibatch.Incontrast,DϵpSinitially
only trains the full model and delays the shrinking. Finally, CompOFA changes
the architecture search space of OFA and performs Progressive Shrinking with
reduced phases. DϵpS algorithmically changes the shrinking procedure in once-
for-all training and is complementary to architecture space changes proposed in
CompOFA.
4 Proposed Approach
We present DϵpS, a once-for-all training technique that trains supernets in less
training time. DϵpS consists of three key components that meet the challenges
C1 and C2. We describe each component in detail and highlight the core con-
tributions of our work.
4.1 Full-Model Warmup Period (Pfm ): When to Shrink the Full
warmup
Model?
Shrinkingthefullmodelatanappropriatetimeisvitalforreducingtrainingcost
(meet C1). Both early or late shrinking isn’t sufficient to meet the challenges
in once-for-all training. Early shrinking (BigNAS [40] in Fig. 1) doesn’t meet
the challenge C1. It increases the overall training time as multiple subnets are
ycaruccA
1-poT
ycaruccA
1-poT
ycaruccA
1-poT6 Annavajjala et al.
sampled in each update (increasing per-epoch time) to optimize objective (1).
Early shrinking also requires a lot of hyper-parameter tuning to meet challenge
C2. It becomes sensitive to training hyper-parameters due to interference. For
instance,trainingthefullmodelwithearlyshrinkingbecomesunstablewiththe
standard initialization of the full model [40].
Ontheotherhand,ifshrinkinghappenslateafterthefullmodeliscompletely
trained (OFA [3] in Fig. 1), the supernet weights become too specialized for the
full model architecture and require a large number of training epochs to reduce
interference. Hence, late shrinking meets challenge C2 but not C1.
We argue that shrinking should occur after the full model is partially trained
(warmed up, trained at least 50%, proposed approach in Fig. 1).
Delayed Shrinking has numerous advantages. It reduces the overall training
time to meet challenge C1. The initial updates in DϵpS are cheap compared to
early shrinking as only the full model gets trained and no subnets are sampled.
Moreover, since supernet weights are not specialized for the full model, DϵpS
can meet challenge C2 in less number of epochs. To validate our hypothesis, we
ask whether a partially trained full model serves as a good initialization for the
supernet. To do this, we compare the accuracy of small subnets (shrinking) on
multiple datasets (CIFAR-10, CIFAR-100, ImageNet-1k) in a mobilenet-based
supernet [3] when initialized with a partially trained (50%), and completely
trained full model (∼600 MFLOPs) in Fig. 2.
ThetakeawayfromtheexperimentinFig.2isthatapartially-trainedfullmodel-
based initialization performs better for smaller subnets than the initialization
with the full model completely trained. This validates our hypothesis that su-
pernetweightsbecometoospecializedifthe fullmodelistrainedtocompletion.
Hence, warming up the full model helps in meeting challenge C1. We introduce
a hyperparameter Pfm in DϵpS that denotes the percentage of total epochs
warmup
that were used to warmup the full model. Pfm is usually kept ≥ 50% in
warmup
DϵpS.
4.2 E-Shrinking: Learning Rates for Subnets
In addition to the full model warmup, we propose E-Shrinking that enables the
full model to reach comparable accuracy with SOTA and meet challenge C2a.
E-Shrinking ensures that the full model’s accuracy doesn’t get affected when
shrinking is introduced in between its training. When the shrinking starts, the
learning rate of subnets is gradually ramped to reach the full model’s learning
rate (E-Shrinking) as the full model gets sampled with other subnets in each
update step.
Without the gradual warmup, the full model becomes prone to an accuracy
drop as the supernet weights change rapidly at the start of shrinking. To under-
stand this change, we compare the updates in the supernet with and without
shrinking for a minibatch B. Consider supernet weights W at iteration t. With-
t
out shrinking, the update is given by -DϵpS: Delayed ϵ-Shrinking 7
WnoShrink =W −η ∇l (S(W ,a )) (2)
t+1 t t B t full
(cid:124) (cid:123)(cid:122) (cid:125)
=GB,t
noShrink
where l (S(W ,a ) denotes the loss of the full model on minibatch B and
B t full
equals 1 (cid:80) l(x,S(W ,a ));xdenotesthesamplesinB.η denotesthelearn-
|B| t full t
x∈B
ingrateatiterationtusedtoupdatetheweights.Whereasintroducingshrinking
for the same supernet weights W yields the following update -
t
 
shrinking
(cid:122) (cid:125)(cid:124) (cid:123)
W tS +h 1rink =W t−η t  (cid:88) ∇l B(S(W t,a))  (3)
 
a∈Uk(A)
(cid:124) (cid:123)(cid:122) (cid:125)
=GB,t
Shrink
where U (A) denotes uniformly sampling k subnets from the architecture space
k
A. This update step is the approximation of the objective (1). Clearly, the up-
datesdiffer,itisimprobable thatWShrink =WNoShrink.Thisdifferenceinupdates
t+1 t+1
causesthesupernetweightstochangerapidlywhenshrinkingisintroduced.The
rapid change in supernet weights causes degradation in the full model’s accu-
racy. To avoid rapid changes in weights, a widely adopted technique is to use
less aggressive learning rates via learning rate warmup schedules [9,13].
However, applying such principles in the context of weight-sharing is non-
trivial but at the same time important. Our key idea is two-fold to a) always
sample the full model with other subnets while shrinking, and b) use less ag-
gressive learning rates for subnets at the start of shrinking. Particularly, it is
important to ensure GB,t ≈ GB,t to make WShrink ≈ WNoShrink initially
noShrink Shrink t+1 t+1
when the shrinking starts. To do this, we introduce a parameter E that con-
trols the effective learning rate of subnets and makes GB,t ≈ GB,t . The
noShrink Shrink
gradient in E-Shrinking is given as follows -
E−shrinking
(cid:122) (cid:125)(cid:124) (cid:123)
GB,t (E )=GB,t +E ∗ (cid:88) ∇l (S(W ,a)) (4)
Shrink t noShrink t B t
a∈Uk−1(A\{afull})
where E ∈ (0,1]. Note that the effective learning rate becomes η ∗E for sub-
t t t
nets and remains η for the full model in E-Shrinking. Hence, slowly increas-
t
ing E warms up the effective learning of subnets. We start with a small value
t
of E (=10−4) and increment it by a constant amount to reach 1. Once E
t t
reaches 1, it stays constant for the rest of the training. We empirically verify
ifGB,t ,GB,t differinmagnitude(l -norm)anddirection(cosinesimilar-
noShrink Shrink 2
ity)and whetherE-ShrinkingisabletoreducethedifferenceswithGB,t (E ).
noShrink t
Fig. 3 compares the magnitude and direction of the gradients of the full model
(G ), shrinking (G ) and E-Shrinking (G (E)) (E = 0.001) on
noShrink Shrink noShrink
the weights of a mobilenet-based supernet [3] for the ImageNet dataset [28].
G andG differbothinmagnitudeanddirectionacrosssupernetlay-
noShrink Shrink
ers.8 Annavajjala et al.
G
GShrink GShrink()
noShrink 1.0
G
Shrink 0.8
G Shrink()
0.6
0.4
0.2
1 2 13 18 0
1 2 13 18
# Layer # Layer
(a) Magnitude (||.|| 2) (b) Direction (cos. sim.)
Fig.3:Gradientsw/&w/oShrinkingonMobilenet-BasedSupernet.Delayed
Shrinkingcausesgradients(G )todifferfromthefullmodelgradient(G )
Shrink noShrink
leadingtorapidchangesinthesupernet’sweights.E-Shrinking’sgradient(G (E))
Shrink
reduces such differences and avoids rapid weight changes.
3.0
ThemagnitudeofG isanorder
Shrink
of magnitude higher than G 2.5
noShrink
for early layers. E-Shrinking main- 2.0 wo/ -Shrinking
tains the low magnitude of gradient w/ -Shrinking
throughout the training as shown in 1.5
Fig. 4. The magnitude of G Shrink is 1.0
consistently higher than G (E )
Shrink t 0 625 1250 1875 2500
when normalized with the magni- Shrinking Step # (t)
tude of G . Such differences
noShrink Fig.4: Gradient Magnitude Over
causepoorconvergenceatthestartof
Time. Gradient magnitude with
shrinking and often lead to accuracy
(G (E,t)) and without (G (t)) E-
shrink shrink
drops.Whereas,G (E)hasmin-
noShrink shrinking is compared w.r.t the initial full
imal differences w.r.t. G noShrink en- model gradient (G Noshrink) over shrinking
ablinghealthyconvergenceandnopo- steps. E-shrinking avoids sudden changes
tential accuracy drops. inthesupernetparametersbyloweringthe
gradient magnitude.
4.3 IKD-Warmup: In-Place Knowledge Distillation (KD) from
Warmed-up Full Model
We now discuss IKD-Warmup that distills knowledge from the full model to
subnets and meets challenge C2b. Effectively distilling the knowledge from the
fullmodelbecomesnon-trivialduetoweight-sharing.Ononehand,KDrequires
the supernet weights biased to the full model to offer meaningful knowledge
transfer to subnets. On the other hand, having a large bias in the supernet
weights toward the full model may result in subnets’ sub-optimal performance
sincetheweightsareshared.Totacklethistrade-off,OFA[3]biasesthesupernet
weights to a trained full model and then uses it to perform vanilla-KD [14].
However, this results in a long training time during shrinking as the supernet
weights are trained to fit subnets’ architectures. Another approach like BigNAS
||.||
2
2−01
3−01
knirhSonG
trw
soc
2||)t(
||
2||knirhSoN
||DϵpS: Delayed ϵ-Shrinking 9
[40] doesn’t bias the shared weights to the full model by using inplace-KD [39]
but lacks in providing rich knowledge transfer to subnets (initially).
This is because inplace-KD distills the knowledge "on the fly" to other subnets
as the full model gets trained from randomly initialized weights. Precisely, the
full model predictions become ground truth for other subnets. Hence, when the
full model is under-trained initially, it doesn’t offer rich knowledge transfer.
We believe that the proposed delayed shrinking has an added advantage w.r.t.
KDforonce-for-alltraining—thepartiallytrainedfullmodel(50/60%trained)
is rich enough to provide meaningful knowledge transfer to the subnets and
doesn’t bias the supernet weights to the full model. It has been shown that for
vanilla-KD [14], partially trained (intermediate) models provide a comparable
orattimesbetterknowledgetransferthanthecompletelytrainedmodels[5,36].
Thisisbecausetheyprovidemoreinformationaboutnon-targetclassesthanthe
trained models [5]. We use this insight in DϵpS that performs inplace-KD from
a partially trained full model (IKD-Warmup).
IKD-Warmup offers two advantages, it — a) distills knowledge from multiple
progressivelybetterpartiallytrainedmodelsasthefullmodelgetstrained(unlike
a single partially/fully trained model used in vanilla-KD [36]), and b) provides
rich knowledge transfer to the subnets at all times (unlike inplace-KD [39] that
uses under-trained full model initially).
5 Experiments
We establish that DϵpS a) reduces training cost w.r.t. SOTA in once-for-all
training [3,29,40], b) performs at-par or better than SOTA’s accuracy across
subnets (covering the entire range of architectural space), c) generalizes across
datasets, d) generalizes to different deep neural network (DNN) architecture
spaces,ande)producesspecializedsubnetsfortargethardwarewithoutretrain-
ing(once-for-allproperty).Wealsoaimtodemonstrateattributionofbenefitsin
DϵpS by providing detailed ablation on a) a full model warmup period: empiri-
cally demonstrating a sweet spot, b) E-Shrinking: showing healthy convergence,
and c) IKD-Warmup: distilling knowledge better than existing distillation ap-
proaches in weight-sharing.
5.1 Setup
Baselines. We first compare DϵpS with the other NAS methods or efficient
DNNs [4,15,33,35] w.r.t. accuracy. Then, We compare DϵpS with once-for-
all training techniques — OFA [3], BigNAS [40], CompOFA [29] w.r.t. both
training cost and accuracy of subnets spanned across supernet’s FLOP range.
The training time of all the techniques is measured on NVIDIA A40 GPUs. As
once-for-alltrainingtrainsmultiplesubnets,thecomparisonisdonebyuniformly
dividing the entire FLOP range into 6 buckets and picking the most accurate
subnet from each bucket for every baseline.10 Annavajjala et al.
Group Approach MACs (M) Top-1 Test Acc (%)
OFA [3] 67 70.5
0-100 (M)
DϵpS 67 72.3
OFA [3] 141 71.6
100-200 (M)
DϵpS 141 73.7
FBNetv2 [35] 238 76.0
BigNAS [40] 242 76.5
200-300 (M)
OFA [3] 230 76
DϵpS 230 77.3
MNasnet [32] 315 75.2
ProxylessNAS [4] 320 74.6
300-400 (M) FBNetv2 [35] 325 77.2
MobileNetV3 [15] 356 76.6
EfficientNetB0 [33] 390 77.3
Table 1: Comparison of DϵpS with state of the art neural architecture search ap-
proaches on Imagenet. DϵpS consistently outperforms the baselines.
Success Metrics. DϵpS is compared against the baselines on the following
success metrics — a) Training cost measured in GPU hours or dollars (lower is
better),b)Pareto-frontier:Accuracyofbest-performingsubnetsasafunctionof
FLOPs/latency. To compare Pareto-frontiers obtained from different baselines,
we use a metric called mean pareto accuracy that is defined as the area under
the curve (AUC) of accuracy and normalized FLOPs/latency. The higher the
mean pareto accuracy the better.
Datasets. We evaluate all methods on CIFAR10/100 [21], ImageNet-100 [34]
andImageNet-1k[7]datasets.Thecomplexityofdatasetsprogressivelyincreases
from CIFAR10 to ImageNet-1k. The datasets vary in the number of classes,
image resolution, and number of train/test samples.
DNN Architecture Space. All methods are trained on the supernets derived
from two different DNN architecture spaces — MobilenetV3 [15] and Proxyless-
NAS [4] (same as OFA [3]). The base architecture of ProxylessNAS is derived
from ProxylessNAS run for the GPU as a target device. To avoid confounding,
we evaluate all baselines on the same DNN architecture space.
TrainingHyper-parameters.Thetraininghyper-parametersofDϵpSaresim-
ilar to the hyper-params of the full model training. The hyper-parameters for
MobilenetV3,andProxylessNAStrainingareborrowedfrom[15]and[4]respec-
tively. Specifically, we use SGD with Nesterov momentum 0.9, a CosineAnneal-
ing LR [24] schedule, and weight decay 3e−5. Unless specified, the shrinking is
introduced in DϵpS after the full model gets ∼50% trained.DϵpS: Delayed ϵ-Shrinking 11
SmallestSubnet LargestSubnet TrainingCost
Approach meanparetoacc.
Acc(%)MACs(M)Acc(%)MACs(M) #EpochsAvg. Total DollarCost($)
GPU Time
min. /(GPU
epoch hours.)
OFA[3] 71.8 150 77.2 230 75.77 605 125 1256 2675
CompOFA[29] -∗ 150 -∗ 230 -∗ 330 142 782 1665
BigNAS[40] 70.6 150 74 230 72.51 400 266 1778 3787
DϵpS 73.8 150 77.3 230 75.81 270 155 700 1491
Table 2: Comparison of DϵpS vs SOTA on ImageNet. Accuracy and Training
CostcomparisonofDϵpSagainstSOTAapproachesareshownforMobilenetV3-based
architecture space. DϵpS outperforms SOTA and achieves 2% better accuracy for the
smallest subnet and is at-par with the largest subnet (full model) respectively at 1.8x
trainingcostreduction(in$)comparedtoOFA.Dollar-costiscalculatedbasedonthe
on-demand prices for A40 GPUs from exoscale.com
5.2 Evaluation
Comparison with NAS methods/Efficient Nets on ImageNet. We com-
pare DϵpS with MobilenetV3 [15], FBNet [35], ProxylessNAS [4], BigNAS [40]
and efficient nets [33] on the Imagenet Dataset.
Takeaway. Tab. 1 compares accuracy vs MACs of the baselines. DϵpS consis-
tentlysurpassesthebaselinesovermultipleMACranges.Especiallyinthelower
MAC region (0-100M), DϵpS is 1.8% more accurate. Moreover, in the larger
MAC region (200-300M), DϵpS achieves 77.3% accuracy with upto 1.69x MACs
improvement compared to the baselines (efficientNet-B0). DϵpS benefits from
supernet initialization and effective knowledge distillation to get superior per-
formance.
Comparison with Once-for-all training methods on ImageNet We now
demonstratetheaccuracyandtrainingcostbenefitsofDϵpSonImageNetdataset
[28]. Tab. 2 compares DϵpS with the baselines∗ on a) the upper-bound (largest
subnet) and lower-bound (smallest subnet) top1 accuracy, and b) GPU hours
and dollar costs.
Takeaway. DϵpS is atleast 2% more accurate at 150 MACs (smallest subnet)
than baselines and at-par w.r.t. accuracy at 230 MACs (largest subnet). DϵpS
matchesthePareto-optimalityofbaselines(withhighestmeanparetoaccuracy)
at a reduced training cost (least among all the baselines). It takes 1.8x and 2.5x
less dollar cost (or GPU hours) than OFA and BigNAS respectively.
ThetrainingcostimprovementofDϵpScomesduetoFM-Warmup.FM-Warmup
allows DϵpS to train subnets in less number of total epochs (lowest among the
baselines) and a lower average time per epoch than BigNAS (Tab. 2). The full
model’s accuracy (largest subnet in Tab. 2) is improved as E-Shrinking enables
its smooth convergence. Finally, DϵpS improves accuracy at lower FLOPs (150
MACs) as IKD-Warmup distills knowledge effectively in once-for-all training.
Generalization across datasets. We establish that the accuracy improve-
ments of DϵpS generalize to other vision datasets.
∗ Thereisnoavailableopen-sourcecheckpointofCompOFA[29].CompOFAclaimsto
matchOFA’sPareto-optimality.Hence,wereportPareto-frontierofOFA[3]instead.12 Annavajjala et al.
DpS BigNAS OFA/CompOFA
DpS BigNAS OFA/CompOFA
95.0 DpS BigNAS OFA/CompOFA 78 85.0 DpS BigNAS OFA/CompOFA 77
999 344 ... 505 1.36%2.4x 76 2.1%2.3x 888 344 ... 505 77 56
99 23 .. 50 74 888 223 ... 050 1.4x 77 34 11 .8.3 5x %
92.0 72 81.5 1.34% 72
91.5 81.0 71
91.0 50 75 F1 L0 O0 Ps1 2 (M5 )150175 50 75 F1 L0 O0 Ps1 2 (M5 )150175 200 3 FL0 O0 Ps4 0 (M0 )500 200 30 F0
LOPs
4 (M00
)
500
(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet-100 (d) ImageNet-1k
Fig.5:DϵpS’sAccuracyImprovementacrossDatasets.ThecomparisonofDϵpS
with the baselines is shown w.r.t. accuracy (of subnets) for CIFAR10/100, ImageNet-
100,andImageNet-1kdatasets.DϵpSconsistentlyoutperformsthebaselinesacrossall
thedatasetsandachievesupto2.1%betteraccuracyforthesameFLOPsorupto2.3x
FLOP reduction at same accuracy.
Training Details.DϵpSusesthestandardhyper-parametersoftheMobileNetV3
for all the datasets using SGD with cosine learning rate decay and nestrov mo-
mentum, and shrinking is introduced when the full model is 50% trained. For
OFA, we first train the largest network independently. Shrinking occurs after
the full model is completely trained and vanilla KD is used for distillation. The
depthandexpandphasesarerunfor100epochseach.Theinitiallearningrateof
differentphasesissetasperOFA[3].BigNASusesRMSPropoptimizerwithits
proposed hyper-parameters for ImageNet-1k. However, we use SGD optimizer
in BigNAS for CIFAR10/100 and ImageNet-100 datasets as we empirically find
thatSGDperformsbetterthanRMSProponthesedatasets.Fig.5comparesthe
Pareto-frontiers of top1 test accuracy and FLOPs obtained from each baseline
across various datasets. The subnets are present in six different FLOP buckets
that uniformly divide the supernet’s FLOP range. The comparison includes the
performanceofthesmallestandlargestsubnetstomeasurethelower-boundand
upper-bound test accuracy reached by the baselines.
Takeaway. DϵpS outperforms baselines w.r.t. accuracy of smaller subnets (≤
300 MFLOPs) on all the datasets. It achieves slightly better or at-par accu-
racy for larger subnets (≥ 300 MFLOPs) than OFA/CompOFA. DϵpS outper-
forms BigNAS and achieves a better Pareto-Frontier across all the datasets.
Generalization across DNN-
Architecture Spaces. We demon-
1250 74
strate that DϵpS generalizes to other 1.77x 1000 73
DNN-architecture spaces. We train
750
DϵpS on ImageNet-1k dataset using 72 500
ProxylessNAS-based supernet (DNN- 250 71 DpS
OFA
architecture space) with training- 0 DpS OFA 70
hyperparameters borrowed from [4]. FLOPs (M)
Fig. 6 compares Pareto-frontiers ob-
Fig.6: DϵpS on ProxyLessNAS archi-
tained from DϵpS and OFA on
tecture space: superior Pareto-Frontier
ImageNet-1k dataset. with a 1.8% improvement in ImageNet-1k
Takeaway. DϵpS outperforms OFA test accuracy on the smallest subnet.
w.r.t. ImageNet-1k test accuracy
ycaruccA
tseT
ycaruccA
tseT
ycaruccA
tseT
)sruoh(
UPG
ycaruccA
tseT
ycaruccA
tseT
350 400 450DϵpS: Delayed ϵ-Shrinking 13
Pareto Curve Full Model Training IKD Warmup Inplace KD
DpS (50%) DpS (75%) DpS (25%) 77
77 70 76 76 60 75 76
50 74
75 74 40 73 FMWarmup
74 w/ -shrinking 30 w/ -shrinking 72
72 wo/ -shrinking 20 wo/ -shrinking 71
73 200 300 400 500 0 50 100150200250
200 30 F0 LOPs 4 (0 M0 ) 500 FLOPS (M) Epoch 200 30 F0 LOPs 4 (M00 ) 500
(a) P wf am rmup (b) E-Shrinking (c) Distillation
Fig.7: DϵpS Ablations.ThreeablationsareshownforDϵpS—Fullmodelwarmup
period (Pfm ), E-Shrinking, and Distillation. a) There exists a sweet spot w.r.t.
warmup
accuracy (of subnets) in Pfm (=50%), b) E-Shrinking improves the entire pareto
warmup
front (left) and prevents drop in accuracy of the full model (right), c) IKD-Warmup
performs better than Inplace KD as it uses more information from non-target classes
(further details are provided in supplementary material).
(with0.5%bettermeanparetoaccuracy).Itimprovestheaccuracyofthesmall-
est subnet by 1.8%. The accuracy improvements come with 1.8x training cost
reduction compared to OFA.
5.3 Ablation Study
WeprovidedetailedablationonDϵpScomponents—FM-Warmup,E-Shrinking,
and IKD-Warmup to attribute their benefits.
Full Model Warmup Period (Pfm ). In this ablation, we establish the
warmup
benefitsofdelayedshrinkingasopposedtoearlyorlateshrinking.Todothis,we
configure DϵpS to run with different full model warmup periods (Pfm ) – the
warmup
time at which shrinking starts in DϵpS. Our goal is to empirically demonstrate
the existence of a sweet spot in Pfm w.r.t. accuracy (of subnets). Fig. 7a
warmup
comparestheaccuracyofbest-performingsubnetsinsixdifferentFLOPbuckets
of three Pfm periods {25%, 50%, 75%} on Imagenet-1k dataset. Pfm
warmup warmup
=25%, 75% represents early and late shrinking respectively.
Takeaway. DϵpS with Pfm = 50% achieves the best test accuracy across
warmup
subnets compared to DϵpS configured to run with Pfm =25%,75%. Hence,
warmup
a sweet spot exists in Pfm . The existence of a sweet spot demonstrates that
warmup
both early (25%) or late (75%) shrinking is sub-optimal in training the model
family (discussed in §4.1). Early shrinking results in sub-optimal accuracy of
the larger subnets as training interference occurs very early in the training.
While late shrinking causes the specialization of supernet weights to the full
model architecture that results in sub-optimal accuracy of smaller subnets (≈
1% accuracy degradation around 200 MFLOPs for Pfm =75% compared to
warmup
Pfm =50%).
warmup
E-Shrinking.Weinvestigatewhetheranaccuracydropoccursinthefullmodel’s
accuracywhenshrinkingisintroducedinDϵpSandifE-Shrinkingpreventsit.In
this ablation, we run DϵpS with and without E-Shrinking and introduce shrink-
ing at 150th epoch while keeping all other training-hyperparameters constant.
ycaruccA
tseT
ycaruccA
tseT
ycaruccA
tseT
ledoM
lluF
ycaruccA
tseT14 Annavajjala et al.
Fig. 7b (right) compares DϵpS with and without E-Shrinking on ImageNet-1k
top1testaccuracyofthefullmodelovertrainingepochs.Fig.7b(left)compares
subnets for six different FLOP buckets with and without E-Shrinking.
Takeaway. DϵpS without E-Shrinking observes a 2% drop in full model’s ac-
curacy at 150th epoch when the shrinking starts. And, DϵpS with E-Shrinking
prevents this huge accuracy drop at the start of shrinking that leads to better
full model accuracy overall. The prevention of the drop in full model’s accuracy
demonstrates that E-Shrinking leads to smooth optimization of the full model.
E-Shrinkingachievesthisbyincrementallywarmingupsubnets’learningrateat
the start of shrinking to avoid sudden changes in the supernet weight (Fig. 7b,
right).E-ShrinkingalsoachievessuperioraccuracyacrosstheentireFLOPrange
when compared to the supernet trained without E-Shrinking (Fig. 7b, left).
IKD-Warmup. We assess the benefits of IKD-Warmup in this ablation. IKD-
Warmup performs inplace knowledge distillation from a partially trained full
model instead of performing it from the beginning with a randomly initialized
full model (inplace KD) as proposed in [41]. Hence, to show benefits of IKD-
Warmup, we run DϵpS with inplace KD and our proposed IKD-Warmup. Fig.
7c compares DϵpS run with IKD-Warmup (blue) and inplace KD (orange) on
theImageNet-1ktop1testaccuracyofbest-performingsubnetsinsevendifferent
FLOP buckets.
Takeaway. IKD-Warmup outperforms inplace KD across all the subnets that
cover the supernet’s FLOP range on the ImageNet-1k dataset. It is 3.5% and
2% more accurate at 560 MFLOPs and 150 MFLOPs respectively. This shows
that IKD-Warmup distills knowledge effectively in once-for-all training as mul-
tipleprogressivelybetterpartiallytrainedfullmodeltransfertheirknowledgeto
smaller subnets (§4.3). Inplace KD is not able to provide meaningful knowledge
transfer as the full model is under-trained initially.
6 Conclusion
DϵpSisatrainingtechniquethatincreasesthescalabilityofonce-for-alltraining.
DϵpS consists of three key components — FM-Warmup that decreases training
costs,E-Shrinkingthatkeepstheaccuracyofthefullmodelon-parwithexisting
works, and IKD-Warmup that performs effective knowledge distillation in once-
for-all training. FM-Warmup’s key idea is to delay the process of shrinking till
thefullmodelgetspartiallytrained(∼50%)toreducetrainingcost.E-Shrinking
circumvents accuracy drop in the full model by avoiding rapid changes in the
supernet weights and enabling smooth optimization by incrementally warming
up subnets’ learning rates. IKD-Warmup provides rich knowledge transfer to
subnets from multiple partially trained full models that are progressively better
w.r.t. accuracy. DϵpS generalizes to different datasets and DNN architecture
spaces. It improves the accuracy of smaller subnets, achieves on-par Pareto-
optimality, and reduces training cost by upto 2.5x when compared with existing
once-for-all weight-shared training techniques.DϵpS: Delayed ϵ-Shrinking 15
References
1. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. CoRR abs/1803.01271 (2018),
http://arxiv.org/abs/1803.01271
2. Bonnard, J., Abdelouahab, K., Pelcat, M., Berry, F.: On building a cnn-based
multi-view smart camera for real-time object detection. Microprocessors and Mi-
crosystems 77, 103177 (2020)
3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-all: Train one network
andspecializeitforefficientdeployment.In:InternationalConferenceonLearning
Representations (2020), https://openreview.net/forum?id=HylxE1HKwS
4. Cai,H.,Zhu,L.,Han,S.:Proxylessnas:Directneuralarchitecturesearchontarget
task and hardware. arXiv preprint arXiv:1812.00332 (2018)
5. Cho, J.H., Hariharan, B.: On the efficacy of knowledge distillation. In: Proceed-
ingsoftheIEEE/CVFinternationalconferenceoncomputervision.pp.4794–4802
(2019)
6. Cob-Parro, A.C., Losada-Gutiérrez, C., Marrón-Romera, M., Gardel-Vicente, A.,
Bravo-Muñoz, I.: Smart video surveillance system based on edge computing. Sen-
sors 21(9), 2958 (2021)
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)
8. Gog,I.,Kalra,S.,Schafhalter,P.,Wright,M.A.,Gonzalez,J.E.,Stoica,I.:Pylot:A
modularplatformforexploringlatency-accuracytradeoffsinautonomousvehicles.
In:2021IEEEInternationalConferenceonRoboticsandAutomation(ICRA).pp.
8806–8813. IEEE (2021)
9. Goyal,P.,Dollár,P.,Girshick,R.,Noordhuis,P.,Wesolowski,L.,Kyrola,A.,Tul-
loch, A., Jia, Y., He, K.: Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 (2017)
10. Han,K.,Wang,Y.,Zhang,Q.,Zhang,W.,Xu,C.,Zhang,T.:Modelrubik’scube:
Twistingresolution,depthandwidthfortinynets.AdvancesinNeuralInformation
Processing Systems 33, 19353–19364 (2020)
11. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 (2015)
12. Hashemi, H.B., Asiaee, A., Kraft, R.: Query intent detection using convolutional
neuralnetworks.In:Internationalconferenceonwebsearchanddatamining,work-
shop on query understanding (2016)
13. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016)
14. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 (2015)
15. Howard,A.,Sandler,M.,Chu,G.,Chen,L.C.,Chen,B.,Tan,M.,Wang,W.,Zhu,
Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: Proceedings of
theIEEE/CVFinternationalconferenceoncomputervision.pp.1314–1324(2019)
16. Hubara,I.,Courbariaux,M.,Soudry,D.,El-Yaniv,R.,Bengio,Y.:Binarizedneural
networks. Advances in neural information processing systems 29 (2016)
17. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet:Alexnet-levelaccuracywith50xfewerparametersand<0.5mbmodel
size. arXiv preprint arXiv:1602.07360 (2016)16 Annavajjala et al.
18. Inc., N.: Nvidia jetson. https://www.nvidia.com/en-in/autonomous-machines/
embedded-systems/, [Accessed 13-May-2023]
19. Inc, N.: Nvidia v100. https://www.nvidia.com/en-in/data-center/v100/, [Ac-
cessed 13-May-2023]
20. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,
Kalenichenko,D.:Quantizationandtrainingofneuralnetworksforefficientinteger-
arithmetic-only inference. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2704–2713 (2018)
21. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009)
22. Li,H.,Kadav,A.,Durdanovic,I.,Samet,H.,Graf,H.P.:Pruningfiltersforefficient
convnets. arXiv preprint arXiv:1608.08710 (2016)
23. Lin,M.,Ji,R.,Wang,Y.,Zhang,Y.,Zhang,B.,Tian,Y.,Shao,L.:Hrank:Filter
pruningusinghigh-rankfeaturemap.In:ProceedingsoftheIEEE/CVFconference
on computer vision and pattern recognition. pp. 1529–1538 (2020)
24. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.
arXiv preprint arXiv:1608.03983 (2016)
25. Luo, J.H., Wu, J., Lin, W.: Thinet: A filter level pruning method for deep neural
network compression. In: Proceedings of the IEEE international conference on
computer vision. pp. 5058–5066 (2017)
26. Ouyang, Z., Niu, J., Liu, Y., Guizani, M.: Deep cnn-based real-time traffic light
detector for self-driving vehicles. IEEE transactions on Mobile Computing 19(2),
300–313 (2019)
27. Real,E.,Aggarwal,A.,Huang,Y.,Le,Q.V.:Regularizedevolutionforimageclas-
sifier architecture search. In: Proceedings of the aaai conference on artificial intel-
ligence. vol. 33, pp. 4780–4789 (2019)
28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3), 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-y
29. Sahni,M.,Varshini,S.,Khare,A.,Tumanov,A.:Comp{ofa}–compoundonce-for-
allnetworksforfastermulti-platformdeployment.In:InternationalConferenceon
LearningRepresentations(2021),https://openreview.net/forum?id=IgIk8RRT-
Z
30. Sanh,V.,Wolf,T.,Rush,A.:Movementpruning:Adaptivesparsitybyfine-tuning.
Advances in Neural Information Processing Systems 33, 20378–20389 (2020)
31. Sun, W., Zhou, A., Stuijk, S., Wijnhoven, R., Nelson, A.O., Corporaal, H., et al.:
Dominosearch: Find layer-wise fine-grained n: M sparse schemes from dense neu-
ralnetworks.Advancesinneuralinformationprocessingsystems34,20721–20732
(2021)
32. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.:
Mnasnet:Platform-awareneuralarchitecturesearchformobile.In:Proceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.2820–
2828 (2019)
33. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural
networks.In:Internationalconferenceonmachinelearning.pp.6105–6114.PMLR
(2019)
34. Tian, Y., Krishnan, D., Isola, P.: Contrastive multiview coding. In: Computer
Vision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,
Proceedings, Part XI 16. pp. 776–794. Springer (2020)DϵpS: Delayed ϵ-Shrinking 17
35. Wan, A., Dai, X., Zhang, P., He, Z., Tian, Y., Xie, S., Wu, B., Yu, M., Xu, T.,
Chen,K.,etal.:Fbnetv2:Differentiableneuralarchitecturesearchforspatialand
channel dimensions. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 12965–12974 (2020)
36. Wang, C., Yang, Q., Huang, R., Song, S., Huang, G.: Efficient knowledge dis-
tillation from model checkpoints. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho,
K. (eds.) Advances in Neural Information Processing Systems (2022), https:
//openreview.net/forum?id=0ltDq6SjrfW
37. Wang, L., Dong, X., Wang, Y., Liu, L., An, W., Guo, Y.: Learnable lookup table
for neural network quantization. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 12423–12433 (2022)
38. Wu,B.,Dai,X.,Zhang,P.,Wang,Y.,Sun,F.,Wu,Y.,Tian,Y.,Vajda,P.,Jia,Y.,
Keutzer,K.:Fbnet:Hardware-awareefficientconvnetdesignviadifferentiableneu-
ralarchitecturesearch.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 10734–10742 (2019)
39. Yu, J., Huang, T.S.: Universally slimmable networks and improved training tech-
niques. In: Proceedings of the IEEE/CVF international conference on computer
vision. pp. 1803–1811 (2019)
40. Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.J., Tan, M., Huang, T., Song,
X.,Pang,R.,Le,Q.:Bignas:Scalingupneuralarchitecturesearchwithbigsingle-
stagemodels.In:Vedaldi,A.,Bischof,H.,Brox,T.,Frahm,J.M.(eds.)Computer
Vision–ECCV2020.pp.702–717.SpringerInternationalPublishing,Cham(2020)
41. Yu, J., Yang, L., Xu, N., Yang, J., Huang, T.: Slimmable neural networks. arXiv
preprint arXiv:1812.08928 (2018)
42. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures
forscalableimagerecognition.In:ProceedingsoftheIEEEconferenceoncomputer
vision and pattern recognition. pp. 8697–8710 (2018)