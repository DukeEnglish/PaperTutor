1
Depression Detection and Analysis using Large
Language Models on Textual and Audio-Visual
Modalities
Avinash Anand∗, Chayan Tank†, Sarthak Pol‡, Vinayak Katoch§, Shaina Mehta¶ and Rajiv Ratn Shah∥
∗ † ‡ § ¶ ∥ Indraprastha Institute of Information Technology, Delhi, India.
∗avinasha@iiitd.ac.in †chayan23030@iiitd.ac.in ‡sarthak23082@iiitd.ac.in §vinayak23105@iiitd.ac.in
¶shaina23139@iiitd.ac.in ∥rajivratn@iiitd.ac.in
Abstract—Clinicaldepression,alsoknownasMajordepressive with estimates suggesting that it affects over 3.8% of the
disorder (MDD), is a prevalent psychological disorder affecting global population [2]. This statistic translates to about 280
the general population worldwide. Depression has proven to
millionpeoplegrapplingwiththiscondition,spanningvarious
be a significant public health issue, profoundly affecting the
demographicsandgeographies.Depressionmanifestsuniquely
psychologicalwell-beingofindividuals.Ifitremainsundiagnosed,
depression can lead to severe health issues, which can manifest across various age groups and genders, with around 5% of
physically and even lead to suicide. Generally, Diagnosing de- adults affected by the condition—comprising 4% of men and
pression or any other mental disorder involves conducting semi- 6% of women. This indicates that women are more prone to
structured interviews alongside supplementary questionnaires,
depression than men. Among older adults, particularly those
includingvariantsofthePatientHealthQuestionnaire(PHQ)by
over 60 years old, the prevalence increases to approximately
Cliniciansandmentalhealthprofessionals.Thisapproachplaces
significant reliance on the experience and judgment of trained 5.7%. These numbers underline the significant impact of
physicians, making the diagnosis susceptible to personal biases. depression on public health and the critical need for effective
Given that the underlying mechanisms causing depression are managementstrategies.Depressioncanbetriggeredbyarange
still being actively researched, physicians often face challenges
offactors,bothbiologicalandenvironmental.Individualswho
indiagnosingandtreatingthecondition,particularlyinitsearly haveundergonetraumaticexperiences,suchasabuseorsevere
stages of clinical presentation. Recently, significant strides have
been made in Artificial neural computing to solve problems in- loss, are particularly susceptible to developing depression.
volvingtext,image,andspeechinvariousdomains.Ouranalysis Additionally,certainbiologicalfactors,includinggeneticsand
has aimed to leverage these state-of-the-art (SOTA) models in changes in brain chemistry, play crucial roles in the onset and
ourexperimentstoachieveoptimaloutcomesleveragingmultiple
progression of the disorder. This complex interplay of factors
modalities. The experiments were performed on the Extended
makesthedetectionandtreatmentofdepressionachallenging
Distress Analysis Interview Corpus Wizard of Oz dataset (E-
DAIC) corpus presented in the Audio/Visual Emotion Challenge endeavour. [1]
(AVEC) 2019 Challenge. The proposed solutions demonstrate One of the primary challenges in managing depression
better results achieved by Proprietary and Open-source Large effectivelyisitsdiagnosis.Traditionalmethodsfordiagnosing
Language Models (LLMs), which achieved a Root Mean Square
depression primarily rely on subjective assessments, such
Error (RMSE) score of 3.98 on Textual Modality, beating the
as patient-reported questionnaires like the PHQ. These tools
AVEC 2019 challenge baseline results and current SOTA regres-
sion analysis architectures. Additionally, the proposed solution depend on the individuals’ ability to report their feelings and
achieved an accuracy of 71.43% in the classification task. The behaviours accurately, a process often complicated by the
paper also includes a novel audio-visual multi-modal network nature of depression itself, which can distort self-perception
that predicts PHQ-8 scores with an RMSE of 6.51. The paper
andawareness.Thesubjective natureofthesediagnostictools
also discussed the potential limitations of the dataset and how
can lead to considerable variability in diagnosing depression,
they can be overcome.
with significant implications for treatment. Misdiagnosis or
Index Terms—Depression Detection and Severity Analysis,
delayed diagnosis can prevent individuals from receiving the
Multi-modal Analysis, Deep Learning, Large Language Models,
appropriate care and potentially aggravate the severity of the
OpenAI Whisper, RoBERTa, BiLSTM, DepRoBERTa, OpenAI
GPT 3.5, OpenAI GPT 4, LLAMA 3 8B Instruct. condition. Moreover, the lack of straightforward, objective
diagnostic tests means that clinicians must rely on regular
screenings to monitor the severity of depression in patients,
I. INTRODUCTION
a process that is not only time-consuming but also fraught
DEPRESSIONisawidespreadanddebilitatingpsychiatric with the potential for inconsistency.
disorder experienced by people around the world [1]. Given these challenges, there is increasing interest in af-
It is marked by a continuous low mood and a diminished fective computing, especially in utilising behavioural clues to
interest or pleasure in everyday activities. The effects of this help detect and assess depression. Quantifiable data from be-
disordergowellbeyondemotionalsuffering,affectingallareas haviouralmarkerslikefacialexpressions,speechpatterns,and
of a person’s life, including their personal health and societal changesinphysicalactivitylevelscanbeanalysedobjectively.
productivity. The prevalence of depression is alarmingly high, Thesemarkerscanofferinsightsintoanindividual’semotional
4202
luJ
8
]CH.sc[
1v52160.7042:viXra2
Fig. 1: The Proposed Textual Network & Audio - visual network, which predicts the PHQ-8 scores of patients using their Audio, visual
and textual clues. In the Textual Network, we have used whisper to extract the transcripts from audio and input them to LLMs along with
prompts for PHQ-8 Score and Class prediction. In the Audio-Visual network, we use a Whisper + BiLSTM-based network, which outputs
the Predicted PHQ-8 scores
state, potentially helping clinicians detect depression more AVECChallenge Task Dataset Results
reliably and at an earlier stage. This emerging approach is 2016[5] BinaryClassification DAIC F1-0.583
part of a broader trend towards multi-modal frameworks in
2017[6] PHQ-8ScorePrediction DAIC RMSE-7.05
psychiatric assessment, where data from different sources are
2019[7] PHQ-8ScorePrediction E-DAIC RMSE-6.37
integrated to form a more complete and precise picture of a
patient’s mental health. By combining traditional psycholog- TABLE I: Baseline System Depression Detection and Analysis in
ical assessments with cutting-edge technology in behaviour AVEC 2016, 2017 and 2019 Challenges
analysis,researchershopetodevelopmorerobustmethodsfor
not only diagnosing depression but also assessing its severity.
A. AVEC 2019 Challenge
In conclusion, Depression continues to be a significant
globalhealthissueduetoitshighprevalence,profoundimpact From AVEC 2019 challenge [7] published papers, some of
onqualityoflife,andthecomplexitiesinvolvedinitsdiagnosis the exciting approaches we found based on machine learning
and treatment. Developing new tools that offer more objective and deep learning-based techniques were mentioned in [10],
and reliable assessments of depression is essential. This paper [11], [12], [13], and [14]. Firstly, Zhang et al. [10], who
introduces a multi-modal architecture and analysis designed segregated the voices of Ellie and the patient from the audio
to utilise behavioural clues for more effective depression recordings by using the timestamps given in the transcripts
detection, marking a significant advancement in mental health andextractedExtendedGenevaMinimalisticAcousticParam-
diagnostics. eter Set (eGeMAPS) features using DigiVoice featurisation
pipelineandbasedfeaturesusingCollaborativeVoiceAnalysis
II. LITERATUREREVIEW Repository (COVAREP) toolkit and applied Logistic Regres-
The AVEC workshop was started in 2011 and ended in sionwithL1regularisationandRandomForestAlgorithmwith
2019. Initially, the challenge focuses on multi-modal ap- hyperparameter tuning. They achieved the RMSE and Mean
proaches for sentiment analysis [3]. Later, specific challenges Absolute Error (MAE) of 6.78 and 5.77 on audio modality,
related to mental health diagnosis using Artificial Intelligence which is lower than the baseline results of the AVEC 2019
were introduced in the Depression Detection task introduced challenge[7].AnotherresearchisproposedbyFanetal.[11],
intheAVEC2013challenge[4].InAVEC2016[5],and2017 created a multi-scale temporal dilated Convolutional Neural
[6]andAVEC2019challenge[7],DistressAnalysisInterview Network (CNN) for depression severity analysis on audio
Corpus (DAIC) [8] [9], and E-DAIC dataset was used for and textual features of the E-DAIC dataset and achieved the
depression detection and analysis respectively. The baseline RMSE of 5.07 and 5.91, MAE of 4.06 and 4.39 and Con-
system of depression detection and analysis of these AVEC cordance Correlation Coefficient (CCC) of 0.466 and 0.430
challenges and their results for their multi-modal frameworks on validation and test data, respectively. Makiuchi et al. [12]
combining audio-video on the test set is given in TableI. applied a multi-modal fusion approach which combines audio3
and textual features for depression assessment by processing Yuan et al. [18] extracted textual features using sentence-
audio features from a pre-trained VGG-16 network through a level vector (sent2vec) encoders, the Universal Sentence En-
Gated Convolutional Neural Network (GCNN) followed by a coder, and Bi-LSTM and extracted audio and visual features
Long Short Term Memory (LSTM) layer and textual features using a combination of the PCA and Midmax algorithms.
obtained from Bidirectional Encoder Representations from Then,theyappliedtheMulti-modalMulti-orderFactorFusion
Transformers (BERT) are processed using a CNN followed (MMFF)algorithmonallthefeatures,achievingRMSE,MAE
by an LSTM layer. Then, all the features were fused using a andCCCof4.91,3.98,and0.676,respectively,onatestsetof
fully connected layer and yielded a CCC score of 0.696 and the E-DAIC dataset [7]. Saggu et al. [19] proposed Depress-
0.403 on the development and test sets, respectively. Net as a novel multi-modal machine learning framework for
depression detection, which uses a Bi-LSTM layer network
Yin et al. [13] suggested a multi-modal architecture for with an attention mechanism. This approach yields an RMSE
depression assessment using a hierarchical recurrent neural and CCC of 4.32 and 5.36, and 0.662 and 0.457 development
network which incorporates two hierarchies of Bidirectional and test set of the E-DAIC dataset [7] respectively. Wang
Long Short-Term Memory (Bi-LSTM) memories for multi- et al. [20] processed head pose and Action Units (AUs) as
modal fusion of data and applied the adaptive sample weight- features and audio-based features from the COVAREP toolkit
ing mechanism to training data. On the validation and test using CNN and LSTM in series and converted them into
sets, they achieved RMSE and CCC values of 4.94 and 0.402 a feature matrix. The textual features are extracted using
and 5.50 and 0.442. Finally, Ray et al. [14] introduced a Sentence BERT (s-BERT) and passed through the network
multi-levelattentionnetworkconsistingofonlyBi-LSTMand consistingofCNNandBi-LSTMlayers.Then,allthefeatures
attention mechanism for the depression severity analysis task. are passed through Multi-modal Multi-Utterance-Bimodal At-
They achieved the RMSE, MAE and CCC of 4.37, 4.02 and tention Networks, followed by an attention mechanism, two
0.67 on textual modality, winning the AVEC 2019 challenge LSTM and two dense layers. They trained their network on
[7].However,themodelssheproposedbasedonaudio,video, E-DAIC[7]andDAIC[8][9]datasetandachievedtheRMSE
and multi-modality beat the baseline results of the challenge. and MAE of 4.03 and 3.05, respectively, on the development
set.
Sun et al. [21] proposed a multi-modal architecture called
B. Post AVEC 2019 Challenge
’Tensorformer’, which allows all the modalities to exchange
After the AVEC challenge, several researchers have also all the relevant information simultaneously. They trained the
proposed various machine learning and deep learning-based network on the E-DAIC [7] dataset and achieved RMSE and
architectures capable of handling single as well as multi- CCC scores of 4.31 and 0.491, respectively, on the test set,
modalityonseveraldatasets,mainlyfocusingonAVEC2016, beating the SOTA model of AVEC 2019 DDS challenge win-
2017 and 2019 challenges [5], [6], and [7] datasets. Firstly, ner [14] in terms of RMSE only. Mao et al. [22] categorised
Zhang et al. [15] introduced a multi-modal framework for the severity of depression based on the PHQ-8 Score into five
depression assessment by processing audio and video features categoriesthatarehealthy,mild,moderate,moderatelysevere,
using Multi-modal Deep Denoising Auto-Encoder (DDAE) and severe. They used COVAREP features of the audio and
and encoding them into fisher vectors. Relevant features are Bi-LSTMandtime-distributedCNN,andfortextualmodality,
selected using the tree-based model. They used paragraph they used global vectors for word representation (GloVe)
vector (doc-to-vec) models to extract textual-level features. embeddingsfedintotheBi-LSTMnetwork.Finally,thefusion
Finally, the Multi-task Deep Neural Network is trained on of modalities is performed using self-attention, dense layers,
these features plus the ResNet features and achieves a CCC and majority voting. They achieved the F1 score of 95.80 per
of 0.560 in textual features, Mean Squared Error (MSE) of centonpatient-leveldepressiondetectionontheDAICdataset
20.06 and F1 Score of 91.7 per cent on multi-modal settings, [8] and [9]. Teng et al. [23] proposed a study Integrating
accuracy of 89.3 per cent on audio-visual modalities, on the AVEC2019[7]andCMU-MOSEI[?]datasets.Theirresearch
development set of EDAIC dataset [7]. Another research is adoptsamulti-modal,multi-tasklearningapproachtoenhance
proposed by Jo et al. [16] who introduced a four-stream depression detection accuracy. By leveraging emotional data,
depressiondetectionmodelthatusesanensembleofBi-LSTM their method significantly boosts precision, achieving a CCC
and CNN to analyse audio and textual elements. They trained and MAE of 0.466 and 5.21 on a test set of the E-DAIC
themodelusingtheDAIC[8][9],andE-DAIC[7]datasetsand dataset.
obtained F1 scores of 97 per cent and 99 per cent, Precision
of97percentand100percent,andRecallof97percentand Li et al. [24] proposed the Flexible Parallel Transformer
98 per cent, respectively. Sun et al. [17] proposed the multi- (FPT) model, which integrates video and audio descriptors.
modal framework for depression analysis known as Cube Tested on the E-DAIC dataset, the FPT model achieved an
MLP, which comprises three separate MLP units, each with RMSE of 4.80, an MAE of 4.58, and a depression classifica-
two affine transformations, and each unit performs separate tionaccuracyof0.79ataPHQ-8thresholdof10,outperform-
operations that are sequential mixing, modality mixing, and ing fewer complex models and proving the effectiveness of
channelmixingonthedata.TheyachievedanMAEandCCC multi-modal approaches. Go´mez et al. [25] proposed a multi-
of 4.37 and 0.583, respectively, on the E-DAIC dataset [7]. level temporal model analogous to multi-modal transformer4
architecture for depression detection by processing audio- multimodalphysicsproblemsand’Mathquest’amathsdataset
visual embeddings, facial and body landmarks, and feeding derived from 11th and 12th standard NCERT Mathematics
into the transformer-based encoder. This approach achieved textbooks respectively. These papers show their study to
the precision, recall and F1-scores of 74 per cent, 84 per cent evaluate the performance of LLMs on these datasets, and
and 78 per cent, respectively, on DAIC [8] [9] dataset and 59 their approach highlights the potential for enhancing LLMs’
percent,58percentand56percent,respectively,onE-DAIC capability in textual corpora with specialized datasets and
dataset respectively. Steijn et al. [26] converted the audio techniques.
into text using the Google ASR toolkit and extracted textual Moreover, using the same dataset MM-PhyQA, [33] pro-
featuresusings-BERTandLinguisticInquiryandWordCount posed an LLM-based chatbot to answer multimodal physics
(LIWC) toolkit and handcrafted features such as emotion, multiple-choice questions (MCQs). They demonstrated that
speechrate,etc.,fromtheaudiofiles.TheyalsousedtheKer- their Reinforcement Learning from Human Feedback (RLHF)
nel Extreme Learning Machine (KELM) algorithm for single and Image Captioning techniques improve the quality and
multi-task regression and single-task classification purposes accuracy of the model’s responses compared to supervised
and the K-Fold cross-validation technique. They achieved fine-tuned LLMs.
RMSE and CCC scores of 6.06 and 0.62 when performing a
summation of multi-task regression symptom predictions and In conclusion, the existing approaches in the literature
RMSE and CCC of 5.37 and 0.53 using the second stage have primarily been centred around the constraints of the
RF approach, which uses single-task classification symptom AVEC 2019 challenge, and the integration of recent SOTA
predictions on E-DAIC dataset [7]. language models (LLMs) into their architectures needs to
be addressed. This omission represents a significant gap, as
modernLLMshavedemonstratedremarkablenaturallanguage
C. Large Language Models Based Approaches
understanding, generation, and contextual reasoning capabili-
Inrecentyears,severalresearchershavestartedusingLarge ties.Wheneffectivelyincorporated,thesemodelscanenhance
Language Models for depression assessment, as done in [27], thegeneralizabilityandefficiencyofvariousapplications,from
[28] and [29]. Firstly, Sadeghi et al. [27] proposed the text- sentiment analysis to complex predictive tasks; our research
based architecture for depression severity analysis on the E- aims to bridge this divide by integrating SOTA LLMs into
DAIC dataset [7] by obtaining the textual transcripts from the our proposed framework. By leveraging models such as GPT,
OpenAI’s Whisper-large model, extracted its features using Llama models and their successors, we aim to create a
OpenAI’sGPT3.5Turbomodel,fine-tunedtheDepRoBERTa more robust, adaptable, and efficient architecture capable of
modelandextracteditsfeaturesandtrainedtheSupportVector handling complex linguistic nuances with minimal domain-
Regression(SVR)model.TheyachievedtheMAEof3.56and specific adjustments.
4.26 and RMSE of 5.27 and 5.36 on the development and test
set, respectively. Danner et al. [28] in which they combined III. DATASET
the training set of DAIC [8] [9], and E-DAIC [7] datasets and
train the BERT model for the depression detection task. They
evaluated the model using the test set of DAIC [8] [9] dataset
and combination of test set both DAIC [8] [9] and E-DAIC
[7] datasets and achieved precision scores of 63 per cent and
83 per cent, recall scores of 66 per cent and 82 per cent and
F1 scores of 64 per cent and 82 per cent, respectively. They
also performed a direct evaluation of the GPT 3.5 model and
ChatGPT4modelusingatestsetofDAIC[8][9]andachieved
a precision score of 78 per cent, and 70 per cent, recall score
of79percentand60percentandF1scoreof78percentand
61 per cent respectively. Finally, Hadzic et al. [29] performed Fig. 2: Number of Depressed & Non-Depressed according to
adirectevaluationofChat-GPT3.5andGPT4usingatestset PHQ-8 score of Train Set
of DAIC [8] [9] and achieved precision, recall and F1 scores
of 81 per cent, 70 per cent, and 71 per cent respectively for E-DAIC [7] is an extension of the DAIC-WOZ dataset and
two-classclassification.TheyalsotrainedtheBERTmodelon includes semi-clinical interviews intended to assist in diag-
a training set of DAIC [8] [9] and E-DAIC [7] datasets. nosingpsychologicaldistressconditions,includingdepression,
Furthermore, to demonstrate the capabilities of large lan- anxiety, and post-traumatic stress disorder. This dataset was
guagemodels,A.Anandetal.[30]finetunedanLLMthrough also utilised in the AVEC 2019. It comprises 275 interviews
instructional calibration on a dataset tailored to High School conductedbyananimatedvirtualinterviewernamedEllieand
Physics and utilizing retrieval augmentation. Their finetuned byanAI-basedagent,ofwhich163,56,and56areincludedin
retrieval-augmentedmodel,SciPhy-RAG,demonstratessignif- thetraining,validation,andtestsets,respectively.Thetraining
icant improvements over Vicuna-7b. andvalidationsetsconsistofinterviewstakenbyEllieandAI-
Also, in their other papers [31] and [32] A. Anand et al. based agents, whereas the test set consists of only AI-based
propose MM-PhyQA, a dataset featuring high school-level agents. Each interview record consists of the following files:5
measures representing the LLD information of the audio
filegiveninthedataset.Thesearealsoin.csvfileformat.
• The Bag of Video Words (BoVW) obtained from the
visual features mentioned above are processed and sum-
marised over a block of 4 seconds for each step of 1
second.
• The Bag of Audio Words (BoAW), derived from the
above-mentioned audio features, is processed and sum-
marised over 4-second blocks with a step duration of 1
second.
• Metadata files in the form .csv format consists of PHQ-
Fig. 3: Distribution of Participants based on PHQ-8 Scores of
8 Score, PHQ-8 Binary Score, Session ID etc. and the
Train Set
gender of the patient whose information is given in the
table II where the PHQ Binary 0 Represents Healthy
patient and one as Depressed patient and the figures 2,
3, 4 and 5.
PHQBinary Male Female Total
0 75 51 126
1 18 19 37
Overall 93 70 163
TABLE II: PHQ-8 Binary Distribution by Gender of Train Set
Fig. 4: Gender Distribution Based on PHQ-8 Binary Scores of
Train Set
IV. EVALUATIONMETRICS
For evaluating our experiments, we have used different
• Audio files in the form of .wav format. evaluation metrics such as RMSE is a standard metric used
• Transcripts of the interview in the form of .csv files. to quantify the error of a model in predicting numerical data.
• Visual features obtained from the OpenFace toolkit [34]. ItisusedforEvaluatingRegressionexperimentsandisdefined
These features include head pose features, eye gaze as:
features, position coordinates, head rotation coordinates,
(cid:118)
and 17 Facial Action Units (FAU or AU) stored in row (cid:117) M
(cid:117) 1 (cid:88)
and column format, making it 35 features corresponding RMSE=(cid:116) (a −aˆ )2 (1)
M i i
to 17 AU. These features are also in the form of .csv i=1
files.
where aˆ is the predicted value, a is the actual value, and M
i i
• The Mel-Frequency Cepstral Coefficients (MFCCs) fea- is the number of observations.
tures extracted from the openSMILE toolkit [35]. These
In parallel, another metric we used for regression analysis
contain 13 MFCCs features with 13 delta and 13 double
is MAE, which measures the average magnitude of errors in
deltafeaturescombinedtomake39columnsrepresenting
a set of predictions without considering their direction. It is
acousticLow-LevelDescriptors(LLD).Thesearein.csv
defined as:
file format.
• The eGeMaPS [36] features extracted from the openS- 1 (cid:88)M
MILE toolkit [35]. The eGeMaPS features contain 88 MAE= |a −aˆ | (2)
M i i
i=1
where a is the actual value, aˆ is the predicted value, and
i i
M is the number of observations.
Finally, we also evaluated our regression results using
the Concordance Correlation Coefficient (CCC) [37], which
quantifies the agreement between two variables, yielding a
value between -1 and 1, with 1 indicating perfect agreement.
It is defined as:
2ησ σ
CCC= a b (3)
σ2+σ2+(α−β)2
a b
where η represents the Pearson correlation coefficient be-
Fig. 5: Gender Distribution of Train Set tween the predicted and actual values, σ and σ are the
a b6
standard deviations of the predicted and actual values, and depression is lower than that of the non-depression. Spectral
α and β are the means of the predicted and actual values. Flux represents the rate of change in the power spectrum. It
For the classification experiments regarding the textual helpsidentifychangesinspeechpatterns.TheSpectralFluxof
modality, we used accuracy, F1-scores, precision, and recall the depression is lower than that of the non-depression. Jitter
asevaluationmetricstocalculatetheresults’performanceand Indicates frequency variation between cycles of the vocal
compare them easily with other approaches. waveform. High jitter values can be associated with voice
These metrics have been kept the same as the AVEC 2019 pathologies. The mean Jitter of the depression is lower than
challenge metrics so that we can compare our results with that of the non-depression. Shimmer Represents amplitude
them. variation between cycles. Like jitter, higher shimmer values
can indicate vocal issues. The mean Shimmer is lower in
V. PROPOSEDMETHODOLOGY depressed individuals than in non-depressed individuals.
For Modelling tasks, we have performed the following
The Methodologies adopted to analyse the various modali-
experiments:
ties,suchasaudio,video,andtextual,arebasedonregression
analysisacrossmultiplemodelsandalgorithms.Fortheregres-
1) BiLSTM: To preprocess the audio features such that
sion task, we used the PHQ-8 scores, which ranged from 0 to
they can be used for experimentation, we clipped the first
24; each of the eight questions in the PHQ-8 questions was
two columns (’name’ and ’frame-time’) from MFCC features
scoredfrom0to3todeterminetheseverityofthedepression.
so that only features remain; then, we normalised the MFCC
Each score from 0-3 measures how many days the subjects
values using standard scalar so that all samples are scaled.
have been experiencing the questionnaire problems given in
Also, to make all the samples consistent for training, we set
the past two weeks, with 0 being ”Not any day” and 3 being
a padding threshold of 80,000 rows. We found that MFCC
”Everyday”.Finally,thescoresfromtheseeightquestionsare
features performed better than eGeMaps, so we only used
summed to calculate the PHQ-8 score. [38]. For the textual
MFCCs in this experiment. Although the eGeMaps did
analysis, the methodology adopted has been based on two
contribute to a deeper understanding of audio features as
Supervised learning tasks: classification and regression. For
discussed in III. According to the majority of approaches in
classificationexperiments,theSubjectsweredividedintothree
the AVEC challenge 2019, BiLSTM-based networks were
classes: Healthy (PHQ-8 from 0-8), Mild (PHQ-8 from 9-15),
ubiquitous; this is due to the sequential nature of features
and Depressed (PHQ-8 from 16-24).
given, which are on a frame basis for every milli second;
in fact, the SOTA approach was also BiLSTM-based, so we
A. Audio Data first tried a BiLSTM-based architecture to perform regression
Upon analysis of various audio features to understand on MFCC-feature data given in the dataset. This architecture
patient conditions better, we used eGeMaps features. For a employs an attention-based BiLSTM model. The model
comprehensive understanding, we have calculated the mean consists of two LSTM layers with 200 hidden units, which
(µ) and standard deviation (σ) of these features for patients connect to a fully connected layer, taking 39 MFCC features
divided into three classes: Healthy, Mild, and Depressive. as input and producing a single output for regression.
These statistical measures help illustrate the differences and
similarities in audio characteristics across the groups. The 2) Whisper: Whisper[39]isanAutomaticspeechrecogni-
mean values indicate each feature’s central tendency or tion(ASR)andSpeechtranslationmodelreleasedbyOpenAI.
average level within a group, while the standard deviation Itsremarkablespeechrecognition,translation,andtranscribing
provides information about the variability or dispersion of capabilities have been leveraged for our interview audio files.
the features around the mean. Table III presents the detailed We have fine-tuned the Whisper-Base model (74M param-
results, showcasing µ and σ for each audio feature across the eters) applied to patients’ raw interview audio files from
classes divided based on PHQ-8 scores: PHQ-8 scores from the dataset and performed a downstream Regression task.
0-8 as Healthy, 9-15 as Mild, and scores 16-24 as depressed. Whisper is an encoder-decoder architecture-based model in
This data was essential for identifying patterns and trends whichtheencodermoduletransformstheinputaudiofilesinto
that can distinguish between healthy individuals and those encodings, which then are given to the decoder that converts
with varying degrees of depression. By examining these them into text tokens. So we have used the encoder module
metrics, we understood how certain audio features correlate to encode the audio files, and then these 512-dimensional
with different levels of depression, ultimately aiding in more encodings are followed by regression layers composed of a
accurate and early diagnosis. seriesoffullyconnectedlayers(4098,2048,1024,512,1)with
dropouts.Eachlayerreducesthedimensionalitythroughlinear
As the trends Observed in III. Loudness Reflects the transformations and ReLU activation, culminating in a single
perceived intensity of the sound. Variations in loudness neuron output for performing regression. We have performed
can indicate different emotional states. The loudness in the the training on a 50 GB RTX A6000 GPU for all our tasks.
depression class is lower than the loudness in the non-
B. Visual Data
depression class. Hammarberg Index Measures the ratio of
high-frequency to low-frequency energy. It helps in assessing For the video features, we used the Pose, Gaze, and
the quality of voice. As shown, the Hammarberg Index of AU pairs, represented by 49 features, containing six poses,7
Feature ClassHealthy ClassMild ClassDepression
Mean Std Mean Std Mean Std
Loudness 0.091398 0.101953 0.085814 0.101882 0.074468 0.082629
HammarbergIndex 27.279813 8.794111 27.241393 8.768655 27.205442 8.470930
SpectralFlux 0.025524 0.043432 0.019987 0.039191 0.019588 0.035508
Jitter 0.005448 0.021401 0.005146 0.021018 0.004727 0.021474
Shimmer 0.255305 0.696044 0.234829 0.661562 0.211419 0.641799
TABLE III: Comparison of audio features across different classes
eight gazes, and 35 AU features (which represent 17 fa-
User Prompt:
cial action units). We have used all three of these features
in combined form only as given in the dataset. For pre-
processing the data, we first clipped the first four columns Ihaveinterviewtranscriptsofmanypatientsfromadepression
diagnosisinterviewbasedonPHQ-8scoreswhichrangefrom
(’frame’,’timestamp’,’confidence’, and ’success’) of sample
0-24,signifying0-8asHealthy,9-15asmildlydepressed,16-24
files containing the features corresponding to each patient. asDepressed.
Then, we normalised these features using a standard scalar
Oneofthesamplesisfollowing:<Sample from train set>
and truncated or padded the output files to 30,000 rows by ThePHQ-8scoreofthispatientis<score>andintheclassof
setting a padding threshold. A similar Bi-LSTM attention- <label>.
based approach was adopted for audio data analysis; the
Similarly,anothersampleis:<Sample from val set>The
architecture includes two LSTM layers with 128 hidden units PHQ-8scoreofthispatientis<score>andintheclassof
each, followed by an attention module and a single output <label>.
neuron for regression output. We calculated the RMSE score
NowpredicttheExactPHQ-8scoreandclassofthissample:
onthevalidationset.Oneofthelimitationsofthisdatasetwas <Sample from test set>
theunavailabilityoftherawvideofiles,sotheexperimentwith
pre-trained architectures was not performed.
Model Response:
C. Textual Data .....<Score>......<Label>.....
We have experimented with the pre-trained RoBERTa,
TABLE IV: Prompt for Predicting PHQ-8 Score and Class using
DepRoBERTa, Proprietary models such as GPT 3.5 and GPT
GPT 3.5 and 4 Models
4 and Open-source models such as LLAMA 3 8B instruct
for textual modality. We extracted the detailed transcripts
of the interviews from the respective audio file using a 2) Proprietary LLMs GPT 3.5 and GPT 4: GPT-3.5 [43]
whisper-large-v3 model (1550 M parameters) using the full is an LLM developed by OpenAI comprising 175 billion
encoder-decoder architecture and then performed the task of parameters. It is part of the GPT models and stands for
transcription of these files to extract the text, which contains ’Generative pre-trained transformers’ [44]. It utilises trans-
both the interviewer and the interviewee transcripts. This former architecture to produce text comparable to human-
created a larger corpus for textual modality than was already level proficiency based on input prompts. Trained on exten-
giveninthedataset.Wefoundthatthesetranscriptsperformed sive datasets comprising vast amounts of internet text, this
better than the text provided in the dataset for LLM-based model can understand and generate contextually relevant and
experiments as they provided more context for the interviews. coherent responses. It can perform tasks like text completion,
translation, summarisation, and conversational agents.
1) RoBERTa and DepRoBERTa: RoBERTa [40] model Whereas GPT-4 [45], an advancement over GPT-3.5, of-
is an extension of BERT [41] model introduced by Liu et fers enhanced text understanding, generation accuracy, and
al. Like BERT, RoBERTa is a transformer-based language broader contextual comprehensions due to a more significant
model that employs self-attention to process input sequences parameter estimated to be around 1.7 Trillion, which helps
and generates contextualised word representations. However, in more sophisticated training. This results in more reliable
RoBERTa is trained on larger datasets and employs and nuanced outputs, making it superior for complex tasks
dynamic masking during training, which helps it learn like detailed text analysis and advanced natural language
more generalisable word representations. We have used the processing applications.
transcripts provided in the dataset for this experiment. Firstly, We have used the OpenAI’s ChatGPT interface for these
we cleaned the data by expanding the acronyms as done by models as their weight checkpoints are not available as
Ray et al. [14] and then removed the punctuations. Then, the their architectures are private. We have performed two-shot
text is encoded via the tokeniser of the DepRoBERTa model, prompting (as shown in Table IV) where we have taken
which introduces [42]. Then, the tokenised and encoded text one-on-one samples from the training and validation sets,
is trained on the RoBERTa model, and its output is used to respectively, and then Prompted the LLMs to predict their
calculate the results on a test set of the dataset. PHQ-8 score and class based on given samples.8
hallucinations and could not provide valid responses in a spe-
System Prompt:
cific format mentioning the PHQ-8 scores and corresponding
classes in its response. This is due to fewer samples to fine-
Yourareadepressiondiagnosistool.Ihaveinterviewtranscripts tunean8Bparametermodel.Anincreasednumberofsamples
ofmanypatientsfromadepressiondiagnosisinterviewbasedon
may be fruitful in such experiments.
PHQ8scoreswhichrangefrom0-24,signifying(0-8ashealthy,
9-15asmild,16-24asdepressed).Yourtaskistoclassifythe
stateofdepressionbasedonthefollowinginterviewtranscript D. Fusion of Modalities
amongthethreecategoriesandprovidetheexactPHQ-8score
For the multimodal fusion framework, we have combined
(range0to24)thepatienthasandnothingelse.
the corresponding audio and video experiments using the
Whisper module and BiLSTM networks to make a novel
User Prompt:
frameworkthatinputsaudiofilesandvideofeaturestopredict
Interview:<sample from train set> the PHQ-8 scores. We have yet to use the textual modality
for this network, as the transcripts themselves are generated
Assistant:
using the Whisper model on audio inputs, so their encodings
Label:<label>,score:<PHQ-8 score> would be the same as the audio encodings, and hence, text
modality is already inherent in the whisper encodings. The
User Prompt:
network consists of 2 Modules. The first module is the
Interview:<sample from validation set> whisper encoder, which uses the whisper-base (74M) model
and gives the encodings of input audio files. The second
Assistant:
module is a BiLSTM-attention network with a configuration
Label:<label>,score:<PHQ-8 score> similar to video experimentation. These encodings are then
appropriatelypaddedforequal-sizedtensors,andtheoutputof
thesemodulesisconcatenatedtomakeencodingsrepresenting
User Prompt:
both audio and video features in the same dimensional space.
Interview:<sample from train set> These combined features are then fed to a BiLSTM-attention
network of the same configuration, as the resultant features
Model Response: also had sequential information due to the sequential video
featureembeddings.ThefinaloutputsofthisBiLSTMnetwork
aregiventoafullyconnectedregressionlayerthatpredictsthe
Label:<label>,score:<PHQ-8 score>
PHQ-8 scores used to perform the regression experiment, as
shown in figure 6.
TABLE V: Prompt for Predicting PHQ-8 Score and Class using
Llama-3 8B VI. RESULTSANDDISCUSSION
A. Regression Analysis
1) Results on Textual Modality: The results from different
3) Open-source LLM Llama-3: Llama 3 [46] introduced
models on the textual modality in terms of evaluation
by Meta AI also presented a significant advancement in
metrics on the test set are displayed in Table VI. The textual
open-source large language models, aiming to match the
experiments are performed on the test set in contrast to the
performance of leading proprietary models. Llama 3 uses a
audio and video experiments performed on the validation
decoder-only transformer architecture. The Llama-3 8B and
set. This has made it easy to compare most audio-visual and
70B parameter models boast a vocabulary of 128K tokens,
multimodal results on the validation set. The approaches on
drastically improving language encoding efficiency, conse-
textual modalities such as in [28] have been performed in
quentlyenhancingoverallmodelperformancesetanewbench-
the test set, and hence, we have also adopted the evaluation
mark, demonstrating superior performance due to enhanced
on the test set. DeepRoBERTa tokeniser + RoBERTa model
pretraining and post-training methods.
achieved the RMSE and MAE scores of 6.047 and 4.885,
For using the Llama-3 model, we used the Llama-3-8B- respectively; GPT 3.5 achieved the RMSE, MAE, and CCC
instructmodelforthepromptingtask.SimilartoGPTmodels, scores of 5.896, 4.589 and 0.474, respectively, and LLAMA
wehaveperformedtwo-shotprompting(asshowninTablesV) 8B Instruct model achieved the RMSE, MAE and CCC
with a slightly different prompting technique as we have not scores of 6.293, 4.893 and 0.494. On the other hand, GPT 4
usedachatinterfacebutfollowedtheinstructmodelformatto achieved the RMSE, MAE and CCC scores of 3.975, 3.161
interactwiththemodel.Wehavenotedtheseoutputscoresand and 0.781, respectively, beating the current SOTA results for
classes from model output and then computed the regression textualregression[14].TheknowledgeofPHQquestionnaires
and classification results. helps the models accurately predict the subject’s state as
We have also experimented with fine-tuning llama-3-8B they try to identify the PHQ-8 answers and scores from
on our textual data, using 4-bit Quantization and LoRA the transcript text and aggregate them to determine the
techniques [47] on our 219 samples from the training and PHQ-8 score. The knowledge of class division defined in
validation set To perform efficient training on available hard- two-shotpromptsalsohelpspredictthecorrectscoreandlabel.
ware.Thefine-tunedmodelresponsegenerationsufferedfrom9
Model RMSE
BiLSTM(Audio) 5.39
Whisper(Audio) 5.7
Bi-LSTM(Video) 6.45
Whisper+BiLSTM(Audio+Video) 6.51
TABLE VII: RMSE Scores of Audio-Visual Modalities on
Validation Set
3) Comparative Analysis: Table VIII compares RMSE
and CCC scores of various state-of-the-art architectures with
models trained or prompt engineered on various modalities
(mainly textual modality) when inferred on a test set. From
Table VIII, it is evident that the DepRoBERTa tokeniser and
RoBERTa model outperformed the baseline models cited by
the competition [7] and the models proposed by Steijn et
al. (using multi-task regression symptom predictions) [26],
Makiuchi et al. [12], and Zhang et al. [10] in terms of
RMSEscores.Additionally,itperformedbetterthanthemodel
proposed by Teng et al. [23] and Zhang et al. [10] regarding
MAE.
TheLLAMA8Binstructmodelexceededthecompetition’s
baseline models [7] and the model by Zhang et al. [10]
regarding RMSE score. It also outperforms models by Teng
et al. [23] and Zhang et al. [10] in MAE and performed
betterthancompetition’sbaselinemodel[7]aswellasmodels
proposed by Fan et al. [11], Yin et al. [13], Teng et al.
[23], Makiuchi et al. [12], and Saggu et al. [19] in terms of
CCC score. The GPT 3.5 model, using two-shot prompting,
also surpassed the competition’s baseline models [7] and the
models by Fan et al. [11], Steijn et al. (with multi-task
regressionsymptompredictions)[26],Makiuchietal.[12]and
Zhangetal.[10]inRMSEscoreswhileperformingbetterthan
Fig. 6: Multimodal architecture (Whisper + BiLSTM), In the
Tengetal.’smodel[23],Sunetal.[21]andZhangetal.[6]in
proposed network, we use only the Whisper encoder to pass the
MAE.Additionally,itperformedbetterthanthecompetition’s
encodings of audio to the next layer after being concatenated with
videofeaturestoaBiLSTMnetwork.TheVideofeaturesarepassed baseline model [7] as well as models proposed by Fan et al.
through a separate BiLSTM network before concatenation. The [11],Yinetal.[13],Tengetal.[23],Makiuchietal.[12],and
output of BiLSTM network is feeded to a regression layer, which Saggu et al. [19] in terms of CCC score.
outputs the Predicted PHQ-8 scores
Moreover, the GPT 4 model outperformed all current state-
of-the-art models and the competition’s baseline in RMSE,
Model RMSE MAE CCC
MAE, and CCC scores on the test set, establishing new
DepROBERTaTokenizer+RoBERTaModel 6.047 4.885 -
GPT3.5 5.896 4.589 0.474 SOTAperformance.Inaudioandvisualmodalities,themodels
GPT4 3.975 3.161 0.781 performedbetterthancompetitionbaselinemodels[7]butnot
LLAMA8BInstruct 6.293 4.893 0.494 much better than other SOTA approaches regarding RMSE
TABLE VI: RMSE, MAE and CCC Scores of Textual Modality scores on the validation set.
on Test Set
B. Classification Analysis
2) Results on Audio and Visual Modalities: The results This section discusses the classification results of all the
obtained from different audio and visual modalities models models in textual modality. Although we tried performing
on the validation set in terms of RMSE, MAE and CCC are classification on audio video experiments by modifying the
shown in Table VII. The whisper model on audio features architectures by replacing the last regressor layer with a
achieves an RMSE score of 5.7 on the validation set, whereas classification layer and cross-entropy loss, the results were
the Bi-LSTM model on visual features achieves an RMSE of unsatisfactory; the confusion matrices showed all the samples
6.45. On merging both audio and visual features, we achieved classified as the healthy class. Therefore, the results are
an RMSE score of 6.72 on the validation set when they were computed for the test set based solely on the textual modality,
trainedontheWhisperandBiLSTMmodel,whichis6.51and including metrics such as accuracy, F1 scores, precision, and
an RMSE score of 5.39 on the validation set when trained on recallscores.TheresultsareinTableIXandthecorresponding
the BiLSTM model. confusion matrices are in figures 7 8 9:10
Model Modality RMSE MAE CCC
Sadeghietal.[27] T 5.36 4.26 -
Rayetal.[14] T 4.37 4.02 0.67
Ringevaletal.(AVEC2019DDSChallenge
A,V,T 6.37 - 0.111
BaselineResults)[7]
Fanetal.[11] A,T 5.91 4.39 0.43
Yinetal.[13] A,V,T 5.50 - 0.442
Sunetal.[17] A,V,T - 4.37 0.583
Yuanetal.[18] A,V,T 4.91 3.98 0.676
Steijnetal.(Methodology3)[26] T 6.06 - 0.62
Steijnetal.(Methodology5)[26] T 5.39 - 0.53
Tengetal.[23] A,V,T - 5.21 0.466
Makiuchietal.[12] A,T 6.11 - 0.403
Sagguetal.[19] A,V,T 5.36 - 0.457
Lietal.[24] A,V 4.80 4.58 -
Zhangetal.[6] A 6.78 5.77 -
Sunetal.[21] A,V,T 4.31 - 0.491
DepRoBERTaTokenizer+RoBERTaModel
T 6.047 4.885 -
(Ours)
Whisper+BiLSTM(Ours) A,V 6.51 - -
GPT3.5(Ours) T 5.896 4.589 0.474
GPT4(Ours) T 3.975 3.161 0.781
LLAMA8BInstruct(Ours) T 6.293 4.893 0.494
TABLE VIII: Comparative Analysis concerning RMSE, MAE and CCC Scores on Test Set
Model GPT3.5 GPT4 LLAMA38B
Accuracy 58.93% 71.43% 73.21%
Macroaverageprecision 57.94% 65.96% 72.72%
Weightedaverageprecision 65.73% 71.53% 81.30%
Macroaveragerecall 60.00% 66.53% 78.19%
Weightedaveragerecall 58.93% 71.43% 73.21%
MacroaverageF1 57.93% 66.18% 72.35%
WeightedaverageF1 60.59% 71.44% 73.99%
TABLE IX: Classification Analysis using GPT 3.5, GPT 4 and
LLAMA 3 8B
Fig. 8: Confusion Matrix of GPT 4 Model on Test Set
Fig. 7: Confusion Matrix of GPT 3.5 Model on Test Set
Overall, the LLAMA 3 8B model performs better than
other approaches for classification tasks done via a few shot
promptingonthetestset,butitstillneedsfurtherimprovement
Fig. 9: Confusion Matrix of LLAMA 3 8B Model on Test set
for real-time applications. Meanwhile, GPT-4 prompting has
proved to perform better for regression than the Llama 3 8B
models.
LLMsfortext-basedtaskssuchasregressionandclassification
over other proposed architectures.
VII. CONCLUSIONANDFUTURESCOPE
A significant challenge during our experimentations was
In our experimentations, we have demonstrated multiple the limited number of samples in the dataset. The increased
approachesondifferentmodalitiesandachievedaSOTAresult number of samples might result in better results by fine-
on regression analysis of two-shot prompting from the GPT tuning LLMs for textual modality. Also, raw video files
-4model.TheseExperimentsshowthesuperiorcapabilitiesof and audio availability might result in more comprehensive11
behavioural clues for analysis and detection. The method of [14] A. Ray, S. Kumar, R. Reddy, P. Mukherjee, and R. Garg, “Multi-
Data augmentation to increase the number of samples might level Attention Network using Text, Audio and Video for Depression
Prediction,” in Proceedings of the 9th International on Audio/Visual
not be ethical, given the nature and sensitivity of the data and
Emotion Challenge and Workshop, in AVEC ’19. New York, NY,
the patient’s privacy. Hence, reliable and valid data annotated USA:AssociationforComputingMachinery,Oct.2019,pp.81–88.doi:
by clinical experts must be created. As seen in this paper, 10.1145/3347320.3357697.
[15] Z.Zhang,W.Lin,M.Liu,andM.Mahmoud,“MultimodalDeepLearn-
the Textual modality, due to LLMs performance, was leading
ing Framework for Mental Disorder Recognition,” in 2020 15th IEEE
among all other modalities. For future scope, multimodal International Conference on Automatic Face and Gesture Recognition
LLM-based architecture might perform better than the current (FG2020),Nov.2020,pp.344–350.doi:10.1109/FG47880.2020.00033.
[16] A.-H. Jo and K.-C. Kwak, “Diagnosis of Depression Based on Four-
SOTA networks.
StreamModelofBi-LSTMandCNNFromAudioandTextInformation,”
IEEE Access, vol. 10, pp. 134113–134135, 2022, doi: 10.1109/AC-
CESS.2022.3231884.
REFERENCES [17] H. Sun, H. Wang, J. Liu, Y.-W. Chen, and L. Lin, “CubeMLP: An
MLP-based Model for Multimodal Sentiment Analysis and Depression
[1] Z.Li,M.Ruan,J.Chen,andY.Fang,“MajorDepressiveDisorder:Ad- Estimation,”inProceedingsofthe30thACMInternationalConferenceon
vances in Neuroscience Research and Translational Applications,” Neu- Multimedia,inMM’22.NewYork,NY,USA:AssociationforComputing
rosci.Bull.,vol.37,no.6,pp.863–880,Jun.2021,doi:10.1007/s12264- Machinery,Oct.2022,pp.3722–3729.doi:10.1145/3503161.3548025.
021-00638-3. [18] C. Yuan, Q. Xu, and Y. Luo, “Depression Diagnosis and Analysis
[2] “Depressive disorder (depression).” Accessed: Apr. 12, 2024. [Online]. via Multimodal Multi-order Factor Fusion.” arXiv, Dec. 31, 2022. doi:
Available:https://www.who.int/news-room/fact-sheets/detail/depression. 10.48550/arXiv.2301.00254.
[3] B.Schuller,M.Valstar,F.Eyben,G.McKeown,R.Cowie,andM.Pantic, [19] G. S. Saggu, K. Gupta, and K. V. Arya, “DepressNet: A Multimodal
“AVEC 2011–The First International Audio/Visual Emotion Challenge,” Hierarchical Attention Mechanism approach forDepression Detection,”
in Affective Computing and Intelligent Interaction, S. D’Mello, A. InternationalJournalofEngineeringSciences,vol.15,no.1,2022,doi:
Graesser,B.Schuller,andJ.-C.Martin,Eds.,Berlin,Heidelberg:Springer, 10.36224/ijes.150104.
2011,pp.415–424.doi:10.1007/978-3-642-24571-8-53. [20] C. Wang et al., “A Multi-modal Feature Layer Fusion Model for
[4] M. Valstar et al., “AVEC 2013: the continuous audio/visual emotion Assessment of Depression Based on Attention Mechanisms,” in 2022
and depression recognition challenge,” in Proceedings of the 3rd ACM 15thInternationalCongressonImageandSignalProcessing,BioMedical
internationalworkshoponAudio/visualemotionchallenge,inAVEC’13. Engineering and Informatics (CISP-BMEI), Nov. 2022, pp. 1–6. doi:
NewYork,NY,USA:AssociationforComputingMachinery,Oct.2013, 10.1109/CISP-BMEI56279.2022.9979894.
pp.3–10.doi:10.1145/2512530.2512533. [21] H.Sun,Y.-W.Chen,andL.Lin,“TensorFormer:ATensor-BasedMul-
[5] M.Valstaretal.,“AVEC2016:Depression,Mood,andEmotionRecog- timodalTransformerforMultimodalSentimentAnalysisandDepression
nitionWorkshopandChallenge,”inProceedingsofthe6thInternational Detection,” IEEE Transactions on Affective Computing, vol. 14, no. 4,
WorkshoponAudio/VisualEmotionChallenge,inAVEC’16.NewYork, pp.2776–2786,Oct.2023,doi:10.1109/TAFFC.2022.3233070.
NY, USA: Association for Computing Machinery, Oct. 2016, pp. 3–10. [22] K.Maoetal.,“PredictionofDepressionSeverityBasedontheProsodic
doi:10.1145/2988257.2988258. and Semantic Features With Bidirectional LSTM and Time Distributed
[6] F.Ringevaletal.,“AVEC2017:Real-lifeDepression,andAffectRecog- CNN,” IEEE Transactions on Affective Computing, vol. 14, no. 3, pp.
nition Workshop and Challenge,” in Proceedings of the 7th Annual 2251–2265,Jul.2023,doi:10.1109/TAFFC.2022.3154332.
WorkshoponAudio/VisualEmotionChallenge,inAVEC’17.NewYork, [23] S.Teng,S.Chai,J.Liu,T.Tateyama,L.Lin,andY.-W.Chen,“Multi-
NY,USA:AssociationforComputingMachinery,Oct.2017,pp.3–9.doi: ModalandMulti-TaskDepressionDetectionwithSentimentAssistance,”
10.1145/3133944.3133953. in2024IEEEInternationalConferenceonConsumerElectronics(ICCE),
[7] F.Ringevaletal.,“AVEC2019WorkshopandChallenge:State-of-Mind, Jan.2024,pp.1–5.doi:10.1109/ICCE59016.2024.10444213.
DetectingDepressionwithAI,andCross-CulturalAffectRecognition,”in [24] Y. Li et al., “FPT-Former: A Flexible Parallel Transformer of Recog-
Proceedingsofthe9thInternationalonAudio/VisualEmotionChallenge nisingDepressionbyUsingAudiovisualExpert-Knowledge-BasedMulti-
andWorkshop,inAVEC’19.NewYork,NY,USA:AssociationforCom- modalMeasures,”InternationalJournalofIntelligentSystems,vol.2024,
putingMachinery,Oct.2019,pp.3–12.doi:10.1145/3347320.3357688. p.e1564574,Jan.2024,doi:10.1155/2024/1564574.
[8] J.Gratchetal.,“TheDistressAnalysisInterviewCorpusofhumanand [25] D.Gimeno-Go´mez,A.-M.Bucur,A.Cosma,C.-D.Mart´ınez-Hinarejos,
computerinterviews”. and P. Rosso, “Reading Between the Frames: Multi-Modal Depression
[9] D. DeVault et al., “SimSensei kiosk: a virtual human interviewer for Detection in Videos from Non-Verbal Cues.” arXiv, Jan. 05, 2024. doi:
healthcare decision support,” in Proceedings of the 2014 international 10.48550/arXiv.2401.02746.
conferenceonAutonomousagentsandmulti-agentsystems,inAAMAS [26] F. Van Steijn, G. Sogancioglu, and H. Kaya, “Text-based Interpretable
’14.Richland,SC:InternationalFoundationforAutonomousAgentsand DepressionSeverityModelingviaSymptomPredictions,”inProceedings
MultiagentSystems,May2014,pp.1061–1068. ofthe2022InternationalConferenceonMultimodalInteraction,inICMI
[10] L. Zhang, J. Driscol, X. Chen, and R. Hosseini Ghomi, “Evaluating ’22. New York, NY, USA: Association for Computing Machinery, Nov.
AcousticandLinguisticFeaturesofDetectingDepressionSub-Challenge 2022,pp.139–147.doi:10.1145/3536221.3556579.
Dataset,” in Proceedings of the 9th International on Audio/Visual Emo- [27] M. Sadeghi et al., “Exploring the Capabilities of a Language Model-
tion Challenge and Workshop, in AVEC ’19. New York, NY, USA: Only Approach for Depression Detection in Text Data,” in 2023 IEEE
Association for Computing Machinery, Oct. 2019, pp. 47–53. doi: EMBS International Conference on Biomedical and Health Informatics
10.1145/3347320.3357693. (BHI),Oct.2023,pp.1–5.doi:10.1109/BHI58575.2023.10313367.
[11] W.Fan,Z.He,X.Xing,B.Cai,andW.Lu,“Multi-modalityDepression [28] M. Danner et al., “Advancing Mental Health Diagnostics: GPT-Based
DetectionviaMulti-scaleTemporalDilatedCNNs,”inProceedingsofthe MethodforDepressionDetection,”in202362ndAnnualConferenceof
9thInternationalonAudio/VisualEmotionChallengeandWorkshop,in theSocietyofInstrumentandControlEngineers(SICE),Sep.2023,pp.
AVEC’19.NewYork,NY,USA:AssociationforComputingMachinery, 1290–1296.doi:10.23919/SICE59929.2023.10354236.
Oct.2019,pp.73–80.doi:10.1145/3347320.3357695. [29] B. Hadzic et al., “Enhancing early depression detection with AI: a
[12] M. Rodrigues Makiuchi, T. Warnita, K. Uto, and K. Shinoda, “Mul- comparativeuseofNLPmodels,”SICEJournalofControl,Measurement,
timodal Fusion of BERT-CNN and Gated CNN Representations for and System Integration, vol. 17, no. 1, pp. 135–143, Dec. 2024, doi:
Depression Detection,” in Proceedings of the 9th International on Au- 10.1080/18824889.2024.2342624.
dio/VisualEmotionChallengeandWorkshop,inAVEC’19.NewYork, [30] A.Anand,A.Goel,M.Hira,S.Buldeo,J.Kumar,A.Verma,R.Gupta,
NY,USA:AssociationforComputingMachinery,Oct.2019,pp.55–63. andR.R.Shah,”SciPhyRAG-RetrievalAugmentationtoImproveLLMs
doi:10.1145/3347320.3357694. onPhysicsQ&A,”inProc.Int.Conf.BigDataAnalytics,Springer,2023,
[13] S.Yin,C.Liang,H.Ding,andS.Wang,“AMulti-ModalHierarchical pp.50-63.
RecurrentNeuralNetworkforDepressionDetection,”inProceedingsof [31] A.Anand,J.Kapuriya,A.Singh,J.Saraf,N.Lal,A.Verma,R.Gupta,
the9thInternationalonAudio/VisualEmotionChallengeandWorkshop, and R. Shah, ”MM-PhyQA: Multimodal Physics Question-Answering
inAVEC’19.NewYork,NY,USA:AssociationforComputingMachin- with Multi-image CoT Prompting,” in Proc. Pacific-Asia Conf. Knowl-
ery,Oct.2019,pp.65–71.doi:10.1145/3347320.3357696. edgeDiscoveryandDataMining,Springer,2024,pp.53-64.12
[32] A.Anand,M.Gupta,K.Prasad,N.Singla,S.Sanjeev,J.Kumar,A.R.
Shivam, and R. R. Shah, ”Mathify: Evaluating Large Language Models
onMathematicalProblemSolvingTasks,”NeurIPS,2023.
[33] A.Anand,J.Kapuriya,C.Kirtani,A.Singh,J.Saraf,N.Lal,J.Kumar,
A. R. Shivam, A. Verma, R. R. Shah, and others, ”MM-PhyRLHF:
Reinforcement Learning Framework for Multimodal Physics Question-
Answering,”arXivpreprintarXiv:2404.12926,2024.
[34] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “OpenFace
2.0:FacialBehaviorAnalysisToolkit,”in201813thIEEEInternational
ConferenceonAutomaticFace&GestureRecognition(FG2018),May
2018,pp.59–66.doi:10.1109/FG.2018.00019.
[35] F. Eyben, M. Wo¨llmer, and B. Schuller, “Opensmile: the munich
versatile and fast open-source audio feature extractor,” in Proceedings
of the 18th ACM international conference on Multimedia, in MM ’10.
NewYork,NY,USA:AssociationforComputingMachinery,Oct.2010,
pp.1459–1462.doi:10.1145/1873951.1874246.
[36] Eyben, Florian, Klaus R. Scherer, Bjo¨rn W. Schuller, Johan Sundberg,
ElisabethAndre´,CarlosBusso,LaurenceY.Devillersetal.”TheGeneva
minimalistic acoustic parameter set (GeMAPS) for voice research and
affectivecomputing.”IEEEtransactionsonaffectivecomputing7,no.2
(2015):190-202.
[37] Lawrence, I., and Kuei Lin. ”A concordance correlation coefficient to
evaluatereproducibility.”Biometrics(1989):255-268.
[38] Kroenke, Kurt, Tara W. Strine, Robert L. Spitzer, Janet BW Williams,
JoyceT.Berry,andAliH.Mokdad.”ThePHQ-8asameasureofcurrent
depressioninthegeneralpopulation.”Journalofaffectivedisorders114,
no.1-3(2009):163-173.‘
[39] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I.
Sutskever, “Robust Speech Recognition via Large-Scale Weak Supervi-
sion.”arXiv,Dec.06,2022.doi:10.48550/arXiv.2212.04356.
[40] Y. Liu et al., “RoBERTa: A Robustly Optimised BERT Pretraining
Approach.”arXiv,Jul.26,2019.doi:10.48550/arXiv.1907.11692.
[41] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“BERT:Pre-training
ofDeepBidirectionalTransformersforLanguageUnderstanding.”arXiv,
May24,2019.doi:10.48550/arXiv.1810.04805.
[42] R.Pos´wiataandM.Perełkiewicz,“OPI@LT-EDI-ACL2022:Detecting
SignsofDepressionfromSocialMediaTextusingRoBERTaPre-trained
LanguageModels,”inProceedingsoftheSecondWorkshoponLanguage
Technology for Equality, Diversity and Inclusion, B. R. Chakravarthi,
B. Bharathi, J. P. McCrae, M. Zarrouk, K. Bali, and P. Buitelaar, Eds.,
Dublin, Ireland: Association for Computational Linguistics, May 2022,
pp.276–282.doi:10.18653/v1/2022.ltedi-1.40.
[43] OpenAI. (2021). ChatGPT (Version 3.5) [Software]. Retrieved from
https://openai.com/.
[44] Radford,Alec,KarthikNarasimhan,TimSalimans,andIlyaSutskever.
”Improvinglanguageunderstandingbygenerativepre-training.”(2018).
[45] OpenAI et al., “GPT-4 Technical Report.” arXiv, Mar. 04, 2024. doi:
10.48550/arXiv.2303.08774.
[46] “IntroducingMetaLlama3:ThemostcapableopenlyavailableLLMto
date,”ai.meta.com.https://ai.meta.com/blog/meta-llama-3/.
[47] Hu,EdwardJ.,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,Yuanzhi
Li,SheanWang,LuWang,andWeizhuChen.”Lora:Low-rankadaptation
oflargelanguagemodels.”arXivpreprintarXiv:2106.09685(2021).