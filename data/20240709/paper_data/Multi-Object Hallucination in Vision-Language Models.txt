Multi-Object Hallucination in Vision-Language Models
XuweiyiChen1,2*,ZiqiaoMa1*,XuejunZhang1*
SihanXu1,ShengyiQian1,JianingYang1,DavidF.Fouhey3,JoyceChai1
1UniversityofMichigan 2UniversityofVirginia 3NewYorkUniversity
https://multi-object-hallucination.github.io/
Abstract entities. Despitetheirpromisingperformanceson
Largevisionlanguagemodels(LVLMs)often variousdownstreamapplications(Nasirianyetal.,
sufferfromobjecthallucination,producingob- 2024), LVLMs often suffer from object halluci-
jects not present in the given images. While nation (Rohrbach et al., 2018; Dai et al., 2023b;
currentbenchmarksforobjecthallucinationpri- Li et al., 2023a), where they produce objects not
marilyconcentrateonthepresenceofasingle
presentinagivenimage.
objectclassratherthanindividualentities,this
Although object hallucination was initially ob-
worksystematicallyinvestigatesmulti-object
servedinimagecaptioningdescribingmultipleob-
hallucination,examininghowmodelsmisper-
jects(Rohrbachetal.,2018),currentbenchmarks
ceive (e.g., invent nonexistent objects or be-
come distracted) when tasked with focusing for object hallucination primarily concentrate on
onmultipleobjectssimultaneously. Weintro- thepresenceofasingleobjectclassratherthanin-
duceRecognition-basedObjectProbingEval- dividualentities. Thesebenchmarkseitherverifyif
uation(ROPE),anautomatedevaluationpro-
anobjectclassmentionedinthecaptioncanground
tocolthatconsidersthedistributionofobject
to an object in the image (Rohrbach et al., 2018;
classes within a single image during testing
Jingetal.,2023),orprobethemodelabouttheexis-
andusesvisualreferringpromptstoeliminate
tenceofanobjectclass,sometimeswithadditional
ambiguity. Withcomprehensiveempiricalstud-
iesandanalysisofpotentialfactorsleadingto attributes or relations to other objects (Li et al.,
multi-object hallucination, we found that (1) 2023a;Loveniaetal.,2023). Thereare,however,
LVLMssuffermorehallucinationswhenfocus- two key limitations with these setups as shown
ing on multiple objects compared to a single by a case study in Figure 1. First, grounding is
object. (2)Thetestedobjectclassdistribution
notsimplyone-to-onebetweenobjectsandclasses,
affectshallucinationbehaviors,indicatingthat
butamany-to-manymappingbetweenobjectsand
LVLMsmayfollowshortcutsandspuriouscor-
phrases(Kamathetal.,2021;Maetal.,2023). For
relations. (3) Hallucinatory behaviors are in-
instance,‚Äúapple‚Äùcouldpotentiallycorrespondto
fluencedbydata-specificfactors,salienceand
frequency,andmodelintrinsicbehaviors. We multiple referents in Figure 1(c), and the model
hopetoenableLVLMstorecognizeandreason doesn‚Äôtnecessarilyneedtorecognizeallofthemto
aboutmultipleobjectsthatoftenoccurinrealis- providesucharesponse. Therefore,beingableto
ticvisualscenes,provideinsights,andquantify
produceanobjectthatexistsinanimagedoesnot
ourprogresstowardsmitigatingtheissues.
necessarilyindicatethatthemodelisfreeofhalluci-
1 Introduction nations. Second,explicitlyinstructingthemodelto
recognizemultipleobjectsposesgreaterchallenges
Recentadvancesinlargelanguagemodels(LLMs)
compared to simple yes/no inquiries that contain
have motivated increasing efforts in adapting
explicittextdescriptionsforindividualobjects. For
them for understanding visual semantics, giving
instance, while the model can correctly identify
rise to a surge of large vision language models
that a whisk is positioned to the left of a knife
(LVLMs) (Alayrac et al., 2022; OpenAI, 2023;
when‚Äúawhisk‚Äùisdeliberatelyprompted,asshown
Reid et al., 2024). These models, whether ex-
inFigure1(b-d),itmayhallucinatea‚Äúfork‚Äùwhen
plicitlytrainedwithgroundingdata(Zhangetal.,
directlypromptedtorecognizeboththewhiskand
2024b)orwithout(Liuetal.,2023b),demonstrate
the knife (i.e., Figure 1a). This could be due to
an impressive grounded understanding of visual
thecommonassociationbetweenknivesandforks,
*Equalcontribution,alphabetizedbylastnames. whichleadstopotentialhallucinationswhenmod-
1
4202
luJ
8
]VC.sc[
1v29160.7042:viXraüë§
Provide a detailed description of the given image.
User
... To the side, there's a bottle of water, and utensils
including a whisk, a knife, and some spoons placed
inside a blue mug‚Ä¶ GPT-4V
(b) Captioning-based evaluation.
üë§
Is there an apple in this image?
User
Yes, there is an apple.
üë§ GPT-4V
Is there a whisk in this image?
User
Yes, there is a whisk in the image, placed next to a
knife and a bowl on the right side of the table. GPT-4V
Select one and the most appropriate class for (c) Polling-based object probing.
each object located within red bounding
boxes from the following list: apple, orange, üë§ Is there a whisk next to a knife?
banana, lemon, pear, plate, bowl, jar, User
Yes, there is a whisk next to a knife on the table in
bottle, soda can, knife, fork, whisk, scissors,
üë§ packaged snack. the image you provided. GPT-4V
(d) Object attribute/relation probing.
User Provide the class names in the format: 'obj1:
<class1>, obj2: <class2>, obj3: <class3>, obj4: üë§ Does the caption accurately describe the image:
<class4>, obj5: <class5>', with no additional User "A whisk is placed to the right of a knife."
words or punctuations.
No, the caption does not accurately describe the
obj1: apple, obj2: knife, obj3: fork, image. The whisk is actually placed to the left of the
obj4: apple, obj5: jar GPT-4V knife on the table. GPT-4V
(a) Recognition-based object probing. (e) Counterfactual probing.
Figure1: AcasestudythatcomparesourRecognition-basedObjectProbingEvaluation(ROPE)benchmarkwith
existing benchmarks for object hallucination in GPT-4V. ROPE offers an automated evaluation protocol with
controlledoutputformattingandusesvisualpromptstodistinctlygroundtoobjects,thusmitigatingreferential
ambiguity. Unlikebinaryinquiriesrelyingsolelyontextualdescriptions,ROPEchallengesthemodeltoidentify
multipleobjectsconcurrently. Weobservethat,whileGPT-4Vcanidentifythewhisktotheleftofaknifewhen
promptedaboutit,themodelhallucinatesa‚Äúfork‚Äùwhendirectlytaskedtorecognizemultipleobjects.
els are tasked to focus on multiple objects at the thehallucinationbehaviorsofLVLMsofdifferent
same time. To enable LVLMs to recognize and scales and training data (e.g., whether grounding
reason about multiple objects that often occur in data and conversational data are used), and pro-
realistic visual scenes and to better quantify the videacomprehensiveanalysisofpotentialfactors
complex phenomena we observed, this paper in- that lead to multi-object hallucination. Our main
vestigates multi-object hallucination, examining findingsare: (1)LVLMssufferfrommorehalluci-
how models may misperceive (e.g., by inventing nationswhentaskedtofocusonmultipleobjects,
nonexistentobjectsorbecomingdistracted)when compared to focusing on a single object; (2) The
tasked to focus on multiple objects concurrently, testedobjectclassdistributionaffectsthehallucina-
andwhichfactorscausethehallucinations. tionbehaviors,revealingthatLVLMsmaybefol-
lowingshortcutsandspuriouscorrelations;(3)The
WestartbyintroducingRecognition-basedOb-
hallucinatorybehaviorsofLVLMsareaffectedby
ject Probing Evaluation (ROPE) for assessing
data-specificfactors,salienceandfrequency,and
multi-object hallucination with formatted output
modelintrinsicbehaviors. Thesefindingsprovide
control. ROPE features an automated evaluation
key insights for the development and application
protocol without black-box neural models or hu-
of LVLMs, suggesting for more balanced object
mansasevaluators,andleveragesvisualprompts
distributions, diverse annotations, and enhanced
touniquelyrefertoobjectstoavoidambiguityand
multi-objectinstructionsingroundedLVLMs. We
multiple referents caused by object class names.
hope this work takes a step towards LVLMs that
ROPEconsidersthedistributionofobjectclasses
recognize and reason about multiple objects that
withineachimageattesttime,dividingROPEinto
oftenoccurinrealisticvisualscenes.
4 subsets: In-the-Wild, Homogeneous, Heteroge-
neous, and Adversarial. For instance, we investi-
2 RelatedWork
gatescenarioswherealltestedobjectsbelongtothe
sameclassorwhereeachtestedobjectrepresentsa Large Vision-Language Models. There is a
differentclass. Weconductanin-depthanalysisof growing trend to harness and adapt the powerful
2largelanguagemodels(LLMs)formultimodalun- DesignConsiderations
Benchmark #Test
derstandingbeyondtext(Tsimpoukellietal.,2021; Multi. Distr. Source Ref. Eval.
Alayrac et al., 2022; Yu et al., 2024a; Qian et al., CCEval(2023) ‚úî Seen Text N 0.1k
2024). Especially, visual instruction tuning has
GAVIE(2023a) ‚úî Mixed Text N 1k
FAITHScore(2023) ‚úî Unseen Text N 2k
gainedprominenceforitscompetitiveperformance HaELM(2023b) ‚úî Unseen Text N 5k
with a comparatively moderate amount of data M-HalDetect(2024) ‚úî Unseen Text H 0.8k
andcomputationalresources,leadingtoavariety MMHal-Bench(2023) ‚úî Unseen Text N,H 0.1k
ofLargeVision-LanguageModels(LVLMs)(Liu
CHAIR(2018) ‚úî Unseen A 46k
AMBER(2023a) ‚úî Unseen Text A 1k
et al., 2023b, 2024; Dai et al., 2023a; Zhu et al., CIEM(2023) ‚úî Unseen Text A 5k
2023; Gong et al., 2023; Wang et al., 2023d; Ye NOPE(2023) Unseen A 3k
et al., 2023; Lauren√ßon et al., 2023a). Ground- POPE(2023a) Train Unseen A 0.5k
ing datasets have been shown to benefit vision- Seen&
ROPE(Ours) ‚úî Test Unseen Vis. A 5k
language pre-training (Lu et al., 2022; Li et al.,
2022;Maetal.,2023). Researchershavedeveloped Table 1: An overview of object hallucination bench-
a family of grounded LVLMs focusing on object marksetups. Wesummarizethenumberoftestedim-
groundingtoboundingbox(Pietal.,2023;Chen ages,andifmultipleclassesandobjectclassdistribution
(at training and test time) are considered. The image
etal.,2023;Zhaoetal.,2023;Baietal.,2023;You
sourcesincludethoseseenorunseenduringinstruction
etal.,2023;Zhangetal.,2024a;Pengetal.,2024)
tuning. Torefertoanobject,textualdescriptionsand
and segmentation masks (Lai et al., 2023; Zhang
visualcuescanbeadopted. Forevaluation,neuralmod-
etal.,2023;Xiaetal.,2024;Rasheedetal.,2024; els,humans,andautomaticpipelinesareused.
Zhangetal.,2024b). OfthelargespaceofLVLMs,
ourworkismostrelatedtovisualprompting(Yang ObjectHallucination. Despitetheirpromising
etal.,2023a,b)andobjecthallucination(Rohrbach performance on benchmarks, these models fre-
et al., 2018; Dai et al., 2023b). The paragraphs quently generate objects that do not exist in the
belowdescribethetwolinesofworkindetail. providedimages,aproblemknownasobjecthallu-
cination(Rohrbachetal.,2018;Daietal.,2023b).
Visual Prompting. LVLMs demonstrate their Severalmethodshavebeensuggestedtomitigate
grounded understanding of user-provided visual theobjecthallucinationissue,suchasintegrating
cues, giving rise to a practical and user-friendly anexternalobjectdetector(Zhaietal.,2023),ap-
prompting paradigm known as visual prompt- plying visually grounded visual instruction tun-
ing (Yang et al., 2023a,b). Early work on vi- ing(Youetal.,2023;Zhangetal.,2024b)orrein-
sual prompting in vision-language models can forcementlearning(Sunetal.,2023;Gunjaletal.,
date back to tuning-based methods (Bahng et al., 2024),performingiterativerefinement(Zhouetal.,
2022;Yaoetal.,2024). Recentstudiesshowthat 2024),andadaptingthedecodingstrategies(Huang
LVLMs demonstrate zero-shot understanding of et al., 2024). To quantify progress on mitigating
user-providedvisualcues(e.g.,aredcircle)(Sht- them, various benchmarks have been developed
edritskietal.,2023;Yangetal.,2023b). Thisobser- andhaverevealedtheprevalenceofobjecthalluci-
vationallowspromptingLVLMsbyeditingimages nation,eveninimagesthatareseenduringinstruc-
directly in the pixel space, e.g., by adding visual tion tuning (Zhai et al., 2023; Liu et al., 2023a).
marks or visual text (Li et al., 2023b). Starting We contrast our ROPE benchmark against exist-
fromSet-of-Marks(SoM)prompting(Yangetal., ing benchmarks and setups in Table 1. ROPE,
2023a), several training-free methods have been whichisdesignedforevaluatingmulti-objecthal-
introduced(Leietal.,2024;Yangetal.,2024;Wan lucination,isdistinguishedinseveralways. First,
etal.,2024). Recentworkfurtherenhancesvisual wedeliberatelyconsiderthedistributionofobject
promptingbyadditionalvisualinstructiontuning classeswithinasingleimageattesttime. Object
with diverse visual prompts overlaid on the im- hallucinationisobservedoriginallyinimagecap-
ages (Cai et al., 2024), or explicit visual pointer tioning applications, where multiple objects are
tokens in the models (Lai et al., 2023; You et al., described (Rohrbach et al., 2018). While exist-
2023; Zhang et al., 2024b). We leverage visual ingresearchhasdemonstratedthattheobjectclass
promptingtoavoidpotentialambiguityintextual distribution in the instruction tuning dataset can
descriptions,especiallywhenevaluatingmultiple influence hallucination patterns (Li et al., 2023a;
objecthallucinationsforobjectsofthesameclass. Zhou et al., 2024; Wang et al., 2023a), the im-
3obj1: ?, obj2: ?, obj1: ?, obj2: ?,
üë§
obj3: ?, obj4: ?, obj5: ? obj3: ?, obj4: ?, obj5: ?
User
obj1: obj1:
Predicted
Ground
apple class apple truth
LVLM
obj1: apple, obj2: Predicted obj1: fork, obj2:
class Ground
üîí apple knife truth
Forced obj1: apple, obj2: apple, obj3: obj1: fork, obj2: knife, obj3:
apple fork
obj1: ? soda can obj1: ?, obj2: ?,
‚Ä¶ ‚Ä¶
obj3: ?, obj4: ?, obj5: ?
obj2: ? knife
obj1: apple, obj2: apple, obj1: fork, obj2: knife,
obj1: apple, obj2: apple,
‚Ä¶ obj3: apple, obj4: lemon, obj5: obj3: whisk, obj4: lemon, obj5:
obj3: apple, obj4: lemon,
obj5: ? soda can obj5: pear pear soda can
(a) Single-object. (b) Multi-object. (c) Student forcing. (d) Teacher forcing.
Figure2: DifferenttypesofinstructionsettingsofROPE.Inasingleturnofpromptingwithoutformatenforcement,
weprobethemodeltorecognizethe5objectsreferredtobythevisualprompts(a)oneatatimeinthesingle-object
settingand(b)concurrentlyinthemulti-objectsetting. Wefurtherenforcethemodeltofollowtheformattemplate
anddecodeonlytheobjecttokensforeachofthefiveobjects(c)withoutoutputmanipulationinstudentforcing
and(d)replacingallpreviouslygeneratedobjecttokenswiththegroundtruthclassesinteacherforcing.
pact of object class distribution within an image modelsaretaskedwithrecognizing5objectsout
attesttimeremainsunder-explored. Second,cur- of50candidateobjectclasses.
rentbenchmarksconcentrateonthepresenceofan LanguageInstructionPrompts. Forafaircom-
objectclassordistinguishinstancesusingtextual parisonthataccommodatesbothopen-weightand
descriptions like attributes, which can still result API-basedLVLMs,ROPEexplicitlyinstructsmod-
in ambiguity and multiple referents. We instead elstogenerateaformattedoutputofobjectclasses,
leveragethevisualreferringpromptingsetupsand e.g., obj1:<class1>, ..., obj5:<class5>.
use visual cues (i.e., marked bounding boxes) to Thisformatenablesautomatedevaluationthrough
uniquely refer to objects. Finally, our evaluation simpleparsing,avoidingblack-boxneuralmodels
isautomated,withoutblack-boxneuralmodelsor orhumanevaluatorsWithdifferentanalyticalpur-
humanevaluators. poses, we designed 3 types of task prompts for
Multi-Objectqueries,asillustratedinFigure2and
3 Recognition-basedObjectProbing
describedasfollows.
Evaluation(ROPE)
‚Ä¢ Default: Weprobethemodeltorecognizethe5
WeintroducetheRecognitionObjectProbingEval-
objectsreferredtobythevisualpromptsconcur-
uation(ROPE),anautomatedprotocolforassess-
rentlyinasingleturnofprompting. Thissetting
ingLVLMsinmulti-objectrecognition.
tasksthemodelwithfocusingonandrecognizing
3.1 TaskSetup all 5 objects simultaneously, aiming to capture
Problem Definition. To avoid ambiguity from thecomplexityinvolvedwhenthemodelgener-
multiple candidate referents when using text ateslanguagethatincludesmultipleobjects.
prompts, ROPE leverages visual prompts to ‚Ä¢ Student-Forcing: Onepotentialconfounderinthe
uniquelyrefertoobjects. ROPEtasksLVLMswith defaultsettingisthemodel‚Äôsabilitytogenerate
selecting the best matching class for multiple ob- datainthespecifiedformat. Toseparateouter-
jects, as referred to by the visual prompt, from a rorsduetofollowinginstructions,weforcethe
predefinedsetofobjectclasses. Specifically,each modeltofollowtheformattemplateanddecode
sampleintheROPEprotocolconsistsofaquadru- onlytheobjecttokensforeachofthefiveobjects.
ple {I,L,‚ü®p ,¬∑¬∑¬∑ ,p ‚ü©,‚ü®o ,¬∑¬∑¬∑ ,o ‚ü©}: (1) an im- Ideally, this setting allows the model to focus
1 n 1 n
age I consisting of at least n objects; (2) a natu- solelyonobjectrecognition.
rallanguageinstructionLthatspecifiestherecog- ‚Ä¢ Teacher-Forcing: Thissettingeliminatescumu-
nition task, including N candidate object classes lativeerror,allowingthemodeltoconditionon
c ,¬∑¬∑¬∑ ,c ;(3)nvisualpromptsp ,¬∑¬∑¬∑ ,p ,each thecorrectpreviouscontextwhengeneratingob-
1 N 1 n
queries an object in the image; and (4) n object jectclasses,leadingtoupperboundperformance
classeso ,¬∑¬∑¬∑ ,o astheanswers. Inthiswork,we inmulti-objectrecognition. Wesimilarlyforce
1 n
construct a dataset with N = 50 and n = 5, i.e., the model to follow the provided template and
4Select one and the most appropriate class for each object located within red bounding boxes from the following list:
apple, orange, banana, lemon, pear, plate, bowl, jar, bottle, soda can, knife, fork, whisk, scissors, packaged snack.
üë§ Provide the class names in the format: 'obj1: <class1>, obj2: <class2>, obj3: <class3>, obj4: <class4>, obj5: <class5>', with
User no additional words or punctuations.
GPT-4V Gemini 1.0 Pro Qwen-VL-Chat üåãLLaVA-7B
obj1: apple obj1: apple obj1: apple obj1: apple
obj2: knife obj2: orange obj2: lemon obj2: orange
obj3: fork obj3: banana obj3: bottle obj3: banana
obj4: apple obj4: lemon obj4: packaged snack obj4: lemon
obj5: jar obj5: pear obj5: jar obj5: pear
Ground Truth GPT-4O Gemini 1.5 Pro Qwen-VL-Max üåãLLaVA-34B
obj1: fork obj1: packaged snack obj1: fork obj1: packaged snack obj1: apple
obj2: knife obj2: knife obj2: knife obj2: knife obj2: apple
obj3: whisk obj3: whisk obj3: whisk obj3: soda can obj3: apple
obj4: lemon obj4: lemon obj4: lemon obj4: lemon obj4: lemon
obj5: jar obj5: jar obj5: jar obj5: jar obj5: pear
Figure3:AheterogeneousROPEsampletestedwithDefaultmulti-objectquery,whereeachofthe5objectsbelongs
todifferentobjectclasses. Welabeltheoutputclassaseithercorrectorhallucinated.
üåãLLaVA-7B üåãLLaVA-7B
obj1: apple obj1: apple
obj2: apple obj2: apple
obj3: apple obj3: apple
obj4: apple obj4: apple
obj5: apple obj5: apple
Ground Truth üåãLLaVA-34B Ground Truth üåãLLaVA-34B
obj1: apple obj1: apple obj1: apple obj1: apple
obj2: apple obj2: apple obj2: apple obj2: apple
obj3: apple obj3: apple obj3: apple obj3: apple
obj4: apple obj4: apple obj4: apple obj4: apple
obj5: apple obj5: apple obj5: orange obj5: apple
Figure4: AhomogeneousROPEsample,wherethe5objectsbelongtothesameobjectclass,andacorresponding
adversarialROPEsample,wherethelastobjectbelongstoadifferentobjectclass.
decodeonlytheobjecttokensforeachofthefive Panoptic(Linetal.,2014;Caesaretal.,2018)and
objects,butwereplacethepreviouslygenerated ADE20K (Zhou et al., 2017), to ensure access to
objecttokenswiththegroundtruth. Thisessen- allobjectinstancesandtheirsemanticclasses. We
tially follows the few-shot in-context learning notethatonecanbuildadatasetusingtheROPE
setting. Teacher forcing benefits models espe- protocolwithanydatasetcontainingmultipleob-
ciallywhentheytakeshortcutsbyrepeatingthe jects and their bounding boxes, such as Visual
object class list as ordered in the prompt, e.g., Genome (Krishna et al., 2017). We describe the
LLaVA-7B (Liu et al., 2023b) and Gemini 1.0 datacurationpipelineinAppendixA.1.
Pro(Teametal.,2023)inFigure3. SplitsbyQueryDistributions. AsshowninFig-
Forcomparison,wealsodesignedtaskpromptsfor ure 3and 4, our initial observations indicate that
Single-Object query. We probe the model to rec- LVLMsarelesslikelytohallucinateobjectswhen
ognizetheobjectreferredtobythevisualprompts theyrecognizethesameobjectclassmultipletimes.
oneatatime,repeatingthisas5independentand However,theytendtomakemoremistakeswhen
individual prompts. Unlike Default multi-object all tasked object classes are different or when a
query,themodelonlyneedstofocusononeobject, new object class is introduced after multiple re-
whichcanbeseenasanextensionofthePOPE(Li peatedtasks. Wethusvarythedistributionofob-
etal.,2023a)setupfromyes/nopollingtoclassifi- jectclasseswithineachimageattesttime,dividing
cation. WerefertoAppendixA.1fortheprompt ROPE into 4 subsets: Homogeneous, Heteroge-
templatesforeachtypeoftaskprompt. neous,andAdversarial,In-the-Wild.
3.2 DatasetConstruction ‚Ä¢ Homogeneous: Allthe5testedobjectsareofthe
sameclass,e.g.,AAAAA.
DataSourcesandCuration. Sinceourgoalis
toevaluateandanalyzemulti-objecthallucination, ‚Ä¢ Heterogeneous: All the 5 tested objects are of
the image data must contain multiple objects of differentclasses,e.g.,ABCDE.
diverseclasseswithinstance-levelsemanticanno- ‚Ä¢ In-the-Wild: Asubsetwithmixedobjectclassdis-
tations. Webuildourdatasetuponexistingpanop- tribution,wherethe5testedobjectsarerandomly
tic segmentation datasets, including MSCOCO- chosenandorderedgivenatestimage.
5DefaultMulti-Object Student-Forcing Teacher-Forcing Single-Object
Models Wild Hom. Het. Wild Hom. Het. Wild Hom. Het. Wild Hom. Het.
Seen
Yi-VL-6B 2.95 5.65 1.99 3.44 6.80 3.78 5.45 26.25 4.36 0.19 0.30 0.13
Yi-VL-34B 8.50 15.35 3.33 8.97 16.30 4.23 10.09 19.75 4.94 0.22 2.60 0.13
LLaVA-7B 31.29 67.50 8.00 31.28 67.25 11.22 31.49 92.15 12.37 35.32 62.35 17.37
LLaVA-13B 31.54 67.63 12.64 31.49 73.25 11.54 34.97 94.25 16.03 43.13 80.60 23.91
LLaVA-34B 39.95 85.75 18.85 52.75 85.20 33.91 56.41 95.81 25.31 55.05 86.50 18.97
QwenVL 2.73 6.60 1.03 6.25 16.00 3.65 18.74 71.50 5.45 8.73 16.05 5.58
QwenVL-C 8.72 16.90 6.67 5.26 8.60 4.10 12.11 47.75 8.08 25.99 43.40 13.21
CogVLM 0.04 0.00 0.00 0.00 0.00 0.00 0.10 0.95 0.00 0.00 0.00 0.00
CogVLM-G 0.00 0.00 0.00 9.86 13.50 6.79 22.64 75.45 0.45 11.25 22.65 7.12
CogVLM-C 12.89 22.75 7.18 25.37 43.63 12.03 28.25 72.80 17.50 30.16 56.00 16.35
LLaVA-7B* N/A 9.16 16.40 5.51 N/A 11.68 23.55 9.36
GLaMM* N/A 27.11 53.35 13.01 N/A 63.81 81.75 53.40
GroundHOG* N/A 23.57 30.80 24.23 N/A 44.80 43.10 38.97
IDEFICS 0.00 1.45 0.13 6.25 18.70 0.64 17.37 76.15 10.06 4.62 0.00 0.32
CogVLM-2 21.51 37.55 17.31 37.02 70.85 12.69 37.10 73.50 17.44 21.16 38.75 13.65
MiniCPM-V 34.75 59.91 17.37 31.62 62.80 13.65 32.16 68.05 16.79 27.42 55.35 16.92
GPT4V** 53.80 77.55 40.83 N/A N/A 55.89 78.25 41.03
GPT4O** 71.27 89.25 66.03 N/A N/A 60.77 73.92 54.31
Unseen
Yi-VL-6B 2.74 3.88 1.14 3.18 4.24 5.20 4.04 10.90 10.57 0.14 0.45 0.08
Yi-VL-34B 7.77 15.63 4.23 10.28 18.04 7.97 11.24 22.49 12.03 0.46 2.37 0.41
LLaVA-7B 30.56 68.12 10.33 30.55 68.16 10.24 31.89 90.33 13.25 34.88 64.41 16.18
LLaVA-13B 27.56 63.10 8.37 27.41 63.10 8.37 35.65 91.09 14.80 42.66 71.92 23.41
LLaVA-34B 29.30 79.43 17.72 29.45 91.18 14.39 37.40 95.51 17.92 51.71 77.88 30.81
QwenVL 2.80 1.95 7.06 7.17 16.41 4.15 10.34 58.00 4.07 17.73 31.22 9.51
QwenVL-C 18.86 30.73 8.78 16.16 27.80 7.72 21.81 58.00 11.14 34.20 57.31 15.37
CogVLM 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.15 0.00 0.00 0.00 0.00
CogVLM-G 0.00 0.00 0.00 8.20 1.47 5.77 23.82 81.20 1.81 10.32 10.74 9.11
CogVLM-C 15.56 26.57 5.53 17.18 41.27 6.02 22.81 56.04 6.67 30.56 52.00 13.50
LLaVA-7B* N/A 7.59 12.12 4.88 N/A 12.71 22.49 8.46
GLaMM* N/A 29.11 54.53 14.23 N/A 68.65 77.06 52.28
GroundHOG* N/A 23.11 24.69 26.26 N/A 40.73 30.37 38.13
IDEFICS 0.39 0.37 0.33 9.03 24.45 2.68 24.80 83.02 7.64 4.62 3.67 6.50
CogVLM-2 20.99 35.06 15.93 24.64 38.04 23.17 26.74 46.04 26.59 11.13 30.94 5.77
MiniCPM-V 32.96 59.92 16.60 31.77 58.98 14.15 31.87 60.98 16.34 25.56 47.76 14.39
GPT4V** 45.46 63.12 34.17 N/A N/A 47.34 64.94 35.45
GPT4O** 63.27 80.29 54.47 N/A N/A 63.45 79.84 53.74
* MechanisticallygroundedLVLMstakevisualpromptsbydedicatedpointertokens.Weslightlyadaptthetextpromptand
probetheobjectclasseswiththehighestprobabilities.WealsoapplysuchprobabilisticprobingtoLLaVA-7Bforcomparison,
asallofthreemodelsadoptVicuna-7Bv1.5(Chiangetal.,2023)asthebaseLLM.SeeAppendixA.1fordetails.
**ForGPTmodels,student/teacherforcingdoesn‚ÄôtapplyastheyareAPI-only.
Table2: AveragedaccuracyofbaselinesontheIn-the-Wild,Homogeneous,andHeterogeneoussplits. Thebold
markerdenotesthebest-performingbaselineandtheunderlinedmarkerdenotesthesecond-best-performingbaseline.
AttendingtoDataContamination. Whiledata imagewasseenduringinstructiontuning(2splits),
contaminationhasbeenexplicitlyhandledinmost wedividethetestinto8folders.
of the existing benchmarks, object hallucination
4 ExperimentsandResults
has been observed even in images that appear
in the instruction tuning dataset, such as Visual 4.1 LVLMBaselines
Genome(Zhaietal.,2023;Liuetal.,2023a). To
TheproposedROPEframework, inprinciple, ap-
evaluate whether multi-object hallucination can
pliestoallLVLMsthatcanfollowformatinstruc-
beobservedinbothseenandunseenimages,and
tionsandunderstandmultiplevisualprompts. To
tocriticallydetermineiftrainingontheseimages
cover a variety of LVLMs of different scales and
helpsreducehallucinations,weexplicitlysplitour
training data (e.g., whether grounding data and
dataset into Seen and Unseen based on the origi-
conversationaldataareused),weselectedthefol-
nalsplitofthedatasets.1 Dependingontheobject
lowingLVLMsasbaselines.
querydistributions(4splits)andwhetherthetested
‚Ä¢ LVLMs with base LLMs at different scales:
1Webelievethisapproachisthebestpractice,butwealso
LLaVA v1.6 (7B/13B/34B) (Liu et al., 2023b,
acknowledge that the distinction between seen and unseen
imagesmaynotbestrict.Uncuratedwebimagesoftenoverlap 2024)andYi-VL(6B/34B)(Youngetal.,2024).
withpublictestimages,andresearchershavenotransparent
‚Ä¢ LVLMs with conversational/grounded instruc-
access to the datasets used to train some of these LVLMs
unfortunately(Dodgeetal.,2021). tiontuning: QwenVL-Base/Chat(7B)(Baietal.,
6100
99.7 100 9. 60 .7 97.3 85.6 100.0 100 9. 90 .7 100.0 68.2 94.8 95.3 92.3 79.3 78.8 AAAAB¬≠SO
50.6
50 55.4 52.4 52.1 54.0 51.9 B AA AA AA AA B¬≠ ¬≠S TO F
19.2 29.0 0.0 13.8 26.1 0.0 7.7 36.5 5.2 BAAAA¬≠TF
0
obj 1 obj 2 obj 3 obj 4 obj 5 obj 1 obj 2 obj 3 obj 4 obj 5 obj 1 obj 2 obj 3 obj 4 obj 5
Object Index Object Index Object Index
(a)LLaVA-7B. (b)LLaVA-13B. (c)LLaVA-34B.
Figure5: TheperformanceoftheLLaVAontheadversarialsplit,organizedbythequerysequenceofAAAABand
BAAAA,revealssignificantvulnerabilitiesasthemodel‚Äôsaccuracydramaticallydeclinesforobject5inAAAAB.SO
standsforsingle-objectprobingandTFstandsforteacher-forcingprobing.
2023)andCogVLM-Base/Chat/Groundingv1.1 methods,moreheterogeneousqueriesleadtosub-
(19B)(Wangetal.,2023c). stantially more hallucinations, with performance
‚Ä¢ Mechanistically grounded LVLMs: GLaMM decreasingfromhomogeneoustoin-the-wildtohet-
(7B) (Rasheed et al., 2024) and GroundHOG erogeneoustestsets. Theimpactofheterogeneity
(7B)(Zhangetal.,2024b). applies to even start-of-the-art LVLMs like GPT-
‚Ä¢ Other LVLMs: IDEFICS-instruct (9B) (Lau- 4O (Figure 3), although this performance gap is
ren√ßonetal.,2023b),MiniCPM-Vv2.5(8B)(Hu moresignificantinopen-weightmodels.
etal.,2024;Yuetal.,2024b),GPT-4V(OpenAI, Languagebiasandshortcutscanleadtomulti-
2023),andGPT-4O(OpenAI,2024). objecthallucinations. Intheteacher-forcingset-
For mechanistically grounded LVLMs that ting,wheretherearenocumulativeerrors,LLaVA
take visual prompts through specially designed modelsscoreover90%accuracy. Therearethree
mechanisms, such as pointer tokens in Ground- possiblehypothesesforthisabnormalobservation:
HOG(Zhangetal.,2024b),weadditionallyexperi- (1)LVLMsaresmartenoughtolearnobjectrecog-
mentwiththeirdefaultformatandreportwhichever nitioningeneralthroughfew-shotin-contextlearn-
yieldshigherperformance. ForotherLVLMs,we ingintheteacher-forcingsetting;(2)LVLMslearn
overlaythevisualpromptsontheimagesusinga torecognizeonespecificobjectthroughfew-shot
redboundingboxwithawidthof2andvisualtext in-context learning intheteacher-forcingsetting;
specifyingtheobjectindex,presentedwithawhite or(3)LVLMssimplyexploitlanguagebiasesand
italic font on a black background with an alpha rule-based shortcuts (e.g., repeating previous an-
valueof0.75forcontrastandvisibility. swers). To reach a conclusion on this, we exam-
ine an Adversarial split, in which the first four
4.2 MainResultsandFindings
testedobjectsareofthesameclassandweprobe
Wesummarizetheaverageresultsacrossthesplits anobjectofadifferentclassforthelastone(e.g.,
inTable2andpresentthemostimportantfindings AAAAB).Wecomparethesingle-objectqueryper-
below. ThefulltablesappearinAppendixA.2. formancewiththeteacher-forcingperformanceon
Multi-object tasks introduce more hallucina- the fifth object (object B). We anticipate the fol-
tions. OurimmediateobservationisthatLVLMs lowingoutcomes: Ifhypothesis(1)iscorrect,the
sufferfrommorehallucinationswhentaskedwith teacher-forcingperformanceshouldoutperformthe
focusingonandrecognizingmultipleobjectscom- single-objectquery. Ifhypothesis(2)iscorrect,the
paredtoasingleobject. Acrossmostofthemodels teacher-forcingperformanceshouldperformonpar
andtestsplits,wefindthattheaverageaccuracyof with the single-object query. If hypothesis (3) is
single-object queries (i.e., probing object classes correct,theteacher-forcingperformanceshouldun-
oneatatime)significantlyoutperformsthatofall derperform compared to the single-object query.
threetypesofmulti-objectqueries. Thefirstexcep- For a controlled comparison in the multi-object
tionsareGPT-4O,MiniCPM-V,andCogVLM-2, setting, we also reverse the order of queries (i.e.,
thelattertwoleverageLLaMA-3(Meta,2024). An- BAAAA)andrepeattheexperiments.
otherexceptiontothisiswhenteacher-forcingis WepresenttheresultsofLLaVAmodelsonthe
appliedtohomogeneoustestsplits,whichdemon- unseensplitinFigure5,withthefullresultsavail-
stratesanunreasonablyhighaccuracy. Wediscuss able in Appendix A.2. We find that the model‚Äôs
themlaterinthissection. predictionsonclassAprogressivelyimprove,scor-
Heterogeneousqueriesintroducemorehalluci- ing nearly perfectly starting from the third repe-
nations. We find that for all models and query tition. However, the model‚Äôs performance on the
7
ycaruccAlastobject(withthedifferentclasslabelB)drops etal.,2023)andYi(34B)(Youngetal.,2024)with
to nearly zero, with almost all hallucinations la- 2Tand3Tpre-trainingtokens,thesemodelsunder-
belingitasA.Thisisinstarkcontrastto23.35% performinquantitativemeasuresduetoinstruction
iftheseobjectsareprobedindividuallyor19.16% followingerrorbutexhibitgreaterrobustnesswhen
whentheseobjectsareplacedasthefirsttoquery multiplevisualpromptsarepresented.
inmulti-objectsettings. Ourfindingssuggestthat Visualinstructionfine-tuning: chatandground-
hypothesis(3)istrue,indicatingthattheLVLMs‚Äô ing. Whileit‚Äôssurprisingthatconversationaltun-
highperformanceonhomogeneousqueriescould ingreducesmulti-objecthallucinations,weobserve
beanillusionresultingfromtextualshortcuts. We thatmodelswithoutconversationaltuningstruggle
observe that models show lower performance in to follow instructions and are prone to shortcuts,
identifyingobjectclassBinsingle-objectanalysis, such as repeating the list of all object class can-
potentiallyduetothehighersalienceofobjectclass didatesinorderorconsistentlyrepeatingthefirst
Aintheseimages. candidate. Thismightalsoexplainwhygrounded
Multi-objecthallucinationsoccurinbothseen tuninginCogVLM-Gisoflittlehelpinreducing
and unseen images. We finally investigate multi-objecthallucinationsthusfar. Thesemodels
whether our observations and findings hold uni- typicallylackconversationalfine-tuning,andthere
formly in both seen and unseen splits. We ob- is currently no available grounded dialogue data
serve that the gap between multi-object halluci- atscale. WhilemechanisticallygroundedLVLMs
nation and single-object hallucination, as well as show strong performances in single-object prob-
therelianceonshortcuts,persists. Althoughmost ing, there remain a gap in multi-object probing
of the models perform slightly better on seen im- with student forcing. This could be attributed to
ages,thetrendsremainconsistentacrossbothsplits. a significant portion of the grounded instruction
Whilelarge-scaletrainingisinvolvedindeveloping tuningdatasetconsistingmainlyofshortcaptions
theseLVLMs,itappearstheymightnothavefully orquestionsfeaturingonesingleorfewobjects.
exploitedthefine-grainedinformationinthedata. 5 AnalysisofHallucinatoryBehaviors
Trainingontheseimagesdoesnotsignificantlyre-
5.1 PotentialHallucinatoryFactors
duceobjecthallucinations.
Thetasksetupdescribedaboveallowsustoevalu-
4.3 WhatMayHelpandWhatMayNot? ateLVLMsinmulti-objecthallucinationsandiden-
tify hallucinatory behaviors. Based on existing
ComparingthetestedLVLMs,wediscussourob-
literatureandourcasestudies,wefurtheridentify
servationsregardingdesignconsiderationsthatmay
potential factors that correlate to and potentially
ormaynothelpreducemulti-objecthallucinations.
explainthesehallucinations.
Scaling the base LLM: data and parameters.
Data-specificFactors. Weconsiderthefollow-
WefindthatusingbaseLLMswithmoreparame-
ing factors that are specific to the tested sample
tersreducessingle-objecthallucinations,butmay
(e.g.,objectandtokenpositions),andarenotrele-
nothavethesameeffectonmulti-objecthallucina-
vanttothefrequencydistribution.
tions. Weobserveaconsistentincreaseinperfor-
‚Ä¢ InputOrder: weconsidertheorderinwhichthe
mance with larger LLaVA models in the seen set
objectclassesarepresentedintheinputprompt
andinsingle-objectqueries,butnotintheunseen
containingallcandidates.
setwithmulti-objectqueries. Onepossibleexplana-
‚Ä¢ QueryHomogeneity: Wedefinequeryhomogene-
tionforthisfindingisthatLLMswithmoreparam-
ityasthetotalnumberoftaskobjectsofthesame
etersarebetteratmemorizingseenimages,asthe
class,normalizedbythetotalnumberofqueried
performancegapbetweenseenandunseenimages
objects(fiveinthiswork).
isalsomoresignificantinlargermodels. Wealso
notice that the performance gap between single- ‚Ä¢ Object Token Position: Zhou et al. (2024) has
objectprobingandmulti-objectprobingdoesnot shownthatmorehallucinationsoccurinthelatter
applytoMiniCPM-VandCogVLM-2,whichadopt partofcaptions. Inthiswork,theobjectindices
a LLaMA-3 (8B) (Meta, 2024) base LLM pre- directlycorrespondtotheobjecttokenpositions.
trainedwith15Ttokens,astheyfailtofollowthe ‚Ä¢ Object Homogeneity: We define object homo-
instructionsometimes. ComparedtoLLaVAmod- geneityasthenumberofobjecttypesintheim-
els developed upon LLaMA-2 (7/13B) (Touvron age,calculateduponpanopticannotations.
8Non Hallucinate
Hallucinate
Hallucinate Mean
Non Hallucinate Mean
0.20 0.40 0.60 0.80 1.00 1 2 3 4 5 12.9 16.2 64
(a)QueryHomogeneity. (b)Objecttokenposition. (c)ObjectHomogeneity.
0 0.52 0.56 1 0 0.08 1 0 0.13 0.21 1
(d)Objectcentrality. (e)Objectsalience. (f)Semanticsalience.
0 52832 168583 250000 0 0.6 1 3 0.06 0.11 0.12 0.18
(g)Trainingsalience. (h)Objecttokenentropy. (i)Visualmodalitycontribution.
Figure6: Acomparisonofthedistributionofhallucinatoryversusnon-hallucinatoryobjectclassesinLLaVA-13B,
acrosstheunseensplitunderstudentforcing.
‚Ä¢ ObjectCentrality: Previousresearchhasidenti- training salience following previous work, and
fiedacenterbiasindatasetsandmodels,indicat- hypothesizethatLVLMstendtohallucinatemore
ingthatobjectsaredisproportionatelylocatedat onlessfrequentobjectsinthetrainingset.
thecenterofimagesindetectionmodels(Szab√≥
andHorv√°th,2022;Taesirietal.,2023). Wede- ModelBehaviors. Weconsiderthefollowingfac-
fine object centrality as the distance d between torsrelevanttothemechanisticbehaviors.
theobject‚Äôsboundingboxcenterandtheimage ‚Ä¢ Object Token Entropy: Zhou et al. (2024) have
center, normalized by the diagonal distance D shownthatobjecthallucinationsaremorelikely
fromthecentertothecorner. when the decoded object tokens have a higher
log perplexity. In our work, we define object
Salience and Frequency. We consider the fol-
tokenentropyastheentropyofthelogitsofthe
lowing factors that are related to the saliency or
firsttokeninthegeneratedword. Givensasthe
frequencyofthevisualobjectortheobjectclass.
softmax logits of the generated word‚Äôs first to-
‚Ä¢ Object Salience: Previous research has shown ken,wecalculatetheentropyusingthefollowing
that smaller objects are harder to detect and formula: H(s) = ‚àí(cid:80) s log(s ). Simply put,
i i i
groundto(Hoiemetal.,2012;Maetal.,2023). higher entropy indicates greater uncertainty in
Wedefineobjectsalienceastheratioofthenum- themodel‚Äôspredictionforthefirsttoken,which
ber of pixels occupied by the object‚Äôs instance canleadtomorefrequentobjecthallucinations.
segmentationmasktothetotalnumberofpixels
‚Ä¢ Visual Modality Contribution: We hypothesize
intheimage.
that LVLMs pay less attention to the visual
‚Ä¢ SemanticSalience: Weobserveandhypothesize modalityduringobjecthallucinations. Motivated
thatLVLMsarelesslikelytohallucinateobjects by the modality importance score (Cao et al.,
whentheyco-occurwithmultiplecopiesofthe 2020),wedefineVisualModalityContribution
sameclass(‚Äújar‚ÄùinFigure3). Wedefineseman- (VMC) as the proportion of attention allocated
tic salience as the ratio of the total number of tovisualtokenscomparedtotextualtokens. To
pixels in all instances of the same class, to the quantifythis,weanalyzetheattentionweightsof
totalnumberofpixelsintheimage. thelastgeneratedtokenacrossallheadsandlay-
‚Ä¢ TrainingSalience: Previousresearchhasshown ers. TheVMCiscomputedasfollows: VMC =
(cid:80) (cid:0)(cid:80) (cid:80) (cid:1)
thatspuriousco-occurringpatternsinthetraining / Œ± + Œ± ,whereŒ±
i‚ààVŒ±ij i‚ààV ij k‚ààT kj ij
data can lead to object hallucinations (Li et al., representstheattentionweightassignedtovisual
2023a; Zhou et al., 2024). We use the log fre- token i at head j, and Œ± represents the atten-
kj
quency of classes in MSCOCO as a proxy for tionweightassignedtotextualtokenk atheadj.
9
tnuoC
ycneuqerF
ycneuqerF
tnuoC
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFActual Class
Predicted Class
Mean Actual Class
Mean Predicted Class
0.2 0.4 0.6 0.8 1.0 0 50849 106916 250000 0 10 20 30 40 50
(a)Semanticsalience. (b)Trainingsalience. (c)Inputorder.
Figure7: Acomparisonofthedistributionofactualversuspredictedobjectclassesforallhallucinatoryobjectsin
thestudentforcingsettingontheunseensplitusingLLaVA-13B.
The sets V and T denote the visual and textual IntrinsicBehaviors. Theintrinsicbehaviorsof
tokens,respectively. ByexaminingtheVMC,we themodelprovidesignificantinsightsintoitsten-
can determine how much attention is given to denciestohallucinate. SimilartoZhouetal.(2024),
visualinputsincomparisontotextualinputs. A wefindthatmodelsaremorepronetohallucination
lowerVMCmayindicateahigherlikelihoodof whentheyexperienceuncertaintyorconfusion,es-
objecthallucinationsduetoinsufficientattention peciallyinscenariosinvolvingmultipleobjects,as
tovisualcues. evidenced by higher token entropy. Furthermore,
the contribution from the visual modality consis-
5.2 WhenDoLVLMsExperience tentlyregistersbelow20%,suggestingthatcurrent
Multi-ObjectHallucinations? LVLMs may rely more heavily on linguistic con-
In Figure 6, we compare the distribution of texts. Thereisamarginalincreaseinthelikelihood
these factors between hallucinatory and non- ofhallucinationwhenmodelspaylessattentionto
hallucinatoryobjectsinthestudentforcingsetting thevisualcontext.
ontheunseensplitusingLLaVA-13B.Forcontinu- 5.3 HowDoLVLMsExperienceMulti-Object
ousvalues,weuseridgelineplots,andfordiscrete Hallucinations?
valueswithfewerbins,weusebarcharts.
InFigure7, weconductedadetailedcomparison
Data/Task-specific Factors. We observed that ofthedistributionofactualversuspredictedobject
specific data factors, such as query and object classeswithinthecontextofhallucinatoryobjects,
homogeneity, significantly influence model per- examiningfactorssuchassemanticsalience,train-
formance,withincreasedhallucinationoccurring ingsalience, andinputorder. Although semantic
whenmodelsprocessimagesfeaturingmultipleob- salience is a key factor in determining whether a
jectclassesoravarietyofobjects. Forpositional modelhallucinates,itappearstohaveminimalim-
factors,thepositionofobjecttokensseemstohave pactonthepredictionofhallucinatedobjects. Our
minimalimpactandtheobjectcentralityhasonly analysis also shows that models are more likely
a slight influence as LVLMs tend to hallucinate to hallucinate object classes that are prevalent in
objectsmorefrequentlywhentheyarepositioned thetrainingdata,butthereverseisnotnecessarily
away from the center. This tendency may stem true. Additionally,thereisanotablepreferencefor
fromareportingbias,asobjectsmentionedincap- modelstohallucinateobjectsthatarelistedearly
tionsaretypicallyforegroundobjectsthatdistribute intheinputpromptascandidateclasses. Overall,
towardthecentersofimages. ourfindingsindicatethatspuriouscorrelationsmay
SalienceandFrequency. Wenotethatsemantic leadtohallucinationsinvolvingmultipleobjects.
salience significantly affects the model‚Äôs perfor-
6 DiscussionsandConclusion
mance,asitismorepronetohallucinateanobject
class that is less salient within the image. Con- Hallucinations in large vision-language models
versely,thesalienceofindividualobjectsdoesnot (LVLMs) can occur at different scales and gran-
statisticallycorrelatewithhallucinationincidents. ularities. In this study, we study the problem of
This implies that LVLMs may rely more on the multi-objecthallucination,examininghowLVLMs
presenceofco-occurringobjectsofthesameclass maymisperceivewhentaskedtofocusonmultiple
topredictthelabelsofqueriedobjects,ratherthan objectsconcurrently,andwhichfactorscausethe
solely on the presence or salience of the objects hallucinations. we introduce Recognition-based
themselves. Additionally,trainingsalienceplaysa ObjectProbingEvaluation(ROPE),anautomated
crucialroleasmodelsarelesslikelytohallucinate evaluationprotocoldesignedtoaccountforthedis-
objectclassesthatfrequentlyappearintraining. tribution of object classes within a single image
10
ycneuqerF ycneuqerF ycneuqerFduringtestingandtousevisualreferringprompts References
to reduce ambiguity. Our research provides key
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
insights for the development and application of
Antoine Miech, Iain Barr, Yana Hasson, Karel
LVLMs. Sincemodelstendtoexperiencemorehal- Lenc,ArthurMensch,KatherineMillican,Malcolm
lucinationswithmultipleobjectsthanwithsingle Reynolds,etal.2022. Flamingo: avisuallanguage
modelforfew-shotlearning. InAdvancesinNeural
ones,itmaybeadvantageoustoprobeobjectsindi-
InformationProcessingSystems,volume35,pages
viduallyinvisualpromptstoenhanceperformance.
23716‚Äì23736.
Thelikelihoodofamodel‚Äôshallucinatoryoutputis
linkedtovariousdatafactorsandmodelbehaviors. HyojinBahng,AliJahanian,SwamiSankaranarayanan,
and Phillip Isola. 2022. Exploring visual prompts
Particularlyinsituationsinvolvingheterogeneous
for adapting large-scale models. arXiv preprint
dataandlowcertaintyfromthemodel,thereisan
arXiv:2203.17274.
increasedriskofhallucinations,andusersshould
bevigilant. Moreover,ouranalysisindicatesthat Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
SinanTan, PengWang, JunyangLin, ChangZhou,
merelyadopting(grounded)instructiontuningand
andJingrenZhou.2023. Qwen-vl: Afrontierlarge
scalingthebaselanguagemodelmaynotbeenough
vision-languagemodelwithversatileabilities. arXiv
to fully address the issue of object hallucination. preprintarXiv:2308.12966.
There is a need for more balanced object distri-
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
butions, annotations of objects away from image
2018. Coco-stuff: Thingandstuffclassesincontext.
centers, and an increase in diversity. Introducing InProceedingsoftheIEEEconferenceoncomputer
instructions that require multiple visual pointers visionandpatternrecognition,pages1209‚Äì1218.
andcomplexmulti-objectreasoningisalsocrucial.
MuCai,HaotianLiu,SivaKarthikMustikovela,Gre-
Acknowledgement Thisworkwassupportedin gory P Meyer, Yuning Chai, Dennis Park, and
partbyNSFIIS-1949634,NSFSES-2128623,and YongJaeLee.2024. Vip-llava: Makinglargemulti-
modalmodelsunderstandarbitraryvisualprompts.
the DARPA Machine Common Sense Program.
InIEEEConferenceonComputerVisionandPattern
OurexperimentshavealsobenefitedfromtheMi-
Recognition.
crosoft Accelerate Foundation Models Research
(AFMR)grantprogramandthemodelaccessfrom JizeCao,ZheGan,YuCheng,LichengYu,Yen-Chun
Chen,andJingjingLiu.2020. Behindthescene: Re-
theAmazonAGIteam. Theauthorswouldliketo
vealingthesecretsofpre-trainedvision-and-language
thankYichiZhangforhisvaluablefeedback.
models. InComputerVision‚ÄìECCV2020:16thEuro-
peanConference,Glasgow,UK,August23‚Äì28,2020,
Limitations ROPErepresentsoneofthepioneer-
Proceedings,PartVI16,pages565‚Äì580.Springer.
ingeffortstopubliclyaddresstheissueofmultiple
object hallucination. However, we acknowledge KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,
several limitations in our work: (1) The lack of FengZhu,andRuiZhao.2023. Shikra: Unleashing
multimodalllm‚Äôsreferentialdialoguemagic. arXiv
transparency in the LVLMs makes it difficult to
preprintarXiv:2306.15195.
guaranteethatourunseendatasethasnotbeenpre-
viously exposed. (2) Our evaluation benchmark Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
usesafixedsetofsemanticobjects,whichmayin- ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal.
troducebiasandimposeunnecessaryconstraintson
2023. Vicuna: Anopen-sourcechatbotimpressing
theLVLMs‚Äôabilitytofollowinstructionsandrea-
gpt-4with90%*chatgptquality.
soneffectively. (3)Theevaluationprocesscanbe
slow,asitinvolvesperformingfiveinferencesper Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
imageforbothstudentforcingandteacherforcing.
Boyang Li, Pascale N Fung, and Steven Hoi.
Ethicalconcernsandrisks Thisstudydoesnot 2023a. Instructblip: Towardsgeneral-purposevision-
require human annotators or participants for its language models with instruction tuning. In Ad-
vances in Neural Information Processing Systems,
interactiveexperiments. Instead,itutilizespublicly
volume36.
availabledatasetsandcontentcreatedbymodelsfor
evaluationpurposes. Weareawarethatthesepublic WenliangDai,ZihanLiu,ZiweiJi,DanSu,andPascale
datamightintroducebiasesandsensitiveelements, Fung.2023b. Plausiblemaynotbefaithful: Probing
objecthallucinationinvision-languagepre-training.
and it is essential for future research to address
InProceedingsofthe17thConferenceoftheEuro-
theseconcerns,possiblybycreatingdatasetsthat
peanChapteroftheAssociationforComputational
incorporatefairness-basedfilteringandmetrics. Linguistics,pages2128‚Äì2140.
11Jesse Dodge, Maarten Sap, Ana Marasovic¬¥, William XinLai,ZhuotaoTian,YukangChen,YanweiLi,Yuhui
Agnew,GabrielIlharco,DirkGroeneveld,Margaret Yuan, Shu Liu, and Jiaya Jia. 2023. Lisa: Reason-
Mitchell, and Matt Gardner. 2021. Documenting ing segmentation via large language model. arXiv
largewebtextcorpora: Acasestudyonthecolossal preprintarXiv:2308.00692.
clean crawled corpus. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan- Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon,
guageProcessing,pages1286‚Äì1305. Stas Bekman, Amanpreet Singh, Anton Lozhkov,
Thomas Wang, Siddharth Karamcheti, Alexander
TaoGong,ChengqiLyu,ShilongZhang,YudongWang, Rush,DouweKiela,etal.2023a. Obelics: Anopen
MiaoZheng,QianZhao,KuikunLiu,WenweiZhang, web-scalefiltereddatasetofinterleavedimage-text
PingLuo,andKaiChen.2023. Multimodal-gpt: A documents. InAdvancesinNeuralInformationPro-
visionandlanguagemodelfordialoguewithhumans. cessingSystems,volume36.
arXivpreprintarXiv:2305.04790.
Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon,
Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. De- Stas Bekman, Amanpreet Singh, Anton Lozhkov,
tectingandpreventinghallucinationsinlargevision Thomas Wang, Siddharth Karamcheti, Alexander
languagemodels. InProceedingsoftheAAAICon- Rush,DouweKiela,etal.2023b. Obelics: Anopen
ferenceonArtificialIntelligence,volume38,pages web-scalefiltereddatasetofinterleavedimage-text
18135‚Äì18143. documents. InAdvancesinNeuralInformationPro-
cessingSystems,volume36.
Derek Hoiem, Yodsawalai Chodpathumwan, and
XuanyuLei,ZonghanYang,XinruiChen,PengLi,and
Qieyun Dai. 2012. Diagnosing error in object de-
YangLiu.2024. Scaffoldingcoordinatestopromote
tectors. InEuropeanconferenceoncomputervision,
vision-language coordination in large multi-modal
pages340‚Äì353.Springer.
models. arXivpreprintarXiv:2402.12058.
HongyuHu,JiyuanZhang,MinyiZhao,andZhenbang
LiunianHaroldLi,PengchuanZhang,HaotianZhang,
Sun.2023. Ciem: Contrastiveinstructionevaluation
Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan
method for better instruction tuning. In NeurIPS
Wang,LuYuan,LeiZhang,Jenq-NengHwang,etal.
2023WorkshoponInstructionTuningandInstruction
2022. Grounded language-image pre-training. In
Following.
ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition,pages10965‚Äì
ShengdingHu,YugeTu,XuHan,ChaoqunHe,Ganqu
10975.
Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxi-
ang Huang, Weilin Zhao, et al. 2024. Minicpm:
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Unveiling the potential of small language models
Wayne Xin Zhao, and Ji-Rong Wen. 2023a. Eval-
with scalable training strategies. arXiv preprint
uatingobjecthallucinationinlargevision-language
arXiv:2404.06395.
models. InProceedingsofthe2023Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing.
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,
Conghui He, Jiaqi Wang, Dahua Lin, Weiming
ZongjieLi,ChaozhengWang,ChaoweiLiu,Pingchuan
Zhang,andNenghaiYu.2024. Opera: Alleviating
Ma, Daoyuan Wu, Shuai Wang, and Cuiyun Gao.
hallucinationinmulti-modallargelanguagemodels
2023b. Vrptest: Evaluatingvisualreferringprompt-
viaover-trustpenaltyandretrospection-allocation. In
ing in large multimodal models. arXiv preprint
ProceedingsoftheIEEE/CVFConferenceonCom-
arXiv:2312.04087.
puterVisionandPatternRecognition.
Tsung-YiLin,MichaelMaire,SergeBelongie,James
LiqiangJing,RuosenLi,YunmoChen,MengzhaoJia, Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
and Xinya Du. 2023. Faithscore: Evaluating hal- and C Lawrence Zitnick. 2014. Microsoft coco:
lucinationsinlargevision-languagemodels. arXiv Common objects in context. In Computer Vision‚Äì
preprintarXiv:2311.01477. ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Aishwarya Kamath, Mannat Singh, Yann LeCun, PartV13,pages740‚Äì755.Springer.
Gabriel Synnaeve, Ishan Misra, and Nicolas Car-
ion.2021. Mdetr-modulateddetectionforend-to-end FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,Yaser
multi-modal understanding. In Proceedings of the Yacoob, and Lijuan Wang. 2023a. Mitigating hal-
IEEE/CVF International Conference on Computer lucination in large multi-modal models via robust
Vision,pages1780‚Äì1790. instructiontuning. InTheTwelfthInternationalCon-
ferenceonLearningRepresentations.
RanjayKrishna,YukeZhu,OliverGroth,JustinJohn-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Lee.2024. Improvedbaselineswithvisualinstruc-
2017. Visualgenome: Connectinglanguageandvi- tiontuning. InProceedingsoftheIEEE/CVFCon-
sion using crowdsourced dense image annotations. ferenceonComputerVisionandPatternRecognition,
Internationaljournalofcomputervision,123:32‚Äì73. pages26296‚Äì26306.
12HaotianLiu,ChunyuanLi,QingyangWu,andYongJae AnnaRohrbach,LisaAnneHendricks,KayleeBurns,
Lee.2023b. Visualinstructiontuning. InAdvances TrevorDarrell,andKateSaenko.2018. Objecthallu-
inneuralinformationprocessingsystems,volume36. cinationinimagecaptioning. InProceedingsofthe
2018ConferenceonEmpiricalMethodsinNatural
HolyLovenia,WenliangDai,SamuelCahyawijaya,Zi-
LanguageProcessing,pages4035‚Äì4045.
wei Ji, and Pascale Fung. 2023. Negative object
presenceevaluation(nope)tomeasureobjecthallu- AleksandarShtedritski,ChristianRupprecht,andAn-
cinationinvision-languagemodels. arXivpreprint dreaVedaldi.2023. Whatdoesclipknowaboutared
arXiv:2310.05338. circle? visualpromptengineeringforvlms. InPro-
ceedingsoftheIEEE/CVFInternationalConference
JiasenLu,ChristopherClark,RowanZellers,Roozbeh
onComputerVision,pages11987‚Äì11997.
Mottaghi,andAniruddhaKembhavi.2022. Unified-
io: Aunifiedmodelforvision,language,andmulti- ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,
modaltasks. InTheEleventhInternationalConfer- Chunyuan Li, Yikang Shen, Chuang Gan, Liang-
enceonLearningRepresentations.
YanGui,Yu-XiongWang,YimingYang,etal.2023.
Aligninglargemultimodalmodelswithfactuallyaug-
Ziqiao Ma, Jiayi Pan, and Joyce Chai. 2023. World-
mentedrlhf. arXivpreprintarXiv:2309.14525.
to-words: Grounded open vocabulary acquisition
throughfastmappinginvision-languagemodels. In
Gergely Szab√≥ and Andr√°s Horv√°th. 2022. Mitigat-
Proceedings of the 61st Annual Meeting of the As-
ingthebiasofcenteredobjectsincommondatasets.
sociationforComputationalLinguistics(Volume1:
In 2022 26th International Conference on Pattern
LongPapers),pages524‚Äì544.
Recognition(ICPR),pages4786‚Äì4792.IEEE.
Meta.2024. Introducingmetallama3: Themostcapa-
Mohammad Reza Taesiri, Giang Nguyen, Sarra
bleopenlyavailablellmtodate.
Habchi,Cor-PaulBezemer,andAnhNguyen.2023.
Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Imagenet-hard: Thehardestimagesremainingfrom
Jacky Liang, Ishita Dasgupta, Annie Xie, Danny astudyofthepowerofzoomandspatialbiasesinim-
Driess,AyzaanWahid,ZhuoXu,etal.2024. Pivot: ageclassification. InAdvancesinNeuralInformation
Iterativevisualpromptingelicitsactionableknowl- ProcessingSystems,volume36.
edgeforvlms. arXivpreprintarXiv:2402.07872.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
OpenAI.2023. Gpt-4v(ision)systemcard. Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
OpenAI.2024. Hellogpt-4o.
Anja Hauth, et al. 2023. Gemini: a family of
highlycapablemultimodalmodels. arXivpreprint
ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shao-
arXiv:2312.11805.
hanHuang,ShumingMa,QixiangYe,andFuruWei.
2024. Groundingmultimodallargelanguagemodels
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
to the world. In The Twelfth International Confer-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
enceonLearningRepresentations.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Bhosale, et al. 2023. Llama 2: Open founda-
Dong,JipengZhang,LeweiYao,JianhuaHan,Hang tion and fine-tuned chat models. arXiv preprint
Xu, Lingpeng Kong, et al. 2023. Detgpt: Detect arXiv:2307.09288.
whatyouneedviareasoning. InProceedingsofthe
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
2023ConferenceonEmpiricalMethodsinNatural
SMEslami,OriolVinyals,andFelixHill.2021. Mul-
LanguageProcessing,pages14172‚Äì14189.
timodalfew-shotlearningwithfrozenlanguagemod-
Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, els. InAdvancesinNeuralInformationProcessing
ZhuowenTu,andLiErranLi.2024. Affordancellm: Systems,volume34,pages200‚Äì212.
Groundingaffordancefromvisionlanguagemodels.
arXivpreprintarXiv:2401.06341. DavidWan,JaeminCho,EliasStengel-Eskin,andMo-
hitBansal.2024. Contrastiveregionguidance: Im-
HanoonaRasheed,MuhammadMaaz,SahalShaji,Ab- provinggroundinginvision-languagemodelswith-
delrahmanShaker,SalmanKhan,HishamCholakkal, outtraining. arXivpreprintarXiv:2403.02325.
Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and
FahadSKhan.2024. Glamm: Pixelgroundinglarge JunyangWang,YuhangWang,GuohaiXu,JingZhang,
multimodalmodel. InProceedingsoftheIEEE/CVF Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Ji-
ConferenceonComputerVisionandPatternRecog- tao Sang. 2023a. An llm-free multi-dimensional
nition. benchmarkformllmshallucinationevaluation. arXiv
preprintarXiv:2311.07397.
Machel Reid, Nikolay Savinov, Denis Teplyashin,
Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste JunyangWang,YiyangZhou,GuohaiXu,Pengcheng
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFi- Shi,ChenlinZhao,HaiyangXu,QinghaoYe,Ming
rat,JulianSchrittwieser,etal.2024. Gemini1.5: Un- Yan,JiZhang,JihuaZhu,etal.2023b. Evaluation
lockingmultimodalunderstandingacrossmillionsof andanalysisofhallucinationinlargevision-language
tokensofcontext. arXivpreprintarXiv:2403.05530. models. arXivpreprintarXiv:2308.15126.
13Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang,
Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He,
LeiZhao,XixuanSong,etal.2023c. Cogvlm: Vi- Zhiyuan Liu, Tat-Seng Chua, et al. 2024b. Rlaif-
sual expert for pretrained language models. arXiv v: Aligningmllmsthroughopen-sourceaifeedback
preprintarXiv:2311.03079. for super gpt-4v trustworthiness. arXiv preprint
arXiv:2405.17220.
WenhaiWang,ZheChen,XiaokangChen,JiannanWu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie BohanZhai,ShijiaYang,XiangchenZhao,Chenfeng
Zhou, Yu Qiao, et al. 2023d. Visionllm: Large Xu,ShengShen,DongdiZhao,KurtKeutzer,Man-
language model is also an open-ended decoder for ling Li, Tan Yan, and Xiangjun Fan. 2023. Halle-
vision-centrictasks. InAdvancesinNeuralInforma- switch: Rethinkingandcontrollingobjectexistence
tionProcessingSystems,volume36. hallucinations in large vision language models for
detailedcaption. arXivpreprintarXiv:2310.01779.
ZhuofanXia,DongchenHan,YizengHan,XuranPan,
HaoZhang,HongyangLi,FengLi,TianheRen,Xueyan
Shiji Song, and Gao Huang. 2024. Gsva: Gener-
Zou,ShilongLiu,ShijiaHuang,JianfengGao,Lei
alizedsegmentationviamultimodallargelanguage
Zhang,ChunyuanLi,etal.2023. Llava-grounding:
models. InProceedingsoftheIEEE/CVFConference
Groundedvisualchatwithlargemultimodalmodels.
onComputerVisionandPatternRecognition.
arXivpreprintarXiv:2312.02949.
JianweiYang,HaoZhang,FengLi,XueyanZou,Chun-
HaotianZhang,HaoxuanYou,PhilippDufter,Bowen
yuan Li, and Jianfeng Gao. 2023a. Set-of-mark
Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu,
promptingunleashesextraordinaryvisualgrounding
WilliamYangWang,Shih-FuChang,ZheGan,etal.
ingpt-4v. arXivpreprintarXiv:2310.11441.
2024a. Ferret-v2:Animprovedbaselineforreferring
and grounding with large language models. arXiv
LingfengYang,YuezeWang,XiangLi,XinlongWang,
preprintarXiv:2404.07973.
and Jian Yang. 2024. Fine-grained visual prompt-
ing. InAdvancesinNeuralInformationProcessing Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila
Systems,volume36. Shakiah, Qiaozi Gao, and Joyce Chai. 2024b.
Groundhog: Grounding large language models
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng to holistic segmentation. In Proceedings of the
Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan IEEE/CVFConferenceonComputerVisionandPat-
Wang. 2023b. The dawn of lmms: Preliminary ternRecognition.
explorations with gpt-4v (ision). arXiv preprint
arXiv:2309.17421,9(1):1. Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang,
JiashiFeng,andBingyiKang.2023. Bubogpt: En-
YuanYao,AoZhang,ZhengyanZhang,ZhiyuanLiu, ablingvisualgroundinginmulti-modalllms. arXiv
Tat-SengChua,andMaosongSun.2024. Cpt: Col- preprintarXiv:2307.08581.
orfulprompttuningforpre-trainedvision-language
models. AIOpen,5:30‚Äì38. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler,
AdelaBarriuso,andAntonioTorralba.2017. Scene
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, parsingthroughade20kdataset. InProceedingsof
Ming Yan, Yiyang Zhou, Junyang Wang, An- theIEEEconferenceoncomputervisionandpattern
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. recognition,pages633‚Äì641.
mplug-owl: Modularization empowers large lan-
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun
guage models with multimodality. arXiv preprint
Zhang,ZhunDeng,ChelseaFinn,MohitBansal,and
arXiv:2304.14178.
HuaxiuYao.2024. Analyzingandmitigatingobject
hallucination in large vision-language models. In
HaoxuanYou, HaotianZhang, ZheGan, XianzhiDu,
The Twelfth International Conference on Learning
Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-
Representations.
Fu Chang, and Yinfei Yang. 2023. Ferret: Refer
andgroundanythinganywhereatanygranularity. In
DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and
The Twelfth International Conference on Learning
MohamedElhoseiny.2023. Minigpt-4: Enhancing
Representations.
vision-languageunderstandingwithadvancedlarge
languagemodels. InTheTwelfthInternationalCon-
Alex Young, Bei Chen, Chao Li, Chengen Huang,
ferenceonLearningRepresentations.
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652.
ShoubinYu,JaehongYoon,andMohitBansal.2024a.
Crema: Multimodalcompositionalvideoreasoning
via efficient modular adaptation and fusion. arXiv
preprintarXiv:2402.05889.
14A AdditionalExperiments,Results,and
(LLaVA)Therearefiveredboundingboxesinthis
Discussions image.Foreachobjectwithintheredboundingboxes,
identifyitsclass.Providetheclassnamesinthe
A.1 Reproducibility format:‚Äôobj1:<class1>,obj2:<class2>,obj3:
<class3>,obj4:<class4>,obj5:<class5>‚Äô,withno
Data Curation Pipeline. Our data curation
additionalwordsorpunctuation.
pipelineinvolvesseveralessentialstepsdesigned
to prepare and refine our dataset for evaluating ‚Ä¢ SingleObjectDefaultProbing:
multi-object hallucination. The pipeline begins
Selectthesingle,mostappropriateclassfor
byfilteringimagesandcandidateobjectstoquery. obj<obj_num>locatedwithintheredboundingbox
We consider valid objects to be those belonging fromthefollowinglist:[CLASS NAMES].Your
responseshouldconsistsolelyoftheclassnamethat
to the top 50 ‚Äúthing‚Äù classes and exclude objects
obj<obj_num>belongsto,formattedasonlytheclass
withaboundingboxarealessthan1%ofthetotal name,withoutanyextracharactersorpunctuations.
image area. We discard images containing fewer
‚Ä¢ SingleObjectProbabilisticProbing:
than5validobjects,andallowanintersection-over-
unionbetweenboundingboxesofnomorethan0.1, (GroundHOG)Describeobject<PTR>inaword.
whichpreservesdataintegritywhileensuringhigh
imagequality. WeapplythispipelinetoMSCOCO-
(GLaMM)Whatistheclassof<bbox>?
Panoptic(Linetal.,2014;Caesaretal.,2018)and
ADE20K(Zhouetal.,2017).2
(LLaVA)Describetheobjectintheredboundingbox
labeledobj<obj_num>inaword.
Dataset Total COCO ADE
Wild 1539/1172 732/547 807/625 Computational Resources. Our experiments
Hom. 312/490 168/289 144/201 wereconductedoneightA40andfourA100GPUs
Het. 400/246 200/76 200/170 slightly over a week. The computational bottle-
Adv. 168/334 54/170 114/164 neck was not the numerical accuracy values but
thecollectionofpotentialhallucinatoryfactorsfor
Table3: Datastatistics(Train/Validation)
analyticalpurposes,includinglogitsandattention
LanguageInstructionPromptTemplates. We valuesforeachheadandlayer.
illustrate the 4 types of task prompts for Single-
Artifactsandlicenses Wereportalistoflicenses
Object andMulti-Object queriesinFigure2, and
foralldatasetsandmodelsusedinourexperiment
documentthepromptsbelow.
inTable4. Westrictlyfollowallthemodellicenses
and limit the scope of these models to academic
‚Ä¢ Multi-ObjectDefaultProbing,StudentForcing,
researchonly.
andTeacherForcing:
DataSources URL License
Selectoneandthemostappropriateclassforeach
objectlocatedwithinredboundingboxesfromthe MSCOCO2017 Link CCBY4.0
followinglist:[CLASS NAMES].Providetheclass ADE20K Link BSD-3-Clause
namesintheformat:‚Äôobj1:<class1>,obj2:<class2>,
obj3:<class3>,obj4:<class4>,obj5:<class5>‚Äô,with SoftwareCode URL License
noadditionalwordsorpunctuations.
LLaVA Link LlamaCommunityLicence
Qwen-VL Link TongyiQianwenLicence
‚Ä¢ Multi-ObjectProbabilisticProbing:
CogVLM Link CogVLMLicence
IDEFICS Link LlamaCommunityLicence
(GroundHOG)Describeobject1<PTR>andobject2 Yi-VL Link YiCommunityLicence
<PTR>andobject3<PTR>andobject4<PTR>and
MiniCPM-V Link ApacheLicense2.0
object5<PTR>.obj1:<class1>,obj2:<class2>,obj3:
GLAMM Link ApacheLicense2.0
<class3>,obj4:<class4>,obj5:<class5>
GPT-4V/4O Link OpenAITermofUse
Table4: Licenseinformationforthescientificartifacts.
(GLaMM)Whataretheclassesof[<bbox> LIST]?
obj1:<class1>,obj2:<class2>,obj3:<class3>,obj4: A.2 AdditionalExperimentsandResults
<class4>,obj5:<class5>
Weprovidetheper-objectperformanceinthefol-
lowingtables.
2Available at https://huggingface.co/datasets/
sled-umich/ROPE
15Multi-Object StudentForcing TeacherForcing Single-Object
Model
Wild Hom. Het. Adv. Wild Hom. Het. Adv. Wild Hom. Het. Adv. Wild Hom. Het. Adv.
object1
Yi-VL-6B 3.17 3.67 1.22 3.29 3.51 3.88 1.63 3.59 3.51 3.88 1.63 3.59 0.09 1.02 0.00 0.30
Yi-VL-34B 10.01 16.12 5.69 10.48 10.35 18.16 6.91 11.38 10.35 18.16 6.91 11.38 0.43 2.24 0.00 0.60
LLaVA-7B 34.99 67.14 13.41 55.39 34.90 67.14 13.41 55.39 34.90 67.14 13.41 55.39 34.99 65.71 15.04 53.89
LLaVA-13B 29.77 62.86 12.60 51.20 29.52 62.86 12.60 50.60 32.34 63.09 12.60 50.60 40.96 72.24 23.17 40.00
LLaVA-34B 26.61 77.55 17.48 62.87 38.58 88.19 14.23 60.97 38.58 88.19 14.23 60.97 48.33 77.55 27.64 66.47
QwenVL 4.01 2.44 5.92 4.19 9.56 20.41 6.91 16.47 10.49 20.82 5.28 17.96 18.39 32.04 9.76 19.76
QwenVL-C 22.53 28.16 13.41 24.25 22.61 33.88 13.41 24.25 23.21 32.86 8.94 23.65 34.13 59.80 16.67 47.01
CogVLM 1.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 8.94 1.47 9.76 7.02 10.16 16.50 4.17 10.71 11.83 13.97 8.94 7.78
CogVLM-G 17.88 27.76 7.72 27.54 23.18 44.90 9.76 33.83 23.18 44.90 9.76 33.83 30.89 50.82 10.98 42.22
CogVLM-2 24.55 36.33 16.26 31.44 24.89 37.14 18.70 44.61 24.89 37.14 18.70 44.61 10.85 31.43 3.25 27.25
IDEFICS 1.45 1.63 1.22 1.50 11.95 24.29 6.10 14.67 11.95 24.29 6.10 14.67 4.86 3.88 5.28 2.10
MiniCPM-V 33.86 63.21 18.62 55.69 35.59 65.31 15.04 54.79 35.59 65.31 15.04 54.79 25.15 47.76 12.60 37.13
LLaVA-7B* N/A 7.08 11.43 6.50 8.68 N/A 13.45 25.75 13.14 24.40
GLaMM* N/A 53.50 50.20 41.06 50.30 N/A 68.34 77.55 54.07 73.35
GroundHOG* N/A 15.27 20.41 10.98 19.76 N/A 40.10 28.98 31.30 32.34
GPT-4V** 49.53 67.35 38.21 56.29 N/A N/A 47.05 64.49 36.59 57.19
GPT-4O** 64.42 80.61 56.10 73.05 N/A N/A 63.56 81.22 53.66 73.65
object2
Yi-VL-6B 2.65 4.08 2.44 3.29 1.88 8.37 4.47 5.69 3.68 7.96 5.69 9.58 0.17 0.61 0.00 0.00
Yi-VL-34B 8.55 17.55 3.25 15.57 10.61 19.59 8.54 12.28 11.21 22.24 10.98 14.37 0.51 2.45 0.41 0.60
LLaVA-7B 29.77 67.96 8.13 56.89 29.52 67.96 8.13 56.89 32.00 84.69 5.28 79.34 34.13 61.02 15.45 50.30
LLaVA-13B 29.17 63.06 4.07 52.40 28.92 63.06 4.07 52.10 37.97 92.37 6.10 85.63 41.64 70.82 21.54 42.58
LLaVA-34B 30.89 79.39 18.29 68.56 27.56 90.55 15.45 65.81 34.40 91.84 2.85 68.25 52.18 78.16 30.08 68.86
QwenVL 3.84 2.44 8.98 6.59 10.24 23.06 4.07 20.36 13.05 37.35 4.07 33.53 16.94 27.35 9.35 20.66
QwenVL-C 23.12 35.71 10.57 28.74 17.75 26.12 7.32 19.46 22.61 36.33 11.38 30.24 33.45 53.88 13.82 45.81
CogVLM 0.00 0.00 0.00 0.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 8.12 1.47 4.88 7.02 25.90 93.50 2.08 94.64 8.39 10.29 6.91 6.89
CogVLM-G 16.34 28.37 4.07 29.64 15.57 39.80 4.88 34.13 21.81 44.90 2.85 39.52 30.72 51.43 14.23 43.71
CogVLM-2 25.75 34.08 19.51 28.14 24.72 36.53 22.76 45.21 26.43 40.61 26.83 45.21 11.32 32.45 5.69 25.45
IDEFICS 0.17 0.20 0.41 0.60 11.69 27.76 4.07 20.06 31.91 91.43 2.03 85.93 4.10 3.67 6.50 2.10
MiniCPM-V 35.79 57.73 17.00 49.10 31.99 58.37 13.01 53.29 31.48 60.41 13.41 53.29 26.09 49.80 14.23 34.13
LLaVA-7B* N/A 8.28 12.86 4.47 11.08 N/A 10.59 23.00 5.45 21.43
GLaMM* N/A 23.46 54.69 6.50 52.99 N/A 69.62 75.92 49.19 72.46
GroundHOG* N/A 31.31 27.14 32.52 27.25 N/A 41.13 29.59 40.65 33.83
GPT-4V** 48.16 67.55 36.18 60.18 N/A N/A 47.48 65.51 32.52 57.78
GPT-4O** 63.48 80.20 56.50 73.05 N/A N/A 62.62 80.41 54.07 72.16
object3
Yi-VL-6B 2.91 3.88 1.22 2.69 3.68 3.88 5.69 5.69 4.02 8.37 17.89 15.87 0.17 0.00 0.41 0.30
Yi-VL-34B 8.30 15.92 6.10 14.67 10.95 17.96 9.35 11.98 11.46 22.86 12.60 15.57 0.43 2.86 0.41 0.30
LLaVA-7B 27.97 68.57 12.60 57.19 27.99 68.57 12.20 57.19 29.95 99.80 11.38 99.70 31.74 63.27 16.26 51.20
LLaVA-13B 25.32 62.86 8.54 52.40 25.17 62.86 8.54 52.10 36.52 100.00 13.01 100.00 43.77 72.65 24.39 45.16
LLaVA-34B 29.97 80.00 21.54 69.16 27.56 92.91 13.01 67.42 41.20 99.32 8.54 94.79 54.32 78.37 34.15 69.46
QwenVL 2.30 2.03 7.55 7.19 6.48 16.53 4.47 14.37 21.73 91.02 4.47 45.51 18.31 33.47 12.20 23.95
QwenVL-C 18.09 33.27 8.54 24.25 14.85 28.16 6.50 19.16 21.67 68.37 6.91 60.78 34.47 57.96 15.85 49.10
CogVLM 0.00 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.60 0.00 0.00 7.84 1.47 4.47 7.02 26.02 97.50 0.69 97.62 10.04 6.62 11.79 7.49
CogVLM-G 12.15 25.31 4.88 25.15 13.34 37.55 5.69 31.14 18.39 56.73 9.35 50.30 30.12 52.04 14.23 43.71
CogVLM-2 19.16 35.31 16.26 27.84 24.98 38.98 24.80 42.81 27.46 43.47 32.93 51.50 10.50 30.00 6.91 21.56
IDEFICS 0.09 0.00 0.00 0.30 6.23 23.27 1.22 13.77 26.02 99.80 8.94 100.00 5.20 3.67 8.54 2.10
MiniCPM-V 35.12 61.06 13.77 52.10 28.83 55.71 14.63 49.70 29.43 57.14 15.04 47.90 26.52 46.53 14.23 32.93
LLaVA-7B* N/A 8.62 12.65 4.07 10.18 N/A 10.53 23.00 9.62 21.43
GLaMM* N/A 22.53 56.33 7.32 54.19 N/A 67.15 77.96 54.07 73.35
GroundHOG* N/A 24.49 25.31 30.08 26.35 N/A 40.10 30.82 40.24 35.33
GPT-4V** 46.96 62.24 34.15 52.69 N/A N/A 47.73 65.51 34.55 56.59
GPT-4O** 61.51 80.41 57.32 74.55 N/A N/A 63.05 79.59 51.22 71.56
object4
Yi-VL-6B 2.48 4.29 0.41 3.29 3.17 2.24 7.72 6.29 4.45 15.10 13.41 15.27 0.09 0.20 0.00 0.30
Yi-VL-34B 6.67 15.31 4.47 11.98 10.01 18.37 7.72 13.17 11.80 24.90 15.45 17.66 0.51 2.04 0.81 0.60
LLaVA-7B 30.37 68.37 7.32 56.89 30.20 68.57 7.32 56.89 29.95 100.00 17.89 100.00 37.64 66.12 15.85 57.19
LLaVA-13B 26.26 63.47 7.32 52.69 26.19 63.47 7.32 52.40 35.41 100.00 18.29 100.00 44.45 72.86 26.42 43.87
LLaVA-34B 30.58 80.20 18.29 70.36 31.50 92.13 13.41 67.10 35.60 99.32 6.91 95.26 50.56 77.14 28.05 70.06
QwenVL 2.65 2.03 8.78 7.19 5.20 12.45 4.07 11.08 9.04 97.35 3.25 38.32 18.14 30.41 8.94 26.65
QwenVL-C 15.44 29.18 6.50 23.35 12.54 25.51 6.91 18.86 19.97 72.04 13.41 63.77 32.85 40.41 17.48 49.10
CogVLM 0.00 0.00 0.00 0.30 0.00 0.00 0.00 0.00 26.60 47.55 29.67 57.49 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 7.98 1.47 4.07 7.02 28.00 98.50 2.08 98.81 11.14 10.29 11.38 7.78
CogVLM-G 15.31 25.92 5.69 26.35 17.19 42.86 5.28 36.23 22.75 60.20 0.81 60.48 30.29 55.10 12.60 43.41
CogVLM-2 18.05 34.69 16.26 29.64 25.15 38.57 28.86 41.62 27.63 49.18 20.73 57.49 10.97 31.84 6.10 26.95
IDEFICS 0.09 0.00 0.00 0.30 8.70 23.47 1.22 13.47 26.88 99.80 8.13 100.00 4.95 3.67 8.13 2.10
MiniCPM-V 30.26 60.86 20.24 52.40 31.39 58.78 13.82 53.89 31.39 61.63 19.92 55.39 25.15 48.57 15.85 38.92
LLaVA-7B* N/A 6.66 11.63 2.44 9.88 N/A 13.06 23.00 10.90 21.43
GLaMM* N/A 22.27 56.12 8.54 54.19 N/A 68.17 76.33 52.85 74.85
GroundHOG* N/A 19.54 25.31 25.61 25.75 N/A 39.42 32.45 36.18 36.83
GPT-4V** 45.42 64.08 31.71 50.00 N/A N/A 47.82 66.53 35.77 53.59
GPT-4O** 63.82 79.80 57.32 74.85 N/A N/A 64.16 79.59 53.66 70.96
object5
Yi-VL-6B 2.48 3.47 0.41 2.10 3.68 2.86 6.50 5.39 4.53 19.18 14.23 17.66 0.17 0.41 0.00 0.00
Yi-VL-34B 5.30 13.27 1.63 3.29 9.50 16.12 7.32 9.88 11.38 24.29 14.23 18.26 0.43 2.24 0.41 0.30
LLaVA-7B 30.11 68.57 10.16 13.17 30.12 68.57 10.16 13.17 32.68 100.00 18.29 0.00 36.36 65.92 18.29 23.35
LLaVA-13B 27.63 63.27 9.35 10.48 27.22 63.27 9.35 10.48 36.01 100.00 23.98 0.00 42.49 71.02 21.54 21.29
LLaVA-34B 28.44 80.00 13.01 8.38 25.20 91.34 15.85 18.71 37.20 99.32 3.66 5.21 53.81 78.16 34.15 34.73
QwenVL 1.19 0.81 4.08 1.50 4.35 9.59 1.22 1.50 7.68 47.35 3.25 0.00 17.11 32.86 7.32 11.98
QwenVL-C 15.10 27.35 4.88 8.38 13.05 25.31 4.47 5.09 21.59 80.41 15.04 6.29 36.09 59.80 13.01 19.16
CogVLM 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 8.12 1.47 5.69 7.44 29.00 100.00 0.00 0.60 10.18 12.50 6.50 9.58
CogVLM-G 16.34 25.51 5.28 6.59 16.60 41.22 4.47 5.69 27.89 73.47 10.57 3.89 30.80 53.06 15.45 17.66
CogVLM-2 17.71 34.90 11.38 12.87 23.44 38.98 20.73 12.28 27.29 59.80 33.74 12.57 12.02 28.98 6.91 5.09
IDEFICS 0.17 0.00 0.00 0.00 6.57 23.47 0.81 1.80 27.22 99.80 13.01 0.00 4.01 3.47 4.07 4.79
MiniCPM-V 29.76 56.75 13.36 19.16 31.05 56.73 14.23 14.07 31.48 60.41 18.29 17.66 25.24 46.12 15.04 17.96
LLaVA-7B* N/A 7.34 12.04 6.91 4.79 N/A 10.79 23.00 7.69 10.12
GLaMM* N/A 23.81 55.31 7.72 6.29 N/A 69.97 77.55 51.22 53.29
GroundHOG* N/A 24.91 25.31 32.11 18.56 N/A 42.92 30.00 42.28 30.54
GPT-4V** 38.41 56.94 31.30 27.54 N/A N/A 46.62 62.65 37.80 31.44
GPT-4O** 63.74 80.41 54.07 53.59 N/A N/A 64.67 78.37 56.10 54.49
Table5: Completeper-objectresultsontheunseensplit.
16Multi-Object StudentForcing TeacherForcing Single-Object
Model
Wild Hom. Het. Adv. Wild Hom. Het. Adv. Wild Hom. Het. Adv. Wild Hom. Het. Adv.
object1
Yi-VL-6B 2.92 5.50 1.92 4.76 3.12 5.75 2.56 6.55 3.12 20.75 2.56 6.55 0.06 0.25 0.00 0.60
Yi-VL-34B 8.51 15.75 4.81 13.10 8.71 16.00 5.13 11.90 8.71 16.00 5.13 11.90 0.19 2.50 0.00 0.60
LLaVA-7B 31.51 65.75 5.00 51.79 31.45 65.75 10.90 51.79 31.45 65.75 10.90 51.79 33.01 62.75 17.95 47.62
LLaVA-13B 28.14 65.83 10.91 54.17 28.14 72.00 7.69 54.76 28.14 72.00 7.69 54.76 41.46 81.25 24.04 65.48
LLaVA-34B 34.18 83.50 8.65 67.26 48.02 83.75 29.17 72.02 48.02 83.75 29.17 72.02 54.13 85.00 17.20 77.98
QwenVL 2.21 6.00 1.60 4.17 7.41 14.25 1.28 10.71 5.46 15.25 2.24 9.52 8.06 16.50 4.49 17.86
QwenVL-C 5.65 14.25 2.24 10.71 5.59 9.25 2.24 8.33 6.04 8.75 2.24 7.74 25.02 43.50 14.10 30.36
CogVLM 0.00 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 11.18 13.50 4.17 16.67 3.44 3.00 1.92 4.76 12.46 24.75 5.13 25.60
CogVLM-G 11.70 18.25 6.09 19.64 24.44 41.67 10.30 29.19 24.61 49.75 10.90 29.87 28.85 55.25 14.74 40.48
CogVLM-2 18.97 37.00 13.78 32.14 35.48 69.50 10.90 33.33 35.48 69.50 10.90 33.33 18.78 39.25 9.62 30.36
IDEFICS 0.00 3.50 0.64 1.20 7.41 20.25 0.96 14.29 7.41 20.25 0.96 14.29 4.86 0.00 0.00 0.00
MiniCPM-V 32.81 66.25 15.38 58.33 31.51 72.00 13.14 63.69 31.45 72.25 13.14 59.52 25.87 54.00 15.38 58.93
LLaVA-7B* N/A 9.62 16.25 6.41 11.31 N/A 13.45 25.75 13.14 24.40
GLaMM* N/A 45.61 44.50 40.38 42.26 N/A 64.33 81.25 55.77 73.81
GroundHOG* N/A 16.11 24.25 16.03 22.62 N/A 43.86 43.50 45.19 51.79
GPT-4V** 56.79 79.75 41.35 71.43 N/A N/A 55.30 76.25 41.35 71.43
GPT-4O** 69.98 89.50 66.03 79.76 N/A N/A 61.35 73.27 54.47 69.46
object2
Yi-VL-6B 3.83 5.75 3.21 6.55 3.31 6.25 3.53 7.74 6.17 10.25 4.81 6.55 0.19 0.25 0.00 1.19
Yi-VL-34B 9.10 15.50 3.21 13.10 8.90 16.75 5.77 10.12 10.59 21.00 5.77 23.81 0.26 2.50 0.32 1.19
LLaVA-7B 32.36 67.50 15.00 52.98 32.36 67.25 12.50 52.98 35.15 95.00 12.18 91.07 36.32 61.75 20.19 47.62
LLaVA-13B 32.42 67.67 13.48 56.55 32.36 72.75 14.42 56.55 37.04 99.25 11.54 97.02 43.66 80.50 24.36 72.02
LLaVA-34B 41.26 85.50 25.32 70.24 53.15 85.75 31.41 73.21 57.05 98.10 21.84 94.30 54.52 85.50 18.10 77.38
QwenVL 3.38 6.00 0.96 3.57 5.78 15.75 4.49 12.50 19.36 57.50 5.13 42.86 8.45 14.75 4.81 12.50
QwenVL-C 9.03 16.50 9.62 11.31 5.52 8.75 6.73 6.55 16.89 45.25 9.29 44.05 26.19 40.00 15.71 28.57
CogVLM 0.06 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 12.12 13.50 8.65 16.67 29.43 94.00 0.00 94.64 12.38 22.50 7.69 18.45
CogVLM-G 12.15 21.75 6.09 22.02 25.79 42.83 7.12 30.87 27.40 55.75 16.67 33.56 31.06 55.00 18.27 42.86
CogVLM-2 24.95 38.00 21.79 38.10 38.60 72.00 12.50 32.14 36.13 70.25 14.10 31.55 21.90 40.00 14.42 29.76
IDEFICS 0.00 1.00 0.00 0.60 5.78 17.75 0.00 8.33 22.16 77.75 12.18 63.10 4.10 0.00 0.96 0.00
MiniCPM-V 36.19 64.75 19.23 60.71 33.07 62.75 16.67 57.14 33.79 69.00 17.63 61.31 30.06 60.75 17.63 60.12
LLaVA-7B* N/A 9.10 17.75 6.41 13.10 N/A 10.59 23.00 5.45 21.43
GLaMM* N/A 22.22 55.75 6.73 56.55 N/A 64.20 80.25 56.41 72.62
GroundHOG* N/A 31.32 33.50 34.29 30.36 N/A 43.60 44.25 41.03 51.19
GPT-4V** 55.69 78.75 41.35 73.81 N/A N/A 56.27 78.50 37.50 72.62
GPT-4O** 71.28 89.50 64.10 82.14 N/A N/A 60.24 74.90 54.88 68.56
object3
Yi-VL-6B 2.99 6.25 2.56 5.95 3.12 7.00 4.17 8.93 5.26 24.75 6.09 8.33 0.13 0.25 0.32 1.19
Yi-VL-34B 9.10 16.00 1.60 14.88 9.10 16.75 2.24 10.71 11.23 21.75 4.17 36.31 0.19 2.50 0.32 1.19
LLaVA-7B 29.76 68.00 5.00 52.98 29.76 67.50 12.18 52.98 28.40 100.00 11.86 100.00 33.92 61.50 14.42 48.81
LLaVA-13B 32.75 68.00 13.18 58.33 32.68 73.50 10.90 57.74 38.27 100.00 17.63 100.00 44.90 82.00 20.19 67.86
LLaVA-34B 44.70 86.50 24.68 70.24 54.00 86.25 36.86 73.81 57.89 98.80 27.63 98.32 53.41 87.50 16.32 77.98
QwenVL 2.86 7.50 0.96 7.14 6.30 18.00 4.49 16.67 21.25 92.50 5.13 88.10 8.58 16.25 5.77 17.26
QwenVL-C 10.66 21.25 7.05 21.43 6.04 9.00 5.13 9.52 16.89 60.75 9.29 61.31 26.19 46.25 10.90 32.74
CogVLM 0.06 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.13 0.75 0.00 1.19 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 10.84 13.50 5.77 16.67 27.03 94.00 0.00 94.64 11.46 21.25 7.05 16.67
CogVLM-G 12.74 25.75 7.05 24.40 26.89 45.00 14.39 34.23 27.23 81.25 19.23 57.72 29.63 55.50 16.99 42.26
CogVLM-2 20.27 36.50 15.71 36.31 39.12 72.75 13.14 32.14 36.45 74.75 17.31 49.40 19.36 34.75 12.18 31.55
IDEFICS 0.00 1.00 0.00 0.90 6.30 18.75 0.64 11.90 15.01 92.50 8.65 86.90 5.20 0.00 0.32 0.00
MiniCPM-V 36.19 68.50 20.51 61.31 31.38 60.00 12.50 52.38 30.60 68.25 15.71 54.76 26.15 53.75 19.87 95.05
LLaVA-7B* N/A 8.77 16.50 5.45 11.90 N/A 10.53 23.00 9.62 21.43
GLaMM* N/A 22.68 55.25 5.13 54.17 N/A 62.70 83.00 47.44 77.38
GroundHOG* N/A 24.82 32.75 24.68 30.36 N/A 45.09 43.50 35.26 50.00
GPT-4V** 54.45 79.25 40.06 70.24 N/A N/A 55.62 80.25 39.42 72.02
GPT-4O** 71.35 90.75 65.71 83.93 N/A N/A 60.07 74.29 53.66 68.26
object4
Yi-VL-6B 3.25 5.50 1.60 4.76 4.35 7.75 5.13 10.71 6.30 37.75 4.17 8.93 0.26 0.25 0.00 1.19
Yi-VL-34B 8.32 15.75 4.49 13.69 9.16 17.00 5.13 8.93 11.05 22.75 5.45 34.52 0.26 3.00 0.00 1.19
LLaVA-7B 32.42 68.75 10.00 59.52 32.42 68.50 10.90 54.17 30.28 100.00 15.06 100.00 36.84 63.00 17.31 49.40
LLaVA-13B 31.38 68.17 14.39 94.05 31.32 73.75 11.54 58.93 36.06 100.00 24.68 100.00 44.83 80.00 25.96 64.88
LLaVA-34B 40.68 86.75 19.23 73.21 54.06 85.25 34.62 73.81 57.96 99.10 25.26 97.65 56.14 86.75 22.12 80.95
QwenVL 2.47 7.25 0.00 4.76 5.91 14.75 2.88 12.50 22.87 95.50 7.69 95.24 9.75 14.25 6.41 16.07
QwenVL-C 8.58 16.25 7.05 16.07 5.46 8.00 2.88 7.74 11.96 56.50 10.26 54.17 24.82 38.50 11.22 28.57
CogVLM 0.00 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.06 1.00 0.00 0.60 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 10.24 13.50 9.94 16.67 26.25 93.75 0.32 93.45 9.83 21.25 9.29 20.83
CogVLM-G 14.10 24.75 9.62 27.98 26.09 45.33 15.30 34.23 30.91 86.00 23.40 62.08 29.89 57.75 16.99 44.05
CogVLM-2 22.94 37.75 19.87 36.31 36.58 66.75 14.74 33.93 38.40 75.25 20.51 61.31 24.24 37.50 15.71 26.79
IDEFICS 0.00 1.25 0.00 0.60 5.91 18.25 0.64 11.90 21.12 95.00 14.74 93.45 4.95 23.00 0.00 0.00
MiniCPM-V 37.36 64.75 18.59 60.71 32.42 59.75 13.14 56.55 34.05 67.75 19.55 55.36 29.14 56.00 16.35 62.50
LLaVA-7B* N/A 8.90 15.75 5.45 11.31 N/A 13.06 23.00 10.90 21.43
GLaMM* N/A 21.70 55.75 6.73 55.95 N/A 64.91 80.25 55.45 67.86
GroundHOG* N/A 20.86 31.50 21.15 30.95 N/A 43.99 42.00 35.58 50.00
GPT-4V** 53.09 77.25 43.59 72.62 N/A N/A 56.40 78.75 44.87 74.40
GPT-4O** 72.25 87.00 68.27 82.14 N/A N/A 60.67 72.65 55.69 70.06
object5
Yi-VL-6B 1.75 5.25 0.64 1.19 3.31 7.25 3.53 8.93 6.37 37.75 4.17 0.60 0.32 0.50 0.32 0.60
Yi-VL-34B 7.47 13.75 2.56 3.57 8.97 15.00 2.88 5.36 8.86 17.25 4.17 4.17 0.19 2.50 0.00 0.60
LLaVA-7B 30.41 67.50 5.00 7.74 30.41 67.25 9.62 8.93 32.16 100.00 11.86 0.00 36.52 62.75 16.99 20.83
LLaVA-13B 33.01 68.50 11.21 21.43 32.94 74.25 13.14 7.74 35.35 100.00 18.59 0.00 40.81 79.25 25.00 23.81
LLaVA-34B 38.92 86.50 16.35 9.52 54.52 85.00 37.50 32.14 61.13 99.30 22.63 8.05 57.05 87.75 21.13 32.14
QwenVL 2.73 6.25 1.60 1.79 5.85 17.25 5.13 3.57 24.76 96.75 7.05 0.00 8.84 18.50 6.41 4.17
QwenVL-C 9.68 16.25 7.37 4.76 3.70 8.00 3.53 1.79 8.77 67.50 9.29 0.60 27.75 48.75 14.10 15.48
CogVLM 0.06 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.06 3.00 1.00 0.00 0.00 0.00 0.00 0.00
CogVLM-C 0.00 0.00 0.00 0.00 9.98 13.50 5.45 8.93 27.03 92.50 0.00 0.60 10.14 21.75 6.41 9.52
CogVLM-G 13.78 23.25 7.05 10.12 23.64 43.33 8.94 19.13 31.12 91.25 17.31 8.72 31.38 56.50 14.74 13.10
CogVLM-2 20.40 38.50 15.38 23.21 35.35 73.25 12.18 32.74 39.05 77.75 24.36 19.64 22.09 42.25 16.35 14.88
IDEFICS 0.00 0.50 0.00 0.60 5.85 18.50 0.96 1.79 21.18 95.25 13.78 0.60 4.01 0.00 0.32 0.00
MiniCPM-V 31.19 60.00 13.14 17.26 29.69 59.50 12.82 16.07 30.93 63.00 17.95 16.67 25.87 52.25 15.38 22.02
LLaVA-7B* N/A 9.42 15.75 3.85 4.76 N/A 10.79 23.00 7.69 10.12
GLaMM* N/A 23.33 55.50 6.09 4.76 N/A 62.90 84.00 51.92 46.43
GroundHOG* N/A 24.76 32.00 25.00 26.19 N/A 43.86 42.25 37.82 42.26
GPT-4V** 48.99 72.75 37.82 43.45 N/A N/A 55.88 77.50 41.99 39.88
GPT-4O** 71.47 89.50 66.35 64.29 N/A N/A 61.52 74.49 52.85 53.89
Table6: Completeper-objectresultsontheseensplit.
17