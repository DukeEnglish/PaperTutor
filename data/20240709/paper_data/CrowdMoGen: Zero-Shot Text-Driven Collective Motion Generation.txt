CrowdMoGen: Zero-Shot Text-Driven Collective
Motion Generation
XinyingGuo MingyuanZhang HaozheXie ChenyangGu ZiweiLiu(cid:66)
S-Lab,NanyangTechnologicalUniversity
(cid:66)correspondingauthor
https://gxyes.github.io/projects/CrowdMoGen.html
Three people greeting someone running over Acrowd running and avoiding a car
One person calling a crowd to gather Someone fell and others comingto help
Figure1: CrowdMoGenisazero-shot,text-drivenframeworkthatenablesgeneralizableplanning
andgenerationofcrowdmotions. Givenascenecontext,weaimtogeneraterealisticcrowdmotions
thatfitthescenesettings. ThemotionsabovearegeneratedbytheproposedCrowdMoGen.
Abstract
CrowdMotionGenerationisessentialinentertainmentindustriessuchasanimation
andgamesaswellasinstrategicfieldslikeurbansimulationandplanning. This
newtaskrequiresanintricateintegrationofcontrolandgenerationtorealistically
synthesizecrowddynamicsunderspecificspatialandsemanticconstraints,whose
challengesareyettobefullyexplored. Ontheonehand,existinghumanmotion
generationmodelstypicallyfocusonindividualbehaviors,neglectingthecomplex-
itiesofcollectivebehaviors. Ontheotherhand,recentmethodsformulti-person
motion generation depend heavily on pre-defined scenarios and are limited to
a fixed, small number of inter-person interactions, thus hampering their practi-
cality. Toovercomethesechallenges,weintroduceCrowdMoGen,azero-shot
text-drivenframeworkthatharnessesthepowerofLargeLanguageModel(LLM)
toincorporatethecollectiveintelligenceintothemotiongenerationframeworkas
guidance,therebyenablinggeneralizableplanningandgenerationofcrowdmotions
withoutpairedtrainingdata. Ourframeworkconsistsoftwokeycomponents: 1)
CrowdScenePlannerthatlearnstocoordinatemotionsanddynamicsaccording
tospecificscenecontextsorintroducedperturbations,and2)CollectiveMotion
Generatorthatefficientlysynthesizestherequiredcollectivemotionsbasedonthe
holisticplans. Extensivequantitativeandqualitativeexperimentshavevalidated
theeffectivenessofourframework,whichnotonlyfillsacriticalgapbyproviding
scalableandgeneralizablesolutionsforCrowdMotionGenerationtaskbutalso
achieveshighlevelsofrealismandflexibility.
Preprint.Underreview.
4202
luJ
8
]VC.sc[
1v88160.7042:viXra1 Introduction
CrowdMotionGenerationholdssignificantpotentialacrossvariousfields,notablyinentertainment,
andurbansimulationandplanning. Despiteitsimportance,currenttechniquespredominantlyaddress
crowddynamicsthroughsimulationsratherthanactualmotiongeneration. Thesetraditionalmethods
oftenreduceindividualswithincrowdstosimplisticparticlemodelsthatonlyaccountfortrajectories
withoutdetailed,context-specificmotions. Alternatively,theyrelyonbasic,non-transferablerule-
basedmotionpatternsthataretightlyboundtospecificscenarios. Suchapproaches,therefore,donot
genuinelygeneratecrowdmotions;theymerelysimulatethemonarudimentarylevel. Thislimitation
revealsacleargapinresearchfocusedonthegenerationofdiverseandadaptablecrowdmotionsthat
canbetailoredtomeetuser-specificrequirements. Addressingthisgap,ourworktakesapioneering
stepforwardbydefiningthetaskofCrowdMotionGenerationasthecreationofcrowdmotionsin
responsetotextualdescriptionsthatspecifyuserneeds,andweintroduceCrowdMoGen,azero-shot
text-drivenframeworkdesignedtogenerateauthenticandflexiblecrowdmotions.
Currentsinglemotiongenerationmethodsnowincorporatemulti-modalcontrols,includingaction
categories[15,41],text[14,68],music[52,12],andhybridinputs[70]. However,theseapproaches
encounterscalingissuesinmulti-personscenariosdueprimarilytoinadequatehandlingofcomplex
inter-humaninteractions. Furthermore,existingmulti-personmotiongenerationtechniquesgenerally
relyondatasetsthatfeatureonlytwointeractingindividuals[9,6,28]orareconstrainedtolimited
pre-definedscenarios[73]. Consequently,thesemethodsnotonlyconfrontchallengesinscalingto
largercrowdsbutalsodemonstratelimitedadaptabilitytomoregeneralscenesthatrequirevaried
interactionsanddynamiccrowdbehaviors.
There are two primary challenges in addressing the Crowd Motion Generation task. 1) There is
a marked scarcity of crowd datasets with paired data. Current multi-person datasets are often
limitedinsize[32],featureonlyafixedandsmallnumberofparticipants[9,6,28],haverestricted
themes [73], and lack comprehensive annotations [38]. 2) As crowd sizes increase, effectively
modelingthecomplexinteractionsamongnumerousindividualsandgeneratingtheirmovements
becomeschallengingbutcrucial.
Toaddressthesechallenges,weproposeCrowdMoGen,anoveltwo-stage,zero-shotframeworkfor
CrowdMotionGeneration,whichseparatesmotiondecision-makingfrommotiongenerationinto
twodistincttasks. 1)CrowdScenePlannerusesaLargeLanguageModel(LLM)tointerpretand
decideoncrowdmovementsbasedonuserscenarios,givingourmethodzero-shotcapabilities. It
providesdetailedsemanticattributes(likeactioncategories)andspatialattributes(liketrajectories
andinteractions)foreachindividual,managingbothoverallcrowddynamicsandindividualinter-
actions. 2)CollectiveMotionGeneratorenhancestherealismofgeneratedmotionsandensures
strict adherence to control signals through joint-wise InputMixing, customized ControlAttention
mechanisms,andcarefullydesignedtrainingobjectives.
Theprimarycontributionsareasfollows:
1)WeintroducetheconceptofCrowdMotionGeneration,whichintegratesstrategicmotionplanning
withadvancedcontrolmechanismstocreaterealisticcrowdscenestailoredtouserrequirements.
2)WeproposeCrowdMoGen,whichleveragesLLMtoprovidezero-shotcapabilities,effectively
addressingtheCrowdMotionGenerationtaskbyseparatingmotiondecision-makingfrommotion
generationintotwodistinctsub-goals.
3)Extensivequantitativeandqualitativeevaluationsdemonstratetheeffectivenessofourmethod. It
fillstheresearchgapbyachievinghighrealismandflexibilityingeneratingcrowdmotions.
2 RelatedWorks
HumanMotionGeneration. VariationalAutoencoders(VAEs)anddiffusionmodelsarewidely
usedinmotiongeneration,leveragingdiversecontrolpromptstocreatehigh-qualityhumanmotions.
Motionprediction[13,53,16,4,2,1,11,62,58,37,5,7,22,54],forexample,generatecomplete
sequencesfromincompletehistoricaldataorpartialmovements,ensuringthegeneratedmotions
aresmoothandrealistic. Arangeofexternalcontrols, suchasactioncategories[15,41,72,25],
music [27, 74, 52, 39, 63], text [42, 56, 57, 24, 3, 34, 71, 50, 35, 47, 68, 21, 19, 64, 43, 30, 69],
scenes [20, 29, 33, 60], objects [10, 26, 55, 17, 31, 44, 40, 51], and trajectories [23, 46], further
2allowsforstylizedandcustomizedmotions. Additionally,recentinnovationshaveintroducedunified
motionmodels[70]thatcombinemulti-modalinputsandmultitasklearningtoenhancetheversatility
andcontrollabilityofindividualhumanmotiongeneration.
Recently, multi-person motion generation has gained increased attention. Some methods, like
unsupervisedmotioncompletionforgroups[73],failtocapturethesemanticandspatialdynamicsof
interactionsduetorelianceonlimited-themeddatasetslackingcomprehensiveannotations. Moreover,
whiletwo-persontext-motiondatasetssuchasDLP[6]andInterHuman[28]havebeenintroduced,
methodsdependingonthemeffectivelymodelpairwiseinteractionsbutstruggletoextendtolarger
groups,failingtoaddressbroaderinter-humandynamics. InterControl[59]makesinitialstridesto
createmulti-personinteractionsbyexplicitlycontrollingjointpositionsasperpredefinedmotion
plans,butitseffectivenessdiminishesinscenariosinvolvingmorethantwopeopleorlargercrowds
duetorequiringheavymanualwork.
Consequently,existingmethodsexposesignificantshortcomingsinrealisticallygeneratingcrowd
motions,highlightingtheneedformoreadaptableandlogicalmotionplanning,aswellasefficient
motioncontrollingandgenerationstrategies.
Controllable Motion Generation with Diffusion Models. Crowd Motion Generation requires
precisemanagementofcrowddynamicsandindividualmotionsemantics,withtheintegrationof
spatialconstraintsintotext-drivenmotiongenerationmodelposingsignificantchallenges. Successful
methodsneedtoalignwithtextualdescriptions,adheretospatialcontrols,maintainnaturalmotion,
andrespecthumanbodyprior. ExistingmethodslikeMDM[57]andPriorMDM[48],whichrely
oninpaintingtechniques,predictmissingmotioncomponentsfromobserveddatabutoftenfailto
effectivelymanipulatejointsotherthanthepelvisorhandlesparsespatialconstraints. GMD[23]
addresses the issue of sparse guidance with dense guidance propagation technique yet struggles
to flexibly control spatial constraints across various joints and frames. OmniControl [61] and
InterControl [59] utilize a hybrid method combining classifier guidance with a ControlNet [67],
enhancingspatialsignalintegrationandcontrolaccuracy. However,thisapproachimpactstherealism
of the motion. Inspired by these insights, we designed a transformer-based diffusion model that
incorporatesefficientcontrolsignalprocessing,acustomizedattentionmechanism,androbustjoint
loss propagation. This configuration achieves an optimal balance between high-quality motion
generationandprecisemodelcontrolcapabilities.
3 Methodology
AsillustratedinFigure2,ourinnovativeframework,CrowdMoGen,enablestheflexiblecreation
of crowd scenes customized to diverse requirements. The structure of the following subsections
is as follows: Section 3.1 outlines the architecture and key functionalities of CrowdMoGen. In
Section3.2,wedetailthedesignandfunctionofourCrowdScenePlanner,whichconvertsdiverse
crowdscenerequirementsintounifiedcontrolsignals. Finally,Section3.3exploreshowthesecontrol
signalsareutilizedtogeneratecontrollablehumanmotionsusingourCollectiveMotionGenerator,
therebysynthesizingrealisticcrowdscenes.
3.1 FrameworkOverview
Problem Defnition. We define the task of Crowd Motion Generation as the creation of crowd
motionsinresponsetotextualscenedescriptions. Formally,givenatextualscenedescriptionT
scene
andaspecifiedtotalnumberofpeopleN,theobjectiveistogenerateacrowdmotionΘnotonly
alignswiththetextualscenedescriptionT butalsoappearsnaturalandcoherent. Inthiscontext,
scene
Θ∈RN×F×D definesthegeneratedcrowdmotion,whereF isthenumberofmotionframesandD
isthedimensionofindividualmotionrepresentationperframe.
OverallFramework. Duetothecomplexityofcrowdmotioninbothsemanticandspatialaspects,
thistaskrequiresstrategicmotionplanningandadvancedcontrolmechanismstoensurequalified
generationresults. Intheinitialstageofourframework,theextensivepriorknowledgeofcrowd
motions from GPT-4 enables the development of our Crowd Scene Planner P. This planner
addressestheinherentuncertaintiesincrowdscenariosbygeneratingdetailed,stochasticexecution
plansC =(Cs,Ct)forthecrowdbasedonscenedescriptionsT .Specifically,Cs ∈RN×F×3
plan scene
outlines the spatial joint and path plans for individuals, formatted in a global keypoint motion
3Figure2: OverviewofCrowdMoGen. TheCrowdMoGenframeworkcomprisestwomaincom-
ponents: 1)CrowdScenePlanner,whichusesaLargeLanguageModel(LLM)tointerpretand
arrangecrowdmotionsbasedontextualrequirementsfromtheuser. Thiscomponentthenprovides
unifiedcontrolsignalsinbothtextualandspatialformats. 2)CollectiveMotionGenerator,which
leveragesthesecontrolsignalstomanipulateandgeneraterealisticindividualmotions.
representation. Concurrently,Ct,n∈[1,N]providessemanticmotiondescriptionstoguideeach
n
individual. Usingatree-of-thoughtapproachenhancestheplanner’sabilitytosystematicallyorganize
crowddynamics. Forinstance,giventhedescriptionT as"agroupofpeopleonaplayground"
scene
withaspecificnumberofpeopleN,P organizesthecrowdintointeractiongroups,assignsrelevant
activitieslikedancing,exercising,orwalking,andimposesspatialconstraintsforgrouppositioning,
movementtrajectory,andjointinteractions. TheoutputplanintegratestextualmotiondescriptionsCt
withspatialguidelinesCs,formingcomprehensivecontrolsignalsforeachindividual’smovement. In
thesecondstage,aCollectiveMotionGeneratorG takesoverandaccuratelygeneratesindividual
motionsbasedonthecrowdmotioncontrolplanC = (Cs,Ct), therebygeneratingthecrowd
plan
motionthatpreciselyconformtoscenerequirementT inazero-shotmanner.
scene
3.2 CrowdScenePlanner
Inthissection,wedetailtheCrowdScenePlannermodule,whichiscapableofbothmacroscopic
andmicroscopiccontrolovercrowdmotions,asillustratedinFigure2.
SceneParameters. Inbothcontrolmodes,weestablishspecificscenehyper-parameters,C ,
params
to determine the initial rough layout of the scene. These parameters include the total number of
individualsN, crowddensityP, averagegroupsizeG, andtheintensityofcrowdinteractionsI.
Averagegroupsizeindicateswhetherindividualstendtoformlargerorsmallerinteractiongroups,
whiletheintensityofcrowdinteractionsmeasuresthelevelofphysicalcontact,rangingfromstrong
toweak. Usershavetheoptiontomanuallysettheseparametersorallowtheplannertoautomatically
adjustthemaccordingtotheprovidedscenedescriptions.
MacroscopicControl. Effectivemacroscopiccontroliscrucialforaccuratelysimulatingvividand
dynamic crowd motions. We initiate the process by inputting a textual scene description T
scene
containingthecurrentstateofthecrowd,C ,andanyperturbations,P . BasedonthisT ,the
t t scene
plannerP generatesaplanC =(Cs,Ct)toguidetheevolutionofsubsequentcrowdstates. We
plan
haveidentifiedsixprincipalcrowdresponsepatternstovariousdisturbances: 1)Following,where
the crowd follows a leader; 2) Avoiding, involving the crowd dodging a fast-moving vehicle; 3)
Queuing,withthecrowdforminganeat,orderlyline; 4)Encircling,involvingthecrowdgathers
aroundaninterestingobject;5)Passing,withthecrowdmakingtemporaryadjustmentstonavigate
aroundadisturbancebeforeresumingoriginalactivities;6)Random,characterizedbytheabsenceof
clearcollectivemovement. LeveragingGPT-4’scapabilitiesandembeddedcrowdknowledge,our
plannerselectsanappropriatecrowdresponsepatterntoP ,updatesthecurrentmotionsforaffected
t
individuals,andappliespathchangealgorithmsbasedontheresponsepatterntoadjusttheirpositions,
asillustratedinFigure3(a).
4MicroscopicControl. Multi-personcloseinteractionsincrowdscenesrequireamorerefinedcontrol
mechanism.GivenascenecontextT ,theplannerP ispromptedtoorchestratethecrowd’smove-
scene
ments. Utilizingatreeofthoughtapproach,theplannerP systematicallystructuresthescenesetup
intwostages: (1)SceneLevel: Initially,theplannergeneratesallpotentialmotionsapplicabletothe
contextT . Eachmotionisrepresentedasatuple(motion,possibility,interaction_intensity,
scene
number_of_participants), detailing the motion type, its likelihood of occurrence, the intensity
of interaction, and the required number of participants. Leveraging predefined scene parameters
C ,theplannerthenselectsanappropriatesetoffinalmotionsMspecifictothecontextT .
params scene
(2) Group Level: For each motion in M, the
plannerPestablisheskeyframesandissuescom-
prehensivesemanticandspatialplansforeach
individual’smotions,positions,andjointinter-
actions. Theseplansencompassdetailedtextual
descriptionsofmotions,quantifiedpositionsfor
movement,andspecifieddistancesbetweenindi-
viduals’joints. Finally,theplannerP efficiently
coordinatesthesevariousinteractiongroupsinto
logicallypositionedsegments,consideringtheir
participantnumbersandspatialrequirements,as
showninFigure3(b).
Unified Format. To facilitate seamless inte-
grationwiththemotiongenerationprocess,we Figure3: MacroandMicroControl. TheCrowd
standardizecontrolsignalsfrombothmodesinto ScenePlannerisabletodealwithcrowdmotions
semantic and spatial components as C = effectivelyatboththesceneandmotionlevels. It
plan
(Cs,Ct). Semanticsignalsconsistoftextualmo- managesbothmacroscopiccrowddynamicsand
tiondescriptions,whereasspatialsignalsman- microscopicdetailedindividualinteractions,ensur-
ageindividualtrajectoriesandjointpositionsat ingrealisticandcoherentcrowdscenarios.
specificframes,asshowninFigure2.
3.3 CollectiveMotionGenerator
Toenhancespatialcontrollability,previousworks[59,61]utilizeControlNet[67]toachievespatial
control. However,wearguethatControlNetdoesnotconsiderthepropertiesofmotionrepresentation,
leadingtounsatisfactorycontrollability. First,thespatialcontrolsignalsinmotiongenerationare
moreprecisethanthatinimagegeneration. Thesynthesizedtrajectoryshouldbeassameasthe
givensignal,leadingtoadifferentoptimalarchitecturedesign. Second,thespatialcontrolsignalis
giveninthejoint-specificformat. Thus,introducingtheconceptofjoint-independentmodelingis
beneficialtocapturefeaturesfromspatialcontrolsignals. InourproposedCrowdMoGen,adiffusion
model-basedmotiongenerationmodel,wecarefullydesign1)InputMixingtechniquetoeffectively
injectcsintotheinputnoisedsequencex withinajoint-wisemanner,2)ControlAttentionmoduleto
t
adaptivelyrefinemotionfeaturebasedonthegivenspatialcontrolsignalsand3)trainingobjectives
thatdrivethemodeltobetteralignwiththegivencontrol.
MotionDiffusionModel. FollowingapproacheslikeMDM[57]andFineMoGen[71],weestablish
ournetworkbasedontheoff-the-shelfdiffusionmodel[18].Atransformer-basedbackboneisapplied
torecoverthecleanmotionsequenceS(x ,t,c={ct,cs})fromnoisedsequencex ,withadditional
t t
textual control signal ct and spatial control signal cs. The raw noised motion sequence x and
t
providedspatialcontrolsignalcs arefirstprocessedbyourproposedInputMixingtechnique. The
mixedmotionfeaturesarethenpassedbyseveraltransformerlayers. Similartothedesignsinthe
literature[69,6],eachlayerincludesanattentionmodule,anFFNmodule,andstylizationblocks.
InputMixing. During training, cs ∈ RF×J×3 and a spatial control mask M ∈ {0,1}F×J are
i
provided to indicate the 3D trajectory of the controlled keypoints, where F denotes the number
offramesandJ representsthenumberofjoints. Toextractjoint-wiseinformation,weintroduce
twojoint-wiseencodersets{Es},{Em}. Theoutputisformattedass = M Es(cs )+(1−
j j i,j i,j j i,j
M )ctemp+Em((x ) )foreachframeiandeachjointj,wherectemp ∈RF×J×Listhelearnable
i,j i,j j t i,j
jointtemplate,ensuringthatbothcontrolledanduncontrolledjointsareaccountedforinarational
manner. Listhedimensionoflatentfeatureforeachjoint.
5Table1: ControllablemotiongenerationquantitativeresultsonHumanML3Dtestset. ‘↑’(‘↓’)
indicatesthatthevaluesarebetterifthemetricislarger(smaller). ‘→’meansclosertorealdatais
better. Thebestresultareinbold.
R-precision Traj.err. Loc.err. Avg.err.
Method Joint FID↓ Diversity→ Footskatingratio↓
(Top-3)↑ (50cm)↓ (50cm)↓ (m)↓
Real 0.002 0.797 9.503 0.000 0.000 0.000 0.000
PriorMDM[48] 0.475 0.583 9.156 0.0897 0.3457 0.2132 0.4417
GMD[23] 0.576 0.665 9.206 0.109 0.0931 0.0321 0.1439
Pelvis
OmniControl[61] 0.218 0.687 9.422 0.0547 0.0387 0.0096 0.0338
InterControl[59] 0.159 0.671 9.482 0.0729 0.0132 0.0004 0.0496
OursCrowdMoGen 0.132 0.784 9.109 0.0762 0.0000 0.0000 0.0196
OmniControl[61] 0.310 0.693 9.502 0.0608 0.0617 0.0107 0.0404
InterControl[59] RandomOne 0.178 0.669 9.498 0.0968 0.0403 0.0031 0.0741
OursCrowdMoGen 0.147 0.781 9.461 0.0829 0.0000 0.0000 0.0028
InterControl[59] 0.184 0.670 9.410 0.0948 0.0475 0.0030 0.0911
RandomTwo
OursCrowdMoGen 0.178 0.777 9.149 0.0865 0.0000 0.0000 0.0027
InterControl[59] 0.199 0.673 9.352 0.0930 0.0487 0.0026 0.0969
RandomThree
OursCrowdMoGen 0.192 0.778 9.169 0.0871 0.0000 0.0000 0.0030
ControlAttention. EfficientAttention[49,68]moduleanditsvariantsSMA[69],DSMA[6]and
SAMI[71]canaggregateglobalinformationeffectively,enhancingsemanticunderstandingofmotion
sequencesbeyondclassicalself-attentionmechanisms. However,itsfocusonglobalinformation
often complicates the realization of sparse and dynamic spatial control. The pattern of spatial
controlsignaliscriticalbutignoredinthesedesigns. Toaddressthis,weupgradethearchitecture
ofEfficientAttentionandintroduceanewattentionmechanismControlAttention. Specifically,we
havetwomodifications: First,considerthenatureofmulti-headattentionmechanismandourneeds
forjoint-wisemodelling,wesetthenumberofattentionheadstobesameasthenumberofjointsJ.
Thus,eachheadcanfocusmoreonthecorrespondingjointandbetterutilizethegivenjoint-specific
controlsignal;Second,ControlAttentionincorporatesadditionallearnablespatialembeddingsintothe
attentionmechanismtobettercapturespatialguidance. WedefinetheseembeddingsfortheQueryQ
andValueVvectorsasfollows:
emb =embmaskM +embcontrol(1−M),
Q Q Q
(1)
emb =embmaskM +embcontrol(1−M).
V V V
Inthisformulation,embmask ∈RF,J,Landembmask ∈RF,J,Lareusedtolearnandintegratespatial
Q V
informationfromuncontrolledjoints,whereasembcontrol andembcontrol focusoncontrolledjoints.
Q V
Therefore,thecombinedembeddingsemb andemb encapsulatecomprehensivespatialdetailsfor
Q V
bothactiveandinactivejoints,andarethenaddedtothemoduleinputandutilizedforcomputing
theQandV vectors. OurControlAttentionthusenhancesthemodel’sabilitytomanagejointstatus
flexiblyandaccurately.
TrainingObjectives. Duringtraining,ourmethoddivergesfromtraditionalapproaches[61,59]that
typicallycomputethemeansquareerror(MSE)solelybasedonthewholebodymotionxˆ,presented
0
inarelativemotionformat. Toenhancemotionprecisionandthenaturalnessoftheanimations,we
extendthecommonlyusedWholeBodyLossL byincorporatingtwoadditionallosscomponents:
whole
ControlledKeypointLossL andFootSkatingLossL . TheControlledKeypointLosscomputes
con foot
the3-dimensionalEuclideandistancebetweencontrolledkeypointsandtheircorrespondingglobal
signals. TheFootSkatingLossassessestheslidingofthefootjoint,supervisingthe3-dimensional
Euclideandistancebetweenthefootjointsintwoconsecutiveframesiftheirheightsarebelowa
certainthresholdh . Specifically,wetransformtheoutputpredictionsxˆ ∈RF×D intoaglobal
thresh 0
3Dkeypointrepresentationxˆg ∈RF×J×3. WethencomputetheControlledKeypointLossL and
0 con
FootSkatingLossL asfollows:
foot
 
L con =M∥cs−xˆg 0∥ 2,L foot =F (cid:88)−1  (cid:88) (cid:16) (xˆg 0) i,j,2 <h thresh(cid:17) ·(cid:13) (cid:13) (cid:13)(xˆg 0) i+1,j−(xˆg 0) i,j(cid:13) (cid:13) (cid:13) . (2)
2
i=1 j∈{jleft,jright}
Here, M ∈ RF×D aggregates all M , representing the control status across the entire motion
i
sequence. Theindicesj andj denotetheleftandrightfootkeypoints,respectively. Theoverall
left right
trainingobjectivescanbeformulatedas:
L=λ ·L +λ ·L +λ ·L , (3)
whole whole con con foot foot
6Table2: Text-to-motionevaluationonKIL-ML Table 3: Text-to-motion evaluation on Hu-
testset.‘↑’(‘↓’)indicatesthatthevaluesarebetter manML3Dtestset. Thebestresultsareinbold.
ifthemetricislarger(smaller). ‘→’meanscloser
torealdataisbetter. Thebestresultsareinbold.
R-precision
Method FID↓ Diversity→
R-precision (Top-3)↑
Method FID↓ Diversity→
(Top-3)↑
Real 0.002 0.797 9.503
Real 0.031 0.779 11.08
T2M[14] 1.067 0.740 9.188
T2M[14] 3.022 0.681 10.72 MotionDiffuse[68] 0.630 0.782 9.410
MotionDiffuse[68] 1.954 0.739 11.10 MLD[8] 0.473 0.772 9.724
MLD[8] 0.404 0.734 10.80 PhysDiff[65] 0.413 0.631 -
T2M-GPT[66] 0.514 0.745 10.92 T2M-GPT[66] 0.116 0.775 9.761
MotionGPT[21] 0.510 0.680 10.35 MotionGPT[21] 0.232 0.778 9.528
MDM[57] 0.497 0.396 10.84 MDM[57] 0.544 0.611 9.446
PriorMDM[48] 0.830 0.397 10.54 PriorMDM[48] 0.540 0.640 9.160
GMD[23] 1.537 0.385 9.78 GMD[23] 0.212 0.670 9.440
OmniControl[61] 0.702 0.397 10.93 OmniControl[61] 0.218 0.687 9.422
InterControl[59] 0.580 0.397 10.88 InterControl[59] 0.159 0.671 9.482
OursCrowdMoGen 0.217 0.777 10.33 OursCrowdMoGen 0.132 0.784 9.109
whereλ ,λ ,λ arehyper-parameters.
whole con foot
InferenceStrategies. Duringinferencestage,weapplya50-stepdenoisingprocesstosynthesize
motionsequenceswhileusingclassfier-freeguidanceforbettergenerationquality. Inaddition,we
useIKguidance[59]inthelastiterationsforbetterspatialalignment.
4 Experiments
4.1 DatasetsandMetrics
Datasets. WeconductexperimentsontheHumanML3D[14]andKIT-ML[45]datasetstoevaluate
the generation and control capabilities of the proposed Collective Motion Generator. The Hu-
manML3Ddataset,areannotatedcombinationoftheHumanAct12[15]andAMASS[36]datasets,
comprises 14,616 motions and 44,970 textual descriptions. The KIT Motion Language Dataset
includes3,911motionsaccompaniedby6,363naturallanguagedescriptions.
EvaluationMetrics. Following[14],weuseFrechetInceptionDistance(FID),R-Precision,and
Diversity to evaluate the quality of generated motions, similarity to the textual descriptions, and
generationvariability,respectively. FollowingOmniControl[61],weemployFootSkatingRatio,
TrajectoryError,LocationError,andAverageErrortoevaluatethecontrolaccuracyoftheproposed
CollectiveMotionGenerator.
4.2 ImplementationDetails
Weconstructa4-layertransformerasthemotiondecoder. Forthetextencoder,weinitiallyusethe
CLIPViT-B/32textencoderandthenaddtwoadditionaltransformerencoderlayers. Boththetext
encoderandthemotiondecoderhavealatentdimensionof512. λ ,λ ,λ areallsetto1.0.
whole con foot
Thediffusionmodelemploys1000diffusionsteps,withvariancesβ linearlyrangingfrom0.0001to
t
0.02. TrainingisconductedoneightTeslaV100GPUs,eachprocessing64samples,resultingina
totalbatchsizeof512. Thetotalnumberofiterationsisaround40,000forKIT-MLand100,000for
HumanML3D.WeutilizetheAdamoptimizertotrainthemodelwithaninitiallearningrateof2e−4
anddecreaseitto2e−5inthelast20%iterations.
4.3 QuantitativeResults
Table 1 shows the quantitative comparison about spatial controllability. Our Collective Motion
Generator(CMG)hasbettermotionquality(FID),text-motionconsistency(R-precision),andspatial
error. Wealsowanttohighlightthatthetrajectoryofourgeneratedmotionsequencesarealmostas
sameasthegivenspatialcondition. TheseresultsdemonstrateCMG’sabilitytoproducerealistic
andtext-alignedmotionsequencesthatmeetspecificspatialconditions. Inaddition,wecompare
7Table4: AblationstudiesontheHumanML3Ddataset. ‘↑’(‘↓’)indicatesthatthevaluesarebetter
ifthemetricislarger(smaller). ‘→’meansclosertorealdataisbetter. Thebestresultsareinbold.
R-precision Traj.err. Loc.err. Avg.err.
Item Method FID↓ Diversity→ Footskatingratio↓
(Top-3)↑ (20cm)↓ (20cm)↓ (m)↓
(1) CrowdMoGen 0.132 0.784 9.109 0.0762 0.0051 0.0000 0.0196
(2) w/oControlAttention 0.150 0.775 9.446 0.1037 0.0069 0.0001 0.0223
(3) w/oInputMixing 0.207 0.771 9.090 0.0919 0.0159 0.0003 0.0246
(4) w/oJointLoss 0.219 0.761 9.143 0.1316 0.0174 0.0004 0.0254
(5) w/oSkatingLoss 0.125 0.777 9.244 0.1947 0.0058 0.0001 0.0147
(6) w/oIKguidance 0.130 0.782 9.320 0.0751 0.5710 0.2720 0.2387
(7) ControlNet 0.225 0.763 9.814 0.0915 0.0210 0.0005 0.0427
ourgeneratedresultstoexistingtext-to-motionmethodsandfindthatCMGperformscompetitively,
despitenotbeingspecificallydesignedforthistask(seeTables2and3).
UserStudy. Consideringthechallengesinevaluat-
ing text consistency and crowd motion quality, we
GPT 4 TM. Con. GPT 4 M. Qual.
conductedauserstudytoassessCrowdScenePlan- 100 Ours TM. Con. Ours M. Qual.
ner. Weinvited50participants,includinganimators, 90
80
AIresearchers,andgamingenthusiasts,aged20to 70
35. They were provided with textual descriptions 60
50
andaskedtocomparecrowdmotionsplannedbyour
40
moduleversusthosegeneratedbytheoriginalGPT- 30
4, focusing on motion quality and text consistency. 20
10
TheoriginalGPT-4,withoutspecializedpromptengi-
0
neering,strugglestoaccuratelyintegratejointinterac- Scene 1 Scene 2 Scene 3 Scene 4 Scene 5 Scene 6 Scene 7 Scene 8
tions,globalpaths,andoverallplanning. Incontrast, Figure4: UserStudy: ComparativeAnaly-
ourmoduleemploysasophisticatedtree-of-thought sisofPlanningMethods. Thischartpresents
approachthatdividesplanningintomacroandmicro thecomparisonresultsbetweenourCrowd
levels,effectivelyaddressingthechallengesofseman- Scene Planner and plain GPT-4, based on
ticandspatialintegrationthatGPT-4faces.Asshown participant preferences for text-motion con-
inFigure4,ouruserstudyvalidatesthesuperiorper- sistency(TM.Con.) andmotionquality(M.
formanceofourplannerinproducingcoherentand Qual.). Thepercentagesreflecttheproportion
contextuallyaccuratecrowdmotionplans. ofparticipantswhofavoredeachmethod.
4.4 QualitativeResults
CrowdMotionGeneration. AsshowninFigure5(a)-(f),werandomlygeneratedthreegroupsof
crowdmotionsinvolvingmulti-personcloseinteractions(a)-(b),larger-scalecrowddynamicsunder
givenperturbations(c)-(d), andcomplexcrowdscenearrangements(e)-(f)usingCrowdMoGen.
Thegeneratedcrowd’shavenaturalandreasonablemotionsemantics,realisticdynamics,aswell
as accurate joint interactions. This illustrate that our proposed method excels at both macro and
micro-levelcontrolandgenerationforcrowdmotions.
4.5 AblationStudy
WepresenttheablationresultsinTable4. AlthoughIKguidanceisessentialforsatisfyingspatial
alignmentasshowninExperiment(6),thearchitecturedesignandlossdesignarealsoindispens-
able. For example, in Experiment (7), we attempt to use only the ControlNet structure and IK
guidance. Theresultsshowsignificantlypoorermotionqualityandspatialconsistencycomparedto
ourfinalCrowdMoGenalgorithm,whichstronglyprovethenecessityofourproposedcomponents.
Experiments(1)(2)(3)demonstratethatourproposedarchitecturaldesigneffectivelyincorporates
the given spatial signal into the motion transformer, enabling the model to generate high-quality
motionsequencesthatareconsistentwiththeprovidedtextualdescriptionandspatialcontrolsignals
simultaneously. TheeffectivenessoflossdesignarealsoprovedinExperiments(1)(4)(5),which
mainlyamelioratesthekeypointerrorandfootskatingratiorespectively.
4.6 LimitationsandPotentialSocietalImpacts
Limitations. OurCrowdScenePlannerdependsonthecapabilitiesoftheLargeLanguageModel
(LLM),whichmaynotalwaysaccuratelypredictcomplexorrarecrowdscenarios. Additionally,
8
)%(
egatnecreP(a) A person fell down, other people come to help him get up.
(b) Three people greet a person running over.
(c) A crowd walks forward, and a car drives into them from the side.
(d) A person waves hands to call the crowd to gather.
(e) People walking on a busy street.
(f) People playing on the playground.
Figure 5: Qualitative Visualizations. Displayed are selected frames from the crowd motion se-
quencesgeneratedbyourproposedCrowdMoGen. Iteffectivelycreatesscenariosinvolvingmulti-
person close interactions (a)-(b), dynamic crowd movements (c)-(d), and complex crowd scenes
(e)-(f)thataccuratelyandnaturallyreflectthespecifiedscenedescriptions.
theCollectiveMotionGeneratormayexperienceconflictsfrommultiplecontrolsignals. These
challengeshighlighttheneedforfurtherenhancementsinmodelingcrowddynamics.
PotentialSocialImpacts. TheproposedCrowdMoGenmayoffersignificantbenefitsbyenhancing
realismandinteractivityinvirtualenvironmentsforentertainmentandurbanplanning. However,it
canalsobemisusedtofabricatedeceptivecrowdscenesforsimulationsorentertainment,potentially
misrepresentingpubliceventsorinfluencingopinions.
5 Conclusion
In this work, we introduce and explore the novel task of Crowd Motion Generation, aimed at
realisticallyproducingcollectivehumanmotionstailoredtospecificscenesettings. Moreover,we
alsoproposeCrowdMoGen,agroundbreaking,zero-shot,text-drivenframeworkforcrowdmotion
generation. Thisframeworkutilizestheoff-the-shelfLargeLanguageModelalongsideacarefully
designedmotiongenerationmodelthatallowsforfine-grainedspatialandsemanticcontrol,enabling
the autonomous and efficient generation of versatile crowd dynamics. Our innovative approach
providesfreshinsightsintohumanmotiongenerationandwouldbeagoodinspirationforfuture
developmentsinvariousindustriessuchasanimation,gaming,andurbanplanning.
6 Acknowledegment
This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2
(MOET2EP20221-0012),NTUNAP,andundertheRIE2020IndustryAlignmentFund–Industry
CollaborationProjects(IAF-ICP)FundingInitiative,aswellascashandin-kindcontributionfrom
theindustrypartner(s).
9References
[1] HyeminAhnandDongheuiMascaro,VallsEsteveanLee. Canweusediffusionprobabilistic
modelsfor3dmotionprediction? In2023IEEEInternationalConferenceonRoboticsand
Automation(ICRA),May2023.
[2] HyeminAhn,EsteveVallsMascaro,andDongheuiLee. Canweusediffusionprobabilistic
modelsfor3dmotionprediction? arXivpreprintarXiv:2302.14503,2023.
[3] NikosAthanasiou,MathisPetrovich,MichaelJBlack,andGülVarol. Teach: Temporalaction
composition for 3d humans. In 2022 International Conference on 3D Vision (3DV), pages
414–423.IEEE,2022.
[4] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for
behavior-driven human motion prediction. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages2317–2327,2023.
[5] Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan: Probabilistic 3d human motion
predictionviagan. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognitionworkshops,pages1418–1427,2018.
[6] ZhongangCai,JianpingJiang,ZhongfeiQing,XinyingGuo,MingyuanZhang,ZhengyuLin,
Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, et al. Digital life project: Autonomous 3d
characterswithsocialintelligence. arXivpreprintarXiv:2312.04547,2023.
[7] Ling-HaoChen,JiaweiZhang,YewenLi,YirenPang,XiaoboXia,andTongliangLiu. Human-
mac: Maskedmotioncompletionforhumanmotionprediction. 2023.
[8] XinChen,BiaoJiang,WenLiu,ZilongHuang,BinFu,TaoChen,andGangYu. Executingyour
commandsviamotiondiffusioninlatentspace. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18000–18010,2023.
[9] BaptisteChopin,HaoTang,andMohamedDaoudi. Bipartitegraphdiffusionmodelforhuman
interactiongeneration. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
ComputerVision,pages5333–5342,2024.
[10] ChristianDillerandAngelaDai. Cg-hoi: Contact-guided3dhuman-objectinteractiongenera-
tion. arXivpreprintarXiv:2311.16097,2023.
[11] ChristianDiller,ThomasFunkhouser,andAngelaDai. Forecastingcharacteristic3dposesof
humanactions. 2022.
[12] KehongGong,DongzeLian,HengChang,ChuanGuo,ZihangJiang,XinxinZuo,MichaelBi
Mi,andXinchaoWang.Tm2d:Bimodalitydriven3ddancegenerationviamusic-textintegration.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages9942–
9952,2023.
[13] AnandGopalakrishnan,AnkurMali,DanKifer,LeeGiles,andAlexanderGOrorbia. Aneural
temporalmodelforhumanmotionprediction. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages12116–12125,2019.
[14] ChuanGuo,ShihaoZou,XinxinZuo,SenWang,WeiJi,XingyuLi,andLiCheng. Generating
diverseandnatural3dhumanmotionsfromtext. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages5152–5161,2022.
[15] ChuanGuo,XinxinZuo,SenWang,ShihaoZou,QingyaoSun,AnnanDeng,MinglunGong,
andLiCheng. Action2motion: Conditionedgenerationof3dhumanmotions. InProceedings
ofthe28thACMInternationalConferenceonMultimedia,pages2021–2029,2020.
[16] WenGuo,YumingDu,XiShen,VincentLepetit,XavierAlameda-Pineda,andFrancescMoreno-
Noguer. Backtomlp: Asimplebaselineforhumanmotionprediction. InProceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofComputerVision,pages4809–4819,2023.
[17] YuzeHao,JianrongZhang,TaoZhuo,FuanWen,andHeheFan.Hand-centricmotionrefinement
for 3d hand-object interaction via hierarchical spatial-temporal modeling. arXiv preprint
arXiv:2401.15987,2024.
[18] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inNeuralInformationProcessingSystems,33:6840–6851,2020.
10[19] NhatMHoang,KehongGong,ChuanGuo,andMichaelBiMi. Motionmix:Weakly-supervised
diffusionforcontrollablemotiongeneration. arXivpreprintarXiv:2401.11115,2024.
[20] SiyuanHuang,ZanWang,PuhaoLi,BaoxiongJia,TengyuLiu,YixinZhu,WeiLiang,and
Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
16750–16761,2023.
[21] BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt: Humanmotion
asaforeignlanguage. AdvancesinNeuralInformationProcessingSystems,36,2024.
[22] ChiyuJiang,AndreCornman,CheolhoPark,BenjaminSapp,YinZhou,DragomirAnguelov,
et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pages
9644–9653,2023.
[23] KorraweKarunratanakul,KonpatPreechakul,SupasornSuwajanakorn,andSiyuTang. Guided
motiondiffusionforcontrollablehumanmotionsynthesis. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages2151–2162,2023.
[24] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion
synthesis&editing.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,
pages8255–8263,2023.
[25] SumithKulal,JiayuanMao,AlexAiken,andJiajunWu. Programmaticconceptlearningfor
human motion description and synthesis. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages13843–13852,2022.
[26] JiamanLi, AlexanderClegg, RoozbehMottaghi, JiajunWu, XavierPuig, andCKarenLiu.
Controllablehuman-objectinteractionsynthesis. arXivpreprintarXiv:2312.03913,2023.
[27] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music
conditioned3ddancegenerationwithaist++. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages13401–13412,2021.
[28] HanLiang,WenqianZhang,WenxuanLi,JingyiYu,andLanXu. Intergen: Diffusion-based
multi-humanmotiongenerationundercomplexinteractions. arXivpreprintarXiv:2304.05684,
2023.
[29] DonggeunLim,CheongiJeong,andYoungMinKim. Mammos: Mappingmultiplehuman
motionwithsceneunderstandingandnaturalinteractions. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages4278–4287,2023.
[30] JunfanLin,JianlongChang,LingboLiu,GuanbinLi,LiangLin,QiTian,andChang-wenChen.
Ohmg: Zero-shotopen-vocabularyhumanmotiongeneration. arXivpreprintarXiv:2210.15929,
2022.
[31] Pei Lin, Sihang Xu, Hongdi Yang, Yiran Liu, Xin Chen, Jingya Wang, Jingyi Yu, and Lan
Xu. Handdiffuse: Generativecontrollersfortwo-handinteractionsviadiffusionmodels. arXiv
preprintarXiv:2312.04867,2023.
[32] JunLiu,AmirShahroudy,MauricioPerez,GangWang,Ling-YuDuan,andAlexCKot. Ntu
rgb+d120: Alarge-scalebenchmarkfor3dhumanactivityunderstanding. IEEEtransactions
onpatternanalysisandmachineintelligence,42(10):2684–2701,2019.
[33] XinpengLiu,HaowenHou,YanchaoYang,Yong-LuLi,andCewuLu. Revisithuman-scene
interactionviaspaceoccupancy. arXivpreprintarXiv:2312.02700,2023.
[34] YunhongLou,LinchaoZhu,YaxiongWang,XiaohanWang,andYiYang. Diversemotion: To-
wardsdiversehumanmotiongenerationviadiscretediffusion.arXivpreprintarXiv:2309.01372,
2023.
[35] ShunlinLu,Ling-HaoChen,AilingZeng,JingLin,RuimaoZhang,LeiZhang,andHeung-
Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint
arXiv:2310.12978,2023.
[36] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J
Black. Amass: Archiveofmotioncaptureassurfaceshapes. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages5442–5451,2019.
11[37] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History repeats itself: Human motion
predictionviamotionattention. InComputerVision–ECCV2020: 16thEuropeanConference,
Glasgow,UK,August23–28,2020,Proceedings,PartXIV16,pages474–489.Springer,2020.
[38] DushyantMehta,HelgeRhodin,DanCasas,PascalFua,OleksandrSotnychenko,WeipengXu,
andChristianTheobalt. Monocular3dhumanposeestimationinthewildusingimprovedcnn
supervision. In3DVision(3DV),2017FifthInternationalConferenceon.IEEE,2017.
[39] MikiOkamura,NaruyaKondo,TatsukiFushimiMakiSakamoto,andYoichiOchiai. Dance
generationbysoundsymbolicwords. arXivpreprintarXiv:2306.03646,2023.
[40] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang.
Hoi-diff: Text-drivensynthesisof3dhuman-objectinteractionsusingdiffusionmodels. arXiv
preprintarXiv:2312.06553,2023.
[41] Mathis Petrovich, Michael J Black, and Gül Varol. Action-conditioned 3d human motion
synthesiswithtransformervae. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages10985–10995,2021.
[42] MathisPetrovich,MichaelJBlack,andGülVarol. Temos: Generatingdiversehumanmotions
from textual descriptions. In European Conference on Computer Vision, pages 480–497.
Springer,2022.
[43] MathisPetrovich,OrLitany,UmarIqbal,MichaelJBlack,GülVarol,XueBinPeng,andDavis
Rempe.Multi-tracktimelinecontrolfortext-driven3dhumanmotiongeneration.arXivpreprint
arXiv:2401.08559,2024.
[44] HuaijinPi,SidaPeng,MinghuiYang,XiaoweiZhou,andHujunBao.Hierarchicalgenerationof
human-objectinteractionswithdiffusionprobabilisticmodels. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages15061–15073,2023.
[45] MatthiasPlappert,ChristianMandery,andTamimAsfour. Thekitmotion-languagedataset.
Bigdata,4(4):236–252,2016.
[46] DavisRempe,ZhengyiLuo,XueBinPeng,YeYuan,KrisKitani,KarstenKreis,SanjaFidler,
andOrLitany.Traceandpace:Controllablepedestriananimationviaguidedtrajectorydiffusion.
InConferenceonComputerVisionandPatternRecognition(CVPR),2023.
[47] Jiawei Ren, Mingyuan Zhang, Cunjun Yu, Xiao Ma, Liang Pan, and Ziwei Liu. Insactor:
Instruction-drivenphysics-basedcharacters. AdvancesinNeuralInformationProcessingSys-
tems,36,2024.
[48] YonatanShafir,GuyTevet,RoyKapon,andAmitHBermano. Humanmotiondiffusionasa
generativeprior. arXivpreprintarXiv:2303.01418,2023.
[49] ZhuoranShen,MingyuanZhang,HaiyuZhao,ShuaiYi,andHongshengLi. Efficientattention:
Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on
applicationsofcomputervision,pages3531–3539,2021.
[50] XuShi,ChuanchenLuo,JunranPeng,HongwenZhang,andYunlianSun. Generatingfine-
grainedhumanmotionsusingchatgpt-refineddescriptions. arXivpreprintarXiv:2312.02772,
2023.
[51] SoshiShimada,FranziskaMueller,JanBednarik,BardiaDoosti,BerndBickel,DanhangTang,
Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, and Thabo Beeler. Macs: Mass
conditioned3dhandandobjectmotionsynthesis. arXivpreprintarXiv:2312.14929,2023.
[52] LiSiyao,WeijiangYu,TianpeiGu,ChunzeLin,QuanWang,ChenQian,ChenChangeLoy,
andZiweiLiu. Bailando: 3ddancegenerationbyactor-criticgptwithchoreographicmemory.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages11050–11059,2022.
[53] JiangxinSun,ZihangLin,XintongHan,Jian-FangHu,JiaXu,andWei-ShiZheng. Action-
guided 3d human motion prediction. Advances in Neural Information Processing Systems,
34:30169–30180,2021.
[54] Jiarui Sun and Girish Chowdhary. Towards globally consistent stochastic human motion
predictionviamotiondiffusion. arXivpreprintarXiv:2305.12554,2023.
[55] Purva Tendulkar, Dídac Surís, and Carl Vondrick. Flex: Full-body grasping without full-
bodygrasps. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages21179–21189,2023.
12[56] GuyTevet,BrianGordon,AmirHertz,AmitHBermano,andDanielCohen-Or. Motionclip:
Exposinghumanmotiongenerationtoclipspace. InEuropeanConferenceonComputerVision,
pages358–374.Springer,2022.
[57] GuyTevet,SigalRaab,BrianGordon,YoniShafir,DanielCohen-or,andAmitHaimBermano.
Humanmotiondiffusionmodel. InTheEleventhInternationalConferenceonLearningRepre-
sentations,2022.
[58] XinshunWang,QiongjieCui,ChenChen,andMengyuanLiu. Gcnext: Towardstheunityof
graphconvolutionsforhumanmotionprediction. arXivpreprintarXiv:2312.11850,2023.
[59] ZhenzhiWang,JingboWang,YixuanLi,DahuaLin,andBoDai. Intercontrol: Generatehuman
motioninteractionsbycontrollingeveryjoint,2024.
[60] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and
Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. arXiv
preprintarXiv:2309.07918,2023.
[61] YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang. Omnicontrol: Control
anyjointatanytimeforhumanmotiongeneration. arXivpreprintarXiv:2310.08580,2023.
[62] Haodong Yan, Zhiming Hu, Syn Schmitt, and Andreas Bulling. Gazemodiff: Gaze-guided
diffusion model for stochastic human motion prediction. arXiv preprint arXiv:2312.12090,
2023.
[63] SiyueYao,MingjieSun,BingliangLi,FengyuYang,JunleWang,andRuimaoZhang. Dance
withyou: Thediversitycontrollabledancergenerationviadiffusionmodels. InProceedingsof
the31stACMInternationalConferenceonMultimedia,pages8504–8514,2023.
[64] PayamJomeYazdian,EricLiu,LiCheng,andAngelicaLim. Motionscript: Naturallanguage
descriptionsforexpressive3dhumanmotions. arXivpreprintarXiv:2312.12634,2023.
[65] YeYuan,JiamingSong,UmarIqbal,ArashVahdat,andJanKautz. Physdiff: Physics-guided
humanmotiondiffusionmodel. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages16010–16021,2023.
[66] JianrongZhang,YangsongZhang,XiaodongCun,ShaoliHuang,YongZhang,HongweiZhao,
HongtaoLu,andXiShen. T2m-gpt: Generatinghumanmotionfromtextualdescriptionswith
discreterepresentations. arXivpreprintarXiv:2301.06052,2023.
[67] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels,2023.
[68] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,LeiYang,and
ZiweiLiu. Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. IEEE
TransactionsonPatternAnalysisandMachineIntelligence,2024.
[69] MingyuanZhang,XinyingGuo,LiangPan,ZhongangCai,FangzhouHong,HuirongLi,Lei
Yang,andZiweiLiu. Remodiffuse:Retrieval-augmentedmotiondiffusionmodel. InIEEE/CVF
InternationalConferenceonComputerVision,ICCV2023,Paris,France,October1-6,2023,
pages364–373,2023.
[70] MingyuanZhang,DaishengJin,ChenyangGu,FangzhouHong,ZhongangCai,JingfangHuang,
ChongzhiZhang,XinyingGuo,LeiYang,YingHe,andZiweiLiu. Largemotionmodelfor
unifiedmulti-modalmotiongeneration,2024.
[71] MingyuanZhang,HuirongLi,ZhongangCai,JiaweiRen,LeiYang,andZiweiLiu. Finemogen:
Fine-grainedspatio-temporalmotiongenerationandediting. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[72] Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and Nicu Sebe. Modiff: Action-
conditioned3dmotiongenerationwithdenoisingdiffusionprobabilisticmodels. arXivpreprint
arXiv:2301.03949,2023.
[73] WentaoZhu,JasonQin,YukeLou,HangYe,XiaoxuanMa,HaiCi,andYizhouWang. Social
motionpredictionwithcognitivehierarchies. InA.Oh,T.Naumann,A.Globerson,K.Saenko,
M.Hardt, andS.Levine, editors, AdvancesinNeuralInformationProcessingSystems, vol-
ume36,pages77675–77690.CurranAssociates,Inc.,2023.
[74] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia.
Music2dance: Dancenetformusic-drivendancegeneration. ACMTransactionsonMultimedia
Computing,Communications,andApplications(TOMM),18(2):1–21,2022.
13A DetailsofCrowdMoGen
A.1 DiffusionModel
(cid:82)
Diffusion models can be defined using a Markov chain p (x ) := p (x )dx , where the
θ 0 θ 0:T 1:T
intermediatevariablesx ,··· ,x representnoisedversionsoftheoriginaldatax ∼ q(x ),and
1 T 0 0
maintainsthesamedimensionalityastheoriginaldata. Inmotiongeneration,everyx correspondsto
t
amotionsequenceθ ∈RD,i=1,2,...,F,whereDdenotesthedimensionalityofeachpose,and
i
F representsthetotalnumberofframes. Inthediffusionprocessofdiffusionmodels, [18]efficiently
√ √
derive x from x by approximating q(x ) as x := α¯ x + 1−α¯ ϵ, where α := 1 − β ,
t 0 t t 0 t t t
α¯ :=(cid:81)t α ,andβ isapredefinedvariancescheduleatthetthnoisingstep. Then,thediffusion
t s=1 s t
processisreversedforcleanmotionrecovery.Specifically,amotiongenerationmodelSisconfigured
torefinenoisydatax backtoitsoriginal, cleanstatex . FollowingapproacheslikeMDM[57]
t 0
and ReMoDiffuse [69], we estimate x as S(x ,t,c), where c represents the conditioning signal
0 t
containingthespatialcontrolsignalcsandthetextualcontrolsignalct,andt∈U(0,T)marksthe
timestepinthediffusionprocess. Finallyinthesamplingphase,wederivex bysamplingitfrom
t−1
theGaussiandistributionN(µ (x ,t,c),β ),wherethemeanisspecifiedasfollows:
θ t t
√ √
µ (x ,t,c)= α¯ S(x ,t,c)+ 1−α¯ ϵ (x ,t,c),
θ t t t t θ t
x (cid:114) 1 (4)
ϵ (x ,t,c)=(√t −S(x ,t,c)) −1.
θ t α¯ t α¯
t t
A.2 HumanMotionRepresentation
Humanmotioncanbeparameterizedusingtwoprimarymethods: relativeorglobaljointrepresen-
tation. The relative representation, proposed in [14], is a redundant format that includes pelvis
velocity,localjointpositions,velocities,androtationsrelativetothepelvis,complementedbyfoot
contactbinarylabels. Thisrepresentationmethodsimplifiesthemodellearningprocessandsupports
thegenerationofnaturalhumanmotionsaligningwithinherenthumanbodypriors. However, it
faceschallengesinprecisespatialcontrol,especiallyinsparsetimeframesandwhencontrolling
jointsbeyondthepelvis,duetoitslackofastableglobalreference. Consequently,controllingmeth-
ods[57,23]usingthisformatstrugglewithconsistencyandprecisioninjoint-wisespatialcontrol
acrosstimesteps. Incontrast,globaljointrepresentationprovideseachjoint’sabsolutecoordinates
withinaglobalframework,offeringclearanddirectreferenceforfine-grainedcontrol. Therefore,
inspiredby[61], weemployahybridapproachtorepresentmotionsinourframework: utilizing
relativerepresentationfortrainingandroutinegeneration,andswitchingtoglobalpositioningfor
enhancedspatialcontrol.
A.3 IKGuidance
Noiseoptimizationduringthesamplingstageenhancesthespatialcontrolcapabilitiesofourmodel.
Indiffusionsteps,gradientdescentoptimizationisperformedontheoutputmeaninresponsetothe
inputspatialcontrolsignals. Specifically,D(µ (x ,t,c),cs)quantifiesthediscrepancybetweenthe
θ t
currentlygeneratedmeanµ (x ,t,c)andtheprovidedspatialsignalcs.Toensureprecisecontrol,we
θ t
canselectivelysupervisespecifictimeframesandjoints,whilemaskingunsupervisedcomponents.
(cid:80) (cid:80) O K(cs −µg )
D(µ (x ,t,c),c )= i j ij ij ij (5)
θ t s (cid:80) (cid:80) O
i j ij
Here, O isabooleanvaluerepresentingifatframei, jointj iscontrolled. Thepredictedmean
ij
istransformedintoaglobaljointrepresentationµg andassessedwiththecontrolsignalcs inthe
sameformatunderapre-definedlosscalculationfunctionK,suitablefordifferentcircumstances,
ensuringthatjointpositionsalignwiththespatialcontrolrequirementsfromaglobalperspectivein
bothtimeandlayout. Additionally,sincegradientupdatestargetthemeanx inarelativemotion
t
representation,theseupdatesarenaturallypropagatedacrossalltimeframesandjointsthroughout
themotionsequence. Thiscoherentpropagationfacilitatesanaturaladjustmentoftheentirebody’s
joints,evenwhenonlyafewjointsarebeingactivelycontrolledandedited.
14