Solving Zebra Puzzles
Using Constraint-Guided Multi-Agent Systems
Shmuel Berman Baishakhi Ray Kathleen McKeown
sb6870@princeton.edu rayb@cs.columbia.edu kathy@cs.columbia.edu
Abstract particularly difficult because translating natu-
ral language into a precise logical or computa-
Prior research has enhanced the ability of tional form requires sophisticated understand-
Large Language Models (LLMs) to solve
ing and interpretation, making it a significant
logicpuzzlesusingtechniquessuchaschain-
challenge in AI research.
of-thoughtpromptingorintroducingasym-
bolic representation. These frameworks are
stillusuallyinsufficienttosolvecomplicated
logicalproblems,suchasZebrapuzzles,due
House 1 House 2 House 3
to the inherent complexity of translating
Color
natural language clues into logical state-
ments. We introduce a multi-agent system, Nationality
ZPS, that integrates LLMs with an off the
Animal
shelf theorem prover. This system tackles
Sport
thecomplexpuzzle-solvingtaskbybreaking
downtheproblemintosmaller,manageable
Clues
parts, generating SMT (Satisfiability Mod-
1. The Brazilian does not live in house two.
uloTheories)codetosolvethemwithathe-
orem prover, and using feedback between 2. The person with the Dogs plays Basketball.
the agents to repeatedly improve their an- 3. There is one house between the person who
swers. We also introduce an automated plays Football and the Red house on the right.
grid puzzle grader to assess the correctness 4. The person with the Fishes lives directly to the left
of our puzzle solutions and show that the of the person with the Cats.
automated grader is reliable by evaluating 5. The person with the Dogs lives directly to
it in a user-study. Our approach shows the right of the Green house.
improvement in all three LLMs we tested, 6. The German lives in house three.
with GPT-4 showing 166% improvement in
the number of fully correct solutions.
Figure 1: An Example Zebra Puzzle.
1 Introduction
Automated problem solving has long been a Inthispaper,wefocusonaparticulartypeof
major goal in the field of Artificial Intelligence. unstructured natural language problem known
This task ranges from trivial problems, like as a logic grid problem, or colloquially, an Ein-
simple arithmetic or string searches, to more steinorZebrapuzzle. AZebrapuzzleisasetof
complex ones, such as solving a chess position.. natural language assertions involving multiple
However, unstructured problems presented in entities that are linked by various attributes
naturallanguageintroduceadditionalcomplica- (Fig. 1 shows an example). To solve a puzzle,
tions in modeling the problem accurately. Solv- the user must correctly assign attributes to all
ing such problems has been extensively studied, of the entities. These attributes range from
from simple mathematical problems in the sub- descriptions to relative ordering. Participants
field of word problem solving to applications are provided with a series of clues in natu-
like automated code generation by Large Lan- ral language, which they must use to deduce
guage Models (LLMs) (Mukherjee and Garain, the correct relationships using logical reasoning
2009; Chen et al., 2021). These problems are and by adhering to implicit domain constraints.
1
4202
luJ
4
]AM.sc[
1v65930.7042:viXraThese puzzles require the solver to map from for accuracy. Prior research using ChatGPT to
natural language to structured space, under- solve Zebra puzzles reported a correctness rate
stand implicit assumptions, and in some cases of only 8.33% (Groza, 2023), with performance
use domain-specific knowledge. For instance, deterioratingsignificantlyastheproblem’scom-
as illustrated in Fig. 1, the solver must assign plexity increases.
the correct attributes for three houses based on Due to their complexity, solving Zebra puz-
a series of interconnected clues. zles effectively requires the use of a constraint
Zebra puzzles are particularly challenging solver; a solver can efficiently determine the
due to: feasible and infeasible solution space within the
given constraints. However, converting natu-
• Complex Inferences: Each clue provides
ral language clues into a formal representation
partialinformationthatmustbecombined
suitable for a solver is a non-trivial task. This
with others to deduce the solution.
process often involves intricate interpretation
of clues, which must be precise to ensure that
• High Interdependency: An error on one
the solver can operate correctly. Additionally,
clue significantly impacts others, making
maintainingconsistencyacrossallcluesrequires
the solution space highly interconnected.
iterative back-and-forth reasoning.
• NaturalLanguage(NL)Clues: Translating To address the challenges inherent in solv-
ambigous NL clues into logical statements ing Zebra puzzles, we introduce a multi-agent
or formal representations is challenging. based system, ZPS. This system decomposes
the problem-solving process into discrete, man-
• Large Solution Space: The solver needs to
ageable components, enhancing the handling
explore numerous possibilities and combi-
of complex interdependencies and constraints.
nations to find the solution.
Each agent is responsible for a specific aspect
of the problem, working collaboratively and us-
• Consistency Checking: Potential solutions
ing feedback loops to refine their answers and
must be checked against all clues, which is
ensure consistency.
a computationally intensive and requires
In this framework, we conceptualize integrat-
sophisticated, domain-specific reasoning.
ing Large Language Models (LLMs) with for-
The factors mentioned make Zebra puzzles dif- mal reasoning. First, an LLM agent decompose
ficult for both humans and AI systems due to a given puzzle to sub-problems. Then, another
the need for precise interpretation, inference, LLM agent interprets NL clues of each sub-
and logical reasoning. In Fig. 1, for example, problem and generates SMT-LIB translations
a solver cannot simply map the spatial rela- of the constraints and parameters. An off-the-
tionship between the Football house and the shelf SMT solver 1 then processes these trans-
Red house. It must also encode additional con- lations to produce a model that corresponds to
straints: the Football and Red houses occupy the solution. The output, including the model
House1orHouse3,respectively,andtheymust andanysyntacticerrors,isfedbacktotheLLM
not be the same house. Encoding these con- which generates a new translation addressing
straints is non-trivial, as it requires detailed syntactic and semantic errors, emulating back
semantic interpretation of the clue’s subtext. and forth reasoning. This continuous feedback
Failure to accurately encode these subtleties refines the model’s predictions and ensures the
usually renders the puzzle unsolvable. translations are both syntactically correct and
This complexity has empirically been shown solvable. To this end, our approach demon-
to challenge puzzle-solving models significantly. strates improvements across all three LLMs we
Prior work often employed human-in-the-loop tested, with GPT-4 showing up to a 166% in-
methods. Milicevic et al. (2012) translated crease in the number of fully correct solutions.
puzzles into formal logic but required users to
1Satisfiability Modulo Theories (SMT) is a decision
rephrase or rewrite ambiguous clues. Claes
problem that involves determining whether a given
et al. (2019) developed ZebraTutor, which cre- logical formula is satisfiable, considering various back-
ates a puzzle-specific lexicon to formalize the ground theories like Arithmetic, Arrays, Bit-Vector,
etc. SMT extends the concept of Boolean satisfiability
problem but needed users to edit the lexicon
(SAT) by incorporating more complex theories.
2on feedback. This approach aims to leverage
the strengths of both LLMs and formal solvers,
ensuring robust problem-solving capabilities.
Decom-
Zebra Puzzles
position
2.1 Multi-Agent Workflow
The workflow, as illustrated in Figure 2, inte-
Generate
SMT-LIB Provide grates multiple agents to transform a natural
Feedback
with LLM
language puzzle into a logically solvable struc-
ture and then iteratively refines the solution.
The process is initiated by the Decomposition
Solve with
theorem Acceptable Answer Agent and continuously refined through a feed-
Solution?
prover
back loop that encompasses both the transla-
tion to SMT-LIB and the solving phases. Fig-
Figure 2: Logic Puzzle Solver Workflow ure3showsaworkingexampleofpuzzlesolving
by our method.
The main contributions of our research are Decomposition The input puzzle, expressed
as follows: in natural language, is first decomposed by the
Decomposition LLM-Agent. This agent identi-
1. We demonstrate that combining a formal
fies and isolates key entities, attributes, and re-
constraint solver with an LLM interpreter
lationships, structuring them into smaller, sys-
using an agent-based approach for solving
tematically translatable components. This is a
Zebra Puzzles significantly improves upon
first step that ensures the puzzle is presented
existing baseline methodologies.
in a format amenable to formal processing.
2. We implement a plan generation and de-
Feedback Loop The core of our methodol-
composition strategy, enabling step-by-
ogy lies in the feedback loop where continuous
step reasoning that enhances the solving
refinement of the solution occurs. This loop
process.
integrates the translation of decomposed com-
ponents into SMT-LIB format by the Solver
3. We introduce an iterative conversation-
LLM-Agent and the subsequent problem solv-
based feedback mechanism that allows for
ingusingatheoremprover,whoseoutputserves
continualrefinementofsolutions, adapting
as feedback. Each iteration through the loop
dynamically to the solving context.
consists of the following steps:
4. We incorporate an autograder within our • Translation to SMT-LIB: After decompo-
system to evaluate the accuracy of solu- sition, the puzzle components are systemat-
tions, ensuring reliability and precision in ically translated into SMT-LIB (Satisfiabil-
automated assessments. We also present ity Modulo Theories Library) by the Solver
the results of a user study showing that LLM-Agent. This format is essential for in-
this autograder correlates very well with terfacing with theorem provers and ensures
human graders. that logical constraints and relationships are
accurately represented.
Our code is available at https: • SolvingwithTheoremProver: TheSMT-
//anonymous.4open.science/r/anon_ LIB formatted components are then pro-
emnlp-1AD0/README.md.
cessed by the theorem prover (Z3 in our im-
plementation). Thetheoremproverattempts
2 Methodology
to find a satisfying assignment that adheres
We integrate LLMs with formal systems within to all given constraints.
a multi-agent framework to solve Zebra puzzles. • Evaluate and Refine: The solution gen-
The process involves a series of steps where erated by the theorem prover is evaluated
the problem is decomposed, translated into a by the Solver LLM-Agent to determine if it
formal language (SMT-LIB), solved using a meets the puzzle’s requirements. If the so-
theorem prover, and iteratively refined based lution is deemed insufficient– either due to
3Figure 3: Example Feedback Puzzle Solving Process. The puzzle is decomposed and then the LLM-agent
attempts to translate it into a logical SMT formula. The theorem prover attempts to solve it, and the
feedback is fed back into the LLM-agent so that it can modify its formal representation.
to explicit errors or because of how the at- T: appliesthetheoremprovertofindasolution
tributesareassigned–modificationsaremade that satisfies the logical constraints.
to the translation of the SMT-LIB formal- E: evaluates this solution to determine its ade-
ization of the puzzle and the cycle repeats. quacy in solving the puzzle’s clues.
Otherwise, the LLM-agent submits its final F: adjusts the translation based on the evalua-
answer. tion, aiming to correct any errors or optimize
This iterative process ensures that the Solver the solution.
LLM-Agent and the Theorem Prover continu-
Convergence Criteria The convergence of
ally refine the solution until the Solver LLM-
this iterative process is governed by the Solver
Agent is satisfied with the final assignments.
LLM-agent’s evaluation function E, which as-
2.2 Modeling the Agent Environment sesses both the correctness of the solution
against the domain-specific requirements and
The feedback loop is how the agents engage
the presence of any syntactic or semantic er-
with each other. This loop is mathematically
modeledusingacombinationofevaluationfunc- rors detected by T. We assume that E is a
black-box function defined by the instructions
tions and error detection mechanisms, which
given to the LLM. The loop terminates when
together guide the system towards a solution
that optimally satisfies the problem constraints. E returns a value indicating that the solution
More formally, let D,G,T,E, and F repre- S k sufficiently meets all puzzle requirements
and contains no detectable errors, or when a
sent the decomposition, translation to SMT-
maximum retry limit has been reached.
LIB, theorem solving, evaluation, and feedback
functions, respectively. The feedback loop can
Optimization and Refinement Each iter-
be described by the following recursive func-
ation through the feedback loop serves to pro-
tion:
gressively refine the solution, optimizing the
representation and alignment with the puzzle’s
constraints. This optimization process is crit-
S = F(E(T(G(D(P)),S )),S )
k+1 k k ical for moving the solution towards a local
optimum, where no further improvements can
Where,
be detected by E or F.
P: initial puzzle in natural language.
S : solution state at the k-th iteration.
k 3 Experimental Setup
D(P): decomposes P into a structured format
amenable to translation. To comprehensively evaluate ZPS’s perfor-
G: translates this structure into the SMT-LIB mance, we examine its effectiveness across 114
format. Zebra Puzzles. Our assessment emphasizes
4ZPS’s capability to solve the puzzles using the of each solution. For each problem, the model
different agents. compared the logical assignments produced by
the solving agent against the reference assign-
3.1 Selection of Logic Puzzles ments, producing a final accuracy score.
We compiled two datasets to evaluate the Todemonstratetheautogradingprocess,con-
problem-solving capabilities of our agent- sider a scenario where the output of the SMT-
centric approach. The first dataset, sourced LIB solver is evaluated against a pre-defined
from GitHub2, contains 59 Zebra puzzles in- answerkey. Thesolver’soutputandsubsequent
volving entity-attribute matching. We further interpretation by the grader are detailed below.
curated 55 additional puzzles from different
SMT-LIB Solver Output Below is a
sources from the Web and manually cross-
sample SMT solution for the logic grid puz-
checked them to determine they are valid zebra
zle given in Fig. 1.
problems.
; 1 is Brazilian, 2 is German
3.2 Agent Configuration ; 3 is American
We experimented with three different LLMs: (define-fun H1_Color () String "Blue")
GPT-4, GPT-3.5,and Llama3-8b . We used z3 (define-fun H1_N () Int 1)
as the automated theorem solver. Our total (define-fun H1_Anml () String "Cats")
cost across all experiments was approximately (define-fun H1_Sp () String "Football")
2500 USD. (define-fun H2_Color () String "Green")
(define-fun H2_N () Int 3)
Number of Retries In our experimental (define-fun H2_Anml () String "Dogs")
setup, we initially conduct the feedback loop (define-fun H2_Sp () String "Basketball")
once. Toenhanceperformanceandaddresssyn- (define-fun H3_Color () String "Red")
tacticalerrorsinthefinaloutput,weimplement (define-fun H3_N () Int 2)
an additional cold-start retry mechanism if we (define-fun H3_Anml () String "Fishes")
reach the action-limit without an error-free so- (define-fun H3_Sp () String "Baseball")
lution. This involves restarting the workflow
Ground Truth Answer
from scratch with an increased temperature.
• House 1: Blue, Brazilian, Fishes, Football
Response Limit To limit the conversation • House 2: Green, American, Cats, Baseball
length and prevent hallucination, we define a
• House 3: Red, German, Dogs, Basketball
maximum number of actions that the LLM-
The autograder evaluates the solution by
agent can take. All experiments performed
mapping the SMT-LIB output to the expected
allow the LLM to perform up to 4 actions; this
results either using contextual clues or an ex-
limitisresetifthepuzzle-solvingtaskisretried.
plicitly defined lookup table, which would be
3.2.1 Grading defined in the SMT-LIB comments, convert-
To assess solution accuracy, we created an au- ing function definitions into comparable assign-
tograder LLM-agent that provides a numeric ments, as in Table 1.
gradetoeverysolutiongeneratedbythesolving
Partial Scoring (PS) Eachcorrectmatch
agent. Each assignment is worth 1 point. In or-
between the SMT-LIB output and the answer
dertoevaluatethereliabilityofthisautograder,
key earns a point. The autograder agent also
we also conducted a user study where a subset
calculates the total number of assignments
of the problems were regraded by humans and
which is equal to the number of points it is pos-
then compared to the autograder’s results.
sible to receive. In this example, all matches
are correct, thus:
Autograding GPT-4o was used for auto-
grading. It received the ground-truth answer,
final SMT-LIB output, and conversation his- Correct Matches 8
Partial Score = = = 0.67
tory to assess the consistency and correctness Total Matches 12
If the animals and sports had been chosen
2https://github.com/ross-nordstrom/
correctly, the score would be 1.
LogicSolver/tree/master/data
5when integrating solver feedback, evaluating
Entity Assignment Result
how external theorem provers enhance LLM
House 1 Color: Blue ✓
effectiveness. Thirdly, we explore the impact of
House 1 Nationality: Brazilian ✓
House 1 Animal: Cats ✗ usingadecompositionagent,analyzingwhether
House 1 Sport: Football ✓ segmentingpuzzlesintosimplercomponentsbe-
fore solving improves overall solution quality.
House 2 Color: Green ✓
House 2 Nationality: American ✓ Four, we conduct a user study to evaluate our
House 2 Animal: Dogs ✓ LLM-Grader and substantiate the validity of
House 2 Sport: Basketball ✗ our results.
House 3 Color: Red ✓
House 3 Nationality: German ✓
House 3 Animal: Fishes ✗
House 3 Sport: Baseball ✗ 4.1 ZPS Performance over Baselines
Table 1: Validation Results To establish a baseline, we first evaluate the
performance of LLMs without the assistance of
a solver by asking the LLM to solve the logic
Manual User Study Grading A separate
grid puzzle. This baseline configuration yields
user study manually graded 50 solutions from
mediocre puzzle-solving accuracy, as detailed
the state-of-the-art workflow, 35 solutions from
in Table 2. We report both the average partial
a non-optimal variant, and 20 solutions from
score, given by the "Avg. PS" column, and the
the naive approach. Though it was impractical
number of puzzles solved fully correctly, given
to have all of the thousands of solutions that
by the "#Solved" column. For instance, GPT-
the LLM-agent generated be hand-graded, this
4 under a baseline achieves an average partial
user study allows us to quantify the correctness
score of of 52.4% and solves 27/114 logic grid
of our results and verify that our autograder
puzzles completely correctly.
correlates well with the ground-truth grades.
The manual grading team included five un- The effectiveness of the LLM-agent work-
dergraduate computer science students and one flow increases markedly when solver feedback
master’s student. We then used their manual is incorporated. As shown in Table 3, the in-
grades to capture various statistical measures tegration of theorem prover feedback, without
of similarity between human grading and LLM retries and under a deterministic generation
grading; these stats are explained in the "Re- setting (temperature = 0), increases GPT-4’s
sults" section. average partial score to 0.687 from baseline of
Alargepercentageoftheattemptedsolutions 0.524 (∆ = 31.1%). The inclusion of a decom-
included explicit lookup tables, making these position agent further improves this to 0.700
solutions significantly more time-consuming to (∆ = 33.58%). In terms of the total number
grade (see "SMT-LIB Solver Output" and "An- solutions that can be completely solved, GPT4
swer Key" above). The lookup table could withsolversolvesupto133.33%moreproblems
appear anywhere in the generated text, which than the baseline settings. GPT-3.5 shows a
comprises multiple blocks of SMT-LIB code, er- similar positive trend.
rors, and intermediate SMT models. We there-
Llama3’s improvement is more subtle; we
fore do not include them in our user study.
believe this is because its fewer number of pa-
rameters limits its ability to generate syntac-
4 Results
tically correct SMT-LIB code. This theory is
This analysis is structured around four key re- supported by the fact that in every Llama3
search questions: Firstly, we examine the base- experiment, no less than 50 final solutions con-
line performance of different LLMs in solving tained errors, whereas in every GPT-4 or GPT-
logic puzzles without solver assistance to un- 3.5 experiment, the number was no more than
derstand their intrinsic problem-solving capa- 42. Nonetheless, Llama3 can also improve the
bilities. Secondly, we assess the improvements total number of correct solutions by 50% over
in accuracy and problem-solving completeness baseline.
64.2 ZPS Performance under Different Model T D Avg. PS #Solved ∆#Solved
Settings Llama3-8b 0 ✗ 0.496 21(18.4%) 50.0%
GPT-3.5 0 ✗ 0.493 22(19.3%) 29.4%
For the variable temperature experiments, we GPT-4 0 ✗ 0.687 59(51.8%) 118.5%
setthemodeltemperaturetozeroandincreased Llama3-8b Var. ✗ 0.436 15(13.2%) 7.1%
GPT-3.5 Var. ✗ 0.484 24(21.0%) 41.2%
it if the solution contained errors. While this
GPT-4 Var. ✗ 0.761 72(63.2%) 166.7%
approach provides the flexibility to bypass a
Llama3-8b 0 ✓ 0.468 18(15.8%) 28.6%
solution if the deterministic solution is erro- GPT-3.5 0 ✓ 0.520 24(21.0%) 41.2%
neous, it risks generating less stable solutions GPT-4 0 ✓ 0.700 63(55.3%) 133.3%
that may inadvertently replace syntactically
Table 3: Enhancements from Solver Integration
incorrect yet valid solutions with syntactically
with Percentage Improvement Over Baseline. The
correct but logically flawed ones. This phe-
"D" column indicates if a decomposition agent was
nomenon is particularly pronounced in models present in the workflow. The "T" column indicates
with fewer parameters, where the performance Temperature.
tendstodeclinewiththeintroductionofretries.
For example, under variable temperature con-
The average magnitude of the difference be-
ditions with retries, GPT-4 maintains a high
tweenthepartialscoregivenbytheLLM-grader
accuracy rate of 76.1%, while Llama3’s accu-
and the human evaluator. (ii) Avg. Rel. Diff. :
racy degrades to 48.4%.
The expected percent difference between the
The addition of a decomposition agent to
the partial score given by the LLM-grader and
the SMT-integrated LLM-agent yielded mixed
the human evaluator. % problems for which
results. For both GPT-4 and GPT-3.5, the
the LLM-grader (iii) overestimated and (iv) un-
average partial score and number solved fully
derestimated the partial score provided by the
correctly slightly improved, in all cases by less
human evaluator. (v) % problems for which
than 5.5%. However, Llama-3’s average partial
the LLM-grader and the human evaluator gave
score declined by less than 5% and it was able
exactly the same partial score, and (vi) Joint
to solve 3 fewer problems than with just SMT
Full Credit: The count of problems for which
integration. Because of the relatively small
both the user and the LLM assigned full credit,
differences in all cases, more experimentation
normalized over the total number of problems
is needed to determine when decomposition
that either party marked for full credit. This
increases performance.
metric helps in understanding the extent of
agreement in the grading of solutions between
Model T D Avg. P.S #Solved the human and machine evaluators.
The metric of "Joint Full Credit," which con-
Llama3-8b 0 ✗ 0.47 14 (12.3%)
sistentlyregistersabove85%, servesasarobust
GPT-3.5 0 ✗ 0.471 17 (15.0%)
indicator of the LLM-grader’s capability to ac-
GPT-4 0 ✗ 0.524 27 (23.7%)
curately assess fully correct solutions as demon-
strated by the level of agreement between the
Table 2: Baseline Performance of LLMs Without
LLM and a human grader. Additionally, the
Solver Integration.The "D" column indicates if a
decomposition agent was present in the workflow. analysis indicates a propensity for the grader
The "T" column indicates Temperature. to overestimate the score of the LLM without
SMT integration, whereas the integration of
SMT tends to result in slight underestimations
4.3 Manual Analysis of Grading by the grader. This observation suggests that
Based on our user study, our LLM-based grad- the integration of SMT and a feedback based
ing systems demonstrate high accuracy accross loop may contribute more significantly to per-
a variety of models and settings. The system formance improvements than the raw grading
maintains consistent scoring accuracy, with ex- differentials indicate.
act match rates exceeding 78% across all tested
5 Background and Related Work
scenarios.
To evaluate the LLM-grader, we employed The concept of agent-centric LLM agents, as
variousstatisticalmeasures: (i)Avg.Abs.Diff.: discussed in recent literature revolves around
72020). Forourapproach,itwasthusvitaltocre-
GPT-4 GPT-3.5 GPT-4
SMT+D Naive SMT ate an agent that can take into account context
Statistic (50) (20) (35)
and background knowledge to figure out the
ExactMatch(%) 78.26 78.94 82.35
correct translation into a formal space. Even if
Avg. Abs. Diff 0.056 0.040 0.117
Avg. Rel. Diff(%) -3.547 +13.8 +2.916 the clues were perfectly translated as they are
LLMOverestimated(%) 13.04 21.05 11.76
presented, a formal solver will not be able to
LLMUnderestimated(%) 8.70 0.00 5.88
JointFullCredit(%) 89.19 100 86.96 generate a fully correct solution without addi-
SpearmanCorrelation 0.73 0.948 0.70
tional encoding by the problem translator of
this general context. Our approach is different
Table 4: User Study Statistics comparing different
from prior agent approaches in that we use a
experimental setups. The first column is GPT-4
with SMT integration and the decomposition (D) structured symbolic space (SMT-LIB) but use
agentover50problems. ThesecondcolumnisGPT- thesyntacticandsemanticfeedbackfromanau-
3.5withoutSMTintegrationover20problems. The tomatedtheoromproverforanalysisinanLLM
third column is GPT-4 with just SMT integration agent. We also provide a conceptual framework
over 35 problems. All problems were graded by
to understand LLM interaction with the auto-
both the manual grader and the LLM.
mated theorem prover and its generated text
as an agent.
creating systems (usually backed by LLMs)
that can act independently in diverse environ-
ments, both physical and virtual (Wang et al., 6 Conclusion
2024). This framework shifts the focus from
passive systems to proactive entities capable of
This research shows the effectiveness of a multi-
dynamic interaction and problem-solving. In
agent LLM and SMT framework that bol-
these models, agents are designed to perceive
sters the performance of large language models
and react to multi-modal data, integrating vi-
(LLMs) in solving logic puzzles and other nat-
sual, auditory, and textual input to generate
ural language task. Our work demonstrates
appropriate actions in real time. The most
the importance of integrating LLMs and SMTs
tangible benefit of this framework is feedback,
in the task, boosting preformance over an
which can take the form of a physical environ-
LLM alone. We also show that the inter-
ment, error correction, or manual input (Du-
agent critique mechanism plays a crucial role.
rante et al., 2024).
Through dialogues, agents critique and refine
Significant work has been done to apply this
each other’s contributions, which leads to more
framework to solving text-based puzzles. Zhou
accurate and consistent results. The develop-
et al. (2023) used a process called Language
ment of an autograder, with a verified corre-
Agent Tree Search (LATS), which integrates
lation to human evaluation, played a role in
planning, reasoning, and acting within LLMs
the feedback mechanism by indicating when a
to decompose and solve a high-level reasoning
solution was not judged logically correct and
task. Gao et al. (2023) showed that gener-
also enabled iterative development of our ap-
ating intermediate representations as Python
proach. Our findings suggest that structured
programs allowed small LLMs to outperform
planning and agent-feedback greatly enhance
much larger ones Logic-LM and SatLM both
LLMs’ capability to solve logical problems.
used LLMs to generate formal representations
of general natural language problem and used Looking ahead, further research could opti-
off the shelf theorem provers to generate an- mize retry mechanisms for discovering more
swers (Pan et al., 2023; Ye et al., 2023). While effective solutions, informed by approaches like
none of these approaches focus on Zebra puz- Program-of-Thoughts and Graph-of-Thoughts-
zles, they each show that LLMs perform better Rationale(Chenetal.,2023;Bestaetal.,2024).
when used as agents in a formally grounded Additionally, increasing the agent-environment
system. size and the feedback loop length would en-
Research has shown that natural language hance the solving agent’s self-correction capa-
cannot be mapped one-to-one with a formal bilities by expanding the actual and effective
spaceduetoinherentambiguities(Osamaetal., context limits for remembering past strategies.
87 Limitations full disclosure about the nature of the study
and its unpaid nature. No ethical violations
This study, while advancing our understanding
were committed in such setting.
of LLMs in solving logic puzzles, has several
limitations that warrant further investigation.
Acknowledgments
Firstly, the experiments were confined to only
three models: GPT-4, GPT-3.5, and Llama3- References
8b. Investigation of generalizability across dif-
Maciej Besta, Nils Blach, Ales Kubicek, Robert
ferent LLMs is warranted, especially because
Gerstenberger, Michal Podstawski, Lukas Gian-
our performance gains occurred mainly in the inazzi, Joanna Gajda, Tomasz Lehmann, Hu-
GPT family. bert Niewiadomski, Piotr Nyczyk, and Torsten
Hoefler. 2024. Graph of thoughts: Solving
Secondly, our approach relied on specific
elaborate problems with large language mod-
prompt constructions for both the grader and
els. Proceedings of the AAAI Conference on
thesolveragents. Thereexistsapossibilitythat Artificial Intelligence, 38(16):17682–17690.
alternative prompting strategies could yield
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
more accurate or efficient problem-solving and
Yuan, Henrique Ponde de Oliveira Pinto, Jared
grading results. Further research is needed to
Kaplan, Harri Edwards, Yuri Burda, Nicholas
explore and optimize these prompts to fully Joseph, Greg Brockman, Alex Ray, Raul Puri,
leverage the potential of LLMs in this domain. GretchenKrueger,MichaelPetrov,HeidyKhlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan,
Additionally, our user study inherently car-
ScottGray,NickRyder,MikhailPavlov,Alethea
ries some uncertainty regarding its correlation
Power, Lukasz Kaiser, Mohammad Bavarian,
to actual problem-solving performance. So- Clemens Winter, Philippe Trottier, Felipe Pet-
lutions involving complex lookup tables were roski Such, Dave Cummings, Matthias Plappert,
excluded from the user study due to how time FotiosChantzis,ElizabethBarnes,ArielHerbert-
Voss, William Hebgen Guss, Alex Nichol, Alex
consuming they were to grade, which might
Paino,NikolasTezak,JieTang,IgorBabuschkin,
affect the study’s comprehensiveness and the
Suchir Balaji, Shantanu Jain, William Saunders,
general applicability of our findings. Christopher Hesse, Andrew N. Carr, Jan Leike,
Lastly, our bank of logic grid puzzles used Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage,
in this study was somewhat limited in both
Mira Murati, Katie Mayer, Peter Welinder, Bob
size– we used 114 problems– and range of diffi-
McGrew, Dario Amodei, Sam McCandlish, Ilya
culty. The majority of puzzles used were sub- Sutskever, and Wojciech Zaremba. 2021. Eval-
jectively rated as medium difficulty. Extending uating large language models trained on code.
arXiv.
this research to include a larger and more var-
ied dataset would verify the usefulness of our Wenhu Chen, Xueguang Ma, Xinyi Wang, and
findings. William W. Cohen. 2023. Program of thoughts
prompting: Disentangling computation from rea-
soning for numerical reasoning tasks. Preprint,
8 Ethics Statement
arXiv:2211.12588.
Use of Generative AI. Generative models
Jens Claes, Bart Bogaerts, Rocsildes Canoy, and
carry ethical risks, including the potential to
Tias Guns. 2019. User-oriented solving and ex-
produceharmfulcontentorcontentthatclosely
plainingofnaturallanguagelogicgridpuzzles. In
mirrors pre-training data. However, we are The Third Workshop on Progress Towards the
using the generative models to solve puzzles Holy Grail, volume 14.
rather than showing their direct output, mini-
Zane Durante, Qiuyuan Huang, Naoki Wake, Ran
mizing this risk.
Gong, Jae Sung Park, Bidipta Sarkar, Rohan
Compute. Employing deep learning models Taori, Yusuke Noda, Demetri Terzopoulos, Yejin
is computationally intensive and can have envi- Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, and
Jianfeng Gao. 2024. Agent ai: Surveying the
ronmental implications. However, as no models
horizons of multimodal interaction. arXiv.
were trained as part of this research, the com-
putational impact remains relatively low. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Human Evaluator. We use only 5 human Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. 2023. Pal: Program-aided lan-
evaluators who are undergraduate/masters stu-
guage models. In International Conference on
dents in the lab environment and were given Machine Learning, pages 10764–10799. PMLR.
9Adrian Groza. 2023. Measuring reasoning capabili- (set-logic QF_LIA)
ties of chatgpt. arXiv.
; Place of each ostrich
Aleksandar Milicevic, Joseph P Near, and Rishabh (declare-const Bridget_Place Int)
Singh.2012. Puzzler: Anautomatedlogicpuzzle (declare-const Kermit_Place Int)
solver. Massachusetts Institute of Technology (declare-const Ophelia_Place Int)
(MIT). (declare-const Stretch_Place Int)
Anirban Mukherjee and Utpal Garain. 2009. A re- ; Number of each ostrich
(declare-const Bridget_Number Int)
viewofalgorithmsforsolvingmathematicalword
(declare-const Kermit_Number Int)
problems in natural language texts. Artificial
(declare-const Ophelia_Number Int)
Intelligence Review, 32(4):285–298.
(declare-const Stretch_Number Int)
Mohamed Osama, Aya Zaki-Ismail, Mohamed Ab-
(assert (and
delrazek, John Grundy, and Amani Ibrahim.
(or (= Bridget_Place 1)
2020. Score-based automatic detection and res-
(= Bridget_Place 2)
olution of syntactic ambiguity in natural lan- (= Bridget_Place 3) (= Bridget_Place 4))
guagerequirements. In2020IEEEInternational (or (= Kermit_Place 1)
Conference on Software Maintenance and (= Kermit_Place 2)
Evolution (ICSME), pages 651–661. (= Kermit_Place 3) (= Kermit_Place 4))
(or (= Ophelia_Place 1)
Shirui Pan, Yizhen Zheng, and Yixin Liu. 2023. (= Ophelia_Place 2)
Integrating graphs with large language models: (= Ophelia_Place 3) (= Ophelia_Place 4))
Methods and prospects. arXiv. (or (= Stretch_Place 1)
(= Stretch_Place 2)
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, (= Stretch_Place 3) (= Stretch_Place 4))
))
Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai
Tang, Xu Chen, Yankai Lin, et al. 2024. A sur-
(assert (and
vey on large language model based autonomous
(or (= Bridget_Number 105)
agents. Frontiers of Computer Science, 18(6):1–
(= Bridget_Number 118)
26. (= Bridget_Number 126)
(= Bridget_Number 128))
Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett.
(or (= Kermit_Number 105)
2023. Satlm: Satisfiability-aided language mod- (= Kermit_Number 118)
els using declarative prompting. In Advances (= Kermit_Number 126)
in Neural Information Processing Systems, vol- (= Kermit_Number 128))
ume 36, pages 45548–45580. Curran Associates, (or (= Ophelia_Number 105)
Inc. (= Ophelia_Number 118)
(= Ophelia_Number 126)
Andy Zhou, Kai Yan, Michal Shlapentokh- (= Ophelia_Number 128))
Rothman, Haohan Wang, and Yu-Xiong Wang. (or (= Stretch_Number 105)
(= Stretch_Number 118)
2023. Language agent tree search unifies rea-
(= Stretch_Number 126)
soning acting and planning in language models.
(= Stretch_Number 128))
arXiv.
))
A Appendix
; The ostrich that finished second was #128.
(assert (or (and (= Ophelia_Place 2)
A.1 Example Puzzle Workflow (= Ophelia_Number 128))
(and (= Bridget_Place 2)
We begin by inputting the puzzle and its enti-
(= Bridget_Number 128))
ties into the Solver LLM-Agent (in this case, (and (= Kermit_Place 2)
GPT-4). (= Kermit_Number 128))
(and (= Stretch_Place 2)
(= Stretch_Number 128))))
first, second, third, fourth
Bridget, Kermit, Ophelia, Stretch
; The ostrich that finished first was either #118 or #126.
#105, #118, #126, #128
(assert (or (and (= Ophelia_Place 1)
(or (= Ophelia_Number 118)
The ostrich that finished second was #128.
(= Ophelia_Number 126)))
First place was either #118 or #126.
(and (= Bridget_Place 1)
Third place was either #126 or Bridget.
(or (= Bridget_Number 118)
Ophelia finished second.
(= Bridget_Number 126)))
Stretch finished 2 places after Kermit.
(and (= Kermit_Place 1)
The solver agent responds by attempting to (or (= Kermit_Number 118)
(= Kermit_Number 126)))
translate the puzzle and its clues into SMT-
(and (= Stretch_Place 1)
LIB: (or (= Stretch_Number 118)
10(= Stretch_Number 126))))) 105)
(define-fun Ophelia_Place () Int
; The runner that finished third was 2)
; either #126 or Bridget. (define-fun Kermit_Place () Int
(assert (or (and (= Ophelia_Place 3) 1)
(= Ophelia_Number 126)) )
(and (= Bridget_Place 3)
(or (= Bridget_Number 126) This output is fed back into the Solver LLM-
(= Bridget_Number 105)
Agent, which then produces the following mod-
(= Bridget_Number 118)
(= Bridget_Number 128))) ified SMT-LIB code:
(and (= Kermit_Place 3)
(= Kermit_Number 126))
(set-logic QF_LIA)
(and (= Stretch_Place 3)
(= Stretch_Number 126))))
; Place of each ostrich
(declare-const Bridget_Place Int)
; Ophelia finished second.
(declare-const Kermit_Place Int)
(assert (= Ophelia_Place 2))
(declare-const Ophelia_Place Int)
(declare-const Stretch_Place Int)
; Stretch finished 2 places after Kermit.
(assert (= Stretch_Place (+ Kermit_Place 2)))
; Number of each ostrich
(declare-const Bridget_Number Int)
; Distinct places
(declare-const Kermit_Number Int)
(assert (distinct Bridget_Place
(declare-const Ophelia_Number Int)
Kermit_Place Ophelia_Place Stretch_Place))
(declare-const Stretch_Number Int)
; Distinct numbers
; The places are in the set {1, 2, 3, 4}
(assert (distinct Bridget_Number
(assert (and
Kermit_Number Ophelia_Number Stretch_Number))
(or (= Bridget_Place 1)
(= Bridget_Place 2) (= Bridget_Place 3)
ThisSMT-LIBcodeisthenfedintoz3,which
(= Bridget_Place 4))
produces the following output: (or (= Kermit_Place 1)
(= Kermit_Place 2) (= Kermit_Place 3)
(error ""line 15 (= Kermit_Place 4))
column 0: invalid command, ’(’ expected"") (or (= Ophelia_Place 1)
(error ""line 15 (= Ophelia_Place 2) (= Ophelia_Place 3)
column 2: unexpected character"") (= Ophelia_Place 4))
(error ""line 15 (or (= Stretch_Place 1)
column 77: unexpected character"") (= Stretch_Place 2) (= Stretch_Place 3)
(error ""line 15 (= Stretch_Place 4))))
column 88: unexpected character"")
(error ""line 15 ; Numbers are in the set {105, 118, 126, 128}
column 117: unexpected character"") (assert (and
(error ""line 15 (or (= Bridget_Number 105)
column 136: unexpected character"") (= Bridget_Number 118) (= Bridget_Number 126)
(error ""line 15 (= Bridget_Number 128))
column 142: unexpected character"") (or (= Kermit_Number 105)
(error ""line 31 (= Kermit_Number 118) (= Kermit_Number 126)
column 0: invalid command, ’(’ expected"") (= Kermit_Number 128))
(error ""line 31 (or (= Ophelia_Number 105)
column 7: unexpected character"") (= Ophelia_Number 118) (= Ophelia_Number 126)
(error ""line 57 (= Ophelia_Number 128))
column 0: invalid command, ’(’ expected"") (or (= Stretch_Number 105)
(error ""line 57 (= Stretch_Number 118) (= Stretch_Number 126)
column 7: unexpected character"") (= Stretch_Number 128))))
(error ""line 65
column 0: invalid command, ’(’ expected"") ; The ostrich that finished second was #128.
sat (assert (or (and (= Ophelia_Place 2)
( (= Ophelia_Number 128))
(define-fun Ophelia_Number () Int (and (= Bridget_Place 2)
128) (= Bridget_Number 128))
(define-fun Stretch_Place () Int (and (= Kermit_Place 2)
3) (= Kermit_Number 128))
(define-fun Stretch_Number () Int (and (= Stretch_Place 2)
126) (= Stretch_Number 128))))
(define-fun Bridget_Place () Int
4) ; The ostrich that finished first was
(define-fun Kermit_Number () Int ; either #118 or #126.
118) (assert (or (and (= Ophelia_Place 1)
(define-fun Bridget_Number () Int (or (= Ophelia_Number 118)
11(= Ophelia_Number 126))) (define-fun Kermit_Place () Int
(and (= Bridget_Place 1) 1)
(or (= Bridget_Number 118)
)
(= Bridget_Number 126)))
(and (= Kermit_Place 1)
(or (= Kermit_Number 118)
(= Kermit_Number 126)))
(and (= Stretch_Place 1) Ground Truth Answer
(or (= Stretch_Number 118)
(= Stretch_Number 126)))))
; The runner that finished third
; was either #126 or Bridget. • Kermit: First, #118
(assert (or (and (= Ophelia_Place 3)
(= Ophelia_Number 126))
(and (= Bridget_Place 3) • Ophelia: Second, #128
(or (= Bridget_Number 126)
(= Bridget_Number 105)
(= Bridget_Number 118) • Stretch: Third, #126
(= Bridget_Number 128)))
(and (= Kermit_Place 3)
• Bridget: Fourth, #105
(= Kermit_Number 126))
(and (= Stretch_Place 3)
(= Stretch_Number 126))))
; Ophelia finished second.
(assert (= Ophelia_Place 2)) The autograder evaluates the solution by
mapping the SMT-LIB output to the expected
; Stretch finished 2 places after Kermit.
results either using contextual clues or an ex-
(assert (= Stretch_Place (+ Kermit_Place 2)))
plicitly defined lookup table, which would be
; Distinct places defined in the SMT-LIB comments, convert-
(assert (distinct
ing function definitions into comparable assign-
Bridget_Place Kermit_Place
Ophelia_Place Stretch_Place)) ments, as in Table 5. In this case, the solution
gets full credit.
; Distinct numbers
(assert (distinct
Bridget_Number Kermit_Number
Ophelia_Number Stretch_Number))
Table 5: Validation Results
(check-sat)
(get-model) Entity Result
Z3 produces the following output, which is Kermit Place: First (Correct)
then determined to be both error-free and con- Kermit Number: 118 (Correct)
sistent: Ophelia Place: Second (Correct)
Ophelia Number: 128 (Correct)
sat
Stretch Place: Third (Correct)
(
Stretch Number: 126 (Correct)
(define-fun Ophelia_Number () Int
Bridget Place: Fourth (Correct)
128)
Bridget Number: 105 (Correct)
(define-fun Stretch_Place () Int
3)
(define-fun Stretch_Number () Int
126)
(define-fun Bridget_Place () Int
A.2 User Study Instructions
4)
(define-fun Kermit_Number () Int
118) Thefollowinginstructionswerepresentedtoour
(define-fun Bridget_Number () Int manualgradersbeforetheybegangrading. The
105) full UI can be found at https://anonymous.
(define-fun Ophelia_Place () Int 4open.science/r/anon_emnlp-1AD0 by run-
2) ning the "autograder_flask.py" file.
1213