What’s Wrong with Your Code Generated by Large Language
Models? An Extensive Study
ShihanDou1∗,HaoxiangJia3∗,ShenxiWu1,HuiyuanZheng1,WeikangZhou1,MulingWu1,
MingxuChai1,JessicaFan5,CaishuangHuang1,YunboTao1,YanLiu1,EnyuZhou1,MingZhang1,
YuhaoZhou1,YuemingWu4,RuiZheng1,MingWen2†,RongxiangWeng6,JingangWang6,
XunliangCai6,TaoGui1†,XipengQiu1,QiZhang1,XuanjingHuang1
1FudanUniversity,China
2HuazhongUniversityofScienceandTechnology,China
3PekingUniversity,China
4NanyangTechnologicalUniversity,Singapore
5UNCChapelHill,USA
6MeituanInc.,China
shdou21@m.fudan.edu.cn,haoxiangjia@stu.pku.edu.cn,mwenaa@hust.edu.cn,tgui@fudan.edu.cn
Abstract languagerequirementdescriptionsandunittestexamples[41,72].
Theincreasingdevelopmentoflargelanguagemodels(LLMs)in Researchincodegenerationhasgainedconsiderableattentiondue
codegenerationhasdrawnsignificantattentionamongresearchers. toitssignificantimpactonthesoftwareindustry,particularlyin
ToenhanceLLM-basedcodegenerationability,currenteffortsare enhancingproductivityandaccessibilitybyacceleratingprogram
predominantlydirectedtowardscollectinghigh-qualitydatasets developmentandimprovingcodereliability[20,67,95].
andleveragingdiversetrainingtechnologies.However,thereisa Recently,theadvancementoflargelanguagemodels(LLMs)has
notablelackofcomprehensivestudiesexaminingthelimitations significantlypropelledthefieldofcodegeneration[23,49,81,97].
andboundariesoftheseexistingmethods.Tobridgethisgap,we Trainedonlargecorporaoftextandcode,LLMscanunderstand
conductedanextensiveempiricalstudyevaluatingtheperformance humanrequirementsandgeneratecorrespondingcodethrough
ofthreeleadingclosed-sourceLLMsandfourpopularopen-source “nextcodetokenprediction”[15,59,60].However,theexistingLLMs
LLMs on three commonly used benchmarks. Our investigation, struggletogeneratecorrectcodeinthefaceofmorecomplicated
whichevaluatedthelength,cyclomaticcomplexityandAPInum- humanrequirements[22,31,43,63].Despiteeffortstoenhance
berofthegeneratedcode,revealedthattheseLLMsfacechallenges codegenerationaccuracyusingdiversepre-trainedcodecorpora
ingeneratingsuccessfulcodeformorecomplexproblems,andtend [30,48,71],fine-tuningmodelsonhigher-qualitycode[84,100],or
toproducecodethatisshorteryetmorecomplicatedascompared employingvariousnaturallanguageprocess(NLP)techniques[79,
to canonical solutions. Additionally, we developed a taxonomy 82,92,101],thereremainsalackofathoroughempiricalstudyon
ofbugsforincorrectcodesthatincludesthreecategoriesand12 “what’swrongwiththecodegeneratedbylargelanguagemodels”.
sub-categories,andanalyzetherootcauseforcommonbugtypes.
First,accuracy(i.e.,passingrateofunittests)isbyfarthemost
Furthermore, to better understand the performance of LLMs in dominantcodegenerationcriterion,encompassingbothsyntactic
real-worldprojects,wemanuallycreatedareal-worldbenchmark andsemanticrequirements[24,93].Othercharacteristicssuchas
comprising140codegenerationtasks.Ouranalysishighlightsdis- codelengthandcomplexityareequallysignificantduetotheimpor-
tinctdifferencesinbugdistributionsbetweenactualscenariosand tanceofcodemaintenance[8,75].Therefore,weexpectthecode
existingbenchmarks.Finally,weproposeanoveltraining-freeitera- generatedbyLLMstobeprecise,withshortcodelengthandlow
tivemethodthatintroducesself-critique,enablingLLMstocritique
codecomplexity.Second,annotatingthebugtypesingenerated
andcorrecttheirgeneratedcodebasedonbugtypesandcompiler codeandanalyzingtheirdistributionisnecessary.Thereareamulti-
feedback.Experimentalresultsdemonstratethatourapproachcan tudeofreasonswhycodegeneratedbyLLMsmaynotreachhuman
significantlymitigatebugsandincreasethepassingrateby29.2% requirements.Duringcodegeneration,LLMsmustfullycompre-
aftertwoiterations,indicatingsubstantialpotentialforLLMsto hendtheproblemdescriptionandgeneratethecorrespondingcode
handlemorecomplexproblems. [36,46,53,87].However,deviationsintheLLM’sunderstandingof
theactualintentcanleadtotaskfailure.Additionally,thegenerated
Keywords codemightcontainsyntacticorsemanticerrors.Thegeneration
paradigmofLLMs,whichisthepatternofpredictingthenextcode
CodeGeneration,LLM,BugTaxonomy,Real-worldBenchmark
token,couldalsointroduceformattingerrorsasitisdrivenbyproba-
bilityanddoesnotinherentlyensureconsistencyinindentationand
1 Introduction
syntax[22,41].Bycategorizingbugsandanalyzingtheirdistribu-
Codegeneration(orprogramsynthesis)aimstoaccuratelygenerate tions,wecancomprehensivelyunderstandthelimitationsofLLMs
executableprogramsfromproblemspecificationssuchasnatural incodegenerationtasks.Thisanalysisiscrucialfordeveloping
strategiestoaddressthesebugs,therebyimprovingthequalityand
∗ShihanDouandHaoxiangJiacontributedequallytothispaper.†MingWenandTao
accuracyofgeneratedcode.Finally,thereisanoticeabledeficiency
Guiarethecorrespondingauthors.
4202
luJ
8
]ES.sc[
1v35160.7042:viXrainbenchmarksforevaluatingLLMsinreal-worldscenarios.The Additionally,toexploretherootcausesofincorrectcodegen-
benchmarkscommonlyusedareconstructedfromprogramming eratedbyLLMs,weconductedacomprehensivestudyonthebug
questions[11,16,31]orderivedfromreal-worldcoderepositories taxonomyingeneratedcode.Specifically,thisanalysisincludedtwo
andlibraries[42,94].Thesesamerepositoriesandlibrariesareused keysteps:(1)First,weutilizedscriptstoautomaticallycreatean
forpre-trainingLLMs,whichcouldcausedatacontaminationand initialtaxonomyofpotentialbugsbasedoncompilerfeedback.(2)
test-caseleakage[16,28,35,61,62,85].Therefore,developinga Second,humanexpertsmanuallyexaminedtheincorrectcodeand
high-quality,previouslyuntrainedbenchmarktoevaluateLLMs annotatedthebugtype.Duringthisphase,expertsweregiventhe
underreal-worldconditionsiscrucial. flexibilitytoexpandandrefinethebugtaxonomyasneeded.This
Inthispaper,weconductacomprehensivelarge-scaleempirical processinvolved22humanexpertswithbackgroundsinsoftware
studytoexplorethecapabilitiesandlimitationsofcodegeneration engineering,withatleasttwoexpertsassignedtodouble-check
usinglargelanguagemodels.Wethoroughlyexaminethecorrect- eachannotation.Ultimately,weclassifythebugsintothreepri-
nessandcomplexityofthegeneratedcode.Wecategorizebugs marybugtypesand12secondarybugtypes.Basedontheabove
usingscript-basedinitialannotationanddetailedmanualanno- result,somenotablefindingsare:(i)Closed-sourcemodelsoutper-
tation,statisticallymodelingtheirdistributionandprovidingan formopen-sourcemodels,particularlywhenhandlingmorecom-
in-depthanalysis.ToassesstheeffectivenessofLLMsinreal-world plextasks.(ii)Functionalbugsconstitutethehighestproportion,
codegeneration,wemanuallyconstructedahigh-qualitybench- whereassyntaxbugsrepresentthelowest.(iii)Thecodegenerated
mark,RWPB,andevaluatedLLMperformanceonit.Basedonour bythemodelstypicallyhasfewerlinesbuthighercomplexityand
findings,weproposeanovel,practicalmethodintroducingself- asimilarnumberofAPIsascomparedtothecanonicalsolutions.
critiquetoiterativelyidentifyandfixbugsinLLM-generatedcode (iv)Althoughgeneratingmorecommentsduringcodegeneration
withoutadditionaltraining. canenhanceLLMs’reasoningabilities,LLMsstillstruggletosolve
Ourstudyaimstoanswerthefollowingresearchquestions: morecomplicatedproblems.(v)Forcomplexproblems,LLMsoften
failtogenerateoptimalalgorithms,leadingtotimeouterrors.
• RQ1:EffectivenessinCodeGeneration(§3.1).Whatisthe
ToevaluatetheefficacyofLLMcodegenerationinreal-world
performanceofLLMsincodegeneration?Howdoesthecomplex-
projects,wedevelopedareal-worldbenchmarkbymanuallycollect-
ityoftasksaffectcodegeneration?Whatarethecharacteristics
ing140codegenerationtasksfromGitHubrepositoriesestablished
(e.g.,codecomplexityandAPInumber)ofthegeneratedcode?
in2024.Ourexperimentalresultsrevealedthatclosed-sourceLLMs
• RQ2:BugsinCodeGeneratedbyLLMs(§4).Whatarethe
outperformopen-sourceLLMsincodegenerationqualityforreal-
differenttypesofrootcauses(i.e.,bugs)thatcauseincorrectcode
worldprojects,particularlyexcellinginreducingsyntaxbugsand
tobegenerated?Howarethesebugsdistributedacrosswidely
runtimebugs.Claude-3achievesthebestperformancewithanac-
usedLLMsandpopularbenchmarks?
curacyof45.7%,whilePhi-3hasthelowestaccuracyatonly22%.
• RQ3:BenchmarkConstructionandEvaluation(§5).How
Wealsofoundthatthedistributionsofbugtaxonomyareincon-
tobuildareal-worldbenchmarkwhileminimizingdataleakage
sistentbetweentheexistingcommonlyusedbenchmarksandthe
(i.e.,ensuringitisnotusedinpre-training).Howdoreal-world
real-worldprojects.
projectbugsdifferfromcommon,widelyusedbenchmarks?
OurstudyrevealedthatexistingLLMsarepronetointroducing
• RQ4:MitigatingBugsinGeneratedCode(§6).Howtofix
bugsincodegenerationduetotheirfailuretocaptureandinterpret
bugsbasedonourfindingswithoutintroducingadditionalre-
humanintentandperformthecorrectcorrespondinglogic,evi-
sourceconsumption,suchasfine-tuningLLMs.
dencedinbothcurrentpopularbenchmarksandreal-worldprojects.
ToinvestigatetheseRQs,wecollected1,164programmingprob- Tomitigatethesebugs,weproposeaninteractiveapproachinvolv-
lemsfromthreewidelyusedbenchmarks(HumanEval+[16],MBPP+ ingself-critique[74]withoutadditionaltrainingcosts.Specifically,
[11],andAPPS+[31]).Toperformcodegeneration,weselected we make LLMs critique their incorrect code based on our con-
sevenLLMs,includingthreestate-of-the-artclosed-sourceLLMs structedbugtaxonomyandcompilerfeedback(i.e.,errormessages
(GPT-4[3],GPT-3.5[2],andClaude-3[10]),twowell-knownopen- fromfailedexecutions)tounderstandtheproblemin-depthand
sourceLLMs(LLama-3-Instruct[5]andPhi-3-Instruct[6]),andtwo analyzethecausesofthebug,andthenre-correctthecode.This
popularopen-sourcecodeLLMs(StarCoder-2[48],DeepSeekCoder methodcanbeperformediteratively.Experimentalresultsshowed
[30]).WeevaluatedtheseLLMsongeneratedcodebyusingunit thatourproposedmethodcanmitigatebugsandincreasethepass-
tests.ExperimentalresultsshowthattheseLLMscouldachieveon ingrateby29.2%aftertwoiterations,whichindicatessubstantial
averageapassingrateof41.6%.Specifically,GPT-4andClaude- potentialforLLMstohandlemorecomplextasks.
3 demonstrate the best performance, which is 63.8% and 56.7%, Insummary,ourpapermakesthefollowingcontributions:
respectively,whilePhi-3hasthelowestaccuracyat30.9%. • AthoroughevaluationofthecapabilitiesofLLMsincode
Furthermore, we collected the token number of the problem generation.Weconductedextensiveexperimentstoevaluate
description, the length, the cyclomatic complexity and the API theperformanceofcurrentwidelyusedLLMs.Wealsoexplored
numberofthecanonicalsolution.Thisinvestigationallowsusto theeffectofdifferenttypesoftasksoncodegenerationandthe
analyzehowthelengthofproblemdescriptionsandthecomplexity characteristicsofgeneratedcode.
ofcanonicalsolutionsaffecteachmodel’sperformance.Wealso • AbugtaxonomyofLLM-generatedcode.Wedevelopeda
examinedthecharacteristicsofthegeneratedcode,focusingon bugtaxonomybasedoncodegeneratedbyLLMs.Wefurther
factorssuchascodecharacteristicsandcommentnumbersacross analyzedthesebugsandmodeledtheirdistributiontofindthe
differentLLMsandbenchmarks. rootcausesofinaccuraciesinLLM-generatedcode.
2• Areal-worldbenchmark.Wemanuallyconstructedareal- currentresearchaimingtoimproveamodel’scode-generationabil-
worldbenchmarkbycollectingcodeonthenewestGitHubrepos- ityutilizesPython[17,30,103].Finally,weselectedthreewidely
itories.Wealsoanalyzedandcomparedthedistributionofbugs usedbenchmarks,HumanEval+[16],MBPP+[11]andAPPS+[31],
betweenexistingbenchmarksandourreal-worldbenchmark, andconstructedourcollectionofassessmentsbasedonthem.Hu-
highlightingtheinadequaciesofcurrentmodelsinreal-world manEval is a popular benchmark comprising 164 hand-written
scenarios. Pythoninterviewproblemsdesignedtotestlanguagecomprehen-
• Anovelmethodtomitigatecodegenerationbugs.Wepro- sion, algorithmic thinking, and basic mathematics skills. MBPP
poseanovelapproachtobugmitigationbyintroducingself- isanotherprevalentbenchmarkcontaining974introductoryin-
critique:allowingLLMstofindandfixbugsbyiterativelycri- stancescreatedbycrowd-sourceddevelopersknowledgeablein
tiquingtheirgeneratedcode.Wealsooffersomesuggestionsand Python. Both HumanEval+ and MBPP+ are expansions of their
possiblefuturesolutionsforimprovement1. originalversions,augmentingadditionalunittests[45].APPSis
a code generation benchmark collected from open-access sites,
Table 1: Overview of seven LLMs used in our study. C.W. includingCodewars,AtCoder,Kattis,andCodeforces.APPS+is
denotesContextWindow. acuratedversionofAPPSthatexcludesinstanceslackinginput,
output,orcanonicalsolutions,andincludesmanualanalysisto
eliminateissuessuchasincompletecode,syntaxerrors,APImisuse,
Modality Model C.W. ReleaseDate
andmissinglibrarydependencies[22].Ourbenchmarkincludesall
Llama-3-8B-Instruct 8k Apr,2024 samplesfromHumanEval+andMBPP+,alongwith600samples
Open-source/Text Phi-3-3.8B-Instruct 128k Apr,2024 fromAPPS+,totaling1,000samples.Specifically,APPS+categorizes
StarCoder-2-15B 16k Feb,2024 samplesintothreelevelsofdifficulty:introductory,interview,and
Open-source/Code DeepseekCoder-7B 4k Jan,2024 competition.Werandomlyselected200samplesfromeachcategory,
withatotalof600samples,toensureabalanceofvaryinglevelsof
GPT-4-Turbo 128k Dec,2023
Closed-source/Text GPT-3.5-Turbo-0125 16k Jan,2024 difficulty.
Claude-3 200k Mar,2024 Additionally,toevaluatetheefficacyofLLMcodegeneration
inreal-worldprojects,wecreatedareal-worldbenchmarkRWPB
fromGitHubrepositoriesestablishedin2024(detailsinSection5.1).
2 ExperimentalDesign
PromptDesign.Toensureexperimentalconsistencyacrossvar-
LargeLanguageModelSelection.TothoroughlyevaluateLLM- iousLLMs,weemployedastandardizedpromptformat,asoutlined
basedcodegeneration,weselectedsevenlargelanguagemodels in[45].Notably,ourpromptsexcludedlibraryimportinformation,
ofthreetypes:open-sourcetextLLMs,open-sourcecodeLLMs, necessitatingthatthemodelsautonomouslyidentifyandimport
andclosed-sourceLLMs.Foropen-sourcetextLLMs,weselected therequiredlibrariesfortheAPIsutilized.
twocommonmodels,namelyLlama-3[5]andPhi-3[6].Llama-3
isanadvancedandpopularLLMdevelopedbyMeta-AI,whichis
TaskPrompt:
trainedonextensivetextandcodecorpora.Phi-3isanotherexten-
Pleaseprovideaself-containedPythonscriptthatsolvesthe
sivelyusedLLM,notedforitstrainingonhigh-qualitytextbook
followingprobleminamarkdowncodeblock:
datasets.Inthecategoryofopen-sourcecodeLLMs,ourstudyused
{Programmingproblem}
StarCoder-2[48]andDeepseekCoder[48].StarCoder-2isawidely
BelowisaPythonscriptwithaself-containedfunctionthat
usedcodemodeltrainedon3.3to4.3trilliontokensofcodefrom
solvestheproblemandpassescorrespondingtests:
TheStackv2dataset[48],encompassingover600programming
languages.DeepseekCoderisanotherstate-of-the-artopen-source
codemodel.Itstrainingcorpusiscomprisedof2trilliontokens, ComputationandLaborResources.Toevaluatecurrentlarge
combiningcodeandnaturallanguages.Wealsoselectedthreeuni- languagemodels(LLMs)forcodegenerationandvalidateourpro-
versallyusedclosed-sourceLLMs,namelyGPT-3.5[2],GPT-4[3], posedimprovements,weconductedexperimentsonasinglenode
and Claude-3 [10]. Additional information and details on these witheightA100-80GBGPUsand2TBofCPUmemory,usingPython
modelsareprovidedinTable1. 3.11.Forcodeevaluation,buganalysis,andtheconstructionofour
BenchmarkSelection.Toevaluatetheaccuracyandbehavior real-worldprojectbenchmark(RWPB),weassigned37software
ofthegeneratedcodeandtoanalyzetheirbugs,weconstructed engineeringannotators,withbackgroundsinsoftwareengineering.
a collection of assessments from well-known benchmarks. Our Giventheircountriesofresidence,allannotatorsobtainedadequate
selectioncriteriawereasfollows:benchmarksmustbewidelyrec- compensation.
ognizedtoensurerelevance;theyshouldnotoriginatefrompopular
librariesorprojectstopreventdataleakage,ascurrentLLMsare
3 LLM-BasedCodeGeneration
trainedonasignificantportionofsuchdata[16,35,56,61,62,85];
andtheymusthavediverseunittestswithhighcodecoveragefor Foreachselectedmodel,wegeneratedcodeforprogrammingprob-
thesample.Moreover,weselectedbenchmarksrequiringthetarget lemsfromtheHumanEval+andMBPP+,andasubsetoftheAPPS+.
programminglanguageofthegeneratedcodetobePython,asmost Weevaluatedthemodel’seffectivenessbythepassingrateofunit
tests(Section3.1)andfurtheranalyzedthefactorsthatinfluenced
1Ourlabellingresults,evaluatingscripts,andreal-worldbenchmarkareavailablein
ourartifactrepository[1]. theirperformance(Section3.2).
3Table2:Modalitiesofthreecommon-usedbenchmarksandperformanceofsevenpopularLLMsincodegenerationonthese
benchmarks.CCdenotesCyclomaticComplexity.Allvaluesarein%.
Open-Source Closed-Source
TokenNums Lines CCof APINums
Dataset ofDescription ofCode Codes ofCode StarCoder-2 DeepSeekCoder Llama-3 Phi-3 GPT-4 GPT-3.5 Claude-3 Average
HumanEvalPlus 131.3 11.3 3.1 3.4 61.6 58.5 51.8 67.1 85.4 64.6 79.3 64.8
MBPPPlus 48.7 6.1 2.2 1.8 63.2 66.4 61.1 63.2 79.2 73.9 74.9 67.9
Introductory 307.7 10.3 3.2 5.6 42.0 45.5 37.5 13.5 72.0 56.0 69.5 44.4
Interview 480.3 27.4 6.4 15.1 10.5 20.5 17.5 9.0 46.5 25.0 38.0 21.5
APPSPlus
Competition 554.9 33.7 6.9 18.4 2.0 1.0 7.5 1.5 36.0 8.5 22.0 9.4
Average 447.6 23.8 5.5 13.0 18.2 22.3 20.8 8.0 51.5 29.8 43.2 25.1
Average 304.6 17.8 4.4 8.9 35.9 38.4 35.1 30.9 63.8 45.6 56.7 41.6
3.1 EffectivenessofLLMsinCodeGeneration. Finding2: Asthecomplexityofthebenchmarksincreases,the
Withintheconstraintsofourexperiment,weconsiderthegener- accuracyofLLMstendstodecline,especiallywhenthelinesof
atedcodetobecorrectifitpassescompilerverification,executes codeandthenumberofAPIsincrease.
withouterror,andoutputstheexpectedresultforpredefinedinputs.
Additionally,correctcodemustpassallunittestcasesinthebench-
mark,asdifferentunittestcasescovervarioussemantics.Only 3.2 FactorsforLLMsinCodeGeneration
bypassingalltestscanthegeneratedcodedemonstratecomplete
Inthissection,weanalyzespecificcharacteristicsinthegenerated
fulfillmentofalltherequirements.
code,includingthenumberoflinesofcode,cyclomaticcomplexity,
BasedonTable2,wecanobservethatGPT-4performsthebest,
numberofAPIs,andthenumberofcommentlines.
followedbyClaude-3,thenGPT-3.5,withallopen-sourcemodels
First,weinvestigatedthedifferencesincodecharacteristicsbe-
performingworsethanGPT-3.5.SpecificallyinaccordancewithHu-
tweenthecorrectgeneratedcodeandthecanonicalsolution.Figure
manEval+andMBPP+,GPT-4demonstratesthebestperformance
1showsthedifferencesinthenumberoflinesofcode,cyclomatic
on HumanEval+, while Claude-3 performs best on MBPP+. For
complexity,andnumberofAPIsonHumanEval+.Itcanbeobserved
APPS+,GPT-4maintainsa71.5%passingrateonsimplerIntroduc-
thatthecodegeneratedbythemodelstypicallyhasfewerlinesbut
toryproblems.However,whenfacedwithmorecomplexproblems
slightlyhighercomplexity,whilethenumberofAPIsissimilarto
(i.e.,InterviewandCompetition),theaccuracyofallmodelsdrops
thatofthecanonicalsolutions.Notably,codegeneratedbyClaude-3
significantly.GPT-3.5andallopen-sourcemodelshavelessthana
tendstoincludefewerlinesofcodeandlowercomplexitywhile
10%passingrateonCompetitionproblems.
utilizingmoreAPIs,whichislikelyduetoClaude-3utilizingAPIs
toreplacepartsofthecomplexcodeimplementation.Duetospace
limitationsinthispaper,weplacetheexperimentalresultsforthe
Finding1: Closed-sourceLLMsoutperformopen-sourceLLMs,
remainingbenchmarksintheopen-sourcerepository[1].
particularlywhenhandlingmorecomplextasks.Specifically,
GPT-4performswiththebestaccuracyacrossdifferentbench-
Finding3: Thecodegeneratedbythemodelstypicallyhas
marks.
fewerlinesbuthighercodecomplexityandasimilarAPInum-
berascomparedtothecanonicalsolutions.
Additionally,weinvestigatedthetokencountofproblemde-
scriptionsinthethreebenchmarks,aswellasthelinesofcode, Furthermore,weobservedadistributionpatterninthenum-
cyclomaticcomplexity,andnumberofAPIsusedinthecorrespond- berofcommentsincorrectcodeversusincorrectcode,asshown
ingcanonicalsolutions.Thetokencountofproblemdescriptions inFigure2.Currently,researchershavefoundthatintroducinga
andtheaforementionedcodecharacteristicscan,tosomeextent, thinkingprocessduringtheinferenceorsupervisedfine-tuning
representthecomplexityoftheproblems.TheHumanEval+and phasecanincreasethereasoningandunderstandingabilityofLLMs,
MBPP+haveconciseproblemdescriptions,shortcodelengths,low suchasinmathreasoning[32,77,80]andtaskplanning[65,83].
complexity,andfewerAPIcalls.Incontrast,APPS+ismorecom- Addingcommentsinthecodecorpuscanalsoenhancetheper-
plex,withextensiveproblemdescriptionsandintricatecode,in- formanceofLLMsincode-relevanttasks[7,54,81,90].Incode
cluding447.6tokensintheproblemdescription,23.8linesofcode, generation,codecommentsgeneratedbyLLMscanenhancetheir
5.5cyclomaticcomplexityand13APIcalls.Basedonthemanual understandingofcodeproblemsandhelpthembettergenerate
reviewofAPPS+,wefoundthattheproblemdescriptionscontain thesubsequentcode[52,71].However,ourexperimentrevealed
anamountofredundantandirrelevantinformation,suchasback- thatthevariabilityinthenumberofcommentsdiffersbetween
groundknowledgeunrelatedtoalgorithms.Therefore,asthelength correctandincorrectcode,withincorrectcodeoftenshowinga
andcomplexityoftheproblemsincrease,theaccuracyofthecode widerspread.Figure2illustratesthenumberofcommentsincorrect
generatedbythemodelssignificantlydecreases,indicatingthat andincorrectcodefordifferentLLMsacrossvariousbenchmarks.
thedifficultyandcharacteristicsofvarioustaskshaveasubstantial ForHumanEval+,incorrectcode,particularlyfromClaude-3and
impactonmodelcapabilities. Deepseek-Coder,tendstohavemoreandmorevariedcomments
4Lines of Code CC of Code API in Code
StarCoder-2 0.6
Claude-3
0.12 Llama-3 0.4
Phi-3 0.5
0.10 DeepseekCoder
GPT-3.5
GPT-4 0.4 0.3
0.08
0.3
0.06 0.2
0.04 0.2
0.1
0.02 0.1
0.00 0.0 0.0
40 20 0 20 0 5 10 5 0 5 10
Difference Difference Difference
Figure1:ThecodecharacteristicdifferencesbetweencodegeneratedbymodelsandcanonicalsolutionsonHumanEval+.CC
denotescyclomaticcomplexity.
Comment Line Nums in Correct Code Comment Line Nums in Incorrect Code inthegenerationofmorecommentswhentacklingcomplexchal-
20 20
lenges.Secondly,LLMsmightgenerateadditionalcommentsasa
15 15 formofcontextualorexplanatorycontentwhentheyareuncer-
10 10 tainaboutthecorrectnessofthegeneratedcode.However,these
5 5 commentsdonotsignificantlyimprovetheLLMs’accuracy,asthe
0 0 correctnessrateremainslow.Thus,theincorrectcodecontains
CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 morecommentsthanthecorrectcode.
20 20
Finding4: Incorrectcodeisgenerallyassociatedwithahigher
15 15
numberofcomments,suggestingthatLLMsmightincludemore
10 10 explanatorycommentswhenthecodeiscomplexandlesslikely
5 5 tobecorrect.
0 0
CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 4 BugsinCodeGeneratedbyLLMs
20 20 Tofullyunderstandtherootcausesofbugsinexperiments,we
15 15 categorized5,072bugsgeneratedfromsevenLLMsusingscript-
10 10 basedinitialannotationandmanualdetailedannotation,identifying
5 5 threeprimarybugtypesand12secondarybugtypes(Section4.1).
We further performed a comprehensive bug analysis, including
0 0
CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 aninvestigationofthedistributionofvariousbugs,anddiscussed
findingsfordifferentbugtypes(Section4.2).
20 20
15 15 4.1 TaxonomyofCodeGenerationBugs
10 10
4.1.1 Methodology. Inspiredbyexistingresearch,wefirstanno-
5 5
tatedallbugsintothreeprimarybugtypes:SyntaxBug,Runtime
0 0
CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 CL3 D GC PT3.5 GPT4 LL3 Phi3 SC2 B usu ig n, gan thd eF Pu yn tc hti oo nna inl tB eu rg p. reT th ee rs ’se oth ur tpee utp ,r ei nm ca or my pb au sg st ay lp le ps o, si sd ie bn leti bfi ue gd
Figure2:Thedifferenceofcommentnumbersbetweencor- types.Basedontheprimarybugtypesabove,weadoptedatwo-
rect and incorrect code generated by LLMs. SC2 denotes stageprocessinvolvingscript-basedinitialannotationandmanual
StarCoder-2,DCdenotesDeepSeekCoder,LL3denotesLlama- detailedannotation.Theinitialscript-basedannotationaimsto
3,andCL3denotesClaude-3. efficientlyannotatetheprimarybugtypesusingoutputfromthe
Pythoninterpreter,enhancingthemanualdetailedannotation.The
manualdetailedannotationinvolvedexpertsdiscussingandrefin-
thancorrectcode.Similarly,intheAPPS+dataset,additionalmod- ingthetaxonomyofsecondarybugtypes,ensuringthatallbugs
elstendtoincludecommentsinbothcorrectandincorrectcode, werecorrectlyannotatedintoappropriatesecondarytypes.
withatendencytoproduceahighermediannumberofcomments Inthefirststage,thescriptcollectedoutputfromthePython
inincorrectcode.Thisphenomenoncouldbeattributedtotwo interpreterduringcodeexecutionandusedregularexpressionsfor
potentialfactors.Firstly,thecomplexproblemspresentinthecode primarybugannotation.Syntaxbugsareidentifiedbymatching
corporausedtotraintheseLLMsoftencontainmorecomments. “SyntaxInvalid”or“SyntaxError”intheerrormessages.Runtime
Consequently,LLMsmaylearntoreplicatethispattern,resulting bugsaredetectedbymatchingthe“Traceback”field.Functional
5
egatnecreP egatnecreP
HumanEval
Plus
MBPP
Plus
APPS
Plus
RWPB
egatnecrePA.1 Incorrect Syntax Structure Bugs due to incorrect syntax structure.
A Syntax Bug A.2 Incorrect Indentation Bugs due to incorrect use of indentation.
Bugs due to violation of synatx rule, A.3 Library lmport Error Bugs due to missing library or incorrectly import library.
detected before execution.
B.1 APl Misuse Bugs due to misjudge variable type or misunderstand API usage.
Bugs due to violation of runtime reference,
unexpectly terminating execution.
B.2 Definition Missing LLMs refer functions or variables without definition.
Bug Type B Runtime Bug
B.3 Incorrect Boundary Condition Check Bugs due to out-of-bound access or missing corner case check.
B.4 Incorrect Argument LLMs output code with an incorrect number of arguments.
Bugs due to incorrect logic implementation or
deviation from requirement, leading to unit LLMs misunderstand the problem requirement, lack knowledge to
test failures. C.1 Misunderstanding and Logic Error implement, or produce the faulty logic.
C Functional Bug C.2 Hallucination LLMs output code without any logic or apply incorrect knowledge.
C.3 Input/Output Format Error Bugs due to incorrectly follow input-output format.
Figure3:TaxonomyofbugsthatoccurredincodegeneratedbyLLMs.
bugsarerecognizedbythe“AssertionError”phrase,asassertstate- codeexecution.Therearethreesecondarysyntaxbugtypes:In-
mentsareusedtoverifyoutputsintheexperiment.Itshouldbe completeSyntaxStructure,IncorrectIndentation,andLibraryImport
notedthattheannotationpriorityis:syntaxbug,runtimebug,and Error.
functionalbug.Thismeansweonlyidentifyafunctionalbugif A.1IncompleteSyntaxStructure.Anincompletesyntaxstruc-
thecodedoesnotexhibitsyntaxorruntimebugs.However,the tureindicatesthatthegeneratedcodeincludesanopenorpartially
interpreterdoesnotprovidedetailederrorinformationfor“Asser- writtensyntaxelementthathasnotbeenproperlycompleted.This
tionError”.Therefore,thescriptutilizedGPT-4togeneratethree typeofbugincludesincompletestatements,unmatchedparenthe-
possiblerootcausesalongwithexplanations,aidinginsubsequent ses,un-closedquotes,ormissingcolons.
manualanalysis.Wealsodiscusstheaccuracyofthreeannotation
print(" ".join(map(str, range(1, n + 1))) # unclosed Parentheses
methods(i.e.,fullyautomated,fullymanual,andAI-assisted)for
labelingbugtypesinSection7. A.2 Incorrect Indentation Python relies on indentation to
Inthesecondstage,22expertswithabackgroundinsoftware definethescopeofloops,conditionals,functions,andothercode
engineering,reviewedtheincorrectcodewiththeprimarybug blocks.Incorrectindentationviolatesthesyntaxstructure,causing
typesandindependentlycreatedataxonomyforsecondarybug syntaxerrorsorinconsistentsemantics.Asshowninthefollowing
types.Then,theexpertscombinedeachindividualtaxonomyand example,theindentationlevelofthesecondlinedoesnotmatch
discussedtheirfindingstoestablishafinaltaxonomy.Finally,differ- thesubsequentlineswithinthesamecodeblock,whichviolates
entexpertsdouble-checkedtheannotationtoensureallbugswere thePythonspecification.
correctlyannotated.Forbugswithdisagreementsinannotation,
def right_angle_triangle(a, b, c):
allexpertsconvenedforajointdiscussionuntilaconsensuswas sides = [a, b, c] # inconsistent indentation level
reached.Additionally,expertswererequiredtoreviewandrevise sides.sort()
theannotationofsimilarbugstoensureconsistency.Notably,be-
A.3LibraryImportError.LibraryimportallowsPythonpro-
causethecodegenerationprocessisbasedon“nextcodetoken
gramstoutilizeexternalcode,avoidingredundantdevelopment.
prediction”,thegeneratedcodedirectlyinfluencessubsequentcode
Common import errors include missing import statements and
generation.Therefore,weannotatedbugsbasedontherootcauseof
incorrectimportlevels.Inthefollowingexample,thegenerated
thefirsterroneouscodeinthegeneratedcode.Additionally,during
codeincorrectlyimportsallpublicfunctionsfromtheheapqlibrary
thecodeannotationprocess,wefoundthatsomeproblemshave
withinthefunctionbody.However,thisoperationisonlypermitted
incompleteorambiguousfunctionalitydescriptions.Themodels
outsidethefunctionbody.
attempttoinferthemissingdetailstocompletethecodegeneration,
leadingtofailuresinunittests.WerefertotheseissuesasAmbigu- def xuDRm():
from heapq import * # import * only allowed at module level
ousProblems.Thetwostagescollectivelytook2,400person-hours
andculminatedintheclassificationofataxonomyofthreepri-
marybugtypesand12secondarybugtypes,asshowninFigure3. 4.1.3 TypeB:RuntimeBug. RuntimeBugsrefertobugsthatarise
Detailedintroductionsofeachbugtypewillbediscussedinthe whenthecodefailstoconformtotheruntimespecification,which
followingsections. isdetectedduringexecution.Basedonthetaxonomy,therearefive
secondaryruntimebugs:APIMisuse,DefinitionMissing,Incorrect
BoundaryConditionCheck,IncorrectArgument,andMinors.
B.1APIMisuse.LLMsutilizeAPIstoenhancecodeexecution
4.1.2 TypeA:SyntaxBug. Syntaxbugsviolatethegrammatical efficiencyandachievedesiredfunctionalities.However,misinter-
rulesoftheprogramminglanguagebeingused.Thesebugsare pretationofcallerattributes,incorrectAPIusage,orimproperar-
detectedbythePythoninterpreterwhenittriestoparsebeforethe gument type identification can lead to API misuse, resulting in
6runtimeerrorsinthegeneratedcode.Thefollowingexampleillus- asnumericaladdition.What’smore,evenifLLMsfullyunderstand
tratesanattributeerrorarisingfromthemisinterpretationofthe theproblemdescription,convertingthisknowledgeintocorrect
variabletup’stype. logicremainschallenging.
def find_min_diff(tup, n): ------ Problem ------
tup.sort() # 'tuple' object has no attribute 'sort Write a function to convert a given tuple of positive integers
into a single integer.
B.2DefinitionMissing.Pythonrequiresvariablesandfunc- assert tuple_to_int((1,2,3))==123
tionsmustbedefinedpriortotheiruseinaprogramHowever, ------ Incorrect Code ------
def tuple_to_int(tuple_of_ints):
occasionallyomitthedefinitionofcommonlyusedvariablesor
result = 0
simplefunctions.Asillustratedinthecodebelow,theLLMover- for digit in tuple_of_ints:
looksthedefiningvariableMOD,whichiscommonlyemployedin result = result * 10 + digit
return result
algorithmicproblems. ----- Canonical Solution -----
return result % MOD # variable MOD is undefined def tuple_to_int(nums):
return int(''.join(map(str,nums)))
B.3IncorrectBoundaryConditionCheck.Incorrectbound-
C.2Hallucination.Hallucinationreferstoinstanceswherethe
aryconditioncheckreferstotheimproperimplementationoflogic
LLMgeneratescodethatissyntacticallyplausiblebutfactually
forhandlingtheedgesorlimitsofarangeinaprogram.Asshown
incorrectorsemanticallymeaningless.Asshowninthebelowcode,
inthecodebelow,theprogramfailstoverifythelistlengthbefore
thecodegeneratedbytheLLMdoesnotalignwiththeproblem
performingaremainderoperation,leadingtoaZeroDivisionError
requirementsatall.
whenprocessinganemptylist.
def rotate_right(lst, n): # lst = [] ------ Problem ------
length = len(lst) Write a python function to find whether a number is divisible by
rotation_count = n % length # ZeroDivisionError 11.
assert is_Diff (12345) == False
B.4IncorrectArgument.LLMsoccasionallydisregardthespec- ------ Incorrect Code ------
def is_Diff(n):
ifiedinputformatinproblemdescriptions,resultinginmismatches sum = 0
in the number or type of arguments in the generated code. As diff = 0
showninthecodebelow,theproblemincludestwoinputs:thefirst while n > 0:
digit = n % 10
indicatesthenumberofelementsandthesecondrepresentsthe n = n // 10
elementstobeprocessed.However,thegeneratedcodeonlysets sum += digit
diff += digit * 2 if n > 0 else digit
oneparametertoreceivetheelements.
return sum % 2 != 0
----- Input ----- ----- Canonical Solution -----
5 def is_Diff(n):
WEEWW return n % 11 == 0
----- Incorrect Code -----
def min_reversals(directions): # lack the first argument C.3Input/OutputFormatError.DifferentfrombugtypeB.4
for direction in directions: (i.e.,incorrectargument),input/outputformaterrorreferstothe
if direction == 'W':
incorrectorderofinputsandoutputsaswellastheincorrectpre-
B.5Minors.MinorsinruntimebugsincludeTimeoutErrorsand cisionoftheoutputdata.Asshowninthecodebelow,theLLM
LLM-DefinedExceptions.Programsexceedingthislimitduetohigh incorrectlyconvertsafloatoutputtoaninteger.
algorithmcomplexityorexcessiveloopiterationsaremarkedto
------ Incorrect Code ------
containatimeouterror.LLM-definedexceptionsrefertotheLLM def dog_age(human_years):
settingexceptionsforconditionalbranchesintheproblemthatdo if human_years <= 2:
dog_years = human_years * 10.5
nothaveexplicitlyprovidedsolutions.
else:
dog_years = 21 + (human_years - 2) * 4
4.1.4 TypeC:FunctionalBug. Functionalbugsrefertothebugin
return int(dog_years) # Incorrect output format
theprogramthatcausesittobehaveincorrectlyornotasintended ----- Canonical Solution -----
accordingtoitsfunctionalrequirements(i.e.,thecoderunssuccess- def dog_age(h_age):
if h_age <= 2:
fullybutfailstopassallunittests).Basedonthetaxonomy,there d_age = h_age * 10.5
arefoursecondaryfunctionalbugs:MisunderstandingandLogic else:
d_age = 21 + (h_age - 2) * 4
Error,Hallucination,Input/OutputFormatError,andMinors.
return d_age
C.1 Misunderstanding and Logic Error. Code generation
tasksinvolvealgorithmicproblemswhereLLMsmustextractin- C.4Minors.MinorsinfunctionalbugsincludeIncorrectInitial-
formationfromnaturallanguagesandapplytheirknowledgeto ization,Sub-optimalCode,andInfiniteLoop.Incorrectinitialization
understandtherequirementsandestablishcorrectlogic.However, indicatesthecodelogiciscorrect,butincorrectinitializationval-
whenfacedwithcomplexnaturallanguagedescriptions,models uesforsomevariablespreventthecodefrompassingunittests.
oftenstruggletofullycomprehendconcepts,referencerelation- Asshowninthecodebelow,thevariablemax_sumisincorrectly
ships,andconditionalbranches.Forexample,LLMsmayincorrectly initialized to 0. Sub-optimal code refers to instances where the
interpretintegerconcatenationasnumericaladdition.Asshownin LLMgeneratescodeusingsub-optimalalgorithms(e.g.,greedyal-
thecodebelow,LLMsincorrectlyinterpretintegerconcatenation gorithms)tosolvetheproblem,resultingincodethatcanonlypass
7Table3:TypesofbugsintroducedduringcodetranslationbysevenpopularLLMsonthreewidelyusedbenchmarks(i.e.,
HumanEval+,MBPP+,andAPPS+).SC2denotesStarCoder-2,DCdenotesDeepSeekCoder,LL3denotesLlama-3,andCL3denotes
Claude-3.Allvaluesarein%.
HumanEvalPlus MBPPPlus APPSPlus
BugTypes Open-Source Closed-Source Open-Source Closed-Source Open-Source Closed-Source
SC2 DC LL3 Phi3 GPT4 GPT3.5 CL3 SC2 DC LL3 Phi3 GPT4 GPT3.5 CL3 SC2 DC LL3 Phi3 GPT4 GPT3.5 CL3
A.1IncompleteSyntaxStructure 0.0 0.0 0.0 0.0 0.0 0.6 0.0 0.3 0.5 2.4 1.9 0.0 0.5 0.0 0.3 3.5 2.7 0.8 0.2 1.5 0.2
A.2IncorrectIndentation 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 1.2 0.0
A.3LibraryImportError 3.7 2.4 10.4 0.0 0.6 2.4 0.0 0.3 0.0 0.8 0.0 0.3 0.5 0.0 0.0 0.0 0.5 0.2 1.0 2.0 2.5
ASyntaxBug 3.7 2.4 10.4 0.0 0.6 3.0 0.0 0.5 0.5 3.2 1.9 0.3 1.0 0.0 0.3 3.7 3.2 1.0 1.2 4.7 2.7
B.1APIMisuse 1.2 1.2 1.8 0.0 0.0 2.4 0.6 2.9 2.1 1.3 2.4 1.0 1.8 1.3 1.8 4.7 6.5 7.8 0.5 1.3 1.0
B.2DefinitionMissing 0.0 2.4 0.0 0.6 0.0 5.5 0.0 0.8 0.3 0.0 0.0 0.0 0.0 0.0 1.0 1.2 2.3 3.3 0.5 2.2 1.8
B.3IncorrectBoundaryConditionCheck 0.6 2.4 0.6 0.0 0.0 3.0 0.0 2.4 1.3 2.1 1.9 1.5 0.5 1.5 1.8 4.2 5.2 5.7 2.8 4.3 1.7
B.4IncorrectArgument 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.8 0.0 0.5 1.0 0.0 0.3 37.7 0.7 0.2 2.8 1.5 0.3 1.0
B.5Minors 3.7 1.8 1.2 1.2 0.6 0.6 0.6 1.1 0.8 0.5 0.3 0.5 0.0 2.3 1.0 1.8 2.7 1.3 1.8 1.7 2.2
BRuntimeBug 5.5 7.9 3.7 1.8 0.6 11.6 1.2 7.1 5.3 4.0 5.0 4.0 2.3 5.3 43.3 12.5 16.8 21.0 7.2 9.8 7.7
C.1MisunderstandingandLogicError 29.3 29.9 34.1 31.1 12.8 20.7 17.1 19.0 18.8 19.8 20.9 12.0 15.5 13.8 26.7 48.3 44.5 65.7 31.3 46.5 37.5
C.2Hallucination 0.0 1.2 0.0 0.0 0.6 0.0 1.2 7.4 7.7 10.3 8.2 1.8 5.8 3.0 5.3 7.5 10.0 0.5 7.0 3.0 4.5
C.3Input/OutputFormatError 0.0 0.0 0.0 0.0 0.0 0.0 1.2 2.6 1.3 1.1 0.5 2.8 0.8 2.8 3.7 2.8 1.2 2.3 0.3 2.7 1.7
C.4Minors 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.3 0.0 0.8 0.3 1.7 2.0 2.7 0.7 0.7 2.7 2.0
CFunctionalBug 29.3 31.1 34.1 31.1 13.4 20.7 19.5 29.1 27.8 31.7 29.9 16.5 22.8 19.8 37.3 60.7 58.3 69.2 39.3 54.8 45.7
DAmbiguousProblem 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.8 0.8 0.8 0.8 0.8 0.8 0.8
someunittestsbutnotall.Infiniteloopreferstothecodefailing Finding5: AmongallLLMsandbenchmarks,functionalbugs
tomeettheloopexitconditionsundercertaininputs,causingthe constitutethehighestproportion,whereassyntaxbugsmake
codetorunindefinitely. upthelowest.Furthermore,asthecomplexityofthebenchmark
increases,theproportionoffunctionalbugsalsorises.
------ Problem ------
Write a function that returns the list in a list of lists whose
sum of elements is the highest.
assert max_sum_list([[1,2,3], [4,5,6], [10,11,12], [7,8,9]]) Amongallsecondarybugtypes,misunderstandingandlogic
==[10, 11, 12]
------ Incorrect Code ------ errorsmakeupthehighestproportion,whichisconsistentacross
def max_sum_list(lists): all LLMs and benchmarks. Furthermore, the more complex the
max_sum = 0 # Incorrect initialization
benchmark,themorepronouncedtheLLMs’shortcomingsinun-
max_list = None
for lst in lists: derstandingcapabilitiesbecome.Wefurthersubdividedthisbug
current_sum = sum(lst) typeintosixlabels:Test-case-drivenCodeGeneration,MissingChecks
if current_sum > max_sum: forCornerCases,ReferenceRelationshipMisunderstanding,Incorrect
max_sum = current_sum
max_list = lst ConditionalBranches,SpecificConceptionMisunderstanding,and
return max_list ResidualLogicMisunderstanding,asshowninFigure4.Test-case-
----- Canonical Solution -----
def max_sum_list(lists): drivencodegenerationindicatesthattotheLLMgeneratingcode
return max(lists, key=sum) solelybasedontheinputandoutputofthetestcasesprovidedin
theproblemdescription,ignoringtheactualfunctionality.Missing
checksforcornercasesindicatesLLMsgeneratefunctionallycorrect
codebutfailtohandleedgecases.Referencerelationshipmisunder-
4.2 BugAnalysis
standingindicatesLLMsmisinterpretreferencerelationshipssuch
Wefurtheranalyzedthedistributionofeachprimaryandsecondary asnumericalrelationshipsandorderofoperations.Incorrectcondi-
bugtype,asshowninTable3.Alldatainthetablerepresentthe tionalbranchesreferstotheLLMonlyimplementingorcorrectly
proportionofeachbugtypewithintheentirebenchmark.Among handlingsomeconditionalbranches.Specificconceptionmisun-
thethreeprimarybugtypes,syntaxerrorsconstitutethesmallest derstandingreferstotheLLMincorrectlyunderstandingspecific
proportion,whereasfunctionalbugsarethemostprevalent.As conceptsdefinedintheproblemdescription.Thefivelabelsabove
datasetcomplexityincreases,theincidenceoffunctionalbugsalso indicatethatLLMsgenerallycomprehendproblemdescriptionsbut
rises.MostLLMshaveasyntaxbugproportionoflessthan10%, makespecificunderstandingerrors.Residuallogicmisunderstand-
whereasDeepseekCoder,LLama-3,Phi-3,andGPT-3.5haveafunc- ingreferstoanyremainingunderstandingerrorsnotcoveredby
tionalbugproportionexceeding50%ontheAPPS+.Thissuggests theaforementionedfivecategories,aswellassituationswherethe
thatcurrentLLMshaveeffectivelymitigatedsyntaxbugsincode understandingiscorrectbuttheLLMfailstoconstructacorrectly
generation.FutureeffortsshouldprioritizeenhancingtheLLMs’ logicalimplementationofthecode.Thelastlabelindicatesthat
comprehensioncapabilities. LLMsmisunderstandthewholeproblemdescription.
8Bug Types
C.1.1 Test-case-driven Code Generation C.1.3 Reference Relationship Misunderstanding C.1.5 Specific Conception Misunderstanding
C.1.2 Missing Checks for Corner Cases C.1.4 Incorrect Conditional Branches C.1.6 Residual Logic Misunderstanding
C.1.2 C.1.2 C.1.6 C.1.2
54.4% 53.2%
30.1% 69.1%
4.5% C.1.3 C.1.3
1. 14 .% 4% CC .. 11 .. 51 331 .. 0. 29 %%% CC .1.1 .5.4 3.1% C.1.3 1.60 1%. .8 2% %C.1.1
58.9% 2.8% C.1.3 10.8% 12.8% C.1.5
38.3% C.1.1 30.4%
13.2% 2.1% C.1.2 C.1.4
C.1.6 C.1.6 C.1.11.6% C.1.5 C.1.6
C.1.4
HumanEval Plus MBPP Plus APPS Plus RWPB
Figure4:Distributionofmisunderstandingandlogicerror.
FromFigure4,itcanbeobservedthatthemissingchecksfor APIs,includingtheattributesofthecaller,thetypesofAPIargu-
cornercasesaresubstantial,accountingfor54.4%inHumanEval+ ments,andtheappropriaterangeofargumentvalues.Therefore,
and30.1%inMBPP+.Meanwhile,residuallogicmisunderstanding futureworkcanfocusonimprovingtheLLM’sinferenceofvariable
ismost prevalent inthe APPS+,where itreaches 69.1%,with a attributes,typesandvaluestopreventAPImisuse.
relativelyhighproportioninHumanEval+andMBPP+.Thisindi-
catesthatLLMsfinditmorechallengingtounderstandproblem Finding8: APImisuserepresentsthemostcommonruntime
descriptionsinthemorecomplexAPPS+.Incontrast,theirper- buginbenchmarksandLLMs,necessitatingaccurateinference
formancesimproveonHumanEval+,sincetheycangraspmore ofcallerattributes,argumenttypes,andvalueranges.
problemdetailseffectively.
TimeouterrorsindicatethatwhiletheLLM-generatedcodeiscor-
rect,thealgorithm’simplementationisoftensuboptimal,typically
Finding6: AmongallsecondarybugtypesinLLMsandbench-
withhightimecomplexity,preventingtheprogramfromfinish-
marks,misunderstandingandlogicerrorsarethemostpreva-
ingwithinthetimelimit.Formorecomplexalgorithmicproblems,
lent,andthedistributionofthesixsub-categorieshighlights
LLMstendtouseloopsordeepsearcheswithoutoptimizedexit
LLMs’weaknessesincomprehensiveproblemunderstanding.
conditions.Additionally,thefrequentuseofrecursivealgorithms
significantlyincreasesthetimecomplexity.
Additionally,wefoundthatasproblemdescriptionsinbench-
m i i p dnn l eea cp txor auk m it p ls - srpol o [e lu be 2n t t l 2pg ee ,u mt sh 4t y 1e df n ]n o e ,tr saa am cxn nra id s dpt trb t tr iu he e occ eq nto u u s lm ei r mr nee e s a gm .m k tT he eo hn ir oi tte s fs hc i ta aso hn rbm edd e ep gca rl eare f nue ox s er, m e rL t aho lL o ter eM ne dLgs l Lei cks Mr ot er a dslu ny etg d ot cg o m arle e ng o mt e r lo n ee e amem c dr o bae m tete oe rt - 0 1 2 3 4
5
- d- e- f- i e- f f li sI b n en ( r r:c n e e<o : t t=r u ur i r r1e n n n: -c t 2t ) n
f
): iC bo (d ne -1- )-- +--
fib(n
0 1 2 3 4
5
6
d- e- f- i i a f- f f f , o- i rb n n bC ( a_a n ,= < =n : = = io bn1n i ,0 2 ,i n : : rc t =a1a ) nr rl : bge e ,et tS u u (o ar r 3l n n ,u +t 0 1 ni bo +n 1- )- :---
bugslikeunmatchedparentheses.Thisindicatesthatdespitethe 7 return b
largecontextwindowsofcurrentLLMs(asshowninFigure1),their
abilitytohandlelongtextsisstillinsufficient,leadingthemtoeasily Finding9: Forcomplexproblems,LLMsoftenfailtogenerate
forgetboththeproblemdetailsandthecontentofthecodealready optimalalgorithms,frequentlyusingloopsordeepsearches
generated. withoutoptimizedexitconditionsorrecursivealgorithms,lead-
ingtotimeouterrors.
Finding7: DespitethelargecontextwindowsofcurrentLLMs,
theircapacitytohandlelengthyandcomplexproblemsisstill Incorrectargumentbugtypeisuncommon,butintheAPPS+,
limited. They struggle to retain all problem details, and the 37.7%ofStarCoder-2’scodecontainsthisissue.Specifically,while
extendedcodegenerationresultsinmoresyntacticalerrorslike StarCoder-2understandsinputformats,itfailstogenerateredun-
unmatchedparentheses. dantargumentsasrequiredbytheproblemstatement.Theredun-
dantargumentreferstoargumentthatcanbeinferredfromother
APImisuseaccountsforthehighestproportionofruntimebugs parameters.AsillustratedinthefinalcodeexampleinSection4.1.3,
acrossmostbenchmarksandlargelanguagemodels(LLMs).There theincorrectcodedoesnotfollowinputrequirementsandignores
arethreeprimaryerrorsassociatedwithAPImisuse:AttributeError thefirstargument,whichcanbeinferredfromthelengthofthe
(20.9%),TypeError(50%),andValueError(26.9%).AnAttributeError secondargument.
occurswhenaninvalidreferenceismadetoamissingAPIattribute.
ATypeErrorariseswhenanAPIisappliedtoanobjectofanin- Finding10: IncorrectargumentisrareinmostLLMswhile
correcttype,whileaValueErroroccurswhenanargumentofthe StarCoder-2usuallyfailstogenerateredundantargumentsre-
correcttypebutinappropriatevalueisprovidedThus,LLMsmust quiredbytheproblemstatement.
accuratelyinfermultiplefactorssimultaneouslywheninvoking
9GitHub thenrankedtheretainedrepositoriesindescendingorderofthe
numberofstars,andultimatelyobtained600practicalrepositories.
(1) Repository Selection (2) Function Filtering. We extracted functions from these
projectsandwroteautomatedscriptstofilteroutemptyorini-
Repositories
tializationfunctions.Wealsoremovedfunctionsthatincorporated
privateAPIsanddependencies,exceeded100linesofcode,and
(2) Function Filtering
containedmanyconstantslikestringsandnumbers.Wefoundthat
Functions (Signature, Canonical Solution) mostoftheretainingfunctionsexistedintheutilityfilesofprojects.
Inthisstage,weobtained2,307candidatefunctions.
(3)FunctionDe-Duplication.Developersfrequentlyconstruct
theirprojectsusingexistingpopulartoolsandcoderepositories.
Code Corpora However,thesecodesmayresultindataleakagesincecurrentLLMs
(3) Function De-Duplication
Functions aretrainedonalargefractionofGitHubandlibraries.Tosolve
thischallenge,weutilizedtheMinHashalgorithm[14]toensure
theabsenceofduplicationbetweenfunctions,aswellasbetween
Double-check
&GPT-4 Aidin (4) Human Annotation thesefunctionsandcodebases,includingopen-sourcebases(i.e.,
+ Docstring RedPajamaDatav2,Stackv1[39]andv2[48],RefinedWeb[58],and
(5) Unit Tests Synthesis Pile[25]),andourinternalcodetrainingcorpora.Thismethodsig-
+ Test Cases nificantlyreducesthepotentialfordataleakageinourbenchmark.
Finally,weretained316candidatefunctions.
(4) Human Annotation. After removing potentially leaked
RWPB functions,weannotatedthedocstringforthesefunctions.Ween-
gagednineexpertstomanuallyannotateeachfunction’sdocstring,
Figure5:TheprocessofconstructingRWPB. includingdescriptionsofitsfunctionalityandinput-output.Thean-
notateddocstringfollowsthreecriteria:naturalness,accuracy,and
5 CodeGenerationinReal-WorldProjects comprehensiveness.Specifically,thedocstringshouldfollowthehu-
manwritingstyle,makingitnaturallyreadablefromadeveloper’s
Constructingareal-worldcodegenerationbenchmarkiscrucialfor
perspective.Additionally,itshouldcomprehensivelydescribethe
evaluatingtheeffectivenessofLLMsingeneratingcodeapplicable
function’sfunctionalityandinput-outputparameters,suchascon-
toreal-worldprojects.Moreover,thedistributionofbugsinthe
taininginput-outputtypesandcoveringallconditionalbranches.
incorrectcodemayvarybetweentheexistingpopularbenchmarks
Finally,thedocstringshouldbeclear,preciseandconcise,avoiding
andthereal-worldbenchmark,emphasizingtheimportanceofthe
anyredundantandirrelevantinformation.
real-worldbenchmarkinunderstandingandimprovingLLMper-
Eachdocstringinvolvedadualannotationprocessandthree
formanceinreal-worldscenarios.Inthissection,wefirstintroduce
annotators:oneexpertmanuallywroteadocstringandtheothers
theprocessofconstructingourreal-worldbenchmarkRWPB.Sub-
providedameticulousdouble-check.Ifthetwoexpertsresponsible
sequently,weevaluatewidelyusedLLMs’performanceincode
forthedouble-checkprovideddifferingresults,thedrafteddoc-
generationonRWPBanddiscussthechallengesandlimitations
stringwouldbemanuallyrefineduntilaconsensuswasreached.
inherentintheseLLMsforreal-worldprojects.
Duringthisprocess,expertswereallowedtoutilizeGPT-4toassist
withannotationtasks.Specifically,thefirstexpertwaspermittedto
5.1 BenchmarkConstruction
useGPT-4tohelpsummarizethefunctionality,whileotherexperts
OurconstructedbenchmarkRWPBcontains140real-worldpro- wereallowedtouseittoaidinthecheckprocess.Expertsalsoelim-
grammingrequirementscollectedfromGithub.Eachrequirement inatedfunctionsthatexhibitedaconglomerationoffunctionalities
includesafunctionsignature,docstring(i.e.,descriptionsofthe andweredifficulttosummarizeexactlyandthoroughly.Finally,
function’sfunctionalityandinput-output),functionbody,andunit thisstageresultedintheretentionof140functions,eachcontaining
testcases.AsshowninFigure5,theconstructingprocessincluded anaccurateandcomprehensivedocstring.
fivesteps: (5)UnitTestsSynthesis.Weengagedsixexpertstocreateunit
(1)RepositorySelection.Currently,commonlyusedbench- testcasesforeachfunction.Thegenerationoftestcasesisbasedon
marksareconstructedfromprogrammingquestions,whichare thefollowingkeycriteria:Eachfunctioncontainsatleastthreetest
disconnectedfromreal-worldprogramdevelopment.Inaddition, cases;thesetestcasesareensuredtocovertheentirecodeofthe
otherreal-worldbenchmarksaresourcedfromwidelyusedreal- functionbody;andtheycompriseregularandboundarycases.In
world repositories and libraries. These benchmarks suffer from addition,asinStageFour,weimplementedadouble-checksynthesis
datacontaminationandleakageduetosignificantoverlapwith process.Specifically,foreachfunction,oneexpertundertookthe
thedatasetsusedfortrainingLLMs[16,28,61,62].Toaddressthe taskofgeneratingtestcases,whilesubsequently,twootherexperts
shortcomingsofcurrentbenchmarks,wecollectedPythonreposi- validatedthesynthesizedcasesandcheckedwhethertheycould
toriesestablishedin2024onGitHub.Specifically,wefocusedon coverthefullcanonicalsolutionofthisfunction.Attheendofthe
repositoriescreatedbeforeApril15,2024,andfilteredoutthose stage,onaverage,eachfunctioncontained4.9testcases.
withlicensesthatrestrictedopen-sourceorcommercialuses.We
10Finally,weselectivelyretained140functionstoconstructour with HumanEval+, MBPP+, and APPS+. Similar to the existing
real-worldbenchmarkRWPB.Eachfunctioncontainsafunction benchmarks,theproportionofsyntaxbugsinRWPBisthelowest,
signature,anaccurateandcomprehensivedocstring,acanonical indicatingthatLLMscaneffectivelyavoidsyntaxbugsinreal-world
solution,andanaverageof4.9testcases. scenariosaswell.Furthermore,Figure2demonstratesthatthecom-
mentnumberbetweencorrectandincorrectcodeinRWPBexhibits
Table4:Typesofbugsintroducedduringcodegenerationby thesametrendastheexistingbenchmarks:incorrectcodecontains
sevenpopularLLMsonourconstructedreal-worldbench- morecommentsthancorrectcode.What’smore,thedistribution
markRWPB.Allvaluesarein%. of misunderstanding and logic error is similar. Figure 4 shows
thatmissingchecksforcornercasesarethemostprevalentsub-
Open-Source Closed-Source
Bug categories(53.2%),followedbytheresiduallogicmisunderstanding
Types StarCoder2 DeepSeekCoder Llama3 Phi3 GPT4 GPT3.5 Claude3
(30.4%).However,unlikeexistingbenchmarks,theproportionof
PASS 31.4 32.9 22.9 22.1 44.3 34.3 45.7
runtimebugsishigherforallLLMs,particularlyforincorrectargu-
A.1 0.0 0.0 0.7 0.0 0.7 0.7 0.0
mentsandincorrectboundaryconditionchecks.
A.2 2.1 0.0 0.7 2.9 2.1 2.1 0.0
A.3 4.3 2.9 2.9 2.1 2.9 4.3 1.4 Finding12:ForRWPB,LLMsalignwithexistingbenchmarksin
A 6.4 2.9 4.3 5.0 5.7 7.1 1.4 mostparts.However,theproportionofruntimebugs,especially
B.1 7.9 7.1 10.7 10.0 2.9 5.0 2.9 incorrectargumentsandincorrectboundaryconditionchecks,
B.2 1.4 0.7 0.7 5.7 1.4 2.1 7.9 issignificantlyhighercomparedtoexistingbenchmarks.
B.3 5.0 4.3 3.6 2.1 1.4 3.6 2.9
B.4 12.9 19.3 19.3 16.4 7.1 12.1 7.1 We further analyzed the incorrect code with incorrect argu-
B.5 0.0 1.4 3.6 2.9 0.0 1.4 1.4 ments.Wefoundthat,differentfromexistingbenchmarks,real-
B 27.1 32.9 37.9 37.1 12.9 24.3 22.1 worldprojectscontainarichervarietyofargumenttypes,suchas
C.1 27.1 21.4 24.3 28.6 26.4 25.7 25.0 multi-dimensionaltensors.Asaresult,generatedcodeoftenfails
C.2 6.4 5.7 7.1 4.3 5.0 6.4 0.0 duetodimensionmismatchesinthesearguments.Thissuggests
C.3 1.4 2.1 0.7 0.7 2.1 1.4 3.6 thatLLMsneedtoperformmoreeffectiveinferenceonparameters
C.4 0.0 2.1 2.9 2.1 3.6 0.7 2.1 toavoidincorrectargumentsinreal-worldprojects.
C 35.0 31.4 35.0 35.7 37.1 34.3 30.7
Finding13: RWPBinvolvesdiverseargumenttypes,leading
highproportionofincorrectarguments.
5.2 EffectivenessinRWPBandBugsAnalysis
5.2.1 Effectiveness. As shown in Table 4, among closed-source Inreal-worldprojects,handlinghigh-dimensionalvariablessuch
LLMs,Claude-3hasthehighestpassrateat45.7%,followedbyGPT- asTensorarraysoftenrequiresextensiveboundarychecksacross
4at44.3%.Foropen-sourceLLMs,StarCoder-2andDeepSeekCoder multipledimensions.Forinstance,considerthefollowingcodethat
havethehighestpassratesat31.4%and32.9%,respectively,while generatesandfillsamasktofitagivencontextwindow.However,
Llama-3andPhi-3havelowerpassratesat22.9%and22.1%,respec- itneglectsboundarychecksforthetokendimension.Whenthe
tively.Open-sourceLLMshaveaslightlyloweroverallpassrate tokenisaone-dimensionalarray,itcanleadtoout-of-boundsaccess
comparedtoclosed-sourceLLMs,particularlyperformingworse issues.
inruntimebugs.Thisindicatesthatclosed-sourceLLMscaneffec- ----- Incorrect Code -----
tivelyavoidruntimebugsingeneratedcode,whichalsomeansthey import numpy as np
aremorelikelytoexposefunctionalbugsduetotheannotation def pad_tokens(tokens, context_window, pad_token):
max_seq_len = np.max(tokens.shape)
priority.ThedifferencesamongLLMsinfunctionalbugsaremini- # tuple index out of range
mal.Wefurtherinvestigatedtheoverlapoffunctionalbugsforeach mask = np.arange(max_seq_len)[None, :] < tokens.shape[1]
LLM.AmongthesevenLLMs,100problemsfunctionalbugsand
outofthese,onlysevenproblemshavefunctionalbugsreportedby Finding14: ForRWPBthatinvolveshigh-dimensionalvari-
everymodel.Thisindicatesthat,forreal-worldprojects,different ables,itiscrucialfortheLLMtoperformboundarycheckson
LLMshavevaryingareasoffocusinreal-worldscenarios.This dimensioninformationtoenhancecoderobustness.
variabilityislikelyduetodifferencesinthesourceoftrainingdata,
LLMsize,trainingmethods,andhyper-parameterdetails,which 6 MitigatingBugsinGeneratedCode
resultinvaryinglevelsofunderstandingacrossmodels.
Inthissection,wediscusshowLLMsidentifyandfixbugsintheir
Finding 11: Closed-source LLMs outperform open-source incorrectcodeintheinferencetime.
LLMsincodegenerationqualityforreal-worldprojects,partic- Ourproposedmethodisbasedonself-critiqueandiscost-
ularlyexcellinginreducingsyntaxbugsandruntimebugs.For effective.Self-critiqueistheprocessbywhichLLMscritiqueand
functionalbugs,differentLLMsperformsimilarly,witheach reviewtheresponsestheygeneratetoenhancetheirunderstanding
modelencounteringerrorsondifferentproblems. andreasoningabilityinproblem-solving[27,29,64,69,74].Wang
etal.[74]introducedself-criticismduringthefine-tuningphaseto
5.2.2 BugAnalysis. Weanalyzedthebugdistributioninthereal- reducetheharmfulresponsesgeneratedbyLLMs.Saundersetal.
worldprojectbenchmark,RWPBandcomparedthedistribution [64]fine-tunedLLMstogeneratenaturallanguagecritiquestohelp
11wh
b m d i
b
t
bn huy e
eu
ea g es
tgm
kt
n
tcr sLh ei
rea
n rL miie -
nn
pg ds M
uats
e e tiL
k
nhsosfi
m L
e
di en
an
gM ee , tn
cnd
h
rt i oash n
setfl
dl tco ch y
m
aea
r od ze
n.iw
r ets
rB
drpi,
es
q e t arw -hu c
tsoi
c
hn
t e eee m
o
edc rt
ra
i p oh rn o
o
pedt
te ot
n
ro
cr et t oco
tp
e , to c bhd
mi
a
tac
d
h
leu n
eu-
pe ec
db
mr, sle oa
ca
ea o
o
ots
sns
auee
te dd
nord
l
c
et ff dt o c a-
hs
tbo uc
hu
oce unr so en
rm
i egst
y
in t si
gsfim
rq t
g
iaux a au
nna
ei nce t an
nr
dth d
li
e
eiz
e t ln d i
r
eh
oa
s d
a
re
ct
e bt e
t
ri
h au
eo
b n oo te
dgun
ritr oi .i ig fcn cg
nt
Tys
aa
oif s.
otnes
l dS e
ook
r a
h
egpe
fcl.
,eoen a
bI
p
l
wc
rn
tc pur yi
ies
ofi o
g
e,
Lp
b n sct m
L
ai
i ,a l
sr
m e
M
lwale
sl om
kyed
oe
sf-, , )d lOC (F gu un Bc eti mo in tnal uRB 22u B33g 3377 11( %%00 %%O 44%%l 6622d %%) 2288 %% 22%%
55%%
77B 55R %%untimeBug
guF Bix le ad
noitcnuFC
lOd)
(guB
emC itF nu un Rcti Bo 11 33n 66a 22l %%
%%
55B 22u 99g %%33(
%%Old 44)
%% 33%%
BRuntimeBug
Fixed guBlC aF nu oin tc
incorporatethefeedbackofincorrectcodefromthecompiler(i.e., )dlO(guBxatnySA )dlO(guBxatnySA
errormessagesfromfailedexecutions)withintheprompttemplate.
Figure7:Resultsoffixingbugsbyusingourself-critic-based
Thewholeprocessisfullyautomated.
approach.
Your task is to analyze the root causes and identify the locations Input Effectivenessofourproposedmethodinmitigatingbugs.
of bugs in the provided incorrect code, and re-correct the code Toevaluatetheeffectivenessofourfixingbugsmethod,wecon-
according to the original programming problem, the bug
ductedextensiveexperimentsacrossthepriorsevenLLMs.Foreach
category, and the compiler feedback.
LLM,werandomlyselected120incorrectcodesgeneratedbythe
Original programming problem: respectivemodels,thenutilizedourmethodsofself-critiqueand
$DOC_STRING
self-revisiononthecodetofixbugs.Weconductedtwoiterations
Incorrect code: ofourmethodtofixbugsoneachincorrectcode.Theperformance
$INCORRECT_CODE onGPT-4isshowninFigure7andresultsfortheothersixLLMs
areavailableinourartifactrepository[1].)Theresultssuggestthat
Bug category:
$BUG_CATEGORY ourproposedmethodcaneffectivelyidentifyandfixbugsinincor-
rectcode,therebyenhancingthesuccessrateofcodegeneration
Compiler feedback:
usingLLMs.Specifically,intheinitialiteration,GPT-4fixes24.1%
$COMPILER_FEEDBACK
(29in120)ofincorrectself-generatedcodethroughtheproposed
Based on this given information, please identify the root causes method.Afurther6.6%(6in91)ofthecodesarecorrectedinthe
and locations of the bugs, then re-correct the code:
seconditeration.Thesyntaxbugsarecompletelyfixedinthefirst
$CAUSE_AND_CODE Output iteration.Forruntimebugs,23%oferrorcodesarecorrectedinthe
firstiterationand16%intheseconditeration.Forfunctionalbugs,
Figure6:TheprompttemplateofLLMstocritiqueandre-
23%arefixedinthefirstiterationand3%intheseconditeration.
correcttheincorrectcode.
Figure7alsopresentstheevolutionofbugsforGPT-4.Forrun-
timebugs,46%and32%aretransformedtofunctionalbugsatthe
The simplified prompt for critiquing and fixing is shown in
Figure6.$DOC_STRINGand$INCORRECT_CODEdenotetheoriginal endofthefirstandseconditerations,respectively.Conversely,the
functionalbugsarehardertotransformtothetwoothertypesof
programmingproblemandtheincorrectcodegeneratedbyLLM.
$BUG_CATEGORY contains our constructed bug category and the bugs.22%and4%functionalbugsarefixedandonly8%and3%
naturalinterpretationofbugtypes.$COMPILER_FEEDBACKisthe bugstransformedtoruntimebugsaftertwoiterations,respectively.
Experimentalresultsshowthatourapproachcaneffectivelyfixall
outcomeofthecompilerexecution.Iftheresultbelongstotype
typesofcodetoimprovetheperformanceofLLMsincodegenera-
A(i.e.,SyntaxBug)andtypeB(i.e.,RuntimeBug),weprovidethe
tion,whilefacilitatingthetransitionofsyntaxandruntimebugs
errorlog;ifthecompilerdoesnotreturnanerrorbutthecodedoes
towardsfunctionalbugs.
notpasstheunittests(i.e.,FunctionalBug),weprovidethemessage
Additionally,althoughexperimentsdemonstratethatourmethod
“Thefunctionalityofcodeisincorrect.”;andiftheexecutionofthe
improvesLLMcodegenerationthroughself-critique,furtheren-
codeexceedsthetimelimit,weofferthemessage“Theexecutionof
thecodehasexceededthetimelimit.”$CAUSE_AND_CODEdenotes hancementscanbepursued.Fromanartificialintelligencestand-
point,introducingcommentsincodetrainingcorporaandemploy-
therootcausesandlocationsofbugs,andthere-correctedcode
ingfew-shotpromptlearningduringtheinferencephasecould
generatedbyLLM.Thefinalversionofthepromptsisavailablein
enhancecodequality.Fromasoftwareengineeringperspective,in-
ourartifactrepository[1].TheLLMusedforcritiquingcodeand
corporatingcompilerfeedbackwithpost-trainingcouldhelpLLMs
fixingbugsisthesamemodelutilizedforgeneratingcodebased
betterunderstandcode-relatedtasks.
onhumanrequirements.Consequently,ourmethodeliminatesthe
needforadditionalpost-trainingcosts.
7 Discussion
Ourproposedmethodisinteractiveanditerative.Weex-
tractthere-correctedcodefrom$CAUSE_AND_CODEthroughscripts BenchmarkAnalysis.HumanEval+andMBPP+arewidelyused
andthenverifythecorrectnessofthecodebyinteractingwiththe benchmarksincodegenerationwithhigh-qualityproblemdescrip-
compiler.Ifthere-correctedcodestilldoesnotpassunittests,we tions,diverseunittests,andformattedinput/output.TheAPPS+,
updatetheprompttemplatewiththelatestincorrectcodeandthe sourcedfromvariousalgorithmonline-judgewebsites,presents
compileroutcometoconductthenextre-correctiteration. high-difficulty problems. However, algorithmic problems result
12inlengthyandoftenredundantdescriptions(e.g.,extensiveback- 8 ThreatstoValidity
groundtointroducetheproblem),complicatingcomprehensive Inthissection,wediscusssomesignificantthreatstothevalidity
understandingbyLLMs.Furthermore,thealgorithmicproblems ofourresults.
originallyincludedimagesastheproblemexplanation,butthese DatasetValidity.Thefirstconcerninvolvespotentialissues
imagesaremissingintheAPPS+,furtherincreasingthedifficulty with the popular benchmarks we used. APPS+ contains a wide
formodelstounderstandthetasks.Toensureexperimentquality, rangeofprogrammingproblemswithvaryingdifficulty,whichcan
wemanuallyexcludedproblemswithmissingimages. thoroughlytestthecodegenerationcapabilitiesofLLMs.However,
AnnotationProcess.Duringtheprocessofanalyzingandla- therearecertainissueswiththisbenchmark.First,theproblem
belingbugtypesinincorrectcode,expertsareallowedtoutilize descriptionsoftenincludealotofbackgroundinformationirrel-
GPT-4toaidinannotationtasks.Specifically,forfunctionalbugs evanttothecorealgorithm,whichcaninterferewiththeLLM’s
thatlackdetailedcompilererrorinformation,weemployGPT-4to understandingoftheproblem.Furthermore,somedescriptionsuse
generatethreepotentialrootcauseswithexplanations,facilitating imagesforexplanations,buttheseimagesaremerelyrepresentedby
theanalysisandclassificationofbugtypes.Althoughthefinalan- “[image]”,whichpreventstheLLMfromfullycomprehendingthe
notationsaremanuallyconductedandsubjecttodouble-checking, problem.Toavoidtheseissues,wemanuallycheckedAPPS+and
weareinterestedinexploringthedifferencesineffectivenessand carefullyremovedcaseswithinaccurateorincompletedescriptions.
efficiencyamongthreeannotationmethods:fullyautomated,fully Anotherissueisthatallbenchmarksweusedweremonolingual,
manual,andAI-assisted,inidentifyingbugtypesingeneratedcode. i.e.,theprogramminglanguageofcanonicalcodewasinPython.
Weconductanexperimenttoevaluatedifferentannotationmeth- In practice, current commonly used benchmarks including Hu-
odsforbugtypesinincorrectcode.Werandomlyselect100incor- manEval+[16],MBPP+[11]andAPPS+[31](thesebenchmarks
rectcodesandengagefiveexpertswhohavenotlabelledthese alsobeingthosewhichweusedinourstudy)exclusivelyfeature
codes,tomanuallyannotatethebugtypesseparatelywithoutAI Python,andresearchersalsofocusonenhancingLLM’scodegen-
assistance.Anothergroupoffiveexpertsannotatesthesamecodes, erationcapabilitiesinPython[12,17,45,103].Consequently,the
butwiththeassistanceofGPT-4,whichprovidespotentialroot resultsofLLMsonthesebenchmarksareindicativeoftheiroverall
causesforthebugs.Additionally,weincorporateabugtaxonomy codegenerationproficiency,andourfindingscanbegeneralizedfor
intheprompttoguideGPT-4inanalyzingandclassifyingthebugs applicationinotherexperimentalsettings.Inthefuture,weplan
accordingtothetaxonomy.Foreachannotationmethod,wecom- toinvestigateandanalyzelimitationsfacedbyLLMsingenerating
paretheaverageresultwiththedouble-checkingresults.When otherprogramminglanguages.Thefinalissueisthatthebench-
annotatingtertiarycategories,theaccuracyoffullymanual,AI- marksmaycausedataleakage[28,35,61,62,66,85,102]ascurrent
assistedandfullyautomatedare91.7%,84.4%and5.2%,respectively. LLMsaretrainedonalargefractionofGitHub,StackOverflowand
ResultssuggestthatAI-assistedannotationcanimprovetheaccu- codelibraries[16].Inourprocessofbenchmarkconstruction,we
racyofannotationcomparedtofullymanualannotation,which collectedsamplesfromthelatestcoderepositoriesandutilizeda
isaconclusionsimilartothefindingsofSaundersetal.[64]on de-duplicationalgorithm(i.e.,MinHash[14])oncodetrainingcor-
thetopic-basedsummarizationtask.Moreover,GPT-4isnotyet poraincludingopen-sourcebases(i.e.,RedPajamaDatav2,Stackv1
abletoreplacethemanualannotationtoachievefullyautomated [39]andv2[48],RefinedWeb[58],andPile[25])andourinternal
annotationinourtask.Furthermore,weevaluatetheaccuracyof codetrainingcorpora,tomitigatedataleakage.
fullyautomatedannotationonsecondarycategories,whichstands External Validity. Threats to external validity relate to the
at35.7%.ThisindicatesthatGPT-4canprovidecoarse-grainedbug abilityofourresultstobegeneralizedaccordingtodifferentscenar-
categoriesofbugstoaidexpertannotation.Ontheotherhand, ios.Weaddresstheseconcernsbyselectingadiversesetofseven
AI-assistedcanenhancetheefficiencyoftheannotationprocess. state-of-the-artLLMs,includingtwoopen-sourcetexturalLLMs,
Theexpertsprovidefeedbackthatannotatingwiththehelpofbug twoopen-sourcecodeLLMs,andthreeclosed-sourceLLMs.These
causesgivenbytheGPT-4canexpeditetheirlabellingtasks. LLMsarewidelyusedinavarietyoffields.Regardingbenchmarks,
During the annotation process, we also observe that a small weusethreewell-knowndatasets.Wealsocarefullyconstructa
proportionofannotatorsheavilyrelyontherootcausesprovidedby real-worldbenchmark,RWPB,toevaluatetheperformanceofLLMs
GPT-4,neglectingmanualanalysisofthebugsintheincorrectcode. onreal-worldcodegeneration.OurexperimentalsettingofLLMs
Werefertothisbehavioras“lazyannotation”.Tomitigatethisissue, adherestoprotocolsestablishedinpriorworks,whichensuresthat
weimplementastrategywhere10%oftheannotationsareselected ourresultsandfindingscanbegeneralizedforapplicationinother
fordouble-checkingmanually,andweemphasizetheimportanceof scenarios.
manualerrorcodeanalysistotheannotators.Thisphenomenonof InternalValidity.Whenlabelingbugtypesincodegenera-
“lazyannotation”appearstobecommonacrossAI-assistedlabeling tion,weusemanualcategorizationwiththeassistanceofGPT-4.
scenarios and presents a challenge to maintaining high-quality Thisapproachcanimprovetheaccuracyoftheprocessbutmay
annotations.Inthefuture,weplantofurtherinvestigatemethods cause“lazyannotation”(detailsinSection7).Additionally,differ-
toprevent“lazyannotation”invariousannotationtaskstoenhance entexpertsmayhavevaryingunderstandingsofbugtypes,which
boththeefficiencyandeffectivenessofthelabelingprocess. potentiallymighthaveresultedinlabelingerrors.Toensureconsis-
tency,weemploydouble-checkingamongdifferentexperts.This
methodhelpsusachieveaconsensusonbugtypes,avoidingin-
correctlabeling.Furthermore,whenfacedwiththesameproblem,
13LLMsmaygeneratedifferingresponses.Toensuretheconsistency duringthefilteringphaseofconstructingRWPB,wede-duplicated
ofresponses,wesetthetemperatureto0.1andtop-ktoone,in datawithlargeopen-sourceandourprivatecodecorporatoavoid
accordancewithpriorcommonsettings[3,30]. data leakage. We also evaluated the distributions of bugs intro-
ducedbyLLMsbasedonourconstructedbugtaxonomy,which
furtherhelpsusunderstandhowtheyperforminreal-worldcode
9 RelatedWork
generationscenarios.
LargeLanguageModelsforCodeGeneration.Recently,the
advancementofLLMshassignificantlypropelledthefieldofcode 10 Conclusion
generation.Duetotrainingonlargecodecorpora,severaltextLLMs
Inthispaper,weconductacomprehensiveempiricalinvestigation
havedemonstratedremarkableabilityinunderstandingnaturallan-
intotheeffectivenessandlimitationsofcodegenerationusinglarge
guageandsynthesizingcode,asseeninmodelssuchasMistral[37],
languagemodels(LLMs).Wefirstconductextensiveexperiments
Qwen1.5[12],DeepSeek-LLM[19],theChatGPTfamily[2,55],the
frommultipleperspectivesonsevenwidely-usedLLMsacrossthree
Claude3family[10],andtheLLamafamily[4,5,76].Ontheother
widelyusedbenchmarks.OurresultsrevealthattheseLLMsstrug-
hand,codeLLMs,primarilytrainedonextensivecodedataincluding
gletogenerateaccuratecodeforthosemorecomplexproblems.
CodeGPT[49],SantaCoder[9],CodeGeex[98],CodeX[16],Wizard-
Subsequently,wemanuallyannotatebugtypesincodegenerated
Coder[50],CodeLlama[63],DeepSeekCoder[30],Phi-3-Instruct
byLLMs,constructataxonomyofthesebugs,analyzetheirdis-
[6],andStarCoder-2-Instruct[43],havealsoexhibitedremarkable
tributions,andsummarize14findingsleadingLLMstogenerate
capabilitiesincodegeneration.Researchershavefurtherimproved
erroneouscode.ToevaluatetheeffectivenessofLLMsinreal-world
theperformanceofLLMsoncodegenerationbypromptengineer-
projects,wedesignarigorousbenchmarkconstructionprocessto
ing[21,40,68]orbyintroducingsupervisedfine-tuning[13,36,86]
minimizedataleakageandconstructareal-worldprojectbench-
andreinforcementlearning[22,41,46,70,92,99].
mark,RWPB.Furthermore,tomitigatebugsduringcodegeneration
Bug Study in Software Engineering. The investigation of
byLLMs,weproposeanovelmethodthatintroducesself-critique,
bugshelpsdeveloperscomprehensivelyunderstandthecodetasks
enablingLLMstoiterativelycritiquetheirgeneratedcodesandfix
andthedefectsofmodels,andintroduceimprovementapproaches
bugs.
basedonfindings.Severalworksanalyzebugsintroducedindeep
learningmodelsandframeworks[26,33,78,96].Additionally,tech-
References
niquesbasedonLLMshavedemonstratedimpressiveabilitiesin
codingtasks.Thusseveralworkshaveanalyzedbugsintroduced [1] 2023.ArtifactRepository. https://github.com/LLMCodeGenerationStudy/LL
byLLMsinavarietyofcodingtasks[44,51,57,73,88,89].Incom- MCodeGenerationStudy.
[2] 2023.gpt-3.5-turbo. https://platform.openai.com/docs/models/gpt-3-5.
parisontothesestudies,westudiedbugsintroducedbyLLMsin [3] 2023.GPT-4TechnicalReport. https://cdn.openai.com/papers/gpt-4.pdf.
codegeneration.Tambonetal.[73]analyzedbugsintroducedby [4] 2023.Llama-2. https://ai.meta.com/research/publications/llama-2-open-foun
dation-and-fine-tuned-chat-models.
LLMswhilegeneratingnon-standalonecode,whichdiffersfrom
[5] 2024.Llama-3. https://ai.meta.com/blog/meta-llama-3.
ourstudy[73].Moreover,theLLMsusedinthisstudyareoutdated, [6] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,Ahmed
whichmayleadtoexperimentalfindingsthatdonotfitintocurrent Awadallah,HanyAwadalla,NguyenBach,AmitBahree,ArashBakhtiari,Harki-
ratBehl,etal.2024.Phi-3technicalreport:Ahighlycapablelanguagemodel
commonlyusedLLMs.Theyalsolackananalysisofdataleakage
locallyonyourphone.arXivpreprintarXiv:2404.14219(2024).
whenconstructingthebenchmark,asthedatamayhavebeenseen [7] WasiAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2021.
inthetrainingphaseofLLMs,whichwouldleadtoinaccuratere- UnifiedPre-trainingforProgramUnderstandingandGeneration.InProceed-
ingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssocia-
sults.Incontrasttothisstudy,weevaluatedthecodegenerated tionforComputationalLinguistics:HumanLanguageTechnologies,Kristina
byLLMsfrommultipleperspectives,understoodbugsintroduced Toutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-Tur,IzBelt-
agy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou
bythesemodelsandintroducedsomedirectmethodstoimprove
(Eds.).AssociationforComputationalLinguistics,Online,2655–2668. https:
thecodegenerationabilityofLLMsbasedonourfindings.Wealso //doi.org/10.18653/v1/2021.naacl-main.211
elaboratelyconstructedareal-worldbenchmarkwhileworkingto [8] DuaaAlawad,ManishaPanta,MinhazZibran,andMdRakibulIslam.2019.An
empiricalstudyoftherelationshipsbetweencodereadabilityandsoftware
avoiddataleakage,andcomprehensivelyinvestigatedthecapability
complexity.arXivpreprintarXiv:1909.01760(2019).
ofthesemodelsoncodegenerationforreal-worldprojects. [9] LoubnaBenAllal,RaymondLi,DenisKocetkov,ChenghaoMou,Christopher
CodeGenerationBenchmarks.Severalworkshaveproposed Akiki,CarlosMunozFerrandis,NiklasMuennighoff,MayankMishra,AlexGu,
MananDey,etal.2023.SantaCoder:don’treachforthestars!arXivpreprint
benchmarkstoevaluatethecapabilityofLLMsincodegeneration. arXiv:2301.03988(2023).
Somebenchmarksareconstructedfromcodeexercisesandintroduc- [10] AIAnthropic.2024.Theclaude3modelfamily:Opus,sonnet,haiku.Claude-3
ModelCard1(2024).
toryprogrammingproblems[11,16,45],orcompletionquestions
[11] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
[18,31].However,thesebenchmarksarefarremovedfromreal- Michalewski,DavidDohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,
worldprogramdevelopment.Somebenchmarksarecollectedfrom etal.2021. Programsynthesiswithlargelanguagemodels. arXivpreprint
arXiv:2108.07732(2021).
thepopularpubliclibrariesandcoderepositories[82,94],which
[12] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,Yang
maycausedataleakageduetosignificantoverlapwiththedatasets Fan,WenbinGe,YuHan,FeiHuang,etal.2023.Qwentechnicalreport.arXiv
usedfortrainingLLMs[16,43].Furthermore,severalresearchers preprintarXiv:2309.16609(2023).
[13] RamakrishnaBairi,AtharvSonwane,AdityaKanade,DCVageesh,ArunIyer,
aimtointroducenon-standalone[34,91],repository-level[42,47], SureshParthasarathy,SriramRajamani,BAshok,andShashankShet.2023.
andGitHubissue[38]benchmarks.Incontrasttothesebenchmarks, CodePlan:Repository-levelCodingusingLLMsandPlanning.InNeurIPS2023
FoundationModelsforDecisionMakingWorkshop.
ourproposedRWPBaimstoevaluatethecodegenerationcapability
[14] AndreiZBroder.1997. Ontheresemblanceandcontainmentofdocuments.
ofLLMsinreal-worldprojectsandstandalonescenarios.Moreover, InProceedings.CompressionandComplexityofSEQUENCES1997(Cat.No.
1497TB100171).IEEE,21–29. [35] NamanJain,KingHan,AlexGu,Wen-DingLi,FanjiaYan,TianjunZhang,
[15] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, SidaWang,ArmandoSolar-Lezama,KoushikSen,andIonStoica.2024.Live-
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda codebench:Holisticandcontaminationfreeevaluationoflargelanguagemodels
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural forcode.arXivpreprintarXiv:2403.07974(2024).
informationprocessingsystems33(2020),1877–1901. [36] NamanJain,TianjunZhang,Wei-LinChiang,JosephEGonzalez,KoushikSen,
[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde andIonStoica.2023. Llm-assistedcodecleaningfortrainingaccuratecode
deOliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph, generators.arXivpreprintarXiv:2311.14904(2023).
GregBrockman,etal.2021.Evaluatinglargelanguagemodelstrainedoncode. [37] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De-
arXivpreprintarXiv:2107.03374(2021). vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,
[17] OpenCompassContributors.2023. OpenCompass:AUniversalEvaluation GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint
PlatformforFoundationModels. https://github.com/open-compass/opencomp arXiv:2310.06825(2023).
ass. [38] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,Ofir
[18] JianboDai,JianqiaoLu,YunlongFeng,RongjuRuan,MingCheng,Haochen Press,andKarthikRNarasimhan.2023. SWE-bench:CanLanguageModels
Tan,andZhijiangGuo.2024. MHPP:ExploringtheCapabilitiesandLimi- ResolveReal-worldGithubIssues?.InTheTwelfthInternationalConferenceon
tationsofLanguageModelsBeyondBasicCodeGeneration. arXivpreprint LearningRepresentations.
arXiv:2405.11430(2024). [39] DenisKocetkov,RaymondLi,LIJia,ChenghaoMou,YacineJernite,Margaret
[19] DeepSeek-AI.2024. DeepSeekLLM:ScalingOpen-SourceLanguageModels Mitchell,CarlosMuñozFerrandis,SeanHughes,ThomasWolf,DzmitryBah-
withLongtermism.arXivpreprintarXiv:2401.02954(2024). https://github.com danau,etal.[n.d.]. TheStack:3TBofpermissivelylicensedsourcecode.
/deepseek-ai/DeepSeek-LLM TransactionsonMachineLearningResearch([n.d.]).
[20] EnriqueDehaerne,BappadityaDey,SandipHalder,StefanDeGendt,and [40] HungLe,HailinChen,AmritaSaha,AkashGokul,DoyenSahoo,andShafiq
WannesMeert.2022.Codegenerationusingmachinelearning:Asystematic Joty.2023.CodeChain:TowardsModularCodeGenerationThroughChainof
review.IeeeAccess10(2022),82434–82455. Self-revisionswithRepresentativeSub-modules.InTheTwelfthInternational
[21] PaulDenny,VirajKumar,andNasserGiacaman.2023.Conversingwithcopilot: ConferenceonLearningRepresentations.
Exploringpromptengineeringforsolvingcs1problemsusingnaturallanguage. [41] HungLe,YueWang,AkhileshDeepakGotmare,SilvioSavarese,andSteven
InProceedingsofthe54thACMTechnicalSymposiumonComputerScienceEdu- ChuHongHoi.2022.Coderl:Masteringcodegenerationthroughpretrained
cationV.1.1136–1142. modelsanddeepreinforcementlearning. AdvancesinNeuralInformation
[22] ShihanDou,YanLiu,HaoxiangJia,LimaoXiong,EnyuZhou,JunjieShan, ProcessingSystems35(2022),21314–21328.
CaishuangHuang,WeiShen,XiaoranFan,ZhihengXi,etal.2024.StepCoder: [42] JiaLi,GeLi,YunfeiZhao,YongminLi,HuanyuLiu,HaoZhu,LechengWang,
ImproveCodeGenerationwithReinforcementLearningfromCompilerFeed- KaiboLiu,ZhengFang,LanshenWang,etal.2024. DevEval:AManually-
back.arXivpreprintarXiv:2402.01391(2024). AnnotatedCodeGenerationBenchmarkAlignedwithReal-WorldCodeReposi-
[23] ZhangyinFeng,DayaGuo,DuyuTang,NanDuan,XiaochengFeng,Ming tories.arXivpreprintarXiv:2405.19856(2024).
Gong,LinjunShou,BingQin,TingLiu,DaxinJiang,etal.2020. Codebert: [43] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis
Apre-trainedmodelforprogrammingandnaturallanguages.arXivpreprint Kocetkov,ChenghaoMou,MarcMarone,ChristopherAkiki,JiaLi,Jenny
arXiv:2002.08155(2020). Chim,etal.2023. StarCoder:maythesourcebewithyou! arXivpreprint
[24] GaryFix.2009.Thedesignofanautomatedunittestcodegenerationsystem.In arXiv:2305.06161(2023).
2009SixthInternationalConferenceonInformationTechnology:NewGenerations. [44] FangLiu,YangLiu,LinShi,HoukunHuang,RuifengWang,ZhenYang,andLi
IEEE,743–747. Zhang.2024.ExploringandEvaluatingHallucinationsinLLM-PoweredCode
[25] LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,Charles Generation.arXivpreprintarXiv:2404.00971(2024).
Foster,JasonPhang,HoraceHe,AnishThite,NoaNabeshima,etal.2020.The [45] JiaweiLiu,ChunqiuStevenXia,YuyaoWang,andLingmingZhang.2024.Is
pile:An800gbdatasetofdiversetextforlanguagemodeling. arXivpreprint yourcodegeneratedbychatgptreallycorrect?rigorousevaluationoflarge
arXiv:2101.00027(2020). languagemodelsforcodegeneration.AdvancesinNeuralInformationProcessing
[26] JoshuaGarcia,YangFeng,JunjieShen,SumayaAlmanee,YuanXia,andQiAl- Systems36(2024).
fredChen.2020.Acomprehensivestudyofautonomousvehiclebugs.InPro- [46] JiateLiu,YiqinZhu,KaiwenXiao,QiangFu,XiaoHan,WeiYang,andDeheng
ceedingsoftheACM/IEEE42ndInternationalConferenceonSoftwareEngineering. Ye.2023.RLTF:ReinforcementLearningfromUnitTestFeedback.arXivpreprint
ACM. arXiv:2307.04349(2023).
[27] ZelalemGero,ChandanSingh,HaoCheng,TristanNaumann,MichelGalley, [47] TianyangLiu,CanwenXu,andJulianMcAuley.2023. Repobench:Bench-
JianfengGao,andHoifungPoon.2023. Self-verificationimprovesfew-shot marking repository-level code auto-completion systems. arXiv preprint
clinicalinformationextraction.arXivpreprintarXiv:2306.00024(2023). arXiv:2306.03091(2023).
[28] ShahriarGolchinandMihaiSurdeanu.2023.Timetravelinllms:Tracingdata [48] AntonLozhkov,RaymondLi,LoubnaBenAllal,FedericoCassano,JoelLamy-
contaminationinlargelanguagemodels.arXivpreprintarXiv:2308.08493(2023). Poirier,NouamaneTazi,AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,
[29] ZhibinGou,ZhihongShao,YeyunGong,YujiuYang,NanDuan,WeizhuChen, etal.2024.Starcoder2andthestackv2:Thenextgeneration.arXivpreprint
etal.[n.d.]. CRITIC:LargeLanguageModelsCanSelf-CorrectwithTool- arXiv:2402.19173(2024).
InteractiveCritiquing.InTheTwelfthInternationalConferenceonLearning [49] ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambro-
Representations. sioBlanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,etal.2021.
[30] DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang, Codexglue:Amachinelearningbenchmarkdatasetforcodeunderstandingand
GuantingChen,XiaoBi,Y.Wu,Y.K.Li,FuliLuo,YingfeiXiong,andWenfeng generation.arXivpreprintarXiv:2102.04664(2021).
Liang.2024. DeepSeek-Coder:WhentheLargeLanguageModelMeetsPro- [50] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,WenxiangHu,
gramming–TheRiseofCodeIntelligence. https://api.semanticscholar.org/Co ChongyangTao,JingMa,QingweiLin,andDaxinJiang.2023.WizardCoder:
rpusID:267211867 EmpoweringCodeLargeLanguageModelswithEvol-Instruct.arXivpreprint
[31] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul arXiv:2306.08568(2023).
Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, [51] ZahraMousavi,ChadniIslam,KristenMoore,AlsharifAbuadbba,andMuham-
and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence madAliBabar.2024. AnInvestigationintoMisuseofJavaSecurityAPIsby
With APPS. In Proceedings of the Neural Information Processing Systems LargeLanguageModels.arXivpreprintarXiv:2404.03823(2024).
Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks [52] NiklasMuennighoff,QianLiu,ArmelRandyZebaze,QinkaiZheng,Binyuan
2021,December2021,virtual,JoaquinVanschorenandSai-KitYeung(Eds.). Hui,TerryYueZhuo,SwayamSingh,XiangruTang,LeandroVonWerra,and
https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24c ShayneLongpre.[n.d.].OctoPack:InstructionTuningCodeLargeLanguage
d76e1ce41366a4bbe8a49b02a028-Abstract-round2.html Models.InTheTwelfthInternationalConferenceonLearningRepresentations.
[32] ShimaImani,LiangDu,andHarshShrivastava.2023.Mathprompter:Mathe- [53] LincolnMurr,MorganGrainger,andDavidGao.2023. TestingLLMson
maticalreasoningusinglargelanguagemodels.arXivpreprintarXiv:2303.05398 CodeGenerationwithVaryingLevelsofPromptSpecificity. arXivpreprint
(2023). arXiv:2311.07599(2023).
[33] MdJohirulIslam,GiangNguyen,RangeetPan,andHrideshRajan.2019. A [54] ArvindNeelakantan,TaoXu,RaulPuri,AlecRadford,JesseMichaelHan,Jerry
comprehensivestudyondeeplearningbugcharacteristics.InProceedingsof Tworek,QimingYuan,NikolasTezak,JongWookKim,ChrisHallacy,etal.
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference 2022. Textandcodeembeddingsbycontrastivepre-training. arXivpreprint
andSymposiumontheFoundationsofSoftwareEngineering.510–520. arXiv:2201.10005(2022).
[34] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. [55] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
2018. Mappinglanguagetocodeinprogrammaticcontext. arXivpreprint [56] YonatanOren,NicoleMeister,NiladriChatterji,FaisalLadhak,andTatsunoriB
arXiv:1808.09588(2018). Hashimoto.2023.Provingtestsetcontaminationinblackboxlanguagemodels.
arXivpreprintarXiv:2310.17623(2023).
15[57] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lam- [78] GanWang,ZanWang,JunjieChen,XiangChen,andMingYan.2022. An
bertPouguemWassi,MicheleMerler,BorisSobolev,RajuPavuluri,Saurabh empiricalstudyonnumericalbugsindeeplearningprograms.InProceedingsof
Sinha,andReyhanehJabbarvand.2024. Lostintranslation:Astudyofbugs the37thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering.
introducedbylargelanguagemodelswhiletranslatingcode.InProceedingsof 1–5.
theIEEE/ACM46thInternationalConferenceonSoftwareEngineering.1–13. [79] ShangwenWang,BoLin,ZhensuSun,MingWen,YepangLiu,YanLei,and
[58] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru, XiaoguangMao.2023.Twobirdswithonestone:Boostingcodegenerationand
AlessandroCappelli,HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei, codesearchviaagenerativeadversarialnetwork.ProceedingsoftheACMon
andJulienLaunay.2023. TheRefinedWebdatasetforFalconLLM:outper- ProgrammingLanguages7,OOPSLA2(2023),486–515.
formingcuratedcorporawithwebdata,andwebdataonly. arXivpreprint [80] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel
arXiv:2306.01116(2023). Khashabi,andHannanehHajishirzi.2022. Self-instruct:Aligninglanguage
[59] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.2018. modelwithselfgeneratedinstructions.arXivpreprintarXiv:2212.10560(2022).
Improvinglanguageunderstandingbygenerativepre-training.(2018). [81] YueWang,WeishiWang,ShafiqJoty,andStevenCHHoi.2021. Codet5:
[60] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,Ilya Identifier-awareunifiedpre-trainedencoder-decodermodelsforcodeunder-
Sutskever,etal.2019.Languagemodelsareunsupervisedmultitasklearners. standingandgeneration.arXivpreprintarXiv:2109.00859(2021).
OpenAIblog1,8(2019),9. [82] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022.
[61] MartinRiddell,AnsongNi,andArmanCohan.2024.Quantifyingcontamination Execution-basedevaluationforopen-domaincodegeneration.arXivpreprint
inevaluatingcodegenerationcapabilitiesoflanguagemodels.arXivpreprint arXiv:2212.10481(2022).
arXiv:2403.04811(2024). [83] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
[62] ManleyRoberts,HimanshuThakur,ChristineHerlihy,ColinWhite,andSamuel QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsrea-
Dooley.2023.Tothecutoff...andbeyond?alongitudinalperspectiveonLLM soninginlargelanguagemodels.AdvancesinNeuralInformationProcessing
datacontamination.InTheTwelfthInternationalConferenceonLearningRepre- Systems35(2022),24824–24837.
sentations. [84] YuxiangWei,ZheWang,JiaweiLiu,YifengDing,andLingmingZhang.2024.
[63] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,Xiao- Magicoder:Empoweringcodegenerationwithoss-instruct.InForty-firstInter-
qingEllenTan,YossiAdi,JingyuLiu,TalRemez,JérémyRapin,etal.2023. nationalConferenceonMachineLearning.
Codellama:Openfoundationmodelsforcode.arXivpreprintarXiv:2308.12950 [85] OrionWeller,MarcMarone,NathanielWeir,DawnLawrie,DanielKhashabi,
(2023). andBenjaminVanDurme.2023."Accordingto...":PromptingLanguageModels
[64] WilliamSaunders,CatherineYeh,JeffWu,StevenBills,LongOuyang,Jonathan ImprovesQuotingfromPre-TrainingData. arXivpreprintarXiv:2305.13252
Ward,andJanLeike.2022.Self-critiquingmodelsforassistinghumanevaluators. (2023).
arXivpreprintarXiv:2206.05802(2022). [86] MartinWeyssow,XinZhou,KisubKim,DavidLo,andHouariSahraoui.2023.
[65] SPSharan,FrancescoPittaluga,ManmohanChandraker,etal.2023.Llm-assist: Exploringparameter-efficientfine-tuningtechniquesforcodegenerationwith
Enhancingclosed-loopplanningwithlanguage-basedreasoning.arXivpreprint largelanguagemodels.arXivpreprintarXiv:2308.10462(2023).
arXiv:2401.00125(2023). [87] Man-FaiWong,ShangxinGuo,Ching-NamHang,Siu-WaiHo,andChee-Wei
[66] WeijiaShi,AnirudhAjith,MengzhouXia,YangsiboHuang,DaogaoLiu,Terra Tan.2023. Naturallanguagegenerationandunderstandingofbigcodefor
Blevins,DanqiChen,andLukeZettlemoyer.2023.Detectingpretrainingdata AI-assistedprogramming:Areview.Entropy25,6(2023),888.
fromlargelanguagemodels.arXivpreprintarXiv:2310.16789(2023). [88] ChunqiuStevenXiaandLingmingZhang.2022.Lesstraining,morerepairing
[67] JihoShinandJaechangNam.2021. Asurveyofautomaticcodegeneration please:revisitingautomatedprogramrepairviazero-shotlearning.InPro-
fromnaturallanguage.JournalofInformationProcessingSystems17,3(2021), ceedingsofthe30thACMJointEuropeanSoftwareEngineeringConferenceand
537–555. SymposiumontheFoundationsofSoftwareEngineering.959–971.
[68] JihoShin,ClarkTang,TahminehMohati,MaleknazNayebi,SongWang,and [89] ZhenYang,FangLiu,ZhongxingYu,JackyWaiKeung,JiaLi,ShuoLiu,Yifan
HadiHemmati.2023.PromptEngineeringorFineTuning:AnEmpiricalAs- Hong,XiaoxueMa,ZhiJin,andGeLi.2024. Exploringandunleashingthe
sessmentofLargeLanguageModelsinAutomatedSoftwareEngineeringTasks. poweroflargelanguagemodelsinautomatedcodetranslation.arXivpreprint
arXivpreprintarXiv:2310.10508(2023). arXiv:2404.14646(2024).
[69] NoahShinn,BeckLabash,andAshwinGopinath.2023. Reflexion:anau- [90] PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGraham
tonomousagentwithdynamicmemoryandself-reflection. arXivpreprint Neubig.2018.Learningtominealignedcodeandnaturallanguagepairsfrom
arXiv:2303.113662,5(2023),9. stackoverflow.InProceedingsofthe15thinternationalconferenceonmining
[70] ParshinShojaee,AneeshJain,SindhuTipirneni,andChandanKReddy.2023. softwarerepositories.476–486.
Execution-basedcodegenerationusingdeepreinforcementlearning. arXiv [91] HaoYu,BoShen,DezhiRan,JiaxinZhang,QiZhang,YuchiMa,Guangtai
preprintarXiv:2301.13816(2023). Liang,YingLi,QianxiangWang,andTaoXie.2024.Codereval:Abenchmarkof
[71] DeminSong,HonglinGuo,YunhuaZhou,ShuhaoXing,YudongWang,Zifan pragmaticcodegenerationwithgenerativepre-trainedmodels.InProceedings
Song,WenweiZhang,QipengGuo,HangYan,XipengQiu,etal.2024.Code ofthe46thIEEE/ACMInternationalConferenceonSoftwareEngineering.1–12.
NeedsComments:EnhancingCodeLLMswithCommentAugmentation.CoRR [92] ZishunYu,YunzheTao,LiyuChen,TaoSun,andHongxiaYang.2023.Beta-
(2024). Coder:OnValue-BasedDeepReinforcementLearningforProgramSynthesis.
[72] AlexeySvyatkovskiy,ShaoKunDeng,ShengyuFu,andNeelSundaresan.2020. InTheTwelfthInternationalConferenceonLearningRepresentations.
Intellicodecompose:Codegenerationusingtransformer.InProceedingsof [93] ZhiqiangYuan,YilingLou,MingweiLiu,ShijiDing,KaixinWang,Yixuan
the28thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand Chen,andXinPeng.2023.NoMoreManualTests?EvaluatingandImproving
SymposiumontheFoundationsofSoftwareEngineering.1433–1443. ChatGPTforUnitTestGeneration. ArXivabs/2305.04207(2023). https:
[73] FlorianTambon,ArghavanMoradiDakhel,AminNikanjam,FoutseKhomh, //api.semanticscholar.org/CorpusID:258557344
MichelCDesmarais,andGiulianoAntoniol.2024. Bugsinlargelanguage [94] DaoguangZan,BeiChen,DejianYang,ZeqiLin,MinsuKim,BeiGuan,Yongji
modelsgeneratedcode.arXivpreprintarXiv:2403.08937(2024). Wang,WeizhuChen,andJian-GuangLou.[n.d.].CERT:ContinualPre-Training
[74] XiaoyuTan,ShaojieShi,XiheQiu,ChaoQu,ZhentingQi,YinghuiXu,and onSketchesforLibrary-OrientedCodeGeneration.([n.d.]).
YuanQi.2023. Self-Criticism:AligningLargeLanguageModelswiththeir [95] DaoguangZan,BeiChen,FengjiZhang,DianjieLu,BingchaoWu,BeiGuan,
UnderstandingofHelpfulness,Honesty,andHarmlessness.InProceedingsofthe YongjiWang,andJian-GuangLou.2022.Largelanguagemodelsmeetnl2code:
2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Industry Asurvey.arXivpreprintarXiv:2212.09420(2022).
Track,MingxuanWangandImedZitouni(Eds.).AssociationforComputational [96] YuhaoZhang,YifanChen,Shing-ChiCheung,YingfeiXiong,andLuZhang.
Linguistics,Singapore,650–662. https://doi.org/10.18653/v1/2023.emnlp-indu 2018.AnempiricalstudyonTensorFlowprogrambugs.InProceedingsofthe
stry.62 27thACMSIGSOFTinternationalsymposiumonsoftwaretestingandanalysis.
[75] YahyaTashtoush,NoorAbu-El-Rub,OmarDarwish,ShorouqAl-Eidi,Dirar 129–140.
Darweesh,andOlaKarajeh.2023.ANotionalUnderstandingoftheRelationship [97] ZiyinZhang,ChaoyuChen,BingchangLiu,CongLiao,ZiGong,HangYu,Jian-
betweenCodeReadabilityandSoftwareComplexity.Information14,2(2023), guoLi,andRuiWang.2023.Unifyingtheperspectivesofnlpandsoftwareengi-
81. neering:Asurveyonlanguagemodelsforcode.arXivpreprintarXiv:2311.07989
[76] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi, (2023).
YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,Shruti [98] QinkaiZheng,XiaoXia,XuZou,YuxiaoDong,ShanWang,YufeiXue,Zihan
Bhosale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. Wang,LeiShen,AndiWang,YangLi,etal.2023. Codegeex:Apre-trained
arXivpreprintarXiv:2307.09288(2023). modelforcodegenerationwithmultilingualevaluationsonhumaneval-x.arXiv
[77] RobertVacareanu,AnuragPratik,EvangeliaSpiliopoulou,ZhengQi,Giovanni preprintarXiv:2303.17568(2023).
Paolini,NehaAnnaJohn,JieMa,YassineBenajiba,andMiguelBallesteros.2024. [99] RuiZheng,ShihanDou,SongyangGao,YuanHua,WeiShen,BinghaiWang,
GeneralPurposeVerificationforChainofThoughtPrompting.arXivpreprint YanLiu,SenjieJin,QinLiu,YuhaoZhou,etal.2023. Secretsofrlhfinlarge
arXiv:2405.00204(2024). languagemodelsparti:Ppo.arXivpreprintarXiv:2307.04964(2023).
16[100] TianyuZheng,GeZhang,TianhaoShen,XuelingLiu,BillYuchenLin,JieFu, [102] KunZhou,YutaoZhu,ZhipengChen,WentongChen,WayneXinZhao,Xu
WenhuChen,andXiangYue.2024. Opencodeinterpreter:Integratingcode Chen,YankaiLin,Ji-RongWen,andJiaweiHan.2023.Don’tmakeyourllman
generationwithexecutionandrefinement. arXivpreprintarXiv:2402.14658 evaluationbenchmarkcheater.arXivpreprintarXiv:2311.01964(2023).
(2024). [103] TerryYueZhuo,MinhChienVu,JennyChim,HanHu,WenhaoYu,Ratnadira
[101] DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,Xuezhi Widyasari,ImamNurBaniYusuf,HaolanZhan,JundaHe,IndraneilPaul,etal.
Wang,DaleSchuurmans,ClaireCui,OlivierBousquet,QuocLe,etal.2022. 2024.BigCodeBench:BenchmarkingCodeGenerationwithDiverseFunction
Least-to-mostpromptingenablescomplexreasoninginlargelanguagemodels. CallsandComplexInstructions.arXivpreprintarXiv:2406.15877(2024).
arXivpreprintarXiv:2205.10625(2022).
17