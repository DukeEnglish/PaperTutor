Using Grammar Masking to Ensure Syntactic Validity in
LLM-based Modeling Tasks
LukasNetz JanReimar BernhardRumpe
netz@se.rwth-aachen.de jan.reimar@rwth-aachen.de rumpe@se.rwth-aachen.de
ChairofSoftwareEngineering ChairofSoftwareEngineering ChairofSoftwareEngineering
Aachen,NRW,Germany Aachen,NRW,Germany Aachen,NRW,Germany
Abstract However,asthesemethodsrelyonpromptengineering,theyhave
Wepresentandevaluateamethodcalledgrammarmasking,which onecommonelement:theyonlyimprovethelikelihoodthatthe
is used to guide large language models (LLMs) toward produc- LLMproducessyntacticallycorrectmodelsbutcannotguaranteeit.
ing syntactically correct models for a given context-free gram- Inthiswork,weintroduceanapproachthatusesthecontext-free
mar.Promptengineeringmethodssuchasfew-shotlearningor grammar(CFG)ofthetargetedDSLtofilteroutanysyntactically
primingcanbeusedtoimprovethechancesofanLLMproduc- invalidoutputduringthegenerationprocessoftheLLM.Wewill
ingcorrectsyntax,butthemorecomplexthegrammar,themore evaluateresultsbycomparingthisapproachtoprevioussuccessful
time-consumingandlesspromisingthesemethodsbecome.Pre- modelingtasksforLLMsusingonlyfew-shotlearning.
viousworkisfocusedprimarilyontheusageofeitherlanguage
modeltrainingorpromptengineering.Inthiswork,amethodis 2 Foundations
presentedthatrestrictstheoutputtoagivengrammarusingcon-
We introduce several foundations, such as the used framework
straineddecodingtoensuretheoutputadherestoavalidsyntax.
Guidance,andtheDSLsforwhichwewillgeneratemodels.
WeuseseveralDSLsbuiltwithMontiCoreandtaskmultipleLLMs
toproducemodelswithandwithoutconstraineddecoding.Acorre-
spondingparserisusedtoconfirmthesyntacticcorrectnessofeach 2.1 LargeLanguageModel
model.Weshowthatgrammarmaskingcandramaticallyimprove
ALLMisalanguagemodelthatistrainedonavastamountof
themodelingcapabilitiesofseveralLLMs,reducingtheneedfor
textdata.Itisdistinguishedbyitscapabilityforgeneral-purpose
well-refinedpromptingwhileincreasingthechanceofproducing
language understanding and generation. These models acquire
correctmodels.
theirabilitiesbylearningstatisticalrelationshipsfromtextdoc-
umentsthroughacomputationallyintensiveself-supervisedand
Keywords
semi-supervisedtrainingprocess[32].LLMscanperformtextgen-
LLM,MDSE,Guidance,CFG,ConstrainedDecoding eration,atypeofgenerativeAI[4],bytakinganinputtextand
ACMReferenceFormat: iteratively predicting the next token or word. In the context of
LukasNetz,JanReimar,andBernhardRumpe.2024.UsingGrammarMask- software engineering, LLMs present significant potential to en-
ingtoEnsureSyntacticValidityinLLM-basedModelingTasks.InPreprint., hanceandautomatevarioustasks[22],particularlythoserelated
8pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn toModel-DrivenSoftwareEngineering(MDSE)andmodelinglan-
guages[8,24].
1 Introduction
Largelanguagemodels(LLM)[21,37]arehighlysophisticatedtools
2.2 Few-Shotlearning
that,amongotherthings,arecapableofgeneratingcodeartifacts
basedonanaturallanguageinput[7,14,28].Asweoperatein Few-shotlearning(FSL)isawell-establishedin-contextlearning
thecontextofmodel-drivensoftwareengineering,wefocuson approachforlargelanguagemodels[9,16,27].Apre-trainedLLM
the synthesis of textual models using the predefined syntax of canbepromptedwithasetof𝑁 exemplaryquestion-answerpairs
a given domain-specific language (DSL). Given that the syntax
(𝑞𝑖,𝑎𝑖) 𝑖𝑁
=1
beforebeingprovidedwiththeactualquestion𝑞.The
definitionofthetargetedDSLmightnotbeincludedinthecorpus FSLoutput𝑎forthequestion𝑞isdefinedas𝑃 𝐿𝐿𝑀(𝑎|𝑞,(𝑞𝑖,𝑎𝑖) 𝑖𝑁 =1).
oftrainingdatausedforthelanguagemodel,itisnecessaryto Inaddition,furtherinstructionscanbeaddedtoimprovetheresults
rely on post-training optimization techniques such as few-shot [35].FurtherworkispublishedontheFSLimprovementintroducing
learning[9,16],fine-tuning[27],orpromptengineering[13,23]. intermediatereasoningsteps(e.g.Chainofthought)[15,34,36].
The success of few-shot in-context learning depends on the
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
utilityoftheimplicitknowledgewithintheprovidedexamplesand
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation theclaritywithwhichthetaskspecificationsarecommunicated
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe throughtheprovidedexamples.InthecaseofDomain-Specific
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Languages,thestructurednatureofthecombinatorialoutputspace,
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. representedbytheCFGoftheDSL,isnoteasilycoveredbythe
Preprint,July2024,Aachen,Germany limitednumberofdemonstrations.Thus,generatingmodelsfora
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
DSLwithanFSL-basedapproachremainsasignificantchallenge
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn forLLMs.
4202
luJ
8
]LC.sc[
1v64160.7042:viXraPreprint,July2024,Aachen,Germany Netzetal.
2.3 Guidance-AI tokensandconstrainingthesearchspacequickly.Anothertrieis
Guidance1isatooldesignedtooptimizeandenhancetheprocess constructedforthegrammar,andbytraversingitalongsidegen-
ofgeneratingtextoutputwithLargeLanguageModels.Itprovides eration,theforcedprefixesandsuffixesareaddedtotheoutput
aflexibleandefficientwaytocontroland’guide’theoutputof withoutinvokingtheLLM.
these models to achieve specific goals or adhere to desired for- Thisisincontrasttotemplate-basedapproaches,whichmay
mats.Inourcase,weusetheformattingcapabilitiesofguidance forcetokensthataltertheattentiondistributionandpotentially
toproduceoutputthatadherestotheformattingrulesofagiven degrade the LLM output. Online parsers can maintain minimal
DSL.GuidanceinternallydefinesaDSLimplementedbydevelopers invasivenessbycheckingtokensonebyone.However,thismethod
fromMicrosoftforstructuredpromptingofLLMs.Anyprompting oftenincurshighinferenceoverheadsincetheymayneedtocheck
templateconsistingofamixofunconstrainedgeneration,func- theentiremodelvocabularyateachstep,asseenin1.
tioncalls,constantstrings,orgrammar-constrainedgenerationis
transformedintoatree-likedatastructure,whereeachnoderepre- Select
sentsdifferentpartsofthegrammar.ThecoreclassesareFunction
anditssubclassesGrammarFunction,whichrepresentsgrammar
rules,andRawFunction,whichisusedtointerleavenativePython
functionswithinagrammar.Grammarfunctionsareeitherajoin Byte(b’a’) Join
oraselect,andregexexpressionsinthegrammararereducedto
thesetwooperations.Terminalsareeitherindividualbytesorbyte
ranges.Figure1illustratesasimplegrammartreewiththepossible
productions"a"or"ab". Byte(b’a’) Byte(b’b’)
Themainideaforachievingastructuredoutputisonlineparser-
guided generation synchronizing a parser and scanner with an Figure1:GrammarTreeStructure
LLMtodeterminevalidtokensateachstepdynamically.Inthe
mainLoop,asseeninfigureFigure3.WhenLLMgeneratestext,
itpredictsthenextTokeninasequence.Foreachpossibletoken,
2.4 MontiCorebasedModelingLanguages
alogitisgenerated,representingtheconfidencethatthistoken
iscorrectbasedonthetrainingoftheLLM.Tothoseconfidence AsthechairforSoftwareEngineering,wedevelopandmaintainthe
values,asoftmaxfunctionisappliedtotheconfidencescoresso
languageworkbenchMontiCore2[18].Thelanguageworkbenchis
theyalladdupto1andcanbeusedasprobabilities.Basedonthese employedtogenerateanumberofDSLs.Forbrevity,wewillfocus
probabilities,amultinomialdistributioniscalculated.Now,foreach ontwolanguages:oneusedtodefinerequirementsandspecifica-
step,thetokensaretriedonebyoneandcheckedbytheguidance tionsinsimplifiedstructuredEnglish(SEN)andanotherusedto
parsertoseeiftheyresultinavalidpartialparse. defineUMLclassdiagrams:CD4A.
TheParserisanEarleyParserconstructedbyGuidancebased
2.4.1 StructuredEnglish–AControlledNaturalEnglish(DSL)for
onthesuppliedgrammar.AnEarleyparserefficientlyprocesses
RegulatoryComliance. ThisDSLwasprimarilydevelopedtostan-
context-freegrammarsinthreephases.Duringprediction,itgen-
dardizethedefinitionofrequirementsandisbasedontheworkof
eratesnewstatesbasedongrammarrulesfornon-terminals.In
KonradandCheng[19].TheDSLcanbeusedtowriterequirements
thescanningphase,itmatchesandconsumesterminalsymbolsin
andexpressionsincontrolledsimplifiedEnglish[17],whilestill
theinput.Completionadvancesstateswhenaruleends,prepar-
beingabletobeparsedandprocessedbytooling.
ingforthenextparsingsteps.Thismethodhandlesallpossible
paths,accommodatingambiguousandcomplexgrammarseffec- 1 After starting the engine, each time we
tively.ManyotherFrameworksonlysupportsubsetsofCFGssince 2 pull the turn indicator lever up, the right
theyarebasedonLR(1)orLALR(1)parsers.TheGuidanceparser 3 indicator blinks within 500 ms.
enhancesthestandardEarleyparserbyintroducingcommitpoints,
GenericRequirementinEnglish
whichforcetheparsertocommittospecificparsepaths,effectively
pruningthesearchspaceandavoidingbacktracking.Nootheral- WecanidentifyscopesandpatternsintheprovidedRequirement:
ternativesareconsideredonceacommitpointisreached,ensuring
thattheparseradheresstrictlytochosenpaths.
The parser in the guidance framework employs several opti- 1 After Q:Formula, if P:Formula holds,
mizationstrategiestoenhanceperformanceandefficiencybymin- 2 then in response S:Formula eventually holds
imizingthefrequencyofcallstotheLLM.InfigureFigure3,we 3 time:TimeBound.
canalsoseethattriesareusedintwoinstances.Atrie,orprefix
PatternsfoundinRequirement
tree,isadatastructurethatefficientlystoresandretrieveskeys,
typicallystrings.Eachnoderepresentsacommonprefixshared Next,wecanderivetheSEN-RequirementfromthenaturalEnglish
bysomekeys,allowingfastlookupsbyaprefix.Thetokensofthe one:
LLMareconvertedtoatrie,makingitpossibletoidentifypossible
1https://github.com/guidance-ai 2https://monticore.github.io/UsingGrammarMaskingtoEnsureSyntacticValidityinLLM-basedModelingTasks Preprint,July2024,Aachen,Germany
spacecraft design. However, the modeling process still requires
1 After engine equals started, if humansupervisionandcannotbeyetfullyautomated.Asimi-
2 turn_indicator_lever equals up holds, larconclusionisdrawnbyBuschetal.[10].Intheirapproach,a
3 then in response right_indicator equals Low-Codeisdevelopedusingavisualmodelinglanguage.Similar
4 blinking eventually holds within 500
toTimberlyetal.fullautomationisnotyetpossibleduetothe
5 Milliseconds.
uncertaintyintroducedbyrelyingonanLLMtogeneratecode.
DerivedStructuredEnglishfromRequirement [5]exploresthecapabilitiesofcurrentLLMstocreategeneral-
purposecode.
2.4.2 ClassDiagramsforAnalysis(CD4A). Themodelinglanguage
CD4AisbasedonUMLclassdiagramsandcloselyimplementsall 3.2 SynCode
commonfeaturesofclassdiagramse.g.,inheritance,associations,
SynCode[1,31]isaframeworkforgrammar-guidedgeneration
andenumerations(see[12]).ThesyntaxfollowsaJava-notation,
withlargelanguagemodels.SynCodetriestoaddresstheselimita-
making it easy for developers to adopt the language. Listing 1
tionsbyusinganoffline-constructedlookuptablecalledtheDFA
depictsasimpleclassdiagraminCD4Asyntax.Thediagramistitled
maskstore.ThistableisbasedontheDFAofthelanguagegrammar
’LibraryDiagram’anddefinesthefourclassesLibrary,Member,
terminalsandisdesignedtoretainonlysyntacticallyvalidtokens
Librarian,andBook.Inaddition,theinheritancefromMemberto
duringthegenerationprocess.
LibrarianandanassociationfromLibrarytoBookismodeled.
ThecoreofSynCode’sapproachinvolvesatwo-stepprocess
duringtheLLMdecodingstage.First,thepartialoutputgenerated
1 classdiagram LibraryDiagram { bytheLLMisparsedtoproduceacceptsequencesandaremain-
2 class Library {
der.Acceptsequencesrepresentvalidterminalsequencesthatcan
3 String name;
4 String adress; followthecurrentpartialoutput,whiletheremainderaccounts
5 } foranyunparsedorpartiallyparsedtokens.Next,usingtheac-
6 class Member { ceptsequencesandremainder,SynCodetraversestheDFAstates
7 String name; andretrievesmasksfromtheDFAmaskstore.Thesemasksfilter
8 Long memberID; outsyntacticallyinvalidtokensfromtheLLM’svocabulary,ensur-
9 String contacInfo; ingthatonlyvalidtokensareconsideredduringeachstepofthe
10 } generationprocess.
11 class Librarian extends Member{} Oneofthegreatestadvantagesofthisapproachisthattheen-
12 class Book { tireconstraintinfrastructurecanbepre-computed.Byacceptinga
13 String title;
longerinitialsetuptimetogeneratethemaskstores,theinference
14 }
15 association [1]Library -> Book[*]; processbecomessignificantlyfaster,withonlyaminimaloverhead
16 } ofabout10%,evenforcomplexgrammars.Thismakesitparticu-
larlywell-suitedforhandlingcomplexgrammars,suchasthose
Listing1:CD4AClassDiagramDefiningPerson,Studentand
foundinMontiCore.
AnimalClassandtheirrelations.
4 Approach
3 RelatedWork
Althoughopen-sourceframeworkssuchasGuidancehavebeen
publishedinrecentyears,littleresearchhasbeendoneonusing
LLMswithconstraineddecodingasamodelingtool.
InitialworkwaspublishedbyWangetal.in[33].Thepresented
approachusesgrammarpromptingtoguideanLLMtowardsacon-
strainedoutput.Theydemonstratetheviabilityoftheirapproach
withaselectionofDSLsandLLMs.
3.1 MBSEwithgenerativeAI
Baderetal.useanFSL-basedapproachontheGPT-3.5-turbo-1106
LLMtoproducetextualUMLModelsinXMLnotationbasedon
natural-languageinput.Inthiswork,Badershowsthatvalidmodels Figure2:Evaluatingtheperformanceofafew-shotlearning-
canbecreated;howeveralsopointsoutsomechallengesoftheLLM- basedapproach
basedapproach,suchaslimitedcontextlengthandhallucination-
relatedproblemswiththegeneratedmodels[6]. Withinthiswork,wefocusonthesyntacticcorrectnessofthe
Timperleyetal.assesstheusageofLLMstogeneratemodel- modelsproducedbytheLLM.Weevaluatebycomparingtwoap-
basedspacecraftsystemarchitectures[30].Theapproachrelieson proaches:onethatonlyusesFSLandonethatcombinesFSLwith
generatingtextualmodelsforsystemarchitectures,requirements, grammarmasking.Evenifamodelissyntacticallycorrect,there
andontologies.TheanalysisconcludesthatLLMscanprovidea canstillbesemanticerrors,e.g.themodelcouldbeanemptymodel,
highdegreeofassistanceinmodelingtasksintheearlystagesof whichmightbesyntacticallyvalid,butdoesnotsatisfythegivenPreprint,July2024,Aachen,Germany Netzetal.
modelingtask.Wewereabletoexcludethistrivialerrorcaseinour
tests,however,anindepthsemanticanalysiswasnotperformed.
PrevioustestshaveshownthatLLMsimplementalargepartof
therequirementsinthegeneratedartifactsinthemajorityofcases
[24].Thegeneratedresultsdidnotindicateadeviationfromthe
previousmeasurementsonsemanticaccuracy.
4.1 UsingFew-Shotlearning-basedModeling
Method
Sofar,FSLisoneofthebestpromptingapproachestogetanLLMto
producesyntaxinapredefinedgrammar.Onedrawbackisthatits
performanceheavilyreliesonthecomplexityofthegrammar,the
dependencyontheLLM’sfamiliaritywiththeconceptsunderlying
themodelingtask,andagoodselectionofexamplestorepresent
therulesofthegrammarofthetargetedDSL.FSLislimitedto
asetofexamplestoconveyallsyntacticrelevantelementsofa
grammar[33],whilealsopassingon’bestpractices’foramodeling
taskinthislanguage.AsLLMshaveatendencytoloseaccuracyon
increasinglylargerprompts[20],wehavetochoosetheexamples
foreachgrammar,orevenforeachusecasecarefully.InourFSL
approach,genericdomain-independentexamplesforeachgram-
marwereselected,asouroverreachinggoalisthedevelopment
ofadomain-independentDSL-specificmodelingapproach,that
isnotoptimizedforaspecificusecaseordomain.Ahigheraccu-
racyisverylikely,bynarrowingdowntheapproachtospecific
targetdomainsandthuschoosingcorrespondingexamplesfrom
correspondingusecases.
Withinthiswork,wecompareourapproachwiththeperfor-
mance of the FSL approach developed in [24]. The approach is
depictedinFigure2.Auserinformallydefinesamodelingtask,that
isextendedwithsamplemodelsofthetargetDSL.Theextended
promptisprovidedtoaLLMandtheresultingmodelischeckedbya
parser.Allmodelsareprovidedwiththesameprompt-engineering
andwiththesamesetofmoldingtasks.Thecomputation,withthe
Figure3:CombiningtheGuidanceFrameworkwithMonti-
exceptionoftheOpenAImodel,wasrunonthesamehardware.
Coretogeneratesyntacticallyvalidmodels.
4.2 UsingaGrammarMasking-basedModeling
Method
InourconstraineddecodingapproachweuseGuidanceasdiscussed
inSection2.3.Theconstraineddecodingapproachisevaluatedsim-
ilarlyastheFSLapproach(cf.Figure4.Thepromptcontainingthe
modelingtaskissupplementedwiththesameadditionalmodels,as
inthepreviousapproach.ThegrammarofthetargetDSListrans-
formedandprovidedthroughGuidancetotheLLM.Allgenerated
modelsareparsedwithaparserthatisbasedonthesamegrammar.
ThepipelineinvolvestransformingtheMontiCoreGrammarusing
aVisitorPatternintoaLarkGrammar[2](cf.Listing2),which
is then integrated with Guidance. A detailed setup is shown in
Figure3.Theframeworkstartswithaninitialprompt,thatatthe
beginningisalsothecurrentprompt.Theframeworkislimitedtoa
Figure4:Evaluatingtheperformanceofagrammar-masking-
fixedsetoftokens;iftherearestilltokensleft,thesystemchecks
basedapproach
withthehelpofagrammartrieifthereisanunambiguouscontin-
uationforthecurrentprompt(e.g.,’bool’hastobecompletedto
’boolean’).ThisisusedasashortcuttocircumventLLMusage.If
thisisnotthecase,theLLMisusedtorecommendtokens(trans- invalidtokensuggestions.Validtokensarepassedonandaddedto
former),whicharepassedtoanearleyparser,whichcanidentify thecurrentoutput.Thecyclestartsagainatthecurrentoutput.UsingGrammarMaskingtoEnsureSyntacticValidityinLLM-basedModelingTasks Preprint,July2024,Aachen,Germany
1 start: automaton significantlyincreasesthepercentageofsyntacticallycorrectout-
2 automaton: "automaton" NAME "{" (state |
putsfrom46.52%to92.63%(Llama3).However,thisimprovement
transition)* "}"
comesatthecostofincreasedgenerationtime,withconstrained
3 state: "state" NAME ("<<" ("initial" | "
final") ">>")* ( ("{" (state | generationtakinganaverageof74.09secondscomparedto5.71
transition)* "}") | ";" ) secondsforunconstrainedgeneration.Similarresultsareobserved
4 transition: NAME "-" NAME ">" NAME ";" forotherLLMs.36.57%ofthemodelsproducedbyPhi3Miniin4
5 NAME: /[a-zA-Z_$][a-zA-Z_0-9$]*/ BitQuantizationinanunconstrainedmodeareparsable,compared
6 %ignore WS to86.98%intheconstrainedmode.Gemma7Bin4-BitQuantiza-
7 %import common.WS tionproducesonly0.003%inanunconstrainedmode,comparedto
Listing2:ExampleofLarkGrammar 93.00%intheconstrainedmode.Mistral7Bin3-BitQuantization
produces20.99%parsablemodelscomparedto92.37%parsable
modelsinaconstrainedconfiguration.Quantizationwaschosento
Currently,Guidanceonlyusesgreedydecoding,whichpicks accommodatethehardwareconstraintsoftheexperimentalsetup.
themostprobableallowedtoken.Thisreducestheeffectoflogit Modelswerelimitedto8GB.Allmodelstooksignificantlymore
probabilitybiasing,suchastemperature,hencethesameprompt computationtimewhenusingconstraineddecoding.UsingthePhi3
willproducethesamegeneratedartifact.Thustotestthesystem,we modelinaconstrainedconfigurationtook34timeslongerthan
needmanydistinctusecases.Severalmodelingtasksfromsoftware usingtheModelinanunconstrainedconfiguration.Theseincreases
engineeringexamswereselectedastemplatestosynthesizefurther couldbecombatedbypre/computingtheconstraints,e.g.byusing
examtasks.AnLLMwascommissionedtogeneratealistof1000 theSyncodeapproach(cf.subsection3.2)whichisconnectedto
domains(cf.Listing3). furtherchallenges.
Usingthesamefirst100prompts,GPT-4o[26]wasalsotested.We
1 Automotive Systems, wereunabletoapplyconstraineddecodingasitisaclosed-source
2 Hydraulic Press Control Systems, model.Atthetimeofwriting,GPT-4oisoneofthemostcapable
3 Healthcare Management Systems, LLMsinseveralbenchmarks[3].Thus,itisexpectedthatthemodel
4 E-commerce Platforms, performsbetter(76%parsablemodels)inanunconstrainedmode.
5 Financial Trading Systems, However,mostlikelyduetothecommunicationoverhead,thetime
6 Telecommunication Networks, neededtocreateamodelis,onaverage,doubledincomparisonto
7 Smart Home Automation, thelocallyrunningopen-sourcemodels.
8 [...] WealsotestedtheStructuredEnglishDSLSENinadditionto
Listing3:Exceptfromsynthesizeddomains.Acompletelist theClassDiagramDSLCD4A.Weobservethesamepatternsasin
canbefoundat[25] CD4A:InanunconstrainedsettingusingLlama3,26.54%ofthe
producedmodelsareparsable,whereasinaconstrainedconfigura-
1 A voting system is being designed for a local tion,90.26%areparsable.ThesamecanbeshownfortheLLMsPhi3
2 election. The system should be able to handle andMistral:significantlymoreproducedmodelsareparsableus-
3 multiple voting stations, each with its own ingconstraineddecoding,thanusingunconstrainedgeneration.In
4 set of voters. Each voter has a unique comparisontotheCD4Amodelingtask,GPT-4odoesnotperform
5 identifier and can cast one vote per election. significantlybetterthanthelocallyrunningLLMs:with24.63%
6 The vote is recorded as a preference for a
parsablemodelsinanunconstrainedconfiguration.
7 particular candidate.
8 [...] Notallrunscouldbecompletedinbothcases(CD4AandSEN).
Although1000promptsforCD4Aand123promtsforSENwere
Listing4:Exceptfromsynthesizedusecase.Acompletelist provided to the LLMs, a run was aborted in case a token limit
canbefoundat[25] wasreached.ThusforexampleLlama38B4-Bitonlyproduced991
modelsinsteadof1000.
ThesewereinturngivenindividuallytotheLLMinordertocreate
Constraineddecodingdoesnotcurrentlyachievecompletecor-
newtasksusinganFSLapproachwiththeabove-mentionedexam
rectnessbecauseMonticore’sgrammarincludeskeywordsnotyet
tasks.Asetof1000examtaskswithsimilarspecificationlevelswas
supportedbyourapproach.Forexample,enum(on,off,finished)
thuscreated(cf.Listing4).Thetasksynthetizationwasexecuted
isinterpretedasafunction.Incontrast,Monticore’simplementa-
byLlama38Bina4-bitquantization.
tionwouldnotallowenumtobereadasafunctionname,thereby
Wethencomparetheartifactsthatparseinconstrainedgener-
guidingthegenerationincorrectly.Mostofthesedifferenceswere
ationwiththosethatparseinunconstrainedgenerationforeach
adaptedbyhand.
task.
InTable2,wecanseeapeculiarityofconstrainedgeneration.
Whiletheoverallnumberoflanguageconstructsusedissimilar,the
5 Results
numberofcompositionsandassociationsisthesame.Thisoccurs
To evaluate the presented approach several class diagrams and becausethemodeltriestoextendtokensmaximally.Duetothe
structuredEnglishmodelsweregenerated,theparsingsubset(4.225 promptingandgrammarconstraints,bothconstructs,whichhave
CD4Amodelsand359SENmodels)canbefoundhere:[25].The exactly11characters,areequallylikelytobegenerated.
results(cf.Table1)indicatethattheconstrainedgenerationmethodPreprint,July2024,Aachen,Germany Netzetal.
1 modifier: stereotype? ("public" | ... | "/" | Table1:MeanProcessingTimeandParsingRates
...)*
LeadingtothisCodebeingparseable: Unconstrained Constrained
CD4A(1000examples)
1 class Frame {
2 Material material; // steel Llama38B4-Bit
3 Wheel ; Time(s) 5.71 74.10
4 } Parsed(%) 416/991(41.97%) 918/991(92.63%)
CD4ACode
Phi3Mini4-Bit
Time(s) 4.63 138.29
Figure5:CD4AExample
Parsed(%) 357/976(36.57%) 849/976(86.98%)
Gemma7B4-Bit
Atthispoint,wewouldliketopointoutthattheresultsdonot
Time(s) 2.46 54.20
showthebestpossiblemodelingcapabilitiesoftheindividualmod-
Parsed(%) 2/658(0.003%) 612/658(93.00%)
els,aswehavedeliberatelynotoptimizedthefew-shotlearning
promptingintensively.Theresultsmainlyshowtheperformance Mistral7B3-Bit
gainwithconstantpromptingwithandwithoutgrammarprompt- Time(s) 4.89 73.59
ing.TheresultswithGPT-4o,whichreceivedthesameprompts, Parsed(%) 190/905(20.99%) 836/905(92.37%)
serveasacomparison. GPT-4o(100examples)
Weencounteredmanyunforeseenproblemsinachievingthese Time(s) 8.77 N/A
results.First,EBNFgrammars,whicharesupportedbymostofthe Parsed(%) 76/100(76.00%) N/A
currentlyavailableframeworks,areweakinexpressingcommon
SEN(123examples)
featuresofinterestinglanguages.
• WhitespacesMontiCoreparsersare,inparts,whitespace- Llama38B4-Bit
agnosticandignorethem,similartomostcompilerssuchas Time(s) 1.04 5.23
theCcompiler.However,whitespaceisimportantforthe Parsed(%) 30/113(26.54%) 102/113(90.26%)
attentiondistributionoftheLLMwhenevaluatinginputand Phi-3-Mini4-Bit
output.Therefore,grammarsshouldbemodifiedtoenforce Time(s) 1.13 10.12
correctformatting,forexample,byintegratingthemwitha Parsed(%) 4/122(3.27%) 105/122(86.06%)
linter.
• FuzzyTestingAdditionally,grammarsareoftenonlytested Mistral7B3-Bit
Time(s) 1.22 5.39
againsthumaninputinsteadofsomeextensivefuzzytesting,
Parsed(%) 11/96(11.45%) 85/96(88.54%)
which usually results in no additional benefits. However,
foranLLM,thegrammarshouldbeentirelycorrect.Often, GPT-4o
ingrammarsthatarecommonontheinternet,somerules Time(s) 1.84 N/A
nevertripahumanup,butanLLMwillmakeeveryerror Parsed(%) 17/69(24.63%) N/A
youallowitto.
Numbersincludeonlygenerationswithoutout-of-tokenerrors.
• GrammarMistakesInsteadofamodifierbeingonlyal-
lowedonceornotatall,theKleenestarallows//toberead
asamodifierasshowninFigure5.Byabusingincorrect
Table2:MeanValuesofSyntacticElements(Llama3CD4A)
grammartheLLMmakescommentswhicharenotallowed
byusingtwomodifiers.
• EndlessrepetitionsandlimitedtokensACD4Afilecan Unconstrained Constrained
havearbitrarilymanyclasses,andinfact,therecanbearbi- CompositionCount 5.669021 5.809284
trarilymanyofmostconstructs.However,sinceourVRAM AssociationCount 0.21998 5.809284
is limited, the LLM can only generate a finite amount of ClassCount 7.849647 8.095863
tokens.Therefore,grammarmaskingonlyguaranteescor-
rectartifactsifthegenerationstopsbeforethetokenlimit
is reached. In some cases, the LLM gets stuck in endless
repetitions,makingitunlikelytoterminateinaparsable 6.1 Applicabilitytoothergrammars
state.
TheapproachpresentedinthisworkisbasedonMontiCoreGram-
mars, which are transformed into LARK grammars. Hence this
6 Discussion
approachcanbeappliedtoanyMontiCoreGrammar.MontiCore
TheresultsshowninTable1andTable2showverypromising providesinfrastructurethatpermitsthedeveloperstodefinecon-
results,inthefollowingwediscussaspectssuchaslimitationsand textconditions(CoCos)[11].TheseCoCosarerulesthatcheckthe
generalizabilityoftheapproach. well-formednessofmodels.ThesecontextconditionsarecrucialUsingGrammarMaskingtoEnsureSyntacticValidityinLLM-basedModelingTasks Preprint,July2024,Aachen,Germany
for ensuring that models adhere to the specified rules and con- 6.4 Limitations
straintsofthelanguage.DSLswithfewerCoCosaretransferable Oneofthekeylimitationsofthisapproachisitsmissingsupport
tothismethodwithlesseffort,asthisapproachonlyimpactsthe forcontextconditions.Ascontextconditionsoftenneedtheentire
adherencetothegrammarandnottheCoCos.Inaddition,gram- modeltobeapplied,theycannotbeusedinafilteringcapacitydur-
marmaskingisexpectedtoyieldfewerimprovementsonDSLs ingmodelcreation.Thus,theyhavetobeappliedinasucceeding
alreadyencounteredinpretraining,suchasPlantUML[29],orSQL, stepafteramodelforaspecificDSLhasbeencreated.Grammar
as these languages should already perform well. LLMs that are maskingreducesthenumberofmodelsthatdonotadheretothe
alreadytrainedonaspecificDSLwillbemorelikelytoproduce providedgrammar,thusalsoincreasingtheoverallnumberofcor-
syntacticallycorrectmodels. rectmodelsthatpotentiallycomplywiththecontextconditions,
ThisapproachwasdevelopedwithMontiCoregrammarsinmind. asanysyntacticallyinvalidmodelsarefilteredoutataveryearly
However, the approach can be applied to any grammar that is stage.Anotherlimitationisthehighdegreeofspecializationinone
transformableintoaLARKgrammar(cf.Figure3,Figure4). grammar.ThisapproachlimitstheLLMtoonlyproducingmodels
foronespecificgrammar.Asecondsetupandadelegatorareneeded
iftheframeworkismeanttoswitchbetweenDSLs.Switchingout
6.2 ImpactGrammarMaskingonModel
thepromptingalonewillnotsuffice.
Semantics Asmentionedabove,grammarsmightneedtobeadaptedand
Theapproachpresentedinthispaperreliesprimarilyonfiltering refinedtooperatewiththisapproach,asLLMstendtofindloopholes
outsyntacticallyinvalidtokens.Althoughwedidnotnoticeany in’incompletely’definedgrammars.Thisrequiresthedeveloper
changesinthesemanticsoftheresultingmodels,wecannotrule whosetsuptheframeworktohavesomeexperienceinlanguage
outthepossibilitythatrelevantcontentisexcludedfromthemodel design.
or modified so that the semantics of the model are changed. A
closerlookatrandomsamples,inwhichbothapproachesresulted 7 Conclusion
in syntactically correct models, revealedthat models generated
Wewereabletoshowthatframeworksthatenableconstrained
withtheconstrainedapproachdonotsystematicallycontainfewer
decodingenablesmaller,lessperformantLLMstoproducesyntac-
elementsthanthosegeneratedwiththeunconstrainedapproach.
ticallycorrectmodelsatareasonablerate.Ourexperimentsshow,
Table2impliestheopposite.Thedifferencesobservedweremainly
thatgrammarmaskingcansignificantlyincreasethechanceofan
intheformattingandnamingofelements.
LLM-basedapproachtoproducevalidmodelsforagivenDSL.We
Modelsgeneratedwithcontaineddecodingmightbelessdetailed,
coulddemonstratethisimprovementfortwoDSLs.Withinthis
asthisdoesnotnecessarilyappearinquantitativeanalysis(e.g.
paper,weonlyaddresssyntacticvalidationoftheproducedmodels.
countingclassesandattributes).Afurtherin-depthanalysisofthe
Furtheranalysishastobeperformedtosystematicallyevaluateif
contentwouldbenecessary.
therearesystematicdifferencesinthesemanticsofunconstrained
and constrained models. In addition, an extensive difference in
computationtimebetweenunconstrainedandconstrainedgener-
6.3 ChoosingBetweenConstrainedand
ationwasmeasured.Asaresult,werecommendthatthemethod
UnconstrainedGeneration describedinthisworkshouldonlybeusedifasatisfactoryout-
AswecouldshowinthecaseoftheDSLCD4A,FSLalonecansuf- comecannotbeachievedusingconventionalpromptengineering
ficetocreateanapproachthatisverylikelytoyieldasyntactically methods.Improvementsinframeworks,suchasprecomputations
correctmodel(e.g.byusingasufisticatedLLMsuchasGPT-4o (Syncode)andruntimeoptimations,couldsoonreducethegapin
cf.Table1).ThisapproachisnotlimitedtotheDSLspresentedin computationtime.
thispaperandcanbeappliedtofurthermodelinglanguageswith
sufficientpromptengineering.Nevertheless,thisapproachrequires References
expertsinthefieldofgenerativeAI(e.g.aPromptEngineer)andex-
[1] [n.d.]. EfficientandgeneralsyntacticaldecodingforLargelanguageModels.
pertsinthespecificmodelinglanguagetoprovidespecificandideal https://github.com/uiuc-focal-lab/syncode. [Accessed02-07-2024].
representativeexamplesofthetargetedlanguage.Hence,anFSL- [2] [n.d.].Larkdocumentation.https://lark-parser.readthedocs.io/en/stable/. [Ac-
cessed27-06-2024].
only-basedapproachwouldbeunsuitablefordevelopersunfamiliar [3] [n.d.].LLMLeaderboard-CompareGPT-4o,Llama3,Mistral,Gemini&other
withthetargetedDSL.Incontrast,thegrammar-masking-based models|ArtificialAnalysis—artificialanalysis.ai. https://artificialanalysis.ai/
leaderboards/models. [Accessed01-07-2024].
approachislessreliantongoodpromptingandcanbederivedfrom
[4] ThimiraAmaratunga.[n.d.].UnderstandingLargeLanguageModels.([n.d.]).
agivenMontiCoregrammar,thusonlyneedingthegrammarfile [5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
andafewsamplesofthetargetedmodelinglanguagetooperate.In Michalewski,DavidDohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,
etal.2021. Programsynthesiswithlargelanguagemodels. arXivpreprint
addition,thegrammarmaskingapproachpresentedinthiswork
arXiv:2108.07732(2021).
enablessmallerlessperformantmodelstoserveasmodelingtools. [6] EliasBader,DominikVereno,andChristianNeureiter.[n.d.].FacilitatingUser-
SmallermodelssuchastheLlama38Bina4-bitquantizationcan CentricModel-BasedSystemsEngineeringusingGenerativeAI.([n.d.]).
[7] MarcoBarenkamp,JonasRebstadt,andOliverThomas.2020.ApplicationsofAI
be executed on hardware that is available for the end user (e.g. inclassicalsoftwareengineering.AIPerspectives2,1(2020),1.
NVIDIAGeForceRTX3070),makingthisapproachindependent [8] NilsBaumann,JuanSebastianDiaz,JudithMichael,LukasNetz,HaronNqiri,Jan
Reimer,andBernhardRumpe.2024.CombiningRetrieval-AugmentedGeneration
fromexternalserverclusterssuchastheonesneededforanOpenAI
andFew-ShotLearningforModelSynthesisofUncommonDSLs.InModellierung
basedapproach. 2024SatelliteEvents.GesellschaftfürInformatikeV,10–18420.Preprint,July2024,Aachen,Germany Netzetal.
[9] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,AndrewKon-
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda drich,ArisKonstantinidis,KyleKosic,GretchenKrueger,VishalKuo,Michael
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural Lampe,IkaiLan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMing
informationprocessingsystems33(2020),1877–1901. Li,RachelLim,MollyLin,StephanieLin,MateuszLitwin,TheresaLopez,Ryan
[10] DanielBusch,GerritNolte,AlexanderBainczyk,andBernhardSteffen.2023. Lowe,PatriciaLue,AnnaMakanju,KimMalfacini,SamManning,TodorMarkov,
ChatGPTintheloop:anaturallanguageextensionfordomain-specificmodeling YanivMarkovski,BiancaMartin,KatieMayer,AndrewMayne,BobMcGrew,
languages.InInternationalConferenceonBridgingtheGapbetweenAIandReality. ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,JakeMcNeil,David
Springer,375–390. Medina,AalokMehta,JacobMenick,LukeMetz,AndreyMishchenko,Pamela
[11] ArvidButting,RohitGupta,NicoJansen,NikolausRegnat,andBernhardRumpe. Mishkin,VinnieMonaco,EvanMorikawa,DanielMossing,TongMu,MiraMurati,
2023. TowardsModularDevelopmentofReusableLanguageComponents OlegMurk,DavidMély,AshvinNair,ReiichiroNakano,RajeevNayak,Arvind
forDomain-SpecificModelingLanguagesintheMagicDrawandMontiCore Neelakantan,RichardNgo,HyeonwooNoh,LongOuyang,CullenO’Keefe,Jakub
Ecosystems. Journal of Object Technology 22, 1 (September 2023), 1:1–21. Pachocki,AlexPaino,JoePalermo,AshleyPantuliano,GiambattistaParascan-
https://doi.org/10.5381/jot.2023.22.1.a4 dolo,JoelParish,EmyParparita,AlexPassos,MikhailPavlov,AndrewPeng,
[12] ChairofSoftwareEngineering.2023.ClassDiagramForAnalysis. AdamPerelman,FilipedeAvilaBelbutePeres,MichaelPetrov,HenriquePonde
[13] BanghaoChen,ZhaofengZhang,NicolasLangrené,andShengxinZhu.2024. deOliveiraPinto,Michael,Pokorny,MichellePokrass,VitchyrH.Pong,Tolly
UnleashingthepotentialofpromptengineeringinLargeLanguageModels:a Powell,AletheaPower,BorisPower,ElizabethProehl,RaulPuri,AlecRadford,
comprehensivereview.arXiv:2310.14735[cs.CL]https://arxiv.org/abs/2310.14735 JackRae,AdityaRamesh,CameronRaymond,FrancisReal,KendraRimbach,
[14] TaliaCrawford,ScottDuong,RichardFueston,AyorindeLawani,SamuelOwoade, CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,TedSanders,
Abel Uzoka, Reza M Parizi, and Abbas Yazdinejad. 2023. Ai in software ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchul-
engineering:Asurveyonprojectmanagementapplications. arXivpreprint man,DanielSelsam,KylaSheppard,TokiSherbakov,JessicaShieh,SarahShoker,
arXiv:2307.15224(2023). PranavShyam,SzymonSidor,EricSigler,MaddieSimens,JordanSitkin,Katarina
[15] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Slama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePet-
Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, roskiSuch,NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,MadeleineB.
JaschaSohl-Dickstein,etal.2022. Languagemodelcascades. arXivpreprint Thompson,PhilTillet,AminTootoonchian,ElizabethTseng,PrestonTuggle,
arXiv:2207.10342(2022). NickTurley,JerryTworek,JuanFelipeCerónUribe,AndreaVallone,ArunVi-
[16] AndreaFedele.2023. ExplainandInterpretFew-ShotLearning..InxAI(Late- jayvergiya,ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang,
breakingWork,Demos,DoctoralConsortium).233–240. BenWang,JonathanWard,JasonWei,CJWeinmann,AkilaWelihinda,Peter
[17] NorbertEFuchs,KaarelKaljurand,andTobiasKuhn.2008.Attemptocontrolled Welinder,JiayiWeng,LilianWeng,MattWiethoff,DaveWillner,ClemensWinter,
englishforknowledgerepresentation.ReasoningWeb:4thInternationalSummer SamuelWolrich,HannahWong,LaurenWorkman,SherwinWu,JeffWu,Michael
School2008,Venice,Italy,September7-11,2008,TutorialLectures(2008),104–124. Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,QimingYuan,WojciechZaremba,
[18] KatrinHölldobler,OliverKautz,andBernhardRumpe.2021.MontiCoreLanguage RowanZellers,ChongZhang,MarvinZhang,ShengjiaZhao,TianhaoZheng,
WorkbenchandLibraryHandbook:Edition2021. ShakerVerlag. http://www. JuntangZhuang,WilliamZhuk,andBarretZoph.2024.GPT-4TechnicalReport.
monticore.de/handbook.pdf arXiv:2303.08774[cs.CL] https://arxiv.org/abs/2303.08774
[19] SaschaKonradandBettyHCCheng.2005.Real-timespecificationpatterns.In [27] MengyeRen,EleniTriantafillou,SachinRavi,JakeSnell,KevinSwersky,JoshuaB
Proceedingsofthe27thinternationalconferenceonSoftwareengineering.372–381. Tenenbaum,HugoLarochelle,andRichardSZemel.2018. Meta-learningfor
[20] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua, semi-supervisedfew-shotclassification.arXivpreprintarXiv:1803.00676(2018).
FabioPetroni,andPercyLiang.2024.Lostinthemiddle:Howlanguagemodels [28] AhmedRSadik,SebastianBrulin,andMarkusOlhofer.2023.Codingbydesign:
uselongcontexts.TransactionsoftheAssociationforComputationalLinguistics Gpt-4empowersagilemodeldrivendevelopment.arXivpreprintarXiv:2310.04304
12(2024),157–173. (2023).
[21] YihengLiu,TianleHan,SiyuanMa,JiayueZhang,YuanyuanYang,JiamingTian, [29] DSinghandHJSSidhu.2018.OptimizingthesoftwaremetricsforUMLstructural
HaoHe,AntongLi,MengshenHe,ZhengliangLiu,etal.2023. Summaryof andbehavioraldiagramsusingmetricstool.AsianJournalofComputerScience
chatgpt-relatedresearchandperspectivetowardsthefutureoflargelanguage andTechnology7,2(2018),11–17.
models.Meta-Radiology(2023),100017. [30] LouisTimperley,LucyBerthoud,ChrisSnider,andTheoTryfonas.[n.d.].Assess-
[22] StephenMacNeil,AndrewTran,ArtoHellas,JoanneKim,SamiSarsa,Paul mentofLargeLanguageModelsforUseinGenerativeDesignofModelBased
Denny,SethBernstein,andJuhoLeinonen.2023.Experiencesfromusingcode SpacecraftSystemArchitectures.AvailableatSSRN4823264([n.d.]).
explanationsgeneratedbylargelanguagemodelsinawebsoftwaredevelopment [31] ShubhamUgare,TarunSuresh,HangooKang,SasaMisailovic,andGagandeep
e-book.InProceedingsofthe54thACMTechnicalSymposiumonComputerScience Singh.2024.Improvingllmcodegenerationwithgrammaraugmentation.arXiv
EducationV.1.931–937. preprintarXiv:2403.01632(2024).
[23] GolamMdMuktadir.2023. ABriefHistoryofPrompt:LeveragingLanguage [32] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Models.(ThroughAdvancedPrompting).arXive-prints(2023),arXiv–2310. AidanN.Gomez,LukaszKaiser,andIlliaPolosukhin.2017. AttentionIsAll
[24] LukasNetz,JudithMichael,andBernhardRumpe.2024.FromNaturalLanguage YouNeed.CoRRabs/1706.03762(2017).arXiv:1706.03762
toWebApplications:UsingLargeLanguageModelsforModel-DrivenSoftware [33] BailinWang,ZiWang,XuezhiWang,YuanCao,RifASaurous,andYoonKim.
Engineering.InModellierung2024.GesellschaftfürInformatikeV,179–195. 2024.Grammarpromptingfordomain-specificlanguagegenerationwithlarge
[25] LukasNetzandJanReimer.2024.LLMs4MBSESynthetic-Artifacts. https://github. languagemodels.AdvancesinNeuralInformationProcessingSystems36(2024).
com/Lukas-Netz/llm4mbse-synthetic-artifacts [34] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,
[26] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,Ilge AakankshaChowdhery,andDennyZhou.2022.Self-consistencyimproveschain
Akkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,Sam ofthoughtreasoninginlanguagemodels.arXivpreprintarXiv:2203.11171(2022).
Altman,ShyamalAnadkat,RedAvila,IgorBabuschkin,SuchirBalaji,ValerieBal- [35] JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,Brian
com,PaulBaltescu,HaimingBao,MohammadBavarian,JeffBelgum,IrwanBello, Lester,NanDu,AndrewMDai,andQuocVLe.2021.Finetunedlanguagemodels
JakeBerdine,GabrielBernadett-Shapiro,ChristopherBerner,LennyBogdonoff, arezero-shotlearners.arXivpreprintarXiv:2109.01652(2021).
OlegBoiko,MadelaineBoyd,Anna-LuisaBrakman,GregBrockman,TimBrooks, [36] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
MilesBrundage,KevinButton,TrevorCai,RosieCampbell,AndrewCann,Brit- QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
tanyCarey,ChelseaCarlson,RoryCarmichael,BrookeChan,CheChang,Fotis inlargelanguagemodels.Advancesinneuralinformationprocessingsystems35
Chantzis,DerekChen,SullyChen,RubyChen,JasonChen,MarkChen,Ben (2022),24824–24837.
Chess,ChesterCho,CaseyChu,HyungWonChung,DaveCummings,Jeremiah [37] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,
Currier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,Damien YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,YifanDu,ChenYang,
Deville,ArkaDhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcoffet, YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,
AttyEleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,SimónPosada ZikangLiu,PeiyuLiu,Jian-YunNie,andJi-RongWen.2023.ASurveyofLarge
Fishman,JustonForte,IsabellaFulford,LeoGao,ElieGeorges,ChristianGibson, LanguageModels. arXiv:2303.18223[cs.CL] https://arxiv.org/abs/2303.18223
VikGoel,TarunGogineni,GabrielGoh,RaphaGontijo-Lopes,JonathanGor-
don,MorganGrafstein,ScottGray,RyanGreene,JoshuaGross,ShixiangShane
Gu,YufeiGuo,ChrisHallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton,
JohannesHeidecke,ChrisHesse,AlanHickey,WadeHickey,PeterHoeschele,
BrandonHoughton,KennyHsu,ShengliHu,XinHu,JoostHuizinga,Shantanu
Jain,ShawnJain,JoanneJang,AngelaJiang,RogerJiang,HaozhunJin,Denny
Jin,ShinoJomoto,BillieJonn,HeewooJun,TomerKaftan,ŁukaszKaiser,Ali
Kamali,IngmarKanitscheider,NitishShirishKeskar,TabarakKhan,LoganKil-
patrick,JongWookKim,ChristinaKim,YongjikKim,JanHendrikKirchner,