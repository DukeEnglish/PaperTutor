Sketchy Moment Matching: Toward Fast and Provable
Data Selection for Finetuning
Yijun Dong* Hoang Phan* Xiang Pan* Qi Lei
NewYorkUniversity
{yd1319,hvp2011,xiangpan,ql518}@nyu.edu
Abstract
We revisit data selection in a modern context of finetuning from a fundamental per-
spective. Extendingtheclassicalwisdomofvarianceminimizationinlowdimensionsto
high-dimensional finetuning, our generalization analysis unveils the importance of addi-
tionallyreducingbiasinducedbylow-rankapproximation. Inspiredbythevariance-bias
tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching
(SkMM),ascalabledataselectionschemewithtwostages. (i)First,thebiasiscontrolled
usinggradientsketchingthatexploresthefinetuningparameterspaceforaninformative
low-dimensionalsubspaceS;(ii)thenthevarianceisreducedoverS viamomentmatching
betweentheoriginalandselecteddatasets. Theoretically,weshowthatgradientsketching
isfastandprovablyaccurate: selectingnsamplesbyreducingvarianceoverS preserves
thefast-rategeneralizationO(dim(S)/n),independentoftheparameterdimension. Empir-
ically,weconcretizethevariance-biasbalanceviasyntheticexperimentsanddemonstrate
theeffectivenessofSkMMforfinetuninginrealvisiontasks.
1 Introduction
Asthedatavolumeandtrainingcostexplodewiththeunprecedentedmodelperformance,the
long-standing problem of data selection [4, 30] is getting increasing attention in the modern
context of deep learning from various perspectives, including data pruning [70, 89], coreset
selection[6,30,41,54,88,90],anddatafiltering[4,27,38,66,83]. Acommongoalsharedby
theseperspectivesistotrainamodelfromscratchonlessdatatolearnhigh-qualityrepresen-
tationsandachievecompetitivegeneralization. However,empiricalobservationsalsosuggest
thelimitationofdataremovalduringpre-training: aseeminglyinevitabletradeoffbetweenless
computation and higher-qualityrepresentations [2,29]. While existingworks on data selection
haveadominatingfocusonthetraining-from-scratchsetting,thesensitivityofrepresentation
learningtodataandthegrowingavailabilityofpowerfulpre-trainedmodelscallsforattention
toalessstudied[87]butequallyimportantproblem: dataselectionforfinetuning.
In the simplest finetuning setting—linear probing on low-dimensional representations1, data
selection falls in the classical frames of coreset selection for linear regression [3, 11, 19, 46,
63, 65, 69, 85] and optimal experimental design [5, 8, 26, 61, 82] where the generalization
*Equalcontribution.
1Throughoutthiswork,wereferto“low-dimension”asthesettingwherethenumberoffinetuningparametersr
issmallerthantheselecteddownstreamsamplesizen,while“high-dimension”referstotheopposite,r >n.
1
4202
luJ
8
]GL.sc[
1v02160.7042:viXraFigure1: Controllingvariance-biastradeoffindataselectionforhigh-dimensionalfinetuning
viagradientsketching+momentmatching(SkMM).ConsideratoydatasetwithN samples(in
blue)whosefinetuninggradientslieinahigh-dimensionalparameterspaceRr (visualizedin3D)
withalowintrinsicdimension(e.g.,threeclusters). Thegoalistoselectn = n +n +n < r
1 2 3
samplesforfinetuning. (a)Biasreductionfocusesonminimizingthelow-rankapproximation
error,resultinginuniformselectionacrossclustersregardlessoftheirvariance. (b)Variance
reduction2places more emphasis on high-variance clusters and could lead to large bias by
missinglow-varianceones. (c)Gradientsketchingefficientlyfindsalow-dimensionalsubspace
S (where dim(S) < n) with small bias. (d) Moment matching in S controls the variance
within the low-bias subspace, leading to a variance-bias balance with fast-rate generalization
O(dim(S)/n).
gap can be reduced by selecting data that minimize the associated variance. However, for
high-dimensional finetuning, variance minimization alone is insufficient to characterize the
generalization due to the overparametrized nature of modern architectures. Even for linear
probing, when the parameter dimension r is higher than the sample size n, the selected data
necessarily fails to capture a subspace of the parameter space with dimension at least r −n,
leadingto errorsinaddition tovariance. Nevertheless,the prevailingempirical andtheoretical
evidence [2, 76] on the ubiquitous intrinsic low-dimensional structures of high-dimensional
data/modelmotivatesanaturalquestion:
Canthelowintrinsicdimensionbeleveragedindataselectionforhigh-dimensionalfinetuning?
Lowintrinsicdimensionleadstovariance-biastradeoffindataselection. Weprovidea
positive answer to this question through a variance-bias tradeoff perspective. Intuitively, we
consideralow-dimensionalsubspaceS inthefinetuningparameterspacewherethemodellearns
the necessary knowledge for the downstream task. The generalization gap can be controlled
by simultaneously reducing the bias (redundant information) by “exploring” the finetuning
parameterspacetofindasuitableS andthevarianceby“exploiting”theusefulknowledgein
S.
Given the high-dimensional nature of the finetuning parameter space, direct search for such
suitable subspace S is computationally infeasible in general. This leads to a follow-up ques-
tion:
Howtoexploretheintrinsiclow-dimensionalstructureefficientlyfordataselection?
We propose a two-stage solution—Sketchy Moment Matching (SkMM): (i) dimensionality
reduction via gradient sketching to efficiently explore the finetuning parameter space, and
(ii)variancecontrolviamomentmatchingtoexploitusefulknowledgeinthelow-dimensional
subspace.
2Mostclassicalvariancereductionmethodsaretailoredonlyforlow-dimensionalsettingswithn>r. Here,
2Gradientsketchingfindsagoodlow-dimensionalsubspacefastandprovably. First,we
constructalow-dimensionalparametersubspaceS bysketchingthemodelgradients. Sketch-
ing[37,85]isawell-establisheddimensionalityreductiontoolknownforaffordableandaccurate
low-rank approximations [31, 52]. In deep learning, sketching recently extends its empirical
applicationstoscalableestimationsofinfluencefunctionsfordataselection[58,87]. Wemake
a first step toward the theoretical guarantee of gradient sketching for data selection: gradient
sketching efficiently finds a low-dimensional subspace S with small bias such that selecting
n samples by reducing variance over S is sufficient to preserve the fast-rate generalization
O(dim(S)/n), linear in the low intrinsic dimension dim(S) while independent of the high
parameterdimensionr.
Moment matching in low dimension selects data that control the variance. Second, we
select data that reduce variance in the low-dimensional subspace S via moment matching.
Thevarianceofdataselectionischaracterizedbymatchingbetweengradientmomentsofthe
original and selected datasets, Σ,Σ , whose exact form tr(Σ(Σ )†) involves ill-conditioned
S S
pseudoinverse and leads to a hard integer programming problem [7, 82]. Inspired by the
generalization analysis, we introduce a quadratic relaxation with linear constraints that is
numericallystable(freeofpseudoinverse)andcanbeefficientlyoptimizedviaprojectedgradient
descent.
Thecontributionsofthisworkaresummarizedasfollows:
• Weprovidearigorousgeneralizationanalysisondataselectionforfinetuning,illustratingthe
criticalroleofdimensionalitybyunveilingthevariance-biastradeoffinhighdimensions.
• Weshowthatgradientsketchingprovablyfindsalow-dimensionalparametersubspaceS with
small bias,reducing varianceoverwhich preservesthe fast-rategeneralizationO(dim(S)/n).
Techniquesusedinanalyzinggradientsketchingfordataselectionareagnostictotheselection
methodorthefinetuningsettingandcouldbeofindependentinterest.
• WeintroduceSkMM,ascalabletwo-stagedataselectionmethodforfinetuningthatsimultane-
ously “explores” the high-dimensional parameter space via gradient sketching and “exploits”
theinformationinthelow-dimensionalsubspaceviamomentmatching.
1.1 Related Works
Coresetselectionandlow-rankapproximations. Fromthevariance-biastradeoffperspective,
dataselectionforhigh-dimensionalfinetuningcanbeviewedasacombinationof(i)variance
reductionincoresetselectionforlinearregression[3,11,19,65,69,85]oroptimalexperimental
design [8, 26, 61] with low-dimensional features, and (ii) bias reduction via sample-wise
low-rankapproximationforhigh-dimensionalmatrices[13,17,18,21,22,23,50,60,79].
Gradient sketching. Gradient sketching [31, 85] based on Johnson-Lindenstrauss trans-
forms (JLTs) [37] has achieved impressive recent successes in efficient data selection [87]
and attribution [58]. Despite the empirical success, theoretical understanding of the effect
of gradient sketching on generalization remains limited. We make a first step toward this in
the context of data selection leveraging existing theories on sketching (vide Remark 3.1 and
AppendixC).
weintuitivelyreferto“variancereduction”asthedirectextensionoflow-dimensionalvariancereductiontohigh
dimensions,withoutcarefulcontrolofthebias(videSection4.1forsomeconcretizations).
3Momentmatching. Momentmatchingisanintuitiveideaforselectinglow-dimensionaldata
(i.e., the coreset size n is larger than the data/representation dimension r), bearing diverse
concreteobjectivesliketheV-andA-optimalityconditions[5,82]. Formultimodalcontrastive
learning, recent works [38, 83] illustrated the effectiveness of moment matching via tailored
data selection criteria for CLIP [62]. Distinct from our setting of general finetuning in both
lowandhighdimensions,theseworksfocusondatafiltering(with n > r)forpretrainingfrom
scratch.
(Unsupervised) data selection. In this work, we focus on unsupervised data selection that
instead of relying on labels3, leverages the geometry of the feature space and aims to select
samplesthatarespreadout,withabroadspectrumofconcretizationsincludingherding[12,84],
k-center greedy [67], leverage score sampling [10, 25, 69], adaptive sampling [13, 20], and
volumesampling[17,21].
Aninspiringrecentwork[43]investigatesthegeneralizationofweaklysuperviseddataselection
viaindependentsampling inthelow- (n → ∞withfixedr)andhigh-dimensional (n,r → ∞
with n/r → constant) asymptotics. Instead of the asymptotic regime, we consider a realistic
settingwithfinitenandr,withoutspecificassumptionsonthedata/featuredistributionother
thanthe lowintrinsicdimension. Along thisline, (weakly)superviseddata selectioncommonly
make choices based on the uncertainty [47, 49, 68] or sensitivity of the loss to samples (e.g.,
influence function [72, 81, 89], sensitivity scores [51, 56, 90], and heuristics based on losses
andtheirgradients[36,59,78]).
1.2 Notations
Given any n ∈ Z , we denote [n] = {1,··· ,n}. Let e be the n-th canonical basis of the
+ n
conformabledimension;I bethen×nidentitymatrix;and0 ,1 ∈ Rn beingvectorswithall
n n n
entriesequaltozeroandone,respectively. LetSn−1 := {x ∈ Rn|∥x∥ = 1}betheunitsphere
inRn,and∆ := (cid:8) p ∈ [0,1]b (cid:12) (cid:12)∥p∥ = 1(cid:9) bethedimension-nproba2 bilitysimplex. Weadapt
n 1
the standard asymptotic notations: for any functions f,g : R → R , we write f = O(g) or
+ +
f ≲ g if there exists some constant C > 0 such that f(x) ≤ Cg(x) for all x ∈ R ; f = Ω(g)
+
or f ≳ g if g = O(f); f ≍ g if f = O(g) and f = Ω(g). For any matrix A ∈ Rn×d,
let s (A) ≥ ··· ≥ s (A) ≥ 0 be the singular values; and A† be the Moore-Penrose
1 rank(A)
pseudoinverse. Additionallyforanyk ≤ rank(A),let⟨A⟩ = argmin ∥A−B∥
k B:rank(B)≤k F
be the optimal rank-k approximation of A (characterized by the rank-k truncated SVD). For
any symmetric matrices A,B ∈ Rd×d, we write A ≽ B or A−B ≽ 0 if A−B is positive
semidefinite.
2 Data Selection for Finetuning
GivenadataspaceX ⊆ Rd andalabelspaceY ⊆ R,letD = {(x ,y ) ∈ X ×Y |i ∈ [N]}be
i i
alargedataset,withmatrixform(X,y) ∈ RN×d ×RN,forsomedownstreamtaskwherethe
performanceismeasuredbyalossfunctionℓ : Y ×Y → R .
≥0
3Fromthetheoryperspective,dataselectionforfinetuningislesssensitivetolabelscomparedtotrainingfrom
scratch,especiallygivensuitablepre-trainedmodelswithreasonablezero-shotaccuracy(e.g.,Assumption2.2).
4Finetuning. Let F be a class of prediction functions where each f = h ◦ ϕ ∈ F can be
expressedasthecompositionofanexpressiverepresentationfunctionϕandapredictionhead h.
Weconsiderapre-trainedmodelϕthatyieldshigh-qualityrepresentationsforsomedownstream
tasks on D and denote F ⊆ F as the class of finetuned models based on ϕ. Assume that
|ϕ
for every (x ,y ) ∈ D, y ∼ P (y | x ) i.i.d. such that there exists fϕ ∈ F with respect to ϕ
i i i i ∗ |ϕ
satisfying(i)E[y | ϕ(x )] = fϕ(x ),and(ii)V[y | ϕ(x )] ≤ σ2 forsomeσ > 0(whichwill
i i ∗ i i i
beformalizedlaterinrespectivesettings).
Dataselection. Insteadof finetuningonthe entiredatasetD,we aimtoselect asmallcoreset
D ⊆ D of size n ≪ N where the generalization is close. Precisely, let D be indexed by
S S
S ⊂ [N] and denoted as (X ,y ) ∈ Rn×d × Rn. With L (f) = 1 (cid:80) ℓ(f(x),y)
and a regularization R : F
S →S
R associated with a
hD ypS erparamn eter( αx,y) ≥∈DS
0, we want
|ϕ ≥0
f = argmin L (f) + α · R(f) to provide a low excess risk over D: ER(f ) :=
S f∈F
|ϕ
DS S
1 (cid:80)N ℓ(f (x ),fϕ(x )).
N i=1 S i ∗ i
2.1 Low-dimensional Linear Probing: Variance Minimization
Warmingupwithlinearprobing,weconcretizethegeneralassumptiononthegroundtruth(i.e.,
E[y | ϕ(x )] = fϕ(x ) = ϕ(x )⊤θ andV[y | ϕ(x )] ≤ σ2)asfollows:
i i ∗ i i ∗ i i
Assumption 2.1 (linear probing ground truth). Assume y = ϕ(X)θ +z for some θ ∈ Rr
∗ ∗
wherez = [z ,··· ,z ]⊤ ∈ RN consistsofi.i.d.entrieswithE[z] = 0 andE(cid:2) zz⊤(cid:3) ≼ σ2I .
1 N N N
Considerthepre-trainedrepresentationsϕ(X) ∈ RN×r andϕ(X ) ∈ Rn×r withrespectivemo-
S
mentsΣϕ := 1ϕ(X)⊤ϕ(X)andΣϕ := 1ϕ(X )⊤ϕ(X ). Forlow-dimensionallinearprob-
N S n S S
ing with r ≤ n (s.t. rank(Σϕ) = r), the linear regression θ = argmin 1 ∥ϕ(X )θ −y ∥2
S S θ n S S 2
has a unique solution with excess risk ER(θ ) = ∥θ −θ ∥2 4 controlled by Σϕ and Σϕ,
S S ∗ Σϕ S
analogoustotheV-optimalitycriterion[5,82]inoptimalexperimentaldesign:
σ2
E[ER(θ )] ≤ tr(Σϕ(Σϕ)−1), (1)
S n S
If D satisfies Σϕ ≼ c Σϕ for some c ≥ n, then E[ER(θ )] ≤ c σ2r (proof in Ap-
S S S S N S S n
pendix B.1), where c characterizes the variance controlled by D , i.e., smaller c implies
S S S
lowervariance.
Despiteitssimplicity,uniformsamplingisoftenobservedinpracticetoserveasastrongbaseline
fordataselection[30],especiallywhennislarge. Inthelow-dimensionallinearprobingscenario,
(1)providesatheoreticaljustificationforsucheffectivenessofuniformsampling:
Proposition 2.1 (Uniform sampling for low-dimensional linear probing (Appendix B.2)). As-
sume there exists (i) B > 0 such that ∥ϕ(x)∥ ≤ B ∀ x ∈ D; and (ii) γ > 0 with Σϕ ≽ γI .
ϕ 2 ϕ r
For S sampled uniformly (with replacement) over D, with probability at least 1 − δ over S,
Σϕ ≼ c Σϕ foranyc > 1ifn ≳ B ϕ4 · r+log(1/δ).
S S S γ2 (1−1/cS)2
That is, for linear probing with sufficiently low dimension r ≪ n, under mild regularity
assumptionsondata,uniformsamplingenjoysanear-optimalgeneralizationO(r/n).
4Foranyu∈Rr,∥u∥ :=(cid:112) u⊤ΣϕuistheseminormassociatedwithΣϕ ≽0.
Σϕ
52.2 High-dimension Finetuning with Low Intrinsic Dimension: Variance-
Bias Tradeoff
Extending the analysis to general finetuning, we consider a set of r finetuning parameters
θ ∈ Rr5 (potentially with r ≫ n) over a pre-trained model ϕ (e.g., θ can be the parameters
of the last layer (i.e., linear probing), last few layers, the entire network, or the LoRA [33]
matrices).
Let F =
(cid:8)
fϕ(·;θ) : X →
R(cid:12)
(cid:12)θ ∈
Rr(cid:9)
be the finetuning function class. Without loss of
|ϕ
generality,weassumezeroinitializationofθ suchthatfϕ(·;0 )correspondstothepre-trained
r
model. Analogous to the assumption in [87], under locality constraint on θ (e.g., ∥θ∥ < 1),
2
thedynamicsoffinetuningfallsinthekernelregime[35]wherefϕ canbeapproximatedbyits
first-orderTaylorexpansion: fϕ(x;θ) ≈ fϕ(x;0 )+∇ fϕ(x;0 )⊤θ. Then,weformalizethe
r θ r
groundtruthasfollows:
Assumption 2.2 (Finetuning ground truth). Given the pre-trained ϕ, there exists a bounded
groundtruthθ ∈ Rr with∥θ ∥ < 1suchthatforall(x,y) ∈ D,(i)E[y | ϕ(x)] = fϕ(x) =
∗ ∗ 2 ∗
fϕ(x;θ ),and(ii)V[y | ϕ(x)] ≤ σ2 forsomeσ > 0.
∗
Intuitively,Assumption2.2impliesthatthepre-trainedmodelfϕ(·;0 )hasareasonablezero-
r
shotperformance. GivenanyS ⊂ [N]with|S| = n,letfϕ(X ;θ) ∈ Rn and∇ fϕ(X ;θ) ∈
S θ S
Rn×r be the evaluation of fϕ(x;θ) and its Jacobian over X at θ. We observe that with
S
z := y − fϕ(X;θ ), Assumption 2.2 implies y − fϕ(X;0 ) ≈ Gθ + z where E[z] =
∗ r ∗
0 and E(cid:2) zz⊤(cid:3) ≼ σ2I ; while G := ∇ fϕ(X;0 ) ∈ RN×r is the Jacobian over D at
N N θ r
initialization.
Theninthekernelregime[35],thefinetuningobjectivemin
θ∈Rr
n1 (cid:13) (cid:13)fϕ(X S;θ)−y S(cid:13) (cid:13)2 2+α∥θ∥2
2
canbewellapproximatedbyaridgeregressionproblem:
θ = argmin 1 (cid:13) (cid:13)∇ fϕ(X ;0 )θ −(cid:0) y −fϕ(X ;0 )(cid:1)(cid:13) (cid:13)2 +α∥θ∥2. (2)
S n θ S r S S r 2 2
θ∈Rr
Recall G := ∇ fϕ(X;0 ) ∈ RN×r and G := ∇ fϕ(X ;0 ) ∈ Rn×r. With the moments
θ r S θ S r
Σϕ = 1G⊤GandΣϕ = 1G⊤G ,theexcessriskER(θ ) = ∥θ −θ ∥2 satisfies6:
N S n S S S S ∗ Σϕ
Theorem 2.2 (Main result I: variance-bias tradeoff (Appendix B.3)). Given S, let P ∈
S
Rr×r be an orthogonal projector onto some subspace S ⊆ Range(Σϕ), and P⊥ = I −
S S r
P be its orthogonal complement. Under Assumption 2.1, there exists an α > 0 such that
S
(2) satisfies E[ER(θ )] ≤ variance + bias with (i) variance = 2σ2 tr(Σϕ(P ΣϕP )†) and
S n S S S
(ii)bias = 2tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2.
S ∗ 2
Specifically, thevariance-bias tradeoffiscontrolled bythe unknownS: expanding S leadsto
highervariance butlowerbias. Reducing the generalization gapinvolvesfindingasuitableS
inthehigh-dimensionalparameterspace, acomputationallychallengingproblemaddressedin
Section3.1.
It is worth highlighting that Theorem 2.2 encapsulates both the low- and high-dimensional
finetuning. For low-dimensional linear probing, (1) is a special case of Theorem 2.2 (up to
5Noticethatristhedimensionofthefinetuningparameterspace. ForlinearprobinginSection2.1,risthe
sameasthepre-trainedrepresentationdimension;butforgeneralfinetuning,rcanbemuchlarger.
6Noticethatforlinearprobing,fϕ(x;θ)=fϕ(x;0 )+∇ fϕ(x;0 )⊤θwith∇ fϕ(x;0 )=ϕ(x). There-
r θ r θ r
fore, the finetuning objective can be exactly formulated as (2), and the excess risk of high-dimensional linear
probingsatisfiesTheorem2.2withΣϕ := 1ϕ(X)⊤ϕ(X)andΣϕ := 1ϕ(X )⊤ϕ(X ).
N S n S S
6constants)withP = I . Whileinhighdimension,anintrinsiclow-dimensionalstructure(e.g.,
S r
Assumption2.3)isnecessaryfortheeffectivenessofdataselection7.
Assumption2.3(Lowintrinsicdimension). ConsiderthesecondmomentΣϕ ≽ 0overD with
(cid:0) (cid:1) (cid:0) (cid:1)
N samples. Letr := min{t ∈ [r] | tr Σϕ −⟨Σϕ⟩ ≤ tr Σϕ /N}betheintrinsicdimension.
t
AssumethatΣϕ hasalowintrinsicdimension: r ≪ min{N,r}.
When the high-dimensional finetuning parameter space has a low intrinsic dimension r ≪
min{N,r},Theorem2.2canbefurtherconcretizedwithsuitableD andassociatedS:
S
Corollary2.3(Exploitation+exploration(AppendixB.3)). UnderthesamesettingasTheo-
rem2.2andAssumption2.3,ifS satisfiesforsomesubspaceS ⊆ Range(Σϕ)withrank(P ) ≍
S S
r andc ≥ n that(i)P (c Σϕ −Σϕ)P ≽ 0and(ii)tr(ΣϕP⊥) ≤ N tr(Σϕ −⟨Σϕ⟩ ),then8
S N S S S S S n r
1
E[ER(θ )] ≤ variance+bias ≲ (cid:0) c σ2r+tr(cid:0) Σϕ(cid:1) ∥θ ∥2(cid:1) . (3)
S n S ∗ 2
Inparticular,withc ,σ ≲ 1,∥θ ∥2 < 1,andtr(Σϕ) ≍ r (dependingonlyonthe lowintrinsic
S ∗ 2
dimension),thegeneralizationachievesafastrateO(r/n),independentofr ≫ r.
In (3), (i) bias is reduced by exploring the parameter space for an S with small low-rank
approximation error tr(ΣϕP⊥) ≤ 1 tr(Σϕ); while (ii) variance is reduced by exploiting
S n
informationinS throughmomentmatching,P (c Σϕ −Σϕ)P ≽ 0,wheresmallerc means
S S S S S
betterexploitation.
3 Sketchy Moment Matching
A gap between Corollary 2.3 and practice is how to find a suitable S efficiently in the high-
dimensional parameter space. In this section, we introduce a simple scalable algorithm for
constructing S and D that satisfies the exploration and exploitation conditions in Corol-
S
lary2.3.
3.1 Find Low Intrinsic Dimension via Gradient Sketching
Forhigh-dimensionalfinetuningwithr ≫ n,acriticallimitofTheorem2.2andCorollary2.3is
that thelarge moment matricesΣϕ,Σϕ are notinvertible, storable, oreven directly computable,
S
due to the prohibitive cost. As a remedy, sketching [31, 85] via Johnson-Lindenstrauss trans-
forms[37]is aclassical dimensionalityreductionstrategythatgets increasingrecent attention
forgradientapproximationinlarge-scalemachinelearningproblems[58,87]9.
Remark3.1(Gradientsketching). Inthehigh-dimensionalsettingwithr ≫ n,toreducethe
dimensionality of the gradients G = ∇ fϕ(X;0 ) ∈ RN×r with a low intrinsic dimension
θ r
r ≪ min{N,r} (Assumption 2.3), we draw a Johnson-Lindenstrauss transform [37] (JLT,
7Otherwise,ifalldirectionsofRange(Σϕ)areequallyimportant,withn≪r,θ learnedfromD necessarily
S S
failstocapturetheorthogonalcomplementofRange(Σϕ)andthereforeE[ER(θ )]≳r−n.
S √ S
8WenotethatincontrasttotheclassicalslowrateO(1/ n)inlowdimension(whenn>r),ridgeregression
on D in the high-dimensional finetuning (with n < r) achieves a fast rate O(1/n). This is granted by the
S
low-ranknessofΣϕ,whichenablesamorefine-grainedanalysisoftheregularization(videAppendixB.3).
S
9Wehighlightakeynuancehere: forfastinfluencefunctionapproximationin[58,87],thegradientoftheloss
functionissketched,whereasinoursetting,wesketchthegradientofthepre-trainedmodelfϕ(x;0 ).
r
7formally in Definition C.1) Γ ∈ Rr×m that projects the dimension-r gradients to a lower
dimension m ≍ r ≪ r: G(cid:101) = GΓ ∈ RN×m. One of the most common constructions of JLT is
the Gaussian embedding (i.e., a Gaussian random matrix with i.i.d. entries Γ ∼ N(0,1/m)
ij
discussedinLemmaC.3,videRemarkC.1forabriefoverviewofvarious(fast)JLTsandtheir
efficiency).
WhilesketchingisknownforpreservingEuclideandistances[37]andprovidingaccuratelow-
rank approximations [31, 52, 85], whether gradient sketching can convert Theorem 2.2 to an
efficientlycomputableformwithoutcompromisingthegeneralizationguarantee? Weanswer
thisquestionaffirmativelywiththefollowingtheorem.
Theorem 3.1(Mainresult II:gradientsketching (formallyinTheorem C.1)). UnderAssump-
tion2.2and2.3withalowintrinsicdimensionr ≪ min{N,r},drawaGaussianembedding
Γ ∈ Rr×m (Lemma C.3) with m ≥ 11r. Let Σ(cid:101)ϕ := Γ⊤ΣϕΓ and Σ(cid:101)ϕ := Γ⊤ΣϕΓ be the
S S
sketched gradient moments. For any D with n > m samples such that rank(Σϕ) = n, and
S S
the⌈1.1r⌉-thlargesteigenvalues (Σ(cid:101)ϕ) ≥ γ forsomeγ > 0,withprobabilityatleast0.9
⌈1.1r⌉ S S S
overΓ,thereexistsα > 0where(2)satisfiesE[ER(θ )] ≲ variance+sketchingerror+bias
S
with (i) variance = σ2 tr(Σ(cid:101)ϕ(Σ(cid:101)ϕ)†), (ii) sketchingerror = σ2 1 ∥Σ(cid:101)ϕ(Σ(cid:101)ϕ)†∥ tr(Σϕ), and
n S n mγS S 2
(iii)bias = 1∥Σ(cid:101)ϕ(Σ(cid:101)ϕ)†∥ tr(Σϕ)∥θ ∥2.
n S 2 ∗ 2
IfS furthersatisfiesΣ(cid:101)ϕ ≼ c Σ(cid:101)ϕ forsomec ≥ n,withm = max{(cid:112) tr(Σϕ)/γ ,11r},
S S S N S
c
E[ER(θ )] ≲ variance+sketchingerror+bias ≲ S (cid:0) σ2m+tr(cid:0) Σϕ(cid:1) ∥θ ∥2(cid:1) . (4)
S n ∗ 2
Comparing (4) with (3), we observe that by controlling the variance with Σ(cid:101)ϕ ≼ c Σ(cid:101)ϕ in low
S S
dimension m ≍ r ≪ r, gradient sketching preserves the fast-rate generalization O(m/n) =
O(r/n) up to constants. That is, gradient sketching implicitly finds a random subspace S ⊆
Range(Σϕ) (vide (9)) that satisfies the exploration assumption in Corollary 2.3. Meanwhile,
S
thechoiceofsketchingsizembalancesthetradeoffbetweenvarianceandsketchingerror: a
largermreducesthesketchingerroratthecostofhighervariance. Suchtradeoffisoptimizedat
(cid:112)
m = tr(Σϕ)/γ .
S
3.2 Control Variance via Moment Matching
Giventheintrinsiclow-dimensionalstructurewithsmallbiasinSection3.1,Theorem3.1con-
nectsgeneralizationtothevariancecontrolledbythematchingbetweenΣ(cid:101)ϕ andΣ(cid:101)ϕ. Specifically,
S
whentheselecteddataD satisfiesΣ(cid:101)ϕ ≼ c Σ(cid:101)ϕ forsomec ≥ n,wehavetr(Σ(cid:101)ϕ(Σ(cid:101)ϕ)†) ≤ c m
S S S S N S S
and∥Σ(cid:101)ϕ(Σ(cid:101)ϕ)†∥ ≤ c upperbounded,leadingtothefast-rategeneralizationin(4).
S 2 S
Whiledirectlyminimizingtr(Σ(cid:101)ϕ(Σ(cid:101)ϕ)†)involvesintegerprogrammingandpseudoinverse,caus-
S
inghardandnumericallyunstableoptimization,Σ(cid:101)ϕ ≼ c Σ(cid:101)ϕ hasastraightforwardrelaxation
S S
(vide Remark 3.2), leading to the simple and stable moment matching objective (5) in Algo-
rithm3.1.
Remark 3.2 (Relaxing Σ(cid:101)ϕ ≼ c Σ(cid:101)ϕ to (5)). Given the spectral decomposition Σ(cid:101)ϕ = VΛV⊤,
S S
Σ(cid:101)ϕ ≼ c Σ(cid:101)ϕ canberewrittenasV⊤(1G(cid:101)⊤G(cid:101) )V ≽ 1 Λ,and (5)isarelaxation: (i)insteadof
S S n S S cS
enforcingΣ(cid:101)ϕ ≼ c Σ(cid:101)ϕ,constraintsareonlyimposedonthediagonal: v⊤(1G(cid:101)⊤G(cid:101) )v ≥ λ /c ,
S S j n S S j j S
j ∈ [m];and(ii)theselectionofS isrelaxedtoaweightvectors ∈ ∆ withlinearconstraints
N
0 ≤ s ≤ 1/n. Free of integer constraints and pseudoinverse, the quadratic data selection
i
8Algorithm3.1SketchyMomentMatching(SkMM)
1: Input: fϕ(·;0 ),n ≪ N,m < n,a > 0,c ∈ [n,1].
r S N
2: Drawa(fast)Johnson-LindenstrausstransformΓ ∈ Rr×m (Remark3.1).
3: ComputegradientsketchingG(cid:101) = ∇ fϕ(X;0 )Γ ∈ RN×m. (Noticethatgradientsketching
θ r
canbecomputeddistributedlyinparallelwithacostofnomorethanonetrainingepoch.)
4: ComputethespectraldecompositionofΣ(cid:101)ϕ = 1G(cid:101)⊤G(cid:101) ≽ 0: Σ(cid:101)ϕ = VΛV⊤ where
N
(a) V = [v ,··· ,v ] ∈ Rm×m consistsoftheorthonormaleigenvectors,and
1 r
(b) Λ = diag(λ ,··· ,λ )containsdescendingeigenvaluesλ ≥ ··· ≥ λ ≥ 0.
1 m 1 m
5: Initializes = [s ,··· ,s ]withs = 1 onnuniformlysampledi’sands = 0elsewhere.
1 N i n i
6: Letdiag(s) ∈ RN×N beadiagonalmatrixwithsondiagonal. Optimizing:
m
(cid:88)(cid:16) (cid:17)2
min min v⊤G(cid:101)⊤diag(s)G(cid:101)v −γ ·λ
j j j j
s∈∆N γ=[γ1,···,γm]∈Rm (5)
j=1
s.t. 0 ≤ s ≤ 1/n∀i ∈ [N], γ ≥ 1/c ∀j ∈ [m].
i j S
7: Output: S ⊂ [N]bysamplingndatafroms ∈ ∆ withoutreplacement.
N
objectivewithlinearconstraintsin(5)canbesolvedefficientlyandstablyviaprojectedgradient
descent.
Remark3.3(Smallerc impliesbettermomentmatching). InAlgorithm3.1,c controlsthe
S S
strength ofmoment matching. Intuitively, smallerc enforces Σ(cid:101)ϕ toexploit moreinformation in
S S
Σ(cid:101)ϕ, bringing lower variance and better generalization. While the lower bound c ≥ n (vide
S N
Appendix B.1) could be tight in theory (e.g., when G(cid:101) consists of N − n rows of zeros), the
smallest feasible c depends on the data distribution and tends to be larger in practice (e.g.,
S
c ≈ 1intheexperiments).
S
4 Experiments
4.1 Synthetic High-dimensional Linear Probing
Togroundthetheoreticalinsightonvariance-biastradeoffinhigh-dimensionalfinetuning,we
simulatelinearprobingwithasyntheticunderdeterminedridgeregressionproblem10.
Setup. WeconsiderasetofN = 2000sampleswithhigh-dimensionalpre-trainedrepresenta-
tions ϕ(X) ∈ RN×r, r = 2400, modeled by a Gaussian mixture model (GMM) consisting of
r = 8well-separatedclusters, eachwithrandomsizes andvariances(videFigure 2). Samples
within each cluster share the same randomly generated label. We solve the ridge regression
problem(2)overtheselectedcoresetofnsampleswithhyperparameterαtuning. Theempirical
risk is evaluated over the full dataset L (θ ) = 1 ∥ϕ(X)θ −y∥2 (vide Appendix D.1 for
D S N S 2
implementationdetails).
Dataselection. ForSkMM(Algorithm3.1),weuseasketchingdimensionm = 4r = 32and
setc = 0.999. Weoptimize(5)viaAdam[42]withconstraintprojectionunderlearningrate
S
10−7 for104 iterationsandsampleS froms ∈ ∆ withthelowestobjectivevalue.
N
10Ourexperimentcodeisavailableathttps://github.com/Xiang-Pan/sketchy_moment_matching
9Figure2: Selectingn = 80data(coloredinred)fromtheGMMdataset. Intuitively,acoreset
D withlowbiascontainsatleastonesamplepercluster;whereasalow-varianceD selects
S S
moredatafromclusterswithlargervariance. WerecallfromTheorem2.2thatthevariance-bias
balanceisessentialforgoodgeneralization.
We compare SkMM to representative unsupervised data selection methods for regression, in-
cludinguniform,leveragescore[3,15,24,48,65],adaptivesampling[13,20],herding[12,84],
and k-center greedy [67]. Specifically, (i) SkMM, truncated leverage score (T-leverage),
and ridge leverage score sampling (R-leverage) can be viewed as different ways of vari-
ance-bias balancing; (ii) adaptive sampling (Adaptive) and k-center greedy (K-center)
focus on bias reduction (i.e., providing good low-rank approximation/clustering for ϕ(X));
while(iii)Herdinganduniformsampling(Uniform)reducevariance(videAppendixD.2
forbaselinedetails).
Table1: Empiricalrisk L (θ )ontheGMM datasetatvariousn,under thesamehyperparam-
D S
eter tuning where ridge regression over the full dataset D with N = 2000 samples achieves
L (θ ) =2.95e-3. Formethodsinvolvingsampling,resultsarereportedover8randomseeds.
D [N]
n 48 64 80 120 400 800 1600
Herding 7.40e+2 7.40e+2 7.40e+2 7.40e+2 7.38e+2 1.17e+2 2.95e-3
Uniform (1.14±2.71)e-1 (1.01±2.75)e-1 (3.44±0.29)e-3 (3.13±0.14)e-3 (2.99±0.03)e-3 (2.96±0.01)e-3 (2.95±0.00)e-3
K-center (1.23±0.40)e-2 (9.53±0.60)e-2 (1.12±0.45)e-2 (2.73±1.81)e-2 (5.93±4.80)e-2 (1.18±0.64)e-1 (1.13±0.70)e+0
Adaptive (3.81±0.65)e-3 (3.79±1.37)e-3 (4.83±1.90)e-3 (4.03±1.35)e-3 (3.40±0.67)e-3 (7.34±3.97)e-3 (3.19±0.16)e-3
T-leverage (0.99±1.65)e-2 (3.63±0.49)e-3 (3.30±0.30)e-3 (3.24±0.14)e-3 (2.98±0.01)e-3 (2.96±0.01)e-3 (2.95±0.00)e-3
R-leverage (4.08±1.58)e-3 (3.48±0.43)e-3 (3.25±0.31)e-3 (3.09±0.06)e-3 (3.00±0.02)e-3 (2.97±0.01)e-3 (2.95±0.00)e-3
SkMM (3.54±0.51)e-3 (3.31±0.15)e-3 (3.12±0.07)e-3 (3.07±0.08)e-3 (2.98±0.02)e-3 (2.96±0.01)e-3 (2.95±0.00)e-3
We observe from Figure 2 and Table 1 that balancing the variance-bias tradeoff is crucial
for the generalization of data selection in high dimensions. In particular, SkMM achieves
the best empirical risk across different coreset sizes n, especially when n is small. While as
n/N → 1, uniform sampling provides a strong baseline, coinciding with common empirical
observations[30].
104.2 Experiments on Image Classification Tasks
Whileouranalysisfocusesondataselectionforfinetuningregressionmodels,anaturalquestion
iswhethertheideaofSkMMappliestobroaderscopes. Toanswerthis,weextendourempirical
investigation to classification. In particular, we consider an imbalanced classification task:
StanfordCars[44]with196classes,8144trainingsamples,and8041testingsampleswherethe
classesarehighlyimbalancedwithtrainingsamplesizesrangingfrom24to68.
Table2: AccuracyandF1score(%)ofLPoverCLIPonStanfordCars
n 2000 2500 3000 3500 4000
Acc 67.63±0.17 70.59±0.19 72.49±0.19 74.16±0.22 75.40±0.16
UniformSampling
F1 64.54±0.18 67.79±0.23 70.00±0.20 71.77±0.23 73.14±0.12
Acc 67.22±0.16 71.02±0.13 73.17±0.22 74.64±0.18 75.71±0.29
Herding[84]
F1 64.07±0.23 68.28±0.15 70.64±0.28 72.22±0.26 73.26±0.39
Acc 67.64±0.13 70.82±0.23 72.66±0.12 74.46±0.17 75.77±0.12
ContextualDiversity[1]
F1 64.51±0.17 68.18±0.25 70.05±0.11 72.13±0.15 73.35±0.07
Acc 67.60±0.24 70.85±0.27 73.07±0.26 74.63±0.21 76.00±0.20
Glister[40]
F1 64.50±0.34 68.07±0.38 70.47±0.35 72.18±0.25 73.69±0.24
Acc 67.27±0.07 70.38±0.07 72.56±0.05 74.67±0.06 75.77±0.12
GraNd[59]
F1 64.04±0.09 67.48±0.09 69.81±0.08 72.13±0.05 73.44±0.13
Acc 67.59±0.10 70.99±0.05 72.54±0.07 74.81±0.05 75.74±0.01
Forgetting[73]
F1 64.85±0.13 68.53±0.07 70.30±0.05 72.59±0.04 73.74±0.02
Acc 67.77±0.29 70.73±0.22 73.24±0.22 74.57±0.23 75.71±0.15
DeepFool[55]
F1 64.16±0.68 68.49±0.53 70.93±0.32 72.44±0.27 73.79±0.15
Acc 67.95±0.11 71.00±0.10 73.28±0.10 75.02±0.08 75.82±0.06
Entropy[16]
F1 64.55±0.10 67.95±0.12 70.68±0.12 72.46±0.12 73.29±0.04
Acc 67.53±0.14 71.19±0.09 73.09±0.14 74.66±0.11 75.57±0.13
Margin[16]
F1 64.16±0.15 68.33±0.14 70.37±0.17 72.03±0.11 73.14±0.20
Acc 67.68±0.11 70.99±0.14 73.04±0.05 74.65±0.09 75.58±0.08
LeastConfidence[16]
F1 64.09±0.20 68.03±0.20 70.30±0.07 72.02±0.10 73.15±0.12
Acc 68.27±0.03 71.53±0.05 73.61±0.02 75.12±0.01 76.34±0.02
SkMM-LP
F1 65.29±0.03 68.75±0.06 71.14±0.03 72.64±0.02 74.02±0.10
Finetuning. We consider two common ways of finetuning: (i) linear probing (LP) over the
lastlayerand(ii)funetuning(FT)overthelastfewlayers,coveringboththelow-(i.e.,n ≥ r for
LP)andhigh-dimensional(i.e.,r > nforFT)settings. ForLP,welearnthelastlayeroverthe
embeddingsfromaCLIP-pretrainedViT-B/32[62]withalearningrateof10−1. ForFT11,we
finetuningthelasttwolayersofanImageNet-pretrainedResNet18[32]withalearningrateof
10−2. Inbothsettings,weoptimizeviaAdamfor50epochs.
Data selection. For SkMM-LP, the gradients (of the last layer) are given by the pretrained
featuresfromCLIPofdimensionr = 512. ForSkMM-FT,thegradients(ofthelasttwolayers)
are calculated based on a random classification head. We tune the sketching dimension m ∈
{32,64,128,256,512} and the lower bound for slackness variables c ∈ {0.6,0.7,0.8,0.9}.
S
Withinsuitableranges,smallermandlargerc leadtobetterperformanceinthelowdataregime.
S
Intuitively,smallermencouragesvariancereductioninamorecompressedsubspaceandlarger
c leadstoeasieroptimization.
s
11WenoticethatfinetuningthelastfewlayersofstrongpretrainedmodelslikeCLIPcandistortthefeaturesand
hurttheperformance,asstudiedin[45]. Therefore,westaywithaweakerpretrainedmodelforfinetuning.
11Table3: AccuracyandF1score(%)ofFTover(thelasttwolayersof)ResNet18onStanfordCars
n 2000 2500 3000 3500 4000
Acc 29.19±0.37 32.83±0.19 35.69±0.35 38.31±0.16 40.35±0.26
UniformSampling
F1 26.14±0.39 29.91±0.16 32.80±0.37 35.38±0.19 37.51±0.23
Acc 29.19±0.21 32.42±0.16 35.83±0.24 38.30±0.19 40.51±0.19
Herding[84]
F1 25.90±0.24 29.48±0.23 32.89±0.27 35.50±0.22 37.56±0.21
Acc 28.50±0.34 32.66±0.27 35.67±0.32 38.31±0.15 40.53±0.18
ContextualDiversity[1]
F1 25.65±0.40 29.79±0.29 32.86±0.31 35.55±0.14 37.81±0.23
Acc 29.16±0.26 32.91±0.19 36.03±0.20 38.16±0.12 40.47±0.16
Glister[40]
F1 26.33±0.19 30.05±0.28 33.26±0.18 35.41±0.14 37.63±0.17
Acc 28.59±0.17 32.67±0.20 35.83±0.16 38.58±0.15 40.70±0.11
GraNd[59]
F1 25.66±0.15 29.70±0.22 32.76±0.16 35.72±0.15 37.83±0.11
Acc 28.61±0.31 32.48±0.28 35.18±0.24 37.78±0.22 40.24±0.13
Forgetting[73]
F1 25.64±0.25 29.58±0.30 32.38±0.20 35.16±0.18 37.41±0.14
Acc 24.97±0.20 29.02±0.17 32.60±0.18 35.59±0.24 38.20±0.22
DeepFool[55]
F1 22.11±0.11 26.08±0.29 29.83±0.27 32.92±0.33 35.47±0.22
Acc 28.87±0.13 32.84±0.20 35.64±0.20 37.96±0.11 40.29±0.27
Entropy[16]
F1 25.95±0.17 30.03±0.17 32.85±0.23 35.19±0.12 37.33±0.34
Acc 29.18±0.12 32.73±0.15 35.67±0.30 38.27±0.20 40.58±0.06
Margin[16]
F1 26.15±0.12 29.66±0.05 32.86±0.30 35.61±0.17 37.77±0.07
Acc 29.05±0.07 32.88±0.13 35.66±0.18 38.25±0.20 39.91±0.09
LeastConfidence[16]
F1 26.18±0.04 30.03±0.14 32.79±0.15 35.42±0.16 37.14±0.12
Acc 29.44±0.09 33.48±0.04 36.11±0.12 39.18±0.03 41.77±0.07
SkMM-FT
F1 26.71±0.10 30.75±0.05 33.24±0.05 36.38±0.05 39.07±0.10
WecompareSkMMtovariousunsupervisedand(weakly)superviseddataselectionmethodsfor
classification,includinguniformsampling,herding[84],ContextualDiversity[1],Glister[40],
GraNd[59],Forgetting[73],DeepFool[55],aswellasthreeuncertainty-basedmethods,Entropy,
Margin,andLeastConfidence[16].
Observations. WefirstobservethatforbothLP(Table2)andFT(Table3),SkMMachieves
competitive finetuning accuracy on StanfordCars. Since SkMM is an unsupervised process
agnosticoftrueclasssizes,theappealingperformanceofSkMMontheimbalancedStanfordCars
dataset echoes the ability of SkMM to handle data selection among clusters of various sizes
throughvariance-biasbalance(cf. syntheticexperimentsinFigure2). Meanwhile,forLPinthe
low-dimensional setting (Table 2), uniform sampling provides a surprisingly strong baseline.
ThiscoincideswiththetheoreticalinsightfromProposition2.1andtheempiricalobservations
in[30].
5 Discussion, Limitations, and Future Directions
We investigated data selection for finetuning in both low and high dimensions from a theoretical
perspective. Beyondvariancereductioninlowdimension,ouranalysisrevealedthevariance-
bias tradeoff in data selection for high-dimensional finetuning with low intrinsic dimension
r, balancing which led to a fast-rate generalization O(r/n). For efficient control of such
variance-biastradeoffinpractice,weintroducedSkMMthatfirstexploresthehigh-dimensional
parameterspaceviagradientsketchingandthenexploitstheresultinglow-dimensionalsubspace
via moment matching. Theoretically, we showed that the low-dimensional subspace from
12gradientsketchingpreservesthefast-rategeneralization. Moreover,wegroundthetheoretical
insightonbalancingthevariance-biastradeoffviasyntheticexperiments,whiledemonstrating
theeffectivenessofSkMMforfinetuningrealvisiontasks.
In this work, we focus only on moment matching via optimization inspired by the analysis
for variance reduction after gradient sketching. Nevertheless, there is a remarkable variety
of existing low-dimensional data selection strategies (e.g., via greedy selection or sampling)
thatcouldpotentiallybeextendedtohighdimensionsleveragingsketchingasanefficientpre-
processingstep. Inlinearalgebra,sketchinghasbeenwidelystudiedforaccelerating,aswellas
stabilizing,large-scalelow-rankapproximationsandlinearsolvers. However,theintuitionsand
theoriestheremayormaynotbedirectlyapplicabletothestatisticallearningregime. Inlight
of the high-dimensional nature of deep learning where sketching brings an effective remedy,
we hopethat providinga rigorous generalizationanalysis for sketching in dataselection would
makeasteptowardbridgingtheclassicalwisdomofsketchingandtheanalogouschallengesin
modernlearningproblems.
References
[1] S.Agarwal,H.Arora,S.Anand, andC.Arora. Contextualdiversityforactivelearning. In
ComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,
2020,Proceedings,PartXVI16,pages137–153.Springer,2020.
[2] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the
effectivenessoflanguagemodelfine-tuning. arXivpreprintarXiv:2012.13255,2020.
[3] A.Alaoui andM.W.Mahoney. Fastrandomizedkernelridge regressionwithstatistical
guarantees. Advancesinneuralinformationprocessingsystems,28,2015.
[4] A. Albalak, Y. Elazar, S. M. Xie, S. Longpre, N. Lambert, X. Wang, N. Muennighoff,
B. Hou, L. Pan, H. Jeong, et al. A survey on data selection for language models. arXiv
preprintarXiv:2402.16827,2024.
[5] Z. Allen-Zhu, Y. Li, A. Singh, and Y. Wang. Near-optimal discrete optimization for
experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174,
2017.
[6] Z. Borsos, M. Mutny, and A. Krause. Coresets via bilevel optimization for continual
learningandstreaming. Advancesinneural information processingsystems,33:14879–
14890,2020.
[7] M.Cˇerny` and M.Hladík. Twocomplexity resultsonc-optimalityin experimentaldesign.
ComputationalOptimizationandApplications,51(3):1397–1408,2012.
[8] K.ChalonerandI.Verdinelli. Bayesianexperimentaldesign: Areview. Statisticalscience,
pages273–304,1995.
[9] M.Charikar,K.Chen,andM.Farach-Colton. Findingfrequentitemsindatastreams. In
International Colloquium on Automata, Languages, and Programming, pages 693–703.
Springer,2002.
13[10] S.ChatterjeeandA.S.Hadi. Influentialobservations,highleveragepoints,andoutliersin
linearregression. Statisticalscience,pages379–393,1986.
[11] X.ChenandE.Price. Activeregressionvialinear-samplesparsification. InConferenceon
LearningTheory,pages663–695.PMLR,2019.
[12] Y.Chen,M.Welling,andA.Smola. Super-samplesfromkernelherding. arXivpreprint
arXiv:1203.3472,2012.
[13] Y. Chen, E. N. Epperly, J. A. Tropp, and R. J. Webber. Randomly pivoted cholesky:
Practical approximation of a kernel matrix with few entry evaluations. arXiv preprint
arXiv:2207.06503,2022.
[14] M. B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In
Proceedingsofthe twenty-seventh annualACM-SIAMsymposiumon Discrete algorithms,
pages278–287.SIAM,2016.
[15] M.B.Cohen,C.Musco,andC.Musco. Inputsparsitytimelow-rankapproximationvia
ridgeleveragescoresampling. In Proceedings oftheTwenty-Eighth AnnualACM-SIAM
SymposiumonDiscreteAlgorithms,pages1758–1777.SIAM,2017.
[16] C.Coleman,C.Yeh,S.Mussmann,B.Mirzasoleiman,P.Bailis,P.Liang,J.Leskovec,and
M.Zaharia. Selectionviaproxy: Efficientdataselectionfordeeplearning. arXivpreprint
arXiv:1906.11829,2019.
[17] M.DerezinskiandM.W.Mahoney. Determinantalpointprocessesinrandomizednumeri-
callinearalgebra. NoticesoftheAmericanMathematicalSociety,68(1):34–45,2021.
[18] M. Derezinski, R. Khanna, and M. W. Mahoney. Improved guarantees and a multiple-
descentcurveforcolumnsubsetselectionandthenystrommethod. AdvancesinNeural
InformationProcessingSystems,33:4953–4964,2020.
[19] M. Derezin´ski, M. K. Warmuth, and D. Hsu. Unbiased estimators for random design
regression. JournalofMachineLearningResearch,23(167):1–46,2022.
[20] A.DeshpandeandS.Vempala. Adaptivesamplingandfastlow-rankmatrixapproximation.
InInternationalWorkshoponApproximationAlgorithmsforCombinatorialOptimization,
pages292–303.Springer,2006.
[21] A.Deshpande,L.Rademacher,S.S.Vempala,andG.Wang. Matrixapproximationand
projectiveclusteringviavolumesampling. TheoryofComputing,2(1):225–247,2006.
[22] Y. Dong and P.-G. Martinsson. Simpler is better: a comparative study of randomized
pivotingalgorithmsforcurandinterpolativedecompositions. AdvancesinComputational
Mathematics,49(4):66,2023.
[23] Y.Dong,C.Chen,P.-G.Martinsson,andK.Pearce. Robustblockwiserandompivoting:
Fastandaccurateadaptiveinterpolativedecomposition. arXivpreprintarXiv:2309.16002,
2023.
[24] P.Drineas,M.W.Mahoney,andS.Muthukrishnan. Relative-errorcurmatrixdecomposi-
tions. SIAMJournalonMatrixAnalysisandApplications,30(2):844–881,2008.
14[25] P. Drineas, M. W. Mahoney, S. Muthukrishnan, and T. Sarlós. Faster least squares
approximation. Numerischemathematik,117(2):219–249,2011.
[26] V.V.Fedorov. Theoryofoptimalexperiments. Elsevier,2013.
[27] S.Y.Gadre,G.Ilharco,A.Fang,J.Hayase,G.Smyrnis,T.Nguyen,R.Marten,M.Worts-
man,D. Ghosh,J.Zhang, etal. Datacomp: In searchofthenextgenerationof multimodal
datasets. AdvancesinNeuralInformationProcessingSystems,36,2024.
[28] K.GotoandR.VanDeGeijn. High-performanceimplementationofthelevel-3blas. ACM
TransactionsonMathematicalSoftware(TOMS),35(1):1–14,2008.
[29] S.Goyal,P.Maini,Z.C.Lipton,A.Raghunathan,andJ.Z.Kolter. Scalinglawsfordata
filtering–data curation cannot be compute agnostic. arXiv preprint arXiv:2404.07177,
2024.
[30] C.Guo, B.Zhao, andY. Bai. Deepcore: Acomprehensive libraryforcoreset selectionin
deeplearning. InInternationalConferenceonDatabaseandExpertSystemsApplications,
pages181–195.Springer,2022.
[31] N.Halko,P.-G.Martinsson,andJ.A.Tropp. Findingstructurewithrandomness: Proba-
bilisticalgorithmsforconstructingapproximatematrixdecompositions. SIAMreview,53
(2):217–288,2011.
[32] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778,2016.
[33] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. Lora:
Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
[34] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse
ofdimensionality. InProceedingsofthethirtiethannualACMsymposiumonTheoryof
computing,pages604–613,1998.
[35] A.Jacot,F.Gabriel,andC.Hongler.Neuraltangentkernel: Convergenceandgeneralization
inneuralnetworks. Advancesinneuralinformationprocessingsystems,31,2018.
[36] A. H. Jiang, D. L.-K. Wong, G. Zhou, D. G. Andersen, J. Dean, G. R. Ganger, G. Joshi,
M.Kaminksy,M.Kozuch,Z.C.Lipton,etal. Acceleratingdeeplearningbyfocusingon
thebiggestlosers. arXivpreprintarXiv:1910.00762,2019.
[37] W.B.Johnson. Extensionsoflipshitzmappingintohilbertspace. InConferencemodern
analysisandprobability,1984,pages189–206,1984.
[38] S.Joshi,A.Jain,A.Payani,andB.Mirzasoleiman. Data-efficientcontrastivelanguage-
imagepretraining: Prioritizingdataqualityoverquantity. arXivpreprintarXiv:2403.12267,
2024.
[39] D.M.KaneandJ.Nelson. Sparserjohnson-lindenstrausstransforms. JournaloftheACM
(JACM),61(1):1–23,2014.
15[40] K.Killamsetty,D.Sivasubramanian,G.Ramakrishnan,andR.Iyer. Glister: Generalization
baseddatasubsetselectionforefficientandrobustlearning. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume35,pages8110–8118,2021.
[41] K.Killamsetty,X.Zhao,F.Chen,andR.Iyer. Retrieve: Coresetselectionforefficientand
robustsemi-supervisedlearning. Advancesinneuralinformationprocessingsystems,34:
14488–14501,2021.
[42] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
[43] G.Kolossov,A.Montanari,andP.Tandon. Towardsastatisticaltheoryofdataselection
underweaksupervision. arXivpreprintarXiv:2309.14563,2023.
[44] J.Krause,J.Deng,M.Stark,andL.Fei-Fei. Collectingalarge-scaledatasetoffine-grained
cars. 2013.
[45] A. Kumar, A. Raghunathan, R. Jones, T. Ma, and P. Liang. Fine-tuning can distort
pretrainedfeaturesandunderperformout-of-distribution. arXivpreprintarXiv:2202.10054,
2022.
[46] B.W.LarsenandT.G.Kolda. Sketchingmatrixleastsquaresvialeveragescoresestimates.
arXivpreprintarXiv:2201.10638,2022.
[47] D. D. Lewis. A sequential algorithm for training text classifiers: Corrigendum and
additionaldata. InAcmSigirForum,volume29,pages13–19.ACMNewYork,NY,USA,
1995.
[48] M. Li, G. L. Miller, and R. Peng. Iterative row sampling. In 2013 IEEE 54th Annual
SymposiumonFoundationsofComputerScience,pages127–136.IEEE,2013.
[49] D.V.Lindley. Onameasureoftheinformationprovidedbyanexperiment. TheAnnalsof
MathematicalStatistics,27(4):986–1005,1956.
[50] M.W.MahoneyandP.Drineas. Curmatrixdecompositionsforimproveddataanalysis.
ProceedingsoftheNationalAcademyofSciences,106(3):697–702,2009.
[51] T. Mai, C. Musco, and A. Rao. Coresets for classification–simplified and strengthened.
AdvancesinNeuralInformationProcessingSystems,34:11643–11654,2021.
[52] P.-G.MartinssonandJ.A.Tropp. Randomizednumericallinearalgebra: Foundationsand
algorithms. ActaNumerica,29:403–572,2020.
[53] X.MengandM.W.Mahoney. Low-distortionsubspaceembeddingsininput-sparsitytime
andapplicationstorobustlinearregression. InProceedingsoftheforty-fifthannualACM
symposiumonTheoryofcomputing,pages91–100,2013.
[54] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of
machine learning models. In International Conference on Machine Learning, pages
6950–6960.PMLR,2020.
[55] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate
methodtofooldeepneuralnetworks. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages2574–2582,2016.
16[56] A. Munteanu, C. Schwiegelshohn, C. Sohler, and D. Woodruff. On coresets for logistic
regression. AdvancesinNeuralInformationProcessingSystems,31,2018.
[57] J.NelsonandH.L.Nguyên. Osnap: Fasternumericallinearalgebraalgorithmsviasparser
subspaceembeddings. In2013ieee54thannualsymposiumonfoundationsofcomputer
science,pages117–126.IEEE,2013.
[58] S. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. Madry. Trak: Attributing model
behavioratscale. arXivpreprintarXiv:2303.14186,2023.
[59] M.Paul,S.Ganguli,andG.K.Dziugaite. Deeplearningonadatadiet: Findingimportant
examples early in training. Advances in Neural Information Processing Systems, 34:
20596–20607,2021.
[60] K. J.Pearce, C. Chen,Y. Dong,and P.-G.Martinsson. Adaptiveparallelizable algorithms
forinterpolativedecompositionsviapartiallypivotedlu. arXivpreprintarXiv:2310.09417,
2023.
[61] F.Pukelsheim. Optimaldesignofexperiments. SIAM,2006.
[62] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,
P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language
supervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[63] G. Raskutti and M. W. Mahoney. A statistical perspective on randomized sketching for
ordinaryleast-squares. JournalofMachineLearningResearch,17(213):1–31,2016.
[64] M.RudelsonandR.Vershynin. Smallestsingularvalueofarandomrectangularmatrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant
InstituteofMathematicalSciences,62(12):1707–1739,2009.
[65] T. Sarlos. Improved approximation algorithms for large matrices via random projections.
In 2006 47th annual IEEE symposium on foundations of computer science (FOCS’06),
pages143–152.IEEE,2006.
[66] C.Schuhmann,R.Beaumont,R.Vencu,C.Gordon,R.Wightman,M.Cherti,T.Coombes,
A.Katta,C.Mullis,M.Wortsman,etal. Laion-5b: Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. AdvancesinNeuralInformationProcessingSystems,
35:25278–25294,2022.
[67] O.Sener andS.Savarese. Active learning forconvolutionalneuralnetworks: Acore-set
approach. arXivpreprintarXiv:1708.00489,2017.
[68] H.S.Seung,M.Opper,andH.Sompolinsky. Querybycommittee. InProceedingsofthe
fifthannualworkshoponComputationallearningtheory,pages287–294,1992.
[69] A.Shimizu,X.Cheng,C.Musco,andJ.Weare. Improvedactivelearningviadependent
leveragescoresampling. arXivpreprintarXiv:2310.04966,2023.
[70] B. Sorscher, R. Geirhos,S. Shekhar,S. Ganguli, andA. Morcos. Beyondneural scaling
laws: beating power law scaling via data pruning. Advances in Neural Information
ProcessingSystems,35:19523–19536,2022.
17[71] D.B.Szyld. Themanyproofsofanidentityonthenormofobliqueprojections. Numerical
Algorithms,42:309–323,2006.
[72] D.TingandE.Brochu. Optimalsubsamplingwithinfluencefunctions. Advancesinneural
informationprocessingsystems,31,2018.
[73] M.Toneva,A.Sordoni,R.T.d.Combes,A.Trischler,Y.Bengio,andG.J.Gordon. An
empiricalstudyofexampleforgettingduringdeepneuralnetworklearning. arXivpreprint
arXiv:1812.05159,2018.
[74] J. A. Tropp. Improved analysis of the subsampled randomized hadamard transform.
AdvancesinAdaptiveDataAnalysis,3(01n02):115–126,2011.
[75] J.A.Tropp,A.Yurtsever,M.Udell,andV.Cevher. Fixed-rankapproximationofapositive-
semidefinite matrix from streaming data. Advances in Neural Information Processing
Systems,30,2017.
[76] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM
JournalonMathematicsofDataScience,1(1):144–160,2019.
[77] R. Vershynin. High-dimensional probability: An introduction with applications in data
science,volume47. Cambridgeuniversitypress,2018.
[78] K.Vodrahalli,K.Li,andJ.Malik. Arealltrainingexamplescreatedequal? anempirical
study. arXivpreprintarXiv:1811.12569,2018.
[79] S. Voronin and P.-G. Martinsson. Efficient algorithms for cur and interpolative matrix
decompositions. AdvancesinComputationalMathematics,43:495–516,2017.
[80] M.J.Wainwright. High-dimensionalstatistics: Anon-asymptoticviewpoint,volume48.
CambridgeUniversityPress,2019.
[81] H. Wang, R. Zhu, and P. Ma. Optimal subsampling for large sample logistic regression.
JournaloftheAmericanStatisticalAssociation,113(522):829–844,2018.
[82] Y.Wang,A.W.Yu,andA.Singh. Oncomputationallytractableselectionofexperiments
inmeasurement-constrainedregressionmodels. JournalofMachineLearningResearch,
18(143):1–41,2017.
[83] Y.Wang,Y.Chen,W.Yan,K.Jamieson,andS.S.Du. Variancealignmentscore: Asimple
buttough-to-beatdataselectionmethodformultimodalcontrastivelearning. arXivpreprint
arXiv:2402.02055,2024.
[84] M. Welling. Herding dynamical weights to learn. In Proceedings of the 26th annual
internationalconferenceonmachinelearning,pages1121–1128,2009.
[85] D. P. Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and
Trends®inTheoreticalComputerScience,10(1–2):1–157,2014.
[86] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for
the approximation of matrices. Applied and Computational Harmonic Analysis, 25(3):
335–366,2008.
18[87] M.Xia,S.Malladi,S.Gururangan,S.Arora,andD.Chen. Less: Selectinginfluentialdata
fortargetedinstructiontuning. arXivpreprintarXiv:2402.04333,2024.
[88] X.Xia, J. Liu, J. Yu, X.Shen, B. Han,and T.Liu. Moderate coreset: A universalmethod
ofdataselectionforreal-worlddata-efficientdeeplearning. InTheEleventhInternational
ConferenceonLearningRepresentations,2022.
[89] S.Yang,Z.Xie,H.Peng,M.Xu,M.Sun,andP.Li. Datasetpruning: Reducingtraining
databyexamininggeneralizationinfluence. arXivpreprintarXiv:2205.09329,2022.
[90] H. Zheng, R. Liu, F. Lai, and A. Prakash. Coverage-centric coreset selection for high
pruningrates. arXivpreprintarXiv:2210.15809,2022.
19A Additional Notations
GivenanymatrixA ∈ Rn×d,alongwithindicesi ∈ [n],j ∈ [d],I ⊆ [n],andJ ⊆ [d],let[A]
i,j
be the (i,j)-th entry of A, [A] be the i-th row (or the i-th entry if A ∈ Rn is a vector), and
i
[A] bethej-thcolumn;A = [A] consistsofrowsinAindexedbyI;andletA = [A]
:,j I I,: I,J I,J
bethesubmatrixofAwithrowsindexedbyI andcolumnsindexedbyJ.
B Proofs for Section 2.1
B.1 Proofs of (1)
Proofof (1)andbeyond. Under the assumption rank(ϕ(X )) = r, both ϕ(X ),ϕ(X) have
S S
fullcolumnrank. Thereforeϕ(X )†ϕ(X ) = ϕ(X)†ϕ(X) = I ,andθ = ϕ(X )†y . Then,
S S r S S S
sincey = ϕ(X)θ +zandy = ϕ(X )θ +z ,wehave
∗ S S ∗ S
(cid:16) (cid:17)
θ −θ = ϕ(X )†y −θ = ϕ(X )†ϕ(X )θ −θ +ϕ(X )†z = ϕ(X )†z ,
S ∗ S S ∗ S S ∗ ∗ S S S S
whichleadsto
(cid:20) (cid:21)
1
E[ER(θ )] = E ∥ϕ(X)(θ −θ )∥2
S N S ∗ 2
(cid:18)(cid:18) (cid:19) (cid:19)
=tr
1
ϕ(X)⊤ϕ(X) ϕ(X )†E(cid:2) z
z⊤(cid:3)(cid:16)
ϕ(X
)†(cid:17)⊤
N S S S S
(cid:18)(cid:18) (cid:19) (cid:19)
1 (cid:16) (cid:17)−1
=σ2tr ϕ(X)⊤ϕ(X) ϕ(X )⊤ϕ(X )
S S
N
σ2 (cid:18) (cid:16) (cid:17)−1(cid:19)
= tr Σϕ Σϕ .
n S
Lowerboundofc . Nowweexplainthenecessityofassumingc ≥ n/N forΣϕ ≼ c Σϕ.
S S S S
Since ϕ(X)⊤ϕ(X) ≽ ϕ(X )⊤ϕ(X ), we observe that NΣϕ ≽ nΣϕ, which implies Σϕ ≽
S S S
nΣϕ. Therefore,Σϕ ≼ c Σϕ isonlypossiblewhenc ≥ n/N.
N S S S S
Low-dimensionallinearprobingwithmomentmatching. Recallfrom(1)thatE[ER(θ )] =
S
(cid:18) (cid:19)
(cid:16) (cid:17)−1
σ2 tr Σϕ Σϕ . FurtherassumingasuitableselectionofD withΣϕ ≼ c Σϕ,wehave
n S S S S
(cid:18) (cid:19)
(cid:16) (cid:17)−1
tr Σϕ Σϕ ≤ c tr(I ) = c r
S S r S
andtherefore,E[ER(θ )] ≤ c σ2r.
S S n
B.2 Proof of Proposition 2.1
ProofofProposition2.1. Let Σ(cid:98)ϕ := (cid:0) Σϕ(cid:1)−1/2 Σϕ (cid:0) Σϕ(cid:1)−1/2 . The goal of Σϕ ≼ c Σϕ can be
S S S S
(cid:13) (cid:13)
re-expressedasc Σ(cid:98)ϕ ≽ I ,or equivalentlywhenc > 1,(cid:13)Σ(cid:98)ϕ −I (cid:13) ≤ 1− 1 . Withuniform
S S r S (cid:13) S r(cid:13)
2
cS
20sampling,since
(cid:34) (cid:35)
(cid:104) (cid:105) 1 (cid:88) (cid:104) (cid:105)
E Σϕ = E ϕ(X)ϕ(X)⊤ = E ϕ(X)ϕ(X)⊤ = Σϕ,
S S S n x
x∈S
(cid:104) (cid:105)
we have E Σ(cid:98)ϕ = I . For any fixed unit vector z ∈ Sr−1, let Z := z⊤(cid:0) Σϕ(cid:1)−1/2 ϕ(x ) be
S S r i i
random variables with randomness on i ∈ [N]. Since ∥ϕ(x)∥ ≤ B ∀ x ∈ D and Σϕ ≽ γI ,
2 ϕ r
weobservethat
(cid:13) (cid:13) B
|Z | ≤ (cid:13)(cid:0) Σϕ(cid:1)−1/2 ϕ(x )(cid:13) ≤ √ϕ ∀i ∈ [N]
i (cid:13) i (cid:13) γ
2
is bounded. Therefore, Z is
(cid:16)
B
ϕ2(cid:17)
-subGaussian, and (Z2 −E[Z2]) =
z⊤(cid:16)
Σ(cid:98)ϕ −I
(cid:17)
z is
i γ i i S r
(cid:16) B2(cid:17)
16 ϕ -subexponential. Then, by Bernstein’s inequality [77, Theorem 2.8.2][80, Section
γ
2.1.3],forany0 < ϵ ≤ 16B2/γ,
1 ϕ
(cid:32) (cid:33)
(cid:104) (cid:16) (cid:17) (cid:105) n ϵ2γ2
P z⊤ Σ(cid:98)ϕ −I z ≥ ϵ ≤ exp − · 1 . (6)
S r 1 2 162B4
ϕ
(cid:13) (cid:13) (cid:16) (cid:17)
Byrecallingthat(cid:13) (cid:13)Σ(cid:98)ϕ
S
−I r(cid:13)
(cid:13)
= max u∈Sr−1u⊤ Σ(cid:98)ϕ
S
−I
r
u,Equation(6)forafixedz ∈ Sr−1
2
canbeextendedtotheentireunitsphereSr−1 throughanϵ-netargumentasfollows. Recallthat
(cid:16) (cid:17)r
foranyϵ > 0,thereexistsanϵ -netU ⊂ Sr−1 suchthat|U| ≤ 1+ 2 . Then,bytheunion
2 2 ϵ2
bound,
(cid:32) (cid:33)
(cid:20) (cid:16) (cid:17) (cid:21) (cid:18) 2(cid:19)r n ϵ2γ2
P maxu⊤ Σ(cid:98)ϕ −I u > ϵ ≤ 1+ exp − · 1
u∈U S r 1 ϵ 2 2 162B ϕ4
(cid:32) (cid:33)
(cid:18) 2(cid:19) n ϵ2γ2
=exp rlog 1+ − · 1 .
ϵ 2 162B4
2 ϕ
(cid:16) (cid:17)
Thatis,withprobabilityatleast1−δ,max u⊤ Σ(cid:98)ϕ −I u ≤ ϵ when
u∈U S r 1
512B4 (cid:18) (cid:18) 2(cid:19) (cid:18) 1(cid:19)(cid:19)
ϕ
n ≥ rlog 1+ +log .
γ2ϵ2 ϵ δ
1 2
Bytheconstructionoftheϵ -netU,forallv ∈ Sr−1,thereexistsu ∈ U suchthat∥u−v∥ ≤ ϵ .
2 2 2
Therefore,foranyv ∈ Sr−1,wehave
(cid:16) (cid:17)
v⊤ Σ(cid:98)ϕ −I v
S r
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
=u⊤ Σ(cid:98)ϕ −I u+(v−u)⊤ Σ(cid:98)ϕ −I (v−u)+2(v−u)⊤ Σ(cid:98)ϕ −I u
S r S r S r
(cid:13) (cid:13)
≤ϵ +(cid:13)Σ(cid:98)ϕ −I (cid:13) (cid:0) ϵ2 +2ϵ (cid:1) ,
1 (cid:13) S r(cid:13) 2 2
2
21(cid:13) (cid:13)
whichimplies(cid:13)Σ(cid:98)ϕ −I (cid:13) ≤ ϵ1 . Bytakingϵ asasmallconstant(e.g.,ϵ = (cid:112) 3/2−1),
(cid:13)
(cid:13) S
(cid:13)
r(cid:13)
2
2−(1+ϵ2)2 2 2
wehave(cid:13)Σ(cid:98)ϕ −I (cid:13) ≤ 1− 1 when
(cid:13) S r(cid:13)
2
cS
B4 r+log(1/δ)
n ≳ ϕ · .
γ2 (1−1/c )2
S
B.3 Proof of Theorem 2.2
ProofofTheorem2.2. WithΣϕ = 1G⊤G,wehave
N
(cid:20) (cid:21)
1 (cid:104) (cid:105)
E[ER(θ )] = E ∥G(θ −θ )∥2 = E ∥θ −θ ∥2 ,
S N S ∗ 2 S ∗ Σϕ
Observingthatbytheoptimalityofθ ,wehave
S
2
G⊤(G θ −y )+2αθ = 0 .
n S S S S S r
RecallingthatΣϕ := 1G⊤G ,thisimplies
S n S S
(cid:18)
1
(cid:19)−1
1
θ = G⊤G +αI G⊤y
S n S S r n S S
1 (cid:16) (cid:17)−1
= Σϕ +αI G⊤(G θ +z )
n S r S S ∗ S
(cid:16) (cid:17)−1 1 (cid:16) (cid:17)−1
= Σϕ +αI Σϕθ + Σϕ +αI G⊤z .
S r S ∗ n S r S S
Therefore,withE [z] = 0 ,E[ER(θ )]canbedecomposedthebiastermandvarianceterms
z N S
asfollows:
(cid:104) (cid:105)
E[ER(θ )] = E ∥θ −θ ∥2
S S ∗ Σϕ
(cid:34) (cid:35)
=E (cid:13) (cid:13) (cid:13)(cid:18) (cid:16) Σϕ +αI (cid:17)−1 Σϕ −I (cid:19) θ + 1 (cid:16) Σϕ +αI (cid:17)−1 G⊤z (cid:13) (cid:13) (cid:13)2
z (cid:13) S r S r ∗ n S r S S (cid:13)
Σϕ
(cid:34) (cid:35)
=(cid:13) (cid:13) (cid:13)(cid:18) (cid:16) Σϕ +αI (cid:17)−1 Σϕ −I (cid:19) θ (cid:13) (cid:13) (cid:13)2 +E (cid:13) (cid:13) (cid:13)1 (cid:16) Σϕ +αI (cid:17)−1 G⊤z (cid:13) (cid:13) (cid:13)2 .
(cid:13) S r S r ∗ (cid:13) z (cid:13)n S r S S (cid:13)
Σϕ Σϕ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Bias Variance
SinceE (cid:2) z z⊤(cid:3) ≼ σ2I ,thevariancetermcanbeboundedas
z S S n
σ2 (cid:18) (cid:16) (cid:17)−1 (cid:16) (cid:17)−1 (cid:19)
Variance ≤ tr Σϕ +αI Σϕ Σϕ +αI Σϕ
n S r S r S
σ2 (cid:18) (cid:16) (cid:17)−1(cid:19)
≤ tr Σϕ Σϕ +αI ,
n S r
(cid:13) (cid:13)
wherethesecondinequalityfollowsfromthefactthat(cid:13)
(cid:13)(cid:16)
Σϕ +αI
(cid:17)−1
Σϕ(cid:13) (cid:13) ≤ 1.
(cid:13) S r S(cid:13)
2
22(cid:16) (cid:17)
Recall that P ∈ Rr×r is an orthogonal projector onto any subspace S of Range Σϕ , and
S S
P⊥ = I −P istheorthogonalprojectorontoitsorthogonalcomplement. Byobservingthat
S r S
Σϕ +αI ≽ P ΣϕP +αP⊥,sinceRange(P ) ⊥ Range(cid:0) P⊥(cid:1) ,wehave
S r S S S S S S
(cid:16) (cid:17)−1 (cid:16) (cid:17)† 1
Σϕ +αI ≼ P ΣϕP + P⊥.
S r S S S α S
Therefore,
Variance ≤
σ2 (cid:18) tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+
1
tr(cid:0)
ΣϕP⊥(cid:1)(cid:19)
.
n S S S α S
Forthebiaspart,wefirstobservethat
(cid:16) (cid:17)−1 (cid:16) (cid:17)−1
I − Σϕ +αI Σϕ = α Σϕ +αI .
r S r S S r
Therefore,
Bias
=(cid:13)
(cid:13)
(cid:13)(cid:18) (cid:16)
Σϕ +αI
(cid:17)−1
Σϕ −I
(cid:19)
θ
(cid:13)
(cid:13)
(cid:13)2
=
(cid:13)
(cid:13)
(cid:13)α(cid:16)
Σϕ +αI
(cid:17)−1
θ
(cid:13)
(cid:13)
(cid:13)2
(cid:13) S r S r ∗ (cid:13) (cid:13) S r ∗ (cid:13)
Σϕ Σϕ
(cid:18) (cid:19)
(cid:16) (cid:17)−1 (cid:16) (cid:17)−1
=α2tr Σϕ +αI Σϕ Σϕ +αI θ θ⊤
S r S r ∗ ∗
(cid:18) (cid:19)
(cid:16) (cid:17)−2
≤α2tr Σϕ Σϕ +αI ∥θ ∥2.
S r ∗ 2
(cid:16) (cid:17)2 (cid:16) (cid:17)2
Since Σϕ +αI ≽ P ΣϕP +αI ≽ 2α·P ΣϕP +α2P⊥,wehave
S r S S S r S S S S
(cid:16) (cid:17)−2 1 (cid:16) (cid:17)† 1
Σϕ +αI ≼ P ΣϕP + P⊥,
S r 2α S S S α2 S
andthus
(cid:18) (cid:18) (cid:19) (cid:19)
Bias ≤
α
tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
+tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2.
2 S S S S ∗ 2
Combiningthebiasandvarianceterms,wehave
E[ER(θ )]
≤σ2 (cid:18) tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+
1
tr(cid:0)
ΣϕP⊥(cid:1)(cid:19)
S n S S S α S
(cid:18) (cid:18) (cid:19) (cid:19)
+
α
tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
+tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2
2 S S S S ∗ 2
≤σ2 tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2
n S S S S ∗ 2
+1
·
σ2
tr(cid:0) ΣϕP⊥(cid:1) +α·
∥θ ∗∥2
2
tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
.
α n S 2 S S S
23(cid:115)
(cid:18) (cid:18) (cid:19)(cid:19)
Bytakingα = σ2 tr(cid:0)
ΣϕP⊥(cid:1)(cid:46)
∥θ∗∥2 2 tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
,wehave
∗ n S 2 S S S
1
·
σ2
tr(cid:0) ΣϕP⊥(cid:1) +α ·
∥θ ∗∥2
2
tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
α n S ∗ 2 S S S
∗
(cid:115)
≤2
σ2
tr(cid:0) ΣϕP⊥(cid:1) ·
∥θ ∗∥2
2
tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
n S 2 S S S
≤√1 (cid:18) σ2 tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+tr(cid:0) ΣϕP⊥(cid:1) ∥θ
∥2(cid:19)
.
2 n S S S S ∗ 2
Thereforeoverall,wehave
E[ER(θ )]
≤2σ2 tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+2tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2.
S n S S S S ∗ 2
ProofofCorollary2.3. GivenP (c Σϕ −Σϕ)P ≽ 0andrank(P ) ≍ r,thevarianceterm
S S S S S
isasymptoticallyupperboundedby
2σ2 (cid:18) (cid:16) (cid:17)†(cid:19) σ2
variance = tr Σϕ P ΣϕP ≲ ·c r.
n S S S n S
(cid:0) (cid:1)
Meanwhile,giventr(ΣϕP⊥) ≤ N tr(Σϕ−⟨Σϕ⟩ )andtr(Σϕ−⟨Σϕ⟩ ) ≤ tr Σϕ /N,thebias
S n r r
termcanbeasymptoticallyupperboundedby
2
bias = 2tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2 ≤ tr(cid:0) Σϕ(cid:1) ∥θ ∥2.
S ∗ 2 n ∗ 2
TheresultfollowsfromTheorem2.2bycombiningthevarianceandbiasterms.
C Proofs for Section 3.1
C.1 Formal Statement and Proof of Theorem 3.1
Theorem C.1 (Formal version of Theorem 3.1). Under Assumption 2.2 and 2.3 with a small
intrinsic dimension r ≪ min{N,r}, for any δ ∈ (0,1), draw a Gaussian random matrix
Γ ∈ Rr×m with i.i.d. entries from N (0,1/m) where m ≍ k/δ for some k ≥ 1.1r. Let
Σ(cid:101)ϕ := Γ⊤ΣϕΓ and Σ(cid:101)ϕ := Γ⊤ΣϕΓ be the sketched gradient moments. For any S ⊆ [N] with
S S
n > msamplessuchthat(i)rank(Σϕ) = n,and(ii)thek-thlargesteigenvalues (Σ(cid:101)ϕ) ≥ γ
S k S S
forsomeγ > 0,withprobabilityatleast1−δ overΓ,thereexistsα > 0where(2)satisfies
S
σ2 (cid:16) (cid:17)
E[ER(θ )] ≲ tr Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩† (variance)
S n S k
σ2 1 (cid:13) (cid:13)
+ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1) (sketchingerror) (7)
n mγ (cid:13) S k(cid:13)
S 2
1 (cid:13) (cid:13)
+ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1) ∥θ ∥2 (bias).
n (cid:13) S k(cid:13) ∗ 2
2
24If S further satisfies Σ(cid:101)ϕ ≼ c Σ(cid:101)ϕ for some c ≥ n, taking m = max{(cid:112) tr(Σϕ)/γ ,1.1r/δ}
S S S N S
leadsto
c
E[ER(θ )] ≲ variance+sketchingerror+bias ≲ S (cid:0) σ2m+tr(cid:0) Σϕ(cid:1) ∥θ ∥2(cid:1) . (8)
S n ∗ 2
Westartbyintroducingsomehelpfulnotationsfortheproofs. LetG := ∇ fϕ(X;0 ) ∈ RN×r
θ r
and G = [G] ∈ Rn×r be the original gradients of D and D , respectively. Recall that
S S S
Σϕ = G⊤G/N andΣϕ = G⊤G /narethecorrespondingsecondmoments.
S S S
WeconsideraJohnson-Lindenstrausstransform(JLT)[37]Γ ∈ Rr×m asfollows:
DefinitionC.1(JLT[65](adapting[85,Definition3])). Foranyϵ > 0,δ ∈ (0,1),andn ∈ N,a
random matrix Γ ∈ Rr×m is a (ϵ,δ,k)-Johnson-Lindenstrauss transform ((ϵ,δ,k)-JLT) if for
anyU ∈ Rr×k consistingofk orthonormalcolumnsinRr,withprobabilityatleast1−δ,
(cid:13) (cid:13)
(cid:13)I −U⊤ΓΓ⊤U(cid:13) ≤ ϵ.
k 2
DefinitionC.2(JLsecondmomentproperty[39](adapting[85,Definition12])). Foranyϵ > 0,
δ ∈ (0,1),arandommatrixΓ ∈ Rr×m satisfiesthe(ϵ,δ)-JLsecondmomentpropertyif
(cid:20) (cid:21)
E
(cid:16)
(cid:13) (cid:13)Γ⊤u(cid:13) (cid:13)2
−1(cid:17)2
≤ ϵ2δ ∀u ∈ Sr−1.
2
Lemma C.2 (Approximated matrix-matrix multplication [39] (adapting [85, Theorem 13])).
Given ϵ > 0, δ ∈ (0,1/2), and a random matrix Γ ∈ Rr×m satisfying the (ϵ,δ)-JL second
momentproperty(DefinitionC.2),foranymatricesA,Beachwithr rows,
(cid:2)(cid:13) (cid:13) (cid:3)
Pr (cid:13)A⊤ΓΓ⊤B−A⊤B(cid:13) > 3ϵ∥A∥ ∥B∥ ≤ δ.
F F F
OneofthemostclassicalconstructionsofaJLTwithJLsecondmomentpropertyistheGaussian
embedding:
Lemma C.3 (Gaussian embedding [85, Theorem 6]). For any ϵ > 0, δ ∈ (0,1), a Gaus-
sian random matrix Γ ∈ Rr×m with i.i.d. entries Γ ∼ N(0,1/m) (i) is a (ϵ,δ,k)-JLT if
ij
m ≳ (k +log(1/δ))ϵ−2;and(ii)satisfiesthe(ϵ,δ)-JLsecondmomentpropertyifm ≳ ϵ−2δ−1.
ProofofLemmaC.3. The(ϵ,δ,k)-JLTconditionfollowsdirectlyfrom[85,Theorem6].
Toshowthe(ϵ,δ)-JLsecondmomentproperty,weobservethatforanyu ∈
Sr−1,(cid:13) (cid:13)Γ⊤u(cid:13) (cid:13)2
=
2
u⊤ΓΓ⊤uis an average of mindependentχ2 random variableswith mean 1and variance2, we
(cid:20) (cid:21)
haveE(cid:104)
(cid:13) (cid:13)Γ⊤u(cid:13)
(cid:13)2(cid:105)
= 1anditsvarianceisE
(cid:16)
(cid:13) (cid:13)Γ⊤u(cid:13) (cid:13)2
−1(cid:17)2
= 2/m. Therefore,m ≳ ϵ−2δ−1
2 2
leadstothe(ϵ,δ)-JLsecondmomentproperty.
RemarkC.1((Fast)Johnson-Lindenstrausstransforms). WhilewemainlyfocusontheGaussian
embedding in the analysis for simplicity, there is a rich spectrum of JLTs with the JL second
moment property [34, 57, 74, 86], some of which enjoy remarkably better efficiency than the
Gaussianembedding withoutcompromisingaccuracy empirically. We referinterested readers
to [31, 52, 85] for in-depth reviews on different JLTs and their applications, while briefly
synopsizingtwocommonchoicesandtheirefficiencyasfollows.
25(a) Subgaussian embedding [34] is a random matrix Γ ∈ Rr×m with i.i.d. entries from a
zero-mean subgaussian distribution with variance 1/m. Common choices include the
RademacherdistributionandGaussiandistribution(i.e.,Gaussianembedding).
Applying subgaussian embeddings to an N × r matrix takes O(Nrm) time, while the
involvedmatrix-matrix multiplicationcanbe computeddistributedly inparallelleveraging
the efficiency of Level 3 BLAS [28]. In practice, generating and applying Rademacher
random matrices tend to be slightly faster than Gaussian embeddings due to the simple
discretesupport.
(cid:113)
(b) Sparse sign matrix [53, 57] is a sparse random matrix Γ = r [γ ,··· ,γ ]⊤ ∈ Rr×m
ξ 1 r
(ξ ∈ N) with i.i.d. rows γ ∈ Rm each consisting of ξ non-zero entries at uniformly
j
random coordinates filled with Rademacher randomvariables. Whenξ = 1, Γis knownas
CountSketch[9]andrequiresasmanyasm = O(k2)columnstosatisfytheJLTproperty
withconstantdistortion. Increasingthe sparsityslightly,[14]showedthatm = O(klogk)
is sufficient for constant-distortion JLT when ξ = O(logk). In practice, [75] suggested
thatasmallconstantsparsityξ ≥ 8isusuallyenoughformanyapplicationslikelow-rank
approximations.
ThesparsesignmatrixcanbeappliedtoanN ×r matrixinO(Nrξ)time,independentof
thesketchingsizem. Withcarefulimplementation,sketchingviasparsesignmatricescan
besignificantlyfasterthanthesubgaussianembeddingsinpractice[22,52].
LetG(cid:101) := GΓ ∈ RN×m andG(cid:101) = G Γ ∈ Rn×m bethesketchedgradientssuchthat
S S
Σ(cid:101)ϕ := Γ⊤ΣϕΓ = G(cid:101)⊤G(cid:101)/N ∈ Rm×m, Σ(cid:101)ϕ := Γ⊤ΣϕΓ = G(cid:101)⊤G(cid:101) /n ∈ Rm×m.
S S S S
In particular, for a Gaussian embedding Γ, when rank(Σϕ) = rank(G ) = n, rank(Σ(cid:101)ϕ) =
S S S
rank(G(cid:101) ) = malmostsurely.
S
Recallthelowintrinsicdimensionr fromAssumption2.3. Foranyk ∈ Nwith1.1r ≤ k < m,
letP ∈ Rr×r beanorthogonalprojectorontoadimension-k subspaceS ⊆ Range(Σϕ):
S S
P := (⟨G(cid:101) ⟩†G )†(⟨G(cid:101) ⟩†G ) = G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G , (9)
S S k S S k S S S k S k S
andP⊥ = I −P beitsorthogonalcomplement. Throughout theproofofTheoremC.1,we
S r S
assumethefollowing:
AssumptionC.1. Letmin{N,r} ≫ n > m > k ≥ 1.1r suchthatrank(Σϕ) = n. Weconsider
S
a Gaussian embedding (Lemma C.3) Γ ∈ Rr×m with m ≍ k such that s (Σ(cid:101)ϕ) ≥ γ for some
k S S
γ > 0.
S
ProofofTheorems3.1andC.1. WefirstrecallfromTheorem2.2that
E[ER(θ )] ≤
2σ2 tr(cid:18) Σϕ(cid:16)
P ΣϕP
(cid:17)†(cid:19)
+2tr(cid:0) ΣϕP⊥(cid:1) ∥θ ∥2.
S n S S S S ∗ 2
LemmaC.4suggeststhatform ≍ k/δ,withprobabilityatleast1−δ/2,
(cid:18) (cid:19)
tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
≲
tr(cid:16) Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:17)
+
n
tr(cid:0) ΣϕP⊥(cid:1) .
S S S S k mγ S
S
26Therefore,
σ2 (cid:16) (cid:17) (cid:18) σ2 (cid:19)
E[ER(θ )] ≲ tr Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩† + +∥θ ∥2 tr(cid:0) ΣϕP⊥(cid:1) .
S n S k mγ ∗ 2 S
S
Then,applyingLemmaC.7withtheunionbound,wehave
1 (cid:13) (cid:13)
tr(cid:0) ΣϕP⊥(cid:1) ≲ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1)
S n (cid:13) S k(cid:13)
2
withprobabilityatleast1−δ. Thisimplies
σ2 (cid:18) (cid:16) (cid:17) 1 (cid:13) (cid:13) (cid:19)
E[ER(θ )] ≲ tr Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩† + (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1)
S n S k mγ (cid:13) S k(cid:13)
S 2
1 (cid:13) (cid:13)
+ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1) ∥θ ∥2.
n (cid:13) S k(cid:13) ∗ 2
2
IfS furthersatisfiesΣ(cid:101)ϕ ≼ c Σ(cid:101)ϕ forsomec ≥ n,thenwehave
S S S N
(cid:16) (cid:17) (cid:16) (cid:17) (cid:13) (cid:13) (cid:13) (cid:13)
tr Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩† ≤ tr Σ(cid:101)ϕ(Σ(cid:101)ϕ)† ≤ c m, (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) ≤ (cid:13)Σ(cid:101)ϕ(Σ(cid:101)ϕ)†(cid:13) ≤ c .
S k S S (cid:13) S k(cid:13) (cid:13) S (cid:13) S
2 2
Therefore,(7)canbefurthersimplifiedas
(cid:32) (cid:0) (cid:1)(cid:33)
c σ2 tr Σϕ c
E[ER(θ )] ≲ S m+ + S tr(cid:0) Σϕ(cid:1) ∥θ ∥2.
S n mγ n ∗ 2
S
(cid:112)
On the right-hand-side, the first (variance) term is minimized at m = tr(Σϕ)/γ where
S
(cid:0) (cid:1) (cid:112)
m+tr Σϕ /(mγ ) ≤ 2 tr(Σϕ)/γ = 2m. In addition, incorporting the assumption that
S S
(cid:112)
m ≍ k/δ forsomek ≥ 1.1r,wetakem = max{ tr(Σϕ)/γ ,1.1r/δ}andget
S
c σ2 c c
E[ER(θ )] ≲ S m+ S tr(cid:0) Σϕ(cid:1) ∥θ ∥2 = S (cid:0) σ2m+tr(cid:0) Σϕ(cid:1) ∥θ ∥2(cid:1) .
S n n ∗ 2 n ∗ 2
Theorem3.1issimplifiedfromTheoremC.1bytakingk = ⌈1.1r⌉andδ = 0.1.
C.2 Upper Bounding Variance
Lemma C.4. For any δ ∈ (0,1), let Γ ∈ Rr×m be a Gaussian embedding (Lemma C.3) with
m ≍ k/δ columns. Then,withprobabilityatleast1−δ overΓ,
(cid:18) (cid:19)
tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
≲
tr(cid:16) Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:17)
+
n
tr(cid:0) ΣϕP⊥(cid:1) .
S S S S k mγ S
S
ProofofLemmaC.4. Wefirstobservethatsincerank(Σϕ) = nimpliesG G† = I ,
S S S n
G P Γ = G G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G Γ = ⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G(cid:101) = ⟨G(cid:101) ⟩ ,
S S S S S k S k S S k S k S S k
andtherefore,G(G P )† = argmin ∥G−ZG P ∥2 +∥Z∥2 and
S S Z∈RN×n S S F F
G(cid:101)⟨G(cid:101) ⟩† = argmin = ∥(G−ZG P )Γ∥2 +∥Z∥2
S k S S F F
Z∈RN×n
isanapproximatedsolutionfromasketchedleastsquareproblem.
27Accuracy of sketched least square residual. For m ≍ k/(ϵ2δ), Lemma C.3 implies that
√
a Gaussian embedding Γ is a (1/2,δ/2,k)-JLT (Definition C.1) with (ϵ/ k,δ/2)-JL second
moment property (Definition C.2). Then, since rank(G P ) = k, by Lemma C.5, with
S S
probabilityatleast1−δ overΓ,
(cid:13) (cid:13)(cid:16) G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:17) G P (cid:13) (cid:13)2 ≤ ϵ2(cid:13) (cid:13)G−G(G P )†(G P )(cid:13) (cid:13)2 .
(cid:13) S S S k S S(cid:13) S S S S 2
F
SinceG P = G G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G = ⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G ,
S S S S S k S k S S k S k S
(G P )†(G P ) = G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G = P .
S S S S S S k S k S S
Therefore,
(cid:13) (cid:13)(cid:16) G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:17) G P (cid:13) (cid:13)2 ≤ ϵ2(cid:13) (cid:13)GP⊥(cid:13) (cid:13)2 . (10)
(cid:13) S S S k S S(cid:13) S F
F
(cid:13) (cid:13)
Accuracy of sketched least square solution. To upper bound (cid:13)G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:13) ,
(cid:13) S S S k(cid:13)
F
wefirstobservefrom(10)that
(cid:13) (cid:13)G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:13) (cid:13)2 ≤ ϵ2(cid:13) (cid:13)(G P )†(cid:13) (cid:13)2(cid:13) (cid:13)GP⊥(cid:13) (cid:13)2 .
(cid:13) S S S k(cid:13) S S 2 S F
F
G P G⊤ = ⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G G⊤. Since rank(G ) = n, Lemma C.6 implies that for a
S S S S k S k S S S
GaussianembeddingΓ,G G⊤ ≽ O(cid:0) m(cid:1) G ΓΓ⊤G⊤ withhighprobability. Therefore,
S S n S S
(cid:16)m(cid:17) (cid:16)m(cid:17)
G P G⊤ ≽ O ⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G(cid:101) G(cid:101)⊤ = O ⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩⊤.
S S S n S k S k S S n S k S k
Recallthat⟨Σ(cid:101)ϕ⟩ = 1⟨G(cid:101) ⟩⊤⟨G(cid:101) ⟩ ands (Σ(cid:101)ϕ) ≥ γ ,wehave
S k n S k S k k S S
(cid:13) (cid:13)
(cid:13) (cid:13)(G P )†(cid:13) (cid:13)2 =(cid:13) (cid:13)(G P G⊤)†(cid:13) (cid:13) ≤
O(cid:16)n(cid:17)
(cid:13)
(cid:13)(cid:16)
⟨G(cid:101) ⟩⊤⟨G(cid:101) ⟩
(cid:17)†(cid:13)
(cid:13)
S S 2 S S S 2 m (cid:13) S k S k (cid:13)
2
(cid:18) (cid:19)
(cid:16)n(cid:17) 1 1
≤O = O .
m nγ mγ
S S
Therefore,applyingaunionboundgivesthatwithprobabilityatleast1−δ overΓ,
(cid:13) (cid:13)G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:13) (cid:13)2 ≤ O(cid:18) ϵ2 (cid:19) (cid:13) (cid:13)GP⊥(cid:13) (cid:13)2 . (11)
(cid:13) S S S k(cid:13) mγ S F
F S
Toupperbound(cid:13)
(cid:13)G(G P
)†(cid:13) (cid:13)2
,weobservethatby(11),
S S F
(cid:13) (cid:13)G(G P )†(cid:13) (cid:13)2 ≤2(cid:13) (cid:13)G(cid:101)⟨G(cid:101) ⟩†(cid:13) (cid:13)2 +2(cid:13) (cid:13)G(G P )† −G(cid:101)⟨G(cid:101) ⟩†(cid:13) (cid:13)2
S S F (cid:13) S k(cid:13) (cid:13) S S S k(cid:13)
F F
≲(cid:13) (cid:13)G(cid:101)⟨G(cid:101) ⟩†(cid:13) (cid:13)2 + ϵ2 (cid:13) (cid:13)GP⊥(cid:13) (cid:13)2 .
(cid:13) S k(cid:13) mγ S F
F S
Finally,normalizingbymultiplyingn/N onbothsidesgives
(cid:18) (cid:19)
tr
Σϕ(cid:16)
P ΣϕP
(cid:17)†
≲
tr(cid:16) Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:17)
+ϵ2
n
tr(cid:0) ΣϕP⊥(cid:1) .
S S S S k mγ S
S
Takinganysmallconstantϵ > 0completestheproof.
28Lemma C.5 (Adapting [85, Theorem 23]). For any ϵ > 0 and δ ∈ (0,1), let Γ ∈ Rr×m b a
√
(1/2,δ/2,k)-JLT(DefinitionC.1)with(ϵ/ k,δ/2)-JLsecondmomentproperty(DefinitionC.2).
GivenA ∈ Rr×n withrank(A) = k andB ∈ Rr×N,let
W(cid:99) = argmin(cid:13) (cid:13)Γ⊤(AW−B)(cid:13) (cid:13)2 +∥W∥2 , W = argmin∥AW−b∥2 +∥W∥2 .
F F ∗ F F
W W
(cid:13) (cid:13)
Then,withprobabilityatleast1−δ overΓ,(cid:13)A(W(cid:99) −W )(cid:13) ≤ ϵ∥AW −B∥ .
(cid:13) ∗ (cid:13) ∗ F
F
ProofofLemmaC.5. Analogoustotheproofof[85,Theorem23],letA = QRbeareduced
QR decomposition of A such that Q ∈ Rr×k is an orthonormal basis for Range(A), and
R ∈ Rk×n. Reparametrizing Z(cid:98) = RW(cid:99) and Z = RW , up to constant scaling of ϵ, it is
∗ ∗
sufficienttoshow
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Q(Z(cid:98) −Z )(cid:13) = (cid:13)Z(cid:98) −Z (cid:13) ≤ O(ϵ)∥QZ −B∥ .
(cid:13) ∗ (cid:13) (cid:13) ∗(cid:13) ∗ F
F F
SinceΓ ∈ Rr×k isan(1/2,δ/2,k)-JLT,we have(cid:13) (cid:13)I −Q⊤ΓΓ⊤Q(cid:13) (cid:13) ≤ 1/2withprobability at
k 2
least1−δ/2and
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Z(cid:98) −Z (cid:13) ≤(cid:13)Q⊤ΓΓ⊤Q(Z(cid:98) −Z )(cid:13) +(cid:13)Q⊤ΓΓ⊤Q(Z(cid:98) −Z )−(Z(cid:98) −Z )(cid:13)
(cid:13) ∗(cid:13) (cid:13) ∗ (cid:13) (cid:13) ∗ ∗ (cid:13)
F F F
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
≤(cid:13)Q⊤ΓΓ⊤Q(Z(cid:98) −Z )(cid:13) +(cid:13)I −Q⊤ΓΓ⊤Q(cid:13) (cid:13)Z(cid:98) −Z (cid:13)
(cid:13) ∗ (cid:13) k 2(cid:13) ∗(cid:13)
F F
(cid:13) (cid:13) (cid:13) (cid:13)
≤(cid:13)Q⊤ΓΓ⊤Q(Z(cid:98) −Z )(cid:13) +1/2(cid:13)Z(cid:98) −Z (cid:13)
(cid:13) ∗ (cid:13) (cid:13) ∗(cid:13)
F F
(cid:13) (cid:13)2 (cid:13) (cid:13)2
whichimplies(cid:13)Z(cid:98) −Z (cid:13) ≤ 2(cid:13)Q⊤ΓΓ⊤Q(Z(cid:98) −Z )(cid:13) withprobabilityatleast1−δ/2.
(cid:13) ∗(cid:13) (cid:13) ∗ (cid:13)
F F
Bythenormalequationofthesketchedleastsquareproblem,Q⊤ΓΓ⊤QZ (cid:98) = Q⊤ΓΓ⊤B. Thus,
(cid:13) (cid:13)Z(cid:98) −Z (cid:13) (cid:13)2 ≤ 2(cid:13) (cid:13)Q⊤ΓΓ⊤(QZ −B)(cid:13) (cid:13)2 .
(cid:13) ∗(cid:13) ∗ F
F
√
(cid:0) (cid:1)
Since Q⊤(QZ − B) = −Q⊤ I −QQ⊤ B = 0 and Γ has (ϵ/ k,δ/2)-JL second
∗ r k×N
momentproperty,LemmaC.2impliesthatwithprobabilityatleast1−δ/2,
(cid:13) (cid:13)Q⊤ΓΓ⊤(QZ −B)(cid:13) (cid:13)2 ≤ O(cid:0) ϵ2/k(cid:1) ∥Q∥2 ∥QZ −B∥2 = O(ϵ2)∥QZ −B∥2 .
∗ F F ∗ F ∗ F
Then,bytheunionbound,withprobabilityatleast1−δ overΓ,wehave
(cid:13) (cid:13)Z(cid:98) −Z (cid:13) (cid:13)2 ≤ 2(cid:13) (cid:13)Q⊤ΓΓ⊤(QZ −B)(cid:13) (cid:13)2 ≤ O(ϵ2)∥QZ −B∥2 .
(cid:13) ∗(cid:13) ∗ F ∗ F
F
LemmaC.6([64]). ForarandommatrixΩ ∈ Rn×m (n > m)consistingofi.i.d.subgaussian
entrieswithmeanzeroandvarianceone,withhighprobability,
(cid:16) √ √ (cid:17) (cid:16) √ √ (cid:17)
O (cid:0) n− m(cid:1)2 I ≼ ΩΩ⊤ ≼ O (cid:0) n+ m(cid:1)2 I .
n n
29C.3 Upper Bounding Low-rank Approximation Error
Lemma C.7. Under Assumption 2.3, let Γ ∈ Rr×m be a Gaussian embedding (Lemma C.3)
such that there exists m > k ≥ 1.1r satisfying s (Σ(cid:101)ϕ) ≥ γ for some γ > 0. Then, with
k S S S
probabilityatleast1−exp(−Ω(r)),
1 (cid:13) (cid:13)
tr(cid:0) ΣϕP⊥(cid:1) ≲ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1) .
S n (cid:13) S k(cid:13)
2
ProofofLemmaC.7. Herewefollowasimilarproofstrategyas[22,Theorem1]. LetΠ :=
S
[I ] ∈ Rn×N be the selection matrix associated with S ⊆ [N]. We introduce the following
N S
N ×N obliqueprojectors:
M := GG†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†Π , M(cid:102) := G(cid:101)⟨G(cid:101) ⟩†Π .
S S S k S k S S S k S
Inparticular,M andM(cid:102) aretheobliqueprojectorssincewithΠ G = G andG G† = I ,
S S S S S S n
M2 =GG†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†Π
S S S k S k S S S k S k S
=GG†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†Π = M ,
S S k S k S S
andwithΠ G(cid:101) = G(cid:101) ,
S S
M(cid:102)2 =G(cid:101)⟨G(cid:101) ⟩†G(cid:101) ⟨G(cid:101) ⟩†Π = G(cid:101)⟨G(cid:101) ⟩†Π = M(cid:102) .
S S k S S k S S k S S
RecallingP from(9),weobservethefollowingidentities:
S
M G = GG†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†G = GP ; (12)
S S S k S k S S
sinceG G† = I ,
S S n
M(cid:102) M = G(cid:101)⟨G(cid:101) ⟩†G G†⟨G(cid:101) ⟩ ⟨G(cid:101) ⟩†Π = G(cid:101)⟨G(cid:101) ⟩†Π = M(cid:102) ; (13)
S S S k S S S k S k S S k S S
and
M(cid:102) G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ = G(cid:101)⟨G(cid:101) ⟩†G(cid:101) ⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ = G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ . (14)
S S k S k S k S S k S k S k S k
Combining(12),(13),and(14),wehave
GP =G−GP = (I −M )G (by(12))
S S N S
(cid:16) (cid:17)
= I −M(cid:102) (I −M )G (by(13))
N S N S
(cid:16) (cid:17)
= I −M(cid:102) GP⊥ (by(12))
N S S
(cid:16) (cid:17)(cid:16) (cid:17)
= I −M(cid:102) I −G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)† GP⊥ (by(14)).
N S N S k S k S
Since(cid:13) (cid:13)P⊥(cid:13) (cid:13)2
= 1,thisimplies
S 2
tr(cid:0) ΣϕP⊥(cid:1) = 1 ∥GP ∥2 = 1 (cid:13) (cid:13)(cid:16) I −M(cid:102) (cid:17)(cid:16) I −G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)†(cid:17) GP†(cid:13) (cid:13)2
S N S F N (cid:13) N S N S k S k S(cid:13)
F
1 (cid:13) (cid:13)2(cid:13)(cid:16) (cid:17) (cid:13)2
≤ (cid:13)I −M(cid:102) (cid:13) (cid:13) I −G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)† G(cid:13) .
N (cid:13) N S(cid:13) (cid:13) N S k S k (cid:13)
2 F
30Bytheoperatornormidentityforprojectors[71],wehave
(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2 N (cid:13) (cid:13)
(cid:13)I −M(cid:102) (cid:13) = (cid:13)M(cid:102) (cid:13) = (cid:13)G(cid:101)⟨G(cid:101) ⟩†Π (cid:13) = (cid:13)G(cid:101)⟨G(cid:101) ⟩†(cid:13) = (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) ,
(cid:13) N S(cid:13) (cid:13) S(cid:13) (cid:13) S k S(cid:13) (cid:13) S k(cid:13) n (cid:13) S k(cid:13)
2 2 2 2 2
andtherefore,
tr(cid:0) ΣϕP⊥(cid:1) ≤ 1 (cid:13) (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) I −G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)†(cid:17) G(cid:13) (cid:13)2 .
S n (cid:13) S k(cid:13) (cid:13) N S k S k (cid:13)
2 F
SinceG(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)† isarank-k orthogonalprojectoronto
S k S k
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
Range G(cid:101)⟨G(cid:101) ⟩† = Range G Γ⟨G(cid:101) ⟩†
S k S k
andGaussianembeddingsarerotationallyinvariant,G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)† sharesthesamedistribu-
S k S k
tion as (GΩ)(GΩ)† for a r ×k Gaussian embedding Ω with [Ω] ∼ N(0,1/k) i.i.d.. Then,
i,j
weobservethat∥(I −G(cid:101)⟨G(cid:101) ⟩†⟨G(cid:101) ⟩ G(cid:101)†)G∥2 istherank-k randomizedrange-findererrorof
N S k S k F
G,whichcanbecontrolledaccordingtoLemmaC.8: withprobabilityatleast1−exp(−Ω(r)),
1 (cid:13) (cid:13) N (cid:13) (cid:13)
tr(cid:0) ΣϕP⊥(cid:1) ≲ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) ∥G−⟨G⟩ ∥2 = (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ −⟨Σϕ⟩ (cid:1) .
S n (cid:13) S k(cid:13) r F n (cid:13) S k(cid:13) r
2 2
(cid:0) (cid:1) (cid:0) (cid:1)
Bythedefinitionofr inAssumption2.3,tr Σϕ −⟨Σϕ⟩ ≤ tr Σϕ /N andthus,
r
1 (cid:13) (cid:13)
tr(cid:0) ΣϕP⊥(cid:1) ≲ (cid:13)Σ(cid:101)ϕ⟨Σ(cid:101)ϕ⟩†(cid:13) tr(cid:0) Σϕ(cid:1) .
S n (cid:13) S k(cid:13)
2
LemmaC.8(Randomizedrange-findererror(simplifying[31,Theorem10.7])). LetΩ ∈ Rr×k
be a Gaussian embedding with [Ω] ∼ N(0,1/k) i.i.d.. For any G ∈ RN×r and r ∈ N such
i,j
that1.1r ≤ k ≪ min{N,r},withprobabilityatleast1−exp(−Ω(r)),
(cid:13) (cid:13)(cid:0) I −(GΩ)(GΩ)†(cid:1) G(cid:13) (cid:13) ≲ ∥G−⟨G⟩ ∥ .
N F r F
D Experiment Details for Section 4.1
D.1 Implementation Details
Syntheticdatageneration. WeconsiderasetofN = 2000sampleswithhigh-dimensional
pre-trained representations ϕ(X) ∈ RN×r where r = 2400, modeled by a Gaussian mixture
model(GMM)consistingofr = 8well-separatedclusters,eachwithrandomsizesandvariances.
Specifically,wegeneratetheGMMdatasetasfollows:
• RandomlypartitiontheN samplesintor = 8clusterswithsizes{N |j ∈ [r]}.
j
• Foreachj ∈ [r],generatetheclustermeanµ ∈ Rr withµ = (Z r)·e whereZ ∼ Unif([r])
j j j j j
andvarianceσ = Z′ ·σ whereZ′ ∼ Unif([0,1])andσ = 0.04.
j j max j max
(cid:8) (cid:12) (cid:9)
• Generaterepresentations ϕ(x ) ∼ N(µ ,σ2I )(cid:12)i ∈ [N ] i.i.d.foreachclusterj ∈ [r].
i j j r j
• Drawalatentlabelgeneratorθ ∼ N(0 ,I ). Foreachclusterj ∈ [r],assignthesamelabel
g r r
y = µ⊤θ forallsamplesi ∈ [N ]withinthecluster.
i j g j
31Ridgeregression. WesolvetheridgeregressionproblemovertheselectedcoresetD ofn
S
samplesandtunetheregularizationhyperparameterα viagridsearchover100linearlyspaced
valuesin[10−2,102]with2-foldcross-validation.
D.2 Baselines
WecompareSkMMtothefollowingunsuperviseddataselectionmethodsforregression:
(a) Uniformsampling(Uniform)selectsnsamplesuniformlyatrandomfromthefulldataset
D.
(b) Herding[12,84](Herding)selectsdatagreedilytominimizethedistancebetweenthe
centers of the coreset D and the original dataset D. Notice that although herding aims
S
to reduce the “bias” of the coreset center, it fails to control our notion of bias in the low-
rankapproximationsense. GiventheconstructionoftheGMM dataset, herdinghasmore
emphasisonvariancereduction,asillustratedinFigure2.
(c) K-centergreedy[67](K-center)providesagreedyheuristicfortheminimaxfacility
locationproblemthataimstominimizethemaximumdistancebetweenanynon-coreset
sampleandthenearestcoresetsample.
(d) Adaptive sampling [13, 20] (Adaptive) iteratively samples data based on their squared
normsandadaptivelyupdatesthedistributionbyeliminatingthespanningsubspaceofthe
selectedsamplesfromthedataset. Itisprovedintherecentwork[13]thatadaptivesampling
achievesnearlyoptimalsamplecomplexityforlow-rankapproximations,matchingthatof
volume sampling[17, 21](withthe bestknow theoreticalguarantee)upto alogarithmic
factor.
Inpractice,adaptivesamplinggenerallyachievescomparableaccuracytovolumesampling
for low-rank approximations, with considerably better efficiency [13, 23]. Due to the
prohibitive cost of volume sampling in high dimensions, we choose adaptive sampling in
thecomparison.
(e) Truncated[24,65]andridgeleveragescoresampling[3,15,48](T/R-leverage)are
theextensions ofclassicalleverage scoresampling[10]to highdimensions. In particular,
leverage score sampling is originally designed for low-dimensional linear regression, while
degenerating to uniform sampling in high dimensions. Consider the high-dimensional
representationsϕ(X) ∈ RN×r (r > N)inoursetting,foreachi ∈ [N],
• leveragescore: l := ϕ(x )⊤(ϕ(X)⊤ϕ(X))†ϕ(x ),
i i i
• truncatedleveragescore: l(m) := ϕ(x )⊤(⟨ϕ(X)⟩⊤⟨ϕ(X)⟩ )†ϕ(x )foragiventruncation
i i m m i
rankm,and
• ridgeleveragescore: l(ρ) := ϕ(x )⊤(ϕ(X)⊤ϕ(X)+ρI )†ϕ(x )foragivenregularization
i i r i
parameter ρ > 0. Larger ρ brings ridge leverage score sampling closer to uniform
sampling.
Therefore, both truncated and ridge leverage score sampling balance the variance-bias
tradeoffbyadjustingthetruncationrankmandregularizationparameterρ,respectively.
Baseline details. For both Herding and K-center, we adopt the DeepCore implemen-
tation [30]. Notice that Herding is a deterministic algorithm. For Adaptive, we use the
32implementationfrom[23]. ForT-leverage,weuse arank-mtruncatedSVDto compute the
leverage scores, with m = 4r = 32 as in SkMM (i.e., providing both methods approximately
thesameamountofinformationandcompute). ForR-leverage,wechooseρ = 103.
33