Vision-Language Models under Cultural and Inclusive Considerations
AntoniaKaramolegkou,PhillipRust,YongCao,
RuixiangCui,AndersSøgaard,DanielHershcovich
DepartmentofComputerScience,UniversityofCopenhagen
Correspondence:antka@di.ku.dk
Abstract
Rating
Largevision-languagemodels(VLMs)canas-
1 2 3 4 5
sistvisuallyimpairedpeoplebydescribingim-
agesfromtheirdailylives. Currentevaluation
helpful
datasets may not reflect diverse cultural user
backgroundsorthesituationalcontextofthis
usecase. Toaddressthisproblem, wecreate
asurveytodeterminecaptionpreferencesand important
proposeaculture-centricevaluationbenchmark
byfilteringVizWiz,anexistingdatasetwithim-
0 10 20 30 40 50 60
agestakenbypeoplewhoareblind. Wethen Number of Participants
evaluateseveralVLMs,investigatingtheirrelia-
bilityasvisualassistantsinaculturallydiverse Figure1: Surveyresultsfrompeoplewithvisualimpair-
setting. While our results for state-of-the-art mentsratingimportanceandhelpfulnessofculturalin-
modelsarepromising,weidentifychallenges formationinimagecaptions. WeuseaLikertscalefrom
suchashallucinationandmisalignmentofauto- 1(notimportant/helpful)to5(veryimportant/helpful).
maticevaluationmetricswithhumanjudgment.
We make our survey, data, code, and model
outputspubliclyavailable.
the cultural knowledge of VLMs, despite being
coastalcph/vizwiz-culture usefulforassessingtheirmultilingualcapabilities.
Additionally,evaluatingthesesystemsasvisualas-
1 Introduction
sistantspresentsfurtherchallengesduetovarying
photoquality,usergoals,andphotocontent(Chiu
WiththeincreasingintegrationofAIapplications
etal.,2020;Jungetal.,2022). Recently,Gonzalez
into our lives, it is important to consider human-
etal.(2024)conductedadiarystudywithblindand
centeredusecaseswhenevaluatingsuchsystems.
low-visionindividualsusinganAI-poweredscene
Largemultimodallanguagemodelsarenowused
description application, revealing that significant
asvisualassistantsforblindandvisuallyimpaired
improvements are still needed for satisfying and
individuals. Giventhatpeopleacrossdifferentcul-
trustworthyuserexperiences.
turesusesuchapplications,itisessentialtoensure
not only their accuracy and faithfulness (Brady Toaddressbothculturalandvisualchallenges,
et al., 2013; Gonzalez et al., 2024) but also their wefirstsurveyedvisuallyimpairedindividualsto
culturalrepresentationandinclusion(Hershcovich gather their caption preferences and determine if
etal.,2022;Shietal.,2024). cultural details are necessary. Then, we filtered
ExistingevaluationbenchmarksforVLMsfocus anexistingdatasetwithimagestakenfrompeople
primarily on English with few, implicit mutlicul- who are blind, identifying implicit cultural con-
tural references. Although multicultural evalua- cepts. Thisisusedasachallengingbenchmarkto
tion datasets like MaRVL (Liu et al., 2021) and evaluateimagecaptioningperformanceoncultural
XM3600 (Thapliyal et al., 2022) include culture- imagesofstate-of-the-artmodelsacrossdifferent
specific images (e.g., traditional wedding cos- promptsettings. Withtheseexperiments,weinves-
tumes), they also contain images with minimal tigatehowAIapplications,suchasimagecaption-
culturalsignificance(e.g.,abagofcarrots). Conse- ing,canfosteramoreinclusiveandculture-aware
quently,thesedatasetsmaynotaccuratelymeasure experienceforall.
4202
luJ
8
]VC.sc[
1v77160.7042:viXraBackground Currentmodelsaretrainedwithout ofimagesandquestionssubmittedbypeoplewho
consideration for the subjective perspectives and areblind,togetherwithcrowdsourcedanswersand
culturalinfluencesofthosewhoprovidedtheimage imagecaptions(Gurarietal.,2020). Theselection
descriptions(Yeetal.,2023). Thisraisestheneed ofthisdatasetservestwomainpurposes. Firstly,it
forcarefullycuratedsourcesofdataandannotation isachallengingdatasetspecificallytailoredtoreal-
paradigmsthataremoreculturallyawareandinclu- worldchallengesfacedbypeopleseekingtoaccess
sive(Aroraetal.,2023;Caoetal.,2023). Lately, visualinformation. Secondly,VizWizmightcon-
therehas beenagrowingbody ofwork releasing tainimplicitculturalreferencesthatarecurrently
multiculturalmultimodaldatasetsforvisiolinguis- notcapturedduetothelackofculture-specificcap-
ticreasoning(Liuetal.,2021),texttoimagegen- tions. (2)Weevaluatedtheimagecaptioningperfor-
eration (Liu et al., 2023b; Ventura et al., 2023), manceofstate-of-the-artclose-sourcedandopen-
andimagecaptioning(Thapliyaletal.,2022). Be- sourcedmodelsinaculturallydiversesettingusing
yondthefocusonthemultilingualismofthecap- our filtered VizWiz dataset. We performed both
tions,concurrentworkalsoaddressesthecultural anautomaticscoringofmodel-generatedcaptions
conceptsdepictedintheimages(Caoetal.,2024; against two sets of annotations using the COCO
Burda-Lassen et al., 2024; Romero et al., 2024; evaluationpackage1 andahumanevaluation.
Mukherjeeetal.,2024;Bhatiaetal.,2024). How-
2.1 DataFiltering
ever, they still do not take into account specific
use cases, such as visual assistance. Gurari et al. To filter the data we hired a total of 165 annota-
(2020)releasedthefirstimage-captioningdataset torsthroughtheProlificplatform.2 Wefirstasked
withphotosfrompeoplewhoareblind,andaseries participants to specify their country of origin, lo-
ofchallengesformultimodalsystemsacrossdiffer- cation, and their cultural background. Then, we
enttasks(Gurarietal.,2018). Afterthisinitiative, asked them to retrieve images from the VizWiz
therehavebeenmanyworkstryingtoimprovecur- dataset visualizer3 related to their cultural back-
rentmodelsforaspecificuse-case,toassistpeople ground, provide the image name, the reason they
withvisualdisabilities(Dogninetal.,2022;Ahsan think the image is culture-related, and their pre-
etal.,2021;DelloulandLarabi,2023). Therehas ferredcaptionfromthedataset(VizWizprovides
alsobeenresearchinhuman-computerinteraction fivedifferentimagecaptionsperimage). Wealso
(HCI)andaccessibilityondesigningimagedescrip- gavethemtheoptiontosuggestabettercaptionthat
tions for visually impaired individuals, primarily includes cultural aspects. After collecting all the
focusingonscreenreadersandfunctionaldescrip- culture-specific candidate images, we proceeded
tionsofonline,publiclyavailableimages(Morris to a second step of verification. In this step, we
etal.,2018;Bennettetal.,2021;Schaadhardtetal., retainedonlythoseimagesthathadreceivedcon-
2021). Despitetheseefforts,therestillseemstobe sensusagreementfromatleasttwoindividuals. We
a lack of focus on image captioning for the visu- collected a total of 324 images and 648 captions
ally impaired (Ghandi et al., 2023), especially in spanning60differentidentifiedcultures. Itshould
multi-culturalsettings. also be noted that more than 96% of the annota-
tors suggested a cultural revision of the original
2 Methodology captions. We refer to Appendix B for further in-
formationabouttheannotationguidelinesanddata
We first created a survey seeking to understand filteringapproachandresults.
the preferences of visually impaired individuals
for image captions, focusing on the inclusion of 2.2 Modelsandevaluation
culturalinformationandthedesiredlevelofdetail
Weconductedexperimentsontheimagecaptioning
(seeAppendixA).Weaggregatetheparticipants’
taskinthezero-shotsetting,inwhichapretrained
assessmentsofthehelpfulnessandimportanceof
modelisqueriedtoproduceatextualdescription
culturalinformationinFigure1.
foranimagewithoutfinetuningonthesamedataset.
Wethenfocusedontwolinesofcontribution: (1) We relied on four commonly used open-access
WefilteredtheVizWizdatasetforimplicitcultural
1https://github.com/tylin/coco-caption
concepts. VizWizisawidelyusedvisualquestion
2https://www.prolific.com/
answeringandimagecaptioningdatasetrepresent-
3https://vizwiz.cs.colorado.edu/VizWiz_
ingareal-worldusecase,whereexamplesconsist visualization/view_dataset.phpModel BLEU-4 METEOR CIDEr SPICE
Prompt Default Cultural Default Cultural Default Cultural Default Cultural
Annotation Original Cultural Original Cultural Original Cultural Original Cultural Original Cultural Original Cultural Original Cultural Original Cultural
BLIP-2 8.0±0.4 4.8 7.0±0.4 4.6 12.6±0.2 10.2 12.3±0.3 10.3 51.3±3.2 39.9 44.0±3.0 36.7 13.8±0.4 12.5 12.8±0.5 11.5
InstructBLIP 14.0±0.5 8.7 14.1±0.4 9.0 17.3±0.3 13.2 17.7±0.3 13.3 77.1±3.4 60.0 78.8±3.2 60.2 18.5±0.4 15.6 18.2±0.5 14.9
Idefics2 12.0±0.5 10.1 9.8±0.5 10.7 18.1±0.3 15.1 18.9±0.3 17.1 80.2±1.9 78.4 74.1±2.2 78.2 18.0±0.5 16.7 18.8±0.2 17.8
LLaVA-1.6 10.0±0.5 11.4 6.7±0.3 7.7 18.9±0.4 17.3 18.4±0.3 17.0 60.2±2.3 75.2 40.3±1.7 56.3 16.3±0.6 16.5 15.8±0.5 15.4
Gemini-1.5-Pro 10.8±0.3 14.1 5.8±0.1 8.7 20.8±0.4 21.3 18.2±0.1 21.0 71.5±2.1 88.8 14.8±0.5 34.1 19.6±0.4 21.6 14.9±0.3 17.7
GPT-4o 11.9±0.6 16.4 8.1±0.3 12.2 22.4±0.4 23.4 19.9±0.3 22.6 66.8±2.8 99.8 40.4±1.0 72.8 19.1±0.4 21.8 16.6±0.3 20.1
Table 1: Performance of various VLMs on our filtered VizWiz dataset across captioning prompts (default &
culture-specific)andannotations(original&culture-specific). Weuse2referenceannotationsperimage. Sincethe
originalVizWizhas5annotationsperimage,wereportthemeanandstandarddeviationoverall10combinations
withtworeferences. Weunderlinethebestresultforeachmodelanddisplaythetopresultforeachmetricinbold.
models:4 BLIP-26.7B(Lietal.,2023a)withOPT 3 Results
as LLM backbone (Zhang et al., 2022), Instruct-
BLIP7B(Daietal.,2023)withVicunabackbone Automaticevaluation Wepresenttheresultsof
(Chiangetal.,2023),Idefics28B(Laurençonetal., ourautomaticevaluationofmodel-generatedcap-
2024),andLLaVa-1.67B(Liuetal.,2023a)with tions in Table 1. Note that due to using two ref-
Mistralbackbone(Jiangetal.,2023). Wealsoused erencecaptionsperimage,resultsfortheoriginal
two state-of-the-art closed-access models: GPT- annotationsareslightlydifferentthanwhenusing
4o (OpenAI, 2024) and Gemini Pro 1.5 (Gemini allfiveatonce;wereportthelatterinAppendixE
Team, 2024). For all of these models, we experi- forcompleteness.
mentedwithtwodifferentprompttypesincludinga Asexpected,theclosed-accessmodels(Gemini
culture-specificpromptfollowingShietal.(2024) andGPT-4o)scorebestoverall. Slightlylowerper-
and a default captioning prompt taken from Dai formanceisachievedbytheinstruction-tunedopen-
et al. (2023). The exact prompts can be found in accessVLMs(LLaVa,Idefics2,andInstructBLIP).
AppD.Weevaluatedthemodel-generatedcaptions BLIP-2, which has not been instruction-tuned, is
intwoways: (1)viatheCOCOevaluationsuiteand laggingbehindacrossallmetrics. SinceVizWizis
(2)throughhumanevaluation. TheCOCOevalu- naturallynoisyduetothehighratiooflow-quality,
ation suite was first introduced by (Chen et al., blurryimages,theincreasedscaleandoverallmul-
2015)asaframeworktoassessimagecaptionsus- timodalreasoningcapabilitiesoftheclosed-source
ingnumerousautomaticmetrics,includingBLEU modelsappeartogiveasignificantadvantage.
(Papineni et al., 2002), CIDEr (Vedantam et al.,
Strikingly, Gemini and GPT-4o achieve much
2015), METEOR (Denkowski and Lavie, 2014),
better performance on our newly annotated cap-
and SPICE (Anderson et al., 2016). For consis-
tionsthatincludeculturalinformationthanonthe
tencywithourculture-specificre-annotations(two
originalcaptions(e.g.,11.9vs. 16.4BLEU-4and
captions per image), we also used two reference
66.8vs. 99.8CIDErforGPT-4owiththedefault
captionsperimagetoscoremodelsontheoriginal
prompt), while we observe the opposite for the
annotations. Since each image has five original
open-accessmodels(e.g,14.0vs. 8.7BLEU-4and
captions, we report aggregate results over all ten
77.1vs. 60.0CIDErforInstructBLIPwiththede-
two-captioncombinations. Ourhumanevaluation
faultprompt). Onepossibleexplanationisthatthe
hadtwostages. Inthefirststage,weasked60par-
closed-sourcemodelshavebeentunedtogenerate
ticipantstodetermineifacaptionisaccurate(on
more descriptive captions that are aligned better
abinaryscale)giventhecorrespondingimage. In
with human preferences and our cultural caption
the second stage, we asked the same participants
annotations,whereastheopen-accessmodelshave
torankallcaptions(human-generated,andmodel-
beentunedtogenerateslightlymoreconcisecap-
generated) according to their preference. We did
tionsthatalignwellwithbenchmarkdatasetslike
notmaketheannotatorsawarethatonecaptionwas
COCOCaptions. Ournewculturalannotationsare
model-generatedtominimizebias. Weprovidefur-
alsoguaranteedtonothaveleakedintotheVLMs’
therdetailsonthehumanevaluationinAppendixF.
trainingdata,thusfavoringmoreobjectivelycapa-
blemodelssuchasGPT-4o.
Next,whileindividualmodels(Idefics2andIn-
4WeusedimplementationsandmodelweightsfromHug-
gingFace(Wolfetal.,2020). structBLIP in particular) seem amenable to cul-Figure2: ExamplesofvariousimagesfromthefilteredVizWizdatasetwiththeoriginal( )andculture-specific(
)annotations,andgeneratedcaptionsfromGemini-1.5-Pro,GPT-4o,InstructBLIP,andLLaVA-1.6withdefault(
)andculture-specific( )prompting.
100 Human evaluation The results of the human
10
80 evaluationareshowninFigure3. Inlinewiththe
8
6 60 automatic metrics, our human annotators tend to
4 40 preferthecaptionsproducedbyclosed-accessmod-
2 20 accurate
not accurate els,GPT-4oandGemini-Pro,withtheBLIP-family
capti0 o cn a_ ptC ion G_ PTD G- e4 Go miP_ T nC G- i- e4 1o . m_ i5- nD iP -r 1o ._ 5-D P Ir do e_ f LiC Lcs a2 V_ A-C 1 I. d6 e_ fi LD c Ls a I2 V n_ A s-D tr1. u I6 c n_ t stC B rLI uP ct_ BC LIP B_ LID P-2 B_ LIC P-2_D GPT-0 4 Go eG_ P mC iT- ni4 - Go 1 e_ .5 mD i-P nir -o 1_ .5D -Pr Io d_ Ie nfC i sc trs u2_ ctC BLI IP d_ efD i Lc Ls a2_ VAD - L1 L. a6_ V IAC n- s1 t. r6 u_ ctD BLIP_ BC LIP-2_ BLC IP-2_D m rao ted del as sh aa cv cin ug rat th ee inlow me os rt era thn ak nin 9g 0. %Th oe ffo thr em ce ar sa er se
,
Figure3: Resultsofthehumanevaluationfor100im- whilethelatteraredeemedinaccurateinmorethan
ages and their captions selected at random from the halfofthecases. Despitethestrongperformance
filteredVizWizdataset. Theleftplotshowstheprefer- oftheclosed-accessmodels,ourpreferencecom-
encescore(participantswereaskedtorankthecaptions;
parisonalsoshowsthattheculture-specifichuman-
lowerisbetter). Therightplotshowstheaccuracyeval-
annotated captions are still preferred over all of
uation(participantswereaskedtoassesswhetheracap-
the models, suggesting there is ample room for
tionisaccurate;higherisbetter). ‘_D’and‘_C’denote
improvement.
defaultandculture-specificprompting,respectively.
In spite of the often stark differences in auto-
turalprompting,leadingtoimprovedperformance matic evaluation scores between cultural and de-
even on the original image captions, the cultural fault prompting (with a preference for the latter),
promptingstrategyisoveralllargelyineffectiveat human participants prefer the model generations
improving performance on the cultural captions. obtained via cultural prompting in 4/6 cases (for
This result may be due to the models’ tendency boththerankingandtheaccuracyassessment),sup-
forsycophanticbehaviorandthembeingprimedto portingourhypothesisthatculturalpromptingsim-
pointoutculturalinformationoverotherrelevant ply elicits an answer format that is disfavored by
contentintheimage(Sharmaetal.,2023). Alterna- automaticmetrics.
tively,culturalpromptingmightelicitmoreverbose
captionsthataredisfavoredbytheautomaticeval- Overall, our results are promising in regard to
uation metrics, in which case the automatic eval- thereliabilityofVLMsatzero-shotgeneratingcap-
uationresultspaintanincompleteandpotentially tionsthatareaccurateandusefultouserswhoare
misleadingpicture. blindinculturallydiversescenarios.
gniknaR
ecnereferP
egarevA
egatnecreP
ycaruccA4 FurtherAnalysis 5 Discussion
GiventhecurrentintegrationofVLMsasvirtualas-
Tofurtheranalyzeourresultsandassessthemodel- sistantsforpeoplewhoseeksightedsupport,their
generatedcaptionsinamorefine-grainedmanner, performanceonculture-specificimagecaptioning
wemanually inspectedall generatedcaptions for seemspromising. Examplesfromourerroranaly-
our 324 images filtered VizWiz dataset and pro- sisandcasestudieshighlightsomeremainingchal-
videdsomeexamplesinFigure2. lenges. Measuredbyautomaticevaluationmetrics,
theperformanceofthemodelsisoverallrelatively
WefindthatInstructBLIPandBLIP-2captions
lowcomparedtoresultsinexistingstudiesevalu-
tendtobeveryshort,lackalotofinformation,and
ating (finetuned) VLMs for image captioning on
areoftenirrelevanthallucinations. Thisis,toanex-
the full VizWiz and other datasets (Gurari et al.,
tent,expectedasweperformzero-shotcaptioning,
2020;Chenetal.,2023;Wangetal.,2022). Onthe
so the models are not necessarily accustomed to
otherhand,ourhumanevaluationanderroranalysis
thedesiredcaptioningstyle. Inthiscase,few-shot
show that the generated captions by Gemini-1.5-
prompting or finetuning the models would likely
ProandGPT-4oareaccurateandpreferredinmany
improvemodelperformance(Brownetal.,2020;
cases. Therealsoseemstobeanextendedhalluci-
Mañasetal.,2023;Ramosetal.,2023). Theclosed-
nationproblem,whichremainsanexistingmajor
accessmodels,incontrast,largelyprovidefurther
challengenotonlyforVLMs(Lietal.,2023b)but
ormoreusefulandculture-specificdetailsaboutthe
acrossvariouslanguagemodelapplications(Bang
imagethangivenbythehumancaptioners. They
etal.,2023;Jietal.,2023).
alsoseemtoprovidemoreaccuratecaptionscom-
pared to the open-sourced models. These points
6 Conclusion
mayexplainwhyGPT-4oandGemini-1.5-Proand
wereoverallpreferredinourhumanevaluation.
Weevaluatedtheculturalperformanceofvarious
Overall,weobservedthattheclosed-accessmod- modelsonimagecaptioningusingamulticultural
els can transcribe various language scripts from datasettailoredtoareal-worldusecase. Although
books, food or beverage packages, giving them theperformanceofstate-of-the-artclosed-source
an advantage over the smaller models. In most modelsispromising,thereisplentyroomforim-
cases,inbothculture-specificanddefaultprompts, provement. Examplesfromourerroranalysispro-
themodelscanidentifyculture-specificbeverages videinsightsintothemodels’performance, help-
likeJapanesematchatea,Chinesejellygrassorly- ing us identify some of their weak spots. In our
cheejuice,andfoodsuchastheIndianlijjatpapad, use case, we find that automatic evaluation met-
Japanesemochi,TomKhaGaiThaisoup,Korean rics might not be fully representative of model
kimchi,etc. Therearealsocaseswheretheyiden- performance,andthereforeencourageresearchers
tifyreligiousorfolkitemsliketheWayangGolek to reconsider a more comprehensive assessment
puppets,ajarwithtraditionalNorwegiancostumes, framework. Forfuturework,weaimtoextendour
oradelftplaquewithtraditionalDutchcostumes. smallfilteredculturaldatasetbyincludingquestion-
answeringtaskswithPOVculturalquestions.
Thereis,however,atendencytogeneratelonger
text in the culture-specific prompts by adding
Limitations
genericphrasessuchas‘hintingatthedrink’scul-
tural origin’, ‘suggesting a celebration of their Our work focuses primarily on data curation and
shared heritage’, ‘highlighting its appeal across empirical analysis of large multimodal language
agegroupsinIndianculture’,etc. Themostchal- models. Our survey, while aimed at determining
lengingcasesfortheclosed-sourcemodelsseemto captionpreferences,maynotcapturethefullrange
beforeigncurrencies(especiallytheArabicones), of needs and preferences of all people with vi-
historicfigures,andpaintings. Forexample,mod- sual impairment. Further, through our analysis,
elsseemtoconfuseBahraini,Jordan,andEgyptian wegained insights intosomeweakspotswith re-
banknotes,andtheydonotrecognizetheChinese spect to what cultures and cultural concepts are
historical figure of Sun Yat-sen, or paintings of wellrecognizedbythemodels. However,sincewe
Joan Miró or Frederick Morgan. We provide fur- useafiniteamountofdata,theremightbeadata
therexamplesinAppendixG. bias in identifying particular cultures or culturalconceptsasproblematic. Lastly,culturalcomplex- oftheFirstWorkshoponCross-CulturalConsidera-
ities and variations make it difficult to develop a tionsinNLP(C3NLP),pages114–130,Dubrovnik,
Croatia.AssociationforComputationalLinguistics.
standardizedapproachtoculturalinclusioninAI.
Wedo,however,hopethatourculture-centricap- YejinBang,SamuelCahyawijaya,NayeonLee,Wen-
proachinthedatafilteringandannotationprocess liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei
Ji,TiezhengYu,WillyChung,QuyetV.Do,YanXu,
canserveasaninitialsteptowardsevaluatingand
andPascaleFung.2023. Amultitask,multilingual,
understandingtheculturalawarenessandabilities
multimodalevaluationofChatGPTonreasoning,hal-
ofvision-languagemodelsforreal-worlduses. lucination,andinteractivity. InProceedingsofthe
13thInternationalJointConferenceonNaturalLan-
EthicsStatement guageProcessingandthe3rdConferenceoftheAsia-
PacificChapteroftheAssociationforComputational
The motivation behind this study is that large Linguistics(Volume1:LongPapers),pages675–718,
NusaDua,Bali.AssociationforComputationalLin-
vision-languagemodelshaverapidlybecomemain-
guistics.
streamandareusedevenbythosewhoseeksighted
supportandcannoteasilyassessmodelhallucina- Cynthia L. Bennett, Cole Gleason, Morgan Klaus
tionsorinaccuracies. Theprimarypurposeofour Scheuerman,JeffreyP.Bigham,AnhongGuo,and
Alexandra To. 2021. “it’s complicated”: Negotiat-
experimentsistoassesstheperformanceofvision-
ing accessibility and (mis)representation in image
language models in the task of image captioning
descriptionsofrace,gender,anddisability. InPro-
usingamulticulturaldatasetofimagestakenfrom ceedings of the 2021 CHI Conference on Human
peoplewhoareblind. However,itiscrucialtorec- FactorsinComputingSystems,CHI’21,NewYork,
NY,USA.AssociationforComputingMachinery.
ognizethatresultsfromourcurrentfiltereddataset
may not be representative of model performance Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, Eu-
across cultures. Furthermore, our refined dataset njeong Hwang, and Vered Shwartz. 2024. From
local concepts to universals: Evaluating the multi-
might retain biases present in the original source
cultural understanding of vision-language models.
dataset.
arXivpreprint.
Wefinditimprobablethatourexperimentsand
ErinBrady,MeredithRingelMorris,YuZhong,Samuel
thefiltereddatasetwillmeaningfullybenefitthose
White, and Jeffrey P. Bigham. 2013. Visual chal-
intendingtocreatedeceptivemodelsformalicious
lenges in the everyday lives of blind people. In
purposes. Additionally, the VizWiz dataset may Proceedings of the SIGCHI Conference on Hu-
lackcoverageofhighlyspecificsubjects,offering manFactorsinComputingSystems,CHI’13,page
2117–2126, New York, NY, USA. Association for
onlyageneraloverviewoffactualtopics. People
ComputingMachinery.
whointendtouseourresources,however,should
statetheirpurposeofusageandbeaccountablefor Tom Brown, Benjamin Mann, Nick Ryder, Melanie
theirownwork. Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Acknowledgments
Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
References
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Hiba Ahsan, Daivat Bhatt, Kaivan Shah, and Nikita
Clark, ChristopherBerner, SamMcCandlish, Alec
Bhalla.2021. Multi-modalimagecaptioningforthe
Radford, Ilya Sutskever, and Dario Amodei. 2020.
visuallyimpaired. InProceedingsofthe2021Con-
Language models are few-shot learners. In Ad-
ferenceoftheNorthAmericanChapteroftheAsso-
vances in Neural Information Processing Systems,
ciationforComputationalLinguistics: StudentRe-
volume 33, pages 1877–1901. Curran Associates,
searchWorkshop,pages53–60,Online.Association
Inc.
forComputationalLinguistics.
Olena Burda-Lassen, Aman Chadha, Shashank
PeterAnderson,BasuraFernando,MarkJohnson,and
Goswami, and Vinija Jain. 2024. How culturally
Stephen Gould. 2016. SPICE: semantic proposi-
awarearevision-languagemodels? arXivpreprint.
tionalimagecaptionevaluation. InComputerVision-
ECCV2016-14thEuropeanConference,Amsterdam, Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan, Anto-
TheNetherlands,October11-14,2016,Proceedings, nia Karamolegkou, and Daniel Hershcovich. 2024.
PartV. Exploring visual culture awareness in GPT-4V: A
comprehensiveprobing. arXivpreprint.
ArnavArora,Lucie-aiméeKaffee,andIsabelleAugen-
stein.2023. Probingpre-trainedlanguagemodelsfor YongCao,LiZhou,SeolhwaLee,LauraCabello,Min
cross-culturaldifferencesinvalues. InProceedings Chen, and Daniel Hershcovich. 2023. Assessingcross-culturalalignmentbetweenChatGPTandhu- TaranehGhandi,HamidrezaPourreza,andHamidreza
mansocieties:Anempiricalstudy. InProceedingsof Mahyar.2023. Deeplearningapproachesonimage
theFirstWorkshoponCross-CulturalConsiderations captioning: Areview. ACMComput.Surv.,56(3).
inNLP(C3NLP),pages53–67,Dubrovnik,Croatia.
AssociationforComputationalLinguistics. RicardoGonzalez,JazminCollins,ShiriAzenkot,and
Cynthia Bennett. 2024. Investigating use cases of
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier- ai-poweredscenedescriptionapplicationsforblind
giovanni, Piotr Padlewski, Daniel Salz, Sebastian andlowvisionpeople. arXivpreprint.
Goodman, Adam Grycner, Basil Mustafa, Lucas
Beyer,AlexanderKolesnikov,JoanPuigcerver,Nan DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,
Ding,KeranRong,HassanAkbari,GauravMishra, ChiLin,KristenGrauman,JieboLuo,andJeffreyP.
LintingXue,AshishVThapliyal,JamesBradbury, Bigham. 2018. Vizwiz grand challenge: Answer-
Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, ing visual questions from blind people. In 2018
Burcu Karagol Ayan, Carlos Riquelme Ruiz, An- IEEE/CVFConferenceonComputerVisionandPat-
dreasPeterSteiner,AneliaAngelova,XiaohuaZhai, ternRecognition,pages3608–3617.
Neil Houlsby, and Radu Soricut. 2023. PaLI: A
DannaGurari,YinanZhao,MengZhang,andNilavra
jointly-scaled multilingual language-image model.
Bhattacharya. 2020. Captioning images taken by
InTheEleventhInternationalConferenceonLearn-
peoplewhoareblind. InComputerVision–ECCV
ingRepresentations.
2020,pages417–434,Cham.SpringerInternational
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- Publishing.
ishna Vedantam, Saurabh Gupta, Piotr Dollar, and
Daniel Hershcovich, Stella Frank, Heather Lent,
C.LawrenceZitnick.2015. Microsoftcococaptions:
Miryam de Lhoneux, Mostafa Abdou, Stephanie
Datacollectionandevaluationserver. arXivpreprint.
Brandl, Emanuele Bugliarello, Laura Cabello Pi-
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, queras, Ilias Chalkidis, Ruixiang Cui, Constanza
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan Fierro,KaterinaMargatina,PhillipRust,andAnders
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion Søgaard.2022. Challengesandstrategiesincross-
Stoica, and Eric P. Xing. 2023. Vicuna: An open- cultural NLP. In Proceedings of the 60th Annual
sourcechatbotimpressingGPT-4with90%*Chat- Meeting of the Association for Computational Lin-
GPTquality. blogpost. guistics(Volume1: LongPapers),pages6997–7013,
Dublin,Ireland.AssociationforComputationalLin-
Tai-YinChiu,YinanZhao,andDannaGurari.2020. As- guistics.
sessingimagequalityissuesforreal-worldproblems.
2020IEEE/CVFConferenceonComputerVisionand Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
PatternRecognition(CVPR),pages3643–3653. DanSu,YanXu,EtsukoIshii,YejinBang,Andrea
Madotto,andPascaleFung.2023. Surveyofhalluci-
WenliangDai,JunnanLi,DongxuLi,AnthonyTiong, nationinnaturallanguagegeneration. ACMComput.
Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Surv.,55(12):248:1–248:38.
Fung,andStevenHoi.2023. InstructBLIP:Towards
general-purposevision-languagemodelswithinstruc- AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
tiontuning. InThirty-seventhConferenceonNeural sch,ChrisBamford,DevendraSinghChaplot,Diego
InformationProcessingSystems. delasCasas,FlorianBressand,GiannaLengyel,Guil-
laumeLample,LucileSaulnier,LélioRenardLavaud,
KhadidjaDelloulandSlimaneLarabi.2023. Towards Marie-AnneLachaux,PierreStock,TevenLeScao,
realtimeegocentricsegmentcaptioningfortheblind Thibaut Lavril, Thomas Wang, Timothée Lacroix,
andvisuallyimpairedinrgb-dtheatreimages. arXiv and William El Sayed. 2023. Mistral 7b. arXiv
preprint. preprint.
Michael Denkowski and Alon Lavie. 2014. Meteor Ju Yeon Jung, Tom Steinberger, Junbeom Kim, and
universal: Languagespecifictranslationevaluation Mark S. Ackerman. 2022. “so what? what’s that
foranytargetlanguage. InProceedingsoftheNinth todowithme?”expectationsofpeoplewithvisual
WorkshoponStatisticalMachineTranslation,pages impairmentsforimagedescriptionsintheirpersonal
376–380, Baltimore, Maryland, USA. Association photo activities. In Proceedings of the 2022 ACM
forComputationalLinguistics. DesigningInteractiveSystemsConference,DIS’22,
page1893–1906,NewYork,NY,USA.Association
Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit forComputingMachinery.
Padhi, Mattia Rigotti, Jarret Ross, Yair Schiff,
RichardA.Young,andBrianBelgodere.2022. Im- HugoLaurençon, LéoTronchon, MatthieuCord, and
agecaptioningasanassistivetechnology: Lessons Victor Sanh. 2024. What matters when building
learned from vizwiz 2020 challenge. J. Artif. Int. vision-languagemodels? arXivpreprint.
Res.,73.
JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.
Gemini Team. 2024. Gemini 1.5: Unlocking multi- Hoi.2023a. Blip-2: Bootstrappinglanguage-image
modal understanding across millions of tokens of pre-training with frozen image encoders and large
context. arXivpreprint. languagemodels. arXivpreprint.YifanLi,YifanDu,KunZhou,JinpengWang,XinZhao, David Romero et al. 2024. Cvqa: Culturally-diverse
andJi-RongWen.2023b. Evaluatingobjecthalluci- multilingualvisualquestionansweringbenchmark.
nationinlargevision-languagemodels. InProceed- arXivpreprint.
ingsofthe2023ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages292–305,Sin- Anastasia Schaadhardt, Alexis Hiniker, and Jacob O.
gapore.AssociationforComputationalLinguistics. Wobbrock.2021. Understandingblindscreen-reader
users’experiencesofdigitalartboards. InProceed-
Fangyu Liu, Emanuele Bugliarello, Edoardo Maria ingsofthe2021CHIConferenceonHumanFactors
Ponti,SivaReddy,NigelCollier,andDesmondEl- inComputingSystems,CHI’21,NewYork,NY,USA.
liott.2021. Visuallygroundedreasoningacrosslan- AssociationforComputingMachinery.
guages and cultures. In Proceedings of the 2021
Mrinank Sharma, Meg Tong, Tomasz Korbak, David
Conference on Empirical Methods in Natural Lan-
Duvenaud, Amanda Askell, Samuel R. Bowman,
guageProcessing,pages10467–10485,Onlineand
NewtonCheng,EsinDurmus,ZacHatfield-Dodds,
Punta Cana, Dominican Republic. Association for
ScottR.Johnston,ShaunaKravec,TimothyMaxwell,
ComputationalLinguistics.
Sam McCandlish, Kamal Ndousse, Oliver Rausch,
Nicholas Schiefer, Da Yan, Miranda Zhang, and
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
Ethan Perez. 2023. Towards understanding syco-
Lee. 2023a. Visual instruction tuning. In Ad-
phancyinlanguagemodels. arXivpreprint.
vances in Neural Information Processing Systems,
volume36.
Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems,
Chunhuayu,RayaHoresh,RogérioAbreudePaula,
ZhixuanLiu, YoueunShin, Beverley-ClaireOkogwu,
and Diyi Yang. 2024. Culturebank: An online
YoungsikYun, LiaColeman, PeterSchaldenbrand,
community-driven knowledge base towards cultur-
JihieKim,andJeanOh.2023b. Towardsequitable
allyawarelanguagetechnologies. arXivpreprint.
representationintext-to-imagesynthesismodelswith
thecross-culturalunderstandingbenchmark(ccub) Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and
dataset. arXivpreprint. RaduSoricut.2022. Crossmodal-3600: Amassively
multilingualmultimodalevaluationdataset. InPro-
Oscar Mañas, Pau Rodriguez Lopez, Saba Ahmadi, ceedingsofthe2022ConferenceonEmpiricalMeth-
Aida Nematzadeh, Yash Goyal, and Aishwarya odsinNaturalLanguageProcessing,pages715–729,
Agrawal. 2023. MAPL: Parameter-efficient adap- AbuDhabi,UnitedArabEmirates.Associationfor
tation of unimodal pre-trained models for vision- ComputationalLinguistics.
languagefew-shotprompting. InProceedingsofthe
17thConferenceoftheEuropeanChapteroftheAs- RamakrishnaVedantam,C.LawrenceZitnick,andDevi
sociationforComputationalLinguistics,pages2523– Parikh. 2015. Cider: Consensus-based image de-
2548,Dubrovnik,Croatia.AssociationforComputa- scriptionevaluation. In2015IEEEConferenceon
tionalLinguistics. ComputerVisionandPatternRecognition(CVPR),
pages4566–4575.
Meredith Ringel Morris, Jazette Johnson, Cynthia L.
MorVentura,EyalBen-David,AnnaKorhonen,andRoi
Bennett,andEdwardCutrell.2018. Richrepresenta-
Reichart.2023. Navigatingculturalchasms: Explor-
tionsofvisualcontentforscreenreaderusers. Pro-
ingandunlockingtheculturalpovoftext-to-image
ceedings of the 2018 CHI Conference on Human
models. arXivpreprint.
FactorsinComputingSystems.
JianfengWang,ZhengyuanYang,XiaoweiHu,Linjie
AnjishnuMukherjee,ZiweiZhu,andAntoniosAnas-
Li, KevinLin, ZheGan, ZichengLiu, CeLiu, and
tasopoulos.2024. Crossroadsofcontinents: Auto-
LijuanWang.2022. GIT:Agenerativeimage-to-text
matedartifactextractionforculturaladaptationwith
transformerforvisionandlanguage. Transactions
largemultimodalmodels. arXivpreprint.
onMachineLearningResearch.
OpenAI.2024. Hellogpt-4o. blogpost. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
KishorePapineni,SalimRoukos,ToddWard,andWei- ricCistac,TimRault,RemiLouf,MorganFuntow-
JingZhu.2002. Bleu: amethodforautomaticevalu- icz,JoeDavison,SamShleifer,PatrickvonPlaten,
ationofmachinetranslation. InProceedingsofthe Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
40thAnnualMeetingoftheAssociationforCompu- Teven Le Scao, Sylvain Gugger, Mariama Drame,
tational Linguistics, pages 311–318, Philadelphia, QuentinLhoest,andAlexanderRush.2020. Trans-
Pennsylvania,USA.AssociationforComputational formers:State-of-the-artnaturallanguageprocessing.
Linguistics. InProceedingsofthe2020ConferenceonEmpirical
Methods in Natural Language Processing: System
RitaRamos,BrunoMartins,DesmondElliott,andYova Demonstrations,pages38–45,Online.Association
Kementchedjhieva.2023. Smallcap: Lightweightim- forComputationalLinguistics.
agecaptioningpromptedwithretrievalaugmentation.
In2023IEEE/CVFConferenceonComputerVision Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X.
andPatternRecognition(CVPR),pages2840–2849. Zhang, and Ranjay Krishna. 2023. Cultural andlinguisticdiversityimprovesvisualrepresentations. A SurveyonCaptionPreferences
arXivpreprint.
Wecreatedasurveyaimingtounderstandthepref-
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
erencesofindividualswhoseeksightedsupportre-
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
gardingimagecaptioning. Ourinterestwasparticu-
wan,MonaDiab,XianLi,XiVictoriaLin,TodorMi-
haylov,MyleOtt,SamShleifer,KurtShuster,Daniel larlyfocusedonwhethertheypreferimagecaptions
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu toincludeculturalinformation,andhowdetailed
Wang,andLukeZettlemoyer.2022. OPT:Openpre-
they prefer the descriptions to be. We published
trainedtransformerlanguagemodels. arXivpreprint.
oursurveythroughtheProlificplatform,bychoos-
ing60participantswithanequalgendersampleof
and representative across countries compensated
with$18perhour. Wealsoaddedascreenerandse-
lectedparticipantswithoutcorrected/normalvision.
Overall, the participants were positive regarding
the helpfulness and importance of cultural infor-
mationinthecaptionswithaverageratingsof4.1
and3.9,respectively.5 Participantsalsotendedto
prefershortcaptionscomparedtolongerones.
B VizWizDataFiltering–Human
Annotation
As mentioned in the experimental set-up section,
to filter the data we created a survey through the
Prolificannotationplatform. Allannotatorswere
compensatedwith18$perhour. Weranthissurvey
4timesaskingfor40participantseachtime.
We asked people to identify images from the
VizWizdatasetbasedontheirculturalbackground,
provide an original and a corrected caption, and
specify the reason they selected the image as
culture-specific. Wegroupedthereasonsthatthe
annotatorsprovidedforselectingculture-specific
imagesinFigure4.
item brand
currency
18.8% 20.7% cultural food
item language
cultural item
10 ..90 2.26 3001. 63.3700 %9 %9%%% m pau inlti t- infa gctor
1.54% book
17.9% 1.85% animal
2.47% flag
3.09% movie
location
9.57% music
11.1% clothing
9.57% historical figure
Figure4: Distributionofthefactors/indicatorsthatlead
the annotators to select a specific image as culture-
relatedandspecifythecorrespondingculture.
Theculturalconceptsidentifiedbyourannota-
torscanbefoundinFigure5.
5The scale is from 1. Not important/helpful at all to 5.
Veryimportant/helpful)US multicultural UK Germany Japan home-countryof originand currentcountryloca-
8.02% Australia France China India Saudi Arabia
4 35 . .. 36 78 .3 %6
4
3.% %% 07
9
3..
%
07
9
22
.%
4%
7% 2.47% 2.16% 2.16%
1.85%1.85%1.85%1 14 .85%.5 1.54%
%1.54%1.231 %.231 %.230 %.902.690 %2.90 62.
%0960. .23 00. %0 9309.0 6.0%309. 20 903%9. %0 0.03%. 690 20 93% 0.90 .03%. %9 000
9
63 2.% .9003.%90 903 .. %2% 60903. 90% 0903.0 2% 0903 6. %0 0. 0%390.0 20 06.30 0%90 0. 60 0003 .06 .0. %9% .03 ..10 6 69% .
.
63 . .. 60 6..9 1%3 6
6
%0 6 69 667% 1 109 1% 17 %19 1% 1 111%7 7% 7 7% 7 77 777% %% %% %% %%% N J B C S
C
T A T P
Caa uu c
u
oe h
o
aim rod
s
rwt r
l lko
ih ti ft td s ua
ea
ol rme at ih gi
yan
ricr na
b
ni al saa dn
i
ilm
a
an isd ms T M G S I
J
A M E S
ds
e
eh w
s g
or re awe a aa
i y
ux li
a
ate e fi
pi
ti zl
-s
hca yc l bte ho sen lAr iild naa
f
drn icd
a
C W P S I
K
R N H U
Sno o
o
wa
u o u
Ade l u rn
s r n E
eoas
e
wsta
g
dnt n h ae
i
ad
aa
eed r yKa
r
nsn yo iarea I D A A B
S
E N H
It ra er f e
p
ue
o
aral ll
a
rin
r
wy agcb
o
tim
h
na ai ni puc dia
e A
imr fk
rica
S S B A B
J
M C M
Uop i a r r
r Se
un g a ora h
d
d s
Sg e z an r
la
ia i Ria n
t
tl mi
e
ins p ai tn i rh o n rar ae
nean
t o
o
ai ru fo ern
th
pci
e
run elf
a
sto u enr nr nm a
to
ela
t
db ati e
t
io
o
nlin re Fsfs is ci gn oa uc n
u
re d ents 6p io
e
brm sa .ce ott i fi cm oes re i.s gT ib nho e ath nd dic sa t lrn oib ca u af ttf iie ooc nnt
Figure5: Distributionoftheculturalconceptsidentified
Thelaststepistoanswersomefinalques-
intheVizWizdatasetbytheannotators.
tionsaboutyourculturalbackground, and
age. Wedonotcollectanyotherpersonalin-
Thefullannotationguidelineswerethefollow- formation. Youranswerswillonlybeused
ing: forstatisticalresearchpurposes.
• What is your country of origin that
Creating datasets that reflect a variety of
youconsideryour’home’,influencing
culturesisachallengingtask. Thisiswhy
yourculturalbeliefsandotheraspects
wewilltrytofilteranexistingdataset. Your
ofyouridentity?
taskistofindculture-relatedimagesfroma
datasetcalledVizWiz. Youneedto: • Is there a country in which you are
- Visit the dataset website[link]. - Browse currentlylocatedforalongperiodof
thedatasetorusethesearchbarsontheleft time?
side of the page and search key-terms re-
• Howoldareyou? Fillinyearsinnum-
lated to your culture ’Within visual ques-
bers.
tion’, ’Within visual answer’ or ’Within
captions’. - Try to find an image that is
Aftercollectingalltheresponses,wekeptonly
relatedtoyourculture/culturalbackground
theimageswhereatleasttwoannotatorsagreedto
(i.e. food brand, currency, books, culture-
selecttheimageasculture-specific. Afterthisex-
specific locations etc.) - Provide your an-
travalidation,weresultedinatotalof324images
swerstothe5followingquestions.
spanning60differentidentifiedcultures. Wecom-
1. Copy and paste the image name paredthesimilaritybetweenthesuggestedcaptions
(VizWiz_train_**number**.jpg). bytheannotatorsandtheoriginalVizWizcaptions
andtheresultscanbefoundinTable2indicating
2. Based on your cultural background,
ahighsimilaritybetweenthenewculture-specific
specify what culture you think is the
suggestedcaptions.
imagerelatedto.
Captions BLEU-4 ROUGE-L F1
3. Selectacaptionfortheimagefromthe
Culture-specific 37.10 61.90 93.0
suggestedImageCaptions.
Table 2: Results from comparing the culture-specific
4. Do you have a better suggestion for
captionsofthetwoannotatorsagainstthefiveoriginal
theimagecaption? Toguideyourcap-
VizWizcaptions.
tion generation, imagine that you are
describingtheimagetoavisuallyim-
pairedfriend. Thecaptionshouldex-
plain the whole image, including all
the main objects, activities, and their
relationships, and reflect the culture
informationoftheimage.
5. Provideareasonastowhytheimage
isculture-specific.
Afterthis,wecollectedinformationaboutthean-
notators’culturalbackgrounds. WeaskedforbothAnnotators Country Location
count participants. Eachannotatorevaluated12images
andtheircaptionsandforeachimage,weassigned
15
twoannotatorsandaveragedtheirscores. Wepro-
videdthefollowinginstructionstotheannotators
10
forevaluatingthecaptions:
5
Thisstudyinvolvesevaluatingcaptions. To
Annotators Country Location
(a)Distributionofthecurrentcountrylocationofthean- guide your ratings, imagine that you are
notators.
describingtheimagetoavisuallyimpaired
count
10 friend. Thenconsider:
Howwelldoesthecaptiondescribetheim-
8
agetothisfriend? Doesittakeintoaccount
6 culturalconsiderations? Youwillbegiven
twosetsofcaptionsdescribinganimage.
4
1. Specify which caption you prefer for
2
thegivenimage(1,2orboth).
(b)Distributionofthecountryoforiginoftheannotators.
2. Determineifeachcaptionisaccurate
Figure6: Plotsassubfigures.
andrelevanttothegivenimage.
C ModelOverview Asageneralguidanceyoushouldconsider
acaptionasbadwhenithasoneormoreof
WelistmodelswiththeirAPIidentifiersinTable3
thefollowingissues:
below.
a)Captionmissesthemaintopicoftheim-
age. b)Captionhasmajorgrammaticaler-
Name Identifier Reference
BLIP-2 Salesforce/blip2-opt-6.7b Lietal.(2023a) rors (such as being incomplete, words in
InstructBLIP Salesforce/instructblip-vicuna-7b Daietal.(2023)
Idefics2 HuggingFaceM4/idefics2-8b Laurençonetal.(2024) the wrong order, etc). Please ignore the
LLaVA-1.6 llava-hf/llava-v1.6-mistral-7b-hf Liuetal.(2023a) capitalizationofwordsandpunctuation. c)
Gemini-1.5-Pro gemini-1.5-pro-preview-0514 GeminiTeam(2024)
GPT-4o gpt-4o-2024-05-13 OpenAI(2024) Caption includes hallucinations and men-
tionsobjects,activities,orrelationshipsthat
Table3: Overviewofmodelsusedinthisstudy
aredefinitelynotintheimage. d)Caption
is not as informative. e) Caption does not
D ModelPrompting reflecttheculturalinformationdepictedin
theimage.
We provide the templates we used to prompt our
models. Thedefaulttemplateshavebeensourced
fromDaietal.(2023)andShietal.(2024).
G ErrorAnalysisII
E VizwizResults–5OriginalReferences
We report model performance on our filtered Weprovidefurtherexamplesfromcurrency-related
VizWiz dataset when using all five original cap- imagesinFigure7. Wecanseethatforcountries
tionsperimage(ratherthancombinationsoftwo suchasUS,orAustralia,theoriginalVizWizcap-
referencesatatime)inFigure5. tionsprovideculture-specificinformation,butthis
is not the case for Japanese or Arabic currencies.
F HumanEvaluation
Moreover,themodelsseemrobustinwesternand
Toconductthehumanevaluationofthemodelgen- Asiancurrencies,butnotwithalltheArabicones.
eratedresponseswecreatedasurveyandhired54 TheexampleprovidedinFigure7showshowthe
annotators through the Prolific platform compen- models confuse a Jordan currency with Egyptian
satedwith18$perhour. Weaddedascreeningin orSaudiArabiancurrenciesand howthesmaller
the platform for a representative sample of coun- open-source models are more prone to hallucina-
triesandanevendistributionofmaleandfemale tions.Defaultprompting
<Image> A short image description:
<Image> Write a caption that describes the photo.
Format your response in JSON as follows:
{
"caption": "Caption for the image"
}
<Image> A photo of
<Image> Can you briefly describe the content of the image?
<Image> Write a caption that describes the photo.
Culture-specificprompting
<Image> A short, culture-aware image description:
<Image> Cultural information encompasses content that showcases the distinctive characteristics, artifacts, or manifestations of a
(cid:44)→ specific group, community, or region.
This includes, but is not limited to, practices, behaviors, norms, values, beliefs, habits, customs, architectural styles,
(cid:44)→ environmental engagements, and any other elements that are emblematic of a particular cultural setting.
It does not include generic information or widespread practices that are not distinctly tied to a specific cultural identity.
For this task, consider information as "cultural" if:
1. It is associated with or characteristic of a specific identified group (e.g., Americans, Italians, midwestern Americans, etc.).
2. It reveals a unique aspect of that group’s way of life, including social conventions, physical creations, or interactions with
(cid:44)→ their surroundings that are not typically seen in other cultures.
3. It provides insight into the cultural uniqueness, whether through social practices, material culture, or other culturally
(cid:44)→ significant elements.
Please exclude generic or ubiquitous statements or observations that do not clearly relate to the unique cultural context of a
(cid:44)→ specific group.
Given this image, do two things:
1. Determine whether the provided example contains cultural information.
2. Write a caption that describes the photo and includes the cultural information extracted.
Format your response in JSON as follows:
{
"caption": "Caption for the image",
"is_cultural": true/false,
"justification": "Why or why not the image contains cultural information"
}
<Image> Write a caption that describes the photo and includes any cultural information present.
Table4: Imagecaptioningtemplatesusedtopromptourmodels.
H CaseStudy target culture behind each image and the GPT-4
VisionoutputafterloadingtheimageintheBeMy
WeillustratethevalueofculturalandinclusiveVL Eyesapplication.
modelsviaacasestudyonevaluatingGPT-4Vas
avisualassistantintegratedintothe‘BeMyEyes’
platform. In this case study, we took a random
sampleof20imagesfromtheMaRVLdataset(Liu
etal.,2021). Hereweprovideaselectionofimages
wetriedinourcasestudy. EachfigureincludestheModel BLEU-4 METEOR CIDEr SPICE
Prompt Default Cultural Default Cultural Default Cultural Default Cultural
Annotation Original-Full Original-Full Original-Full Original-Full
BLIP-2 14.9 12.1 16.1 15.5 51.7 44.3 10.6 9.9
InstructBLIP 25.3 24.5 22.0 22.1 77.4 78.9 15.0 15.0
Idefics2 20.8 16.7 22.2 23.3 82.0 76.1 15.1 16.6
LLaVA-1.6 17.3 11.8 23.3 22.1 60.9 40.5 15.3 15.3
Gemini-1.5-Pro 18.6 9.9 25.5 21.9 73.0 15.0 18.0 15.3
GPT-4o 20.5 13.8 27.4 24.4 67.7 41.0 18.4 16.6
Table 5: Performance of various VLMs on our filtered VizWiz dataset across captioning prompts (default &
culture-specific)usingthefiveoriginalreferencecaptionsperimage. Weunderlinethebestresultforeachmodel
anddisplaythetopresultforeachmetricinbold.
Figure7: Examplesfromimagesrelatedtocurrencycomparingoriginal( )withculture-specific( )annotations
andgeneratedcaptionsfromGemini-ProandGPT-4owithdefault( )andculture-specific( )prompting.Figure8: ApictureextractedfromMaRVLdepicting
astatueofAgamasandtheGPT-4Vimagedescription Figure10: ApictureextractedfromMaRVLdepicting
providedinBeMyEyes. sambar,atraditionalTamildish,andtheGPT-4Vimage
descriptionprovidedinBeMyEyes.
Figure9: ApictureextractedfromMaRVLdepicting
Figure11:ApictureextractedfromMaRVLdepictinga
Buddhist statues and the GPT-4V image description
döner,atraditionalTurkishdish,andtheGPT-4Vimage
providedinBeMyEyes.
descriptionprovidedinBeMyEyes.