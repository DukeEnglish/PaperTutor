Transfer Learning with Self-Supervised Vision
Transformers for Snake Identification
AnthonyMiyaguchi1,*, MuriloGustineli1,*, AustinFischer1 and RyanLundqvist1
1GeorgiaInstituteofTechnology,NorthAveNW,Atlanta,GA30332
Abstract
WepresentourapproachfortheSnakeCLEF2024competitiontopredictsnakespeciesfromimages.Weexplore
anduseMeta’sDINOv2visiontransformermodelforfeatureextractiontotacklespecies’highvariabilityand
visualsimilarityinadatasetof182,261images.Weperformexploratoryanalysisonembeddingstounderstand
theirstructure,andtrainalinearclassifierontheembeddingstopredictspecies.Despiteachievingascoreof
39.69,ourresultsshowpromiseforDINOv2embeddingsinsnakeidentification. Allcodeforthisprojectis
availableathttps://github.com/dsgt-kaggle-clef/snakeclef-2024.
Keywords
TransferLearning,DINOv2,DimensionalityReduction,CEUR-WS
1. Introduction
SnakeCLEF[1]isataskintheLifeCLEFlab[2]thatpromptsthedevelopmentofaclassificationsystem
foridentifyingvenomousandharmlessspeciesofsnakesfromimages. Thedatasetincludes103,404
observations with 182,261 images of 1,784 species from 214 countries, sourced from the iNaturalist
andHerpmapperplatforms. 19.5%of thespecies arevenomous, andthe datasetisimbalanced. The
datasetposessignificantchallengesinclassificationduetothehighvariabilityinspeciesfoundinany
particulargeographicallocationandtherelativelyhighdegreeofvisualsimilaritybetweenspeciesdue
tomimicryandconvergence.
2. Self-Supervised Vision Transformers
Visiontransformers(ViTs)treatimagesasaregulargridofsmallerpatchesprocessedsequentially,like
wordsinasentence. Semanticinformationisembeddedintoahigh-dimensionalspacethatpreserves
semanticinformation. Theattentionmechanismoftransformerscanlearntooptimizepatternsacrossa
sequenceofembeddedpatchtokens.Thebasicmethodforself-supervisiononimagesinvolvesrandomly
maskingpatchesinanauto-encoderarchitecture,whichenableslearninginlargedataregimeseffectively
[3]. Visiontransformershavedemonstratedstate-of-the-artperformance,overtakingconvolutional
neuralnetworks(CNN)inthecomputervisiondomain.
DINOv2[4]isaself-supervisedvisiontransformerdevelopedbyMeta. DINOv2demonstratesstrong
out-of-distribution performance, making it ideal for dealing with various previously unseen snake
photographsinvariedscenarios. TheDINOv2architectureisavailableinvariousparameterizations
with distinctive patch token dimensions: small (S) with 382, base (B) with 768, large (L) with 1024,
andgiant(g)with1536. ThemodelistrainedusingLVD-142M,amassivecollectionofimagesthat
combinesotherdatasetssuchasImageNet-22k. Thebasemodelgeneratesembeddingswithafixedsize
ofR257×768,regardlessoftheinputimagedimensions. Thisisachievedbydividingeachimageinto
256fixed-sizepatcheswithanadditional[CLS]tokeninR768.
CLEF2024:ConferenceandLabsoftheEvaluationForum,September9-12,2024,Grenoble,France
*Correspondingauthor.
$acmiyaguchi@gatech.edu(A.Miyaguchi);murilogustineli@gatech.edu(M.Gustineli);afischer39@gatech.edu(A.Fischer);
rlundqvist3@gatech.edu(R.Lundqvist)
(cid:26)0000-0002-9165-8718(A.Miyaguchi)
©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
luJ
8
]VC.sc[
1v87160.7042:viXraFigure1: UMAPprojectionofDCTand[CLS]embeddingsofthetop5snakespeciesbyimagecount.
3. Exploratory Data Analysis
The DINOv2 embeddings are learned representations of the data that map images into a manifold,
preserving geometrical notions of distance from the input space. The quality and differentiability
oftheseoutputsarecriticalfordownstreammodeling. Weperformdimensionalityreductionofthe
embeddingstocompressthedataintwoways: byextractingthetop8×8coefficientsofthe2Ddiscrete
cosinetransform(DCT)ontheR256×768patchtokens,andthe[CLS]tokeninR768. TheDCTeffectively
capturesperiodicpatternsinperceptualdomainssuchasimagesandaudio,soweexploreitsbehavior
asacomputationallyefficientdata-independentlossycompressiontechnique. The[CLS]tokenisa
discriminatorintheself-supervisionprocessoftrainingtheViT,andthus,itshouldalsoserveasagood
representationofhowdistantpointsareinspace. Transferlearningtasksoftenusethe[CLS]token
fromvisiontransformerstotrainclassifiersondomain-specifictasks[4].
We visually inspect embeddings by reducing their dimension and scatter plotting the results in
twodimensionsusingclasslabelsascolors. Weexpectsimilarimagestoclustersincetheprojected
manifoldslearnaconceptualdistancebetweenpoints. InFigure1weplotUMAP[5]projectionsof
theseembeddings,boththeir[CLS]tokensandtheDCTcoefficientsofthepatchtokens. [CLS]tokens
demonstratebetterclusteringbetweenspeciesthantheDCToutput,whichhaslittleapparentcoherence
intheir2Dprojections. The2DDCTiseitherfilteredtooharshly(i.e.,64coefficientsarenotenoughfor
finedetailsinthelandscape)orthereislittleperiodicityinthespectralstructureofthetokenstotake
advantageof. The[CLS]tokenscanbeseentoclusterdistinctlyfortheimagesofsomespecies,suchas
IndotyphlopsBraminusinFigure1andBitisGabonicainFigure2. Thereappeartobenodesformost
species,butsomepairsofspecieshavehighlyintersperseddistributions,suchasCoronellaAustriaca
andViperaBerusinFigure1. Therichstructureexpressedinthe2Dprojectionoftheembeddingsignals
arepresentationamenabletotasksinotherdomains.
Given the ability to visually discriminate between species images through a clustering analysis,
weconstructadatasetcontainingasubsetofsixspeciesthatmayrepresentchallengesposedbythe
competitioninFigure2.Thefirstselectedspecies,Micrurusfulvius,isacoralsnakebelongingtoElapidae
andthushasmedicallysignificantvenom. Thesecondselectedsnake,Lampropeltistriangulum,orthe
non-venomousmilksnake,isamemberofColubridaeandisknowntobevisuallysimilartovenomous
coral snakes due to Batesian mimicry. Effectively classifying these two snakes is significant for the
mission of the competition and for improving human health. The third selected species is Morelia
spilota, the carpet python, which diversifies the subset of data to include individuals belonging to
Pythonidaeandhasampleassociatedindividualimagesinthedataset. Finally,weexamine3venomous
thick-bodiedviperswithsimilarlifestrategiesandpatterningasleaflitterambushpredators,being:
gaboonvipers(Bitisgabonica),copperheads(Agkistrodoncontortrix),andcottonmouths(Agkistrodon
piscivorus). ThesearebeneficialtostudyastheyrepresentViperidaeandvaryingeographicalrange,Figure2: Aselectedsubsetofsnakespecieswithuniquefeaturesrelevanttothetask.Weextractedthe[CLS]
tokenembeddingsusingDINOv2andcreatedprojectionsviaUMAP.Thedistributionisexpected,withspecies
likeAgkistrodonpiscivorusandAgkistrodoncontortrixbeingrepresentedsimilarly.Additionally,speciesthatare
morebiologicallypolymorphicanddifferinappearanceregionally,likeLampropeltistriangulumandMorelia
spilota,exhibitawiderspread.Incontrast,speciesthatappearmoreuniform,likeBitisgabonicaandMicrurus
fulvius,arefoundindistinctclusters.
withGaboonvipersresidinginsub-SaharanAfricaandthelatterresidinginthecontiguousUnited
States. Additionally,differentiatingbetweenyoungcopperheadsandyoungcottonmouthscanprove
challengingyetessential,withcottonmouthspossessingmoremedicallysignificantvenom.
4. Methodology
WetrainedaclassifiertoidentifysnakesusingDINOv2astheprimaryfeatureextractor. Thepipeline
comprisespre-processing,modeling,andinferencestagesimplementedinseparatemodulestofacilitate
thedevelopmentandtestingofthemodel,asillustratedinFigure3.
4.1. Preprocessing
Preprocessingincludesjoiningmetadataandimagesintoacolumnardatastoretohelpaidwithdata
processing in a map-reduce setting. We use Luigi to orchestrate our pipeline and Apache Spark to
processtheimagesandmetadataintoaParquetdatasetof200partitions. Theimagebinarydataisread
fromtheextractedtarballs,andthefilenameisjoinedtothecorrespondingentryinthemetadatafile.
ThespeciesclassIDisusedasthelabelinthetransferlearningprocess,whichusestheunderlying
embeddingmodeltopredictnewtasksanddomains.
We select the ViT-B/14 (distilled) base model, which is available via HuggingFace under
pre-trainedonLVD-142M.Wepre-computetheDINOv2embeddingsfor
facebook/dinov2-base
eachimage, usingthebasepre-trainedmodeltotransformeachimageandextractthe[CLS]token.
Wealsokeepcoefficientsfromthe2DDCTonpatchtokensasanalternativerepresentation. The2D
DCTmapstheembeddingsinR256×768 tocoefficientsinR256×768,ofwhichwekeepthetopleft8×8
coefficientsflattenedintoavector. WeusetheHuggingFaceTransformerslibrarywithPySparkona
singleg2-standard-8machineonGoogleCloudPlatform(GCP)usinganNVIDIATeslaL4GPU.Figure3: End-to-endpipeline.Thedownloadingmoduleretrievesthetrainingandtestimagesandthemetadata
file,storingtheminaGoogleCloudStorage(GCS)bucket.Thepreprocessingmoduleconvertstheimagesto
binarydataandwritesthemasparquetfilestoGCS.Inthemodelingmodule,thebaseDINOv2modelextracts
embeddingsfromthetrainingandtestdata,andalinearclassifieristrainedonthetrainingembeddings.During
inference,thetrainedclassifiermakespredictionsonthetestembeddings,formattingtheresultsforleaderboard
submission.
4.2. Modeling
Thetrainingmoduleusestheextractedembeddingstotrainaclassifier. Wetrainaneuralnetworkwith
alinearlayer,mappingtheembeddingspacetoanoutputspaceinthedomainofclassIDs. Themodel
istrainedwithLightningusinganegativelog-likelihood(NLL)loss. WeuseAdamastheoptimizer,and
Lightningfindsalearningrateautomatically. Weuseatrain/validationsplitof80/20andthehidden
testsetforthefinalmodelevaluation.
Theinferencemoduleusesthetrainedmodeltomakepredictionsonnewimages. Webuildascript
to run offline without access to the network. Due to limitations in the HuggingFace platform, all
network access is restricted, including the local loopback network. As an unfortunate side-effect,
PyTorchdataloadersmustberuninasingleprocesssincemanyconcurrencycontroloperationsare
remote-procedurecallsoverthenetworkinsteadofsharedmemory. Wechaintheembeddingextraction
andmodelpredictionsandtaketheargmaxoftheoutputtogetthepredictedclass. Eachobservation
mayhavemultipleimages. Weusethemodeofthepredictions,usingthefirstimageasatie-breaker
forthefinalprediction. Thefirsttie-breakerisdoneforeaseofimplementationbutcanalsobedone
randomlyorbycalculatingthemode.
4.3. Evaluation
WeevaluateourmodelsagainsttheleaderboardhostedonHuggingFace. Theevaluationmetricsarethe
sameasthe2023editionofthecompetition[6]. TheTrack1metricistheweightedaveragepercentage
ofthemacro-F1scoreandweightedaccuracyofdifferenttypesofvenomousconfusion. Ahigherscore
isbetter. TheTrack2metricisacumulativesumoferrorsrelatedtoidentifications,penalizingmistakes
suchasidentifyingavenomoussnakeasaharmlessone. Lowerscoresarebetter. TheF1andAccuracy
scoresarebasedsimplyonmodelprediction.5. Results
Wenotethescoresforourrunintable5. Weachievedascoreof39.69onTrack1,significantlylower
thanthetopscoreof85.6andthebaselinescoreof67.0. Track2scoresarenegativelycorrelatedto
track1scores,withascoreof3790versusthetopscoreof687andthebaselinescoreof1861. Themodel
alsoscored0forF1andAccuracy,meaningmostpredictionswerewrong.
Table1
Atablecapturingthefirstplace,baseline,andDS@GTscores.
Rank ID Track1 Track2 F1 Accuracy Timestamp
1 upupup 85.63 687 43.66 72.04 2024-05-2320:02:18
7 BaselinewithSwin-v2tiny 67.01 1861 13.34 39.88 2024-02-2808:13:33
15 DS@GT-LifeCLEF 39.69 3790 0 0 2024-05-2421:03:59
6. Discussion
Themodelperformanceissuspiciouslybadandcouldlikelybeattributedtoabuginimplementinglabel
indexing. TheclassIDsaremappedtoacontiguousrangetoavoidtrainingerrorsinlabelsbutnotinthe
inferencecode. Thisbehaviorpresentsasthe0scoreforF1andAccuracyin5,wheremostanswersare
wrong. ThisisconsistentwiththescoresinTrack1and2. InTrack1,theweightedaverageconsiders
situationswherewemakeharmlesspredictions. Thescorelikelyreflectsthedistributionofthedata,
wheretherearemorevenomousthanharmlessones. TheTrack2scoreisgenerallyhighbecausethe
modelracksupmispredictions. Iftheindexingbugwerefixed,theperformanceofourmethodcould
likelyoutperformthebaseline. WebelieveclearclusteringbehaviorintheDINOv2embeddingspace
(seeFigure2)showspotentialforfuturemodels.
7. Future Work
7.1. Task-specificModeling
Ourmethodologydidnotconsiderthecustomcompetitionmetricsrelatedtovenomousclassification.
Futuremethodswouldincludealternativelossesthatmightbetterguidetheoptimizertowardsolutions
thatbestfitthedatadistributionandidentificationconstraints. Oneofthechallengesinadeeplearning
settingistofindanappropriatelossthatcanactasasurrogateforourtaskmetric. Inparticular,the
taskmetricsarenon-differentiablepiece-wisefunctionsunsuitedforgradientdescent.
Wewouldalsoliketoexplorealternativestothenegativelog-likelihoodlosstohandleclassimbalances
withoutmodifyingtheinputdatadistribution. Theasymmetricloss(ASL)proposedby[7]handlesmulti-
classimbalancesthroughdynamicdown-weightingofeasyexamplesandisamenabletosingle-class
taskstoo.
7.2. Imagesegmentation
Tolearntoperformtheimageclassificationtask,themachinelearningmodelmustlearntoextract
thesignalfromeachimage,whichisthemostcriticalfactorfordifferentiatingbetweenspecies. Given
sufficientdata,deeplearningmodelscanlearntolocatethissignalthroughgradientdescent. However,
theamountofrequireddatadependsonthepredictionspace,variancewithindatainputs,andmodel
architecture. Furthermore,manyofthepicturesinthisdatasetonlyhaveasmallorobstructedportion
ofthepictureinwhichthesnakeispresent. Whilethemodelmaybeabletopickupsomesignalfrom
thesurroundingcontextofthesnake(e.g. plantspeciesfrequentlyassociatedwiththesnakespecies),
themostreliablesignalwillalwaysbetheinpixelsrepresentingthesnakeitself. Tothisend,weproposefutureworkthatincludesmethodstoisolatethissignalfromtherestoftheimageandforcethemodel
tolearnpatternsfromthisportion.
Inparticular,wefocusontechniquesthatinvolvemodelstrainedongeneralimagesegmentation
tasks. Someofthesemodelscanbeusedoutoftheboxtoperformthetaskunsupervised,whileothers
involvetransferlearningthroughsupervisedfine-tuningofpre-trainedmodels.
7.2.1. SegmentAnythingModel(SAM)
Figure4:UnsupervisedSAM Figure5:ManuallyselectedSAMsegment
ThefirstmodelofinterestisMeta’sSegmentAnythingModel(SAM)[8]. Thismodelistrainedto
separate images into distinct segments based on detected objects in the image. Although it can be
prompted with a point placed on the image, it can also run entirely autonomously and return a set
ofsegmentswithnohumaninput. ThechallengetousingSAMistheinabilitytodeterminewhich
returnedsegmentscorrespondtothesnakewithoutfurtherdownstreamwork.
7.2.2. OWL-ViT
OWL-ViT[9]isapre-trainedobjectdetectionmodelthattakescandidatelabeltextsandanimageas
input and returns predicted bounding boxes for the candidate labels’ positions in the image. Since
OWL-ViTcanalsoperformzero-shotinference,itpresentsalowbarriertouse.
Inourtests,itlabeledsnakeinonlyaround44%ofimages,mostofwhichwereeasilydiscernibleto
thehumaneye. Thislevelofperformanceisbelowourneedsforprocessingincomingimages,although
itcouldbehelpfulincreatingadatasetoflabeledimagesfromitsoutputs.
7.2.3. Fine-TuningYOLOv8
Usingsupervisedlearningtotrainamodelonthetaskofidentifyingpixelsthatbelongtosnakesis
perhaps the most promising approach we identified to identify the pixels with the most signal for
speciesclassification. Extraeffortisrequiredforthisapproach,asalabeleddatasetofimagesmustbe
gatheredtofine-tunethemodelforthetask. Fortunately,open-sourcedatasetsarealreadyavailablefor
thistask,includingthosefoundonRoboflow,whichcontainover5000imagesofsnakeslabeledfor
imagesegmentation. Modelstrainedonthesedatasetshaveachievedameanaverageprecision(MAP)
of80.1%[10]orhigher.
Usingsuchamodel,itisfeasibletoidentifyrelevantimagesegmentsinahighportionofcases. After
successfullyidentifyingthesegmentsoftheimagescontainingsnakes,theseoutputboundingboxesor
masksmustbepresentedtothedownstreammodelingtasksforembeddingandclassification. Inthe
caseofboundingboxes,thiscanbeastraightforwardcroppingoftheimagetothebox’sboundaries.
ThisisfeasiblebecauseDINOv2,ourembeddingmodel,acceptsimagesofanyaspectratio. Inthecase
of image segmentation masks, there is the possibility of filling the spaces of the image outside the
maskswithnullpixels,preservingonlytheinformationidentifiedbyourimagesegmentationmodelas
belongingtoasnake. Whilethismethodallowsforthemostsignalfiltering,DINOv2willneedtobeFigure6:OWL-ViTprediction Figure7:Fine-tunedYOLOv8prediction
evaluatedforitsperformanceinhandlingimageswithnullpixelvalues,asthesearelikelyoutsideof
itstrainingdistributionandcouldresultinunexpectedbehavior.
8. Conclusions
This paper explored the application of Meta’s DINOv2 vision transformer model for snake species
identification as part of the SnakeCLEF 2024 competition. Our approach involved training a linear
classifier on features extracted by DINOv2 from a diverse dataset comprising over 182,000 images.
Despiteascoreof39.69,ourworkdemonstratesthepotentialofusingpre-trainedvisionmodelsfor
thisspeciesclassificationtask.
OurfindingsindicatethatwhiletheDINOv2embeddingsprovideastrongfoundationforidentifying
snakespecies,therearesignificantopportunitiesforimprovement. Theseincludeapplyingandopti-
mizingmoreadvancedneuralnetworkarchitecturesandincludingimagesegmentationtechniquesto
isolaterelevantfeaturesofsnakesthatcouldsubstantiallyenhancemodelperformance.
Our initial results lay the groundwork for more refined approaches in snake identification using
transfer learning. We believe significant performance gains can be achieved by addressing current
limitationsandincorporatingadvancedsegmentationmethods,contributingvaluableinsightstospecies
classificationandbiodiversitymonitoring. Allcodeforthisprojectisavailableathttps://github.com/
dsgt-kaggle-clef/snakeclef-2024.
Acknowledgements
ThankyoutotheDS@GTCLEFteamforprovidingthehardwareforrunningexperiments. Thankyou
totheorganizersoftheSnakeCLEFtaskandLifeCLEFlabforrunningthecompetition.
References
[1] L.Picek,M.Hruz,A.M.Durso, OverviewofSnakeCLEF2024: Revisitingsnakespeciesidentifica-
tioninmedicallyimportantscenarios, in: WorkingNotesofCLEF2024-ConferenceandLabsof
theEvaluationForum,2024.
[2] A. Joly, L. Picek, S. Kahl, H. Goëau, V. Espitalier, C. Botella, B. Deneu, D. Marcos, J. Estopinan,
C.Leblanc,T.Larcher,M.Šulc,M.Hrúz,M.Servajean,etal., Overviewoflifeclef2024: Challenges
onspeciesdistributionpredictionandidentification, in: InternationalConferenceoftheCross-
LanguageEvaluationForumforEuropeanLanguages,Springer,2024.
[3] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, R. Girshick, Masked autoencoders are scalable vision
learners, in: ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
2022,pp.16000–16009.[4] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haziza,
F.Massa,A.El-Nouby,etal., Dinov2: Learningrobustvisualfeatureswithoutsupervision, arXiv
preprintarXiv:2304.07193(2023).
[5] L. McInnes, J. Healy, J. Melville, Umap: Uniform manifold approximation and projection for
dimensionreduction, arXivpreprintarXiv:1802.03426(2018).
[6] A. Joly, C. Botella, L. Picek, S. Kahl, H. Goëau, B. Deneu, D. Marcos, J. Estopinan, C. Leblanc,
T. Larcher, et al., Overview of lifeclef 2023: evaluation of ai models for the identification and
predictionofbirds,plants,snakesandfungi, in: InternationalConferenceoftheCross-Language
EvaluationForumforEuropeanLanguages,Springer,2023,pp.416–439.
[7] T.Ridnik,E.Ben-Baruch,N.Zamir,A.Noy,I.Friedman,M.Protter,L.Zelnik-Manor, Asymmetric
lossformulti-labelclassification, in: ProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,2021,pp.82–91.
[8] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.Berg,
W.-Y.Lo,P.Dollár,R.Girshick, Segmentanything(2023). .
arXiv:2304.02643
[9] M.Minderer,A.Gritsenko,A.Stone,M.Neumann,D.Weissenborn,A.Dosovitskiy,A.Mahendran,
A.Arnab,M.Dehghani,Z.Shen,X.Wang,X.Zhai,T.Kipf,N.Houlsby,Simpleopen-vocabulary
objectdetectionwithvisiontransformers,2022. .
arXiv:2205.06230
[10] Snakesegmentationdataset,https://universe.roboflow.com/snake-ic96y/snake_seg/dataset/1,2023.
Accessed: 30May2024.