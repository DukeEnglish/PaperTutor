When LLMs Play the Telephone Game:
Cumulative Changes and Attractors
in Iterated Cultural Transmissions
JérémyPerez∗,1,CorentinLéger1,GrgurKovacˇ1,CédricColas1,2,GaiaMolinaro1,3,Maxime
Derex4,Pierre-YvesOudeyer1,andClémentMoulin-Frier1
1Inria,Flowersteam,UniversitédeBordeaux,France
2MIT,ComputationalCognitiveScienceLab,Cambridge,MA,USA
3DepartmentofPsychology,UniversityofCalifornia,Berkeley,Berkeley,CA,USA
4InstituteforAdvancedStudyinToulouse,Toulouse,France
Abstract
Aslargelanguagemodels(LLMs)startinteractingwitheachotherandgenerating
anincreasingamountoftextonline,itbecomescrucialtobetterunderstandhow
informationistransformedasitpassesfromoneLLMtothenext.Whilesignificant
researchhasexaminedindividualLLMbehaviors, existingstudieshavelargely
overlookedthecollectivebehaviorsandinformationdistortionsarisingfromiterated
LLMinteractions. Smallbiases,negligibleatthesingleoutputlevel,riskbeing
amplifiediniteratedinteractions,potentiallyleadingthecontenttoevolvetowards
attractorstates. Inaseriesoftelephonegameexperiments,weapplyatransmission
chaindesignborrowedfromthehumanculturalevolutionliterature: LLMagents
iterativelyreceive,produce,andtransmittextsfromtheprevioustothenextagent
inthechain. Bytrackingtheevolutionoftexttoxicity,positivity,difficulty,and
lengthacrosstransmissionchains,weuncovertheexistenceofbiasesandattractors,
andstudytheirdependenceontheinitialtext,theinstructions,languagemodel,
and model size. For instance, we find that more open-ended instructions lead
tostrongerattractioneffectscomparedtomoreconstrainedtasks. Wealsofind
thatdifferenttextpropertiesdisplaydifferentsensitivitytoattractioneffects,with
toxicity leading to stronger attractors than length. These findings highlight the
importanceofaccountingformulti-steptransmissiondynamicsandrepresenta
firststeptowardsamorecomprehensiveunderstandingofLLMculturaldynamics.
1 Introduction
Largelanguagemodels(LLMs)areplayinganincreasinglysignificantroleintheproductionofmedia
contentacrossvariousdomains[10]. Theyarebeingusedforacademicwriting[12,2,16,24,37,
40,14],journalism[55],storygeneration[70,78,79],aschatbotsonsocialmedia[61]andinthe
workplaceatlarge[11,27]. AsLLMsbecomemoreperformant,controllable,andmorewidespread,
theirimpactonthecreationanddisseminationofinformationcontenthumansconsumeisexpected
togrowevenfurther[10].
GiventheimplicationsofLLMusagefortheproductionofculturalinformation,numerousresearchers
havestudiedthepropertiesofLLM-generatedcontent. Inthesestudies,LLMsshowedseveralbiases
withrespecttogender[1,62,72,30,25], race[57], values[5,66], politics[48,3,30], authority,
∗Correspondingauthor:jeremy.perez@inria.fr
Preprint.Underreview.
4202
luJ
5
]hp-cos.scisyhp[
1v30540.7042:viXraFigure1:Thetransmissionchainexperimentaldesign. (a)Single-turntransmission:anLLMagent
receivesahuman-generatedinputtext(e.g. astory)andatask(e.g. “rephrasethetext”)andgenerates
anoutputtext. (b)Multi-turntransmission: achainofLLMagentsisgiventhesametask,withthe
firstagentreceivinganinitialtextandsubsequentagentsreceivingtheoutputoftheprecedingagent.
Measuresoftoxicity,positivity,difficulty,andlengtharerecordedateachstepofthechain.
fallacyoversight,andbeauty[15]. Theywerealsofoundtogenerateatleastasattractive[40]and
compelling[68]textsthanhumansandtodisplaysimilarcognitivebiases[26].
Whilesingle-turnbehaviorsofLLMspromptedwithahuman-generatedpromptareunderactive
scrutiny,littleisknownabouttheeffectofiteratedinteractions. Indeed,LLMsoftenuseexistingcul-
turalcontenttogeneratenewones,forexamplewhenwritingscientificreviews[37,2]orgenerating
newstoriesbasedonexistingexamples[78]. AstheshareofAI-generatedcontentincreases,LLMs
willbeproducingoutputsusingcontentthatisalreadyLLM-generated,makingitcrucialtostudythe
consequenceofthisiterativeprocess. Moreover,LLMsareincreasinglybeingusedinmulti-agent
settings[20,51,52,77,34,71,17]andarealreadyinteractingwithoneanotheraschatbotsonsocial
media2.
Nonetheless,littleisknownabouthowpopulationsofLLMsmightself-organize,asothercomplex
systemsdo. Researchincomplexsystemstraditionallystudieshowglobal-levelpatternsemerge
fromlocalinteractions[29,43]. Here,weaskwhethermulti-turnbehaviorsconditionedonLLM-
generatedcontentcausetheappearanceofnewkindsofbiases,undetectableinsingle-turnbehaviors
butaccumulatingacrossiteratedinteractions.
Toaddressthisquestion,wetakeinspirationfromtheculturalevolutionliterature.Thefieldofcultural
evolutionaimstoprovidecausalexplanationsforthechangeofculture(definedassociallyinherited
information) over time. In particular, we draw insights from a research tradition called cultural
attractiontheory(CAT)[67,47,44]. CATaimstodeterminehownon-randomtransformationsof
culturalinformationduringtransmissioneventsmayleadtotheevolutionofprogressivelymorestable
forms,referredtoasattractors. Althoughthepreciseconceptualizationofattractorsvariesacross
authors,anencompassingdefinitionmaybe"theoreticalpositsthatcapturethewayinwhichcertain
ideational variants are more likely to be the outcome of transformations than others." [13]. For
example,CAThasshownhowthecognitiveappealofbloodletting—thehistoricallycommonplace
butoftendamagingpracticeoflettingbloodouttocureapatient—explainswhyitisfoundinmany
unrelated cultures worldwide [45]. Experiments from the CAT literature showed that successive
transmissionsofastoryaboutamundaneeventcanleadbloodlettingtoacquireacausalrolethatitdid
nothaveintheoriginalstory. Otherexperimentalworkhasalsorevealedhowmodifyinginformation
overtransmissionchainsmakesitconvergetowardinductivebiases[36]orhowecologicalfactors
mayinfluencethepositionofattractors[46].
MostexperimentsinCATemployatransmissionchaindesign[42],firstintroducedby[7]. Inthis
setup,chainsofparticipantsreceive,produceandtransmitsocialinformationfromandtoeachother
in a sequential manner (as in the popular telephone game). This powerful and highly controlled
designallowstoevaluatethehigh-levelpatternsthatemergefromtheaccumulationofdirectional
changesduringsingle-turntransmissionevents. Here, weadaptthisdesigntostudyhowculture
evolvesalongchainsofLLMs,ratherthanhumanparticipants(Figure1).
WeconductedseveraltransmissionchainexperimentswithLLMs,wherethefirstLLM-basedagent
inthechainreceivesahuman-generatedtext,elaboratesonit,andthenpassesittothenextagent
in the chain. This transmission step is repeated with different instances of the LLM agent until
2https://chirper.ai/
2theendofthechainisreached. Byintroducingseveralnovelevaluationmethods,weestimatethe
extenttowhichsuccessivetransmissioneventsaffecttheevolutionofmultipletextproperties,namely
its toxicity, positivity, difficulty, and length. By comparing the properties of the initial (human-
generated)andfinaltexts(afterseveraltransmissionsbetweenLLMs),weillustrateandstudythe
existenceofpotentialattractorsinLLMculturalevolution. Inparticular,wemeasuretheeffectof
consecutiveinteractionscomparedtotheeffectofthefirstone. Tostudyhowtextpropertiesand
attractors are affected by the specific context in which culture evolves, we conduct our analyses
onfivedifferentmodels(ChatGPT-3.5-turbo-0125,Llama3-8B-Instruct,Mistral-7B-Instruct-v0.2,
Llama3-70B-Instruct, and Mixtral-8x7B-Instruct-v0.1), three different tasks (i.e., instructions to
either“rephrase”,“takeinspirationfrom”,or“continue”theinitialtext)and20differentinitialtexts.
AlthoughourfocusisonabetterunderstandingoftheculturaldynamicsofLLMs,themetricsand
evaluationmethodsintroducedheremayalsobeofinteresttoresearchersstudyinghumancultural
evolution.
Thecodeforreproducingthesimulations,analysesandfiguresisavailableonourGitHub3.
Ourmaincontributionsare:
• We introduce an experimental paradigm based on transmission chains to study the biases and
attractorsintroducedbymulti-turnLLMinteractions(Section3.1)
• Weintroduceamethodtostatisticallyassesstheadditionaleffectsofmulti-turninteractionsover
single-turnones. (Section3.2)
• We introduce a method to estimate the existence, position, and strength of potential cultural
attractors(Section3.4).
• Westudytheevolutionoftextproperties(positivity,difficulty,toxicity,andlength)acrossiterated
transmissionsanduncoversystematicbiasesandattractorsthatarespecifictomulti-turninteractions
(Section4.2)
• WeshowhowtheLLMproperties(modelandsize)andthetaskinfluencethepositionandstrength
ofattractors(Section4.3)aswellasthetendencyofdifferentchainstoconvergetosemantically
similartexts(Section4.4).
2 Relatedwork
BiasesinLLMsoutputs LLM-generatedcontentisknowntoexhibitavarietyofstereotypical
biases[8,73]. Insingle-turnsettings,LLMsperpetrate[49]orevenamplifyhumanbiasesbased
ongender,nationality,race,andreligion[39]. Forinstance,theGPTmodelwasshowntoexhibit
culturalvaluessimilartothoseofWEIRD(Western,Educated,Industrial,Rich,Democratic)cultures
[5]. LLMs trained through reinforcement learning with human feedback (RLHF) were found to
overlyexpressleft-wingopinionsonAmericanpolitics—atendencythat,onceformed,isdifficultto
avoidevenaftersteeringthemodeltowarddifferentdemographicgroups[63].
Transmissionchainsfeaturingartificialagents Severalstudieshaveappliedexperimentaldesigns
usedinculturalevolutiontostudyknowledgeandskillaccumulationingroupsofReinforcement
Learningagents[18,64,69,56]. Closertothecurrentstudy,populationsofLLMshavealsobeen
studied[10]. Iterativechainsofgenerativemodelstrainedontheprecedingmodel’soutputhavebeen
showntosometimescollapsetowardthemostlikelyoutputswhilethetailsoftheoriginaldistribution
disappear[65,54].ThisideaofusingLLM-generatedcontenttofine-tunethenextgenerationhasalso
beenappliedtogroupsofLLMswithvariouscommunicationstructures[33]. Similartoourapproach,
iterativechainswithfrozen(i.e.,notre-trained)LLMshavebeenshowntoexpresshuman-likebiases
in terms of gender stereotypes, positivity, and social, threat, and biology-related information [1].
Strong,butnon-human-likebiasesforproducingfactualinformationhavealsobeenobserved[17],
stressingtheimportanceofunderstandingtheevolutionofcontentinLLMsandthewaysitmight
deviatefromhumanculturalevolution.
3https://github.com/jeremyperez2/TelephoneGameLLM
3Attractor Attractor
Position Strength
Predicted value after three chains
(150 generations)
Predicted value after two chains
(100 generations)
Predicted value after one chain
(50 generations)
Initial value
Figure2: MethodforestimatingattractorstrengthandpositionThisfiguresdepictsthemethod
introducedinSection3.4toestimatethestrengthandpositionoftheoreticalattractors. Eachdot
in this figure corresponds to one chain, for a total of 100 chains (20 initial texts * 5 seeds). The
positionofadotonthex-axiscorrespondstothevalueoftheproperty(positivityinthisexample)in
theinitialtext,whilethepositiononthey-axiscorrespondstothevalueofthispropertyofthetext
producedafter50generations. Wethenusedthese100datapointstofitalinearregressionpredicting
the relationship between the initial and final values of the property. As visible in the figure, this
relationshipcanbeusedtopredictthevaluetowardwhichthepropertywouldtheoreticallyconverge
atthelimit. Thisconvergencepointonlyexistsiftheslopeofthecurveislowerthan1. Whenthe
slopeisgreaterthan1,thefittedrelationhsipwouldpredictdivergence. Thepositionofthepotential
theoreticalattractorcorrespondstointersectionbetweenthefittedlineandthediagonal,whileits
strengthcorrespondsto1−s,sbeingtheslopeofthefittedrelationship.
3 Methods
Ourtelephonegameexperimentsaimtostudythepossibleattractorsandbiasesthatmayaccumulate
acrossmultipleturnsofinteractionsbetweenLLMs. Thisisdonewithatransmissionchaindesign
trackingtheevolutionLLMoutputsasafunctionofthenumberofinteractionsintheLLMchain.
Thissectionintroducesourtransmissionchaindesign(Section3.1),thesetofmetricsusedtostudy
theevolutionoftextproperties,semanticsimilarityandtheaddedeffectofmulti-turninteractions
(Section3.2),andourmethodtocharacterizethepropertiesofattractors(Section3.4).
3.1 LLMtransmissionchains
Intransmissionchains,individualparticipantsareorderedlinearly. Eachparticipantreceivessome
information from the previous one, performs a task, and transmits new information to the next
participant. Eachagentispromptedwithatask(instructiononhowthetextshouldbeprocessed)
andatext,whichareconcatenatedandpassedtotheusermessage. Thefirstagentisgivenahuman-
generatedtextandatask,andsubsequentagentsaregiventhesametaskandthetextgeneratedbythe
previousagentinthetransmissionchain:
text =LLM(task,text ), (1)
i+1 i
wheretext istheinitialhuman-generatedtextandLLM generatesanoutputbasedontasktask
0
andthepreviousagent’stextx . Werunthisprocessfor50generations. Examplesoftextsevolving
i
throughgenerationsareprovidedinAppendixSectionA.
4Initialtexts(text ) Weborrowhuman-generatedtextfromvariousdatabasestoprovidetheinitial
0
inputtoeachtransmissionchain. Sincewewereinterestedinhowvariationintheinitialtextwould
impactthepropertiesoftheensuingchain,human-generatedtextsspannedvarioustypesofcontent:
scientificabstracts4,newsarticles5,andsocialmediaposts6. Asweareinterestedintheevolutionof
thetoxicity,positivity,difficultyandlengthofgeneratedtexts,wesampletheentiredatasettoobtain
asubsetof20initialtextsthatcoveredtherangeofpossiblevaluesfortheseproperties. Theexact
methodusedtoextractthesetextsisdetailedinAppendixSectionA.
Tasks TodeterminetheeffectsofinstructionsontheevolutionofcontentovergenerationsofLLMs,
weprompteachchainofLLMswiththreedifferenttasksencompassingtypicalusesofLLMs:
• Rephrase: agentsareinstructedtoparaphrasethereceivedtextwithoutmodifyingitsmeaning.
Thistaskisrelevantforapplicationssuchastextsimplification,orforcontentsummarization.
• Takeinspiration: agentsareinstructedtotakeinspirationfromthereceivedtexttoproduceanew
one. Itcanbeusedincreativewriting,wherethegoalistogeneratenewandoriginalcontent.
• Continue: agentsareinstructedtocontinuethereceivedtext. Itisrelevantforapplicationssuch
asdialoguegeneration,inordertogeneratecoherentandrelevantresponsestouserinputs,orfor
contentgenerationinstorytellingandgaming.
Asformodels,tasksremainedconsistentwithineachchain. Theexactpromptusedforeachtaskis
reportedinAppendixSectionA.
Models Toassesswhetherandhowculturalevolutiondynamicsareaffectedbythemodelspec-
ifications, we run identical experiments using five different models, all commonly used, from
three different companies and with varying sizes: GPT-3.5-turbo-0125 (referred to as GPT3.5) ,
Llama3-8B-Instruct (referred to as Llama3-8B), Mistral-7B-Instruct-v0.2 (referred to as Mistral-
7B),Llama3-70B-Instruct(referredtoasLlama3-70B),Mixtral-8x7B-Instruct-v0.1(referredtoas
Mixtral-8x7B).Forinference, weusedtheOpenAIAPI(TheMITLicense)7 torunGPT3.5and
theHuggingFace’sTransformerlibrary[76]forothermodels(ApacheLicence,v2.0). Inoursetup,
transmissionchainsarealwayshomogeneouswithrespecttothemodel,i.e. eachchainiscomposed
ofapopulationofagentssharingthesameunderlyingmodel.
3.2 Metrics
Textproperties Iteratedtransmissionsmayaffectthegeneratedtextinseveralways. Wefocus
onfour,orthogonalpropertiesforeachtextwhichcouldbeautomaticallymeasured(asopposedto
requiringhumanannotators,whichwouldhavebeenimpracticalgiventhesizeoftheoutputcorpus):
• Toxicity. Companiestypicallyfine-tuneLLMstoavoidthegenerationoftoxic(i.e.,dangerousor
harmful)content.However,toourknowledge,thisfine-tuningstepfocusesonsingle-turndynamics,
andtheevolutionofcontentwithrespecttoitstoxicityiscurrentlyunderstudied. Wemeasurethe
toxicityoftextsbyquantifyingthepresenceofrude,disrespectful,orunreasonablelanguage,using
aprobabilityscorethatrangesfrom0.0(benignandnon-toxic)to1.0(highlylikelytobetoxic),as
estimatedbytheclassifierintroducedin[31].
• Positivity. Evenwhentrainedtoavoidtoxiccontent,LLMshavebeenshowntoexpresssimilar
positivitybiasestohumans,oftenfavoringnegativeoverpositiveinformationinpreservingand
generating new information [1]. To study whether positivity biases over transmission chains
are affected by tasks and models, we measure the positivity of produced contents using the
SentimentIntensityAnalyzer tool from NLTK [32]. It uses this information to calculate a
sentimentscoreforthetext,rangingfrom-1.0(highlynegative)to1.0(highlypositive).
• Difficulty. While LLMs are argued to benefit society by democratizing knowledge [74], such
positiveoutcomesareconditionedontheLLMsgeneratingoutputthatisaccessibleandinclusive
to all kinds of audiences. However, whether text difficulty is preserved, increased, or reduced
4https://huggingface.co/datasets/CCRss/arxiv_papers_cs
5https://huggingface.co/datasets/RealTimeData/bbc_latest
6https://huggingface.co/datasets/FredZhang7/toxi-text-3M/blob/e0e5b168b4a7e14e84f07271bfe1c6b42bc91ccd/multilingual-
train-deduplicated.csv
7https://openai.com/index/openai-api/
5overtransmissionchainsiscurrentlyunknown. WeestimatetextdifficultyusingtheGunning-Fog
index[9],whichdependsontheaveragesentencelengthandthepercentageofdifficultwords. A
standardinterpretationofthisindexisthatitestimatestheyearsofformaleducationrequiredto
fullyunderstandthetext.
• Length: Asimple,yetcrucialaspectofapieceoftextisitslength. Asmoreandmorecontentis
generatedbyandfromLLMoutputs,culturalmediamaybecomepopulatedwithincreasinglyshort
(potentiallyincomplete)orlong(potentiallyredundant,meaningless,orhardtoprocess)material.
Wethereforeassesstheevolutionofcontentlengthasmeasuredbythecharactercountofgenerated
text.
WeprovideadditionaldetailsaboutmetricsinAppendixSectionA.
Semanticsimilarity Aswearealsointerestedinmeasuringtheextenttowhichdifferentchains
converge(ordiverge)towardsproducingsemanticallysimilar(ordifferent)outputs,wemeasurethe
similaritybetweenallpairsoftextproduced. Wedosobycomputingthecosinesimilaritybetween
textembeddings,obtainingavalueofsimilarityintherange[−1,1]. Anaveragesimilarityscore
closeto1indicatesalltextsweresimilartoeachother,whereasscorescloseto0andbelowindicates
pairs of texts were highly dissimilar. Computing similarity scores for text outputs requires text
embeddings,whichweobtainedwithanembeddingmodelfromSentence-Transformer[59].
3.3 Effectofmulti-turntransmissions
Oneofourquestionsishowcontentevolvesovermulti-turntransmissionscomparedtosingle-turn
settings. To address this point, we compare the distribution of a given property in the generated
texts at the first generation to the distributions at subsequent generations. Thus for each model
andtask,welookatthepropertiesofeachofthe100generatedtexts(20transmissionschains*5
seeds)ateachgeneration,whichgivesusasampleof100propertyvaluesforeachvalue. Usinga
Kolmogorov–Smirnovtest[41],wethentestwhetherthesampleobtainateachgenerationcomes
fromthesamedistributionasthesampleobtainedafterthefirstgeneration. Ifwecanconfidently
rejectthehypothesisthatthesampleofpropertyvaluesattheendofthetransmissionchaincomes
fromthesamedistributionasthesampleobtainedafterthefirstgeneration,thiswouldconfirmthat
lookingatoutputsafterasingle-turntransmissionisnotenoughforpredictingoutputpropertiesina
multi-turnsetting.
3.4 Attractorstrengthandposition
Humanculturalevolutionshowsthatculturaltraitssometimesevolvetowardsattractorstates,i.e.,
contentthatinvitesconvergenceevenwithdifferentstartingpoints[36,45,46,13]. Therefore,we
wereinterestedinwhethertransmissionchainswithLLMswouldshowsimilarattractordynamics,
andwhetherthesedependonthemodelandtaskusedinthechain.Theconceptofculturalattractoris
notconsistentlyformalizedinthehumanculturalevolutionliterature[13]. Here,wedefinedattractors
asthetheoreticalequilibriumpointtowhichtheiteratedgenerationprocess(definedinEq.1)may
eventuallyconverge. Wemathematicallydefineattractorsintermsoftwopropertiesofinterest: its
position(i.e.,thelocationinoutputgenerationspacetheprocessconvergestoward)anditsstrength
(i.e., the intensity to which generated outputs are pulled toward it). The strength takes values in
[0,1],whichallowsforacontinuousnotionofanattractor: ratherthanbeingabinaryconceptthat
eitherexistsordoesnot,attractorsherelieonaspectrum,coveringsystemswithoutattractioneffects
(strength=0)toidealattractors(strength=1). Tocomputepositionandstrength,weusethesimulated
datatofitalinearregressionpredictingthevalueofapropertyattheendofthechainasafunctionof
itsvalueintheinitialtext(Figure2).
8Forexampleforagiventextproperty,wefit:
property(generation=50)=I+s∗property , (2)
initial
whereIistheestimatedinterceptandstheestimatedslope.
8LinearregressionswerefitusingtheSciPylibrary(https://scipy.org/)releasedundertheBSDLicence
6Thisenablesustoestimatethefinaloutputofanewchainstartingfromthefinaloutputoftheprevious
chainas:
property(generation=100)=I+s∗property(generation=50). (3)
Thefittedlinearregressionthusallowstodefinearecurrentrelationshipbetweentheoutputofachain
asafunctionoftheoutputofthepreviouschain:
property(generation=n∗50)=I+s∗property(generation=(n−1)∗50). (4)
Thisrelationshipisalinearrecurrencesequencewhichcanberewrittenas:
property(generation=n∗50)=sn∗(property −l)+l, (5)
initial
wherel= I .
1−s
If|s|<1,thenthesequenceconverges,itslimitislanditsconvergencerateis1−s.
WecanthereforeusetheestimatedrelationshipEq.2todetermineifanattractorexists(|s|<1)and,
ifso,estimateitspositionl= I andstrength1−s.
1−s
Tovalidatethatthesetheoreticalfixedpointscorrectlycaptureattractiondynamics,weestimatedtheir
positionsusingonlydatafromthefirst10generationsofeachchain,andcomparedthepredictions
withtheactualoutputafter50generations. Visualinspectionoftheresults(AppendixSectionB)
confirmedthatourmethodissuitedforestimatingthestrengthandpositionofattractors.
4 Results
For each of the 5 models, 3 tasks, and 20 initial texts, we ran 5 transmission chains with 50
transmission steps. We provide some examples of generated texts in Appendix Section 1, and
completedataonthecompanionwebsite9. Byextractingthepropertiesofgeneratedtextsateach
generationofeachchain,wecanstudytheevolutionofthesepropertiesthroughgenerations,measure
howtheyareaffectedbyinteractionsbeyondsingle-turneffects,aswellasdetectandcharacterize
theoreticalattractors. Bycomparingthesemanticsimilaritiesoftextsproducedbydifferentchains,
wecanalsoevaluatewhethersetsofchainstendtoconvergeordiverge.
4.1 Qualitativeanalysisofpropertyevolutionsovergenerations
Wefirsthypothesizedthatiteratedtransmissionswouldaffectthepropertiesofgeneratedtextsbeyond
thefirsttransmission. Acrossmanyinstances,weindeedfoundthattextpropertieskeepevolving
afterthefirstgeneration. InFigure3,weshowtheevolutionoftextdifficulty,positivity,lengthand
toxicity for one of the 20 initial texts, for all models and all tasks. Evolution of these properties
foreachofthe20initialtextscanbefoundinAppendixSectionB.Thisspecificexamplealready
allowstonoticeimportantdifferencesindynamicsbetweenmodels,tasks,andproperties. Indeed,we
observethatwhiletoxicityconvergestovaluesclosetozeroforallmodelsandtasks,thishappensat
aslowerpaceforGPT3.5intheRephrasetask,andforLlama3-70BintheTakeInspirationtask. For
positivity,weobservethatontheTakeInspirationandContinuetasks,GPT3.5andMixtral-8x7B
convergetohighpositivitiesalmostinstantly,whileevolutionismoregradualforothermodels. The
dynamicsofdifficultyappearstobehighlyinfluencedbytaskandmodels,asweobservecumulative
changesforLlama3-8BandLlama3-70BintheTakeInspirationandContinuetasks,butnotsomuch
forothertasksandmodels. Lengthappearstoexhibitlittletonocumulativedynamicsinthisexample.
Interestingly,thereseemstobeadiscontinuityofhighmagnitudeforMistral-7Btowardtheendof
thechainintheContinuetask. Qualitativeobservationofthetextsrevealedthatthisappearstobean
exampleofcollapsingbehavior,whichwediscussmoreextensivelyinAppendixSectionB.
Overall,theseresultssuggestthattheevolutionoftextpropertiesacrossrepeatedtransmissionis
highlysensitivetobothagentmodelsandtasks.
Althoughweherefocusedonaspecificexample,lookingattheevolutionovergenerationsofthe
distributionofeachpropertiesgivesahigher-levelideaofthedynamics. InFigure4,weshowthe
evolutionofpropertydistributionsovergenerationforeachmodelandtask. Thisrevealsimportant
9https://sites.google.com/view/telephone-game-llm
7Evolution of metrics over generations
Rephrase Inspiration Continue
Model
0.6 gpt-3.5-turbo-0125
Meta-Llama-3-8B-Instruct
0.5 M Mi es tt ar -a Ll l- a7 mB- aIn -3s -t 7r 0u Bct -- Iv n0 st.2 ruct
Mixtral-8x7B-Instruct-v0.1
0.4
0.3
0.2
0.1
0.5
0.0
0.5
30
25
20
15
10
4000
3000
2000
1000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure3: Evolutionofpropertiesforoneofthe20initialtextsovergenerations(Mean±SE)
Evolutionovergenerationsoftextstoxicity,positivity,difficultyandlength(rows)fortheRephrase,
TakeinspirationandContinuetasks(columns)forvariousLLMs(colors). Plotsshowaverageand
standarddeviationsover5seeds,startingwithInitialText18. Equivalentcurvesforall20initial
storiesareprovidedasSupplementaryMaterialinAppendixSection B.Weobservethatiterated
transmissions shape the properties of generated texts beyond the effect of the first transmission.
Evolutioncanconvergequickly(e.g. bottomrow)ormoregradually(e.g. secondrow). Thevalueof
convergencepointsvariesbetweentasksandmodels.
differencedependingontheanalyzedproperty,thetaskandthemodel. Forinstance,weobservethat
toxicity(Figure4.a)convergesveryquicklytoaverynarrowpeakcenteredaround0. Thisisvery
differentfromtheevolutionofpositivity(Figure4.b),forwhichtheinitialdistributionappearsto
bequitepreservedfortheRephrasetask(Figure4.b,firstrow),whilelessconstrainedtaskssuchas
Takeinspiration(secondrow)andContinue(thirdrow)leadtomorevisiblechanges. Interestingly,
weobservethatforLlama3-8B(blue)andLlama3-70B(pink),thedistributionofpositivityvalues
converges to a bimodal distribution, while distributions are unimodal for other models. In some
cases,wealsoobservethatdifferentmodelsleadthedistributionstobeshiftedinoppositedirections.
Forinstancewhenlookingattheevolutionoftextslength(Figure4.d), usingGPT3.5(green)or
Llama3-8B (blue) leads text to become on average shorter, while using Mixtral-8x7B shifts the
distributiontowardsgreaterlengthvalues.
4.2 Towhatextentdomulti-turntransmissionsaffecttheevolutionofproperties?
Qualitativeanalysesfromtheprevioussectionappeartosuggestthatmulti-turntransmissionslead
texts to acquire different properties compared to single-turn settings. To quantitatively evaluate
8
erocs
ytivitisoP
erocs
htgneL
erocs
yticixoT
erocs
ytluciffiDFigure4: Evolutionofthedistributionoftextpropertiesingeneratedtextsacrossgenerations
fordifferentmodelsandtasks. Wehererepresentthedistributionofeachofthefourpropertiesat
eachgeneration,foreachmodelandtask. Thesedistributionsthusrepresentthepropertiesobserved
inthesetof100transmissionchains(20initialtexts*5seeds)foreachmodelandtask. Foreach
property,taskandmodel,the50generationsarearrangedvertically,withfirstgenerationsatthetop
andlastgenerationsatthebottom. Thisrepresentationallowstocapturehowiteratedtransmissions
shiftthedistributionstowardcertainvalues,andhowquicklythisshifthappens.
9Figure5: Textpropertiesareaffectedbytransmissionsbeyondthefirstone. p-values ofthe
KS-testforthenullhypothesisH : "Thetextpropertiesatgenerationiaresampledfromthesame
0
distributionasthetextpropertiesaftergeneration1",foreachtask(columns),property(rows)and
models(colors). Thegreyshadedarearepresentsp-valueslowerthan0.05. Overmostinstances,
p-valuesdecreaseovergenerationandbecomecloseto0. Thisindicatesthatmulti-turntransmissions
leadpropertydistributionstobecomesignificantlydifferentfromthedistributionsobservedafter
single-turninteractions.
this observation, we use Kolmogorov–Smirnov (KS) tests [41] to estimate the compare property
distributionsafterasingleinteractionandaftermultipleinteractions. InFigure5,wereportforeach
model,taskandpropertythep-valueoftheKStestforthenullhypothesisH : "Thetextpropertiesat
0
generationiaresampledfromthesamedistributionasthetextpropertiesaftergeneration1". Across
mostinstances,weobservethatH canbeconfidentlyrejected,indicatingthatpropertydistributions
0
becomesignificantlydifferentaftermulti-turntransmissionscomparetosingle-turntransmissions.
Weobservethatthisismoreoftenthecaseforlessconstrainedtasks(TakeInspirationandContinue,
secondandthirdcolumns)thanformoreconstrainedtask(Rephrase, firstcolumn). Thisfinding
confirmsthatstudyingsingle-turninteractionsisingeneralnotsufficientforanalyzingtheproperty
ofinteractingLLMsoutputs. Thiswarrantsamoredetailedaccountoftheculturaldynamicsacross
iteratedinteractionsamongLLMs.
104.3 Whatinfluencesthepresence,strength,andpositionofattractors?
VisualinspectionoftheevolutionoftextpropertiesaspresentedonFigure4indicatethatmulti-turn
transmissionsleaddistributionstobecomeskewedtowardcertainvalues,whichsuggeststhepresence
of attractors. The task assigned to a chain and the model type populating it appear to influence
thepositionofthoseattractors,aswellastheirstrength(i.e. howquicklydoshiftsindistributions
happen). To havequantitative measuresof attractors strengthsand positions, we use themethod
describedin3.4andFigure2. Byfittingalinearregressionpredictingthevalueofagivenproperty
at the end of the chain as a function of the property in the initial text, we can represent for each
property,modelandtaskthepositionandstrengthoftheattractors,providedtheyexist. Figure6
presentstheestimatedstrengthsandpositionsofattractors,andfittedlinearregressionsareprovided
assupplementarymaterialinFigure8.
Forallcombinationsofproperty,taskandmodel,wefoundthattherecurrentrelationshipdefinedby
thefittedlinearregressionconverges. Thismeansthatallconditionsadmitatheoreticalattractoras
definedinSection3.4. Asforthestrengthandpositionofthesetheoreticalattractors,Figure6already
allowstonoticesometendencies. Forinstance,itseemsthatlessconstrainedtasks(e.g. Continue)
leadtostrongerattractorsthanmoreconstrainedtasks(e.g. Rephrase). Tobetterdisentanglethe
respectivecontributionsofmodeltype,taskandpropertyonattractors,wefittedbayesianmodels
predictingattractorStrengthasafunctionofTask,ModelandProperty,andpredictingattractor
PositionasafunctionofTaskandModel,foreachofthefourconsideredproperties. Detailsabout
statisticalanalysesareprovidedinAppendixsectionB.
WefindastrongeffectofTaskonattractorstrength:Continueleadstosignificantlystrongerattraction
thanTakeInspiration,itselfleadingtosignificantlystrongerattractionthanRephrase. Thisconfirms
ourobservationthatlessconstrainedtasksleadtostrongerattractionthanmoreconstrainedtasks.
Differentpropertiesarealsofindtodisplaydifferentsensitivitytoattractioneffects. Wedetectthat
toxicitypossessessignificantlystrongerattractorsthanpositivity,difficultyandlength. Asforthe
effectofmodel,weobservesignificantlyweakerattractionforLlama3-70bcomparedtoGPT3.5,
Llama3-8BandMixtral-8x7b.
Asforthepositionoftheattractors,weobservetheattractorfortoxicityissignificantlyhigherfor
Llama3-8b than for GPT3.5 and Mixtral-8x7b, and significantly higher for Llama3-70b than for
GPT3.5,Mistral-7bandMixtral-8x7b. ItisalsohigherforContinuethanforRephrase. Forpositivity,
wefoundthatthepositionoftheattractorwassignificantlylowerforLlama3-8bthanforGPT3.5,
Mistral-7bandMixtral-8x7b,andthatthetaskTakeinspirationandContinuebothledtosignificantly
higherpositivitythantheRephrasetask.
4.4 Towhatextentdodifferenttransmissionchainsconvergeonsimilarcontent?
Lastly,weinvestigatetheextenttowhichiteratedtransmissionsleaddifferentchainstodivergeor
convergeasdeterminedbywhetherthebetween-chainsimilarityamonggeneratedtextsincreasesor
decreasesafterseveralgenerations. Thiscanbemeasuredbyassessingthecosinesimilaritybetween
finaltextembeddingsversusthecosinesimilaritybetweeninitialtextembeddingsforeachpossible
pairofchains,foreachmodelandtask(Figure7).
Asexpectedgiventhenatureofthetask,chainsinstructedwiththeRephrasetaskmaintainedclose
similaritywiththeinitialtext(Figure7). Outofthefivemodeltested,Llama3-8Bwasmorelikelyto
maintainsemanticsimilarityacrossgenerationsforthisprompt. ForTakeinspirationandRephrase,
thereseemstobeatendencyforchainstoleadtoaspecificdistancebetweenfinaltexts,astheinitial
distancebetweentextshaslittleimpactonthefinaldistance. Thismeansthatovergenerations,chains
thatstartedwithverysimilartextsdivergewhilechainsthatstartedwithverydifferenttextsconverge.
The position of this attractor appears to be influenced by the model, as for example Llama3-8b
displaysmuchmoreconvergencethanthetwoothermodelsfortheTakeinspirationtask.
5 Discussion
WhilecurrentstudiesanalyzingtheoutputsofLLMsarerestrictedtoasingleprompt-outputinterac-
tion,weborrowedthemethodologyfromstudiesonhumanculturalevolutiontoaddresshowcultural
contentmayevolveovertransmissionchainswithLLMs. Thisresultedinaseriesoftelephonegame
experimentsassessingtheevolutionofculturalcontentinLLMsasafunctionofmodels,instructions,
11Toxicity Positivity Difficulty Length
Model
gpt-3.5-turbo-0125
0.015 M M M Me i e is xt tt ta ar r- -a aL Ll ll l- -a a7 8m mB x7-a aI Bn- -3 3 -s I- -t n8 7r sB 0u tBc- rI t u-n - I cv ns tt 0 s -r t. vu 2 r 0uc .t c 1t 1.0 25 3500
0.010 0.8 3000
20
0.005 2500
0.6
15 2000 0.000
0.4 1500
0.005 10
1000
0.010 0.2 5 500
Rephrase Take Inspiration Continue 0.0 Rephrase Take Inspiration Continue 0 Rephrase Take Inspiration Continue 0 Rephrase Take Inspiration Continue
Task Task Task Task
1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 Rephrase Take Inspiration Continue 0.0 Rephrase Take Inspiration Continue 0.0 Rephrase Take Inspiration Continue 0.0 Rephrase Take Inspiration Continue
Task Task Task Task
Figure 6: Attractors strength and position. The heigth of the bars represent the position (top
row)andstrength(bottomrow)oftheoreticalattractorsestimatedusingthemethoddescribedin
Section3.4,foreachproperty(columns),task,andmodel. Visualinspectionoftheseplotsallowsto
noticesometendencies,suchastheeffectoftaskonattractorstrength: lessconstrainedtasks,suchas
Continue,appeartoproducestrongerattractorsthanmoreconstrainedtasks,suchasRephrase. Our
definitionofattractorsalsoallowstocompareattractorsstrengthaccrossproperties: wecannotice
thatattractorsappearstobestrongerfortoxicity(secondrow,firstcolumn)thanforlength(second
row,fourthcolumn). Finally,wecannoticethatthepositionofattractorsappearstovarybetween
models. Forinstance,theattractorfordifficultyapperarstobehigherforLlama3-8bandLlama3-70b
thanforothermodels. Statisticalanalysesallowedtoquantifythesedifferencesandarepresentedin
Section4.3.
Figure7: Convergence: textsinoutputsoftransmissionchainsareoftenmoresimilarthantexts
givenininput. Weperformpair-wisecomparisonsofallsimulatedchainsandplottherelationship
between the similarity between the two texts given an input of the two chains and the similarity
betweenthetwotextsproducedatthelastgenerationofthesetwochains. Pointsabovethediagonal
indicatethatsimilarityishigherattheendofthechainthanatthebeginning,revealingconvergence.
12
noitisoP
rotcarttA
htgnertS
rotcarttAandtextproperties. Ourresultsrevealthatseveralchangesingeneratedcontentappearaftermultiple
iterations. Forexample,weobservedthatthedifficultyofaprovidedtextwaspreservedafteranLLM
waspromptedtoelaborateitasingletime,butchangeddramaticallyafterthetextwasprocessed
iterativelybyachainofLLMs.
Bycomparingthepropertiesofinputtextstothoseoftextsproducedbytransmissionchainsspanning
severalgenerationsofLLMs,weidentifiedproperty-specificpatternsintheconvergenceofLLM
dynamicstowardattractorstates. Althoughtheexistenceoffixedpointsidentifiedwithourmethod
does not prove the existence of an attractor, it allows to perform quantitative evaluations on the
dynamicsoftextevolutionacrossmodels,tasksandproperties. Usingthismethod,wefoundhigh
convergenceratesforsometextproperties(toxicityandpositivity),independentofthechain’smodel
andtaskwhereastheevolutionofothertextcharacteristics(difficultyandlength)wasinfluenced
by the task and the model. Differences in dynamics among properties could be a consequence
of fine-tuning through reinforcement learning with human feedback, a commonplace practice in
LLMtrainingwhichmaytargetsomepropertiesmorethanother(e.g.,specificallyavoidingtoxic
contentwithoutaddressingitsdifficulty),creatingstrongattractors. Itisalsointerestingtonotice
thatconvergencerateswere,onaverage,higherformoreopen-endedtasks(Takeinspirationand
Continue)thanamoreconstrainedone(Rephrase),suggestingthestudyofculturalevolutioninLLM
transmissionchainsmightbeparticularlyrelevanttosituationsinwhichLLMsareusedtosimulate
artificialsocieties[20,51,52,77,34,71,17],wheretheyareoftengrantedarelativelyhighfreedom
inordertowitnessemergentbehaviors.
Wealsointroducedseveralevaluationmetricsforanalyzingculturaldynamics,inparticulardefininga
task-andmetric-independentnotionoftheoreticalattractor. Althoughthesemethodsweredeveloped
tostudytransmissionchainsinLLMs, similartoolsmaybeappliedtostudiesofhumancultural
evolution,allowingforinferencesacrosstasksandculturaldomains,movingbeyonddomain-specific
results.
Limitationsandfuturework Asthestudyoftheculturaldynamicsofgenerativeagentsisan
emergingresearcharea, oursettinginvolvedseveralsimplifications. Whilewefocusedonlinear
transmission chains, real-world interactions typically involve networks of senders and receivers.
Human studies have shown that network size [28, 60, 4, 6, 21] and structure [58, 38, 22, 19, 23]
influence cultural evolution. Following some initial endeavors [50, 53], future work may assess
similar effects in machine networks. To investigate model- and task-specific biases, we studied
transmissionchainsinhomogeneoussettings,whereagentsbelongedtothesamemodeltypeand
receivedthesameinstructions,butfuturestudiesmayaddressculturaldynamicsinheterogeneous
populationsofLLMspromptedwithvariousinstructions. WhilewefocusedonLLMand,therefore,
textoutputs,similarstudiesmayberuntoaddressthepropertiesofvariousgenerativetools(e.g.,for
imagegeneration). Inthefuture,researchersmayalsoaddresshybridnetworksinwhichhumansand
LLMsinteract—ascenariothatisbecomingincreasinglyrelevantasgenerativetoolsbecomemore
widespreadandwhichmay,inturn,shapethefutureofhumanculturalevolution. [10].
Acknowledgements
This research was partially funded by the French National Research Agency (ANR, project
ECOCURL,GrantANR-20-CE23-0006). ThisworkbenefitedfromaccesstotheJeanZay(Idris)
supercomputerassociatedwiththeGencigrantA0151011996. WealsothankChrisFoulon,Marcela
Ovando-TellezandJoanDussauldwhoparticipatedinthehackathonHack1Roboduringwhichthis
projectoriginated.
References
[1] A. Acerbi and J. M. Stubbersfield. Large language models show human-like content bi-
ases in transmission chain experiments. Proceedings of the National Academy of Sciences,
120(44):e2313790120,2023.
[2] S.Agarwal,I.H.Laradji,L.Charlin,andC.Pal. LitLLM:AToolkitforScientificLiterature
Review,Feb.2024. arXiv:2402.01788[cs].
[3] A.Agiza,M.Mostagir,andS.Reda. AnalyzingtheImpactofDataSelectionandFine-Tuning
onEconomicandPoliticalBiasesinLLMs,Apr.2024. arXiv:2404.08699[cs].
13[4] C.AnderssonandD.Read. Groupsizeandculturalcomplexity. Nature,511(7507):E1–E1,
2014. Publisher: NaturePublishingGroupUKLondon.
[5] M.Atari,M.J.Xue,P.S.Park,D.E.Blasi,andJ.Henrich. Whichhumans?,Sep2023.
[6] R.Baldini. Revisitingtheeffectofpopulationsizeoncumulativeculturalevolution. Journalof
CognitionandCulture,15(3-4):320–336,2015. Publisher: Brill.
[7] P.L.Bartlett. Remembering. CambridgeUniversityPress.,1932.
[8] E.M.Bender,T.Gebru,A.McMillan-Major,andS.Shmitchell. Onthedangersofstochastic
parrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACMConferenceon
Fairness,Accountability,andTransparency,FAccT’21,page610–623,NewYork,NY,USA,
2021.AssociationforComputingMachinery.
[9] J.Bogert. Indefenseofthefogindex. TheBulletinoftheAssociationforBusinessCommunica-
tion,48(2):9–12,1985.
[10] L. Brinkmann, F. Baumann, J.-F. Bonnefon, M. Derex, T. F. Müller, A.-M. Nussberger,
A.Czaplicka, A.Acerbi, T.L.Griffiths, J.Henrich, etal. Machineculture. NatureHuman
Behaviour,7(11):1855–1868,2023.
[11] E.Brynjolfsson,D.Li,andL.R.Raymond. GenerativeAIatWork,Apr.2023.
[12] O.O.Buruk. AcademicWritingwithGPT-3.5: ReflectionsonPractices,EfficacyandTrans-
parency. In 26th International Academic Mindtrek Conference, pages 144–153, Oct. 2023.
arXiv:2304.11079[cs].
[13] A.Buskell. Whatareculturalattractors? Biology&Philosophy,32(3):377–394,2017.
[14] G.Cabanac,C.Labbé,andA.Magazinov.Torturedphrases:Adubiouswritingstyleemergingin
science.Evidenceofcriticalissuesaffectingestablishedjournals,July2021. arXiv:2107.06751
[cs].
[15] G.H.Chen,S.Chen,Z.Liu,F.Jiang,andB.Wang. HumansorLLMsastheJudge? AStudy
onJudgementBiases,Apr.2024. arXiv:2402.10669[cs].
[16] H.Cheng,B.Sheng,A.Lee,V.Chaudary,A.G.Atanasov,N.Liu,Y.Qiu,T.Y.Wong,Y.-C.
Tham,andY.Zheng. HaveAI-GeneratedTextsfromLLMInfiltratedtheRealmofScientific
Writing? ALarge-ScaleAnalysisofPreprintPlatforms,Mar.2024. Pages: 2024.03.25.586710
Section: NewResults.
[17] Y.-S.Chuang, A.Goyal, N.Harlalka, S.Suresh, R.Hawkins, S.Yang, D.Shah, J.Hu, and
T.T.Rogers. Simulatingopiniondynamicswithnetworksofllm-basedagents. arXivpreprint
arXiv:2311.09618,2023.
[18] J.Cook,C.Lu,E.Hughes,J.Z.Leibo,andJ.Foerster. ArtificialGenerationalIntelligence:
CulturalAccumulationinReinforcementLearning,June2024. arXiv:2406.00392[cs].
[19] J.F.-L.dePablo,V.Romano,M.Derex,E.Gjesfjeld,C.Gravel-Miguel,M.J.Hamilton,A.B.
Migliano,F.Riede,andS.Lozano. Understandinghunter–gathererculturalevolutionneeds
networkthinking. TrendsinEcology&Evolution,37(8):632–636,2022. Publisher: Elsevier.
[20] I. de Zarzà, J. de Curtò, G. Roig, P. Manzoni, and C. T. Calafate. Emergent cooperation
andstrategyadaptationinmulti-agentsystems: Anextendedcoevolutionarytheorywithllms.
Electronics,12(12):2722,2023. Publisher: MDPI.
[21] M.Derex,M.-P.Beugin,B.Godelle,andM.Raymond. Experimentalevidencefortheinfluence
ofgroupsizeonculturalcomplexity. Nature,503(7476):389–391,2013. Publisher: Nature
PublishingGroupUKLondon.
[22] M. Derex and R. Boyd. Partial connectivity increases cultural accumulation within groups.
ProceedingsoftheNationalAcademyofSciences,113(11):2982–2987,Mar.2016.
[23] M.DerexandA.Mesoudi. Cumulativeculturalevolutionwithinevolvingpopulationstructures.
TrendsinCognitiveSciences,24(8):654–667,2020. Publisher: Elsevier.
[24] I. Dergaa, K. Chamari, P. Zmijewski, and H. Ben Saad. From human writing to artificial
intelligence generated text: examining the prospects and potential threats of ChatGPT in
academicwriting. BiologyofSport,40(2):615–622,Apr.2023.
[25] X. Dong, Y. Wang, P. S. Yu, and J. Caverlee. Disclosure and Mitigation of Gender Bias in
LLMs,Feb.2024. arXiv:2402.11190[cs].
14[26] J. Echterhoff, Y. Liu, A. Alessa, J. McAuley, and Z. He. Cognitive Bias in High-Stakes
Decision-MakingwithLLMs,Feb.2024. arXiv:2403.00811[cs].
[27] T.Eloundou,S.Manning,P.Mishkin,andD.Rock. GPTsareGPTs: AnEarlyLookatthe
LaborMarketImpactPotentialofLargeLanguageModels,Aug.2023. arXiv:2303.10130[cs,
econ,q-fin].
[28] N.Fay,N.DeKleine,B.Walker,andC.A.Caldwell. Increasingpopulationsizecaninhibit
cumulativeculturalevolution. ProceedingsoftheNationalAcademyofSciences,116(14):6726–
6731,Apr.2019.
[29] J.GleickandR.C.Hilborn. Chaos, MakingaNewScience. AmericanJournalofPhysics,
56(11):1053–1054,Nov.1988.
[30] P.Haller,A.Aynetdinov,andA.Akbik. OpinionGPT:ModellingExplicitBiasesinInstruction-
TunedLLMs,Sept.2023. arXiv:2309.03876[cs].
[31] L.HanuandUnitaryteam. Detoxify. Github.https://github.com/unitaryai/detoxify,2020.
[32] N.Hardeniya,J.Perkins,D.Chopra,N.Joshi,andI.Mathur. Naturallanguageprocessing:
pythonandNLTK. PacktPublishingLtd,2016.
[33] H.Helm,B.Duderstadt,Y.Park,andC.E.Priebe. Trackingtheperspectivesofinteracting
languagemodels,June2024. arXiv:2406.11938[cs].
[34] W. Hua, L. Fan, L. Li, K. Mei, J. Ji, Y. Ge, L. Hemphill, and Y. Zhang. War and Peace
(WarAgent): LargeLanguageModel-basedMulti-AgentSimulationofWorldWars,Nov.2023.
arXiv:2311.17227[cs].
[35] C.HuttoandE.Gilbert. Vader: Aparsimoniousrule-basedmodelforsentimentanalysisof
social media text. In Proceedings of the international AAAI conference on web and social
media,volume8,pages216–225,2014.
[36] M.L.Kalish,T.L.Griffiths,andS.Lewandowsky. Iteratedlearning: Intergenerationalknowl-
edgetransmissionrevealsinductivebiases. PsychonomicBulletin&Review,14(2):288–294,
2007. Place: USPublisher: PsychonomicSociety.
[37] Q.Khraisha,S.Put,J.Kappenberg,A.Warraitch,andK.Hadfield. Canlargelanguagemodels
replacehumansinsystematicreviews? EvaluatingGPT-4’sefficacyinscreeningandextracting
datafrompeer-reviewedandgreyliteratureinmultiplelanguages. ResearchSynthesisMethods,
Mar.2024.
[38] S.KirbyandM.Tamariz. Cumulativeculturalevolution,populationstructureandtheorigin
ofcombinatorialityinhumanlanguage. PhilosophicalTransactionsoftheRoyalSocietyB:
BiologicalSciences,377(1843):20200319,Jan.2022.
[39] H.Kotek,R.Dockum,andD.Sun. Genderbiasandstereotypesinlargelanguagemodels. In
ProceedingsofTheACMCollectiveIntelligenceConference,pages12–24,2023.
[40] R.MarlowandD.Wood. Ghostinthemachineormonkeywithatypewriter—generatingtitles
forChristmasresearcharticlesinTheBMJusingartificialintelligence: observationalstudy.
TheBMJ,375:e067732,Dec.2021.
[41] F. J. Massey. The kolmogorov-smirnov test for goodness of fit. Journal of the American
StatisticalAssociation,46(253):68–78,1951.
[42] A.Mesoudi. Experimentalstudiesofculturalevolution,July2021.
[43] M.Mitchell. Complexity: AGuidedTour. OxfordUniversityPress,Oxford,NewYork,Apr.
2009.
[44] H.Miton. CulturalAttraction,Feb.2024.
[45] H.Miton,N.Claidière,andH.Mercier. Universalcognitivemechanismsexplainthecultural
successofbloodletting. EvolutionandHumanBehavior,36(4):303–312,July2015.
[46] H. Miton, T. Wolf, C. Vesper, G. Knoblich, and D. Sperber. Motor constraints influence
cultural evolution of rhythm. Proceedings of the Royal Society B: Biological Sciences,
287(1937):20202001,Oct.2020. Publisher: RoyalSociety.
[47] O.Morin. HowTraditionsLiveandDie. OxfordUniversityPress,2016. Google-Books-ID:
kSukCgAAQBAJ.
15[48] F.Motoki,V.PinhoNeto,andV.Rodrigues. Morehumanthanhuman: measuringChatGPT
politicalbias. PublicChoice,Aug.2023.
[49] M.Nadeem,A.Bethke,andS.Reddy. Stereoset: Measuringstereotypicalbiasinpretrained
languagemodels. arXivpreprintarXiv:2004.09456,2020.
[50] E. Nisioti, M. Mahaut, P.-Y. Oudeyer, I. Momennejad, and C. Moulin-Frier. Social Net-
work Structure Shapes Innovation: Experience-sharing in RL with SAPIENS, Nov. 2022.
arXiv:2206.05060[cs].
[51] J.S.Park,J.C.O’Brien,C.J.Cai,M.R.Morris,P.Liang,andM.S.Bernstein. Generative
Agents: InteractiveSimulacraofHumanBehavior,Aug.2023. arXiv:2304.03442[cs].
[52] J.S.Park,L.Popowski,C.J.Cai,M.R.Morris,P.Liang,andM.S.Bernstein.SocialSimulacra:
CreatingPopulatedPrototypesforSocialComputingSystems,Aug.2022. arXiv:2208.04024
[cs].
[53] J. Perez, C. Léger, M. Ovando-Tellez, C. Foulon, J. Dussauld, P.-Y. Oudeyer, and
C. Moulin-Frier. Cultural evolution in populations of Large Language Models, Mar. 2024.
arXiv:2403.08882[cs,q-bio].
[54] A.J.Peterson. Aiandtheproblemofknowledgecollapse. arXivpreprintarXiv:2404.03502,
2024.
[55] S. Petridis, N. Diakopoulos, K. Crowston, M. Hansen, K. Henderson, S. Jastrzebski, J. V.
Nickerson, andL.B.Chilton. AngleKindling: SupportingJournalisticAngleIdeationwith
LargeLanguageModels. InProceedingsofthe2023CHIConferenceonHumanFactorsin
ComputingSystems,CHI’23,pages1–16,NewYork,NY,USA,Apr.2023.Associationfor
ComputingMachinery.
[56] B.Prystawski,D.Arumugam,andN.D.Goodman. Culturalreinforcementlearning: aframe-
workformodelingcumulativecultureonalimitedchannel,May2023.
[57] C.Raj,A.Mukherjee,A.Caliskan,A.Anastasopoulos,andZ.Zhu. BreakingBias,Building
Bridges: EvaluationandMitigationofSocialBiasesinLLMsviaContactHypothesis, July
2024. arXiv:2407.02030[cs].
[58] L.Raviv,A.Meyer,andS.Lev-Ari. TheRoleofSocialNetworkStructureintheEmergenceof
LinguisticStructure. CognitiveScience,44(8):e12876,Aug.2020.
[59] N.ReimersandI.Gurevych. Makingmonolingualsentenceembeddingsmultilingualusing
knowledgedistillation.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing.AssociationforComputationalLinguistics,112020.
[60] P.Richerson. Groupsizedeterminesculturalcomplexity. Nature,503(7476):351–352,2013.
Publisher: NaturePublishingGroupUKLondon.
[61] E.Sadikog˘lu,M.Gök,M.Mijwil,andI.Kosesoy. Theevolutionandimpactoflargelanguage
modelchatbotsinsocialmedia:Acomprehensivereviewofpast,present,andfutureapplications,
122023.
[62] L.Salewski,S.Alaniz,I.Rio-Torto,E.Schulz,andZ.Akata. In-ContextImpersonationReveals
LargeLanguageModels’StrengthsandBiases. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[63] S.Santurkar,E.Durmus,F.Ladhak,C.Lee,P.Liang,andT.Hashimoto. Whoseopinionsdo
languagemodelsreflect? InInternationalConferenceonMachineLearning,pages29971–
30004.PMLR,2023.
[64] S.Schmitt, J.J.Hudson, A.Zidek, S.Osindero, C.Doersch, W.M.Czarnecki, J.Z.Leibo,
H.Kuttler,A.Zisserman,K.Simonyan,andS.M.A.Eslami. KickstartingDeepReinforcement
Learning,Mar.2018. arXiv:1803.03835[cs].
[65] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, andR. Anderson. The curse of
recursion: Trainingongenerateddatamakesmodelsforget. arXivpreprintarXiv:2305.17493,
2023.
[66] S.Sivaprasad,P.Kaushik,S.Abdelnabi,andM.Fritz. ExploringValueBiases: HowLLMs
DeviateTowardstheIdeal,Feb.2024. arXiv:2402.11005[cs].
16[67] D.Sperber. AnthropologyandPsychology: TowardsanEpidemiologyofRepresentations. Man,
20(1):73–89,1985. Publisher: [Wiley,RoyalAnthropologicalInstituteofGreatBritainand
Ireland].
[68] G.Spitale,N.Biller-Andorno,andF.Germani. AImodelGPT-3(dis)informsusbetterthan
humans. ScienceAdvances,9(26):eadh1850,June2023. Publisher: AmericanAssociationfor
theAdvancementofScience.
[69] O.E.L.Team,A.Stooke,A.Mahajan,C.Barros,C.Deck,J.Bauer,J.Sygnowski,M.Trebacz,
M.Jaderberg,M.Mathieu,N.McAleese,N.Bradley-Schmieg,N.Wong,N.Porcel,R.Raileanu,
S.Hughes-Fitt,V.Dalibard,andW.M.Czarnecki. Open-EndedLearningLeadstoGenerally
CapableAgents,July2021. arXiv:2107.12808[cs].
[70] M.Valentini, J.Weber, J.Salcido, T.Wright, E.Colunga, andK.Kann. OntheAutomatic
GenerationandSimplificationofChildren’sStories,Oct.2023.
[71] A.S.Vezhnevets,J.P.Agapiou,A.Aharon,R.Ziv,J.Matyas,E.A.Duéñez-Guzmán,W.A.
Cunningham, S. Osindero, D. Karmon, and J. Z. Leibo. Generative agent-based model-
ing with actions grounded in physical, social, or digital space using Concordia, Dec. 2023.
arXiv:2312.03664[cs].
[72] Y.Wan,G.Pu,J.Sun,A.Garimella,K.-W.Chang,andN.Peng. "KellyisaWarmPerson,
Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters, Dec. 2023.
arXiv:2310.09219[cs].
[73] L.Weidinger,J.Mellor,M.Rauh,C.Griffin,J.Uesato,P.-S.Huang,M.Cheng,M.Glaese,
B.Balle,A.Kasirzadeh,etal. Ethicalandsocialrisksofharmfromlanguagemodels. arXiv
preprintarXiv:2112.04359,2021.
[74] D.Weiss.GenerativeAIistheNextStepinDemocratizingKnowledge.https://techstrong.
ai/articles/generative-ai-is-the-next-step-in-democratizing-knowledge/.
[Accessed22-05-2024].
[75] T.Wiecki,R.Vieira,J.Salvatier,M.Kochurov,A.Patil,M.Osthege,B.T.Willard,B.Engels,
O.A.Martin,C.Carroll,A.Seyboldt,A.Rochford,L.Paz,rpgoldman,K.Meyer,P.Coyle,
O.Abril-Pla,V.Andreani,M.E.Gorelli,R.Kumar,J.Lao,A.Andorra,T.Yoshioka,G.Ho,
T.Kluyver,K.Beauchamp,D.Pananos,E.Spaak,andB.Edwards. pymc-devs/pymc: v3.11.6,
May2024.
[76] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,
M.Funtowicz,etal. Huggingface’stransformers: State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771,2019.
[77] B.Xiao,Z.Yin,andZ.Shan. SimulatingPublicAdministrationCrisis: ANovelGenerative
Agent-BasedSimulationSystemtoLowerTechnologyBarriersinSocialScienceResearch,
Nov.2023. arXiv:2311.06957[cs].
[78] Z.Xie, T.Cohn, andJ.H. Lau. The NextChapter: AStudyofLargeLanguageModels in
Storytelling,July2023. arXiv:2301.09790[cs].
[79] Z.Zhao,S.Song,B.Duah,J.Macbeth,S.Carter,M.P.Van,N.S.Bravo,M.Klenk,K.Sick,and
A.L.S.Filipowicz. Morehumanthanhuman: LLM-generatednarrativesoutperformhuman-
LLMinterleavednarratives. InProceedingsofthe15thConferenceonCreativityandCognition,
pages368–370,NewYork,NY,USA,June2023.AssociationforComputingMachinery.
A Additionaldetailsonthemethods
Selectinginitialtexts Weextracted5scientificabstracts10,10newsarticles11,and5socialmedia
comments12fromonlinedatasetsasinitialtexts. Toensurethatthoseinitialtextscoveredtherange
oftextpropertieswewereinterestedin,weproceededasfollows: fordifficulty,wemeasuredthe
maximalandminimaldifficultyd andd oftextsfromthescientificabstractsdatasets,defined
min max
alinearspaceof5values(d ) betweend andd andsampled5texts,eachhavingavalue
i i=1:5 min max
10https://huggingface.co/datasets/CCRss/arxiv_papers_cs
11https://huggingface.co/datasets/RealTimeData/bbc_latest
12https://huggingface.co/datasets/FredZhang7/toxi-text-3M/blob/e0e5b168b4a7e14e84f07271bfe1c6b42bc91ccd/multilingual-
train-deduplicated.csv
17ofdifficultycloseto(d ) . Wethenfollowedthesameprocedurefortoxicity,usingthedatasetof
i i=1:5
socialmediacomments;forpositivity,usingthedatasetofnewsarticles;length,usingthedatasetof
newsarticles.
Pre-processingoutputs Dataanalysesrevealedthat,ontheContinuetask,whenusingMistral-
7B, agents of the chains would sometimes start outputting very long text by filling them with
“#some_keyword”. Asthisbehaviorcreatedafewoutliers,wethoughtitwouldbebettertofilter-
outthose“#some_keyword”whenperformingthemainanalyses. Thisbehaviorisneverthelessan
interestingresult,reminiscentofthecollapsingdynamicsfoundwhentrainingLLMsontheirown
output[65]. WethereforediscussitseparatelyinAppendixB.
Hyperparameters Weusethefollowinghyperparametersforgenerationsinallmodels. Temper-
ature was set to 0.8 with and top_p to 0.95. All models, except GPT3.5, bfloat16 precision was
used.
Computationalresources ExperimentswereconductedwiththeOpenAIAPI(lessthan5million
tokens),andwithaclusterequippedwithA100,andV100GPUgraphiccards. Runningthefinal
experimentsforthefourmodels,whichwererunonthecluster,requiredlessthan3000GPUhours.
ExperimentswithLlama3-8BandMistral-7BwereconductedonV100NVIDIAGPUswith32GB
ofVRAM,andexperimentswithLlama3-70BandMixtral-8x7BontwoA100NVIDIAGPUswith
80GBVRAMinparallel.
Promptsused Inourexperiments,eachtaskwasinducedbyaspecificinstruction(prompt),which
isgiventoeachagentinthechain. FortheRephrasetask,theinstructionis: “Youwillreceiveatext.
Yourtaskistorephrasethistextwithoutmodifyingitsmeaning. Justoutputyournewtext,nothing
else. Hereisthetext:”,fortheInspirationtask,theinstructionis: “Youwillreceiveatext. Yourtask
istocreateaneworiginaltextbytakinginspirationfromthistext. Justoutputyournewtext,nothing
else. Hereisthetext:”,andfortheContinuetask,theinstructionis: “Youwillreceiveatext. Your
taskistocontinuethistext. Justoutputyournewtext,nothingelse. Hereisthetext:”.
Examples of stories Here we provide examples of stories that were given as input and stories
thatweregeneratedinthelastiterationofsomechains. Table1showsoneexampleforeachtask.
Completedatacanbefoundonthecompanionwebsite13usingtheDataExplorertool.
Measuringtextproperties
• Toxicity. We assess the level of toxicity in generated texts using the Detoxify library, a
classifierdevelopedfortheJigsawToxicCommentClassificationChallenges(seehttps:
//github.com/unitaryai/detoxify/tree/master . This classifier defines toxicity
as the presence of rude, disrespectful, or unreasonable language in a text and assigns a
probabilityscorerangingfrom0.0(benignandnon-toxic)to1.0(highlylikelytobetoxic).
Trainedonalargedatasetofhuman-labeledcommentsfromvariousonlineplatforms,the
classifieruseatransformer-basedarchitecturetoanalyzethetext’scontextandmeaning,
identifyingpatternsindicativeoftoxicity.
• Positivity. WeemploytheSentimentIntensityAnalyzertoolfromtheNLTKlibrary
toassessthepositivityofgeneratedtexts. ThetoolisbasedontheValenceAwareDictio-
naryandsEntimentReasoner(VADER)method[35], whichisalexiconandrule-based
sentimentanalysistoolspecificallydesignedforsocialmediadata. Itusesacombination
of lexical features, such as words and their semantic orientation, to determine the over-
all sentiment of a text. In the VADER method, every word in the vocabulary is rated
withrespecttoitspositiveornegativesentimentandtheintensityofthatsentiment. The
SentimentIntensityAnalyzerusesthisinformationtocalculateasentimentscorefor
thetext,rangingfrom-1.0(highlynegative)to1.0(highlypositive).
• Difficulty. WeestimatethedifficultyofgeneratedtextsusingtheGunning-Fogindex. Inthe
method,complexwordsaredefinedasthosewiththreeormoresyllables,excludingproper
nouns,familiarjargon(andremovingcommonsuffixesassyllables). Weusedtextstatto
computeit.
13https://sites.google.com/view/telephone-game-llm
18Table1: ExamplesofstartingstoriesandthecorrespondingfinalstoriesforchainswithGPT3.5on
thethreetasks
Description Story
Startingstory Youarecertainlynofriendofmine. Youhaveaccusedmeoflyingaboutwhat
youhadsaid,andrunawaywhenIprovedbyactualquotationthattheaccusation
wasfalse. Webothknowthat, eventhoughyoupraisehumility, youarenot
humbleenoughtoapologize. Yousneeredatmycombatexperience, saying
“Alotofsocalled“combatveterans”ofVietnamsmelledmoreperfumefrom
callgirlsthantheysmelledcordite”and“SowhenyouwereanArmyofficer
inVietnam(eventhoughyouspentyourtimeinsideofDanangairbase,with
atleasttwobattalionsofinfantrymarinesonthewire)whatwasthecombat
loadforM-16ammopersoldieronadailypatrol?” Anactualfriendwouldnot
havewrittensuchthings. No,youarenofriendofmine,youignorant,arrogant,
hypocriticalblow-hard. YoushouldbeabletoguessthatIdonotfeelfriendly
towardsyou
Final story Yourlaughterandlightwillalwaysbeabeaconofhopeinourdarkestdays.
(Continue) Yourpresencemaybegone,butyourspiritwillforeverbealiveinourhearts.
Wewillcarryonyourlegacyofkindnessandlove,spreadingyourpositivityto
allthoseweencounter. Yourmemorywillneverfade,asweholdyouclosein
ourthoughtsandactions. Resteasy,knowingthatyouaresurroundedbyour
loveandgratitude. Youmaybegone,butyouwillneverbeforgotten. Untilwe
meetagain,dearfriend,knowthatyouarealwayswithus.
Finalstory(In- Amidstthechaosofourbusylives,itisessentialtopauseandprioritizeforming
spiration) genuineconnectionswiththosearoundus. Bybeingfullypresentandshowing
empathy, we can cultivate relationships that are deep and fulfilling. Authen-
ticityisthekeytocreatingbondsthatbringtruejoyandcontentmentintoour
lives. Engaginginopenandsinceredialogues,sharingourvulnerabilities,and
strivingtoforgemeaningfulconnectionsareallessentialinnurturingauthentic
relationships. Let’scometogethertocreateacommunitywhereempathyand
mutualrespectaretheguidingprinciples,andwhereeveryinteractionisinfused
withloveandgratitude. Theseauthenticconnectionsserveasasupportsystem
rootedincompassionandgenerosity,upliftingandinspiringeachotheraswe
navigatetheupsanddownsoflife. Together,wecanweaveanetworkofrela-
tionshipsthatexudepositivityandlight,enrichingourlivesinmeaningfuland
transformativeways.
Final story I need to end our friendship since you won’t apologize for accusing me of
(Rephrase) dishonesty,ignoringevidence,andmakingdisrespectfulcommentsaboutmy
time in Vietnam. Your behavior shows a lack of empathy, arrogance, and
insincerity,qualitiesthatarenotinlinewiththoseofatruefriend.
B Additionalfiguresandanalyses
Statisticalmodels WeperformedstatisticalanalysesusingthePythonpackagepymc[75]tofit
Bayesianmodels.
• Model1 Wefittedamodelpredictingtheattractorstrength(Figure6)asafunctionofthe
Task,ModelandProperty: Strength∼N(µ, σ2)
whereµ=α +β +γ
Task Model Property
Priorsforparametersa,bandcwerestandardnormaldistribution,andstandardhalf-normal
distributionforσ.
• Model2 ForeachProperty,wefittedamodelpredictingtheattractorposition(Figure6)as
afunctionoftheTaskandModel: Position ∼N(µ, σ2)
property
whereµ=α +β
Task Model
19Rephrase Take Inspiration Continue Attractor Position Attractor Strength
1.0
0.8 0.01 0.8
0.6 0.6
0.4 0.00 0.4
0.2 0.01 0.2
0.2 I0 n. i4
tial
t0 o. x6 icity0.8 1.0 0.2 I0 n. i4
tial
t0 o. x6 icity0.8 1.0 0.2 I0 n. i4
tial
t0 o. x6 icity0.8 1.0 Rephrase TakeT Inasspikration Continue 0.0 Rephrase TakeT Inasspikration Continue
Attractor Position Attractor Strength
1.0 1.0 1.0
0.5 0.8 0.8
0.6 0.6
0.0
0.4 0.4
0.5 0.2 0.2
0.5 Initia0
l
. p0 ositivit0 y.5 1.0 0.5 Initia0
l
. p0 ositivit0 y.5 1.0 0.5 Initia0
l
. p0 ositivit0 y.5 1.0 0.0 Rephrase TakeT Inasspikration Continue 0.0 Rephrase TakeT Inasspikration Continue
Attractor Position Attractor Strength
200 25 0.8
150 20 0.6
100 11 05 0.4
50 5 0.2
10 15 Initia2 l0 diffic2 u5
lty
30 35 10 15 Initia2 l0 diffic2 u5
lty
30 35 10 15 Initia2 l0 diffic2 u5
lty
30 35 0 Rephrase TakeT Inasspikration Continue 0.0 Rephrase TakeT Inasspikration Continue
Attractor Position Attractor Strength
11 05 00 00 00 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t 23 00 00 00 001 ... 680
5000 1000 00 .. 24
1000 Ini2 ti0 a0
l
0 length3000 1000 Ini2 ti0 a0
l
0 length3000 1000 Ini2 ti0 a0
l
0 length3000 0 Rephrase TakeT Inasspikration Continue 0.0 Rephrase TakeT Inasspikration Continue
Figure8: Fittedlinearregressionsusedtocomputeattractorsstrengthandposition. Forthree
tasks (Rephrase, Take Inspiration, Continue), five models, and three metrics (toxicity, positivity,
difficulty, length), we plot (Mean ± SE) the relationship between the metric value of the initial
human-writtentext(inputtothefirstagent)andthevalueofthefinalLLM-generatedtext(outputof
thelastagent). Aslopeclosetozeroindicatesstrongattraction,whilethevalueattheintersection
withthediagonalcapturesthepositionoftheattractor.
Figure9: Differencesbetweenposteriorestimatesforthestatisticalmodelpredictingattractor
Strength. DifferencesbetweenestimatefortheeffectofModel(left),Measure(center)andTask
(right) on attractor Strength. Values and color represent the mean value of the 95% credibility
intervals. Starsindicatethatthisintervaldoesnotcontain0.
To determine the significance of the difference between estimated parameters, we computed the
95%credibility intervals ofthe differenceby samplingfromthe posteriors. InFigures 9and 10,
weprovidethematricesindicatingthemeanofthisdifferencebetweenestimatedparameters. Stars
indicatethatthecredibilityintervaldoesnotcontain0.
Discontinuitiesandcollapsingbehavior Inthemaintext,theexperimentwiththeMistral-7B
modelontheContinuetaskwasanalyzedbyfirstfilteringthehashtags,asdiscussedinAppendixA.
Given,thatthisbehaviorisinterestinginitself,wediscussithereinmoredetails.
Figure11showstheaveragelengthoftextgeneratedwiththeMistral-7BmodelchainontheContinue
taskforfivedifferentseedsofthesamestory. Wecanobserveseveraldiscontinuitiesintermsof
20
yticixot
laniF
ytivitisop
laniF
ytluciffid
laniF
htgnel
laniF
noitisop
rotcarttA
noitisop
rotcarttA
noitisop
rotcarttA
noitisop
rotcarttA
htgnerts
rotcarttA
htgnerts
rotcarttA
htgnerts
rotcarttA
htgnerts
rotcarttAFigure10: Differencesbetweenposteriorestimatesforthestatisticalmodelpredictingattractor
Position. DifferencesbetweenestimatefortheeffectofModel(toprow)andTask(bottomrow)on
attractorPositionfortoxicity(firstcolumn),positivity(secondcolumn),difficulty(thirdcolumn)and
length(fourthcolumn). Valuesandcolorrepresentthemeanvalueofthe95%credibilityintervals.
Starsindicatethatthisintervaldoesnotcontain0.
16000 Seed 1
Seed 2
14000 Seed 3
Seed 4
12000 Seed 5
10000
8000
6000
4000
2000
0
0 10 20 30 40 50
Generation
Figure11: DiscontinuitiesandcollapseintheMistral-7BmodelchainThelengthsofgenerated
storiesareshown(withoutfilteringoutthehashtags)forfivechainsstartingwiththesameinitialstory.
Weobservediscontinuities,wherethelengthdrasticallyincreasesordecreases. Afterdecreasing,the
lengthofthestorygoesbacktotheoriginallength,suggestingtheexistenceofanattractor.
thegeneratedtextlength,i.e. atsomeiterationsthelengthdrasticallyincreasesordecreases. Itis
interestingtonotethatwhenthelengthdecreases,itreturnstotheoriginalvalueasbeforethefirst
discontinuity. Thissuggeststheexistenceofanattractorregardingthisspecificlength. Tobetter
understandthecauseofthesediscontinuities,Figure2showsexamplesofstoriesgeneratedbefore
andafterthosediscontinuities(fortheseednumberthreeinfigure11). Wecanseethatatgeneration
14themodelabruptlystartstogeneratemanyhashtags. Itgenerates283hashtags,comparedto12in
thepreviousgeneration. Atgeneration45,wecanseethattheoverallqualityofthetextdecreased
intogeneratingsolelyhashtagsandbriefdescriptions. Thisreductionintextqualityisreminiscent
ofcollapsingdynamicsobservediniterativechainsofLLMs,whereeachmodelwastrainedonthe
outputofapreviousone[65].
21
htgneLTable2: ExamplesofstoriesdemonstratingthecollapseandrecoveryinMistral-7Bchainsonthe
Continuetaskfor(seed2fromfigure11
Gen. Story
13 Wearethrilledabouttheprogresswe’vemadeinmakingXiangqimoreaccessibleforall.
Let’skeeptheconversationgoingandexplorenewwaystoadaptthegamefordifferent
abilities. Shareyourideas,experiences,andsuccessstoriesusingthehashtags#CXAIn-
clusiveXiangqi#XiangqiForAll#AccessibleXiangqi#XiangqiUnited#DisabilityInclusion
#AdaptiveXiangqi#InclusiveGaming#AccessibleCommunity. Let’scontinuetolearnfrom
each other and create a vibrant, inclusive Xiangqi community that celebrates diversity
andwelcomeseveryone. #UnitedWeGame#AccessibleXiangqiJourney#TogetherWeCan
#XiangqiForEveryone
14 Let’sexchangeinnovativeideasonmodifyingXiangqipieces,boards,andrulestoaccom-
modatevariousdisabilities. #AdaptiveXiangqiDesigns#InclusiveXiangqiSolutions#To-
getherWeAdapt#XiangqiEmpowerment#AccessibleXiangqiProgress#DisabilityFriend-
lyXiangqi#XiangqiInclusiveCommunity#BreakingBarriers#XiangqiForAllPlayers#Ac-
cessibleXiangqiChampions#XiangqiInclusionSuccessStories#XiangqiUnitedForAll#Ac-
cessibleXiangqiFuture#InclusiveXiangqiVision
(omitted264hashtagsforclarity)
#XiangqiInclusiveGamingCommunityVision #XiangqiAccessibleGamingCommunity-
Growth#XiangqiAccessibleGamingCommunityInnovation#XiangqiAccessibleGaming-
CommunityEmpowerment#XiangqiAccessibleGamingCommunityPassion
45 #DesignWithInclusiveDesignPhilosophyScaling: Embracingdiversityandequalityinde-
signpractices.
#DesignWithUserCenteredDesignPhilosophyScaling:Puttingusersfirstindesigndecisions
andexperiences.
#DesignWithInclusiveTechnologyPhilosophyScaling: Makingtechnologyaccessibletoall
users,regardlessofabilities.
#DesignWithDigitalInclusionPhilosophyScaling: Ensuringeveryonehasequalaccessto
digitalresourcesandservices.
(omitted176linesforclarity).
#DesignWithUserTestingTrainingScaling: Scalingusertestingtrainingopportunities.
#DesignWithAssistiveTechnologyTrainingScaling:Expandingassistivetechnologytraining
opportunities.
#DesignWithInclusive
49 #DesignWithGlobalAccessibilityInitiativesScaling: Expandingglobalaccessibilityinitia-
tivesandcollaborations.
(omitted7linesforclarity)
#DesignWithInclusiveDesignTrendsScaling: Growingtrendsandinnovationsininclusive
designandaccessibility.
#DesignWithInclusiveDesignResourcesScaling: Expandingresourcesforinclusivedesign
andaccessibilityknowledgeandtools.
22(a) (b) (c)
Figure12: Empiricalvalidationofattractorspositionandstrengthestimation. Toempirical
verify that the method introduced in Section 3.4 makes accurate prediction, we used the first 10
generationsofeachchaintofitthelinearregressionbetweeninitialandfinalpropertyvalues(a).
Wethenusedourmethodtoestimateattractors’strengthandposition(b). Wethencomparedthose
predictionswiththeactualshiftsindistributionobservedafter50generations(c). Thegreyarea
representstheinitialdistributionofthecorrespondingproperty,andcoloredlineshowthedistribution
after50generationsforeachmodel. Crossesindicatetheestimatedpositionoftheoreticalattractors,
andtheirsizerepresentitsstrength. Forthefourthrow,secondcolumn,oneattractorwasoutsidethe
rangeofrepresentedvaluesandisthusrepresentedwith"->X".
Validationofattractorspositionandstrengthestimation ThemethodintroducedinSection3.4
givesthepositionandstrengthofatheoreticalattractor(ortheoreticalfixedpoint).Inordertovalidate
ourmethod,weverifiedthatthistheoreticalpredictionmatchestheactualdata. Todoso,weused
the first 10 generations of each simulated chain to predict the strength and position of attractors
foreachtask,modelandproperty. Wethencomparedthispredictionwiththeactualpropertiesof
textsobtainedafter50generations. AsshowninFigure12, transmissionchainsshiftstheinitial
distributionofvaluesinthedirectionofthepredictedattractor. Moreover,thevarianceofthefinal
distributionappearstoreflectthepredictedstrengthoftheattractor. Theseresultsconfirmedthatthe
methodweintroduceisindeedsuitedforestimatingthestrengthofpositionofattractors.
Evolutionoftextpropertiesforallinitialstories Wehereprovidethefiguresrepresentingthe
evolutionofeachofthefourmetricforeachmodel,foreachofthe20initialstories.Linesrepresented
theaverageover5seeds,andshadedareasrepresentthestandarderrors.
23Evolution of metrics over generations
Rephrase Inspiration Continue
000 ... 000 000 456 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.003
0.002
0.001
0.8
0.6
0.4
0.2
0.0
30
25
20
15
10
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure13: EvolutionoftextpropertiesstartingwithInitialText1
Evolution of metrics over generations
Rephrase Inspiration Continue
00 .. 00 01 80 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.006
0.004
0.002
0.8
0.6
0.4
0.2
0.0
0.2
25
20
15
10
2000
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure14: EvolutionoftextpropertiesstartingwithInitialText2
Evolution of metrics over generations
Rephrase Inspiration Continue
0.0020 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.0015
0.0010
0.75
0.50
0.25
0.00
0.25
27.5
25.0
22.5
20.0
17.5
15.0
2000
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure15: EvolutionoftextpropertiesstartingwithInitialText3
24
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
000 ... 000 001 680 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.004
0.002
0.8
0.6
0.4
0.2
0.0
0.2
30
25
20
15
6000
5000
4000
3000
2000
1000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure16: EvolutionoftextpropertiesstartingwithInitialText4
Evolution of metrics over generations
Rephrase Inspiration Continue
00 .. 00 00 11 24 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.0010
0.0008
0.0006
0.8
0.6
0.4
0.2
0.0
35
30
25
20
15
2000
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure17: EvolutionoftextpropertiesstartingwithInitialText5
Evolution of metrics over generations
Rephrase Inspiration Continue
00 .. 00 34 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.02
0.01
0.5
0.0
0.5
25
20
15
10
10000
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure18: EvolutionoftextpropertiesstartingwithInitialText6
25
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
00 .. 00 68 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.04
0.02
0.5
0.0
0.5
35
30
25
20
15
10
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure19: EvolutionoftextpropertiesstartingwithInitialText7
Evolution of metrics over generations
Rephrase Inspiration Continue
0.15 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.10
0.05
0.5
0.0
0.5
80
60
40
20
12000
10000
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure20: EvolutionoftextpropertiesstartingwithInitialText8
Evolution of metrics over generations
Rephrase Inspiration Continue
000 ... 000 223 050 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.015
0.010
0.005
0.8
0.6
0.4
0.2
0.0
40
35
30
25
20
15
3500
3000
2500
2000
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure21: EvolutionoftextpropertiesstartingwithInitialText9
26
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
00 .. 22 05 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.15
0.10
0.05
0.75
0.50
0.25
0.00
0.25
0.50
35
30
25
20
15
10
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure22: EvolutionoftextpropertiesstartingwithInitialText10
Evolution of metrics over generations
Rephrase Inspiration Continue
000 ... 122 505 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.10
0.05
0.5
0.0
0.5
30
25
20
15
10
10000
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure23: EvolutionoftextpropertiesstartingwithInitialText11
Evolution of metrics over generations
Rephrase Inspiration Continue
00 .. 12 50 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.10
0.05
0.5
0.0
0.5
40
35
30 25
20
15
10
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure24: EvolutionoftextpropertiesstartingwithInitialText12
27
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
0.3 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.2
0.1
0.5
0.0
0.5
50
40
30
20
10
12000
10000
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure25: EvolutionoftextpropertiesstartingwithInitialText13
Evolution of metrics over generations
00 .. 12 50 Re gMMMMpp eieis xth t t- t ta a3 r rr - -a a.a L L5 l ll l-s --a a7 8tMe m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t Inspiration Continue
0.10
0.05
0.75
0.50
0.25
0.00
0.25
0.50
40
30
20
10
4000
3000
2000
1000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure26: EvolutionoftextpropertiesstartingwithInitialText14
Evolution of metrics over generations
000 ... 000 456 Re gMMMMpp eieis xth t t- t ta a3 r rr - -a a.a L L5 l ll l-s --a a7 8tMe m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t Inspiration Continue
0.03
0.02
0.01
0.75
0.50
0.25
0.00
0.25
0.50
30
25
20
15
10
2000
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure27: EvolutionoftextpropertiesstartingwithInitialText15
28
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
00 .. 00 46 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.02
0.75
0.50
0.25
0.00
0.25
100
80
60
40
20
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure28: EvolutionoftextpropertiesstartingwithInitialText16
Evolution of metrics over generations
Rephrase Inspiration Continue
0.15 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.10
0.05
0.5
0.0
0.5
40
30
20
10
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure29: EvolutionoftextpropertiesstartingwithInitialText17
Evolution of metrics over generations
Rephrase Inspiration Continue
000 ... 456 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.3
0.2
0.1
0.5
0.0
0.5
30
25
20
15
10
4000
3000
2000
1000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure30: EvolutionoftextpropertiesstartingwithInitialText18
29
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneLEvolution of metrics over generations
Rephrase Inspiration Continue
00 .. 68 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.4
0.2
0.5
0.0
0.5
40
30
20
10
8000
6000
4000
2000
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure31: EvolutionoftextpropertiesstartingwithInitialText19
Evolution of metrics over generations
Rephrase Inspiration Continue
00 .. 68 gMMMMpeieis xtt t- t ta a3 r r- -a a.L L5 l ll l- --a a7 8tM m mu B xro 7-a ab I Bd n- -o3 3-se- I- -t n0l 8 7r s1B 0u t2 Bc- rI5 t u-n-Icvns tt0s-r t. vu2r0uc .t c1t
0.4
0.2
0.5
0.0
0.5
40
30
20
10
1500
1000
500
0 9 19 29 39 49 0 9 19 29 39 49 0 9 19 29 39 49
Generation Generation Generation
Figure32: EvolutionoftextpropertiesstartingwithInitialText20
30
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL
erocs
yticixoT
erocs
ytivitisoP
erocs
ytluciffiD
erocs
htgneL