Stepping on the Edge:
Curvature Aware Learning Rate Tuners
VincentRoulet* AtishAgarwala* Jean-BastienGrill GrzegorzSwirszcz
GoogleDeepMind GoogleDeepMind GoogleDeepMind GoogleDeepMind
vroulet@google.com thetish@google.com jbgrill@google.com swirszcz@google.com
MathieuBlondel FabianPedregosa
GoogleDeepMind GoogleDeepMind
mblondel@google.com pedregosa@google.com
Abstract
Curvatureinformation–particularly, thelargesteigenvalueofthelossHessian,
knownasthesharpness–oftenformsthebasisforlearningratetuners. However,
recentworkhasshownthatthecurvatureinformationundergoescomplexdynamics
duringtraining,goingfromaphaseofincreasingsharpnesstoeventualstabilization.
We analyze the closed-loop feedback effect between learning rate tuning and
curvature.Wefindthatclassicallearningratetunersmayyieldgreaterone-steploss
reduction,yettheyultimatelyunderperforminthelongtermwhencomparedto
constantlearningratesinthefullbatchregime.Thesemodelsbreakthestabilization
ofthesharpness,whichweexplainusingasimplifiedmodelofthejointdynamics
of the learning rate and the curvature. To further investigate these effects, we
introduceanewlearningratetuningmethod,CurvatureDynamicsAwareTuning
(CDAT), which prioritizes long term curvature stabilization over instantaneous
progressontheobjective. Inthefullbatchregime,CDATshowsbehaviorakin
toprefixedwarm-upschedulesondeeplearningobjectives,outperformingtuned
constantlearningrates. Intheminibatchregime, weobservethatstochasticity
introducesconfoundingeffectsthatexplaintheprevioussuccessofsomelearning
rate tuners at appropriate batch sizes. Our findings highlight the critical role
of understanding the joint dynamics of the learning rate and curvature, beyond
greedyminimization,todiagnosefailuresanddesigneffectiveadaptivelearning
ratetuners.
1 Introduction
Thelearningrate,a.k.a.stepsize,isthemainhyperparametercontrollingtheefficiencyandstability
ofgradient-basedtrainingofdeepneuralnetworks. Thelearningrateistypicallyadjustedthrougha
predeterminedschedule–oftenconsistingofawarm-upphase,wherethelearningrateisgradually
increasedtoapeak,followedbyanannealingphase,whereitisdecreasedtozero[Loshchilovand
Hutter,2016,Goyaletal.,2017]. Tuningtheshapeoftheschedule(warm-uptime,peaklearning
rate,decayscaleandshape)isessentialforgoodperformance. Despiterecenteffortstounderstand
theireffectiveness,theoptimalshapeoftheseschedulesremainsanareaofactiveresearch[Liuetal.,
2019,Shietal.,2020]. Thecostoftuningthesescheduleshasledtointerestinautomaticselectionof
thesehyperparameterswithlearningratetuners-methodswhichaimtoautomaticallyadjustthe
learningratethroughtraining.
Thesemethodshaverootsintraditionaloptimizationtheory,includinginexactlinesearchwithArmijo-
Goldsteincriterion[Armijo,1966,NocedalandWright,1999]andPolyakstepsizes[Polyak,1964],
Preprint.Underreview.
4202
luJ
8
]GL.sc[
1v38160.7042:viXrawhichselectthelearningrateviaestimatesofthegaptooptimalityoftheobjective. TheArmijo-
Goldsteincriterionisacrucialcomponentofpopularfull-batchconvexoptimizers,suchasL-BFGS
[LiuandNocedal,1989]. Recenteffortshaveadaptedlinesearchestostochasticoptimization,with
somepartialempiricalsuccessesandwithsomeapproachesofferingconvergenceguarantees[Vaswani
etal.,2019,Gallietal.,2023,MutschlerandZell,2020]. SimilareffortshavebeenmadeforPolyak
stepsizes [Loizouetal.,2021,Berradaetal.,2020], inadditiontonewmethodswhichcombine
distancetooptimalitywithonlinelearningconvergencebounds [DefazioandMishchenko,2023,
Cutkoskyetal.,2023,MishchenkoandDefazio,2023,Ivgietal.,2023].
Classically-inspiredmethods,however,havegenerallystruggledtogaintractionindeeplearning.
Thisispartlyduetotheirdesign, whichprioritizesconvex, Lipschitz-continuous, and/orsmooth
(Lipschitz-continuousgradients)objectives. Incontrast,thelosslandscapeofdeepnetworksisknown
tobenon-convex[Lietal.,2018],andnon-Lipschitzcontinuous[Hochreiteretal.,2001]. Moreover,
non-linearmodels,especiallyneuralnetworks,willcommonlyundergodramaticchangesingeometry
duringtraining[Jastrze˛bskietal.,2018,Wuetal.,2018,KopitkovandIndelman,2020,Jastrzebski
et al., 2020]. In particular, most models undergo a phase of progressive sharpening - where the
sharpness,thelargesteigenvalueoftheHessian,increasesduringtraining[Cohenetal.,2021]. These
potentiallydetrimentaleffectsaremitigatedbynon-linearstabilizationarisingfromthediscreteness
ofthedynamics–namely,theedgeofstability(EOS)phenomenon[Cohenetal.,2021]. Thiscauses
largeHessianeigenvaluestostabilizeatthecriticalvalueforagivenlearningrateinanequivalent
smoothsetting(forexample,maxHessianeigenvaluestabilizesatλ =2/ηforlearningrateη)
max
Cohenetal.[2021,2022]. Gilmeretal.[2021]consideredEOSstabilizationasaleadingcandidate
for the necessity of the warm-up procedure; as the learning rate η increases, λ is effectively
max
annealed.
Thisraisessomenaturalquestions. Howdothesesharpnessdynamicsaffecttheperformanceof
learningratetuners? Whatinsightscanwegaintodesignbettertunersfordeeplearning? Ourwork
takesafirststepatansweringthesequestions,startingwithastudyofsomeclassicallearningrate
tuners: alinesearchensuringsufficientdecreaseandanapproximatelygreedymethodthatminimizes
aquadraticapproximationoftheobjective. Specifically,wefindthefollowing.
• Weempiricallyobservethatclassicallearningratetunersqualitativelyunderperformtheirconstant
learningratecounterpartsacrossseveraldeeplearningbenchmarks,inthefullbatchregime,for
whichthesemethodswereoriginallydesigned.
• Ourempiricalanalysisofcurvaturedynamicsrevealsthatclassicallearningratetunersgenerally
undershoottheedgeofstability. Thisundershootingcreatesasnowballeffectofever-increasing
sharpnessandever-decreasinglearningrates.
• Weproposeatheoreticalmodelthateffectivelycapturestheseempiricallyobservedfailures.
Ouranalysissuggeststhatstabilizingthesharpnessmaybeamoreimportantgoalforthelong-term
successoftraining,comparedtogreedilyoptimizingtheobjective. Toexplorethisidea,wepropose
theCurvatureDynamicsAwareTuning(CDAT)method,whichdynamicallydrivesthelearningrate
totheEOS.Inourexploration,wefindthefollowing.
• Weobserveempiricallythattheproposedlearningratetunercanoutperformfine-tunedconstant
learningratecounterpartsinafullbatchregime.
• We analyze the sharpness dynamics induced by CDAT in these examples and observe that the
progressivesharpeningismitigatedbythetuner,increasinglearningratesatearlytimesbefore
stabilizing,akintoanautomaticwarm-upschedule.
• WeproposeatheoreticalmodelthatclarifiesthedynamicalmechanismsbywhichCDATmaintains
proximitytotheEOS,whilehighlightingthelimitationsofexistingmodelsofcurvaturedynamics.
Our work suggests that the design of learning rate tuners benefits from exploiting curvature sta-
bilization rather than focusing on loss decrease. The introduction of simple learning rate tuners
canalsorefineourunderstandingofsharpnessdynamicsthroughfeedbackloopeffects. Additional
experimentsandexperimentaldetailsarepresentedinAppendixBandAppendixCrespectively.
2ResNet34 with squared loss MixerTi/8 with squared loss NanoLM with cross-entropy loss ViTTi/32 with cross-entropy loss
on subset of Cifar10 on subset of Cifar10 on subset of Tiny Shakespeare on subset of Imagenet
0.8 0.5 8
5
0.6 0.4 4 6
0.3
0.4 3 4 0.2 2
0.2 2
0.1 1
0
0.0 0.0 0
0 5000 10000 15000 0 2000 4000 6000 8000 0 500 1000 1500 2000 0 500 1000 1500 2000
Epoch Epoch Epoch Epoch
GD Linesearch GD Quad. Greedy GD RMSProp Linesearch RMSProp Quad. Greedy RMSProp
Figure1: Simplelearningratestunersqualitativelyunderperformtheirconstantlearningrate
counterparts. GradientdescentorRMSPropwithatunedconstantlearningrateversusself-tuned
gradientdescentbyalinesearchmethod(1),oraquadraticallygreedyrule(3)onvariousdatasets,
architecturesandlossesinafullbatchregime. Thelinesearchmayperformbetteratearlytimesbut
stallsinthelongterm.
2 TheInterplayBetweenLearningRateTunersandCurvatureDynamics
Aleitmotifinthedesignoflearningratetunershasbeentoselectthelearningratetoensureamaximal
orsufficientdecreaseoftheobjectiveateachiteration. Wefocushereontwocanonicalexamples.
Polyakstepsizesandhyper-gradientdescentarealsobrieflyexaminedinAppendixB, Fig.13.
2.1 Canonicallearningratetunersfailuresindeeplearning
Thefirstclassicalapproachweconsiderisalinesearch(ls)methodthatselectsthelearningrateη
suchthattheobjectivef satisfiesacertaindecreasecriterion[Armijo,1966,NocedalandWright,
1999]. Formally, given current parameters w and an update direction u , the learning rate ηls is
t t t
chosensuchthat
f(w +ηlsu )≤f(w )+cηlsu⊤∇f(w ). (1)
t t t t t t t
Thisruleassumesthatu isadescentdirection(∇f(w )⊤u <0),whichensurestheexistenceof
t t t
alearningratesatisfying(1). ThisholdstrueforsimpleGradientDescent(GD)orpreconditioned
variantslikeRMSProp[Hintonetal.,2012]. Inthecriterion(1),cisusuallyasmallconstantsetto
10−4or0. Avalidlearningrateissearchedwithausualbacktrackinglinesearch(AppendixC).
Thesecondmethodweconsiderinvolvesselectingthelearningrateateachiterationtominimize
a quadratic approximation of the objective. Formally, the objective f at parameters w can be
t
approximatedalonganupdatedirectionu byaquadraticapproximationq as
t f
1
f(w +ηu )≈q (η;w ,u ):=f(w )+η∇f(w )⊤u + η2u⊤∇2f(w )u . (2)
t t f t t t t t 2 t t t
Provided that this quadratic approximation is strongly Linear model with squared loss
convex in η (u⊤∇2f(w )u > 0), the minimum of the on subset of Cifar10
t t t 0.500
quadraticapproximationq (η;w ,u )isreachedforthe
f t t GD
quadraticallygreedy(qg)learningrateηqggivenby 0.475
Linesearch GD
0.450 Quad. Greedy GD
−∇f(w )⊤u
ηqg = t t . (3) 0.425
t u⊤ t ∇2f(w t)u t 0.400
0.375
Settingthelearningratebyminimizingthequadraticap-
proximations(3)isasimpleintuitiveideastudiedforex- 0.350
amplebySchauletal.[2013],MartensandGrosse[2015,
0 2000 4000 6000 8000
Section 6.4]. This approach as well as linesearches are Epoch
effectiveonsimplelinearproblems(Fig.2). Whiletheir
Figure2: Classicallearningratetuners
rationaleoriginatesinnon-stochasticoptimization,they
canbeeffectiveonlinearmodels.
havebeenanalyzedinthecontextofstochasticoptimiza-
tionfordeeplearning[Vaswanietal.,2019,Schauletal.,
2013].
3
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarTResNet34 with squared loss on subset of Cifar10
100 4
101 103 3 100
102 102 2
103 101 1
101
0
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch Epoch
GD Linesearch GD Quad. Greedy GD
Figure 3: Classical learning rate tuners can undershoot the edge of stability. Learning rate,
sharpness,theirproduct,andthegradientnormevolutionofaconstantlearningrateandlearningrate
tuners,fullbatchgradientdescent. Learningratedecreasesby3ordersofmagnitudefortuners(1st
panel)whilesharpnessincreases(2ndpanel). Theirproductremainsrelativelysteady,justbelowthe
edgeofstability(3rdpanel). Thegradientnormincreasesbylessthanafactorof10,consistentwith
slowtrainingatlatetimes(4thpanel).
2.2 Analyzinglearningratetunersthroughcurvaturedynamics
Fullbatchregime. WerevisittheperformanceofthelearningtunerspresentedinSection2.1inthe
fullbatchregimeondeeplearningproblemsinFig.1. AsdemonstratedinFig.1,alinesearch(1)or
thequadraticallygreedyrule(3)qualitativelyunderperformtheirconstantlearningratecounterpart
inthedeeplearningbenchmarksconsidered. Notably,alltheseresultsareobtaineddespitebeingin
afullbatchregime,forwhichthesemethodsareoriginallydesigned. Tounderstandthefailuresof
theseapproaches,weconsiderseveralmeasurespresentedinFig.3(seealsoFig.12).
First,weobserveaconsistentdecreaseinthechosenlearningrateovertime,spanningseveralorders
ofmagnitude(1stpanelofFig.3). Thisissurprising,asnoneoftheseapproachesexplicitlyencodea
decreasinglearningratemechanism. Specifically,thelinesearchalwaysinitiatesitssearchwitha
guesslargerthanthepreviouslyselectedlearningrate(seeAppendixCforimplementationdetails).
Decreasinglearningratesaretheoreticallyoptimalfornon-smoothobjectives[Nesterovetal.,2018],
suchastheonesinducedbyusingtheReLUactivation;howeverinourexample,thegradientnorm
doesnotincreasebeyondoneorderofmagnitude(4th panelofFig.3). Thissuggestsboththatan
increaseingradientnormisnottheprimarycauseoflearningratedecrease,andalsoexplainswhy
thelearningratedecreaseiscorrelatedwithslowerprogressonthetrainingloss.
FollowingtheworkofCohenetal.[2021],weanalyzethedynamicsofthesharpness,thatisthe
largesteigenvalueoftheHessian,λ (∇2f(w )). Inthe2ndpanelofFig.3,weobservethatwhile
max t
sharpnessstabilizesforgradientdescent,itdoesnotexhibitthesamebehaviorfortheconsidered
learningratetuners. Byplottingtheproductofthelearningrateη andthesharpness(3rd panelof
t
Fig.3),wefindthatthisproductcanexceedthestabilitythresholdof2,eventuallystabilizingbelow
thisthresholdforconstantlearningrategradientdescent. Incontrast,forthelearningratetuners,this
productneithersurpassesthestabilitythresholdnorstabilizesaround2inthelongrun. Therefore,
theseclassicallearningratetunersdonotoperateattheedgeofstability.
Fromatheoreticalperspective,objectivesaretypicallyclassifiedaseithersmoothornon-smooth.
SmoothobjectiveshavegradientsthatareLipschitz-continuous,atleastlocallyaroundanypoint.
Non-smoothobjectives,ontheotherhand,maycontainpointswithkinks(non-differentiablepoints).
However,thistaxonomymightnotfullycapturethecurvaturedynamicsobservedbyCohenetal.
[2021, 2022] for constant learning rates, and in Fig. 1 for the classical learning rate tuners. In
particular,theconceptofsmoothnessmightnotbeentirelyrelevantinthecontextofdeeplearning,
whereitslocalestimate(thespectralnormoftheHessian,alsoknownassharpness)cancontinueto
increasethroughouttraining. Topushthelimitsofclassicalsmoothnessassumptions,weconsider
inSection3alearningratetunerthatpropelstheoptimizerattheedgeofstabilityorabove,aregime
thatusualsmoothnessassumptionswouldtheoreticallyprohibit.
Mini-batch regime. The results presented in Fig. 1 in the full batch regime do not contradict
previous results from Vaswani et al. [2019], who studied linesearches for deep learning in the
stochasticregime,asillustratedinFig.14,andpreviouslyreportedbyRouletetal.[2023]. Wesimply
point out that the success of linesearches observed by Vaswani et al. [2019] may not be entirely
attributabletothemethod’soriginalrationale.
4
etaR
gninraeL
ssenprahS
naisseH
ssenprahS
X etaR
gninraeL
mroN
tneidarG1.0 η=1 0.50
η=1.9/λmax 0.25
0.8 15
0.00
0.6 η=1 0.25
10
η=1.9/λmax 0.50
0.4
5 0.75
η=1
0.2 1.00 η=1.9/λmax
0
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100
Steps Steps Steps
Figure4: Thepoorperformanceofclassicallearningratetuners,understoodinasimplified
model. The dynamics of learning rate η, sharpness λ , and normalized centered sharpness
max
y =ηλ −2areexaminedinthesimplifiedmodel(4). Withaconstantη,λ stabilizesandy
max max
oscillatesaround0(blue). Classicallearningratetunersoftenquicklyequilibratearoundy =−ϵ,
t
which we model using η = 1.9λ (orange). This equilibration of y away from zero prevents
max
stabilizationinλ ,leadingtoanincreaseinλ ,andacorrespondingdecreaseinη.
max max
Theactualsuccessoflinesearchesinastochasticregimemayinsteadbeexplainedbytheattenuated
progressive sharpening observed in such a regime [Jastrzkebski et al., 2018, Cohen et al., 2021,
AgarwalaandPennington,2024]. Moreover,linesearchesappliedtomini-batchestendtoselectlarger
learningratesthantheywouldinafull-batchregime[MutschlerandZell,2020]potentiallyallowing
themtoavoidundershootingthefullobjective’sedgeofstability.
2.3 Theoreticalanalysis
Thesharpeningeffectscanbeunderstoodtheoretically.Previousworkhasshownthatthestabilization
provided by EOS is due to non-linear interaction between the component of the gradient in the
largesteigendirection,andthedynamicsofthelargesteigenvaluesthemselves[Damianetal.,2022,
Agarwalaetal.,2022]. Wecanusetheseanalysestounderstandwhythereisnostabilizationfor
someclassicallearningratetuners.
WestartwiththemodelfromDamianetal.[2022],whichfocusesonthedynamicsinthelargest
eigendirection of the Hessian. Given an objective f parameterized by parameters w , let λ be
t t
the largest eigenvalue of the Hessian ∇2f(w ), i.e., λ := λ(w ) := λ (∇2f(w )). Let v be
t t t max t
itsnormalizedeigenvector;themodelassumessloweigenvectorchange,soitistreatedasafixed
direction. Thejointdynamicsofλ andtheprojectionx :=v⊤w canthenbewrittenas
t t t
x =(1−η λ )x , λ =η (a−bx2)+λ . (4)
t+1 t t t t+1 t t t
Here, a := −∇λ(w)⊤∇f(w) corresponds to the instantaneous change of λ along the negative
gradient (the update direction), and b := ∥∇λ(w)∥2 encodes the non-linear negative feedback
betweenx andλ .Bothaandbareconsideredconstantalongiterations.Theseequationsarederived
t t
byDamianetal.[2022]usingaTaylorexpansionoftheiteratescombinedwithacouplingargument.
Intheoriginalmodel,thelearningrateη isalsofixedtoη. Thisleadstothefollowingdynamics:
t
whileηλ <2,themagnitudeofx decreases. This,inturn,leadstoanincreaseinλ . Eventually,
t t t
ηλ >2and|x |increases. Thiseventuallyleadstothebx2termbecominglarge,whichdecreases
t t t
λ . Thereisarangeoflearningratesoverwhichthisdynamicleadstoquasi-stableoscillationsofλ
t t
aroundtheedgeofstabilityvalue2/η(Fig.4,bluecurves).
When using a learning rate tuner, η is also a dynamical variable. This introduces the additional
t
complicationofashiftingedgeofstability. Therefore,itisadvantageoustoanalyzethedynamical
systemusingnormalizedvariables[Agarwalaetal.,2022]. Wedefiney :=η λ −2,wherey =0
t t t
correspondstotheEOS,andp :=x2. Thisgivesusthedynamicalequations(AppendixA.1)
t t
(cid:18) (cid:19) (cid:20) (cid:21)
η η
p =(1+y )2p , y =η [η (a−bp )]+ t+1 y +2 t+1 −1 . (5)
t+1 t t t+1 t+1 t t η t η
t t
Wemustthensupplyaruleforη . InFig.3,weobservedthatinthefullbatchsetting,thelearning
t+1
ratemultipliedbythesharpnessappearstoquicklyapproachathresholdof2−ϵ(correspondingto
y =−ϵ),andthenvariesslowlybelowtheEOSthreshold. Wemodelthevaryinglearningrateas
η :=2(1−ϵ)/λ . (6)
t t
5
η
λ
xam
yResNet34 with squared loss on subset of Cifar10 NanoLM with cross entropy loss on subset of Tiny Shakespeare
0.4 4 100
100
0.3 3 102
101
104 0.2 2
102 106
0.1 1
103 108
0.0 0
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
GD GD CDAT Scale 2.0 RMSProp RMSProp CDAT Scale 2.0
GD CDAT Scale 1.0 GD CDAT Scale 2.06 RMSProp CDAT Scale 1.0 RMSProp CDAT Scale 2.06
GD CDAT Scale 1.94 GD CDAT Scale 2.5 RMSProp CDAT Scale 1.94 RMSProp CDAT Scale 2.5
MixerTi/8 with squared loss on subset of Cifar10 ViTTi/32 with cross entropy loss on subset of Imagenet with edge EMA
100
101
100 100
101 101 102 101
102
102
104
103
103
103
104 104 106
105
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
GD Mom GD Mom CDAT Scale 2.0 Adam Adam CDAT Scale 2.0
GD Mom CDAT Scale 1.0 GD Mom CDAT Scale 2.06 Adam CDAT Scale 1.0 Adam CDAT Scale 2.06
GD Mom CDAT Scale 1.94 Adam CDAT Scale 1.94 Adam CDAT Scale 2.5
Figure5: Enforcingoptimizerstostayonedge(σ = 2.0)improvesperformanceovergreedy
approximation (σ = 1.0). Train loss and learning rate behaviors for fine-tuned optimizers vs
self-tunedcounterpartswithCDATonvariousdatasets,architectures,lossesinafullbatchregime.
Tuningthelearningrate“onedge”(σ ≈ 2)improvesperformanceovergreedytuning(σ = 1)as
wellasconstantlearningrate.
Thismaintainsy =−ϵ. Notably,thisschedulewasexplicitlyproposedbyCohenetal.[2021](see
t
alsoFig.15). Inthisregime,p decreasesmonotonically,aligningwiththeoriginalgoalofthese
t
methodstodecreasetheloss(Fig.10). However,thiseliminatesfeedbackforcontrollingtheincrease
inλ ,resultinginsignificantprogressivesharpening(Fig.4,orangecurve).
t
Consequently, when attempting to enforce monotonicity, learning rate tuners may inadvertently
disruptthenon-linearstabilizationthatmakesgradientdescentrobustandeffectivefortrainingdeep
neuralnetworks. ContinuallyundershootingtheEOStriggersasnowballeffectofdecreasinglearning
rateandincreasingsharpness. Ifthereisnocorrespondingincreaseingradientnorms,thiscauses
optimizationtoslowdown.
ThepoorperformanceoftheclassicallearningratetunersinFig.1thereforeappearstronglycorrelated
withtheirtendencytoundershoottheedgeofstabilityinthenormalizedsharpnesscoordinatey. In
thefollowing,wefocusonunderstandingtunersthatprioritizetrainingatorneartheedgeofstability.
3 OptimizingontheEdgeofStability
BasedonourobservationsinSection2,wedesignlearningratetunersthatpositiontheunderlying
optimizerontheedgeofstability(y = 0). Weanalyzeatunercapableofoperatingbothslightly
belowandslightlyabovetheEOSinordertoexploitnonlinearstabilization.
Formally, we investigate a generalization of the quadratically greedy rule from Section 2, which
soughtη tominimizethequadraticapproximationq in(2). Weinsteadchoosethelearningrateto
t f
beonedgebyseekingthelargestvalueofηsuchthatq issmallerorequaltotheoriginalvalueoff,
f
∇f(w )⊤u
ηoe :=max{η ≥0:q (η;w ,u )≤f(w )}=−2 t t , (7)
t f t t t u⊤∇2f(w )u
t t t
wherethelastformulaholdsprovidedthatu⊤∇2f(w )u >0(convexquadratic)and∇f(w )⊤u <
t t t t t
0(u isadescentdirection). Foru =−∇f(w ),andif−∇f(w )isalignedwiththeeigendirection
t t t t
v associatedwiththelargesteigenvalueλ ofH,werecoverthefamiliarηoe =2/λ . Note
max max t max
however, that contrarily to using directly η = 2/λ , the on-edge rule can naturally take into
t max
accountthealignmentwithv (seeFig.15).
max
6
ssoL
niarT
ssoL
niarT
etaR
gninraeL
etaR
gninraeL
ssoL
niarT
ssoL
niarT
etaR
gninraeL
etaR
gninraeLResNet34 with squared loss on subset of Cifar10
6
103 5 100
4
102 3 101
2
101
1 102
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch
GD GD CDAT Scale 1.94 GD CDAT Scale 2.06
GD CDAT Scale 1.0 GD CDAT Scale 2.0
Figure6: Optimizingonedgeinducesdifferentcurvaturedynamics. Sharpness,productbetween
learningrateandsharpness,andgradientnormevolutionsforgradientdescentwithCDAT.Byputting
thelearningrateonedge(σ ≈2),thesharpnessdoesnoteverincreaseandactuallydecreasesslightly
overtime. GDwithCDAToperatesslightlyabovetheedgeconstantlyduringtraining. Itsgradient
normevolutionisakintoafine-tunedconstantlearningratebaseline.
Wenotethattheonlydifferencebetweenthisandthequadraticallygreedyruleisafactorof2inthe
numerator. Inspiredbythisobservation,andwithaneyetowardsrobustness,wedefineourCurvature
DynamicsAwareTuning(CDAT)ruleby:
n
ηcdat =σ t, forn =max{−∇f(w )⊤u ,0}, d =|u⊤∇2f(w )u |+ε. (8)
t d t t t t t t t
t
Thescalingfactorσletsusinterpolatebetweengreedy(σ =1)andon-edge(σ =2). Wearemost
interestedinthebehaviornearσ = 2. In(8),themaxfunctiontakescareofthecasewhereu is
t
anascentdirection(∇f(w )⊤u > 0),theabsolutevaluetakescareofcaseswheretheobjective
t t
hasnegativecurvatureintheupdatedirections(seeAppendixCforadditionaljustification), and
wesimplysetε = 0aswealwaysobservednon-negligiblepositivecurvature. Thedefinitionsof
thenumeratorn andthedenominatord allowforthepossibilityofexponentialmovingaverages
t t
(EMA)ofeachquantitysuchasn˜ = (1−β )n +β n˜ forβ referredtoastheCDAT
t+1 cdat t cdat t cdat
EMAparameterthereafter. Weobservedthatsmoothingtheestimatesofn andd byanEMAis
t t
particularlyrelevantwhentheupdatesarethemselvesdefinedthroughanexponentialmovingaverage
asinAdam,orwhenusingtheproposedruleinastochasticsetting.
CDAThastwomajoradvantages: itissensitivetoinformationfromalleigenvaluesof∇2f(w ),and
t
itdependsonupdatesu comingfromanybaseoptimizer. Wewilltakeadvantageoftheseproperties
t
toexplorethebehaviorof“onedge”optimizationinavarietyofsettings.
3.1 Onedgeoptimizersinpractice
Full batch regime. Fig. 5 presents results for training with CDAT across various optimizers,
architectures,datasets,andlosses. Overall,selectingthelearningratetobeonedge(σ =2)ison
parwithorbetterthanafine-tunedconstantlearningrateandisalwaysbetterthanaquadratically
greedyapproach(σ =1). Thisobservationholdseventhoughthequadraticallygreedyruleensures
largerinstantaneousdecrease(Fig.16). Onenotesthattargetingslightlyabovetheedge(σ =2.0625)
providesevenbetterperformancethantheonedgerule(σ = 2)onallexamplesexcepttheMLP
MixeronCIFAR10. However,targetinghigherabovetheedge(σ =2.5)generallygivesdiverging
resultsintheshortorlongterms. Remarkably,allchoicesaroundtheedge(1.9375,2.0,20625)show
a progressive increase of the learning rate that results generally in a better performance than the
constantlearningratecounterparts,exceptforRMSPropontheNanoLMexperiment. Theincreasing
learningratebehaviorisakintothewarm-upphasegenerallyhard-codedbyascheduler. Tointegrate
theproposedrulewiththeAdamoptimizer,weobservedthattheestimationofthecurvaturesthrough
n ,d in(8)wasnecessary. InFig.19,weobservethattheCDATruledisplayssimilarbehavioras
t t
warm-upschedules,yetitmaynotfullycapturethebenefitsofprefixedschedules.
InFig.6,weanalyzethedynamicsofthecurvaturewhenoptimizingonedge. Weobservethatthe
sharpness can be pushed to reduce over the iterations (1st panel of Fig. 6). The CDAT rule may
operate constantly slightly above the edge (2nd panel of Fig. 6). By reducing the sharpness, the
algorithmmaybeabletotakelargerstepsizesandconvergefaster. Sensitivitytoarchitecure’swidth
anddepth,aswellasweightdecay,arealsoanalyzedinFig.18.
7
ssenprahS
naisseH
ssenprahS
X etaR
gninraeL
mroN
tneidarGResNet50 with squared loss SGD Mom on Cifar10
CDAT EMA : 0.0 CDAT EMA : 0.9 CDAT EMA : 0.99
2.6 2.6 2.6 1e+00
2.4 2.4 2.4
2.2 2.2 2.2 1e-01
2.0 2.0 2.0
1.8 1.8 1.8 1e-02
1.6 1.6 1.6
1.4 1.4 1.4
1.2 1.2 1.2 1e-03 1.0 1.0 1.0
0.8 0.8 0.8 1e-04
0.6 0.6 0.6
0.4 0.4 0.4 1e-05
32 64 128 256 512 1024 32 64 128 256 512 1024 32 64 128 256 512 1024
Batch Size Batch Size Batch Size
Figure7: Stochasticityshiftstheoptimalscaling. Normalizedperformanceofgradientdescentwith
momentumequippedwithCDATinastochasticregimewithvaryingbatchsizes. Inamini-batch
regime,theoptimalscaledecreasesasthebatchsizedecreases. Usinganexponentialmovingaverage
smoothsouttheperformanceoftheCDATruleoverbatchsizes.
ResNet50 with squared loss on Cifar10
Batch size: 256
0.4
100
0.8
0.3 101
0.2 0.6 102
0.1 0.4 103
104
0.0 0.2
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
GD Mom Scheduled GD Mom GD Mom On Edge
ViTS/16 with cross-entropy loss on Imagenet
Batch size: 256
1.0
6 101
0.8
4 103
0.6
2 105
0.4
0 25 50 75 100 125 0 25 50 75 100 125 0 25 50 75 100 125
Epoch Epoch Epoch
Adam Scheduled Adam Adam On Edge
Figure8: TheperformanceofCDATissubduedinthestochasticregime. Fine-tunedconstant,
scheduled,andself-tunedwithCDATlearningratesinastochasticregime. Inastochasticregime,
CDATcanalsoexhibitaformoflearningratewarm-up(topfigure). However,theinterplaybetween
sharpeningandlearningrateareknowntobemitigatedinastochasticregimewhichmayexplainthe
underperformanceofCDATinthisregime(bottomfigure).
Minibatchregime. TheCDATrulecanbeusedinastochasticregimebyreplacingf in(8)byits
stochasticcounterpartf(mt)onamini-batchm t. However,twodifficultiesmayarise.
First, the on edge rule is motivated by the sharpening effects of the overall objective, which can
beoverestimatedorunderestimatedbyasinglemini-batch. Asaresulttheoptimalscalingfactor
mayvarywiththemini-batch. InFig.7,weobservethattheoptimalscalingoftheon-edgeruleis
proportionaltothebatchsizeuptosomesize. Inparticular,atspecificbatchsizes,weobservethatthe
greedyrule(σ =1)outperformstheon-edgerule.Thisresultisconsistentwiththegoodperformance
oflinesearchesorgreedyrulesinamini-batchregimepreviouslymentionedandobservedinFig.14.
WealsoobserveinFig.7,thatintegratinganEMAintotheestimationoftheedgein(8)smoothout
theselectionoftheoptimalscalingfactor.
Second,thesharpeningeffectsareknowntobegenerallymitigatedinthestochasticregime[Jas-
trzkebskietal.,2018,Cohenetal.,2021,AgarwalaandPennington,2024]. Thebenefitsoftheon
edgeruleappearalsosubduedinthisregime(Fig.8,Fig.20,Fig.21).
8
rotcaF
gnilacS
ssoL
niarT
ssoL
niarT
rotcaF
gnilacS
rorrE
tseT
rorrE
tseT
rotcaF
gnilacS
etaR
gninraeL
etaR
gninraeL
ssol
niart
laitinI/ssol
niart
dne
.gvA0.070 3.8×101 0.5
σ=1.9 σ=1.9
σ=2.0 3.7×101 0.4 σ=2.0
0.065 3.6×101
0.3
3.5×101
0.060 3.4×101 σ=1.9 0.2 σ=2.0 0.1
3.3×101
0.055 0.0
3.2×101
0.1
3.1×101
0.050 0.2
0 101 102 0 200 400 600 0 101 102
Steps Steps Steps
Figure9: AsimplemodelpartiallycapturesthebenefitsinducedbytheproposedCDATrule.
DynamicsoftheoreticalmodelofCDAT(10). Forσ = 2,feedbackstabilizesy closetotheEOS
(y =0),whichstabilizesλ (orange). Forσ =2−ϵandsmallϵ(blue,ϵ=0.1),modelpredicts
max
thatλ slowlygrows(middle),butpredictsthatystabilizestoavalue−ϵ≪y <0(right).
max t
3.2 ModelingCDATdynamics
The classical optimization framework is insufficient to fully explain the benefits of CDAT. For
example,onaconvexquadraticobjective,σ = 1istheoptimalchoice,andσ > 2results(inthe
worstcase)inadivergentalgorithm. However,wecanuseasimplifiedmodeltobeginunderstanding
thejointdynamicsofthelearningrateandsharpnessunderCDAT.
Weapproximatethegradientsaroundastationarypointw ,where∇f(w )=0,as∇f(w )≈Hw¯
⋆ ⋆ t t
forw¯ := w −w ,andH beingasymmetricmatrix. Inthisscenario,thelearningrategivenby
t t ⋆
CDATisηcdat =σ(w¯⊤H2w¯ )/(w¯⊤H3w¯ ). ConsiderthecasewhereH hastwoeigenvaluesλand
t t t t t
ν,withλ>ν ≥0. InthiscasetheCDATlearningratecanbewrittenas
λ2p +ν2g λ2p /g +ν2
ηcdat =σ t t =σ t t . (9)
t λ3p +ν3g λ3p /g +ν3
t t t t
Herep ,g aretheprojectionsp :=(w¯⊤v)2,g :=(w¯⊤v )2,respectivelyontotheeigendirections
t t t t t t ⊥
vandv associatedwithλ,ν. Therefore,ηcdatinterpolatesbetweenitsminimumvalueσ/λtothe
⊥ t
largervalueσ/ν,dependingonthealignmentratiop /g . For2ν/λ<σ <2,thisrulecanachieve
t t
learningratesbothaboveandbelowtheEOS.
We can gain additional insight by modeling a dynamical λ , extending the model of Section 2.3.
t
Whilemodel(5)capturesthedynamicsinthelargesteigendirectionv,hereweaimtomodelthe
dynamicsintheorthogonalsubspace. Tosimplify,weconsidertheeigendirectionsv,v ,andsmall
⊥
eigenvalueν fixed. Wethenmodelthegradientsas∇f(w )≈H w¯ withH =λ vvT+νv vT. If
t t t t t ⊥ ⊥
weupdatewinthedirectionv usinggradientdescentonνg ,weobtainthefollowingdynamical
⊥ t
systemdescribingtheCDATlearningratetuner:
λ2p +ν2g
η =σ t t t, g =(1−η ν )2g , p =(1+y )2p . (10)
t+1 λ3p +ν3g t+1 t t t t+1 t t
t t t
Combiningthiswiththeupdaterulefory givenin(5)completesthemodel.
t
Therearetwoimportantregimesofbehaviorinthismodel. First, ify > 0, p willincreaseand
t t
eventuallyy willdecreaseasinthenormalEOScase. Ify <0,thekeythresholdisy <−η ν .
t t t t t
Inthiscase,theratiop /g decreases-leadingtoanincreaseinη accordingtotheonedgerule. If
t t t
a−bp > 0(asitisifp hasbecomesmallduetoy < 0),thenweseefrom(5)thatthisleadsto
t t t
anincreaseiny . ThissuggeststhatCDAThasatendencytopushy closertotheEOS–sendingy
t t
towards0ifthelearningrateisdrivenbytheeigendirectionscorrespondingtosmallereigenvalues.
Numericalsimulationsonthismodel(Fig.9)suggestthatthiseffectcanindeedcauseremarkably
smallvaluesofy (3rd panelofFig.9). Weemphasizethatthisisduetothejointdynamicsofη
t
(inducedbythelearningratetuner),andλ ,p ,andg (inducedbyGD).Therearealsoimportant
t t t
limitationsinthismodel’sabilitytofullyexplainCDAT’sbehavior. Forexample,themodelpredicts
runawaysharpeningforσ <2(2ndpanelofFig.9),anddivergenceforσ >2. Inpractice,wesawa
rangeofstableandusefulsettingsforscalecenteredaround2. Thismodelinglimitationlikelystems
fromneglectingthedynamicsorthogonaltovaswellashigher-orderterms,whichempiricallytend
tostabilizeEOSdynamics[Agarwalaetal.,2022].
9
η λ xam y4 ConclusionandFutureDirections
Summary. Ourempiricalresultsshowedthatsimplelinesearchesandapproximategreedylearning
ratetunersunderperformconstantlearningrateapproachesinthefullbatchregime–despitebeing
betteronindividualsteps. Theideathat“locallygreedy”methodsperformpoorlyonlongtimescales
has been shown in other settings as well, including evolutionary dynamics Agarwala and Fisher
[2019]. Ourexperimentsandtheoreticalworksuggestthefailureoftheseclassicaltunersisdueto
thefactthattheysuppressthefeedbackwhichstabilizessharpnessinthefixedlearningratesetting.
Asthesharpnessincreases,tunersareforcedtotakesmallersteps,whichendsupleadingtoslower
learning.
Wefind,incontrast,thatprioritizingstabilityofthesharpnessyieldstangiblebenefits. OurCDAT
methodpushesthenetworktowardstheedgeofstabilityviaadynamicallydrivenprocess. Italso
naturallydisplayssomeformofprogressiveincreaseofthelearningrateakintoprefixedwarm-up
schedules. CDATalsoshedslightonthemorecomplicateddynamicsinsmallminibatchregime,
whereestimationofalocallygreedyrulemayactuallyplacetheoptimizerontheedgeofstabilityof
thefullbatchobjective.
Limitationsandfuturedirections. Weexploredsomelimitationsofthecurrentmodelingframe-
work in Section 2.3 – in particular, the failure to capture stabilization due to higher order terms.
Developingimprovedmodels(eitheranalyticallyornumerically)wouldallowforpowerfultoolsfrom
otherdisciplinestoaidalgorithmdesign–particularly,methodsfromcontroltheory. Forexample,
state feedback schemes can be designed through the analysis of nonlinear dynamical systems to
ensureasymptoticstabilization[Isidori,1995,Chapter7]. Webelieveacrossdisciplinaryapproach
willbeusefulfordesigningthenextgenerationoflearningratetuners.
10References
IlyaLoshchilovandFrankHutter. Sgdr: Stochasticgradientdescentwithwarmrestarts. InInterna-
tionalConferenceonLearningRepresentations,2016.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
AndrewTulloch,YangqingJia,andKaimingHe. Accurate,largeminibatchsgd:Trainingimagenet
in1hour. arXivpreprintarXiv:1706.02677,2017.
LiyuanLiu,HaomingJiang,PengchengHe,WeizhuChen,XiaodongLiu,JianfengGao,andJiawei
Han. Onthevarianceoftheadaptivelearningrateandbeyond. arXivpreprintarXiv:1908.03265,
2019.
BinShi, WeijieJSu, andMichaelIJordan. OnlearningratesandSchrödingeroperators. arXiv
preprintarXiv:2004.06977,2020.
LarryArmijo. Minimizationoffunctionshavinglipschitzcontinuousfirstpartialderivatives. Pacific
Journalofmathematics,16(1):1–3,1966.
JorgeNocedalandStephenJWright. Numericaloptimization. Springer,1999.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
computationalmathematicsandmathematicalphysics,4(5):1–17,1964.
DongCLiuandJorgeNocedal. Onthelimitedmemorybfgsmethodforlargescaleoptimization.
Mathematicalprogramming,1989.
SharanVaswani,AaronMishkin,IssamLaradji,MarkSchmidt,GauthierGidel,andSimonLacoste-
Julien. Painlessstochasticgradient: Interpolation,line-search,andconvergencerates. Advancesin
neuralinformationprocessingsystems,32,2019.
LeonardoGalli,HolgerRauhut,andMarkSchmidt. Don’tbesomonotone: Relaxingstochasticline
searchinover-parameterizedmodels. arXivpreprintarXiv:2306.12747,2023.
MaximusMutschlerandAndreasZell. Parabolicapproximationlinesearchfordnns. Advancesin
NeuralInformationProcessingSystems,33:5405–5416,2020.
NicolasLoizou,SharanVaswani,IssamHadjLaradji,andSimonLacoste-Julien. Stochasticpolyak
step-sizeforsgd: Anadaptivelearningrateforfastconvergence. InInternationalConferenceon
ArtificialIntelligenceandStatistics.PMLR,2021.
LeonardBerrada,AndrewZisserman,andMPawanKumar. Trainingneuralnetworksforandby
interpolation. InInternationalconferenceonmachinelearning.PMLR,2020.
Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. arXiv
preprintarXiv:2301.07733,2023.
AshokCutkosky,AaronDefazio,andHarshMehta. Mechanic: Alearningratetuner. arXivpreprint
arXiv:2306.00144,2023.
Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free
learner. arXivpreprintarXiv:2306.06101,2023.
MaorIvgi,OliverHinder,andYairCarmon. Dogissgd’sbestfriend: Aparameter-freedynamicstep
sizeschedule. arXivpreprintarXiv:2302.12022,2023.
HaoLi,ZhengXu,GavinTaylor,ChristophStuder,andTomGoldstein.Visualizingthelosslandscape
ofneuralnets. Advancesinneuralinformationprocessingsystems,31,2018.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. Gradient flow in
recurrentnets: thedifficultyoflearninglong-termdependencies,2001.
StanisławJastrze˛bski, ZacharyKenton, NicolasBallas, AsjaFischer, YoshuaBengio, andAmos
Storkey. Ontherelationbetweenthesharpestdirectionsofdnnlossandthesgdsteplength. arXiv
preprintarXiv:1807.05031,2018.
11Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning: A
dynamicalstabilityperspective. AdvancesinNeuralInformationProcessingSystems,31,2018.
DmitryKopitkovandVadimIndelman. Neuralspectrumalignment: Empiricalstudy. InArtificial
NeuralNetworksandMachineLearning–ICANN2020:29thInternationalConferenceonArtificial
NeuralNetworks,Bratislava,Slovakia,September15–18,2020,Proceedings,PartII29,pages
168–179.Springer,2020.
StanislawJastrzebski,MaciejSzymczak,StanislavFort,DevanshArpit,JacekTabor,Kyunghyun
Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
networks. arXivpreprintarXiv:2002.09572,2020.
JeremyMCohen,SimranKaur,YuanzhiLi,JZicoKolter,andAmeetTalwalkar. Gradientdescenton
neuralnetworkstypicallyoccursattheedgeofstability. arXivpreprintarXiv:2103.00065,2021.
JeremyMCohen,BehroozGhorbani,ShankarKrishnan,NamanAgarwal,SourabhMedapati,Michal
Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient
methodsattheedgeofstability. arXivpreprintarXiv:2207.14484,2022.
Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David
Cardoze,GeorgeDahl,ZacharyNado,andOrhanFirat. Alosscurvatureperspectiveontraining
instabilityindeeplearning. arXivpreprintarXiv:2110.04369,2021.
GHinton,SNitish,andKSwersky. Dividethegradientbyarunningaverageofitsrecentmagnitude.
COURSERA:NeuralNetworksforMachineLearning,2012.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International
conferenceonmachinelearning,pages343–351.PMLR,2013.
JamesMartensandRogerGrosse. Optimizingneuralnetworkswithkronecker-factoredapproximate
curvature. InInternationalconferenceonmachinelearning,pages2408–2417.PMLR,2015.
YuriiNesterovetal. Lecturesonconvexoptimization,volume137. Springer,2018.
VincentRoulet,AtishAgarwala,andFabianPedregosa. Ontheinterplaybetweenstepsizetuning
andprogressivesharpening. InOPT2023: OptimizationforMachineLearning,2023.
StanisławJastrzkebski,ZacharyKenton,DevanshArpit,NicolasBallas,AsjaFischer,YoshuaBengio,
andAmosStorkey. ThreeFactorsInfluencingMinimainSGD,September2018.
AtishAgarwalaandJeffreyPennington. Highdimensionalanalysisrevealsconservativesharpening
andastochasticedgeofstability,April2024.
AlexDamian,EshaanNichani,andJasonDLee. Self-stabilization: Theimplicitbiasofgradient
descentattheedgeofstability. InTheEleventhInternationalConferenceonLearningRepresenta-
tions,2022.
AtishAgarwala,FabianPedregosa,andJeffreyPennington. Second-orderregressionmodelsexhibit
progressivesharpeningtotheedgeofstability,October2022.
Atish Agarwala and Daniel S. Fisher. Adaptive walks on high-dimensional fitness landscapes
and seascapes with distance-dependent statistics. Theoretical Population Biology, 130:13–49,
2019. ISSN 0040-5809. doi: https://doi.org/10.1016/j.tpb.2019.09.011. URL https://www.
sciencedirect.com/science/article/pii/S0040580919301765.
AlbertoIsidori. Nonlinearcontrolsystems. Springer,thirdedition,1995.
LuísBAlmeida,ThibaultLanglois,JoséDAmaral,andAlexanderPlakhov. Parameteradaptationin
stochasticoptimization. InOn-linelearninginneuralnetworks,pages111–134,1999.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Onlinelearningrateadaptationwithhypergradientdescent. arXivpreprintarXiv:1703.04782,
2017.
12MartinRiedmillerandHeinrichBraun. Rprop: afastadaptivelearningalgorithm. InProc.oftheInt.
SymposiumonComputerandInformationScienceVII,1992.
YannLeCun,CorinnaCortes,andCJBurges. Mnisthandwrittendigitdatabase. ATTLabs[Online].
Available: http://yann.lecun.com/exdb/mnist,2,2010.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technicalreport,UniversityofToronto,ON,Canada,2009.
AndrejKarpathy. Theunreasonableeffectivenessofrecurrentneuralnetworks. http://karpathy.
github.io/2015/05/21/rnn-effectiveness/,2015.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehier-
archicalimagedatabase. In2009IEEEConferenceonComputerVisionandPatternRecognition,
pages248–255.IEEE,2009.
JeremyHoward. imagenette,2019. URLhttps://github.com/fastai/imagenette/.
MostafaDehghani,AlexeyGritsenko,AnuragArnab,MatthiasMinderer,andYiTay. Scenic: Ajax
libraryforcomputervisionresearchandbeyond. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pages21393–21398,2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
SergeyIoffeandChristianSzegedy. Batchnormalization: Acceleratingdeepnetworktrainingby
reducinginternalcovariateshift. InInternationalconferenceonmachinelearning,pages448–456.
pmlr,2015.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450,2016.
IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,ThomasUn-
terthiner,JessicaYung,AndreasSteiner,DanielKeysers,JakobUszkoreit,etal. Mlp-mixer: An
all-mlparchitectureforvision. Advancesinneuralinformationprocessingsystems,34:24261–
24272,2021.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter
Buchlovsky,DavidBudden,TrevorCai,AidanClark,IvoDanihelka,AntoineDedieu,Claudio
Fantacci,JonathanGodwin,ChrisJones,RossHemsley,TomHennigan,MatteoHessel,Shaobo
Hou,StevenKapturowski,ThomasKeck,IuriiKemaev,MichaelKing,MarkusKunesch,Lena
Martens,HamzaMerzic,VladimirMikulik,TamaraNorman,GeorgePapamakarios,JohnQuan,
RomanRing,FranciscoRuiz,AlvaroSanchez,LaurentSartran,RosaliaSchneider,ErenSezener,
Stephen Spencer, Srivatsan Srinivasan, Milovs Stanojevic´, Wojciech Stokowiec, Luyu Wang,
GuangyaoZhou,andFabioViola. TheDeepMindJAXEcosystem,2020. URLhttp://github.
com/google-deepmind.
MathieuBlondelandVincentRoulet. TheElementsofDifferentiableProgramming. arXivpreprint
arXiv:2403.14606,2024.
MathieuDagréou,PierreAblin,SamuelVaiter,andThomasMoreau. Howtocomputehessian-vector
products? InICLRBlogposts2024,2024. URLhttps://iclr-blogposts.github.io/2024/
blog/bench-hvp/. https://iclr-blogposts.github.io/2024/blog/bench-hvp/.
HongLiu, ZhiyuanLi, DavidHall, PercyLiang, andTengyuMa. Sophia: Ascalablestochastic
second-orderoptimizerforlanguagemodelpre-training. arXivpreprintarXiv:2305.14342,2023.
BehroozGhorbani,ShankarKrishnan,andYingXiao. Aninvestigationintoneuralnetoptimization
viahessianeigenvaluedensity. InInternationalConferenceonMachineLearning,pages2232–
2241.PMLR,2019.
13A TheoreticalModel
A.1 Dynamicsofrescaledvariables
InSection2.3,weusedEquation4toderiveequationsinp:=x2andy :=ηλ−2. Wearrivedat
(cid:18) (cid:19) (cid:20) (cid:21)
η η
p =(1+y )2p , y =η [η (a−bp )]+ t+1 y +2 t+1 −1 . (11)
t+1 t t t+1 t+1 t t η t η
t t
Thedynamicalequationforpisobtainedbysquaringtheequationforx inEquation4. Toderivethe
t
equationfory ,wefirstderivethedynamicsofη λ as
t t t+1
η λ =η [η (a−bp )]+η λ . (12)
t t+1 t t t t t
Wethenhave
η
y = t+1[η λ −2]+2. (13)
t+1 η t t+1
t
EvaluatingcompletesEquation5.
A.2 Dynamicsoftheprojectiononthelargesteigenvector
Wepresentresultsonthedynamicsofpinthevariousmodels;thesewereomittedfromthemaintext
inordertosimplifythepresentation. Forconstantlearningrate,pinitiallydecreasesuntilEOSis
crossed,afterwhichitentersintoacycleofincreaseanddecrease(Figure10,blue). Forourmodel
oflinesearches, whereη = 2(1−ϵ)/λ , pdecaysto0quicklyandthereisnomediationof
t max,t
sharpening(orange,ϵ=0.1).
ForourmodelofCDATpresentedinSection3.2,pstabilizesforσ = 2(Figure11,orange). For
σ <2,themodelstillpredictsdecayofp,buttheratioofp totheorthgonalcomponentg remains
t t
constant(Figure11,blue). Thisfixedratiostabilizesytoavaluenear0.
Inpractice,thehigherordertermsinthedynamicsprovideadditionalstability,intheon-edgemodel,
whichallowsptostabilizeaswell,seeFig.12,Fig.17. Thekeyisthatthesetermscanoperatewhen
yiscloseto0forlongperiodsoftime. Theseresultssuggestthatadditionalmodeldevelopmentis
requiredtounderstandthebehavioroflearningratetunerswhichtargettheEOS.
B AdditionalExperiments
B.1 Furtheranalyzesofbaselearningratetuners
Fig.12completesFig.3withmeasuresofsharpeningandlearningratesonthesettingsconsidered
inFig.1. ForRMSPropweconsideredthepreconditionedHessianfollowingtheobservationsdone
byCohenetal.[2022]thatforadaptivegradientmethodssuchasRMSProporAdam,thesharpness
ofthepreconditionedHessian,ratherthanthesharpnessoftheHessian,definestheedgeofstability.
Namely,recallthatRMSProptakesupdatesoftheform
√
w =w −P−1∇f(w ), forP =diag( ν +ε)
t+1 t t t t t
for
ν =(1−β )g2+β ν , ν =0, g =∇f(w ),
t 2 t 2 t−1 −1 t t
withβ anexponentialmovingaverageparameter. ThepreconditionedHessiantakesthentheform
2
H˜ =P−1/2∇2f(w )P−1/2,
t t t t
andwereportλ (H˜ ).
max t
WeobservesimilarbehaviorsintheseregimesasinFig.3. Namely,thesharpnessorpreconditioned
sharpnesseverincrease(2nd panelsofFig.12), whilethelearningrateseverdecrease(1st panels
of Fig. 12). The constant learning counterpart can operate above the edge of stability while the
self-tunedmethodsgenerallyavoidcrossingtheedgeofstability(3rdpanelsofFig.12).
14102
100
100
102
104 102
106 104 σ=1.9
σ=2.0
108 η η= =1 1.9/λmax 106 p pm ora thx
0 25 50 75 100 0 200 400 600
Steps Steps
Figure10: Dynamicsofthelargesteigenvalue Figure 11: Dynamics of p for toy model of
projection p. For constant learning rate, p cy- CDAT.Forσ =2,pisstable,whichinducessta-
clesbetweenstabilityandinstabilityandλ max bilizationofsharpness(orange). Forσ =2−ϵ
stabilizes (blue). If learning rate tuner sets forsmallϵ(blue,ϵ = 0.1),pdecreasesbutthe
η t =2(1−ϵ)/λ max,t,pdecaysto0andthereis ratioofptotheorthogonalprojectionremains
nonegativefeedbackpreventingsharpening. constantwhichstabilizesynear0.
B.2 Analyzingadditionallearningratetuners
Weconsidertheperformanceoftwoadditionalclassicallearningratetuners,Polyakstepsize[Polyak,
1964,Berradaetal.,2020,Loizouetal.,2021]andhyper-gradientdescent[Almeidaetal.,1999,
Baydinetal.,2017]akintotheresilientbackpropagationscheme[RiedmillerandBraun,1992].
Briefly,Polyakstepsizesconsidersettingthelearningrateas
(cid:26) f(w )−f⋆ (cid:27)
η =min t ,η , (14)
t ∥∇f(w )∥2 max
t
wheref⋆ =min f(w)istheminimumoftheobjectivesetto0byassumingthataneuralnetwork
w
canoverfitthedata,andη isamaximalstepsizeselectedas1or100inourexperiments(wetake
max
thebestinstance).
Hyper-gradientdescentconsidersupdatingthestepsizetowardsmaximaldecreaseoftheobjective.
Namely,definingtheobjectiveobtainedafteronesteph (η)=f(w +ηu ),thealgorithmupdates
t t t
η by a gradient step on h resulting a priori in η = η − α∇f(w + η u )⊤u for a given
t t t+1 t t t t t
hyper-learningrateα. Almeidaetal.[1999],Baydinetal.[2017]arguedforusingmultiplicative
updatesoftheform
(cid:18) ∇f(w +η u )⊤u (cid:19)
η =η 1−β t t t t . (15)
t+1 t ∥∇f(w +η u )∥ ∥u ∥
t t t 2 t 2
Intuitively,thelearningrateincreasesiftheupdateisalignedwiththenegativegradientdirectionand
decreasesotherwise. Resilientbackpropagation[RiedmillerandBraun,1992]adoptsasimilarlogic
componentwise. Inourexperimentswevaryβ andselectthebestinstance,seeAppendixC.5for
moredetails.
WeobservethatPolyakstepsizes(topfigureofFig.13)generallyselectlargerlearningratesthanthe
constantlearningratecounterpart. TheefficiencyofPolyakstepsizesisnotreachedbytheCDATrule
withσ =2butwithaslightlylargerscaleσ =2.06. TheefficiencyofthePolyakstepsizemethod,in
particularcomparedtoasimplelinesearch,inafullbatchregime,hasalsobeenreportedbyRoulet
etal.[2023]. TheproposedCDATrulemaycapturethebenefitsofaggressivelearningratestakenby
Polyakstepsizesinasmootherwaybyallowingvariousscales.
On the other hand, the hyper-gradient descent performs just on par with the fine-tuned constant
learningratecounterpart(bottomfigureofFig.13). Wealsoobserveaslow,yetsteady,progressive
sharpeningwhenusingthehyper-gradientdescent. Aswiththelinesearchmethodorthequadratically
greedyrule,thehyper-gradientdescentfocusesonselectingalearningratethatdecreasestheloss,
whichappears,acrossthosetuners,topotentiallysuppresseffectivestabilizationeffectsnaturally
appearingwithconstantlearningrate.
15
p pB.3 Baselearningratetunersinastochasticregime
InFig.14,wereporttheperformanceofclassicallearningratetuners(linesearchorquadratically
greedymethod)inastochasticregimeforvaryingbatch-sizes. AsobservedpreviouslybyVaswani
etal.[2019]orRouletetal.[2023],alinesearchforexamplecanperformwellinastochasticregime.
Notethatthetwoapproaches(linesearchandquadraticallygreedymethod)displaysimilarbehaviors
(justastheydisplayedsimilarbehaviorsinthefullbatchregime). Thishintsthat,ratherthanplaying
withthenumeroushyperparametersofalinesearchwemayfocussimplyonanadditionalscaling
factorforthequadraticallygreedyrule,whichmotivatedtheproposedCDATrule.
B.4 Targetingtheedgeofstabilityusingtheexactsharpness
Cohenetal.[2021,AppendixF]reportedbadperformanceofadaptivelearningratetunersselecting
thestepsizeas
η =2/λ (∇2f(w)),
max
which may fix the learning rate just at the edge of stability. Note that such a definition does not
takeintoaccounttheadditionalalignmentoftheupdatewiththelargesteigenvector. Ourproposed
diagnosticruleCDATratherconsiderstheedgeofstabilitygivenbyalocalapproximationofthe
objective along the update so to take into account the alignment of the update with the largest
eigenvectoroftheHessian. Weranexperimentswitharule
η =σ/λ (∇2f(w)), (16)
max
thatletsthescalingfactorvaryjustasdonewithCDAT.Theonlydifferenceisintheestimationof
thebaseestimateoftheedgeofstability(CDATdoesitwiththehelpofaquadraticapproximationof
theobjective,whiletherule(16)usesanexactcomputationofthesharpness). InFig.15,weobserve
thatsettingthescaleσ ≈2leadstopoorperformanceaspreviouslyobservedbyCohenetal.[2021].
Notehoweverthatbysettingthescalingmuchabove2(likeσ =3)sucharulemayoutperforma
constantlearningrate. Thishintsthattherule(16)missesthealignmentoftheupdatewiththelargest
eigenvector,whichmotivatedtheCDATrule.
B.5 Analyzinginstantaneousgainsversuslong-termgains
InFig.16,weinvestigatethedifferenceofinstantaneousdecreaseusingthequadraticallygreedy
rule(CDATwithσ =1)comparedtotheonedgerule(CDATwithσ =2). Throughoutatraining,
the quadratically rule ensures a larger instantaneous decrease as intended through its definition
as a learning rate that minimizes the loss. Yet, in the long term, the quadratically greedy rule
underperformstheonedgerule(Fig.5).
B.6 AdditionalmetricsfortheCDATrule
InFig.17,weadditionallymeasurethealignmentoftheupdateswiththelargesteigenvectorandthe
anglebetweensuccessiveupdates. WeobservethattheCDATruleforσ ≈2behavessimilarlyasthe
constantlearningratecounterpart. Inparticular,theupdatestendtoquicklybeinopposeddirections.
Thequadraticallygreedyruledoesnotdemonstratesuchabehavior.
B.7 Sensibilityanalysistoarchitecturehyperparameters
InFig.18,westudyCDATforsimpleMLPsinafullbatchregimeontheMNISTdataset. Ourgoalis
tounderstandthebenefitsoftheproposedCDATruleforvaryinghyperparameters. First,weanalyze
thesensibilitytowidthanddepthofanMLPinasimilarfashionasCohenetal.[2021,AppendixD]
didtoanalyzeprogressivesharpening.
WeobservethataccruedgainscanbeobtainedwiththeCDATruleforlargerwidths(topleftpanel
ofFig.18). NotethatCohenetal.[2021,AppendixD]foundlesssharpeningathigherwidths. In
termsofdepth(toprightpanelofFig.18),theCDATruleworksbestwithlargerdepthswhilewe
noteaslightshiftofoptimalscalingfactorsfrom2toslightlybelow2.
TheCDATruleappearstoworkbestwithsmallornoweightdecay(bottomleftpanelofFig.18)
whileitsbenefitsfadewithlargerweightdecay(nodifferencebetweengreedyσ =1andonedge
σ = 2). Finally,whilethemethodnaturallyfindssmallertrainlosseswithlargersubsetsofdata,
16wedonotobserveasignificantshiftofrelativeperformancebetweenscalesasthesizeofthedata
increases(bottomrightpanelofFig.18).
B.8 CDATruleversusprefixedscheduleinfullbatchregime
InFig.19,wecomparetheproposedCDATrulewithprefixedschedulesinafullbatchregime. We
observethatwhileplacingtheoptimizeronedgecouldimproveonconstantlearningratecounterparts,
prefixedschedulescanoutperformtheCDATrule. Thispointsoutthatthefeedbackloopexploited
byCDATmaymisssomeadditionalnonlineareffectsthatcouldfurtherenhanceself-tuningrules.
B.9 DetailedperformancesofCDATinstochasticregime
InFig.20andFig.21, wedetailtheperformancesoftheCDATruleinthestochasticregimefor
varyingbatchsizes. Inthestochasticregime,recallthattheheatmapoftheperformanceofCDAT
in terms of batch-size heavily depended on the appropriate scaling factor Fig. 7 (in comparison
a scaling factor of approximately σ = 2 appeared generally good in the full batch regime). In
bothFig.20andFig.21,weobservethatthemethodmaygenerallyworkbetteratlargerbatchsizes.
Understandingbettertherightstatisticstoestimateaswellasappropriateestimatorsoftheedgeof
stabilityinastochasticregimeisafuturedirection.
17ResNet34 with squared loss on subset of Cifar10
100 4 1.0
101 103 3 0.8
0.6 102 102 2 0.4
103 101 1 0.2
0 0.0
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch Epoch
GD Linesearch GD Quad. Greedy GD
MixerTi/8 with squared loss on subset of Cifar10
100 4 1.0
3 0.8
101 0.6 102 2 0.4
102 1 0.2
0 0.0
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
GD Linesearch GD Quad. Greedy GD
NanoLM with cross entropy loss on subset of Tiny Shakespeare
109 6
101 5 0.8
107
4 0.6 103 3 105 0.4
2
105 0.2
103 1
0 0.0
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
RMSProp Linesearch RMSProp Quad. Greedy RMSProp
ViTTi/32 with cross entropy loss on subset of Imagenet
1012 6 1.0
101 1010 5 0.8
4 103 108 3 0.6
0.4
106 2
105 0.2
104 1
0 0.0
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
RMSProp Linesearch RMSProp Quad. Greedy RMSProp
Figure12: Learningratesandsharpnessdynamicsofbaselinelearningratetuners. Learning
rate, HessianorpreconditionedHessiansharpness, theirproduct, andthealignmentbetweenthe
updateandthelargesteigenvectoroftheHessianorthepreconditionedHessian. AsinFig.12,we
observe that a linesearch (1) or a quadratically greedy (3) learning rate tuner display decreasing
learningratesalongtraining. ThesharpnessoftheHessian(forGD)orpreconditionedHessian(for
RMSProp)keepincreasingfortheself-tunedbaselineswhiletheystabilizefortheconstantlearning
ratecounterparts. Theself-tunedmethodsperformgenerallybelowtheedgeofstabilityoratleast
muchlessabovethantheconstantlearningratecounterpart.
18
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
ssenprahS
naisseH
ssenprahS
naisseH
ssenprahS
naisseH
dnocerP
ssenprahS
naisseH
dnocerP
ssenprahS
X etaR
gninraeL
ssenprahS
X etaR
gninraeL
ssenprahS
X etaR
gninraeL
ssenprahS
X etaR
gninraeL
giE
ngilA
naisseH
giE
ngilA
naisseH
giE
ngilA
naisseH
dnocerP
giE
ngilA
naisseH
dnocerPResNet34 with squared loss on subset of Cifar10
100 100 105 10
101 8
101 102 104 6
103 4
102 104 103 2
103
105
0
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch Epoch
GD Polyak GD GD CDAT Scale 2.0 GD CDAT Scale 2.06
ResNet34 with squared loss on subset of Cifar10
100 100 10
101 8
101
104
6 102
4
102 103 103
2
104
103 0
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch Epoch
GD Hyper GD GD CDAT Scale 2.0 GD CDAT Scale 2.06
Figure13: Analyzingadditionallearningratetuners. Trainloss,learningrate,sharpness,andtheir
productalongtrainingwithgradientdescentusingaconstantorself-tunedlearningratewithvarious
tuners. Polyakstepizes(14)areeffectiveinafullbatchregime(topfigure)outperformingCDAT
onedge(σ =2). TheeffectivenessofthePolyakstepsizesarepartiallycapturedbyanaggressive
CDATruleplacingtheoptimizeronedgeσ =2.06. Ontheotherhand,ahyper-gradientdescent(15)
performsjustonparwiththeconstantlearningratecounterpartinthisregime. Italsodisplaysan
ever-increasing sharpening akin to the one observed for a linesearch or the quadratically greedy
ruleFig.3.
19
ssoL
niarT
ssoL
niarT
etaR
gninraeL
etaR
gninraeL
ssenprahS
naisseH
ssenprahS
naisseH
ssenprahS
X
etaR
gninraeL
ssenprahS
X
etaR
gninraeLResNet50 with squared loss on Cifar10
Batch size: 64
0.35 0.55
0.30
0.50
0.25
0.20 104 0.45
0.15
0.10 0.40
0.05
0 200 400 0 200 400 0 200 400
Epoch Epoch Epoch
Batch size: 256
0.40
0.38 0.60
0.36
0.55
0.34
104
0.32 0.50
0.30
0.28 0.45
0.26
0 200 400 0 200 400 0 200 400
Epoch Epoch Epoch
Batch size: 1024
0.41
0.40 0.650
0.39 0.625
0.38 0.600 104 0.37 0.575
0.36 0.550
0.35 0.525
0.34
0 200 400 0 200 400 0 200 400
Epoch Epoch Epoch
SGD Linesearch SGD Quad. Greedy SGD
Figure 14: Classical learning rate tuners can work well in stochastic regime. In a stochastic
regime, we observe that, e.g., a linesearch can perform on par or even better than the constant
learningcounterparts. However,thisgoodperformanceisnotexplainedbythecommonbeliefthat
linesearchesworkwellinafullbatchregime(Fig.1). Thelinesearchandquadraticallygreedyrule
performsimilarlyinthissetting.
MLP[256, 256, 256] with squared loss on subset of Cifar10
0.4 100 5
0.3 4
101 0.2 3
0.1 2
102
0.0 1
0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch Epoch
GD GD CDAT Scale 2.0 GD CDAT Scale 2.5
GD CDAT Scale 1.94 GD CDAT Scale 2.06 GD CDAT Scale 3.0
Figure15: Onedgewithexactsharpness. WeconsiderarulesimilartoCDAT(8)butusingthe
exactsharpness,thatisthelargesteigenvalueoftheHessian,asabaselearningratewhilevaryingan
additionalscalefactor(see(16)). Byusingtheexactsharpness,ascalingfactorofσ =2leadsnow
topoorperformance,whileascalingfactorofσ =3isperformant. Byusingtheexactsharpness(16)
wedonottakeintoaccounttheactualalignmentoftheupdatewiththelargesteigenvectorwhich
mayexplaintheshiftofoptimalscalingfactorsinthiscase.
20
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
rorrE
tseT
rorrE
tseT
rorrE
tseT
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
ssenprahS
X etaR
gninraeLTraining with CDAT Scale 2.0 Training with CDAT Scale 2.0
5 0.4 0.4
GD CDAT Scale 1.0
4 GD CDAT Scale 2.0 0.3 0.3
3
0.2 0.2
2
0.1 0.1
1
0 0.0 0.0
0 100 200 300 400 0 100 200 300 400 0 100 200 300 400
Steps Steps Steps
Figure 16: The quadratically greedy rule ensures larger instantaneous decrease of the loss.
Trainlossesanddifferenceinlossbetweenonestepusingtheon-edgerule(CDAT,scale=2)and
onestepusingquadraticallygreedyrule(CDAT,scale=1). Resultsaveragedover10initializations
and disjoint 4096-sample subsets of CIFAR100. MLP architecture: single hidden layer of size
1024, ReLU activations, trained with GD and cross-entropy loss in a full batch setting. We plot
f(w +ηoeu )−f(w +ηqgu )alongatrainingperformedeitherwiththequadraticallygreedyrule
t t t t
(σ = 1) or the on-edge rule (σ = 2). In both cases, this difference is positive meaning that the
quadraticallygreedyruleensuresalargerinstantaneousdecreaseoftheloss. Yetthequadratically
greedyruleunderperformsinthelongterm(seeFig.5,alsoholdsinthisfullbatchsetting).
ResNet34 with squared loss on subset of Cifar10
1.0 1.0
0.8
0.5
0.6
0.4 0.0
0.2 0.5
0.0
1.0
0 5000 10000 15000 0 5000 10000 15000
Epoch Epoch
GD GD CDAT Scale 1.94 GD CDAT Scale 2.06
GD CDAT Scale 1.0 GD CDAT Scale 2.0
Figure17: FurtheranalysisoftheCDATrule. ThisisthesamesettingasinFig.6exceptthat
weplotthealignmentbetweenthelargesteigenvectoroftheHessianandtheupdate,andtheangle
between successive updates. We observe that the CDAT rule for σ ≈ 2 behaves similarly as the
constantlearningratecounterpart. Inparticular,theupdatestendtoquicklybeinopposeddirections.
Thequadraticallygreedyruledoesnotdemonstratesuchabehavior.
21
ssol
niarT
giE
ngilA
naisseH
etadpu
eno
retfa
ssol
ni
.ffiD
0.1
dna
0.2
elacS
TADC
wtb
setadpU
elgnA
etadpu
eno
retfa
ssol
ni
.ffiD
0.1
dna
0.2
elacS
TADC
wtbSubset of Mnist MLP Square Loss SGD On Edge Subset of Mnist MLP Square Loss SGD On Edge
Depth 3 Width 256
3.0 1e+00 3.0 1e+00
2.5 2.5
2.25 2.25
22 .0.1 62 25 5 1e-01 22 .0.1 62 25 5 1e-01
2.03125 2.03125
2.0156 22 .05 1e-02 2.0156 22 .05 1e-02 1.984375 1.984375
1.96875 1.96875
1.9375 1.9375 1e-03
1.875 1e-03 1.875
1.75 1.75 11 .. 05
1e-04
11 .. 05 1e-04
32 64 128 256 512 1 2 3 4 5 6
Width Depth
Subset of Mnist MLP Square Loss SGD On Edge Subset of Mnist Square Loss SGD On Edge
Hidden sizes [256, 256, 256] Hidden sizes [256, 256, 256]
3.0 1e+00 3.0 1e+00
2.5 2.5
2.25 2.25
2.125 1e-01 2.125 1e-01 2.0625 2.0625
2.03125 2.03125
2.0156 22 .05 1e-02 2.0156 22 .05 1e-02 1.984375 1.984375
1 1.9 1.96 .838 777 555 1e-03 1 1.9 1.96 .838 777 555 1e-03
1 1.7 .55 1 1.7 .55 1e-04
1.0 1e-04 1.0
0.0 1e-05 0.0001 0.001 2048 4096 8192
Weight Decay Data Size
Figure18: ImprovementsofCDATruleonedgeforvaryinghyperparameters. Varyingwidth,
depth,weightdecayandsizeofthesubsetconsideredwhenusingtheCDATrulewithvaryingscaling
factors. WeobservethataccruedgainscanbeobtainedwiththeCDATruleforlargerwidths(topleft
panel). Intermsofdepth(toprightpanel),theCDATruleworksbestwithlargerdepths,whilewe
notethereaslightshiftofoptimalscalingfactorsfrom2toslightlybelow2. TheCDATruleappears
toworkbestwithsmallornoweightdecay(bottomleftpanel)whileitsbenefitsfadewithlarger
weightdecay(nodifferencebetweengreedyσ =1andonedgeσ =2). Finally,whilethemethod
naturallyfindssmallertrainlosseswithlargersubsetsofdata,wedonotobserveasignificantshiftof
relativeperformancebetweenscalesasthesizeofthedataincreases(bottomrightpanel).
ResNet34 with squared loss on subset of Cifar10 NanoLM with cross entropy loss on subset of Tiny Shakespeare
0.4 100 4
101
0.3 3
101
0.2 2 103
102
0.1 1 105
103
0.0 0
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
GD Scheduled GD RMSProp Scheduled RMSProp
GD CDAT Scale 1.0 GD CDAT Scale 2.0 RMSProp CDAT Scale 1.0 RMSProp CDAT Scale 2.0
MLPMixerTi/8 with squared loss on subset of Cifar10 ViTTi/32 with cross entropy loss on subset of Imagenet with edge EMA
100
101
100 100
101
102 101 102 101
103 102 104 103
104 103 105
106
105 104
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Epoch Epoch Epoch Epoch
GD Mom. GD Mom. CDAT Scale 2.0 Adam Scheduled Adam
GD Mom. CDAT Scale 1.0 Scheduled GD Adam CDAT Scale 1.0 Adam CDAT Scale 2.0
Figure19: CDATrulemaynotfullycapturethebenefitsofpre-fixedschedules. Trainlossand
learningratebehaviorsforfine-tunedoptimizerswithorwithoutschedulesvsself-tunedcounterparts
withCDATonvariousarchitecture,datasets,lossesinafullbatchregime. WhiletheCDATrule
displays a behavior to warmup schedules, it does not completely catch the benefits of pre-fixed
schedules. Noteforthetop-leftpartthattheperformanceofthepre-fixedscheduleisakintothe
performancereportedwithCDATσ =2.5atearlytimessuggestingthatavaryingscalingfactor,or
takinghigherorderdynamicsmaybeimportanttofullycapturethebenefitsofwarm-upschedules.
22
ssoL
niarT
ssoL
niarT
rotcaF
gnilacS
rotcaF
gnilacS
etaR
gninraeL
etaR
gninraeL
ssol
niart
tinI/ssol
niart
laniF
ssol
niart
.tinI/ssol
niart
laniF
ssoL
niarT
ssoL
niarT
rotcaF
gnilacS
rotcaF
gnilacS
etaR
gninraeL
etaR
gninraeL
ssol
niart
.tinI/ssol
niart
laniF
ssol
niart
.tinI/ssol
niart
laniFResNet50 with squared loss on Cifar10
Batch size: 32
0.40 0.9 100
0.35
0.30 0.8 101 0.7 0.25 102
0.20 0.6
0.15 0.5 103
0.10 0.4 104
0.05 0.3
0.00
105
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
Batch size: 64
0.40 0.9 100
0.35
0.8
0.30 101 0.7 0.25
0.6 102 0.20 0.15 0.5 103
0.10 0.4
0.05 0.3 104
0.00 0.2
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
Batch size: 128
0.40 0.9 100
0.35
0.8
0.30 101 0.7 0.25
0.6 102 0.20 0.15 0.5 103
0.10 0.4
0.05 0.3 104
0.00 0.2
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
Batch size: 256
0.40
0.9 100
0.35
0.8
0.30 0.7 101 0.25
0.20 0.6 102 0.15 0.5
0.10 0.4 103
0.05 0.3 104
0.00 0.2
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
Batch size: 512
0.40
0.9
0.35 100
0.8
0.30 0.7 101 0.25
0.6 0.20 102
0.15 0.5
0.10 0.4 103
0.05 0.3
104
0.00 0.2
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
Batch size: 1024
0.40 0.9 100
0.35
0.8
0.30 101 0.7 0.25
0.20 0.6 102
0.15 0.5
0.10 0.4 103
0.05 0.3
104
0.00
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Epoch Epoch Epoch
SGD Mom Scheduled SGD SGD Mom CDAT Scale 2.2
Figure20: Performanceoffine-tunedalgorithmsinstochasticregimewithSGDMomentum.
InthestochasticregimewithSGDmomentum,weobservethattheCDATrulemayoutperformthe
constant learning rate counterpart (particularly for large batch sizes) while performing on par or
underperformingthescheduledlearningratecounterparts. Interestingly,awarm-upphaseappears
naturallyinducedbytheCDATrule(rightplots).
23
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
rorrE
tseT
rorrE
tseT
rorrE
tseT
rorrE
tseT
rorrE
tseT
rorrE
tseT
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
etaR
gninraeL
etaR
gninraeLViTS/16 with cross-entropy loss on Imagenet
Batch size: 64
7 1.0 100
6 0.9
5 0.8 102
4 0.7
3 0.6 104
2 0.5
1 0.4 106
0.3
0 25 50 75 100 125 0 25 50 75 100 125 0 25 50 75 100 125
Epoch Epoch Epoch
Batch size: 256
7 1.0 100
6 0.9 101
5 0.8 102
4 0.7 103
3 0.6 104
2 0.5 105
1 0.4 106
0.3
0 25 50 75 100 125 0 25 50 75 100 125 0 25 50 75 100 125
Epoch Epoch Epoch
Batch size: 1024
7 1.0 100
6 0.9 101
5 0.8 102
4 0.7 103
3 0.6 104
2 0.5 105
1 0.4 106
0.3
0 25 50 75 100 125 0 25 50 75 100 125 0 25 50 75 100 125
Epoch Epoch Epoch
Adam Scheduled Adam Adam CDAT Scale 0.4
Figure 21: Performance of fine-tuned algorithms in stochastic regime with Adam. In the
stochasticregimewithAdamweobservevaryingperformanceoftheCDATrule,generallyonpar
orunderperformingthebaselines. Severalfactorscanexplaintheunderperformance. Thescaling
factorisnotwellunderstood,andafinergridsearchcouldimprovethescheme. Similarly,abetter
estimateorabetterunderstandingoftheedgeofstabilityinastochasticregimecouldimprovethe
approach. TacklingcurvaturedynamicsbyanalyzingfeedbackeffectsasintheCDATrulemayhelp
suchdesign.
24
ssoL
niarT
ssoL
niarT
ssoL
niarT
rorrE
tseT
rorrE
tseT
rorrE
tseT
etaR
gninraeL
etaR
gninraeL
etaR
gninraeLC ExperimentalDetails
C.1 Datasets
MNIST. MNISTisanimageclassificationdatasetofhandwrittendigits[LeCunetal.,2010]. We
normalized the pictures so that each pixel is between 0 and 1. We did not standardize the data.
WeonlyusedthisdatasettotestvaryingMLParchitecturesinFig.18. SeeAppendixC.5forany
additionalrelevantdetails.
CIFAR10. CIFAR10isanimageclassificationdatasetofcoloredimagesofsize32×32with10
classes[Krizhevskyetal.,2009]. Wenormalizedthepicturessothateachpixelisbetween0and1.
Wedidnotstandardizethedata. Inthefullbatchregime,weconsideredasubsetof4096samples. In
theminibatchregime,weconsideredthefulldatasetof50,000samplesfortraining(droppingout
theremainderbatch)andtestedonthe10,000validationsamples.
TinyShakespeare. Thisconsistsin40,000linesofShakespearefromavarietyofShakespeare’s
plays[Karpathy,2015]. Thetaskconsistsinacharacter-levelpredictiontaskamong64characters. In
thefullbatchregime,weconsideredasubsetof2048samplesconsistingofblocksof64characters.
Imagenet. Imagenetisanimageclassificationdatasetofvariousimagesfromtheweb[Dengetal.,
2009]. Theimageshavevarioussizes. TheoriginalImagenet-1Kdatasetcontains1000classes. For
thefullbatchexperiments,weconsidertheImagenette[Howard,2019]subsetthatconsistsinonly
10classesandtook1024samplesoutofit. WeconsidertheusualprepreocessingforImagenetas
detailed in the Scenic library [Dehghani et al., 2022]. Namely, for training we consider random
croppingandrandomflipattraining. Fortesting,wecentercroptheimages. Eachtimethecropping
reducesthecoloredimagestoa224×224size. Inthemini-batchregimeweconsiderthecomplete
trainingdatasetof1.2millionimages(Imagenet-1K),droppedtheremainderbatch,andreportedtest
erroronthe50,000validationimages.
C.2 Architectures
Residual Network (ResNet). We considered the standard ResNet architectures (ResNet34,
ResNet50)ofHeetal.[2016]asimplementedintheSceniclibrary[Dehghanietal.,2022]. For
theexampleswithResNet34weremovedthebatchnormalizationlayers[IoffeandSzegedy,2015].
FortheexampleswithResNet50wereplacethebatchnormalizationlayerswithlayernormalization
layers[Baetal.,2016].
Multi-Layer Perceptron (MLP) Mixer. We consider the standard MLP Mixer architec-
tures [Tolstikhin et al., 2021] as implemented in the Scenic library [Dehghani et al.,
2022]. By Mixer Ti/8, we mean the tiny model of Mixer provided in the Scenic li-
brary(seehttps://github.com/google-research/scenic/blob/main/scenic/projects/
baselines/configs/imagenet/imagenet_augreg_mixer_config.py) with patches of size
8×8. Weremoveddropout(bothlayeranddepthwise).
Nano Language Model (NanoLM). We consider a simpel sequence-to-sequence Transformer
model implemented in Optax (https://github.com/google-deepmind/optax/blob/main/
examples/nanolm.ipynb). Themodelconsistsof6stackedtransformerblocks, eachofwhich
containsamulti-headattentionlayerfollowedbyafeed-forwardlayer. Layernormalizationisused
used within the transformer blocks to improve training stability. Finally, a dense layer maps the
model’soutputtothevocabularysize,producingprobabilitiesforeachcharacterasthenextpotential
character. Theonlydifferencewithrespecttothepreviouscodeisthatweremoveddropout(for
deterministictraining)and,asmentionedintheprevioussubsection,reducedthesizeofthedatasetto
beabletoestimatethesharpness.
Vision Transformer (ViT). We consider the standard Vision Transformer architecture [Doso-
vitskiy et al., 2020] as implemented in the Scenic library [Dehghani et al., 2022]. By
ViT Ti/32, we mean the tiny model of Vision Transformer provided in the Scenic li-
brary(seehttps://github.com/google-research/scenic/blob/main/scenic/projects/
25baselines/configs/imagenet/imagenet_augreg_vit_config.py)withpatchesofsize32×
32. Weremoveddropout(bothlayeranddepthwise).
Weightdecay. Inallexamples,exceptmentionedotherwise,weconsiderafixedweightdecayof
10−5. SeeFig.18(bottomleftpanel)forananalysisofsensitivityoftheproposedCDATrulewith
varyingweightdecay.
C.3 Algorithms
Inallexperiments,whenselectingthe"best"tunerweconsideredtheaveragetrainlossoverthelast
5iterates.
Fine-tuning base optimizers. The implementation of all optimizers are taken from the Optax
library[DeepMindetal.,2020]. Inallexperiments,wefixallhyperparametersofthebaseoptimizer
((S)GD,(S)GDwithMomentum,RMSProp,Adam)totheirdefaultvalues: 0.9forthemomentum
of(S)GDwithMomentum,0.999fortheEMAparameterofthesecondmomentofRMSPropand
Adam,0.9fortheEMAparameterofthefirstmomentofAdam. Wefine-tunethelearningrateona
logarithmicbaseof2aroundabaselearningratesuchas10−3or10−4(dependingonthealgorithm,
thearchitectureandthemini-batchsizeinthestochasticregimeasdetailedbelowinAppendixC.5),
whilemakingsurethatthegridissufficientlylargesuchthatthebestlearningrateisfoundinsidethe
gridandnotasthesmallestorthelargest.
Forthescheduledversionsofthebaseoptimizers,weconsiderthreeshapes: linearwarm-upfollowed
by constant learning rate, linear warm-up followed by linear decay, linear warm-up followed by
cosinedecay. Thenumberofiterationsforthewarm-upperiodischosenasafractionoftheoverall
numberofstepsdetailedinAppendixC.5. Wealsovariedthehorizonforthedecayingschedules,see
againAppendixC.5.
Implementationofthelinesearchprocedure. Toimplementthelinesearchproceduredescribed
inSection2,weconsiderthefollowingcriterion
f(w +ηlsu )≤(1+δ)f(w )+cηls∇f(w )⊤u .
t t t t t t t
Comparedto(1),weaddedarelativedecreasehyperparmeterδasweobservedthatthelinesearch
cansometimesstaystuckatvanishinglearningratesotherwise.
Tofindavalidcriterionweconsiderausualbacktrackinglinesearchthatstartsfromaguessη =
t,0
min{c ηls ,1}. Choosingc =+∞meansthatwestartwithaninitialguessof1ateachiteration.
+ t−1 +
Thelearningrateisthendecreasedbyafactorc untilthecriterionissatisfied. Formally,theselected
−
stepsizeisthen
ηls =max{η =ckη :f(w +η u )≤(1+δ)f(w )+cη ∇f(w )⊤u }.
t t,k − t,0 t t,k t t t,k t t
Werunthesearchuntilthecriterionissatisfiedinthefullbatchregimeandforamaximumof30
iterationsinthemini-batchregime. Intheexperiments,weconsiderthefollowingvariations.
• c∈[0,10−4,0.5],
• c ∈[4,+∞],
+
• c ∈[0.8,0.9],
−
• δ ∈[0,1e−3]forc=0andδ =0,forc∈[10−4,0.5].
Implementation of quadratically greedy tuner and CDAT. To implement the quadratically
greedytunerorCDAT,wecomputethedenominatoru⊤∇2f(w)uasthesecondpartialderivativeof
f alongu,thatis,
u⊤∇2f(w)u=∂2f(w)[u,u]=∂(∂f(·)[u])(w)[u],
where∂g(w)[u]amountstoaJacobianvectorproduct(jvp)computedwithforwardmodeauto-diff
indifferentiableprogramminglanguagessuchasJAX[DeepMindetal.,2020].
ComputingthedenominatorintheCDATrulebyforwardmodeautomaticdifferentiationenablesa
muchlowermemoryconsumptionthanusingHessianvectorproducts(see,e.g.,[BlondelandRoulet,
2024,Chapter8],[Dagréouetal.,2024]formoredetails). Thecomputationofthedenominatorby
applyingtwiceforwardmodeautomaticdifferentiationstillincursapproximatelythreetimesthe
26memorynecessarytocomputetheobjective[BlondelandRoulet,2024,Chapter8]. Thecomputa-
tionalcostofcomputingthedenominatorisalsoapproximatelythreetimesthecomputationalcost
ofcomputingtheobjective. Theaboveapproximationsaredonewiththefollowingreasoning. The
secondpartialderivativerequirestofollowthegraphofcomputationbutwiththreevariables,one
fortheparameters,oneforacopyoftheupdatedirection,oneforanothercopy. Ateachnodein
thecomputationgraph,theprogramcomputestheoriginalcomputation,computesitsfirstderivative
alongthefirstcopyoftheupdatedirection,andcomputesthesecondderivativealongthesecond
copy.
Inpractice, weobservedfor, e.g., theexperimentonthefullImagenetdatasetinmini-batchthat
theproposedCDATrulerequiredtwicethewalltimeoftheconstantorscheduledlearningrates
counterparts. Forthisproject,weconsideredCDATasadiagnostictooltounderstandtheinterplay
betweencurvatureandlearningratetuners. Forfuturework,thecostofcomputingtheapproximate
edgemaybecircumventedoramortizedbyusing,e.g.,parabolicapproximationsasdonebyMutschler
andZell[2020],orbycomputingitatgivenintervalsasdonebyLiuetal.[2023].
FurtherjustificationfortheCDATformula. In(8)wetooktheabsolutevalueofthedenominator
todealwithconcaveapproximations. EigenvaluemodificationsinaNewtonmethodarediscussed
byNocedalandWright[1999,Section3.4]. Takingtheabsolutevalueisonepossibleoption. In
practice,weobservedpositivecurvaturesalongtheupdatedirectionsuchthatthischoicedidnot
matter.
C.4 Metricsimplementation
Sharpness estimation. We estimated the sharpness by a power iteration method run for 1000
iterationswithanearlystoppingcriteriondefinedbylessthan10−3relativeaccuracy. Weaccessed
theHessianbyHessianvectorproducts,whichlimitedthesizeofthefullbatchdatasetsconsidered
onTPUs. Thepoweriterationapriorireturnsthelargesteigenvalueinmagnitude|λ| andnot
max
necessarilythelargestpositiveeigenvalueλ . Butinpracticethelargesteigenvalueinmagnitude
max
isthelargesteigenvalue,see,e.g.,[Ghorbanietal.,2019]foranin-depthstudyofthespectrumofthe
Hessianalongtheiterationsofdeeplearning.
C.5 Additionalexperimentaldetailsperfigure
Wedetailhereanyadditionaldetailperfigurenotdetailedinthesummaryabove.
Fig. 1. The ResNet34 has no batch normalization layers. For the GD baseline on ResNet and
theMLPMixertheconstantlearningratewastunedonagrid{10−3 ·2i,i ∈ {−1,...,7}}. For
the RMSPRop baseline on the NanoLM and ViT, the constant learning rate was tuned on a grid
{10−3·2i,i∈{−1,...7}}and{10−5·2i,i∈{0,...7}}respectively.
Fig.2. Weconsideralinearclassification(inotherwordsusinganMLPwithouthiddenlayers)
onthesubsetofCIFAR10detailedabove. Wesearchtheconstantstepsizeofgradientdescentin
{10−3·2i,i∈{0,1,2}]}. Thegridiscenteredaround1/∥∇2f(w )∥ ,thatistheoptimalstepsize
0 2
inaconvexsmoothsetting. Asforaboveexperiments, thelargestlearningrateinthegridledto
divergence.
Fig.3. ThisisthesamesettingasinthefirstpanelofFig.1.
Fig.4. Forconstantlearningrate,modelsettingsweregivenbya=3·10−2,b=3·10−1,η =1.
Forfixedy =−0.1training,a=1,b=0.5,η =1.0.
0
Fig.5. WeconsideredthesamesettingsasinFig.1. ThegridsearchforGDonResNet34and
RMSProp on NanoLM are the same as in Fig. 1. For the MLPMixer, we fine-tuned GD with
momentum on a grid {10−2 ·2i : i ∈ {1,...,8}}. For the ViT, we fine-tuned Adam on a grid
{10−5·2i :i∈{1,...,8}}.
Fig.6. ThisisexactlythesamesettingasinthefirstpanelofFig.5.
27Fig.7. WeconsideredthefulldatasetofCIFAR10withResNet50withlayernormalizationinplace
ofbatchnormalization.
Fig.8. SeethedetailsgivenforFig.20andFig.21.
Fig.9. Forbothvaluesofσ,a=5·10−2,b=10−1,ν =0.1,λ =18,η =0.05,g =p =4.
0 0 0 0
Fig.10. SamesettingsasFigureFig.4.
Fig.11. SamesettingsasFigureFig.9.
Fig.12. ThisisthesamesettingasinFig.1.
Fig.13. WeconsiderthesamesettingasinFig.3. ForthePolyakstepsizes(14),weletη vary
max
between1and100andselectthebest. Forthehypergradientdescent,weletthehyperlearningrate
varyinβ ∈{10i,i∈{−3,...,0}}.
Fig.14. WeconsideredagaintheResNet50withlayernormalizationinsteadofbatchnormalization.
Fortheconstantlearningratebaselinewesearchedoveragridof{η ·2i,i∈{−1,...,5}]},for
m
(cid:112)
η =10−4· m/4096formthebatchsize.
m
Fig.15. WeconsideredasimpleMLPwithhiddensizes(256,256,256),ReLUactivations. We
tunedtheconstantlearningratebaselineon{10−3·2i,i∈{−1,7}]}.
Fig.16. Detailsareprovidedinthelegend.
Fig.17. ThisisthesamesettingasinFig.6.
Fig.18. Inthisfigure,theMLPsconsidereduseReLuactivations. Ifnotdetailed,theweightdecay
issetto10−5andthesubsetconsideredisofsize8192.
Fig.19. ThesettingsarethesameasinFig.5. Fortheconstantlearningratebaselineswesearched
onagird{η ·2i,i∈{−3,...,9}}. Thebaselearningrateη waschosentobe10−3forResNet,
base base
10−2 fortheMixer,10−4 fortheNanoLMandViT.Fortheschedules’shapes,wesearchedover
linearwarm-up,linearwarm-upwithlineardecay,linearwarm-upwithcosinedecay. Theinitialand
endlearningrateweresetto0. Thehorizonsforthescheduleswerechosenin[N,N/2,N/4]for
N =8192fortheNanoLM,N =16384fortheViT,MixerandResNet. Thefractionofwarm-up
stepswassearchedin{0.05,0.1,0.2}.
Fig.20. Fortheconstantlearningratebaseline,weconsidersearchingthebestconstantlearning
(cid:112)
rateonagrid{η ·2i,i∈{−1,...,7}}forη =10−4· m/4096wheremdenotesthevarying
m m
batchsize.
Forthescheduledbaseline,weconsiderthevariantspresentedabove(linearwarm-upfollowedby
constant,linearwarm-upfollowedbycosinedecay,linearwarm-upfollowedbylineardecay)with
varyingfractionofwarm-upsteps(0.05,0.1,0.2)andaninitiallearningrateof0,afinallearningrate
of0forafixedhorizonof512epochs,andapeaklearningratesearchedover{η ·4i,i∈{4,...,9}}.
m
ThescalingfactorσofCDATwassearchedonagrid{0.4,0.6,...,2.8},andwealsotunedtheEMA
parameterβ inthecomputationofthenumeratorsanddenominatorsoftheedgein{0,0.9,0.99}.
cdat
ThebestparametersfoundforCDATcanbeinferredfromFig.7. Namely,wefoundthatnon-zero
EMAparameterfortheestimationoftheedgedecaywasessentialforgoodperformanceandthatthe
bestscalingfactorvariedwiththebatchsize. Forexample,atbatchsize256thebestscalingfactoris
σ =1.8withβ =0.9.
cdat
Fig.21. Fortheconstantlearningratebaseline,weconsidersearchingthebestconstantlearning
(cid:112)
rateonagrid{η ·2i,i∈{−1,...,7}}forη =10−4· m/1024wheremdenotesthevarying
m m
batchsize.
28Forthescheduledbaseline,weconsideralinearwarm-upfollowedbycosinedecay,withafraction
ofwarm-upstepsof0.1andaninitiallearningrateof0,afinallearningrateof0forafixedhorizon
of128epochs,andapeaklearningratesearchedover{η ·4i,i∈{1,...,5}}.
m
ThescalingfactorσofCDATwassearchedonagrid{0.4,0.6,...,2.6},andwealsotunedtheEMA
parameterβ inthecomputationofthenumeratorsanddenominatorsoftheedgein{0,0.9,0.99}.
cdat
C.6 Assetslicenseandcomputingressources
Assets. Allexperimentsaredoneintheopen-sourceJAXecosystem[DeepMindetal.,2020]: archi-
tecturesaretakenfromScenic[Dehghanietal.,2022],datasetsfromTensorFlowDataset,algorithms
fromOptax. ThedatasetsareMNIST[LeCunetal.,2010],(CreativeCommonsAttribution-Share
Alike3.0license)CIFAR10[Krizhevskyetal.,2009](noavailablelicense),Imagenet[Dengetal.,
2009](ImageNetexplicitlypermitstheuseofthedatasetfornon-commercialresearchpurposes,
howeverthereisnosinglelicensesincetheimagesarescrappedfromdifferentsourceswithdifferent
licenses),TinyShakespeare[Karpathy,2015](Apache2.0licenseinTensorFlowdataset,thoughthe
worksofWilliamShakespeareareinthepublicdomain).
Computingresources. ExperimentshavemostlybeenrunonTensorProcessingUnits(TPUs)
v2(180TeraFloating-PointOperationsperSecond(TFLOPS),64GBHighBandwidthMemory
(HBM)).ExperimentsonMLPMixersrequiredTPUsv3(420TFLOPS128GBHBM).Verysmall
scaleexperimentsonMNISTwithMLPswererunonCPUs. Intermsofwalltime,asdiscussedin
AppendixC.3,weobservedthattheCDATrulecanbetwiceslowerthantheconstantorscheduled
learningratecounterparts. WeconsiderCDATasadiagnostictoolandleaveasfutureworkefficient
implementations. Preliminaryexperimentsandadditionalattemptstofurtheradaptthemomentum
parameteronedgearenotreported.
Authorscontributions
• VincentRouletconductedtheexperimentalworkfromthefailuresoflinesearchestotheanalysisof
theCDATruleinvarioussettings.
• Atish Agarwala developed the theoretical model, with associated figures, interpretations and
comments. Healsohelpedtoguidetheexperimentalstudywiththeinsightsgatheredbythemodel.
• JeanBastienGrilldidaninitialempiricalstudythatgatheredfirstintuitionsonthemethod. He
participatedinthediscussionsandcontributedtothewriting.
• GrzegorzSwirszczparticipatedinthediscussions.
• Mathieu Blondel participated in the discussions, contributed to the writing and proposed an
alternativeruleusingaGauss-Newtonapproximationoftheobjective.
• FabianPedregosainitiatedtheproject,performedalargerscaleempiricalstudyoftheCDATrule
ontheMLCommonsbenchmark,participatedinthediscussions,andcontributedtothewriting.
29