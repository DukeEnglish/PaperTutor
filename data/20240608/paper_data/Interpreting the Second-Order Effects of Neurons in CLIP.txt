Interpreting the Second-Order Effects
of Neurons in CLIP
YossiGandelsman,AlexeiA.Efros,JacobSteinhardt
UCBerkeley
{yossi_gandelsman,aaefros,jsteinhardt}@berkeley.edu
Abstract
WeinterpretthefunctionofindividualneuronsinCLIPbyautomaticallydescribing
themusingtext. Analyzingthedirecteffects(i.e. theflowfromaneuronthrough
theresidualstreamtotheoutput)ortheindirecteffects(overallcontribution)fails
tocapturetheneurons’functioninCLIP.Therefore,wepresentthe“second-order
lens”,analyzingtheeffectflowingfromaneuronthroughthelaterattentionheads,
directly to the output. We find that these effects are highly selective: for each
neuron,theeffectissignificantfor<2%oftheimages. Moreover,eacheffectcan
beapproximatedbyasingledirectioninthetext-imagespaceofCLIP.Wedescribe
neuronsbydecomposingthesedirectionsintosparsesetsoftextrepresentations.
Thesetsrevealpolysemanticbehavior—eachneuroncorrespondstomultiple,often
unrelated, concepts(e.g. shipsandcars). Exploitingthisneuronpolysemy, we
mass-produce“semantic”adversarialexamplesbygeneratingimageswithconcepts
spuriouslycorrelatedtotheincorrectclass. Additionally,weusethesecond-order
effectsforzero-shotsegmentationandattributediscoveryinimages. Ourresults
indicatethatascalableunderstandingofneuronscanbeusedformodeldeception
andforintroducingnewmodelcapabilities.1
1 Introduction
Automatedinterpretabilityoftherolesofcomponentsinneuralnetworksenablesthediscoveryof
model limitations and interventions to overcome them. Recently, such a technique was applied
for interpreting the attention heads in CLIP [13], a widely used class of image representation
models[32]. However,thisapproachhasonlyscratchedthesurface,failingtoexplainamajorset
ofCLIP’scomponents—neurons. Herewewillintroduceanewinterpretabilitylensforstudying
theneuronsandusethegainedunderstandingforsegmentation,propertydiscoveryinimages,and
mass-productionofsemanticadversarialexamples.
InterpretingtheneuronsinCLIPisahardertaskthaninterpretingtheattentionheads. First,there
aremoreneuronsthanindividualheads,whichrequiresamoreautomatedapproach. Second,their
directeffectontheoutput—theflowfromtheneuron, throughtheresidualstreamdirectlytothe
output—isnegligible[13]. Third,mostinformationisstoredredundantly—manyneuronsencodethe
sameconcept,sojustablatinganeuron(i.e. examiningindirecteffects)doesnotrevealmuchsince
otherneuronsmakeupforit.
Thelimitationspresentedabovemeanthatwecanneitherlookatthedirecteffectnortheindirect
effecttoanalyzeasingleneuron.Toaddressthis,weintroducea“second-orderlens”forinvestigating
thesecond-ordereffectofaneuron—itstotalcontributiontotheoutput,flowingviaalltheconsecutive
attentionheads(seeFig.1).
Westartbyanalyzingtheempiricalbehaviorofsecond-ordereffectsofneurons. Wefindthatthese
effectshavehighsignificanceinthelatelayers. Additionally,eachneuronishighlyselective: its
1Projectpageandcode:https://yossigandelsman.github.io/clip_neurons/
Preprint.Underreview.
4202
nuJ
6
]VC.sc[
1v14340.6042:viXraSecond order effects of CLIP’s neurons
MLP 0 MLP L-1 MLP L
MSA 0 MSA L
P
Text-based sparse neuron
Automatic generation of adversarial examples
decomposition
1. dog “A cat lounging in the CLIP’s
2. elephant sun, with a group of prediction:
3. value Language elephants in the Text-to-Image
4. cabbage Model background and a Model dog: 65%
... value sign in the
cat: 35%
32. sun foreground”
Figure1: SecondordereffectsofCLIP’sneurons.Top:Weanalyzethesecond-ordereffectsofneuronsin
CLIP-ViT(flowinpink).Bottom-left:Eachsecond-ordereffectofaneuroncanbedecomposedtoasparseset
ofworddirectionsinthejointtext-imagespace.Bottom-right:co-appearingwordsinthesesetscanbeusedfor
mass-generationofsemanticadversarialimages.
second-ordereffectissignificantforonlyasmallset(about2%)oftheimages. Finally,thiseffectcan
beapproximatedbyasingledirectioninthejointtext-imagerepresentationspaceofCLIP(Sec.3.3).
As each direction that corresponds to a neuron lives in a joint representation space, it can be
decomposedasasparsesumoftextrepresentationsthatdescribestheneurons’functionality(see
Fig.1).Thesetextrepresentationsshowthatneuronsarepolysemantic[11]—eachneuroncorresponds
tomultiplesemanticconcepts. Toverifythattheneurondecompositionsaremeaningful,weshow
thattheseconceptscorrectlytrackwhichinputsactivateagivenneuron(Sec.4).
The polysemantic behavior of neurons allows us to find concepts that inadvertently overlap in
thenetwork, duetobeingrepresentedbythesameneuron. Weusethesespuriouscuesformass
productionof“semantic”adversarialexamplesthatwillfoolCLIP(seebottomofFig.1). Weapply
thistechniquetoautomaticallyproduceadversarialimagesforavarietyofclassificationtasks. Our
qualitativeandquantitativeanalysisshowsthatincorporatingspuriouslyoverlappingconceptsinan
imagedeceivesCLIPwithasignificantsuccessrate(Sec.5.1).
Wepresenttwoadditionalapplications–discoveryofconceptsinimages,andzero-shotsegmentation.
Forconceptdiscovery,weinterprettheconceptsthatCLIPassociateswithagivenimagebyaggre-
gatingthetextdescriptionsofthesparsely-activatedneuronsforthatimage. Forsegmentation,we
cangenerateattributionheatmapsfromtheactivationpatternsofneurons. Wedothisbyidentifying
class-relevantneuronswiththesecond-orderlensandaveragingtheirheatmaps. Thisyieldsastrong
zero-shotimagesegmenterthatoutperformsrecentwork[8,13].
Insummary, wepresentanautomatedinterpretabilityapproachforCLIP’sneuronsbymodeling
theirsecond-ordereffectsandspanningthemwithtextdescriptions. Weusethesedescriptionsto
automaticallyunderstandneuronrolesandapplythistothreeapplications. Thisshowsthatascalable
understandingofinternalmechanismsbothuncoverserrorsandelicitsnewcapabilitiesfrommodels.
2 Relatedwork
Contrastivevision-languagemodels. ModelslikeALIGN[19],CLIP[32],anditsvariants[45,22]
produceimagerepresentationsfrompre-trainingonimagesandtheircaptions. Theydemonstrated
impressivezero-shotcapabilitiesforvariousdownstreamtasks,includingOCR,geo-localization,
andclassification[44]. Thesemodels’representationsarealsousedforsegmentation[23],image
generation[34,36]and3Dunderstanding[20]. Weaimtorevealtherolesofneuronsinsuchmodels.
Mechanisticinterpretabilityofvisionmodels. Mechanisticinterpretabilityaimstoreverseengineer
thecomputationprocessinneuralnetworks. Incomputervision,thisapproachwasappliedtoextract
intermediatemechanismslikecurvedetectors[29],objectsegmenters[3,2],high-frequencyboundary
detectors[37],andmultimodalconceptsdetectors[15]. Morecloselytous,afewworksmadeuseof
theintrinsiclanguage-imagespaceofCLIPtointerpretthedirecteffectofattentionheadsandthe
outputrepresentationinCLIPwithautomatictextdescriptions[13,4]. Wegobeyondtheoutputand
directeffectsofindividuallayerstointerpretintermediateneuronsinCLIP.
2Neuronsinterpretability. Theroleofindividualneurons(post-non-linearitysinglechannelacti-
vations)isbroadlystudiedincomputervisionmodels[3,2,15]andlanguagemodels[31,14,25].
[10,17]demonstratethatneuronscanlearnuniversalmechanismsacrossdifferentmodelsinboth
domains. [11]showthatneuronscanbepolysemantic(i.e. activatedonmultipleconcepts)andexploit
thispropertyforgenerationofL2adversarialexamples.Someworkseekstoextractneurons’concepts
bylearningsparsedictionaries[7,33]. Othermethodsuselargelanguagemodelstoautomatically
describeneuronsbasedonwhichexamplestheyactivateon[5,28,18,40]. Incontrast,wefocuson
thecontributionofneuronstotheoutputrepresentation.
3 Second-ordereffectsofneurons
WestartbypresentingtheCLIP-ViTarchitecture. Then,wederivethesecond-ordereffectofneurons
andpresenttheirbenefitsoverfirst-orderandtheindirecteffects. Finally,weempiricallycharacterize
thesecond-ordereffects,settingthestageforautomaticallyinterpretingthemviatextinSec.4.
3.1 CLIP-ViTPreliminaries
Contrastivepre-training. CLIPistrainedviaacontrastivelosstoproduceimagerepresentations
fromweaktextsupervision. ThemodelincludesanimageencoderM andatextencoderM
image text
thatmapimagesandtextdescriptionstoasharedlatentspaceRd. Thetwoencodersaretrained
togethertomaximizethecosinesimilaritybetweentheoutputrepresentationsM (I)andM (t)
image text
formatchinginputtext-imagepairs(t,I):
sim(I,t)=⟨M (I),M (t)⟩/(||M (I)|| ||M (t)|| ). (1)
image text image 2 text 2
Using CLIP for zero-shot classification. Given a set of classes, each name of a class c (e.g.
i
the class “dog”) is mapped to a fixed template template(c ) (e.g. “A photo of a {class}”), and
i
encoded via the text encoder M (template(c )). The classification prediction for a given
text i
image I is the class c whose text representation is most similar to the image representation:
i
argmax sim(I,template(c )).
ci i
CLIP-ViTarchitecture. TheCLIP-ViTimageencoderconsistsofaVisionTransformerfollowed
byalinearprojection2. Thevisiontransformer(ViT)isappliedtotheinputimageI ∈RH×W×3to
obtainad′-dimensionalrepresentationViT(I). DenotingtheprojectionmatrixbyP ∈Rd×d′:
M (I)=P(ViT(I)). (2)
image
The input I to ViT is first split into K non-overlapping image patches that are encoded into K
d′-dimensionalimagetokens. Anadditionallearnedtoken,namedtheclasstoken,isincludedand
used later as the output token. As shown in Fig. 1, the tokens are processed simultaneously by
applyingLalternatingresiduallayersofmulti-headself-attention(MSA)andMLPblocks.
MLPneuronsinCLIP.TheMLPlayersareappliedseparatelyoneachimagetokenandtheclass
token. Theyconsistofaninputlinearlayer,parametrizedbyWl ∈ RN×d′,followedbyaGELU
in
non-linearityσandanoutputlinearlayer,parametrizedbyWl ∈Rd′×N. Herelisthelayernumber
out
and N is the width (number of neurons) of the MLP. We next analyze the contributions of each
individualneuronn∈{1,...,N}foreachlayer.
3.2 Analyzingtheneuroneffectsontheoutput
Individualneuronshavedifferenttypesofcontributionstotheoutput—thefirst-order(direct)effects,
second-ordereffects,and(higher-order)indirecteffects.Weintroducethemandexplainthelimitations
ofthedirectandindirecteffectsbeforecontinuingtocharacterizethesecond-ordereffectsinSec.3.3.
First-ordereffects(logitlens[27]). Thefirst-ordereffectisthedirectcontributionofacomponent
totheresidualstream,multipliedbytheprojectionlayer(seeblueflowinFig.2). Foranindividual
neuronninlayerl,letpl,n(I)∈Rdenoteitspost-GELUactivationonthei-thtokenoftheinput
i
imageI. Thenthecontributionel,nofthen-thneurontothei-thtokenintheresidualstreamis:
i
el,n =pl,n(I)wl,n (3)
i i
2Throughoutthepaper,weignorelayer-normalizationtermstosimplifyderivations. Weaddresslayers-
normalizationindetailinA.5.
3First order MSA l’
60
Second order Wl’,1 Head 1
VO
MLP l Wl’,2 Head 2 50 baseline
VO w/o all neurons
w/o large norms
40
wl,n W Vl’,H O Head H Projection w/o small norms
rec. from PC #1
30
P 0 1 2 3 4 5 6 7 8 9 10
Layer index
Figure2: First/Second-ordereffects.Thefirstorder Figure 3: Mean-ablation of second order effects
istheflowcomingfromaneurontotheprojectionlayer (ViT-B-32). WeevaluatetheperformanceonINtest
and the output (blue). The second order goes from set. Second-order effects concentrate in late layers,
asingleneuronthroughalltheconsecutiveattention significantforonlyapartoftheimages,andcanbe
heads,totheprojectionlayer,andtotheoutput(pink). approximatedbyonedirectionintheoutputspace.
wherewl,n ∈ Rd′ isthethen-thcolumnofWl . Astheoutputrepresentationistheclasstoken
out
(indexed0)multipliedbyP,thefirst-ordereffectforneuronnontheoutputisPel,n.
0
AsobservedbyGandelsmanetal.[13],thefirst-ordereffectsofMLPlayersareclosetoconstantsin
CLIPandmostofthefirst-ordercontributionsarefromthelateattentionlayers. Wethereforefocus
onthesecond-ordereffects: theflowofinformationfromtheneuronsthroughtheattentionlayers.
Second-ordereffects. Thecontributionel,ntotheresidualstreamdirectlyaffectstheinputtolater
i
layers. Wefocusontheflowofel,nthroughsubsequentMSAsandthentotheoutput(pinkflowin
i
Fig.2). Wecallthisinterpretabilitylensthe“second-orderlens”,inanalogytothe“logitlens”.
Following[12],theoutputofanMSAlayerMSAl thatcorrespondstotheclasstokenisaweighted
sumofitsK+1inputtokens[z ,...,z ]:
0 K
H K
(cid:104) (cid:105) (cid:88)(cid:88)
MSAl([z ,...,z ]) = al,h(I)Wl,hz (4)
0 K i VO i
0
h=1i=0
whereWl,h ∈ Rd′×d′ aretransitionmatrices(theOVmatrices)andal,h(I) ∈ Raretheattention
VO i
weightsfromtheclasstokentothei-thtoken((cid:80)K al,h =1).
i=0 i
Toobtainthesecond-ordereffectofaneuronnatlayerl,ϕl(I),wecomputetheadditivecontribution
n
oftheneuronthroughallthelaterMSAsandprojectittotheoutputspaceviaP. PlugginginEq.3as
thecontributiontoz inEq.4andsummingoverlayers,thesecondordereffectofneuronnisthen:
i
L H K
ϕl(I)=
(cid:88) (cid:88)(cid:88) (cid:16) pl′,n(I)al′,h(I)(cid:17) (cid:16) PWl′,hwl′,n(cid:17)
(5)
n i i VO
l′=l+1h=1i=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
attention-weightedactivations input-independent
Indirecteffects.Analternativeapproachistoanalyzetheindirecteffectofaneuronbymeasuringthe
changeinoutputrepresentationwheninterveningonaneuron’soutput. Specifically,theintervention
isdonebyreplacingtheactivationpl,noftheneuronforeachtokenwithapre-computedper-token
i
mean. However,aswasshownbyMcGrathetal.[24],modelsoftenlearn“self-repair”mechanisms
thatcanobscuretheindividualrolesofneurons. Weillustratetheseissuesinthenextsection.
3.3 Characterizingthesecond-ordereffects
Weanalyzetheempiricalbehaviorofthesecond-ordereffectsofneuronsϕl derivedintheprevious
n
section. WefindthatonlyneuronsfromthelateMLPlayershaveasignificantsecond-ordereffect
andthateachindividualneuronhasasignificanteffectforlessthan2%oftheimages. Finally,we
showthatϕl canbeapproximatedbyonelineardirectionintheoutputspace. Thesefindingswill
n
helpmotivateouralgorithmfordescribingoutputspacesofneuronswithtextinSec.4.
Experimentalsetting. Toevaluatethesecond-ordereffectsandtheircontributionstotheoutput
representation, we measurethe downstream performanceon theImageNet classificationtask [9]
4
)%(
ycaruccA60
55
50
variance
accuracyafter Baseline (rec. from PC #1)
effecttype mean-ablation explained 45 Most common words (10k)
byfirstPC Most common words (30k)
40 IN Class Descriptions
indirect 52.3 11.0
second-order 29.6 48.2 22 23 24 25 26 27
Descriptions per Neuron
Table1: Comparisontoindirecteffect. Wecom- Figure4: Accuracyforneuronreconstructedfrom
parethesecond-ordereffectsandtheindirecteffects sparsetextrepresentations(ViT-B-32,layer9).We
bymean-ablatinglayer9inViT-B-32onImageNet evaluatethesparsetextdecompositionsfordifferent
testset. initialdescriptionpoolsanddescriptionsetsizes.
afterablatingtheseeffectsforeachneuron. Specifically,weapplymean-ablation[26],replacingthe
additivecontributionsofindividualϕl(I)’stotherepresentationwiththemeancomputedacrossa
n
datasetD. Inourexperiments,wemean-ablatealltheneuronsinalayersimultaneouslyandevaluate
thedownstreamclassificationperformancebeforeandafterablation. Componentswithlargereffects
shouldresultinlargeraccuracydrops.
WetakeDtobe∼5000imagesfromtheImageNet(IN)trainingset.Wereportzero-shotclassification
accuracyontheINtestset. OurmodelisOpenAI’sViT-B-32CLIP,whichhas12layers(additional
resultsforViT-L-14areinApp.A.1)
Second-ordereffectsconcentrateinmoderatelylatelayers. Weevaluatethecontributionsofall
theϕl acrossdifferentlayersandobservethattheneuronswiththemostsignificantsecond-order
n
effectsappearrelativelylateinthemodel. TheresultsfordifferentlayersinViT-B-32CLIPmodel
arepresentedinFig.3(“w/oallneurons”). Asshown,mean-ablatinglayers8-10leadstothelargest
dropinperformance. TheselayersappearrightbeforetheMSAlayerswiththemostsignificantdirect
effects,asshownin[13](layers9-11;seeApp.A.2).
Thesecond-ordereffectissparse. Wefindthatthesecond-ordereffectofeachindividualneuronis
significantonlyforlessthan2%oftheimagesacrossthetestset. Werepeatthesameexperimentas
before,butthistimeweonlymean-ablateϕl(I)forasubsetofimages,whilekeepingtheoriginal
n
effectsforotherimages. Wefindthatformostoftheimages,exceptthesubsetofimagesinwhich
ϕl(I)hasalargenorm,wecanmean-ablateϕl(I)withoutchangingtheaccuracysignificantly,as
n n
showninFig.3(“w/osmallnorm”). Differently,mean-ablatingthecontributionsforthe100images
withthelargestϕl(I)normsresultsinasignificantdropinperformance(“w/olargenorm”).
n
Thesecond-ordereffectisapproximatelyrank1. Whilethesecond-ordereffectforagivenneuron
canwritetodifferentdirectionsinthejointrepresentationspaceforeachimage,wefindthatϕl(I)
n
can be approximated by one direction rl ∈ Rd in this space, multiplied by a coefficient xl(I)
n n
thatdependsontheimage. WeusethesetSl,whichcontainsthelargestsecond-ordereffectsin
n
normfromD,andsetrl tobethefirstprinciplecomponentcomputedfromSl. Weapproximate
n n
ϕl(I)withxl(I)rl +bl,wherebl ∈Rd isthebiascomputedbyaveragingϕl(I)acrossD,and
n n n n n n
xl(I)∈Risthenormoftheprojectionofϕl(I)ontorl.
n n n
Toverifythatthisapproximationrecoversϕl(I)wereplaceeachϕl(I)foreachneuronandimage
n n
inthetestsetwiththeapproximation. Wethenevaluatethedownstreamclassificationperformance.
AsshowninFig.3(“reconstructionfromPC#1”), foreachlayerl, thisreplacementresultsina
negligibledropinperformancefromthebaseline,thatusesthefullrepresentation.
Comparisontoindirecteffect. Wecomparethesecond-ordereffecttotheindirecteffectandpresent
thevarianceexplainedbythefirstprinciplecomponentforeachofthemandthedropinperformance
whensimultaneouslymean-ablatingalltheeffectsfromonelayer. AsshowninTab.1,Mean-ablating
theindirecteffectsresultsinasmallerdropinperformanceduetoself-repairbehavior. Moreover,
thefirstprinciplecomponentexplainssignificantlylessofthevarianceintheindirecteffect,thanin
thesecond-ordereffect. Thisdemonstratestwoadvantagesofthesecond-ordereffects—uncovering
neuronfunctionalitythatisobfuscatedbyself-repair,andone-dimensionalbehaviorthatcanbeeasily
modeledanddecomposed,aswewillshowinthenextsection.
5
)%(
ycaruccANeuron ImageNetclassdescriptions Commonwords(30k)
+“Picturewithfallingsnowflakes” +“snowy”
+“Pictureportrayingaperson[...]inextremeweatherconditions” +“frost”
#4
-“Picturewithabucketinaconstructionsite” +“closings”
+“Photographtakenduringaholidayservice” +“advent”
+“Imagewithatraditionalwoodensled” +“woodworking”
+“Imagewithawoodencuttingboard” -“swelling”
#391
+“Pictureshowcasingbeachaccessories” +“cedar”
-“Photographwithasyringeandasurgicalmask” +“heirloom”
+“Photowithalimegarnish” +“refreshments”
+“Imagewithcandiesinglasscontainers” +“gelatin”
#2137
-“Picturefeaturinglifeboatequipment” +“sour”
+“Close-upphotoofameltingpopsicle” +“cosmopolitan”
+“Photothatfeaturesastretchlimousine” +“motorhome”
+“Imagecapturingasuitwithpinstripes” +“yacht”
#2914
+“Caricaturewithacelebrityendorsingthebrand” +“cirrus”
+“ImageshowcasingaBullmastiff’sprominentneckfolds” +“cabriolet”
Table2: Examplesofsparsedecompositions.Foreachneuron,wepresentthetop-4textscorrespondingtothe
sparsedecompositionwithm=128andthesignsofthecoefficientsinthedecompositionfortwoinitialpools.
Neuron
#4
Neuron
#391
Neuron
#2137
Neuron
#2914
Figure5: Imageswithlargestsecond-ordereffectnormperneuron.Wepresentthetopimagesfrom10%
ofImageNettestsetfortheneuronsinTab.2.Notethatneuronsarepolysemantic-theyhavelargesecond-order
effectsonimagesthatshowmultipleconcepts(e.g.carsandboats).
4 Sparsedecompositionofneurons
Weaimtointerpreteachneuronbyassociatingitssecond-ordereffectwithtext. Webuildonthe
previousobservationthateachsecond-ordereffectofaneuronϕl isassociatedwithavectordirection
n
rl. Sincerl liesinasharedimage-textspace,wecandecomposeittoasparsesetoftextdirections.
n n
Weuseasparsecodingmethod[30]tomineforasmallsetoftextsforeachneuron,outofalarge
poolofdescriptions. Weevaluatethefoundtextsacrossdifferentinitialpoolswithdifferentsetsizes.
Decomposinganeuronintoasparsesetofdescriptions. Giventhefirstprincipalcomponentofthe
second-ordereffectofeachneuron,rl,wewilldecomposeitasasparsesumoftextdirectionst :
n j
rl ≈rˆl =(cid:80)M γl,nM (t ). Todothis,westartfromalargepoolT ofM textdescriptions(e.g.
n n j=1 j text j
themostcommonwordsinEnglish). Weapplyasparsecodingalgorithmtoapproximaterl asthe
n
sumabove,whereonlymoftheγl,n’sarenon-zero,forsomem≪M.
j
Experimentalsettings. Weverifythatthereconstructedrˆl fromthetextrepresentationscapturesthe
n
variationintheimagerepresentation,asmeasuredbyzero-shotaccuracyonIN.Wesimultaneously
replacetheneurons’second-ordercontributionsinasinglelayerwiththeapproximationxl(I)rˆl +bl.
n n n
Toobtainsparsedecompositionforeachneuron,weusescikit-learn’simplementationoforthogonal
matchingpursuit[30]. WeconsidertwostrategiesforconstructingthepooloftextdescriptionsT.
Thefirsttypeissinglewords-the10kand30kmostcommonwordsinEnglish. Thesecondtype
isimagedescriptions-wepromptChatGPT-3.5toproducedescriptionsofimagesthatincludean
6horse automobile dog deer frog bird truck ship ship automobile
A horse is driven by a A dog is running through a A poodle wearing a ring A duo of trainers are run- A ship sailing through a
trucker on a road with a fishery, chasing after a and holding a frog, ning alongside a truck, with quartet of cyclists on a
sign saying "Emissions" in squirrel and a rabbit, with standing next to a sign a pirate flag waving in the Brooklyn road, with a hus-
the background. a scenic view of the saying "Thanksgiving" in a background, and a sea of band and wife driving a mo-
surrounding forests. festive atmosphere. people cheering them on. torcycle in the background.
Figure6: Adversarialimagesgeneratedbyourmethod. Foreachbinaryclassificationtask,wepresent
thegeneratedimages,theinputtexttothetext-to-imagemodel(wordsfromWvarebold),andanattribution
map[13]fortheclassification(areasthatcontributetotheincorrectclassscorearered).
objectofaspecificclass. RepeatingthisprocessforalltheINclassesresultsin∼28kuniqueimage
descriptions. Wethenevaluatethereconstructionofrl fordifferentm’sandpools.
n
Effectofsparsesetsizemanddifferentpools. Weexperimentwithm ∈ {4,8,16,32,64,128}
andthethreetextpools,andpresenttheaccuracyon10%ofINtestsetinFig.4. Weapproachthe
originalclassificationaccuracywith128textdescriptionsperneuronreconstructionrˆl. Usingfull
n
descriptionsoutperformsusingsinglewordsforthetextpool,butthegapvanishesforlargerm.
Qualitativeresults. Wepresenttheimageswiththelargestsecond-ordernormsinFig.5,andthe
correspondingtop-4textdescriptionsinTab.2. Asshown,thefounddescriptionsmatchtheobjectsin
thetop10images. Moreover,someindividualneuronscorrespondtomultipleconcepts(e.g. writing
bothtoward“yacht”andatypeofacar-“cabriolet”). Thiscorroborateswithpreviousliterature
on neurons’ polysemantic behavior [11] - single neurons behave as a superposition of multiple
interpretablefeatures. ThispropertywillallowustogenerateadversarialimagesinSec.5.1.
5 Applications
5.1 Automaticgenerationofadversarialexamples
Thesparsedecompositionofrl’sallowsustofindoverlappingconceptsthatneuronsarewriting
n
to. We use these spurious cues to generate semantic adversarial images. Our pipeline, shown in
Fig.1,minesforspuriouswordsthatcorrelatewiththeincorrectclassinCLIP(e.g. “elephant”,that
correlateswith“dog”),combinesthemintoimagedescriptionsthatincludethecorrectclassname
(“cat”),andgeneratesadversarialimagesbyprovidingthesedescriptionstoatext-to-imagemodel.
Weexplainthestepsinthepipelineandprovidequantitativeandqualitativeresults.
Findingrelevantspuriouscuesinneurons. Giventwoclassesc andc ,wefirstselectneurons
1 2
that contribute the most toward the classification direction v = M (c )−M (c ), then mine
text 2 text 1
theirsparsedecompositionsforspuriouscues. Specifically,weextractthesetofneuronsN whose
directionsaremostsimilartov: N =top-k |⟨v,rl⟩|. Utilizingthesparsedecompositionfrom
n∈N n
before,wecomputeacontributionscorew foreachphrasej inthepoolT:
j
(cid:88)
wv = γl,n⟨v,rl⟩. (6)
j j n
n∈N
ThislooksattheweightthateachneuroninN assignstoj initssparsedecomposition,weightedby
howimportantthatneuronisforclassification. Aphrasewithahighcontributionscorehassignificant
weightinoneormoreimportantneurons,andsoisapotentialspuriouscue. Thetopphrases,sorted
bythecontributionscorearecollectedintoasetofphrasecandidatesW .
v
Generating“semantic”adversarialexamples. Weusetextandimagegenerativemodelstocreate
exampleswiththeobjectc thatareclassifiedasc . First,wegenerateimagedescriptionswitha
2 1
70.39 ducks 0.69 violin Input image
0.20 chickens 0.49 guitar
0.19 eagle 0.40 chords
0.16 clover 0.32 faceted
0.16 carmel 0.32 cranes
0.16 park 0.27 elixir
0.14 hollister 0.25 sweetwater
0.14 golfing 0.24 additives TextSpan
0.14 goose 0.24 cello
0.13 wynn 0.23 parlor
1.29 primates 0.90 bridge
0.66 chimp 0.36 fog
0.63 alejandro 0.26 staten
0.58 zoology 0.23 tektronix Ours
0.58 kong 0.23 nel
0.47 bolivia 0.21 postmaster
0.4 inverter 0.21 bridges
0.39 ears 0.20 yugioh
0.37 motif 0.20 continually
0.36 chests 0.20 lisbon
Figure7: Conceptdiscoveryinimages(ViT-B-32). Figure8: QualitativeresultsonIN-Segmentation
Weincludetop-10wordsdiscoveredbyaggregating (ViT-B-32). Ourheatmapscapturemoreobjectparts
wordsinsparsedecompositionsofactivatedneurons. thanthefirst-ordertokendecompositionof[13].
largelanguagemodel(LLM)byprovidingitphrasesfromthesetWv andtheclassnamec and
1
promptingittogenerateimagedescriptionsthatincludeelementsfromboth. Wepromptthemodel
toexcludeanythingrelatedtoc fromthedescriptionsandusevisuallydistinctivewordsfromW .
2 v
Theresultingdescriptionsarefedintoatext-to-imagemodeltogeneratetheadversarialimages. Note
thattheadversarialimageslieonthemanifoldofgeneratedimages,differentlyfromnon-semantic
adversarialattacksthatmodifyindividualpixels.
Experimentalsettings. Wegenerateadversarialimagesforclassifyingbetweenpairsofclasses
fromCIFAR-10[21]. Weusethe30kmostcommonwordsasourpoolT. Wechoosethetop100
neurons from layers 8-10 for N, and the top 25 words according to their contribution scores for
promptingtheLLM.WepromptLLaMA3[42]togenerate50descriptionsforeachclassificationtask
(seepromptinApp.A.6). Wethenfilteroutdescriptionsthatincludetheclassnameandchoose10
randomdescriptions. Wegenerate10imagesforeachdescriptionwithDeepFloydIFtext-to-image
model [41]. This results in 100 images per experiment. We repeat the experiment 3 times and
manuallyremoveimagesthatincludec objectsordonotincludec objects.
2 1
Wereportthreeadditionalbaselines. First,werepeatthesameprocesswith100randomneurons
insteadofthesetN. Second,werepeatthesamegenerationprocesswithsparsetextdecompositions
computedfromthefirstprinciplecomponentsoftheindirecteffectsinsteadofthesecond-ordereffect.
Third,wedonotrelyontheneurondecompositions,andinsteadpromptthelanguagemodelwiththe
wordsfromM forwhichtheirtextrepresentationsarethemostsimilartov. Bothforourpipelineand
thebaselines,weautomaticallyfilteroutsynonymsofc fromthephrasesprovidedtothelanguage
2
modelaccordingtotheirsentencesimilaritytoc [35].
2
Quantitativeresults. Theclassificationaccuracyresultsfortheadversarialimagesarepresented
inTab.3. Thesuccessrateofouradversarialimagesissignificantlyhigherthantheindirecteffect
baseline,thesimilarwordsbaseline,andtherandombaseline,whichsucceedsonlyaccidentally.
Qualitativeresults. Fig.6includesgeneratedadversarialexamplesandthedescriptionsthatwere
used in their generation. The presented attribution heatmaps [13] show that the found spurious
objectsfromW contributethemosttothemisclassification,whiletheobjectfromthecorrectclass
v
(e.g. ahorseintheleft-mostimage)contributestheleast. Weprovidemoreresultsforadditional
classificationtasks(e.g. “stop-signv.s. yield”)inFig.13.
Weshowthatunderstandinginternalcomponentsinmodelscanbegroundedbyexploitingthemfor
adversarialattacks. Ourattackisoptimization-freeandisnotcompute-intensive. Hence,itcanbe
usedformeasuringinterpretabilitytechniques,withbetterunderstandingleadingtoimprovedattacks.
5.2 Conceptdiscoveryinimages
We aim to discover concepts in image I, by aggregating phrases that correspond to the neurons
thatareactivatedonI. Here,westartfromthesetofactivatedneuronsN (forwhich||ϕl(I)|| is
n 2
abovethe98thpercentileofnormscomputedacrossINimages). Similarlytothecontributionscore
describedearlier, wecomputeanimage-contributionscorewI foreachphrasej accordingtoits
j
combinedweightinthedecompositionsofneuronsinN. Formally,wI istheoverallsumofweights
j
8Indirect Similar Second Pix.Acc.↑ mIoU↑ mAP↑
Task Random
effect words order
Partial-LRP[43] 55.0 35.5 66.9
horse→ Rollout[1] 61.8 42.6 74.0
automobile
1.0(±1.4) 2.8(±3.7) 1.0(±1.4) 5.3(±1.9)
LRP[6] 62.9 35.8 68.5
dog→deer 0.3(±0.5) 6.3(±4.8) 3.3(±0.9) 22.7(±0.5) GradCAM[39] 67.3 39.3 61.9
bird→frog 0.3(±0.5) 1.0(±1.4) 5.0(±2.9) 8.0(±4.5) Cheferetal.[8] 68.9 49.1 79.7
ship→truck 0.0(±0.0) 0.0(±0.0) 0.0(±0.0) 5.7(±0.9) Raw-attention 69.6 49.8 80.0
ship→ TextSpan[13] 76.5 58.1 84.1
automobile
1.3(±1.9) 0.0(±0.0) 1.3(±0.9) 7.0(±4.5)
Ours 78.1 59.0 84.9
Table3: Accuracyofadversarialimages.Wereporthow Table 4: Segmentation performance on IN-
manygeneratedimagesoutof100,fooledthebinaryclassi- segmentation.Ourzero-shotsegmentationismore
fier(standarddeviationinparentheses). accuratethanpreviousmethodsacrossallmetrics.
thateachneuroninN assignstoj initsdecomposition,weightedbytheneuronsecond-ordernorms:
wI = (cid:80) γl,n||ϕl(I)|| . Thephraseswiththehighestimage-contributionscorearepickedto
j n∈N j n 2
describetheimageconcepts.
Qualitativeresults. Wepresentqualitativeresultsforneuronsandthetop-10discoveredconcepts
fromlayer9ofViT-B-32inFig.7,whenusingthemostcommonwordsasthepool. Thenumber
ofneuronsactivatedontheseimages,|N|,isbetween29and59,lessthan2%oftheneuronsinthe
layer. Nevertheless,thetopwordsextractedfromtheseneuronsrelatesemanticallytotheobjectsin
theimageandtheirlocations. Surprisingly,thetopwordforeachoftheimagesappearsonlyinone
ortwooftheneuronsparsedecompositionsandisnotspreadacrossmanyactivatedneurons.
5.3 Zero-shotsegmentation
Finally,weuseourunderstandingofneuronsforzero-shotsegmentation. Eachneuroncorresponds
to an attribution map, by looking at its activations pl,n(I) on each image patch. Ensembling all
i
theneuronsthatcontributetowardsaconceptresultsinanaggregatedattributionmapthatcanbe
binarizedtogeneratereliablesegmentations.
Specifically,togenerateasegmentationmapforanimageI,wefindasetofneuronswiththelargest
absolutevalueofthedotproductwiththeencodedclassnameweaimtosegment: |⟨rl,M (c )⟩|.
n text i
Wethenaveragetheirspatialactivationmapspl,n(I),standardizetheaverageactivationsinto[0,1],
i
andbinarizethevaluesintoforeground/backgroundsegmentsbyapplyingathresholdof0.5.
Segmentation results. We provide results on IN-Segmentation [16], which includes fore-
ground/background segmentation maps of IN objects. We use activation maps from the top 200
neuronsoflayers8-10. Tab.4presentsaquantitativecomparisontopreviousexplainabilitymethods.
Ourmethodoutperformsotherzero-shotsegmentationmethodsacrossallstandardevaluationmetrics.
WeprovidequalitativeresultsbeforethresholdinginFig. 8. Whilethefirst-ordereffectshighlight
individualdiscriminativeobjectparts,ourheatmapscapturemorepartsofthefullobject.
6 Limitationsanddiscussion
Weanalyzedthesecond-ordereffectsofneuronsontheCLIPrepresentationandusedourunderstand-
ingtoperformzero-shotsegmentation,discoverimageconcepts,andgenerateadversarialimages.
Wepresentmechanismsthatwedidnotanalyzeinourinvestigationandconcludewithadiscussion
ofbroaderimpactandfuturedirections.
Neuron-attentionmapsmechanisms. Weinvestigatedhowtheneuronsflowthroughindividual
consecutiveattentionvalues,andignoredtheeffectofneuronsonconsecutivequeriesandkeysin
theattentionmechanism. Investigatingtheseeffectswillallowustofindneuronsthatmodifythe
attentionmappatterns. Weleaveitforfuturework.
Neuron-neuronmechanisms. Wedidnotanalyzethemutualeffectsbetweenneuronsinthesame
layeroracrossdifferentlayers. Returningtoouradversarial“frog/bird”attackexample,aneuron
thatwritestoward“dog”maynotbeactivatedifadifferentneuronwritessimultaneouslytoward
“frog”,thusreducingourattackefficiency. Whilewecanstillgeneratemultipleadversarialimages,
webelievethatunderstandingdependenciesbetweenneuronscanimproveitfurther.
9Futureworkandbroaderimpact. Themassproductionofadversarialimagescanbeharmfulto
systemsthatrelyonneuralnetworks. Ontheotherhand,automaticextractionofsuchcasesallows
thedefendertobepreparedforthemand,possibly,fine-tunethemodelonthegeneratedimagesto
avoidsuchattacks. WeplantoinvestigatethisapproachtoimproveCLIP’srobustnessinthefuture.
Acknowledgments. WewouldliketothankAlexanderPanforhelpfulfeedbackonthemanuscript.
YGissupportedbytheGoogleFellowship. AEissupportedinpartbyDoD,includingDARPA’s
MCSandONRMURI,aswellasfundingfromSAP.JSissupportedbytheNSFAwardsNo.1804794
&2031899.
References
[1] SamiraAbnarandWillemZuidema. Quantifyingattentionflowintransformers. InProceedingsofthe
58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4190–4197,Online,July
2020.AssociationforComputationalLinguistics.
[2] DavidBau,Jun-YanZhu,HendrikStrobelt,AgataLapedriza,BoleiZhou,andAntonioTorralba. Under-
standingtheroleofindividualunitsinadeepneuralnetwork. ProceedingsoftheNationalAcademyof
Sciences,2020.
[3] DavidBau,Jun-YanZhu,HendrikStrobelt,BoleiZhou,JoshuaB.Tenenbaum,WilliamT.Freeman,and
AntonioTorralba. Gandissection: Visualizingandunderstandinggenerativeadversarialnetworks. In
ProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR),2019.
[4] UshaBhalla,AlexOesterling,SurajSrinivas,FlavioP.Calmon,andHimabinduLakkaraju. Interpreting
clipwithsparselinearconceptembeddings(splice),2024.
[5] StevenBills,NickCammarata,DanMossing,HenkTillman,LeoGao,GabrielGoh,IlyaSutskever,Jan
Leike,JeffWu,andWilliamSaunders. Languagemodelscanexplainneuronsinlanguagemodels. https:
//openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,2023.
[6] AlexanderBinder,GrégoireMontavon,SebastianLapuschkin,Klaus-RobertMüller,andWojciechSamek.
Layer-wiserelevancepropagationforneuralnetworkswithlocalrenormalizationlayers. ArtificialNeural
NetworksandMachineLearning-ICANN2016,9887:63–71,2016.
[7] TrentonBricken,AdlyTempleton,JoshuaBatson,BrianChen,AdamJermyn,TomConerly,NickTurner,
Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
Schiefer,TimMaxwell,NicholasJoseph,ZacHatfield-Dodds,AlexTamkin,KarinaNguyen,Brayden
McLean,JosiahEBurke,TristanHume,ShanCarter,TomHenighan,andChristopherOlah. Towards
monosemanticity:Decomposinglanguagemodelswithdictionarylearning. TransformerCircuitsThread,
2023.
[8] HilaChefer,ShirGur,andLiorWolf. Transformerinterpretabilitybeyondattentionvisualization. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
782–791,June2021.
[9] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
[10] Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher. Rosetta neurons: Mining the
commonunitsinamodelzoo. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pages1934–1943,October2023.
[11] NelsonElhage,TristanHume,CatherineOlsson,NicholasSchiefer,TomHenighan,ShaunaKravec,Zac
Hatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,RogerGrosse,SamMcCandlish,JaredKaplan,
DarioAmodei,MartinWattenberg,andChristopherOlah. Toymodelsofsuperposition. Transformer
CircuitsThread,2022.
[12] NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac
Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathe-
matical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-
circuits.pub/2021/framework/index.html.
[13] YossiGandelsman,AlexeiA.Efros,andJacobSteinhardt. InterpretingCLIP’simagerepresentationvia
text-baseddecomposition. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
[14] MorGeva,RoeiSchuster,JonathanBerant,andOmerLevy.Transformerfeed-forwardlayersarekey-value
memories,2021.
10[15] Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert,
Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 2021.
https://distill.pub/2021/multimodal-neurons.
[16] MatthieuGuillaumin,DanielKüttel,andVittorioFerrari. Imagenetauto-annotationwithsegmentation
propagation. Int.J.Comput.Vision,110(3):328–348,dec2014.
[17] WesGurnee,TheoHorsley,ZifanCarlGuo,TaraRezaeiKheirkhah,QinyiSun,WillHathaway,Neel
Nanda,andDimitrisBertsimas. Universalneuronsingpt2languagemodels,2024.
[18] EvanHernandez,SarahSchwettmann,DavidBau,TeonaBagashvili,AntonioTorralba,andJacobAndreas.
Naturallanguagedescriptionsofdeepvisualfeatures. CoRR,abs/2201.11114,2022.
[19] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InInternationalConferenceonMachineLearning,2021.
[20] JustinKerr,ChungMinKim,KenGoldberg,AngjooKanazawa,andMatthewTancik. Lerf:Language
embeddedradiancefields,2023.
[21] AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages,2009.
[22] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichtenhofer,andKaimingHe.Scalinglanguage-image
pre-trainingviamasking,2023.
[23] TimoLüddeckeandAlexanderEcker. Imagesegmentationusingtextandimageprompts. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages7086–7096,
June2022.
[24] ThomasMcGrath,MatthewRahtz,JanosKramar,VladimirMikulik,andShaneLegg. Thehydraeffect:
Emergentself-repairinlanguagemodelcomputations,2023.
[25] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov. Locatingandeditingfactualassociations
inGPT. AdvancesinNeuralInformationProcessingSystems,36,2022. arXiv:2202.05262.
[26] NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmeasuresfor
grokkingviamechanisticinterpretability,2023.
[27] nostalgebraist. interpretinggpt:thelogitlens,2020.
[28] TuomasOikarinenandTsui-WeiWeng. Clip-dissect:Automaticdescriptionofneuronrepresentationsin
deepvisionnetworks,2023.
[29] ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShanCarter. Zoomin:
Anintroductiontocircuits. Distill,2020. https://distill.pub/2020/circuits/zoom-in.
[30] YagyenshC.Pati, RaminRezaiifar, andPerinkulamS.Krishnaprasad. Orthogonalmatchingpursuit:
recursive function approximation with applications to wavelet decomposition. Proceedings of 27th
AsilomarConferenceonSignals,SystemsandComputers,pages40–44vol.1,1993.
[31] Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering
sentiment,2017.
[32] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InMarinaMeilaandTongZhang,editors,
Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsof
MachineLearningResearch,pages8748–8763.PMLR,18–24Jul2021.
[33] SenthooranRajamanoharan,ArthurConmy,LewisSmith,TomLieberum,VikrantVarma,JánosKramár,
RohinShah,andNeelNanda. Improvingdictionarylearningwithgatedsparseautoencoders,2024.
[34] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,and
IlyaSutskever. Zero-shottext-to-imagegeneration. CoRR,abs/2102.12092,2021.
[35] NilsReimersandIrynaGurevych. Sentence-bert:Sentenceembeddingsusingsiamesebert-networks. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Association
forComputationalLinguistics,112019.
[36] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages10684–10695,2022.
[37] LudwigSchubert,ChelseaVoss,NickCammarata,GabrielGoh,andChrisOlah. High-lowfrequency
detectors. Distill,2021. https://distill.pub/2020/circuits/frequency-edges.
11[38] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeWGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaR
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5b:An
openlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InThirty-sixthConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022.
[39] RamprasaathR.Selvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,DeviParikh,and
DhruvBatra. Grad-cam:Visualexplanationsfromdeepnetworksviagradient-basedlocalization. InICCV,
pages618–626.IEEEComputerSociety,2017.
[40] TamarRottShaham, SarahSchwettmann, FranklinWang, AchyutaRajaram, EvanHernandez, Jacob
Andreas,andAntonioTorralba. Amultimodalautomatedinterpretabilityagent,2024.
[41] StabilityAI. DeepFloyd-IF. https://github.com/deep-floyd/IF,2023.
[42] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels,2023.
[43] ElenaVoita,DavidTalbot,FedorMoiseev,RicoSennrich,andIvanTitov. Analyzingmulti-headself-
attention:Specializedheadsdotheheavylifting,therestcanbepruned. InProceedingsofthe57thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages5797–5808,Florence,Italy,July2019.
AssociationforComputationalLinguistics.
[44] MitchellWortsman. Reaching80%zero-shotaccuracywithopenclip:Vit-g/14trainedonlaion-2b,2023.
[45] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training,2023.
12A Appendix
A.1 SecondorderablationsforViT-L
WerepeatthesameexperimentsfromSec.3.3forViT-L-14,trainedonLAIONdataset[38]. For
thismodel,weonlyuse10%ofImageNettestset. Here,themaximaldropinperformancewhen
ablatingthesecondorderisrelativelysmallerandisspreadacrossmorelayers. Nevertheless,the
samepropertiespresentedanddiscussedinSection3.3holdforthismodel.
A.2 Firstorderablations
Forthetwomodelsdiscussedabove,ViT-B-32andViT-L-14,weprovidethemean-ablationresults
for the first-order effects of MSA layers, as computed in [13]. For each model, we present the
performancebeforeandafteraccumulativemean-ablationofallthefirst-ordereffectsofMSAlayers.
AsshowninFigures10and11,theneuronswiththesignificantsecond-ordereffectsappearright
beforethelayerswiththesignificantfirst-ordereffects.
A.3 Additionaladversarialimages
Wepresentadditionalsemanticadversarialresults,generatedbyourmethodforViT-B-32,inFig-
ure13. Wedemonstrateawidevarietyoftasks,includingadditionalpairsfromCIFAR-10dataset,
andadversarialattacksrelatedtotrafficsigns(e.g. misclassificationbetweenastopsignandayield
signoracrossroad). Foreachimage,weprovidethetextusedforgeneratingit,andhighlightthe
spuriouscueswordsfromthesparsedecompositions.
A.4 Additionalsparsedecompositionresults
WeprovideadditionalexamplesofsparsedecompositionsofneuronsinTab.5andtheimageswith
the top norms for the second-order effects of the same neurons in Fig. 12. As shown, the found
descriptionsmatchtheobjectsinthetop10images.
A.5 DerivationswithLayerNormalization
InmanyimplementationsofCLIP,thereisalayernormalizationbetweentheVisionTransformerand
theprojectionlayerP. Inthiscase,therepresentationis:
M (I)=P(LN(ViT(I))) (7)
image
wheretheLN isthelayernormalization. Specifically,LN canbewrittenas:
(cid:20) (cid:21) (cid:20) (cid:21)
x−µ γ µγ
LN(x)=γ∗ √ +β = √ ∗x− √ −β , (8)
σ2+ϵ σ2+ϵ σ2+ϵ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=A =B
wherex ∈ Rd′ istheinputtoken,µ ,σ ∈ Rarethemeanandstandarddeviation,andγ,β ∈ Rd′
l l
arelearnedvectors. ToincludeAandBinthesecond-ordereffectofaneuronflow,wereplacethe
input-independentcomponentinEq.5,PWl′,hwl′,n,with:
VO
P(A∗Wl′,hwl′,n+ B
) (9)
VO c
WherecisanormalizationconstantthatsplitsBequallyacrossalltheneuronsthatcanadditively
contributetoit.
Exceptforthelayernormalizationbeforetheprojectionlayer,theinputtotheMSAlayersthatcomes
fromtheresidualstreamalsoflowsthroughalayernormalization. Thus,iftheinputtotheMSAlayer
inlayerlisthelistoftokens[zl,...zl ],theoutputthatcorrespondstotheclasstokenis:
0 K
H K
(cid:104) (cid:105) (cid:88)(cid:88)
MSAl([z ,...,z ]) = al,h(I)Wl,hLNl(z ), (10)
0 K i VO i
0
h=1i=0
13where LNl is the normalization layer at layer l, that can be parameterized similarly to Eq. 8 by
Al,Bl ∈Rd′. Wemodifythedefinitionofthesecond-ordereffectaccordingly:
ϕl(I)=
(cid:88)L (cid:88)H (cid:88)K (cid:16) pl′,n(I)al′,h(I)(cid:17)(cid:32)
P
(cid:32)
A∗Wl′,h(Al∗wl′,n+
Bl′
)+
B(cid:33)(cid:33)
, (11)
n i i VO cl′ c
l′=l+1h=1i=0
wherecl′ isisanormalizationcoefficientthatsplitsBl′ equallyacrossalltheneuronsbeforelayerl′.
Inallofourexperiments,weusethismodification. Mostoftheelementsinthemodificationadd
constantbiases. Therefore,theycanbeignoredinourexperimentsasinmanyoftheexperiments
constantbiasesdonotchangetheresults. Forexample,inourmean-ablationexperiment,wesubtract
themean,computedacrossadataset.
75.0
72.5
70.0
67.5 baseline
w/o all neurons
65.0 w/o large norms
w/o small norms
62.5
rec. from PC #1
0 2 4 6 8 10 12 14 16 18 20 22
Layer index
Figure9: ViT-L-14second-orderablations.
60
50 60
40
40
30
20 20
baseline baseline
10
attentions mean-ablation attentions mean-ablation
0
0
0 1 2 3 4 5 6 7 8 9 10 11 0 2 4 6 8 10 12 14 16 18 20 22
Layer index Layer index
Figure10: ViT-B-32first-orderMSAsablation. Figure11: ViT-L-14first-orderMSAsablation.
A.6 Prompts
WeprovidethepromptthatwasusedforgeneratingsentencesgiventhesetofwordsW ,aspresented
v
inSection5.1,inTab.6. ThispromptisgiventoLLAMA3model[42].
Additionally, we provide the prompt that was used for generating the pool of ImageNet class
descriptions, presented in Section 4. We prompt ChatGPT (GPT 3.5) with the prompt template
providedinTab.7.
A.7 Compute
Asourmethoddoesnotrequireadditionaltraining,thetimeofourexperimentsdependslinearlyon
theinferencetimeofCLIP(andothergenerativemodelsthatwereusedfortheadversarialimages
generation),andonthenumberofimagesweusefortheexperiments(∼5000inourcase). Allour
experimentswererunononeA100GPU.Themosttime-consumingexperiment—computingthe
per-layermean-ablationresultsforViT-L-14—took5days.
14
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccANeuron
#600
Neuron
#974
Neuron
#1517
Neuron
#2002
Figure12: Imageswithlargestsecond-ordereffectnormperneuron. Wepresentthetopimages
from10%ofImageNettestset,fortheneuronsinTab.5.
Neuron ImageNetclassdescriptions Commonwords(30k)
+“Imagewithawiry,weather-resistantcoat” +“tents”
+“Imageshowcasingacompactandlightweightsleepingbag” +“svalbard”
#600
+“Pictureofacampertowingbicycles” +“miles”
+“ImagewithaBorderTerrierjumping” -“mountainous”
-“Photographtakenduringarace” +“runners”
-“Silhouetteofarunningdog” +“races”
#974
-“Picturetakeninafishingcompetition” -“dolphin”
+“Silhouetteofhammerheadsharkwithotheroceancreatures” +“expiration”
+“Chairwithafootpedalcontrol” +“bus”
-“Picturethatcapturesthebreed’sintelligence” -“filings”
#1517
-“Imagewithsnow-cappedmountainsasscenery” -“percussion”
+“Imagewithgraffitionatrain” +“wheelchairs”
+“Imagedepictingasustainablelivingoption” -“genres”
+“Phototakeninatrainyard” +“governance”
#2002
-“Imagefeaturingsnow-coveredrooftops” +“‘gravel”
+“Rescueequipment” +“conserve”
Table5: Additionalexamplesofsparsedecompositionresults. Foreachneuron,wepresentthe
top-4textscorrespondingtothesparsedecompositionwithm=128andthesignsofthecoefficients
inthedecomposition.
Youareacapableinstruction-followingAIagent.
Iwanttogenerateanimagebyprovidingimagedescriptionsasinputtoatext-to-imagemodel.
Theimagedescriptionsshouldbeshort. Eachofthemmustincludetheword"{class_1}".
Theymustnotincludetheword"{class_2}",anysynonymofit,orapluralversion!
Theimagedescriptionsshouldincludeasmanywordsaspossiblefromthenextlistandalmostno
otherwords:
{list}
Donotusenamesofpeopleorplacesfromthelistunlesstheyarefamousandthereissomething
visually distinctive about them. In each of the image descriptions mention as many objects and
animalsaspossiblefromthelistabove. Ifyouwanttomentiontheplaceinwhichtheimageistaken
oranameofaperson,describeitwithvisuallydistinctivewords. Forexample,if"Paris"isinthelist,
insteadofsaying"... inParis",say"... withtheEiffelTowerinthebackground"or"... nexttoasign
saying’Paris’". Don’tmentionwordsthataretoosimilarto"{class_2}",eveniftheyareinthelist
above. Forexample,ifthewordwas"tree"youshouldnotmention"trees","bush"or"eucalyptus".
Onlyusewordsthatyouknowwhattheymean.
Generatealistof50imagedescriptions.
Table6: Thelanguagemodelpromptforgeneratingimagedescriptions.
15stop sign stop sign stop sign
dog deer dog deer yield yield yield
A dog is walking with a pat- A dog is running through a A group of people wandered A stop sign stands in front of A stop sign stands in front of
terned leash through a forest, chasing after a through a market filled with a building with a sign that a building with a sign that
forest with rabbits and squirrel, with a helicopter cans, eggs, and perfumes, says "Yu's Banking Ser- says "Yu's Banking Ser-
squirrels, with a symmetri- flying overhead and a pat- with a stop sign in the dis- vices". vices".
cal patterned tree in the terned stream in the dis- tance.
background. tance. cat vacuum cat vacuum
dog cat frog bird frog bird cleaner cleaner
A dog is sitting on a moon- A writer sitting on a winged A frog riding on the back of A cat is playing with a A cat is brushing its fur with
light, looking at a group of pony, holding a poodle and an elephant, with auras of hockey stick near a shovel a blunt comb, surrounded
owls perched on a nearby wearing a yuri-themed hat, purple and orange sur- and a venous injection kit. by drops of ethanol and a
branch. with a frog on its shoulder. rounding them. dvr recording in the corner.
dog cat bird frog bird frog bird frog bird cat
A dog is sitting on a moon- A bird sits on a turtle's back, A bird perched on a green A bird perched on a pug's A tank driving through a
light, looking at a group of as it swims in a pool filled fence, with a turtle swimming back, with a green emerald jungle, bird soaring above
owls perched on a nearby with reptiles and butter- in the nearby pond and a fred in its beak and a tues flag
branch. flies. fisherman in the distance. waving in the wind.
stop sign stop sign stop sign stop sign
bird horse crossroad crossroad crossroad crossroad
A bird feeding from a hand A stop sign stands tall in a A stop sign marks the end of A stop sign is placed on a A stop sign is painted on a
while elephants bathe in a gorge, surrounded by blocks a journey, with a grandson block of wood, with a chick- rock, with a chicken
river. of colorful rocks, with a and his grandfather sitting en sitting on top, and a perched on top, and a path-
chicken perched on top, and on a bench, surrounded by crossword puzzle laid out way leading to a distant
a pathway leading to a dis- perfumes and blocks. below. journey.
tant marathon finish line.
Figure13: Additionaladversarialexamplesgeneratedbyourmethod. Weprovidethesentence
thatwasgiventothetext-to-imagemodeltogenerateit. WordsfromWv arehighlightedinbold.
Provide40imagecharacteristicsthataretrueforalmostalltheimagesof{class}. Beasgeneralas
possibleandgiveshortdescriptionspresentingonecharacteristicatatimethatcandescribealmost
allthepossibleimagesofthiscategory. Don’tmentionthecategorynameitself(whichis“{class}”).
Here are some possible phrases: “Image with texture of ...”, “Picture taken in the geographical
locationof...”,"Photothatistakenoutdoors”,“Caricaturewithtext”,“Imagewiththeartisticstyle
of...”,“Imagewithone/two/threeobjects”,“Illustrationwiththecolorpalette...”,“Phototakenfrom
above/below”,“Photographtakenduring... season”. Justgivetheshorttitles,don’texplainwhy,and
don’tcombinetwodifferentconcepts(with“or”or“and”).
Table7: Thepromptforgeneratingthepoolofclassdescriptions. Wepromptthemodelwithall
theImageNetclasses.
16