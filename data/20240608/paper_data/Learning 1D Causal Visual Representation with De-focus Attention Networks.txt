Learning 1D Causal Visual Representation
with De-focus Attention Networks
ChenxinTao1,3∗,XizhouZhu1,2*,ShiqianSu1,3*,LeweiLu3,ChangyaoTian4,XuanLuo1,
GaoHuang1,HongshengLi4,YuQiao2,JieZhou1,JifengDai1,2(cid:0)
1TsinghuaUniversity 2ShanghaiArtificialIntelligenceLaboratory
3SenseTimeResearch 4TheChineseUniversityofHongKong
{tcx20,ssq20,luoxuan21}@mails.tsinghua.edu.cn,
{zhuxizhou,gaohuang,jzhou,daijifeng}@tsinghua.edu.cn,luotto@sensetime.com
tcyhost@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn
Abstract
Modalitydifferenceshaveledtothedevelopmentofheterogeneousarchitectures
forvisionandlanguagemodels. Whileimagestypicallyrequire2Dnon-causal
modeling, texts utilize 1D causal modeling. This distinction poses significant
challengesinconstructingunifiedmulti-modalmodels. Thispaperexploresthe
feasibility of representing images using 1D causal modeling. We identify an
"over-focus" issue in existing 1D causal vision models, where attention overly
concentrates on a small proportion of visual tokens. The issue of "over-focus"
hindersthemodel’sabilitytoextractdiversevisualfeaturesandtoreceiveeffec-
tivegradientsforoptimization. Toaddressthis,weproposeDe-focusAttention
Networks,whichemploylearnablebandpassfilterstocreatevariedattentionpat-
terns. During training, large and scheduled drop path rates, and an auxiliary
loss on globally pooled features for global understanding tasks are introduced.
These two strategies encourage the model to attend to a broader range of to-
kensandenhancenetworkoptimization. Extensiveexperimentsvalidatetheef-
ficacy of our approach, demonstrating that 1D causal visual representation can
performcomparablyto2Dnon-causalrepresentationintaskssuchasglobalper-
ception, dense prediction, and multi-modal understanding. Code is released at
https://github.com/OpenGVLab/De-focus-Attention-Networks.
1 Introduction
Duetoinherentmodalitydifferences,visionandlanguagemodelshaveevolvedintodistinctheteroge-
neousarchitectures. Akeydifferenceisthatimagesusuallyrequire2Dnon-causalmodeling,while
textsoftenutilize1Dcausalmodeling.Thisdistinctionpresentsasignificantchallengeinconstructing
unifiedmulti-modalmodels. Manyexistingmulti-modalmodels[37,3,11,5,28]havetotrainvision
andlanguageencodersseparatelybeforecombiningthem. Acrucialquestioninadvancingunified
vision-languagemodelingishowtorepresentimagesusing1Dcausalmodeling.
Followingthesuccessofcausallanguagemodeling(e.g.,GPT-series[52,53,8]),somestudies[10,17]
haveexploredcausalmodelinginthevisiondomain. Theseeffortsprimarilyfocusonauto-regressive
visualpre-trainingbyaddingacausalattentionmasktostandardTransformers[15].Despitenumerous
attempts,thegapbetween1Dcausaland2Dnon-causalvisionmodelsremainsunbridged. Asshown
inSec.5,many1Dcausalvisionmodels,suchasStateSpaceModels[61,20]andcausalViTs[15],
performinferiorlycomparedtotheirmodified2Dnon-causalcounterparts.
∗Equalcontribution.ThisworkisdonewhenChenxinTaoandShiqianSuareinternsatSenseTimeResearch.
(cid:0)CorrespondingtoJifengDai<daijifeng@tsinghua.edu.cn>.
4202
nuJ
6
]VC.sc[
1v24340.6042:viXraNon-causal Causal Causal De-focus Non-causal Causal Causal De-focus
ViT ViT Mamba Attention Network ViT ViT Mamba Attention Network
(a) Attention Map (b) Gradient Map
Figure1:Visualizationsof(a)AttentionMapand(b)GradientMapofdifferentmodels,including
Non-causalViT,CausalViT,CausalMambaandourDe-focusAttentionNetwork(Mamba-based).
Theresultsarefromthe11thlayerofViT(12intotal)and22ndlayerofMamba(24intotal). (a)The
approximatedattentionmapsofallimagetokens: Therowandcolumnaxesrepresentthequeryand
keytokenindexrespectively. Brightercolorindicateslargerattentionvalues. (b)Thegradientmaps
ofeachimagetokeninputafterback-propagation: Reddercolorsindicatelargergradientnorms. See
AppendixAformorevisualizationsondifferentlayers.
Inthispaper,weidentifyan"over-focus"issueinexisting1Dcausalvisionmodels. Fig.1visualizes
theattentionandgradientmapsofseveralImageNet-trainednetworks,including2Dnon-causalViT,
1DcausalViT,and1DcausalMamba. Theresultsshowthatin1Dcausalvisionmodels,theattention
patterns are overly concentrated on a small proportion of visual tokens, especially in the deeper
network layers close to the output. This phenomenon hinders the model from extracting diverse
visualfeaturesduringtheforwardcalculationandobtainingeffectivegradientsduringthebackward
propagation. Werefertothisphenomenonasthe"over-focus"issuein1Dcausalvisionmodels.
To address the issue, a "de-focus attention" strategy is introduced. The core idea is to guide the
networktoattendtoabroaderrangeoftokens. Ononehand,learnablebandpassfiltersareintroduced
tofirstfilterdifferentsetsoftokeninformation,thencombinetheirattentionpatterns. Thisensures
thatevenifover-focusingoccurs,theattentionpatternremainsdiverseduetothevaryingconstraints
of each set. On the other hand, optimization strategies are improved. A large drop path rate is
employedtoencouragethenetworktoattendtomoretokenswithinonelayer,ratherthanrelyingon
depthtogetlargereceptivefields.Fortasksrequiringglobalunderstanding(e.g.,imageclassification),
anauxiliarylossisappliedtothegloballypooledfeaturestoenhancetheeffectivegradientsforall
tokensinthesequence.
ExtensiveexperimentsdemonstratetheeffectivenessofourDe-focusAttentionNetworksfor1D
causalvisualrepresentationlearning. Itachievescomparableorevensuperiorperformanceto2D
non-causalViTsacrossvarioustasks,includingimageclassification,objectdetection,andimage-text
retrieval. Our method has been validated on both ViTs and Mambas. Our contributions can be
summarizedasfollows:
• Weidentifytheover-focusissuein1Dcausalvisualmodeling,wherethemodeloverlyfocuseson
asmallproportionofvisualtokensinthedeeperlayersofthenetwork.
• To address this issue, we propose a "de-focus attention" strategy. This involves integrating
learnablebandpassfiltersintotheexistingattentionoperatorstoachievediverseattentionpatterns.
Additionally,weintroducealargedroppathprobabilityandanauxiliarylossonaveragepooled
featuresduringtrainingtoenhancenetworkoptimization.
• OurDe-focusAttentionNetworkshavedemonstratedthat1Dcausalvisualrepresentationcan
achieveperformanceequivalentto2Dnon-causalrepresentationintasksrequiringglobalpercep-
tion,densepredictionandmulti-modalunderstandingtasks.
22 RelatedWork
StateSpaceModels(SSMs)areintrinsicallycausalmodels,originatedfromtheclassicKalman
filter[30]. SSMsdescribethebehaviorofcontinuous-dynamicsystems,enablingparalleltraining
andlinearcomplexityinference. [23]proposedaLinearStateSpaceLayer,mergingthestrengthsof
continuous-timemodels,RNNsandCNNs. HIPPO[21]introducedmethodstofacilitatecontinuous-
timeonlinememorization. Buildingonthesefoundations,StructuredSSMs(e.g.,S4[22],Diagonal
StateSpaces(DSS) [24],S5[58]),RecurrentSSMs(e.g.,RWKV[49],LRU[45])andGatedSSMs
(e.g., GSS[43], Mega[42]) further expand the SSMs landscape. Notably, Mamba [20] excels in
long-sequencemodelingwithitsselectivescanoperatorforinformationfilteringandhardware-aware
algorithmsforefficientstorageofintermediateresults. AsSSMshavedrawnmoreandmoreattention
recently,theyalsohaveextensiveapplicationsindomainsthatneedlongsequencesprocessingsuchas
medical[44,6],video[33],tabulardomain[2]andaudio/speech[19,29]. Thesesuccessesachieved
bySSMspromptustoexploretheirapplicationinvisualmodelingwithinthiscausalframework.
2DNon-CausalVisualModelingaredominantinvisiondomains. ConvolutionalNeuralNetworks
(CNNs),operatingina2Dsliding-windowmanner[32]withinductivebiasessuchastranslation
equivarianceandlocality,havedemonstratedremarkableadaptability[31,57,62,70,27,26,63].
Vision Transformers (ViTs) [15] utilize a non-causal self-attention mechanism, enabling global
receptivefields. Subsequentimprovementsfocusonenhancinglocality[39],refiningself-attention
mechanisms[73,4],andintroducingnovelarchitecturaldesigns[68,69,46,25],whilemaintaining
non-causality. RecentadvancesinStateSpaceModels(SSMs)haveinspirednewvisionbackbone
networks,suchasVMamba[38],VisionMamba[74],andVision-RWKV[16]. AlthoughSSMsare
inherentlycausal,theseworksincorporatenon-causaladjustmentstoenhancevisionperformance.
VMambaintroducedafour-wayscanningstrategy,VisionMambaincorporatedbidirectionalSSMs,
andVision-RWKVadoptedbidirectionalglobalattentionandaspecialtokenshiftmethod. These
designsofarrangementhindertheunificationofvisionandlanguagemodeling.
1DCausalVisualModeling. While1Dcausalmodelinghasprimarilybeenusedinlanguage[7]
and speech[65], it has also been explored for visual representation. In recent years, the causal
visualmodelinghasbeenadoptedinTransformer-basedvisualgenerationmethodssuchasImage
Transformer [47], iGPT [10] and VQGAN [18]. These models first discretize images into grids
of 2D tokens, which are then flattened for auto-regressive learning. However, their performance
significantlylagsbehind[48,1]. Ofparticularinterest,iGPTalsoemployedauto-regressivecausal
modelingforpre-training,followedbylinearprobingorfine-tuningtoachievecommendableresults
invariousdownstreamtasks,thoughstillworsethannon-causalmodels[14,9]. Similarly,AIM[17]
appliedcausalmaskstotheself-attentionlayers,andpre-trainedwithanauto-regressiveobjective,
showinggoodscalingpotential. Despitemanyattempts,theperformancegapbetween1Dcausaland
2Dnon-causalvisionmodelsremains.
3 Preliminary
Transformers[66]withcausalattentionconsistofmultipleattentionlayers.Eachattentionlayercom-
putesaweightedaveragefeaturefromtheprecedingcontextforeveryinputtoken,withaggregated
featuresweightedbythesimilaritiesbetweentokens. Theattentionlayeriswrittenas:
(cid:88)
y = Softmax(Q⊤K )V , (1)
t t s s
s≤t
wheresandtareindexesofdifferentlocationsoftheinputsequence,Q ,K ,V areprojectionsof
i i i
inputx ,andy istheoutputoftheattentionlayer.
i t
StateSpaceModels(SSMs)areclassicallatentstatemodelswidelyusedinvariousscientificfields
[44,6,33,67,50,72]. Originally, SSMsaredefinedforcontinuoussignals, mappinga1Dinput
signalx(t)∈Rtoalatentstateh(t)∈RN andcomputingtheoutputy(t)∈Rfromthelatentstate.
ToapplySSMstodiscretesequences,theirdiscreteformisdefinedas
h =A h +K x , y =Q⊤h , (2)
t t t−1 t t t t t
where A ∈ RN×N, K ∈ RN×1, Q ∈ RN×1 are parameters of the system. Note that we use
t t t
notationsdifferentfromtheoriginalSSMs(K ,Q insteadofB ,C )forabettercomparisonwith
t t t t
Transformersabove.
3SSMscanalsobetransformedintoanotherformulationbyexpandingtherecurrentprocess:
(cid:18) (cid:19)
(cid:88)
y = Q⊤ A ...A K x . (3)
t t t s+1 s s
s≤t
Thisformulationresemblestheconventionalattentionmoduleandexplicitlyrevealstherelationship
betweendifferentinputsinthesequence. Weusethisformforfurtherdiscussion.
TherearemultiplevariantsofSSMs,mainlydifferingintheparameterizationof(A ,K ,Q ). We
t t t
introducesomewell-knownSSMsanddiscusstheirdifferencesbelow.
RetNet[61]andTransnormer[51]employafixedAandconvertitintoanexponentialdecay(defined
byλ∈R)witharelativepositionalembedding(definedbyθ ∈RN):
(cid:88)
y = Q⊤eλ(t−s) eiθ(t−s) K x . (4)
t t s s
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
s≤t expdecayrelativeposembed
Mamba[20]andS4[22]usezero-orderhold(ZOH)rulefordiscretization,introducingatime-scale
parameter∆ .ThediscretizationruleisA =exp(∆ Aˆ)andK =(∆ Aˆ)−1(exp(∆ Aˆ)−I)·∆ Kˆ ,
t t t t t t t t
where Aˆ and Kˆ are learnable parameters. S4 uses data-independent parameters, while Mamba
t
computestheseparametersbasedoninputs. Theformulationcanbewrittenas:
y =(cid:88) Q⊤exp(cid:0) Aˆ(∆ +···+∆ )(cid:1) K x . (5)
t t s+1 t s s
s≤t (cid:124) (cid:123)(cid:122) (cid:125)
learnableexponentialdecay
4 Method
ThissectionintroducesourDe-focusAttentionNetworksfor1Dcausalvisualrepresentationlearn-
ing. Sec.4.1elucidatesthemaincomponentsofDe-focusAttentionasLearnableBandpassFilter,
whileSec.4.2furtherdiscussestwotrainingstrategiesadoptedinDe-focusNetwork. Theoverall
architectureofourmodelispresentedinFig.2.
4.1 De-focusAttentionwithLearnableBandpassFilter
To de-focus on a few salient tokens and enhance the extraction of diverse features from images,
learnablebandpassfiltersareincorporatedtofirstadaptivelyfilterdiverseinformationfromtheinput
andtheirattentionpatternsarethencombinedtogether. Duetothevaryingcontentsfromdifferent
filters,theattentioncanstillbediverseeveniftheover-focusissuehappens.
Thesebandpassfilterscanbeimplementedthroughexponentialspatialdecayandrelativeposition
embeddingsimilartothoseinRoPE[59]andxPos[60],bothofwhicharefurthermadelearnable.
Ourresultsdemonstratethatthesefactorsarecrucialforthemodeltolearndiverseattentionpatterns.
Toshowhowspatialdecayandrelativepositionembeddingsworkasabandpassfilter,considera
simplifiedversionof1Dcausalattentionequippedwiththem:
(cid:90)
y(t)= eλ(t−s)eiθ(t−s)x(s)ds, (6)
s≤t
wherex(s)istheinputsignalattimes.eλ(t−s)(λ<0)representsthesimplestversionofexponential
spatialdecay,whichisalsousedbyRetNet[61]andTransnormer[51].eiθ(t−s)istherelativeposition
embedding proposed by RoPE [59] and xPos [60]. Here, the continuous time domain is used to
facilitatederivationwithoutlosinggenerality.
Theaboveequationimpliesatimedomainconvolutionbetweeneλseiθsandx(s). Bytransforming
Eq.(6)intothefrequencydomainandusingxˆ(ω),yˆ(ω)torepresentFouriertransformofcorrespond-
ingx(s),y(t),thefrequencydomainexpressionbecomes:
1 1 1
yˆ(ω)= xˆ(ω), ∥yˆ(ω)∥= ∥xˆ(ω)∥. (7)
(cid:113)
−λ+i(ω−θ) |λ|
1+(ω−θ)2
λ
ThisequationindicatesthatEq.(6)isactuallyabandpassfilter,whereθisitscenterfrequencyandλ
controlsitspassbandwidth. Eq.(7)presentssomeinterestingpropertiesof1Dcausalmodeling:
41. Ifthereisnospatialdecayorrelativepositionembedding(e.g.,TransformerswithoutSoftmax),
Eq.(6)willdegeneratetoasummationoftheinputs,losingtheabilitytofilterspatialinformation;
2. Ifthereisnorelativepositionembedding(e.g.,Mamba),1Dcausalattentionwillperformlow-pass
frequency filtering, causing the query to miss the full information of features and resulting in
informationloss;
3. Ifonlyrelativepositionembeddingisused,itwilldegeneratetospecificfrequencyselecting,which
mayalsoresultininformationloss;
4. Ifbothspatialdecayandrelativepositionembeddingareused(suggested),1Dcausalattention
willactasabandpassfilter. Foragivenquery,whendifferentcomponentsofthefeaturevector
usedifferentcenterfrequencies(i.e.,differentθ)andpassbandswidth(i.e.,differentλ),amore
diverserangeofinformationwillbegathered. Duetothediversefrequencypassbands,evenifthe
over-focusissueoccurs,theattentionremainsdiverseacrossdifferentcomponentsofthefeature
vector.
To fully leverage the bandpass filtering mechanism, a learnable one is preferable. Experiments
demonstratethatperformanceworsenswhenvaluesarefixedornotwellset.
Our De-focus Attention can be incorporated into different architectures. Below, examples of its
implementationincausalViTandMambaarepresented.
De-focusCausalViT.ViThasadditionalattentionactivation(i.e.,Softmax)comparedwithSSMs.
Learnableexponentialspatialdecayandlearnablerelativepositionembeddingsareappendedbefore
applyingtheattentionactivation,followingthecommonimplementationofRoPE,asshownbelow:
y =(cid:88) Softmax(cid:0) Q⊤ eλ(t−s) eiθ(t−s) K (cid:1) x , (8)
t t s s
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
s≤t learnabledecaylearnableRoPE
wherethetermsofeλandeiθ functionasthelearnablebandpassfilter.
De-focusMamba. SinceMambaalreadyhaslearnableanddata-dependentexponentialspatialdecay,
onlyattachmentoflearnablerelativepositionembeddingstoitisnecessary:
y =(cid:88) Q⊤exp(cid:0) Aˆ(∆ +···+∆ )(cid:1) eiθ(t−s) K x , (9)
t t s+1 t s s
(cid:124) (cid:123)(cid:122) (cid:125)
s≤t (cid:124) (cid:123)(cid:122) (cid:125) learnableRoPE
learnableexponentialdecay
wherethetermsofAˆ∆andeiθ functionasthelearnablebandpassfilter.
4.2 De-focusAttentioninNetworkOptimization
Duringnetworktraining,performanceof1Dcausalmodelscanbefurtherenhancedwithimproved
optimizationstrategies. Specifically,usingalargedroppathratewithalinearschedulehelpsthe
modelattendtomoretokensineachlayer. Additionally, applyinganauxiliarylosstotheglobal
averagefeaturemitigatestheunder-learningoffeaturesindeeperlayers. Theeffectsofthesetraining
strategiesareillustratedinFig.3.
LargeDropPathRatewithLinearSchedule. Twowaysforthefinalpredictiontoaccessinfor-
mationfrompreviousinputsareobserved: 1)NetworkDepth: Progressivelylookingforwardafew
tokensineachlayeruntilreachingtheearliesttokens;2)Intra-LayerAttention: Usingtheattention
mechanismwithinthesamelayertodirectlycaptureinformationfrommoredistanttokens.
Ourgoalisforeachlayertofullyutilizetheexistingattentionmechanismtocapturemoreandfurther
informationinonelayer. Therefore,alargedroppathrate(upto0.7)isemployedtoencouragethe
networktorelylessondepthandrelymoreontrainingtheattentionmechanismineachlayer. Sincea
largedroppathratemayhinderthemodelwhenonlyafewfeaturesarelearned,i.e.,atthebeginning
oftraining,alinearschedulethatgraduallyincreasethedroppathrateisfollowed.
Fig.3demonstratestheeffectivenessofthisstrategy,indicatingthatwithoutlargedroppathstrategies,
thenetworktendstoprefertoseelesstokensinonelayerandrelyonnetworkdepthtoincreasethe
receptivefield.
AuxiliaryLossforImageClassification. Toaddressover-focusissueinbackwardgradients,an
auxiliarylossisproposedtoenrichthegradientsvarietyandaidintherepresentationlearningof
5Query Tokens Output Auxiliary Loss Loss
Causal Attention Layer FC
(e.g.Causal Transformer, Mamba) FC
Average Pooling
Learnable … …
Bandpass
Filter "#∆ '& (! N×
Drop Path
Learnable Learnable
Decay RoPE De-focus Causal Attention Block
exp(1⋅) exp 567 ×
∆ ( ) … …
<CLS>
Projection Layer
Query Tokens Input Input Image
Figure2: ArchitectureofourDe-focusAttentionNetwork. Left: DetailedarchitectureofDe-focus
AttentionBlock: TheinputtokensareprojectedtoQ,K,andotherparametersrequiredbycertain
causalattentionlayer(e.g. TransformerorMamba). ∆isdata-dependentinDe-focusMamba,while
is set to 1 in De-focus ViT. Learnable decay and learnable relative position embeddings form a
learnablebandpassfilterandarecalculatedbeforebeingfedintothecausalattentionlayer. Parameter
λinDe-focusViTcorrespondstoAinthisfigure. Right: OverallarchitectureofDe-focusAttention
Network: DroppathsareincorporatedaftereachDe-focusAttentionBlock. Alloutputimagetokens
arepassedthroughAveragePoolingandafullyconnectedlayertoproducetheauxiliaryloss.
unnoticedtokens. Thefinalrepresentationsofallimagetokens(excludingthefinal<CLS>token)
areaveragedandfedintoanadditionallinearlayer. Theauxiliarylossfunctionisdefinedasthe
cross-entropylossbetweentheoutputoftheadditionallinearlayerandthegroundtruthlabel. This
approachhelpsenrichthebackpropagatedgradients,therebyaddressingtheover-focusissue.
AsshowninFig.3,afterapplyingtheauxiliarylossstrategy,thebackwardgradientsaresignificantly
improvedinitsdensity,globality,anddiversityindeeperlayers.
4.3 OverallArchitecture
TheoverallarchitectureofourDe-focusAttentionNetworksisillustratedinFig.2. Thefollowing
explainshowlearnablebandpassfiltersandoptimizationstrategiesareintegratedintoexistingmodels.
De-focusAttentionBlocks. Eachblockconsistsofthreemainparts,whichareaprojectionlayer,
alearnablebandpassfilter,andacausalattentionlayer. Theinputtokensarefirstprojectedintoa
queryQ,akeyK. ∆isdata-dependentinDe-focusMamba,whileissetto1inDe-focusViT.Other
projectionsmayberequiredbythecausalattentionlayer. Theblockhaslearnabledecayparameters
A(correspondstoλinDe-focusViT)andlearnablerelativepositionembeddingparametersθ. Given
theselearnableparameters,exponentialspatialdecayandrelativepositionembeddingarecomputed
as illustrated in Eq. (8) and Eq. (9). Q,K and the exponential spatial decay term are integrated
intothecausalattentionlayer. Thus,theoutputsofaDe-focusAttentionblockaggregatetheinput
informationfilteredbyaseriesoflearnablebandpassfilters.
De-focusAttentionNetworks. Givenanimage,ourDe-focusAttentionNetworksfirsttransformit
intoasequenceofimagetokensandappendanextra<CLS>tokentothesequenceend. Thewhole
network then stacks N De-focus Attention blocks to process the input sequence. Each block is
equippedadroppathrate,whichincreaseslinearlyduringtraining. Inthefinallayer,the<CLS>token
isfedthroughalinearlayerandusedtocomputeacross-entropylosswithclasslabels. Allimage
tokens,excludingthefinal<CLS>token,areaveragedandpassedthroughaseparatelinearlayer. An
auxiliarycross-entropylossisappliedtothisprojectedaveragedfeature. Thetwolossesarethen
addedwithequalweightstoformthefinallossfunction.
6Table1: Comparisonofcausalandnon-causalattentionsforimageclassificationonImageNet-1K.
ImageNet
Method Causal Size #Param
Top-1Acc
DeiT-Small[64] 2242 22.1M 79.9
Mamba-ND-Small[34] 2242 24M 79.4
VisionMamba-Small[74] 2242 26M 80.5
VisionRWKV-Small[16] 2242 23.8M 80.1
DeiT-Small ✓ 2242 22.1M 78.6
Mamba-Small[20] ✓ 2242 24.7M 78.7
Mamba-ND-Small[34] ✓ 2242 24M 76.4
De-focusMamba-Small ✓ 2242 25.1M 80.3
DeiT-Base[64] 2242 86.6M 81.8
S4ND-ViT-B[44] 2242 88.8M 80.4
VisionRWKV-Base[16] 2242 93.7M 82.0
De-focusViT-Base 2242 87.4M 81.8
DeiT-Base ✓ 2242 86.6M 80.1
RetNet-Base[61] ✓ 2242 93.6M 79.0
Mamba-Base[20] ✓ 2242 91.9M 80.5
De-focusViT-Base ✓ 2242 87.4M 81.5
De-focusRetNet-Base ✓ 2242 92.7M 81.7
De-focusMamba-Base ✓ 2242 92.7M 82.0
ViT-Large[15] 3842 309.5M 85.2
VisionRWKV-Large[16] 3842 334.9M 86.0
De-focusMamba-Large ✓ 3842 330.1M 85.9
5 Experiments
5.1 ExperimentSetup
ImplementationDetails. TheDe-focusAttentionmechanismsareintegratedintoMamba,RetNet,
and ViT, referred to as De-focus Mamba, De-focus RetNet, and De-focus ViT, respectively. To
improve optimization stability, λ = −exp(λˆ) is used and λˆ is the parameter to be optimized. In
De-focusViTandDe-focusRetNet,differentλsareassignedtodifferentheads. Mambainherently
implementsdata-dependentdecayAˆ∆,whereAˆisalearnableparameterand∆isaprojectionfrom
theinput. Thedroppathrateincreasesfollowingalinearschedulefrom0.1to0.7.
ImageClassification. ImageNet-1K[13]isused,whichcontains1.28Mimagesfortrainingand
50Kimagesforvalidation. ThetrainingrecipeofDeiT[64]isfollowed. Thesmall-andbase-size
models are trained on ImageNet for 300 epochs. The large-size model is firstly pre-trained on
ImageNet-21K[55]for90epochs,andthenfine-tunedonImageNet-1Kfor20epochs. TheAdamW
optimizer[40]withapeaklearningrateof5e-4,atotalbatchsizeof1024,amomentumof0.9,anda
weightdecayof0.05areused. Thesemodelsaretrainedon32Nvidia80GA100GPUsfor30hours.
ObjectDetection. TheMS-COCOdataset[36]andtheDINOdetectionframework[71]areused,
withdifferentnetworksservingasthebackbones. TheDe-focusAttentionNetworksimplemented
herearepre-trainedonImageNet-1Kdatasetfor300epochs. Thesemodelsaretrainedon16Nvidia
80GA100GPUsfor40hours.
Theentirenetworkisfine-tunedusingbotha1×schedule(12epochs)anda3×schedule(36epochs).
Thebaselearningrateissetto2e-4,withamulti-steplearningratestrategyemployedtodecreaseit
byafactoroftenafter11epochs(1×schedule)orafter27and33epochs(3×schedule). Theweight
decayandthetotalbatchsizeissetto1e-4and16,respectively.
Contrastive Language-Image Pre-training (CLIP). The Laion-400M dataset [56] is used for
pre-training. StrategyintroducedinOpenCLIP[12]isfollowedtotrainthemodelfor32epochs. The
zero-shotclassificationperformanceisevaluatedonImageNet-1K.TheAdamWoptimizer[40]is
employedwithapeaklearningrateof5e-4,atotalbatchsizeof32768,amomentumof0.9,anda
weightdecayof0.1. Thesemodelsaretrainedon128Nvidia80GA100GPUsfor128hours.
7Table2: ResultsofobjectdetectionontheCOCO[36]datasetwithDINO[71]detector.
Method Causal #Param Epochs APbox APbox APbox
50 75
ResNet-50[71] 47M 12 49.0 66.6 53.5
DeiT-Base 110M 12 49.1 69.9 52.7
De-focusMamba-Base ✓ 115M 12 50.2 68.2 54.5
ResNet-50[71] 47M 36 50.9 69.0 55.3
DeiT-Base 110M 36 52.3 72.5 56.7
De-focusMamba-Base ✓ 115M 36 53.2 71.5 58.0
Table3: Resultsonzero-shotimageclassificationofCLIPpre-trainedmodels.
ImageNetZero-shot
Method Causal #Param
Top-1Acc
OpenAICLIP-Base/32[54] 151.3M 63.3
OpenCLIP-Base/32[12] 151.3M 62.9
De-focusMamba-Base/32 ✓ 161.9M 62.7
5.2 MainResults
ImageClassification.TheclassificationresultsarepresentedinTable1.Evaluationofdifferenttypes
ofDe-focusNetworksatvariousscalesisconducted,withcomparisonstobothcausalandnon-causal
models. Theresultsshowthatpreviouscausalmodelshaveinferiorperformance. Incontrast,our
modeldefiesthistrend,significantlyoutperformingother1Dcausalmodelsandachievingcomparable
performanceto2Dnon-causalmodels.
Notably,theDe-focusAttentionmechanismworkswellacrossvariousnetworks,e.g.,CausalViT,
Mamba,andRetNet. Andasthemodelsizeincreasesfromsmalltolarge,itremainsonparwiththe
2Dnon-causalViTs.
Object Detection. As shown in Table 2, De-focus Mamba remarkably outperforms non-causal
models such as DeiT and ResNet-50. This trend of superior performance persists even with an
increasingnumberoftrainingepochs. Additionally,excellentperformanceontheAPboxmetricmay
75
suggestthatDe-focusAttentionNetworksaremoreeffectiveatfine-grainedlocalization.
Image-text CLIP Pre-training. The model is pre-trained using OpenCLIP to demonstrate its
outstandingperformanceonlarge-scaleimage-texttraining. AsshowninTable3,themodelperforms
comparablyto2Dnon-causalmodels. Theseresultsindicatethatthemodelhasasimilarscalinglaw
tonon-causalViTsonlargerdataset,demonstratingitsrobustnessandscalabilityacrossvariousof
tasksanddatasets. Additionally,thisexperimentdemonstratesthepotentialof1Dcausalmodeling
forunifiedvision-languagemodeling.
Table4: AblationstudiesofvariousdesignchoicesofDe-focusMamba-BasemodelonImageNet-
1k[13]. Thedefaultsettingsaresetas(a)dpr=0.4,withauxiliaryloss,(b)withauxiliaryloss,data
dependentdecayandlearnableRoPE,(c)dpr=0.4,withdatadependentdecayandlearnableRoPE.
“dpr”isdroppathrate. Thetextin(c)denotestheinputfeatureforthelossfunction.
(a)AblationonBandpassFilter (b)AblationonDropPath. (c)AblationonLossFunction
Decay RoPE Acc DropPath Acc Loss AuxLoss Acc
w/o w/o 75.2
0.1 79.6 <CLS> – 81.6
w/o fixed 75.3
fixed w/o 79.9 0.4 81.6 avg – 77.2
fixed fixed 80.0 0.7 80.9 <CLS>+avg – 79.7
fixed learnable 80.6 linear(0.1,0.7) 82.0 <CLS> avg 82.0
learnable w/o 80.4
learnable learnable 81.2
datadependent learnable 81.3
8(a) Reception field w/o Scheduled DropPath(constant 0.1) (c) Gradient map w/o Auxiliary Loss
(b) Reception field w/ Scheduled DropPath(linear(0.1, 0.7)) (d) Gradient map w/ Auxiliary Loss
Figure3: Qualitativeablationresultsofusingscheduleddroppathandauxiliaryloss. (a)-(b):
Thereceptivefieldsofourmodeltrainedwithandwithoutscheduleddroppath. Thescheduleddrop
pathstrategyenablesalargerreceptivefield,facilitatingthecaptureofdensersemanticdetails.(c)-(d):
Thebackwardgradientmapsofourmodeltrainedwithandwithoutauxiliaryloss. Whentrainedwith
theauxiliaryloss,themodelcanattendtodenserandmorediverseimagetokens,particularlythoseat
thefrontofthesequence.
5.3 AblationStudy
Learnable Bandpass Filter. As discussed in Sec. 4.1, exponential spatial decay and relative
positionembedding(RoPE)togetheractasabandpassfilter. Tab.4(a)showstheeffectsofdifferent
configurations. When decay is not used, the performance significantly deteriorates. Employing
learnabledecayleadstoanimprovementofapproximately0.5%comparedtofixeddecay, while
learnableRoPEcanfurtherenhanceperformanceby0.8%. Incontrast,thedata-dependentdecay
usedinMambaonlyresultsinamarginalimprovementof0.1%. Theseresultsindicatetheintegration
oflearnabledecayandRoPEarenecessaryforgoodperformance.
DropPath. Tab.4(b)showstheperformanceofdifferentdroppathstrategies,withratesranging
from0.1to0.7. Thebestperformanceisachievedwithascheduleddroppathratelinear(0.1,0.7).
Fig.3(a)-(b)visualizethereceptivefieldofthe22ndlayerofthenetwork. Theresultsdemonstrate
thatusingalargeandscheduleddroppathratestrategyallowsforlargerreceptivefieldandhelps
capturemoredensesemanticdetails.
AuxiliaryLoss.Tab.4(c)comparesvariousimplementationsofthelossfunction,whicharegenerated
from<CLS>tokenonly,averagetokenonly,concatenationof<CLS>tokenandaveragetoken,and
<CLS>tokenwithauxiliaryaveragetoken. Theresultsrevealthattheaveragepooledfeaturealone
performspoorlyintrainingthenetwork. Itmayresultfromthefactthatprevioustokensoftenhave
incompleteinformation. However,itservesasaneffectiveauxiliarycomponent,therebyenhancing
thenetworktraining. Thevisualizationofgradientmapsatthe22ndlayerofthenetworkareshown
inFig.3(c)-(d). Whentrainingwithauxiliaryloss,thedensity,globality,anddiversityofbackward
gradientsaresignificantlyimproved.
6 Conclusion
WeproposeDe-focusAttentionNetworkstoenhancetheperformanceofcausalvisionmodelsby
addressingtheissueofover-focusinthem. Theover-focusphenomenon, i.e. attentionpatternis
overlyfocusedonasmallproportionofvisualtokens,isobservedbothduringtheforwardcalculation
andbackpropagation. TheseDe-focusmodelsincorporateadecaymechanismandrelativeposition
embeddings, functioning together as diverse and learnable bandpass filters to introduce various
attentionpatterns. Themodelsaretrainedwithalargescheduleddroppathrateandauxiliarylossto
enhancethedensity,globality,anddiversityofbackwardgradients.AseriesofDe-focusmodelsbased
onMamba,RetNet,andViTsignificantlyoutperformothercausalmodelsandachievecomparable
orevensuperiorperformancetostate-of-the-artnon-causalmodels. Byimplementingthede-focus
strategy,ourworkbridgestheperformancegapbetweencausalandnon-causalvisionmodels,paving
thewayforthedevelopmentofstate-of-the-artunifiedvision-languagemodels.
Acknowledgements. TheworkispartiallysupportedbytheNationalNaturalScienceFoundationof
ChinaunderGrants62321005.
9References
[1] Alpha-vllm.large-dit-imagenet. https://github.com/Alpha-VLLM/LLaMA2-Accessory/
tree/f7fe19834b23e38f333403b91bb0330afe19f79e/Large-DiT-ImageNet,2024.
[2] M.A.AhamedandQ.Cheng. Mambatab: Asimpleyeteffectiveapproachforhandlingtabular
data. arXivpreprintarXiv:2401.08867,2024.
[3] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,etal. Flamingo: avisuallanguagemodelforfew-shotlearning. NeurIPS,2022.
[4] A.Ali,H.Touvron,M.Caron,P.Bojanowski,M.Douze,A.Joulin,I.Laptev,N.Neverova,
G.Synnaeve,J.Verbeek,etal. Xcit: Cross-covarianceimagetransformers. NeurIPS,2021.
[5] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: A
versatilevision-languagemodelforunderstanding,localization,textreading,andbeyond. 2023.
[6] M.Ballarotto,S.Willems,T.Stiller,F.Nawa,J.A.Marschner,F.Grisoni,andD.Merk. De
novodesignofnurr1agonistsviafragment-augmentedgenerativedeeplearninginlow-data
regime. JournalofMedicinalChemistry,2023.
[7] Y.Bengio, R.Ducharme, andP.Vincent. Aneuralprobabilisticlanguagemodel. NeurIPS,
2000.
[8] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G.Sastry,A.Askell,etal. Languagemodelsarefew-shotlearners. NeurIPS,2020.
[9] H.Chang,H.Zhang,J.Barber,A.Maschinot,J.Lezama,L.Jiang,M.-H.Yang,K.Murphy,
W.T.Freeman,M.Rubinstein,etal. Muse: Text-to-imagegenerationviamaskedgenerative
transformers. arXivpreprintarXiv:2301.00704,2023.
[10] M.Chen,A.Radford,R.Child,J.Wu,H.Jun,D.Luan,andI.Sutskever. Generativepretraining
frompixels. InICML.PMLR,2020.
[11] Z.Chen,J.Wu,W.Wang,W.Su,G.Chen,S.Xing,Z.Muyan,Q.Zhang,X.Zhu,L.Lu,etal.
Internvl: Scalingupvisionfoundationmodelsandaligningforgenericvisual-linguistictasks.
arXivpreprintarXiv:2312.14238,2023.
[12] M.Cherti,R.Beaumont,R.Wightman,M.Wortsman,G.Ilharco,C.Gordon,C.Schuhmann,
L.Schmidt,andJ.Jitsev. Reproduciblescalinglawsforcontrastivelanguage-imagelearning.
InCVPR,2023.
[13] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InCVPR.Ieee,2009.
[14] J.DonahueandK.Simonyan. Largescaleadversarialrepresentationlearning. NeurIPS,2019.
[15] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersfor
imagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[16] Y.Duan,W.Wang,Z.Chen,X.Zhu,L.Lu,T.Lu,Y.Qiao,H.Li,J.Dai,andW.Wang. Vision-
rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint
arXiv:2403.02308,2024.
[17] A. El-Nouby, M. Klein, S. Zhai, M. A. Bautista, A. Toshev, V. Shankar, J. M. Susskind,
and A. Joulin. Scalable pre-training of large autoregressive image models. arXiv preprint
arXiv:2401.08541,2024.
[18] P.Esser,R.Rombach,andB.Ommer. Tamingtransformersforhigh-resolutionimagesynthesis.
InCVPR,2021.
[19] K.Goel,A.Gu,C.Donahue,andC.Ré. It’sraw! audiogenerationwithstate-spacemodels. In
ICML.PMLR,2022.
[20] A.GuandT.Dao. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXiv
preprintarXiv:2312.00752,2023.
[21] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré. Hippo: Recurrent memory with optimal
polynomialprojections. NeurIPS,33,2020.
[22] A.Gu,K.Goel,andC.Ré. Efficientlymodelinglongsequenceswithstructuredstatespaces.
arXivpreprintarXiv:2111.00396,2021.
10[23] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Ré. Combining recurrent,
convolutional,andcontinuous-timemodelswithlinearstatespacelayers. NeurIPS,2021.
[24] A.Gupta,A.Gu,andJ.Berant. Diagonalstatespacesareaseffectiveasstructuredstatespaces.
NeurIPS,35,2022.
[25] B.Heo,S.Yun,D.Han,S.Chun,J.Choe,andS.J.Oh. Rethinkingspatialdimensionsofvision
transformers. InICCV,2021.
[26] J.Hu,L.Shen,andG.Sun. Squeeze-and-excitationnetworks. InCVPR,2018.
[27] G.Huang,Z.Liu,L.VanDerMaaten,andK.Q.Weinberger. Denselyconnectedconvolutional
networks. InCVPR,2017.
[28] S.Huang, L.Dong, W.Wang, Y.Hao, S.Singhal, S.Ma, T.Lv, L.Cui, O.K.Mohammed,
B. Patra, et al. Language is not all you need: Aligning perception with language models.
NeurIPS,2024.
[29] X. Jiang, C. Han, and N. Mesgarani. Dual-path mamba: Short and long-term bidirectional
selectivestructuredstatespacemodelsforspeechseparation. arXivpreprintarXiv:2403.18257,
2024.
[30] R.E.Kalman. Anewapproachtolinearfilteringandpredictionproblems. 1960.
[31] A.Krizhevsky,I.Sutskever,andG.E.Hinton. Imagenetclassificationwithdeepconvolutional
neuralnetworks. NeurIPS,2012.
[32] Y.LeCun,K.Kavukcuoglu,andC.Farabet. Convolutionalnetworksandapplicationsinvision.
InProceedingsof2010IEEEinternationalsymposiumoncircuitsandsystems,pages253–256.
IEEE,2010.
[33] K.Li,X.Li,Y.Wang,Y.He,Y.Wang,L.Wang,andY.Qiao. Videomamba: Statespacemodel
forefficientvideounderstanding. arXivpreprintarXiv:2403.06977,2024.
[34] S.Li,H.Singh,andA.Grover.Mamba-nd:Selectivestatespacemodelingformulti-dimensional
data. arXivpreprintarXiv:2402.05892,2024.
[35] Y.Li,H.Mao,R.Girshick,andK.He. Exploringplainvisiontransformerbackbonesforobject
detection. InECCV.Springer,2022.
[36] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.
Microsoftcoco: Commonobjectsincontext. InECCV,2014.
[37] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. NeurIPS,2024.
[38] Y.Liu,Y.Tian,Y.Zhao,H.Yu,L.Xie,Y.Wang,Q.Ye,andY.Liu. Vmamba: Visualstate
spacemodel. arXivpreprintarXiv:2401.10166,2024.
[39] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer:
Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021.
[40] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[41] W.Luo,Y.Li,R.Urtasun,andR.Zemel. Understandingtheeffectivereceptivefieldindeep
convolutionalneuralnetworks. NeurIPS,2016.
[42] X.Ma,C.Zhou,X.Kong,J.He,L.Gui,G.Neubig,J.May,andL.Zettlemoyer. Mega: moving
averageequippedgatedattention. arXivpreprintarXiv:2209.10655,2022.
[43] H.Mehta,A.Gupta,A.Cutkosky,andB.Neyshabur. Longrangelanguagemodelingviagated
statespaces. arXivpreprintarXiv:2206.13947,2022.
[44] E.Nguyen,K.Goel,A.Gu,G.W.Downs,P.Shah,T.Dao,S.A.Baccus,andC.Ré. S4nd:
Modelingimagesandvideosasmultidimensionalsignalsusingstatespaces. arXivpreprint
arXiv:2210.06583,2022.
[45] A.Orvieto,S.L.Smith,A.Gu,A.Fernando,C.Gulcehre,R.Pascanu,andS.De. Resurrecting
recurrentneuralnetworksforlongsequences. InICML.PMLR,2023.
[46] Z.Pan,B.Zhuang,J.Liu,H.He,andJ.Cai. Scalablevisiontransformerswithhierarchical
pooling. InICCV,2021.
[47] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image
transformer. InICML.PMLR,2018.
11[48] W.PeeblesandS.Xie. Scalablediffusionmodelswithtransformers. InICCV,2023.
[49] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung,
M.Grella,K.K.GV,etal. Rwkv: Reinventingrnnsforthetransformerera. arXivpreprint
arXiv:2305.13048,2023.
[50] Y.Qiao,Z.Yu,L.Guo,S.Chen,Z.Zhao,M.Sun,Q.Wu,andJ.Liu. Vl-mamba: Exploring
statespacemodelsformultimodallearning. arXivpreprintarXiv:2403.13600,2024.
[51] Z.Qin,D.Li,W.Sun,W.Sun,X.Shen,X.Han,Y.Wei,B.Lv,F.Yuan,X.Luo,etal. Scaling
transnormerto175billionparameters. arXivpreprintarXiv:2307.14995,2023.
[52] A.Radford,K.Narasimhan,T.Salimans,I.Sutskever,etal. Improvinglanguageunderstanding
bygenerativepre-training. 2018.
[53] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal. Languagemodelsare
unsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[54] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InICML.PMLR,2021.
[55] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the
masses. arXivpreprintarXiv:2104.10972,2021.
[56] C.Schuhmann,R.Vencu,R.Beaumont,R.Kaczmarczyk,C.Mullis,A.Katta,T.Coombes,
J.Jitsev,andA.Komatsuzaki. Laion-400m: Opendatasetofclip-filtered400millionimage-text
pairs. arXivpreprintarXiv:2111.02114,2021.
[57] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. arXivpreprintarXiv:1409.1556,2014.
[58] J.T.Smith,A.Warrington,andS.W.Linderman. Simplifiedstatespacelayersforsequence
modeling. arXivpreprintarXiv:2208.04933,2022.
[59] J.Su,M.Ahmed,Y.Lu,S.Pan,W.Bo,andY.Liu. Roformer: Enhancedtransformerwith
rotarypositionembedding. Neurocomputing,2024.
[60] Y.Sun,L.Dong,B.Patra,S.Ma,S.Huang,A.Benhaim,V.Chaudhary,X.Song,andF.Wei. A
length-extrapolatabletransformer. arXivpreprintarXiv:2212.10554,2022.
[61] Y.Sun,L.Dong,S.Huang,S.Ma,Y.Xia,J.Xue,J.Wang,andF.Wei. Retentivenetwork: A
successortotransformerforlargelanguagemodels. arXivpreprintarXiv:2307.08621,2023.
[62] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,and
A.Rabinovich. Goingdeeperwithconvolutions. InCVPR,2015.
[63] M.TanandQ.Le. Efficientnet: Rethinkingmodelscalingforconvolutionalneuralnetworks. In
ICML.PMLR,2019.
[64] H.Touvron,M.Cord,M.Douze,F.Massa,A.Sablayrolles,andH.Jégou.Trainingdata-efficient
imagetransformers&distillationthroughattention. InICML.PMLR,2021.
[65] A.VanDenOord,S.Dieleman,H.Zen,K.Simonyan,O.Vinyals,A.Graves,N.Kalchbrenner,
A.Senior,K.Kavukcuoglu,etal. Wavenet: Agenerativemodelforrawaudio. arXivpreprint
arXiv:1609.03499,2016.
[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. NeurIPS,2017.
[67] C.Wang,O.Tsepa,J.Ma,andB.Wang. Graph-mamba: Towardslong-rangegraphsequence
modelingwithselectivestatespaces. arXivpreprintarXiv:2402.00789,2024.
[68] W.Wang,E.Xie,X.Li,D.-P.Fan,K.Song,D.Liang,T.Lu,P.Luo,andL.Shao. Pyramid
visiontransformer: Aversatilebackbonefordensepredictionwithoutconvolutions. InICCV,
2021.
[69] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pvt
v2: Improvedbaselineswithpyramidvisiontransformer. ComputationalVisualMedia,8(3):
415–424,2022.
[70] S.Xie,R.Girshick,P.Dollár,Z.Tu,andK.He. Aggregatedresidualtransformationsfordeep
neuralnetworks. InCVPR,2017.
12[71] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum. Dino:
Detrwithimproveddenoisinganchorboxesforend-to-endobjectdetection. arXivpreprint
arXiv:2203.03605,2022.
[72] H.Zhao,M.Zhang,W.Zhao,P.Ding,S.Huang,andD.Wang. Cobra: Extendingmambato
multi-modallargelanguagemodelforefficientinference. arXivpreprintarXiv:2403.14520,
2024.
[73] D.Zhou,B.Kang,X.Jin,L.Yang,X.Lian,Z.Jiang,Q.Hou,andJ.Feng. Deepvit: Towards
deepervisiontransformer. arXivpreprintarXiv:2103.11886,2021.
[74] L.Zhu,B.Liao,Q.Zhang,X.Wang,W.Liu,andX.Wang. Visionmamba: Efficientvisual
representationlearningwithbidirectionalstatespacemodel. arXivpreprintarXiv:2401.09417,
2024.
13A MoreVisualizationResults
Thissectionprovidesvisualizationresultsofattentionmapsandgradientmapsfrommoredifferent
layersofdifferentmodels,asshowninFig.4,Fig.5andFig.6. Comparedtoothercausalmodels,
ourde-focusattentionnetworkhasdenserattentionmapsanddiversegradientmaps,especiallyin
deeplayers.
Non-causal Causal Causal De-focus Non-causal Causal Causal De-focus
ViT ViT Mamba Attention Network ViT ViT Mamba Attention Network
(a) Attention Map (b) Gradient Map
Figure4:Visualizationsof(a)AttentionMapand(b)GradientMapofdifferentmodels,including
non-causalViT,causalViT,CausalMambaandourDe-focusAttentionNetwork(Mamba-based).
Theresultsarefromthe3rdlayerofViT(12intotal)and6thlayerofMamba(24intotal). (a)The
approximatedattentionmapsofallimagetokens: Therowandcolumnaxisrepresentthequeryand
keytokenindexrespectively. Brightercolorindicateslargerattentionvalues. (b)Thegradientmaps
ofeachimagetokeninputafterback-propagation: Reddercolorsindicatelargergradientnorms.
B MoreImplementationDetails
B.1 Visualization
Thissubsectiondiscussesthedetailedimplementationofdifferentvisualizationmethodsadopted
inourpaper,includingreceptivefields(Fig.3(a)-(b)),attentionmaps(Fig.1(a),Fig.4(a),Fig.5(a),
Fig.6(a)),andgradientmaps(Fig.1(b),Fig.3(c)-(d),Fig.4(b),Fig.5(b),Fig.6(b)).
Receptivefields[41]ofacertainlayeraredefinedasthegradientnormsofallimagetokensonthe
inputside. Thegradientshereareobtainedbyback-propagatingfromtheL2-normofthe<CLS>
tokenfeatureonoutputsideofthesamelayer. Reddercolorsindicatelargerreceptivescores.
Attentionmaps. Similartoreceptivefields,theapproximatedattentionmapsinourpaperarealso
thegradientnormsofallinputimagetokens(as‘key’). However,differentfromreceptivefields,
thesegradientscomefromback-propagationofthefeaturenormacrossallimagetokens(as‘query’)
onthesamelayer’soutputside. Brightercolorsindicatelargerattentionweights.
Gradientmaps. Differentfromreceptivefields,thegradientmapsofacertainlayerarecalculated
bydirectlyback-propagatingfromthefinaltraininglosstothislayer’sinputimagetokens. Then
theL2-normofeachimagetoken’sgradientisusedforplottingthegradientmaps. Reddercolors
indicatelargergradientnorms.
By default, the values of receptive fields, attention maps, and gradient maps are divided by the
maximumvalueamongallinputimagetokensfornormalization. Forattentionmaps,thediagonal
valuesaresetas0manuallytoeliminatetheinfluenceinducedbyresidualconnection. Allimage
samplesarerandomlyselected.
14Non-causal Causal Causal De-focus Non-causal Causal Causal De-focus
ViT ViT Mamba Attention Network ViT ViT Mamba Attention Network
(a) Attention Map (b) Gradient Map
Figure5:Visualizationsof(a)AttentionMapand(b)GradientMapofdifferentmodels,including
non-causalViT,causalViT,CausalMambaandourDe-focusAttentionNetwork(Mamba-based).
Theresultsarefromthe6thlayerofViT(12intotal)and12thlayerofMamba(24intotal). (a)The
approximatedattentionmapsofallimagetokens: Therowandcolumnaxisrepresentthequeryand
keytokenindexrespectively. Brightercolorindicateslargerattentionvalues. (b)Thegradientmaps
ofeachimagetokeninputafterback-propagation: Reddercolorsindicatelargergradientnorms.
Non-causal Causal Causal De-focus Non-causal Causal Causal De-focus
ViT ViT Mamba Attention Network ViT ViT Mamba Attention Network
(a) Attention Map (b) Gradient Map
Figure6:Visualizationsof(a)AttentionMapand(b)GradientMapofdifferentmodels,including
non-causalViT,causalViT,CausalMambaandourDe-focusAttentionNetwork(Mamba-based).
Theresultsarefromthe9thlayerofViT(12intotal)and18thlayerofMamba(24intotal). (a)The
approximatedattentionmapsofallimagetokens: Therowandcolumnaxisrepresentthequeryand
keytokenindexrespectively. Brightercolorindicateslargerattentionvalues. (b)Thegradientmaps
ofeachimagetokeninputafterback-propagation: Reddercolorsindicatelargergradientnorms.
15ImagePatchesdividedintoSections SectionsSpannedIndependently
SectionsConcatenated
Figure7:Patchrearrangeforresolutiontransfer.Thisillustrationpresentshowresolutiontransfers
fromapre-trainedresolutionof2×2toafine-tunedresolutionof4×4. Thewholeimageisfirst
divided into 2×2 sections, each containing 2×2 patches. Each section is first spanned into a
sequence,andthenconcatenatedinaz-scanorder.
B.2 ImageClassification
Thehyper-parametersfortrainingonImageNet-1K[13]fromscratchareprovidedinTab.5.
Thehyper-parametersforpre-trainingonImageNet-21K[55]areprovidedinTab.7. Thehyper-
parametersforfinetuningonImageNet-1K[13]afterpre-trainingareprovidedinTab.8.
ToimprovethetrainingstabilityofMamba,anextranormalizationisaddedafteritsselectivescan
module. Specifically,∆isinitializedwithvalueslinearlydistributedfrom0.001to0.1,ratherthan
randomlysampledfromthisrange. Thisinitializationstrategyensuresthatfeaturevectorsatnearby
channelshavesimilarmagnitudes. ThenRMSnormalizationisappliedtotheoutputsoftheselective
scanmodulewithagroupsizeof64.
Our large model is first trained on ImageNet-21K with resolution of 192, and then finetuned on
ImageNet-1Kwithresolutionof384. Toreducethediscrepancybetweentheresolutionsofimages,
spatialdecayparameters(e.g.,∆)andpositionembeddingindicesarescaleddownbyfactorsofr
andr2,whererreferstotheresolutionratiobetweenfine-tunedsizeandpre-trainedsize.
B.3 ObjectDetection
Thehyper-parametersfortrainingonCOCOobjectdetection[36]areprovidedinTab.6.
DINO[71]requiresmulti-scalefeaturemapsasinputs,whileourmodelscanonlyproduceasingle-
scalefeaturemap. Toremedythisissue,asimplefeaturepyramidisadoptedtoproducemulti-scale
featuremapswithasetofconvolutionsanddeconvolutions,followingViTDet[35].
Thepre-trainedmodelsarealsopre-processedfollowingSec.B.2toreducethediscrepancyofimage
resolution.Inaddition,theorderofimagetokensisrearrangedasshowninFig.7,witheach224×224
sectionoftheimagefirstbeingspanned,followedbyaconcatenationofthesespannedsequencesina
z-scanorder.
16Table5: Hyper-parametersfortrainingfromscratchonImageNet-1K.
Hyper-parameters Value
Inputresolution 224×224
Trainingepochs 300
Warmupepochs 20
Batchsize 1024
Optimizer AdamW
Peaklearningrate 1.0×10−3
Learningrateschedule cosine
Weightdecay 0.05
AdamWβ (0.9,0.999)
EMA 0.9999
Augmentation
Colorjitter 0.4
Randaugment 9/0.5
Erasingprob. 0.25
Mixupprob. 0.8
Cutmixprob. 1.0
Labelsmoothing 0.1
repeatedaugmentation True
Droppathrate linear(0.1,0.7)
Table6: Hyper-parametersforCOCOobjectdetection.
Hyper-parameters Value
Inputresolution 1024×1024
Finetuningepochs 12/36
Batchsize 16
Optimizer AdamW
Peaklearningrate 2×10−4
Learningrateschedule Step(11)/Step(27,33)
Weightdecay 1×10−4
Adamβ (0.9,0.999)
Augmentation
Randomflip 0.5
Droppathrate 0.5
B.4 ContrastiveLanguage-ImagePre-training(CLIP)
Thehyper-parametersforContrastiveLanguage-ImagePre-trainingonLaion-400m[56]areprovided
inTab.9.
17Table7: Hyper-parametersforpre-trainingonImageNet-21K.
Hyper-parameters Value
Inputresolution 192×192
Trainingepochs 90
Warmupepochs 5
Batchsize 4096
Optimizer AdamW
Peaklearningrate 1.0×10−3
Learningrateschedule cosine
Weightdecay 0.05
AdamWβ (0.9,0.999)
EMA 0.9999
Augmentation
Mixupprob. 0.8
Cutmixprob. 1.0
Labelsmoothing 0.1
Droppathrate linear(0.1,0.5)
Table8: Hyper-parametersforfinetuningonImageNet-1K.
Hyper-parameters Value
Inputresolution 384×384
Finetuningepochs 20
Warmupepochs 2
Batchsize 1024
Optimizer AdamW
Peaklearningrate 4×10−5
Learningrateschedule cosine
Weightdecay 0.05
Adamβ (0.9,0.999)
Augmentation
Mixupprob. 0.8
Cutmixprob. 1.0
Labelsmoothing 0.1
Droppathrate linear(0.1,0.7)
Table9: Hyper-parametersforcontrastivevision-languagepre-trainingonLaion-400m.
Hyper-parameters Value
Inputresolution 224×224
Trainingepochs 32
Warmupepochs 20000iters
Batchsize 32768
Optimizer AdamW
Peaklearningrate 5×10−4
Learningrateschedule cosine
Weightdecay 0.1
AdamWβ (0.9,0.98)
18