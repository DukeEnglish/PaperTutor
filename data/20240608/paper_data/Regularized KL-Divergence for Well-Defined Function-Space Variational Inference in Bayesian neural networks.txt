Regularized KL-Divergence for Well-Defined
Function-Space Variational Inference in Bayesian
neural networks
TristanCinquin RobertBamler
UniversityofTübingen UniversityofTübingen
tristan.cinquin@uni-tuebingen.de robert.bamler@uni-tuebingen.de
Abstract
Bayesianneuralnetworks(BNN)promisetocombinethepredictiveperformanceof
neuralnetworkswithprincipleduncertaintymodelingimportantforsafety-critical
systemsanddecisionmaking. However,posterioruncertaintyestimatesdepend
onthechoiceofprior,andfindinginformativepriorsinweight-spacehasproven
difficult. Thishasmotivatedvariationalinference(VI)methodsthatposepriors
directly on the function generated by the BNN rather than on weights. In this
paper,weaddressafundamentalissuewithsuchfunction-spaceVIapproaches
pointedoutbyBurtetal.(2020),whoshowedthattheobjectivefunction(ELBO)
isnegativeinfiniteformostpriorsofinterest. Oursolutionbuildsongeneralized
VI(Knoblauchetal.,2019)withtheregularizedKLdivergence(Quang,2019)
andis,tothebestofourknowledge,thefirstwell-definedvariationalobjectivefor
function-spaceinferenceinBNNswithGaussianprocess(GP)priors. Experiments
show that our method incorporates the properties specified by the GP prior on
synthetic and small real-world data sets, and provides competitive uncertainty
estimatesforregression,classificationandout-of-distributiondetectioncompared
toBNNbaselineswithbothfunctionandweight-spacepriors.
1 Introduction
Neuralnetworkshaveshownimpressiveresultsinmanyfieldsbutfailtoprovidewell-calibrated
uncertaintyestimates,whichareessentialinapplicationsassociatedwithrisk,suchashealthcare
(Kompaetal.,2021;Abdullahetal.,2022)orfinance(Bewetal.,2019;Wong,2023).Bayesianneural
networks(BNNs)offertocombinethescalabilityandpredictiveperformanceofneuralnetworks
withprincipleduncertaintymodelingbyexplicitlycapturingepistemicuncertainty,i.e.,uncertainty
resultingfromfinitetrainingdata. WhilethechoiceofpriorintheBayesianframeworkstrongly
affectstheuncertaintyestimateslaterobtainedfromtheposterior,specifyinginformativepriorson
BNNweightshasprovendifficultandishypothesizedtohavelimitedtheirpracticalapplicability
(Knoblauchetal.,2019;Cinquinetal.,2021;Tranetal.,2022). Forinstance,thedefaultisotropic
Gaussianprior,whichisoftenchosenfortractabilityratherthanforthebeliefsitcarries(Knoblauch
etal.,2019),isknowntohavepathologicalbehaviorinsomecases(Tranetal.,2022). Apromising
approachtosolvethisissueistoplacepriorsdirectlyonthefunctionrepresentedbytheBNNinstead
oftheweights. Whilebeingtechnicallymorechallenging,function-spacepriorsallowincorporating
interpretableknowledgeintotheinference,forinstanceenablingtheuseoftheextensiveGaussian
Process(GP)literaturetoimprovepriorselectionanddesign(WilliamsandRasmussen,2006).
Arecentlineofworkhasfocusedonusingfunction-spacepriorsinBNNswithvariationalinference
(VI)(Sunetal.,2019). VIisanappealingmethodbecauseofitssuccessfulapplicationtoBNNs,
itsflexibilityintermsofapproximateposteriorparameterization,anditsscalabilitytolargedatasets
andmodels(Hoffmanetal.,2013;Blundelletal.,2015;Tomczaketal.,2020). Unfortunately,in
Preprint.Underreview.
4202
nuJ
6
]GL.sc[
1v71340.6042:viXraMatern-1/2 kernel Matern-3/2 kernel Matern-5/2 kernel RBF kernel Periodic kernel
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure1: BNNinferenceonsyntheticdata(graycircles)withGaussianprocesspriorsencodingdif-
ferentpropertiessuchassmoothness(increasingfromMatern-1/2toRBF)andperiodicity(periodic).
thecontextofBNNswithfunction-spacepriors,theKullbach-Leibler(KL)divergenceterminthe
VI objective (ELBO) involves two intractabilities: (i) a supremum over infinitely many subsets
and (ii) access to the density of the distribution of function represented by the BNN, which has
noclosed-formexpression. Sunetal.(2019)proposetoaddressproblem(i)byapproximatingthe
supremumintheKLdivergencebyanexpectation,andproblem(ii)byusingimplicitscorefunction
estimators(whichmakethismethoddifficulttouseinpractice(MaandHernández-Lobato,2021)).
However,theproblemisactuallymoresevere. NotonlyistheKLdivergenceintractable,itisinfinite
inmanycasesofinterest(Burtetal.,2020),suchaswhenthepriorisanon-degenerateGPoraBNN
withadifferentarchitecture. Thus,inthese(andmanymore)situations,theKLdivergencecannot
evenbeapproximated. Asaconsequence,morerecentworkabandonsusingBNNsandinsteaduses
deterministicneuralnetworkstoparameterizebasisfunctions(MaandHernández-Lobato,2021)ora
GPmean(Wildetal.,2022). Theonlypriorwork(Rudneretal.,2022b)onfunction-spaceVIin
BNNsthatovercomestheissuepointedoutbyBurtetal.(2020)doessobydeliberatelylimiting
itselftocaseswheretheKLdivergenceisknowntobefinite(bydefiningthepriorasthepushforward
ofaweight-spacedistribution). Therefore,themethodbyRudneretal.(2022b)suffersfromthesame
issuesregardingpriorspecificationasanyotherweight-spaceinferencemethodforBNNs.
Inthispaper,weaddresstheargumentbyBurtetal.(2020)thatVIdoesnotprovideavalidobjective
forinferenceinBNNswithgenuinefunction-spacepriors,andweproposetoapplytheframeworkof
generalizedvariationalinference(Knoblauchetal.,2019). Wepresentasimplemethodforfunction-
spaceinferencewithGPpriorsthatbuildsontheso-calledregularizedKLdivergence(Quang,2019),
whichgeneralizestheconventionalKLdivergenceandisfiniteforanypairofGaussianmeasures.
WeobtainaGaussianmeasureforthevariationalposteriorbyconsideringthelinearizedBNNfrom
Rudner et al. (2022b), and we are free to choose a prior from a large set of GPs which have an
associatedGaussianmeasureontheconsideredfunctionspace. WhiletheregularizedKLdivergence
isstillintractable,itcanbeconsistentlyestimatedfromsampleswithaknownerrorbound. Wefind
thatourmethodeffectivelyincorporatesthebeliefsspecifiedbyGPpriorsintotheinferenceprocess
(seeFigure1)andprovidescompetitiveperformancecomparedtoBNNbaselines. Tothebestofour
knowledge,ourmethodisthefirsttoprovideawell-definedobjectiveforfunction-spaceinferencein
BNNswithinformativeGPpriors. Ourcontributionsaresummarizedbelow:
1. WeproposetousegeneralizedVIwiththeso-calledregularizedKLdivergencetomitigate
theissueofaninfiniteKLdivergencewhenusingVIinBNNswithfunction-spacepriors.
2. Wepresentanew,well-definedobjectiveforfunction-spaceinferenceinthelinearizedBNN
withGPpriors,whichresultsinasimplealgorithm.
3. Weanalyzeourmethodempiricallyonsyntheticandreal-worlddatasets,andfindthatit
accuratelycapturesstructuralpropertiesspecifiedbytheGPpriorandprovidescompeti-
tiveuncertaintyestimatesforregression,classification,andout-of-distributiondetection
comparedtoBNNbaselineswithbothfunction-spaceandweight-spacepriors.
Thepaperisstructuredasfollows.Section2introducesfunction-spaceVIinBNNsandtheregularized
KLdivergence. Section3presentsourproposedmethodforgeneralizedfunction-spaceVI(GFSVI)
inBNNs. Section4presentsexperimentalresults. WediscussrelatedworkinSection5andconclude
inSection6.
2 Background
Inthissection,weprovidebackgroundonfunction-spacevariationalinference(VI)inBNNsand
discussthefundamentalissueofaninfiniteKL-divergence. WethenintroducetheregularizedKL
divergence,whichisthebasisforoursolutionpresentedinSection3.
22.1 Function-spaceVIinBNNs
Weconsideraneuralnetworkf(·;w)withweightsw ∈Rp,andadatasetD ={(x ,y )}N with
i i i=1
featuresx ∈X ⊂Rdandvaluesy ∈Y. BayesianNeuralNetworks(BNNs)arespecifiedfurther
i i
byalikelihoodfunctionp(D|w)=(cid:81)N
p(y |f(x ;w))and—traditionally—apriorp(w)onthe
i=1 i i
weights,andoneseekstheposteriordistributionp(w|D)∝p(D|w)p(w). Themethodproposed
inthispaperbuildsonvariationalinference(VI),whichapproximatesp(w|D)withavariational
distributionq (w),whosevariationalparametersϕmaximizetheevidencelowerbound(ELBO),
ϕ
L(ϕ):=E [logp(D|w)]−D (q ∥p) (2.1)
qϕ(w) KL ϕ
whereD istheKullback-Leibler(KL)divergence
KL
D (q ∥p):=E (cid:2) log(cid:0) q (w)(cid:14) p(w)(cid:1)(cid:3) . (2.2)
KL ϕ qϕ(w) ϕ
At test time, we approximate the predictive distribution for given features x∗ as p(y∗|x∗) =
E (cid:2) p(y∗|f(x∗;w))(cid:3) ≈E (cid:2) p(y∗|f(x∗;w))(cid:3) .
p(w|D) qϕ(w)
Function-spacevariationalinference Sinceweightsofneuralnetworksarenotinterpretable,we
abandontheweight-spacepriorp(w)andinsteadposeapriorPdirectlyonthefunctionf(·;w),
whichwedenotesimplyasf whenthereisnoambiguity. Here,thesymbolPdenotesaprobability
measurethatdoesnotadmitadensitysincethefunctionspaceisinfinite-dimensional. Wecompute
theexpectedlog-likelihoodasinthefirsttermofEq.2.1,butwecalculatetheKL-term(Eq.2.2)
usingthepushforwardofq (w)alongthemappingw (cid:55)→ f(·;w), whichdefinesthevariational
ϕ
measureQ ,resultingintheELBOinfunctionspace,
ϕ
L(ϕ):=E [logp(D|w)]−D (Q ∥P) (2.3)
qϕ(w) KL ϕ
withD theKLdivergencebetweenmeasuresgivenby
KL
(cid:90) (cid:18) dQ (cid:19)
D (Q ∥P)= log ϕ(f) dQ . (2.4)
KL ϕ dP ϕ
Here, the Raydon-Nikodym derivative dQ /dP generalizes the density ratio from Eq. 2.2. Like
ϕ
Eq.2.1,theELBOinEq.2.3isalowerboundontheevidence(Burtetal.,2020). Infact,ifPisthe
push-forwardofp(w)thenEq.2.3isatighterboundthanEq.2.1bythedataprocessinginequality,
D (Q ∥P) ≤ D (q ∥p). However, we motivated function-space VI to avoid weight-space
KL ϕ KL ϕ
priors,andinthiscasetheboundinEq.2.3canbelooser. Wewillindeedseebelowthatthebound
becomesinfinitelylooseinpractice,andwethusproposeadifferentobjectiveinSection3.
TwointractabilitiespreventdirectlymaximizingtheELBOinfunctionspace(Eq2.3). First,itisnot
obvioushowtopracticallyevaluateorestimatetheKLdivergencebetweentwomeasuresinfunction
spaceinEq2.4. Sunetal.(2019)showedthatitcanbeexpressedasasupremumofKLdivergences
betweenfinite-dimensionaldistributions,
D (Q ∥P)= sup D (q (f(x))∥p(f(x))). (2.5)
KL ϕ KL ϕ
x∈XM
Here, x = {x(i)}M ∈ XM is a set of M points in feature space X, and q (f(x)) and p(f(x))
i=1 ϕ
are densities of the marginals of Q and P on {f(x(i))}M respectively. To obtain a tractable
ϕ i=1
approximationofthesupremumoverinfinitelymanysets,Sunetal.(2019)replaceitbyanexpectation
andRudneretal.(2022b)estimateitfromsamples.
Second,wecannotexpressthepushforwardmeasureQ ofq (w)inaclosedformbecausetheneural
ϕ ϕ
networkisnonlinear. Previousworkhasproposedtomitigatethisissueusingimplicitscorefunction
estimators(Sunetal.,2019)oralinearizedBNNf toobtainaclosed-formGaussianvariational
L
measure(Rudneretal.,2022a,b). OurproposalinSection3followsthelinearizedBNNapproach
asitonlyminimallymodifiestheBNN,preservingmostofitsinductivebias(Maddoxetal.,2021)
whileconsiderablysimplifyingtheproblembyturningthepushforwardofq (w)intoaGaussian
ϕ
process(GP).Morespecifically,weconsideraGaussianvariationaldistributionq (w)=N(m,S)
ϕ
withparametersϕ={m,S},andweobtainf bylinearizingf intheweightsaroundw =m,
L
f (x;w):=f(x;m)+J(x;m)(w−m) where J(x;m)=∇ f(x;w)| . (2.6)
L w w=m
Thus, for w ∼ q (w) we have, f (x;w) ∼
N(cid:0) f(x;m),J(x;m)SJ(x;m)⊤(cid:1)
∀x, and the
ϕ L
functionf (·;w)isadegenerateGP(asrank(J(·;m)SJ(·;m)⊤)≤numberofweights<∞),
L
f
∼GP(cid:0) f(·;m),J(·;m)SJ(·;m)⊤(cid:1)
. (2.7)
L
3D (Q ∥P)isinfiniteinmostpracticallyrelevantcases. Burtetal.(2020)pointoutaneven
KL ϕ
moresevereissueoffunction-spaceVIinBNNs: D (Q ∥P)isinfactinfiniteinmanyrelevant
KL ϕ
cases,inparticularfornon-degenerateGP-priorswithBNNs. Thus,approximationsinthesecases
are futile. The proof in Burt et al. (2020) is somewhat involved, but the fundamental reason for
D (Q ∥P) = ∞ is that Q has support on a finite-dimensional submanifold of the infinite-
KL ϕ ϕ
dimensionalfunctionspace,whilethemeasurePinducedbya(non-degenerate)GPpriorhassupport
ontheentirefunctionspace. ThatsuchadimensionalitymismatchcanleadtoinfiniteKLdivergence
canbeseeninafinite-dimensionalexample: considertheKL-divergencebetweentwoGaussians
inRn forsomen ≥ 2, oneofwhichhassupportontheentireRn (i.e., itscovariancematrixΣ
1
hasfullrank)whiletheotheronehassupportonlyonapropersubspaceofRn(i.e.,itscovariance
matrixΣ issingular). TheKLdivergencebetweentwomultivariateGaussianshasaclosedform
2
expression(Eq2.9withγ =0)thatcontainslogdet(cid:0) Σ−1Σ (cid:1) ,whichisinfiniteforsingularΣ .
2 1 2
WefindthatthefactthatD (Q ∥P)=∞hasseverepracticalconsequencesevenwhentheKL
KL ϕ
divergence is only estimated from samples. It naturally explains the stability issues discussed in
AppendixD.1ofSunetal.(2019)(wecomparetheauthors’solutiontothisstabilityissuetoour
methodinSection3.2). Surprisingly,similarcomplicationsariseeveninthesetupbyRudneretal.
(2022b),whichperformsVIinfunctionspacewiththepushforwardofaweight-spaceprior. While
theKLdivergenceistechnicallyfinitebecausepriorandvariationalposteriorhavethesamesupport,
numericalerrorsleadtomismatchingsupportsandthustostabilityissueseventhere.
Insummary,theELBOforVIinBNNsisnotwell-definedforalargeclassofinterestingfunction-
spacepriors. InSection3,weproposeasolutionbyusingtheso-calledregularizedKLdivergence,
whichweintroducenext.
2.2 RegularizedKLdivergenceforGaussianmeasures
Oursolutiontothenegativeinfinitefunction-spaceELBObuildsonaregularizedKLdivergence,
whichisexpressedintermsofGaussianmeasuresforthevariationalposteriorandprior. Weobtain
theseGaussianmeasuresfromcorrespondingGaussianprocesses(GP).Wefirstdiscussunderwhich
conditionsaGPinducesaGaussianmeasure,andthenpresenttheregularizedKLdivergence.
GaussianmeasuresandGaussianprocesses TheregularizedKLdivergenceisdefinedinterms
ofGaussianmeasures,andthusweneedtoverifythattheGPvariationalposteriorinducedbythe
linearizedBNN(Eq2.7)hasanassociatedGaussianmeasure.WeconsidertheHilbertspaceL2(X,ρ)
ofsquare-integrablefunctionswithrespecttoaprobabilitymeasureρonacompactsetX ⊂Rd,with
(cid:82)
innerproduct⟨f,g⟩= f(x)g(x)dρ(x). Thisisnotarestrictiveassumptionaswecantypically
X
bound the region in feature space that contains the data and any points where we might want to
evaluatetheBNN.
Definition2.1(Gaussianmeasure,Kerriganetal.(2023),Definition1). Let(Ω,B,P)beaprobability
space. A measurable function F : Ω (cid:55)→ L2(X,ρ) is called a Gaussian random element (GRE)
if for any g ∈ L2(X,ρ) the random variable ⟨g,F⟩ has a Gaussian distribution on R. For every
GRE F, there exists a unique mean element m ∈ L2(X,ρ) and a finite trace linear covariance
operator C : L2(X,ρ) (cid:55)→ L2(X,ρ) such that ⟨g,F⟩ ∼ N(⟨g,m⟩,⟨Cg,g⟩) for all g ∈ L2(X,ρ).
ThepushforwardofPalongF denotedPF :=F PisaGaussian(probability)measureonL2(X,ρ).
#
GaussianmeasuresgeneralizeGaussiandistributionstoinfinite-dimensionalspaceswheremeasures
do not have associated densities. Following Wild et al. (2022), we notate the Gaussian measure
obtainedfromtheGREF withmeanelementmandcovarianceoperatorC asPF :=N(m,C).
GaussianprocessesprovideaconvenientmethodtospecifyGaussianmeasuresoverfunctionsvia
mean and covariance functions (Wild et al., 2022; Kerrigan et al., 2023). A GP f ∼ GP(µ,K)
has an associated Gaussian measures in L2(X,ρ) if its mean function satisfies µ ∈ L2(X,ρ)
(cid:82)
and its covariance function K is trace-class, i.e., if K(x,x)dρ(x) < ∞ (Wild et al., 2022,
X
Theorem1). TheGPvariationalposteriorinducedbythelinearizedBNN(Eq2.7)satisfiesboth
properties as neural networks are well-behaved functions on the compact subset X ⊂ Rd. It
thus induces a Gaussian measure QF ∼ N(m ,C ) with mean element m = f(·;m) and
ϕ Q Q Q
covarianceoperatorC g(·) = (cid:82) J(·;m)SJ(x′,m)⊤g(x′)dρ(x′). Itturnsoutthattheinfinite
Q X
KLdivergencediscussedinSection2.1iseasiertoproveforGaussianmeasures,andweprovidethe
proofinAppendixA.1.1fortheinterestedreader.
4Definition 2.2 (Regularized KL divergence, Quang (2022) Definition 5). Let ν = N(m ,C )
1 1 1
and ν = N(m ,C ) be two Gaussian measures with m ,m ∈ L2(X,ρ) and C ,C bounded,
2 2 2 1 2 1 2
self-adjoint,positiveandtrace-classlinearoperatorsonL2(X,ρ). Letγ ∈R,γ >0befixed. The
regularizedKLdivergenceisdefinedasfollows,
Dγ (ν ∥ν ):=1 ⟨m −m ,(C +γI)−1(m −m )⟩+ 1 Tr (cid:2) (C +γI)−1(C +γI)−I(cid:3)
KL 1 2 2 1 2 2 1 2 2 X 2 1
− 1 logdet (cid:2) (C +γI)−1(C +γI)(cid:3) . (2.8)
2 X 2 1
Foranyγ >0,theregularizedKLdivergenceiswell-definedandfinite,eveniftheGaussianmeasures
aresingular(Quang,2019). ItconvergestotheconventionalKLdivergence(ifitiswell-defined)for
γ →0(Quang,2022,Theorem6). Furthermore,andimportantlyforourapplicationtoVI,iftwo
Gaussianmeasuresν andν areinducedbyGPsf ∼GP(µ ,K )fori=1,2,thenDγ (ν ∥ν )
1 2 i i i KL 1 2
canbeconsistentlyestimated(Quang,2022)fromafinitenumberM ofsamplesusingtheestimator
Dˆγ (ν ∥ν ):=1 (m −m )⊤(Σ(γ))−1(m −m )+ 1 Tr(cid:2) (Σ(γ))−1Σ(γ)−I (cid:3)
KL 1 2 2 1 2 2 1 2 2 2 1 M
−
1 logdet(cid:2) (Σ(γ))−1Σ(γ)(cid:3)
(2.9)
2 2 1
withm :=µ (x)andΣ(γ) :=K (x,x)+γMI whereµ (x)andK (x,x)arethemeanvector
i i i i M i i
andthecovariancematrixobtainedbyevaluatingµ andK respectively, atmeasurementpoints
i i
x={x(i)}M ,x(1),...,x(M) i∼.i.dρ(x).Theright-handsideofEq.2.9istheclosed-formexpression
i=1
fortheKL-divergencebetweenmultivariateGaussiandistributionsN(m ,Σ(γ))andN(m ,Σ(γ)).
1 1 2 2
Quang (2022) shows that the absolute error of the estimator is bounded by O(1/M) with high
probabilitywithconstantsdependingonγ andpropertiesoftheGPmeanandcovariancefunctions
(seeAppendixA.1.2fortheexactbound).
3 Generalizedfunction-spaceVIwiththeregularizedKLdivergence
Thissectionpresentsourproposedgeneralizedfunction-spacevariationalinference(GFSVI)method,
whichaddressestheproblemoftheinfiniteKLdivergencediscussedinSection2.1,whichwetake
for an indication that VI is too restrictive if one wants to use genuine function-space priors. We
insteadconsidergeneralizedvariationalinference(Knoblauchetal.,2019),whichreinterpretsthe
ELBOinEq.2.1asaregularizedexpectedlog-likelihoodandexploresalternativedivergencesfor
theregularizer. Specifically,weproposetousetheregularizedKLdivergence. Thepresentationin
thissectionbuildsheavilyontoolsintroducedinSection2,whichturnouttofittogetherperfectly:
thepushforwardofaGaussianvariationaldistributioninweight-spacethroughthelinearizedneural
network(Eq.2.6)inducesaGPvariationalposterior(Eq.2.7)thatadmitsaGaussianmeasureon
L2(X,ρ)(Definition2.1). FurtherselectingaGPpriorwhichhasanassociatedGaussianmeasureon
L2(X,ρ)allowsustousetheregularizedKLdivergence(Eq.2.8). WepresenttheGFSVImethodin
Section3.1anddiscussconnectionstopriorworkinSection3.2.
3.1 Generalizedfunction-spacevariationalinference
Wepresentasimpleandwell-definedobjectiveforfunction-spaceinference,andanalgorithmforits
optimization.
Objectivefunction WestartfromtheELBOinEq.2.3,whereweusetheGaussianvariationalmea-
sureQF inducedbythepushforwardofaGaussianvariationaldistributionq (w)=N(w|m,S)
ϕ ϕ
along the linearized network f (Eq. 2.6). The function-space prior may be any GP that has
L
an associated Gaussian measure PF on L2(X,ρ) (see Definition 2.1 and discussion below it).
We now replace the KL divergence in the ELBO with the regularized KL divergence Dγ
KL
(Eq. 2.8), which is well-defined and finite for any pair of Gaussian measures. For a likelihood
p(D|w)=(cid:81)N
p(y |f (x ;w)),wethusobtain
i=1 i L i
N
L(ϕ):=(cid:88) E [logp(y |f (x ;w))]−Dγ (cid:0)QF ∥PF(cid:1) . (3.1)
qϕ(w) i L i KL ϕ
i=1
5Estimationandoptimization Theexpected Algorithm1:Generalizedfunction-spacevaria-
log-likelihood(firsttermofEq.3.1)canbeesti- tionalinference(GFSVI)
matedbysamplingfromq (w).ForaGaussian
ϕ Input:linearizedBNNf withmeasureQF,Gaussian
likelihood, it can also be computed in closed L ϕ
form because (unlike Rudner et al. (2022b))
processpriorGP(µ,K)withmeasurePF,
measurementpointdistributionρ(x),data
weusethelinearizednetworkf toparameter-
L D={(x ,y )}N ,batchsizeB,γ >0.
izethelikelihoodfunction,whichmadetrain- i i i=1
ing more stable in our experiments. We esti- foreachminibatch(x B,y B)∼Ddo
mate the regularized KL divergence (second Calculateℓˆ 1 = N BE qϕ(w)[logp(y B|f L(x B,w))];
termofEq.3.1)usingitsconsistentestimator Drawmeasurementpointsx={x(i)}M i.∼i.d.ρ(x);
i=1
( Js (e xe ;E mq. )S2. J9 (), xw ;mith )⊤m +1 γ= Mf I( Mx ,;m m) 2, =Σ( 1 µγ) (x=
),
C Ca al lc cu ul la at te eℓ Lˆ ˆ2 (ϕ= )D =ˆ Kγ ℓˆL(cid:0)Q −F ϕ ℓˆ(cid:12) (cid:12)(cid:12) (cid:12) ;PF(cid:1) usingx(Eq.2.9);
andΣ( 2γ) =K(x,x)+γMI M,whereµandK Updateϕusingaste1 pint2 hedirection∇ ϕLˆ(ϕ);
arethemeanfunctionandkerneloftheGPprior,
end
and x = {x(i)}M , x(1),...,x(M) i∼.i.d ρ(x)
i=1
aremeasurementpointssampledfromtheprobabilitymeasureρ(x)onX. Wemaximizetheesti-
matedobjectiveoverthemeanmandcovarianceS oftheGaussianvariationaldistributionq (w),
ϕ
andoveranylikelihoodparameter(e.g.,σ foraGaussianlikelihood),seeAlgorithm1.AppendixA.2
y
providesexplicitexpressionsfortheestimatorwithGaussianandCategoricallikelihoods.
Technical details (γ and ρ) It turns out that increasing γ reduces the influence of the prior on
inference(seeFigure17inAppendix). Atthesametime,γactsasjitterthatpreventsnumericalerrors
(seeSection3.2). Werecommendsettingγ largeenoughtoavoidnumericalerrorsbutsufficiently
smalltostronglyregularizetheobjectiveinEq.3.1(seeFigure15inAppendix). Wefoundthat
GFSVI is robust to a wide range of values and fixed γ = 10−10. Furthermore, the probability
measureρdefinedwithL2(X,ρ)hastoassignnon-zeroprobabilitytoanyopensetofX toregularize
theBNNonallofitssupport. FollowingRudneretal.(2022b),wedrawmeasurementpointsfroma
uniformdistributionoverX whenusingtabulardataandexploredifferentconfigurations(suchas
samplesfromotherdatasets)whenusinghigh-dimensionalimagedata(seeAppendixA.3.4).
3.2 Connectionstopriorwork
TFSVI (Rudner et al., 2022b) and FVI (Sun et al., 2019) solve stability issues by introducing
jitter/noise, which has a similar effect as the regularization in Eq. 2.8. However, as mentioned
in Section 2.1, TFSVI introduces jitter only to overcome numerical issues and is fundamentally
restrictedtopriorspecificationinweightspacesinceitsfunction-spaceprioristhepushforwardofa
weight-spaceprior. FVIdoesnotlinearizetheBNN,andthereforedoesnothaveaccesstoanexplicit
variationalmeasureinfunctionspace. Thisseverelycomplicatestheestimationof(gradientsof)the
KLdivergence,andtheauthorsresorttoimplicitscorefunctionestimators,whichmaketheirmethod
difficulttouseinpractice(MaandHernández-Lobato,2021). OurproposedGFSVIdoesnotsuffer
fromthesedifficultiesasthevariationalposteriorisanexplicit(Gaussian)measure. Thisallowsusto
estimatethe(regularized)KLdivergencewithoutsamplinganynoiseorusingimplicitscorefunction
estimators.
4 Experiments
Inthissection,weevaluateourgeneralizedfunction-spacevariationalinference(GFSVI)method
qualitatively on synthetic data (Section 4.1) and quantitatively on real-world data (Section 4.2).
We find that GFSVI accurately captures structural properties specified by the GP prior, that it
approximatestheexactGaussianprocess(GP)posteriormorefaithfullythanBNNbaselines,andthat
itperformscompetitivelyonregression,classificationandout-of-distributiondetectiontasks. We
alsodiscusstheinfluenceoftheBNN’sinductivebiases.
Baselines WecomparetheproposedGFSVImethodtotwoweight-spaceinferencemethods(mean-
fieldvariationalinference(Blundelletal.,2015)andlinearizedLaplace(Immeretal.,2021))andto
threefunction-spaceinferencemethods(FVI(Sunetal.,2019),TFVSI(Rudneretal.,2022b)and
VIP(Maetal.,2019),whereTFSVIperformsinferenceinfunctionspacebutwiththepushforward
of a weight-space prior and VIP using a BNN prior). All BNNs have the same architecture and
6Exact posterior (GP) GFSVI (ours) FVI
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
TFSVI MFVI Laplace
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure2: Inferenceonsyntheticdata(graycircles)usinganRBFpriorforfunction-spacemethods
GFSVIandFVI.TheproposedGFSVIprovidesthebestapproximationoftheexactGPposterior.
fully-factorized Gaussian approximate posterior. We also include results for a sparse GP with a
posteriormeanparameterizedbyaneuralnetwork(GWI)(Wildetal.,2022),aswellasforaGaussian
Process(GP)(WilliamsandRasmussen,2006)(whenthesizeofthedatasetallowsit), andfora
sparseGP(Hensmanetal.,2013)forregressiontasks. W˙ econsidertheGP,sparseGPandGWIas
goldstandardsastheyrepresenttheexact(ornearexact)posteriorformodelswithGPpriors.
4.1 Qualitativeresultsonsyntheticdata
Regression Weconsidersyntheticregressiondatawithaone-dimensionalfeature-spaceX ⊂R,
where the values y are randomly sampled around sin(2πx ) (circles in Figures 1-11, see Ap-
i i
pendixA.3.1). ThegreenlinesinFigures1-8and11showfunctionssampledfromthe(approximate)
posteriors, and the red lines are the inferred mean functions. Figure 2 compares GFSVI with an
RBFGPpriortobaselinesandtotheexactposterior. WefindthatGFSVIvisuallymatchesthetrue
posteriorbest(thesameholdsforaMatern-1/2prior,seeFigure5intheAppendix). Figure1in
the Introduction and Figure 6 in the Appendix show that GFSVI notably adapts to varying prior
assumptions(varyingsmoothnessandlengthscale,respectively). Inaddition,Figures4and8inthe
AppendixshowthatGFSVIprovidesstrongregularizationwhenthedatagenerativeprocessisnoisy,
andthatitcanbetrainedwithfewermeasurementpointsM thanFVIwithoutsignificantdegradation.
Classification Wefurtherconsiderabinaryclassificationtaskderivedfromthe2-dimensionaltwo
moonsdataset(Pedregosaetal.,2011),seeredandbluedotsinFigures9and10intheAppendix.
Thefirstandsecondrowinbothfiguresshowtheinferredmeanprobabilityofclass1(bluedots)
andits2-standarddeviationswithrespecttoposteriorsamples. Justlikeforregression,wefindthat
GFSVIcapturesthebeliefsoftheRBFandMatern-1/2GPpriorsbetterthanallBNN-baselines,and
showsgreateruncertaintyoutsideofthesupportofthedatathantheBNNbaselines.
Inductivebiases Figure11intheAppendixcomparesGFSVItotheexactGP-posterioracross
twodifferentpriorsandthreemodelarchitectures(detailsinAppendixA.3.1). Wefindthat,with
ReLUactivations,smallmodelsarepronetounderfittingforsmoothpriors(RBF),andtocollapsing
uncertainty for rough priors (Matern-1/2). By contrast, with smooth activations (Tanh), smaller
modelssuffice,andtheyarecompatiblewithmoststandardGPpriors(theresultsshowninFigure11
extend to RBF, Matern, and Rational Quadratic in our experiments). We also analyzed how the
numberM ofmeasurementpointsaffectsperformance. Figures7and14inAppendixshowthat
capturingthebeliefsofroughGPpriorsandestimatingDγ withthesepriorsrequireslargerM.
KL
4.2 Quantitativeresultsonreal-worlddata
WestudythefidelityofGFSVIcomparedtothetrueGP-posterioronrealdata,andevaluateGFSVI
onregression,classification,andout-of-distributiondetectiontasks. Inalltables,weboldthehighest
scoreandanyscorewhoseerrorbar(standarderror)overlapswiththehighestscore’serrorbar.
Oceancurrentmodeling WetesttheextendtowhichGFSVIcanincorporatepriorknowledge
specifiedviaaGPpriorbyconsideringtheproblemofmodelingoceancurrentsintheGulfofMexico.
Wearegiven20two-dimensionalvelocityvectorscollectedfromoceandrifterbuoys,andweare
7
roirp
ecaps-noitcnuF
roirp
ecaps-thgieWTable1: Testexpectedlog-likelihood(higherisbetter)ofevaluatedmethodsonregressiondatasets.
GFSVIperformscompetitivelycomparedtoallBNNbaselinesandobtainsthebestmeanrank.
DATASET FUNCTION-SPACEPRIORS WEIGHT-SPACEPRIORS GAUSSIANPROCESSES(GOLDSTANDARDS)
GFSVI(OURS) FVI TFSVI MFVI VIP LAPLACE GWI SPARSEGP GP
BOSTON -0.733±0.144 -0.571±0.113 -1.416±0.046 -1.308±0.052 -0.722±0.196 -0.812±0.205 -0.940±0.145 -0.884±0.182 -1.594±0.556
CONCRETE -0.457±0.041 -0.390±0.017 -0.983±0.012 -1.353±0.018 -0.427±0.050 -0.715±0.025 -0.744±0.079 -0.966±0.025 -2.099±0.421
ENERGY 1.319±0.052 1.377±0.042 0.797±0.098 -0.926±0.197 1.046±0.378 1.304±0.043 0.461±0.093 -0.206±0.027 -0.205±0.022
KIN8NM -0.136±0.013 -0.141±0.023 -0.182±0.011 -0.641±0.225 -0.102±0.013 -0.285±0.014 -0.708±0.054 -0.443±0.014 (infeasible)
NAVAL 3.637±0.132 2.165±0.194 2.758±0.044 1.034±0.160 1.502±0.061 3.404±0.084 -0.301±0.254 4.951±0.014 (infeasible)
POWER 0.044±0.011 0.031±0.021 0.007±0.013 -0.003±0.015 0.036±0.018 -0.002±0.019 0.043±0.009 -0.100±0.010 (infeasible)
PROTEIN -1.036±0.005 -1.045±0.005 -1.010±0.004 -1.112±0.007 -0.994±0.007 -1.037±0.006 -1.050±0.009 -1.035±0.002 (infeasible)
WINE -1.289±0.040 -1.215±0.007 -2.138±0.221 -1.248±0.018 -1.262±0.025 -1.249±0.025 -1.232±0.038 -1.240±0.037 -1.219±0.035
YACHT 1.058±0.080 0.545±0.735 -1.187±0.064 -1.638±0.030 -0.062±1.378 0.680±0.171 0.441±0.138 -0.976±0.092 -0.914±0.045
WAVE 5.521±0.036 6.612±0.008 5.148±0.117 6.883±0.008 4.043±0.093 4.658±0.027 1.566±0.123 4.909±0.001 (infeasible)
DENMARK -0.487±0.013 -0.801±0.005 -0.513±0.013 -0.675±0.007 -0.583±0.021 -0.600±0.008 -0.841±0.026 -0.768±0.001 (infeasible)
MEANRANK 1.545 2.000 2.727 3.455 2.091 2.455 - - -
Table 2: Test expected log-likelihood, accuracy, expected calibration error and OOD detection
accuracyonMNISTandFashionMNIST.
METRIC FUNCTION-SPACEPRIORS WEIGHT-SPACEPRIORS GP-BASED
GFSVI(RND) GFSVI(KMNIST) FVI(RND) FVI(KMNIST) TFSVI(RND) TFSVI(KMNIST) MFVI VIP LAPLACE GWI
LOG-LIKE.(↑) -0.033±0.000 -0.041±0.000 -0.145±0.005 -0.238±0.006 -0.047±0.003 -0.041±0.001 -0.078±0.001 -0.033±0.001 -0.108±0.002 -0.090±0.003
ACC.(↑) 0.992±0.000 0.991±0.000 0.976±0.001 0.943±0.001 0.989±0.000 0.989±0.000 0.990±0.000 0.989±0.000 0.984±0.000 0.971±0.001
ECE(↓) 0.002±0.000 0.006±0.000 0.064±0.001 0.073±0.003 0.007±0.000 0.006±0.000 0.021±0.000 0.002±0.001 0.048±0.001 0.003±0.000
OODACC.(↑) 0.921±0.008 0.980±0.004 0.894±0.010 0.891±0.006 0.887±0.011 0.893±0.005 0.928±0.002 0.871±0.012 0.903±0.007 0.829±0.007
LOG-LIKE.(↑) -0.260±0.003 -0.294±0.006 -0.300±0.002 -0.311±0.005 -0.261±0.001 -0.261±0.002 -0.290±0.002 -0.252±0.001 -0.426±0.009 -0.260±0.001
ACC.(↑) 0.910±0.001 0.909±0.001 0.910±0.002 0.906±0.002 0.909±0.001 0.907±0.001 0.913±0.001 0.911±0.001 0.886±0.001 0.906±0.000
ECE(↓) 0.020±0.003 0.042±0.002 0.027±0.005 0.024±0.002 0.022±0.002 0.021±0.002 0.010±0.001 0.024±0.001 0.060±0.004 0.016±0.001
OODACC.(↑) 0.853±0.005 0.997±0.001 0.925±0.005 0.975±0.002 0.802±0.006 0.779±0.010 0.805±0.010 0.790±0.010 0.826±0.006 0.792±0.005
interestedinestimatingoceancurrents. WeusetheGulfDriftersdataset(LillyandPérez-Brunius,
2021)andwefollowthesetupbyShalashilin(2024). WecompareourGFSVItoaGPandtoTFSVI.
WeincorporateknownphysicalpropertiesoffluidmotionsbyapplyingtheHelmholtzdecomposition
totheGPprior(Berlinghierietal.,2023)aswellastotheneuralnetworks. Moredetailscanbefound
inAppendixA.3.2.
Results are shown in Table 3 and in Figure 3. We find Table 3: Results for the ocean current
thatincorporatingpriorknowledgeviaaninformativeGP modeling task. We find that incorpo-
prior in GFSVI improves expected log-likelihood (Log- ratingpriorknowledgeviaaGaussian
like.) andmeansquared-error(MSE)overweight-space processpriorinGFSVIimprovesscore
priorsinTFSVI.However,forthisrelativelysmalldataset, overweight-spacepriorsinTFSVI.
exact GP inference is still possible, and it outperforms
bothBNN-basedpredictions. Thissuggeststhatthephys- DATASET GFSVI(OURS) TFSVI GP
LOG-LIKE. -6.627±0.753 -22.651±2.947 -0.507±0.000
icallymotivatedGPdescribesthetruefluiddynamicswell MSE 0.021±0.002 0.034±0.003 0.013±0.000
enoughthattheadditionalinductivebiasintroducedbya
neuralnetworkhurtsperformanceratherthanhelpingit. Inthefollowing,weconsiderexperiments
withlargerdatasets(makingexactGPinferencecomputationallyinfeasibleinmanycases),andwhere
structuralpriorknowledgeinfunctionspaceexistsbutisnotderivedfromstrictlawsofnature.
Regression WeassessthepredictiveperformanceofGFSVIondatasetsfromtheUCIrepository
(DuaandGraff,2017)(describedinTable6intheAppendix). Table1,andTable7intheAppendix,
showexpectedlog-likelihoodandmeansquarederror,respectively. Weperform5-foldcrossval-
idationandreportmeansandstandarderrorsacrossthetestfolds. Wealsorankthemethodsfor
eachdatasetandreportthemeanrankofeachmethodacrossalldatasets. SeeAppendixA.3.3for
moredetails. WefindthatGFSVIperformscompetitivelycomparedtobaselinesandobtainsthe
bestmeanrankforbothmetrics,matchingthetopperformingmethodsonnearlyalldatasets. In
particular,wefindthatusingGPpriorsinthelinearizedBNNwithGFSVIyieldsimprovementsover
theweight-spacepriorsusedinTFSVIandthatGFSVIperformsslightlybetterthanFVI.
Table4: Out-of-distributionaccuracy(higherisbetter)ofevaluatedmethodsonregressiondatasets.
GFSVI(ours)performscompetitivelyonOODdetectionandobtainsthehighestmeanrank.
DATASET FUNCTION-SPACEPRIORS WEIGHT-SPACEPRIORS GAUSSIANPROCESSES(GOLDSTANDARDS)
GFSVI(OURS) FVI TFSVI MFVI VIP LAPLACE GWI SPARSEGP GP
BOSTON 0.893±0.011 0.594±0.024 0.705±0.107 0.563±0.013 0.628±0.010 0.557±0.009 0.817±0.017 0.947±0.011 0.952±0.003
CONCRETE 0.656±0.016 0.583±0.022 0.511±0.003 0.605±0.012 0.601±0.024 0.578±0.015 0.730±0.020 0.776±0.006 0.933±0.004
ENERGY 0.997±0.002 0.696±0.017 0.997±0.001 0.678±0.014 0.682±0.037 0.782±0.020 0.998±0.001 0.998±0.001 0.998±0.001
KIN8NM 0.588±0.007 0.604±0.023 0.576±0.008 0.570±0.009 0.563±0.015 0.606±0.009 0.602±0.011 0.608±0.014 (infeasible)
NAVAL 1.000±0.000 1.000±0.000 1.000±0.000 0.919±0.017 0.621±0.059 1.000±0.000 1.000±0.000 1.000±0.000 (infeasible)
POWER 0.698±0.006 0.663±0.021 0.676±0.008 0.636±0.019 0.514±0.004 0.654±0.013 0.754±0.004 0.717±0.004 (infeasible)
PROTEIN 0.860±0.011 0.810±0.022 0.841±0.018 0.693±0.020 0.549±0.020 0.629±0.013 0.942±0.002 0.967±0.001 (infeasible)
WINE 0.665±0.013 0.517±0.004 0.549±0.015 0.542±0.009 0.706±0.028 0.531±0.007 0.810±0.008 0.781±0.014 0.787±0.007
YACHT 0.616±0.030 0.604±0.025 0.659±0.043 0.642±0.035 0.688±0.040 0.612±0.024 0.563±0.014 0.762±0.018 0.787±0.011
WAVE 0.975±0.005 0.642±0.004 0.835±0.034 0.658±0.026 0.500±0.000 0.529±0.005 0.903±0.001 0.513±0.001 (infeasible)
DENMARK 0.521±0.006 0.612±0.008 0.519±0.006 0.513±0.003 0.500±0.000 0.529±0.008 0.688±0.003 0.626±0.002 (infeasible)
MEANRANK 1.455 2.364 1.909 2.909 3.364 2.909 - - -
8
ATADTSINM
TSINMFGround Truth GFSVI (ours) GP TFSVI
0.8
27 27 0.6
26 26 0.4
25 25 0.2
0.0
-90 -88 -86 -84
Longitude 27 0.7
0.6
Ocean current 26 0.4
Drifting buoy 25 0.2
0.0
-90 -88 -86 -84 -90 -88 -86 -84 -90 -88 -86 -84
Longitude Longitude Longitude
Figure3: Resultsfortheoceancurrentmodelingexperiment. Wereportthemeanvelocityvectors
andthesquarederrorsofcomparedmethods. UnlikeTFSVI,wefindthatGFSVIaccuratelycaptures
oceancurrentdynamics.
Table5: Averagepoint-wiseWasserstein-2distance(lowerisbetter)betweenexactandapproximate
posteriorofreportedmethods. GFSVI(ours)providesamoreaccurateapproximationthanFVI.
DATASET BOSTON CONCRETE ENERGY WINE YACHT MEANRANK
GFSVI(OURS) 0.0259±0.0040 0.0499±0.0029 0.0035±0.0004 0.0571±0.0097 0.0036±0.0006 1.0
FVI 0.0469±0.0044 0.0652±0.0037 0.0037±0.0004 0.1224±0.0167 0.0052±0.0013 1.6
GPSPARSE 0.0074±0.0022 0.0125±0.0016 0.0042±0.0003 0.0170±0.0035 0.0035±0.0008 -
Classification WefurtherevaluatetheclassificationperformanceofourmethodontheMNIST
(LeCunetal.,2010)andFashionMNIST(Xiaoetal.,2017)imagedatasets. Wefitthemodelsona
randomsubsetof90%ofthetrainingset,usetheremaining10%asvalidationdata,andevaluateon
theprovidedtestsplit. Werepeatwith5differentrandomseedsandreportthemeanandstandard
erroroftheexpectedlog-likelihood,accuracyandexpectedcalibrationerror(ECE)ofthepredictive
distribution’smeaninTable2. ForGFSVI,FVI,andTFSVI,wetestedmeasurementpointsfrom
botharandom(RND)distributionρ(x)andfromKMNIST(seeAlgorithm1andAppendixA.3.4).
WefindthatGFSVIperformscompetitivelyonMNIST,exceedingtheexpectedlog-likelihoodand
accuracyoftop-scoringbaselines(seeTable2)andsimilarlytobestbaselinesonFashionMNIST.
Additionally,GFSVIyieldswell-calibratedmodelswithlowECE.
Out-of-distributiondetection Wefurtherevaluateourmethodbytestingifitsepistemicuncertainty
ispredictiveofout-of-distribution(OOD)data. Weconsidertwosettings: (i)withtabulardataand
aGaussianlikelihoodfollowingMalininetal.(2020), and(ii)withimagedataandacategorical
likelihood following Osawa et al. (2019). We report the accuracy of classifying OOD from in-
distribution(ID)datausinga(learned)thresholdonthepredictiveuncertainty. Additionaldetailsare
providedinAppendixA.3.5. Insetting(i)(Table4),wefindthatGFSVIperformscompetitivelyand
obtainsthehighestmeanrank. Likewise,insetting(ii)(Table8intheAppendix),GFSVIstrongly
outperformsallbaselineswhenusingtheKMNISTmeasurementpointdistributionρ(x). Wefindthat
withhigh-dimensionalimagedata,thechoiceofmeasurementpointdistributionhighlyinfluences
OODdetectionaccuracy. WeprovidemoredetailsonthispointinAppendixA.4.4. Inbothsettings,
wefindthatusingGPpriorswithGFSVIratherthanweight-spacepriorswithTFSVIisbeneficial,
andthatGFSVIimprovesoverFVI.
Variationalmeasureevaluation Table5evaluatesourinferencemethodbycomparingsamples
drawn from the exact posterior (where computationally feasible) with the approximate posterior
obtained with our method (GFSVI). More details are provided in Appendix A.3.6. We find that
GFSVIapproximatestheexactposteriormoreaccuratelythatFVI,obtainingahighermeanrank,but
worsethanthegoldstandardsparseGP,whichdemonstratestobemostaccurate.
5 Relatedwork
Inthissection,wereviewrelatedworkonfunction-spacevariationalinferencewithneuralnetworks,
andonapproximatingfunctions-spacemeasureswithweight-spacepriors.
9
edutitaL edutitaL
edutitaL
||[y|
,x]||2
||y
[y|
,x]||2Function-space inference with neural networks Prior work on function-space VI in BNNs
hasaddressedissues(i)and(ii)mentionedinSection2.1. Sunetal.(2019)address(i)(intractable
variationalposteriorinfunctionspace)byusingimplicitscorefunctionestimators,and(ii)(intractable
KLdivergence)byreplacingthesupremumwithanexpectation. Rudneretal.(2022a,b)address(i)
byusingalinearizedBNN(Khanetal.,2020;Immeretal.,2021;Maddoxetal.,2021), and(ii)
by replacing the supremum with a maximum over a finite set of samples. Other work abandons
approximatingtheposterioroverneuralnetworkweightsaltogetherandinsteadusesaBNNonlyto
specifyaprior(Maetal.,2019),ordeterministicneuralnetworkstofitbasisfunctionsforBayesian
linearregression(MaandHernández-Lobato,2021)orthemeanofageneralizedsparseGPwith
Wasserstein-2 metric (Wild et al., 2022). Our work combines linearized BNNs with generalized
variational inference, but we use the regularized KL divergence (Quang, 2019), which naturally
generalizestheconventionalKLdivergenceandallowsforinformativeGPpriors.
Approximatingfunction-spacemeasureswithweight-spacepriorsinBNNs Flam-Shepherd
etal.(2017);Tranetal.(2022)minimizeadivergencebetweentheBNN’spriorpredictiveandaGP
beforeperforminginferenceonweights,whileWuetal.(2023)directlyincorporatethebridging
divergenceinsidetheinferenceobjective. Alternatively,Pearceetal.(2020)deriveBNNarchitectures
mirroringGPs,andMatsubaraetal.(2022)usetheRidgelettransformtodesignweight-spacespriors
approximatingaGPinfunctionspace. Anotherlineofworkconsidersweight-spacepriorswhich
regularize in function space by comparing the model’s predictions to those of a reference model
(Nalisnicketal.,2020)andusinganempiricalprior(Rudneretal.,2023).
6 Discussion
We proposed a simple inference method with a well-defined variational objective function for
BayesianneuralnetworkswithGaussianprocess(GP)priorsinfunction-space. AsstandardVIwith
functions-spacepriorssuffersfromaninfiniteKLdivergenceproblem, weproposetofollowthe
generalizedVIframework. Specifically,wesubstitutetheconventionalKLdivergenceintheELBO
bytheregularizedKLdivergence,whichisalwaysfinite,andwhichcanbeestimatedconsistently
withinthelinearizedBNNapproximation. Wedemonstratedthatourmethodallowstoincorporate
interpretablestructuralpropertiesviaaGPprior,accuratelyapproximatesthetrueGPposterioron
syntheticandsmallreal-worlddatasets,andprovidescompetitiveuncertaintyestimatesforregression,
classificationandout-of-distributiondetectioncomparedtoBNNbaselineswithbothfunction-space
andweight-spacepriors.
Futureworkshouldinvestigatetheuseofmoreexpressivevariationaldistributions,suchasGaussian
withlow-rankplusdiagonalcovarianceproposedbyTomczaketal.(2020),whichiscompatiblewith
ourproposedmethod.
Acknowledgments
ThisworkwasfundedbytheDeutscheForschungsgemeinschaft(DFG,GermanResearchFoundation)
underGermany’sExcellenceStrategy–EXCnumber2064/1–Projectnumber390727645.Additional
supportwasprovidedbytheGermanFederalMinistryofEducationandResearch(BMBF):Tübingen
AI Center, FKZ: 01IS18039A. Robert Bamler acknowledges funding by the German Research
Foundation(DFG)forproject448588364oftheEmmyNoetherProgramme. Theauthorsextend
theirgratitudetotheInternationalMaxPlanckResearchSchoolforIntelligentSystems(IMPRS-IS)
forsupportingTristanCinquin. Finally,wethankVincentFortuinandMarvinPförtnerforuseful
discussionsandfeedback.
References
Abdullah,A.,Hassan,M.,andMustafa,Y.(2022). AreviewonBayesiandeeplearninginhealthcare:
Applicationsandchallenges. IEEEAccess,10:1–1. 1
Belkin,M.(2018). Approximationbeatsconcentration? anapproximationviewoninferencewith
smoothradialkernels. 23
10Berlinghieri, R., Trippe, B. L., Burt, D. R., Giordano, R., Srinivasan, K., Özgökmen, T., Xia, J.,
andBroderick,T.(2023). Gaussianprocessesatthehelm(holtz): Amorefluidmodelforocean
currents. 8
Bertin-Mahieux,T.,Ellis,D.P.,Whitman,B.,andLamere,P.(2011). Themillionsongdataset. In
Proceedingsofthe12thInternationalConferenceonMusicInformationRetrieval(ISMIR2011).
18
Bew, D., Harvey, C. R., Ledford, A., Radnor, S., and Sinclair, A. (2019). Modeling analysts’
recommendationsviaBayesianmachinelearning. TheJournalofFinancialDataScience,1(1):75–
98. 1
Biewald,L.(2020).Experimenttrackingwithweightsandbiases.Softwareavailablefromwandb.com.
17
Blundell,C.,Cornebise,J.,Kavukcuoglu,K.,andWierstra,D.(2015). Weightuncertaintyinneural
networks. 1,6,16
Bradbury,J.,Frostig,R.,Hawkins,P.,Johnson,M.J.,Leary,C.,Maclaurin,D.,Necula,G.,Paszke,
A.,VanderPlas,J.,Wanderman-Milne,S.,andZhang,Q.(2018). JAX:composabletransformations
ofPython+NumPyprograms. 19
Burt,D.R.,Ober,S.W.,Garriga-Alonso,A.,andvanderWilk,M.(2020). Understandingvariational
inferenceinfunction-space. arXivpreprintarXiv:2011.09421. 1,2,3,4,14
Chen, H., Zheng, L., Kontar, R. A., and Raskutti, G. (2021). Gaussian process inference using
mini-batchstochasticgradientdescent: Convergenceguaranteesandempiricalbenefits. 16,17,18,
23
Cinquin,T.,Immer,A.,Horn,M.,andFortuin,V.(2021). Pathologiesinpriorsandinferencefor
bayesiantransformers. InI(Still)Can’tBelieveIt’sNotBetter! NeurIPS2021Workshop. 1
Clanuwat, T., Bober-Irizar, M., Kitamoto, A., Lamb, A., Kazuaki, Y., and Ha, D. (2018). Deep
learningforclassicaljapaneseliterature. 17
Dua,D.andGraff,C.(2017). Ucimachinelearningrepository. 8,16
Flam-Shepherd,D.,Requeima,J.,andDuvenaud,D.(2017). Mappinggaussianprocesspriorsto
bayesianneuralnetworks. 10
Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L.,
Corenflos,A.,Fatras,K.,Fournier,N.,Gautheron,L.,Gayraud,N.T.,Janati,H.,Rakotomamonjy,
A.,Redko,I.,Rolet,A.,Schutz,A.,Seguy,V.,Sutherland,D.J.,Tavenard,R.,Tong,A.,andVayer,
T.(2021). Pot: Pythonoptimaltransport. JournalofMachineLearningResearch,22(78):1–8. 18
Hennigan,T.,Cai,T.,Norman,T.,Martens,L.,andBabuschkin,I.(2020). Haiku: SonnetforJAX.
19
Hensman,J.,Fusi,N.,andLawrence,N.D.(2013). Gaussianprocessesforbigdata. 7,17
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference.
JournalofMachineLearningResearch,14(40):1303–1347. 1
Immer,A.,Korzepa,M.,andBauer,M.(2021). Improvingpredictionsofbayesianneuralnetsvia
locallinearization. 6,10,16
Kerrigan,G.,Ley,J.,andSmyth,P.(2023). Diffusiongenerativemodelsininfinitedimensions. 4
Khan, M.E., Immer, A., Abedi, E., andKorzepa, M.(2020). Approximateinferenceturnsdeep
networksintogaussianprocesses. 10
Kingma,D.P.andBa,J.(2017). Adam: Amethodforstochasticoptimization. 17,18
Knoblauch, J., Jewson, J., and Damoulas, T. (2019). Generalized variational inference: Three
argumentsforderivingnewposteriors. arXivpreprintarXiv:1904.02063. 1,2,5
11Kompa,B.,Snoek,J.,andBeam,A.(2021). Secondopinionneeded: communicatinguncertaintyin
medicalmachinelearning. npjDigitalMedicine,4. 1
LeCun,Y.,Cortes,C.,andBurges,C.(2010). Mnisthandwrittendigitdatabase. ATTLabs[Online].
Available: http://yann.lecun.com/exdb/mnist,2. 9,17,23
Lilly,J.M.andPérez-Brunius,P.(2021). GulfDrifters: Aconsolidatedsurfacedrifterdatasetforthe
GulfofMexico. 8,16
Ma,C.andHernández-Lobato,J.M.(2021). Functionalvariationalinferencebasedonstochastic
processgenerators. InRanzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,andVaughan,J.W.,
editors, AdvancesinNeuralInformationProcessingSystems, volume34, pages21795–21807.
CurranAssociates,Inc. 2,6,10
Ma,C.,Li,Y.,andHernández-Lobato,J.M.(2019). Variationalimplicitprocesses. 6,10
Maddox,W.,Tang,S.,Moreno,P.,GordonWilson,A.,andDamianou,A.(2021). Fastadaptation
withlinearizedneuralnetworks. InBanerjee,A.andFukumizu,K.,editors,ProceedingsofThe
24thInternationalConferenceonArtificialIntelligenceandStatistics,volume130ofProceedings
ofMachineLearningResearch,pages2737–2745.PMLR. 3,10
Malinin,A.,Prokhorenkova,L.,andUstimenko,A.(2020). Uncertaintyingradientboostingvia
ensembles. 9,18
Matsubara, T., Oates, C. J., and Briol, F.-X. (2022). The ridgelet prior: A covariance function
approachtopriorspecificationforbayesianneuralnetworks. 10
Milios,D.,Camoriano,R.,Michiardi,P.,Rosasco,L.,andFilippone,M.(2018). Dirichlet-based
gaussianprocessesforlarge-scalecalibratedclassification. InBengio,S.,Wallach,H.,Larochelle,
H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information
ProcessingSystems,volume31.CurranAssociates,Inc. 16,18
Nalisnick,E.,Gordon,J.,andHernández-Lobato,J.M.(2020). Predictivecomplexitypriors. 10
Osawa,K.,Swaroop,S.,Jain,A.,Eschenhagen,R.,Turner,R.E.,Yokota,R.,andKhan,M.E.(2019).
Practicaldeeplearningwithbayesianprinciples. 9,18
Pearce,T.,Tsuchida,R.,Zaki,M.,Brintrup,A.,andNeely,A.(2020). Expressivepriorsinbayesian
neuralnetworks: Kernelcombinationsandperiodicfunctions. InAdams,R.P.andGogate,V.,
editors,ProceedingsofThe35thUncertaintyinArtificialIntelligenceConference,volume115of
ProceedingsofMachineLearningResearch,pages134–144.PMLR. 10
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
M.,Perrot,M.,andDuchesnay,E.(2011). Scikit-learn: MachinelearninginPython. Journalof
MachineLearningResearch,12:2825–2830. 7,16
Pinder, T. and Dodd, D. (2022). Gpjax: A gaussian process framework in jax. Journal of Open
SourceSoftware,7(75):4455. 19
Quang,M.H.(2019). Regularizeddivergencesbetweencovarianceoperatorsandgaussianmeasures
onhilbertspaces. 1,2,5,10,25
Quang,M.H.(2022). Kullback-leiblerandrenyidivergencesinreproducingkernelhilbertspaceand
gaussianprocesssettings. 5,14
Rudner,T.G.J.,BickfordSmith,F.,Feng,Q.,Teh,Y.W.,andGal,Y.(2022a). Continuallearningvia
sequentialfunction-spacevariationalinference. InChaudhuri,K.,Jegelka,S.,Song,L.,Szepesvari,
C.,Niu,G.,andSabato,S.,editors,Proceedingsofthe39thInternationalConferenceonMachine
Learning,volume162ofProceedingsofMachineLearningResearch,pages18871–18887.PMLR.
3,10
Rudner,T.G.J.,Chen,Z.,Teh,Y.W.,andGal,Y.(2022b). TractabeFunction-SpaceVariational
InferenceinBayesianNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems.
2,3,4,6,10,16,17,22,23
12Rudner,T.G.J.,Kapoor,S.,Qiu,S.,andWilson,A.G.(2023). Function-spaceregularizationin
neuralnetworks: Aprobabilisticperspective. InKrause,A.,Brunskill,E.,Cho,K.,Engelhardt,B.,
Sabato,S.,andScarlett,J.,editors,Proceedingsofthe40thInternationalConferenceonMachine
Learning,volume202ofProceedingsofMachineLearningResearch,pages29275–29290.PMLR.
10
Santin, G. and Schaback, R. (2016). Approximation of eigenfunctions in kernel-based spaces.
AdvancesinComputationalMathematics,42(4):973–993. 23
Sensoy,M.,Kaplan,L.,andKandemir,M.(2018). Evidentialdeeplearningtoquantifyclassification
uncertainty. 23
Shalashilin,I.(2024). Gaussianprocessesforvectorfieldsandoceancurrentmodelling. 8,16
Simpson,D.(2022). PriorsfortheparametersinaGaussianprocess. 14
Sun,S.,Zhang,G.,Shi,J.,andGrosse,R.(2019). FUNCTIONALVARIATIONALBAYESIAN
NEURALNETWORKS. InInternationalConferenceonLearningRepresentations. 1,2,3,4,6,
10,16,17
Tomczak,M.,Swaroop,S.,andTurner,R.(2020). Efficientlowrankgaussianvariationalinference
for neural networks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.,
editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages4610–4622.Curran
Associates,Inc. 1,10
Tran,B.-H.,Rossi,S.,Milios,D.,andFilippone,M.(2022). Allyouneedisagoodfunctionalprior
forbayesiandeeplearning. JournalofMachineLearningResearch,23(74):1–56. 1,10
Wild,V.D.,Hu,R.,andSejdinovic,D.(2022). Generalizedvariationalinferenceinfunctionspaces:
Gaussianmeasuresmeetbayesiandeeplearning. 2,4,7,10,16
Williams,C.K.andRasmussen,C.E.(2006). Gaussianprocessesformachinelearning,volume2.
MITpressCambridge,MA. 1,7,16,17,23
Wilson,A.G.,Izmailov,P.,Hoffman,M.D.,Gal,Y.,Li,Y.,Pradier,M.F.,Vikram,S.,Foong,A.,
Lotfi,S.,andFarquhar,S.(2022). Evaluatingapproximateinferenceinbayesiandeeplearning. In
Kiela,D.,Ciccone,M.,andCaputo,B.,editors,ProceedingsoftheNeurIPS2021Competitions
andDemonstrationsTrack, volume176ofProceedingsofMachineLearningResearch, pages
113–124.PMLR. 18
Wong,Y.K.(2023). Machinelearninginportfoliomanagement. PhDthesis,UniversityofSydney. 1
Wu,M.,Xuan,J.,andLu,J.(2023).Indirectfunctionalbayesianneuralnetworks.InFifthSymposium
onAdvancesinApproximateBayesianInference. 10
Xiao,H.,Rasul,K.,andVollgraf,R.(2017). Fashion-mnist: anovelimagedatasetforbenchmarking
machinelearningalgorithms. 9,17,23
13A Appendix
A.1 DivergencesbetweenGaussianmeasures
A.1.1 TheKLdivergenceisinfinite
Inthissection,weshowthattheKullbach-Liebler(KL)divergencebetweentheGaussianmeasures
QF ∼N(m ,C )andPF ∼N(m ,C ),respectivelyinducedbythelinearizedBNNinEq2.7
ϕ Q Q P P
andbyanon-degenerateGaussianprocesssatisfyingconditionsgiveninSection2.2,isinfinite.While
thishasalreadybeenshownbyBurtetal.(2020),theproofiseasierforGaussianmeasures. We
firstneedtheFeldman-HàjektheoremwhichtellsuswhentheKLdivergencebetweentwoGaussian
measuresiswell-defined.
TheoremA.1(Feldman-Hàjek,Quang(2022)Theorem2,Simpson(2022)Theorem7). Consider
twoGaussianmeasuresν = N(m ,C )andν = N(m ,C )onL2(X,ρ). Thenν andν are
1 1 1 2 2 2 1 2
calledequivalentifandonlyifthefollowingholds:
1. m −m ∈Im(C1/2)
1 2 2
2. TheoperatorT suchthatC =C1/2(I−T)C1/2isHilbert-Schmidt,thatisT hasacountable
setofeigenvaluesλ
thatsa1 tisfyλ2 <1and(cid:80)2
∞ λ2 <∞.
i i i=1 i
otherwiseν andν aresingular. Ifν andν areequivalent,thentheRadon-Nikodymdervative
1 2 1 2
existsandD (ν ∥ν )admitsanexplicitformula. Otherwise,D (ν ∥ν )=∞.
KL 1 2 KL 1 2
LetusnowshowthattheKLdivergencebetweenQF andPF isindeedinfinite.
ϕ
Proposition1. TheGaussianmeasuresQF andPF aremutuallysingularandD (QF||PF)=∞.
ϕ KL ϕ
Proof. TheprooffollowsfromtheFeldman-Hàjektheorem(TheoremA.1). Inourcase,C hasat
Q
mostpnon-zeroeigenvaluesasthecovariancefunctionoftheGPinducedbytheBNNisdegenerate,
while C has a set of (countably) infinite non-zeros eigenvalues (prior is non-degenerate as per
P
assumption). Hence, for the equality in condition (2) to hold, T must have eigenvalue 1 which
violatestherequirementthatT isHilbert-Schmidti.e.thatitseigenvalues{λ }∞ satisfyλ <1and
i i=1 i
(cid:80)∞ λ2 <∞. Therefore,QF andPF aremutuallysingularandD (QF||PF)=∞.
i=1 i ϕ KL ϕ
A.1.2 TheregularizedKLdivergence
Inthissection,weprovidethebounddescribingtheasymptoticconvergenceoftheregularizedKL
divergenceestimator.
TheoremA.2(Convergenceofestimator,Quang(2022)Theorem45). Assumethefollowing:
1. LetTbeaσ−compactmetricspace,thatisT = ∪∞ T ,whereT ⊂ T ⊂ ··· witheachT
i=1 i 1 2 i
beingcompact.
2. ρisanon-degenerateBorelprobabilitymeasureonT,thatisρ(B)>0foreachopensetB ⊂T.
3. K ,K : T × T → R are continuous, symmetric, positive definite kernels and there exists
1 2
κ >0,κ >0suchthat(cid:82) K (x,x)dρ(x)≤κ2 fori=1,2.
1 2 T i i
4. sup K (x,x)≤κ2 fori=1,2.
x∈T i i
5. f ∼GP(µ ,K ),whereµ ∈L2(T,ρ)fori=1,2.
i i i i
6. ∃B >0suchthat∥µ ∥ ≤B fori=1,2.
i i ∞ i
Letx={x(i)}M ,x(1),...,x(M) i∼.i.dρ(x). IfGaussianmeasuresN(m ,C )areinducedbyGPs
i=1 i i
f ∼GP(µ ,K )fori=1,2,thenforany0<δ <1,withprobabilityatleast1−δ,
i i i
|D (N(µ (x),K (x,x)+MγI )∥N(µ (x),K (x,x)+MγI ))
KL 1 1 M 2 2 M
−Dγ (N(m ,C )∥N(m ,C ))|
KL 1 1 2 2
 (cid:115) 
1
2log48 2log48
≤ 2γ(B 1+B 2)2[1+κ2 2/γ]
M
γ +
M
γ 
 (cid:115) 
1
2log12 2log12
+ 2γ2[κ4 1+κ4 2+κ2 1κ2 2(2+κ2 2/γ)]
M
γ +
M
γ  (A.1)
14A.2 AdditionaldetailsontheGFSVIobjectiveestimator
Inthissection, wepresentdetailsontheestimationofthegeneralizedfunction-spacevariational
inference(GFSVI)objective.Letf (·;w)bethelinearizedBNN(Eq2.6)withweightsw ∈Rp,and
L
D ={(x ,y )}N adatasetwithfeaturesx ∈X ⊂Rd andassociatedvaluesy ∈Y. Assuming
i i i=1 i i
a likelihood p(D|w) =
(cid:81)N
p(y |f(x ;w)) and a Gaussian variational distribution on model
i=1 i i
weightsq (w)=N(w|m,S),theGFSVIobjectivefunctionis
ϕ
N
L(ϕ)=(cid:88) E [logp(y |f (x ;w))]−Dγ (cid:0)QF ∥PF(cid:1) (A.2)
qϕ(w) i L i KL ϕ
i=1
whereQF andPF aretheGaussianmeasuresinducedbythelinearizedBNNandaGaussianprocess
ϕ
priorrespectively.
Expected log-likelihood When considering a Gaussian likelihood, we use the closed form ex-
pressionavailableduetotheGaussianvariationalmeasureoverfunctionsinducedbythelinearized
BNN
E (cid:2) logN(cid:0) y |f (x ;w),σ2(cid:1)(cid:3) =−1 log(cid:0) 2πσ2(cid:1) −(y i−f(x i;m))2+J(x i;m)SJ(x i;m)⊤ .
qϕ(w) i L i y 2 y 2σ2
y
(A.3)
WhenconsideringaCategoricallikelihoodwithC differentclasses,weestimatetheexpectedlog-
likelihoodtermusingMonte-Carlointegrationas
K C
1 (cid:88)(cid:88) (cid:104)
E [logCat(y |σ(f (x ;w)))]= I[y =c] fc(x ;w(k))
qϕ(w) i L i K i L i
k=1c=1
(A.4)
(cid:34) C (cid:35)
−log
(cid:88) exp(cid:16) fc′
(x
;w(k))(cid:17) (cid:105)
L i
c′=1
wherew(k) ∼ q (w)fork = 1,...,K,I[·]istheindicatorfunction,σ(·)isthesoftmaxfunction
ϕ
andfc(·;w)isthelogitforclasscobtainedfromf .
L L
Regularized KL divergence We estimate the regularized KL divergence using its consistent
estimator(Eq.2.9)
Dˆγ (cid:0)QF ∥PF(cid:1) = 1 (f(x;m)−µ(x))⊤(K(x,x)+γMI )−1(f(x;m)−µ(x))
KL ϕ 2 M
+ 1 Tr(cid:2) (K(x,x)+γMI )−1(J(x;m)SJ(x;m)⊤+γMI )−I (cid:3)
2 M M M
− 1 logdet(cid:2) (K(x,x)+γMI )−1(J(x;m)SJ(x;m)⊤+γMI )(cid:3) (A.5)
2 M M
with measurement points x = {x(i)}M , x(1),...,x(M) i∼.i.d ρ(x) sampled from a probability
i=1
measureonX.
A.3 Additionaldetailsontheexperimentalsetup
A.3.1 Experimentsonsyntheticdata
Regression Weconsiderthefollowinggenerativemodelforthetoydata
y =sin(2πx )+ϵ with
ϵ∼N(cid:0) 0,σ2(cid:1)
(A.6)
i i n
anddrawx ∼ U([−1,−0.5]∪[0.5,1]). Whennototherwisespecified,weuseσ = 0.1. Onthe
i n
plots,thedatapointsareshownasgraycircles,inferredmeanfunctionsasredlines,their2-standard-
deviations interval around the mean in light green, and functions sampled from the approximate
posteriorasgreenlines. Ingeneral,weconsidertwohidden-layerBNNswith30neuronsperlayer
andhyperbolictangentactivation(Tanh)functions. SpecificallyinFigure11,thesmallBNNhasthe
15samearchitectureasabovewhilethelargeBNNhas100neuronsperlayer. AlltheBNNbaselines
havethesamearchitectureandfully-factorizedGaussianapproximateposterior. Thepriorscaleof
TFSVI(Rudneretal.,2022b)issettoσ =0.2andσ =0.75forMFVI(Blundelletal.,2015)and
p p
Laplace(Immeretal.,2021). FortheGaussianprocessposteriorbaseline,wefitthepriorparameters
bymaximizingthelog-marginallikelihood(WilliamsandRasmussen,2006). Apartfromthecases
wheretheparametersoftheGPpriorusedforGFSVI(ourmethod)andFVI(Sunetal.,2019)are
explicitlystated,weconsideraconstantzero-meanfunctionandfindtheparametersofthecovariance
functionbymaximizingthelog-marginallikelihoodfrommini-batches(Chenetal.,2021). Except
whereotherwisestated,weestimatethefunctionalKLdivergenceswith500measurementpointsand
usetheregularizedKLdivergencewithγ =10−10.
Classification Wesample100datapointsperturbedbyGaussiannoisewithσ = 0.1fromthe
n
twomoonsdata(Pedregosaetal.,2011). Ontheplots,thedatapointsareshownasred(class0)and
blue(class1)dots. Weplotthemeanand2-standard-deviationsoftheprobabilitythatxbelongs
toclass1withrespecttotheposterior(i.e.p(y = 1|w(k),x))whichweestimatefromK = 100
samplesw(k) ∼q (w)fork =1,...,K. Weconsidertwohidden-layerBNNswith100neurons
ϕ
perlayerandhyperbolictangentactivation(Tanh)functions. AlltheBNNbaselineshavethesame
architectureandfully-factorizedGaussianapproximateposterior. ThepriorscaleofMFVI(Blundell
etal.,2015)issettoσ =0.8andσ =1.0forTFSVI(Rudneretal.,2022b)andLaplace(Immer
p p
etal.,2021). FortheGaussianprocessposteriorbaseline,weapproximatetheintractableposterior
using the Laplace approximation and find the prior parameters by maximizing the log-marginal
likelihood(WilliamsandRasmussen,2006). TheGPpriorforGFSVI(ourmethod)andFVI(Sun
etal.,2019)hasaconstantzero-meanfunctionandwefindtheparametersofthecovariancefunction
bymaximizingthelog-marginallikelihoodfrommini-batches(Chenetal.,2021)usingthemethod
totransformclassificationslabelsintoregressiontargetsfromMiliosetal.(2018). Weestimatethe
functionalKLdivergenceswith500measurementpointsandusetheregularizedKLdivergencewith
γ =10−10.
A.3.2 Oceancurrentmodelingexperiment
Weuse20two-dimensionalvelocityvectorsfromtheGulfDriftersdataset(LillyandPérez-Brunius,
2021)andwefollowthesetupbyShalashilin(2024). WeapplytheHelmholtzdecompositiontothe
neuralnetworkf usingthefollowingparameterization
f(·,w)=gradΦ(·,w )+rotΨ(·,w ) (A.7)
1 2
wherew ={w ,w }and,Φ(·,w )andΨ(·,w )aretwo2-layeredfully-connectedneuralnetworks
1 2 1 2
withhyperbolictangentactivationfunctionsand50hiddenunitsperlayer. GFSVIandTFSVIboth
use160fixedcontextpoints. ThepriorscaleofTFSVIissettoσ =0.5. Wefittheneuralnetworks
p
ontheentiredatasetandaveragethescoreswithrespecttofivedifferentrandomseeds.
A.3.3 Regressionexperimentswithtabulardata
Datasetsandpre-processing Weevaluatethepredictiveperformanceofourmodelonregression
datasetsfromtheUCIrepository(DuaandGraff,2017)describedinTable6. Thesedatasetsare
alsoconsideredinSunetal.(2019);Wildetal.(2022)butweincludetwoadditionallargerones
(WaveandDenmark). Weperform5-foldcrossvalidation,leaveoutonefoldfortesting,consider
10%oftheremaining4foldsasvalidationdataandtherestastrainingdata. Wereportmeanand
standard-deviationoftheaverageexpectedlog-likelihoodandaveragemeansquareerroronthetest
fold. Wealsoreportthemeanrankofthemethodsacross all datasetsbyassigningrank1tothe
bestscoringmethodaswellasanymethodwho’serrorbarsoverlapwiththehighestscore’serror
bars,andrecursivelyapplythisproceduretothemethodsnothavingyetbeenassignedarank. The
expectedlog-likelihoodisestimatedbyMonteCarlointegrationwhenitisnotavailableinclosed
form(MFVI,TFSVIandFVI)with100posteriorsamples. Wepreprocessthedatasetbyencoding
categoricalfeaturesasone-hotvectorsandstandardizingthefeaturesandlabels.
Baselinespecification WecompareourGFSVImethodtotwoweight-spaceinferencemethods
(mean-fieldvariationalinference(Blundelletal.,2015)andlinearizedLaplace(Immeretal.,2021))
andtwofunction-spaceinferencemethods(FVI(Sunetal.,2019)andTFSVI(Rudneretal.,2022b)).
WhileFVIusesGPpriors,TFSVIperformsinferenceinfunctionspacebutwiththepushforward
16Table6: UCIregressiondatasetdescription
DATASET BOSTON NAVAL POWER PROTEIN YACHT CONCRETE ENERGY KIN8NM WINE WAVE DENMARK
NUMBERSAMPLES 506 11934 9568 45730 308 1030 768 8192 1599 288000 434874
NUMBERFEATURES 13 16 4 9 6 8 8 8 11 49 2
tofunctionspaceofthevariationaldistributionandpriorontheweights. Wecomputethefunction-
space(regularized)KLdivergenceusingasetof500measurementpointssampledfromauniform
distributionforGFSVIandTFSVI,and50pointsdrawnfromauniformdistributionalongwith450
samplesfromthetrainingbatchforFVIasspecifiedinSunetal.(2019). AlltheBNNbaselines
havethesamearchitectureandfully-factorizedGaussianapproximateposterior. Wealsoprovide
resultswithaGP(WilliamsandRasmussen,2006)whenthesizeofthedatasetallowsit,andasparse
GP(Hensmanetal.,2013). AswerestrictourcomparisontoBNNs,wedonotconsidertheGPand
sparseGPasbaselinesbutratherasgold-standards. AllmodelshaveaGaussianheteroskedastic
noisemodelwithalearnedscaleparameter. AlltheBNNsarefitusingtheAdamoptimizer(Kingma
andBa,2017)usingamini-batchsizeof2000samples. Wealsoperformearlystoppingwhenthe
validationlossstopsdecreasing.
Modelselection Hyper-parameteroptimizationisconductedusingtheBayesianoptimizationtool
providedbyWandb(Biewald,2020).BNNparametersareselectedtomaximizetheaveragevalidation
expectedlog-likelihoodacrossthe5cross-validationfolds. Weoptimizeoverpriorparameters(kernel
and prior scale), learning-rate and activation function. We select priors for GFSVI, FVI, sparse
GPandGPamongtheRBF,Matern-1/2,Matern-3/2,Matern-5/2,LinearandRationalQuadratic
covariancefunctions. TheGPpriorparametersusedwithGFSVIandFVIareselectedbymaximizing
thelog-marginallikelihoodfrombatchesasproposedbyChenetal.(2021)anddoneinSunetal.
(2019). Hyper-parametersforGPsandsparseGPs(kernelparametersandlearning-rate)areselected
tomaximizethemeanlog-marginallikelihoodofthevalidationdataacrossthe5cross-validation
folds.
A.3.4 Classificationexperimentswithimagedata
Datasets and pre-processing We further evaluate the predictive performance of our model on
classificationtaskswiththeMNIST(LeCunetal.,2010)andFashionMNIST(Xiaoetal.,2017)
imagedatasets. Wefitthemodelsonarandomsubsetof90%oftheprovidedtrainingsplit,consider
theremaining10%asvalidationdataandevaluateontheprovidedtestsplit. Werepeatthisprocedure
5 times with different random seeds and report the mean and standard-deviation of the average
expectedlog-likelihood,accuracyandexpectedcalibrationerror(ECE)ofthemeanofthepredictive
distributiononthetestset. Theexpectedlog-likelihoodisestimatedbyMonteCarlointegrationwith
100posteriorsampleswhenitisnotavailableinclosedform(MFVI,TFSVIandFVI).Weestimate
the mean of the predictive distribution to compute the accuracy and the ECE with 100 posterior
samples. Wepreprocessthedatasetbystandardizingtheimages.
Baselinespecification WecompareourGFSVImethodtothesamebaselinesasfortheregression
experiments (see A.3.3). All the BNN baselines have the same architecture and fully-factorized
Gaussianapproximateposterior. Morespecifically, weconsideraCNNwiththreeconvolutional
layers(withoutputchannels16,32and64)beforetwofullyconnectedlayers(withoutputsize128
and10). Theconvolutionallayersuse3×3shapedkernels. Eachpairofconvolutionallayersis
interleavedwithamax-poolinglayer. Weconsiderthreedifferentmeasurementpointdistributionsρ
toestimatethe(regularized)KLdivergenceinGFSVI,FVIandTFSVI:RANDOM,RANDOMPIXEL
and KMNIST. The RANDOM measurementpointdistributionissampledfrombydrawing50%of
thesamplesfromthetrainingdatabatchand50%ofthesamplesfromauniformdistributionover
[p ,p ]H×W×C,whereH,W andC arerespectivelytheheight,widthandnumberofchannels
min max
oftheimages,andp =v −0.5×∆andp =v +0.5×∆where∆=v −v
min min max max max min
isthedifferencebetweentheminimal(v )andmaximal(v )pixelvaluesofthedataset. The
min max
RANDOMPIXELmeasurementpointdistributionistakenfromRudneretal.(2022b)andissampled
frombyrandomlychoosingeachpixelvalueamongtheonesavailablefromthetrainingdatabatchat
thesamepositioninthe28×28pixelgrid. Finally,theKMNISTmeasurementpointdistributionis
alsotakenfromRudneretal.(2022b)andisdrawnfrombyrandomlysamplingdatapointsfromthe
Kuzushiji-MNIST(KMNIST)dataset(Clanuwatetal.,2018). TheKMNISTdatasetisacollection
17of70’000gray-scaleimagesofsize28×28whichwepreprocessbystandardizingtheimages. We
sample10measurementpointswhenusingRANDOM,25measurementpointswhenusingRANDOM
PIXELand20whenusingKMNIST. AlltheBNNsaretrainedusingtheAdamoptimizer(Kingmaand
Ba,2017)usingamini-batchsizeof100. Wealsoperformearlystoppingwhenthevalidationloss
stopsdecreasing.
Model selection Hyper-parameter optimization is conducted just like for the regression tasks
(see A.3.3). The Gaussian process prior parameters used with GFSVI and FVI are selected by
maximizing the log-marginal likelihood from batches (Chen et al., 2021) using the method to
transformclassificationslabelsintoregressiontargetsfromMiliosetal.(2018). Weoptimizethe
samehyper-parametersasfortheregressionexperimentswiththeexceptionoftheadditionalα
ϵ
parameterintroducedbyMiliosetal.(2018)forthefunction-spaceVImethodswithGPpriors(FVI
andGFSVI).
A.3.5 OODdetection
Tabular data with a Gaussian likelihood Following the setup from Malinin et al. (2020) we
takeepistemicuncertaintytobethevarianceofthemeanpredictionwithrespecttosamplesfrom
the posterior. We consider the test data to be in-distribution (ID) data and a subset of the song
dataset(Bertin-Mahieuxetal.,2011)ofequallengthandwithanequalnumberoffeaturesasout-
of-distribution(OOD)data. Weusethesamepreprocessingasforregressionaswellasthesame
baselineswiththesamehyper-parameters(seeAppendixA.3.3). Wefirstfitamodel,thenevaluate
theextendbywhichtheepistemicuncertaintyunderthemodelispredictiveoftheIDandOODdata
usingasinglethresholdobtainedbyadepth-1decisiontreefittominimizetheclassificationloss.
WereportthemeanandstandarderroroftheaccuracyofthethresholdtoclassifyOODfromID
databasedonepistemicuncertaintyacrossthe5foldsofcross-validation. Wealsoprovideresults
obtainedusingaGPandsparseGPasgoldstandard.
ImagedatawithaCategoricallikelihood FollowingthesetupbyOsawaetal.(2019),wetake
theepistemicuncertaintytobetheentropyofthemeanofthepredictivedistributionwithrespectto
samplesfromtheposterior. WeevaluatemodelstrainedonMNISTusingMNIST’stestsplitasID
dataandasubsetofthetrainingsetofFashionMNISTasOODdata. Likewise,weevaluatemodels
trainedonFashionMNISTusingFashionMNIST’stestsplitasIDdataandasubsetofthetraining
setofMNISTasOODdata. Weusethesamepreprocessingasforclassification,aswellasthesame
baselineswiththesamehyper-parameters(seeAppendixA.3.4). Wefirstfitamodel,thenevaluate
the extend by which the epistemic uncertainty under the model is predictive of the ID and OOD
datausingasinglethresholdobtainedbyadepth-1decisiontreefittominimizetheclassification
loss. WeestimatemeanofthepredictivedistributionbyMonte-Carlointegrationusing100posterior
samples. WereportthemeanandstandarderroroftheaccuracyofthethresholdtoclassifyOOD
fromIDdatabasedonepistemicuncertaintyforthe5modelstrainedondifferentrandomseeds(see
AppendixA.3.4).
A.3.6 Variationalmeasureevaluation
Weevaluateourinferencemethodbycomparingthesamplesdrawnfromtheexactposteriorover
functionswiththeapproximateposteriorobtainedwithourmethod(GFSVI).Wefollowthesetupby
Wilsonetal.(2022)andwecomputetheaverageWasserstein-2metricbetween1000samplesdrawn
fromaGPposteriorwithaRBFkernelevaluatedatthetestpoints,andsamplesfromtheapproximate
posteriorofGFSVI,sparseGPandFVIevaluatedatthesamepointsandwiththesameprior. We
considertheBoston,Concrete,Energy,WineandYachtdatasetsforwhichtheexactGPposteriorcan
becomputedandusethesamepreprocessingasforregression(seeAppendixA.3.3). Wereportthe
meanandstandarderroroftheaverageWasserstein-2metricacrossthe5foldsofcross-validation.
TheWasserstein-2metriciscomputedusingthePythonOptimalTransportlibrary(Flamaryetal.,
2021).
Baselinespecification FVIandGFSVIhavethesametwohiddenlayerneuralnetworkarchitecture
with100neuronseachandhyperbolictangentactivation. Thesemodelsarefitwiththesamelearning
rateandsetof500measurementpointsjointlysampledfromauniformdistributionoverthefeature-
18σn=0.1 σn=1
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2
Figure4: Ourmethod(GFSVI)effectivelyregularizesfunctionsgeneratedbytheBayesianneural
network(BNN)bothinsettingswherethegenerativeprocessisverynoisy(σ =1)ornot(σ =0.1).
n n
spaceandmini-batchoftrainingsamples. Weuseγ =10−15fortheregularizedKLdivergence. We
furtherconsiderasparseGPwith100inducingpoints.
A.3.7 Software
WeusetheJAX(Bradburyetal.,2018)andDM-Haiku(Henniganetal.,2020)Pythonlibrariesto
implementourBayesianneuralnetworks. MFVI,linearizedLaplaceandTFSVIwereimplemented
basedontheinformationinthepapers,andcodeforFVIwasadaptedtotheJAXlibraryfromthe
implementationprovidedbytheauthors. WefurtherusetheGPJAXPythonlibraryforexperiments
involvingGaussianprocesses(PinderandDodd,2022).
A.3.8 Hardware
AllmodelswerefitusingasingleNVIDIARTX2080TiGPUwith11GBofmemory.
A.4 Additionalexperimentalresults
Inthis section, we presentadditionalfiguresfor ourqualitativeuncertaintyevaluation aswellas
furtherexperimentalresultsonregression,out-of-distributiondetectionandrobustnessunderinput
distributionshifttasks. Wealsoprovideplotsillustratingtheeigenvaluedecayofdifferentkernels,
andfiguresshowingtheinfluenceofγ intheregularizedKLdivergence.
A.4.1 Qualitativeuncertaintyevaluation
Regression Wefurtherfindthatourmethod(GFSVI)providesstrongregularizationwhenthedata
generative process is noisy (see Figure 4) and is more robust than FVI to situations where ones
computationalbudgetconstrainsthenumberofmeasurementpointsM tobesmall(Figure8). In
contrasttoFVI,GFSVIaccuratelyapproximatestheexactGPposteriorunderrough(Matern-1/2)
GPpriorseffectivelyincorporatingpriorknowledgedefinedbytheGPpriortotheinferenceprocess
(seeFigure5). Likewise,GFSVIadaptstothevariabilityofthefunctionsspecifiedbythekernel(see
Figure6). WealsofindthatGFSVIrequiresalargernumberofmeasurementpointstocapturethe
behaviorofarougherprior(seeFigure7).
Exact posterior (GP) GFSVI (ours) FVI
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
TFSVI MFVI Laplace
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure5: Ourmethod(GFSVI)withaMatern-1/2Gaussianprocess(GP)prioraccuratelyapprox-
imatestheexactGPposteriorunlikethefunction-spacepriorbaseline(FVI).Weight-spaceprior
baselinesdonotprovideastraight-forwardmechanismtoincorporatepriorassumptionsregarding
the functions generated by BNNs and underestimate the epistemic uncertainty (MFVI, Laplace).
ThelowerrowisidenticaltotheoneinFigure2inthemaintextandisreproducedheretomake
comparisoneasier.
19
roirp
ecaps-noitcnuF
roirp
ecaps-thgieWλ=0.15 λ=0.35 λ=0.5
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure6: Ourmethod(GFSVI)allowstoincorporatepriorbeliefsintermsoffunctionvariability
usingthecharacteristiclength-scaleparameteroftheGaussianprocess(GP)prior. GFSVIwasfit
usingaGPpriorwithRBFcovariancefunction.
RBF kernel Matern-1/2 kernel
M=10 M=100 M=10 M=100
2 2
0 0
-2 -2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure7: Ourmethod(GFSVI)capturesthesmoothbehaviorofaGaussianprocess(GP)priorwith
RBFcovariancefunctionevenifthenumberofmeasurementpointsissmall(M=10). However,in
thatsettingGFSVIfailstoreproducetherougheffectofaGPpriorwithaMatern-1/2covariance
function,andrequiresalargeramountofmeasurementpointstodoso(M=100).
Exact posterior (GP) GFSVI (ours) - M=10 FVI - M=10 GFSVI (ours) - M=100 FVI - M=100
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure8: Ourmethod(GFSVI)alreadyprovidesareasonableapproximationtotheexactposterior
withsmallnumbersofmeasurementpoints(M=10)whilefunction-spacebaselineFVIrequiresmany
more(M=100).
Classification We find thatGFSVI better captures the beliefs induced by the smooth RBF and
roughMatern-1/2GaussianprocesspriorscomparedtoFVI(seeFigures9and10).Moreover,GFVSI
bothaccuratelyfitsthetrainingdataandshowsgreateruncertaintyoutsideofitssupportrelative
toBNNsbaselineswithweight-spaceandfunction-spacepriors. Unlikeforthetoydataregression
experimentswheretheGPposteriorwasthegroundtruth,theLaplace(approximate)GPposteriorin
Figures9and10onlyrepresentsapossibleapproximationtothenowin-tractableposterior(dueto
thesoftmaxinverselinkfunction). ThustheGPshouldnotbeconsideredasthegroundtruthnoras
theoptimalapproximationintheclassificationsetting,butisneverthelessusefultogiveaideaofthe
levelofuncertaintyaBNNwithaGPpriorshouldprovideoutsideofthesupportofthedata.
GFSVI (ours) FVI TFSVI MFVI Laplace GP (not BNN)
4 1.0
0.9 2 00 .. 78
0.6
0 0.5
0.4
-2 00 .. 23
0.1
-4 0.0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
4
0.9
2 00 .. 78
0.6
0 0.5
0.4
-2 00 .. 23
0.1
-4 0.0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
Figure9: Ourmethod(GFSVI)withaRBFGaussianprocess(GP)prioraccuratelycapturesthe
smoothdecisionboundaryinducedbythepriorandshowshighuncertaintyoutsideofthedatasupport.
Weight-spacebaselinesdonotprovideastraight-forwardmechanismtoincorporatepriorassumptions
regardingthefunctionsgeneratedbyBNNsandunderestimatetheepistemicuncertainty(TFSVI,
Laplace)orunderfitthedata(MFVI).
20
[y=1|,x]
2[y=1|,x]GFSVI (ours) FVI TFSVI MFVI Laplace GP (not BNN)
4 1.0
0.9 2 00 .. 78
0.6
0 0.5
0.4
-2 00 .. 23
0.1
-4 0.0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
4
0.9
2 00 .. 78
0.6
0 0.5
0.4
-2 00 .. 23
0.1
-4 0.0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
Figure10: Ourmethod(GFSVI)withaMatern-1/2Gaussianprocess(GP)prioraccuratelycaptures
theroughdecisionboundaryunlikethefunction-spacebaseline(FVI).Weight-spacebaselinesdo
notprovideastraight-forwardmechanismtoincorporatepriorassumptionsregardingthefunctions
generatedbyBNNsandunderestimatetheepistemicuncertainty(TFSVI,Laplace)orunderfitthe
data(MFVI).
Exact posterior (GP) Small BNN & Tanh Small BNN & ReLU Large BNN & ReLU
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure11: OurmethodrequiresthattheBayesianneuralnetwork(BNN)andGaussianprocess(GP)
priorsharesimilarinductivebiasestoprovideanaccurateapproximationtotheexactposterior.
Inductivebiases Figure11comparesGFSVItotheexactposterioracrosstwodifferentpriorsand
threemodelarchitectures(detailsinA.3.1). WefindthattheBNN’sabilitytoincorporatethebeliefs
introducedbytheGPpriordependsonitssizeandactivationfunction. Whenusingpiece-wiselinear
activations(ReLU),smallmodelsarepronetounderfittingforsmoothpriors(RBF),andtocollapsing
uncertaintyforroughpriors(Matern-1/2). Bycontrast,whenusingsmoothactivations(Tanh),smaller
modelssuffice,andtheyarecompatiblewithmoststandardGPpriors(theresultsshowninFigure11
extendtoRBF,Maternfamily,andRationalQuadraticinourexperiments). Wealsoanalyzedhow
thenumberM ofmeasurementpointsaffectsperformance. Figures7and14showthatcapturingthe
propertiesofroughGPpriorsandestimatingDγ withthesepriorsrequireslargerM.
KL
A.4.2 Regressionontabulardata
Table7: Testmeansquareerror(MSE)ofevaluatedmethodsonregressiondatasets. Wefindthat
GFSVI(ours)alsoperformscompetitivelyintermsofMSEcomparedtobaselinesandobtainsthe
bestmeanrank,matchingbesttheperformingmethodsonnearlyalldatasets.
DATASET FUNCTION-SPACEPRIORS WEIGHT-SPACEPRIORS GAUSSIANPROCESSES(GOLDSTANDARDS)
GFSVI(OURS) FVI TFSVI MFVI VIP LAPLACE GWI SPARSEGP GP
BOSTON 0.123±0.021 0.136±0.022 0.995±0.092 0.532±0.072 0.201±0.056 0.203±0.047 0.273±0.069 0.122±0.014 0.115±0.020
CONCRETE 0.114±0.008 0.116±0.004 0.389±0.015 0.698±0.046 0.109±0.008 0.116±0.007 0.145±0.017 0.399±0.020 0.116±0.007
ENERGY 0.003±0.000 0.003±0.000 0.003±0.000 0.152±0.024 0.043±0.036 0.002±0.000 0.003±0.001 0.087±0.005 0.087±0.004
KIN8NM 0.071±0.001 0.075±0.003 0.073±0.001 0.290±0.111 0.068±0.002 0.083±0.001 0.071±0.001 0.088±0.002 (infeasible)
NAVAL 0.000±0.000 0.001±0.001 0.000±0.000 0.007±0.003 0.002±0.000 0.000±0.000 0.197±0.174 0.000±0.000 (infeasible)
POWER 0.052±0.001 0.054±0.002 0.054±0.001 0.058±0.002 0.054±0.002 0.054±0.002 0.052±0.001 0.071±0.001 (infeasible)
PROTEIN 0.459±0.005 0.466±0.004 0.429±0.004 0.537±0.008 0.421±0.005 0.446±0.006 0.425±0.003 0.408±0.002 (infeasible)
WINE 0.652±0.022 0.663±0.009 1.297±0.093 0.655±0.023 0.627±0.013 0.637±0.031 0.682±0.048 0.607±0.033 0.585±0.032
YACHT 0.003±0.001 0.004±0.001 0.221±0.037 0.682±0.140 0.004±0.001 0.002±0.001 0.008±0.003 0.399±0.064 0.355±0.030
WAVE 0.000±0.000 0.000±0.000 0.000±0.000 0.000±0.000 0.000±0.000 0.000±0.000 0.001±0.001 0.000±0.000 (infeasible)
DENMARK 0.155±0.004 0.287±0.003 0.163±0.004 0.225±0.003 0.189±0.008 0.194±0.003 0.197±0.004 0.260±0.001 (infeasible)
MEANRANK 1.364 2.000 2.182 3.182 1.636 1.727 - - -
21
lenrek
FBR
lenrek
2/1-nretaM
[y=1|,x]
2[y=1|,x]Wepresentadditionalregressionresultsreportingthemeansquareerror(MSE)ofevaluatedmethods
acrosstheconsideredbaselines,seeTable7. WefindthatGFSVIalsoperformscompetitivelyin
termsofMSEcomparedtobaselinesandobtainsthebestmeanrank,matchingbesttheperforming
methodsonnearlyalldatasets. Inparticular, wefindthatusingGPpriorsinthelinearizedBNN
setupwithGFSVIyieldsimprovementsovertheweight-spacepriorsusedinTFSVIandthatGFSVI
performsslightlybetterthanFVI.Function-spaceVImethods(TFSVI,GFSVI,FVI)significantly
improvesoverweight-spaceVImostlyperformingsimilarlytothelinearizedLaplaceapproximation.
FurtherimprovementoverbaselinesareobtainedwhenconsideringGPpriorswithGFSVIandFVI.
Finally,GFSVIcomparesfavorablytotheGPandsparseGP.
A.4.3 Out-of-distributiondetectionwithimagedata
Wehereshowanadditionalplotfromourout-of-distributiondetectionexperimentwithimagedata
(detailsinA.3.5). Figure12showsthe(normalized)histogramsoftheentropyofthemeanprediction
producedbyeachmodelonthein-distribution(blue)andout-of-distribution(red)datasetsconsidered
in our OOD detection experiment. Methods which estimate the (regularized) KL-divergence in
function-space(GFSVI,FVIandTFSVI)usetheKMNISTmeasurementdistribution. Wefindthat
the entropy produced by GFSVI on in-distribution data highly peaks around 0 while the entropy
producedfromout-of-distributiondatastronglyconcentratesarounditsmaximumln(10). GFSVI
bestpartitionsIDandOODdatabasedonpredictiveentropyimprovingoverthefunction-spaceprior
(FVI)andweight-spaceprior(TFSVI,MFVI,Laplace)BNNbaselines(seeTable2).
GFSVI (ours) FVI GWI TFSVI MFVI VIP Laplace
2
5.0 4 5.0 4 5.0 I OD OD
1 2
2.5 2 2.5 2 2.5
0.0 0 0 0.0 0 0.0 0
0 2 0 2 0 2 0 2 0 2 0 2 0 2
4
4 4 4 4
4
2 2 2 2 2 2 1
0 0 0 0 0 0 0
0 2 0 2 0 2 0 2 0 2 0 2 0 2
Predictive entropy Predictive entropy Predictive entropy Predictive entropy Predictive entropy Predictive entropy Predictive entropy
Figure 12: Histograms of the entropy of the mean predictive distribution produced by evaluated
methodsintheout-of-distributiondetectionwithimagedataexperiment. GFSVI(ours)bestpartitions
in-distributionandout-of-distributiondatabasedontheentropyofitsmeanpredictivedistribution.
A.4.4 Influenceofmeasurementpointdistributionforimagedata.
Wepresentadditionalresultsevaluatingtheinfluenceofthemeasurementpointdistributionρonthe
theperformanceoffunction-spaceinferencemethodswhenusinghigh-dimensionalimagedata. The
measurementpointdistributionaredescribedinAppendixA.3.4. JustlikeinRudneretal.(2022b),
wefindthatthechoiceofmeasurementpointdistributionmayhighlyinfluencetheOODdetection
accuracy. Whiletheexpectedlog-likelihood,accuracyandexpectedcalibrationerror(ECE)ofa
modelgenerallyremainscomparableacrossmeasurementpointdistributions, theOODaccuracy
of GFSVI is greatly improved by using samples from KMNIST to evaluate the (regularized) KL
divergence. ThemeasurementpointdistributiondetermineswheretheBNNisregularizedandthus
shouldbecarefullyselectedespeciallyforhighdimensionaldata.
Table8: Influenceofthemeasurementpointdistributionρonexpectedlog-likelihood(log-like.),
accuracy(acc.),expectedcalibrationerror(ECE)andout-of-distributiondetectionaccuracy(OOD
acc.). ρdetermineswheretheBNNwillberegularizedandstronglyinfluencestheout-of-distribution
performanceoftheBNN.
METRIC GFSVI FVI TFSVI
RANDOM RANDOMPIXEL KMNIST RANDOM RANDOMPIXEL KMNIST RANDOM RANDOMPIXEL KMNIST
LOG-LIKE.(↑) -0.033±0.000 -0.034±0.000 -0.041±0.000 -0.145±0.005 -0.038±0.000 -0.238±0.006 -0.047±0.003 -0.032±0.001 -0.041±0.001
ACC.(↑) 0.992±0.000 0.989±0.000 0.991±0.000 0.976±0.001 0.988±0.000 0.943±0.001 0.989±0.000 0.989±0.000 0.989±0.000
ECE(↓) 0.002±0.000 0.004±0.000 0.006±0.000 0.064±0.001 0.003±0.000 0.073±0.003 0.007±0.000 0.003±0.000 0.006±0.000
OODACC.(↑) 0.921±0.008 0.868±0.010 0.980±0.004 0.894±0.010 0.863±0.003 0.891±0.006 0.887±0.011 0.861±0.008 0.893±0.005
LOG-LIKE.(↑) -0.260±0.003 -0.258±0.002 -0.294±0.006 -0.300±0.002 -0.293±0.003 -0.311±0.005 -0.261±0.001 -0.258±0.001 -0.261±0.002
ACC.(↑) 0.910±0.001 0.908±0.001 0.909±0.001 0.910±0.002 0.900±0.001 0.906±0.002 0.909±0.001 0.908±0.001 0.907±0.001
ECE(↓) 0.020±0.003 0.022±0.001 0.042±0.002 0.027±0.005 0.018±0.002 0.024±0.002 0.022±0.002 0.018±0.001 0.021±0.002
OODACC.(↑) 0.853±0.005 0.867±0.005 0.997±0.001 0.925±0.005 0.842±0.006 0.975±0.002 0.802±0.006 0.800±0.007 0.779±0.010
22
TSINM
TSINM
noihsaF
ATAD
TSINM
TSINMFA.4.5 Inputdistributionshiftwithrotatedimagedata
We here provide an experiment evaluating our method’s (GFSVI) robustness in detecting input
distributionshift. Weexpectthepredictiveuncertaintyofawell-calibratedBayesianmodeltobelow
forin-distributiondataandtograduallyincreaseastheinputdistributionshiftsfurtherawayfromthe
trainingdatadistribution. Totestthisproperty,wefollowthesetupbySensoyetal.(2018);Rudner
etal.(2022b)andassumeliketherelatedworkthatincreasingtherotationangleofimagesgradually
increasesthelevelofinput"distributionshift". Wereportthemeanandstandard-deviationofthe
averagemeanpredictiveentropyofmodelsfitonMNIST(LeCunetal.,2010)andFashionMNIST
(Xiaoetal.,2017)forincreasinglylargeanglesofrotationoftheirrespectivetestdatapartition. We
findthatGFSVIisconfident(lowpredictiveentropy)forimageswithsmallrotationangles,andthat
itspredictiveentropyincreaseswiththeangle. GFSVIthereforeexhibitstheexpectedbehaviorofa
well-calibratedBayesianmodel. WenotethatFVI,LaplaceandMFVItendtobeunder-confident
(highpredictiveentropy)forsmallrotationangles,whichmightbeasymptomofunderfittingfurther
supportedbytheresultsinTable2. Also,withtheexceptionofTFSVI,thepredictiveentropyof
baselinesacrossdifferentrotationanglesisgenerallyhigherthantheoneproducedbyGFSVI.
MNIST Fashion MNIST
2.0
GFSVI TFSVI Laplace VIP GWI
FVI MFVI
1.5
1.0
0.5
0.0
0 20 40 60 80 100 120 140 160 180 0 20 40 60 80 100 120 140 160 180
Angle of rotation Angle of rotation
Figure 13: Average predictive entropy of models trained on MNIST and Fashion MNIST and
evaluatedfordifferentrotationanglesoftheirrespectivetestdatapartitions. Weseethatourmethod
(GFSVI)exhibitsthebehaviorofawell-calibratedBayesianmodel.
A.4.6 Additionalplotsforkerneleigenvaluedecay
Figure14showsaplotdemonstratingthedecayrateoftheeigenvaluesofRBFandMatern-1/2kernels
evaluatedatpointssampleduniformlyoverX .Therateofdecayofcovarianceoperator’seigenvalues
givesimportantinformationaboutthesmoothnessofstationarykernels(WilliamsandRasmussen,
2006)andthatincreasedsmoothnessofthekernelleadstofasterdecayofeigenvaluesSantinand
Schaback (2016). For instance, RBF covariance operator eigenvalues decay at near exponential
rateindependentoftheunderlyingmeasure(Belkin,2018)andMaternkernelseigenvaluesdecay
polynomialy(Chenetal.,2021). Wefindthatthekernelevaluatedatpointssampledfromauniform
distributionoverX sharethissamebehavior(seeFigure14).
RBF kernel Matern-1/2 kernel RBF kernel Matern-1/2 kernel
11 00 −1 22 λ λ λ λ= = = =0 0 1 1. . 00 11 11 00 −1 22 λ λ λ λ= = = =1 1 1 10 0 00
00
10−4 10−4 λ=10000
10−6 10−6
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Eigenvalue index Eigenvalue index Eigenvalue index Eigenvalue index
(a) D=1 (b) D=100
Figure 14: Mean eigenvalues of the Gram matrix obtained for different kernels and for varying
length-scales over 10 draws from a uniform distribution on [−2,2]D. The mean eigenvalues are
arrangedinincreasingorder. TheeigenvaluesoftheGrammatrixassociatedwiththesmoothRBF
kerneldecaysmuchfasterthanthoseoftheMatern-1/2. Furthermore,theeigenvaluesdecayata
slowerrateinhighdimensions(D=100).
23
yportne
evitciderP
eulavnegiE eulavnegiEA.4.7 Additionalplotsforchoosingγ inDγ
KL
Theγ parametercontrolsthemagnitudeoftheregularizedKLdivergence(seeFigure17)andadjusts
therelativeweightoftheregularizedKLdivergenceandexpectedlog-likelihoodterminthetraining
objective (see Figure 15). Furthermore, γ also acts as "jitter" preventing numerical errors. We
recommendchoosingγ largeenoughtoavoidnumericalerrorswhileremainingsmallenoughto
providestrongregularization.
γ=10−10 γ=10−5 γ=10−2
2
0
-2
-2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2
Figure15:TheγparameteroftheregularizedKLdivergencecontrolsthemagnitudeoftheregularizer
intheobjectiveandshouldbesmallenoughtoprovidestrongregularization.
GP prior - RBF GP prior - Matern-1/2
p 104 γ=10−15
100 B B PrN N ioN N
r
T Ra en Lh U 11 00 23 γ γ= =1 10 0− −1 50
50 Max rank 101 γ=10−3
0 1
0 50 100 p 200 250 300 0 100 200 300 400 500 0 100 200 300 400 500
Number of measurement points Number of measurement points Number of measurement points
Figure 16: The BNN’s covariance adapta- Figure 17: γ explicitly controls the magnitude of
tion to the prior’s covariance rank depends theregularizedKL-divergenceDγ . Rougherpriors
KL
on its activation function. BNNs fit with a (Matern-1/2)requiremoremeasurementpointstoac-
RBFprior(full)showlowerrankthanwitha curatelyestimateDγ thansmoothpriors(RBF).
KL
Matern-1/2(dotted).
24
knaR Lγ KDNeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: Thewell-definitenessofourvariationalobjectivefollowsfromthefactthat
theregularizedKLdivergenceisfiniteforanypairofGaussianmeasuresQuang(2019).
Tables1,2,4and5aswellasFigures2to10supportourclaimabouttheperformanceof
ourmethod.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
madeinthepaper.
• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NAanswertothisquestionwillnotbeperceivedwellbythereviewers.
• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
muchtheresultscanbeexpectedtogeneralizetoothersettings.
• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: Theinductivebiasesparagraphinsection4.1discusseslimitations.
Guidelines:
• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
• Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors
shouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe
implicationswouldbe.
• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
onlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften
dependonimplicitassumptions,whichshouldbearticulated.
• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe
usedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle
technicaljargon.
• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms
andhowtheyscalewithdatasetsize.
• If applicable, the authors should discuss possible limitations of their approach to
addressproblemsofprivacyandfairness.
• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-
tantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers
willbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
25Answer: [Yes]
Justification: OuronlytheoreticalresultisprovedinAppendixandisthedirectapplication
oftheFeldman-Hàjektheorem. Wespecifyallassumptions.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif
theyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort
proofsketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: TheexperimentalsetupisrigorouslydescribedinAppendixA.3.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
well by the reviewers: Making the paper reproducible is important, regardless of
whetherthecodeanddataareprovidedornot.
• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct
thedataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.
Inthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin
someway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers
tohavesomepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
26Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification: Thepseudocode, experimentdescriptionsinappendixaswellasthecode
providesalltherelevantinformationtoreproduceexperiments.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: TheexperimentalsetupisrigorouslydescribedinAppendixA.3.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: Wereportmeanandstandard-errorofourmethodacrossfivecross-validation
folds(regression)orrandompartitionsofthedata(classification).
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confi-
denceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport
themainclaimsofthepaper.
• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
27• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,
calltoalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror
ofthemean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: WespecifycomputationalresourcesinAppendixA.3.7.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
orcloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute
thantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat
didn’tmakeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Tothebestofourknowledge,wehavefullyconformedwiththecodeofethics.
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-
erationduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [No]
Justification: ThispaperpresentsworkwhosegoalistoadvancethefieldofBayesiandeep
learning. Therearemanypotentialsocietalconsequencesofourwork,nonewhichwefeel
mustbespecificallyhighlightedhere.
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
28• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from(intentionalorunintentional)misuseofthetechnology.
• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Themethodsdevelopedinthepaperhavenoriskformisuse.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest
faitheffort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: Datasetsusedfortheexperimentsarecitedinthemaintextandtheirlicense
andtermsofuseareproperlyrespected.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
29• If assets are released, the license, copyright information, and terms of use in the
packageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
licenseofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
thederivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
theasset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [NA]
Justification: Thepaperdoesnotreleasenewassets.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: thepaperdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: that the paper does not involve crowdsourcing nor research with human
subjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
30• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
31