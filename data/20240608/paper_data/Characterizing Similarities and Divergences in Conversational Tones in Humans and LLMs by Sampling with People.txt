Characterizing Similarities and Divergences in Conversational Tones in
Humans and LLMs by Sampling with People
Dun-MingHuang1,2,PolvanRijn2,IliaSucholutsky3,RajaMarjieh4,andNoriJacoby2
1DepartmentofElectricalEngineeringandComputerSciences,UniversityofCalifornia,Berkeley
2ComputationalAuditoryPerceptionGroup,MaxPlanckInstituteforEmpiricalAesthetics
3DepartmentofComputerScience,PrincetonUniversity
4DepartmentofPsychology,PrincetonUniversity
Abstract van Schendel and Cuijpers, 2015). Inability to
dosoresultsinthecontentofconversationbeing
Conversational tones – the manners and atti-
“lost-in-translation”betweenspeakersofdifferent
tudes in which speakers communicate – are
languagesandcultures(Yusifova,2018). Notably,
essentialtoeffectivecommunication. Amidst
whiletraditionallythestudyofconversationaltones
the increasing popularization of Large Lan-
guage Models (LLMs) over recent years, it involved only humans, the increasing prevalence
becomes necessary to characterize the diver- of Large Language Models (LLMs) in everyday
gencesintheirconversationaltonesrelativeto decision-making, especially their conversational
humans. However, existing investigations of (“Chat”) variants, renders the study of conversa-
conversationalmodalitiesrelyonpre-existing
tional tones in LLMs necessary for human align-
taxonomiesortextcorpora,whichsufferfrom
ment (Ouyang et al., 2022; Rudolph et al., 2023;
experimenterbiasandmaynotberepresenta-
Sucholutsky et al., 2023b; Marjieh et al., 2023a).
tiveofreal-worlddistributionsforthestudies’
Developingtoolsforeffectivelycharacterizingcon-
psycholinguisticdomains. Inspiredbymethods
fromcognitivescience,weproposeaniterative versational tones in humans and LLMs is hence
methodforsimultaneouslyelicitingconversa- essential for the development of human-centered
tionaltonesandsentences,whereparticipants AI,human-computerinteractionresearch,andcog-
alternatebetweentwotasks:(1)oneparticipant nitivesciences(Figure1A).
identifiesthetoneofagivensentenceand(2)a
Background: ConversationResearch. Incon-
differentparticipantgeneratesasentencebased
versationresearch,someliteratureengagesexplic-
onthattone. Werun100iterationsofthispro-
itly with the composition and usage of conversa-
cesswithhumanparticipantsandGPT-4,then
obtainadatasetofsentencesandfrequentcon- tionalattitudesandlinguisticmarkers(Fintel,2006;
versationaltones. Inanadditionalexperiment, Yeomans et al., 2022; Jakobson, 1960). A wider
humansandGPT-4annotatedallsentenceswith arrayofliteratureusesconversationalanalysistoin-
alltones. Withdatafrom1,339humanpartic- vestigateotherdynamicsthatcanaffectthecontent
ipants,33,370humanjudgments,and29,900
ofconversation,suchasturn-taking(vanSchendel
GPT-4queries,weshowhowourapproachcan
andCuijpers,2015)andface-saving(Ting-Toomey
be used to create an interpretable geometric
et al., 1991; Oetzel et al., 2001), as well as other
representation of relations between conversa-
cross-culturalsemanticdifferencesthatcanleadto
tionaltonesinhumansandGPT-4. Thiswork
demonstrates how combining ideas from ma- differentbehaviorwithinthesameconversational
chine learning and cognitive science can ad- tone, such as refusal (Chang, 2009), shame (Kol-
dress challenges in human-computer interac- larethetal.,2018),andpoliteness(AlemánCarreón
tions.
etal.,2021;Ogiermann,2009). Ontheotherhand,
theintroductionofLargeLanguageModels,espe-
1 Introduction
cially its chatbot applications, brings attention to
Conversational tones, the manner and attitude thealignmentofconversationalbehaviorinLLMs
in which a speaker communicates, is essential with that of human ideals (Ouyang et al., 2022;
to human communication (Yeomans et al., 2022; Rafailov et al., 2023), which can potentially con-
Saiewitz and Kida, 2018). Effective communica- tributetothealignmentofLLMs’perceptionand
tion relies on people’s understanding of conver- production of conversational tones with those of
sational patterns and tones, and their ability to humans. Beingabletoeffectivelyfine-tuneLLMs
promptlyreacttothem(KreuzandRoberts,1993; alsocreatesnewopportunitiestogeneratetextstyle-
4202
nuJ
6
]LC.sc[
1v87240.6042:viXraA Problem statement B Sampling paradigm C Quality-of-fit rating D Shared space
What are similarities and
Humans
divergences in conversational How sarcastic is the following
tones in humans and LLMs? T S T … S sentence:
“Oh sweetie, I suppose it’s quite
C tho au t l Td o i mt b ae t ep o as ll s ti hb ele polite? S T S … T r 5a 0n sd eo nm tely n cp eic sk a likn e a yc oc uo .m ”plishment for someone
cookies from the jar? GPT-4 E Benchmark
T S T … S
How strong is the provided
I am pretty certain, S T S … T pick top terms conversational tone?
Tom ate all the that co-occur Tone: sarcastic
Sentence: <sentence to rate>
cookies from the jar. accusing? Ssentence Ttone Respond with only a number.
Figure1: Summaryofourapproach. A:Problemstatement. B:TheSamplingwithPeopleparadigmthataims
tocollectarepresentativesampleofconversationaltonesandsentences. C:Aquality-of-fitratingprocedurethat
allowsustoobtainvectorrepresentationsofconversationaltoneswithrespecttotheirusagecontext. D:Ageometric
representationofthesharedembeddingspaceacrosseliciteddomains(human,GPT).E:Asanapplicationofour
obtaineddata,webenchmarkaselectionofpopularunsupervisedcross-domainalignmentmethods.
transfer corpora that specify sentences with spe- 2017),facialexpressions(Goodfellowetal.,2013).
cificconversationaltones,suchaspoliteness(Wang Thisstrategyispronetoresearcherbias,potentially
etal.,2022)andformality(deRiveroetal.,2023; skewingthefindingsawayfromanaccuraterepre-
Wangetal.,2019). sentationoflabelsastheyoccurintherealworld
Challenge: biasedaprioritaxonomy. However, andwithinagivenculture(Kollarethetal.,2018;
the domain of conversational tones is, like emo- Henrichetal.,2010).
tion (Schiller et al., 2023; Lindquist et al., 2022; Challenge: biasedstimulusset. Anotherchal-
Athanasiadou and Tabakowska, 1998) and color lenge that almost all studies faced when study-
(BerlinandKay,1969),aninstanceofgroundedse- ing grounded semantics is that they may use a
mantics(Tannen,1984;Semnani-AzadandAdair, constrained set of stimuli to be annotated (e.g.
2013). Whileallparticipantsobservethesamestim- emotion(CowenandKeltner,2017;Cowenetal.,
ulus (e.g., an emotional recording, a solid color, 2019, 2020; Cowen and Keltner, 2020; Cowen
or in our case a sentence), people may use dif- et al., 2018); object recognition and similari-
ferent words or labels to describe it (in our case ties (Gifford et al., 2022; Hebart et al., 2019);
conversational tones such as “polite”, “excited”, word-associations(DeDeyneetal.,underreview;
and “grateful”) which makes it difficult to study DeDeyneetal.,2019);musicalperception(Juslin
groundedsemanticsatscaleandespeciallyacross and Sloboda, 2013); facial expression (Khaired-
multiple languages or cultures. One challenge is dinandChen,2021;Linetal.,2021);prosody(El
that studying grounded semantics often involves Ayadietal.,2011;Batlineretal.,2008)). Thisintro-
adoptingapredefinedtaxonomy,typicallysourced ducesresearcherbias,ascuratingthestimulimay
from previous studies and curated by investiga- influencetheelicitedlabels,whichweoutlineus-
tors(e.g.;colors(AdamsandOsgood,1973;Wang ingthefollowingexample. Imagineanexperiment
andWang,2016);facialemotion(Ekman,1992); whereaparticularsemantictermcanbeassociated
musicalemotionperception(JuslinandVästfjäll, withsomeclassofobjects(e.g.,theterm“red”can
2008; Palmer et al., 2013), concepts such as ani- beusedtodescriberedfruits). Iftheobjectclass
malterms(Martietal.,2023);sentimentofnews isnotincludedinthepredeterminedlistofobjects
items(Rozadoetal.,2022)prosody(Sauteretal., (e.g., red fruits are not included in the list of ob-
2010;Bussoetal.,2008). Notably,manymachine jects),thentheelicitedtermswillnotincludethis
learningdatasetsalsosufferfromthesamelimita- association(wewillconcludethat“red”doesnot
tionofusingapredefinedlistofusingapredefined describefruits),anditwillbemissingfromthere-
listofstimulithatcanbeoutdatedorunrepresen- sultingsemanticnetwork. Biasinobjectselection
tative of the correspond modality. Examples of canalsooccurinmoresubtlewayswhereaskewin
such datasets span through realms of: object im- thedistributionofselectedobjectsalsoskewsthe
ages(Dengetal.,2009;Krizhevsky,2009),visual distributionofelicitedterms,potentiallyevenam-
scenes(Zhouetal.,2017),sounds(Gemmekeetal., plifyingtheinitialbias. Furthermore,alargebody
2017), video and its categorizations (Kay et al., ofcross-culturalresearcherssuggeststhatstudies
etalerrocshould not impose a terminology inherited from Rating;Figure1C).Weusethesetoconstructage-
theexperimenterorevenfromonegroupofstudied ometricembeddingthatcanbeusedtoevaluatethe
agents(e.g.,Englishspeakers)onanotheragentor alignmentbetweenhumanandLLMconversational
groupofagents(e.g.,Speakersofanotherlanguage tones(Figure1D).
ordemographics;Blasietal.,2022;Barrett,2020; We show how our approach can be effectively
Henrichetal.,2010). usedtorevealdivergencesintherepresentationof
Additionalchallenges. Finally,whilesomestud- conversationaltonesbetweenhumansandLLMs.
iesadvocatefortheexclusiveuseoflargetextual Moreover, we demonstrate how our new dataset
corporaandtheextractionofsemanticdescriptors andcross-evaluationscanbeusedtobenchmarkun-
via data mining (Thompson et al., 2020), this in- supervisedcross-domainsemanticalignmentmeth-
directapproachraisesconcernsaboutitsabilityto ods used in existing work, and identify which of
accuratelyrepresentthenuancesofconversational theseworkwellforcasesinwhichcross-evaluation
tones as experienced in everyday human interac- isnotpossible(e.g.,inmultilingualscenarios;Fig-
tions. Itisalsodifficulttorigorouslycomparehu- ure 1E). Our method can be generalized to many
mansandLLMsusingsuchcorporabecausethese morepsycholinguisticmodalities(e.g.,sentiment,
same textual corpora are also the basis for LLM color), languages, and cultures beyond those in-
training. Notably, (Thompson et al., 2020) also volved in this paper. We believe it will help ad-
studiestheproblemofaligningsemanticnetworks vancebothhuman-machinealignmentresearchas
ofdifferentindividualsorgroupsinthecontextof wellascross-culturalresearch.
cross-linguisticandcross-culturalcomparisons. It
turnsoutthatthisisakeypartofthemachinelearn- 2 DetailedApproach
ingproblemofautomatictranslation(Zinszeretal.,
2.1 ElicitationviaSamplingwithPeople
2016; Liu et al., 2021). Recent research has fo-
cusedonaligningsemanticsinhumanswithLarge Thecoreofourapproachisthejointelicitationofa
LanguageModels(Sucholutskyetal.,2023b;Atari representativesampleofconversationaltonesand
etal.,2023),withsignificantapplicationstodesign- sentences from both humans and LLMs. Specif-
inghuman-computerinterfaces(Houetal.,2024) ically, we propose an iterative procedure that is
andAIsafety. composed of two steps per iteration. Step one,
Ourapproach. Inlightofthesechallenges,we in which humans and LLMs are presented with
propose a method that enables the characteriza- sentences and are asked to classify their conver-
tionofconversationaltonesandtheirtaxonomies sational tones in an open-ended fashion (Figure
inanytargethumanpopulationaswellasLLMs, 1B).Steptwo,theresultingconversationaltonede-
basedonahuman-in-the-loopSamplingwithPeo- scriptors (adjectives) are then presented to a new
ple(SP)technique(Sanbornetal.,2010;Griffiths group of agents, from whom we ask to produce
andKalish,2005;Harrisonetal.,2020)(Figure1). sentences that reflect those conversational tones.
Specifically,weproposeaniterativeprocedurein This process is then iterated multiple times. For-
whichhumansandLLMsarepresentedwithsen- mally, this process instantiates a Gibbs Sampler
tencesandareaskedtolabeltheirconversational fromthejointdistributionofsentencesandconver-
tones in an open-ended fashion (Figure 1B). The sationaltones,foranytargetpopulationwechoose
resulting conversational-tone terms are then pre- tosamplefrom,beithumansandLLMs(Griffiths
sentedtoanewgroupofagentswhoareaskedto etal.,2024). Therefore,byconstrainingthesetof
producesentencesreflectingthoseconversational humanparticipantstothosefromaspecificcultural
tones. Thisprocessisthenrepeatedmultipletimes. group,weexpecttoobtainarepresentativesample
Withmathematicalformalism,thisprocessinstanti- ofpsycholinguisticcontentsfromthegroupofour
atesaGibbsSamplerfromthejointdistributionof participantpopulation.
sentencesandconversationaltonesinhumansand Here we draw inspiration from human-in-the-
LLMs(Harrisonetal.,2020;Griffithsetal.,2024). loopelicitationprocedures(Griffithsetal.,2024).
Giventheresultingsample,wederiverepresenta- Inthesemethodssuchasserialreproduction(Xu
tive sentences and tone taxonomies of our target andGriffiths,2010;Anglada-Tortetal.,2023;Ja-
population,thenhaveanindependentgroupofhu- coby and McDermott, 2017; Jacoby et al., 2024;
manevaluatorsandLLMsratetheextenttowhich Langlois et al., 2017, 2021), iterated learning
each tone matched each sentence (Quality-of-fit (Xu et al., 2010; Griffiths and Kalish, 2005),MCMCP (Sanborn et al., 2010) and GSP (Harri- onomies. Then,wehaveangroupofhumanevalu-
son et al., 2020; van Rijn et al., 2022b; Van Rijn atorsindependentfrompriorparticipants,aswell
etal.,2021;VanGeertandJacoby,2024;Marjieh asGPTratetheextenttowhicheachtonematched
etal.,2024;vanRijnetal.,2024),aMarkovChain each sentence (Quality-of-fit Rating; Figure 1B).
is constructed by interspersing human decisions We show how our approach can be effectively
withinasamplingchaintocharacterizelatentrep- usedtorevealdivergencesintherepresentationof
resentations in the human mind (e.g., perceptual conversationaltonesbetweenhumansandLLMs.
prior,orsubjectiveutility). Inournovelapproach, Specifically,afterSPsampling,wecollectquality-
SamplingwithPeople(SP),humansarerecruited of-fitratingsofallsentenceswithalltones(Figure
toperformtwotasks(Figure1B):(1)elicitasen- 1C),andcomputesemanticsimilaritymatricesof
tence based on a conversational tone (“S” task), differenttones. Then,fortwotonest ,t ,letR
i j i,j
and (2) annotate a conversational tone of a given denotethecorrelationofthetwotonesacrossthe
sentence (“T” task). Under a probability theory vectorofaverageratingsofallsentences,suchthat
framework,“S”and“T”tasksareessentiallysam- t and t are similar if they have similar ratings
i j
plingoperations,respectivelyfromtheconditional acrossallsentences. Wecancomputesuchmatrix
distribution of a sentence given a conversational Rineitheranintra-domainmanner(onlyusingem-
tonep(S|T),andtheconditionaldistributionofa beddingsfromeitherhumansorGPTbutnotboth),
tonegivenasentencep(T|S). Inpractice,werun or a cross-domain manner (where we exploit the
severalparallelsamplingchains,andineachtrial, sharedlistofsentencestocomputethecorrelation
aparticipantisassignedtoonechainandperforms betweenallconversationaltoneembeddings).
an “S” or “T” task as needed. This means that
2.3 GeometricRepresentationof
each sampling chain alternates between “S” and
ConversationalTones
“T”tasks. Importantly,tosatisfytheformulationof
aGibbs’Sampler,wedesignourparadigmtosat- We use the resulting cross-domain similarity ma-
isfytheMarkovianpropertybyconstrainingeach tricestoobtainageometricrepresentationofboth
participant to see only the output of the previous humanandGPTdatawithinthesamespace(Fig-
iteration(Harrisonetal.,2020;GriffithsandKalish, ure1D).Wecomputethefullcorrelationofall80
2005). tone embeddings (40 conversational tones, each
UsingSP,weelicitedalargedatabaseoftones onewithanembeddingfromhumandataandan-
andsentencesfromhumansandLLMs(separately otherfromGPTdata),anduseMultidimensional
for each). We elicited 40 tones and 80 sentences Scaling(MDS;CarrollandArabie,1998;Anowar
with955participants,90chains,and100iterations et al., 2021) to project them into a shared low-
each. Thelistof40conversationaltonesweinvesti- dimensionalembeddingspace. Thespacethusrep-
gateistheunionofthetop24conversationaltones resentsnotonlytherelationbetweentoneswithin
from each instance of SP experiments. We then humans but also the way they relate to GPT, es-
took40randomsentencesfromeachofthehumans pecially as the proximity of tones in the MDS
and GPT, forming a balanced corpus of humans Euclidean space corresponds to the proximity of
andGPTintermsofsentencesources. Wepreserve tonesintermsoftheirsemanticsimilarity(Shepard,
therepresentativenessofoursamplefromtheinter- 1980). Therefore, tones that appear closer in the
naldistributionofsentencesP(S)bychoosingthe sharedspacearelocatednearerinEuclideanspace.
randomsentencesinauniformsamplingfashion.
2.4 Application: BenchmarkingSemantic
In the design of this study, we use a shared array
AlignmentMethods
ofconversationaltonestoworkwithaconsensus
taxonomy,enablingdirectcomparabilitybetween Todemonstratetheusabilityofouralignmentdata,
theconversationalbehaviorofhumansandGPT-4 we show how it can be used to benchmark se-
whenpromptedunderthesamelanguage1. mantic alignment methods as ground truth when
cross-annotation is unavailable (Figure 1E) and
2.2 AnnotationviaQuality-of-fitRating
only intra-domain correlation matrices are used.
Giventhedistributionsfrompriorsubsection,we Specifically, we benchmark the performances of
havederivedrepresentativesentencesandtonetax- (1)Gromov-WassersteinOptimalTransport(Grave
etal.,2018;Conneauetal.,2017;Kawakitaetal.,
1Fromhereon,allreferencestoGPT-4willbeabbreviated
asGPT. 2023), (2) Bilingual Lexicon Induction (Ruderet al., 2018; Artetxe et al., 2016, 2017, 2018a,b), (Figure 2A) was much more concentrated (en-
and(3)OrthogonalProcrustes(Schönemann,1966; tropyof3.10bitsCI=[3.06, 3.15]viabootstrap-
Beauducel,2018). ping)comparedtothatofhumans(entropyof5.48
bits CI = [5.43, 5.52]). Overall, there was some
3 Method similarity between the histograms (r = 0.39 p =
0.006CI=[0.3,0.454]),butalsosignificantdiffer-
3.1 Participants
ences. Specifically,theprominentconversational
HumanParticipants. WerecruitedN=1,339partic- tones had different weights: in humans the three
ipantsforthefourhumanexperimentsinthisstudy mostprominentconversationaltoneswere“grate-
(AppendixTable1providesthenumberofpartici- ful”(mean4.03%CI=[3.45%,4.62%]),“excited”
pantsandresponsesforeachexperiment). Partici- (2.79%CI=[2.35%,3.28%]),then“happy”(2.64%
pantswererecruitedfromtherecruitingplatform CI = [2.18%, 3.1%]) whereas in GPT they were
Prolific and provided informed consent under an “excited”(mean24.66%CI=[23.41%,25.91%]),
approvedprotocol(seeEthicssectionforfurtherin- “grateful” (10.79% CI = [9.89%, 11.68%]), then
formation). Humanexperimentsareimplemented “concerned” (6.79% CI = [6.05%, 7.52%]). The
usingPsynet(Harrisonetal.,2020),aPythonpack- resultshighlightthattheelicitationprocessresults
ageforimplementingcomplexonlinepsychology inadifferentdistributionoftermsusedtodescribe
experiments. conversationaltones.
GPTexperimentsWeusedtheJune13th,2023
4.2 AnnotationviaQuality-of-fitRating
release of GPT-4. Overall, we ran 29,900 GPT
To create a detailed semantic embedding based
queriesacrossallexperiments.
on the given sentences and tones, we conducted
3.2 GeneralProcedure a further experiment involving both humans and
GPT.Inthisstudy,participantsevaluatedalltarget
Each human experiment began with detailed in-
sentencesusingapredefinedlistof40prominent
structions and practice trials. We emulated each
conversationaltonesthatwereextractedfromthe
ofthehumanexperimentswithGPT-4agentsthat
terms elicited using the SP procedure above, en-
usedtheverysameprocedure,usingtheexperiment
suringadirectcomparisonbyemployingthesame
interfaceinstructionsasLLMprompts. Thehuman
tones for both human and GPT assessments. As
and GPT-4 experiments were otherwise identical
a result, every sentence is relabelled with all 40
in their design. Appendix A contains the full in-
tonesregardlessoftheoriginaltoneitwasassigned
structions/promptsusedinourexperiments. Allthe
in the SRE step. The sentence set was evenly di-
dataoftheexperiments,codeforreproduciblehu-
vided,withonehalforiginatingfromhumansand
manandGPTexperiments,andanalysisscriptcan
theotherhalfgeneratedbyGPT.
be found here: https://github.com/jacobyn/
We recruited an additional 275 human partici-
SamplingTonesACL.
pants for these annotations. Participants rate the
strengthofconversationaltonesinasentenceusing
4 Results
aLikertscale,resultingin16,000ratingjudgments.
Likertscaleswereusedbecausethedegreetowhich
4.1 Elicitation(SamplingwithPeople)
asentencerepresentsatonemayvaryandcannotbe
Figure 2A shows the histogram of the 24 most
capturedbycategoricallabels(Sucholutskyetal.,
popular conversational tones from 4,500 human
2023a). GPTdataunderwentthesameratingpro-
and4,500GPTannotatedsentences. Werecruited
cesswithGPTagentsasraters. Wethenanalyzed
955 human participants for SP experiments. The
thecorrelationbetweentonesbyexaminingtherat-
collected histograms were reliable: the split-half
ing vectors across the 80 sentences elicited from
reliabilitycomputedviabootstrapping2 washigh
Section4.1. Additionally,wefindthatthemajor-
for both humans and GPT conversational tones
ityofsentencesarelabeledtopossessatleastone
(humans: r = 0.91, CI = [0.87, 0.93]; GPT: r
specificconversationaltone: allhuman-originated
= 0.87 CI = [0.73, 0.94]3). The GPT histogram
sentenceshadatleastoneconversationaltonewith
anaverageratingexceeding2.89,andexceedinga
2Throughoutthepaper,bootstrappingoccurswith5000
rating of 2 for 95% of GPT-originated sentences.
repetitions,unlessspecifiedotherwise.
3ThroughoutthepaperallCIarereportedas95%. ThiswouldsuggestthatveryfewsentenceswereA Selected terms in humans and GPT B Correlation matrix on quality-of-fit ratings in humans and GPT
conge crx eac rt ni ete efud dl de rs e rc cf elr ue li irp ec io vt ti i euv v de e s
frustrated reassuring
e aan pfa dc f pp eno e rco ro ei u sr etl s cr pr ho i li oa t iit e ag eaaa ng r p ve tlt aai g ine ept vttiid dg eecc e y e aan pfc f peo a rc p eu d f et r l cr hm i x eipoa e j iac a o arng i nor pi s y ttai i d iu en n e fp vt ul dd dg g ee yy l
angry grateful
ds c rsa eeu o tadc h ffr nr lepmu ac ef nurr n cai ii sr so s ks ti i ie en t fu vv uid dg ec es l sy c dm os n es u npt o a swhh r cas n ppoa oett x er hn c a rr ii rrne oskk l aig ee ee tf u tu ii dd dd c ec sl
urgent urgent
dis dr ae e up
sa
ra s
n
ep ka p wncfs
c
ser ln o ns r ei
o
e
ipe i jx gu i oap orn rni
t
nrr tsyso yt it ii ad ci eaee en e fu v uial ddddddg neys
ll
disa a fdp uci
r
sp sn ar e
u
o
no e
a
kip d
n
rf
s
cnl s erreoo i
n ct
ig eai
f
pn
r
tg i og
u
an
aa
rnn tsn ts
y
se
s
ittt ga
a
cie a
e
eeee tt v nr
i i
ai d
d
dd
d
dd ne cc yt
l
indignant shocked proud sympathetic 0% 5% 10% 15% 20% 25% C 0 0. .8 6 Cross-domain matrix de ss kc seri ap ep rt nict ri c ca ev a o fsle lat ui e pr pc ca rot olg ii ov une g dg etic C -1ross-correlation 1 C 0orrelation 1 0.4 0.2 0.0
D Similarity judgements of coversational tones in humans and GPT
de a sa in sypd cf ar adc mf p fe e dos p ue cir n
r
s
spo nea r ass a re
e
uu npc
o
not e ap o kr a ie g pu w sd dh nc fc es h rf e
f
sr ct ca nl s rr esl nl c u reprr op om h ir o ns iu x e ae ci tl p oe igo eat ia fa pti ei j xi nr
r
te gu ic a r ogrp c h ue a o nac aar rr ng
aa
rnnn tig n i i ti srorr ri p n nk tv s s ys e e o e so kt t y t sl it
t
t
tge aaaii i igd
a
ci i ii ue n e e e a
e
e
en n e e e e
e
efp f f t t tuuv v vv n nt t ru uu ii i i
i
aldd d d d d d d d d
d
d d d
dg g g ne e c c e c ee e cyy s ys t tll l
l
de saa in sypd f c ar adc mf p fe e dos p ue cinr
r
sp
so nea r as a sre
e
uu npc
o
not ep aor kae ig psu d dw nfc c hhs ef re
f
sr ct ca nl sr erl s ncl rup err op ohm iir u s nox e ae ci tl p oe
g
io et aia fa ptei ji xi
n
rr te gu iac grr ohp ua oe n cac aar rr ng
aa
rnntng i n iii tsro rrr nnpi te e o es so v
y
skky tt t sl it
t
ttse gaaa ii ii gd
a
cii i iu e ee e eene n en
e
e
eeef tf tfp tuu vv v van ntt ruu u ii ii
i
al d dd dd dd dd
d
d dd
dg gg nc ee c ce e e
e
cdsyys yt tl l l
l
Figure 2: Results of Sampling with People and Quality-of-fit Rating paradigms and comparison to similarity
judgmentsparadigm. A:SelectionofmostpopularconversationaltonesfromeachofhumanandGPTinstances,
and their frequencies in respective samples (red for humans, blue for GPT). Error bars represent one standard
deviationviabootstrapping. B:Correlationmatricesofconversationaltonequality-of-fitratingembeddingswithin
humans(ontheleft)andwithinGPT(ontheright). C:Cross-domain(Cross-correlation)matrixofhumanrating
embeddingsandGPTratingembeddingsforconversationaltones,andabarplotshowingthecorrelationbetween
humanratingsandGPTratingembeddingsforeachconversationaltoneword. Errorbarsrepresentonestandard
deviationviabootstrapping. D:Similarityjudgment-derivedsimilaritymatricesofconversationaltonefromhumans
(ontheleft)andGPT(ontheright). SeeenlargedversionofthisfigureintheAppendix(Figure17).
perceivedascompletelyneutralwithrespecttoall are similar and different in the structure of tone-
40tones. similaritywenowcomputethecross-domainma-
trix,namelyforeachtonewecorrelatedthevector
Figure2Bpresentsthetone-correlationmatrices
ofsentenceratingsinhumansandinGPT(Figure
for humans (left) and GPT (right). The matrices
2C).Asexpected,giventhatbothhumansandGPT
showreliabletone-similarityforbothhumansand
showseparatelythepatternofvalanceclusters,the
GPT(humans: r =0.94CI[0.91,0.96]; GPTrat-
cross-domainmatrixalsoshowedthispattern. In-
ings, r = 0.86 CI [0.78, 0.91]). It is visually ap-
terestingly,themaindiagonalofthismatrixshows
parent that there are two clusters of tones which
thatthealignmentbetweentoneratingsinhumans
roughlyrelatedtothevalenceoftonesandthatthis
andGPTvariessignificantly. Sometonessuchas
structure is preserved in humans and GPT as we
“joyful”,“pleased”,and“happy”werehighlycorre-
foundahighcorrelationbetweentheuppertriangle
lated(r=0.89,0.89,0.87CI=[0.88,0.91],[0.88,
ofthetwomatrices(r=0.81CI[0.76,0.85]).
0.91], [0.86, 0.90], respectively) suggesting that
To better understand how humans and GPT
lufyoj desaelp yppah de d n etd ea nid aded tr ra e tee eye ryr tipo cet t rari s ngr cn sc tou ie onnnx rwrd r u caae f i
ee e s d g g g du tv v n n ne u ai i o v i i i ot t r g r np c i e u i rr a m ope ii u slr r iel tc c s duf cre as ao ere e c frd fn ae
d ee tt an d tc ni no e ids ogy ap su etl i nld n tu apo v c gefn ta ei en s ix ge ds i foon lri e fi nr d nu aca rf i
y d d y l e l l d duu ul p ve e e ed f f fip y e kt s k stn i o a na t icc a re ai ae j o px h ci rr hle h rgepf ut sr sppa
evisnefed
ciglatson
ce iv ti etdg a hde lin ste ucs a ui k feir pr eoi c rpm m tp ido arr pd uhu a y rgcaa ssss
c s d e t c d dd nui t ie e aet ta eoe ei n sn r r gh i g gr r ex re ot o ip un sa c w lsa eop n e rpm o d acys
lufknaht
detnioppasid
gn dir eu ns gs ia see rr
e t yn rv gais n nn g ae idfe nd i detartsurf deyonna detatirri desufnoc niatrecnu citsacras lacitpeks
e e s d g g g d e y d d y l e l l d d c c d s d e t c d d d e t y d d d d n c lu u u an nu l p u rv v t v i i t i v iin n ne u e e e e e e a e e e e eed g t t t cf f f aga a e ai i i io op y e k e e sv i i i t s k s n i n s t t y t sot t t s in tlr g r rn r g n ni n a a tap c o a ani e a t i h i g o uc rc a r gu i rr e nr r x pea m o a g ar r t eie op te ci ai e ji o t fu p o nx h os ip iu tec nlr i s er r s nai ci rc we srl rhlec c s t h nd dlr pu g s f rf e ap of e ko ap o nc nu ur ee itas s aa no e pr sn r spe c um os rdr pe e d ic a ff acd r pf yn sa a s ie d
e e s d g g g d e y d d y l e l l d d c c d s d e t c d d d e t y d d d d n c luu u an nu l p u rv v t v i i t i v iin n ne u e e e e e e e a e e e e ed g t t t cf f f aga a e ai i i io op y e k e e sv i i i t s k s n i n s t t y t sot t t s in tlr g r rn r g n ni n a a tap c o a ani e a t i h i g o uc rc a r gu i rr e nr r x pea m o a g ar t erie op te i cai e ji o t fu p o nx h os ip iu tec nlr i s er r s nai ci rc we srl rhlec c s t h nd dlr pu g s f rf e ap of e ko ap o nc nu ur ee itas s aa no e pr sn r spe c um os rdr pe e d ic a ff acd r pf yn sa a s ie d
evitpircsed
evitpircsed
evitcelfer
evitcelfer
suoiruc
suoiruc
deveiler
deveiler
gnirussaer
gnirussaer
gnigaruocne
gnigaruocne
gnirimda
gnirimda
duorp
duorp
etanoitceffa
etanoitceffa
yldneirf
yldneirf
deticxe
deticxe
desaelp
desaelp
yppah
yppah
lufyoj
lufyoj
evitaicerppa
evitaicerppa
lufetarg
lufetarg
lufknaht
lufknaht
dekcohs
dekcohs
desirprus
desirprus
ciglatson
ciglatson
citehtapmys
citehtapmys
denrecnoc
denrecnoc
suoixna
suoixna
deirrow
deirrow
etarepsed
etarepsed
tnegru
tnegru
citegolopa
citegolopa
dengiser
dengiser
das
das
detnioppasid
detnioppasid
evisnefed
evisnefed
tnangidni
tnangidni
yrgna
yrgna
detartsurf
detartsurf
deyonna
deyonna
detatirri
detatirri
desufnoc
desufnoc
niatrecnu
niatrecnu
citsacras
citsacras
lacitpeks
lacitpeksthese tones have a similar relation to other tones ence are between humans and GPT and compare
in both humans and GPT. However, other tones ourresultswithrespecttopreviousliterature(Yeo-
suchas“proud”,“apologetic”,and“reflective”had mansetal.,2022;Russell,1980),weperformedan
relativelylowalignment(r=0.30,0.46,0.49CI= additionalexperiment. 38participantsratedeach
[0.20,0.39],[0.34,0.52],[0.41,0.54]). tone from 1 to 5 on a Likert scale based on four
Finally, our tone similarity calculations were theoreticaldimensions: valence,emotionalarousal,
somewhatindirectsincewedidnotcomparetone informational,andrelational(overall800ratings).
similarity directly but computed it based on the WealsoconductedasimilarexperimentwithGPT
similaritiesofsentenceratings. Toaddressthis,we (800 ratings). These features are defined in Ap-
recruitedaseparategroupof71participants. Each pendix A.7. To observe how these ratings relate
participantperformed50-60trialsandwasasked totheconversationaltoneembedding’MDSsolu-
toprovidesimilarityratingsbetweentwotoneson tions,weprojectedtheaverageratingovertonesto
aLikertscale. Asimilarprocedurewasappliedfor theMDS(seeAppendixB.3forprojectionmethod
GPT.Theresultingmatrices(2D)indicatethatdi- using linear regression). Each theoretical dimen-
rectsimilarityjudgmentsalsoshowedthevalence sionisrepresentedbyanarrow(direction)inMDS
blockstructure,butthesimilaritymatrixappearsto space, with the length of the arrow representing
benoisier. Nevertheless,thedatawasstillreliable: its relevance (measured by how much the dimen-
split-halfcorrelationsfortheupperdiagonalorthe sionexplainedthevarianceamongthepointsinthe
matrices were humans: r = 0.65 CI [0.63, 0.68], MDS).
and that for GPT’s similarity matrix to be mean The theoretical component of conversational
0.981withCI[0.978,0.987]. Thehighreliability tone that seems to explain most of the variance
ofGPTpartiallyoriginatesfromthemonotonere- intheirMDSsolutionsis“positiveinvalence”(hu-
sponsethatGPTprovidesinsimilarityjudgments mans: meanr=0.71CI=[0.69,0.87];GPT:mean
(unlikehumansGPTagentsprovidesthesamere- r = 0.873 CI = [0.872, 0.878]), where the other
sponseagainandagainforthesameinput). Impor- terms explained significantly less variance (GPT:
tantly,thedirectsimilaritymatriceswerealigned r = 0.45, 0.17, 0.13; humans: r = 0.37, 0.54, 0.3
withthequality-of-fitratingapproach(humans: r for“relational”,“aroused”,“informational”respec-
=0.76CI=[0.73,0.78],GPT:r=0.83CI=[0.81, tively). This is consistent with the idea that the
0.84]). Thisisanindependentvalidationthatour mainaxisofthequadraticshownbyMDSsolutions
approachdoescapturetonesimilaritystructure. It corresponds roughly to the positive/negative va-
alsohighlightsthedifficultyofworkingwithdirect lence(“joyful”,“happy”,“excited”,and“pleased”
similarity(Marjiehetal.,2023b). areinthebottomleft,whereas“concerned”,“wor-
ried”,and“anxious”areinthetopright).
4.3 ConversationalToneRepresentation
FromFigure3A,however,weseethatthehuman
(Multidimensionalscaling)
andGPTarrowmarksforthesameconversational
Inordertounderstandtheorganizationoftonerep- tone may have minor to medium directional dif-
resentations, we applied Multidimensional Scal- ferences, such as that for the feature “positive in
ing (MDS) to the combined within/across corre- valence”and“aroused”(indicatedbyalargecosine
lation matrix, such that each conversational tone similarity;mean=0.88CI=[0.86,0.94]andmean
possessesa2-dimensionalembeddingintheresult- =0.46CI=[0.04,0.53],respectively). Meanwhile,
ingsharedMDSspace(Figure3A). “relational” is consistently aligned (mean = 0.99
We connected identical tones in GPT and hu- CI = [0.86, 0.99]), while “informational” is also
mans with gray lines. Overall, it is visually ap- stronglyaligned(mean=0.83CI=[-0.26,0.89];
parent from Figure 3A that the structure of tones seeAppendixA.7). Thissuggestsadeviationbe-
in humans and GPT is similar, which is consis- tweenthehumanandGPTunderstandingofthese
tent with the analysis of Figure 2. However, we features. Furthermore,itvalidatesYeomansetal.’s
alsofounddifferencesintheproximityoftonesin chosen features for conversational tone composi-
the shared space. The furthest tones away were tion,“informational”and“relational”,asanaspect
“proud”, “sad”, and “thankful”, while the closest of well-aligned conversational understanding be-
were “uncertain”, “desperate”, and “conncerned” tween humans and GPT (Yeomans et al., 2022).
(Figure3B). Theseresultsalsoallowustofurtherinterpretthe
Tobetterinterpretwhatthedimensionsofdiffer- MDSofFigure3A,forexample,“proud”islocatedA MDS of cross correlation B Conversational tone Euclidian distance
irritated ia a n dn n d en g i fro g eyy sn ne ka sd en iv pt e td ici asa lpf pru os intr t ra e et d se id gned c o nfu se dde us rp ge er na tte unc co en rc taae w n ixr no in or ue r sid ed 0112233 ....... 5050505 proud sad thankfu gl ratefu rl eass eu nri cn og ur aa dgi mn irg in cg urious appre rci ela iti ev ve e dd efen as fi fv ee ctio sn ka et pe ti ec xal cite id ndign aa pnt olog ae nti nc oye jd oyful angry confus se ad rcas sti yc mpat hh ae pti pc y resig in rre itd ate frd ustrat ne od stal ag ni xc ious disap fp rioi en nt de ld y please ud rgent shocke sd urpris de ed scrip wti orv rie e rd eflec cti ov ne cer dn ee sd per uat ne certain
sus rar
pca
rst ii sc
ed shocke ad roa uro seu pd os sitie ved i pn oc f sou itr r ivi imo enu fa os t r ri mo eln aaa tt ii ol o nn r aea lll atioa np aol
logeti rec fls ea
cd
sy tmp iat vheti ec dis aa df dcu piC rs ps n ea ru eon oeak i pd snr
f0
snc s lr e ur
pe.
oin
ocN0
t gi ae ifp r rnt g eiou ga n gaa nnrt s rnsyst esie ttt eag aac i eaeee ee tt v nn tri ia ia ddd ddddn ecec y ttlrest neighbor of d aic df d scu iss ra r n pi ea eeo unak r uo n da or sp sfse r nc rn si n ei lc pt in gpgtp e of gaa o ngrua eenr o gt a d nt yv srs sti rne a ic t eya ee ai nt e ee vd tna i ti dd ttc dn d ie etl e cr dsational tone emb s duarsddiafaicus nr r ee k aa pnno r rn eieu d s si ge dr gntnc sofsd a ic ia ep egg poe rf t lpa ot nu nyrtd nnr eye ai s p gsst tc eae rda tt ie oi e aa e v di ndin c dn ti tl d et n i ecg tes d 1
e x cite djo hy afu pl
pyrelieved
pllu efk n aa hg t sra ete fu dld u o rp a p p re
c
ia
tiv
e
g n irim d ae n c o fu rr ia eg i nn g
dly
re
a s s u rin g
affectionatedescripti nv ose talgic
es a a ny p
dc
f rm cfp
e
eo
s en roas
a
sr
eun
p ct p
roa
eg d
uw
h cfe ch
s
ferc
ta
rln
s lc rr hp mo
s
uix rea p ei
lo e
ot aa et
i
ijx
i ua ccr reah pn o caa
rrr
n gt
inii iopn rrr
ik
ovsso
ee k y tt tl t a
iiii
g d ii iu
nnneeee eee
pft ff
uu
vv vtu uuii lddd
ddddd
ggg eeecc eyy
ss
l l l es a a
dc
rny p fs rn eas
eo
ft p
rma ecgw
cfepu eho ah edh r p
snn
j l for uxr r i co e
so
asp
l
lo a cmea
rc
p e
ix
ec u ret sa ot
ynr
pa c rnt
ie ci
ir ca i
rr
oi ie
uo
vof s utkit k r pp d
ai r
i u tsl
ee
h ui ef a e
ru ngn
f d ie n g ty lue l
i
vu
dd
e y sdt d
ins ae
i d g vil i ec d l nt v
gtd
ei eec g
rcrreapafephagtssnscaw
djo rh eeehuy
undfxpon
ao r rl
eio
y e fe flaa a om cor
i
rcpx n
e mps slf
er
a enp sn it
uiui
oct
cor cc
pre
to
vp s
s
cda kr
ii
de elk rt
ue re
uyfi ee
uu
a tl f iils iu c deg
pord
y rn isu dd
rs
t
ven
aildi ih n tgl a ec nd
ge
ive at
gd
ii et t nvi eec g0
Figure3: Cross-correlationalignmentinformation. Bluepoints/arrowmarksinAandCrepresentGPT-originated
data,whileredrepresentshuman-originatedinstead. A:TheMDSsolutionofappliedtothecombinedwithin/across
cross-domain (cross-correlation) matrix as a set of high-dimensional embedding to represent shared space of
conversational tones embeddings across humans and GPT. Grey edges connect points representing the same
conversationaltoneword. Arrowmarksrepresentrating-deriveddimensionsofconversationaltones. B:Abarplot
exhibitingtheEuclideandistancebetweenpairsofthesameconversationaltoneembeddingsinMDSspace. Error
bars represent one standard deviation via bootstrapping. C: A graph showing the nearest neighbor matches of
conversationaltoneembeddingsacrosshumansandGPT.Tomeasurerobustnessinmatching,webootstrapped
theprocess5000times. Darkedgesrepresentthefrequencyofitsmatchingthroughoutbootstrapprocesses. See
enlargedversionofthisfigureintheAppendix(Figure18).
farther away from neutral in the MDS space and humansandGPThavetermsthatarerepresented
stronger in the positive valence direction for hu- morebroadlyintheothergroup.
mans. Thus,“proud”hasamoreneutralmeaning
for GPT. This shows how we can use Figure 3A 4.4 Application: GroundTruthfor
tocharacterizehowtonesareconveyedinhumans BenchmarkingSemanticAlignment
andGPT. Methods
Finally,Figure3Cprovidesamappingofsimi- Todemonstratetheutilityofourdata,wedemon-
lartonesacrosshumansandLLMs. Foreachtone strate how it can be used to benchmark semantic
in one domain (humans or GPT), we present its alignmentmethods. Notethatinourapproachwe
nearest neighbor in another domain. This map is leverage the fact that we had the same taxonomy
importantbecauseitallowsusto"translate"conver- oftonesforbothhumansandGPT.Inmanyother
sationaltoesfromhumanstoGPTandviceversa. usecases,thisisnotpossible. Forexample,when
As a result, Figure 3C can also help summarize translatingwordsfromonelanguagetoanotherit
distancesinFigure3A:thelinesinFigure3Crep- is impossible to ask speakers of one language to
resentconceptsthatarenearoneanotherinFigure annotatewordsinadifferentlanguage. Sinceour
3A.Interestingly,wefoundcaseswheremultiple method has this kind of cross-linguistic informa-
human tones (e.g., “grateful”, “joyful”, “happy”) tionavailable,weuseitasasourceofgroundtruth
werecollapsedtoasingleLLMtone(“pleased”), totestmethodsthatdonothaveaccesstothiskind
suggesting that these terms were conveyed in a ofinformation. Specifically,wesurveythecapabil-
more limited way by GPT. We also found the re- ityoffrequentlyusedunsupervisedcross-domain
versephenomenonthattheGPTtones: “irritated”, alignment paradigms: Gromov-Wasserstein Op-
“annoyed”, and “disappointed” collapsed to “an- timal Transport (Grave et al., 2018), Orthogonal
gry” on the human side. This suggests that both Procrustes Transformation (Schönemann, 1966),
ecnatsiD
Robustnessand Bilingual Lexicon Induction (Ruder et al., suggestingthatitwouldbeappropriateforapplica-
2018). We found that Bilingual Lexicon Induc- tionssuchasmachinetranslation.
tionvialatentvariablemodel(abbreviatedhereon Ourworkopensupmultipleavenuesforfuture
asBLI)isthebest-performingapproach. Itrecov- research. First, the same approach can be easily
ers well the quality-of-fit rating similarity matrix extendedtootherdomains. Forexample,speakers
entries (r = 0.81, CI = [0.81, 0.81]), followed by of different languages and different cultures, as
Orthogonal Procrustes Transformation (r = 0.56 well as different language models. Second, our
CI = [0.54,0.58]) and Gromov-Wasserstein Opti- dataset can be used as a training signal for better
mal Transport (r = 0.54 CI = [0.53,0.58]). We aligninghumanandLLMconversationaltones;for
foundsimilarresultsfortherecoveryofk-Nearest- example, via an iterative process of refinements
Neighborstructures(seeAppendixTable3). This likereinforcementlearning(Jietal.,2024). Third,
indicates that not only is Bilingual Lexicon In- futureworkcouldlookintowhetherwecanuseour
duction superior at recovering the cross-domain alignmentmapstopredictperformanceinhuman-
proximitystructurepresentedinourcross-domain AIcommunication. Morebroadly,thisworkshows
groundtruth,butalsothatitcanpreservetheintra- howcombiningapproachesfrommachinelearning
domainproximitystructureofourembeddingsaf- and cognitive science provides routes for better
ter the alignment procedure. Then, in turn, this understandingandresolvingchallengesinhuman-
performance turns to show the superiority of our computerinteractions.
method in constructing a dataset that allows for
almost-perfectreconstructionofsimilaritymetrics Limitations
forelementsofthepsycholinguisticmodality.
Thereareseveraltechnicallimitationsofourstudy
5 Discussion that are important to highlight. First, we did not
testawidevarietyofLLMsandLLMparameters,
Usingacognitivescience-inspiredSamplingwith including varying the prompt and temperature of
Peopleparadigm,weelicitedtonesandsentences themodels. Thislimitsthegeneralizabilityofour
forbothhumansandGPTcreatingashareddataset results, as therobustness ofsome of our findings
of sentences and tones. Then, via quality-of-fit maydependontheseparameters. Second,weused
ratings,wefurthershowedthatthedegreeofalign- afinitenumberofsamplingchainswithonly100
mentfordifferenttonesinhumansandGPTvaries. generations. Futureworkcanexplorewhatwould
Tone alignment was high for tones such as “joy- be the effect of changing these hyperparameters.
ful”and“pleased”whileitwassignificantlylower Finally, we only tested participants in the UK. It
for tones such as “proud” and “apologetic”. In a wouldbeinformativetotestUSparticipantsaswell
separateexperiment,wefoundthatsimilarityjudg- as a wider range of speakers of other languages
mentswereconsistentwiththeresultingrelation- (Blasietal.,2022).
shipbetweentones. Next,byprojectingtherating More importantly, it is crucial to acknowledge
vectors to a joint semantic space, we found vari- that employing free elicitation methods could in-
abilityintoneproximity,whichisexplainedmost advertently generate sentences that reinforce so-
by the well-known theoretical dimension of va- cietal biases, including racial and gender stereo-
lence(Russell,1980). Additionally,weprovideda types. However,futureresearchcouldinvestigate
mappingofsimilartonesacrosshumansandLLMs. alternative filtering approaches, possibly involv-
Wefoundcaseswheremultiplehumantones(e.g., ing human moderation, to actively reduce biases
“grateful”,“joyful”,“happy”)werecollapsedtoa within the Sampling with People iterations (van
singleLLM-proposedtone(“pleased”),aswellas Rijnetal.,2022a). Nonetheless,ourapproachcan
in the opposite direction (e.g., GPT’s “irritated”, beusedwithparticipantsinanylanguage, which
“annoyed”,and“disappointed”collapsedontohu- canhelpwithcreatingAIsystemsforlow-resource
mans’“angry”),suggestingthatbothdistributions languages(Atarietal.,2023;Rathjeetal.,2023).
endedupinvolvingahypernymforconversational Inparticular,weareexcitedaboutthepotentialap-
tonecategories. Finally,wedemonstratehowour plicationofourapproachtostudyingcross-cultural
data can be used to benchmark methods for se- differencesintoneofvoice. Wealsobelievethat
mantic alignment. We found that Bilingual Lexi- our research holds the potential to facilitate nu-
calInductionsurpassesother(geometric)methods, anced cross-cultural communication and supportthedevelopmentofAIsystemsthatcommunicate MikelArtetxe,GorkaLabaka,andEnekoAgirre.2017.
effectivelywithusersfromdiversebackgrounds. Learningbilingualwordembeddingswith(almost)
no bilingual data. In Proceedings of the 55th An-
Ethics nualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),pages451–462.
Werunourhumanexperimentapplyingbestprac-
MikelArtetxe,GorkaLabaka,andEnekoAgirre.2018a.
tices in the responsible and ethical treatment of
Generalizingandimprovingbilingualwordembed-
human subjects. We have deliberately reviewed
dingmappingswithamulti-stepframeworkoflinear
theACLCodeofEthicsandconfirmthatourwork transformations. InProceedingsoftheThirty-Second
conforms to all principles addressed in this code. AAAI Conference on Artificial Intelligence, pages
5012–5019.
Allhumanexperimentparticipantsarepaid$9per
hour and join the experiment after signing an in-
MikelArtetxe,GorkaLabaka,andEnekoAgirre.2018b.
formedconsentofanapprovedprotocol. Specifi- Arobustself-learningmethodforfullyunsupervised
cally,participantswererecruitedonlineviaProlific cross-lingualmappingsofwordembeddings. InPro-
ceedingsofthe56thAnnualMeetingoftheAssocia-
and provided consent in accordance with an ap-
tionforComputationalLinguistics(Volume1: Long
provedprotocol(MaxPlanckEthicsCouncil#2021
Papers),pages789–798.
42).
InourSamplingwithPeoplehumanexperiments, MohammadAtari,MonaJXue,PeterSPark,DamiánE
toavoidtheappearanceofprofanewordsortrigger- Blasi,andJosephHenrich.2023. Whichhumans?
ingtopics,wehaveaddedaprofanityfiltertoour
A.AthanasiadouandE.Tabakowska.1998. Speakingof
experimentsuchthatresponsescontainingprofan- Emotions: ConceptualisationandExpression. Cog-
ityorvulgarwordscannotbepropagatedalongour nitivelinguisticsresearch.MoutondeGruyter.
samplingchains. Datawascollectedanonymously
H Clark Barrett. 2020. Towards a cognitive science
(beyond participants’ Prolific IDs that were used
of the human: cross-cultural approaches and their
for compensation). The published data was fully
urgency. Trends in cognitive sciences, 24(8):620–
anonymized. 638.
AntonBatliner,StefanSteidl,andElmarNoeth.2008.
References Releasing a thoroughly annotated and processed
spontaneousemotionaldatabase:theFAUAiboEmo-
Francis M. Adams and Charles E. Osgood. 1973. tionCorpus.
A cross-cultural study of the affective meanings
of color. Journal of Cross-Cultural Psychology, André Beauducel. 2018. Recovering wood and mc-
4(2):135–156. carthy’s erp-prototypes by means of erp-specific
procrustes-rotation. JournalofNeuroscienceMeth-
Elisa Claire Alemán Carreón, Hugo Alberto Men-
ods,295:20–36.
dozaEspaña,HirofumiNonaka,andToruHiraoka.
2021. Differences in chinese and western tourists
BrentBerlinandPaulKay.1969. BasicColorTerms:
faced with japanese hospitality: a natural lan-
TheirUniversalityandEvolution.
guageprocessingapproach. InformationTechnology;
Tourism,23(3):381–438.
DamiánE.Blasi,JosephHenrich,EvangeliaAdamou,
David Kemmerer, and Asifa Majid. 2022. Over-
Manuel Anglada-Tort, Peter MC Harrison, Harin
relianceonenglishhinderscognitivescience. Trends
Lee, and Nori Jacoby. 2023. Large-scale iterated
inCognitiveSciences,26(12):1153–1170.
singingexperimentsrevealoraltransmissionmecha-
nismsunderlyingmusicevolution. CurrentBiology,
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
33(8):1472–1486.
Kazemzadeh, Emily Mower, Samuel Kim, Jean-
FarzanaAnowar,SamiraSadaoui,andBassantSelim. nette N. Chang, Sungbok Lee, and Shrikanth S.
2021. Conceptualandempiricalcomparisonofdi- Narayanan.2008. IEMOCAP:interactiveemotional
mensionality reduction algorithms (pca, kpca, lda, dyadic motion capture database. Language Re-
mds,svd,lle,isomap,le,ica,t-sne). ComputerSci- sourcesandEvaluation,42(4):335–359.
enceReview,40:100378.
J Douglas Carroll and Phipps Arabie. 1998. Multi-
MikelArtetxe,GorkaLabaka,andEnekoAgirre.2016. dimensional scaling. Measurement, judgment and
Learningprincipledbilingualmappingsofwordem- decisionmaking,pages179–250.
beddingswhilepreservingmonolingualinvariance.
In Proceedings of the 2016 Conference on Empiri- Yuh-Fang Chang. 2009. How to say no: an analysis
calMethodsinNaturalLanguageProcessing,pages of cross-cultural difference and pragmatic transfer.
2289–2294. LanguageSciences,31(4):477–493.AlexisConneau,GuillaumeLample,Marc’AurelioRan- JortF.Gemmeke,DanielP.W.Ellis,DylanFreedman,
zato, Ludovic Denoyer, and Hervé Jégou. 2017. ArenJansen,WadeLawrence,R.ChanningMoore,
Wordtranslationwithoutparalleldata. arXivpreprint ManojPlakal,andMarvinRitter.2017. Audioset:
arXiv:1710.04087. An ontology and human-labeled dataset for audio
events. InProc.IEEEICASSP2017,NewOrleans,
Alan Cowen, Hillary Elfenbein, Petri Laukka, and LA.
Dacher Keltner. 2018. Mapping 24 emotions con-
veyedbybriefhumanvocalization. AmericanPsy- AlessandroT.Gifford,KshitijDwivedi,GemmaRoig,
chologist. andRadoslawM.Cichy.2022. Alargeandricheeg
datasetformodelinghumanvisualobjectrecognition.
AlanCowen,XiaFang,DisaSauter,andDacherKeltner. NeuroImage,264:119754.
2020. Whatmusicmakesusfeel: Atleast13dimen-
sionsorganizesubjectiveexperiencesassociatedwith IanJ.Goodfellow,DumitruErhan,PierreLucCarrier,
musicacrossdifferentcultures. Proceedingsofthe Aaron Courville, Mehdi Mirza, Ben Hamner, Will
NationalAcademyofSciences,117:201910704. Cukierski,YichuanTang,DavidThaler,Dong-Hyun
Lee,YingboZhou,ChetanRamaiah,FangxiangFeng,
Alan Cowen, Petri Laukka, Hillary Elfenbein, Run- RuifanLi,XiaojieWang,DimitrisAthanasakis,John
jing Liu, and Dacher Keltner. 2019. The primacy Shawe-Taylor, Maxim Milakov, John Park, Radu
of categories in the recognition of 12 emotions in Ionescu, Marius Popescu, Cristian Grozea, James
speechprosodyacrosstwocultures. NatureHuman Bergstra,JingjingXie,LukaszRomaszko,BingXu,
Behaviour,3:1. Zhang Chuang, and Yoshua Bengio. 2013. Chal-
lengesinrepresentationlearning: Areportonthree
AlanS.CowenandDacherKeltner.2017. Self-report
machinelearningcontests.
captures27distinctcategoriesofemotionbridgedby
continuousgradients. ProceedingsoftheNational
EdouardGrave, ArmandJoulin, andQuentinBerthet.
AcademyofSciences,114:E7900–E7909.
2018. Unsupervisedalignmentofembeddingswith
wassersteinprocrustes.
Alan S. Cowen and Dacher Keltner. 2020. What the
face displays: Mapping 28 emotions conveyed by
M.J.Greenacre.2010. BiplotsinPractice. Fundación
naturalisticexpression. TheAmericanpsychologist.
BBVA.
Simon De Deyne, Danielle J. Navarro, Amy Perfors,
Thomas L Griffiths and Michael L Kalish. 2005. A
MarcBrysbaert,andGertStorms.2019. The“small
bayesianviewoflanguageevolutionbyiteratedlearn-
worldofwords”englishwordassociationnormsfor
ing. In Proceedings of the annual meeting of the
over12,000cuewords. BehaviorResearchMethods,
cognitivesciencesociety,volume27.
51(3):987–1006.
TLGriffiths,AdamNSanborn,RMarjieh,TLanglois,
SimonDeDeyne,SophieWarner,andAndrewPerfors.
J Xu, and N Jacoby. 2024. Estimating subjective
under review. Common words, uncommon mean-
probabilitydistributions.
ings: Evidenceforwidespreadgenderdifferencesin
wordmeaning. InProceedingsofthe45thAnnual
Peter M. C. Harrison, Raja Marjieh, Federico Adolfi,
ConferenceoftheCognitiveScienceSociety.
Pol van Rijn, Manuel Anglada-Tort, Ofer Tcherni-
chovski,PaulineLarrouy-Maestri,andNoriJacoby.
MarianodeRivero,CristhiamTirado,andWillyUgarte.
2020. Gibbssamplingwithpeople.
2023. Formalstyler: Gpt-based model for formal
styletransferwithmeaningpreservation. SNCom-
Martin N Hebart, Adam H Dickter, Alexis Kidder,
puterScience,4(6):739.
Wan Y Kwok, Anna Corriveau, Caitlin Van Wick-
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi, lin,andChrisIBaker.2019. THINGS:Adatabase
andLiFei-Fei.2009. Imagenet: Alarge-scalehier- of1,854objectconceptsandmorethan26,000natu-
archicalimagedatabase. In2009IEEEConference ralisticobjectimages. PLoSOne,14(10):e0223792.
onComputerVisionandPatternRecognition,pages
248–255. JosephHenrich,StevenJHeine,andAraNorenzayan.
2010. Theweirdestpeopleintheworld? Behavioral
Paul Ekman. 1992. An argument for basic emotions. andbrainsciences,33(2-3):61–83.
Cognition&Emotion,6:169–200.
YihanHou,ManlingYang,HaoCui,LeiWang,JieXu,
MoatazElAyadi,MohamedS.Kamel,andFakhriKar- andWeiZeng.2024. C2ideas: Supportingcreative
ray. 2011. Survey on speech emotion recognition: interior color design ideation with large language
Features,classificationschemes,anddatabases. Pat- model. ArXiv,abs/2401.12586.
ternRecognition,44(3):572–587.
NoriJacobyandJoshHMcDermott.2017. Integerratio
Kai Von Fintel. 2006. Modality and language. In priorsonmusicalrhythmrevealedcross-culturallyby
D. Borchert, editor, Encyclopedia of Philosophy, iteratedreproduction. CurrentBiology,27(3):359–
pages20–27.MacmillanReference. 370.NoriJacoby,RainerPolak,JessicaAGrahn,DanielJ ThomasLanglois,NoriJacoby,JordanWSuchow,and
Cameron, Kyung Myun Lee, Ricardo Godoy, Ed- Thomas L Griffiths. 2017. Uncovering visual pri-
uardo A Undurraga, Tomás Huanca, Timon Thal- orsinspatialmemoryusingserialreproduction. In
witzer,NoumoukéDoumbia,etal.2024. Commonal- CogSci.
ityandvariationinmentalrepresentationsofmusic
revealed by a cross-cultural comparison of rhythm Thomas A Langlois, Nori Jacoby, Jordan W Suchow,
priors in 15 countries. Nature Human Behaviour, andThomasLGriffiths.2021. Serialreproduction
pages1–32. revealsthegeometryofvisuospatialrepresentations.
Proceedings of the National Academy of Sciences,
RomanJakobson.1960. Closingstatement: Linguistics 118(13):e2012938118.
andpoetics. InStyleinLanguage,pages350–.
Chujun Lin, Umit Keles, and Ralph Adolphs. 2021.
JiamingJi,TianyiQiu,BoyuanChen,BorongZhang, Fourdimensionscharacterizeattributionsfromfaces
Hantao Lou, Kaile Wang, Yawen Duan, Zhong- usingarepresentativesetofenglishtraitwords. Na-
hao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, tureCommunications,12(1):5168.
Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan
O’Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, KristenA.Lindquist,JoshuaConradJackson,Joseph
Stephen McAleer, Yaodong Yang, Yizhou Wang, Leshin,AjayB.Satpute,andMariaGendron.2022.
Song-ChunZhu,YikeGuo,andWenGao.2024. Ai Theculturalevolutionofemotion. NatureReviews
alignment: Acomprehensivesurvey. Psychology,1(11):669–681.
DiJin,ZhijingJin,ZhitingHu,OlgaVechtomova,and XiaoLiu, JingZhao, ShiliangSun, HuawenLiu, and
RadaMihalcea.2022. DeepLearningforTextStyle Hao Yang. 2021. Variational multimodal machine
Transfer: A Survey. Computational Linguistics, translationwithunderlyingsemanticalignment. In-
48(1):155–205. formationFusion,69:73–80.
PatrikJuslinandJohnSloboda.2013. MusicandEmo- HansPeterLuhn.1958. Theautomaticcreationofliter-
tion,pages583–645. atureabstracts. IBMJournalofResearchandDevel-
opment,2(2):159–165.
PatrikNJuslinandDanielVästfjäll.2008. Emotional
responses to music: The need to consider underly- Raja Marjieh, Peter MC Harrison, Harin Lee, Fotini
ing mechanisms. Behavioral and brain sciences, Deligiannaki,andNoriJacoby.2024. Timbraleffects
31(5):559–575. on consonance disentangle psychoacoustic mecha-
nisms and suggest perceptual origins for musical
GenjiKawakita,ArielZeleznikow-Johnston,Naotsugu scales. NatureCommunications,15(1):1482.
Tsuchiya,andMasafumiOizumi.2023. Comparing
colorsimilaritystructuresbetweenhumansandllms RajaMarjieh,IliaSucholutsky,PolvanRijn,NoriJa-
viaunsupervisedalignment. coby,andThomasLGriffiths.2023a. Largelanguage
modelspredicthumansensoryjudgmentsacrosssix
WillKay,JoaoCarreira,KarenSimonyan,BrianZhang, modalities. arXivpreprintarXiv:2302.01308.
ChloeHillier,SudheendraVijayanarasimhan,Fabio
Viola,TimGreen,TrevorBack,PaulNatsev,Mustafa Raja Marjieh, Pol van Rijn, Ilia Sucholutsky,
Suleyman,andAndrewZisserman.2017. Thekinet- TheodoreR.Sumers,HarinLee,ThomasL.Griffiths,
icshumanactionvideodataset. and Nori Jacoby. 2023b. Words are all you need?
languageasanapproximationforhumansimilarity
YousifKhaireddinandZhuofaChen.2021. Facialemo- judgments.
tion recognition: State of the art performance on
fer2013. Louis Marti, Shengyi Wu, Steven T. Piantadosi, and
CelesteKidd.2023. LatentDiversityinHumanCon-
DolichanKollareth,José-MiguelFernandez-Dols,and cepts. OpenMind,7:79–92.
James Russell. 2018. Shame as a culture-specific
emotionconcept. JournalofCognitionandCulture, LelandMcInnes,JohnHealy,andJamesMelville.2020.
18:274–292. Umap: Uniformmanifoldapproximationandprojec-
tionfordimensionreduction.
RogerJ.KreuzandRichardM.Roberts.1993. When
collaborationfails:Consequencesofpragmaticerrors JohnOetzel,StellaTing-Toomey,TomokoMasumoto,
inconversation. JournalofPragmatics,19(3):239– Yumiko Yokochi, Xiaohui Pan, Jiro Takai, and
252. RichardWilcox.2001. Faceandfaceworkinconflict:
across-culturalcomparisonofchina,germany,japan,
Alex Krizhevsky. 2009. Learning multiple layers of andtheunitedstates. CommunicationMonographs,
featuresfromtinyimages. 68(3):235–258.
GuillaumeLample,AlexisConneau,LudovicDenoyer, Eva Ogiermann. 2009. Politeness and in-directness
andMarc’AurelioRanzato.2017. Unsupervisedma- across cultures: A comparison of english, german,
chine translation using monolingual corpora only. polish and russian requests. Journal of Politeness
arXivpreprintarXiv:1711.00043. Research,5(2):189–216.LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car- DanielaSchiller,NCAlessandra,NellyAlia-Klein,Su-
rollL.Wainwright,PamelaMishkin,ChongZhang, sanneBecker,HowardCCromwell,FlorinDolcos,
SandhiniAgarwal,KatarinaSlama,AlexRay,John PaulJEslinger,PaulFrewen,AndrewHKemp,Ed-
Schulman,JacobHilton,FraserKelton,LukeMiller, wardFPace-Schott, etal.2023. Thehumanaffec-
Maddie Simens, Amanda Askell, Peter Welinder, tome. Neuroscience&BiobehavioralReviews,page
Paul Christiano, Jan Leike, and Ryan Lowe. 2022. 105450.
Traininglanguagemodelstofollowinstructionswith
humanfeedback. PeterH.Schönemann.1966. Ageneralizedsolutionof
theorthogonalprocrustesproblem. Psychometrika,
StephenEPalmer,KarenBSchloss,ZoeXu,andLiliaR 31:1–10.
Prado-León.2013. Music-colorassociationsareme-
diatedbyemotion. Proc.Natl.Acad.Sci.U.S.A.,
Zhaleh Semnani-Azad and Wendi L. Adair. 2013.
110(22):8836–8841.
Watch your tone ... relational paralinguistic mes-
sagesinnegotiation. InternationalStudiesofMan-
PaulPortner.2009. Modality. Modality.OUPOxford.
agement&Organization,43(4):64–89.
RafaelRafailov,ArchitSharma,EricMitchell,Stefano
Ermon,ChristopherD.Manning,andChelseaFinn. Roger N. Shepard. 1980. Multidimensional scaling,
2023. Directpreferenceoptimization:Yourlanguage tree-fitting,andclustering. Science,210:390–398.
modelissecretlyarewardmodel.
IliaSucholutsky, RuairidhMBattleday, KatherineM
SteveRathje,Dan-MirceaMirea,IliaSucholutsky,Raja Collins,RajaMarjieh,JoshuaPeterson,PulkitSingh,
Marjieh,ClaireRobertson,andJayJVanBavel.2023. Umang Bhatt, Nori Jacoby, Adrian Weller, and
Gptisaneffectivetoolformultilingualpsychological ThomasLGriffiths.2023a. Ontheinformativeness
textanalysis. ofsupervisionsignals. InUncertaintyinArtificial
Intelligence,pages2036–2046.PMLR.
David Rozado, Ruth Hughes, and Jamin Halberstadt.
2022. Longitudinalanalysisofsentimentandemo-
Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller,
tion in news media headlines using automated la-
Andi Peng, Andreea Bobu, Been Kim, Bradley C.
belling with transformer language models. PLOS
Love, Erin Grant, Iris Groen, Jascha Achterberg,
ONE,17(10):1–14.
JoshuaB.Tenenbaum,KatherineM.Collins,Kather-
ine L. Hermann, Kerem Oktar, Klaus Greff, Mar-
Sebastian Ruder, Ryan Cotterell, Yova Kementched-
tinN.Hebart,NoriJacoby,QiuyiZhang,RajaMar-
jhieva,andAndersSøgaard.2018. Adiscriminative
jieh,RobertGeirhos,SherolChen,SimonKornblith,
latent-variablemodelforbilinguallexiconinduction.
SunayanaRane,TaliaKonkle,ThomasP.O’Connell,
ThomasUnterthiner,AndrewK.Lampinen,Klaus-
JürgenRudolph,ShannonTan,andSamsonTan.2023.
RobertMüller,MariyaToneva,andThomasL.Grif-
Warofthechatbots: Bard,bingchat,chatgpt,ernie
fiths. 2023b. Getting aligned on representational
andbeyond.thenewaigoldrushanditsimpacton
alignment.
highereducation. JournalofAppliedLearningand
Teaching,6(1).
Deborah Tannen. 1984. The pragmatics of cross-
James A. Russell. 1980. A circumplex model of af- cultural communication. Applied linguistics,
fect. Journalofpersonalityandsocialpsychology, 5(3):189–195.
39(6):1161–1178.
BillD.Thompson,SeánG.Roberts,andGaryLupyan.
AaronSaiewitzandThomasKida.2018. Theeffectsof 2020. Culturalinfluencesonwordmeaningsrevealed
anauditor’scommunicationmodeandprofessional throughlarge-scalesemanticalignment. NatureHu-
toneonclientresponsestoauditinquiries. Account- manBehaviour,4:1029–1038.
ing,OrganizationsandSociety,65:33–43.
StellaTing-Toomey,GeGao,PaulaTrubisky,Zhizhong
AdamN.Sanborn,ThomasL.Griffiths,andRichardM.
Yang, Hak-Soo Kim, Sung-Ling Lin, and Tsukasa
Shiffrin.2010. Uncoveringmentalrepresentations
Nishida.1991. Culture,facemaintenance,andstyles
withmarkovchainmontecarlo. CognitivePsychol-
ofhandlinginterpersonalconflict:Astudyinfivecul-
ogy,60(2):63–106.
tures. InternationalJournalofConflictManagement,
2:275–296.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
ofbert: smaller, faster, cheaperandlighter. ArXiv, ElineVanGeertandNoriJacoby.2024. Usinggibbs
abs/1910.01108. samplingwithpeopletocharacterizeperceptualand
aestheticevaluationsinmultidimensionalvisualstim-
Disa Sauter, Frank Eisner, Paul Ekman, and Sophie ulusspace.
Scott. 2010. Cross-cultural recognition of basic
emotionsthroughnonverbalemotionalvocalizations. PolvanRijn,HarinLee,andNoriJacoby.2022a. Bridg-
ProceedingsoftheNationalAcademyofSciencesof ingtheprosodygap: Geneticalgorithmwithpeople
theUnitedStatesofAmerica,107:2408–12. toefficientlysampleemotionalprosody.PolvanRijn,SilvanMertes,KathrinJanowski,Katha- TransactionsonPatternAnalysisandMachineIntel-
rinaWeitz,NoriJacoby,andElisabethAndré.2024. ligence.
Givingrobotsavoice: Human-in-the-loopvoicecre-
ationandopen-endedlabeling. InProceedingsofthe BenjaminD.Zinszer,AndrewJ.Anderson,OliviaKang,
CHI Conference on Human Factors in Computing Thalia Wheatley, and Rajeev D. S. Raizada. 2016.
Systems,pages1–34. SemanticStructuralAlignmentofNeuralRepresen-
tationalSpacesEnablesTranslationbetweenEnglish
Pol van Rijn, Silvan Mertes, Dominik Schiller, Piotr and Chinese Words. Journal of Cognitive Neuro-
Dura, Hubert Siuzdak, Peter Harrison, Elisabeth science,28(11):1749–1759.
André, and Nori Jacoby. 2022b. Voiceme: Per-
sonalized voice generation in tts. arXiv preprint
arXiv:2203.15379.
Pol Van Rijn, Silvan Mertes, Dominik Schiller, Peter
Harrison,PaulineLarrouy-Maestri,ElisabethAndré,
andNoriJacoby.2021. Exploringemotionalproto-
typesinahighdimensionalttslatentspace. arXiv
preprintarXiv:2105.01891.
JefA.vanSchendelandRaymondH.Cuijpers.2015.
Turn-yielding cues in robot-human conversation.
AISB Convention 2015, Society for the Study of
ArtificialIntelligenceandSimulationofBehaviour,
20-22 April 2015, Canterbury, 2015, AISB2015 ;
Conferencedate: 20-04-2015Through22-04-2015.
TieyanWangandTieshanWang.2016. Effectsofcolor
onexpectationsofdrugeffects:Across-gendercross-
culturalstudy. ColorResearch&Application,42:n/a–
n/a.
Xun Wang, Tao Ge, Allen Mao, Yuki Li, Furu Wei,
andSi-QingChen.2022. Payattentiontoyourtone:
Introducinganewdatasetforpolitelanguagerewrite.
YunliWang,YuWu,LiliMou,ZhoujunLi,andWenhan
Chao.2019. Harnessingpre-trainedneuralnetworks
withrulesforformalitystyletransfer. InProceedings
ofthe2019ConferenceonEmpiricalMethodsinNat-
uralLanguageProcessingandthe9thInternational
JointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP),pages3573–3578,HongKong,
China.AssociationforComputationalLinguistics.
JingXu,ThomasGriffiths,andMikeDowman.2010.
Replicatingcolortermuniversalsthroughhumaniter-
atedlearning. InProceedingsoftheAnnualMeeting
oftheCognitiveScienceSociety,volume32.
JingXuandThomasLGriffiths.2010. Arationalanal-
ysisoftheeffectsofmemorybiasesonserialrepro-
duction. Cognitivepsychology,60(2):107–126.
Michael Yeomans, Maurice E. Schweitzer, and Ali-
son Wood Brooks. 2022. The conversational cir-
cumplex: Identifying,prioritizing,andpursuingin-
formational andrelational motivesin conversation.
CurrentOpinioninPsychology,44:293–302.
PustakhanimYusifova.2018. Threelayersofpragmatic
failureacrosslanguagesandcultures. International
JournalofEnglishLinguistics.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude
Oliva, and Antonio Torralba. 2017. Places: A 10
millionimagedatabaseforscenerecognition. IEEEA Appendix: AdditionalMethods tence with a conversational tone (T trials; Figure
1A,6b)orcreatearesponsesentencematchinga
A.1 ImplementationoftheExperiments
displayedconversationaltone(Strials;Figure1A,
HumanExperiments. Humanexperimentsareim- 6a). Therewere90SPsamplingchains,eachwith
plementedusingPsynet(Harrisonetal.,2020),a 100 iterations per chain (50 S and T trials). This
Pythonpackageforimplementingcomplexonline results in exactly 4,500 generated sentences and
psychology experiments. PsyNet automates the 4,500toneannotations.
online hosting of experiments and automatically Quality-of-fitRatingExperiment. Werecruited
paysparticipantsthroughavarietyofrecruitment 275humanparticipantsfortheQuality-of-fitRat-
platforms,suchasProlific. ing experiments. Participants performed 12 rat-
GPT Experiments. GPT experiments are im- ingtrials,wheretheyratedeachpairofsentence-
plemented in Python and GPT responses are all tonesonfiveLikertscales(1beingtheweakest,5
elicitedusingitschatcompletionmode. Wealways being the strongest) based on the strength of the
usedatemperatureof0.8toelicitresponseswith conversational tone. For each sentence-tone pair,
highervariance. Inallexperiments,weusedGPT-4 we collected approximately five ratings. Overall
(theJune13th,2023release). we collected 5 responses for all participants and
Licensed Code The implementations of unsu- sentence-tonepairs.
pervisedalignmentmethodsthatwereferencefor Similarity Judgement Experiment. We re-
GWOTandlatentvariablemodel-basedBilingual cruited71humanparticipantsfortheexperiment.
Lexicon Induction follow respectively a CC BY- Participantsperformed50to60trialswherethey
NC4.0licenseandaGPL-3.0license. were asked to provide five similarity judgments
Data Anonymity We confirm that all human- perpairof(notnecessarilydistinct)conversational
originateddataprovidedwithoursubmissionare tones, on a scale from 1 to 5, with 1 being the
anonymized. mostdissimilarand5beingthemostsimilar. We
ComputationalBudgetWeusedanAWSserver then computed the resulting similarity for each
to host our online human experiments. We used conversation-tone pair (t ,t ), normalized to the
1 2
an AWS EC2 m5.2xlarge container for 90 hours scaleof[0,1].
forcompletingallnecessaryexperimentsonce,and ToneFeatureRatingExperiment. Werecruited
1 AMD Ryzen 7 5800 CPU for 45 hours for all 38humanparticipantsfortheexperiment. Inthis
analysesandGPTrequestsonce. NoGPUswere experiment,participantsperformed30to40trials
usedintheprogressofthiswork. wheretheywereaskedtoratetone-featurepairson
a1-to-5Likertscale. Wetestedfourtonefeatures
A.2 ParticipantsandProcedure
(positivenessinvalence,emotionalarousal,infor-
Werecruitedparticipantsfromthecrowd-sourcing mational,relational;seebelowfordefinition). The
recruiting service Prolific (https://prolific. definitionsofthesefeatureswhichwereprovidedto
com/). Participants were compensated approxi- participantsandGPTagentsarelistedinAppendix
mately 9 GBP per hour for their time. To reduce A.7. The resulting rating for each sentence-tone
culturaldifferencesbetweenEnglish-speakingpar- pairwastheaverageoftheobtainedfiveratings.
ticipants,wetargetspecificallyEnglish-speaking
A.3 ExplainingtheConceptofConversational
participants in one country. All participants sat-
Tone
isfied the following three criteria: (1) Were over
18yearsofage,(2)LivedintheUnitedKingdom, Inallhumanexperiments,participantsfirstreceive
(3)NativeEnglishspeakers. Participantswerere- instructionsexplainingwhataconversationaltone
cruitedonlineviaProlificandprovidedconsentin isandexamplesentencesclarifyingtheconcepts.
accordancewithanapprovedprotocol(MaxPlanck Uponenteringtheexperiments,allparticipants
EthicsCouncil#202142). arepresentedwiththisdefinitionofconversational
Sampling with People Experiment. We re- tone. Afterbeingpresentedwithanoperationalized
cruited955humanparticipantsforSamplingwith definitionofconversationaltone,participantsare
People(SP)experiments. Theseexperimentshad provided examples that show usages of different
the following procedure. Each participant com- conversational tones. Participants will be shown
pleted 10 to 12 trials. For each trial, participants an example regarding detecting a conversational
were randomly assigned to either annotate a sen- tonefromasentence,andanexampleofcreatingExperiment #HumanParticipants #HumanResponses #GPTResponses
SPSampling 955 9000 9000
Quality-of-fitRating 275 19470 16000
SimilarityJudgment 71 4100 4100
ToneFeatureRating 38 800 800
Overall 1339 33370 29900
Table1: SummaryTable. Numberofhumanparticipants,humanjudgments,andGPTjudgmentsinvolvedinour
paper.
(a)Definitionofconversationaltonesshowntoparticipants.
(b)Examplescenariosandappropriateexampleanswers.
(c)Twoexamplesexplainingtheconceptofconversationaltones.
Figure4: Instructionsshowntoparticipants.Create 50 data according to the following format:
{
''example_tone'': An adjective that describes a conversational tone,
''example_sentence_1'': A sentence with at least 7 words that has the a conversational tone
of example_tone,
''example_sentence_1_explanation'': A 20~30 word explanation on 2~3 conversational tones of
example sentence 1,
''example_sentence_2'': A different sentence with at least 7 words that has the a conversation
tone of example_tone,
''example_sentence_2_explanation'': A 20~30 word explanation on 2~3 conversational tones of
example sentence 1
}
(a)ChatGPTpromptforcreatingseedsofinstructionalexamplesonhowtocreatesentencesfromconversationaltones.
Create 50 data according to the following format:
{
''example_sentence'': A sentence with at least 7 words,
''example_tone_1'': An adjective representing a different conversational tone you observe
from ''example_sentence'',
''example_tone_2'': An adjective representing a different conversational tone you observe
from ''example_sentence'',
''example_tone_1_explanation'': Explain how you observed tone 1 from ''example_sentence''
with 20 to 30 words,
''example_tone_2_explanation'': Explain how you observed tone 2 from ''example_sentence''
with 20 to 30 words
}
(b)ChatGPTpromptforcreatingseedsofinstructionalexamplesonhowtodetectconversationaltonesfromsentences.
Figure5: ChatGPTpromptsforcreatingseedsthatgeneratethetextseenininterfacedetailedatFigure4b
asentencethathasaprovidedconversationaltone. A.3.2 SliderInstruction
TheseinterfacesareexemplifiedinFigure4.
Insomeexperiments,participantswillneedtorate
There is a pool of 50 items for each type of somepropertiesofconversationaltonesonaLikert
example,allcreatedviaChatGPT.Tomitigatebias scaleof1to5. Forthoseexperiments,afterexplain-
resulting from examples, participants are shown ing the concept of conversational tones, they are
a random item from each pool of examples. The presentedwiththefollowingintroductiontolearn
promptofcreatingsuchexamplesisexhibitedas howtousethesliderusedforratingasdepictedin
showninFigure5. Figure7.
A.4 Experiment1: SPSampling
A.4.1 HumanExperiment
A.3.1 Considerationforhumaninstructions
andGPTprompts Thissectiondescribestheimplementationaldetails
of SP Sampling (see Section 3) human and GPT
In pilot experiments, several participants create experiments.
sentences of a provided conversational tone T in Theparticipantsfirstgothroughthegeneralin-
theformof“<Subject>felt<T>”. However,such structions(seeFigure4)anddotwopracticetrials
a sentence may not necessarily convey conversa- tofamiliarizethemselveswiththetaskstheywill
tionaltoneT. Forexample,fortheconversational beperforming: (1)detectingaconversationaltone
tone“apologetic”,thesentence“Hefeltapologetic” from a sentence and (2) creating a sentence that
communicates that a “He” is apologetic, but not conveyssomeprovidedconversationaltone.
thatthespeakerisapologetic,whichcontradictsthe In the main experiment, each participant does
definitionofconversationaltoneas“thespeaker’s tentrials. ThehumaninterfaceisshowninFigure
attitudeinaconversation”. So,asanefforttopre- 6
ventparticipantsfromprovidingsuchresponses,an To avoid low-quality sentences, we automati-
instructionaslistedbelowispresentedasinFigure callycheckthesubmittedsentenceforthefollow-
4c. ing criteria: (1) The sentence has to have more(a)TheinterfaceofaStrial.
(b)TheinterfaceofaTtrial.
Figure6: TheinterfacesofaStrialandaTtrialinourSamplingwithPeoplehumanexperiment.
Figure7: Thispageoftheexperimentdetailstheinstructionsforusingourdesignedscaleinterfaceforrating.ResponseType ValidationCriterion Implementation
Sentence Musthavemorethan5words RegEx
Sentence Mustbeagrammaticallycorrectsentence GingerIt
Tone Theresponsecanonlycontainalphabetsandhyphens RegEx
Tone Theresponsemustbeanadjective PyDictionary
Tone Theresponsemustbecorrectlyspelled PyDictionary
Both Cannotcontainanystemmedvariationofprompt nltk
Both Cannotcontainprofanity profanity-check
Table2: SummaryofSamplingwithPeopleresponsefilters
thanfivewords;(2)Thesentenceisgrammatically participants. Thestrengthofconversationaltones
correct (checked using gingerit 4) (3) The sen- inasentenceisratedonaLikertscalefrom1to5,
tence does not contain any stemmed variation of with1beingtheweakestand5beingthestrongest.
somewordthatisalreadyintheprovidedconversa- Thefinalratingofsuchatone-sentencepairwould
tional tone (for example “politely” for “polite”) betheaverageofall5ratings. Anexampleofthe
using nltk5; (4) Does not contain offensive or ratinginterfaceisprovidedinFigure9.
vulgar words checked with the Python package
profanity-check6.
A.5.2 GPT-4Experiments.
Theconversationaltoneswereverifiedinasim-
GPTreceivesthepromptforquality-of-fitratingas
ilar fashion: (1) The tone response did not con-
outlinedinFigure10.
tain any stemmed variation of some word that is
alreadyintheprovidedconversationaltoneorsen-
tence,(2)Theresponsemustbeanadjectiveusing A.6 Experiment3: conversationaltone
PyDictionary7, and (3) It must not contain pro- SimilarityJudgment
fanity. SeeTable2forasummary.
A.6.1 HumanExperiment
In the human experiment, we elicited 90 sam-
plingchains,eachwith100iterations. Participants After reading the general (see Figure 4) and
cannotrevisitthesamechain. experiment-specificinstructions(seeFigure11a),
participants proceed to the main experiment in
A.4.2 GPT-4Experiments
which they judge the similarity of a pair of two
TheGPTprompts(seeFigure8)werenearlyidenti- conversationaltones.
caltothehumanexperiments. IntheGPTinstance, Eachparticipantwouldratethesimilarityofsev-
wesimilarlyelicited90samplingchains,eachwith eraldistinctpairsofconversationaltonesonaLik-
100iterations. ertscaleof1to5,where1represents“Semantically
dissimilarconversationaltones”and5represents
A.5 Experiment2: SPQuality-of-fitRating
“Semanticallysimilarconversationaltones”. Each
A.5.1 HumanExperiment pair of conversational tones would be rated five
Afterreadingthegeneralinstruction(seeFigure4), times,withtheaveragescoreofthose5ratingsas
participantsfirstdotwopracticetrialstofamiliarize thefinalsimilarityscoreforsuchpair. Onlydistinct
themselveswiththetasktheywillbeperforming: pairsarerated. Thatmeansthesimilarityjudgment
rating the strength of a conversational tone when of40conversationaltonesconcernsonlythe820
given a sentence. Then, participants proceed to distinct pairs among all possible tuples of tones.
themainexperiment. Participantscanonlyratea And,similartopreviousexperiments,participants
conversationaltone-sentenceonce,andeachtone- cannotrateanyconversationaltonepairmorethan
sentencepairisratedbyapproximately5distinct once. Anexampleoftheratinginterfacefollowsin
Figure11b.
4gingerit: https://github.com/Azd325/gingerit/
blob/main/gingerit/gingerit.py
5https://www.nltk.org/ A.6.2 GPTExperiment
6profanity-check: https://pypi.org/project/
profanity-check/ The prompt for similarity judgment that GPT re-
7https://pypi.org/project/PyDictionary/ ceivesisasoutlinedin12.A conversational tone is the style and manner in which someone speaks.
Provide an adjective for conversational tone in English that you sense in the following sentence:
<sentence of previous response>. Respond using only an adjective.
(a)GPTPromptforsamplingaconversationaltonegivenasentence.
A conversational tone is is the style and manner in which someone speaks.
Provide one sentence with at least five words in English that has the conversational tone:
<conversational tone of previous response>.
(b)GPTPromptforsamplingasentencegivenaconversationaltone.
Figure8: CollectionofGPTPromptsforGPTparticipantinsamplingandratingaspectsofSPparadigm.
Figure9: Exampleinterfaceforquality-of-fitratinghumanexperimentinterface.
A conversational tone is the style and manner in which someone speaks.
On a scale of 1 to 5, with 5 being strongest, how strong is the provided conversational tone in
the following English sentence?
Tone: <conversational tone to rate>
Sentence: <sentence to rate>
Respond with only a number.
(a)GPTPromptforratingthestrengthofaconversationaltoneonasentence.
Figure10: GPTPromptsusedforquality-of-fitratingexperiment.(a)Interfacethatintroduceshumanparticipantstosimilarityjudgmentexperiment.
(b)Interfacethatrepresentsthemainbodyofsimilarityjudgmenthumanexperiment.
People described conversational tones using words.
A conversational tone is the style and manner in which someone speaks,
and sometimes, it is also referred to as the tone of a sentence.
How similar are the conversational tones in each pair on a scale of 0-1
where 0 is completely dissimilar and 1 is completely similar?
conversational tone 1: {tone_a}
conversational tone 2: {tone_b}
Respond only with the numerical similarity rating.
Figure12: GPTPromptforratingthesimilarityofconversationaltones.
A conversational tone is the style and manner in which someone speaks.
{explanation of rated tone feature using its definition in Table}
On a scale of 1 to 5, where 5 means strongest and 1 means weakest, how {feature} is the conversational tone
''{tone}''?
Respond with only a number.
Figure13: GPTPromptforratingthestrengthoffeaturesinconversationaltones.
Figure14: Interfaceforthemainbodyoftonefeatureratinghumanexperiment.A.7 Experiment4: conversationaltone OverN =5000bootstrapprocessesforboththe
FeatureRating humanandGPTinstances,wemeasurethehuman
conversationaltonedistributionsplit-halfcorrela-
A.7.1 HumanExperiment
tionstober=0.91[0.87,0.93],andtheGPTcon-
Afterreadingthegeneralinstructions(seeFigure
versationaltonedistributionsplit-halfcorrelations
4),humanparticipantsproceedtothemainexperi-
tober=0.87[0.73,0.94].
mentinwhichtheyrateafeatureofconversational
Semantic Interpretation of Sentence Space
tones. Based on psycholinguistic literature (Yeo-
Figure 16 shows the joint-embedding space via
mans et al., 2022; Fintel, 2006; Jin et al., 2022;
UMAP (McInnes et al., 2020) for sentences en-
Oetzeletal.,2001;Portner,2009)weselectedthe
codedusing distilbert-base-uncasedembed-
followingfeaturesalongwiththeirdefinitions:
dings (Sanh et al., 2019) from both humans and
GPT.Consistentwithourfindingsregardingtones
• positiveinvalence: Positivenessinvalence
in the Result section, the distribution was much
meansthepositivenessofemotionalvalence.
moreconcentrated(entropyof5.05bits[5.03,5.07]
• aroused: Arousedmeanstheamountofemo- viabootstrapping)comparedwithhumans(entropy
tionalarousalobserved. of4.12bits[4.10,4.15]). Figure16alsoshowsdif-
ferenttopicsindifferentpartsofthespace,which
• Informational: Informationalmeanstheex- alsoshowsdifferencesintheproducedsentences
tent to which a speaker’s motive focuses on forhumansandGPT.Fromthisfigure,weobserve
givingand/orreceivingaccurateinformation. high repetition of sentence literal content across
many GPT-occupied locations of the shared sen-
• Relational: Relational means the extent to
tenceembeddingspace(e.g.,excitedtogotoDis-
whichaspeaker’smotivefocusesonbuilding
neyland),whileinregionsdominatedbyhumans,
therelationship.
weusuallyobserveahighervarianceofwordsused.
ThehighlightedregionsinFigure16showthesen-
Thestrengthoffeaturesinaconversationaltoneis
tence space shows dense semantic topics, (e.g.,
ratedonaLikertscalefrom1to5,with1beingthe
“gratefulness”incircle(ii)).
weakestand5beingthestrongest. Participantsare
providedaninterfaceforratingthefeaturesincon-
B.2 Quality-of-fitRatingExperiment
versational tones. See Figure 14 for a screenshot
Sample Reliability of Correlation Matrices. We
ofthetask.
measure the sample reliability of human percep-
A.7.2 GPTExperiment tion’sandGPTperception’scorrelationmatrixus-
ingthefollowingprocedure. First,forasetofgath-
The prompt for tone feature rating that GPT re-
ered quality-of-fit ratings, we randomly partition
ceivesisasoutlinedin13.
suchdatasetbysentences. Then,withineachparti-
B SupplementaryStatisticalAnalyses tion,weproduceacorrelationmatrix. Finally,we
computethecorrelationofthesematrices(treated
B.1 SPSampling
as vectors). We used 5000 bootstrapped dataset.
Sample Reliability. For testing the reliability of Forhumansratings,wefindthiscorrelationtober
our elicited conversational tone distribution, for =0.95[0.92, 0.96]; forGPTratings, wefindthis
both human and GPT instances, we measure the correlationtober =0.90[0.84,0.93]. Thecross-
split-half correlation of their conversational tone domainmatrixitselfhasahalfsplitcorrelationofr
distributionsfromtheacquireddatasetoftheirSP =0.95[0.92,0.96].
instances. Thissplit-halfcorrelationiscomputed SampleReliabilityofSimilarityJudgmentEx-
alongthefollowingprocedure. First,werandomly periment. First,forasetofgatheredquality-of-fit
partitionasetofSP-gathereddataintotwohalves. ratings,wepartitionedtheratingsofeachconver-
Then,wefindthefrequencyofeachconversational sational tone into two halves and computed the
tonewithinthedataset’shalves. Atlast,wecom- quality-of-fit rating correlation matrix from each
putethecorrelationbetweenthefrequencyofcon- halfofthequality-of-fitratingdata. Wethencom-
versationaltonestobethesplit-halfcorrelationof putethecorrelationbetweenthesesimilaritymatri-
conversational tone distribution within an SP in- ces. Weused5000bootstrapssamplesandfound
stance. thesplit-halfcorrelationofhuman’ssimilarityma-Figure15: Thedynamicsofjoint-embeddingspaceforsentencesandconversationaltonesthroughoutaselection
ofiterations. Embeddingsareproducedviafirstobtainingsentence-embeddingsortoneword-embeddingsusing
distilbert-base-uncased(Sanhetal.,2019)pretrainedweights,andthenprojectedontoa2-dimensionalspace
usingUMAPmanifoldembedding(McInnesetal.,2020). A:Thetimeevolutionofjoint-embeddingsentencespace.
B:Thetimeevolutionofjoint-embeddingtonespace.
trix to be r = 0.72 CI = [0.81, 0.84], and that for puting the arrowmark for humans’ feature rating,
GPT’ssimilaritymatrixtober=0.987CI=[0.978, weonlyfitthefeatureratingstothehumans’con-
0.983]. versationaltoneMDSsolution. GPT’sarrowmark
dimensionswasonlyfittedtoGPT’sfeatureratings
B.3 Cross-Domain(Cross-Correlation)
too.
Analysis
CosineSimilarityofFeatures. Asperformedin
Computation of Feature Arrowmarks. We used Section4.3,webootstrapoverthecosinesimilarity
linearregressiontoregressthetoneratingforeach of these feature vectors over different MDS solu-
of the four theoretic dimensions using a projec- tions,andfindthatwhilethefeature“informational”
tiontechniqueandtheresponsesofthisexperiment isconsistentlyalignedwithhighcosinesimilarity
(MDS biplot (Greenacre, 2010) treating the tone inarrowmarkdirectionacrossbothgroups(mean
feature ratings as biplot arrows as shown in Fig- 0.98CI=[0.97,0.99]),thefeature“relational”is
ure 3A. The concrete computation process is as notsostronglyaligned(mean0.6CI=[0.57,0.87]).
follows. First, we construct a feature rating vec- Furthermore,thefeatures“positiveinvalence”and
torf⃗ foreachvector. Then, wefitthesefeatures “aroused”bothobservenegativecosinesimilarityin
i
andtheMDSembeddingxcoordinates(⃗x)using directions(respectively,mean-0.69CI=[-0.71,-
linearregression,arrivingatsomeregressionline: 0.64];mean-0.65CI=[-0.69,-0.4]). Thissuggests
⃗xˆ = (cid:80) α f⃗ The coefficient α is then taken to a deviation between the human and GPT under-
i i i i
be the x-axis direction of feature i’s arrowmark. standingofthesefeatures.
Thesameprocedurewasperformedtocomputethe ExplainedVarianceofFeatures. Additionally,
arrowmarks’y-axisdirection. Notethatwhencom- weinvestigatethesignificanceofeachfeaturevec-Figure16: Sentenceembedding. Wordcloudsshowthefrequencyofwords(rightandleftinsets)incorresponding
circlesonthesentenceUMAPembeddingspace(center). Redpointsresembleeachsentencesampledfromhuman
instances,andbluepointsresembleGPTinstancesentences. Brighterredandbluehuesindicaterespectivelyhigh
TF-IDF(Luhn,1958)scoresinhuman,GPTsentencesineachwordcloud(i.e.,brightpurplewordsarehighly
frequentacrosshumansandGPT).
torbycomputingits“explainedvariance”within tation (Grave et al., 2018; Conneau et al., 2017;
thesharedembeddingspace. Wecomputetheex- Lample et al., 2017) using 500 iterations for its
plained variance of a feature vector is computed convexinitiation,andalearningrateof10,batch
as the variance of scalar projections of all MDS sizeof10,regularizationcoefficientof0.5,with15
toneembeddingsontothatfeaturevector. ForGPT epochsforitsstochasticiterationinGWOTproce-
toneembeddings,theorderofconversationaltone dure.
featuresfromhighesttolowestexplainedvariance Bilingual Lexicon Induction via Latent Vari-
is“positiveinvalence”(mean0.873CI=[0.872, able Model. For this method, we adopted Ruder
0.878]),“relational”(mean0.45CI=[0.42,0.76]), etal.’simplementation(Ruderetal.,2018;Artetxe
“aroused”(mean0.17CI=[0.16,0.24]),then“in- etal.,2016,2017,2018a,b). Duringlexiconinduc-
formational”(mean0.126CI=[0.12,0.127]). For tion,weusedabackwarddirection,considering5
humantoneembeddings,theorderoffeaturesfrom nearestneighborsintranslationretrieval. Wedid
highesttolowestexplainedvarianceisinstead“pos- notuseaseeddictionary. Wealsomadesmallmod-
itive in valence” (mean 0.71 CI = [0.69, 0.87]), ificationstoRuderetal.’simplementation(model
“aroused”(mean0.54CI=[0.5,0.79]),“relational” training batch size from 1000 to 5) to adapt the
(mean0.37CI=[0.34,0.76]),followedby“infor- paradigmtowardsoursmallersetofembeddings.
mational” (mean 0.3 CI = [0.27, 0.79]). In both
spaces,wefind“positiveinvalence”tobeadomi- D DeclarationofGenerativeAIand
nantdimensionofconversationaltoneembeddings, AI-AssistedTechnologiesinthe
whilehumansandGPTdonotfullyagreeuponthe WritingProcess
dominanceofotherdirections.
Duringthepreparationofthiswork,wesometimes
C HyperparametersinAlignment used GPT for edits. After using this tool, the au-
Paradigms thorsreviewedandsignificantlyeditedthecontent
asneededandtookfullresponsibilityforthecon-
Gromov-WassersteinOptimalTransport(GWOT). tentofthepublication. Additionally,weusedword-
For GWOT, we adopted Grave et al.’s implemen- tune https://www.wordtune.com/ and Gram-PerformanceCategory k Procrustes GWOT BLI
DomainSimilarity N/A 0.625[0.607,0.642] 0.625[0.607,0.642] 0.8[0.8,0.8]
Preservation(Human)
DomainSimilarity N/A 0.499[0.473,0.513] 0.454[0.405,0.519] 0.82[0.82,0.82]
Preservation(GPT)
kNNMatchingRate 1 0.392[0.325,0.463] 0.339[0.293,0.388] 0.657[0.638,0.675]
w.r.t. CCAlignment 2 0.41[0.363,0.460] 0.354[0.288,0.397] 0.635[0.606,0.65]
3 0.456[0.408,0.496] 0.403[0.350,0.461] 0.653[0.635,0.667]
4 0.499[0.451,0.541] 0.461[0.397,0.520] 0.705[0.686,0.719]
5 0.531[0.476,0.568] 0.503[0.422,0.563] 0.730[0.709,0.745]
Table3: Tableofbenchmarkingresultsonproposedmetricsforunsupervisedcross-domainalignmentmethods.
Procrustes: OrthogonalProcrustesTransformation. GWOT:Gromov-WassersteinOptimalTransport(Graveetal.,
2018). BLI:BilingualLexiconInductionviaLatentVariableModel(Ruderetal.,2018). Resultsareaggregated
across100seedsforstochasticmethods.
marly(https://www.grammarly.com/)tocheck
syntaxandproofreadthedocument. Writingtheex-
perimentalcodeweusedcode-suggestionsbyMi-
crosoft Copilot ((https://copilot.microsoft.
com/). Wereviewedallsuggestionstomakesure
theyreflectedourintentions.
E EnlargedFiguresfromMainPaper
Inthissection,weattachenlargedFigures2,3as
Figure17,18fromthemainpaperforreadability.lacitpeks lacitpeks
citsacras citsacras
niatrecnu niatrecnu
desufnoc desufnoc
detatirri detatirri
deyonna deyonna
detartsurf detartsurf
yrgna yrgna
tnangidni tnangidni
evisnefed evisnefed
detnioppasid detnioppasid
das das
dengiser dengiser
citegolopa citegolopa
tnegru tnegru
etarepsed etarepsed
deirrow deirrow
suoixna suoixna
denrecnoc denrecnoc
citehtapmys citehtapmys
ciglatson ciglatson
desirprus desirprus dekcohs dekcohs
lufknaht lufknaht
lufetarg lufetarg
evitaicerppa evitaicerppa lufyoj lufyoj
yppah yppah
desaelp desaelp
deticxe deticxe yldneirf yldneirf
etanoitceffa etanoitceffa
duorp duorp
gnirimda gnirimda
gnigaruocne gnigaruocne
gnirussaer gnirussaer
deveiler deveiler suoiruc suoiruc
evitcelfer evitcelfer
evitpircsed evitpircsed
lacitpeks lacitpeks
citsacras citsacras
niatrecnu niatrecnu
desufnoc desufnoc
detatirri detatirri deyonna deyonna
detartsurf detartsurf
yrgna yrgna
tnangidni tnangidni
evisnefed evisnefed
detnioppasid detnioppasid
das das
dengiser dengiser
citegolopa citegolopa
tnegru tnegru
etarepsed etarepsed deirrow deirrow
suoixna suoixna
denrecnoc denrecnoc citehtapmys citehtapmys
ciglatson ciglatson desirprus desirprus
dekcohs dekcohs
lufknaht lufknaht lufetarg lufetarg
evitaicerppa evitaicerppa
lufyoj lufyoj
yppah yppah
desaelp desaelp
deticxe deticxe
yldneirf yldneirf
etanoitceffa etanoitceffa
duorp duorp gnirimda gnirimda gnigaruocne gnigaruocne
gnirussaer gnirussaer
deveiler deveiler
suoiruc suoiruc
evitcelfer evitcelfer evitpircsed evitpircsed
evitpl ia rg cc cn si it ti eg s p daa eec cr kvi ut r sie aotcg sceo nll f eo ep rd auorp
dengiser
ddn tddl yca nri i ee eec t a
g
as ys tti t nnaat a ourp gae t rc f ni te nc ir sr
n
drk a on ui
a
ns s cu
r if
gnirussaer evisnefed
lufknaht detnioppasid
lufetarg das
c ei vt ie t dah d eis t ec sau ke ip ro cr pm pi d or rpu h uayc as s s
s
ecd sdt dn uti ee et ae oen i nrrg igg rre xr eooi pu ns cwl s ae o ner p oda
c
gnirimda citehtapmys evisnefed ciglatson
tnangidni desirprus
etanoitceffa dekcohs detndc ioei dg pse tl n ua pv e ft ae ns g sio ol re idn u cr dll yle uuu pv efff ie pk y st oan at aa ia ej hcrh lg e pt rppa
yldneirf deticxe suoixna yldneirf
denrecnoc etanoitceffa
niatrecnu duorp
ed te ad d t rae e ey rt i ptr a r sg r st o ui en r w r dra fi edg sgg u vnn n e ioi vii trgr ce ii u ram ei us lr e lcsd u fr eaa o rec rne
deyonna evitpircsed deticxe yppah desaelp lufyoj
Figure17: EnlargedversionofFigure2
TPG
dna
snamuh
ni
sgnitar
tif-fo-ytilauq
no
xirtam
noitalerroC
B TPG
dna
snamuh
ni smret
detceleS
A
e e suv vi i ot tp c irei ur lc cfes red
ld ue feti tc ax re g
deveiler
denrecnoc
gnirussaer
detartsurf
gnigaruocne
detatirri
gnirimda
citegolopa
d e y d d yl ptu e eda pt so nn i ac ar eop ex hii r let pfceffa
eeecg vttin g aa ii tlg nra aea ot ip csr itu s eo ceo rn e pdc f pfn a ae
lufyoj
yppah
evitaicerppa
deveiler
lufetarg
yrgna
lufknaht
suoiruc
d d c c d s d eui i te e e eg t aoek s n il r rac i h ir r er xe oto tp pns a c wh r sao p nus en m os dcys
eledcdg u vvin ee t f i i ks ssi tsr ca nuii nrm ec ap f en lr hd r f fa ou e eta s cs d r
tnegru
tnegru
citegolopa
gnirussaer
dengiser
suoixna
d d e t yn rva e gais t s n nn n g aio e idp fe np d iasid
dy del ve ed it stn n p ae i eio rir lcp pf sp ea dsid
detartsurf
deyonna
deyonna
deirrow
detatirri
lacitpeks
desufnoc
das
n c laii t ca s it tar pe c ec r kan ssu
dln ui efa y nt or ge j isc en ru tnangidni dekcohs duorp citehtapmys
%52
%02
%51
%01
%5
%0
xirtam
niamod-ssorC
C 8.0
noitalerroC
noitalerroc-ssorC
6.0
1
0
1
1-
4.0 2.0 0.0
TPG
dna
snamuh
ni
senot
lanoitasrevoc
fo
stnemegduj
ytiralimiS
D d e t ye e s d g g g d e y d dd y l e l l d d c c d s d e t c du u u nn ru l p u a vv v t v i i t in n n ee u e e e e e e ed g t tf f f ga a ae s ii i io op y e k e ek tv i i i t s s n i no st t tn lr g r rn r n ng ni ap c o a n ci e a t i h i grc a grru i e nr r xea m o a g ar ie op te i oi ae ji tu x p oh os ip u ec nlr i sr r sai ic we rl hle hc c s t dd lr pu g fsf e ap of eopc nur ee a t ss nao e pr rn pe m os dr pe e d ic af acrd pf yn sa a s ie d
se e d g g g e y d d y l e l l d d c c d s d e t c d d d e t yd u u u n nu l p u rv v t v i i t i vn n ne e e e e e e e a eu d g t tf f f ga a e ai i i io op y e k e ev i i i t s k s n i n s tot t t sn lr g r rn r g n ni nap c o a ni e a t i h i gcc a gru i r r e nr r xea m o a g ar iep ote i ai e ji o tu p ox h os ip u ec nlr i sr r sai ic we rl hlecc s t hd dlr pu g s ff e ap of eopc nur ee tas sa no e pr n rpe m os dr pe e d ic af acrd pf yn sa a s ie d
d d de e et y ta aor tn it rs n ru iarf
d d de e et y ta aor tn it rs n ru iarf
desufnoc
desufnoc
niatrecnu
niatrecnu
citsacras
citsacras
lacitpeks
lacitpeksg
gne
ln
ui
lv
g
ui
fre
gi kat
fuc
st
a
n
nerd
ec
a
si
u
ud
it
i
t
ave
c
ri
sne alc
o
ot
ed
a
i
hd
t
it
ce
e
ae
mo
sh
rdid
n
i
cvnd
e
ct
ted
d
griv
r
eg
nee
it s
e
ad
eg
es
nd
ui
ptt
i
de
a
si e
eo rd
tovt
ea
cat yd
y
nnu
il
e
ece ytn pna
pu ap
lap s
tp
yi
fe
c
lln
er
eop
ggoe tt ilk oui er r
ef
ati
t
m
crdi aca
ps
r
fe
rr ne e npr si
is
iic
g f
p
dk
far dsp
x
xc te nt
a yag
ap c
u
noor
a
yo
i
nr
sasen
enrs rl eo
e
ose rn
r
as csnf hhu
r
araue ife
l i
iw jc ie o
p r
dssdr fn d cu Robustness
duorpdas
ecnatsiD
d
oncerned worr aie nxio
n
ia
t r
eus
sad
citehtapmys ective
d e t a rt
s u drf
ed ye t on
i no p p
a
s
id
yrd ge
nc
tg
n
ai s ne
gc
ir
do
ed
v
ise nns
e
fep fera
ute
s skee
ptit cn alde
gc n
u
r
ucit e
g ol o hp
oa
ckec
durious
arousedinformational
d
e
si
unf
oo rr
am eatio
vr
ne alf il
pr
te ir ol
sel
a
sta
ii
oti
to
io pnn va al
l
e
descriptivec reig ala
st ss uon
re
i
ent na
gg
cn noo iri ut ic mrae adgff
pia
an pg rf eri ce ian tdl ivy
e
n an an id s duorp
d e t a ti rri citsacras surprised deveiler lg ur fa kt ne afu hl yt
ful
pl ae pa ps yed
jo h
e x
cite d
Figure18: EnlargedversionofFigure3
ecnatsid
naidilcuE
enot
lanoitasrevnoC
B
noitalerroc
ssorc fo
SDM
A
5.3 0.3 5.2 0.2 5.1 0.1 5.0 0.0
sgniddebme
enot
lanoitasrevnoc
fo
robhgien
tseraeN
C
lacitpeks
lacitpeks
lacitpeks
citsacras
citsacras
citsacras
niatrecnu
niatrecnu
niatrecnu
desufnoc
desufnoc
desufnoc
detatirri
detatirri
detatirri
1 deyonna
deyonna
deyonna
detartsurf
detartsurf
detartsurf
yrgna
yrgna
yrgna
tnangidni
tnangidni
tnangidni
evisnefed
evisnefed
evisnefed
detnioppasid
detnioppasid
detnioppasid
das
das
das
dengiser
dengiser
dengiser
citegolopa
citegolopa
citegolopa
tnegru
tnegru
tnegru
etarepsed
etarepsed
etarepsed
deirrow
deirrow
deirrow
suoixna
suoixna
suoixna
denrecnoc
denrecnoc
denrecnoc
citehtapmys
citehtapmys
citehtapmys
ciglatson
ciglatson
ciglatson
desirprus
desirprus
desirprus
dekcohs
dekcohs
dekcohs
lufknaht
lufknaht
lufknaht
lufetarg
lufetarg
lufetarg
evitaicerppa
evitaicerppa
evitaicerppa
lufyoj
lufyoj
lufyoj
yppah
yppah
yppah
desaelp
desaelp
desaelp
0 deticxe
deticxe
deticxe
yldneirf
yldneirf
yldneirf
etanoitceffa
etanoitceffa
etanoitceffa
duorp
duorp
duorp
gnirimda
gnirimda
gnirimda
gnigaruocne
gnigaruocne
gnigaruocne
gnirussaer
gnirussaer
gnirussaer
deveiler
deveiler
deveiler
suoiruc
suoiruc
suoiruc
evitcelfer
evitcelfer
evitcelfer
evitpircsed
evitpircsed
evitpircsed