VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval
JunjieZhou1*,ZhengLiu2,3†,ShitaoXiao2,BoZhao2,YongpingXiong1
1 StateKeyLaboratoryofNetworkingandSwitchingTechnology,
BeijingUniversityofPostsandTelecommunications
2 BeijingAcademyofArtificialIntelligence
3 TheHongKongPolytechnicUniversity
zhoujunjie@bupt.edu.cn zhengliu1026@gmail.com stxiao@baai.ac.cn
Abstract representedasembeddings,andtheirsemanticrela-
tionshipcanbereflectedbytheembeddingsimilar-
Multi-modal retrieval becomes increasingly
ity(Yatesetal.,2021). Withthecontinualprogress
popularinpractice. However,theexistingre-
on pre-trained model and training algorithm, in-
trieversaremostlytext-oriented,whichlackthe
creasinglypowerfulembeddingmodelshavebeen
capability to process visual information. De-
developed,suchasDPR(Karpukhinetal.,2020),
spitethepresenceofvision-languagemodels
likeCLIP,thecurrentmethodsareseverelylim- Contriever (Izacard et al., 2022), GTR (Ni et al.,
ited in representing the text-only and image- 2022), E5 (Wang et al., 2022), BGE (Xiao et al.,
only data. In this work, we present a new 2023),etc.,whichsubstantiallyimprovesthequal-
embeddingmodelVISTAforuniversalmulti- ityanduniversalityofdenseretrieval.
modalretrieval. Ourworkbringsforththree-
Mostoftheexistingdenseretrievalmodelsare
foldtechnicalcontributions. Firstly,weintro-
text-oriented, which can only deal with the data
duce a flexible architecture which extends a
presented in human language. However, a large
powerful text encoder with the image under-
standing capability by introducing visual to- portionoftheworldknowledgenaturallycontains
ken embeddings. Secondly, we develop two bothtextandimage,e.g.,webarticleswithvisual
data generation strategies, which bring high- illustration(Changetal.,2022);meanwhile,peo-
quality composed image-text to facilitate the ple’s queries can also be flexibly expressed with
training of the embedding model. Thirdly,
multipledatamodalities,e.g.,searchquerieswith
weintroduceamulti-stagetrainingalgorithm,
exemplarimages(Liuetal.,2021;Wuetal.,2021).
whichfirstalignsthevisualtokenembedding
Despitethedevelopmentofvisual-languagerepre-
with the text encoder using massive weakly
sentationmodels(VLM),likeCLIP(Radfordetal.,
labeled data, and then develops multi-modal
representation capability using the generated 2021)andALIGN(Jiaetal.,2021),theaboveprob-
composed image-text data. In our experi- lemisstillchallenginginmanyperspectives. On
ments,VISTAachievessuperiorperformances onehand,theexistingVLMsareseverelylimitedin
acrossavarietyofmulti-modalretrievaltasks textrepresentationcapability,whoseretrievalper-
inbothzero-shotandsupervisedsettings. Our
formanceisfarbehindtherecenttext-onlyembed-
model,data,andsourcecodeareavailableat
dingmodels,likeE5andBGE.Ontheotherhand,
https://github.com/FlagOpen/FlagEmbedding.
theexistingVLMsfocusmoreontheindependent
1 Introduction encodingoftextandimage;nevertheless,thejoint
representationofimage-textdata(e.g.,documents
Informationretrieval(IR)isacriticaltaskinmany withillustrations)islargelyunexplored.
real-world scenarios, e.g., search engines, open- In this work, we propose VISualized Text
domain question answering, and retrieval aug- embedding for universal multi-modal retrievAl,
mentedgeneration(Karpukhinetal.,2020;Lewis namely VISTA. It takes the best of the existing
etal.,2020a). Itaimstofindrelevantdatafroma textencoderandimageencoderwherehigh-quality
largedatabasesuchthatthedownstreamproblems multi-modalityembeddingcanbegeneratedfrom
can be faithfully solved on top of proper knowl- it. In particular, our work presents the following
edge. OneimportantIRparadigmisdenseretrieval, threetechnicalcontributions.
wherethequeryandcandidates,i.e. document,are First of all, we come up with a flexible model
architecture to facilitate the generation of multi-
*WorkdoneduringJunjie’sinternshipatBAAI.
†Correspondingauthor. modalembedding. Itisbuiltuponapowerfuland
4202
nuJ
6
]RI.sc[
1v29240.6042:viXrawell-trainedtextencoder,whichexhibitsproficient Transformer(ReimersandGurevych,2019),Ope-
textretrievalcapability. Meanwhile,itmakesthe nAI text embedding (Neelakantan et al., 2022),
incorporationofvisualtokensgeneratedbyanex- BGE (Xiao et al., 2023), and M3 (Chen et al.,
pressive image encoder, thereby augmenting the 2024), etc. These models have demonstrated im-
capabilityofimageprocessing. Suchanarchitec- pressivegeneralizabilityandrobustperformancein
ture brings forth two important advantages. 1) It therealmoftextretrieval. However, theyexhibit
establishes the in-depth fusion of text and image limitationswhenitcomestohandlingmulti-modal
data,whichsubstantiallycontributestothequality data. This becomes particularly salient with the
ofmulti-modalembedding. 2)Italsoenablesthe risingpopularityofmulti-modalretrieval(Chang
preservation of the original performance of text etal.,2022;Voetal.,2019;Luoetal.,2023)and
embedding,asthetextencoderisfullyfixedwhile multi-modalretrieval-augmentedgeneration(Chen
thevisualtokensareincorporated. etal.,2022;Yasunagaetal.,2023).
Secondly,weproposetwoinnovativepipelines
2.2 GeneralMulti-ModalEmbedding
for the automatic generation of Image-Text Com-
poseddatasets,therebysecuringlarge-scale,high- Multi-modal retrieval, characterized by queries
qualitydataforthetrainingofmulti-modalembed- and/orcandidatescomposedofimage-textdata,is
dingmodels. Thesepipelinesaredesignedtocater gainingincreasingpopularityinpractice(Voetal.,
toscenarioswhereeitherthequeryorthecandidate 2019;Changetal.,2022;Luoetal.,2023). Differ-
comprisesimage-textpairs,therebyfacilitatingthe entfromcross-modalityretrievalmodels(Radford
modeltoadapttoadiverserangeofmulti-modal et al., 2021) which independently process image
retrievalsituations. and text modalities, multi-modal retrieval neces-
Thirdly, we design a two-stage training algo- sitates models to have an in-depth understanding
rithmtolearnthemulti-modalembeddingmodel. of the composed image-text data. Most existing
Initially,weperformthebasictext-to-imagematch- modelsformulti-modalembeddingprimarilyrely
ingtaskwithmassiveweakly-labeledcross-modal onthepre-trainedCLIP(Radfordetal.,2021)or
data (Schuhmann et al., 2022), which aligns the BLIP(Lietal.,2022). Forinstance,modelssuch
visual token embedding with the text encoder. as UniVL-DR (Liu et al., 2022), Clip4Cir (Bal-
Subsequently,weperformcomposedtext&image drati et al., 2023), and UniIR (Wei et al., 2023)
matchingwithourgeneratedcomposedimage-text initiallyencodeimageandtextseparatelyusingthe
datasets,whichestablishesthemulti-modalrepre- correspondingencodersfromCLIPorBLIP.These
sentationcapabilityfortheembeddingmodel. modelsthenemployafusionstrategy,suchasscore
VISTAisempiricallyverifiedbycomprehensive fusion,tointegratefeaturesfrombothmodalities.
experiments. Particularly,itachievessuperiorper- However,thesemodelslackin-depthimage-text
formanceacrossvariousmulti-modalretrievaltasks fusion mechanisms (Wei et al., 2023; Liu et al.,
inbothzero-shotandsupervisedsettings. Without 2022)oraredesignedforspecifictasks(Liuetal.,
any task-specific optimization, VISTA is able to 2022;Saitoetal.,2023;Baldratietal.,2023),rather
outperformormatchtheleadingapproachinevery thanforabroadspectrumofmulti-modalembed-
downstreamevaluationscenario. Besides,VISTA’s dingapplications. Furthermore,thetextembedding
performancecanalsobesubstantiallyimprovedif capabilitiesofCLIPandBLIParenotonparwith
itiscontinuallyfine-tunedforcorrespondingtasks. recentgeneraltextembeddingmodels,whichcan
potentiallycompromisetheirperformanceintasks
2 RelatedWork thatinvolveprocessingtext-heavymulti-modaldoc-
uments (Chang et al., 2022; Luo et al., 2023). A
2.1 GeneralTextEmbedding
concurrentwork,Marvel(Zhouetal.,2023),lever-
Generaltextembeddingplaysanimportantrolein agespre-trainedtextembeddingmodelsasafoun-
variousapplicationssuchaswebsearch,question dation for encoding composed image-text docu-
answering (Karpukhin et al., 2020), and retrieval ments, facilitated by a visual plugin. However,
augmented generation for large language mod- Marvel is a task-specific model trained for multi-
els(Lewisetal.,2020b;Borgeaudetal.,2022;Shi modaldocumentretrieval(Changetal.,2022;Liu
etal.,2023). Inrecent,numerouseffectivegeneral et al., 2022), and it cannot be utilized as a gen-
text embedding models have been developed, in- eralmulti-modalembeddingmodeltohandleother
cludingContriever(Izacardetal.,2022),Sentence- tasks,suchascomposedimageretrieval.3 VISTAModel
Image Multimodal Text
Embeddings Embeddings Embeddings
3.1 ModelArchitecture
ThecoreideaofourVISTAistheuseoftheViT
encoderasanimagetokenizerforthetextencoder.
Pre-trained Text Encoder
This enables VISTA to encode a variety of data
types,includingimages,text,andcomposedimage-
... ...
textdata. AsshowninFigure1,wetreattheVision
Transformer(ViT)(Dosovitskiyetal.,2021)asan
ViTEncoder Text Tokenizer
imagetokenizerofthetextencoder,whichallows
the pre-trained text model to recognize image to-
kens while remaining frozen. The benefit of this
approachisthatitfacilitatesanin-depthfusionof
textandimagedata,whilethetextencoderretains
Image Multimodal data Text
itsrobusttextembeddingcapabilities.
Specifically, VISTA encodes text data directly
Figure1: ThemodelarchitectureofourVISTAmodel.
usingthepre-trainedtextencoder,asillustratedby We use the pre-trained language model as the foun-
thefollowingformula: dation,makingtheViTencodertransfertheImageto
recognizedtokensofthetextencoder.
e = Bert({t ,...,t }) (1)
t 0 m
Here, Bert represents the text encoder (Devlin
3.2 DataConstruction
et al., 2018) and is initialized with a pre-trained
general text embedding model. {t 0,...,t m} and Existing hybrid multi-modal datasets predomi-
e t denotethetextsequenceanditscorresponding nantlyrequirehumanannotation,suchaswriting
textembedding,respectively. Notably,weutilize queriesformulti-modaldocumentretrieval(Chang
thenormalizedhiddenstateofBert’sspecialtoken, et al., 2022), annotating semantic relations for
[CLS],astheoutputoftheembedding. Forimage composed image retrieval (Liu et al., 2021; Wu
data,theencodingprocessisdefinedasfollows: et al., 2021), and creating questions and answers
forknowledgeretrieval(Luoetal.,2023). These
{ϵ ,...,ϵ } = ViT({i ,...,i })
0 n 0 n
(2) costlyhumanannotationslimitthescaleofhybrid
e = Bert({ϵ ,...,ϵ })
i 0 n multi-modaldatasets,posingchallengesfortrain-
where ViT is a vision transformer serving as an ing multi-modal embedding models. To address
imagetokenizer,{i ,...,i }isthetokensequence thesechallenges,wehavedesignedtwopipelinesto
0 n
oftheinputimagepatches,while{ϵ ,...,ϵ }corre- generatehybridmulti-modaldata. Thesepipelines,
0 n
spondstothesequenceofhiddenstatesforimage based on the scenarios where either the query or
tokens, as produced by ViT. The image token thecandidateiscomposedofimageandtext,pro-
sequence {ϵ ,...,ϵ } is then encoded by Bert to videaversatiletrainingdatasetthatcanaccommo-
0 n
derivethecorrespondingimageembeddinge . For datediversemulti-modalretrievalsituations. Our
i
thecomposedimage-textdata,weencodeitas: pipelines facilitate the production of two large-
scale multi-modal embedding training datasets.
{ϵ ,...,ϵ } = ViT({i ,...,i })
0 n 0 n
(3) Thestatisticalinformationofourgenerateddataset
e = Bert({ϵ ,...,ϵ }; {t ,...,t })
h 0 n 0 m ispresentedinTable1.
We concatenate the sequence {ϵ ,...,ϵ } and
0 n
3.2.1 Image&TextToImage(IT2I)Dataset
{t ,...,t } together, forming an interleaved se-
0 m
quenceofimageandtexttokens. Thisinterleaved InspiredbyInstructPix2Pix(Brooksetal.,2023),
sequenceisthenencodedbyBerttoyieldthehy- which devises a synthetic image-editing dataset
bridmulti-modaldatarepresentatione . forimageeditingmodels,weestablishapipeline
h
WeexclusivelytrainedViT duringthetraining forcreatingadatasetthatischaracterizedbycom-
procedurewhilemaintainingthetextencoderBert posedimage-textqueries. AsshowninFigure2,we
in a frozen state. This strategy is adopted to pre- feedthecaptionofthesourceimageC intoGPT-
s
servethepowerfultextembeddingcapabilitiesof 3.5(Ouyangetal.,2022),promptingittogenerate
thepre-trainedtextgeneralembeddingmodel. multipledistincteditinginstructions{T1,...,Tm}
à
à
à
à
à
à
à
à
à
à
à
à
à
à
à
à
à
à
à
àMan On A Balcony Metal
Youareacreativeimageeditor.Youneedtomakevarious Print by Gustave Caillebotte
creativemodificationstoanimage...
Thecaptionofthisimageis:
Input image caption from
InstructPix2Pixdataset.
(1)Generate10differenteditinginstructionsbasedon...
(2)Generate10newimagecaptionscorrespondingto...
SourceImage
Add a flock of birds behind the man CLIP
Filter
GPT3.5 Editing Instructions
MMana nO On nA A B Balaclocnoyn yW Witihth two
MFPaaolnlti ntOgen d FA Plo lawBneatlrc MoPneeytt aWalls iP tMrhin eAtt abl y
FPlrGoiucnsktt Obayvf e GB Cuirasditllsae vbMeo eCtttaaeilll ePbrionttt
by Gustave Caillebotte
Stable Our 370KSelected
New Image Captions Diffusion Edited Images Edited Image Triples
with hard negatives
Figure2: TheconstructionpipelineofImage&TextToImage(IT2T)dataset.
Dataset Queries H.Annot. other. Therefore,itpreventsthetrainingtaskfrom
CIRR(Liuetal.,2021) 36K ✓✓✓ collapsingintoanaiveimage-to-imagematching
FashionIQ(Wuetal.,2021) 30K ✓✓✓ task,whichenablesthemodeltojointlyunderstand
OurIT2IData 307K ×××
theimageandtextdata.
WebQA(Changetal.,2022) 21K⋆ ✓✓✓
OurT2ITData 213K ××× 3.2.2 TextToImage&Text(T2IT)Dataset
Table 1: Comparison of our generated datasets with Weestablishanotherpipelinetoconstructapseudo
existingdatasets. Queriesreferstothenumberofquery- multi-modaldocumentretrievaldataset,inwhich
candidate pairs. H. Annot. denotes the necessity of the candidates are composed of both images and
humanannotation. ⋆Thenumberofqueriescorrespond- text. Ourpipelineoperatesonahighlydescriptive
ingtomulti-modaldocumentsinWebQA.
imagecaptioningdatasetShareGPT4V(Chenetal.,
2023). ShareGPT4V is characterized by the de-
tailedtextualimagedescriptionthatincludesmulti-
along with their corresponding image captions granularinformation,encompassingworldknowl-
{C t1,...,C tm}, which are then fed into the stable edge, properties of objects, spatial relationships,
diffusion model (Rombach et al., 2022) to gen- etc.
erate the edited images {I1,...,Im}. We desig-
t t Specifically,foreachimageI accompaniedby
natedifferenteditedimagesIioriginatingfromthe
t adescriptivecaptionC,wefirstinputC intoGPT-
same source image I as hard negatives for each
s 3.5 and prompt it to generate an article T that is
other. Consequently, we obtain multiple triples
relatedtoasubtopicoftheimage. Consequently,
(I ,Ti,Ii), where I and Ti are the composed
s t s weobtainamulti-modaldocumentcandidate,de-
image-text query, and Ii is the target image. We
t notedasD = (I,T). WethenpromptGPT-3.5to
furtheremployCLIP(Radfordetal.,2021)tofil-
generateaqueryQforthegeneratedmulti-modal
terthesetriples,resultingin307Kquery-candidate
documentD. Throughthisprocess,weobtainover
pairswithhardnegatives.
213K triples (Q,I,T), where Q represents the
Amajordistinctionisthatourapproachgener- queryand(I,T)formsthemulti-modaldocument
ates multiple editing instructions for each source candidate. Wedemonstratethatthedatagenerated
image,whileInstructPix2Pixprovidesonlyasin- by this simple yet effective pipeline exhibits su-
gleeditinginstructionpersourceimage. Different periorgeneralizationcapabilitiescomparedtothe
editedimagescanworkashardnegativeswitheach manuallyannotatedWebQA(Changetal.,2022)
InstructPix2Pix
Datasetwhenusedtotrainmulti-modalembeddingmodels, where q and c represent the embeddings of the
as detailed in Section 4.3.1. More details of the queryandcandidateofthesetwotasks,respectively.
datagenerationprocessareshowninAppendixA. Wediscoverthata600-steptrainingprocessonour
generatedmulti-modaltrainingdatasetissufficient
3.3 Two-StageTraining
to equip VISTA with robust multi-modal embed-
Wedevelopatwo-stagetrainingstrategytofacili- ding capabilities. This not only underscores the
tatethetextencoder’sabilitytoencodebothimage effectivenessofourmodelarchitecturebutalsoval-
andhybridmulti-modaldataintoaunifiedembed- idatestheutilityofourgeneratedcomposedimage-
ding space. We initialize the text encoder with a texttrainingdatasets. Formoredetailsontraining
general embedding model BGE-Base-v1.5 (Xiao andthehyper-parametersettingsused,pleaserefer
et al., 2023) and initialize the ViT Encoder with toAppendixB.
EVA-CLIP-02-Base(Sunetal.,2023).
Stage1: Cross-ModalTraining. Inthefirsttrain- 4 ExperimentalResults
ingstage,weconductcontrastivelanguage-image
Wecarryoutbothzero-shotevaluationsandsuper-
pre-training (Radford et al., 2021) to our VISTA.
visedfine-tuningacrossvariousbenchmarkstosub-
Alltrainingdataareuni-modalinthisstage,andwe
stantiatetheefficacyandversatilityofourVISTA
utilizetheLaion-2B(Schuhmannetal.,2022)for
model. Inaddition,weperformcomprehensiveab-
in-depthalignmenttraining,therebytransforming
lation studies to scrutinize both the design of the
theViTencoderintoahigh-qualityimagetokenizer
VISTAmodelandtheeffectivenessofourstage-2
forthegeneraltextembeddingmodel. Thetraining
trainingdatasets.
objectivesareasfollows:
4.1 Zero-ShotRetrievalPerformance
min L = L (e ,e )+L (e ,e ) (4)
s1 con t i con i t
{θI} Benchmarks. We collect five distinct datasets,
encompassingfourdifferentmulti-modalretrieval
where θ is the parameters of ViT. L (e ,e )
I con t i tasks. To construct a challenging zero-shot eval-
andL (e ,e )arebidirectionalcross-modalcon-
con i t uation setup, we perform our evaluation on the
trastivelearninglosses,andL (u,v)canbefor-
con entire corpus of each dataset. The overall statis-
mulatedas:
tical information is shown in Table 2, while the
L (u,v) = −
1 (cid:88) exp(uT
i
v i/τ)
(5)
detailed information for each benchmark can be
con |B| (cid:80) exp(uTv /τ) foundinAppendixC.
i∈B j∈B i j
Metrics. We uniformly employ Recall@5 as the
whereBrepresentsthesetofin-batchsamples,and evaluation metric across all datasets. We employ
τ is the temperature parameter that controls the thedenseretrievalapproachtoevaluateallmodels
strengthofpenaltiesonnegativesamples. Follow- across various dataset benchmarks. Each model
ingthefirststageoftraining,theimagetokenizer encodesthequeryandcandidateitemsintocorre-
develops the ability to encode image tokens in a sponding embedding spaces, and retrieval is per-
formatthatthetextencodercaninterpret. formed based on cosine similarity scores using
Stage 2: Multi-Modal Training. After the first FAISS(Johnsonetal.,2019).
stage of training, the text encoder has gained the Baselines. We benchmark our VISTA model
ability to independently process image and text against three established baseline models: CLIP-
modalities, and align them into a unified embed- B1 (Radfordetal.,2021),BLIP-B(Lietal.,2022),
dingspace. Thishaslaidthegroundworkforencod- and Pic2Word (Saito et al., 2023). We utilize
inginterleavedsequencesoftextandimage. Build- the strategies outlined in (Wei et al., 2023; Liu
inguponthisfoundation,wefurthertrainVISTA etal.,2022;Saitoetal.,2023)toencodecomposed
toenhanceitsmulti-modalencodingcapabilities. image-textdataforthesebaselinemodels. Further
Specifically, weutilizeourgeneratedIT2Iand detailscanbefoundinAppendixD.Furthermore,
T2IT datasets, as constructed in Section 3.2, for tovalidatetheuniversalityofourgeneratedcom-
multi-modal training. The training objective of posedimage-textdataset,weapplythemulti-modal
boththetwotaskscanbeformulatedas: training(Section3.3)toallbaselinemodels. These
baselinemodelsX aredenotedasX-MM.
min L = L (q,c) (6)
s2 con 1https://huggingface.co/openai/clip-vit-base-patch16
{θI}Dataset Task QueryCount CorpusSize Domain
WebQA(Changetal.,2022) q → c /c 4,966 944,766 Wikipedia
t t it
CIRR(Voetal.,2019) q → c 4,181 21,551 Open-Domain
it i
FashionIQ(Wuetal.,2021) q → c 6,016 74,381 FashionProducts
it i
ReMuQ(Luoetal.,2023) q → c 3,609 195,387 Wikipedia
it t
OVEN-QS(Huetal.,2023) q → c /c 3,291 6,084,491 Wikipedia
it t it
Table2: Statisticalinformationforthezero-shotmulti-modalretrievalbenchmarkdatasets. qandcrepresentquery
andcandidaterespectively, withthesubscriptsi, t, anditdenotingimage, text, andcomposedimage-textdata
respectively. Duringthezero-shotevaluation,weutilizethequeriesfromthevalidationortestsetofeachdatasetto
performretrievalassessmentswithintheentirecorpusoftherespectivedataset.
Models #Params WebQA CIRR FashionIQ OVEN-QS ReMuQ Average
CLIP 149M 10.54 13.37 3.56 1.06 65.05 18.72
CLIP-MM 149M 28.77 19.64 5.55 0.40 58.86 22.64
BLIP 224M 10.03 8.25 1.50 0.06 1.25 4.22
BLIP-MM 224M 30.11 10.31 1.23 0.27 55.66 19.52
Pic2Word 224M 12.72 23.42 8.24 0.97 68.99 22.87
Pic2Word-MM 224M 24.15 26.09 7.65 0.82 78.09 27.36
VISTA(Ours) 196M 60.11 22.51 7.51 8.39 84.73 36.65
Table3: Zero-shotevaluationresultswithRecall@5onvarioushybridmulti-modalretrievalbenchmarks. The
’-MM’notationindicatesbaselinemodelsthathaveundergonemulti-modaltrainingonourgenerateddata. For
zero-shotevaluation,weutilizetheentirecorpusofeachdataset,encompassingalldatasplits,asthecandidatepool.
OverallPerformance. Thezero-shotperformance ingWebQA,CIRR,andReMuQ.Duringthesuper-
of various models is presented in Table 3. Our visedfine-tuningprocess,wetrainallparameters
VISTAmodelachievesstate-of-the-artaverageper- of VISTA. Importantly, we abstain from making
formanceacrossalltasks,withmorethan9%im- any task-specific modifications to VISTA and do
provementonRecall@5. OntheWebQA,OVEN- not utilize any additional training data. The ex-
QS,andReMuQdatasets,ourmodelsignificantly perimentalresultsdemonstratetherobustnessand
outperformsallbaselinesinzero-shotretrievalper- exceptionaladaptabilityofVISTAacrossvarious
formance. While the performance of our model hybridmulti-modalretrievaltasks.
on the composed image retrieval task on CIRR
4.2.1 Fine-TuningPerformanceonWebQA
and FashionIQ is slightly lower than the propri-
etary model, pic2word, it should be noted that Details&Metrics. Followingtheapproachof(Liu
pic2wordisamodelspecificallydesignedforthis etal.,2022),wefine-tuneourVISTAonthetrain-
task. These results affirm the versatility and effi- ingsetofWebQA(Changetal.,2022). Weemploy
cacyofVISTAinhybridmulti-modalretrieval. In hardnegativesfrom(Liuetal.,2022)fortraining
addition,throughmulti-modaltrainingonourgen- and set the count of hard negatives to 9. During
erateddataset,wehaveseenasignificantimprove- fine-tuning,wesetthebatchsizeto288,andtheini-
mentinthezero-shotperformanceofallbaseline tiallearningrateto2e-5,andfine-tunefor700steps.
modelsacrossvarioustasks. Thisdemonstratesthe Duringtesting,weusethevalidationquerysetto
efficiencyanduniversalityofourgenerateddataset. retrieve from the entire corpus. Recall@5/10/20
In addition, the qualitative zero-shot retrieval re- andMRR@10serveasourevaluationmetrics.
sultsofVISTAcanbefoundinAppendixF. Results. The experimental results are presented
inTable4. VISTAachieves70.8%inRecall@5and
4.2 SupervisedFine-TuningPerformance
71.0%inRecall@10. VISTAoutperformsthepre-
Wefine-tuneourVISTAmodelacrossavarietyof viousSOTAmethod(Liuetal.,2022)byover6%
hybridmulti-modalretrievalbenchmarks,includ- andexceededtheconcurrentworkMarvel(ZhouMethods R@5 R@10 R@20 MRR@10 Methods R@5 R@10
CLIP-DPR 49.6 60.1 70.2 50.6
BM25(Robertsonetal.,2009) 8.8 10.8
UniVL-DR 64.5 72.9 78.8 66.8
Marvel-DPR 60.1 69.6 78.0 61.6 DPR(Karpukhinetal.,2020) 43.4 48.8
Marvel-ANCE 70.8 78.8 84.3 71.0 SEAL(Bevilacquaetal.,2022) 66.4 74.1
VISTA(Ours) 74.9 83.7 89.4 75.3 ReViz(Luoetal.,2023) 62.4 71.6
ReViz-ICT(Luoetal.,2023) 76.2 83.3
Table4: Supervisedfine-tuningresultsontheWebQA
GeMKR(Longetal.,2024) 90.3 92.7
dataset. ThebaselinemodelsCLIP-DPRandUniVL-
DRaretakenfrom(Liuetal.,2022),whileMravel-DPR VISTA(Ours) 96.3 97.3
andMarvel-ANCEaretakenfrom(Zhouetal.,2023).
Allretrievalsareperformedonthededuplicatedcorpus. Table6: Supervisedfine-tuningresultsontheReMuQ
testset.
Methods R@5 R @1 Avg.
sub
4.2.3 Fine-TuningPerformanceonReMuQ
CIRPLANT(Liuetal.,2021) 52.6 39.2 45.9
CompoDiff(Guetal.,2023) 54.4 35.8 45.1 Details & Metrics. We fine-tune our model on
Combiner(Baldratietal.,2022) 65.4 62.4 63.9
thetrainingsetofReMuQandtestitonitstestset.
Blip4CIR+Bi(Liuetal.,2024) 73.1 72.1 72.6
CLIP4Cir(Baldratietal.,2023) 77.0 73.2 75.1 The evaluation is conducted on the entire knowl-
CoVR(Venturaetal.,2023) 78.6 75.0 76.8
edge base of ReMuQ. We report Recall@5 and
VISTA(Ours) 76.1 75.7 75.9 Recall@10,andcompareourresultswithstate-of-
the-art methods. The initial learning rate is set
Table5: Supervisedfine-tuningresultsontheCIRRtest
to 2e-5, the batch size to 1,920, and the model is
set.
fine-tunedfor200steps.
Results. As shown in Table 6, VISTA achieves
ashighas96.3%inRecall@5,surpassingthelat-
etal.,2023)bymorethan4%.
eststate-of-the-artmethod(Longetal.,2024)by
morethan5%. Theseresultsdemonstratethepow-
erful capability of VISTA in multi-modal knowl-
4.2.2 Fine-TuningPerformanceonCIRR
edgebaseretrievalandhighlightitsconsiderable
Details&Metrics. Followingthecommonproto- potential for application in multi-modal retrieval
colsforcomposedimageretrieval,weevaluatethe augmentedgenerationforLLMs.
modelperformanceonthetestsetoftheCIRR(Liu
4.3 AblationAnalysis
et al., 2021) dataset. CIRR includes two bench-
marks: a standard one where the target search 4.3.1 TheImpactofStage-2TrainingData
space encompasses the entire test corpus, and a
Table7investigatestheeffectofourgeneratedcom-
fine-grainedsubsetwherethesearchspaceislim-
posed image-text training data (stage-2 training
itedtoasubgroupofsiximagessimilartothequery
data) on the zero-shot performance across a va-
image. WereportRecall@5(R@5)fortheformer
riety of tasks. VISTA denotes the model post
S1
andRecall@1(R @1)forthelatter,andcalculate
sub thefirststageofimage-textcross-modalalignment.
theaverageofthesetworecallmeasures. During
Buildingon thismodel, we usedifferent training
fine-tuning,wesettheinitiallearningrateto2e-5
dataconfigurationstoanalyzetheinfluenceofour
andthebatchsizeto720,treatingthesubgroupas
generatedstage-2trainingdataonthemulti-modal
hardnegativesfortraining. Themodelisfine-tuned
embeddingcapabilitiesofVISTA.
foratotalof900steps.
ComparedtotheVISTA model,theuseofour
S1
Results. TheexperimentalresultsareshowninTa- generated Image&Text To Image (IT2I) Dataset
ble 5. Our VISTA achieves 76.1% in Recall@5, leadstosignificantperformanceenhancementson
75.7% in R @1, and an overall average perfor- the CIRR, FashionIQ, and ReMuQ benchmarks.
sub
manceof75.9%onthetestset. Withoutemploying These benchmarks are characterized by their in-
anytask-specificmodule,ourVISTAachievesper- herentneedforacomprehensiveunderstandingof
formanceonparwithstate-of-the-artmodelsthat multi-modalqueries. Incomparisontotrainingon
havebeenpre-trainedorspecificallydesignedfor theInstructPix2Pixdataset(Daietal.,2023)that
composedimageretrieval. lackshardnegatives,themodeltrainedonourIT2IModel WebQA CIRR FashionIQ OVEN-QS ReMuQ Avg.
VISTA 35.70 9.59 1.33 3.82 21.53 14.39
S1
w/InstructPix2Pix 44.24 14.47 2.88 5.14 80.60 29.47
w/Ours-IT2I 51.87 21.29 6.73 3.40 89.06 34.47
w/WebQA - 10.64 2.03 3.92 77.42 -
w/Ours-T2IT 57.28 15.86 3.81 5.38 74.67 31.40
VISTA 60.11 22.51 7.51 8.39 84.73 36.65
VISTA-SF 59.46 15.93 5.27 1.19 83.04 32.98
Table7: Ablationstudies: Thezero-shotperformanceofmodelsthatusedifferentstage-2trainingdataormulti-
modalfusionmethods. IT2IandT2ITrespectivelyrepresentourgeneratedImage&TextToImageDatasetandText
ToImage&TextDataset. Underlinedvaluesindicatewheretheassociatedtrainingdatasethassignificantlyimproved
performanceonthecorrespondingbenchmarks. VISTA-SFisanablationmodelthatemploysthescore-fusion
methodtoencodecomposedimage-textdata.
datasetdemonstratessuperiormulti-modalretrieval 4.3.2 Multi-ModalFusionMethods
performance. The hard negatives that we gener-
Table 7 also examines the benefits of the VISTA
ate can foster the model’s comprehension of the
modelarchitectureinencodingcomposedimage-
interplaybetweenimagesandtext,therebyprevent-
text data in comparison to VISTA-SF (last line).
ingthemodelfromexcessivelyrelyingonimage
For VISTA-SF, a score-fusion approach is em-
featuresimilarityratherthansemanticcorrelation
ployed during the encoding of multi-modal data.
duringthetrainingprocess.
Thisinvolvesindependentlyencodingimagesand
text using the VISTA model, followed by an
IncontrasttotheVISTA model,theemploy- element-wiseadditionoftheembeddingsderived
S1
mentofourgeneratedTextToImage&Text(T2IT) from both modalities. The experimental results
datasetnotablyenhancesthezero-shotperformance demonstrate that VISTA achieves substantial im-
ontheWebQAandOVEN-QSbenchmarks. These provement,significantlyoutperformingtheVISTA-
benchmarksrequireacomposedunderstandingof SF model. This can be attributed to the fact that
multi-modal candidates, a demand directly met score-fusion cannot deeply comprehend the inte-
by the training scenarios presented in our T2IT grationofimagesandtext,whereasVISTAisprofi-
dataset. Additionally,weevaluatetheperformance cientinconsistentlyencodingandinterpretingcom-
ofthemodelusingWebQAforsecond-stagetrain- posedimage-textdata. Thesefindingsunderscore
ing. Except for ReMuQ, which is sourced from the advantages of the VISTA model in encoding
WebQA,themodelexhibitssuperiorperformance interleavedtextandvisualsequences.
when trained with our T2IT dataset compared to
5 Conclusion
whenitistrainedwithWebQA.Thissuggeststhat
ourT2ITdataoffersenhancedgeneralizationcapa-
Inthispaper,weintroduceVISTA,anVISualized
bilities compared to manually annotated datasets
Text embedding approach for universal multi-
whenusedtotrainmulti-modalembeddingmodels.
modalretrievAl. Ourworkmakesthreesignificant
contributions. Firstly,wedesignaflexiblemodel
In the final Stage-2 training, we conduct dual- architecturethatenablesthein-depthfusionoftext
tasktrainingusingbothourIT2IandT2ITdatasets and image data, while maintaining the powerful
on the basis of VISTA , resulting in the devel- performanceofthegeneraltextembeddingmodels.
S1
opment of VISTA. Among all different training Secondly, we develop two data generation strate-
dataconfigurations,VISTAachievesthebestper- gies for training multi-modal embedding models
formanceinfouroutofthefivebenchmarks. This without the need for manual annotation. Lastly,
result confirms the synergistic effect of integrat- we introduce a two-stage training algorithm that
ing both datasets in the training of multi-modal rapidlyenhancesthemulti-modalrepresentationca-
embeddingmodels. pabilityofVISTA.ExtensiveexperimentalresultsdemonstratethesuperiorperformanceofVISTAin clip-basedfeatures. ACMTransactionsonMultime-
bothzero-shotandsupervisedfine-tuningsettings diaComputing,CommunicationsandApplications,
20(3):1–24.
forvariousmulti-modalretrievaltasks.
MicheleBevilacqua,GiuseppeOttaviano,PatrickLewis,
Limitations ScottYih,SebastianRiedel,andFabioPetroni.2022.
Autoregressivesearchengines:Generatingsubstrings
As we reflect on the work conducted, we iden- asdocumentidentifiers. AdvancesinNeuralInfor-
tify two areas of potential refinement in our ap- mationProcessingSystems,35:31668–31683.
proach. The first area concerns the diversity of
SebastianBorgeaud,ArthurMensch,JordanHoffmann,
imagestylesinourImage&TextToImage(IT2T) TrevorCai,ElizaRutherford,KatieMillican,George
dataset. Theimagesaregeneratedusingastabledif- vandenDriessche, Jean-BaptisteLespiau, Bogdan
Damoc,AidanClark,DiegodeLasCasas,Aurelia
fusionmodel,whichmighthavelimitedtherange
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
of styles in the dataset. The second area relates
SaffronHuang,LorenMaggiore,ChrisJones,Albin
to the handling of image tokens. In our current Cassirer, AndyBrock, MichelaPaganini, Geoffrey
approach, we feed all image tokens directly into Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan,JackW.Rae,ErichElsen,andLaurentSifre.
the text encoder. This procedure might inadver-
2022. Improvinglanguagemodelsbyretrievingfrom
tentlyheightenthecomputationalloadduetothe
trillionsoftokens. InInternationalConferenceon
uniformsequencelength. Apotentialimprovement MachineLearning,ICML2022,17-23July2022,Bal-
couldbetheimplementationofvariable-lengthim- timore,Maryland,USA,volume162ofProceedings
of Machine Learning Research, pages 2206–2240.
agetokensequences,whichcouldreducesequence
PMLR.
lengths and consequently lead to more efficient
computation. TimBrooks,AleksanderHolynski,andAlexeiAEfros.
2023. Instructpix2pix:Learningtofollowimageedit-
EthicsStatement ing instructions. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecog-
nition,pages18392–18402.
Alltrainingdatausedinourmodelhaveundergone
rigorousscreeningtoremoveharmfulcontent. This YingshanChang,MriduNarang,HisamiSuzuki,Gui-
includesboththeLAION-5Bdatasetconstructed hong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.
Webqa: Multihop and multimodal qa. In Proceed-
by(Schuhmannetal.,2022)andthemulti-modal
ingsoftheIEEE/CVFConferenceonComputerVi-
datasetwebuiltourselves. Despiteourbestefforts,
sionandPatternRecognition,pages16495–16504.
we acknowledge that we cannot fully guarantee
thatthesescreeningswereentirelycomprehensive JianlvChen,ShitaoXiao,PeitianZhang,KunLuo,Defu
Lian, and Zheng Liu. 2024. Bge m3-embedding:
or without omissions. Furthermore, we strongly
Multi-lingual,multi-functionality,multi-granularity
discouragetheuseofVISTAforencodingandre- textembeddingsthroughself-knowledgedistillation.
trievingsensitivecontent. arXivpreprintarXiv:2402.03216.
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-
Acknowledgements
ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. 2023. Sharegpt4v: Improving large multi-
This research is supported by National Science
modalmodelswithbettercaptions. arXivpreprint
andTechnologyMajorProject(2023ZD0121504) arXiv:2311.12793.
andNationalNaturalScienceFoundationofChina
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and
(NSFC-62306046,NSFC-62272054).
William W. Cohen. 2022. Murag: Multimodal
retrieval-augmentedgeneratorforopenquestionan-
swering over images and text. In Proceedings of
References the2022ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing,EMNLP2022,AbuDhabi,
AlbertoBaldrati,MarcoBertini,TiberioUricchio,and UnitedArabEmirates,December7-11,2022,pages
AlbertoDelBimbo.2022. Effectiveconditionedand 5558–5570.AssociationforComputationalLinguis-
composedimageretrievalcombiningclip-basedfea- tics.
tures. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages Wenliang Dai, Junnan Li, Dongxu Li, Anthony
21466–21474. Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven C. H. Hoi.
AlbertoBaldrati,MarcoBertini,TiberioUricchio,and 2023. Instructblip: Towardsgeneral-purposevision-
Alberto Del Bimbo. 2023. Composed image re- language models with instruction tuning. CoRR,
trievalusingcontrastivelearningandtask-oriented abs/2305.06500.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kiela. 2020b. Retrieval-augmented generation for
KristinaToutanova.2018. Bert: Pre-trainingofdeep knowledge-intensiveNLPtasks. InAdvancesinNeu-
bidirectionaltransformersforlanguageunderstand- ralInformationProcessingSystems33: AnnualCon-
ing. arXivpreprintarXiv:1810.04805. ferenceonNeuralInformationProcessingSystems
2020,NeurIPS2020,December6-12,2020,virtual.
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Thomas Unterthiner, Mostafa Dehghani, Matthias Hoi.2022. Blip: Bootstrappinglanguage-imagepre-
Minderer, Georg Heigold, Sylvain Gelly, Jakob training for unified vision-language understanding
Uszkoreit, and Neil Houlsby. 2021. An image andgeneration. InInternationalConferenceonMa-
is worth 16x16 words: Transformers for image chineLearning,pages12888–12900.PMLR.
recognitionatscale. In9thInternationalConference
on Learning Representations, ICLR 2021, Virtual Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph
Event,Austria,May3-7,2021.OpenReview.net. Feichtenhofer, and Kaiming He. 2023. Scaling
language-image pre-training via masking. In Pro-
GeonmoGu,SanghyukChun,WonjaeKim,HeeJaeJun, ceedingsoftheIEEE/CVFConferenceonComputer
YoohoonKang,andSangdooYun.2023. Compod- VisionandPatternRecognition,pages23390–23400.
iff: Versatile composed image retrieval with latent
diffusion. arXivpreprintarXiv:2303.11916. ZhenghaoLiu,ChenyanXiong,YuanhuiyiLv,Zhiyuan
Liu, and Ge Yu. 2022. Universal vision-language
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-
dense retrieval: Learning a unified representation
wal,MandarJoshi,KentonLee,KristinaToutanova,
spaceformulti-modalretrieval. InTheEleventhIn-
and Ming-Wei Chang. 2023. Open-domain visual
ternationalConferenceonLearningRepresentations.
entity recognition: Towards recognizing millions
of wikipedia entities. In IEEE/CVF International ZheyuanLiu,CristianRodriguez-Opazo,DamienTeney,
ConferenceonComputerVision,ICCV2023,Paris, andStephenGould.2021. Imageretrievalonreal-life
France, October 1-6, 2023, pages 12031–12041. imageswithpre-trainedvision-and-languagemodels.
IEEE. InProceedingsoftheIEEE/CVFInternationalCon-
ferenceonComputerVision,pages2125–2134.
GautierIzacard,MathildeCaron,LucasHosseini,Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
Zheyuan Liu, Weixuan Sun, Yicong Hong, Damien
andEdouardGrave.2022. Unsuperviseddensein-
Teney,andStephenGould.2024. Bi-directionaltrain-
formationretrievalwithcontrastivelearning. Trans.
ing for composed image retrieval via text prompt
Mach.Learn.Res.,2022.
learning. In Proceedings of the IEEE/CVF Win-
terConferenceonApplicationsofComputerVision,
ChaoJia,YinfeiYang,YeXia,Yi-TingChen,Zarana
pages5753–5762.
Parekh,HieuPham,QuocLe,Yun-HsuanSung,Zhen
Li, and Tom Duerig. 2021. Scaling up visual and
Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan
vision-language representation learning with noisy
Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou.
textsupervision. InInternationalconferenceonma-
2024. Generative multi-modal knowledge re-
chinelearning,pages4904–4916.PMLR.
trievalwithlargelanguagemodels. arXivpreprint
arXiv:2401.08206.
JeffJohnson,MatthijsDouze,andHervéJégou.2019.
Billion-scale similarity search with gpus. IEEE
ManLuo,ZhiyuanFang,TejasGokhale,YezhouYang,
TransactionsonBigData,7(3):535–547.
and Chitta Baral. 2023. End-to-end knowledge re-
trievalwithmulti-modalqueries. InProceedingsof
VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
the61stAnnualMeetingoftheAssociationforCom-
S.H.Lewis,LedellWu,SergeyEdunov,DanqiChen,
putationalLinguistics(Volume1:LongPapers),ACL
andWen-tauYih.2020. Densepassageretrievalfor
2023,Toronto,Canada,July9-14,2023,pages8573–
open-domainquestionanswering. InProceedingsof
8589.AssociationforComputationalLinguistics.
the2020ConferenceonEmpiricalMethodsinNat-
ural Language Processing, EMNLP 2020, Online,
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-
November16-20,2020,pages6769–6781.Associa-
ford, Jesse Michael Han, Jerry Tworek, Qiming
tionforComputationalLinguistics.
Yuan,NikolasTezak,JongWookKim,ChrisHallacy,
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio Johannes Heidecke, Pranav Shyam, Boris Power,
Petroni,VladimirKarpukhin,NamanGoyal,Hein- Tyna Eloundou Nekoul, Girish Sastry, Gretchen
richKüttler, MikeLewis, Wen-tauYih, TimRock- Krueger,DavidSchnurr,FelipePetroskiSuch,Kenny
täschel,etal.2020a. Retrieval-augmentedgeneration Hsu, Madeleine Thompson, Tabarak Khan, Toki
forknowledge-intensivenlptasks. AdvancesinNeu- Sherbakov,JoanneJang,PeterWelinder,andLilian
ralInformationProcessingSystems,33:9459–9474. Weng. 2022. Text and code embeddings by con-
trastivepre-training. CoRR,abs/2201.10005.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-
Goyal,HeinrichKüttler,MikeLewis,Wen-tauYih, tavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,
Tim Rocktäschel, Sebastian Riedel, and Douwe YiLuan,KeithB.Hall,Ming-WeiChang,andYinfeiYang.2022. Largedualencodersaregeneralizable Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang,
retrievers. InProceedingsofthe2022Conferenceon and Yue Cao. 2023. Eva-clip: Improved train-
EmpiricalMethodsinNaturalLanguageProcessing, ing techniques for clip at scale. arXiv preprint
EMNLP2022,AbuDhabi,UnitedArabEmirates,De- arXiv:2303.15389.
cember7-11,2022,pages9844–9855.Association
forComputationalLinguistics.
Lucas Ventura, Antoine Yang, Cordelia Schmid, and
GülVarol.2023. Covr: Learningcomposedvideo
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
retrieval from web video captions. arXiv preprint
CarrollWainwright,PamelaMishkin,ChongZhang,
arXiv:2308.14746.
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
tions with human feedback. Advances in Neural NamVo,LuJiang,ChenSun,KevinMurphy,Li-JiaLi,
InformationProcessingSystems,35:27730–27744. LiFei-Fei,andJamesHays.2019. Composingtext
andimageforimageretrieval-anempiricalodyssey.
AlecRadford,JongWookKim,ChrisHallacy,Aditya InProceedingsoftheIEEE/CVFconferenceoncom-
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- puter vision and pattern recognition, pages 6439–
try, Amanda Askell, Pamela Mishkin, Jack Clark, 6448.
etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconfer-
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
enceonmachinelearning,pages8748–8763.PMLR.
Jiao,LinjunYang,DaxinJiang,RanganMajumder,
NilsReimersandIrynaGurevych.2019. Sentence-bert: and Furu Wei. 2022. Text embeddings by weakly-
Sentenceembeddingsusingsiamesebert-networks. supervisedcontrastivepre-training. arXivpreprint
In Proceedings of the 2019 Conference on Empiri- arXiv:2212.03533.
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu,
LanguageProcessing,EMNLP-IJCNLP2019,Hong Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.
Kong,China,November3-7,2019,pages3980–3990. 2023. Uniir: Trainingandbenchmarkinguniversal
AssociationforComputationalLinguistics. multimodal information retrievers. arXiv preprint
arXiv:2311.17136.
StephenRobertson,HugoZaragoza,etal.2009. The
probabilistic relevance framework: Bm25 and be-
HuiWu, YupengGao, XiaoxiaoGuo, ZiadAl-Halah,
yond. FoundationsandTrends®inInformationRe-
StevenRennie,KristenGrauman,andRogerioFeris.
trieval,3(4):333–389.
2021. Fashioniq: Anewdatasettowardsretrieving
RobinRombach,AndreasBlattmann,DominikLorenz, imagesbynaturallanguagefeedback. InProceedings
Patrick Esser, and Björn Ommer. 2022. High- oftheIEEE/CVFConferenceoncomputervisionand
resolutionimagesynthesiswithlatentdiffusionmod- patternrecognition,pages11307–11317.
els. In Proceedings of the IEEE/CVF conference
oncomputervisionandpatternrecognition, pages Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
10684–10695. Muennighof.2023. C-pack: Packagedresourcesto
advancegeneralchineseembedding. arXivpreprint
KuniakiSaito,KihyukSohn,XiangZhang,Chun-Liang
arXiv:2309.07597.
Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.
2023. Pic2word:Mappingpicturestowordsforzero-
shot composed image retrieval. In Proceedings of MichihiroYasunaga, ArmenAghajanyan, WeijiaShi,
theIEEE/CVFConferenceonComputerVisionand Richard James, Jure Leskovec, Percy Liang, Mike
PatternRecognition,pages19305–19314. Lewis, LukeZettlemoyer, andWen-TauYih.2023.
Retrieval-augmented multimodal language model-
Christoph Schuhmann, Romain Beaumont, Richard ing. InInternationalConferenceonMachineLearn-
Vencu, Cade Gordon, Ross Wightman, Mehdi ing,ICML2023,23-29July2023,Honolulu,Hawaii,
Cherti, Theo Coombes, Aarush Katta, Clayton USA,volume202ofProceedingsofMachineLearn-
Mullis, Mitchell Wortsman, Patrick Schramowski, ingResearch,pages39755–39769.PMLR.
Srivatsa Kundurthy, Katherine Crowson, Ludwig
Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
AndrewYates,RodrigoNogueira,andJimmyLin.2021.
2022. LAION-5B: an open large-scale dataset for
Pretrainedtransformersfortextranking: Bertandbe-
trainingnextgenerationimage-textmodels. InAd-
yond. InProceedingsofthe14thACMInternational
vancesinNeuralInformationProcessingSystems35:
Conferenceonwebsearchanddatamining, pages
AnnualConferenceonNeuralInformationProcess-
1154–1156.
ingSystems2022,NeurIPS2022,NewOrleans,LA,
USA,November28-December9,2022.
Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu,
WeijiaShi,SewonMin,MichihiroYasunaga,Minjoon Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu.
Seo,RichJames,MikeLewis,LukeZettlemoyer,and 2023. Unlock multi-modal capability of dense re-
Wen-tauYih.2023. REPLUG:retrieval-augmented trieval via visual module plugin. arXiv preprint
black-boxlanguagemodels. CoRR,abs/2301.12652. arXiv:2310.14037.Appendix
You are a creative image editor. You need to make
variouscreativemodificationstoanimage.
A MoreDetailsofDataConstruction Thecaptionofthisimageis:
Image&TextToImage(IT2T)Dataset. Thespe- Source Image caption from
InstructPix2Pix dataset.
cificpromptsutilizedincreatingtheI2ITdataset
Youneedtomakedifferenttypesofeditstothisimage,
are illustrated in Figure 3. We direct GPT-3.5
so you need to write 10 edit instruction statements
to generate a range of image editing instructions and the corresponding new captions based on the
thatsignificantlyalterthesemanticcontentofthe originalcaption.
images. This strategy diverges from how image
Please unleash your creativity and devise imaginative
editingdatasetsforimageeditingmodelsarecon-
image modifications, ensuring each one is unique to
structed. DatasetssuchasInstructPix2Pix(Brooks preserve the diversity of generated editing instruction
etal.,2023)prioritizedealingwiththechallenges statements. Generating editing instructions that can
result in substantial and meaningful changes in the
of intricate image edits, whereas our focus with
images’contentismorepreferred.
embedding models is more on understanding the
relationshipsamongimagesemantics. Please return the generated edit statements and
TexttoImage&Text(T2IT)Dataset. Thesteps corresponding output captions in JSON format, with
theformatof....
andpromptsusedintheconstructionoftheT2IT
dataset are illustrated in Figure 4. As described
Figure3: Thespecificpromptsutilizedduringthegen-
in Section 3.2.2, the generation process of the
erationoftheImage&TextToImage(IT2T)dataset.
T2IT dataset is divided into two parts: the first
stepinvolvesgeneratinganarticleabouttheimage
subtopic,andthesecondstepinvolvesgeneratinga hybrid data in either the query or candidate com-
queryforthemultimodaldocument. Itisworthnot- ponents,requiringajointembeddingofimageand
ingthat,inthesecondstepofgeneratingaqueryfor textdata. Unlessotherwisestated,foreachdataset,
themultimodaldocument,westillusedescriptive all zero-shot evaluations are conducted using the
captionsfromShareGPT4V(Chenetal.,2023)to devsplitasthequeryset,withretrievalcarriedout
representtheimagesinthemultimodaldocuments, acrosstheentiretyofitscorpus.
asGPT-3.5cannotdirectlyprocessimagedata. WebQA (Chang et al., 2022) is a multi-hop
andmulti-modalopen-domainquestionanswering
B MoreTrainingDetailsofVISTA
dataset. TheMulti-modalDocumentsRetrievaltask
onWebQAisproposedby(Liuetal.,2022). This
Inthefirsttrainingstage, weutilizetheFLIP(Li
taskinvolvesidentifyingsuitabletextorimage-text
etal.,2023)strategytoimprovethetimeefficiency
pair candidates based on a query text. Notably,
of image-text contrastive training. We randomly
wede-duplicatethecorpusofWebQA,following
mask50%ofimagetokens. Thisphaseistrained
the same procedure as UniIR (Wei et al., 2023).
for 116K steps. Subsequently, we conduct un-
Theprocessedcorpusencompasses544,489unique
masked tuning (Li et al., 2023) for an extra 48K
textdocuments,and403,277distinctimage-caption
steps. Thebatchsizeis16Kthroughoutthisstage.
pairs.
In the second training stage, the quantity of hard
CIRR(Liuetal.,2021)isanopen-domaindataset
negative examples in the IT2I dataset is set to 3.
designedfortheComposedImageRetrieval(CIR)
Thisstageistrainedforonly600stepswithabatch
task (Vo et al., 2019). In this task, each query
sizeof1920. Acrossbothstages,wesetthetemper-
comprises an image-text pair, which includes a
aturecoefficientτ forcontrastivelearningat0.02.
referenceimageandinstructivetextthatdelineates
Weinitiatethelearningrateat2e−5andapplya
the differences between the reference and target
lineardecaystrategyforsubsequentadjustments.
images.
FashionIQ(Wuetal.,2021)servesasanotherrele-
C DetailedInformationofBenchmarks
vantdatasetfortheCIRtask,specificallyfocusing
ToevaluatetheeffectivenessofourVISTAmodel onfashionproducts,includingdresses,shirts,and
inhybridmulti-modalretrievaltasks,wecollected top-tees.
fivedistinctdatasets,encompassingfourdifferent ReMuQ (Luo et al., 2023) is a dataset curated
multi-modalretrievaltasks. Eachdatasetfeatures forKnowledgeRetrievalwithMulti-modalQueriesStep1:Re-writing caption for image:
Iwillprovideyouwithadetaileddescriptionofanimage.Basedonthisdescription,pleaseimaginethesceneofthis
imageandwriteanarticleaboutaspecificsubtopicrelatedtothisimage.
Thedescriptionofthisimageis:
Descriptive caption from
ShareGPT4Vdataset.
Thetopicofthearticlecanbeaboutanobject,acharacter,oranycontentinthebackgroundoftheimage.Youcan
imagine yourself as a sharer, a popular science writer, or any identity you like. You can write a description,
reflection, or informative article about the image from any specific perspective. Use your imagination. However,
pleaseavoidgeneratinggenericimagedescriptiontext.Thelengthofarticleisaround...
Step2:Generatingquerysentenceforimage-captionpairs:
I will provide you with a detailed description of an image and its accompanying caption. Please imagine that you
have already seen this image, and combine it with the corresponding caption to understand them as a multimodal
document.
Here are the image description and caption:
Descriptive caption from Rewritten caption
ShareGPT4Vdataset fromStep1
Please imagine yourself as a multimodal document search user and write a suitable query statement that can
locate this multimodal document. The length of query sentence is around...
Figure4: ThespecificpromptsemployedinthegenerationoftheTexttoImage&Text(T2IT)dataset,withthe
lengthsofthearticlesandqueriesrandomlyassignedineachdatagenerationiterationtoensurediversity. Typically,
articlesareapproximately50words,andqueriesarewithin20words.
task. In the ReMuQ dataset, each query is com- image data respectively, while Φ and Φ refer
T I
posedofanimageandanassociatedtextualques- tothetextandimageencodersoftheCLIP/BLIP
tion,theobjectivebeingtosearchpertinentcontent models,correspondingly.
fromatextualknowledgebase. Duetotheabsence ForPic2Wordmodel,aproprietarymodeldedi-
ofadevsplit,weevaluateReMuQonthetestsplit. catedtozero-shotcomposedimageretrieval. We
OVEN-QS is the Query Split of the OVEN employ this model by mapping an image to a
benchmark (Hu et al., 2023). OVEN-QS is pseudolanguagetokeninthetextencoder,adher-
proposed for the Entity Retrieval with Visually- ingtotheirmethodologyforencodingcomposed
Situated Queries task. Each query in OVEN-QS image-textqueries:
isanimage-textpair,withthetextcomponentbe-
[*] = Img2Txt(Φ (I))
I
ing visually situated and filtered out from VQA (8)
e = Φ ("a photo of [*]";T)
scenarios. Thisdatasetnecessitatesidentifyingthe h T
correct entity from a diverse set of text or image- Inthisequation,Img2Txtisthemappingnetwork
textcandidates. thattranslatestheimagefeatureintoapseudolan-
guage token, denoted as [∗]. The image and text
D ImplementationdetailsofZero-Shot
encoders,Φ andΦ arederivedfromtheCLIP-L
I T
Baselines
model (Radford et al., 2021). It derives the com-
posedimage-textfeaturesbyintegratingthepseudo
ForbothCLIPandBLIPmodels,weadoptascore
languagetokenwiththetext,subsequentlyprocess-
fusionapproachonimage-textpairdataasoutlined
ingthisunifiedinputthroughthetextencoder.
in (Wei et al., 2023; Liu et al., 2022). The score
fusionprocessisrepresentedasfollows: E TheImpactofTokenOrder
e = Φ (T)+Φ (I) (7) AsoutlinedinEquation(3),weexclusivelyutilized
h T I
the (image tokens, text tokens) order to process
where e denotes the embedding of composed theinterleavedimage-textdatainallpreviousex-
h
image-text data, T and I represent the text and periments. To investigate the potential impact ofTable8: InfluenceoftokenorderonVISTAperformanceinprocessinginterleavedimage-textdata.
TokenOrder WebQA CIRR FashionIQ OVEN-QS ReMuQ Avg.
(visualtokens,texttokens) 60.11 22.51 7.51 8.39 84.73 36.65
(texttokens,visualtokens) 59.49 22.10 7.36 9.54 84.68 36.63
tokenorderontheperformanceofourVISTA,we
conductedadditionalexperimentsusingthe(textto-
kens,imagetokens)order. Theevaluationresultsin
zero-shotsettingsarepresentedinTable8,withall
reportedresultsbasedontheRecall@5metric. The
experimentalfindingsindicatethatalteringtheor-
derofimageandtexttokensdoesnotsignificantly
impactthemodel’sperformance.
F Zero-ShotQualitativeResults
Figures5,6,and7illustratequalitativezero-shot
examplesfromourVISTAmodelontheCIRR(Liu
etal.,2021),FashionIQ(Wuetal.,2021),andWe-
bQA(Changetal.,2022)datasets,respectively. In
eachfigure,thequeryispresentedontheextreme
left,withthetop@Kretrievalresultsdisplayedto
its right. The ground-truth candidate, as identi-
fiedbythebenchmark,isdenotedbyagreenbox.
Weconductourretrievalprocessacrosstheentire
corpus for each dataset, which leads to identify-
ingcandidatesthatmeettheretrievalrequirements
but are not tagged as ground truth. These results
demonstratetheimpressivezero-shotmulti-modal
retrievalperformanceofourVISTAmodel.Put a leash
on the black
dog on the
pavement.
Make the
background
dark, as if the
camera has
taken the
photo at night
The balcony
of a house and
a three lamps
hangging.
Figure5: TheQualitativeExamplesofourVISTAModelontheCIRRBenchmark.
is sage with Hebrew
textandlighter
colored
is grey with a wider
plaid pattern and
darker colored
is a red long
sleeve with small
stripes and has
buttons on it
Figure6: TheQualitativeExamplesofourVISTAModelontheFashionIQBenchmark.Are the tallest pipes on the pipe
organ at cathedral Saint Pierre in
the middle or on the sides?
Beauvais, cathedral Saint Organ pipes, St Finbarre's Cathedral, Strasbourg Cathedral pipe organ
Pierre, the pipe organ Cork St Finbarres Cathedral full frontal
The main entrance to the
college is on Broad Street,
located between Balliol
College and Blackwell's
Are the columns further apart at bookshop, and opposite Turl
the entrance of Lane Hall or the Street. It is enclosed by an
iron palisade rather than a
Coram Library?
wall, and the college's
distinctive blue gates provide
it with a more open and
accessible appearance than
Entrance to Coram Library CoramLib Coram Library
many others in Oxford.
The inflorescence is an umbel
of six to 20 white flowers,
lacking the bulbils produced by
some other Allium species
such as Allium vineale (crow
Do the flowers of the Allium garlic) and Allium oleraceum
aflatuense grow in bunches? (field garlic). The flowers are
star-like with six white tepals,
Allium aflatuense about 16-20 mm in diameter,
(Alliaceae) flower Allium-aflatunense-0567 with stamens shorter than the
perianth.
Figure7: TheQualitativeExamplesofourVISTAModelontheWebQABenchmark.