Chimera: Effectively Modeling Multivariate Time Series with
2-Dimensional State Space Models
AliBehrouz★,MicheleSantacatterina†,andRaminZabih★
★DepartmentofComputerScience,CornellUniversity
†NYUGrossmanSchoolofMedicine
, ,
ab2947@cornell.edu santam13@nyu.edu rdz@cs.cornell.edu
Abstract
Modelingmultivariatetimeseriesisawell-establishedproblemwithawiderangeofapplicationsfromhealthcare
tofinancialmarkets.It,however,isextremelychallengingasitrequiresmethodsto(1)havehighexpressivepowerof
representingcomplicateddependenciesalongthetimeaxistocapturebothlong-termprogressionandseasonalpatterns,(2)
capturetheinter-variatedependencieswhenitisinformative,(3)dynamicallymodelthedependenciesofvariateandtime
dimensions,and(4)haveefficienttrainingandinferenceforverylongsequences.TraditionalStateSpaceModels(SSMs)
areclassicalapproachesforunivariatetimeseriesmodelingduetotheirsimplicityandexpressivepowertorepresentlinear
dependencies.They,however,havefundamentallylimitedexpressivepowertocapturenon-lineardependencies,areslow
inpractice,andfailtomodeltheinter-variateinformationflow.Despiterecentattemptstoimprovetheexpressivepower
ofSSMsbyusingdeepstructuredSSMs,theexistingmethodsareeitherlimitedtounivariatetimeseries,failtomodel
complexpatterns(e.g.,seasonalpatterns),failtodynamicallymodelthedependenciesofvariateandtimedimensions,
and/orareinput-independent.WepresentChimera,anexpressivevariationofthe2-dimensionalSSMswithcarefuldesign
ofparameterstomaintainhighexpressivepowerwhilekeepingthetrainingcomplexitylinear.UsingtwoSSMheadswith
differentdiscretizationprocessesandinput-dependentparameters,Chimeraisprovablyabletolearnlong-termprogression,
seasonalpatterns,anddesirabledynamicautoregressiveprocesses.Toimprovetheefficiencyofcomplex2Drecurrence,
wepresentafasttrainingusinganew2-dimensionalparallelselectivescan.Wefurtherpresentanddiscuss2-dimensional
MambaandMamba-2asthespacialcasesofour2DSSM.Ourexperimentalevaluationshowsthesuperiorperformance
ofChimeraonextensiveanddiversebenchmarks,includingECGandspeechtimeseriesclassification,long-termand
short-termtimeseriesforecasting,andtimeseriesanomalydetection.
1 Introduction
Modelingtimeseriesisawell-establishedproblemwithawiderangeofapplicationsfromhealthcare(Behrouz,Delavari,
andHashemi2024;Ivanovetal.1999)tofinancialmarkets(Gajamannage,Park,andJayathilake2023;PincusandKalman
2004) and energy management (H. Zhou et al. 2021). The complex nature of time series data, its diverse domains of
applicability,anditsbroadrangeoftasks(e.g.,classification(Behrouz,Delavari,andHashemi2024;Wagneretal.2020),
imputation(LuoandX.Wang2024;H.Wu,Hu,etal.2023),anomalydetection(Behrouz,Delavari,andHashemi2024;
Su et al. 2019), and forecasting (H. Zhou et al. 2021)), however, raise fundamental challenges to design effective and
generalizablemodels:(1)Thehigher-order,seasonal,andlong-termpatternsintimeseriesrequireaneffectivemodeltobe
abletoexpressivelycapturecomplexandautoregressivedependencies;(2)Inthepresenceofmultiplevariatesoftime
series,aneffectivemodelneedtocapturethecomplexdynamicsofthedependenciesbetweentimeandvariateaxes.More
specifically,mostexistingmultivariatemodelsseemtosufferfromoverfittingespeciallywhenthetargettimeseriesisnot
correlatedwithothercovariates(Zengetal.2023a).Accordingly,aneffectivemodelneedstoadaptivelylearntoselect
(resp.filter)informative(resp.irrelevant)variates;(3)Thediversesetofdomainsandtasksrequireseffectivemodelstobe
freeofmanualpre-processinganddomainknowledgeandinsteadadaptivelylearnthem;and(4)Duetotheprocessingof
verylongsequences,effectivemethodsneedefficienttrainingandinference.
Classicalmethods(e.g.,StateSpaceModels(Aoki2013;Harvey1990),ARIMA(Bartholomew1971),SARIMA(Benderand
Simonovic1994),ExponentialSmoothing(ETS)(Winters1960))requiremanualdatapreprocessingandmodelselection,
1
4202
nuJ
6
]GL.sc[
1v02340.6042:viXraFigure1: TheOverviewofContributionsandArchitectureofChimera. Wepresenta2-dimensionalSSMwithcarefuland
expressiveparameterization.Itusesdifferentlearnablediscretizationprocessestolearnseasonalandlong-termprogressionpatterns,
andleveragesaparallelizableandfasttrainingprocessbyre-formulatingthe2Dinputdependentrecurrenceasa2Dprefixsumproblem.
andoftenarenotabletocapturecomplexnon-linear dynamics.Theraiseofdeeplearningmethodsandmorespecifically
Transformers(Vaswanietal.2017)hasledtosignificantresearcheffortstoaddressthelimitationofclassicalmethods
anddevelopeffectivedeepmodels(Z.Chenetal.2023;Kitaev,Kaiser,andLevskaya2020;LimandZohren2021;S.Liu
etal.2021;YongLiu,Hu,etal.2024;Wooetal.2022;H.Wu,J.Wu,etal.2022;H.Wu,Xu,etal.2021;Y.ZhangandYan
2023;T.Zhou,Z.Ma,Wen,X.Wang,etal.2022). Unfortunately,mostexistingdeepmodelsstruggletoachieveallthe
abovefourcriteria.Themainbodyofresearchinthisdirectionhasfocusedondesigningattentionmodulesthatusethe
specialtraitsoftimeseries(Wooetal.2022;H.Wu,Xu,etal.2021).However,theinherentpermutationequivarianceof
attentionscontradictsthecausalnatureoftimeseriesandoftenresultsinsuboptimalperformancecomparedtosimple
linearmethods(Zengetal.2023a).Moreover,theyofteneitheroverlookdifferenceofseasonalandlong-termtrendoruse
non-learnablemethodstohandlethem(Wooetal.2022).
Aconsiderablesubsetofdeepmodelsoverlooktheimportanceofmodelingthedependenciesofvariates(Nieetal.2023;
Zengetal.2023a;M.Zhangetal.2023).Thesedependencies,however,arenotalwaysuseful;specificallywhenthetarget
timeseriesisnotcorrelatedwithothercovariates(S.-A.Chenetal.2023).Despiteseveralstudiesexploringtheimportance
oflearningcrossvariatedependencies(S.-A.Chenetal.2023;YongLiu,Hu,etal.2024;Y.ZhangandYan2023),therehas
beennouniversalstandardandtheconclusionhasbeendifferentdependingonthedomainandbenchmarks.Accordingly,
wearguethataneffectivemodelneedtoadaptively learntocapturethedependenciesofvariatesinadata-dependent
manner.Inthisdirection,recently,YongLiu,Hu,etal.(2024)arguethatattentionmechanismsaremoreeffectivewhen
theyareusedacrossvariates,showingtheimportanceofmodelingcomplexnon-lineardependenciesacrossthevariate
axisinadata-dependentmanner.However,thequadraticcomplexityofTransformerschallengesthemodelonmultivariate
timeserieswithalargenumberofvariates(e.g.,brainactivitysignals(Behrouz,Delavari,andHashemi2024)ortraffic
forecasting(H.Zhouetal.2021)),limitingtheefficienttrainingandinference(seeTable3,andTable5).
Theobjectiveofthisstudyistodevelopaprovablyexpressivemodelformultivariatetimeseriesthatnotonlycanmodel
thedynamicsofthedepenendenciesalongbothtimeandvariates,butitalsotakesadvantageoffasttrainingandinference.
To this end, we present a Chimera, a three-headed two-dimensional State Space Model (SSM) that is based on linear
layers along (i) time, (ii) variates, (iii) time→variate, and (iv) variate→time. Chimera has a careful parameterization
based on the pair of companion and diagonal matrices (see Figure 1), which is provably expressive to recover both
2classicalmethods(Bartholomew1971;BenderandSimonovic1994;Winters1960),linearattentions,andrecentSSM-based
models(Behrouz,Santacatterina,andZabih2024;Nguyenetal.2022).Itfurtherusesanadaptivemodulebasedona2DSSM
withanespeciallydesigneddiscretizationprocesstocaptureseasonalpatterns.Whileourtheoreticalresultsanddesign
ofChimeraguaranteethefirstthreecriteriaofaneffectivemodel,duetoits2Drecurrence,thenaiveimplementation
ofChimeraresultsinslowtraining. Toaddressthisissue,wereformulateits2Drecurrenceastheprefixsumproblem
with a 2-dimensional associative operators. This new formulation can be done in parallel and has hardware-friendly
implementation,resultinginmuchfastertrainingandinference.
Wediscussnewvariantsofour2DSSMinSection3.2bylimitingitstransitionmatrices.Theresultedmodelscanbeseen
asthegeneralizationofMamba(GuandDao2023)andMamba-2(DaoandGu2024)to2-dimensionaldata.Whilethemain
focusofthispaperisontimeseriesdata,thesepresentedmodelsduetotheir2Dinductivebiasarepotentiallysuitablefor
otherhighdimensionaldatamodalitiessuchasimages,videos,andmulti-channelaudio.
Inourexperimentalevaluation,weexploretheperformanceofChimerainawiderangeoftasks:ECGandaudiospeechtime
seriesclassification,long-andshort-termtimeseriesforecasting,andanomalydetectiontasks.WefindthatChimeraachieve
superiororonparperformancewithstate-of-the-artmethods,whilehavingfastertrainingandlessmemoryconsumption.
We perform a case study on the human brain activity signals (Behrouz, Delavari, and Hashemi 2024) to show (1) the
effectivenessofChimeraand(2)evaluatetheimportanceofmodelingthedynamicsofthevariatesdependencies.
2 Preliminaries
Notations.Inthispaperwemainlyfocusonclassificationandforecastingtasks.Notethatanomalydetectioncanbeseen
asabinaryclassificationtask,where0means“normall”and1means“anomaly”.WeletX= {x1,...,x𝑁} ∈R𝑁×𝑇 bethe
inputsequences,where𝑁 isthenumberofvariatesand𝑇 isthetimesteps.Weusex𝑣,𝑡 torefertothevalueoftheseries𝑣
attime𝑡.Inclassification(anomalydetection)tasks,weaimtoclassifyinputsequencesandforforecastingtasks,givenan
inputsequencex𝑖,weaimtopredictx˜
𝑖
∈R1×𝐻,i.e.,thenext𝐻 timestepsforvariatex𝑖,where𝐻 iscalledhorizon.In2D
SSMsformulation,fora2-dimensionalvector𝑥 ∈C1,weuse𝑥(1) and𝑥(2) torefertoitsrealandimaginarycomponents,
respectively.
Multi-DimensionalStateSpaceModels.WebuildourapproachonthecontinuousStateSpaceModel(SSM)butlater
wemakeeachcomponentofChimeradiscretebyadesigneddiscretizationprocess.Foradditionaldiscussionon1DSSMs
seeAppendixA.GivenparametersA𝜏1 ∈R𝑁(𝜏1)×𝑁(𝜏1),B𝜏2 ∈C𝑁(𝜏2)×1,andC∈C𝑁1×𝑁2 for𝜏 1 ∈ {1,...,4}and𝜏 2 ∈ {1,2},the
generalformofthetime-invariant2DSSMisthemapx∈C1 ↦→y∈C1definedbythelinearPartialDifferentialEquation
(PDE)withinitialconditionℎ(0,0) =0:
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ 𝑡(1),𝑡(2) = A1ℎ(1) 𝑡(1),𝑡(2) ,A2ℎ(2) 𝑡(1),𝑡(2) +B1x 𝑡(1),𝑡(2) , (1)
𝜕𝑡(1)
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ 𝑡(1),𝑡(2) = A3ℎ(1) 𝑡(1),𝑡(2) ,A4ℎ(2) 𝑡(1),𝑡(2) +B2x 𝑡(1),𝑡(2) , (2)
𝜕𝑡(2)
(cid:16) (cid:17) (cid:16) (cid:17)
y 𝑡(1),𝑡(2) = ⟨C,x 𝑡(1),𝑡(2) ⟩. (3)
Contrarytothemulti-dimensionalSSMsdiscussedbyGuandDao(2023)andGu,Goel,andRe(2022),inwhichmulti-
dimensionreferstothedimensionoftheinputbutwithonetimevariable, theaboveformulationusestwovariables,
meaningthatthemappingisfroma2Dgridtoa2Dgrid.
(Seasonal)AutoregressiveProcess. Autoregressiveprocessisabasicyetessentialpremisefortimeseriesmodeling,
whichmodelsthecausalnatureoftimeseries.Given𝑝 ∈N,x𝑘 ∈R𝑑,thesimplelinearautoregressiverelationshipsbetween
x𝑘 anditspastsamplesx𝑘−1,x𝑘−2,...,x𝑘−𝑝 canbemodeledasx𝑘 =𝜙 1x𝑘−1+𝜙 2x𝑘−2+...,𝜙 𝑝x𝑘−𝑝,where𝜙 1,...,𝜙 𝑝 are
coefficients. ThisiscalledAR(𝑝). Similarly,inthepresenceofseasonalpatterns,theseasonalautoregressiveprocess,
SAR(𝑝,𝑞,𝑠),is:
x𝑘 =𝜙 1x𝑘−1+𝜙 2x𝑘−2+...,𝜙 𝑝x𝑘−𝑝+𝜂 1x𝑘−𝑠 +𝜂 2x𝑘−2𝑠 +···+𝜂 𝑞x𝑘−𝑞𝑠, (4)
where𝑠 isthefrequencyofseasonality,and𝜙 ,...,𝜙 and𝜂 ,...,𝜂 arecoefficients.Notethatonecansimplyextendthe
1 𝑝 1 𝑞
aboveformulationtomultivariatetimeseriesbylettingcoefficientstobevectorsandreplacetheproductwithelement-wise
product.
33 Chimera: A Three-headed 2-Dimensional State Space Model
Inthissection,wefirstpresentamathematicalmodelformultivariatetimeseriesdataandthenbasedonthismodel,we
presentaneuralarchitecturethatcansatisfyallthecriteriadiscussedin§1.
3.1 Motivations&ChimeraModel
SSMshavebeenlong-standingmethodsformodelingtimeseries(Aoki2013;Harvey1990),mainlyduetotheirsimplicity
andexpressivepowertorepresentcomplicatedandautoregressivedependencies.Theirstates,however,arethefunctionof
asingle-variable(e.g.,time).Multivariatetimeseries,ontheotherhand,requirecapturingdependenciesalongbothtime
andvariatedimensions,requiringthecurrentstateofthemodeltobethefunctionofbothtimeandvariate.Classical2D
SSMs(Eising1978;FornasiniandMarchesini1978;Hinamoto1980;Kungetal.1977),however,struggletoachievegood
performancecomparedtorecentadvanceddeeplearningmethodsastheyare:(1)onlyabletocapturelineardependencies,
(2)discretebydesign, havingapre-determinedresolution, andsocannotsimplymodelseasonalpatterns, (3)slowin
practiceforlargedatasets,(4)theirupdateparametersarestaticandcannotcapturethedynamicsofdependencies.Deep
learning-basedmethods(S.-A.Chenetal.2023;YongLiu,Hu,etal.2024;H.Zhouetal.2021),ontheotherhand,potentially
areabletoaddressasubsetoftheabovelimitations,whilehavingtheirowndrawbacks(discussedin§1).Inthissection,
westartwithcontinuousSSMsduetotheirconnectiontobothclassicalmethods(Aoki2013;Harvey1990)andrecent
breakthroughindeeplearning(GuandDao2023;Gu,Goel,andRe2022).Wethendiscussourcontributionsonhowto
taketheadvantagesofthebestofbothworlds,addressingalltheabovementionedlimitations.
Discrete2DSSM.Weuse2-dimensionalSSMs,introducedinEquation1-3,tomodelmultivariatetimeseries,wherethe
firstaxiscorrespondstothetimedimensionandthesecondaxisisthevariates.Accordingly,eachstateisafunctionofboth
timeandvariates.Thefirststageistotransformthecontinuousformof2DSSMstodiscreteform.GiventhestepsizeΔ
1
andΔ 2,whichrepresenttheresolutionoftheinputalongtheaxes,discreteformoftheinputisdefinedasx𝑘,ℓ =x(𝑘Δ 1,ℓΔ 2).
UsingZero-OrderHold(ZOH)method,wecandiscretizetheinputas(seeAppendixCfordetails):
(cid:32) ℎ ℎ𝑘 𝑘( (1 2, +ℓ)
)
1+ ,1 ℓ(cid:33) = (cid:18) AA ¯¯ 31 AA ¯¯ 42(cid:19) (cid:32) ℎ ℎ𝑘 𝑘( (1 2, ,ℓ ℓ) )(cid:33) + (cid:18) B B¯ ¯1 2(cid:19) ⊗ (cid:18) x x¯ ¯𝑘 𝑘, +ℓ 1+ ,1 ℓ(cid:19) , (5)
whereA¯ 𝑖 =exp(cid:16) Δ ⌊𝑖+ 21⌋A𝑖(cid:17) for𝑖 =1,2,3,4,B¯ 1 = (cid:34) AA 21 −− 11 (cid:0)(cid:0) AA ¯¯ 21 −− II (cid:1)(cid:1) BB 11 (( 21 ))(cid:35) ,andB¯ 2 = (cid:34) AA 43 −− 11 (cid:0)(cid:0) AA ¯¯ 43 −− II (cid:1)(cid:1) BB 22 (( 21 ))(cid:35) .
NotethatthisformulationcanalsobeviewedasthemodificationofthediscreteRoesser’sSSMmodel(Kungetal.1977)
whenweaddalagof1intheinputs(cid:18) x¯ 𝑖,𝑗(cid:19)
.Thismodification,however,missesthediscretizationstep,whichisanimportant
x¯
𝑖,𝑗
stepinourmodel. Welaterusethediscretizationstepto(1)empowerthemodeltoselect(resp. filter)relevant(resp.
irrelevant)information,(2)adaptivelyadjusttheresolutionofthemethod,capturingseasonalpatterns.
Fromnowon,weuse𝑡 (resp.𝑣)torefertotheindexalongthetime(resp.variate)dimension.Therefore,forthesakeof
simplicity,wereformulateEquation5asfollows:
ℎ 𝑣( ,1 𝑡)
+1
=A¯ 1ℎ 𝑣( ,1 𝑡) +A¯ 2ℎ 𝑣( ,2 𝑡) +B¯ 1x𝑣,𝑡+1, (6)
ℎ 𝑣(2 +)
1,𝑡
=A¯ 3ℎ 𝑣( ,1 𝑡) +A¯ 4ℎ 𝑣( ,2 𝑡) +B¯ 2x𝑣+1,𝑡, (7)
y𝑣,𝑡 =C1ℎ 𝑣( ,1 𝑡) +C2ℎ 𝑣( ,2 𝑡), (8)
where A¯ 1,A¯ 2,A¯ 3,A¯ 4 ∈ R𝑁×𝑁, B¯ 1,B¯ 2 ∈ R𝑁×1, and C1,C2 ∈ R1×𝑁 are parameters of the model,ℎ 𝑣( ,1 𝑡),ℎ 𝑣( ,2 𝑡) ∈ R𝑁×𝑑 are
hiddenstates,andx𝑣,𝑡 ∈R1×𝑑 istheinput.Inthisformulation,intuitively,ℎ 𝑣( ,1 𝑡) isthehiddenstatethatcarriescross-time
information(eachstatedependsonitsprevioustimestampbutwithinthesamevariate),whereA¯
1
andA¯
2
controlthe
emphasisonpastcross-timeandcross-variateinformation,respectively. Similarly,ℎ(2) isthehiddenstatethatcarries
𝑣,𝑡
cross-variateinformation(eachstatedependsonothervariatesbutwiththesametimestamp).Laterinthissection,we
discusstomodifythemodeltobi-directionalsettingalongthevariatedimension,toenhanceinformationflowalongthis
non-causaldimension.
4Figure2: DifferentformsofChimera.(Top-Left)Chimerahasarecurrenceform(bi-directionalalongthevariates),whichalsocan
becomputedasaglobalconvolutionintraining.(Top-Right)Inforecasting,wepresentthemultivariateclosed-looptoimprovethe
performanceforlonghorizons.(Bottom)Usingdata-dependentparameters,Chimeratrainingcanbedoneasaparallel2Dscan.
InterpretationofDiscretization.Timeseriesdataareoftensampledfromanunderlyingcontinuousprocess(Hebartetal.
2023;Warden2018).Inthesecases,variableΔ inthediscretizationofthetimeaxiscanbeinterpretedasresolutionorthe
1
samplingratefromtheunderlyingcontinuousdata.However,discretizationalongthevariateaxis,whichisdiscretebyits
nature,orwhenworkingdirectlywithdiscretedata(A.E.Johnsonetal.2023)isanunintuitiveprocess,andraisequestions
aboutitssignificance.Thediscretizationstepin1DSSMshasdeepconnectionstogatingmechanismsofRNNs(Guand
Dao2023;TallecandOllivier2018),automaticallyensuresthatthemodelisnormalized(Gu,I.Johnson,Timalsina,etal.
2023),andresultsindesirablepropertiessuchasresolutioninvariance(Nguyenetal.2022).
Proposition3.1. The2DdiscreteSSMintroducedinEquation6-8withparameters({A¯ 𝑖},{B¯ 𝑖},{C¯ 𝑖},𝑘Δ 1,ℓΔ 2)evolvesatarate
𝑘 (resp.ℓ)timesasfastasthe2DdiscreteSSMwithparameters({A¯ 𝑖},{B¯ 𝑖},{C¯ 𝑖},Δ 1,ℓΔ 2)(resp. ({A¯ 𝑖},{B¯ 𝑖},{C¯ 𝑖},𝑘Δ 1,Δ 2)).
Accordingly,parametersΔ canbeviewedasthecontrollerofthelengthofdependenciesthatthemodelcaptures.Thatis,
1
basedontheaboveresult,weseethediscretizationalongthetimeaxisasthesettingoftheresolutionorsamplingrate:
whilesmallΔ cancapturelong-termprogression,largerΔ capturesseasonalpatterns.Fornow,weseethediscretization
1 1
alongthevariateaxisasamechanismsimilartogatinginRNNs(GuandDao2023;Gu,Gulcehre,etal.2020),whereΔ
2
controlsthelengthofthemodelcontext.LargervaluesofΔ meanslesscontextwindow,ignoringothervariates,while
2
smallervaluesofΔ meansmoreemphesisonthedependenciesofvariates. Later,inspiredbyGuandDao(2023),we
2
discussmakingΔ asthefunctionoftheinput,resultinginaselectionmechanismthatfiltersirrelevantvariates.
2
StructureofTransitionMatrices.ForChimeratobeexpressiveandabletorecoverautoregressiveprocess,hiddenstates
ℎ 𝑣( ,1 𝑡) shouldcarryinformationaboutpast timestamps.WhilemakingalltheparametersinA𝑖 learnableallowsthemodel
tolearnanyarbitrarystructureforA𝑖,previousstudiesshowthatthisisnotpossibleunlessthestructureoftransition
matricesarerestricted(Gu,Goel,Gupta,etal.2022;Gu,I.Johnson,Goel,etal.2021).Tothisend,inspiredbyM.Zhang
etal.(2023)thatarguethatcompanionmatricesareeffectivetocapturethedependenciesalongthetimedimension,we
restrictA1andA2matricestohavecompanionstructure:
0 0 ... 0 𝑎(𝑖) 0 0 ... 0 0 0 0 ... 0 𝑎(𝑖)
1 1
A𝑖 =(cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173)1 0 0. . . 0 1 0. . . . . .... . ... .
.
0 0 1. . . 𝑎 𝑎 𝑎2 3( ( (. . .𝑖 𝑖 𝑖) ) )(cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174)=(cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173)1 0 0. . . 0 1 0. . . . . .... . ... .
.
0 0 1. . . 0 0 0. . .(cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174)+(cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173)0 0 0. . . 0 0 0. . . . . .... . ... .
.
0 0 0. . . 𝑎 𝑎 𝑎2 3( ( (. . .𝑖 𝑖 𝑖) ) )(cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174), (9)
(cid:171) 𝑁 (cid:172) (cid:171) (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125)(cid:172) (cid:171)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝑁 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:172)
(cid:124) (cid:123)(cid:122) (cid:125)
ShiftMatrix
Low-rankMatrix
5for𝑖 =1,2.Notethatthesetwomatricesareresponsibletofusetheinformationalongthetimeaxis(seeFigure2).Notonly
thisformulationisshowntobeeffectiveforcapturingdependenciesalongthetimedimension(M.Zhangetal.2023)(also
seeTheorem3.4),butitalsocanhelpustocomputethepowerofA1andA2fasterintheconvolutionalform,asdiscussed
byM.Zhangetal.(2023).Also,forA3andA4,weobservethatevenasimplerstructureofdiagonalmatricesiseffectiveto
fuseinformationalongthevariatedimension.Notonlythesesimplestructuredmatricesmakethetrainingofthemodel
faster,buttheyalsoareproventobeeffective(Gu,Goel,Gupta,etal.2022).
Bi-Directionality. Thecausalnatureofthe2DSSMresultinlimitedinformationflowalongthevariatedimensionas
variatearenotordered.Toovercomethischallenge,inspiredbythebi-directional1DSSMs(J.Wangetal.2023),weuse
twodifferentmodulesforforwardandbackwardpassalongthevariatedimension:
ℎ 𝑣( ,1 𝑡) +𝑓
1
=A¯ 1𝑓ℎ 𝑣( ,1 𝑡) +A¯ 2𝑓ℎ 𝑣( ,2 𝑡)𝑓 +B¯ 1𝑓 x𝑣,𝑡+1,
ℎ 𝑣( ,1 𝑡) +𝑏
1
=A¯𝑏 1ℎ 𝑣( ,1 𝑡) +A¯𝑏 2ℎ 𝑣( ,2 𝑡) +B¯𝑏 1x𝑣,𝑡+1, (10)
ℎ 𝑣(2 +) 1𝑓
,𝑡
=A¯ 3𝑓ℎ 𝑣( ,1 𝑡) +A¯ 4𝑓ℎ 𝑣( ,2 𝑡)𝑓 +B¯ 2𝑓 x𝑣+1,𝑡,
ℎ 𝑣(2 −) 1𝑏
,𝑡
=A¯𝑏 3ℎ 𝑣( ,1 𝑡)𝑏 +A¯𝑏 4ℎ 𝑣( ,2 𝑡)𝑏 +B¯𝑏 2x𝑣−1,𝑡, (11)
y𝑓 =C𝑓ℎ(1)𝑓 +C𝑓ℎ(2)𝑓 , (12)
𝑣,𝑡 1 𝑣,𝑡 2 𝑣,𝑡
y𝑏
𝑣,𝑡
=C𝑏 1ℎ 𝑣( ,1 𝑡)𝑏 +C𝑏 2ℎ 𝑣( ,2 𝑡)𝑏 , (13)
y𝑣,𝑡 =y 𝑣𝑓
,𝑡
+y𝑏 𝑣,𝑡, (14)
whereA¯𝜏,A¯𝜏,A¯𝜏,A¯𝜏 ∈ R𝑁×𝑁,B¯𝜏,B¯𝜏 ∈ R𝑁×1,andC𝜏,C𝜏 ∈ R1×𝑁 areparametersofthemodel,ℎ(1)𝜏 ,ℎ(2)𝜏 ∈ R𝑁×𝑑 are
1 2 3 4 1 2 1 2 𝑣,𝑡 𝑣,𝑡
hiddenstates,x𝑣,𝑡 ∈R1×𝑑 istheinput,and𝜏 ∈ {𝑓,𝑏}.Figure2illustratesthebi-directionalrecurrenceprocessinChimera.
Forthesakeofsimplicity,wecontinuewithunidirectionalpass,butadaptingthemforbi-directionalsettingissimpleas
weusetwoseparateblocks,eachofwhichforadirection.
ConvolutionForm. Similarto1DSSMs(Gu,Goel,andRe2022),ourdata-independent formulationcanbeviewedas
aconvolutionwithakernelK. Thisformulationnotonlyresultsinfastertrainingbyprovidingtheabilityofparallel
processing, but it also connect Chimera with very recent studies of modern convolution-based architecture for time
series(LuoandX.Wang2024).ApplyingtherecurrentrulesinEquation6-8,wecanwritetheoutputas:
y𝑣,𝑡 = ∑︁ ∑︁ (cid:16) C1K 𝑣( ˆ,1 𝑡ˆ) +C2K 𝑣( ˆ,2 𝑡ˆ)(cid:17) x𝑣ˆ,𝑡ˆ, (15)
1≤𝑣ˆ≤𝑣1≤𝑡ˆ≤𝑡
wherekernelsK 𝑣( ˆ𝜏 ,𝑡ˆ) =(cid:205) (𝑧1,...,𝑧5)∈P(𝜏)𝑞
𝑖
A¯𝑝 11A¯𝑝 22A¯𝑝 33A¯𝑝 44B¯ 𝑝5,andP(𝜏) isthepartitioningofthepathsfromthestartingpoint
to (𝑣ˆ,𝑡ˆ) for𝜏 ∈ {1,2}. As discussed by Baron, Zimerman, and Wolf (2024), if the power of A¯ 𝑖s are given and cached,
calculatingthepartitioningofallpathscanbedoneveryefficiently(near-linearly)asitthegeneralizationofpascaltriangle.
TocalculatethepowerofA¯ 𝑖,notethatweusediagonalmatricesasthestructureofA¯ 3,andA¯ 4,andsocomputingtheir
powersisveryfast.Ontheotherhand,forA¯ 1andA¯ 2withcompanionstructures,wecanusesparsematrixmultiplication,
whichresultsinlinearcomplexityintermsofthesequencelength.
Data-DependentParameters. Asdiscussedearlier,parametersA¯ 1 andA¯ 2 controlstheemphasisonpastcross-time
andcross-variateinformation.Similarly,parametersΔ 1andB¯ 1controlstheemphasisonthecurrentinputandhistorical
data.Sincetheseparametersaredata-independent,onecaninterpretthemasaglobalfeatureofthesystem.Incomplex
systems(e.g.,humanneuralactivity),however,theemphasisdependsonthecurrentinput,requiringtheseparametersto
bethefunctionoftheinput(see§4.1).Theinput-dependencyofparametersallowsthemodeltoselectrelevantandfilter
irrelevantinformationforeachinputdata,providingasimilarmechanismastransformers(GuandDao2023).Additionally,
asweargueearlier,dependingonthedata,themodelneedstoadaptivelylearnifmixinginformationalongthevariates
isuseful.Makingparametersinput-dependentfurtherovercomesthischallengeandletsourmodeltomixrelevantand
filterirrelevantvariatesforthemodelingofavariateofinterest.OneofourmaintechnicalcontributionsistoletB¯ 𝑖,C¯ 𝑖,
andΔ 𝑖 for𝑖 ∈ {1,2}bethefunctionoftheinputx𝑣,𝑡. Thisinput-dependent2DSSM,unfortunately,doesnothavethe
convolutionform,limitingthescalabilityandefficiencyofthetraining.Weovercomethischallengebycomputingthe
modelrecurrentlywithanew2Dscan.
62DSelectiveScan.Inspiredbythescanningin1DSSMs(GuandDao2023;Smith,Warrington,andLinderman2023),we
presentanalgorithmtodecreasethesequentialstepsthatarerequiredtocalculatehiddenstates.Given𝑝,𝑞,eachofwhich
with6elements,wefirstdefineoperation⋇as:(⊙ismatrix-matrixand⊗ismatrix-vectormultiplication)
𝑝⋇𝑞 =
(cid:18)𝑝
1
𝑝
2
𝑝 3(cid:19)
⋇
(cid:18)𝑞
1
𝑞
2
𝑞 3(cid:19)
=
(cid:18)𝑞
1
⊙𝑝
1
𝑞
2
⊙𝑝
2
𝑞
1
⊗𝑝 3+𝑞
2
⊗𝑝 6+𝑞 3(cid:19)
𝑝
4
𝑝
5
𝑝
6
𝑞
4
𝑞
5
𝑞
6
𝑞
4
⊙𝑝
4
𝑞
5
⊙𝑝
5
𝑞
4
⊗𝑝 3+𝑞
5
⊗𝑝 6+𝑞
6
TheproofsofthenexttwotheoremsareinAppendixE.
Theorem3.2. Operator⋇isassociative:Given𝑝,𝑞,and𝑟,wehave: (𝑝⋇𝑞)⋇𝑟 =𝑝⋇(𝑞⋇𝑟).
Theorem3.3. 2DSSMrecurrencecanbedoneinparallelusingparallelprefixsumalgorithmswithassociativeoperator⋇.
3.2 NewVariantsof2DSSM:2DMambaand2DMamba-2
Figure2(Top-Left)showstherecurrenceformofour2DSSM.Eachsmallsquareisastateofthesystem,i.e.,thestateofa
variateatacertaintimestamp.2DSSMconsiderstwohiddenstatesforeachstate(representedbytwocolors:lightredand
blue),encodingtheinformationalongthetime(red)andvariate(blue),respectively.Furthermore,eacharrowrepresentsa
transitionmatrixA𝑖 thatdecidestohowinformationneedtobefused.Inthissection,wediscussdifferentvariantsofour
2DSSMbylimitingitsparameters.
2DMamba.WeletA2 =A3 =0intheformulationofour2DSSM.Theresultingmodelisequivalentto:
ℎ 𝑣( ,1 𝑡)
+1
=A¯ 1ℎ 𝑣( ,1 𝑡) +B¯ 1x𝑣,𝑡+1, (16)
ℎ 𝑣(2 +)
1,𝑡
=A¯ 4ℎ 𝑣( ,2 𝑡) +B¯ 2x𝑣+1,𝑡, (17)
y𝑣,𝑡 =C1ℎ 𝑣( ,1 𝑡) +C2ℎ 𝑣( ,2 𝑡), (18)
whereA¯ 1 =exp(Δ 1A1),A¯ 2 =exp(Δ 2A2),B¯ 1 = (cid:20) A 1−1(cid:0) A¯ 1 0−I(cid:1) B 1(1)(cid:21) ,andB¯ 2 = (cid:20)
A
4−1(cid:0) A¯ 40 −I(cid:1)
B
2(2)(cid:21) .Thisformulationwith
data-dependentparameters,isequivalenttousingtwoS6blocks(GuandDao2023)eachofwhichalongadimension.
Notably,thesetwoS6blocksarenotseparateastheoutputy𝑣,𝑡 isbasedonbothhiddenstatesℎ 𝑣( ,1 𝑡) andℎ 𝑣( ,2 𝑡),capturing2D
inductivebias.
2DMamba-2.Recently,DaoandGu(2024)presentMamba-2thatre-formulatesS6blockusingstructuredsemi-separable
matrices,resultinginmoreefficienttrainingandabilityofhavinglargerrecurrentstatesizes. Althoughweleavethe
explorationofhowgeneric2DSSMscanbere-formulatedbytensors(seeSection5forfurtherdiscussion),thespecialcase
ofA2 =A3 =0,similartotheaboveformulation,canbere-formulatedastwoSSDblocks(DaoandGu2024)eachofwhich
alongadimension.Furthermore,forbi-directionalityalongthevariates,onecanusequasi-separablestructuredmatrices,
whichinherentlycapturesbi-directionalityasdiscussedbyBehrouz,Santacatterina,andZabih(2024):
y𝑣,𝑡
=(cid:169)
(cid:173) (cid:173)
(cid:173)
C1C 𝑣,21 A𝑣,1
1 . .
.B¯ 𝑣,21 B𝑣¯,1
1𝑣,1
C1𝑣,20
. .
.B¯
1𝑣,2
. . ... . .. . 0 0
. .
.
(cid:170)
(cid:174) (cid:174) (cid:174)x𝑣,: (19)
(cid:173) (cid:174)
(cid:171)C (cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:0) (cid:32)(cid:32)(cid:206) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝑖𝑡 (cid:32)(cid:32)=(cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)(cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑖(cid:32)(cid:32)(cid:32)(cid:1) (cid:32)(cid:32)(cid:32)B (cid:32)¯ (cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)1(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)C (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:0) (cid:32)(cid:206) 𝑖𝑡 (cid:32)(cid:32)=(cid:32)(cid:32)(cid:32)3 (cid:32)(cid:32)(cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑖(cid:32)(cid:32)(cid:1) (cid:32)(cid:32)(cid:32)(cid:32)B¯ (cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)2(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32). (cid:32)(cid:32)(cid:32). (cid:32)(cid:32)(cid:32). (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)C (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑡(cid:32)(cid:32)(cid:32)B¯ (cid:32)(cid:32)(cid:32)(cid:32)1 (cid:32)(cid:32)𝑣(cid:32)(cid:32),(cid:32)𝑡(cid:32)(cid:172)
(cid:124) (cid:123)(cid:122) (cid:125)
SSDBlock
(cid:169)
𝛾 1 C 2′ 𝑣−1,𝑡A 4′ 𝑣−1,𝑡B¯ 2′
𝑣,𝑡
... C 2′
𝑣,𝑡
(cid:16) (cid:206) 𝑖𝑣 =− 11 A 4′ 𝑖,𝑡(cid:17) B¯ 2′
1,𝑡 (cid:170)
+(cid:173) (cid:173) (cid:173) C22,𝑡A42,𝑡B¯ 21,𝑡 𝛾 2 ... C 2′ 𝑣−1,𝑡 (cid:16) (cid:206) 𝑖𝑣 =− 21 A 4′ 𝑖,𝑡(cid:17) B¯ 2′ 2,𝑡(cid:174) (cid:174) (cid:174)x:,𝑡, (20)
(cid:173)
(cid:173)
. .
.
. .
.
... . .
.
(cid:174)
(cid:174)
(cid:173) (cid:174)
(cid:171)C (cid:32)(cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)(cid:32)𝑣(cid:32)(cid:32),𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:0) (cid:32)(cid:32)(cid:206) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝑖 (cid:32)𝑣 (cid:32)=(cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)(cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)4 (cid:32)(cid:32)𝑖(cid:32)(cid:32),𝑡(cid:32)(cid:32)(cid:32)(cid:1) (cid:32)(cid:32)(cid:32)(cid:32)B (cid:32)¯ (cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)(cid:32)1(cid:32),(cid:32)𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)C (cid:32)(cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)(cid:32)𝑣(cid:32)(cid:32),𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:0) (cid:32)(cid:32)(cid:206) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝑖𝑣 (cid:32)(cid:32)=(cid:32)(cid:32)(cid:32)3 (cid:32)(cid:32)(cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)4𝑖,𝑡(cid:1) (cid:32)(cid:32)(cid:32)(cid:32)B (cid:32)¯ (cid:32)(cid:32)(cid:32)2 (cid:32)(cid:32)2(cid:32)(cid:32),(cid:32)𝑡(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32). (cid:32)(cid:32). (cid:32)(cid:32)(cid:32). (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝛾 (cid:32)(cid:32)(cid:32)(cid:32)𝑣
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:172)
(cid:124) (cid:123)(cid:122) (cid:125)
Quasi-SeparableBlock
wherex𝑣,:andx:,𝑡 arethevectorswhenwefix𝑣 and𝑡 ininputx,respectively.
73.3 ChimeraNeuralArchitecture
In this section, we use a stack of our 2D SSMs, with non-linearity in between, to enhance the expressive power and
capabilitiesoftheabovementioned2DSSM.Tothisend,similartodeepSSMmodels(M.Zhangetal.2023),weallowall
parameterstobelearnableandineachlayerweusemultiple2DSSMs,eachofwhichwithitsownresponsibility.Also,in
thedata-dependentvariantofChimera,weletparametersB𝑖,C𝑖,andΔ 𝑖 for𝑖 ∈ {1,2}bethefunctionoftheinputx:
B𝑖 =Linear B𝑖(𝑥), C𝑖 =Linear C𝑖(𝑥), Δ 𝑖 =Softplus(cid:0) LinearΔ𝑖(𝑥)(cid:1). (21)
Chimerafollowsthecommonlyuseddecompositionoftimeseries,anddecomposesthemintotrendcomponentsand
seasonalpatterns.it,however,usesspecialtraitsof2DSSMtocapturetheseterms.
SeasonalPatterns.Tocapturethemulti-resolutionseasonalpatterns,wetakeadvantageofthediscretizationprocess.
Proposition 3.1 states that if x(𝑣,𝑡) ↦→ y(𝑣,𝑡) with parameters ({A¯ 𝑖},{B¯ 𝑖},{C¯ 𝑖},Δ 1,Δ 2) then x(𝑣,𝑘𝑡) ↦→ y(𝑣,𝑘𝑡) with
({A¯ 𝑖},{B¯ 𝑖},{C¯ 𝑖},𝑘Δ 1,Δ 2).Accordingly,weuse2D-SSM(.)modulewithaseparatelearnableΔ
𝑠
thatisresponsibletolearn
thebestresolutiontocaptureseasonalpatterns.AnotherinterpretationforthismoduleisbasedonSAR(𝑝,𝑞,𝑠)(Equation4).
Inthiscase,Δ aimstolearnaproperparameter𝑠 tocaptureseasonalpatterns.Sinceweexpecttheresolutionbeforeand
𝑠
afterthismodulematches,weaddadditionalre-discretizationmodule(asimplelinearlayer),afterthismodule.
TrendComponents.ThesecondmoduleofChimera,2D-SSM𝑡 (.)simplyusesasequenceofmultiple2DSSMstolearn
trendcomponents. Propercombinationoftheoutputsofthisandthepreviousmodulescancapturebothseasonaland
trendcomponents.
Both Modules Together. We followed previous studies (Toner and Darlow 2024) and consider residual connection
modelingforlearningtrendandseasonalpatterns.GiveninputdataX˜
0
=X,andℓ =0,...,L,wehave:
Xˆ ℓ+1 =2D-SSM𝑡 (cid:16) X˜ ℓ(cid:17) , (22)
X˜ ℓ+1 =Re-Discretization(cid:16) 2D-SSM𝑠 (cid:16) X˜ ℓ −Xˆ ℓ+1(cid:17)(cid:17) . (23)
Figure1illustratethearchitectureofChimera. Duetotheabilityofour2DSSMtorecoversmoothingtechniques(see
Theorem3.4),thiscombinationofmodulesfortrendandseasonalpatternscanbeviewedasageneralizationoftraditional
methodsthatusemovingaveragewithresidualconnectiontomodelseasonality(TonerandDarlow2024).
GatingwithLinearMapping.InspiredbythesuccessofgatedrecurrentandSSM-basedmodels(GuandDao2023;Qin,
Yang,andZhong2023),weuseaheadofafullyconnectedlayerwithSwish(Ramachandran,Zoph,andLe2017),resulting
inSwiGLUvariant(Touvronetal.2023).Whilewevalidatethesignificanceofthishead,this
Closed-Loop2DSSMDecoder. Toenhancethegeneralizabilityandtheabilityofourmodelforlonger-horizon,we
extendtheclosed-loopdecodermodule(M.Zhangetal.2023),whichissimilartoautoregression,tomultivariatetime
series.Weusedistinctprocessesfortheinputsandoutputs,usingadditionalmatricesD1andD2ineachdecoder2DSSM,
wemodelfutureinputtime-stepsexplicitly:
y𝑣,𝑡 =C1ℎ 𝑣( ,1 𝑡) +C2ℎ 𝑣( ,2 𝑡), (24)
u𝑣,𝑡 =D1ℎ 𝑣( ,1 𝑡) +D2ℎ 𝑣( ,2 𝑡), (25)
whereu𝑣,𝑡 isthenextinputandy𝑣,𝑡 istheoutput.Notethattheotherparts(recurrence)arethesameasEquation6.Figure2
illustratethearchitectureofclosed-loop2DSSM.
3.4 TheoreticalJustification
Inthissection,weprovidesometheoreticalevidencesfortheperformanceofChimera.Theseresultsaremostlyrevisiting
the theorems by M. Zhang et al. (2023) and Baron, Zimerman, and Wolf (2024), and extending them for Chimera. In
thefirsttheorem, weshowthatChimerarecoversseveralclassicmethods, andpre-processingstepsasitcanrecover
SpaceTime(M.Zhangetal.2023)andadditionallybecauseofitsdesign,itcanrecoverSARIMA(BenderandSimonovic
1994):
8Theorem3.4. Chimeracanrepresentseasonalautoregressiveprocess,SARIMA(BenderandSimonovic1994),SpaceTime(M.
Zhang et al. 2023), and so ARIMA (Bartholomew 1971), exponential smoothing (Winters 1960), and controllable linear
time–invariantsystems(C.-T.Chen1984).
Theorem 3.5. Chimera can represent S4nd (Nguyen et al. 2022), TSM2 (Behrouz, Santacatterina, and Zabih 2024), and
TSMixer(S.-A.Chenetal.2023).
NexttheoremcomparestheexpressivenessofChimerawithsomeexisting2DdeepSSMs. SinceChimeracanrecover
2DSSM(Baron,Zimerman,andWolf2024),itcanexpressfull-rankkernelswithaconstantnumberofparameters:
Theorem 3.6. Similar to 2DSSM (Baron, Zimerman, and Wolf 2024), Chimera can express full-rank kernels with O(1)
parameters,whileexistingdeepSSMs(Behrouz,Santacatterina,andZabih2024;Nguyenetal.2022)requireO(𝑁)parameters
toexpress𝑁-rankkernels.
4 Experiments
GoalsandBaselines.WeevaluateChimeraonawiderangeoftimeseriestasks.In§4.1wecompareChimerawiththe
state-of-the-artgeneralmultivariatetimeseriesmodels(Behrouz,Santacatterina,andZabih2024;Dasetal.2023;Limand
Zohren2021;M.Liuetal.2022;YongLiu,Hu,etal.2024;LuoandX.Wang2024;BadriN.PatroandVijayS.Agneeswaran
2024;Wooetal.2022;H.Wu,Hu,etal.2023;H.Wu,Xu,etal.2021;Y.ZhangandYan2023;T.Zhou,Z.Ma,Wen,X.Wang,
etal.2022)onlong-termforecastingandclassificationtasks. Inthenextpart,wetesttheperformanceofChimerain
short-termforecasting.In§4.1weperformacasestudyonhumanneuralactivitytoclassifyseenimages,whichrequires
capturingcomplexdynamicdependenciesofvariates,totesttheabilityofChimeraincapturingcross-variateinformation
andthesignificanceofdata-dependency.In§4.2,weevaluatethesignificanceoftheChimera’scomponentsbyperforming
ablationstudies.In§4.2,weevaluatewhetherthesuperiorperformanceofChimeracoincidewithitsefficiency.Finally,
wetesttheChimera’sgeneralizabilityonunseenvariatesandfurtherevaluateitsabilitytofilterirrelevantcontextin§4.3.
ThedetailsandadditionalexperimentsareinAppendixG.
Table1: AveragePerformanceonlong-termforecastingtask.Thefirstandsecondresultsarehighlightedinred(bold)andorange
(underline).FullresultsarereportedinAppendixG.
Chimera TSM2 Simba TCN iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear
(ours) 2024 2024 2024 2024 2023 2023 2023 2023 2023 2023
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ETTm1 0.3450.377 0.361 - 0.3830.396 0.351 0.381 0.407 0.410 0.4140.4070.3870.4000.513 0.496 0.4190.4190.4000.4060.4030.407
ETTm2 0.250 0.316 0.267 - 0.2710.327 0.253 0.3140.288 0.332 0.2860.3270.2810.3260.757 0.610 0.3580.4040.2910.3330.3500.401
ETTh1 0.405 0.424 0.403 - 0.4410.432 0.404 0.4200.454 0.447 0.4460.4340.4690.4540.529 0.522 0.5410.5070.4580.4500.4560.452
ETTh2 0.3180.375 0.333 - 0.3610.391 0.322 0.379 0.383 0.407 0.3740.3980.3870.4070.942 0.684 0.6110.5500.4140.4270.5590.515
ECL 0.1540.249 0.169 - 0.1850.274 0.156 0.253 0.178 0.270 0.2190.2980.2050.2900.244 0.334 0.2510.3440.1920.2950.2120.300
Exchange 0.311 0.358 0.443 - - - 0.302 0.366 0.360 0.403 0.3780.4170.3670.4040.940 0.707 0.3700.4130.4160.4430.3540.414
Traffic 0.403 0.286 0.420 - 0.4930.2910.3980.2700.428 0.282 0.6260.3780.4810.3040.550 0.304 0.7600.4730.6200.3360.6250.383
Weather 0.2190.258 0.239 - 0.2550.280 0.224 0.264 0.258 0.278 0.2720.2910.2590.2810.259 0.315 0.2710.3200.2590.2870.2650.317
1stCount 5 5 1 - 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0
4.1 MainResults: ClassificationandForecasting
Long-TermForecasting.Weperformexperimentsinlong-termforecastingtaskonbenchmarkdatasets(H.Zhouetal.
2021).Table1reportstheaverageofresultsoverdifferenthorizons(fortheresultsofeachseeTable8).Chimerashows
outstandingperformance,achievingthebestorthesecondbestresultsinallthedatasetsandoutperformsbaselinesin5
outof8benchmarks. Notably,itsurpassesextensivelystudiedMLP-basedandTransformer-basedmodelswhilebeing
moreefficient(seeTable3,Figure4,andAppendixG),providingabetterbalanceofperformanceandefficiency.Itfurther
significantlyoutperformsrecurrentmodels,includingveryrecentMamba-basedarchitectures(Behrouz,Santacatterina,
9andZabih2024;BadriN.PatroandVijayS.Agneeswaran2024),unleashingthepotentialofclassicalmodels,SSMs,when
arecarefullydesignedindeeplearningsettings.
ClassificationandAnomalyDetection. WeevaluatetheperformanceofChimerainECGclassificationonPTB-XL
dataset (Wagner et al. 2020) (see Table 2), speech classification (Warden 2018)(Table 3), 10 multivariate datasets from
UEATimeSeriesClassificationArchive(Bagnalletal.2018)(seeFigure3andTable10),andanomalydetectiontasks
onfivewidely-usedbenchmarks: SMD(Suetal.2019),SWaT(MathurandTippenhauer2016),PSM(Abdulaal,Z.Liu,
andLancewicki2021)andSMAP(Hundmanetal.2018)(seeFigure3andTable11). Foreachbenchmark, weusethe
state-of-the-artmethodsthatareapplicabletothetaskasthebaselines.Table2reportstheperformanceofChimeraand
baselinesonECGclassificationtasks.Chimeraoutperformsallthebaselinesin4/6tasks,whileachievingthesecondbest
resultsontheremainingtasks.Sincethesetasksareunivariatetimeseries,weattributetheoutstandingperformanceof
Chimera,specificallycomparedtoSpaceTime(M.Zhangetal.2023),toitsabilityofcapturingseasonalpatternsandits
input-dependentparameters,resultingindynamicallylearndependencies.
Table3reportstheresultsonspeechaudioclassificationtask,whichrequirelong-rangemodelingoftimeseries.Dueto
thelengthofthesequence(16K),LSSL(Gu,Goel,andRe2022)andTransformer(Vaswanietal.2017)hasoutofmemory
(OOM)issue,showingtheefficiencyofChimeracomparedtoalternativebackbones.
Finally,wereportthesummaryoftheresultsinmultivariatetimeseriesclassificationandanomalydetectiontasksin
Figure3.ThefulllistofresultscanbefoundinTable10andTable11.Chimerashowsoutstandingperformance,achieving
highestaverageaccuracyandF1scoreinclassificationandanomalydetectiontasksevencomparedtoveryrecentand
state-of-the-artmethods(LuoandX.Wang2024;H.Wu,Hu,etal.2023).
Table2:ECGstatementclassificationonPTB-XL(100Hzversion). Table3:Speechclassification.
Tasks All Diag Sub-diag Super-diag Form Rhythm Method Acc.(%)
Chimera 0.941 0.947 0.935 0.930 0.901 0.975 Chimera 98.40
SpaceTime(M.Zhangetal.2023) 0.936 0.941 0.933 0.929 0.883 0.967
SpaceTime 97.29
S4(Gu,Goel,andRe2022) 0.938 0.939 0.929 0.931 0.895 0.977
S4 98.32
Inception 0.925 0.931 0.930 0.921 0.899 0.953
LSSL OOM
xRN-101 0.925 0.937 0.929 0.928 0.896 0.957
LSTM 0.907 0.927 0.928 0.927 0.851 0.953 WaveGan-D 96.25
Transformer 0.857 0.876 0.882 0.887 0.771 0.831 Transformer OOM
Short-Term Forecasting. Our evaluation on short-term forecasting tasks on M4 benchmark datasets (Godahewa et
al.2021)reportsinTable4(FulllistinTable9), whichalsoshowsthesuperiorperformanceofChimeracomparedto
baselines.
Table4:Short-termforecastingtaskontheM4dataset.FullresultsarereportedinAppendixG.
Models
ChimeraModernTCNPatchTSTTimesNetN-HiTSN-BEATS∗ ETS∗ LightTSDLinearFED∗StationaryAuto∗Pyra∗ In∗ Re∗ LSTM
(ours) 2024 2023 2023 2022 2019 2022 2022 2023 2022 2022 2021 2021 2021 2020 1997
SMAPE 11.618 11.698 11.807 11.829 11.927 11.851 14.718 13.525 13.639 12.840 12.780 12.90916.98714.08618.200160.031
MASE 1.528 1.556 1.590 1.585 1.613 1.599 2.408 2.111 2.095 1.701 1.756 1.771 3.265 2.718 4.223 25.788
OWA 0.827 0.838 0.851 0.851 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.775 12.642
CaseStudyofBrainActivity.Inputdependencyisamusttocapturethedynamicofdependencies.Tosupportthisclaim,
weuseBVFC(Behrouz,Delavari,andHashemi2024)(multivariatetimeseriesonly),whichaimtoclassifyseenimages
byitscorrespondingbrainactivityresponse. Thistask,requiresfocusingmoreonthedependenciesofbrainunitsand
theirresponsesratherthantheactualtimeseries. Also,sinceeachwindowcorrespondstoaspecificimage,themodel
needstocapturethedependenciesbasedonthecurrentwindow,requiringtobeinput-dependent.Resultsarereportedin
Table5.ChimerasignificantlyoutperformsallthebaselinesincludingourChimerabutwithoutdata-dependentparameters
(convolutionform).Duetothelargenumberofbrainunits,i.e.,9K,inthefirstdataset,transformer-basedmethodsface
OOMissue.However,theyarealsodata-dependentandsoshowsthesecondbestresultsinsecondandthirddatasets.This
resultssupportthesignificanceofdata-dependencyinChimera.
10
dethgieW egarevA4.2 AblationStudyandEfficiency
ToevaluatethesignificanceoftheChimera’sdesign,weperformablationstudiesandremoveoneofthecomponentsat
eachtime,keepingotherpartsunchanged.Table6reportstheresults.ThefirstrowreportstheChimera’sperformance,
whilerow2usesunidirectionalrecurrencealongthevariatedimension,row3removesthegatingmechanism,row4uses
convolutionform(data-independent),androw5removesthemoduleforseasonalpatterns.Theresultsshowthatallthe
componentsofChimeracontributestoitsperformance.
Table6:AblationstudyontheChimera’sdesign.
Table5:Imageclassificationbybrainactivity(Acc.%).
ETTh1 ETTm1 ETTh2
Method
Chimera Chimera(ind.) SpaceTime S4 iTrans. Trans. DLinear MSE MAE MSE MAE MSE MAE
Method
(ours) (ours) 2023 2022 2024 2017 2023
Chimera 0.405 0.424 0.345 0.377 0.318 0.375
BVFC(9K) 69.41 62.36 41.20 40.89 OOM OOM 39.74 Uni.-directional 0.409 0.429 0.354 0.385 0.326 0.381
BVFC(1K) 58.99 50.25 34.31 35.19 54.18 43.60 33.09 w/oGating 0.418 0.433 0.351 0.384 0.321 0.379
BVFC(400) 51.08 45.17 33.58 33.76 48.22 38.05 32.73 Input-independent 0.471 0.498 0.361 0.389 0.372 0.401
w/oseasonal 0.426 0.431 0.357 0.382 0.331 0.386
Figure3: Classificationandanomalydetectionperformance. Fulllistwithadditional
Figure4:Wall-clockscaling.
baselinesisinAppendixG.
Length of Time Series. We perform experiments on the effect of the sequence length on the efficiency of Chimera
andbaselines.TheresultsarereportedinFigure4.Chimerascaleslinearlywithrespecttothesequencelengthandhas
smootherscalingthanS4(Gu,Goel,andRe2022)andTransformers(Vaswanietal.2017). Theseresultsalsohighlight
thesignificanceofouralgorithmthatuses2DparallelscansfortrainingChimera.Thisalgorithmresultsin≈×4faster
training,whichisveryclosedtotheconvolutionalformatwithout datadependency.Chimeraalsohasacloserunningtime
toSpaceTime(M.Zhangetal.2023),whichhas1Drecurrent.
4.3 SelectionMechanismAlongTimeandVariate
VariateGeneralization.Wearguethatthedata-dependencywithdiscretizationallowsthemodeltofiltertheirrelevant
contextbasedontheinput,resultinginmoregeneralizability.InspiredbyYongLiu,Hu,etal.(2024),wetrainourmodel
(andbaseline)on20%ofvariatesandevaluateitsgeneralizabilitytounseenvariates.TheresultsarereportedinFigure5.
ChimerahasonpargeneralizabilitycomparedtoTransformers(whenappliedalongthevariatedimension),whichwe
attributestoitsdata-dependentparametersasChimerawithconvolutionformperformspoorlyonunseenvariates.
ContextFiltering.IncreasingthelookbacklengthdoesnotnecessarilyresultinbetterperformanceforTransformers(Yong
Liu, Hu, et al. 2024). Due to the selection mechanism of Chimera, we expect it to filter irrelevant information and
monotonicallyperformsbetter.Figure6reportstheChimera’sperformance(w/andw/odata-dependency)andtransformer-
basedbaselines(H.Wu,J.Wu,etal.2022;H.Zhouetal.2021)whilevaryingthelookbacklength. Chimeraduetoits
selectionmechanismmonotonicallyperformsbetterwithincreasingthelookback.
11Figure6:Effectoflookbacklength.
Figure5:Selectionresultsingeneralizationtounseenvariates.
5 Conclusion and Future Work
ThispaperpresentsChimera,athree-headed2-dimensionalSSMmodelwithprovablyhighexpressivepower.Chimera
is based on 2D SSMs with careful design of parameters that allows it to dynamically and simultaneously capture the
dependenciesalongbothtimeandvariatedimensions.Weprovidedifferentviewsofour2DSSMforefficienttraining,and
presentadata-dependentformulationwithafastimplementationusing2Dscans.Chimerausestwodifferentmodulesto
capturetrendandseasonalpatternsanditsdiscretizationprocessallowsthesemodulestoadjusttheresolutionateachtime
stampandforeachvariate.OurexperimentalandtheoreticalresultssupporttheeffectivenessandefficiencyofChimerain
awiderangeoftasks.
OtherDataModalities.WhiletheparameterizationofChimeraisdesignedtoexpressivelymodeltimeseriesdata,the
overallarchitectureofChimeraandourdata-dependent2DSSMwithits2Dscanformintrainingarepotentiallyapplicable
forotherhigherdimensionaldatatypes,e.g.,images,videos,multi-channelspeech,etc.Despiterecentattemptstodesign
effectiveSSM-basedvisionmodels(BadriNarayanaPatroandVijaySrinivasAgneeswaran2024),theexistingmodels
sufferfromthelackof2Dspatialinductivebias.Our2DSSM,however,isabletoprovide2Dinductivebias,potentially
beingmoreeffectivethanexisting1DselectiveSSMs.Accordingly,apromisingdirectionistoexplorethepotentialof2D
selectiveSSMsforotherhighdimensionaldatamodalitiesanddifferenttasks.
VariantsofChimera.AsdiscussedinSection3.2,differentvariantsofChimeraresultintheextensionofwell-known
architectureslikeMamba(GuandDao2023)to2-dimensionaldata,orextensionofmethodslikeS4ND(Nguyenetal.2022),
and2DSSM(Baron,Zimerman,andWolf2024)tohavedata-dependentweights.Despitethefactthatourformulationof
the2DSSMwithdiscretizationanddatadependentparametersprovidesamoregeneralframeworktoextendSSMsto
higher-dimensionaldata,itdoesnotnecessarilymeanthatforanydatamodalitiesandnetworksize,itsgenericformcan
achievethebestresult.Whileourexperimentalevaluationislimitedtothegenericformofour2DSSMandChimera,it
isapromisingfuturedirectiontoseeiflimitingtransitionmatrixA𝑖 (i.e.,2DMamba,2DMamba-2)canresultinmore
powerfulmodels.Weleavetheexperimentalevaluationsofthesespacialcasesofour2DSSMforfuturework.
Efficiency.Whileour2Dscandecreasesthenumberofrequiredrecurrencetocomputethehiddenstates,itsstillbasedon
anaiveimplementationofparallelscan.Thereisapotentialforfurtherimprovementof2Dparallelscan’sefficiencyby
usingmorehardware-awareimplementationssimilartoselectivescanbyGuandDao(2023).
References
[1] AhmedAbdulaal,ZhuanghuaLiu,andTomerLancewicki.“Practicalapproachtoasynchronousmultivariatetime
seriesanomalydetectionandlocalization”.In:Proceedingsofthe27thACMSIGKDDconferenceonknowledgediscovery
&datamining.2021,pp.2485–2494.
[2] MasanaoAoki.Statespacemodelingoftimeseries.SpringerScience&BusinessMedia,2013.
[3] AnthonyBagnall,HoangAnhDau,JasonLines,MichaelFlynn,JamesLarge,AaronBostrom,PaulSoutham,and
EamonnKeogh.“TheUEAmultivariatetimeseriesclassificationarchive,2018”.In:arXivpreprintarXiv:1811.00075
(2018).
[4] EthanBaron,ItamarZimerman,andLiorWolf.“A2-DimensionalStateSpaceLayerforSpatialInductiveBias”.In:
TheTwelfthInternationalConferenceonLearningRepresentations.2024.url:https://openreview.net/forum?id=
BGkqypmGvm.
[5] DavidJBartholomew.TimeSeriesAnalysisForecastingandControl.1971.
12[6] AliBehrouz,ParsaDelavari,andFarnooshHashemi.“UnsupervisedRepresentationLearningofBrainActivityvia
BridgingVoxelActivityandFunctionalConnectivity”.In:Internationalconferenceonmachinelearning(ICML).2024.
[7] AliBehrouzandFarnooshHashemi.“GraphMamba:TowardsLearningonGraphswithStateSpaceModels”.In:
arXivpreprintarXiv:2402.08678(2024).
[8] AliBehrouz,MicheleSantacatterina,andRaminZabih.“Mambamixer:Efficientselectivestatespacemodelswith
dualtokenandchannelselection”.In:arXivpreprintarXiv:2403.19888(2024).
[9] MichaelBenderandSlobodanSimonovic.“Time-seriesmodelingforlong-rangestream-flowforecasting”.In:Journal
ofWaterResourcesPlanningandManagement 120.6(1994),pp.857–870.
[10] GeorgeEPBoxandGwilymMJenkins.“Somerecentadvancesinforecastingandcontrol”.In:JournaloftheRoyal
StatisticalSociety.SeriesC(AppliedStatistics)17.2(1968),pp.91–109.
[11] CristianChallu,KinGOlivares,BorisNOreshkin,FedericoGarza,MaxMergenthaler,andArturDubrawski.“N-HiTS:
NeuralHierarchicalInterpolationforTimeSeriesForecasting”.In:arXivpreprintarXiv:2201.12886(2022).
[12] Si-AnChen,Chun-LiangLi,NateYoder,SercanOArik,andTomasPfister.“Tsmixer:Anall-mlparchitecturefor
timeseriesforecasting”.In:arXivpreprintarXiv:2303.06053(2023).
[13] Chi-TsongChen.Linearsystemtheoryanddesign.Saunderscollegepublishing,1984.
[14] ZongleiChen,MinboMa,TianruiLi,HongjunWang,andChongshouLi.“Longsequencetime-seriesforecasting
withdeeplearning:Asurvey”.In:InformationFusion97(2023),p.101819.
[15] JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.“Empiricalevaluationofgatedrecurrent
neuralnetworksonsequencemodeling”.In:arXivpreprintarXiv:1412.3555(2014).
[16] TriDaoandAlbertGu.“TransformersareSSMs:GeneralizedModelsandEfficientAlgorithmsThroughStructured
StateSpaceDuality”.In:InternationalConferenceonMachineLearning(ICML).2024.
[17] AbhimanyuDas,WeihaoKong,AndrewLeach,ShaanKMathur,RajatSen,andRoseYu.“Long-termForecasting
withTiDE:Time-seriesDenseEncoder”.In:TransactionsonMachineLearningResearch(2023).issn:2835-8856.url:
https://openreview.net/forum?id=pCbC3aQB5W.
[18] RikusEising.“Realizationandstabilizationof2-Dsystems”.In:IEEETransactionsonAutomaticControl23.5(1978),
pp.793–799.
[19] EttoreFornasiniandGiovanniMarchesini.“Doubly-indexeddynamicalsystems:State-spacemodelsandstructural
properties”.In:Mathematicalsystemstheory12.1(1978),pp.59–72.
[20] Jean-YvesFranceschi,AymericDieuleveut,andMartinJaggi.“UnsupervisedScalableRepresentationLearningfor
MultivariateTimeSeries”.In:NeurIPS.2019.
[21] Kelum Gajamannage, Yonggi Park, and Dilhani I Jayathilake. “Real-time forecasting of time series in financial
marketsusingsequentiallytraineddual-LSTMs”.In:ExpertSystemswithApplications223(2023),p.119879.
[22] RakshithaGodahewa,ChristophBergmeir,GeoffreyIWebb,RobJHyndman,andPabloMontero-Manso.“Monash
timeseriesforecastingarchive”.In:arXivpreprintarXiv:2105.06643(2021).
[23] AlbertGuandTriDao.“Mamba:Linear-timesequencemodelingwithselectivestatespaces”.In:arXivpreprint
arXiv:2312.00752(2023).
[24] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. “Hippo: Recurrent memory with optimal
polynomialprojections”.In:Advancesinneuralinformationprocessingsystems33(2020),pp.1474–1487.
[25] AlbertGu,KaranGoel,AnkitGupta,andChristopherRé.“OntheParameterizationandInitializationofDiagonal
StateSpaceModels”.In:AdvancesinNeuralInformationProcessingSystems.Ed.byAliceH.Oh,AlekhAgarwal,
DanielleBelgrave,andKyunghyunCho.2022.url:https://openreview.net/forum?id=yJE7iQSAep.
[26] AlbertGu,KaranGoel,andChristopherRe.“EfficientlyModelingLongSequenceswithStructuredStateSpaces”.
In: International Conference on Learning Representations. 2022. url: https://openreview.net/forum?id=
uYLFoz1vlAC.
[27] AlbertGu,CaglarGulcehre,ThomasPaine,MattHoffman,andRazvanPascanu.“Improvingthegatingmechanism
ofrecurrentneuralnetworks”.In:InternationalConferenceonMachineLearning.PMLR.2020,pp.3800–3809.
[28] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé.“Combiningrecurrent,
convolutional, and continuous-time models with linear state space layers”. In: Advances in neural information
processingsystems34(2021),pp.572–585.
[29] AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra,andChristopherRe.“HowtoTrainyourHIPPO:StateSpace
ModelswithGeneralizedOrthogonalBasisProjections”.In:InternationalConferenceonLearningRepresentations.
2023.url:https://openreview.net/forum?id=klK17OQ3KB.
[30] AndrewCHarvey.“Forecasting,structuraltimeseriesmodelsandtheKalmanfilter”.In:Cambridgeuniversitypress
(1990).
13[31] MartinNHebart,OliverContier,LinaTeichmann,AdamHRockter,CharlesYZheng,AlexisKidder,AnnaCorriveau,
Maryam Vaziri-Pashkam, and Chris I Baker. “THINGS-data, a multimodal collection of large-scale datasets for
investigatingobjectrepresentationsinhumanbrainandbehavior”.In:Elife12(2023),e82580.
[32] TsHinamoto.“Realizationsofastate-spacemodelfromtwo-dimensionalinput-outputmap”.In:IEEETransactions
onCircuitsandSystems27.1(1980),pp.36–44.
[33] S.HochreiterandJ.Schmidhuber.“LongShort-TermMemory”.In:NeuralComput.(1997).
[34] KyleHundman,ValentinoConstantinou,ChristopherLaporte,IanColwell,andTomSoderstrom.“Detectingspace-
craftanomaliesusinglstmsandnonparametricdynamicthresholding”.In:Proceedingsofthe24thACMSIGKDD
internationalconferenceonknowledgediscovery&datamining.2018,pp.387–395.
[35] RomainIlbert,AmbroiseOdonnat,VasiliiFeofanov,AladinVirmaux,GiuseppePaolo,ThemisPalpanas,andIevgen
Redko.“UnlockingthePotentialofTransformersinTimeSeriesForecastingwithSharpness-AwareMinimization
andChannel-WiseAttention”.In:arXivpreprintarXiv:2402.10198(2024).
[36] PlamenChIvanov,LuisANunesAmaral,AryLGoldberger,ShlomoHavlin,MichaelGRosenblum,ZbigniewR
Struzik,andHEugeneStanley.“Multifractalityinhumanheartbeatdynamics”.In:Nature399.6735(1999),pp.461–
465.
[37] AlistairEWJohnson,LucasBulgarelli,LuShen,AlvinGayles,AyadShammout,StevenHorng,TomJPollard,Sicheng
Hao,BenjaminMoody,BrianGow,etal.“MIMIC-IV,afreelyaccessibleelectronichealthrecorddataset”.In:Scientific
data10.1(2023),p.1.
[38] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are rnns: Fast au-
toregressive transformers with linear attention”. In: International conference on machine learning. PMLR. 2020,
pp.5156–5165.
[39] NikitaKitaev,LukaszKaiser,andAnselmLevskaya.“Reformer:TheEfficientTransformer”.In:ICLR.2020.
[40] Sun-YuanKung,BernardCLevy,MartinMorf,andThomasKailath.“Newresultsin2-Dsystemstheory,PartII:2-D
state-spacemodels—realizationandthenotionsofcontrollability,observability,andminimality”.In:Proceedingsof
theIEEE65.6(1977),pp.945–961.
[41] GuokunLai,Wei-ChengChang,YimingYang,andHanxiaoLiu.“Modelinglong-andshort-termtemporalpatterns
withdeepneuralnetworks”.In:SIGIR.2018.
[42] ShiyangLi,XiaoyongJin,YaoXuan,XiyouZhou,WenhuChen,Yu-XiangWang,andXifengYan.“Enhancingthe
LocalityandBreakingtheMemoryBottleneckofTransformeronTimeSeriesForecasting”.In:NeurIPS.2019.
[43] ZheLi,ShiyiQi,YiduoLi,andZenglinXu.“Revisitinglong-termtimeseriesforecasting:Aninvestigationonlinear
mapping”.In:arXivpreprintarXiv:2305.10721(2023).
[44] BryanLimandStefanZohren.“Time-seriesforecastingwithdeeplearning:asurvey”.In:PhilosophicalTransactions
oftheRoyalSocietyA379.2194(2021),p.20200209.
[45] MinhaoLiu,AilingZeng,MuxiChen,ZhijianXu,QiuxiaLai,LingnaMa,andQiangXu.“Scinet:Timeseriesmodeling
andforecastingwithsampleconvolutionandinteraction”.In:AdvancesinNeuralInformationProcessingSystems35
(2022),pp.5816–5828.
[46] ShizhanLiu,HangYu,CongLiao,JianguoLi,WeiyaoLin,AlexXLiu,andSchahramDustdar.“Pyraformer:Low-
complexitypyramidalattentionforlong-rangetimeseriesmodelingandforecasting”.In:Internationalconferenceon
learningrepresentations.2021.
[47] YongLiu,TenggeHu,HaoranZhang,HaixuWu,ShiyuWang,LintaoMa,andMingshengLong.“iTransformer:
InvertedTransformersAreEffectiveforTimeSeriesForecasting”.In:TheTwelfthInternationalConferenceonLearning
Representations.2024.url:https://openreview.net/forum?id=JePfAI8fah.
[48] YongLiu,HaixuWu,JianminWang,andMingshengLong.“Non-stationarytransformers:Exploringthestationarity
intimeseriesforecasting”.In:AdvancesinNeuralInformationProcessingSystems35(2022),pp.9881–9893.
[49] YongLiu,HaixuWu,JianminWang,andMingshengLong.“Non-stationaryTransformers:RethinkingtheStationarity
inTimeSeriesForecasting”.In:NeurIPS.2022.
[50] YueLiu,YunjieTian,YuzhongZhao,HongtianYu,LingxiXie,YaoweiWang,QixiangYe,andYunfanLiu.“Vmamba:
Visualstatespacemodel”.In:arXivpreprintarXiv:2401.10166(2024).
[51] DonghaoLuoandXueWang.“ModernTCN:AModernPureConvolutionStructureforGeneralTimeSeriesAnalysis”.
In:TheTwelfthInternationalConferenceonLearningRepresentations.2024.url:https://openreview.net/forum?
id=vpJMJerXHU.
[52] JunMa,FeifeiLi,andBoWang.“U-Mamba:EnhancingLong-rangeDependencyforBiomedicalImageSegmentation”.
In:arXivpreprintarXiv:2401.04722(2024).
14[53] EricMartinandChrisCundy.“ParallelizingLinearRecurrentNeuralNetsOverSequenceLength”.In:International
ConferenceonLearningRepresentations.2018.url:https://openreview.net/forum?id=HyUNwulC-.
[54] AdityaPMathurandNilsOleTippenhauer.“SWaT:AwatertreatmenttestbedforresearchandtrainingonICS
security”.In:2016internationalworkshoponcyber-physicalsystemsforsmartwaternetworks(CySWater).IEEE.2016,
pp.31–36.
[55] EricNguyen,KaranGoel,AlbertGu,GordonDowns,PreeyShah,TriDao,StephenBaccus,andChristopherRé.
“S4nd:Modelingimagesandvideosasmultidimensionalsignalswithstatespaces”.In:Advancesinneuralinformation
processingsystems35(2022),pp.2846–2861.
[56] YuqiNie,NamHNguyen,PhanwadeeSinthong,andJayantKalagnanam.“ATimeSeriesisWorth64Words:Long-
termForecastingwithTransformers”.In:TheEleventhInternationalConferenceonLearningRepresentations.2023.
url:https://openreview.net/forum?id=Jbdc0vTOcol.
[57] BorisNOreshkin,DmitriCarpov,NicolasChapados,andYoshuaBengio.“N-BEATS:Neuralbasisexpansionanalysis
forinterpretabletimeseriesforecasting”.In:ICLR(2019).
[58] BadriN.PatroandVijayS.Agneeswaran.SiMBA:SimplifiedMamba-BasedArchitectureforVisionandMultivariate
Timeseries.2024.arXiv:2403.15360[cs.CV].
[59] BadriNarayanaPatroandVijaySrinivasAgneeswaran.“Mamba-360:SurveyofStateSpaceModelsasTransformer
AlternativeforLongSequenceModelling:Methods,Applications,andChallenges”.In:arXivpreprintarXiv:2404.16112
(2024).
[60] StevePincusandRudolfEKalman.“Irregularity,volatility,risk,andfinancialmarkettimeseries”.In:Proceedingsof
theNationalAcademyofSciences101.38(2004),pp.13709–13714.
[61] ZhenQin,SonglinYang,andYiranZhong.“Hierarchicallygatedrecurrentneuralnetworkforsequencemodeling”.
In:AdvancesinNeuralInformationProcessingSystems36(2023).
[62] PrajitRamachandran,BarretZoph,andQuocVLe.“Searchingforactivationfunctions”.In:arXivpreprintarXiv:1710.05941
(2017).
[63] DavidSalinas,ValentinFlunkert,JanGasthaus,andTimJanuschowski.“DeepAR:Probabilisticforecastingwith
autoregressiverecurrentnetworks”.In:Internationaljournalofforecasting36.3(2020),pp.1181–1191.
[64] YairSchiff,Chia-HsiangKao,AaronGokaslan,TriDao,AlbertGu,andVolodymyrKuleshov.“Caduceus:Bi-directional
equivariantlong-rangednasequencemodeling”.In:arXivpreprintarXiv:2403.03234(2024).
[65] ImanolSchlag,KazukiIrie,andJürgenSchmidhuber.“Lineartransformersaresecretlyfastweightprogrammers”.In:
InternationalConferenceonMachineLearning.PMLR.2021,pp.9355–9366.
[66] JimmyT.H.Smith,AndrewWarrington,andScottLinderman.“SimplifiedStateSpaceLayersforSequenceModeling”.
In:TheEleventhInternationalConferenceonLearningRepresentations.2023.url:https://openreview.net/forum?
id=Ai8Hw3AXqks.
[67] YaSu,YoujianZhao,ChenhaoNiu,RongLiu,WeiSun,andDanPei.“Robustanomalydetectionformultivariate
timeseriesthroughstochasticrecurrentneuralnetwork”.In:Proceedingsofthe25thACMSIGKDDinternational
conferenceonknowledgediscovery&datamining.2019,pp.2828–2837.
[68] CorentinTallecandYannOllivier.“Canrecurrentneuralnetworkswarptime?”In:InternationalConferenceon
LearningRepresentations.2018.url:https://openreview.net/forum?id=SJcKhk-Ab.
[69] WilliamTonerandLukeDarlow.“AnAnalysisofLinearTimeSeriesForecastingModels”.In:Internationalconference
onmachinelearning(ICML)(2024).
[70] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,etal.“Llama:Openandefficientfoundationlanguagemodels”.In:
arXivpreprintarXiv:2302.13971(2023).
[71] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,and
IlliaPolosukhin.“Attentionisallyouneed”.In:Advancesinneuralinformationprocessingsystems30(2017).
[72] Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Wojciech Samek, and
TobiasSchaeffter.“PTB-XL,alargepubliclyavailableelectrocardiographydataset”.In:Scientificdata7.1(2020),
pp.1–15.
[73] JunxiongWang,JingNathanYan,AlbertGu,andAlexanderRush.“PretrainingWithoutAttention”.In:Findingsofthe
AssociationforComputationalLinguistics:EMNLP2023.Ed.byHoudaBouamor,JuanPino,andKalikaBali.Singapore:
AssociationforComputationalLinguistics,Dec.2023,pp.58–69.doi:10.18653/v1/2023.findings-emnlp.5.url:
https://aclanthology.org/2023.findings-emnlp.5.
[74] PeteWarden.“Speechcommands:Adatasetforlimited-vocabularyspeechrecognition”.In:arXivpreprintarXiv:1804.03209
(2018).
15[75] PeterRWinters.“Forecastingsalesbyexponentiallyweightedmovingaverages”.In:Managementscience6.3(1960),
pp.324–342.
[76] GeraldWoo,ChenghaoLiu,DoyenSahoo,AkshatKumar,andStevenC.H.Hoi.“ETSformer:ExponentialSmoothing
TransformersforTime-seriesForecasting”.In:arXivpreprintarXiv:2202.01381(2022).
[77] HaixuWu,TenggeHu,YongLiu,HangZhou,JianminWang,andMingshengLong.“TimesNet:Temporal2D-Variation
ModelingforGeneralTimeSeriesAnalysis”.In:TheEleventhInternationalConferenceonLearningRepresentations.
2023.url:https://openreview.net/forum?id=ju_Uqw384Oq.
[78] HaixuWu,JialongWu,JiehuiXu,JianminWang,andMingshengLong.“Flowformer:LinearizingTransformers
withConservationFlows”.In:ICML.2022.
[79] HaixuWu,JiehuiXu,JianminWang,andMingshengLong.“Autoformer:Decompositiontransformerswithauto-
correlationforlong-termseriesforecasting”.In:Advancesinneuralinformationprocessingsystems34(2021),pp.22419–
22430.
[80] JiehuiXu,HaixuWu,JianminWang,andMingshengLong.“AnomalyTransformer:TimeSeriesAnomalyDetection
withAssociationDiscrepancy”.In:ICLR.2021.
[81] SonglinYang,BailinWang,YikangShen,RameswarPanda,andYoonKim.“Gatedlinearattentiontransformers
withhardware-efficienttraining”.In:Internationalconferenceonmachinelearning(ICML).2024.
[82] AilingZeng,MuxiChen,LeiZhang,andQiangXu.“AreTransformersEffectiveforTimeSeriesForecasting?”In:
AAAI.2023.
[83] AilingZeng,MuxiChen,LeiZhang,andQiangXu.“Aretransformerseffectivefortimeseriesforecasting?”In:
ProceedingsoftheAAAIconferenceonartificialintelligence.Vol.37.2023,pp.11121–11128.
[84] MichaelZhang,KhaledKamalSaab,MichaelPoli,TriDao,KaranGoel,andChristopherRe.“EffectivelyModeling
TimeSerieswithSimpleDiscreteStateSpaces”.In:TheEleventhInternationalConferenceonLearningRepresentations.
2023.url:https://openreview.net/forum?id=2EpjkjzdCAa.
[85] T.Zhang,YizhuoZhang,WeiCao,J.Bian,XiaohanYi,ShunZheng,andJianLi.“LessIsMore:FastMultivariate
TimeSeriesForecastingwithLightSampling-orientedMLPStructures”.In:arXivpreprintarXiv:2207.01186(2022).
[86] YunhaoZhangandJunchiYan.“Crossformer:Transformerutilizingcross-dimensiondependencyformultivariate
timeseriesforecasting”.In:Theeleventhinternationalconferenceonlearningrepresentations.2023.
[87] HaoyiZhou,ShanghangZhang,JieqiPeng,ShuaiZhang,JianxinLi,HuiXiong,andWancaiZhang.“Informer:
Beyondefficienttransformerforlongsequencetime-seriesforecasting”.In:ProceedingsoftheAAAIconferenceon
artificialintelligence.Vol.35.2021,pp.11106–11115.
[88] TianZhou,ZiqingMa,QingsongWen,LiangSun,TaoYao,WotaoYin,RongJin,etal.“Film:Frequencyimproved
legendrememorymodelforlong-termtimeseriesforecasting”.In:AdvancesinNeuralInformationProcessingSystems
35(2022),pp.12677–12690.
[89] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. “Fedformer: Frequency enhanced
decomposedtransformerforlong-termseriesforecasting”.In:Internationalconferenceonmachinelearning.PMLR.
2022,pp.27268–27286.
16A Background
A.1 1DSpaceStateModels
1DSpaceStateModels(SSMs)arelineartime-invariantsystemsthatmapinputsequence𝑥(𝑡) ∈R𝐿 ↦→𝑦(𝑡) ∈R𝐿 (Aoki
2013).SSMsusealatentstateℎ(𝑡) ∈R𝑁×𝐿,transitionparameterA∈R𝑁×𝑁,andprojectionparametersB∈R𝑁×1,C∈R1×𝑁
tomodeltheinputandoutputas:
ℎ′(𝑡) =Aℎ(𝑡)+B𝑥(𝑡), 𝑦(𝑡) =Cℎ(𝑡). (26)
MostexistingSSMs(Behrouz,Santacatterina,andZabih2024;GuandDao2023;Gu,Goel,andRe2022),firstdiscretizethe
signalsA,B,andC.Thatis,usingaparameter𝚫andzero-orderhold,thediscretizedformulationisdefinedas:
ℎ
𝑡
=A¯ ℎ 𝑡−1+B¯ 𝑥 𝑡, 𝑦
𝑡
=Cℎ 𝑡, (27)
whereA¯ =exp(𝚫A)andB¯ = (𝚫A)−1 (exp(𝚫A−𝐼)) .𝚫B.(Gu,Dao,etal.2020)showthatdiscreteSSMscanbeinterpreted
asbothconvolutionsandrecurrentnetworks:i.e.,
K¯ = (cid:16) CB¯,CA¯ B¯,...,CA¯𝐿−1 B¯(cid:17) ,
𝑦 =𝑥 ∗K¯, (28)
whichmakestheirtrainingandinferenceveryefficientasaconvolutionandrecurrentmodel,respectively.
A.2 DataDependency
AbovediscreteSSMsarebasedondata-independentparameters.Thatis,parameters𝚫,A¯,B¯,andCaretimeinvariantand
arethesameforanyinput.GuandDao(2023)arguethatthistimeinvariancehasthecostoflimitingSSMseffectivenessin
compressingcontextintoasmallerstate(GuandDao2023).Toovercomethischallenge,theypresentaselectiveSSMs
(S6)blockthateffectivelyselectsrelevantcontextbyenablingdependenceoftheparametersB¯,C¯,and𝚫ontheinput𝑥 𝑡,
i.e.:
B¯ 𝑡 =Linear B(𝑥 𝑡) (29)
C¯ 𝑡 =Linear C(𝑥 𝑡) (30)
𝚫
𝑡
=Softplus(Linear𝚫(𝑥 𝑡)), (31)
whereLinear(.) isalinearprojectionandSoftplus(.) = log(1+exp(.)). Thisdatadependencycomesatthecostof
efficiencyasthemodelcannotbetrainedasaconvolution.Toovercomethischallenge,GuandDao(2023)showthatthe
linearrecurrenceinEquation1canbeformulatedasanassociativescan(MartinandCundy2018),whichacceptsefficient
parallelalgorithms.
B Additional Related Work
ClassicalApproach.Modelingtimeseriesdataisalong-standingproblemandhasattractedmuchattentionduringthepast
60years.Therehavebeenseveralmathematicalmodelstocapturethetimeseriestraitslikeexponentialsmoothing(Winters
1960),autoregressiveintegratedmovingaverage(ARIMA)(Bartholomew1971),SARIMA(BenderandSimonovic1994),
Box-Jenkinsmethod(BoxandJenkins1968),andmorerecentlystate-spacemodels(Aoki2013;Harvey1990). Despite
theirmoreinterpretability,thesemethodsusuallyfailtocapturenon-lineardependenciesandalsooftenrequiremanually
analyzingtimeseriesfeatures(e.g.,trendorseasonality),resultinginlackofgeneralizability.
Recurrent and Deep State Space Models. Another group of relevant studies to ours is deep sequence models. A
commonclassofarchitecturesforsequencemodelingarerecurrentneuralnetworkssuchaslikeGRUs(Chungetal.2014),
DeepAR(Salinasetal.2020),LSTMs(HochreiterandJ.Schmidhuber1997).ThemaindrawbackofRNNsistheirpotential
for vanishing/exploding gradients and also their slow training. Recently, linear attention methods with fast training
attractedattention(Katharopoulosetal.2020;Schlag,Irie,andJürgenSchmidhuber2021;Yangetal.2024).Katharopoulos
etal.(2020)showthatthesemethodshaverecurrentformulationandcanbefastininference.
17Recently,deepstatespacemodelshaveattractedmuchattentionasthealternativeofTransformers(Vaswanietal.2017),
duetotheirfasttrainingandinference(Gu,Dao,etal.2020).ThesemethodsarethecombinationoftraditionalSSMswith
deepneuralnetworksbydirectlyparameterizingthelayersofaneuralnetworkwithmultiplelinearSSMs,andovercome
commonrecurrenttrainingdrawbacksbyleveragingtheconvolutionalviewofSSMs(Gu,Dao,etal.2020;Gu,Goel,Gupta,
etal.2022;Gu,Goel,andRe2022;Gu,I.Johnson,Goel,etal.2021;Smith,Warrington,andLinderman2023).Recently,Gu
andDao(2023)presentanewformulationofdeepSSMsbyallowingtheparameterstobethefunctionofinputs. This
architectureshowspromissingpotentialinvariousdomainslikeNLP(GuandDao2023),vision(Behrouz,Santacatterina,
andZabih2024;YueLiuetal.2024;J.Ma,F.Li,andB.Wang2024),graphs(BehrouzandHashemi2024),DNAmodeling(Gu
andDao2023;Schiffetal.2024).
Alltheabovemethodsaredesignfor1Ddata,meaningthatthestatesdependsononevariable.Thereare,however,afew
studiesthatuses2DSSMsindeeplearningsettings.S4ND(Nguyenetal.2022)usescontinuoussignalstomodelimages.
ThesemethodsnotonlyconsidertwoseparateSSMfortheaxes, butitalsodirectlytreatthesystemasacontinuous
systemwithoutdiscretizationstep.Furthermore,S4NDhasdata-independentparameters.Anothersimilarapproachis
2DSSM(Baron,Zimerman,andWolf2024),thatmodelsimagesasdiscretesignals.Thatis,theinitialSSMmodelisdiscrete
andagainthereisalackofdiscretizationstep,whichisimportantfortimeseriesaswediscussedearlier.Also,theirmethod
againisbasedondata-independentparameters.BothS4NDand2DSSMcanbecomputedasaconvolution.We,however,
presentanewscanningtechniqueforfasttrainingof2DSSMs,evenwithinput-dependentparameters.
Othermethods.Transformer-basedmodelshaveattractedmuchattentionoverrecentyearsformultivariatetimeseries
forecasting,whenmodelingthecomplexrelationshipsofco-variatesoralongthetimedimensionisrequired(Ilbertetal.
2024;Kitaev,Kaiser,andLevskaya2020;S.Liuetal.2021;Nieetal.2023;H.Wu,Xu,etal.2021;Zengetal.2023a;Y.Zhang
andYan2023;H.Zhouetal.2021;T.Zhou,Z.Ma,Wen,X.Wang,etal.2022).Severalstudieshavefocusedondesigning
moreefficientandeffectiveattentionswithusingspecialtraitsoftimeseries(Wooetal.2022).Someotherstudieshave
focusedonextractinglong-terminformationforbetterforecasting(Nieetal.2023;T.Zhou,Z.Ma,Wen,Sun,etal.2022).
Inadditiontotransformers,linearmodelsalsohaveshownpromisingresults(S.-A.Chenetal.2023;H.Wu,Hu,etal.2023).
Forexample,S.-A.Chenetal.(2023)presentTSMixer,anall-MLParchitecturefortimeseriesforecasting,withpromising
performance.Duetotheexpressivepowerofour2DSSM,theselinearmethodssometimescanbeviewedasaspecialcase
of2DSSMs.Recently,convolution-basedmodelsfortimeserieshaveshownpromisingresults(LuoandX.Wang2024).
Thesemethodsbyusingglobalkernelsenhancetheglobalreceptivefield.Ourdata-independentformulationofChimerais
connectedtothislineofworkasitcanbewrittenasaglobalconvolution.
C Details of the Discretization
GivenPDEwithinitialconditionℎ(0,0) =0:
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ(1) 𝑡(1),𝑡(2) = A1ℎ(1) 𝑡(1),𝑡(2) ,A2ℎ(2) 𝑡(1),𝑡(2) +B1x 𝑡(1),𝑡(2) , (32)
𝜕𝑡(1)
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ(2) 𝑡(1),𝑡(2) = A1ℎ(1) 𝑡(1),𝑡(2) ,A2ℎ(2) 𝑡(1),𝑡(2) +B1x 𝑡(1),𝑡(2) , (33)
𝜕𝑡(1)
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ(1) 𝑡(1),𝑡(2) = A3ℎ(1) 𝑡(1),𝑡(2) ,A4ℎ(2) 𝑡(1),𝑡(2) +B2x 𝑡(1),𝑡(2) , (34)
𝜕𝑡(2)
𝜕 (cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
ℎ(2) 𝑡(1),𝑡(2) = A3ℎ(1) 𝑡(1),𝑡(2) ,A4ℎ(2) 𝑡(1),𝑡(2) +B2x 𝑡(1),𝑡(2) , (35)
𝜕𝑡(2)
overthesamplingintervals [𝑘Δ𝑡(1),(𝑘+1)Δ𝑡(1)] and [ℓΔ𝑡(2),(ℓ +1)Δ𝑡(2)] wehave:
∫ (𝑘+1)Δ𝑡(1) 𝜕 (cid:16) (cid:17)
ℎ(1) 𝑡(1),𝑡(2) 𝑑𝑡(1)
𝑘Δ𝑡(1)
𝜕𝑡(1)
= ∫ (𝑘+1)Δ𝑡(1) (cid:16) A1ℎ(1) (cid:16) 𝑡(1),𝑡(2)(cid:17) +B 1(1) x(1) (cid:16) 𝑡(1),𝑡(2)(cid:17)(cid:17) 𝑑𝑡(1) (36)
𝑘Δ𝑡(1)
18andso:
∫ (𝑘+1)Δ𝑡(1) 𝜕 (cid:16) (cid:17)
ℎ(2) 𝑡(1),𝑡(2) 𝑑𝑡(1)
𝑘Δ𝑡(1)
𝜕𝑡(1)
= ∫ (𝑘+1)Δ𝑡(1) (cid:16) A2ℎ(2) (cid:16) 𝑡(1),𝑡(2)(cid:17) +B 1(2) x(2) (cid:16) 𝑡(1),𝑡(2)(cid:17)(cid:17) 𝑑𝑡(1) (37)
𝑘Δ𝑡(1)
Similarly,forthesecondequationwehave:
∫ (ℓ+1)Δ𝑡(2) 𝜕 (cid:16) (cid:17)
ℎ(1) 𝑡(1),𝑡(2) 𝑑𝑡(2)
ℓΔ𝑡(2)
𝜕𝑡(2)
= ∫ (ℓ+1)Δ𝑡(2) (cid:16) A3ℎ(1) (cid:16) 𝑡(1),𝑡(2)(cid:17) +B 2(1) x(1) (cid:16) 𝑡(1),𝑡(2)(cid:17)(cid:17) 𝑑𝑡(2) (38)
ℓΔ𝑡(2)
andso:
∫ (ℓ+1)Δ𝑡(2) 𝜕 (cid:16) (cid:17)
ℎ(2) 𝑡(1),𝑡(2) 𝑑𝑡(2)
ℓΔ𝑡(2)
𝜕𝑡(2)
= ∫ (ℓ+1)Δ𝑡(2) (cid:16) A4ℎ(2) (cid:16) 𝑡(1),𝑡(2)(cid:17) +B 2(2) x(2) (cid:16) 𝑡(1),𝑡(2)(cid:17)(cid:17) 𝑑𝑡(2) (39)
ℓΔ𝑡(2)
Next,theintegralscanbesimplifiedas:
(cid:16) (cid:17)
ℎ(1) (𝑘+1)Δ𝑡(1),𝑡(2)
=𝑒A1Δ𝑡(1)ℎ(1) (cid:16) 𝑘Δ𝑡(1),𝑡(2)(cid:17) +∫ (𝑘+1)Δ𝑡(1) 𝑒A1(𝑡(1)−𝑘Δ𝑡(1))B(1) x(1) (cid:16) 𝑡(1),𝑡(2)(cid:17) 𝑑𝑡(1), (40)
1
𝑘Δ𝑡(1)
and
(cid:16) (cid:17)
ℎ(2) (𝑘+1)Δ𝑡(1),𝑡(2)
=𝑒A2Δ𝑡(1)ℎ(2) (cid:16) 𝑘Δ𝑡(1),𝑡(2)(cid:17) +∫ (𝑘+1)Δ𝑡(1) 𝑒A2(𝑡(1)−𝑘Δ𝑡(1))B(2) x(2) (cid:16) 𝑡(1),𝑡(2)(cid:17) 𝑑𝑡(1), (41)
1
𝑘Δ𝑡(1)
andsimilarlyforthethirdandfourthequationswehave:
(cid:16) (cid:17)
ℎ(1) 𝑡(1),(ℓ +1)Δ𝑡(2)
=𝑒A3Δ𝑡(2)ℎ(1) (cid:16) 𝑡(1),ℓΔ𝑡(2)(cid:17) +∫ (ℓ+1)Δ𝑡(2) 𝑒A3(𝑡(2)−ℓΔ𝑡(2))B(1) x(1) (cid:16) 𝑡(2),𝑡(1)(cid:17) 𝑑𝑡(2) (42)
2
ℓΔ𝑡(2)
and
(cid:16) (cid:17)
ℎ(2) 𝑡(1),(ℓ +1)Δ𝑡(2)
=𝑒A4Δ𝑡(2)ℎ(2) (cid:16) 𝑡(1),ℓΔ𝑡(2)(cid:17) +∫ (ℓ+1)Δ𝑡(2) 𝑒A4(𝑡(2)−ℓΔ𝑡(2))B(2) x(2) (cid:16) 𝑡(2),𝑡(1)(cid:17) 𝑑𝑡(2) (43)
2
ℓΔ𝑡(2)
UsingZOHassumption,wehave:
19∫ Δ𝑡(1) 𝑒A1𝑠𝑑𝑠 =A(1)−1 (cid:16) 𝑒A1Δ𝑡(1) −I(cid:17)
0
∫ Δ𝑡(1) 𝑒A2𝑠𝑑𝑠 =A(2)−1 (cid:16) 𝑒A2Δ𝑡(1) −I(cid:17) (44)
0
∫ Δ𝑡(2) 𝑒A3𝑠𝑑𝑠 =A(3)−1 (cid:16) 𝑒A3Δ𝑡(2) −I(cid:17) (45)
0
∫ Δ𝑡(2) 𝑒A4𝑠𝑑𝑠 =A(4)−1 (cid:16) 𝑒A4Δ𝑡(2) −I(cid:17) (46)
0
Accordingly,thediscretizedformisasfollows:
ℎ(1) =𝑒A1Δ𝑡(1)ℎ(1) +A(1)−1 (cid:16) 𝑒A1Δ𝑡(1) −I(cid:17) B(1) x(1) (47)
𝑘+1,ℓ 𝑘,ℓ 1 𝑘+1,ℓ
ℎ(2) =𝑒A2Δ𝑡(1)ℎ(2) +A(2)−1 (cid:16) 𝑒A2Δ𝑡(1) −I(cid:17) B(2) x(2) (48)
𝑘+1,ℓ 𝑘,ℓ 1 𝑘+1,ℓ
ℎ(1) =𝑒A3Δ𝑡(2)ℎ(1) +A(3)−1 (cid:16) 𝑒A3Δ𝑡(2) −I(cid:17) B(1) x(1) (49)
𝑘,ℓ+1 𝑘,ℓ 2 𝑘,ℓ+1
ℎ(2) =𝑒A4Δ𝑡(2)ℎ(2) +A(4)−1 (cid:16) 𝑒A4Δ𝑡(2) −I(cid:17) B(2) x(2) , (50)
𝑘,ℓ+1 𝑘,ℓ 2 𝑘,ℓ+1
whichmeansthat:
A¯
1
=exp(A1Δ 1), (51)
A¯
2
=exp(A2Δ 1), (52)
A¯
3
=exp(A3Δ 2), (53)
A¯
4
=exp(A4Δ 2), (54)
(55)
and
B¯ 1 = (cid:34) A A( (1 2) )− −1 1 (cid:0) (cid:0)𝑒 𝑒A A1 2Δ Δ1
1
− −I I(cid:1) (cid:1)B B1( (1 2) )(cid:35) , (56)
1
B¯ 2 = (cid:34) A A( (3 4) )− −1 1 (cid:0) (cid:0)𝑒 𝑒A A3 4Δ Δ2
2
− −I I(cid:1) (cid:1)B B2( (1 2) )(cid:35) . (57)
2
D Details of the Structure of Transition Matrices
DefinitionD.1(CompanionMatrix). Amatrix𝐴 ∈R𝑁×𝑁 hascompanionformifitcanbewrittenas:
0 0 ... 0 𝑎
1
(cid:169)1 0 ... 0 𝑎 (cid:170)
(cid:173) 2 (cid:174)
(cid:173)0 1 ... 0 𝑎 (cid:174)
𝐴=(cid:173) (cid:173) (cid:173). .
.
. .
.
... . .
.
. . .3 (cid:174) (cid:174) (cid:174). (58)
(cid:173) (cid:174)
(cid:173)0 0 ... 0 𝑎 (cid:174)
(cid:173) 𝑁1(cid:174)
0 0 ... 1 𝑎
𝑁
(cid:171) (cid:172)
20Thesematricescanbedecomposeintoashiftandalow-rankmatrix.Thatis:
0 0 ... 0 𝑎 0 0 ... 0 0 0 0 ... 0 𝑎
1 1
(cid:169)1 0 ... 0 𝑎 (cid:170) (cid:169)1 0 ... 0 0(cid:170) (cid:169)0 0 ... 0 𝑎 (cid:170)
(cid:173) 2 (cid:174) (cid:173) (cid:174) (cid:173) 2 (cid:174)
(cid:173)0 1 ... 0 𝑎 (cid:174) (cid:173)0 1 ... 0 0(cid:174) (cid:173)0 0 ... 0 𝑎 (cid:174)
𝐴=(cid:173) (cid:173) (cid:173). .
.
. .
.
... . .
.
. . .3 (cid:174) (cid:174) (cid:174)=(cid:173) (cid:173) (cid:173). .
.
. .
.
... . .
.
. . .(cid:174) (cid:174) (cid:174)+(cid:173) (cid:173) (cid:173). .
.
. .
.
... . .
.
. . .3 (cid:174) (cid:174) (cid:174). (59)
(cid:173) (cid:174) (cid:173) (cid:174) (cid:173) (cid:174)
(cid:173)0 0 ... 0 𝑎 (cid:174) (cid:173)0 0 ... 0 0(cid:174) (cid:173)0 0 ... 0 𝑎 (cid:174)
(cid:173) 𝑁1(cid:174) (cid:173) (cid:174) (cid:173) 𝑁1(cid:174)
0 0 ... 1 𝑎 0 0 ... 1 0 0 0 ... 0 𝑎
𝑁 𝑁
(cid:171) (cid:172) (cid:171)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:172) (cid:171)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:172)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ShiftMatrix Low-rankMatrix
Thisformulationcanhelpustocomputethepowerof𝐴fasterintheconvolutionalform,asdiscussedbyM.Zhangetal.
(2023).
E Theoretical Results
E.1 ProofofTheorem3.2
Inthispart,wewanttoprovethat⋇isassociative.Thisoperatorisdefinedas:
𝑝⋇𝑞 =
(cid:18)𝑝
1
𝑝
2
𝑝 3(cid:19)
⋇
(cid:18)𝑞
1
𝑞
2
𝑞 3(cid:19)
=
(cid:18)𝑞
1
⊙𝑝
1
𝑞
2
⊙𝑝
2
𝑞
1
⊗𝑝 3+𝑞
2
⊗𝑝 6+𝑞 3(cid:19)
𝑝
4
𝑝
5
𝑝
6
𝑞
4
𝑞
5
𝑞
6
𝑞
4
⊙𝑝
4
𝑞
5
⊙𝑝
5
𝑞
4
⊗𝑝 3+𝑞
5
⊗𝑝 6+𝑞
6
Accordingly,wehave:
(𝑝⋇𝑞)⋇𝑟 =
(cid:18)𝑞
1
⊙𝑝
1
𝑞
2
⊙𝑝
2
𝑞
1
⊗𝑝 3+𝑞
2
⊗𝑝 6+𝑞 3(cid:19)
⋇
(cid:18)𝑟
1
𝑟
2
𝑟 3(cid:19)
, (60)
𝑞
4
⊙𝑝
4
𝑞
5
⊙𝑝
5
𝑞
4
⊗𝑝 3+𝑞
5
⊗𝑝 6+𝑞
6
𝑟
4
𝑟
5
𝑟
6
re-usingthedefinitionof⋇,wehave:
(𝑝⋇𝑞)⋇𝑟 =
(cid:18)𝑞
1
⊙𝑝
1
𝑞
2
⊙𝑝
2
𝑞
1
⊗𝑝 3+𝑞
2
⊗𝑝 6+𝑞 3(cid:19)
⋇
(cid:18)𝑟
1
𝑟
2
𝑟 3(cid:19)
(61)
𝑞
4
⊙𝑝
4
𝑞
5
⊙𝑝
5
𝑞
4
⊗𝑝 3+𝑞
5
⊗𝑝 6+𝑞
6
𝑟
4
𝑟
5
𝑟
6
= (cid:18)𝑟 1 ⊙ (𝑞 1 ⊙𝑝 1) 𝑟 2 ⊙ (𝑞 2 ⊙𝑝 2) 𝑟 1 ⊗ (𝑞 1 ⊗𝑝 3+𝑞 2 ⊗𝑝 6+𝑞 3)+𝑟 2 ⊙ (𝑞 4 ⊗𝑝 3+𝑞 5 ⊗𝑝 6+𝑞 6)+𝑟 3(cid:19) (62)
𝑟 4 ⊙ (𝑞 4 ⊙𝑝 4) 𝑟 5 ⊙ (𝑞 5 ⊙𝑝 5) 𝑟 4 ⊗ (𝑞 1 ⊗𝑝 3+𝑞 2 ⊗𝑝 6+𝑞 3)+𝑟 4 ⊗ (𝑞 4 ⊗𝑝 3+𝑞 5 ⊗𝑝 6+𝑞 6)+𝑟 6
Usingthefactthat⊙and⊗areassociative,wehave:
(𝑝⋇𝑞)⋇𝑟 =
(cid:18)𝑞
1
⊙𝑝
1
𝑞
2
⊙𝑝
2
𝑞
1
⊗𝑝 3+𝑞
2
⊗𝑝 6+𝑞 3(cid:19)
⋇
(cid:18)𝑟
1
𝑟
2
𝑟 3(cid:19)
(63)
𝑞
4
⊙𝑝
4
𝑞
5
⊙𝑝
5
𝑞
4
⊗𝑝 3+𝑞
5
⊗𝑝 6+𝑞
6
𝑟
4
𝑟
5
𝑟
6
= (cid:18)𝑟 1 ⊙ (𝑞 1 ⊙𝑝 1) 𝑟 2 ⊙ (𝑞 2 ⊙𝑝 2) 𝑟 1 ⊗ (𝑞 1 ⊗𝑝 3+𝑞 2 ⊗𝑝 6+𝑞 3)+𝑟 2 ⊙ (𝑞 4 ⊗𝑝 3+𝑞 5 ⊗𝑝 6+𝑞 6)+𝑟 3(cid:19) (64)
𝑟 4 ⊙ (𝑞 4 ⊙𝑝 4) 𝑟 5 ⊙ (𝑞 5 ⊙𝑝 5) 𝑟 4 ⊗ (𝑞 1 ⊗𝑝 3+𝑞 2 ⊗𝑝 6+𝑞 3)+𝑟 4 ⊗ (𝑞 4 ⊗𝑝 3+𝑞 5 ⊗𝑝 6+𝑞 6)+𝑟 6
=
(cid:18)𝑝
1
𝑝
2
𝑝 3(cid:19)
⋇
(cid:18)𝑟
1
⊙𝑞
1
𝑟
2
⊙𝑞
2
𝑟
1
⊗𝑞 3+𝑟
2
⊗𝑞 6+𝑟 3(cid:19)
(65)
𝑝
4
𝑝
5
𝑝
6
𝑟
4
⊙𝑞
4
𝑟
5
⊙𝑞
5
𝑟
4
⊗𝑞 3+𝑟
5
⊗𝑞 6+𝑟
6
=𝑝⋇(𝑞⋇𝑟), (66)
whichprovesthetheorem.
E.2 ProofofTheorem3.3
Foreach𝑣,𝑡,wecanpre-computeB1x𝑣,𝑡 andB2x𝑣,𝑡+1.Accordingly,allthefollowingparametersarepre-computed:
(cid:18) (cid:19)
𝑐(𝑖,𝑗,𝑘,ℓ) = A1 A2 B1x𝑣+𝑖,𝑡+𝑗 , (67)
𝑣,𝑡 A3 A4 B2x𝑣+𝑘,𝑡+ℓ
21(cid:32) S(1)(cid:33) (cid:18)𝐼 𝐼 0(cid:19)
forallinputsx𝑣,𝑡 and𝑖,𝑗,𝑘,ℓ ∈ {0,1}.Now,startingfrom 0 (, 20
)
=
𝐼 𝐼 0
,wehave:
S
0,0
(cid:18) S0,1(cid:19)
=
(cid:18)𝐼 𝐼 0(cid:19)
⋇
(cid:18) A1 A2 B1x0,1(cid:19)
(68)
S1,0 𝐼 𝐼 0 A3 A4 B2x1,0
(cid:18) (cid:19)
=
A1 A2 B1x0,1
. (69)
A3 A4 B2x1,0
Re-usingoperator⋇,wehave:
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
S1,1
=
S0,1
⋇
A1 A2 B1x1,1
(70)
S1,1 S1,0 A3 A4 B2x1,1
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Pre-computed
=
(cid:18) A2
1
A2
2
A1B1x0,1+A2B2x1,0+B1x1,1(cid:19)
(71)
A3 A2
4
A3B1x0,1+A4B2x1,0+B2x1,1
Lookingatthethirdelementofeachrow,theseelementsarecalculatingthehiddenstatesoftherecurrent(itcanbeshown
byastraightforwardinduction). Accordingly,usingthisoperation,wecanrecursivelycalculatethetheoutputsof2D
SSM.
However,usingTheorem3.2,weknowthatthisisanassociativeoperation,soinsteadofcalculatingintherecurrentform,
wecanuseparallelpre-fixsummakethiscomputationparallel,decreasingthesequentialoperationsrequiredtocalculate
thehiddenstates.Notethatsinceouraboveoperationcanmodeltheproblemasanparallelprefix,allthealgorithmsfor
thisproblemcanbeusedtoenhancetheefficiency.
E.3 ProofofTheorem3.4
Toprovethistheorem,weneedto(1)showthatChimeracanrecoverSpaceTime.Giventhis,sinceSpaceTimeiscapableof
recoveringARIMA(Bartholomew1971),exponentialsmoothing(Winters1960),andcontrollablelineartime–invariant
systems(C.-T.Chen1984),wecanconcludethatChimeracanalsorecoverthesemethods.Then,(2)weneedtoprovethat
ChimeracanrecoverSARIMA.ThisisthemodelthatSpaceTimeisnotcapableofrecoveringduetotheadditionalseasonal
terms.
NotethatusingA2 =A3 =A4 =0,resultsina1DSSM,withcompanionmatrixasthestructureofA1,whichisSpaceTime.
Accordingly,SpaceTimeisaspecialcaseofChimerawhentherecurrenceonlyhappenalongthetimedirection.
Note that as discussed in Proposition 3.1, multiplying the discretization parameter Δ results in multiplying the steps.
Accordingly,using𝑠 astheΔinourseasonalmoduleandalsolettingA2 =A3 =A4 =0fortheseasonalmodule,wecan
modeltheseasonaltermsintheformulationofSAR(𝑝,𝑞,𝑠),meaningthatChimeracanalsorecoverSARIMAwhichis
ARIMAwithseasonalterms. NotethatthereasonthatChimeraiscapableofsuchmodelingisthatitusestwoheads
separatelyfortrendandseasonalterms.Therefore,usingdifferentdiscretizationparameters,eachcanmodeltheirown
correspondingtermsinSAR(𝑝,𝑞,𝑠).
E.4 ProofofTheorem3.5
Similartotheabove,usingA2 = A3 = 0,ourformulationisequivalenttoS4D,whileweusediagonalmatricesasthe
structureofA1.Similarly,asdiscussedbyBehrouz,Santacatterina,andZabih(2024),MambaMixerisequivalenttoS4ND
but on patched data. Using our Theorem 5, we can recover linear layers, resulting in recovering TSMixer by setting
A2 =A3 =0.
E.5 ProofofTheorem3.6
WeinfactwillshowthatrestrictingChimeraresultsinrecovering2DSSM(Baron,Zimerman,andWolf2024).Asdiscussed
earlier,thismethoddonotusediscretizationandinitiallystartsfromadiscretesystem.Also,itusesinput-independent
parameters. Therefore, we use LinearΔ1(.) = LinearΔ2(.) as broadcast function, and restrict Chimera to have input-
independentparameters,thenChimeracanrecover2DSSM(Baron,Zimerman,andWolf2024).
22F Experimental Settings
WeprovidethedescriptionofdatasetsinTable7.
Table7:Datasetdescriptions.Thedatasetsizeisorganizedin(Train,Validation,Test).
Tasks Dataset Dim SeriesLength DatasetSize Information(Frequency)
ETTm1,ETTm2 7 {96,192,336,720} (34465,11521,11521) Electricity(15mins)
ETTh1,ETTh2 7 {96,192,336,720} (8545,2881,2881) Electricity(15mins)
Forecasting Electricity 321 {96,192,336,720} (18317,2633,5261) Electricity(Hourly)
(Long-term) Traffic 862 {96,192,336,720} (12185,1757,3509) Transportation(Hourly)
Weather 21 {96,192,336,720} (36792,5271,10540) Weather(10mins)
Exchange 8 {96,192,336,720} (5120,665,1422) Exchangerate(Daily)
ILI 7 {24,36,48,60} (617,74,170) Illness(Weekly)
M4-Yearly 1 6 (23000,0,23000) Demographic
M4-Quarterly 1 8 (24000,0,24000) Finance
Forecasting M4-Monthly 1 18 (48000,0,48000) Industry
(short-term) M4-Weakly 1 13 (359,0,359) Macro
M4-Daily 1 14 (4227,0,4227) Micro
M4-Hourly 1 48 (414,0,414) Other
EthanolConcentration 3 1751 (261,0,263) AlcoholIndustry
FaceDetection 144 62 (5890,0,3524) Face(250Hz)
Handwriting 3 152 (150,0,850) Handwriting
Heartbeat 61 405 (204,0,205) HeartBeat
Classification JapaneseVowels 12 29 (270,0,370) Voice
(UEA) PEMS-SF 963 144 (267,0,173) Transportation(Daily)
SelfRegulationSCP1 6 896 (268,0,293) Health(256Hz)
SelfRegulationSCP2 7 1152 (200,0,180) Health(256Hz)
SpokenArabicDigits 13 93 (6599,0,2199) Voice(11025Hz)
UWaveGestureLibrary 3 315 (120,0,320) Gesture
SMD 38 100 (566724,141681,708420) ServerMachine
Anomaly MSL 55 100 (44653,11664,73729) Spacecraft
Detection SMAP 25 100 (108146,27037,427617) Spacecraft
SWaT 51 100 (396000,99000,449919) Infrastructure
PSM 25 100 (105984,26497,87841) ServerMachine
F.1 Baselines
Inourexperiments,weusethefollowingbaselines:
• Table8:TSM2(Behrouz,Santacatterina,andZabih2024),Simba(BadriN.PatroandVijayS.Agneeswaran2024),
TCN(LuoandX.Wang2024),iTransformer(YongLiu,Hu,etal.2024),RLinear(Z.Lietal.2023),PatchTST(Nieetal.
2023),Crossformer(Y.ZhangandYan2023),TiDE(Dasetal.2023),TimesNet(H.Wu,Hu,etal.2023),DLinear(Zeng
etal.2023b),SCINet(M.Liuetal.2022),FEDformer(T.Zhou,Z.Ma,Wen,X.Wang,etal.2022),Stationary(YongLiu,
H.Wu,etal.2022a),Autoformer(H.Wu,Xu,etal.2021)
23• Table 9: ModernTCN (Luo and X. Wang 2024), PatchTST (Nie et al. 2023), TimesNet (H. Wu, Hu, et al. 2023),
N-HiTS(Challuetal.2022),N-BEATS∗(Oreshkinetal.2019),ETSformer(Wooetal.2022),LightTS(T.Zhangetal.
2022),DLinear(Zengetal.2023a),FEDformer(T.Zhou,Z.Ma,Wen,X.Wang,etal.2022),Stationary(YongLiu,
H.Wu,etal.2022b),Autoformer(H.Wu,Xu,etal.2021),Pyraformer(S.Liuetal.2021),Informer(H.Zhouetal.
2021),Reformer(Kitaev,Kaiser,andLevskaya2020),LSTM(HochreiterandJ.Schmidhuber1997)
• Table 10: LSTM (Hochreiter and J. Schmidhuber 1997), LSTNet (Lai et al. 2018), LSSL (Gu, Goel, and Re 2022),
Trans.former(Vaswanietal.2017),Reformer(Kitaev,Kaiser,andLevskaya2020),Informer(H.Zhouetal.2021),
Pyraformer (S. Liu et al. 2021), Autoformer (H. Wu, Xu, et al. 2021), Station. (Yong Liu, H. Wu, et al. 2022b),
FEDformer(T.Zhou,Z.Ma,Wen,X.Wang,etal.2022),ETSformer(Wooetal.2022),Flowformer(H.Wu,J.Wu,etal.
2022),DLinear(Zengetal.2023a),LightTS.(T.Zhangetal.2022),TimesNet(H.Wu,Hu,etal.2023),PatchTST(Nie
etal.2023),MTCN(LuoandX.Wang2024)
For the results of the baselines, we re-use the results reported by H. Wu, Hu, et al. (2023), or from the original cited
papers.
G Additional Experimental Results
G.1 LongTermForecastingFullResults
ThecompleteresultsoflongtermforecastingarereportedinTable8.
G.2 Short-TermForecasting
ThecompleteresultsofshorttermforecastingarereportedinTable9.
G.3 Classification
ThecompleteresultsoftimeseriesclassificationarereportedinTable10.
G.4 AnomalyDetection
ThecompleteresultsofanomalydetectiontasksarereportedinTable11.
24Table8:Long-termforecastingtaskwithdifferenthorizonsH.Thefirst,second,andthirdbestresultsarehighlightedinred(bold),
orange(underline),andpurple.
Chimera TSM2 Simba TCN iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear SCINet FEDformerStationaryAutoformer
(ours) 2024 2024 2024 2024 2023 2023 2023 2023 2023 2023 2022 2022 2022 2021
MSE MAEMSEMAEMSEMAE MSE MAEMSE MAE MSEMAEMSEMAEMSE MAE MSEMAEMSEMAEMSEMAEMSEMAEMSEMAE MSEMAEMSE MAE
96 0.2930.3510.322 - 0.3240.360 0.2920.3460.334 0.368 0.3550.3760.3290.3670.404 0.426 0.3640.3870.3380.3750.3450.3720.4180.4380.3790.4190.3860.3980.505 0.475
192 0.3290.3620.349 - 0.3630.382 0.3320.3680.377 0.391 0.3910.3920.3670.3850.450 0.451 0.3980.4040.3740.3870.3800.3890.4390.4500.4260.4410.4590.4440.553 0.496
336 0.3520.3830.366 - 0.3950.405 0.3650.3910.426 0.420 0.4240.4150.3990.4100.532 0.515 0.4280.4250.4100.4110.4130.4130.4900.4850.4450.4590.4950.4640.621 0.537
720 0.4080.4120.407 - 0.4510.437 0.4160.4170.491 0.459 0.4870.4500.4540.4390.666 0.589 0.4870.4610.4780.4500.4740.4530.5950.5500.5430.4900.5850.5160.671 0.561
Avg 0.3450.3770.361 - 0.3830.396 0.3510.3810.407 0.410 0.4140.4070.3870.4000.513 0.496 0.4190.4190.4000.4060.4030.4070.4850.4810.4480.4520.4810.4560.588 0.517
96 0.1680.2610.173 - 0.1770.263 0.1660.2560.180 0.264 0.1820.2650.1750.2590.287 0.366 0.2070.3050.1870.2670.1930.2920.2860.3770.2030.2870.1920.2740.255 0.339
192 0.2150.2890.230 - 0.2450.306 0.2220.2930.250 0.309 0.2460.3040.2410.3020.414 0.492 0.2900.3640.2490.3090.2840.3620.3990.4450.2690.3280.2800.3390.281 0.340
336 0.2780.3370.279 - 0.3040.343 0.2720.3240.311 0.348 0.3070.3420.3050.3430.597 0.542 0.3770.4220.3210.3510.3690.4270.6370.5910.3250.3660.3340.3610.339 0.372
720 0.3410.3780.388 - 0.4000.399 0.3510.3810.412 0.407 0.4070.3980.4020.4001.730 1.042 0.5580.5240.4080.4030.5540.5220.9600.7350.4210.4150.4170.4130.433 0.432
Avg 0.2500.3160.267 - 0.2710.327 0.2530.3140.288 0.332 0.2860.3270.2810.3260.757 0.610 0.3580.4040.2910.3330.3500.4010.5710.5370.3050.3490.3060.3470.327 0.371
96 0.3620.3910.375 - 0.3790.395 0.3680.3940.386 0.405 0.3860.3950.4140.4190.423 0.448 0.4790.4640.3840.4020.3860.4000.6540.5990.3760.4190.5130.4910.449 0.459
192 0.3980.4150.398 - 0.4320.424 0.4050.4130.441 0.436 0.4370.4240.4600.4450.471 0.474 0.5250.4920.4360.4290.4370.4320.7190.6310.4200.4480.5340.5040.500 0.482
336 0.4020.4160.419 - 0.4730.443 0.3910.4120.487 0.458 0.4790.4460.5010.4660.570 0.546 0.5650.5150.4910.4690.4810.4590.7780.6590.4590.4650.5880.5350.521 0.496
720 0.4580.4770.422 - 0.4830.469 0.4500.4610.503 0.491 0.4810.4700.5000.4880.653 0.621 0.5940.5580.5210.5000.5190.5160.8360.6990.5060.5070.6430.6160.514 0.512
Avg 0.4050.4240.403 - 0.4410.432 0.4040.4200.454 0.447 0.4460.4340.4690.4540.529 0.522 0.5410.5070.4580.4500.4560.4520.7470.6470.4400.4600.5700.5370.496 0.487
96 0.2570.3250.253 - 0.2900.339 0.2630.3320.297 0.349 0.2880.3380.3020.3480.745 0.584 0.4000.4400.3400.3740.3330.3870.7070.6210.3580.3970.4760.4580.346 0.388
192 0.3140.3690.334 - 0.3730.390 0.3200.3740.380 0.400 0.3740.3900.3880.4000.877 0.656 0.5280.5090.4020.4140.4770.4760.8600.6890.4290.4390.5120.4930.456 0.452
336 0.3160.3810.347 - 0.3760.406 0.3130.3760.428 0.432 0.4150.4260.4260.4331.043 0.731 0.6430.5710.4520.4520.5940.5411.0000.7440.4960.4870.5520.5510.482 0.486
720 0.3880.4270.401 - 0.4070.431 0.3920.4330.427 0.445 0.4200.4400.4310.4461.104 0.763 0.8740.6790.4620.4680.8310.6571.2490.8380.4630.4740.5620.5600.515 0.511
Avg 0.3180.3750.333 - 0.3610.391 0.3220.3790.383 0.407 0.3740.3980.3870.4070.942 0.684 0.6110.5500.4140.4270.5590.5150.9540.7230.4370.4490.5260.5160.450 0.459
96 0.1320.2340.142 - 0.1650.253 0.1290.2260.148 0.240 0.2010.2810.1810.2700.219 0.314 0.2370.3290.1680.2720.1970.2820.2470.3450.1930.3080.1690.2730.201 0.317
192 0.1440.2230.153 - 0.1730.262 0.1430.2390.162 0.253 0.2010.2830.1880.2740.231 0.322 0.2360.3300.1840.2890.1960.2850.2570.3550.2010.3150.1820.2860.222 0.334
336 0.1560.2590.175 - 0.1880.277 0.1610.2590.178 0.269 0.2150.2980.2040.2930.246 0.337 0.2490.3440.1980.3000.2090.3010.2690.3690.2140.3290.2000.3040.231 0.338
720 0.1840.2800.209 - 0.2140.305 0.1910.2860.225 0.317 0.2570.3310.2460.3240.280 0.363 0.2840.3730.2200.3200.2450.3330.2990.3900.2460.3550.2220.3210.254 0.361
Avg 0.1540.2490.169 - 0.1850.274 0.1560.2530.178 0.270 0.2190.2980.2050.2900.244 0.334 0.2510.3440.1920.2950.2120.3000.2680.3650.2140.3270.1930.2960.227 0.338
96 0.0770.1980.163 - - - 0.0800.1960.086 0.206 0.0930.2170.0880.2050.256 0.367 0.0940.2180.1070.2340.0880.2180.2670.3960.1480.2780.1110.2370.197 0.323
192 0.1590.2700.229 - - - 0.1660.2880.177 0.299 0.1840.3070.1760.2990.470 0.509 0.1840.3070.2260.3440.1760.3150.3510.4590.2710.3150.2190.3350.300 0.369
336 0.3110.3440.383 - - - 0.3070.3980.331 0.417 0.3510.4320.3010.3971.268 0.883 0.3490.4310.3670.4480.3130.4271.3240.8530.4600.4270.4210.4760.509 0.524
720 0.6970.6230.999 - - - 0.6560.5820.847 0.691 0.8860.7140.9010.7141.767 1.068 0.8520.6980.9640.7460.8390.6951.0580.7971.1950.6951.0920.7691.447 0.941
Avg 0.3110.3580.443 - - - 0.3020.3660.360 0.403 0.3780.4170.3670.4040.940 0.707 0.3700.4130.4160.4430.3540.4140.7500.6260.5190.4290.4610.4540.613 0.539
96 0.3660.2480.396 - 0.4680.268 0.3680.2530.395 0.268 0.6490.3890.4620.2950.522 0.290 0.8050.4930.5930.3210.6500.3960.7880.4990.5870.3660.6120.3380.613 0.388
192 0.3940.2920.408 - 0.4130.317 0.3790.2610.417 0.276 0.6010.3660.4660.2960.530 0.293 0.7560.4740.6170.3360.5980.3700.7890.5050.6040.3730.6130.3400.616 0.382
336 0.4090.3110.427 - 0.5290.284 0.3970.2700.433 0.283 0.6090.3690.4820.3040.558 0.305 0.7620.4770.6290.3360.6050.3730.7970.5080.6210.3830.6180.3280.622 0.337
720 0.4430.2940.449 - 0.5640.297 0.4400.2960.467 0.302 0.6470.3870.5140.3220.589 0.328 0.7190.4490.6400.3500.6450.3940.8410.5230.6260.3820.6530.3550.660 0.408
Avg 0.4030.2860.420 - 0.4930.291 0.3980.2700.428 0.282 0.6260.3780.4810.3040.550 0.304 0.7600.4730.6200.3360.6250.3830.8040.5090.6100.3760.6240.3400.628 0.379
96 0.1460.2060.161 - 0.1760.219 0.1490.2000.174 0.214 0.1920.2320.1770.2180.158 0.230 0.2020.2610.1720.2200.1960.2550.2210.3060.2170.2960.1730.2230.266 0.336
192 0.1890.2390.208 - 0.2220.260 0.1960.2450.221 0.254 0.2400.2710.2250.2590.206 0.277 0.2420.2980.2190.2610.2370.2960.2610.3400.2760.3360.2450.2850.307 0.367
336 0.2440.2810.252 - 0.2750.297 0.2380.2770.278 0.296 0.2920.3070.2780.2970.272 0.335 0.2870.3350.2800.3060.2830.3350.3090.3780.3390.3800.3210.3380.359 0.395
720 0.2970.3090.337 - 0.3500.349 0.3140.3340.358 0.347 0.3640.3530.3540.3480.398 0.418 0.3510.3860.3650.3590.3450.3810.3770.4270.4030.4280.4140.4100.419 0.428
Avg 0.2190.2580.239 - 0.2550.280 0.2240.2640.258 0.278 0.2720.2910.2590.2810.259 0.315 0.2710.3200.2590.2870.2650.3170.2920.3630.3090.3600.2880.3140.338 0.382
25
1mTTE
2mTTE
1hTTE
2hTTE
LCE
egnahcxE
cffiarT
rehtaeWTable9: Fullresultsfortheshort-termforecastingtaskintheM4dataset. ∗.intheTransformersindicatesthenameof
∗former.StationarymeanstheNon-stationaryTransformer.
Models
ChimeraModernTCNPatchTSTTimesNetN-HiTSN-BEATS∗ ETS∗ LightTSDLinearFED∗StationaryAuto∗Pyra∗ In∗ Re∗ LSTM
(ours) 2024 2023 2023 2022 2019 2022 2022 2023 2022 2022 2021 2021 2021 2020 1997
SMAPE 13.107 13.226 13.258 13.387 13.418 13.436 18.009 14.247 16.965 13.728 13.717 13.97415.53014.72716.169176.040
MASE 2.902 2.957 2.985 2.996 3.045 3.043 4.487 3.109 4.283 3.048 3.078 3.134 3.711 3.418 3.800 31.033
OWA 0.767 0.777 0.781 0.786 0.793 0.794 1.115 0.827 1.058 0.803 0.807 0.822 0.942 0.881 0.973 9.290
SMAPE 9.892 9.971 10.179 10.100 10.202 10.124 13.376 11.364 12.145 10.792 10.958 11.33815.44911.36013.313172.808
MASE 1.105 1.167 0.803 1.182 1.194 1.169 1.906 1.328 1.520 1.283 1.325 1.365 2.350 1.401 1.775 19.753
OWA 0.853 0.878 0.803 0.890 0.899 0.886 1.302 1.000 1.106 0.958 0.981 1.012 1.558 1.027 1.252 15.049
SMAPE 12.549 12.556 12.641 12.670 12.791 12.677 14.588 14.014 13.514 14.260 13.917 13.95817.64214.06220.128143.237
MASE 0.914 0.917 0.930 0.933 0.969 0.937 1.368 1.053 1.037 1.102 1.097 1.103 1.913 1.141 2.614 16.551
OWA 0.864 0.866 0.876 0.878 0.899 0.880 1.149 0.981 0.956 1.012 0.998 1.002 1.511 1.024 1.927 12.747
SMAPE 4.685 4.715 4.946 4.891 5.061 4.925 7.267 15.880 6.709 4.954 6.302 5.485 24.78624.46032.491186.282
MASE 3.007 3.107 2.985 3.302 3.216 3.391 5.240 11.434 4.953 3.264 4.064 3.865 18.58120.96033.355119.294
OWA 0.983 0.986 1.044 1.035 1.040 1.053 1.591 3.474 1.487 1.036 1.304 1.187 5.538 5.013 8.679 38.411
SMAPE 11.618 11.698 11.807 11.829 11.927 11.851 14.718 13.525 13.639 12.840 12.780 12.90916.98714.08618.200160.031
MASE 1.528 1.556 1.590 1.585 1.613 1.599 2.408 2.111 2.095 1.701 1.756 1.771 3.265 2.718 4.223 25.788
OWA 0.827 0.838 0.851 0.851 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.775 12.642
Table10: Fullresultsfortheclassificationtask(accuracy%). Weomit“former”fromthenamesofTransformer-based
methods.Forallmethods,thestandarddeviationislessthan0.1%.
LSTMLSTNetLSSLTrans.Re. In. Pyra.Auto.Station.FED./ETS./Flow./DLinear/LightTS./TimesNet/PatchTST/MTCN/Chimera
Datasets/Models
1997 2018 2022 2017 20202021 2021 2021 2022 2022 2022 2022 2023 2022 2023 2023 2024 (ours)
EthanolConcentration32.3 39.9 31.1 32.7 31.931.6 30.8 31.6 32.7 31.2 28.1 33.8 32.6 29.7 35.7 32.8 36.3 39.8
FaceDetection 57.7 65.7 66.7 67.3 68.667.0 65.7 68.4 68.0 66.0 66.3 67.6 68.0 67.5 68.6 68.3 70.8 70.4
Handwriting 15.2 25.8 24.6 32.0 27.432.8 29.4 36.7 31.6 28.0 32.5 33.8 27.0 26.1 32.1 29.6 30.6 32.9
Heartbeat 72.2 77.1 72.7 76.1 77.180.5 75.6 74.6 73.7 73.7 71.2 77.6 75.1 75.1 78.0 74.9 77.2 81.3
JapaneseVowels 79.7 98.1 98.4 98.7 97.898.9 98.4 96.2 99.2 98.4 95.9 98.9 96.2 96.2 98.4 97.5 98.8 99.1
PEMS-SF 39.9 86.7 86.1 82.1 82.781.5 83.2 82.7 87.3 80.9 86.0 83.8 75.1 88.4 89.6 89.3 89.1 89.5
SelfRegulationSCP1 68.9 84.0 90.8 92.2 90.490.1 88.1 84.0 89.4 88.7 89.6 92.5 87.3 89.8 91.8 90.7 93.4 93.7
SelfRegulationSCP2 46.6 52.8 52.2 53.9 56.753.3 53.3 50.6 57.2 54.4 55.0 56.1 50.5 51.1 57.2 57.8 60.3 59.9
SpokenArabicDigits 31.9100.0100.0 98.4 97.0100.099.6100.0 100.0 100.0100.0 98.8 81.4 100.0 99.0 98.3 98.7 100.0
UWaveGestureLibrary41.2 87.8 85.9 85.6 85.685.6 83.4 85.9 87.5 85.3 85.0 86.6 82.1 80.3 85.3 85.8 86.7 86.7
AverageAccuracy 48.6 71.8 70.9 71.9 71.572.1 70.8 71.1 72.7 70.7 71.0 73.0 67.5 70.4 73.6 72.5 74.2 75.3
26
dethgieW
ylraeY
ylretrauQ
ylhtnoM
srehtO
egarevATable11: Fullresultsfortheanomalydetectiontask. TheP,RandF1representtheprecision, recallandF1-score(%)
respectively.AhighervalueofP,RandF1indicatesabetterperformance.
Datasets SMD MSL SMAP SWaT PSM AvgF1
Metrics P R F1 P R F1 P R F1 P R F1 P R F1 (%)
LSTM 1997 78.5265.4771.41 78.0486.2281.93 91.0657.4970.48 78.0691.7284.34 69.2499.5381.67 77.97
Transformer 2017 83.5876.1379.56 71.5787.3778.68 89.3757.1269.70 68.8496.5380.37 62.7596.5676.07 76.88
LogTrans 2019 83.4670.1376.21 73.0587.3779.57 89.1557.5969.97 68.6797.3280.52 63.0698.0076.74 76.60
TCN 2019 84.0679.0781.49 75.1182.4478.60 86.9059.2370.45 76.5995.7185.09 54.5999.7770.57 77.24
Reformer 2020 82.5869.2475.32 85.5183.3184.40 90.9157.4470.40 72.5096.5382.80 59.9395.3873.61 77.31
Informer 2021 86.6077.2381.65 81.7786.4884.06 90.1157.1369.92 70.2996.7581.43 64.2796.3377.10 78.83
Anomaly∗ 2021 88.9182.2385.49 79.6187.3783.31 91.8558.1171.18 72.5197.3283.10 68.3594.7279.40 80.50
Pyraformer 2021 85.6180.6183.04 83.8185.9384.86 92.5457.7171.09 87.9296.0091.78 71.6796.0282.08 82.57
Autoformer 2021 88.0682.3585.11 77.2780.9279.05 90.4058.6271.12 89.8595.8192.74 99.0888.1593.29 84.26
LSSL 2022 78.5165.3271.31 77.5588.1882.53 89.4353.4366.90 79.0593.7285.76 66.0292.9377.20 76.74
Stationary 2022 88.3381.2184.62 68.5589.1477.50 89.3759.0271.09 68.0396.7579.88 97.8296.7697.29 82.08
DLinear 2023 83.6271.5277.10 84.3485.4284.88 92.3255.4169.26 80.9195.3087.52 98.2889.2693.55 82.46
ETSformer 2022 87.4479.2383.13 85.1384.9385.03 92.2555.7569.50 90.0280.3684.91 99.3185.2891.76 82.87
LightTS 2022 87.1078.4282.53 82.4075.7878.95 92.5855.2769.21 91.9894.7293.33 98.3795.9797.15 84.23
FEDformer 2022 87.9582.3985.08 77.1480.0778.57 90.4758.1070.76 90.1796.4293.19 97.3197.1697.23 84.97
TimesNet(I) 2023 87.7682.6385.12 82.9785.4284.18 91.5057.8070.85 88.3196.2492.10 98.2292.2195.21 85.49
TimesNet(R) 2023 88.6683.1485.81 83.9286.4285.15 92.5258.2971.52 86.7697.3291.74 98.1996.7697.47 86.34
CrossFormer 2023 83.6 76.6179.70 84.6883.7184.19 92.0455.3769.14 88.4993.4890.92 97.1689.7393.30 83.45
PatchTST 2023 87.4281.6584.44 84.0786.2385.14 92.4357.5170.91 80.7094.9387.24 98.8793.9996.37 84.82
ModernTCN 2024 87.8683.8585.81 83.9485.9384.92 93.1757.6971.26 91.8395.9893.86 98.0996.3897.23 86.62
Chimera (ours) 87.7483.2985.46 84.0186.8385.39 93.0558.1271.55 92.1895.9394.01 97.3096.1996.74 86.69
27