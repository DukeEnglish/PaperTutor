Coherent Zero-Shot Visual Instruction Generation
QuynhPhung SongweiGe Jia-BinHuang
UniversityofMarylandCollegePark
https://instruct-vis-zero.github.io/
1. A person holds a 2. A person paints the 3. A person pours 4. Once the egg is coated 1. Pouring milk into a steel 2. Adding chocolate 3. Aperson uses a 4. Pouring the hot
white plastic egg egg with green layer green glitter over the in glitter, give it thirty to pot and heating it on the powder or chopped spoon to stir the chocolate from pot into
egg sixty minutes to dry stove until it simmers chocolate to the hot milk chocolate a mug
Decorating an Easter egg Making a chocolate milk
1. Choosing a suitable 2. Using a tiller to turn 3. Planting various 4. Watering the newly 1. Looking for 2. Carefully detaching 3. Placing the plucked 4. Storing the basket in a
location in the yard for and aerate the soil in the vegetable seeds in the planted seeds with a strawberries that are fully ripe strawberries from strawberries into a cool place or
the vegetable garden marked area prepared soil watering can red without white or the plant by pinching the basket gently to avoid refrigerating to maintain
green spots, ready to pick stem between fingers squashing freshness
Plant vegetable in garden Harvest strawberries
1. Prepare the warm 2. A person massaging 3. A person gently pouring 4.A person using a 1. Adding potting soil 2. Placing a small 3. Using a can to water 4. Placing the pot in a
water for bathing dog shampoo into the or spraying water down towel to dry the dog to an empty pot houseplant or seedling the around the sunny spot
dog’s facial fur using the the back of the dog’s neck into soil houseplant
corner of a wet cloth from just below the ears
Bath a dog Plant a houseplant
1. Assembling and 2. Wrapping string 3. Hanging various 4. Placing a star on top 1. Use a knife to peel the 2. Slice the potato into 3. Fry the thin potato 4. Use a spoon to
positioning the Christmas lights around the ornaments on the of the Christmas tree. potato. thin, round pieces pieces in hot oil in drain the chips onto
tree in the living room Christmas tree branches of the tree using the knife the frying pan a plate
Making a Christmas tree Making potato chips
Figure1. Visualinstructiongenerationresults. Givenasequenceoftextualinstructionsforacertaintask, ourmethodgeneratesthe
visualinstructionsthatillustratetheindividualsteps.Ourmethodistraining-freeandthuspreservesthequalityandgeneralizabilityofthe
underlyingimagegenerationmodels. Weshowcasethegeneratedvisualinstructionsfordifferenttasksfromcookingtogardening. The
samplespossesshighvisualquality,alignwiththeinstructions,andmaintaincoherentobjectidentitywithdesiredchangesateachstep.
Abstract that require consistent representation and smooth state
transitions of objects across sequential steps remains a
formidable challenge. This paper introduces a simple,
Despite the advances in text-to-image synthesis, particu-
training-free framework to tackle the issues, capitalizing
larly with diffusion models, generating visual instructions
1
4202
nuJ
6
]VC.sc[
1v73340.6042:viXraMotivation
1.Pouringmilkintoa 2.Addingchocolate 3.Apersonusesa 4. Pouring the hot image diffusion models [1, 25, 50], which has shown re-
steelpotandheating powderorchopped spoon to stirthe chocolate from pot markablezero-shotcapacityandphotorealism,tovisualize
itonthestoveuntil chocolate to the chocolate intoamug
itsimmers hotmilk theinstructionsacrossawiderangeofproblemcategories.
Insteadoffine-tuningthemodelontheinstructionalimage
datasetasintheexistingmethods[4,41,65], whichcould
compromisethegenerationqualityandlimititselftocertain
categories,wedevelopatraining-freemethodofgenerating
visualinstructions.
Making hot chocolate
Directly inputting the instructions to the text-to-image
1. Adding potting 2. Placing a small 3. Using a can to 4.Placingthepot
soil to an empty houseplant or Water the around inasunnyspot models incurs several issues, as shown by the examples in
pot seedlingintosoil thehouseplant
Figure 2. First, most instructions explain the procedure of
actions,whilethetext-to-imagemodelexpectsthedescrip-
tionoftheimagecontent. Asshowninstep2ofthe“Mak-
ing hot chocolate” example, the container of the milk be-
comes a glass bowl instead of the steel pot as mentioned
Planting a houseplant in step 1 due to the lack of information in the instruction.
This necessitates an approach to bridging the gap between
Figure2. Limitationoftext-to-imagegenerationinvisualin-
instructional texts and the conditioning text used for im-
structionstask. Thecrucialcomponentsofgoodvisualinstruc-
age generation. Formally, given the procedures at the cur-
tionare1)alignmentwiththetext-basedinstructionand2)coher-
rent and previous steps, one needs to infer the proper im-
enceacrossdifferentstepsdemonstratingthestatechanges. The
agecontentatthecurrentstep. Tothisend,weproposean
current text-to-image generation methods focus only on the for-
mer. Consequently, the results may confuse the readers. In this instruction re-captioning strategy [3, 67] to convert the in-
paper,wedevelopatraining-freemethodtoenableamorecoher- structionaltextsintoactionsandstatesusinglargelanguage
entvisualinstructiongeneration. models (LLMs). We show that combining the action and
stateastheconditionsignificantlyenhancesthequalityand
relevanceofthegeneratedillustrations.
on the advancements in diffusion models and large lan-
In addition, the objects’ identity may alter arbitrarily
guage models (LLMs). Our approach systematically inte-
across different steps. As found in the “planting a house-
gratestextcomprehensionandimagegenerationtoensure
plan” example, the shapeand texture ofthe potin the first
visualinstructionsarevisuallyappealingandmaintaincon-
steplookdifferentfromtheoneinsteps3and4. Thisposes
sistencyandaccuracythroughouttheinstructionsequence.
acommonchallengeencounteredwhenusingtext-to-image
We validate the effectiveness by testing multi-step instruc-
models to generate multiple images - there often lacks co-
tions and comparing the text alignment and consistency
herency across the generated illustrations. Recent studies
withseveralbaselines. Ourexperimentsshowthatourap-
have made progress in maintaining consistency for human
proachcanvisualizecoherentandvisuallypleasinginstruc-
portraits[10,21,29],wheretheidentityfeaturescanbede-
tions.
rived from the models trained on extensive human-centric
datasets. However, our problem cannot benefit from the
same idea since the identity features of the general object
1.Introduction
categories,suchaskitchenutensils,areunavailable. There-
Textualinstructionsareamongthemostprevalenttoolsfor fore,werefertothegeneralmethodstoachieveconsistency,
grasping new skills and knowledge and solving real-world suchasfeaturesharingandinjection[69,71].
problems,largeandsmall.Generatingvisualillustrationsof Althoughthesemethodsimprovethegenerationconsis-
instructions has been a vital research problem [13, 28, 75] tency, we find that they also induce the “over-consistent”
for their straightforwardness and ability to transcend lan- probleminvisualinstructiongeneration.Specifically,many
guage barriers, providing an intuitive understanding of the instructionsinvolvechangedobjectsamongdifferentsteps.
textual instructions [74]. In addition, apart from convey- For instance, a recipe may contain a raw ingredient that is
ing information to humans, such visual data has also been chopped,seasoned,andpresentedinentirelydifferentstates
widelyadoptedtotrainroboticpolicies[5,14]. throughout the process. This inherent variability compli-
In this paper, we focus on generating static visual in- cates the problem of maintaining object consistency and
structions. Unlike generating instructional videos [42, 78] makesitunsuitabletousethevanillafeature-sharingstrat-
that demand temporal consistency, generating static visual egy. To this end, we propose an adaptive feature-sharing
illustrations poses a relatively more approachable prob- method with finer-grained constraints. First, similar to the
lem [65]. We leverage the recent advancements in text-to- previous method, we adopta local region constraint to en-
2force sharing to happen on certain pixels. However, we ation[77],personalizedimagegeneration[33,55],andeven
find that localizing the objects using attention maps be- non-generativetasks[34,66]. Inthispaper,weexploreap-
comes less reliable when the base model architecture dif- plyingthesepre-trainedmodelstogeneratevisualguidance
fers from the UNet in Stable Diffusion [46]. Instead, inazero-shotway.
we utilize large-scale segmentation models to produce the
Improving Consistency in Image Generation . Generat-
masks[32,35,38,53].Second,weapplyaglobalconstraint
ingconsistentimageshasbeenanimportantsub-problemin
onthefeaturesharingscalebetweeneachpairofstepsbased
differenttasks,includingvideogeneration[19,31,73,79],
ontheirsimilarity. Weexploitthereasoningcapabilitiesof
multi-view image generation [39, 60, 70], and character
the LLMs to develop a similarity matrix that characterizes
generation[10,21,29,69,72].Comparingwiththesetasks,
such state similarity. We show that our adaptive feature-
wefocusonimprovingtheconsistencyofthesharedobjects
sharingmethodenablesthevariabilityofobjectsacrossin-
in different steps of the visual instructions. Unlike video
structionalstepswhilestillpreservingobjectidentity.
or multi-view image generation, we don’t enforce hard-
AsshowcasedinFigure1,ourproposedmethodcangen-
constraint geometrical or temporal consistency. Different
erate high-quality, consistent visual representations based
fromthecharactergenerationthatonlycaresabouthuman
ontheinstructionaltexts. Wealsoperformquantitativeex-
identity,weneedtodealwithgeneralobjectsthatdon’thave
perimentswithvariousevaluationmetricstoablateindivid-
IDfeatureextractorsasthoseforhumans[10,21,29,72].
ualcomponentsofourmethodonthetext-imagealignment
and consistently. However, we notice that the image sim- Toachievebetterconsistency, manyexistingstudiesre-
ilarity metric only emphasizes the consistency aspect and sort to fine-tuning [17, 29, 37] partially or completely the
conflictswiththegoalofachievingvariationacrossdiffer- diffusionmodelsontheconsistentimagedata.Inthispaper,
entsteps. Tothisend,weproposeaframeworktoevaluate wetacklewithzero-shotconsistentimagegeneration[69].
the visual instruction generation quality using large-scale Previous studies have found that features in the diffusion
visual language models. We will release the code and full modelsencodedifferentinformationandcanbeutilizedto
evaluation suite for reproducible research. The contribu- control the generation. Cross-attention maps connect gen-
tionsofourworkcanbesummarizedasfollows: erationwithtextpromptsandcanbemanipulatedforaddi-
1. We develop a training-free method for generating vi- tionaltextualinformation[1,9,22,49].Self-attentionmaps
link pixels and encode rich structural information, which
sualinstructionswithpre-trainedtext-to-imagediffusion
hasbeenutilizedtoextractormodifythelayouts[7,45,49].
models.
The feature maps and noised latents contain more detailed
2. Weproposeanillustrationre-captioningstrategy,which
information and can be used to reproduce the exact intact
greatlyimprovesthegenerationqualityandrelevance.
regions like background [18, 71]. In this paper, we build
3. We introduce an adaptive feature-sharing method with
ontheseobservationsandproposeseveraltechniquestoim-
finer-grained constraints to maintain object identity
provetheconsistencyrequiredinvisualinstructiongenera-
acrossdifferentstepswhileallowingfornecessaryvari-
tion.Specifically,wecombinethecontrollabilityofferedby
ations.
these methods with the semantical understanding capacity
4. Wepresentaframeworktoevaluatethevisualinstruction
of Large Language Models [15, 36] for finer-grained co-
quality using large-scale visual language models. We
herency.
showthatourmethodcanpreservethegenerationquality
andshowapplicabilityacrossvariouscategories.
VisualInstructionGeneration.Bothvideoandimagecan
serveasthemediaforvisualinstructions. Wefocusonlyon
2.Relatedwork
generating static images as visual instructions [4, 41, 65].
Earlierworksoninstructiongenerationincluderecipegen-
Text-to-Image Generation with Diffusion Models . Dif- eration[11,57,58],whilewearealsointerestedinillustrat-
fusion models [24, 62–64] have become the ubiquitous ingrecipestepswithvisualinstructions. Morerecentworks
choice for visual generation for their effectiveness in scal- leveragethegreatgenerativepowerofpre-traineddiffusion
ingonvisualdatadistribution. Whentrainingonthelarge- modelsandfine-tunethesemodelsonthevisualinstruction
scaledatasets[6,59],improvedtrainingandsamplingwith datasets [42, 76]. Bordalo et al. [4] integrates an Alpaca-
advancedtechniques[23,26,43,54],thestate-of-the-artre- 7BmodelwithaStableDiffusionmodelforfine-tuningand
sults have been achieved in image [1, 12, 48, 50, 52, 56] generatingsequencesofvisualillustrationofrecipes. Gen-
and video [2, 17, 20, 27, 61] generation. As learning HowTo[65]curatesadatasetofstates, actions, andresult-
frombillionsofdatasamples,thepretraineddiffusionmod- ing transformations triplets and trains a conditioned diffu-
els have been shown to have great generalization capac- sion model on it. StackedDiffusion [41] fine-tuned a pre-
ity and thus been adapted to various downstream applica- trained text-to-image diffusion model with the stacked in-
tionssuchasimageediting[22,40,44],controllablegener- put on the Visual GoalStep Inference (VGSI) dataset [76].
3Different from these existing methods, we aim to use the H =A+·V+ ∈RP×dv, (3)
i i
pre-traineddiffusionmodelinazero-shotmannerforvisual
where⊕denotesconcatenation,N isthenumberofimages,
illustrationgeneration.
and i ∈ {1,2,···N}. However, this technique may not
3.Method generalizetoourproblembecausethegeneratedframesare
supposedtobesimilarinbothbackgroundandforeground,
Givenasetofinstructions,weharnessapre-trainedtext-to- wherevisualinstructionsmaycontaindynamicelements.
image diffusion model to generate the visual illustrations. Consistory [68] further proposed Self-Driven Self-
AsshowninFig.3,ourapproachcontainstwomajorstages. Attention, focusing on sharing keys and values within in-
First,tofillthedistributiongapbetweentheinstructionsand dividualobjectsacrossdifferentframes. Specifically,itex-
image descriptions, we perform in-context planning with tracts object masks M from the cross-attention maps and
LLMstore-captiontheinstructions.Second,weproposean assignsa−∞scoretotheself-attentionmapsforanypixel
adaptivefeature-sharingmethodfordynamic,coherentim- where the object mask value is zero, indicating that there
agegenerationgiventhere-captionedinstructions. Inboth shouldbenosharingintheseregions. Thisupdatestheself-
stages,weuseoff-the-shelfpre-trainedmodelswithoutany attentionmapcalculationwiththeobjectmasks:
extratraining.
M+ =[M ...M ⊕M ⊕M ...M ], (4)
3.1.Preliminaries i 1 i−1 i i+1 N
(cid:18) Q K+T (cid:19)
Text-to-image diffusion models. The text-to-image dif- A+ =softmax √i +logM+ ∈RP×N·P, (5)
i d i
fusion model incorporates a denoiser network D that is k
trained to estimate the noise in the current image, ϵ t = whereM i = 1,correspondingtotheimagesi-th. Thisen-
D(x t;c), where t represents the timestep, and c denotes suresthateachimageonlyattendstoitselforobjectregions
the conditional information embedding. During the infer- ofotherimages.However,thismethodstillassumestheob-
ence time, an initial random Gaussian noise is iteratively jectstobefullyconsistentacrossdifferentimages,whichis
denoisedtogeneratearealimage. notoftenthecaseinvisualinstructions.
Self-attentionlayer isessentialinDforintegratingglobal
3.2.Re-captioninginstructionsasdescriptivetexts
informationacrosstheentireimage. Itredistributesthefea-
turesfromeachspatiallocationtosimilarregions. Suppose Generating visual instructions from a sequence of textual
that x ∈ Rw×h×d denote the input feature map of some instructionspresentsasignificantchallengetocurrenttext-
self-attentionlayer,wherew,h,anddarethewidth,height, to-imagemodels. Tosolvetheproblem,themodelneedsto
anddimension. Forsimplicity, letP = h×w. Byapply- understand objects’ states and relationships across succes-
ing linear mappings to the feature map x to obtain the key sivesteps. Forinstance,asshownintheFig.2,considerthe
K ∈ RP×dk, value V ∈ RP×dv, and query Q ∈ RP×dk first two steps of making hot chocolate, where the milk is
where d is dimension of key K and query Q, d is di- firstpouredintoasteelpot,andthechocolateisthenadded
k v
mension of value V, the self-attention map At at step t is tothemilk.Ifonedirectlyusesinstructionsastheinput,the
generatedby: text-to-imagemodelswillnotbeinformedwiththecontext
of“milkinthesteelpot”,leadingtoanincorrectcontainer.
(cid:18) QK⊤(cid:19)
At =softmax √ ∈[0,1]P×P, (1) Animmediatesolutiontothisissueistoconcatenatethe
d k adjacentinstructions. However,thismayleadtoconflicted
informationastheinstructionsexpressdifferentactions. (It
Attention maps mechanism calculates the similarity be-
can seen in the second row of the Figure 9). Therefore,
tween query (Q) and key (K), which determine how much
we propose to leverage LLMs’ conversational understand-
attentioneachvalue(V)receives.
ing capabilities to first re-caption the instruction into de-
KV sharing. To maintain object consistency across in- tailedinputtexts.
stitutionalsteps,wedrawinspirationfromtechniquesused
Since the text-to-image models take the descriptive im-
invideogeneration[30,73]. Thesestudiessharekeysand
age captions as the input, we prompt the LLM to predict
values within the self-attention layers across frames to al-
the state of the scene given each instruction. The process
lowthequeriestoattendtoconsistentelementsinprevious isillustratedinthat 3,wherethestatesS = (s ,s ,...s )
0 1 N
frames,implementedbyconcatenatingkeysandvalues: arepredictedgivenasequentialsetofN instructionsA =
K+ =[K ⊕K ⊕···⊕K ],V+ =[V ⊕V ⊕···⊕V ] (a 0,a 1,...a N). We prompt the LLM by turns so that the
1 2 N 1 2 N
model predicts the state of scene s given the current in-
i
(cid:18) Q K+T(cid:19) struction and previous instructions and states. Therefore,
A+ =softmax √i ∈[0,1]P×N·P (2)
i d k the state s i contains the scene context after the first i in-
4The goal:Making pumpkin soup Stage 1: In-context planning with LLM
Action State (after the action at current step) State similarity
𝑎 !: Peel and chop the pumpkin into cubes on a cutting board 𝑠 !:There is a cutting board with peeled and cubed pumpkin [ [1.0, 0.6, 0.6] 𝑊 = 𝑊
LLM #,% %,#
𝑎 ": Place pumpkin cubes in a and boil 𝑠 ": A si tp so ot n c to hn et a si tn oi vn eg softly boiled pumpkin cubes in broth [0.6, 1.0, 0.6] Measures the
similarity between
𝑎 #: Season with salt, pepper, and cream, then heat through 𝑠 #: T soh ue pre t' hs aa t 'p so rt e c ao dn yt ta oin bin eg s ecr re veam dy, seasoned pumpkin [0.6, 0.6, 1.0] ] step 𝑖and step 𝑗
From action to state and state similarity with LLM
Instruction: <Task introduction > + <In-context examples>
Step 1: Peel and chop the pumpkin into cubes on a cutting board
There is a cutting board with peeled and cubed pumpkin
Step 2: Place pumpkin cubes in a and boil
A pot containing softly boiled pumpkin cubes in broth sits on the stove
Place pumpkin cubes in a and boil.
StepT 3h:e Sreea isso an c wutitthin sga blto, apredp wpeitrh, apnede lcerde am, then heat through
and cubed pumpkin
There's a pot containing creamy, seasoned pumpkin soup that's ready to be served
What is the visual similarity between steps? [[1.0, 0.6, 0.6],
[0.6, 0.6, 1.0],
* The similarity explained in task introduction and in-context example [0.6, 0.6, 1.0]]
Stage 2: Dynamic consistent image generation
Shared attention layer
# # #
Current Action + previous state Unet ! " # $ !
!
! * + % *
Peel and chop the pumpkin into …
$ !: cubes on a cutting board " ! $
"
State
$ " +! !: P Thla ec re e p isu m a cp uk ti tn i nc gu b be os a rin d a w a itn hd boil. … " " similarity 11. 0.6 0.6 log $ #
peeled and cubed pumpkin +
Season with salt, pepper, and
$ # +! ": c cr oe na tm ain, t inh ge n so h fe tla yt b t oh ir lo edu g ph u. mA pp ko int … " # M (opa ts ik o nal) 11. log
cubes in broth sits on the stove
Grounding DINO mask
Figure3. Ourframeworkforzero-shotinstructionvisualization. Ourframeworkoperatesintwodistinctphases. Inthefirstphase,
weuseanLLM(e.g.,theGPT-4model)togeneratethescenestateaftereachstepinthelistofinstructions. Thegeneratedscenestate
helpsguidetheimagegenerationinthenextstage. WealsoasktheLLMtogeneratethesimilaritybetweenstates. Thismatrix,witheach
row,indicatingthevisualsimilarityofacurrentvisualsteptoothers,guidesthegenerationprocess. Forexample,toachievehighstate
similarity,wewishtomaintainconsistencyasmuchaspossibleacrossthetwosteps.Alowstatesimilarityindicatestheperformedaction
changesthescenestatesubstantially. Insuchcases,blindlyencouragingconsistencyacrossstepsmayhurtthequalityofthevisualized
instructionimage.Inthesecondphase,weutilizeasharedattentionlayer—replacingthestandardmodel—toallowqueriesfromoneimage
toaccesskeysandvaluesfromotherswithinthesameinstructionset. Weenhancethissharingmechanismbyapplyingstandardattention
masking,controlledbythesimilaritymatrix,totunetheinteractionbetweenvisualelementsfinely.
structional steps, which wecombine withthe next instruc- vealedandshouldbemaintainedintheprevioussteps. Asa
tiona astheinputpromptstothetext-to-imagemodel: result, the text-to-image model can achieve better continu-
i+1
ityandcontextaccuracyinthegeneratedvisualinstruction
(cid:40)
p = a i+s i−1 ifi>0 (6) sequence.
i
a otherwise
i
Wefindthatthisre-captioningmethodmakesindividual
text prompts contain the necessary information that is re-
5Text: A woman holding an umbrella in the rain structionalsteps. Byadjustingitsvalues,onecantailorthe
Generated image cross-attention map visual output to either highlight consistency or emphasize
transformation,dependingontheinstructions. Toautomat-
ically infer this matrix from the instructions, we provide
thoughtful instruction and in-context examples to LLMs.
Let W ∈ [0,1]N×N be the state similarity matrix, where
N isthenumberofsteps. W representstheoverallsimi-
i,j
laritybetweenthei-thandthej-thsteps.
WeinflateW intoamatrixS ,whichisusedforcom-
i,j j
woman umbrella puting the attention matrix of the i-th image. We use the
matricesderivedfromthesimilaritymatrixtoregularizethe
Figure4. Cross-attentionmapofStableCascade. Wevisualize attentionmapsinEquation5isasfollows:
thecorss-attentionmapsinstageCofStableCascademodel. Itis
foundthattheattentionmapsarenoisyandfailtoaccuratelydelin- S+ =[S ...,S ⊕1⊕S ,...,S ] (7)
eatethespecificregionsofthemainobjects:womanandumbrella. i 1 i−1 i+1 N
(cid:18) Q K+T (cid:19)
3.3.Dynamicconsistentimagegeneration A+ =softmax √i +log(S+)+log(M+) ∈RP×N·P
i d i i
k
Our re-captioning technique ensures the description conti- (8)
nuityintextprompts,whilewestillneedtogenerateconsis- Here, A+ i represents the adjusted attention weights,
tentcontentsharedacrossinstructions. Differentfrompre- wheretheflowofinformationinself-attentionisscaledin-
viousmethodslikevideoorstorygeneration,whichrequire versely by the values in S i+. When S j approaches zeros
absoluteconsistencyontheentiresceneortheobjectregion, (indicatingnosimilaritybetweenstepsi-thandj-thstates),
instructional generation requires a more nuanced consis- the sharing of information between those specific steps is
tency.Aswediscussedintheprevioussections,somevisual nullified. The magnitude of information sharing between
instruction steps may demand maintaining object consis- thei-thandj-thstepsissubstantialwhenthevaluesofS j
tency,whileothersmayinvolvesignificanttransformations are high, which encourages more consistent regions to be
oftheobjectsorbackgroundaccordingtotheprocedure.To generated. This method allows us to precisely control the
thisend,weproposeanadaptiveKVsharingmethodbased trade-offbetweenconsistencyandvariationacrossdifferent
onlocalregionandstatesimilarityconstraints. steps in the visual instruction generation process, ensuring
thateachgeneratedstepisappropriatelyalignedwithboth
KVsharingwithlocalregionconstraint. SimilartoCon- theinstructionandtheotherimagesinthesequence.
sistory [68], visual instruction generation only cares about
consistency within certain regions. We identify these con- 4.Experiments
sistentobjectsintheinstructionsduringourin-contentplan-
4.1.Experimentsetup
ningphase. Withourre-captionedinstructions,theimages
generated realize most information about the instructions, Inthissection,weevaluateourmethodforgeneratingvisual
like the layout, except for the object consistency. There- illustrations of textual instructions. We propose a frame-
fore, we propose to use off-the-shelf segmentation models work to leverage the vision language models (VLMs) for
togeneratetheobjectmasks[38,53]. Wealsoexploredus- theevaluation. Wealsoperformablationstudiesonourin-
ingcross-attentionmapsforproducingthemaskasinCon- dividualdesigns.
sistory. However, we find that the link between text and
Evaluation Following the previous studies, we utilize
imagecanbeweakandnoisyinthestate-of-the-arttext-to-
CLIP-Score [51]tomeasurethetext-imagealignment,and
imagemodels[47]asshowninFigure4. Instead,weshow
Dreamsim [16] and L2 Dinov2 [8] to evaluate the con-
thatexistingsegmentationmodelsworkwellwithgenerated
sistency. However, the tasks exhibited in the instructions
imagesandarenotoverfittedtoasinglemodelarchitecture.
span a wide range of categories. Each of these may have
KVsharingwithstatesimilarityconstraint. Tomanage adistinctgoalthatgoesbeyondsimpleobjectconsistency.
thedynamicscenarioofgeneratingconsistentobjectswith Understandingthequalityofthevisualillustrationdemands
variations, we generalize the KV sharing in a continuous nuancedreasoning—forinstance,whilesomescenariosne-
manner. InsteadofonlysharingornotsharingKVfeatures cessitatemaintainingobjectconsistency,othersrequirede-
among different regions, we regulate the scaling of shar- liberatechangesintheobject’sstate,suchascuttingordec-
ing with a state similarity matrix. This matrix is designed orating. Therefore, we find that conventional metrics are
tocontrolthedegreeofsimilaritybetweeneachpairofin- ofteninsufficienttoevaluatesuchacomplextask. Instead,
6we turn to recent VLMs like GPT-4V and Gemini-Pro 1.5 Gemni 1.5 Pro GPT-4V
which show great visual understanding and reasoning ca- 51.3% 51.9%
Ours
pacity. Wemainlyevaluateinthefollowingaspects: Textual 53.7% 56.2% Action
Action-concat
1. Textual Alignment measures how well the visual con- 0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
tentmatchesthetextualinstructions. 53.0% 51.0%
2. Continuity evaluates the transition process in a se- Continuity 58.2% 58.8%
quenceofimagesorwithinelementsofasingleimage. 0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
3. Consistency assesses whether the objects in the image 48.6% 52.3%
Consistency 68.4% 64.2%
remain the same throughout a sequence or within the
0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
contextoftheimage.
4. Relevancedetermineshowfocusedtheimageisonthe 52.1% 53.8%
Relevance 50.9% 54.4%
mainobjectorthemeasdescribedintheinput.
0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
To apply these metrics, we use carefully designed
instructions and in-context examples to query vision- Figure5. Evaluationofdifferentdesignchoicesoftextprompts
languagemodels. Ateachtime,weprovideapairofgener- usingLLM,includingGeminiandGPT-4. Amongdifferenteval-
atedimages,onebyourmethodandtheotherbythebase- uation aspects, including text alignment, continuity, consistency,
linemethod.TheVLMisrequestedtopicktheimagebetter andrelevance,ourchoiceofconcatenatingactionandstatebeats
attheaboveaspects. usingactiononlyorconcatenatingwithpreviousactions.
Dataset To facilitate the study of visualizing textual in-
Gemni 1.5 Pro GPT-4V
structions,weuseGPT-4togenerate200goalsandinstruc-
45.8% 52.3%
tionsforwide-rangingtasks,includingcooking,gardening, 58.8% 53.5% Ours
Textual 57.1% 52.4% KV Local
anddecorating. Eachinstructioncontains3to5steps. 53.6% 56.8% KV
0% 25% 50% 75% 100% 0% 25% 50% 75% 100% KV Global
Independent
Implementation details. Our method includes two main 49.0% 50.3%
54.3% 51.0%
components. We use GPT-4 API for in-context planning. Continuity 60.1% 57.0%
49.5% 56.2%
Weprovideallthein-contextexamplesandpromptsinthe 0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
appendix. However,wenotethatuserscanachievesimilar 58.4% 52.5%
42.1% 48.5%
qualityusingChatGPT3or4. Fordynamic,consistentim- Consistency 45.2% 56.1%
35.8% 51.4%
agegeneration,weuseStableCascade [47]asourtext-to- 0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
imagemodel,whichhasthreestages:A,B,andC.Basedon 56.9% 50.5%
61.3% 51.7%
thereportandexploration,wefindthatonlystageCforms Relevance 59.6% 53.5%
50.0% 52.7%
theimagebasedonthetextcondition(stageBusestextas 0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
a condition, but in this stage, the text condition barely af-
Figure6.EvaluationofdifferentconsistencymethodsusingLLM,
fects the generation results). Thus, we apply our adaptive
including Gemini and GPT-4. Using both masks and weights
KV sharing methodin stage C. Specifically, we apply it in achievesoverallthebestperformancesamongallthechoices.
thefirst15stepsofatotalof20.
Table 1. Ablation study of different design choices of the text
4.2.Quantitativeresults
prompts.Wefindthatusinginstructionsonlyprovidesthebesttext
imagealignmentwhileconcatenatingwithpreviousinstructionsor
We evaluate our method and baseline approaches using
statesimprovesthecoherency.
vision-language models and show results in Figure 5 and
Figure 6. We quantitatively compare using different con-
Inputtexts Clip-score↑ Dreamsim↓ L2-Dinov2↓
ditioningtextsasinputinFigure5, demonstratingthatour
re-captioning approach achieves overall better results than Instructions 0.6980 0.0.4829 47.7013
baselinemethodsthatrelysolelyoninstructionsforimage Concatenation 0.4559 0.3433 38.8717
generation. Our method shows consistently improved be- Re-captioning 0.5138 0.3797 41.6487
havioracrossallfouraspects. Wethenquantitativelycom-
paredifferentfeaturesharingmethodsinFigure 6. Wefind
lessidenticalresultsasdesiredwhilegreatlyimprovingthe
that our adaptive KV sharing methods improve almost all
textalignment.
metricsusingGeminiandGPT-4V.
We also show the quantitative results with traditional
4.3.Qualitativeresults
metrics in Table 1 and 2. which demonstrates that our
method is comparable with baselines. Specifically, as Figure 9 illustrates the impact of various design choices
shown in Table 2, applying adaptive KV sharing leads to for prompts on the coherence and alignment of the gener-
71.Put a raw chicken in 2. Seasoning chicken 3. Fry it until it turns 4. Wait a few minutes 1. Using hands to 2. Using a 3. Pouring the 4.Usingaspatula
pan golden brown. and enioyit cracktwoeggsand whisk to beat beaten eggs from to gently fold
pourtheircontents theegg the bowl into a half of the
intothebowl heated non-stick omelette over
fryingpan itself
Fry chicken
1. Use the scissors to 2. Use the scissors to cut 3. Glue the cut-out 4. Use markers to draw
cut pieces of cardboard pieces of cardboard in the pieces together to form windows, doors, and
in the shape of castle shape of castle turrets and the structure of the other details on the
turrets and walls walls castle castle
Make a castle model
1. Filling a pot with 2. Gently placing raw 3. Bringing the water 4. Removing the egg
enough water eggs into the water to a boil on the stove from the water, cooling
it under cold water, and
peeling off the shell
Boil eggs
Figure7.Failurescases.Duetothelimitedcapabilityofthetext-
to-imagemodel,itcannotgeneraterawchickenorraweggs.
Preparing a Basic Omelette
Table2. Quantitativeevaluationofourmethodstoachieveobject
coherency. Our method, which uses both local region mask and Figure8.Visualcomparisonsofconsistencyacrosssteps.Here,
globalsimilarityconstraints, greatlyimprovesthetextalignment weusetheproposedpromptingapproachandfocusonvalidating
foritsflexibilityinrealizingthevariationofobjectstatesacross thevariouswaysofsharingkeysandvaluesinattentionlayersfor
steps,whichisshownasanincreaseinsimilaritymeasurement. consistentimagegeneration.Allmethodstakethesameinputtext
prompt. KV:sharingacrossearlysteps; KVlocal: sharingcon-
GlobalLocal Clip-score↑ Dreamsim↓ L2-Dinov2↓ trolledbymasks;KVglobal: sharingcontrolledbytheproposed
state similarity matrix; Ours: sharing controlled by both masks
× × 0.4929 0.3638 40.5037
and the state similarity matrix. For example, in the omelette,
✓ × 0.5358 0.3792 41.5642 na¨livelysharingthekeyandvalueacrossstepscausecontentleak-
× ✓ 0.5138 0.3799 41.6487
ing (e.g., the pan in step 3 and 4 looks like the bowl in step 1
✓ ✓ 0.6708 0.4377 45.6434
and2).Withweightcontrol,themethodcanmaintainconsistency,
avoidingleakingfeaturewhilerespectingtheaction.Addingmask
controlhelpsimprovetextualalignment.(e.g,theactionsaremore
ated instructions. When only the action from a single step alignwithtext)
is used as input, the generated images lack coherence due
to unawareness of the context. For instance, in “cooking Concatenating action and state significantly enhances con-
Hot Chocolate”, the pot is only mentioned in step 1. As a textunderstanding,leadingtomoreaccurateimagegenera-
result, steps 2 and 3 do not have such information that the tionbasedonpriorsteps. Forinstance,thismethodensures
pot is in the stove and contains the hot milk. Therefore, the generated image accurately places the pot on the stove
themodelfailstomaintainthecontext;thepotisnotinthe in the case of a pot and cooking. In another example in-
stoveanddoesnotcontainthemilkanymoreinsteps2and volvingcookies,theapproachallowsforgeneratingimages
3. Inscenarioswherecurrentandpreviousactionsarecon- ofcookieswithchocolatechips, ensuringtheactionaligns
catenated, the model generally can understand the context closelywiththeinputtext.
better. However,suchmodelstendtoprioritizethemostre- In Figure 8, the stable cascade model, when generat-
centactionatthecostofaccuratelygeneratingearlierones. ingimagesindependently,failstoensureconsistencyacross
8
(cid:123)tnedn(cid:124)e(cid:125)pednI(cid:122)(cid:123)
V(cid:124)(cid:125)K
(cid:122)(cid:123)
laco(cid:124)l(cid:125)VK
(cid:122)(cid:123)
labo(cid:124)lg(cid:125)VK
(cid:122)(cid:123)
sr(cid:124)u(cid:125)O
(cid:122)1. Pouringmilkintoa 2. Addingchocolate 3.apersonusesa 4.Pouringthehot 1. Combining flour, 2.Stirringchocolate 3. Usingaspoonto 4.Placingthebaking
steelpotandheating powderorchopped spoon to stir the chocolatefrompot sugar, eggs, and chips into the scoop dough onto sheetintheovento
itonthestoveuntilit chocolatetothehot chocolate intoamug butter in a mixing cookiedough thebakingsheet bake until it turns
simmers milk bowl goldenbrown.,
State: A steel pot State: The steel State: A spoon is State:Amugfilled State: There is a State: The mixing State: A baking State: The scene
filled with pot now contains visible in the pot, withhotchocolate combined mixture bowl contains sheet is studded shows a baking
simmeringmilksits hot milk mixed whichisfilledwitha standsnexttothe of flour, sugar, cookie dough withspaceddollops sheet chocolate
onastove with dissolved well-mixed chocolate pug eggs,andbutterin studded with of chocolate chip chipcookies.
chocolate milkmixture amixingbowl chocolatechips cookiedough
Cooking hot chocolate Baking chocolate chip cookie
Figure9. Visualcomparisonswithconcatenatingsteps. Ours: concatenatingcurrentinstructionandinferredstates; Concatenation:
concatenatingcurrentandpreviousinstructions;Instruction:usingonlycurrentinstruction
1. Aman in a blue 2. A man holding up 1. A person is 2. Sliced apples on a 1. Aperson cleaning 2. Aclose up view of
shirt pouring beer a glass of beer cutting an apple in a white plate on a a toilet with a brush a white toilet bowl
into a glass bowl kitchen counter
['Pouring milk into a steel pot and heating it on the stove until it simmers', 'Adding chocolate
powder or chopped chocolate tPo othue rh obt meielk'r, 'a person uses a spoon to stir the Slice apple"saction": "Combining flour, sugar, eggs, and butter inC a lmeixainng btoowill."e,t
chocolate.','Pouringthe hot chocolate from pot into a mug.'] "state_of_main_object": " There is a combined mixture of flour, sugar, eggs, and butter in a mixing bowl."
F['iAg stuereel po1t 0fil.ledV wiisthu saimlmceorinmg mpilak rsitiss oonn a sstowvei',t 'hTheG steeenl phoot nwowto con[ta6in5s] h.otC moilkm parisonofour"macteiotnh"o: "dStirwrinigt hchotchoelatGe cehinpsH inotow thTe ocoomkiee dtohuoghd.",oninstructionsfromtheir
mixed with dissolved chocolate','Aspoon is visible in the pot, which is filled with a well-mixed "state_of_main_object": " The mixing bowl contains cookie dough studded with chocolate chips."
pcahopcoelra.te mTihlk emixstmurea',l'Almimug afilgleed switihn hotth cehotcoolapte lsetafntdso nfextth toe thGe peont.'HowToimagesarether},ealimagesusedasinputsfortheirmethod. Ourfree-training
methodiscompatiblewiththeirpretrainedmodelandproducesvisuallymo"racetiopnl"e: "aUssiinng ga srpeoosnu tlot sscowopi tdhouoguh tonrteo qthue ibraikningg saheerte."a,limageasinput.
"state_of_main_object": " A baking sheet is studded with evenly spaced dollops of chocolate chip cookie
dough.
"action": "Placing the baking sheet in the oven to bake until it turns golden brown.",
different steps. Using basic KV sharing makes images in tion, as seen in the poor depiction of pouring eggs. Ours
"state_of_main_object": " The scene shows a baking sheet with golden-brown chocolate chip cookies fresh
differentstepsgloballysimilar.However,errorsoccur,such comfrobm itnhee osvenb."othmethodsandeffectivelysolvestheissuesof
Baking Chocolate Chip Cookies
asapanbeingmisinterpretedasabowlandtheactionsnot missingactionsandobjectmisrepresentationacrossdiffer-
aligning with the input text well. Applying a local region entsteps.
mask addresses some issues of action omission, though it
In Figure 10, we compare our method with Gen-
stillmisrepresentsabowlasapan. RegularizingKVshar-
HowTo[65],apretrainedmodelfo1r. gAepneersroant icnugttiancg taino n2s. Salincedd avocados on a
ingwithstatesimilarityresolvestheproblemofmissingat-
states from real images. We use apvroocmadpo twsitfhr ao mkniftehebGlacekn p-late next to a knife
tributes between steps, yet it inadequately captures the ac-
HowTo test dataset, which belongs to the same categories
9
Input
noitcurtsnI
noitanetacnoC
gninoitpac-eR
(cid:123)
otwo(cid:124)h(cid:125)neG
(cid:122)(cid:123)
sr(cid:124)u(cid:125)O
(cid:122)astheirtrainingdata. Notethatourmethodistraining-free [4] Joa˜oBordalo,VascoRamos,RodrigoVale´rio,DiogoGlo´ria-
anddoesnotrequirearealinputimageasinput. Ourspro- Silva, Yonatan Bitton, Michal Yarom, Idan Szpektor, and
ducesresultsthatarecomparableintermsoftextualalign- Joao Magalhaes. Generating coherent sequences of visual
ment, consistency, and even better visual quality. This is illustrations for real-world manual tasks. arXiv preprint
arXiv:2405.10122,2024.
because our approach leverages a state-of-the-art text-to-
[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
imagemodel. UnlikeGenHowTo [65], whichrequiresre-
Chebotar,JosephDabis,ChelseaFinn,KeerthanaGopalakr-
training the model for each new base text-to-image model
ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.
andmaycompromisethegenerationqualityduetothetrain-
Rt-1: Robotics transformer for real-world control at scale.
ingdataquality.
arXivpreprintarXiv:2212.06817,2022.
[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
5.Failurecasesanddiscussion Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:
Image-text pair dataset. https://github.com/
We shown several failure cases within our model in Fig- kakaobrain/coyo-dataset,2022.
ure 7, where it did not accurately generate the stated ob- [7] MingdengCao,XintaoWang,ZhongangQi,YingShan,Xi-
jects and attributes from the instructions, thereby mislead- aohuQie,andYinqiangZheng. Masactrl: Tuning-freemu-
ing the process. For example, in a step described as fry- tualself-attentioncontrolforconsistentimagesynthesisand
ing raw chicken, the model erroneously generated an im- editing. InIEEEInternationalConferenceonComputerVi-
age of cooked chicken. Similarly, in another instance in- sion(ICCV),2023.
[8] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
volving boiling a raw egg, the output also deviated from
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
thespecifiedrawstate. Additionally,themodelexhibiteda
ingpropertiesinself-supervisedvisiontransformers.InPro-
biastowardsrenderingcastlesinapaintingstyle,whichled
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
to inconsistencies in the style of generation across differ-
putervision,pages9650–9660,2021.
enttasks. Webelievethatastext-to-imagemodelsimprove
[9] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
inthefuture,ourmethodwillgreatlybenefitfromreduced
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
limitationsinherentincurrenttext-to-imagemodels. mantic guidance for text-to-image diffusion models. ACM
TransactionsonGraphics(TOG),42(4):1–10,2023.
6.Conclusion [10] Weifeng Chen, JiachengZhang, Jie Wu, HefengWu, Xue-
feng Xiao, and Liang Lin. Id-aligner: Enhancing identity-
In this paper, we tackle the problem of generating static preserving text-to-image generation with reward feedback
visual illustrations from textual instructions by leveraging learning,2024.
pretrained diffusion models, enabling high-quality genera- [11] PrateekChhikara,DhirajChaurasia,YifanJiang,OmkarMa-
tion without expensive fine-tuning. We propose a frame- sur,andFilipIlievski.Fire:Foodimagetorecipegeneration.
2024.
worktoaddresstheuniquechallengesofmaintainingobject
[12] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang
consistency across instructional steps while managing the
Wang,RuiWang,PeizhaoZhang,SimonVandenhende,Xi-
variability of objects that change across states. Our exten-
aofang Wang, Abhimanyu Dubey, et al. Emu: Enhanc-
siveevaluationsdemonstratedthatourmethodoutperforms
ingimagegenerationmodelsusingphotogenicneedlesina
baselinemodelsinbothconsistencyandaccuracy.
haystack. arXivpreprintarXiv:2309.15807,2023.
[13] YilunDu,SherryYang,BoDai,HanjunDai,OfirNachum,
References Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel.
Learning universal policies via text-guided video genera-
[1] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,Ji- tion. ConferenceonNeuralInformationProcessingSystems
amingSong,QinshengZhang,KarstenKreis,MiikaAittala, (NeurIPS),2024.
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. eDiff-I:
[14] FrederikEbert,YanlaiYang,KarlSchmeckpeper,Bernadette
Text-to-imagediffusionmodelswithanensembleofexpert
Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea
denoisers. arXivpreprintarXiv:2211.01324,2022.
Finn, and Sergey Levine. Bridge data: Boosting general-
[2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her- izationofroboticskillswithcross-domaindatasets.InInter-
rmann,RoniPaiss,ShiranZada,ArielEphrat,JunhwaHur, nationalConferenceonMachineLearning(ICML),2021.
Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space- [15] WeixiFeng,WanrongZhu,Tsu-juiFu,VarunJampani,Ar-
time diffusion model for video generation. arXiv preprint jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and
arXiv:2401.12945,2024. WilliamYangWang.Layoutgpt:Compositionalvisualplan-
[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng ningandgenerationwithlargelanguagemodels. InConfer-
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce enceonNeuralInformationProcessingSystems(NeurIPS),
Lee, Yufei Guo, et al. Improving image generation with 2024.
better captions. Computer Science. https://cdn. openai. [16] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
com/papers/dall-e-3.pdf,2(3):8,2023. Chai,RichardZhang,TaliDekel,andPhillipIsola. Dream-
10sim: Learning new dimensions of human visual similar- ProceedingsoftheIEEE/CVFInternationalConferenceon
ity using synthetic data. arXiv preprint arXiv:2306.09344, ComputerVision,pages15954–15964,2023.
2023. [31] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
[17] SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,Andrew vosyan, Roberto Henschel, Zhangyang Wang, Shant
Tao,BryanCatanzaro,DavidJacobs,Jia-BinHuang,Ming- Navasardyan, and Humphrey Shi. Text2video-zero: Text-
YuLiu,andYogeshBalaji.Preserveyourowncorrelation:A to-imagediffusionmodelsarezero-shotvideogenerators.In
noisepriorforvideodiffusionmodels.InIEEEInternational IEEEConferenceonComputerVisionandPatternRecogni-
ConferenceonComputerVision(ICCV),2023. tion(CVPR),2023.
[18] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin [32] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
Huang. Expressive text-to-image generation with rich ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
text. InIEEEInternationalConferenceonComputerVision head, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and
(ICCV),2023. RossGirshick. Segmentanything. arXiv:2304.02643,2023.
[19] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. [33] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Tokenflow:Consistentdiffusionfeaturesforconsistentvideo Shechtman,andJun-YanZhu. Multi-conceptcustomization
editing. InInternationalConferenceonLearningRepresen- oftext-to-imagediffusion.InIEEEConferenceonComputer
tations(ICLR),2024. VisionandPatternRecognition(CVPR),2023.
[20] RohitGirdhar,MannatSingh,AndrewBrown,QuentinDu- [34] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis
val,SamanehAzadi,SaiSakethRambhatla,AkbarShah,Xi Brown,andDeepakPathak.Yourdiffusionmodelissecretly
Yin, Devi Parikh, and Ishan Misra. Emu video: Factoriz- azero-shotclassifier. InIEEEInternationalConferenceon
ingtext-to-videogenerationbyexplicitimageconditioning. ComputerVision(ICCV),2023.
arXivpreprintarXiv:2311.10709,2023. [35] FengLi,HaoZhang,PeizeSun,XueyanZou,ShilongLiu,
[21] ZinanGuo,YanzeWu,ZhuoweiChen,LangChen,andQian JianweiYang, ChunyuanLi, LeiZhang, andJianfengGao.
He. Pulid: Pure and lightning id customization via con- Semantic-sam:Segmentandrecognizeanythingatanygran-
trastivealignment. arXivpreprintarXiv:2404.16022,2024. ularity. arXivpreprintarXiv:2307.04767,2023.
[22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, [36] LongLian,BoyiLi,AdamYala,andTrevorDarrell. LLM-
YaelPritch,andDanielCohen-or. Prompt-to-promptimage grounded diffusion: Enhancing prompt understanding of
editingwithcross-attentioncontrol.InInternationalConfer- text-to-imagediffusionmodelswithlargelanguagemodels.
enceonLearningRepresentations(ICLR),2023. Transactions on Machine Learning Research, 2024. Fea-
[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion turedCertification.
guidance. arXivpreprintarXiv:2207.12598,2022. [37] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
[24] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
sionprobabilisticmodels. 2020. Zero-shot oneimage to3d object. In IEEEConference on
[25] JonathanHo, WilliamChan, ChitwanSaharia, JayWhang, ComputerVisionandPatternRecognition(CVPR),2023.
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben [38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
video:Highdefinitionvideogenerationwithdiffusionmod- Zhu, etal. Groundingdino: Marryingdinowithgrounded
els. arXivpreprintarXiv:2210.02303,2022. pre-training for open-set object detection. arXiv preprint
[26] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet, arXiv:2303.05499,2023.
MohammadNorouzi,andTimSalimans.Cascadeddiffusion [39] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie
modelsforhighfidelityimagegeneration. JMLR,23:47–1, Liu,TakuKomura,andWenpingWang. Syncdreamer:Gen-
2022. eratingmultiview-consistentimagesfromasingle-viewim-
[27] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William age. In International Conference on Learning Representa-
Chan, Mohammad Norouzi, and David J Fleet. Video dif- tions(ICLR),2024.
fusion models. In Conference on Neural Information Pro- [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
cessingSystems(NeurIPS),2022. junWu,Jun-YanZhu,andStefanoErmon. SDEdit: Guided
[28] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, imagesynthesisandeditingwithstochasticdifferentialequa-
and Jie Tang. Cogvideo: Large-scale pretraining for tions. InInternationalConferenceonLearningRepresenta-
text-to-video generation via transformers. arXiv preprint tions(ICLR),2022.
arXiv:2205.15868,2022. [41] SachitMenon,IshanMisra,andRohitGirdhar. Generating
[29] Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun illustratedinstructions. arXivpreprintarXiv:2312,2023.
Zhou,YuhaoCheng,ShutaoLiao,LongChen,YiqiangYan, [42] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
ShengcaiLiao,etal. Consistentid: Portraitgenerationwith Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
multimodalfine-grainedidentitypreserving. arXivpreprint Howto100m: Learningatext-videoembeddingbywatching
arXiv:2404.16771,2024. hundredmillionnarratedvideoclips. InIEEEInternational
[30] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- ConferenceonComputerVision(ICCV),2019.
vosyan, Roberto Henschel, Zhangyang Wang, Shant [43] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
Navasardyan, and Humphrey Shi. Text2video-zero: Text- denoising diffusion probabilistic models. In International
to-imagediffusionmodelsarezero-shotvideogenerators.In ConferenceonMachineLearning(ICML),2021.
11[44] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
Li,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image etal.Photorealistictext-to-imagediffusionmodelswithdeep
translation. 2023. languageunderstanding. InConferenceonNeuralInforma-
[45] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch- tionProcessingSystems(NeurIPS),2022.
Elor, and Daniel Cohen-Or. Localizing object-level shape [57] AmaiaSalvador,NicholasHynes,YusufAytar,JavierMarin,
variationswithtext-to-imagediffusionmodels. InIEEEIn- FerdaOfli,IngmarWeber,andAntonioTorralba. Learning
ternationalConferenceonComputerVision(ICCV),2023. cross-modal embeddings for cooking recipes and food im-
[46] Pablo Pernias, Dominic Rampas, Mats Leon Richter, ages. InIEEEConferenceonComputerVisionandPattern
Christopher Pal, and Marc Aubreville. Wu¨rstchen: An ef- Recognition(CVPR),2017.
ficient architecture for large-scale text-to-image diffusion [58] AmaiaSalvador,MichalDrozdzal,XavierGiro´-iNieto,and
models. InTheTwelfthInternationalConferenceonLearn- AdrianaRomero. Inversecooking: Recipegenerationfrom
ingRepresentations,2023. foodimages. InIEEEConferenceonComputerVisionand
PatternRecognition(CVPR),2019.
[47] Pablo Pernias, Dominic Rampas, Mats Leon Richter,
Christopher Pal, and Marc Aubreville. Wu¨rstchen: An ef- [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
ficient architecture for large-scale text-to-image diffusion Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
models. InTheTwelfthInternationalConferenceonLearn- Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
ingRepresentations,2023. man,PatrickSchramowski,SrivatsaRKundurthy,Katherine
Crowson,LudwigSchmidt,RobertKaczmarczyk,andJenia
[48] Pablo Pernias, Dominic Rampas, Mats Leon Richter,
Jitsev. LAION-5b: An open large-scale dataset for train-
Christopher Pal, and Marc Aubreville. Wu¨rstchen: An ef-
ingnextgenerationimage-textmodels. InThirty-sixthCon-
ficient architecture for large-scale text-to-image diffusion
ferenceonNeuralInformationProcessingSystemsDatasets
models. InInternationalConferenceonLearningRepresen-
andBenchmarksTrack,2022.
tations(ICLR),2024.
[60] YichunShi,PengWang,JianglongYe,LongMai,KejieLi,
[49] QuynhPhung,SongweiGe,andJia-BinHuang. Grounded
andXiaoYang.MVDream:Multi-viewdiffusionfor3dgen-
text-to-image synthesis with attention refocusing. In IEEE
eration. InInternationalConferenceonLearningRepresen-
Conference on Computer Vision and Pattern Recognition
tations(ICLR),2024.
(CVPR),2024.
[61] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,
[50] Dustin Podell, Zion English, Kyle Lacey, Andreas
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
OranGafni,DeviParikh,SonalGupta,andYanivTaigman.
Robin Rombach. Sdxl: Improving latent diffusion models
Make-a-video: Text-to-video generation without text-video
forhigh-resolutionimagesynthesis,2023.
data. InInternationalConferenceonLearningRepresenta-
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
tions(ICLR),2023.
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[62] JiamingSong,ChenlinMeng,andStefanoErmon. Denois-
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
ing diffusion implicit models. In International Conference
Krueger, and Ilya Sutskever. Learning transferable visual
onLearningRepresentations(ICLR),2021.
modelsfromnaturallanguagesupervision. InInternational
[63] YangSongandStefanoErmon. Generativemodelingbyes-
ConferenceonMachineLearning(ICML),2021.
timatinggradientsofthedatadistribution. 2019.
[52] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
[64] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
and Mark Chen. Hierarchical text-conditional image gen-
hishekKumar,StefanoErmon,andBenPoole. Score-based
erationwithcliplatents. arXivpreprintarXiv:2204.06125,
generative modeling through stochastic differential equa-
2022.
tions. In International Conference on Learning Represen-
[53] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kun- tations(ICLR),2021.
changLi,HeCao,JiayuChen,XinyuHuang,YukangChen, [65] Toma´sˇ Soucˇek, Dima Damen, Michael Wray, Ivan Laptev,
FengYan, ZhaoyangZeng, HaoZhang, FengLi, JieYang, and Josef Sivic. Genhowto: Learning to generate actions
HongyangLi, QingJiang, andLeiZhang. Groundedsam: and state transformations from instructional videos. arXiv
Assembling open-world models for diverse visual tasks, preprintarXiv:2312.07322,2023.
2024. [66] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng
[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Phoo, and Bharath Hariharan. Emergent correspondence
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn- fromimagediffusion. InConferenceonNeuralInformation
thesiswithlatentdiffusionmodels. InIEEEConferenceon ProcessingSystems(NeurIPS),2024.
ComputerVisionandPatternRecognition(CVPR),2022. [67] OmostTeam. Omostgithubpage,2024.
[55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, [68] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten,
MichaelRubinstein,andKfirAberman. Dreambooth: Fine Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-
tuning text-to-image diffusion models for subject-driven free consistent text-to-image generation. arXiv preprint
generation. In IEEE Conference on Computer Vision and arXiv:2402.03286,2024.
PatternRecognition(CVPR),2023. [69] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior
[56] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Wolf, GalChechik, andYuvalAtzmon. Training-freecon-
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, sistenttext-to-imagegeneration. 2024.
12[70] Hung-YuTseng,QinboLi,ChangilKim,SuhibAlsisan,Jia- examples help the model understand the context better
BinHuang, andJohannesKopf. Consistentviewsynthesis andproducethedesiredboundingboxesandcorrespond-
withpose-guideddiffusionmodels. InIEEEConferenceon inglabels.
ComputerVisionandPatternRecognition(CVPR),2023. • User prompt: This is appended to the instruction and
[71] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali supportingexamples. Themodelthencontinuesthecon-
Dekel. Plug-and-play diffusion features for text-driven
versationbasedontheuserpromptandprovidesthelay-
image-to-image translation. In IEEE Conference on Com-
outinthespecifiedformat.
puterVisionandPatternRecognition(CVPR),2023.
When users provide a prompt (user prompt), it is com-
[72] QingheWang,BaoluLi,XiaominLi,BingCao,LiqianMa,
binedwiththepredefinedtexttocreateacompleteprompt
HuchuanLu,andXuJia.Characterfactory:Samplingconsis-
tentcharacterswithgansfordiffusionmodels.arXivpreprint as shown in Table 3. The GPT-4 API then processes this
arXiv:2404.15677,2024. complete prompt and returns the information about each
[73] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian steps,similaritymatrixandthemainobjectsineachstep.
Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
Metrices using Multimodel Gemini and GPT-4V We
Qie,andMikeZhengShou. Tune-a-video: One-shottuning
evaluateourgenereratedvisualinstructionusingGemini15-
of image diffusion models for text-to-video generation. In
proandgpt-4Vtoevaluatefouraspects,weusetheinstruc-
IEEEInternationalConferenceonComputerVision(ICCV),
tion sjown in Figure 12 to guide the multi-model to as-
2023.
sessourvisualinstructions. Weshuffletheorderofthetwo
[74] Aman Yadav, Michael M Phillips, Mary A Lundeberg,
MatthewJKoehler,KatherineHilden,andKathrynHDirkin. methodswhicharecomparedtoavoidthemulti-modelbias
Ifapictureisworthathousandwordsisvideoworthamil- towardtheorder.
lion? differences in affective and cognitive processing of
videoandtextcases. JournalofComputinginHigherEd-
ucation,23:15–37,2011.
[75] SherryYang,YilunDu,SeyedKamyarSeyedGhasemipour,
Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuur-
mans, and Pieter Abbeel. Learning interactive real-world
simulators. In International Conference on Learning Rep-
resentations(ICLR),2024.
[76] Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang,
Mark Yatskar, and Chris Callison-Burch. Visual goal-step
inferenceusingwikihow. InConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessing,2021.
[77] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
IEEE Conference on Computer Vision and Pattern Recog-
nition(CVPR),2023.
[78] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
automatic learning of procedures from web instructional
videos. InAAAIConferenceonArtificialIntelligence,2018.
[79] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi
Feng, and Qibin Hou. Storydiffusion: Consistent self-
attentionforlong-rangeimageandvideogeneration. 2024.
A.Appendix
A.1. Additional discussion and details about large
languagemodels
Full prompt for GPT-4 Our complete prompt for GPT-4
includesthreemaincomponents:
• Instruction: This specifies the task and defines the out-
put format, helping GPT-4 perform effectively in layout
generationtasks.
• In-contextexemplars: Theseenhancethemodel’scapa-
bilityforthetaskbyprovidingmultipleexamples. These
13Table3.Thefullpromptforgpt4apitogenerateinstructions.
Role Content
Instruction System: ”YouareChatGPT-4,actlikevisualandinstructionalexperts,generatestep-by-stephowto
dosomething. eachstepincludetheactiontoindicatehowpeopleinteractwithobjecs,andstateto
showstateofobjectsafterfinishthisaction. Andrelationmatrixisthecorrelationofonestepwith
othersinvisual. objectfieldindicatetheobjectsineachstepsimilarwithpriviousstepinsome
extends: similar(totalsimilar),shapesimilar(onlysimilarshape),texturesimilar(transformshape,
onlysametexture)”
User: ”Theinstructionondecoratingacakein2steps.”
In-contextexamples
Assistant: [examplersinFigure 11]
Userprompt User: ”Theinstructionon”+[userprompt]
14{
"goal": "Decorating a Cake",
"steps": [
{
"step": "Setting the Cake
on a Platter",
"object": [["cake", "new"],
["platter", "new"]],
"action": "Set the baked
cake on a platter.",
"state_of_main_object": "A
baked cake on the
platter."
},
{
"step": "Applying Icing",
"object": [["cake", "
similar shape", 1], ["
spoon", "new"]],
"action": "Person using a
spoon to place some
icing on the top of the
cake.",
"state_of_main_object": "
The cake covered by
icing."
}
],
"relation": [
[1.0 , 0.5, 0.4, 0.3],
[0.9, 1.0 , 0.5, 0.4],
[0.8, 0.9, 1.0, 0.4],
[0.7, 0.8, 0.9, 1.0 ]
]
}
Figure11.In-contextexamplersforfullprompts
15Our task here is to compare visual step-by-step instructions, generated from the
same step-by-step textual instruction. We want to decide which one is better
according to the provided criteria.
# Instruction
1. Text prompt and Asset Alignment: Focus on whether the key elements mentioned in
the text are clearly visible and identifiable in the image. The visual is good
if all key elements are clearly depicted and easily identifiable.
2. Continuity: This measures how well the image captures the progression from the
previous step(s), maintaining context and demonstrating the changes or actions
described in the current step. The visual is good if the image effectively
shows the progression from previous steps and integrates new elements/actions
as described in the current step.
3. Consistency: Evaluates whether the same objects are used consistently across
all images in a way that reflects their continued presence and role as
described in the text. This is particularly important for objects that are
central to the action or instructions. For example, a pot in first step should
look like the pot mentioned other step, even it can be in different views.
4. Relevance: Assesses whether the visual focuses on the most critical aspect of
the step as described in the text. The visual is good if the visual focuses
precisely on the primary action or element described in the step.
Take a really close look at each of the multi-image instructions for the
corresponding textual instruction before providing your answer.
When evaluating these aspects, focus on one of them at a time.
Try to make independent decisions between these criteria.
# Output format
To provide an answer, please provide a short analysis for each of the
abovementioned evaluation criteria. The analysis should be very concise and
accurate.
For each of the criteria, you need to make a decision using these options:
1. The first row visual is better;
2. The second row visual is better;
... or Cannot decide.
IMPORTANT: PLEASE USE THE ’Cannot decide’ OPTION SPARSELY.
Then, in the last row, summarize your final decision by <option for criterion 1> <
option for criterion 2> <option for criterion 3> <option for criterion 4>.
# Example
Analysis:
1. Text prompt and Asset Alignment: The first one ...; The second one ...; The
first/second/third/... one is better or cannot decide.
2. Continuity: The first one ...; The second one ...; The first/second/third/...
one is better or cannot decide.
3. Consistency: The first one ...; The second one ...; The first/second/third/...
one is better or cannot decide.
4. Relevance: The first one ...; The second one ...; The first/second/third/...
one is better or cannot decide.
Final answer:
x, x, x ,x (e.g., 1, Cannot decide, 3, 1/ 2, Cannot decide,5, 1 / 1, 3, 2,4)
Figure12.InstructionforGeminiandGPT-4Vtoassesthevisualinstruction
16