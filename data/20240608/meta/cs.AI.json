[
    {
        "title": "Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion",
        "authors": "Fangfu LiuHanyang WangShunyu YaoShengjun ZhangJie ZhouYueqi Duan",
        "links": "http://arxiv.org/abs/2406.04338v1",
        "entry_id": "http://arxiv.org/abs/2406.04338v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04338v1",
        "summary": "In recent years, there has been rapid development in 3D generation models,\nopening up new possibilities for applications such as simulating the dynamic\nmovements of 3D objects and customizing their behaviors. However, current 3D\ngenerative models tend to focus only on surface features such as color and\nshape, neglecting the inherent physical properties that govern the behavior of\nobjects in the real world. To accurately simulate physics-aligned dynamics, it\nis essential to predict the physical properties of materials and incorporate\nthem into the behavior prediction process. Nonetheless, predicting the diverse\nmaterials of real-world objects is still challenging due to the complex nature\nof their physical attributes. In this paper, we propose \\textbf{Physics3D}, a\nnovel method for learning various physical properties of 3D objects through a\nvideo diffusion model. Our approach involves designing a highly generalizable\nphysical simulation system based on a viscoelastic material model, which\nenables us to simulate a wide range of materials with high-fidelity\ncapabilities. Moreover, we distill the physical priors from a video diffusion\nmodel that contains more understanding of realistic object materials. Extensive\nexperiments demonstrate the effectiveness of our method with both elastic and\nplastic materials. Physics3D shows great potential for bridging the gap between\nthe physical world and virtual neural space, providing a better integration and\napplication of realistic physical principles in virtual environments. Project\npage: https://liuff19.github.io/Physics3D.",
        "updated": "2024-06-06 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04338v1"
    },
    {
        "title": "Coherent Zero-Shot Visual Instruction Generation",
        "authors": "Quynh PhungSongwei GeJia-Bin Huang",
        "links": "http://arxiv.org/abs/2406.04337v1",
        "entry_id": "http://arxiv.org/abs/2406.04337v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04337v1",
        "summary": "Despite the advances in text-to-image synthesis, particularly with diffusion\nmodels, generating visual instructions that require consistent representation\nand smooth state transitions of objects across sequential steps remains a\nformidable challenge. This paper introduces a simple, training-free framework\nto tackle the issues, capitalizing on the advancements in diffusion models and\nlarge language models (LLMs). Our approach systematically integrates text\ncomprehension and image generation to ensure visual instructions are visually\nappealing and maintain consistency and accuracy throughout the instruction\nsequence. We validate the effectiveness by testing multi-step instructions and\ncomparing the text alignment and consistency with several baselines. Our\nexperiments show that our approach can visualize coherent and visually pleasing\ninstructions",
        "updated": "2024-06-06 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04337v1"
    },
    {
        "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
        "authors": "Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal",
        "links": "http://arxiv.org/abs/2406.04331v1",
        "entry_id": "http://arxiv.org/abs/2406.04331v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04331v1",
        "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable output, via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Then,\ngiven any alignment task, we instruct a concept partitioner to efficiently\nannotate the concepts as benign or undesirable. Finally, at inference time, we\ndecompose the LLM activations along the concept dictionary via sparse coding,\nto accurately represent the activation as a linear combination of the benign\nand undesirable components. By removing the latter ones from the activation, we\nreorient the behavior of LLMs towards alignment goals. We conduct experiments\non tasks such as response detoxification, faithfulness enhancement, and\nsentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
        "updated": "2024-06-06 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04331v1"
    },
    {
        "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
        "authors": "Qianlan YangYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2406.04323v1",
        "entry_id": "http://arxiv.org/abs/2406.04323v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04323v1",
        "summary": "Training autonomous agents with sparse rewards is a long-standing problem in\nonline reinforcement learning (RL), due to low data efficiency. Prior work\novercomes this challenge by extracting useful knowledge from offline data,\noften accomplished through the learning of action distribution from offline\ndata and utilizing the learned distribution to facilitate online RL. However,\nsince the offline data are given and fixed, the extracted knowledge is\ninherently limited, making it difficult to generalize to new tasks. We propose\na novel approach that leverages offline data to learn a generative diffusion\nmodel, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates\nsynthetic trajectories, serving as a form of data augmentation and consequently\nenhancing the performance of online RL methods. The key strength of our\ndiffuser lies in its adaptability, allowing it to effectively handle varying\ntrajectory lengths and mitigate distribution shifts between online and offline\ndata. Because of its simplicity, ATraDiff seamlessly integrates with a wide\nspectrum of RL methods. Empirical evaluation shows that ATraDiff consistently\nachieves state-of-the-art performance across a variety of environments, with\nparticularly pronounced improvements in complicated settings. Our code and demo\nvideo are available at https://atradiff.github.io .",
        "updated": "2024-06-06 17:58:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04323v1"
    },
    {
        "title": "Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models",
        "authors": "Ali BehrouzMichele SantacatterinaRamin Zabih",
        "links": "http://arxiv.org/abs/2406.04320v1",
        "entry_id": "http://arxiv.org/abs/2406.04320v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04320v1",
        "summary": "Modeling multivariate time series is a well-established problem with a wide\nrange of applications from healthcare to financial markets. Traditional State\nSpace Models (SSMs) are classical approaches for univariate time series\nmodeling due to their simplicity and expressive power to represent linear\ndependencies. They, however, have fundamentally limited expressive power to\ncapture non-linear dependencies, are slow in practice, and fail to model the\ninter-variate information flow. Despite recent attempts to improve the\nexpressive power of SSMs by using deep structured SSMs, the existing methods\nare either limited to univariate time series, fail to model complex patterns\n(e.g., seasonal patterns), fail to dynamically model the dependencies of\nvariate and time dimensions, and/or are input-independent. We present Chimera\nthat uses two input-dependent 2-D SSM heads with different discretization\nprocesses to learn long-term progression and seasonal patterns. To improve the\nefficiency of complex 2D recurrence, we present a fast training using a new\n2-dimensional parallel selective scan. We further present and discuss\n2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Our\nexperimental evaluation shows the superior performance of Chimera on extensive\nand diverse benchmarks, including ECG and speech time series classification,\nlong-term and short-term time series forecasting, and time series anomaly\ndetection.",
        "updated": "2024-06-06 17:58:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04320v1"
    }
]