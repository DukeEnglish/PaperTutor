[
    {
        "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
        "authors": "Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu",
        "links": "http://arxiv.org/abs/2406.04344v1",
        "entry_id": "http://arxiv.org/abs/2406.04344v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04344v1",
        "summary": "Motivated by the large progress made by large language models (LLMs), we\nintroduce the framework of verbalized machine learning (VML). In contrast to\nconventional machine learning models that are typically optimized over a\ncontinuous parameter space, VML constrains the parameter space to be\nhuman-interpretable natural language. Such a constraint leads to a new\nperspective of function approximation, where an LLM with a text prompt can be\nviewed as a function parameterized by the text prompt. Guided by this\nperspective, we revisit classical machine learning problems, such as regression\nand classification, and find that these problems can be solved by an\nLLM-parameterized learner and optimizer. The major advantages of VML include\n(1) easy encoding of inductive bias: prior knowledge about the problem and\nhypothesis class can be encoded in natural language and fed into the\nLLM-parameterized learner; (2) automatic model class selection: the optimizer\ncan automatically select a concrete model class based on data and verbalized\nprior knowledge, and it can update the model class during training; and (3)\ninterpretable learner updates: the LLM-parameterized optimizer can provide\nexplanations for why each learner update is performed. We conduct several\nstudies to empirically evaluate the effectiveness of VML, and hope that VML can\nserve as a stepping stone to stronger interpretability and trustworthiness in\nML.",
        "updated": "2024-06-06 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04344v1"
    },
    {
        "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
        "authors": "Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal",
        "links": "http://arxiv.org/abs/2406.04331v1",
        "entry_id": "http://arxiv.org/abs/2406.04331v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04331v1",
        "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable output, via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Then,\ngiven any alignment task, we instruct a concept partitioner to efficiently\nannotate the concepts as benign or undesirable. Finally, at inference time, we\ndecompose the LLM activations along the concept dictionary via sparse coding,\nto accurately represent the activation as a linear combination of the benign\nand undesirable components. By removing the latter ones from the activation, we\nreorient the behavior of LLMs towards alignment goals. We conduct experiments\non tasks such as response detoxification, faithfulness enhancement, and\nsentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
        "updated": "2024-06-06 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04331v1"
    },
    {
        "title": "Improving Alignment and Robustness with Short Circuiting",
        "authors": "Andy ZouLong PhanJustin WangDerek DuenasMaxwell LinMaksym AndriushchenkoRowan WangZico KolterMatt FredriksonDan Hendrycks",
        "links": "http://arxiv.org/abs/2406.04313v1",
        "entry_id": "http://arxiv.org/abs/2406.04313v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04313v1",
        "summary": "AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that \"short-circuits\" models as they respond with harmful outputs.\nExisting techniques aimed at improving alignment, such as refusal training, are\noften bypassed. Techniques such as adversarial training try to plug these holes\nby countering specific attacks. As an alternative to refusal training and\nadversarial training, short-circuiting directly controls the representations\nthat are responsible for harmful outputs in the first place. Our technique can\nbe applied to both text-only and multimodal language models to prevent the\ngeneration of harmful outputs without sacrificing utility -- even in the\npresence of powerful unseen attacks. Notably, while adversarial robustness in\nstandalone image recognition remains an open challenge, short-circuiting allows\nthe larger multimodal system to reliably withstand image \"hijacks\" that aim to\nproduce harmful content. Finally, we extend our approach to AI agents,\ndemonstrating considerable reductions in the rate of harmful actions when they\nare under attack. Our approach represents a significant step forward in the\ndevelopment of reliable safeguards to harmful behavior and adversarial attacks.",
        "updated": "2024-06-06 17:57:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04313v1"
    },
    {
        "title": "Measuring and Addressing Indexical Bias in Information Retrieval",
        "authors": "Caleb ZiemsWilliam HeldJane Dwivedi-YuDiyi Yang",
        "links": "http://arxiv.org/abs/2406.04298v1",
        "entry_id": "http://arxiv.org/abs/2406.04298v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04298v1",
        "summary": "Information Retrieval (IR) systems are designed to deliver relevant content,\nbut traditional systems may not optimize rankings for fairness, neutrality, or\nthe balance of ideas. Consequently, IR can often introduce indexical biases, or\nbiases in the positional order of documents. Although indexical bias can\ndemonstrably affect people's opinion, voting patterns, and other behaviors,\nthese issues remain understudied as the field lacks reliable metrics and\nprocedures for automatically measuring indexical bias. Towards this end, we\nintroduce the PAIR framework, which supports automatic bias audits for ranked\ndocuments or entire IR systems. After introducing DUO, the first\ngeneral-purpose automatic bias metric, we run an extensive evaluation of 8 IR\nsystems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k\nqueries spanning 1.4k controversial issue topics. A human behavioral study\nvalidates our approach, showing that our bias metric can help predict when and\nhow indexical bias will shift a reader's opinion.",
        "updated": "2024-06-06 17:42:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04298v1"
    },
    {
        "title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval",
        "authors": "Junjie ZhouZheng LiuShitao XiaoBo ZhaoYongping Xiong",
        "links": "http://arxiv.org/abs/2406.04292v1",
        "entry_id": "http://arxiv.org/abs/2406.04292v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04292v1",
        "summary": "Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.",
        "updated": "2024-06-06 17:37:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04292v1"
    }
]