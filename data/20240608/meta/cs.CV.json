[
    {
        "title": "Stereo-Depth Fusion through Virtual Pattern Projection",
        "authors": "Luca BartolomeiMatteo PoggiFabio TosiAndrea ContiStefano Mattoccia",
        "links": "http://arxiv.org/abs/2406.04345v1",
        "entry_id": "http://arxiv.org/abs/2406.04345v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04345v1",
        "summary": "This paper presents a novel general-purpose stereo and depth data fusion\nparadigm that mimics the active stereo principle by replacing the unreliable\nphysical pattern projector with a depth sensor. It works by projecting virtual\npatterns consistent with the scene geometry onto the left and right images\nacquired by a conventional stereo camera, using the sparse hints obtained from\na depth sensor, to facilitate the visual correspondence. Purposely, any depth\nsensing device can be seamlessly plugged into our framework, enabling the\ndeployment of a virtual active stereo setup in any possible environment and\novercoming the severe limitations of physical pattern projection, such as the\nlimited working range and environmental conditions. Exhaustive experiments on\nindoor and outdoor datasets featuring both long and close range, including\nthose providing raw, unfiltered depth hints from off-the-shelf depth sensors,\nhighlight the effectiveness of our approach in notably boosting the robustness\nand accuracy of algorithms and deep stereo without any code modification and\neven without re-training. Additionally, we assess the performance of our\nstrategy on active stereo evaluation datasets with conventional pattern\nprojection. Indeed, in all these scenarios, our virtual pattern projection\nparadigm achieves state-of-the-art performance. The source code is available\nat: https://github.com/bartn8/vppstereo.",
        "updated": "2024-06-06 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04345v1"
    },
    {
        "title": "Learning 1D Causal Visual Representation with De-focus Attention Networks",
        "authors": "Chenxin TaoXizhou ZhuShiqian SuLewei LuChangyao TianXuan LuoGao HuangHongsheng LiYu QiaoJie ZhouJifeng Dai",
        "links": "http://arxiv.org/abs/2406.04342v1",
        "entry_id": "http://arxiv.org/abs/2406.04342v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04342v1",
        "summary": "Modality differences have led to the development of heterogeneous\narchitectures for vision and language models. While images typically require 2D\nnon-causal modeling, texts utilize 1D causal modeling. This distinction poses\nsignificant challenges in constructing unified multi-modal models. This paper\nexplores the feasibility of representing images using 1D causal modeling. We\nidentify an \"over-focus\" issue in existing 1D causal vision models, where\nattention overly concentrates on a small proportion of visual tokens. The issue\nof \"over-focus\" hinders the model's ability to extract diverse visual features\nand to receive effective gradients for optimization. To address this, we\npropose De-focus Attention Networks, which employ learnable bandpass filters to\ncreate varied attention patterns. During training, large and scheduled drop\npath rates, and an auxiliary loss on globally pooled features for global\nunderstanding tasks are introduced. These two strategies encourage the model to\nattend to a broader range of tokens and enhance network optimization. Extensive\nexperiments validate the efficacy of our approach, demonstrating that 1D causal\nvisual representation can perform comparably to 2D non-causal representation in\ntasks such as global perception, dense prediction, and multi-modal\nunderstanding. Code is released at\nhttps://github.com/OpenGVLab/De-focus-Attention-Networks.",
        "updated": "2024-06-06 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04342v1"
    },
    {
        "title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image",
        "authors": "Stanislaw SzymanowiczEldar InsafutdinovChuanxia ZhengDylan CampbellJoão F. HenriquesChristian RupprechtAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2406.04343v1",
        "entry_id": "http://arxiv.org/abs/2406.04343v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04343v1",
        "summary": "In this paper, we propose Flash3D, a method for scene reconstruction and\nnovel view synthesis from a single image which is both very generalisable and\nefficient. For generalisability, we start from a \"foundation\" model for\nmonocular depth estimation and extend it to a full 3D shape and appearance\nreconstructor. For efficiency, we base this extension on feed-forward Gaussian\nSplatting. Specifically, we predict a first layer of 3D Gaussians at the\npredicted depth, and then add additional layers of Gaussians that are offset in\nspace, allowing the model to complete the reconstruction behind occlusions and\ntruncations. Flash3D is very efficient, trainable on a single GPU in a day, and\nthus accessible to most researchers. It achieves state-of-the-art results when\ntrained and tested on RealEstate10k. When transferred to unseen datasets like\nNYU it outperforms competitors by a large margin. More impressively, when\ntransferred to KITTI, Flash3D achieves better PSNR than methods trained\nspecifically on that dataset. In some instances, it even outperforms recent\nmethods that use multiple views as input. Code, models, demo, and more results\nare available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.",
        "updated": "2024-06-06 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04343v1"
    },
    {
        "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
        "authors": "Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu",
        "links": "http://arxiv.org/abs/2406.04344v1",
        "entry_id": "http://arxiv.org/abs/2406.04344v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04344v1",
        "summary": "Motivated by the large progress made by large language models (LLMs), we\nintroduce the framework of verbalized machine learning (VML). In contrast to\nconventional machine learning models that are typically optimized over a\ncontinuous parameter space, VML constrains the parameter space to be\nhuman-interpretable natural language. Such a constraint leads to a new\nperspective of function approximation, where an LLM with a text prompt can be\nviewed as a function parameterized by the text prompt. Guided by this\nperspective, we revisit classical machine learning problems, such as regression\nand classification, and find that these problems can be solved by an\nLLM-parameterized learner and optimizer. The major advantages of VML include\n(1) easy encoding of inductive bias: prior knowledge about the problem and\nhypothesis class can be encoded in natural language and fed into the\nLLM-parameterized learner; (2) automatic model class selection: the optimizer\ncan automatically select a concrete model class based on data and verbalized\nprior knowledge, and it can update the model class during training; and (3)\ninterpretable learner updates: the LLM-parameterized optimizer can provide\nexplanations for why each learner update is performed. We conduct several\nstudies to empirically evaluate the effectiveness of VML, and hope that VML can\nserve as a stepping stone to stronger interpretability and trustworthiness in\nML.",
        "updated": "2024-06-06 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04344v1"
    },
    {
        "title": "Interpreting the Second-Order Effects of Neurons in CLIP",
        "authors": "Yossi GandelsmanAlexei A. EfrosJacob Steinhardt",
        "links": "http://arxiv.org/abs/2406.04341v1",
        "entry_id": "http://arxiv.org/abs/2406.04341v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04341v1",
        "summary": "We interpret the function of individual neurons in CLIP by automatically\ndescribing them using text. Analyzing the direct effects (i.e. the flow from a\nneuron through the residual stream to the output) or the indirect effects\n(overall contribution) fails to capture the neurons' function in CLIP.\nTherefore, we present the \"second-order lens\", analyzing the effect flowing\nfrom a neuron through the later attention heads, directly to the output. We\nfind that these effects are highly selective: for each neuron, the effect is\nsignificant for <2% of the images. Moreover, each effect can be approximated by\na single direction in the text-image space of CLIP. We describe neurons by\ndecomposing these directions into sparse sets of text representations. The sets\nreveal polysemantic behavior - each neuron corresponds to multiple, often\nunrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we\nmass-produce \"semantic\" adversarial examples by generating images with concepts\nspuriously correlated to the incorrect class. Additionally, we use the\nsecond-order effects for zero-shot segmentation and attribute discovery in\nimages. Our results indicate that a scalable understanding of neurons can be\nused for model deception and for introducing new model capabilities.",
        "updated": "2024-06-06 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04341v1"
    }
]