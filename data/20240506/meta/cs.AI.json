[
    {
        "title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models",
        "authors": "Piotr PadlewskiMax BainMatthew HendersonZhongkai ZhuNishant RelanHai PhamDonovan OngKaloyan AleksievAitor OrmazabalSamuel PhuaEthan YeoEugenie LamprechtQi LiuYuqi WangEric ChenDeyu FuLei LiChe ZhengCyprien de Masson d'AutumeDani YogatamaMikel ArtetxeYi Tay",
        "links": "http://arxiv.org/abs/2405.02287v1",
        "entry_id": "http://arxiv.org/abs/2405.02287v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02287v1",
        "summary": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval",
        "updated": "2024-05-03 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02287v1"
    },
    {
        "title": "What matters when building vision-language models?",
        "authors": "Hugo LaurençonLéo TronchonMatthieu CordVictor Sanh",
        "links": "http://arxiv.org/abs/2405.02246v1",
        "entry_id": "http://arxiv.org/abs/2405.02246v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02246v1",
        "summary": "The growing interest in vision-language models (VLMs) has been driven by\nimprovements in large language models and vision transformers. Despite the\nabundance of literature on this subject, we observe that critical decisions\nregarding the design of VLMs are often not justified. We argue that these\nunsupported decisions impede progress in the field by making it difficult to\nidentify which choices improve model performance. To address this issue, we\nconduct extensive experiments around pre-trained models, architecture choice,\ndata, and training methods. Our consolidation of findings includes the\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\nIdefics2 achieves state-of-the-art performance within its size category across\nvarious multimodal benchmarks, and is often on par with models four times its\nsize. We release the model (base, instructed, and chat) along with the datasets\ncreated for its training.",
        "updated": "2024-05-03 17:00:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02246v1"
    },
    {
        "title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs",
        "authors": "Deepa TilwaniYash SaxenaAli MohammadiEdward RaffAmit ShethSrinivasan ParthasarathyManas Gaur",
        "links": "http://arxiv.org/abs/2405.02228v1",
        "entry_id": "http://arxiv.org/abs/2405.02228v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02228v1",
        "summary": "Automatic citation generation for sentences in a document or report is\nparamount for intelligence analysts, cybersecurity, news agencies, and\neducation personnel. In this research, we investigate whether large language\nmodels (LLMs) are capable of generating references based on two forms of\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\nthe title of a mentioned article when given a sentence from a different\narticle. To demonstrate where LLM stands in this task, we introduce a large\ndataset called REASONS comprising abstracts of the 12 most popular domains of\nscientific research on arXiv. From around 20K research articles, we make the\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\npercentage (PP) to minimize the hallucination rate (HR). When tested with\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\nHR across all domains and models decreased by an average of 41.93% and the PP\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\nstruggle to understand context, but the extent of this issue was small in\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\nreliability of RAG for automated citation generation tasks.",
        "updated": "2024-05-03 16:38:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02228v1"
    },
    {
        "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
        "authors": "Lujing ZhangAaron RothLinjun Zhang",
        "links": "http://arxiv.org/abs/2405.02225v1",
        "entry_id": "http://arxiv.org/abs/2405.02225v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02225v1",
        "summary": "This paper introduces a framework for post-processing machine learning models\nso that their predictions satisfy multi-group fairness guarantees. Based on the\ncelebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G},\n\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\nmulti-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a\npre-specified threshold level $\\alpha$. We propose associated algorithms to\nachieve this notion in general settings. This framework is then applied to\ndiverse scenarios encompassing different fairness concerns, including false\nnegative rate control in image segmentation, prediction set conditional\nuncertainty quantification in hierarchical classification, and de-biased text\ngeneration in language models. We conduct numerical studies on several datasets\nand tasks.",
        "updated": "2024-05-03 16:32:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02225v1"
    },
    {
        "title": "Automatic Programming: Large Language Models and Beyond",
        "authors": "Michael R. LyuBaishakhi RayAbhik RoychoudhuryShin Hwei TanPatanamon Thongtanunam",
        "links": "http://arxiv.org/abs/2405.02213v1",
        "entry_id": "http://arxiv.org/abs/2405.02213v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02213v1",
        "summary": "Automatic programming has seen increasing popularity due to the emergence of\ntools like GitHub Copilot which rely on Large Language Models (LLMs). At the\nsame time, automatically generated code faces challenges during deployment due\nto concerns around quality and trust. In this article, we study automated\ncoding in a general sense and study the concerns around code quality, security\nand related issues of programmer responsibility. These are key issues for\norganizations while deciding on the usage of automatically generated code. We\ndiscuss how advances in software engineering such as program repair and\nanalysis can enable automatic programming. We conclude with a forward looking\nview, focusing on the programming environment of the near future, where\nprogrammers may need to switch to different roles to fully utilize the power of\nautomatic programming. Automated repair of automatically generated programs\nfrom LLMs, can help produce higher assurance code from LLMs, along with\nevidence of assurance",
        "updated": "2024-05-03 16:19:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02213v1"
    }
]