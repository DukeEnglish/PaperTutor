[
    {
        "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
        "authors": "Lujing ZhangAaron RothLinjun Zhang",
        "links": "http://arxiv.org/abs/2405.02225v1",
        "entry_id": "http://arxiv.org/abs/2405.02225v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02225v1",
        "summary": "This paper introduces a framework for post-processing machine learning models\nso that their predictions satisfy multi-group fairness guarantees. Based on the\ncelebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G},\n\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\nmulti-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a\npre-specified threshold level $\\alpha$. We propose associated algorithms to\nachieve this notion in general settings. This framework is then applied to\ndiverse scenarios encompassing different fairness concerns, including false\nnegative rate control in image segmentation, prediction set conditional\nuncertainty quantification in hierarchical classification, and de-biased text\ngeneration in language models. We conduct numerical studies on several datasets\nand tasks.",
        "updated": "2024-05-03 16:32:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02225v1"
    },
    {
        "title": "Position Paper: Rethinking Empirical Research in Machine Learning: Addressing Epistemic and Methodological Challenges of Experimentation",
        "authors": "Moritz HerrmannF. Julian D. LangeKatharina EggenspergerGiuseppe CasalicchioMarcel WeverMatthias FeurerDavid RügamerEyke HüllermeierAnne-Laure BoulesteixBernd Bischl",
        "links": "http://arxiv.org/abs/2405.02200v1",
        "entry_id": "http://arxiv.org/abs/2405.02200v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02200v1",
        "summary": "We warn against a common but incomplete understanding of empirical research\nin machine learning (ML) that leads to non-replicable results, makes findings\nunreliable, and threatens to undermine progress in the field. To overcome this\nalarming situation, we call for more awareness of the plurality of ways of\ngaining knowledge experimentally but also of some epistemic limitations. In\nparticular, we argue most current empirical ML research is fashioned as\nconfirmatory research while it should rather be considered exploratory.",
        "updated": "2024-05-03 15:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02200v1"
    },
    {
        "title": "Optimistic Regret Bounds for Online Learning in Adversarial Markov Decision Processes",
        "authors": "Sang Bin MoonAbolfazl Hashemi",
        "links": "http://arxiv.org/abs/2405.02188v1",
        "entry_id": "http://arxiv.org/abs/2405.02188v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02188v1",
        "summary": "The Adversarial Markov Decision Process (AMDP) is a learning framework that\ndeals with unknown and varying tasks in decision-making applications like\nrobotics and recommendation systems. A major limitation of the AMDP formalism,\nhowever, is pessimistic regret analysis results in the sense that although the\ncost function can change from one episode to the next, the evolution in many\nsettings is not adversarial. To address this, we introduce and study a new\nvariant of AMDP, which aims to minimize regret while utilizing a set of cost\npredictors. For this setting, we develop a new policy search method that\nachieves a sublinear optimistic regret with high probability, that is a regret\nbound which gracefully degrades with the estimation power of the cost\npredictors. Establishing such optimistic regret bounds is nontrivial given that\n(i) as we demonstrate, the existing importance-weighted cost estimators cannot\nestablish optimistic bounds, and (ii) the feedback model of AMDP is different\n(and more realistic) than the existing optimistic online learning works. Our\nresult, in particular, hinges upon developing a novel optimistically biased\ncost estimator that leverages cost predictors and enables a high-probability\nregret analysis without imposing restrictive assumptions. We further discuss\npractical extensions of the proposed scheme and demonstrate its efficacy\nnumerically.",
        "updated": "2024-05-03 15:44:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02188v1"
    },
    {
        "title": "Metalearners for Ranking Treatment Effects",
        "authors": "Toon VanderschuerenWouter VerbekeFelipe MoraesHugo Manuel Proença",
        "links": "http://arxiv.org/abs/2405.02183v1",
        "entry_id": "http://arxiv.org/abs/2405.02183v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02183v1",
        "summary": "Efficiently allocating treatments with a budget constraint constitutes an\nimportant challenge across various domains. In marketing, for example, the use\nof promotions to target potential customers and boost conversions is limited by\nthe available budget. While much research focuses on estimating causal effects,\nthere is relatively limited work on learning to allocate treatments while\nconsidering the operational context. Existing methods for uplift modeling or\ncausal inference primarily estimate treatment effects, without considering how\nthis relates to a profit maximizing allocation policy that respects budget\nconstraints. The potential downside of using these methods is that the\nresulting predictive model is not aligned with the operational context.\nTherefore, prediction errors are propagated to the optimization of the budget\nallocation problem, subsequently leading to a suboptimal allocation policy. We\npropose an alternative approach based on learning to rank. Our proposed\nmethodology directly learns an allocation policy by prioritizing instances in\nterms of their incremental profit. We propose an efficient sampling procedure\nfor the optimization of the ranking model to scale our methodology to\nlarge-scale data sets. Theoretically, we show how learning to rank can maximize\nthe area under a policy's incremental profit curve. Empirically, we validate\nour methodology and show its effectiveness in practice through a series of\nexperiments on both synthetic and real-world data.",
        "updated": "2024-05-03 15:31:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02183v1"
    },
    {
        "title": "An Information Theoretic Perspective on Conformal Prediction",
        "authors": "Alvaro H. C. CorreiaFabio Valerio MassoliChristos LouizosArash Behboodi",
        "links": "http://arxiv.org/abs/2405.02140v1",
        "entry_id": "http://arxiv.org/abs/2405.02140v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02140v1",
        "summary": "Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.",
        "updated": "2024-05-03 14:43:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02140v1"
    }
]