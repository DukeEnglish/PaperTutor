[
    {
        "title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models",
        "authors": "Piotr PadlewskiMax BainMatthew HendersonZhongkai ZhuNishant RelanHai PhamDonovan OngKaloyan AleksievAitor OrmazabalSamuel PhuaEthan YeoEugenie LamprechtQi LiuYuqi WangEric ChenDeyu FuLei LiChe ZhengCyprien de Masson d'AutumeDani YogatamaMikel ArtetxeYi Tay",
        "links": "http://arxiv.org/abs/2405.02287v1",
        "entry_id": "http://arxiv.org/abs/2405.02287v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02287v1",
        "summary": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval",
        "updated": "2024-05-03 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02287v1"
    },
    {
        "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
        "authors": "Aaron KleinJacek GolebiowskiXingchen MaValerio PerroneCedric Archambeau",
        "links": "http://arxiv.org/abs/2405.02267v1",
        "entry_id": "http://arxiv.org/abs/2405.02267v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02267v1",
        "summary": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\nstate-of-the-art for natural language understanding task when fine-tuned on\nlabeled data. However, their large size poses challenges in deploying them for\ninference in real-world applications, due to significant GPU memory\nrequirements and high inference latency. This paper explores neural\narchitecture search (NAS) for structural pruning to find sub-parts of the\nfine-tuned network that optimally trade-off efficiency, for example in terms of\nmodel size or latency, and generalization performance. We also show how we can\nutilize more recently developed two-stage weight-sharing NAS approaches in this\nsetting to accelerate the search process. Unlike traditional pruning methods\nwith fixed thresholds, we propose to adopt a multi-objective approach that\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\nand automated compression process.",
        "updated": "2024-05-03 17:34:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02267v1"
    },
    {
        "title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs",
        "authors": "Deepa TilwaniYash SaxenaAli MohammadiEdward RaffAmit ShethSrinivasan ParthasarathyManas Gaur",
        "links": "http://arxiv.org/abs/2405.02228v1",
        "entry_id": "http://arxiv.org/abs/2405.02228v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02228v1",
        "summary": "Automatic citation generation for sentences in a document or report is\nparamount for intelligence analysts, cybersecurity, news agencies, and\neducation personnel. In this research, we investigate whether large language\nmodels (LLMs) are capable of generating references based on two forms of\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\nthe title of a mentioned article when given a sentence from a different\narticle. To demonstrate where LLM stands in this task, we introduce a large\ndataset called REASONS comprising abstracts of the 12 most popular domains of\nscientific research on arXiv. From around 20K research articles, we make the\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\npercentage (PP) to minimize the hallucination rate (HR). When tested with\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\nHR across all domains and models decreased by an average of 41.93% and the PP\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\nstruggle to understand context, but the extent of this issue was small in\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\nreliability of RAG for automated citation generation tasks.",
        "updated": "2024-05-03 16:38:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02228v1"
    },
    {
        "title": "Impact of emoji exclusion on the performance of Arabic sarcasm detection models",
        "authors": "Ghalyah H. AleryaniWael DeabesKhaled AlbishreAlaa E. Abdel-Hakim",
        "links": "http://arxiv.org/abs/2405.02195v1",
        "entry_id": "http://arxiv.org/abs/2405.02195v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02195v1",
        "summary": "The complex challenge of detecting sarcasm in Arabic speech on social media\nis increased by the language diversity and the nature of sarcastic expressions.\nThere is a significant gap in the capability of existing models to effectively\ninterpret sarcasm in Arabic, which mandates the necessity for more\nsophisticated and precise detection methods. In this paper, we investigate the\nimpact of a fundamental preprocessing component on sarcasm speech detection.\nWhile emojis play a crucial role in mitigating the absence effect of body\nlanguage and facial expressions in modern communication, their impact on\nautomated text analysis, particularly in sarcasm detection, remains\nunderexplored. We investigate the impact of emoji exclusion from datasets on\nthe performance of sarcasm detection models in social media content for Arabic\nas a vocabulary-super rich language. This investigation includes the adaptation\nand enhancement of AraBERT pre-training models, specifically by excluding\nemojis, to improve sarcasm detection capabilities. We use AraBERT pre-training\nto refine the specified models, demonstrating that the removal of emojis can\nsignificantly boost the accuracy of sarcasm detection. This approach\nfacilitates a more refined interpretation of language, eliminating the\npotential confusion introduced by non-textual elements. The evaluated AraBERT\nmodels, through the focused strategy of emoji removal, adeptly navigate the\ncomplexities of Arabic sarcasm. This study establishes new benchmarks in Arabic\nnatural language processing and presents valuable insights for social media\nplatforms.",
        "updated": "2024-05-03 15:51:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02195v1"
    },
    {
        "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
        "authors": "Negar ArabzadehSiging HuoNikhil MehtaQinqyun WuChi WangAhmed AwadallahCharles L. A. ClarkeJulia Kiseleva",
        "links": "http://arxiv.org/abs/2405.02178v1",
        "entry_id": "http://arxiv.org/abs/2405.02178v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02178v1",
        "summary": "The rapid development of Large Language Models (LLMs) has led to a surge in\napplications that facilitate collaboration among multiple agents, assisting\nhumans in their daily tasks. However, a significant gap remains in assessing to\nwhat extent LLM-powered applications genuinely enhance user experience and task\nexecution efficiency. This highlights the need to verify utility of LLM-powered\napplications, particularly by ensuring alignment between the application's\nfunctionality and end-user needs. We introduce AgentEval, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the effectiveness and robustness of AgentEval for two\nopen source datasets including Math Problem solving and ALFWorld House-hold\nrelated tasks. For reproducibility purposes, we make the data, code and all the\nlogs publicly available at https://bit.ly/3w3yKcS .",
        "updated": "2024-05-03 15:26:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02178v1"
    }
]