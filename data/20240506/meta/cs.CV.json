[
    {
        "title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models",
        "authors": "Piotr PadlewskiMax BainMatthew HendersonZhongkai ZhuNishant RelanHai PhamDonovan OngKaloyan AleksievAitor OrmazabalSamuel PhuaEthan YeoEugenie LamprechtQi LiuYuqi WangEric ChenDeyu FuLei LiChe ZhengCyprien de Masson d'AutumeDani YogatamaMikel ArtetxeYi Tay",
        "links": "http://arxiv.org/abs/2405.02287v1",
        "entry_id": "http://arxiv.org/abs/2405.02287v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02287v1",
        "summary": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval",
        "updated": "2024-05-03 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02287v1"
    },
    {
        "title": "DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos",
        "authors": "Wen-Hsuan ChuLei KeKaterina Fragkiadaki",
        "links": "http://arxiv.org/abs/2405.02280v1",
        "entry_id": "http://arxiv.org/abs/2405.02280v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02280v1",
        "summary": "Existing VLMs can track in-the-wild 2D video objects while current generative\nmodels provide powerful visual priors for synthesizing novel views for the\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\nprogress, we present DreamScene4D, the first approach that can generate\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\nvideos with large object motion across occlusions and novel viewpoints. Our key\ninsight is to design a \"decompose-then-recompose\" scheme to factorize both the\nwhole video scene and each object's 3D motion. We first decompose the video\nscene by using open-vocabulary mask trackers and an adapted image diffusion\nmodel to segment, track, and amodally complete the objects and background in\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\nmove in space and time. We also factorize the observed motion into multiple\ncomponents to handle fast motion. The camera motion can be inferred by\nre-rendering the background to match the video frames. For the object motion,\nwe first model the object-centric deformation of the objects by leveraging\nrendering losses and multi-view generative priors in an object-centric frame,\nthen optimize object-centric to world-frame transformations by comparing the\nrendered outputs against the perceived pixel and optical flow. Finally, we\nrecompose the background and objects and optimize for relative object scales\nusing monocular depth prediction guidance. We show extensive results on the\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\nand provide future directions. Besides 4D scene generation, our results show\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\ninferred 3D trajectories to 2D, while never explicitly trained to do so.",
        "updated": "2024-05-03 17:55:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02280v1"
    },
    {
        "title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?",
        "authors": "Maxime ZanellaIsmail Ben Ayed",
        "links": "http://arxiv.org/abs/2405.02266v1",
        "entry_id": "http://arxiv.org/abs/2405.02266v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02266v1",
        "summary": "The development of large vision-language models, notably CLIP, has catalyzed\nresearch into effective adaptation techniques, with a particular focus on soft\nprompt tuning. Conjointly, test-time augmentation, which utilizes multiple\naugmented views of a single image to enhance zero-shot generalization, is\nemerging as a significant area of interest. This has predominantly directed\nresearch efforts toward test-time prompt tuning. In contrast, we introduce a\nrobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based\nmethods without requiring this intensive training procedure. This positions MTA\nas an ideal solution for both standalone and API-based applications.\nAdditionally, our method does not rely on ad hoc rules (e.g., confidence\nthreshold) used in some previous test-time augmentation techniques to filter\nthe augmented views. Instead, MTA incorporates a quality assessment variable\nfor each view directly into its optimization process, termed as the inlierness\nscore. This score is jointly optimized with a density mode seeking process,\nleading to an efficient training- and hyperparameter-free approach. We\nextensively benchmark our method on 15 datasets and demonstrate MTA's\nsuperiority and computational efficiency. Deployed easily as plug-and-play\nmodule on top of zero-shot models and state-of-the-art few-shot methods, MTA\nshows systematic and consistent improvements.",
        "updated": "2024-05-03 17:34:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02266v1"
    },
    {
        "title": "What matters when building vision-language models?",
        "authors": "Hugo LaurençonLéo TronchonMatthieu CordVictor Sanh",
        "links": "http://arxiv.org/abs/2405.02246v1",
        "entry_id": "http://arxiv.org/abs/2405.02246v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02246v1",
        "summary": "The growing interest in vision-language models (VLMs) has been driven by\nimprovements in large language models and vision transformers. Despite the\nabundance of literature on this subject, we observe that critical decisions\nregarding the design of VLMs are often not justified. We argue that these\nunsupported decisions impede progress in the field by making it difficult to\nidentify which choices improve model performance. To address this issue, we\nconduct extensive experiments around pre-trained models, architecture choice,\ndata, and training methods. Our consolidation of findings includes the\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\nIdefics2 achieves state-of-the-art performance within its size category across\nvarious multimodal benchmarks, and is often on par with models four times its\nsize. We release the model (base, instructed, and chat) along with the datasets\ncreated for its training.",
        "updated": "2024-05-03 17:00:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02246v1"
    },
    {
        "title": "Designed Dithering Sign Activation for Binary Neural Networks",
        "authors": "Brayan MonroyJuan EstupiñanTatiana Gelvez-BarreraJorge BaccaHenry Arguello",
        "links": "http://arxiv.org/abs/2405.02220v1",
        "entry_id": "http://arxiv.org/abs/2405.02220v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02220v1",
        "summary": "Binary Neural Networks emerged as a cost-effective and energy-efficient\nsolution for computer vision tasks by binarizing either network weights or\nactivations. However, common binary activations, such as the Sign activation\nfunction, abruptly binarize the values with a single threshold, losing\nfine-grained details in the feature outputs. This work proposes an activation\nthat applies multiple thresholds following dithering principles, shifting the\nSign activation function for each pixel according to a spatially periodic\nthreshold kernel. Unlike literature methods, the shifting is defined jointly\nfor a set of adjacent pixels, taking advantage of spatial correlations.\nExperiments over the classification task demonstrate the effectiveness of the\ndesigned dithering Sign activation function as an alternative activation for\nbinary neural networks, without increasing the computational cost. Further,\nDeSign balances the preservation of details with the efficiency of binary\noperations.",
        "updated": "2024-05-03 16:27:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02220v1"
    }
]