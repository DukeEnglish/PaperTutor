[
    {
        "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
        "authors": "Aaron KleinJacek GolebiowskiXingchen MaValerio PerroneCedric Archambeau",
        "links": "http://arxiv.org/abs/2405.02267v1",
        "entry_id": "http://arxiv.org/abs/2405.02267v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02267v1",
        "summary": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\nstate-of-the-art for natural language understanding task when fine-tuned on\nlabeled data. However, their large size poses challenges in deploying them for\ninference in real-world applications, due to significant GPU memory\nrequirements and high inference latency. This paper explores neural\narchitecture search (NAS) for structural pruning to find sub-parts of the\nfine-tuned network that optimally trade-off efficiency, for example in terms of\nmodel size or latency, and generalization performance. We also show how we can\nutilize more recently developed two-stage weight-sharing NAS approaches in this\nsetting to accelerate the search process. Unlike traditional pruning methods\nwith fixed thresholds, we propose to adopt a multi-objective approach that\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\nand automated compression process.",
        "updated": "2024-05-03 17:34:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02267v1"
    },
    {
        "title": "Subgraph2vec: A random walk-based algorithm for embedding knowledge graphs",
        "authors": "Elika BozorgiSaber SoleimaniSakher Khalil AlqaiidiHamid Reza ArabniaKrzysztof Kochut",
        "links": "http://arxiv.org/abs/2405.02240v1",
        "entry_id": "http://arxiv.org/abs/2405.02240v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02240v1",
        "summary": "Graph is an important data representation which occurs naturally in the real\nworld applications \\cite{goyal2018graph}. Therefore, analyzing graphs provides\nusers with better insights in different areas such as anomaly detection\n\\cite{ma2021comprehensive}, decision making \\cite{fan2023graph}, clustering\n\\cite{tsitsulin2023graph}, classification \\cite{wang2021mixup} and etc.\nHowever, most of these methods require high levels of computational time and\nspace. We can use other ways like embedding to reduce these costs. Knowledge\ngraph (KG) embedding is a technique that aims to achieve the vector\nrepresentation of a KG. It represents entities and relations of a KG in a\nlow-dimensional space while maintaining the semantic meanings of them. There\nare different methods for embedding graphs including random walk-based methods\nsuch as node2vec, metapath2vec and regpattern2vec. However, most of these\nmethods bias the walks based on a rigid pattern usually hard-coded in the\nalgorithm. In this work, we introduce \\textit{subgraph2vec} for embedding KGs\nwhere walks are run inside a user-defined subgraph. We use this embedding for\nlink prediction and prove our method has better performance in most cases in\ncomparison with the previous ones.",
        "updated": "2024-05-03 16:51:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02240v1"
    },
    {
        "title": "Learning Optimal Deterministic Policies with Stochastic Policy Gradients",
        "authors": "Alessandro MontenegroMarco MussiAlberto Maria MetelliMatteo Papini",
        "links": "http://arxiv.org/abs/2405.02235v1",
        "entry_id": "http://arxiv.org/abs/2405.02235v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02235v1",
        "summary": "Policy gradient (PG) methods are successful approaches to deal with\ncontinuous reinforcement learning (RL) problems. They learn stochastic\nparametric (hyper)policies by either exploring in the space of actions or in\nthe space of parameters. Stochastic controllers, however, are often undesirable\nfrom a practical perspective because of their lack of robustness, safety, and\ntraceability. In common practice, stochastic (hyper)policies are learned only\nto deploy their deterministic version. In this paper, we make a step towards\nthe theoretical understanding of this practice. After introducing a novel\nframework for modeling this scenario, we study the global convergence to the\nbest deterministic policy, under (weak) gradient domination assumptions. Then,\nwe illustrate how to tune the exploration level used for learning to optimize\nthe trade-off between the sample complexity and the performance of the deployed\ndeterministic policy. Finally, we quantitatively compare action-based and\nparameter-based exploration, giving a formal guise to intuitive results.",
        "updated": "2024-05-03 16:45:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02235v1"
    },
    {
        "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
        "authors": "Lujing ZhangAaron RothLinjun Zhang",
        "links": "http://arxiv.org/abs/2405.02225v1",
        "entry_id": "http://arxiv.org/abs/2405.02225v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02225v1",
        "summary": "This paper introduces a framework for post-processing machine learning models\nso that their predictions satisfy multi-group fairness guarantees. Based on the\ncelebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G},\n\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\nmulti-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a\npre-specified threshold level $\\alpha$. We propose associated algorithms to\nachieve this notion in general settings. This framework is then applied to\ndiverse scenarios encompassing different fairness concerns, including false\nnegative rate control in image segmentation, prediction set conditional\nuncertainty quantification in hierarchical classification, and de-biased text\ngeneration in language models. We conduct numerical studies on several datasets\nand tasks.",
        "updated": "2024-05-03 16:32:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02225v1"
    },
    {
        "title": "Discretization Error of Fourier Neural Operators",
        "authors": "Samuel LanthalerAndrew M. StuartMargaret Trautner",
        "links": "http://arxiv.org/abs/2405.02221v1",
        "entry_id": "http://arxiv.org/abs/2405.02221v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02221v1",
        "summary": "Operator learning is a variant of machine learning that is designed to\napproximate maps between function spaces from data. The Fourier Neural Operator\n(FNO) is a common model architecture used for operator learning. The FNO\ncombines pointwise linear and nonlinear operations in physical space with\npointwise linear operations in Fourier space, leading to a parameterized map\nacting between function spaces. Although FNOs formally involve convolutions of\nfunctions on a continuum, in practice the computations are performed on a\ndiscretized grid, allowing efficient implementation via the FFT. In this paper,\nthe aliasing error that results from such a discretization is quantified and\nalgebraic rates of convergence in terms of the grid resolution are obtained as\na function of the regularity of the input. Numerical experiments that validate\nthe theory and describe model stability are performed.",
        "updated": "2024-05-03 16:28:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02221v1"
    }
]