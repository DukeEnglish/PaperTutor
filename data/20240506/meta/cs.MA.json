[
    {
        "title": "On the Utility of External Agent Intention Predictor for Human-AI Coordination",
        "authors": "Chenxu WangZilong ChenAngelo CangelosiHuaping Liu",
        "links": "http://arxiv.org/abs/2405.02229v1",
        "entry_id": "http://arxiv.org/abs/2405.02229v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02229v1",
        "summary": "Reaching a consensus on the team plans is vital to human-AI coordination.\nAlthough previous studies provide approaches through communications in various\nways, it could still be hard to coordinate when the AI has no explainable plan\nto communicate. To cover this gap, we suggest incorporating external models to\nassist humans in understanding the intentions of AI agents. In this paper, we\npropose a two-stage paradigm that first trains a Theory of Mind (ToM) model\nfrom collected offline trajectories of the target agent, and utilizes the model\nin the process of human-AI collaboration by real-timely displaying the future\naction predictions of the target agent. Such a paradigm leaves the AI agent as\na black box and thus is available for improving any agents. To test our\nparadigm, we further implement a transformer-based predictor as the ToM model\nand develop an extended online human-AI collaboration platform for experiments.\nThe comprehensive experimental results verify that human-AI teams can achieve\nbetter performance with the help of our model. A user assessment attached to\nthe experiment further demonstrates that our paradigm can significantly enhance\nthe situational awareness of humans. Our study presents the potential to\naugment the ability of humans via external assistance in human-AI\ncollaboration, which may further inspire future research.",
        "updated": "2024-05-03 16:39:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02229v1"
    },
    {
        "title": "The Cambridge RoboMaster: An Agile Multi-Robot Research Platform",
        "authors": "Jan BlumenkampAjay ShankarMatteo BettiniJoshua BirdAmanda Prorok",
        "links": "http://arxiv.org/abs/2405.02198v1",
        "entry_id": "http://arxiv.org/abs/2405.02198v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02198v1",
        "summary": "Compact robotic platforms with powerful compute and actuation capabilities\nare key enablers for practical, real-world deployments of multi-agent research.\nThis article introduces a tightly integrated hardware, control, and simulation\nsoftware stack on a fleet of holonomic ground robot platforms designed with\nthis motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles,\noffer a balance between small robots that do not possess sufficient compute or\nactuation capabilities and larger robots that are unsuitable for indoor\nmulti-robot tests. They run a modular ROS2-based optimal estimation and control\nstack for full onboard autonomy, contain ad-hoc peer-to-peer communication\ninfrastructure, and can zero-shot run multi-agent reinforcement learning (MARL)\npolicies trained in our vectorized multi-agent simulation framework. We present\nan in-depth review of other platforms currently available, showcase new\nexperimental validation of our system's capabilities, and introduce case\nstudies that highlight the versatility and reliabilty of our system as a\ntestbed for a wide range of research demonstrations. Our system as well as\nsupplementary material is available online:\nhttps://proroklab.github.io/cambridge-robomaster",
        "updated": "2024-05-03 15:54:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02198v1"
    },
    {
        "title": "Simulating the economic impact of rationality through reinforcement learning and agent-based modelling",
        "authors": "Simone BrusatinTommaso PadoanAndrea ColettaDomenico Delli GattiAldo Glielmo",
        "links": "http://arxiv.org/abs/2405.02161v1",
        "entry_id": "http://arxiv.org/abs/2405.02161v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02161v1",
        "summary": "Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined, not fully\nrational, behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of fully rational agents that learn\ntheir policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for a thorough study of the impact of rationality on\nthe economy. We find that RL agents spontaneously learn three distinct\nstrategies for maximising profits, with the optimal strategy depending on the\nlevel of market competition and rationality. We also find that RL agents with\nindependent policies, and without the ability to communicate with each other,\nspontaneously learn to segregate into different strategic groups, thus\nincreasing market power and overall profits. Finally, we find that a higher\ndegree of rationality in the economy always improves the macroeconomic\nenvironment as measured by total output, depending on the specific rational\npolicy, this can come at the cost of higher instability. Our R-MABM framework\nis general, it allows for stable multi-agent learning, and represents a\nprincipled and robust direction to extend existing economic simulators.",
        "updated": "2024-05-03 15:08:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02161v1"
    },
    {
        "title": "Learning from Evolution: Improving Collective Decision-Making Mechanisms using Insights from Evolutionary Robotics",
        "authors": "Tanja Katharina Kaiser",
        "links": "http://dx.doi.org/10.1145/3638529.3653988",
        "entry_id": "http://arxiv.org/abs/2405.02133v1",
        "pdf_url": "http://arxiv.org/pdf/2405.02133v1",
        "summary": "Collective decision-making enables multi-robot systems to act autonomously in\nreal-world environments. Existing collective decision-making mechanisms suffer\nfrom the so-called speed versus accuracy trade-off or rely on high complexity,\ne.g., by including global communication. Recent work has shown that more\nefficient collective decision-making mechanisms based on artificial neural\nnetworks can be generated using methods from evolutionary computation. A major\ndrawback of these decision-making neural networks is their limited\ninterpretability. Analyzing evolved decision-making mechanisms can help us\nimprove the efficiency of hand-coded decision-making mechanisms while\nmaintaining a higher interpretability. In this paper, we analyze evolved\ncollective decision-making mechanisms in detail and hand-code two new\ndecision-making mechanisms based on the insights gained. In benchmark\nexperiments, we show that the newly implemented collective decision-making\nmechanisms are more efficient than the state-of-the-art collective\ndecision-making mechanisms voter model and majority rule.",
        "updated": "2024-05-03 14:37:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.02133v1"
    },
    {
        "title": "Detecting and Deterring Manipulation in a Cognitive Hierarchy",
        "authors": "Nitay AlonLion SchulzJoseph M. BarnbyJeffrey S. RosenscheinPeter Dayan",
        "links": "http://arxiv.org/abs/2405.01870v1",
        "entry_id": "http://arxiv.org/abs/2405.01870v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01870v1",
        "summary": "Social agents with finitely nested opponent models are vulnerable to\nmanipulation by agents with deeper reasoning and more sophisticated opponent\nmodelling. This imbalance, rooted in logic and the theory of recursive\nmodelling frameworks, cannot be solved directly. We propose a computational\nframework, $\\aleph$-IPOMDP, augmenting model-based RL agents' Bayesian\ninference with an anomaly detection algorithm and an out-of-belief policy. Our\nmechanism allows agents to realize they are being deceived, even if they cannot\nunderstand how, and to deter opponents via a credible threat. We test this\nframework in both a mixed-motive and zero-sum game. Our results show the\n$\\aleph$ mechanism's effectiveness, leading to more equitable outcomes and less\nexploitation by more sophisticated agents. We discuss implications for AI\nsafety, cybersecurity, cognitive science, and psychiatry.",
        "updated": "2024-05-03 05:53:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01870v1"
    }
]