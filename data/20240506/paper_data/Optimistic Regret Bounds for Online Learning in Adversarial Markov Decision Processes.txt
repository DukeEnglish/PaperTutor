Optimistic Regret Bounds for Online Learning in
Adversarial Markov Decision Processes
Sang Bin Moon Abolfazl Hashemi∗
Abstract
The Adversarial Markov Decision Process (AMDP) is a learning framework that deals with
unknown and varying tasks in decision-making applications like robotics and recommendation
systems. A major limitation of the AMDP formalism, however, is pessimistic regret analysis
resultsinthesensethatalthoughthecostfunctioncanchangefromoneepisodetothenext,the
evolution in many settings is not adversarial. To address this, we introduce and study a new
variantofAMDP,whichaimstominimizeregretwhileutilizingasetofcostpredictors. Forthis
setting, we develop a new policy search method that achieves a sublinear optimistic regret with
high probability, that is a regret bound which gracefully degrades with the estimation power of
the cost predictors. Establishing such optimistic regret bounds is nontrivial given that (i) as
we demonstrate, the existing importance-weighted cost estimators cannot establish optimistic
bounds, and(ii)thefeedbackmodelofAMDPisdifferent(andmorerealistic)thantheexisting
optimistic online learning works. Our result, in particular, hinges upon developing a novel
optimisticallybiasedcostestimatorthatleveragescostpredictorsandenablesahigh-probability
regretanalysiswithoutimposingrestrictiveassumptions. Wefurtherdiscusspracticalextensions
of the proposed scheme and demonstrate its efficacy numerically.
1 Introduction
Reinforcement learning studies the problem of sequential decision-making modeled as a Markov
Decision Process (MDP), where a learner interacts with an environment and solves the optimal
policy that minimizes the cumulative cost incurred by the environment. The learner interacts
with the environment by observing a state, choosing an action, and suffering a cost, repeatedly
for a finite number of time steps. The process is sequential in the sense that the chosen action
affects the environment state, and thus the next state is observed through a stochastic transition
probabilityfunction,andthecostsufferedbythelearnerisdeterminedbyanunknowncostfunction
accordingly. After a number of episodes, one can measure the performance of the learner’s policy
with regret, i.e., how larger the total cost suffered by the learner is compared to the total cost of
a fixed optimal policy in hindsight. MDPs are useful for decision-making in various fields, such
as robotics [1], finance [2, 3], and healthcare [4]. However, in many real-world applications, the
tasks and environment may change over time, leading to non-stationary dynamics. In such cases,
the assumptions of MDP may not hold, and the performance of the decision-making system may
deteriorate.
In this paper, we consider the problem of learning policies in Adversarial MDP (AMDP) as a
generalizationofthetraditionalMDPmodel, wheretheenvironmentcanchoosedifferentcostfunc-
tions for each episode. AMDP gives greater flexibility to account for changing environments and
eventheexistenceofotheragents. Forexample,AMDPcanmodelanenergy-efficientdronenaviga-
tion problem [5], where wind incurs higher energy consumption while it is not observed in advance
∗Authors are with the School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN
47907, USA.
1
4202
yaM
3
]LM.tats[
1v88120.5042:viXraand changes arbitrarily. Stochastic inventory control [6] can also be modeled as AMDP, because
item price and inventory cost change from time to time due to economic conditions. Eventually,
AMDP can be extended to hierarchical or multi-agent problems, because parent policy or other
agents evolve and incur different costs to a learner. Existing online learning [6, 7, 8, 9, 10, 11, 12]
and policy optimization approaches [13, 14] to AMDP solves the optimization problem to minimize
the cost in hindsight. However, it can be too restrictive and result in conservative regret bounds.
For instance, in multiplayer games, the action, and in turn the policies of other players may be
predicted from simulation and historical observation; this insight if leveraged properly may lead to
turning the game to a specific player’s advantage [15].
Motivatedbythisshortcoming,weproposetostudyanewformulationforRLwithtime-varying
cost functions where the aim is to learn a policy that minimizes its regret while resorting to a given
set of time-varying predictive estimators of the cost functions, denoted by {c }T and {M }T ,
t t=1 t t=1
respectively. We propose a novel policy search scheme that utilizes the set of optimistic cost
predictors and achieves sub-linear regret bounds. Specifically, we make the following contributions:
(cid:16)(cid:113) (cid:17)
• We show the worst-case regret bound of O˜ d(cid:0) {c }T ,{M }T (cid:1) for the full-information
t t=1 t t=1
feedback setting1 and O˜(cid:0) d({c }T ,{M }T )2/3(cid:1) in expectation for bandit feedback setting,
t t=1 t t=1
where d(·,·) captures cumulative estimation error of the cost predictors. It is also shown that
with high probability the algorithm achieves the regret bound of O˜(cid:0) d({c }T ,{M }T )3/4(cid:1) .
t t=1 t t=1
These regret bounds are optimistic in nature, i.e., the bound scales with the prediction power
of optimistic cost predictors, and can lead to constant regret with perfect prediction. In the
worst case, on the other hand, the proposed scheme to learn a policy satisfies sublinear regret
bounds.
• Crucial to the establishment of these results is the development of a new cost estimator. This
new estimator leverages the bandit information about the cost as well as the set of predictive
estimators to update the policy. We show the proposed estimator has variance-reduction
benefits and thus it may be of independent interest in similar problems.
• We also introduce the anytime extensions for continuous training beyond the fixed number
of episodes and establish similar regret guarantees. Then we generalize the setting to the
unknown transition setting and establish high probability regret bounds by leveraging the
idea of transition estimation via confidence sets.
2 Background and Related Work
WestartwiththeprecisedefinitionofanAMDP.Astandarddefinitionfollowsanepisodicloop-free
AMDP [8] or a loop-free stochastic shortest path [16].
Definition 1. An episodic loop-free Adversarial Markov decision process (AMDP) is a tuple M =
(X,A,P,L,{c }T ) which consists of a finite discrete state space denoted by X, a finite discrete
t t=1
action space denoted by A, a probabilistic transition function denoted by P : X ×A×X → [0,1],
and a sequence of cost functions denoted by c : X ×A → R such that:
t
• The cost functions are bounded, that is, c ∈ [0,1]|X|×|A| for t = 1,2,...,T.
t
• The state space X is partitioned into L non-overlapping layers X ,X ,...,X such that X =
0 1 L
∪L X and, it holds that X ∩X = ∅ for any l ̸= l .
l=0 l l1 l2 1 2
1Recall the notation O˜(·) hides the logarithmic terms in its argument.
2• The state transition function Pr(x′|x,a) is stationary.
• If for some x ∈ X and some layer l ∈ {0,...,L−1}, Pr(x′|x,a) > 0, then x′ ∈ X ; that is,
l l+1
state transition happens only between two consecutive layers.
• X and X are singletons; that is, X = {x } and X = {x }.
0 L 0 0 L L
Policy search in AMDP. Online learning approaches to MDP, such as Follow-the-Regularized-
Leader (FTRL) or Online Mirror Descent (OMD), solve the linear optimization problem with occu-
pancy measure ρ. Occupancy measure quantifies the joint probability of the probability of visiting
a state x and the probability of taking an action a given the state. Thus, conversely, an occupancy
measure controls the behavior of an agent under a stationary, stochastic, and known/unknown
transition probability distribution. The behavior is governed by the policy π defined as
ρ (x,a)
t
π (a|x) = . (1)
t (cid:80) ρ (x,a′)
a′∈A t
Therefore, given an MDP, the optimization objective is to minimize the total cost suffered by an
occupancy measure. Since occupancy measure quantifies the probability of a specific state and
action pair, the total (expected) cost can be formulated by a linear objective function with respect
(cid:80)
to a cost function c, i.e., ⟨ρ ,c ⟩ = c (x,a)ρ (x,a). This leads to the following definition
t t x∈X,a∈A t t
of regret (w.r.t. the policy corresponding to ρ) that underlies the problem of learning policies in
AMDPs,
T
(cid:88)
R (ρ∗,{c }T ) = ⟨ρ −ρ∗,c ⟩. (2)
T t t=1 t t
t=1
Here, ρ ∈ ∆(M) where ∆(M) denote the space of all occupancy measures over AMDP M, ⟨.,.⟩
represents the Euclidean inner product over the space of X ×A, and ρ denotes the agent’s selected
t
occupancy measure in episode t.
OREPS [8] is the baseline algorithm for learning policies in AMDPs that solves the constrained,
regularized regret minimization problem via a mirror descent update with stepsize η, i.e., ρ =
t+1
argmin η⟨ρ,c ⟩+D (ρ∥ρ ), where R is negative entropy
ρ∈∆(M) t R t
(cid:88) (cid:88)
R(ρ) = ρ(x,a)logρ(x,a)− ρ(x,a),
x∈X,a∈A x∈X,a∈A
and D is the unnormalized KL divergence being the corresponding Bregman divergence [17, 18]
R
(cid:88) ρ(x,a)
D (ρ∥ρ′) = ρ(x,a)log
R ρ′(x,a)
x∈X,a∈A (3)
−
(cid:88) (cid:0) ρ(x,a)−ρ′(x,a)(cid:1)
.
x∈X,a∈A
KL divergence regularizes the information loss from the history that previous solutions were opti-
mizedfor. OREPSsolvestheunconstrainedversionoftheoriginalproblemandthedualformulation
of the projection onto ∆(M).
3Optimistic online learning. Let {M }T be a sequence of time varying predictive estimators
t t=1
such that M : X ×A → [0,1] for all t. For online linear optimization, [19] show that optimistic
t
mirror descent (OMD) [20] equipped with a similar cost predictor sequence can achieve optimistic
(cid:113)
regret bounds, i.e., O˜( d({c }T ,{M }T )), where d(·,·) captures cumulative estimation error of
t t=1 t t=1
the cost predictors. This result shows with perfect estimation the regret is O˜(1) while for futile
√
estimation, i.e., the worst case, the regret is O˜( T). In this paper, we aim to establish optimistic
regret bounds for a class of policy search methods in AMDPs. In contrast to [19], our setting is
more general in the sense that it accounts for the dynamic and state-full nature of the interaction
between the learner and the environment which is captured by the notion of state space. Further,
although[19]leveragesthemethodfrom[21]toproposeano-regretschemeforthebanditsettingin
online linear optimization, their algorithm is not applicable in our setting since the bandit feedback
model of the present paper is different from [19] and more meaningful in the sense that the learner
observes the cost of the chosen action, not the mixture of cost of all feasible actions. Consequently,
the proposed method and its analysis differ considerably from [19]. Further, we leverage a single-
projection method adopted from [22] to reduce the computational cost of optimistic policy search
compared to OMD which requires two projection steps.
Bandit cost estimation. Learning a policy in the bandit case relies on estimating the unknown
cost function for each episode. Given the connection of AMDPs to adversarial bandits, [8] incorpo-
rate the celebrated importance-weighted cost estimator in OREPS which was originally exhibited
in the EXP3 algorithm [23]. Recently, [12, 24] have utilized the implicit exploration estimator from
[25], i.e.,
c (x,a)
cˆ′(x,a) = t I{(x,a) ∈ u¯ (t)}, (4)
t ρ (x,a)+γ L
t
in a similar OREPS-based update, where γ ≥ 0 is the exploration parameter and u¯ (t) denotes
L
th
the history of states and actions up to and including the L layer of episode t. As we discuss
later, such estimators fail to result in optimistic regret guarantees that degrade gracefully with
d({c }T ,{M }T ). Thus, we develop a new cost estimator, characterize its properties, and show
t t=1 t t=1
that it results in optimistic bounds.
3 Optimistic Learning in AMDPs
Given that in the bandit setting, we need to resort to cost estimation, the estimation error of the
estimator is an integral part of the regrets of the underlying algorithms. In order to establish
optimistic bounds, our regret analysis shows that it is crucial to have an estimator whose error is
controlled with d({c }T ,{M }T ). Let us consider the estimator (4), define E [·] = E[·|u(t)],
t t=1 t t=1 t
and examine E ∥cˆ′ −M ∥2 which can be thought of as some notion of variance. Note that (4)
t−1 t t
with γ = 0 may suffer from an unbounded variance.2 With γ > 0 immediate calculation shows
E (cid:2) (cˆ′(x,a)−M (x,a))2(cid:3) cannot be written as a function of |c (x,a)−M (x,a)| which, as our
t−1 t t t t
regret analysis demonstrates, results in failure of achieving optimistic expected regret bounds when
utilizing (4) with γ ≥ 0.
We thus propose a new cost estimator that provably results in an optimistic expected regret
bound in conjunction with a mirror descent-based update. The proposed estimator defined for all
2This property is known to be the underlying reason that EXP3 cannot satisfy sublinear regret with high proba-
bility in adversarial bandits [18].
4γ ≥ 0 is as follows
cˆ(x,a) (5)
t
c (x,a)−M (x,a)
= t t I{(x,a) ∈ u¯ (t)}+M (x,a).
L t
ρ (x,a)+γ
t
Crucially,theproposedestimatorcˆ(x,a)leveragesthepredictiveestimatorsM (x,a). Inparticular,
t t
in contrast to (4) the unexplored state and action pairs incur the cost predicted by M (x,a) as
t
opposed to incurring zero cost. Also, [26] suggested a similar cost estimator as (5) with γ = 0 for
the multi-armed bandit problem. However, our estimators in this paper address the problem of
learning in MDPs and exploration parameter γ > 0 is crucial to our analysis of high probability
guarantee with Lemma 2 in Appendix A.4.
Lemma 1 studies the statistical properties of the proposed estimator.
Lemma 1. The proposed cost estimator (5) satisfies
ρ (x,a)c (x,a)+γM (x,a)
E [cˆ(x,a)] = t t t ,
t−1 t
ρ (x,a)+γ
t
(cid:0) (cid:1)2
c (x,a)−M (x,a)
E (cid:2) (cˆ(x,a)−M (x,a))2(cid:3) ≤ t t .
t−1 t t
ρ (x,a)+γ
t
Variance reduction property. Thisresultshowsthatifγ > 0thevarianceisprovablybounded.
Furthermore, if M (x,a) ≤ 2c (x,a) for all (x,a) ∈ X ×A and t = 1,...T, immediate calculation
t t
shows |c (x,a)−M (x,a)|2 ≤ |c (x,a)|2. That is, the proposed estimator enjoys a lower variance
t t t
compared to (4). Also if the predictors {M }T are optimistic, i.e., M (x,a) ≤ c (x,a), for all
t t=1 t t
t = 1,...,T and (x,a) ∈ X ×A then the proposed cost estimator (5) is an optimistically biased
estimator given that
ρ (x,a)c (x,a)+γM (x,a)
E [cˆ(x,a)] = t t t ≤ c (x,a).
t−1 t t
ρ (x,a)+γ
t
Therefore, as long as M (x,a) ≤ c (x,a), compared to (4), the proposed estimator has the same
t t
bias while having a lower variance. Note that the condition M (x,a) ≤ c (x,a) is very mild and
t t
may be ensured in a variety of non-adversarial settings based on the observed cost signal. Finally,
note that different from (4) the variance of the proposed estimator is controlled by the estimation
power of the cost predictors. A feature we will leverage to achieve optimistic regret bounds.
Withtheproposedcostestimator,wethenutilizeitinamirror-descenttypeupdatebyadopting
the result of [22]. In particular, given ρ the agent runs an episode exploration subroutine and
t
subsequently employs
ρ = arg min η⟨ρ,cˆ +M −M ⟩+D (ρ∥ρ ). (6)
t+1 t t+1 t R t
ρ∈∆(M)
Please see Algorithm 1 for a detailed description of the learning process. We call the resulting
scheme OREPS-OPIX. Analogous to the standard MD and OREPS algorithms, this update can
be tackled efficiently through a well-known two-step procedure [17, 18, 8]. Specifically, by adopting
the result of [8],
ρ t(x,a)eβ(x,a|vˆt,cˆt)
ρ (x,a) = , (7)
t+1 (cid:80) x′∈X,a∈Aρ t(x′,a)eβ(x′,a|vˆt,cˆt)
l
5Algorithm 1 OREPS with Optimistic Predictor and Implicit eXploration (OREPS-OPIX)
Require: Learning rate η, exploration parameter γ
1: Initialize occupancy measure ρ 1(x,a) as a uniform distribution over x ∈ X l and a ∈ A for
l = 1,2,...,L−1
2: Initialize cost predictor as M 1 = 0
3: for Episodes t = 1,2,...,T do
4: Initialize cost estimator as cˆ t = 0
5: for Time steps l = 1,2,...,L−1 do
6: Observe state x l ∈ X l from the environment
7: Choose action a l ∼ ρ t(x l,·)
8: Observe cost c t(x l,a l)
9: Save x l, a l and c t(x l,a l) to u t
10: end for
11: for Tuples x,a,c t(x,a) in u t do
12: cˆ t(x,a) ← (c t(x,a)−M t(x,a))/(ρ t(x,a)+γ)+M t(x,a)
13: Update M t+1(x,a)
14: end for
15: Solve ρ t+1 = argmin ρ∈∆(M)η⟨ρ,cˆ t+M t+1−M t⟩+D R(ρ∥ρ t).
16: end for
where l denotes the layer in which state x belongs to, β is defined as
β(x,a|vˆ,cˆ) = −η(cˆ(x,a)+M (x,a)−M (x,a))
t t t t+1 t
(cid:88)
− vˆ(x′)Pr(x′|x,a)+vˆ(x),
t t
x′∈X
l+1
and vˆ is defined as
t
 
L
(cid:88)  (cid:88) 
vˆ = argmin ln ρ (x,a)eβ(x,a|v,cˆt) .
t t
v  
l=0 x∈X l,a∈A
Note that by setting M = M = 0, one recovers the OREPS algorithm. Further, in the
t t+1
full-information case, one can replace cˆ with the observed cost vector c .
t t
4 Optimistic Regret Bounds
In this section, we provide a detailed regret analysis of the proposed OREPS-OPIX scheme in (6)
equipped with the proposed cost estimator in (5).
Theorem 1 establishes the regret bound under full information. For compactness, we denote
the prediction error in episode t as σ = c −M .
t t t
Theorem 1 (Full information). Under full information feedback, there exists a stepsize η such that
OREPS-OPIX satisfies
(cid:118) 
(cid:117) T
(cid:117) (cid:88)
R T(ρ∗,{c t}T t=1) = O˜ (cid:116)L ∥σ t∥2 ∞. (8)
t=1
To understand the benefit of leveraging cost predictors, assume (cid:80)T ∥c −M ∥2 = O(Tα) for
t=1 t t ∞
some 0 ≤ α ≤ 1 where α = 0 and α = 1 correspond to perfect estimation and futile estimation,
6respectively. Then, if η = O(T−α/2), we have R (ρ∗,{c }T ) = O˜(Tα/2). That is, the regret can
T √ t t=1
be constant while in the worst case, the regret is O˜( T).
AdownsideofTheorem1istherequirementoffullinformationonc ∈ [0,1]|X|×|A| whichisnota
t
realisticassumption. Therefore, wenextestablishaboundontheexpectedregretofOREPS-OPIX
under bandit feedback. As we discussed before, establishing optimistic regret bounds in the bandit
setting for AMDPs seems to necessitate utilizing an estimator with bounded variance. Following
[9], one could impose an assumption that ensures ρ (x,a) > α and establish regret bounds that
t
scales with O(α−1). Instead, we set γ > 0 but impose the mild assumption that the cost predictors
{M }T are optimistic, i.e., M (x,a) ≤ c (x,a).
t t=1 t t
[27] proposed an algorithm that directly estimates a state-action value function instead of a
cost function that is used to exponentially update a policy. They further extended the algorithm to
alternately update policy and value function twice, mirroring the two-step optimization of OMD.
Conceptually,itisanalogoustohavingapredictorasaQ-functionthatisupdatedwiththeprevious
√
episode’scostfunction. Intheworstcase,theirstaticregretbound,whereP = 0,scalesasO( T).
T
[28] investigated ensemble algorithms and imposed a lower bound on the occupancy measure for all
states and actions. This regularization serves to bound the difference between the losses incurred
by any two policies. They also explored optimistic variants by incorporating the two-projection
OMD as originally proposed by [19, 20]. By leveraging this optimistic algorithm, they achieve
(cid:113)
static regret bounds of O˜(L (cid:80)T ∥c −M ∥2 ) in expectation, as opposed to 8. It is worth noting
t t ∞
that both works exclusively explored the full information setting. In the subsequent discussion, we
analyze the bandit feedback setting.
Theorem 2 (Bandit – Expected). Under bandit feedback, there exists a stepsize η and an explo-
ration parameter γ such that OREPS-OPIX utilizing the proposed cost estimator (5) satisfies
E[R (ρ∗,{c }T )]
T t t=1
 (cid:32) T (cid:33)2 3 (9)
= O˜ L1 3 (cid:88) ∥σ t∥2 2+∥σ t∥ 1 .
t=1
Note that the regret bound is optimistic as it scales with the estimation power of the cost
predictors. Further, leveraging cost predictors is beneficial in the bandit feedback setting. In
particular, the result of Theorem 2 demonstrates if (cid:80)T ∥c −P ∥ = O(Tα−1) for some 0 ≤ α ≤ 1
t=1 t t 1
setting η = O(T−2α/3) and γ = O(T−α/3), OREPS-OPIX with the proposed cost estimator suffers
O˜(T2α/3) worst-case expected regret. Therefore, in the best case, the expected regret is constant
while in the worst case, the regret is O˜(T2/3). Note that here our theoretical results may be sub-
√
optimal in the worst-case as we cannot achieve O˜( T) worst-case expected regret. Further study
in this direction is a valuable future work.
√
Also, [29] achieved the dynamic regret bound of O˜(min{ QT,∆1/3T2/3}), where Q and ∆
denote the number and amount of changes in the cost function respectively. This is comparable
to Theorem 2 when Q grows faster than O˜(T1/4). Still, the bound with the change parameter
satisfying∆(t) ≥ max |c (π)−c (π)|ispessimisticwhileourresultscanstillleadanoptimistic
π∈Π t t+1
bound. To see this, consider a predictor designed with the cost suffered in the last episode: i.e.,
M (π ) = c (π ). Then, the optimistic bound becomes σ = |M (π¯)−c (π¯)|, where π¯ is a policy
t+1 t t t t t t
that visits all state-action pair once, and is a special case with the specific choice of the predictor.
Finally, we present our main result, which establishes a high probability sublinear optimistic
regret bound for OREPS-OPIX.
7Theorem 3 (Bandit – High probability). Under bandit feedback, there exists a stepsize η and an
exploration parameter γ such that with probability 1−δ OREPS-OPIX utilizing the proposed cost
estimator (5) satisfies
(cid:118)
(cid:32)(cid:117) T
(cid:117)(cid:88)
R (ρ∗,{c }T ) = O˜ (cid:116) ∥σ ∥2 (10)
T t t=1 t 1
t=1
(cid:16) (cid:17)1 (cid:32) (cid:88)T (cid:33)3 4 (cid:33)
+ Lmax∥σ ∥ 4 ∥σ ∥2 +∥σ ∥ .
t ∞ t ∞ t 1
t
t=1
We point out that the regret is, again, optimistic as it scales with the estimation power of the
cost predictors. Therefore, in the best case, i.e., under perfect estimation, the regret is constant
while in the worst case, the regret is O˜(T3/4), with high probability. Integral to establishing this
result is the development of tailored technical lemmas and a new concentration inequality to ensure
each of the individual terms in the regret remains optimistic. Further study to see the possibility
√
of improving the regret to O˜( T) is left as a future work. [30] studies the AMDP setting and
√
achieves a high probability guarantee with sublinear regret in the order of T using the log-barrier
(cid:18)(cid:113) (cid:19)
methodinsteadofimplicitexploration. However,theirboundO
⟨ρ∗,(cid:80)T
c ⟩ isintermsofthe
t=1 t
loss of the best policy as opposed to being optimistic while our bound O(cid:0) d({c }T ,{M }T )3/4(cid:1)
t t=1 t t=1
diminishes with the estimation power of cost predictors.
Proof highlights. Herewehighlightthekeystepstowardsestablishingourmainresultsstated
in Theorem 3. The regret can be decomposed into
R (ρ∗,{c }T )
T t t=1
T T
(cid:88) (cid:88)
= ⟨ρ −ρ∗,cˆ⟩+ ⟨ρ ,c −E [cˆ]⟩
t t t t t−1 t
(11)
t=1 t=1
T T
(cid:88) (cid:88)
+ ⟨ρ ,E [cˆ]−cˆ⟩+ ⟨ρ∗,cˆ −c ⟩.
t t−1 t t t t
t=1 t=1
Thefirsttermin(11)canbethoughtofastheregretoftheproposedalgorithmwithfullinformation
when the sequence of the cost functions are {cˆ}T . Hence we can use Theorem 1 as well as the
t t=1
result of Lemma 1 to upper bound it with probability one according to
T T
(cid:88) L |X||A| η (cid:88)
⟨ρ −ρ∗,cˆ⟩ ≤ log + ∥σ ∥2 .
t t η L 2γ2 t ∞
t=1 t=1
We then show that the second term can be bound with probability one using the definition of the
proposed estimator (5) and the result of Lemma 1 with
T T
(cid:88) (cid:88)
⟨ρ ,c −E [cˆ]⟩ ≤ γ∥σ ∥ .
t t t−1 t t 1
t=1 t=1
To bound the third term, we show that it is the sum of a martingale difference sequence, hence by
using the Azuma–Hoeffding inequality and a careful computation we can bound it with probability
at least 1−δ with an optimistic term:
(cid:118)
T (cid:117) T
(cid:88) (cid:117) 1 (cid:88)
⟨ρ ,E [cˆ]−cˆ⟩ ≤ (cid:116)2log ∥σ ∥2.
t t−1 t t δ t 1
t=1 t=1
8Algorithm 2 Anytime OREPS-OPIX with Doubling Trick
Require: Initial learning rate η , κ = 2 (expected regret) or κ = 3 (high probability regret)
0
1: Initialize phase number i = 1, starting episode number s 1 = 1, learning rate η 1 = η 0/2 and
optimistic parameter γ = η 1/κ
1 1
2: for Episodes t = 1,2,... do
3: Interact with the environment and suffer the cost to compute Ψ si:t
4: if η i−1D 0 < η i1/κΨ si:t then
5: i ← i+1
6: s i ← t
7: η i ← 2−iη 0
8: γ i ← η i1/κ
9: end if
10: Run the rest of Algorithm 1 to compute cˆ t, M t+1 and ρ t+1 using η i and γ i
11: end for
√
Notably, this term is independent of η and γ and in the worst case scales as O( T).
The last term in (11) requires the development of a new Bernstein-type inequality (See Lemma
2 in the supplementary) to ensure this term can be bounded by an optimistic term. Using this new
result we show that with probability at least 1−δ
T
(cid:88) L L
⟨ρ∗,cˆ −c ⟩ ≤ log max ∥σ ∥ .
t t t ∞
γ δ t=1,...,T
t=1
Finally, optimizing for η and setting γ = η1/3 furnishes the proof of Theorem 3.
5 Extension
5.1 Anytime Optimistic Regret Bounds
In this section, we discuss the extension of OREPS-OPIX to the anytime setting. To obtain the
regret bounds in Section 4, we have to utilize stepsize and exploration parameters that require the
knowledge of typically unknown quantities, e.g., the horizon T. We alleviate this issue by utilizing
the doubling trick technique [31]. Note that compared to typical applications of the doubling trick,
our setting necessitates further efforts. In particular, usually in the doubling trick the learning
is divided into phases that double in length, and accordingly the stepsize is divided in half to
compensate for the growing phase lengths. That is, the condition to decide when a particular
phase ends is apparent. In our setting, similar to [19], this condition is more involved as we
outline next. Additionally, compared to [19], given the more complicated setting of our problem
and the intricate nature of the regret bounds, carrying out the doubling trick technique requires
furtherinnovations, especiallyforthehighprobabilityresults. Asdiscussed, similartothestandard
doubling trick [31, 18, 19], the learning rate η is reduced by half after every phase i instead of a
i
fixed η that depends on T. However, the length of each phase does not necessarily double.
|X||A|
Let us first consider the setting of Theorem 2. Let D = Llog and c¯(x,a) be an unbiased
0 L t
costestimator,i.e.,Equation(5)withγ = 0. AnddefineΨ = (cid:80)τ′ (cid:8) ∥c¯ −M ∥2/2+∥c¯ −M ∥ (cid:9) .
τ:τ′ t=τ t t 2 t t 1
Note that E[Ψ ] = (cid:80)T 1∥c −M ∥2+∥c −M ∥ .
1:T t=1 2 t t 2 t t 1
ThereasontodefineΨinthiswayistouseit(inadditiontoD )todeterminewhentoterminate
0
eachphase(seestep4inAlgorithm2). Therefore,Ψmustonlycontaininformationthatisavailable
9to the learner. Since the optimistic regret bounds, naturally, depend on c which is unknown in the
t
bandit setting, directly utilizing the optimistic regret bound from Theorem 2 is not feasible. This
subtle reason as well as the different feedback model of our setting results in significantly different
anytime algorithms and analyses compared to [19].
The above discussion leads to an anytime extension of OREPS-OPIX which is summarized in
Algorithm 2. This method satisfies the following expected regret bound under bandit feedback,
which is comparable to Theorem 2.
Theorem 4 (Anytime – Bandit – Expected). Under bandit feedback, there exists an initial stepsize
√
η such that Algorithm 2 with the exploration parameter γ = η satisfies
0 i i
E[R (ρ∗,{c }T )]
T t t=1
=
O˜
L1/3(cid:32) (cid:88)T ∥σ t∥2
2 +∥σ t∥
1(cid:33)2/3
.
(12)
2
t=1
In the full information setting, a similar doubling trick can be applied by comparing η−1D and
i 0
η Ψ , where Ψ = (cid:80)τ′ ∥c −M ∥2 /2. Since here c is observed by the learner, we can directly
i si:t τ:τ′ t=τ t t ∞ t
leverage the bound from Theorem 1
Theorem 5 (Anytime – Full information). Under full information feedback, there exists an initial
stepsize η such that Algorithm 2 satisfies
0
(cid:118) 
(cid:117) T
(cid:117) (cid:88)
R T(ρ∗,{c t}T t=1) = O˜ (cid:116)L ∥σ t∥2 ∞. (13)
t=1
5.2 Handling Unknown Transition
In this section, we extend our prior results to the unknown transition setting. This allows the
algorithm the flexibility to be used when the dynamics of MDP is not revealed to the learner.
To model the unknown transition, we construct a confidence set of transition functions using the
counting method as explored by [32, 33, 34, 12]. Specifically, we adopt a tighter confidence set from
[12, Equation 5]:
(cid:26) (cid:12) (cid:12)
P = Pˆ : (cid:12)Pˆ(x′|x,a)−P¯(x′|x,a)(cid:12) ≤ ϵ(x′|x,a),
(cid:12) (cid:12)
(14)
(cid:27)
∀(x,a,x′) ∈ X ×A×X ,k ∈ (0,L−1)
k k+1
where P¯ is the count-based empirical transition probability and the confidence margin ϵ(x′|x,a) is
defined as
(cid:118)
(cid:117) (cid:117)P¯(x′|x,a)log(cid:16) T|X||A|(cid:17) 14log(cid:16) T|X||A|(cid:17)
(cid:116) δ δ
2 +
max{1,N(x,a)−1} 3max{1,N(x,a)−1}
for δ ∈ (0,1) and state-action visit counter N(x,a). And we propose a cost estimator as
cˆ(x,a) (15)
t
c (x,a)−M (x,a)
= t t I{(x,a) ∈ u¯ (t)}+M (x,a),
L t
u (x,a)+γ
t
10where u t(x,a) = max P∈PρP,πt(x,a) is the upper occupancy bound over P and ρP,πt is the occu-
pancy measure under the transition probability P and the induced policy π from ρ as (1). Again,
t t
(15) is an optimistically biased estimator given that the predictor is optimistic and u (x,a) ≥
t
ρ (x,a) by definition. Utilizing this new estimator in OREPS-OPIX, we obtain the following re-
t
sult.
Theorem 6 (Unknown transition – Bandit – High probability). Under bandit feedback with un-
known transition, there exists a stepsize η and an exploration parameter γ such that with probability
at least 1−7δ OREPS-OPIX utilizing the proposed cost estimator (15) satisfies
R (ρ∗,{c }T )
T t t=1
(cid:32) (cid:18) (cid:19)1
1 |X||A| L 4
= O L4 log +log max∥σ t∥
∞
L δ t
(cid:32) T (cid:33)3 4 (cid:118) (cid:117) T (16)
(cid:88) (cid:117)(cid:88)
· ∥σ ∥2 +∥σ ∥ +(cid:116) ∥σ ∥2
t ∞ t 1 t 1
t=1 t=1
(cid:114) (cid:33)
T|X||A|
+L|X| |A|T log .
δ
(cid:18) (cid:113) (cid:19)
T|X||A|
Noticethatinanoptimisticcase,theboundisdominatedbythetermO L|X| |A|T log .
δ
Then the Theorem 6 achieves the same bound as [12] but with higher probability. This term arises
from a judicious application of the Bennet’s concentration inequality [35, Corollary 5] to study how
the error of the estimated occupancy measure ρP,πt with respect to ρ
t
of known transition setting
is bounded within the confidence set (14); it is nontrivial and therefore an interesting direction of
research to see if an optimistic version of this concentration inequality can be established, using,
e.g., the techniques that led to our new Bernstein-type inequality (See Lemma 2 in Appendix A.4).
6 Numerical Experiments
In this section, we perform a simple experiment to demonstrate the benefit of implicit exploration
and cost predictors. 3 We consider a drone navigation task modeled by a 2D grid, where the goal
of the agent is to move by one cell at a time to reach the goal with minimal cost. If a drone enters a
cell with turbulence or wind gust, it incurs higher cost due to higher fuel consumption and possible
damage to the aircraft. The AMDP of the environment is described below:
• State space: X = {(l,A ,A ,G ,G )},
x y x y
where l ∈ {1,...,L} is time step, (A ,A ) is agent location and (G ,G ) is goal location.
x y x y
• Action space: A = {left, right, up, down}
• Cost function:

0, if reaching the goal


c (x,a) = 1, if encountering a turbulence
t

ϵ, otherwise,
3The code for this experiment is accessible at this link: https://github.itap.purdue.edu/moon182/OREPS-
OPIX.git
1125 OREPS (=0) OREPS-OPIX (latest predictor)
OREPS-IX 12 OREPS-OPIX (perfect predictor)
OREPS-OPIX (latest predictor)
OREPS-OPIX (perfect predictor)
20
10
15 8
10 6
5 4
2
0
0
5
0 2000 4000 6000 8000 0 2000 4000 6000 8000
episodes episodes
(a) Average regret and variance of OREPS-OPIX, (b) Errorofcostpredictorsagainstthetruecostfunction.
OREPS, and OREPS-IX.
Fig.1: TheresultofnumericalexperimentofOREPS,OREPS-IXandOREPS-OPIXwithdifferent
predictors plotted versus the number of episodes. Figure 1(a) shows the regret reduction benefit as
well as the variance reduction property of the proposed cost estimator (5). Figure 1(b) shows that
the cost predictors comply with the optimistic prediction assumption.
where0 < ϵ < 1isasmallpositiveconstant. c changeseveryt episodeswhentheoccurrence
t w
of turbulence randomly move to one of its neighbors. It is not observable to the agent but
results in higher cost.
• Bandit feedback: agent observes c (x,a) only for its trajectory (x,a) ∈ u(t) in episode t.
t
• State transition is deterministic:
(cid:40)
1, when (x,a) results in s’
Pr(s′|s,a) =
0, otherwise.
• Wind incurs cost but does not affect state transitions.
• Timeout L is the maximum time steps in an episode.
• When the agent reaches the goal, it remains in that terminal state sl until the end of
terminal
the episode regardless of its action, that is, P(sl+1 |sl ,A) = 1 and X = {sL }
terminal terminal L terminal
is singleton.
The details of the experiment setting are provided in the Appendix.
Figure 1(a) depicts the performance (in terms of cumulative average regret) of OREPS-OPIX
compared with vanilla OREPS and OREPS with implicit exploration. For OREPS-OPIX with
perfect predictor, it is assumed that we have access to a perfect predictor with full information
(M = c , M = c ). A more realistic latest predictor predicts the cost based on the cost
t t t+1 t+1
that the learner suffered in the last visit to the state and the action. It mildly assumes that we
have access to the period t and it resets its value to zero every t episodes to assure optimistic
w w
prediction.
Therearetwonotablepointstothisresult. First, OREPSwithoutimplicitexploration(inblue)
explodesaslearningprogresses. Thishappenswhenthevalueofoccupancymeasureforsomestates
andactionsapproach0: ρ (x,a) → 0. Then, theunbiasedcostestimator, i.e., (4)withγ = 0, which
t
divides cost signal by occupancy measure, grows infinitely large and ρ (x,a) actually becomes 0
t
due to the precision of the floating point. And it remains to be 0 for the remainder of the episode,
12
terger
egareva
)tM
tc(
rorre25 OREPS-OPIX (latest predictor) OREPS-OPIX (latest predictor)
OREPS-OPIX (perfect predictor) 12.5 OREPS-OPIX (perfect predictor)
OREPS-OPIX (latest predictor, more reset) OREPS-OPIX (latest predictor, more reset)
OREPS-OPIX (latest predictor, less reset) 10.0 OREPS-OPIX (latest predictor, less reset)
20
7.5
15 5.0
2.5
10
0.0
5 2.5
5.0
0 2000 4000 6000 8000 0 2000 4000 6000 8000
episodes episodes
(a) Average regret of OREPS-OPIX with different pre- (b) Errorofcostpredictorsagainstthetruecostfunction.
dictors.
Fig.2: TheresultofnumericalexperimentofOREPS-OPIXwithdifferentpredictorsplottedversus
the number of episodes. Figure 2(a) shows that less accurate information about t do not cause
w
significant harm in the performance of OREPS-OPIX. Figure 2(b) shows the consequences on error
when cost predictors are constructed based on inaccurate information about the environment.
because the occupancy measure is updated multiplicatively according to (7). This phenomenon
is consistent with the fact that the naive importance-weighted cost estimator in OREPS which is
based on EXP3 suffers from a high variance. Secondly, OREPS-OPIX (in green and red) improves
both convergence and variance over OREPS-IX (in orange), which is consistent with the result of
Lemma 1 on the reduced variance of the proposed cost estimator (5) while retaining the same bias.
Figure 1(b) demonstrates the error of optimistic cost predictors with respect to the true cost.
By observing the positive values of error, we confirm that the formulation of cost predictors does
not violate the optimistic prediction assumption. Every t = 1000 episodes, the error of the latest
w
predictor spikes, because it periodically resets its value to zero.
InFigure2(a), werelaxtheoptimisticpredictionassumptionwithinaccurateinformationabout
how frequently the cost function changes. The latest predictor with more reset (in purple) and less
reset (in brown) assumes shorter and longer periods of change, respectively, than the true value
of t . However, the results show that the performance degradation is not noticeable compared
w
to the latest predictor with accurate information about the period (in green). In fact, the strict
optimism of the predictor is introduced for mathematical convenience and it is sufficient to hold in
(cid:80)
a (weighted) sum: ω(x,a)(c (x,a)−M (x,a)) ≥ 0 with ω(·) = 1 or ω(·) > 0. Intuitively, what
x,a t t
is more critical is how far the prediction is to the true cost function.
(cid:80)
Figure 2(b) shows the error, i.e., c (x,a)−M (x,a), of different predictors. The latest
x,a t t
predictor with more reset and less reset is built based on incorrect information of the period of
cost change, as tˆ = 500 and tˆ = 2000 respectively. Although Figure 2(a) demonstrates minimal
m m
loss in the performance of OREPS-OPIX when predictor design is based on a flawed information,
Figure 2(b) shows that the predictor error is actually aggravated by the flaws (purple and brown
as opposed to green). It even shows that the latest predictor with less reset (brown) violates
the optimistic prediction assumption when cost function changes without the reset, observed at
t = 1000,3000,...,9000. The result hints at the practical success of our algorithm in the presence
of minor uncertainties in the predictor design.
Finally, Figures 1(b) and 2(b) also exhibits a tendency that the error grows higher over time
as the occupancy measure converges. It is the result of slower convergence of M , which is caused
t
by the reduced entropy of the occupancy measure. Equation (7) updates the occupancy measure
13
terger
egareva
)tM
tc(
rorreby discounting its value exponentially with respect to the loss (estimate) and forces the value of
a state-action pair with relatively high loss (estimate) to approach to zero. From the OREPS
regret plot (blue) in Figure 1(a), the exploding regret is also observed, that is due to the fact
that a state-action pair with near-zero occupancy measure cannot be visited again without implicit
exploration.
7 Conclusion
We studied the problem of establishing optimistic regret bounds for online learning in AMDPs.
Our theoretical analysis demonstrated that such bounds in the bandit feedback setting necessitate
cost estimators with a bounded variance that scales with the estimation power of cost predictors.
To that end, we proposed a new estimator that benefits from variance reduction and proved that
this estimator in conjunction with a variant of mirror descent enjoys optimistic regret bounds in
both full information and bandit feedback settings. Notably, we showed the proposed method and
its anytime extension enjoy high probability sublinear optimistic regrets, a result which crucially
reliedonthecharacteristicsofthenewcostestimatorandthedevelopmentofnewtechnicallemmas
to ensure every term in the regret decomposition can be bounded by optimistic terms. Finally, we
provided an extension to the unknown transition setting and established similar results.
In MDP setting, the cost function remains constant over time and direct optimization of the
costfunctionwithoutboundingtherelativeentropybecomesfeasible. Inthecaseoffullinformation
feedback, the cost function is fully observed after the initial episode, resulting in zero regret from
the second episode onward. In bandit feedback case, we have a bound with diminishing prediction
error c −M , as costs are revealed for additional states and actions. The rate at which the error
t t
reduces and efficient strategies for its reduction present an interesting direction for future research.
References
[1] I.Akkaya,M.Andrychowicz,M.Chociej,M.Litwin,B.McGrew,A.Petron,A.Paino,M.Plap-
pert, G. Powell, R. Ribas, et al., “Solving rubik’s cube with a robot hand,” arXiv preprint
arXiv:1910.07113, 2019.
[2] H. Wei, Y. Wang, L. Mangu, and K. Decker, “Model-based reinforcement learning for predic-
tions and control for limit order books,” NeurIPS 2019 Workshop on Robust AI in Financial
Services: Data, Fairness, Explainability, Trustworthiness, and Privacy, 2019.
[3] H. Buehler, L. Gonon, J. Teichmann, B. Wood, B. Mohan, and J. Kochems, “Deep hedging:
Hedging derivatives under generic market frictions using reinforcement learning. ssrn scholarly
paper id 3355706,” Social Science Research Network, Rochester, NY, 2019.
[4] A. Tsoukalas, T. Albertson, I. Tagkopoulos, et al., “From data to optimal decision making:
a data-driven, probabilistic machine learning approach to decision support for patients with
sepsis,” JMIR medical informatics, vol. 3, no. 1, p. e3445, 2015.
[5] D. Hong, S. Lee, Y. H. Cho, D. Baek, J. Kim, and N. Chang, “Energy-efficient online path
planning of multiple drones using reinforcement learning,” IEEE Transactions on Vehicular
Technology, vol. 70, no. 10, pp. 9725–9740, 2021.
[6] E. Even-Dar, S. M. Kakade, and Y. Mansour, “Online Markov decision processes,” Mathemat-
ics of Operations Research, vol. 34, no. 3, pp. 726–736, 2009.
14[7] J. Y. Yu, S. Mannor, and N. Shimkin, “Markov decision processes with arbitrary reward
processes,” Mathematics of Operations Research, vol. 34, no. 3, pp. 737–757, 2009.
[8] A. Zimin and G. Neu, “Online learning in episodic Markovian decision processes by relative
entropy policy search,” in Proceedings of Advances in neural information processing systems
(NeurIPS), pp. 1583–1591, 2013.
[9] G. Neu, A. Gy¨orgy, and C. Szepesv´ari, “The online loop-free stochastic shortest-path prob-
lem.,” in Proceedings of Conference on Learning Theory (COLT), pp. 231–243, Citeseer, 2010.
[10] G. Neu, A. Gy¨orgy, C. Szepesv´ari, and A. Antos, “Online markov decision processes under
bandit feedback,” in Proceedings of the 23rd International Conference on Neural Information
Processing Systems-Volume 2, pp. 1804–1812, 2010.
[11] G. Neu, A. Gyorgy, C. Szepesvari, and A. Antos, “Online Markov decision processes under
banditfeedback,”IEEETransactionsonAutomaticControl(TAC),vol.3,no.59,pp.676–691,
2014.
[12] C. Jin, T. Jin, H. Luo, S. Sra, and T. Yu, “Learning adversarial Markov decision processes
with bandit feedback and unknown transition,” in Proceedings of International Conference on
Machine Learning (ICML), vol. 119, pp. 4860–4869, 2020.
[13] L.Shani,Y.Efroni,A.Rosenberg,andS.Mannor,“Optimisticpolicyoptimizationwithbandit
feedback,” in Proceedings of the 37th International Conference on Machine Learning (H. D.
III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 8604–8613,
PMLR, 13–18 Jul 2020.
[14] H. Luo, C.-Y. Wei, and C.-W. Lee, “Policy optimization in adversarial mdps: Improved explo-
ration via dilated bonuses,” in Advances in Neural Information Processing Systems (M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 22931–
22942, Curran Associates, Inc., 2021.
[15] B. Vundurthy, A. Kanellopoulos, V. Gupta, and K. G. Vamvoudakis, “Intelligent players in a
fictitious play framework,” IEEE Transactions on Automatic Control, 2023.
[16] G. Neu, A. Gyorgy, and C. Szepesv´ari, “The adversarial stochastic shortest path problem
with unknown transition probabilities,” in Proceedings of Artificial Intelligence and Statistics
(AISTATS), pp. 805–813, 2012.
[17] J. Abernethy and A. Rakhlin, “Beating the adaptive bandit with high probability,” in 2009
Information Theory and Applications Workshop, pp. 280–289, IEEE, 2009.
[18] T. Lattimore and C. Szepesv´ari, “Bandit algorithms,” preprint, 2018.
[19] A. Rakhlin and K. Sridharan, “Online learning with predictable sequences,” in Conference on
Learning Theory, pp. 993–1019, PMLR, 2013.
[20] C.-K.Chiang,T.Yang,C.-J.Lee,M.Mahdavi,C.-J.Lu,R.Jin,andS.Zhu,“Onlineoptimiza-
tion with gradual variations,” in Conference on Learning Theory, pp. 6–1, JMLR Workshop
and Conference Proceedings, 2012.
[21] J. D. Abernethy, E. Hazan, and A. Rakhlin, “Interior-point methods for full-information and
bandit online learning,” IEEE Transactions on Information Theory, vol. 58, no. 7, pp. 4164–
4175, 2012.
15[22] P. Joulani, A. Gy¨orgy, and C. Szepesv´ari, “A modular analysis of adaptive (non-) convex op-
timization: Optimism, composite objectives, and variational bounds,” in International Con-
ference on Algorithmic Learning Theory, pp. 681–720, PMLR, 2017.
[23] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games. Cambridge university press,
2006.
[24] M. Ghasemi, A. Hashemi, U. Topcu, and H. Vikalo, “No-regret learning with high-probability
in adversarial markov decision processes,” in Proceedings of Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 1–7, 2021.
[25] G. Neu, “Explore no more: Improved high-probability regret bounds for non-stochastic
bandits,” in Proceedings of Advances in Neural Information Processing Systems (NeurIPS),
pp. 3168–3176, 2015.
[26] C.-Y. Wei and H. Luo, “More adaptive algorithms for adversarial bandits,” in Proceedings of
the 31st Conference On Learning Theory (S. Bubeck, V. Perchet, and P. Rigollet, eds.), vol. 75
of Proceedings of Machine Learning Research, pp. 1263–1291, PMLR, 06–09 Jul 2018.
[27] Y.Fei,Z.Yang,Z.Wang,andQ.Xie,“Dynamicregretofpolicyoptimizationinnon-stationary
environments,” Advances in Neural Information Processing Systems, vol. 33, pp. 6743–6754,
2020.
[28] P. Zhao, L.-F. Li, and Z.-H. Zhou, “Dynamic regret of online markov decision processes,”
arXiv preprint arXiv:2208.12483, 2022.
[29] C.-Y. Wei and H. Luo, “Non-stationary reinforcement learning without prior knowledge: an
optimal black-box approach,” in Proceedings of Thirty Fourth Conference on Learning Theory
(M. Belkin and S. Kpotufe, eds.), vol. 134 of Proceedings of Machine Learning Research,
pp. 4300–4354, PMLR, 15–19 Aug 2021.
[30] C.-W.Lee, H.Luo, C.-Y.Wei, andM.Zhang, “Biasnomore: high-probabilitydata-dependent
regret bounds for adversarial bandits and mdps,” Advances in neural information processing
systems, vol. 33, pp. 15522–15533, 2020.
[31] L.BessonandE.Kaufmann,“Whatdoublingtrickscanandcan’tdoformulti-armedbandits,”
arXiv preprint arXiv:1803.06971, 2018.
[32] T. Jaksch, R. Ortner, and P. Auer, “Near-optimal regret bounds for reinforcement learning,”
Journal of Machine Learning Research (JMLR), vol. 11, no. Apr, pp. 1563–1600, 2010.
[33] M.G.Azar, I.Osband, andR.Munos, “Minimaxregretboundsforreinforcementlearning,” in
Proceedings of International Conference on Machine Learning (ICML), vol. 70, pp. 263–272,
JMLR. org, 2017.
[34] A. Rosenberg and Y. Mansour, “Online convex optimization in adversarial Markov decision
processes,”inProceedingsofInternationalConferenceonMachineLearning(ICML),pp.5478–
5486, 2019.
[35] A. Maurer and M. Pontil, “Empirical bernstein bounds and sample variance penalization,” in
Conference on Learning Theory, 2009.
16A Proofs
A.1 Proof of Lemma 1
By the definition of cˆ(x,a) as (5),
(cid:20) (cid:21)
c (x,a)−M (x,a)
E [cˆ(x,a)] = E t t I{(x,a) ∈ u¯ (t)}+M (s,a)
t−1 t t−1 L t
ρ (x,a)+γ
t
c (x,a)−M (x,a)
t t
= ρ (x,a)+M (s,a)
t t
ρ (x,a)+γ
t
ρ (x,a)c (x,a)+γM (x,a)
t t t
= .
ρ (x,a)+γ
t
By (5), ρ (·) ≥ 0 and γ ≥ 0,
t
(cid:34) (cid:35)
(cid:18)
c (x,a)−M (x,a)
(cid:19)2
E [(cˆ(x,a)−M (x,a))2] = E t t I{(x,a) ∈ u¯ (t)}
t−1 t t t−1 L
ρ (x,a)+γ
t
(cid:18)
c (x,a)−M
(x,a)(cid:19)2
t t
= ρ (x,a)
t
ρ (x,a)+γ
t
(c (x,a)−M (x,a))2
t t
≤ .
ρ (x,a)+γ
t
■
A.2 Proof of Theorem 1
First, decompose the regret of ρ with respect to ρ∗ as
t
⟨c ,ρ −ρ∗⟩ = ⟨c ,ρ −ρ∗⟩+⟨c ,ρ −ρ ⟩. (17)
t t t t+1 t t t+1
If ρ is the solution of (6) with c instead of cˆ, then for any other ρ∗ ∈ ∆(M), the gradient of
t+1 t
the objective function is negative in the direction of ρ from ρ∗: i.e., ⟨∇ {η⟨ρ,c +M −M ⟩+
t+1 ρ t t+1 t
D (ρ∥ρ )} ,ρ −ρ∗⟩ ≤ 0. Thus,
R t ρ=ρt+1 t+1
⟨η(c +M −M )+∇R(ρ )−∇R(ρ ),ρ −ρ∗⟩ ≤ 0.
t t+1 t t+1 t t+1
The first term of the decomposition (17) is then bounded as
1
⟨c ,ρ −ρ∗⟩ ≤ ⟨∇R(ρ )−∇R(ρ ),ρ −ρ∗⟩+⟨M −M ,ρ −ρ∗⟩.
t t+1 t t+1 t+1 t t+1 t+1
η
By the definition of Bregman divergence: D (ρ∥ρ′) = R(ρ)−R(ρ′)−⟨∇R(ρ′),ρ−ρ′⟩,
R
1
⟨c ,ρ −ρ∗⟩ ≤ {D (ρ∗∥ρ )−D (ρ∗∥ρ )−D (ρ ∥ρ )}+⟨M −M ,ρ −ρ∗⟩
t t+1 R t R t+1 R t+1 t t t+1 t+1
η
1
= {D (ρ∗∥ρ )−D (ρ∗∥ρ )−D (ρ ∥ρ )}+⟨M −M ,ρ∗−ρ ⟩
R t R t+1 R t+1 t t+1 t t+1
η
1
= {D (ρ∗∥ρ )−D (ρ∗∥ρ )−D (ρ ∥ρ )}
R t R t+1 R t+1 t
η
+⟨M −M ,ρ∗−ρ ⟩+⟨M −M ,ρ −ρ ⟩.
t+1 t t t+1 t t t+1
17Plugging the result back to (17),
1
⟨c ,ρ −ρ∗⟩ ≤ {D (ρ∗∥ρ )−D (ρ∗∥ρ )−D (ρ ∥ρ )}
t t R t R t+1 R t+1 t
η
+⟨M −M ,ρ∗−ρ ⟩+⟨M ,ρ −ρ ⟩+⟨c −M ,ρ −ρ ⟩
t+1 t t t+1 t t+1 t t t t+1
1
= {D(ρ∗∥ρ )−D(ρ∗∥ρ )−D(ρ ∥ρ )}
t t+1 t+1 t
η (18)
−⟨M ,ρ∗−ρ ⟩+⟨M ,ρ∗−ρ ⟩+⟨c −M ,ρ −ρ ⟩.
t t t+1 t+1 t t t t+1
By Holder’s and Young’s inequalities,
η 1
⟨c −M ,ρ −ρ ⟩ ≤ ∥c −M ∥2 + ∥ρ −ρ ∥2.
t t t t+1 2 t t ∞ 2η t t+1 1
Since negative entropy is 1-strongly convex with respect to L norm,
1
1
∥ρ −ρ ∥2 ≤ R(ρ )−R(ρ )−⟨∇R(ρ ),ρ −ρ ⟩ = D (ρ ∥ρ ).
2 t t+1 1 t+1 t t t+1 t R t+1 t
Plugging the result back to (18),
1
⟨c ,ρ −ρ∗⟩ ≤ {D(ρ∗∥ρ )−D(ρ∗∥ρ )−D(ρ ∥ρ )}
t t t t+1 t+1 t
η
η 1
−⟨M ,ρ∗−ρ ⟩+⟨M ,ρ∗−ρ ⟩+ ∥c −M ∥2 + D(ρ ∥ρ )
t t t+1 t+1 2 t t ∞ η t+1 t
1
= {D(ρ∗∥ρ )−D(ρ∗∥ρ )}−⟨M ,ρ∗−ρ ⟩+⟨M ,ρ∗−ρ ⟩
t t+1 t t t+1 t+1
η
η
+ ∥c −M ∥2 .
2 t t ∞
By summing over T episodes,
T
(cid:88)
R (ρ∗,{c }T ) = ⟨c ,ρ −ρ∗⟩
T t t=1 t t
t=1
1
≤ {D(ρ∗∥ρ )−D(ρ∗∥ρ )}−⟨M ,ρ∗−ρ ⟩+⟨M ,ρ∗−ρ ⟩
1 T+1 1 1 T+1 T+1
η
T
(cid:88) η
+ ∥c −M ∥2 .
2 t t ∞
t=1
Without losing generality, we can set M = M = 0. And by the non-negativity and definition
1 T+1
of Bregman divergence,
T
1 (cid:88) η
R (ρ∗,{c }T ) ≤ D(ρ∗∥ρ )+ ∥c −M ∥2
T t t=1 η 1 2 t t ∞
t=1
T
1 (cid:88) η
= {R(ρ∗)−R(ρ )−⟨∇R(ρ ),ρ∗−ρ ⟩}+ ∥c −M ∥2 .
η 1 1 1 2 t t ∞
t=1
18SincenegativeentropyR(·) ≤ 0andρ isinitializedasauniformdistribution,ofwhich∇R(ρ ) = 0,
1 1
T
1 (cid:88) η
R (ρ∗,{c }T ) ≤ − R(ρ )+ ∥c −M ∥2
T t t=1 η 1 2 t t ∞
t=1
L−1 T
1 (cid:88) (cid:88) (cid:88) (cid:88) η
= (−ρ (x,a)log(ρ (x,a))+ ∥c −M ∥2
η 1 1 2 t t ∞
k=0x∈X
k
a t=1
L−1 T
1 (cid:88) (cid:88) (cid:88) 1 (cid:88) η
= log|X ||A|+ ∥c −M ∥2
η |X ||A| k 2 t t ∞
k
k=0x∈X
k
a t=1
L−1 T
1 (cid:88) (cid:88) η
= log|X ||A|+ ∥c −M ∥2
η k 2 t t ∞
k=0 t=1
T
L |X||A| (cid:88) η
= log + ∥c −M ∥2 .
η L 2 t t ∞
t=1
(cid:113)
If η = 2L log |X||A| ,
(cid:80)∥ct−Mt∥2
∞
L
(cid:118)
(cid:117) T
(cid:117) |X||A| (cid:88)
R (ρ∗,{c }T ) ≤ (cid:116)2Llog ∥c −M ∥2 .
T t t=1 L t t ∞
t=1
■
A.3 Proof of Theorem 2
The expected total regret with respect to ρ∗ ∈ ∆(M) can be decomposed into
(cid:34) T (cid:35) (cid:34) T (cid:35) (cid:34) T (cid:35) (cid:34) T (cid:35)
(cid:88) (cid:88) (cid:88) (cid:88)
E ⟨ρ −ρ∗,c ⟩ = E ⟨ρ −ρ∗,cˆ⟩ +E ⟨ρ ,c −cˆ⟩ −E ⟨ρ∗,c −cˆ⟩ . (19)
t t t t t t t t t
t=1 t=1 t=1 t=1
For the first term in (19), follow the same proof as Appendix A.2 with c ← cˆ. Let ρ be the
t t t+1
solution of (6) and decompose the term as
⟨ρ −ρ∗,cˆ⟩ = ⟨ρ −ρ∗,cˆ⟩+⟨ρ −ρ ,cˆ⟩.
t t t+1 t t t+1 t
Forallρ∗ ∈ ∆(M),thegradientof (6)isnegativeinthedirectionofρ fromρ∗: i.e.,⟨∇ {η⟨ρ,cˆ +
t+1 ρ t
M −M ⟩+D(ρ∥ρ )} ,ρ −ρ∗⟩ ≤ 0. Thus, following Appendix A.2,
t+1 t t ρ=ρ˜t+1 t+1
1
⟨ρ −ρ∗,cˆ⟩ ≤ ⟨ρ −ρ∗,∇R(ρ )−∇R(ρ )⟩+⟨ρ −ρ∗,M −M ⟩+⟨ρ −ρ ,cˆ⟩
t t t+1 t t+1 t+1 t t+1 t t+1 t
η
1 η
≤ {D(ρ∗∥ρ )−D(ρ∗∥ρ )}−⟨M ,ρ∗−ρ ⟩+⟨M ,ρ∗−ρ ⟩+ ∥cˆ −M ∥2 .
η t t+1 t t t+1 t+1 2 t t ∞
Adding over t episodes, following Appendix A.2 again,
T T
(cid:88) L |X||A| (cid:88) η
⟨ρ −ρ∗,cˆ⟩ ≤ log + ∥cˆ −M ∥2 .
t t η L 2 t t ∞
t=1 t=1
19Taking expectation over the randomness associated with u(T),
(cid:34) T (cid:35) (cid:34) T (cid:35)
(cid:88) L |X||A| (cid:88) η
E ⟨ρ −ρ∗,cˆ⟩ ≤ E log + ∥cˆ −M ∥2
t t η L 2 t t ∞
t=1 t=1
T
= L log |X||A| + η (cid:88) E(cid:2) ∥cˆ −M ∥2 (cid:3) .
η L 2 t t ∞
t=1
By tower expectation,
(cid:34) T (cid:35) T
E (cid:88) ⟨ρ −ρ∗,cˆ⟩ ≤ L log |X||A| + η (cid:88) E(cid:2)E [∥cˆ −M ∥2 ](cid:3) .
t t η L 2 t−1 t t ∞
t=1 t=1
By ∥·∥2 ≤ ∥·∥2 and Lemma 1,
∞ 2
(cid:34) T (cid:35) T
E (cid:88) ⟨ρ −ρ∗,cˆ⟩ ≤ L log |X||A| + η (cid:88) E(cid:2)E [∥cˆ −M ∥2](cid:3)
t t η L 2 t−1 t t 2
t=1 t=1
T (cid:34) (cid:35)
L |X||A| η (cid:88) (cid:88)
= log + E E [(cˆ(x,a)−M (x,a))2]
t−1 t t
η L 2
t=1 x,a
≤
L
log
|X||A|
+
η
(cid:88)T E(cid:34)
(cid:88) (c t(x,a)−M
t(x,a))2(cid:35)
.
η L 2 ρ (x,a)+γ
t
t=1 x,a
Since ρ (·) ≥ 0 and γ > 0,
t
(cid:34) T (cid:35) T
(cid:88) L |X||A| η (cid:88)
E ⟨ρ −ρ∗,cˆ⟩ ≤ log + ∥c −M ∥2. (20)
t t η L 2γ t t 2
t=1 t=1
The second term in (19) can be decomposed into
⟨ρ ,c −cˆ⟩ = ⟨ρ ,c −E [cˆ]⟩+⟨ρ ,E [cˆ]−cˆ⟩.
t t t t t t−1 t t t−1 t t
By Lemma 1,
(cid:26) (cid:27)
⟨ρ ,c −E [cˆ]⟩ =
(cid:88)
ρ (x,a) c (x,a)−
ρ t(x,a)c t(x,a)+γM t(x,a)
t t t−1 t t t
ρ (x,a)+γ
t
x,a
(cid:88) γc t(x,a)−γM t(x,a)
= ρ (x,a) .
t
ρ (x,a)+γ
t
x,a
Since ρ (·) ≥ 0, γ > 0 and M (x,a) ≤ c (x,a) for all x,a,
t t t
(cid:88)
⟨ρ ,c −E [cˆ]⟩ ≤ γ|c (x,a)−M (x,a)| = γ∥c −M ∥ .
t t t−1 t t t t t 1
x,a
Adding over T episodes and taking expectation with respect to the randomness associated with
u(T),
(cid:34) T (cid:35) T
(cid:88) (cid:88)
E ⟨ρ ,c −E [cˆ]⟩ ≤ γ∥c −M ∥ .
t t t−1 t t t 1
t=1 t=1
20Also, since {E [cˆ]−cˆ} is a Martingale difference sequence (MDS),
t−1 t t t
(cid:34) T (cid:35)
(cid:88)
E ⟨ρ ,E [cˆ]−cˆ⟩ = 0.
t t−1 t t
t=1
Thus, the second term in (19) is bounded as
(cid:34) T (cid:35) T
(cid:88) (cid:88)
E ⟨ρ ,c −cˆ⟩ ≤ γ ∥c −M ∥ . (21)
t t t t t 1
t=1 t=1
Finally, since ρ∗ is constant with respect to t and the randomness associated with u(T),
(cid:34) T (cid:35) (cid:34) T (cid:35)
(cid:88) (cid:88)
E ⟨ρ∗,c −cˆ⟩ = ⟨ρ∗,E c −cˆ ⟩.
t t t t
t=1 t=1
By tower expectation and Lemma 1,
(cid:34) T (cid:35) (cid:34) T (cid:35)
(cid:88) (cid:88)
E ⟨ρ∗,c −cˆ⟩ = ⟨ρ∗,E c −E [cˆ] ⟩
t t t t−1 t
t=1 t=1
(cid:34) T (cid:35)
=
(cid:88)
ρ∗(x,a)E
(cid:88)
c (x,a)−
ρ t(x,a)c t(x,a)+γM t(x,a)
t
ρ (x,a)+γ
t
x,a t=1
(cid:34) T (cid:35)
=
(cid:88)
ρ∗(x,a)E
(cid:88) γ(c t−M t(x,a))
ρ (x,a)+γ
t
x,a t=1
Since ρ (·),ρ∗(·) ≥ 0, γ > 0 and M (x,a) ≤ c (x,a) for all x,a,
t t t
(cid:34) T (cid:35)
(cid:88)
E ⟨ρ∗,c −cˆ⟩ ≥ 0 (22)
t t
t=1
Applying (20), (21) and Equation (22) to Equation (19):
(cid:34) T (cid:35) T T
(cid:88) L |X||A| η (cid:88) (cid:88)
E ⟨ρ −ρ∗,c ⟩ ≤ log + ∥c −M ∥2+γ ∥c −M ∥
t t η L 2γ t t 2 t t 1
t=1 t=1 t=1
(cid:16) (cid:17)2/3 √
If η = L log |X||A| and γ = η,
(cid:80)1 2∥ct−Mt∥2 2+∥ct−Mt∥1 L
(cid:18) |X||A|(cid:19)1/3(cid:18)
(cid:88) 1
(cid:19)2/3
E[R (ρ∗,{c }T )] ≤ Llog ∥c −M ∥2+∥c −M ∥
T t t=1 L 2 t t 2 t t 1
■
A.4 Proof of Theorem 3
The total regret can be decomposed as (11). The first term in (11) can be thought of as the regret
of the the proposed algorithm with full information when the sequence of the cost functions are
{cˆ}T . By Theorem 1,
t t=1
T T
(cid:88) L |X||A| η (cid:88)
⟨ρ −ρ∗,cˆ⟩ ≤ log + ∥cˆ −M ∥2 .
t t η L 2 t t ∞
t=1 t=1
21By (5),
(cid:18)
c (x,a)−M (x,a)
(cid:19)2
∥cˆ −M ∥2 = max t t I{(x,a) ∈ u¯ (t)}
t t ∞ x,a ρ t(x,a)+γ L
(cid:18)
c (x,a)−M
(x,a)(cid:19)2
t t
= max
(x,a)∈u¯ L(t) ρ t(x,a)+γ
(cid:18)
c (x,a)−M
(x,a)(cid:19)2
t t
≤ max .
x,a ρ t(x,a)+γ
By ρ (·) ≥ 0 and γ ≥ 0,
t
max (c (x,a)−M (x,a))2
∥cˆ −M ∥2 ≤ x,a t t
t t ∞ γ2
∥c −M ∥2
= t t ∞
γ2
Thus, the first term is bounded with probability one as
T T
(cid:88) L |X||A| η (cid:88)
⟨ρ −ρ∗,cˆ⟩ ≤ log + ∥c −M ∥2 . (23)
t t η L 2γ2 t t ∞
t=1 t=1
Using Lemma 1, the second term is rewritten as
T T (cid:18) (cid:19)
(cid:88)
⟨ρ ,c −E [cˆ]⟩ =
(cid:88)(cid:88)
ρ (x,a) c (x,a)−
ρ t(x,a)c t(x,a)+γM t(x,a)
t t t−1 t t t
ρ (x,a)+γ
t
t=1 t=1 x,a
T (cid:18) (cid:19)
(cid:88)(cid:88) γc t(x,a)−γM t(x,a)
= ρ (x,a)
t
ρ (x,a)+γ
t
t=1 x,a
By M (x,a) ≤ c (x,a), ρ (·) ≥ 0 and γ ≥ 0, it is bounded with probability one with
t t t
T T
(cid:88) (cid:88)(cid:88)
⟨ρ ,c −E [cˆ]⟩ ≤ γc (x,a)−γM (x,a)
t t t−1 t t t
t=1 t=1 x,a
T
(cid:88)
= γ ∥c −M ∥ . (24)
t t 1
t=1
By Lemma 1 and (5),
ρ (x,a)c (x,a)+γM (x,a)
E [cˆ(x,a)]−cˆ(x,a) = t t t
t−1 t t
ρ (x,a)+γ
t
(cid:18) (cid:19)
c (x,a)−M (x,a)
− t t I{(x,a) ∈ u¯ (t)}+M (x,a)
L t
ρ (x,a)+γ
t
(ρ (x,a)−I{(x,a) ∈ u¯ (t)})(c (x,a)−M (x,a))
t L t t
= .
ρ (x,a)+γ
t
By M (x,a) ≤ c (x,a), ρ (·) ≥ 0 and γ ≥ 0,
t t t
E [cˆ(x,a)]−cˆ(x,a) ≤ |c (x,a)−M (x,a)|
t−1 t t t t
22Thus,
T T
(cid:88) (cid:88)
⟨ρ ,E [cˆ]−cˆ⟩ ≤ ⟨ρ ,∥c −M ∥ ⟩.
t t−1 t t t t t 1
t=1 t=1
Since {⟨ρ ,E [cˆ]−cˆ⟩}T is a martingale difference sequence, by using the Azuma–Hoeffding
t t−1 t t t=1
inequality,
(cid:32) (cid:88)T (cid:33) (cid:32) −ϵ2 (cid:33)
Pr ⟨ρ ,E [cˆ]−cˆ⟩ ≥ ϵ ≤ exp = δ.
t t−1 t t 2(cid:80)T ∥c −M ∥2
t=1 t=1 t t 1
Therefore, with probability at least 1−δ, the third term is bounded with
(cid:118)
T (cid:117) T
(cid:88) (cid:117) 1 (cid:88)
⟨ρ ,E [cˆ]−cˆ⟩ ≤ (cid:116)2log ∥c −M ∥2. (25)
t t−1 t t δ t t 1
t=1 t=1
Lemma 2. Let {X }T be an F-adapted sequence with the Filtration F = (F ) . Define E [·] =
t t=1 t t t
E[·|F]. Let {η }T be an F-predictable sequence. Then, if η ≥ 0 and η (X −E [X ]) ≤ 1.79, we
t t=1 t t t t−1 t
have
(cid:32) T T (cid:33)
(cid:88) (cid:88) 1
Pr η (X −µ ) ≥ η2E [X2]+log ≤ δ. (26)
t t t t t−1 t δ
t=1 t=1
Proof. Let α = E [η (X −E [X ])2] = η E [(X −E [X ])2] and µ = E [X ]. Since
t t−1 t t t−1 t t t−1 t t−1 t t t−1 t
η is F-predictable, by Markov inequality,
t
(cid:32) T (cid:33) (cid:32) (cid:32) T (cid:33) (cid:33)
(cid:88) 1 (cid:88) 1
Pr η (X −µ −α ) ≥ log = Pr exp η (X −µ −α ) ≥
t t t t t t t t
δ δ
t=1 t=1
(cid:34) (cid:32) T (cid:33)(cid:35)
(cid:88)
≤ δE exp η (X −µ −α ) . (27)
t t t t
t=1
Let Z =
exp((cid:80)n
η (X −µ −α )) and y = exp(η (X −µ −α )). Since Z =
n t=1 t t t t n+1 n+1 n+1 n+1 n+1 n+1
Z y and Z is F-adapted,
n n+1 n
E[Z |F ] = E[Z y |F ]
n+1 n n n+1 n
= Z E[y |F ].
n n+1 n
By the fact that η α is F-predictable, exp(x) ≤ 1+x+x2 for x < 1.79 and 1+x ≤ exp(x),
n n
E [y ] = exp(−η α )E [exp(η (X −µ ))]
n−1 n n n n−1 n n n
≤ exp(−η α )E [1+(η (X −µ ))+(η (X −µ ))2]
n n n−1 n n n n n n
= exp(−η α )(1+η E [X ]−η µ +η2E [(X −µ )2])
n n n n−1 n n n n n−1 n n
= exp(−η α )(1+η2E [(X −µ )2])
n n n n−1 n n
(By µ = E [X ])
n n−1 n
≤ exp(−η α )exp(η2E [(X −µ )2])
n n n n−1 n n
= exp(−η2E [(X −µ )2])exp(η2E [(X −µ )2]) = 1
n n−1 n n n n−1 n n
(By α = η E [(X −µ )2].
n n n−1 n n
23Therefore Z is a supermartingale: i.e.
n
E [Z ] = Z E [y ] ≤ Z .
n n+1 n n n+1 n
By tower expectation,
E[Z ] = E[E [Z ]] ≤ E[Z ] ≤ ... ≤ E[Z ] = E[y ] ≤ 1.
n n−1 n n−1 1 1
Apply this result back to (27),
(cid:32) T (cid:33)
(cid:88) 1
Pr η (X −µ −α ) ≥ log ≤ δ
t t t t
δ
t=1
Since α = η E [(X −µ )2] and E [(X −µ )2] ≤ E [X2],
t t t−1 t t t−1 t t t−1 t
(cid:32) T T (cid:33)
(cid:88) 1 (cid:88)
Pr η (X −µ ) ≥ log + η2E [(X −µ )2] ≤ δ
t t t δ t t−1 t t
t=1 t=1
(cid:32) T T (cid:33)
(cid:88) 1 (cid:88)
Pr η (X −µ ) ≥ log + η2E [X2] ≤ δ
t t t δ t t−1 t
t=1 t=1
■
(cid:80)
To use Lemma 2 for the last term in (11), let X = ρ(x,a)[cˆ(x,a)−M (x,a)] and
t x∈X,a∈A t t
η = η = γ , where ∥c−M∥ = max max l |c (x,a)−M (x,a)|. Then, by (5),
t ∥c−M∥Xl X l t=1,...,T x∈X l,a∈A t t
 
(cid:88)
µ t = E t−1 ρ(x,a)[cˆ t(x,a)−M t(x,a)]
x∈X,a∈A
l
 
= E t−1
(cid:88) ρ(x,a)c t(x,a)−M t(x,a)
I{(x,a) ∈ u¯ L(t)}
ρ (x,a)+γ
t
x∈X,a∈A
l
(cid:88) ρ t(x,a)[c t(x,a)−M t(x,a)]
= ρ(x,a)
ρ (x,a)+γ
t
x∈X,a∈A
l
By Lemma 2, with probability 1−δ′,
 
T (cid:20) (cid:21)
(cid:88) γ (cid:88) ρ t(x,a)[c t(x,a)−M t(x,a)]
 ρ(x,a) cˆ t(x,a)−M t(x,a)− 
∥c−M∥ ρ (x,a)+γ
X t
t=1 l x∈X,a∈A
l
 2
1
(cid:88)T (cid:18)
γ
(cid:19)2
(cid:88)
≤ log
δ′
+
∥c−M∥
E t−1 ρ(x,a)[cˆ t(x,a)−M t(x,a)] 
X
t=1 l x∈X,a∈A
l
By (5),
 
T
(cid:88) γ

(cid:88) ρ(x,a)c t(x,a)−M t(x,a)
[I{(x,a) ∈ u¯ L(t)}−ρ t(x,a)]
∥c−M∥ ρ (x,a)+γ
X t
t=1 l x∈X,a∈A
l
1
(cid:88)T (cid:18)
γ
(cid:19)2
(cid:88)
(cid:18)
c t(x,a)−M
t(x,a)(cid:19)2
≤ log + ρ(x,a) ρ (x,a)
δ′ ∥c−M∥ ρ (x,a)+γ t
X t
t=1 l x∈X,a∈A
l
24Since ρ (·) ≥ 0 and γ > 0,
t
T
γ (cid:88) (cid:88) ρ(x,a)c t(x,a)−M t(x,a)
[I{(x,a) ∈ u¯ (t)}−ρ (x,a)]
L t
∥c−M∥ ρ (x,a)+γ
X t
l t=1x∈X,a∈A
l
≤ log
1
+
γ
(cid:88)T
(cid:88)
(cid:18)
ρ(x,a)2[c t(x,a)−M
t(x,a)]2(cid:19)
γ
δ′ ∥c−M∥ ρ (x,a)+γ ∥c−M∥
X t X
l t=1x∈X,a∈A l
l
T
γ (cid:88) (cid:88) c t(x,a)−M t(x,a)
ρ(x,a)
∥c−M∥ ρ (x,a)+γ
X t
l t=1x∈X,a∈A
l
(cid:20) (cid:21)
γρ(x,a)[c (x,a)−M (x,a)]
I{(x,a) ∈ u¯ (t)}−ρ (x,a)− t t
L t
∥c−M∥
X
l
1
≤ log
δ′
Since ρ(x,a),ρ (x,a) ≥ 0, γ > 0 and M (x,a) ≤ c (x,a), by ρ(x,a) ≤ 1 and c (x,a)−M (x,a) ≤
t t t t t
∥c−M∥ ,
X
l
T
γ (cid:88) (cid:88) ρ(x,a)c t(x,a)−M t(x,a)
[I{(x,a) ∈ u¯ (t)}−ρ (x,a)−γ] ≤ log
1
∥c−M∥ ρ (x,a)+γ L t δ′
X t
l t=1x∈X,a∈A
l
By (5),
T
γ (cid:88) (cid:88) 1
ρ(x,a)[cˆ(x,a)−c (x,a)] ≤ log
t t
∥c−M∥ δ
X
l t=1x∈X,a∈A
l
For each layer l, with probability 1−δ′,
T
(cid:88) (cid:88) ∥c−M∥ X 1
ρ(x,a)[cˆ(x,a)−c (x,a)] ≤ l log
t t γ δ′
t=1x∈X,a∈A
l
By union bound on all layers, with probability 1−Lδ′,
T L
(cid:88)(cid:88) (cid:88) ∥c−M∥ X 1
ρ(x,a)[cˆ(x,a)−c (x,a)] ≤ l log
t t γ δ′
t=1 x,a l=1
L 1
≤ log max ∥c−M∥
γ δ′ l=1,...,L X l
L 1
= log max ∥c −M ∥
γ δ′ t=1,...,T t t ∞
Setting δ′ = δ/L, with probability 1−δ, the last term in (11) is bounded as
T
(cid:88) L L
⟨ρ∗,cˆ −c ⟩ ≤ log max ∥c −M ∥ . (28)
t t t t ∞
γ δ t=1,...,T
t=1
25Applying (23), (24), (25) and (28) back to (11),
T T
L |X||A| η (cid:88) (cid:88)
R (ρ∗,{c }T ) ≤ log + ∥c −M ∥2 +γ ∥c −M ∥
T t t=1 η L 2γ2 t t ∞ t t 1
t=1 t=1
(cid:118)
(cid:117) T
(cid:117) 1 (cid:88) L L
+(cid:116)2log ∥c −M ∥2+ log max ∥c −M ∥
δ t t 1 γ δ t=1,...,T t t ∞
t=1
T T
L |X||A| η (cid:88) (cid:88)
≤ log + ∥c −M ∥2 +γ ∥c −M ∥
η L 2γ2 t t ∞ t t 1
t=1 t=1
(cid:118)
(cid:117) T
(cid:117) 1 (cid:88) L L
+(cid:116)2log ∥c −M ∥2+ log max ∥c −M ∥
δ t t 1 η δ t=1,...,T t t ∞
t=1
(Since η ≤ γ if γ = η1/3 and η,γ ≤ 1)
(cid:18) (cid:19)3/4
Let η = Llog|X L||A|+LlogL δ maxt∥ct−Mt∥∞ and γ = η1/3.
(cid:80)T
t=1
∥ct−M 2t∥2 ∞+∥ct−Mt∥1
(cid:18)
|X||A| L
(cid:19)1/4
R (ρ∗,{c }T ) ≤ Llog +Llog max∥c −M ∥
T t t=1 L δ t t t ∞
×(cid:32) (cid:88)T ∥c t−M t∥2
∞ +∥c −M ∥
(cid:33)3/4
t t 1
2
t=1
(cid:118)
(cid:117) T
(cid:117) 1 (cid:88)
+(cid:116)2log ∥c −M ∥2
δ t t 1
t=1
■
A.5 Proof of Theorem 4
Theexpectedtotalregretcanbedecomposedintolocalregretsofeachphaseasbelow,whereN > 1
denotes the number of phases that T episodes are broken into.
 
(cid:34) T (cid:35) N si+1−1
(cid:88) (cid:88) (cid:88)
E ⟨ρ t−ρ∗,c t⟩ = E  ⟨ρ t−ρ∗,c t⟩
t=1 i=1 t=si
Since the randomness of the future does not affect the past,
 
(cid:34) T (cid:35) N si+1−1 si+1−1
(cid:88) (cid:88) (cid:88) (cid:88)
E ⟨ρ t−ρ∗,c t⟩ = E u(si+1−1) ⟨ρ t,c t⟩− ⟨ρ∗,c t⟩
t=1 i=1 t=si t=si
 
N si+1−1 si+1−1
(cid:88) (cid:88) (cid:88)
≤ E u(si+1−1) ⟨ρ t,c t⟩− ρ∈m ∆i (n
M)
⟨ρ,c t⟩. (29)
i=1 t=si t=si
Nowwecanconsidersolvingfortheupperboundastheproblemofeachphaseindependently,where
thelocalregretofeachphaseisexpressedwithrespecttoitslocaloptimum: ρ∗ = argmin (cid:80)si+1−1 ⟨ρ,cˆ⟩.
i ρ∈∆(M) t=si t
26If ρ is the solution of (6) with η = η and t = s ,...,s −1, by Theorem 2,
t+1 i i i+1
 
E
si (cid:88)+1−1
⟨ρ t−ρ∗ i,c t⟩ ≤
ηL
log
|X L||A|
+
2η
γi
si (cid:88)+1−1
∥c t−M t∥2
2+si (cid:88)+1−1
γ i∥c t−M t∥ 1.
i i
t=si t=si t=si
Note that the term L log |X||A| represents the initial suboptimality assuming that ρ is initial-
ηi L si
ized as the uniform distribution. However, the logic behind regularizing the Bregman divergence
(between the current and past occupancy measures) is that the occupancy measure learned in an
episode will suffer a lower cost in the next episode than random initialization. Therefore the bound
conservatively holds for ρ learned from the previous phase instead of initializing it every phase.
si √
By Algorithm 2, use γ = η .
i i
 
E
si (cid:88)+1−1
⟨ρ t−ρ∗ i,c t⟩ ≤
ηL
log
|X L||A|
+
2η
γi
si (cid:88)+1−1
∥c t−M t∥2
2+si (cid:88)+1−1
γ i∥c t−M t∥
1
i i
t=si t=si t=si
L |X||A| √
si (cid:88)+1−1(cid:26)
1
(cid:27)
= log + η ∥c −M ∥2+∥c −M ∥ .
η L i 2 t t 2 t t 1
i
t=si
SinceE[|c¯(x,a)−M (x,a)|] = |c (x,a)−M (x,a)|withrespecttotherandomnessofthetrajectory
t t t t
u¯ (t),
L
   
si (cid:88)+1−1
L |X||A| √
si (cid:88)+1−1(cid:26)
1
(cid:27)
E  ⟨ρ t−ρ∗ i,c t⟩ ≤
η
log
L
+ η iE 
2
∥c¯ t−M t∥2 2+∥c¯ t−M t∥ 1 .
i
t=si t=si
√
By step 4 of Algorithm 2, η−1D ≥ η Ψ for all i = 1,...,N.
i 0 i si:si+1−1
 
si (cid:88)+1−1
2L |X||A|
E  ⟨ρ t−ρ∗ i,c t⟩ ≤
η
log
L
i
t=si
Adding over N phases, by Equation (29),
(cid:34) T (cid:35) (cid:32) N (cid:33) (cid:32) N (cid:33)
(cid:88) |X||A| (cid:88) 1 (cid:88) 1
E ⟨ρ −ρ∗,c ⟩ = Llog 2 = D 2 .
t t 0
L η η
i i
t=1 i=1 i=1
From (cid:80)N (1/2)i ≤ 1 and η = η /2N−1,
i=1 N−1 0
(cid:88)N 1 2 (cid:88)N 2 (cid:88)N 2N+2 (cid:88)N 1 2N+2 8
2 = 2i = 2N+1 2i−N−1 = ( )i ≤ = . (30)
η η η η 2 η η
i 0 0 0 0 N−1
i=1 i=1 i=1 i=1
√ √
By η Ψ > η−1 D ≥ η Ψ and the monotonicity of Ψ,
N−1 sN−1:sN N−1 0 N−1 sN−1:sN−1
Ψ Ψ
η−3/2
<
sN−1:sN
≤
1:T
.
N−1 D D
0 0
27Then,
8
E[R (ρ∗,{c }T )] ≤ D
T t t=1 0 η
N−1
(cid:18)
Ψ
(cid:19)2/3
1:T
< 8D
0
D
0
1/3 2/3
= 8D Ψ
0 1:T
(cid:18) |X||A|(cid:19)1/3(cid:32) (cid:88)T
1
(cid:33)2/3
= 8 Llog ∥c¯ −M ∥2+∥c¯ −M ∥ .
L 2 t t 2 t t 1
t=1
Taking expectation over the randomness of the trajectory u¯ (T),
L
(cid:18) |X||A|(cid:19)1/3(cid:32) (cid:88)T
1
(cid:33)2/3
E[R (ρ∗,{c }T )] ≤ 8 Llog ∥c −M ∥2+∥c −M ∥ .
T t t=1 L 2 t t 2 t t 1
t=1
√
Note that determining η−1D < η Ψ for each episode t does not require additional suffering
i 0 i si:t
of cost. As in Algorithm 2, determining η and γ can come after the entire rollout of episode t,
i i
as they are only needed for computing cˆ, M and ρ . Therefore this is the final bound unlike
t t+1 t+1
Lemma 12 of [19], which suffers additional cost for finding Ψ. ■
A.6 Proof of Theorem 5
The proof is similar to Appendix A.5 but simpler. Again, the total regret can be decomposed into
local regrets of each phase.
T N si+1−1
(cid:88) (cid:88) (cid:88)
⟨c ,ρ −ρ∗⟩ = ⟨ρ −ρ∗,c ⟩
t t t t
t=1 i=1 t=si
N si+1−1 si+1−1
(cid:88) (cid:88) (cid:88)
≤ ⟨ρ ,c ⟩− min ⟨ρ,c ⟩
t t t
ρ∈∆(M)
i=1 t=si t=si
Now the upper bound is the problem of optimizing (6) with η = η and t = s ,...,s −1. By
i i i+1
Theorem 1,
si (cid:88)+1−1
⟨ρ −ρ∗,c ⟩ ≤
L
log
|X||A|
+(cid:88)T
η
i ∥c −M ∥2
t i t η L 2 t t ∞
i
t=si t=1
where ρ∗ = argmin (cid:80)si+1−1 ⟨ρ,c ⟩.
i ρ∈∆(M) t=si t
By η−1D ≥ η Ψ according to our doubling trick algorithm,
i 0 i si:si+1−1
si (cid:88)+1−1
2L |X||A|
⟨ρ −ρ∗,c ⟩ ≤ log .
t i t η L
i
t=si
Adding over N phases,
T (cid:32) N (cid:33)
(cid:88) |X||A| (cid:88) 1
⟨c ,ρ −ρ∗⟩ ≤ Llog 2
t t
L η
i
t=1 i=1
28By (30), η−1 D < η Ψ and the monotonicity of Ψ,
N−1 0 N−1 sN−1:sN
8
R (ρ∗,{c }T ) ≤ D
T t t=1 0 η
N−1
(cid:18)
Ψ
(cid:19)1/2
1:T
< 8D
0
D
0
1/2 1/2
= 8D Ψ
0 1:T
(cid:118)
(cid:117) T
(cid:117)L |X||A| (cid:88)
= 8(cid:116) log ∥c −M ∥2
2 L t t ∞
t=1
■
A.7 Proof of Theorem 6
Since the constraint set of occupancy measure is unknown, the regret of OREPS-OPIX under
unknown transition setting can be decomposed as (31). Note the additional error term ρ −ρˆ as
t t
opposed to (11) used for the analysis of Theorem 3.
T
(cid:88)
R (ρ∗,{c }T ) = [⟨ρ −ρˆ,c ⟩+⟨ρˆ,c −cˆ⟩+⟨ρˆ −ρ∗,cˆ⟩+⟨ρ∗,cˆ −c ⟩]. (31)
T t t=1 t t t t t t t t t t
t=1
Lemma 3 (Lemma 5 of [12]). With probability at least 1−6δ, for ρˆ
t
estimated with ρP,πt under
transition probability P ∈ P, where the confidence set P is defined as (14),
T (cid:32) (cid:115) (cid:18) (cid:19)(cid:33)
(cid:88) T|X||A|
⟨ρ −ρˆ,c ⟩ = O L|X| |A|T log .
t t t
δ
t=1
By Lemma 3, with probability at least 1−6δ, the first term is bounded as
T (cid:32) (cid:115) (cid:18) (cid:19)(cid:33)
(cid:88) T|X||A|
⟨ρ −ρˆ,c ⟩ = O L|X| |A|T log . (32)
t t t
δ
t=1
The second term can be decomposed further as
T T T
(cid:88) (cid:88) (cid:88)
⟨ρˆ,c −cˆ⟩ = ⟨ρˆ,c −E [cˆ]⟩+ ⟨ρˆ,E [cˆ]−cˆ⟩.
t t t t t t−1 t t t−1 t t
t=1 t=1 t=1
From the definition of our cost estimator with the upper confidence bound,
T T (cid:20) (cid:21)
(cid:88)
⟨ρˆ,c −E [cˆ]⟩ =
(cid:88)(cid:88)
ρˆ(x,a) c (x,a)−
c t(x,a)−M t(x,a)
ρ (x,a)−M (s,a)
t t t−1 t t t t t
u (x,a)+γ
t
t=1 t=1 x,a
T
(cid:88)(cid:88) ρˆ t(x,a)(c t(x,a)−M t(x,a))
= (u (x,a)+γ −ρ (x,a)).
t t
u (x,a)+γ
t
t=1 x,a
29By M (x,a) ≤ c (x,a) ≤ M (x,a)+1, ρˆ(·) ≥ 0 and u (x,a) ≥ ρˆ(x,a),
t t t t t t
T T
(cid:88) (cid:88)(cid:88)
⟨ρˆ,c −E [cˆ]⟩ ≤ (c (x,a)−M (x,a))(u (x,a)+γ −ρ (x,a))
t t t−1 t t t t t
t=1 t=1 x,a
T
(cid:88)(cid:88)
≤ |u (x,a)−ρ (x,a)|+(c (x,a)−M (x,a))γ
t t t t
t=1 x,a
T T
(cid:88)(cid:88) (cid:88)
= |u (x,a)−ρ (x,a)|+γ ∥c −M ∥ .
t t t t 1
t=1 x,a t=1
Lemma 4 (Lemma 4 of [12]). With probability at least 1−6δ, for transition functions Px ∈ P for
t
all states x ∈ X, where the confidence set P is defined as (14), the cumulative error of occupancy
measure with respect to ρ of known transition setting is bouned as
t
T (cid:32) (cid:115) (cid:18) (cid:19)(cid:33)
(cid:88) (cid:88) (cid:12) (cid:12)ρP tx,πt(x,a)−ρ t(x,a)(cid:12) (cid:12) ≤ O L|X| |A|T log T|X||A|
δ
t=1x∈X,a∈A
Since u t(x,a) = max P∈PρP,πt(x,a), by Lemma 4, with probability at least 1−6δ,
T (cid:32) (cid:115) (cid:18) (cid:19)(cid:33) T
(cid:88) T|X||A| (cid:88)
⟨ρˆ,c −E [cˆ]⟩ ≤ O L|X| |A|T log +γ ∥c −M ∥
t t t−1 t t t 1
δ
t=1 t=1
Since {⟨ρˆ,E [cˆ]−cˆ⟩}T is a martingale difference sequence, by using the Azuma–Hoeffding
t t−1 t t t=1
inequality, with probability at least 1−δ,
(cid:118)
T (cid:117) T
(cid:88) (cid:117) 1 (cid:88)
⟨ρˆ,E [cˆ]−cˆ⟩ ≤ (cid:116)2log ∥c −M ∥2.
t t−1 t t δ t t 1
t=1 t=1
With probability at least 1−7δ, the second term is bounded as
T  (cid:115) (cid:18) (cid:19) (cid:118) (cid:117) T  T
(cid:88) T|X||A| (cid:117) 1 (cid:88) (cid:88)
⟨ρˆ t,c t−cˆ t⟩ ≤ OL|X| |A|T log
δ
+(cid:116)log
δ
∥c t−M t∥2 1+γ ∥c t−M t∥
1
t=1 t=1 t=1
(33)
Since ρˆ optimizes for cˆ, from the analyses of Theorem 1 and 3, the third term is bounded as
t t
T T
(cid:88) L |X||A| (cid:88) η
⟨ρˆ −ρ∗,cˆ⟩ ≤ log + ∥cˆ −M ∥2
t t η L 2 t t ∞
t=1 t=1
T
L |X||A| η (cid:88)
≤ log + ∥c −M ∥2 . (34)
η L 2γ2 t t ∞
t=1
Since u (x,a) ≥ ρ (x,a), from the analysis of Theorem 3 using Lemma 2, the fourth term is
t t
bounded as
T
(cid:88) L L
⟨ρ∗,cˆ −c ⟩ ≤ log max ∥c −M ∥ . (35)
t t t t ∞
γ δ t=1,...,T
t=1
30Finally, applying 32, 33, 34 and 35 back to 31 and letting γ = η1/3, with probability at least
1−7δ,
 (cid:115) (cid:18) (cid:19) (cid:118) (cid:117) T 
T|X||A| (cid:117) 1 (cid:88)
R T(ρ∗,{c t}T t=1) ≤ OL|X| |A|T log
δ
+(cid:116)log
δ
∥c t−M t∥2 1
t=1
+η1/3(cid:88)T (cid:20) ∥c t−M t∥2
∞ +∥c −M ∥
(cid:21)
t t 1
2
t=1
(cid:20) (cid:21)
L |X||A| L
+ log +log max ∥c −M ∥
t t ∞
η L δ t=1,...,T
(cid:18) (cid:19)3/4
Let η =
Llog|X L||A|+LlogL
δ
maxt∥ct−Mt∥∞
,
(cid:80)T
t=1
∥ct−M 2t∥2 ∞+∥ct−Mt∥1
(cid:18)
|X||A| L
(cid:19)1/4
R (ρ∗,{c }T ) ≤ Llog +Llog max∥c −M ∥
T t t=1 L δ t t t ∞
×(cid:32) (cid:88)T ∥c t−M t∥2
∞ +∥c −M ∥
(cid:33)3/4
t t 1
2
t=1
 (cid:115) (cid:18) (cid:19) (cid:118) (cid:117) T 
T|X||A| (cid:117) 1 (cid:88)
+OL|X| |A|T log
δ
+(cid:116)log
δ
∥c t−M t∥2 1
t=1
B Experiments
B.1 Experimental details
WeprovidethedetailsoftheexperimentinSection6asTable1. Additionally,wespecifyt = 1000
m
episodes between the change of obstacle locations for better predictability. Also, agent’s starting
location was randomly assigned at the beginning of each episode and the goal location was fixed
across episodes. And all three obstacles moved randomly every t episodes, but in a restricted
m
manner so that they do not obstruct the way from the starting point to the goal: that is, there is
always a way from the start to the goal without encountering any obstacles. Lastly, the experiment
was repeated ten times and the mean and variance of ten repetitions are shown in Figure 1.
Table 1: Parameters used in the experiments
Parameter Description Value
ϵ Default cost 0.01
L Timeout (number of layers) 200
η Learning rate for OREPS and OREPS-IX 2.1×10−3
OREPS
η Learning rate for OREPS-OPIX 0.2
OREPS-OPIX
(cid:114)
log|X||A|
From[8],thelearningrateforOREPSandOREPS-IXwasdeterminedasη = L L .
OREPS T|X||A|
However, since the perfect predictor we used for OREPS-OPIX has zero error for cost estimation,
we can set an arbitrarily high learning rate as long as it is less than 1 (for the high probability
31guarantee in Theorem 3). After a sparse exploration of parameters, we chose η = 0.2.
OREPS-OPIX
Withahigherlearningrate, thealgorithmconvergesevenfasteratthecostofhighervariance. And
the same learning rate was used for OREPS-OPIX with latest predictors.
32