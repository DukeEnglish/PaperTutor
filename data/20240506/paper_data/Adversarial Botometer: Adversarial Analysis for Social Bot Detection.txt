Adversarial Botometer: Adversarial Analysis for Social
Bot Detection
S. Najaria,b, D. Rafieib, M. Salehia,c,∗, R. Farahbakhshd
aFaculty of New Sciences and Technologies, University of Tehran, Tehran, Iran
bComputing Science Department, University of Alberta, Edmonton, Alberta
cSchool of Computer Science, Institute for Research in Fundamental Science (IPM),
P.o.Box 19395-5746, Tehran, Iran
dInstitut Polytechnique de Paris, Telecom SudParis, Evry, France
Abstract
Social bots play a significant role in many online social networks (OSN) as
they imitate human behavior. This fact raises difficult questions about their
capabilities and potential risks. Given the recent advances in Generative AI
(GenAI), social bots are capable of producing highly realistic and complex
content that mimics human creativity. As the malicious social bots emerge
to deceive people with their unrealistic content, identifying them and dis-
tinguishing the content they produce has become an actual challenge for
numerous social platforms. Several approaches to this problem have already
been proposed in the literature, but the proposed solutions have not been
widely evaluated. To address this issue, we evaluate the behavior of a text-
based bot detector in a competitive environment where some scenarios are
proposed: First, the tug-of-war between a bot and a bot detector is exam-
ined. It is interesting to analyze which party is more likely to prevail and
which circumstances influence these expectations. In this regard, we model
the problem as a synthetic adversarial game in which a conversational bot
and a bot detector are engaged in strategic online interactions. Second, the
bot detection model is evaluated under attack examples generated by a social
bot; tothisend, we poisonthedatasetwithattackexamples and evaluatethe
model performance under this condition. Finally, to investigate the impact
∗Corresponding author
Email addresses: najari.shaghayegh@ut.ac.ir, najarigh@ualberta.ca (S.
Najari), drafiei@ualberta.ca (D. Rafiei), Mostafa_salehi@ut.ac.ir (M. Salehi),
reza.farahbakhsh@it-sudparis.eu (R. Farahbakhsh)
Preprint submitted to Elsevier May 6, 2024
4202
yaM
3
]IS.sc[
1v61020.5042:viXraof the dataset, a cross-domain analysis is performed. Through our compre-
hensive evaluation of different categories of social bots using two benchmark
datasets, wewereabletodemonstratesomeachivementthatcouldbeutilized
in future works.
Keywords: Social Bot Detection, Adversarial Training, Conversational
Models
1. Introduction
Along with the development of Artificial Intelligence (AI) and since it
enters to bots like Joseph Weizenbaum’s ELIZA for emulating a Rogerian
psychotherapist, many things have changed
The utilization of these automated algorithms in social networks, com-
monly known as social bots may initially have some positive effects; however,
as time passes, their activities can become increasingly destructive.
In 2017, the average presence of bots on active Twitter accounts was
estimated to be around 15% [1], while on Facebook, it was approximately
11% in 2019 [2]. These numbers indicate a considerable share of automated
accounts on both platforms. Moreover, the presence of bots tends to increase
significantly when there are strong political or economic interests involved.
A study conducted in 2019 revealed that 71% of Twitter users discussing
trending US stocks were likely to be bots [3]. Similar results were found
regarding the presence of bots in online cryptocurrency discussions [4] and
their involvement in spreading “infodemics” during the COVID-19 pandemic
[5].
In recent years, the rapid development of new models in GenAI leads to
the emergence of powerful transformer-based bots such as Generative Pre-
trained Transformer (GPT) [6]. These advancements have enabled social
bots to engage in more complex interactions and penetrate popular discus-
sions, such as participating in entertaining conversations, leaving comments
on posts, and responding to questions [7].
In response to detrimental activities of social bots, extensive research
has been devoted to identification and mitigation of social bots. However,
a major challenge with bot detection is the poor performance of the models
under more complex circumstances, especially when a social bot employs
deceptive tactics. This is mainly because a social bot detector is considered
afixedmodulewithoutanyprogress, andthusthedataonwhichthemodelis
2trainedonisalsoconsideredfixed[8]. Thisisproblematicwhenthedatathat
the model is trained on fails to capture unforeseen or future patterns. Even
a small irregularity in the training data can cause the model’s performance
to drop significantly.
Evaluating the behavior of bot detection models in the presence of attack
examples generated by human-like bots is an under-researched area. This
study aims to address this gap by exploring a novel approach wherein a bot
engages in an adversarial game with a bot detection model. In this game,
where a bot automatically generates perturbations, the performance of bot
detection model can be evaluated using a process called adversarial training
[9]. In particular, we formulate adversarial scenarios in which a bot simulates
human behavior to generate attack examples. In contrast, a bot detection
model is then tasked with distinguishing between real and fake examples.
With this approach, we can effectively evaluate the behavior of bot detection
models to detect and defend against attacks by fraudulent bots.
Our evaluation, conducted on two benchmark datasets: Midterm-2018
and Cresci-2017 that includes three different categories of social bots and one
category of human users. Through analyzing the results, we have reached
some achievements that could utilize for the future works.
The main contributions of this study can be summarized as follows:
• We model a social bot as an interactive and automated conversational
model.
• To evaluate the bot detector’s behavior in a dynamic condition, we
design an adversarial game between bot detector and the bot that is
producing some adversarial attacks.
• To thoroughly evaluate the performance of the bot detector, we ran 3
different scenarios and presented our achievements, which we can use
for future work.
The rest of the paper is organized as follows. In Section 2, we review
the literature related to our work. Section 3 presents our proposed model
and scenarios and Section 4 presents our results and analysis. Finally, we
conclude the work with a discussion of some future research directions in
Section 5.
32. Related Works
To evaluate the performance of the bot detection module when faced with
a complex social bot that mimics human behavior, it is important to have an
overview of generative bots and the bot detectors. Therefore, we will discuss
them in the following sections.
2.1. Bot Generation
As automated computer programs, bots have existed in various forms
since the early days of computers. These forms range from those controlled
by humans, such as spam generators [10], to fully automated algorithms like
chatbots. Advances in natural language processing, particularly the use of
simple neural networks and transfer learning-based models [11], have enabled
botstoengagewithrealusersandgeneratetextthatcloselyresembleshuman
text. This has led to a major challenge in distinguishing between content
generated by bots and content created by humans [12].
One type of automated generative models that is designed to behave like
a human is a dialogue model [13] . This is a model that is able to capture the
structure and meaning of conversations between two entities, typically a user
and an interactive computer system. Dialogue systems can be categorized
into two main classes [14]: task-oriented and non-task-oriented (also referred
to as open-ended conversational agents). Task-oriented dialogue systems are
developed to accomplish specific tasks, whereas non-task-oriented systems
are more versatile and capable of engaging in broader and more general
conversations. A widely adopted approach for constructing dialogue models
is the utilization of a sequence-to-sequence model [15]. This model comprises
an encoder component responsible for mapping the input sequence to an
intermediate vector, and a decoder component that generates a response
utilizing the hidden state of the encoder and the intermediate information
obtained.
In the recent decade, Generative Adversarial Network (GAN) as a gener-
ative model designed for sequential data such as image generation [16], some
later they were adapted to process discrete and textual data. SeqGAN is
an example of a GAN, which has been specifically adapted for text genera-
tion [17]. Here, both the generator and the discriminator work together to
improve the quality of the text produced by the generator. The generator
makes decisions at each time step to maximize expected rewards, which are
determined by the discriminator. [17].
4As the goal of this paper is to analyze the behavior of bot and bot de-
tection in a tug-of-war, we utilized the dialogue models beside SeqGAN to
design some competition scenarios based on.
2.2. Bot Detection
In general, contributions to analyze the bot detectors behavior could to
fall into two main categories: feature-based and model-based approaches.
In Feature-based category, social bots are identified by some simple and
well-known machine learning models, such as Random Forest and Support
VectorMachine, byanalysingasetofbehaviouralfeaturesextractedfromthe
input dataset [18, 19, 20], directly. Thus, the performance of these models
are more dependent on the features selected.
Features can be divided into two main classes: content-based and user-
based. Content-based features focus on the content of the behaviour-based
post, including the text of a tweet or linguistic annotations such as part-
of-speech tags [21]. User-based features, on the other hand, are based on a
user’s profile, behavior [22], and connections to other users in the network
[23].
In another category, models are improving to extract more informative
features from the original contexts. Recently, some models such as deep
learning-based techniques have emerged that attempt to improve social bot
detectors by automatically extracting features and feeding them into a deep
neural network that enables the extraction of more informative feature vec-
tors. By using multiple layers of interconnected neurons, deep learning mod-
els can capture intricate patterns and representations in the data, leading to
more effective feature extraction [24].
Long Short-Term Memory (LSTM) and Convolutional Neural Networks
(CNN)arepopularneuralnetworkarchitecturesthathaveachievedsuccessful
results in tasks such as bot detection [25, 26, 27]. In this area, a novel
deep neural network model named RGA (ResNet, GRU, and Attention) is
developed to address the task of detecting social bots. The RGA model
combines the power of a residual network (ResNet), a bidirectional gated
recurrent unit (BiGRU), and an attention mechanism [28]. These methods
often represent the raw text as a vector and use pre-trained linguistic models
such as Global Vectors for Word Representation (GloVE [29]) to encode
initial features.
52.3. Existing Research GAP and Objective
With the advance of bots through recent developments in genAI, there is
a pressing need for powerful social bot detection methods in social networks
to understand better the influence and impact of these bots and develop
effective countermeasures [30].
As previous studies have shown, there is a lack of effective studies to
evaluate social bot detection methods when confronted with intelligent bots
that attempt to deceive the detectors. This was a strong incentive for us to
take a new perspective and try to address this problem in more detail.
Most of the previous studies use the GAN framework to augment the
lower class data such as bot [31], fake reviews [32], or spam [33] to make
the detectors stronger. Here, we would like to use GAN not as an auxiliary
module but as an adversary module to design a game between bot and bot
detector to analyze their behavior and strength under dynamic conditions.
Specifically, we use GAN to extend the bot class not by bot-like samples,
but to include examples generated by human or human-like models: Human-
like examples are generated by the adversarial bot (pre-trained on the class
botandfine-tunedontheclasshuman),whichknowsthebotdetectionresults
as feedback. This extension will allow the evaluation of bot detection models
in terms of their ability to distinguish between bots and humans in more
complex states. In this study, we focus on evaluating the effectiveness of bot
detection models in the presence of attack examples produced by generative
model.
3. Proposed Method: Adversarial Botometer
In this part, we have review the proposed scenarios.
3.1. Scenario 1: Adversarial Game between bot and bot bot detection
Figure 1 presents an overview of our proposed framework, which entails
the design of an adversarial game between a bot and bot detection model.
In this setup implemented by GAN, the bot is responsible for generating
samples that resemble human behavior. On the other hand, the bot detec-
tion model tries to distinguish between the patterns generated by the bot
and those generated by humans. Through this adversarial interaction, the
generative model seeks to improve its ability to generate samples that are
indistinguishable from those generated by humans, while the discriminator
aims to accurately identify bot-generated samples.
6Figure 1: Proposed Adversarial Game
To design this synthetic game, it is first necessary to specify a generative
modelthatplaystheroleofthebot, andadiscriminatormodelthatplaysthe
role of the bot detector. As shown in the previous section, there are a wide
range of models to do this selection. Here, we select the Seq2Seq model as
the generative model and the Contextual-LSTM as the discriminator model
for several reasons:
• Bot Generation: Seq2Seq Model
With the growth of bots through recent developments in GenAI, the
recent social bots are not rigid generative models; rather, they are able
to interact with other users, mimic their behaviour. To address this,
we chose the seq2seq model as the basis for dialogue models.
• Bot Detection: Contextual-LSTM
Since in our problem a bot tries to generate text like a human, the
generated content is text. As the text is sequential data and LSTM is
known to be the best example of DL-based models for sequential data,
we chose Contextual-LSTM as the basis for text-based bot detectors.
7The training phase for the bot and bot detection models consists of two
main steps: Pre-training, and Adversarial-training.
3.1.1. Pre-training Phase
In the context of dialog settings, our main focus is on generating answers
to questions or comments based on observed posts. To achieve this, we use
a sequence-to-sequence model consisting of an encoder and a decoder.
The encoder component captures an encoded representation of the input
question or comment to help the model to understand the input and extract
relevant features. The decoder component takes the encoded representation
from the encoder and generates a response, in return.
Formally speaking, given a corpus of message pairs (S,R) where the
source message S, consists of tokens s ,...,s and the response (target) mes-
1 n
sage R, consists of tokens r ,...,r , the bot is trained to maximize the total
1 m
log likelihood of observed target messages, given their respective source mes-
sages:
(cid:88)
logP(r ,...,r |s ,...,s ) (1)
1 m 1 n
(S,R)
Here, the goal of decoder D is to find an approximated distribution of train-
set conditioned on generator parameter θ, and initial source S. Then, the
decoder produces each token of samples based on preceding ones as shown
in eq.2.
(cid:89)
D (R |θ,S) = D (r |r ,θ,S) (2)
θ 1:T θ t 1:t−1
t:1,2,...,T
As illustrated in Figure 1, the bot detector takes two types of inputs:
fake targets generated by the generative model, or real ones selected from
the target dataset. Given a corpus of pair message (S,R), score y = 1 if
R was sampled from the training data and y = 0 otherwise, this model is
trained to maximize the probability of correct labeling as follows:
(cid:88)
logP(y|s ,...,s ,r ,...,r ) (3)
1 m 1 n
(y,S,R)
3.1.2. Adversarial Training
During the adversarial training phase, the goal is to generate samples
that have higher realism, resulting in higher rewards. Here, a mismatch
8arises between the generative bot that produces sentences token by token
and the discriminator model that evaluates the entire generated sequence.
To solve this problem, several approaches have been proposed previously.
One effective method is to use a Monte Carlo Search Tree (MCST). MCST
uses a tree-based search algorithm that explores the potential upcoming se-
quences [17]. The signaling feedbacks obtained from the discriminator can
then be propagated backwards to the generator. The minimax objective
function of this adversarial game could be as follows:
min max (E logBD(r)+log(1−BD(B(r|s))))
B BD (s,r)∼P
(S,R) (4)
Here, B and BD demonstrate the bot and bot detector models, respec-
tively. B generates samples satisfying P (r|s); where, the generator B as
B
the bot generate sample r selected from the response set R and given s se-
lected from the source set S. Practically, we use a Long Short Term Memory
(LSTM) to generate words, where each recurrent unit has embedding size 25,
hidden dimension and a batch size of 64.
In this game, the bot detector (BD) provide a feedback to the bot (B).
This feedback serves as an indicator of the bot’s success or failure in fooling
the bot detector. Based on this information, the bot regulates its generation
strategy and produces more patterns, which are then sent to the bot detector
for evaluation. This iterative process continues in a loop, forming a dynamic
game between the bot and the bot detection model. Algorithm 1 provides
more details about the process of this game.
As shown in Algorithm 1, the proposed framework first normalizes and
pre-processes inputs [25] in order to prepare tweets for input to the LSTM
network. This pre-processing phase involves removing punctuation, tokenz-
ing the tweets using the methods from Global Vectors for Word Representa-
tion (GloVE) [29]
We evaluate this framework through both live and offline adversarial
game, and the results are reported in the next section.
3.2. Scenario2: Data Poisoning
To assess the effectiveness of our bot detection model on a more com-
plicated dataset that include examples of attacks, we adopt two distinct
9Algorithm 1 Adversarial game of bot and bot detection models.
Require: Bot detector BD, Bot B, Dataset X containing pairs of (s, t), as
a symbol of (Source, Target)
Initialize B, BD parameters with the random weights
Pre-process and tokenize pairs (s, t) available in X
Pre-train B and BD
for Training Iterations do
for BD-training-steps do
Sample t from the dataset X
Sample tˆ∼ B(s)
Update BD using (t) as positive sample and (tˆ) as negative sample
using Eq. 3
end for
for B-training-steps do
Sample (s,t) from the dataset X
Sample tˆ∼ B(s)
Update BD using (t) as positive sample and (tˆ) as negative sample
and use output of BD as reward BD(B(s)) in Eq. 4
Update B using defined objective function in Eq. 4
end for
end for
10approaches. Firstly, we can select attack examples directly from the dataset
itself. Alternatively, we can generate attack examples by simulating bot be-
havior using the GAN framework.
In the first approach, we examine the existing dataset and specifically
identify instances that illustrate attack behaviors. These examples may in-
volvevariousformsofmaliciousactivities, suchasspamming, misinformation
propagation, or coordinated manipulation. By incorporating these attack ex-
amples into our evaluation, we can evaluate the robustness and accuracy of
our bot detection model in detecting and classifying these malicious behav-
iors.
The second approach is to generate attack patterns by simulating the
behaviorofthebot. UsingtheGANtechniqueandmethodsdescribedearlier,
we can simulate the actions and patterns of bots involved in attacks. By
generating synthetic attack patterns, we can create a controlled environment
to comprehensively evaluate the performance of our bot detection model in
identifying and distinguishing between normal user behavior and malicious
bot behavior.
Both approaches provide valuable insights into the effectiveness of our
bot detection model in processing complex datasets of attack patterns. By
combining real-world attacks with simulated attack scenarios, we can thor-
oughly evaluate the model’s capabilities and improve its ability to detect and
mitigate various forms of bot-driven attacks.
3.3. Scenario3: Domain and Model Explanation
To comprehensively evaluate the textual distinctions among social bots,
we carried experiments on 14 NLP features extracted from each class of so-
cial bot datasets. These features included mention count, hashtag count,
stopwords count, word count, unique word count, quoted word count, char-
acter count, sentence count, capital character count, capital word count,
unique-to-total word ratio, average sentence length, average word length,
and stopword-to-total word ratio.
In order to assess the significance of these features in prediction, we em-
ployed SHAP (SHAPley Additive Explanations) values to reverse-engineer
the output of the predictive algorithm. This involved training the bot detec-
tion model on the aforementioned features from the three distinct classes of
social bots. Subsequently, we evaluated the model’s performance by testing
it on the corresponding test set.
11Table 1: Dataset - Cresci 2017 [34] and Midtrerm 2018 [35] .
Dataset Label # Tweets # Conversations
Human 39,264 (38,516 , 748) 16,967 (16593, 374)
Bot1-Political 3810 (1,054, 2,756) 1778 (400, 1378)
Cresci2017 Bot2-Financial 932 (932, 0) 434 (434, 0)
Bot3-Commercial 430 (176, 254) 200 (73, 127)
Bot(Bot1+Bot2+Bot3) 5172 (2,162 , 3,010) 2,412 (907, 1,505 )
Human 8,092
midterm-2018
Bot 42,446
By utilizing SHAP values, we gain insights into the importance and con-
tributions of each feature towards the model’s predictions. This analysis
enables us to understand the relative influence of different textual character-
istics on the bot detection process. Through this approach, we can identify
the key indicators and linguistic patterns that distinguish the various classes
of social bots, further improving the effectiveness of our detection model.
4. Experimental Results
4.1. Datasets
In this study, we use a dataset of Twitter platform that serves as a refer-
ence point for spambot detection [34]. This dataset includes two categories
of accounts: genuine accounts and social bots. The genuine accounts man-
aged by humans and social bots handeling by bots are divided into three
subgroups: political bots (referred to as social spambots 1), financial bots
(referred to as social spambots 2), and commercial bots (referred to as so-
cial spambots 3). To identify the political bots, we analyzed their activity
of retweeting posts related to an Italian political candidate. The financial
bots were identified as those responsible for spamming paid mobile apps.
Finally, the commercial bots were identified as spammers promoting prod-
ucts on Amazon.com. We named these subgroups according to their main
functions.
Here, we have extracted the conversations based on done replies on tweets
by using the in reply to status id parameter available in the dataset. The
statistical information of the Cresci’s dataset is summarized in Table 1. To
prepare the dataset, we extracted Tweet-Retweet and Tweet-Reply relation-
ships between users based on human-human and bot-bot interactions avail-
12able in the dataset. Also, we used the midterm dataset that is filtered based
on political tweets collected during the 2018 U.S. midterm elections [35].
Figure2: TrainingprocedureofbotinthecontextofGAN(Perplexityvs. Cosinesimilarity
of generated samples to the real ones
4.2. scenario 1
To gain insight into the behavior of our models and identify areas where
performance degradation occurs and finally propose solutions, we carried
adversarial training.
Weuseddialoguemodelstogenerateresponsesasasocialbot,considering
the context of ongoing conversations. On the other hand, we used a Contex-
tual LSTM bot detection model to distinguish between machine-generated
(social bots) and human-generated examples.
In our adversarial scenario, our first goal was to evaluate the generative
model’s ability to produce samples that closely resembled real samples. We
focused on evaluating how well the generated samples reproduce the features
and patterns observed in the genuine data.
By comparing the generated samples to the real ones, we were able to
determine the truth and realism of the generative model results based on
GAN framework. This evaluation was important in determining the model’s
ability to reproduce the nuances and subtleties of the genuine data. It served
as a fundamental benchmark for measuring the performance and accuracy
13(b) Rewards received from the bot detector to
(a)BotdetectionLoss
thebot
Figure 3: Training procedure of bot detection in the context of GAN.
of the generative model and provided valuable understanding into its capa-
bilities. Figure 2 shows the result of this evaluation. The x-axis represents
the number of training iterations for the generator model, while the y-axis
represents the perplexity of the generated samples and the cosine similarity
between the generated and actual samples. As can be seen in the figure, the
cosine similarity between the generated samples and the real ones increases
with the training of the bot model and the complexity of the bot decreases.
This is to be expected as the generative model gets better at generating
samples that can fool the discriminator model.
Figure 4: Training speed of bot and bot detection in the context of GAN
14Our next goal is to quantify the fluctuations of the bot detector druring
the adversarial training process. Figure 3 shows the loss and the rewards
that the bot detector sends to the bot throughout the training process.
By analyzing the loss and reward curves, we represent the performance of
bot detector model and its ability to discriminate human and bot-generated
samples.
The observed decreasing loss of the bot detection model in the context of
GAN demonstrate its improvement during training. As the model optimizes
its parameters, it becomes more skilled at discriminating between samples
generated by social bots and those generated by humans. This indicates a
positive trend in the model’s ability to accurately detect machine-generated
samples.
According to the GAN concept, the increasing signal rewards received to
the social bot suggests that the bot detection model is improving at iden-
tifying and classifying bot-generated samples. So, the improvement in the
bot detection is a direct consequence of the training process in the GAN
framework.
Here, we modeled a GAN-based game between bot and bot detection
models. As shown, each of bot and bot detection models (traind on the bot
dataset) are working right like the generative and discriminator models in
the context of GAN.
Now, we evaluate the training speed of the bot and bot detection models
based on their training process in the designed competition. Earlier in the
evaluation of bot detector’s performance 3 it can be observed that although
the reward increases and the loss decreases, the plots in the later iterations
exhibit smoother slopes. This trend suggests that the bot detection model
encountersdifficultyinextractingsubstantialinformationfromthegenerated
samples during these iterations. Moreover, the training loss of two models
are compared in Figure 4. Here, it is obvious that the training speed of
the bot outperforms the performance of the bot detector model. In fact,
the comparison of the loss values confirms the faster training speed of the
generative model, indicating that it can outperform the bot detection model
in terms of training efficiency.
Achievement 1: In the GAN framework, a weak discriminator can be
the reason of the other problems such as collapse mode, which can be solved
by replacing autoencoders [36], Energy-based GANs [37] or a strong cost
function [38].
Since the behavior of the bot and the bot detector on the human-bot data
15(a)Bot1-Political (b)Bot2-Financial (c)Bot3-Commercial
Figure5: Probabilitydistributionofthreeclassesofsocialbotsforattackexamplesselected
from data.
is the same as the behavior of the generative and the discriminator models
in the GAN context, we can use the solutions of the powerful discriminators
proposed in the GAN framework for the bot detection method, too.
4.3. scenario 2
In this part, we evaluate the bot detection model based on the attack
examples poisoned to the dataset based on two different scenarios: attack
examples are pinching from the human dataset or generating by a social bot.
4.3.1. Attack examples are pinching from the human’s dataset
In social networks, bots can use deceptive tactics by spreading content
written by humans and making other users believe that it comes from a real
person. In this evaluation phase, we focus on evaluating the ability of our
bot detection model to identify repeat bots that use this strategy on social
media platforms.
Bots can use various strategies to mimic human behavior, as mentioned
earlier. They can adapt their actions based on the performance of bot de-
tection models and specifically target examples where they are less likely to
be detected. In this experiment, we specifically select examples with a low
probabilityofdetectiontoevaluatetheeffectivenessofthemodelindetecting
these deceptive behaviors.
The results presented in Figure 5 show the probability distribution be-
tween three classes: human, bot, and attack examples. The attack examples
are derived from the human class and placed in the bot class to simulate
deceptive behavior. As expected, the detection probability differ among the
threemodelstrainedondifferenttypesofbotsduringthetrainingandtesting
process. Figure 5 shows that detecting attack examples within the bot class
16(a)Bot1-Political (b)Bot2-Financial (c)Bot3-Commercial
Figure 6: Probability distribution of three classes of social bots for attack examples gen-
erated in adversarial games.
is more difficult for the model trained on commercial bots than for the mod-
els trained on political and financial bots. Furthermore, the model trained
on financial bots shows greater difficulty in detecting attack examples than
the model trained on political bots.
Achievement 2: These results show that attack example detection
varies in difficulty across different types of bots. Attack example detec-
tion confirm to be more difficult for the model trained on commercial bots,
followed by the model trained on financial bots, while the model trained on
political bots performs comparatively better.
4.3.2. Attack examples are Generated Human-like samples
To generate examples that mimic human behavior, we used a generative
model trained in an adversarial game, treating it like a bot. These generated
examples that resembled human-like interactions were then included in the
bot class. A classifier was then used to evaluate the likelihood that these
examples belonged to either the bot or human class.
The probability distribution of the bot, human, and attack examples is
shown in Figure 6c. The results reveal interesting patterns. Models trained
on bot examples from the political and financial domains have difficulty dis-
tinguishing attack examples from human examples, resulting in predictions
that are close to thehuman class. However, when the generated examples are
input to the model trained on commercial bot examples, it exhibits a higher
probability of categorizing them as belonging to the bot class. This indicates
that the model trained on commercial social bots performs relatively worse
than the models trained on political and financial bots.
Achievement 3: These results suggest that the model trained on com-
mercial bots has difficulty in accurately detecting and distinguishing attack
17(a)Politicalbot (b)FinancialBot (c)CommercialBot
Figure 7: Explain bot detection model based on 14 NLP-based features.
samples from human samples, in contrast to the models trained on political
and financial bots. The differences in performance suggest differences in the
effectiveness of the models in detecting deceptive behaviors and highlight the
need for further analysis.
4.4. Scenario3
In this section, we will take a closer look at the textual characteristics
of the three different classes of social bots and examine their differentiating
features in more detail.
Figure 7 illustrates the relationship between the SHAP values and the
predictionperformance,withthehorizontalaxisrepresentingtheSHAPvalue
and the color of the points indicating the relative values of the observations
compared to those with higher or lower values.
The analysis shows that not all features have the same importance in
prediction. For example, the number of mentions has a negative effect on
prediction performance when its value is higher. Vice versa, lower mention
count values have a positive effect on prediction performance for all three
types of social bots.
In the case of political bots, the stopword to total word ratio feature
displays a negative impact when its value is higher, while lower values have
a positive impact on the prediction. On the other hand, this feature appears
to be less significant for financial and commercial bots compared to other
features. Consequently, it may be feasible to exclude this feature during the
pre-processing step, given its relatively lower importance.
18Figure 8: Accuracy values for cross-data analysis.
Furthermore, the results highlight the similarity between the features as-
sociatedwithfinancialandcommercialbots,whilepoliticalbotsshowdistinct
behavioral characteristics in these identified features.
Achievement 4: This analysis provides valuable insights into the im-
portance and influence of different features on the bot detection model’s
predictions. By understanding the diverse impacts of these features, we can
clarify our pre-processing steps and improve the overall accuracy and perfor-
mance of the bot detection model, particularly when distinguishing between
different types of social bots.
There have been some studies that aim to conduct a cross-domain analy-
sis, in which the goal is to evaluate the generalization of a model by training
it on a dataset from one domain and testing it on a dataset from a different
domain[35]. To further assess the performance of a text-based classifier, we
performed an evaluation where the classifier was trained on the human class
and a specific social bot, and then tested on a human test set and another
social bot. The outcomes of this evaluation are shown in Fig. 8.
Theresultsdemonstratethatthemodelexhibitedpoorperformancewhen
predicting the political social bot using a model trained on the commercial
bot. Thislackofaccuracycanbeattributedtothelowestcorrelationbetween
the two domains. On the contrary, the model performed relatively similarly
when predicting the commercial and financial bots using a model trained on
the political bot, or when predicting the political bot using a model trained
19(a)Probabilitydistributionforclassbot (b)Probabilitydistributionforclasshuman
Figure 9: Model Comparison - The Probability distribution of two class bot and human
in different models.
on the political bot itself.
This results indicate that the prediction of political bots using models
trained on commercial and financial bots yields a higher accuracy. However,
when the model trained on the financial bot is tested on the same dataset,
its accuracy is lower compared to the other bot datasets. This discrepancy
may be attributed to the fact that the model trained on the financial dataset
exhibits better generalization capabilities, while the models trained on the
commercial and political datasets are more prone to over-fitting on the train-
ing set, resulting in worse performance in different circumstances.
Also, these findings suggest that the classifier may be more effective at
predicting political bots using models trained on commercial and financial
bots. However, it may not generalize well to other scenarios or domains.
Achievement 5: Here, our findings indicate that the classifier may not
behighlyeffectiveinaccuratelypredictingsocialbotsfromdifferentdomains.
The divergent characteristics and behaviors exhibited by social bots in dif-
ferent domains pose challenges for the classifier in effectively capturing and
distinguishing their distinguishing features.
4.5. Comparison
In this section, we have evaluated the effectiveness of four different ap-
proaches.
SincethegenerationofwordembeddingsfromBidirectionalEncoderRep-
resentations from Transformers (BERT), a pre-trained language model, is
an efficient technique, we used this method together with neural networks.
20Table2: DatasetComparison-Cresci2017[34]Vs. Midtrerm2018[35](Cresci/Midterm).
Model Accuracy AUCROC
Default 0.915/ 0.825 0.971/ 0.588
CNN 0.893/ 0.799 0.962/ 0.704
LSTM 0.819/ 0.825 0.902/ 0.522
NonLinear 0.875/ 0.825 0.951/ 0.523
Therefore, we used four models for comparison: CNN, LSTM, Nonlinear and
Default. The default model relies exclusively on BERT for predictions.
Fig 9 shows the probability distribution of two classes, bot and human,
for the Cresci dataset. It is noticeable that the probabilities for the bot class
tend towards 1, while the probabilities for the human class tend towards 0.
The default model has the highest distribution for both classes, followed by
theCNN,LSTMandNonlinearmodelsinsuccessiveorder. Table2showsthe
performance of these four models in two datasets, Cresci-2017 and Midterm-
2018, confirming the previous results.
5. Conclusions
This research employed a synthetic adversarial game to assess the effec-
tivenessoftext-basedsocialbotdetectionmethodsindifferentscenarios. The
focus was on evaluating the performance of generative models in generating
attack examples that mimic human textual behavior. The findings revealed
that these detection methods exhibit varying levels of strength across differ-
ent types of social bots, with performance variations based on the specific
domain and content generated by the bots.
In future works, the aim is to conduct more extensive bot detection mod-
els based on this works achievements. By gaining deeper insights into the
behavioral patterns of social bots, the research aims to enhance the perfor-
mance of detection models. This will involve refining the understanding of
bot behavior and exploring novel approaches to improve detection accuracy
and effectiveness.
By addressing the limitations identified in this study and further inves-
tigating the complexities of social bot detection, the research endeavors to
advance the field and contribute to the development of more robust and
reliable methods for detecting and combating social bots.
21References
[1] O. Varol, E. Ferrara, C. Davis, F. Menczer, and A. Flammini, “Online
human-bot interactions: Detection, estimation, and characterization,”
in Proceedings of the international AAAI conference on web and social
media, vol. 11, no. 1, 2017, pp. 280–289.
[2] M. Zago, P. Nespoli, D. Papamartzivanos, M. G. Perez, F. G. Marmol,
G. Kambourakis, and G. M. Perez, “Screening out social bots interfer-
ence: Are there any silver bullets?” IEEE Communications Magazine,
vol. 57, no. 8, pp. 98–104, 2019.
[3] S. Cresci, F. Lillo, D. Regoli, S. Tardelli, and M. Tesconi, “Cashtag
piggybacking: Uncovering spam and bot activity in stock microblogs on
twitter,” ACM Transactions on the Web (TWEB), vol. 13, no. 2, pp.
1–27, 2019.
[4] L. Nizzoli, S. Tardelli, M. Avvenuti, S. Cresci, M. Tesconi, and E. Fer-
rara, “Charting the landscape of online cryptocurrency manipulation,”
IEEE Access, vol. 8, pp. 113230–113245, 2020.
[5] R. Gallotti, F. Valle, N. Castaldo, P. Sacco, and M. De Domenico, “As-
sessing the risks of ‘infodemics’ in response to covid-19 epidemics,” Na-
ture human behaviour, vol. 4, no. 12, pp. 1285–1293, 2020.
[6] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A.N.Gomez,
L(cid:32) . Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems, vol. 30, 2017.
[7] E. Ferrara, O. Varol, C. Davis, F. Menczer, and A. Flammini, “The rise
of social bots,” Communications of the ACM, vol. 59, no. 7, pp. 96–104,
2016.
[8] R. De Nicola, M. Petrocchi, and M. Pratelli, “On the efficacy of
old features for the detection of new bots,” Information Processing
& Management, vol. 58, no. 6, p. 102685, 2021. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0306457321001709
[9] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
22[10] A. H. Wang, “Detecting spam bots in online social networking sites:
a machine learning approach,” in Data and Applications Security and
Privacy XXIV: 24th Annual IFIP WG 11.3 Working Conference, Rome,
Italy, June 21-23, 2010. Proceedings 24. Springer, 2010, pp. 335–342.
[11] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer
learning,” Journal of Big data, vol. 3, no. 1, pp. 1–40, 2016.
[12] A. Alarifi, M. Alsaleh, and A. Al-Salman, “Twitter turing test: Iden-
tifying social machines,” Information Sciences, vol. 372, pp. 332–346,
2016.
[13] Y. Xu, H. Zhao, and Z. Zhang, “Topic-aware multi-turn dialogue mod-
eling,” in Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 35, no. 16, 2021, pp. 14176–14184.
[14] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on dialogue systems:
Recent advances and new frontiers,” Acm Sigkdd Explorations Newslet-
ter, vol. 19, no. 2, pp. 25–35, 2017.
[15] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-
promoting objective function for neural conversation models,” arXiv
preprint arXiv:1510.03055, 2015.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Advances in neural information processing systems, vol. 27, 2014.
[17] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative
adversarial nets with policy gradient,” in Proceedings of the AAAI con-
ference on artificial intelligence, vol. 31, no. 1, 2017.
[18] M.Heidari,H.JamesJr,andO.Uzuner,“Anempiricalstudyofmachine
learning algorithms for social media bot detection,” in 2021 IEEE In-
ternational IOT, Electronics and Mechatronics Conference (IEMTRON-
ICS). IEEE, 2021, pp. 1–5.
[19] M. Aljabri, R. Zagrouba, A. Shaahid, F. Alnasser, A. Saleh, and D. M.
Alomari, “Machinelearning-basedsocialmediabotdetection: acompre-
hensive literature review,” Social Network Analysis and Mining, vol. 13,
no. 1, p. 20, 2023.
23[20] M. Orabi, D. Mouheb, Z. Al Aghbari, and I. Kamel, “Detection of bots
in social media: a systematic review,” Information Processing & Man-
agement, vol. 57, no. 4, p. 102250, 2020.
[21] S. B. Jr, G. F. Campos, G. M. Tavares, R. A. Igawa, M. L. P. Jr,
and R. C. Guido, “Detection of human, legitimate bot, and malicious
bot in online social networks based on wavelets,” ACM Transactions on
Multimedia Computing, Communications, and Applications (TOMM),
vol. 14, no. 1s, pp. 1–17, 2018.
[22] T. Velayutham and P. K. Tiwari, “Bot identification: Helping analysts
for right data in twitter,” in 2017 3rd international conference on ad-
vances in computing, communication & automation (ICACCA)(fall).
IEEE, 2017, pp. 1–5.
[23] A. Dorri, M. Abadi, and M. Dadfarnia, “Socialbothunter: Botnet de-
tection in twitter-like social networking services using semi-supervised
collective classification,” in 2018 IEEE 16th Intl Conf on Depend-
able, Autonomic and Secure Computing, 16th Intl Conf on Perva-
sive Intelligence and Computing, 4th Intl Conf on Big Data Intel-
ligence and Computing and Cyber Science and Technology Congress
(DASC/PiCom/DataCom/CyberSciTech). IEEE, 2018, pp. 496–503.
[24] K.Hayawi,S.Saha,M.M.Masud,S.S.Mathew,andM.Kaosar,“Social
media bot detection with deep learning methods: a systematic review,”
Neural Computing and Applications, pp. 1–16, 2023.
[25] S. Kudugunta and E. Ferrara, “Deep neural networks for bot detection,”
Information Sciences, vol. 467, pp. 312–322, 2018.
[26] D. M. Beskow and K. M. Carley, “Its all in a name: detecting and label-
ing bots by their name,” Computational and Mathematical Organization
Theory, vol. 25, no. 1, pp. 24–35, 2019.
[27] E.ArinandM.Kutlu, “Deeplearningbasedsocialbotdetectionontwit-
ter,” IEEE Transactions on Information Forensics and Security, vol. 18,
pp. 1763–1772, 2023.
[28] Y. Wu, Y. Fang, S. Shang, J. Jin, L. Wei, and H. Wang, “A novel
frameworkfordetectingsocialbotswithdeepneuralnetworksandactive
learning,” Knowledge-Based Systems, vol. 211, p. 106525, 2021.
24[29] J.Pennington,R.Socher,andC.D.Manning,“Glove: Globalvectorsfor
wordrepresentation,” inProceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP), 2014, pp. 1532–1543.
[30] E. Ferrara, “Social bot detection in the age of chatgpt: Challenges and
opportunities,” First Monday, 2023.
[31] S. Najari, M. Salehi, and R. Farahbakhsh, “Ganbot: a gan-based frame-
work for social bot detection,” Social Network Analysis and Mining,
vol. 12, no. 1, pp. 1–11, 2022.
[32] H. Aghakhani, A. Machiry, S. Nilizadeh, C. Kruegel, and G. Vigna,
“Detecting deceptive reviews using generative adversarial networks,” in
2018 IEEE Security and Privacy Workshops (SPW). IEEE, 2018, pp.
89–95.
[33] S. Shehnepoor, R. Togneri, W. Liu, and M. Bennamoun, “Scoregan: A
fraud review detector based on regulated gan with data augmentation,”
IEEE Transactions on Information Forensics and Security, 2021.
[34] S. Cresci, R. Di Pietro, M. Petrocchi, A. Spognardi, and M. Tesconi,
“The paradigm-shift of social spambots: Evidence, theories, and tools
for the arms race,” in Proceedings of the 26th international conference
on world wide web companion, 2017, pp. 963–972.
[35] K.-C. Yang, O. Varol, P.-M. Hui, and F. Menczer, “Scalable and gener-
alizable social bot detection through data selection,” in Proceedings of
the AAAI conference on artificial intelligence, vol. 34, no. 01, 2020, pp.
1096–1103.
[36] W. H. L. Pinaya, S. Vieira, R. Garcia-Dias, and A. Mechelli, “Autoen-
coders,” in Machine learning. Elsevier, 2020, pp. 193–208.
[37] J. Zhao, M. Mathieu, and Y. LeCun, “Energy-based generative adver-
sarial network,” arXiv preprint arXiv:1609.03126, 2016.
[38] T. Che, R. Zhang, J. Sohl-Dickstein, H. Larochelle, L. Paull, Y. Cao,
and Y. Bengio, “Your gan is secretly an energy-based model and you
should use discriminator driven latent sampling,” Advances in Neural
Information Processing Systems, vol. 33, pp. 12275–12287, 2020.
25