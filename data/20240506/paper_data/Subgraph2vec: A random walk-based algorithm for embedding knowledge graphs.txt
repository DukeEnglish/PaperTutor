Subgraph2vec: A random walk-based algorithm for
embedding knowledge graphs
Elika Bozorgi1,∗, Saber Soleimani1, Sakher Khalil Alqaiidi1, Hamid Reza Arabnia1, Krzysztof Kochut1
1School of Computing, The University of Georgia, Athens, GA, USA
{elika.bozorgi, saber.s, sakher.a, hra, kkochut}@uga.edu
Abstract—Graph is an important data representation which external resources which are often incomplete as well. Also,
occurs naturally in the real world applications [9]. Therefore, these outside resources have different structure and format
analyzing graphs provides users with better insights in different
which makes integrating data more difficult. In addition,
areas such as anomaly detection [18], decision making [8],
knowledge graphs usually grow in size and complexity which
clustering [25], classification [27] and etc. However, most of
these methods require high levels of computational time and makes them inefficient and therefore special infrastructure
space. We can use other ways like embedding to reduce these and algorithms are needed to make them more scalable [20].
costs. Knowledge graph (KG) embedding is a technique that This complexity also makes interpreting patterns and drawing
aims to achieve the vector representation of a KG. It represents
insights more challenging and time-consuming. Moreover,
entities and relations of a KG in a low-dimensional space while
buildingandmaintainingknowledgegraphsoftenneeddomain
maintaining the semantic meanings of them. There are different
methods for embedding graphs including random walk-based expertise to ensure the accuracy of the represented data.
methods such as node2vec, metapath2vec and regpattern2vec. Areasonablenumberofsuchissuescanbeaddressedusing
However, most of these methods bias the walks based on a ArtificialIntelligence(AI).Infact,differentmethodsofAIcan
rigid pattern usually hard-coded in the algorithm. In this
be used for resolving the previous mentioned problems with
work, we introduce subgraph2vec for embedding KGs where
knowledgegraphsaswellas:knowledgegraphcompletion[4],
walks are run inside a user-defined subgraph. We use this
embedding for link prediction and prove our method has better node representation learning [31], semantic search [21], ques-
performanceinmostcasesincomparisonwiththepreviousones. tion answering [22], anomaly detection [17], quality assess-
ment [12], etc. However, applying AI algorithms on different
Keywords:RepresentationLearning,InformationEngineering, types of Knowledge graphs requires a significant number of
Link Prediction, Deep Learning, Graph Embedding
adjustments due to the high number of the dimensions of the
input data. In fact, in many cases, these methods cannot be
I. INTRODUCTION
applied on Knowledge graphs directly. Therefore, it is desired
Knowledge graphs play a crucial role in organizing, un- to reduce the number of the dimensions of the input data or
derstanding, and leveraging information in various domains. knowledge graph by embedding it with different AI methods.
Hence,theybecomeincreasinglypopularindifferentareasdue Knowledgegraphembeddingistheactoftranslatingthehigh-
to their valuable features. For example, they are widely used dimensional data to a low-dimensional space, while trying
inrecommendationsystemsinE-commercetomodeltherela- to maintain the semantic meanings of the KG elements. The
tionships between users and items. Next, they enable person- embedding of each element in the dataset is a unique vector-
alized recommendations by leveraging knowledge about user representation of that element. The resulted embedding of the
interactionsanditemcharacteristics[13].Asanotherexample, Knowledge graph can be used for different purposes, such
they are widely used in healthcare to support practitioners as link prediction, entity classification, semantic search, and
for disease diagnosis by knowledge discovery from patients’ others. There exist different types of embedding methods for
personal health repositories [24]. In addition, they provide a Knowledgegraphsbasedonsupervision,whichincludesuper-
structuredwayofconnectingentitieswithrelationships,which visedmethodssuchasGraphConvolutionalNetworks(GCNs)
createsanetworktoorganizeandrepresentinformation.Thus, [30], Unsupervised methods like TransE [2] and DeepWalk
it is easier to discover relevant information either manually, and Hybrid ones such as GraphSAGE [11]. In this paper, we
by navigating through nodes to discover visions, or by using introduce an unsupervised algorithm based on random walks
machine learning and AI to make predictions and generate forembeddingKnowledgegraphs.Therearepreviousrandom
insights into data. In addition, Knowledge graphs facilitate walk-basedmethodsforembeddingknowledgegraphs,suchas
querying and discovery of complex patterns within data [26] node2vec, metapath2vec and regpattern2vec. However, these
and also capture semantic connections which leads to more methods come with challenges. For example, in node2vec the
accurate queries. random walks are biased to highly visible types of nodes, the
Although knowledge graphs offer many benefits, they have ones with a dominant number of paths. On the other hand,
several issues. For instance, Knowledge graphs are often in metapath2vec and regpattern2vec the walks are biased by a
incomplete [3], as they are frequently populated using various seriesofrelationships(ornodetypes)orafixedregularpattern
4202
yaM
3
]GL.sc[
1v04220.5042:viXraof relationships (or node types), respectively. In our method, embedded by giving them as an input to a Skip-gram model,
the user enters an arbitrary pattern which defines a schema a variant of the one used in the word2Vec model. After the
subgraph in the actual knowledge graph and the walk is done skip-gram is trained, it predicts the context nodes based on its
withinthissubgraph.Inthenextsection,wewillcompareour input and the output is the embedding of the nodes.
method with the previous related ones. Node2vec [10], is a technique for learning node embeddings
in a graph. In fact, it is an extension of the word2vec model
II. PRELIMINARIES
which is a method for word embedding in textual data.
In this section, we will explain some primitive concepts It learns embeddings of the nodes in a graph by using a
which are fundamental to the understanding of our method neighborhoodsamplingstrategywhichcapturesbothlocaland
and the previous ones, starting with the explanation of the global structures. Node2vec generates random walks which
Knowledge graphs (or heterogeneous networks). are balanced between breadth-First Search (BFS) and depth-
First search (DFS) and applies it on the input data. After
Knowledge graph: A knowledge graph is a data set that generating random walks, node2vec learns embeddings using
represents real-world facts and semantic relationships in the the Skip-gram model. The Skip-gram model predicts the
form of triplets, where the triplets are represented as a graph context (neighboring nodes) of a target node based on its
with edges as relations and nodes as entities [1]. Mathematic- embedding. Node2vec optimizes the obtained embeddings by
ally, consider G=(V,E) where G represents the knowledge maximizing the likelihood of observing neighboring nodes
graph and V are the nodes or entities and E represent the within the context window of each target node.
relations. Metapath2vec [7], is a representation learning method de-
Walk: A walk is a finite sequence of edges which join a signedspecificallyforHINstolearnembeddingsofthenodes
sequence of vertices. In G=(V,E) where G is a knowledge and captures both semantic and structure of the network. In
graph and V represents the nodes and E represents the edges, thismethod,meta-pathguidedrandomwalksareusedtomake
a finite walk is a sequence of edges (e 1,e 2,...,e n−1) for asentenceofthenodes.Themeta-pathsarecreatedbydomain
which there is a sequence of vertices (v 1,v 2,...,v n) such that experts based on nodes according to the dataset. For example,
ϕ(e i)=(v i,v i+1) for i=1,2,...,n−1. (v 1,v 2,...,v n) is the on a DBLP computer science bibliographic dataset [15], the
vertex sequence of the walk. The walk is closed if v 1 = v n, created meta-paths are : APA, APVPA, OAPVPAO, where A
and it is open otherwise [28]. represents the author, P the paper, O the organization and
Path: A path is a walk on a graph where the vertices are V the venue. Consider APA as an example. The first node
repeated. to choose must be of type A, the second node must be of
Subgraph: Graph S =(V S,E S) is considered as a subgraph type P and the third node must be of type A. This means
of G=(V G,E G) if and only if its vertex set (V S) and edge that at each step the next node to visit is chosen according
set (E S) are subsets of those of G. In other words: V S ∈V G to a pre-defined meta-path (APA in this example), ensuring
and E S ∈E G. that the walk follows a meaningful path in the network. After
Schema:GivenaknowledgegraphG,anedgewitharelation generating a large number of meta-path based random walks,
type R connects source nodes of type S and target nodes of these sequences of nodes are used as training data for the
type T defines a meta edge S −→R T . A schema graph (aka Skip-gram model. Finally, these embeddings are aggregated
knownasmeta-template)forGisasetofallsuchmetaedges. to obtain comprehensive representations for each node in the
In fact, a schema graph is a directed graph defined over node network beside capture the diverse relationships and semantic
types T, with edges from R, denoted as G =(T,R) [23]. meanings associated with each node.
S
Heterogeneous Network [29]: or HIN, is a graph denoted Regpattern2vec [14], is a method for embedding KGs which
as G = (V,E,T), where each v ∈ V and e ∈ E has a samples a large knowledge graph to learn node embedding
mapping function Φ(V) = V → T and ϕ(E) = E → T while capturing the semantic relationships among the nodes
v e
and T and T denote sets of node and relation types where of the graph. In this method, the walk is biased by a fixed
v e
|T |+|T | > 2. In simple words, in these networks, nodes pattern H[^T] + HT which is based on edges. The walk starts
v e
and edges can belong to different types, and the connections at a random node from a set of given nodes known as source
between nodes can have various semantic meanings. nodes (S) on the knowledge graph. After choosing the source
node, the walk chooses an edge of type H and moves to a
III. RELATEDWORKS
randomly chosen neighbor creating a path at each time. Next,
DeepWalk [19], is a graph embedding method which aims the walk chooses a random edge of type ^T and moves to the
to learn continuous representations of nodes in a graph. For next random node. The walk follows the pattern H[^T] + HT
each walk, it begins by generating the walks from a random and continues the walk according to the a parameter called
starting node and moves to the next random node on the walk length. We can control the number of the walks in each
graph. Each random walk sequence is treated as a sentence, pathbywalklengthandwhenthenumberofthewalksreaches
in which the nodes are considered as the words of the to walk length, the walk is complete. Once the first walk is
sentence. Next, the word2vec [6] model from NLP is applied complete, we can start another walk by choosing a random
tolearnembeddingsforthenodes.Theresultingsentencesare node from S. The number of the times we start a new walk isbasedonaparametercallednumberofwalk.Choosinganode
fromthesetofsourcenodeseachtimeasthestartingnodeand 
1 × 1 (r ∈S)
creating random walks will result in several paths of random rti (cid:80)n i=1ti
walks. To obtain the embedding of these nodes, the resulted
paths are fed into a modified version of a skip-gram model. 0 (cid:0) ri, ri, ri+1(cid:1) ∈/ G′
This method of embedding is an unsupervised method and it
can be applied on any given raw text corpus or document. In
regpattern2vec,thealgorithmrunsthewalksonarigidregular wheret denoteseachtypeofedgesconnectedtothecurrent
i
expression H[^T] + HT which is defined by domain experts node and r denotes the number of the edges of each type.
ti
and is hard-coded in the algorithm and cannot be changed. We choose the type of the next edge based on its probability.
An important thing to consider is that we have a hierarchy of
Contribution: In the previously mentioned methods, the edges in our knowledge graph; which means that a type of
algorithms are either based on a pre-defined sequence of edge might have different sub-types. With that being said, all
node/edge types or a pattern designed by domain experts or the sub-types of a specific type should be considered as the
is biased toward specific nodes by experts. It means that they same type.
are not generic and the user does not have any role in guiding In addition, we have a parameter called number of walks in
the walks. In our algorithm, however, we define a method our code, which defines the number of the walks that should
in which the algorithm runs on any arbitrary random walk bewalkedineachpath.Thedefaultnumberforitissetas40,
path inside a user-defined schema subgraph based on edges. which the user can adjust on their own interest in the code.
Theuserentersaschemasubgraphbyenteringintegerswhere
A. Walk-based Embedding
each integer represents an edge in the knowledge graph. This
schema subgraph defines the subgraph. After the subgraph After the user has entered the schema subgraph, the sub-
is defined, we choose the first random node as the source graph (S) is defined and we are able to conduct the walk.
node inside this subgraph and continue the walk based on the We start the walk by choosing the first node randomly within
walklengthparameterandmovetoanyrandomnodesviaany the sub-graph. Then we will choose an edge connected to this
random edges. The walks are valid only if they are within noderandomlyandifitisinsidetheschema-graph,thechosen
the subgraph and invalid otherwise. The advantage of using edge is valid which means we choose the next random edge.
a subgraph is that it is more permissive; since we can run Otherwise, we will delete that edge type from the neighbor
the walks totally randomly inside the user-defined subgraph of the node and choose another random edge and check its
rather than having biased walks based on a rigid pattern like validation. We have decided to conduct the walk based on
the previous mentioned methods. The walks are valid as long 40 steps (walk_length = 40). We should repeat this scenario
as they are within the subgraph and invalid otherwise. for 40 walks (number_of_walks = 40) and these walks are
written to a file. Both walk-length and number_of_walks can
be modified by the user. After traversing all the walks and
writingthemonthefile,weobtainthewalkfilewhichcontains
IV. METHODOLOGY
all the walks and their steps and then we should embed
these walks. Embedding, in simple words is translating high-
dimensionalvectorstoalow-dimensionalspace.Toembedthe
Our algorithm runs with any user-given schema subgraph (s)
walksofthewalkfile,wecanconsidereachwalkasasentence
based on the edges. The schema subgraph is entered in the
of the words in which each node is considered as one word.
form of integers where each integer denotes an edge of the
Therefore,wecangethelpfromtheword2vecmodelinwhich
KG. This schema subgraph which defines the subgraph (S)
a neural network learns node embedding from a corpus.
is actually a part of the original graph (G). Let’s assume the
In this paper, we are using a modified version of the skip
user enters a schema subgraph such as s = ’x ,x ,x ’ where
1 2 3 gram model which captures the similarity of the walks based
x isanintegerrepresentinganedgebasedonthedataset.
i=1,2,3 on their types. Skip gram is from the word2vec family and
This schema subgraph defines subgraph S = (V′,E′) ∈ all of the word2vec models are consisting of two-layer neural
G = (V,E) where G is the primary knowledge graph. The networks used for word embedding. In a general sense, in
algorithm chooses the first node inside S randomly and from the skip gram architecture, the model uses the current word
there, uses the below equation to calculate the probability of (input) to predict the surrounding window -usually of size 5
each neighbor edge based on its type (t). To calculate the to 10- of the context words (output). In fact, a skip gram
probability of moving to the next edge based on subgraph S, is trying to find a semantic similarity between the words in
we use this equation: a context by learning a meaningful representation of each
word (embedding) in the document. After feeding the walk
file as an input to the skip gram model and getting the
l
(cid:88) P(ri+1|ri,S)= embedding file, we are able to use it. The embedding file
can be used for various tasks such as link prediction, node
i=1+1(a) ROC on YAGO Dataset, pre- (b) ROC on YAGO Dataset, pre- (c) ROC on YAGO Dataset, pre-
dicted link:is citizen of dicted link:Located In dicted link:Leader of
(d) ROC on NELL Dataset, pre- (e) ROC on NELL Dataset, pre-
dicted link:competes with dicted link:plays against
Figure 1: Comparing ROC of different links of Subgraph2vec, Regpattern2vec and Metapath2vec on NELL and YAGO.
Table I: Statistics of split data (MST method) based on the
classification, community detection and etc. In this work, we
relation to be predicted [14]
use the embedding file for link prediction.
Dataset Relations Train est Test set
B. Application
CompetesWith 9,154 1,070
Inthispaper,wedecidedtomoveforwardwiththelinkpre- NELL
PlaysAgainst 2,945 2,225
diction task. For conducting link prediction, we need to train
isLocatedIn 44,542 44,541
our model first. We do it by using the vector representation YAGO
isCitizenOf 3,128 342
of the current edges in the graph which is considered as the isLeaderOf 855 106
positive example. Therefore, for negative examples, we can
consider combining pairs of edges in the graph that are not
connected.Bothofthepositiveandnegativeexamplesareused
B. Experimental Setup
to train the classification model. We use Logistic Regression
astheclassifier,whichcanbeusedforlinkpredictionaswell.
To apply random walks in Subgraph2vec, we set the num-
ber_of_walks=40andmaximumwalk_length=40.Also,the
V. EXPERIMENTS
logistic regression parameters are constant for all the datasets.
For both of our datasets, we split the dataset in order to
In this section, we will evaluate the conductance of the
train and test for any relation we want to predict in either of
subgraph2vec method by running it on different datasets.
our datasets. Note that in our model, it is necessary to have
the pair of the nodes we want to do prediction on in both the
A. Dataset
training and the test dataset. However, the relation between
We use two different datasets to evaluate our model. that pair is different in each of the train and test dataset. To
(i) The first dataset is YAGO39K [16], which includes data achievethisgoal,weapplyminimumspanningtreemethodto
from Wikipedia, WordNet and GeoNames and is a subset of take the minimum possible nodes from the graph to prepare
the YAGO knowledge base [5]. It contains 123,182 unique the test dataset.
entities and 1,084,040 unique edges with 37 different relation We have split the dataset for each of the relations we want
types. (ii) The second dataset used was NELL, which is built to predict individually. Table 1 illustrates the number of the
from the Web via an intelligent agent and contains 49,869 rows for each dataset (split with MST) based on the relation
unique nodes 296,013 edges and 827 relation types. to be predicted.C. Link Prediction REFERENCES
To explain the link prediction, we will give a brief explan- [1] Antoine Bordes et al. ‘Learning structured embeddings
ation for each dataset. For the YAGO dataset, we decided to of knowledge bases’. In: Proceedings of the AAAI
predict these relations: ’isLocatedIn’, ’isCitizenOf’, ’isLead- conference on artificial intelligence. Vol. 25. 1. 2011,
erOf’. Here is S defined for these relations: pp. 301–306.
For ’isLeaderOf’, S consists of these edges: ’PlayisIn’, [2] Antoine Bordes et al. ‘Translating embeddings for
’isLeaderOf’ and ’isLocatedIn’. For ’isCitizenOf’, S consists modeling multi-relational data’. In: Advances in neural
oftheseedges:’isCitizenOf’,’isLocatedIn’and’isLocatedIn’. information processing systems 26 (2013).
For ’isLeaderOf’, S consists of these edges: ’isLeaderOf’, [3] Xuelu Chen, Ziniu Hu and Yizhou Sun. ‘Fuzzy logic
’isLocatedIn’ and ’wasBornIn’. based logical query answering on knowledge graphs’.
For the NELL dataset, we chose two relations of In: Proceedings of the AAAI Conference on Artificial
interest: ’competesWith’ and ’playsAgainst’. For ’com- Intelligence. Vol. 36. 4. 2022, pp. 3939–3948.
peteswith’, S consists of :’Competeswith’, ’hasofficeincity’ [4] Zhe Chen et al. ‘Knowledge graph completion: A
and ’cityhascompanyoffice’. For ’playsAgainst’, S consists of review’. In: Ieee Access 8 (2020), pp. 192435–192456.
:’teamplaysagainstteam’orinshort’playsagainst’whichisthe [5] PA Chirita et al. WWW’07: Proceedings of the 16th
link to be predicted, ’teamplaysinleague’ and ’sportsgamet- International Conference on World Wide Web. 2007.
eam’. [6] Kenneth Ward Church. ‘Word2Vec’. In: Natural Lan-
Figure 1 shows the prediction results on the test data- guage Engineering 23.1 (2017), pp. 155–162.
sets. Here, we are comparing the results of our method [7] YuxiaoDong,NiteshVChawlaandAnanthramSwami.
to the results we obtained from running regpattern2vec and ‘metapath2vec:Scalablerepresentationlearningforhet-
metapath2vecalgorithmsusingNellandYAGOdatasets.Our erogeneous networks’. In: Proceedings of the 23rd
results imply that our method outperforms the regpattern2vec ACM SIGKDD international conference on knowledge
and metapath2vec methods in most cases. That is due to discovery and data mining. 2017, pp. 135–144.
being capable of choosing the nodes/edges randomly within [8] Fan Fan et al. ‘A Graph Neural Network Model with a
the subgraph rather than choosing them based on a regular Transparent Decision-Making Process Defines the Ap-
expression. Figure 1 illustrates the ROC curve from each of plicability Domain for Environmental Estrogen Screen-
the algorithms. ing’. In: Environmental Science & Technology 57.46
(2023), pp. 18236–18245.
VI. CONCLUSIONANDFUTUREWORK
[9] Palash Goyal and Emilio Ferrara. ‘Graph embedding
techniques, applications, and performance: A survey’.
In this paper, we present Subgraph2vec, a random walk-based In: Knowledge-Based Systems 151 (2018), pp. 78–94.
method in which a subgraph is used for limiting random [10] Aditya Grover and Jure Leskovec. ‘node2vec: Scalable
walks on a knowledge graph in a generic fashion. There feature learning for networks’. In: Proceedings of the
are different random walk-based methods for embedding 22ndACMSIGKDDinternationalconferenceonKnow-
knowledge graphs such as node2vec, metapath2vec and ledge discovery and data mining. 2016, pp. 855–864.
regpattern2vec which were discussed earlier. In the previous [11] Will Hamilton, Zhitao Ying and Jure Leskovec. ‘In-
methods, random walks are biased based on different ductive representation learning on large graphs’. In:
algorithms such as BFS and DFS in node2vec or fixed Advances in neural information processing systems 30
patterns in metapath2vec and regpattern2vec. However, our (2017).
goal is to implement an algorithm which runs the random [12] Elwin Huaman. ‘Steps to Knowledge Graphs Qual-
walks inside on a user-given subgraph; which makes the ity Assessment’. In: arXiv preprint arXiv:2208.07779
embedding algorithm more broad. The generated walk file is (2022).
embedded using skipgram and the resulted embedding files [13] Zan Huang, Wingyan Chung and Hsinchun Chen. ‘A
can be used for various purposes such as node classification, graph model for E-commerce recommender systems’.
linkprediction,communitydetectionandetc.Inthiswork,we In: Journal of the American Society for information
decidedtouseitforlinkprediction.OurresultsonNELLand science and technology 55.3 (2004), pp. 259–274.
YAGO datasets prove our method outperforms other methods [14] Abbas Keshavarzi, Natarajan Kannan and Krys Ko-
such as regpattern2vec and metapath2vec in most cases. In chut. ‘RegPattern2Vec: link prediction in knowledge
the future work, we will use the obtained embedding file for graphs’. In: 2021 IEEE International IOT, Electronics
the mentioned tasks and also in another work we will add and Mechatronics Conference (IEMTRONICS). IEEE.
weights to the edges to make our model more customizable. 2021, pp. 1–7.
[15] Michael Ley and Patrick Reuther. ‘Maintaining an On-
line Bibliographical Database: The Problem of Data
Quality.’ In: EGC. Citeseer. 2006, pp. 5–10.[16] Xin Lv et al. ‘Differentiating concepts and instances ceedings of the AAAI conference on artificial intelli-
for knowledge graph embedding’. In: arXiv preprint gence. Vol. 33. 01. 2019, pp. 7370–7377.
arXiv:1811.04588 (2018). [31] Muhan Zhang et al. ‘Labeling trick: A theory of using
[17] Rongrong Ma et al. ‘Deep graph-level anomaly detec- graph neural networks for multi-node representation
tion by glocal knowledge distillation’. In: Proceedings learning’. In: Advances in Neural Information Pro-
of the fifteenth ACM international conference on web cessing Systems 34 (2021), pp. 9061–9073.
search and data mining. 2022, pp. 704–714.
[18] Xiaoxiao Ma et al. ‘A comprehensive survey on graph
anomalydetectionwithdeeplearning’.In:IEEETrans-
actions on Knowledge and Data Engineering 35.12
(2021), pp. 12012–12038.
[19] Bryan Perozzi, Rami Al-Rfou and Steven Skiena.
‘Deepwalk: Online learning of social representations’.
In:Proceedingsofthe20thACMSIGKDDinternational
conference on Knowledge discovery and data mining.
2014, pp. 701–710.
[20] Axel Polleres et al. ‘How does knowledge evolve in
open knowledge graphs?’ In: Transactions on Graph
Data and Knowledge 1.1 (2023), pp. 11–1.
[21] Ridho Reinanda, Edgar Meij, Maarten de Rijke et al.
‘Knowledge graphs: An information retrieval perspect-
ive’. In: Foundations and Trends® in Information Re-
trieval 14.4 (2020), pp. 289–444.
[22] Apoorv Saxena, Aditay Tripathi and Partha Talukdar.
‘Improving multi-hop question answering over know-
ledge graphs using knowledge base embeddings’. In:
Proceedings of the 58th annual meeting of the asso-
ciation for computational linguistics. 2020, pp. 4498–
4507.
[23] Michael Sipser. ‘Introduction to the Theory of Com-
putation. Cengage Learning’. In: International edition
(2012).
[24] Xiaohui Tao et al. ‘Mining health knowledge graph for
health risk prediction’. In: World Wide Web 23 (2020),
pp. 2341–2362.
[25] Anton Tsitsulin et al. ‘Graph clustering with graph
neural networks’. In: Journal of Machine Learning
Research 24.127 (2023), pp. 1–21.
[26] Ruben Verborgh et al. ‘Triple pattern fragments: a low-
costknowledgegraphinterfacefortheweb’.In:Journal
of Web Semantics 37 (2016), pp. 184–206.
[27] Yiwei Wang et al. ‘Mixup for node and graph classi-
fication’. In: Proceedings of the Web Conference 2021.
2021, pp. 3663–3674.
[28] Wikipedia contributors. Path (graph theory) — Wiki-
pedia, The Free Encyclopedia. https://en.wikipedia.
org / w / index . php ? title = Path _ (graph _ theory )
&oldid=1191420024. [Online; accessed 16-February-
2024]. 2023.
[29] CarlYangetal.‘Meta-graphbasedhinspectralembed-
ding: Methods, analyses, and insights’. In: 2018 IEEE
International Conference on Data Mining (ICDM).
IEEE. 2018, pp. 657–666.
[30] Liang Yao, Chengsheng Mao and Yuan Luo. ‘Graph
convolutional networks for text classification’. In: Pro-