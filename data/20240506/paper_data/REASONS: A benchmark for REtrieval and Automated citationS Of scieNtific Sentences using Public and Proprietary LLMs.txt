REASONS: A benchmark for REtrieval and Automated citationS Of
scieNtific Sentences using Public and Proprietary LLMs
DeepaTilwani YashSaxena AliMohammadi
AIInstitute,USC,SC,USA UMBC,MD,USA UMBC,MD,USA
dtilwani@mailbox.sc.edu yashsaxena2111@gmail.com m294@umbc.edu
EdwardRaff AmitSheth SrinivasanParthasarathy
UMBC,MD,USA AIInstitute,USC,SC,USA OSU,OH,USA
BoozAllenHamilton amit@sc.edu srini@cse.ohio-state.edu
edraff1@umbc.edu
ManasGaur
UMBC,MD,USA
manas@umbc.edu
Abstract tributesvaluableinsightsintothereliabilityof
RAGforautomatedcitationgenerationtasks.
Automatic citation generation for sentences
in a document or report is paramount for in-
1 Introduction
telligenceanalysts,cybersecurity,newsagen-
cies,andeducationpersonnel. Inthisresearch, ThedevelopmentofLLMsmarksasignificant
weinvestigatewhetherlargelanguagemodels advancementincomputationallinguisticsandarti-
(LLMs) are capable of generating references ficialintelligence(AI)(TamkinandGanguli,2021).
basedontwoformsofsentencequeries: (a)Di-
LLMs,suchasOpenAI’sGPTseries,haveshown
rectQueries,LLMsareaskedtoprovideauthor
remarkable capabilities in text generation (Zhao
namesofthegivenresearcharticle,and(b)In-
etal.,2023),andquestion-answeringsystems(Ra-
directQueries,LLMsareaskedtoprovidethe
sooletal.,2023;Elgedawyetal.,2024). However, titleofamentionedarticlewhengivenasen-
tencefromadifferentarticle. Todemonstrate theirlimitationsbecomeapparentastheybecome
where LLM stands in this task, we introduce more integrated into various domains, including
a large dataset called REASONS comprising defense(Schwinnetal.,2023),newsmedia(Fang
abstracts of the 12 most popular domains of etal.,2023),andeducation(Yanetal.,2024;Hung
scientificresearchonarXiv. From∼20Kre-
etal.,2023;Augensteinetal.,2023). Thecritical
searcharticles,wemakethefollowingdeduc-
issue is their propensity to generate hallucinated
tionsonpublicandproprietaryLLMs:(a)State-
sentencesandpropagatefactuallyinaccuratepieces
of-the-art,oftencalledanthropomorphicGPT-4
ofinformationwithoutreference(Jietal.,2023;
andGPT-3.5,suffersfromhighpasspercentage
(PP)tominimizethehallucinationrate(HR). Rawteetal.,2023). Theseinaccuraciesdiminish
WhentestedwithPerplexity.ai(7B),theyun- themodels’reliabilityanderodeusers’trust,avital
expectedlymademoreerrors;(b)Augmenting componentintheirwidespreadadoption.
relevantmetadataloweredthePPandgavethe
CommercialLLM-basedsearchsystems,includ-
lowest HR; (c) Advance retrieval-augmented
ing Bing Search-powered GPT 4 (Mehdi, 2024)
generation(RAG)usingMistraldemonstrates
and Perplexity.ai (Roose, 2024), are still not
consistent and robust citation support on in-
capable enough of resolving the issue of citation
direct queries, and matched performance to
GPT-3.5 and GPT-4. The HR across all do- generation to confirm the scientific feasibility of
mainsandmodelsdecreasedbyanaverageof eitherageneratedsentence(s)orgivensentence(s)
41.93%, and the PP reduced to 0% in most from the scientific literature. For instance, Fig-
cases. In terms of generation quality, the av-
ure1showshowproprietaryLLMsrespondtothe
erage F1 Score and BLEU were 68.09% and
zero-shot indirect query. It is evident from the
57.51%,respectively;(d)Testingwithadversar-
figurethatwhilegeneral-purposeLLMslikeGPT-
ialsamplesshowedthatLLMs,includingthe
3.5andGPT-4‘pass’thequery,task-specificLLM
AdvanceRAGMistral,struggletounderstand
context,buttheextentofthisissuewassmall Perplexity does generate relevant citations but
inMistralandGPT-4-Preview. Ourstudycon- still shows hallucination. Consider the following
4202
yaM
3
]LC.sc[
1v82220.5042:viXrabyaddressingthesechallengeswiththefollowing:
(A)IntroduceREASONS,adatasetcreatedbyex-
tractingrelatedworksfromIEEEarticlesspanning
12scientificdomainsfrom2017to2023. (B)We
employ a new RAG training regime to develop
Advance RAG. Advance RAG and Naïve RAG
examinethefactualintegrityoftheinformationre-
trievedbydenseretrieversanditspresentationasci-
tationsbyLLMs. (C)Weevaluatebothproprietary
andpublicLLMsandtheirRAGcounterparts(10
models)toassesstheircontextualawarenessusing
metrics like Pass Percentage (PP) and Hallucina-
tionrate(HR).Additionally,wehavemeasuredthe
qualityofcitationgenerationusingF-1andBLEU
scores. (D)Weconductanadversarialexamination
toprovideaclearassessmentofcontextawareness
Figure 1: An illustration and motivating example for in- regardingcitationgenerationinLLMs.
vestigating LLMs for automatic citation generation task.
Findings:(I)Perplexity,facesamajorchallenge
Perplexity.ai,whichisanLLM-basedsearchengine,yields
acitationthatdoesn’texist[1],anincorrectone[3],anda whendealingwithindirectanddirectqueryonthe
correctcitation[2].AdvanceRAG(definedinthisresearch) REASONSdataset(Figure2-Figure5,andinAp-
improvedcontextunderstandingandcitationgenerationqual-
pendixATable6-Table9).(II)Citationgeneration
ity.Time:Feb.05,2024.
is enhanced uniformly across public and propri-
etaryLLMswhenmetadatalikeabstractandtitle
threerealworldexamplesofthisresearch: are considered with indirect query (Figure 3 and
CitationGenerationinResearchArticlesandNews Figure5,alongwithTable7andTable9). (III)Ad-
Reports: LLMscangeneratehighlypersuasiveand vanceRAGwithMistralLLMoutperformsother
realisticcontent,especiallyinwritingresearchar- competitive proprietary and public LLMs. This
ticles or news reports, making it challenging for performanceisrealizedbyareductionintheHR
userstodistinguishbetweengenuineandfabricated andincrementsinF-1andBLEUscores(Figure3
information Nakano et al. (2021); Menick et al. and Figure 5 (last two bars) and Table 7 and Ta-
(2022);KumarageandLiu(2023). ble9(lasttwocolumns)). (IV)Fordomainssuch
asQuantumComputingandBiomoleculesthatare
CitationGenerationinReportsforOrganizational
heavy in mathematics and numerals, there was a
Cybersecurity: Incybersecurity, wheredecisions
substantial decline in citation generation quality
often need to be made quickly and are based on
and an increase in HR. Adversarial examination
the data provided, the accuracy and reliability of
strengthens our understanding that despite being
information are paramount (Divakaran and Ped-
exorbitantly large, LLMs lack context awareness
dinti,2024). Inaccuratecitationscanleadtomisin-
(Table2). (V)AdvanceRAGdidprovideconvinc-
formationandpotentiallysevereconsequencesin
ing evidence of context understanding (Table 2).
decision-making processes. LLMs can automate
FurtherimprovementsinRAG-basedLLMsarede-
thecitationgenerationprocessbutneedtobecare-
sirable,andutilizingREASONSdatasetcanprovide
fullydesignedfororganizationspecificcybersecu-
valuable insights into context understanding and
rity.
provenanceintaskssuchashypothesisgeneration.
Citation Generation in Reports for Legal: In a
2 Background
significantevent,anattorneytriedemployingChat-
GPT for legal analysis during a trial (see subsec- EarlyTechniquesinCitationRecommendation:
tionA.1)(Bohannon,2023). WhileChatGPTgen- The practice of citing sources is a cornerstone of
eratedinformation,itfailedtocapturethenuanced academicandprofessionalwriting,servingasthe
complexitiesandcriticallegalprecedentsneeded bedrock for reliability, and truthfulness in schol-
for the case. This underscores the importance of arlywork(Cronin,1981). Theevolutionofcitation
confirming and sourcing accurate legal citations recommendation systems mirrors the broader ad-
andprecedentsrelevanttothecase. Wecontribute vancements in computational linguistics and nat-urallanguageprocessing(NLP)(Baietal.,2019; (Honovich et al., 2022; Yue et al., 2023) have fo-
Alietal.,2021). Initialmethodsincitationrecom- cusedonautomatingAIS.Concurrentworkby(Liu
mendationfocusedonbasictechniquessuchastext etal.,2023)exploredhumanevaluationofcommer-
feature-basedsystems(Strohmanetal.,2007),sim- cialgenerativesearchenginessuchasBing. Chat,
plekeywordmatching,andbasicstatisticalmeth- NeevaAI,Perplexity.ai,andYouChat.
ods(BethardandJurafsky,2010). Context-aware
Despite these advancements, LLMs in citation
citation recommendation systems supplemented
recommendationstillstrugglewithgeneratingac-
these methods (He et al., 2010; Ebesu and Fang,
curate information and providing references, as
2017;Jeongetal.,2020a;Huangetal.,2021). How-
shown in studies by (Ji et al., 2023; Zheng et al.,
ever,theirinabilitytograspdeepertextualcontexts
2023).
limitedtheireffectiveness.
Weconductempiricalandinvestigativeresearch
MachinelearninginCitationRecommendation
on why public and proprietary LLMs, including
The incorporation of machine learning into ci-
thepowerfulGPT-4(whichhasnotbeenexamined
tationrecommendationsystemsrepresentsanini-
yet),arepronetoincorrectcitationgeneration. Fur-
tial step toward automating the citation process,
ther,weprovidemeansforimprovingthecitation
which is typically regarded as manual and labor-
generationinpublicLLMsthroughacustomized
intensive(Agarwal et al., 2005; Küçüktunç et al.,
designusingRAG.
2012). These systems began to exhibit an im-
Thislimitationnecessitatesanapproachclosely
provedunderstandingofthetext,althoughtheystill
aligning with RAG. RAG compels LLMs to pro-
lackedanuancedgraspofcomplexcontexts(Tran
vide citations alongside the generated text. The
et al., 2015). The application of neural networks
conceptofretrieval-augmentedLLMshasgained
revolutionizedcitationrecommendation. NLPal-
tractioninrecentyearsfollowing(Guuetal.,2020;
gorithms, capable of parsing complex sentence
Borgeaudetal.,2022;Izacardetal.,2022;Khan-
structures,startedidentifyingrelevantthemesfor
delwaletal.,2019;Schicketal.,2023;Jiangetal.,
contextuallyappropriatecitationrecommendations
2023b; Yao et al., 2022; Gao et al., 2023). We
(ZarrinkalamandKahani,2013;Beeletal.,2016;
evaluate public and proprietary LLMs and their
Iqbaletal.,2020). Concurrently,graph-basedmod-
RAG counterparts on citation generation using
els, visualizing literature as interconnected net-
REASONS, a meticulously curated dataset from
works,enhancedcitationrecommendationsbycon-
arXivspanningkeydomainsincomputerscience
sideringcontentsimilarityandcitationpatterns(Ali
and related fields. This allows us to assess the
etal.,2020;Chakrabortyetal.,2015). Withdeep
LLM’sabilitytoidentifyagivensentence’ssource
learning,citationrecommendationsystemsbegan
accurately.
incorporatingsemanticanalysis,employingmod-
elslikewordembeddingsandneuralnetworksfor
amorenuancedunderstanding(Yangetal.,2018;
Domain Paper IEEE Citation
Bhagavatula et al., 2018; Vajdecka et al., 2023). Count Papers Count
Adaptedfromcommercialuse,collaborativefilter-
CV 5488 1028 3437
ing also emerged, recommending citations based Robotics 3656 292 776
onsimilarcitationbehaviors(Wangetal.,2020). Graphics 1796 384 1417
IR 1741 564 1654
LargeLanguageModelsinCitationGeneration: AI 1697 531 2021
NLP 1526 293 1092
The advent of LLMs like GPT-3 and its succes-
Cryptography 1084 371 1106
sorshasfurthertransformedNLP.Initiallanguage
NNC 892 111 326
modelsystemssuchasthosebasedonBERThave HCI 761 112 229
Databases 723 115 182
significantly improved citation recommendation
QC 421 126 456
by converting unstructured text into meaningful Biomolecules 119 17 27
vectors (Jeong et al., 2020b; Devlin et al., 2018;
Total 19904 3944 12723
Bhowmick et al., 2021). Recent studies have fo-
Table1:Ourbenchmarkdataset,REASONS,includespapers
cusedonevaluatingthefidelityofgeneratedtextto and sentences from 12 domains. It primarily features ten
itssources(Jietal.,2023). (Rashkinetal.,2023) domainsincomputerscienceand2inbiology.Fullformsof
domainacronymsareprovidedinsubsectionA.5.
introducedthe“attributabletoidentifiedsources”
(AIS)score,while(Bohnetetal.,2022)andothers3 ProblemSetup associatedcostsareprovidedin(subsectionA.2).
Scope of REASONS: The dataset comprises sen-
tencesgatheredfromtherelatedworksectionsof
articlesincomputerscienceandbiologyavailable
JSONStructure
on arXiv (arX). Summary is provided in Table 1.
It should be noted that GPT-3.5 or its successors {"Computer Vision": {
"http://arXiv.org/abs/2012.05435v2": {
may have processed all the papers published on "Paper Title": "Optimization-Inspired..",
"Sentences": [
arXivfrom2017to2021whiletraining. Toensure
{"Sentence ID": 32,
our dataset is unbiased, we include papers pub- "Sentence": "... For GM, ... ",
"Citation Text": "C. Ledig,...",
lishedin2022and2023thattestthememoryand "Citation": {
"Citation Paper ID": "arXiv:1609.04802",
understanding of LLMs. Exclusions were made "Citation Paper Title": "Title:Photo..",
formathematics,statistics,andphysicsduetothe "Citation Paper Abstract": "Abstract:.",
"Citation Paper Authors": "Authors:..." }}]}}}
abundanceofequationsintherelatedworksection,
and the crawling method theoremKb1 lacked the
requiredversatility. WechosetofocusonIEEEpa-
3.1 ProblemFormulation
persastheyarerepresentedacrossall12domains
weconsidered. Eachsentenceintherelatedwork
WedefinetwotasksforLLMsovertheREASONS
sectionencapsulatestheauthor’sthoughtprocess
dataset R: (a) Direct Querying and (b) Indirect
in citing related works: (A) Every sentence cap-
Querying. Forexperimentation,wesegmentRinto
tures the author’s interpretation and emphasis on
R andR . R representssentencesandpaper
originalmethodology,critiqueofpriorwork,cor- S M S
titlesforwhichreferencesaretobegeneratedwith
rectionstopreviousresearch,oracknowledgment
orwithoutthesupportfrommetadataR .
ofpioneers. Thisencompassessummarizingthese M
aspectsbrieflyandconcisely. (B)Thecitedwork
Direct Querying Task: Given a title t ∈ R ,
in the related work section is either incidental or i S
the LLM should generate the author list. For the
importanttocurrentwork(Valenzuelaetal.,2015).
taskofdirectqueryingwithmetadata,theLLMis
REASONS is inspired by previously constructed
giventhefollowinginput: t ∈ R ,theAdvance
s2ORC and UnarXive datasets containing aca- i S
RAGmodelretrievestop-40chunksofinformation
demic papers (see Table 4 in Appendix A); how-
a ,...,a ∈ R ,andgeneratesthenames.
ever,wedivergeonthefollowingpoints: (A)We i1 i40 M
provide sentence-level annotation of citations on
Indirect Querying Task: Given a sentence
majorcomputationaldomainsonarXiv. (B)Each
s ∈ R , the LLM should generate a paper ti-
sentence is accompanied by its metadata, which i S
tle in zero-shot setting. For the task of indirect
includesthepapertitle,abstract,andauthornames
queryingwithmetadatacalledSequentialIndirect
ofthepaperitcites. Italsocontainsthetitleofthe
andDirectPrompting(SIDPrompting),theLLM
paper from which it was taken. (C) The dataset
isgiventhefollowinginput: s ∈ R andground
structureallowsforaneasyexaminationofLLMs i S
truth abstract abs ∈ R as well as the authors
usingindirectanddirectqueries. s M
au ∈ R ,andthemodelisaskedtogeneratethe
Crawling Process: The web crawler employs s M
citationpapertitle.
theOxylabs2 SERPScraperAPIasitsmethodol-
ogy,enablingreal-timedataextractionfrommajor
Examplesofdirectandindirectqueriesare:
searchengines. ThisAPIoffersaproxychaining
platformforefficientdataextraction. Thedataset
ismeticulouslyorganizedinJSONformatwitha
detailed outline (see “JSON Structure”). A com- DirectPrompt
pleteGitHubrepositoryisprovided,containingthe
Prompt:Whoweretheauthorsoftheresearchpaper
datasetandthecodeforreproducibility(seedetails "ResearchPaperTitle"?
insubsectionA.3). Weplantokeepupdatingthe Instruction:Listonlyauthornames,formattedas<
firstname><lastname>,separatedbycomma.
repository with more articles and metadata. The
Donotmentionthepaperinthetitle,also,ifyoudon’t
know,write’pass’.
1https://github.com/PierreSenellart/theoremkb Response:AuthorNames.
2https://oxylabs.io/IndirectPrompt the field, Preplexity.ai focuses on models with
uniquefunctionalities,suchasrecommendingcita-
Prompt: Ihavetakenasentencefromtheresearch
tionsandutilizingnaturallanguagepredictionfor
papertitled“ResearchPaperTitle”,givemethere-
searchpaperthatthissentenceisciting.Ifyoucannot innovativesearchexperiences.
come up with the paper titles, write ‘pass.’ Don’t
PublicModels: WechooseLLAMA2(Touvron
writeanythingelse.
Instruction:Sentence"usesfractionalmax-pooling et al., 2023) and Mistral (Jiang et al., 2023a) as
torandomlyspecifynon-integerratiosbetweenthe thetwopubliclyavailableLLMsthathavedemon-
spatialdimensionsizesoftheinputandtheoutputto
stratedcompetitiveperformancecomparedtopro-
poolinglayers."
Response:CitationPaperTitle. prietary LLMs. We evaluate their effectiveness
ontheREASONSdatasetunderthestandardstate
Implementation of Direct and Indirect Query- andretrieval-augmentationconditions. Thisanal-
ing: Direct querying is executed using zero-shot ysis goes beyond simply comparing proprietary
prompting for scenarios without metadata and and public models, extending to evaluating mod-
chain-of-thoughts prompting for metadata situa- elsbasedontheirsize,particularlythosewith7B
tions. Wemodifythechain-of-thoughtsprompting parameters.
with SID Prompting. It begins with an indirect 3.3 EvaluationMetrics
query. Followinganincorrectresponseora‘pass,’ Our evaluation uses four key metrics: 1) The
more details about the cited paper are given (i.e., BLEU Score assesses the structural alignment
direct query), including its abstract and authors’ through clipped n-gram matching. 2) The F-1
names. Thisisaniterativeapproachtogeneratethe Scoreevaluatesthebalancebetweenprecisionand
correctcitation. Followingarethetwoexamplesof recall,reflectingthemodels’effectivenessincap-
thesepromptingstrategies: turing key information. 3) Hallucination rate
(HR),whichweestimatebyaveragingoverincor-
DirectQuerywithMetadataPrompting
rectandpartiallycorrectgeneratedcitations. HR
Prompt:Whoweretheauthorsoftheresearchpaper = 1 (cid:80)I[cˆ̸= c]+ 1 (cid:80)|Uw|I[cˆ ̸= c ],where
“ResearchPaperTitle"?Letmegiveyousomemore
QD |Uw| w=1 w w
Q : querieswithinadomain,and|U |: totalnum-
contextbyprovidingtheabstractoftheresearchpa- D w
per.Abstract:’....’. berofuniquewordsingeneratedcitation(cˆ)and
Instruction: Listonlyauthornames,formattedas true citation (c). 4) Pass Percentage (PP) mea-
<firstname><lastname>,separatedbycomma.Do
sures the tendency of an LLM to either respond
notmentionthepaperinthetitle.Also,ifyoudon’t
know,write‘pass.’ orabstainfromgivingaresponse. Itiscalculated
Response:AuthorNames. as follows: 1 (cid:80)I[cˆ= Pass]. It is crucial to em-
QD
phasize that PP serves as a safeguard to prevent
SIDPrompting LLMsfromgeneratinghallucinatoryresponsesbut
alsoreducesengagement. Additionally,evenwith
Prompt: Ihavetakenasentencefromtheresearch
papertitled"ResearchPaperTitle."givemethetitle ahighPP,theHRcanbehigh. Thisimpliesthatthe
ofthepossibleresearchpaperthatthissentenceis modelstrugglestodiscernwhetheritofferscorrect
citing. Ifyoucannotcomeupwiththepapertitles,
orincorrectcitationsintheremaininginstances.
write’pass’.Don’twriteanythingelse.
Instruction: Sentence:"......". Let me give you 3.4 RetrievalAugmentedGeneration(RAG)
some more context by providing the authors and
RAG combines a retriever and a generator to
theabstractofthepaperthesentenceisciting. Au-
thors:"......",Abstract:"......." create better answers. RAG can access external
Response:CitationPaperTitle. knowledge, unlike methods that feed the model
prompts. Thisletsitcraftmoreaccurate,relevant,
3.2 ModelsandEvaluation and informative responses than models that rely
Ourresearchhasfocusedonadiversearrayof solelyonwhattheywerepre-trained.
LLMs,carefullychosentoprovideabroadperspec- WeinvestigateRAG’sabilitytoimproveLLMs’
tiveonthecapabilitiesandlimitationsinherentin accuracy. Ideally, RAG would help LLMs avoid
currentlanguagemodeltechnologies. givingwronganswers(lowPP)andmakingthings
ProprietaryModels: Ourselectionofproprietary up(HR).WealsoinvestigatewhetherRAGworks
modelsincludesthosefromOpenAIandPreplex- consistently with direct and indirect questions
ity.ai. WhileOpenAIisknownforitscutting-edge acrossdifferentscientificfields(12domains). We
NLPmodels,drivingsignificantadvancementsin experiment with two forms of RAG architecture:(a) Naïve RAG and (b) Advance RAG. Both ar- 4 Results
chitecturesleveragethesamebi-encoder-basedre- Weconductedexperimentsencompassingfour
trieverarchitecture(Karpukhinetal.,2020). distinct prompting styles applied to twelve scien-
GivenacorpusofdocumentsR M andasentence tific domains. This extensive analysis involved
s ∈ R S, the document encoder maps d ∈ R M 12,723sentences,resultinginasubstantialdataset
to an embedding E θ(c) and the query encoder rigorously evaluated using ten different models.
maps s to an embedding E θ(s). The top-k rele- Thisequatesto508920instanceassessmentsin-
vant documents for s are retrieved based on the volving4(promptingstyles)×12,723(sentences
sentence-document embedding similarity, which foralldomains)×10(models). Thetotalduration
is often computed via dot product: z(s,d) = requiredtoexecuteallexperimentsontheGPUis
exp(E θ(s)TE θ(d)). We start with a bi-encoder 238 days, 6 hours, and 59 minutes. For detailed
retrieverusinganembeddingmodelfromOpenAI information regarding the time spent on experi-
(subsectionA.4). Otherwaystosetupabi-encoder mentsacrossvariousdomains,pleaserefertothe
retriever,suchasDRAGON+(Linetal.,2023),are appendix(seesubsectionA.6andTable5).
possible. However, those are more useful when
Zero-Shot Indirect Prompting: In Figure 4, a
involvinglarge-scaledataaugmentation.
majorityofthemodelsexhibitedhighHR.Asex-
The retrieved documents are ranked in two
pectedforahugemodelGPT-4-1106-preview(1
ways,whichseparatesNaïveRAGfromAdvance
TrillionParameters)showsarelativelylowerHR
RAG.UndertheNaïveRAG,weuseBM25rele-
of67.73%andahigherPPof89%averagedacross
vancescoringtorankthedocuments,whereas,in
12 domains. Perplexity-7b-Chat showed an ex-
Advance RAG, we fine-tune a cross-encoder on
ceptionally high PP of 97.5%, which is surpris-
REASONS document index R to better align it
M ing, as this LLM is designed specifically for ci-
withourtaskofcitationgenerationwithLLM.For
tation generation. RAG Mistral was a competi-
thefine-tuningofthecross-encoder,weuselocal-
tive model with GPT-4 with a lower PP of 21%
izedcontrastiveloss(LCL)fortworeasons: (a)In
andHRof72.49%incomparisontootherLLMs.
R ,wedonothavelabeledpositiveandnegative
M Analysis shows RAG Mistral is competitive be-
documents,and(b)forasentencesthereisapos-
cause of the high variance in HR compared to
sibilityformorethanonetruepositivedocuments
GPT-4-1106-preview. Generation quality mea-
(Pradeepetal.,2022). LCLisformallydefinedas
suredbyF-1andBLEUscoreswerepredominantly
follows:
lowacrosstheboard,withGPT-4(notthepreview,
G1)comparativelybetterscores. RAGMistraland
exp(z )
s,{d+}
L LCLs := −log (cid:80)
exp(z )
RAGLLAMA2ranksecondandthirdbestrespec-
d∈Gs s,d tively.
1 (cid:88) SID Prompting In Figure 5, showed improve-
L := L
LCL
|S|
LCLs ment across all the LLMs in citation genera-
s∈Rs,Gs∈Rs
M tion over indirect queries. An average improve-
where G represents a set of documents for a ment of 21% was measured, with a reduction
s
sentence s, which consist of a set of relevant in variance. Even though some models like
documents ({d+}) and n-1 non-relevant docu- Perplexity-7b-ChatandLLAMA2stillhadhigh
ments {d−} sampled from Rs using biencoder. HRrates,thePPdroppedsignificantly,especially
M
The training of Advance RAG happens through for GPT-4-1106-preview. The results of this ex-
the standard cross entropy loss: L (cˆ|s,ϕ) = perimentindicatethatSIDpromptinginLLMscan
CE
(cid:80)b I(cˆw = cw)·logPr(cˆw|ϕ) where, ϕ is pa- balancethetrade-offbetweenPPandHR,signifi-
i=1 i i i
rameter of the generator LLM and b is the mini- cantlyenhancinggenerationqualitywithan(8%↑)
batch fine-tuning in Advance RAG. cˆ represents increase in BLEU and a (13%↑) in F-1 (The Ap-
i
ith citationgeneration,andI(cˆw = cw)represents pendixBprovidesexamplesforvisualinspection.).
i i
wordlevelcomparisonwithgroundtruthcitation Zero-Shot Direct Prompting presents a very
(directquery: authornames;indirectquery: paper idealistic scenario where the LLMs have access
titles). Forthe Naïve andAdvanceRAG, weem- to context through direct query. This leads to
ployLLAMA-27BandMistral7Bascompetitive both lower PP and HR. The citation generation
modelsagainstproprietaryLLMs. quality significantly improves from zero-shot in-HallucinationRate F-1Score BLEUScore PassPercentage
0.8 0.8
100
100
0.6 0.6
50 0.4 0.4 50
0.2 0.2
0 0 0 0
G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M
Figure2:AveragedZero-ShotDirectPromptingresultsofdifferentLLMsacrossall12domains.G1showsnotablylowerHR
andhigherF-1andBLEUscores,indicatingsuperiorperformanceingeneratingcitations.Incontrast,modelPexhibitsthehighest
HRandthelowestscoresinF-1andBLEU,suggestingchallengesingeneratingaccurateandcontextuallyrelevantcitations.
TheRAGmodels(RMandRL)demonstratevariedresults,withRMshowingabetteraccuracyandcoherencebalancethanRL.
G1:gpt-4-1106-preview,G2:gpt-4,G3:gpt-3.5-turbo,P:pplx-7b-chat,RM:Naïve RAG mistral-7b-instruct,M:
mistral-7b-instruct,RL:Naïve RAG llama-2-7b-chat,L:llama-2-7b-chat,AL:Advance RAG llama-2-7b-chat,
AM:Advance RAG mistral-7b-instruct.Forthepurposesofclarityandsavingspace,thetermsALandAMareusedinthe
figurestodenoteAdvanceRAGllama-2-7b-chatandAdvanceRAGmistral-7b-instruct,respectively.Inthemaintextofthe
paper,thesearereferredtoasAdvRAG(L)andAdvRAG(M).
HallucinationRate F-1Score BLEUScore PassPercentage
100 1 1
1
50 0.5 0.5
0.5
0 0 0 0
G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M
Figure3:AveragedDirectPromptingwithMetadataresultsofdifferentLLMsacrossall12domains.Theplotindicatesthat
modelsG1,G2,andG3standoutwiththeirlowHRandimpressiveF-1andBLEUscores,incontrasttoothermodelsthatface
challenges.AllmodelsexceptRMreacha0%PP,suggestingthatincludingmetadatasignificantlyenhancestheircontextual
understanding.
HallucinationRate F-1Score BLEUScore PassPercentage
0.6 0.6
100 100
0.4 0.4
50 0.2 0.2 50
0 0 0 0
G1 G2 G3 P RM M RL L G1 G2 G3 P RM M RL L G1 G2 G3 P RM M RL L G1 G2 G3 P RM M RL L
Figure4:AveragedZero-ShotIndirectPromptingacross12domains.ThispromptingmethodledtoelevatedHRamongthe
models.TherewasalsoanotablevarianceinPP,withmodelsG3,P,andLexhibitinghigherscores.Bothconditionsindicate
challengesinunderstandingcontextandgeneratingaccuratecitationswhenusingindirectprompts.
HallucinationRate F-1Score BLEUScore PassPercentage
0.8
0.8
100 0.6 100
0.6
0.4
50 0.4 50
0.2 0.2
0 0 0 0
G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M G1G2G3 P RM M RL L AL
A
M
Figure5:AveragedSIDPromptingresultsofdifferentLLMsacrossall12domains.ModelsG1,G2,andG3exhibitrelatively
betteroutcomeswithlowerHRandhigherF-1andBLEUscores,suggestingmorecontextualunderstanding. Othermodels
demonstratedhighHR,indicatingdifficultiesinaccuratecitationgenerationwithSIDPrompting.Notably,whilemodelsG1
andG3havehighPPs,indicatingsomedifficultieswithSID,theiroverallperformancestillreflectsamoreadvancedlevelof
languageprocessingandcontextualcomprehensioncomparedtotheothermodels.
direct and SID promptings, achieving high F-1 ever,Perplexity-7b-Chat,oddly,hadhighPPand
and BLEU scores (see Figure Figure 4). How- HR,suggestinganeedformoreresearchonsuchspecializedLLMsearchengines. Weobservedthat Group PP(%) BLEU F1 HR
Perplexity-7b-Chat expands its search queries
ChangingPaperTitle
andaddsreferencestothebroadercontentitfinds.
G1 96.23 0.6210 0.8470 17.99
Theissueisthattheexpandedversionsdrifttoofar G2 31.45 0.0524 0.2640 83.66
inmeaningfromtheoriginal. G3 68.55 0.0389 0.1828 87.35
RM 3.14 0.0796 0.1584 86.78
In Direct Prompting with Metadata, when M 0.00 0.0003 0.0221 94.95
metadata such as abstracts and titles were used RL 5.03 0.0628 0.1448 87.56
L 0.00 0.0066 0.0254 98.30
withindirectquestions,alltheLLMsgotbetterat
AdvRAG(L) 0.00 0.1322 0.4763 85.72
generatingcitationsandhadlowHRandPP.This AdvRAG(M) 0.00 0.1569 0.5839 75.41
showsthathavingmoreinformationhelpsLLMs ChangingPaperAbstract
createmoreaccurateandrelatedcitations,proving
G1 95.60 0.4595 0.6451 38.49
theimportanceofenoughdataforgoodlanguage G2 32.70 0.0396 0.2186 86.22
G3 76.10 0.0034 0.1013 91.64
processing. Note that PP dropped to zero for al-
RM 7.55 0.0520 0.1216 89.44
most all models when direct promoting includes
M 0.00 0.0074 0.0161 90.20
metadata. AllGPTLLMsachievedF-1andBLEU RL 2.52 0.0445 0.1112 90.16
L 0.00 0.0017 0.0146 99.01
scores close to 1.0 and showed more consistent
AdvRAG(L) 0.00 0.4101 0.5780 39.67
resultsoverall. Twomainpointsfromthisexperi- AdvRAG(M) 0.00 0.4904 0.6954 39.57
mentare: First,addingmetadatatoLLMsiseffec- Table2:PerformanceofvariousLLMsonadversarialset,de-
tive for all of them, especially RAG models that signedbyswappingtitlesandabstracts.ModelsG1,G2,and
G3,possiblyexposedtosimilardataduringtraining,struggled
integrate this augmentation in their learning pro-
withtheadversarialsets,resultinginhighHRandPP.Con-
cess. Second, smallermodelswithadvanceRAG versely,modelslikeAdvRAG(L)andAdvRAG(M)showed
(MistralandLLAMA-2)adjustbettertometadata betterperformance,suggestingthatthesemodelsattemptto
understandthecontextbeforegeneratingthecitations.
thanGPT-4-Preview/4/3.5(seeFigure3).
Overall: AdvanceRAGMistral7boutperformed
othercompetitiveproprietaryandpublicLLMsin
behindtheseexperimentswastoprovidethemod-
all prompting styles. This superior performance
els with incorrect yet similar metadata about the
was notably marked by reduced HR, suggesting
sentencesintheprompts. Theaimwastodiscern
this model is more adept at generating accurate
whether the models generated citations based on
andrelevantresponseswhenaddingmetadata. Fur-
thecontextualgraspoftheprovidedmetadataorif
thermore,improvementsinF-1scoresreinforceits
themetadatahadminimalinfluenceonthecitation
reliabilityinretrievinginformation. HigherBLEU
generationprocess. Theseadversarialexperiments
scoreswereobserved,signifyingthatthelanguage
comprisedtwotypes: 1)Providinginaccuratepa-
outputofthemodelalignscloselywithhuman-like
per titles related to the sentences. 2) Providing
textintermsoffluency&coherence.
incorrectpaperabstractsassociatedwiththesen-
5 AdversarialExamination tences. Both experiments were conducted using
The analysis of LLMs using the REASONS theSIDprompting.
datasethighlightssignificantvariabilityintheirper- To facilitate these experiments, we curated a
formance across different domains. While they subsample of 200 sentences from the REASONS
perform moderately better in areas like AI and dataset spanning all the domains. We extracted
CV with lower HR and higher F-1/BLEU scores, eachsentence’smostsimilarpapertitleorabstract
they struggle in complex domains such as QC, from this dataset and replaced the original meta-
Biomolecules,andCryptography,likelyduetolim- data. Forsimilaritycalculation,weusetheRatcliff-
itedtrainingdataandthecomplexityofthesesub- Obershelpmetric,whichiscalculatedastwicethe
jects. This variability in performance indicates lengthofthelongestcommonsubstringplusrecur-
thatLLMshavevaryingdegreesofcontextualun- sively the number of matching characters in the
derstanding,withatendencytoperformbetterin non-matchingregionsonbothsidesofthelongest
domainswithmoreextensivetrainingdataandless commonsubstring(Tangetal.,2023). According
complexstructures(e.g.,mathsandnumerics). tothismetric,forthefollowingexampletitle“Dif-
MotivationandSetup: Weconductedadversar- fusionmodelsforcounterfactualexplanations,”the
ialexperimentsacrossallmodelstobetterassess bestreplacementis“Octet: Object-awaremodels
theircontextualunderstanding. Thecoreconcept forcounterfactualexplanations(0.736)”asopposedto“Adversarialcounterfactualvisualexplanations is to employ the Toulmin model (Naveed et al.,
(0.638)”. Weconsideredathresholdof0.70effec- 2018))withinAdvanceRAG.Webelievetheseim-
tiveinpreparingtheadversarialset. provements will improve the quality of citation
Findings: We found that incorrect paper titles generationandbetterequipthemodelstomanage
and abstracts easily fool most LLMs if it is sim- complexreasoning(e.g.,hypothesisgenerationand
ilar to accurate information. In Table 2, G1 is verification(TyaginandSafro,2023))challenges
displayed at 17.99%, and its pairing with a high confidently.
PP of 96.23% indicates a defensive mechanism. Limitations
This means the LLMs are not very good at un- Several factors constrain our study on applying
derstanding the true meaning of what they are LLMsforcitationgeneration. (a)Primarily,inte-
given. On such a small adversarial set, we ex- gratinghigh-parameter-sizemodels(>13B;referto
pectLLMslikeGPT-4-1106-previewandGPT-4 Table5forcomputationtime)withRAGisnotfea-
to perform exceedingly well because of their ex- sible,limitingourabilitytoleveragemorecomplex
tensiveknowledge;however,weobservedcounter- models. (b)Additionally,thehighcomputational
intuitive results in Table 2, all models show the resourcesrequiredforsuchmodelsareofteninac-
effect. We do see promising direction with Ad- cessibleinacademicsettings. (c)Oneconstraintin
vRAG(M) and AdvRAG(L); however, further in- ourstudywasthedatasetcreation,wherewecon-
vestigationisrequiredintohowrichgraphicalmeta- finedourselvestopredominantlyIEEEformatpa-
data (e.g., knowledge graph) and graph-theoretic pers,particularlywithdomainswithahighcountof
approaches to information retrieval can improve submissions. (d)Anothersignificantlimitationis
LLMeffectiveness(Heetal.,2024). thecurrentinabilityofLLMstoeffectivelyprocess
6 Conclusion andinterpretmathematicalexpressions, acrucial
aspectinmanyacademicpapers. (e)Duetothelat-
We have developed a new resource called
estversionofGoogleAPI(timestamp: December
REASONS(REtrievalandAutomatedcitationSOf
04, 2023) lacking the citation generation feature,
scieNtificSentences),abenchmarkdesignedtoas-
wehavelimitedourexperimentstoOpenAIonly.
sesstheabilityofLLMstounderstandcontextand
(f)Whilecross-encoderscanbemorepowerfulin
generateappropriatecitations. Thisbenchmarkin-
understanding text relationships, they tend to be
cludessentencesfromtherelatedworksectionsof
more computationally intensive. This is because
papers,alongwithcitationsandmetadataacross12
theyneedtoprocesseverypossiblepairofinputs
scientificandcomputationalfields. Weevaluated
together,whichcanbeasignificantworkload,espe-
proprietaryandpublicLLMs’abilitytocorrectly
ciallyincaseswheretherearemanypotentialpairs
provide author names and paper titles under two
toconsider(likeinlarge-scaleretrievaltasksinour
conditions: direct and indirect citation. Surpris-
REASONSdataset). Theseconstraintshighlightthe
ingly, none of the LLMs demonstrated the readi-
needforadvancementsinmodeladaptability,com-
ness to annotate draft reports in various profes-
putationalresourceaccessibility,datasetdiversity,
sionalsettings,suchasmarketanalysis,misinfor-
andspecializedcontentprocessingformorerobust
mationprevention,defensestrategy,andhealthcare
andwide-rangingapplications.
reporting. Weobservedatrade-offbetweenPPand
EthicalConsiderations
HR, where GPT-4 and GPT-3.5 achieved higher
WefollowedtheOxylabsAcceptableUsePolicy3
accuracy at the cost of a lower HR. In contrast,
andworkedalongsidesomeOxylabsdevelopersto
though smaller with only 7B parameters, the Ad-
ensurewerespectedthetermsofservicesonarXiv.
vance RAG model showed reasonable efficiency.
arXiv’s terms of service place restrictions on au-
Unlikeothermodels,inadversarialtestswhereab-
tomatedcrawlingoftheirsiteforarticlesmarked
stractsorpapertitleswereswapped,AdvanceRAG
by“arxiv.orgperpetual,non-exclusivelicenseand
unexpectedly outperformed GPT-4, suggesting it
CCBY-NC-ND”.Wepaidattentiontothefollow-
doescapturecontextbeforegeneratingcitations.
ingkeyethicalissues: (a)PrivacyandConsent:
Future Work: Through reasoning and explana-
ThecontentonarXivispubliclyavailable,butthe
tion, we plan to explore and mitigate the noted
authorswhouploadtheirworktheremaynothave
shortcomingsincitationgeneration(trade-offbe-
tweenHRandPP,highvarianceinBLEUscores,
3https://oxylabs.io/legal/
sub-parscoresonadversarialset). Oneapproach oxylabs-acceptable-use-policyconsented to having their preprints crawled and recommendation: A survey. Ieee Access, 7:9324–
usedforotherpurposes. It’simportanttorespect 9339.
theprivacyandintellectualpropertyrightsofthere-
Joeran Beel, Bela Gipp, Stefan Langer, and Corinna
searcherswhocontributetoarXiv. Weonlycrawled Breitinger. 2016. Paper recommender systems: a
articlesmarkedasCCZero,CCBY,andCCBY- literature survey. International Journal on Digital
SA.(b)Potentialmisuse: WepreparedREASONS Libraries,17:305–338.
only to test the citation generation capability of
StevenBethardandDanJurafsky.2010. InWhoShould
LLMsforsubsequentfuturedownstreamapplica- ICite? LearningLiteratureSearchModelsfromCi-
tions,suchasannotatingdraftanalyticreports. Our tationBehaviorABSTRACT,pages609–618. [link].
focus on HR and PP for citation generation and
ChandraBhagavatula,SergeyFeldman,RussellPower,
its quality using BLEU and F-1 shows that the and Waleed Ammar. 2018. Content-based citation
datascrapedisnotformaliciouspurposes,suchas recommendation. arXivpreprintarXiv:1802.08301.
fine-tuning LLMs to generate misinformation or
Anubrata Bhowmick, Ashish Singhal, and Shenghui
infringeoncopyrights. (c)TransparencyandAc-
Wang.2021. Augmentingcontext-awarecitationrec-
countability: Wehavebeenmindfulofourcrawl- ommendationswithcitationandco-authorshiphis-
ingprocess,andtothebestofourknowledge,we tory. In 18th International Conference on Sciento-
metricsandInformetrics,ISSI2021,pages115–120.
have enumerated sufficient details regarding the
International Society for Scientometrics and Infor-
process. Thiswouldhelpbuildtrustregardingre-
metrics.
producibility, extend REASONS, and ensure that
MollyBohannon.2023. Lawyerusedchatgptincourt
thecrawlingprocesswasnotabused. (d)Author
andcitedfakecases,ajudgeisconsideringsanctions.
IdentityandContact: Noauthorsofthecrawled
Forbes.
papers were contacted through their provided in-
formation in the publicly available arXiv papers. Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aha-
roni,DanielAndor,LivioBaldiniSoares,Massimil-
Thisuserstudywasdulyapprovedbytheauthors’
ianoCiaramita,JacobEisenstein,KuzmanGanchev,
organization’sInstitutionalReviewBoard(IRB).
JonathanHerzig,etal.2022. Attributedquestionan-
swering:Evaluationandmodelingforattributedlarge
References languagemodels. arXivpreprintarXiv:2212.08037.
arXiv submission rate statistics 2021- arXiv info —
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
info.arxiv.org. https://info.arxiv.org/help/
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
stats/2021_by_area/index.html. [Accessed16-
can,GeorgeBmVanDenDriessche,Jean-Baptiste
04-2024].
Lespiau, BogdanDamoc, AidanClark, etal.2022.
NitinAgarwal,EhteshamHaque,HuanLiu,andLance Improvinglanguagemodelsbyretrievingfromtril-
Parsons. 2005. Research paper recommender sys- lionsoftokens. InInternationalconferenceonma-
tems: Asubspaceclusteringapproach. InAdvances chinelearning,pages2206–2240.PMLR.
inWeb-AgeInformationManagement: 6thInterna-
Tanmoy Chakraborty, Natwar Modani, Ramasuri
tional Conference, WAIM 2005, Hangzhou, China,
Narayanam, and Seema Nagar. 2015. Discern: a
October11–13,2005.Proceedings6,pages475–491.
diversifiedcitationrecommendationsystemforsci-
Springer.
entificqueries. In2015IEEE31stinternationalcon-
ZafarAli,GuilinQi,PavlosKefalas,WaheedAhmad ferenceondataengineering,pages555–566.IEEE.
Abro,andBahadarAli.2020. Agraph-basedtaxon-
omyofcitationrecommendationmodels. Artificial Blaise Cronin. 1981. The need for a theory of citing.
IntelligenceReview,53:5217–5260. Journalofdocumentation,37(1):16–24.
ZafarAli,IrfanUllah,AminKhan,AsimUllahJan,and Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
KhanMuhammad.2021. Anoverviewandevalua- KristinaToutanova.2018. Bert: Pre-trainingofdeep
tionofcitationrecommendationmodels. Scientomet- bidirectionaltransformersforlanguageunderstand-
rics,126:4083–4119. ing. arXivpreprintarXiv:1810.04805.
IsabelleAugenstein,TimothyBaldwin,MeeyoungCha, DinilMonDivakaranandSaiTejaPeddinti.2024. Llms
Tanmoy Chakraborty, Giovanni Luca Ciampaglia, forcybersecurity:Newopportunities. arXivpreprint
DavidCorney,ReneeDiResta,EmilioFerrara,Scott arXiv:2404.11338.
Hale,AlonHalevy,etal.2023. Factualitychallenges
intheeraoflargelanguagemodels. arXivpreprint TravisEbesuandYiFang.2017. Neuralcitationnet-
arXiv:2310.05189. workforcontext-awarecitationrecommendation. In
Proceedings of the 40th international ACM SIGIR
XiaomeiBai,MengyangWang,IvanLee,ZhuoYang, conferenceonresearchanddevelopmentininforma-
XiangjieKong,andFengXia.2019. Scientificpaper tionretrieval,pages1093–1096.RanElgedawy,SudarshanSrinivasan,andIoanaDan- Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
ciu. 2024. Dynamic Q&A of Clinical Documents cas Hosseini, Fabio Petroni, Timo Schick, Jane
with Large Language Models. arXiv preprint Dwivedi-Yu,ArmandJoulin,SebastianRiedel,and
arXiv:2401.10733. Edouard Grave. 2022. Few-shot learning with re-
trievalaugmentedlanguagemodels. arXivpreprint
Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe arXiv:2208.03299.
Zhang,MingZhao,andXiaohangZhao.2023. Bias
of AI-generated content: an examination of news Chanwoo Jeong, Sion Jang, Eunjeong Park, and
producedbylargelanguagemodels. arXivpreprint Sungchul Choi. 2020a. A context-aware citation
arXiv:2309.09825. recommendationmodelwithbertandgraphconvolu-
tionalnetworks. Scientometrics,124:1907–1922.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chanwoo Jeong, Sion Jang, Eunjeong Park, and
Chen,ArunTejasviChaganty,YichengFan,Vincent
Sungchul Choi. 2020b. A context-aware citation
Zhao, NiLao, HongraeLee, Da-ChengJuan, etal.
recommendationmodelwithbertandgraphconvolu-
2023. Rarr: Researchingandrevisingwhatlanguage
tionalnetworks. Scientometrics,124.
modelssay,usinglanguagemodels. InProceedings
of the 61st Annual Meeting of the Association for
ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan
ComputationalLinguistics(Volume1: LongPapers),
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
pages16477–16508.
Madotto,andPascaleFung.2023. Surveyofhalluci-
nationinnaturallanguagegeneration. ACMComput-
KelvinGuu,KentonLee,ZoraTung,PanupongPasu-
ingSurveys,55(12):1–38.
pat,andMingweiChang.2020. Retrievalaugmented
languagemodelpre-training. InInternationalconfer- AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
enceonmachinelearning,pages3929–3938.PMLR. sch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil-
QiHe,JianPei,DanielKifer,PrasenjitMitra,andLee laumeLample,LucileSaulnier,LélioRenardLavaud,
Giles.2010. Context-awarecitationrecommendation. Marie-AnneLachaux,PierreStock,TevenLeScao,
InProceedingsofthe19thinternationalconference Thibaut Lavril, Thomas Wang, Timothée Lacroix,
onWorldwideweb,pages421–430. andWilliamElSayed.2023a. Mistral7b.
XiaoxinHe,YijunTian,YifeiSun,NiteshVChawla, Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing
ThomasLaurent,YannLeCun,XavierBresson,and Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,
BryanHooi.2024. G-retriever: Retrieval-augmented JamieCallan, andGrahamNeubig.2023b. Active
generationfortextualgraphunderstandingandques- Retrieval Augmented Generation. arXiv preprint
tionanswering. arXivpreprintarXiv:2402.07630. arXiv:2305.06983.
OrHonovich, RoeeAharoni, JonathanHerzig, Hagai VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
Taitelbaum,DoronKukliansy,VeredCohen,Thomas Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Scialom, Idan Szpektor, Avinatan Hassidim, and Wen-tauYih.2020. Densepassageretrievalforopen-
Yossi Matias. 2022. TRUE: Re-evaluating factual domainquestionanswering. InProceedingsofthe
consistencyevaluation. InProceedingsofthe2022 2020ConferenceonEmpiricalMethodsinNatural
Conference of the North American Chapter of the LanguageProcessing(EMNLP),pages6769–6781,
AssociationforComputationalLinguistics: Human Online.AssociationforComputationalLinguistics.
LanguageTechnologies,pages3905–3920,Seattle,
UrvashiKhandelwal,OmerLevy,DanJurafsky,Luke
United States. Association for Computational Lin-
Zettlemoyer,andMikeLewis.2019. Generalization
guistics.
throughmemorization: Nearestneighborlanguage
models. arXivpreprintarXiv:1911.00172.
Zihan Huang, Charles Low, Mengqiu Teng, Hongyi
Zhang, Daniel E Ho, Mark S Krass, and Matthias
Tharindu Kumarage and Huan Liu. 2023. Neural au-
Grabmair.2021. Context-awarelegalcitationrecom-
thorshipattribution:Stylometricanalysisonlargelan-
mendation using deep learning. In Proceedings of
guagemodels. In2023InternationalConferenceon
theeighteenthinternationalconferenceonartificial
Cyber-EnabledDistributedComputingandKnowl-
intelligenceandlaw,pages79–88.
edgeDiscovery(CyberC),pages51–54.IEEE.
Chia-ChienHung,WiemBenRim,LindsayFrost,Lars OnurKüçüktunç,ErikSaule,KamerKaya,andUmit
Bruckner,andCarolinLawrence.2023. Walkinga Catalyurek.2012. Diversifyingcitationrecommen-
tightrope–evaluatinglargelanguagemodelsinhigh- dations. ACM Transactions on Intelligent Systems
riskdomains. arXivpreprintarXiv:2311.14966. andTechnology,5.
Sehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani, Sheng-ChiehLin,AkariAsai,MinghanLi,BarlasOguz,
SalemAlelyani,RaheelNawaz,andLutzBornmann. JimmyLin,YasharMehdad,Wen-tauYih,andXilun
2020. Adecadeofin-textcitationanalysisbasedon Chen.2023. HowtotrainyourDRAGON:Diverse
natural language processing and machine learning augmentationtowardsgeneralizabledenseretrieval.
techniques: anoverviewofempiricalstudies. Scien- In Findings of the Association for Computational
tometrics,126:6551–6599. Linguistics: EMNLP2023,pages6385–6400.Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023. 01/technology/perplexity-search-ai-google.
Evaluatingverifiabilityingenerativesearchengines. html. [Accessed12-04-2024].
arXivpreprintarXiv:2304.09848.
TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta
Yusuf Mehdi. 2024. Confirmed: the new Bing Raileanu,MariaLomeli,LukeZettlemoyer,Nicola
runs on OpenAI’s GPT-4 — blogs.bing.com. Cancedda,andThomasScialom.2023. Toolformer:
https://blogs.bing.com/search/march_2023/ Languagemodelscanteachthemselvestousetools.
Confirmed-the-new-Bing-runs-on-OpenAI%E2% arXivpreprintarXiv:2302.04761.
80%99s-GPT-4. [Accessed12-04-2024].
LeoSchwinn,DavidDobre,StephanGünnemann,and
Jacob Menick, Maja Trebacz, Vladimir Mikulik, Gauthier Gidel. 2023. Adversarial attacks and de-
John Aslanides, Francis Song, Martin Chadwick, fensesinlargelanguagemodels:Oldandnewthreats.
Mia Glaese, Susannah Young, Lucy Campbell- arXivpreprintarXiv:2310.19737.
Gillingham,GeoffreyIrving,etal.2022. Teaching
language models to support answers with verified Trevor Strohman, W. Croft, and David Jensen. 2007.
quotes. arXivpreprintarXiv:2203.11147. Recommendingcitationsforacademicpapers. pages
705–706.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,
Ouyang Long, Christina Kim, Christopher Hesse, AlexTamkinandDeepGanguli.2021. Howlargelan-
ShantanuJain,VineetKosaraju,WilliamSaunders, guage models will transform science, society, and
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen ai.
Krueger,KevinButton,MatthewKnight,Benjamin
Chess,andJohnSchulman.2021. Webgpt: Browser- XiangruTang,YimingZong,YilunZhao,ArmanCohan,
assisted question-answering with human feedback. and Mark Gerstein. 2023. Struc-bench: Are large
ArXiv,abs/2112.09332. languagemodelsreallygoodatgeneratingcomplex
structureddata? arXivpreprintarXiv:2309.08963.
SidraNaveed,TimDonkers,andJürgenZiegler.2018.
Argumentation-basedexplanationsinrecommender Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
systems: Conceptual framework and empirical re- bert, Amjad Almahairi, Yasmine Babaei, Nikolay
sults. InAdjunctPublicationofthe26thConference Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
onUserModeling,AdaptationandPersonalization, Bhosale,DanBikel,LukasBlecher,CristianCanton
pages293–298. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
RonakPradeep,YuqiLiu,XinyuZhang,YilinLi,An-
CynthiaGao,VedanujGoswami,NamanGoyal,An-
drewYates,andJimmyLin.2022. Squeezingwater
thonyHartshorn,SagharHosseini,RuiHou,Hakan
fromastone: Abagoftricksforfurtherimproving
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
cross-encodereffectivenessforreranking. InEuro-
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
pean Conference on Information Retrieval, pages
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
655–670.Springer.
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Petrov,GauravSinghTomar,IuliaTurc,andDavid
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Reitter.2023. Measuringattributioninnaturallan-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
guagegenerationmodels. ComputationalLinguistics,
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
49(4):777–840.
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Zafaryab Rasool, Scott Barnett, Stefanus Kurniawan, Melanie Kambadur, Sharan Narang, Aurelien Ro-
Sherwin Balugo, Rajesh Vasa, Courtney Chesser, driguez,RobertStojnic,SergeyEdunov,andThomas
and Alex Bahar-Fuchs. 2023. Evaluating llms on Scialom.2023. Llama2: Openfoundationandfine-
document-based qa: Exact answer selection and tunedchatmodels.
numerical extraction using cogtale dataset. ArXiv,
HungNghiepTran,TinHuynh,andKiemHoang.2015.
abs/2311.07878.
A potential approach to overcome data limitation
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, in scientific publication recommendation. In 2015
AnubhavSarkar,SMTowhidulIslamTonmoy,Aman SeventhInternationalConferenceonKnowledgeand
Chadha,AmitSheth,andAmitavaDas.2023. The SystemsEngineering(KSE),pages310–313.IEEE.
troubling emergence of hallucination in large lan-
guagemodels-anextensivedefinition,quantification, Ilya Tyagin and Ilya Safro. 2023. Dyport: Dynamic
andprescriptiveremediations. InProceedingsofthe importance-basedhypothesisgenerationbenchmark-
2023ConferenceonEmpiricalMethodsinNatural ingtechnique. arXivpreprintarXiv:2312.03303.
LanguageProcessing,pages2541–2573.
PeterVajdecka,ElenaCallegari,DesaraXhura,andAtli
Kevin Roose. 2024. Can This A.I.-Powered Search Ásmundsson.2023. Predictingthepresenceofinline
Engine Replace Google? It Has for Me. — ny- citationsinacademictextusingbinaryclassification.
times.com. https://www.nytimes.com/2024/02/ In Proceedings of the 24th Nordic Conference onComputationalLinguistics(NoDaLiDa),pages717– repercussionsofneglectingthiscriticalstep. The
722. subsequentrequirementfortheinvolvedattorneyto
issueapologiesandacceptsanctionsdemonstrates
Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015.
Identifyingmeaningfulcitations. InAAAIworkshop: thedireneedforrobustcitationpracticesinthede-
Scholarlybigdata,volume15,page13. ploymentofLLMsandservesasacruciallearning
pointforallsectorsconsideringtheintegrationof
WeiWang,TaoTang,FengXia,ZhiguoGong,Zhikui
Chen, and Huan Liu. 2020. Collaborative filter- LLMsintotheirworkflow. LinkstotheNewYork
ingwithnetworkrepresentationlearningforcitation Timesnewsarticlescoveringthewholestory:
recommendation. IEEETransactionsonBigData,
8(5):1233–1246. • https://www.nytimes.com/
2023/05/27/nyregion/
Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li,
Roberto Martinez-Maldonado, Guanliang Chen, avianca-airline-lawsuit-chatgpt.
Xinyu Li, Yueqiao Jin, and Dragan Gaševic´. 2024. html,
Practical and ethical challenges of large language
modelsineducation: Asystematicscopingreview.
• https://www.nytimes.com/
BritishJournalofEducationalTechnology,55(1):90–
2023/06/22/nyregion/
112.
lawyers-chatgpt-schwartz-loduca.
Libin Yang, Yu Zheng, Xiaoyan Cai, Hang Dai, De- html
jun Mu, Lantian Guo, and Tao Dai. 2018. A lstm
basedmodelforpersonalizedcontext-awarecitation
• https://www.nytimes.com/2023/06/08/
recommendation. IEEEaccess,6:59618–59627.
nyregion/lawyer-chatgpt-sanctions.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak html
Shafran,KarthikNarasimhan,andYuanCao.2022.
React: Synergizingreasoningandactinginlanguage A.2 ResearchCostBreakdown
models. arXivpreprintarXiv:2210.03629.
The cost associated with this research in-
XiangYue,BoshiWang,KaiZhang,ZiruChen,YuSu, cludesexpensesforutilizingOpenAIAPI,totaling
and Huan Sun. 2023. Automatic evaluation of at- $640.37. Additionally,theuseofPerplexityAPI
tributionbylargelanguagemodels. arXivpreprint
incurredcostsamountingto$259.39. Furthermore,
arXiv:2305.06311.
GPU resources, we used Replicate4 API for our
FattaneZarrinkalamandMohsenKahani.2013. Semcir: experiments,amountedto$466.22. Fordatasetcre-
Acitationrecommendationsystembasedonanovel
ation, we used Oxylab for $249 for a month. In
semantic distance measure. Program: electronic
total,theexpensesforconductingthisresearchsum
libraryandinformationsystems,47.
upto$1614.98.
YilunZhao,HaoweiZhang,ShengyunSi,LinyongNan,
A.3 Reproduciblity
Xiangru Tang, and Arman Cohan. 2023. Investi-
Ourpipelineisstraightforwardtoimplementand
gatingtable-to-textgenerationcapabilitiesoflarge
languagemodelsinreal-worldinformationseeking canbeeasilyreproduced. Wehavethoroughlydoc-
scenarios. InProceedingsofthe2023Conferenceon umentedallexperimentaldetailsinthemaintext
EmpiricalMethodsinNaturalLanguageProcessing:
andtheappendices. Althoughthefulltextofeach
IndustryTrack,pages160–175,Singapore.Associa-
promptistoolengthytoinclude,weofferexamples
tionforComputationalLinguistics.
ofeachinAppendixBtohelpreadersunderstand
ShenZheng,JieHuang,andKevinChen-ChuanChang.
thestyleused. Allofourresources,includingcom-
2023. Whydoeschatgptfallshortinansweringques-
pletepromptscripts,crawlingdata,andcodefor
tionsfaithfully? arXivpreprintarXiv:2304.10513.
evaluatingourapproach,areavailabletothepub-
A Appendix licrepositoryhere:
A.1 TheStoryofaLawyerwhoemployed
ChatGPT • https://anonymous.4open.science/r/
REASONS_BENCHMARK-D04D/README.md
InFigure6,therelianceonLLM-generatedcon-
tentbylegalprofessionals,highlightedbyTheNew
A.4 Modelsspecificationsusedduring
York Times, illuminates the pitfalls when these
experimentation
LLMs produce content that lacks proper verifica-
The ‘temperature’ hyper-parameter in the
tion. This incident not only signifies the impor-
LLMscontrolsthecreativityoftheLLMsintheir
tance of cross-checking LLM outputs against re-
liable sources but also exemplifies the potential 4https://replicate.com/Figure6:TheperilsofinadequateverificationofLLMs-generatedcitationsinlegaldocuments.
response. The lower the temperature, the lower retrievedchunks.
the creativity in the response, and the higher the Ourresearchutilizedadual-configurationserver
temperaturevalue,thehigherthecreativityinthe setup provided by the University. Configuration
response. Bydefault,thetemperatureformostof 1 consists of two nodes, with each node housing
theLLMsissetto1. The‘max_tokens’describes 128 cores (totaling 256 cores), 256GB of RAM,
themaximumnumberoftokenstheLLMcangen- and two NVIDIA L40S GPUs, each equipped
erate. The‘top_p’isnucleussampling,whichhelps with 48GB of GPU memory. Configuration 2 is
limittheirrelevanttokensinthegeneration. equippedwith8NVIDIAA100-40GBcards,1TB
The‘top_k’isthenumberofretrievedchunks ofRAM,and256CPUs. Duetoresourceavailabil-
of information that will be considered during the ity in the queue, we alternate between these two
generationintheRAGprocess. The‘tokenizer’ configurations. Currently,wehavenotbeenableto
converts the retrieved chunks of information and comparetheirperformance.
thepromptsintotokens.
WeconcludedthattheZeroShotIndirectprompt-
We have used two different tokeniz-
ingapproachissusceptibletohallucinationsandis
ers ‘NousResearch/Llama-2-7b-chat-hf’
ineffectiveforthecitationgenerationtask. Hence,
5 for LLAMA-2-7b-chat and
wedidnotconductAdvanceRAGexperimentswith
‘mistralai/Mistral-7B-v0.1’ 6 for Mistral-
this prompting due to earlier results from other
7b-instruct. The “Embedding Model” gen-
models, and also, the Advance RAG approach is
erates embeddings for tokens produced
computationallymoreexpensiveTable6.
during tokenization. We have utilized the
A.5 DatasetComparison
‘BAAI/bge-small-en-v1.5’7 model for this
We contrast the REASONS dataset with other
purpose. And finally, the Cross-Encoder
‘ms-marco-MiniLM-L-12-v2’8 is fine-tuned similar datasets that could have been utilized for
citation generation. However, due to constraints
using the LCL function for re-ranking of the
within these datasets—such as the absence of
5https://huggingface.co/NousResearch/
sentence-levelannotationofcitations,metadataof
Llama-2-7b-chat-hf
6https://huggingface.co/mistralai/ citations,andpapertitles—wewouldnotbeable
Mistral-7B-v0.1 toeffectivelyassesstheabilityofLLMsandRAG
7https://huggingface.co/BAAI/bge-small-en-v1.
LLMstoaccuratelygraspthecontextandgenerate
5
suitablecitations(seeTable4). Acronymsusedin
8https://huggingface.co/cross-encoder/
ms-marco-MiniLM-L-12-v2 thepaper: ComputerVision(CV),InformationRe-Hyperparameter Value G3, RM, and RL models, while IR and CV con-
temperature 1.0
sistentlyshowlowHRacrossG1,G2,RM,andM
max_tokens 256
top_p 0.95 models.
NaïveRAG
For direct prompting with metadata also
top_k 2
shows common domains across the models. No-
EmbeddingModel BAAI/bge-small-en-v1.5
AdvanceRAG table high HR domains such as NNC, IR, NLP,
top_k 40 QC,andGraphicsfeatureprominentlyacrossdif-
Cross-Encoder ms-marco-MiniLM-L-12-v2
ferent models, indicating frequent challenges in
LLAMA-2Tokenizer NousResearch/Llama-2-7b-
chat-hf theseareas.
MistralTokenizer mistralai/Mistral-7B-v0.1
LowHRresultsconsistentlyappearinCV,NLP,
Table3:Hyper-parametersalongwiththeirvaluesusedduring
Cryptography, and Biomolecules, showcasing
experimentation
generalrobustnessagainsthallucinationsinthese
domains. Specifically, NNC is recurrently ob-
trieval(IR),ArtificialIntelligence(AI)NaturalLan- served with high HR in the G1, AdvRAG(L),
guageProcessing(NLP),Cryptography(Crypto), and AdvRAG(M) models, while QC shows up
NeuronsandCognition(NNC),Human-Computer frequently in high HR scenarios (G1, G2, L,
Interaction(HCI),QuantumComputing(QC),and AdvRAG(M)).
Biomolecules. Similarly,IRishighlightedinhighHRfortheP,
A.6 GPUMachineHours RM,RL,andAdvRAG(L)models,indicatingits
Withtheexceptionofdirectprompting,allother susceptibility, whereas NLP and Graphics show
prompting styles required a substantial number variabilityinHRacrossmultiplemodels.
of GPU hours (see Table 5). Training Advance Forzero-shotdirectpromptingalsoshowsig-
RAGprovedtobeahighlytime-intensiveendeavor, nificantpatternsincommondomains.
whichweattemptedtomitigatebyalternatingbe- HighHRiscommonlyobservedindomainslike
tween NVIDIA L40S and A100. We also found QC,Cryptography,Robotics,andDatabases,in-
thatLLAMA2requiredlesstimeintrainingthan dicating areas prone to hallucinations. Low HR
Mistral. Thereasonsbehindthiscanbeasubjectof domains frequently include IR, HCI, CV, and
futurework. Weprovidemachine-hourestimates Biomolecules, highlighting resilience in these ar-
to assist other researchers interested in RAG and eas.
itsapplicationsinprovenanceandcontextcompre-
Specifically,QCappearsasahighHRdomain
hension,facilitatingbettertimemanagement.
in the G1, G2, G3, RL, L, AdvRAG(L), and
B ExamplesofPromptsinDirectand
AdvRAG(M)models,reflectingaconsistentchal-
IndirectQueries lenge across these models. IR and HCI are no-
In the following visual examples, each model tably present as low HR domains in G2, G3,
is followed by a checkbox indicating whether it AdvRAG(L),showingwidespreadreliability.
generated citations correctly or incorrectly. See Moreover,RoboticsandCryptographyarefre-
Figure7,Figure8,Figure9,Figure10,Figure11, quently observed in high HR scenarios in mod-
Figure12,Figure13. elslikeG2,M,andAdvRAG(M),whileCVand
B.1 IndividualResultsofallthedomains Biomolecules commonly appear in low HR set-
acrossallthepromptingstyles tingsacrossG2,G3,M,andAdvRAG().
A comparative analysis of hallucination rates For SID prompting, high HR domains such
(HR) across several LLMs in zero-shot indirect as QC, Cryptography, Databases, NNC, and
prompting reveals distinct patterns, focusing on Roboticsfrequentlyappearacrossseveralmodels,
common domains. The G1, G2, G3, P, RM, M, highlightingageneralsusceptibilityintheseareas.
RL,andLmodelsconsistentlyshowvariationsin Ontheotherhand,lowHRdomainscommonlyin-
HR.HighHRdomainslikeNNC,Cryptography, cludeIR,HCI,CV,andGraphics,demonstrating
andNLPappearrecurrentlyacrossseveralmodels. resilienceagainsthallucinations.
LowHRresultsfrequentlyoccurinIR,CV,and Specifically, QC is observed as a high HR do-
HCI, indicating a general resilience in these ar- main in the G1, G2, G3, RM, RL, AdvRAG(L),
eas across different settings. For instance, NNC andAdvRAG(M)models,signifyingaconsistent
featuresprominentlywithhighHRintheG1,G2, challenge in this area. IR and HCI are notablyREASONS UnarXive PubMed CiteULike S2orc
MainPurpose SentenceAnno- Citation Rec- Medical Re- Benchmark for Citationrecom-
tation ommendation search Recommenda- mendation,text
tion Systems summarization
and Collabora-
tive Filtering
Algorithms
ContainsSentences? ✓ ✗ ✗ ✗ ✗
ContainsPaperTitle? ✓ ✗ ✓ ✓ ✓
ContainsAbstract? ✓ ✓ ✓ ✓ ✗(Notalldocu-
ments)
ContainsAuthorsNames? ✓ ✓ ✓ ✓ ✓
ContainsKeywords? ✗ ✗ ✓ ✓ ✗
CoverMultipleDomains? ✓ ✓ ✗ ✓ ✓
CoversMetadataofcitation ✓ ✗ ✗ ✗ ✗
DataTimePeriod 2017-2023 1991-2023 1990-2023 2004-2023 Last release:
2021-02-01
Table4:Comparisonofdifferentdatasets
Domain OpenAI-AllModels M L RM RL Perplexity AdvRAG(L) AdvRAG(M)
AI 34:25 26:03 11:10 74:49 73:09 34:31 156:24 163:28
Biomolecules 01:11 00:41 00:10 4:38 4:10 00:20 7:29 7:40
CV 47:45 18:35 19:24 189:20 198:45 42:05 259:32 302:14
Crytography 03:50 02:18 04:59 83:28 89:21 13:23 190:19 194:25
Databases 01:27 00:51 00:40 49:34 45:46 00:51 96:19 97:48
Graphics 07:08 08:55 06:08 108:08 127:48 16:52 214:25 227:23
HCI 03:01 01:10 00:42 48:32 50:51 02:47 95:56 98:44
IR 20:31 11:40 06:52 91:30 99:43 19:50 193:37 202:23
NLP 28:26 11:42 05:09 91:07 88:40 13:06 175:58 156:49
NNC 05:00 01:39 02:12 34:56 41:09 01:19 70:17 84:07
QC 07:26 02:46 01:59 61:09 67:56 03:17 109:21 113:54
Robotics 19:39 05:41 06:11 42:07 46:55 09:17 93:07 98:45
Table5:Timetakenbydifferentmodelswithrespecttoeachdomainduringexperimentation,convertedtohoursandminutes.
RedColor:TimerecordedwhileusingReplicateAPI,andBlueColor:TimerecordingwhileusingNVIDIAA100/L40SUSC
server.
present as low HR domains in G1, G2, G3, RM, B.2 FurtherDiscussiononAdversarial
andAdvRAG(L),indicatingwidespreadreliability Examination
intheseareas. Thisanalysisemphasizesthestrengthsandweak-
Moreover,CryptographyandRoboticsarefre- nessesofcurrentLLMsandtheneedfordomain-
quentlyobservedinhighHRscenariosinmodels specifictraining. Itshowsthatageneralapproach
like G1, G2, and RM, while CV and Graphics isinsufficientandhighlightstheimportanceofspe-
commonlyappearinlowHRsettingsacrossG2,L, cialized training to meet the unique demands of
andAdvRAG(L). Tosummarizeourresults different fields. As LLMs evolve, aligning their
developmentwithhumanknowledge’svariedand
intricatenatureiscrucial.
• The zero-shot indirect and SID promoting The study finds a significant relationship be-
stylesaremorepronetohallucinations,which tweenthespecificityofprompts,especiallythose
lackcontextualunderstanding. withmetadata,andthelinguisticaccuracyofLLMs,
as evidenced by higher F-1 and BLEU scores.
Thissuggeststhatprovidingdetailed,context-rich
• Notably, NNC and QC consistently show
prompts can significantly improve the quality of
highHRacrossmultiplemodelsandpromot-
generatedcitations.
ingstyles,indicatingcommonchallengingdo-
PassPercentage(PP):ThevaryingPPamongdif-
mains.
ferent models points to a key challenge in LLM
development: theabilitytounderstandandreason
• Conversely,CVandIRlowHR,whichshow throughcomplexsituations. ModelswithlowerPP
robustness in models, suggesting reliability strugglewithgeneratingrelevantresponsesincom-
in these domainsacross different prompting plex or critical scenarios, underlining the impor-
strategies. tanceofenhancingreasoningcapabilitiesinLLMsFigure7:Example1ofanindirectquerywhereasentencefromtheresearchpaperisprovidedandaskedforthecorrecttitle.We
havegroundtruthforthepapertitleandresponsesfromvariousLLMs.OnlyAdv.RAG+LLAMAgeneratedthecorrecttitle.
foreffectiveapplication.
Prompt Design: There’s a noticeable difference
in how individual models, such as gpt-4-1106-
preview and gpt-4, respond to different prompts.
This underscoring the significance of prompt de-
signinleveragingthefullpotentialofLLMssug-
gests a complex interplay between the model’s
structure,promptformulation,andperformance.Zero-ShotIndirect
Domain G1 G2 G3 P RM M RL L
HallucinationRate(%)
AI 63.61 72.44 81.87 96.27 93.98 97.16 92.21 95.87
Biomolecules 96.82 69.77 84.68 95.06 96.63 85.14 96.25 95.57
Crypto 75.04 70.21 81.97 94.16 93.07 96.11 93.83 97.23
CV 51.83 64.3 79.34 94.63 91.42 97.12 94.68 95.96
Databases 76.66 69.99 78.93 96.99 93.42 97.28 95.68 95.84
Graphics 57.49 70.76 85.39 97.25 92.32 97.55 96.1 95.92
HCI 51.83 73.46 73.41 96.71 93.01 96.83 96.85 95.61
IR 51.78 67.89 73.41 96.80 92.01 96.81 96.85 96.01
NLP 63.03 73.98 74.77 97.11 94.10 97.05 94.29 97.93
NNC 77.27 80.75 82.11 95.49 94.32 97.13 97.92 96.14
QC 91.72 84.85 76.09 95.15 92.13 97.14 95.34 95.56
Robotics 55.78 71.55 76.73 95.81 94.26 97.2 97.51 95.67
Mean 67.73 72.49 79.05 95.95 93.38 96.04 95.64 96.10
StandardDeviation 15.64 5.51 4.19 1.05 1.40 3.45 1.67 0.72
F-1Score
AI 0.02 0.22 0.00 0.00 0.10 0.08 0.07 0.05
Biomolecules 0.00 0.26 0.00 0.07 0.09 0.06 0.06 0.05
Crypto 0.01 0.25 0.00 0.00 0.08 0.04 0.06 0.04
CV 0.06 0.29 0.00 0.00 0.07 0.05 0.05 0.04
Databases 0.00 0.26 0.00 0.00 0.09 0.06 0.05 0.04
Graphics 0.06 0.25 0.00 0.00 0.05 0.03 0.03 0.01
HCI 0.04 0.23 0.00 0.00 0.07 0.03 0.04 0.03
IR 0.06 0.29 0.00 0.00 0.04 0.01 0.03 0.02
NLP 0.02 0.21 0.00 0.00 0.07 0.04 0.04 0.03
NNC 0.02 0.16 0.00 0.00 0.06 0.04 0.02 0.01
QC 0.01 0.13 0.00 0.00 0.05 0.02 0.03 0.01
Robotics 0.03 0.21 0.00 0.00 0.08 0.05 0.03 0.02
Mean 0.02 0.23 0.00 0.00 0.07 0.04 0.04 0.02
StandardDeviation 0.02 0.04 0.00 0.02 0.01 0.01 0.01 0.01
BLEUScore
AI 0.01 0.09 0.00 0.00 0.05 0.00 0.06 0.00
Biomolecules 0.00 0.12 0.00 0.00 0.00 0.00 0.04 0.00
Crypto 0.01 0.12 0.00 0.00 0.07 0.00 0.05 0.00
CV 0.04 0.16 0.00 0.00 0.02 0.00 0.03 0.00
Databases 0.00 0.12 0.00 0.00 0.08 0.00 0.03 0.00
Graphics 0.04 0.12 0.00 0.00 0.03 0.00 0.01 0.00
HCI 0.03 0.09 0.00 0.00 0.05 0.00 0.02 0.00
IR 0.04 0.14 0.00 0.00 0.01 0.00 0.02 0.00
NLP 0.02 0.09 0.00 0.00 0.06 0.00 0.00 0.00
NNC 0.02 0.05 0.00 0.00 0.02 0.00 0.00 0.00
QC 0.00 0.02 0.00 0 0.01 0.00 0.00 0.00
Robotics 0.02 0.08 0.00 0.00 0.06 0.00 0.00 0.00
Mean 0.01 0.10 0.00 0.00 0.03 0.00 0.02 0.00
StandardDeviation 0.01 0.03 0.00 0.00 0.02 0.00 0.02 0.00
PassPercentage(%)
AI 92.92 24.15 97.08 97.77 4.95 0.05 0 0
Biomolecules 88.89 19.76 97.81 0 0 0 0 0
Crypto 92.45 20.47 98.17 99.01 5.63 0.09 0 0
CV 86.7 23.8 95.66 96.48 3.84 0 0 0
Databases 97.25 20.11 97.67 97.14 6.23 0 0 0
Graphics 86.38 19.69 97.32 98.8 1.34 0 0 0
HCI 90.83 19.21 96.61 98.32 6.11 0 0 0
IR 87.67 16.69 96.61 97.83 5.21 0 0 0
NLP 92.4 21.98 97.89 98.53 6.75 0 0 0
NNC 87.73 20.86 98.16 95.21 6.39 0 0 0
QC 75 17.76 99.34 95.09 5.72 0 0 0
Robotics 92.91 31.7 97.68 95.95 5.73 0 0 0
Mean 89.26 21.34 97.50 89.17 4.82 0.01 0.00 0.00
StandardDeviation 5.528 3.91 0.94 28.11 2.10 0.02 0.00 0.00
Table6:Zero-ShotIndirectDirectwithMetadata
Domain G1 G2 G3 P RM M RL L AdvRAG(L) AdvRAG(M)
HallucinationRate(%)
AI 0.32 0.10 6.04 61.31 37.6 71.39 72.16 80.90 19.24 7.67
Biomolecules 0.46 0.01 5.29 73.99 94.5 67.98 87.10 79.15 8.15 0.07
Crypto 0.42 0.05 5.41 61.77 40.87 71.56 73.18 80.45 6.76 4.15
CV 0.42 0.07 4.9 62.35 41.60 73.67 74.16 78.93 5.51 2.22
Databases 0.20 0.15 5.05 62.55 39.60 73.33 75.16 0.79 9.73 7.60
Graphics 0.20 0.15 5.43 62.64 42.31 71.43 78.21 79.80 11.45 8.10
HCI 0.24 0.26 5.26 60.38 40.75 73.29 75.45 80.66 17.65 7.04
IR 0.39 0.09 5.26 63.88 48.98 73.1 79.43 80.98 19.71 7.81
NLP 0.64 0.27 6.20 58.79 37.44 69.68 71.24 80.17 12.60 5.80
NNC 0.51 0.16 5.82 61.12 38.73 72.04 75.14 81.31 28.11 57.95
QC 0.54 0.17 4.95 61.97 38.54 69.34 72.09 81.70 18.19 9.25
Robotics 0.45 0.12 5.98 61.89 39.01 70.62 71.02 80.34 10.27 3.88
Mean 0.39 0.13 5.46 62.72 44.99 71.45 75.36 80.28 13.94 10.70
StandardDeviation 0.13 0.07 0.44 3.76 15.89 1.79 4.52 0.90 6.67 15.01
F-1Score
AI 0.99 0.89 0.95 0.69 0.71 0.36 0.33 0.28 0.84 0.92
Biomolecules 0.97 0.99 0.96 0.36 0.07 0.07 0.21 0.32 0.96 0.95
Crypto 0.93 0.97 0.96 0.61 0.60 0.40 0.37 0.31 0.91 0.94
CV 0.98 0.99 0.96 0.39 0.52 0.38 0.34 0.35 0.98 0.98
Databases 0.99 0.98 0.96 0.42 0.59 0.34 0.34 0.33 0.92 0.95
Graphics 0.99 0.99 0.96 0.45 0.64 0.44 0.41 0.32 0.94 0.90
HCI 0.99 0.98 0.96 0.34 0.58 0.35 0.35 0.34 0.82 0.94
IR 0.99 0.98 0.94 0.52 0.54 0.39 0.39 0.30 0.84 0.92
NLP 0.99 0.92 0.95 0.53 0.62 0.42 0.40 0.31 0.86 0.91
NNC 0.99 0.99 0.95 0.51 0.62 0.41 0.36 0.30 0.92 0.39
QC 0.99 0.99 0.96 0.58 0.65 0.43 0.33 0.29 0.82 0.86
Robotics 0.99 0.99 0.95 0.63 0.69 0.35 0.49 0.31 0.92 0.95
Mean 0.98 0.98 0.95 0.50 0.56 0.36 0.35 0.32 0.89 0.88
StandardDeviation 0.01 0.00 0.00 0.11 0.16 0.09 0.06 0.02 0.05 0.15
BLEUScore
AI 0.99 0.99 0.93 0.31 0.43 0.24 0.11 0.12 0.81 0.92
Biomolecules 0.95 0.99 0.94 0.22 0.00 0.00 0.07 0.12 0.93 0.02
Crypto 0.95 0.97 0.94 0.33 0.41 0.24 0.13 0.12 0.93 0.95
CV 0.95 0.99 0.94 0.32 0.39 0.22 0.13 0.13 0.95 0.96
Databases 0.98 0.99 0.94 0.33 0.41 0.21 0.13 0.13 0.79 0.86
Graphics 0.99 0.99 0.94 0.33 0.45 0.24 0.17 0.12 0.91 0.91
HCI 0.99 0.98 0.94 0.33 0.43 0.22 0.13 0.14 0.91 0.92
IR 0.99 0.99 0.94 0.36 0.48 0.23 0.16 0.11 0.87 0.92
NLP 0.99 0.99 0.93 0.37 0.46 0.27 0.12 0.12 0.82 0.91
NNC 0.99 0.99 0.93 0.34 0.46 0.22 0.12 0.11 0.90 0.17
QC 0.98 0.98 0.93 0.28 0.38 0.26 0.15 0.11 0.80 0.83
Robotics 0.99 0.99 0.93 0.34 0.49 0.26 0.18 0.12 0.89 0.94
Mean 0.97 0.98 0.93 0.32 0.39 0.21 0.13 0.12 0.87 0.77
StandardDeviation 0.01 0.00 0.00 0.03 0.13 0.07 0.02 0.00 0.05 0.32
PassPercentage(%)
AI 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Biomolecules 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Crypto 0.00 0.00 0.00 0.15 0.00 0.00 0.00 0.00 0.00 0.00
CV 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00
Databases 0.00 0.00 0.00 0.72 0.00 0.00 0.00 0.00 0.00 0.00
Graphics 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00
HCI 0.00 0.00 0.00 0.24 0.44 0.00 0.00 0.00 0.00 0.00
IR 0.00 0.00 0.00 0.03 0.67 0.00 0.00 0.00 0.00 0.00
NLP 0.00 0.00 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00
NNC 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
QC 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Robotics 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Mean 0.00 0.00 0.00 0.13 0.09 0.00 0.00 0.00 0.00 0.00
StandardDeviation 0.00 0.00 0.00 0.21 0.22 0.00 0.00 0.00 0.00 0.00
Table7:DirectwithMetadataZero-ShotDirectPrompting
Domain G1 G2 G3 P RM M RL L AdvRAG(L) AdvRAG(M)
HallucinationRate(%)
AI 30.9 53.99 73.13 95.64 56.45 94.23 72.17 76.85 43.77 34.42
CV 35.9 36.32 61.38 95.84 58.45 92.84 73.17 76.67 35.38 35.43
NLP 27.51 52.49 72.28 96.18 63.92 93.89 83.17 75.91 47.95 36.63
IR 24.82 42.55 64.19 95.23 63.12 91.59 77.38 78.16 42.01 37.93
Databases 37.48 53.33 74.08 95.98 55.45 93.81 74.17 77.92 58.11 40.23
Graphics 29.3 54.29 73.71 95.67 52.4 92.99 71.19 75.57 47.41 40.26
HCI 22.92 38.02 64.19 95.01 62.67 92.64 78.15 76.49 38.51 41.11
Biomolecules 21.01 53.25 73.88 90.83 94.00 43.84 91.2 79.92 67.56 46.28
NNC 36.05 53.13 72.39 93.37 63.51 91.18 83.73 78.24 48.51 46.31
Crypto 34.41 54.68 73.01 95.39 54.45 94.78 76.59 76.44 66.16 50.08
Robotics 34.71 56.62 76.29 93.25 60.89 94.69 81.99 75.92 59.017 50.65
QC 53.04 70.01 82.26 93.70 65.07 89.75 85.64 81.24 69.108 60.81
Mean 32.33 51.55 71.73 94.67 62.53 88.85 79.04 77.44 51.95 43.34
StandardDeviation 8.52 9.02 5.80 1.58 10.76 14.25 6.14 1.73 11.66 7.75
F-1Score
AI 0.42 0.39 0.21 0.04 0.41 0.06 0.31 0.36 0.46 0.53
Biomolecules 0.37 0.42 0.21 0.08 0.07 0.05 0.14 0.31 0.29 0.65
Crypto 0.42 0.41 0.22 0.04 0.43 0.06 0.32 0.36 0.40 0.56
CV 0.42 0.60 0.33 0.05 0.39 0.07 0.32 0.36 0.62 0.62
Databases 0.40 0.42 0.21 0.05 0.41 0.06 0.31 0.34 0.42 0.55
Graphics 0.49 0.41 0.22 0.05 0.44 0.07 0.33 0.38 0.42 0.56
HCI 0.51 0.55 0.29 0.05 0.36 0.07 0.27 0.36 0.62 0.56
IR 0.51 0.52 0.29 0.05 0.35 0.08 0.26 0.34 0.57 0.69
NLP 0.39 0.38 0.21 0.04 0.35 0.06 0.21 0.37 0.52 0.66
NNC 0.39 0.39 0.19 0.06 0.37 0.08 0.24 0.34 0.48 0.57
QC 0.22 0.25 0.12 0.06 0.34 0.09 0.18 0.30 0.30 0.40
Robotics 0.35 0.36 0.20 0.06 0.33 0.05 0.15 0.37 0.41 0.54
Mean 0.40 0.42 0.22 0.05 0.35 0.06 0.25 0.34 0.45 0.57
StandardDeviation 0.07 0.09 0.05 0.01 0.09 0.01 0.06 0.02 0.10 0.07
BLEUScore
AI 0.37 0.31 0.11 0.00 0.24 0.00 0.17 0.15 0.38 0.49
Biomolecules 0.34 0.33 0.10 0.00 0.00 0.02 0.04 0.11 0.27 0.60
Crypto 0.37 0.32 0.11 0.00 0.25 0.00 0.18 0.15 0.26 0.47
CV 0.40 0.52 0.23 0.00 0.24 0.00 0.16 0.15 0.57 0.58
Databases 0.32 0.33 0.10 0.00 0.25 0.00 0.18 0.14 0.31 0.42
Graphics 0.44 0.31 0.11 0.00 0.23 0.00 0.19 0.16 0.70 0.51
HCI 0.46 0.46 0.18 0.00 0.22 0.00 0.13 0.15 0.64 0.51
IR 0.45 0.44 0.18 0.00 0.28 0.00 0.17 0.14 0.48 0.62
NLP 0.34 0.32 0.11 0.00 0.21 0.00 0.12 0.16 0.46 0.51
NNC 0.33 0.28 0.11 0.00 0.19 0.00 0.10 0.14 0.48 0.57
QC 0.17 0.14 0.02 0.00 0.17 0.00 0.08 0.11 0.20 0.29
Robotics 0.30 0.28 0.09 0.00 0.18 0.00 0.09 0.16 0.30 0.41
Mean 0.35 0.33 0.12 0.00 0.20 0.00 0.13 0.14 0.42 0.49
StandardDeviation 0.07 0.09 0.05 0.00 0.07 0.00 0.04 0.01 0.16 0.09
PassPercentage(%)
AI 37.26 9.70 12.37 0.66 1.65 0.00 0.00 0.00 0.00 0.00
Biomolecules 51.85 6.77 6.77 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Crypto 33.4 5.43 10.52 0.20 2.15 0.00 0.00 0.00 0.00 0.00
CV 32.26 3.84 8.67 0.09 3.12 0.09 0.00 0.00 0.00 0.00
Databases 32.42 6.70 10.59 0.95 2.49 0.00 0.00 0.00 0.00 0.00
Graphics 28.86 6.49 10.30 0.15 0.45 0.07 0.00 0.00 0.00 0.00
HCI 31.00 8.30 14.51 0.32 0.56 0.00 0.00 0.00 0.00 0.00
IR 30.11 6.11 14.51 0.86 0.87 0.00 0.00 0.00 0.00 0.00
NLP 44.6 15.75 17.03 0.18 1.76 0.00 0.00 0.00 0.00 0.00
NNC 37.12 13.19 21.47 0.74 1.53 0.00 0.00 0.00 0.00 0.00
QC 50.22 10.09 19.96 0.00 1.94 0.00 0.00 0.00 0.00 0.00
Robotics 45.10 11.60 9.02 0.00 4.54 0.00 0.00 0.00 0.00 0.00
Mean 37.85 8.66 12.97 0.34 1.75 0.01 0.00 0.00 0.00 0.00
StandardDeviation 8.06 3.50 4.61 0.35 1.25 0.03 0.00 0.00 0.00 0.00
Table8:Zero-ShotDirectSID
Domain G1 G2 G3 P RM M RL L AdvRAG(L) AdvRAG(M)
HallucinationRate(%)
AI 29.44 48.49 61.18 95.08 85.21 94.18 86.68 98.42 51.47 38.45
Biomolecules 35.71 54.99 66.34 95.79 96.87 86.32 96.51 99.06 52.15 40.89
Crypto 40.44 48.15 66.48 91.18 85.28 94.78 86.91 98 53.67 45.77
CV 34.44 38.15 59.77 93.47 87.65 94.13 89.58 99.56 38.82 39.25
Databases 40.74 62.34 66.00 93.91 86.66 93.96 86.10 98.67 62.49 43.2
Graphics 25.54 62.34 66.55 95.28 85.91 94.39 86.41 58.83 59.65 47.72
HCI 27.35 39.58 57.01 94.41 85.68 93.87 88.15 98.12 30.53 23.39
IR 24.01 41.87 57.01 94.68 85.61 93.33 88.45 98.57 58.58 40.97
NLP 29.2 50.69 61.68 95.87 88.46 93.88 89.28 98.64 60.26 37.72
NNC 32.68 57.13 74.64 95.97 88.01 95.14 89.56 99.34 59.42 64.43
QC 51.83 63.63 80.05 92.10 89.75 95.49 90.73 98.98 69.18 59.84
Robotics 32.45 49.76 57.27 95.07 89.46 94.36 90.86 98.27 49.24 34.95
Mean 33.65 51.42 64.49 94.40 87.87 93.65 89.10 95.371 53.788 43.048
StandardDeviation 7.80 8.85 7.16 1.51 3.25 2.38 2.85 11.51 10.60 10.84
F-1Score
AI 0.30 0.54 0.05 0.09 0.12 0.11 0.20 0.02 0.50 0.61
Biomolecules 0.15 0.51 0.03 0.05 0.05 0.03 0.05 0.00 0.52 0.57
Crypto 0.35 0.67 0.03 0.07 0.13 0.10 0.19 0.02 0.62 0.71
CV 0.35 0.67 0.06 0.09 0.13 0.11 0.16 0.03 0.72 0.73
Databases 0.21 0.03 0.03 0.08 0.14 0.10 0.19 0.02 0.29 0.48
Graphics 0.41 0.03 0.03 0.05 0.13 0.09 0.18 0.41 0.38 0.58
HCI 0.33 0.66 0.07 0.08 0.15 0.13 0.18 0.03 0.70 0.85
IR 0.38 0.64 0.07 0.08 0.14 0.12 0.15 0.02 0.43 0.68
NLP 0.30 0.51 0.05 0.07 0.16 0.10 0.13 0.02 0.41 0.49
NNC 0.21 0.45 0.03 0.09 0.11 0.08 0.17 0.00 0.50 0.31
QC 0.10 0.37 0.02 0.06 0.10 0.07 0.13 0.01 0.31 0.42
Robotics 0.28 0.54 0.05 0.07 0.13 0.09 0.14 0.02 0.60 0.62
Mean 0.28 0.46 0.04 0.07 0.12 0.09 0.15 0.05 0.49 0.58
StandardDeviation 0.09 0.22 0.01 0.01 0.02 0.02 0.04 0.11 0.14 0.14
BLEUScore
AI 0.25 0.31 0.02 0.00 0.06 0.00 0.04 0.00 0.32 0.51
Biomolecules 0.14 0.34 0.01 0.00 0.00 0.00 0.00 0.00 0.32 0.56
Crypto 0.27 0.48 0.01 0.00 0.06 0.00 0.06 0.00 0.47 0.55
CV 0.25 0.46 0.03 0.00 0.03 0.01 0.06 0.00 0.51 0.51
Databases 0.17 0.01 0.01 0.00 0.06 0.00 0.03 0.00 0.12 0.42
Graphics 0.35 0.01 0.01 0.00 0.03 0.00 0.01 0.26 0.22 0.44
HCI 0.28 0.45 0.03 0.00 0.07 0.01 0.05 0.00 0.53 0.71
IR 0.32 0.39 0.03 0.00 0.07 0.01 0.07 0.00 0.54 0.45
NLP 0.26 0.27 0.03 0.00 0.04 0.01 0.04 0.00 0.23 0.43
NNC 0.15 0.24 0.01 0.00 0.05 0.00 0.05 0.00 0.40 0.11
QC 0.08 0.17 0.00 0.00 0.04 0.00 0.03 0.00 0.20 0.31
Robotics 0.22 0.28 0.03 0.00 0.04 0.00 0.03 0.00 0.30 0.44
Mean 0.22 0.28 0.01 0.00 0.04 0.00 0.03 0.02 0.34 0.45
StandardDeviation 0.07 0.15 0.01 0.00 0.02 0.00 0.02 0.07 0.14 0.14
PassPercentage(%)
AI 56.8 4.21 87.14 1.86 7.25 0.00 0.00 0.00 0.00 0.00
Biomolecules 74.07 7.21 89.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Crypto 53.34 3.6 89.7 0.84 6.89 0.00 0.00 0.00 0.00 0.00
CV 52.3 1.6 83.42 0.79 4.94 0.00 0.00 0.00 0.00 0.00
Databases 63.19 89.98 90.61 0.00 6.04 0.00 0.00 0.00 0.00 0.00
Graphics 44.25 88.91 90.19 0.64 6.29 0.00 0.79 0.00 0.00 0.00
HCI 54.15 0.44 83.68 0.96 4.37 0.00 0.00 0.00 0.00 0.00
IR 49.52 1.45 83.68 0.79 4.39 0.00 0.00 0.00 0.00 0.00
NLP 57.33 5.49 86.45 2.38 4.91 0.00 0.00 0.00 0.00 0.00
NNC 69.33 5.21 87.42 2.88 5.93 0.00 0.00 0.00 0.00 0.00
QC 76.75 7.46 88.6 2.14 5.97 0.00 0.00 0.00 0.00 0.00
Robotics 57.6 3.35 86.86 2.65 7.31 0.00 0.00 0.00 0.00 0.00
Mean 59.05 18.33 87.31 1.30 5.357 0.00 0.65 0.00 0.00 0.00
StandardDeviation 9.92 33.53 2.63 1.02 1.97 0.00 0.22 0.00 0.00 0.00
Table9:SIDFigure8:Example2ofanindirectqueryiswhereasentencefromtheresearchpaperisprovidedandaskedforthecorrecttitle.
Here,wecanseethatGPT-4,RAG+Mistral,Adv.RAG+Mistral,Adv.RAG+LLAMAandPerplexityyieldthecorrecttitle.
Figure9:Example1ofzero-shotdirectpromptingdemonstratedthatonlytheAdv.RAG(M),i.e.,withcross-encoderreranking,
accuratelyproducedallthecorrectauthornames.It’snoteworthythatthebasicRAG+Mistralversiononlymadeasingleerrorin
theauthornames,buttheadditionoftheadvancererankingprocessinAdv.RAG+Mistralrectifiedthisandyieldedthecorrect
title.Figure10:Example2ofzero-shotdirectpromptingdemonstratedRAG+LLAMA,Adv.RAG+LLAMA,Adv.RAG+Mistral
yieldsthecorrecttitle.
Figure11:InSIDprompting,askingtheindirectqueryyieldedapassforallmodels.Afterprovidingacompleteabstract([...],in
theimage,wedidnotaddacompleteabstractbecauseofspaceconstraints,buttheactualpromptwasprovidedwithacomplete
abstract),itstillyieldedapass.Then,weprovidedtheabstractnames,whichshowsthatonlyRAGmodelsyieldedtheright
titles.Figure12:WorstcaseexampleofSIDpromptingwhereitdidnotyieldcorrecttitletoanymodel.
Figure13:Anexampleofadirectpromptscenariowhereinitiallyallmodelsfailedtoidentifytheauthornamesandresponded
pass.Uponpresentingtheabstract,allbuttheLLAMAmodel,andtosomeextentMistral(afewofthewrongnamesinthelist
withcorrectnamesweregenerated),failedtorespondappropriatelytotheprompt.