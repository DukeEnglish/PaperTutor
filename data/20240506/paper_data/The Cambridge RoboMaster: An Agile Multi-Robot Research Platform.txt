The Cambridge RoboMaster:
An Agile Multi-Robot Research Platform
Jan Blumenkamp, Ajay Shankar,
Matteo Bettini, Joshua Bird, and Amanda Prorok
Department of Computer Science and Technology
University of Cambridge
United Kingdom
{jb2270, as3233, mb2389, jyjb2, asp45}@cam.ac.uk
Abstract. Compactroboticplatformswithpowerfulcomputeandactu-
ation capabilities are key enablers for practical, real-world deployments
ofmulti-agentresearch.Thisarticleintroducesatightlyintegratedhard-
ware, control, and simulation software stack on a fleet of holonomic
ground robot platforms designed with this motivation. Our robots, a
fleetofcustomisedDJIRobomasterS1vehicles,offerabalancebetween
smallrobotsthatdonotpossesssufficientcomputeoractuationcapabil-
ities and larger robots that are unsuitable for indoor multi-robot tests.
They run a modular ROS2-based optimal estimation and control stack
for full onboard autonomy, contain ad-hoc peer-to-peer communication
infrastructure,andcanzero-shotrunmulti-agentreinforcementlearning
(MARL)policiestrainedinourvectorizedmulti-agentsimulationframe-
work. We present an in-depth review of other platforms currently avail-
able, showcase new experimental validation of our system’s capabilities,
andintroducecasestudiesthathighlighttheversatilityandreliabiltyof
oursystemasatestbedforawiderangeofresearchdemonstrations.Our
system as well as supplementary material is available online. 1
1 Introduction
Multi-agent robotics research necessitates robotic platforms that can be used as
deployment testbeds for rapid development and evaluations [32, 16, 52, 6, 41, 8,
34, 38, 54, 43, 50, 28, 42, 7, 14, 11, 13, 27, 55]. The choices for such platforms
are usually informed by several factors such as the configuration space of the
problem, the number of agents involved, and the computational sophistication
expected from each agent. For instance, small-scale robots, such as the Turtle-
bot [51], will frequently be limited in their agility of motion and are restricted
in their onboard computing capabilities. On the other hand, while larger robots
(Jackal[18]orAutoRally[53])havegreatermaneuverabilityandcancarrymore
compute payloads, operating multiple of them in smaller research spaces can
be cost- and space- prohibitive. Successfully testing and developing multi-robot
solutions requires platforms that strike a fine balance between these objectives.
1 https://proroklab.github.io/cambridge-robomaster
4202
yaM
3
]OR.sc[
1v89120.5042:viXra2 Blumenkamp et al.
Fig.1. We show three snapshots of a swarm of five RoboMasters running a passage
scenario[14],inwhichrobotshavetobreakandreestablishformationtomovethrough
a narrow constriction. On the right, we show a close-up of one RoboMaster equipped
with an NVidia Jetson Orin NX and a forward-facing camera.
In this article, we present an agile and affordable system and platform,
shown in Fig. 1 as a multi-robot team setup. Our system, an omnidirectional
indoor ground robot based on the DJI Robomaster S1, is affordable (starting at
USD 689 in the base configuration to USD 1633 with all sensors and an Jetson
Orin NX), agile (we demonstrate indoor trajectory tracking capabilities of up
to 4.45m/s and accelerations of up to 5m/s2, versatile (we show experiments
in classical control to zero-shot deployment of MARL policies trained in our
simulator VMAS [10]), and is easily replicable (our entire stack is open-sourced,
requiringminimalconstructiontime).Thesehavebeenmadepossibleduetoour
retrofitting of the onboard compute module and careful reverse engineering of
the main communication interface. We present two further contributions in this
work. Second, through an expansive review of related systems and literature,
we contrast our solution against others’ and position our system at the frontier
of low-cost agile research platforms. Third, we describe enhancements to our
prior work (VMAS [10]) that enable rapid deployment of control policies learnt
in simulation onto this platform. We present evaluations and case studies that
corroboratetheclaimspresentedabove,andshowcasethewiderangeofresearch
avenues that this system makes accessible.
2 Related Work
In the last decade, a wide range of (multi-) robot platforms have been intro-
duced to the research community. Here we present an in-depth literature review
of commercial as well as research platforms. Our classification scheme will dif-
ferentiate between them based on their dynamics (Ackermann, differential, and
omnidirectional drive, as well as airborne/multirotor platforms), and also based
on their specific use-cases in single- or multi-agent research. Since we are pri-
marily interested in multi-robot research, we investigate the tradeoff between
cost and speed of these platforms, as agility is an attribute we require in the
team (note that we use ‘speed’ and ‘agility’ interchangeably – this is a justified
simplificationforindoorrobotsoperatinginconfinedspaces),andcostlimitsthe
size of the team.
Fig. 2 visualises our survey in a 2D plot, and Tab. 1 provides detailed statis-
tics specifically for for omnidirectional drive robots. We find that the majorityCambridge RoboMaster for Multi-Robot Research 3
A4WD3[37]
Agilicious[26]
Ackermann [20] single Alice[16]
[19] AMir[8]
Differential multi ASL-Flight[48]
O Aim rbn oid rnir eectional [12] [31] [18][4] A B B C CA o e ou lx at R lo e cuC hrR lo b[a [ 2 ol [2 0l t49y ] 2] 2][5 [53 2]
]
[2] Colias[7]
104 [17]
[25]
[53] C Co ram zp ya fls is esQ [2 28[ ]30]
[47] [1] D De iae bp ora loce [r 22[9 ]]
Dingo-O[17]
103
[41]
[51][21] [30[5]0] [37[ ]4[2 62 ] [[ 23] 93 ]] [3 [26 4] ] [5] [4[4 08 ]] [26] D e F G H J J J- a a eL uu Rp tc sA nc u amkAk t uc a- S e iy Qk tl nrPt o[ euo [ 1S-4 aw [Q 8 [E 41 d 3n ] 6] u 4 ][[ a ][ 144 d ]03 ] [] 36]
[46] [44] RoboMaster(ours) K Mh ee cp aber oa tI PV ro[5 [0 4] 6]
[35] MecabotX[47]
[9] Minicars[32]
MITRacecar[33]
[52]
[38][54]
[28] Future Work
M
M
NaI
o
vT
n
ia- gQ
a[
tu
6
o]a rd R[5 2]
[31]
102 [16] [6] [[ 84 ]2] [34] [43] O P Rh -R oeC nen eA o [3[ [3 855 ]4] ]
[32] RangerMini[2]
Ridgeback[19]
Robotino[25]
[7] R TiP tG an-Q [4u ]ad[24]
TurtleBot3[51]
TurtleBot4[21]
10−1 100 101 X yo3 uB[4 o4 t]
[12]
MaximumVelocity[m/s]
Fig.2. We compare a variety of different research and commercial robotic platforms
concerningtheirtradeoffbetweencostperunitandmaximumvelocity.Wedifferentiate
between Ackermann, Differential, Airborne and Omnidirectional platforms as well as
between platforms specifically targetting multi-agent research. A clear correlation can
be seen between speed and cost, as well as a trend for multi-agent platforms to be
low-cost, but as a consequence slow. We draw the pareto front as a dashed line, with
our proposed platform pushing the limits on the cost and speed tradeoff.
of multi-agent research platforms utilize differential drive [41, 34, 6, 43, 54, 38,
50, 8, 16], thus positioning them in the bottom left corner of Fig. 2. The reason
is that such platforms tend to be designed to minimize cost, with 10s of such
agents in mind. The slowest of these platforms is Alice [16] at 4cm/s, and the
fastest is the Khepera IV [50] with 0.93m/s. Three ground-based platforms are
specifically geared towards multi-agent research but use dynamics other than
differential drive, specifically the Minicar [32], Cellulo[42], and Colias [7]. More
recently, slightly faster commercial differential drive robots have become popu-
lar [18, 21, 51, 22], ranging from 0.22m/s for the Turtlebot 3 [51] to 2m/s for
the Diabolo [22].
From the bottom left, we can see two clusters emerging towards the top and
the right. The omnidirectional drive robots [17, 19, 25, 12, 31, 47, 30, 46, 46,
42, 7, 44, 20, 37, 3, 2] tend to move faster than differential drive, ranging from
0.15m/s (Cellulo [42]) to 3m/s [37], but are also significantly more expensive,
with a mean cost of USD 14k.
Aerial platforms (such as multirotors) on the other hand are moderately
priced at a mean of USD 1600, and can move at much higher speeds of up to
36m/s(Agilicious[26]),butcomewithotherdisadvantagescomparedtoground
robots, such as operational overheads like short runtime (minutes as opposed to
]DSU[tinureptsoC4 Blumenkamp et al.
Table 1. We list critical specifications for omnidirectional robots from related work.
‘Multi-Agent’ indicates whether the platform is introduced as a multi-agent platform,
and ‘Commercial’ indicates whether the platform can be obtained commercially.
Vel Cost Mass LxBxH Multi
Brand/Uni Product Commercial
[m/s] [USD] [kg] [mm] Agent
EPFL Cellulo[42] 0.15 150 0.18 73×80×80 ✓
LincolnUni Colias[7] 0.35 40 0.01 40×40×40 ✓
Hiwonder Jetauto[46] 0.50 650 4.00 324×659×260 ✓
Hangfa CompassQ2[30] 0.75 3,500 16.00 430×330×115 ✓
KUKA youBot[12] 0.80 30,000 20.00 580×380×140 ✓
Clearpath Ridgeback[19] 1.10 50,000 135.00 790×960×310 ✓
Hangfa NavigatorR2[31] 1.20 20,300 35.00 520×480×240 ✓
Clearpath Dingo-O[17] 1.30 10,000 13.00 680×510×111 ✓
RoboWorks MecabotX[47] 1.32 6,000 20.50 630×581×203 ✓
Lynxmotion A4WD3[37] 1.40 900 5.55 372×374×152 ✓
RDK X3[44] 1.50 590 1.93 181×236×185 ✓
Agilex RangerMini[2] 1.50 12,400 64.50 500×738×338 ✓
RoboWorks MecabotPro[46] 1.83 2,600 10.80 541×581×225 ✓
Clearpath Boxer[20] 2.00 60,000 127.00 550×750×340 ✓
Festo Robotino[25] 2.70 8,000 22.80 450×450×325 ✓
Cambridge RoboMaster 4.45 670 3.00 240×320×200 ✓ ✓
hours)ormorechallenginginfrastructureneeds(recoveryandsafetymechanisms
and the availability of accurate indoor tracking). These issues become particu-
larly noticeable and more pronounced when developing multi-agent systems –
the feasibility of an experiment depends on all robots concurrently operating
without faults and risks. In a multi-robot system, individual robots requiring
additional operator attention and overhead can thus pose a significant barrier
to rapid deployment and testing.
To the right of the plot, we observe a cluster of Ackermann drive robots
[4, 33, 29, 35, 32, 9, 53, 1]. These platforms move from top speeds ranging from
0.7m/s for the Minicar [32] (USD 76.5) to 8m/s for the AutoRally [53] (ca.
USD 10k), with an average of USD 4850. While some of these platforms are
agile, they tend to be expensive and not suitable for constrained indoor spaces.
Our solution sits at a frontier beyond which agility (speed) is attained only
by aerial platforms, and a lower cost is only attained by sacrificing agility. Fur-
thermore,aswewilldescribeinlatersections,oursoftwarestacksupportsawide
range of relevant research problems, and has a proven track-record throughout
six previous publications over the last three years [14, 11, 13, 27, 55, 15].
3 Technical Details
This section details the background and design decisions made when developing
our robot platform. We also cover all the supporting network, control and simu-
lationinfrastructurethatcomplementit,andthusenablerapidexperimentation.
3.1 Platform
WeselectedtheDJIRoboMasterS1asourplatform’sbase,leveragingitsdesign
inspiredbytheRoboMastercompetition—aDJIinitiativetofostertechnologicalCambridge RoboMaster for Multi-Robot Research 5
exchangeandinnovationamongregionaluniversities[45].TheS1,aneducational
robot, integrates a tank-like structure with a spring-dampened beam axle front
suspensionandafour-wheelomnidirectionaldrivesystem,andisdesignedforin-
dooruse.Itfeaturesagimbal-mountedblastertoygunandanonboardcomputer.
DJI specifies the base platform’s maximum speeds as 3.5m/s forward, 2.5m/s
backward, 2.8m/s laterally, and 600deg/s rotationally, with a weight of 3.3 kg
(note that as part of our contribution, we side-step these manufacturer-imposed
restrictions). Each wheel has a diameter of 100mm, is powered by a brushless
motorwithintegratedgearsandadriverandsupportsclosed-loopspeedcontrol
up to 1000 RPM and a maximum torque of 0.25 Nm. This gives the platform a
theoretical maximum speed of (π·0.1m·1000RPM)/(60s/M)=5.23m/s. The
system is powered by a 25.91Wh smart battery, offering 2.4Ah capacity. [23].
Theonboardcomputerinterfaceswithasub-controller,managingallsensors
and wheels. We replaced the onboard computer, gimbal, and blaster gun with a
customcomputingsolution.Welaser-cutcomponentsfromacrylictomountour
custom expansion in place of the turret. All our design files are also part of our
open-source release. 2
3.2 Compute
The stock onboard compute module on the RoboMasters is impractical for run-
ning custom algorithms or implementing additional interfaces to cameras, sen-
sors, networks, etc. We therefore upgraded the onboard computer to a more
powerful and versatile single board computer (SBC), focusing on the NVidia
Jetson Orin family for its recency and lifecycle availability until January 20303.
While the Jetson Orin NX 16GB was selected for its robust processing capabili-
ties and suitability for our research, our design easily admits other cost-effective
single-board-computers such as the Raspberry Pi. The Jetson Orin series ne-
cessitates a carrier board. Our choice, the AverMedia D131 carrier board, was
influenced by the availability of a CAN interface – essential for communicat-
ing with the RoboMaster subcontroller – and an optimal balance of interface
availability and cost. It accommodates two PCIe M.2 slots, where we installed
a 256 GB SSD and an Intel 9260 WiFi module. In an alternative Raspberry Pi
setup, on the other hand, we used the built-in ports and interfaces and added
an additional CAN shield for communication with the RoboMaster base.
3.3 CAN Communication
DJI offers an API for the RoboMaster EP, the educational counterpart to the
RoboMaster S1, which enables control via a secondary computer (e.g., a Rasp-
berry Pi) through USB. However, this platform is not listed for sale on the
DJI store, is unavailable in many parts of the world (including the UK), and at
aroundUSD1500,issignificantlycostlierthantheS1model.TheAPI,whichDJI
2 https://github.com/proroklab/cambridge-robomaster
3 https://developer.nvidia.com/embedded/lifecycle, accessed 24/02/226 Blumenkamp et al.
UI Camera UI Camera
Camera GPIO CSI GPIO CSI
USB AverMedia D131
Jetson Orin Raspberry Pi
DJI Smart
Controller WiFi WiFi
Memory Memory
USB USB SPI 5V
CAN C PoA wN e, r CAN Hat D DC C/
CAN Power 12V
Distribution Distribution Distribution
Board Board Board
Fig.3.WemodifytheRoboMasterS1PlatformbyremovingtheUSBCamera,Smart
Controller, Blaster and Turret and by replacing them with either a Jetson Orin on an
AverMedia D131 carrier board or a Raspberry Pi. The platform is equipped with two
WiFi modules, one for infrastructure communication (dark blue) and one specifically
chosen for ad-hoc peer-to-peer communication (green).
CAN Header Ctrl Da 0ta … Da nta CRC EOF … Header Ctrl Da 0ta … Da nta CRC EOF
S Te rari na sl port Start Le Mn Sg Bth Le Ln Sg Bth CRC Da 0ta … Da nta C MR SC B C LR SC B
Package Source Dest S Me Sq B S Le Sq B Ack C Sm ed t C Im Dd Da 0ta … Da nta
Fig.4.TheCAN-basedprotocolusedintheRoboMasters.DJIusesCANastransport
layer to allow multiple connected devices on the bus to broadcast information. Due
to the limited length of CAN frames, a serial transport protocol is built on top, with
a higher-level package definition permitting publisher/subscriber-type communication
architectures.
has made open-source4, served as the foundation for our reverse engineering of
thecommunicationprotocolbetweenthemaincomputerandthesub-controller.
DJI employs Controller Area Network (CAN) for hardware layer transport but
overlays it with a simplified serial package-based protocol. This design likely
addresses the limitations of CAN’s 8-byte data size per packet while facilitat-
ing multi-node network connectivity, leveraging CAN’s collision and arbitration
management capabilities. We outline the protocol’s structure in Fig. 4, noting
its use of a higher-level transport layer to specify node source and destination
IDs, sequence counters for package tracking, various command sets and IDs,
and acknowledgments, enabling subscription to specific package IDs for contin-
uous data stream over the network. We release our C++ implementation of the
messaging protocol as stand-alone repository.5
4 https://github.com/dji-sdk/RoboMaster-SDK, accessed 03/19/24
5 https://github.com/proroklab/robomaster sdk can
iFiW iFiWCambridge RoboMaster for Multi-Robot Research 7
Table 2. We provide a cost breakdown for six different compute and sensing config-
urations for our platform at the time of paper writing. Optional equipment, such as
sensors, are put in parenthesis. All costs are stated in USD.
Item RaspberryPi OrinNano OrinNX
RobotBase:RoboMasterS1 549 549 549
Compute 55 255 670
Carrier:AverMediaD131 185 185
WiFi:Intel9260 15 15
Mem:MicronMTFDKBA256TFK 65 65
Mem:SDCard256GB 30
Infrastructure:DC/DCConverter 20
Infrastructure:WaveshareCANHat 15
Infrastructure:Acrylics 10 10 10
Infrastructure:Mounting 10 10 10
Infrastructure:OLEDDisplay (10) (10) (10)
WiFi:NetgearA6210 (50) (50) (50)
Sensing:PiHQCamera (50) (50) (50)
Sensing:FisheyeLens (20) (20) (20)
TotalBase 689 1089 1504
TotalOptional (819) (1219) (1633)
3.4 Infrastructure
Here we describe components of our supporting infrastructure, each of which is
a product of over 4 years of refining and development. The high reliability of
these subsystems has been vital to our work in deploying multi-agent research.
We provide a high-level overview of the infrastructure in Fig. 5
Wireless Communication. Our system employs dual network interfaces
tofacilitatedistinctcommunicationchannels:abackboneinfrastructure network
for all participating systems, and, an ad-hoc network exclusively for inter-robot
communication. For infrastructure-related communication, we utilize an Intel
9260 PCIe module for the Jetson and the integrated WiFi module for the Rasp-
berry Pi that connects to a central WiFi router (described below). This setup
integrates with broader infrastructure components, such as desktop systems op-
erating the Multi-Robot Manager UI, a motion capture system, and essential
hardware like an emergency-stop button. This network also allows for SSH ac-
cess into the robots, file sharing, and centralized ROS messaging. In scenarios
requiring only centralized control, a desktop machine executes our entire con-
trol stack (Freyja, described later) for each robot, and transmits low-level wheel
commands to each robot via ROS2.
Conversely, for direct robot-to-robot communication, we use Netgear A6210
USB3.0 modules to establish an adhoc multicast network. This configuration
does not rely on external infrastructure, enabling seamless peer-to-peer network
interactions.
Network. The deployment of an enterprise-grade router proved crucial for
centralized robot operation. We opted for a Mikrotik Router, selected for its su-
periorconfigurabilityandflexibleantennasetup.WeemployastructuredIPas-
signmentstrategywithinthe10.x.y.zaddressrangeacrossalldevices,andbridge8 Blumenkamp et al.
Infrastructure
Infrastructure
Router
Ad-hoc P2P
Motion Capture
Docker
Fig.5. We utilize two wireless networks, one for infrastructure (blue) and one for ad-
hoc peer-to-peer communication between agents. The infrastructure network is used
to communicate with fixed infrastructure. The multi-agent user interface to control
multiple agents efficiently is displayed in the top. The ad-hoc network is used for
experiments requiring decentralized communication.
appropriate subnetworks that contain different categories of devices (robots,
computers, guest-devices, etc).
Our router and ad-hoc setups typically operate at a broadcast channel ca-
pacity of 26Mb/s, and we typically observe around 1Mb/s network load when
operating 5 robots concurrently in a centralized mode (transmitting wheel ve-
locity commands to each robot at 50 Hz).
Emergency Stop. Crash prevention is a priority in high-velocity multi-
robot experiments, and can be particularly complex to implement in a fully
decentralized framework. Inevitable system latencies in re-establishing SSH ses-
sions and the finite interval between terminating a control process and robot
immobilization necessitates a more reliable stop mechanism. Consequently, we
integrateanemergencystopserviceintoourRoboMasterROSdriver,accessible
viatheinfrastructurenetwork,thatimmediatelyhaltsvelocitycommandsatthe
lowest level. Additionally, we developed a stand-alone physical emergency stop
button, powered by a Raspberry Pi Zero. This button connects to the infras-
tructurenetworkandfeaturescolorLEDstodisplaytheconnectivityandstatus
of the robots.
On-Robot User Interface.Incorporatingon-robotdisplayshasprovenad-
vantageous,especiallyforoperationsoutsidetheconventionallaboratorysetting
where an infrastructure network may be absent. These displays, compact and
economical at 128×64 pixels, present vital statistics including CPU and mem-
oryusage,CPUtemperature,networkdetails(suchasIPaddressesandinterface
connectivity), battery status, and the robot’s hostname. An integrated button
allows users to navigate through different information pages and, crucially, ac-
tivates the emergency stop service if the robot is in motion. This feature isCambridge RoboMaster for Multi-Robot Research 9
particularly useful in field deployments, offering a direct control and monitoring
mechanism when network access is unavailable.
ROS2 Drivers We implemented a custom ROS2 Driver that accepts both
high-levelvelocitycommandsintherobot’sbodyframeaswellaslow-levelwheel
speed commands. We use systemd to start these drivers automatically during
the system boot to make the experimental setup seamless.
Decentralized DeploymentFordeployingcodeacrossmultiplerobots,we
utilizetheinfrastructurenetwork,employingrsyncoverSSHtosynchronizeup-
dates from a central workstation to each robot. We extensively leverage Docker
containers to remove experimental cross-interference among different users and
setups. This approach is particularly beneficial on the NVidia Jetson platform,
for which an extensive library of pre-configured containers is available. These
containers include environments for ROS, ROS2, PyTorch, and various other
learning frameworks, either standalone or in combination6. Docker is also con-
figured to manage local image repositories within the infrastructure network,
facilitating efficient image sharing among robots.
Multi-Robot User Interface We developed a configurable Python-based
User Interface (UI) operated from a workstation linked to the infrastructure
network. This UI facilitates connections to multiple nodes through SSH and
supports scripting through a configuration file. It is designed to automate the
parallel execution of repetitive commands that normally need to be executed in
different terminals on different devices, thus significantly reducing the manual
overhead associated with logging into individual robots, sourcing workspaces,
and running launch files, especially when identical commands are required on
multiple robots. A snapshot of the UI is shown in Fig. 5.
3.5 Sensors
Our platform is equipped with an array of exteroceptive and proprioceptive
sensors, supported by ROS2 drivers for seamless integration.
Proprioceptive The DJI RoboMaster features an onboard suite of sensors
including a 3D attitude sensor, wheel encoders, body velocity sensors, and bat-
tery level sensors, all accessible through the CAN interface.
Exteroceptive The D131 carrier board accommodates two camera CSI
inputs. We utilize this capability by attaching a Raspberry Pi HQ camera,
equippedwithafisheyelens,deliveringarectifiedimagewitha120◦ fieldofview
(FOV) using OpenCV. Additionally, the RoboMaster is outfitted with physical
bumper sensors for enhanced interaction with its environment.
3.6 Control: Freyja
A necessary step in successful multi-robot deployments is the dependability of
individualstotracktheirtargettrajectoriesaccurately.Towardsthisend,ouron-
boardcontrolstack,Freyja [49],comprisesofanoptimalfeedbackcontrollerand
6 https://github.com/dusty-nv/jetson-containers, accessed on 20/03/2024.10 Blumenkamp et al.
a library of state filters developed entirely in C++ over the ROS2 middleware.
The model-based controller is a linear quadratic regulator (LQR) that regulates
the position and velocity of the robot along a trajectory by commanding wheel
speedsforthelow-leveldriver.Denotingx≡[p⃗,⃗v]⊤ asthestatevariable(withp⃗
and⃗v astheworld-framepositionandvelocityvectors),wecanwriteadynamic
modelforthesystemasx˙ =Ax+Bu,whereAandB aresystemmatrices,and
u is the applied control input. Then, for some reference state x , it is possible
ref
to design a matrix K for the control law, u=−K(x−x ) that stabilizes the
ref
system to that (possibly time-varying) reference state.
3.7 Simulation: VMAS
Simulation is essential in multi-robot research as it enables safety and efficiency
indatacollectionformachinelearningpipelines.State-of-the-artsimulatorsem-
ploy GPU vectorization to massively parallelize this process [39], leveraging
the Single Instruction Multiple Data paradigm. Towards this end, we provide
a model for the RoboMaster platform in the Vectorized Multi-Agent Simulator
(VMAS)[10].VMASisasimulatorconsistingofaPyTorchdifferentiablephysics
engine and a set of challenging multi-robot tasks. Thanks to this, it is possible
to rapidly train and deploy multi-robot policies from the simulator to the real
worldinazero-shotmanner,asdemonstratedinpreviouswork[11](videohere).
To enable this deployment pipeline, we integrated several new features into
the simulator. Firstly, to generate the ⃗v input required by Freyja, we imple-
ref
ment a PID controller layer that regulates the input forces required by VMAS
based on these reference velocities. This layer allows us to train neural network
policies that output desired velocities⃗v and directly use them on our physical
ref
platform,withoutanychange.Secondly,akeytothereal-worlddeploymentwas
an iterative fine-tuning process for some of the physics parameters in VMAS.
These are parameters that depend on the robot platform and its interaction
with the environment, such as: friction coefficients, drag, control ranges, state
boundaries, and robot geometry. This tuning process can be done once for each
robot-environmentpairandcanbethenutilizedforanysubsequentdeployments.
AmodelfortheRoboMasterplatforminVMASenablesapplicationsbeyond
prototyping in simulation and deploying in the real world. For instance, thanks
to the accelerated compute available on our platform, policies trained in VMAS
on approximated representations of the environment can be fine-tuned on the
robot using data collected in the real world.
4 Evaluations
Wenowdemonstratethecharacteristicsofourplatformandthesupportingsuite
of software framework through a wide range of experiments. These explore the
versatile and diverse nature of studies that this platform enables, such as,
– running classical model-based control (Freyja) for agile trajectory tracking,
– zero-shotsim-to-realtransferofmulti-agentnavigationtaskslearntinVMAS,Cambridge RoboMaster for Multi-Robot Research 11
0.5 2 Real
Ref
0.0
0
4
3
2 2
− 1
0
2 0 2 0.0 0.5 1.0 1.5 2.0 2.5 3.0
−
Pos. N [m] Time [s]
0.11 2 0.10
0.09
0
1.8
2 1.6
−
2 0 2 0 2 4 6 8 10
−
Pos. N [m] Time [s]
Fig.6. We demonstrate the efficiency of our platform combined with the Freyja con-
troller suite in two single-agent trajectory tracking experiments, in a straight line tra-
jectory(top)andacirculartrajectory(bottom).Wereportthereferencepositionand
velocity (orange) and measured position and velocity (blue) as well as the tracking
error dp over time.
– runningdistributedvisualSLAMonboardtotrackrelativepositionandthus
avoid collisions, and,
– utilizing large neural networks based on the DinoV2-s architecture to esti-
mate relative poses and then track trajectories.
Finally,wesummarizepriorworkfromourresearchgroupthatutilizesthisplat-
form for real-world deployments of multi-robot systems research.
Classical Trajectory Tracking. We first demonstrate the agility of our plat-
form in a single-robot experiment in which we benchmark the tracking perfor-
mance of Freyja as well asthe velocity of our robot platform athigh speeds. We
demonstrate this on a straight line trajectory and a circle trajectory in Fig. 6.
The straight line trajectory has a length of ca. 5m, and the circle trajectory a
diameter of 1.5m. We show that the robot reaches a top velocity of 4.45m/s,
thus exceeding the maximum rated speed of the DJI RoboMaster platform by
1.15m/s, and accelerations of up to 5m/s2 in the straight line trajectory with a
maximum error of 0.5m, and maintains a velocity of 1.7m/s on the circle tra-
jectory with an average error of 0.1m. In the line trajectory, we are constrained
by the size of the laboratory, which prohibits the robots from going faster.
Sim-to-real Multi-Agent Navigation. We now demonstrate zero-shot de-
ploymentofMARLpoliciesforamulti-robotnavigationtasktrainedinVMAS[10]
following the pipeline described in Sec. 3.7. Agents are rewarded for navigating
]m[
E
.soP
]m[
E
.soP
.soP
.soP
.leV
]m[
∆
.leV
]m[
∆
]s/m[
]s/m[12 Blumenkamp et al.
Time = 0.5 s Time = 2.0 s Time = 3.5 s Time = 5.0 s
2
1
0
1
−
2
−
2 1 0 1 2 2 1 0 1 2 2 1 0 1 2 2 1 0 1 2
− − − − − − − −
Pos. N [m] Pos. N [m] Pos. N [m] Pos. N [m]
Fig.7.Weshowcaseoursim-to-realcapabilitiesonaMARLpositionswappingpolicy
trained entirely in VMAS [10]. Four agents (color-coded) start at the corners of a
1m×1m square and attempt to swap positions with the diagonally opposite agent,
which leads to a necessary conflict at the origin. The lines indicate their trajectories
up to a particular time point, and circles indicate their corresponding goals. Agents
are able to seamlessly execute the learned policy in the previously unseen real-world
environment. We show snapshots from the real-world setup in the bottom.
to their goal while avoiding collisions with each other, to do so they use the
GPPO model ([11]), which leverages a Graph Neural Network (GNN) in the
policy for inter-agent communication. The GNN utilizes a 5-layer MLP with a
total of 2300 trainable parameters, which can be evaluated on the Jetson Orin
NX in 0.5ms. The results, reported in Fig. 7, show that the agents are able to
seamlessly execute the learned policy in the previously unseen real-world envi-
ronment.
Distributed Visual SLAM. In this demonstration, we showcase the system’s
ability to run a decentralized visual SLAM system locally onboard using the
optionallyattachedRaspberryPiHQcameras.OurcollaborativeSLAMsystem
enables the agents to share a unified map of the world, thus facilitating rela-
tive positioning even when their views do not overlap, and even when the other
agent is not explicitly detected in view. To validate the quality of this shared
map, we use a nonlinear model predictive controller avoid collisions with both
peers and static obstacles. Fig. 8 tests a collision avoidance scenario at an inter-
section,wheretworobots(travelingalongthehorizontalandverticalaxes)would
nominally collide at the intersection. However, they are able localize each other
close to the origin when common features are available in the environment, and
therobottravelingalongtheverticalaxisisabletoslowdowntoavoidcolliding.
Multi-robot control using CoViS-Net Foundation Model. In this exper-
iment, we deploy our CoViS-Net foundation model [15], a pre-trained Neural
Network to estimate relative poses between two camera frames, on two different
]m[
E
.soPCambridge RoboMaster for Multi-Robot Research 13
1.0
5
0.5 4
0.0 3
2
0.5
−
1
1.0
−
0
1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − −
Pos. E[m]
Fig.8. We demonstrate multi-agent collision avoidance, facilitated by a distributed
visual SLAM system running locally on the robots. Two robots are set 90° to each
other in an intersection environment (right). The agent travelling along the E axis
is given a goal pose on the other side of the intersection, and successfully avoids a
collisionwhentheagenttravellingalongtheNaxisispushedthroughtheintersection.
The trajectories generated by the SLAM system are presented on the left chart.
robots.Thecameraimagesoftworobotsareencoded,andthefeaturesarebroad-
casted over the ad-hoc network to other robots within communication range. A
second model then compares the features in the image to predict the relative
pose,whichisthenusedtotrackatrajectoryusingFreyja.Inthissetup,weuse
two robots, one that serves as origin for the reference coordinate frame of the
other robot, which is commanded to move along a shifted circle trajectory. The
result, visualized in Fig. 9, shows that we are able to move with a velocity of
about 1m/s on the circle, with an average estimated error of 0.25m. Our model
usestheDinoV2-sbaseneuralnetworkwith21Mparameters,whichweareable
to run in 20ms on the Jetson Orin NX after optimising with NVidia TensorRT.
Case Studies.Oursystemhasreachedthislevelofmaturityoverthecourseof
severaliterationsofresearchanddevelopmentinthelastfouryears.Apartfrom
the capabilities demonstrated above, this framework has featured as a primary
research platforms in several prior multi-agent work from our group:
– Multi-agent passage scenario, where five RoboMasters navigate through a
narrowpassage,coordinatedbyonboarddecentralizedhomogeneousGNN[14];
– Real-world HetGPPO, where heterogeneous GNNs trained in VMAS de-
ployedonRoboMastersdemonstratesuperiorresiliencetoreal-worldnoise[11];
– Visual Navigation, where a RoboMaster navigates an environment guided
entirely using visual sensors and a policy trained in simulations [13];
– CBFs for multi-agent control, where four RoboMasters use a control barrier
function based GNN strategy to navigate [27]; and,
– Single-agentsearch&navigation,whereLQR,CBFandRRT*arecombined
for safe and optimal single-robot motion planning [55].
]m[N
.soP
]s[emiT14 Blumenkamp et al.
2
0.50
1
0.25
0
Real 2
1
− Ref 1
2
−
2 0 2 0.0 2.5 5.0 7.5 10.0 12.5 15.0
−
Pos. N [m] Time [s]
Fig.9. The decentralized trajectory tracking tracks a reference trajectory relative to
a stationary robot (red dot), based on the relative poses estimated by a pre-trained
neuralnetworkthatonlyutilizesfeaturesoftheenvironmentitself.WerunthisNeural
Network on-board, in real-time. The position error and velocity is estimated from the
predicted poses.
– Deployment of multi-agent foundation models,whereuptofourRoboMaster
estimatetherelativeposetoperformdecentralizedformationcontrolinreal-
world scenarios [15].
5 Conclusion
This article introduces a versatile omni-directional ground robot system that is
basedonextensivemodificationandenhancementoftheoriginalDJIRoboMas-
ter S1 platform. We significantly extend the capabilities of the base platform by
providingmultipleoptionsformoreflexibleandpowerfulcomputesolutions,on-
board sensors, model-based control, and sim-to-real capabilities for distributed
RLpolicies.Afteranintensiveresearchanddevelopmentpipelineoverfouryears,
ourfullplatform(hardwareandsoftware)nowservesasahighlyreliabletestbed
well-suitedforawiderangeofexperiments.Weshowcaseitscapabilitiesthrough
four new evaluations, and refer to five case studies from prior work that point
to its proven track record.
Acknowledgments
This work was supported by ARL DCIST CRA W911NF-17-2-0181 and Euro-
pean Research Council (ERC) Project 949949 (gAIa). J. Blumenkamp acknowl-
edges the support of the ‘Studienstiftung des deutschen Volkes’ and an EPSRC
tuition fee grant. We also acknowledge a gift from Arm. We further thank all
other members of the Prorok Lab for their suggestions and helpful discussions
that have contributed towards topics reported in this article.
References
1. Agilex. Agilex hunter website. https://global.agilex.ai/chassis/9. Accessed: 2024-
03-14.
]m[
E
.soP
.soP ]m[
∆
.leV ]s/m[Cambridge RoboMaster for Multi-Robot Research 15
2. Agilex. Agilex ranger mini website. https://global.agilex.ai/chassis/6. Accessed:
2024-03-14.
3. Agilex. Agilex ranger website. https://global.agilex.ai/chassis/7. Accessed: 2024-
03-14.
4. Agilex. Agilex titan website. https://global.agilex.ai/chassis/15. Accessed: 2024-
03-14.
5. AmadoAntonini,WinterGuerra,VarunMurali,ThomasSayre-McCord,andSer-
tac Karaman. The blackbird dataset: A large-scale dataset for uav perception
in aggressive flight. In International Symposium on Experimental Robotics, pages
130–139. Springer, 2018.
6. Farshad Arvin, Jose Espinosa, Benjamin Bird, Andrew West, Simon Watson, and
Barry Lennox. Mona: an affordable open-source mobile robot for education and
research. Journal of Intelligent & Robotic Systems, 94:761–775, 2019.
7. Farshad Arvin, John C. Murray, Licheng Shi, Chun Zhang, and Shigang Yue.
Development of an autonomous micro robot for swarm robotics. In 2014 IEEE
International Conference on Mechatronics and Automation, pages 635–640, 2014.
8. Farshad Arvin, Khairulmizam Samsudin, Abdul Rahman Ramli, et al. Develop-
mentofaminiaturerobotforswarmroboticapplication. International Journal of
Computer and Electrical Engineering, 1(4):436–442, 2009.
9. Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet
Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, Eddie Calleja, Sunil
Muralidhara,andDhanasekarKaruppasamy. Deepracer:Autonomousracingplat-
form for experimentation with sim2real reinforcement learning. In 2020 IEEE
International Conference on Robotics and Automation (ICRA), pages 2746–2754,
2020.
10. Matteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. Vmas:
A vectorized multi-agent simulator for collective robot learning. In Proceedings
ofthe16thInternationalSymposiumonDistributedAutonomousRoboticSystems,
DARS ’22. Springer, 2022.
11. Matteo Bettini, Ajay Shankar, and Amanda Prorok. Heterogeneous multi-robot
reinforcement learning. In Proceedings of the 2023 International Conference on
Autonomous Agents and Multiagent Systems,AAMAS’23,page1485–1494,Rich-
land, SC, 2023. International Foundation for Autonomous Agents and Multiagent
Systems.
12. RainerBischoff,UlrichHuggenberger,andErwinPrassler. Kukayoubot-amobile
manipulator for research and education. In 2011 IEEE International Conference
on Robotics and Automation, pages 1–4, 2011.
13. Jan Blumenkamp, Qingbiao Li, Binyu Wang, Zhe Liu, and Amanda Prorok. See
what the robot can’t see: Learning cooperative perception for visual navigation.
In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 7333–7340. IEEE, 2023.
14. Jan Blumenkamp, Steven Morad, Jennifer Gielis, Qingbiao Li, and Amanda Pro-
rok. A framework for real-world multi-robot systems running decentralized gnn-
based policies. In 2022 International Conference on Robotics and Automation
(ICRA), pages 8772–8778, 2022.
15. JanBlumenkamp,StevenMorad,JenniferGielis,andAmandaProrok. Covis-net:
A cooperative visual spatial foundation model for multi-robot applications, 2024.
16. Gilles Caprari and Roland Siegwart. Design and control of the mobile micro
robot alice. In Proceedings of the 2nd International Symposium on Autonomous
Minirobots for Research and Edutainment, AMiRE 2003: 18-20 February 2003,
Brisbane, Australia, pages 23–32. CITI, 2003.16 Blumenkamp et al.
17. Clearpath Robotics Inc. Dingo: Indoor robotic platform. Technical report,
Clearpath Robotics Inc., 2019.
18. Clearpath Robotics Inc. Jackal unmannaged ground vehicle: Technical specifica-
tions. Technical report, Clearpath Robotics Inc., 2020.
19. ClearpathRoboticsInc. Ridgeback:Omnidirectionaldevelopmentplatform. Tech-
nical report, Clearpath Robotics Inc., 2020.
20. Clearpath Robotics Inc. Boxer: Indoor robotic platform. Technical report,
Clearpath Robotics Inc., 2022.
21. ClearpathRoboticsInc. Turtlebot4:Roboticslearningplatform. Technicalreport,
Clearpath Robotics Inc., 2022.
22. DirectDriveTech. Diablo: World’s first direct-drive self-balancing
wheeled-leg robot. https://shop.directdrive.com/products/
diablo-world-s-first-direct-drive-self-balancing-wheeled-leg-robot. Accessed:
2024-03-14.
23. DJI. DJI RoboMaster S1 website. https://www.dji.com/uk/robomaster-s1. Ac-
cessed: 2023-02-21.
24. Matthias Faessler, Antonio Franchi, and Davide Scaramuzza. Differential flatness
of quadrotor dynamics subject to rotor drag for accurate tracking of high-speed
trajectories. IEEE Robotics and Automation Letters, 3(2):620–626, 2017.
25. FESTO. Festo robotino. https://ip.festo-didactic.com/InfoPortal/Robotino/
Overview/EN/index.html. Accessed: 2024-03-14.
26. PhilippFoehn,EliaKaufmann,AngelRomero,RobertPenicka,SihaoSun,Leonard
Bauersfeld, Thomas Laengle, Giovanni Cioffi, Yunlong Song, Antonio Loquercio,
etal. Agilicious:Open-sourceandopen-hardwareagilequadrotorforvision-based
flight. Science robotics, 7(67):eabl6259, 2022.
27. ZhanGao,GuangYang,andAmandaProrok. Onlinecontrolbarrierfunctionsfor
decentralized multi-agent navigation. In 2023 International Symposium on Multi-
Robot and Multi-Agent Systems (MRS), pages 107–113. IEEE, 2023.
28. Wojciech Giernacki, Mateusz Skwierczyn´ski, Wojciech Witwicki, Pawe(cid:32)l Wron´ski,
andPiotrKozierski. Crazyflie2.0quadrotorasaplatformforresearchandeduca-
tioninroboticsandcontrolengineering. In201722ndInternationalConferenceon
Methods and Models in Automation and Robotics (MMAR), pages 37–42. IEEE,
2017.
29. J Gonzales, F Zhang, K Li, and F Borrelli. Autonomous drifting with onboard
sensors. In Advanced Vehicle Control, pages 133–138. CRC Press, 2016.
30. Hangfa. Hangfa Compass Q2. http://hangfa.com/EN/robot/CompassQ2.html.
Accessed: 2024-03-14.
31. Hangfa. Hangfa Navigator Q2. http://hangfa.com/EN/robot/NavigatorQ2.html.
Accessed: 2024-03-14.
32. Nicholas Hyldmar, Yijun He, and Amanda Prorok. A fleet of miniature cars for
experimentsincooperativedriving. In2019 International Conference on Robotics
and Automation (ICRA), pages 3238–3244, 2019.
33. Sertac Karaman, Ariel Anders, Michael Boulet, Jane Connor, Kenneth Gregson,
WinterGuerra,OwenGuldner,MubarikMohamoud,BrianPlancher,RobertShin,
and John Vivilecchia. Project-based, collaborative, algorithmic robotics for high
school students: Programming self-driving race cars at mit. In 2017 IEEE Inte-
grated STEM Education Conference (ISEC), pages 195–203, 2017.
34. Serge Kernbach, Ronald Thenius, Olga Kernbach, and Thomas Schmickl. Re-
embodimentofhoneybeeaggregationbehaviorinanartificialmicro-roboticsystem.
Adaptive Behavior, 17(3):237–259, 2009.Cambridge RoboMaster for Multi-Robot Research 17
35. AlexanderLiniger,AlexanderDomahidi,andManfredMorari.Optimization-based
autonomousracingof1:43scalerccars. Optimal Control Applications and Meth-
ods, 36(5):628–647, 2015.
36. GiuseppeLoianno,ChrisBrunner,GaryMcGrath,andVijayKumar. Estimation,
control, and planning for aggressive flight with a small quadrotor with a single
camera and imu. IEEE Robotics and Automation Letters, 2(2):404–411, 2017.
37. Lynxmotion. Lynxmotion A4WD3. https://www.lynxmotion.com/
a4wd3-rugged-rovers/. Accessed: 2024-03-14.
38. James McLurkin, Adam McMullen, Nick Robbins, Golnaz Habibi, Aaron Becker,
Alvin Chou, Hao Li, Meagan John, Nnena Okeke, Joshua Rykowski, et al. A
robot system design for low-cost multi-robot manipulation. In 2014 IEEE/RSJ
international conference on intelligent robots and systems, pages 912–918. IEEE,
2014.
39. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller,
Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar,
Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: A unified
simulation framework for interactive robot learning environments. IEEE Robotics
and Automation Letters, 8(6):3740–3747, 2023.
40. KartikMohta,MichaelWatterson,YashMulgaonkar,SikangLiu,ChaoQu,Anurag
Makineni, Kelsey Saulnier, Ke Sun, Alex Zhu, Jeffrey Delmerico, et al. Fast,
autonomous flight in gps-denied and cluttered environments. Journal of Field
Robotics, 35(1):101–120, 2018.
41. Francesco Mondada, Michael Bonani, Xavier Raemy, James Pugh, Christopher
Cianci,AdamKlaptocz,StephaneMagnenat,Jean-ChristopheZufferey,DarioFlo-
reano, and Alcherio Martinoli. The e-puck, a robot designed for education in
engineering. In Proceedings of the 9th conference on autonomous robot systems
and competitions, volume 1, pages 59–65. IPCB: Instituto Polit´ecnico de Castelo
Branco, 2009.
42. AyberkO¨zgu¨r,S´everinLemaignan,WafaJohal,MariaBeltran,ManonBriod,L´ea
Pereyre, Francesco Mondada, and Pierre Dillenbourg. Cellulo: Versatile handheld
robotsforeducation. InProceedingsofthe2017ACM/IEEEInternationalConfer-
ence on Human-Robot Interaction, HRI ’17, page 119–127, New York, NY, USA,
2017. Association for Computing Machinery.
43. Liam Paull, Jacopo Tani, Heejin Ahn, Javier Alonso-Mora, Luca Carlone, Michal
Cap, Yu Fan Chen, Changhyun Choi, Jeff Dusek, Yajun Fang, Daniel Hoehener,
Shih-YuanLiu,MichaelNovitzky,IgorFranzoniOkuyama,JasonPazis,GuyRos-
man,ValerioVarricchio,Hsueh-ChengWang,DmitryYershov,HangZhao,Michael
Benjamin,ChristopherCarr,MariaZuber,SertacKaraman,EmilioFrazzoli,Domi-
tilla Del Vecchio, Daniela Rus, Jonathan How, John Leonard, and Andrea Censi.
Duckietown: An open, inexpensive and flexible platform for autonomy education
andresearch. In2017IEEEInternationalConferenceonRoboticsandAutomation
(ICRA), pages 1497–1504, 2017.
44. RDK. RDK X3. https://category.yahboom.net/collections/r-omnidirection/
products/rdk-x3-robot?variant=46378068967740. Accessed: 2024-03-14.
45. RoboMasterOrganizingCommittee(RMOC). Robomaster2023universityleague
rules manual. Technical report, DJI, T2, 22F, DJI Sky City, No. 55 Xianyuan
Road, Nanshan District, Shenzen, China, November 2022.
46. RoboWorks. RoboWorks Mecabot Pro. https://www.roboworks.net/store/p/
mecabotpro. Accessed: 2024-03-14.
47. RoboWorks. RoboWorks Mecabot X. https://www.roboworks.net/store/p/
mecabot-x. Accessed: 2024-03-14.18 Blumenkamp et al.
48. Inkyu Sa, Mina Kamel, Michael Burri, Michael Bloesch, Raghav Khanna, Marija
Popovi´c, Juan Nieto, and Roland Siegwart. Build your own visual-inertial drone:
Acost-effectiveandopen-sourceautonomousdrone. IEEERobotics&Automation
Magazine, 25(1):89–103, 2017.
49. AjayShankar,SebastianElbaum,andCarrickDetweiler. Freyja:Afullmultirotor
systemforagile&preciseoutdoorflights. In2021 IEEE International Conference
on Robotics and Automation (ICRA), pages 217–223. IEEE, 2021.
50. Jorge M Soares, Inaki Navarro, and Alcherio Martinoli. The khepera iv mobile
robot:performanceevaluation,sensorydataandsoftwaretoolbox. InRobot 2015:
Second Iberian Robotics Conference: Advances in Robotics, Volume 1, pages 767–
781. Springer, 2016.
51. Turtlebot. Turtlebot3website. https://www.turtlebot.com/turtlebot3/. Accessed:
2024-03-14.
52. Hanlin Wang and Michael Rubenstein. Shape formation in homogeneous swarms
using local task swapping. IEEE Transactions on Robotics, 36(3):597–612, 2020.
53. Grady Williams, Paul Drews, Brian Goldfain, James M. Rehg, and Evangelos A.
Theodorou. Aggressive driving with model predictive path integral control. In
2016 IEEE International Conference on Robotics and Automation (ICRA), pages
1433–1440, 2016.
54. Sean Wilson, Ruben Gameros, Michael Sheely, Matthew Lin, Kathryn Dover,
Robert Gevorkyan, Matt Haberland, Andrea Bertozzi, and Spring Berman.
Pheeno,aversatileswarmroboticresearchandeducationplatform. IEEERobotics
and Automation Letters, 1(2):884–891, 2016.
55. GuangYang,MingyuCai,AhmadAhmad,CalinBelta,andRobertoTron.Efficient
lqr-cbf-rrt*: Safe and optimal motion planning. arXiv preprint arXiv:2304.00790,
2023.