1
On the Utility of External Agent Intention Predictor
for Human-AI Coordination
Chenxu Wang, Zilong Chen, Angelo Cangelosi and Huaping Liu∗
Abstract—Reaching a consensus on the team plans is vital widely applied in Human-Agent Collaboration [12, 13, 14].
to human-AI coordination. Although previous studies provide Typically, the team may communicate through graphical or
approaches through communications in various ways, it could
textual ways to align their mental states [15]. However, as
still be hard to coordinate when the AI has no explainable plan the tasks and environments become complex and erratic, tra-
to communicate. To cover this gap, we suggest incorporating
externalmodelstoassisthumansinunderstandingtheintentions ditional planning-based models are not always feasible. When
ofAIagents.Inthispaper,weproposeatwo-stageparadigmthat it comes to Deep Reinforcement Learning (DRL), a class of
firsttrainsaTheoryofMind(ToM)modelfromcollectedoffline algorithms that have been widely used in many challenging
trajectories of the target agent, and utilizes the model in the
robotic tasks [6, 16, 17], the model may be opaque and only
process of human-AI collaboration by real-timely displaying the
output an atomic action at each time step with no human-
future action predictions of the target agent. Such a paradigm
leaves the AI agent as a black box and thus is available explainableplanstoshow.ThoughtheexplainableAIhasbeen
for improving any agents. To test our paradigm, we further proven important for human-AI collaboration [18], there may
implementatransformer-basedpredictorastheToMmodeland exist a trade-off between the explainability and performance
developanextendedonlinehuman-AIcollaborationplatformfor
[19, 20]. Prior works also substantiate the importance of
experiments.Thecomprehensiveexperimentalresultsverifythat
communicating about future actions in human-robot collabo-
human-AI teams can achieve better performance with the help
of our model. A user assessment attached to the experiment ration [21, 22], however, those actions are usually determined
furtherdemonstratesthatourparadigmcansignificantlyenhance in advance and are not applicable in real-time interactive
the situational awareness of humans. Our study presents the collaboration, let alone deep learning models that generate
potentialtoaugmenttheabilityofhumansviaexternalassistance
atomactions.Therefore,howtoconductcommunicationwhen
in human-AI collaboration, which may further inspire future the AI agent is not explainable is still an open problem.
research.
To mitigate this predicament, we suggest building an exter-
Index Terms—Human-AI Cooperation; Deep Reinforcement
nalTheoryofMind(ToM)modeltopredictthefutureactions
Learning; Human-Agent Interaction; Intention Prediction
oftheAIagentandassisthumansinrecognizingitsintentions.
As the example shown in Figure 1(b), the ToM model which
I. INTRODUCTION has watched past records of the robot can predict the future
WiththeflourishinggrowthofAI,Human-AICollaboration action of the robot (left, up, left), implying the robot will
has been receiving increasing research interest [1, 2, 3] and choose the top way. The predictions can then be presented in
the relevant techniques are applied in various domains, such appropriate ways, e.g., drawn on the map, to help the human
as robotics [4, 5, 6], data science [7], and decision making better coordinate with the robot.
[8,9].Unlikecompetitiveorsolotasks,AIagentsandhumans Carrying on this idea, we propose a two-stage paradigm to
share the same goal in collaboration tasks. This naturally assist humans in the human-AI collaboration as illustrated in
encourages each individual to think over the plan and avoid Figure2.Fortheagentwewanttopredict,whichisdenotedas
conflict to coordinate better with their teammates to get a the target agent hereinafter, we first collect the trajectories by
higher payoff [10, 11]. Therefore, building a shared mental pairing the target agent with other agents to form a dataset,
modeltounderstandtheintentionofpartnersisfundamentally and then train a stand-alone ToM model from its historical
essential, which enables all teammates to reach a consensus data to model the behavior of the target agent. In the human-
on task allocations and route planning, and may be vital to AI collaboration stage, we utilize the ToM model to predict
collaboration.ConsiderasimplecaseshowninFigure1(a),in the future action sequence of the target agent from the states
whichahumanandarobotneedtointerchangetheirpositions of the ongoing task. The predictions are visually displayed in
in a narrow place. To pass the in-between obstacle, they must real-time to help the human better understand the intention of
reach a consensus on the route selection to avoid collisions. the AI agent, which can further benefit the efficiency of the
However, this could be difficult for the team since they have collaboration.
no access to the other’s plan. Ourproposedparadigmisdistinguishedfromexistingtech-
niques such as plan communication [14, 21] by the following
Anintuitivewaytoresolvethisdilemmaiscommunication,
characteristics: (1) our paradigm does not require any prior
which is natural in human-human collaboration and has been
knowledge of the environment, the prediction is at the action
*Correspondingauthor level, ensuring its availability in general DRL scenarios. (2)
Chenxu Wang, Zilong Chen and Huaping Liu are with Department of
The ToM model is trained from offline data and can be
Computer Science and Technology, Tsinghua University, Beijing, China.
AngeloCangelosiiswiththeUniversityofManchester,U.K. regarded as a complete post-process that has no effect on
4202
yaM
3
]CH.sc[
1v92220.5042:viXra2
TThhee rroobboott mmaayy ggoo SSeeeemmss tthhee rroobboott wwiillll
WWhhiicchh wwaayy wwiillll tthhee I can only output ←←↑↑←←，，II ddrraaww tthheemm ggoo tthhee ttoopp wwaayy,, II sshhoouulldd
rroobboott cchhoooossee?? an action ...
oonn tthhee mmaapp.. cchhoooossee tthhee bboottttoomm oonnee..
？？
？？
(a) (b)
Fig.1. Arepresentativecaseinwhichthehumanandtherobotwanttoswitchtheirpositions.Theflagsarethedestinationsofthecorrespondingcharacter
withrespecttothecolors,andthegrayareadenotesthewallorobstacle.Thelinesdenotethehuman’sguessoftheroutes,whereasthefulllinesstandfor
higherconfidence.Thepotentialdifficultyinthecollaborationisreachinganagreementontheselectionofrouteswithoutcollisions.
the behavior of the target agent, providing compatibility for
all DRL algorithms. (3) In our paradigm, the agent can be
regarded as a gray box or even a black box, which establishes Self-play
Cross-play
the potential of the ToM model to be a third-party assistant
for practical applications. The target agent
PPaarrttnneerrss
We assume the ToM model can assist humans to be better
Historical data
aware of the intentions of their AI partners and coordinate
better. To evaluate our paradigm and test our hypotheses,
we first implement a transformer-based model to predict the
Modeling Utilization
desired future action sequence. We then develop an extended
online experimental platform that is capable of displaying the
prediction and user study based on the Overcooked environ- ToM model
ment [3], which is a two-player common-payoff collaborative
game that has been widely used for studying human-AI
Featurize Inference
collaborationandzero-shotcollaboration[23,24,25,26].Our
paradigmandmodelarecomprehensivelytestedwithtwoDRL ToM model
methods in extensive human-AI collaboration experiments Act Display
acrossmultiplelayouts.Theresultsdemonstratethatourwork
Human
can help humans better grasp the intentions of AI partners
Human-AI collaboration
to enhance their situational awareness and ultimately improve
collaboration performance in various situations.
We summarize our main contributions as follows: Fig.2. Anillustrationofourtwo-stageparadigmforhuman-AIcollaboration.
• We propose a novel human-AI collaboration paradigm
that incorporates an external ToM model to predict the recentworksthatarerelatedtoourstudy.Theimplementation
intention of AI agents for humans to facilitate the coor- ofourmethods,includingproblemformulation,datacollection
dination of human-AI teams. process, and the destination of the ToM model are introduced
• Weimplementatransformer-basedpredictoranddesigna in Section III. In Section IV, we expatiate the experiment
frameworktotrainsuchamodel.Thepredictoriscapable designation and details of the environment. We then present
of predicting the sequence of future actions without quantitative results and subjective user assessment with corre-
access to the target agent. sponding analysis. Finally, we make a conclusion and discuss
• We extend the Overcooked environment to support the the limitations and future works in Section V.
visual presentation of intentions and set up an online
experiment platform for human-AI collaboration exper-
II. RELATEDWORK
iments.
• Weperformextensiveexperimentstovalidateourmethod A. Human-AI collaboration
with two types of DRL agents. The results demonstrate AsafrontierofAIapplication,human-AIcollaborationhas
the utility of adopting an external ToM model in human- recentlyattractedmuchattentionfromresearchers.Inpractice,
AI collaboration and our analysis reveals the underlying Deep Reinforcement Learning has been proven to be a decent
mechanism of the improvements. solutionfortrainingAIagents,whichempiricallyshowsbetter
Thispaperisorganizedasfollows:inSectionII,wereview performance [27, 3]. Though self-play may train agents with3
good capability, they do not perform well in the cooperative C. Explainable AI
setting. To address this problem, many studies concentrate
TohelphumansbetterunderstandAI,explainableAI(XAI)
on mitigating the disparity between training data and real
has been widely studied [20, 45], and is also extensively
collaboration by finding appropriate partners to train with. [3]
utilizedinhuman-AIteaming[46,47,9].Aclassicalapproach
shows that training with behavior cloning models of humans
is to adopt policies with intrinsic explainability, such as using
as partners can achieve better performance. Without incorpo-
a decision tree [18] or planning-based methods [15]. Another
rating human data, treating the human-AI collaboration task
schema is to generate a human-comprehensible explanation
as a special case of zero-shot collaboration and improving the
of the motivation and decision-making factors through specif-
general cooperation ability of the agent is also a popular and icallydesignmodels,e.g.,generatingacaptionforautonomous
promising direction. Several prior works train the agent with
driving [48] or explaining the reward of RL-based policies
a diversified group of pretrained partners [23, 24, 25, 28, 29],
[49]. However, most existing works focus on explaining the
urge the agent to accommodate various strategies [30], or
decision-making motivation instead of the future intention.
utilize the underlying symmetries [31].
Though planning-based or symbolic algorithms may have
Another direction of improvement is modeling the team-
explainable behaviors, they may not perform as well as those
matestoinferandmakeuseoftheirintentions[32,33],which
based on deep learning.
has fruitful achievements in the human-robot collaboration
Generally, one can not expect the AI agent to have explain-
[34, 35, 36]. Some recent studies employ the Theory of Mind
ableintentions,especiallyincomplexscenariosandwithDRL-
(ToM) to update the belief of other agents’ intents through
based agents. In contrast, the external intention predictor in
Bayes inference and accordingly adjust their plans [37, 38].
our paradigm has no limitation on the algorithm of the agent,
Beyond Bayes, ToM models can also be implemented by
which can be regarded as a universally available plug-in for
neural networks, which show their advantages in many multi-
human-AI collaboration.
agent collaboration scenarios [39, 40].
However, most of the studies focus on improving the
III. METHODOLOGY
capability of AI or helping AI to better model and utilize the
In this section, we introduce the implementation details of
intentsofteammates,fewofthemcareaboutassistinghumans
theToMmodelinourparadigmshowninFigure2,whichare
to understand the intention of AI for better collaboration.
trained from the collected offline data and predict the future
B. Communication in Human-AI teaming actions of the agent from the trajectory of the ongoing game
at the inference stage.
Communication,asthebondthatconnectsmultipleindivid-
uals, has been widely recognized as important for Human-AI
Collaborationandstudiedinvariousaspects[12,15,41].[13] A. Preliminaries
shows that visual communication can effectively improve the A typical two-player Markov Decision Process (MDP) is
shared situational awareness in human-robot teaming. From defined as a tuple <S,{Ai},T,R>, where S is a finite set
the content aspect, delivering the intention of AI to humans of states. Ai is the finite set of actions available to agent i,
is also known as helpful for collaboration. [21] shows that where i ∈ {1,2}. The joint action space is written as A =
unilateral communication that depicts an image of the robot’s A1 × A2, and the joint action is denoted as a = (a1,a2),
future plan through a head-mount display could significantly where a1 ∈ A1 and a2 ∈ A2 are the actions of the two
enhance collaboration performance. [18] studies the utility of agents.Risareal-valuedrewardfunction,andT :S×A(cid:55)→S
explaining the decision-making logic of AI and finds that is the transition function that maps the current state and the
explanationcanimprovesituationalawarenessandfurtherhelp joint actions to the next state. In collaboration settings, both
the human AI team to achieve better performance. [14] shows agents share the same reward and are trained to maximize the
(cid:104) (cid:105)
theeffectivenessofvisualizingthebranchedplansforhuman- expectedrewardE (cid:80)T R(s ,a ) ,whereτ isthetrajectory,
τ t t t
agent decision making.
T is the number of total time steps, and s and a are the
t t
However, most communication is on the basis of the ex-
state and the joint action at t-th time step, respectively. For
plainability of the agent since the information must be con-
human-AI collaboration, one of the agents will be controlled
veyed in human-compatible manners, e.g., in visual or textual
byhumans,andtheotherwillbecontrolledbyatrainedpolicy
form. Therefore, the existing works either require rule-based
π. The action space of the human and the AI are denoted as
models which are relatively simple [21, 22, 42], or require
AH and AAI, respectively.
specifiedalgorithms[15,19,18],orrelyonthecommunication
mechanism embed in the environment [43]. Generally, the
B. Problem Setting
powerful DRL methods are not universally transparent and
lack explainability [44], therefore are not directly applicable Suppose that we have a trained policy π which will coop-
to communication. erate with humans later and is denoted as the target agent.
Our communication form which visually displays future We aim to train a ToM model F with trainable parameters θ
actions has been widely adopted and verified by previous from the offline trajectories to predict the future actions of π.
works. The contribution of our proposed paradigm lies in The model F is expected to take the trajectory of the current
filling the gap of communicating the intentions of black-box game as input and outputs a sequence of action predictions.
and unexplainable agents. The input trajectory can be formed as (H,s ), where n is the
n4
......
...... Action ......
Embedding
Joint Actions + Transformer MLP ......
Encoder Decoder
Predictions
......... Feature ...... ......
Extractor
History Features
Pre-trained Backbone
States
Fig.3. Anoverviewofthearchitectureofthepredictionmodel.
length of trajectory, H = {(s ,a ),...,(s ,a )} is the Algorithm 1 Data collection process
1 1 n−1 n−1
history observation-action pairs and s is the current observa- Input: The MDP M, target policy π, game setting set
n
tion. The prediction sequence is denoted by y ={y ,...,y }, C
1 l
where l is the expected prediction length, and y ∈ AAI is Output: Trajectory dataset D
l
the predicted actions. Note that the ToM model is assumed to 1: D ←∅
be external and has no access to the action that π A will take 2: P ←{π}
for the current step, thus the prediction starts from the current 3: agents← train agents(M)
step, i.e., y 1 corresponds to a n. 4: for each agent∈agents do
We train the ToM model via supervised learning, where an 5: checkpoints← select checkpoints(agent)
(cid:83)
offline trajectory dataset D is built for training. The task can 6: P ←P checkpoints
then be formulated into an optimization problem to maximize 7: for each π p ∈P do
the log-likelihood: 8: for each c∈C do
m θax E
(H,sn,y)∼D
logF θ(y|H,s n). (1) 19 0:
:
Dt← ←g Det (cid:83)tra tjectory(π p, π, c)
However, such a problem may be difficult to optimize due to Return: D
thesparsityofthelabelspace,whichgrowsexponentiallywith
the length of predictions. Therefore, we simplify the problem
totheactionlevelbyignoringthetemporalcorrelationsamong where collaboration may not be ideal. Specifically, we train
actions and to get an approximate solution as a baseline. The an independent group of agents with various DRL algorithms
reduced problem is written as: andpick3checkpointsofeachagentwithdifferentskilllevels
to enrich the diversity in the skill sense and form the partner
l
max (cid:88) (cid:88) logF(i)(y |H,s ), (2) population.DifferentfromFCP[23],wedeemhumanstohave
θ θ i n at least a basic level of skill and choose the checkpoints that
(H,sn,y)∈Di=1
achieve 35%, 70%, and 100% rewards of the best checkpoint.
where F(i) denotes the model for the i-th prediction. We For each selected pair of agents, we collect trajectories under
θ
further implement a model as an approach to this problem. different settings, including the roles of agents and whether
the partner agent is deterministic.
C. Data Collection
D. Model Implementation
As high-quality data is vitally important in deep learning,
buildingadiversifiedandrepresentativedatasetisthefirststep We present the architecture of our ToM model in Figure 3.
intrainingtheToMmodel.However,collectinghumandatais To utilize deep learning models, the past trajectory (H,s ) is
n
too expensive to be a practical solution in both temporal and firstdecoupledintoasequenceofjointactionsandasequence
economical sense. Inspired by fictitious co-play (FCP) [23], of states,denoted as {a ,...,a }) and {s ,...,s }), respec-
1 n 1 n
we find that deliberately chosen AI partners can also provide tively. Note that the action at the current step is unknown,
diversified data to improve the generalization performance of thus a is padded as a zero-vector. The actions of both
n
the model and further benefit the human-AI collaboration. characters are first converted into the one-hot representations,
Our data collection process is presented in Algorithm 1, then concatenated into a vector with a dimension of |A|.
where we gather data from both self-play and cross-play. In We encode the actions through a linear layer to get the
self-play,thetargetagentplayswithacopyofitself,wherethe embeddings. Similarly, the states are encoded by a feature
trajectories demonstrate the ideal plan in the agent’s opinion. extractor, which is a Convolutional Neural Network (CNN) in
Incross-play,thetargetagentispairedwithagroupofpartners practice. Specifically, we train a separate agent and utilize its
to obtain the behavior pattern when cooperating with others, feature extractor as the pre-trained backbone, which offers a5
decentstartoftrainingwhilekeepingtheblackboxassumption
of the target agent.
The action embeddings and state features are then con-
catenated to get a unified representation of history features,
denoted as H′ = {h ,...,h }. Next, we employ a trans-
1 n
former model to encode the history feature sequence H′ and
decode the hidden representation to get the prediction yˆ via
a decoder. Since our application scenario only needs fixed-
length predictions, we use an MLP as the decoder instead of
auto-regressive models. As described in Section III-B, each
element of the prediction sequence is inferred independently,
sotheSoftmaxfunctionisusedastheactivationfunctionofthe
lastlayeroftheMLPdecoder.Finally,themodelisoptimized
by minimizing Cross-Entropy loss, which has the following
form:
l |AAI| Fig. 4. Our extended experiment platform based on Overcooked [3]. We
(cid:88) (cid:88) presenttheillustrationoftheiconsandascreenshotoftheuserinterfacewith
L=− y log(yˆ ). (3)
ij ij shownpredictions.
i=1 j=1
Where yˆ denotes the probability of j-th action, y is 1 for
ij ij
play (SP) and Fictitious Co-Play (FCP) [23]. Following the
the correct action and 0 otherwise.
results of prior works, the SP agent is regarded as strong but
IV. EXPERIMENTS egocentric,whileFCPismorecooperativeandperformsbetter
inzero-shotcollaborationscenarios.WeexpecttheToMmodel
A. Environment
can help in both situations.
To inspect the effectiveness of our human-AI collaboration
Fortheagents,weusethesamedesignationasin[3]which
framework, we conduct experiments in the Overcooked envi-
hasa3-layerCNNasthefeatureextractor(thekernelsizesare
ronment [3], which has been widely used for studying zero-
5×5,3×3,and3×3,respectively)anda3-layerMLPtooutput
shot coordination and human-AI collaboration [3, 25, 50, 23,
the actions. We use the PPO algorithm for the optimization
24, 26, 29]. In Overcooked, players should coordinate to pick
with hyper-parameters that are generally consistent with [3].
up ingredients, put the ingredients into pots to cook, dish out
Each agent is trained for 1e7 time steps with a learning rate
the soup when it is done, and serve it to the designated loca-
of 1e−3. More training details can be found in our provided
tion. In our experiment, the pot automatically starts cooking
supplementary. For both SP and FCP, we train 3 agents and
whenithas3onionsandittakes20timestepstocook.Allthe
select the one with the best performance for the human-AI
playersgetthesamerewardwhenadishissuccessfullyserved,
collaboration experiment.
thus the mutual goal is to serve as many soups as possible in
To train the ToM models, we first train 5 partner agents
a limited time. There are 6 available actions for each player:
in both SP and FCP and randomly divide them for training,
move {up,down,left,right}, interact, and wait. A player
validation, and testing, with a proportion of 3:1:1. Then we
can not move to the cell where the other player is already
pair the target agent with each partner to collect the trajectory
there. If the two players try to go to the same grid at the
dataset. We collect 5 trajectories of 800 steps for each pair
same time, both of them will fail to move. To perform well,
of agents following the procedure described in Algorithm 1,
the team needs to coordinate both high-level strategies and
resulting in 4.96e5 steps of data. We train an individual FCP
low-level path planning.
agent and take its trained CNN as the pretrained feature
To provide action predictions in the human-AI collabora-
extractor for all ToM models. In our experiment, the length
tion process, we extend the front-end user interface to show
of history is 10 and the prediction length is set to 3. The
predictions by depicting successive arrows and icons with
transformer is encoder-only, having 4 stacked transformer
graduallyvariedsizesandtransparencyatestimatedlocations.
layers,eachwith8attentionheads,andthehiddensizeissetto
OurextendedversionofOvercookedisillustratedinFigure4.
128. The predictor is trained for 100 epochs under a learning
To avoid confusion, we always let the human control the blue
rate of 1e−3 with an early stopping on the validation set.
character and let the AI control the green one. In the given
example, the arrows correspond to the prediction sequence
{move right, move right, move up}, which may imply the C. User study designation
AI is going to put its holding ingredient into the top-left pot. We run an online human-AI collaboration experiment to
We use 5 different layouts as illustrated in Figure 5 in the
test our paradigm and models. Our experiment goes through
experiment, which are expected to offer various challenges.
an internal ethics review. All participants are informed of the
experiment information, including the process, compensation,
B. Experimental setup
and privacy protection. Explicit informed consent is required
To comprehensively study the utility of our ToM model, for participation. We recruited 87 participants (42 female, 45
we incorporate two types of agents in the experiment: Self- male, ages 18-55) in various ways for the experiment.6
Fig.5. TheadoptedLayouts.Weuse5variouslayoutstotestourparadigmindifferentcircumstances.CoordinationRingchallengestheabilityofthehuman-
AIteamtocooperateinanarrowspace.InDoubleRings,theteammustcoordinateinbothpathchoicesandtaskallocation.DoubleCountersisavariantof
DoubleRingsthatreducesthedifficultyofcoordinationwhileincreasingthepunishmentofconflicts.Matrixisdesignedasacomplicatedenvironmentwhere
existsmultiplereasonablecoordinationsolutions.Finally,ClearDivisionhasanintuitiveplanfortaskallocationandlowdifficultyinoperationthoughitis
large.
Performanceinonlinehuman-AIcollaborationexperiments
SP FCP
300
SP+random FCP+random
SP+ToM FCP+ToM
200
100
0
CoordinationRing DoubleRings DoubleCounters Matrix ClearDivision Overall
Fig. 6. Average rewards with standard error bar. Each successful delivery gains 20 to the reward. To make the experiment more fine-grained, we give a
discountrewardaccordingtotheprocessofmakingsoupsattheendofthegame.Toreducerandomness,theinitialpositionsarefixedineachlayout.
Sinceourmainpurposeistostudytheutilityofpredictions, fill in an integer between 1 and 7 for each question.
weintroducetwobaselinesasthecontrolgroup:noprediction An online platform is developed for the experiment, which
andrandomprediction.Weuseabetween-subjectdesignation, providesasetofinterfacesfortheadministratortomonitorthe
in which the participants are divided into 3 groups, playing experiment. After the collaboration, we check the trajectories
with one setting of predictors respectively. Each participant and remove clearly broken games (about 1.1%). Finally, we
only plays with the same class of predictor throughout the got 860 episodes of data for analysis.
experiment.ParticipantsfirstreadanintroductiontotheOver-
cooked game and are required to pass a tutorial game. They
D. Results
then play with the two agents (SP and FCP) in the 5 layouts,
1) Quantitative performance: The most immediate indi-
in total 10 episodes. The order of layouts is the same as
cator is the reward in each episode, thus we present the
illustrated in Figure 5. The order of agents in the first layout
average reward on each layout and across layouts in Figure 6.
is randomly assigned, and the others are reversed with respect
We first perform Kruskal-Wallis H tests (a non-parametric
to the predecessor. Each episode lasts 400 time steps (80
one-way ANOVA) to compare the rewards in different ToM
seconds). Compared to previous work [3], we set a lower
settings for both SP and FCP agents. The results show
gamespeed(5fps)togivetheparticipantsmoreroomtoplay.
statistically significant differences in rewards for both SP
After finishing each episode, participants are asked to give a
(H(2,n = 430) = 10.83,p = .004) and FCP (H(2,n =
subjective assessment by filling out a questionnaire with the
430) = 15.23,p < .001). Subsequent ANOVA conducted on
following questions:
theresultsineachlayoutshowthatsignificantdifferencesexist
• How satisfied are you with your AI teammate? on the first 4 layouts (with a significance level of p < .05).
• How satisfied are you with the predictor (if applicable)? Since we are mainly curious about the efficacy of our trained
• To what extent can you predict the intention of your AI ToMmodel,werunpairwisecomparisonsbetweenthereward
teammate?
acquired in each layout with each agent with or without
• How would you rate the efficiency of cooperation? the ToM model with the T-test. Even after the Benjamini-
Thefourquestionscorrespondtosatisfactionwiththepartner, Hochberg correction for false discovery control, the ToM
satisfaction with the predictor, situational awareness, and model still significantly improves the rewards in the first 4
cooperation efficiency, respectively. Participants are asked to layouts (within the significance level of p<.05).
draweregarevA7
TABLEI
PREDICTIONACCURACY(%)OFTHETOMMODEL
Agent Dataset Coord.Ring DoubleRings DoubleCounters Matrix ClearDivision
Offlinetest 79.42 70.91 73.70 76.08 73.44
SP
Userstudy 82.29 61.77 65.84 60.37 77.88
Offlinetest 89.54 87.61 85.40 76.83 91.44
FCP
Userstudy 86.60 78.34 76.51 76.97 83.13
averagescoresacrosssettingsintheFigure7.Sinceweaimto
Statisticsofthequestionnairesacrosslayouts
7 investigate the factor in positive situations, only settings with
SP FCP significant improvement are taken into account, i.e., data on
6 SP+random FCP+random the first 4 layouts.
SP+ToM FCP+ToM
As expected, the satisfaction of agents has no significant
5
difference, because the agents are exactly the same. As
for the satisfaction of the predictor, the ToM model gets
4
significantly higher scores than the random baseline (SP:
3 t(224) = 2.33,p = .021; FCP: t(226) = 4.16,p < .001),
underscoring that humans care about the predictions and can
2 identify the capability of predictors. The ToM model also
significantly improves situational awareness compared to both
1
S.Agent S.Predictor SA Efficiency the random baseline and the no-predictor setting (all with a
significance level of p < .05), whereas the random baseline
shows no significant difference from no predictor, confirming
Fig.7. Averagescoresinuserassessmentacrosslayouts.
the utility of appropriate predictions.
AlthoughwehavequantitativelyprovedthattheToMmodel
The results corroborate that our ToM model can benefit can improve the efficiency of human-AI collaboration, it is
human-AI collaboration in various situations. Such improve- not fully reflected in the subjective feelings of participants.
ments cover multiple layouts, which implies the predictions However, the difference between the ToM model and the
help the collaboration in both high-level strategy coordination random baseline turns out to be significant (SP: t(224) =
and low-level routine coordination. Furthermore, the ToM 2.44,p = .016; FCP: t(226) = 3.68,p < .001), which may
model is helpful for both the SP agent and FCP agent, though indicate that inappropriate predictions make humans confuse
the FCP agent has already significantly outperformed the SP and spoil their subjective feelings.
agent and achieved a relatively high reward. We deem that The between-subject designation may reduce the statistical
the ToM model helps the collaboration in another aspect, significance of the user assessment since the user can not
which is orthogonal to the ability of agents. Besides, the compare among different settings of predictors and tends to
random baseline confirms that the improvement is not caused give a middle score. Though the participants who cooperate
byanyplaceboeffect,andthevisualpresentationitselfhasno with a random predictor can not forecast their partner well,
remarkable impact on the human-AI collaboration. they tend to give a score of 3 to 4 for satisfaction of the
We are also interested in the negative results, where the predictor, since they are not told that it is a random predictor.
ToM model fails to improve the performance on Clear Divi- Accordingly, this suggests that the ToM model’s impact could
sion. The performance in all three settings has no significant be more substantive than the questionnaire results indicate.
difference from the others. Qualitatively, Clear Division has 3) Prediction Accuracy: As discussed above, performance
a straightforward scheme for high-level task allocation: one improvement highly depends on the accuracy of predictions,
player goes top to put ingredients into the pots, and the random predictor leads to no improvement and even negative
other is responsible for serving cooked soup. In such a task effect. Here we show the accuracy of our ToM model on each
allocation schema, the routes of the two players have almost layoutinTableI,wheretheaccuracyiscalculatedattheaction
no intersections, which further eliminates the challenge of level. The offline test refers to the test set of the collected
low-level motion coordination. We subjectively analyze the dataset,inwhichthetargetagentispairedwithDRLpartners.
playback of the experiment and find that most teams adhere WealsotesttheexactaccuracyoftheToMmodelinhuman-AI
to such a scheme, thus no room for improvement is left for collaborations, denoted as user study.
theToMmodel.Thisperspectiveisfurthersubstantiatedbythe Note that the future actions of the target agent also depend
comparison between SP and FCP agent performances, which on its partner, e.g., the agent may fail to execute its plan
demonstrated no notable advantage for the latter though it is if the human blocks its way. Although our ToM model can
more cooperative. recognize if the agent is stuck from the current game history
2) User assessment: To test the hypothesis that the im- and adjust the prediction, there still exists an upper limit
provement in coordination is the main factor of performance of accuracy which is lower than 100% when the partner
improvements, we scrutinize user assessments and present the is non-deterministic. Overall, the ToM model has a decent
serocsegarevA8
accuracy that can well support human-AI collaboration tasks.
However, compared to the offline test where collaborating
with a DRL agent, there is a general decrease in accuracy
intherealhuman-AIcollaboration(8of10scenarios).Sucha
resultmayindicatethathumansaregenerallymorediversified,
leading to novel situations that are out of the distribution of
the training data, and humans also act more randomly. This
indicates further room for improvement in the designation of
the predictor and the presentation of predictions.
E. Case Study
To qualitatively analyze the effect of the ToM model and
our paradigm, we review some records of the experiment and
present some representative cases in Figure 8 as a case study.
Naturally, the ToM model contributes the most when there
exist multiple reasonable plans. In both examples in Figure 8,
the agent is going to the serving place and there are two
reasonable ways. With the shown future actions, the human
can understand the intention of the AI agent and manage to
notblockitsway.WearguethattheToMmodelisimportantto
human-AI collaboration because the intentions of AI are hard
to infer by humans. The plan of the same AI agent can vary Fig. 8. Representative screenshots in the human-AI collaboration. In all
presented cases, the future actions of the agents are perfectly predicted and
with the situation. For instance, in Figure 8 (a), the shortest
indicatetheplans.
path towards the destination is moving downward, however,
the agent chooses the left-hand way when the human is in the
way, to avoid possible collisions. Moreover, even in similar to the human subjects. Even the quantitative results highly
cases, different agents may make different choices. As shown depend on the game skills of participants. Since the par-
in Figure 8 (b), the FCP agent will choose the upper route ticipants are primarily recruited from the Internet, we have
whereastheSPagentchoosesthelowerone,disregardingthat no commitment to the representativeness of the participants.
the human is in its way. Generally, the behaviors of agents Thoughtheexperimenthasvalidatedthegeneraleffectiveness
are hardly predictable for humans, especially when humans of our paradigm, it may require further analysis when applied
are not familiar with their partners. Therefore, our paradigm to specified realistic scenarios.
whichintroducesanexternalintentionpredictorcanhelpalot For future work, a possible direction is to study the safety
in the human-AI collaboration process. and variation of human trust on the proposed external in-
tention predictor. As implied by the random baseline in our
V. DISCUSSION experiment,inappropriatepredictionsmayreducehumantrust.
A. Conclusion Especially,itmaybeharmfuliftheToMmodelismaliciously
manipulated to mislead humans, which may cause a serious
In this paper, we introduce a new human-AI collaboration
failure.
paradigmthatincorporatesaToMmodelthattrainedfromthe
SinceourToMmodelonlypredictstheactionsequenceand
offline trajectories of the target AI agent and utilized in the
lets humans speculate the intention from such a prediction, a
collaboration process. We further implement a framework in
possible direction of improvement is to generate semantically
which trajectories from both cross-play and self-play to build
comprehensible intentions, such as the current goal of the
adiversifiedreplaydataset,andtrainatransformer-basedToM
agent.Besides,itshouldbemeaningfultoverifyourparadigm
model as the predictor. We then develop an online experiment
in more complicated and practical human-AI collaboration
platform based on the extended Overcooked environment and
domains, especially the domains in which the global visual
design a between-subject experiment with post-game user
display is not applicable may bring new challenges to com-
assessment to testify our paradigm and implemented ToM
munication.
models. The results of the extensive experiment demonstrated
the helpfulness of our paradigm to human-AI collaboration
in both quantitative rewards and subjective measurement. Our REFERENCES
study of such a paradigm and implementation of the platform
[1] D. Dellermann, A. Calma, N. Lipusch, T. Weber,
build a foundation for further research on assisting humans to
S. Weigel, and P. Ebel, “The future of human-ai col-
understand the intention of agents in human-AI collaboration.
laboration: A taxonomy of design knowledge for hybrid
intelligencesystems,”inProceedingsofthe52ndHawaii
B. Limitation and future work International Conference on System Sciences, 2019.
Our paradigm mainly contributes to the human-AI collab- [2] K.Sowa,A.Przegalinska,andL.Ciechanowski,“Cobots
oration task, where the experiment results are highly related in knowledge work: Human–ai collaboration in manage-9
rialprofessions,”JournalofBusinessResearch,vol.125, [14] J.Porteous,A.Lindsay,andF.Charles,“Communicating
pp. 135–142, 2021. agent intentions for human-agent decision making under
[3] M. Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, uncertainty,” in Proceedings of the 2023 International
P. Abbeel, and A. Dragan, “On the utility of learning Conference on Autonomous Agents and Multiagent Sys-
about humans for human-ai coordination,” in Advances tems, 2023, pp. 290–298.
in neural information processing systems, vol. 32, 2019. [15] L. Yuan, X. Gao, Z. Zheng, M. Edmonds, Y. N.
[4] S. Murata, Y. Li, H. Arie, T. Ogata, and S. Sugano, Wu, F. Rossano, H. Lu, Y. Zhu, and S.-C. Zhu, “In
“Learning to achieve different levels of adaptability for situ bidirectional human-robot value alignment,” Science
human–robot collaboration utilizing a neuro-dynamical Robotics, vol. 7, no. 68, p. eabm4183, 2022.
system,” IEEE Transactions on Cognitive and Develop- [16] H. Nguyen and H. La, “Review of deep reinforcement
mental Systems, vol. 10, no. 3, pp. 712–725, 2018. learning for robot manipulation,” in 2019 Third IEEE
[5] A. Ghadirzadeh, X. Chen, W. Yin, Z. Yi, M. Bjo¨rkman, International Conference on Robotic Computing (IRC).
and D. Kragic, “Human-centered collaborative robots IEEE, 2019, pp. 590–595.
with deep reinforcement learning,” IEEE Robotics and [17] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decen-
Automation Letters, vol. 6, no. 2, pp. 566–571, 2020. tralized non-communicating multiagent collision avoid-
[6] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, ance with deep reinforcement learning,” in 2017 IEEE
and S. Levine, “How to train your robot with deep international conference on robotics and automation
reinforcement learning: lessons we have learned,” The (ICRA). IEEE, 2017, pp. 285–292.
International Journal of Robotics Research, vol. 40, no. [18] R.Paleja,M.Ghuy,N.RanawakaArachchige,R.Jensen,
4-5, pp. 698–721, 2021. and M. Gombolay, “The utility of explainable ai in ad
[7] D. Wang, J. D. Weisz, M. Muller, P. Ram, W. Geyer, hoc human-machine teaming,” in Advances in Neural
C. Dugan, Y. Tausczik, H. Samulowitz, and A. Gray, Information Processing Systems, vol. 34, 2021, pp. 610–
“Human-ai collaboration in data science: Exploring data 623.
scientists’ perceptions of automated ai,” Proceedings of [19] M.Edmonds,F.Gao,H.Liu,X.Xie,S.Qi,B.Rothrock,
the ACM on Human-Computer Interaction, vol. 3, no. Y. Zhu, Y. N. Wu, H. Lu, and S.-C. Zhu, “A tale of two
CSCW, pp. 1–24, 2019. explanations:Enhancinghumantrustbyexplainingrobot
[8] A. Malhi, S. Knapic, and K. Fra¨mling, “Explainable behavior,” Science Robotics, vol. 4, no. 37, p. eaay4663,
agents for less bias in human-agent decision making,” 2019.
in Explainable, Transparent Autonomous Agents and [20] D.Gunning,M.Stefik,J.Choi,T.Miller,S.Stumpf,and
Multi-Agent Systems: Second International Workshop, G.-Z. Yang, “Xai—explainable artificial intelligence,”
EXTRAAMAS 2020, Auckland, New Zealand, May 9–13, Science robotics, vol. 4, no. 37, p. eaay7120, 2019.
2020, Revised Selected Papers 2. Springer, 2020, pp. [21] S. Maccio`, A. Carf`ı, and F. Mastrogiovanni, “Mixed re-
129–146. ality as communication medium for human-robot collab-
[9] L. Ibrahim, M. M. Ghassemi, and T. Alhanai, “Do oration,” in 2022 International Conference on Robotics
explanations improve the quality of ai-assisted human and Automation (ICRA). IEEE, 2022, pp. 2796–2802.
decisions? an algorithm-in-the-loop analysis of factual [22] E. Rosen, D. Whitney, E. Phillips, G. Chien, J. Tomp-
& counterfactual explanations,” in Proceedings of the kin, G. Konidaris, and S. Tellex, “Communicating robot
2023 International Conference on Autonomous Agents arm motion intent through mixed reality head-mounted
and Multiagent Systems, 2023, pp. 326–334. displays,” in Robotics Research: The 18th International
[10] J. Lyu, P. Ruppel, N. Hendrich, S. Li, M. Go¨rner, and Symposium ISRR. Springer, 2020, pp. 301–316.
J. Zhang, “Efficient and collision-free human-robot col- [23] D. Strouse, K. R. McKee, M. Botvinick, E. Hughes, and
laboration based on intention and trajectory prediction,” R. Everett, “Collaborating with humans without human
IEEETransactionsonCognitiveandDevelopmentalSys- data,” in Advances in Neural Information Processing
tems, 2022. Systems, vol. 34, 2021, pp. 14502–14515.
[11] S. Aldini, A. K. Singh, D. Leong, Y.-K. Wang, M. G. [24] R. Zhao, J. Song, H. Haifeng, Y. Gao, Y. Wu, Z. Sun,
Carmichael, D. Liu, and C.-T. Lin, “Detection and es- and Y. Wei, “Maximum entropy population based train-
timation of cognitive conflict during physical human- ing for zero-shot human-ai coordination,” arXiv preprint
robot collaboration,” IEEE Transactions on Cognitive arXiv:2112.11701, 2021.
and Developmental Systems, 2022. [25] R. Charakorn, P. Manoonpong, and N. Dilokthanakul,
[12] A. Ajoudani, A. M. Zanchettin, S. Ivaldi, A. Albu- “Investigating partner diversification methods in cooper-
Scha¨ffer, K. Kosuge, and O. Khatib, “Progress and ative multi-agent deep reinforcement learning,” in Inter-
prospects of the human–robot collaboration,” Au- national Conference on Neural Information Processing.
tonomous Robots, vol. 42, no. 5, pp. 957–975, 2018. Springer, 2020, pp. 395–402.
[13] A. Tabrez, M. B. Luebbers, and B. Hayes, “Descriptive [26] M. Yang, M. Carroll, and A. Dragan, “Optimal behavior
and prescriptive visual guidance to improve shared situ- prior:Data-efficienthumanmodelsforimprovedhuman-
ational awareness in human-robot teaming,” in Proceed- ai collaboration,” arXiv preprint arXiv:2211.01602,
ingsofthe21stInternationalConferenceonAutonomous 2022.
Agents and Multiagent Systems, 2022, pp. 1256–1264. [27] H.C.Siu,J.Pen˜a,E.Chen,Y.Zhou,V.Lopez,K.Palko,10
K. Chang, and R. Allen, “Evaluation of human-ai teams resentations, 2021.
forlearnedandrule-basedagentsinhanabi,”inAdvances [41] D. Ferrari, F. Benzi, and C. Secchi, “Bidirectional com-
inNeuralInformationProcessingSystems,vol.34,2021, munication control for human-robot collaboration,” in
pp. 16183–16195. 2022InternationalConferenceonRoboticsandAutoma-
[28] A. Lupu, B. Cui, H. Hu, and J. Foerster, “Trajectory tion (ICRA). IEEE, 2022, pp. 7430–7436.
diversity for zero-shot coordination,” in International [42] R. Newbury, A. Cosgun, T. Crowley-Davis, W. P. Chan,
Conference on Machine Learning. PMLR, 2021, pp. T. Drummond, and E. A. Croft, “Visualizing robot in-
7204–7213. tent for object handovers with augmented reality,” in
[29] X.Lou,J.Guo,J.Zhang,J.Wang,K.Huang,andY.Du, 2022 31st IEEE International Conference on Robot and
“Pecan: Leveraging policy ensemble for context-aware Human Interactive Communication (RO-MAN). IEEE,
zero-shot human-ai coordination,” in Proceedings of the 2022, pp. 1264–1270.
2023 International Conference on Autonomous Agents [43] Y. Gao, F. Liu, L. Wang, Z. Lian, W. Wang, S. Li,
and Multiagent Systems, 2023, pp. 679–688. X. Wang, X. Zeng, R. Wang, Q. FU, et al., “Towards
[30] K. Lucas and R. E. Allen, “Any-play: An intrinsic aug- effective and interpretable human-agent collaboration in
mentation for zero-shot coordination,” in Proceedings of moba games: A communication perspective,” in The
the21stInternationalConferenceonAutonomousAgents Eleventh International Conference on Learning Repre-
and Multiagent Systems, 2022, pp. 853–861. sentations, 2023.
[31] H. Hu, A. Lerer, A. Peysakhovich, and J. Foerster, [44] A. Heuillet, F. Couthouis, and N. D´ıaz-Rodr´ıguez, “Ex-
““other-play” for zero-shot coordination,” in Interna- plainabilityindeepreinforcementlearning,”Knowledge-
tional Conference on Machine Learning. PMLR, 2020, Based Systems, vol. 214, p. 106685, 2021.
pp. 4399–4410. [45] F.Xu,H.Uszkoreit,Y.Du,W.Fan,D.Zhao,andJ.Zhu,
[32] S.QiandS.-C.Zhu,“Intent-awaremulti-agentreinforce- “Explainableai:Abriefsurveyonhistory,researchareas,
ment learning,” in 2018 IEEE International Conference approaches and challenges,” in Natural Language Pro-
on Robotics and Automation (ICRA). IEEE, 2018, pp. cessing and Chinese Computing: 8th CCF International
7533–7540. Conference, NLPCC 2019, Dunhuang, China, October
[33] R.Choudhury,G.Swamy,D.Hadfield-Menell,andA.D. 9–14, 2019, Proceedings, Part II 8. Springer, 2019, pp.
Dragan,“Ontheutilityofmodellearninginhri,”in2019 563–574.
14th ACM/IEEE International Conference on Human- [46] A. Tabrez and B. Hayes, “Improving human-robot in-
Robot Interaction (HRI). IEEE, 2019, pp. 317–325. teraction through explainable reinforcement learning,”
[34] B. Chen, Y. Hu, R. Kwiatkowski, S. Song, and H. Lip- in 2019 14th ACM/IEEE International Conference on
son, “Visual perspective taking for opponent behavior Human-Robot Interaction (HRI). IEEE, 2019, pp. 751–
modeling,” in 2021 IEEE International Conference on 753.
Robotics and Automation (ICRA). IEEE, 2021, pp. [47] A. Tabrez, S. Agrawal, and B. Hayes, “Explanation-
13678–13685. based reward coaching to improve human performance
[35] R. C. Luo and L. Mai, “Human intention inference via reinforcement learning,” in 2019 14th ACM/IEEE
and on-line human hand motion prediction for human- International Conference on Human-Robot Interaction
robot collaboration,” in 2019 IEEE/RSJ International (HRI). IEEE, 2019, pp. 249–257.
Conference on Intelligent Robots and Systems (IROS). [48] B. Jin, X. Liu, Y. Zheng, P. Li, H. Zhao, T. Zhang,
IEEE, 2019, pp. 5958–5964. Y. Zheng, G. Zhou, and J. Liu, “Adapt: Action-
[36] C. Wang, C. Pe´rez-D’Arpino, D. Xu, L. Fei-Fei, K. Liu, aware driving caption transformer,” arXiv preprint
and S. Savarese, “Co-gail: Learning diverse strategies arXiv:2302.00673, 2023.
for human-robot collaboration,” in Conference on Robot [49] A. Iucci, A. Hata, A. Terra, R. Inam, and I. Leite,
Learning. PMLR, 2022, pp. 1279–1290. “Explainable reinforcement learning for human-robot
[37] T. X. Lim, S. Tio, and D. C. Ong, “Improving multi- collaboration,”in202120thInternationalConferenceon
agent cooperation using theory of mind,” arXiv preprint Advanced Robotics (ICAR). IEEE, 2021, pp. 927–934.
arXiv:2007.15703, 2020. [50] P. Knott, M. Carroll, S. Devlin, K. Ciosek, K. Hofmann,
[38] S. A. Wu, R. E. Wang, J. A. Evans, J. B. Tenenbaum, A. Dragan, and R. Shah, “Evaluating the robustness of
D.C.Parkes,andM.Kleiman-Weiner,“Toomanycooks: collaborativeagents,”inProceedingsofthe20thInterna-
Bayesian inference for coordinating multi-agent collabo- tionalConferenceonAutonomousAgentsandMultiAgent
ration,” Topics in Cognitive Science, vol. 13, no. 2, pp. Systems, 2021, pp. 1560–1562.
414–432, 2021.
[39] N. Rabinowitz, F. Perbet, F. Song, C. Zhang, S. A.
Eslami, and M. Botvinick, “Machine theory of mind,” in
International conference on machine learning. PMLR,
2018, pp. 4218–4227.
[40] Y.Wang,J.Xu,Y.Wang,etal.,“Tom2c:Target-oriented
multi-agent communication and cooperation with theory
of mind,” in International Conference on Learning Rep-