Are We in The Zone? Exploring The Features and Method of Detecting
Simultaneous Flow Experiences Based on EEG Signals
BAIQIAOZHANG,
ShandongUniversity,China
XIANGXIANLI,
ShandongUniversity,China
YUNFANZHOU,
ZhejiangUniversity,China
JUANLIU,
ShandongUniversity,China
WEIYINGLIU,
ShandongUniversity,China
CHAOZHOU,
InstituteofSoftwareChineseAcadamyofSciences,China
YULONGBIAN∗,
ShandongUniversity,China
Fig.1. Detectingteammembers’simultaneousflowexperienceduringatwo-usercollaborativetasksbasedonmulti-channel
electroencephalogram(EEG)signals.
Whenexecutinginterdependentpersonaltasksfortheteam’spurpose,simultaneousindividualflow(simultaneousflow)is
theantecedentconditionofachievingsharedteamflow.Detectingsimultaneousflowhelpsbetterunderstandingthestatusof
teammembers,whichisthusimportantforoptimizingmulti-userinteractionsystems.However,thereiscurrentlyalack
explorationonobjectivefeaturesandmethodsfordetectingsimultaneousflow.Basedonbrainmechanismofflowinteamwork
andpreviousstudiesonelectroencephalogram(EEG)-basedindividualflowdetection,thisstudyaimstoexplorethesignificant
∗YulongBianisthecorrespondingauthor.
Authors’addresses:BaiqiaoZhang,baiqiao@mail.sdu.edu.cn,ShandongUniversity,Weihai,China;XiangxianLi,ShandongUniversity,
Jinan,China,larst@affiliation.org;YunfanZhou,ZhejiangUniversity,Hangzhou,China,zhouyunfan00@163.com;JuanLiu,Shandong
University,Weihai,China,zzzliujuan@sdu.edu.cn;WeiyingLiu,ShandongUniversity,Weihai,China,2202237565@mail.sdu.edu.cn;Chao
Zhou,InstituteofSoftwareChineseAcadamyofSciences,Beijing,China,zhouchao@iscas.ac.cn;YulongBian,ShandongUniversity,Weihai,
China,bianyulong@sdu.edu.cn.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat
copiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.
Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopy
otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom
permissions@acm.org.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
2474-9567/2024/0-ART0$15.00
https://doi.org/XXXXXXX.XXXXXXX
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
4202
yaM
3
]CH.sc[
1v54020.5042:viXra0:2 • Zhangetal.
EEGfeaturesrelatedtosimultaneousflow,aswellaseffectivedetectionmethodsbasedonEEGsignals.First,atwo-player
simultaneousflowtaskisdesigned,basedonwhichweconstructthefirstmulti-EEGsignalsdatasetofsimultaneousflow.
Then,weexplorethepotentialEEGsignalfeaturesthatmayberelatedtoindividualandsimultaneousflowandvalidatetheir
effectivenessinsimultaneousflowdetectionwithvariousmachinelearningmodels.Theresultsshowthat1)theinter-brain
synchronyfeaturesarerelevanttosimultaneousflowduetoenhancingthemodels’performanceindetectingdifferenttypesof
simultaneousflow;2)thefeaturesfromthefrontallobeareaseemtobegivenpriorityattentionwhendetectingsimultaneous
flows;3)RandomForestsperformedbestinbinaryclassificationwhileNeuralNetworkandDeepNeuralNetwork3performed
bestinternaryclassification.
CCSConcepts:•Human-centeredcomputing→HCIdesignandevaluationmethods.
AdditionalKeyWordsandPhrases:Teamflowexperience,EEGsignals,Evaluationmethod,Dataset
ACMReferenceFormat:
BaiqiaoZhang,XiangxianLi,YunfanZhou,JuanLiu,WeiyingLiu,ChaoZhou,andYulongBian.2024.AreWeinTheZone?
ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals.Proc.ACMInteract.
Mob.WearableUbiquitousTechnol.0,0,Article0(2024),36pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Theflowexperience(shortasflow)isanimportantuserexperienceduringhuman-computerinteraction[8].It
isahighlyenjoyablementalstateinwhichtheindividualisfullyimmersedandengagedinactivities,andis
thusconsideredtheoptimalexperience[22][23][69].Flowexistsnotonlyinperformingindividualactivities,but
alsoinperformingcollaborativeactivities.Teammembersmaysimultaneouslyexperienceflowandpositive
collectiveexperienceswhileperformingtasksfortheteam’spurpose,reflectingtheunifiedandcoordinated
feelingsgeneratedamongteammembersastheystrivetowardacommongoal,whichisknownasteamflowor
groupflow[64].Teamflowisconsideredtobethegoalofoptimalcollaboration[73].Studieshaveshownthatthe
experienceofteamflowisimportantforoptimizingteamperformance:itcanfostercloseremotionalconnections
betweenteammembersandtheworkenvironment,increaseteamexperienceandwell-beingattheteamlevel,and
therebystimulatehighermotivation,creativity,andbothteamandindividualperformance[71][78][12][39][73].
Asmulti-usercollaborativeapplicationsbecomemorecommon,theteamflowexperienceisincreasinglyfocused
onandconsideredanimportantuserexperiencegoalincollaborationandcooperation.
Althoughsomeresearchersconsiderteamflowasasolelygroupphenomenon[64],vandenHoutetaldefined
itasaconcatenativeexperienceofteammembers’individualflow[72].Simultaneousindividualflowwhile
executing personal tasks for the team’s purpose may be the antecedent condition of achieving shared team
flow[10]. Consideringshared teamflow isa complex stateand hardto operationalize,according tovan den
Houtetal.’sconcatenativeidea,wefirstfocusontheantecedentconditionanddescribeitas"simultaneous
flowexperienceincollaborativetask"(shortassimultaneousflow),whichmeansteammembersexperiencing
individualflowatthesametime[10].Therefore,methodsofdetectingsimultaneousflowarecriticalforbetter
understandingteammembers’status,whichisimportantforthedynamicoptimizationofmulti-usercollaborative
systems.
Thecurrentprimarymethodstoevaluateflowarequestionnaires,scales,interviewandtheexperiencesampling
method(ESM)[24][36][53].Thesemethodsareretrospectiveandhavesubjectiveresponsebias[79].Bycontrast,
physiologicalmeasurementshaveshowngreaterpotentialinevaluatingflowinamoreobjectiveandreal-time
mannerwithoutdisruptingtheongoingexperienceprocess[7].Forindividualflow,recentstudieshavefound
correlations between flow and neural activity signals (e.g. frontal lobe activity)[15][49], and explored flow
detectionmethodsbasedonelectroencephalogram(EEG)signals[17][37][57][76].Thesestudiespreliminarily
demonstratetheeffectivenessofthismethod.However,themethodsofdetectingsimultaneousflow/teamflow
havenotbeeninvestigatedingreaterdetail.Therefore,thisstudyfocusonthesimultaneousflow,exploringits
relatedfeaturesandpotentialdetectmethodsbasedonEEGsignals.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:3
Wefirstconsideredthesmallestteamsizeanddesignedatwo-usergametoinducesimultaneousflow.Based
onthetask,werecordedthepairedparticipants’multichannelEEGsignalsfrombrainregionsrelatedtoflow
experience.Afterlabeling,slicing,andprocessingthedata,weconstructedthefirstmultichannelEEGdataset
withmultiplelabelsofflowexperiencesincollaborativetasks.Next,wefocusedoninter-brainsynchronyfeatures
fromdifferentbrainregions’EEGsignalsandexploredtheireffectivenessindetectingsimultaneousflowby
usingseveralclassifiers,includingLogisticRegression(LR),SupportVectorMachines(SVM),DecisionTrees(DT),
RandomForests(RF)andseveralNeuralNetwork(NN)models.
Themaincontributionsofthisresearchinclude:
• Wedesignedatwo-usertaskforcontinuouslymeasuringsimultaneousflowincollaborationbasedon
whichweconstructedthefirstmultichannelEEGdatasetofsimultaneousflow.
• Weproposedinter-brainsynchronyfeaturesfromEEGsignalswhichpotentiallyrelevanttosimultaneous
flowforthefirsttime.Basedoncomparisonexperiments,featureimportanceexperimentsandablation
study,wevalidatedtheeffectivenessofinter-brainsynchronyfeaturesindetectingsimultaneousflow,and
foundfeaturesfromfrontallobeareaweregivenpriorityattentionbythemodels.
• WemakethefirstattempttorecognizesimultaneousflowbasedonEEGsignalsbyusingmultiplemachine
learningmethods.Inthisstudy,RFachievedthehighestaccuracyinbinaryclassification(83.9%);NNand
DNN3performedbestinternaryclassification(87.2%).
2 RELATEDWORK
2.1 WhatareTeamFlowandSimultaneousFlow?
Individualflowreferstoamentalstatecharacterizedbyperceivingoneselftobeentirelyabsorbedinthecurrent
personalactivity[9][21].Comparedtoindividualflow,teamflow(orgroupflow)placesgreateremphasison
theinteractionsamongteammembersandachievementofanenjoyableexperienceinteamlevel.Sawyer[64]
wasoneofthefirstresearcherstoproposetheconceptofteamflow.Hearguedthatteamflowdependsonthe
interactionsbetweenteammembersandmaymanifestthroughsuchinteractions.Comparedwithindividualflow,
threemajordifferenceswereidentified:(1)Teamflowrequiresbehavioral,cognitive,andemotionalinteractions
amongteammembers,whereasindividualflowisgeneratedfromasoloactivity[71];(2)Inteamflow,collective
goalstakeprecedenceoverotherelementsofthegroup,whereasindividualflowismorefocusedonpersonal
goalsandoutcomes;(3)Teamflowrequiresahigherlevelofskillamongteammembers[58].Sharedteamflowis
acomplexstateofandmore.
Inmorerecentstudies,whilevandenHoutetal.concededthatagroupcanattaina"collectivestateofmind",
theydisagreewiththeviewthatteamflowallowsforindividualsnottoexperienceflow[72].VandenHoutetal.
definedteamflowasasasharedexperienceofflowderivedfromanoptimizedteamdynamicduringtheexecution
ofinterdependentpersonaltasks.Inthisdefinition,"shared"meansthatindividualteammembersareexperiencing
flowsimultaneouslyandcollectivelywhileexecutingtheirpersonaltasksfortheteam’spurpose(s)[72].Inother
words,theyviewteamflowasaconcatenativeexperience,proposingthatsimultaneousindividualflowwhile
executingpersonaltasksfortheteam’spurposemaybetheantecedentconditionofachievingsharedteamflow.
As we explained in the Section 1, we introduce the term "simultaneous flow" to describe simultaneous flow
experienceincollaborativetask,representingthestatethatteammembersexperiencingindividualflowatthe
sametime[10].Inthispaper,wefocuson"simultaneousflow"forthefirststep,andpreliminaryexplorewhether
ithassimilaritieswithsharedteamflow.
2.2 FlowDetectionbasedonEEGSignals
Because traditional flow evaluation methods (such as questionnaires and interviews) are retrospective and
subjective,researchershavepaidincreasingattentiontothepotentialofobjectivephysiologicalsignalsinflow
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:4 • Zhangetal.
experienceevaluations[7][20][79].Currentstudiesonthistopicprimarilyfocusonevaluatingindividualflow
experiencesusingphysiologicalsignals,suchaselectrocardiograms(ECG),electrodermalactivity(EDA),and
electroencephalograms (EEG). Based on these physiological signals with flow experience labels, supervised
learning(throughtraditionalmachinelearningordeeplearningmodels)hasbeenusedtorecognizeanddetect
differentflowexperiences.Yeetal.[79]constructedseveralmachinelearningmodelstodetectusergamingflow
experiencesusingskinconductanceandheartratesignals.Theyachievedanaccuracyrateof90%inbinary
classificationtasksbyusingasupportvectormachineclassifierandachievedanaccuracyrateof61%internary
classificationtasksbyusingrandomforestalgorithm.Intermsofdeeplearning,Maieretal.[45]proposeda
DeepFlowmodelbasedondeeplearningtoclassifyhigh-levelandlow-levelflow,aswellastoclassifydifferent
experiencesofflow,boredom,andstress.Cherepetal.[20]usedwearabledevicestocollectEEGsignalsand
usedthedeeplearningmodelEEGNettoidentifyflowexperienceswithanaccuracyofover65%.EEGNetuses
rawmultichannelEEGsignalsasinputswithoutpre-extractedfeatures.Accordingtoresearchontheneural
mechanisms of the team flow experience[65], EEG signals contain valuable information regarding the flow
experience.Existingresearchonflowmetricsfocusesprimarilyontheindividualtasks,andlacksexplorationof
flowmetricsatthecollaborativetasks.Therefore,thisstudyfocusesondetectingflowexperienceincollaborative
tasksbasedonEEGsignals.
2.3 PhysiologicalSynchrony
Chatterjeeetal.pointedoutthatthephenomenoninwhichthephysiologicalresponsesoftwoindividualsbecome
moresimilariscalled"physiologicalsynchrony"[18].Researchontheneuralactivitiesofteamflowshowedthat
teamflowischaracterizedbyhigherinter-brainsynchronyinthelefttemporalcortex[65],revealingtheneural
activitymechanismsofteamflowincollaborativetasksbasedonneuralphysiologicalsignals.Currentresearch
onphysiologicalsynchronyindetectingteamflow/simultaneousflowhasnotyetbeenconducted.Therefore,
webeginbyreferringtorelevantresearchinaffectivecomputingandsummarizethefollowingthreeimportant
aspects.
2.3.1 ExperimentalTasksforPhysiologicalSynchrony. Intermsofexperimentaldesign,studiesonphysiological
synchronycommonlyrequirethegroupparticipantstoengageincollaborativetasks.Kevinetal.askedparticipants
toplaydifferentrolesasateaminaflightoperationtaskandcollectedtheirEEGsignalstodetectthecognitive
loadandcooperationlevelamongtheteammembers[74].Darzietal.measuredeachparticipant’sphysiological
response,physiologicalconnectivityindices,andtaskperformanceunderdifferentconditions[25].Theysuggested
thatinmulti-usersettings,physiologicalsynchronyinformationrelatedtothespecificityofinteractionsbetween
participantscanbeobtainedbyexaminingthesimilaritiesintheirphysiologicalresponses.Thedegreeoflinkage
increaseswiththenumberofcollaborations,theintensityofcompetition,andthesheeramountofjointattention.
Thesefindingscanserveasareferencefordesigningsimultaneous/teamflowtasks.
2.3.2 FeaturesofPhysiologicalSynchrony. Intermsoffeatureselection,theexplorationofinter-brainsynchrony
featuresmainlyfocusedonneuralresponseconsistencyacrossparticipants.Chatterjeeetal.categorizedfeatures
intoindividualfeatures,computedfromthephysiologicaldataofasingleparticipant,andsynchronyfeatures,
computedfromthephysiologicaldataoftwoparticipants[18].Darzietal.constructedafeaturesetbycombining
individual physiological features with physiological connectivity features[25]. Javier et al.[32] found that a
combination of features extracted from the child’s EDA activity and physiological synchrony between the
childandtheadultresultedinthehighestclassificationaccuracy.Shkurtaetal.[29]computedthesynchrony
featuresfromEDAsignalsusingdynamictimewarping(DTW)algorithm.Theresultsshowthatanincrementin
physiologicalsynchronyamongstudentsduringalectureisrelatedtoanimprovementinstudents’emotional
state.Studiesusingmulti-personEEGdatashowthatinter-braincorrelationsinthetaandalphawaveamplitudes
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:5
intherighttemporoparietaljunctionandalphaandbetawaveamplitudesinthefrontalregionareassociatedwith
understandingothers’intentionsandhigh-levelcooperativestrategies[65].Dingetal.[27]arguedthatinter-brain
amplitudefeaturesandacombinationofallinter-brainfeaturesoutperformedsingle-brainfeaturesinthemost
competentparticipant.Inter-brainsynchronyfeaturesshouldbeconsideredinthisstudy.
2.3.3 RecognitionPerformanceBasedonPhysiologicalSynchronyFeatures. Dingetal.[27]usedamethodthat
combinesphysiologicalsynchronyindicesandclassificationalgorithms.Usingmachinelearningtechniques
suchaslineardiscriminantanalysisandrandomforests,theyperformedafour-categoryclassificationtaskand
ultimatelyachievedaclassificationaccuracyof75%.Whenthesynchronyfeatureswereremoved,theaccuracy
decreasedto65.6%.Darzietal.[25]usedthephysiologicalresponsesfrompairedparticipantstoautomatically
classifyemotionswithinacompetitivecontext.Theyusedlinearkernelsupportvectormachinesandensemble
decisiontreestoclassifytheparticipants’psychologicalstatesusingmultiplephysiologicalsignalssuchasskin
conductance,achievingabinaryclassificationaccuracyof84.3%andaternaryclassificationaccuracyof60.5%.
Kevinetal.[74]assigneddifferentrolesandtaskstoparticipantswithinthesamescenarioandconstructeda
lineardiscriminantclassifierbasedonspectralEEGfeaturestocategorizedifferentpsychologicalstates,achieving
aclassificationaccuracyof60%forteamcooperationlevels.Therefore,inter-brainsynchronyfeaturesshouldbe
includedinsimultaneousflowdetection.
Insummary,neural-activitysignalsreflectflowexperiencesandcanbeeffectivelyusedtoobjectivelyevaluate
them.ThisresearchfocusesonEEGsignals,fullyconsideringthephysiologicalsynchronyfeaturesofteam
members,andexploressimultaneousflowdetectionmethodsbasedonEEGsignals.Asthereiscurrentlyno
specialized,labeledEEGdatasetforsimultaneousflow,webeginbyconstructinganEEGdatasetforsimultaneous
flow.
3 CONSTRUCTIONOFSIMULTANEOUSFLOWEEGDATASET
Inthisstudy,westartwiththesmallestteamsize(twoteammembers)toconstructthefirstEEGdatasetfor
simultaneousflow.Beforecollectingdata,weneedtofirstdesignatwo-usercollaborativeexperimentaltaskto
inducesimultaneousflow.
3.1 DesignofAnExperimentaltaskforInducingSimultaneousFlow
3.1.1 DesignRationale. Currently,nounifiedtaskparadigmisspecificallydesignedtoinducesimultaneousflow.
Toconstructasimultaneousflowtask,wefirstsummarizedthecommonlyusedtasksininducingindividualflow,
including"Whack-a-Mole,""Tetris,"andothergametasks[6][7][30][79],aswellasmentalarithmetic,chess,and
learningquizzes,amongothers[3][37][76].Afteracomprehensivecomparisonoftheseflow-inducingtasks,we
designedthe"Two-playerCollaborativeWhack-a-MoleGame"basedonthetraditionalsingle-playerWhack-a-
Moletask.Themainselectioncriteriawereasfollows:
• Gamerulesandoperationsaresimpleandprovidehighaccessibilitytovarioususergroups.
• Thegamecaneasilybedesignedasacollaborativetaskinvolvinginterdependentpersonaltasks.
• Thegamecanensurehighfluencyinanoperatingtaskbecauseitislesssusceptibletorandomoperational
errors(e.g.,unlikeTetris,inwhichasmallerrorcanaccumulatealargeeffect).
• Thegamerequirestheparticipantstobefastandhighlyfocused,facilitatingtherapidinductionoftheflow
experience.
• Thedifficultyoftaskcanbedynamicallyadjustedinaneasyway.
Finally,wedevelopedanexperimentalsystemusingthewidelyusedgameengineUnity3Dbasedonthe
designdescribedinsubsequentsections.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:6 • Zhangetal.
3.1.2 Storyline. Previousstudiesdemonstratedthattheuseofthemesandstorylinescanenhanceusers’involve-
mentandintentionalengagement[5][11].Thestorylineforthisgameisasfollows:
Youandyourpartnerrunafarmasateambyproducingandsellingagriculturalproductssuchasvegetables.A
groupofmolesistryingtostealagriculturalproducts.Youandyourteammatesneedtoworktogethertoprotect
youragriculturalproductsandfightthemolestominimizelossesandmaximizeprofits.
3.1.3 TaskGoalandCollaborationPattern. AccordingtovandenHoutetal’sconceptualizationofflow[72][73],
thetaskGoalandcollaborationPatternweredesignedasfollows.
TaskGoal:Whenteammembersaimtoachieveacommonteamgoalandpursueoptimalteamperformance,
theycooperateandcollectivelyfocusontheirownpersonaltask,therebyinducingsimultaneousfloworevent
teamflow.Thegoaloftheteamtaskistocollaborativelyprotectasmanyvegetablesandeliminateasmanymoles
aspossibleinalimitedamountoftimetomaximizeprofits.
CollaborationPattern:User1isresponsibleforhittingasmanymolesaspossibletorecapturethevegetables
(Fig.2a),whereasuser2isresponsibleforhittingasmanymolesaspossibletocatchthem(Fig.2b).Althoughthe
divisionoflaborinthegamesisdifferent,theoperationalmethodsareidentical.Individualperformancesare
displayedonauserinterfaceinrealtime.
Hitting targets continuously as much as possible can result in additional rewards for team profits. Team
performanceandprofitsarealsodisplayedontheuserinterfaceinrealtime(Fig.2).Eachuser’sperformancein
thepersonaltaskcontributestotheteam’sperformance,whichmakesthemcollaborateasateam.
Fig.2. Screenshotsofthecollaborativegameinterfaceforuser1(a)anduser2(b).Theinterfacedisplaysvariousparameters
relatedtoteamtasks,individualperformance,andteamperformanceinreal-time.
3.1.4 FlowExperienceSampling. Differentfrompreviousstudieswhichmeasuredflowonlyonceafterthetask,
thisstudyaimedtocapturemoreimmediateinstancesofflow.Therefore,weemployedtheExperienceSampling
Method(ESM)tosampleflowexperiencemultipletimesduringthetask,whichisacommonmethodinflow
research[53].Eachroundofthegamelastsfor5min.Afterthegamestarts,theuserexperienceissampledevery1
minute(fivetimesintotal).Thesystemwasdesignedtopopoutanevaluationinterfacebyaskingparticipantsto
scoretheirindividualflowexperiences.Moreover,thegamescreenremainsvisiblewhentheevaluationinterface
isdisplayed,whichhelpsminimizedisruptiontothecontinuityofgameexperience.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:7
Astothemeasureofflow,whenimplementingESM,repeatedlyansweringlengthyflowquestionnaireswill
interrupt the flow state during the playing session and decrease the validity of the measurements[54], thus
single-itemmeasureisaneffectivewaytominimizedisruptiontothecontinuityofflowexperience[40].The
effectivenessofthesingle-itemmeasurehasbeensupportedbypreviousstudies[60][61].Inthisstudy,weadopted
asingle-itemscaletoevaluatetheflowexperienceonaLikertscalerangingfrom0(noflow)to3(highdegreeof
flow)(seeFig.3).TheitemwasdevelopedfromascientificquestionnairebyCsikszentmihalyitomeasureflow
experiences[53][62][54][40].Accordingtotheestablishedprocedureforusingthisquestionnaire,weprovided
participantswithaone-paragraphdescriptionoftheflowconceptandexamplestoensuretheircomprehensive
understandingbeforeenteringthegame.
Toenhancetherationalityandeffectivenessofthesingle-itemmeasureinESM,incorporatingatest-retest
procedureisbeneficialtominimizethepotentialmeasurementerrorofusingsingle-itemmeasures[26].Accord-
ingly,thisstudyincludedaprocedurewhereusersreviewedthevideorecordingimmediatelyaftercompleting
thegametoverifytheaccuracyofthescoring.
Fig.3. Duringthetask,flowexperiencesaresampledatintervalsbypoppingoutanevaluationinterface(a).Followingthe
sampling,thegamedifficultycanbeadjustedaccordingtothetwoplayer’sskillbypoppingoutanadjustmentinterface(b).
3.1.5 Adjustment of Game Difficulty. According to the classic flow-channel model, achieving flow experi-
encerequiresamatchbetweentheindividual’sskillandtheleveloftaskchallenge(optimalchallenge–skill
balance)[23][79].Appropriatedifficultyisacriticalfactorforinducingaflowexperience.Therefore,amechanism
foradjustingthedifficultyofflow-inducinggametasksisnecessary.
Toourknowledge,thereiscurrentlynoobjectivemethodthatcanaccuratelyandadaptivelyadjustthedifficulty
forpairedparticipants.Therefore,consideringtheexistingmethods,quicklycommunicatingandreachingan
agreementbetweentwoplayerscouldbeareliableapproach.Inthisgame,theoptimaltaskdifficultyisachieved
byallowingthetwouserstoactivelyadjustthespeedsofthemoles(timeofeachoccurrence).Therearefour
molesperoccurrenceandthespeedissetatfivelevels.Duringeachuserexperiencesampling,theparticipants
canquicklycommunicatewhethertoadjustthedifficultylevel,asshowninFig.3(b).
3.1.6 Recording of Game Data. The system continuously records data for both users throughout the game,
including:(1)thetimepointsofgamestart,experiencesampling,andgameover;(2)responsetimesfortemporal
alignmentandsegmentationwithEEGsignals;(3)ratingscoresofflowexperiencefromsampling;and(4)game
performancedata.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:8 • Zhangetal.
3.2 AcquisitionofEEGData
3.2.1 Participants. 52pairsofparticipantswererecruitedfromalocaluniversitytoparticipatethisstudy(mean
age:18.35±1.81years).Noneoftheparticipantsexperiencedalcoholconsumption,excessivefatigue,medication
use,orillnessfromthedaybeforetheexperimentuntilthedayoftesting.Eachpairofparticipantswasrandomly
assignedtooneoftworoles:recapturingvegetablesorcatchingmoles.Atotalof97.6%oftheparticipantswere
right-handed,96.97%hadpreviouslyplayedvideogames,and84.69%werefamiliarwiththeirteammates.The
study was conducted in accordance with the guidelines of the Declaration of Helsinki and approved by the
HumanResearchEthicsCommitteeofthelocalhospital.Informedconsentwasobtainedfromeachparticipant.
Note:Inthispaper,weuseddifferentwordstodescribeparticipants,suchas"user","player"and"teammember".
Theirmeaningsarethesameandweonlyusedifferentexpressionsindifferentcontexts.
3.2.2 Data Acquisition Apparatus and Environment. To acquire EEG data of simultaneous flow, each pair of
participantswasrequiredtoperformthecollaborativetasksimultaneously.TheyusedtwoPCs(LegionTower7i
Gen7withRTX3070andDellmonitors)withthesamemodeltoplaythegame(Fig.4),andperformedthetasks
usingmiceofthesamemodel(RazerDeathAdderV3Pro).
EachparticipantworeanEEGrecordingdevicetoacquiretheEEGsignalsthroughoutthecollaborativetask.
TheEEGrecordingdevicesadoptedinthisstudyweretwosetsofEmotivEpoc+,whichsupportstheacquisition
ofupto14channelsofEEGsignalsatasamplingrateof256Hz[43].Thesoftwareusedfordataacquisitionis
EmotivePRO.Theelectrodedistributionforthe14-channelEEGsignalsfollowsthe10-20internationalsystem,
asshowninFig.5.WechosetheEmotivEpoc+becauseitiseasytousewithincreasedpracticabilityandless
physicalrestriction.Moreover,ithasbeeneffectivelyusedinbuildingEEGdatasetsforflowexperience,emotions,
andpsychologicalbehaviors[47][82][38][56][70].
Fig.4. Dataacquisitionapparatusesandexperimentalenvironment.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:9
Fig.5. ElectrodeplacementdistributionforEEGdataacquisition.
3.2.3 EEGDataAcquisitionProcedure. ThedataacquisitionprocessisshowninFig.6.
First,theresearcherexplainedthegamedetailstoeachparticipantpair.Subsequently,eachpairofparticipants
practicedtheoperationsinapracticescene.TheythenworetheEEGacquisitiondevicesandenteredarelaxed
state.Duringthisperiod,theirbaselineEEGdatawerecollectedforapproximately1min.Theparticipantsthen
playedthreeroundsofthegame,withsufficientrestbetweenrounds.Tocapturevariousflow-relatedexperiences,
thestartingdifficultylevelsforthethreeroundswerevaried.Flowexperiencesampling(introducedinSection
3.1.4)wasperformedduringthisperiod.Duringthegame,theparticipantswereaskedtoavoidheadmovements
andfacialexpressionstoreduceartifactsintheEEGsignals.Theresearchermonitoredtheparticipants’EEG
dataonaseparatedisplaythroughoutthegame.Ifthereweresignalabnormalitiesduringtheprocess,theywere
recordedandconsideredtoexcludethecorrespondingdatafromsubsequentprocessing.Ittookapproximately
30–40 minutes for each pair of participants to complete the EEG data acquisition process. Each participant
obtainedacorrespondingcoursecreditasareward.Finally,welabeledandpreparedtherecordedEEGsample
dataforfurtherdatasetorganizationbasedontheflowscoresobtainedduringuserexperiencesampling.
3.3 DatasetOrganization
3.3.1 Selection of EEG Channels. It has been demonstrated that the flow experience is associated with the
activityinthefrontallobeofthebrain[37][53].Moreover,Shehataetal.[65]foundthatthelefttemporallobeacts
downstreamincausalinformationrelationships,receivingandintegratinginformationfrombrainregionsrelated
toindividualflowandsocialinteraction,andparticipatinginhigher-orderinter-brainneuralsynchrony[24].This
areaisactivatedduringtheemergenceofteamflowandplaysaroleingeneratingsharedflowexperiences[65].
Therefore,wehaveselectedtheEEGsignalsfromhefrontalandlefttemporallobestoconstructthesimultaneous
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:10 • Zhangetal.
Fig.6. ProcessDiagramofSimultaneousFlowCollectionSystem.
flowdataset.Finally,accordingtothedistributionoftheelectrodesshowninFig.5,eight-channelEEGsignals
wereselected:F3,F4,F7,F8,AF3,AF4,T7,andP7.
3.3.2 EEGSignalSlicing. Foreachparticipant,continuousEEGsignalswererecordedduringeachroundof
thegamefor5min.Thedatafromthe6-secondintervalsprecedingeachsamplingpointwereusedforEEG
dataslicing.Theformofdatastoredineachslicedsignalfilewaschannels×datapoints.Thesamplingrateis
256Hz,thenumberofdatapointsineachfileis1536.Weselected6-secondsegmentsofEEGsignals,duetoa
6-secondtimewindowsizestrikesabalancebetweenaccuracyandcomputationalefficiency.Itislongenoughto
capturesufficientinformationofasubjectivestate[82],andwhilebeingshortenoughtoreducecomputational
complexityfordynamiccomputing[16].
3.3.3 DatasetComposition. Although52pairsofparticipantsparticipatedinEEGdataacquisition,47pairswere
includedinthefinaldatasetafterexcludinggroupswithdataacquisitionerrorsandoperationalfailures.Thesize
ofthedatasetwasapproximately34.4GB,andcontainedEEGsignalsrecorded6sbeforeeachflowexperience
sampling.AlldataweresavedasCSVfiles,eachofwhichwasamatrixofdimensions[14,1536].Thesedata
represent14-channelEEGdatawith1536samplepoints;thesamplingrateoftheEEGsignalsis256Hzi.e.,each
CSVfilehasadurationof6seconds.
Duringthegame,theEEGsignalscollectedfromeachpairofparticipantswereorganizedby"groupnumber-
participantnumber-gamesetnumber.”Eachparticipant’sfoldercontainedEEGdataslicedfromfivegamerounds.
Thedataandcodeofthispaperisreleasedat:https://anonymous.4open.science/r/Team-Flow-Dataset-494E
anonymously.Detailedinformationaboutthedataisalsosummarizedinthe"README.md"file.
4 FEATUREEXTRACTIONFORINDIVIDUALANDSIMULTANEOUSFLOW
Accordingtopreviousstudies,EEGsignalscontainbothindividual[17][37][76]andsimultaneousflowfeatures[65].
Basedonpreviousstudies,wehavesummarizedtheEEGsignalfeaturesrelatedtoindividualflowandproposed
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:11
featuresthatmayreflectinter-brainsynchronyamongteammembersforsimultaneousflow.Methodsfordata
denoisingandfeatureextractionareintroducedinthissection.
4.1 DataDenoising
RawEEGdatacontainnoiseandredundancy,thusrequiringinitialpreprocessing.Themainsourcesofnoisein
EEGsignalsincludephysiologicalartifacts,powerlineinterference,andinterferencegeneratedbyactivitiessuch
asblinkingandfacialmusclemovements.ConsideringthefrequencyrangeofEEGsignalsandthefrequency
rangeofnoiseinterference,thisstudyemployedwaveletthresholddenoisingmethodstodenoiseeachchannelof
therawEEGsignals.
4.2 FeatureExtractionandNormalization
Based on previous research analyzing the EEG signals of individual flow experiences, this study extracted
sometime-andfrequency-domainfeaturesthatmayberelevanttoindividualflowexperiences.Moreover,we
preliminarilyextractedEEGfeaturesrelatedtointer-brainsynchronyaspotentialsimultaneousflowfeatures.
Detailedexplanationsareasfollows.
4.2.1 FeaturesforIndividualFlow. Thisstudyextractstimeandfrequency-domainfeaturesthatmayberelated
to individual flow experiences, as listed in Appendix Table 2. The time-domain features include the Mean,
StandardDeviation(SD),Variance,andAverageAbsoluteFirst-OrderDifference(AAFOD),NormalizedFirst-Order
Difference(NFOD),Energy,Power,HjorthParameters(Activity,Mobility),Higher-OrderZero-Crossing(HOZC),
Peak-to-PeakMean(PPM),andKurtosis.
Frequency-domainfeaturesmainlyinvolvewaveletdecomposition[46]ofthecollectedEEGsignalsintofour
non-overlappingsub-bands:𝛿 (0–4Hz),𝜃 (4–8Hz),𝛼 (8–16Hz),and𝛽 (16–32Hz).
Thefrequency-domainfeaturesarethenextractedfromthesesub-bandsintheeightEEGchannels.Power
SpectralDensity(PSD)andLogarithmicbandpower(LBP)arecalculatedforeachofthefourbands.Differential
Entropy(DE)featuresareextractedfromfourfrequencybands(𝛿,𝜃,𝛼,and𝛽)andthefullfrequencyband(FB)of
EEGsignalsineachchannel.ThisstudyemploystheWelchmethod[77]toextractthepowerspectraldensity
offourfrequencybands:delta,theta,alpha,andbeta,andcomputestheaverageofthepowerspectrumacross
channeldimensions.
4.2.2 Features for Simultaneous Flow. In the context of simultaneous flow, this study extracted inter-brain
synchronyfeaturesfromtheEEGsignalsofeachpairofparticipants,includingcross-correlationcoefficientsand
DynamicTimeWarping(DTW)distance.
Let𝐴= {𝑎 1,𝑎 2,𝑎 3,...,𝑎 𝑚}representtheEEGsignalvaluessampledfromParticipant1atacertainelectrode,
and𝐵 = {𝑏 1,𝑏 2,𝑏 3,...,𝑏 𝑛}representtheEEGsignalvaluessampledfromParticipant2atthesameelectrode.The
featuresareintroducedasfollows:
(1) Cross-correlationCoefficient:ThePearsoncorrelationcoefficientforthebrainwavesignalsbetweeneach
channelandthefrequencybandofthetwoparticipantsarecomputed[33].Thisservesasanindexfor
assessingthesignalcorrelationbetweenteammembers.
cov(𝐴,𝐵)
𝜌 𝐴,𝐵 = 𝜎 𝜎 (1)
𝐴 𝐵
Wherecov(𝐴,𝐵)calculatesthecovariancebetweenthetwosignals,𝜎 𝐴 isthestandarddeviationofEEG
signalA,and𝜎 𝐵 isthestandarddeviationofEEGsignalB.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:12 • Zhangetal.
(2) DynamicTimeWarpingDistance[18][25][48]:Thisisatechniquebasedondynamicprogrammingforquan-
tifyingthesimilaritybetweentwosignals,andisrobusttotimedecorrelation.Thespecificcomputational
stepsareasfollows:
(a) CalculatetheEuclideandistance𝐷(𝑎 𝑖,𝑏 𝑗)betweeneverytwosamplepointsinthetwosignalsequences,
where1 ≤𝑖 ≤𝑚,1 ≤ 𝑗 ≤𝑛.
(b) Findtheshortestpathdistancefrom(𝑎 1,𝑏 1)to(𝑎 𝑚,𝑏 𝑛).Thenextnodefromacurrentnode(𝑎 𝑖,𝑏 𝑗)must
beamong(𝑎 𝑖+1,𝑏 𝑗),(𝑎 𝑖,𝑏 𝑗+1),or(𝑎 𝑖+1,𝑏 𝑗+1),andthepathmustbetheshortest.
(c) Duringeachiterationtoreach(𝑎 𝑖,𝑏 𝑗),followthedynamicprogrammingparadigmtochoosetheshortest
distanceto(𝑎 𝑖,𝑏 𝑗)fromamong(𝑎 𝑖−1,𝑏 𝑗),(𝑎 𝑖,𝑏 𝑗−1),or(𝑎 𝑖−1,𝑏 𝑗−1).
𝐷(𝑎 𝑖,𝑏 𝑗) =Dist(𝑎 𝑖,𝑏 𝑗)+min(cid:8)𝐷(𝑎 𝑖−1,𝑏 𝑗),𝐷(𝑎 𝑖,𝑏 𝑗−1),𝐷(𝑎 𝑖−1,𝑏 𝑗−1)(cid:9) (2)
4.2.3 FeatureNormalization. DuetothetypicallysignificantvariabilityinthefeaturevaluesofEEGsignals
amongdifferentparticipants,thisstudyemploystheZ-scorenormalizationmethodtostandardizethefeatures
ofallparticipants.Thisstrategyaimstomitigatetheimpactofinter-individualdifferencesintherangeand
variabilityofphysiologicalsignals[79].Let𝑓 representthefeaturevaluesextractedforallparticipantsonagiven
feature,𝑓¯bethemeanvalueofthesefeaturevalues,and 𝑓 𝑠𝑡𝑑 bethestandarddeviationofthefeaturevalues.The
normalizedvalueofthefeatureisthengivenby:
𝑓 −𝑓¯
𝑧 =
𝑓
𝑠𝑡𝑑
4.2.4 OverviewofFeatures. Theindividualandinter-brainsynchronyflowfeaturesselectedinthisstudyare
listedinAppendixTable2.Byextractingfeaturesfromtheeight-channelEEGsignals,272features(208features
ofindividualflowand64featuresofinter-brainsynchrony)wereselected,constitutingthesetofindividualand
simultaneousflowfeaturesusedinthisstudy.
5 VALIDATIONOFFLOWFEATURES
5.1 Purpose
The primary purpose of this section is to validate the effectiveness of individual and inter-brain synchrony
featuresindetectingdifferenttypesofsimultaneousflowandtoanalyzetheimpactofthesefeaturesinthis
process.Webeganwithvalidatingtheeffectivenessofthesefeaturesinbinaryandternaryclassificationtasks.
5.2 DatasetOrganization
BasedontheflowratingscoresonEEGdataset,weestablishedtwosetsofclassificationlabels.Thefirstset
distinguishesbetween"LowSimultaneousFlow"and"HighSimultaneousFlow."Thesecondsetcategorizesthe
dataintothreegroups:"SimultaneousFlow","IndividualbutnotSimultaneousFlow,"and"NeitherIndividualnor
SimultaneousFlow".
5.2.1 Binary Classification Labels. The binary classification labels of "Low Simultaneous Flow" and "High
SimultaneousFlow"areco-determinedbytheflowexperiencesofthepairedusers.Accordingtothedefinition
of simultaneous flow [10], this study operationally defined it as "simultaneous flow occurs only when both
participants achieve a high flow experience in collaborative task". Scores of "0" and "1" represent low flow
experiences,whilescoresof"2"and"3"representhighflowexperiences.Takingtheflowexperienceofparticipant
1asanexample,thelabelingrulesoftheSimultaneousFlowLabelsarelistedinTable1,andthedistributionof
samplelabelsinthedatasetislistedinTable2.
5.2.2 TernaryClassificationLabels. Accordingtothedefinitionofsimultaneousflow,theternaryclassification
labelsareco-determinedbythepairedparticipants’flowexperience.TakingtheflowexperienceofParticipant1
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:13
Table1. SimultaneousFlowLabelsbasedonthetwousers’flowscore(BinaryClassification)
Scoreofparticipant1 Scoreofparticipant2 BinaryClassificationlabels
0 0/1 LowSimultaneousFlow
1 0/1 LowSimultaneousFlow
2 2/3 HighSimultaneousFlow
3 2/3 HighSimultaneousFlow
Table2. SampleLabelDistribution(BinaryClassification)
SimultaneousFlowLabel SampleCount Proportion(%)
LowSimultaneousFlow 512 37.1
HighSimultaneousFlow 868 62.9
asanexample,thelabelingrulesarelistedinTable3.Thedistributionoftheternaryclassificationsamplelabels
inthedatasetispresentedinTable4,revealingtheproblemofsampleimbalance.
Table3. ComparisonofIndividualFlowRatingsandSimultaneousFlowLabels(TernaryClassification)
Individual Individual
Flowof Flowof SimultaneousFlow TernaryClassificationLabelsofParticipant1
Participant1 Participant2
Low Low LowSimultaneousFlow NeitherIndividualnorSimultaneousFlow
Low High LowSimultaneousFlow NeitherIndividualnorSimultaneousFlow
High Low LowSimultaneousFlow IndividualbutnotSimultaneousFlow
High High HighSimultaneousFlow SimultaneousFlow
Table4. SampleLabelDistribution(TernaryClassification)
SimultaneousFlowLabel SampleCount Proportion(%)
NeitherIndividualnorSimultaneousFlow 278 20.1
IndividualbutnotSimultaneousFlow 234 17.0
SimultaneousFlow 868 62.9
5.3 ValidationTools
Researchonflowcomputationshasdemonstratedthatmachinelearninganddeeplearningmodelsareeffective
in detecing the individual flow experiences [45][59][79]. Building on these findings, this study employed a
rangeofclassicclassifiersandneuralnetworkstovalidatetheeffectivenessofEEGfeaturesforindividualand
simultaneousflow.Toevaluatemodelperformance,thisstudyusedmetricssuchasaccuracy,precision,recall
andF1scoreforacomprehensiveevaluation.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:14 • Zhangetal.
5.3.1 DetailsofImplementation. TrainingandevaluationofallthemodelswereimplementedusingPython3.9.12,
withtheScikit-Learn[52]library(version1.2.0)andPyTorch[51](version1.12.0).Modeltrainingwasperformed
onNvidiaGeforceRTX3070Ti.Weusedclassificationmodelstopredictthesimultaneousflowexperienceusing
thefeaturesintroducedinSection4.2.ArangeoftraditionalmachinelearningalgorithmsfromScikit-Learnwere
usedincludingLogisticRegression(LR),SupportVectorMachines(SVM),DecisionTrees(DT),andRandom
Forests(RF).Inaddition,wedesignedseveralNeuralNetwork(NN)modelswithdifferentarchitecturesusing
PyTorch,distinctfromScikit-Learn.AppendixTable3providesadetaileddescriptionsandconfigurationsofthe
models.
5.3.2 Metrics. Forbinaryandternaryclassificationtasks,weusedmetricsincludingAccuracy,Precision,Recall,
andF1Score.DetaileddefinitionsoftheseformulascanbefoundinAppendixTable1.
5.4 ValidationProcedure
Thefollowingstrategieswereusedtoreducethebiasintroducedbydatadistributionandtheselectionoftraining
andtestingsets.
5.4.1 Balancing the Dataset. For the binary classification task, the ratio of low simultaneous flow to high
simultaneousflowsamplesintheunbalanceddatasetwas1:1.64.WeusedtheSMOTEalgorithm[19]togenerate
minority-classdata,adjustingtheratioofdifferentclassesto1:1.Afteradjustment,thesamplesizeforboth
classeswas838,foratotalsamplesizeof1676.
Forthemulti-classificationtask,theratioof"NeitherIndividualFlownorSimultaneousFlow,""Individualbut
notSimultaneousFlow"and"SimultaneousFlow"intheunbalanceddatasetwas1.2:1:3.7.WeusedtheSMOTE
algorithmtogenerateminorityclassdataandadjusttheratioofallthreeclassesto1:1:1,therebyincreasingthe
totalnumberofsamplesto2604.
5.4.2 Ten-foldCross-Validation. Toavoidthebiasintroducedbydatapartitioning,weusedk-foldcross-validation
todividethesampledatasetrandomlyintokfolds.Ineachrun,onefoldwasusedasthetestsetandtheremaining
k-1foldswereusedasthetrainingset.Classifieraccuracy,precision,recall,andF1scorewerecalculatedbased
onthetestsetforeachtrainingset.
Tomitigateproblemsarisingfromimproperdatasplittingandevaluatethegeneralizabilityofthemodelmore
accurately,weusedten-foldcross-validation(k=10).Thefinalperformancemetricwastheaverageofthescores
acrossthetenfolds.
5.5 ValidationResults
5.5.1 ValidationResultsoftheBinaryClassification. Thevalidationresultsareintroducedasfollows.Table5
presentstheperformanceofvariousmachinelearningmodelsinabinaryclassificationtask,withorwithout
consideringinter-brainsynchronyfeatures,asevaluatedthroughten-foldcross-validation.Themainresultsare
summarizedasfollows:
(1) Modelsincludinginter-brainsynchronyfeaturesoutperformedthoseexcludingthem,demon-
stratingtheirbenefitstotheclassificationtask.Thesefindingsimplythatincludinginter-brainsynchrony
featurescanenhancetheclassificationperformanceofsimultaneousflowmodels,therebypreliminary
validatingthecontributionoftheEEGinter-brainsynchronyfeaturestosimultaneousflowclassification.
TheAppendix’sTable4displaystheT-testresults,showing𝑝-valuesunder0.05(𝑝𝑠<0.05).Thissuggests
thestatisticalsignificanceofthemodel’sperformanceenhancementuponincludingsynchronyfeatures.
(2) Tree-basedmodelsexhibitthemostsignificantimprovementaftertheadditionofsynchrony
features.Whenconsideringsynchronyfeatures,theRandomForestmodelsurpassedmostothersinbinary
classification,achievinganaccuracyof83.9%.DecisionTreesdemonstratedthemostnotableimprovement,
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:15
Model Accuracy Recall Precision F1Score
Mean Std Mean Std Mean Std Mean Std
LR 0.606 0.032 0.606 0.033 0.607 0.033 0.605 0.032
LR(S) 0.632 0.023 0.632 0.025 0.633 0.025 0.630 0.025
SVM 0.663 0.034 0.664 0.035 0.664 0.034 0.663 0.034
SVM(S) 0.696 0.030 0.697 0.032 0.698 0.033 0.696 0.030
DT 0.618 0.049 0.618 0.049 0.620 0.050 0.616 0.049
DT(S) 0.757 0.039 0.757 0.039 0.757 0.039 0.756 0.039
RF 0.758 0.032 0.759 0.033 0.760 0.033 0.758 0.032
RF(S) 0.839 0.024 0.840 0.025 0.841 0.024 0.838 0.024
NN 0.714 0.031 0.714 0.035 0.717 0.036 0.712 0.032
NN(S) 0.757 0.026 0.757 0.027 0.759 0.026 0.756 0.027
DNN1 0.718 0.037 0.717 0.038 0.730 0.036 0.712 0.038
DNN1(S) 0.753 0.023 0.753 0.023 0.763 0.022 0.750 0.024
DNN2 0.722 0.030 0.724 0.031 0.729 0.031 0.720 0.031
DNN2(S) 0.754 0.035 0.753 0.038 0.766 0.030 0.749 0.040
DNN3 0.732 0.033 0.733 0.033 0.738 0.033 0.730 0.033
DNN3(S) 0.762 0.019 0.751 0.020 0.768 0.028 0.747 0.019
Table5. Averageperformanceof10-foldcrossvalidationonbinaryclassificationfordifferentmachinelearningmodels.Std
denotesstandarddeviation.TheconfigurationsofdifferentmodelsareprovidedinTable3ofAppendix.
enhancingby13.9%.ThisimprovementmightbeattributedtothemechanismofDTandRF.Effective
featureextractionandselectionensurethedecisionnodessplitdataeffectively,therebyenhancingthe
models’predictiveaccuracy.Thediscussionsectionprovidesamorein-depthexplanationofthepossible
reasonsforthisresult.
5.5.2 Validation Results of the Ternary Classification. Table 6 presents the performance of various machine
learningmodelsinaternaryclassificationtask,consideringbothwithandwithoutinter-brainsynchronyfeatures,
asevaluatedthroughten-foldcross-validation.Themainresultsaresummarizedasfollows:
(1) Consistentwiththetrendsobservedinthebinaryclassificationtask,modelsincludingsynchrony
featuresgenerallyoutperformedthosewithoutthesefeatures.Thisvalidatestheeffectivenessof
inter-brainsynchronyfeaturesinenhancingmodelperformance.Thissuggeststhatinter-brainsynchrony
features contain a certain amount of information valuable for the three-class classification. Table 5 in
theAppendixpresentstheresultsoftheT-test,with𝑝-valueslessthan0.05(𝑝𝑠<0.05),indicatingthatthe
improvementinmodelperformanceisstatisticallysignificant.
(2) Themetricsoftheternaryclassificationtaskweregenerallyhigherthanthoseofthebinary
classificationtask.Thiscouldbeattributedtothemoredistinctivecategoriesintheternaryclassification
task. The categories in ternary classification task offer clearer distinctions compared to the high and
lowsimultaneousflowstatesinbinaryclassification.Thediscussionsectionprovidesamorein-depth
explanationofthepossiblereasonsforthisresult.
(3) Fortheperformanceofdifferentmodels,theNNandDNN3withsynchronyfeaturesoutper-
formedthanothermodelswhichachieveanaccuracyof87.2%.However,wefoundthatmorecomplex
DNNmodelsmayoverfitthetrainingdata,especiallywhenthedatasetsizeisinadequate.Overfittingcan
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:16 • Zhangetal.
Model Accuracy Recall Precision F1Score
Mean Std Mean Std Mean Std Mean Std
LR 0.607 0.029 0.607 0.029 0.604 0.029 0.604 0.03
LR(S) 0.634 0.028 0.634 0.027 0.632 0.028 0.629 0.028
SVM 0.729 0.020 0.730 0.022 0.729 0.022 0.725 0.020
SVM(S) 0.758 0.029 0.758 0.029 0.756 0.029 0.755 0.027
DT 0.629 0.022 0.629 0.023 0.628 0.022 0.625 0.022
DT(S) 0.666 0.026 0.666 0.024 0.665 0.024 0.660 0.024
RF 0.842 0.016 0.842 0.017 0.842 0.017 0.839 0.017
RF(S) 0.868 0.019 0.870 0.019 0.869 0.019 0.867 0.019
NN 0.809 0.015 0.809 0.012 0.808 0.013 0.804 0.015
NN(S) 0.872 0.017 0.872 0.014 0.874 0.017 0.868 0.017
DNN1 0.837 0.016 0.837 0.014 0.841 0.015 0.829 0.016
DNN1(S) 0.862 0.020 0.863 0.015 0.868 0.019 0.856 0.019
DNN2 0.843 0.020 0.842 0.023 0.846 0.021 0.836 0.023
DNN2(S) 0.862 0.019 0.862 0.019 0.871 0.018 0.856 0.020
DNN3 0.845 0.020 0.844 0.024 0.848 0.020 0.838 0.024
DNN3(S) 0.872 0.016 0.871 0.018 0.876 0.019 0.866 0.018
Table6. Averageperformanceof10-foldcrossvalidationonternaryclassificationfordifferentmachinelearningmodels.Std
denotesstandarddeviation.TheconfigurationsofdifferentmodelsareprovidedinTable3ofAppendix.
limittheperformanceimprovementontestdata.ComparedtotheDNN2withfivehiddenlayers,theDNN3
withninehiddenlayersshowedlimitedimprovementinperformanceandoffersnoadvantageinnumber
ofparameters.
5.5.3 FeatureImportance. Afterthecomparisonexperiments,wehavethefollowingquestions:
• Q1:Whichbrainregion’sfeaturescontributemoresignificantlytoflowclassification?
• Q2:Howsignificantisthecontributionofsynchronyfeaturestoclassification?
Toanswerthequestionsandfurthervalidatetheeffectivenessofsynchronyfeatures,weemployedfeature
selectionmethodsusingLR,RF,NNandDNN3.Table7and8respectivelyshowthetop20featuresforbinaryand
ternaryclassificationtasksincludinginter-brainsynchronyfeaturesonfourrepresentativemodels.Tables6and
7,8and9intheAppendixpresentthetop20featuresandtheirimportancevaluesforbinaryalongwithternary
classificationtasks,includingandexcludinginter-brainsynchronyfeatures.Thefeaturesandtheircalculation
methodsareshowninTable2oftheAppendix.
IntheLRmodel,eachfeatureisassociatedwithacoefficient,indicatingtheextentanddirectionofthefeature’s
impactontheprediction.Thesignofthecoefficient(COEF)denotesthepositiveornegativecorrelationofthe
featurewiththeresult.Themagnitudeofthecoefficient’sabsolutevaluereflectstheimportanceofthefeature.
WecalculatedtheaveragecoefficientforeachfeatureusingLogisticRegressionmodelstrainedineachfoldof
ten-foldcross-validation.FeatureswerethenrankedaccordingtotheabsolutevalueoftheirCOEF,selectingthe
top20withthelargestabsolutevalues.
InthefeatureselectionbasedonRF,weusedrandomforestmodelstrainedineachfoldoften-foldcross-
validationtocalculatetheaverageimportanceofeachfeature.Featureswererankedaccordingtotheirimportance
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:17
(IMP), and the top 20 features with the highest importance were selected. The importance of a feature was
computedbasedontheMeanDecreaseinImpurity(MDI)[13].
SHapleyAdditiveexPlanations(SHAP)[44]valuesareamethodforinterpretingthepredictionsofmachine
learningmodels.SHAPworksbyassigningthemodel’soutputtoeachinputfeature,thusshowinghowmuch
eachfeaturecontributestotheprediction.WecalculatedtheaverageSHAPvaluesforeachfeatureusingNNand
DNN3modelstrainedineachfoldoften-foldcross-validation.Featuresweresubsequentlyrankedbasedonthe
absolutevaluesoftheirSHAPvalues,andthetop20featureswiththelargestabsolutevalueswereselected.
LR RF NN DNN3
P7NFOD P7DTW𝛼 F7PSD𝛼 F7CCC𝛿
F7PSD𝛼 AF4CCC𝛼 F8CCC𝛿 AF4HOZC
AF4DTW𝛼 F7CCC𝛽 F7PSD𝜃 F3CCC𝜃
F3NFOD F4MeanPSD AF3DTW𝛿 T7DE𝛿
P7Mobility F3CCC𝛼 F8CCC𝛼 AF4CCC𝛽
AF3Mobility F3DTW𝜃 F4LBP𝛿 T7PSD𝛽
AF3DTW𝜃 F7DTW𝛿 F4AAFOD F4LBP𝛽
F8DTW𝜃 F3DTW𝛼 F3DTW𝛿 F8CCC𝛿
T7PSD𝜃 AF3CCC𝛼 P7Variance F7CCC𝛼
F8DTW𝛼 P7CCC𝛼 F4DTW𝜃 AF4Energy
F4DTW𝛼 AF4CCC𝛽 F3CCC𝜃 F4HOZC
F3DTW𝛽 F8CCC𝛼 AF3PPM T7AAFOD
AF3DTW𝛼 F8CCC𝛽 P7DTW𝛽 AF3DEFB
T7SD F8CCC𝛿 T7PSD𝛼 F3DE𝜃
T7Mobility F4CCC𝛼 F4LBP𝜃 AF3CCC𝜃
AF3DTW𝛿 AF4CCC𝜃 P7PSD𝛽 AF3PSD𝛿
F7DTW𝛿 F7CCC𝛿 F8DTW𝜃 F7DE𝛼
F4DTW𝛽 F4average F8PPM T7Mobility
F3Mobility F4PSD𝛿 F4Energy AF3CCC𝛽
T7PSD𝛼 P7CCC𝜃 P7PPM F8Mobility
Table7. Theresultsoffeatureimportanceexperimentonbinaryclassificationmodels.Featuresinboldrepresentinter-brain
synchronyfeatures,graycellsrepresentfeaturesextractedfromthefrontallobe.Thefullnamesofthefeaturesaredetailed
insection4.2.1.
Themainresultsaresummarizedasfollows:
(1) Comparedtofeaturesofthelefttemporallobe(T7,P7),thefeatureslocatedinthefrontallobe
(AF3,AF4,F3,F4,F7,F8)weremorepreferredbythemodels.AsshowninTables7and8,thefrontal
lobefeatures(graycells)accountforasignificantproportionbothinbinaryandternaryclassification.This
suggeststhatthefrontalloberegionlikelycarriesmoreeffectiveinformationthanthelefttemporallobe.
ThisisconsistentwithBruyaetal.’sopinion[15]thattheflowexperienceiscloselyrelatedtotheactivation
levelofcerebralcortexandfrontalcortex.
(2) Theinclusionofsynchronyfeaturesaccountedforasignificantproportioninthetop20features.
Inthebinaryclassificationtaskincludingsynchronyfeatures,thesynchronyfeaturesoccupiedover35%of
thetop20features,whileinternaryclassificationtasks,theyrepresentedover20%.Thissubstantiatesthe
efficacyofinter-brainsynchronyfeaturesinrecognizingsimultaneousflow.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:18 • Zhangetal.
LR RF NN DNN3
P7SD F4PSD𝜃 P7PSD𝛽 AF4CCC𝜃
AF4DTW𝛼 P7DTW𝛼 F7PSD𝛽 P7Kurtosis
AF4Mobility F4average F8DTW𝛿 AF3Kurtosis
F4DE𝛿 AF3Kurtosis P7Kurtosis T7LBP𝛽
P7Mobility AF3PSD𝛽 F8PSD𝜃 F4CCC𝜃
AF3DTW𝜃 P7DE𝜃 AF4PSD𝛿 F7DE𝛽
F7NFOD F8PPM AF3LBP𝛽 F7DEFB
F8SD F4LBP𝛿 AF3PSD𝛽 F7Kurtosis
P7Variance P7LBP𝜃 AF4AAFOD F4LBP𝛽
P7Activity F4Energy F4DE𝛿 AF3DE𝜃
T7DEFB F8Kurtosis P7HOZC F3NOFD
AF4DEFB F4Power F4DTW𝛿 T7CCC𝜃
P7NFOD P7CCC𝛽 F3LBP𝜃 P7DEFB
F8DEFB AF4CCC𝛼 T7PSD𝜃 F4LBP𝜃
AF4PPM F8CCC𝛼 T7DEFB T7CCC𝛿
T7DTW𝛿 F7PSD𝛽 F3DE𝜃 F3CCC𝛼
F7Mobility F4LBP𝛼 T7PSD𝛼 F4DTW𝛿
F8PSD𝛽 T7DTW𝜃 AF3DTW𝛼 F7CCC𝛿
F4DTW𝛼 F4DE𝛼 P7PSD𝛼 T7LBP𝜃
AF4DE𝛿 T7DTW𝛽 F7CCC𝛿 T7AAFOD
Table8. Theresultsoffeatureimportanceexperimentonternaryclassificationmodels.Featuresinboldrepresentinter-brain
synchronyfeatures,graycellsrepresentfeaturesextractedfromthefrontallobe.Thefullnamesofthefeaturesaredetailed
insection4.2.1.
(3) Synchronyfeaturesweremorefocusedinbinaryclassificationtasks.Thepreferenceforsynchrony
featuresinbinaryclassificationmayattributetotheircriticalroleinreflectingteamcollaborationlevels
todifferentiatebetweenhighandlowsimultaneousflow.Incontrast,theternaryclassificationrequiresa
balancebetweenindividualandsynchronyfeaturesduetoitsclearercategorydistinctions.Thediscussion
sectionprovidesamorein-depthexplanationofthepossiblereasonsforthisresult.
5.5.4 AblationStudy. Toevaluatetheimpactofdifferentfeaturesetsontheperformanceofdifferentmodels,we
conductedablationexperiment.Thisexperimentaimstofurthervalidatethesignificanceoffeaturesrelatedto
thefrontallobeandinter-brainsynchrony.ThemodelsconsideredincludeLR,RF,NNandDNN3.Eachmodel
wastrainedusinga10-foldcross-validationtoensurethereliabilityoftheresults.Table9showstheresultsof
ablationexperimentsforLR,RF,NN,DNN3onternaryclassification.
Thefeaturesetsusedareextractedfromthelefttemporallobe(L),thefrontallobe(F),andtheircorresponding
synchronyfeatures(LSforlefttemporallobe,FSforfrontallobe).Thebaselinefeaturesetconsistedoffeatures
fromthelefttemporallobe(L).WecalculatedΔtorepresentthepercentageincreaseinperformancemetrics.
Themainresultsaresummarizedasfollows:
(1) Featureadditionimprovesperformanceandtheimprovementsacrossdifferentmetricsisconsis-
tent.AsshowninTable9,theinclusionofadditionalfeaturesgenerallyimprovedtheperformancemetrics
forallmodels.Moreover,theconsistenttrendofperformanceimprovementsacrossallmetricsforeach
modelindicatestherobustnessofperformancegains.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:19
Model FeatureSet Accuracy Recall Precision F1Score
Mean Δ(%)↑ Mean Δ(%)↑ Mean Δ(%)↑ Mean Δ(%)↑
L 0.479 - 0.480 - 0.478 - 0.476 -
F 0.580 10.1 0.580 10.0 0.576 9.8 0.574 9.8
L+F 0.607 12.8 0.607 12.7 0.604 12.6 0.604 12.8
LR
L+F+LS 0.608 12.9 0.608 12.8 0.604 12.6 0.603 12.7
L+F+FS 0.633 15.4 0.633 15.3 0.630 15.2 0.629 15.3
L+F+FS+LS 0.634 15.5 0.634 15.4 0.632 15.4 0.629 15.3
L 0.772 - 0.772 - 0.772 - 0.768 -
F 0.831 5.9 0.831 5.9 0.830 5.8 0.828 6.0
L+F 0.842 7.0 0.842 7.0 0.842 7.0 0.839 7.1
RF
L+F+LS 0.851 7.9 0.852 8.0 0.851 7.9 0.850 8.2
L+F+FS 0.856 8.4 0.857 8.5 0.857 8.5 0.855 8.7
L+F+FS+LS 0.868 9.6 0.870 9.8 0.869 9.7 0.867 9.9
L 0.630 - 0.631 - 0.631 - 0.624 -
F 0.796 16.6 0.796 16.5 0.795 16.4 0.791 16.7
L+F 0.809 17.9 0.809 17.8 0.808 17.7 0.804 18.0
NN
L+F+LS 0.845 21.5 0.846 21.5 0.847 21.6 0.840 21.6
L+F+FS 0.863 23.3 0.864 23.3 0.865 23.4 0.860 23.6
L+F+FS+LS 0.872 24.2 0.872 24.1 0.874 24.3 0.868 24.4
L 0.762 - 0.762 - 0.766 - 0.753 -
F 0.824 6.2 0.823 6.1 0.831 6.5 0.815 6.2
L+F 0.845 8.3 0.844 8.2 0.848 8.2 0.838 8.5
DNN3
L+F+LS 0.858 9.6 0.858 9.6 0.864 9.8 0.852 9.9
L+F+FS 0.868 10.6 0.868 10.6 0.871 10.5 0.863 11.0
L+F+FS+LS 0.872 11.0 0.871 10.9 0.876 11.0 0.866 11.3
Table9. AblationexperimentsforLR,RF,NN,DNN3onternaryclassification.Δrepresentsthepercentageincreasein
performancemetricscomparedtothebaselinefeatureset(usingonlylefttemporallobefeatures,denotedas’L’).Lstands
forfeaturesfromthelefttemporallobe,Fforfeaturesfromthefrontallobe,FSforsynchronyfeaturesofthefrontallobe,
andLSforsynchronyfeaturesofthelefttemporallobe.Thesamemodelconfigurationsareusedacrossdifferentfeaturesets,
asdetailedinTable3ofAppendix.
(2) Modelsusingfrontallobefeatures(F)exhibitsignificantimprovementsonallmetricscompared
to which uses using left lobe features(L). The performance enhancements(Δ) across four metrics
rangefrom5.8%to16.7%whichsuggestfeaturesfromfrontallobecarrymoreinformationrelatedtoflow
experience.
(3) Synchrony features in the frontal lobe appear to be more significant than those in the left
temporallobe.ThemodelsexhibitedgreaterperformanceenhancementswiththeinclusionoftheFS
featuresetcomparedtotheLSfeatureset.Thissuggeststhatsynchronyfeaturesfromthefrontallobe
containmorerelevantinformation,contributingtogreaterperformancegainscomparedtothosefromthe
lefttemporallobe.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:20 • Zhangetal.
(4) Inter-brainsynchronyfeaturesplayedasignificantroleinclassifyingflowstatesduringcoop-
erativetasks.Performanceimprovementswereobservedinnearlyallmodelsfollowingtheadditionof
theLS,FS,andcombinedLS+FSfeaturesets.Thisresult,inconjunctionwiththeresultsofthefeature
importanceexperiments,collectivelyvalidatestheimportanceofinter-brainsynchronyfeatures.
6 DISCUSSION
Thisstudyfocusonthesimultaneousflow,preliminarilyexploringitsfeaturesandthemethodsfordetecting
simultaneousflowexperiencesincollaborationbasedonEEGsignals.Thefollowingdiscussionaddressesseveral
importantquestions.
6.1 ValidityofSimultaneousFlowTasksandDataset
6.1.1 Thetwo-playerWhack-A-Molegameiseffectiveininducingsimultaneousflowexperiences. Experimentson
neuralsynchronyinsimultaneousflowarerare.AndThereisnounifiedtaskparadigmisspecificallydesigned
toinducesimultaneousflow.Ourstudyusedatwo-playerWhack-A-Molegame,whichiscommonlyusedin
individualflowresearch.Thetaskisrule-basedandstraightforward,allowingforquickconcentrationandthe
inductionofflowexperiencesinashorttime.Inaddition,thetaskisrobusttoisolatedandrandomoperator
errorsandiswell-suitedtotraditionalflowevaluationtechniques,suchastheExperienceSamplingMethod(ESM).
Ourresultsindicatethattheexperimentaltaskiseffectiveininducingsimultaneousflowexperiencesinthe
participants.
6.1.2 TheEEGdatasettoclassifysimultaneousflowwasconstructedeffectively. Ratherthanaimingforthehighest
numberofchannels,weselectedthosewithmostpotentialrelevancetoindividualandsimultaneousflow.Based
ontheneuralmechanismsofflow,thefrontallobebrainregionsarehighlyassociatedwithflowexperiences.
Inaddition,thelefttemporallobe,whichisinvolvedinadvancedneuralsynchrony,waspreliminarilyfound
activatedduringflowincollaborativetasks(relevanttosimultaneousflow).Therefore,wefocusedontheEEG
signalsfromthefrontal(AF3,AF4,F3,F4,F7,F8)andlefttemporal(T7,P7)lobestoconstructtheEEGdatasetof
simultaneousflow.
Allpreviousdatasetofflowfocusedonindividualflow,labelingtypesofexperienceinducedbydifferentmatch
ofskillsandchallenges,suchasboredom,flowandanxiety[37][57][66][79].Incontrast,ourstudyclassifiedexpe-
riencesintohighandlowsimultaneousflow,aswellassituationsthatwere"NeitherIndividualnorSimultaneous
Flow","IndividualbutnotSimultaneousFlow",and"SimultaneousFlow".Notonlydidwedelveintotheintensity
ofsimultaneousflow,butalsoconsideredontherelationshipbetweenindividualandsimultaneousflow,andour
resultsshowthatthisEEGdatasetcontainseffectiveflowlabelsandfeaturessuitablefordetectingsimultaneous
flowexperiences.
6.2 EEGFeaturesofSimultaneousFlow
6.2.1 Itisnecessarytofocusthefeaturesthatreflectthesynchronyofneuralactivity. AccordingtoYunetal.[80],
thesynchronyofneuralactivitybetweentheparticipantsincreasedaftercooperativeinteraction.Sinhaetal.[67]
havedemonstratedthatinter-brainsynchronyismorelikelytooccurwhenparticipantsarecooperatingtowards
achievingacommongoal.Simultaneousflowisauniquebrainexperienceassociatedwithenhancedinformation
integrationandneuralsynchrony.Therefore,toassesssimultaneousflow,itisnecessarytofocusnotonlyon
relevantbrainregionsbutalsoonEEGfeaturesthatreflectthesynchronyofneuralactivityacrossthebrain.The
globalinter-brainintegratedinformationandneuralsynchronyareenhancedinthestateofteamflow(relevant
tosimultaneousflow),particularlywithspecificfluctuationsobservedinthelefttemporallobecortex[65].The
individualflowexperienceiscloselyrelatedtotheactivationlevelofthefrontallobe[15].Asscientificknowledge
oftheneuralmechanismsofsimultaneousflowisscarce,ourstudyinitiallyusedthecross-correlationcoefficient
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:21
to measure the signal correlation between participants and Dynamic Time Warping (DTW) to quantify the
similaritybetweentwosignals.Althoughthesefeaturesmaynotfullycaptureallaspectsofinter-brainsynchrony,
theyserveasafirstattempt.
6.2.2 Theinter-brainsynchronyfeaturesweextractediseffective. Tovalidatetheeffectivenessofthesefeatures,
weemployedeightcommonlyusedclassifiersandneuralnetworks.Ourcomparisonexperimentsacrossbinary
andternaryclassificationtasksdemonstratedthattheinclusionofthepreliminaryinter-brainsynchronyfeatures
improvedtheperformanceofalleightmodelssignificantly.Furthermore,synchronyfeatureswereprominently
representedamongthetop-20featuresinfeatureimportanceexperiments,suggestingtheirimportance.Notably,
performanceenhancementswereobservedinnearlyallmodelswiththeadditionoftheLS,FS,andcombined
LS+FSfeaturesets.Theseresultscollectivelyvalidatedtheeffectivenessofinter-brainsynchronyfeaturesin
detectingsimultaneousflow.
6.3 ClassificationofSimultaneousFlow
6.3.1 Thesimultaneousflowcomputationpipelineproposedinthispaperisfeasible. Weemployedaten-fold
cross-validationmethodtoevaluatetheperformanceofLR,SVM,DT,RF,NN,DNN1,DNN2,andDNN3inbinary
andternaryclassification.Theaccuracyofthesemachinelearningmodelssurpassedtheresultsoftherandom
selection(50%,33.3%).Thissuggeststhefeasibilityofthesimultaneousflowcomputationpipelineproposed
inthisstudy,includingdataacquisition,featureextraction,andtheselectionofmachinelearningmodels.We
validatedtheeffectivenessofinter-brainsynchronyfeaturesinsimultaneousflowcomputationforthefirsttime,
offeringanovelperspectiveforthisfield.
6.3.2 RandomForestexhibitedthebestperformanceinbinaryclassificationtask. Inbinaryclassificationtasks,the
RandomForestmodelincludinginter-brainsynchronyfeaturesexhibitedthebestperformancewithanaccuracy
of83.9%,surpassingDNN3by8.3%.Morecomplicatedmodelsmayencountertheproblemofoverfittingwith
limiteduserdata.Ifalargerdatasamplecouldbecollectedastrainingdata,theDNNmodelscouldpotentially
achievebetterperformance.
6.3.3 DTandRFshowedsignificantimprovementsinexperimentswhenincludingsynchronyfeaturesonbinary
classification(𝑝𝑠 <0.001). DecisionTreesimprovedby13.9%,whileRandomForestenhancedby8.1%.Neural
network-based models like NN and DNN already perform well without the addition of synchrony features,
but their improvement is not as significant as tree models when these features are included. This might be
becauseneuralnetworksusuallyneedtoencodecategoricalfeaturesintonumericalformstoaccommodate
themathematicaloperationsofthenetwork.Suchtransformationscanleadtoinformationloss,whereastree
modelsinherentlysupporttheprocessingofcategoricalfeatures.Treemodelscreatedecisionrulesbysplitting
data,formingdifferentpathsforeachfeature’sdistinctvalues.ThisisalsoevidencedinTable7and8,whereRF
focusedmoreonthesynchronyfeatures.
6.3.4 Ternaryclassificationmetricsgenerallysurpassbinaryones. Ternaryclassificationmetricstypicallysur-
passedthoseofbinaryclassification,Thiscouldbeattributedtothemoredistinctivecategoriesintheternary
classificationtask.Intheternaryclassificationtask,thecategoriesaredefinedasfollows:1)NeitherIndividual
norSimultaneousFlow;2)IndividualbutNoSimultaneousFlow;3)SimultaneousFlow.Thesecategoriesoffer
clearerdistinctionscomparedtothehighandlowsimultaneousflowstatesinbinaryclassification.Specifically,
thecategoryof"individualflowbutnosimultaneousflow"cannotbeeffectivelyidentifiedinbinaryclassification,
andmightbemistakenlyclassifiedasloworhighsimultaneousflow.AsshowninTable8,duetotheclearer
categorydistinctions,theternaryclassificationrequiresabalancebetweenindividualandsynchronyfeatures.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:22 • Zhangetal.
6.3.5 Thefeatureslocatedinthefrontallobearemorepreferredbythemodels. Comparedtothefeaturesofthe
lefttemporallobe(T7,P7),thefeatureslocatedinthefrontallobe(AF3,AF4,F3,F4,F7,F8)aremorepreferredby
themodels.However,thisobservationslightlycontrastswiththetheoryproposedbyShehataetal.[65],which
suggeststhattheactivationofthelefttemporalcortexduringteamflowexperiences.Thisdifferencecanbe
attributedtotaskdesigns:Shehataetal.conductedtheirtasksonthesameiPad,whereasourstudydesigned
differenttasksperformedontwoseparatecomputers.Inourexperiment,bothparticipantsexhibitedhighlevels
ofconcentration,thedegreeofindividualflowisstrongerthanthatofteamflow.Thisalignswiththeopinionof
Bruyaetal.[15]that"individualflowiscloselyrelatedtotheactivationlevelofthefrontallobe."
6.3.6 Fromsimultaneousflowtoteamflow. Simultaneousflowmeansteammembersexperiencingindividual
flowatthesametime.VandenHoutetal.defineteamflowasaconcatenativeexperienceofteammembers’
individualflow.Inourexperiments,bothparticipantsaimedtoachievethecommonteamgoalandoptimize
teamperformance,theyexhibitedhighlevelsofconcentrationinthestateofsimultaneousflow.Wheneach
participantisinindividualflowanddoingtheirowntasksfortheteamperformance,itmightleadtotheshared
teamflowtogether.
6.4 LimitationsandFutureDirections
Thereremainsomelimitationstothisstudythatneedtobeaddressedinfuturestudies.
First,furtherexplorationofsimultaneousflowfeaturesreflectinginter-brainsynchrony.Thisstudyextracted
featuresrelatedtointer-brainsynchronyinteamflow.However,asafirstattempt,thesefeatureswererelatively
simple. Future work could explore a more diverse set of features such as coherence and phase synchrony.
Therefore,ongoingresearchontheneuralmechanismsofsimultaneousflowisrequiredtoprovideamorerobust
neuroscientificbasisforfeatureextraction.
Second,extendingsimultaneousflowinductiontasksanddatasetstolargerteams.Asanexploratorystudy,
thisresearchbeganwiththesmallestscaledyadicteamstoconstructflowtasksandEEGdatasets.Inthefuture,
higherprecisionEEGdeviceswillbeusedtoacquiresignalsanditwillbeimportanttoextendthesetasksand
datasetstolargerteamstoaccommodateawidervarietyofteam-interactionscenarios.
Third,real-timedetectionofsimultaneousflowexperience.ThisstudyfocusedonEEG-basedsimultaneous-flow
detectionmethods.Throughfeatureextractionandtrainingmachinelearningmodels,weachievedthedetection
ofsimultaneousflowexperience.However,thismethoddoesnotcurrentlyprovideareal-timeevaluationofflow
experience.Basedonthisstudy,futureworkcouldbeexpectedtoachieveareal-timedetectionofsimultaneous
flowexperiencesforevaluationandhuman-computerinteractionpurposes.
Finally,experiencingflowsimultaneouslyisfoundationaltoachievingsharedteamflow.Simultaneousflowis
animportantstepbeforefocusingonteamflow.Futureworkwillexplorehowtoinduceanddetectteamflowin
differenttasks,potentiallyenhancingteamperformanceinmulti-usercollaborativesystems.
7 CONCLUSION
ThisstudyexploredmethodsfordetectingsimultaneousflowbasedonEEGsignals.Themainconclusionsofthis
studyareasfollows.
First,wedesignedatwo-usercollaborativeexperimentaltasktoeffectivelyinducesimultaneousflow.Based
onthistask,werecordedtheuser’seight-channelEEGsignals(F3,F4,F7,F8,AF3,AF4,T7,andP7)fromthe
frontallobebrainregions(relatedtoindividualflow)andlefttemporallobebrainregions(relatedtointer-brain
synchronyofteamflow)andthenconstructedthefirsteffectiveEEGdatasetwithmultiplelabelsofsimultaneous
flow.
Second,weextractedspecificfeaturesrelevanttoindividualflowandinter-brainsynchrony,andprovedthat
usingthemcaneffectivelydetectingdifferenttypesofsimultaneousflow.Particularly,Wepreliminarilyproposed
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:23
inter-brain synchrony features relevant to simultaneous flow from EEG signals for the first time, including
Cross-CorrelationCoefficientsandDynamicTimeWarpingDistance.Basedonvariousmachinelearningmodels,
wevalidatedtheireffectivenessinenhancingthemachinelearningmodels’performanceinbinaryandternary
classificationtasks.
Third,Wecomparedthemetricsondifferentmodelsandconductedathoroughanalysisoftheexperiments.We
rankedtheextractedfeaturesintermsofimportancebyusingLR,RF,NNandDNN3andconductedaablation
study.Basedontheresultsoftheseexperiments,wefoundtheinter-brainsynchronyfeatureshashighimportance
insimultaneousflowdetectionandthefeaturesfromfrontallobeareaaregivenpriorityattentionbythemodels.
REFERENCES
[1] RamiAlazrai,RashaHomoud,HishamAlwanni,andMohammadIDaoud.2018. EEG-basedemotionrecognitionusingquadratic
time-frequencydistribution.Sensors18,8(2018),2739.
[2] KhalilAlSharabi,YasserBinSalamah,AkramMAbdurraqeeb,MajidAljalal,andFahdAAlturki.2022. EEGsignalprocessingfor
Alzheimer’sdisordersusingdiscretewavelettransformandmachinelearningapproaches.IEEEAccess10(2022),89781–89797.
[3] OmarAlZoubi,SidneyKD’Mello,andRafaelACalvo.2012.Detectingnaturalisticexpressionsofnonbasicaffectusingphysiological
signals.IEEETransactionsonaffectivecomputing3,3(2012),298–310.
[4] ZhongliBai,ZeyuLi,ZhiweiLi,YuSong,QiangGao,andZeminMao.2023.Domain-AdaptiveEmotionRecognitionBasedonHorizontal
VerticalFlowRepresentationofEEGSignals.IEEEAccess11(2023),55023–55034. https://doi.org/10.1109/ACCESS.2023.3270977
[5] IleneRBerson,MichaelJBerson,AmyMCarnes,andClaudiaRWiedeman.2018.Excursionintoempathy:exploringprejudicewith
virtualreality.SocialEducation82,2(2018),96–100.
[6] RiccardoBerta,FrancescoBellotti,AlessandroDeGloria,DanuPranantha,andCarlottaSchatten.2013. Electroencephalogramand
physiologicalsignalanalysisforassessingflowingames.IEEETransactionsonComputationalIntelligenceandAIinGames5,2(2013),
164–175.
[7] YulongBian,ChengleiYang,FengqiangGao,HuiyuLi,ShishengZhou,HanchaoLi,XiaowenSun,andXiangxuMeng.2016.Aframework
forphysiologicalindicatorsofflowinVRgames:constructionandpreliminaryevaluation.PersonalandUbiquitousComputing20(2016),
821–832.
[8] YulongBian,ChengleiYang,ChaoZhou,JuanLiu,WeiGai,XiangxuMeng,FengTian,andChiaShen.2018. ExploringtheWeak
AssociationbetweenFlowExperienceandPerformanceinVirtualEnvironments.InProceedingsofthe2018CHIConferenceonHuman
FactorsinComputingSystems(MontrealQC,Canada)(CHI’18).AssociationforComputingMachinery,NewYork,NY,USA,1–12.
https://doi.org/10.1145/3173574.3173975
[9] YulongBian,ChaoZhou,JuanLiu,WenxiuGeng,andYingShi.2022.Theeffectofreducingdistractionontheflow-performancelinkin
virtualexperientiallearningenvironment.Virtualreality26,4(2022),1277–1290.
[10] MarioBoot,LauraKahnt,DeesPostma,MehmetBaranUlak,KarstGeurs,andPaulHavinga.2024. AreWeinFlow?Measuring
andSupportingSimultaneousFlowinDuosofElderlyCyclists.In2024IEEEInternationalConferenceonPervasiveComputingand
CommunicationsWorkshopsandotherAffiliatedEvents(PerComWorkshops).IEEE,255–260.
[11] StéphaneBouchardandAARizzo.2019.Virtualrealityforpsychologicalandneurocognitiveinterventions.Springer.
[12] AysunBozanta,BirgulKutlu,NuketNowlan,andShervinShirmohammadi.2016.Effectsofseriousgamesonperceivedteamcohesiveness
inamulti-uservirtualenvironment.Computersinhumanbehavior59(2016),380–388.
[13] LeoBreiman.2001.Randomforests.Machinelearning45(2001),5–32.
[14] LeoBreiman.2017.Classificationandregressiontrees.Routledge.
[15] BrianBruya.2010.Effortlessattention:Anewperspectiveinthecognitivescienceofattentionandaction.MITPress.
[16] HenryCandra,MitchellYuwono,RifaiChai,ArdiHandojoseno,IrraivanElamvazuthi,HungTNguyen,andStevenSu.2015.Investigation
ofwindowsizeinclassificationofEEG-emotionsignalwithwaveletentropyandsupportvectormachine.In201537thAnnualinternational
conferenceoftheIEEEEngineeringinMedicineandBiologySociety(EMBC).IEEE,7250–7253.
[17] DebatriChatterjee,AniruddhaSinha,MeghamalaSinha,andSanjoyKumarSaha.2016.AProbabilisticApproachforDetectionand
AnalysisofCognitiveFlow..InBMA@UAI.44–53.
[18] ImanChatterjee,MajaGoršič,MohammadSHossain,JoshuaDClapp,andVesnaDNovak.2023.Automatedclassificationofdyadic
conversationscenariosusingautonomicnervoussystemresponses.IEEETransactionsonAffectiveComputing(2023).
[19] NiteshVChawla,KevinWBowyer,LawrenceOHall,andWPhilipKegelmeyer.2002. SMOTE:syntheticminorityover-sampling
technique.Journalofartificialintelligenceresearch16(2002),321–357.
[20] ManuelCherep,MikolajKegler,Jean-PhilippeThiran,andPabloMainar.2022.MentalFlowEstimationthroughWearableEEG.In2022
44thAnnualInternationalConferenceoftheIEEEEngineeringinMedicine&BiologySociety(EMBC).IEEE,4672–4678.
[21] MihalyCsikszentmihalyi.2000.Beyondboredomandanxiety.Jossey-bass.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:24 • Zhangetal.
[22] MihalyCsikszentmihalyi.2008.Flow:thepsychologyofoptimalexperience.Flow:thepsychologyofoptimalexperience.
[23] MihalyCsikszentmihalyiandMihalyCsikzentmihaly.1990.Flow:Thepsychologyofoptimalexperience.Vol.1990.Harper&RowNew
York.
[24] MihalyCsikszentmihalyi,ReedLarson,andMihalyCsikszentmihalyi.2014.Theexperiencesamplingmethod.Flowandthefoundations
ofpositivepsychology:ThecollectedworksofMihalyCsikszentmihalyi(2014),21–34.
[25] AliDarziandDomenNovak.2021.Automatedaffectclassificationandtaskdifficultyadaptationinacompetitivescenariobasedon
physiologicallinkage:Anexploratorystudy.InternationalJournalofHuman-ComputerStudies153(2021),102673.
[26] EgonDejonckheere,FebeDemeyer,BirteGeusens,MaartenPiot,FrancisTuerlinckx,StijnVerdonck,andMerijnMestdagh.2022.
Assessingthereliabilityofsingle-itemmomentaryaffectivemeasurementsinexperiencesampling.PsychologicalAssessment34,12
(2022),1138.
[27] YueDing,XinHu,ZhenyiXia,Yong-JinLiu,andDanZhang.2018. Inter-brainEEGfeatureextractionandanalysisforcontinuous
implicitemotiontaggingduringvideowatching.IEEETransactionsonAffectiveComputing12,1(2018),92–102.
[28] Ruo-NanDuan,Jia-YiZhu,andBao-LiangLu.2013. DifferentialentropyfeatureforEEG-basedemotionclassification.In20136th
InternationalIEEE/EMBSConferenceonNeuralEngineering(NER).IEEE,81–84.
[29] ShkurtaGashi,ElenaDiLascio,andSilviaSantini.2018.UsingStudents’PhysiologicalSynchronytoQuantifytheClassroomEmotional
Climate.InProceedingsofthe2018ACMInternationalJointConferenceand2018InternationalSymposiumonPervasiveandUbiquitous
ComputingandWearableComputers. https://doi.org/10.1145/3267305.3267693
[30] LászlóHarmat,ÖrjandeManzano,TöresTheorell,LennartHögman,HåkanFischer,andFredrikUllén.2015.Physiologicalcorrelatesof
theflowexperienceduringcomputergameplaying.InternationalJournalofPsychophysiology97,1(2015),1–7.
[31] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition.770–778.
[32] JavierHernandez,IvanRiobo,AgataRozga,GregoryD.Abowd,andRosalindW.Picard.2014.Usingelectrodermalactivitytorecognize
easeofengagementinchildrenduringsocialinteractions.InProceedingsofthe2014ACMInternationalJointConferenceonPervasiveand
UbiquitousComputing. https://doi.org/10.1145/2632048.2636065
[33] JavierHernandez,IvanRiobo,AgataRozga,GregoryDAbowd,andRosalindWPicard.2014.Usingelectrodermalactivitytorecognize
easeofengagementinchildrenduringsocialinteractions.InProceedingsofthe2014ACMinternationaljointconferenceonpervasiveand
ubiquitouscomputing.307–317.
[34] GeoffreyE.Hinton,SimonOsindero,andYee-WhyeTeh.2006.AFastLearningAlgorithmforDeepBeliefNets.NeuralComputation(Jul
2006),1527–1554. https://doi.org/10.1162/neco.2006.18.7.1527
[35] SergeyIoffeandChristianSzegedy.2015.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.
InInternationalconferenceonmachinelearning.pmlr,448–456.
[36] SusanA.JacksonandHerbertW.Marsh.1996.Developmentandvalidationofascaletomeasureoptimalexperience:theFlowState
Scale.JournalofSport&ExercisePsychology18,1(1996),17–35.
[37] KenjiKatahira,YoichiYamazaki,ChiakiYamaoka,HiroakiOzaki,SayakaNakagawa,andNorikoNagata.2018.EEGcorrelatesofthe
flowstate:Acombinationofincreasedfrontalthetaandmoderatefrontocentralalpharhythminthementalarithmetictask.Frontiersin
psychology9(2018),300.
[38] StamosKatsigiannisandNaeemRamzan.2018.DREAMER:ADatabaseforEmotionRecognitionThroughEEGandECGSignalsFrom
WirelessLow-costOff-the-ShelfDevices.IEEEJournalofBiomedicalandHealthInformatics(Jan2018),98–107. https://doi.org/10.1109/
jbhi.2017.2688239
[39] MarkJ.Keith,GregAnderson,JamesGaskin,andDouglasL.Dean.2018. TeamVideoGamingforTeamBuilding:EffectsonTeam
Performance.AssociationforInformationSystemstransactionsonhuman-computerinteraction10,4(2018),205.
[40] KristianKiili,AnteroLindstedt,AnttiKoskinen,HilmaHalme,ManuelNinaus,andJakeMcMullen.2021.Flowexperienceandsituational
interestingame-basedlearning:Cousinsoridenticaltwins.InternationalJournalofSeriousGames8,3(2021),93–114.
[41] DiederikP.KingmaandJimmyBa.2014.Adam:AMethodforStochasticOptimization.arXiv:Learning,arXiv:Learning(Dec2014).
[42] XiangLi,DaweiSong,PengZhang,YazhouZhang,YuexianHou,andBinHu.2018.ExploringEEGfeaturesincross-subjectemotion
recognition.Frontiersinneuroscience12(2018),162.
[43] Yong-JinLiu,MinjingYu,GuozhenZhao,JinjingSong,YanGe,andYuanchunShi.2018.Real-TimeMovie-InducedDiscreteEmotion
RecognitionfromEEGSignals.IEEETransactionsonAffectiveComputing9,4(2018),550–562. https://doi.org/10.1109/TAFFC.2017.2660485
[44] ScottMLundbergandSu-InLee.2017.Aunifiedapproachtointerpretingmodelpredictions.Advancesinneuralinformationprocessing
systems30(2017).
[45] MarcoMaier,DanielElsner,ChadlyMarouane,MeikeZehnle,andChristophFuchs.2019.DeepFlow:DetectingOptimalUserExperience
FromPhysiologicalDataUsingDeepNeuralNetworks..InAAMAS.2108–2110.
[46] StephaneGMallat.1989.Atheoryformultiresolutionsignaldecomposition:thewaveletrepresentation.IEEEtransactionsonpattern
analysisandmachineintelligence11,7(1989),674–693.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:25
[47] TimothyMcMahan,IanParberry,andThomasD.Parsons.2015. EvaluatingPlayerTaskEngagementandArousalUsingElectroen-
cephalography.Procediamanufacturing3(2015),2303–2310.
[48] MichalMuszynski,TheodorosKostoulas,PatriziaLombardo,ThierryPun,andGuillaumeChanel.2018.Aesthetichighlightdetectionin
moviesbasedonsynchronizationofspectators’reactions.
[49] LennartENackeandCraigALindley.2010.Affectiveludology,flowandimmersioninafirst-personshooter:Measurementofplayer
experience.arXivpreprintarXiv:1004.0248(2010).
[50] VinodNairandGeoffreyEHinton.2010. Rectifiedlinearunitsimproverestrictedboltzmannmachines.InProceedingsofthe27th
internationalconferenceonmachinelearning(ICML-10).807–814.
[51] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,Natalia
Gimelshein,LucaAntiga,AlbanDesmaison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,AlykhanTejani,Sasank
Chilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumithChintala.2019.PyTorch:AnImperativeStyle,High-PerformanceDeep
LearningLibrary.InAdvancesinNeuralInformationProcessingSystems32,H.Wallach,H.Larochelle,A.Beygelzimer,F.d’AlchéBuc,
E.Fox,andR.Garnett(Eds.).CurranAssociates,Inc.,8024–8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-
high-performance-deep-learning-library.pdf
[52] FabianPedregosa,GaëlVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,OlivierGrisel,MathieuBlondel,Peter
Prettenhofer,RonWeiss,VincentDubourg,etal.2011. Scikit-learn:MachinelearninginPython. theJournalofmachineLearning
research12(2011),2825–2830.
[53] CorinnaPeiferandStefanEngeser.2021.Advancesinflowresearch.Springer.
[54] ArttuPerttula,KristianKiili,AnteroLindstedt,andPauliinaTuomi.2017.Flowexperienceingamebasedlearning–asystematicliterature
review.(2017).
[55] PanagiotisCPetrantonakisandLeontiosJHadjileontiadis.2009.EmotionrecognitionfromEEGusinghigherordercrossings.IEEE
TransactionsoninformationTechnologyinBiomedicine14,2(2009),186–197.
[56] TrungDuyPhamandDatTran.2012.EmotionRecognitionUsingtheEmotivEPOCDevice.394–399. https://doi.org/10.1007/978-3-642-
34500-5_47
[57] APlotnikov,NStakheika,AlessandroDeGloria,CarlottaSchatten,FrancescoBellotti,RiccardoBerta,CFiorini,andFlavioAnsovini.
2012.Exploitingreal-timeEEGanalysisforassessingflowingames.In2012IEEE12thInternationalConferenceonAdvancedLearning
Technologies.IEEE,688–689.
[58] RyanW.Quinn.2003.Nuclearweaponsanddailydeadlines:Theenergyandtensionofflowinknowledgework.(2003).
[59] RaphaelRissler,MarioNadj,MaximilianXilingLi,NicoLoewe,MichaelTKnierim,andAlexanderMaedche.2020.Tobeornottobein
flowatwork:physiologicalclassificationofflowusingmachinelearning.IEEEtransactionsonaffectivecomputing(2020).
[60] GabrielaRodríguez-Aflecht,TomiJaakkola,NonmanutPongsakdi,MinnaHannula-Sormunen,BoglárkaBrezovszky,andErnoLehtinen.
2018.Thedevelopmentofsituationalinterestduringadigitalmathematicsgame.JournalofComputerAssistedLearning34,3(2018),
259–268.
[61] MiiaRonimus,JanneKujala,AskoTolvanen,andHeikkiLyytinen.2014.Children’sengagementduringdigitalgame-basedlearningof
reading:Theeffectsoftime,rewards,andchallenge.Computers&Education71(2014),237–246.
[62] JeromeIRotgansandHenkGSchmidt.2018. Howindividualinterestinfluencessituationalinterestandhowbotharerelatedto
knowledgeacquisition:Amicroanalyticalinvestigation.TheJournalofEducationalResearch111,5(2018),530–540.
[63] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.1986.Learningrepresentationsbyback-propagatingerrors.nature323,
6088(1986),533–536.
[64] R.KeithSawyer.2016.Groupcreativity:musicalperformanceandcollaboration.PsychologyofMusic34,2(2016),148–165.
[65] MohammadShehata,MiaoCheng,AngusLeung,NaotsuguTsuchiya,Daw-AnWu,Chia-hueiTseng,ShigekiNakauchi,andShinsuke
Shimojo.2021.Teamflowisauniquebrainstateassociatedwithenhancedinformationintegrationandinterbrainsynchrony.eneuro8,
5(2021).
[66] AniruddhaSinha,RahulGavas,DebatriChatterjee,RajatDas,andArijitSinharay.2015.Dynamicassessmentoflearners’mentalstate
foranimprovedlearningexperience.In2015IEEEfrontiersineducationconference(FIE).IEEE,1–9.
[67] NishantSinha,TomaszMaszczyk,ZhangWanxuan,JonathanTan,andJustinDauwels.2016.EEGhyperscanningstudyofinter-brain
synchronyduringcooperativeandcompetitiveinteraction.In2016IEEEinternationalconferenceonsystems,man,andcybernetics(SMC).
IEEE,004813–004818.
[68] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.2014.Dropout:asimplewaytoprevent
neuralnetworksfromoverfitting.Thejournalofmachinelearningresearch15,1(2014),1929–1958.
[69] PenelopeSweetserandPetaWyeth.2005.GameFlow:amodelforevaluatingplayerenjoymentingames.ComputersinEntertainment
(CIE)3,3(2005),3–3.
[70] GrantS.TaylorandChristinaSchmidt.2012.EmpiricalEvaluationoftheEmotivEPOCBCIHeadsetfortheDetectionofMentalActions.
ProceedingsoftheHumanFactorsandErgonomicsSocietyAnnualMeeting(Sep2012),193–197. https://doi.org/10.1177/1071181312561017
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:26 • Zhangetal.
[71] J.J.JVan,denHout,O.CDavis,andBWalrave.2016. Theapplicationofteamflowtheory. FlowExperienceEmpiricalResearch&
Applications(2016).
[72] JefJJvandenHout,OrinCDavis,andMathieuCDPWeggeman.2018.Theconceptualizationofteamflow.TheJournalofpsychology
152,6(2018),388–423.
[73] JefJ.JvandenHoutandOrinC.Davis.2019.TeamFlow:ThePsychologyofOptimalCollaboration.SpringerInternationalPublishing
AG,Cham.
[74] KevinJVerdière,FrédéricDehais,andRaphaëlleNRoy.2019. SpectralEEG-basedclassificationforoperatordyads’workloadand
cooperationlevelestimation.In2019IEEEInternationalConferenceonSystems,ManandCybernetics(SMC).IEEE,3919–3924.
[75] KalyaniPWaghandKVasanth.2022.Performanceevaluationofmulti-channelelectroencephalogramsignal(EEG)basedtimefrequency
analysisforhumanemotionrecognition.BiomedicalSignalProcessingandControl78(2022),103966.
[76] Chih-ChienWangandMing-ChangHsu.2014.Anexploratorystudyusinginexpensiveelectroencephalography(EEG)tounderstand
flowexperienceincomputer-basedinstruction.Information&Management51,7(2014),912–923.
[77] PeterWelch.1967.TheuseoffastFouriertransformfortheestimationofpowerspectra:amethodbasedontimeaveragingovershort,
modifiedperiodograms.IEEETransactionsonaudioandelectroacoustics15,2(1967),70–73.
[78] PaulT.P.Wong.2012.Flourishing:AVisionaryNewUnderstandingofHappinessandWell-Being,ByMartinE.P.Seligman.1(2012).
[79] XiaozhenYe,HuanshengNing,PerBacklund,andJianguoDing.2020. Flowexperiencedetectionandanalysisforgameusersby
wearable-devices-basedphysiologicalresponsescapture.IEEEInternetofThingsJournal8,3(2020),1373–1387.
[80] KyongsikYun,KatsumiWatanabe,andShinsukeShimojo.2012.Interpersonalbodyandneuralsynchronizationasamarkerofimplicit
socialinteraction.ScientificReports(Dec2012). https://doi.org/10.1038/srep00959
[81] GuanhuaZhang,MJYu,GuoChen,YihengHan,DanZhang,GZZhao,andYong-JinLiu.2019.AreviewofEEGfeaturesforemotion
recognition.Scientiasinicainformationis49,9(2019),1097–1118.
[82] GuanhuaZhang,MinjingYu,Yong-JinLiu,GuozhenZhao,DanZhang,andWenmingZheng.2023.SparseDGCNN:RecognizingEmotion
FromMultichannelEEGSignals.IEEETransactionsonAffectiveComputing14,1(2023),537–548. https://doi.org/10.1109/TAFFC.2021.
3051332
A APPENDIX
A.1 Metrics
Metrics Description Definition
BinaryClassificationMetrics
Accuracy The proportion of true results among Accuracy= 𝑇𝑃+𝑇𝑁
𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁
thetotalnumberofcasesexamined.
Precision The proportion of positive identifica- Precision= 𝑇𝑃
𝑇𝑃+𝐹𝑃
tionsthatwereactuallycorrect.
Recall Theproportionofactualpositivesthat Recall= 𝑇𝑃
𝑇𝑃+𝐹𝑁
werecorrectlyidentified.
F1Score Theharmonicmeanofprecisionandre- F1Score=2× 𝑃𝑟𝑒×𝑅𝑒𝑐
𝑃𝑟𝑒+𝑅𝑒𝑐
call,providingabalancebetweenthem.
TernaryClassificationMetrics
Accuracy The proportion of true results for a Accuracy=
(cid:205) 𝑖3 =1𝑇𝑃𝑖+(cid:205) 𝑖3 =1𝑇𝑁𝑖
𝑁
three-classclassificationproblem.
(cid:16) (cid:17)
Precision Theaverageproportionofcorrectposi- Precision= 1 𝑇𝑃 1 + 𝑇𝑃 2 + 𝑇𝑃 3
3 𝑇𝑃 1+𝐹𝑃 1 𝑇𝑃 2+𝐹𝑃 2 𝑇𝑃 3+𝐹𝑃 3
tivepredictionsforeachclass.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:27
Metrics Description Definition
(cid:16) (cid:17)
Recall The average proportion of correctly Recall= 1 𝑇𝑃 1 + 𝑇𝑃 2 + 𝑇𝑃 3
3 𝑇𝑃 1+𝐹𝑁 1 𝑇𝑃 2+𝐹𝑁 2 𝑇𝑃 3+𝐹𝑁 3
identifiedpositivesforeachclass.
F1Score The harmonic mean of precision and F1Score=2× 𝑃𝑟𝑒×𝑅𝑒𝑐
𝑃𝑟𝑒+𝑅𝑒𝑐
recallforternaryclassification.
Table1. EvaluationMetricsandCalculationMethods
𝑇𝑃 isthenumberoftruepositives.𝐹𝑃 isthenumberoffalsepositives.𝑇𝑁 isthenumberoftruenegatives.𝐹𝑁
isthenumberoffalsenegatives.
A.2 Features
FeatureCategory Description Definition
Mean ThemeanasafeatureoftheEEG 𝜇 𝑠 = 𝑇1 (cid:205)𝑇 𝑛=1𝑠(𝑛)
signal reflects the average level
of electrical potential over a pe-
riodoftime,revealingthebaseline
stateofbrainactivity.
√︃
StandardDeviation The standard deviation as a fea- 𝜎 𝑠 = 𝑇1 (cid:205)𝑇 𝑛=1(𝑠(𝑛)−𝜇 𝑠)2
ture of the EEG signal indicates
the dispersion of signal values,
quantifyingthevariabilityorsta-
bilityofbrainactivity.
Variance[81] Thevariancerepresentstherange Var𝑠 = 𝑇1 (cid:205)𝑇 𝑛=1(𝑠(𝑛)−𝜇 𝑠)2
of fluctuation of the EEG signal
andalsothepowerofthesignal
todeviatefromthemeanvalue.
AverageAbsolute Theaverageabsolutevalueofthe 𝛿 𝑠 = 𝑇1 −1(cid:205)𝑇 𝑛=− 11|𝑠(𝑛+1)−𝑠(𝑛)|
First-OrderDifference[81] first-orderdifferencemeasuresthe
rate of change of the EEG sig-
nalovertime,helpingtocapture
short-termfluctuationswithinthe
signal.
NormalizedFirst-OrderDifference[81] Thenormalizedfirst-orderdiffer- 𝛿 𝑠 = 𝜎𝛿𝑠
𝑠
ence eliminates the influence of
signal strength, making the fea-
turesmorecomparableunderdif-
ferentconditions.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:28 • Zhangetal.
FeatureCategory Description Definition
Energy[33] Fluctuationsinenergycanreflect 𝐸 𝑠 =(cid:205)𝑇 𝑛=1𝑠(𝑛)2
thelevelofactivityinthecerebral
cortex, often represented by the
squareofthetime-domainsignal.
Power[81] The power of the EEG signal is 𝑃 𝑠 = 𝑇1 (cid:205)𝑇 𝑛=1𝑠(𝑛)2
obtained by dividing the energy
bythenumberofsamples.
HjorthParameters[66] Activitymeasuresthedeviationof Activity=𝜎 𝑠2
theamplitude,Mobilitymeasures Mobility=
𝜎𝑓
𝜎𝑠
thechangeinslope.
Higher-OrderZero-Crossing[55,81] Higher-order zero-crossing re- By using the mean 𝜇 𝑠, transform the
flects the oscillation level of the EEGsignalintoazero-meansequence
EEGsignal,indicatingthefluctua- 𝑠 new(𝑡) =𝑠(𝑡)−𝜇 𝑠.
tionfeatures. 𝐻 𝑠 =(cid:205)𝑇 𝑛=− 11(𝑥 𝑠(𝑡 +1)−𝑥 𝑠(𝑡))2
Peak-to-PeakMean[42] Peak-to-peak mean is the arith- ppm =
max𝑡𝑖∈𝑡𝑠(𝑡𝑖)−min𝑡𝑖∈𝑡𝑠(𝑡𝑖)
𝑠 𝑇
metic mean of the distance be-
tween the maximum and mini-
mum values in a time-series sig-
nal.
Kurtosis[2,75] Kurtosisreflectsthefrequencyof Kur𝑠 =
𝐸(𝑠(𝑡 𝜎) 𝑠4−𝜇𝑠)4
peakvaluesappearingaswellas
thewaveformfeaturesofthesig-
nal.
(cid:16) (cid:17)
LogarithmicBandPower[4] Thelogarithmicbandpowercon- LBP𝑠 =log 𝑇1 (cid:205)𝑇 𝑛=1𝑠(𝑛)2
vertsthefrequencyinformationof
EEGsignalsintoanenergydistri-
butiononalogarithmicscale.
DifferentialEntropy[4,28] Differentialentropyisoftenused DE𝑠 = 1 2log(cid:0) 2𝜋𝑒𝜎 𝑠2(cid:1)
to represent the complexity of a
timeseries.
PowerSpectralDensity[1,4] The power spectral density de- PSD𝑠 = 1 2log(cid:0) 2𝜋𝑒𝜎 𝑠2(cid:1)
scribeshowthepowerofasignal
varieswithfrequency.
Cross-CorrelationCoefficient TheCross-CorrelationCoefficient 𝜌 𝐴,𝐵 = co 𝜎v 𝐴(𝐴 𝜎𝐵,𝐵)
isusedtomeasurethesimilarity
betweentwotimeseriesatdiffer-
enttimelags
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:29
FeatureCategory Description Definition
DynamicTimeWarpingDistance TheDynamicTimeWarpingDis- 𝐷(𝑎 𝑖,𝑏 𝑗) = Dist(𝑎 𝑖,𝑏 𝑗) +
tanceisusedtomeasurethesimi- min{𝐷(𝑎 𝑖−1,𝑏 𝑗),𝐷(𝑎 𝑖,𝑏 𝑗−1),𝐷(𝑎 𝑖−1,𝑏 𝑗−1)}
laritybetweentwotimeseries.
Table2. FeaturesExtractedforIndividualandTeamFlow
A.3 DetailsofImplementation
Model Description Configuration
LogisticRegression(LR) LogisticRegressionusesalinearfunc- Feature standardization was
tion of input features𝑥 to predict the performed using the Standard-
label𝑦.Thelabel𝑦canbeeitherbinary Scaler parameter, followed by
ormulti-class.Inbinaryclassification, the sklearn’s LogisticRegression
the model predicts the probability of model. The maximum number
the label belonging to one of two cat- of iterations, max_iter, was
egories.Thisisusuallyrepresentedas set to 1000. For the ternary
𝑦ˆ = 1 ,where𝑒 isthebaseofthe classification task, we speci-
1+𝑒−𝑤𝑇𝑥
naturallogarithm,and𝑤 representsthe fied multi_class=’multinomial’,
modelparameters.Inmulti-classscenar- enabling the model to handle
ios,logisticregressiontypicallyusesthe multi-classproblems.
softmaxfunctiontohandlemultiplecat-
egories and predict the probability of
each category. The goal of the model
is to estimate parameters 𝑤 by maxi-
mizingthelikelihoodfunction,usually
achievedbyminimizingtheloglossof
thetrainingdata.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:30 • Zhangetal.
Model Description Configuration
SupportVectorMachine(SVM) SVM classification models predict the The SVM model is configured
label𝑦ˆ=𝑤𝑇𝑥+𝑏bysolvingthefollow-
with C=1.0, kernel=’rbf’, and
ingoptimizationproblem:min𝑤 1∥𝑤∥2 gamma=’scale’. The C parame-
2
subject to the classification ter controls the penalty for mis-
constraint:𝑦 (𝑤𝑇𝑥 +𝑏) ≥ 1−𝜉 classification, and kernel speci-
train train
where𝑥 representstheinputfeatures, fies the kernel type. The value
𝑤 is the weight vector, 𝑏 is the bias ’scale’ means that gamma is set
term,and𝜉 areslackvariablesallowing to 1 ,whereXisthe
𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠∗𝑋.𝑣𝑎𝑟()
for some data points to violate the feature data. This setting takes
margin.Forbinaryclassificationtasks, into account the variance of fea-
the model output 𝑦ˆ is based on the tures and automatically adjusts
sign function determining to which thegammavalue.
category a data point belongs. In
ternaryormulti-classscenarios,SVM
can be extended to multi-category
classification by using one-vs-rest or
one-vs-onestrategies.
DecisionTree(DT) The Decision Tree model we used is The Decision Tree model is con-
basedonClassficationandRegression figuredwithmin_samples_split=4.
Tree(CART)[14].Abinarytreeiscon- Themin_samples_splitparameter
structed by iteratively splitting data specifiestheminimumnumberof
basedonfeaturethresholdsthatmaxi- samplesrequiredtosplitaninter-
mizeclassseparation. nalnode.Thesplittingstopswhen
thenumberofsamplesislessthan
min_samples_split.
RandomForest(RF) Random Forest[13] is an ensemble of TheRandomForestmodeliscon-
Decision Trees. It trains each tree on figured with max_features=0.3,
a random subset of the training data bootstrap=ture,max_samples=0.8
and uses a random subset of features and min_samples_leaf=3. The
foreachsplit.Thefinalpredictionisthe max_featuresparameterspecifies
averagepredictionofalltrees. thenumberoffeaturestoconsider
when looking for the best split.
Thebootstrapparameterspecifies
whether bootstrap samples
are used when building trees.
The max_samples parameter
specifies the number of samples
todrawfromXtotraineachbase
estimator.Themin_samples_leaf
parameterspecifiestheminimum
numberofsamplesrequiredtobe
ataleafnode.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:31
Model Description Configuration
NeuralNetwork(NN)(1-hidden) NeuralNetwork[63]isamodelinspired The NN includes a single hid-
by the biological neural networks. It den layer with 100 neurons, us-
consistsofmultiplelayersofneurons. ing ReLU function[50]. Batch
Each neuron is a linear function of normalization[35] is applied to
the input features followed by a non- the inputs of each layer, and
linearactivationfunction.Theoutput dropout[68]issetatarateof0.1.
ofthenetworkistheoutputofthelast The network is optimized using
layer.The output layer consists of an the Adam optimizer[41] with a
affinetransformtopredicttheflowex- learningrateof0.001,anditem-
perience. ploys cross-entropy loss as the
lossfunction.Wesetthebatchsize
to64for10-foldscrossvalidation.
Deep Neural Network1 (DNN1) (5- TheDNNreferstoNeuralNetworkwith In DNN1, we use 5 hidden lay-
hidden) multiplehiddenlayers[34]. ers where each layer consists of
abatchnormalizationlayer,ReLU
activationlayer,anddropoutlayer.
The number of neurons in each
layeris100.Otherconfigurations
arethesameasNN.
DNN2(5-hidden+residual) In Deep Neural Networks, a residual In DNN2, we employ a residual
connection[31]createsashortcutbydi- connection to connect the out-
rectlylinkingtwolayers,therebypro- putofthefirstandfourthlayers.
vidingashorterpathbetweenthem. Otherconfigurationsarethesame
asDNN1.
DNN3(9-hidden+residual) TheDNN3isconfiguredwith9hidden The DNN3 model is configured
layers with9hiddenlayerswithresidual
connectionsthatlinktheoutputof
thefirsthiddenlayertothefourth
andtheoutputofthefifthhidden
layertotheeighth.Otherconfigu-
rationsarethesameasDNN2.
Table3. Machinelearningmodelsandconfigurations
A.4 T-test
A.5 FeatureImportance
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:32 • Zhangetal.
Model PValueforAccuracy PValueforRecall PValueforPrecision PValueforF1Score
LR 0.022 0.024 0.021 0.032
SVM 0.034 0.041 0.036 0.034
DT 1.50×10−6 1.50×10−6 2.15×10−6 1.36×10−6
RF 4.98×10−6 7.70×10−6 6.42×10−6 5.84×10−6
NN 0.003 0.007 0.008 0.004
DNN1 0.021 0.020 0.024 0.015
DNN2 0.042 0.078 0.014 0.087
DNN3 0.023 0.157 0.042 0.175
Table4. BinaryClassificationComparisonofPValuesforDifferentModels
Model PValueforAccuracy PValueforRecall PValueforPrecision PValueforF1Score
LR 0.048 0.045 0.041 0.070
SVM 0.018 0.026 0.031 0.011
DT 0.003 0.002 0.002 0.003
RF 0.004 0.003 0.004 0.003
NN 6.28𝑒×10−8 2.68𝑒×10−9 1.31𝑒×10−8 4.97𝑒×10−8
DNN1 0.006 8.27𝑒×1048 0.002 0.003
DNN2 0.043 0.048 0.010 0.053
DNN3 0.004 0.011 0.005 0.009
Table5. TernaryClassificationComparisonofPValuesforDifferentModels
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:33
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features COEF Top-20features COEF Top-20features COEF Top-20features COEF
P7NFOD -1.78 P7NFOD -1.73 P7SD 1.82 P7SD 1.76
P7Mobility 1.27 F7PSD𝛼 1.25 AF4Mobility -1.28 AF4DTW𝛼 -1.48
F7PSD𝛼 1.25 AF4DTW𝛼 1.13 P7Mobility -1.21 AF4Mobility -1.14
F3NFOD -1.22 F3NFOD -1.09 P7NFOD 1.09 F4DE𝛿 1.10
AF3Mobility 1.06 P7Mobility 1.03 F4DE𝛿 1.08 P7Mobility -1.05
F3Mobility 1.02 AF3Mobility 1.03 F8SD 1.01 AF3DTW𝜃 0.98
T7PSD𝜃 0.89 AF3DTW𝜃 -0.99 F7NFOD 0.95 F7NFOD 0.98
T7Mobility 0.79 F8DTW𝜃 -0.94 P7Variance -0.94 F8SD 0.93
P7Variance 0.78 T7PSD𝜃 0.93 P7Activity -0.94 P7Variance -0.91
P7Activity 0.78 F8DTW𝛼 -0.91 F4Mobility 0.87 P7Activity -0.91
AF3SD -0.77 F4DTW𝛼 -0.88 F8DEFB -0.87 T7DEFB -0.90
AF4Mobility 0.76 F3DTW𝛽 -0.88 T7PPM 0.85 AF4DEFB -0.90
T7SD -0.75 AF3DTW𝛼 0.88 F7Mobility -0.85 P7NFOD 0.87
F7NFOD -0.74 T7SD -0.83 F7SD 0.84 F8DEFB -0.86
P7DEFB -0.70 T7Mobility 0.82 AF4DEFB -0.75 AF4PPM 0.85
T7PSD𝛼 -0.66 AF3DTW𝛿 0.82 F8PSD𝛽 0.75 T7DTW𝛿 -0.83
F8PSD𝛽 -0.60 F7DTW𝛿 -0.81 F4SD -0.74 F7Mobility -0.81
F4AAFOD 0.60 F4DTW𝛽 -0.80 P7DE𝛿 -0.74 F8PSD𝛽 0.79
AF3NFOD -0.56 F3Mobility 0.79 T7DEFB -0.72 F4DTW𝛼 0.79
F8average 0.56 T7PSD𝛼 -0.77 AF4PPM 0.72 AF4DE𝛿 -0.76
Table6. Thetop-20importantfeaturesselectedbysortingtheabsolutesignificantcoefficient(COEF)valuesofLR.Features
inboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincludinginter-brain
synchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:34 • Zhangetal.
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features IMP(%) Top-20features IMP(%) Top-20features IMP(%) Top-20features IMP(%)
F4LBP𝛿 1.30 P7DTW𝛼 3.13 F4PSD𝜃 1.10 F4PSD𝜃 1.38
F4MeanPSD 1.20 AF4CCC𝛼 2.01 P7DE𝜃 1.10 P7DTW𝛼 1.17
AF4Kurtosis 1.15 F7CCC𝛽 1.90 F4Energy 1.03 F4average 1.12
P7DE𝛼 1.13 F4MeanPSD 1.17 F4Power 0.95 AF3Kurtosis 1.01
F8Kurtosis 1.09 F3CCC𝛼 1.07 F4average 0.94 AF3PSD𝛽 1.01
P7LBP𝜃 1.09 F3DTW𝜃 1.06 F4LBP𝛿 0.90 P7DE𝜃 0.94
P7DE𝜃 1.07 F7DTW𝛿 1.02 F4MeanPSD 0.90 F8PPM 0.92
F7PSD𝛼 1.03 F3DTW𝛼 1.01 AF3Kurtosis 0.90 F4LBP𝛿 0.88
P7LBP𝛼 1.01 AF3CCC𝛼 1.01 F8PPM 0.87 P7LBP𝜃 0.79
F4average 0.99 P7CCC𝛼 0.99 AF3PSD𝛽 0.83 F4Energy 0.78
T7PSD𝜃 0.95 AF4CCC𝛽 0.98 P7LBP𝜃 0.80 F8Kurtosis 0.76
T7Kurtosis 0.93 F8CCC𝛼 0.97 F4LBP𝛼 0.80 F4Power 0.75
F4Energy 0.92 F8CCC𝛽 0.90 P7Kurtosis 0.76 P7CCC𝛽 0.70
F8PSD𝛽 0.90 F8CCC𝛿 0.87 AF3LBP𝛽 0.75 AF4CCC𝛼 0.66
F4Power 0.90 F4CCC𝛼 0.87 AF3AAFOD 0.75 F8CCC𝛼 0.65
P7Kurtosis 0.90 AF4CCC𝜃 0.84 F4DE𝛼 0.75 F7PSD𝛽 0.65
F4PSD𝜃 0.88 F7CCC𝛿 0.81 F7LBP𝛼 0.74 F4LBP𝛼 0.64
F4PSD𝛿 0.85 F4average 0.79 AF3DE𝛽 0.72 T7DTW𝜃 0.62
T7HOZC 0.80 F4PSD𝛿 0.78 AF4DE𝛽 0.69 F4DE𝛼 0.62
AF4PSD𝛼 0.80 P7CCC𝜃 0.77 F8Kurtosis 0.69 T7DTW𝛽 0.61
Table7. Thetop-20importantfeaturesselectedbysortingtheabsolutesignificantimportance(IMP)valuesofRF.Features
inboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincludinginter-brain
synchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals • 0:35
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features SHAP Top-20features SHAP Top-20features SHAP Top-20features SHAP
F7PSD𝛼 2.89 F7PSD𝛼 5.34 AF3AAFOD 4.88 P7PSD𝛽 -7.72
AF3Mobility -2.55 F8CCC𝛿 -4.72 F7PSD𝛼 -4.26 F7PSD𝛽 -4.68
T7Activity -2.46 F7PSD𝜃 4.63 AF3PSD𝛽 3.94 F8DTW𝛿 4.63
F7meanPSD -2.23 AF3DTW𝛿 -4.14 AF3DE𝛽 -3.64 P7Kurtosis -4.53
F8Activity 2.20 F8CCC𝛼 -3.50 F4AAFOD -3.11 F8PSD𝜃 4.09
P7DEFB -2.00 F4LBP𝛿 3.44 F8AAFOD -3.11 AF4PSD𝛿 -4.08
F4DE𝛼 -1.67 F4AAFOD -3.38 F3PSD𝛼 3.06 AF3LBP𝛽 -3.97
F7PSD𝜃 1.62 F3DTW𝛿 3.25 P7AAFOD 2.92 AF3PSD𝛽 -3.87
T7Kurtosis -1.62 P7Variance -3.15 AF3Kurtosis 2.91 AF4AAFOD 3.86
T7Mobility -1.52 F4DTW𝜃 -3.08 F7meanPSD -2.86 F4DE𝛿 -3.81
P7AAFOD 1.47 F3CCC𝜃 -2.84 F7PSD𝛽 -2.73 P7HOZC -3.77
F3HOZC -1.33 AF3PPM 2.74 T7DE𝛿 2.61 F4DTW𝛿 -3.76
F8PSD𝛼 -1.19 P7DTW𝛽 2.70 AF4LBP𝛼 -2.47 F3LBP𝜃 3.69
T7PSD𝛽 1.17 T7PSD𝛼 -2.65 AF3PSD𝛼 -2.41 T7PSD𝜃 3.63
F8Variance 1.11 F4LBP𝜃 -2.64 F4LBP𝛼 -2.15 T7DEFB -3.50
AF4Variance 1.10 P7PSD𝛽 -2.61 F8DE𝛽 2.03 F3DE𝜃 3.39
F3NFOD -1.07 F8DTW𝜃 2.57 F4Energy 1.97 T7PSD𝛼 3.26
F7PSD𝛽 1.01 F8PPM 2.51 F3DE𝛽 1.95 AF3DTW𝛼 3.20
F8PSD𝛽 1.01 F4Energy -2.48 F7PSD𝜃 1.93 P7PSD𝛼 3.12
AF4Mobility 1.00 P7PPM -2.40 AF4PPM -1.89 F7CCC𝛿 -3.06
Table8. Thetop-20importantfeaturesselectedbysortingtheabsoluteSHapleyAdditiveexPlanations(SHAP)valuesof
NN.Featuresinboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincluding
inter-brainsynchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.Inbinaryclassificationtasks,the
unitofSHAPvalueis10−9,andinternaryclassificationtasks,theunitofSHAPvalueis10−8.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:36 • Zhangetal.
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features SHAP Top-20features SHAP Top-20features SHAP Top-20features SHAP
P7DEFB 2.21 F7CCC𝛿 1.14 T7Kurtosis -2.13 AF4CCC𝜃 -2.27
F4PSD𝛿 1.92 AF4HOZC -1.12 AF4LBP𝛼 -1.90 P7Kurtosis -2.22
P7DE𝛽 -1.82 F3CCC𝜃 1.08 T7HOZC 1.81 AF3Kurtosis -2.17
F3LBP𝜃 -1.81 T7DE𝛿 1.06 AF4DE𝛼 -1.74 T7LBP𝛽 -1.63
F4LBP𝛽 1.50 AF4CCC𝛽 0.98 F7AAFOD -1.57 F4CCC𝜃 -1.43
F3Activity 1.48 T7PSD𝛽 -0.98 T7Mobility -1.51 F7DE𝛽 1.41
F7Kurtosis 1.43 F4LBP𝛽 0.95 F3LBP𝛽 1.50 F7DEFB -1.39
F3Variance 1.41 F8CCC𝛿 0.93 F7DE𝛼 -1.40 F7Kurtosis -1.38
F4meanPSD 1.37 F7CCC𝛼 -0.91 F7LBP𝛼 -1.37 F4LBP𝛽 -1.30
P7PPM -1.35 AF4Energy -0.90 F7HOZC -1.35 AF3DE𝜃 1.26
F4LBP𝛼 -1.29 F4HOZC -0.88 T7DE𝜃 1.27 F3NOFD -1.25
F3Energy 1.23 T7AAFOD 0.87 T7DE𝛽 1.23 T7CCC𝜃 -1.24
F8DE𝛼 -1.21 AF3DEFB -0.85 F3PSD𝛿 -1.18 P7DEFB 1.19
AF4Energy -1.20 F3DE𝜃 -0.74 F8HOZC 1.18 F4LBP𝜃 -1.16
F3PSD𝛿 1.15 AF3CCC𝜃 -0.74 F4meanPSD -1.16 T7CCC𝛿 1.12
AF4DE𝛿 1.15 AF3PSD𝛿 -0.74 P7Kurtosis 1.14 F3CCC𝛼 -1.11
F3DEFB -1.14 F7DE𝛼 -0.72 AF3HOZC -1.12 F4DTW𝛿 -1.10
F3Mobility 1.11 T7Mobility -0.69 P7LBP𝜃 -1.08 F7CCC𝛿 -1.08
AF3SD -1.10 AF3CCC𝛽 0.67 F3DE𝛼 -1.07 T7LBP𝜃 1.07
F7PPM -1.09 F8Mobility -0.65 F3PSD𝛽 -1.06 T7AAFOD -1.06
Table9. Thetop-20importantfeaturesselectedbysortingtheabsoluteSHapleyAdditiveexPlanations(SHAP)valuesof
DNN3.Featuresinboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincluding
inter-brainsynchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.TheunitofSHAPvalueis10−8
bothinbinaryandternaryclassificationtasks.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.