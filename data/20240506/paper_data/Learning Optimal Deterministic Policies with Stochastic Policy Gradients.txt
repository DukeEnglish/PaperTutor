Learning Optimal Deterministic Policies with Stochastic Policy Gradients
AlessandroMontenegro1 MarcoMussi1 AlbertoMariaMetelli1 MatteoPapini1
Abstract function. Nonetheless, as always in RL, the exploration
Policygradient(PG)methodsaresuccessfulap- problemhastobeaddressed,andpracticalmethodsinvolve
proachestodealwithcontinuousreinforcement injecting noise in the actions or in the parameters. This
learning (RL) problems. They learn stochastic limits the application of PG methods in many real-world
parametric(hyper)policiesbyeitherexploringin scenarios, such as autonomous driving, industrial plants,
thespaceofactionsorinthespaceofparameters. androboticcontrollers. Thisisbecausestochasticpolicies
Stochasticcontrollers,however,areoftenundesir-
typicallydonotmeetthereliability,safety,andtraceability
ablefromapracticalperspectivebecauseoftheir standardsofthiskindofapplications.
lack of robustness, safety, and traceability. In Theproblemoflearningdeterministicpolicieshasbeenex-
commonpractice,stochastic(hyper)policiesare
plicitlyaddressedinthePGliteraturebySilveretal.(2014)
learnedonlytodeploytheirdeterministicversion. with their deterministic policy gradient, which spawned
Inthispaper,wemakeasteptowardsthetheoreti-
verysuccessfuldeepRLalgorithms(Lillicrapetal.,2016;
calunderstandingofthispractice. Afterintroduc-
Fujimotoetal.,2018). Thisapproach,however,isaffected
inganovelframeworkformodelingthisscenario, byseveraldrawbacks,mostlyduetoitsinherentoff-policy
westudytheglobalconvergencetothebestdeter-
nature. First,thismakesDPGhardtoanalyzefromatheo-
ministicpolicy,under(weak)gradientdomination
reticalperspective: localconvergenceguaranteeshavebeen
assumptions. Then,weillustratehowtotunethe
establishedonlyrecently,andonlyunderassumptionsthat
explorationlevelusedforlearningtooptimizethe
areverydemandingfordeterministicpolicies(Xiongetal.,
trade-offbetweenthesamplecomplexityandthe
2022). Furthermore,itspracticalversionsareknowntobe
performanceofthedeployeddeterministicpolicy.
verysusceptiblehyperparametertuning.
Finally,wequantitativelycompareaction-based
andparameter-basedexploration,givingaformal Westudyhereasimplerandfairlycommonapproach: that
guisetointuitiveresults.
oflearningstochasticpolicieswithPGalgorithms,thende-
ployingthecorrespondingdeterministicversion,“switching
off”thenoise.1 Intuitively,theamountofexploration(e.g.,
1.Introduction thevarianceofaGaussianpolicy)shouldbeselectedwisely.
Indeed,thesmallertheexplorationlevel,theclosertheopti-
Withinreinforcementlearning(RL,Sutton&Barto,2018)
mizedobjectiveistothatofadeterministicpolicy. Atthe
approaches,policygradients(PGs,Deisenrothetal.,2013)
sametime,withasmallexploration,learningcanseverely
algorithmshaveprovedveryeffectiveindealingwithreal-
slowdownandgetstuckonbadlocaloptima.
worldcontrolproblems. Theiradvantagesincludetheap-
plicabilitytocontinuousstateandactionspaces(Peters& Policy gradient methods can be partitioned based on the
Schaal,2006),resiliencetosensorandactuatornoise(Grav- spaceonwhichtheexplorationiscarriedout,distinguish-
ellet al.,2020), robustnessto partialobservability(Aziz- ingbetween: action-based(AB)andparameter-based(PB,
zadeneshelietal.,2018),andthepossibilityofincorporating Sehnkeetal.,2010)exploration. Thefirst,ofwhichREIN-
priorknowledgeinthepolicydesignphase(Ghavamzadeh FORCE(Williams,1992)andGPOMDP(Baxter&Bartlett,
&Engel,2006),improvingexplainability(Likmetaetal., 2001;Suttonetal.,1999)aretheprogenitoralgorithms,per-
2020). PG algorithms search directly in a space of para- formsexplorationintheactionspace,withastochastic(e.g.,
metricpoliciesfortheonethatmaximizesaperformance Gaussian) policy. On the other hand, PB exploration, in-
troducedbyParameter-ExploringPolicyGradients(PGPE,
1PolitecnicodiMilano,PiazzaLeonardodaVinci32,20133, Sehnkeetal.,2010),implementstheexplorationatthelevel
Milan, Italy. Correspondence to: Alessandro Montenegro
ofpolicyparametersbymeansofastochastichyperpolicy.
<alessandro.montenegro@polimi.it>.
Thelatterperformsperturbationsoftheparametersofa(typ-
Proceedings of the 41st International Conference on Machine
1Thiscanbeobservedinseverallibraries(e.g.,Raffinetal.,
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
2021b)andbenchmarks(e.g.,Duanetal.,2016).
theauthor(s).
1
4202
yaM
3
]GL.sc[
1v53220.5042:viXraLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
icallydeterministic)actionpolicy. Ofcourse,thisdualism ofnotation,wewillinterchangeablyusex P orx pto
„ „
onlyconsidersthesimplestformofnoise-based,undirected denotethatrandomvariablexissampledfromtheP. For
exploration. Efficientexplorationinlarge-scaleMDPsisa n N,wedenoteby n : 1,...,n .
veryactiveareaofresearch,withalargegapbetweentheory P (cid:74) (cid:75) “t u
Lipschitz Continuous and Smooth Functions. A func-
andpractice(Ghavamzadehetal.,2020)placingthematter
tion f:X Rd R is L-Lipschitz continuous (L-LC) if
wellbeyondthescopeofthispaper.Also,weconsidernoise Ď Ñ
f x f x1 L x x1 for every x,x1 X. f is L -
magnitudes that are fixed during the learning process, as | p q´ p q|ď } ´ }2 P 2
Lipschitz smooth (L -LS) if it is continuously differen-
thecommonpracticeoflearningtheexplorationparameters 2
tiable and its gradient ∇ f is L -LC, i.e., ∇ f x
themselvesbreaksallknownsamplecomplexityguarantees x 2 } x p q´
∇ f x1 L x x1 foreveryx,x1 X.
ofvanillaPG(cf. AppendixC). x p q}2 ď 2 } ´ }2 P
Markov Decision Processes. A Markov Decision Pro-
Tothisday,alargeefforthasbeenputintoprovidingconver-
cess (MDP, Puterman, 1990) is represented by M:
g exe pn lc oe rag tu ioa nra an lt ge oe rs ita hn md ss (a em .gp .,le Pac po im nip ele tx ai lt .,y 2a 0n 1a 8ly ;s Ye us af no er tA aB
l.,
pS,A,p,r,ρ 0,γ q,whereS ĎRdS andA ĎRdA aretheme“ a-
surablestateandactionspaces,p:S A ∆ S isthetran-
2022;Fatkhullinetal.,2023a),whilethetheoreticalanalysis ˆ ÑÝ p q
sitionmodel,wherep s1 s,a specifiestheprobabilityden-
ofPBexplorationhasbeentakingabackseatsince(Zhao p | q
sity of landing in state s1 S by playing action a A in
etal.,2011). Wearenotawareofanyglobalconvergence P P
states S,r:S A R ,R istherewardfunc-
resultsforparameter-basedPGs. Furthermore,evenforAB P ˆ ÑÝ r´ max max s
tion, where r s,a specifies the reward the agent gets by
exploration,currentstudiesfocusontheconvergencetothe p q
playingactionainstates,ρ ∆ S istheinitial-statedis-
beststochasticpolicy. 0 P p q
tribution,andγ 0,1 isthediscountfactor. Atrajectory
Original Contributions. In this paper, we make a step τ s ,a ,..P .,r s s ,a oflengthT N
τ,0 τ,0 τ,T´1 τ,T´1
“p q P Yt`8u
towards the theoretical understanding of the practice of isasequenceofT state-actiřonpairs. Thediscountedreturn
deployingadeterministicpolicylearnedwithPGmethods: ofatrajectoryτ isR τ : T´1γtr s ,a .
p q “ t“0 p τ,t τ,t q
• Weintroduceaframeworkformodelingthepracticeof DeterministicParametricPolicies. Weconsiderapara-
deployingadeterministicpolicy,byformalizingthenotion metricdeterministicpolicyµ θ:S A,whereθ Θ RdΘ
ofwhitenoise-basedexploration,allowingforaunified istheparametervectorbelongingtÑ otheparameteP rspĎ aceΘ.
treatmentofbothABandPBexploration. Theperformanceofµ isassessedviatheexpectedreturn
θ
• Westudytheconvergencetothebestdeterministicpolicy J :Θ R,definedas:
D
forbothABandPBexploration.Forthisreason,wefocus Ñ J θ : E R τ , (1)
ontheglobalconvergence,ratherthanonthefirst-order D p q “τ ś„pDp¨|θqr p qs
stationarypoint(FOSP)convergence,andweleverageon wherep τ;θ : ρ s T´1p s s ,µ s is
commonlyused(weak)gradientdominationassumptions. D p q “ 0 p τ,0 q t“0 p τ,t`1 | τ,t θ p τ,t qq
the density of trajectory τ induced by policy µ .2 The
θ
• We quantitatively show how the exploration level (i.e.,
agent’sgoalconsistsoffindinganoptimalparameterθ˚
noise)generatesatrade-offbetweenthesamplecomplex- DP
argmax J θ andwedenoteJ˚: J θ˚ .
ity and the performance of the deployed deterministic θPΘ D p q D “ D p Dq
policy. Then,weillustratehowitcanbetunedtooptimize Action-Based(AB)Exploration. InABexploration,we
suchatrade-off,deliveringsamplecomplexityguarantees. consider a parametric stochastic policy π ρ:S ∆ A ,
Ñ p q
whereρ P istheparametervectorbelongingtotheparam-
In light of these results, we compare the advantages and eterspacP eP RdP. Thepolicyisusedtosampleactions
disadvantagesofABandPBexplorationintermsofsample- Ď
a π s tobeplayedinstates foreverysteptofinter-
complexity and requested assumptions, giving a formal t „ ρ p¨| t q t
action. Theperformanceofπ isassessedviatheexpected
guise to intuitive results. We also elaborate on how the ρ
returnJ :P R,definedas:
assumptionsusedintheconvergenceanalysiscanberecon- A Ñ
nectedtobasiccharacteristicsoftheMDPandthepolicy J A ρ : E R τ , where (2)
classes. We conclude with a numerical validation to em-
p q “τ ś„pAp¨|ρqr p qs
piricallyillustratethediscussedtrade-offs. Theproofsof p A pτ;ρ q: “ρ 0 ps τ,0
q
T t“´ 01π ρ pa τ,t |s τ,t qp ps τ,t`1 |s τ,t,a τ,t
q
theresultspresentedinthemainpaperarereportedinAp- isthedensityoftrajectoryτ inducedbypolicyπ ρ.2 InAB
pendixD.TherelatedworksarediscussedinAppendixB. exploration, we aim at learning ρ˚ APargmax ρPPJ A pρ
q
and we denote J ˚: J ρ˚ . If J ρ is differentiable
A “ A p Aq A p q
w.r.t. ρ, PG methods (Peters & Schaal, 2008) update the
2.Preliminaries
2ForbothJ (resp.J ,J )andp (resp.p ,p ),weusetheD
D A P D A P
Notation. ForameasurablesetX,wedenotewith∆ X (resp.A,P)subscripttodenotethatthedependenceonθ(resp.ρ)
p q
the set of probability measures over X. For P ∆ X , isthroughaDeterministicpolicy(resp.Action-basedexploration
P p q
we denote with p its density function. With little abuse policy,Parameter-basedexplorationhyperpolicy).
2LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
p
parameterρviagradientascent: ρ ρ ζ ∇ J ρ , icyπ :S ∆ A issuchthat,foreverystates S,action
t`1 t t ρ A t θ
w ofh ∇ere Jζ t ą ρ0 .i Is nth pae rs tit ce up las ri ,ze tha en Gd P∇p Oρ MJ DAÐÝ p Pρ eq s` i ts ima an toe rst ii sm :p 3atoq r a de„ ntπ lyθ p a¨ t|s eÑ q ves ra ytp i ss tfi eq e ps .a “µ θ ps q`ϵwhereϵ „Φ dP A indepen-
ρ A p q ˜ ¸
∇p ρJ A pρ q: “N1
ÿNTÿ´1 ÿt
∇ ρlogπρ pa τi,k |s τi,k
q
γtr ps τi,t,a τi,t q, Thisdefinitionconsidersstochasticpoliciesπ θ p¨|s qthatare
i“1t“0 k“0 obtainedbyaddingnoiseϵfulfillingDefinition3.1,sampled
whereN isthenumberofindependenttrajectories tτ i uN i“1 independentlyateverystep,totheactionµ θ ps qprescribed
collectedwithpolicyπ (τ p ;ρ ),calledbatchsize. bythedeterministicpolicy(i.e.,ABexploration),resulting
ρ i A
„ p¨ q
inplayingactionµ s ϵ. Ananalogousdefinitioncanbe
Parameter-Based(PB)Exploration. InPBexploration, θ p q`
formulatedforhyperpolicies.
we use a parametric stochastic hyperpolicy ν ∆ Θ ,
ρ
where ρ RdP is the parameter vector. The hypĎ erpop licq y Definition 3.3 (White noise-based hyperpolicies). Let
i ds etu es re md into iP sts ia cm pop ll ie cypa µra θm atet te hr es bθ eg„ inν nρ int go ob fe ep velu rg yg tre ad jei cn toth rye
.
θ anP dΘ lea tn Φd dΘµ θ be:S aÑ whA iteb -ne oa isp ea (r Da em fie nt ir tii oc nd 3e .t 1e )r .m Ain wis hti ic tep no ol ii sc ey
-
T exh pe ecp te ar tf io or nm oa vn ec re θ Ji on Pfd ρe Jx D :po θf qν Edρ efiis n JeJ DdP θa:R s:2d .ρ ÑÝ R, that is the b r ϵaa „mse Φed t de Θh ry θ ip nPe dr Θ ep p,o epl ni ac dry ea nmν tlθ e ytP e in∆ r eθp vΘ 1 e„ rq yνi θ ts rass a ju etc cih s tfi oet rh ysa .θt, 1 “fo θr `ev ϵer wy hep ra e-
p q “θ„νρr p qs
wPB edex enp olo tera Jtio ˚n :aim Jsa ρt ˚lea .r In fin Jgρ ρ˚ PP isar dg ifm fea rex nρ tP iaP bJ leP p wρ .rq .ta .n ρd
,
T tah inis edde bfi yn ai dti do in ngco nn os ii sd ee ϵrs fus lt fio lc lh ina gsti Dc eh fiy np ite ir op no 3li .c 1i ,e ss aν mθ plo eb d-
PGPE(SehnP ke“ etaP lp .,2P 0q 10)upD dp atq esthehyperparameterρ independently at the beginning of each trajectory, to the
viagradientaccent: ρ
t`1
ρ
t
ζ t∇p ρJ
P
ρ
t
. Inparticular, parameterθdefiningthedeterministicpolicyµ θ,resulting
PGPEusesanestimatoroÐÝ f∇ ρ` J
P
ρ defip nedq as: inplayingdeterministicpolicyµ θ`ϵ(i.e.,PBexploration).
∇p J ρ 1
ÿN
∇
lop gνq
θ R τ ,
D use efi dn (i hti yo pn es r)3 p.2 ola icn id es3 ,. l3 ika ell Gow aut so sir ae npr he ys pe en rt pa olc il ca is es so af ndw Gid ae uly s-
-
ρ P ρ ρ i i
p q“N p q p q sianpolicieswithstate-independentvariance. Furthermore,
i“1
where N is the number of independent parameters- oncetheparameterθislearnedwitheitherABandPBex-
trajectoriespairs θ ,τ N ,collectedwithhyperpolicy ploration,deployingthecorrespondingdeterministicpolicy
ν (θ ν andτtp pi i q ;u θi“ )1 ,calledbatchsize. (i.e.,“switchingoff”thenoise)isstraightforward.4
ρ i ρ i D i
„ „ p¨ q
3.White-NoiseExploration 4.FundamentalAssumptions
Weformalizeaclassofstochastic(hyper)policieswidely Inthissection,wepresentthefundamentalassumptionson
employedinthepracticeofABandPBexploration,namely theMDP(pandr),deterministicpolicyµ θ,andwhitenoise
whitenoise-based(hyper)policies. Thesepoliciesπ s Φ. Forthesakeofgenerality,wewillconsiderabstractas-
θ
(resp. hyperpolicies ν ) are obtained by adding a wp h¨| iteq sumptionsinthenextsectionsand,then,showtheirrelation
θ
noiseϵtothedeterministicactiona µ s (resp. tothe tothefundamentalones(seeAppendixAfordetails).
θ
“ p q
parameterθ)independentofthestates(resp. parameterθ).
AssumptionsontheMDP. Westartwiththeassumptions
Definition3.1(WhiteNoise). Letd Nandσ 0. Aprob- ontheregularityoftheMDP,i.e.,ontransitionmodelpand
abilitydistributionΦ ∆ Rd isawP hite-noiseą if: rewardfunctionr,w.r.t.variationsoftheplayedactiona.
d
P p q
E ϵ 0 , E ϵ 2 dσ2. (3) Assumption4.1(LipschitzMDP(logp,r)w.r.t. actions).
ϵ„Φdr s“ d ϵ„Φdr} }2sď
Thelogtransitionmodellogp s1 s, andtherewardfunc-
p | ¨q
tion r s, are L -LC and L -LC, respectively, w.r.t. the
Thisdefinitioncomplieswiththezero-meanGaussiandistri- p r
p ¨q
bution ϵ N 0 ,Σ , where E ϵ 2 tr Σ actionforeverys,s1 S,i.e.,foreverya,a A:
dλ max pΣ„ q. Ip n pd artiq cular, for aϵ n„N isop0 trd o,Σ pq icr} G} a2 us s“ sianp Σqď
“
|logp ps1 |s,a qP ´logp ps1 |s,a q|ďL p }P a ´a }2, (4)
σ no2 tI id o, nw ofe wh ha iv tee nth oa ist et -r bp aΣ seq d“ (d hσ yp2 e. r)W poe licn yo .w formalize the |r ps,a q´r ps,a q|ďL r }a ´a }2. (5)
Assumption 4.2 (Smooth MDP (logp, r) w.r.t. actions).
Definition3.2(Whitenoise-basedpolicies). Letθ Θand Thelogtransitionmodellogp s1 s, andtherewardfunc-
µ beθ a:S wÑ hitA enb oe isa ep (a Dra em fine it tr ii oc nd 3et .e 1r )m
.
Aini wst hic itp eo nl oic iy sea -n bd aP sle et dΦ pd oA
l-
tionr ps, ¨qareL 2,p-LSandL 2p ,r-| LS¨ ,q respectively,w.r.t.the
4Forwhitenoise-based(hyper)policiesthereexistsaone-to-
3WelimitouranalysistotheGPOMDPestimator(Baxter& onemappingbetweentheparameterspaceof(hyper)policiesand
Bartlett, 2001), neglecting the REINFORCE (Williams, 1992) thatofdeterministicpolicies(P“Θ).Forsimplicity,weassume
sinceitisknownthatthelattersuffersfromlargervariance. Θ“RdΘ andA“RdA (seeAppendixC).
3LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
actionforeverys,s1 S,i.e.,foreverya,a A: 5.DeployingDeterministicPolicies
P P
∇ logp s1 s,a ∇ logp s1 s,a L a a ,
}
a
p | q´
a
p |
q}2
ď
2,p
} ´
}2 Inthissection,westudytheperformanceJ
D
ofthedeter-
}∇ ar ps,a q´∇ ar ps,a q}2 ďL 2,r }a ´a }2. ministicpolicyµ θ,whentheparameterθislearnedviaAB
orPBwhitenoise-basedexploration(Section3). Wewill
Intuitively,theseassumptionsensurethatwhenweperform
refer to this scenario as deploying the parameters, which
ABand/orPBexplorationalteringtheplayedactionw.r.t.a
reflectsthecommonpracticeof“switchingoffthenoise”
deterministicpolicy,theeffectontheenvironmentdynamics
oncethelearningprocessisover.
andonreward(andontheirgradients)iscontrollable.
PBExploration. LetusstartwithPBexplorationbyobserv-
Assumptionsonthedeterministicpolicy. Wenowmove
ingthatforwhitenoise-basedhyperpolicies(Definition3.3),
to the assumptions on the regularity of the deterministic
wecanexpresstheexpectedreturnJ asafunctionofJ
policyµ w.r.t.theparameterθ. P D
θ andofthenoiseϵforeveryθ Θ:
Assumption 4.3 (Lipschitz deterministic policy µ w.r.t. P
θ J θ E J θ ϵ . (8)
parameters θ). The deterministic policy µ θ ps
q
is L µ-LC P p q“ϵ„ΦdΘr D p ` qs
w.r.t.parameterforeverys S,i.e.,foreveryθ,θ Θ:
ThisillustratesthatPBexplorationcanbeobtainedbyper-
P P
µ s µ s L θ θ . (6) turbingtheparameterθofadeterministicpolicyµ viathe
} θ p q´ θp q}2 ď µ } ´ }2 θ
Assumption4.4(Smoothdeterministicpolicyµ
θ
w.r.t. pa- noiseϵ „Φ dΘ. Toachieveguaranteesonthedeterministic
rametersθ). Thedeterministicpolicyµ θ s isL 2,µ-LSw.r.t. performance J D of a parameter θ learned with PB explo-
parameterforeverys S,i.e.,foreveryp θ,q θ Θ: ration,weenforcethefollowingregularitycondition.
P P
∇ µ s ∇ µ s L θ θ . (7) Assumption5.1(LipschitzJ w.r.t.θ). J isL -LCinthe
} θ θ p q´ θ θp q}2 ď 2,µ } ´ }2 D D J
parameterθ,i.e.,foreveryθ,θ1 Θ:
Similarly, these assumptions ensure that if we deploy an P
J θ J θ1 L θ θ1 . (9)
alteredparameterθ,likeinPBexploration,theeffectonthe | D p q´ D p q|ď J } ´ }2
playedaction(andonitsgradient)isbounded.
WhentheMDPandthedeterministicpolicyareLCasin
Assumptions 4.1 and 4.3 are standard in the DPG litera- Assumptions4.1and4.3,L isO 1 γ ´2 (seeTable2in
J
pp ´ q q
ture(Silveretal.,2014). Assumption4.2,instead,canbe AppendixAforthefullexpression).Thisway,weguarantee
interpretedasthecounterpartoftheQ-functionsmoothness thatperturbationϵontheparameterθdeterminesavariation
usedintheDPGanalysis(Kumaretal.,2020;Xiongetal., on function J depending on the magnitude of ϵ, which
D
2022), while Assumption 4.4 has been used to study the allowsobtainingthefollowingresult.
convergenceofDPG(Xiongetal.,2022). Similarcondi-
Theorem5.1(Deterministicdeploymentofparameters
tionstoourAssumption4.1wereadoptedbyPirottaetal.
learnedwithPBwhite-noiseexploration). Ifthehyper-
(2015),butmeasuringthecontinuityofpintheKantorovich
policycomplieswithDefinition3.3,underAssumption5.1:
metric,aweakerrequirementthat,unfortunately,doesnot
comewithacorrespondingsmoothnesscondition. (i) (Uniform bound) for every θ Θ, it holds that
P
J θ J θ L ?d σ ;
D P J Θ P
Assumptionsonthe(hyper)policies. Weintroducethe (ii) (| J p uq p´ per bp oq u| nď d) Let θ˚ argmax J θ , it
assumptionsonthescorefunctionsofthewhitenoiseΦ. hoD ldsthat: J˚ J θ˚ P 2P L ?d σθ ;PΘ P p q
Assumption 4.5 (Bounded Scores of Φ). Let Φ ∆ Rd (iii) (J lowerbouD n´ d)TD hp erP eq eď xistsJ anMΘ DPP ,adeterminis-
P p q D
beawhitenoisewithvarianceboundσ 0(Definition3.1) tic policy class µ fulfilling Assumption 5.1, and
ą θ
anddensityϕ. ϕisdifferentiableinitsargumentandthere a noise complying with Definition 3.1, such that
existsauniversalconstantc 0s.t.: J˚ J θ˚ 0.28L ?d σ .
ą D´ D p Pqě J Θ P
(i) E ∇ logϕ ϵ 2 cdσ´2;
(ii) Eϵ„Φ r} ∇ϵ 2logϕp ϵq}2sď cσ´2. Someobservationsareinorder. (i)showsthattheperfor-
ϵ„Φ r} ϵ p q}2 sď mance of the hyperpolicy J θ is representative of the
P
p q
Intuitively,thisassumptionisequivalenttothemorecom- deterministic performance J θ up to an additive term
D
p q
mononesrequiringtheboundednessoftheexpectednorms depending on L ?d σ . As expected, this term grows
J Θ P
of the score function (and its gradient) (Papini et al., withtheLipschitzconstantL ofthefunctionJ ,withthe
J D
2022; Yuan et al., 2022, cf. Appendix E). Note that a standard deviation σ of the additive noise, and with the
P
zero-meanGaussianΦ N 0 ,Σ fulfillsAssumption4.5. dimensionalityoftheparameterspaced . Inparticular,this
d Θ
“ p q
Indeed, one has ∇ logϕ ϵ Σ´1ϵ and ∇2logϕ ϵ impliesthatlim J θ J θ . (ii)isaconsequence
Σ´1. Thus, E ∇ϵ logϕp ϵq“ 2 tr Σ´1 dϵ λ Σp q ´“ 1 of(i)andprovidσ eP sÑ a0 n` upP pp erq b“ ounD dp bq etweentheoptimalper-
and E ∇2logϕr} ϵ ϵ λp q}2 Σs“ ´1.p In pq aď rticulm arin ,p forq an formanceobtainedifwewereabletodirectlyoptimizethe
r} ϵ p q}2 s“ min p q
isotropicGaussianΣ σ2I,wehaveλ Σ σ2,fulfill- deterministic policy max J θ and the performance
min θPΘ D
“ p q“ p q
ingAssumption4.5withc 1. oftheparameterθ˚ learnedbyoptimizingJ θ ,i.e.,via
“ P P p q
4LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
PBexploration,whendeployedonthedeterministicpolicy. bound on the difference between the policy performance
Finally,(iii)providesalowerboundtothesamequantity J θ andthecorrespondingdeterministicpolicyJ θ and
A D
p q p q
onaspecificinstanceofMDPandhyperpolicy,provingthat ontheperformanceofθ˚whendeployedonadeterministic
A
thedependenceonL ?d σ istightuptoconstantterms. policy. Clearly, also in the AB exploration, we have that
J Θ P
lim J θ J θ . As in the PB case, (iii) shows
ABExploration. LetusmovetotheABexplorationcase σAÑ0` A p q“ D p q
thattheupperbound(ii)istightuptoconstantterms.
whereunderstandingtheeffectofthenoiseismorecomplex
sinceitisappliedtoeveryactionindependentlyateverystep. Finally,letusnotethatourboundsforPBexplorationde-
Tothisend,weintroducethenotionofnon-stationarydeter- pend on the dimension of the parameter space d that is
Θ
ministicpolicyµ µ T´1,whereattimesteptthedeter- replacedbythatoftheactionspaced inABexploration.
“p t qt“0 A
ministicpolicyµ :S Aisplayed,anditsexpectedreturn
t
(withabuseofnotati śonÑ )isJ D pµ q“E τ„pDp¨|µq rR pτ qswhere 6.GlobalConvergenceAnalysis
p µ : ρ s T´1p s s ,µ s . Let ϵ
D p¨| q “ 0 p τ,0 q t“0 p τ,t`1 | τ,t t p τ,t qq “ Inthissection,wepresentourmainresultsaboutthecon-
ϵ T´1 ΦT beasequenceofnoisessampledindepen-
p t qt“0 „ dA vergenceofABandPBwhitenoise-basedexplorationto
dently, we denote with µ ϵ µ ϵ T´1 the non-
stationary policy that, at
tiθ m`
e
t“
,
p perθ tu` rbst qt t“ he0
action as
g tel ro mba inl io stp ictim poa ll icp ya Jram .e Lt ee tr Kθ D˚o Nf bth ee thp eer nf uo mrm bea rnc oe fio tef rt ah te iod ne s-
D
µ s ϵ . Since the noise is independent on the state, P
θ p t q` t andN thebatchsize;givenanaccuracythresholdϵ 0,our
we express J as a function of J for every θ Θ as fol- ą
A D P goal is to bound the sample complexity NK to fulfill the
lows: ” ı followinglast-iterateglobalconvergencecondition:
J A pθ q“ϵ„E ΦT
dA
J D pµ θ`ϵ q . (10) J D˚ ´E rJ D pθ K qsďϵ, (12)
whereθ isthe(hyper)parameterattheendoflearning.
Thus, to ensure that the parameter learned by AB explo- K
rationachievesperformanceguaranteeswhenevaluatedas
adeterministicpolicy,weneedtoenforcesomeregularity 6.1.GeneralGlobalConvergenceAnalysis
conditiononJ asafunctionofµ.
D Inthissection, weprovideaglobalconvergenceanalysis
Assumption 5.2 (Lipschitz J w.r.t. µ). J of the non- foragenericstochasticfirst-orderalgorithmoptimizingthe
D D
s st ta at ti io on na ar ry yd pe ot le icrm y,i in .eis .t ,i fc op ro el vic ey ryµ µi ,s
µp
1L :t qT t“´ 01-LCinthenon- Θdiffe Rre dn ,ti ta hb al te co ab nj bec eti iv ne stf au nn cc et dio fn orJ b: oo tn ht Ah Bep (a sera ttm ine gte Jrs :sp Jac Ae
)
Ď “
andPB(settingJ J )exploration,whenoptimizingthe
Tÿ´1 › › : “ P
J µ J µ1 L sup› µ s µ1 s › . (11) corresponding objective. At every iteration k K , the
| D p q´ D p q|ď
t“0
t řsPS t p q´ tp q 2 algorithmperformsthegradient pascentupdate: P(cid:74) (cid:75)
Furthermore,wedenoteL:
“
T t“´ 01L t. θ k`1 ÐÝθ k `ζ k∇ pθJ : pθ k q, (13)
whereζ 0isthestepsizeand∇ J θ isanunbiased
WhentheMDPisLCasinAssumptions4.1,LisO 1 k ą θ : p k q
γ ´2 (seeTable2inAppendixAforthefullexpresp sp ion´ ). estimateof∇ θJ : pθ k qanddenoteJ :˚ “max θPΘJ : pθ q. We
q q enforcethefollowingstandardassumptions.
Theassumptionenforcesthatchangingthedeterministicpol-
icyatsteptfromµ ttoµ1 t,thevariationofJ Discontrolled Assumption6.1(WeakgradientdominationforJ :). There
bytheactiondistance(intheworststates)multipliedbya existα 0andβ 0suchthatforeveryθ Θitholdsthat
time-dependentLipschitzconstant. Thisformofcondition J˚ Ją θ α ∇ě J θ β. P
allowsustoshowthefollowingresult. : ´ : p qď } θ : p q}2 `
Theorem5.2(Deterministicdeploymentofparameters Assumption6.1isthegoldstandardfortheglobalconver-
learnedwithABwhite-noiseexploration). Ifthepolicy genceofstochasticoptimization(Yuanetal.,2022;Masiha
complieswithDefinition3.2andunderAssumption5.2: et al., 2022; Fatkhullin et al., 2023a). Note that, when
(i) (Uniform bound) for every θ Θ, it holds that: β 0, we recover the (strong) gradient domination (GD)
(ii) (| hJ J oD
D
ldp uθ spq tp h´ e ar tJ b JA op ˚θ unq| d Jď )LL e θ? t ˚tid nA gσ θ 2A LA˚; ?Pa drP g σma ;x θPΘJ
A
pθ q,it p s titr mr“ o icp at .e er Irt ny t s: h teaJ an:˚ dW ,´ WGJ : GDp Dθ ,q a aď n dd mα r i} e t∇ squ lθ oiJ r cep as: lθ t mhq} a a2 t xiJf mo :r aha aal sl sθ ln oP o nΘ glo. ac sG a tlD ho epi is r-
(iii) (J
lowerboD u´ nd)D Tp heA reqď existsanA MDA
P,adeterminis-
performanceisβ-closetothegloballyoptimalone.5
D
tic policy class µ fulfilling Assumption 5.1, and
θ Assumption 6.2 (Smooth J w.r.t. parameters θ). J is
a noise complying with Definition 3.1, such that : :
J D˚ ´J D pθ A˚ qě0.28L?d Aσ A. 5Inthissection,wewillassumethatJ :(i.e.,eitherJ AorJ A)
is already endowed with the WGD property. In Section 7, we
Similarly to Theorem 5.1, (i) and (ii) provide an upper illustratehowitcanbeobtainedinseveralcommonscenarios.
5LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
L -LSw.r.t.parametersθ,i.e.,foreveryθ,θ1 Θ: samplecomplexityboundsaresummarizedinTable1and
2,:
P
∇ J θ1 ∇ J θ L θ1 θ . (14) presentedextensivelyinAppendixD.Theyallfollowfrom
θ : θ : 2 2,: 2
} p q´ p q} ď } ´ } ourgeneralTheorem6.1andourresultsonthedeployment
Assumption6.2isubiquitousintheconvergenceanalysis ofdeterministicpoliciesfromSection5.
of policy gradient algorithms (Papini et al., 2018; Agar-
PGPE. We start by commenting on the sample com-
wal et al., 2021; Yuan et al., 2022; Bhandari & Russo,
plexity of PGPE for a constant, generic hyperpolicy vari-
2024), which is usually studied as an instance of (non-
ance σ , shown in the first column. First, the guaran-
convex)smoothstochasticoptimization. Thesmoothness tee on JP ˚ E J θ contains the additional variance-
ofJ J ,J canbe: (i)inheritedfromthedeterministic D´ r D p K qs
obje: ctP ivt eA J (P ou riginating,inturn,fromtheregularityofthe dependentterm3L P?d Θσ P originatingfromthedetermin-
D istic deployment. Second, the sample complexity scales
MDP)andofthedeterministicpolicyµ (Assumptions4.1- r
θ with O ϵ´3 . Third, by enforcing the smoothness of the
4.4); or (ii) enforced through the properties on the white p q
MDP and of the deterministic policy (Assumptions 4.2
noiseΦ(Assumption4.5). Thefirstresultwasobservedin
and 4.4), we improve the dependence on d and on σ
asimilarformbyPirottaetal.(2015,Theorem3),whilea Θ P
atthepriceofanadditional 1 γ ´1factor.
generalizationofthesecondwasestablishedbyPapinietal. p ´ q
(2022)andrefinedbyYuanetal.(2022). Achoiceofσ P whichadaptstoϵallowsustoachievethe
p globalconvergenceonthedeterministicobjectiveJ ,upto
Assumption 6.3 (Bounded estimator variance ∇ J θ ). D
p θ : p q ϵ β only. Movingtothesecondcolumn,weobservethat
The estimator ∇ θJ
:
pθ
q
computed with batch size N has th`
e convergence rate becomes
Or
ϵ´7 , which reduces to
a evb eo ryun θde Θd ,v war eia hn ac ve e, :i V.e a., rth ∇pere Jex θistsV : Vě0 Ns .uchthat,for Or pϵ´5 qwiththeadditionalsmoothp nessq assumptions,which
P r θ : p qsď : { alsoimprovethedependenceonboth 1 γ ´1andd Θ.The
p ´ q
slower rate ϵ´5 or ϵ´7, compared to the ϵ´3 of the fixed-
Assumption 6.3 guarantees that the gradient estimator is
variancecase,iseasilyexplainedbythemorechallenging
characterizedbyaboundedvarianceV whichscaleswith
: requirementofconvergingtotheoptimaldeterministicpol-
the batch size N. Under Assumptions 4.5 (and 4.4 for
icy rather than the optimal stochastic hyperpolicy, as for
GPOMDP),thetermV canbefurthercharacterized(see
: standard PGPE. Note that we have set the standard devi-
Table2inAppendixA).
Wearenowreadytostatetheglobalconvergenceresult.
a exti po en cte eq du ,a dl et co reσ
asP e“ sw6L itP
hϵ?
thdΘ ed“
eO
sirp
eϵ dp1
a´
ccγ
uq
ra2 cd yΘ´ ϵ1{ .62
q
that, as
Theorem 6.1. Consider an algorithm running the up-
GPOMDP. We now consider the global convergence of
date rule of Equation (13). Under Assumptions 6.1, 6.2,
GPOMDP, starting again with a generic policy variance
and 6.3, with a suitable constant step size, to guarantee
σ (third column). The result is similar to that of PGPE
J˚ E J θ ϵ β thesamplecomplexityisatmost: A
: ´ r : p K qsď ` withthreenotableexceptions. First,anadditional 1 γ ´1
NK 16α4L 2,:V : logmax t0,J :˚ ´J : pθ 0 q´β u. (15) factorappearsinthesamplecomplexityduethep va´ rianq ce
“ ϵ3 ϵ bound of GPOMDP (Papini et al., 2022). This suggests
thatGPOMDPstrugglesmorethanPGPEinlong-horizon
r
ThisresultestablishesaconvergenceoforderO ϵ´3 tothe environments, as already observed by Zhao et al. (2011).
p q
globaloptimumJ :˚ ofthegeneralobjectiveJ :. Recalling Second, the dependence on the dimensionality of the pa-
thatJ : PtJ A,J P u,Theorem6.1provides: (i)thefirstglobal rameterspaced Θisreplacedwiththedimensionalityofthe
convergenceguaranteeforPGPEforPBexploration(setting actionspaced . Thisisexpectedandderivesfromthena-
A
J : J P) and (ii) a global convergence guarantee for PG tureofexplorationthatisperformedintheparameterspace
“
(e.g.,GPOMDP)forABexplorationofthesameorder(upto for PGPE and in the action space for GPOMPD. Finally,
logarithmictermsinϵ´1)ofthestate-of-the-artoneofYuan the smoothness of the deterministic policy (Asm. 4.4) is
et al. (2022) (setting J : J A). Note that our guarantee alwaysneeded. AddingalsothesmoothnessoftheMDP
“
is obtained for a constant step size and holds for the last (Asm.4.2),wecantradead factorfora 1 γ ´1one.
A
parameter θ , delivering a last-iterate result, rather than p ´ q
K
Again, a careful ϵ-dependent choice of σ allows us to
abest-iterateoneasin(Yuanetal.,2022, Corollary3.7). A
achieveglobalconvergenceonthedeterministicobjective
Clearly,thisresultisnotyetourultimategoalsince,weneed
J . Inthelastcolumn,wecannoticethattheconvergence
toassesshowfartheperformanceofthelearnedparameter D
θ isfromthatoftheoptimaldeterministicobjectiveJ˚.
ratesdisplaythesamedependenceonϵasinPGPE.How-
K D
6Theseresultsshouldbeinterpretedasademonstrationthat
6.2.GlobalConvergenceofPGPEandGPOMDP globalconvergencetodeterministicpoliciesispossibleratherthan
apracticalrecipetosetthevalueofσ .Wedohopethatourtheory
P
Inthissection,weprovideresultsontheglobalconvergence canguidethedesignofpracticalsolutionsinfutureworks.
ofPGPEandGPOMDPwithwhite-noiseexploration. The
6LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
r
Table1.SamplecomplexityNK“Op¨qofGPOMDPandPGPEtoconvergetoadeterministicoptimalpolicy,retainingonlydependencies
onϵ,p1´γq´1,σ A,σ P,d Θ,d A,andα.Task-dependentconstantsL
P
andL AareOpp1´γq´2q—seeTable2inAppendixA.
ever,thedependenceontheeffectivehorizon 1 γ ´1 is the two objectives encoded in σ . Note that even if J
P D
p ´ q
worse. Inthiscase,theadditionalsmoothnessassumption enjoysa(strong)GD(i.e.,β 0),ingeneral,J inheritsa
D P
“
improvesthedependencyond and 1 γ ´1. WGDproperty. InthesettingofTheorem7.1,convergence
A
p ´ q in the sense of J˚ E J θ ϵ β can be achieved
r D´ r D p K qsď ` D
7.AbouttheWeakGradientDomination withO α6ϵ´5d2 1 γ ´11 samplesbycarefullysetting
p D Θp ´ q q
thehyperpolicyvariance(seeTheoremD.12fordetails).
So far, we have assumed WGD for the AB J and PB
A
AnanalogousresultcanbeobtainedforABexploration.
J (Assumption 6.1). In this section, we discuss several
P
scenariosinwhichsuchanassumptionholds. Theorem7.2(InheritedweakgradientdominationonJ A).
UnderAssumptions7.1,4.1,4.3,4.2,4.4,foreveryθ Θ:
a
P
7.1.InheritedWeakGradientDomination J ˚ J θ α ∇ J θ β α ψ L σ d ,
A A D θ A 2 D D A A A
´ p qď } p q} ` `p ` q
whereψ O 1 γ ´4 (fullexpressionintheproof).
Westartbydiscussingthecaseinwhichthedeterministic
“ pp ´ q q
policyobjectiveJ Dalreadyenjoysthe(W)GDproperty.
The sample complexity, in this case, is
Or
α6 ϵ´5d2 1
Assumption7.1(WeakgradientdominationforJ D). There γ ´14 (seeTheoremD.13fordetails). p D Ap ´
exist α 0 and β 0 such that for every θ Θ it holds q q
D D
ą ě P
thatJ˚ J θ α ∇ J θ β .
D´ D p qď D } θ D p q}2 ` D 7.2.Policy-inducedWeakGradientDomination
AlthoughthenotionofWGDhasbeenmostlyappliedto Whenthetheobjectivefunctiondoesnotenjoyweakgradi-
stochasticpoliciesintheliterature(Liuetal.,2020;Yuan entdominationinthespaceofdeterministicpolicies,wecan
etal.,2022),thereisnoreasonwhyitshouldnotbeplausible stillhaveWGDwithrespecttostochasticpoliciesiftheysat-
fordeterministicpolicies.Bhandari&Russo(2024)provide isfyaconditionknownasFisher-non-degeneracy(Liuetal.,
sufficient conditions for the performance function not to 2020;Dingetal.,2022).Asfarasweknow,WGDbyFisher-
haveanylocaloptima,whichisastrongercondition,without non-degeneracy is a peculiar property of AB exploration
discriminatingbetweendeterministicandstochasticpolicies thathasnoequivalentinPBexploration. White-noisepoli-
(cf. theirRemark1). Moreover, oneoftheirexamplesis ciessatisfyingAssumption4.5areFisher-non-degenerate
linear-quadraticregulatorswithdeterministiclinearpolicies.
underthefollowingstandardassumption(Liuetal.,2020):
Weshowthat,underLipschiztianityandsmoothnessofthe Assumption 7.2 (Explorability). There exists λ 0 s.t.
E
MDPanddeterministicpolicy(Assumptions4.1-4.4),this E ∇ µ s ∇ µ s J ľλ I for all θ Θ, wheą re the
πθr θ θ
p q
θ θ
p q s
E
P
issufficienttoenforcetheWGDpropertyforboththePB expectationoverstatesisinducedbythestochasticpolicy.
J andtheABJ objectives. LetusstartwithJ .
P A P
WecanusethisfacttoproveWGDforwhite-noisepolicies:
Theorem7.1(InheritedweakgradientdominationforJ ).
P
UnderAssumptions7.1,4.1,4.3,4.2,4.4,foreveryθ Θ:
aP
Theorem7.3(Policy-inducedweakgradientdomination).
J ˚ J θ α ∇ J θ β α L L σ d ,
P ´ P p qď D } θ P p q}2 ` D `p D 2 ` P q P Θ UnderAssumptions4.5,7.2andD.1,wehave:
whereL O 1 γ ´3 (fullexpressioninLemmaE.2).
2 “ pp ´ q q J ˚ J θ C?d Aσ A ∇ J θ ?ϵ bias ,
A A θ A 2
TheresultshowsthattheWGDpropertyofJ Dentailsthat ´ p qď λ E } p q} ` 1 ´γ
of J with the same α coefficient, but a different β forsomenumericalconstantC 0,thatis,Assumption6.1
β αP L L σ ?d D thataccountsforthegapbetwee“ n ( =A)issatisfiedwithα C? dą AσA andβ ? ϵbias.
D p D 2 ` P q P Θ : “ λE “ 1´γ
7LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Hereϵ isthecompatible-criticerror,whichcanbevery 200 200
bias
small for rich policy classes (Ding et al., 2022). We can
100 100
leveragethistoprovetheglobalconvergenceofGPOMDP
? asinSection7.1,thistimetoJ D ´E rJ D pθ qsďϵ
`
1ϵ ´bi γas. 0
JD
0
JD
Tuning σ , we can achieve a sample complexity of
JP JA
Or ϵ´1λ´4A
d4 1 γ ´10 (see Theorem D.16 for details)
´100
10´2 10´1 100 101 102
´100
10´2 10´1 100 101 102
Thp isseemE stA op vio´ lateq theq Ω ϵ´2 lowerboundbyAzaretal. σP2 σA2
p q
(2013). However,thefactorλ candependonσ O ϵ (a)PGPEonHalfCheetah. (b)GPOMDPonHalfCheetah.
E A
“ p q
in highly non-trivial ways, and, thus, can hide additional
250 250
factorsofϵ.Forthisreason,theresultsgrantedbytheFisher-
non-degeneracyofwhite-noisepoliciesarenotcompared
withtheonesgrantedbyinheritedWGDfromSection7.1. 200 200
Intuitively,λ E encodessomedifficultiesofexplorationthat JD JD
areabsentin“nice”MDPssatisfyingAssumption7.1. See
JP JA
150 150
AppendixD.4forfurtherdiscussionandomittedproofs. 10´2 10´1 100 101 102 10´2 10´1 100 101 102
σP2 σA2
8.NumericalValidation (c)PGPEonHopper. (d)GPOMDPonHopper.
I icn at lh ri es ss ue lc tstio pn re, sw ee nte em dp iniri tc ha ell py av pa el ri .d Wate es co om nde uo cf tt ahe stt uh de yor oe nt- 60 60 J JD
A
thegapinperformancebetweenthedeterministicobjective
40 40
J andtheonesofGPOMDPandPGPE(respectivelyJ
D A JD
andJ P)byvaryingthevalueoftheirexplorationparameters JP
(σ andσ ,respectively). Detailsontheemployedversions 20 20
A P 10´2 10´1 100 101 102 10´2 10´1 100 101 102
ofPGPEandGPOMDPcanbefoundinAppendixG.Addi- σP2 σA2
tionalexperimentalresultscanbefoundinAppendixH.
(e)PGPEonSwimmer. (f)GPOMDPonSwimmer.
WerunPGPEandGPOMDPforK 2000iterationswith
“ Figure1.VariancestudyonMujoco(5runs,mean˘95%C.I.).
batch size N 100 on three environments from the Mu-
“
JoCo(Todorovetal.,2012)suite: Swimmer-v4(T 200), 9.Conclusions
“
Hopper-v4(T 100),andHalfCheetah-v4(T 100). For
“ “
all the environments the deterministic policy is linear in Wehaveperfectedrecenttheoreticalresultsontheglobal
the state and the noise is Gaussian. We consider σ2 convergence of policy gradient algorithms to address the
:P
0.01,0.1,1,10,100 . MoredetailsinAppendixH.1. practicalproblemoffindingagooddeterministicparametric
t u
policy. Wehavestudiedtheeffectsofnoiseonthelearning
FromFigure1,wenotethatastheexplorationparameter
process and identified a theoretical value of the variance
grows,thedistanceofJ θ andJ θ fromJ θ
P p K q A p K q D p K q of the (hyper)policy that allows to find a good determin-
increases,coherentlywithTheorems5.1and5.2. Among
istic policy using a polynomial number of samples. We
the tested values for σ and σ , some lead to the highest
P A havecomparedthetwocommonformsofnoisyexploration,
valuesofJ θ . Empirically,wenotethatPGPEdelivers
D p K q action-basedandparameter-based,bothfromatheoretical
thebestdeterministicpolicywithσ2 10forSwimmerand
P“ andanempiricalperspective.
withσ2 1fortheotherenvironments. GPOMDPperforms
P“
thebestwithσ2 1forSwimmer,andwithσ2 10inthe Ourworkpavesthewayforseveralexcitingresearchdirec-
A“ A“
other cases. These outcomes agree with the theoretical tions. First,ourtheoreticalselectionofthepolicyvariance
resultsinshowingthatthereexistsanoptimalvalueforσ . isnotpractical,butourtheoreticalfindingsshouldguidethe
:
designofsoundandefficientadaptive-varianceschedules.
We can also appreciate the trade-off between GPOMDP
Wehaveshownhowwhite-noiseexplorationpreservesweak
andPGPEw.r.t. theparameterdimensionalityd andthe
Θ gradientdomination—thenaturalnextquestioniswhether
horizonT,bycomparingthebestvaluesofJ foundbythe
D asufficientamountofnoisecansmoothoreveneliminate
two algorithms in each environment. GPOMDP is better
thelocaloptimaoftheobjectivefunction. Finally,wehave
than PGPE in Hopper and HalfCheetah. This can be ex-
focusedon“vanilla”policygradientmethods,butourideas
plainedbythefactthatsuchenvironmentsarecharacterized
couldbeappliedtomoreadvancedalgorithms,suchasthe
by higher values of d . Instead, in Swimmer, PGPE per-
Θ onesrecentlyproposedbyFatkhullinetal.(2023a),tofind
formsbetterthanGPOMDP.Thiscanbeexplainedbythe r
optimaldeterministicpolicieswithO ϵ´2 samples.
highervalueofT andthelowervalueofd . p q
Θ
8
qKθ
p:J
qKθ
p:J
qKθ
p:J
qKθ
p:J
qKθ
p:J
qKθ
p:JLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
ImpactStatement Deisenroth,M.P.andRasmussen,C.E. PILCO:Amodel-
based and data-efficient approach to policy search. In
Thispaperpresentsworkwhosegoalistoadvancethefield
InternationalConferenceonMachineLearning(ICML),
of Machine Learning. There are many potential societal
pp.465–472.Omnipress,2011.
consequences of our work, none which we feel must be
specificallyhighlightedhere. Deisenroth, M.P., Neumann, G., andPeters, J. Asurvey
onpolicysearchforrobotics. FoundationsandTrendsin
Robotics,2(1-2):1–142,2013.
References
Ding,Y.,Zhang,J.,andLavaei,J. Ontheglobaloptimum
Agarwal, A., Kakade, S.M., Lee, J.D., andMahajan, G.
convergence of momentum-based policy gradient. In
On the theory of policy gradient methods: Optimality,
InternationalConferenceonArtificialIntelligenceand
approximation,anddistributionshift. JournalofMachine
Statistics(AISTATS),volume151ofProceedingsofMa-
LearningResearch(JMLR),22:98:1–98:76,2021.
chineLearningResearch,pp.1910–1934.PMLR,2022.
Ahmed,Z.,Roux,N.L.,Norouzi,M.,andSchuurmans,D.
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and
Understandingtheimpactofentropyonpolicyoptimiza-
Abbeel,P. Benchmarkingdeepreinforcementlearning
tion. InInternationalConferenceonMachineLearning
forcontinuouscontrol. InInternationalConferenceon
(ICML),volume97ofProceedingsofMachineLearning
MachineLearning(ICML),pp.1329–1338.PMLR,2016.
Research,pp.151–160.PMLR,2019.
Fatkhullin,I.,Barakat,A.,Kireeva,A.,andHe,N. Stochas-
Allgower, E. L. and Georg, K. Numerical continuation ticpolicygradientmethods: Improvedsamplecomplex-
methods-anintroduction,volume13ofSpringerseries ity for fisher-non-degenerate policies. arXiv preprint
incomputationalmathematics. Springer,1990. arXiv:2302.01734,2023a.
Arjevani,Y.,Carmon,Y.,Duchi,J.C.,Foster,D.J.,Sekhari, Fatkhullin,I.,Barakat,A.,Kireeva,A.,andHe,N. Stochas-
A.,andSridharan,K. Second-orderinformationinnon- ticpolicygradientmethods:Improvedsamplecomplexity
convex stochastic optimization: Power and limitations. forfisher-non-degeneratepolicies. InInternationalCon-
In Proceedings of the Annual Conference on Learning ference on Machine Learning (ICML), volume 202 of
Theory(COLT),volume125ofProceedingsofMachine ProceedingsofMachineLearningResearch,pp.9827–
LearningResearch,pp.242–299.PMLR,2020. 9869.PMLR,2023b.
Fazel,M.,Ge,R.,Kakade,S.M.,andMesbahi,M. Global
Arjevani,Y.,Carmon,Y.,Duchi,J.C.,Foster,D.J.,Srebro,
convergence of policy gradient methods for the linear
N.,andWoodworth,B.E. Lowerboundsfornon-convex
quadraticregulator. InInternationalConferenceonMa-
stochastic optimization. Math. Program., 199(1):165–
chineLearning(ICML),volume80ofProceedingsofMa-
214,2023.
chineLearningResearch,pp.1466–1475.PMLR,2018.
Azar,M.G.,Munos,R.,andKappen,H.J. MinimaxPAC Fujimoto, S., van Hoof, H., and Meger, D. Addressing
boundsonthesamplecomplexityofreinforcementlearn- functionapproximationerrorinactor-criticmethods. In
ingwithagenerativemodel. MachineLearning,91(3): InternationalConferenceonMachineLearning(ICML),
325–349,2013. volume80ofProceedingsofMachineLearningResearch,
pp.1582–1591.PMLR,2018.
Azizzadenesheli, K., Yue, Y., and Anandkumar, A.
Policy gradient in partially observable environments: Ghavamzadeh,M.andEngel,Y. Bayesianpolicygradient
Approximation and convergence. arXiv preprint algorithms. AdvancesinNeuralInformationProcessing
arXiv:1810.07900,2018. Systems(NeurIPS),19,2006.
Ghavamzadeh,M.,Lazaric,A.,andPirotta,M. Exploration
Baxter,J.andBartlett,P.L. Infinite-horizonpolicy-gradient
inreinforcementlearning. TutorialatAAAI’20,2020.
estimation. Journal of Artificial Intelligence Research
(JAIR,15:319–350,2001. Gravell, B., Esfahani, P. M., and Summers, T. Learning
optimalcontrollersforlinearsystemswithmultiplicative
Bhandari,J.andRusso,D. Globaloptimalityguaranteesfor
noise via policy gradient. IEEE Transactions on Auto-
policygradientmethods. OperationsResearch,2024.
maticControl,66(11):5283–5298,2020.
Bolland,A.,Louppe,G.,andErnst,D. Policygradientalgo- Kakade,S.M. Anaturalpolicygradient. InAdvancesin
rithmsimplicitlyoptimizebycontinuation. Transactions Neural Information Processing Systems (NeurIPS), pp.
onMachineLearningResearch,2023. 1531–1538.MITPress,2001.
9LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Karimi, H., Nutini, J., and Schmidt, M. Linear conver- Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and
genceofgradientandproximal-gradientmethodsunder Restelli, M. Stochastic variance-reduced policy gradi-
thepolyak-łojasiewiczcondition. InMachineLearning ent. InInternationalConferenceonMachineLearning
andKnowledgeDiscoveryinDatabases: EuropeanCon- (ICML),volume80ofProceedingsofMachineLearning
ference(ECMLPKDD),pp.795–811.Springer,2016. Research,pp.4023–4032.PMLR,2018.
Kingma,D.P.andBa,J. Adam: Amethodforstochastic Papini,M.,Battistello,A.,andRestelli,M. Balancinglearn-
optimization. arXivpreprintarXiv:1412.6980,2014. ing speed and stability in policy gradient via adaptive
exploration. In International Conference on Artificial
Kumar,H.,Kalogerias,D.S.,Pappas,G.J.,andRibeiro,A. IntelligenceandStatistics(AISTATS),volume108ofPro-
Zeroth-orderdeterministicpolicygradient.arXivpreprint ceedingsofMachineLearningResearch,pp.1188–1199.
arXiv:2006.07314,2020. PMLR,2020.
Papini,M.,Pirotta,M.,andRestelli,M. Smoothingpolicies
Li,G.,Wei,Y.,Chi,Y.,Gu,Y.,andChen,Y.Softmaxpolicy
andsafepolicygradients. MachineLearning,111(11):
gradientmethodscantakeexponentialtimetoconverge.
4081–4137,2022.
In Proceedings of the Annual Conference on Learning
Theory(COLT),volume134ofProceedingsofMachine
Peters, J. and Schaal, S. Policy gradient methods for
LearningResearch,pp.3107–3110.PMLR,2021.
robotics. InIEEE/RSJInternationalConferenceonIntel-
ligentRobotsandSystems,pp.2219–2225.IEEE,2006.
Likmeta,A.,Metelli,A.M.,Tirinzoni,A.,Giol,R.,Restelli,
M.,andRomano,D. Combiningreinforcementlearning Peters,J.andSchaal,S. Reinforcementlearningofmotor
with rule-based controllers for transparent and general skillswithpolicygradients. Neuralnetworks,21(4):682–
decision-makinginautonomousdriving. Roboticsand 697,2008.
AutonomousSystems,131:103568,2020.
Peters, J., Vijayakumar, S., and Schaal, S. Natural actor-
Lillicrap,T.P.,Hunt,J.J.,Pritzel,A.,Heess,N.,Erez,T., critic. InEuropeanConfereneceonMachineLearning
Tassa,Y.,Silver,D.,andWierstra,D. Continuouscon- (ECML), volume 3720 of Lecture Notes in Computer
trolwithdeepreinforcementlearning. InInternational Science,pp.280–291.Springer,2005.
ConferenceonLearningRepresentations(ICLR),2016.
Pirotta,M.,Restelli,M.,andBascetta,L. Policygradientin
lipschitzmarkovdecisionprocesses. MachineLearning,
Liu, Y., Zhang, K., Basar, T., and Yin, W. An improved
100:255–283,2015.
analysis of (variance-reduced) policy gradient and nat-
ural policy gradient methods. In Advances in Neural
Polyak,B.T.etal. Gradientmethodsforminimizingfunc-
InformationProcessingSystems(NeurIPS),2020.
tionals. Zhurnalvychislitel’noimatematikiimatematich-
eskoifiziki,3(4):643–653,1963.
Lojasiewicz, S. Une proprie´te´ topologique des sous-
ensemblesanalytiquesre´els. Lese´quationsauxde´rive´es Puterman,M.L. Markovdecisionprocesses. Handbooksin
partielles,117:87–89,1963. operationsresearchandmanagementscience,2:331–434,
1990.
Masiha, S., Salehkaleybar, S., He, N., Kiyavash, N., and
Thiran,P. Stochasticsecond-ordermethodsimprovebest- Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,M.,
knownsamplecomplexityofsgdforgradient-dominated andDormann,N. Stable-baselines3: Reliablereinforce-
functions. AdvancesinNeuralInformationProcessing mentlearningimplementations. TheJournalofMachine
Systems(NeurIPS),35:10862–10875,2022. LearningResearch(JMLR),22(1):12348–12355,2021a.
Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,
Mei,J.,Xiao,C.,Szepesva´ri,C.,andSchuurmans,D. On
M.,andDormann,N. Stable-baselines3: Reliablerein-
the global convergence rates of softmax policy gradi-
forcementlearningimplementations. JournalofMachine
ent methods. In International Conference on Machine
LearningResearch(JMLR),22(268):1–8,2021b.
Learning(ICML),volume119ofProceedingsofMachine
LearningResearch,pp.6820–6829.PMLR,2020.
Saleh, E., Ghaffari, S., Bretl, T., and West, M. Truly de-
terministicpolicyoptimization. InAdvancesinNeural
Metelli, A. M., Papini, M., Faccio, F., and Restelli, M.
InformationProcessingSystems(NeurIPS),2022.
Policyoptimizationviaimportancesampling. InNeural
InformationandProcessingSystems(NeurIPS),pp.5447– Scherrer,B.andGeist,M. Localpolicysearchinaconvex
5459,2018. spaceandconservativepolicyiterationasboostedpolicy
10LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
search. InMachineLearningandKnowledgeDiscovery Xu,P.,Gao,F.,andGu,Q. Sampleefficientpolicygradient
in Databases: European Conference (ECML PKDD), methodswithrecursivevariancereduction. InInterna-
volume8726ofLectureNotesinComputerScience,pp. tionalConferenceonLearningRepresentations(ICLR).
35–50.Springer,2014. OpenReview.net,2020.
Schwefel,H.-P.P.Evolutionandoptimumseeking:thesixth Yuan,R.,Gower,R.M.,andLazaric,A. Ageneralsample
generation. JohnWiley&Sons,Inc.,1993. complexityanalysisofvanillapolicygradient. InInterna-
tionalConferenceonArtificialIntelligenceandStatistics
Sehnke,F.,Osendorfer,C.,Ru¨ckstieß,T.,Graves,A.,Pe-
(AISTATS),pp.3332–3380.PMLR,2022.
ters,J.,andSchmidhuber,J. Parameter-exploringpolicy
gradients. NeuralNetworks,23(4):551–559,2010. The Zhao,T.,Hachiya,H.,Niu,G.,andSugiyama,M. Analysis
InternationalConferenceonArtificialNeuralNetworks andimprovementofpolicygradientestimation.InNeural
(ICANN). InformationandProcessingSystems(NeurIPS),pp.262–
270,2011.
Shani,L.,Efroni,Y.,andMannor,S. Adaptivetrustregion
policyoptimization: Globalconvergenceandfasterrates
forregularizedmdps. InAAAI,pp.5668–5675.AAAI
Press,2020.
Shen,Z.,Ribeiro,A.,Hassani,H.,Qian,H.,andMi,C.Hes-
sianaidedpolicygradient. InInternationalConference
onMachineLearning(ICML),volume97ofProceedings
ofMachineLearningResearch,pp.5729–5738.PMLR,
2019.
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,
andRiedmiller,M. Deterministicpolicygradientalgo-
rithms.InInternationalConferenceonMachineLearning
(ICML),pp.387–395.PMLR,2014.
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An
introduction. MITpress,2018.
Sutton, R.S., McAllester, D., Singh, S., andMansour, Y.
Policygradientmethodsforreinforcementlearningwith
functionapproximation. AdvancesinNeuralInformation
ProcessingSystems(NeurIPS),12,1999.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
engineformodel-basedcontrol. In2012IEEE/RSJinter-
nationalconferenceonintelligentrobotsandsystems,pp.
5026–5033.IEEE,2012.
Williams,R.J. Simplestatisticalgradient-followingalgo-
rithmsforconnectionistreinforcementlearning. Machine
learning,8:229–256,1992.
Xiong,H.,Xu,T.,Zhao,L.,Liang,Y.,andZhang,W. De-
terministicpolicygradient: Convergenceanalysis. InUn-
certaintyinArtificialIntelligence(UAI,pp.2159–2169.
PMLR,2022.
Xu,P.,Gao,F.,andGu,Q. Animprovedconvergenceanal-
ysis of stochastic variance-reduced policy gradient. In
ProceedingsoftheConferenceonUncertaintyinArtifi-
cial Intelligence (UAI), volume 115 of Proceedings of
MachineLearningResearch,pp.541–551.AUAIPress,
2019.
11LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
A.AssumptionsandConstants: QuickReference
AsmentionedinSection7,wecanstartfromfundamentalassumptionsontheMDPandthe(hyper)policyclassestosatisfy
moreabstractassumptionsthatcanbeuseddirectlyinconvergenceanalyses. Figure2showstherelationshipbetweenthe
assumptions,andTable2theconstantsobtainedintheprocess. Allproofsoftheassumptions’implicationscanbefoundin
AppendixE.
L (Lipschitz) L (Smooth) V (Variancebound)
: 2,: :
ABExploration LpRmax Lr 2L2 pL2 µRmax 2L2 µLpLr`L2,µL2,pRmax L2,µL2,r RmaxcpdA`1qpL2 µ`L2,µq; RmaxcdAL2 µ
( =A)
p1´γq2 `1´γ p1´γq3 ` p1´γq2 ` 1´γ σ A2p1´γq2 σ A2p1´γq3
:
Assumptions: 4.1 4.1,4.3,4.2,4.4 4.3,4.4,4.5 4.5,4.3
Reference: LemmaE.1 LemmaD.7 LemmaD.6
PBExploration LpLµRmax LrLµ 2L2 pL2 µRmax 2L2 µLpLr`L2,µL2,pRmax L2,µL2,r RmaxcpdΘ`1q RmaxcdΘ
( =P)
p1´γq2 ` 1´γ p1´γq3 ` p1´γq2 ` 1´γ σ P2p1´γq2 σ P2p1´γq2
:
Assumptions: 4.1,4.3 4.1,4.3,4.2,4.4 4.5 4.5
Reference: LemmaE.1 LemmaD.3 LemmaD.2
Table2.BoundstotheLipschitzandsmoothnessconstantsfortheABandPBobjectives(J andJ )andvarianceoftheGPOMDPand
A P ?
PGPEestimators.BothpresentedboundsonL 2,:holdunderdifferentsetsofassumptions.;ifσ Aă d A.
12LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
13
J
dna J no snoitpmussA
P
A
J
no
snoitpmussA
D
ycilop
citsinimreted
dna
PDM
no
snoitpmussA
θ .t.r.w SL- L J A,2 A )2.6 noitpmussA(
µ
.t.r.w
CL-L
DJ
............................
)2.5
noitpmussA(
r
dna
pgol
............................
)
qµ,2L`µ2
2L qγpq ´1 1`ď pA A2A σd,2 pcL xamR ,2L!
nim
)1γr
´ .L E1`
nox 2a iq tm iγ sR ´ o1p ppL oď
rPL
(
a ). 1t
..r 4.w
noC
itL p- mq
r uL
ss,
ApL
(p
)7.D ammeL( θ .t.r.w SL- P,2L PJ
θ
.t.r.w
CL- JL
DJ
.).....)
.2 .. .6 .n ..o .it .p .m ..u .!s .s .A ..( ......
......)
.1
.. .5
.n
..o .it .p .m ..u .s .s
.A ..(
......
θ
.t.r.w
CL-
µL
θµ
q1`
2qΘ γd ´ )p 31c .px D2 Pa σm aR m,2 mL eLn (imďP,2L
µ γL ´ )1r
1L .E`
nx oam 2 itqR iγ s´µ oL 1 ppp oL
rPď
(JL
)3.4
noitpmussA(
θ
.t.r.w
SL- L
J
2
D
)1.E
noitpmussA(
............................
D )1G .6W n- oq iA tβ pm, A uα sp sAA (J
`xam
3qR γµ2 ´L 1p p2L2 ď2L
S
)L
2-
.4q r n,2
oL it,
pp m,2
uL sp sAr
(d
ana
.tp .r.g
wol
`xamRp,2L
2µ q, γ2 ´L 1` prLpLµ2L2
)2.E
ammeL(
r,2Lµ,2L
γ´1
DGW-q Pβ, Pαp PJ
DG)W(-q
Dβ, Dαp
DJ
.t.r.w
SL-
µ,2L
θµ
)1.6 noitpmussA(
)1.7
noitpmussA(
)4.4
noitpmussA(
θ
.repapehtnidetneserpsnoitpmussaehtfoemosfoycnednepeD
.2erugiFLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
B.RelatedWorks
Inthissection,weprovideadiscussionofpreviousworksthataddressedsimilarquestionstotheonesconsideredinthis
paper.
Convergencerates. TheconvergenceofPGtostationarypointsatarateofO ϵ´4 wasclearatleastsince(Suttonetal.,
p q
1999),althoughtherecentworkbyYuanetal.(2022)clarifiesseveralaspectsoftheanalysisandtherequiredassumptions.
VariantsofREINFORCEwithfasterconvergence,basedonstochasticvariancereduction,wereexploredmuchlater(Papini
etal.,2018;Xuetal.,2019),andtheO ϵ´3 rateof(Xuetal.,2020)isnowbelievedtobeoptimalduetolowerbounds
p q
fromnonconvexstochasticoptimization(Arjevanietal.,2023). Thesameholdsforsecond-ordermethods(Shenetal.,2019;
Arjevanietal.,2020). AlthoughtheconvergencepropertiesofPGPEareanalogoustothoseofPG,theyhavenotreceived
thesameattention,withtheexceptionof(Xuetal.,2020),wheretheO ϵ´3 rateisprovedforavariance-reducedversionof
p q
PGPE.StudyingtheconvergenceofPGtogloballyoptimalpoliciesunderadditionalassumptionsisamorerecentendeavor,
pioneeredbyworkssuchasScherrer&Geist(2014), Fazeletal.(2018),Bhandari&Russo(2024). Theseworksintroduced
tothepolicygradientliteraturetheconceptofgradientdomination,orgradientdominance,orPolyak-Łojasiewiczcondition,
whichhasalonghistoryintheoptimizationliterature(Lojasiewicz,1963;Polyaketal.,1963;Karimietal.,2016). Several
worksstudytheiterationcomplexityofpolicygradientwithexactgradients(e.g., Agarwaletal.,2021;Meietal.,2020;Li
etal.,2021). Theseresultsarerestrictedtospecificpolicyclasses(e.g.,softmax,directtabularparametrization)forwhich
gradientdominationisguaranteed. Anotableexceptionisthestudyofsample-basednaturalpolicygradientforgeneral
smoothpolicies(Agarwaletal.,2021). Asforvanillasample-basedPG(i.e.,GPOMDP),Liuetal.(2020)werethefirst
tostudythesamplecomplexityofthisalgorithminconvergingtoaglobaloptimum. Theyalsointroducedtheconcept
ofFisher-non-degeneracy(Dingetal.,2022),whichallowstoexploitaformofgradientdominationforageneralclass
r
ofpolicies. Wereferthereaderto(Yuanetal.,2022)whichachievesabetterO ϵ´3 samplecomplexityunderweaker
p q
assumptions. Moresophisticatedalgorithms,suchasvariance-reducedmethodsmentionedabove,canachieveevenbetter
r r
sample complexity. The current state of the art is (Fatkhullin et al., 2023b): O ϵ´2.5 for hessian-free and O ϵ´2 for
p q p q
second-orderalgorithms. Thelatterisoptimaluptologarithmicterms(Azaretal.,2013). WheninstantiatedtoGaussian
policies,alloftheworksmentionedinthisparagraphimplicitlyassumethatthecovarianceparametersarefixed. Inthis
r
case,ourTheoremD.4recoverstheO ϵ´3 rateofYuanetal.(2022,Corollary3.7),thebest-knownresultforGPOMDP
p q
undergeneralWGD.
Deterministicpolicies. Value-basedRLalgorithms,suchasQ-learning,naturallyproducedeterministicpoliciesastheir
finalsolution,whilemostpolicy-gradientmethodsmustsearch,bydesign,inaspaceofnon-degeneratestochasticpolicies.
In(Suttonetal.,1999)thisispresentedasanopportunity, ratherthanasalimitation, sincetheoptimalpolicyisoften
stochasticforpartially-observableproblems. Thepossibilityofdeployingdeterministicpoliciesonlyisoneoftheappealsof
PGPEandrelatedevolutionarytechniques(Schwefel,1993),butalsoofmodel-basedapproaches(Deisenroth&Rasmussen,
2011). Inthecontextofaction-basedpolicysearch,theDPGalgorithmbySilveretal.(2014)wasthefirsttosearchin
a space of deterministic policies. Differently from PGPE, stochastic policies are run during the learning process from
explorationpurposes,similarlytovalue-basedmethods. Moreover,thedistributionmismatchduetooff-policysamplingis
largelyignored. Nonetheless,populardeepRLalgorithmswerederivedfromDPG(Lillicrapetal.,2016;Fujimotoetal.,
2018). (Xiongetal.,2022)provedtheconvergenceofon-policy(hence,fullydeterministic)DPGtoastationarypoint,with
O ϵ´4 samplecomplexity. However,theyrelyonanexplorabilityassumption(Assumption4intheirpaper)thatisstandard
p q
forstochasticpolicies,butverydemandingfordeterministicpolicies. Amorepracticalwayofachievingfullydeterministic
DPGwasproposedbySalehetal.(2022),whoalsoprovideathoroughdiscussionoftheadvantagesofdeterministicpolicies.
Unsurprisingly,trulydeterministiclearningisonlypossibleunderstrongassumptionsontheregularityoftheenvironment.
Inthispaper,forPG,weconsideredthemorecommonscenarioofevaluatingstochasticpoliciesattrainingtime,onlyto
deployagooddeterministicpolicyintheend. PGPE,bydesign,doesthesamewithhyperpolicies.
Policyvariance. WhenoptimizingGaussianpolicieswithpolicy-gradientmethods,thescaleparameters(thoseofthe
varianceor,moreingeneral,ofthecovariancematrixofthepolicy)aretypicallyfixedintheory,andoptimizedviagradient
descent in practice. To the best of our knowledge, there is no satisfying theory of the effects of a varying policy (or
hyperpolicy)varianceontheconvergenceratesofPG(orPGPE). Ahmedetal.(2019)werethefirsttotakeintoserious
considerationtheimpactofthepolicystochasticityonthegeometryoftheobjectivefunction,althoughtheirfocuswason
entropyregularization. Papinietal.(2020),focusingonmonotonicimprovementratherthanconvergence,proposedto
usesecond-orderinformationtoovercomethegreedinessofgradientupdates,arguingthatthelatterisparticularlyharmful
forscaleparameters.Bollandetal.(2023)proposetostudyPGwithGaussianpoliciesunderthelensofoptimizationby
14LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
continuation(Allgower&Georg,1990),thatis,asasequenceofsmoothedversionofthedeterministicpolicyoptimization
problem. Unfortunately,thetheoryofoptimizationbycontinuationisratherscarce. Westudiedtheimpactofafixedpolicy
varianceonthenumberofsamplesneededtofindagooddeterministicpolicy. Wehopethatthiscanprovidesomeinsight
onhowtodesignadaptivepolicy-variancestrategiesinfuturework. Weremarkherethatthecommonpracticeoflearning
theexplorationparameterstogetherwithalltheotherpolicyparametersbreaksalloftheknownconvergenceresultsof
GPOMDP,sincethesmoothnessofthestochasticobjectiveisinverselyproportionaltothepolicyvariance(Papinietal.,
2022). Inthisregard,entropy-regularizedpolicyoptimizationisdifferent,andisbetterstudiedusingmirrordescenttheory,
ratherthanstochasticgradientdescenttheory(Shanietal.,2020).
ComparingABandPBexploration. AclassiconthetopicisthepaperbyZhaoetal.(2011). Theyproveupperbounds
onthevarianceoftheREINFORCEandPGPEestimators,highlightingthebetterdependenceonthetaskhorizonofthe
latter. Theideathatvariancereductiondoesnottellthewholestoryabouttheefficiencyofpolicygradientmethodsisrather
recent(Ahmedetal.,2019). Werevisitedthecomparisonofaction-basedandparameterbasedmethodsunderthelensof
modernsamplecomplexitytheory. Wereachedsimilarconclusionsbutachieved,webelieve,amorecompleteunderstanding
ofthematter. Toourknowledge,theonlyotherworkthatthoroughlycomparesABandPBexplorationis(Metellietal.,
2018),wherethetrade-offbetweenthetaskhorizonandthenumberofpolicyparametersisdiscussedbothintheoryand
experiments,butinthecontextoftrust-regionmethods.
C.AdditionalConsiderations
Weonlyconsidered(hyper)policyvariancesσ2,σ2thatarefixedforthedurationofthelearningprocess,albeittheycanbe
A P
setasfunctionsofproblem-dependentconstantsandofthedesiredaccuracyϵ. Thisisduetoourfocusonconvergence
guaranteesbasedonsmoothoptimizationtheory,asexplainedinthefollowing.
RemarkC.1(Aboutlearningthe(hyper)policyvariance). Itisawellestablishedpracticetoparametrizethepolicyvariance
and learn these exploration parameters via gradient descent together with all the other policy parameters (again, for
examples,seeDuanetal.,2016;Raffinetal.,2021a). Thesameistrueforparameter-basedexploration(Schwefel,1993;
Sehnkeetal.,2010). However,itiseasytoseethatanadaptive(inthesenseoftime-varying)policyvariancebreaksthe
samplecomplexityguaranteesofGPOMDP(Yuanetal.,2022)anditsvariance-reducedvariants(e.g.,Liuetal.,2020).
That is because these guarantees all rely on Assumption 6.2, or equivalent smoothness conditions, and obtain sample
complexityupperboundsthatscalewiththesmoothnessconstantL . However,thelattercandependinverselyonσ2,as
2,A A
alreadyobservedbyPapinietal.(2022)forGaussianpolicies. Thus,unconstrainedlearningofσ breakstheconvergence
A
guarantees. AnalogousconsiderationsholdforPGPEwithadaptivehyperpolicyvariance. Differentconsiderationsapplyto
entropy-regularizedpolicyoptimizationmethods,whichwerenotconsideredinthispaper,mostlybecausetheyconvergetoa
surrogateobjectivethatisevenfurtherfromoptimaldeterministicperformance. Thesemethodsarebetteranalyzedusing
thetheoryofmirrordescent. Wereferthereaderto(Shanietal.,2020).
Inordertoproperlydefinethewhitenoise-based(hyper)policies, weneedthatµ s ϵ A(forABexploration)and
θ
θ ϵ Θ(forPBexploration),wewillassumethatA RdA andΘ RdΘ forsimplicp itq y` . P
` P “ “
RemarkC.2(AboutA RdA andΘ RdΘ assumption). WehaveassumedthattheactionspaceAandtheparameter
spaceΘcorrespondtoR“
dA
andRdΘ,“
respectively. Ifthisisnotthecase,wecaneasilyalterthetransitionmodelpandthe
rewardfunctionr(fortheABexploration),andthedeterministicpolicyµ (forthePBexploration)bymeansofaretraction
θ
function. LetX Rd beameasurableset,aretractionfunctionι :Rd X issuchthatι x xifx X,i.e.,itisthe
X X
Ď Ñ p q“ P
identityoverX.
• For the AB exploration, we redefine the transition model as p s1 s,a : p s1 s,ι a for every s,s1 S and a A.
A
p | q “ p | p qq P P
Furthermore,weredefinetherewardfunctionasr s,a : r s,ι a foreverys S anda A.
A
p q “ p p qq P P
• ForthePBexploration,weredefinethedeterministicpolicyasµ s : µ ,foreveryθ Θ.
θp q “ ιΘpθq P
15LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
D.Proofs
D.1.ProofsfromSection5
LemmaD.1. LetL 0,considerthefunctionf:R Rdefinedforeveryx Rasfollows:
$
ą Ñ P
’&0 ifx 1 Lorx 2 L
ă´ { ą {
f x Lx 1 if 1 L x 0 . (16)
p
q“’%
` ´ { ď ă
1 Lx if0 x 2 L
´ 2 ď ď {
Considerthefunctionψ :R R definedforeveryx Rasfollows:
σ ě0 #
Ñ P
?1 if ?3σ x ?3σ
ψ x 2 3σ ´ ď ď , (17)
σ
p q“ 0 otherwise
i.e.,thep.d.f. ofauniformdistributionwithzeromeanandvarianceσ2. Letf σ: “f ˚ψ σ,letx˚ “argmax xPRf px q,andlet
x˚ σ“argmax xPRf
σ
px q. Thenf isL-LCand,if?3σ ď1 {L,itholdsthatf px˚ q´f px˚ σq“Lσ {p2?3 q.
Proof. Letusfirstverifythatthedistributionwhosep.d.f. isϕ haszeromeanandvarianceσ2:
ż σ
ψ x xdx 0, (18)
σ
R p q “
ż ż?
3σ
ψ x x2dx 2 ψ x x2dx σ2. (19)
σ σ
R p q “ 0 p q “
Undertheassumption?3σ 1 L,functionsf andψ canberepresentedasfollows:
σ
ď {
f x
p q
ψ x
σ
p q
x
1 ?3σ ?3σ 2
´L ´ L
Letusnowcomputetheconvolution:
ż
f x f ψ ψ x t f t dt. (20)
σ σ σ
p q“ ˚ “ R p ´ q p q
Itisclearthattheglobaloptimumoffunctionf islocatedintheintervalgivenby x 1 L. Thiscombined,withthe
σ
| |ď {
assumption?3σ 1 L,allowstosimplifytheintegralas:
ď {ż ż ż ? ˆ ˙
0 1 x` 3σ 1 L
ψ x t f t dt Lt 1 dt 1 t dt (21)
R σ p ´ q p q “ x´? 3σ 2? ˆ3σp ` q ` 0 2?3σ ˙ ´ 2
1 L L
1 x ?3σ 2 x ?3σ 2 . (22)
´“2?3σ 2p ´ q ` 4p ` q
Thelatterisaconcave(quadratic)functionofx,whichismaximizedforx˚ σ“σ {?3.Noticingthatx˚ “argmax xPRf px q=0,
wehave: ˆ ˙
Lσ Lσ
f x˚ f x˚ f 0 f σ ?3 1 1 . (23)
p q´ p σq“ p q´ p { q“ ´ ´2?3 “2?3
Theorem5.1(DeterministicdeploymentofparameterslearnedwithPBwhite-noiseexploration). Ifthehyperpolicy
complieswithDefinition3.3,underAssumption5.1:
16LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
(i) (Uniformbound)foreveryθ Θ,itholdsthat J θ J θ L ?d σ ;
D P J Θ P
P | p q´ p q|ď
(ii) (J upperbound)Letθ˚ argmax J θ ,itholdsthat: J˚ J θ˚ 2L ?d σ ;
D PP θPΘ P p q D´ D p Pqď J Θ P
(iii) (J lower bound) There exists an MDP, a deterministic policy class µ fulfilling Assumption 5.1, and a noise
D θ
complyingwithDefinition3.1,suchthatJ˚ J θ˚ 0.28L ?d σ .
D´ D p Pqě J Θ P
Proof. Beforestartingthederivation,weremarkthat:
J θ E J θ ϵ , (24)
P D
p q“ϵ„ΦdΘr p ` qs
whereE ϵ 2 d σ2. FromAssumption5.1,wecaneasilyderive(i):
ϵ„ΦdΘr} }2sď Θ P
J θ J θ J θ E J θ ϵ (25)
D P D D
| p q´ p q|“| p q´ϵ„ΦdΘr p ` qs|
E J θ J θ ϵ (26)
D D
ďϵ„ΦdΘr| p q´ p ` q|s
L E ϵ (27)
J 2
ď cϵ„ΦdΘr} } s
L E ϵ 2 (28)
ď J ϵ a„ΦdΘr} }2s
L σ d . (29)
J P Θ
ď
For(ii),letθ˚ argmax J θ ,wehave:
P θPΘ D p q
maxJ θ J θ˚ J θ˚ J θ˚ J θ˚ (30)
θPΘ D p q´ D p Pq“ D p q´ D p Pq˘ P p q
J θ˚ J θ˚ J θ˚ J θ˚ (31)
ď D p q´ P p q` P p Pq´ D p Pq
2max J θ J θ (32)
D P
ď θPΘ|ap q´ p q|
2L σ d , (33)
J P Θ
ď
whereline(31)followsfromJ θ˚ max J θ J θ˚ ,andline(32),followsbyapplyingtwiceresult(i).
P p Pq“ θPΘ P p qě P p q ř
Toprove(iii)weconstructtheMDP pts u,RdΘ,p,r,ρ 0,γ q(i.e.,abandit),wherer ps,a q“d1
Θ
d i“Θ 1f pa
i
q,wheref isdefined
inLemmaD.1andµ
θ
s θwithθ RdΘ. Thus,wecancomputetheexpectedreturnasfollows:
p q“ P
1 γT 1
ÿdΘ
J θ ´ f θ . (34)
D i
p q“ 1 γ ¨d p q
´
Θi“1
LetuscomputeitsLipschitzconstantrecallingthatf isL-LCthankstoLemmaD.1. Inparticular,wetakeθ 0 and
“
dΘ
θ1 η1 withη 0,1 L ,recallingthat θ θ1 η?d andthatf θ 1andf θ1 ηL 1,wehave:
“´ dΘ Pp { q } ´ˇ }2 “ Θ p i q“ p iq“ˇ´ `
ˇ
ˇ1 γT 1
ÿdΘ
1 γT 1
ÿdΘ ˇ
ˇ
J θ J θ1 ˇ ´ f θ ´ f θ1 ˇ (35)
| D p q´ D p q|“ˇ 1 γ ¨d p i q´ 1 γ ¨d p iqˇ
´
Θi“1
´
Θi“1
1 γT 1
ÿdΘ
´ f θ f θ1 (36)
“ 1 γ ¨d | p i q´ p iq|
´
Θi“1
1 γT L
ÿdΘ
´ θ θ1 (37)
“ 1 γ ¨d | i ´ i|
´
Θi“1
1 γT
´ Lη (38)
“ 1 γ
´
1 γT L
´ θ θ1 . (39)
“ 1 γ ¨?d } ´ }2
´ ¯ ´ Θ
Thus,wehavethatJ
D
pθ qis 1 1´ ´γ γT ¨?L
dΘ
-LC.BynamingL
J
“1 1´ ´γ γT ¨?L dΘ,wehaveL “11 ´´ γγ T?d ΘL J. Wenowcon-
sidertheadditivenoiseΦ dΘ Uni ?3σ,?3σ ,i.e.,thed -dimensionaluniformdistributionwithindependent
dΘ“bi“1
pr´ sq
Θ
components over the hypercube ?3σ,?3σ dΘ. From Lemma D.1, we know that each dimension has variance σ2,
r´ s
17LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
consequently:
ÿdΘ
E ϵ 2 E ϵ2 d σ2, (40)
ϵ„ΦdΘr} }2s“ i“1ϵi„Unipr´? 3σ,? 3σsqr is“ Θ
thuscomplyingwithDefinition3.2. Consequently:
ÿdΘ ÿdΘ
J θ E J θ ϵ E f θ ϵ f ψ θ , (41)
P p q“ϵ„ΦdΘr D p ` qs“ i“1ϵi„Unipr´? 3σ,? 3σsqr p i ` i qs“ i“1p ˚ σ qp i q
whereψ isthep.d.f. oftheconsidereduniformdistributionasdefinedinLemmaD.1. FromLemmaD.1andobservingthat
σ
bothJ andJ decomposeintoasumoverthed dimensions,wehavefor?3σ 1 L:
D P Θ
ă {
σ
θ˚ argmaxJ θ 0 , θ˚ argmaxJ θ 1 . (42)
“ θPRdΘ D p q“ dΘ P“ θPRdΘ P p q“?3 dΘ
Itfollowsthat: ˆ ˙
σ
J θ˚ J θ˚ J 0 J 1 (43)
D p q´ D p Pq“ D p dΘq´ D ?3 dΘ
1 γT 1
ÿdΘ
´ f 0 f σ ?3 (44)
“ 1 γ d p q´ p { q
´
Θi“1
1 γT Lσ
´ (45)
“ 1 γ 2?3
´ a
1
L d σ. (46)
“2?3 J Θ
Theorem5.2(DeterministicdeploymentofparameterslearnedwithABwhite-noiseexploration). Ifthepolicycomplies
withDefinition3.2andunderAssumption5.2:
(i) (Uniformbound)foreveryθ Θ,itholdsthat: J θ J θ L?d σ ;
D A A A
P | p q´ p q|ď
(ii) (J upperbound)Lettingθ˚ argmax J θ ,itholdsthatJ˚ J θ˚ 2L?d σ ;
D AP θPΘ A p q D´ D p Aqď A A
(iii) (J lower bound) There exists an MDP, a deterministic policy class µ fulfilling Assumption 5.1, and a noise
D θ
complyingwithDefinition3.1,suchthatJ˚ J θ˚ 0.28L?d σ .
D´ D p Aqě A A
Proof. FromAssumption5.2,notingthatJ θ J µ wecaneasilyderive(i):
D p ˇq“ D p θq ˇ
ˇ ” ıˇ
ˇ ˇ
J θ J θ ˇJ θ E J µ ϵ ˇ (47)
| D p q´ A p q|“ˇ D p q´ϵ„ΦT D p θ` q ˇ
ˇ dA ˇ
ˇ ” ıˇ
ˇ ˇ
ˇJ µ E J µ ϵ ˇ (48)
“ˇ D p θq´ϵ„ΦT D p θ` q ˇ
« dA ff
Tÿ´1
E L sup µ s µ s ϵ (49)
ďϵ„ΦT
dA t“0
t stPS} θ p t q´p θ p t q` t q}2
Tÿ´1
L E ϵ 2 (50)
“
t“a0
t ϵ„ΦdAr} }2s
L d σ . (51)
A A
ď
For(ii),letθ˚ argmax J θ ,wehave:
P θPΘ D p q
maxJ θ J θ˚ J θ˚ J θ˚ J θ˚ (52)
θPΘ D p q´ D p Aq“ D p q´ D p Aq˘ A p q
J θ˚ J θ˚ J θ˚ J θ˚ (53)
ď D p q´ A p q` A p Aq´ D p Aq
2max J θ J θ (54)
D A
ď θPΘa| p q´ p q|
2Lσ d , (55)
A A
ď
18LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
whereline(53)followsfromJ θ˚ max J θ J θ˚ ,andline(54)followsbyapplyingtwiceresult(i). The
A p Aq“ θPΘ A p qě A p q
proof of iii isidentical tothat ofTheorem 5.1since, for theparticular instance, wehave enforced µ s θ (which
θ
p q p q“
impliesd d )and,thus,ABexplorationisequivalenttoPBexploration.
A Θ
“
D.2.ProofsfromSection6
p
LemmaD.2(Varianceof∇ J θ bounded). UnderAssumption4.5,thevariancethePGPEestimatorwithbatchsizeN
θ P
p q
isboundedforeveryθ Θas:
P ” ı
Var ∇p θJ P pθ
q
ďR m2 a Nxξ 2 1p1 ´ γγ 2T q2 ďNR m2 1axξ γ12 2.
p ´ q p ´ q
withξ cd σ´2.
2 ď Θ P
Proof. WerecallthattheestimatoremployedbyPGPEinitsupdateruleis:
p 1
ÿN
∇ J θ ∇ logν θ R τ ,
θ P θ θ i i
p q“N p q p q
i“1
whereN isthenumberofparameterconfigurationtested(ononetrajectory)ateachiteration. Thus,wecancomputethe
varianceofsuchanestimatoras:
” ı “ ‰
Var ∇p J θ1 1 Var ∇ logν θ1 R τ
θ P θ θ 1
θ1„νθ p q “N θ1„νθ”› p q ›p q
ı
1 E › ∇ logν θ1 ›2 R τ 2
“N θ1„νθ θ θ p q 2 p 1 q
R2 ξ2 1 γT 2
max 1p ´ q ,
ď N 1 γ 2
p ´ q ” ı
wherethelastlinefollowsformAssumption4.5andLemmaE.4afterhavingdefinedξ E ∇ logν θ1 2 and
2 “ θ1„νθ } θ θ p q}2
fromthefactthat,givenatrajectoryτ,R τ isdefinedas:
p q
Tÿ´1
R τ γtr s ,a ,
τ,t τ,t
p q“ p q
t“0
withr s,a R ,R foreverys S anda A.
max max
p qPr´ s P P
LemmaD.3(BoundedJ Hessian). UnderAssumption4.5andusingahyperpolicycomplyingwithDefinition3.2, θ Θ
P
@ P
itholdsthat:
› › ∇2 θJ P pθ q› › 2ďL 2,PR max 1p1 ´
γ
γT q pξ 2 `ξ 3 q,
´
where ξ cd σ´2 and ξ cσ´2. Furthermore, under Assumptions 4.1, 4.3, 4.2, and 4.4, and using a hyperpolicy
2 ď Θ P 3 ď P
complyingwithDefinition3.2, θ Θitholdsthat:
› ›
@ P › ›
∇2J θ L ,
θ P p q 2ď 2
whereL isboundedasinLemmaE.2.
2
Proof. TheperformanceindexJ ofahyperpolicyν canbeseenastheexpectationoverthesamplingofaparameter
P θ
configurationθ1fromthehyperpolicyν ,orastheperturbationaccordingtotherealizationϵofasub-gaussiannoiseσ of
θ P
theparameterconfigurationofthedeterministicpolicyµ .
θ
Usingthefirstcharacterizationwecanwrite:
“ ‰
J θ E J θ1 . (56)
P D
p q“θ1„νθ p q
Equivalently,wecanwrite:
J θ E J θ ϵ . (57)
P D
p q“ϵ„Φr p ` qs
19LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Byusingthelatter,wehavethat:
› ›
› › › ›
› ∇2 θJ P pθ q› 2“› ›∇2 θ ϵE „grJ D pθ `ϵ qs› ›
› ›2
› “ ‰›
“› › ϵE
„g
∇2 θJ D pθ `ϵ
q
› ›
“› › 2‰
E › ∇2J θ ϵ ›
ďϵ„g θ D p ` q 2
L , (58)
2
ď
wherethelastinequalitysimplyfollowsfromAssumptionE.1.
ByusingEquation(56),instead,wehavethefollowing:
“ ‰
∇2J θ ∇2 E J θ1
θ P p q“ż θ θ1„νθ D p q
` ˘
∇2 ν θ1 J θ1 dθ1
“ż θ θ p q D p q
` ˘
∇ ∇ ν θ1 J θ1 ν θ1 ∇ J θ1 dθ1
θ θ θ D θ θ D
“ż p q p q` p q p q
` ` ˘˘
∇ ν θ1 ∇ logν θ1 J θ1 dθ1
θ θ θ θ D
“ p q p q p q
ż ˆ ˙
∇ ν θ1 ∇ logν θ1 J θ1 ν θ1 ∇2logν θ1 J θ1 ∇ logν θ1 ∇ J θ1 dθ1
“ θ θ p q θ θ p q D p q` θ p q θ θ p q D p q` θ θ p q θ D p q
ż ˆ ˙
ν θ1 ∇ logν θ1 ∇ logν θ1 JJ θ1 ∇2logν θ1 J θ1 dθ1
“ θ p q θ θ p q θ θ p q D p q` θ θ p q D p q
„ˆ ˙ ȷ
E ∇ logν θ1 ∇ logν θ1 J ∇2logν θ1 J θ1 .
“θ1„νθ θ θ p q θ θ p q ` θ θ p q D p q
Now,giventhepreviousargument,itfollowsthat:
› „ˆ ˙ ȷ›
› › › ›
› ∇2 θJ P pθ q› 2“› › θ1„E
νθ”›
∇ θlogν θ pθ ›1 q∇
ˇ
θlogν ˇθ pθ ›1 qJ `∇2 θlogν ›θ
p
ˇθ1
q
J D
ˇp
ıθ1
q
› ›
2
E › ∇ logν θ1 ›2ˇ J θ1 ˇ › ∇2logν θ1 › ˇ J θ1 ˇ
ďθ1„νθ θ θ p q 2 D p q ` θ θ p q 2 D p q
R 1 γT
max p ´ q ξ ξ . (59)
2 3
ď 1 γ p ` q
´
WeemployLemmaE.4toboundξ andξ .
2 3
Theorem D.4 (Global convergence of PGPE - Fixed σ ). Under Assumptions 6.1 (with J J ), 4.1, 4.3, 4.5, with
P : P
asuitableconstantstepsize,toguaranteeJ˚ E J θ ϵ β 3L ?d σ ,where3L ?“ d σ O ?d σ 1
D´ r D p K qsď ` ` P Θ P P Θ P “ p Θ P p ´
γ ´2 thesamplecomplexityofPGPEisatmost:
q q ˆ ˙
r
α4d2
NK O Θ . (60)
“ σ4 1 γ 4ϵ3
Pp ´ q
Furthermore,underAssumptions4.2and4.4,thesameguaranteeisobtainedwithasamplecomplexityatmost:
ˆ ˙
NK Or
α4d
Θ . (61)
“ σ2 1 γ 5ϵ3
Pp ´ q
Proof. WefirstapplyTheoremF.1withJ J ,recallingthattheassumptionsenforcedinthestatemententailthoseof
: P
“
TheoremF.1:
16α2L V max 0,J ˚ J θ β
J ˚ E J θ ϵ β with NK 2,P P log t P ´ P p 0 q´ u. (62)
P ´ r P p K qsď ` “ ϵ3 ϵ3
ByTheorem5.1(i)and(ii),wehavethat:
a
J˚ E J θ J˚ J ˚ E J θ J θ J ˚ E J θ J ˚ E J θ 3L d σ . (63)
D´ r D p K qs“p D´ P q` r P p K q´ D p K qs` P ´ r P p K qsď P ´ r P p K qs` J Θ P
20LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
After renaming L : L for the sake of exposition, the result follows by replacing in the sample complexity NK the
P J
“
boundsonL ,L ,andV fromTable2underthetwosetofassumptionsandretainingonlythedesireddependenceswith
P 2,P P
r
theBig-Onotation.
Theorem D.5 (Global convergence of PGPE - ϵ-adaptive σ ). Under Assumptions 6.1 (with J J ), 4.1, 4.3, 4.5,
P : P
“
withasuitableconstantstepsizeandσ
P
“6LPϵ? dΘ“O pϵ p1 ´γ q2d´ Θ1{2 q,toguaranteeJ D˚ ´E rJ
D
pθ
K
qsďϵ `βthesample
complexityofPGPEisatmost:
ˆ ˙
r
α4d4
NK O Θ . (64)
“ 1 γ 12ϵ7
p ´ q
Furthermore,underAssumptions4.2and4.4,thesameguaranteeisobtainedwithasamplecomplexityatmost:
ˆ ˙
r
α4d2
NK O Θ . (65)
“ 1 γ 9ϵ5
p ´ q
Proof. WeapplyTheoremD.4withϵ ϵ 2andsetσ sothat:
Ð { a P
ϵ ϵ
3L d σ σ . (66)
J Θ P “2 ùñ P “6L ?d
J Θ
AfterrenamingL : L forthesakeofexposition,theresultfollowssubstitutingthisvalueinthesamplecomplexityand
P J
“
boundingtheconstantL asinTable2.
P
p
LemmaD.6(Varianceof∇ J θ bounded). UnderAssumptions4.5and4.3,thevariancetheGPOMDPestimatorwith
θ A
p q
batchsizeN isboundedforeveryθ Θas:
P ” ı
Var ∇p θJ A pθ
q
ďR m2 Naxξ 12 p1 γ´ 3γT q ďNR m2 1ax γξ 2 3.
p ´ q p ´ q
withξ cd σ´2.
2 ď A A
Proof. ItfollowsfromLemma29ofPapinietal.(2022)andfromtheapplicationofLemmaE.3toboundξ .
2
LemmaD.7(BoundedJ Hessian). UnderAssumptions4.5,4.3,and4.4, θ Θitholdsthat:
A ` ˘
@ P
› › ∇2J θ › › R max 1 ´γT`1 υ υ ,
θ A p q 2ď 1 γ 2 p 2 ` 3 q
p ´ q
whereυ cd σ 2 L2 andυ cσ´2L2 c?d σ´1L .Furthermore,underAssumptions4.1,4.3,4.2,and4.4, θ Θ
2 ď A ´ A µ 3 ď A µ` A A 2,µ @ P
itholdsthat:
› ›
› ›
∇2J θ L ,
θ A p q 2ď 2
whereL isboundedinLemmaE.2.
2
Proof. UnderAssumption4.5, byaslightmodificationoftheproofofLemma4.4byYuanetal.(2022)(inwhichwe
considerafinitehorizonT),itfollowsthat:
` ˘ ` ˘
› › ∇2J θ › › R max 1 ´pT `1 qγT `TγT`1 υ υ R max 1 ´γT υ υ .
θ A p q 2ď 1 γ 2 p 1 ` 2 qď 1 γ 2 p 1 ` 2 q
p ´ q p ´ q
AsintheproofofTheoremE.1,weintroducethefollowingconvenientexpressionforthetrajectorydensityfunctionhaving
fixedasequenceofnoiseϵ ΦT :
„ dA
Tź´1
p τ;µ ϵ ρ s p s s ,µ s ϵ .
D p θ` q“ 0 p τ,0 q p τ,t`1 | τ,t θ p τ,t q` t q
t“0
ThisallowsustoexpressthefunctionJ θ ,foragenericθ Θ,as:
A p «qż P ff
Tÿ´1
J θ E p τ;µ ϵ γtr s ,µ s ϵ dτ .
A p q“ϵ„ΦT
dA τ
D p θ` q
t“0
p τ,t θ p τ,t q` t q
21LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Withaslightabuseofnotation,letuscallJ µ ϵ thefollowingquantity:
D θ
żp ` q
Tÿ´1
J µ ϵ : p τ;µ ϵ γtr s ,µ s ϵ dτ.
D p θ ` q “ D p θ` q p τ,t θ p τ,t q` t q
τ t“0
Now,consideringthenormofthehessianw.r.t.θofJ ,wehavethat:
› › A ”› › ı
› ∇2J θ › E › ∇2J µ ϵ › L ,
θ A p q 2ďϵ„ΦT θ D p θ ` q 2 ď 2
dA
whichfollowsfromAssumptionsE.1.
TheoremD.8(GlobalconvergenceofGPOMDP-Fixedσ ). UnderAssumptions6.1(withJ J ),4.1,4.3,4.4,4.5,with
A : A
a suitable constant step size, to guarantee J˚ E J θ ϵ β 3L ?d σ , where 3L“ ?d σ O ?d σ 1
D´ r D p K qsď ` ` A A A A A A “ p A A p ´
γ ´2 thesamplecomplexityofGPOMDPisatmost:
q q ˆ ˙
NK Or
α4d2
A . (67)
“ σ4 1 γ 5ϵ3
Ap ´ q
Furthermore,underAssumption4.2,thesameguaranteeisobtainedwithasamplecomplexityatmost:
ˆ ˙
r
α4d
A
NK O . (68)
“ σ2 1 γ 6ϵ3
Ap ´ q
Proof. WefirstapplyTheoremF.1withJ J ,recallingthattheassumptionsenforcedinthestatemententailthoseof
: A
“
TheoremF.1:
16α2L V max 0,J ˚ J θ β
J ˚ E J θ ϵ β with NK 2,A A log t A ´ A p 0 q´ u. (69)
A ´ r A p K qsď ` “ ϵ3 ϵ3
ByTheorem5.2(i)and(ii),wehavethat:
a
J˚ E J θ J˚ J ˚ E J θ J θ J ˚ E J θ J ˚ E J θ 3L d σ . (70)
D´ r D p K qs“p D´ A q` r A p K q´ D p K qs` A ´ r A p K qsď A ´ r A p K qs` A A
AfterrenamingL : Lforthesakeofexposition,theresultfollowsbyreplacinginthesamplecomplexityNK thebounds
A
“
onL ,L ,andV fromTable2underthetwosetofassumptionsandretainingonlythedesireddependenceswiththe
A 2,A A
r
Big-Onotation.
TheoremD.9(GlobalconvergenceofGPOMDP-ϵ-adaptiveσ ). UnderAssumptions6.1(withJ J ),4.1,4.3,4.4,
P : A
“
4.5,withasuitableconstantstepsizeandsettingσ
A
“6LAϵ? dA“O pϵ p1 ´γ q2d´ A1{2 q,toguaranteeJ D˚ ´E rJ
D
pθ
K
qsďϵ `β
thesamplecomplexityofGPOMDPisatmost:
ˆ ˙
NK Or
α4d4
A . (71)
“ 1 γ 13ϵ7
p ´ q
Furthermore,underAssumption4.2,thesameguaranteeisobtainedwithasamplecomplexityatmost:
ˆ ˙
NK Or
α4d2
A . (72)
“ 1 γ 10ϵ5
p ´ q
Proof. WeapplyTheoremD.8withϵ ϵ 2andsetσ sothat:
Ð { a A
ϵ ϵ
3L d σ σ . (73)
A A A
“2 ùñ “6L?d
A
AfterrenamingL : Lforthesakeofexposition,theresultfollowssubstitutingthisvalueinthesamplecomplexityand
A
“
boundingtheconstantL asinTable2.
A
D.3.ProofsfromSection7.1
LemmaD.10. UnderAssumptions7.1,4.1,4.3,4.2,4.4,andusingahyperpolicycomplyingwithDefinition3.2, θ Θit
@ P
holdsthat:
a
J˚ J θ α ∇ J θ β α L σ d .
D´ D p qď D } θ P p q}2 ` D ` D 2 P Θ
22LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Proof. Westartbyobservingthat
“ ‰
J θ E J θ1 E J θ ϵ .
P D D
p q“θ1„νθ p q “ϵ„Φr p ` qs
Fromthisfact,wecanproceedasfollows:
∇ J θ ∇ E J θ ϵ
θ P θ D
p q“ ϵ„Φr p ` qs
E ∇ J θ ϵ .
θ D
“ϵ„Φr p ` qs
For what follows, we define θ˜ as an intermediate parameter configuration between θ and θ ϵ. More formally, let
ϵ
λ 0,1 , then θ˜ λθ 1 λ θ ϵ . We can proceed by rewriting the term ∇ J θ ϵ e` xploiting the first-order
ϵ θ D
Pr s “ `p ´ qp ` q p ` q
Taylorexpansioncenteredinϵ: thereexistsaλ 0,1 suchthat
Pr s ” ı
E ∇ J θ ϵ E ∇ J θ ϵT∇2J θ˜
ϵ„gr θ D p ` qs“ϵ„g θ D p q`” θ D p ϵ qı
∇ J θ E ϵT∇2J θ˜ .
“ θ D p q`ϵ„Φ θ D p ϵ q
Now,wecanconsiderthe2-normofthegradient:
› ” ı› › ” ı›
› › › ›
∇ J θ ›∇ J θ E ϵT∇2J θ˜ › ∇ J θ › E ϵT∇2J θ˜ ›
} θ P p q}“ θ D p q`ϵ„Φ θ D p ϵ q 2ě} θ D p q}2 ´ ϵ„Φ θ D p ϵ q 2
∇ J θ L E ϵ (74)
θ D 2 2 2
ě} p q} ´ ϵ„Φr} } s
1 β
J˚ J θ D L E ϵ (75)
ěα Dp D´ D p qq´α D´ 2 ϵ„Φr} }2 s
a
1 β
J˚ J θ D L σ d ,
ěα p D´ D p qq´α ´ 2 P Θ
D D
whereEquation(74)followsfromAssumptionE.1,andEquation(75)followsfromAssumption6.1. Thus,itsimplyfollows
that:
a
J˚ J θ α ∇ J θ β α L σ d .
D´ D p qď D } θ P p q}2 ` D ` D 2 P Θ
Theorem7.1(InheritedweakgradientdominationforJ ). UnderAssumptions7.1,4.1,4.3,4.2,4.4,foreveryθ Θ:
P a P
J ˚ J θ α ∇ J θ β α L L σ d ,
P P D θ P 2 D D 2 P P Θ
´ p qď } p q} ` `p ` q
whereL O 1 γ ´3 (fullexpressioninLemmaE.2).
2
“ pp ´ q q
Proof. We recall that under the assumptions in the statement, the results of Lemma D.10 and of Theorem 5.1 hold In
particular,weneedtheresultfromTheorem5.1,sayingthat θ Θitholdsthat
a @ P a
J θ L σ d J θ J θ L σ d . (76)
P J P Θ D P J P Θ
p q´ ď p qď p q`
Thus,usingtheresultofLemmaD.10,weneedtoworkontheleft-handsideofthefollowinginequality:
a
J˚ J θ α ∇ J θ β α L σ d .
D´ D p qď D } θ P p q}2 ` D ` D 2 P Θ
Moreover,bydefinitionofJ ,wehavethatJ˚ J ˚. Thus,itholdsthat:
P Dě P
J˚ J θ J ˚ J θ
D´ D p qě P ´ P p q a
J ˚ J θ L d ,
P P J Θ
ě ´ p q´
wherethelastlinefollowsfromLine(76). WerenameL : L inthestatement.
P J
“
LemmaD.11. UnderAssumptions7.1,4.1,4.3,4.2,4.4,usingapolicycomplyingwithDefinition3.2, θ Θ,itholdsthat:
a
@ P
J˚ J θ α ∇ J θ β α ψσ d ,
D´ D p qď D } θ A p q}2 ` D ` D A A
23LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
where ˜ ¸
ψ L
L2 pR maxγ pL rL
p
`R maxL
2,p
`L pL rγ
q
L
2,r 1 γT .
“ µ 1 γ 4 ` 1 γ 2 `1 γ p ´ q
p ´ q p ´ q ´
Proof. AsintheproofofTheoremE.1,weintroducethefollowingconvenientexpressionforthetrajectorydensityfunction
havingfixedasequenceofnoiseϵ ΦT :
„ dA
Tź´1
p τ;µ ϵ ρ s p s s ,µ s ϵ . (77)
D p θ` q“ 0 p τ,0 q p τ,t`1 | τ,t θ p τ,t q` t q
t“0
Alsointhiscase,wedenotewithp τ ;µ ϵ thedensityfunctionofatrajectoryprefixoflengthl:
D p 0:l θ` q
źl´1
p τ ;µ ϵ ρ s p s s ,µ s ϵ . (78)
D p 0:l θ` q“ 0 p τ,0 q p τ,t`1 | τ,t θ p τ,t q` t q
t“0
FromtheproofofPropositionE.1,consideringagenericparametricconfigurationθ Θ,wecanwritetheABperformance
P
indexJ θ as:
A p q « ż ff
Tÿ´1
J θ E p τ;µ ϵ γtr s ,µ s ϵ dτ
A p q“ϵ„ΦT
dA» τ
D p θ` q
t“0
p t θ p t q` t q
fi
— ż ffi
—Tÿ´1 ffi
“ϵ„E ΦT dA— — – lt“oo0ooooτo 0o :o toop oD oop ooτ o0 o: ot o; ooµ ooθ oo` oooϵ omqγ ot or oop os ot oo, oµ ooθ oop os oot oq oo` oooϵ ot oq ood ooτ o0 o: ntffi ffi fl,
“:fpϵq
moreover,byusingtheTaylorexpansioncenteredinϵ 0,forrϵ xϵ(forsomex 0,1 )thefollowingholds:
«“ “ Pffr s
Tÿ´1
J A pθ q“ϵ„E ΦT f p0 q` ϵJ t ∇ ϵtf pϵ q| ϵ“rϵ
dA t“0
Tÿ´1
“J D pθ q` ϵ„E ΦT rϵJ t ∇ ϵtf pϵ q| ϵ“rϵ s.
t“0 dA
Here,weareinterestedinthegradientofJ :
A
Tÿ´1
∇ θJ A pθ q“∇ θJ D pθ q` ∇ θ ϵ„E ΦT rϵJ t ∇ ϵtf pϵ q| ϵ“rϵ s
t“0 dA
Tÿ´1
“∇ θJ D pθ q` ϵ„E ΦT rϵJ t ∇ θ∇ ϵtf pϵ q| ϵ“rϵ s.
t“0 dA
Now,consideringthenormofthegradientwehave:
› ›
› ›Tÿ´1 ›
›
}∇ θJ A pθ q}2ě}∇ θJ D pθ q}2´› › ϵ„E
ΦT
rϵJ t ∇ θ∇ ϵtf pϵ q| ϵ“rϵ s› ›
t“0 dA› 2 ›
1 β
› ›Tÿ´1 ›
›
ěα DpJ D˚ ´J D pθ qq´αD D´› › ϵ„E ΦT rϵJ t ∇ θ∇ ϵtf pϵ q| ϵ“rϵ s› ›
t“0 dA 2
1 β
Tÿ´1
ěα
DpJ D˚ ´J D pθ qq´αD
D´
t“0ϵt„E ΦdAr}ϵ t }2 2s1{2 ϵ„E
ΦT
dAr}∇ θ∇ ϵtf pϵ q| ϵ“rϵ }2 2s1{2
1 β
a Tÿ´1
ěα
DpJ D˚ ´J D pθ qq´αD D´σ A d A ϵ„E
ΦT
r}∇ θ∇ ϵtf pϵ q| ϵ“rϵ }2 2s1{2,
t“0 dA
24LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
wherethesecondinequalityisbyAssumption7.1. Re-arrangingthelastinequality,wehave:
a Tÿ´1
J D˚ ´J D pθ qďα D }∇ θJ A pθ q}2`β D `α Dσ A d A ϵ„E
ΦT
r}∇ θ∇ ϵtf pϵ q| ϵ“rϵ }2 2s1{2.
ř t“0 dA
In order to conclude the proof, we need to bound the term T t“´ 01E ϵ„ΦT dAr}∇ θ∇ ϵtf pϵ q| ϵ“rϵ }2 2s1{2. From the proof of
PropositionE.1,foranyindexk T ,wehavethat:
« P(cid:74) (cid:75) ff
Tÿ´1
∇ f ϵ E γtr s ,µ s ϵ ∇ logp s s ,µ s ϵ γk∇ r s ,µ s ϵ ,
ϵk
p q“τ„pDp¨;µ θ`ϵq
t“k
p
t θ
p
t
q`
t
q
ϵk
p
k`1
|
k θ
p
k
q`
k
q`
ϵk
p
k θ
p
k
q`
k
q
fromwhichwecanderive∇ ∇ f ϵ asfollows:
θ ϵk
p q
∇ ∇ f ϵ
θ ϵ żk p q ˜ ¸
Tÿ´1
∇ p τ;µ ϵ γtr s ,µ s ϵ ∇ logp s s ,µ s ϵ γk∇ r s ,µ s ϵ dτ
“ θ D p θ` q p t θ p t q` t q ϵk p k`1 | k θ p k q` k q` ϵk p k θ p k q` k q
τ t“k
ż
Tÿ´1
∇ p τ;µ ϵ γtr s ,µ s ϵ ∇ logp s s ,µ s ϵ dτ
“ θ D p θ` q p t θ p t q` t q ϵk p k`1 | k θ p k q` k q
loooooτoooooooooooooooooooooto“oookoooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooooooooooon
ż (i)
∇ p τ;µ ϵ γk∇ r s ,µ s ϵ dτ.
`looθ oooooooD oopoooooooθoo`ooooqooooomϵ okooopoook oooooθ oopoook oqoo`oooook
oqooon
τ
(ii)
Wewillconsidertheterms(i)and(ii)separately. However,wefirstneedtoclarifywhathappenswhenwetrytocompute
thegradientw.r.t.θandϵ ,foragenerict 0,...,T 1 . Tothispurposeletg ,a beagenericdifferentiablefunctionof
t
Pt ´ u p¨ q
theactiona µ s ϵ . Thenormofitsgradientw.r.t.ϵ canbewrittenas:
θ t t t
“ p q`
∇ g ,a | ∇ g ,a | ∇ µ s ϵ
}
ϵt
p¨
qa“µθpstq`ϵt}2
“}
a
p¨
qa“µθpstq`ϵt ϵtp θ
p
t
q`
t q}2
∇ g ,a | .
“}
a
p¨
qa“µθpstq`ϵt}2
Ontheotherhand,thenormofthegradientofgw.r.t.θcanbewrittenas:
∇ g ,a | ∇ g ,a | ∇ µ s ϵ
}
θ
p¨
qa“µθpstq`ϵt}2
“}
a
p¨
qa“µθpstq`ϵt θ
p
θ
p
t
q`
t q}2
∇ g ,a | ∇ µ s .
“}
a
p¨
qa“µθpstq`ϵt θ θ
p
t q}2
Moreover,thenormofthegradientw.r.t.θofthegradientofgw.r.t.ϵ ,canbewrittenas:
t
∇ ∇ g ,a | ∇ ∇ g ,a |
}
θ ϵt
p¨
qa“µθpstq`ϵt}2
“}
θ a
p¨
qa“µθpstq`ϵt}2
∇2g ,a | ∇ µ s .
“} a p¨ qa“µθ`ϵt θ θ p t q}2
Havingsaidthis,wecanproceedbyanalyzingtheterms(i)and(ii).
Theterm(i)canberewrittenas:
ż
Tÿ´1
(i) ∇ p τ;µ ϵ γtr s ,µ s ϵ ∇ logp s s ,µ s ϵ dτ
“ θ D p θ` q p t θ p t q` t q ϵk p k`1 | k θ p k q` k q
τ t“k
ż
Tÿ´1
γt∇ p τ ;µ ϵ r s ,µ s ϵ ∇ logp s s ,µ s ϵ dτ
“ θ D p 0:t θ` q p t θ p t q` t q ϵk p k`1 | k θ p k q` k q 0:t
t“k τ0:t «
Tÿ´1
γt E ∇ logp τ ,µ ϵ r s ,µ s ϵ ∇ logp s s ,µ s ϵ
“
t“k
τ0:t„pDp¨;µ θ`ϵq
θ D
p
0:t θ
` q p
t θ
p
t
q`
t
q
ϵk
p
k`1
|
k θ
p
k
q`
k
q
∇ r s ,µ s ϵ ∇ logp s |s ,µ s ϵ
` θ p t θ p t q` t q ϵk p k`1 k θ p k q` k qff
r s ,µ s ϵ ∇ ∇ logp s |s ,µ s ϵ
` p
t θ
p
t
q`
t
q
θ ϵk
p
k`1 k θ
p
k
q`
k
q
25LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
«
Tÿ´1 tÿ´1
γt E ∇ logp s |s ,µ s ϵ r s ,µ s ϵ ∇ logp s s ,µ s ϵ
“
t“k
τ0:t„pDp¨;µ θ`ϵq
j“0
θ
p
j`1 j θ
p
j
q`
j
q p
t θ
p
t
q`
t
q
ϵk
p
k`1
|
k θ
p
k
q`
k
q
∇ r s ,µ s ϵ ∇ logp s |s ,µ s ϵ
` θ p t θ p t q` t q ϵk p k`1 k θ p k q` k qff
r s ,µ s ϵ ∇ ∇ logp s |s ,µ s ϵ .
` p
t θ
p
t
q`
t
q
θ ϵk
p
k`1 k θ
p
k
q`
k
q
Weneedtobounditsnorm,thuswecanproceedasfollows:
i
}p q}2 «
Tÿ´1 tÿ´1
γt E ∇ logp s |s ,µ s ϵ r s ,µ s ϵ ∇ logp s s ,µ s ϵ
ď
t“k
τ0:t„pDp¨;µ θ`ϵq j“0} θ p j`1 j θ p j q` j q}2| p t θ p t q` t q|} ϵk p k`1 | k θ p k q` k q}2
∇ r s ,µ s ϵ ∇ logp s |s ,µ s ϵ
`} θ p t θ p t q` t q}2} ϵk p k`1 k θ p k q` k q}ff2
r s ,µ s ϵ ∇ ∇ logp s |s ,µ s ϵ
`| p t θ p t q` t q|} θ ϵk p k`1 k θ p k q` k q}2
Tÿ´1 Tÿ´1
L L2 R tγt L L L R L L γt
ď µ logp max `p µ r logp ` max µ 2,logp q
t“ˆk t“k ˙
1 TγT´1 T 1 γT 1 kγk´1 k 1 γk γk γT
L L2 R γ ´ `p ´ q ´ `p ´ q L L L R L L ´
“ µ logp max 1 γ 2 ´ 1 γ 2 `p µ r logp ` max µ 2,logp q 1 γ
ˆ p ´ q ˙ p ´ q ´
1 γT 1 kγk´1 γk γT
L L2 R γ ´ ´ L L L R L L ´
ď µ logp max 1 γ 2´ 1 γ 2 `p µ r logp ` max µ 2,logp q 1 γ
ˆp ´ q p ´ q ˙ ´
1 γT 1 kγk´1 γk γT
L L2 R γ ´ ´ L L L R L L ´ .
ď µ logp max 1 γ 2´ 1 γ 2 `p µ r logp ` max µ 2,logp q 1 γ
p ´ q p ´ q ´
Finally,wehavetosumoverk T :
P(cid:74) (cid:75) ˆ ˙
Tÿ´1 Tÿ´1
1 γT 1 kγk´1 γk γT
i L L2 R γ ´ ´ L L L R L L ´
}p q}2ď µ logp max 1 γ 2´ 1 γ 2 `p µ r logp ` max µ 2,logp q 1 γ
k“0 k“0 ˆ p ´ q p ´ q ˙ ´
1 γT T 1 γT TγT´1
L L2 R γ T ´ ´
“ µ logp max 1 γ 2´ 1 γ 2` 1 γ 4´ 1 γ 3
p ´ q p ´ q p ´ q p ´ q
1 2γT γT`1
L L L R L L ´ `
`p µ r logp ` max µ 2,logp q 1 γ 2
p ´ q
1 γT 1 γT
L L2 R γ ´ L L R L L ´ .
ď µ logp max 1 γ 4`p r logp ` max 2,logp q µ 1 γ 2
p ´ q p ´ q
Theterm(ii)canberewrittenas:
ż
(ii) ∇ p τ;µ ϵ γk∇ r s ,µ s ϵ dτ
“ θ D p θ` q ϵk p k θ p k q` k q
żτ
∇ p τ ;µ ϵ γk∇ r s ,µ s ϵ dτ
“ θ D p 0:k θ` q ϵk p k θ p k q` k q 0:k
żτ0:k ´ ¯
γk p τ ;µ ϵ ∇ logp τ ;µ ϵ ∇ r s ,µ s ϵ ∇ ∇ r s ,µ s ϵ
“ D p 0:k θ` q θ D p 0:k θ` q ϵk p k θ p k q` k q` θ ϵk p k θ p k q` k q
τ0:k « ff
kÿ´1
γk E ∇ logp s |s ,µ s ϵ ∇ r s ,µ s ϵ ∇ ∇ r s ,µ s ϵ .
“ τ0:k„pDp¨;µ θ`ϵq
j“0
θ
p
j`1 j θ
p
j
q`
j
q
ϵk
p
k θ
p
k
q`
k
q`
θ ϵk
p
k θ
p
k
q`
k
q
Weneedtobounditsnorm,thuswecanproceedasfollows:
«
kÿ´1
(ii) γk E ∇ logp s |s ,µ s ϵ ∇ r s ,µ s ϵ
} }2ď τ0:k„pDp¨;µ θ`ϵq j“0} θ p j`1 j θ p j q` j q}2} ϵk p k θ p k q` k q}2
26LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
ff
∇ ∇ r s ,µ s ϵ
`} θ ϵk p k θ p k q` k q}2
L L L kγk L L γk.
µ p r µ 2,r
ď `
Finally,wehavetosumoverk T :
P(cid:74) (cid:75)
Tÿ´1 Tÿ´1 Tÿ´1
(ii) L L L kγk L L γk
} }2“ µ p r ` µ 2,r
k“0 k“0 k“0
1 TγT´1 T 1 γT 1 γT
L L L γ ´ `p ´ q L L ´
ď µ p r 1 γ 2 ` µ 2,r 1 γ
p ´ q ´
1 γT 1 γT
L L L γ ´ L L ´ .
ď µ p r 1 γ 2` µ 2,r 1 γ
p ´ q ´
Puttingtogethertheboundson(i)and(ii):
Tÿ´1
ϵ„E
ΦT
r}∇ θ∇ ϵtf pϵ q| ϵ“rϵ }2 2s1{2
t“0 dA
Tÿ´1 Tÿ´1
i (ii)
ď }p q}2` } }2
k“0 k“0
1 γT 1 γT
L L2R γ ´ L L R L L ´
ď µ p max 1 γ 4`p r p ` max 2,p q µ 1 γ 2
p ´ q p ´ q
1 γT 1 γT
L L L γ ´ L L ´
` µ p r 1 γ 2` µ 2,r 1 γ
˜ p ´ q ´ ¸
L
L2 pR maxγ pL rL
p
`R maxL
2,p
`L pL rγ
q
L
2,r 1 γT ,
ď µ 1 γ 4 ` 1 γ 2 `1 γ p ´ q
p ´ q p ´ q ´
whichconcludestheproof.
Theorem7.2(InheritedweakgradientdominationonJ ). UnderAssumptions7.1,4.1,4.3,4.2,4.4,foreveryθ Θ:
A a
P
J ˚ J θ α ∇ J θ β α ψ L σ d ,
A A D θ A 2 D D A A A
´ p qď } p q} ` `p ` q
whereψ O 1 γ ´4 (fullexpressionintheproof).
“ pp ´ q q
Proof. ThisproofdirectlyfollowsfromthecombinationofLemmaD.11andTheoremE.1,andwecanproceedasinthe
proofofTheorem7.1. Indeed,recallingthatLis
γ γ1`T T 1 TγT 1 γT
L ` p ´ q´ L R ´ L ,
“ 1 γ 2 logp max ` 1 γ r
p ´ q ´
fromTheoremE.1,itfollowsthat:
a a
J θ Lσ d J θ J θ Lσ d . (79)
A A A D A A A
p q´ ď p qď p q`
AnalogouslytotheproofofTheorem7.1,itisusefultonoticethatbydefinitionofJ ,wehaveJ˚ J ˚. Thus,itholds
A Dě A
that:
J˚ J θ J ˚ J θ
D´ D p qě A ´ D p q a
J ˚ J θ Lσ d ,
A A A A
ě ´ p q´
wherethelastlinefollowsfromLine(79). WerenameL : Linthestatement.
A
“
TheoremD.12(GlobalconvergenceofPGPE-InheritedWGD). ConsiderthePGPEalgorithm. UnderAssumptions7.1,
4.1,4.3,4.2,4.4,4.5,withasuitableconstantstepsizeandsettingσ P “pαDL2`4ϵ LJq? dΘ“O pϵ p1 ´γ q3d´ Θ1{2 q,toguarantee
J˚ E J θ ϵ β thesamplecomplexityisatmost:
D´ r D p K qsď ` D ˆ ˙
NK
Or α D6d2
Θ . (80)
“ 1 γ 11ϵ5
p ´ q
27LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Proof. Simply apply Theorem F.1 and Theorem 5.1 to obtain a guarantee of J˚ E J θ ϵ 2 β α L
D´ r D p K qsď { ` D `p D 2 `
L ?d σ 3L ?d σ ϵ β α L 4L ?d σ . Then, we set σ to ensure that α L 4L ?d σ ϵ 2.
J Θ P J Θ P D D 2 J Θ P P D 2 J Θ P
q ` “ ` `p ` q p ` q “ {
TheoremD.13(GlobalconvergenceofGPOMDP-InheritedWGD). ConsidertheGPOMDPalgorithm. UnderAssump-
tions7.1,4.1,4.3,4.2,4.4,4.5,withasuitableconstantstepsizeandsettingσ A “pαDΨ`4ϵ Lq? dA“O pϵ p1 ´γ q4d A´1{2 q,to
guaranteeJ˚ E J θ ϵ β thesamplecomplexityisatmost:
D´ r D p K qsď ` D ˆ ˙
NK Or α D6d2 A . (81)
“ 1 γ 14ϵ5
p ´ q
Proof. Simply apply Theorem F.1 and Theorem 5.2 to obtain a guarantee of J˚ E J θ ϵ 2 β α Ψ
D´ r D p K qsď { ` D `p D `
L ?d σ 3L?d σ ϵ β α Ψ 4L ?d σ . Then,wesetσ toensurethat α Ψ 4L ?d σ ϵ 2.
Θ A Θ A D D Θ A A D A A
q ` “ ` `p ` q p ` q “ {
D.4.ProofsfromSection7.2
Inthissection,wefocusonABexplorationwithwhite-noisepolicies(Definition3.2),andgivetheproofsthatwereomitted
inSection7.2. Wedenotebyυ , thestate-actiondistributioninducedbythe(stochastic)policyπ ,and,withsome
θ θ
abuseofnotation,υ todenotep t¨ he¨q correspondingstatedistribution. WedenotebyAθ:S A Rtheadvantagefunction
θ
p¨q ˆ Ñ
ofπ (forthestandarddefinitions,seeSutton&Barto,2018).
θ
Wefirsthavetogiveaformalcharacterizationofϵ . Equivalentdefinitionsappearedin(Liuetal.,2020;Dingetal.,2022;
bias
Yuanetal.,2022),buttheconceptdatesbackatleastto(Petersetal.,2005).
` ˘
Definition D.1. Let ℓ w;s,a,θ Aθ s,a 1 γ wJ∇ logπ as 2 , and w‹ θ
θ θ
a Ergmin wP ℓRd wΘ ‹E θs,a ;„ sυ ,θ ar ,ℓ θpw;s ϵ,a,θ ,q ws. hep rW ee θ‹de afi rnq ge“ mϵ ab xia Jsp a θsq .t´ hep s´ malq lest positive p co| nsq tant such that, for all p θ Pq Θ“ ,
s,a„υθ‹r
p p q qsď
bias
P
A
p q
Webeginbyshowingthatwhite-noisepoliciesareFisher-non-degenerate,inthesenseof(Dingetal.,2022). Firstweneed
tointroducetheconceptofFisherinformationmatrix,thatforstochasticpoliciesisdefinedas(Kakade,2001):
F θ : E ∇ logπ as ∇ logπ as J . (82)
θ θ θ θ
p q “s,a„υθr p | q p | q s
LemmaD.14. Letπ beawhite-noisepolicy(Definition3.2). UnderAssumption7.2,forallθ Θ,F θ ľλ I,where
θ F
P p q
λ
λ : E .
F “σ2
A
Proof. LetΣ E ϵϵJ bethecovariancematrixofthenoise,whichbydefinitionhasλ Σ σ2. Byasimple
“ ϵ„ΦdAr s max p qď A
changeofvariableandCramer-Rao’sbound:
F θ E ∇ logπ as ∇ logπ as J
θ θ θ θ
p q“s,a„υ„θr p | q p | q s
ȷ
“ ‰
“s„E υθ„∇ θµ θ ps qa„πE
θp¨|sq
∇ ϵlogϕ pϵr q|ϵr“a´µθpsq∇ ϵlogϕ pϵr qJ ȷ|ϵr“a´µθpsq ∇ θµ θ ps qJ
“ ‰
E ∇ µ s E ∇ logϕ ϵ ∇ logϕ ϵ J ∇ µ s J
θ θ ϵ ϵ θ θ
“s„υθ
“
p qϵ„ΦdA ‰p q p q p q
ľ E ∇ µ s Σ´1∇ µ s J (Cramer-Rao)
θ θ θ θ
s„υθ p q p q
“ ‰
1
ľ E ∇ µ s ∇ µ s J
θ θ θ θ
λ max Σ s„υθ p q p q
p q
λ
E
ľ I.
σ2
A
WecanthenuseCorollary4.14byYuanetal.(2022),itselfarefinementofLemma4.7byDingetal.(2022),toprovethat
J enjoystheWGDproperty.
A
28LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Theorem7.3(Policy-inducedweakgradientdomination). UnderAssumptions4.5,7.2andD.1,wehave:
J ˚ J θ
C?d Aσ
A ∇ J θ
?ϵ
bias ,
A A θ A 2
´ p qď λ } p q} ` 1 γ
E
´ ? ?
forsomenumericalconstantC 0,thatis,Assumption6.1( =A)issatisfiedwithα C dAσA andβ ϵbias.
ą : “ λE “ 1´γ
Proof. Corollary4.14byYuanetal.(2022)tellsusthat,underAssumptionD.1,
ξ ?ϵ
J ˚ J θ ∇ J θ bias ,
A A θ A
´ p qďλ } p q}` 1 γ
F
´
wheneverF θ ľλ I andE ∇ logπ as 2 ξ2 holdforallθ Θands S. ByLemmaD.14,andthefact
p q
F a„πθp¨|sq
r}
θ θ
p | q} sď P P
thatξ ?cd σ´1isavalidchoiceunderAssumption4.5,thepreviousdisplayholdswith
“ A A
ξ ?cd σ´1 ?cd σ
A A A A ,
λ F ď λ Eσ A´2 “ λ E
theproofisconcludedbylettingC ?c,wherecistheconstantfromAssumption4.5.
“
Finally,wecanusetheWGDpropertyjustestablished,withitsvaluesofαandβ,toprovespecialcasesofTheoremsD.8
andD.9. Thekeydifferencewithrespecttotheothersamplecomplexityresultspresentedinthepaperisthattheamountof
noiseσ hasaneffectontheαparameteroftheWGDproperty.
A
Wefirstconsiderthecaseofagenericσ :
A
TheoremD.15. ConsidertheGPOMDPalgorithm. UnderAssumptions4.1,4.3,4.4,4.5,7.2,andD.1,withasuitable
?
constantstepsize,toguaranteeJ˚ E J θ ϵ ϵbias 3L?d σ ,where3L?d σ O ?d σ 1 γ ´2 the
D´ r D p K qsď ` 1´γ ` A A A A “ p A A p ´ q q
samplecomplexityisatmost:
ˆ ˙
NK Or
d4
A . (83)
“ λ4 1 γ 5ϵ3
Ep ´ q
Furthermore,underAssumption4.2,thesameguaranteeisobtainedwithasamplecomplexityofatmost:
ˆ ˙
NK
Or d3 Aσ A2
. (84)
“ λ4 1 γ 6ϵ3
Ep ´ q
Proof. ByTheoremD.8andLemma7.3.
The first bound seem to have no dependence on σ . However, a complex dependence is hidden in λ4. Also, it may
A E
seem that σ 0 is a good choice, especially for the second bound. However, λ can be very large (or infinite) for a
A E
»
(quasi-)deterministicpolicy.
Ifweinsteadsetσ asinSection6inordertoconvergetoagooddeterministicpolicy(which,ofcourse,completelyignores
A
thecomplexdependenciesofλ andϵ onσ ),weobtainthefollowing:
E bias A
TheoremD.16. ConsidertheGPOMDPalgorithm. UnderAssumptions4.1,4.3,4.4,4.5,7.2,andD.1withasuitable
?
constant step size and setting σ
A
“6L?ϵ dA“O pϵ p1 ´γ q2d A´1{2 q, to guarantee J D˚ ´E rJ
D
pθ
K
qsďϵ
`
1ϵ ´bi γas the sample
complexityisatmost:
ˆ ˙
NK Or
d6
A . (85)
“ λ4 1 γ 13ϵ3
Ep ´ q
Furthermore,underAssumption4.2,thesameguaranteeisobtainedwithasamplecomplexityofatmost:
ˆ ˙
NK Or
d4
A . (86)
“ λ4 1 γ 10ϵ
Ep ´ q
Proof. ByTheoremD.9andLemma7.3.
Theapparentlybettersamplecomplexityw.r.t. TheoremD.9iseasilyexplained: usingasmallσmakestheαparameterof
WGDfromLemma7.3smallerifweignoretheeffectofλ ,andsmallerαyieldsfasterconvergence.However,Equation(86)
E
29LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
clearlyshowsthatλ cannotbeignored. Inparticular,λ mustbeO σ1{4 nottoviolatetheclassicΩ ϵ´2 lowerbound
E E p A q p q
onthesamplecomplexity(Azaretal.,2013). Thismaybeofindependentinterest.
E.Assumptions’Implications
LemmaE.1(LandL characterization). Assumption4.1impliesAssumption5.2with:
J
γk`1 γT
L ´ L R γkL , (87)
t p max r
ď 1 γ `
´
γ 1 γT 1 γT γL R L
L p ´ qL R ´ L p max r . (88)
ď 1 γ 2 p max ` 1 γ r ď 1 γ 2 `1 γ
p ´ q ´ p ´ q ´
Assumption4.1and4.3implyAssumption5.1withL LL .
J µ
ď
Proof. InABexploration,weintroducethefollowingconvenientexpressionforthetrajectorydensityfunctionhavingfixed
asequenceofnoiseϵ:
Tź´1
p τ;µ ϵ ρ s p s s ,µ s ϵ . (89)
D 0 τ,0 τ,t`1 τ,t t τ,t t
p ` q“ p q p | p q` q
t“0
Furthermore,wedenotewithp τ ;µ ϵ thedensityfunctionofatrajectoryprefixoflengthl:
D 0:l
p ` q
źl´1
p τ ;µ ϵ ρ s p s s ,µ s ϵ . (90)
D 0:l 0 τ,0 τ,t`1 τ,t t τ,t t
p ` q“ p q p | p q` q
t“0
Letusdecomposeµ1 µ1 ϵ. Wehave:
“ ` ż
Tÿ´1
J µ1 p τ;µ ϵ γtr s ,µ s ϵ dτ
D D t t t t
p q“ p ` q p p q` q
τ t“0
ż
Tÿ´1
p τ ;µ ϵ γtr s ,µ s ϵ dτ .
D 0:t t t t t 0:t
“ p ` q p p q` q
lt“oo0ooooτo 0oo :toooooooooooooooooooomooooooooooooooooooooooooooooon
“:fpϵq
Notethatgiventhedefinitionoff ϵ ,wehavethatf 0 J µ . UsingTaylorexpansion,wehaveforrϵ xϵ,forsome
p q p
dAq“ D
p q “
x 0,1 :
Pr s
J µ1 f ϵ
D
p q“ p q
“f p0 dAq`ϵJ∇ ϵf pϵ q| ϵ“rϵ
Tÿ´1
“J D pµ q` ϵJ t ∇ ϵtf pϵ q| ϵ“rϵ
t“0
Tÿ´1
ďJ D pµ
q`
}ϵ t }2 }∇ ϵtf pϵ q| ϵ“rϵ }2.
t“0
Wewanttofindaboundforthe }∇ ϵtf pϵ q| ϵ“rϵ }2 2whichisdifferentforeveryt. ThiswillresultintheLipschitzconstantL t.
Wehavefork 0,T 1 :
P(cid:74) ´ (cid:75)« ff
› › Tÿ´1› › Tÿ´1
› ∇ f ϵ › E › ∇ logp τ ;µ ϵ › γt r s ,µ s ϵ γt ∇ r s ,µ s ϵ
ϵ k p q 2ďτ„pDp¨;µ`ϵq
«t“0
ϵk D p 0:t ` q 2 | p t t p t q` t q|`
t“0
} ϵk p t t p t q` t q}2
ff
Tÿ´1 tÿ´1 Tÿ´1
E γt r s ,µ s ϵ ∇ p s s ,µ s ϵ γt ∇ r s ,µ s ϵ
ďτ„pDp¨;µ`ϵq
«t“0
| p
t t
p
t
q`
t
q| l“0}
ϵk
p
l`1
|
l l
p
l
q`
l q}2
`
t“0
}
ϵk
p
t t
p
t
q`
t
q}
ff2
Tÿ´1
E γt r s ,µ s ϵ ∇ p s s ,µ s ϵ γk ∇ r s ,µ s ϵ
“τ„pDp¨;µ`ϵq
t“k`1
| p
t t
p
t
q`
t
q|}
ϵk
p
k`1
|
k k
p
k
q`
k q}2
` }
ϵk
p
k k
p
k
q`
k q}2
30LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Tÿ´1
γtR L γkL
max p r
ď `
t“k`1
γk`1 γT
´ L R γkL :L .
p max r k
“ 1 γ ` “
´
Thus,wehave:
Tÿ´1 Tÿ´1γk`1
γT
L ´ L R γkL
t p max r
“ 1 γ `
k“0 k“0 ´
γ γ1`T T 1 TγT 1 γT
` p ´ q´ L R ´ L
“ 1 γ 2 p max ` 1 γ r
p ´ q ´
γ 1 γT 1 γT
p ´ qL R p ´ qL :L.
ď 1 γ 2 p max ` 1 γ r “
p ´ q ´
ForthePBexploration,weconsiderthetrajectorydensityfunction:
Tź´1
p τ;θ ϵ ρ s p s s ,µ s , (91)
A 0 τ,0 τ,t`1 τ,t θ`ϵ τ,t
p ` q“ p q p | p qq
t“0
andthecorrespondingversionforatrajectoryprefix:
źl´1
p τ ;θ ϵ ρ s p s s ,µ s . (92)
D 0:l 0 τ,0 τ,t`1 τ,t θ`ϵ τ,t
p ` q“ p q p | p qq
t“0
Withsuchanotation,wecanwritetheθ1 θ ϵindexasfollows:
“ `
ż ż
Tÿ´1 Tÿ´1
J θ1 p τ;θ ϵ γtr s ,µ s dτ p τ ;θ ϵ γtr s ,µ s .
D D t θ`ϵ t D 0:t t θ`ϵ t
p q“ p ` q p p qq “ p ` q p p qq
τ t“0 lτoooto“oo0ooooooooooooooooomooooooooooooooooooooooon
“:gpϵq
Werecallthatg 0 J θ . ByusingTaylorexpansion,whereϵr xϵforsomex 0,1 :
p
dΘq“ D
p q “ Pr s
J θ1 g ϵ (93)
D
p q“ p q
“g p0 dΘq`ϵJ∇ ϵg pϵ q| ϵ“ϵr (94)
J D θ ϵ 2 ∇ ϵg ϵ | ϵ“ϵr 2. (95)
ď p q`} } } p q }
Wenowboundthenormofthegradient:
« ff
Tÿ´1 Tÿ´1
∇ g ϵ E ∇ logp τ ;θ ϵ γt r s ,µ s γt ∇ r s ,µ s
ϵ 2 ϵ 0:t 2 t θ`ϵ t ϵ t θ`ϵ t 2
} p q} ďτ„pAp¨;µθ`ϵq «t“0} p ` q} | p p qq|`
t“0
} p p qq}
Tÿ´1 tÿ´1
E γt r s ,µ s ∇ logp s s ,a | ∇ µ s
ďτ„pAp¨;µθ`ϵq
t“0
| p
t θ`ϵ
p
t
qq| l“0}
a
p
ffl`1
|
l qa“µθ`ϵpstq }2
}
ϵ θ`ϵ
p
l q}2
Tÿ´1
γt ∇ r s ,a | ∇ µ s
` }
a
p
t qa“µθ`ϵpstq }2
}
ϵ θ`ϵ
p
t q}2
t“0
Tÿ´1
1 γT
γtR tL L ´ L L
max p µ r µ
ď ` 1 γ
t“0 ´
γ γ1`T T 1 TγT 1 γT
` p ´ q´ L L R ´ L L LL .
“ 1 γ 2 p µ max ` 1 γ r µ “ µ
p ´ q ´
AssumptionE.1(SmoothJ w.r.t. parameterθ). J isL -LSw.r.t.parameterθ,i.e.,foreveryθ,θ1 Θ,wehave:
D D 2
P
∇ J θ1 ∇ J θ L θ1 θ . (96)
θ D θ D 2 2 2
} p q´ p q} ď } ´ }
31LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
LemmaE.2(L Characterization). Assumptions4.1,4.3,4.2,and4.4implyAssumptionE.1with
2
L γ p1 `γ qL2 µL2 pR max γ p2L2 µL pL r `L 2,µL 2,pR max q L 2,µL 2,r .
2 ď 1 γ 3 ` 1 γ 2 ` 1 γ
p ´ q p ´ q ´
› ›
› ›
Proof. Itsufficestofindaboundtothequantity ∇2J θ ,foragenericθ Θ. Noticethatinthefollowingweusethe
θ D p q 2 P
notationτ torefertoatrajectoryoflengthl. Recallingthat:
0:l « ff
Tÿ´1
J θ E γtr s ,µ s ,
D t θ t
p q“τ„pDp¨|θq
t“0
p p qq
wehavewhatfollows: « ff
Tÿ´1
∇2J θ ∇2 E γtr s ,µ s
θ D p q“ θ τ„pDp¨|θq
t“0
p t θ p t qq
ż
Tÿ´1
∇2 p τ,θ γtr s ,µ s dτ
“ θ D p q p t θ p t qq
τ t“0
ż
Tÿ´1
∇2 p τ ,θ γtr s ,µ s dτ
“ θ D p 0:t q p t θ p t qq 0:t
t“0 τ0:t
ż
Tÿ´1 ` ˘
∇ p τ ,θ ∇ logp τ ,θ γtr s ,µ s γt∇ r s ,µ s dτ
θ D 0:t θ D 0:t t θ t θ t θ t 0:t
“ p q p q p p qq` p p qq
t“0 τ0:t «
Tÿ´1 ` ˘
E ∇ logp τ ,θ ∇ logp τ ,θ γtr s ,µ s γt∇ r s ,µ s
θ D 0:t θ D 0:t t θ t θ t θ t
“ t“0τ0:t„pDp¨|θq p q p q p p qq` p p qq
ff
∇2logp τ ,θ γtr s ,µ s ∇ logp τ ,θ γt∇ r s ,µ s γt∇2r s ,µ s .
` θ D p 0:t q p t θ p t qq` θ D p 0:t q θ p t θ p t qq` θ p t θ p t qq
Nowthatwehavecharacterized∇2J θ ,wecanconsideritsnormbyapplyingtheassumptionsinthestatement,obtaining
θ D p q
thefollowingresult:
› ›
› ›
∇2J θ
θ D p q «
Tÿ´1 ` ˘
E ∇ logp τ ,θ ∇ logp τ ,θ γt r s ,µ s γt ∇ r s ,µ s
ď t“0τ0:t„pDp¨|θq } θ D p 0:t q}2 } θ D p 0:t q}2 | p t θ p t qq|` } θ p t θ p t qq}2
ff
› › › ›
› › › ›
∇2logp τ ,θ γt r s ,µ s ∇ logp τ ,θ γt ∇ r s ,µ s γt ∇2r s ,µ s
` θ D p 0:t q 2 | p t θ p t qq|`} θ D p 0:t q}2 } θ p t θ p t qq}2` θ p t θ p t qq 2
Tÿ´1
L2L2R t2γt 2L2L L L L R tγt L L γt
ď µ p max `p µ p r ` 2,µ 2,p max q ` 2,µ 2,r
t“0
1 γ T2γT´1 2 T 1 2 2 T 1 1 γT T 1 2γT`1
L2L2R γ ` ´ `p p ´ q ` p ´ q´ q ´p ´ q
ď µ p max 1 γ 3
p ´ q
1 TγT´1 T 1 γT 1 γT
2L2L L L L R γ ´ `p ´ q L L ´
`p µ p r ` 2,µ 2,p max q 1 γ 2 ` 2,µ 2,r 1 γ
p ´ q ´
1 γ γT 1 γT 1 γT
L2L2R γ ` ´ 2L2L L L L R γ ´ L L ´
ď µ p max 1 γ 3 `p µ p r ` 2,µ 2,p max q 1 γ 2` 2,µ 2,r 1 γ
p ´ q p ´ q ´
γ p1 `γ qL2 µL2 pR max γ p2L2 µL pL r `L 2,µL 2,pR max q L 2,µL 2,r .
ď 1 γ 3 ` 1 γ 2 ` 1 γ
p ´ q p ´ q ´
LemmaE.3. Letπ beawhitenoise-basedpolicy. UnderAssumption4.5,4.3,and4.4,itholdsthatforeverys S:
θ
P
(i) E ∇ logπ as 2 cd σ´2L2;
a„πθpa|sq r} θ θ p | q}2sď A A µ
(ii) E ∇2logπ as cσ´2L2 c?d σ´1L .
a„πθpa|sq r} θ θ p | q}2 sď A µ` A A 2,µ
32LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Proof. Sinceπ isawhitenoise-basedpolicy,wehavethatπ as ϕ a µ s . Consequently,wehave:
θ θ θ
p | q“ p ´ p qq
∇ logπ as ∇ logϕ a µ s ∇ µ s ∇ logϕ ϵ | , (97)
θ θ
p | q“
θ
p ´
θ
p qq“´
θ θ
p q
ϵ
p
qϵ“a´µθpsq
∇2logπ as ∇2logϕ a µ s ∇ µ ∇2logϕ ϵ | ∇ µJ ∇2µ ∇ logϕ ϵ | . (98)
θ θ p | q“ θ p ´ θ p qq“ θ θ ϵ p qϵ“a´µθpsq θ θ´ θ θ ϵ p qϵ“a´µθpsq
Thus,recallingthata µ s Φ andusingtheLipschitzinityandsmoothnessofµ ,wehave:
´
θ
p q„
dA θ
E ∇ logπ as 2 E ∇ µ s ∇ logϕ ϵ | 2 (99)
a„πθpa|sqr} θ θ p | q}2s“a„πθpa|sqr} θ θ p q ϵ p qϵ“a´µθpsq }2s
L2 E ∇ logϕ ϵ 2 cd σ2L2, (100)
ď µ ϵ„ΦdΘr}´ ϵ p q}2sď Θ P µ
E ∇2logπ as E ∇ µ ∇2logϕ ϵ | ∇ µJ ∇2µ ∇ logϕ ϵ | (101)
a„πθpa|sqr} θ θ p | q}2 s“a„πθpa|sqr} θ θ ϵ p qϵ“a´µθpsq θ θ´ θ θ ϵ p qϵ“a´µθpsq }2 s
L2 E ∇2logϕ ϵ L E ∇ logϕ ϵ (102)
ď µ ϵ„ΦdAr} aϵ p q}2 s` 2,µ ϵ„ΦdAr} ϵ p q}2 s
cσ´2L2 c d σ´1L . (103)
ď A µ` A A 2,µ
LemmaE.4. Letν beawhitenoise-basedhyperpolicy. UnderAssumption4.5,itholdsthat:
θ
(i) E ∇ logν θ1 2 cd σ´2;
(ii)
Eθ1„νθr} ∇2θ logνθ p θ1q}2sď cσΘ ´2.P
θ1„νθr} θ θ p q}2 sď P
Proof. Sinceν isawhitenoise-basedhyperpolicy,wehavethatν θ1 ϕ θ1 θ . Consequently,wehave:
θ θ
p q“ p ´ q
∇ logν θ1 ∇ logϕ θ1 θ ∇ logϕ ϵ | , (104)
θ θ θ ϵ ϵ“θ1´θ
p q“ p ´ q“´ p q
∇2logν θ1 ∇2logϕ θ1 θ ∇2logϕ ϵ | . (105)
θ θ p q“ θ p ´ q“ ϵ p qϵ“θ1´θ
Thus,recallingthatθ1 θ Φ
´ „
dΘ
E ∇ logν θ1 2 E ∇ logϕ ϵ | 2 E ∇ logϕ ϵ 2 cd σ2, (106)
θ1„νθr} θ θ p q}2s“θ1„νθr} ϵ p qϵ“θ1´θ }2s“ϵ„ΦdΘr} ϵ p q}2sď Θ P
E ∇2logν θ1 E ∇2logϕ ϵ | E ∇2logϕ ϵ cσ2. (107)
θ1„νθr} θ θ p q}2 s“θ1„νθr} ϵ p qϵ“θ1´θ }2 s“ϵ„ΦdΘr} ϵ p q}2 sď P
F.GeneralConvergenceAnalysisunderWeakGradientDomination
Inthissection,weprovidethetheoreticalguaranteesontheconvergencetotheglobaloptimumofagenericstochasticfirst-
orderoptimizationalgorithmA(e.g.,policygradientemployingeitherABorPBexploration). Letθbetheparametervector
optimizedbyA,andletΘ RdΘ betheparameterspace. TheobjectivefunctionthatAaimsatoptimizingisJ:Θ R,
“ ÑÝ
whichisagenericfunctiontakingasargumentaparametervectorθ Θandmappingitintoarealvalue. Examplesof
P
objectivefunctionsofthiskindareJ ,J ,orJ ,whicharealldefinedinSection2. ThealgorithmAisrunforK iterations
D A P
anditupdatesdirectlytheparametervectorθ Θ. Atthek-thiteration,theupdateis:
P p
θ θ ζ ∇ J θ ,
k`1 k k θ k
ÐÝ ` p q
p
whereζ isthestepsize,θ istheparameterconfigurationatthek-thiteration,and∇ J θ isanunbiasedestimateof
k k θ k
p q
∇ J θ computedfromabatchD ofN samples. Inthefollowing,werefertoN asbatchsize. Examplesofunbiased
θ k k
p q
gradientestimatorsaretheonesemployedbyGPOMDPandPGPE,whichcanbefoundinSection2.ForGPOMDP,samples
aretrajectories;forPGPE,parameter-trajectorypairs. Inwhatfollows,werefertotheoptimalparameterconfigurationas
θ˚ argmax J θ . Forthesakeofsimplicity,wewillshortenJ θ˚ asJ˚. Givenanoptimalitythresholdδ 0,weare
P θPΘ p q p q ě
interestedinassessingthelast-iterateconvergenceguarantees:
J˚ E J θ δ,
K
´ r p qsď
wheretheexpectationistakenoverthestochasticityofthelearningprocess.
TheoremF.1. UnderAssumptions6.1,6.2,and6.3,runningtheAlgorithmAforK 0iterationswithabatchsizeofN 0
ą ą
trajectoriesineachiterationwiththeconstantlearningrateζ fulfilling:
# ˆ ˙ +
1{3
1 1 N
ζ min , ,
ď L µmax 0,J˚ J θ β L Vµ
2 0 2
t ´ p q´ u
33LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
whereµ 1 . Then,itholdsthat:
“α2 ˜ c ¸ d
K
1 µζ3L V L Vζ
J˚ E J θ β 1 2 max 0,J˚ J θ β 2 .
K 0
´ r p qsď ` ´2 N t ´ p q´ u` µN
Inparticular,forsufficientlysmallϵ 0,settingζ
ϵ2µN,thefollowingtotalnumberofsamplesissufficienttoensurethat
J θ˚ E J θ β ϵ: ą “4L2V
K
p q´ r p qsď `
16L V max 0,J˚ J θ β
KN 2 log t ´ p 0 q´ u. (108)
ě ϵ3µ2 ϵ
Proof. Beforestartingtheproof,weneedapreliminaryresultthatimmediatelyfollowsfromAssumption6.1,byrearranging:
1
max 0,J θ˚ β J θ ∇ J θ , (109)
θ 2
α t p q´ ´ p quď} p q}
r r
andwewillusethenotationJ θ˚ : J θ˚ β andµ α´2. NotethatJ θ˚ J θ canbenegative. Consideringa
p q “ p q´ “ p q´ p q
k K ,itfollowsthat:
P(cid:74) (cid:75) r r
J θ˚ J θ J θ˚ J θ J θ J θ
k`1 k k`1 k
p q´ p q“ p q´ p q´p p q´ p qq
Jr θ˚ J θ ⟨θ θ ,∇ J θ ⟩ L 2 θ θ 2
ď p q´ p k q´ k`1 ´ k θ p k q ` 2 } k`1 ´ k }2
Jr θ˚ J θ ζ (cid:68) ∇p J θ ,∇ J θ (cid:69) L 2 ζ2 ∇p J θ 2,
ď p q´ p k q´ k θ p k q θ p k q ` 2 k} θ p k q}2
wherethefirstinequalityfollowsbyapplyingtheTaylorexpansionwithLagrangeremainderandexploitingAssumption6.2,
p
andthelastinequalityfollowsfromthefactthattheparameterupdateisθ θ ζ ∇ J θ .
k`1 k k θ k
Ð ` p q
Inthefollowing,weusetheshorthandnotationE todenotetheconditionalexpectationw.r.t. thehistoryuptothek-th
k
r¨s
iterationnotincluded. Moreformally,letF σ θ ,D ,D ,...,D betheσ-algebraencodingallthestochasticityupto
k 0 0 1 k
“ p q
iterationkincluded. Notethatallthestochasticitycomesfromthesamples(exceptfromtheinitialparameterθ ,which
0
mayberandomlyinitialized),andthatθ isF -measurable,thatis,deterministicallydeterminedbytherealizationofthe
k k´1
samplescollectedinthefirstk 1iterations. Then,E : E F . WewillmakeuseofthebasicfactsE E E
k k´1 k
andE X X forF
-mea´
surableX.
Thevarianr c¨ es o“ f∇pr¨ J|
θ
s mustbealwaysunderstoodasconditior n¨s a“ lonr Fr¨ss
.
k k´1 k k´1
r s“ p q
Now,foranyk K :
”P(cid:74) (cid:75) ı „ ȷ
E Jr θ˚ J θ E Jr θ˚ J θ ζ (cid:68) ∇p J θ ,∇ J θ (cid:69) L 2 ζ2 ∇p J θ 2
k p q´ p k`1 q ďk p q´ p k q´ k θ p k q θ p k q ` 2 k} θ p k q}2
” ı
Jr θ˚ J θ ζ ∇ J θ 2 L 2 ζ2E ∇p J θ 2
ď p q´ p k q´ k }ˆ θ p k q}˙2` 2 k k } θ p k q}2
” ı
Jr θ˚ J θ ζ 1 L 2 ζ ∇ J θ 2 L 2 ζ2Var ∇p J θ
ď p q´ p k q´ k ´ 2 k } θ p k q}2` 2 k θ p k q
ˆ ˙
Jr θ˚ J θ ζ 1 L 2 ζ ∇ J θ 2 L 2V ζ2,
ď p q´ p k q´ k ´ 2 k } θ p k q}2` 2N k
wherethethirdinequalityfollowsfromthefactthat∇p J θ isanunbiasedestimatorandfromthedefinitionofVar ∇p J θ ,
θ
andthelastinequalityisbyAssumption6.3. Now,selecp tinq gastepsizeζ 1 ,wehavethat1 L2ζ 1,wecar nusep thqs e
k ďL2 ´ 2 k ě2
boundderivedinEquation(109):
” ı ! )
E Jr θ˚ J θ Jr θ˚ J θ µζ k max 0,Jr θ˚ J θ 2 L 2V ζ2.
k p q´ p k`1 q ď p q´ p k q´ 2 p q´ p k q ` 2N k
Thenextstepistoconsiderthetotalexpectationoverboththetermsoftheinequalityandobservethat
„ ! ) ȷ ” ! )ı ! ” ı)
E max 0,Jr θ˚ J θ 2 E max 0,Jr θ˚ J θ 2 max 0,E Jr θ˚ J θ ,
k k k
p q´ p q ě p q´ p q ě p q´ p q
havingappliedJensen’sinequalitytwice,beingboththesquareandthemaxconvexfunctions. Inparticular,wedefine
r : E J θ˚ J θ . Wecanthenrewritethepreviousinequalityasfollows:
k k
“ r p q´ p qs
µζ L V
r r k max 0,r 2 2 ζ2.
k`1 ď k ´ 2 t k u ` 2N k
34LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Tostudytherecurrence,wedefinethehelpersequence:
#
ρ r
0 “ 0 . (110)
ρ ρ µζkmax 0,ρ 2 L2Vζ2 ifk 0
k`1 “ k ´ 2 t k u ` 2N k ě
Wenowshowthatunderasuitableconditiononthestepsizeζ ,thesequenceρ upperboundsthesequencer .
k k k
LemmaF.2. Ifζ 1 foreveryk 0,then,r ρ foreveryk 0.
k ďµρk ě k ď k ě
ProofofLemmaF.2. Byinductiononk. Fork 0,thestatementholdssinceρ r . Supposethestatementholdsforevery
0 0
“ “
j k,weprovethatitholdsfork 1:
ď `
µζ L V
ρ ρ k max 0,ρ 2 2 ζ2 (111)
k`1 “ k ´ 2 t k u ` 2N k
µζ L V
r k max 0,r 2 2 ζ2 (112)
ě k ´ 2 t k u ` 2N k
r . (113)
k`1
ě
wherethefirstinequalityholdsbytheinductivehypothesisandbyobservingthatthefunctionf x x µζkmax 0,x 2
p q“ ´ 2 t u
is non-decreasing in x when ζ 1 µx . Indeed, if x 0, then f x x, which is non-decreasing; if x 0, we have
k
f x x
µζkx2,thatcanbeproď ved{p tobq enon-decreasină gintheintp ervq a“
l 0,1 µζ
simplybystudyingthě
esignofthe
p q“ ´ 2 r {p k qs
derivative. Therequirementζ 1 µρ ensuresthatρ fallsinthenon-decreasingregion,andsodoesr bytheinductive
k k k k
ď {p q
hypothesis.
Thus,fromnowon,westudythepropertiesofthesequenceρ andenforcethelearningratetobeconstant,ζ : ζ forevery
k k
“
k 0. Letusnotethat,ifρ isconvergent,thanitconvergestothefixed-pointρcomputedasfollows:
ě k d
µζ L V L Vζ
ρ ρ max 0,ρ 2 2 ζ2 ρ 2 , (114)
“ ´ 2 t u ` 2N ùñ “ µN
havingretainedthepositivesolutionofthesecond-orderequationonly,sincethenegativeoneneverattainsthemaximum
max 0,ρ . Letusnowstudythemonotonicitypropertiesofthesequenceρ .
k
t u
LemmaF.3. Thefollowingstatementshold:
• Ifr ρandζ 1 ,thenforeveryk 0itholdsthat: ρ ρ ρ .
0 ą ďµr0 ě ď k`1 ď k
• Ifr ρandζ 1 ,thenforeveryk 0itholdsthat: ρ ρ ρ .
0 ă ďµρ ě ě k`1 ě k
Beforeprovingthelemma,letuscommentonit. Wehavestatedthatifweinitializethesequencewithρ r abovethe
0 0
“
fixed-pointρ,thesequenceisnon-increasingandremainsintheinterval ρ,r . Symmetrically,ifweinitializeρ r
0 0 0
r s “
(possibly negative) below the fixed-point ρ, the sequence is non-decreasing and remains in the interval r ,ρ . These
0
r s
propertiesholdunderspecificconditionsonthelearningrate.
ProofofLemmaF.3. We first prove the first statement, by induction on k. The inductive hypothesis is “ρ ρ and
k`1 k
ď
ρ ρ”. Fork 0,forthefirstinequality,wehave:
k`1
ě “
ζµ L V ζµ L V
ρ ρ ρ2 2 ζ2 ρ ρ2 2 ζ2 ρ , (115)
1 “ 0 ´ 2 0` 2N ď 0 ´ 2 ` 2N “ 0
havingexploitedthefactthatρ ρ 0andthedefinitionofρ. Forthesecondinequality,wehave:
0
ą ą
ζµ L V ζµ L V
ρ ρ ρ2 2 ζ2 ρ ρ2 2 ζ2 ρ, (116)
1 “ 0 ´ 2 0` 2N ě ´ 2 ` 2N “
recallingthatthefunctionx ζµx2 isnon-decreasinginxforx ρ sinceζ 1 µρ ,andbydefinitionofρ. Suppose
´ 2 ď 0 ď {p 0 q
nowthatthestatementholdsforeveryj k. Firstofall,weobservethat,underthisinductivehypothesis,ρ ρ and,
k 0
ă ď
consequently,theconditionζ 1 µρ entailsζ 1 µρ . Thus,forthefirstinequality,wehave:
0 k
ď {p q ď {p q
ζµ L V ζµ L V
ρ ρ ρ2 2 ζ2 ρ ρ2 2 ζ2 ρ , (117)
k`1 “ k ´ 2 k` 2N ď k ´ 2 ` 2N “ k
havingusedtheinductivehypothesisandthedefinitionofρ. Forthesecondinequality,wehave:
ζµ L V ζµ L V
ρ ρ ρ2 2 ζ2 ρ ρ2 2 ζ2 ρ, (118)
k`1 “ k ´ 2 k` 2N ě ´ 2 ` 2N “
35LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
having used the inductive hypothesis and recalled that the function x ζµx2 is non-decreasing in x for x ρ since
´ 2 ď k
ζ 1 µρ .
k
ď {p q
Forthesecondstatement,weobservethatifρ r 0,wehave:
0 0
“ ă
L V
ρ ρ k 2 ζ2, (119)
k 0
“ ` 2N
forallk k˚,wherek˚istheminimumkinwhichρ kL2Vζ2 0. Fromthatpointon,wecanproceedinananalogous
ď 0 ` 2N ě
wayasforthefirststatement,simplyswitchingthesignsoftheinequalitiesandrecallingthatthelargestvalueofρ is
k
boundedbyρinthiscase.
Wenowfocusonthefirstcaseofthepreviouslemmainwhichr ρ,asthesecondone,asweshallseelater,isirrelevant
0
ą
fortheconvergencerate. Wenowwanttoshowthatthesequenceρ actuallyconvergestoρandcharacterizeitsconvergence
k
rate. Tothisend,weintroduceanewauxiliarysequence:
#
η 0 ρ 0´ ¯
“ . (120)
η 1 µζρ η L2Vζ2 ifk 0
k`1 “ ´ 2 k ` 2N ě
Weshowthatthesequenceη upperboundsρ whenρ r ρ.
k k 0 0
“ ě
LemmaF.4. Ifr ρandζ 1 ,then,foreveryk 0,itholdsthatη ρ .
0 ą ďµr0 ě k ě k
Proof. Byinductiononk. Fork 0,wehaveη ρ ,so,thestatementholds. Supposethestatementholdsforeveryj k,
0 0
“ “ ď
weproveitfork 1:
ˆ ˙
`
µζρ L V
η 1 η 2 ζ2 (121)
k`1 k
“ ´ 2 ` 2N
ˆ ˙
µζρ L V
1 k η 2 ζ2 (122)
k
ě ´ 2 ` 2N
ˆ ˙
µζρ L V
1 k ρ 2 ζ2 (123)
k
ě ´ 2 ` 2N
ζµ L V
ρ max 0,ρ 2 2 ζ2 ρ . (124)
k k k`1
“ ´ 2 t u ` 2N “
havingexploitedthatρ ρ(byLemmaF.3)inthesecondline;usingtheinductivehypothesisinthethirdline,exploiting
k
thefactthat1 µζρk 0ě wheneverζ 2 µρ ,whichisentailedbytherequirementζ 1 µρ ;andbyrecallingthat
´ 2 ě ď {p k q ď {p 0 q
ρ 0sinceρ 0inthelastline.
k
ą ą
Thus, we conclude by studying the convergence rate of the sequence η . This can be easily obtained by unrolling the
k
recursion:
ˆ ˙ ˆ ˙
µζρ k`1 L Vζ2 ÿk µζρ j
η 1 η 2 1 (125)
k`1 0
“ ´ 2 ` 2N ´ 2
j“0
ˆ ˙ ˆ ˙
µζρ k`1 L Vζ2 `ÿ8 µζρ j
1 η 2 1 (126)
0
ď ´ 2 ` 2N ´ 2
j“0
ˆ ˙
k`1
µζρ L Vζ
1 η 2 (127)
0
“ ´ 2 ` Nµρ
˜ c ¸ d
k`1
1 µζ3L V L Vζ
1 2 η 2 . (128)
0
“ ´2 N ` µN
Puttingalltheconditionsonthestepsizeζ together,wemustset:
# ˆ ˙ +
1{3
1 1 N
ζ min , , . (129)
“ L µmax 0,r L Vµ
2 0 2
t u
36LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
Thus,wehave:
˜ c ¸ d
K
1 µζ3L V L Vζ
J θ˚ E J θ β 1 2 max 0,J θ˚ J θ β 2 . (130)
K 0
p q´ r p qsď ` ´2 N t p q´ p q´ u` µN
Wederivethenumberofiterations(settingK k 1):
˜ c ¸ Ð ` d
K
1 µζ3L V ϵ log2η0 4N 2η
1 2 η K ϵ log 0 , (131)
´2 N 0 ď2 ùñ ďlog b1 ď µζ3L V ϵ
1´1 µζ3L2V 2
2 N
havingexploitedtheinequalitylog 1 x. Furthermore,letusobservethat:
1´xě d
L Vζ ϵ ϵ2µN
ρ 2 ζ . (132)
“ µN ď2 ùñ ď4L V
2
Thus,recallingthatρ η r ,wehavethat: (i)whenr ρ,wehavethatr ρ ρ ϵ 2;(ii)whenr ρ,wehave
0 0 0 0 k k 0
r ρ η ϵ.
Thus,“ forsu“
fficientlysmallϵ,weplugζ
ϵă2µN inEquation(13ď 1)toď obtaď in{ thefollowingupě
perboundon
k ď k ď k ď “4L2V
thesamplecomplexity:
16L V max 0,J θ˚ J θ β
KN 2 log t p q´ p 0 q´ u, (133)
ď ϵ3µ2 ϵ
whichguaranteesJ θ˚ E J θ β ϵ.
K
p q´ r p qsď `
Theorem6.1. ConsideranalgorithmrunningtheupdateruleofEquation(13). UnderAssumptions6.1,6.2,and6.3,with
asuitableconstantstepsize,toguaranteeJ˚ E J θ ϵ β thesamplecomplexityisatmost:
: ´ r : p K qsď `
NK 16α4L 2,:V : logmax t0,J :˚ ´J : pθ 0 q´β u. (15)
“ ϵ3 ϵ
Proof. DirectlyfollowsfromthesecondstatementofTheoremF.1.
37LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
G.AlgorithmsSpecification
PGPE. InthissectionwereportthealgorithmPGPEasitisreportedinitsoriginalpaper(Sehnkeetal.,2010). Inparticular,
weshowthepseudo-code(Algorithm1)ofitsoriginalbasicversion,thatisalsotheoneweanalyzedthroughoutthiswork,
evenifseveralvariantsareavailable. Inthepseudo-codewewriteν σ toindicatethehyperpolicyν parameterizedvia
θ P
p q
theparametervectorθandusinganamountofexplorationcharacterizedbyσ .
P
Algorithm1PGPE.
Input :NumberofiterationsK,batchsizeN,initialparametervectorθ ,environmentM,deterministicpolicyµ ,hyperpolicyν ,step
0 θ θ
sizeschedulepζ qK´1,explorationparameterσ .
k k“0 P
InitializeθÐÝθ
0
foriP K do
Se(cid:74)tth(cid:75)ehyperpolicyparameters:ν
θ
forlP N do
Sa(cid:74)mp(cid:75)leaparameterconfiguration:ρ „ν pσ q
l θ P
Collectatrajectoryτ byactinginMwithµ
l ρl
ComputethecumulativediscountedrewardRpτq
l
end
ř
Computethegradientestimator:∇p θJ PpθqÐÝ N1 N j“´ 01∇ θlogν θpρ jqRpτ jq
p
Updatethehyperpolicyparametervector:θÐÝθ`ζ i∇ θJ Ppθq
end
Returnθ.
Noticethat,theoriginalversionofPGPEbySehnkeetal.(2010)considerstocollectM trajectoriesforeachparameter
configurationρsampledfromthehyperpolicyν . Inthepseudo-code(aswellasinthepaper)weconsiderM 1(i.e.,we
θ
“
collectasingletrajectory)inordertomakeGPOMDPandPGPEtestingthesamenumberoftrajectoriesineachiteration,
givenanequalbatchsizeN. IntheoriginalpaperalsoothervariantsofPGPEareconsidered,thatwehavenotconsidered
inorwork. Forinstance,theonewithsymmetricsampling,ortheoneemployingabaselinewhilesampling. Moreover,it
wouldbepossibletolearnaproperexplorationamountσ whilelearningthehyperpolicyparameters,howeverwedecided
P
tokeepσ fixed,forreasonsremarkedinAppendixC.
P
GPOMDP. AsdoneforPGPE,herewereportthealgorithmGPOMDPinitsoriginalversion(Baxter&Bartlett,2001;
Peters&Schaal,2006). Weshowthepseudo-code(Algorithm2)ofsuchoriginalbasicversion,thatisalsotheonewe
analyzedthroughoutthiswork.
Algorithm2GPOMDP.
Input :NumberofiterationsK,batchsizeN,initialparametervectorθ ,environmentM,deterministicpolicyµ ,stepsizeschedule
0 θ
pζ qK´1,explorationparameterσ ,horizonT,discountfactorγ.
k k“0 A
InitializeθÐÝθ
0
foriP K do
Se(cid:74)tth(cid:75)edeterministicpolicyparameters:µ
θ
forlP N do
Sa(cid:74)mp(cid:75)leanoisevectorsequencepε tqT t“´ 01accordingtotheexplorationparameterσ A.
Initializetrajectoryτ asanemptytuple
l
fortP T do
Ob(cid:74)ser(cid:75)vestates
t
Playactiona t“µ θps tq`ε
t
Observerewardr
t
Addtoτ lthetupleps t,a t,r tq
end
end
ř ř `ř ˘
Computethegradientestimator:∇p θJ ApθqÐÝ N1 N
i“1
T t“´ 01 t k“0∇ θlogπ ρpa τi,k|s τi,kq γtrps τi,t,a τi,tq
p
Updatethepolicyparametervector:θÐÝθ`ζ i∇ θJ Apθq
end
Returnθ.
Intheoriginalpaper,itisavailableavariantofGPOMDPwhichemploysbaselineswhilesampling,butinourworkwedo
notnotconsiderthisapproach,asforPGPE.Alsointhiscase,wedecidedtoemployafixedvalueforσ ,evenifitwould
A
bepossibletoadaptitatruntime(AppendixC).
38LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
H.AdditionalExperimentalResults
Inthissection,wepresentadditionalexperimentalresultsforwhatconcernsthecomparisonofGPOMDPandPGPE,and
thesensitivityanalysisontheexplorationparameters,respectivelyσ andσ .
A P
H.1.LearningCurvesoftheVarianceStudyofSection8.
Setting. We show the results gained by learning in three environments of increasing complexity taken from the Mu-
JoCo(Todorovetal.,2012)suite: Swimmer-v4,Hopper-v4,andHalfCheetah-v4. Detailsontheenvironmentalparameters
are shown in Table 3. In order to facilitate the exploration, thus highlighting the results of the sensitivity study on the
explorationparameters,weaddedanactionclippingtotheenvironments.7 Thetargetdeterministicpolicyµ islinearin
θ
thestate,whilethehyperpolicyν employedbyPGPEisGaussianwithaparameterizedmean,andthestochasticpolicy
θ
π employedbyGPOMDPisGaussianwithameanlinearinthestate. BothPGPEandGPOMDPwererunforK 2000
θ
“
iterations,generatingN 100trajectoriesperiteration. Weconductedasensitivityanalysisontheexplorationparameters,
“
using 0.01,0.1,1,10,100 asvaluesforσ2 andσ2. WeemployedAdam(Kingma&Ba,2014)tosetthestepsizewith
t u P A
initialvalues0.1forPGPEand0.01forGPOMDP.Thelatterdoesnotsupportalargerstepsizeduetothehighervariance
oftheemployedestimatorw.r.t.theoneusedbyPGPE.
Environment T γ d d d
S A Θ
Swimmer 200 1 8 2 16
Hopper 100 1 11 3 33
HalfCheetah 100 1 17 6 102
Table3. Parametersoftheenvironments.
HereweshowthelearningcurvesofJ andJ (andtheassociatedempiricalJ )obtainedinthesamesettingofSection8,
P A D
whichisalsosummarizedinTable3.Inparticular,Figures3and4showthelearningcurvesassociatedwiththeHalfCheetah-
v4environment,Figures5and6showtheonesfortheHopper-v4environment,whileFigures7and8showtheonesfor
theSwimmer-v4environment. Inalltheenvironments,itispossibletonoticethat,forincreasingvaluesoftheexploration
parametersσ andσ ,thelearningcurvesJ andJ (optimizedrespectivelybyPGPEandGPOMDP)differincreasingly
P A P A
withtheassociatedempiricaldeterministiconeJ (reportedinright-handsidecolumnintheplots). Thisisduetothefact
D
thatsmallvaluesofσ andσ leadtoaloweramountofexploration. Poorlyexploratoryν andπ makethealgorithmstest
P A θ θ
actionsthatareverysimilartotheonesthattargetdeterministicpolicyµ wouldsuggest. Conversely,largevaluesofσ and
θ P
σ leadtoahigheramountofexploration,thusJ andJ tendtoshowahigheroffsetw.r.t.totheassociatedempiricalJ .
A P A D
HalfCheetah. In Figures 3 and 4, it is possible to see the learning curves of J and J (and the associated empirical
P A
J )seenbyPGPEandGPOMDPwhilelearningonHalfCheetah-v4. Notethat,inthiscase,theoptimalvalueforσ2 is
D P
1, whiletheoneforσ2 is10. WithT 100, PGPEseemstostruggleabitmoreinfindingagooddeterministicpolicy
A “
w.r.t.GPOMDP.Thiscanbeexplainedbythefactthattheparameterdimensionalityd isthehighestthroughoutthethree
Θ
presentedenvironments.
Hopper. InFigures5and6,itispossibletoseethelearningcurvesofJ andJ (andtheassociatedempiricalJ )seenby
P A D
PGPEandGPOMDPwhilelearningonSwimmer-v4. Alsointhiscase,theoptimalvalueforσ2is1,whiletheoneforσ2
P A
is10. AsforHalfCheetah,withT 100,PGPEseemstostruggleabitmoreinfindingagooddeterministicpolicyw.r.t.
“
GPOMDP,evenifthisistheintermediatedifficultyenvironmentforwhatconcernstheparameterdimensionalityd .
Θ
Swimmer. InFigures7and8,itispossibletoseethelearningcurvesofJ andJ (andtheassociatedempiricalJ )seen
P A D
byPGPEandGPOMDPwhilelearningonSwimmer-v4. Notethat,inthiscase,theoptimalvalueforσ2is10,whiletheone
P
forσ2 is1. HereweemployedanhorizonT 200. Indeed,asalsocommentedinSection8,GPOMDPstrugglesmorethan
A “
PGPEinfindingagooddeterministicpolicy.
7Whenthepolicydrawsanactiontheenvironmentperformsaclipoftheactionbeforetherewardiscomputed.
39LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 0.01 JD with P2= 0.01
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 0.1 JD with P2= 0.1
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 1.0 JD with P2= 1.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 10.0 JD with P2= 10.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 100.0 JD with P2= 100.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure3. J andJ learningcurves(5runs,mean˘95%C.I.)forPGPEonHalfCheetah-v4.
P D
40
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JA with A2= 0.01 JD with A2= 0.01
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 0.1 JD with A2= 0.1
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 1.0 JD with A2= 1.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 10.0 JD with A2= 10.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 100.0 JD with A2= 100.0
200 200
150 150
100 100
50 50
0 0
50 50
100 100
150 150
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure4. J andJ learningcurves(5runs,mean˘95%C.I.)forGPOMDPonHalfCheetah-v4.
A D
41
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 0.01 JD with P2= 0.01
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 0.1 JD with P2= 0.1
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 1.0 JD with P2= 1.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 10.0 JD with P2= 10.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 100.0 JD with P2= 100.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure5. J andJ learningcurves(5runs,mean˘95%C.I.)forPGPEonHopper-v4.
P D
42
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JA with A2= 0.01 JD with A2= 0.01
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 0.1 JD with A2= 0.1
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 1.0 JD with A2= 1.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 10.0 JD with A2= 10.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 100.0 JD with A2= 100.0
250 250
200 200
150 150
100 100
50 50
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure6. J andJ learningcurves(5runs,mean˘95%C.I.)forGPOMDPonHopper-v4.
A D
43
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 0.01 JD with P2= 0.01
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 0.1 JD with P2= 0.1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 1.0 JD with P2= 1.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 10.0 JD with P2= 10.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JP with P2= 100.0 JD with P2= 100.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure7. J andJ learningcurves(5runs,mean˘95%C.I.)forPGPEonSwimmer-v4.
P D
44
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JA with A2= 0.01 JD with A2= 0.01
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 0.1 JD with A2= 0.1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 1.0 JD with A2= 1.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 10.0 JD with A2= 10.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
JA with A2= 100.0 JD with A2= 100.0
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
Figure8. J andJ learningcurves(5runs,mean˘95%C.I.)forGPOMDPonSwimmer-v4.
A D
45
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
H.2.GPOMDPvs. PGPE:thecaseofSwimmer-v4.
Inthissection, weconduceexperimentstohighlightthetrade-offbetweenparameterdimensionalityd andtrajectory
Θ
lengthT. Asemergesfromthetheoreticalresultsshowninthemainpaper,GPOMDPshouldstruggleinfindingagood
deterministicpolicyµ fromlargevaluesofT,whilePGPEshouldstruggleinthesametaskformlargevaluesofd . Notice
θ Θ
thatthisbehaviorwasalreadyvisibleinthevariancestudyconductedinSection8andAppendixH.1,whereweadded
actionclippingtoenvironments. Tobetterillustratethetrade-offathand,weremovedtheactionclippingtoconducethe
followingexperimentalresults,restoringtheoriginalversionoftheMuJoCoenvironments. Indeed,weremarkthataction
clippingwasintroducedtofacilitatetheexploration,highlightingtheoutcomesofthevariancestudy.
Setting. Weconsidertwodifferenttargetdeterministicpoliciesµ :
θ
• linear: PGPEandGPOMDParerunforK 2000,withN 100,d 16(parametersinitializedto0);
Θ
“ “ “
• neuralnetwork(twodensehiddenlayerswith32neuronsandwithhyperbolictangentactivationfunctions): PGPEand
GPOMDParerunforK 2000,withN 100,d 1344(parametersinitialvaluessampledbyN 0,1 ).
Θ
“ “ “ p q
Forthelearningrateschedule,weemployedAdamwiththesamestepsizes0.1forPGPEand0.01forGPOMDP(the
reasonisthesameexplainedinSection8). Foralltheexperimentswefixedbothσ andσ to1.
P A
IncreasingT. HereweshowtheresultsoflearningonSwimmer-v4withT 100,200 (andγ 1).Thetargetdeterministic
Pt u “
policy in this case is the linear one, thus d 16. Figures 9 and 10 show the learning curves of J and J , with their
Θ P A
“
associatedempiricalJ .ForT 100,PGPEandGPOMDPreachdeterministicpoliciesexhibitingsimilarvaluesofJ θ .
D D K
“ p q
ForT 200, instead, thealgorithmsreachdeterministicpoliciesshowinganoffsetinthevaluesofJ θ infavorof
D K
“ p q
PGPE.Assuggestedbythetheoreticalresultsshowninthepaper,thefactthatGPOMDPstrugglesinreachingagood
deterministicpolicycanbeexplainedbythedoublingofthehorizonvalue.
Increasingd . HereweshowtheresultsoflearningonSwimmer-v4withT 100,200 (andγ 1),withtwodifferent
Θ
Pt u “
targetdeterministicpolicies: thelinearone(d 16)andtheneuralnetworkone(d 1344).
Θ Θ
“ “
Figures11and 13showthelearningcurvesofJ ,withtheirassociatedempiricalJ ,forboththetargetpolicies,when
P D
learningwithtrajectoriesrespectivelyoflength100and200. Forboththevaluesofthehorizon,itispossibletonoticethat
withasmallervalueofd PGPEmanagestofindabetterdeterministicpolicy. Indeed,thefoundlinearandneuralnetwork
Θ
deterministicpoliciesshowanoffsetinJ θ infavorofthelinearone. Assuggestedbythetheoreticalresultsshownin
D K
p q
thepaper,thefactthatPGPEstrugglesinreachingagooddeterministicpolicycanbeexplainedbytheheavilyincreased
parameterdimensionalityd .
Θ
Figures12and 14showthelearningcurvesofJ ,withtheirassociatedempiricalJ ,forboththetargetpolicies,when
A D
learningwithtrajectoriesrespectivelyoflength100and200. FromFigure12,evenwiththetargetneuralnetworkpolicy,
forT 100GPOMDPisablehowevertofindadeterministicpolicywithsimilarperformancestotheonefoundwhenthe
“
targetdeterministicpolicyisthelinearone. SwitchingtoT 200(Figure14),itispossibletonoticeasevereoffsetbetween
“
thelearningcurvesoftheempiricalJ associatedtoJ ,infavorofthecaseinwhichthetargetpolicyisthelinearone. As
D A
donefortheanalysisontheincreasingT,thiscanbeexplainedbythefactthatthehorizonhasbeendoubled,whichisin
linewiththetheoreticalresultsshownthroughoutthiswork.
46LearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 1 JD with P2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)PGPE.
JA with A2= 1 JD with A2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)GPOMDP.
Figure9. PGPEandGPOMDPonSwimmer-v4withlinearpolicyandT“100(5runs,mean˘95%C.I.).
JP with P2= 1 JD with P2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)PGPE.
JA with A2= 1 JD with A2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)GPOMDP.
Figure10. PGPEandGPOMDPonSwimmer-v4withlinearpolicyandT“200(5runs,mean˘95%C.I.).
47
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 1 JD with P2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)Linear.
JP with P2= 1 JD with P2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)NeuralNetwork.
Figure11. PGPEonSwimmer-v4withlinearandneuralnetworkpolicies,andT“100(5runs,mean˘95%C.I.).
JA with A2= 1 JD with A2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)Linear.
JA with A2= 1 JD with A2= 1
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)NeuralNetwork.
Figure12. GPOMDPonSwimmer-v4withlinearandneuralnetworkpolicies,andT“100(5runs,mean˘95%C.I.).
48
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofrePLearningOptimalDeterministicPolicieswithStochasticPolicyGradients
JP with P2= 1 JD with P2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)Linear.
JP with P2= 1 JD with P2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)NeuralNetwork.
Figure13. PGPEonSwimmer-v4withlinearandneuralnetworkpolicies,andT“200(5runs,mean˘95%C.I.).
JA with A2= 1 JD with A2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(a)Linear.
JA with A2= 1 JD with A2= 1
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 0
10 10
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Iteration Iteration
(b)NeuralNetwork.
Figure14. GPOMDPonSwimmer-v4withlinearandneuralnetworkpolicies,andT“200(5runs,mean˘95%C.I.).
49
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP
xednI
ecnamrofreP