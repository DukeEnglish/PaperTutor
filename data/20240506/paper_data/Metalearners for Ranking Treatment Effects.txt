METALEARNERS FOR RANKING TREATMENT EFFECTS
APREPRINT
Toon Vanderschueren∗ WouterVerbeke FelipeMoraes HugoManuelProença
KULeuven,UniversityofAntwerp KULeuven Booking.com Booking.com
May6,2024
ABSTRACT
Efficientlyallocatingtreatmentswithabudgetconstraintconstitutesanimportantchallengeacross
variousdomains. Inmarketing,forexample,theuseofpromotionstotargetpotentialcustomersand
boostconversionsislimitedbytheavailablebudget. Whilemuchresearchfocusesonestimating
causaleffects,thereisrelativelylimitedworkonlearningtoallocatetreatmentswhileconsidering
theoperationalcontext. Existingmethodsforupliftmodelingorcausalinferenceprimarilyestimate
treatmenteffects,withoutconsideringhowthisrelatestoaprofitmaximizingallocationpolicythat
respectsbudgetconstraints. Thepotentialdownsideofusingthesemethodsisthattheresultingpre-
dictivemodelisnotalignedwiththeoperationalcontext. Therefore,predictionerrorsarepropagated
totheoptimizationofthebudgetallocationproblem,subsequentlyleadingtoasuboptimalallocation
policy. Weproposeanalternativeapproachbasedonlearningtorank. Ourproposedmethodology
directly learns an allocation policy by prioritizing instances in terms of their incremental profit.
Weproposeanefficientsamplingprocedurefortheoptimizationoftherankingmodeltoscaleour
methodologytolarge-scaledatasets. Theoretically,weshowhowlearningtorankcanmaximizethe
areaunderapolicy’sincrementalprofitcurve. Empirically,wevalidateourmethodologyandshow
itseffectivenessinpracticethroughaseriesofexperimentsonbothsyntheticandreal-worlddata.
Keywords CausalInference TreatmentEffectEstimation LearningtoRank
· ·
1 Introduction
Decision-makersneedtodealwithuncertaintyregardingtheconsequencesoftheirdecisions. Anincreasinglypopular
paradigmtoaddressthischallengeistheprediction-optimizationframework. Inafirstpredictionstage,dataisusedto
estimatetheeffectofpossibleactions. Inasecondoptimizationstage,thesepredictionsareintegratedinanoptimization
problemwiththeaimofassigningpersonalizedtreatmentrecommendations,i.e.,allocatingtreatmentstoinstances
tooptimizeanobjectivefunction,whilesatisfyingoperationalconstraints. Theseproblemsarecommoninvarious
domains: e.g.,marketing[1],healthcare[2],maintenance[3],orpolicydesign[4](seeTable1forsomeexamples). We
focusonaspecificclassoftreatmentrecommendationproblemswhereinstancesneedtobeprioritizedfortreatment
(e.g., recommending whom to treat). Our goal is to learn a treatment policy that prioritizes the optimal instances
for treatment. A key part of this problem is estimating each instance’s response to a treatment–i.e., its treatment
effects–fromdatausingcausalinference.
Prediction-FocusedLearning: EffectEstimation Acommonapproachtotackletreatmentallocationproblemsisto
firstpredicttheeffectofanactionforeachinstance. Tothisaim,causalinferencecansupportmanydecision-making
problems: byanalyzingthecausaleffectofpastdecisions,futuredecisionscanbeoptimized. Acommonapproach
istofirstestimatethecausaleffectofpossibledecisionsusingmethodsfortreatmenteffectestimation. Forexample,
inmarketing,toestimatehowdifferentcustomerswouldrespondtoamarketingincentive. Theeffectestimatescan
beintegratedinanoptimizationproblemtomakethefinaldecisionsregardingtreatmentallocation(e.g.,totargeta
specificcustomersegment). Thisapproachhasbeenadoptedtoaiddecision-makingbyavarietyoftechnologyand
e-commercecompanies[5,6,7].
∗Correspondenceto:toon.vanderschueren@gmail.com.WorkconductedwhilethefirstauthorwasaninternatBooking.com.
4202
yaM
3
]GL.sc[
1v38120.5042:viXraMetalearnersforRankingTreatmentEffects APREPRINT
Application Allocationproblem Treatmentoutcome Objective Constraints
Marketing Targetedadvertising Conversion Incrementalsales Marketingbudget
¿ Healthcare Pandemicresponse Infection Mortalityreduction Vaccinesupply
r Maintenance Preventivemaintenance Failurerate Assetuptime Availabletechnicians
˙ Policydesign Targetedsubsidies Bednetpurchase Malariaprevention Public/policybudget
Table1: TreatmentAllocationExamples. Wehighlightseveralapplicationswheretreatmentallocationisrequired,
characterizedby(1)anestimatedtreatmenteffect,(2)anoptimizationobjective,and(3)operationalconstraints. In
marketing,thegoalistotargetcustomersegmentsanddriveconversions,whileadheringtobudgetconstraints. In
healthcareandepidemiology,optimalvaccineallocationduringpandemicsaimstominimizepopulationmortality,
subjecttovaccinesupplies.Inmaintenance,techniciansareassignedtomaintainassets,preventfailures,andmaximizing
operationaluptime.Finally,ineconomicsandpolicydesign,subsidyusageneedstobeoptimizedbyefficientlyallocating
publicfundsandmaximizinghealthcareimpact.
Decision-FocusedLearning:EffectRanking Recentwork,referredtoasdecision-focusedlearning,aimstointegrate
thelearningandoptimizationsteps. Thisapproachrecognizesthatthepredictivetask(i.e.,estimatingtreatmenteffects)
isonlypartofalargeroptimizationproblem(i.e.,allocatingtreatments).Byintegratingthepredictivemodelinthelarger
optimizationpipeline,decision-focusedlearningaimstolearnapredictivemodelthatresultsinbetterperformancefor
thedownstreamtask[8]. Thekeyideaistoaligntheconstructionofthepredictivemodelwiththeoptimizationtask.
We analyze a common type of treatment allocation problem, where treatments are allocated to the instances with
thelargesttreatmenteffect. Inthesesettings,wearguethatdirectlylearninganeffectrankingmightbemoreuseful
than effect estimation. As operational constraints might prevent decision-makers from treating every instance, we
requireknowinghowtoprioritizeinstancesbasedontheirtreatmenteffect. Comparedtoindependentlyestimatingeach
instance’seffect,wearguethatdirectlylearningtherankingacrossinstancesmayyieldbetterresults. Becauseeffect
estimationprioritizesaccurateandwell-calibratedeffectestimates,itoverlookstheestimates’rankingandresulting
decisionquality(i.e., estimationandoptimizationarenotaligned). Whilesuccessfulwhenpredictionsareperfect,
thismisalignmentcanresultinsuboptimaldecision-makinginreality. Conversely,ourworkdemonstratesthateffect
rankingcandirectlyoptimizethequalityofthetreatmentassignment(i.e.,therankingobjectiveisperfectlyaligned
withthedecision-makingtask). WecontrastbothapproachesinTable2. Additionally,empiricalriskminimization
onlyguaranteesmodelgeneralizationforthespecificobjectivethatwasoptimizedfor[9],furthermotivatingustofind
objectivesthatarealignedtothefinaloptimizationproblem.
Contributions Thisworkproposesdecision-focusedlearningframeworkfortreatmentrecommendationproblems.
Weformalizetheclassofproblemsthatcanbetackledusingeffectrankinganddiscusstheunderlyingassumptions. We
describehowlearningtorankcanbeusedforthistaskandproposedifferentcausalmetalearnersforrankingeffects.
Ourcontributionsareasfollows. (1)Conceptually, weformalizetreatmentallocationproblemsthatcanbesolved
usingrankingandanalyzetheunderlyingassumptions(seeSection3). (2)Methodologically,weproposedifferent
metalearnersforrankingtreatmenteffects,basedonpairwiseandlistwiserankingobjectivesthatscaleefficientlyto
large-scaledatasets. Weshowhowourproposedlistwiseobjectivedirectlyoptimizesthepolicy’sareaundertheQini
curve(seeSection4). (3)Empirically,wecompareourproposedeffectrankingwitheffectestimationusingsynthetic
andreal-worlddatasets(seeSection5).
2 RelatedWork
In the following, we discuss two areas of related work. First, causal inference and, more specifically, estimating
treatmenteffects. Second,learningpoliciesfortreatmentrecommendation.
2.1 Prediction-FocusedLearning: EffectEstimation
Understandingtheimpactofanactiononaninstanceorindividualiscrucialinavarietyofdomainswherepersonalized
decision-makingisvaluable,suchasmarketing,healthcare,oreducation. Centraltothisiscausalinference: using
datatodrawconclusionsregardingcausalrelationships. Especiallyrelevanttoourworkistreatment(orcausal)effect
estimation,wheretheaimistolearnhowsometreatment,action,orinterventionwillaffectaninstance’soutcome
ofinterest. Foracomprehensivereview,werefertoZhangetal. [10]. Inthecontextofmarketing,usingmachine
2MetalearnersforRankingTreatmentEffects APREPRINT
Instances(e.g.,customers) Performance
MSE Ranking
Truetreatmenteffect 0.20 0.17 0.16 0.15 0.11 0.10 — —
Effectestimation: model1 0.22 0.15 0.16 0.17 0.10 0.11  
Effectranking: model2 0.25 0.22 0.20 0.15 0.10 0.05  
Table2: ComparingPointwiseEstimationandEffectRankingforTreatmentAllocation. Thisillustrativeexampleshows
thetruetreatmenteffectforseveralinstances,orderedfromlargesttosmallest. Thefirstmodelestimateseachinstance’s
treatmenteffectfairlyaccuratelyintermsofMSE.However,therankingofinstancesbasedontheseestimatesdiffers
significantlyfromtheirtrueorder,whichwillresultinsuboptimaltreatmentallocationwhenonlysomeinstancescanbe
treated. Forthesecondmodel,weobservetheoppositescenario: estimatesarepoorintermsofMSE,buttheirranking
respectsthetrueorder.
learning(ML)fortreatmenteffectestimationisgenerallyreferredtoasupliftmodeling,asdiscussedinseveralsurveys
[11,12,10].
Specializedmethodshavebeenproposedforestimatingtreatmenteffects.First,generalstrategiesexistforlearningthese
effects. Causalmetalearnersaremodelingframeworksforeffectestimation,compatiblewithvariousMLalgorithms
[13,14,15]. Relatedly,responsetransformationapproachestransformaninstance’soutcomesothatitcanbemodeled
usingastandardclassifier[16,11]. Second,MLalgorithmshavebeenadaptedforeffectestimation,suchasdecision
trees[17]orrandomforests[18,19].
Whileupliftmodelinghastraditionallyfocusedonoptimizingconversion,practitionersoftenseektooptimizeother
metricsrelatedtotheirbusinessandoperationalcontext. Recentworkexplorescost-sensitiveorprofit-drivenuplift
modeling,wheretheaimistoestimateandmaximizeprofitandcostresultingfromtargetingpolicies[20,21,22,23].
Forexample,theIncrementalProfitperConversion(IPC)hasbeenproposedasaresponsetransformationapproachfor
incrementalprofit[24].
Allworkinthiscategoryaimstoestimatetheeffectofatreatment(e.g.,acustomer’sincrementalconversionprobability
asaresultofreceivingamarketingincentive). Asdiscussedintheintroduction,theseestimatescanbeusedtodesigna
treatmentallocationpolicy(e.g.,bytargetingcustomerswithalargeestimatdeffect). Inpractice,operationalconstraints
(suchasbudgetlimitations)maycallformorecomplexoptimizationprocedures[25,26,27,1]. Theresultingtreatment
allocationproblemrequiressolvingaconstrainedoptimizationproblemusingtheeffectestimatesasinput.
2.2 Decision-FocusedLearning: EffectRanking
Asarguedbefore,theprediction-focusedapproachmaysufferfromamisalignmentbetweenpredictionandoptimiza-
tion, leading to suboptimal treatment decisions. Additionally, empirical risk minimization only guarantees model
generalizationforthespecificobjectivethatwasoptimized[9]. Decision-focusedlearningaimstoalignthetwophases
byusinganintegratedapproachanddirectlyoptimizingapredictivemodelforthefinaloptimizationproblem. Related
tothisinsight,ahandfulofmethodshaverecentlybeenproposedfortreatmentrecommendationthataimtolearna
rankingofinstancesintermsoftheirtreatmenteffect. Althoughthesemethodswereoriginallyproposedinthecontext
ofupliftmodeling–framingtargetedmarketingasarankingproblem–theyaremoregenerallyapplicable. Finally,ithas
beennotedthatanyscore–evennon-causalestimands–canbeusedtoprioritizeinstancesfortreatment,aslongasitisa
goodproxyforthetreatmenteffect’smagnitude[28,29].
Table3highlightsrelatedmethodsforrankingtreatmenteffects,describingthemetalearnersandoptimizationstrategies
thatwere used, where wefollowtheliteratureon learningtorank [30, 31]. Thefirstapproach, pointwise ranking,
reliesonanestimateofthetreatmenteffectandcorrespondstoprediction-focusedlearning. Thesecondapproach,
pairwiseranking,aimstopredicttherelativerankingbetweeninstancesoverallpairsofinstances. Thefinalapproach,
listwiseranking,optimizestherankingacrossallinstancesintherankingsimultaneously. Intheliteratureonlearningto
rank,pairwiseandlistwiserankingapproacheshavesurpassedpointwiseapproaches,withlistwisemethodstypically
performingbest[31].
Mostexistingapproachesforrankingeffectsrelyonalternativeobjectivefunctionsthataimtointegratepredictionand
optimizationusingLagrangianduality[32,33,34]orgradientestimationtechniques[35]. Alternatively,thecausal
profitranker[23]ranksinstancesinapost-processingstageusingpointwiseestimatesoftheexpectedconversionMore
3MetalearnersforRankingTreatmentEffects APREPRINT
Objective Metalearners
Ref. Point Pair List Z S T X DR R
[32] ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗
[33] ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓
[36] ✓ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗
[9] ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗
[23] ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗
[34] ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗
Ours ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table3: LiteratureTable. Wecategorizerelatedworkoneffectrankingbydifferentiatingbetweendifferent(1)ranking
approaches(point-,pair-,andlistwise)and(2)metalearners.
advancedpairwise[9]andlistwise[36]learningtorankhavealsobeenexploredinthiscontext. Conversely,ourwork
explorespointwise,pairwise,andlistwiseobjectives,aswellasawidevarietyofmetalearners.
Generalmethodologieshavebeenproposedforlearningatreatmentpolicy,directlymappinganinstance’scharacteristics
to a recommended treatment [37, 38, 39, 40, 41, 42]. In the context of our work, these can be seen as pointwise
approaches,astheydonotconsidertherankingstructureoftheoptimizationtask.
3 ProblemFormulation
Inthiswork,weaimtolearnatreatmentpolicythatprioritizesinstancesfortreatmenttomaximizetheaggregateeffect.
Asopposedtotheexistingwork,whichassumesaproblemformulationimplicitly,weexplicitlyformalizeourproblem
setting. Indoingso,werevealtheunderlyingassumptionsrequiredforourapproach.
3.1 NotationandOptimizationProblem
Letaninstance(e.g.,acustomer)bedescribedbyatuple(x ,t ,y ),representingcovariatesX Rd,anadministered
i i i
treatmentt 0,1 ,andtheoutcometobeoptimizedY R. Wedenotethepotentialoutcom⊂ eY associatedwitha
∈{ } ⊂
treatmenttasY(t)andaninstancesi’streatmenteffectasτ =Y (1) Y (0)(e.g.,acustomer’sincrementalconversion
i i i
−
probabilityresultingfromreceivingadiscount). Weaimtolearnapolicyπthatassignstreatmentstoinstancesand
maximizestheoveralltreatmenteffect,whilerespectingpossibleoperationalconstraints. Attesttime,weassumen
instancescanbetreated,subjecttoatreatmentbudgetBwithB n. Thisyieldsthefollowingoptimizationproblem:
≤
n
(cid:88)
max τ (t )
i i
ti
i=1
n
(cid:88)
s.t. t B
i
≤
i=1
t 0,1 i 0,...,n
i
∈{ } ∀ ∈{ }
Attesttime,treatmenteffectsτ areunknownandneedtobeestimated. Tothisend,weassumeaccesstoahistorical
dataset = x ,t ,y (t ) m describingpasttreatmentdecisionsandtheresultingoutcomes. Thesedatacanbeused
toestimaD teth{ eci ondi itii onai l} ai v= e1 ragetreatmenteffect(CATE):τˆ=E(Y(1) Y(0) X =x).
− |
3.2 Assumptions
Wemakeseveralassumptionsregardingthecausalstructureofthedataandtheoperationalconstraint. Toestimate
thecausaleffectfromhistoricaldata,werequirethestandardassumptionsforidentifiabilityincausalinference[43]
(seeAppendixAforamoreextensivediscussion). Additionally,wemoreformallydefinetheoperationalconstraint.
Althoughweassumethatnotallinstancescanbetreatedand,thus,instanceprioritizationisrequired,weassumethat
theexactbudgetisnotknowntothedecision-makerapriori. Moreformally,westatethereisnoinformationregarding
thebudgetBapriori:
Assumption1(Operationalconstraint). Weassumetheexactbudgetisunknown, buttheexpectationisuniformly
distributedamong 1,...,n : B (1,n).
{ } ∼U
4MetalearnersforRankingTreatmentEffects APREPRINT
Cumulative
Perfect
effect
Policy
Random
AUQC
Instances
treated
1 n
Figure1: EvaluatingaTreatmentAllocationPolicy. WecomparetargetingpoliciesusingaQinicurve,depictingthe
cumulativetotaleffectofapolicyforanumberoftreatedinstances, summarizedbytheareaundertheQinicurve
(AUQC).
Wediscusshowalternativeassumptionsregardingthebudgetwouldaffectourproposedsolutionbelow.
3.3 EvaluatingaTreatmentPolicy
WeassessthequalityofaproposedpolicyusingtheQinicurve,illustratedinFigure1. Thiscurveshowsthecumulative
totaleffectofapolicyforanumberoftreatedinstances[44,45]. Giventhatweassumenoinformationregarding
thebudget,wemeasureapolicy’soverallqualityusingtheareaundertheQinicurve(AUQC),quantifyingthetotal
cumulativeeffectovertheentireranking. Formally,wedefinethe(hypothetical)AUQCas
n k
(cid:88)(cid:88)
AUQC= τ (1)
i
k=1i=1
withτ theeffectoftheinstanceatpositioniintheranking. ThenormalizedAUQCisobtainedbycomparingitwith
i
theexpectedAUQCofarandomrankingandAUQCofaperfectranking. Typically,thenormalizedAUQCranges
betweenzeroandone [0,1],thoughaworsethanrandompolicywithAUQC<0isalsopossible. Becauseeffectsτ
∈
arenotobservedinreality,Qinicurvesneedtobeestimatedfromdataonpasttreatmentallocations[36,45].
4 Methodology
Giventheproblemsetupdescribedabove,wenowpresentourproposedmethodology,whichessentiallylearnsaranking
(orsorting)ofeachinstance’streatmenteffect. Theoptimalityofthissolutioncanbeseenasfollows. Ifonlyone
instancecanbetreated(i.e.,B = 1),theoptimalsolutionistoassignthetreatmenttotheinstancewiththelargest
treatmenteffectτ . Givenanunknownbudgetanduniformexpectationregardingthisbudget,theoptimalsolutionis
i
thentorankallinstancesbytheirtreatmenteffectτ andassigntreatmentstothetopinstancesuntilthebudgetruns
i
out. Therefore,ourgoalistopredictanoptimalorderingorassignmentpolicyπ Π thatpermutesthetestinstances
n
∈
1,...,n totheoptimalorderingbasedondescendingtreatmenteffectsτ.
{ }
Aspreviouslydiscussed,mostexistingapproachesfirstestimatestheeffectsτˆandthenranktheseestimates. However,
asdiscussedaboveandinTable2,thisapproachhastwodrawbacks. First,theestimator’sobjectiveisnotalignedwith
theoptimizationtask,possiblyresultinginsuboptimaldecisions[8]. Second,theresultingmodelisonlyguaranteedto
generalizeforthepredictiveobjectivethatwasused[9]. Theseissuesmotivateustodirectlylearnarankingpolicyπ
basedoninstancecharacteristicsX,whichrequiresaddressingtwochallenges. First,tofindanobjectivethatoptimizes
arankingofinstancesinsteadofapointwiseestimate(seeSection4.1). Second,therankingneedstobebasedon
thetreatmenteffectτ,whichisnotobserved. Therefore,weextendmetalearnersforeffectestimationtoranking(see
Section4.2).
4.1 OptimizingaRankingObjective
Inthissection,weexploreapproachesforoptimizingaranking.Wediscusspointwise,pairwise,andlistwiseapproaches.
Weproposealistwiseobjectivethatoptimizesthepolicy’sAUQCdirectly. Additionally,weproposeasamplingstrategy
toimprovetheefficiencyofourproposedrankingobjectives.
4.1.1 Rankingobjectives
WedescribethreeobjectivesforlearningarankingpolicyπthatcanbeusedbyMLalgorithms.
5MetalearnersforRankingTreatmentEffects APREPRINT
Pointwise The first approach, used by most existing work, is to learn a pointwise estimate of the effect. As the
treatmenteffectitselfisneverobserved,thiscorrespondstoeitherlearningtheobservedoutcomey(t)oratransformed
outcome, depending on the metalearner used (see Section 4.2). In this work, we use the mean squared error as a
pointwiseobjectivetolearntheestimand:
n
(y,yˆ)= 1 (cid:88) (y yˆ)2 . (2)
Point i i
L n −
i=1
Whileotherobjectivesarepossible[e.g.34],pointwiseapproachesbydefinitionignoretheinstancerankingresulting
fromthepointestimates. Thismotivatesourexplorationofalternativeobjectives.
Pairwise Thefirstrankingapproachisthepairwiseapproach. Theideaistopredict,foreachpairofinstances,how
bothinstancesarerankedrespectivetoeachother. Ifallpairsarerankedcorrectly,theoverallrankingwillalsobe
correct. WebuildupontheapproachproposedforRankNet[46]. Beforewedefinethepairwiseobjective,wedefinethe
pairwiseoutcomey thatspecifieswhetherinstanceiorj shouldberankedhigher,foreachpairofinstancesiandj:
i,j
(cid:26)
1 ify y
i j
y i,j = 0 ify ≥ <y .
i j
Similarly,wedefineasmoothpairwisepredictionyˆ ,combiningtwoinstances’predictionsyˆ andyˆ asfollows:
i,j i j
1
yˆ i,j = (cid:0) (cid:1), (3)
1+exp σ(yˆ yˆ )
i j
− −
wherethesigmoidparameterσ controlsthesmoothnessofthecomparison. Intheextremeσ = ,thisbecomesa
∞
stepfunction. Thisway,wedefinepairwiserankingasabinaryclassificationtaskwiththepairwisecross-entropyloss
definedas:
n n
(cid:88)(cid:88) (cid:0) (cid:1)
(y,yˆ)= y log(yˆ ) (1 y ) 1 log(yˆ ) . (4)
Pair i,j i,j i,j i,j
L − − − −
i=1j=1
Otherpairwiseobjectivesexist[e.g.9]whichcanbeappliedtoourapproach,thoughweconsiderthisoutsidethescope
ofthiswork.
Listwise Adrawbackofthepairwiseapproachisthatitoverlookstherelativeimportanceofcorrectlyclassifyingone
paironthelistwiserankingquality. Toaddressthisissue,theLambdaMARTobjective[47]addsaweightNDCG to
i,j
thepairwiseobjective,reflectingtheincreaseinnormalizeddiscountedcumulativegain(NDCG,seebelow)achieved
byswappingthatpair:
n n (cid:18) (cid:19)
(cid:88)(cid:88) (cid:0) (cid:1)
(y,yˆ)= y log(yˆ ) (1 y ) 1 log(yˆ ) ∆NDCG .
List i,j i,j i,j i,j i,j
L − − − −
i=1j=1
4.1.2 TheAUQCasaspecificinstanceoftheNDCG
Thenormalized discountedcumulativegain(NDCG)is aclass ofmetricsmeasuring thequality ofa ranking[48].
Formally,wedefinearankingπ,withπ representingthei’thinstanceintheranking. Aninstance’sgaing represents
i i
itsvalueindependentofitspositionintheranking(e.g.,thetreatmenteffectτ). TheNDCGdecreasesthegainfor
lowerranks,reflectingtheirdecreasingimportance,byapplyingadiscountfunctiond(i)totheinstance’sgaing . The
i
discountedcumulativegain(DCG)isthesumofalldiscountedgainsovertherankingDCG=(cid:80)n
d(i)g . Finally,
i=1 πi
thenormalizeddiscountedcumulativegain(NDCG)isobtainedbycomparingtheDCGwiththeperfectranking’sDCG
togetavaluebetweenzeroandone.
WeproposeaspecificinstantiationoftheNDCGthatmatchestheAUQC.Morespecifically,wedefineaninstance’s
gaing asitstreatmenteffectτ. Thediscountfunctionisalinearlydecreasingfunction: forranki,thediscountequals
i
(n i+1)2. Inthisspecification,wecanshowthatthelistwiseobjectivethatoptimizestheNDCGallowsustolearn
−
anoptimalrankingpolicyπthatoptimizesthemetricofinterest,theAUQC(generalizing[36,Section3.2.2]fromthe
Z-Learnertoanymodelthatestimatesτ):
2Technically,wedonotdiscountlowerrankedinstances(d(i)≤1),butratherpromotehigherrankedinstances(d(i)≥1)[see
36,eq.25].
6MetalearnersforRankingTreatmentEffects APREPRINT
Proof. TheareaundertheQinicurve(AUQC)isaninstantiationofthenormalizeddiscountedcumulativegain(NDCG):
n k n n n n n
(cid:88)(cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88)
AUQC= τ = τ = τ 1= τ (n i+1)=NDCG
πi πi πi πi
−
k=1i=1 i=1k=i i=1 k=i i=1
fortheNDCGwithgaing =τ andd(i)=n i+1.
πi πi
−
GiventhatthelistwiseobjectivedescribedabovecanbeshowntooptimizetheNDCG[49,47],thisresultprovesthat
ourproposedobjectivecanbeusedtodirectlyoptimizethemetricofinterest: theAUQC.Thereisoneremaining
challenge: wedonotknowntheinstance’streatmenteffectτ andrequireavalidestimatorτˆ τ. Wewilltacklethis
→
partinSection4.2.
4.1.3 Efficientlyscalingtolarge-scaledatasets
Movingfrompointwisetopairwiseorlistwiseoptimizationrequiresaddressingachallengeregardingcomputational
efficiency. Optimizingoverpairsofinstancesresultsinanincreaseofthealgorithm’stimecomplexityfromO(n)
toO(n2). Thiscomplexityisnotcompatiblewiththelargedatasetscommonlyencounteredinapplicationssuchas
marketingore-commerce[27].
Toaddressthischallenge,weproposeanefficientsamplingprocedurethatfindsastochasticestimateofthegradient.
Intuitively,insteadofcalculatingthegradientbasedonallpossiblepairs,wesamplekpairsperinstance:
n
(y,yˆ)=(cid:88)(cid:88) y log(yˆ ) (1 y )(cid:0) 1 log(yˆ )(cid:1) with ( )k,
Pair i,j i,j i,j i,j [1,...,n]
L − − − − J ∼ U
i=1j∈J
andequivalentlyfor . Thisagainmakestheprocedurescalelinearlyinthenumberofinstanceswithcomplexity
List
L
O(kn). Weobservegoodresultsfork =1,effectivelyobtainingthesamecomputationalcomplexityasthepointwise
objective. Wepresentasensitivityanalysisforthenumberofsampleskbelow. Weoptforthissamplingprocedurefor
itssimplicity,althoughmoreadvancedsamplingschemesarepossible[seee.g.50].
4.2 RankingMetalearners
Onechallengewhenlearningatreatmentassignmentpolicyπisthatdecisionsneedtobemadebasedonthetreatment
effectτ. Indeed,theoptimizationoftheAUQCpresentedaboverequiresamodeltopredictthetreatmenteffectτˆ.
Predicting a treatment effect τ is a challenge. We never observe the treatment effect itself, but only one potential
outcomey(t)foreachinstance,i.e.,theoutcomewhentargetedornottargeted–alsocalledthefundamentalproblemof
causalinference[51]. Inotherwords,weneveractuallyobservewhatwearetryingtoestimateandoptimizeover.
Totacklethischallenge,weimplementtheobjectivefunctionsintroducedabovefordifferentcausalmetalearners–
generalstrategiesforusinganyMLmethodfortreatmenteffectestimation. Whereasmetalearnershaveoriginally
been proposed for effect estimation, we propose adaptations for effect ranking below. In practice, this adaptation
consistsofintegratingranking(i.e.,pairwiseorlistwise)objectivesineachtrainingprocedure–insteadofthetraditional
pointwise (regression or classification) objectives. In this section, we describe each metalearner and introduce its
rankingequivalent. Wefocusonseveralestablishedmetalearners,buttheextensiontoothermetalearnerscouldbedone
usingasimilarapproach. WhileouroptimizationoftheAUQCrequiresmetalearnersthatdirectlypredictthetreatment
effectτ,wealsodiscussadaptationsofothermetalearners–specifically,theS-andT-Learner.
Z-Learner Thefirstmetalearnerestimatesatransformationzoftheoutcomey–alsocalledtheclasstransformation
approach[52,53,24]–adjustingtheoutcomebasedontheinstance’spropensityscore3eˆ(x)=P(T =1X =x)and
|
theobservedtreatment:
(cid:26)
y /eˆ ift =1
i i i
z =
i y /(1 eˆ) ift =0
i i i
− −
TheZ-Learnerestimatesthetreatmenteffectusingthisoutcome:
f (x)=E(Z X) with τˆ=f (x)
Z Z
|
Insteadoftrainingthisfinalmodelf (x)withapointwiseobjective,weproposetooptimizeitusingapairwiseor
Z
listwiseobjective.
3Giventhatweassumedatacomesfromarandomizedtrial,wecanestimatethepropensityscorebytheproportionoftreated
instanceswithoutfittingamodel.
7MetalearnersforRankingTreatmentEffects APREPRINT
S-Learner TheS-Learnerestimatesasinglemodel,whichtakesthetreatmentasa(regular)feature:
f (x,t)=E(Y X =x,T =t) with τˆ=f (x,1) f (x,0)
S S S
| −
Again, the model f (x,t) can be trained with either a pointwise, pairwise, or listwise objective. For the ranking
S
objectives,theestimatedtreatmenteffectτ correspondstotheincreaseintherankingscoreresultingfromreceivingthe
treatment. Importantly,however,becausethismetalearnerdoesnotdirectlyoptimizeforτ,thetheoreticalresultsofthe
previoussectiondonotapply. Nevertheless,althoughthereisnoguaranteethatthelistwiseobjectiveoptimizesthe
AUQCfortheS-Learner,thedifferenceinrankingscorescouldprovideagoodheuristicforthetreatmenteffect.
T-Learner TheT-Learnertrainstwomodels: onemodelforeachtreatmentgroup—f forthetreatment(T =1)
T1
andf forthecontrolgroup(T =0)—trainedasfollows:
T0
f (x)=E(Y X =x,T =1), f (x)=E(Y X =x,T =0).
T1
|
T0
|
Whencombined,thesecanestimatethetreatmenteffectasfollows:
τˆ=f (x) f (x).
T1
−
T0
Weproposeto trainboth modelsf (x) and f (x) witha pairwiseor listwiseobjective, insteadof thetraditional
T1 T0
pointwiseobjective. ThiscorrespondstoaseparateoptimizationoftheAUQCofthetreatment(f (x))andcontrol
T1
(f (x))groups(basedontheoutcomeyinsteadoftheeffectτ). Aninstance’sdifferenceinrankingscoresbetween
T0
both groups is used as a proxy for its treatment effect. Similarly to the S-Learner, the theoretical results from the
previoussectiondonotapply. Nevertheless,thisseparateoptimizationcouldbeagoodheuristicfortheAUQC[see
also11,eq. 26].
X-Learner TheX-Learnerfirstestimatesaninitialtreatmenteffectbyimputingthecounterfactualpotentialoutcome
usingaT-Learnermodelasfollows[14]:
D1 =y f (x ) ift =1, D0 =f (x ) y ift =0.
i i − T0 i i i T1 i − i i
Thefinaltwomodelsarethentrainedontheimputedeffects:
τˆ0 =f0(x)=E(D0 X =x), τˆ1 =f1(x)=E(D1 X =x).
X i| X i|
Toobtainthefinalpredictedeffectτˆ,wecombinethesetwomodels:
i
with τˆ=g(x)f0(x)+(1 g(x))f1(x).
X − X
Fortheweightingfunctiong(x),weusetheestimatedpropensityscoreeˆ(x)=P(T X =x)following[14]. Compared
to the pointwise variant, we propose to train the final models f0(x) and f1(x)| with pairwise or listwise ranking
X X
objectives, with the initial models still trained using a pointwise objective. The final ranking score τˆ is a linear
combinationoftworankingscores[see47,note7.1].
DR-Learner TheDoublyRobustorDR-Learner[54,55]alsoreliesonafinalmodelestimatingapseudo-outcome. In
thiscase,thefirststageisbasedonpointwiseestimatesfromaT-Learnerandapropensitymodeleˆ(x)=P(T X =x).
|
Theseestimatesarecombinedtocreateapseudo-outcomeϕasfollows:
t eˆ(x )
i i
ϕ = − (y f (x ))+f (x ) f (x ).
i
eˆ(x i)(1 eˆ(x i))
i
−
Tti i T1 i
−
T0 i
−
Thispseudo-outcomeisthenusedtolearnafinalmodelτˆ=f (ϕx),whichcanbelearnedusingapoint-,pair-,or
DR
|
listwiseobjective.
R-Learner For the R-Learner [56], we first fit an outcome model mˆ(x) = E(Y X = x) and propensity model
|
eˆ(x)=P(T X =x). ThesecanthenbeusedtominimizetheR-Loss,basedonRobinson’sdecomposition[57],which
|
canbeseenasaweightedMSE:
n n (cid:18)(cid:18) (cid:19) (cid:19)2
R (y,yˆ)= 1 (cid:88) ((y mˆ(x )) (t eˆ(x ))τ(x ))2 = 1 (cid:88) 1 y i −mˆ(x i) τ(x ) .
LMSE n i=1 i − i − i − i i n i=1 (t i −eˆ(x i))2 t i −eˆ(x i) − i
Insteadofthispointwiseobjective,weproposetouseaweightedpair-orlistwiseobjectivebasedonthesameweights
andlabels.
Forsimplicity,wedonotuseout-of-foldestimatesforanyoftheintermediarymodelsforanyofthemetalearners,but
rathertrainallmodelsonthesametrainset.
8MetalearnersforRankingTreatmentEffects APREPRINT
Synthetic Criteo Hillstrom (Female)
AUQC AUQC AUQC
1.0 1.0 1
0.8
0.6 0.5 0
0.4 0.0
1
0.2 −
0.5
0.0 −
Z S T X DR R Z S TSynthXeticDR R Z S T X DR R
Hillstrom (AMaUlQeC) Promotion
AUQC 0.3 AUQC
1.0
1
0.2 0.5 Pointwise
0 Pairwise
0.0 Listwise
−1 0.1 −0.5
Z S T X DR R Z S T X DR R
Figure2: RankingQualityforDifferen0t.0ObjecZtivesanSdMetaTlearnerXs. ForeDaRchmeRtalearner,wecompareapoint-,pair-,
andlistwiseversion. WeshowtheAUQC onestandarderror,scaledtohavethebestresult=1,forfivedifferentdata
±
sets.
5 EmpiricalResults
Thissectionpresentstheempiricalresults,comparingourproposed(pairwiseandlistwise)effectrankingmetalearners
withtraditional(pointwise)effectestimationmetalearners. Ourexperimentsaimtoanswerthreeresearchquestions.
(RQ1)Whatisthetreatmentrecommendationqualityresultingfromthedifferentmethods,asmeasuredinAUQC?
(RQ2)Whataretheperformancetrade-offsofthepointwise,pairwise,andlistwiseobjectives,intermsofMSE(i.e.,
pointwise accuracy), Kendall τ (i.e., pairwise rank correlation), and AUQC (i.e., listwise rankingquality)? (RQ3)
Howsensitiveareourproposedmethodstokeyhyperparameters? Thissectionpresentsthesetupofourexperimental
evaluationandtheempiricalresults.
5.1 DataandBenchmarks
Toevaluatetheperformanceoftheproposedapproaches,weuseatotaloffourdatasetsbasedonrandomizedtrials:
1)asyntheticdataset;2)Criteo[5];3)Hillstrom(MaleandFemale)[58];4)aproprietarydatasetfromapromotion
campaignataglobalonlinetravelagency.
WefirstsimulateaSyntheticdataset. Simulateddataallowsforamorecomprehensiveevaluationthanrealdata,
asweknowthetreatmenteffectfortestinstances. Thedatageneratingprocessisinspiredbyane-commercesetting
andsimilartoexistingwork[24]. First,wegeneratecustomercharacteristicsasfollows: X (0,1)d. Then,we
∼ N
generateasale(orconversion)probabilityS basedonthesecharacteristicsandrandomcoefficientsU ( 1,1)d
s
as S = 1 , where ϵ (0,0.1). Similarly, we generate a potential revenue R u∼ sinU g r− andom
1+exp(−(cid:80) dUsX+ϵs) s ∼ N
coefficientsU ( 1,1)d asR = 1+ (cid:80) U X +ϵ ,whereϵ (0,0.1). ThecostofthetreatmentC (the
r ∼ U − | d r | r r ∼ N
marketing incentive or discount) is defined as 10% of the revenue: C = 0.1R. The observed outcome Y, the net
revenuegeneratedforthatcustomer,istherevenueforthatcustomerminusthetreatmentcost–simulatedasfollows:

r c ift =1ands =1
 i c− i ifti =1andsi
=0
i i i
y i = − r ift =0ands =1
 i i i
0 ift =0ands =0.
i i
Wegenerate10,000instanceswithd=10characteristics.
Next,wealsocomparewiththreereal-worlddatasets. TheCriteodataset[5]istheresultofarandomizedtrialtesting
whethershowinganadvertisementincreasesacustomer’svisitorconversionprobability. Fortheoutcome,wefollow
[34]andtakethenetrevenuey asconversionminusvisit. Toreducetrainingtimes,werandomlysample500,000
instancesfromthisdataset.
TheHillstromdatasetwascollectedtotestwhetherane-mailcampaignresultedinadditionalsales. Twotreatments
wererecorded: aMen’sandWomen’se-mail. Therefore,wesplitthedataintwodatasetsforbothtreatments,andusethe
samecontrolgroupforboth. Wecalculateeachcustomer’snetrevenueasrevenue(conversiontimesspend)minusvisit.
Finally,datafromaPromotioncampaignatalargeonlinetravelagencywasusedasarandomizeddatasettoevaluate
theofflineperformanceofdifferentapproachesinareal-worldsetting.
9MetalearnersforRankingTreatmentEffects APREPRINT
0.3 0.3 0.3
0.2 0.2 0.2
Pointwise(ρ= 0.77) Pointwise(ρ=0.12) Pointwise(ρ=0.22)
−
0.1 Pairwise(ρ= 0.2) 0.1 Pairwise(ρ= 0.37) 0.1 Pairwise(ρ=0.93)
− −
Listwise(ρ= 0.67) Listwise(ρ= 0.67) Listwise(ρ=1.0)
− −
100 200 300 400 500 600 100 200 300 400 500 600 0.05 0.10 0.15 0.20 0.25 0.30
MSE MSE Kendallτ
Figure3: AnalyzingPerformanceTrade-offsonSyntheticData. Wecomparethethreedifferentobjectives(point-,
pair-,andlistwise)acrossmetalearners. UsingtheSyntheticdataset,wecompareperformanceintermsofMSE(i.e.,
pointwiseaccuracy),Kendallτ (i.e.,pairwiserankcorrelation),andAUQC(i.e.,listwisedecisionquality). Foreach,
weshowthecorrelationρ.
Pointwise Pairwise Pairwise(nonormalization) Listwise Listwise(nonormalization)
0.2 0.06 0.06 0.06 0.06 0.06
0.1 0.03 0.03 0.03 0.03 0.03
k k k k k k
0.0 0.00 0.00 0.00 0.00 0.00
100101102 100101102 100101102 100101102 100101102 100101102
(a)Z-Learner (b)S-Learner (c)T-Learner (d)X-Learner (e)DR-Learner (f)R-Learner
Figure4: WhatIstheEffectoftheNumberofSamplingIterationsk? WeshowperformanceintermsofAUQCfor
the different metalearners on the Synthetic data set. We fix the sigmoid parameter σ = 1 and train with default
hyperparameters.
WecomparethethreeobjectivesandmetalearnersforgradientboostingbasedonLightGBM[59]. Weimplementour
proposedpairwiseandlistwiseobjectivesasacustomobjectivesandmetricsinLightGBM.Foreachdataset,werun
afive-foldcross-validationprocedure. Werunhyperparametertuningusing10randomsamplingiterationsoverthe
followinghyperparameters:“num_leaves” [10,50],“learning_rate” [0.01,0.20],“max_depth” [3,10],and
∈ ∈ ∈
“min_data_in_leaf” [10,30]. Weuse64%ofallinstancesfortraining,16%forvalidation,and20%fortesting.
∈
5.2 ComparingPerformancefortheDifferentObjectivesandMetalearners(RQ1)
Foreachmetalearner,wecompareamodeltrainedwithonlyapointwiseobjectivetoourproposedrankingalternatives,
basedoneitherpairwiseandlistwiseobjectives. Weevaluatethequalityofeachtreatmentallocationpolicybylooking
atthecumulativetreatmenteffectovertheinstanceranking,measuredusingtheAUQCpresentedabove. Weevaluate
theperformanceforeachmetalearnerandobjectiveoverthedifferentdatasets(seeFigure2andAppendixB.1). Across
datasetsandmetalearners, weobservethatlistwisemetalearnersgenerallyresultinbettertreatmentprioritization.
Overalltesteddatasetsandmetalearners,apointwiseobjectivegivesthehighestAUQCinonlyaminorityofcases
(7/30),whilethelistwiseobjectiveobtainsbestinclassperformanceinamajorityoftime(16/30)andthepairwise
objectiveperformssimilartothepointwise(7/30). Alistwiseapproachoutperformsapointwiseoneinamajorityof
cases(20/30). OnlyforthePromotiondata,thepointwiseobjectiveperformsrelativelywell.
Interestingly,weobservedifferencesacrossmetalearnersintermsofwhichobjectivegivesthebestresults. Thelistwise
objectiveseemsfavorableforsomemetalearners(Z-,X-,andR-Learner),whilethepairwiseobjectiveseemspreferable
fortheS-LearnerandthepointwiseobjectiveseemsbestfortheDR-Learner. Generally,wealsoobservethatthechoice
ofmetalearnerisatleastequallyimportantasthechoiceofobjective. Thisfindingstressestheimportanceoftesting
differentmetalearners–illustratingthevalueofourcontribution.
Whenlearningamodelusingrankingobjectives, therankingscoresarenotproperlycalibrated. Thisrepresentsa
possiblechallengeformetalearnersthatdonotdirectlylearnthetreatmenteffect,i.e.,theS-andT-Learner. Forthese
metalearners,aninstance’spredictedrankingscoredoesnotnecessarilycorrespondtoitspotentialoutcome. Rather,if
oneinstance’soutcomeislargerthananotherinstance’s(y >y ),itspredictedscorewillbelarger(yˆ >yˆ ). Although
i j i j
therankingmightholdforbothpotentialoutcomes,therearenoguaranteesfortherankingsoftheestimatedtreatment
effectsderivedfromtheserankingscores(τ andτ ). Becauserankingscoresarenotcalibrated,arithmeticoperations
i j
ofthescores(asusedbythesetwometalearners)maynotbemeaningful. Nevertheless,therankingversionsofthe
S-andT-Learnerperformrelativelywellinpractice,illustratingthattheymayprovideameaningfulheuristic. While
calibrationmethodsforrankingmodelsexists,weleavethisextensionforfuturework(seeconclusion).
10
τlladneK
CQUA CQUA
CQUA
CQUA CQUA CQUA
CQUA
CQUAMetalearnersforRankingTreatmentEffects APREPRINT
5.3 AnalyzingAlternativeMetrics(RQ2)andDesignChoices(RQ3)
Thissectionaimstoprovideadeeperunderstandingofourproposedapproach. Tothisend,wehighlightadditional
results using the Synthetic data, which allows for a more comprehensive analysis as we know the ground truth
treatmenteffectsforthetestinstances.
Toanswer(RQ2)regardingperformancetrade-offsofthedifferentobjectivesandmetalearners,wecanpresentadditional
metricstoallowforamoreholisticevaluation(seeFigure3andAppendixB.2). Ourmainmetricofinterestremains
theAUQC:alistwisemetricofrankingquality. Additionally,wepresenttwoothermetrics: apointwiseerrormetric
(MSE)andapairwiserankcorrelationcoefficient(Kendallτ). First,weobservethatrankingobjectivesgivefarworse
performanceintermsofMSE(AppendixB.2). Inotherwords,therankingscoredoesnotaccuratelyreflectthesizeof
thetreatmenteffect. Inrelationtothisfinding,Figure3showsthattheMSEisnotagoodpredictorofperformance
intermsofAUQC.Conversely,Kendallτ isagoodpredictorforAUQCforallmodels(ρ>0),particularlyforthe
rankingmodels(ρ>0.9). Thisfindingunderscorestheimportanceofrankingmetricsforevaluatingdecisionquality
andhighlightstheirrelevanceofoptimizingpointwiseerrorfortreatmentprioritization.
Finally,toanswer(RQ3),weanalyzeseveraldesignchoicesofourproposedrankingmetalearnersinFigure4and
AppendixB.3. Weobservethatthedefaultsettingusedintheexperimentsabove(samplingiterationsk =1,sigmoid
σ =1,normalizingrankingscores)generallyperformswellacrossrankingobjectivesandmetalearners. Interestingly,
weobservelittleperformancebenefitswhentrainingwithmoresamplingiterations.Thisfindingshowsthatoursampled
objective can estimate the ranking objective accurately. Only for the S- and T-Learner without normalization, we
observebetterperformanceforahigherk. Additionally,thestochasticityofoursampledobjectivesmayevenprovidea
formofregularization[60].
6 Conclusion
Thisworkaddressedtheproblemofoptimallyprioritizinginstancesfortreatment,animportantproblemformany
applicationswherenotallinstancescanreceiveatreatment. Existingapproachestypicallytacklethisproblemina
prediction-focusedapproachbyfirstobtainingapointwiseeffectestimateforeachinstance’streatmenteffect, and
thenrankinginstancesbasedontheseestimates. Conversely,weexploreanalternative,decision-focusedapproach:
usingobjectivesthatlearntorankthetreatmenteffects,weaimtooptimizethequalityoftheresultingtreatmentpolicy
directly. Buildingontheliteratureonlearningtorank,weproposepairwiseandlistwiserankingobjectivesandshow
thatourproposedlistwiseobjectivedirectlyoptimizesthepolicy’sAUQC.Moreover,weproposedifferentranking
metalearnersbyintegratingtheserankingobjectivesintheconstructionofeachmetalearner. Empiricalresultsshow
thatourproposedeffectrankingapproachcanoutperformapointwise,effectestimationapproach. Inconclusion,our
proposedrankingmetalearnersofferavaluabletoolforapplicationswhereinstancesneedtobeprioritizedfortreatment.
Ourworkopensupseveralexcitingdirectionsforfuturework. First,bybuildinguponadvancesinlearningtorank,
suchasmoreadvancedlistwiseobjectives[e.g.,50]orcalibrationofrankingscores[e.g.,61,62]. Alternatively,we
envisionextensionsofourapproachtomorecomplicatedsettingswith,e.g.,moreadvancedoperationalconstraints[63],
multipleorcontinuoustreatments,ormorecomplexobjectives(e.g.,incrementalreturnoninvestment). Additionally,
itwouldbeinsightfultoanalyzehowourproposedrankingmetalearnersperformwhenlearningfromconfounded
observationaldatawithnon-randomtreatmentassignments. Finally,whiletheoreticalresultsregardingconvergenceor
errorboundsareoutofthescopeofthiswork,webelievethatextendingtheresultsobtainedfortheeffectestimation
models[e.g.,56,55]toeffectrankingmodelsisafruitfulareaoffuturework.
References
[1] JavierAlbertandDmitriGoldenberg.E-commercepromotionspersonalizationviaonlinemultiple-choiceknapsack
withupliftmodeling. InProceedingsofthe31stACMInternationalConferenceonInformation&Knowledge
Management,pages2863–2872,2022.
[2] AlexSavachkinandAndrésUribe. Dynamicredistributionofmitigationresourcesduringinfluenzapandemics.
Socio-EconomicPlanningSciences,46(1):33–45,2012.
[3] ToonVanderschueren,RobertBoute,TimVerdonck,BartBaesens,andWouterVerbeke.Optimizingthepreventive
maintenancefrequencywithcausalmachinelearning.InternationalJournalofProductionEconomics,258:108798,
2023.
[4] DebopamBhattacharyaandPascalineDupas. Inferringwelfaremaximizingtreatmentassignmentunderbudget
constraints. JournalofEconometrics,167(1):168–196,2012.
11MetalearnersforRankingTreatmentEffects APREPRINT
[5] EustacheDiemert,ArtemBetlei,ChristopheRenaudin,andMassih-RezaAmini. Alargescalebenchmarkfor
upliftmodeling. InKDD,2018.
[6] DmitriGoldenberg,JavierAlbert,LucasBernardi,andPabloEstevez. Freelunch! retrospectiveupliftmodeling
fordynamicpromotionsrecommendationwithinroiconstraints. InProceedingsofthe14thACMConferenceon
RecommenderSystems,pages486–491,2020.
[7] VasilisSyrgkanis,GregLewis,MirunaOprescu,MaggieHei,KeithBattocchi,EleanorDillon,JingPan,Yifeng
Wu,PaulLo,HuigangChen,etal. Causalinferenceandmachinelearninginpracticewitheconmlandcausalml:
Industrialusecasesatmicrosoft, tripadvisor, uber. In Proceedingsofthe27thACMSIGKDDconferenceon
knowledgediscovery&datamining,pages4072–4073,2021.
[8] JayantaMandi,JamesKotary,SenneBerden,MaximeMulamba,VictorBucarey,TiasGuns,andFerdinando
Fioretto. Decision-focusedlearning: Foundations,stateoftheart,benchmarkandfutureopportunities. arXiv
preprintarXiv:2307.13565,2023.
[9] ArtemBetlei,EustacheDiemert,andMassih-RezaAmini. Upliftmodelingwithgeneralizationguarantees. In
Proceedingsofthe27thACMSIGKDDConferenceonKnowledgeDiscovery&DataMining,pages55–65,2021.
[10] WeijiaZhang,JiuyongLi,andLinLiu. Aunifiedsurveyoftreatmenteffectheterogeneitymodellinganduplift
modelling. ACMComputingSurveys(CSUR),54(8):1–36,2021.
[11] FlorisDevriendt,DarieMoldovan,andWouterVerbeke. Aliteraturesurveyandexperimentalevaluationofthe
state-of-the-artinupliftmodeling: Asteppingstonetowardthedevelopmentofprescriptiveanalytics. Bigdata,
6(1):13–41,2018.
[12] DiegoOlaya,KristofCoussement,andWouterVerbeke. Asurveyandbenchmarkingstudyofmultitreatment
upliftmodeling. DataMiningandKnowledgeDiscovery,34:273–308,2020.
[13] ArtemBetlei,EustacheDiemert,andMassih-RezaAmini. Upliftpredictionwithdependentfeaturerepresentation
inimbalancedtreatmentandcontrolconditions. InNeuralInformationProcessing: 25thInternationalConference,
ICONIP2018,SiemReap,Cambodia,December13–16,2018,Proceedings,PartV25,pages47–57.Springer,
2018.
[14] Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous
treatmenteffectsusingmachinelearning. Proceedingsofthenationalacademyofsciences,116(10):4156–4165,
2019.
[15] AliciaCurthandMihaelavanderSchaar. Nonparametricestimationofheterogeneoustreatmenteffects: From
theorytolearningalgorithms. InInternationalConferenceonArtificialIntelligenceandStatistics,pages1810–
1818.PMLR,2021.
[16] KathleenKane,VictorSYLo,andJaneZheng. Miningforthetrulyresponsivecustomersandprospectsusing
true-liftmodeling: Comparisonofnewandexistingmethods. JournalofMarketingAnalytics,2:218–238,2014.
[17] PiotrRzepakowskiandSzymonJaroszewicz.Decisiontreesforupliftmodelingwithsingleandmultipletreatments.
KnowledgeandInformationSystems,32:303–327,2012.
[18] MichałSołtys,SzymonJaroszewicz,andPiotrRzepakowski. Ensemblemethodsforupliftmodeling. Datamining
andknowledgediscovery,29:1531–1559,2015.
[19] LeoGuelman,MontserratGuillén,andAnaMPérez-Marín. Upliftrandomforests. CyberneticsandSystems,
46(3-4):230–248,2015.
[20] WouterVerbeke,DiegoOlaya,JeroenBerrevoets,SamVerboven,andSebastiánMaldonado. Thefoundationsof
cost-sensitivecausalclassification. arXivpreprintarXiv:2007.12582,2020.
[21] FlorisDevriendt,JeroenBerrevoets,andWouterVerbeke. Whyyoushouldstoppredictingcustomerchurnand
startusingupliftmodels. InformationSciences,548:497–515,2021.
[22] RobinMGubelaandStefanLessmann. Upliftmodelingwithvalue-drivenevaluationmetrics. Decisionsupport
systems,150:113648,2021.
[23] Wouter Verbeke, Diego Olaya, Marie-Anne Guerry, and Jente Van Belle. To do or not to do? cost-sensitive
causal classification with individual treatment effect estimates. European Journal of Operational Research,
305(2):838–852,2023.
[24] HugoManuelProençaandFelipeMoraes. Incrementalprofitperconversion: aresponsetransformationforuplift
modelingine-commercepromotions. arXivpreprintarXiv:2306.13759,2023.
[25] KuiZhao,JunhaoHua,LingYan,QiZhang,HuanXu,andChengYang. Aunifiedframeworkformarketing
budgetallocation. InProceedingsofthe25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery
&DataMining,pages1820–1830,2019.
12MetalearnersforRankingTreatmentEffects APREPRINT
[26] YeTu,KinjalBasu,CyrusDiCiccio,RomilBansal,PreetamNandy,PadminiJaikumar,andShaunakChatterjee.
Personalizedtreatmentselectionusingcausalheterogeneity. InProceedingsoftheWebConference2021,pages
1574–1585,2021.
[27] MengAi,BiaoLi,HeyangGong,QingweiYu,ShengjieXue,YuanZhang,YunzhouZhang,andPengJiang. Lbcf:
Alarge-scalebudget-constrainedcausalforestalgorithm. InProceedingsoftheACMWebConference2022,pages
2310–2319,2022.
[28] CarlosFernández-LoríaandJorgeLoría. Learningtherankingofcausaleffectswithconfoundeddata. arXiv
preprintarXiv:2206.12532,2022.
[29] SusanAthey,NiallKeleher,andJannSpiess. Machinelearningwhotonudge: causalvspredictivetargetingina
fieldexperimentonstudentfinancialaidrenewal. arXivpreprintarXiv:2310.08672,2023.
[30] ZheCao,TaoQin,Tie-YanLiu,Ming-FengTsai,andHangLi. Learningtorank: frompairwiseapproachto
listwiseapproach. InProceedingsofthe24thinternationalconferenceonMachinelearning,pages129–136,
2007.
[31] Tie-YanLiuetal. Learningtorankforinformationretrieval. FoundationsandTrends®inInformationRetrieval,
3(3):225–331,2009.
[32] ShuyangDu,JamesLee,andFarzinGhaffarizadeh. Improveuserretentionwithcausallearning. InThe2019
ACMSIGKDDWorkshoponCausalDiscovery,pages34–49.PMLR,2019.
[33] Will Y Zou, Shuyang Du, James Lee, and Jan Pedersen. Heterogeneous causal learning for effectiveness
optimizationinusermarketing. arXivpreprintarXiv:2004.09702,2020.
[34] HaoZhou,ShaomingLi,GuibinJiang,JiaqiZheng,andDongWang. Directheterogeneouscausallearningfor
resourceallocationproblemsinmarketing. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume37,pages5446–5454,2023.
[35] ZiangYan,ShusenWang,GuoruiZhou,JingjianLin,andPengJiang. Anend-to-endframeworkformarketing
effectivenessoptimizationunderbudgetconstraint. arXivpreprintarXiv:2302.04477,2023.
[36] FlorisDevriendt,JenteVanBelle,TiasGuns,andWouterVerbeke. Learningtorankforupliftmodeling. IEEE
TransactionsonKnowledgeandDataEngineering,34(10):4888–4904,2020.
[37] MiroslavDudík,JohnLangford,andLihongLi. Doublyrobustpolicyevaluationandlearning. InProceedingsof
the28thInternationalConferenceonInternationalConferenceonMachineLearning,pages1097–1104,2011.
[38] BaqunZhang,AnastasiosATsiatis,MarieDavidian,MinZhang,andEricLaber. Estimatingoptimaltreatment
regimesfromaclassificationperspective. Stat,1(1):103–114,2012.
[39] EricBLaberandYing-QiZhao.Tree-basedmethodsforindividualizedtreatmentregimes.Biometrika,102(3):501–
514,2015.
[40] Alexander R Luedtke and Mark J van der Laan. Super-learning of an optimal dynamic treatment rule. The
internationaljournalofbiostatistics,12(1):305–332,2016.
[41] Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization methods for
treatmentchoice. Econometrica,86(2):591–616,2018.
[42] SusanAtheyandStefanWager. Policylearningwithobservationaldata. Econometrica,89(1):133–161,2021.
[43] DonaldBRubin. Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomizedstudies. Journalof
educationalPsychology,66(5):688,1974.
[44] PierreGutierrezandJean-YvesGérardy. Causalinferenceandupliftmodelling: Areviewoftheliterature. In
InternationalconferenceonpredictiveapplicationsandAPIs,pages1–13.PMLR,2017.
[45] ErikSverdrup,HanWu,SusanAthey,andStefanWager. Qinicurvesformulti-armedtreatmentrules. arXiv
preprintarXiv:2306.11979,2023.
[46] ChrisBurges,TalShaked,ErinRenshaw,AriLazier,MattDeeds,NicoleHamilton,andGregHullender. Learning
torankusinggradientdescent. InProceedingsofthe22ndinternationalconferenceonMachinelearning,pages
89–96,2005.
[47] ChristopherJCBurges. Fromranknettolambdaranktolambdamart: Anoverview. Learning,11(23-581):81,
2010.
[48] YiningWang,LiweiWang,YuanzhiLi,DiHe,andTie-YanLiu. Atheoreticalanalysisofndcgtyperanking
measures. InConferenceonlearningtheory,pages25–54.PMLR,2013.
13MetalearnersforRankingTreatmentEffects APREPRINT
[49] PinarDonmez,KrystaMSvore,andChristopherJCBurges.Onthelocaloptimalityoflambdarank.InProceedings
ofthe32ndinternationalACMSIGIRconferenceonResearchanddevelopmentininformationretrieval,pages
460–467,2009.
[50] IvanLyzhin,AlekseiUstimenko,AndreyGulin,andLiudmilaProkhorenkova. Whichtricksareimportantfor
learningtorank? InInternationalConferenceonMachineLearning,pages23264–23278.PMLR,2023.
[51] PaulWHolland. Statisticsandcausalinference. JournaloftheAmericanstatisticalAssociation,81(396):945–960,
1986.
[52] MaciejJaskowskiandSzymonJaroszewicz. Upliftmodelingforclinicaltrialdata. InICMLworkshoponclinical
dataanalysis,volume46,pages79–95,2012.
[53] KrzysztofRudas´andSzymonJaroszewicz. Linearregressionforupliftmodeling. DataMiningandKnowledge
Discovery,32:1275–1305,2018.
[54] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference models.
Biometrics,61(4):962–973,2005.
[55] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic
JournalofStatistics,17(2):3008–3049,2023.
[56] XinkunNieandStefanWager.Quasi-oracleestimationofheterogeneoustreatmenteffects.Biometrika,108(2):299–
319,2021.
[57] PeterMRobinson. Root-n-consistentsemiparametricregression. Econometrica: JournaloftheEconometric
Society,pages931–954,1988.
[58] KevinHillstrom. TheMineThatDataE-MailAnalyticsAndDataMiningChallenge—blog.minethatdata.com.
https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html,
2008. [Accessed15-04-2024].
[59] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
Lightgbm: Ahighlyefficientgradientboostingdecisiontree. Advancesinneuralinformationprocessingsystems,
30,2017.
[60] JeromeHFriedman. Stochasticgradientboosting. Computationalstatistics&dataanalysis,38(4):367–378,
2002.
[61] David Sculley. Combined regression and ranking. In Proceedings of the 16th ACM SIGKDD international
conferenceonKnowledgediscoveryanddatamining,pages979–988,2010.
[62] Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. Scale calibration of deep ranking
models. InProceedingsofthe28thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining,pages
4300–4309,2022.
[63] ToonVanderschueren,BartBaesens,TimVerdonck,andWouterVerbeke. Anewperspectiveonclassification:
optimallyallocatinglimitedresourcestouncertaintasks. DecisionSupportSystems,179:114151,2024.
[64] DanielGHorvitzandDonovanJThompson. Ageneralizationofsamplingwithoutreplacementfromafinite
universe. JournaloftheAmericanstatisticalAssociation,47(260):663–685,1952.
[65] Miguel A Hernán and James M Robins. Estimating causal effects from epidemiological data. Journal of
Epidemiology&CommunityHealth,60(7):578–586,2006.
14MetalearnersforRankingTreatmentEffects APREPRINT
Synthetic Criteo
AUQC AUQC
0.25 0.06
0.20 0.04
0.15 0.02
0.10
0.00
0.05
0.02
0.00 −
Z S T X DR R Z S T X DR R
Synthetic
Hillstrom (FeAmUaQleC) Hillstrom (Male)
AUQC 0.3 AUQC
0.10 0.05
0.05 0.2 Pointwise
0.00 0.00 Pairwise
0.05 Listwise
− 0.1
−0.10 −0.05
0.15
− Z S T X DR R Z S T X DR R
0.0
Z S T X DR R
Figure5: RankingQualityforDifferentObjectivesandMetalearners. Foreachmetalearner,wecomparethreedifferent
objectives: point-,pair-,andlistwise. WeshowperformanceintermsofAUQC onestandarderror,forfivedifferent
±
datasets. Asopposedtothefigureinthemainbody,wedonotscaletheresultshere. Duetoconfidentialityreasons,we
cannotsharetherawresultsforthePromotiondataset.
A Problemformulation: IdentifiabilityAssumptions
Asintroducedinthemainbody,werequirethestandardassumptionsfromcausalinferencetoidentifythecausaleffect.
Inthiswork,weassumedthathistoricaldatacomesfromarandomizedcontrolledtrial:
Assumption2(Consistency). WhenY =yandT =t,weassumethatY(T =t)=y. Thisimplicatesthat,foreach
instance,whengiventreatmentt,theoutcomeweobserveisthepotentialoutcomeassociatedtothattreatmentY(t).
Assumption3(Nointerference). Aninstance’soutcomegivenatreatmentisindependentoftreatmentsadministeredto
otherinstances: Y (t ,...,t ,...,t )=Y (t ).
i 0 i n i i
Assumption4(Unconfoundedness). WeassumeY(T) T,i.e.,pasttreatmentdecisionsweremadeatrandom,i.e.,
⊥⊥
notbasedontheinstance’scharacteristics.
If we do not have data from a randomized trial, we require a stronger assumption called strong ignorability or no
hiddenconfounding: Y(T) T x,i.e.,pasttreatmentdecisionswereexclusivelybasedontheinstance’sobserved
⊥⊥ |
characteristics x. In this case, we also require positivity: for each instance, the probability of administering each
treatmenthastobelargerthanzero,i.e.,P(T x) > 0. Wedonotconsiderthisscenarioinourwork. However,our
|
rankingmetalearnerscaneasilybeextendedtothesescenarios:whilesomemetalearnersalreadyintegratethepropensity
scoreintheirconstruction,othersmaybeimprovedbyinversepropensityscoreweighting[64,65].
B EmpiricalResults: AdditionalExperiments
Thissectionpresentsadditionalexperimentalresults. WedescribeadditionalresultsforRQ1inAppendixB.1,andfor
RQ2andRQ3inAppendixB.2whereweanalyzetheeffectofseveraldesignchoicesandhyperparameters.
B.1 ComparingPerformancefortheDifferentObjectivesandMetalearners(RQ1): AdditionalResults
Wedisplaytheresultspresentedinthemainbodyofthetextwithoutnormalizingthebestvalueto1inFigure5and
providethemintableformatinTable4.
B.2 AnalyzingAlternativeMetrics(RQ2)andDesignChoices(RQ3): AdditionalResults
First,wepresentadditionalresultstosupportourinvestigationsurroundingRQ2. Foreachmetalearner,wevisualize
thetrade-offbetweendifferentmetrics: theMSE(i.e.,pointwiseaccuracy),Kendallτ (i.e.,pairwiserankcorrelation),
andAUQC(i.e.,listwiserankingquality). ThisshowsthatmodelsthatperformwellintermsofAUQC,alsoperform
wellintermsofKendallτ. Conversely,performanceintermsofMSEdoesnotseemrelatedtoAUQCorKendallτ.
Finally,wealsodisplaytheseresultsintableformatinTable5.
15MetalearnersforRankingTreatmentEffects APREPRINT
Objective Data
Meta
Point Pair List Synthetic Criteo Hillstrom| Hillstrom| Promotion*
✓ ✗ ✗ +0.183(0.027) +0.038(0.018) +0.057(0.030♀ ) +0.013(0.03♂ 3) +0.407(0.111)
Z ✗ ✓ ✗ +0.137(0.016) 0.014(0.010) +0.016(0.039) +0.022(0.044) +0.370(0.296)
✗ ✗ ✓ +0.242(0.026) − +0.048(0.009) +0.092(0.018) +0.027(0.033) +0.593(0.037)
✓ ✗ ✗ +0.051(0.050) +0.030(0.020) +0.011(0.048) +0.025(0.041) +0.370(0.074)
S ✗ ✓ ✗ +0.052(0.039) +0.002(0.008) +0.042(0.014) 0.021(0.039) +0.593(0.296)
✗ ✗ ✓ +0.037(0.027) +0.048(0.016) +0.039(0.054) − +0.021(0.035) +0.222(0.148)
✓ ✗ ✗ +0.054(0.049) +0.043(0.021) 0.004(0.036) +0.015(0.009) +0.333(0.148)
T ✗ ✓ ✗ +0.046(0.044) 0.015(0.010) − +0.044(0.014) 0.017(0.045) +0.630(0.185)
✗ ✗ ✓ +0.058(0.043) − +0.039(0.013) +0.021(0.037) − +0.032(0.028) +0.556(0.222)
✓ ✗ ✗ +0.052(0.049) +0.034(0.017) +-0.000(0.031) 0.028(0.039) +1.000(0.296)
X ✗ ✓ ✗ +0.054(0.042) +0.002(0.009) +0.028(0.039) +− 0.029(0.034) +0.704(0.259)
✗ ✗ ✓ +0.061(0.038) +0.036(0.012) +0.033(0.051) +0.018(0.033) +0.852(0.111)
✓ ✗ ✗ +0.055(0.046) +0.044(0.007) 0.101(0.044) +0.043(0.025) +0.667(0.111)
DR ✗ ✓ ✗ +0.048(0.047) +0.022(0.012) − +0.027(0.019) +0.006(0.054) 0.481(0.222)
✗ ✗ ✓ +0.056(0.038) +0.039(0.014) +0.005(0.055) +0.039(0.033) − 0.148(0.222)
−
✓ ✗ ✗ +0.054(0.046) +0.029(0.018) 0.034(0.042) +0.033(0.028) +0.593(0.222)
R ✗ ✓ ✗ +0.050(0.047) +0.021(0.022) − +0.036(0.013) +0.012(0.043) 0.296(0.111)
✗ ✗ ✓ +0.056(0.039) +0.038(0.014) +0.048(0.048) +0.034(0.031) − 0.259(0.259)
−
*ForthePromotiondata,weonlypresentscaledresultssuchthatthebestAUQC=1duetoreasonsofconfidentiality.
Table4: RankingQualityforDifferentObjectivesandMetalearners. Foreachmetalearner,wecomparethreedifferent
objectives: point-,pair-,andlistwise. WeshowperformanceintermsofAUQC(withstandarderrorinbrackets),forthe
Synthetic,Criteo,andHillstrom(Women( )andMen( )e-mail),andPromotiondata.
♀ ♂
Kendallτ Kendallτ Kendallτ
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
000..00015030045M0SE 000..00015030045M0SE 000..00015030045M0SE
0.1 0.1 0.1
0.2 0.2 0.2
0.3 0.3 0.3
AUQC AUQC AUQC
(a)Z-Learner (b)S-Learner (c)T-Learner
Kendallτ Kendallτ Kendallτ
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
000..00015030045M0SE 000..00015030045M0SE 000..00015030045M0SE
0.1 0.1 0.1
0.2 0.2 0.2
0.3 0.3 0.3
AUQC AUQC AUQC
(d)X-Learner (e)DR-Learner (f)R-Learner
Figure6: AnalyzingPerformanceTrade-offsonSyntheticData. Foreachmetalearner, wecomparethreedifferent
objectives: pointwise,pairwise,andlistwise. UsingtheSyntheticdataset,wecompareperformanceintermsof
MSE(measuringpointwiseaccuracy),Kendallτ (measuringpairwiserankcorrelation),andAUQC(measuringglobal,
listwisedecisionquality).
16MetalearnersforRankingTreatmentEffects APREPRINT
Objective Metric
Meta
Point Pair List MSE Kendallτ AUQC
✓ ✗ ✗ +2.474(0.255) +0.128(0.021) +0.183(0.027)
Z ✗ ✓ ✗ +2.867(0.263) +0.101(0.013) +0.137(0.016)
✗ ✗ ✓ +6.465(1.260) +0.164(0.020) +0.242(0.026)
✓ ✗ ✗ +37.916(3.557) +0.044(0.033) +0.051(0.050)
S ✗ ✓ ✗ +483.712(159.054) +0.050(0.028) +0.052(0.039)
✗ ✗ ✓ +562.509(300.946) +0.038(0.022) +0.037(0.027)
✓ ✗ ✗ +41.014(3.852) +0.045(0.033) +0.054(0.049)
T ✗ ✓ ✗ +213.265(30.716) +0.049(0.030) +0.046(0.044)
✗ ✗ ✓ +513.908(94.592) +0.057(0.030) +0.058(0.043)
✓ ✗ ✗ +40.417(3.697) +0.043(0.033) +0.052(0.049)
X ✗ ✓ ✗ +88.987(16.651) +0.054(0.030) +0.054(0.042)
✗ ✗ ✓ +162.146(40.750) +0.058(0.028) +0.061(0.038)
✓ ✗ ✗ +42.007(4.017) +0.043(0.031) +0.055(0.046)
DR ✗ ✓ ✗ +61.014(10.268) +0.049(0.032) +0.048(0.047)
✗ ✗ ✓ +311.236(105.607) +0.055(0.028) +0.056(0.038)
✓ ✗ ✗ +41.263(4.760) +0.042(0.031) +0.054(0.046)
R ✗ ✓ ✗ +51.008(12.094) +0.051(0.032) +0.050(0.047)
✗ ✗ ✓ +124.400(18.588) +0.055(0.028) +0.056(0.039)
Table 5: Analyzing Performance Trade-offs on Synthetic Data. For each metalearner, we compare three different
objectives: pointwise,pairwise,andlistwise. UsingtheSyntheticdataset,wecompareperformanceintermsof
MSE(measuringpointwiseaccuracy),Kendallτ (measuringpairwiserankcorrelation),andAUQC(measuringglobal,
listwisedecisionquality). Foreach,weshowthestandarderrorinbrackets.
B.3 SensitivityAnalysis
Tosingleouttheeffectoftheanalyzeddesignchoice,wetrainwithdefaulthyperparametersforeachtrainingobjective
and metalearner. We explore three hyperparameters: (1) the number of sampling iterations k, i.e., the number of
sampledpairsperinstance,(2)thesigmoidparameterσcontrollingthesteepnessofthecomparisonintheconstruction
ofthepairwisescore(seeEquation(3)),and(3)whetherwenormalizetherankingscorebyfeedingittoalogistic
sigmoidandconstrainingitto[0,1]. Relatedworkhasshownthatthisnormalizationmighteffectivelyserveasaform
ofregularizationandhelpwithoverfitting[32].
InFigure7,wevarythenumberofsampledpairskintherankingobjectives,foreachrankingmetalearner,andcompare
ittothepointwisemodelontheSyntheticdata. Wealsoshowresultsforthemetalearnerswithnormalizationofthe
score(i.e.,constrainingthemodeltooutputsbetweenzeroandone)andwithoutnormalization. Somewhatsurprisingly,
withnormalization,weseethatincreasingkdoesnotyieldbetterresults. Conversely,withoutnormalization,increasing
kdoesinfactimproveperformanceformostmetalearners. Nevertheless,fortheZ-,X-,DR-,andR-Learner,thebest
performanceisachievedwithnormalizationandk =1–thesamesettingsusedintheexperimentsinthemainbody(i.e.,
Table4). FortheS-andT-Learner,deviatingfromthesesettingsmightimproveperformance. Overall,weseethatfor
eachmetalearnerandobjective,wecanobtainthesameperformanceorbetterthanthepointwiseequivalentgiventhat
therighthyperparametersarechosen. Thisinsightprovidesanothervalidationofourproposedapproach.
InFigure8,wevarythesigmoidparametereσ,controllingthesteepnessofthecomparisonoftheinstancescoresin
theconstructionofthepairwisescore(seeEquation(3)). Generally,weobtaingoodperformanceforsmallervalues
(σ 1). Whenusingnormalization,ourmethodseemsmoresensitivetothishyperparametercomparedtotraining
≤
withoutscorenormalization. Althoughtheremaybesomebenefitoftuningthishyperparameter,weobservethatfixing
thesigmoidparameteratσ =1seemslikeagoodchoiceoverall–thiswasthesettingusedtogeneratetheexperimental
resultsinthemainbody.
17MetalearnersforRankingTreatmentEffects APREPRINT
Pointwise Pairwise Pairwise(nonormalization) Listwise Listwise(nonormalization)
0.20
0.06 0.06
0.15
0.04 0.04
0.10
0.02 0.02
0.05
k k k
0.00 0.00 0.00
100 101 102 100 101 102 100 101 102
(a)Z-Learner (b)S-Learner (c)T-Learner
0.06 0.06 0.06
0.04 0.04 0.04
0.02 0.02 0.02
k k k
0.00 0.00 0.00
100 101 102 100 101 102 100 101 102
(d)X-Learner (e)DR-Learner (f)R-Learner
Figure7: WhatIstheEffectoftheNumberofSamplingIterationsk? WeshowperformanceintermsofAUQC(higher
isbetter)forthedifferentmetalearnersontheSyntheticdataset. Wefixthesigmoidparameterσ =1andtrainwith
defaulthyperparameters.
18
CQUA
CQUA
CQUA
CQUA
CQUA
CQUAMetalearnersforRankingTreatmentEffects APREPRINT
Pointwise Pairwise Pairwise(nonormalization) Listwise Listwise(nonormalization)
0.20
0.06 0.06
0.15
0.04 0.04
0.10
0.02 0.02
0.05
σ σ σ
0.00 0.00 0.00
10−2 10−1 100 101 102 10−2 10−1 100 101 102 10−2 10−1 100 101 102
(a)Z-Learner (b)S-Learner (c)T-Learner
0.06 0.06 0.06
0.04 0.04 0.04
0.02 0.02 0.02
σ σ σ
0.00 0.00 0.00
10−2 10−1 100 101 102 10−2 10−1 100 101 102 10−2 10−1 100 101 102
(d)X-Learner (e)DR-Learner (f)R-Learner
Figure8: WhatIstheEffectoftheSigmoidParameterσ? WeshowperformanceintermsofAUQC(higherisbetter)
forthedifferentmetalearnersontheSyntheticdataset. Wefixthenumberofsamplingiterationsk = 1andtrain
withdefaulthyperparameters.
19
CQUA
CQUA
CQUA
CQUA
CQUA
CQUA