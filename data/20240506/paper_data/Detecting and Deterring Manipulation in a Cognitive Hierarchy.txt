DETECTING AND DETERRING MANIPULATION IN A COGNITIVE
HIERARCHY
APREPRINT
NitayAlon1,2,LionSchulz1,JosephM.Barnby4,JeffreyS.Rosenschein2,andPeterDayan1,3
1DepartmentofComputationalNeuroscience,MaxPlanckInstituteforBiologicalCybernetics,Tu¨ebingen,Germany
2DepartmentofComputerScience,TheHebrewUniversityofJerusalem,Jerusalem,Israel
3DepartmentofComputerScience,UniversityofTuebingen,Tuebingen,Germany
4DepartmentofPsychology,RoyalHollowayUniversityofLondon,London,UK
May6,2024
ABSTRACT
Social agents with finitely nested opponent models are vulnerable to manipulation by agents with
deeperreasoningandmoresophisticatedopponentmodelling. Thisimbalance,rootedinlogicand
the theory of recursive modelling frameworks, cannot be solved directly. We propose a compu-
tational framework, ℵ-IPOMDP, augmenting model-based RL agents’ Bayesian inference with an
anomaly detection algorithm and an out-of-belief policy. Our mechanism allows agents to realize
they are being deceived, even if they cannot understand how, and to deter opponents via a credi-
ble threat. We test this framework in both a mixed-motive and zero-sum game. Our results show
theℵmechanism’seffectiveness,leadingtomoreequitableoutcomesandlessexploitationbymore
sophisticated agents. We discuss implications for AI safety, cybersecurity, cognitive science, and
psychiatry.
1 Introduction
Deceptionisomnipresentinhumanandanimalcultures. Humansusevariouslevelsofdeception, from“whitelies”
toharmfullies,manipulatingthebeliefsfortheirownbenefit. Tomanipulate,adeceiverneedstoknowwhichactions
create or reinforce false beliefs, and which actions avoid disclosing their true intentions. The capacity of an agent
to place itself in somebody else’s shoes is known as Theory of Mind [ToM; Premack and Woodruff, 1978], a key
ingredienttosuccessfuldeception.
ToMencompassesthecapacitytosimulateothers’actionsandbeliefs,oftenrecursively. Thedepthoftherecursionis
knownasanagent’sdepthofmentalisation[DoM;Barnbyetal.,2023,FrithandFrith,2021].Thisrecursivestructure,
framedusingk-levelhierarchicalToM[Camereretal.,2004],guaranteesthatagentswithlowerDoMareincapableof
makinginferencesabouttheintentionsofthosewithhigherDoM[GmytrasiewiczandDoshi,2005]. Suchanability
wouldsuggestthatagentshadrevokedtheparadoxofself-reference. Thislimitation,foundinallrecursivemodelling
frameworks[PacuitandRoy,2017],impliesthatagentswithlowDoMaredoomedtobemanipulatedbyotherswith
higherDoM.Thisasymmetryhaspreviouslybeenexplored[Doshietal.,2014,Hulaetal.,2018,Alonetal.,2023a,
Sarkadietal.,2021,2019a],illustratingthevariouswayhighDoMagentscantakeadvantageoflowDoMagents.
However, all is not lost. Low DoM agents may still notice that the behaviour they observe is inconsistent with the
behaviourtheyexpect,eveniftheylackthewherewithaltounderstandhoworwhyHulaetal.[2018]. Suchaheuristic
mismatch warns the victim that they are facing an unmodeled opponent, meaning that they can no longer use their
opponent models for optimal planning. By switching to an out-of-belief (OOB) policy, where actions are sampled
against their beliefs, they can act defensively, and even deter more competent opponents from trying to manipulate
them. Forexample,anagentmightquitagameagainsttheirownbestintereststoavoidwhattheyperceiveas“being
takenadvantageof”,knowingthatthiswillalsoharmthedeceiver.
4202
yaM
3
]AM.sc[
1v07810.5042:viXraDetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure 1: Paper overview: (Cognitive Hierarchy:) Agents with finite recursive opponent modeling with varying
DepthofMentalisation(DoM)interactindifferentenvironments. (ℵ-IPOMDP:)Augmentingagents’inferencepro-
cesseswithananomalydetectionmechanismallowsthemtocopewithdeceptiveothers. (IUG:)Agentswithdifferent
degreesofDoMinteractintheiteratedultimatumgame(IUG).(Row/columngame:) Agentsinteractinaniterated
Bayesianzero-sumgame,wheredeceptiontakesplaceaswell.
In this work, we present a computational framework for multi-agent RL (MARL) called ℵ-IPOMDP. We augment
the well-known IPOMDP approach [Gmytrasiewicz and Doshi, 2005] to allow agents to engage with unmodeled
opponents. We first discuss how agents use their ToM to manipulate others, preying on the limited computational
resourcesoftheirvictims. Wethenpresentthemaincontributionofthiswork: adeceptiondetectionmechanism,the
ℵ-mechanism,bywhichagentswithshallowDoMuseaheuristictodetectthattheyarebeingdeceived,andtheOOB
ℵ-policy,aimedatbest-respondingtotheunknownopponent. Weillustratethismechanisminbothmixed-motiveand
zero-sumBayesianrepeatedgamestoshowhowℵ-IPOMDPagentscanmitigatetheadvantagesofagentswithdeeper
recursivemodels,anddetermanipulation.
Ourworkisrelevanttomultiplefields. TotheMARLcommunity,ourworkservestoshowhowagentswithlimited
opponentmodeling(forexampleboundedrationalityagents)canstillcopewithmoreadequateopponentsviaanomaly
detectionandgametheoreticprinciples.Tothecybersecuritycommunity,wepresentaMARLmasqueradingdetection
use-case which can be used to overcome a learning adversarial attacks [Rosenberg et al., 2021]. To the psychology
community,weprovideaheuristicmechanismthatmaybecomemaladaptivelysensitiveandoverestimatedeception,
providing a model of suspicious or conspiratorial thinking. Lastly, there has recently been substantial interest in AI
deception[Sarkadietal.,2019b,2021,Savasetal.,2022]. Ourworkmayserveasablueprintforsystemsthatregulate
andpossiblypreventAIfromdeceivingotherAIorhumans.
2 Previouswork
TherecursivestructureofToMimpliesthatitisimpossibletomakeinferencesaboutagentswithDoMlevelsthatare
higherthanone’sown. Thisisbecausemodellingamoresophisticatedagentwouldrequireonetomodeloneself—
outsideoftheDoMhierarchy—andthus,atleastinprinciple,toviolatekeylogicalprinciples[PacuitandRoy,2017].
Such a restriction is not unique to ToM-based models but is evident in general bounded rationality environments
[NachbarandZame,1996].
Methods of detecting masquerading using information theory have previously been used as a method for inferring
deceptioninthecontextofintrusiondetection. Forexample, Evansetal.[2007]usedtheMDLCompressalgorithm
toidentifyintrudersdisguisedaslegitimateusers. Maguireetal.[2019]suggestthathumansapplyatypicalset-like
mechanismtoidentifyanon-randompattern.However,thisisveryspecifictorandombehaviour,whileinthisworkwe
explorevariousdeviationsfromexpectedbehaviour. BehaviouralbasedIntrusionDetectionSystems(IDS)methods
wereproposedbyPannellandAshman[2010],Pengetal.[2016]. Inthesesystems,thesystemadministratormonitors
2DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
thebehaviourofuserstodecideandrespondtomalevolentbehaviour. However, unlikeourproposedmethod, these
systemsoftenrequirelabeleddatamakingthemsusceptibletoanawareadversarywhoknowshowtoavoiddetection.
InthecontextofPOMDP,severalgoalrecognitionmethodshavebeenproposed,forexample: RamırezandGeffner,
LeGuillarmeetal.[2016]. Whilethesemethodsassumethattheobservedagentmaybeill-intenttheybothassume
that (a) the observer can invert the actions to infer about the goals, and (b) they both utilize the observed behaviour
likelihoodforintentrecognition. Inthisworkweshowhowlikelihoodbasedinferenceisusedagainst theinferring
agentaswellasproposingamechanismthatdetectsdeviationfromexpectedbehaviour,flaggingoutmalevolentagents
withouttheneedtomakeinferenceabouttheirgoals.
Furthermore, Yu et al. [2021] explore how to adapt to higher DoM agents. They suggest that an agent can learn a
best response against each k level opponent from experience. However, it lacks the mechanism to detect when the
opponentDoMlevelexceedstheagent’sDoMlevelandhencetheagentshouldretrainitsmodel.
3 Model
Inthiswork,weproposeageneralframework,theℵ-IPOMDPthataugmentstheconventionalIPOMDPframework
(whichinvolvesrecursivereasoninginanintentionalPOMDP)withamechanismfordetectinganomalousbehaviour
andanout-of-beliefpolicythatservesasadeterrent. Wefirstreviewthesenewcomponentsandthenillustratetheir
effectivenessindeflectingdeceptioninrepeatedBayesiangames.
Repeated Bayesian games model a series of interactions between agents with partial information. Agents may be
uninformedabouttheirstate(e.g.,thecardstheyareholding),thestatesofothers(theircards)andsocialorientation
(friendorfoe),andsoon. Agentsaddressthisuncertaintybyformingbeliefsaboutsuchunknownquantities,which
theyarethenassumedtoupdateinaBayesianmannerfromobservations. Eachagentinthesegamesischaracterized
byitstype,(θ ∈Θ)[Harsanyi,1968]. Thetypedescribesalltheparametersgoverningtheagent’sdecision-making—
its utility (u(·)) and beliefs (b(·)), where these beliefs may include beliefs about the types (including the beliefs) of
otheragents. Weindextheinferringagentbyαandtheiropponentbyβ.
Agents interact with each other by acting (at) and observing others’ actions (at). The history, ht =
α β
{a0,a0,··· ,at,at}, is a vector of actions that are used to make inferences about the opponent’s type and their
α β α β
stateifthatisalsounobserved(which,forconvenience,weavoidhere):
bt+1(θ )=p(θ |ht)∝P(ht|θ )p(θ ) (1)
α β β β β
Inthisworkwedoawaywithenvironmentaluncertaintyorincludeitaspartoftheopponent’stypeforreductionof
complexity.
We consider games in which agents form finitely recursive beliefs about others (known as cognitive hierarchy). At
the basic level agents only maximize their utility based on whatever information they have at hand. These agents,
known as subintentional or DoM(−1) agents do not model their opponent at all, treating their opponent as part of
theenvironment. DoM(0)agentsmakeinferencesaboutthegoals(utility)ofDoM(−1)ones,inaprocesssimilarto
Bayesian-IRL[NgandRussell,2000,RamachandranandAmir,2007]:1.
bt+1(θ )= (2)
β0 α
p(u (·)|ht)∝P(at|u (·),ht−1)p(u (·)|ht−1)
α−1 α α−1 α−1
AgentswithhigherDoMlevel(DoM(k), k ∈ [1...))alsoformbeliefsaboutthebeliefsofothers, inferringothers’
beliefsaboutthem(“whatdoyouthinkaboutme”)andsoon:
bt+1(θ )=p(u (·),bt (θ )|ht) (3)
αk βk−1 βk−1 βk−1 αk−2
wherebt (θ )arethenestedbeliefsofothers. TheDoM(k)agentsusetheirbeliefstocomputetheactionvalue:
βk−1 αk−2
Q(at;u ,bt (θ ))= (4)
α α αk βk−1
E [u (at,at)+max{Q(at+1;u ,bt+1(θ ))}]
at β∼πˆ(θβ) α α β
at α+1
α α αk βk−1
whereπˆ(θ ) = πˆ(u ,ˆb (θ ))isthesimulatedpolicyofatypeθ opponentafterobservingat, repre-
β βk−1 βk−1 αk−2 βk−1 α
sented byˆb (θ ) (which are α’s nested beliefs about β’s uncertainty about α) 2 Agents then select an action
βk−1 αk−2
1Withtheksub-subscriptdenotingtheagent’sDoM
2Formally,E ot∼πˆk−1(θβ)isaniteratedexpectationoverthebeliefs:E ot∼πˆk−1(θβ)E θβ∼bt k(θβ),shortenedforbrevity
3DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
usingaSoftMaxpolicywithknowninversetemperatureT:
P(at;u ,bt (θ ))∝exp1Q(at;u ,bt (θ )) (5)
α α αk βk−1 T α α αk βk−1
Since agents update their beliefs from observation, savvy opponents can shape the agent’s beliefs (and consequent
behaviour) by choosing specific actions [e.g. Alon et al., 2023b,a]. For example, in [Alon et al., 2023b], through
judiciousplanning,buyerswithhigherDoMfakeinterestinanundesirediteminabuyer-sellertask. Thisisintended
todeceiveanobservingsellerintoformingfalsebeliefsaboutthebuyer’spreferences,causingthesellertoofferthe
buyer’sdesireditematalowerprice. Thesedeceptivedynamicstakeadvantageoftwobuilt-infeaturesofrecursive
reasoning. Oneisthedeceiver’sabilitytosimulatethevictim’sreaction(atleastuptoepistemicuncertainty):
P(at|at)=(cid:88) P(at|πˆ(u ,ˆb (θ ))bt−1(θ ) (6)
β α β βk−1 βk−1 αk−2 αk β
θβ
This allows the deceiver to affect the behaviour of a lower DoM victim to its benefit. It is similar to “white-box”
adversarialattacks,whentheattackerpossessestheblueprintsofitsvictim’sdecision-makingEbrahimietal.[2018].
Since the victim is limited to making inferences about lower-level DoM agents, they cannot directly interpret the
deceiver’sactionsasbeingdeceptive. Violatingthisconstraintwouldimplythatthevictimwouldbecapableofself-
reflection, violating logical principles [Pacuit and Roy, 2017]. The combination of the deceiver’s ability to deceive
andthevictim’sinabilitytoresistdeceptionspellsdoomforthelowerDoMvictim.
Thecentralideaofthispaperisthat,despitetheselimitationsintheiropponentmodelling,thevictimmaystillrealize
thattheylackthemeasureoftheiropponent,andthuscanresist. Therealizationstemsfromassessingthe(mis-)match
betweentheexpected(basedonthevictim’slowerDoM)andobservedbehaviouroftheopponent—aformofheuristic
behaviourverification. Forexample,consideraparasite,masqueradingasananttoinfiltrateanantcolonyandsteal
the food. While its appearance may mislead the guardian ants, its behaviour—feasting instead of working—should
trigger an alarm. Thus, even if the victim lacks the cognitive power to resist manipulation, they can detect the ploy
throughaformofconductvalidation.
Ifthevictimdetectssuchadiscrepancy,itcanconcludethattheobservedbehaviourisgeneratedbyanopponentthat
lies outside their world model. In principle, there is a wealth of possibilities for mismatches, from the form of the
DoM(−1)orDoM(0)agenttopriors. Here, though, weassumethattheonlysourceofexternaltypesisthelimited
DoM level. Given the infinite number of DoM levels (bounded below by (−1)), of which only a finite subset lies
withinthek-levelworld-model(Θ ⊂ Θ), wedenotethisexternalsetbyΘ . ForaDoM(k)agent, allthepossible
k ℵ
DoM(k −1) models are in Θ . Thus, a mismatch between the observed and expected behaviour implies that the
k
unknownagenthasDoMleveldifferent than(k−1). Thisisapivotalconcept,allowingtheagenttoengagewitha
known,yetunmodeled,opponent.
Oncethebehaviouralmismatchhasbeenidentified,thevictimhastodecidehowtoact. Defensivecounter-deceptive
behaviourshouldtakeintoaccounttheobservationthatanout-of-modeldeceivermighthavethecapacitytopredict
thevictim.Thus,thevictim’spolicycouldbeaimedathurtingthedeceiver(atacosttothevictim),todeterthemfrom
doingso.
3.1 Detectingandrespondingtodeceptionwithlimitedopponentmodel
Detecting abnormal, and potentially risky, behaviour from observed data is related to Intrusion Detection Systems
(IDS).Thisdomainassumesthat“behaviourisnotsomethingthatcanbeeasilystole[n]”[Salemetal.,2008]. Thus,
anyatypicalbehaviourisflaggedasapotentialintruder,alarmingthesystemthattheobserveduserposesarisktothe
system.
Severalmethodshavebeensuggestedtocombatamasqueradinghacker[Salemetal.,2008]. Inspiredbythesemeth-
ods, we augment the victim’s inference with what we call an ℵ-mechanism. This mechanism, f(Θ ,ht), evaluates
k
the opponent’s behaviour against the expected behaviour, based on the agent’s DoM level and the history. The ex-
pectedbehaviourincludesthepresumedopponent’sresponsetotheagent’sactions,inawaysimilartothesimulated
policyinEquation6. Theℵ-mechanismreturnsabinaryvectorofsize|Θ |asanoutput. Eachentryrepresentsthe
k
ℵ-mechanism’s evaluation per opponent type: θ ∈ Θ . The evaluation either affirms or denies that the observed
β k
behaviour(discussednext)sufficientlymatchestheexpectedbehaviourofeachagenttype.
A critical issue is that, as in Liebald et al. [2007], we allow the deceiver to be aware of the detailed workings of
theℵ-mechanism,andsobeabletoavoiddetectionbyadheringtotheregularitiesunderlyingit. Thisrendersmany
methods impotent for detecting deception. Nonetheless, two key factors of deception make it detectable. First, to
executeitsdeception,thedeceiverwillhavetodeviatefromthetypicalbehaviourofthemasqueradedagentatsome
point. For example, a hacker masquerading as a legitimate user will try at some point to access sensitive data—a
4DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
non-typicalbehaviourforthebenevolentuser. Thisalertsthevictimthattheymightbeengagingwithanunmodeled
agent. Second,innon-cooperativegames,thedeceiver’srewardmaximizationisattheexpenseofthevictim. Since
the victim uses their ToM to compute the expected cumulative reward (from Equation 4), then if the actual reward
deviates(statistically)fromtheexpectedreward,itisanotherindicationthattheopponentisnotintheworldmodelof
thevictim.
3.1.1 ℵ-mechanism
Thebehaviourverificationmechanismassesseswhethertheobservedopponentbelongstothesetofpotentialoppo-
nentsusingtwomainconcepts: δ-strongtypicalsetandexpectedreward. Theδ-strongtypicalsetisanInformation
Theory(IT)concept,definedasthesetofrealizationsfromagenerativemodelwithempiricalandtheoreticalfrequen-
cies that are sufficiently close according to an appropriate measure of proximity. If an observed trajectory does not
belongtothissetthereisahighprobabilitythatitwasnotgeneratedbytheunderlyingopponentmodel. Thesecond
componentconfirmsthattherewardisconsistentwiththeexpectedreward(averagedacrosstypes).
Formally,letFˆ ht(a′ β)denotetheempiricallikelihoodofanopponentaction,definedasFˆ ht(a′ β) = N(a′ β)/|ht|,where
N(a′ )isthenumberoftimestheactiona′ appearsinthehistoryset. Theδ-stronglytypicalsetofthetrajectories,for
β β
agentwithtypeθ,istheset:
T (θ )={ht :|Fˆ (a′ )−F (a′ )|≤δ·F (a′ )} (7)
δ β ht β θβ β θβ β
whereF (a′ )isthetheoreticalprobabilitythatanagentwithtypeθ willacta′ acrossrounds. Thiscomponentof
θβ β β β
theℵ-mechanism,denotedbyg (Θ ,ht,δ),outputsabinaryvector. Eachentryinthevectorindicateswhetherornot
1 k
theobservedsequencebelongstotheδ-strongtypicalsetofθ ∈Θ .
β k
The parameter δ governs the size of the set, which in turn affects the sensitivity of the mechanism. It can be tuned
usingthenestedopponentmodelstoreducefalsepositives. However,sincethedeceivingagentmodelisabsentfrom
this “training” set, this parameter cannot be tuned to balance true negatives. An additional issue with setting this
parameterisitslackofsensitivitytohistorylength. Thisposesaproblemsincetheempiricalfrequencyisafunction
ofthesamplesize. Weaddressthisissuebymakingtrial-dependent,denotedbyδ(t). Wediscusstask-specificdetails
inAppendix6.1.2.
Thesecondcomponent,denotedbyg (Θ ,ht,rt,ω),verifiestheopponenttypebycomparingtheexpectedrewardto
2 k
theobservedreward.InanyMARLtask,agentsaremotivatedtomaximizetheirutility.Inmixed-motiveandzero-sum
games, thedeceivingagentincreasesitsportionofthejointrewardbyreducingthevictim’sreward. Thisbehaviour
willcontradictthevictim’sexpectationstoearnmore(duetoitassumptionthatithasthehigherDoMlevel). Hence,it
isexpectedthatthevictimisassensitivetodeviationfromtheexpectedrewardastoothersortsofabnormalbehaviour
(suchasthosepickedupbytheδ-strongtypicalitycomponent). Duetothecouplingbetweentheagent’srewardand
itsactions, thiscomponentcomputesthehistory-conditionedexpectedrewardrˆt, byaveragingtheexpectedactions
α
andreactionspertype. Formally,theexpectedreward,peropponenttypeis:
rˆt(θ )=E [E (u (at,at))|ht−1] (8)
α βk−1 at α∼πα at β∼πˆβ α α β
AccountingfortherandombehaviourstemmingfromthepresumedSoftMaxpolicy,theempiricalrewardisverifiedto
bewithinanωconfidenceinterval: rt ∈[rˆt ±z σ(rˆt)],whereσ(rˆt)isthestandarddeviationofthereward(which
α α ω α
has similar dependencies as in equation 8). This component’s output is similar to the output of g (Θ ,ht), namely
1 k
a vector of size |Θ | with each entry is a binary variable, indicating if the observed reward is within the CI or not,
k
implyingwhethertherewardisgeneratedbyaθ ∈ Θ opponent. Notably,iftheexpectedrewardishigherthanthe
β k
upperlimitthiscomponentisactivated,eventhoughsuchaneventbenefitsthevictim. Thiscomponentmaybetuned
toonlyalertwhentheexpectedrewardistoolow,however,thismodificationdoesn’taffectthedetectionasweassume
thatthedeceptionisaimedatharmingthevictim.
Theoutputofbothcomponentsiscombinedusinglogicalconjunction:f =g (·)∧g (·).Notably,thecomponentsare
1 2
correlated,asα’sreward,monitoredbyg isafunctionofβ’sactions,whicharemonitoredbyg . Themechanismis
2 1
describedinAlgorithm1. Theoutput,abinaryvector,isthentakenasinputbytheℵ-policyalongsidethebeliefs.
3.1.2 ℵ-policy
Theagent’sbehaviourisgovernedbyitsℵ-policy(Algorithm2). Ittakesasaninputtheoutputoftheℵ-mechanism
and the updated beliefs. If the opponent’s behaviour passes the ℵ-mechanism, the agent behaviour is its DoM(k)
policy,inthisworkaSoftMaxpolicy. HereweusetheIPOMCPalgorithmfortheQ-valuescomputationHulaetal.
[2015]. This algorithm extends the POMCP algorithm for IPOMDP, however any planning algorithm is applicable.
Inthecasethattheopponent’sbehaviourtriggerstheℵ-mechanism,thevictim’soptimalpolicyswitchestoanOOB
5DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Algorithm1ℵ-mechanism
Input: Θ ,ht,rt,δ,ω
k
Output: f
1: procedureℵ-MECHANISM(Θ k,ht,rt,δ,ω)
2: x←g 1(Θ k,ht,δ)
3: y ←g 2(Θ k,ht,rt,ω)
4: f ←x∧y
5: returnf
6: endprocedure
7: procedureg 1(Θ k,ht,δ)
8: ComputeT δ(θ) ▷From7
9: x←ht ∈T δ(θ)
10: returnx
11: endprocedure
12: procedureg 2(Θ k,ht,rt,ω)
13: Computerˆtgivenht ▷From8
14: y ←rt ∈[rˆt±z ασ(rˆt)]
15: returny
16: endprocedure
policy. While lacking the capacity to simulate the external opponent, the ℵ-policy utilizes the property highlighted
above: the victim knows that the deceiver is of an unknown DoM level. If the deceiver’s DoM level is higher than
thevictim’s,itmeansthatthedeceivercanfullysimulatethevictim’sbehaviour. Hence,iftheOOBpolicyderailsthe
opponent’sutilitymaximizationplans,theopponentwillavoidit. Thebestresponseinevitablydependsonthenature
ofthetask. Weillustratetheℵ-IPOMDPfortwodifferentpayoutstructures: zero-sumandmixed-motive.
Inzero-sumgames,theMinimaxalgorithm[Shannon,1993]computesthebestresponseinthepresenceofanunknown
opponent. Thisprincipleassumesthattheunknownopponentwilltrytoactinthemostharmfulmanner,andtheagent
should be defensive to avoid exploitation. While this policy is beneficial in zero-sum games it is not rational in
mixed-motivegames;itpreventstheagentfromtakingadvantageofthemutualdependencyoftherewardstructure.
Inrepeatedmixed-motivegamesGrimtrigger[Friedman,1971]andTit-for-Tat[Milgrom,1984,NowakandSigmund,
1992]policieshavebeensuggestedaswaysofdeterringadeceptiveopponent. AnagentfollowingtheGrimtrigger
policyrespondstoanydeviationfromcooperativebehaviourwithendlessanti-cooperativebehaviour,evenattherisk
ofself-harm. Whilebeingefficientatdeterringtheopponentfromdefecting,thispolicyhasitspitfalls. First,itmight
be that the opponent’s defective behaviour is by accident and random (i.e., due to SoftMax policy) and so endless
retaliationismisplaced. Second,ifbothplayerscancommunicate,thenawarningshotisasufficientsignal,allowing
the opponent to change their nasty behaviour. This requires a different model, for example, the Communicative-
IPOMDP [Gmytrasiewicz and Adhikari, 2019]. However, in this work, we illustrate how the presence of a simple
Grimtriggerpolicysufficestodeterasavvyopponentfromengagingindeceptivebehaviour.
Algorithm2ℵ-policy
Input: bt (θ ),f
αk βk−1
Output: at+1
1: procedureℵ-POLICY(bt αk(θ βk−1),f)
2: iff ̸=0then:
3: at+1 ∼π(bt αk(θ βk−1))
4: else
5: at+1issampledfromOOBpolicy
6: endif
7: returnat+1
8: endprocedure
6DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
4 ApplicationsinrepeatedBayesiangames
Toillustratetheeffectivenessoftheℵ-IPOMDPinmitigatingmanipulationinrepeatedBayesiangames,wecompare
it to unadorned k-level reasoning (IPOMDP) in a mixed-motive and a zero-sum game. The IPOMDP serves as a
baselinetotheℵ-IPOMDPmodels. Thiscomparisonshowshowtheabilitytodetectandresentdeceptionimproves
the lower DoM agent’s outcome by serving as a protection mechanism, limiting the higher DoM agent’s deceptive
strategies. Inbothgames,theℵ-IPOMDPcanreducetherewardgapbetweenthedeceiverandvictimcomparedtothe
IPOMDPcase.
4.1 Mixed-motivegame
Theiteratedultimatumgame(IUG)[Camerer,2011,Alonetal.,2023a](illustratedinFig1(UltimatumGame); de-
scribedindetailinAlonetal.[2023a])isaparadigmaticrepeatedBayesianmixed-motivegame. Briefly,thegameis
playedbetweentwoagents—asender(α)andareceiver(β). Oneachtrialofthegame,thesendergetsanendowment
of 1 monetary unit and offers a partition of this sum to the receiver: at ∈ [0,1],t ∈ [1,T]. If the receiver accepts
α
the offer (at = 1), the receiver gets a reward of rt = at and the sender a reward of rt = 1−at. Alternatively,
β β α α α
the receiver can reject the offer (at = 0), in which case both parties receive nothing. In this game, agents need to
β
compromise(oratleastconsiderthedesiresoftheother),butatthesametimewishtomaximisetheirreward—making
thetaskausefultestbedfortheadvantagesofhighDoM.
Wesimulatedsenderswith2levelsofDoM:k ∈[−1,1]interactingwithaDoM(0)receiver,inaddition,weincludea
uniformlyrandomsender. AllnonrandomagentsselectactionsviaaSoftMaxpolicy(Eq. 5)withknowntemperature
(T = 0.1). Eachnon-randomsenderisalsocharacterizedbyathreshold,η ∈ {0.1,0.5},whichisaparameterofits
subjectiveutility: ut(at,at,θ )=(1−at −η )·at.
α α β α α α β
Eachagentusesitsnestedmodeloftheopponentforinverseinferenceandplanning.TheDoM(0)receiverinfersfrom
theoffersaboutthetypeoftheDoM(−1)sender—randomorthresholdusinganIRLprocess(Eq.2).Itthenintegrates
thesebeliefsintoitsQ-valuecomputation(viaExpectimaxsearch[Russell,2010]). TheDoM(1)sendersimulatesthe
DoM(0)receiver’sbeliefsandresultingactionsduringthecomputationofitsQ-valuesusingtheIPOMCPalgorithm
[Hulaetal.,2015].
As hypothesized, in the pure IPOMDP case, agents with high DoM levels take advantage of those with lower DoM
levels. The complexity of this manipulation rises with the agents’ cognitive hierarchy. The DoM(0) receiver infers
thetypeoftheDoM(−1)senderandusesitsactionstoalterthebehaviourofthesender(ifpossible). Specifically,it
tricksthethresholdDoM(−1)sendertooffermoreviastrategicrejection(asillustratedinFig. 5). Ontheother,since
therandomsenderaffordsnoagency,theoptimalpolicyforthereceiver,inthiscase,istoacceptanyoffer.
ThisbehaviourdrivestheDoM(1)sender’sploy. IthackstheDoM(0)IRLusingitsDoM(0)nestedmodel. Itspolicy
causes the DoM(0) to falsely believe that it engages with a random agent, by sending a relatively high first offer,
as illustrated in Fig. 2(A). It then executes its ruse by reducing offers and exploiting the DoM(0) receiver’s docile
behaviour against what this receiver falsely perceives to be a random sender (Fig. 2(B)). A full analysis of this is
presented in Alon et al. [2023a] and in Appendix 6.1. This ploy yields the DoM(1) sender a considerable higher
cumulativerewardcomparedtotheDoM(0)receiver(Fig. 3(B)).
Repeatingthesimulationwiththeℵ-IPOMDPframeworkallowsustoillustratehowthispowerstructureisdilutedvia
thedetectionandretaliationoftheℵ-mechanismandℵ-policy.TheparametertuningisdetailedinAppendix6.1.2.As
mentionedabove,theℵ-policyistheGrimTriggerpolicy.Theeffectoftheℵ-IPOMDPisillustratedinFig.3.Firstthe
ℵ-mechanism and the ℵ-policy change the deceiver’s behaviour (compare Figures 2(A) and 3(A)). Each component
limitsthedeceiver’sactionsifitwishestoavoiddetection. Masqueradingagainasarandomsender,thedeceiverhas
toadheretothestatisticalregularitiesoftheℵ-mechanism. Thisisillustratedbythevaryingofferssequence,avoiding
repeatingthesameoffertwiceuntilthe“desirable”offerset(asdefinedbythesender’stype)isdepleted,afterwhich
thesenderdeliberatelytriggerstheℵ-mechanism,effectivelyterminatingtheinteractionasmarkedbytheline-type.In
turn,thislimitationreducestheincomegapbetweentheagents,asillustratedinFig. 3)(B).Thesizeoftheinequality
reductionisafunctionoftheℵ-mechanismparameters. Narrowtheexpectedrewardbounds,bysettingsmallω,will
force the deceiver to make offers that are closer to the masqueraded agent mean offers, reducing the size of the set
of available actions (to avoid alerting the victim). Setting larger values of δ allows the deceiver to repeat the offer
severaltimes. Thecombinationofthetwodeterminestheoutcomeofthegame. However,thisrigiditymayharmthe
victim’sperformancewheninteractingwithagenuinerandomagent,inthecaseoftheIUGtask. Hence,settingthese
parametersrequiresadelicatebalancingoffalseandtruenegativeswithsomedesiredrewardmetric.
7DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure2:IllustrationofdeceptioninIUG:(A)TheDoM(1)actsinadeceptivewaytomasqueradeitselfasarandom
sender,hackingtheDoM(0)BayesianIRL,startingwitharelativelyhighfirstoffer,beforesharpdecrease. (B)This
stratagem exploits the pitfall of Bayesian inference—the likelihood that any offer sequence is equal for the random
sender.
4.2 AZero-sumgame
Deceptionisnotlimitedtomixed-motivegames,itmayalsooccurinzero-sumgames. AcanonicalexampleisPoker
[Paloma¨ki et al., 2016], where players deliberately bluff to lure others into increasing the stakes, only to learn in
hindsightthattheyweretricked. Simplesuchgameswerepresentedandsolvedby[Zamir,1992]. Here,wepresentan
RLvariantofoneofthesegames[Example1.3;Zamir,1992],modelingitwithToM,illustratinghowtheℵ-IPOMDP
allowthebestowedvictimtoresistthemanipulation.
TwoagentswithdifferentDoMlevelsplaythegamepresentedinFig.1(Row/Columngame).Inthisgame,oneoftwo
payoutmatricesG ,G (Eq. 9)ispickedbynaturewithequalprobability,theentriesdenotetherowplayerpayoff.
1 2
(cid:18) (cid:19) (cid:18) (cid:19)
4 0 2 0 4 −2
G1 = ,G2 = (9)
4 0 −2 0 4 2
Therowplayer(α)mayormaynotknowwhichmatrixisselected(alsowithequalprobabilities),whilethecolumn
player(β)isalwaysignorantofthisoutcome. Wedenotebyθ ∈{0,1,2}therowplayer’stype,where1,2markthat
α
αknowswhichpayoffmatrixissampled. ForT >0trialstheagentssimultaneouslychooseactions. Therowplayer
pickseithertheToporBottomrow,whilethecolumnplayerselectstheLeft,MiddleorRightcolumn. Thepayoffis
accordingtotheselectedcell. Crucially,theagentsgetthecumulativerewardonlyattheendofthegameanddonot
getanintermediaterewardthroughout. Thislimitsβ’sinferencetodependonlyonα’sactions. Eachagentselectsits
actionstomaximizeitsdiscountedlong-termreward. AgentscomputeQ-valuesusingtheirDoMlevelandthehistory,
and select an action via SoftMax (Eq.5) policy with a known T. A detailed description of the game is presented in
Appendix6.2.
The informed DoM(−1) row player (θ ∈ {1,2}) assumes a uniform column player Q (a |θ ) =
α−1 α−1 α α−1
E [u (a ,a )]. The DoM(0) column player makes inference about the payoff matrix from the DoM(−1) ac-
aβ∼U α α β
tions(Eq. 2). Forexample,iftherowplayerconstantlyplaysT,thisisastrongsignalthatthepayoffmatrixisG ,
1
as this action’s q-value is Q α−1(T|θ
α−1
= 1) = 2 compared to Q α−1(B|θ
α−1
= 1) = 2/3. Using these beliefs it
computestheQ-valueofeachaction:
Q (a |b (θ ))=E [u (a ,a )] (10)
β0 β β0 α−1 aα∼πα−1(θα−1) β α β
Itspolicyistoselectthecolumnthatyieldsitandtherowplayera0reward(LinG andM inG ). Inturn,DoM(1)
1 2
row player tricks the DoM(0) into falsely believing that the payoff matrix is the other one (for example, if the true
8DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure 3: Mitigation of deception in IGU with ℵ-IPOMDP: (A) Trapped by the ℵ-mechanism’s typical set and
expectedrewardmonitoring,theDoM(1)sendercannotexecuteitsdeception. Instead,theDoM(1)isforcedtoapply
adifferentstrategy,offeringhigherofferstothereceiveruntilitisboundedbyitsutility. (B)Inturn,thisreducesthe
cumulativerewardratio(sender/receiver)bymorethan40%.
payoutmatrixisG itactsinawaytypicalforaDoM(−1)inG ). Thisdeceptionutilizesthesameconceptsasin
1 2
the IUG—the limited opponent modeling of the lower DoM column player and its Bayesian IRL (illustrated in Fig.
4(A)-leftcolumn). Inturn,theDoM(0)Q-valuescomputation(Eq. 10)takesasinputthesefalsebeliefs,resultingin
selectionofthecolumnthatinsteadofyieldingita0reward,isactuallytheleastfavourablecolumn(M inG ,Lin
1
G )yieldingitanegativeutilityof−4. ThissubstantiallybenefitstheDoM(1)rowplayer. UsingitsnestedDoM(1)
2
model,theDoM(2)columnplayer“callsthebluff”andmakescorrectinferencesaboutthepayoutmatrixFig.(4(A).
ItspolicyexploitstheDoM(1)ruseagainstitselfasillustratedinFig. 4(B),bypickingtherightcolumn,yieldingita
rewardof2andarewardof−2toα .
1
Lackingthecapacitytomodelsuchcounter-deceptivebehaviour, theDoM(1)erroneouslyattributingthisbehaviour
totheSoftMaxpolicy, anditsnestedbeliefsaboutthecolumnplayerbeliefsarethedistortedDoM(0)beliefs. This
inability to resist manipulation by higher DoM column player yields high income gap, as illustrated in Fig.4(C).
Simulatingthetaskwiththeℵ-IPOMDPframeworksolvesthiscognitiveadvantageasymmetry: theDoM(1)detects
themismatchbetweenitsworldmodelanditsopponent,asthebehaviouroftheDoM(2)ishighlynontypicalforaβ ,
0
triggeringtheℵ-mechanism. TheDoM(1)MinMaxℵ-policy,i.e.,playingtruthfully,causestheDoM(2)toadaptits
behaviourappropriately(Fig. 4(B)).Inthiscase,bothpartiesget0reward,whichdropstheaverageabsolutereward
differencecomparedtotheIPOMDPcase,asillustratedinFig. 4(C).
5 Discussion
We imbued agents with the ability to assess whether they are being deceived, without (at least fully) having to con-
ceptualisehow. WedosobyaugmentingtheirBayesianinferencewiththeℵ-IPOMDPmechanism, involvingenvi-
ronmental and behavioural heuristic statistical inference. The ℵ-policy of these agents is based on a computational
concept,theabilitytoinferthattheyarefacinganagentoutsideoftheirworldmodelthatthreatenstoharmthem. The
9DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure 4: Effect of ℵ-IPOMDP in a zero sum game: (A) Interacting with a θ = 1, the DoM(0) (left column) is
α
deceived by α’s actions and form false beliefs. However, the DoM(2) utilizes its nested model to read through the
bluffandcorrectlyidentifiesα’stype. (B)(greyline)Inturn,theDoM(2)policyexploitsoftheDoM(1)ruseagainst
it.Augmentedwithℵ-IPOMDP(redline),theDoM(1)infersthisunexpectedbehaviourasasignofanexternalentity,
triggeringtheℵ-mechanism,markedbythedashedline. Itsℵ-policycausestheDoM(2)columnplayertoadaptand
alteritsabusivebehaviour. (C)Theeffectoftheℵ-IPOMDPinthistaskisillustratedviathereductionintheaverage
absoluterewarddifference.
netresultisamoreequitableoutcome. Thisframeworkofferssomeprotectiontomodel-basedIPOMDPagentsthat
areatriskofbeingoutclassedbytheiropponents.
Wetestedthismechanisminmixed-motiveandzero-sumBayesianrepeatedgames. Weshowthatourmechanismcan
protectthevictimagainstagreedy,cleverdeceiverwhoutilizesitsnestedvictimmodeltocoercethevictimtoactin
aself-harmingmanner. Suchapretencedependsonatypicalsequencesofactionsdetectablebytheℵ-mechanism. Of
course,asinGoodhart’slaw(“anyobservedstatisticalregularitywilltendtocollapseoncepressureisplaceduponit
forcontrolpurposes”), thehigherDoMagentcan, atleastifwell-calibratedwithitssimplerpartner, predictexactly
whentheℵ-mechanismwillfire,andtaketailoredoffensivemeasures. However,theneteffectinbothgamesshows
thattheirhandsmightbesufficientlytiedtomaketheoutcomefairer.
Thetwodefiningpointsfortheℵ-IPOMDPframeworkaretheaugmentedinference(theℵ-mechanism),andthepolicy
computation(theℵ-policy). Fortheinference,therecouldbe,aswepresenthere,exquisitelytailoredparametersthat
perform most competently in a given interaction; however, this might not scale. One inspiration for the policy was
thenotionofirritationinthecontextofthemulti-roundtrusttask[Hulaetal.,2018]. Intermsoftheℵ-policy, total
non-cooperation is a rather blunt, and often self-destructive, instrument—despite being a credible and thus effective
threat [McNamara and Houston, 2002]. Alternatively, agents might decide that it is worth investing more cognitive
effort,andthenincreaseone’sDoM[Yuetal.,2021]. Thiscouldbecomeacognitivelyexpensivearmsrace[Sarkadi,
2023].
Our model lacks the capacity to reason about the goals and plans of the deceiver, which may be crucial to facilitate
opponent learning (as inthe work of [Yu et al.,2021], where agents can learn how to adapt their recursive level via
learning). ADoM(k)agentwouldbenefitfromsuchanability(sayviaself-play)inrepeatedinteractions. However,a
10DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
savvyopponent,awareofthislearningcapacity,canstillmanipulatethelearningprocesstoitsbenefit[Foersteretal.,
2018]. Here,weshowhowtoovercomethisissuewithlimited,fixedcomputationalcapacity.
A particular concern for the ℵ-IPOMDP arises when the mismatch stems not from strategic manipulation but from
simplemodelerrororadiscrepancybetweentheactualandassumedpriordistributionsovercomponentsoftheoppo-
nent.Thisissuegivesrisetotworelatedproblems.First,withthespiritofthenofreelunchtheorem,theparametersof
theℵ-policyneedtobalancesensitivityandspecificity. Themechanismsmightreducefalsealarmsattheexpenseof
missingtruedeception,ormightbeoveractiveandcausethevictimtomisclassifytrulyrandombehaviour. Thelatter
may result in paranoid-like behaviour Alon et al. [2023a]. Second, the model currently assumes k-level reasoning.
Thismeansthattheℵ-mechanismisactivatedbyagentswithlowerthank−1DoMlevel.However,aDoM(k)isfully
capableofmodelingtheseagents,astheyarepartofitsnestedopponentmodels. Thus,followingtheideassuggested
by[Camereretal.,2004],futureworkmayextendtheopponentsettoincludeallDoMleveluptok.
Opponent verification is relevant to cybersecurity [Obaidat et al., 2019], where legitimate users need to be verified
and malevolent ones blocked. However, savvy hackers learn to avoid certain anomalies while still exploiting the
randomness of human behaviour. To balance effectively between defence and freedom of use, these systems need
to probe the user actively to confirm the user’s identity. Our model proposes one such solution, but lacks the active
learningcomponent,whichfutureworkmayincorporate.
Inkeepingwithitsrootsincompetitiveeconomics, wefocusedonhowlowerDoMagentsmightbeexploited. One
could also imagine the case that the higher DoM agent exceeds expectations by sharing more than the lower DoM
agentexpects. AlthoughthelowerDoMagentmightconsiderthistobeaploy,itcouldalsobeasignthatthehigher
DoMhasasocialorientation‘baked’intoitspolicytoagreaterextentthanexpected.Inthiscase,thelowerDoMagent
mightwanttohavethecapacitytocompensatefortheover-fairactions—perhapsananalogueofcertaingovernmental
subsidies. Naturally,thesemechanismsarepronetoexcessmanipulation,andsowouldneedcarefulmonitoring.
References
Nitay Alon, Lion Schulz, Peter Dayan, and Joseph M Barnby. Between prudence and paranoia: Theory of mind
gone right, and wrong. In First Workshop on Theory of Mind in Communicating Agents, 2023a. URL https:
//openreview.net/forum?id=gB9zrEjhZD.
Nitay Alon, Lion Schulz, Jeffrey S. Rosenschein, and Peter Dayan. A (dis-) information theory of revealed and
unrevealedpreferences: Emergingdeceptionandskepticismviatheoryofmind. OpenMind,7:608–624,2023b.
JosephMBarnby,PeterDayan,andVaughanBell.Formalisingsocialrepresentationtoexplainpsychiatricsymptoms.
TrendsinCognitiveSciences,2023.
ColinFCamerer. Behavioralgametheory: Experimentsinstrategicinteraction. Princetonuniversitypress,2011.
Colin F. Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A Cognitive Hierarchy Model of Games*. The Quarterly
JournalofEconomics,119(3):861–898,August2004. ISSN0033-5533. doi: 10.1162/0033553041502225. URL
https://doi.org/10.1162/0033553041502225.
Prashant Doshi, Xia Qu, and Adam Goodie. Chapter 8 - Decision-Theoretic Planning in Multiagent Settings with
ApplicationtoBehavioralModeling. InGitaSukthankar,ChristopherGeib,HungHaiBui,DavidV.Pynadath,and
Robert P. Goldman, editors, Plan, Activity, and Intent Recognition, pages 205–224. Morgan Kaufmann, Boston,
January 2014. ISBN 978-0-12-398532-3. doi: 10.1016/B978-0-12-398532-3.00008-7. URL https://www.
sciencedirect.com/science/article/pii/B9780123985323000087.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-Box Adversarial Examples for Text
Classification,May2018. URLhttp://arxiv.org/abs/1712.06751. arXiv:1712.06751[cs].
ScottEvans,EarlEiland,StephenMarkham,JeremyImpson,andAdamLaczo. Mdlcompressforintrusiondetection:
Signatureinferenceandmasqueradeattack. InMILCOM2007-IEEEMilitaryCommunicationsConference,pages
1–7,2007. doi: 10.1109/MILCOM.2007.4455304.
Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch.
Learning with Opponent-Learning Awareness. arXiv:1709.04326 [cs], September 2018. URL http://arxiv.
org/abs/1709.04326. arXiv: 1709.04326.
James W Friedman. A non-cooperative equilibrium for supergames. The Review of Economic Studies, 38(1):1–12,
1971.
Chris D Frith and Uta Frith. Mapping mentalising in the brain. In The neural basis of mentalizing, pages 17–45.
Springer,2021.
11DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
P.J.GmytrasiewiczandP.Doshi. AFrameworkforSequentialPlanninginMulti-AgentSettings. JournalofArtificial
IntelligenceResearch,24:49–79,July2005. ISSN1076-9757. doi: 10.1613/jair.1579. URLhttps://jair.org/
index.php/jair/article/view/10414.
Piotr J Gmytrasiewicz and Sarit Adhikari. Optimal sequential planning for communicative actions: A bayesian ap-
proach. InAAMAS,volume19,pages1985–1987,2019.
JohnC.Harsanyi.Gameswithincompleteinformationplayedby”bayesian”players,i-iii.partiii.thebasicprobability
distribution of the game. Management Science, 14(7):486–502, 1968. ISSN 00251909, 15265501. URL http:
//www.jstor.org/stable/2628894.
AndreasHula,P.ReadMontague,andPeterDayan. MonteCarloPlanningMethodEstimatesPlanningHorizonsdur-
ingInteractiveSocialExchange. PLOSComputationalBiology,11(6):e1004254,June2015. ISSN1553-7358. doi:
10.1371/journal.pcbi.1004254. URL https://journals.plos.org/ploscompbiol/article?id=10.1371/
journal.pcbi.1004254. Publisher: PublicLibraryofScience.
AndreasHula,IrisVilares,TerryLohrenz,PeterDayan,andP.ReadMontague.Amodelofriskandmentalstateshifts
duringsocialinteraction. PLOSComputationalBiology, 14(2):e1005935, February2018. ISSN1553-7358. doi:
10.1371/journal.pcbi.1005935. URL https://journals.plos.org/ploscompbiol/article?id=10.1371/
journal.pcbi.1005935. Publisher: PublicLibraryofScience.
Nicolas Le Guillarme, Abdel-Illah Mouaddib, Sylvain Gatepaille, and Amandine Bellenger. Adversarial Intention
RecognitionasInverseGame-TheoreticPlanningforThreatAssessment. In2016IEEE28thInternationalConfer-
enceonToolswithArtificialIntelligence(ICTAI),pages698–705,November2016.doi:10.1109/ICTAI.2016.0111.
URLhttps://ieeexplore.ieee.org/document/7814671. ISSN:2375-0197.
Benjamin Liebald, Dan Roth, Neelay Shah, and Vivek Srikumar. Proactive Detection of Insider Attacks. Technical
ReportUIUCDCS-R-2007-2879,UniversityofIllinois,2007.
PhilMaguire,PhilippeMoser,RebeccaMaguire,andMarkTKeane.Seeingpatternsinrandomness:Acomputational
modelofsurprise. Topicsincognitivescience,11(1):103–118,2019.
JohnMMcNamaraandAlasdairIHouston. Crediblethreatsandpromises. PhilosophicalTransactionsoftheRoyal
SocietyofLondon.SeriesB:BiologicalSciences,357(1427):1607–1616,2002.
PaulRMilgrom. Axelrod’s”theevolutionofcooperation”,1984.
JohnHNachbarandWilliamRZame. Non-computablestrategiesanddiscountedrepeatedgames. Economictheory,
8:103–122,1996.
Andrew Y. Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. In in Proc. 17th International
Conf.onMachineLearning,pages663–670.MorganKaufmann,2000.
MartinANowakandKarlSigmund. Titfortatinheterogeneouspopulations. Nature,355(6357):250–253,1992.
Mohammad S Obaidat, Issa Traore, and Isaac Woungang. Biometric-based physical and cybersecurity systems.
Springer,2019.
Eric Pacuit and Olivier Roy. Epistemic Foundations of Game Theory. In Edward N. Zalta, editor, The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, summer 2017 edition, 2017. URL
https://plato.stanford.edu/archives/sum2017/entries/epistemic-game/.
Jussi Paloma¨ki, Jeff Yan, and Michael Laakasuo. Machiavelli as a poker mate — A naturalistic behavioural
study on strategic deception. Personality and Individual Differences, 98:266–271, August 2016. ISSN 0191-
8869. doi: 10.1016/j.paid.2016.03.089. URL https://www.sciencedirect.com/science/article/pii/
S0191886916302434.
Grant Pannell and Helen Ashman. User Modelling for Exclusion and Anomaly Detection: A Behavioural In-
trusion Detection System. In Paul De Bra, Alfred Kobsa, and David Chin, editors, User Modeling, Adapta-
tion, and Personalization, pages 207–218, Berlin, Heidelberg, 2010. Springer. ISBN 978-3-642-13470-8. doi:
10.1007/978-3-642-13470-8 20.
JianPeng,Kim-KwangRaymondChoo,andHelenAshman. Userprofilinginintrusiondetection: Areview. Journal
of Network and Computer Applications, 72:14–27, September 2016. ISSN 1084-8045. doi: 10.1016/j.jnca.2016.
06.012. URLhttps://www.sciencedirect.com/science/article/pii/S1084804516301412.
DavidPremackandGuyWoodruff. Doesthechimpanzeehaveatheoryofmind? Behavioralandbrainsciences,1
(4):515–526,1978.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th inter-
nationaljointconferenceonArtificalintelligence,IJCAI’07,pages2586–2591,SanFrancisco,CA,USA,January
2007.MorganKaufmannPublishersInc.
12DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
MiquelRamırezandHectorGeffner. GoalRecognitionoverPOMDPs: InferringtheIntentionofaPOMDPAgent.
Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial machine learning attacks and defense
methodsinthecybersecuritydomain. ACMComputingSurveys(CSUR),54(5):1–36,2021.
StuartJRussell. Artificialintelligence: Amodernapproach. PearsonEducation,Inc.,2010.
Malek Ben Salem, Shlomo Hershkop, and Salvatore J. Stolfo. A Survey of Insider Attack Detection Research. In
SalvatoreJ.Stolfo,StevenM.Bellovin,AngelosD.Keromytis,ShlomoHershkop,SeanW.Smith,andSaraSinclair,
editors, Insider Attack and Cyber Security: Beyond the Hacker, Advances in Information Security, pages 69–90.
Springer US, Boston, MA, 2008. ISBN 978-0-387-77322-3. doi: 10.1007/978-0-387-77322-3 5. URL https:
//doi.org/10.1007/978-0-387-77322-3_5.
StefanSarkadi. AnArmsRaceinTheory-of-Mind: DeceptionDrivestheEmergenceofHigher-levelTheory-of-Mind
inAgentSocieties. In4thIEEEInternationalConferenceonAutonomicComputingandSelf-OrganizingSystems
ACSOS2023.IEEEComputerSociety,2023.
Stefan Sarkadi, Alison R. Panisson, Rafael H. Bordini, Peter McBurney, Simon Parsons, and Martin Chapman.
Modelling deception using theory of mind in multi-agent systems. AI Communications, 32(4):287–302, January
2019a. ISSN 0921-7126. doi: 10.3233/AIC-190615. URL https://content.iospress.com/articles/
ai-communications/aic190615. Publisher: IOSPress.
S¸tefan Sarkadi, Alison R. Panisson, Rafael H. Bordini, Peter McBurney, Simon Parsons, and Martin Chapman.
Modelling deception using theory of mind in multi-agent systems. AI Communications, 32(4):287–302, January
2019b. ISSN 0921-7126. doi: 10.3233/AIC-190615. URL https://content.iospress.com/articles/
ai-communications/aic190615. Publisher: IOSPress.
S¸tefan Sarkadi, Alex Rutherford, Peter McBurney, Simon Parsons, and Iyad Rahwan. The evolution of decep-
tion. Royal Society Open Science, 8(9):201032, September 2021. doi: 10.1098/rsos.201032. URL https:
//royalsocietypublishing.org/doi/full/10.1098/rsos.201032. Publisher: RoyalSociety.
YagizSavas,ChristosK.Verginis,andUfukTopcu. DeceptiveDecision-MakingunderUncertainty. Proceedingsof
theAAAIConferenceonArtificialIntelligence,36(5):5332–5340,June2022. ISSN2374-3468. doi: 10.1609/aaai.
v36i5.20470. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/20470. Number: 5.
Claude E Shannon. Programming a computer for playing chess. In first presented at the National IRE Convention,
March9,1949,andalsoinClaudeElwoodShannonCollectedPapers,pages637–656.IEEEPress,1993.
XiaopengYu,JiechuanJiang,HaobinJiang,andZongqingLu. Model-BasedOpponentModeling. arXiv:2108.01843
[cs],August2021. URLhttp://arxiv.org/abs/2108.01843. arXiv: 2108.01843.
ShmuelZamir. Chapter5Repeatedgamesofincompleteinformation: Zero-sum. InHandbookofGameTheorywith
EconomicApplications,volume1,pages109–154.Elsevier,January1992. doi: 10.1016/S1574-0005(05)80008-6.
URLhttps://www.sciencedirect.com/science/article/pii/S1574000505800086.
6 Appendix
6.1 DetaileddescriptionoftheIUGtask
TheIUGtaskispresentedindetailinAlonetal.[2023a].Herewerepresentthegamestructureandthemaindynamics
forcontext. LetO denotethesetofpotentialoffers(α’sactions),a ∈ [0,1]discretizedinthisworktobinsof0.1,
α
yielding11potentialoffersintotal. InadditionwesetT =12inthiswork. WemodelthisinteractionasanIPOMDP
problem.Theagents’goalistomaximizetheirdiscountedcumulativereward(cid:80)T eΓtut,whereγ =exp(Γ)∈[0,1]
t=1
isadiscountfactor(heresetto0.99). EachagentcomputestheQ-valuesforitsactionsgiventhecurrenthistory, its
typeandDoMlevel,forexample,theQ-valueofaDoM(k)senderaredenotedby: Q (at;ht−1,θ ).
αk α α
Therandomsender’sactionsaredrawnuniformly,anddonotreacttothereceiver’sresponses. Thethresholdsenders
arecharacterizedbyboththeirthresholdandtheirDoMlevel. Thisvalueisanalogoustoaseller’swholesaleprice: if
therewardislowerthanη ,theutilityisnegative,andhencetheactionisunfavorable. Inthisproblem,wefollowthe
α
alternatingcognitivehierarchyformationsuggestedbyHulaetal.[2015],andfirstoutlinetherespectivesendersand
receivers.
TheDoM(−1)sender’spolicyissimpleandmodel-free—ifthecurrentofferisrejected,theyoffermore(uptotheir
threshold) because rejection signals their offer was too low. In turn, if the offer is accepted, the DoM(−1) sender
offersless. Formally,theseagentsmaintaintwoboundsthatareupdatedonline(andthusareadeterministicfunction
13DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
ofthehistoryht−1):
Lt =Lt−1·at−1+at−1·(1−at−1) (11)
β α β
Ut =Ut−1·(1−at−1)+at−1·at−1 (12)
β α β
withL1 =0,U1 =1.Wemakethefurtherassumptionthattheagentsaremyopic,meaningthattheirplanninghorizon
islimitedtothecurrenttrial. Then,theirQ-valuesare:
Q (at;ht−1,θ )=ut(at,θ )∗1 (13)
α−1 α α α α α at α∈(Lt,Ut]
where1 istheindicatorfunction,whichequals1iftheofferisbetweenthebounds.Thatis,itonlycomputes
at∈(Lt,Ut]
α
theQ-valuesforactionsinthisset.
The DoM(0) receiver models the sender as DoM(−1). Since these agents do not form beliefs, the inference is
limitedtotypeinference,threshold: (θ =η )orrandom. TheDoM(0)receiverbeliefsfollowthegeneralinverse
α−1 α
reinforcementlearningoutlinedinEquation1:
bt (θ )= (14)
β0 α−1
pt(η |ht−1,at)∝Pt (at|η ,ht−1)pt−1(η )
α α α−1 α α α
wherePt (at|η ,ht−1)iscomputedfromEquations13and5usingboundsLt,Utthatare,asnoted,adeterministic
α−1 α α
functionofthepriorht−1. TheQ-valuesofeachactionare:
Q (at;at,ht−1,θ )=E (cid:2) ut (at,at)+
β0 β α β at α+ −1 1∼π α∗
−1
β0 α β
γmax {Q (at+1;at+1,ht,θ )(cid:3) (15)
at R+1 β0 β α−1 β
whereE istheexpectationgiventheDoM(−1)sender’soptimalpolicy,computedbytheDoM(0),using
at α+ −1 1∼π α∗
−1
itsnestedmodel. Theprimaryfocusofthereceiver’splanningismanipulatingtheDoM(−1)sender’sboundsthrough
strategicrejectionoracceptance.
Inturn,theDoM(1)sendermodelsthereceiverasaDoM(0). LiketheDoM(−1),thisagent’stypeisitsthreshold,
butalsoitsbeliefsabouttheDoM(0)receiver’sbeliefsaboutitself. Itsutilityfunctionisthesameandtheirfirst-order
beliefs follow the process in Equation 1. Lacking a threshold or any other characteristics, the DoM(1) inference is
boundtotheDoM(0)beliefs: θ = b (η ). Sincethepriorsarecommonknowledge,andtheoffersandresponses
β0 β0 α
arefullyobserved—theDoM(1)inferenceperfectlypredictstheseupdatedbeliefs.
6.1.1 DoM(1)senderdeception
TheDoM(1)sender’sdeceptiontakesadvantageoftheDoM(0)behaviour. WebeginwithpresentingtheDoM(0)vs
DoM(−1)dynamicstoillustratehowtheDoM(0)IRLaffectsitsbehaviour, whichinturnisbeingexploitedbythe
DoM(1). TheDoM(0)receivermakesinferencesabouttheDoM(−1)senderfromitsoffers. Itsoptimalpolicyisa
functionofthesebeliefsasdepictedinFig. 5. WhentheDoM(0)receiverbeliefspointtowardsathresholdsender,the
optimalpolicyistorejecttheoffersuntilthesenderisunwillingtoimproveitsoffers(yieldingthereceiveralarger
shareoftheendowment)(Fig.5(A)).ThisbehaviourtakesadvantageoftheagencytheDoM(0)hasoveranon-random
DoM(−1)sender. Theswitchingtimebetweenrejectionandacceptanceisafunctionofthetaskduration,balancing
explorationwithexploitation. Overall,itsabilitytoaffectthesender’sbehaviouryieldsexcesswealthtotheDoM(0)
receiver. On the other hand, if the beliefs point towards a random opponent, the optimal policy of the receiver is to
acceptanyoffer,asitcannotaffectthissender’sbehaviour.
Usingitscapacitytofullysimulatethisbehaviour,theDoM(1)senderactsdeceptivelybymasqueradingasarandom
sender. ThispreysonthelackofagencythattheDoM(0)hasovertherandomsender. Thisruseischaracterizedby
a signaturemove, depictedin Fig. 2. Thesender beginsby making arelatively highoffer (Fig. 2(A)).This offer is
very unlikely for the threshold DoM(−1) senders, hence the belief update of the DoM(0) receiver strongly favours
the random sender hypothesis (Fig. 2(B)). Once the receiver’s beliefs are misplaced, the sender takes advantage of
the statistical nature of the random sender policy—every offer has the same likelihood: 1 . This allows the sender
|A|
todroptheirsubsequentofferssubstantially,whileavoiding“detection”,sincethereceiver’sbeliefsarefooledbythis
probabilistictrait.
6.1.2 ℵ-IPOMDPparametersinIUG
Simulating this interaction as a ℵ-IPOMDP allows us to illustrate how the ℵ-mechanism detects the masquerading
sender and how the ℵ-policy deters it from doing so. We set the general parameters T = 0.1,γ = 0.99 and the
14DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure5: DoM(0)vsDoM(−1)inIUGIPOMDP:(A)InteractingwithaDoM(−1),theDoM(0)quicklyinfersthe
typeoftheα fromitsfirstactions. TheDoM(−1)thresholdsenderfirstoffertellitapartfromtherandomsenderas
−1
itsinitialofferisalways0.TheDoM(0)policyisafunctionofitsupdatesbeliefs(B).Whenengagingwithathreshold
DoM(−1)sender,itrejectstheoffersuntilthesenderisunwillingto“improve”itsoffers,whichalsocorrespondsto
thecertaintyofitsbeliefs
ℵ-mechanismparameters: δ(t) = max{(T−t)/t,0.5}andsetω = 1.5. Notably,δ(t)isafunctionofthedurationof
the interaction. Typically δ is a fixed, small size. However, due to the sequential nature of the interaction, it seems
implausibletosetthissizeinthismatterforthefollowingreason:theempiricaldistributionisafunctionofthesample
size as well, hence the size of the strong typical set varies with sample size. At the limit lim δ(t) → 0. To
t→T
increasethesetofδ-strongtypicalitytrajectoriesweboundthislimitby0.5—thismeansthatthesizeoftheset(Eq.
7)decreaseswithtime,butremainswideenough. Futureworkcanexploretheinteractionbetweentheseparameters
andfindanoptimalwayoftuningthem.
6.2 IteratedBayesianzero-sumgame
In this game a payout matrix (Eq. 9) is selected randomly (with equal probability) and remains fixed throughout
the interaction. The game is played for T > 0 trials. In each trial the row player picks one of two actions a ∈
α
{T,B}, corresponding to either the top row or the bottom one, and the column player picks one of the columns:
a ∈ {L,M,R}. Theagentspickactionssimultaneouslyandobservetheactionselectedbytheopponentbeforethe
β
nexttrialbegins. Asintheoriginalpaper,thepayoffswerehiddenandrevealedonlyattheendofthegametoavoid
disclosingthegameplayed.
The row player may or may not know which matrix is selected (this event too is uniformly distributed). We denote
therowplayertypebyθ ∈{0,1,2},where0correspondstozeroknowledgeofthegame,θ ∈{1,2}indicatesthat
α α
α knowswhichgameisplayed. Formally: P(θ
α
= 0) = P(θ
α
∈ {1,2}) = 1/2. Bycontrast, β isoblivioustothe
selection,butdrawsinferencesaboutitfromthebehaviourofα,asdescribednext.
In this game we simulate 2 types of row players: either with DoM level k ∈ {−1,1}. We also consider 2 types
of β’s DoM level {0,2}. The DoM(−1) α follows a simple policy. If θ ∈ {1,2}, meaning that the row player
α−1
knowswhichpayoffmatrixisselected,theypickanactionviaSoftMaxpolicy(Equation5)oftheQ-values(assuming
uniformcolumndistribution):
Q (a |θ )=E [u (a ,a )] (16)
α−1 α α−1 aβ∼U α α β
Elseituniformlyrandomizesbetweenthecolumns.
Knowingthis,β appliesIRLtomakeinferencesaboutthegameplayedfromα’sactions:
0
bt (θ )=p(θ |ht)∝P(at|θ )p(θ |ht−1) (17)
β0 α−1 α−1 α α−1 α−1
Weassumeaflatpriorovertherowplayer’stype.AnillustrationofthisIRLispresentedinFig. 6(A).Inthisexample,
thepayoutmatrixisG ,andθ = 1,thatis,therowplayerisinformedaboutthepayoutmatrix. Inthiscase,the
1 α−1
DoM(−1)Q-valuesareQ α−1(T|θ
α−1
= 1) = 2andQ α−1(B|θ
α−1
= 1) = 2/3, implyingthatT isthemostlikely
actionforit. Inturn,theDoM(0)columnplayerisquicktodetectthisanditsoptimalpolicyistoselectthecolumn
thatyieldsit0reward.
TheDoM(1)rowplayermakeinferencesabouttheDoM(0)playerIRLandplansthroughitsbeliefupdateandoptimal
policytomaximizeitsreward:
Q(at α|ht,θ α1)=E aβ∼π β∗ 0[u α(a α,a β)|ht] (18)
whereπ∗ isβ optimalpolicyafterobservingat. Lastly,theDoM(2)modelstherowplayerasaDoM(1). Itinverts
β0 0 α
itsactionstomakeinferenceaboutthepayoffmatrixfromandactoptimally,similarlytotheDoM(0).
15DetectingandDeterringManipulationinaCognitiveHierarchy APREPRINT
Figure6: Row/Columnzero-sumgame: (A)IllustrationofDoM(0)Bayesian-IRL.Giventherowplayer’sactions,
the DoM(0) column player quickly detects the true type (and payoff matrix). (B) Advantage of high DoM level: in
each of the simulated dyads, the higher DoM agent has an edge. Its ability to simulate and predict its opponent’s
behaviourallowsittogainexcesswealth
6.2.1 Manipulation,countermanipulationandcounter-countermanipulation
WebeginbysimulatingthegameasanIPOMDP.TheDoM(1)rowplayerinfersthroughsimulationhowtodeceive
theDoM(0)bymanipulatingthelatter’sbeliefs. Inthistask,itactsinawaythatsignalsthatitknowswhichpayoff
matrixhadbeenchosen,butinawaythatcausestheDoM(0)toformfalsebeliefsaboutthematrix. Forexample,if
thetruematrixisG ,theDoM(1)rowplayerselectsthebottomrow(a = B),atypicalbehaviourforθ = 2(as
1 α −1
thisrowhasthehighestexpectedrewardinG ). Inturn,theDoM(0)pickstheleftcolumn,mistakenlybelievingthat
2
theyarerewardedwith0. Thisployyieldsarewardof4totheDoM(1)player,resultinginahighincomegapinits
favour, as illustrated in Fig. 6(B). Remarkably, this policy exposes α to potential risk as the right column yields it
1
anegativereward. However,givenitsabilitytopredicttheDoM(0)rowplayer’sactionthisriskismitigated. Being
awareofthisruse,theDoM(2)columnplayertricksthetricksterbylearningtoselecttherightcolumn. TheDoM(2)
takeadvantageoftheDoM(1)inabilitytomodelitsbehaviourasdeceptiveandresentit,whichyieldsitarewardof2
ateachtrial,depictedinFig. 6(B).
Wesolvethisissuebysimulatingthegameagainusingtheℵ-IPOMDPframework. Duetotherewardmasking,the
DoM(1)detectsthattheyarematchedwithanexternalopponentonlythroughthetypical-setcomponent. Identifying
thattheyareoutmatched,theℵ-policyistoplaytheMinMaxpolicy—selectingtherowthatyieldsthehighest-lowest
reward. Interestingly,inthistask,thispolicyissimilartotheoptimalpolicyofthe“truth-telling”DoM(−1)agent. In
thiscasetheDoM(2)respondistoselectthecolumnwhichyieldsitthehighestreward—namelytheonethatyieldsit
azeroreward,asevidentin4(C).
16