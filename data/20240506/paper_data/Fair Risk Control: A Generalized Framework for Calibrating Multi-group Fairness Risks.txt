Fair Risk Control: A Generalized Framework for Calibrating
Multi-group Fairness Risks
Lujing Zhang∗ Aaron Roth† Linjun Zhang‡, §
May 6, 2024
Abstract
This paper introduces a framework for post-processing machine learning models so that their
predictionssatisfymulti-groupfairnessguarantees. Basedonthecelebratednotionofmulticalibration,
we introduce (s,G,α)−GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional
mappingss,constraintsetG,andapre-specifiedthresholdlevelα. Weproposeassociatedalgorithms
to achieve this notion in general settings. This framework is then applied to diverse scenarios
encompassing different fairness concerns, including false negative rate control in image segmentation,
prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text
generation in language models. We conduct numerical studies on several datasets and tasks.
1 Introduction
A common theme across the fairness in machine learning literature is that some measure of error or risk
should be equalized across sub-populations. Common measures evaluated across demographic groups
include false positive and false negative rates (Hardt et al., 2016) and calibration error (Kleinberg et al.,
2016; Chouldechova, 2017). Initial work in this line gave methods for equalizing different risk measures on
disjoint groups. A second generation of work gave methods for equalizing measures of risk across groups
even when the groups could intersect – e.g. for false positive and negative rates (Kearns et al., 2018),
calibration error (Úrsula Hébert-Johnson et al., 2018), regret (Blum & Lykouris, 2019; Rothblum & Yona,
2021), prediction set coverage (Jung et al., 2021, 2022; Deng et al., 2023), among other risk measures.
In general, distinct algorithms are derived for each of these settings, and they are generally limited to
one-dimensional predictors of various sorts.
In this work, we propose a unifying framework for fair risk control in settings with multi-dimensional
outputs, based on multicalibration (Úrsula Hébert-Johnson et al., 2018). This framework is developed
as an extension of the work by Deng et al. (2023); Noarov & Roth (2023), and addresses the need for
calibrating multi-dimensional output functions. To illustrate the usefulness of this framework, we apply
it to a variety of settings, including false negative rate control in image segmentation, prediction set
conditional coverage guarantees in hierarchical classification, and de-biased text generation in language
models. These applications make use of the additional power granted by our multi-dimensional extension
of multicalibration.
1.1 Related Work
Multicalibration was introduced by Úrsula Hébert-Johnson et al. (2018) as a fairness motivated constraint
that informally asks that a 1-dimensional predictor of a binary-valued outcome be unbiased, conditional
1WorkwasdoneduringLujingZhang’sremoteresearchinternshipatRutgersandPenn. Email: misdrifter@stu.pku.edu.cn
2UniversityofPennsylvania. Email: aaroth@cis.upenn.edu
3RutgersUniversity. Email: linjun.zhang@rutgers.edu
3CorrespondingAuthor.
1
4202
yaM
3
]LM.tats[
1v52220.5042:viXraon both its own prediction and on membership of the input in some number of pre-defined groups (see
alsoalineofpriorworkthatasksforasimilarsetofguaranteesunderslightlydifferentconditions(Dawid,
1985; Sandroni et al., 2003; Foster & Kakade, 2006)). Subsequently, multicalibration has been generalized
in a number of ways. Jung et al. (2021) generalizes multicalibration to real-valued outcomes, and defines
and studies a variant of multicalibration that predicts variance and higher moments rather than means.
Gupta et al. (2022) extends the study of multicalibration of both means and moments to the online
setting, and defines a variant of mulicalibration for quantiles, with applications to uncertainty estimation.
Bastani et al. (2022); Jung et al. (2022) gives more practical variants of quantile multicalibration with
applications to conditional coverage guarantees in conformal prediction, together with experimental
evaluation. Deng et al. (2023) gives an abstract generalization of 1-dimensional multicalibration, and
show how to cast other algorithmic fairness desiderata like false positive rate control in this framework.
Noarov & Roth (2023) gives a characterization of the scope of 1-dimensional multicalibration variants via
a connection to property elicitation: informally, a property of a distribution can be multicalibrated if and
only if it minimizes some 1-dimensional separable regression function. The primary point of departure of
this paper is that we propose a multi-dimensional generalization of multicalibration: it can be viewed as
the natural multi-dimensional generalization of Deng et al. (2023).
Another line of work generalizes multicalibration in an orthogonal direction, leaving the outcomes
binary valued but generalizing the class of checking rules that are applied. Dwork et al. (2021) defines
outcome indistinguishability, which generalizes multicalibration to require indistinguishability between
the predicted and true label distributions with respect to a fixed but arbitrary set of distinguishers.
Kakade & Foster (2008); Foster & Hart (2018) define “smooth calibration” that relaxes calibration’s
conditioning event to be a smooth function of the prediction. Gopalan et al. (2022) defines a hierarchy of
relaxations called low-degree multicalibration that further relaxes smooth calibration and demonstrates
desirable statistical properties. Zhao et al. (2021) and Noarov et al. (2023) define notions of calibration
tailored to the objective function of a downstream decision maker. These last lines of work focus on
multi-dimensional outputs.
These lines of work are part of a more general literature studying multi-group fairness. Work in this
line aims e.g. to minimize disparities between false positive or false negative rates across groups (Kearns
et al., 2018, 2019), or to minimize regret (measured in terms of accuracy) simultaneously across all groups
(Blum & Lykouris, 2019; Rothblum & Yona, 2021; Globus-Harris et al., 2022; Tosh & Hsu, 2022). A
common theme across these works is that the groups may be arbitrary and intersecting.
1.2 Notation
Let X represent a feature domain, Y represent a label domain, and D denote a joint (feature, label) data
distribution. For a finite set A, we use |A| and ∆A, to denote the cardinality of A and the simplex over A
respectively. Specifically,∆A={(p ,p ,...,p ):0≤p ≤1,(cid:80)|A| p =1}. GivenasetF,weuseProj
1 2 |A| i i=1 i F
to denote the ℓ -projection onto the set. We also introduce some shorthand notation. For two vectors a
2
and b, ⟨a,b⟩ represents their inner product. For a positive integer T, we define [T]={1,2,...,T}. For a
function f(x)=(f (x),f (x),...,f (x)), we denote ∥f∥ =sup [f (x)].
1 2 m ∞ x∈X,i∈[m] i
2 Formulation and Algorithm
2.1 A generalized notion of Multicalibration
Let x∈X represent the feature vector of the input, y ∈Y represent the label, and let h(x)∈H denote a
multi-dimensional scoring function associated with the input. For example, in image segmentation tasks,
h(x)∈Rk (k is the number of pixels) is intended to approximate the probability of a pixel being part of
a relevant segment, often learned by a neural network. In text generation tasks, h(x) is the distribution
over the vocabulary produced by a language model given context x.
For x ∈ X, consider an output function f : X → F ⊂ Rm, defined as f(x) = (f (x),...,f (x)),
1 m
where F is a convex set. We denote the class of functions that f belongs to by Q. For example, in text
2generation tasks, f(x) is the calibrated distribution over the output vocabulary and is multi-dimensional
(with dimension equal to the vocabulary size); in binary classification tasks where h and f are both
scalars, f(x) is the threshold used to convert the raw score h(x) into binary predictions, i.e. 1 .
{h(x)>f(x)}
Wewrites(f,x,h,y,D):Q×X×H×Y×P →Rl todenoteamappingfunctionalofinterest, where
D is the joint distribution of (x,h,y) and P is the distribution space. Here, s is set to be a functional of
f rather than a function of f(x), which offers us more flexibility that will be useful in our applications.
For example, in text generation, where h(x)∈∆Y is the distribution over tokens output by an initial
language model, our goal might be to find f(x)∈∆Y, an adjusted distribution over tokens y ∈Y with
|Y| = m. In this case we could set s = f(x)−E f(x) ∈ Rm to be the mapping functional. We can
x
calibrate the probabilities (through s) to be “fair” in some way – e.g. that the probability of outputting
various words denoting professions should be the same regardless of the gender of pronouns used in the
prompt. We note that we do not always use the dependence of s on all of its inputs and assign different
s in different settings.
We write G to denote the class of functions that encode demographic subgroups (along with other
information) and for each g ∈ G, g(f(x),x) ∈ Rl, consistent with the dimension of s(f,x,h,y,D) so
that we can calibrate over every dimension of s. For example, when l = 1, G can be set to be the
indicatorfunctionofdifferentsensitivesubgroupsofX. Alternately,infairtextgenerationtasks,whenthe
dimension of s equals the size of the set Y, denoted as l=m, we can set the vector g ∈G to have a value
of 1 in the dimensions corresponding to certain types of sensitive words, and 0 in all other dimensions.
We now formally introduce the (s,G,α)-Generalized Multicalibration ((s,G,α)-GMC) definition.
Definition 1 ((s,G,α)-GMC). Let x,h,y,D denote the feature vector, the scoring function, the label
vector, and the joint distribution of (x,h,y) respectively. Given a function class G, mapping functional s,
and a threshold α>0, we say f satisfies (s,G,α)-Generalized Multicalibration ((s,G,α)-GMC) if
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α, ∀g ∈G.
(x,h,y)∼D
(s,G,α)-GMC is a flexible framework that can instantiate many existing multi-group fairness notions,
including s-HappyMap (Deng et al., 2023), property multicalibration (Noarov & Roth, 2023), calibrated
multivalid coverage (Jung et al., 2022) and outcome indistinguishability (Dwork et al., 2021). More
generally, compared to these notions, (s,G,α)-GMC extends the literature in two ways. First, it allows
the functions s and g to be multi-dimensional (most prior definitions look similar, but with 1-dimensional
s and g functions). Second, the function s here is more general and allowed to be a functional of f
(rather than just a function of f(x), the evaluation of f at x). These generalizations will be important in
our applications.
2.2 Algorithm and Convergence Results
To achieve (s,G,α)-GMC, we present the (s,G,α)-GMC Algorithm, which can be seen as a natural
generalization of algorithms used for more specific notions of multicalibration in previous work (Úrsula
Hébert-Johnson et al., 2018; Dwork et al., 2021; Jung et al., 2022; Deng et al., 2023):
Algorithm 1 (s,G,α)-GMC lgorithm
Input: step size η >0, initialization f(0) ∈Q, max iteration T. Initialization: t=0.
while t<T,∃g(t) ∈G s.t:E [⟨s(f(t),x,h,y,D),g(t)(f(t)(x),x)⟩]>α do
(x,h,y)∼D
Let g(t) ∈G be an arbitrary function satisfying the condition in the while statement
f(t+1)(x)=Proj (cid:0) f(t)(x)−ηg(t)(f(t)(x),x)(cid:1)
F
t=t+1
end while
Output: f(t)
It is worth noting that our goal involves functionals concerning our objective function f in order
to capture its global properties. We aim to find a function f such that a functional associated with it
3(obtained by taking the expectation over x) satisfies the inequalities we have set to meet different fairness
demands. Before delving into the main part of our convergence analysis, we introduce some definitions
related to functionals. Examples of these definitions can be found in the appendix Section B.
Definition 2 (The derivative of a functional). Given a function f : X → F, consider a functional
L(f,D):Q×P →R,whereQisthefunctionspaceoff andP isadistributionspaceoverX. Assumethat
L(f,D) follows the formulation that L(f,D)=E [L(f(x))]. The derivative function of L(f,D) with
x∼D
respect to f, denoted as ∇ L(f,D):X →F, exists if ∀w ∈Q,y ∈Rm,D ∈P,E [⟨∇ L(f,D),w⟩]=
f x∼D f
∂ L(f +ϵw,D)| .
∂ϵ ϵ=0
In the following, we introduce the definitions of convexity and smoothness of a functional.
Definition 3 (Convexity of a functional). Let L and f be defined as in Definition 2. A functional L is
convex with respect to f if for any f ,f ∈Q,L(f ,D)−L(f ,D)≥E [⟨∇ L(f ,D),f −f ⟩].
1 2 1 2 x∼D f 2 1 2
Definition 4 (K -smoothnessofafunctional). Let L and f be defined as in Definition 2. A functional L
L
isK L−smoothifforanyf 1,f
2
∈Q,L(f 1,D)−L(f 2,D)≤E x∼D[⟨∇L(f 2,D),f 1−f 2⟩]+E x∼D[K 2L∥f 1−
f ∥2].
2
We will prove that this algorithm converges and outputs an f satisfying (s,G,α)-GMC whenever the
following assumptions are satisfied. These are multidimensional generalizations of the conditions given by
Deng et al. (2023).
Assumptions
(1). There exists a potential functional L(f,h,y,D), such that ∇ L(f,h,y,D)(x)=s(f,x,h,y,D),
f
and L(f,h,y,D) is K -smooth with respect to f for any x∈X.
L
(2). Let f∗(x)≜Proj f(x) for all x∈X. For any f ∈Q, L(f∗,h,y,D)≤L(f,h,y,D) .
F
(3). There exists a positive number B, such that for all g ∈G and all f ∈Q, E [∥g(f(x),x)∥2]≤B.
x∼D
(4). ThereexiststwonumbersC ,C suchthatforallf ∈Q, L(f,h,y,D)≥C ,L(f(0),h,y,D)≤C .
l u l u
Assumption (1) says that a potential functional L exists and it satisfies a K -smoothness condition
L
with respect to f. For example, when f is a predicted distribution, we often set s=f(x)−E f(x).
x∼D
In this situation, L=E [1∥f(x)−E f(x)∥2] satisfies the assumption.
x∼D 2 x∼D
Assumption (2) states that the potential function decreases when projected with respect to f. A
specific example is when F =Y =[0,1] and L=E |f(x)−y|2.
(x,y)∼D
Assumption (3) states that the ℓ -norm of the functions in G is uniformly bounded. It always holds
2
when G contains indicator functions, which is the most common case in fairness-motivated problems
(these are usually the indicator functions for subgroups of the data).
Assumption (4) says that the potential functional L is lower bounded and this generally holds true
when L is convex. One concrete example is when s(f(x),h,y)=f(x)−y and we have L(f,h,y,D)=
E [(f(x)−y)2], which is lower bounded by 0.
x∼D
Theorem1. UnderAssumptions1-4,the(s,G,α)-GMCAlgorithmwithasuitablychosenη =O(α/(K B))
L
converges in T =O(2KL(Cu−Cl)B)) iterations and outputs a function f satisfying
α2
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G.
(x,h,y)∼D
Theproofisprovidedin AppendixC. At ahighlevel, ifweconsider g asa generalizeddirectionvector
and s as the gradient of L, each violation can be interpreted as detecting a direction where the first-order
difference of L is significant. By introducing the assumption of smoothness, our update can result in a
decrease in L that exceeds a constant value. Since L is lower bounded by assumption, the updates can
terminate as described.
42.3 Finite-Sample Results
We have presented Algorithm 1 as if we have direct access to the true data distribution D. In practice,
we only have a finite calibration set D, whose data is sampled i.i.d from D. In this subsection, we show
how a variant of Algorithm 1 achieves the same goal from finite samples.
First, we introduce a useful measure which we call the dimension of the function class, as similarly
defined by Kim et al. (2019); Deng et al. (2023). For a dataset D, we use E to denote the
(x,h,y)∼D
empirical expectation over D. We need T datasets in all and we assume that the whole sample size is m
(m/T for each dataset).
Definition 5 (Dimension of the function class). We use d(G) to denote the dimension of class G, defined
to be a quantity such that if the sample size m≥C d(G)+log(1/δ), then a random sample S of m elements
1 α2 m
from D guarantees uniform convergence over G with error at most α with failure probability at most δ.
That is, for any fixed f and fixed s with ∥s∥ ≤C (C ,C >0 are universal constants):
∞ 2 1 2
sup|E [⟨s(f,x,h,y,D),g(f(x),x)⟩]−E [⟨s(f,x,h,y,D),g(f(x),x)⟩]|≤α.
(x,h,y)∼D (x,h,y)∼Sm
g∈G
A discussion of this definition is given in the appendix.
We now give the finite sample version of the (s,G,α)-GMC Algorithm and its convergence results
below. The detailed proof is in the appendix; we use the uniform convergence guarantee arising from
Definition 5 to relate the problem to its distributional counterpart.
Algorithm 2 (s,G,α)-GMC Algorithm (Finite Sample)
Input: step size η > 0, initialization f(0)(x) ∈ F, validation datasets D , max iteration T.
[2T]
Initialization: t=0.
while t<T,∃g(t) ∈G,s.t.:E [⟨s(f(t)(x),h,y,D ),g(t)(f(t)(x),x)⟩]> 3α do
(x,h,y)∼D2t−1 2t 4
Let g(t) ∈G be an arbitrary function satisfying the condition in the while statement
f(t+1)(x)=Proj (cid:0) f(t)(x)−ηg(t)(f(t)(x),x)(cid:1)
F
t=t+1
end while
Output: f(t)
Theorem 2. Under the assumptions 1-4 given in section 3, suppose we run Algorithm 2 with a suitably
(cid:16) (cid:17)
chosen η =O(α/(κ B)) and sample size m=O T · d(G)+log(T/δ) , then with probability at least 1−δ,
L α2
the algorithm converges in T
=O(cid:0)
(C −C )κ
B/α2(cid:1)
steps and returns a function f satisfying:
u l L
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G.
(x,h,y)∼D
3 Applications
In this section, we explore three applications of our framework: De-biased text generation in language
modeling – where the output function is multi-dimensional and can’t be addressed in other frameworks,
uncertainty quantification in hierarchical classification — in which we can offer prediction set conditional
coverage guarantees, and group-wise false-positive rate control in image segmentation. We begin by
outliningthechallengesrelatedtofairnessandrobustnessinherenttotheseapplications. Subsequently,we
illustrate how to integrate these challenges within the (s,G,α)-GMC framework, enabling their resolution
through Algorithm 1.
3.1 De-Biased Text Generation
This section applies our framework to fair word prediction in language modelling. We think of a language
model as a function that maps prompts to a distribution over the next word. More specifically, we write
5x∈X to denote a prompt, given which the language model outputs a distribution over the vocabulary,
denoted by Y. Namely, the language model generates the probability vector h(x) ∈ ∆Y, and then
samples a word (output) following o(x)∼h(x). Previous studies (Lu et al., 2018; Hoffmann et al., 2022)
demonstrated the presence of gender bias in contemporary language models. Our objective in this section
is to mitigate this issue through an approach that post-processes h(x) to a probability distribution
p(x)∈∆Y that has better fairness properties in specific ways. To take advantage of the information in
initial language model, p is initialized at h.
At the high level, we aim to produce p(x) so that the probabilities of certain groups of words remain
the same whether the prompt includes male-indicating words or female-indicating words. For example,
we might not want “He was a ” to be completed with “doctor” more frequently than “She was a ”
to be completed with “doctor”. We define an attribute set U as a collection of specific sensitive words
and U to be the set of all U, which stands for different kinds of sensitive words. Following the work by
Lu et al. (2018); Hoffmann et al. (2022), we measure the bias of the model on sensitive attribute U by
|P(o(x)∈U|x∈F)−P(o(x)∈U|x∈M)|, where the probability is taken over o(x)∼p(x), and x∈F
and x∈M denotes that x indicates female and male pronouns respectively.
Suppose the marginal distribution over prompt x (which is drawn uniformly from the given corpus)
satisfies that P(x∈F),P(x∈M)≥γ for some positive constant γ >0, we get:
|P(o(x)∈U|x∈F)−P(o(x)∈U|x∈M)|
1
≤ (|P(x∈F)(P(o(x)∈U|x∈F)−P(o(x)∈U))|+|P(x∈M)(P(o(x)∈U|x∈M)−P(o(x)∈U))|).
γ
(1)
As a result, we only need to control the terms on the right side of (1) instead. More specifically, we
want to calibrate the output so that for any subset U ∈U ⊂Y (e.g., gender-stereotyped professions) and
subgroups A∈A⊂X (e.g., gender-related pronouns),
|P(x∈A)·[P(o(x)∈U|x∈A)−P(o(x)∈U)]|≤α.
To better understand this fairness notion, let us consider a toy example where X = {he, she, his,
her}, A = {{he,his},{she,her}}, Y = {lawyer, doctor, dream, nurse}, U = {{lawyer, doctor},{nurse}}.
Our aim is to calibrate the output so that |P[o(x) ∈ {lawyer, doctor}|x ∈ {she, her}] − P[o(x) ∈
{lawyer, doctor}]|≤α and |P[o(x)∈{lawyer, doctor}|x∈{he, his}]−P[o(x)∈{lawyer, doctor}]|≤α.
We can define V ≜{(1,1,0,0),(0,0,0,1)} to be the set of indicator vectors of sensitive attributes defined
by U.
Setting G ≜{1 v :A∈A,v ∈V}∪{−1 v :A∈A,v ∈V}, this problem can be cast in the
{x∈A} {x∈A}
GMC framework, and leads to the following theorem:
Theorem 3. Assuming that x is a prompt that is uniformly drawn from the given corpus, and h is given
by any fixed language model and the size of the largest attribute set in U is upper bounded by B. With a
suitably chosen η =O(α/B), our algorithm halts after T =O(B/α2) iterations and outputs a function p
satisfying: ∀A∈A,U ∈U, when o(x)∼p(x),
sup |P(x∈A)·[P(o(x)∈U|x∈A)−P(o(x)∈U)]|≤α.
A∈A
For the finite-sample counterpart, by applying theorem 2, the sample complexity required in this
setting is O(log(2|U||A|)+log( δ1) ).
α2
3.2 Prediction-Set Conditional Coverage in Hierarchical Classification
Hierarchical classification is a machine learning task where the labels are organized in a hierarchical tree
structure (Tieppo et al., 2022). More specifically, at the most granular level, predictions are made using
labels on the leaves of the tree. These leaves are grouped together into semantically meaningful categories
through their parent nodes, which are, in turn, grouped together through their parents, and so on up to
6the root of the tree. Such a tree structure allows us—when there is uncertainty as to the correct label—to
predict intermediate nodes, which correspond to predicting sets of labels — the set of leaves descended
from the intermediate node — giving us a way to quantify the uncertainty of our predictions. Our goal is
to produce such set-valued predictions that have a uniform coverage rate conditional on the prediction we
make, where a prediction set is said to “cover” the true label if the true label is a descendent of (or equal
to) the node we predicted.
For example, in a K-class hierarchical text classification problem, our input x ∈ X is a document
and the label is a leaf node y on a classification tree with nodes V and edges E. For simplicity, set
V = {1,2,...,|V|} where the first K indices {1,2,..,K} denote leaf nodes (so the groundtruth label
y ∈{1,...,K}). The tree is of depth H. For a given single-class classification model h:x→[0,1]K, let
u(x) ≜ argmax h (x) denote the candidate with the highest score over all leaf nodes according to h.
k k
u(x) here corresponds to the most natural point prediction we might make given h.
Figure 1: A demo of hierarchical text classification using a subset of labels from the Web of Science
dataset. (Kowsari et al., 2017).
As a concrete example, in the tree diagram above, we map the set {1,2,3,4,5,6,7} to represent
the categories: Green Building, Water Pollution, Cancer, Alzheimer’s Disease, Civil, Medical and
Root. Consider a document x with the true label ‘Cancer’ and an initial model predicting scores
h(x)=(0.1,0.1,0.5,0.6). If we used the scores to make a point prediction, we would be incorrect — the
highest scoring label u(x) is “Altzheimer’s disease”, and is wrong: u(x)̸=y. If we output the parent node
( ‘Medical’) instead, our prediction would be less specific (a larger prediction set, here corresponding to
both “Cancer” and “Alzheimer’s Disease”), but it would cover the true label. We would like to output
nodes such that we obtain our target coverage rate (say 90%), without over-covering (say by always
outputting “Root”, which would be trivial). Traditional conformal prediction methods (see Angelopoulos
& Bates (2021) for a gentle introduction) give prediction sets that offer marginal guarantees of this sort,
but not prediction-set conditional guarantees: i.e. they offer that for 90% of examples, we produce a
prediction set that covers the true label. Recent applications of multicalibration related techniques ((Jung
et al., 2021; Gupta et al., 2022; Bastani et al., 2022; Jung et al., 2022; Deng et al., 2023; Gibbs et al.,
2023) are able to give “group conditional” coverage guarantees which offer (e.g.) 90% coverage as averaged
over examples within each of a number of intersecting groups, but once again these methods are not able
to offer prediction-set conditional guarantees. Prediction set conditional guarantees promise that for each
prediction set that we produce, we cover 90% of example labels, even conditional on the prediction set
we offer. This precludes the possibility of our model being over-confident in some prediction sets and
under-confident in others, as demonstrated in our experimental results.
We now define some useful functional notation. Let A:V →VH return the set of all the ancestor
nodes of the input node. Let q :V ×V →V compute the nearest common ancestor of its two input nodes.
Let R : X → R|V| be the function that computes for each node i, R , the sum of the raw scores h(x)
i
assignedtoeachleafthatisadescendentofnodei(oritselfifiisaleaf). Whenneeded,wemayrandomize
R by letting r (x)≜R (x)+ϵ (x), where ϵ(x) is an independent random variable with zero-mean and
i i i
constant variance. We define a natural method to choose a node o(x) to output given a scoring function
h(x) and a threshold function λ(x). We define o(x)≜argmin {d(v):v ∈A(u(x)),r <λ(x)}, where
v v
d(v) denotes the depth of the node v in the tree. In other words, we output the highest ancestor i of u(x)
(which we recall is the point prediction we would make given h alone) whose cumulative score r is below
i
7some threshold — which we will select to obtain some target coverage probability. Other natural choices
of o(x) are possible — what follows uses this choice for concreteness, but is not dependent on the specific
choice.
Recall that an output covers the label if it is the ancestor of the label or the label itself. Our goal is
to find a λ(x), such that the rate at which the output covers the label is roughly equal to a given target
σ, not just overall, but conditional on the prediction set we output lying in various sets U ⊂2V:
|E [1 (σ−1 )]|≤α,∀U ∈U.
(x,h,y)∼D {o(x)∈U} {o(x)coversy}
Backtoourexample,wecanspecifyU invariousways. Forexample,wecansetU ={{1,2,5},{3,4,6}}
to require equal coverage cross the parent categories ‘Civil’ and ‘Medical’. Or, we can set U =
{{1},{2},...,{6},{7}} to obtain our target coverage rate σ conditionally on the prediction set we
output for all possible prediction sets we might output.
We set G ≜ {1 : U ∈ U} ∪ {−1 : U ∈ U}, fitting this problem into our GMC
{o(x)∈U} {o(x)∈U}
framework:
|E [g(o(x))(σ−1 )]|≤α,∀g ∈G.
(x,h,y)∼D {o(x)coversy}
Using (cid:80)K 1 1 = 1 and applying Algorithm 1, we obtain the following
theorem:i=1 {rq(i,u)(x)<λ} {y=i} {o(x)coversy}
Theorem 4. Assume (1). ∀u,∀i∈V,f (u)≤K , where f (u) denotes the density function of r
ri|x p ri|x i
conditioned on x; (2). There exists a real number M >0 such that ∀i∈V,r ∈[−M,M]. With a suitably
i
chosen η =O(α/K ), our algorithm halts after T =O(K M/α2) iterations and outputs a function λ
P P
satisfying that ∀U ∈U,
|E [1 (σ−1 )]|≤α.
(x,h,y)∼D {o(x)∈U} {o(x)coversy}
Applying theorem 2, we can see that the sample complexity for the finite-sample version of the
algorithm is O(log(2|U|)+log( δ1) ).
α2
3.3 Fair FNR Control in Image Segmentation
In image segmentation, the input is an image of m = w×l (w for width and l for length) pixels and
the task is to distinguish the pixels corresponding to certain components of the image, e.g., tumors in a
medical image, eyes in the picture of a face, etc. As pointed out by Lee et al. (2023), gender and racial
biases are witnessed when evaluating image segmentation models. Among the common evaluations of
image segmentation, we consider the False Negative Rate (FNR), defined as FalseNegatives .
FalseNegatives+TruePositives
In image segmentation when O,O′ denotes the set of the actual selected segments and the predicted
segments respectively, FNR=1− |O∩O′|.
|O|
We write x∈X to denote the input, which includes both image and demographic group information
and y ∈{0,1}m to denote the label, which is a binary vector denoting the true inclusion of each of the m
pixels. To yield the prediction of y, namely yˆ∈{0,1}m, a scoring function h(x)∈Rm and a threshold
function λ(x) are needed, so that yˆ =1 for i∈[m]. As in Section 3.2, for technical reasons
i {hi(x)>λ(x)}
we may randomize h by perturbing it with a zero-mean random variable of modest scale. Our objective
i
is to determine the threshold function λ(x).
In the context of algorithmic fairness in image segmentation, one specific application is face segmen-
tation, where the objective is to precisely identify and segment regions containing human faces within
an image. The aim is to achieve accurate face segmentation while ensuring consistent levels of precision
across various demographic groups defined by sensitive attributes, like gender and race. Thus, our
objective is to determine the function λ(x) that ensures multi-group fairness in terms of the FNR — a
natural multi-group fairness extension of the FNR control problem for image segmentation studied by
Angelopoulos et al. (2023).
Letting A be the set of sensitive subgroups of X, our goal is to ensure that the FNR across different
8groups are approximately (1−σ) for some prespecified σ >0:
|O∩O′|
|E [1 (1− −σ)]|≤α, ∀A∈A.
(x,h,y)∼D {x∈A} |O|
We can write |O∩O′|=(cid:80)m y 1 , so the object is converted to
i=1 i {hi(x)>λ(x)}
(cid:80)m y 1
sup |E [1 (1− i=1 i {hi(x)>λ(x)} −σ)]|≤α.
(x,h,y)∼D {x∈A} (cid:80)m y
A∈A i=1 i
we
gL ee tt :s(λ,x,h,y)=1− (cid:80)m i=1y (cid:80)i1
m
i{ =h 1i( yx i)>λ(x)} −σ and G ≜{±1
{x∈A}
:A∈A}. Rewriting the inequality
supE [g(λ(x),x)s(λ,x,h,y)]≤α.
(x,h,y)∼D
g∈G
Cast in the GMC framework, we obtain the following result:
Theorem 5. Assume (1) For all i∈[n], |h |≤M for some universal constant M >0; (2) the density
i
function of h conditioned on x is upper bounded by some universal constant K > 0. Let C be the
i p
set of sensitive subgroups of X. Then with a suitably chosen η =O(α/(K )), the algorithm halts after
P
T =O(2KPM) iterations and outputs a function λ satisfying:
α
|O∩O′|
|E [1 (1− −σ)]|≤α, ∀A∈A.
(x,h,y)∼D {x∈A} |O|
Similar to the previous two applications, by applying Theorem 2 for the finite-sample version of the
algorithm, the sample complexity required is O(log(2|A|)+log( δ1) ).
α2
We note that equalizing false negative rates across groups can be achieved trivially by setting λ to be
large enough so that the FNR is equalized (at 0) — which would of course destroy the accuracy of the
method. Thus when we set an objective like this, it is important to empirically show that not only does
the method lead to low disparity across false negative rates, but does so without loss in accuracy. The
experiments that we carry out in Section 4 indeed bear this out.
4 Experiments
In this section, we conduct numerical experiments and evaluate the performance of our algorithms within
each application from both the fairness and accuracy perspectives. We compare the results with baseline
methods to assess their effectiveness. The code can be found in the supplementary material. For more
detailed experiment settings and additional results, please refer to Appendix D.
4.1 De-Biased Text Generation
In text generation, we consider two datasets and run experiments separately. The first dataset is the
corpus data from Liang et al. (2021), which extracts sentences with both terms indicative of biases (e.g.,
gender indicator words) and attributes (e.g., professions) from real-world articles. The second dataset is
made up of synthetic templates based on combining words indicative of bias targets and attributes with
simple placeholder templates, e.g., “The woman worked as ...”; “The man was known for ...”, constructed
in (Lu et al., 2019).
Then,wedefinetwokindsoftermsindicativeofbiastargets: female-indicatorwordsandmale-indicator
words; we also define six types of attributes: female-adj words, male-adj words, male-stereotyped jobs,
female-stereotyped jobs, pleasant words, and unpleasant words, by drawing on existing word lists in the
fair text generation context (Caliskan et al., 2017) (Gonen & Goldberg, 2019).
Each input x is a sentence where sensitive attributes are masked. We use the BERT model (Devlin
et al., 2018) to generate the initial probability distribution over the entire vocabulary for the word at
9the masked position, denoted by h(x). We then use our algorithm to post-process h(x) and obtain the
functionp(x),whichisthecalibratedprobabilityoftheoutput. Wedefinetwosetsofprompts: A and
female
A bethesetofpromptscontainingfemale-indicatorandmale-indicatorwords,respectively. Weaimto
male
control the gender disparity gap |P(x∈A)·[P(o(x)∈U|x∈A)−P(o(x)∈U)]| for A∈{A ,A }.
female male
Figure 2 plots the disparity gap for A=A (the result for A=A is deferred to the appendix
male female
due to space constraints). It is evident that our post-processing technique effectively limits the disparity
between the probabilities of outputting biased terms related to different gender groups, ensuring that it
remains consistently below a specified threshold value of α=0.002 (we will further discuss the way of
choosing α in the Appendix D). Additionally, we assess the cross-entropy loss between the calibrated
output distribution and the corresponding labels. Unlike the calibration set where sensitive words are
deliberately masked, we randomly mask words during the cross-entropy test to evaluate the model’s
overall performance, extending beyond the prediction of sensitive words. The cross-entropy of the test
set is 9.9291 before post-processing and 9.9285 after it, indicating that our algorithm does not reduce
the accuracy of the model while reducing gender disparities. We would like to note that our algorithm
is not designed to enhance accuracy but to improve fairness while ensuring that the performance of
cross-entropy does not deteriorate too much.
Figure 2: The bias on outputting different types of sensitive attributes measured on the corpus data. The
results for the synthetic data are deferred to the appendix.
4.2 Prediction-Set Conditional Coverage in Hierarchical Classification
For hierarchical classification, we use the Web of Science dataset (Kowsari et al., 2017) that contains
46,985 documents with 134 categories including 7 parent categories. We choose HiAGM (Wang et al.,
2022) as the network to generate the initial scoring. Our algorithm is then applied to find the threshold
function that yields a fair output.
We set our coverage target to be σ = 0.95 with a tolerance for coverage deviations of α = 0.025.
Equivalentlyput,ourgoalisthatforeachofthepredictions,weaimtocoverthetruelabelwithprobability
95±2.5%, even conditional on the prediction we make. We choose naively outputting the leaf node
(denoted as “unprocessed” in the figure) as one baseline and the split conformal method (Angelopoulos
et al., 2023) as another baseline. Figure 3 shows that our method achieves coverage within the target
tolerance for all predictions, while the two baselines fail to satisfy the coverage guarantee for predicting
’CS’ and ’Medical’.
10Figure 3: The deviation of prediction-set conditional coverage from the target.
4.3 Fair FNR Control in Image Segmentation
We use the FASSEG (Khan et al., 2015) dataset and adopt the U-net (Ronneberger et al., 2015) network
to generate the initial scoring function for each pixel, representing the predicted probability of this
pixel corresponding to the signal. The dataset contains 118 human facial images and their semantic
segmentations. We set our target FNR to be σ = 0.075 with a tolerance for deviations of α = 0.005
and calibrate the FNR across different gender subgroups and racial subgroups. In addition, we compare
with the method proposed in (Angelopoulos et al., 2023) that controls on-average FNR in a finite-sample
manner based on the split conformal prediction method. The results yielded by U-net and the split
conformalareplottedasbaselinesforcomparisoninFigure4. Ouralgorithmdemonstratesitseffectiveness
as the deviations of the FNRs of GMC from the target α across all subgroups are controlled below σ,
while the baselines are found to perform poorly on male and white subgroups. Since equalizing FNR
does not necessarily imply accuracy, we compute the accuracy of our model’s output together with that
of the baseline. The accuracy of our model, measured as the ratio of correctly predicted pixels to the
total number of pixels, is 0.86. In comparison, the baseline models achieve an accuracy of 0.84 and 0.92,
respectively. This result suggests that our algorithm empirically yields significant gains in mitigating
FNR disparities without a significant sacrifice in accuracy.
Figure 4: The deviation of the false negative rate from the target in image segmentation.
11References
Angelopoulos, A. N. and Bates, S. A gentle introduction to conformal prediction and distribution-free
uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021.
Angelopoulos, A. N., Bates, S., Fisch, A., Lei, L., and Schuster, T. Conformal risk control, 2023.
Bastani, O., Gupta, V., Jung, C., Noarov, G., Ramalingam, R., and Roth, A. Practical adversarial
multivalid conformal prediction. Advances in Neural Information Processing Systems, 35:29362–29373,
2022.
Bauschke, H. H. and Combettes, P. L. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.
Springer, 2017.
Blum, A. and Lykouris, T. Advancing subgroup fairness via sleeping experts. arXiv preprint
arXiv:1909.08375, 2019.
Caliskan, A., Bryson, J. J., and Narayanan, A. Semantics derived automatically from language corpora
contain human-like biases. Science, 356(6334):183–186, 2017. doi: 10.1126/science.aal4230.
Chouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. Big data, 5(2):153–163, 2017.
Dawid, A. P. Calibration-based empirical probability. The Annals of Statistics, 13(4):1251–1274, 1985.
Deng, Z., Dwork, C., and Zhang, L. Happymap: A generalized multi-calibration method, 2023.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers
for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.
Dwork, C., Kim, M. P., Reingold, O., Rothblum, G. N., and Yona, G. Outcome indistinguishability. In
Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 1095–1108,
2021.
Dwork, C., Lee, D., Lin, H., and Tankala, P. From pseudorandomness to multi-group fairness and back,
2023.
Foster, D. P. and Hart, S. Smooth calibration, leaky forecasts, finite recall, and nash dynamics. Games
and Economic Behavior, 109:271–293, 2018.
Foster, D. P. and Kakade, S. M. Calibration via regression. In 2006 IEEE Information Theory Workshop-
ITW’06 Punta del Este, pp. 82–86. IEEE, 2006.
Gibbs, I., Cherian, J. J., and Candès, E. J. Conformal prediction with conditional guarantees, 2023.
Globus-Harris, I., Kearns, M., and Roth, A. An algorithmic framework for bias bounties. In Proceedings
of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 1106–1124, 2022.
Gonen, H. and Goldberg, Y. Lipstick on a pig: Debiasing methods cover up systematic gender biases
in word embeddings but do not remove them. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pp. 609–614, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1061. URL https://aclanthology.org/N19-1061.
Gopalan, P., Kim, M. P., Singhal, M. A., and Zhao, S. Low-degree multicalibration. In Conference on
Learning Theory, pp. 3193–3234. PMLR, 2022.
Gupta, V., Jung, C., Noarov, G., Pai, M. M., and Roth, A. Online multivalid learning: Means, moments,
and prediction intervals. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022).
Dagstuhl Publishing, 2022.
Hardt, M., Price, E., and Srebro, N. Equality of opportunity in supervised learning. Advances in neural
information processing systems, 29, 2016.
12Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L.,
Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv
preprint arXiv:2203.15556, 2022.
Jung, C., Lee, C., Pai, M., Roth, A., and Vohra, R. Moment multicalibration for uncertainty estimation.
In Conference on Learning Theory, pp. 2634–2678. PMLR, 2021.
Jung, C., Noarov, G., Ramalingam, R., and Roth, A. Batch multivalid conformal prediction, 2022.
Kakade, S. M. and Foster, D. P. Deterministic calibration and nash equilibrium. Journal of Computer
and System Sciences, 74(1):115–130, 2008.
Kearns, M., Neel, S., Roth, A., and Wu, Z. S. Preventing fairness gerrymandering: Auditing and learning
for subgroup fairness. In International conference on machine learning, pp. 2564–2572. PMLR, 2018.
Kearns, M., Neel, S., Roth, A., and Wu, Z. S. An empirical study of rich subgroup fairness for machine
learning. In Proceedings of the conference on fairness, accountability, and transparency, pp. 100–109,
2019.
Khan, K., Mauro, M., and Leonardi, R. Multi-class semantic segmentation of faces. In 2015 IEEE
International Conference on Image Processing (ICIP), pp. 827–831, 2015. doi: 10.1109/ICIP.2015.
7350915.
Kim, M. P., Ghorbani, A., and Zou, J. Multiaccuracy: Black-box post-processing for fairness in
classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp.
247–254, 2019.
Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent trade-offs in the fair determination of risk
scores. arXiv preprint arXiv:1609.05807, 2016.
Kowsari, K., Brown, D. E., Heidarysafa, M., Jafari Meimandi, K., , Gerber, M. S., and Barnes, L. E.
Hdltex: Hierarchical deep learning for text classification. In Machine Learning and Applications
(ICMLA), 2017 16th IEEE International Conference on. IEEE, 2017.
Lee, T., Puyol-Antón, E., Ruijsink, B., Aitcheson, K., Shi, M., and King, A. P. An investigation into the
impact of deep learning model choice on sex and race bias in cardiac mr segmentation. arXiv preprint
arXiv:2308.13415, 2023.
Liang, P. P., Wu, C., Morency, L., and Salakhutdinov, R. Towards understanding and mitigating social
biases in language models. CoRR, abs/2106.13219, 2021. URL https://arxiv.org/abs/2106.13219.
Lu, K., Mardziel, P., Wu, F., Amancharla, P., and Datta, A. Gender bias in neural natural language
processing. CoRR, abs/1807.11714, 2018. URL http://arxiv.org/abs/1807.11714.
Lu, K., Mardziel, P., Wu, F., Amancharla, P., and Datta, A. Gender bias in neural natural language
processing, 2019.
Noarov, G. and Roth, A. The statistical scope of multicalibration. In Krause, A., Brunskill, E., Cho, K.,
Engelhardt,B.,Sabato,S.,andScarlett,J.(eds.),InternationalConferenceonMachineLearning,ICML
2023,23-29July2023,Honolulu,Hawaii,USA,volume202ofProceedingsofMachineLearningResearch,
pp. 26283–26310. PMLR, 2023. URL https://proceedings.mlr.press/v202/noarov23a.html.
Noarov, G., Ramalingam, R., Roth, A., and Xie, S. High-dimensional prediction for sequential decision
making. arXiv preprint arXiv:2310.17651, 2023.
Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image
segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv.org/abs/1505.04597.
Rothblum, G. N. and Yona, G. Multi-group agnostic pac learnability. In International Conference on
Machine Learning, pp. 9107–9115. PMLR, 2021.
Sandroni, A., Smorodinsky, R., and Vohra, R. V. Calibration with many checking rules. Mathematics of
operations Research, 28(1):141–153, 2003.
13Tieppo, E., Santos, R. R. d., Barddal, J. P., et al. Hierarchical classification of data streams: a systematic
literature review. Artificial Intelligence Review, 55:3243–3282, 2022. doi: 10.1007/s10462-021-10087-z.
Tosh, C. J. and Hsu, D. Simple and near-optimal algorithms for hidden stratification and multi-group
learning. In International Conference on Machine Learning, pp. 21633–21657. PMLR, 2022.
Wang, Z., Wang, P., Liu, T., Lin, B., Cao, Y., Sui, Z., and Wang, H. Hpt: Hierarchy-aware prompt
tuning for hierarchical text classification, 2022.
Zhao,S.,Kim,M.,Sahoo,R.,Ma,T.,andErmon,S.Calibratingpredictionstodecisions: Anovelapproach
to multi-class calibration. Advances in Neural Information Processing Systems, 34:22313–22324, 2021.
Úrsula Hébert-Johnson, Kim, M. P., Reingold, O., and Rothblum, G. N. Calibration for the
(computationally-identifiable) masses, 2018.
14A Detailed Discussion of the dimension of the function class
We first state the concentration bounds to use in this section.
Theorem 6 (Generalized Chernoff Bound). Let {X :i∈[n]} be independent random variables satisfying
i
X ∈[a ,b ]. Let X =(cid:80)n X ,µ=E[X], then for λ>0,
i i i j=1 j
2λ2
P(|X−µ|≤λ)≤2exp(− ).
(cid:80)n (b −a )2
j=1 i i
In this section, we will give the specific form of d(G) for G used in some of our applications. In
high-level, the Chernoff Bound is the basis for the derivation of d(G).
We first restate Definition 1 here:
Definition 6 (Dimension of the function class). We use d(G) to denote the dimension of an agnostically
learnable class G, such that if the sample size m≥C d(G)+log(1/δ) for some universal constant C >0,
1 α2 1
then two independent random samples S and S from D guarantee uniform convergence over G with
m1 m2
error at most α with failure probability at most δ, that is, for any fixed f and fixed s with ∥s∥ ≤C for
∞ 2
some universal constant C >0 :
2
sup(cid:12) (cid:12)E (x,h,y)∼D[⟨s(f,x,h,y,D),g(f(x),x)⟩]−E (x,h,y)∼Sm1[⟨s(f,x,h,y,S m2),g(f(x),x)⟩](cid:12) (cid:12)≤α.
g∈G
The quantity d(G) can be upper bounded by the VC dimension for boolean functions, and the metric
entropy for real-valued functions.
When G is a finite function class (which is the case in our applications), we can establish the relation
between d(G) and |G| by the following theorem:
Theorem 7. When G is a finite function class, and for any g ∈G, there exists a real number A>0 such
that ∥g(f(x),x)∥≤A holds for any f(x)∈F and x∈X. We have d(G)=2|G|,C =2(AC )2.
1 2
Proof. From the assumption, we know that
⟨s(f,x,h,y,D),g(f(x),x)⟩∈[−AC ,AC ],∀∥s∥ ≤C ,g ∈G,x∈X,f ∈Q.
2 2 ∞ 2
For any fixed g ∈G, apply theorem 6, we have
m|(E [⟨s(f,x,h,y,D),g(f(x),x)⟩]−E [⟨s(f,x,h,y,S ),g(f(x),x)⟩]|≤λ.
(x,h,y)∼D (x,h,y)∼Sm1 m2
with the probability of failure less than 2exp(− 2λ2 )=2exp(− 2λ2 ). Set λ=αm,m=
(cid:80)m j=1(AC2+AC2)2 4m(AC2)2
2(AC2)2 log(2|G|), We have
α2 δ
|E [⟨s(f,x,h,y,D),g(f(x),x)⟩]−E [⟨s(f,x,h,y,S ),g(f(x),x)⟩]|≤α.
(x,h,y)∼D (x,h,y)∼Sm1 m2
with the probability of failure less than δ .
|G|
Taking a union bound,
sup (cid:12) (cid:12)E (x,h,y)∼D[⟨s(f,x,h,y,D),g(f(x),x)⟩]−E (x,h,y)∼Sm1[⟨s(f,x,h,y,S m2),g(f(x),x)⟩](cid:12) (cid:12)≤α.
g(f(x),x)∈G
with the probability of failure less than δ.
So here we can set d(G)=2|G|,C =2(AC )2.
1 2
15B Examples of the functional definitions
We recall the definitions given in the paper and give some examples for them to give more insights.
Definition 7 (The derivative of a functional). Given a function f : X → F, consider a functional
L(f,D) : Q×P → R, where Q is the function space of f, P is a distribution space over X. Assume
that L follows the formulation that L=E [L(f(x))]. The derivative function of L with respect to f,
x∼D
denoted as ∇ L(f,D):X →F, exists if ∀w ∈Q,y ∈Rm,D ∈P,E [⟨∇ L(f,D),w⟩]
f x∼D f
= ∂ L(f +ϵw,D)| .
∂ϵ ϵ=0
And it’s defined to be a function satisfying the equation.
Example1. WhenL(f,D)=E [1[f(x)−y]2](hereyandf(x)are1-dimensional),∇ L(f,D)(x)=
(x,y)∼D 2 f
f(x)−y.
Example 2. When L(f,D) = E 1∥f(x)−E [f(x)]∥2 (here f is multi-dimensional), ∇ L(f,D) =
x2 x f
f(x)−E [f(x)].
x∼D
Definition 8 (Convexity of a functional). Let L and f be defined as in Definition 2. A functional L is
convex with respect to f if for any f ,f ∈Q,L(f ,D)−L(f ,D)≥E [⟨∇ L(f ,D),f −f ⟩].
1 2 1 2 x∼D f 2 1 2
Definition 9 (K -smoothnessofafunctional). Let L and f be defined as in Definition 2. A functional L
L
isK L−smoothifforanyf 1,f
2
∈Q,L(f 1,D)−L(f 2,D)≤E x∼D[⟨∇L(f 2,D),f 1−f 2⟩]+E x∼D[K 2L∥f 1−
f ∥2].
2
Example 3. L(f,D)=E 1[f(x)−y]2 is 1-smooth and convex with respect to f.
(x,y)∼D2
Example 4. L(f,y)=E 1∥f(x)−E [f(x)]∥2 is 1-smooth and convex with respect to f.
x2 x
Example 5. F =∆Y and L(f,h,y)= 1∥f −E f(x)∥2. Then f∗ =E f(x)∈F and assumption (2)
2 x x
is satisfied in this situation.
C Proof of the theorems
Assumptions
1. There exists a potential functional L(f,h,y,D), such that ∇ L(f,h,y,D)(x)=s(f,x,h,y,D),
f
and L(f,h,y,D) is K -smooth with respect to f for any x∈X.
L
2. Let f∗(x)≜Proj f(x) for all x∈X. For any f ∈Q, L(f∗,h,y,D)≤L(f,h,y,D) .
F
3. There exists a positive number B, such that for all g ∈G and all f ∈Q, E [∥g(f(x),x)∥2]≤B.
x∼D
4. ThereexiststwonumbersC ,C suchthatforallf ∈Q, L(f,h,y,D)≥C ,L(f(0),h,y,D)≤C .
l u l u
Theorem 1. Under the assumptions above, the (s,G,α)-GMC Algorithm with a suitably chosen η =
O(α/(K B)) converges in T =O(2KL(Cu−Cl)B) iterations and outputs a function f satisfying
L α2
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G.
(x,h,y)∼D
Proof. According to the selection of g(t), we have
E [⟨s(f(t),x,h,y,D),g(t)(f(x),x)⟩]>α.
(x,h,y)∼D
L(f(t),h,y,D)−L(f(t+1),x,h,y,D)≥L(f(t),h,y,D)−L(f(t)−ηg(t),h,y,D) (assumption 2)
≥E [⟨∇ L(f(t),h,y,D)(x),ηg(t)(f(t)(x),x)⟩]
(x,h,y)∼D f
η2K
− LE [∥g(f(t)(x),x)∥2] (assumption 1)
2 x
η2K
=ηE [⟨s(f(t),x,h,y),g(t)(f(x),x)⟩]− LE [∥g(f(t)(x),x)∥2]
x,h,y∼D 2 x
η2K
≥ηα− LE [∥g(f(t)(x),x)∥2].
2 x,y∼D
16Set η =α/(K B) and use assumption 3, we get
L
α2
L(f(t),h,y,D)−L(f(t+1),h,y,D)≥ .
2K B
L
So
α2
L(f(0),h,y,D)−L(f(t+1),h,y,D)≥t .
2K B
L
On the other hand,
L(f(0)(x),h,y,D)−L(f(T),h,y,D)≤C −C .
u l
So the iterations will end in 2KLB(Cu−Cl).
α2
Remark 1. The functional-based formulation seems excessive and too complicated in the case where
s(f,x,h,y,D) can degenerate into the form of s′(f(x),h,y), such as in the case where s=f(x)−y.
So we provide another set of assumptions for the degenerated version so that such degenerated version
can be analyzed more easily.
Degenerated version of Assumptions
1. There exists a degenerated mapping functional s′ :F ×H×Y →Rl, such that s(f,x,h,y,D)=
s′(f(x),h,y).
2. There exists a degenerated potential function L(f(x),h,y),s.t.∇ L(f(x),h,y)=s(f(x),h,y),
f(x)
and E L(f(x),h,y) is K -smooth with respect to f(x).
h,y|x L
3. For any f(x)∈F, L(Proj (f(x)),h,y)≤L(f(x),h,y).
F
4. There exists a real number B, such that for all g ∈G, E [∥g(f(x),x)∥2]≤B .
x
5. There exists two real numbers C ,C such that for all h, E L(f(x),h,y)≥C ,E
l u (x,h,y)∼D l (x,h,y)∼D
L(f(0)(x),h,y)≤C .
u
(cid:82)f(x)
Remark 2. When f is not a vector-valued function, we can easily construct L by L= s(u,h,y)du.
0
Remark3. ToprovethatE LisK −smoothinthisversion,wemayonlyprovethatE ∥ ∂ s′(u,h,y)∥
h,y|x L h,y|x ∂u
≤K uniformly.
L
Theorem 2. Under the assumptions 1-4 given in section 3, suppose we run Algorithm 2 with a suitably
(cid:16) (cid:17)
chosen η =O(α/(κ B)) and sample size m=O T · d(G)+log(T/δ) , then with probability at least 1−δ,
L α2
the algorithm converges in T
=O(cid:0)
(C −C )κ
B/α2(cid:1)
, which results in
u l L
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G.
(x,h,y)∼D
for the final output f of Algorithm 2.
(cid:16) (cid:17)
Proof. We can take a suitably chosen m=Ω T · d(G)+log(T/δ) , such that for all t∈[T],
α2
(cid:12) (cid:12)E (x,h,y)∼D[⟨s(f,x,h,y,D),g(f(x))⟩]−E (x,h,y)∼D2t−1[⟨s(f,x,h,y,D 2t),g(f(x))⟩](cid:12) (cid:12)≤α/4.
with failing probability less than δ. Thus, whenever Algorithm 1 updates, we know
T
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≥α/2.
(x,h,y)∼D
with failing probability less than δ. (Taking a union bound over all T iterations.) Thus, the progress for
the underlying potential function is at least α2 . Following similar proof of Algorithm 1, as long as T
8KLB
satisfying (C −C )/ α2 <T, we know Algorithm 2 provides a solution f such that
u l 8KLB
(cid:12) (cid:12)E (x,h,y)∼D[⟨s(f,x,h,y,D),g(f(x),x)⟩](cid:12) (cid:12)≤α.
with probability at least 1−δ.
17Theorem 3. Assuming that x is a prompt that is uniformly drawn from the given corpus, and h is given
by any fixed language model and the size of the largest attribute set in U is upper bounded by B. With a
suitably chosen η =O(α/B), our algorithm halts after T =O(B/α2) iterations and outputs a function p
satisfying: ∀A∈A,U ∈U, when o(x)∼p(x),
sup |P(x∈A)·[P(o(x)∈U|x∈A)−P(o(x)∈U)]|≤α.
A∈A
Proof. We set L(p)= 1E [∥p(x)−E p(x)∥2], and we have
2 x x
(cid:12) (cid:12)
∂∂ ϵL(p+ϵw)(cid:12) (cid:12)
(cid:12)
= 1
2
∂∂ ϵE x[∥p(x)+ϵw(x)−E xp(x)−ϵE xw(x)∥2](cid:12) (cid:12)
(cid:12)
ϵ=0 ϵ=0
=E [⟨p(x)−E p(x),w(x)−E w(x)⟩]
x x x
=E [⟨p(x)−E p(x),w(x)⟩].
x x
so by definition we have ∇ L(p)=p(x)−E p(x)=s. For any p ,p ∈Q,
p x 1 2
(cid:90) 1 ∂
L(p )=L(p )+ E [L(p +t(p −p ))]dt
1 2 ∂t x 2 1 2
0
(cid:90) 1
=L(p )+ E [⟨(p (x)−p (x)),∇ L(p +t(p −p ))(x)⟩]dt.
2 x 1 2 p 2 1 2
0
=L(p )+E [⟨p (x)−p (x),∇ L(p )(x)⟩]
2 x 1 2 p 2
(cid:90) 1
+ E [⟨∇ L(p +t(p −p ))−∇ L(p ),p (x)−p (x)⟩]dt.
x p 2 1 2 p 2 1 2
0
=L(p )+E [⟨p (x)−p (x),∇ L(p )(x)⟩]
2 x 1 2 p 2
(cid:90) 1
+ E [⟨p (x)−E [p (x)]−p (x)+E [p (x)],p (x)−p (x)⟩]tdt
x 1 x 1 2 x 2 1 2
0
=L(p )+E [⟨p (x)−p (x),∇ L(p )(x)⟩]
2 x 1 2 p 2
(cid:90) 1
+ E [∥p (x)−E [p (x)]−p (x)+E [p (x)]∥2]tdt
x 1 x 1 2 x 2
0
(cid:90) 1
≤L(p )+E [⟨∇ L(p )(x),p (x)−p (x)⟩]+ E [∥(p (x)−p (x))∥2]tdt.
2 x p 2 1 2 x 1 2
0
1
=L(p )+E [⟨∇ L(p )(x),p (x)−p (x)⟩]+E [ ∥p (x)−p (x)∥2].
2 x p 2 1 2 x 2 1 2
So L(p) is 1−smooth respect to p, which satisfies the assumption 1.
Obviously, L(p)≥0 for any p∈F. Set p(0) =h∈F, then L(p )≤1. So C =1,C =0. Moreover,
0 u l
E [∥1 v∥2]≤1. So we set B =1.
x {x∈A}
On the other hand, we set F =∆Y in the problem, which is a convex set. We have that for any p,
1 1
L(Proj (p))= E [∥Proj (p(x))−E [Proj (f(x))]∥2]≤ E [∥p(x)−E [p(x)]∥2]=L(f).
F 2 x F x F 2 x x
The last inequality is yielded by the projection lemma commonly used in convex optimization. (See
proposition 4.16 in the work by Bauschke & Combettes (2017).)
Theorem 4. Assume (1). ∀u,∀i∈V,f (u)≤K , where f (u) denotes the density function of r
ri|x p ri|x i
conditioned on x; (2). There exists a real number M >0 such that ∀i∈V,r ∈[−M,M]. With a suitably
i
chosen η =O(α/K ), our algorithm halts after T =O(K M/α2) iterations and outputs a function λ
P P
satisfying that ∀U ∈U,
|E [1 (σ−1 )]|≤α.
(x,h,y)∼D {o(x)∈U} {o(x)coversy}
18Proof. Define h(x)=(r ,r ,...,r ) and set F =[−M,M].
q(1,u(x))) q(2,u(x)) q(K,u(x))
K
(cid:88)
s(λ,x,h,y,D)=σ− 1 1
{hi>λ} {y=i}
i=1
K
(cid:88)
L(λ,h,y,D)=E [σλ(x)− 1 min{λ(x),h (x)}].
(x,h,y)∼D {y=i} i
i=1
Define s′(u,h,y)=s(λ(x),h,y), then L(u,h,y)=σλ(x)−(cid:80)K 1 min{λ(x),h (x)}.
i=1 {y=i} i
Easily we have L(λ,h,y) ≥ (σ−1)M for any λ. Set λ = M, we have L(λ ,h,y) ≤ (σ+1)M. So
0 0
C =(σ+1)M,C =(σ−1)M. On the other hand, obviously, E [12 ]≤1, so we can set B =1.
u l x o(x)∈U

σM −h (x)<σλ−h (x)=L(λ,h,y), λ>M.
 y y
L(Proj (λ),h,y)= L(λ,h,y), λ∈[−M,M].
F
−(σ−1)M
<(σ−1)λ=L(λ,h,y), λ<−M.
Lastly, we check that there exists K such that |∂ E [s′(u,h,y)]|≤K .
p u h|x p
K
(cid:88)
|∂ E [s′(u,h,y)]|=| f (u)1 |≤K .
u h|x hi|x {y=i} p
i=1
Theorem 5. Assume (1). For all i∈[n], |h |≤M for some universal constant M >0; (2). the density
i
function of h conditioned on x is upper bounded by some universal constant K > 0. Let C be the
i p
set of sensitive subgroups of X.Then with a suitably chosen η = O(α/(K )), the algorithm halts after
P
T =O(2KPM) iterations and outputs a function λ satisfying:
α
|O∩O′|
|E [1 (1− −σ)]|≤α, ∀A∈A.
(x,h,y)∼D {x∈A} |O|
Proof. We only need to fit the formulation into the framework of the theorem 1.
(cid:80)m y 1
s(λ,x,h,y,D)=1− i=1 i {hi(x)>λ(x)} −σ.
(cid:80)m
y
i=1 i
m
1 (cid:88)
L(λ,h,y,D)=E [(1−σ)λ(x)− y min{λ(x),h (x)}].
(x,h,y)∼D (cid:80)m y i i
i=1 i i=1
m
1 (cid:88)
=E [ y [(1−σ)λ(x)−min{λ(x),h (x)}]].
x (cid:80)m
y
i i
i=1 i i=1
Define s′(u,h,y)=s(λ(x),h,y) and F =[−M,M].
So L(λ,h,y)=(1−σ)λ(x)− 1 (cid:80)m y min{λ(x),h (x)}.
(cid:80)m i=1yi i=1 i i
Easily we have L(λ,h,y) ≥ −(1−σ)M for any λ. On the other hand, set λ = −M, we have
0
L(λ ,h,y)=σM. So C =σM,C =−(1−σ)M. On the other hand, obviously E [(1 )2]≤1, so
0 u l x {x∈A}
B =1.
 (1−σ)M − 1 (cid:80)m y h (x)<=L(λ,h,y), λ>M.
 (cid:80)m i=1yi i=1 i i
L(Proj (λ),h,y)= L(λ,h,y), λ∈[−M,M].
F
−(1−σ)M
−(−M)=σM <−σλ=L(λ,h,y), λ<−M.
19Lastly, We check that there exists K such that |∂ E [s′(u,h,y)]|≤K .
P u y,h|x p
1 (cid:88)m (cid:90) ∞
|∂ E [s′(u,h,y)]|=|∂ [− y 1 f (v)dv]|
u h|x u (cid:80)m y i {hi(x)>u} hi|x
i=1 i i=1 −∞
m
1 (cid:88)
=| y f (u)|≤K
(cid:80)m y i hi|x p
i=1 i i=1
So |∂ E [s′(u,h,y)]|≤E |∂ E [s(u,h,y)]|≤K .
u h,y|x y|h,x u h|x p
D Experimental Details and Additional Experiments
D.1 De-Biased Text Generation
The experiment is implemented on GeForce MX250 GPU with CUDA version 10.1 and the random seed
we use is 43. We divide the dataset into a calibration set and a test set using a 1:1 ratio.
The value of α is determined as follows: We begin by quantifying the bias present in the unprocessed
model. Subsequently, we setα tobeabout halfthe valuebelow themeasured bias. Thisapproachensures
that our algorithm effectively mitigates the bias while avoiding excessive processing that could potentially
result in a performance decline. We set α=0.002 for both synthetic data and real-world data.
We describe the way to generate our dataset here. For real-world-base corpus, we follow the steps in
the work by Liang et al. (2021) and extract sensitive sentences from 5 real-world text corpora including
WikiText-2, Stanford Sentiment Treebank, Reddit, MELD, POM.
Figure 5: A demonstration of the input data
For the artificial templates, we have adopted the templates from the study by Lu et al. (2018),
which are shown below. The placeholder [OCCUPATION] indicates the word we are predicting, and the
underlined part is where we change different indicator words. For the first 20 templates, we substitute the
underlined part with "he" and "she" respectively, resulting in 40 sentences. For the last two templates,
we substitute the underlined part with female-indicator and male-indicator words, generating additional
sentences. In total, we have produced 503 artificial sentences for the experiment.
• "The [OCCUPATION] ate because he was hungry."
• "The [OCCUPATION] ran because he was late."
• "The [OCCUPATION] drove because he was late."
• "The [OCCUPATION] drunk water because he was thirsty."
• "The [OCCUPATION] slept because he was tired."
• "The [OCCUPATION] took a nap because he was tired."
• "The [OCCUPATION] cried because he was sad."
20• "The [OCCUPATION] cried because he was depressed."
• "The [OCCUPATION] laughed because he was happy."
• "The [OCCUPATION] smiled because he was happy."
• "The [OCCUPATION] went home because he was tired."
• "The [OCCUPATION] stayed up because he was busy."
• "The [OCCUPATION] was absent because he was sick."
• "The [OCCUPATION] was fired because he was lazy."
• "The [OCCUPATION] was fired because he was unprofessional."
• "The [OCCUPATION] was promoted because he was hardworking."
• "The [OCCUPATION] died because he was old."
• "The [OCCUPATION] slept in because he was fired."
• "The [OCCUPATION] quitted because he was unhappy."
• "The [OCCUPATION] yelled because he was angry."
• "He is a [OCCUPATION]"
• "He works as a [OCCUPATION]"
We list the word lists and some basic statistical descriptions here.
Biased
Tokens
Terms
Female-
indicator Mary, Lily, Lucy, Julie, Rose, Rachel, Monica, Jane, Jennifer, Sophia, Ann, Jane,
Words Anna, Carol, Kathy, hers, her, herself, she, lesbian, maternity, motherhood, sister-
hood, goddess, heroine, heroines, woman, women, lady, ladies, miss, queen, queens,
girl, girls, princess, princesses, female, females, mother, godmother, mothers, moth-
ered, motherhood, witch, witches, sister, sisters, daughter, daughters, stepdaugh-
ter, stepdaughters, stepmother, stepmothers, adultress, fiancees, mrs, aunt, aunts,
grandma, grandmas, grandmother, grandmothers, granddaughter, granddaughters,
granny, grannies, momma, mistress, fiancee, hostess, mum, niece, nieces, wife, wives,
bride, brides, widow, widows, madam
Male-
indicator Michael, Mike, John, Jackson, Ham, Ross, Chandler, Joey, Aaron, David, James,
Words Jerry, Tom, his, himself, he, gay, fatherhood, brotherhood, god, hero, heroes, man,
men, sir, gentleman, gentlemen, mr, king, kings, boy, boys, prince, princes, male,
males, father, fathers, godfather, godfathers, fatherhood, brother, brothers, son,
sons, stepson, stepsons, stepfather, stepfathers, adult, fiance, fiances, uncle, uncles,
grandpa, grandpas, grandfather, grandfathers, grandson, grandsons, papa, host, dad,
nephew, nephews, husband, husbands, groom, grooms, bridegroom, bridegrooms,
wizard, wizards, emperor, emperors, boyhood
21Sensitive
Tokens
Attributes
Male-
stereotyped doctor, doctors, professor, professors, lawyer, lawyers, physician, physicians, man-
Professions ager, managers, dentist, dentists, physicist, physicists, scientist, scientists, headmas-
ter, headmasters, governer, governers, architect, architects, supervisor, supervisors,
engineer, engineers, specialist, specialists, teacher, teachers, pharmacist, pharma-
cists, professor, professors
Female-
stereotyped assistant, assistants, secretary, secretaries, nurse, nurses, cleaner, cleaners, adminis-
Professions trator, administrators, typist, typists, accountant, accountant
Female-adj
family, futile, afraid, fearful, dependent, sentimental, delicate, patient, quiet, polite,
Words
considerate, indecisive, pretty
Male-adj
offensive, strong, rude, firm, decisive, stubborn, powerful, brave, cool, professional,
Words
clever
Pleasant
caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond,
Words
gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy,
laughter, paradise, vacation
Unpleasant
abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault,
Words
disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten,
vomit, agony, prison
We also put complete experiment results here, which aren’t put in the paper because of space limits.
Recall that the bias on female-indicated prompts and the bias on male-indicated prompts are defined as
P(x indicates female)[P(o(x)∈U|x indicates female)−P(o(x)∈U)], and
P(x indicates male)[P(o(x)∈U|x indicates male)−P(o(x)∈U)] respectively, where U is a sensitive
attribute to consider. Different sensitive attributes subgroups ranging from “male-stereotyped professions”
to “unpleasant words” are plotted in the graph.
Figure 6: Results over synthetic data. Left: bias on female-indicated prompts. Right: bias on male-
indicated prompts.
22Figure 7: Results over real-world data.Left: bias on female-indicated prompts. Right: bias on male-
indicated prompts.
We conduct the experiments 10 times, varying the random seed, and record the bias. The table below
displays the mean and standard deviation of these deviations. It is evident that our algorithm exhibits a
significant improvement over the baseline.
female adj male adj female occupation male occupation pleasant unpleasant
GMC baseline GMC baseline GMC baseline GMC baseline GMC baseline GMC baseline
mean 0.0012 0.0017 0.0003 0.0003 0.0004 0.0004 0.0025 0.0031 0.0027 0.0034 0.0004 0.0004
std 0.0001 0.0001 0.0001 0.0001 0.0003 0.0003 0.0010 0.0007 0.0019 0.0019 0.0002 0.0002
Table 1: Bias on female-indicated prompts.
female adj male adj female occupation male occupation pleasant unpleasant
GMC baseline GMC baseline GMC baseline GMC baseline GMC baseline GMC baseline
mean 0.0018 0.0019 0.0005 0.0004 0.0004 0.0004 0.0018 0.0025 0.0031 0.0026 0.0004 0.0005
std 0.0011 0.0010 0.0003 0.0002 0.0002 0.0002 0.0010 0.0007 0.0019 0.0014 0.0002 0.0002
Table 2: Bias on male-indicated prompts.
D.2 Prediction-Set Conditional Coverage in Hierarchical Classification
The experiment is implemented on GeForce MX250 GPU with CUDA version 10.1 and the random seed
we use is 45. We divide the dataset into a calibration set and a test set using a 1:1 ratio. To introduce
randomization, we added noise independently sampled from a uniform distribution [−0.005,0.005] to each
point of each dimension of the scoring function h.
We now illustrate the conformal risk control baseline method in detail. As in the former application
We set λ(x) uniformly for all x as λˆ according to the equation (4) in (Angelopoulos et al., 2023),
n B
λˆ =inf{λ: Rˆ (λ)+ }
n+1 n n+1
where Rˆ (λ) = 1(L (λ)+L (λ)+...+L (λ)) and n is the sample size of the calibration set. In the
n n 1 2 n
hierarchical classification setting, denote r(i), x(i), y(i) and u(i) as the ith data point in the calibration
set, we have L (λ)=σ−(cid:80)K 1 1 .
i j=1 {r(i) (x(i))<λ} {y(i)=j}
q(j,u(i))
We provide some basic statistical information regarding the Web of Science dataset(Kowsari et al.,
2017) in the table 3.
23Category CS Medical Civil ECE Biochemistry MAE Psychology
number of sub category 17 53 11 16 9 9 19
number of passages 1287 2842 826 1131 1179 707 1425
Table 3: Basic statistical information of the Web of Science dataset.
We conduct the experiments 50 times, varying the random seed, and record the deviation of the
coverage from the desired coverage conditioned on each subgroup of the output. The table D.2 displays
themeanandstandarddeviationofthesedeviations. Itisevidentthatouralgorithmexhibitsasignificant
improvement over the baseline. Although our approach is not the best across all subgroups, it maintains
a consistently low deviation compared to the other two baselines. The conformal baseline performs poorly
in ’all’ and ’CS’, while the unprocessed data performs poorly in ’ECE’, ’all’, and ’Psychology’.
all CS Medical Civil
GMC con un GMC con un GMC con un GMC con un
mean 0.016 0.068 0.093 0.027 0.071 0.014 0.003 0.001 0.004 0.001 0.001 0.004
std 0.004 0.005 0.005 0.003 0.004 0.003 0.002 0.001 0.003 0.001 0.001 0.001
ECE Biochemistry MAE Psychology
GMC con un GMC con un GMC con un GMC con un
mean 0.002 0.002 0.005 0.002 0.001 0.001 0.001 0.001 0.003 0.002 0.001 0.017
std 0.001 0.001 0.001 0.001 0.001 0.002 0.001 0.001 0.001 0.001 0.001 0.002
Table 4: The deviation of the coverage conditioned on each subgroup of the output in the Hierarchical
Classification.’con’ stands for the results by conformal method and ’un’ stands for the results by unpro-
cessed data.
D.3 Fair FNR Control in Image Segmentation
The experiment is implemented on GeForce MX250 GPU with CUDA version 10.1 and the random seed
we use is 42. We divide the dataset into a calibration set and a test set using a 7:3 ratio.
Figure 8: An overview of the data of the FASSEG dataset (Khan et al., 2015).
The values of σ, and α should be carefully set in the experiment to ensure that the accuracy achieves
good level when the algorithm halts. Intuitively, if the scoring function is trained well, the accuracy will
be at a good level when false negative rate is in an interval close to 0. In our experiment, we set f to be
0
1.5 globally and σ =0.075,α =0.005. So the FNR is controlled to fall within the range of [0.07,0.08]
in our experiment. To introduce randomization, we added noise independently sampled from a uniform
distribution [−0.1,0.1] to each point of each dimension of the scoring function.
We now illustrate the conformal risk control baseline method in detail. We set λ(x) uniformly for all
x as λˆ according to the equation (4) in (Angelopoulos et al., 2023),
n B
λˆ =inf{λ: Rˆ (λ)+ }
n+1 n n+1
where Rˆ (λ) = 1(L (λ)+L (λ)+...+L (λ)) and n is the sample size of the calibration set. In the
n n 1 2 n
24image segmentation setting, denote y(i) and x(i) as the ith data point in the calibration set, we have
L (λ)=1− (cid:80)m j=1y j(i)1 {hj(x(i))>λ} −σ.
i (cid:80)m y(i)
j=1 j
To prove the efficiency and robustness of our results, we conduct the experiments repeatly for 50
times, varying the random seed, and recording the deviation of the empirical False Negative Rate (FNR)
from the desired FNR for each subgroup. The table below displays the mean and standard deviation of
these deviations. It is evident that our algorithm exhibits a significant improvement over the baseline.
female group male group white group non-white group
GMC con un GMC con un GMC con un GMC con un
mean 0.0034 0.0025 0.002 0.0066 0.0092 0.0068 0.0068 0.0118 0.0077 0.0029 0.0037 0.0037
std 0.0026 0.0018 0.0044 0.0044 0.0058 0.0040 0.0045 0.0064 0.0053 0.0026 0.0020 0.0022
Table 5: The deviation of the target FNR rate of each subgroup in the Fair FNR Control in Image
Segmentation. ’con’ stands for the results by conformal method and ’un’ stands for the results by
unprocessed data.
E Embedding Definitions from Related Work into the GMC
Framework
E.1 Existing definitions
Denote x∈X as input, y ∈Y as the labels, f as the prediction model we aim to learn, and C as a set of
functions (for example, the indicator function of certain sensitive groups). We recall the definition of
multi-accuracy, which is widely used to ensure a uniformly small error across sensitive groups:
E [c(f(x),x)(f(x)−y)]≤α, ∀c∈C.
(x,y)∼D
and multicalibration(Úrsula Hébert-Johnson et al., 2018)
E [c(f(x),x)(f(x)−y)|f(x)]≤α, ∀c∈C.
(x,y)∼D
In some settings, f(x)−y can’t explain all the properties of our prediction model. So generalizing
f(x)−y in the multi-accuracy to be s(f(x),y) to generalize the form of the measure of inaccuracy and
yields the definition of s-happy multicalibration in the paper HappyMap(Deng et al., 2023):
|E [c(f(x),x)s(f(x),y)]|≤α, ∀c∈C.
(x,y)∼D
Furthermore in some more complex settings, the labels to predict aren’t true numbers (for example,
sometimes we want to output a permutation), and neither do we predict the label directly. Instead, we
predictavectorfunctionthatgeneratestheoutput(Forexample, wepredictavectorfunctionthatstands
for the probability distribution of the output.) In that setting, we have an outcome indistinguishability
definition(Dwork et al., 2023):
E [A(x,o˜ ,p˜)−A(x,o∗,p˜)]≤ϵ, ∀A∈A
(x,o∗ x)∼D x x
where A is the set of discriminators, o˜ is the output distribution to learn and o∗ is the true underlying
x x
distribution. The goal is to find o˜ such that all discriminators fail to identify it from the true o∗.
x x
Also, in the context of conformal risk control, there exist two definitions to guarantee multivalid
coverage, which are also related to our work. The goal in this context is to find a threshold function such
that it covers the label for an approximate ratio q. The first definition, denoted as Threshold Calibrated
Multivalid Coverage, is defined for sequential data (Gupta et al., 2022; Bastani et al., 2022).
Suppose that there are T rounds of data in total, namely {(x(t),y(t))}T . Define T ⊆ Y to be
t=1
the conformal prediction and s(t) : X ×Y → R to be the given scoring function. Denote q(t) as
≥0
25round-dependent threshold, which gives us a prediction set T(t) =(cid:8) y ∈Y :s(t)(cid:0) x(t),y(cid:1) ≤q(t)(cid:9). Fix a
coverage target (1−δ) and a collection of groups G ⊂2X.
A sequence of conformity thresholds {q(t)}T is said to be (θ,m)-multivalid (Jung et al., 2022) with
t=1
respect to δ and G for some function θ : N → R, if for every i ∈ [m] ≜ {1,2,...,m} and G ∈ G, the
following holds true:
(cid:12) (cid:16) (cid:17) (cid:12) (cid:16)(cid:12) (cid:12)(cid:17)
(cid:12)H¯ G(T)(i) −(1−δ)(cid:12)≤θ (cid:12)G(T)(i)(cid:12) .
(cid:12) (cid:12) (cid:12) (cid:12)
where
(cid:26) (cid:27)
i−1 i
G(t)(i)= τ ∈[t]:x ∈G,q(t) ∈[ , ) ,
τ m m
H¯(S)= 1 (cid:88) 1 ).
|S| {s(t)≤q(t)}
t∈S
The second definition, denoted as q-quantile predictor is defined for batch data (Jung et al., 2022). Using
g′(x)=1 to denote the membership of certain subgroup of x, the quantile calibration error of q-quantile
predictor f :X →[0,1] on group g′ is:
Q(f,g′)= (cid:88) P (f(x)=v |g′(x)=1)(cid:0) q−P (s≤f(x)|f(x)=v,g′(x)=1)(cid:1)2 .
(x,s)∼S (x,s)∼S
v∈R(f)
We say that f is α-approximately q-quantile multicalibrated with respect to group collection G′ if
α
Q(f,g′)≤ for every g′ ∈G′.
P (g′(x)=1)
(x,s)∼S
E.2 Expressing These Definitions as Generalized Multicalibration
We prove that the definition can be reduced to both the definition of HappyMap and the definition of
Indistinguishability.
E.2.1 s-HappyMap
Recall that the definition of s-HappyMap is
E [c(f(x),x)s(f(x),y)]≤α, ∀c∈C.
(x,y)∼D
And (s,G,α)−GMC (where s(x)∈Rm) is defined as:
E [⟨g(x,f(x))s(f,x,h,y,D)⟩]≤α, ∀g ∈G.
(x,h,y)∼D
We transform (s,G,α)-GMC into s-HappyMap by defining the following function: (The left side of
the equation represents the notation of (s,G,α)-GMC, while the right side represents the notation of
s-HappyMap.)
m=1,G =C,g =c,f(x)∈R,s(f,x,h,y,D)=s(f(x),y).
Then we have
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]=E [s(f(x),y)c(f(x),x)].
(x,h,y)∼D (x,y)∼D
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G.
(x,h,y)∼D
⇔ E [s(f(x),y)c(f(x),x)]≤α,∀c∈C.
(x,y)∼D
Thus, the formulation of (s,G,α)-GMC is reduced to the s-HappyMap.
26E.2.2 Outcome Indistinguishability
Recall the definition of the outcome indistinguishability is
E [A(x,o˜ ,p˜)−A(x,o∗,p˜)]≤ϵ, ∀A∈A
(x,o∗ x)∼D x x
where A is the set of discriminators, o˜ is the output distribution to learn and o∗ is the true underlying
x x
distribution. The goal is to find o˜ such that all discriminators fail to identify it from the true o∗.
x x
And (s,G,α)−GMC (where s(x)∈Rm) is defined as:
E [⟨g(x,f(x))s(f,x,h,y,D)⟩]≤α, ∀g ∈G.
(x,h,y)∼D
We transform (s,G,α)-GMC into Outcome Indistinguishability by defining the following function:
(The left side of the equation represents the notation of (s,G,α)-GMC, while the right side represents the
notation of Outcome Indistinguishability.)
O =∆O ={x∈[0,1]m :(cid:80)m x =1}, f(x)=p˜(x)∈Rm is the probability distribution of over
i=1 i
the output space. Denote Y ={Y ,Y ,...,Y } and p∗ =(P[y =Y ],P[y =Y ],...,P[y =Y ]), we further
1 2 K y 1 2 K
set
s(p˜,x,h,y,D)=p˜(x)−p∗ ∈Rm. G ={g(p˜(x),x)=A(x,·,p˜)∈Rm :A∈A}.
y
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]=E[A(x,o˜ ,p˜)−A(x,o∗,p˜)].
(x,h,y)∼D x x
E [⟨s(f,x,h,y,D),g(f(x),x)⟩]≤α,∀g ∈G. ⇔ E[A(x,o˜ ,p˜)−A(x,o∗,p˜)]≤α,∀A∈A.
(x,h,y)∼D x x
Thus, the formulation of (s,G,α)-GMC is reduced to the Outcome Indistinguishability.
E.2.3 Multivalid Prediction
Recall the definition of the (θ,m)−multivalid prediction is
(cid:12) (cid:16) (cid:17) (cid:12) (cid:16)(cid:12) (cid:12)(cid:17)
(cid:12)H¯ G(T)(i) −(1−δ)(cid:12)≤θ (cid:12)G(T)(i)(cid:12) . ∀G∈G,∀i∈[m].
(cid:12) (cid:12) (cid:12) (cid:12)
where
(cid:26) (cid:27)
i−1 i
G(t)(i)= τ ∈[t]:x ∈G,q(t) ∈[ , ) ,
τ m m
H¯(S)= 1 (cid:88) 1 .
|S| {s(t)≤q(t)}
t∈S
Recall that the finite-sampling version (s,G,α)-GMC (where s(x)∈Rm) is defined as:
T
1 (cid:88)
[⟨g(x ,f(x )),s(f,x ,h,y ,D)⟩]≤α, ∀g ∈G.
T (i) (i) (i) (i)
i=1
Where T denotes the sample size, {(x ,y )} denotes the data. We transform the finite sampling
(i) (i)
version of (s,G,α)-GMC into (θ,m)−multivalid by defining the following function: (The left side of
the equation represents the notation of (s,G,α)-GMC, while the right side represents the notation of
(θ,m)−multivalid.)
Let m = 1,h = s,f = q,s(f,x,h,y,D) = 1 −(1−δ),G = {1 1 : G ∈ G,i ∈
{s≤q} {x∈G} q∈[i−1,i)
m m
[m]},θ(|G(t)(i)|)= αT .
|G(t)(i)|
27When g(f(x),x)=1 1 ,
{x∈G} {q∈[i−1,i)}
m m
(T) T
1 (cid:88) 1 (cid:88)
[⟨g(x ,f(x ))s(f,x ,h,y ,D)⟩]= g(q(x(t)),x(t))(1 −(1−δ))
T (i) (i) (i) (i) T {s(x(t),y(t))≤q(x(t))}
i=1 t=1
|G(T)(i)|
= |H¯(G(T)(i))−(1−δ)|
T
≤α
(cid:16) (cid:17) αT
⇔|H¯ G(T)(i) −(1−δ)|≤ =θ(|G(T)(i)|)).
|G(T)(i)|
So
T
1 (cid:88)
[⟨g(x ,f(x ))s(f,x ,h,y ,D)⟩]≤α, ∀g ∈G
T (i) (i) (i) (i)
i=1
(cid:16) (cid:17) αT
⇔ |H¯ G(T)(i) −(1−δ)|≤ =θ(|G(T)(i)|)),∀G∈G,∀i∈[m].
|G(T)(i)|
Thus, the sampling version of (s,G,α)−GMC is reduced to the MultiValid Prediction where θ is of a
specific form.
E.2.4 Quantile Multicalibration
The quantile calibration error of q-quantile predictor f :X →[0,1] on group g′ is:
Q(f,g′)= (cid:88) P (f(x)=v |g′(x)=1)(cid:0) q−P (s≤f(x)|f(x)=v,g′(x)=1)(cid:1)2 .
(x,s)∼S (x,s)∼S
v∈R(f)
Recall that f is α-approximately q-quantile multicalibrated with respect to group collection G′ if
α
Q(f,g′)≤ for every g′ ∈G′.
P (g′(x)=1)
(x,s)∼S
where g′(x)=1 denotes that x belong to a certain subgroup. This is equal to
(E [1 1 12 )]
q2E [1 ]−2qE [1 1 ]+ (cid:88) (x,s)∼S {s≤f} {g′(x)=1} {f(x)=v} ≤α,
(x,s)∼S {g′(x)=1} (x,s)∼S {s≤f} {g′(x)=1} E [1 1 ]
(x,s)∼S {f(x)=v} {g′(x)=1}
v∈R(f)
∀g′ ∈G′.
We set G = {±1 : g′ ∈ G′},s(f,x,h,y,D) = q−1 , then E = E [g(x)(q−
{g′(x)=1} {s≤f} (x,h,y)∼D (x,s)∼S
1 )]. (The left side of the equation represents the notation of (s,G,α)-GMC, while the right side
{s≤f}
represents the notation of α-approximately q-quantile.) We have that
|E [1 (q−1 )]|≤α ∀g′ ∈G′.
(x,s)∼S {g′(x)=1} {s≤f}
This is equal to
|qE [1 ]−E [1 1 ]|≤α ∀g′ ∈G′.
(x,s)∼S {g′(x)=1} (x,s)∼S {g′(x)=1} {s≤f}
The two definitions can’t be reduced to each other because of the presence of the term
(cid:80) (E (x,s)∼S[1 {s≤f}1 {g′(x)=1}12 {f(x)=v})] . Despite their differences, they both provide robust measures
v∈R(f) E (x,s)∼S[1 {f(x)=v}1 {g′(x)=1}]
for assessing the extent of coverage in the given context.
28