DISCRETIZATION ERROR OF FOURIER NEURAL OPERATORS∗
SAMUEL LANTHALER†, ANDREW M. STUART∗, AND MARGARET TRAUTNER∗
Abstract. Operator learning is a variant of machine learning that is designed to approximate
mapsbetweenfunctionspacesfromdata. TheFourierNeuralOperator(FNO)isacommonmodel
architecture used for operator learning. The FNO combines pointwise linear and nonlinear opera-
tionsinphysicalspacewithpointwiselinearoperationsinFourierspace,leadingtoaparameterized
mapactingbetweenfunctionspaces. AlthoughFNOsformallyinvolveconvolutionsoffunctionson
a continuum, in practice the computations are performed on a discretized grid, allowing efficient
implementationviatheFFT.Inthispaper,thealiasingerrorthatresultsfromsuchadiscretization
is quantified and algebraic rates of convergence in terms of the grid resolution are obtained as a
functionoftheregularityoftheinput. Numericalexperimentsthatvalidatethetheoryanddescribe
modelstabilityareperformed.
Key words. Operatorlearning,machinelearning,erroranalysis
MSC codes. 41A35,65T50,68T07
1. Introduction.
1.1. Overview. While most machine learning architectures are designed to ap-
proximate mapsbetween finite-dimensionalspaces, operatorlearningis adata-driven
methodthatapproximatesmapsbetweeninfinite-dimensionalfunctionspaces. These
maps appear commonly in scientific machine learning applications such as surrogate
modelingofpartialdifferentialequations(PDE)ormodeldiscoveryfromdata. Fourier
Neural Operators (FNOs) [16] are a type of operator learning architecture that pa-
rameterize the model directly in function space, naturally generalizing deep neural
networks (DNNs). In particular, each hidden layer of an FNO assigns a trainable
integral kernel that acts on the hidden states by convolution in addition to the usual
affine weights and biases of a DNN. Taking advantage of the duality between con-
volution and multiplication under Fourier transforms, these convolutional kernels are
represented by Fourier multiplier matrices, whose components are optimized during
training, along with the regular weights and biases acting in physical space. FNOs
have proven to be an effective and popular operator learning method in several PDE
application areas including weather forecasting [21], biomedical shape optimization
[24], and constitutive modeling [3]. It is thus of interest to study their theoretical
properties.
Although FNOs approximate maps between function spaces, in practice, these
functions must be discretized. FNOs are discretization invariant in the sense that
varying discretizations of the function space data may be used for both training and
testing of the model without changing any model parameters. However, the model
performs convolutions on the hidden states, and the discretization of the data can
affect the accuracy of these convolutions through aliasing error. During a forward
∗SubmittedtotheeditorsMonday6th May,2024.
Funding: SLissupportedbyPostdoc.MobilitygrantP500PT-206737fromtheSwissNational
Science Foundation. The work of AMS is supported by a Department of Defense Vannevar Bush
FacultyFellowship,andbytheSciAICenter,fundedbytheOfficeofNavalResearch(ONR),under
Grant Number N00014-23-1-2729. MT is supported by the Department of Energy Computational
ScienceGraduateFellowshipunderawardnumberDE-SC00211.
Allcodeanddataforthisworkisavailableathttps://github.com/mtrautner/BoundFNO.
†Department of Computing and Mathematical Sciences, California Institute of Technology,
Pasadena,CA(slanth@caltech.edu,astuart@caltech.edu,trautner@caltech.edu).
1
This manuscript is for review purposes only.
4202
yaM
3
]AN.htam[
1v12220.5042:viXra2 S.LANTHALER,A.STUART,ANDM.TRAUTNER
passoftheFNO,thediscretizationerrorsofeachhiddenlayerwillpropagatethrough
the subsequent layers of the FNO and may be amplified by nonlinearities. In previ-
ous theoretical analyses of the universal approximation (UA) properties of the FNO
[11, 12], the consequent discretization error is ignored completely as the states are
considered functions rather than discretizations of functions. While this approach to
UA is theoretically sound, it leaves the discretization components of the error un-
quantified in practice. In this paper, we analyze such components of the error both
theoretically and experimentally.
Theerrorresultingfromperformingasingleconvolutiononagridratherthanon
a continuum depends on the regularity, or smoothness, of the input function in the
Sobolev sense. Thus, to bound the error for an entire FNO, regularity must be main-
tained as the state passes through the layers of the network, including the nonlinear
activation function. In particular, regularity-preserving properties of compositions of
nonlinearfunctionsarerequired. BoundsofthistypearegivenbyMoser[19]andform
a key component of the proofs in this work. Because the smooth GeLU (Gaussian
Error Linear Unit) [7] activation preserve regularity, while the (non-differentiable)
ReLU activations do not, the analysis in this paper is confined to the former and
extends to other smooth activation functions.
1.2. Contributions. In this paper, we make the following contributions.
(C1) Weboundtheoreticallythealiasingerrorthatresultsfromapproximatingthe
continuum FNO on a grid.
(C2) We validate this theory concerning the discretization error of the FNO with
numerical experiments.
(C3) We provide heuristics for avoiding the effects of discretization error in prac-
tice.
(C4) We propose an adaptive subsampling algorithm for faster operator learning
training.
InSection2wesetuptheframeworkforourtheoreticalresults. Section3studies
thediscretizationerroroftheFNOtheoretically,makingcontribution(C1). InSection
4wepresentnumericalexperimentsthatillustratethetheoreticalresultsandproposes
an algorithm for adaptively refining the discretization during training. Thus, Section
4makescontributions(C2,C3,C4). WeconcludeinSection5. Sometechnicaldetails
are contained in the appendices.
1.3. Background. Neuralnetworkshavebeenverysuccessfulinapproximating
solutions of partial differential equations using data. Several approaches are used for
such models, including physics-informed neural networks (PINNs), constructive net-
works, and operator learning models. In the case of PINNs, a standard feed-forward
machine learning architecture is trained with a loss function involving a constraint
of satisfying the underlying PDE [22]. Such models have shown empirical success in
many problems of application [5, 4, 10]. One disadvantage of PINNs is that only a
single solution of the underlying PDE is approximated. Thus, adapting the model
to different initial or boundary conditions often requires retraining it from scratch.
Another approach to applying machine learning to PDEs is to construct approximat-
ing networks from classical PDE-solver methods. For example, in [8, 9, 17], ReLU
neural networks are shown to replicate polynomial approximations and continuous,
piecewise-linear elements used in finite element methods exactly; thus, the construc-
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 3
tiveapproximationproofsusedforpolynomialsandfiniteelementmethodsapply,and
the network weights may be constructed exactly. This approach leverages classical
approximation theory to support ReLU neural networks, but in the practical setting
of data-driven learning, these methods are often less efficient than methods involving
training. Both of these two approaches to approximating PDE solution maps require
a choice of discretization to approximate an infinite-dimensional operator.
Operatorlearningisabranchofmachinelearningthataimstoapproximatemaps
between function spaces, which include solution maps defined by partial differential
equations (PDEs) [12]. Several operator learning architectures exist, including Deep-
ONet [18], Fourier Neural Operators (FNO) [16], PCA-Net [2], and random features
models [20]. Our paper focuses on FNOs, which directly parameterize the model in
Fourierspaceandallowforchangesindiscretizationinboththeinputandtheoutput
functions, potentiallyallowingfornon-uniformgrids[15]. Inaddition,FNOtakesad-
vantage of the computational speedup of the FFT to gain additional model capacity
with less evaluation time.
Error analysis for operator learning begins with establishing UA: results which
guaranteethat,foraclassofpossiblemaps,aparticularchoiceofmodelarchitecture,
and a desired maximum error, there exists a parameterization of the model that
gives at most that error. UA results are established for a variety of architectures
including ReLU NN in [6], DeepONet in [14], FNO in [11], and a general class of
neuraloperatorsin[12]. FollowingUA,modelsizeboundsgiveaworst-caseboundon
the model parameter sizes required to achieve a certain error threshold for particular
classes of problems. These have been established for FNO [11, 13], but the analysis
considers the states of the model to be functions on the continuum and ignores the
practical requirement of working with a discretized version of the function. In this
work, we focus on this source of error.
Perhapsthemostconceptuallysimilarworktooursis[1],whichaddressesthefact
that discretizations of neural operators deviate from their continuum counterparts.
The authors of [1] introduce an “alias-free” neural operator that bypasses inconsis-
tencies resulting from discretization. In practice, this research direction has led to
operator learning frameworks such as Convolutional Neural Operators (CNO) [23],
which are not strictly alias-free, but reduce aliasing errors via spatial upsampling.
These prior works have empirically shown the benefits and importance of carefully
controlling discretization errors in operator learning.
FNOsremainawidespreadneuraloperatorarchitecture,andatheoreticalanalysis
of errors resulting from numerical discretization have so far been missing from the
literature. To fill this gap, in this paper we bound the discretization error of FNOs
theoretically and perform experiments that provide greater insight into the behavior
of this error.
2. Set-Up. In this section, we establish notation for the paper (Subsection 2.1)
and define the FNO (Subsection 2.2).
2.1. Notation. Fix integer m. Let |·| denote the Euclidean norm on Rm and
∥ · ∥ the L2(Td,Rm) norm. Here, Td denotes the d-dimensional torus, which we
identify with [0,1]d with periodic boundary conditions; we simply write L2(Td) when
no confusion will arise. We define the Sobolev space Hs(Td)=Hs(Td,Rm) as
(cid:40) (cid:41)
(cid:12) (cid:88)
(2.1) Hs(Td)= f :Td →Rm (cid:12) (1+|k|2s)|f(cid:98)(k)|2 <∞
(cid:12)
k∈Zd
This manuscript is for review purposes only.4 S.LANTHALER,A.STUART,ANDM.TRAUTNER
where f(cid:98)denotes the Fourier transform of f. We denote by ∥·∥
∞
the L∞(Td) =
L∞(Td,Rm) norm on Td. Define the semi-norm
(cid:90)
(2.2) |v|2 := v(−∆)sv dx
s
Td
for functions v :Td →Rm. It is useful to consider the following equivalent definition
of the space Hs(Td) for integer s>d/2 in terms of this seminorm:
(2.3) Hs(Td)={f :Td →Rm |∥f∥ <∞}
Hs
(2.4) ∥f∥
=(cid:0) (2π)−2s|f|2+∥f∥2(cid:1)1/2
Hs s
Note that ∥f∥2
Hs
=(cid:80) k∈Zd(1+|k|2s)|f(cid:98)(k)|2, with this definition. We say an element
f ∈Hs− if f ∈Hs−ϵ for any ϵ>0. Further, let X(N) denote the set 1[N]d where
N
[N]d :={n∈Zd |n <N, i∈{1,...,d}}.
≥0 i
We also introduce the following (symmetric) index set for the Fourier coefficients:
[[N]]d =[[N]]×···×[[N]], where
(cid:40)
{−K,...,K}, (N =2K+1 is odd),
[[N]]:=
{−K,...,K−1}, (N =2K is even).
We note that, irrespective of whether N is odd or even, [[N]]d contains Nd elements.
For functions u : Td → Rm, we abuse notation slightly and use ∥u∥ to
ℓ2(n∈[N]d)
indicate the quantity,
 1/2
(cid:88)
∥u∥ ℓ2(n∈[N]d) := |u(x n)|2  .
n∈[N]d
This is a norm for the vector found by evaluating u at grid points. Note that for
x = 1n where n ∈ [N]d, it holds that x ∈ Td, and if u ∈ L2(Td) is Riemann
n N n
integrable,
1
(2.5) lim ∥u∥ =∥u∥ .
N→∞Nd/2 ℓ2(n∈[N]d) L2(Td)
2.2. FNO Definition. The FNO is a composition of layers, where the first and
final layers are lifting and projection maps, and the internal layers are an activation
function acting on the sum of an affine term, a nonlocal integral term, and a bias
term. The details are contained in the following definition.1
Definition 2.1 (Fourier Neural Operator). Let A and U be two Banach spa-
ces of real vector-valued functions over domain Td. Assume input functions a ∈ A
are Rda-valued while the output functions u∈U are Rdu-valued. The neural operator
architecture G :A→U is
θ
G =Q◦L ◦···◦L ◦P,
θ T−1 0
v =L v =σ (W v +K v +b ), t=0,1,...,T −1
t+1 t t t t t t t t
1WeremarkthatthisconstitutesthestandarddefinitionoftheFNOwiththeexceptionthatwe
askforsmoothactivationfunctions.
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 5
withv
0
=P(a),u=Q(v T)andG θ(a)=u. Here,P :Rda →Rd0 isalocalliftingmap,
Q : RdT → Rdu is a local projection map and the σ
t
are fixed nonlinear activation
functions acting locally as maps Rdt+1 → Rdt+1 in each layer (with all of P, Q and
the σ viewed as operators acting pointwise, or pointwise almost everywhere, over the
t
domain Td), W
t
∈ Rdt+1×dt are matrices, K
t
: {v
t
: Td → Rdt} → {v
t+1
: Td →
Rdt+1} are integral kernel operators and b
t
: Td → Rdt+1 are bias functions. The
activation functions σ are restricted to the set of globally Lipschitz, non-polynomial,
t
C∞ functions. The integral kernel operators K are parameterized in the Fourier
√ t
domain in the following manner. Let i= −1 denote the imaginary unit. Then, for
each t, the kernel operator K is parameterized by
t
(2.6)
   
(K tv t)(x)= (cid:88) (cid:88)dt (P t(k)) ℓj⟨e2πi⟨k,x⟩,(v t) j⟩ L2(Td;C)e2πi⟨k,x⟩(x) ∈Rdt+1.
 
k∈[[K]]d j=1
ℓ∈[dt+1]
Here, each P(k) ∈ Cdt+1×dt constitutes the learnable parameters of the integral oper-
t
ator, and K ∈ Z+ is a mode truncation parameter. We denote by θ the collection
of parameters that specify G , which include the weights W , biases b , kernel weights
θ t t
P(k), and the parameters describing the lifting and projection maps P and Q, which
t
are usually multilayer perceptrons or affine transformations.
In the error analysis in the following section, we are interested in the discrepancy
betweentakingtheinnerproductinequation(2.6)onagridinsteadofonacontinuum
– the errors due to aliasing. We consider the other parameters, including the mode
countK,tobefixedandintrinsictotheFNOmodelconsidered,irrespectiveofwhich
grid it is approximated on.
3. Theoretical Results. In this section, we state the main theoretical result,
Theorem 3.2, concerning the error that arises from taking convolutions on a discrete
grid instead of on the continuum and is then propagated through the network. We
show that the approximate L2 norm of the error after any number of layers decreases
like N−s, where s describes the regularity of the input.
Specifically, we bound the error that occurs when the kernel operator K in Defi-
t
nition2.1actsonafunctionvN thatisonlydefinedpointwiseonX(N), thesetofNd
t
uniform gridpoints on Td, rather than at every point x ∈ Td. Fixing the parameters
θ of the FNO G as well as K and input a, we denote by v the ground truth value of
θ t
theFNOstateatlayert, i.e. thestateproducedbytreatingallv asfunctionsonthe
t
whole domain Td and only afterwards evaluating on the grid. In practice, the layer
functions must be discretized, and we denote by vN the value of the FNO state at
t
layer t produced via discretization on Nd gridpoints. With these definitions, vN =v
0 0
on X(N), but it does not necessarily hold that vN = v on X(N) for t > 0. Within
t t
a single layer, we define the following quantities to track the error origin and propa-
gation, noting that, for values of m
t
that will vary with layer t, E t(j) : X(N) → Rmt,
j =0,3 and E(j) :[[K]]d →Cmt, j =1,2.
t
0. E(0)(x )=vN(x )−v (x ), x ∈X(N).
t n t n t n n
1. E t(1)(k)= N1
d
(cid:80) n∈[N]dv t(x n)e−2πi⟨k,xn⟩−(cid:82) Tdv t(x)e−2πi⟨k,x⟩ dx, k ∈[[K]]d.
2. E t(2)(k)= N1
d
(cid:80) n∈[N]dE t(0)(x n)e−2πi⟨k,xn⟩, k ∈[[K]]d.
This manuscript is for review purposes only.6 S.LANTHALER,A.STUART,ANDM.TRAUTNER
3. E t(3)(x n)=(cid:80) k∈[[K]]dP t(k)(cid:0) E(1)(k)+E(2)(k)(cid:1) e2πi⟨k,xn⟩, x
n
∈X(N).
(cid:16) (cid:17)
4. E(0)(x )=σ W v (x )+K v (x )+b +W E(0)(x )+E(3)(x )
t+1 n t t n t t n t t t n t n
−σ(W v (x )+K v (x )+b ), x ∈X(N).
t t n t t n t n
Here, E(0) is the initial error in the inputs to FNO layer t, E(1) is the aliasing error,
t
E(2) is the initial error E(0) after the discrete Fourier transform, and E(3) is the error
t t t
aftertheoperationofthekernelK . Finally,theinitialerrorforthenextlayerisgiven
t
byE(0) intermsoftheerrorquantitiesofthepreviouslayer. Intuitively,thequantity
t+1
E(1) is the source of the error within each layer since it depends only on the ground
truth v . All other error quantities are propagation of existing error. A derivation of
t
this breakdown may be found in Appendix B.
To prove Theorem 3.2, we make use of the following set of assumptions.
Assumptions 3.1. We make the following assumptions on the model parameters
and activation functions for a fixed FNO with T layers:
(A1) All σ possess continuous derivatives up to order s which are bounded by
t
B ≥1, and σ is defined to be max{max σ (0),1}.
0 0≤t≤T t
(A2) v ∈Hs(Td).
0
(A3) K < N.
2
(A4) s> d.
2
(A5) FNO parameters P , W , and b are each bounded above by M ≥ 1 in the
t t t
(cid:16) (cid:17)1/2
following norms: ∥P ∥ := (cid:80) ∥P(k)∥2 ≤ M, ∥W ∥ ≤ M, and
t F k∈[[K]]d t F t 2
|b |≤M for all t∈[0,...,T −1], where ∥·∥ is the induced matrix 2-norm,
t 2
and ∥·∥ denotes the Frobenius norm.
F
(A6) v (x )=vN(x ) for all x ∈X(n).
0 n 0 n n
(A7) N >1.
The main result is the following theorem concerning the behavior of the error
with respect to the size of the discretization. To interpret the theorem statement in
terms of norm-scaling on the left-hand side, recall (2.5).
Theorem 3.2. Under Assumptions 3.1,
1
(3.1) ∥v −vN∥ ≤CN−s
Nd/2 t t ℓ2(n∈[N]d)
where the constant C depends on B,M,d,s,t, and v .
0
The exact form of the constant C in the above theorem is detailed in Appendix
E along with the proof.
Remark 3.3. A trivial consequence of the above theorem is that under Assump-
tions 3.1,
1
(3.2) lim ∥E(0)∥ =0.
N→∞Nd/2 t ℓ2(n∈[N]d)
Indeed,astrongerresultholdsthatthediscreteℓ∞ normconvergesatarateN−s+d/2
by a straightforward inverse inequality.
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 7
We can also state the following variant of Theorem 3.2, which shows that the
same convergence rate is obtained at the continuous level, when vN(x ) is replaced
t n
by a trigonometric polynomial interpolant:
Theorem 3.4. Let pN(x)=(cid:80) DFT(vN)(k)e2πi⟨k,x⟩ denote the interpolat-
t k∈[[N]]d t
ing trigonometric polynomial of {vN(x )} . Under Assumptions 3.1, the follow-
t n n∈[N]d
ing bound holds:
(3.3) ∥v −pN∥ ≤C′N−s.
t t L2(Td)
Here, C′ depends on B,M,d,s,t, and v .
0
The exact form of the constant C′ may be found in the proof in Appendix F.
TheproofofTheorem3.2dependsonafewkeylemmas. Thefirstlemmaboundsthe
error resulting from a single FNO layer in terms of the initial error and ground truth
state, and the proof may be found in Appendix C.
Lemma 3.5. Under Assumptions 3.1, the following bound holds:
(cid:18) (cid:19)
1 2
(3.4) ∥E(0)∥ ≤BM ∥E(0)∥ +α N−s∥v ∥
Nd/2 t+1 ℓ2(n∈[N]d) Nd/2 t ℓ2(n∈[N]d) d,s t Hs
where α is a constant dependent only on d and s.
d,s
Theresultoftheprecedinglemmacanbeusedinastraightforwardwaytobound
the error after L layers; this is the content of the following corollary:
Corollary 3.6. Under Assumptions 3.1 and letting E(0)(x ) ≡ 0, we have the
0 n
following bound on the error after L layers of FNO for L≥1:
(cid:32)L−1 (cid:33)
1 ∥E(0)∥ ≤c (cid:88) CL−t−1∥v ∥ N−s
Nd/2 L ℓ2(n∈[N]d) d,s 2 t Hs
t=0
where c :=2BMα and C =BMc.
d,s d,s 2
Proof. The result follows from applying discrete Gronwall’s inequality to bound
in Lemma 3.5.
After the result of Corollary 3.6, the remaining step in the proof of Theorem 3.2
is to provide a bound on the Sobolev norm of the ground truth state ∥v ∥ at each
t Hs
layer. The following lemma accomplishes this for a single layer. The proof may be
found in Appendix D.
Lemma 3.7. Under Assumptions 3.1, the following bounds hold:
• ∥v ∥ ≤σ +BM(1+∥v ∥ +Kd/2∥v ∥ ).
t+1 ∞ 0 t ∞ t L2(Td)
• |v | ≤BcMsKds/2(1+∥v ∥ )s(1+|v | )
t+1 s t ∞ t s
for some constant c dependent on d and s.
TheproofofTheorem3.2isgiveninAppendixEbybuildingontheabovelemmas.
The derivation of Theorem 3.4 is a consequence of Theorem 3.2, which is detailed in
Appendix F, and builds on general results on trigonometric interpolation reviewed in
Appendix A.
The result of Theorem 3.2 guarantees that the discretization error converges as
gridresolutionincreases. ThealgebraicdecayrateinadiscreteL2 normisdetermined
by the regularity of the input; this in turn builds on Lemma 3.7 which ensures that
the regularity of the state is preserved through each layer of the FNO.
This manuscript is for review purposes only.8 S.LANTHALER,A.STUART,ANDM.TRAUTNER
4. Numerical Experiments. In this section we present and discuss results
from numerical experiments that empirically validate the theory of error in Fourier
Neural Operators resulting from discretization. In particular, we validate the results
ofTheorem3.2thattheL2 errorateachlayerdecreaseslikeN−s wheresgovernsthe
inputregularityandN isthediscretizationusedtoperformconvolutionsintheFNO.
For each FNO model in this section, we use a computation of a discrete FNO on a
high resolution grid as the ground truth. We compare states at each layer resulting
from inputs of lower resolution with the state resulting from the ground truth. To
obtain evaluations of v at higher discretizations than N, the inverse Fourier trans-
ℓ
formoperationisinterpolatedtoadditionalgridpointsusingtrigonometricpolynomial
interpolation; Theorem 3.4 states that the same N−s rate is achieved when the inter-
polant is compared to the truth in lieu of the coarser-grained state.
We perform experiments for inputs of varying regularity by generating Gaussian
random field (GRF) inputs with prescribed smoothness Hs− for s ∈ {0.5,1,1.5,2}.
The GRF inputs are discretized for values of N ∈ {32,64,128,256,512,1024,2048}
where the 2-dimensional grid is N ×N. Grid size 2048 is used as the ground truth,
and the relative error at layer ℓ for v compared with the truth v† is computed with
ℓ ℓ
∥v†−v ∥
(4.1) Relative Error = ℓ ℓ ℓ2(n∈[2048]d) .
∥v†∥
ℓ ℓ2(n∈[2048]d)
Finally, in FNO training, it is common practice to append positional information
about the domain at each evaluation point in the form of Euclidean grid points; i.e.
(x ,x ) ∈ [0,1]2 for two dimensions. However, this grid information is not periodic,
1 2
and an alternative is to append periodic grid information; i.e.
(sin(x ),cos(x ),sin(x ),cos(x )) for two dimensions. In these experiments, we also
1 1 2 2
compare the error of models with these two different positional encodings.
In Subsection 4.1 we discuss experiments on FNOs with random weights, and in
Subsection4.2wediscussexperimentsontrainedFNOs. InSubsection4.3,wediscuss
some guidelines for avoiding the effects of discretization error in practice. Finally, in
Subsection 4.4, we propose an application of discretization subsampling to speed up
operatorlearningtrainingbyleveragingadaptivegridsizes. Allcodeforthenumerical
experiments in this section may be found at
https://github.com/mtrautner/BoundFNO.
4.1. Experiments with Random Weights. In this subsection, we consider
five different FNOs with random weights and study their discretization error and
model stability with respect to perturbations of the inputs. All models are defined in
spatial dimension d=2, with K =12 modes in each dimension, a width of 64, and 5
layers.
The default model has randomly initialized iid U(−√1 ,√1 ) weights (uniformly
dt dt
distributed)fortheaffineandbiasterms,whered isthelayerwidth,andiidU(0, 1 )
t d2
t
spectral weights.
Initializing the weights this way is the standard default for FNO. The second
model has the same initialization, but every weight is then multiplied by a factor of
10. The third model has weights all set equal to 1. All three of these models use
the GeLU activation function standard in FNO. The fourth model has the default
initialization but uses ReLU activation instead of GeLU. Finally, the fifth model uses
the default weight initialization with appended non-periodic positional encoding.
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 9
s = 0.5, Average Slope = -0.61 s = 1.0, Average Slope = -1.05 s = 1.5, Average Slope = -1.52 s = 2.0, Average Slope = -2.00
101 101 101 101 L La ay ye er
r
1
2
102 102 102 102 Layer 3
Layer 4
103 103 103 103 Layer 5
104 Layer 1 104 104 104
Layer 2
105 Layer 3 105 105 105
106 Layer 4 106 106 106
Layer 5
107
64 128 256 512
1024107
64 128 256 512
1024107
64 128 256 512
1024107
64 128 256 512 1024
N N N N
Fig. 1: Relative error versus N and s for an FNO with default weight initialization.
s = 0.5, Average Slope = -0.60 s = 1.0, Average Slope = -1.05 s = 1.5, Average Slope = -1.53 s = 2.0, Average Slope = -1.80
Layer 1
101 101 101 101 Layer 2
Layer 3
102 102 102 102 Layer 4
Layer 5
103 103 103 103
Layer 1
104 L La ay ye er
r
2
3
104 104 104
Layer 4
105 Layer 5 105 105 105
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig. 2: Relative error versus N and s for an FNO with default ×10 initial weights.
4.1.1. DiscretizationError. Therelativeerrorofthestateateachlayerversus
the discretization for inputs of varying regularity may be seen for each of the five
models in Figures 1, 2, 3, 4, and 5 respectively. In these figures, from left to right,
s ∈ {0.5,1,1.5,2} where v ∈ Hs−. The uncertainty shading indicates two standard
0
deviations from the mean over five inputs to the FNO.
As can be seen in Figure 1 for the model with the default weight initialization,
theempiricalbehavioroftheerrormatchesthebehaviorexpectedfromTheorem3.2.
One question that arises from Figure 1 is why the error decreases as the number of
layers increases; this is an effect of the magnitude of the weights. When the model
weights are multiplied by 10, then the error begins to increase with the number of
layers, as can be seen in Figure 2. This phenomenon is also showcased in Figure 9a
wherethestatenormremainsthesameorderofmagnitudethroughthelayersforthe
default model but increases exponentially for the other three model initializations.
While the model behavior in the first two figures follows the theory, when all the
weights are set equal to 1 the behavior is more erratic; this can be seen in Figure 3.
TheerrordecreasesfasterthanexpectedandwithlessconsistencythantheGaussian
weight models, and the decay rate increases with each layer. In this sense, the all-
ones model has a smoothing effect on the state at each layer. We note that this
generally occurs with any initialization that sets the spectral weights on the same
order of magnitude as the affine weights; for instance, the same super-convergence
effect occurs when all weights are initialized U(0,1).
The results shown in Figure 4 justify the use of the GeLU activation function,
whichbelongstoC∞,overtheReLUactivationfunction,whichisonlyLipschitz. The
figure shows that the benefit of having sufficiently smooth inputs is negated by the
ReLUactivation: theerrordecayislimited. Notethatthiseffectdoesnotoccurforthe
first layer since at that point ReLU has been applied once, and the Fourier transform
isnotappliedtotheoutputofanactivationfunctionuntilthesecondlayer. Sincethe
ReLU activation function has regularity of s = 1.5, no improvement in convergence
This manuscript is for review purposes only.
rorrE
evitaleR
rorrE
evitaleR10 S.LANTHALER,A.STUART,ANDM.TRAUTNER
s = 0.5, Average Slope = -0.91 s = 1.0, Average Slope = -1.32 s = 1.5, Average Slope = -1.74 s = 2.0, Average Slope = -2.07
101 101 101 101 Layer 1
Layer 2
102 102 102 102 Layer 3
Layer 4
103 103 103 103 Layer 5
104 Layer 1 104 104 104
Layer 2
105 Layer 3 105 105 105
Layer 4
106 Layer 5 106 106 106
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig. 3: Relative error versus N and s for an FNO with all weights equals to 1.
rate is observed when the inputs have higher regularity than this. Additionally, in
the default model with ×10 magnitude weights in Figure 2, the large weights mean
thattheGeLUactivationactslikeaReLUactivationforsmallerdiscretizations. This
phenomenon is apparent for inputs with regularity s = 2, where the first layer has
theappropriateslope, buttheotherlayersonlybegintoapproachthatrateathigher
discretizations. Earlier layers achieve this rate first because of the smaller magnitude
state norm in earlier layers for this model.
A similar effect to the ReLU model occurs when positional encoding information
is appended to the input; see Figure 5. Since this grid data has a jump discontinuity
across the boundary of [0,1]d, it has regularity of s = 0.5, and the convergence rate
is thus impacted.
4.2. Experiments with Trained Networks. In this subsection, we consider
twodifferentmapsandtrainFNOsondatafromeachmap. Thenweperformthesame
discretization error analysis as in Subsection 4.1. The first map is a PDE solution
mapintwodimensionswhosesolutionisatleastasregularastheinputfunction. The
s = 0.5, Average Slope = -0.59 s = 1.0, Average Slope = -1.05 s = 1.5, Average Slope = -1.52 s = 2.0, Average Slope = -1.64
101 101 101 101 L La ay ye er
r
1
2
Layer 3
102 102 102 102 Layer 4
Layer 5
103 103 103 103
Layer 1
104 Layer 2 104 104 104
Layer 3
105 L La ay ye er
r
4
5
105 105 105
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig. 4: Relative error versus N and s for a default FNO with a ReLU activation.
s = 0.5, Average Slope = -0.61 s = 1.0, Average Slope = -0.81 s = 1.5, Average Slope = -0.55 s = 2.0, Average Slope = -0.54
Layer 1
101 101 101 101 Layer 2
Layer 3
Layer 4
102 102 102 102 Layer 5
Layer 1
103 L La ay ye er
r
2
3
103 103 103
Layer 4
104 Layer 5 104 104 104
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig. 5: Relative error versus N and s for a default FNO with non-periodic position
encoding appended to the input.
This manuscript is for review purposes only.
rorrE
evitaleR
rorrE
evitaleR
rorrE
evitaleRDISCRETIZATIONERROROFFOURIERNEURALOPERATORS 11
Input Output Predicted Output Input Output Predicted Output
(a) Data for the PDE Solution FNO. (b) Data for the Gradient FNO.
Fig. 6: Visualization of the input and output data for the trained model examples.
s = 0.5, Average Slope = -0.84 s = 1.0, Average Slope = -1.23 s = 1.5, Average Slope = -1.68 s = 2.0, Average Slope = -2.12
101 101 101 101 Layer 1
Layer 2
102 102 102 102 Layer 3
103 103 103 103
104 104 104 104
105 Layer 1 105 105 105
Layer 2
106 Layer 3 106 106 106
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig.7: ErrorversusdiscretizationforinputsofvaryingregularityfortheFNOtrained
on data corresponding to a PDE solution.
second map is a simple gradient, but in this setting the output data of the gradient
is, of course, less regular by one Sobolev smoothness exponent than that of the input
function. In both experiments, periodic positional encoding information is appended
to the inputs.
4.2.1. Discretization Error for PDE Solution Model. In this example, we
train an FNO to approximate the solution map to the following PDE:
(4.2) ∇·(∇χA)=∇·A, y ∈T2
(cid:90)
(4.3) χ is 1−periodic, χdy =0.
T2
Here, the input A : T2 (cid:55)→ R2×2 is symmetric positive definite at every point in
the domain T2 and is bounded and coercive. For the output data we take the first
component of χ:T2 (cid:55)→R2. In our experiments the model is trained to <5% relative
L2 test error. A visualization of the data is in Figure 6a.
The error versus discretization analysis can be seen in Figure 7. The error de-
creases slightly faster than predicted by the theory; a potential explanation is that
the trained model itself has a smoothing effect that is not exploited in our analysis.
4.2.2. Discretization Error for a Gradient Map. In the final example, we
train an FNO to approximate a simple gradient map
(4.4) u(cid:55)→∇u.
ThetrainingdataconsistsofiidGaussianrandomfieldinputswithregularitys=
2. Since a gradient reduces regularity, we expect the model outputs to approximate
functions with regularity s = 1, which is at odds with the smoothness-preserving
properties of the FNO described by theory.
The error versus discretization for inputs of various smoothness is shown in Fig-
ure 8. The error decreases according the the smoothness of the input despite the
This manuscript is for review purposes only.
rorrE
evitaleR12 S.LANTHALER,A.STUART,ANDM.TRAUTNER
s = 0.5, Average Slope = -0.81 s = 1.0, Average Slope = -1.25 s = 1.5, Average Slope = -1.71 s = 2.0, Average Slope = -2.17
Layer 1
101 101 101 101 Layer 2
Layer 3
102 102 102 102
103 103 103 103
104
Layer 1
104 104 104
Layer 2
105 Layer 3 105 105 105
64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024 64 128 256 512 1024
N N N N
Fig.8: ErrorversusdiscretizationforinputsofvaryingregularityfortheFNOtrained
on data corresponding to a gradient map.
smoothness-decreasing properties of the data. Indeed, the model does produce more
regular predicted outputs than the true gradient, as can be seen in Figure 6b where
the predicted output is visibly smoother than the true output.
4.3. Avoiding Discretization Error. The discretization error analyses per-
formed in this section can be done on any FNO, trained or untrained. In practice,
for a particular model size and input data regularity, these experiments may be done
to calibrate which discretization level to use to achieve relative error from discretiza-
tion at, or below, the order of magnitude of the desired test error of the model.
Furthermore, to increase accuracy with discretization, the theory and experiments
promotetheuseofperiodicpositionalencodingsinsteadofnon-periodicencodingsas
well as the use of GeLU activation instead of ReLU. Finally, an additional potential
application is adaptive subsampling, as described in the next subsection.
Weights 0.175 Original
1012 All 1s Subsampling
Uniform [0,1] 0.150
1010 10x Default 0.125 Default
108 0.100
0.075
106
0.050
104
0.025
102 1 2 3 4 5 0.000 0 50 100 150 200 250
Layer Minutes
(a) State norm versus layer for (b) Adaptive grid refinement
various untrained model initial- leads to greater training effi-
izations. ciency.
Fig. 9
4.4. Speeding Up Training via Adaptive Subsampling. The fact that the
FNOarchitectureanditsparametrizationareindependentofthenumericaldiscretiza-
tion allows for increased flexibility. Specifically, it is possible to adaptively choose an
optimal discretization for a given objective. We close this section by exploring one
such possibility with the aim of optimizing computational time during training.
The overall approximation error of the FNO can be split into a contribution due
to the numerical discretization and another contribution due to model discrepancy,
Ψ†−ΨN = (cid:2) Ψ†−Ψ (cid:3) +(cid:2) Ψ −ΨN (cid:3) .
FNO FNO FNO FNO
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
modeldiscrepancy discretizationerror
This manuscript is for review purposes only.
rorrE
evitaleR
mroN
etatS
rorrE
tseTDISCRETIZATIONERROROFFOURIERNEURALOPERATORS 13
Here, Ψ† is the ground truth operator, ΨN represents the discretized FNO with
FNO
gridsizeN,andΨ representsthecontinuousFNOintheabsenceofdiscretization
FNO
errors. The basic idea of our proposed approach is that, during training, it is not
necessary to compute model outputs to a numerical accuracy that is substantially
betterthanthemodeldiscrepancy. Thissuggestsanadaptivechoiceofthenumerical
discretization, where we employ a coarser grid during the early phase of training
and refine the grid in later stages. In practice, we realize this idea by introducing a
subsampling scheduler. The subsampling scheduler tracks a validation error on held
out data, and adaptively changes the numerical resolution via suitable subsampling
of the training data. Starting from a coarse resolution, we iteratively double the grid
size once the validation error plateaus.
WetrainFNOfortheellipticPDE(4.2)withandwithoutthesubsamplingsched-
uler. Our model has 4 hidden layers, channel width 64 and Fourier cut-off 12. Our
results are based on 9000 training samples and 500 test samples. For training with
a subsampling scheduler, we include an additional 500 samples for validation. Com-
pared to training without subsampling, training with a subsampling scheduler there-
fore requires the same number of forward and backward passes over the network for
the training and test set, plus an additional overhead due to the validation set. Since
we are mainly interested in the training time, our choice of adding validation sam-
ples, rather than performing a training/validation split of the 9000 original samples,
ensures that computational timings are not skewed in favor of subsampling. Over
the course of training, we iterate through the following grid sizes: 32x32, 64x64, and
128x128. Our criterion for a plateau is that the validation error has not improved for
40 training epochs. Models are trained for 300 epochs on an Nvidida P100 GPU.
The results of training with and without subsampling scheduler for the PDE
solution model (4.2) are shown in Figure 9b. We observe that training time can
be substantially reduced with subsampling. This points to the potential benefits of
developingadaptivenumericalmethodsformodelevaluationwithinoperatorlearning.
5. Conclusions. In this paper, we analyze the error that results from Fourier
Neural Operators (FNOs) when implemented on a grid rather than on a continuum.
We bound the L2 norm of the error in Theorem 3.2, proving an upper bound that
decreasesasymptoticallyasN−s,whereN isthediscretizationineachdimension,and
sistheinputregularity. WeshowempiricallythatFNOswithrandomweightschosen
asthedefaultFNOweightsfortrainingbehavealmostexactlyasthetheorypredicts.
Furthermore, our theory and experiments justify the use of the GeLU activation
function in FNO over ReLU, as the former preserves regularity. Additional analyses
ontrainedmodelsshowthattheerrorbehaveslesspredictablyinrelationtoourtheory
in the low-discretization regime. Finally, we provide basic guidelines for mitigating
againstdiscretizationerrorinpracticalsettingsandproposeanadaptivesubsampling
algorithm for decreasing training time with operator learning. As FNOs become a
morecommontoolinscientificmachinelearning,understandingthevarioussourcesof
erroriscritical. ByboundingFNOdiscretizationerroranddemonstratingitsbehavior
in numerical experiments, we understand its effect on learning and the potential to
minimize computational costs by an adaptive choice of numerical resolution.
Acknowledgments. TheauthorsaregratefultoNicholasNelsenforhelpfuldis-
cussions on FNO implementation. The computations presented here were conducted
in the Resnick High Performance Computing Center, a facility supported by Resnick
Sustainability Institute at the California Institute of Technology.
This manuscript is for review purposes only.14 S.LANTHALER,A.STUART,ANDM.TRAUTNER
REFERENCES
[1] F. Bartolucci, E. de Be´zenac, B. Raonic´, R. Molinaro, S. Mishra, and R. Alaifari,
Areneuraloperatorsreallyneuraloperators? frametheorymeetsoperatorlearning,arXiv
preprintarXiv:2305.19913,(2023).
[2] K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart, Model reduction and
neural networks for parametric pdes,TheSMAIjournalofcomputationalmathematics,7
(2021),pp.121–157.
[3] K. Bhattacharya, N. Kovachki, A. Rajan, A. M. Stuart, and M. Trautner, Learning
homogenization for elliptic operators,SIAMJournalonNumericalAnalysis,(2024).
[4] S.Cai,Z.Mao,Z.Wang,M.Yin,andG.E.Karniadakis,Physics-informedneuralnetworks
(pinns) for fluid mechanics: A review,ActaMechanicaSinica,37(2021),pp.1727–1738.
[5] S.Cuomo,V.S.DiCola,F.Giampaolo,G.Rozza,M.Raissi,andF.Piccialli,Scientific
machine learning through physics–informed neural networks: Where we are and what’s
next,JournalofScientificComputing,92(2022),p.88.
[6] G.Cybenko,Approximationbysuperpositionsofasigmoidalfunction,Mathematicsofcontrol,
signalsandsystems,2(1989),pp.303–314.
[7] D. Hendrycks and K. Gimpel, Gaussian error linear units (gelus), arXiv preprint
arXiv:1606.08415,(2016).
[8] L. Herrmann, J. A. Opschoor, and C. Schwab, Constructive deep relu neural network
approximation,JournalofScientificComputing,90(2022),p.75.
[9] L.Herrmann,C.Schwab,andJ.Zech,Deep relu neural network expression rates for data-
to-qoi maps in bayesian pde inversion,SAMRes.Rep,(2020).
[10] G.E.Karniadakis,I.G.Kevrekidis,L.Lu,P.Perdikaris,S.Wang,andL.Yang,Physics-
informed machine learning,NatureReviewsPhysics,3(2021),pp.422–440.
[11] N. Kovachki, S. Lanthaler, and S. Mishra, On universal approximation and error bounds
for fourier neural operators,JournalofMachineLearningResearch,22(2021),pp.1–76.
[12] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and
A. Anandkumar, Neural operator: Learning maps between function spaces with appli-
cations to pdes,JournalofMachineLearningResearch,24(2023),pp.1–97.
[13] N. B. Kovachki, S. Lanthaler, and A. M. Stuart, Operator learning: Algorithms and
analysis,arXivpreprintarXiv:2402.15715,(2024).
[14] S. Lanthaler, S. Mishra, and G. E. Karniadakis, Error estimates for deeponets: A deep
learning framework in infinite dimensions,TransactionsofMathematicsandItsApplica-
tions,6(2022),p.tnac001.
[15] Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar, Fourier neural operator with learned
deformations for pdes on general geometries, Journal of Machine Learning Research, 24
(2023),pp.1–26.
[16] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and
A. Anandkumar, Fourier neural operator for parametric partial differential equations,
InternationalConferenceonLearningRepresentations,(2021).
[17] M. Longo, J. A. Opschoor, N. Disch, C. Schwab, and J. Zech, De rham compatible deep
neural network fem,NeuralNetworks,165(2023),pp.721–739.
[18] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear operators
via deeponet based on the universal approximation theorem of operators, Naturemachine
intelligence,3(2021),pp.218–229.
[19] J.Moser,Arapidlyconvergentiterationmethodandnon-linearpartialdifferentialequations-
i,AnnalidellaScuolaNormaleSuperiorediPisa-ScienzeFisicheeMatematiche,20(1966),
pp.265–315.
[20] N. H. Nelsen and A. M. Stuart, The random feature model for input-output maps between
banach spaces,SIAMJournalonScientificComputing,43(2021),pp.A3212–A3243.
[21] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani,
T. Kurth, D. Hall, Z. Li, K. Azizzadenesheli, et al., Fourcastnet: A global data-
driven high-resolution weather model using adaptive fourier neural operators, arXiv pre-
printarXiv:2202.11214,(2022).
[22] M.Raissi,P.Perdikaris,andG.E.Karniadakis,Physics-informedneuralnetworks: Adeep
learning framework for solving forward and inverse problems involving nonlinear partial
differential equations,JournalofComputationalphysics,378(2019),pp.686–707.
[23] B.Raonic,R.Molinaro,T.DeRyck,T.Rohner,F.Bartolucci,R.Alaifari,S.Mishra,
and E. de Be´zenac, Convolutional neural operators for robust and accurate learning of
pdes,AdvancesinNeuralInformationProcessingSystems,36(2024).
[24] T. Zhou, X. Wan, D. Z. Huang, Z. Li, Z. Peng, A. Anandkumar, J. F. Brady, P. W.
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 15
Sternberg,andC.Daraio,Ai-aidedgeometricdesignofanti-infectioncatheters,Science
Advances,10(2024),p.eadj1741.
Appendices.
Appendix A. Trigonometric Interpolation and Aliasing.
In this section, we present a self-contained analysis of aliasing errors for v ∈
Hs(Td). The primary goal is to state and prove Proposition A.6, which controls the
difference between a function defined over Td and the trigonometric interpolation of
a function defined on a grid. In the following, we denote by N an integer. We recall
that X(N) is a set of equidistant grid points on the torus Td,
X(N) ={x ∈Td|x=n/N, n∈[N]d}.
n
We note that the discrete Fourier transform gives rise to a natural correspondence
between grid values and Fourier modes,
(A.1) {v(x )} ↔ {c˜ } ,
n n∈[N]d k k∈[[N]]d
where
1 (cid:88)
(A.2) c˜ = v(x )e−2πi⟨k,xn⟩ =:DFT(v)(k).
k Nd n
n∈[N]d
We begin with the following observation:
Lemma A.1. Let N be given. Then,
1 (cid:88)
(A.3) e2πi⟨k,xm−xn⟩ =δ , ∀m,n∈[N]d,
Nd mn
k∈[[N]]d
(A.4) N1
d
(cid:88) e2πi⟨k−k′,xn⟩ =δ kk′, ∀k,k′ ∈[[N]]d.
n∈[N]d
Proof. This follows from an elementary calculation, which we briefly recall here.
For d=1, the claim follows by noting that x =n/N, and using the identity
n
N (cid:88)−1 (cid:40) qN−1,
(q ̸=1),
(A.5) qℓ = q−1
N, (q =1),
ℓ=0
with q =e2πi(m−n)/N and q =e2πi(k−k′)/N, respectively. Indeed, assuming d=1 and
denoting −K :=min[[N]], then the above identity implies, for example,
N−1
(cid:88) e2πik(xm−xn) = (cid:88) (cid:2) e2πi(m−n)/N(cid:3)k = (cid:88) qk =q−K (cid:88) qℓ.
(cid:124) (cid:123)(cid:122) (cid:125)
k∈[[N]] k∈[[N]] =:q k∈[[N]] ℓ=0
If q ̸=1, then qN =e2πi(m−n) =1. By (A.5), this implies that the last sum is 0. On
the other hand, if q =1, then the last sum is trivially =N. We finally note that, for
m,n∈[N], we have q =1 if and only if m=n, implying that
N
(cid:88)
q−K qℓ =Nδ .
mn
ℓ=0
This manuscript is for review purposes only.16 S.LANTHALER,A.STUART,ANDM.TRAUTNER
Thus,
(cid:88)
e2πik(xm−xn) =Nδ ,
mn
k∈[[N]]
and (A.3) follows. The argument for (A.4) is analogous. For d > 1, the sum over
[[N]]d = [[N]]×···×[[N]] is split into sums along each dimension, and the same
argument is applied for each of the d components, yielding the claim also for d>1.
A trigonometric polynomial p:Td (cid:55)→Rm is a function of the form
(cid:88)
(A.6) p(x)= c e2πi⟨k,x⟩
k
k∈[[N]]d
with c ∈ Cm chosen to make p(x) Rm-valued at each x ∈ Td. We note that the
k
discrete and continuous L2-norms are equivalent for trigonometric polynomials:
Lemma A.2. Let N be a positive integer. If p(x) is a trigonometric polynomial,
then
1
∥p∥ =∥p∥ .
Nd/2 ℓ2(n∈[N]d) L2(Td)
Proof. We have
(cid:90) (cid:90)
∥p∥2 = |p(x)|2dx= (cid:88) c c e2πi⟨k−k′,x⟩dx= (cid:88) |c |2,
L2(Td) k k′ k
Td Td
k,k′∈[[N]]d (cid:124) (cid:123)(cid:122) (cid:125) k∈[[N]]d
=δ kk′
and
1 1 (cid:88)
∥p∥2 = |p(x )|2
Nd ℓ2(n∈[N]d) Nd n
n∈[N]d
=
(cid:88)
c kc
k′
N1
d
(cid:88) e2πi⟨k−k′,xn⟩
k,k′∈[[N]]d n∈[N]d
(cid:124) (cid:123)(cid:122) (cid:125)
=δ kk′
(cid:88)
= |c |2.
k
k∈[[N]]d
This proves the claim.
Let v : Td → R be a function with grid values {v(x )} . Let DFT(v)(k)
n n∈[N]d
denote the coefficients of the discrete Fourier transform defined by (A.1). Then
(cid:88)
(A.7) p(x):= DFT(v)(k)e2πi⟨k,x⟩,
k∈[[N]]d
is the trigonometric polynomial associated to v. The next lemma shows that p(x)
interpolates v(x).
Lemma A.3. The trigonometric polynomial p(x) defined by (A.7) interpolates
v(x) at the grid points, i.e., we have p(x )=v(x ) for all n∈[N]d.
n n
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 17
Proof. Fix n∈[N]d. Then
(cid:88)
p(x )= DFT(v)(k)e2πi⟨k,xn⟩
n
k∈[[N]]d
 
(cid:88)  1 (cid:88) 
= v(x )e−2πi⟨k,xm⟩ e2πi⟨k,xn⟩
Nd m
 
k∈[[N]]d m∈[N]d
 
(cid:88)  1 (cid:88) 
= v(x ) e2πi⟨k,xn−xm⟩
m Nd
 
m∈[N]d k∈[[N]]d
(cid:88)
= v(x )δ
m mn
m∈[N]d
=v(x ),
n
where we have made use of (A.3) to pass to the fourth line.
The following trigonometric polynomial interpolation estimate for functions in
Sobolev spaces Hs(Td) will be useful in stating our main proposition:
Lemma A.4. Let v ∈Hs(Td) for s>d/2. Let p denote the interpolating trigono-
metric polynomial given by (A.7). Then
(A.8)
 
(cid:88) (cid:88)  (cid:88) 
v(x)−p(x)= v(k)e2πi⟨k,x⟩− v(k+ℓN) e2πi⟨k,x⟩.
(cid:98) (cid:98)
 
k∈Zd\[[N]]d k∈[[N]]d ℓ∈Zd\{0}
Furthermore, there exists a constant c >0, such that
s,d
(A.9) ∥v−p∥ ≤c ∥v∥ N−s.
L2(Td) s,d Hs(Td)
Remark A.5. The first sum on the right-hand side of (A.8) is the L2-orthogonal
Fourierprojectionofvontothecomplementofspan{e2πi⟨k,x⟩|k ∈[[N]]d}. Thesecond
sum in (A.8) is an “aliasing” error; It arises because two Fourier modes are indistin-
guishable on the discrete grid whenever k−k′ ∈NZd, i.e. e2πi⟨k,xn⟩ =e2πi⟨k′,xn⟩ for
all n∈[N]d.
Proof. Since v ∈Hs(Td) has Sobolev smoothness s for s>d/2, it can be shown
that the Fourier series of v is uniformly convergent, and the following manipulations
can be rigorously justified: First, substitution of v(x n)=(cid:80) k′∈Zdv (cid:98)(k′)e2πi⟨k′,xn⟩ into
DFT(v)(k) yields
 
DFT(v)(k)=
1 (cid:88)  (cid:88) v(k′)e2πi⟨k′,xn⟩
e−2πi⟨k,xn⟩
Nd (cid:98)
 
n∈[N]d k′∈Zd
 
=
(cid:88) v(k′) 1 (cid:88) e2πi⟨k′−k,xn⟩
(cid:98) Nd
 
k′∈Zd n∈[N]d
We now note that
(cid:40)
1 (cid:88) e2πi⟨k′−k,xn⟩
=
0, (k′ ̸≡k mod N),
Nd 1, (k′ ≡k mod N),
n∈[N]d
This manuscript is for review purposes only.18 S.LANTHALER,A.STUART,ANDM.TRAUTNER
as a consequence of the trigonometric identity (A.4). Writing k′ = k+ℓN for all k′
for which the sum inside the braces does not vanish, it follows that
(cid:88)
DFT(v)(k)= v(k+ℓN).
(cid:98)
ℓ∈Zd
Thus,
(cid:88) (cid:88)
v(x)−p(x)= v(k)e2πi⟨k,x⟩− DFT(v)(k)e2πi⟨k,x⟩
(cid:98)
k∈Zd k∈[[N]]d
(cid:88) (cid:88)
= v(k)e2πi⟨k,x⟩+ {v(k)−DFT(v)(k)}e2πi⟨k,x⟩
(cid:98) (cid:98)
k∈Zd\[[N]]d k∈[[N]]d
 
(cid:88) (cid:88)  (cid:88) 
= v(k)e2πi⟨k,x⟩− v(k+ℓN) e2πi⟨k,x⟩.
(cid:98) (cid:98)
 
k∈Zd\[[N]]d k∈[[N]]d ℓ∈Zd\{0}
We proceed to bound the last two terms. For the first term, we have
(cid:13) (cid:88) (cid:13)2 (cid:88)
(cid:13) v(k)e2πi⟨k,x⟩(cid:13) = |v(k)|2
(cid:13) (cid:98) (cid:13) (cid:98)
L2(Td)
k∈Zd\[[N]]d k∈Zd\[[N]]d
1 (cid:88)
≤ (1+|k|2s)|v(k)|2
(1+(N/2)2s) (cid:98)
k∈Zd
≤4sN−2s∥v∥2 ,
Hs(Td)
where ∥v∥2 =(cid:80) (1+|k|2s)|v(k)|2, and for the second term
Hs(Td) k∈Zd (cid:98)
(cid:13) (cid:88) (cid:110) (cid:88) (cid:111) (cid:13)2
(cid:13) v(k+ℓN) e2πi⟨k,x⟩(cid:13)
(cid:13) (cid:98) (cid:13)
L2(Td)
k∈[[N]]d ℓ∈Zd\{0}
(cid:88) (cid:12) (cid:88) (cid:12)2
= (cid:12) v(k+ℓN)(cid:12)
(cid:12) (cid:98) (cid:12)
k∈[[N]]d ℓ∈Zd\{0}
(cid:88) (cid:16) (cid:88) (cid:17)
≤ (1+|k+ℓN|2s)−1
k∈[[N]]d ℓ∈Zd\{0}
(cid:16) (cid:88) (cid:17)
× (1+|k+ℓN|2s)|v(k+ℓN)|2 .
(cid:98)
ℓ∈Zd\{0}
We note that for k ∈ [[N]]d, we have |k| ≤ N/2, and hence, for any integer vector
∞
ℓ̸=0, we obtain
N N N
(A.10) |k+ℓN|≥|k+ℓN| ≥|ℓ| N −|k| ≥|ℓ| N − ≥ |ℓ| ≥ √ |ℓ|.
∞ ∞ ∞ ∞ 2 2 ∞ 2 d
We can now bound
(cid:88) (cid:88)
(cid:18)
N
(cid:19)−2s
(A.11a) (1+|k+ℓN|2s)−1 ≤ √ |ℓ|−2s
2 d
ℓ∈Zd\{0} ℓ∈Zd\{0}
(A.11b) ≤c N−2s,
d,s
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 19
where c :=(4d)s(cid:80) |ℓ|−2s <∞ is finite, since s>d/2 implies that the last
d,s ℓ∈Zd\{0}
series converges. Substitution of this bound in the estimate above implies,
(cid:13) (cid:13)2
(cid:13) (cid:88) (cid:110) (cid:88) (cid:111) (cid:13)
(cid:13) v(k+ℓN) e2πi⟨k,x⟩(cid:13)
(cid:13) (cid:98) (cid:13)
(cid:13) (cid:13)
k∈[[N]]d ℓ∈Zd\{0} L2(Td)
 
(cid:88) (cid:88)
≤c d,sN−2s  (1+|k+ℓN|2s)|v (cid:98)(k+ℓN)|2 
k∈[[N]]d ℓ∈Zd\{0}
≤c N−2s∥v∥2 .
d,s Hs(Td)
Combining the above estimates, we conclude that
∥v−p∥ ≤c ∥v∥ N−s,
L2 d,s Hs(Td)
where we have re-defined c :=2s+(4d)s/2(cid:80) |ℓ|−2s.
d,s ℓ∈Zd\{0}
We can now state the main outcome of this section:
Proposition A.6. Let v ∈Hs(Td) be given for s>d/2 and let {uN(x )}
n n∈[N]d
be any grid values. Let pN(x) = (cid:80) DFT(uN)(k)e2πi⟨k,x⟩ be the interpolating
k∈[[N]]d
trigonometric polynomial of uN. Then,
1
∥v−pN∥ ≤ ∥v−uN∥ +c ∥v∥ N−s.
L2(Td) Nd/2 ℓ2(n∈[N]d) d,s Hs(Td)
Proof. Letp(x)=(cid:80) DFT(v)(k)e2πi⟨k,x⟩betheinterpolatingtrigonometric
k∈[[N]]d
polynomial given the point-values {v(x )} . Then,
n n∈[N]d
(A.12) ∥v−pN∥ ≤∥v−p∥ +∥p−pN∥ .
L2(Td) L2(Td) L2(Td)
By Lemma A.4, we have
∥v−p∥ ≤c ∥v∥ N−s.
L2(Td) d,s Hs
By Lemma A.2, and since p(x )=v(x ), pN(x )=uN(x ) by Lemma A.3, we have
n n n n
1
∥p−pN∥ = ∥p(x )−pN(x )∥
L2(Td) Nd/2 n n ℓ2(n∈[N]d)
1
= ∥v(x )−uN(x )∥ .
Nd/2 n n ℓ2(n∈[N]d)
Substitution in (A.12) gives the claimed bound.
Appendix B. Discretization Error Derivation.
In this section, we derive the error breakdown within each FNO layer. This error
breakdown is used in the proofs of subsequent sections.
Let E(0) be the error in the inputs to FNO layer t such that
t
E(0)(x )=vN(x )−v (x ), x ∈X(N).
t n t n t n n
LetF denotetheFouriertransformandDFTasinequation(A.2). Thenfork ∈[[K]]d,
DFT(vN)(k)= 1 (cid:88) v (x )e−2πi⟨k,xn⟩+ 1 (cid:88) E(0)(x )e−2πi⟨k,xn⟩
t Nd t n Nd t n
n∈[N]d n∈[N]d
=F(v )(k)+E(1)(k)+E(2)(k)
t t t
This manuscript is for review purposes only.20 S.LANTHALER,A.STUART,ANDM.TRAUTNER
where E(1) is the error resulting from computing the Fourier transform of v on a
t t
discrete grid rather than all of Td, i.e.
E(1)(k)= 1 (cid:88) v (x )e−2πi⟨k,xn⟩−(cid:90) v (x)e−2πi⟨k,x⟩ dx
t Nd t n t
Td
n∈[N]d
and E(2) is the error E(0) after the discrete Fourier transform, i.e.
t t
E(2)(k)= 1 (cid:88) E(0)(x )e−2πi⟨k,xn⟩.
t Nd t n
n∈[N]d
For x ∈X(N), the output of the kernel integral operator is given by
n
(K vN)(x )=
(cid:88) P(k)(cid:16)
F(v
)(k)+E(1)(k)+E(2)(k)(cid:17)
e2πi⟨k,xn⟩
t t n t t t t
k∈[[K]]d
=(K v )(x )+E(3)(x )
t t n t n
where
E(3)(x )= (cid:88) P(k)(cid:16) E(1)(k)+E(2)(k)(cid:17) e2πi⟨k,xn⟩.
t n t
k∈[[K]]d
Finally, the output of layer t is given by
vN (x )=σ(cid:16) W (cid:0) v (x )+E(0)(x )(cid:1) +(K vN)(x )+b (cid:17)
t+1 n t t n t n t t n t
(cid:16) (cid:17)
=σ W v (x )+K v (x )+b +W E(0)(x )+E(3)(x ) .
t t n t t n t t t n t n
Therefore, the initial error for the next layer is given by
(cid:16) (cid:17)
E(0)(x )=σ W v (x )+K v (x )+b +W E(0)(x )+E(3)(x )
t+1 n t t n t t n t t t n t n
−σ(W v (x )+K v (x )+b ).
t t n t t n t
Appendix C. Proofs of Approximation Theory Lemmas.
The proof of Lemma 3.5 involves bounds on the error components described in
Appendix B. We bound these components in the following proposition.
Proposition C.1. Under Assumptions 3.1, it holds that
1. ∥E(1)∥ ≤α N−s∥v ∥ where α is independent of N,v ;
t ℓ2(k∈[[K]]d) d,s t Hs d,s t
2. ∥E(2)∥ =N−d/2∥E(0)∥ ;
t ℓ2(k∈[[N]]d) t ℓ2(n∈[N]d)
(cid:16) (cid:17)
3. ∥E(3)∥ ≤Nd/2∥P ∥ ∥E(1)∥ +∥E(2)∥ ;
t ℓ2(n∈[N]d) t F t ℓ2(k∈[[K]]d) t ℓ2(k∈[[K]]d)
(cid:16) (cid:17)
4. ∥E(0)∥ ≤B ∥W ∥ ∥E(0)∥ +∥E(3)∥ .
t+1 ℓ2(n∈[N]d) t 2 t ℓ2(n∈[N]d) t ℓ2(n∈[N]d)
Proof. Beginning with the definition of E(1)(k), we have
t
∥E(1)∥2 =(cid:13) (cid:13) 1 (cid:88) v (x )e−2πi⟨k,xn⟩−(cid:90) e−2πi⟨k,x⟩v (x)dx(cid:13) (cid:13)2 .
t ℓ2(k∈[[K]]d) (cid:13)Nd t n
Td
t (cid:13)
ℓ2(k∈[[K]]d)
n∈[N]d
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 21
Denote the terms in the above expression v (k) and v†(k), respectively. Since s> d,
(cid:98)t (cid:98)t 2
(cid:88)
v (x )= v†(k)e2πi⟨k,xn⟩,
t n (cid:98)t
k∈Zd
and it follows that
 
v (cid:98)t(k′)= N1
d
(cid:88) (cid:88) v (cid:98)t†(k)e2πi⟨k,xn⟩ e−2πi⟨k′,xn⟩
n∈[N]d k∈Zd
= (cid:88) v†(k) 1 (cid:88) e2πi⟨k−k′,xn⟩
(cid:98)t Nd
k∈Zd n∈[N]d
(cid:88)
= v†(k′+Nℓ).
(cid:98)t
ℓ∈Zd
Therefore,
∥E(1)∥2 =∥v −v†∥2
t ℓ2(k∈[[K]]d) (cid:98)t (cid:98)t ℓ2(k∈[[K]]d)
(cid:12) (cid:12)2
(cid:12) (cid:12)
=
(cid:88) (cid:12)
(cid:12)
(cid:12)
(cid:88)
v
(cid:98)t†(k+Nℓ)(cid:12)
(cid:12)
(cid:12)
k∈[[K]]d(cid:12)ℓ∈Zd\{0} (cid:12)
 
(cid:88) (cid:88) 1 (cid:88)
≤  |k+ℓN|2s |k+Nℓ|2s|v (cid:98)t†(k+Nℓ)|2
k∈[[K]]d ℓ∈Zd\{0} ℓ∈Zd\{0}
We bound each component separately. It is clear from Definition 2.1 that
(cid:88) (cid:88)
(C.1) |k+Nℓ|2s|v†(k+Nℓ)|2 ≤∥v ∥2 .
(cid:98)t t Hs
k∈[[K]]dℓ∈Zd\{0}
Toboundthefirstcomponentindependentlyofk, wenotefromK < N andequation
2
(A.10) that
(cid:88) 1 (cid:88)
(cid:18)
N
(cid:19)−2s
≤ √ |ℓ|
|k+ℓN|2s 2 d
ℓ∈Zd\{0} ℓ∈Zd\{0}
≤α2 N−2s
d,s
by equation (A.11), where α2 = (4d)s(cid:80) |ℓ|−2s is finite since s ≥ d. We
d,s ℓ∈Zd\{0} 2
express the final bound as
∥E(1)∥ ≤α N−s∥v ∥ .
t k∈[[K]]d d,s t Hs
For E(2)(k) we have the definition
t
E(2)(k)= 1 (cid:88) E(0)(x )e−2πi⟨k,xn⟩.
t Nd t n
n∈[N]d
By Parseval’s Theorem, we have
1
(C.2) ∥E(2)∥2 = ∥E(0)∥2 .
t ℓ2(k∈[[N]]d) Nd t ℓ2(n∈[N]d)
This manuscript is for review purposes only.22 S.LANTHALER,A.STUART,ANDM.TRAUTNER
For P
t
∈Rdvt+1×Kd×dvt
we define the tensor Frobenius norm
∥P ∥2 =(cid:80) ∥P(k)∥2.
t F k∈[[K]]d t F
(cid:12) (cid:12)2
(cid:12) (cid:12)
∥E(3)∥2 = (cid:88) (cid:12) (cid:12) (cid:88) P(k)(cid:16) E(1)(k)+E(2)(k)(cid:17) e2πi⟨k,xn⟩(cid:12) (cid:12)
t ℓ2(n∈[N]d) (cid:12) t t t (cid:12)
n∈[N]d(cid:12)k∈[[K]]d (cid:12)
(cid:12) (cid:12)2
(cid:12) (cid:12)
≤Nd(cid:12)
(cid:12)
(cid:88) |P(k)(E(1)(k)+E(2)(k))|(cid:12)
(cid:12)
(cid:12) t t t (cid:12)
(cid:12)k∈[[K]]2 (cid:12)
≤Nd (cid:88) ∥P(k)∥2 (cid:88) |E(1)(k)+E(2)(k)|2
t F t t
k∈[[K]]d k∈[[K]]d
=Nd∥P ∥2∥E(1)+E(2)∥2
t F t t ℓ2(k∈[[K]]d)
(cid:16) (cid:17)
∥E(3)∥ ≤Nd/2∥P ∥ ∥E(1)∥ +∥E(2)∥
t ℓ2(n∈[N]d) t F t ℓ2(k∈[[K]]d) t ℓ2(k∈[[K]]d)
Finally, we have the definition
∥E(0)∥2 = (cid:88) (cid:12) (cid:12)σ(W v +K v +b +W E(0)(x )+E(3)(x ))−σ(W v +K v +b )(cid:12) (cid:12)2
t+1 ℓ2(n∈[N]d) (cid:12) t t t t t t t n t n t t t t t (cid:12)
n∈[N]d
≤ (cid:88) B2(cid:12) (cid:12)W E(0)(x )+E(3)(x )(cid:12) (cid:12)2
(cid:12) t t n t n (cid:12)
n∈[N]d
(cid:16) (cid:17)
∥E(0)∥ ≤B ∥W ∥ ∥E(0)∥ +∥E(3)∥
t+1 ℓ2(n∈[N]d) t 2 t ℓ2(n∈[N]d) t ℓ2(n∈[N]d)
where ∥·∥ is the matrix-2 norm.
2
The results of Proposition C.1 allow us to easily prove the following lemma.
Lemma 3.5. Under Assumptions 3.1, the following bound holds:
(cid:18) (cid:19)
1 2
(3.4) ∥E(0)∥ ≤BM ∥E(0)∥ +α N−s∥v ∥
Nd/2 t+1 ℓ2(n∈[N]d) Nd/2 t ℓ2(n∈[N]d) d,s t Hs
where α is a constant dependent only on d and s.
d,s
Proof. From Proposition C.1, and shortening the notation ℓ2(n∈[N]d) to ℓ2,
(cid:16) (cid:16) (cid:17)(cid:17)
∥E(0)∥ ≤B ∥W ∥ ∥E(0)∥ +Nd/2∥P ∥ α N−s∥v ∥ +N−d/2∥E(0)∥
t+1 ℓ2 t 2 t ℓ2 t F d,s t Hs t ℓ2
Combining terms gives
(C.3) ∥E(0)∥ ≤B(cid:16)(cid:0) ∥W ∥ +∥P ∥ (cid:1) ∥E(0)∥ +α Nd/2−s∥P ∥ ∥v ∥ (cid:17) .
t+1 ℓ2 t 2 t F t ℓ2 d,s t F t Hs
Replacing ∥W ∥ and ∥P ∥ with M and rescaling gives
t 2 t
(cid:18) (cid:19)
1 2
∥E(0)∥ ≤BM ∥E(0)∥ +α N−s∥v ∥ .
Nd/2 t+1 ℓ2(n∈[N]d) Nd/2 t ℓ2(n∈[N]d) d,s t Hs
Appendix D. Proofs of Regularity Theory Lemmas.
The proof of Lemma 3.7 relies on another result for bounding the Hs norm of
compositions of functions, which is largely taken from the lemma on page 273 of
Section 2 in [19] without assuming an L∞ norm of v less than 1. We state a proof
here for completeness.
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 23
Lemma D.1. Assume φ : Td → Td possesses continuous derivatives up to order
r which are bounded by B. Then
|φ◦v|
≤Bc(cid:0) 1+∥v∥r−1(cid:1)
∥v∥
r ∞ Hr
provided v ∈Hr(Td), where c is a constant dependent on r and d.
Proof. By Fa`a di Bruno’s formula, we have
(cid:88) dρφ (cid:89)r
(D.1) Dr(φ◦v(x))= C (v(x)) (Djv(x))αj
x α,rdxρ x
j=1
where the sum is over all nonnegative integers α ,...,α such that α +2α +···+
1 r 1 2
rα =r, the constant C = r! , and ρ:=α +α +···+α .
r α,r α1!α2!2!α2...αr!r!αr 1 2 r
We seek a bound on square integrals of (D.1). Setting v = dρφv, v = Dλv,
α = 1, p = ∞, and p = r and noting that (cid:80)r 1 = 10 , wed hx aρ ve byλ H¨oldex r’s
0 0 λ λαλ λ=0 2pλ 2
inequality for multiple products that
(cid:12) (cid:12)2
(cid:90) Td(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)d dρ xφ ρ(v(x)) j(cid:89) =r 1(D xjv(x))αj(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
dx≤(cid:90)
Td
λ(cid:89) =r 0|v λ|2αλ dx≤ λ(cid:89) =r 0(cid:18)(cid:90) Td|v λ|2αλpλ dx(cid:19)1/pλ
(cid:89)r (cid:18)(cid:90) (cid:19)1/pλ
=∥v 0∥2
∞
|v λ|2αλpλ dx
Td
λ=1
ThefirstfactorisboundedabovebyB2 byassumption. ByapplicationofGagliardo-
Nirenberg, the second factor may be bounded by
(cid:89)r (cid:18)(cid:90)
|Dλv|2r/λ
dx(cid:19)λαλ/r
≤Cr
(cid:89)r
∥v∥2αλ(1−λ/r)(cid:0) ∥Drv∥2+∥v∥2(cid:1)αλλ/r
x ∞ x
Td
λ=1 λ=1
≤Cr∥v∥2ρ−2∥v∥2
∞ Hr
(cid:80) (cid:80)
since λα =r, and α =ρ. Combining the bounds,
λ λ λ λ
(cid:90) r
(cid:89)
|v λ|2αλ dx≤B2Cr∥v∥2 ∞ρ−2∥v∥2 Hr.
Td
λ=0
If ∥v∥ <1, we have the bound
∞
(cid:90) r
(cid:89)
(D.2) |v λ|2αλ dx≤B2Cr∥v∥2 Hr,
Td
λ=0
and otherwise since ρ≤r,
(cid:90) r
(cid:89)
(D.3) |v λ|2αλ dx≤B2Cr∥v∥ ∞2r−2∥v∥2 Hr.
Td
λ=0
Since these bounds hold for any term in the sum D.1, we obtain
(D.4) |φ◦v|
≤Bc(cid:0) 1+∥v∥r−1(cid:1)
∥v∥
r ∞ Hr
for a different constant c depending on r and d.
This manuscript is for review purposes only.24 S.LANTHALER,A.STUART,ANDM.TRAUTNER
Now we may prove Lemma 3.7.
Lemma 3.7. Under Assumptions 3.1, the following bounds hold:
• ∥v ∥ ≤σ +BM(1+∥v ∥ +Kd/2∥v ∥ ).
t+1 ∞ 0 t ∞ t L2(Td)
• |v | ≤BcMsKds/2(1+∥v ∥ )s(1+|v | )
t+1 s t ∞ t s
for some constant c dependent on d and s.
Proof. Firstwebound∥K v ∥ undertheassumptionthattheFouriertransform
t t ∞
is computed exactly (i.e. not on a grid). Let v†(k):=(cid:82) v (x)e−2πi⟨k,x⟩ dx.
(cid:98)t Td t
∥K v ∥ =∥ (cid:88) P(k)v†(k)e2πi⟨k,x⟩∥
t t ∞ t (cid:98)t ∞
k∈[[K]]d
(cid:88)
≤∥P ∥ |v†(k)|
t F (cid:98)t
k∈[[K]]d
≤∥P ∥ Kd/2∥v†(k)∥
t F (cid:98)t ℓ2(k∈[[K]]d)
≤∥P ∥ Kd/2∥v ∥ .
t F t L2(Td)
Then
∥W v +K v +b ∥ ≤∥W ∥ ∥v ∥ +|b |+∥P ∥ Kd/2∥v ∥ ,
t t t t t ∞ t 2 t ∞ t t F t L2(Td)
and by Lipschitzness of σ we have
(cid:16) (cid:17)
∥v ∥ ≤σ +BM 1+∥v ∥ +Kd/2∥v ∥ .
t+1 ∞ 0 t ∞ t L2(Td)
Next we bound |v | . Letting f =W v +K v +b , we see from Lemma D.1 that
t+1 s t t t t t t
bounding ∥f ∥ will give the result.
t Hs
Ds(f )=W (Dsv )+K (Dsv ).
x t t x t t x t
(cid:90) (cid:18)(cid:90) (cid:90) (cid:19)
|Ds(f )|2 dx≤2 |W (Dsv )|2 dx+ |K (Dsv )|2 dx
x t t x t t x t
Td Td Td
The first integral on the right may be bounded by ∥W ∥2|v |2. To bound the second
t 2 t s
integral,
(cid:12) (cid:12)2
(cid:90) (cid:90) (cid:12) (cid:12)
Td|K t(D xsv t)|2 dx= Td(cid:12) (cid:12)
(cid:12)
(cid:12)k∈(cid:88) [[K]]dP t(k)g (cid:98)t†(k)e2πi⟨k,x⟩(cid:12) (cid:12)
(cid:12)
(cid:12)
dx
where g†(k) are the Fourier coefficients of Dsv . Continuing,
(cid:98)t x t
(cid:90) (cid:90)
(cid:88)
|K (Dsv )|2 dx≤Kd ∥P ∥2 |g†(k)|2 dx
t x t t (cid:98)t
Td Td
k∈[[K]]d
≤∥P ∥2∥Dsv ∥2 ,
t F x t L2
giving a bound of
|f | ≤2M|v |
t s t s
This manuscript is for review purposes only.DISCRETIZATIONERROROFFOURIERNEURALOPERATORS 25
Inthefollowing,≲denotesinequalityuptoaconstantmultiplethatdoesnotdepend
on any of the variables involved. Combining Lemma D.1 and the above bounds, we
have
|σ◦f | ≤Bc(1+∥f ∥s−1)∥f ∥
t s t ∞ t Hs
≤Bc(1+(M(1+∥v ∥ +Kd/2∥v ∥ ))s−1)(M(1+∥v ∥ +Kd/2∥v ∥ )+2M|v | )
t ∞ t ∞ t ∞ t ∞ t s
≲BcMsKds/2(1+(1+∥v ∥ )s−1)(1+∥v ∥ +|v | )
t ∞ t ∞ t s
≲BcMsKds/2(1+∥v ∥ )s−1(1+∥v ∥ )(1+|v | )
t ∞ t ∞ t s
≲BcMsKds/2(1+∥v ∥ )s(1+|v | ).
t ∞ t s
Appendix E. Proof of Theorem 3.2.
Theorem 3.2. Under Assumptions 3.1,
1
(3.1) ∥v −vN∥ ≤CN−s
Nd/2 t t ℓ2(n∈[N]d)
where the constant C depends on B,M,d,s,t, and v .
0
Proof. From Lemma 3.7 we have for t≥1,
t−1 t
(cid:88) (cid:88)
∥v ∥ ≲σ (BMKd/2)j + (BMKd/2)j +(BMKd/2)t∥v ∥
t ∞ 0 0 ∞
j=0 j=1
 t t−1  (cid:32)t−1 (cid:33)
(cid:88) (cid:89) (cid:89)
|v t| s ≲  (BcMsKds/2)j (1+∥v ℓ∥ ∞)s +(BcMsKds/2)t (1+∥v ℓ∥ ∞)s |v 0| s.
j=1 ℓ=t−j ℓ=0
Denote max{BMKd/2,B1/sc1/sMKd/2,1} by C . The bound on ∥v ∥ simplifies to
0 t ∞
t
(cid:88)
∥v ∥ ≲σ Cj +Ct∥v ∥
t ∞ 0 0 0 0 ∞
j=1
≤σ tCt+Ct∥v ∥
0 0 0 0 ∞
Plugging in this bound to the product in the bound on |v | , we have
t s
t−1 t−1
(cid:89) (cid:89)
(1+∥v ∥ )s ≲ (ℓσ Cℓ+Cℓ∥v ∥ )s
ℓ ∞ 0 0 0 0 ∞
ℓ=t−j ℓ=t−j
≤Ctsj(t)sj(σ +∥v ∥ )sj.
0 0 0 ∞
Combining these two bounds, we attain the following bound on |v | for t≥1.
t s
 
t
|v t|
s
≲ (cid:88) (C 0)sjC 0tsj(t)sj(σ 0+∥v 0∥ ∞)sj +C 0ts(cid:16) C 0t2s(t)st(σ 0+∥v 0∥ ∞)st(cid:17) |v 0|
s
j=1
 
t
≲ (cid:88) C 02tsj(t)sj(σ 0+∥v 0∥ ∞)sj +C 02t2s(t)st(σ 0+∥v 0∥ ∞)st|v 0|
s
j=1
≲(C2t2stst+1+C2t2stst|v | )(σ +∥v ∥ )st
0 0 0 s 0 0 ∞
This manuscript is for review purposes only.26 S.LANTHALER,A.STUART,ANDM.TRAUTNER
and the following bound on ∥v ∥
t Hs
(E.1) ∥v ∥ ≲(C2t2stst+1|v | )(σ +∥v ∥ )st+σ tCt+Ct∥v ∥ .
t Hs 0 0 s 0 0 ∞ 0 0 0 0 ∞
Denote this upper bound by C , which does not depend on N. From Lemma 3.5, we
1
have
(cid:18) (cid:19)
1 2
∥E(0)∥ ≲BM ∥E(0)∥ +α N−sC .
Nd/2 t+1 ℓ2(n∈[N]d) Nd/2 t ℓ2(n∈[N]d) d,s 1
By the discrete Gronwall lemma,
1 BMα N−sC 1
∥E(0)∥ ≲ d,s 1(1−(2BM)t)+ ∥E(0)∥ (2BM)t.
Nd/2 t ℓ2(n∈[N]d) 1−2BM Nd/2 0 ℓ2(n∈[N]d)
Since we assume we begin with no error, ∥E(0)∥ =0, this simplifies to
0 ℓ2(n∈[N]d)
1 BMα C
∥E(0)∥ ≲ d,s 1(1−(2BM)t)N−s.
Nd/2 t ℓ2(n∈[N]d) 1−2BM
Denoting the factor in front of N−s by C and absorbing the effects of ≲ into C, we
have the result that
1
∥v −vN∥ ≤CN−s.
Nd/2 t t ℓ2(n∈[N]d)
Appendix F. Proof of Theorem 3.4.
Theorem 3.4. Let pN(x)=(cid:80) DFT(vN)(k)e2πi⟨k,x⟩ denote the interpolat-
t k∈[[N]]d t
ing trigonometric polynomial of {vN(x )} . Under Assumptions 3.1, the follow-
t n n∈[N]d
ing bound holds:
(3.3) ∥v −pN∥ ≤C′N−s.
t t L2(Td)
Here, C′ depends on B,M,d,s,t, and v .
0
Proof. Let pN(x) be the interpolating trigonometric polynomial associated with
t
the data {vN(x )} . By Proposition A.6, we have
t n n∈[N]d
1
∥v −pN∥ ≤ ∥v −vN∥ +c ∥v∥ N−s.
t t L2(Td) Nd/2 t t ℓ2(n∈[N]d) d,s Hs(Td)
By (E.1), we have ∥v∥ ≤C . Furthermore, it follows from Theorem 3.2, that
Hs(Td) 1
1
∥v −vN∥ ≤CN−s.
Nd/2 t t ℓ2(n∈[N]d)
We conclude that
∥v −pN∥ ≤(C+c C )N−s
t t L2(Td) d,s 1
Thus, the claimed bound holds with C′ =C+c C .
d,s 1
This manuscript is for review purposes only.