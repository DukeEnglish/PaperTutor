Vibe-Eval: A hard evaluation suite for measuring progress of
multimodal language models
PiotrPadlewski∗ MaxBain∗ MatthewHenderson ZhongkaiZhu
NishantRelan HaiPham DonovanOng KaloyanAleksiev AitorOrmazabal
SamuelPhua EthanYeo EugenieLamprecht QiLiu YuqiWang EricChen DeyuFu LeiLi
CheZheng CypriendeMassond’Autume DaniYogatama MikelArtetxe YiTay
Reka
What does this image say? Golden Reference
There is a message hidden in the image that says "GOOD VIBES ONLY". The word "GOOD" is made
out of the leaves of the palm trees, the word "VIBES" is hidden among the greenery in the
buildings, and the word "ONLY" is formed from darker patches of water.
Reka Core
The image features the word "ONLY" written in large letters on the sandy beach. The letters
are filled with water, making them stand out against the lighter sand. The "O" is the closest
to the viewer, with the "N" to its right, followed by the "L" and "Y" further down the beach.
Evaluator
The assistant response only identified the word "only" in the sand, while the ground truth
response revealed the complete hidden message "GOOD VIBES ONLY" using various elements in the
image, such as palm trees, greenery in the buildings, and darker patches of water. Therefore,
the assistant response did not correctly match the ground truth.
Rating: 2 / 5
Figure1: AnexamplefromourVibe-Evalbenchmark. Thispromptisfromthehard-set,whichcontainsonly
difficultprompts. Theuserpromptandimagearedisplayedontheleft,andontherightisthehuman-written
goldenreferenceresponse,agenerationfromRekaCore,andoutputfromtheautomaticevaluator.
Abstract
WeintroduceVibe-Eval: anewopenbenchmarkandframeworkforevaluatingmultimodalchatmodels.
Vibe-Evalconsistsof269visualunderstandingprompts,including100ofharddifficulty,completewith
gold-standardresponsesauthoredbyexperts.Vibe-Evalisopen-endedandchallengingwithdualobjectives:
(i)vibecheckingmultimodalchatmodelsforday-to-daytasksand(ii)rigorouslytestingandprobingthe
capabilitiesofpresentfrontiermodels. Notably,ourhardsetcontains> 50%questionsthatallfrontier
modelsanswerincorrectly.Weexplorethenuancesofdesigning,evaluating,andrankingmodelsonultra
challengingprompts. Wealsodiscusstrade-offsbetweenhumanandautomaticevaluation,andshowthat
automaticmodelevaluationusingRekaCoreroughlycorrelatestohumanjudgment. WeofferfreeAPI
accessforthepurposeoflightweightevaluationandplantoconductformalhumanevaluationsforpublic
modelsthatperformwellontheVibe-Eval’sautomaticscores.Wereleasetheevaluationcodeanddataat
github.com/reka-ai/reka-vibe-eval.
∗Equalcontribution
4202
yaM
3
]LC.sc[
1v78220.5042:viXra1 Introduction
Frontier multimodal language models (Google, 2023; OpenAI, 2023; Anthropic, 2024; Reka, 2024)1 are
rapidlyapproachinghuman-levelperformanceonawiderangeoftasks,especiallywhenthosetasksarewell
representedintheirtrainingdata. Asthesemodelscontinuetoimprove,staticbenchmarksforevaluating
their capabilities become saturated, making it increasingly difficult to distinguish between models and
discovertheirrespectivestrengthsandweaknesses.
PublicarenassuchasLMSys(Chiangetal.,2024)andWildVision(Luetal.,2024)haverecentlybecome
popularmethodsofevaluation. Thesearenasarezero-sumanddynamicinnature,withvoterschoosing
between two anonymized model generations. While these arenas may provide useful signals, they also
facevariouschallenges. Firstly,itischallengingtocontrolforpromptquality,difficulty,anddistributionin
thesemassivelypublicsystems. Secondly,livetrafficisoftennoisy,andmakingcomparisonsacrosstime
or models is difficult. In our approach, we prioritize well-designed, small-scale evaluations that enable
granularunderstanding,crucialformeasuringtheever-evolvingcapabilitiesoffrontiermodels. Bothtypes
ofevaluationsarecomplimentary.
Inthiswork,weintroduceVibe-Eval,asetof269highquality,diverseimage-textpromptsforevaluating
multimodalchatmodels. Thesepromptsareaccompaniedbygold-standardreferencehumanresponses.
Toensurethehighestquality,promptsandreferenceresponsesarereviewedmultipletimesbyourteam.
Vibe-Evalhasdualobjectives,(i)asaresourcetovibe-checkmultimodallanguagemodelsforday-to-day
tasksand(ii)toprobethecapabilitiesofpresentfrontiermodelsandinducegreaterseparabilitybydesigning
verydifficultmultimodalprompts.
Ofthe269prompts,169areclassiedasthenormal-set,withvaryingdifficulty,coveringarangeofdifficulties
relevanttoeverydaytasksexpectedofamultimodallargelanguagemodel(LLM).Theremaining100are
partofthehard-set,promptsthatRekaCore(Reka,2024)isnotabletosolveatthetimeofcollection.2 To
furtherascertainthedifficultyoftheprompts,wealsonotethat> 50%ofthepromptsareunsolvableby
allexistingmodels(includingfrontiermodels). Additionally,mostoftheVibe-Evalpromptsarefreshly
created(e.g.,newscreenshotsorphotographs)andcrowd-sourcedbytheRekateam. Thus,atthetimeof
publishing,thebenchmarkshouldaccuratelymeasurethegeneralization(contamination-free)performance
ofthemodelsreported. Afewexamplesfromthehard-setarepresentedinFigure2.
Alongsidethisopenbenchmark,wereleaseanofficialevaluationprotocolusingRekaCoreastheautomated
judge. We show that this automatic evaluation correlates with human judgment. We provide free API
accessforVibe-Evalavailablefree-of-chargeandofferafreelyaccessibletoolkitforconductingevaluations.
Recognizing that automatic evaluations may not completely capture the complete picture, we intend to
periodicallyconducthumanevaluationsofpublicmodelsthatperformwellonthisautomatedmetriconthis
benchmarktogainamorecomprehensiveperspective.
Finally,wepublishaninitialrankingofallrepresentativeandwell-establishedmultimodallanguagemodels
includingGPT-4V(OpenAI,2023),Claude-3Opus(Anthropic,2024)andGemini1.5Pro(Google,2024). We
measuretheirperformanceandrelativeperformanceonthisbenchmarkusingbothautomaticandhuman
evaluation.
1
https://i.imgflip.com/1hhv9m.jpg
2Weusethisasacriterionwhencreatingthisdata.HencetheperformanceofRekaCoreonthehard-setisexpectedtobeartificially
low.
2Figure2: ExamplesfromtheVibe-Evalhard-set,onlytheimageandtextpromptareshown. Theprompts
arediverse,difficult,andrequireinvolvedandlanguagereasoningcapabilitiestoanswercorrectly.
32 Vibe-Eval
2.1 Overview
The Vibe-Eval benchmark consists of 269 prompts, each containing an image and a corresponding task
or question requiring visual understanding. Along with each prompt, we provide a human-generated
goldenreferenceresponse. TherearetwocategoriesofdifficultyforpromptsinVibe-Eval: normalandhard,
consistingof169and100examplesrespectively.
• normal-setpromptsarediverseandvaryintheirdifficulty,annotatorswerenotgivenanyconstraints
intermsofdifficultyorcategory.
• hard-setpromptsarethosetowhichRekaCoregeneratedeitherapartiallyorcompletelyincorrect
responseatthetimeofcreation.
Fortheofficialevaluationprotocol,weemployRekaCoreasanautomatedtext-basedevaluator. RekaCore
isgiventhetextprompt,modelgeneration,andreferenceresponseandratestheresponseonitsaccuracy
viaa1-5integerscale. RefertoSection2.3forfurtherdetails. Thepromptsareopen-endedandcanrequire
–particularlyforthehard-set–multiplereasoningstepstosolve. Whereapplicable,referenceresponses
contain the intermediary reasoning/working steps and the evaluator score can assign partial credit for
generationscontainingthesesteps.
AllpromptsinVibe-Evalarediverse,andcollectedbyourteammembersspanningfourcontinents. The
promptsandgoldenreferencesareinEnglish,althoughtheymightcontainpartsthatinotherlanguages,
referencingthecontentofthepicture.
2.2 DataCollection
Toensurethehighestquality,3wecollectallVibe-Evalpromptsandgoldenreferencesourselves. Eachteam
member contributed roughly an equal number of prompts. The guidelines given to each annotator are
providedinTable1. Afterpromptcollection,wegothroughtworoundsofindependentreviewtoensure
thepromptandresponseareofhighqualityandmeetthespecifiedrequirements. Wealsoaimtokeeponly
exampleswherethevalidityofananswercanbejudgedagainstthegroundtruthonly,withoutaccesstothe
image. Thisallowsautomatedevaluationbytext-onlymodels.
Table1: InstructionsprovidedtoannotatorsfortheVibe-Evalbenchmark.
Promptsubset Instructions
normal Provideapromptwithanobjectivelycorrectanswer.
Theimageprovidedshouldbeyourown,orwithoutlicenserestrictions.
Thepromptandreferenceresponseshouldbesingleturn.
Ideallythetaskshouldtakeanexpertnolongerthanoneminutetocomplete.
Whereapplicable,thereferenceresponseshouldcontainanyreasoningstepsthatled
tothefinalanswer(i.e. chain-of-thought).
hard Inadditiontotheabove,hard-difficultypromptsshouldbespecificallyconstructedto
elicitapartiallyorwhollyincorrectresponsefromRekaCore.
Allimagesusedareeitherownedbyusorhavepermissivelicensestoensurethebenchmarkcanbemade
publiclyavailable. Thevastmajorityofimagesarephotosorscreenshotstakenourselves,therebyreducing
3AndtosavemoneyforGPUs
4Figure3: Illustrativediagramofbenchmarkdifficultywithrespecttothescoreachievedonthebenchmark
byafrontiermodel. Forbenchmarksthatarelowindifficultywithrespecttofrontiermodels,scoresare
alreadyclosetotheupperboundplacingitinthe“OversaturatedZone”. The“GoldilocksZone”iswhere
frontiermodelsbegintoshowpartialsuccess,butarestillfarfromhumanperformance. Benchmarksinthis
regionarethemostinformative. Finally,forbenchmarkswithdifficultytoohighforfrontiermodels,little
signalisprovidedsinceallmodelsobtainanear-zeroscore.
thepossibilityoftestsetleakage(atthetimeofpublishing)tonearzero. Additionally,wedonotadhereto
anytasktaxonomyorcategorizationtonotunnecessarilybiasorconstrainthecollectionofpromptsandto
ensurethatthedistributionofhardpromptsemergesnaturally.
Author’sNote.
Creatinghardbenchmarksishardinitselfbecauseonehastoconstantlycalibrateagainsttheperformanceof
currentfrontiermodelstoensureanappropriatedifficultylevel. Specifically,onehastotakecarethattasks
arenottooeasy,wheremodelsperformanceisoversaturated,whilstalsoensuringthetaskisnotimpossibly
difficult. Duringveryearlyphasesofcreatingthisbenchmark,wefoundthatmanyofthequestionscollected
werealittletooeasyforcurrentfrontiermodels,i.e.,preliminaryresultsrevealedthatRekaCoreandGPT-4V
wereabletosolvethemajorityofthesepromptswithoutdifficulty,whichputVibe-Evalslightlytowardsthe
OversaturatedZone(Figure3). Inthiszone,differencesbetweenfrontiermodelsaremarginalanddetermined
byafewunsolvableprompts–alessthanidealsituation.
Therefore,wehadtore-calibrateourselvestotheabilitiesofthecurrentfrontiermodelsandstartedtocollect
hard-difficultyprompts,wherefrontiermodelsarenotabletoperfectlysolve. Specifically,wedeterminethis
bycheckingwhetherthepromptathandisunsolvablebyRekaCoreatthetimeofcollection. Thedecisionto
collectpromptsbymininghardnegativesfromourownmodel’sfailureisthree-fold: first,RekaCoreis
onthefrontierformultimodalreasoningandamodelthatwehavefullcontrolover(suchasversioning).
Second,thesehardpromptswillbemostinformativeforfailurecasesofourownmodels,RekaCore,which
selfishlyiswhatwecaremostabout. Lastly,asourmodelimprovesandwithitthefrontier,wewillcontinue
toextendVibe-Evalwithincreasinglyharderprompts,likeadance,untilAGIisachievedinternally.
Finally, when studying the behavior of hard prompts on our models (and others), we observed some
interesting phenomena. Specifically, there exists a nontrivial subset of hard prompts neither Reka Core,
ClaudeOpus,GeminiPro1.5,norGPT-4Vcouldsolve,yetwereinfactsolvedbythetinyvision-language
models(VLMs)suchasRekaEdge(7B)andIdefics-2(8B).Thissuggeststhatthecurrentapproachesfor
frontierVLMsmightexhibitaninversescalinglaw(McKenzieetal.,2023)forsometypesoftasks.
52.3 AutomatedEvaluationProtocol
ExampleofVibe-EvalAutomatedEvaluation:
Prompt
[Question]
howlongarethescissors
[AssistantResponse]
Thelengthofthescissorsintheimageisapproximately20centimeters,asindicated
bytheyellowmeasuringtapeunderneaththescissors. Themeasurementstarts
from0andthetipofthescissorsalignswiththe20cmmarkonthetape.
[GroundTruthResponse]
Lookingattheimage,thescissorsstartatapproximatelythe113cmmarkonthe
measuringtapeandendaroundthe130cmmark. Thatsuggeststhescissorsare
about17cminlength.
[System]
Ratewhethertheassistantresponsecorrectlymatchesthegroundtruth,it’sabout
animagesharedbytheuser.
Theratingshouldbe1-5,where1isincorrectand5iscorrect.
Yourresponseshouldbeintheformat:
Explanation: (yourexplanation)
Rating: (int)
Reka Explanation: Theassistantresponsewasnotcompletelyincorrect,asitattemptedto
Core measurethelengthofthescissorsusingthemeasuringtapeintheimage. However,
themeasurementprovidedwassignificantlyofffromthegroundtruthmeasurement.
Rating: 2
Figure4: ExampleevaluationofaVibe-Evalpromptandassistantresponse,usingmultimodalRekaCoreas
theevaluator. HeretheassistantbeingevaluatedisRekaCoreitself. Thisexampleshowthattheevaluator
needstobefairlycapableinreasoning,beingabletoassignpartialcreditformeasuringthescissorsusingthe
measuringtape.
AnexampleoftheofficialevaluationprotocolforVibe-EvalcanbeshowninFigure4. WeuseRekaCore
asthemodelevaluator,whichtakesasinput: thetextprompt(question),themodelgeneration(predicted
answer),andthegoldstandardreferencehumanresponse(referenceanswer). Thesystempromptincludes
instructionstoevaluatewhollybasedonitsobjectivitytothetruth,sinceVibe-Evalpromptsweredesigned
tobeasobjectiveaspossible. Theratingisonanintegerscalefromonetofivewhereoneindicatesatotally
6inaccurateresponseandfiveindicatesaresponsethatfullysolvesthetaskwithoutanyfalseinformation.
Unlikemultiple-choicebenchmarks(Yueetal.,2024),thegradingscaleallowsfortheevaluatortoassign
partialcreditforincorrectanswersthatcontainsomeofthereasoningstepsinthereferenceanswer.
Notably, early experiments showed that using multimodal inputs for judging does not improve overall
correlationwithhumanpreferences. However,itmightrequiremorework,oraspecificsetofpromptsor
criteriaforittomatter. Weleavethattofuturework.
2.4 HumanEvaluation
WealsocomputeanELOleaderboardforthe13modelsbycollectinghumanpreferencedatausingathird-
partydataannotationcompany. Foreachprompt,wecreatemultiplepreferencetasksbysampling4ofthe13
modelsuniformlyatrandom. Humanannotatorsareshowntheuserprompt,image,andreferenceanswer,
andthenaskedtoanalyzeeachofthefourmodelresponsessampledforthattask. Intotal,wecollectaround
20kpairwisepreferences(includingties). Forfurtherdetailsontheinterfacepresentedtotheannotators,see
AppendixB.
WecomputeELOscoresfollowingAskelletal.(2021),whereweonlyconsiderpairwisecomparisonswhere
annotatorsexpressapreferencestrongerthantheweakestavailable. Thescoresarecomputedbymaximizing
thelog-probabilityoftheequivalentBradley-Terrymodel.
3 Results
3.1 ImplementationDetailsandSetup
ForallusesofRekaCore–bothasanevaluatorandasanassistant,weusethereka-core-20240415version
availablethroughtheAPI(https://www.reka.ai/reka-api). Forimplementationdetailsofothermodels
pleaserefertothisappendix. ForGPT4-V(OpenAI,2023)weuseversiongpt-4-turbo-2024-04-09. For
theClaudeseries(Anthropic,2024),Opus,Sonnet,andHaiku,weuseversionsclaude-3-opus-20240229,
claude-3-sonnet-20240229, and claude-3-haiku-20240307 respectively. For Gemini Pro 1.5 (Google,
2024)weusethegemini-1.5-pro-preview-0409version. Foropen-sourcemodels,LLaVA1.6(Liuetal.,
2024),Idefics1(Laurençonetal.,2023),Idefics2(Tronchonetal.,2024),andFuyu-8b(Bavishietal.,2023)we
usetheirofficialimplementationsviaHuggingFace. Allmodelgenerationsareperformedwithatemperature
of0.0. Allmodelsareevaluatedinchatstyle,i.e.,zero-shotwithoutexemplars. Forautomatedevaluation,
weruntheCoreevaluatorthreetimeswithatemperatureof0.4andtakethemeanofthescores.
3.2 Vibe-EvalAutomaticResults
Table2reportstheautomaticVibe-Evalscoresforall13models. Overall,wefindthatGeminiPro1.5performs
thebestfollowedbyGPT4-V.ThenextbestmodelsareRekaCoreandFlash,followedbytheClaude-3series
ofmodels. Moreover,opensourcemodelslikeLLaVAandIdeficsseriesofmodelsgenerallyperformworse
thanclosedmodels. Fuyuconsistentlyhastheworstperformance.
Itisworthnotingthatoverallscoresarestronglyweightedbythenormalset. Thestorychangesslightlyfor
thehardset. Similartotheoverallandnormalset,GeminiPro1.5performsthebest,followedbyGPT4-V.
Differentfromthenormalset,Claude-3modelsperformbetterthanRekamodelsonthehardset. RekaCore
notablyperformspoorlyonthehardsetwhichwepostulateismainlyduetotheseedingthecreationofthe
hardsetbasedonpromptsthatCoreisnaturallybadat. Notably,FlashalsooutperformsCoreonthehard
set. AsurprisingdatapointinthehardsetisthatLLaVA1.634Bactuallyperformswellonthishardset,
cominginatthefourthplaceafterClaude3Opus.
7Table2: Vibe-Evalscoreandrankingforexistingmultimodallanguagemodelssortedbyoverallscore.
†NoteweexpecttheresultsofRekaCoretobeworseonthehard-set,astheseare,bytheirverydefinition,
promptsthatCorecannotsolve.
Vibe-EvalScore(%)
Model
all hard normal
GeminiPro1.5 60.40 53.00 64.80
GPT-4V 57.90 46.00 64.90
RekaCore 53.70 38.20† 62.80
ClaudeOpus 52.80 41.80 59.20
RekaFlash 52.20 39.20 59.90
ClaudeSonnet 52.10 39.70 59.50
ClaudeHaiku 49.80 38.50 56.40
Llava-1.6-34b 48.60 39.90 53.70
RekaEdge 45.40 32.20 53.10
Llava-1.6-7b 43.70 35.30 48.60
Idefics-2-8b 40.00 32.20 44.60
Idefics-1-80b 36.00 32.10 38.30
Fuyu-8b 30.80 23.40 35.20
3.3 HumanEvaluationResults
Table3reportshumanpreferenceELOscoresofallmodelsbasedonourhumanevaluationstudy. Ataglance,
we find that the general relative rankings of models remain roughly the same as automatic evaluations.
Table 4 shows the mappings of human preference rankings (95% confidence interval) to the Vibe-Eval
rankings. AkeydifferenceinhumanevaluationrankingsisthatGPT4-VslightlyoutperformsGemini1.5.
However,Gemini1.5outperformsGPT-4Vonthehardset. Nevertheless,theresultsonGemini1.5andGPT-
4Vgenerallybelongtothesametierandcouldoftenswitchplacesdependingonthepromptsorannotators.
Notably,Flashperformsasthethirdbestmodel,outperformingClaudeOpusandallothermodels. Core
doesn’tdoaswellasflash,largelyduetothefactthatthereisnegativebiasinthedatacurationbasedon
Core’sinabilitytosolvecertainexamples. Meanwhile, theotherrankingsgenerallystayconsistentwith
automaticevaluationasshowninTable4.
Table3: RankingsforHumanpreferenceonVibe-Eval,measuredinELOfrompairwisepreferences. ELO
scoresarecomputedonallprompts,aswellasthedisjoint“hard"and“regular"difficultysubsets.
HumanPreference(ELO)
Model
all hard normal
GPT-4V 1290 1224 1277
GeminiPro1.5 1254 1207 1356
RekaFlash 1133 1080 1163
Claude3Opus 1125 1134 1117
RekaCore 1109 1009 1168
Claude3Sonnet 1060 1051 1060
Claude3Haiku 1022 1000 1034
Llava1.634B 993 1025 974
RekaEdge 990 987 988
Llava1.67B 924 906 931
Idefics-28B 774 862 705
Idefics-180b 761 857 705
Fuyu-8b 566 660 509
8Table4: MappingsofHumanPreferencerankingstoVibeEvalranks. OurexperimentsshowthatHuman
preferencerankingofmodelsroughlycorrelatetoVibeEvalrankings.
HumanPreferencerank→VibeEvalrank
Model
hard normal
GPT-4V [1−2]→[1−4] [1]→[1−4]
GeminiPro1.5 [1−2]→[1−2] [2]→[1−4]
RekaFlash [3−6]→[2−7] [3−5]→[2−7]
Claude3Opus [3−4]→[2−7] [4−5]→[2−7]
RekaCore [5−9]→[3−9] [3−5]→[1−6]
Claude3Sonnet [4−8]→[2−7] [6−7]→[2−7]
Claude3Haiku [6−9]→[4−9] [6−8]→[4−9]
Llava1.634B [4−9]→[2−9] [8−10]→[6−10]
RekaEdge [7−9]→[7−10] [7−9]→[7−10]
Llava1.67B [10−12]→[9−11] [9−10]→[9−11]
Idefics-28B [10−12]→[10−11] [11−12]→[12]
Idefics-180b [10−12]→[12−13] [11−12]→[11]
Fuyu-8b [13]→[13] [12−13]→[13]
Table5: AgreementratebetweenRekaCoreasajudge(Automatic),humanannotators,andexperthuman
annotators,onnormalandhardsubsets(givenasnormal/hard). Theagreementrateisthepercentageof
timesthattwotechniquessharethesamepreferenceformodelAvsmodelBoverallprompts,skippingcases
whereatleastonetechniquepredictsatie. Apreferencestrongerthantheweakestpossibleisrequiredtonot
countasatie.
Automatic Human HumanExpert
Automatic 100.0%/100.0% 94.2%/97.2% -/99.8%
Human 99.3%/99.1% -/96.6%
HumanExpert -/99.9%
Tierate 77.7%/80.9% 71.3%/76.7% -/74.2%
Wealsostudytheinter-annotatoragreementamonghumansandtheautomaticresultsinTable5. Generally,
thereisahighlevelofconsensusamongtheannotationmethods. Thisispartlyduetothestipulationthata
comparisonbetweentwomodelgenerationsisonlyvalidiftheapproachdemonstratesapreferencestronger
thantheweakestpossible. Fortheautomaticevaluations,thistranslatestorequiringascoredifferenceofat
least2onascalefrom1to5. Notethattheautomaticresultsshowgreateralignmentwiththeexpertratings
onthehardsubsetcomparedtotheordinarypoolofraters.
3.4 Discussion&Insights
Wediscusssomeinsightsandfindingsthatwehadwhilebuildingthisevaluationsetup.
Hardpromptsarehardtomake. Anidealbenchmarkhardpromptshouldbe: (i)unsolvablebycurrent
frontiermultimodallanguagemodels;(ii)interesting/usefultosolve;(iii)error-free;and(iv)unambiguous
foranevaluator. Creatingthisbenchmarkrevealedparticularchallengeswiththelasttwocriteria. Hard
promptsoftenrequiremultiplereasoningsteps,whichincreasesthelikelihoodofintroducinghumanerror
inthesolution. Althoughmanyerrorswereidentifiedandcorrectedduringthequalityreviewprocess,
otherswereonlydiscoveredduringlaterqualitativeinspectionsoftheresults. Whatmayseemunambiguous
toahumanevaluatorcanstillbeambiguousforatext-basedmodelevaluator,potentiallyduetoalackof
9commonsensereasoningoraninabilitytoaccessvisualinformation. Therefore,insomeinstances,wehad
torefinethepromptsandresponsesonlyafterqualitativelyinspectingtheevaluator’sexplanationsforits
ratings,astherewasclearlyamisunderstandingonthemodel’spart.
Hardpromptsarehardtojudge. Forhardprompts,wefounditparticularlydifficulttoratethemascompared
tonormalprompts. Manyofthesepromptsarechallenging,requirenicheknowledge,oraretime-consuming
forhumans. Hence,itisnaturalforannotatorstoover-relyonthegroundtruthanswertorateresponses.
Indeed,perfectanswerscansimplybestring-matchedagainstthegroundtruth,howeverfortheassignment
ofpartialcreditAbigissuehereisthatassignmentofpartialcredit(ordiscredit)canalsorequiredomain
knowledge, context, or expert opinion. Hence, this is generally high variance and results may fluctuate
acrossdifferentsetsofraters. Whilemostofourhumanevaluationresultswereobtainedfromrelatively
ordinaryraterpools,wewerecuriousaboutwhatwouldhappenifagroupofhighlytechnicalexpertswere
toperformtheevaluationinstead. Tothisend,ourteamformedanexpertratingcommittee,oftendelegating
difficultpromptstoteammemberswithrelevantexpertisewheneverrequired. Whatwefoundwasthatthe
expertratergroupendedupbeingveryharshonallmodeloutputsandresultedinalmostsimilarrandom-ish
performanceforallmodels. TheonlyexceptionwasGeminiPro1.5andGPT4-Vwhichclearlystayedabove
thepack. Tosomeextent,thereissomeformofemergence(Weietal.,2022)intheabilityoffrontiermodels
todealwithverydifficultprompts.
Hardpromptsaretemporary. Ofcourse,thehardpromptsinthisworkareonlyvalidforasnapshotintime.
Asfrontiermodelsimprove,performanceonthesehardpromptswilleventuallysaturatetoo. Thus,toensure
thelongevityofthisbenchmark,itwillbenecessarytoextendthebenchmarkwithharderpromptsasthe
frontieradvances.
Reka Core as an evaluator. Empirically, we find that Core is a generous evaluator relative to humans,
especiallyforhardprompts,withabiastowardshigherratings. Inaddition,wefindthatCoreasanevaluator
gives a narrow range of absolute scores between frontier models and weaker models. While this is not
necessarily an issue in itself, if the number of prompts is small (such as the case for hard set), then the
resultingvariancemeansthatthebenchmarkisnotgreatformeasuringminordifferencesbetweenmodels
ormeasuringincrementalimprovementstoamodel.
HumanvsAutomaticEvaluationDespiteastrongagreementbetweenhumanandmodel-basedevaluations,
wenotethateachevaluationprotocolsuffersfromitsownsetofshortcomings. Humanraterpoolscanbe
noisy,inconsistentandhighvarianceespeciallywhendealingwithharderprompts. Meanwhile,model-based
evaluationscouldpotentiallymissnuanceorsimplymakemistakes. Overall,wefindgreatutilityinemploying
bothtypesofevaluationprotocolsandlearningfromtheagreementand(occasional)disagreementbetween
them.
4 Related Work
Evaluationisnotoriouslychallengingtogetright. Thecurrentresearchlandscapeisplaguedbyamyriad
ofchallengesandissues. Allbenchmarkscansufferfromintrinsicdifficulties, suchasannotatorquality
orproblemsinsolicitingsufficientlychallengingexamples. But,standardizedbenchmarkscanalsosuffer
fromexternalchallengesthataremoremetainnature: “Whenameasurebecomesatarget,itceasestobe
agoodmeasure”–Goodhart’sLaw. Withbenchmarkoverfittingcommonplace,theseevaluationscanno
longerprovideausefulsignaltomodelqualityingeneral(Schaeffer,2023). Thevalidityofbenchmarkscan
bebroughtfurtherintoquestionduetotest-setleakage,eitherintentionaloraccidental,oncetheybecome
prevalentontheinternet,themainsourceoftrainingdata. Finally,theprocessofhowandwhichbenchmarks
become well-established is also not well understood, leading to lottery effects on how models could be
perceivedbythecommunity(Dehghanietal.,2021).
Aprimarychallengeinevaluatingtheselargelanguagemodels(LLMs)liesintheassessmentofopen-ended
textgenerationtasks(e.g.,chat). Whilemultiple-choiceevaluationsarecommonlyusedduetotheirobjective
nature(Yueetal.,2024;Hendrycksetal.,2020),theydonotaccuratelyreflectthecommonusecasesforthese
10models,whereuserinteractionsaretypicallyopen-ended. Evaluatingopen-endedquestionsaddsanother
layerofcomplexityhowever. Whilethisproblemwasmorespecifictotaskdomainssuchassummarization,
translationordialoguemodelsinthepast,theubiquityofchatinterfacesforallmodelsandtasksinthe
modernAIresearchlandscapehasmadethisproblemevenmoreprevalent. Humanevaluationcanbecostly
anddifficultytoscale,yetmaynotbecompletelyimmunetonoisyorlazyannotators. Tothisend,automatic
model-basedevaluationhasbeencommonlyemployedasalightweightevaluationoption. Notably,theidea
ofusingLLMasajudgeisnotnew-previousworkexploredusingittoevaluatetranslation,summarization
andoverallmodelquality(KocmiandFedermann,2023;Zhengetal.,2023;Lietal.,2024;Vergaetal.,2024;
Chenetal.,2024;Huangetal.,2024).
Arenastyleevaluationsarerecentlyverypopular(Luetal.,2024;Chiangetal.,2024)forevaluatinginstruction
tuned(Weietal.,2021)models. Theseevalsrelyonuserssubmittingpromptsandvotingpreferredanswers
amongsttwoblindanonymousmodels. Thesearenasaredynamicandlarge-scale,makingitdifficultto
controlforpromptdifficultyandmakingcontrolledcomparisonsovertime. Comparedtostaticbenchmarks,
theyaremuchhardertogameandthereforeseemtohavewonmoretrustoverthecommunity. Thatsaid,we
thinkthatsmallerscalestudiesdesignedtoprobeatcapabilitiescanbemoremeaningfulthanhowconsistent
modelsareatansweringquestionsinthewild. Wethinkbothtypesofevaluationsarecomplementary.
4.1 Multimodallanguagemodels
Largelanguagemodels(Brownetal.,2020;Raffeletal.,2019;Chowdheryetal.,2022)aregreatbutmultimodal
largelanguagemodelsaregreater. Theeraofpowerfulhighlyperformantmultimodallanguagemodels
startedwithFlamingo(Alayracetal.,2022)andPaLI(Chenetal.,2023b,a). Thefrontierhasshiftedsince
then,withmodelssuchasGPT-4V(OpenAI,2023)andGemini(Google,2024)becomingtheleadingedge
models. Morerecently,modelssuchastheClaude-3series(Anthropic,2024)andRekaseries(Reka,2024)
havestartedtodemonstratefrontier-classmultimodalcapabilities. Asidefromclosedmodelsonthebleeding
edge,itisworthtonotethattherearealsoseveralprettydecentopensourceeffortsinmultimodallanguage
modelssuchasLLaVA(Liuetal.,2024)andIdefics(Tronchonetal.,2024).
5 Conclusion
WeproposeVibe-Eval,achallengingbenchmarkforevaluatingmultimodalchatmodels. Thebenchmark
consistsof269diversepromptsthatarediverseandchallenging,especiallythehard-setforwhichthemajority
of promptscannotbe solved byany current frontiermodels. Along with the prompts, weprovide gold
standardreferenceresponses,includingreasoningandworkingstepsneededtoreachtheanswer,allowing
evaluatorstoawardpartialcreditforanswerswithsomeprogress. Fortheofficialevaluationprotocol,we
showthatRekaCoreasamodelbasedevaluatorcorrelatesstronglywithhumanjudgement,althoughthe
correlation is weaker for hard prompts. Finally, we provide a detailed discussion of the challenges and
considerationswhencuratingadatasetofhardquestions,theirresultsonfrontiermodels,andtheuseof
automatedevaluators.
Acknowledgements. Wearegratefultoourpoolofexternalannotatorsfortheirhardworkontedioustasks.
WewouldalsoliketothankAnnaPadlewskaandKevinConradfortheirphotographyusedinsomecelebrity
identificationprompts.
11A Qualitative Examples
A.1 Hardpromptsarehard
ExampleofhardpromptinVibe-Eval:
Image
Text Iwanttoclimbtheroutethatreachesthehighestpoint,whatshouldIclimb? Give
prompt thenameandrequiredgrade
Reference Thelinethatreachesthehighestverticalpointisclimb#60,named"TheEdge"with
Response agradeofVIIIa/bor7(-).
RekaCore Theroutethatreachesthehighestpointaccordingtotheimageis“RechteWand
desTeufels”(TheDevil’sRightWall). ItisratedIXa,8-. Thisrouteisdescribedas
havingsmallledges,requiringtheclimbertoclosetheireyesandpushthrough,and
issuitableforpracticingwithcamsandnuts.
GPT-4V Toreachthehighestpointontheclimbingwallshownintheimage,youshould
climbtheroutenamedB¨iomechanikw¨hichisnumberedas61. Thisrouteisgraded
Xa,indicatingaveryhighlevelofdifficulty. Itisdescribedasthehardestfriction
routeinthearea,soitrequiresadvancedclimbingskills.
Gemini KraftwerkMensch,IXa
Pro1.5
Claude Basedontheinformationprovidedintheclimbingroutelist,theroutethatreaches
Opus thehighestgradeis“KraftwerkMensch”gradedIXa(8-)byR.Gentschin1998.
Thedescriptionmentions“GibDirdieKante! KSPmusslinksderKantebleiben,
sonstPunktabzug.”,whichroughlytranslatesto“Giveyourselftheedge! KSPmust
stayleftoftheedge,otherwisepointdeduction.”
Soifyouwanttoclimbthemostchallengingroutethatreachesthehighestgrade
onthislist,“KraftwerkMensch”gradedIXa(8-)istheonetoattempt.
Figure5: Exampleofahard-setpromptwhereallmodelsanswerincorrectly. Allmodelsfailtoreasonthat
textontheleftsideoftheimageisalegendforthediagram(“toppo”)ontheright. Imagecredit:Krug
(2014),aclimbingguidebook.
12A.2 Signsofinversescaling
Empirically,wefindpromptsinVibe-Evalthatthelargestfrontiermodelsanswerincorrectly,whilethebest
smallmodels(RekaEdge&Idefics8B)answercorrectly. Suchresultsaresurprisinggiventheempirical
scalinglawswitnessedacrosslanguagemodeling,vision-languagemodeling,anddeeplearningingeneral.
WespeculatethatlargerVLMshavestrongerlanguagebias,whichcouldmeanthatthevisionmodalityis
heavilyundertrained. WeprovidesomeexamplesinFigure6&7.
ExampleIofinversescalinginVibe-Eval:
Image
Text whatcolourismyhat(imsittingonthebear)
prompt
Reference Thehatyouarewearingisblue.
Response
RekaCore Youarewearingablackhatwhilesittingonthelargeteddybearintheimage.
GPT-4V Yourhatisblack.
Claude Theimageshowsalargebrownteddybearstatueorsculptureinsidewhatappears
Opus tobeastore,likelyatoyorgiftshopbasedontheshelvesfullofplushtoysand
othermerchandiseinthebackground. Ontopofthebear’sheadsitsasmallyellow
birdplushtoywearingaredhat. TherearenohumansvisibleintheimagethatI
canidentify.
RekaEdge Thehatyouaresittingonthebearisblue.
Idefics2 Blue.
Figure6: Exampleofadifficultpromptthatexhibitsinversescaling,i.e. alllargefrontiermodels(GPT4-V,
GeminiPro1.5andRekaCore)elicitincorrectresponses,incontrast,RekaEdgeandIdefics-2,small7Band
8Bparametermodelsrespectively,bothcorrectlyidentifythecolourofthehat.
13ExampleIIofinversescalinginVibe-Eval:
Image
Text rankthemethodsfrombesttoworst
prompt
Reference Themethodscanberankedfrombesttoworstbasedontheirperformanceareaunder
Response thePrecision-Recallcurve,whichisacommonevaluationmetricforclassification
tasks,especiallyinthecontextofimbalanceddatasets. Thelargertheareaunder
thecurve,thebettertheperformanceofthemethod.
1. chance: Thechancemethodhasthelargestareaunderthecurve.
2. visual: Theaudiomethodhasslightlylessareaunderthecurvethan"chance".
3. Audiovisual: Thismethodhaslowprecisionacrossalllevelsofrecall,indicating
thatitperformssignificantlyworsethanthe"chance"and"visual"methods. Infact
theconstantlowprecisionindicatesthemodelisnotworkingatall.
RekaCore ThemethodscanberankedfrombesttoworstbasedontheareaunderthePrecision-
Recallcurve(AUC-PR).ThehighertheAUC-PR...<truncated>
1. Audiovisual: ThismethodhasthehighestAUC-PR...<truncated>
2. Visual:Thevisualmethodalsoperformswell,withahighAUC-PR...<truncated>
3. Chance: ThechancemethodhasthelowestAUC-PR...<truncated>.
GPT-4V Basedontheprecision-recallcurveintheimage,therankingofthemethodsfrom
besttoworstisasfollows:
1. Audiovisual(blueline)-Thismethodconsistentlymaintains...<truncated>.
2. Visual(tealline)-Thismethodperformsbetterthanchance...<truncated>
3. Chance(orangeline)-Thismethodshowsthelowestperformance...<truncated>
Gemini ##RankingtheMethodsBasedonthePrecision-RecallCurve:
Pro1.5 Here’showwecanrankthemethods(Audiovisual,Visual,Chance)...<truncated>
1. Audiovisual: Thismethodclearlyoutperformstheothers...<truncated>
2. Visual: Thismethodperformsdecentlybutisinferiorto...<truncated>
3. Chance: This represents a baseline with no predictive power...<truncated>
Therefore,therankingfrombesttoworstis: Audiovisual>Visual>Chance
RekaEdge Thechancemethodisthebest,followedbythevisualmethod,andthentheaudio
visualmethodistheworst.
Idefics-2 Chance,visual,audiovisual.
Figure7: Exampleofadifficultpromptthatexhibitsinversescaling,i.e. alllargefrontiermodels(GPT4-V,
GeminiPro1.5andRekaCore)elicitincorrectresponses,incontrast,RekaEdgeandIdefics-2,small7Band
8Bparametermodelsrespectively,bothcorrectlyrankthemethods.
14A.3 AwardingPartialCredit
An advantage of a scale-based rating system over binary correct/incorrect scoring, like multiple choice
question-answeringYueetal.(2024);Hendrycksetal.(2020),isthattheevaluatorcanawardpartialcredit
foranswersthataresuccessfulatsomeofthereasoning/workingstepstowardstheanswerbutdonotobtain
thefinalanswerintheend. InexampleFigure8modelgivesacorrectrequirementsforexistenceofEuler
path,butwronglyclassifiesoneofthenodesashavingodddegree,whichresultsinscoreof3. InFigure9
themodelgetspartialcountofpillowsright.
ExampleIofawardingpartialcreditinVibe-Eval:
Image
Prompt IsitpossibletofindEulerpathinthisgraph? Ifyes,tellmenodebynode
Reference AnEulerpathinagraphisapaththatuseseveryedgeexactlyonce. Foraconnected
response graphtohaveanEulerpath,itmusthaveexactlyzeroortwoverticesofodddegree
(numberofedgesincidenttothevertex).
NodesAandEhaveodddegree(3),sopossiblepathis:
A->B->E->D->A->C->E
RekaCore AnEulerpathinagraphisapaththatuseseveryedgeexactlyonce. Agraphhasan
Eulerpathifandonlyifitisconnectedandhasexactly0or2verticesofodddegree
(numberofedgesincidenttothevertex).
Inthegivengraph,wehavethefollowingdegreesforeachvertex:
-A:3(odd)
-B:2(even)
-C:3(odd)
-D:2(even)
-E:1(odd)
Sincetherearemorethan2verticeswithanodddegree(A,C,E),thisgraphdoes
nothaveanEulerpath.
Evaluator Explanation: Theassistantresponseinitiallycorrectlyexplainstheconditionsfora
graphtohaveanEulerpath. However,theassistant’sconclusionthatthegraphdoes
nothaveanEulerpathisincorrect. Thegroundtruthresponsecorrectlyidentifies
theEulerpathinthegraph: A->B->E->D->A->C->E.
Rating: 3
TheassistantresponseispartiallycorrectinexplainingtheconditionsforanEuler
pathbutfailstoidentifythecorrectEulerpathinthegivengraph. Thegroundtruth
responseiscorrect,buttheassistantresponsecouldhavebeenmoreaccurateand
complete.
Figure8: Exampleofevaluatorgivingpartialcreditformodelresponse.
15ExampleIIofawardingpartialcreditinVibe-Eval:
Image
Text Listthenumberofpillowsforeachcolourintherainbow
prompt
Reference Intheimage,thereare:
Response -Yellowpillows: 4
-Orangepillows: 1
-Redpillows: 1
RekaCore Thereare3yellowpillows,1redpillow,and1orangepillow.
Evaluator Explanation: Theassistantresponseispartiallycorrectbuthassomeinaccuracies.
Thecorrectnumberofpillowsforeachcolorintherainbow,basedontheground
truthresponse,is:
Yellowpillows: 4
Orangepillows: 1
Redpillows: 1
Theassistantresponsemissedthecorrectnumberofyellowpillowsanddidnot
mentionthattherearenopillowsintheothercolorsoftherainbow(green,blue,
indigo,violet)visibleintheimage.
Rating: 3
Figure9: Exampleoftheevaluatorgivingpartialcreditforamodelresponse.
16B Annotation User Interface
Figure10: TheAnnotationUserInterfacepresentedtohumanannotators. Afterbeingpresentedwiththe
userprompt,image,andreferenceanswer,annotatorsareaskedtoanalyzeaseriesoffouranonymizedmodel
responses. Foreachresponse,theyareaskedseveralquestions,including“Howaccurateisthisresponse?”
(shown). Preferencesamongthefourmodelresponsesareinferredfromtheanswerstothesequestions.
Wefindthataskingpointwisequestionslikethis,ratherthandirectlyaskingforaranking,givesushigher
qualityinferredpreferences. Directlyaskingforarankingtendstobiasannotatorstowardsmakingarbitrary
preferencedecisionsratherthanoptingforties,evenifthereisnosignificantreasontopreferoneresponse
overanother.
17References
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,TengdaHan,
ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,SebastianBorgeaud,AndrewBrock,
Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew
Zisserman,andKarenSimonyan. Flamingo: avisuallanguagemodelforfew-shotlearning,2022.
Anthropic. Theclaude3modelfamily: Opus,sonnet,haiku. 2024.
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,AndyJones,Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson
Kernion,KamalNdousse,CatherineOlsson,DarioAmodei,TomBrown,JackClark,SamMcCandlish,
ChrisOlah,andJaredKaplan. Agenerallanguageassistantasalaboratoryforalignment,2021.
RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,andSağnak
Taşırlar. Introducingourmultimodalmodels,2023. URLhttps://www.adept.ai/blog/fuyu-8b.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
arXivpreprintarXiv:2005.14165,2020.
DongpingChen,RuoxiChen,ShilinZhang,YinuoLiu,YaochenWang,HuichiZhou,QihuiZhang,PanZhou,
YaoWan,andLichaoSun. Mllm-as-a-judge: Assessingmultimodalllm-as-a-judgewithvision-language
benchmark,2024.
XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz, SebastianGoodman, XiaoWang, YiTay, SiamakShakeri, MostafaDehghani, DanielSalz, Mario
Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery,
PaulinaPietrzyk,MarvinRitter,AJPiergiovanni,MatthiasMinderer,FilipPavetic,AustinWaters,GangLi,
IbrahimAlabdulmohsin,LucasBeyer,JulienAmelot,KentonLee,AndreasPeterSteiner,YangLi,Daniel
Keysers,AnuragArnab,YuanzhongXu,KeranRong,AlexanderKolesnikov,MojtabaSeyedhosseini,Anelia
Angelova,XiaohuaZhai,NeilHoulsby,andRaduSoricut. Pali-x: Onscalingupamultilingualvisionand
languagemodel,2023a.
XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,SebastianGoodman,
AdamGrycner,BasilMustafa,LucasBeyer,AlexanderKolesnikov,JoanPuigcerver,NanDing,KeranRong,
HassanAkbari,GauravMishra,LintingXue,AshishThapliyal,JamesBradbury,WeichengKuo,Mojtaba
Seyedhosseini,ChaoJia,BurcuKaragolAyan,CarlosRiquelme,AndreasSteiner,AneliaAngelova,Xiaohua
Zhai,NeilHoulsby,andRaduSoricut. Pali: Ajointly-scaledmultilinguallanguage-imagemodel,2023b.
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,
HaoZhang,BanghuaZhu,MichaelJordan,JosephE.Gonzalez,andIonStoica. Chatbotarena: Anopen
platformforevaluatingllmsbyhumanpreference,2024.
AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguagemodeling
withpathways. arXivpreprintarXiv:2204.02311,2022.
MostafaDehghani,YiTay,AlexeyA.Gritsenko,ZheZhao,NeilHoulsby,FernandoDiaz,DonaldMetzler,
andOriolVinyals. Thebenchmarklottery,2021.
Google. Ournext-generationmodel: Gemini1.5. 2024.
GeminiTeamGoogle. Gemini: Afamilyofhighlycapablemultimodalmodels,2023.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacobSteinhardt.
Measuringmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.
18HuiHuang,YingqiQu,JingLiu,MuyunYang,andTiejunZhao. Anempiricalstudyofllm-as-a-judgefor
llmevaluation: Fine-tunedjudgemodelsaretask-specificclassifiers,2024.
TomKocmiandChristianFedermann. Largelanguagemodelsarestate-of-the-artevaluatorsoftranslation
quality,2023.
GeraldKrug. RotgelbesFelsenland: KletterführerMitteldeutschland: Felsen,SteinbrücheundKletteranlagenum
HalleundLeipzig. Geoquest,2014.
HugoLaurençon,LucileSaulnier,LéoTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,Thomas
Wang,SiddharthKaramcheti,AlexanderM.Rush,DouweKiela,MatthieuCord,andVictorSanh. Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments,2023.
ZhenLi,XiaohanXu,TaoShen,CanXu,Jia-ChenGu,andChongyangTao.Leveraginglargelanguagemodels
fornlgevaluation: Asurvey,2024.
HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024. URLhttps://llava-vl.github.io/blog/
2024-01-30-llava-next/.
YujieLu,DongfuJiang,WenhuChen,WilliamWang,YejinChoi,andBillYuchenLin. Wildvisionarena:
Benchmarking multimodal llms in the wild, February 2024. URL https://huggingface.co/spaces/
WildVision/vision-arena/.
IanRMcKenzie,AlexanderLyzhov,MichaelPieler,AliciaParrish,AaronMueller,AmeyaPrabhu,Euan
McLean,AaronKirtland,AlexisRoss,AlisaLiu,etal. Inversescaling: Whenbiggerisn’tbetter. arXiv
preprintarXiv:2306.09479,2023.
OpenAI. Gpt-4v(ision)systemcard. 2023.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
arXivpreprintarXiv:1910.10683,2019.
TeamReka. RekaCore,Flash,andEdge: Aseriesofpowerfulmultimodallanguagemodels,2024.
RylanSchaeffer. Pretrainingonthetestsetisallyouneed,2023.
LeoTronchon,HugoLaurençon,andVictorSanh. Apowerful8bvision-languagemodelforthecommunity,
2024. URLhttps://huggingface.co/blog/idefics2.
PatVerga,SebastianHofstatter,SophiaAlthammer,YixuanSu,AleksandraPiktus,ArkadyArkhangorodsky,
MinjieXu,NaomiWhite,andPatrickLewis. Replacingjudgeswithjuries: Evaluatingllmgenerationswith
apanelofdiversemodels,2024.
JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXivpreprintarXiv:2109.01652,
2021.
JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,Maarten
Bosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels. Transactionson
MachineLearningResearch(TMLR),2022.
XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,BoyuanZheng,
Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive
multi-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertagi. InProceedingsof
CVPR,2024.
LianminZheng, Wei-LinChiang, YingSheng, SiyuanZhuang, ZhanghaoWu, YonghaoZhuang, ZiLin,
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonzalez,andIonStoica. Judgingllm-as-a-
judgewithmt-benchandchatbotarena,2023.
19