On the test-time zero-shot generalization of vision-language models:
Do we really need prompt learning?
MaximeZanella* IsmailBenAyed
UCLouvain UMons E´TSMontre´al
code: https://github.com/MaxZanella/MTA
Abstract instance,givenasetofcandidateclasses,onecancreatetex-
tual descriptions, with the so-called prompt [34], p = ‘’a k
The development of large vision-language models, no- photo of a [class ]”, and get its corresponding em-
k
tablyCLIP,hascatalyzedresearchintoeffectiveadaptation bedding representation t = θ (p ) with the language
k t k
techniques, with a particular focus on soft prompt tuning. encoder. Similarly, an image x is projected in the same
Conjointly, test-time augmentation, which utilizes multiple embedding space f = θ (x) using the visual encoder.
v
augmented views of a single image to enhance zero-shot Then, one can classify this image by measuring the sim-
generalization, is emerging as a significant area of inter- ilarity between these two encoded modalities and predict-
est. This has predominantly directed research efforts to- ingtheclasscorrespondingtothemostsimilarembedding,
ward test-time prompt tuning. In contrast, we introduce a kˆ =argmax ftt .
k k
robustMeanShiftforTest-timeAugmentation(MTA),which
Despite their impressive capabilities, these models still
surpassesprompt-basedmethodswithoutrequiringthisin-
encountersubstantialchallengesandmayyieldunsatisfac-
tensivetrainingprocedure. ThispositionsMTAasanideal
toryresponsesincomplexsituations[17,46]. Theseissues
solution for both standalone and API-based applications.
areparticularlypronouncedwhenconfrontedwiththeprag-
Additionally,ourmethoddoesnotrelyonadhocrules(e.g.,
matic constraints of real-world scenarios, where labeled
confidencethreshold)usedinsomeprevioustest-timeaug-
data can be scarce (i.e., few-shot scenarios [52]) or com-
mentationtechniquestofiltertheaugmentedviews.Instead,
pletelyabsent(i.e., zero-shotscenarios[28]), thuslimiting
MTA incorporates a quality assessment variable for each
theirbroader usage. Consequently, therehas beena grow-
view directly into its optimization process, termed as the
inginterestinenhancingthetest-timegeneralizationfacul-
inliernessscore. Thisscoreisjointlyoptimizedwithaden-
tiesofthesevision-languagemodels[14,16,36,38,40,45].
sity mode seeking process, leading to an efficient training-
Empiricalfindingsindicatingthatimprovedtextualdescrip-
and hyperparameter-free approach. We extensively bench-
tionscanpositivelyimpactzero-shotpredictions[46]have
mark our method on 15 datasets and demonstrate MTA’s
sparkedinterestinrefiningpromptqualityfordownstream
superiority and computational efficiency. Deployed easily
tasks. Originating in the NLP community [23, 26, 49],
as plug-and-play module on top of zero-shot models and
soft prompt learning, which utilizes learnable continuous
state-of-the-art few-shot methods, MTA shows systematic
tokens as input [29], has rapidly gained popularity. Build-
andconsistentimprovements.
ing on this momentum, CoOp [68] stands out as the sem-
inal work for prompt tuning in vision-language models.
Since then, prompt tuning has appeared as the promi-
1.Introduction
nent approach for adapting vision-language models [63]
acrossbothunsupervised[14,24,38]andfew-shotscenar-
Vision-language models, pretrained on vast sets of image-
ios[3,5,12,35,59,67–69].
text pairs, have emerged as powerful tools for learning
cross-modal representations [25, 30, 46, 60, 62, 63]. The In parallel, test-time augmentation, which has been ex-
joint feature space of visual and textual features enables tensively used in the computer vision community [32, 50,
zero-shot recognition, without any task-specific data. For 64], is now emerging in the vision-language field, with a
focusonprompttuning[14,36,38]. Insteadofexploitinga
*Correspondingauthor:maxime.zanella@uclouvain.be singleimagex,test-timeprompttuningtechniquesleverage
ThisworkwaspartlysupportedbytheWalloonRegion(ServicePublicde
multiple embeddings (f ) , each derived from a dif-
WallonieRecherche,Belgium)undergrantn°2010235(ARIACbyDigi- p 1≤p≤N
talWallonia4.ai). ferentaugmentedview(x p) 1≤p≤N ofthesameoriginalim-
1
4202
yaM
3
]VC.sc[
1v66220.5042:viXraagex. Afterwards,thepromptisoptimizedbyforcingcon- 2. We report comprehensive evaluations and comparisons
sistencyofthepredictionsamongthesedifferentviews[38]. to the existing test-time prompt tuning techniques on
Thefinalclassificationstepisthenperformedbycomputing 15 datasets, showing MTA’s highly competitive perfor-
thesimilaritybetweentheoriginalimageencodingandthe mances,althoughitoperatesinlimitedaccess(i.e.,final
optimizedtextualembeddingt∗,kˆ =argmax ftt∗.These embedding) and training-free mode. This makes MTA
k k k
novel research directions underscore the increasing atten- suitableforbothstandaloneandAPI-basedapplications.
tion in enhancing these models’ robustness, especially in 3. Deployed easily atop current state-of-the-art few-shot
zero-shotscenarios,throughdataaugmentationattest-time. learningmethods,MTAbringsconsistentimprovements,
Alongside this expanding literature, we ask the following abenefitnotobservedwithtest-timeprompttuning.
question: Can we improve the image representation f di-
rectlyintheembeddingspace,achievingsuperiorresultsin 2.Relatedworks
awaythatismoreefficientthanprompttuning?
Vision-languagemodelsadaptation. Largescalevision-
Concurrently,therehasbeenasurgeintheuseofpropri-
language models have shown excellent results in several
etary and closed APIs that encapsulate advanced machine
vision tasks [63]. This success has created interest in de-
learning functionalities, often termed black boxes due to
veloping adaptation techniques that capitalize their gen-
their limited transparency, offering little insight into their
eral knowledge [57]. Among these, prompt tuning [29]
internalmechanismsorarchitectures. Yet, theyarecrucial
hasemergedastheprimarymethodforadaptingCLIP-like
in executing a wide spectrum of tasks in vision and NLP,
models, at test-time based on data augmentations [14, 36,
introducing new challenges in model adaptation [51]. The
38] or with few labeled samples [3, 5, 12, 35, 59, 67–69].
field of NLP, in particular, has seen an emerging literature
CoOp[68]optimizeslearnablecontinuoustokensattached
onfew-shotadaptationofblack-box models[8], drivenby
to the class name, while CoCoOp [67] trains a neural net-
therealitythatlarge-scalemodels(e.g.,GPTfamily[2,41],
worktogenerateinstance-conditionedtokensbasedonthe
Palm[6])areonlyaccessibleviaAPIsandtheirpretrained
image. FurthereffortsincludeProGrad[69],whichguides
weights are not publicly available. Optimizing prompts,
promptstowardpredefinedhandcraftedonesbasedongra-
which necessitates gradient computation from output back
dients,whereasPLOT[5]alignslearnedpromptswithfiner-
toinput,amemory-intensiveandtime-consumingprocess,
grainedvisualfeaturesviaanoptimaltransportformulation.
isimpracticalinthecontextofAPI-reliantapplications. In
Beyond soft prompt tuning, other strategies involve using
contrast, our approach does not require extra assumptions
hierarchicalwordstructurestocreatemoresemanticallyre-
about the model’s internal states or architecture, making it
fined class descriptions [16, 40], or exploiting other large
suitableforblack-boxapplications.
scalemodelstogeneratemoredetailedprompts[45,54,66]
ornewimagesbydiffusionmechanisms[14,66].
Contributions. Inthiswork,weintroducearobustmulti- Contrastingly, methods such as CLIP-Adapter [15] offer
modal MeanShift Test-time Augmentation (MTA), which an alternative strategy by learning feature adapters. How-
enhances the zero-shot generalization of CLIP models, ever, therehasbeenlimitedeffortindevelopingblack-box
leveragingdifferentaugmentedviewsofagivenimage.Un- methods [42], which can effectively capitalize the knowl-
like current prompt tuning solutions, which rely on heavy edge of these models while only accessing their final em-
training procedures and ad hoc thresholds to discard de- beddingstate. Examplesincludezero-shotpredictionwith
generatedviews, MTAusesonlythefinalembeddingstate parameter-free plug-in attention [18], or few-shot settings
anddirectlyintegratesaninliernessassessmentoftheaug- withTip-Adapter[65]usingacachemodel.
mented views into its optimization process. Our objective OurexperimentsdemonstratethatourrobustMeanShiftal-
functionisefficientlysolvableusingiterativeblockcoordi- gorithm significantly enhances the performances in zero-
nate descent updates, and relaxes the need for training the shotscenarios,withoutrelyingonsoftprompttuning,while
model’s parameters or prompts. Empirically, we demon- respecting the black-box constraints. Additionally, we re-
stratethatMTAsurpassesstate-of-the-artprompt-tuningal- port increased performances when applied atop of various
ternatives,whilebeingtimeandmemoryefficient. Ourkey aforementioned few-shot methods, without requiring fur-
contributionsareasfollows: thertrainingorhyperparametertuning.
1. We propose a robust MeanShift formulation, which au-
tomaticallymanagesaugmentedviewsintest-timeaug- Test-time augmentation. Data augmentation during
mentation scenarios by optimizing inlierness variables. training is widely recognized for its capacity to enhance
Used as a versatile plug-and-play tool, MTA improves modelrobustness[20,21]. Also, itsutilityextendstotest-
thezero-shotperformancesofvariousmodelsonalarge timeapplications[32,50,64]. Inparticular, test-timeaug-
varietyofclassificationtasks, withoutanyhyperparam- mentationcanbeusedonasingleimage[64]toadaptmod-
etertuning. els with an entropy minimization term. The latter is of-
2tenusedinthecontextofunsupervisedadaptation[31,55], weintroducealatentassignmentvectory = (y ) ∈
p 1≤p≤N
butisdeployeddifferentlyinthisaugmentationsetting,en- ∆N−1, with∆N−1 = {y ∈ [0,1]N | 1ty = 1}theprob-
forcingconsistentpredictionsacrossthevariousaugmented abilitysimplex,andproposetominimizethefollowingob-
views. This idea is further developed for vision-language jectivefunction:
models with test-time prompt tuning (TPT) [38], where a
promptisoptimizedtomakeconsistentpredictionsamong minL(m,y) s.t. y∈∆N−1 with
m,y
lightaugmentationsinspiredbyAugmix[20].DiffTPT[14]
N
builds up on this work by adding generated images from (cid:88) λ(cid:88)
L(m,y)=− y K(f −m)− w y y
Stable Diffusion [48] to acquire more diverse views. Both p p 2 p,q p q
p=1 p,q
works show improvements when selecting a subset of the
−λ H(y) (1)
augmented views. Specifically, TPT utilizes only the 10% y
most confident views, and DiffTPT measures the similar-
Inthefollowing,wedescribethenotationsoccurringinour
ity with the original image, keeping the unconfident but
modelinEq. (1),aswellastheeffectofeachofitsterms:
correctly classified augmentations. We also demonstrate
that filtering the augmented views can substantially im-
RobustKDE(firstterm) K isakernelfunctionmeasur-
provetest-timeaugmentationtechniques. Additionally,our
ing a robust affinity between f and m, e.g., a Gaussian
method does not rely on arbitrary hard thresholds or rules p
kernel [4]: K(f − m) ∝ exp(−∥f − m∥2/h2), where
as in TPT and DiffTPT; instead, we directly integrate the p p
h is the kernel bandwidth. When variables y are fixed to
weightingoftheaugmentedviewsinouroptimizationpro- p
1 ∀p, the first term reduces to the kernel density estimate
cedurethankstoinliernessvariables.
(KDE) of the distribution of features at point m. Clearly,
3.Robustmulti-modalMeanShift minimizingthistermw.r.tmyieldsthestandardMeanShift
algorithmforfindingthemodeofthedensity(i.e.,thepoint
Similarlytothetest-timegeneralizationsettingrecentlyin- maximizing it). In our model, the additional latent vari-
troduced in TPT [38], let us assume that we are given a abley evaluatestheinliernessofthepth augmentedview,
p
set of image samples (x p) 1≤p≤N, which correspond to N i.e., the model’s belief in f
p
being an inlier or not within
distinct augmented views of a given test sample x. It is the whole set of augmented-view embeddings (f ) .
p 1≤p≤N
important to note that our method is applicable on top of Scorey ∈ [0,1]ishighwhenthemodelconsidersthepth
p
any type of augmentations. Let f = θ (x ) denote the
p v p sample as an inlier, enabling it to contribute more in the
vision-encoded feature embedding corresponding to aug-
KDEevaluationin(1),andsmall(closerto0)otherwise.
mentedsamplex ,θ beingthevisionencoderoftheCLIP
p v
pre-trainedmodel.Astraightforwardwaytousetheensem-
Text-knowledge guided quadratic term (second term)
bleofaugmentationsistoperformthezero-shotprediction
Thistermencouragessampleswithnearbytext-basedzero-
basedontheirmeanembedding,therebygivingexactlythe
shotpredictionstohavesimilarinliernessscoresy .Specif-
same importance to all augmented samples, independently p
ically,weconstructthepairwiseaffinitiesw inthesecond
of the structure of the data. However, the augmentations p,q
termof(1)fromboththetextandvisionembeddingsasfol-
may include degenerated views, which correspond to out-
lows. Let s ∈ RK denotes the text-driven softmax pre-
liers, e.g., in the form of isolated data points or small re- p
diction based on the zero-shot text embedding for the pth
gions with little structure within the feature space. Such
sample,i.e.,thekthcomponentofs isgivenby:
outliersmaybiasglobalstatisticslikethemean. p
3.1.Formulation s = expl p,k ; l =τftt (2)
p,k (cid:80)K
expl
p,k p k
We hypothesize that robust statistics like the modes of the j=1 p,j
densityofthesetofaugmentedviewscouldprovidebetter
where τ is the temperature scaling parameter of the CLIP
representations. Augmented views presenting major char-
model. Affinitiesw aregivenby:
acteristics of the concept to be recognized are likely to be p,q
projectedclosetotheoriginalimageandclosetoeachoth- w =sts (3)
p,q p q
ers. Thismotivatesourformulation,whichcouldbeviewed
as a novel robust and multi-modal extension of the popu-
TheShannonentropy(thirdterm) H(y)istheShannon
larMeanShiftalgorithm[9],anunsupervisedprocedurefor
entropydefinedoversimplexvariablesasfollows:
findingthemodesofthedistributionofagivensetofsam-
ples. In our case, we explicitly model and handle the po-
N
tential presence of outliers in the estimation of the kernel H(y)=−(cid:88) y lny (4)
p p
density of the set of feature vectors (f ) . To do so,
p 1≤p≤N p=1
3Thistermactsasabarrierfunction, forcinglatentvariable
y tostaywithintheprobabilitysimplex. Furthermore,this
entropic regularizer is necessary to avoid a trivial solution
minimizing the quadratic term (i.e., y = 1 ∃p.), as it
p
pushes the solution toward the middle of the simplex (i.e.,
y =1/N ∀p).
p
(a)Inliernessscoresdistribution.Bluegradationrepresentstheinlierness
3.2.Blockcoordinatedescentoptimization
score:thedarker,thehigher.
Ourobjectivein(1)dependsontwotypesofvariables: m
and y. Therefore, we proceed with block-coordinate de-
scent alternating two sub-steps: one optimizing (1) w.r.t y
and keeping density modes m fixed, while the other mini-
mizes(1)w.r.tmwiththeinliernessvariablesfixed.
Optimization w.r.t y via the concave-convex procedure
(b)Inlierness-densitymodeseeking. Thewhitearrowrepresentstheup-
When m is fixed, our objective L(y,m) could be mini-
datedirectionofthetraditionalMeanShift;insteadourrobustMeanShift
mizedefficientlyw.r.ty usingtheConcave-ConvexProce- followstheorangeones.
dure (CCCP) [61], with convergence guarantee. At each
iteration, we update the current solution y(n) as the mini- Figure1. InterpretationofEqs. (6)and(8)in(a)and(b)respec-
tively.OurrobustMeanShiftalternativelysolvesthese2equations
mumofatightupperboundonL,whichensurestheobjec-
untilconvergence.ApseudocodeisavailableinAppendixC.
tivedoesnotincrease. Forthesumofconcaveandconvex
functions, as for our sub-problem, the CCCP replaces the
concave part by its linear first-order approximation at the Thesolutionto(7)couldbeobtainedbythefollowingfixed-
currentsolution,whichisatightupperbound,whilekeep- pointiterations:
ing the convex part. For (1), the quadratic term could be
(cid:80)N y K(f −ml)f
written as ytWy, with W = [w i,j]. It is easy to see that ml+1 =g(ml)= p=1 p p p (8)
this term is concave as affinity matrix W is positive semi- (cid:80)N y K(f −ml)
p=1 p p
definite,whereastheremainingpartofLisconvex. There-
fore, we replace this quadratic term by ytWty(n), obtain- This yields a Cauchy sequence {ml} l∈N, which con-
ing,uptoanadditiveconstant,thefollowingtightbound:
verges to a unique value: m∗ = lim l→−∞ml+1 =
lim l→−∞g(ml) = g(lim l→−∞ml) = g(m∗), and m∗ is
c (cid:88)N theuniquesolutionofthefixed-pointin(7)(AppendixB).
L(y,m)≤− y K(f −m)−λytWty(n)−λ H(y)
p p y
p=1
Finalprediction Theclasspredictioniscomputedbythe
(5)
cosinesimilaritybetweenthemodeminimizingourobjec-
Solving the Karush-Kuhn-Tucker (KKT) conditions for
minimizing bound (9), s.t. simplex constraint y ∈ ∆N−1, tive in Eq. (1), i.e., m∗ obtained at convergence, and the
givesthefollowingupdatesfory: encodedpromptsofeachclassk,i.e.,t k:
exp(cid:16) (K(f −m)+λ(cid:80)N w y(n))/λ (cid:17) kˆ =argmax (m∗)tt k
p q=1 p,q q y k
y(n+1) =
p (cid:80)N exp(cid:16) (K(f −m)+λ(cid:80)N w y(n))/λ (cid:17)
j=1 j q=1 j,q q y
Interpretation Eqs. (6)and(8)canbenicelyinterpreted
(6)
inFigure1. Sub-figure1ashowshowtheinliernessscores
whichhavetobeiterateduntilconvergence. Thecomplete arespreading. Adatapointisgivenahighinliernessscore
derivationofEq. (6)isprovidedinAppendixA. if it is close to the mode and/or close to other data points
withhighinliernessscores. Therefore,inliernessscoresare
Optimization w.r.t m via fixed-point iterations This spreadingiterativelyfromdatapointsclosetothemodeto-
sub-step fixes y, and minimizes the objective in (1) w.r.t wardotherdatapointscontrolledbytheiraffinityrelations.
thedensity modes m. Setting thegradientof L w.r.t m to Sub-figure1bshowstheupdatedirectionfollowedbytradi-
0yieldsthefollowingnecessaryconditionforaminimum, tionalMeanShift,i.e.,updates(8)butwithfixedy p =1, ∀p.
whichtakestheformofafixed-pointequation: The orange arrows follow the update of our robust Mean-
Shift,i.e. jointupdatesin(6)and(8). Ourmodeupdateis
(cid:80)N
y K(f −m)f
m−g(m)=0; g(m)= p=1 p p p (7) directed toward dense regions with high inlierness scores,
(cid:80)N
y K(f −m) thusavoidingcloseregionswithfeworisolateddatapoints.
p=1 p p
4Table1.Zero-shotmethodsonImageNetdatasets.Weuse”a photo of a[class ]”asprompt.Majorityvoteofthe80handcrafted
k
promptsof[46]isusedforfinalpredictionwhenEnsembleisspecified.CoOpisapretrainedprompton16-shotsImageNet.Wehighlight
thebestandsecondbestresultsbyboldingandunderliningthem,respectively.✓ fortraining-freemethodsattest-time,✗ otherwise.
Method ImageNet -A -V2 -R -Sketch Average
CLIP[46] ✓ 66.73 47.87 60.86 73.98 46.06 59.11
CLIP+Ensemble[46] ✓ 68.38 49.95 62.1 77.38 47.96 61.15
TPT[38] ✗ 68.94 54.63 63.41 77.04 47.97 62.40
MTA(Ours) ✓ 69.29 57.41 63.61 76.92 48.58 63.16
MTA+Ensemble(Ours) ✓ 70.08 58.06 64.24 78.33 49.61 64.06
CoOp[68] ✓ 71.51 49.71 64.20 75.21 47.99 61.72
TPT+CoOp[38] ✗ 73.61 57.85 66.69 77.99 49.59 65.14
MTA+CoOp(Ours) ✓ 73.99 59.29 66.97 78.2 49.96 65.68
Table2. Zero-shotmethodson10fine-grainedclassificationdatasets. Wehighlightthebestresultbybolding. +E.meansthatmajority
votewiththe80handcraftedpromptsof[46]isusedforfinalprediction.
Method SUN397 Aircraft EuroSAT Cars Food101 Pets Flower102 Caltech101 DTD UCF101 Average
CLIP[46] 62.59 23.67 42.01 65.48 83.65 88.25 67.44 93.35 44.27 65.13 63.58
CLIP+E.[46] 66.02 23.88 48.8 66.14 83.83 88.42 67.8 93.87 46.04 66.77 65.16
TPT[38] 65.41 23.1 42.93 66.36 84.63 87.22 68.86 94.12 46.99 68.00 64.76
MTA(Ours) 64.98 25.32 38.71 68.05 84.95 88.22 68.26 94.13 45.59 68.11 64.63
MTA+E.(Ours) 66.67 25.2 45.36 68.47 85.00 88.24 68.06 94.21 45.9 68.69 65.58
Table 3. Comparison of augmentation strategies: RandomCrop Vs Diffusion-based. Note that the test set of each dataset is reduced
to 1000 samples for computation reasons and batch size of 128 was used as in the DiffTPT paper [14] (64 randomly cropped and 63
diffusion-generatedimagesforDiffusion).HencereportedperformancecanvaryfromTable1.Wehighlightthebestresultbybolding.
Augmentation Method ImageNet -A -V2 R -Sketch Average
TPT[38] 68.15 51.23 66.17 76.88 49.31 62.35
RandomCrop
MTA 69.11 55.27 65.71 77.48 50.23 63.56
DiffTPT[14] 67.83 53.43 65.18 76.85 50.2 62.7
Diffusion
MTA 69.18 54.5 64.81 76.82 51.09 63.28
4.Experimentalsettings 4.2.Implementationdetails
4.1.Datasets Nohyperparametertuning. IntheMeanShiftalgorithm,
thebandwidthisasensitivehyperparameterthatcancause
To evaluate our proposed MTA, we follow the setting thealgorithmtobecomestuckinsmall,locallydenseareas
of previous works [38, 68]. We assess our method ifsettoolow,orescapesignificantdenseregionsifsettoo
on ImageNet [11] and its four variants (ImageNet- high [10]. We utilize the Gaussian kernel [4] as described
A [22], ImageNet-V2 [47], ImageNet-R [21], ImageNet- in Section 3 and adopt a variable bandwidth [10] in which
Sketch[56])tomeasurerobustnesstonaturaldomainshifts. each point is assigned a unique bandwidth value h2. The
p
Additionally, we also consider 10 datasets for fine-grained bandwidthofapointisestimatedwitharatioρofitsclos-
classificationofscenes(SUN397[58]),aircrafttypes(Air- estneighbors: h2
p
= ρ(N1 −1)(cid:80) q∈Ip∥f p−f q∥2withρsetto
craft [37]), satellite imagery (EuroSAT [19]), automobiles 0.3inspiredby[44]. Here, I representsthesetofindices
p
(StanfordCars [27]), food items (Food101 [1]), pet breeds corresponding to the neighbors of point p. Initial guess of
(OxfordPets [43]), flowers (Flower102 [39]), general ob- the mode can also impact the final solution and is set to
jects(Caltech101[13]),textures(DTD[7])andhumanac- theembeddingoftheoriginal(i.e.,non-augmented)image.
tions(UCF101[53]).Thesediversedatasetsprovideacom- Affinities are based on the text prediction as described in
prehensivebenchmarkforvisualclassificationtasks. Section3.Finally,λandλ aresetto4and0.2respectively
y
5TPT MTA (Ours) Table 4. Averaged top-1 accuracy on ImageNet and its 4 vari-
ants for different visual encoders of CLIP. Hyperparameters are
Forward Pass
Optimization keptidenticalacrossdatasetsandvisualencoderarchitectures.+E.
0.6 Second Forward Pass
standsforEnsemble.
0.4
Architecture ResNet-50 ResNet-101 ViT-B/32 ViT-B/16 ViT-L/14
0.2
CLIP 44.11 49.48 50.65 59.11 70.65
Ensemble 46.23 51.58 52.23 61.12 72.6
0
RN50 RN101ViT-B/32ViT-B/16ViT-L/14 RN50 RN101ViT-B/32ViT-B/16ViT-L/14 TPT 47.23 52.16 53.27 62.40 73.67
Visual Encoder Architecture MTA 47.22 53.15 55.17 63.16 73.88
MTA+E. 48.35 54.23 56.1 64.06 74.71
Figure 2. Runtime in seconds per image on ImageNet for TPT
andMTAwith5differentbackbones:RN50(ResNet-50),RN101
(ResNet-101), ViT-B/32, ViT-B/16 and ViT-L/14. Experiences
wereperformedonasingleA10040GbGPU.
5.Zero-shot
MTA globally outperforms TPT while respecting the
andremainfixedforeveryCLIPvisualencoderanddataset. black-box constraints and running nearly three times
This leads to a hyperparameter-free method across all ex- asfast. Table1demonstratesMTA’sstableimprovement
periments. Unless otherwise mentioned, we report top-1 over TPT on ImageNet and its variants with both the ba-
accuracyusingtheViT-B/16backbone. sic ”a photo of a [class ]” and CoOp’s pretrained
k
prompt. Moreover, Table 2 indicates that, except for the
Comparison methods. We employ the term ensemble EuroSAT dataset [19], MTA consistently enhances base-
forthemajorityvoteamongthe80predefinedhandcrafted line performance across fine-grained datasets. It also sur-
prompts of CLIP [46]. For the zero-shot scenario, TPT is passes TPT when combined with ensembling, which re-
performed with one step as suggested in their work [38]. spectstheblack-boxassumption. TPTisnotabletooutper-
We evaluate the zero-shot setting [28] with the basic ”a form the majority vote strategy in average for the 10 fine-
photo of a [class ]” as prompt initialization and grainedclassificationdatasets,questioningitsapplicability
k
with the ensemble for final prediction if mentioned. Note foralargervarietyoftasks. Finally, wecompareruntimes
thatcombiningensemblewithTPTisnotstraightforwardas onImageNetinFigure2. MTAisnearlythreetimesfaster
thegoalistousetheoptimizedpromptforprediction,hence than TPT, notably due to the quick optimization step and
we only use ensemble with our approach. The weights of theremovedsecondforwardpass.
CoOp [68] are from 16 shots Imagenet with 4 tokens. For
the few-shot scenario, the number of tokens of CoOp [68] MTAbenefitsfromimprovedpromptstrategy. Table1
andProGrad[69]arespecifiedinthecaptionofeachtable. and 2 concur in suggesting that the improvements brought
Asproposedin[33],toestablishafaircomparisonbetween by our approach complement those from refined prompt
few-shot methods, we limit the number of validation sam- strategies, i.e., basic ensemble of handcrafted prompts or
plestomin(n,4)wherenisthenumberoftrainingshots, soft pretrained prompts. This may imply that leveraging
notably for Tip-Adapter and Tip-Adapter-F [65] that tune both modalities in the same optimization process, as sug-
hyperparameters. Because of the randomness inherent in gested in [33] for the few-shot setting, could further en-
test-time augmentation or some training procedures (e.g., hanceperformance.
prompttuning),eachreportedperformanceistheaveraged
top-1 accuracy of three different random seeds. More de-
MTA bridges the transferability gap of pretrained
tailedperformancesareavailableinAppendixD.
prompts due to better image representation. We use
CoOppretrainedonImageNettomeasurethedomaingen-
Test-timeaugmentation. Ourproposedmethodusesran- eralizationabilityofourmethodasin[68]inTable1. Al-
domcropping(RandomCrop)asaugmentedviewgenerator thoughthegainsofCoOpovertheensemblingstrategyre-
identicaltotheoneinTPT[38]. Wealsostudyattheendof mainambiguous,thecombinationofCoOpwithMTAsig-
Section5theimpactofamorecomplexdataaugmentation nificantly enhances accuracy for all datasets. This notable
basedondiffusionasdoneinDiffTPT[14]. NotethatTPT improvementsuggeststhatthemodefoundbyMTAisable
and DiffTPT are using slightly stronger augmentations for tohighlightthegeneralizationabilityofpretrainedprompts.
the10fine-grainedclassificationdatasetsinspiredbyAug- This illustrates an additional use case of MTA, which can
Mix[20]. Inourcase,foramorerealisticzero-shotsetting, beusedinsynergywithfine-tuningforaspecifictask(e.g.,
wekeepthesimpleRandomCropforallthe15datasets. prompttuning). ThisaspectisemphasizedinSection6.
6
)egami/.ces(
emiT
noitatupmoCImageNet SUN397 Aircraft EuroSAT
90
74 76
45
72 74 40 80
70 72 35 70
70 30
68 60
CoOp (M=4) 68 CoOp (M=4) 25 CoOp (M=4) CoOp (M=4)
66 C Co oO Op p ( (M M= =4 4) ) + + M TPT TA 66 C Co oO Op p ( (M M= =4 4) ) + + M TPT TA 20 C Co oO Op p ( (M M= =4 4) ) + + M TPT TA 50 C Co oO Op p ( (M M= =4 4) ) + + M TPT TA
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
Cars Food101 Pets Caltech101
85 85 93 96
92
80 84 91 95
90
75 83 94
89
70 CoOp (M=4) 82 CoOp (M=4) 88 CoOp (M=4) 93 CoOp (M=4)
CoOp (M=4) + MTA CoOp (M=4) + MTA 87 CoOp (M=4) + MTA CoOp (M=4) + MTA
CoOp (M=4) + TPT CoOp (M=4) + TPT CoOp (M=4) + TPT CoOp (M=4) + TPT
1 2 4 8 16 1 2 4 8 16 86 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
UCF101 Flower102 DTD
Average
85 95 70 82.5
80.0
65
80 90 77.5
85 60 75.0
75 55 72.5 CoOp (M=16)
80 CoOp (M=4) 50 70.0 CoOp (M=16) + MTA
70 C Co oO Op p ( (M M= =4 4) ) + MTA 75 C Co oO Op p ( (M M= =4 4) ) + MTA 45 C Co oO Op p ( (M M= =4 4) ) + MTA 67.5 C Co oO Op p ( (M M= =4 1) 6 )+ + M TT PA T
CoOp (M=4) + TPT 70 CoOp (M=4) + TPT CoOp (M=4) + TPT 65.0 CoOp (M=4) + TPT
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
Figure3.Few-shotlearningresultsonthe10fine-graineddatasetsandImageNet.WecompareMTAandTPTwhenaddedontopofCoOp
prompts(M=4tokens)forincreasingnumberofshots. Averagedtop-1accuracyoverthe11datasetsisshownonthebottomright, we
additionallyshowtheaveragedtop-1accuracyforCoOpwithM=16tokens.
MTA generalizes across visual encoder architectures abouttherelianceoftheimagesgeneratedbydiffusion,we
with the same hyperparameters. Generalization across increase ρ to a slightly less restrictive value of 0.5 for the
variousmodelarchitecturesisadesirableattributeofzero- purpose of this experiment. Otherwise, we keep the same
shot methods, as it eliminates the need for laborious and hyperparametersλandλ anddonottreatthetwokindsof
y
computationally demanding hyperparameter tuning. This augmentation differently. As demonstrated in Table 3, our
becomes increasingly critical in the context of the current method surpasses DiffTPT on average, further evidencing
scaling trend, where model complexity is rapidly expand- itseffectivenessacrossabroadrangeofapplications.
ing [2, 6]. Table 4 shows consistent improvement on the
baseline for five different visual backbones of CLIP with
6.Few-shot
fixed hyperparameters. It demonstrates the generalization
abilityofourmethodacrossarchitecturesandmodelscales.
Fine-grainedlearnedpromptsbenefitfromMTAbutnot
fromTPT. AsdepictedinFigure3,MTAimprovesCoOp
MTA is applicable to other data augmentation strate- performances across shots and datasets with notably Air-
gies. WeexplorethecompatibilityofMTAwithdifferent craft40.07%→ 44.33%(+4.26%),EuroSAT83.53%→
typesofdataaugmentation.Specifically,wefollowthepro- 87.38%(+3.85%),andCars79.06%→ 82.66%(+3.6%)
tocol of DiffTPT [14] to generate augmented views from with16shots.Incontrast,TPTgenerallydiminishesperfor-
cropping and diffusion model [48]. We employ a batch mance across most datasets, highlighting its limitations in
of128imagescomposedoftheoriginalone, 63diffusion- enhancingpromptsforfine-grainedtasks. Whiletheaccu-
generated and 64 randomly cropped views. Since gener- racyfor16tokensisonaveragehigherforMTAandlower
ating images by diffusion is more computationally inten- for TPT, we report the 4 tokens results in alignment with
sivethanRandomCrop(generating63imagestakesapprox- the TPT paper [38] to maintain fairness. Detailed perfor-
imately2minutesonaA10040GbGPU),wepresentthese mances for 16 tokens and individual datasets are available
resultsseparately. InspiredbytheobservationsofDiffTPT inAppendixD.
7
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poTTable5.Improvementoffew-shotlearningmethodsonImageNet 65
whenMTAisaddedontop. CoOpandProGradareusing16to-
kens. ∆highlightsthegain. ✓ fortraining-freemethods,✗ oth- 64
erwise. 63
62
Shots 1 2 4 8 16 61 Zero-shot Ensemble
Tip-Adapter[65] ✓ 68.94 69.18 69.75 70.15 70.51
Tip-Adapter-F[65] ✗ 69.36 69.95 70.74 71.82 73.39 60 MTA
C Pro oO Gp ra[ d68 [6] 9] ✗ ✗ 66 75 .. 07 1 6 66 9. .9 07 6 6 78 0. .8 13 5 7 70 1. .5 27 5 7 71 2. .8 17 4 59 Zero-shot MTA + Ens.
Tip-Adapter+MTA ✓ 71.05 71.12 71.4 71.9 72.05 816 32 64 128 256
Number of augmented views
∆ (+2.11) (+1.94) (+1.65) (+1.75) (+1.54)
Tip-Adapter-F+MTA ✗ 71.35 71.48 72.17 73.18 74.23
∆ (+1.99) (+1.53) (+1.43) (+1.36) (+0.84) Figure4. Averagedtop-1accuracyofMTAwithandwithoutma-
CoOp+MTA ✗ 67.82 69.1 71.05 72.85 74.20 jority vote for final prediction on the 5 ImageNet variants with
∆ (+2.12) (+2.13) (+2.22) (+2.28) (+2.33)
increasingnumberofaugmentedviews.
ProGrad+MTA ✗ 69.27 71.39 72.50 73.66 74.41
∆ (+2.26) (+2.33) (+2.35) (+2.41) (+2.27)
Affinitymeasure. Ratherthanusingtheaffinitymeasure
based on text described in Section 3, one could use only
MTA can be applied atop few-shot learning methods. visualfeatures (f ) tocompute theaffinitybetween
p 1≤p≤N
AsdemonstratedinTable5,applyingMTAtoprompt-based two embeddings w = ftf . For the sake of fairness,
p,q p q
andadapter-basedfew-shotlearningmethodsonImageNet we reperform a comprehensive hyperparameter search (λ
resultsinnotableperformanceimprovements. Specifically, ranging from 0.1 to 10 and λ from 0.1 to 0.5). Best per-
y
prompt-based methods see an average gain exceeding 2% formance is reported in Table 6. We observe that vision-
across shots, with adapter-based methods showing slightly basedaffinitymeasuredegradesperformanceincomparison
lower yet significant improvements. Indeed, our affinity totext-basedaffinitymeasure.
term, outlined in Eq. 3, can greatly benefit from refined
prompts. Nevertheless, we can notice that a training-free Table 6. Ablation study on two main components of MTA: the
approach, Tip-Adapter, combined with MTA is competi- inliernessscoresandtheaffinitymeasure. Reportedvalueisthe
tivewithprompttuningmethodswithoutMTA.Thiscould averagedtop-1accuracyoverthe15datasetsstudiedinthiswork.
present a compelling trade-off: opting for more intensive
computationaleffortsduringtrainingorattest-time. Baseline CLIP 62.09
MeanShift(noinliernessscores) 59.93
Filtering
7.Ablationstudy Confidencethresh.(10%) 63.27
Strategy
Inliernessscores 64.14
Numberofaugmentedviews. InFigure4, theaccuracy
Affinity Vision-based 63.15
for MTA and MTA + Ensemble increases as the number Measure Text-based 64.14
ofaugmentedviewsgrowsuntilreachingaplateauaround
128.Evenwhenwerestrictthenumberofaugmentedviews
to16,ourmethodstillbringsabout2.3%gain,whichmakes 8.Conclusion
it a useful tool for lighter applications, up until 4.4% gain
for batches of 128 views. We can observe similar gains Inthiswork,wehaveinvestigatedanovelapproachtohan-
whencombinedwithensembleofprompts,with1.9%gain dletest-timeaugmentationforvision-languagemodels.Our
forbatchesof16upuntil3.1%forlargerbatches. MeanShiftforTest-timeAugmentation(MTA)isbasedon
a robust generalization of a well-known mode seeking al-
gorithm, and operates solely on the final embeddings, in a
Inlierness scores. We compare performance with equal
training-free manner. Extensive experiments demonstrate
weights on each augmented view (i.e., traditionnal Mean-
that our method not only surpasses test-time prompt tun-
Shift),withaconfidencethresholdasinTPT[38]andwith
ing alternatives but also runs significantly faster. With-
our inlierness formulation in Table 6. The inlierness for-
out any other requirements, MTA can easily be deployed
mulationyieldsbetterperformanceonaverageoverthe15
in a zero-shot manner and atop few-shot learning meth-
datasets. Note that the relatively high score of confidence
ods. Webelieveourworkcouldserveasastartingpointto
thresholdismainlyduetopeakperformanceonImageNet-
broadenthecurrentresearchfocusonimprovingzero-shot
A, a trend not consistent on other datasets, see Appendix
vision-language models, and to investigate more efficient
D.Additionally, AppendixCcontainsastudyofλandλ
y
approaches,beyondpromptlearning.
showingtheirinterdependentrelationandimportance.
8
)%(
ycaruccA
1-poTReferences [13] LiFei-Fei,RobFergus,andPietroPerona. Learninggener-
ative visualmodels from fewtraining examples: An incre-
[1] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
mentalbayesianapproachtestedon101objectcategories.In
Food-101–mining discriminative components with random
2004conferenceoncomputervisionandpatternrecognition
forests. In Computer Vision–ECCV 2014: 13th European
workshop,pages178–178.IEEE,2004. 5
Conference, Zurich, Switzerland, September 6-12, 2014,
[14] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and
Proceedings,PartVI13,pages446–461.Springer,2014. 5
WangmengZuo. Diversedataaugmentationwithdiffusions
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
foreffectivetest-timeprompttuning. InProceedingsofthe
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
IEEE/CVF International Conference on Computer Vision,
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
pages2704–2714,2023. 1,2,3,5,6,7
guagemodelsarefew-shotlearners. Advancesinneuralin-
[15] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
formationprocessingsystems,33:1877–1901,2020. 2,7
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
[3] Adrian Bulat and Georgios Tzimiropoulos. Lasp: Text-to-
Clip-adapter: Better vision-language models with feature
text optimization for language-aware soft prompting of vi-
adapters. International Journal of Computer Vision, pages
sion&languagemodels. InProceedingsoftheIEEE/CVF
1–15,2023. 2
Conference on Computer Vision and Pattern Recognition
[16] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang,
(CVPR),pages23232–23241,2023. 1,2
Ming-HsuanYang,HartwigAdam,LaurentItti,BalajiLak-
[4] MiguelA´.Carreira-Perpin˜a´n. Gaussianmean-shiftisanem shminarayanan,andJiapingZhao.Improvingzero-shotgen-
algorithm. IEEETransactionsonPatternAnalysisandMa- eralization and robustness of multi-modal models. In Pro-
chineIntelligence,29:767–776,2007. 3,5 ceedingsoftheIEEE/CVFConferenceonComputerVision
[5] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, andPatternRecognition,pages11093–11101,2023. 1,2
Yongming Rao, and Kun Zhang. Plot: Prompt learning [17] GabrielGoh, NickCammarata, ChelseaVoss, ShanCarter,
with optimal transport for vision-language models. In The MichaelPetrov,LudwigSchubert,AlecRadford,andChris
EleventhInternationalConferenceonLearningRepresenta- Olah.Multimodalneuronsinartificialneuralnetworks.Dis-
tions,2022. 1,2 till,6(3):e30,2021. 1
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, [18] ZiyuGuo,RenruiZhang,LongtianQiu,XianzhengMa,Xu-
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul pengMiao,XumingHe,andBinCui. Calip: Zero-shoten-
Barham, Hyung Won Chung, Charles Sutton, Sebastian hancementofclipwithparameter-freeattention.InProceed-
Gehrmann, et al. Palm: Scaling language modeling with ingsoftheAAAIConferenceonArtificialIntelligence,pages
pathways. arXivpreprintarXiv:2204.02311,2022. 2,7 746–754,2023. 2
[7] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,Sammy [19] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Mohamed, andAndreaVedaldi. Describingtexturesinthe DamianBorth. Eurosat: Anoveldatasetanddeeplearning
wild. In Proceedings of the IEEE conference on computer benchmarkforlanduseandlandcoverclassification. IEEE
visionandpatternrecognition,pages3606–3613,2014. 5 Journal of Selected Topics in Applied Earth Observations
[8] Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Victor andRemoteSensing,12(7):2217–2226,2019. 5,6
Storchan, MyriamTami, IsmailBenAyed, CelineHudelot, [20] DanHendrycks*,NormanMu*,EkinDogusCubuk,Barret
andPabloPiantanida. Transductivelearningfortextualfew- Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Aug-
shot classification in api-based embedding models. arXiv mix:Asimplemethodtoimproverobustnessanduncertainty
preprintarXiv:2310.13998,2023. 2 under data shift. In International Conference on Learning
[9] DorinComaniciuandPeterMeer. Meanshiftanalysisand Representations,2020. 2,3,6
applications. In Proceedings of the seventh IEEE inter- [21] DanHendrycks, StevenBasart, NormanMu, SauravKada-
nationalconferenceoncomputervision, pages1197–1203. vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu,
IEEE,1999. 3,13 SamyakParajuli,MikeGuo,etal.Themanyfacesofrobust-
[10] DorinComaniciu,VisvanathanRamesh,andPeterMeer.The ness:Acriticalanalysisofout-of-distributiongeneralization.
variable bandwidth mean shift and data-driven scale selec- In Proceedings of the IEEE/CVF International Conference
tion. In Proceedings Eighth IEEE International Confer- onComputerVision,pages8340–8349,2021. 2,5
enceonComputerVision.ICCV2001,pages438–445.IEEE, [22] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
2001. 5 hardt, and Dawn Song. Natural adversarial examples. In
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, ProceedingsoftheIEEE/CVFConferenceonComputerVi-
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage sionandPatternRecognition,pages15262–15271,2021. 5
database.In2009IEEEConferenceonComputerVisionand [23] Zhong,DFriedman,andDChen.Factualprobingis[mask]:
PatternRecognition,pages248–255,2009. 5 Learningvs.learningtorecall. InConferenceoftheNorth
[12] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian AmericanChapteroftheAssociationforComputationalLin-
Bulat,VictorGuilhermeTurrisidaCosta,CeesGMSnoek, guistics(NAACL),2021. 1
Georgios Tzimiropoulos, and Brais Martinez. Variational [24] Tony Huang, Jack Chu, and Fangyun Wei. Unsupervised
prompt tuning improves generalization of vision-language promptlearningforvision-languagemodels. arXivpreprint
models. arXivpreprintarXiv:2210.02390,2022. 1,2 arXiv:2204.03649,2022. 1
9[25] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh, [38] Shu Manli, Nie Weili, Huang De-An, Yu Zhiding, Gold-
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom stein Tom, Anandkumar Anima, and Xiao Chaowei. Test-
Duerig. Scaling up visual and vision-language representa- time prompt tuning for zero-shot generalization in vision-
tion learning with noisy text supervision. In International languagemodels. InNeurIPS,2022. 1,2,3,5,6,7,8
conferenceonmachinelearning,pages4904–4916.PMLR, [39] Maria-Elena Nilsback and Andrew Zisserman. Automated
2021. 1 flowerclassificationoveralargenumberofclasses. In2008
[26] ZJiang, FXu, JAraki, andGNeubig. Howcanweknow SixthIndianconferenceoncomputervision,graphics&im-
whatlanguagemodels know. InAssociationforComputa- ageprocessing,pages722–729.IEEE,2008. 5
tionalLinguistics(ACL),2020. 1 [40] Zachary Novack, Julian McAuley, Zachary Chase Lipton,
[27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. and Saurabh Garg. Chils: Zero-shot image classification
3dobjectrepresentationsforfine-grainedcategorization. In withhierarchicallabelsets. InInternationalConferenceon
Proceedings of the IEEE international conference on com- MachineLearning,pages26342–26362.PMLR,2023. 1,2
putervisionworkshops,pages554–561,2013. 5 [41] OpenAI. Gpt-4v(ision)technicalworkandauthors,2023. 2
[28] ChristophHLampert,HannesNickisch,andStefanHarmel- [42] Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios
ing. Learning to detect unseen object classes by between- Tzimiropoulos. Black box few-shot adaptation for vision-
class attribute transfer. In 2009 IEEE conference on com- languagemodels. InProceedingsoftheIEEE/CVFInterna-
putervisionandpatternrecognition,pages951–958.IEEE, tionalConferenceonComputerVision,pages15534–15546,
2009. 1,6 2023. 2
[43] OmkarMParkhi, AndreaVedaldi, AndrewZisserman, and
[29] BrianLester,RamiAl-Rfou,andNoahConstant.Thepower
CVJawahar. Catsanddogs. In2012IEEEconferenceon
ofscaleforparameter-efficientprompttuning.arXivpreprint
computervisionandpatternrecognition,pages3498–3505.
arXiv:2104.08691,2021. 1,2
IEEE,2012. 5
[30] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian-
[44] Fabian Pedregosa, Gae¨l Varoquaux, Alexandre Gramfort,
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
language-image pre-training. In Proceedings of the
etal. Scikit-learn: Machinelearninginpython. Journalof
IEEE/CVF Conference on Computer Vision and Pattern
machinelearningresearch,12(Oct):2825–2830,2011. 5
Recognition,pages10965–10975,2022. 1
[45] SarahPratt,IanCovert,RosanneLiu,andAliFarhadi.What
[31] JianLiang,DapengHu,andJiashiFeng. Dowereallyneed
does a platypus look like? generating customized prompts
toaccessthesourcedata? sourcehypothesistransferforun-
for zero-shot image classification. In Proceedings of the
supervised domain adaptation. In International conference
IEEE/CVF International Conference on Computer Vision,
onmachinelearning,pages6028–6039.PMLR,2020. 3
pages15691–15701,2023. 1,2
[32] JianLiang,RanHe,andTieniuTan. Acomprehensivesur-
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
veyontest-timeadaptationunderdistributionshifts. arXiv
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
preprintarXiv:2303.15361,2023. 1,2
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[33] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and Krueger, and Ilya Sutskever. Learning transferable visual
Deva Ramanan. Multimodality helps unimodality: Cross- modelsfromnaturallanguagesupervision,2021. 1,5,6,13
modal few-shot learning with multimodal models. In Pro-
[47] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
ceedingsoftheIEEE/CVFConferenceonComputerVision
VaishaalShankar. Doimagenetclassifiersgeneralizetoim-
andPatternRecognition,pages19325–19337,2023. 6
agenet? In International conference on machine learning,
[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- pages5389–5400.PMLR,2019. 5
roakiHayashi,andGrahamNeubig. Pre-train,prompt,and [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
predict: Asystematicsurveyofpromptingmethodsinnat- Patrick Esser, and Bjo¨rn Ommer. High-resolution image
urallanguageprocessing. ACMComputingSurveys,55(9): synthesis with latent diffusion models. In Proceedings of
1–35,2023. 1 theIEEE/CVFConferenceonComputerVisionandPattern
[35] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Recognition(CVPR),pages10684–10695,2022. 3,7
andXinmeiTian. Promptdistributionlearning. InProceed- [49] TShin,LoganR.L.IVRazeghi,Y,EWallace,andSSingh.
ingsoftheIEEE/CVFConferenceonComputerVisionand Autoprompt: Eliciting knowledge from language models
PatternRecognition,pages5206–5215,2022. 1,2 withautomaticallygeneratedprompts. InEmpiricalMeth-
[36] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. odsinNaturalLanguageProcessing(EMNLP),2020. 1
Swapprompt: Test-time prompt adaptation for vision- [50] Connor Shorten and Taghi M Khoshgoftaar. A survey on
language models. In Thirty-seventh Conference on Neural imagedataaugmentationfordeeplearning. Journalofbig
InformationProcessingSystems,2023. 1,2 data,6(1):1–48,2019. 1,2
[37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew [51] IreneSolaiman.Thegradientofgenerativeairelease:Meth-
Blaschko, and Andrea Vedaldi. Fine-grained visual classi- odsandconsiderations,2023. 2
fication of aircraft. arXiv preprint arXiv:1306.5151, 2013. [52] YishengSong,TingWang,PuyuCai,SubrotaKMondal,and
5 JyotiPrakashSahoo. Acomprehensivesurveyoffew-shot
10learning: Evolution, applications, challenges, and opportu- [66] RenruiZhang,XiangfeiHu,BohaoLi,SiyuanHuang,Han-
nities. ACMComputingSurveys,2023. 1 qiuDeng,YuQiao,PengGao,andHongshengLi. Prompt,
[53] KhurramSoomro,AmirRoshanZamir,andMubarakShah. generate,thencache: Cascadeoffoundationmodelsmakes
Ucf101:Adatasetof101humanactionsclassesfromvideos strongfew-shotlearners. InProceedingsoftheIEEE/CVF
inthewild. arXivpreprintarXiv:1212.0402,2012. 5 Conference on Computer Vision and Pattern Recognition,
[54] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. pages15211–15222,2023. 2
Sus-x: Training-freename-onlytransferofvision-language [67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
models.InProceedingsoftheIEEE/CVFInternationalCon- wei Liu. Conditional prompt learning for vision-language
ferenceonComputerVision,pages2725–2736,2023. 2 models. InIEEE/CVFConferenceonComputerVisionand
[55] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- PatternRecognition(CVPR),2022. 1,2
shausen,andTrevorDarrell.Tent:Fullytest-timeadaptation [68] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
by entropy minimization. In International Conference on Liu. Learningtopromptforvision-languagemodels. Inter-
LearningRepresentations,2021. 3 nationalJournalofComputerVision(IJCV),2022. 1,2,5,
[56] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P 6,8
Xing. Learningrobustglobalrepresentationsbypenalizing [69] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Han-
localpredictivepower.AdvancesinNeuralInformationPro- wangZhang. Prompt-alignedgradientforprompttuning. In
cessingSystems,32,2019. 5 ProceedingsoftheIEEE/CVFInternationalConferenceon
[57] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, ComputerVision,pages15659–15669,2023. 1,2,6,8
MikeLi,SimonKornblith,RebeccaRoelofs,RaphaelGon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, et al. Robust fine-tuning of zero-shot models.
In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pages7959–7971,2022. 2
[58] JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,
and Antonio Torralba. Sun database: Large-scale scene
recognitionfromabbeytozoo. In2010IEEEcomputerso-
cietyconferenceoncomputervisionandpatternrecognition,
pages3485–3492.IEEE,2010. 5
[59] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-
languageprompttuningwithknowledge-guidedcontextop-
timization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 6757–
6767,2023. 1,2
[60] LeweiYao,RunhuiHuang,LuHou,GuansongLu,Minzhe
Niu,HangXu,XiaodanLiang,ZhenguoLi,XinJiang,and
ChunjingXu.Filip:Fine-grainedinteractivelanguage-image
pre-training. arXivpreprintarXiv:2111.07783,2021. 1
[61] AlanLYuilleandAnandRangarajan. Theconcave-convex
procedure(cccp). Advancesinneuralinformationprocess-
ingsystems,14,2001. 4
[62] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages18123–18133,2022. 1
[63] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.
Vision-language models for vision tasks: A survey. arXiv
preprintarXiv:2304.00685,2023. 1,2
[64] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo:
Test time robustness via adaptation and augmentation. In
AdvancesinNeuralInformationProcessingSystems,pages
38629–38642.CurranAssociates,Inc.,2022. 1,2
[65] RenruiZhang,WeiZhang,RongyaoFang,PengGao,Kun-
chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-
adapter: Training-freeadaptionofclipforfew-shotclassifi-
cation. InEuropeanConferenceonComputerVision,pages
493–510.Springer,2022. 2,6,8
11A.ProofofEquation6 withK(f −ml)=exp(−∥fp−ml ∥2),forsomeh>0.We
p h
define:
Here, we give details of the derivation of the y-updates in
Eq. (6)fromtheupperbound(majorizingfunction)in(9).
Given a solution y(n) at iteration n, the goal is to find the k(x)=exp(−x) (15)
nextiteratey(n+1)thatminimizesthefollowingtightupper N
(cid:88)
bound,s.t. simplexconstrainty∈∆N−1: ul = y K(f −ml) (16)
p p
λ p=1
B(y,y(n))=−ytk− 2ytWty(n)−λ yH(y) (9) (cid:88)N
vl = y K(f −ml)f (17)
p p p
wherek=(K(f −m)) .
p 1≤p≤N p=1
The objective function of (9) is strictly convex. Taking
intoaccountthesimplexconstraintony,theassociatedLa-
grangianreads:
Step 1: First, let us prove that {ul} l∈N is a Cauchy se-
quence. Recall that in a metric space, a convergent se-
L(y,y(n))=−ytk−λ ytWty(n)−λ H(y)+γ(yt1 −1) quence is necessarily a Cauchy sequence. Therefore, we
2 y N only need to show that ul is convergent (i.e bounded and
(10)
strictlymonotonic).
where γ is the Lagrange multiplier for simplex constraint
Noticethatforx>0,0≤k(x)≤1. Therefore:
y ∈ ∆N−1 and1 isthevectorofones. Notethatwedo
N
not impose explicitly the constraints on the non-negativity
of the components of y because these are implicitly en-
ul
=(cid:88)N
y
k(∥f p−ml
∥2) (18)
forcedwiththeentropicbarriertermin(9),i.e.,−λ H(y). p h
y
p=1
Now,computingthegradientofL(y,y(n))w.r.tyyields:
N
(cid:88)
∇ L(y,y(n))=−k−λWty(n)+(γ+λ )1 +λ log(y) ≤ y ≤1 (19)
y y N y p
(11) p=1
By setting the gradients of (11) to 0, we get the optimal
solution: Therefore,ulisboundedbetween0and1.Now,letusstudy
(cid:32) N (cid:33) theconsecutivedifferences∆l =ul+1−ul:
(cid:88)
y(n+1) =exp (K(f −m)+λ w y(n))/λ .
p p p,q q y
exp(−(γ+λ y))
q=1
(12) ∆l
=(cid:88)N
y
p(cid:20) k(∥f p− hm 2l+1∥2 )−k(∥f p− h2ml∥2 )(cid:21)
p=1
Using this expression in the simplex constraint
(20)
(cid:80) y(n+1) = 1 enables to recover the following ex-
p p
pressionofexp(γ+λ y): Becausekisconvex,onecansaythat∀a,b∈R:
N (cid:32) N (cid:33)
(cid:88) (cid:88)
exp (K(f j −m)+λ w j,qy q(n))/λ y k(a)−k(b)≥k′(b)(a−b) (21)
j=1 q=1
Plugging this expression back in (12), we get the final up- Andbecausek′(b)=−k(b)inourcase,oneendsupwith:
dates:
(cid:16) (cid:17)
exp (K(f −m)+λ(cid:80)N w y(n))/λ k(a)−k(b)≥k(b)(b−a) (22)
p q=1 p,q q y
y(n+1) =
p (cid:80)N exp(cid:16) (K(f −m)+λ(cid:80)N w y(n))/λ (cid:17)
j=1 j q=1 j,q q y
Appliedwitha =
∥fp−ml+1∥2
andb =
∥fp−ml∥2
, onecan
(13) h2 h2
obtain:
B.Cauchyandconvergentsequenceproof
Let us consider iteration l, with the associated current in- ∆l
≥(cid:88)N
y K(f
−ml)(cid:20) ∥f p−ml∥2
−
∥f p−ml+1∥2(cid:21)
lierness scores y. Let us prove that {ml} l∈N is a Cauchy p=1 p p h2 h2
sequence. Recalltherecursiverelation:
N
ml+1 = (cid:80)N p=1y pK(f p−ml)f p (14) = h1 2 (cid:88) p=1y pK(f p−ml)(cid:2) ∥ml∥2−∥ml+1∥2
(cid:80)N p=1y pK(f p−ml) −2<ml,f >+2<ml+1,f >(cid:3)
p p
12vl Table7. Effectofλandλ ontheImageNetdataset. Reported
Now is time to recall recursive relation ml+1 = . By y
ul valueisthetop-1accuracyaveragedover3randomseeds.
simplyexpanding,onecanendupwith:
1 (cid:20) ∥vl∥2 ∥vl∥2(cid:21) λ
λy
0.01 0.05 0.1 0.2 0.4 0.8 1.6 3.2 10 100
(Me→ an∞
Shift)
∆l ≥ ∥ml∥2ul− −2<ml,vl >+2
0 66.7 66.7 66.8 68.3 65.8 65.3 65.6 65.9 66.0 66.1 66.1
h2 ul ul
0.5 66.7 66.7 66.8 68.7 67.7 66.8 66.4 66.2 66.1 66.1 -
= 1 ul(cid:2) ∥ml∥2−2<ml,ml+1 >+∥ml+1∥2(cid:3) 1 2 6 66 6. .7 8 6 66 6. .7 8 6 66 7. .9 1 6 68 9. .9 1 6 68 8. .2 8 6 67 8. .4 0 6 66 7. .9 4 6 66 7. .5 0 6 66 6. .3 5 6 66 6. .1 1 - -
h2 4 66.6 66.5 66.9 69.3 69.1 68.6 68.0 67.5 66.8 66.2 -
8 62.0 62.5 64.2 68.7 69.3 69.0 68.5 68.1 67.2 66.3 -
1 16 57.3 58.5 61.0 65.8 69.1 69.3 68.9 68.5 67.7 66.4 -
= ul∥ml−ml+1∥2 (23)
h2
Therefore, ∆l > 0, which shows that {ul} l∈N is strictly cropping (RandomCrop) and the original image) in all our
increasing. Thisconcludestheproofthatul isaconvergent experimentsexceptinTable3tobeconsistentwithDiffTPT
sequence,andthereforeaCauchyone. which uses 128 augmented views (63 from diffusion, 64
from random cropping and the original image). Table 7
Step 2: Now, on top of concluding the proof that showstheinterdependencyofλandλ y andtheroleofthe
{ul} l∈NisaCauchysequence,Eq. (23)alsooffersaninter- inliernessscores:asλ yapproaches0,ittendstowardapeak
estingrelationbetween{∆l} l∈N andthesequenceofinter- selection and trivial solutions; conversely, as λ y grows, it
est{ml} l∈N,whichwecanuse. Indeed,foranyl 0,m∈N, tendstoMeanShiftwithuniforminliernessscores.
wecansumEq. (23):
Algorithm1ModeseekingMeanShift[9]
l0(cid:88)+m
1
l0(cid:88)+m
Require: h > 0thebandwidth,K akernelfunction(e.g.,
∆l ≥ ul∥ml−ml+1∥2 (24)
h2 Gaussiankernel),m0afirstestimateofthemode,aset
l=l0 l=l0
ofdatapoints(f ) ,athresholdvalueϵ
p 1≤p≤N
≥ul0 l0(cid:88)+m ∥ml−ml+1∥2 (25) 1: l←0
h2 2: whilel=0or∥ml−ml−1∥≥ϵdo
≥ul0 ∥l m=l l0
0+m−ml0∥2 (26)
3: ml+1← (cid:80) (cid:80)N p N p= =1 1K K(f (p fp− −m ml) lf )p ▷modeupdate
h2 4: l←l+1
5: endwhile
Where Eq. (25) follows because {ul} l∈N is strictly in- 6: m←ml−1
creasing,andEq. (26)followsfromthetriangleinequality.
7: returnm
Now, the left-hand side of Eq. (24) can be reduced to
(cid:80)l0+m∆l = ul0+m+1 −ul0. But because we proved in
l=l0
Step1that{ul} l∈N wasaCauchysequence,thisdifference D.Additionalresults
is bounded by a constant. This concludes the proof that
{ml} l∈NisitselfaCauchysequenceintheEuclideanspace. Zero-shot(Section5). WereportdetailedresultsforTa-
ble1,Table2andTable3withaveragetop-1accuracyand
Step3: Wejustprovedthat{ml} l∈N wasaCauchyse- standarddeviationinTable8,Table9andTable10respec-
quence. Therefore {ml} l∈N can only converge to a single tively.
valuem∗. Wenowusethecontinuityoffunctiong tocon-
cludethatm∗hastobeasolutionoftheinitialequation(7):
Few-shot (Section 6). Additional results for CoOp with
m∗ = lim ml+1 = lim g(ml) (27) 16 tokens are depicted in Figure 5. A similar trend to that
l→∞ l→∞ shown in Figure 3 is evident, with more pronounced per-
=g(lim ml)=g(m∗) (28) formance degradation observed for TPT. On the contrary,
l→∞ MTAbenefitsfromthesemoreperformantprompts.
C.FurtherdetailsonMTA
Ablation study (Section 7). Details for the 15 datasets
WesummarizethetraditionalmodeseekingMeanShiftpro-
forthefilteringstrategyablationstudyofTable6aregiven
cedure,uponwhichourapproachisbased,inAlgorithm1.
in Table 11. With the exception of ImageNet-A, the con-
Moreover, our robust multi-modal MeanShift for test-time
fidence threshold strategy consistently demonstrates lower
augmentation,namedMTA,ispresentedinAlgorithm2in
performancescomparedtoourinliernessformulation.
a non-vectorized manner to highlight each operation. The
handcrafted prompts [46] for ensembling are listed in Ta-
ble 12. We use N=64 augmented views (63 from random
13Algorithm2MTAwithGaussiankernel
Require: Asetofaugmentedembeddings(f ) withf beingtheoriginalimage,asetofclassembeddings(t ) ,
p 1≤p≤N 1 k 1≤k≤K
athresholdvalueϵ,τ thetemperaturevariableoftheCLIPmodel.
1: w p,q ←Affinity(f p,f q,(t k) 1≤k≤K,τ) ∀p,q ∈{1,...,N} ▷SeeAlgorithm3
2: h2 p← ρ(N1 −1)(cid:80) q∈Ip∥f p−f q∥2 ∀p∈{1,...,N} ▷I ptheclosestneighborsofp,ρsetto0.3
3: m←f 1 ▷modeinitialization
4: y p← N1 ∀p∈{1,...,N} ▷Initialinliernessscoresuniform
5: while(1)and(2)notconvergeddo
6: n←0
7: y0←y
8: whilen=0or∥yn−yn−1∥≥ϵdo
9: y p(n+1)← (cid:80)N je =x 1p e( x( pK (cid:16)( (f Kp− (fm j−)+ mλ )+(cid:80) λN q (cid:80)=1 N qw =p 1, wqy jq ,( qn y) q) (n/ )λ )y /) λy(cid:17) ∀p∈{1,...,N} ▷(1)inliernessscoresupdate
10: n←n+1
11: endwhile
12: y←yn−1
13: l←0
14: m0←m
15: whilel=0or∥ml−ml−1∥≥ϵdo
16: ml+1← (cid:80) (cid:80)N p
N
p= =1 1y yp pK K(f (p fp− −m ml) lf )p ▷(2)modeupdate
17: l←l+1
18: endwhile
19: m←ml−1
20: endwhile
21: returnargmax k mtt k ▷returnpredictionbasedonthemode
Algorithm3Affinitymeasurebasedonpredictions
1: functionAFFINITY(f p,f q,(t k) 1≤k≤K,τ)
2: ifp=qthen
3: return0
4: endif
5: l p,k ←τf ptt k ; l q,k ←τf qtt k ∀k ∈{1,...,K} ▷similaritywithclass k
6: s p,k ← (cid:80)K je =x 1p el xp p,k lp,j; s q,k ← (cid:80)K je =x 1p el xq p,k lq,j ∀k ∈{1,...,K} ▷Softmaxoperation
7: w p,q ←st ps q
8: returnw p,q
9: endfunction
14Table8.DetailsofTable1withaveragedtop-1accuracyandstandarddeviationcomputedover3randomseeds.
Method ImageNet -A -V2 -R -Sketch Average
68.94 54.63 63.41 77.04 47.97 62.40
TPT ✗
±.06 ±.21 ±.12 ±.02 ±.05 ±.03
69.29 57.41 63.61 76.92 48.58 63.16
MTA ✓
±.09 ±.15 ±.07 ±.13 ±.05 ±.07
70.08 58.06 64.24 78.33 49.61 64.06
MTA+Ensemble ✓
±.03 ±.07 ±.09 ±.11 ±.06 ±.06
73.61 57.85 66.69 77.99 49.59 65.14
TPT+CoOp ✗
±.17 ±.34 ±.25 ±.69 ±.34 ±.1
73.99 59.29 66.97 78.2 49.96 65.68
MTA+CoOp ✓
±.18 ±.12 ±.25 ±.76 ±.46 ±.25
Table9.DetailsofTable2withaveragedtop-1accuracyandstandarddeviationcomputedover3randomseeds.
Method SUN397 Aircraft EuroSAT Cars Food101 Pets Flower102 Caltech101 DTD UCF101 Average
65.41 23.1 42.93 66.36 84.63 87.22 68.86 94.12 46.99 68.00 64.76
TPT
±.03 ±.39 ±.2 ±.31 ±.03 ±.19 ±.32 ±.21 ±.31 ±.22 ±.05
64.98 25.32 38.71 68.05 84.95 88.22 68.26 94.13 45.59 68.11 64.63
MTA
±0 ±.25 ±.22 ±.16 ±.06 ±.07 ±.08 ±.02 ±.18 ±.11 ±.02
66.67 25.2 45.36 68.47 85.00 88.24 68.06 94.21 45.9 68.69 65.58
MTA+E.
±.05 ±.37 ±.16 ±.08 ±.03 ±.07 ±.2 ±.21 ±.09 ±.15 ±.05
Table10.DetailsofTable3withaveragedtop-1accuracyandstandarddeviationcomputedover3randomseeds.
Augmentation Method ImageNet -A -V2 R -Sketch Average
68.15 51.23 66.17 76.88 49.31 62.35
TPT
±.3 ±.31 ±.2 ±.2 ±.2 ±.05
RandomCrop
69.11 55.27 65.71 77.48 50.23 63.56
MTA
±.4 ±.15 ±.4 ±.36 ±.4 ±.11
67.83 53.43 65.18 76.85 50.2 62.7
DiffTPT
±.23 ±.64 ±.43 ±.11 ±.36 ±.19
Diffusion
69.18 54.5 64.81 76.82 51.09 63.28
MTA
±.4 ±.31 ±.1 ±.26 ±.4 ±.07
Table11. DetailsofTable6forinliernessscoresablationstudy. (1)MeanShift(noinliernessscores)(2)confidencethresh. (10%)(3)
Inliernessscores.IstandsforImageNet,AforImageNet-A,VforImageNet-V2,RforImageNet-RandKforImageNet-Sketch.Reported
valuesareaveragedtop-1accuracyandstandarddeviationcomputedover3randomseeds.
I A V R K SUN397 Aircraft EuroSAT Cars Food101 Pets Flower102 Caltech101 DTD UCF101 Average
66.1 48.05 60.29 67.69 40.59 63.74 25.11 24.72 66.53 83.12 85.24 66.69 91.52 44.35 65.16 59.93
(1)
±.03 ±.14 ±.23 ±.1 ±.05 ±.09 ±.1 ±.08 ±.2 ±.09 ±.22 ±.25 ±.11 ±.24 ±.05 ±.07
68.26 60.66 63.3 76.14 47.59 63.56 24.52 36.13 67.59 83.39 85.83 66.51 92.69 45.45 67.41 63.27
(2)
±.07 ±.19 ±.13 ±.08 ±.05 ±.11 ±.24 ±.04 ±.09 ±.14 ±.32 ±.42 ±.1 ±.1 ±.39 ±.04
69.29 57.41 63.61 76.92 48.58 64.98 25.32 38.71 68.05 84.95 88.22 68.26 94.13 45.59 68.11 64.14
(3)
±.09 ±.15 ±.07 ±.13 ±.05 ±0 ±.25 ±.22 ±.16 ±.06 ±.07 ±.08 ±.02 ±.18 ±.11 ±.01
15ImageNet SUN397 Aircraft EuroSAT
90
74 76
45
72 74 40 80
70 72 35 70
70 30
68 60
CoOp (M=16) 68 CoOp (M=16) 25 CoOp (M=16) CoOp (M=16)
66 C Co oO Op p ( (M M= =1 16 6) ) + + M TPT TA 66 C Co oO Op p ( (M M= =1 16 6) ) + + M TPT TA 20 C Co oO Op p ( (M M= =1 16 6) ) + + M TPT TA 50 C Co oO Op p ( (M M= =1 16 6) ) + + M TPT TA
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
Cars Food101 Pets Caltech101
85 85 93 96
92
80 84 91 95
90
75 83 94
89
70 CoOp (M=16) 82 CoOp (M=16) 88 CoOp (M=16) 93 CoOp (M=16)
CoOp (M=16) + MTA CoOp (M=16) + MTA 87 CoOp (M=16) + MTA CoOp (M=16) + MTA
CoOp (M=16) + TPT CoOp (M=16) + TPT CoOp (M=16) + TPT CoOp (M=16) + TPT
86
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
UCF101 Flower102 DTD
Average
85 95 70 82.5
80.0
65
80 90 77.5
85 60 75.0
75 55 72.5
80 50 70.0
70 C Co oO Op p ( (M M= =1 16 6) ) + MTA 75 C Co oO Op p ( (M M= =1 16 6) ) + MTA 45 C Co oO Op p ( (M M= =1 16 6) ) + MTA 67.5 C Co oO Op p ( (M M= =1 16 6) ) + MTA
CoOp (M=16) + TPT 70 CoOp (M=16) + TPT CoOp (M=16) + TPT 65.0 CoOp (M=16) + TPT
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
Number of shots Number of shots Number of shots Number of shots
Figure5.AdditionalresultsforFigure3withM=16tokensfortheCoOppretrainedprompts.
16
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poTTable12.The80handcraftedpromptsusedformajorityvote.
”a photo of a[].”,”a bad photo of a[].”,”a photo of many[].”,”a sculpture of a[].”,
”a photo of the hard to see[].”,”a low resolution photo of the[].”,”a rendering of a[].”,
”graffiti of a[].”,”a bad photo of the[].”,”a cropped photo of the[].”,,”a tattoo of a[].”,
”the embroidered[].”,”a photo of a hard to see[].”,”a bright photo of a[].”,
”a photo of a clean[].”,”a photo of a dirty[].”,”a dark photo of the[].”,
”a drawing of a[].”,”a photo of my[].”,”the plastic[].”,”a photo of the cool[].”,
”a close-up photo of a[].”,”a black and white photo of the[].”,”a painting of the[].”,
”a painting of a[].”,”a pixelated photo of the[].”,”a sculpture of the[].”,
”a bright photo of the[].”,”a cropped photo of a[].”,”a plastic[].”,
”a photo of the dirty[].”,”a jpeg corrupted photo of a[].”,”a blurry photo of the[].”,
”a photo of the[].”,”a good photo of the[].”,”a rendering of the[].”,
”a[]in a video game.”,”a photo of one[].”,”a doodle of a[].”,
”a close-up photo of the[].”,”the origami[].”,”the[]in a video game.”,
”a sketch of a [].”, ”a doodle of the [].”, ”a origami [].”, ”a low resolution photo of a [].”,
”the toy[].”,”a rendition of the[].”,”a photo of the clean[].”,”a photo of a large[].”,
”a rendition of a[].”,”a photo of a nice[].”,”a photo of a weird[].”,
”a blurry photo of a[].”,”a cartoon[].”,”art of a[].”,”a sketch of the[].”,
”a embroidered[].”,”a pixelated photo of a[].”,”itap of the[].”,
”a jpeg corrupted photo of the[].”,”a good photo of a[].”,”a plushie[].”,
”a photo of the nice[].”,”a photo of the small[].”,”a photo of the weird[].”,
”the cartoon[].”,”art of the[].”,”a drawing of the[].”,”a photo of the large[].”,
”a black and white photo of a [].”, ”the plushie [].”, ”a dark photo of a [].”, ”itap of a [].”,
”graffiti of the[].”,”a toy[].”,”itap of my[].”,”a photo of a cool[].”,
”a photo of a small[].”,”a tattoo of the[].”
17