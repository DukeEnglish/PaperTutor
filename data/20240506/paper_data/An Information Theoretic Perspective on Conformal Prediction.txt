An Information Theoretic Perspective on
Conformal Prediction
AlvaroH.C.Correia FabioValerioMassoli ChristosLouizos† ArashBehboodi†
QualcommAIResearch∗
Amsterdam,TheNetherlands
{acorreia, fmassoli, clouizos, behboodi}@qti.qualcomm.com
Abstract
ConformalPrediction(CP)isadistribution-freeuncertaintyestimationframework
thatconstructspredictionsetsguaranteedtocontainthetrueanswerwithauser-
specifiedprobability. Intuitively,thesizeofthepredictionsetencodesageneral
notionofuncertainty,withlargersetsassociatedwithhigherdegreesofuncertainty.
Inthiswork,weleverageinformationtheorytoconnectconformalpredictionto
othernotionsofuncertainty.Moreprecisely,weprovethreedifferentwaystoupper
boundtheintrinsicuncertainty,asdescribedbytheconditionalentropyofthetarget
variablegiventheinputs,bycombiningCPwithinformationtheoreticalinequalities.
Moreover,wedemonstratetwodirectandusefulapplicationsofsuchconnection
between conformal prediction and information theory: (i) more principled and
effectiveconformaltrainingobjectivesthatgeneralizepreviousapproachesand
enable end-to-end training of machine learning models from scratch, and (ii) a
naturalmechanismtoincorporatesideinformationintoconformalprediction. We
empiricallyvalidatebothapplicationsincentralizedandfederatedlearningsettings,
showingourtheoreticalresultstranslatetolowerinefficiency(averageprediction
setsize)forpopularCPmethods.
1 Introduction
MachineLearning(ML)modelshaverapidlygrowninpopularityandreach,havingnowfounduse
inmanysafety-criticaldomainslikehealthcareAhsanetal.(2022)andautonomousdrivingKuutti
etal.(2020). Intheseareas,predictionsmustbeaccompaniedbyreliablemeasuresofuncertaintyto
ensuresafedecision-making,butunfortunately,mostMLmodelsaredesignedandtrainedtoproduce
onlypointestimates,whichcaptureonlycrudenotionsofuncertaintywithnostatisticalguarantees.
Conformal Prediction (CP) Vovk et al. (2005), in particular its split variant (SCP) Papadopoulos
etal.(2002),hasrecentlygainedinpopularityasaprincipledandscalablesolutiontoequipany,
potentiallyblack-box,modelwithproperuncertaintyestimatesintheformofpredictionsets;inloose
terms,largersetsareassociatedwithhigherdegreesofuncertainty.
Inthiswork,wetakeacloserlookatconformalpredictionthroughthelensofInformationTheory
(IT),establishingaconnectionbetweenconformalpredictionandtheunderlyingintrinsicuncertainty
ofthedata-generatingprocess,ascapturedbytheconditionalentropyH(Y|X)ofthetargetvariable
Y giventheinputfsX. WeproveconformalpredictioncanbeusedtoboundH(Y|X)fromabove
inthreedifferentways: onederivedfromthedataprocessinginequality,whichwedubDPIbound,
andtwocomingfromavariationofFano’sinequalityFano(1949),amodelagnosticone,whichwe
call(simple)Fanobound,andanotherinformedbythepredictivemodelitself,towhichwerefer
asmodel-basedFanobound. Tothebestofourknowledge,theseboundsrepresentthefirstbridge
∗QualcommAIResearch,QualcommTechnologiesNetherlandsB.V(QualcommAIResearchisaninitiative
ofQualcommTechnologies,Inc.).†Equalcontribution.
Preprint.Underreview.
4202
yaM
3
]GL.sc[
1v04120.5042:viXraconnecting information theory and conformal prediction, which we hope will bring new tools to
bothfields. Wealreadypresenttwosuchtoolsinthispaper: (i)weshowourupperboundsserve
asprincipledtrainingobjectivestolearnMLmodelsthataremoreamenabletoSCP,and(ii)we
advanceasystematicwaytoincorporatesideinformationintotheconstructionofpredictionsets. We
empiricallyvalidatethatboththeseapplicationsofourtheoreticalresultsleadtobetterpredictive
efficiency,i.e.,narrower,moreinformativepredictionsets.
Therestofthepaperisorganizedasfollows.InSection2,wefirstintroducethenecessarybackground
toguidethereaderthroughourmaintheoreticalresults. Weintroduceourthreenewupperboundsto
theintrinsicuncertaintyinSection3,andtheirapplicationstoconformaltrainingandsideinformation
inSections4and5,respectively. Thereafter,weexploretherelatedworkinCPandITinSection6,
presentandanalyzeourexperimentalresultsinSection7,andfinallyconcludeinSection8.
2 Background
Inthissectionweintroducetheneededbackgroundonconformalpredictionandlistdecoding(Elias,
1957; Wozencraft, 1958), a branch of information theory that, as we show, is closely related to
conformalpredictionandespeciallyusefulinderivingourmainresults. Westartbyintroducingthe
notationusedthroughoutthepaper. Asusual,wedenoterandomvariablesinuppercaselettersand
theirrealizationinlowercase,e.g.,X = x. Similarly,wereservecalligraphicletters,e.g. X,for
sets. Throughoutthepaper,weuseP ,Q ,... todenoteprobabilitymeasuresonthespaceX. To
X X
simplifythenotation,wesimplyuseP,Q,... whentheunderlyingspaceisclear. Forexample,with
ameasureQ theprobabilityoftheevent{(X,Y):Y ∈C(X)}isdenotedasQ(Y ∈C(X)).
XY
2.1 ConformalPrediction
Conformalprediction(CP)isatheoreticallygroundedframeworkthatprovidespredictionsetswith
finite-sampleguaranteesunderminimaldistribution-freeassumptions. Concretely,givenasetofn
datapoints(X ,Y ) ∈ X ×Y,i = 1,...,ndrawnfromsome(unknown)jointdistributionP ,
i i XY
CPallowsustoconstructsetsC(X)∈Y,suchthatforanewdatapointfromthesamedistribution
(X ,Y )wehavethefollowingguaranteeforatargeterrorrateα∈(0,1)
test test
P(Y ∈C(X ))≥1−α, (1)
test test
wheretheprobabilityisovertherandomnessinthesample{(X ,Y )}n ∪{(X ,Y )}. Inthis
i i i=1 test test
work,wefocusonavariantcalledsplitconformalprediction(SCP)(Papadopoulosetal.,2002)that
gainedpopularityintheMLcommunity,sinceitcanleverageanypre-trainedmodelf :X ×Y →R
intheconstructionofpredictionsets.Inthissetting,theaforesaidndatapointsconstituteacalibration
datasetD ,whichmustbedisjointfromthetrainingdatasetusedtofitthepredictivemodelf.
cal
Thisseparationbetweentrainingandcalibrationdataiswhatgivesthenamesplittothemethod.
ThefirststepinSCPistodefineascorefunctions :X →Y,whichisitselfafunctionofmodelf
f
andcapturesthemagnitudeofthepredictionerroratagivendatapoint. Wethenevaluatethescore
functionateverydatapoint(X ,Y )∈D toobtainacollectionofscores{S }n ,andattesttime
i i cal i i=1
weconstructpredictionsetC(X )as
test
C(X )={y ∈Y :s(X ,y)≤Quantile(1−α;{S }n ∪{∞})}, (2)
test test i i=1
whereQuantile(1−α;{S }n )}isthelevel1−αquantileoftheempiricaldistributiondefinedby
i i=1
{S }n . Thecentralresultinconformalprediction,whichwerestatebelowforcompleteness,proves
i i=1
thatpredictionsetsthusconstructedachievemarginalvalidcoverage,i.e.,satisfy(1).
Theorem2.1((Vovketal.,2005;Leietal.,2018)). If{(X ,Y )}narei.i.d. (oronlyexchangeable),
i i i
thenforanewi.i.d. draw(X ,Y ),andforanyα ∈ (0,1)andforanyscorefunctionssuch
test test
that{S }n arealmostsurelydistinct,thenC(X )asdefinedabovesatisfies
i i=1 test
1−α≤P(Y
test
∈C(X test))≤1−α n, where α
n
=α+1/n+1.
Itisworthnotingthatvalidcoverageisnotsufficient;theuninformativesetpredictorthatalways
outputC(X )=Y triviallysatisfies(1). Wewouldalsolikeourpredictionsetstobeasnarrowas
test
possible,andthatiswhyCPmethodsareoftencomparedintermsoftheir(empirical)inefficiency,i.e.,
(cid:80)
theaveragepredictionsetsize1/|Dtest| x∈Dtest|C(x)|forsometestdatasetD test. Thisis,infact,
nottheonlytypeofinefficiencycriterion(Vovketal.,2016),butweuseitasourmainperformance
metric,sinceitisthemostcommon.
2WedepicttheSplitconformalpredictionproce-
dure in Figure 1, where we include two extra
D C(X)
cal
variablesthatwillbeusefullaterinthetext: the
modelpredictionYˆ = f(X),andtheeventof
validcoverageE ={Y ∈C(X)},i.e.,theevent Yˆ
ofthepredictionsetcontainingthecorrectclass.
X Y E
2.2 ConformalPredictionasListDecoding
Figure1: GraphicalmodelofSCP.D isacali-
cal
Define yˆ = f(x). Upon observing yˆ, CP pro- brationset,C(X)thepredictionset,Yˆ =f(X)the
videsalistofcandidatesinthetargetsetY. If modelprediction,andE theevent{Y ∈ C(X)}.
weconsiderthemappingbetweenyandyˆasa Square and round nodes are, respectively, deter-
noisy channel, the conformal prediction prob- ministicandstochasticfunctionsoftheirparents.
lemisequivalenttovariable-sizelistdecoding
forthenoisychannelbetweenthetruelabelyandyˆ. Ininformationtheoryandhypothesistesting,the
decoderorthetestercanprovideasetofoutcomes. Ifthecorrectsolutionisnotpartofthesubset,an
errorisdeclared. Thelistdecodingideagoesbacktoworksof(Elias,1957)and(Wozencraft,1958)
incommunicationtheoryfollowedbyaplethoraofworksonthetopicwithvariouslowerandupper
boundsontheerror. WereviewsomeoftheseresultsinAppendixB.2. Inthispaper,weleveragethe
informationtheoreticinterpretationofCPmainlyintwodirections. First,weusetheinformation
theoreticinequalitiesforlistdecodingtoupperboundtheentropyofthedata-generatingprocess.
Thisleadstonewtrainingmechanismsforconformaltraining(seeSection4). Itadditionallyprovides
boundsontheinefficiencyofagivenmodel(seeAppendixDandF.3). Second,weshowwecan
incorporateside-informationintoCPtoimprovepredictiveefficiency(seeSection5).
3 InformationTheoryAppliedtoConformalPrediction
Inthissectionwedevelopourmainresults,linkinginformationtheoryandconformalprediction.
Concretely,weprovidethreedifferentupperboundsontheconditionalentropyH(Y|X):onecoming
fromthedataprocessinginequality,andtwoderivedinasimilarwaytoFano’sinequality.
3.1 DataProcessingInequalityforConformalPrediction
Wecanreusesomeoftheexistingdataprocessinginequalities(DPI)obtainedforf-divergences
togetboundsonconformalprediction. Foraconvexfunctionf withf(1) = 0,thef-divergence
betweentwoprobabilitymeasuresP andQisdefinedas(seeSason&Verdú(2016))
(cid:20) (cid:18) (cid:19)(cid:21)
dP
D (P||Q):=E f .
f Q dQ
In particular, with f(x) = xlogx we recover the familiar notion of KL-divergence (Kullback &
Leibler,1951). TheDPIforf-divergencestatesthatforanytwoprobabilitymeasuresP andQ
X X
definedonaspaceX,andanystochasticmapW ,whichmaps(P ,Q )to(P ,Q ),wehave
Y|X X X Y Y
D (P ||Q )≥D (P ||Q ),
f X X f Y Y
SeeAppendixB.1formoredetailsonf-divergenceanddataprocessinginequalities. TheDPIwould
immediatelyyieldthefollowingproposition.
Proposition3.1(DPIBound). Consideranyconformalpredictionmethodsatisfyingtheupperand
lowerboundsofTheorem2.1forα∈(0,0.5). ForanyarbitraryconditionaldistributionQ ,the
Y|X
trueconditionaldistributionP andtheinputmeasureP ,definethefollowingtwomeasures
Y|X X
Q:=P Q andP :=P P . Then,wehave
X Y|X X Y|X
H(Y|X)≤h (α)+(1−α)logQ(Y ∈C(X))+α logQ(Y ∈/ C(X))−E (cid:2) logQ (cid:3) , (3)
b n P Y|X
whereh (·)denotesthebinaryentropyfunctionwithh (α)=−αlog(α)−(1−α)log(1−α).
b b
NotethattheentropytermH(Y|X)iscomputedusingthemeasureP. Werelegatetheproofto
AppendixC.1. Intheabovebound,thetermQ(Y ∈ C(X))appearsinsidealog,soanempirical
estimateQˆ(Y ∈C(X))wouldresultinalowerboundandwouldbebiased. Wecanprovideanupper
3confidenceboundonthisestimateusingtheempiricalBernsteininequalityMaurer&Pontil(2009),
andusethatinstead. BasedontheempiricalBernsteininequality,withprobability1−δ,wehave
(cid:114)
2V (Z)log(2/δ) 7log(2/δ)
∆ (Z,n):= n +
δ n 3(n−1)
Q(Y ∈C(X))≤Qˆ(Y ∈C(X))+∆ (Z,n):=Q˜(Y ∈C(X)),
δ
Q(Y ∈/ C(X))≤Qˆ(Y ∈/ C(X))+∆ (Z,n):=Q˜(Y ∈/ C(X)),
δ
withV (Z)theempiricalvarianceofZ =(Z ,...,Z ),Z =Q(y ∈C(x )). Usingthesebounds,
n 1 n i i i
wegetthefollowinginequalitywithprobability1−δ:
H(Y|X)≤h (α)+(1−α)logQ˜(Y ∈C(X))+α logQ˜(Y ∈/ C(X))−E (cid:2) logQ (cid:3) . (4)
b n P Y|X
Thisupperboundisoneofourmainresults,whichwedubtheDPIbound. Notethatforthelast
expectation,wecanusetheempiricalestimate,asitisanunbiasedapproximation.
3.2 Model-BasedFano’sInequalityandVariations
Next, we present an inequality which is a variation of Fano’s inequality. Fano’s inequality is a
classicalresultthat,amongotherthings,isusedtoprovetheclassicalShannon’stheoremonchannel
capacity. SeeAppendixB.4formoredetails. Weobtainthefollowinginequalitybymodifyingthe
classicalproofofFano’sinequality,whichcanbefoundinCover&Thomas(2006).
Proposition3.2(Model-BasedFanoBound). Consideranyconformalpredictionmethodsatisfying
theupperandlowerboundsofTheorem2.1forα∈(0,0.5). Then,forthetruedistributionP,and
foranyprobabilitydistributionQ,wehave
H(Y|X)≤h (α)+αE (cid:2) −logQ (cid:3)
b PY,X,Dcal|Y∈/C(X) Y|X,C(X),Y∈/C(X)
+(1−α )E (cid:2) −logQ (cid:3) . (5)
n PY,X,Dcal|Y∈C(X) Y|X,C(X),Y∈C(X)
Notewehaveonetermconditionedontheeventofvalidcoverage,Y ∈C(X),andanothercondi-
tionedonY ∈/ C(X).WeprovidetheproofinAppendixC.2. AgoodchoiceforQisthepredictive
modelitself,andthatiswhywerefertotheboundaboveasModel-Based(MB)Fanobound. Another
naturalchoiceforQistheuniformdistribution,inwhichcasewegetthefollowingresult.
Corollary3.1(SimpleFanoBound). Consideranyconformalpredictionmethodsatisfyingtheupper
andlowerboundsofTheorem2.1forα∈(0,0.5). Then,forthetruedistributionP wehave
H(Y|X)≤h (α)+αE [log(|Y|−|C(X)|)]
b PY,X,Dcal|Y∈/C(X)
+(1−α )E [log|C(X)|]. (6)
n PY,X,Dcal|Y∈C(X)
TheprooffollowsdirectlyfromProposition3.2byreplacingQwiththeuniformdistribution. We
refertotheboundin(6)as(simple)Fanobound,sinceitismodelagnosticandcanbeapproximated
directlywithonlyempiricalestimatesofthepredictionsetsize. Thislastboundexplicitlyrelates
thecentralnotionofuncertaintyinconformalprediction,thepredictionsetsize,toaninformation
theoreticconceptofuncertaintyinH(Y|X). Thisinformationtheoreticinterpretationofconformal
prediction as list decoding introduces various information theoretic tools, potentially useful for
variousapplications. InAppendixDwederivesomenewinequalitiesforconformalprediction,in
particularofferingnewlowerboundsontheinefficiencyoftheconformalprediction. Inthenext
section,wediscusshowtheseinequalitiescanbeusedasconformaltrainingschemes.
4 ConformalTraining
AlthoughwecanapplysplitconformalpredictiontoanypretrainedMLmodelasapost-processing
step,theoverallperformanceofanyCPmethod(commonlymeasuredbyitsinefficiency)ishighly
dependentontheunderlyingmodelitself. Therefore,previousworkshaveproposedtotakeCPinto
accountalsoduringtraininganddirectlyoptimizeforlowpredictiveinefficiency(Bellotti,2021;
Stutzetal.,2022). Weusethetermconformaltrainingtorefertosuchapproachesingeneral(see
4AppendixEforanoverviewofthetopic). Inparticular,wefocusonConfTr(Stutzetal.,2022)since
itgeneralizesandoutperforms(Bellotti,2021).
InConfTr(Stutzetal.,2022)eachtrainingbatchBissplitintocalibrationB andtestB halves
cal test
tosimulatetheSCPprocess(seeSection2)foreachgradientupdateofmodelf andminimizethe
followingsizeloss
(cid:32) (cid:33)
(cid:88)
logE[|C f(X)|]≈log 1/|Btest| |C f(x)| , (7)
x∈Btest
where C (x) is constructed using the statistics of B . Here we use the notation C (x) to make
f cal f
explicitthedependenceofthepredictionsetonthemodelf. Still,SCPinvolvesstepfunctionsand
(Stutz et al., 2022) introduce a couple of relaxations to recover a differentiable objective: (i) the
computationofquantilesisrelaxedviadifferentiablesortingoperators(Cuturietal.,2019;Blondel
etal.,2020;Petersenetal.,2022);(ii)thethresholdingoperationintheconstructionofprediction
setsin(2)isreplacedbysmoothassignmentsoflabelstopredictionsetsviathelogisticsigmoid.
OurupperboundsonH(Y|X),namelyDPI(4),MBFano(5)andsimpleFano(6),presentedinthe
previoussectioncanbemadedifferentiableinthesameway,andthuscanalsoserveasproperloss
functionsforconformaltraining.Themotivationfordoingsoistwofold.First,theconditionalentropy
H(Y|X)capturestheunderlyinguncertaintyofthetask,orequivalently,theuncertaintyunderthe
truelabellingdistributionP . Thus,byminimizingtheseupperbounds,wepushthemodelf
Y|X
closertothetruedistribution,whichisknowntoachieveminimalinefficiency(Vovketal.,2016).
Interestingly,thecross-entropylossalsoboundsH(Y|X)fromabove,andthuscanbemotivatedasa
conformaltrainingobjectivefromthesameangle. Inthatregard,theDPIboundfromProposition3.1
isparticularlyadvantageousasitisprovablytighterthanthecross-entropy(seeAppendixC.1).
Second,wecanconnectthesimpleFanoboundin(6)tothesizeloss(7)from(Stutzetal.,2022). In
AppendixE.1,weshowthatviaJensen’sinequalityandlog(|Y|−|C(X)|)≤log|Y|theboundin
(6)canbefurtherupperboundedas
λ :=h (α)+αlog|Y|−(1−α )log(1−α),
α b n
H(Y|X)≤λ +(1−α )logE[|C(X)|],
α n
whereλ onlydependsonαand|Y|,andthusdoesnotaffectoptimization. Therefore,weground
α
ConfTrasminimizinganupperboundtothetrueconditionalentropythatislooserthanthesimple
Fanobound,andlikelyalsolooserthanthemodel-basedFanoboundforanappropriatechoiceforQ.
5 SideInformation
Withtheinformationtheoreticalinterpretationofconformalprediction,wecantranslatevariousintu-
itionsfrominformationtheory,forexampleaboutdifferenttypeofchannelsornetworkinformation
theory,toconformalprediction. Inthissection,weconsiderthenotionofsideinformation.
LetX betheinputcovariates,Y bethetargetvariable,Z besomesideinformationaboutthetask
andletQ bethemodelwhichweusetoperformconformalprediction. AswecanrelateCPwith
Y|X
Q toanupperboundontheconditionalentropyH(Y|X),wewouldliketodothesameforthe
Y|X
caseoftheconditionalentropywhensideinformationisavailable,i.e.,H(Y|X,Z). Sinceweknow
thattheconditionalentropydirectlyaffectstheexpectedsetsize,i.e.,theinefficiencyofCP,andgiven
thatH(Y|X)≥H(Y|X,Z)wecanexpectthatwiththeadditionalsideinformationtheinefficiency
ofCPwilldecrease. Wecantakesideinformationintoaccountbydefiningconformityscoresasa
functionofQ insteadofQ . AsimplewaytodothatwouldbeviatheBayesrule
Y|X,Z Y|X
Q Q
Y|X Z|X,Y
Q = , (8)
Y|X,Z (cid:80) Q Q
Y Y|X Z|X,Y
whereQ isanauxiliarymodelthatpredictsthesideinformation,giventheinputandtarget
Z|X,Y
variables. SuchamodelcouldbelearnedseparatelyfromthemainmodelQ givenaccesstoa
Y|X
datasetD ={(x ,y ,z )}. Therefore,wecannowcalibratewithCPbytakingintoaccountthe
side i i i
sideinformationandthen,attesttime,givenaccesstotheinputandsideinformation,wecanusethe
appropriateprobabilitiesQ toconstructthepredictionsets. Intuitively,thepredictionsetsfrom
Y|X,Z
suchaprocedureshouldbesmallerthanthepredictionsetsobtainedfromusingQ directly. It
Y|X
5shouldbenotedthat,inthecaseofsideinformationnotbeingavailable,wecanmarginalizeQ
Y,Z|X
overZ,which,byconstruction,fallsbacktotheoriginalmodelQ . IftheavailabilitypatternofZ
Y|X
isconsistentbetweenthecalibrationandtestsets,theconformalpredictionguaranteesstillholdby
definingthemodelas
(cid:26)
Q ifZ isobserved
f = Y|X,Z (9)
Q otherwise.
Y|X
Withthisadditiontothesplitconformalpredictiontoolset,ifnew(side)informationismadeavailable
attesttime,wecanproperlyincorporateitintotheCPpipelinewithouthavingtoretrainthemain
classifier. OneneedsonlytrainanauxiliaryclassifierQ ,whichmightbemuchsimplerthan
Z|X,Y
Q (inourexperimentsQ isgivenbyasinglelinearlayer)andrequiremuchlessdata. One
Y|X Z|X,Y
notableexampleofsideinformationarisesnaturallyinthedistributedsetting,whichwediscussnext.
5.1 TheDistributedLearningSetting
Considerthecasewherewehaveadatasetthatisdistributedamongasetofmdevicesandwewant
torunconformaltrainingforaglobalmodelQ . Further,assumeitisdifficulttogatherthedataat
Y|X
acentrallocation(e.g.,duetoprivacyreasons),andthuswehavetoworkwiththedatastayinglocally
oneachdevice. AnexampleofthiswouldbeFederatedLearning(orFL)(McMahanetal.,2017). In
thiscase,ifZ ∈{1,...,m}identifiesthedevice,theentropyH(Y|X)canbeexpressedas
H(Y|X)=H(Y|X,Z)+I(Y;Z|X)=E [H(Y|X,Z =z)]+I(Y;Z|X),
PZ
whichdecomposesintoaweightedaverageoflocalentropyfunctionsH(Y|X,Z =z). Wecannow
useanyofourproposedboundsforeachoftheconditionalentropiesH(Y|X,Z =z)bycalibrating
withCPindependentlyoneachdevice,endingupwith
H(Y|X)≤E [H (Y|X,Z =z)]+I(Y;Z|X),
PZ ub
whereH (Y|X,Z =z)correspondstoanupperboundoftheconditionalentropyH(Y|X,Z =z).
ub
Furthermore,forthemutualinformationtermwehavethat
(cid:20) P (cid:21)
I(Y;Z|X)=E log Z|X,Y ≤E (cid:2) −logP (cid:3) ≤E (cid:2) −logQ (cid:3)
PZ,X,Y P PZ,X Z|X PZ,X Z|X
Z|X
wherethefirstinequalityisduetoZ beingdiscreteandhavingnon-negativeentropyandthesecond
isduetoGibbsinequalitywithQ beinganauxiliarymodeltrainedtopredicttheuseridZ =z
Z|X
giveninputX =x. Asimilarupperboundhasbeenconsideredbeforeinafederatedsetting(Louizos
etal.,2024). Withthisupperboundweget
H(Y|X)≤E (cid:104) H (Y|X,Z =z)−E (cid:2) logQ (cid:3)(cid:105) . (10)
PZ ub PX|Z=z Z=z|X
Thisgivesusanupperboundontheentropyoftheentirepopulationthatdecomposesintoasum
oflocalfunctions,oneforeachclient,onlyrequiringlocalinformation. Asaresult,wecaneasily
performconformaltrainingforQ byoptimizingthisupperboundinthefederatedsettingwith,
Y|X
e.g,federatedaveraging(McMahanetal.,2017). Attesttime,wecanconsiderthedeviceIDz as
sideinformationforCP.Tothisend,wecaneithertraininparallelamodelQ anduse8with
Z|X,Y
theglobalmodelQ attesttimeinordertocomputeanadjustedmodelQ orobtainQ
Y|X Y|X,Z Y|X,Z
byfinetuningtheglobalmodelQ withlocaldata.
Y|X
6 RelatedWork
Conformal prediction, a powerful framework for uncertainty estimation developed by Vovk and
collaborators(Vovketal.,2005;Shafer&Vovk,2008)hasrecentlywitnessedawideadoptionin
manyfields,e.g.,healthcare(Papadopoulosetal.,2009;Alnemeretal.,2016;Luetal.,2022b,a)
and finance (Wisniewski et al., 2020; Bastos, 2024). The marriage of conformal prediction and
machine learning has been especially fruitful, and since the seminal work by Vovk et al. (2005),
many extensions and applications of CP to ML have been proposed in the literature. Notably,
concerningtopicssuchassurvivalanalysis(Candèsetal.,2021),treatmenteffectevaluation(Lei&
Candès,2021),classification(Guptaetal.,2020;Angelopoulosetal.,2020)andregression(Romano
6Table1: InefficiencywithbothTHRandAPSmethodsforeachobjectiveanddatasetforatarget
α=0.01. Wereportmeanandstandarddeviationacross10differentcalib./testsplitsandshowin
boldallvalueswithinonestandarddeviationofthebestresult. Lowerisbetter.
Method MNIST F-MNIST EMNIST CIFAR10 CIFAR100
THR APS THR APS THR APS THR APS THR APS
CE 2.15±0.21 2.41±0.12 2.18±0.15 2.24±0.21 2.01±0.06 3.27±0.16 1.81±0.20 2.41±0.25 21.11±1.17 28.11±2.38
ConfTr 2.10±0.16 2.10±0.16 1.69±0.06 1.87±0.08 1.95±0.05 2.32±0.06 9.77±0.02 9.98±0.00 41.87±1.71 29.57±1.09
ConfTrclass 2.05±0.15 2.08±0.14 1.73±0.06 1.73±0.05 1.94±0.05 2.29±0.06 2.29±0.07 2.24±0.09 35.23±2.36 49.62±1.42
Fano 2.08±0.21 2.08±0.13 1.73±0.03 1.87±0.06 2.02±0.08 2.68±0.10 2.00±0.10 2.35±0.10 34.51±2.33 35.11±1.53
MBFano 2.15±0.11 2.37±0.24 1.77±0.06 2.25±0.12 2.86±0.22 3.67±0.15 1.70±0.04 2.01±0.08 17.43±0.93 21.27±1.38
DPI 2.18±0.13 2.52±0.12 1.75±0.08 2.06±0.08 1.98±0.07 4.14±0.12 1.71±0.07 1.99±0.10 16.03±1.02 16.86±1.19
etal.,2019)settings,risk-controlling(Angelopoulosetal.,2021;Batesetal.,2021),andcovariate
shift(Tibshiranietal.,2019),tomentionsome.
Theoriginalconformal predictionframework, often referred toasfullor transductive conformal
prediction(Vovketal.,2005),requiresretrainingthepredictivefunctionforeachnewtestdatapoint
and each of its possible labels. This quickly becomes too computationally intensive for modern
machinelearningmethods,whichmakesthesplitconformalpredictionvariant(Papadopoulosetal.,
2002;Leietal.,2015)particularlyappealinginpractice. SCPallowsustotransformanypointwise
predictor, e.g., a large pretrained ML model, into a set predictor encoding a rigorous notion of
uncertainty in the size of each prediction set (Angelopoulos & Bates, 2021; Angelopoulos et al.,
2021;Batesetal.,2021). Unfortunately,thiscomesatthecostofallocatingachunkofthedataset
solelyforcalibration,whichreducestheamountofdataavailablefortrainingmodel.
Tothebestofourknowledge,ourworkrepresentsthefirstattempttobridgeCPandinformation
theory. Amongotherthings,thisallowsustobuildontheconformaltrainingideasofBellotti(2021)
and Stutz et al. (2022), deriving principled learning objectives that generalize their approaches,
dispensewithsometheirhyperparametersandresultinmoreefficientpredictionsets. Moreover,
we also empirically demonstrate that our conformal training objectives provide a strong enough
learningsignaltotraincomplexarchitecturesfromscratch,withstrongresultsonResNet-34and
ResNet-50(Heetal.,2016)fittedonCIFAR10andCIFAR100,respectively. Incontrast,theprevious
state-of-the-artapproach,ConfTr(Stutzetal.,2022),strugglesinthosesettings(seeexperiments
in Section 7) and required pretrained models for consistent results (Stutz et al., 2022). Further,
ourinformationtheoreticalinterpretationofCPprovidesanewsimpleandeffectivemechanismto
leveragesideinformationinsplitconformalprediction. Weareunawareofanyotherapproachesto
treatsideinformationinsidetheconformalpredictionframeworkintheliterature.
On the information theory side, the notion of f-divergences and related inequalities have been
discussedinmanydifferentworks. Theuseoff-divergencegoesbacktoworksofAliandSilvey,
Csiszár,andMorimotointhe60s,asin,forinstance,(Ali&Silvey,1966;Csiszár,1967;Morimoto,
1963). Akeyf-divergenceinequalityisthedataprocessinginequality—see(Sason&Verdú,2016;
Sason,2019)foranextensivesurvey. Itprovidesaunifiedwayofobtainingmanyclassicalandnew
results,forexampleFano’sinequality(Fano,1949). Thetightnessofthedataprocessinginequalities
isdiscussedintermsofBregman’sdivergencein(Collet,2019;Liese&Vajda,2006)andintermsof
χ2-divergencein(Sason,2019). Listdecoding,whichiscloselyconnectedtoCP,wasintroduced
in context communication design by Elias (Elias, 1957) and Wozencraft (Wozencraft, 1958). A
generalizationofFano’sinequalitytolistdecodingwasgivenin(Csiszár&Körner,2011)inthe
context of multi-user information theory, see also the general Fano inequality for list decoding
presentedin(Raginsky&Sason,2013). Variable-sizelistdecodingwasdiscussedin(Sason,2019),
usingideasfirstintroducedin(Polyanskiyetal.,2010)and(Liuetal.,2017). Aselectionofrelevant
inequalitiesforlistdecodingcanbefoundin(Sason,2019).
7 Experiments
Inthissection,weempiricallystudytwoapplicationsofourtheoreticalresults,namelyconformal
predictionwithsideinformationandconformaltrainingwithourupperboundsontheconditional
entropyasoptimizationobjectives. Wefocusourexperimentsonclassificationtasks,sincethisisthe
mostcommonsettinginpreviousworksinconformaltraining(Bellotti,2021;Stutzetal.,2022).
77.1 ConformalTraining
Wetesttheeffectivenessofourupperboundsasobjectivesforconformaltraininginfivedatasets:
MNIST(LeCunetal.,1998),Fashion-MNIST(Xiaoetal.,2017),EMNIST(Cohenetal.,2017),
CIFAR10andCIFAR100(Krizhevskyetal.,2009). Inadditiontoourupperbounds,DPI(4),MB
Fano(5)andFano(6),wealsoevaluatethecross-entropyloss(CE,alsoanotherupperboundtothe
entropy),andthetwomainvariantsproposedin(Stutzetal.,2022),namelyConfTr,whichminimizes
(7)andConfTr thatoptimizesanadditionalclassificationlossterm(seeAppendixE).Wefollow
class
asimilaroptimizationprocedureandexperimentalsetuptothatof(Stutzetal.,2022)butwiththe
keydifferencesthatwelearntheclassifiersfromscratchinallcases,withouttheneedofpretrained
CIFARmodels,andthatweusethelargerdefault“byclass”splitofEMNIST.
Foreachdataset,weusethedefaulttrainandtestsplitsbuttransfer10%ofthetrainingdatatothetest
dataset. Wetraintheclassifiersonlyontheremaining90%ofthetrainingdataand,attesttime,run
SCPwith10differentcalibration/testsplitsbyrandomlysplittingtheenlargedtestdataset. Further,
foreachdataset,objectiveandSCPmethod,werungridsearchtofindthebestvaluesforbatch
size,learningrate,temperature(smoothingparameterintheassignmentoflabelstoprediction
sets)andsteepness(smoothingparameterindiffsort(Petersenetal.,2022),thedifferentiablesorting
operatorweused). FortheremainingConfTrhyperparameters,weusedthebestvaluesreportedin
(Stutzetal.,2022). Toavoidoverfittingtothetestdata,thegridsearchwasdonesolelyonthe90%
ofthetrainingdatanotusedfortesting. SeeAppendixFforadetailedaccountoftheexperimental
settings,includingadditionalresults,modelarchitectures,andoptimalhyperparameters.
In Table 1, we report the empirical inefficiency on the test data considering two SCP methods,
thresholdCPwithprobabilities(orTHR)(Sadinleetal.,2019)andAPS(Romanoetal.,2020). In
allcases,ourupperboundsprovedeffectivelossfunctionstotrainefficientclassifiersend-to-end
andfromscratch. Forthesimplerdatasets(MNIST,Fashion-MNISTandEMNIST),allconformal
trainingmethodsachievedsimilarresultsandoutperformedthecross-entropylossinmostcases,
withConfTr producingsmallerpredictionsetsonaverage,especiallyforAPS.However,forthe
class
remainingandmorechallengingdatasets,bothConfTrvariantslaggedbehind,probablybecausethey
donotprovideastrongenoughsignaltotrainResNetsfromscratch(onCIFARdatasets,Stutzetal.
(2022)onlyusedConfTrtofinetunepretrainedmodels). AsimilarobservationappliestotheFano
bound,wheretheonlysignaltotheclassifiercomesthroughtherelaxedpredictionsetsizein(6).
Table2: InefficiencyonCIFAR100fordifferentαtargetsattesttime,averagedacross10random
calib./testsplits. Allmethodsoptimizedonlyforα=0.01. Lowerisbetter.
Method α=0.01 α=0.05 α=0.1
THR APS THR APS THR APS
CE 21.11 ±1.17 27.41 ±2.16 6.40 ±0.37 9.95 ±0.56 3.24 ±0.14 4.97 ±0.20
ConfTr 41.87 ±1.71 29.57 ±1.09 12.06 ±0.27 21.22 ±0.82 7.15 ±0.26 14.33 ±0.70
ConfTr class 35.23 ±2.36 49.62 ±1.42 14.31 ±0.66 16.56 ±0.58 8.91 ±0.24 11.16 ±0.20
Fano 34.51 ±2.33 35.11 ±1.53 15.14 ±0.56 18.30 ±0.42 8.84 ±0.33 10.86 ±0.23
MBFano 17.43 ±0.93 21.27 ±1.38 5.95 ±0.35 9.33 ±0.27 3.18 ±0.12 5.57 ±0.2
DPI 16.03 ±1.02 16.86 ±1.19 5.21 ±0.17 7.06 ±0.24 2.71 ±0.11 4.14 ±0.12
Inallexperiments,weoptimizetheconformaltrainingobjectivesforatargetcoveragerateof99%,
i.e., α = 0.01. It is then important to evaluate whether the performance of the resulting models
deteriorateatdifferentcoveragerates,“overfitting”tothevalueofαusedfortraining. InTable2,we
seethatisnotthecase,sinceourboundsstilloutperformthebaselines—includingthecross-entropy
loss,whichisagnostictothedesiredcoveragerate—atdifferentαvalues. Table2alsoconfirmsthe
observationintheoriginalpaper(Stutzetal.,2022)thattheperformanceofConfTrdeterioratesfor
higherαvalues. Forα=0.1,bothConfTrmethodsproducepredictionsetsthataremorethantwo
timeslargerthanthoseobtainedviatheCEloss.
7.2 SideInformation
Asafirstexperiment,weconsiderdatasetsforwhichthereexistsanaturalgroupingofthelabelsand
usethegroupassignmentassideinformationZ. InCIFAR100,thereisadisjointpartitionofthe
100classesinto20superclasses,sowedefinezasthesuperclasstowhichexample(x,y)belongs.
8InEMNIST,zindicateswhethertheexampleisadigit,uppercaseorlowercaseletter. Wetrainan
auxiliarymodelR and,attesttime,assumeaccesstosideinformationz torecomputeclass
Z|X,Y
probabilitiesQ fromtheoriginalclassifierQ asinequation(8).
Y|X,Z=z Y|X
WereportresultsinTable3, whereweconsidertwodifferentscenarios. Thefirstisthestandard
SCPsetting,whereweevaluatetheinefficiencyofTHRandAPSmethods,withsideinformation
Z observedfor10,30and100%oftheinstances. Inthiscase,weredefinetheclassifierf asin(9)
toaccountforwhenZ ismissing,butotherwisetheSCPprocessremainunchanged. Thesecond
scenarioisMondrianorGroup-BalancedCP(Vovketal.,2005),whereonesplitsD intodifferent
cal
groupsandrunsCPforeachofthemindividually. Inthissetting,wegroupthecalibrationdatapoints
accordingtoZ andbasethescorefunctiononQ . Inallcases,takingthesideinformationinto
Y|X,Z
accountreducedtheinefficiencyconsiderably.
Table 3: Inefficiency for each method and dataset for a target α = 0.01 with side information.
ThesideinformationisthesuperclassassignmentforCIFAR100andwhethertheclassisadigit/
uppercaseletter/lowercaseletterforEMNIST.
Method CIFAR100 EMNIST
THR APS Acc.(%) THR APS Acc.(%)
CP 21.11 ±1.17 28.11 ±2.38 71.07 2.01 ±0.06 3.25 ±0.15 85.69
CPw/10%SI 19.66 ±1.01 25.35 ±1.66 71.76 1.96 ±0.07 3.13 ±0.17 86.88
CPw/30%SI 16.71 ±1.04 22.57 ±1.41 73.63 1.71 ±0.07 2.81 ±0.15 89.42
CPw/100%SI 10.12 ±1.02 16.82 ±1.36 77.97 1.06 ±0.02 1.64 ±0.07 97.63
GroupCP 19.00 ±1.59 24.11 ±1.60 71.07 2.30 ±0.16 3.76 ±0.21 85.69
GroupCPw/SI 8.52 ±0.59 13.89 ±0.99 77.97 1.14 ±0.03 1.70 ±0.05 97.63
7.3 FederatedLearning(FL)
ApracticallyrelevantapplicationofsideinformationarisesinFL,wherewecantakethedevice
IDassideinformationZ. Inthefederatedsetting,wetraintwoextraheadsontopofthethemain
classifier,onecomputingQ sothatwecanoptimizetheproperupperboundin(10),andanother
Z|X
computingQ (whiledetachinggradientstothemainclassifiersoastonotaffecttheupper
Z|X,Y
bound optimization) that we use to integrate side information into CP using (8). Besides being
apracticallyrelevantapplicationofsideinformationtoCP,FLalsoservesasamorechallenging
testbedforourconformaltrainingmethods, whichhasnotbeenexploredinpreviouswork. We
ranfederatedaveraging(McMahanetal.,2017)withCE,ConfTr,ConfTr ,andourthreeupper
class
bounds as local loss functions. In this setting, we consider CIFAR10, CIFAR100, and EMNIST
with100,500,and1Kdevices,resp. Weassigndatapointstodevicesimposingadistribution-based
label imbalance (Li et al., 2022), where we sample a marginal label distribution for each device
fromaDirichletdistributionDir(1.0). SeeAppendixFforresultswithDir(0.5)andDir(0.1). As
hyperparameteroptimizationinFLisnotablychallengingandcostly(Wangetal.,2021),wekeepthe
samehyperparametersfoundforthecentralizedcaseinSection7.1.
Afterconvergence,weranSCPwiththefinalglobalmodelassumingcalibrationandtestdatasets
attheserver,orequivalentlythattheclientssharetheirscoreswiththeserver. Thisreflectsthebest
inefficiencyresultswecanhopeforwiththeglobalmodel,asinpracticewemightneedtoresort
toprivacypreservingmethodsthatarelikelytohurtperformance. SeeAppendixFforadiscussion
andextraresultsonotherpossiblesettings. Wereportinefficiencyresultsfortheglobalmodelin
Table4,whereweobservesimilartrendstothecentralizedexperimentsinTable1. Namely,ConfTr
methodsstillperformwellonEMNISTbutstruggleonCIFARdata—withthenotableexceptionon
CIFAR100,whereConfTrexcelled—whileourmethodsachievedconsistentresultsacrossalldata
sets. ThisprobablyreflectsthesensitivityofbothConfTrobjectivestohyperparameters,whichmake
themhardtouseinpractice,especiallyinFLwherehyperparameteroptimizationiscomputationally
intensive. Conversely, ourmethodsseemmorerobusttosuchvariations, asthehyperparameters
foundonthecentralizedsettingseemtotranslatewelltothefederatedcase.
OnemarkeddifferencebetweenTables1and4isthatthesimpleFanobound(6),whichlaggedbehind
theDPIboundanditsmodel-basedcounterpartonthecentralizedsetting,achievedthebestresults
onthefederatedsetting. Wehypothesizethiscouldbeduetooverfittingofthelocaloptimization
procedurestotheindividualdatadistributionofeachdevice, whichhurtstheconvergenceofthe
globalmodel. ThiseffectisexacerbatedonCIFAR100,wherewehave500devices,eachofwhich
9Table4: InefficiencyofconformalpredictioninthefederatedsettingwithTHRandAPSforeach
datasetandforatargetα=0.01. Wereportmeanandstandarddeviationoftheinefficiencyofthe
globalfederatedmodel,withandwithoutsideinformation,across10differentcalib./testsplitsand
showinboldallvalueswithinonestandarddeviationofthebestresult. Lowerisbetter.
Method EMNIST CIFAR10 CIFAR100
THR THR+side APS APS+side THR THR+side APS APS+side THR THR+side APS APS+side
CE 2.91±0.02 2.46±0.02 3.69±0.03 3.14±0.04 2.73±0.04 2.30±0.06 2.83±0.07 2.43±0.06 55.41±1.09 52.31±1.03 64.73±0.34 62.67±3.68
ConfTr 4.60±0.05 3.30±0.02 6.14±0.04 5.25±0.04 10.00±0.00 10.00±0.00 10.00±0.00 10.00±0.00 45.60±1.30 41.18±1.16 55.18±2.10 47.58±1.48
ConfTrclass 2.88±0.02 1.98±0.02 2.65±0.02 2.42±0.02 3.53±0.09 3.39±0.08 10.00±0.00 10.00±0.00 58.53±1.40 56.03±1.29 99.92±0.02 99.91±0.01
Fano 2.63±0.02 2.37±0.02 3.12±0.04 2.72±0.03 2.39±0.07 2.07±0.07 2.73±0.07 2.39±0.06 47.91±1.20 41.19±1.02 46.95±0.67 42.75±0.91
MBFano 2.84±0.04 2.25±0.03 4.75±0.03 2.43±0.01 2.52±0.07 2.04±0.07 2.79±0.13 2.33±0.05 52.94±1.40 46.97±1.30 50.72±1.77 45.72±1.38
DPI 2.60±0.02 2.23±0.01 2.98±0.03 2.58±0.02 2.76±0.07 2.28±0.03 2.68±0.15 2.22±0.09 52.36±0.95 48.64±0.70 51.29±1.07 47.18±1.27
endswithveryfewdatapoints. ThesimpleFanoboundislessvulnerabletosuchoverfitting,sinceit
reliesonthemainclassifiertoamuchlesserdegree.
Finally,inalmostallcasesourupperboundsoutperformedthecross-entropy,reassuringthepotential
ofconformaltraining. Moreover,theinclusionofsideinformationreducedinefficiencyinallsettings,
andmarkedlysoinafewinstances. Thisconfirmstheeffectivenessofoursideinformationapproach
inacomplexandpracticallyrelevantscenario,likefederatedlearning.
8 Conclusion
Inthiswork,weaimedatestablishingaconnectionbetweennotionsofuncertaintycomingfrom
informationtheoryandconformalprediction. Weprovedthatwecanusesplitconformalprediction
methods to upper bound the conditional entropy of the target variable given the inputs, and that
theseupperboundsalsoformprincipledobjectivesforconformaltraining. Weempiricallyvalidated
ourapproachtoconformaltraining,withstrongresultsinbothcentralizedandfederatedsettings.
Unfortunately,ourexperimentshavenotsingledoutthebestperformingamongourbounds. The
generaltrendweobserveisthattheDPIandMBFanoboundsperformbetteroncomplexclassification
tasks,whilesimpleFanoseemstoexcelinrelativelysimpletasksorwherethereisahigherriskof
overfitting,likeinthefederatedsetting.
Further,theinformationtheoreticperspectivealsooffersasimpleyetrigorousapproachtoincorporate
sideinformationintoconformalprediction,whichweexperimentallydemonstrateleadstobetter
predictive efficiency in almost all cases. To the best of our knowledge this is the first attempt at
connectinginformationtheoryandconformalprediction. Giventhethusfarlimitedcommunication
betweenthesetworesearchcommunities,weexpectourworktoinciteafruitfulexchangeofnot
onlyideasbutalsotheoryandalgorithmsbetweenthesetworesearchdomains. Weseeanextension
ofourmethodstotheregressionsetting,asaparticularlypromisingavenueforfuturework.
References
Ahsan,M.M.,Luna,S.A.,andSiddique,Z. Machine-learning-baseddiseasediagnosis: Acompre-
hensivereview. Healthcare,10(3),2022. ISSN2227-9032. doi: 10.3390/healthcare10030541.
URLhttps://www.mdpi.com/2227-9032/10/3/541.
Ali,S.M.andSilvey,S.D. AGeneralClassofCoefficientsofDivergenceofOneDistributionfrom
Another. JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),28(1):131–142,1966.
Publisher: [RoyalStatisticalSociety,Wiley].
Alnemer,L.M.,Rajab,L.,andAljarah,I. Conformalpredictiontechniquetopredictbreastcancer
survivability. IntJAdvSciTechnol,96:1–10,2016.
Angelopoulos,A.,Bates,S.,Malik,J.,andJordan,M.I. Uncertaintysetsforimageclassifiersusing
conformalprediction. arXivpreprintarXiv:2009.14193,2020.
Angelopoulos,A.N.andBates,S. Agentleintroductiontoconformalpredictionanddistribution-free
uncertaintyquantification. arXivpreprintarXiv:2107.07511,2021.
Angelopoulos,A.N.,Bates,S.,Candès,E.J.,Jordan,M.I.,andLei,L. Learnthentest: Calibrating
predictivealgorithmstoachieveriskcontrol. arXivpreprintarXiv:2110.01052,2021.
10Bastos,J.A. Conformalpredictionofoptionprices. ExpertSystemswithApplications,245:123087,
2024.
Bates,S.,Angelopoulos,A.,Lei,L.,Malik,J.,andJordan,M. Distribution-free,risk-controlling
predictionsets. JournaloftheACM(JACM),68(6):1–34,2021. Publisher: ACMNewYork,NY.
Bellotti,A. Optimizedconformalclassificationusinggradientdescentapproximation. arXivpreprint
arXiv:2105.11255,2021.
Blondel,M.,Teboul,O.,Berthet,Q.,andDjolonga,J. Fastdifferentiablesortingandranking. In
InternationalConferenceonMachineLearning,pp.950–959.PMLR,2020.
Candès,E.J.,Lei,L.,andRen,Z. Conformalizedsurvivalanalysis. arXivpreprintarXiv:2103.09763,
2021.
Cohen,G.,Afshar,S.,Tapson,J.,andVanSchaik,A. Emnist: Extendingmnisttohandwrittenletters.
In2017internationaljointconferenceonneuralnetworks(IJCNN),pp.2921–2926.IEEE,2017.
Collet,J.-F. AnExactExpressionfortheGapintheDataProcessingInequalityforf-Divergences.
IEEETransactionsonInformationTheory,65(7):4387–4391,July2019. ISSN1557-9654.
Cover,T.M.andThomas,J.A. Elementsofinformationtheory. Wiley-Interscience,Hoboken,N.J,
2ndededition,2006.
Csiszár,I. Ontopologicalpropertiesoff-divergence. StudiaSci.Math.Hungar.,2:330–339,1967.
Csiszár, I. and Körner, J. Information theory: coding theorems for discrete memoryless systems.
CambridgeUniversityPress,Cambridge;NewYork,2ndededition,2011.
Cuturi,M.,Teboul,O.,andVert,J.-P. Differentiablerankingandsortingusingoptimaltransport.
Advancesinneuralinformationprocessingsystems,32,2019.
Elias,P. ListDecodingforNoisyChannels. InIREWESCONConventionRecord,1957,volume2,
pp.94–104,1957.
Fano,R.M. Thetransmissionofinformation,volume65. MassachusettsInstituteofTechnology,
1949.
Gallager,R.G. Informationtheoryandreliablecommunication. Wiley,NewYork,NY,1986. ISBN
978-0-471-29048-3.
Gupta,C.,Podkopaev,A.,andRamdas,A. Distribution-freebinaryclassification: predictionsets,
confidenceintervalsandcalibration. AdvancesinNeuralInformationProcessingSystems, 33:
3711–3723,2020.
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearningforimagerecognition. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pp.770–778,2016.
Humbert, P., Le Bars, B., Bellet, A., and Arlot, S. One-shot federated conformal prediction. In
InternationalConferenceonMachineLearning,pp.14153–14177.PMLR,2023.
Krizhevsky,A.etal. Learningmultiplelayersoffeaturesfromtinyimages,2009.
Kuchibhotla, A. K. Exchangeability, conformal prediction, and rank tests. arXiv preprint
arXiv:2005.06095,2020.
Kullback,S.andLeibler,R.A. Oninformationandsufficiency. Theannalsofmathematicalstatistics,
22(1):79–86,1951.
Kuutti,S.,Bowden,R.,Jin,Y.,Barber,P.,andFallah,S. Asurveyofdeeplearningapplications
toautonomousvehiclecontrol. IEEETransactionsonIntelligentTransportationSystems,22(2):
712–733,2020.
LeCun, Y., Bottou, L., Bengio, Y., andHaffner, P. Gradient-basedlearningappliedtodocument
recognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
11Lei,J.,Rinaldo,A.,andWasserman,L. Aconformalpredictionapproachtoexplorefunctionaldata.
AnnalsofMathematicsandArtificialIntelligence,74:29–43,2015.
Lei,J.,G’Sell,M.,Rinaldo,A.,Tibshirani,R.J.,andWasserman,L. Distribution-freepredictive
inferenceforregression. JournaloftheAmericanStatisticalAssociation,113(523):1094–1111,
2018.
Lei,L.andCandès,E.J. Conformalinferenceofcounterfactualsandindividualtreatmenteffects.
JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology),2021. Publisher:Wiley
OnlineLibrary.
Li,Q.,Diao,Y.,Chen,Q.,andHe,B. Federatedlearningonnon-iiddatasilos: Anexperimental
study. In2022IEEE38thInternationalConferenceonDataEngineering(ICDE),pp.965–978.
IEEE,2022.
Liese,F.andVajda,I. OnDivergencesandInformationsinStatisticsandInformationTheory. IEEE
TransactionsonInformationTheory,52(10):4394–4412,October2006.
Liu,J.,Cuff,P.,andVerdú,S. e -Resolvability. IEEETransactionsonInformationTheory,63(5):
γ
2629–2658,May2017. ConferenceName: IEEETransactionsonInformationTheory.
Louizos, C., Reisser, M., and Korzhenkov, D. A mutual information perspective on federated
contrastivelearning. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
URLhttps://openreview.net/forum?id=JrmPG9ufKg.
Lu, C. and Kalpathy-Cramer, J. Distribution-free federated learning with conformal predictions.
arXivpreprintarXiv:2110.07661,2021.
Lu,C.,Angelopoulos,A.N.,andPomerantz,S. Improvingtrustworthinessofaidiseaseseverity
ratinginmedicalimagingwithordinalconformalpredictionsets. InInternationalConferenceon
MedicalImageComputingandComputer-AssistedIntervention,pp.545–554.Springer,2022a.
Lu,C.,Lemay,A.,Chang,K.,Höbel,K.,andKalpathy-Cramer,J. Fairconformalpredictorsfor
applicationsinmedicalimaging. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume36,pp.12008–12016,2022b.
maintainers,T.andcontributors. Torchvision: Pytorch’scomputervisionlibrary. https://github.
com/pytorch/vision,2016.
Maurer, A. and Pontil, M. Empirical bernstein bounds and sample variance penalization. arXiv
preprintarXiv:0907.3740,2009.
McMahan,B.,Moore,E.,Ramage,D.,Hampson,S.,andyArcas,B.A. Communication-efficient
learningofdeepnetworksfromdecentralizeddata. InArtificialintelligenceandstatistics, pp.
1273–1282.PMLR,2017.
Merhav,N. ListDecoding—RandomCodingExponentsandExpurgatedExponents. IEEETransac-
tionsonInformationTheory,60(11):6749–6759,November2014. ISSN1557-9654. Conference
Name: IEEETransactionsonInformationTheory.
Morimoto,T. MarkovProcessesandtheH-Theorem. JournalofthePhysicalSocietyofJapan,18(3):
328–331,March1963. Publisher: ThePhysicalSocietyofJapan.
Moritz,P.,Nishihara,R.,Wang,S.,Tumanov,A.,Liaw,R.,Liang,E.,Elibol,M.,Yang,Z.,Paul,
W.,Jordan,M.I.,etal. Ray: Adistributedframeworkforemerging{AI}applications. In13th
USENIXsymposiumonoperatingsystemsdesignandimplementation(OSDI18),pp.561–577,
2018.
Paninski,L. Estimationofentropyandmutualinformation. Neuralcomputation,15(6):1191–1253,
2003.
Papadopoulos,H.,Proedrou,K.,Vovk,V.,andGammerman,A. Inductiveconfidencemachinesfor
regression. InMachineLearning: ECML2002: 13thEuropeanConferenceonMachineLearning
Helsinki,Finland,August19–23,2002Proceedings13,pp.345–356.Springer,2002.
12Papadopoulos,H.,Gammerman,A.,andVovk,V. Reliablediagnosisofacuteabdominalpainwith
conformalprediction. EngineeringIntelligentSystems,17(2):127,2009.
Paszke,A.,Gross,S.,Chintala,S.,Chanan,G.,Yang,E.,DeVito,Z.,Lin,Z.,Desmaison,A.,Antiga,
L.,andLerer,A. Automaticdifferentiationinpytorch. InNIPS-W,2017.
Petersen,F.,Borgelt,C.,Kuehne,H.,andDeussen,O. Monotonicdifferentiablesortingnetworks. In
InternationalConferenceonLearningRepresentations(ICLR),2022.
Polyanskiy,Y.andWu,Y. Lecturenotesoninformationtheory. LectureNotesforECE563(UIUC)
and,6(2012-2016):7,2014.
Polyanskiy,Y.,Poor,H.V.,andVerdu,S. ChannelCodingRateintheFiniteBlocklengthRegime.
IEEETransactionsonInformationTheory,56(5):2307–2359,May2010.
Raginsky,M.andSason,I. Concentrationofmeasureinequalitiesininformationtheory,communica-
tions,andcoding. Numbervol.10,no.1-2inFoundationsandTrendsinCommunicationsand
InformationTheory.NowPubl,Boston,Mass.,2013. ISBN978-1-60198-724-2.
Romano,Y.,Patterson,E.,andCandes,E. Conformalizedquantileregression. Advancesinneural
informationprocessingsystems,32,2019.
Romano,Y.,Sesia,M.,andCandes,E. Classificationwithvalidandadaptivecoverage. Advancesin
NeuralInformationProcessingSystems,33:3581–3591,2020.
Sadinle,M.,Lei,J.,andWasserman,L. Leastambiguousset-valuedclassifierswithboundederror
levels. JournaloftheAmericanStatisticalAssociation,114(525):223–234,2019.
Sason, I. On data-processing and majorization inequalities for f-divergences with applications.
Entropy,21(10):1022,2019. Publisher: MDPI.
Sason,I.andVerdú,S. f-DivergenceInequalities. IEEETransactionsonInformationTheory,62(11):
5973–6006,November2016.
Sason,I.andVerdú,S. Arimoto–RényiconditionalentropyandBayesianM-aryhypothesistesting.
IEEETransactionsonInformationtheory,64(1):4–25,2017. Publisher: IEEE.
Shafer,G.andVovk,V. Atutorialonconformalprediction. JournalofMachineLearningResearch,
9(3),2008.
Stutz,D.,Dvijotham,K.D.,Cemgil,A.T.,andDoucet,A. Learningoptimalconformalclassifiers.
InInternationalConferenceonLearningRepresentations,2022.
Tibshirani,R.J.,FoygelBarber,R.,Candes,E.,andRamdas,A.Conformalpredictionundercovariate
shift. Advancesinneuralinformationprocessingsystems,32,2019.
Vovk, V., Gammerman, A., andShafer, G. Algorithmiclearning inarandomworld, volume 29.
Springer,2005.
Vovk,V.,Fedorova,V.,Nouretdinov,I.,andGammerman,A. Criteriaofefficiencyforconformal
prediction. In Conformal and Probabilistic Prediction with Applications: 5th International
Symposium,COPA2016,Madrid,Spain,April20-22,2016,Proceedings5,pp.23–39.Springer,
2016.
Wang,J.,Charles,Z.,Xu,Z.,Joshi,G.,McMahan,H.B.,Al-Shedivat,M.,Andrew,G.,Avestimehr,S.,
Daly,K.,Data,D.,etal. Afieldguidetofederatedoptimization. arXivpreprintarXiv:2107.06917,
2021.
Wisniewski,W.,Lindsay,D.,andLindsay,S.Applicationofconformalpredictionintervalestimations
tomarketmakers’netpositions. InConformalandProbabilisticPredictionandApplications,pp.
285–301.PMLR,2020.
Wozencraft,J.M. Listdecoding. PhDthesis,ResearchLaboratoryofElectronics,MIT,Cambridge,
MA,USA,January1958. Publisher: ResearchLaboratoryofElectronics,MIT.
Xiao,H.,Rasul,K.,andVollgraf,R. Fashion-mnist:anovelimagedatasetforbenchmarkingmachine
learningalgorithms. arXivpreprintarXiv:1708.07747,2017.
13A BackgroundonConformalPrediction
Inthissectionweoutlineabriefintroductiontoconformalprediction,providingthereaderwiththe
necessarybackgroundtofollowourmainresults. Readersalreadyfamiliarwithconformalprediction
cansafelyskipthissection. Westartbyreviewingthedefinitionsofquantilesandexchangeability,
whicharecentraltothemainresultsinconformalprediction.
A.1 Exchangeability,RanksandQuantiles
Themainassumptioninconformalpredictionisthatthedatapointsusedforcalibrationandtesting
areexchangeable. Next,wedefinetheconceptofexchangeabilityanddiscusshowitleadstothe
mainresultsinconformalpredictionviapropertiesofranksofexchangeablerandomvariables. Our
expositionismarkedlybrief, andwereferthereadertoKuchibhotla(2020)foramorethorough
discussiononexchangeabilityanditsimportanceinconformalprediction.
Formally,theconceptofexchangeabilitycanbedefinedasbelow.
DefinitionA.1(ExchangeableRandomVariables). RandomvariablesX ,...,X aresaidtobe
1 n
exchangeableifforanypermutationπ :{1,...,n}→{1,...,n},thesequences(X ,...,X )and
1 n
(X ,...,X )havethesamejointprobabilitydistribution.
π(1) π(n)
Notethatexchangeabilityisaweakerassumptionthanthei.i.d.(independentandidenticallydis-
tributed) assumption commonly relied upon in machine learning. More precisely, exchangeable
randomvariablesmustbeidenticallydistributedbutnotnecessarilyindependentKuchibhotla(2020).
Naturally,i.i.d.randomvariablesarealsoexchangeable.
Onerelevantconsequenceofexchangeability,thatiscentraltoconformalprediction,isthattheranks
ofexchangeablerandomvariablesareuniformlydistributed. Wedefineranksandthispropertymore
formallyinDefinitionA.2andLemmaA.4,respectively.
DefinitionA.2(Rank). ForasetofnelementsX ={x ,...,x },therankofanyoneelementx
1 n i
inX isdefinedas
rank(x ;X)=|{j ∈{1,...,n}:x +ξU ≤x +ξU }|,
i j j i i
forξ ≥0andU ={U ,...,U }asetofi.i.d.randomvariablesuniformlydistributedin[−1,−1].
1 n
Remark A.3. The addition of i.i.d. uniform noise serves as a tie-breaking mechanism. Since
{U ,...,U }arealmostsurelydistinct,{x +ξU }n arealsodistinctwithprobabilityone. This
1 n i i i=1
is necessary to render the rank independent of the distribution of X , which is key to ensure the
i
distribution-freequalityofconformalprediction.
LemmaA.4. If(X ,...,X )areexchangeablerandomvariables,then
1 n
(rank(X ;{X ,...,X }))n ∼Unif({π :{1,...,n}→{1,...,n}}).
i 1 n i=1
In words, Lemma A.4 tell us that the ranking of exchangeable random variables is uniformly
distributedamongallpossiblepermutationsπ :{1,...,n}→{1,...,n}.Thatmeanstheprobability
ofobservinganyonerankingisequalto1/n!and,importantly,independentofthedistributionofX.
ThecorollarybelowfollowsdirectlyfromLemmaA.4.
CorollaryA.1. If(X ,...,X )areexchangeablerandomvariables,then
1 n
⌊t⌋
P(rank(X ;{X ,...,X })≤t)= ,
i 1 n n
for t ∈ [0,n] and ⌊t⌋ the smallest integer smaller or equal to t. Moreover, we can define a valid
p-valueasP :=rank(X ;{X ,...,X })/nsince
i 1 n
P(P ≤α)≤α forallα∈[0,1].
14Proof.
P(rank(X ;{X ,...,X })≤t)=P(rank(X ;{X ,...,X })≤⌊t⌋)
i 1 n i 1 n
⌊t⌋
(cid:88)
= P(rank(X ;{X ,...,X })=i)
i 1 n
i=1
⌊t⌋
(cid:88)(n−1)! ⌊t⌋
= = ,
n! n
i=1
wherethefirstequalityfollowsbecauserank(.)returnsaninteger,andthethirdequalityfollowsdi-
rectlyfromLemmaA.4:eachpermutationof(rank(X ;{X ,...,X }))n hasthesameprobability
i 1 n i=1
1/n!,andthereare(n−1)!configurationswhererank(X i;{X 1,...,X n})=isinceX
i
isfixedat
ranki,andwecanpermutetheother(n−1)variables.
Asweshallsee,CorollaryA.1iscentraltothemainresultinconformalprediction,butbeforeproving
thatresult,weshouldfirstdefinetheconceptofquantiles.
Definition A.5 (Quantile). For Z a random variable with probability distribution F, the level β
quantileofdistributionF isdefinedas
Quantile(β;F)=inf{z :P{Z ≤z}≥β}.
Similarly,forasample{z }n andδ apointmassconcentratedatz ,thequantileoftheempirical
i i=1 zi i
distributionisgivenby
(cid:32) n (cid:33)
(cid:0) (cid:1) 1 (cid:88)
Quantile β;{z } =Quantile β; δ .
i i∈[n] n zi
i=1
A.2 ConformalPrediction
Armedwiththedefinitionsofquantilesandexchangeability,wearepreparedtostudyconformal
prediction,adistribution-freeuncertaintyquantificationframeworkwiththefollowinggoal: given
asetofdatapoints{(X ,Y )}n sampledfromsomedistributionP onX ×Y,toconstructaset
i i i=1
predictorC :X →2Y suchthatforanewdatapoint(X ,Y )andtargeterrorrateα∈(0,1),
test test
wehavetheguarantee
P(Y ∈C(X ))≥1−α,
test test
where the probability is over the randomness of {(X ,Y )}n ∪{(X ,Y )}. In words, this
i i i=1 test test
guaranteesusthatthecorrectlabelY isincludedintheconstructedsetC(X )withprobability
test test
atleast1−α.
Therearedifferentwaystoachievethisguarantee,butforsimplicitywewillfocusonsplitconformal
prediction or SCP Papadopoulos et al. (2002), since it is easier to grasp than other variants of
conformal prediction and is also the main object of study in this paper. We start by assuming
a calibration dataset D which consists of n i.i.d. samples (X ,Y ) drawn from an unknown
cal i i
distributionoverX ×Y. Wealsoassumeaccesstoamodelf :X →Yˆ,wheretheoutputspaceYˆ
canbedifferentfromY. Predictionsetssatisfyingtheguaranteeabovecanbeconstructedviathe
followingthreesteps.
1. Define a non-conformity score function s : X ×Y → R, which assigns high scores to
unusualpairs(x,y). Thescorefunctionistypicallyafunctionofthemodelf itself.
2. ComputeS =s(X ,Y )forall(X ,Y )∈D andcomputeQuantile(1−α;{S }n ∪
i i i i i cal i i=1
{∞}),theempirical1−αquantileofthescoresinD .
cal
3. ConstructpredictionsetsC(X )=(cid:8) y :s(X ,y)≤Quantile(1−α;{S }n ∪{∞})(cid:9)
test test i i=1
Theorem2.1. (sameresultasinthemaintext(Vovketal.(2005);Leietal.(2018))
Let{(X ,Y )}nbei.i.d.(oronlyexchangeable)randomvariables,and{S }n bethecorresponding
i i i i i=1
setofscoresS = s(X ,Y )giventoeachpair(X ,Y )bysomescorefunctions : X ×Y → R.
i i i i i
Then for a new i.i.d. draw (X ,Y ) and any target error rate α ∈ (0,1), the prediction set
test test
constructedas
C(X )=(cid:8) y :s(X ,y)≤Quantile(1−α;{S }n ∪{∞})(cid:9)
test test i i=1
15satisfiesthemarginalcoverageproperty
P(Y ∈C(X ))≥1−α,
test test
Moreover,if{S i}n i=1arealmostsurelydistinct,thisprobabilityisupperboundedby1−α+1/n+1.
Proof. For simplicity we assume that the set of scores {S }n are distinct (or have been made
i i=1
distinctbyasuitablerandomtie-breakingrule). DenoteS = s(X ,Y )andobservethat
test test test
sinces(·)isappliedelement-wisetoeachpair(X ,Y )itpreservesexchangeability,andthusrandom
i i
variables{S }n ∪{S }arealsoexchangeable. Next,weshowthatthefollowingeventsareall
i i=1 test
equivalent
Y ∈C(X )⇐(i ⇒) S ≤Quantile(1−α;{S }n ∪{∞})
test test test i i=1
⇐(i ⇒i) S ≤Quantile(1−α;{S }n ∪{S })
test i i=1 test
⇐(ii ⇒i) rank(S ;{S }n ∪{S })≤⌈(n+1)(1−α)⌉.
test i i=1 test
(i) followsfromtheconstructionofthepredictionsetitselfC(X )itself.
test
(ii) canbeeasilyverifiedasfollows. IfS ≤Quantile(1−α;{S }n ∪{∞}),thenshifting
test i i=1
allvaluesS ≥S toarbitraryvalueslargerthanS willnotchangethevalidityofthe
i test test
inequality,sincethe1−αquantileremainsunchanged. Inparticular,theinequalityholds
whenreplacing{S }with{∞}andvice-versa.
test
(iii) follows from the fact that if S ≤ Quantile(1−α;{S }n ∪{S }), then S is
test i i=1 test test
amongthe⌈(n+1)(1−α)⌉smallestvaluesoftheset{S }n ∪{S }.
i i=1 test
Finally, this where the crucial exchangeability assumption comes into play, allowing us to apply
CorollaryA.1toget
⌈(n+1)(1−α)⌉
P(Y ∈C(X ))=P(rank(S ;{S }n ∪S )≤⌈(n+1)(1−α)⌉)=
test test test i i=1 test n+1
Fromthereitiseasytoverifythattherighthandsideisatleast1−αandatmost1−α+1/n+1.
16B BackgroundonListDecodingandBasicInformationTheoreticResults
Inthissectionweenunciatesomeclassicalresultsfrominformationtheorythatareinstrumental
inderivingourmaintheoreticalresults,asweshowinthedetailedproofsinAppendixC.Wealso
provideabriefintroductiontolistdecodinganddemonstrateconformalpredictioncanbeframedasa
listdecodingproblem.
B.1 Dataprocessinginequalitiesforf-divergence
Westartbypresentingthedataprocessinginequality(DPI)forf-divergence,whichwedefinebelow.
Definition B.1 (f-Divergence). Consider two probability measures P and Q and assume that
the measure P is absolutely continuous with respect to Q, i.e., P ≪ Q. For a convex function
f :(0,∞)→Rwithf(1)=0,thef-divergenceisdefinedas:
(cid:20) (cid:18) (cid:19)(cid:21)
dP
D (P||Q):=E f ,
f Q dQ
where dP isaRadon-Nikodymderivative. Inparticular,usingf(x)=xlogxwerecoverthefamiliar
dQ
notionofKL-divergence
(cid:20) (cid:18) (cid:19)(cid:21)
dP
D (P||Q):=E log .
KL P dQ
Wecansimilarlydefineconditionalf-divergence.
DefinitionB.2(Conditionalf-Divergence). ConsidertwoprobabilitymeasuresP andQsuchthat
P :=P P andQ:=P Q andthatthemeasureP isabsolutelycontinuouswithrespectto
X Y|X X Y|X
Q. Foraconvexfunctionf :(0,∞)→Rwithf(1)=0,theconditionalf-divergenceisdefinedas:
D (P ||Q |P ):=E (cid:2) D (P ||Q )(cid:3) .
f Y|X Y|X X PX f Y|X=x Y|X=x
TheoremB.3istheclassicaldataprocessinginequality(DPI),whichwerestatebelowforthesakeof
completeness. TheclassicalversionsofDPI,statedintermsofmutualinformation,canbefoundin
classicalinformationtheoretictextbookslikeCover&Thomas(2006),whilethegeneralizationof
DPItof-divergencecanbefoundinotherworkswithacomprehensivesurveyinPolyanskiy&Wu
(2014);Sason&Verdú(2016).
TheoremB.3(DataProcessingInequalityforf-divergence). Consideraconditionaldistribution
W . SupposethatP andQ aretwodistributionsobtainedbymarginalizationofP W and
Y|X Y Y X Y|X
Q W overX. Foranyf-divergence,wehave
X Y|X
D (P ||Q )≥D (P ||Q ).
f X X f Y Y
The proof can be found in standard textbooks in information theory, see for example Chapter 7,
Section7.2. inPolyanskiy&Wu(2014)foraderivationoftheDPIenunciatedinthesameformas
above.
Next,weconsidertheapplicationtoconformalprediction.
TheoremB.4(DataProcessingInequalityforConformalPrediction). ForanysetfunctionC :X →
2Y,anyf-divergence,andalldistributionpairsP,Qon(X,Y),wehave:
D (P||Q)≥d (P(Y ∈C(x))||Q(Y ∈C(x))),
f f
whered (p||q)isthebinaryf-divergence,namelyd (p||q)=qf(p/q)+(1−q)f(1−p/1−q).
f f
Theproofsimplyfollowsfromthedataprocessinginequalitybyconsideringtherandomvariable
E = 1(Y ∈ C(x)) and the conditional distribution P = E [1(Y ∈ C(x))|X,Y]. This
E|X,Y Dcal
constructioncanbefurtherimprovedasfollows.
Theorem B.5 (Conditional Data Processing Inequality for Conformal Prediction). For any set
functionC : X → 2Y,anyf-divergence,andallconditionaldistributionpairsP ,Q ,and
Y|X Y|X
P ,wehave:
X
E D (P ||Q )≥E d (cid:0) P (Y ∈C(x)|X =x)||Q (Y ∈C(x)|X =x)(cid:1) ,
PX f Y|X=x Y|X=x PX f Y|X Y|X
whered (p||q)isthebinaryf-divergence,namelyd (p||q)=qf(p/q)+(1−q)f(1−p/1−q).
f f
17Proof. ConsidertheconditionaldistributionP =E [1(Y ∈C(x))|X =x,Y]. Wehave
E|X=x,Y Dcal
that
D (P ||Q )=D (P P ||Q P ) (11)
f Y|X=x Y|X=x f Y|X=x E|X=x,Y Y|X=x E|X=x,Y
=D (P ||Q ) (12)
f Y,E|X=x Y,E|X=x
andfromthemonotonicitypropertyofthef-divergence(Polyanskiy&Wu,2014)wehavethat
D (P ||Q )=D (P ||Q ) (13)
f Y|X=x Y|X=x f Y,E|X=x Y,E|X=x
≥D (P ||Q ) (14)
f E|X=x E|X=x
=d (P (Y ∈C(x)|X =x)||Q (Y ∈C(x)|X =x)). (15)
f Y|X Y|X
BytakingtheexpectationwithrespecttoP ,weconcludetheproof.
X
B.2 ListDecoding
ListdecodingElias(1957)isanotioncomingfromcodingtheory, alargebranchofengineering
and mathematics that arises from the application of information theory to the design of reliable
communicationsystemsandrobustinformationprocessingandstorage.Inparticular,weareinterested
inchannelcoding,afieldconcentratedonthedesignofso-callederror-correctingcodestoenable
reliablecommunicationoverinaccurateornoisycommunicationchannels. Thegeneralsetupstudied
in channel coding, including list decoding, can be summarized as follows. A message y ∈ Y is
encodedandtransmittedoveranoisychannel,andamessageyˆ∈Yˆ isreceived. Thenoisychannel
isgovernedbyprobabilitydensityp(yˆ|y)thatdescribestheprobabilityofobservingoutputYˆ ∈Yˆ
giveninputY ∈ Y. Thereceiverattemptstodecodeyˆ,thatis,toguesstheoriginallytransmitted
message y from the received one, yˆ. At this point, parallels to machine learning should already
havebecomeclear. Ifthereceiverprovidesasingleguessofthetransmittedmessagey,weareina
unique-decodingscenario,whichisakintoapointpredictioninmachinelearning. Conversely,ifthe
receiverisallowedtoguessalistofthemostlikelymessages,wehavelist-decoding,whichclosely
resemblesconformalprediction.
More formally, a list-decoding algorithm can be defined by a set predictor L : Yˆ → 2Y, with
maximum output set size |L(Yˆ)| ≤ M. Naturally, the goal is to design the function L so as to
maximizetheprobabilityofY ∈ L(Yˆ).Similarly,foragiveninput-labelpair(X,Y)thegoalof
conformalpredictionistoprovideasetbasedontheoutputofagivenmodelYˆ =f(X)thatcontains
Y withapre-determinedconfidence. Forinstance,Yˆ couldbethelogitsoutputbythemodel,i.e.,
avectorwiththedimensionequaltonumberofclasses. IfweconsiderYˆ thenoisyobservationof
theground-truthY,thentheproblemofdeterminingasetcontainingY isthelistdecodingproblem.
SinceYˆ = f(X)isafunctionoftheinputdataX, wecanconsiderthecommunicationchannel
P(X|Y),whichmotivatesourboundsontheconditionalentropyH(Y|X).Alternatively,wecanalso
considerthecommunicationchannelP(Yˆ|Y;X)directly,whichjustifiesapplyingthesameupper
boundstoH(Y|Yˆ).Notethatinthislastcase,theinputdataX canalsobetakenassideinformation,
althoughitisrarelyuseddirectlyinbuildingtheconformalpredictionset.
Thisreinterpretationofconformalpredictionaslistdecodingallowsustoapplysomeofthestandard
resultsfromthelistdecodingliteraturetoconformalprediction,asweshowinthenextsection.
B.3 Listdecoding: InformationTheoreticInequalities
Fano’sinequalityforvariablesizelistdecoding. ThefollowinggeneralizationofFano’sinequality
isgivenin[Raginsky&Sason(2013),Appendix3.E].
TheoremB.6. ConsiderascenariowhereadecoderuponobservingYˆ providesanonemptylist
L(Yˆ)thatcontainsanotherrandomvariableY ∈ Y with|Y| = M. DefineP := P(Y ∈/ L(Yˆ)).
e
Wehave:
H(Y|Yˆ)≤h (P )+P log(M)+E(log|L(Yˆ)|).
b e e
Optimallistdecodingandconformalprediction. Itcanbeshownthattheoptimallistdecoder
consistsofselectingL(yˆ)elementsofY withhighestconditionalprobabilityp(y|yˆ).Thatis,consider
18thesortedposteriorsunderthetruedistributionp(y |yˆ)≥p(y |yˆ)≥···≥p(y |yˆ)andchoosethe
1 2 M
first{y ,...,y }forsomelistsize|L|. However,thisruleisforfixed-sizelistdecodinganddoes
1 |L|
notdeterminehowtoselectthecoveragesetsizetoguaranteeagivencoverage. Wecanmodifythis
ruletoobtainavariable-sizelistdecodingwiththerequiredcoverage. Assumingagainthesorted
posteriorsp(y |yˆ)≥p(y |yˆ)≥···≥p(y |yˆ),wecanselectthesetasfollows:
1 2 M
(cid:40) j (cid:41)
(cid:88)
L(yˆ)={y ,...,y } whereℓ :=inf j : p(y |yˆ)≥1−α .
1 ℓy y i
i=1
Itiseasytoseethattheabovesetisthesmallestsetgiveneachy andtheconfidencelevel1−α
(see,forexample,Merhav(2014)). ThesameresultholdsinconformalpredictionVovketal.(2016);
Romanoetal.(2020).
B.4 FanoandDataProcessingInequalitiesforConformalPrediction
First,fromFano’sinequalityforlistdecoding,TheoremB.6,wegetthenextresult‘out-of-the-box’.
PropositionB.7. Supposethat|Y|=M. Anyconformalpredictionmethodwiththepredictionset
C(x)andconfidencelevel1−α,α∈(0,0.5),satisfiesthefollowinginequality:
H(Y|X)≤h (α)+αlog(M)+E([log|C(x)|]+),
b
where h (·) is the binary entropy function, [x]+ := max{x,0} and H(Y|X) is computed using
b
the true distribution P . When the conformal prediction is merely based on the model output
XY
Yˆ =f(X),theinequalitycanbemodifiedto:
H(Y|Yˆ)≤h (α)+αlog(M)+E([log|C(x)|]+).
b
ThepropositionfollowseasilyfromTheoremB.6byusingtheconditionP(Y ∈ C(x)) ≥ 1−α.
NotethatinTheoremB.6,weassumenon-emptylists,whereasinPropositionB.7weallowempty
predictionsetsbutapplythemaximumoperator[x]+ :=max{x,0}. Thisjustifiedbecausethelast
termofFano’sinequalityrelatestotheprobabilityofcorrectassignmentsY ∈C(X)whichnever
happensforemptysets. SeePropositionD.1fortheproof,wherethesameresultappears.
Theboundsthatwepresentinthemainpaper,model-basedandsimpleFanobounds,areactually
derivedthroughaslightlydifferentrootbyleveragingthelowerandupperboundsinthefinite-sample
guaranteeofconformalprediction(Theorem2.1). WederivetheseothertwoboundsinAppendixC.2.
B.5 RelatedWorkonInformationTheoreticInequalities
Theuseoff-divergencegoesbacktoworksofAliandSilvey,Csiszár,andMorimotointhe60s—see
forinstance(Ali&Silvey,1966;Csiszár,1967;Morimoto,1963). Akeyf-divergenceinequality
isthedataprocessinginequality,whichwasusedininformationtheorytoestablishvariousupper
boundsontheachievabilityofcodingschemesfordifferenttasks—see(Sason&Verdú,2016;Sason,
2019)foranextensivesurvey. Moreover,thedataprocessinginequalityforf-divergencesprovidesa
unifiedwayofobtainingmanyclassicalandnewresults,forexampleFano’sinequality(Fano,1949).
ThetightnessofdataprocessinginequalitiesisdiscussedintermsofBregman’sdivergencein(Collet,
2019;Liese&Vajda,2006)andintermsofχ2-divergenceinSason(2019).
Whenitcomestolistdecoding, thereareanumberofrelevantinequalitiesintheliterature. List
decodingwasintroducedincontextcommunicationdesignbyElias(Elias,1957)andWozencraft
(Wozencraft, 1958). For fixed list size, the information theoretic bounds on list decoding were
obtainedin(Gallager,1986)usingerrorexponentanalysis. AgeneralizationofFano’sinequalityto
listdecodingwasgivenin(Csiszár&Körner,2011)inthecontextofmulti-userinformationtheory,
seealsothegeneralFanoinequalityforlistdecodingpresentedin(Raginsky&Sason,2013). For
fixedlistsize,strongerinequalities,somebasedArimoto-Rényiconditionalentropy,werepresented
in (Sason & Verdú, 2017). Variable size list decoding was discussed in (Sason, 2019) using the
notion of E resolvability first introduced in (Polyanskiy et al., 2010) related to the dependence
γ
testingbound. Itwasusedagainincontextofchannelresolvabilityin(Liuetal.,2017),wheresome
relevantinequalitieshavebeenobtainedanddiscussed. Aselectionofmostrelevantinequalitiesfor
listdecodingcanbefoundin(Sason,2019).
19C ProofsofMainTheoreticalResults
Inthissection,weprovidetheproofstoourmainresults,namelytheDPIboundinProposition3.1,
themodel-basedFanoboundinProposition3.2,andthesimpleFanoboundinCorollary3.1. For
notationalconvenience,weusetheshorthandα
n
=α−1/1+ninmostofthestepsofthederivations.
C.1 DPIBound
WestartwiththeDPIboundwhichwerestateandproofbelowusingthedataprocessinginequalities
discussedinAppendixB.Notethat,whenclearfromthecontext,weremoveexplicitdependenceon
thecalibrationsetD fromthederivations. Itisimplicitlyassumedthattheprobabilityoftheevent
cal
Y ∈C(x)iscomputedbymarginalizingoverD .
cal
Proposition3.1. ConsideranyconformalpredictionmethodwiththepredictionsetC(x)withthe
followingfinitesampleguarantee:
1
1−α≤P(Y ∈C(x))≤1−α+
n+1
foranyα ∈ (0,0.5). ForanyarbitraryconditionaldistributionQ ,thetrueconditionaldistri-
Y|X
bution P and the input measure P , define the following two measures Q := P Q and
Y|X X X Y|X
P :=P P . Wehaveforanyα∈(0,0.5)
X Y|X
(cid:18) (cid:19)
1
H(Y|X)≤h (α)+ 1−α+ logQ(Y ∈C(x))
b n+1
+αlogQ(Y ∈/ C(x))−E (cid:2) logQ (cid:3) .
PXY Y|X
Proof. ConsideranarbitrarydistributionQ . ThenwecanuseP ,andP ×Q inthedata
Y|X XY X Y|X
processinginequalityforKL-divergence(TheoremB.4)toget:
D (P P ||P Q )≥d (P(Y ∈C(x))||Q(Y ∈C(x))) (16)
KL X Y|X X Y|X KL
NownotethatwecandecomposeD (P P ||P Q )intermsoftheconditionalentropy
KL X Y|X X Y|X
H(Y|X)andthecross-entropy−E [logQ ]:
PXY Y|X
(cid:20) P P (cid:21) (cid:20) P (cid:21)
D (P P ||P Q )=E log X Y|X =E log Y|X
KL X Y|X X Y|X PXY P Q PXY Q
X Y|X Y|X
=E [logP ]−E [logQ ]
PXY Y|X PXY Y|X
=−H(Y|X)−E [logQ ].
PXY Y|X
Withthedecompositionabove,wecanrearrangethetermsin(16)togetthefollowingupperbound
onH(Y|X)
−H(Y|X)−E (cid:2) logQ (cid:3) ≥d (P(Y ∈C(x))||Q(Y ∈C(x)))
PXY Y|X KL
H(Y|X)≤−d (P(Y ∈C(x))||Q(Y ∈C(x)))−E (cid:2) logQ (cid:3) . (17)
KL PXY Y|X
Wecanthenapplytheupperandlowerboundsfromconformalprediction,i.e. P(Y ∈C(x))≥1−α
andP(Y ∈/ C(x))≥α ,toupperboundd (P(Y ∈C(x))||Q(Y ∈C(x)))asfollows.
n KL
−d (P(Y ∈C(x))||Q(Y ∈C(x)))=
KL
h (P(Y ∈C(x)))+P(Y ∈C(x))logQ(Y ∈C(x))+P(Y ∈/ C(x))logQ(Y ∈/ C(x))
b
≤h (α)+(1−α)logQ(Y ∈C(x))+α logQ(Y ∈/ C(x)),
b n
whereh (·)isthebinaryentropyfunction,thatis,h (α)=−αlog(α)−(1−α)log(1−α).The
b b
equality in the second line follows from the definition of the binary KL divergence, and we get
the inequality simply by upper bounding each of the terms individually. In particular, note that
logQ(Y ∈C(x))andlogQ(Y ∈/ C(x))arebothnegative,andh (·)isdecreasingin[0.5,1.0]and
b
symmetricabout0.5,suchthatfortypicalvaluesofα<0.5
P(Y ∈C(X))≥1−α =⇒ h (P(Y ∈C(x)))≤h (1−α)=h (α). (18)
b b b
20Finally,wecanreplacetheupperboundaboveinto(17)toconcludetheproof.
H(Y|X)≤h (α )+(1−α)logQ(Y ∈C(x))+α logQ(Y ∈/ C(x))−E (cid:2) logQ (cid:3) .
b n n PXY Y|X
Remark C.1. The DPI bound always provides a tighter upper bound on the conditional entropy
H(Y|X)thanthecross-entropy,whichiseasilyverifiedin(17)sincetheKLtermisalwaysnon-
negative. ThisservesasanimportantmotivationtooptimizetheDPIboundinsteadofthecross-
entropyinconformaltraining.
RemarkC.2. ThederivationoftheDPIboundplacesnoassumptionsontheconditionaldistribution
Q . However,inpracticetheunderlyingmodelf oftenalreadyprovidessuchadistribution,and
Y|X
sinceitistypicallytrainedtoapproximateP well,itmakessensetotakeQ asthedistribution
Y|X Y|X
definedbythemodelitself. ThatishowweevaluatetheDPIinallofourexperiments. Finally,we
canagainuseH(Y|Yˆ)insteadofH(Y|X)iftheconformalmethodusesmerelyYˆ.
RemarkC.3. Typically,wehaveα∈(0.0,0.5),andweusethisfactintheprooftoboundthebinary
entropyh (P(Y ∈C(x))).Thesamecouldhavebeendoneforα∈(0.5,1.0),butsince1−αlands
b
intheincreasingpartofthebinaryentropyfunctionbetween0and0.5,wehavetoresorttothelower
boundfromconformalpredictiontoget
P(Y ∈C(X))≤1−α =⇒ h (P(Y ∈C(x)))≤h (1−α )=h (α ).
n b b n b n
RemarkC.4. OneoftheappealsoftheDPIboundisthatthetermscanbecomputedinadata-driven
wayusingsamples. Whilethecross-entropycanbeestimatedinanunbiasedwaywithsamplesfrom
thetruedistributionP ,wemustbecarefulwhenestimatingQ(Y ∈C(x)). Themainchallengeis
XY
thatQ(Y ∈C(x))appearsinsidealog,andthusanempiricalestimateQˆ(Y ∈C(x))wouldyielda
lowerboundofthenegativeKLdivergenceandwouldbebiased. Wecangetanupperconfidence
boundonthisestimateviatheempiricalBernsteininequalityMaurer&Pontil(2009),whichwe
restatebelow.
TheoremC.5(EmpiricalBernsteinInequalityMaurer&Pontil(2009)). LetZ,Z ,...,Z bei.i.d.
1 n
randomvariableswithvaluesin[0,1]andletδ >0. Thenwithprobabilityatleast1−δinthei.i.d.
vectorZ=(Z ,...,Z )wehavethat
1 n
n (cid:114)
E[Z]−
1 (cid:88)
Z ≤
2V n(Z)log(2/δ)
+
7log(2/δ)
,
n i n 3(n−1)
i=1
whereV (Z)istheempiricalvarianceoverthe(Z ,...,Z )samples.
n 1 n
Byusingthisbound,wehavewithprobability1−δ:
(cid:114)
2V (Z)log(2/δ) 7log(2/δ)
Q(Y ∈C(x))≤Qˆ(Y ∈C(x))+ n + :=Q˜(Y ∈C(x)),
n 3(n−1)
(cid:114)
2V (Z)log(2/δ) 7log(2/δ)
Q(Y ∈/ C(x))≤Qˆ(Y ∈/ C(x))+ n + :=Q˜(Y ∈/ C(x)),
n 3(n−1)
whereZ = Q(y ∈ C(x ))forx sampledfromP andapredictionsetC(x )obtainedwitha
i i i i X i
calibrationdatasetsampledfromP . Thisyieldsthefollowinginequalitywithprobability1−δ:
Dcal
H(Y|X)≤h (α)+(1−α)logQ˜(Y ∈C(x))+α logQ˜(Y ∈/ C(x))−E (cid:2) logQ (cid:3) .
b n PXY Y|X
Wecanusethisinequalitytoevaluatethebounds.
C.2 Model-BasedFanoBound
Inthissection,provethemodel-basedFanobound,whichwerestateinProposition3.2below.
Proposition3.2. ConsideranyconformalpredictionmethodwiththepredictionsetC(x),andany
distributionQ,withthefollowingfinitesampleguarantee:
1
1−α≤P(Y ∈C(x))≤1−α+ ,
n+1
21forα∈(0,0.5). Then,forthetruedistributionP,andforanyprobabilitydistributionQ,wehave
H(Y|X)≤h (α)+αE (cid:2) −logQ (cid:3)
b PY,X,Dcal|Y∈/C(X) Y|X,C(x),Y∈/C(X)
(cid:18) (cid:19)
+ 1−α+ 1 E (cid:2) −logQ (cid:3) .
n+1 PY,X,Dcal|Y∈C(X) Y|X,C(x),Y∈C(X)
Proof. For notational convenience, define E = 1(Y ∈ C(x)), which by our assumption on the
conformalpredictionmethodmeansthat1−α≤P(E =1)≤1−α . Thestartingpointissimilar
n
tothewell-knownproofofFano’sinequality:
H(E,Y|X,D )=H(Y|X,D )+H(E|Y,X,D )=H(Y|X,D ),
cal cal cal cal
wherethelaststepfollowsbecauseknowingX,Y andD ,weknowifY ∈ C(x),andtherefore
cal
H(E|Y,X,D ) = 0. Furthermore, giventhestructureofthegraphicalmodeloftheconformal
cal
predictionprocess(c.f. Figure1),wehavethatH(Y|X)=H(Y|X,D ),Wenowfindanupper
cal
boundonH(E,Y|X,D ):
cal
H(E,Y|X,D )=H(E|X,D )+H(Y|X,E,D )
cal cal cal
=H(E|X,D )+P(E =0)H(Y|X,E =0,D )+P(E =1)H(Y|X,E =1,D )
cal cal cal
≤H(E)+P(E =0)H(Y|X,E =0,D )+P(E =1)H(Y|X,E =1,D )
cal cal
≤h (α)+P(E =0)H(Y|X,E =0,D )+P(E =1)H(Y|X,E =1,D ), (19)
b cal cal
wherethefirstinequalityfollowsfromthefactthatH(E|X,D )≤H(E),andthelastonecomes
cal
fromthesameargumentfora<0.5in(18). Wecancontinueasfollows
H(Y|X)≤h (α)+P(E =0)H(Y|X,E =0,D )+P(E =1)H(Y|X,E =1,D )
b cal cal
=h (α)+P(E =0)E (cid:2) −logP (cid:3)
b PY,X,Dcal|E=0 Y|X,Dcal,E=0
+P(E =1)E (cid:2) −logP (cid:3)
PY,X,Dcal|E=1 Y|X,Dcal,E=1
=h (α)+P(E =0)E (cid:2) −logQ (cid:3)
b PY,X,Dcal|E=0 Y|X,C(x),E=0
−E[D (P ||Q )]
KL Y|X,Dcal,E=0 Y|X,C(x),E=0
+P(E =1)E (cid:2) −logQ (cid:3)
PY,X,Dcal|E=1 Y|X,C(x),E=1
−E[D (P ||Q )]
KL Y|X,Dcal,E=1 Y|X,C(x),E=1
≤h (α)+P(E =0)E (cid:2) −logQ (cid:3)
b PY,X,Dcal|E=0 Y|X,C(x),E=0
+P(E =1)E (cid:2) −logQ (cid:3) (20)
PY,X,Dcal|E=1 Y|X,C(x),E=1
Thefirstequalitycomesfromthedefinitionoftheconditionalentropy,whereasinthesecondequality
wereplacethetruedistributionP withanarbitraryconditionaldistributionQ plustheKL
Y|X Y|X
divergence between the two distributions. The last inequality then follows simply from the fact
thattheKLdivergenceisnon-negative. Finally,wecanleveragethefinite-sampleguaranteesfrom
conformalprediction,namelyP(E =0)≤αandP(E =1)≤1−α ,toupperboundeachterm,
n
whichyieldstheproof.
H(Y|X)≤h (α)+P(E =0)E (cid:2) −logQ (cid:3)
b PY,X,Dcal|E=0 Y|X,C(x),E=0
+P(E =1)E (cid:2) −logQ (cid:3)
PY,X,Dcal|E=1 Y|X,C(x),E=1
≤h (α)+αE (cid:2) −logQ (cid:3)
b PY,X,Dcal|E=0 Y|X,C(x),E=0
+(1−α )E (cid:2) −logQ (cid:3) (21)
n PY,X,Dcal|E=1 Y|X,C(x),E=1
RemarkC.6. WhentheconformalpredictionpredictionismerelybasedonthemodeloutputYˆ =
f(X),themodel-basedFanoboundcanbemodifiedto
H(Y|Yˆ)≤h (α)+αE (cid:2) −logQ (cid:3)
b PY,X,Dcal|Y∈/C(X) Y|X,C(x),Y∈/C(X)
(cid:18) (cid:19)
+ 1−α+ 1 E (cid:2) −logQ (cid:3) .
n+1 PY,X,Dcal|Y∈C(X) Y|X,C(x),Y∈C(X)
22C.3 SimpleFano
Inthederivationofthemodel-basedFanoboundabove,weplacednoassumptionsonthedistribution
Q Y|X.OnesimplechoicethatweconsiderinthissectionistheuniformdistributionQ
Y|X
=1/|Y|,
akintotheclassicalFano’sinequalityFano(1949)andthelistdecodingresultinPropositionB.7.
Corollary3.1. ConsideranyconformalpredictionmethodwiththepredictionsetC(x),andany
distributionQ,withthefollowingfinitesampleguarantee:
1
1−α≤P(Y ∈C(x))≤1−α+ .
n+1
Forα∈(0,0.5)wehavethefollowinginequality:
H(Y|X)≤h (α)+αE [log(|Y|−|C(X)|)]
b PY,X,Dcal|Y∈/C(X)
+(1−α )E [log|C(X)|].
n PY,X,Dcal|Y∈C(X)
Proof. Notethatifwemakeanerror(E = 0)thecorrectclasswillnotbeinsideC(X)andsince
Q isuniformtheprobabilitywillbespreadequallyamongtheremaining|Y|−|C(X)|labels,and
Y|X
wehaveQ = 1 .Throughthesamelogic,wegetthatQ = 1 ,
Y|X,C(x),E=0 |Y|−|C(X)| Y|X,C(x),E=1 |C(X)|
andpluggingbothinto(21)wegetthesimpleFanobound
RemarkC.7. Onceagain,whentheconformalpredictionpredictionismerelybasedonthemodel
outputYˆ =f(X),theinequalitycanbemodifiedto
H(Y|Yˆ)≤h (α)+αE [log(|Y|−|C(X)|)]
b PY,X,Dcal|Y∈/C(X)
+(1−α )E [log|C(X)|].
n PY,X,Dcal|Y∈C(X)
23D FurtherTheoreticalResultsandProofsonthePredictionSetSize
D.1 Fano’sinequalityformaximalpredictionsetsize
Ifweleveragetheupperboundonthepredictionsetsize,wecanfindalowerboundonthemaximum
coveragesetsize.
PropositionD.1. Supposethat|Y|=M. Consideranyconformalpredictionmethodthatconstructs
thepredictionsetC(X)withthefollowingfinite-sampleguarantee:
1
1−α≤P(Y ∈C(X))≤1−α+
n+1
forα∈(0,0.5). Then,wehavethefollowinginequality:
1
H(Y|X)≤h (α)+αlogM +(1−α+ )sup sup log|C(x)|.
b n+1
Dcalx∈supp(pX)
WecansimilarlyreplaceH(Y|X)withH(Y|Yˆ).
Proof. Weusetheconditionaldataprocessinginequalitywithf(x)=xlogx,P=P ,and
DcalXY
Q=P ×U whereU istheuniformdistributionoverY. WefixtheinputtoX =x,and
DcalX M M
thecalibrationsetD . Theconditionalf-divergence,conditionedonD andX,isgivenby:
cal cal
D (Q ||P )=D (P ||U )
f Y|X=x,Dcal Y|X=x,Dcal KL Y|DcalX=x M
=logM −H(Y|X =x),
wherethelaststepfollowsfromtheindependenceofthecalibrationsetD and(X,Y). Thismeans
cal
that
E D (P ||Q )=logM −H(Y|X). (22)
Dcal,X f Y|X=x,Dcal Y|X=x,Dcal
Ontheotherhand,wehave
d (Q(Y ∈C(x)|D ,X =x)||P(Y ∈C(x)|D ,X =x))
f cal cal
=d (P(Y ∈C(x)|D ,X =x)||Q(Y ∈C(x)|D ,X =x))
KL cal cal
|C(x)|
=−h (P(Y ∈C(x)|D ,X =x))−P(Y ∈C(x)|D ,X =x)log
b cal cal M
M −|C(x)|
−P(Y ∈/ C(x)|D ,X =x)log . (23)
cal M
Now,wecanplugboth(22)and(23)intothedataprocessinginequalityofTheoremB.5. Rearranging
thetermsandgettingtheexpectationfrombothsidesw.r.t. D andX wouldyield:
cal
H(Y|X)≤E[h (P(Y ∈C(x)|D ,X))]
b cal
+E[P(Y ∈/ C(x)|D ,X)log(M −|C(x)|)]+E[P(Y ∈C(x)|D ,X)log|C(x)|]
cal cal
−E[P(Y ∈/ C(x)|D ,X)logM]−E[P(Y ∈C(x)|D ,X)logM]+logM
cal cal
≤h (P(Y ∈C(x)))+P(Y ∈/ C(x))logM +E[P(Y ∈C(x)|D ,X)log|C(x)|]
b cal
≤h (α)+αlogM +E[P(Y ∈C(x)|D ,X)log|C(x)|].
b cal
Note that the logM terms in the third line cancel each other out, and that the second inequality
comesfromresolvingtheexpectationsandlog(M−|C(X)|)≤logM.Further,inthelastinequality,
we used the concavity of binary entropy as in (18). Finally, using respectively the inequalities
P(Y ∈C(x)|D ,X)≤1andlog|C(x)|≤sup sup log|C(x)|,wegettwoinequalities:
cal Dcal x∈X
H(Y|X)≤h (α)+αlogM
+E(cid:0) [log|C(x)|]+(cid:1)
b
1
H(Y|X)≤h (α)+αlogM +(1−α+ )sup sup log|C(x)|.
b n+1
Dcalx∈supp(PX)
ThefirstoneistheFano’sinequalityforlistdecodingfromPropositionB.7whilethesecondone
yields the theorem. Note that if C(x) is an empty set, the probability P(Y ∈ C(x)|D ,X) is
cal
zero,andthetermP(Y ∈C(x)|D ,X)log|C(x)|disappears. Therefore,ifweusetheinequality
cal
P(Y ∈C(x)|sup ,X)≤1,weneedtointroducetheterm[log|C(x)|]+tokeeptheexpectation
Dcal
welldefined.
24D.2 Fano’sinequalityforlowerboundonpredictionsetsize
Inasimilarmanner,wecanalsoobtainlowerboundsforthesetsize. Morespecifically,wehavethat
q(y|x)I[y ∈C(x)] 1
Q = = q(y|x)I[y ∈C(x)]
Y|X,C(x),E=1 (cid:80) q(y|x) |C(x)|E [q(y|x)]
y∈C(x) u(yC(x))
1
:= Qˆ1 (24)
|C(x)|E [q(y|x)] Y|X
u(yC(x))
q(y|x)I[y ∈/ C(x)] 1
Q = = q(y|x)I[y ∈/ C(x)]
Y|X,C(x),E=0 (cid:80) q(y|x) (M −|C(x)|)E [q(y|x)]
y∈/C(x) u(y C(¯x))
1
:= Qˆ0 (25)
(M −|C(x)|)E [q(y|x)] Y|X
u(y C(¯x))
whereu(y C(x))andu(y C(¯x))denoteuniformdistributionsoverthelabelsintheconfidenceset. By
consideringthestandardconformalpredictionboundsontheerrorprobabilities,wehavethat
(cid:104) (cid:105)
H(Y|X)≤h (α)+αE −logQˆ0 +log(M −C(x))+logE [q(y|x)]
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
(cid:18) 1 (cid:19) (cid:104) (cid:105)
+ 1−α+ E −logQˆ1 +log|C(x)|+logE [q(y|x)]
n+1 PY,X,Dcal|E=1 Y|X u(yC(x))
(cid:104) (cid:105)
≤h (α)+αlogM +αE −logQˆ0 +logE [q(y|x)]
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
(cid:18) 1 (cid:19) (cid:104) (cid:105)
+ 1−α+ E −logQˆ1 +log|C(x)|+logE [q(y|x)]
n+1 PY,X,Dcal|E=1 Y|X u(yC(x))
whichleadsto
E [log|C(x)|]≥
E=1
(cid:104) (cid:105)
H(Y|X)−h (α)−αlogM −αE −logQˆ0 +logE [q(y|x)]
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
1−α+ 1
n+1
(cid:104) (cid:105)
−E −logQˆ1 +logE [q(y|x)] .
PY,X,Dcal|E=1 Y|X u(yC(x))
Note that when E = 1, we know that Y ∈ C(x), and therefore |C(x)| > 0 and [log|C(x)|]+ =
log|C(x)|. Usingthis,wecanfindanupperboundonE (log(|C(x)|))asfollows:
E=1
E (log(|C(x)|))=E ([log(|C(x)|)]+)
E=1 E=1
(cid:18)E([log|C(x)|]+)
P(E =0)
(cid:19)
= − E ([log|C(x)|]+)
P(E =1) P(E =1) E=0
(cid:18)E([log|C(x)|]+)(cid:19) (cid:18)E([log|C(x)|]+)(cid:19)
≤ ≤ .
P(E =1) 1−α
Thisleadsto:
E([log|C(x)|]+)≥
(cid:104) (cid:105)
H(Y|X)−h (α)−αlogM −αE −logQˆ0 +logE [q(y|x)]
(1−α)
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
1−α+ 1
n+1
(cid:104) (cid:105)
−(1−α)E −logQˆ1 +logE [q(y|x)]
PY,X,Dcal|E=1 Y|X u(yC(x))
All the above terms can be approximated from samples. We summarize this in the following
proposition.
25PropositionD.2. Foranyconformalpredictionschemewiththecoverageguaranteeof1−α,and
anydistributionq(·),wehave:
E([log|C(x)|]+)≥
(cid:104) (cid:105)
H(Y|X)−h (a)−alogM −αE −logQˆ0 +logE [q(y|x)]
(1−α)
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
1−α+ 1
n+1
(cid:104) (cid:105)
−(1−α)E −logQˆ1 +logE [q(y|x)] (26)
PY,X,Dcal|E=1 Y|X u(yC(x))
whereQˆ0 =q(y|x)I[y ∈/ C(x)]andQˆ1 =q(y|x)I[y ∈C(x)]. Whentheconformalprediction
Y|X Y|X
predictionismerelybasedonthemodeloutputYˆ =f(X),theinequalitycanbemodifiedto
E([log|C(x)|]+)≥
(cid:104) (cid:105)
H(Y|Yˆ)−h (a)−alogM −αE −logQˆ0 +logE [q(y|x)]
(1−α)
b PY,X,Dcal|E=0 Y|X u(y C(¯x))
1−α+ 1
n+1
(cid:104) (cid:105)
−(1−α)E −logQˆ1 +logE [q(y|x)] (27)
PY,X,Dcal|E=1 Y|X u(yC(x))
RemarkD.3. Ifweusetheuniformdistributionintheabovebound,wegetaboundsimilartowhatis
obtainedfromFano’sinequalitygiveninPropositionB.7,butwithanadditionalfactorof 1−α .
1−α+ 1
n+1
Sincethefactorissmallerthanone, thecurrentboundwiththechoiceofuniformdistributionis
looserthanFano’sbound,althoughthegapvanishesforlargen.
26E ConformalTraining
Splitconformalprediction(SCP)Papadopoulosetal.(2002)hasquicklybecomeapopularframework
foruncertaintyestimation,largelythankstoitscomputationalefficiency. Oneonlyneedsaccesstoa
separatecalibrationdatasettoderivepredictionsetswithvalidmarginalcoveragefromanypretrained
model. Giventhattrainingnewmachinelearningmodelsisbecomingevermoretime-consuming
andexpensivewithnew,largerarchitectures,thisabilityofapplyingconformalpredictiontoexisting
modelsisinvaluableinanumberofapplications. Yet,itisreasonabletoexpectthattheperformance
ofthefinalsetpredictorcouldbeimprovediftheconformalpredictionprocessweretobeaccounted
forduringtrainingofthemodelaswell,steeringthemodeltowardsbetterpredictiveefficiency,asit
were. Thatisthemotivationbehindthelineofworkthatwebroadlyrefertoasconformaltraining
Bellotti(2021);Stutzetal.(2022). Inanutshell,conformaltrainingintroducesadifferentiable,and
henceapproximate,conformalpredictionstepduringtrainingsothatonecandirectlyoptimizefor
desiredpropertiesofthesetpredictor,mostnotablyitspredictiveefficiency.
Tothatbestofourknowledge,Bellotti(2021)werethefirsttoadvancetheideaofconformaltraining.
Thecentralstepoftheirapproachistorelaxthepredictionsetdefinedbythemodelf anddefine
“soft”predictionsetsCˆ (x),whichcontaineachofthelabelsy ∈Y withcertainprobability. Thatis,
f
ifC (x,y) ∈ {0,1}isthehardassignmentoflabelytoC(x),acorrespondingsoftversionofthis
f
assignmentcanbedefinedas
(cid:18) (cid:19)
qˆ−s (x,y)
Cˆ (x,y):=σ f (28)
f T
wheres (x,y)isthenon-conformityscorefunctiondefinedbymodelf evaluatedat(x,y),qˆisa
f
thresholdingvalue,σisthelogisticsigmoidfunction,andT isatemperatureparametercontrolling
thesmoothnessofthesoftassignment. ThenwecandefineCˆ (x)asthevectorcollectionallthesoft
f
assignmentsCˆ (x,y)foralllabelsy ∈Y. SimilarlythesizeofCˆ (x)canbenaturallydefinedas
f f
|Cˆ (x)|:= (cid:88) Cˆ (x,y).
f f
y∈Y
Bellotti(2021)thengoesontoproposethefollowinglossfunctionswhicharecomputedforeach
trainingbatchB
  2
L size(f)= |B1
|
(cid:88) g(cid:16) |Cˆ f(x))|(cid:17) L coverage(f)= |B1
|
(cid:88) Cˆ f(x,y)−(1−α) ,
x∈B (x,y)∈B
whereαisthedesiredcoveragerateandgisauser-definedfunctionofthepredictionsetsize,e.g.
thelogfunction. Intuitively,L encouragessmall(efficient)predictionsets,whereasL
size coverage
penalizes deviations from the target coverage of 1−α. Naturally, there is a trade-off between
thesetwoobjectives, inefficiencyandcoverage, sobothlosstermsareoptimizedtogetherwitha
hyperparameterλgoverningtheinfluenceofeachterm:
L(f)=L (f)+λL (f). (29)
size coverage
Importantly, Bellotti (2021) argues the choice of the threshold qˆin (28) is immaterial since the
modelcanlearntoshiftitslogitvaluesaccordinglytomatchtheconstraintsinL (f). Wecan
coverage
thendirectlyoptimize(29)viastochasticgradientdescentmethodsduringtrainingsinceitisfully
differentiablewithrespecttothemodelparameters.
Stutzetal.(2022)buildontheworkofBellotti(2021)bynoticingthatthecalibrationstepisan
importantcomponentinconformalpredictionthatshouldalsobeaccountedforduringtraining. To
thatend,theyproposetospliteachtrainingbatchBintwo: theB halfusedforcalibration,and
cal
theB usedfortesting. Now,insteadofusinganarbitrarythresholdqˆ,wecomputeitusingthe
test
quantileofB ,orconcretely
cal
(cid:18) (cid:19)
Quantile(α;B )−s(x,y)
Cˆ (x,y):=σ cal .
f T
With this modification, we no longer need to enforce valid coverage via L (f) and can
coverage
optimizeforlowinefficiencydirectlybyminimizingL (f)onB . Inthatcase,however,we
size test
27onlygetalearningsignalfromB ,sincethequantileoperationappliedtoB isnon-differentiable.
test cal
Stutz et al. (2022) bypass that limitation via differentiable sorting operators Cuturi et al. (2019);
Blondel et al. (2020); Petersen et al. (2022), in particular via a version of differentiable sorting
networks. Inourexperiments,weconsideredbothfastsortBlondeletal.(2020)andthemonotonic
differentiablesortingnetworksofPetersenetal.(2022)butfinallychosethelattersincetheyproved
morestableandprovidedrichergradientsignals.
Thisversionoftheirapproach,whichwerefertoasConfTr,onlyoptimizesthesizeloss,butStutz
etal.(2022)alsoproposeanothervariantwhichincludesaclassificationlosstermasfollows
L (Cˆ (x),y)= (cid:88) L (cid:104)(cid:16) 1−Cˆ (x,y)(cid:17) δ[yˆ=y]+Cˆ (x,y)δ[yˆ̸=y](cid:105) ,
class f y,yˆ f f
yˆ∈Y
where δ is the indicator function, and L is a user-defined square matrix of size |Y|2 with L
y,yˆ
capturingsomesimilaritynotionbetweenyandyˆ. Inourexperiments,aswellasmostexperimentsin
theoriginalpaperStutzetal.(2022),nopriorinformationabouttheclassificationproblemisassumed,
in which case L is taken to be the identity matrix of size |Y|. Therefore, we have two variants
of conformal training as proposed in Stutz et al. (2022): ConfTr that optimizes only L , and
size
ConfTr thatjointlyoptimizesL andL ,bothofwhichareincludedinourexperiments.
class size class
OurownapproachtoconformaltrainingfollowsthesamerecipefromStutzetal.(2022),i.e.,we
alsosimulateasplitconformalpredictionstepduringtrainingbysplittingeachtrainingbatchinto
twoandusingdifferentiablesortingoperators. Thekeydifferenceisinhowwedefinethetraining
objectives,whichwederivefromfirstprinciplesandstandardinformationtheoryinequalities. Not
onlydoourupperbounds,DPI(4),MBFano(5)andFano(6),outperformtheConfTrobjectivesin
anumbercases,buttheyalsodoawaywithafewhyperparameters. Namely,thefunctionginthe
L ,andhyperparameterscontrollingtherelativeimportanceofL andL .
size size class
E.1 DerivingConformalTrainingfromFano’sbound
ThroughProposition3.2,wecanconnectFano’sboundforlistdecodingtothesizelossfromStutz
etal.(2022),proposedforconformaltraining. Morespecifically,byassumingauniformdistribution
forQwecanshowthat
H(Y|X)≤h (α)+αE [log(|Y|−|C(x)|)]+(1−α )E [log|C(x)|]
b E=0 n E=1
≤h (α)+αlog|Y|+(1−α )E [log|C(x)|]
b n E=1
≤h (α)+αlog|Y|+(1−α )logE [|C(x)|]
b n E=1
≤h (α)+αlog|Y|−(1−α )log(1−α)+(1−α )logE[|C(x)|].
b n n
Note that in the first line we have the simple Fano bound, whereas in the last one we have the
ConfTrobjective,namelylogE[|C(x)|],multipliedby1−α plusaconstantthatdependsonlyon
n
α.Therefore,wegroundConfTrasminimizingalooserupperboundtothetrueconditionalentropy
ofthedatathanthesimpleFanoboundweprovideinCorollary3.1. Moreover, thesimpleFano
boundcanbefurtherimprovedwithanappropriatechoiceofQ,forinstanceasgivenbythemodelf,
inthemodel-basedFanoboundofProposition3.2.
28F Experiments
Inthissection,wepresentfurtherexperimentalresultsforconformaltraininginthecentralizedand
federatedsetting. Westartbydefiningthesplitsandarchitecturesusedforeachdataset,whichare
listedinTable5. Inmostaspects,wefollowtheexperimentaldesignofStutzetal.(2022),which
serveasthemainbaselineagainstwhichwecompareourmodels. Allexperimentalresultswere
obtainedthroughourownimplementationinPython3andPytorchPaszkeetal.(2017),whichwill
bemadepubliclyavailableuponacceptanceforthesakeofreproducibility.
Table5: Experimentalsettingsforeachdataset,with|D |,|D |and|D |thesizesoftrain,
train cal test
calibrationandtestsplits,respectively.
Dataset |D | |D | |D | Epochs Architecture
train cal test
MNISTLeCunetal.(1998) 55K 5K 10K 50 1-layerMLP
Fashion-MNISTXiaoetal.(2017) 55K 5K 10K 150 2-layerMLP
EMNISTCohenetal.(2017) 628K 70K 116K 75 2-layerMLP
CIFAR10Krizhevskyetal.(2009) 45K 5K 10K 150 ResNet-34
CIFAR100Krizhevskyetal.(2009) 45K 5K 10K 150 ResNet-50
Regardingthearchitectures,wealsocloselyfollowtheexperimentalsetupinStutzetal.(2022). For
MNISTwehaveasimplelinearmodel,whereasforFashion-MNISTandEMNISTweuse2-layer
MLPswith64and128hiddenunitsforfirstandsecondlayers,respectively. FortheCIFARdata
sets,weusethedefaultResNetimplementationsfromtorchvisionmaintainers&contributors(2016),
but changing the first convolution to have a kernel size of 3 and unitary stride and padding. We
usePytorch’sdefaultweightinitializationstrategyforallarchitectures. Foralldatasets,weusea
regularSGDoptimizerwithmomentum0.9andNesterovgradients,accompaniedbyastepscheduler
multiplyingtheinitiallearningrateby0.1after2/5,3/5and4/5ofthetotalnumberofepochs. We
onlyusedataaugmentationsontheCIFARdatasets,anddifferentlyfromStutzetal.(2022),weonly
applyrandomflippingandcroppingforbothCIFAR10andCIFAR100.
F.1 CentralizedSetting
WefollowedtheexperimentalprocedureofStutzetal.(2022),andforeachdatasetandeachmethod,
weranagridsearchoverthefollowinghyperparametersusingraytuneMoritzetal.(2018):
• Batchsizewithpossiblevaluesin{100,500,1000}.
• Learningratewithpossiblevaluesin{0.05,0.01,0.005}.
• Temperature used in relaxing the construction of prediction sets at training time. We
consideredtemperaturevaluesin{0.01,0.1,0.5,1.0}.
• Steepnessofthedifferentiablesortingalgorithm(monotonicsortingnetworkswithCauchy
distributionPetersenetal.(2022)),whichregulatesthesmoothnessofthesortingoperator;
thehigherthesteepnessvalue,thecloserthedifferentiablesortingoperatoristostandard
sorting. Weconsideredsteepnessvaluesin{1,10,100}.
InTables6,7,8,9and10,wereportthebesthyperparametersfoundforeachdatasetandmethod
as well as the average prediction set size for threshold CP Sadinle et al. (2019) computed in the
probabilitydomain(THR)andAPSRomanoetal.(2020),aswellasthetestaccuracy. Importantly,
similarlytoStutzetal.(2022),inallcasesweonlytrainthemodelstooptimizethresholdCPwith
log-probabilities. WeconfirmtheobservationinStutzetal.(2022)thatothermethods,andnotably
APS,areunstableduringtraining,probablybecauseitforcesustooperateintheprobabilitydomain,
asopposedtothemoreoptimization-friendlylogitsorlog-probabilities. Nevertheless,westillselect
hyperparametersaccordingtothebestperformancewithrespecttoeachCPmethod,andthatiswhy
wehavedifferentoptimalhyperparametersforTHRandAPSforeachdatasetandeachconformal
trainingobjective. WenoteConfTrandConfTr requireextrahyperparameterslikethetargetsize
class
andweightsattributedtoeachlossterm(seeAppendixE).Forthosehyperparameters,weusethe
bestvaluesforeachdatasetasreportedinStutzetal.(2022).
Asdescribedinthemainpaper,weusethedefaulttrainandtestsplitsofeachdatasetbuttransfer
10%ofthetrainingdatatothetestdataset. Wetraintheclassifiersonlyontheremaining90%ofthe
29trainingdataand,attesttime,runSCPwith10differentcalibration/testsplitsbyrandomlysplitting
theenlargedtestdataset. Allresultsreportedinthepaperaregivenbytheaverage(±onestandard
deviation)computedacrossthese10randomsplits. Crucially,toavoidoverfittingtothetestdata,the
gridsearchwasdonesolelyonthe90%ofthetrainingdatanotusedfortesting.
Table6: HyperparameterSearchMNISTMLP
Bound Optmizedfor batchsize lr temperature steepness THR APS TestAcc.
THR 100 0.01 - - 2.15 2.40 0.93
CE ±0.21 ±0.13
APS 500 0.05 - - 2.15 2.41 0.93
±0.21 ±0.12
THR 100 0.005 0.1 100 2.08 2.11 0.91
Fano ±0.15 ±0.13
APS 100 0.01 0.5 100 2.05 2.08 0.91
±0.14 ±0.13
THR 1000 0.005 0.1 100 2.15 2.83 0.91
MBFano ±0.11 ±0.11
APS 100 0.005 0.1 10 2.31 2.37 0.92
±0.22 ±0.24
THR 1000 0.005 0.01 100 2.18 2.81 0.92
DPI ±0.13 ±0.12
APS 500 0.05 0.1 100 2.21 2.52 0.93
±0.19 ±0.12
THR 500 0.05 0.1 100 2.10 9.79 0.90
ConfTr ±0.16 ±0.01
APS 500 0.01 1.0 100 2.07 2.10 0.90
±0.15 ±0.16
THR 500 0.005 1.0 10 2.05 2.10 0.90
ConfTr-class ±0.15 ±0.13
APS 500 0.01 0.1 100 2.08 2.08 0.90
±0.11 ±0.14
Table7: HyperparameterSearchFashion-MNISTMLP
Bound Optmizedfor batchsize lr temperature steepness THR APS TestAcc.
THR 1000 0.005 - - 2.18 2.48 0.87
CE ±0.15 ±0.10
APS 100 0.05 - - 2.04 2.24 0.89
±0.16 ±0.21
THR 100 0.05 0.5 100.0 1.73 2.11 0.88
Fano ±0.03 ±0.08
APS 100 0.01 1.0 100.0 1.74 1.87 0.88
±0.05 ±0.06
THR 100 0.05 0.01 10.0 1.75 2.15 0.90
DPI ±0.08 ±0.07
APS 100 0.05 0.01 100.0 1.68 2.06 0.89
±0.06 ±0.08
THR 100 0.05 0.5 10.0 1.77 2.64 0.89
MBFano ±0.06 ±0.12
APS 100 0.05 1.0 100.0 1.77 2.25 0.90
±0.09 ±0.12
THR 100 0.01 1.0 1.0 1.69 1.87 0.89
ConfTr ±0.06 ±0.08
APS 100 0.01 1.0 1.0 1.69 1.87 0.89
±0.06 ±0.08
THR 100 0.01 1.0 10.0 1.73 9.07 0.88
ConfTr-class ±0.06 ±2.37
APS 500 0.005 1.0 100.0 1.71 1.73 0.88
±0.03 ±0.05
Table8: HyperparameterSearchEMNISTMLP
Bound Optmizedfor batchsize lr temperature steepness THR APS TestAcc.
THR 100 0.01 - - 2.01 3.25 0.86
CE ±0.06 ±0.15
APS 100 0.005 - - 2.00 3.27 0.86
±0.07 ±0.16
THR 100 0.01 0.1 100 2.02 9.69 0.84
Fano ±0.08 ±0.57
APS 100 0.01 1.0 100 1.99 2.68 0.82
±0.06 ±0.10
THR 100 0.05 0.5 100 1.98 3.85 0.86
DPI ±0.06 ±0.21
APS 100 0.05 0.1 100 1.99 4.13 0.86
±0.06 ±0.25
THR 100 0.01 1.0 100 2.86 3.67 0.86
MBFano ±0.22 ±0.15
APS 100 0.01 0.1 1 2.86 3.67 0.86
±0.22 ±0.15
THR 100 0.01 0.1 100 1.95 5.04 0.85
ConfTr ±0.05 ±0.26
APS 100 0.005 1.0 100 1.96 2.32 0.83
±0.06 ±0.06
THR 100 0.005 0.1 100 1.94 5.51 0.86
ConfTr-class ±0.05 ±0.41
APS 100 0.05 1.0 100 1.96 2.29 0.84
±0.05 ±0.06
30Table9: HyperparameterSearchCIFAR10
Bound Optmizedfor batchsize lr temperature steepness THR APS TestAcc.
THR 100 0.05 - - 1.81 2.34 0.93
CE ±0.20 ±0.36
APS 100 0.05 - - 1.83 2.41 0.93
±0.10 ±0.25
THR 100 0.01 1.0 1 2.00 2.28 0.89
Fano ±0.10 ±0.08
APS 100 0.01 1.0 1 2.02 2.35 0.89
±0.06 ±0.10
THR 100 0.05 0.5 100 1.70 2.35 0.92
MBFano ±0.04 ±0.09
APS 100 0.01 1.0 10 1.74 2.01 0.92
±0.07 ±0.08
THR 100 0.05 0.01 100 1.71 1.93 0.92
DPI ±0.07 ±0.12
APS 100 0.005 0.01 10 1.87 1.99 0.90
±0.12 ±0.10
THR 100 0.05 0.5 10 9.77 9.98 0.08
ConfTr ±0.02 ±0.05
APS 1000 0.005 0.1 1 10.00 9.98 0.1
±0.00 ±0.00
THR 100 0.01 0.5 10 2.29 2.27 0.85
ConfTr-class ±0.07 ±0.06
APS 100 0.01 0.5 10 2.14 2.24 0.86
±0.09 ±0.09
Table10: HyperparameterSearchCIFAR100
Bound Optmizedfor batchsize lr temperature steepness THR APS TestAcc.
CE THR 100 0.05 - - 21.11 ±1.16 28.11 ±2.38 0.71
APS 100 0.05 - - 21.11 ±1.16 28.11 ±2.38 0.71
Fano THR 100 0.005 0.5 1 36.60 ±2.63 44.39 ±1.89 0.50
APS 500 0.05 1 1 34.51 ±2.33 37.01 ±2.07 0.55
MBFano THR 100 0.05 0.5 100 18.28 ±1.63 22.71 ±0.93 0.68
APS 100 0.05 1 10 16.60 ±1.04 21.27 ±1.38 0.68
DPI THR 100 0.05 1.0 1 16.03 ±1.02 19.07 ±0.83 0.71
APS 100 0.05 1 10 15.54 ±1.20 16.86 ±1.19 0.69
ConfTr THR 100 0.005 1.0 1 29.84 ±2.37 30.41 ±2.69 0.52
APS 100 0.01 0.5 1 33.76 ±1.97 42.44 ±2.05 0.54
ConfTr-class THR 100 0.01 1.0 1 31.25 ±1.41 49.62 ±14.22 0.43
APS 100 0.01 1 10 32.57 ±1.89 32.65 ±1.88 0.37
F.2 FederatedSetting
Inthefederatedlearningsetting,werunconformaltrainingexactlyinthesamefashion,butincluding
theadditionalQ termin(10)togettheproperdistributedboundthatcanbeoptimizedlocally
Z|X
andindependentlybyeachdevice. WeoptimizetheconformaltrainingobjectivewithSGDforone
epochineachdevice,andthencommunicatetheresulting“personalized”modeltotheserver,which
aggregates the model parameters of each device via federated averaging McMahan et al. (2017).
Afteraggregation,theglobalmodelthuscomputediscommunicatedbytotheclientandtheprocess
restarts. Wedo5KsuchcommunicationroundsforEMNIST,and10KforCIFAR10andCIFAR100.
As described in the main text, we divide the data among 100, 500, and 1K clients for CIFAR10,
CIFAR100andEMNIST,respectively.Weassigndatapointstodevicesimposingadistribution-based
labelimbalanceLietal.(2022),wherewesampleamarginallabeldistributionforeachdevicefrom
aDirichletdistribution. WeuseDir(1.0)forallexperiments,butalsostudytheeffectofDir(0.5)
andDir(0.1)onCIFAR10,asshowninTables11and12. Theremainderoftheexperimentalsetup
is similar to that used for the centralized setting, with the same architectures, data augmentation
strategies,andoptimalhyperparametersreportedinTables6,7,8,9. InTables11and12,aswellas
13and14,wereportinefficiencyresultsfortwodifferentsettings.
GLO WerunSCPwiththefinalglobalmodelassumingcalibrationandtestdatasetsattheserver,
orequivalentlythattheclientssharetheirscoreswiththeserver. Thisreflectsthebestinefficiency
results we can hope for with the global model, as in practice we might need to resort to privacy
preserving methods that are likely to hurt performance. Notably, we considered the quantile of
quantiles approach of Humbert et al. (2023) as well as the simpler alternative proposed in Lu &
Kalpathy-Cramer (2021), but in both cases we got varying degrees of coverage due to the data
heterogeneityamongdevicesintroducedinthedistribution-basedlabelimbalancesetup. Addressing
theseshortcomingsisapromisingavenueforfutureresearchforconformalpredictioninthefederated
setting.
31PER Afterlearningtheglobalmodel,wefinetuneitonthelocaltrainingdataofeachdeviceto
obtainapersonalizedmodel. WethenrunSCPindividuallyforeachdevicewithlocalcalibrationand
testdatasetsandreporttheaveragepredictionsetsacrossallclients. Importantly,sinceeachclient
hasaccesstoonlyasmallnumberofdatapoints,wedonotalwaysachievevalidcoverageinthe
personalizedsetting. Moreprecisely,allpersonalizedmodelsonCIFAR10andEMNISTachieved
marginalcoverageofaround97%,whileforCIFAR100thatvaluedroppedto90%. Nonetheless,all
methodsgetsimilarcoveragessotheresultsremaincomparable. Interestingly,evenwithpersonalized
models,whichimplicitlyalreadyestimateQ ,updatingthepersonalizedmodelasin(8)with
Y|X,Z
theglobalheadQ ,stillresultsinnon-negligibleimprovementsinperformance,asshownin
Z|X,Y
Tables11and12,aswellas13and14.
Table11: InefficiencyofConfTrPredictionwiththresholding(THR)forCIFAR10withdifferent
levelsofdataheterogeneityamongclientsbothglobal(GLO)andpersonalized(PER)modelsinthe
federatedsettingforatargetα=0.01.
Method Dirichlet(1.0) Dirichlet(0.5) Dirichlet(0.1)
GLO GLO+side PER PER+side GLO GLO+side PER PER+side GLO GLO+side PER PER+side
CE 2.73±0.04 2.30±0.06 2.07±0.51 1.82±0.39 2.81±0.11 2.41±0.08 2.03±0.49 1.79±0.38 2.73±0.13 2.32±0.06 2.05±0.52 1.82±0.38
ConfTr 10.00±0.00 10.00±0.00 9.84±0.05 9.84±0.05 10.00±0.00 10.00±0.00 9.84±0.05 9.84±0.05 10.00±0.00 10.00±0.00 9.87±0.06 9.87±0.06
ConfTrclass 3.53±0.09 3.39±0.08 2.86±0.38 2.79±0.35 3.54±0.06 3.38±0.07 2.91±0.40 2.84±0.37 3.53±0.05 3.39±0.05 2.84±0.41 2.78±0.38
Fano 2.39±0.07 2.07±0.07 1.84±0.37 1.63±0.30 2.47±0.08 2.10±0.06 1.94±0.43 1.71±0.35 2.46±0.08 2.11±0.06 1.91±0.41 1.68±0.33
MBFano 2.52±0.08 2.04±0.07 1.56±0.29 1.40±0.22 2.66±0.12 2.09±0.06 1.57±0.31 1.40±0.23 2.55±0.11 2.06±0.10 1.98±0.45 1.64±0.32
DPI 2.76±0.07 2.28±0.03 1.64±0.36 1.49±0.28 2.50±0.07 2.11±0.49 1.91±0.42 1.64±0.31 2.51±0.07 2.07±0.05 2.04±0.41 1.70±0.30
Table 12: Inefficiency of ConfTr Prediction with APS for CIFAR10 with different levels of data
heterogeneity among clients both global (GLO) and personalized (PER) models in the federated
settingforatargetα=0.01.
Method Dirichlet(1.0) Dirichlet(0.5) Dirichlet(0.1)
GLO GLO+side PER PER+side GLO GLO+side PER PER+side GLO GLO+side PER PER+side
CE 2.83±0.07 2.43±0.06 2.22±0.06 1.94±0.41 2.70±0.79 2.33±0.42 2.13±0.47 1.87±0.36 2.81±0.14 2.37±0.08 2.15±0.05 1.89±0.39
ConfTr 10±0.00 10±0.00 9.87±0.14 9.87±0.14 10±0.00 10±0.00 9.88±0.09 9.88±0.09 10±0.00 10±0.00 9.87±0.14 9.87±0.14
ConfTrclass 10±0.00 10±0.00 9.92±0.03 9.92±0.03 10±0.00 10±0.00 9.94±0.07 9.95±0.05 10±0.00 10±0.00 9.92±0.03 9.92±0.03
Fano 2.73±0.07 2.39±0.06 2.17±0.45 1.94±0.36 2.61±0.07 2.30±0.05 2.15±0.04 1.92±0.34 2.67±0.08 2.35±0.07 2.13±0.42 1.92±0.34
MBFano 2.79±0.13 2.33±0.05 2.28±0.48 1.97±0.35 2.71±0.09 2.31±0.07 2.27±0.48 1.96±0.35 2.87±0.12 2.40±0.06 2.32±0.51 1.99±0.39
DPI 2.68±0.15 2.22±0.09 2.11±0.51 1.84±0.36 2.54±0.10 2.18±0.04 2.10±0.45 1.84±0.33 2.65±0.10 2.25±0.08 2.13±0.44 1.89±0.34
Table13: InefficiencyofConformalPredictionwiththresholding(THR)foreachdatasetwithboth
global(GLO)andpersonalized(PER)modelsinthefederatedsettingforatargetα=0.01.
Method EMNIST CIFAR10 CIFAR100
GLO GLO+side PER PER+side GLO GLO+side PER PER+side GLO GLO+side PER PER+side
CE 2.91±0.02 2.46±0.02 2.46±0.65 2.08±0.51 2.73±0.04 2.30±0.06 2.07±0.51 1.82±0.39 55.41±1.09 52.31±1.03 20.84±8.02 18.85±7.53
ConfTr 4.60±0.05 3.30±0.02 3.95±1.05 3.05±0.78 10.00±0.00 10.00±0.00 9.84±0.05 9.84±0.05 45.60±1.30 41.18±1.16 16.23±6.27 14.01±5.66
ConfTr+class 2.88±0.02 1.98±0.02 1.69±0.31 1.55±0.24 3.53±0.09 3.39±0.08 2.86±0.38 2.79±0.35 58.53±1.40 56.03±1.29 25.22±7.40 23.87±7.02
Fano 2.63±0.02 2.37±0.02 2.14±0.46 1.98±0.40 2.39±0.07 2.07±0.07 1.84±0.37 1.63±0.30 47.91±1.20 41.19±1.02 18.42±6.95 14.55±5.83
MBFano 2.84±0.04 2.25±0.03 2.47±0.64 1.96±0.45 2.52±0.08 2.04±0.07 1.56±0.29 1.40±0.22 52.94±1.40 46.97±1.30 20.36±7.77 16.41±6.79
DPI 2.60±0.02 2.23±0.01 2.31±0.58 1.97±0.46 2.76±0.07 2.28±0.03 1.64±0.36 1.49±0.28 52.36±0.95 48.64±0.70 20.13±7.84 17.73±7.23
Table14: InefficiencyofConformalPredictionwithAPSforeachdatasetwithbothglobal(GLO)
andpersonalized(PER)modelsinthefederatedsettingforatargetα=0.01.
Method EMNIST CIFAR10 CIFAR100
GLO GLO+side PER PER+side GLO GLO+side PER PER+side GLO GLO+side PER PER+side
CE 3.69±0.03 3.14±0.04 1.42±0.25 1.40±0.24 2.83±0.07 2.43±0.06 2.22±0.56 1.94±0.41 64.73±0.34 62.67±3.68 22.39±8.86 20.22±8.15
ConfTr 6.14±0.04 5.25±0.04 4.79±1.23 3.06±0.63 10.00±0.00 10.00±0.00 9.87±0.14 9.87±0.14 55.18±2.10 47.58±1.48 23.43±8.20 19.12±7.14
ConfTrclass 2.65±0.02 2.42±0.02 3.03±1.57 1.51±0.31 10.00±0.00 10.00±0.00 9.92±0.03 9.92±0.03 99.92±0.02 99.91±0.01 99.97±0.22 99.68±0.24
Fano 3.12±0.04 2.72±0.03 1.17±0.10 1.15±0.01 2.73±0.07 2.39±0.06 2.17±0.45 1.94±0.36 46.95±0.67 42.75±0.91 19.19±7.24 16.9±6.67
MBFano 4.75±0.03 2.43±0.01 2.04±0.51 2.31±0.42 2.79±0.13 2.33±0.05 2.28±0.48 1.97±0.35 50.72±1.77 45.72±1.38 21.12±7.49 18.12±6.84
DPI 2.98±0.03 2.58±0.02 2.73±0.65 1.29±0.16 2.68±0.15 2.22±0.09 2.11±0.51 1.84±0.36 51.29±1.07 47.18±1.27 20.76±7.51 18.15±6.82
32F.3 Evaluatingthelowerboundsonthesetsize
Inthissectionwewillevaluateourtwolowerboundsontheexpected[log|C(X)|]+. Thefirstone
canbeobtainedbyrearrangingthesimpleFanobound(c.f. PropositionB.7)whereasthesecond
onecanbeobtainedfromthemodel-basedFanoentropyupperbounds(c.f. TheoremD.2). The
mainchallengeinevaluatingtheseboundsisinthatwerequirethegroundtruthentropyH(Y|X)
orH(Y|Yˆ)whichingeneralarenotavailable. Toproceed, weadopttheversionsofthebounds
thatdependonH(Y|Yˆ). WhenYˆ isdiscrete,wecangetatractablelowerboundtoH(Y|Yˆ)viaa
maximumlikelihoodestimateoftheentropyPaninski(2003). Morespecifically,wehavethat
H(Y|Yˆ)=H(Y,Yˆ)−H(Yˆ)≥H (Y,Yˆ)−log|Yˆ|,
MLE
where|Yˆ|isthecardinalityofYˆ.
(a)CIFAR10 (b)CIFAR100
Figure2: Expected[log|C(X)|]+asafunctionofα.
Basedonthis,weevaluateoursetsizeboundsonResNetmodelstrainedwithCEonCIFAR10and
CIFAR100. AsthelogitsYˆ usedforCParenotdiscrete,weperformK-meansclusteringonthem
andconstructavectorquantizedYˆ byassigningtoeachlogitvectoritsclosestclustercentroid. We
vq
thenuseYˆ toperformCPwiththresholdingandalsouseYˆ toobtainalowerboundonH(Y|Yˆ )
vq vq vq
viaamaximumlikelihoodestimateforH(Y,Yˆ ). Forthismaximumlikelihoodestimatewealso
vq
performtheMiller-MadowbiascorrectionPaninski(2003).
ForCIFAR10weclusterthelogitsinto32clusterswhereasforCIFAR100weuse256clusters. The
centroidsarelearnedonacalibrationsetof5klogits. Forthemodel-basedFanobound,asitneedsto
computetermsthatdependonthemodelprobabilitiesandonwhetherthelabelwascorrectlycovered
byCPornot,wefurthersplitthecalibrationsetintotwoequal-sizedchunks;thefirstisusedtofind
thequantileforthresholdingandthesecondoneisusedtoevaluatethetermsofthebound. The
obtainedlowerboundson[log|C(X)|]+ forvariousα’scanbeseenatFigure2. Wealsoinclude
an“empiricalestimate”whichisobtainedbycomputingthequantilewiththequantizedcalibration
logitsandthenmeasuringtheaverage[log|C(X)|]+onthetestsetviathresholdingonthequantized
testlogits. Wecanseethatthemodel-basedFanoprovidesrelativelytightestimatesforsmallvalues
ofalpha.
33