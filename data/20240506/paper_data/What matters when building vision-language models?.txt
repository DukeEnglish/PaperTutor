What matters when building vision-language models?
HugoLaurençon∗,1,2 LéoTronchon∗,1 MatthieuCord2 VictorSanh1
∗Theorderwaschosenrandomly.
1HuggingFace2SorbonneUniversité
Abstract
Thegrowinginterestinvision-languagemodels(VLMs)hasbeendrivenbyim-
provementsinlargelanguagemodelsandvisiontransformers. Despitetheabun-
danceofliteratureonthissubject,weobservethatcriticaldecisionsregardingthe
designofVLMsareoftennotjustified. Wearguethattheseunsupporteddecisions
impedeprogressinthefieldbymakingitdifficulttoidentifywhichchoicesim-
provemodelperformance. Toaddressthisissue,weconductextensiveexperiments
aroundpre-trainedmodels,architecturechoice,data,andtrainingmethods. Our
consolidationoffindingsincludesthedevelopmentofIdefics2,anefficientfounda-
tionalVLMof8billionparameters. Idefics2achievesstate-of-the-artperformance
withinitssizecategoryacrossvariousmultimodalbenchmarks,andisoftenonpar
withmodelsfourtimesitssize. Wereleasethemodel(base,instructed,andchat)
alongwiththedatasetscreatedforitstraining.
Prompt Idefics2 output
For 2024, the interest expense is twice what it Yes, I can help you calculate that. First, let's find out
was in 2014, and the long-term debt is 10% the interest expense for 2024:
higher than its 2015 level. Can you calculate the Interest Expense (2024) = 2 * Interest Expense (2014)
combined total of the interest and long-term debt = 2 * 630
for 2024? = 1260
Next, let's find the long-term debt for 2024:
Long-Term Debt (2024) = Long-Term Debt (2015) +
10% of Long-Term Debt (2015)
= 19,260 + 0.1 * 19,260
= 19,260 + 1,926
= 21,186
Now, we can add these two amounts together to get
the combined total of the interest and long-term debt
for 2024:
Combined Total (2024) = Interest Expense (2024) +
Long-Term Debt (2024)
= 1260 + 21,186
= 22,446
So, the combined total of the interest and long-term
debt for 2024 is $22,446.
Figure1: Idefics2-chattyanalyzesthetabletocomputeandanswerthequery.
Preprint.Underreview.
4202
yaM
3
]VC.sc[
1v64220.5042:viXra1 Introduction
Vision-languagemodels(VLMs)thattakeimagesandtextsasinputsandoutputtexts,areuseful
formanytasks,likeretrievinginformationinascannedPDF(Huetal.,2024),explainingchartsor
diagrams(Carbuneetal.,2024),transcribingthetextinanimage(Blecheretal.,2023),counting
objectsinapicture(Goyaletal.,2017)orturningscreenshotsofwebpagesintocode(Laurençon
etal.,2024). Thedevelopmentofpowerfulopenlargelanguagemodels(Touvronetal.,2023;Jiang
etal.,2023;Google,2024b)andimageencoders(Zhaietal.,2023;Sunetal.,2023;Radfordetal.,
2021)enablesresearcherstobuildupontheseunimodalpre-trainedmodelstocreateadvancedVLMs
that solve these problems with increasing accuracy (Dai et al., 2023; Liu et al., 2023; Bai et al.,
2023;Linetal.,2024,2023;Lietal.,2024;Wangetal.,2024). Despitetheprogressinthefield,the
literaturerevealsmanydisparatedesignchoiceswhichareoftennotjustifiedexperimentally,orvery
briefly.
Thissituationmakesitchallengingtodistinguishwhichdecisionstrulyaccountformodelperfor-
mance,therebymakingitdifficultforthecommunitytomakemeaningfulandgroundedprogress.
Forinstance,(Alayracetal.,2022;Laurençonetal.,2023)useinterleavedTransformer-basedcross-
attentionstofusetheimageinformationintothelanguagemodel,while(Lietal.,2023;Liuetal.,
2023)concatenatethesequenceofimagehiddenstateswiththesequenceoftextembeddings,and
feedtheconcatenatedsequencetothelanguagemodel. Toourknowledge,thischoicehasnotbeen
properlyablated, andtrade-offsintermsofcompute, dataefficiencyandperformancearepoorly
understood. Inthiswork,weaimtobringexperimentalclaritytosomeofthesecoredesignchoices
andposethequestion: Whatmatterswhenbuildingvision-languagemodels?
Weidentifytwoareaswherevariousworksadoptdifferentdesignchoices: (a)modelarchitecture,
andin particular, connectormodules thatfusethe visionand textmodalities andtheir impacton
inferenceefficiency,(b)multimodaltrainingprocedureanditsimpactontrainingstability. Foreach
oftheseareas,werigorouslycomparedifferentdesignchoicesinacontrolledenvironmentandextract
experimentalfindings.Notably,wefindthat(a)theprogressofvision-languagemodelsisinlargepart
drivenbytheprogressofpre-trainedunimodalbackbones,(b)themorerecentfullyautoregressive
architectureoutperformsthecross-attentionarchitecture,althoughitrequiresmodificationstothe
optimizationproceduretoensureastabletraining,(c)adaptationofthepre-trainedvisionbackbone
andthemodulesconnectingthetextandvisionmodalitiesallowformoreefficiencyatinference
timeononeside,andhandlingimagesintheiroriginalratioandsizewithoutharmingdownstream
performanceontheotherside,and(d)modificationstotheimageprocessingenablestradinginference
costfordownstreamperformance.
Ourresultsarecomplementarywiththosepresentedin(Karamchetietal.,2024;McKinzieetal.,
2024; Lin et al., 2024) which derive insights about multi-stage training, selective unfreezing of
the pre-trained backbones, data repetition, and impact of training mixture on zero and few-shot
performance. We specifically delve into unexplored aspects such as model architecture, training
methods,stability,andefficiencyimprovementsatinference.
Learning from these insights, we train Idefics2, a foundational VLM with 8 billion parameters.
Idefics2achievesstate-of-the-artperformanceinitssizecategoryonvariousbenchmarkswhilebeing
moreefficientatinference,forboththebaseandthefine-tunedversion. Itisonparwithstate-of-
the-artmodels4timeslargeronsomevision-languagebenchmarksandmatchestheperformanceof
Gemini1.5Proonsomechallengingbenchmarks. Wereleasethebase,instructed,andchatversions
ofIdefics21asresourcesfortheVLMcommunityalongwiththedatacreatedtotrainthemodel.
2 Terminology
Wefirstestablishsharedterminologyfordiscussingthedifferentdesignchoices. TrainingVLMs
typicallyrequiresgluingtogetherapre-trainedvisionbackboneandapre-trainedlanguagebackbone
by initializing new parameters to connect the two modalities. Training these new parameters is
doneduringthepre-trainingphase. Thisstagecommonlyleveragesalargemultimodaldatasetsuch
as image-caption pairs. We note that even though it is most common to start from two separate
unimodalpre-trainedbackbones,theparametersofthesetwobackbonescanbeoptionallysharedand
initializedfromscratchasdonein(Bavishietal.,2023). Asinthelargelanguagemodelsliterature,
1https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe
2thepre-trainingstageisfollowedbyaninstructionfine-tuningstage,inwhichthemodellearnsfrom
task-orientedsamples.
Recent works explore two main choices to combine the visual inputs and the text inputs. In the
cross-attention architecture (Alayrac et al., 2022; Laurençon et al., 2023; Awadalla et al., 2023),
theimagesencodedthroughthevisionbackboneareinjectedatdifferentlayerswithinthelanguage
modelbyinterleavingcross-attentionblocksinwhichthetextcross-attendstotheimagehiddenstates.
Incontrast,inthefullyautoregressivearchitecture(Kohetal.,2023;Driessetal.,2023;Liuetal.,
2023),theoutputofthevisionencoderisdirectlyconcatenatedtothesequenceoftextembeddings,
andtheentiresequenceispassedasinputtothelanguagemodel. Theinputsequenceofthelanguage
modelisthustheconcatenationofvisualtokensandtexttokens. Thesequenceofvisualtokenscanbe
optionallypooledintoashortersequence,providingmorecomputeefficiency. Werefertothelayers
thatmapsthevisionhiddenspacetothetexthiddenspaceasmodalityprojectionlayers. Figure2
highlightsthefully-autoregressivearchitectureweultimatelyuseforIdefics2.
Up to Hidden states
64
980x980
Modality
Vision projection 64
encoder +
Pooling
64 LLM
64
T
T
... ...
Figure2:Idefics2fully-a...utoregressivearchitecture:.I..nputimagesareprocessedbytheVisionencoder.
Theresultingvisualfeaturesaremapped(andoptionallypooled)totheLLM inputspacetogetthe
visualtokens(64inourstandardconfiguration). Theyareconcatenated(andpotentiallyinterleaved)
withtheinputsequenceoftextembeddings(greenandredcolumn). Theconcatenatedsequenceis
fedtothelanguagemodel(LLM),whichpredictsthetexttokensoutput.
3 Exploringthedesignspaceofvision-languagemodels
Inthissection, wecomparerecurrentdesignchoicesinthevision-languagemodelliteratureand
highlight findings. Unless specified otherwise, we run the ablations for 6’000 steps and report
the average score of the 4-shot performance on 4 downstream benchmarks measuring different
capabilities: VQAv2(Goyaletal.,2017)forgeneralvisualquestionanswering,TextVQA(Singh
etal.,2019)forOCRabilities,OKVQA(Marinoetal.,2019)forexternalknowledge,andCOCO
(Linetal.,2014)forcaptioning.
3.1 Areallpre-trainedbackbonesequivalentforVLMs?
MostrecentVLMsstartfrompre-trainedunimodalbackbones. Howdoesthechoiceofthebackbones
(visionandtext)influencetheperformanceoftheresultingVLM?
Wefixthesizeofthepretrainedbackbones,thedatausedformulti-
modalpre-training,andthenumberoftrainingupdates. Underthe LMbackbone Avg.score
cross-attentionarchitecture,weobservethatthegreatestimprove- Llama-1-7B 62.5
mentintheperformanceonvision-languagebenchmarkscomes Mistral-7B 67.6
fromchangingthelanguagemodeltoabetterone. Morespecif- Table1:Ablationonthelanguage
ically,replacingLLaMA-1-7B(Touvronetal.,2023)(35.1%on modelbackbone.
MMLU(Hendrycksetal.,2021))byMistral-7B(Jiangetal.,2023)
(60.1%onMMLU)yieldsaboostof5.1(seeTable1). Additionally,switchingthevisionencoder
fromCLIP-ViT-H(Radfordetal.,2021)(78.0%onImageNet(Dengetal.,2009))toSigLIP-SO400M
3
...
...
... ...(Zhaietal.,2023)(83.2%onImageNet)yieldsa3.3increaseinperformanceonthebenchmarks(see
Table2). Thisresultonbettervisionbackbonescorroboratesobservationsfrom(Karamchetietal.,
2024).
WenotethatChenandWang(2022)reportsastrongerincrease
inperformancebyscalingthesizeofthevisionencodercom- VEbackbone Res. Avg.score
paredtoscalingthesizeofthelanguagemodeleventhough CLIP-ViT-H 224 57.4
scalingthevisionencoderleadstoasmallerparametercountin- EVA-CLIP-5B 224 60.2
SigLIP-SO400M 384 60.7
crease. AlthoughEVA-CLIP-5B(Sunetal.,2023)istentimes
biggerinparametercountsthanSigLIP-SO400M(Zhaietal., Table2:Ablationonthevisionencoder
2023),weobtainsimilarperformanceacross4benchmarks, backbone.
suggestingthatEVA-CLIP-5Bcouldbeheavilyunder-trained,
andweacknowledgethattheopenVLMcommunityismissingalargewell-trainedvisionencoder.
Finding1. Forafixednumberofparameters,thequalityofthelanguagemodelbackbonehas
ahigherimpactontheperformanceofthefinalVLMthanthequalityofthevisionbackbone.
3.2 Howdoesthefullyautoregressivearchitecturecomparetothecross-attention
architecture?
Toourknowledge, thereisnopropercomparisonbetweenthefullyautoregressiveandthecross-
attentionarchitecture. Weaimtofillthisgapbyconsideringtheirtrade-offs,namelyperformance,
parametercount,andinferencecost.
Following(Alayracetal.,2022),wefirstcom-
Backbones
pare the two architectures by freezing the Architecture Avg.score
training
unimodal backbones and training only the
Fullyautoreg.noPerceiver Frozen 51.8
newlyinitializedparameters(cross-attentionon
Fullyautoreg. Frozen 60.3
one side, and modality projection along with Cross-attention Frozen 66.7
learned pooling on the other side), while fix- Cross-attention LoRA 67.3
ingtheamountoftrainingdata. Alayracetal. Fullyautoreg. LoRA 69.5
(2022)showsthatthemorefrequentlythecross-
Table3:Ablationforthearchitectureandmethodoftraining.
attention blocks are interleaved with the lan-
guagemodellayers,andthehigherthevision-languageperformance. Assuch,wenotethatunderthis
setup,thecross-attentionarchitecturehas1.3Bmoretrainableparameters(2Btrainableparametersin
total)thanthefullyautoregressivearchitecture. Additionally,atinferencetime,theformeruses10%
moreflopsthanthelatter. Undertheseconditions,weobservethatthecross-attentionarchitecture
performs7pointsbetterinTable3.
Outofthetotalnumberofparameters,approximately15%forthefullyautoregressivearchitectureand
25%forthecross-attentionaretrained.Wehypothesizethatthislowproportionlimitstheexpressivity
ofthetrainingandhindersperformance. Totestthathypothesis,wecomparethetwoarchitecturesby
unfreezingallparameters(newlyinitializedparametersandparametersofthepre-trainedunimodal
backbones). Undertheseconditions,trainingthefullyautoregressivearchitecturewouldyieldloss
divergences,andwewerenotsuccessfulinstabilizingthetrainingevenbyaggressivelylowering
thelearningrateorgraduallyunfreezingvariouscomponents. Toovercomethisstabilitychallenge,
weleverageLow-RankAdaptation(Huetal.,2022)toadaptthepre-trainedparameterswhileusing
standardfullfine-tuningforthenewlyinitializedones.
This setup yields significantly more stable trainings, and more importantly, we observe a 12.9
pointsincreaseunderthefullyautoregressivearchitecture,and0.6pointunderthecross-attention
architecture. While the cross-attention architecture performs better than the fully autoregressive
architecturewithfrozenbackbones,itisworsewhenweadddegreesoflibertyforthepre-trained
backbones. Besides,usingLoRAallowstrainingtheunimodalbackbonesatafractionoftheGPU
memorycostoffullfine-tuning,andLoRAlayerscanbemergedbackintotheoriginallinearlayers
yieldingnoadditionalcostatinference. Wethereforechoosethefullyautoregressivearchitecturein
therestofthiswork.
Itisinterestingtonotethatthisfindingcontradicts(Karamchetietal.,2024)inwhichtheauthors
observedthatunfreezingthepre-trainedvisualbackbonewouldsignificantlydegradetheperformance.
Wehypothesizethatusingparameter-efficientfine-tuningmethodsisakeydifference.
4Finding2. Thecross-attentionarchitectureperformsbetterthanthefullyautoregressiveone
whenunimodalpre-trainedbackbonesarekeptfrozen. However,whentrainingtheunimodal
backbones, the fully autoregressive architecture outperforms the cross-attention one, even
thoughthelatterhasmoreparameters.
Finding3. Unfreezingthepre-trainedbackbonesunderthefullyautoregressivearchitecture
canleadtotrainingdivergences. LeveragingLoRAstilladdsexpressivitytothetrainingand
stabilizesit.
3.3 Wherearetheefficiencygains?
Numberofvisualtokens RecentVLMstypicallyroutetheentiresequenceofthevisionencoder’s
hiddenstatesdirectlyintothemodalityprojectionlayer,whichsubsequentlyinputsintothelanguage
model,withoutnopooling. Thisismotivatedbypreviousworksinwhichaddingapoolingstrategy,
likeaveragepooling,wasfoundtodeterioratetheperformance(Vallaeysetal.,2024). Thisresults
inahighnumberofvisualtokensforeachimagerangingfrom576forDeepSeek-VL(Luetal.,
2024)to2890forSPHINX-2k(Linetal.,2023). Withtheresultingsequencelengths,trainingis
computationally costly, and in-context learning with interleaved images and texts is challenging
becauseitrequiresmodificationstothelanguagemodelstohandleverylargecontextwindows.
Wereducethesequencelengthofeachimage’shiddenstatesbyusingaperceiverresampler(Jaegle
etal.,2021;Alayracetal.,2022;Baietal.,2023)asaformoftrainableTransformer-basedpooling.
Thenumberofqueries(alsoreferredtoaslatents)correspondstothenumberofresultingvisual
tokensafterthepooling. Weobservethatthelearnedpoolingiseffectiveintwoways: itincreasesthe
performanceby8.5pointsonaverageandreducesthenumberofvisualtokensnecessaryforeach
imagefrom729to64(seeTable3).
Incontrastto(Vallaeysetal.,2024;McKinzieetal.,2024)which
findthatthemorevisualtokensthehighertheperformance,we Pooling #vis.tok. Avg.score
observe no gains when using more than 64 visual tokens. We Perceiver 128 71.2
hypothesize that in a hypothetical scenario of infinite training Perceiver 64 71.7
onunlimiteddata,performancemighteventuallyimprove,atthe Table4: Ablationonthepoolingstrat-
costofalongertrainingtime. OthervariationsoverthePerceiver egy.
architecture (Mañas et al., 2023; Darcet et al., 2024; Vallaeys
etal.,2024)resultedindecreasedperformance.
Finding4. Reducingthenumberofvisualtokenswithlearnedpoolingsignificantlyimproves
computeefficiencyattrainingandinferencewhileimprovingperformanceondownstream
tasks.
Preservingtheoriginalaspectratioandimageresolution Visionencoders,suchasSigLIP,are
typicallytrainedonfixed-sizesquareimages. Resizingimagesalterstheiroriginalaspectratio,which
isproblematic,forinstance,fortasksrequiringreadinglongtexts. Furthermore,conditioningthe
trainingonasingleresolutionsizeinherentlyintroduceslimitations: alowresolutionomitscrucial
visualdetails,whileahighresolutionleadstoinefficiencyintrainingandinference. Allowingthe
modeltoencodeimagesatvariousresolutionsallowsuserstodecidehowmuchcomputeisspenton
eachimage.
FollowingLeeetal.(2023);Dehghanietal.(2023),wepass
the image patches to the vision encoder without resizing Images Res. Avg.score
theimageormodifyingitsaspectratio. GiventhatSigLIP Squareimages 768 73.1
wastrainedonfixed-sizelow-resolutionsquareimages,we ARpreserving 378-768 72.1
interpolatethepre-trainedpositionalembeddingstoallowfor
Table5:Ablationontheaspect-ratiopreserv-
ahigherresolutionandtrainthevisionencoderwithLoRA ingstrategy.
parameters to adapt to these modifications.2 Our findings
2SinceSigLIPistrainedwithafixedresolution,thepositionalembeddingscanbeinterpretedbothasabsolute
orrelativepositions.Withtheaspectratioandresolutionpreserving,thesepositionsbecomerelativepositional
embeddings.
5indicatethattheaspectratiopreservingstrategymaintainsperformancelevelsondownstreamtasks
while unlocking computational flexibility during both training and inference (see Table 5). In
particular,nothavingtoresizeimagestothesamehighresolutionallowsforsavingGPUmemory
andhandlingimagesattheresolutiontheyrequire.
Finding5. Adaptingavisionencoderpre-trainedonfixed-sizesquareimagestopreserve
images’originalaspectratioandresolutiondoesnotdegradeperformancewhilespeedingup
trainingandinferenceandreducingmemory.
3.4 Howcanonetradecomputeforperformance?
(Lin et al., 2023; Li et al., 2024; Liu et al., 2024; McKinzie et al., 2024) show that splitting an
imageintosub-imagesallowsboostingthedownstreamperformancewithnochangestothemodel’s
signature. Animageisdecomposedintosub-images(forinstance4equalsub-images),whichare
thenconcatenatedtotheoriginalimagetoformasequenceof5images. Additionally,thesub-images
areresizedtotheoriginalimage’ssize. Thisstrategyhowevercomesatthecostofamuchhigher
numberoftokenstoencodetheimages.
Weadoptthisstrategyduringtheinstructionfine-tuningstage. Eachsingleimagebecomesalist
of5images: 4cropsandtheoriginalimage. Thisway,atinference,themodelisabletodealwith
standaloneimages(64visualtokensperimage),aswellasartificiallyaugmentedimages(320visual
tokensintotalperimage). Wenoticethatthisstrategyisparticularlyusefulforbenchmarkslike
TextVQAandDocVQA,whichrequireasufficientlyhighresolutiontoextractthetextinanimage
(seeTable9).
Moreover,whenweapplyimagespittingtoonly50%ofthetrainingsamples(insteadof100%ofthe
samples),weobservethatitdoesnotimpairtheperformanceincreasethatimagesplittingprovides.
Surprisingly, wefindatevaluationtimethatincreasingtheresolutionofthesub-images(andthe
standaloneimage)providesonlyaminorboostinperformancecomparedtotheimprovementyielded
bysoleimagesplitting: 73.6%whenincreasingtheresolutionofthesub-imagestothemaximum
vs73.0%accuracyonourvalidationsetofTextVQA,andrespectively72.7vs72.9ANLSonthe
validationsetofDocVQA.
Finding6. Splittingimagesintosub-imagesduringtrainingallowtradingcomputeefficiency
formoreperformanceduringinference. Theincreaseinperformanceisparticularlynoticeable
intasksinvolvingreadingtextinanimage.
4 Idefics2-anopenstate-of-the-artvision-languagefoundationmodel
Withtheselearningsinhand,wetrainanopen8Bparametersvision-languagemodel: Idefics2. This
sectiondescribestheconstructionofthemodel,thechoiceofthedataset,thesequenceoftraining
phasesandcomparestheresultingmodelagainstVLMsbaselines.
4.1 Multi-stagepre-training
WestartfromSigLIP-SO400MandMistral-7B-v0.1andpre-trainIdefics2on3typesofdata.
Interleavedimage-textdocuments WeuseOBELICS(Laurençonetal.,2023),anopenweb-scale
datasetofinterleavedimage-textdocumentswith350millionimagesand115billiontexttokens. As
shownbytheauthors,thelongdocumentsofOBELICSallowforpreservingtheperformanceofthe
languagemodelwhilelearningtodealwithanarbitrarynumberofinterleavedimagesandtextsand
longcontext. Additionally,theauthorsshowthatinterleavedimage-textdocumentsarethebiggest
drivingfactorinincreasingtheperformanceonvisualquestionanswering(VQA)tasks,inparticular
inthein-contextlearningsetup. Weperformanadditionalremovalofnewlyopted-outcontentin
January2024usingtheSpawningAPI3eventhoughOBELICShadalreadybeenfilteredtoexclude
opted-outcontentasofSeptember2023. Wealsoremovedthe5%ofdocumentswiththehighest
perplexityscores,ascomputedbyFalcon-1B(Penedoetal.,2023).
3https://spawning.ai/
6Image-text pairs Training on image-text pairs allows the model to
Captions Avg.score
learnthealignmentbetweenimagesandtheirassociatedtexts. Weuse
Alt-texts 49.8
acombinationofhigh-qualityhuman-annotatedimage-textpairsfrom
Synthetic 52.9
PMD(Singhetal.,2022)andhigher-noiseweb-scaleimage-textpairs
from(Schuhmannetal.,2022). Tolimittheamountofpoor-qualitydata, Table6:Ablationonsynthetic
weoptforthesyntheticcaptionsobtainedthroughtheLAIONCOCO4 captions against alt-text for
image-textpairs.
versionofthedatasetwhereimageshavebeencaptionedwithamodel
trained on COCO. This improves the quality of the training samples and thus the quality of the
resultingmodel(seeTable6). WeuseaNSFWclassifier5withahighrecallandremove7%ofthe
samplesinLAIONCOCO.Wemanuallyinspect5’000examplesandfound28pornographicimages
intheoriginalLAIONCOCOandonly1afterfiltering. Thisfilteringdoesnotnegativelyimpactthe
downstreamperformance.
PDF documents Sun et al. (2023) shows that a large pro-
portionofmistakesofstate-of-theartVLMsstemfromtheir OCRdata Res. DocVQA
failuretoaccuratelyextracttextinimagesordocuments. In W/o 384 22.6
ordertoobtainstrongOCRanddocumentunderstandingabil- W/o 768 42.9
W/ 768 49.9
ities,wetrainIdefics2ondifferentsourcesofPDFdocuments:
19millionindustrydocumentsfromOCR-IDL(Bitenetal., Table 7: Ablation on the synergy be-
2022)and18millionpagesfromPDFA6.Moreover,weadd tweenOCRdataandimageresolution.
Rendered Text7 to complement the dataset with texts writ- We pre-trained the models for 5’500
steps, followed by 500 steps of fine-
ten with a wide variety of fonts and colors and on diverse
tuningonDocVQA.
backgrounds. Theseintegrationssignificantlyboosttheper-
formance on benchmarks that require reading text without decreasing the performance on other
benchmarks(seeTable7).
Tomaximizecomputeefficiency,wedecomposethepre-trainingintwostages. Inthefirststage,we
limitthemaximageresolutionto384pixels,whichallowsustousealargeglobalbatchsizeof2’048
(17kimagesand2.5Mtexttokensonaverage). WesampleOBELICSfor70%oftheexampleswitha
maximumsequencelengthof2’048,andtheimage-textpairsdatasetsfor30%oftheexampleswith
amaximumsequencelengthof1’536. Inthesecondstage,weintroducePDFdocuments. Sincethey
requireahigherimageresolutionforthetexttobelegible,weincreasetheresolutiontoamaximum
of980pixels. Weusethesameglobalbatchsize,buthavetodecreasetheper-devicebatchsizeand
usegradientaccumulationtocompensatefortheadditionalmemorycost. OBELICSrepresents45%
oftheexampleswithamaximumsequencelengthof2’048,image-textpairsrepresent35%ofthe
exampleswithamaximumsequencelengthof1’536,andPDFdocumentsrepresenttheremaining
20%oftheexampleswithamaximumsequencelengthof1’024. Additionally,werandomlyscale
up images to adequately cover the distribution of potential image sizes. We emphasize that the
trainingstagesaredifferentthantheonesablatedin(Karamchetietal.,2024): insteadofselectively
freezing/unfreezingpartsofthemodel,wetraintheentiremodelduringbothstages(someparameters
aretrainedwithLoRA)andincreasetheimageresolutionfromonestagetotheother.
We use a learning rate of 10−4 and do around 2 epochs on our training data. It corresponds to
approximately1.5billionimagesand225billiontexttokens. Wenotethatthisisordersofmagnitude
more training data than other open VLMs. For example, ShareGPT (Chen et al., 2023) uses 1.2
millionimages,whileMonkey(Lietal.,2024)uses1.4millionfortraining.
Toevaluatethebasemodel,weconsiderVQAv2(Goyaletal.,2017),TextVQA(Singhetal.,2019),
OKVQA(Marinoetal.,2019),andCOCO(Linetal.,2014). Table8presentstheresults. While
havingfewertokensperimage,andthusbeingmoreefficient,Idefics2performsfavorablycompared
totheothercurrentbestbaseVLMs(OpenFlamingo(Awadallaetal.,2023),Idefics1(Laurençon
etal.,2023),Flamingo(Alayracetal.,2022),andMM1(McKinzieetal.,2024)). Itisnotablymuch
betteratreadingtextsinanimage. Figure3showsanexampleofanoutputfromthebasemodelona
tasksimilartothepre-training.
4https://laion.ai/blog/laion-coco/
5https://github.com/LAION-AI/LAION-SAFETY
6https://huggingface.co/datasets/pixparse/pdfa-eng-wds
7https://huggingface.co/datasets/wendlerc/RenderedText
7#tokens
Model Size Archi. VQAv2 TextVQA OKVQA COCO
perimage
OpenFlamingo 9B CA - 54.8 29.1 41.1 96.3
Idefics1 9B CA - 56.4 27.5 47.7 97.0
Flamingo 9B CA - 58.0 33.6 50.0 99.0
MM1 7B FA 144 63.6 46.3 51.4 116.3
Idefics2-base 8B FA 64 70.3 57.9 54.6 116.0
Table8: PerformanceofIdefics2-baseagainststate-of-the-artbaseVLMs. Theevaluationsweredone
with8randomin-contextexamples,andinanopen-endedsettingforVQAtasks.
FA:fullyautoregressivearchitecture. CA:cross-attentionarchitecture.
(Task,Metric,Split): (VQAv2,VQAacc.,testdev),(TextVQA,VQAacc.,val),(OKVQA,VQAacc.,
val),(COCO,CIDEr,test)
Prompt:
<image>The content of the letter is:\n
Completion:
Dear Craig,
Actually, I've never been big on letter writing, though it is nice to receive
a hand written note from someone. I am afraid that the art of drawing,
and the related ability to sketch an idea, are also being lost. But perhaps,
as the cliche goes, what goes around comes around.
Good luck with your project.
Tom Cianfichi
Figure3: AnexampleoftexttranscriptionwithIdefics2-base.
4.2 Instructionfine-tuning
Wecontinuethetrainingwithaninstructionfine-tuningphase.
Todoso,wecreateandreleaseTheCauldron8,amassivecollectionof50vision-languagedatasets,
coveringawiderangeoftasks: generalvisualquestionanswering,counting,captioning,texttran-
scription,documentunderstanding,chart/figureunderstanding,tableunderstanding,visualreasoning,
geometry,spottingdifferencesbetween2imagesorconvertingascreenshottoafunctionalcode. Sim-
ilarlyto(Sanhetal.,2022;Weietal.,2022;Bachetal.,2022;Daietal.,2023;Lietal.,2023),each
datasetispromptedintoasharedquestion/answerformat. Whentherearemultiplequestion/answer
pairsperimage,weconcatenatethepairsintoamulti-turnconversation. Wededuplicatethetraining
setagainsttheevaluationsets,ensuringthatthereisminimumcontaminationfromthetrainingtothe
evaluation.
Inadditiontothesevision-languagedatasetsandfollowinginsightsfrom(McKinzieetal.,2024),we
addtext-onlyinstructiondatasetstothemixture. Thedatasetsaimatteachingthemodeltofollow
complexinstructions,solvemathematicalproblems,ordoarithmeticcalculations. Wegivemore
detailsaboutthechosendatasets,thenumberofimages,question-answerpairs,andsizeofeachof
thesubsets,aswellasourselectedmixtureproportioninTable14inAppendixA.2.1.
Weinstruction-tunethebasemodelusingDoRA(Liuetal.,2024)(avariantofLoRA).Duringthe
fine-tuning,weonlycomputethelossonthetokensoftheanswersintheQ/Apairs. Sinceweare
doing many epochs over some of the datasets, we employ several strategies to lower the risk of
overfitting. First,weaddnoisetotheembeddingswiththeNEFTune(Jainetal.,2024)technique.
Then,wescaleuprandomlytheresolutionoftheimagesduringthetraining. Finally,whenapplicable,
weshufflethemultipleuser/assistantturnsrandomlybeforefeedingtheexampletothemodel.
WeevaluateIdefics2oncommonlyadoptedbenchmarks: MMMU(Yueetal.,2024)formultidis-
ciplinecollege-levelproblems,MathVista(Luetal.,2024)formathematicalreasoning,TextVQA
8https://huggingface.co/datasets/HuggingFaceM4/the_cauldron
8#tokens
Model Size MMMU MathVista TextVQA MMBench
perimage
LLaVA-NeXT 13B 2880 36.2/- 35.3 67.1 70.0
DeepSeek-VL 7B 576 36.6/- 36.1 64.4 73.2
MM1-Chat 7B 720 37.0/35.6 35.9 72.8 72.3
Idefics2 8B 64 43.5/37.9 51.6 70.4 76.8
Idefics2 8B 320 43.0/37.7 51.4 73.0 76.7
Table9: PerformanceofIdefics2againststate-of-the-artVLMsuptoasizeof14Bparameters. The
evaluationsaredoneinzeroshot. Idefics2with64or320tokensperimageisthesamemodel(same
weights),onlytheinferencediffers. ThefulltableispresentinAppendixA.3.2.
(Benchmark,Split,Metric): (MMMU,val/test,MMMUscore),(MathVista,testmini,MMMUscore),
(TextVQA,val,VQAacc.),(MMBench,test,accuracy).
(Singhetal.,2019)fortextreadingonnaturalimages,andMMBenchLiuetal.(2023)forvarious
perceptionandreasoningtasks. Table9presentstheresults(seeTable15forthecompleteresult
table)ofIdefics2againstthecurrentstrongestVLMsinitsclasssize: LLaVA-Next(Liuetal.,2024),
DeepSeek-VL(Luetal.,2024)andMM1-Chat(McKinzieetal.,2024). Whilebeingcomputation-
allymuchmoreefficientatinference,Idefics2exhibitsstrongperformanceonvariousbenchmarks,
outperformingthecurrentbestfoundationVLMsinitssizecategory. Itisonparwithstate-of-the-art
models4xitssize,orwithclosed-sourcemodelslikeGemini1.5Proonseveralbenchmarkslike
MathVistaorTextVQA.
4.3 Optimizingforchatscenarios
Theevaluationbenchmarksexpectveryshortanswers,buthumanspreferlonggenerationswheninter-
actingwithamodel. WefindthatIdefics2canexhibitdifficultiesinpreciselyfollowinginstructions
abouttheexpectedformat,makingitdifficulttoreconcile“chattiness“anddownstreamperformance.
Assuch,afterinstructionfine-tuning,wefurthertrainIdefics2ondialoguedata. Wefine-tuneIdefics2
forafewhundredstepsonLLaVA-Conv(Liuetal.,2023)andShareGPT4V(Chenetal.,2023),
withalargebatchsize. OurblindhumanevaluationsrevealthatIdefics2-chattyisoverwhelmingly
preferred over its instruction fine-tuned version in many user interactions. We also adversarially
stress-testedthemodeltogenerateinaccurate,biased,oroffensiveresponsesandreportedthefindings
inAppendixA.4. WeshowexamplesofgenerationswithIdefics2-chattyinFigure1,andinAppendix
inFigures5,6and7.
5 Conclusion
Inthiswork,were-examinecommonchoicesmadeintheVLMliteratureandrigorouslycompare
these choices in controlled experiments. Our findings touch upon the effectiveness of different
architectures, theirperformance/inferencecosttrade-offsaswellastrainingstability. Withthese
learnings at hand, we train Idefics2, an open 8B parameters vision-language model. Idefics2 is
state-of-the-artonvariousbenchmarksinitscategorysizeandismuchmoreefficientatinference.
Byreleasingourfindings,aswellasourmodelsandourtrainingdataset,weaimtocontributetothe
ongoingevolutionofVLMsandtheirapplicationsinsolvingcomplexreal-worldproblems.
9Acknowledgement
WethankMustafaShukorforhelpfulsuggestionsonthepaper,andYacineJernite,SashaLuccioni,
MargaretMitchell,GiadaPistilli,Lucie-AiméeKaffee,andJackKumarforred-teamingthemodel.
References
Acharya,M.,K.Kafle,andC.Kanan(2019). Tallyqa: Answeringcomplexcountingquestions. In
AAAI.
Agrawal,H.,K.Desai,Y.Wang,X.Chen,R.Jain,M.Johnson,D.Batra,D.Parikh,S.Lee,and
P. Anderson (2019, October). nocaps: novel object captioning at scale. In 2019 IEEE/CVF
InternationalConferenceonComputerVision(ICCV).IEEE.
Alayrac,J.-B.,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,R.Ring,E.Rutherford,S.Cabi,T.Han,Z.Gong,S.Samangooei,M.Monteiro,J.L.
Menick,S.Borgeaud,A.Brock,A.Nematzadeh,S.Sharifzadeh,M.a.Bin´kowski,R.Barreira,
O. Vinyals, A. Zisserman, and K. Simonyan (2022). Flamingo: a visual language model for
few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh
(Eds.),AdvancesinNeuralInformationProcessingSystems,Volume35,pp.23716–23736.Curran
Associates,Inc.
Antol, S., A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015). VQA:
VisualQuestionAnswering. InInternationalConferenceonComputerVision(ICCV).
Awadalla, A., I.Gao, J.Gardner, J.Hessel, Y.Hanafy, W.Zhu, K.Marathe, Y.Bitton, S.Gadre,
S.Sagawa,J.Jitsev,S.Kornblith,P.W.Koh,G.Ilharco,M.Wortsman,andL.Schmidt(2023).
Openflamingo: An open-source framework for training large autoregressive vision-language
models.
Bach,S.,V.Sanh,Z.X.Yong,A.Webson,C.Raffel,N.V.Nayak,A.Sharma,T.Kim,M.S.Bari,
T.Fevry,Z.Alyafeai,M.Dey,A.Santilli,Z.Sun,S.Ben-david,C.Xu,G.Chhablani,H.Wang,
J.Fries,M.Al-shaibani,S.Sharma,U.Thakker,K.Almubarak,X.Tang,D.Radev,M.T.-j.Jiang,
andA.Rush(2022,May). PromptSource: Anintegrateddevelopmentenvironmentandrepository
fornaturallanguageprompts. InV.Basile,Z.Kozareva,andS.Stajner(Eds.),Proceedingsofthe
60thAnnualMeetingoftheAssociationforComputationalLinguistics: SystemDemonstrations,
Dublin,Ireland,pp.93–104.AssociationforComputationalLinguistics.
Bai,J.,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou(2023). Qwen-vl: A
versatilevision-languagemodelforunderstanding,localization,textreading,andbeyond.
Bavishi,R.,E.Elsen,C.Hawthorne,M.Nye,A.Odena,A.Somani,andS.Tas¸ırlar(2023). Introduc-
ingourmultimodalmodels.
Belouadi,J.,A.Lauscher,andS.Eger(2024). Automatikz: Text-guidedsynthesisofscientificvector
graphicswithtikz.
Biten,A.F.,R.Tito,L.Gomez,E.Valveny,andD.Karatzas(2022). Ocr-idl: Ocrannotationsfor
industrydocumentlibrarydataset.
Biten, A. F., R. Tito, A. Mafla, L. Gomez, M. Rusiñol, C. Jawahar, E. Valveny, and D. Karatzas
(2019). Scenetextvisualquestionanswering. In2019IEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pp.4290–4300.
Blecher,L.,G.Cucurull,T.Scialom,andR.Stojnic(2023). Nougat: Neuralopticalunderstanding
foracademicdocuments.
Brown,T.,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G.Sastry,A.Askell,S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,
J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,andD.Amodei(2020). Language
modelsarefew-shotlearners. InH.Larochelle,M.Ranzato,R.Hadsell,M.Balcan,andH.Lin
(Eds.),AdvancesinNeuralInformationProcessingSystems,Volume33,pp.1877–1901.Curran
Associates,Inc.
10Carbune,V.,H.Mansoor,F.Liu,R.Aralikatte,G.Baechler,J.Chen,andA.Sharma(2024). Chart-
basedreasoning: Transferringcapabilitiesfromllmstovlms.
Chang,S.,D.Palzer,J.Li,E.Fosler-Lussier,andN.Xiao(2022). MapQA:Adatasetforquestion
answeringonchoroplethmaps. InNeurIPS2022FirstTableRepresentationWorkshop.
Changpinyo,S.,P.Sharma,N.Ding,andR.Soricut(2021). Conceptual12M:Pushingweb-scale
image-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR.
Chen, L., J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin (2023). Sharegpt4v:
Improvinglargemulti-modalmodelswithbettercaptions.
Chen,X.,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,
X.Wang,Y.Tay,S.Shakeri,M.Dehghani,D.Salz,M.Lucic,M.Tschannen,A.Nagrani,H.Hu,
M.Joshi,B.Pang,C.Montgomery,P.Pietrzyk,M.Ritter,A.Piergiovanni,M.Minderer,F.Pavetic,
A.Waters,G.Li,I.Alabdulmohsin,L.Beyer,J.Amelot,K.Lee,A.P.Steiner,Y.Li,D.Keysers,
A.Arnab,Y.Xu,K.Rong,A.Kolesnikov,M.Seyedhosseini,A.Angelova,X.Zhai,N.Houlsby,
andR.Soricut(2023). Pali-x: Onscalingupamultilingualvisionandlanguagemodel.
Chen, X. and X. Wang (2022). Pali: Scaling language-image learning in 100+ languages. In
ConferenceonNeuralInformationProcessingSystems(NeurIPS).
Chen, X., X. Wang, L. Beyer, A. Kolesnikov, J. Wu, P. Voigtlaender, B. Mustafa, S. Goodman,
I.Alabdulmohsin,P.Padlewski,D.Salz,X.Xiong,D.Vlasic,F.Pavetic,K.Rong,T.Yu,D.Keysers,
X.Zhai,andR.Soricut(2023). Pali-3visionlanguagemodels: Smaller,faster,stronger.
Chen,Z.,W.Chen,C.Smiley,S.Shah,I.Borova,D.Langdon,R.Moussa,M.Beane,T.-H.Huang,
B.Routledge,andW.Y.Wang(2021,November). FinQA:Adatasetofnumericalreasoningover
financialdata. InM.-F.Moens,X.Huang,L.Specia,andS.W.-t.Yih(Eds.),Proceedingsofthe
2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,OnlineandPuntaCana,
DominicanRepublic,pp.3697–3711.AssociationforComputationalLinguistics.
Cheng,Z.,H.Dong,Z.Wang,R.Jia,J.Guo,Y.Gao,S.Han,J.-G.Lou,andD.Zhang(2022,May).
HiTab: Ahierarchicaltabledatasetforquestionansweringandnaturallanguagegeneration. In
S.Muresan,P.Nakov,andA.Villavicencio(Eds.),Proceedingsofthe60thAnnualMeetingof
the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, pp.
1094–1110.AssociationforComputationalLinguistics.
Chu,X.,L.Qiao,X.Zhang,S.Xu,F.Wei,Y.Yang,X.Sun,Y.Hu,X.Lin,B.Zhang,andC.Shen
(2024). Mobilevlmv2: Fasterandstrongerbaselineforvisionlanguagemodel.
Conover, M., M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wen-
dell, M. Zaharia, and R. Xin (2023). Free dolly: Introducing the world’s first
truly open instruction-tuned llm. https://www.databricks.com/blog/2023/04/12/
dolly-first-open-commercially-viable-instruction-tuned-llm. Accessed: 2023-
06-30.
Dai,W.,J.Li,D.Li,A.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi(2023). InstructBLIP:
Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh
ConferenceonNeuralInformationProcessingSystems.
Darcet,T.,M.Oquab,J.Mairal,andP.Bojanowski(2024). Visiontransformersneedregisters. In
TheTwelfthInternationalConferenceonLearningRepresentations.
Dehghani,M.,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,A.P.Steiner,M.Caron,
R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang,
C.RiquelmeRuiz,M.Minderer,J.Puigcerver,U.Evci,M.Kumar,S.V.Steenkiste,G.F.Elsayed,
A.Mahendran,F.Yu,A.Oliver,F.Huot,J.Bastings,M.Collier,A.A.Gritsenko,V.Birodkar,C.N.
Vasconcelos,Y.Tay,T.Mensink,A.Kolesnikov,F.Pavetic,D.Tran,T.Kipf,M.Lucic,X.Zhai,
D.Keysers,J.J.Harmsen,andN.Houlsby(2023,23–29Jul). Scalingvisiontransformersto22
billionparameters. InA.Krause,E.Brunskill,K.Cho,B.Engelhardt,S.Sabato,andJ.Scarlett
(Eds.),Proceedingsofthe40thInternationalConferenceonMachineLearning,Volume202of
ProceedingsofMachineLearningResearch,pp.7480–7512.PMLR.
11Dehghani,M.,B.Mustafa,J.Djolonga,J.Heek,M.Minderer,M.Caron,A.P.Steiner,J.Puigcerver,
R.Geirhos,I.Alabdulmohsin,A.Oliver,P.Padlewski,A.A.Gritsenko,M.Lucic,andN.Houlsby
(2023). Patchn’pack: Navit,avisiontransformerforanyaspectratioandresolution. InThirty-
seventhConferenceonNeuralInformationProcessingSystems.
Deng,J.,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei(2009). Imagenet: Alarge-scalehierar-
chicalimagedatabase. In2009IEEEConferenceonComputerVisionandPatternRecognition,pp.
248–255.
Desai,K.,G.Kaul,Z.Aysola,andJ.Johnson(2021). Redcaps: Web-curatedimage-textdatacreated
bythepeople,forthepeople. InJ.VanschorenandS.Yeung(Eds.),ProceedingsoftheNeural
InformationProcessingSystemsTrackonDatasetsandBenchmarks,Volume1.Curran.
Driess, D., F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,
Q.Vuong,T.Yu,W.Huang,Y.Chebotar,P.Sermanet,D.Duckworth,S.Levine,V.Vanhoucke,
K.Hausman,M.Toussaint,K.Greff,A.Zeng,I.Mordatch,andP.Florence(2023). Palm-e: an
embodiedmultimodallanguagemodel. InProceedingsofthe40thInternationalConferenceon
MachineLearning,ICML’23.JMLR.org.
Gao, P., R.Zhang, C.Liu, L.Qiu, S.Huang, W.Lin, S.Zhao, S.Geng, Z.Lin, P.Jin, K.Zhang,
W.Shao,C.Xu,C.He,J.He,H.Shao,P.Lu,H.Li,andY.Qiao(2024). Sphinx-x: Scalingdata
andparametersforafamilyofmulti-modallargelanguagemodels.
Google(2023). Gemini: Afamilyofhighlycapablemultimodalmodels.
Google (2024a). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of
context.
Google(2024b). Gemma: Openmodelsbasedongeminiresearchandtechnology.
Goyal,Y.,T.Khot,D.Summers-Stay,D.Batra,andD.Parikh(2017). Makingthevinvqamatter:
Elevatingtheroleofimageunderstandinginvisualquestionanswering. In2017IEEEConference
onComputerVisionandPatternRecognition(CVPR),pp.6325–6334.
He,X.,Y.Zhang,L.Mou,E.Xing,andP.Xie(2020). Pathvqa: 30000+questionsformedicalvisual
questionanswering.
Hendrycks, D., C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021).
Measuringmassivemultitasklanguageunderstanding. InInternationalConferenceonLearning
Representations.
Hong,W.,W.Wang,Q.Lv,J.Xu,W.Yu,J.Ji,Y.Wang,Z.Wang,Y.Zhang,J.Li,B.Xu,Y.Dong,
M.Ding,andJ.Tang(2023). Cogagent: Avisuallanguagemodelforguiagents.
Hu,A.,H.Xu,J.Ye,M.Yan,L.Zhang,B.Zhang,C.Li,J.Zhang,Q.Jin,F.Huang,andJ.Zhou
(2024). mplug-docowl1.5: Unifiedstructurelearningforocr-freedocumentunderstanding.
Hu, E. J., yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2022).
LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConferenceonLearning
Representations.
Huang,S.,L.Dong,W.Wang,Y.Hao,S.Singhal,S.Ma,T.Lv,L.Cui,O.K.Mohammed,B.Patra,
Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei (2023).
Language is not all you need: Aligning perception with language models. In Thirty-seventh
ConferenceonNeuralInformationProcessingSystems.
Hudson, D. A. and C. D. Manning (2019). Gqa: A new dataset for real-world visual reasoning
andcompositionalquestionanswering. In2019IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pp.6693–6702.
Iyyer,M.,W.-t.Yih,andM.-W.Chang(2017,July). Search-basedneuralstructuredlearningfor
sequentialquestionanswering.InR.BarzilayandM.-Y.Kan(Eds.),Proceedingsofthe55thAnnual
MeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),Vancouver,
Canada,pp.1821–1831.AssociationforComputationalLinguistics.
12Jaegle,A.,F.Gimeno,A.Brock,O.Vinyals,A.Zisserman,andJ.Carreira(2021,18–24Jul). Per-
ceiver: Generalperceptionwithiterativeattention. InM.MeilaandT.Zhang(Eds.),Proceedings
ofthe38thInternationalConferenceonMachineLearning,Volume139ofProceedingsofMachine
LearningResearch,pp.4651–4664.PMLR.
Jain, N., P. yeh Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli, B. R. Bartoldson,
B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping, and T. Goldstein (2024).
NEFTune: Noisy embeddings improve instruction finetuning. In The Twelfth International
ConferenceonLearningRepresentations.
Jhamtani,H.etal.(2018,October-November). Learningtodescribedifferencesbetweenpairsof
similarimages. InE.Riloff,D.Chiang,J.Hockenmaier,andJ.Tsujii(Eds.),Proceedingsofthe
2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,pp.
4024–4034.AssociationforComputationalLinguistics.
Jiang,A.Q.,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.delasCasas,F.Bressand,
G.Lengyel,G.Lample,L.Saulnier,L.R.Lavaud,M.-A.Lachaux,P.Stock,T.L.Scao,T.Lavril,
T.Wang,T.Lacroix,andW.E.Sayed(2023). Mistral7b.
Johnson,J.,B.Hariharan,L.vanderMaaten,L.Fei-Fei,C.L.Zitnick,andR.Girshick(2017). Clevr:
Adiagnosticdatasetforcompositionallanguageandelementaryvisualreasoning. In2017IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR),pp.1988–1997.
Kafle,K.,S.Cohen,B.Price,andC.Kanan(2018). Dvqa: Understandingdatavisualizationsvia
questionanswering. InCVPR.
Kahou,S.E.,V.Michalski,A.Atkinson,A.Kadar,A.Trischler,andY.Bengio(2018). Figureqa: An
annotatedfiguredatasetforvisualreasoning.
Karamcheti,S.,S.Nair,A.Balakrishna,P.Liang,T.Kollar,andD.Sadigh(2024). Prismaticvlms:
Investigatingthedesignspaceofvisually-conditionedlanguagemodels.
Kazemi,M.,H.Alvari,A.Anand,J.Wu,X.Chen,andR.Soricut(2024). Geomverse: Asystematic
evaluation of large models for geometric reasoning. In Synthetic Data for Computer Vision
Workshop@CVPR2024.
Kembhavi,A.,M.Salvato,E.Kolve,M.Seo,H.Hajishirzi,andA.Farhadi(2016). Adiagramis
worthadozenimages. InB.Leibe,J.Matas,N.Sebe,andM.Welling(Eds.),ComputerVision–
ECCV2016,Cham,pp.235–251.SpringerInternationalPublishing.
Kembhavi,A.,M.Seo,D.Schwenk,J.Choi,A.Farhadi,andH.Hajishirzi(2017). Areyousmarter
thanasixthgrader? textbookquestionansweringformultimodalmachinecomprehension. In2017
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.5376–5384.
Kiela, D., H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine (2020).
Thehatefulmemeschallenge: Detectinghatespeechinmultimodalmemes. InH.Larochelle,
M.Ranzato,R.Hadsell,M.Balcan,andH.Lin(Eds.),AdvancesinNeuralInformationProcessing
Systems,Volume33,pp.2611–2624.CurranAssociates,Inc.
Kingma, D. and J. Ba (2015). Adam: A method for stochastic optimization. In International
ConferenceonLearningRepresentations(ICLR),SanDiega,CA,USA.
Koh, J. Y., R. Salakhutdinov, and D. Fried (2023). Grounding language models to images for
multimodalinputsandoutputs.
Lau, J., S. Gayen, A. Ben Abacha, and D. Demner-Fushman (2018, 11). A dataset of clinically
generatedvisualquestionsandanswersaboutradiologyimages. ScientificData5,180251.
Laurençon,H.,L.Saulnier,T.Wang,C.Akiki,A.VillanovadelMoral,T.LeScao,L.VonWerra,
C.Mou,E.GonzálezPonferrada,H.Nguyen,J.Frohberg,M.Šaško,Q.Lhoest,A.McMillan-
Major, G. Dupont, S. Biderman, A. Rogers, L. Ben allal, F. De Toni, G. Pistilli, O. Nguyen,
S.Nikpoor,M.Masoud,P.Colombo,J.delaRosa,P.Villegas,T.Thrush,S.Longpre,S.Nagel,
L.Weber,M.Muñoz,J.Zhu,D.VanStrien,Z.Alyafeai,K.Almubarak,M.C.Vu,I.Gonzalez-
Dios,A.Soroa,K.Lo,M.Dey,P.OrtizSuarez,A.Gokaslan,S.Bose,D.Adelani,L.Phan,H.Tran,
13I. Yu, S. Pai, J. Chim, V. Lepercq, S. Ilic, M. Mitchell, S. A. Luccioni, and Y. Jernite (2022).
Thebigsciencerootscorpus: A1.6tbcompositemultilingualdataset. InS.Koyejo,S.Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),AdvancesinNeuralInformationProcessing
Systems,Volume35,pp.31809–31826.CurranAssociates,Inc.
Laurençon,H.,L.Saulnier,L.Tronchon,S.Bekman,A.Singh,A.Lozhkov,T.Wang,S.Karamcheti,
A. M. Rush, D. Kiela, M. Cord, and V. Sanh (2023). OBELICS: An open web-scale filtered
datasetofinterleavedimage-textdocuments. InThirty-seventhConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack.
Laurençon,H.,L.Tronchon,andV.Sanh(2024). Unlockingtheconversionofwebscreenshotsinto
htmlcodewiththewebsightdataset.
Lee,B.-K.,B.Park,C.W.Kim,andY.M.Ro(2024). Moai: Mixtureofallintelligenceforlarge
languageandvisionmodels.
Lee, K., M.Joshi, I.Turc, H.Hu, F.Liu, J.Eisenschlos, U.Khandelwal, P.Shaw, M.-W.Chang,
andK.Toutanova(2023). Pix2struct: screenshotparsingaspretrainingforvisuallanguageunder-
standing. InProceedingsofthe40thInternationalConferenceonMachineLearning,ICML’23.
JMLR.org.
Li,B.,R.Wang,G.Wang,Y.Ge,Y.Ge,andY.Shan(2023). Seed-bench: Benchmarkingmultimodal
llmswithgenerativecomprehension.
Li,B.,Y.Zhang,L.Chen,J.Wang,F.Pu,J.Yang,C.Li,andZ.Liu(2023). Mimic-it: Multi-modal
in-contextinstructiontuning.
Li,G.,H.A.A.K.Hammoud,H.Itani,D.Khizbullin,andB.Ghanem(2023). CAMEL:Communica-
tiveagentsfor”mind”explorationoflargelanguagemodelsociety. InThirty-seventhConference
onNeuralInformationProcessingSystems.
Li, J., D.Li, S.Savarese, andS.Hoi(2023). Blip-2: bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InProceedingsofthe40thInternational
ConferenceonMachineLearning,ICML’23.JMLR.org.
Li, J., D. Li, C. Xiong, and S. Hoi (2022). Blip: Bootstrapping language-image pre-training for
unifiedvision-languageunderstandingandgeneration. InICML.
Li,L.,Y.Yin,S.Li,L.Chen,P.Wang,S.Ren,M.Li,Y.Yang,J.Xu,X.Sun,L.Kong,andQ.Liu
(2023). M3it: Alarge-scaledatasettowardsmulti-modalmultilingualinstructiontuning.
Li, Y., Y. Du, K. Zhou, J. Wang, X. Zhao, and J.-R. Wen (2023, December). Evaluating object
hallucination in large vision-language models. In H. Bouamor, J. Pino, and K. Bali (Eds.),
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
Singapore,pp.292–305.AssociationforComputationalLinguistics.
Li,Y.,Y.Zhang,C.Wang,Z.Zhong,Y.Chen,R.Chu,S.Liu,andJ.Jia(2024). Mini-gemini:Mining
thepotentialofmulti-modalityvisionlanguagemodels.
Li,Z.,B.Yang,Q.Liu,Z.Ma,S.Zhang,J.Yang,Y.Sun,Y.Liu,andX.Bai(2024). Monkey: Image
resolutionandtextlabelareimportantthingsforlargemulti-modalmodels.
Lin, B., Z.Tang, Y.Ye, J.Cui, B.Zhu, P.Jin, J.Huang, J.Zhang, M.Ning, andL.Yuan(2024).
Moe-llava: Mixtureofexpertsforlargevision-languagemodels.
Lin,J.,H.Yin,W.Ping,Y.Lu,P.Molchanov,A.Tao,H.Mao,J.Kautz,M.Shoeybi,andS.Han
(2024). Vila: Onpre-trainingforvisuallanguagemodels.
Lin,T.-Y.,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick(2014).
Microsoftcoco: Commonobjectsincontext. InD.Fleet,T.Pajdla,B.Schiele,andT.Tuytelaars
(Eds.),ComputerVision–ECCV2014,Cham,pp.740–755.SpringerInternationalPublishing.
Lin, Z., C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen, J. Han,
S.Huang,Y.Zhang,X.He,H.Li,andY.Qiao(2023). Sphinx: Thejointmixingofweights,tasks,
andvisualembeddingsformulti-modallargelanguagemodels.
14Lindström,A.D.(2022).Clevr-math:Adatasetforcompositionallanguage,visual,andmathematical
reasoning.
Liu,F.,G.Emerson,andN.Collier(2023). Visualspatialreasoning. TransactionsoftheAssociation
forComputationalLinguistics11,635–651.
Liu,H.,C.Li,Y.Li,andY.J.Lee(2023). Improvedbaselineswithvisualinstructiontuning. In
NeurIPS2023WorkshoponInstructionTuningandInstructionFollowing.
Liu,H.,C.Li,Y.Li,B.Li,Y.Zhang,S.Shen,andY.J.Lee(2024,January). Llava-next: Improved
reasoning,ocr,andworldknowledge.
Liu,H.,C.Li,Q.Wu,andY.J.Lee(2023). Visualinstructiontuning. InThirty-seventhConference
onNeuralInformationProcessingSystems.
Liu,H.,Q.You,X.Han,Y.Wang,B.Zhai,Y.Liu,Y.Tao,H.Huang,R.He,andH.Yang(2024).
Infimm-hd: Aleapforwardinhigh-resolutionmultimodalunderstanding.
Liu,S.-Y.,C.-Y.Wang,H.Yin,P.Molchanov,Y.-C.F.Wang,K.-T.Cheng,andM.-H.Chen(2024).
Dora: Weight-decomposedlow-rankadaptation.
Liu,T.andB.K.H.Low(2023). Goat: Fine-tunedllamaoutperformsgpt-4onarithmetictasks.
Liu,Y.,H.Duan,Y.Zhang,B.Li,S.Zhang,W.Zhao,Y.Yuan,J.Wang,C.He,Z.Liu,K.Chen,and
D.Lin(2023). Mmbench: Isyourmulti-modalmodelanall-aroundplayer?
Lu, H., W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, Y. Sun,
C.Deng,H.Xu,Z.Xie,andC.Ruan(2024). Deepseek-vl: Towardsreal-worldvision-language
understanding.
Lu, J., C. Clark, S. Lee, Z. Zhang, S. Khosla, R. Marten, D. Hoiem, and A. Kembhavi (2023).
Unified-io2: Scalingautoregressivemultimodalmodelswithvision,language,audio,andaction.
Lu,P.,H.Bansal,T.Xia,J.Liu,C.Li,H.Hajishirzi,H.Cheng,K.-W.Chang,M.Galley,andJ.Gao
(2024). Mathvista: Evaluatingmathematicalreasoningoffoundationmodelsinvisualcontexts. In
InternationalConferenceonLearningRepresentations(ICLR).
Lu,P.,R.Gong,S.Jiang,L.Qiu,S.Huang,X.Liang,andS.-C.Zhu(2021). Inter-gps: Interpretable
geometryproblemsolvingwithformallanguageandsymbolicreasoning. InTheJointConfer-
enceofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th
InternationalJointConferenceonNaturalLanguageProcessing(ACL-IJCNLP2021).
Lu,P.,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord,P.Clark,andA.Kalyan(2022).
Learntoexplain: Multimodalreasoningviathoughtchainsforsciencequestionanswering. In
S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),AdvancesinNeural
InformationProcessingSystems,Volume35,pp.2507–2521.CurranAssociates,Inc.
Lu,P.,L.Qiu,K.-W.Chang,Y.N.Wu,S.-C.Zhu,T.Rajpurohit,P.Clark,andA.Kalyan(2023).
Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In
InternationalConferenceonLearningRepresentations(ICLR).
Lu,P.,L.Qiu,J.Chen,T.Xia,Y.Zhao,W.Zhang,Z.Yu,X.Liang,andS.-C.Zhu(2021). Iconqa:
A new benchmark for abstract diagram understanding and visual language reasoning. In The
35thConferenceonNeuralInformationProcessingSystems(NeurIPS)TrackonDatasetsand
Benchmarks.
Mañas, O., P. Rodriguez Lopez, S. Ahmadi, A. Nematzadeh, Y. Goyal, and A. Agrawal (2023,
May). MAPL:Parameter-efficientadaptationofunimodalpre-trainedmodelsforvision-language
few-shotprompting. InA.VlachosandI.Augenstein(Eds.),Proceedingsofthe17thConference
oftheEuropeanChapteroftheAssociationforComputationalLinguistics,Dubrovnik,Croatia,pp.
2523–2548.AssociationforComputationalLinguistics.
Marino,K.,M.Rastegari,A.Farhadi,andR.Mottaghi(2019). Ok-vqa: Avisualquestionanswer-
ing benchmarkrequiring external knowledge. InConferenceon ComputerVisionand Pattern
Recognition(CVPR).
15Marti,U.-V.andH.Bunke(2002,11). Theiam-database: Anenglishsentencedatabaseforoffline
handwritingrecognition. InternationalJournalonDocumentAnalysisandRecognition5,39–46.
Masry, A., D. Long, J. Q. Tan, S. Joty, and E. Hoque (2022, May). ChartQA: A benchmark for
questionansweringaboutchartswithvisualandlogicalreasoning. InFindingsoftheAssociation
for Computational Linguistics: ACL 2022, Dublin, Ireland, pp. 2263–2279. Association for
ComputationalLinguistics.
Mathew,M.,V.Bagal,R.Tito,D.Karatzas,E.Valveny,andC.V.Jawahar(2022). Infographicvqa. In
2022IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV),pp.2582–2591.
Mathew,M.,D.Karatzas,andC.V.Jawahar(2021). Docvqa: Adatasetforvqaondocumentimages.
In2021IEEEWinterConferenceonApplicationsofComputerVision(WACV),pp.2199–2208.
McKinzie,B.,Z.Gan,J.-P.Fauconnier,S.Dodge,B.Zhang,P.Dufter,D.Shah,X.Du,F.Peng,
F. Weers, A. Belyi, H. Zhang, K. Singh, D. Kang, A. Jain, H. Hè, M. Schwarzer, T. Gunter,
X.Kong, A.Zhang, J.Wang, C.Wang, N.Du, T.Lei, S.Wiseman, G.Yin, M.Lee, Z.Wang,
R.Pang,P.Grasch,A.Toshev,andY.Yang(2024). Mm1: Methods,analysis&insightsfrom
multimodalllmpre-training.
Methani, N., P. Ganguly, M. M. Khapra, and P. Kumar (2020, March). Plotqa: Reasoning over
scientificplots. InTheIEEEWinterConferenceonApplicationsofComputerVision(WACV).
Mishra,A.,S.Shekhar,A.K.Singh,andA.Chakraborty(2019). Ocr-vqa: Visualquestionanswering
byreadingtextinimages.In2019InternationalConferenceonDocumentAnalysisandRecognition
(ICDAR),pp.947–952.
Mitra,A.,H.Khanpour,C.Rosset,andA.Awadallah(2024). Orca-math: Unlockingthepotentialof
slmsingradeschoolmath.
Obeid,J.andE.Hoque(2020,December). Chart-to-text: Generatingnaturallanguagedescriptions
forchartsbyadaptingthetransformermodel. InB.Davis,Y.Graham,J.Kelleher,andY.Sripada
(Eds.),Proceedingsofthe13thInternationalConferenceonNaturalLanguageGeneration,Dublin,
Ireland,pp.138–147.AssociationforComputationalLinguistics.
OpenAI(2024). Gpt-4technicalreport.
Pasupat,P.andP.Liang(2015,July). Compositionalsemanticparsingonsemi-structuredtables. In
C.ZongandM.Strube(Eds.), Proceedingsofthe53rdAnnualMeetingoftheAssociationfor
ComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguagePro-
cessing(Volume1: LongPapers),Beijing,China,pp.1470–1480.AssociationforComputational
Linguistics.
Penedo, G., Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Al-
mazrouei,andJ.Launay(2023). TherefinedwebdatasetforfalconLLM:Outperformingcurated
corpora with web data only. In Thirty-seventh Conference on Neural Information Processing
SystemsDatasetsandBenchmarksTrack.
Pont-Tuset,J.,J.Uijlings,S.Changpinyo,R.Soricut,andV.Ferrari(2020). Connectingvisionand
languagewithlocalizednarratives. InA.Vedaldi,H.Bischof,T.Brox,andJ.-M.Frahm(Eds.),
ComputerVision–ECCV2020,Cham,pp.647–664.SpringerInternationalPublishing.
Radford,A.,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J.Clark,G.Krueger,andI.Sutskever(2021). Learningtransferablevisualmodelsfromnatural
languagesupervision. InInternationalConferenceonMachineLearning.
Ren,M.,R.Kiros,andR.Zemel(2015). Exploringmodelsanddataforimagequestionanswering.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (Eds.), Advances in Neural
InformationProcessingSystems,Volume28.CurranAssociates,Inc.
Sanh,V.,A.Webson,C.Raffel,S.Bach,L.Sutawika,Z.Alyafeai,A.Chaffin,A.Stiegler,A.Raja,
M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani,
N.Nayak,D.Datta,J.Chang,M.T.-J.Jiang,H.Wang,M.Manica,S.Shen,Z.X.Yong,H.Pandey,
16R.Bawden,T.Wang,T.Neeraj,J.Rozen,A.Sharma,A.Santilli,T.Fevry,J.A.Fries,R.Teehan,
T.L.Scao,S.Biderman,L.Gao,T.Wolf,andA.M.Rush(2022). Multitaskpromptedtraining
enableszero-shottaskgeneralization. InInternationalConferenceonLearningRepresentations.
Schuhmann,C.,R.Beaumont,R.Vencu,C.Gordon,R.Wightman,M.Cherti,T.Coombes,A.Katta,
C.Mullis,M.Wortsman,P.Schramowski,S.Kundurthy,K.Crowson,L.Schmidt,R.Kaczmarczyk,
andJ.Jitsev(2022). Laion-5b: Anopenlarge-scaledatasetfortrainingnextgenerationimage-text
models. InS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Advances
inNeuralInformationProcessingSystems,Volume35,pp.25278–25294.CurranAssociates,Inc.
Schuhmann,C.,R.Vencu,R.Beaumont,R.Kaczmarczyk,C.Mullis,A.Katta,T.Coombes,J.Jitsev,
andA.Komatsuzaki(2021). Laion-400m: Opendatasetofclip-filtered400millionimage-text
pairs.
Schwenk,D.,A.Khandelwal,C.Clark,K.Marino,andR.Mottaghi(2022). A-okvqa: Abenchmark
forvisualquestionansweringusingworldknowledge. InComputerVision–ECCV2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII, Berlin,
Heidelberg,pp.146–162.Springer-Verlag.
Sharma,P.,N.Ding,S.Goodman,andR.Soricut(2018). Conceptualcaptions: Acleaned,hyper-
nymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsofACL.
Shayegani,E.,Y.Dong,andN.Abu-Ghazaleh(2024). Jailbreakinpieces: Compositionaladversarial
attacksonmulti-modallanguagemodels. InTheTwelfthInternationalConferenceonLearning
Representations.
Shukor,M.,C.Dancette,andM.Cord(2023,oct). ep-alm: Efficientperceptualaugmentationof
languagemodels. In2023IEEE/CVFInternationalConferenceonComputerVision(ICCV),Los
Alamitos,CA,USA,pp.21999–22012.IEEEComputerSociety.
Sidorov,O.,R.Hu,M.Rohrbach,andA.Singh(2020).Textcaps:Adatasetforimagecaptioningwith
readingcomprehension. InA.Vedaldi,H.Bischof,T.Brox,andJ.-M.Frahm(Eds.),Computer
Vision–ECCV2020,Cham,pp.742–758.SpringerInternationalPublishing.
Singh,A.,R.Hu,V.Goswami,G.Couairon,W.Galuba,M.Rohrbach,andD.Kiela(2022). Flava: A
foundationallanguageandvisionalignmentmodel. In2022IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pp.15617–15629.
Singh,A.,V.Natarjan,M.Shah,Y.Jiang,X.Chen,D.Parikh,andM.Rohrbach(2019). Towards
vqamodelsthatcanread. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,pp.8317–8326.
Srinivasan,K.,K.Raman,J.Chen,M.Bendersky,andM.Najork(2021). Wit: Wikipedia-based
image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th
InternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,
SIGIR’21,NewYork,NY,USA,pp.2443–2449.AssociationforComputingMachinery.
Suhr,A.,S.Zhou,A.Zhang,I.Zhang,H.Bai,andY.Artzi(2019,July). Acorpusforreasoning
aboutnaturallanguagegroundedinphotographs. InA.Korhonen,D.Traum,andL.Màrquez
(Eds.),Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,
Florence,Italy,pp.6418–6428.AssociationforComputationalLinguistics.
Sun,Q.,Y.Cui,X.Zhang,F.Zhang,Q.Yu,Z.Luo,Y.Wang,Y.Rao,J.Liu,T.Huang,andX.Wang
(2023). Generativemultimodalmodelsarein-contextlearners.
Sun,Q.,Y.Fang,L.Wu,X.Wang,andY.Cao(2023). Eva-clip: Improvedtrainingtechniquesfor
clipatscale.
Sun,Z.,S.Shen,S.Cao,H.Liu,C.Li,Y.Shen,C.Gan,L.-Y.Gui,Y.-X.Wang,Y.Yang,K.Keutzer,
andT.Darrell(2023). Aligninglargemultimodalmodelswithfactuallyaugmentedrlhf.
Tanaka, R., K. Nishida, and S. Yoshida (2021). Visualmrc: Machine reading comprehension on
documentimages. InAAAI.
17Tang,B.J.,A.Boggust,andA.Satyanarayan(2023). VisText: ABenchmarkforSemanticallyRich
ChartCaptioning. InTheAnnualMeetingoftheAssociationforComputationalLinguistics(ACL).
Teknium(2023). Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassistants.
Thiel,D.(2023). Identifyingandeliminatingcsamingenerativemltrainingdataandmodels.
Touvron,H.,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro,F.Azhar,A.Rodriguez,A.Joulin,E.Grave,andG.Lample(2023). Llama: Openand
efficientfoundationlanguagemodels.
Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,
J.Fernandes,J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hosseini,
R.Hou,H.Inan,M.Kardas,V.Kerkez,M.Khabsa,I.Kloumann,A.Korenev,P.S.Koura,M.-A.
Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,
I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,R.Silva,E.M.
Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,
I.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,S.Edunov,and
T.Scialom(2023). Llama2: Openfoundationandfine-tunedchatmodels.
Vallaeys, T., M. Shukor, M. Cord, and J. Verbeek (2024). Improved baselines for data-efficient
perceptualaugmentationofllms.
Wang, B., G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li (2021). Screen2words: Automatic
mobileuisummarizationwithmultimodallearning. InThe34thAnnualACMSymposiumonUser
InterfaceSoftwareandTechnology,UIST’21,NewYork,NY,USA,pp.498–510.Associationfor
ComputingMachinery.
Wang,W.,Q.Lv,W.Yu,W.Hong,J.Qi,Y.Wang,J.Ji,Z.Yang,L.Zhao,X.Song,J.Xu,B.Xu,J.Li,
Y.Dong,M.Ding,andJ.Tang(2024). Cogvlm: Visualexpertforpretrainedlanguagemodels.
Wei,J.,M.Bosma,V.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du,A.M.Dai,andQ.V.Le(2022).
Finetuned language models are zero-shot learners. In International Conference on Learning
Representations.
Xiao, J., Z. Xu, A. Yuille, S. Yan, and B. Wang (2024). Palm2-vadapter: Progressively aligned
languagemodelmakesastrongvision-languageadapter.
Young, P., A. Lai, M. Hodosh, and J. Hockenmaier (2014). From image descriptions to visual
denotations: Newsimilaritymetricsforsemanticinferenceovereventdescriptions. Transactions
oftheAssociationforComputationalLinguistics2,67–78.
Yu, L., W.Jiang, H.Shi, J.YU,Z.Liu, Y.Zhang, J.Kwok, Z.Li, A.Weller, andW.Liu(2024).
Metamath: Bootstrapyourownmathematicalquestionsforlargelanguagemodels. InTheTwelfth
InternationalConferenceonLearningRepresentations.
Yue,X.,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,W.Ren,Y.Sun,C.Wei,
B.Yu,R.Yuan,R.Sun,M.Yin,B.Zheng,Z.Yang,Y.Liu,W.Huang,H.Sun,Y.Su,andW.Chen
(2024). Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoningbenchmark
forexpertagi. InProceedingsofCVPR.
Yue, X., X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen (2024). MAmmoTH:
Buildingmathgeneralistmodelsthroughhybridinstructiontuning. InTheTwelfthInternational
ConferenceonLearningRepresentations.
Zhai, X., B. Mustafa, A. Kolesnikov, and L. Beyer (2023). Sigmoid loss for language image
pre-training.
Zhang, C., F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu (2019). Raven: A dataset for relational and
analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and
PatternRecognition(CVPR).
18Zhang, X., C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie (2023). Pmc-vqa: Visual
instructiontuningformedicalvisualquestionanswering.
Zhao,Y.,Y.Li,C.Li,andR.Zhang(2022,May). MultiHiertt: Numericalreasoningovermultihier-
archicaltabularandtextualdata. InProceedingsofthe60thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),Dublin,Ireland,pp.6588–6600.Association
forComputationalLinguistics.
Zhao,Y.,C.Zhao,L.Nan,Z.Qi,W.Zhang,X.Tang,B.Mi,andD.Radev(2023,July). RobuT:A
systematicstudyoftableQArobustnessagainsthuman-annotatedadversarialperturbations. In
A.Rogers,J.Boyd-Graber,andN.Okazaki(Eds.),Proceedingsofthe61stAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1: LongPapers), Toronto, Canada, pp.
6064–6081.AssociationforComputationalLinguistics.
Zhong,V.,C.Xiong,andR.Socher(2017). Seq2sql: Generatingstructuredqueriesfromnatural
languageusingreinforcementlearning.
Zhou,B.,Y.Hu,X.Weng,J.Jia,J.Luo,X.Liu,J.Wu,andL.Huang(2024). Tinyllava:Aframework
ofsmall-scalelargemultimodalmodels.
Zhou,C.,P.Liu,P.Xu,S.Iyer,J.Sun,Y.Mao,X.Ma,A.Efrat,P.Yu,L.YU,S.Zhang,G.Ghosh,
M.Lewis,L.Zettlemoyer,andO.Levy(2023). LIMA:Lessismoreforalignment. InThirty-
seventhConferenceonNeuralInformationProcessingSystems.
Zhu, F., W. Lei, Y. Huang, C. Wang, S. Zhang, J. Lv, F. Feng, and T.-S. Chua (2021, August).
TAT-QA:Aquestionansweringbenchmarkonahybridoftabularandtextualcontentinfinance. In
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe
11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),
Online,pp.3277–3287.AssociationforComputationalLinguistics.
Zhu,W.,J.Hessel,A.Awadalla,S.Y.Gadre,J.Dodge,A.Fang,Y.Yu,L.Schmidt,W.Y.Wang,and
Y.Choi(2023). Multimodalc4: Anopen,billion-scalecorpusofimagesinterleavedwithtext. In
Thirty-seventhConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarks
Track.
Zhu,Y.,O.Groth,M.Bernstein,andL.Fei-Fei(2016). Visual7W:GroundedQuestionAnsweringin
Images. InIEEEConferenceonComputerVisionandPatternRecognition.
19A Appendix
A.1 Furtherexperimentaldetailsoftheablations
A.1.1 Cross-attentionvs. fullyautoregressivearchitectures
We apply LoRA modules to the LLM for the fully autoregressive architecture and to the cross-
attentionmodulesandtheLLMforthecross-attentionarchitecture. InFigure4,wereporttheaverage
performancewithrespecttothenumberofsteps,thenumberofimages,aswellasthenumberoftext
tokens.Weseeanimprovementacrosstheboardwiththefullyautoregressivearchitecture.Comparing
theaveragescorewiththesedifferentaxesisessentialbecausethecross-attentionarchitecturefeedsa
singletokenperimagetothelanguagemodel,against64forthefullyautoregressivearchitecturewith
perceiverpooling. Thisimpliesthatforthesametrainingsequencelength,thenumberofimagesand
texttokensisdifferentforthetwoarchitectures. Equivalently,thesamemultimodaldocumentwill
yielddifferentsequencelengths. Eventhoughwefixthebatchsizeinthecomparison,thenumberof
texttokensandnumberofimagesgrowatdifferentpacesunderthetwoarchitectures.
Figure4:Comparisonofthecross-attentionandfullyautoregressivearchitecturesthroughthenumber
ofsteps,thenumberofimagesandthenumberoftexttokens.
A.1.2 Comparingvariousvisionbackbones
WepresentinTable10thedetailedresultsofcomparingmultiplevisionbackbones.WhileEVA-CLIP-
5BperformssimilarlytoSigLIP-SO400M,weemphasizethatithas11timesmoreparameters. We
alsonoticedinearlyexperimentsthatTextVQAisthemostsensitivebenchmarktoimageresolution,
whichaccountsfortheperformanceincrease.
VEbackbone Size Res. Avg.score VQAv2 OKVQA TextVQA COCO
CLIP-ViT-H 600M 224 57.4 52.4 41.7 28.2 107.5
EVA-CLIP-5B 4.4B 224 60.2 53.4 43.3 30.4 113.7
SigLIP-SO400M 400M 384 60.6 53.6 43.4 33.8 111.6
Table10:Detailedresultsofablationonthevisionencoderbackbone
A.1.3 Comparingvariouspoolingstrategies
Wecomparemultiplepoolingstrategies: asimplelinearlayerthattakestheflattenedsequenceof
visionhiddenstatesandprojectsitintoashortersequenceofvisualtokens,aswellasaMapping
Network(Mañasetal.,2023). Theperceiverresamplersignificantlyoutperformsthesetwooptions
(seeTable11).
Wealsoablatethenumberoflayersintheperceiverresampler,andfindnostatisticallysignificant
differenceswhenincreasingthenumberoflayers,similarlytoresultsfromXiaoetal.(2024). We
settleon3layersoutofcautiontoavoidanypotentialcapacitybottleneck.
Finally, weadda2-layermodalityprojectionMLPontopofthevisionencoderhiddenstatesto
projectthevisionhiddendimensiontothelanguagemodelhiddendimensionpriortotheperceiver
resampler. Thesechangesyieldbetterperformanceaswell(seeTable13).
20Vision-languageConnector Avg.score
LinearProjection 44.5
MappingNetwork(Mañasetal.,2023) 51.8
Perceiver 60.3
Table11:Ablationonthemodalityprojection
Num.perceiverlayers Avg.score
1 69.3
3 68.6
12 69.0
Table12:Ablationonthenumberofperceiverresamplerlayers
MLPmodalityprojection Avg.score
W/ 71.4
W/o 69.6
Table13:Ablationontheadditionofamodalityprojectionbeforetheperceiverresampler
A.1.4 AblationsonOCRdata
WehypothesizethataddingPDFdocumentshelpsthemodellearntoreadtextfromimages.InTable7,
wecomparecheckpointstrainedwithandwithoutOCRdocuments, alongwithimageresolution
increasetoensurethatthetextislegible. Wedonotobservestatisticallysignificantdifferenceswhen
evaluatingcheckpointsinzeroorfewshot. Instead,wefine-tunethecheckpointsonDocVQAfor
500stepswithalearningrateof1e−5,leadingtocheckpointsshowingmuchstrongerdifferences.
A.2 Detailsoftheinstructionfine-tuning
A.2.1 StatisticsofTheCauldron
InTable14,wepresentthestatisticsofthedatasetsincludedinTheCauldron,aswellasthetext-only
instructiondatasetsusedforthesupervisedfine-tuning. Foreachdataset, wegivethenumberof
differentimagesitcontains,thenumberofquestion-answerpairs,thetotalnumberoftokensforthe
answersinthequestion-answerpairs,andtheselectedpercentageoftokensitrepresentsinourfinal
mixtureafterupsamplingordownsampling.
Dataset #images #Q/Apairs #tokens %mixture
Generalvisualquestionanswering
VQAv2(Goyaletal.,2017) 82,772 443,757 1,595,929 5.72%
COCO-QA(Renetal.,2015) 46,287 78,736 286,982 1.47%
Visual7W(Zhuetal.,2016) 14,366 69,817 279,268 1.43%
A-OKVQA(Schwenketal.,2022) 16,539 17,056 236,492 1.21%
TallyQA(Acharyaetal.,2019) 98,680 183,986 738,254 0.57%
OK-VQA(Marinoetal.,2019) 8,998 9,009 38,853 0.40%
HatefulMemes(Kielaetal.,2020) 8,500 8,500 25,500 0.13%
VQA-RAD(Lauetal.,2018) 313 1,793 8,418 0.09%
Captioning
LNarratives(Pont-Tusetetal.,2020) 507,444 507,444 21,328,731 4.56%
Screen2Words(Wangetal.,2021) 15,730 15,743 143,103 0.37%
VSR(Liuetal.,2023) 2,157 3,354 10,062 0.21%
OCR,documentunderstanding,texttranscription
RenderedText9 999,000 999,000 27,207,774 5.57%
DocVQA(Mathewetal.,2021) 10,189 39,463 337,829 3.46%
9https://huggingface.co/datasets/wendlerc/RenderedText
21TextCaps(Sidorovetal.,2020) 21,953 21,953 389,658 2.00%
TextVQA(Singhetal.,2019) 21,953 34,602 181,918 1.86%
ST-VQA(Bitenetal.,2019) 17,247 23,121 127,846 1.31%
OCR-VQA(Mishraetal.,2019) 165,746 801,579 6,073,824 0.93%
VisualMRC(Tanakaetal.,2021) 3,027 11,988 168,828 0.86%
IAM(MartiandBunke,2002) 5,663 5,663 144,216 0.74%
InfoVQA(Mathewetal.,2022) 2,118 10,074 61,048 0.63%
Diagramimage-to-text10 300 300 22,196 0.11%
Chart/figureunderstanding
Chart2Text(ObeidandHoque,2020) 26,985 30,242 2,852,827 4.38%
DVQA(Kafleetal.,2018) 200,000 2,325,316 8,346,234 4.27%
VisText(Tangetal.,2023) 7,057 9,969 1,245,485 1.91%
ChartQA(Masryetal.,2022) 18,271 28,299 185,835 1.90%
PlotQA(Methanietal.,2020) 157,070 20,249,479 8478299.278 0.65%
FigureQA(Kahouetal.,2018) 100,000 1,327,368 3,982,104 0.61%
MapQA(Changetal.,2022) 37,417 483,416 6,470,485 0.33%
Tableunderstanding
TabMWP(Luetal.,2023) 22,729 23,059 1,948,166 2.49%
TAT-QA(Zhuetal.,2021) 2,199 13,215 283,776 2.18%
HiTab(Chengetal.,2022) 2,500 7,782 351,299 1.80%
MultiHiertt(Zhaoetal.,2022) 7,619 7,830 267,615 1.37%
FinQA(Chenetal.,2021) 5,276 6,251 242,561 0.99%
WikiSQL(Zhongetal.,2017) 74,989 86,202 9,680,673 0.99%
SQA(Iyyeretal.,2017) 8,514 34,141 1,894,824 0.97%
WTQ(PasupatandLiang,2015) 38,246 44,096 6,677,013 0.51%
Reasoning,logic,maths
GeomVerse(Kazemietal.,2024) 9,303 9,339 2,489,459 3.83%
CLEVR-Math(Lindström,2022) 70,000 788,650 3,184,656 3.26%
CLEVR(Johnsonetal.,2017) 70,000 699,989 2,396,781 1.23%
IconQA(Luetal.,2021) 27,315 29,859 112,969 1.16%
RAVEN(Zhangetal.,2019) 42,000 42,000 105,081 0.67%
Inter-GPs(Luetal.,2021) 1,451 2,101 8,404 0.17%
Textbook/academicquestions
AI2D(Kembhavietal.,2016) 3,099 9,708 38,832 0.80%
TQA(Kembhavietal.,2017) 1,496 6,501 26,004 0.53%
ScienceQA(Luetal.,2022) 4,985 6,218 24,872 0.25%
Differencesbetween2images
NLVR2(Suhretal.,2019) 50,426 86,373 259,119 1.33%
GSD(Lietal.,2023) 70,939 141,869 4,637,229 0.48%
Spotthediff(Jhamtanietal.,2018) 8,566 9,524 221,477 0.57%
Screenshottocode
WebSight(Laurençonetal.,2024) 500,000 500,000 276,743,299 0.28%
DaTikz(Belouadietal.,2024) 47,974 48,296 59,556,252 0.03%
Text-onlygeneralinstructions,mathproblems,arithmeticcalculations
OpenHermes-2.5(Teknium,2023) 0 1,006,223 248,553,747 12.73%
LIMA(Zhouetal.,2023) 0 1,052 633,867 0.81%
Dolly(Conoveretal.,2023) 0 14,972 1,329,999 0.68%
MetaMathQA(Yuetal.,2024) 0 395,000 74,328,255 3.81%
MathInstruct(Yueetal.,2024) 0 261,781 45,393,559 2.33%
10https://huggingface.co/datasets/Kamizuru00/diagram_image_to_text
22OrcaMath(Mitraetal.,2024) 0 200,031 63,780,702 1.63%
CamelAIMath(Lietal.,2023) 0 49,744 21,873,629 0.06%
AtlasMathSets11 0 17,807,579 455,411,624 3.50%
Goat(LiuandLow,2023) 0 1,746,300 167,695,693 0.86%
Table14: Thestatisticsofdatasetsusedforinstructionfine-tuning. #tokensisthetotalnumberof
tokensforeachdatasetfortheanswersonly. %mixtureisourselectedpercentageofanswertokens
foreachdatasetinthefinalmixture.
A.3 Detailsoftheevaluations
A.3.1 Evaluationsetup
Weperformallevaluationswithabatchsizeof1andgreedydecoding.
Forthemulti-choicequestionsinMMMU,MathVista,MMBench,weevaluatewiththesameprompt
usedforsimilartypesofdatasetsduringtheinstructionfine-tuning:
Question: {question}
Choices:
A.{choice_a}
B.{choice_b}
C.{choice_c}
...
Answerwiththeletter.
Fortheopen-endedquestionsinTextVQA,DocVQA,andVQAv2,weevaluatewiththeprompt:
Question: {question}
Giveaverybriefanswer.
WeusethestopwordsQuestion,User,<end_of_utterance>and<eos>tostopageneration.
A.3.2 Expandedevaluationtable
We report the expanded evaluation of Idefics2 and the comparison to other models in Table 15.
ThisincludesscoresonVQAv2(Goyaletal.,2017),whichiswidelyadoptedforevaluation. We
acknowledge,though,thatthemetricusedfortheopen-endedvisualquestionansweringbenchmarks
stronglypenalizesmodelsthatdonotgenerateinthesameformatasthegroundtruth. Forexample,
answering"large"whenthegroundtruthis"big"ormoreverbosereformulationswillbecountedas
incorrect. OurmanualqualitativeanalysisrevealsthatonbenchmarkslikeVQAv2,thegenerations
oftwomodelsdifferingby5pointswouldbebarelynoticeable. Thisproblemislessconcerningfor
otheropen-endedbenchmarkslikeTextVQAorDocVQAwhichrequirefindingatextinanimage,
makingtheexpectedanswerlesspronetoambiguity.
A.3.3 Qualitativeevaluation
WeshowinFigures5,6,and7,examplesofgenerationswithIdefics2-chatty.
A.4 Red-teaming
Inthecontextofared-teamingexercise,ourobjectiveistoevaluatethepropensityofthemodelto
generateinaccurate,biased,oroffensiveresponses. Weevaluatemorespecificallythechat-optimized
checkpoint12.
11https://huggingface.co/datasets/AtlasUnified/atlas-math-sets
12https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty
23#tokens
Model Size
perimage
7B-14Bmodels
LLaVA-NeXT 13B 2880 36.2/- 35.3 67.1 70.0 - 82.8
DeepSeek-VL 7B 576 36.6/- 36.1 64.4 73.2 49.6 -
MM1-Chat 7B 720 37.0/35.6 35.9 72.8 72.3 - 82.8
Idefics2 8B 64 43.5/37.9 51.6 70.4 76.8 67.3 80.8
Idefics2 8B 320 43.0/37.7 51.4 73.0 76.7 74.0 81.2
≥30Bmodels
Mini-Gemini-HD 34B 2880 48.0/44.9 43.3 74.1 80.6 - -
MM1-Chat 30B 720 44.7/40.3 39.4 73.5 75.1 - 83.7
LLaVA-NeXT 34B 2880 51.1/44.7 46.5 69.5 79.3 - 83.7
Proprietary
Gemini1.0Pro - - 47.9/- 45.2 74.6 - 88.1 71.2
Claude3Haiku - - 50.2/- 46.4 - - 88.8 -
Claude3Sonnet - - 53.1/- 47.9 - - 89.5 -
Gemini1.5Pro - - 58.5/- 52.1 73.5 - 86.5 73.2
Table15: PerformanceofIdefics2againststate-of-the-artVLMsacrossdifferentsizes. Theeval-
uationsaredoneinzeroshot. Idefics2with64or320tokensperimageonlydiffersbytheimage
splitting.
(Benchmark,Split,Metric): (MMMU,val/test,MMMUscore),(MathVista,testmini/test,MMMU
score),(TextVQA,val,VQAacc.),(MMBench,test,accuracy),(DocVQA,test,ANLSscore),(VQAv2,
testdev,VQAacc.).
Prompt Idefics2 output
Extract the name, email, current job, and education. {
Output a JSON format. "name": "Yann Karpathy",
"email": "yann.karpathy@gmail.com",
"current_job": "Machine Learning Engineer, Hugging Face",
"education": [
{
"year": "2012 - 2018",
"degree": "Ph.D. in Computer Science",
"university": "Stanford University"
},
{
"year": "2010 - 2012",
"degree": "Master of Science in Artificial Intelligence",
"university": "Massachusetts Institute of Technology"
},
{
"year": "2006 - 2010",
"degree": "Bachelor of Science in Computer Science",
"university": "University of Toronto"
}
]
}
Figure5: Idefics2-chattyfindstherequestedinformationintheresume,andorganizesitinJSON
format.
While the model typically refrains from responding to offensive inputs, we observe that through
repeatedtrialsorguidedinteractions,ittendstohastilyformjudgmentsinsituationsnecessitating
24
UMMM
atsiVhtaM
AQVtxeT
hcneBMM
AQVcoD 2vAQVPrompt Idefics2 output
Describe the image The image depicts a surreal scene where
three robot soldiers are standing on a street
in front of the Eiffel Tower in Paris. Each robot
soldier is equipped with a large cannon that
resembles a giant bread roll. The robots are
facing towards the right side of the image,
and they appear to be ready for action. The
sky above them is filled with clouds, and
there's a red light visible in the distance. The
overall atmosphere of the image is quite
unusual, combining elements of technology
and architecture in an unexpected way.
Figure6: Idefics2-chattydescribesanAI-generatedimage.
Prompt Idefics2 output
What happens to fish if pelicans increase?
If pelicans increase, they may
consume more fish, which could lead
to a decrease in the fish population
or an imbalance in the ecosystem.
This could potentially affect other
species that rely on fish for food,
such as seals, dolphins, and humans
who fish for consumption.
Figure7: Idefics2-chattyanswersaquestiononascientificdiagram.
nuancedcontextualunderstanding,oftenperpetuatingharmfulstereotypes. Noteworthyinstances
include:
• Speculating or passing judgments, or perpetuating historical disparities on individuals’
professions,socialstatus,orinsuranceeligibilitybasedsolelyonvisualcues(e.g.,age,attire,
gender,facialexpressions).
• Generatingcontentthatpromotesonlineharassmentoroffensivememesreinforcingharmful
associationsfromaportrait,orfromabenignimage.
• Assumingemotionalstatesormentalconditionsbasedonoutwardappearances.
• Evaluatingindividuals’attractivenesssolelybasedontheirvisualappearance.
Additionally,weidentifybehaviorsthatincreasesecurityrisksthatalreadyexist:
• SuccessfullysolvingCAPTCHAsfeaturingdistortedtextwithinimages.
• Developingphishingschemesfromscreenshotsoflegitimatewebsitestodeceiveusersinto
divulgingtheircredentials.
• Craftingstep-by-stepguidesonconstructingsmall-scaleexplosivesusingreadilyavailable
chemicalsfromcommonsupermarketsormanipulatingfirearmstodomaximumdamage.
It’simportanttonotethatthesesecurityconcernsarecurrentlylimitedbythemodel’soccasional
inabilitytoaccuratelyreadtextwithinimages.
25Weemphasizethatthemodelwouldoftenencouragetheusertoexercisecautionaboutthemodel’s
generationorflaghowproblematictheinitialquerycanbeinthefirstplace. Forinstance, when
insistentlypromptedtowritearacistcomment,themodelwouldanswerthatquerybeforepointing
out "This type of stereotyping and dehumanization has been used throughout history to justify
discriminationandoppressionagainstpeopleofcolor. Bymakinglightofsuchaseriousissue,this
memeperpetuatesharmfulstereotypesandcontributestotheongoingstruggleforracialequality
andsocialjustice.".
However,certainformulationscancircumvent(i.e. "jailbreak")thesecautionaryprompts,empha-
sizingtheneedforcriticalthinkinganddiscretionwhenengagingwiththemodel’soutputs. While
jail-breakingtextLLMsisanactiveresearcharea, jail-breakingvision-languagemodelshavere-
centlyemergedasanewchallengeasvision-languagemodelsbecomemorecapableandprominent
(Shayegani et al., 2024). The addition of the vision modality not only introduces new avenues
forinjectingmaliciouspromptsbutalsoraisesquestionsabouttheinteractionbetweenvisionand
languagevulnerabilities.
26