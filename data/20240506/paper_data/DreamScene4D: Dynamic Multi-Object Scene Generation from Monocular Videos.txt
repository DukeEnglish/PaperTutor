DreamScene4D: Dynamic Multi-Object Scene
Generation from Monocular Videos
Wen-HsuanChu†, LeiKe†, KaterinaFragkiadaki
CarnegieMellonUniversity
{wenhsuac,leik,katef}@cs.cmu.edu
https://dreamscene4d.github.io/
Abstract
ExistingVLMscantrackin-the-wild2Dvideoobjectswhilecurrentgenerative
modelsprovidepowerfulvisualpriorsforsynthesizingnovelviewsforthehighly
under-constrained2D-to-3Dobjectlifting. Buildinguponthisexcitingprogress,
wepresentDreamScene4D,thefirstapproachthatcangeneratethree-dimensional
dynamicscenesofmultipleobjectsfrommonocularin-the-wildvideoswithlarge
objectmotionacrossocclusionsandnovelviewpoints. Ourkeyinsightistodesign
a“decompose-then-recompose”schemetofactorizeboththewholevideoscene
andeachobject’s3Dmotion. Wefirstdecomposethevideoscenebyusingopen-
vocabularymasktrackersandanadaptedimagediffusionmodeltosegment,track,
andamodallycompletetheobjectsandbackgroundinthevideo. Eachobjecttrack
ismappedtoasetof3DGaussiansthatdeformandmoveinspaceandtime. We
alsofactorizetheobservedmotionintomultiplecomponentstohandlefastmotion.
Thecameramotioncanbeinferredbyre-renderingthebackgroundtomatchthe
videoframes. Fortheobjectmotion,wefirstmodeltheobject-centricdeformation
oftheobjectsbyleveragingrenderinglossesandmulti-viewgenerativepriorsinan
object-centricframe,thenoptimizeobject-centrictoworld-frametransformations
bycomparingtherenderedoutputsagainsttheperceivedpixelandopticalflow.
Finally,werecomposethebackgroundandobjectsandoptimizeforrelativeobject
scalesusingmonoculardepthpredictionguidance. Weshowextensiveresultson
thechallengingDAVIS,Kubric,andself-capturedvideos,detailsomelimitations,
and provide future directions. Besides 4D scene generation, our results show
thatDreamScene4Denablesaccurate2Dpointmotiontrackingbyprojectingthe
inferred3Dtrajectoriesto2D,whileneverexplicitlytrainedtodoso.
1 Introduction
Videosareresultofentitiesmovingandinteractingin3Dspaceandtime,projectedfromcamera
viewpoints. Generating4Dscenesfrom2Dmonocularvideoshaswideapplicabilityacrossmultiple
endtasks,suchas3Dvideoperception,orcreatingdigitalavatars,assets,andvirtualenvironments
that accurately mirror the real world for graphics or robotics [14, 35]. This task, referred to as
video-to-4D,aimstoinferthe3Dstructureandmotionofvideoobjectsindynamic3Dscenesacross
occlusionsandunseencameraviewpoints,whilepreservingthespatial-temporalconsistencyand
realitytothe2Dobservation. However,thetaskhasremainedunsolvedformanyyearsduetoits
under-constrainednature,especiallyinacasualsetupwithtime-varyinggeometryandlimited3D
supervisionfromin-the-wildvideos.
Despitetheremarkableprogressin4Dgeneration,existingworksmostlyfocusontext-to-4D[45,
2,52,24,1]orimage-to-4D[63,42,56,65]setups,whichtakeasingletextorimageasinputto
single 4D object creation. Several works [15, 12, 42, 31, 59, 60] explore the video-to-4D setup,
†Equalcontribution
4202
yaM
3
]VC.sc[
1v08220.5042:viXrat = t t = t t = t t = t t = t t = t t = t t = t
1 2 3 N 1 2 3 N
… …
Time Time
t = t t = t t = t t = t t = t t = t
1 2 N 1 2 N
Figure1: DreamScene4Dextendsvideo-to-4Dgenerationtomulti-objectvideoswithfastmotion.
WepresentrenderedimagesfromdiverseviewpointsatdifferenttimestepsusingDAVIS[36]videos
withmultipleobjectsandlargemotions.
buttheycanonlyhandleobject-centricvideoswithasingleobjectofsmallmovement. Thislimits
theirpracticalreal-worldusagewhereinputvideosdepictingreal-worldcomplexscenescontaining
multipledynamicobjectswithfastmotions,asillustratedinFigure4.
Inthispaper,weproposeDreamScene4D,thefirstvideo-to-4Dscenegenerationapproachtoproduce
realistic 4D scene representation from a complex multi-object video with large object motion or
deformation. DreamScene4Dproposesa“decompose-then-recompose”strategy. First,theobjects
andbackgroundinthevideoaretrackedandsegmented. Asocclusionsarecommoninmulti-object
videos,weadaptadiffusionmodeltotemporallyinpaintthebackgroundandpotentiallyoccluded
objecttracks,thenindependentlyliftthemto3Dusing3DGaussianrepresentation.
InDreamScene4D,the3Dmotionisfactorizedintothreecomponents: 1)cameramotion,2)object-
centric deformations, and 3) an object-centric to world frame transformation. This factorization
greatlyimprovesthestabilityofthemotionoptimizationprocessbyleveragingpowerfulpre-trained
3Ddiffusionand2Dtrackingpriors[26,48]. Thecameramotioncanbeestimatedbyre-rendering
thestaticbackgroundGaussianstomatchthevideoframes. Finally,were-composethebackground
anddynamicobjectsforjointfine-tuningandoptimizethescaleforeachliftedobjectusingpredicted
monoculardepthtoplacethemintoacommoncoordinateframe.
Weshowthe4DrenderingsatvarioustimestepsanddiverseviewpointsofDreamScene4Dusing
challengingmonocularvideosfromDAVIS[36]inFigure1. DreamScene4Dachievessignificantim-
provementscomparedtotheexistingSOTAvideo-to-4Dapproaches[42,15]onDAVIS,Kubric[13],
andourself-capturedvideos(Figures4). ToevaluatethequalityofthelearnedGaussianmotions,
wemeasurethe2Dendpointerror(EPE)oftheinferred3Dmotiontrajectoriesacrossocclusions
andrevealthatourapproachnotonlyenablespointtrackinginthevisibleviewsbutalsoempowers
accuratepointtrackinginthesynthesizednovelviews.
2 RelatedWork
Video-to-4DGeneration Recentadvancementsingenerativediffusionmodelssparkaseriesof
4Dgenerationworks[45,42],whichtypicallyusescoredistillationsampling(SDS)[37]lossby
large-scalepretraineddiffusionmodels[44]tosynthesizefull4Drepresentationsofobjectsfrom
single text [45, 2, 24, 1], image [56, 65], or a combination of both [63]. Existing video-to-4D
generationworks[42,15,12,31,59,60]mostlycombineSDSwithexisting3Dor4Drepresentations
2
tupnI
weiV
ecnerefeR
1
weiV
levoN
2
weiV
levoN
semarF
gniredneRlikeDyNeRFS[23]or4DGaussians[50], andcanonlytakesimplevideoswithsingleobjectof
smallmotionasinput. Incontrast,owingtoourproposedscenedecouplingandmotionfactorization
schemes,DreamScene4Disthefirstapproachtogeneratecomplicated4Dscenesandsynthesizetheir
arbitrarynovelviewsbytakingreal-worldvideosofmulti-objectscenes.
Video-to-4DReconstruction Dynamic3Dreconstructionisanotherrelatedresearchline,which
extendsstatic3Dreconstructiontodynamicscenes. DynamicNeRF-basedmethods[38,32,23]
extendNeRF[30]todynamicscenes,typicallyusinggridorvoxel-basedrepresentations[27,4,25],
or learning a deformation field [4, 11] that models the dynamic portions of an object or scene.
DynamicGaussianSplatting[29]extends3DGaussianSplatting[19],wherescenesarerepresented
as4DGaussiansandshowfasterconvergencethanNeRF-basedapproaches. However, these4D
scenereconstructionworks[29,50,57]typicallytakealargenumberofsynchronizedmulti-view
videodataasinputinsteadofun-posedmonocularvideoswithinsufficientparallax,whichnecessitate
precisecalibrationofmultiplecamerasandconstrainstheirpotentialreal-worldapplicability. Dif-
ferentfromtheseworks[33,50]onmostlyreconstructingthevisibleregionsofthedynamicscene,
DreamScene4Dcan360◦ synthesizenovelviewsformultipleobjectsofthescene, includingthe
occluded/invisibleregions.
VideoSceneCompletion Existingvideo-to-4Dscenegenerationworks[42,31,59,60]usually
simplifytheinputvideobyassuminganon-occludedandslow-movingobjectwhilereal-worldvideos
withmultipledynamicobjectsinevitablycontainocclusions. Thus,wealsoperformvideoamodal
completioninourworkstoinpainttheoccludedvideoobjectsorbackgroundregionsofthevideo.
Comparedtoexistingimage/videoscenecompletionworks[61,10,17,66],thevideocompletion
inDreamScene4Disnotlimitedtosmallclose-worldcategoriesandonlyservesasanintermediate
stepforthe4Dvideoscenecompletion. Instead,weadaptlarge-scalepre-traineddiffusionmodels
SD-Inpaint[44]withtemporallayersforvideoscenecompletionduetoitsrichprioronobjects.
3 Approach
To generate dynamic 4D scenes of multiple objects from a monocular video input, we propose
DreamScene4D,whichtakesGaussianSplatting[19,50]asthe4Dscenerepresentationandleverages
powerfulpre-trainedmodelstogeneralizetodiversezero-shotsettings.
Our approach follows the “decompose-then-compose” principle to handle complex multi-object
scenes. As depicted in Figure 2, given a monocular video of multiple objects, we first segment
andtrack[43,18,8]each2Dvideoobjectin-the-wildandrecovertheappearanceoftheoccluded
regionsofeachobjectandthebackgroundwithrealistictemporalconsistency(Section3.1). Next,
wedecomposethesceneintomultipleamodalobjectsanduseSDSwithdiffusionpriorstoextract
a 3D Gaussian representation of each object. We optimize the deformation of 3D Gaussians for
dynamicobjectsunder2Dconstraintsbyre-renderingthevideoframesandfactorizingthemotion
intothreecomponents(Figure3): theobject-centricmotion,aglobaltranslation,andthecamera
motion,whichgreatlyimprovesthestabilityandqualityoftheoptimization(Section3.2). Finally,
guidedbymonoculardepthpredictionofthescene,wecomposeeachindividuallyoptimizedobject
toformacomplete4Dscenerepresentationofthemonocularvideo(Section3.3).
3.1 VideoSceneDecompositionandCompletion
Wefirstadoptzero-shotmasktrackers[43,18,8]tosegmentandtrackobjectsinthemonocular
videowhenGTobjectmasksarenotprovided. Fromthemonocularvideoandassociatedobject
tracks(withpotentialocclusions),weamodallycompleteeachobjecttrackinatemporallyconsistent
mannerbeforeliftingitto3DasinFigure2.
Toachievezero-shotobjectappearancerecoveryforoccludedregionsofindividualobjecttracks,we
buildoffofSD-Inpaint,anadaptedStableDiffusionmodel[44]withrichimageinpaintingpriors,and
extendittovideosforamodalvideocompletion. WhendirectlyadoptingSD-Inpainttocompletethe
objectsinvideos,theinpaintingresultssufferfromobvioustemporalartifacts. Therefore,wepropose
twoextensionstoSD-Inpainttoovercomethisproblemwithoutfurthertrainingorfinetuning.
Spatial-TemporalSelf-Attention AcommontechniqueforextendingStableDiffusion-basedmodels
for video generation editing inflates the spatial self-attention layers to additionally attend across
frameswithoutchangingthepre-trainedweights[51,20,5,39]. Similarto[5],weinjecttokensfrom
3adjacentframesduringself-attentiontoenhanceinpaintingconsistency.Specifically,theself-attention
operationcanbedenotedas:
Q=W z , K =W [z ,z ,z ], V =W [z ,z ,z ], (1)
Q t K t−1 t t+1 V t−1 t t+1
where[·]representsconcatenation,z isthelatentrepresentationofframet,andW ,W ,andW
t Q K V
denotethe(frozen)projectionmatricesthatprojectinputstoqueries,keys,andvalues.
LatentConsistencyGuidance Whileinflatingtheself-attentionlayersallowsthediffusionmodel
toattendtoanddenoisemultipleframessimultaneously,itdoesnotensurethattheinpaintedvideo
framesaretemporallyconsistent. Tosolvethisissue,wetakeinspirationfrompreviousworksthat
perform test-time optimization while denoising for structured image editing [34] and panorama
generation[22]andexplicitlyenforcethelatentsduringdenoisingtobeconsistent.
Concretely,wefollowatwo-stepprocessforeachdenoisingstepfornoisylatentzτ atdenoising
timestepτ tolatentzτ−1. Foreachnoisylatentzτ atframet,wecomputethefullydenoisedlatent
t
z0anditscorrespondingimageIˆ directlyinonestep. Toencouragethelatentsofmultipleframesto
t t
becomesemanticallysimilar,wefreezethenetworkandonlyupdatezτ:
zˆτ =zτ −η∇ L , (2)
z c
whereηdeterminesthesizeofthegradientstepandL isasimilarityloss,i.e.,CLIPfeaturelossor
c
theSSIMbetweenpairsofIˆ. Afterthislatentoptimizationstep,wetakezˆτ andpredicttheadded
t
noiseϵˆτ usingthediffusionmodeltocomputezτ−1as:
√
zτ−1 =√ α (cid:18) zˆτ − √1−α tϵˆτ(cid:19) +(cid:112) 1−α ϵˆτ, (3)
t−1 α t−1
t
whereα isthenoisescalingfactordefinedinDDIM[46].
t
3.2 3DDynamicObjectGenerationfromVideos
Next,weaimtogenerate4Dobjectsbasedontheamodallycompleted2DvideosfromSection3.1.
Thisrequiresliftingeachvideoobjectandthebackgroundinto3Dandmodelingitsmotion.
3.2.1 Object-Centric3DLiftingfromGlobalFrame
Afterdecomposingthesceneintoindividualamodalobjecttracks,weuseGaussiansSplatting[19]
withSDStoliftthese2Dobjectsto3D.Foreachobjecttrack,insteadofdirectlyliftingthe3Dobject
ontheoriginalvideoframe,whereobjectareasaresmall,wetakethecroppedandscaledfirstframe
I˜ ofthenewobject-centricvideotooptimizethestatic3DGaussiansrepresentationwithboththe
1
photometricrenderingloss(t=1inEq.5)ontheinputreferenceviewandscoredistillationsampling
(SDS)loss[37],similartotheprocessdescribedin[48]:
(cid:34) (cid:35)
(cid:16) (cid:16) (cid:17) (cid:17)∂Iˆp
∇ Limg =E w(τ) ϵ Iˆp;τ,I˜,p −ϵ 1 , (4)
ϕ SDS τ,ϵ,p θ 1 1 ∂ϕ
wherew(τ)istheSDSweightingfunctionatdenoisingtimestepτ,ϕ(·)representstheGaussian
renderingfunction,Iˆpistherenderedimage,ϵ (·)isthepredictednoisefromadiffusionprior,andϵ
1 θ
istheaddednoise. Wetakethesuperscriptptorepresentanarbitrarycamerapose.
3.2.2 ModelingComplex3DMotionsviaMotionFactorization
To estimate the motion of the first-frame lifted 3D Gaussians {Gobj}, a straightforward solution
1
istomodeltheobjectdynamicsbyoptimizingthedeformationofthe3DGaussiansinthetotalT
world frame of the video. However, this approach falls short in videos with large object motion,
astherenderinglossyieldsinsignificantgradientsuntilthereisanoverlapbetweenthedeformed
Gaussians in the re-rendered frames and the objects in the video frames. Thus, we propose to
decomposethedynamicsintothreecomponents,andindependentlymodelthem: 1)object-centric
motion,modeledusingalearnabledeformationnetwork;and2)theobject-centrictoworldframe
transformation,representedbyasetof3Ddisplacementsvectorsandscalingfactors;and3)camera
4Decomposed and 3D Gaussians 4D Gaussians Predicted Depth 4D Scene
Completed Objects/BG Representation
Camera Motion
…
Input Frames
Object Motion
… Depth Based Composition
…
(a) Video Scene Decomposition and Obj-Centric 3D Lifting (b) 3D Motion Factorization (c) 4D Scene Composition
Figure2: MethodoverviewforDreamScene4D:(a)Wefirstdecomposeandamodallycomplete
eachobjectandthebackgroundinthevideosequenceanduseDreamGaussian[48]toobtainstatic
3DGaussianrepresentation. (b)Next,wefactorizeandoptimizethemotionofeachobjecttrack
independently,detailedinFigure3. (c)Finally,weusetheestimatedmonoculardepthtocompose
theindependentlyoptimized4DGaussiansintooneunifiedcoordinateframe.
motion,representedbyasetofcameraposechanges. Onceoptimized,thethreecomponentscanbe
composedtoformtheobjectdynamicsobservedinthevideo.
Object-Centric Motion Optimization The deformation of the 3D Gaussians includes a set of
learnableparametersforeachGaussian:1)a3Ddisplacementforeachtimestepµ =(µx ,µy ,µz ),
t t t t
2)a3Drotationforeachtimestep,representedbyaquaternionR =(qw ,qx ,qy ,qz ),and3)a
t t t t t
3Dscaleforeachtimesteps =(sx ,sy ,sz ). TheRGB(sphericalharmonics)andopacityofthe
t t t t
Gaussiansaresharedacrossalltimestepsandcopiedfromthefirst-frame3DGaussians.
Tocomputethe3Dobjectmotionintheobject-centricframes,wetakethecroppedandscaledobjects
intheindividualframesI ,forminganewsetofframesI˜r foreachobject,wheretheseobjectsare
t t
centeredandre-scaledto65%oftheinputimage. Following[42],weadoptadeformationnetwork
D (Gobj,t),whichconsistsofamulti-resolutionK-Planes[11,4]andasmallMLP.Thisdeformation
θ 0
networkpredictsthe10-Ddeformationparameters(µ ,R ,s )foreachobjectpertimestep.Wedenote
t t t
therenderedimageattimesteptunderthecameraposepasIˆp =ϕ(D (Gobj, t), p).Following[42],
t θ 0
weoptimizethedeformationnetworkusingtheRGBrenderingandSDSlossas:
Lvideo =
1 (cid:88)T (cid:13)
(cid:13)I˜
−Iˆr(cid:13) (cid:13)2
, (5)
rgb T (cid:13) t t(cid:13) 2
t=1
(cid:34) (cid:35)
(cid:16) (cid:16) (cid:17) (cid:17)∂Iˆp
∇ Lvideo =E w(τ) ϵ Iˆp;τ,I˜,p −ϵ t , (6)
ϕ SDS t,τ,ϵ,p θ t t ∂ϕ
wherew(τ)istheSDSweightingfunction,ϵ (·)isthepredictednoisefromadiffusionprior,andϵ
θ
istheaddednoise. WeusethesuperscriptrtodenotethereferencecameraposecorrespondingtoI˜,
t
andptorepresentanarbitrarycamerapose.
Since3DGaussianscanfreelymovewithintheuniformly-coloredregionwithoutpenalties,wefind
thatthetwolossesinEq.5andEq.6areofteninsufficientforcapturingaccuratemotion,especially
forregionswithnear-uniformcolors. Thus,weadditionallyintroduceaflowrenderinglossL ,
flow
whichisthemaskedL1lossbetweentherenderedopticalflowoftheGaussiansandtheflowpredicted
byanoff-the-shelfopticalflowestimator[53]. Theflowrenderinglossonlyappliestotheconfident
maskedregionsthatpassasimpleforward-backwardflowconsistencycheck.
PhysicalMotionPrior 3Dmotionintherealworldfollowsasetofphysicslaws,whichcanbeused
tofurtherconstraintheGaussiandeformations[29]. Thedynamicobjectsusuallymaintainasimilar
sizeintemporallyneighboringframes. Thus,weincorporatethescaleregularizationlossL as:
scale
T
1 (cid:88)
L = ∥s −s ∥ , (7)
scale T t+1 t 1
t=1
5
..…4D Gaussians
𝐷/(1) 𝐷/(2) … 𝐷/(𝑇) ℒ>?@A,ℒBCD?E Figure 3: 3D Motion Fac-
ℒ;F<FG torization. The 3D motion
isdecomposedinto3compo-
3D Gaussians Object-Centric Deformation 3D D Pi rf if ou rsion nents: 1)theobject-centricde-
R),𝛽)𝑇) R),𝛽,𝑇, … R-,𝛽-𝑇- SDS formation,2)thecameramo-
tion,and3)theobject-centric
ℒ𝑠𝑣 𝑑𝑖𝑑 𝑠𝑒𝑜
to-worldframetransformation.
Camera Motion
After optimization, they can
𝜟𝟏,𝒔𝟏% 𝜟𝟐,𝒔𝟐% … 𝜟𝑻,𝒔𝑻% becomposedtoformtheorig-
Render
inalobjectmotionobservedin
ℒ;𝑣 <𝑖𝑑 =𝑒𝑜
thevideo.
Object-Centric to World Frame Transformation
Input Frames
wherewepenalizelargeGaussianscalechanges. Topreservethelocalrigidityduringmotion,we
penalizechangestorelative3DdistanceandorientationbetweenneighboringGaussiansusingL :
rigid
L rigid = |K1 | (cid:88) w i,j(cid:13) (cid:13)(µ j,t−1−µ i,t−1)−R i,t−1R− i,t1(µ j,t−µ i,t)(cid:13) (cid:13) 2. (8)
i,j∈K
Attimestept,thelocalrigiditylossiscomputedbetweenthe16nearestneighborsofeachGaussian
i with 3D displacement µ and rotation R . The K-Nearest Neighbors of each Gaussian i are
i,t i,t
computedinthebeginningframeanddenotedassetK. Aweightingfactorw isappliedtothe
i,j
individuallosstermsbasedonthedistancebetweenpairsofGaussians(i,j):
(cid:16) (cid:17)
w =exp −λ ∥µ −µ ∥2 , (9)
i,j w j,1 i,1 2
where λ is a fixed hyperparameter. The loss imposes additional constraints on the Gaussian
w
deformations,whichisespeciallyhelpfulunderself-occlusionswhereotherlossesdonotprovide
muchinformation. WedisallowpruninganddensificationoftheGaussianswhenoptimizingforthe
deformationslike[42,29].
GlobalObjectMotionintheWorldFrame Afterobtainingtheobject-centricmotions,wecompute
thetransformationfromtheobject-centricframetotheinputworldframe. Thecroppingandscaling
operation(Sec3.2.1)fromtheoriginaltotheobject-centricframescanberepresentedasanaffine
warp. From this affine warp, we analytically compute a corresponding 3D displacement vector
∆ =(∆ ,∆ ,∆ )andascalingfactors′ foreachobjectineachframebysetting∆ =0that
t x,t y,t z,t t z,t
warpstheGaussiansfromtheobject-centricframetotheworldframe. Wethenadopttherendering
lossinEq.5onI insteadofcenter-croppedframesI˜ tofine-tune∆ withalowlearningrate.
t t t
Tofurtherimprovegenerationquality,itisessentialtoconsidertheperceptualparallaxdifference.This
ariseswhenalteringtheobject’spositionin3Dspacewhilemaintainingafixedcameraperspective,
resulting in subtle changes in rendered object parts. Consequently, we undertake a multi-step
optimizationapproach.Initially,weoptimizethethreemotioncomponentsindividually.Subsequently,
thesecomponentsareamalgamated,andthroughajointoptimizationprocessemployingEq.5,we
fine-tuneboththedeformationnetworkD andaffinedisplacement∆ . Thisrefinementprocess,
θ t
conductedoveralimitednumberofiterations,helpsmitigatetheparallaxeffect.
CameraMotionEstimation Toestimatethecameramotionfrommonocularvideos,weusethe
off-the-shelfalgorithms[21,49],toinitializetherelativecameraposepredictions{R ,T }between
t t
frame1andframet. However,thecameramotioncanonlybeestimateduptoanunknownscale
[58]asthereisnometricdepthusage. Therefore,wealsoestimateascalingtermβ forT usinga
t
renderingorre-projectionloss. Wefirstun-projectthebackgroundpointsusingpredicteddepthinthe
firstframe,theninitializeandoptimizethe3DGaussiansofthebackgroundusingtheunprojected
pointcloudtorefinethecameraposeestimation. FromthebackgroundGaussiansGbg and{R ,T },
t t
weperformahierarchicalgridsearchoverpossiblevaluesofβ tominimizetherenderinglossofthe
backgroundinsubsequentframes:
T
L β =(cid:88)(cid:13) (cid:13)I t−ϕ(cid:0) Gbg, R t, β tT t(cid:1)(cid:13) (cid:13) 2, (10)
t=1
6Empirically,optimizingaseparateβ perframe[3]yieldsbetterresultsbyallowingtherendererto
t
compensateforerroneouscameraposepredictions.
3.3 4DSceneCompositionwithMonocularDepthGuidance
Afterobtainingtheindividuallyoptimized4DGaussians, wecomposethemintoaunifiedworld
coordinateframetoformacoherent4Dscene. Thiscanbeachievedbyexploitingtherelativedepth
relationshipsbetweentheobjects,asdeterminingthedepthofanobjectinacommon3Dcoordinate
framealsodeterminesitsscale.
Wetakeanoff-the-shelfdepthestimator[55]andcomputethedepthofeachobjectandthebackground
inthefirstframe. Next,werandomlypickanobjectasthe“reference"objectandestimatetherelative
depthscalesbetweenthereferenceobjectandallotherobjects. TheGaussiansarescaledalongthe
cameraraysforthegivenrelativescalingfactork. The3Dpositionsµ′ andscaless′ ofthescaled
t t
Gaussiansaredescribedas:
µ′ =Cr−(Cr−µ )∗k, s′ =s ∗k (11)
t t t t
whereCr representsthepositionofthecamera. Finally,wecomposeandrenderthedepthmapofthe
referenceandscaledobject,andcomputetheaffine-invariantL1loss[55,41]betweentherendered
andpredicteddepthmap:
L =
1 H (cid:88)W(cid:13) (cid:13)dˆ∗−dˆ(cid:13)
(cid:13) , dˆ =
d i−t(d)
. (12)
depth HW (cid:13) i i(cid:13) 1 i σ(d)
i=1
Here,dˆ∗anddˆ arethescaledandshiftedversionsoftherendereddepthd∗andpredicteddepthd .
i i i i
t(d)isdefinedasthereferenceobject’smediandepthandσ(d)isdefinedasthedifferencebetween
the90%and10%quantileofthereferenceobject. Thetwodepthmapsarenormalizedseparately
usingtheirownt(d)andσ(d). Finally,weplaceandre-composetheindividualobjectsbackinto
acommoncoordinateframebyminimizingthislossforeachobjectw.r.t. tothechosenreference
object(e.g. usinggridsearchinSection3.2.2). TheseGaussianscanthenberenderedjointlytoform
ascene-level4Drepresentation.
4 Experiments
Datasets WeevaluatetheperformanceofDreamScene4Donmultiplepopularandchallengingvideo
datasets,includingDAVIS[36],Kubric[13],andsomeself-capturedvideos.Forthevideo-to-4Dtask,
weevaluateusingasubsetof30real-worldchallengingDAVIS[36]videos,consistingofmulti-object
monocular videos with various amounts of motion in the real world. We further incorporate the
labeled point trajectories in 10 videos from TAP-Vid-DAVIS [9] to evaluate the accuracy of the
learnedGaussiandeformations.Inaddition,wegenerated50multi-objectvideosfromtheKubric[13]
simulator,whichprovideschallengingscenarioswhereobjectscanbesmallorveryoff-center. To
ablateourextensionstoSD-Inpaintforvideoamodalcompletion,werandomlyselect120videos
fromYoutubeVOS[54]andgeneraterandomocclusionmasksinthevideo[47,7].
EvaluationMetrics Thequalityof4Dgenerationcanbemeasuredintwoaspects:theviewrendering
qualityofthereconstructed3Dgeometryofthescene,andtheaccuracyofthe3Dmotion. Forthe
former,wefollowpreviousworks[15,42]andreporttheCLIP[40]andLPIPS[62]scoresbetween4
novel-viewrenderedframesandthereferenceframe,andcomputeitsaveragescorepervideo. These
metricsallowustoassessthesemanticsimilaritybetweenrenderedandreferenceframes.
Theaccuracyoftheestimatedmotioncanbebenchmarkedusing2Dpointtrajectoryannotations
fromTAP-Vid-DAVISandKubricbymeasuringtheEndPointError(EPE).Thus,forKubric,we
alsoreportthemeanEPEseparatelyforfullyvisiblepointsandpointsthatundergoocclusion. For
DAVIS,wereportthemeanandmedianEPE[64],astheannotationsonlyexistforvisiblepoints.
Forvideoamodalcompletion,wefollowinpaintingworks[47,7]andreporttheoverallPSNR,the
PSNRinthemaskedregion,andperceptualscoressuchasLPIPSandTemporalConsistency(TC)[6].
Implementation Details We run our experiments on one 40GB A100 GPU. We crop and scale
theindividualobjectstoaround65%oftheimagesizeforobjectlifting. Forstatic3DGaussian
optimization,weoptimizefor1000iterationswithabatchsizeof16for5minutes. Foroptimizing
7DAVIS Self-Captured
Figure4: Videoto4DComparisons. WerendertheGaussiansatvarioustimestepsandcamera
views. WedenoteMotionFactorizationasMFandVideoSceneDecompositionasVSD.Ourmethod
producesconsistentandfaithfulrendersforfast-movingobjects,whileDreamGaussian4D[42]and
Consistent4D[15]producedistorted3Dgeometry,blurring,orbrokenartifacts.
thedynamiccomponents,weoptimizefor10timesthenumberofframeswithabatchsizeof10.
Moreimplementationdetailsareprovidedinthesupplementalfile.
4.1 Videoto4DSceneGeneration
Baselines Forvideo-to-4Dscenegeneration,weconsiderDreamGaussian4D[42]asthebaseline,
whichalsousesdynamicGaussianSplattingforits4Dscenerepresentation. Wereplacetheframes
generatedfromimage-to-videomodelsinDreamGaussian4Dwiththereferenceframesfromvideos.
AsthebaselineDreamGaussian4Dmainlyhandlessingle-objectvideos,weadditionallycompare
againstitsvariantwherewecombinedDreamGaussian4DwithVideoSceneDecomposition(VSD)
(Section3.1),thenrecomposedthoseobjectGaussians. Wealsoperformablationexperimentswhere
weremovetheflowrenderinglossandthephysics-basedmotionlosses.
8
D4tnetsisnoC
)D4na
D, iF sS sM V uao
o G/ /W W maerD(
D4naisF
)s DM u Sao
VG/
W +maerD(
D4en
)e sc
ruS
Om
(aerD
D4tnetsisnoC
)D4na
D, iF sS sM V uao
o G/ /W W maerD(
D4naisF
)s DM u Sao
VG/
W +maerD(
D4en
)e sc
ruS
Om
(aerDTable1: 4DGenerationQualityComparisons. WereporttheCLIPscore,LPIPSinKubric[13]
andDAVIS[36]. WedenotemethodswithVideoSceneDecompositionasVSDandmethodswith
MotionFactorizationasMF.
DAVIS Kubric
Method VSD MF
CLIP↑ LPIPS↓ CLIP↑ LPIPS↓
Baseline: DreamGaussian4D[42] ✗ ✗ 77.81 0.181 73.45 0.146
w/VSD ✓ ✗ 81.39 0.169 79.83 0.122
DreamScene4D(Ours) ✓ ✓ 85.09 0.152 85.53 0.112
w/oL ✓ ✓ 84.94 0.152 86.41 0.113
flow
w/oL andL ✓ ✓ 83.24 0.153 84.07 0.115
rigid scale
t
Figure5: GaussianMotionVisualizations. WevisualizetheGaussiantrajectoriesinthereference
viewcorrespondingtothevideoaswellinmultiplenovelviews. TherenderedGaussiansaresampled
independentlyforeachview. DreamScene4Dcanproduceaccuratemotionindifferentcameraposes
w/oexplicitpointtrajectorysupervision.
4DGenerationResultsonDAVIS&Kubric Wepresentthe4Dgenerationqualitycomparison
inTable1,whereourproposedVideoSceneDecomposition(VSD)andMotionFactorization(MF)
schemesgreatlyimprovetheCLIPandLPIPSscorecomparedtotheinputreferenceimages. Wealso
showqualitativecomparisonsof4Dgenerationonmulti-objectvideosandvideoswithlargemotion
inFigure4,wherebothvariantsofDreamGaussian4D[42]andConsistent4D[15]tendtoproduce
distorted3Dgeometry,faultymotion,orbrokenartifactsofobjects. Thishighlightstheapplicability
ofDreamScene4Dtohandlemorecomplexvideoscapturedintherealworld.
4DGenerationResultsonSelf-CapturedVideos Wealsocapturedsomemonocularvideoswith
fastobjectmotionusingasmartphonetotesttherobustnessofDreamScene4D,whereobjectscanbe
off-centerandaresubjecttomotionblur. Wepresentqualitativeresultsoftherendered4DGaussians
intherighthalfofFigure4. Evenundermorecasualvideocapturingsettingswithlargemotionblur,
DreamScene4Dcanstillprovidetemporallyconsistent4Dscenegenerationresults. Atthesametime,
thebaselinesgenerateblurryresultsorcontainbrokenartifactsoftheobjects.
4.2 4DGaussianMotionAccuracy
Baselines Toevaluatetheaccuracyofthe4DGaussianmotion,weconsiderDreamGaussian4D[42]
as the baseline, since extracting motion from NeRF-based methods [15] is highly non-trivial. In
addition, we compare against PIPS++ [64] and CoTracker [16], two fully-supervised methods
explicitlytrainedforpoint-tracking,servingasupperboundsforperformance.
4DMotionAccuracyinVideoReferenceViews InTable2,wetabulatethemotionaccuracycom-
parison,whereDreamScene4DachievessignificantlylowerEPEthanthebaselineDreamGaussian4D
onboththeDAVISandKubricdatasets. Wenotedthatconventionalbaselinesoftenfailwhenobjects
9
weiV
ecnerefeR
1
weiV
levoN
2
weiV
levoNt
Figure 6: Motion Comparisons. The 2D projected motion of Gaussians accurately aligns with
dynamichumanmotiontrajectoryinthevideo,wherethepointtrajectoriesestimatedbyPIPS++[64]
tendtoget“stuck"inthebackgroundwall. ForCoTracker[16],partialpointtrajectoriesaremixed
up,wheresomepointsinthechestregion(yellow/green)endingupintheheadarea(red).
Table2: GaussianMotionAccuracy. WereporttheEPEinKubric[13]andDAVIS[36,9]. We
denotemethodswithourVideoSceneDecompositionincolumnVSDandmethodswith3DMotion
FactorizationincolumnMF.NotethatCoTrackeristrainedonKubric.
DAVIS Kubric
Method VSD MF
EPE(vis)↓ EPE(occ)↓ MeanEPE↓ MedianEPE↓
(a)Nottrainedonpointtrackingdata
Baseline:DreamGaussian4D[42] ✗ ✗ 26.65 6.98 101.79 120.95
w/VSD ✓ ✗ 20.95 6.72 85.27 92.42
DreamScene4D(Ours) ✓ ✓ 8.56 4.24 14.30 18.31
w/oL ✓ ✓ 10.91 3.83 18.54 24.51
flow
w/oL andL ✓ ✓ 10.29 4.78 16.21 22.29
rigid scale
(b)Trainedonpointtrackingdata
PIPS++[64] - - 19.61 5.36 16.72 29.65
CoTracker[16] - - 7.20 2.08 2.51 6.75
are positioned near the edges of the video frame or undergo large motion. In contrast, thanks to
motionfactorization,DreamScene4Dremainslargelyrobustundersuchconditions,asweperform
the deformation optimizations in an object-centric frame instead of directly in the global frame.
Interestingly,themotionaccuracyofDreamScene4DoutperformsPIPS++[64],despiteneverbeing
trainedforpointtracking. ThisisduetothestrongobjectpriorsofDreamScene4D,astheGaussians
adheretoremainingonthesameobjectitgeneratesandtheirmotionisoftenstronglycorrelated. In
contrast,thetrackinginPIPS++isunconstrained,wherethemotionofthepointsgets“stuck"on
otherobjectsorthebackgroundwhenlargemovementsoccur,asshowninFigure6.
4DMotionResultsonGeneratedNovelViews Anadvantageofrepresentingthesceneusing4D
Gaussiansisbeingabletoobtainmotiontrajectoriesforarbitrarycameraposes,whichwevisualize
inFigure5. DreamScene4Dcanbothgeneratea4Dscenewithconsistentappearanceacrossviews
andproducetemporallycoherentmotiontrajectoriesinnovelviewsfrommonocularinputs.
10
++SPIP
rekcarToC
sruO4.3 VideoAmodalCompletion
WecompareagainstRepaint[28]andSD-Inpaint[44]forvideoamodalcompletion. Bothbaseline
methodsarebasedonStableDiffusion. Repaintaltersthereversediffusioniterationsbysampling
theunmaskedregionsoftheimage. SD-Inpaint,ontheotherhand,finetunesStableDiffusionfor
free-forminpainting. Wealsoablatetheperformanceofourproposedamodalcompletionapproach
withouttheinflatedspatiotemporalself-attention(denotedasSTSA)andconsistencyguidancein
Sec.3.1. WesummarizetheresultsinTable3andshowqualitativesamplesinthesupplementalfile.
DreamScene4Dachievesmoreconsistentandaccuratevideocompletionthantheimageinpainting
approaches, by leveraging temporal information during the denoising process. Note that these
techniquescomplementothervideocompletionapproachessinceDreamScene4Dmainlyfocuseson
video-to-4Dscenegeneration.
Table 3: Video Amodal Completion Evaluations. We report the PSNR, LPIPS, and Temporal
Consistency(TC)measuredusingCLIPsimilarityinrandomlymaskedYoutubeVOS[54]videos.
PSNR↑
Method PSNR↑ LPIPS↓ TC↑
(masked)
Repaint[28] 20.76 14.04 0.23 91.18
SD-Inpaint[44] 21.07 14.35 0.23 91.72
DreamScene4D(Ours) 22.27 16.09 0.22 93.40
w/oSTSA 21.56 15.31 0.23 92.58
w/oGuidance 21.71 15.20 0.23 92.91
4.4 Limitations
Despitetheexcitingprogressandresultspresentedinthepaper,severallimitationsstillexist: (1)
The 3D diffusion prior fails to generalize to videos captured from a camera with steep elevation
angles. (2)Scenecompositionmayfallintolocalsuboptimasiftherendereddepthofthelifted3D
objectsisnotwellalignedwiththeestimateddepth. (3)Despitetheinpainting,theGaussiansarestill
under-constrainedwhenheavyocclusionshappen,andartifactsmayoccur. Wehopetoimproveour
proposedapproachforthesecasesinthefuture.
5 Conclusion
We propose DreamScene4D, the first video-to-4D scene generation work to take complex multi-
objectvideosasinputandgeneratedynamic3Dscenesacrossocclusions,largeobjectmotions,and
unseenviewpointswithbothtemporalandspatialconsistency. Thisisachievedbydecomposingthe
videosceneintothebackgroundandindividualobjecttrajectories,alongwithamotionfactorization
scheme to handle fast-moving objects in multi-object scenes. We present arbitrary novel view
generationresultsofDreamScene4DonpopularvideodatasetslikeDAVIS,Kubric,andchallenging
self-capturedvideos. Inaddition,DreamScene4DnotonlyachievesaccurateGaussianmotioninthe
visiblereferenceviewbutalsoenablesrobustmotiontrackinginthesynthesizednovelviews.
AcknowledgmentsandDisclosureofFunding
ThisresearchwassupportedbyToyotaResearchInstitute.
11References
[1] Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu,
JeongJoonPark,SergeyTulyakov,GordonWetzstein,etal. Tc4d: Trajectory-conditionedtext-to-4d
generation. arXivpreprintarXiv:2403.17920,2024.
[2] SherwinBahmani,IvanSkorokhodov,VictorRong,GordonWetzstein,LeonidasGuibas,PeterWonka,
SergeyTulyakov,JeongJoonPark,AndreaTagliasacchi,andDavidBLindell.4d-fy:Text-to-4dgeneration
usinghybridscoredistillationsampling. arXivpreprintarXiv:2311.17984,2023.
[3] WenjingBian,ZiruiWang,KejieLi,Jia-WangBian,andVictorAdrianPrisacariu. Nope-nerf:Optimising
neuralradiancefieldwithnoposeprior. InCVPR,2023.
[4] AngCaoandJustinJohnson. Hexplane:Afastrepresentationfordynamicscenes. InCVPR,2023.
[5] DuyguCeylan,Chun-HaoPHuang,andNiloyJMitra. Pix2video:Videoeditingusingimagediffusion. In
ICCV,2023.
[6] WenhaoChai,XunGuo,GaoangWang,andYanLu. Stablevideo:Text-drivenconsistency-awarediffusion
videoediting. InICCV,2023.
[7] Ya-LiangChang,ZheYuLiu,Kuan-YingLee,andWinstonHsu. Learnablegatedtemporalshiftmodule
fordeepvideoinpainting. BMVC,2019.
[8] HoKeiChengandAlexanderGSchwing. Xmem:Long-termvideoobjectsegmentationwithanatkinson-
shiffrinmemorymodel. InECCV,2022.
[9] CarlDoersch,AnkushGupta,LarisaMarkeeva,AdriaRecasens,LucasSmaira,YusufAytar,JoaoCarreira,
AndrewZisserman,andYiYang. Tap-vid:Abenchmarkfortrackinganypointinavideo. InNeurIPS,
2022.
[10] KianaEhsani,RoozbehMottaghi,andAliFarhadi. Segan:Segmentingandgeneratingtheinvisible. In
CVPR,2018.
[11] SaraFridovich-Keil,GiacomoMeanti,FrederikRahbækWarburg,BenjaminRecht,andAngjooKanazawa.
K-planes:Explicitradiancefieldsinspace,time,andappearance. InCVPR,2023.
[12] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, and
UlrichNeumann. Gaussianflow: Splattinggaussiandynamicsfor4dcontentcreation. arXivpreprint
arXiv:2403.12365,2024.
[13] KlausGreff,FrancoisBelletti,LucasBeyer,CarlDoersch,YilunDu,DanielDuckworth,DavidJFleet,
DanGnanapragasam,FlorianGolemo,CharlesHerrmann,etal. Kubric:Ascalabledatasetgenerator. In
CVPR,2022.
[14] EricHeiden,ZiangLiu,VibhavVineet,ErwinCoumans,andGauravSSukhatme. Inferringarticulated
rigidbodydynamicsfromrgbdvideo. InIROS,2022.
[15] YanqinJiang,LiZhang,JinGao,WeiminHu,andYaoYao.Consistent4d:Consistent360{\deg}dynamic
objectgenerationfrommonocularvideo. arXivpreprintarXiv:2311.02848,2023.
[16] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian
Rupprecht. Cotracker:Itisbettertotracktogether. arXivpreprintarXiv:2307.07635,2023.
[17] LeiKe,Yu-WingTai,andChi-KeungTang. Occlusion-awarevideoobjectinpainting. InICCV,2021.
[18] LeiKe,MingqiaoYe,MartinDanelljan,YifanLiu,Yu-WingTai,Chi-KeungTang,andFisherYu.Segment
anythinginhighquality. InNeurIPS,2023.
[19] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussiansplattingfor
real-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4):1–14,2023.
[20] LevonKhachatryan,AndranikMovsisyan,VahramTadevosyan,RobertoHenschel,ZhangyangWang,
ShantNavasardyan,andHumphreyShi. Text2video-zero:Text-to-imagediffusionmodelsarezero-shot
videogenerators. InICCV,2023.
[21] JohannesKopf,XuejianRong,andJia-BinHuang. Robustconsistentvideodepthestimation. InCVPR,
2021.
[22] YuseungLee, KunhoKim, HyunjinKim, andMinhyukSung. Syncdiffusion: Coherentmontagevia
synchronizedjointdiffusions. NeurIPS,2023.
[23] TianyeLi,MiraSlavcheva,MichaelZollhoefer,SimonGreen,ChristophLassner,ChangilKim,Tanner
Schmidt,StevenLovegrove,MichaelGoesele,RichardNewcombe,etal. Neural3dvideosynthesisfrom
multi-viewvideo. InCVPR,2022.
[24] HuanLing,SeungWookKim,AntonioTorralba,SanjaFidler,andKarstenKreis. Alignyourgaussians:
Text-to-4dwithdynamic3dgaussiansandcomposeddiffusionmodels. arXivpreprintarXiv:2312.13763,
2023.
[25] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,andChristianTheobalt. Neuralsparsevoxelfields.
AdvancesinNeuralInformationProcessingSystems,33:15651–15663,2020.
[26] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.
Zero-1-to-3:Zero-shotoneimageto3dobject. InICCV,2023.
[27] StephenLombardi,TomasSimon,JasonSaragih,GabrielSchwartz,AndreasLehrmann,andYaserSheikh.
Neuralvolumes:Learningdynamicrenderablevolumesfromimages. arXivpreprintarXiv:1906.07751,
2019.
12[28] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.
Repaint:Inpaintingusingdenoisingdiffusionprobabilisticmodels. InCVPR,2022.
[29] JonathonLuiten,GeorgiosKopanas,BastianLeibe,andDevaRamanan. Dynamic3dgaussians:Tracking
bypersistentdynamicviewsynthesis. In3DV,2024.
[30] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,2020.
[31] ZijiePan,ZeyuYang,XiatianZhu,andLiZhang. Fastdynamic3dobjectgenerationfromasingle-view
video. arXivpreprintarXiv2401.08742,2024.
[32] KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman,StevenMSeitz,and
RicardoMartin-Brualla. Nerfies:Deformableneuralradiancefields. InICCV,2021.
[33] KeunhongPark,UtkarshSinha,PeterHedman,JonathanT.Barron,SofienBouaziz,DanBGoldman,
Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional representation for
topologicallyvaryingneuralradiancefields. ACMTrans.Graph.,40(6),dec2021.
[34] GauravParmar,KrishnaKumarSingh,RichardZhang,YijunLi,JingwanLu,andJun-YanZhu. Zero-shot
image-to-imagetranslation. InSIGGRAPH,2023.
[35] XueBinPeng,AngjooKanazawa,JitendraMalik,PieterAbbeel,andSergeyLevine. Sfv:Reinforcement
learningofphysicalskillsfromvideos. ACMTransactionsOnGraphics(TOG),37(6):1–14,2018.
[36] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc
VanGool. The2017davischallengeonvideoobjectsegmentation. arXivpreprintarXiv:1704.00675,
2017.
[37] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
ICLR,2023.
[38] AlbertPumarola,EnricCorona,GerardPons-Moll,andFrancescMoreno-Noguer.D-nerf:Neuralradiance
fieldsfordynamicscenes. InCVPR,2021.
[39] ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,YingShan,andQifengChen.
Fatezero:Fusingattentionsforzero-shottext-basedvideoediting. InICCV,2023.
[40] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021.
[41] RenéRanftl, KatrinLasinger, DavidHafner, KonradSchindler, andVladlenKoltun. Towardsrobust
monoculardepthestimation:Mixingdatasetsforzero-shotcross-datasettransfer. TPAMI,44(3),2022.
[42] JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao,GangZeng,andZiweiLiu.Dreamgaussian4d:
Generative4dgaussiansplatting. arXivpreprintarXiv:2312.17142,2023.
[43] TianheRen,ShilongLiu,AilingZeng,JingLin,KunchangLi,HeCao,JiayuChen,XinyuHuang,Yukang
Chen,FengYan,etal. Groundedsam: Assemblingopen-worldmodelsfordiversevisualtasks. arXiv
preprintarXiv:2401.14159,2024.
[44] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[45] UrielSinger,ShellySheynin,AdamPolyak,OronAshual,IuriiMakarov,FilipposKokkinos,Naman
Goyal,AndreaVedaldi,DeviParikh,JustinJohnson,etal. Text-to-4ddynamicscenegeneration. arXiv
preprintarXiv:2301.11280,2023.
[46] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. ICLR,2021.
[47] RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,ArseniiAshukha,Aleksei
Silvestrov,NaejinKong,HarshithGoka,KiwoongPark,andVictorLempitsky. Resolution-robustlarge
maskinpaintingwithfourierconvolutions. InWACV,2022.
[48] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian:Generativegaussian
splattingforefficient3dcontentcreation. ICLR,2024.
[49] ShuzheWang,VincentLeroy,YohannCabon,BorisChidlovskii,andJeromeRevaud. Dust3r:Geometric
3dvisionmadeeasy. InCVPR,2024.
[50] GuanjunWu,TaoranYi,JieminFang,LingxiXie,XiaopengZhang,WeiWei,WenyuLiu,QiTian,and
XinggangWang. 4dgaussiansplattingforreal-timedynamicscenerendering. InCVPR,2024.
[51] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,WynneHsu,Ying
Shan,XiaohuQie,andMikeZhengShou. Tune-a-video:One-shottuningofimagediffusionmodelsfor
text-to-videogeneration. InICCV,2023.
[52] Dejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,
and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint
arXiv:2403.16993,2024.
[53] HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow:Learningopticalflow
viaglobalmatching. InCVPR,2022.
[54] NingXu,LinjieYang,YuchenFan,JianchaoYang,DingchengYue,YuchenLiang,BrianPrice,Scott
Cohen,andThomasHuang. Youtube-vos:Sequence-to-sequencevideoobjectsegmentation. InECCV,
2018.
13[55] LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao.Depthanything:
Unleashingthepoweroflarge-scaleunlabeleddata. InCVPR,2024.
[56] QitongYang, MingtaoFeng, ZijieWu, ShijieSun, WeishengDong, YaonanWang, andAjmalMian.
Beyond skeletons: Integrative latent mapping for coherent 4d sequence generation. arXiv preprint
arXiv:2403.13238,2024.
[57] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d
gaussiansforhigh-fidelitymonoculardynamicscenereconstruction. arXivpreprintarXiv:2309.13101,
2023.
[58] VickieYe,GeorgiosPavlakos,JitendraMalik,andAngjooKanazawa. Decouplinghumanandcamera
motionfromvideosinthewild. InCVPR,2023.
[59] YuyangYin,DejiaXu,ZhangyangWang,YaoZhao,andYunchaoWei. 4dgen: Grounded4dcontent
generationwithspatial-temporalconsistency. arXivpreprintarXiv:2312.17225,2023.
[60] YifeiZeng,YanqinJiang,SiyuZhu,YuanxunLu,YoutianLin,HaoZhu,WeimingHu,XunCao,andYao
Yao. Stag4d:Spatial-temporalanchoredgenerative4dgaussians. arXivpreprintarXiv:2403.14939,2024.
[61] XiaohangZhan,XingangPan,BoDai,ZiweiLiu,DahuaLin,andChenChangeLoy. Self-supervised
scenede-occlusion. InCVPR,2020.
[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
[63] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124:
Animatingoneimageto4ddynamicscene. arXivpreprintarXiv:2311.14603,2023.
[64] YangZheng,AdamWHarley,BokuiShen,GordonWetzstein,andLeonidasJGuibas. Pointodyssey:A
large-scalesyntheticdatasetforlong-termpointtracking. InICCV,2023.
[65] YufengZheng,XuetingLi,KokiNagano,SifeiLiu,OtmarHilliges,andShaliniDeMello. Aunified
approachfortext-andimage-guided4dscenegeneration. arXivpreprintarXiv:2311.16854,2023.
[66] ShangchenZhou,ChongyiLi,KelvinCKChan,andChenChangeLoy.Propainter:Improvingpropagation
andtransformerforvideoinpainting. InICCV,2023.
14