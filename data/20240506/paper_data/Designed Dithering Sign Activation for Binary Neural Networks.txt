1
Designed Dithering Sign Activation
for Binary Neural Networks
Brayan Monroy∗, Juan Estupin˜an∗, Tatiana Gelvez-Barrera, Jorge Bacca, and Henry Arguello
Department of Computer Science, Universidad Industrial de Santander
Bucaramanga, 680002, Colombia
Abstract—BinaryNeuralNetworksemergedasacost-effective
and energy-efficient solution for computer vision tasks by bina-
rizing either network weights or activations. However, common
binaryactivations,suchastheSignactivationfunction,abruptly
binarize the values with a single threshold, losing fine-grained
details in the feature outputs. This work proposes an activation
that applies multiple thresholds following dithering principles,
shifting the Sign activation function for each pixel according to
a spatially periodic threshold kernel. Unlike literature methods,
the shifting is defined jointly for a set of adjacent pixels,
taking advantage of spatial correlations. Experiments over the
classification task demonstrate the effectiveness of the designed
ditheringSignactivationfunctionasanalternativeactivationfor
binary neural networks, without increasing the computational
cost. Further, DeSign balances the preservation of details with Fig.1. IllustrationoftheoutputwhenapplyingtheReLU,Sign,andproposed
the efficiency of binary operations. DeSign activations to a reference image. (Top) Generated activation maps.
(Bottom)Zoomofaspecificoutputpatch.AlthoughSignandDesignoutputs
Index Terms—Binary Neural Networks, Binary activations, areentirelybinary,Designoffersabetterrepresentationofthestructureand
Quantization, Dithering, Classification tasks. preservationoffine-graineddetailswithintheimage.
I. INTRODUCTION
non-binary parameters, [12] quantizes the ReLU to a fixed
Deep Neural Networks (DNNs) connote the state-of-the-
amount of discrete levels, and [13], [14] learn the shifting
art for most computer vision tasks, such as detection [1],
in the ReLU and Sign activations to minimize the bits of
classification [2], or segmentation [3]. DNNs usually operate
representation and preserve the precision.
over hundreds to millions of real-valued (32-bit or 16-bit)
Besides affecting the performance, the loss of precision
parameters, demanding expensive computational and storage
causesthelossoffine-graineddetailsintheoutput.Thisissue
resources [4]. Binary neural networks (BNNs) connote an
is a recurrent phenomenon in binary image representation,
alternative that applies binarization strategies over the archi-
mitigated through dithering strategies that adjust the density
tecture parameters, including weights [5], activations [6], or
of binary values in the output image to closely approximate
both [7] to handle the complexity.
the average gray-level distribution of the original image [15],
A BNN employs binary values (1-bit) to perform most
[16]. Dithering introduces controlled variations in the form
arithmetic operations as logical operations, significantly ac-
of noise or designed patterns following different schemes.
celerating the running time (up to 52×) and compressing the
Fixed dithering compares all pixels to a uniform threshold,
memory usage (up to 32×) compared to DNNs. Nonetheless,
ensuring consistent thresholding across the entire image [17].
the binarization produces a loss of precision because of the
Random dithering compares the pixels to randomly generated
quantization error inherent in using binary values [8] and the
thresholds, avoiding homogeneous areas [18]. Iterative dither-
gradient mismatch of using binary-valued activations [6].
ing utilizes error diffusion and applies to dither iteratively
Recent works have analyzed and adapted the statistical
overpixelsandtheirneighbors[19].Orderedditheringutilizes
behavior of weights and activations to use binary weights
fixed dither matrices, such as the halftone or Bayer, taking
while bridging the performance gap between BNNs and
advantage of spatial information to produce specific dithering
DNNs. For instance, [8] introduces the Rectified Clamp Unit
effects [20].
(ReCu) to solve the dead weights problem, where weights
Inspiredbytheapproximationofreal-valuedactivationsus-
that reach a stationary state are not updated. Further, the
ingtheSignfunctionandtheditheringprocessinbinaryimage
real-valued Rectified Linear Unit function (ReLU) has been
representation, this work proposes a designed dithering Sign
approximatedusingstrategiesbasedonthebinary-valuedSign
(DeSign) activation for BNNs. DeSign incorporates periodic
function [9]. To name, [10], [11] modify the forward and
spatial shifts to encode the most relevant information from
backward steps of the Sign activation by introducing learned
binaryconvolutions.Unlikepreviousmethodsthatquantizethe
∗Theseareco-firstauthorswithequalcontributions. ReLU to low-bit precision levels or learn shifting parameters
4202
yaM
3
]VC.sc[
1v02220.5042:viXra2
along the features independently, DeSign employs a threshold ⊛ : Z → Z denote the convolution operator convolving a
2
kernel whose values are designed jointly for a set of adjacent binary kernel with a binary matrix. The range of this operator
pixelstaking advantageof localspatialcorrelations. Then,the depends on the kernel size k and can be defined as
kernel is repeated periodically across the spatial dimension. range(⊛)=(cid:8) i∈Z(cid:12) (cid:12)i=−k2+2ℓ, ∀ℓ∈Z∩[0,k2](cid:9) . (2)
The design methodology comprises two steps (i) the optimal
threshold kernel is selected as the one that maximizes an For instance, for k =3 the entry values of X will belong to
c
objective function that measures the ability to preserve struc- the set {−9+2ℓ:ℓ∈Z∩[0 9]}.
tural information. (ii) the entries are re-scaled to match the For ease of notation, this forward process is presented as a
distribution of operations in the BNN forward propagation. 2D process, however, it should be repeated across all features
Figure 1 illustrates the effect of applying the ReLU, Sign, of the input matrix.
and DeSign activations to a real-valued reference image. As
expected, the ReLU preserves most of the structure by retain- B. Batch-normalization Layer
ing real values. Conversely, Sign activation diminishes most
The convolution operation in the binary layer produces
structuraldetailsbymappingvaluestoonlytwolevels.DeSign
integer values even when the weights and activations are
lies between ReLU and Sign, with superior preservation of
binary. Hence, the batch-normalization layer is used between
structural details compared to Sign while mapping only to
the convolution and activation layers to prevent unstable data
two levels, as shown in the zoomed window.
flowing [21]. The batch normalization standardizes the output
Simulations over CIFAR-10, CIFAR-100, and STL-10 clas-
of the previous layer as follows
sification datasets and two state-of-the-art BNN architectures
X −µ
validate the effectiveness of DeSign to boost overall BNN X = √c ·γ+β. (3)
s
accuracy while preserving binary operations without adding σ+ϵ
significantcomputationalload.DeSignalsomitigatestheinflu- In (3) the convolved image X is standardized to a normal
c
enceofreal-valuedlearnedlayers,suchasbatchnormalization, distribution N(0,1) given the moving average and variance
enhancing baseline BNNs accuracy in up to 4.51%. The main estimation µ,σ. Then, the trainable parameters γ,β re-scale
contributions of this work can be summarized as follows the distribution of the normalized input to a Gaussian distri-
1) DeSign, a binary-designed dithering activation based bution, such that X s ∼ N(β,γ). Remark that there is one
on a spatially periodic threshold kernel that shifts the different γ and β operating along the features of the input
Signactivationtopreservethestructureandfine-grained X c, namely each feature of the normalized input is re-scaled
details, described in Section III. to a different mean and deviation Gaussian distribution.
2) An optimization-based methodology to design the
threshold kernel taking advantage of spatial correlations C. Activation Layer
described in Section IV.
BNNs can employ Real-valued or Binary-valued activa-
3) Aperformanceimprovementforclassificationtaskcom-
tions. The Real-valued activation offers superior performance
pared to literature binary approaches and a drawing
but forfeits the ability to harness 1-bit operations inherent
up to real-valued networks performance, described in
to BNNs. Conversely, Binary-valued activation preserves the
Section V.
utilization of 1-bit operations but substantially decreases the
performance. Subsequent subsections explore the most used
II. BINARYNEURALNETWORKSBACKGROUND
activations within the context of BNNs.
The BNN forward propagation involves three layers: (i) A 1) Real-valued Activation: The Rectified Linear Unit
binary convolution layer convolving a binary input with a ReLU(·) : R → R+ is a real-valued activation that maintains
0
binarykernel,(ii)abatchnormalizationlayerthatstandardizes equal positive values and transforms negative ones to zero as
the information range, and (iii) an activation layer that maps follows
the output values to a different space. The activation can be ReLU(x)=max(0,x). (4)
real-valuedtomitigatethelossofinformationorbinary-valued
TherangeoftheReLUcorrespondstoallnon-negativereal-
to obtain a full BNN.
values so that the range of applying the ReLU at the output
of the convolution operator ⊛ in (1) can be expressed as
A. Binary Convolution Layer
The binary convolution layer performs bit-wise operations
Ω=(cid:26)
i∈Z+
(cid:12)
(cid:12) (cid:12)i=−k2+2ℓ,
∀ℓ∈Z∩(cid:20) k2 k2(cid:21)(cid:27)
. (5)
actingasthelogicgateXNOR[10].LetK∈Zk×k denotethe 0 (cid:12) 2
2
binarykernelofsizek×k,X∈Zh×w denoteabinarymatrix 2) Binary-valued Activation: The binary-valued activation
2
of size h×w, and Z denote a binary set with two elements. ishabituallydividedintoonefunctionfortheforwardpassand
2
Inthispaper,thebinarysetisselectedasZ ={−1,1}.Then, one for the backward pass to avoid quantization drawbacks
2
the binary convolution is given by such as discontinuities or vanishing gradients. The Sign acti-
vation Sign(·) : R → Z is a piece-wise function commonly
X =K⊛X, (1) 2
c usedintheforwardpassthatreturnsthesignoftheinputgiven
where ⊛ denotes the 2D convolution operator and X c ∈ by x
Zh−k+1×w−k+1 denotestheconvolvedoutput.Inthismanner, Sign(x)= . (6)
|x|3
Fig.2. BinaryforwardpropagationschemewiththeproposedDeSignactivation.(a)TheinputX∈Zh×w isconvolvedwithbinarykernelsK∈Zk×k.
2 2
(b)TheoutputXc isbatch-normalizedusingthetrainableparametersγ andβthroughthefeatures.(c)Thebatch-normalizedoutputXs ispassedtroughthe
DeSignactivation.Precisely,thethresholdkernelTisincorporatedinthethirdlayer,throughtheoperationXs−(T⊗1)toimposeaditheringstructure
thathelpsinthepreservationofinformation.Then,theconventionalSignactivationisapplied,obtainingthebinaryoutputX b∈Z 2h−k+1×w−k+1.
The constant behavior of the Sign function in (6) produces a approach, such as the methodology outlined in this paper. In
zeroderivativethroughoutitsdomain.Then,theClipfunction practice, the design methodology encompasses two steps. (i)
defined in (7) is used during the backward pass to approx- theselectionoftheoptimalthresholdingkernelTmaximizing
imate the Sign derivative, ensuring the network parameters an objective function that quantifies the preservation of struc-
update [22]. tural information detailed in Section IV-A, and (ii) the scaling
of the threshold entries to align with the batch-normalization
Clip(x)=max(−1,min(1,x)). (7)
process described in Section IV-B.
III. DESIGN:DESIGNEDDITHERINGSIGNACTIVATION
A. Threshold Kernel Selection
Inspired by the dithering process that adjusts the density of
binary values to approximate the average gray-level distribu- The proposed optimization methodology aims to select a
tion, this paper proposes the DeSign activation with periodic threshold kernel that preserves structural information based
spatial shifts for BNNs. DeSign aims to reduce the informa- on the behavior of BNNs forward propagation analyzed in
tion loss while maintaining 1-bit operations. Mathematically, Section II, in particular, in view of the range obtained when
DeSign(·):R→Z is defined as using the ReLU given by the set Ω in (5).
2
For preserving structural information, DeSign should be
X =DeSign(X ;T)=Sign(X −(T⊗1)), (8) able to maintain the largest possible difference between adja-
b s s
centpixelswithinthekernelwindowacrossallassessedspatial
whereT∈Ωd×d d>1isathresholdkernel,shiftingtheSign
windows encompassing the input image. Consequently, we
activation for each pixel in the input before the binarization,
propose maximizing the expected total variation in the output
⊗ denotes the Kronecker product and 1 denotes a one matrix
resulting from applying the ReLU to the DeSign.
fitting the input dimension X . Note that using a threshold
s Mathematically,weexpresstheoptimizationproblemtofind
kernel acting for a small spatial window of size d×d enables the optimal thresholding kernel T∗ ∈Ωd×d as follows
leveraging inherent local spatial correlations in comparison to
using an independent threshold for each location. In addition,
(cid:104)(cid:13) (cid:13) (cid:105)
the domain of the threshold kernel is constrained to the set Ω, T∗ ∈ arg max E (cid:13)ReLU(Sign(X⊛K−(T⊗1)))(cid:13) .
X (cid:13) (cid:13)
based on the definition in (5), provided that this set contains T∈Ωd×d (cid:124) (cid:123)(cid:122) (cid:125) TV
DeSign(X;T)
all possible quantization levels. (9)
ThestructureofTdeterminestheperformanceoftheBNN, ||·|| denotes the Total-Variation (TV) operator [23].
TV
so that we propose to design T in such a manner that it This paper employs a brute force strategy for solving (9)
preserves as much as possible the distribution of the values since the involved variables are all discrete and the domain
producedbybinaryconvolutionsasiftheReLUwereapplied. of the optimization variable T is constrained to the set Ω
The proposed design taking advantage of spatial correlations, with cardinality |Ω| = ⌈k2/2⌉. Let T be the set containing
is presented in Section IV. all possible threshold kernels of size d×d whose entries can
RemarkthattheproposedDeSignactivationcanbeincorpo- take the values in the set Ω; the cardinality of T is given by
rated in the forward propagation of any BNN, as schematized
|T| =
⌈k2/2⌉d2
, i.e., it is reasonable to evaluate all possible
in Fig. 2, where the spatially periodic threshold kernel T
thresholds kernels for small values of k and d.
is included between the batch normalization layer and the
ThecomputationoftheTVscoreforeachparticularthresh-
conventional Sign activation layer.
oldT∈T isdonebysimulatingmultiplebinaryconvolutions
as described in section II-A, using random binary kernels K
IV. THRESHOLDKERNELDESIGN for all images X that belong to a given dataset.
The threshold kernel T can be defined in one of several Once all TV scores have been computed for each kernel,
ways: it can be established ad-hoc, drawn from existing the candidates are arranged in ascending order, based on their
literature patterns [16], or crafted through an optimization respectiveTVscores.Thequantitativefindingsforthecaseof4
Fig.3. TotalVariationscoreofallthresholdkernelcandidates.(a)OrderedTVscore,(b)top-5thresholdkernelswiththehighestTVscore,and(c)bottom-5
thresholdkernelswiththelowestTVscore.
k =3,Ω={0,1,3,5,7,9},d=2,|T|=1296,andusingthe
CIFAR-100 dataset are presented in Figure 3(a). To facilitate
the visual analysis, we focus on the top five kernel candidates
with the highest TV value, as depicted in Figure 3(b). These
topfivekernelsexhibitaBayerfilterspatialdistributiononthe
1,3,and5levels.Thefirstandsecondkernels,whichpossess
the highest TV scores, represent permutations of the T∗ =
[1,1,3,3] tile configuration. Therefore, we select such kernel
as the optimal one maximizing the expected TV according to
our methodology to be used in the subsequent simulations.
Fig.4. Distributionrangeestimation:1)WhenusingtheSign,thereareonly
threeoptions,allnumbersnegative,i.e.,[−3σ,0],allpositivei.e.,[0,3σ],or
B. Entry Scaling to Batch Normalization combined i.e., [−3σ,3σ]. 2) When using the proposed thresholds, different
ranges are possible using the reference threshold values tκ, increasing the
Including batch normalization layers into BNNs induces precisionandapproximatingthebehavioroftheReLUfunction.
the mapping of integer outputs from binary convolutions to
floating-point values. Hence, for implementation purposes,
to Sign, the binary representation provided by the DeSign
the entries of the selected threshold kernel T∗ have to be
activationkeepsthestructureofthebinaryconvolutionoutput.
adapted to align with the normalized data distribution. This
Notice for instance in the layer 1 how the DeSign activation
paper adopts the methodology outlined in [12], which utilizes
canstillpreservethestructureofelementsthatarecompletely
the Half-wave Gaussian quantization technique. Specifically,
destroyed with the Sign activation such as the ship.
the right side of the Gaussian distribution corresponds to the
behavior associated with ReLU activations is used to select
C. 3D scenario Design
N = |Ω| = ⌊k2/2⌋+1 quantization levels, matching the N
The proposed2D thresholdkernel designin Section IVcan
levels generated by the binary convolution (Section II-C1).
be extended to take advantage of inter-channel correlations
Subsequently, a K-means algorithm is employed to quan-
andpromotechannelvarietywhendealingwithimageswhere
tize the Half-wave Gaussian distribution into N clusters as
each channel represents a different modality, such as color,
illustrated in Figure 4(b). Finally, the process of mapping
texture, or class. Thus, instead of using the same threshold
the quantization levels to their corresponding real values
kernel to each channel, we present a technique that applies
involves replacing the quantization levels obtained in the
a different threshold kernel to each channel, referred to as
thresholdpatternTwiththeirleft-sidethresholdvalues.These
DeSign3D, increasing the network’s ability to capture unique
threshold values are determined based on the results obtained
characteristicsanddiverseinformationperchannel.DeSign3D
through the K-means algorithm and define the scaled real-
isbuiltuponthefoundationofthe2Ddesign,whereweutilize
valued threshold T .
s
two techniques based on the set Ω to generate a 3D threshold
Figure 4 compares the distribution range for the Sign and
as follows.
DeSign scenarios, illustrating how using DeSign improves the
preservation of structural information in comparison to Sign. • Circular shift: This technique employs a 2D threshold
andshiftsthevaluesofthesetΩtogenerateanadditional
Precisely,theSigndistributiononlyhasonequantizationlevel
channel, i.e., each channel is created as a circularly
so that it can only discriminate between two fixed groups. In
shifted version of the previous channel.
contrast, the Design distribution increases the possible ranges
by selecting different threshold values. • Complement: This technique assigns complementary
thresholds to generate an additional channel, where the
One further remark on how DeSign preserves the fine-
complement of the threshold t given by Ω[κ], is calcu-
grained details across the middle layers in the architecture is κ
lated by Ω[(N −[κ−1])].
illustrated in Fig. 5. It can be observed that in comparison5
layer 1 layer 2 layer 3 layer 4 layer 5
Fig.5. MiddleoutputsactivationsofBNNarchitectureonSTL-10dataset.(Top)BinaryConvolutionoutputs,(Middle)Signactivationsoutputs,and(Bottom)
DeSignactivationsoutputs.TheincorporationofDeSignactivationsenablesthepreservationoffinedetailsalongBNNswhioutadditionalcomputationalcost.
V. SIMULATIONSANDRESULTS method for each binary activation case in the CIFAR-10
datasetonfixedandlearnablebatch-normalizationparameters.
This section outlines the experiments conducted to validate
The evaluated cases for DeSign can be grouped into three
the efficacy of the DeSign activation. First, Section V-A
categories summarized as follows
presents a comprehensive benchmark analysis of state-of-
the-art BNNs on several classification datasets, comparing 1) DeSign 2D: The design threshold kernel is broadcasted
the performance of the 2D and 3D variants of the DeSign along the dimension of the features.
activation. The influence of batch normalization over the 2) DeSign 3D: The design threshold kernel is permuted
DeSign activation is also analyzed. Finally, a state-of-the-art and reflected to generate 3D threshold patterns. In this
comparison using various binarization strategies demonstrates method, C stands for complementary thresholds, and S
how incorporating the DeSign activation enhances the overall for the circular shift on the designed threshold kernel.
performance. The experiments aim to demonstrate the effec- 3) Learned 2D: The threshold kernel is learned as a pa-
tivenessoftheDeSignactivationinimprovingtheaccuracyof rameter of the network using the end-to-end scheme
BNNs, through a rigorous comparison methodology to ensure presented in [24].
thatthepresentedresultsareaccurateandreliable,contributing TableIpresentsthequantitativecomparisonoftheevaluated
to the development of more efficient and accurate BNNs. scenarios, presenting the metrics obtained in the last and best
epoch.Theresultsshowthatincorporatingafeature-levelbias
A. Comparison Benchmark using DeSign 3D activation significantly improves the overall
network performance compared to using only DeSign 2D.
The benchmark consists of evaluating the influence of
After conducting experiments on the CIFAR-10 dataset, we
using the proposed DeSign between the batch normalization
determined that the highest accuracy was achieved with a
and activation layers. For this, the VGGsmall and ResNet18
spatial kernel of size k = 2 and a circular shift along the
architectures were used across the BNN [7], and ReCU [8]
feature dimension to generate DeSign 3D. We selected the
binarizationstrategies,ontheclassificationtaskintheCIFAR-
DeSign3D-Sspatialpatternforthesubsequentstate-of-the-art
10, CIFAR-100, and STL-10 datasets. The performance is
comparisons based on these results. Notably, DeSign 3D acti-
evaluated using the accuracy metric.
vationreducestheinfluenceofbatch-normalizationparameters
on network accuracy, achieving a base accuracy of 90.48%
B. Selection of Design Strategy
compared to 85.97% for the baseline BNN method that uses
Multiple experiments were carried out to determine the learned batch-normalization parameters, thus demonstrating
kernel size and the possible design scenarios, i.e., 2D and 3D. the efficacy of DeSign 3D in mitigating the effects of learned
The experimental setup consists of 3 runs of the BNN [7] batch normalization parameters, remarked in the similarity
tuptuO
vnoCyraniB
ngiS
ngiSeD6
TABLEI TABLEIII
COMPARISONOFDIFFERENTDESIGN2DAND3DSTRATEGIESON PERFORMANCECOMPARISONWITHTHESTATE-OF-THE-ARTON
CIFAR-10DATASETSWITHTHEBNN[7]METHOD.KERNELSIZE CIFAR-10.W/ADENOTESTHEBITLENGTHOFTHEWEIGHTSAND
k={2,3}ONFIXEDBN(0/1)ANDLEARNEDBN(β/γ) ACTIVATIONS.FPISSHORTFORFULLPRECISION.
BATCH-NORMALIZATIONSETTINGS.
Network Method W/A Top-1
0/1 β/γ
Dither Method k FP 32/32 94.8
Last Best Last Best
RAD [25] 1/1 90.5
w/o Dither [7] - 85.83 85.97 90.41 90.70
IR-Net [26] 1/1 91.5
Random 2D 2 85.72 85.95 89.33 89.68 ResNet-18
RBNN [11] 1/1 92.2
Learned 2D 2 89.92 90.23 90.46 90.75
ReCU [8] 1/1 92.8
DeSign 2D 2 89.70 89.75 89.82 90.09
ReCU* (Proposed) 1/1 92.9
DeSign 3D-C 2 90.30 90.46 90.58 90.98
FP 32/32 94.1
DeSign 3D-S 2 90.36 90.48 90.84 91.09
XNOR-Net [10] 1/1 89.8
DeSign 2D 3 89.63 89.89 90.81 90.83
DoReFa [27] 1/1 90.2
DeSign 3D-C 3 89.97 90.29 90.97 91.07 VGG-small
IR-Net [26] 1/1 90.4
DeSign 3D-S 3 89.80 89.87 90.68 90.97
BNN [7] 1/1 90.9
BNN* (Proposed) 1/1 91.3
TABLEII
BATCHNORMALIZATIONINFLUENCEON
CIFAR-10,CIFAR-100ANDSTL-10DATASETS.
across the activations of batch-normalized neural networks,
Batch BNN[7] ReCU [8]
Dataset improving the performance.
Setting Baseline DeSign Baseline DeSign
In addition, the individual influence of learning the mean
0/1 86.44 90.87 89.81 91.02
β/1 91.15 91.34 91.00 91.54 β and deviation γ of the batch normalization with DeSign is
CIFAR-10
0/γ 86.51 90.22 89.87 92.06 evaluated. As it can be seen in Table II, the use of a learned
β/γ 90.88 91.06 92.84 92.92 meansN(β,1)increasestheobtainedperformanceevenmore,
0/1 55.54 63.13 66.01 69.57
which can be interpreted as a combination of the spatial bias
β/1 62.62 64.19 69.03 69.77
CIFAR-100 provided by DeSign and a feature bias provided by batch
0/γ 55.93 63.67 66.22 69.90
β/γ 62.23 64.30 72.36 72.47 normalization.
0/1 68.51 73.09 75.00 82.05 For the case of learning the deviation γ, it can be noticed
STL-10 β/1 67.74 73.31 78.40 82.55 a performance drop in preliminary results. Analyzing the
0/γ 68.46 73.62 75.45 84.12
relationship between the designed thresholds of the DeSign
β/γ 68.64 73.24 85.16 85.87
activationandtheinfluenceofre-scalingthevaluesofitsinput,
itbecomesevidentthat,whiletheinputoftheDeSignfunction
of the results in the column of fixed BN(0/1) and learned nowcorrespondstoadistributionN(0,γ),thethresholdswere
BN(β/γ) batch-normalization settings. Since DeSign does designed to reduce the quantization error for a distribution
not increase the amount of learnable parameters, it offers a N(0,1). Thus it is necessary to re-scale the values of the
promising solution for improving BNNs performance without thresholdstopreservethequantizationintervalsforwhichthey
adding significant computational burden. were designed. Scale correction on threshold values was used
for all the cases where the learning of the γ deviation was
employed.
C. Batch-normalization Influence Analysis
Once the re-scaling was incorporated into the thresholds,
This section examines the impact of learning the mean
there was no statistically significant increase in performance
(β) and variance (γ) parameters of the batch-normalization
for BNN [7] method, while for ReCU, there was a significant
layerontheperformanceoftheproposedmethod.Specifically,
increase in performance in all datasets.
the configuration using a fixed N(0,1) distribution with no
These findings suggest that synergy between spatial bias
learnedparametersiscomparedagainsttoaconfigurationthat
provided by DeSign activation and feature-level scale and
incorporates both learned parameters, resulting in a N(β,γ)
bias induced by batch normalization could boost even more
distribution.
binarization methods that exploit information entropy and
Table II reports the obtained results, showing that normally
standardization of weights, such as ReCU [8].
the absence of learned parameters significantly impacts the
networkperformance,i.e.,usingadistribution∼N(0,1)leads
D. Classification Performance Comparison.
to a drastic reduction in the performance compared to using
both learned parameters. Contrary, when using the proposed As DeSign aims to increase the information preserved
DeSign, it can be observed a reduction in the performance along binarization neural networks, it could be introduced by
gap between no-learned and learned batch-normalization con- substituting vanilla binarization functions such as Sign. The
figurations, for all evaluated methods and datasets. These performance of DeSign is evaluated in comparison to vanilla
resultssupporttheclaimthatDeSignpreservesthespatialbias binarization functions, such as Sign, by substituting them in7
the DNN [7] and ReCU [8] binarization methods, applied to [9] Osval Antonio Montesinos Lo´pez, Abelardo Montesinos Lo´pez, and
VGG-smallandResNet-18networkarchitectures,respectively. Jose Crossa, “Fundamentals of artificial neural networks and deep
learning,” in Multivariate Statistical Machine Learning Methods for
The quantitative comparison, presented in Table III demon-
GenomicPrediction,pp.379–425.Springer,2022.
strates a performance improvement while preserving the total [10] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali
network parameters and 1-bit operations. Farhadi, “Xnor-net: Imagenet classification using binary convolutional
neural networks,” in Computer Vision–ECCV 2016: 14th European
It is worth noting that BNN prioritizes binarized weights
Conference, Amsterdam, The Netherlands, October 11–14, 2016, Pro-
and activations, while ReCU aims to address weights that ceedings,PartIV.Springer,2016,pp.525–542.
are scarcely updated during BNN training. The incorporation [11] Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang,
YongjianWu,FeiyueHuang,andChia-WenLin,“Rotatedbinaryneural
of DeSign functions in both binarization methods produces
network,” Advancesinneuralinformationprocessingsystems,vol.33,
positive outcomes, emphasizing the adaptable and versatile pp.7474–7485,2020.
characteristics of the proposed activation. [12] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos, “Deep
learning with low precision by half-wave gaussian quantization,” in
Proceedings of the IEEE conference on computer vision and pattern
VI. CONCLUSIONS recognition,2017,pp.5918–5926.
The DeSign activation method introduces a design strat- [13] Jiehua Zhang, Zhuo Su, Yanghe Feng, Xin Lu, Matti Pietika¨inen, and
Li Liu, “Dynamic binary neural network by learning channel-wise
egy to address information loss in Binary Neural Networks thresholds,” in ICASSP 2022-2022 IEEE International Conference on
(BNNs). It improves BNN training accuracy without the Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp.
additional computational overhead. DeSign offers two key 1885–1889.
[14] ZechunLiu,ZhiqiangShen,MariosSavvides,andKwang-TingCheng,
advantages: it enhances BNNs by selectively capturing rele-
“Reactnet: Towards precise binary neural network with generalized
vant information from binary convolutions, and it reduces the activation functions,” in European Conference on Computer Vision.
impactofreal-valuedlearnableparametersinlayerslikebatch- Springer,2020,pp.143–159.
[15] Leonard Schuchman, “Dither signals and their effect on quantization
normalization, enabling the training of BNNs with full binary noise,” IEEETransactionsonCommunicationTechnology,vol.12,no.
parameters and comparable performance. Simulations demon- 4,pp.162–165,1964.
strate that carefully selecting the spatial pattern significantly [16] OndrejHolesovsky,“Compactconvnetswithternaryweightsandbinary
activations,”2017.
boosts BNN accuracy, improving baseline accuracy by up to [17] RobertUlichney, Digitalhalftoning, MITpress,1987.
4.51%.Theproposedthresholdkerneldesignmethodologyhas [18] W.M.Goodall,“Televisionbypulsecodemodulation,”TheBellSystem
the potential for further improvement using decision trees or
TechnicalJournal,vol.30,no.1,pp.33–49,1951.
[19] Robert A Ulichney, “Review of halftoning techniques,” in Color
end-to-end design approaches. Moreover, it can be extended Imaging:Device-IndependentColor,ColorHardcopy,andGraphicArts
to other real-valued activations beyond the ReLU. V.Spie,1999,vol.3963,pp.378–391.
[20] RobertUlichney, “Thevoid-and-clustermethodforditherarraygener-
ation,” SPIEMILESTONESERIESMS,vol.154,pp.183–194,1999.
VII. ACKNOWLEDGEMENT
[21] SergeyIoffeandChristianSzegedy,“Batchnormalization:Accelerating
This work was supported by the Vicerrector´ıa de Investi- deepnetworktrainingbyreducinginternalcovariateshift,” inInterna-
tionalconferenceonmachinelearning.pmlr,2015,pp.448–456.
gacio´n Extensio´n of the Universidad Industrial de Santander,
[22] HaotongQin,RuihaoGong,XianglongLiu,XiaoBai,JingkuanSong,
Colombia under the research project 3735. andNicuSebe, “Binaryneuralnetworks:Asurvey,” PatternRecogni-
tion,vol.105,pp.107281,2020.
REFERENCES [23] Tatiana Gelvez and Henry Arguello, “Nonlocal low-rank abundance
prior for compressive spectral image fusion,” IEEE Transactions on
[1] Christian Szegedy, Alexander Toshev, and Dumitru Erhan, “Deep GeoscienceandRemoteSensing,vol.59,no.1,pp.415–425,2020.
neural networks for object detection,” Advances in neural information [24] JorgeBacca,TatianaGelvez-Barrera,andHenryArguello,“Deepcoded
processingsystems,vol.26,2013. aperture design: An end-to-end approach for computational imaging
[2] Waseem Rawat and Zenghui Wang, “Deep convolutional neural net- tasks,”IEEETransactionsonComputationalImaging,vol.7,pp.1148–
works for image classification: A comprehensive review,” Neural 1160,2021.
computation,vol.29,no.9,pp.2352–2449,2017. [25] RuizhouDing,Ting-WuChin,ZeyeLiu,andDianaMarculescu, “Reg-
[3] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew, ularizing activation distribution for training binarized deep networks,”
“A review of semantic segmentation using deep neural networks,” in Proceedings of the IEEE/CVF conference on computer vision and
International journal of multimedia information retrieval, vol. 7, no. patternrecognition,2019,pp.11408–11417.
2,pp.87–93,2018. [26] HaotongQin,RuihaoGong,XianglongLiu,MingzhuShen,ZiranWei,
[4] Bradley McDanel, Surat Teerapittayanon, and H.T. Kung, “Embedded Fengwei Yu, and Jingkuan Song, “Forward and backward information
binarized neural networks,” in Proceedings of the 2017 International retention for accurate binary neural networks,” in Proceedings of the
ConferenceonEmbeddedWirelessSystemsandNetworks,USA,2017, IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020,
EWSN’17,p.168–173,JunctionPublishing. pp.2250–2259.
[5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David, “Bina- [27] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and
ryconnect: Training deep neural networks with binary weights during Yuheng Zou, “Dorefa-net: Training low bitwidth convolutional neural
propagations,” Advancesinneuralinformationprocessingsystems,vol. networkswithlowbitwidthgradients,”arXivpreprintarXiv:1606.06160,
28,2015. 2016.
[6] Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim, “Bi-
naryduo: Reducing gradient mismatch in binary activation network by
coupling binary activations,” in International Conference on Learning
Representations,2020.
[7] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and
Yoshua Bengio, “Binarized neural networks: Training deep neural
networks with weights and activations constrained to+ 1 or-1,” arXiv
preprintarXiv:1602.02830,2016.
[8] ZihanXu,MingbaoLin,JianzhuangLiu,JieChen,LingShao,YueGao,
YonghongTian,andRongrongJi, “Recu:Revivingthedeadweightsin
binaryneuralnetworks,” inProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,2021,pp.5198–5208.