CITY, UNIVERSITY OF LONDON
DOCTORAL THESIS
Towards Knowledge-Grounded Natural Language
Understanding and Generation
Author: Supervisors:
Chenxi WHITEHOUSE DrTillman WEYDE
DrPranava MADHYASTHA
SchoolofScienceandTechnology
DepartmentorComputerScience
March,2024
4202
raM
22
]LC.sc[
1v46351.3042:viXraiii
Abstract
TowardsKnowledge-GroundedNaturalLanguageUnderstandingand
Generation
byChenxi WHITEHOUSE
This thesis investigates how natural language understanding and generation
with transformer models can benefit from grounding the models with knowledge
representations. Currently,themostprevailingparadigmfortraininglanguagemod-
els is through pre-training on abundant raw text data and fine-tuning on down-
streamtasks. Althoughlanguagemodelscontinuetoadvance,especiallytherecent
trendofLargeLanguageModels(LLMs)suchasChatGPT,thereseemtobelimitsto
whatcanbeachievedwithtextdataaloneanditisdesirabletostudytheimpactof
applyingandintegratingrichformsofknowledgerepresentationtoimprovemodel
performance.
The most widely used form of knowledge for language modelling is structured
knowledge in the form of triples consisting of entities and their relationships, of-
ten inEnglish. Thisthesis exploresbeyond this conventional approach andaims to
addressseveralkeyquestions:
• Canknowledgeofentitiesextenditsbenefitsbeyondentity-centrictaskssuch
asentitylinking?
• Howcanwefaithfullyandeffectivelyextractsuchstructuredknowledgefrom
rawtext,especiallynoisywebtext?
• How do other types of knowledge, beyond structured knowledge, contribute
toimprovingNLPtasks?iv
To this end, we study various tasks including multimodal and multilingual ap-
plications and consider a wide spectrum of knowledge, structured knowledge that
istypicallyrepresentedastripleswithentitiesandtheirrelations,andunstructured
knowledgeincludingparametricknowledgepreservedinlanguagemodels,knowl-
edgedistilledfromLargeLanguageModels,etc.
Knowledge-groundingwithstructuredknowledge.
We begin by investigating the integration of structured knowledge into language
models. Knowledge of entities has shown benefits for entity-centric tasks such as
entity linking and relation extraction, however, most studies have been limited to
monolingual settings. We expand knowledge-grounding with structured knowl-
edge,specificallyentities,intwodirectionsofresearch.
Firstly,westudywhetherknowledgeofentitiescanbenefitreal-worldfakenews
detection. Wehypothesisethattheworldknowledgeembeddedinentitiescancon-
tributetoassessingthetruthfulnessofnewsstatements. Evaluationofvariousknowl-
edge integration approaches on distinct datasets reveals that knowledge-enhanced
language models improve fake news detection when incorporated with a relevant
andup-to-dateknowledgebase.
The second direction expands beyond English and focuses on multilingual en-
tities. We introduce EntityCS, where we first construct a code-switched (CS) train-
ing corpus from Wikipedia, by switching entities in English to their counterparts
in other languages. Then we intermediate-train a pretrained multilingual model
on this corpus for joint masked language modelling and entity prediction. Sub-
sequent fine-tuning of the model on entity-centric downstream tasks consistently
improves zero-shot cross-lingual transferability, demonstrating the benefit of inte-
gratingknowledgeofmultilingualentities.
Extractingstructuredknowledgefromwebtext.
Wecontinuebystudyingeffective,faithful,androbustextractionofstructuredknowl-
edge from web text. Most existing information extraction (IE) datasets are con-
strainedtoWikipediaarticles,andmodelstrainedonsucharichfactualtextcorpusv
showpoorperformancewhenappliedtomorenoisytextfromtheweb. Toaddress
these challenges, we introduce WebIE, a new dataset that takes raw sentences as in-
putandstructuredtriplesasoutput. WebIEemphasisesdataqualitybyintroducing
negative examples and undergoing rigorous human annotation. We also propose
faithful generative information extraction pipelines. Our experiments with entity
planningtrainingandprefix-triedecodingshowimprovementinaccuratelyextract-
ingknowledgeontheweb.
Knowledge-groundingbeyondstructuredknowledge.
To address our last research question, we study the impact of a broader sense of
knowledge, including parametric knowledge (knowledge stored in the latent pa-
rameters of the models) derived from a model’s self-explanations and knowledge
distilledfromLLMsviadataaugmentation.
Weexpandtheapplicationtomultimodallanguagemodelsandstudyknowledge-
intensive visual question answering (VQA). We introduce a unified approach for
fine-tuning multimodal models for jointly generating answers and explanations.
Our experiments demonstrate enhancement in both answer accuracy and explana-
tionquality.
Lastly, as LLMs continue to advance in performance and size, we explore the
utilityofdistillingcommonsenseknowledgefromgeneral-purposeLLMstobenefit
smaller task-specific models. We prompt various LLMs to generate diverse exam-
ples on several challenging and scarce multilingual commonsense datasets. This
augmentation shows consistent enhancements on fine-tuned smaller models, shed-
dinglightondataaugmentationstrategiesforscenarioswithlimitedtrainingdata.
Insummary,thisthesisexplorestheroleofknowledgegroundinginnaturallan-
guage understanding and generation across a broad spectrum of tasks. We found
thatincorporatingrelevantandup-to-dateknowledgeofentitiesbenefitsfakenews
detection,andentity-focusedcode-switchingsignificantlyenhanceszero-shotcross-
lingualtransferonentity-centrictasks. Intermsofeffectiveandfaithfulapproachesvi
to extracting structured knowledge, our study found that integrating negative ex-
amples and training with entity planning significantly improves performance. Ad-
ditionally, we established that other general forms of knowledge, such as para-
metric and distilled knowledge, enhance multimodal and multilingual knowledge-
intensive tasks. This research shows the tangible benefits of diverse knowledge in-
tegrationandmotivatesfurtherexplorationinthisdirection.vii
Acknowledgements
IwanttoexpressmysinceregratitudetomyPhDsupervisorsfortheirconsistent
supportandguidancethroughoutmydoctoraljourney. SpecialthankstoDrTillman
Weyde and Dr Nikos Komninos for selecting me for the PhD studentship. I am
genuinely thankful to Dr Pranava Madhyastha, who joined my PhD supervision
lateronandinspiredmetoexploreandgainadeeperunderstandingofthefieldof
NLP.
Ideeplyappreciateallthosewhosupportedmeduringmyresearchinternships,
which were an integral part of my PhD studies. It was an honour to intern at some
of the world’s leading research institutes and learn from experienced researchers.
Warm thanks to Dr Fenia Christopoulou, my mentor during my first research in-
ternship at Huawei, whose dedication and hard work have consistently motivated
me. IamgratefultoDrClaraVaniaatAmazonforherencouragementandgenuine
support for my future endeavours. Furthermore, I would also like to extend my
gratitude to Dr Fantine Huot and Dr Jasmijn Bastings for the opportunity to intern
at Google DeepMind, a dream come true, and to Dr Mostafa Dehghani and Prof.
MirellaLapatafortheirinvaluableassistanceandguidancethroughoutthejourney.
IextendawarmthankyoutoDrAlhamFikriAjiforhissupport,fromAmazon
toMBZUAI,andforencouragingmetoexpandmyresearchcollaboration.
Lastly,mysincerethanksgotoJaime,whohasbeenasteadfastsourceofsupport
in my life, believing in me and standing by my side through both the best and the
mostchallengingtimes.ix
Contents
Abstract iii
Acknowledgements vii
1 Introduction 1
1.1 ResearchQuestions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 StructureoftheThesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Background: Transformer,Knowledge,Knowledge-EnhancedPLMs 9
2.1 TheTransformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1 TransformerArchitecture . . . . . . . . . . . . . . . . . . . . . . 10
2.1.1.1 AttentionMechanism . . . . . . . . . . . . . . . . . . . 10
2.1.1.2 OtherComponentsintheTransformer . . . . . . . . . 13
2.1.2 TransformerModels . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.2.1 Encoder-OnlyModels . . . . . . . . . . . . . . . . . . . 15
2.1.2.2 Decoder-OnlyModels . . . . . . . . . . . . . . . . . . . 17
2.1.2.3 Sequence-to-SequenceModels . . . . . . . . . . . . . . 18
2.1.2.4 MultimodalTransformers . . . . . . . . . . . . . . . . 19
2.2 KnowledgeTypesandSources . . . . . . . . . . . . . . . . . . . . . . . 20
2.3 Knowledge-EnhancedLanguageModels . . . . . . . . . . . . . . . . . 22
2.3.1 AddingEntityEmbeddings . . . . . . . . . . . . . . . . . . . . . 23
2.3.2 UsingExternalMemory . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.3 AddingKnowledge-RelatedAuxiliaryPre-trainingTasks . . . . 26
2.3.4 AddingAdapters . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.3.5 Retrieval-AugmentedLanguageModels . . . . . . . . . . . . . 29
2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31x
3 Knowledge-EnhancedLanguageModelsforFakeNewsDetection 33
3.1 BackgroundandIntroduction . . . . . . . . . . . . . . . . . . . . . . . . 33
3.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2.1 ApproachestoFakeNewsDetection . . . . . . . . . . . . . . . . 35
3.3 ModelsandDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.3.1 Knowledge-EnhancedPLMs . . . . . . . . . . . . . . . . . . . . 37
3.3.2 FakeNewsDatasets . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.4 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.5 ResultsandDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.5.1 DetectionAccuracy . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.5.2 KBLinking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.6 ConclusionandFutureWork . . . . . . . . . . . . . . . . . . . . . . . . 46
3.7 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
4 Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer 49
4.1 BackgroundandIntroduction . . . . . . . . . . . . . . . . . . . . . . . . 49
4.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.2.1 Cross-LingualPre-Training . . . . . . . . . . . . . . . . . . . . . 52
4.2.2 CodeSwitching . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.2.3 KnowledgeIntegrationintoLanguageModels . . . . . . . . . . 53
4.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.3.1 ENTITYCS CorpusConstruction . . . . . . . . . . . . . . . . . . 54
4.3.2 MaskingStrategies . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.4.2 Hyper-ParameterSettings . . . . . . . . . . . . . . . . . . . . . . 61
4.4.3 LanguagesforIntermediateTraining. . . . . . . . . . . . . . . . 62
4.5 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.5.1 NamedEntityRecognition . . . . . . . . . . . . . . . . . . . . . 63
4.5.2 FactRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.5.3 SlotFilling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66xi
4.5.4 WordSenseDisambiguation . . . . . . . . . . . . . . . . . . . . 67
4.5.5 ComparisonwithModelstrainedwithParallelData . . . . . . . 67
4.6 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.6.1 PerformancevsLanguagesinIntermediateTraining . . . . . . . 68
4.6.2 PerformancevsTrainingSteps . . . . . . . . . . . . . . . . . . . 69
4.6.3 RandomandSameTokenPrediction . . . . . . . . . . . . . . . . 69
4.6.4 EntityMaskingPercentage . . . . . . . . . . . . . . . . . . . . . 70
4.7 ConclusionandFutureWork . . . . . . . . . . . . . . . . . . . . . . . . 71
4.8 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5 FaithfulandRobustKnowledgeExtractionontheWeb 75
5.1 BackgroundandIntroduction . . . . . . . . . . . . . . . . . . . . . . . . 75
5.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.2.1 InformationExtractionDatasets . . . . . . . . . . . . . . . . . . 79
5.2.2 InformationExtractionApproaches . . . . . . . . . . . . . . . . 79
5.2.3 WEBIE Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.2.4 HumanAnnotation . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.2.5 Multilingual WEBIE . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.3 GenerativeInformationExtraction . . . . . . . . . . . . . . . . . . . . . 83
5.3.1 Sentence-to-TriplesGeneration . . . . . . . . . . . . . . . . . . . 83
5.3.2 Entity-LinkingasanAuxiliaryTask . . . . . . . . . . . . . . . . 84
5.3.3 InferencewithaConstraintTrie. . . . . . . . . . . . . . . . . . . 86
5.4 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.4.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.4.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.5 ResultsandAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.5.1 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.5.2 Cross-lingualTransferwithmBART . . . . . . . . . . . . . . . . 90
5.5.3 ResultswithAdditionalELTraining . . . . . . . . . . . . . . . . 90
5.6 ConclusionandFutureWork . . . . . . . . . . . . . . . . . . . . . . . . 91
5.7 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93xii
6 GroundedAnswerandExplanationinKnowledge-IntensiveVQA 95
6.1 BackgroundandIntroduction . . . . . . . . . . . . . . . . . . . . . . . . 95
6.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
6.2.1 MultimodalTransformer-basedModels . . . . . . . . . . . . . . 98
6.2.2 ArtificialPromptTokens . . . . . . . . . . . . . . . . . . . . . . . 98
6.2.3 ExplanationGenerationforVQA . . . . . . . . . . . . . . . . . . 99
6.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.3.1 MultitaskLearningwithArtificialPromptTokens . . . . . . . . 100
6.3.2 PerplexityasMultipleChoiceMetric . . . . . . . . . . . . . . . . 100
6.4 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.4.2 TrainingSetupandHyper-parameters . . . . . . . . . . . . . . . 102
6.4.3 NLGEvaluationMetrics . . . . . . . . . . . . . . . . . . . . . . . 103
6.5 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.5.1 AnswerAccuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.5.2 ExplanationEvaluation . . . . . . . . . . . . . . . . . . . . . . . 106
6.5.3 JointAnswerandExplanationGeneration . . . . . . . . . . . . 108
6.6 AnalysisandDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.6.1 ErrorAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.6.2 DatasetQuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.6.3 ExplanationEvaluation . . . . . . . . . . . . . . . . . . . . . . . 112
6.7 ConclusionandFutureWork . . . . . . . . . . . . . . . . . . . . . . . . 112
6.8 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7 KnowledgeDistillationviaLLM-poweredDataAugmentation 115
7.1 BackgroundandIntroduction . . . . . . . . . . . . . . . . . . . . . . . . 115
7.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.2.1 MultilingualandLow-ResourceNLP . . . . . . . . . . . . . . . 117
7.2.2 MultilingualDataAugmentation . . . . . . . . . . . . . . . . . . 118
7.3 DatasetAugmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
7.3.1 LLMsforDataGeneration . . . . . . . . . . . . . . . . . . . . . . 120
7.3.2 InstructionsandResponses . . . . . . . . . . . . . . . . . . . . . 120xiii
7.3.3 TopicDiversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.4 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.5 ResultsandDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.5.1 GeneralResult . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.5.2 MultilingualDataGeneration . . . . . . . . . . . . . . . . . . . . 126
7.5.3 DatasetScalingUp . . . . . . . . . . . . . . . . . . . . . . . . . . 129
7.5.4 FixedRatioDataAugmentation . . . . . . . . . . . . . . . . . . 129
7.6 HumanEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.6.1 TextNaturalness . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.6.2 LogicSoundness . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.7 ConclusionandFutureWork . . . . . . . . . . . . . . . . . . . . . . . . 133
7.8 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
8 Conclusion 137
8.1 FutureWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
A Language-SpecificResultsfor ENTITYCS Experiments 143
B AdditionalResultsandAnalysisfor WEBIE 147
B.1 AdditionalResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
B.2 ExamplesofReFinEDOutput . . . . . . . . . . . . . . . . . . . . . . . . 147
B.3 MTurkAnnotationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . 148
C AdditionalResultsandAnalysisfor UMAE 153
C.1 DetailedExplanationScores . . . . . . . . . . . . . . . . . . . . . . . . . 153
C.2 ExamplesofGeneratedExplanations . . . . . . . . . . . . . . . . . . . . 154
C.3 ExamplesofIssuesintheDatasets . . . . . . . . . . . . . . . . . . . . . 154
D AdditionalResultsforLLM-poweredDataAugmentation 159
Bibliography 163xv
List of Figures
1.1 Illustration of the role of knowledge bases in text generation: When
theparametricknowledgestoredinthelanguagemodelbecomesout-
dated (such as with GPT-2, trained in 2017), it is important to incor-
porate up-to-date knowledge bases for deriving correct answers, es-
peciallywhenusingthefrozenmodelatinferencetime. . . . . . . . . . 2
2.1 The Transformer Model Architecture. It is composed of a stack of
N encoders (on the left) and N decoders (on the right). Each sub-
layercomprisesamulti-headattentionlayerandafeed-forwardlayer.
LayerNormalisationandResidualconnectionsareappliedaftereach
layer. Source: Vaswanietal.(2017). . . . . . . . . . . . . . . . . . . . . . 11
2.2 Illustration of Multi-Head Attention. Input is passed through learnt
query (Q), key (K), and value (V) matrices to compute attention scores
withrespecttoothertokensinthesequence. Multi-headattention(on
theright)enablesthecalculationofattentionindividuallyandprojects
the input sequence into different subspaces. The outputs of all the
headsarethenconcatenatedandlinearlytransformedtoproducethe
finaloutput. Source: Vaswanietal.(2017). . . . . . . . . . . . . . . . . . 12
2.3 Vision Transformer. Images are split into fixed-size patches and lin-
early embedded as sequences. Then position embeddings are added
and the resulting vectors are fed to a standard Transformer encoder.
Source: Dosovitskiyetal.(2021). . . . . . . . . . . . . . . . . . . . . . . 19
2.4 An example of a knowledge graph in Wikidata, which contains the
informationofentities,theirrelations,andthedescriptionsoftheen-
tities. Source: XiaozhiWangetal.(2021). . . . . . . . . . . . . . . . . . . 21xvi
2.5 Illustration of ERNIE. ERNIE comprises a conventional text encoder
andanadditionalknowledgeencoder. Anaggregator(ontheright)is
applied for the mutual integration of the input tokens and entities.
The information fusion layer takes both token embedding and the
concatenation of the token embedding and entity embedding as in-
put, and outputs new token embeddings and entity embeddings for
thenextlayer. Source: ZhengyanZhangetal.(2019).. . . . . . . . . . . 24
2.6 Illustration the Knowledge Attention and Recontextualisation (KAR)
component in KnowBert. BERT word piece representations (H ) are
i
proj
firstprojectedtoH (1),thenpooledovercandidatementionsspans
i
(2)tocomputeS,andcontextualizedintoSe usingmention-spanself-
attention(3). Anintegratedentitylinkercomputesweightedaverage
entity embeddings E˜ (4), which are used to enhance the span repre-
sentations with knowledge from the KB (5), computing S′e. Finally,
theBERTwordpiecerepresentationsarerecontextualisedwithword-
to-entity-spanattention(6)andprojectedbacktotheBERTdimension
(7)resultinginH′. Source: Petersetal.(2019). . . . . . . . . . . . . . . . 25
i
2.7 Illustration of the KEPLER framework. A language model is jointly
trained on knowledge embedding (KE) and masked language mod-
elling (MLM) objectives, where three types of entity embeddings are
studiedfortheknowledgeembeddingtask: entitydescriptionsasem-
beddings, entity and relation descriptions as embeddings, and entity
embeddings conditioned on relations. Source: Xiaozhi Wang et al.
(2021). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.8 IllustrationofK-ADAPTER.Comparedtoconventionalmulti-tasklearn-
ing for injecting knowledge (a) that may result in catastrophic for-
getting, K-ADAPTER (b) injects knowledge by training adapters in-
dependently on different pre-train tasks, enabling continual knowl-
edge infusion. When injecting new kinds of knowledge, the existing
knowledge-specific adapters will not be affected. KIA represents the
adapter layer and TRM represents the transformer layer. Source: R.
Wangetal.(2021). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28xvii
2.9 IllustrationofRAG.RAGcombinesapre-trainedretriever(QueryEn-
coder + Document Index) with a pre-trained Seq2Seq model (Gener-
ator) and fine-tune end-to-end. For query x, Maximum Inner Prod-
uct Search (MIPS) is utilised to find the top-K documents z. For the
final prediction y, the retrieved documents are treated as latent vari-
ablesandmarginalisedoverSeq2Seqpredictionsgivendifferentdoc-
uments. Source: P.Lewisetal.(2020). . . . . . . . . . . . . . . . . . . . 30
3.1 Numberofwords,POStags,punctuation,andnumber-likewordsper
statementinLIARandCOVID-19,aswellasnumberofhttps-linksper
statement in COVID-19. The mean values are shown as white-filled
circlesintheplots.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2 Wordcloudforthe50mostfrequententitieslinkedbyERNIEinLIAR
andCOVID-19. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.1 Illustration of generating ENTITYCS sentences from an English sen-
tence extracted from Wikipedia. Entities in double square brackets
indicatewikilinks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.2 Number of Code-Switched Entities and Sentences in the ENTITYCS
corpus. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.3 Illustration of the proposed masking strategies. Random tokens are
chosenfromtheentirevocabularyandthuscanbefromdifferentlan-
guages. (c) shows a case where “study” is replaced with a token in
Greek. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.4 ErrorDelta(lowerisbetter)fordifferenttypesoferrorsintheWikiAnn
testsetbetweenvanillaXLM-R-baseandPEP +MLM. Weshowerror
MS
count differences for AR, ID and UR, the three languages with the
largest F1-score improvement, as well as EU and SW, the two lan-
guagesthatunderperformthebaseline. . . . . . . . . . . . . . . . . . . 65
4.5 F1-score comparison on WikiAnn test set (average across five seeds)
as a function of the number of training steps (in ten thousands) with
variousmaskingobjectives. EP-onlystrategiesareontheleft,andEP
+MLMstrategiesareontheright. . . . . . . . . . . . . . . . . . . . . . 70xviii
5.1 Illustrationofthetrainingstrategies. Theblueandgreentextreferto
mention span and its corresponding Wikipedia title (used as entity la-
bels). For standard BART training, the target output is the linearised
triples (subsection 5.3.1). For ENTITY-PROMPT, the target is the EL
output (subsection 5.3.2) concatenated with the linearised triples. In
ARTIFICIAL-PROMPT, we prepend an artificial token to the input to
indicatethedesiredoutput,EL(yellowbox)orlinearisedtriples. For
2LM-HEADS, we add an additional task-specific LM head to the de-
coderfortheELtask(greybox). . . . . . . . . . . . . . . . . . . . . . . . 78
6.1 IllustrationofUMAE:wetrainamultimodalencoder-decodermodel
on the mix of VQA tasks for jointly optimising answer and explana-
tion, where we distinguish the training instances and target output
with artificial prompt tokens (e.g., <#AOKA#>). The top and bottom
examplesarefromA-OKVQAandVCR,respectively. . . . . . . . . . . 96
6.2 Examples of generated explanations from MIX model with different
decoding strategies. Two examples on the left are from A-OKVQA
andtheothertwoontherightarefromVCR. . . . . . . . . . . . . . . . 107
6.3 Distributionoferrortypesindirectanswersfrom50errorsamplesin
OK-VQAandA-OKVQA. . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.4 Example of misassociation between coloured highlights and entities.
The model fails to associate purple box with Person2 and green box
withPerson3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1 Comparison between the 30 most frequent events in the original and
theChatGPT-generatedEnglishStoryClozedataset. . . . . . . . . . . . 123
7.2 Humanevaluationof50randomexamplesfromtheoriginalXCOPA,
ChatGPT(top)andGPT-4(bottom)generateddataintargetlanguages,
andtranslationofEnglishgenerateddata. Examplesareannotatedby
two native speakers in each language. The subplots in the last col-
umn show the logic issues of the XCOPA data, where the three bars
for each language represent Original, Gen , and GenTrans (from left to
XX EN
right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131xix
B.1 MTurkHITguidanceentityandrelationlabelling. . . . . . . . . . . . . 149
B.2 MTurkHITuserinterfaceforentityandrelationlabelling. . . . . . . . 149
B.3 MTurkHITuserinterfaceforcorrectingthemachine-translatedtext. . 151
B.4 MTurkHITuserinterfaceforentitylabellinginthetargetlanguage. . 151
C.1 ExamplesofgeneratedanswersandexplanationsgenerationforVCR. 155
C.2 ExamplesofgeneratedanswersandexplanationsforA-OKVQA. . . . 156
C.3 Questions that require knowledge of the movie plots to generate the
answersfromVCR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
C.4 ExamplesofsubjectivequestionsfromOK-VQA. . . . . . . . . . . . . . 157
C.5 Issuesinthedatasetsthatseverelyimpactthemodelgeneration: wrong
answers(left,fromA-OKVQA),questionsthatdonotneedvisualin-
puttoanswer(middle,fromA-OKVQA),andtypo(right,fromVQA-X).157xxi
List of Tables
3.1 Detectionaccuracyresults(averageoffiveruns). Thefirstsectioncor-
responds to the baseline models. Models in the second section use
Wikidata KB. The third section shows models using other KBs and
features. The best values within each section per dataset are marked
inbold. Thesubscriptnumberswith ± showthestandarddeviation.
Resultswith∗ indicatestatisticallysignificantimprovementsoverthe
baseline, both for the mean (t-test, one-sided, p < .05) and median
(Wilcoxonsignedranktest,one-sided, p < .05). . . . . . . . . . . . . . 42
4.1 Statisticsofthe ENTITYCS Corpus. . . . . . . . . . . . . . . . . . . . . . 56
4.2 Summary of the proposed masking strategies. p corresponds to the
probabilityofchoosingcandidateitems(entity/non-entitytokens)for
masking. MASK, RND, SAME represent the percentage of replacing a
candidate with Mask, Random or the Same item. When combining
WEP/PEPwithMLM (+MLM),welower pto50%. . . . . . . . . . . . . 59
4.3 Besthyper-parametersusedforthedatasets. . . . . . . . . . . . . . . . 62
4.4 Averageperformanceacrosslanguagesonthetestsetofdownstream
tasks. XLM-RPRIORcorrespondstopreviousreportedresultswithXLM-
R-base,referringtoChietal.(2021b)forWikiAnn,Z.Jiangetal.(2020)
forX-FACTRandRaganatoetal.(2020)forXL-WiC.XLM-ROURSshows
our re-implemented results with XLM-R-base. Results (excluding X-
FACTR) are averaged across five seeds with standard deviation re-
portedasasubscript. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.5 F1-score per language on the WikiAnn test set. Results are averaged
acrossfiveseeds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64xxii
4.6 F1-score(averageacrossfiveseeds)forlanguageswithLatinandNon-
LatinscriptonMultiATIS++testsetwhenusingSF-onlytraining. . . . 66
4.7 Comparison with models using parallel data. Results for InfoXLM
and XLM-Align are obtained from Chi et al. (2021a) and Chi et al.
(2021b),respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.8 Results(averageoverfiveseeds)withadifferentnumberofpre-training
languages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
5.1 StatisticsofWebIEandcomparisonwithothersentence-levelRE(top
two rows) and IE datasets. Sentences correspond to the number of
distinct sentences. We report the publicly available version of We-
bRED. † shows the number of examples in each split. Anno. Triple
representsthenumberofhuman-annotatedtriples. . . . . . . . . . . . 82
5.2 ExperimentresultsforbeamsearchwithandwithoutconstraintTrie.
P and R refer to precision and recall, respectively, and Acc-N shows
theaccuracyofthenegativeexamples. BART correspondstomod-
RAND
elswithBARTconfigurationbutrandomlyinitialisedweights. BART
PLM
are models with pretrained weights from M. Lewis et al. (2020). (R),
(W),(R+W)refertomodelstrainedonREBEL,WEBIE,andbothdatasets,
respectively. For WEBIE we show the overall performance and the
accuracy on negative samples. Results in blue shades are zero-shot
performance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.3 Performance on mWEBIE with mBART. Results for non-English are
zero-shot. Empty-Pos(itive)% shows false negatives percentage, re-
vealing zero-shot performance has a high rate of empty results for
positiveexamples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.4 Comparison of various training with entity linking as an auxiliary
task, and beam search with and without constraint Trie decoding.
WEBIE results are on the annotated test set. All models use BART
configuration with randomly initialised weights. We show in bold
thebestresultamongthetrainingobjectives. . . . . . . . . . . . . . . . 91xxiii
6.1 Performance of models for answer generation. Better results are in
bold. OFA* refers to the pre-trained OFA. Prior-best results for the
three datasets are from Gui et al. (2022), D. Schwenk et al. (2022),
Yanan Wang et al. (2023), respectively. † is from a discriminative
modelandthusnotcomparable(seeYeandKovashka,2021). . . . . . 105
6.2 AblationonthemodalitydependencyforansweraccuracyofA-OKVQA.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.3 Explanation Scores. R-L, MET. stand for ROUGE-L, METEOR, respec-
tively. OFA* is the pre-trained OFA, showing the transferability of
OFA for generating explanations with natural language instructions.
Results with e-UG are from Kayser et al. (2021). We show the best
results of A-OKVQA and VCR in bold. The last row in blue shade
showsout-of-domainperformance. . . . . . . . . . . . . . . . . . . . . . 106
6.4 ExplanationscoresonthesamesubsetofA-OKVQA. . . . . . . . . . . 108
6.5 Evaluation of answers generated given questions (Q->A) and jointly
generated with explanations (Q->AE). The last column with a blue
shadowindicatesout-of-domainperformance. Forsimplicity,weuse
theGlovemetric forselectinganswersfor Multiple-Choice questions
inA-OKVQA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6.6 Scores of explanations generated given answers (QA->E) and jointly
generatedwithanswers(Q->AE).Thelastrowwithablueshadowin-
dicatesout-of-domainperformance. . . . . . . . . . . . . . . . . . . . . 108
6.7 HumanperformanceonOK-VQAandA-OKVQAmeasuredfromthe
ground truth answers. For simplicity, we use the Glove metric for
selectinganswersforMultiple-ChoicequestionsinA-OKVQA. . . . . 111
7.1 NumberofexamplesavailableinXCOPA,XWinograd,andXStoryCloze
perlanguage. SinceavalidationsplitisnotspecifiedinXStoryCloze,
wetake60randomexamplesfromthetrainsplitforvalidation. . . . . 116
7.2 ExamplesofinstructionsandLLM-responsesforXCOPA,XWinograd,
andXStoryCloze. WeuseChatGPTfordemonstration. . . . . . . . . . 121xxiv
7.3 Generation Success Rate in English (valid examples obtained / total
examplesrequested)withdifferentLLMsonthethreedatasets. . . . . 122
7.4 Comparison of Average Accuracy across all languages for mBERT,
XLMR-Base, and XLMR-Large on XCOPA, XStoryCloze, and XWino-
grad. Training datasets include ORI (original EN data), GEN (LLM-
generated EN data), and O+G (both), with the number of examples
used for training indicated by the subscripts. The best results ob-
tainedwiththesameamountoftrainingdataarehighlightedinbold.
Greenandredsubscriptsdenoteimprovementanddeclineinperfor-
mance compared to the baseline (ORI). See per language results in
AppendixD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.5 Accuracy on XCOPA. ORI corresponds to the original data, GEN
EN
andGEN representsdatageneratedinEnglishandtargetlanguages.
XX
TransdenotestranslationsoftheEnglish-generateddata. Weshowlan-
guages that are available in all settings. Improvement and decline in
performancearerepresentedwithgreenandredshadows. . . . . . . . 127
7.6 AccuracyonXCOPAwhenscalingupthegenerateddatatoover28K
with ChatGPT. We report average results on all XCOPA languages
excl. QU,sinceitisnotavailablewiththeGoogleTranslateAPI. . . . . 128
7.7 Performance on English test examples training on GPT-4-generated
Englishdataandtheoriginaldata. Originaldatapointsselectedfrom
the three datasets are set to 200. 1× corresponds to using only the
original data, 2× means using 200 original data and 200 generated
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
A.1 F1-score per language on the WikiAnn test set. Results are averaged
acrossfiveseeds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
A.2 X-FACTRresults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
A.3 F1-score(averageacrossfiveseeds)onMultiATIS++SF-onlytraining. 145
A.4 XL-WiCtestsetaccuracy(averageacrossfiveseeds)acrosslanguages. 145xxv
B.1 Comparison of the zero-shot performance on mWEBIE with mBART
when specifying the source language (XX) and keeping the default
setting as the source language (EN). Results are with standard beam
search(withouttheconstraintTrie). . . . . . . . . . . . . . . . . . . . . 147
B.2 ReFinEDoutputson WEBIE validationexamples. . . . . . . . . . . . . 148
C.1 Explanation scores with automatic NLG for generated explanations
(QA→E)fromUMAE modelwithdifferentdecodingstrategies. B1B4
ALL
correspondtoBLEU1BLEU4,R-LmeansROUGE-LandMET. meansME-
TEOR. The last two rows (with blue shadow) indicate out-of-domain
performance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
C.2 Explanation scores with automatic NLG for generated explanations
from Q→AE with UMAE model. B1 B4 correspond to BLEU1 BLEU4,
ALL
R-L means ROUGE-L and MET. means METEOR. The last two rows
(withblueshadow)indicateout-of-domainperformance. . . . . . . . 154
D.1 AccuracyonXCOPAwithdifferentLLM-generatedEnglishdata. . . . 159
D.2 AccuracyonXWinogradwithdifferentLLM-generatedEnglishdata. 160
D.3 AccuracyonXStoryClozewithdifferentLLM-generatedEnglishdata. 160
D.4 Full results on XCOPA (with ChatGPT-generated data). +TLV corre-
spondstoincludingtheoriginalvalidationsetinallTargetLanguages
intheValidationset. Rowsaresortedbythenumberofinstancesused
in training. AVG shows average results for languages that are avail-
ableinallsettings(excl. QU,TH,TR). . . . . . . . . . . . . . . . . . . . 161
D.5 AccuracyonXCOPA.GEN andGEN represents3.7Kand3.6Kdata
EN XX
in English and target languages generated by GPT-4. AVG shows av-
erageresultsforlanguagesthatareavailableinallsettings(excl. QU,
TH,TR). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162xxvii
Dedicated to my parents and Jaime.1
Chapter 1
Introduction
Natural Language Processing (NLP) has witnessed significant progress in recent
years, particularly since the emergence of transformer-based (Vaswani et al., 2017)
languagemodels. MostrepresentativetransformermodelsincludeBERT(Devlinet
al., 2019), RoBERTa (Yinhan Liu et al., 2019), BART (M. Lewis et al., 2020), T5 (Raf-
fel et al., 2020a), to name a few, which are typically pre-trained on raw, unlabeled
textual inputs with objectives such as Masked Language Modelling (Devlin et al.,
2019), and then fine-tuned on labelled task-specific downstream tasks. Although
this pre-training and fine-tuning paradigm has proven effective and achieved new
start-of-the-art performance on various NLP tasks (Devlin et al., 2019), it presents
limitations in adapting to the ever-evolving world knowledge (Zhengyan Zhang et
al., 2019; Weijie Liu et al., 2020). Figure 1.1 shows an example where the knowl-
edgebaseiscrucialforthemodelgenerationwhentheparametricknowledgestored
withinthemodelparametersbecomesstale.
To address this, a growing need has emerged to explore the integration of di-
verse knowledge representations and grounding into the language models to en-
hancetheircapabilitiesintasksparticularlyrequiringintensiveknowledge(P.Lewis
etal.,2020;Izacardetal.,2023)orin-depthcontextunderstanding(T.-Y.Changetal.,
2020;BauerandBansal,2021).
The most commonly used knowledge for such integration is structured knowl-
edge, typically represented in knowledge bases consisting of entities and their re-
lationships (X. Chen, S. Jia, and Y. Xiang, 2020; C. Zhu et al., 2022). Knowledge2 Chapter1. Introduction
LM
Without KB
The name of the
current president of Barack Obama
the US is ...
LM
With KB
The name of the
current president of Joe Biden
the US is ...
(President of the United States,
of ceholder, Joe Biden)
FIGURE1.1: Illustrationoftheroleofknowledgebasesintextgeneration: When
the parametric knowledge stored in the language model becomes outdated
(such as with GPT-2, trained in 2017), it is important to incorporate up-to-date
knowledgebasesforderivingcorrectanswers,especiallywhenusingthefrozen
modelatinferencetime.
bases, such as Wikidata,1 provide a rich source of human-curated factual knowl-
edge, which can be used as a complement to unlabelled raw text. Considerable re-
search has focused on incorporating structured knowledge, especially entity-based
knowledge (i.e., knowledge of entities), into language models (Zhengyan Zhang et
al.,2019;Petersetal.,2019;R.Wangetal.,2021;XiaozhiWangetal.,2021). Although
these prior works have proven the advantages of knowledge integration in specific
tasks such as entity linking, named entity recognition, etc., the potential of a wider
spectrum of knowledge representations (e.g., unstructured knowledge) and appli-
cations (e.g., beyond entity-centric tasks) remains relatively under-explored. This
motivates us to conduct a more extensive investigation into the following research
questions.
1.1 Research Questions
This thesis aims to address the research questions outlined in the following three
aspects.
1https://www.wikidata.org/wiki/Wikidata:Main_Page1.1. ResearchQuestions 3
ExpandingtheUtilisationofStructuredKnowledge
Thefirstquestionwestudyis: Cantheadvantagesofentity-basedknowledgebeextended
tomultilingualsetupsandbeyondentity-centrictasks?
Prior work such as KnowBert (Peters et al., 2019), ERNIE (Zhengyan Zhang et
al., 2019), K-BERT (Weijie Liu et al., 2020), etc. have demonstrated the success of
integratingknowledgeofentitiesintolanguagemodelpre-training. However,their
predominant focuses are limited to monolingual language models (specifically, En-
glish only), and downstream tasks that are exclusively related to entities, such as
entity linking and named entity recognition. This thesis broadens the application
scope of structured knowledge in two ways: (i) we evaluate the effectiveness of
knowledge-enhanced language models on the more complex task of fake news de-
tection(Chapter3),and(ii)weproposetheuseofmultilingualentityknowledgein
anentity-centriccode-switchingmethod,ENTITYCS,toimprovecross-lingualtrans-
ferabilityonlow-resourcelanguages(Chapter4).
EffectiveExtractionofStructuredKnowledge
Wecontinuetoexplore: Howcanstructuredknowledgebeextractedeffectivelyandaccu-
ratelyfromdiversesources,particularlynoisywebtext?
Priorworkintroducedaboveaswellasourfirststudies(Whitehouseetal.,2022;
Whitehouse, Christopoulou, and Iacobacci, 2022) demonstrate the benefit of struc-
tured knowledge in various NLP tasks. Yet the challenge lies in how to effectively
obtain such knowledge, which is essential to adding new facts, keeping the rele-
vance and accuracy of existing facts, etc. Hence, there is a compelling need to de-
velop models capable of automatically extracting structured knowledge from vast
textualsources(Y.Yaoetal.,2019;Y.Yaoetal.,2021;Ormandietal.,2021),whichre-
quireshigh-qualityinformationextractiondatasets. Themajorityofexistingknowl-
edge or information extraction datasets that are used to train such models are con-
structed based on clean and fact-rich resources such as Wikipedia (Trisedya et al.,
2019; Huguet Cabot and Navigli, 2021; Seganti et al., 2021). As a result, models4 Chapter1. Introduction
trained on them may encounter difficulties when applied to noisy web text, as re-
vealed in our preliminary studies. Therefore, this thesis addresses the critical chal-
lenge by proposing a more generalised dataset, WEBIE, as well as modelling ap-
proachesthataresuitableforknowledgeextractionfromthewebtext(Chapter5).
ExplorationofDiverseKnowledgeForms
Thefinalquestionexploredinthisthesisfocuseson: Howcanbroaderformsofunstruc-
turedknowledgecontributetoenhancinglanguagemodels?
The preceding two directions focus on the investigation of structured knowl-
edge. However, there exists a diverse range of unstructured knowledge represen-
tations, such as those found in raw text (C. Zhu et al., 2022; J. Z. Pan et al., 2023),
knowledge derived from the language models after pre-training (i.e., stored in the
latent parameters) (Petroni et al., 2019; Zhiyuan Zhang et al., 2020; Xintao Wang
et al., 2022; Neeman et al., 2023). In this part of the thesis, we aim to broaden the
scope beyond structured knowledge and explore unstructured knowledge, includ-
ing parametric knowledge (Neeman et al., 2023) embedded within language mod-
els, through grounded answer and explanation generation in knowledge-intensive
VisualQuestionAnswering(VQA)(Chapter6),andknowledgedistilledfromLarge
Language Models via data augmentation (Chapter 7). Emphasis is placed on the
applicationofthesestudiesinmultimodalandmultilingualusecases.
1.2 Structure of the Thesis
The thesis begins with an introduction that outlines the background of transform-
ers, knowledge in NLP, and knowledge-enhanced language models in Chapter 2,
which sets out the foundation for all the subsequent studies conducted. The main
body of the thesis is structured around five chapters, each focusing on a published
conferencepaper.
The first focus is on knowledge-grounding with entities and structured knowl-
edge. Chapter 3concentrates onknowledge-enhanced languagemodels forthe ap-
plicationoffakenewsdetection. Weevaluatetheeffectivenessofvariousapproaches1.2. StructureoftheThesis 5
thatincorporateentityknowledgeinlanguagemodelsonfakenewsdetectionaccu-
racy,coveringnewsdatasetsindifferentdomains(i.e.,politicsandCOVID-19)with
different linguistic features, where we find the benefit of incorporating knowledge
thatisrelevantandup-to-date.
Chapter 4 focuses on studying the effectiveness of multilingual entity knowl-
edge in zero-shot cross-lingual transfer. We propose ENTITYCS, where we utilise
entity-centric code-switching with Wikipedia and Wikidata knowledge base to in-
termediatetrainacross-lingualpre-trainedlanguagemodel. Subsequentfine-tuned
models on entity-centric tasks, e.g., named entity recognition, slot filling, and fact
retrieval,demonstratestrongperformancecomparedtothebaseline.
Chapter 5 addresses the extraction of faithful and robust structured knowledge
from the web domain. Information extraction is essential for knowledge base con-
structionandpopulation,whichalsoprovidesreliableknowledgesourcesfortrain-
ing knowledge-enhanced models. In this chapter, we introduce the collection of a
new dataset, WEBIE, via crowdsourcing and propose various joint training strate-
giesformitigatingthehallucinationissuesingenerativeinformationextraction.
The subsequent two chapters explore knowledge-grounding beyond structured
knowledge. Chapter6expandstheapplicationtomultimodallanguagemodelsand
studies knowledge-intensive VQA, utilising parametric knowledge derived from a
model’s self-explanations. We propose a novel direction of jointly generating an-
swersandexplanationswithmultimodalgenerativemodelsmakinguseofartificial
specialtokens,enablingthedistinctionofdifferenttasksanddatasetswhilelearning
sharedsemanticsamongdifferenttasks.
As LLMs become ever more powerful, in Chapter 7, we explore the utility of
distilling commonsense knowledge from general-purpose LLMs to benefit smaller
task-specificmodels. WepromptvariousLLMstogeneratediverseexamplesonsev-
eralchallengingandscarcemultilingualcommonsensedatasets. Thisaugmentation
shows consistent enhancements on fine-tuned smaller models, compared to those
trainedonlimitedhuman-createddata.
In Chapter 8, we provide a summary of the thesis with insights and key take-
aways,aswellasmotivationsforpromisingfutureresearchdirections.6 Chapter1. Introduction
1.3 Publications
The main work proposed in this thesis has been published as conference papers.
Chapter3-7containexisting,improvedorextendedresultstothefollowingpublica-
tions:
1. EvaluationofFakeNewsDetectionwithKnowledge-EnhancedLanguageMod-
els(Whitehouseetal.,2022)
• MaincontentincludedinChapter3
• ProceedingsoftheSixteenthInternationalAAAIConferenceonWebandSocial
Media(AAAI-ICWSM2022)
2. EntityCS:ImprovingZero-ShotCross-lingualTransferwithEntity-CentricCode
Switching(Whitehouse,Christopoulou,andIacobacci,2022)
• MaincontentincludedinChapter4
• FindingsoftheAssociationforComputationalLinguistics: EMNLP2022
• WorkconductedasResearchInternatHuaweiNoah’sArkLab,London,
UnitedKingdom,2021
3. TowardsaUnifiedModelforGeneratingAnswersandExplanationsinVisual
QuestionAnswering(Whitehouse,Weyde,andMadhyastha,2023)
• MaincontentincludedinChapter6
• FindingsoftheAssociationforComputationalLinguistics: EACL2023
4. WebIE: Faithful and Robust Information Extraction on the Web (Whitehouse
etal.,2023b)
• MaincontentincludedinChapter5
• Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics(ACL2023)
• Work conducted as Research Intern at Amazon Alexa AI, Cambridge,
UnitedKingdom,20221.3. Publications 7
5. LLM-poweredDataAugmentationforEnhancedCross-lingualPerformance
(Whitehouse,Choudhury,andAji,2023)
• MaincontentincludedinChapter7
• Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP2023)
• WorkconductedincollaborationwithMBZUAIandMicrosoft
OtherPublicationsduringthePhD
Throughoutthecourseofmydoctoralstudies,thereareseveraladditionalpa-
pers that are not incorporated in the thesis, from my collaboration with re-
searchersfromGoogleDeepMindandMBZUAI.
6. Parameter-EfficientMultilingualSummarization: AnEmpiricalStudy(White-
houseetal.,2023a)
• FindingsoftheAssociationforComputationalLinguistics: NAACL2024
• Work conducted as Research Intern at Google DeepMind, Amsterdam,
Netherlands,2023
7. M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-
GeneratedTextDetection(YuxiaWangetal.,2024)
• Proceedings of the 18th Conference of the European Chapter of the Association
forComputationalLinguistics(EACL2024)
• WorkconductedincollaborationwithMBZUAI,2023
8. MultitaskMultilingualModelAdaptationwithFeaturisedLow-RankMixtures(C.-C.
Linetal.,2024)
• Preprint
• Work conducted as Research Intern at Google DeepMind, Amsterdam,
Netherlands,20239
Chapter 2
Background: Transformer,
Knowledge, Knowledge-Enhanced
PLMs
This section provides a background overview of three fundamental aspects: (i) the
Transformer. All the modelling approaches in this thesis are based on the trans-
formerarchitecture,(ii)knowledgesourcesandknowledgerepresentation,and(iii)
knowledge-enhanced Pre-trained Language Models (PLMs). These topics collec-
tivelyestablishthefoundationforthecorefocusofthisthesis: knowledge-grounding
inlanguagemodels.
2.1 The Transformer
Thetransformerarchitecture, proposed byVaswanietal.(2017), hasrevolutionised
theNLPfield. Comparedtoitspredecessors,includingRNNandLSTM(Hochreiter
and Schmidhuber, 1997), transformers distinguish themselves in the following key
aspects. Firstly,transformersemployanattentionmechanismtoalleviatethelimita-
tionsassociatedwithlong-termdependencies. Secondly,unlikeprevioussequential
models,transformerscanprocessinputsequencesinparallel,makingthemmoreef-
ficient. Transformersarealsohighlyscalable. Thestackedarchitecturecanbeeasily
scaled to be optimised for larger datasets and more complex tasks by adding more
layers or units, which is a key factor in the development of recent powerful LLMs
(Vaswanietal.,2017;Devlinetal.,2019;Brownetal.,2020a;Aniletal.,2023).10 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
Thanks to their advantages, transformers have become the cornerstone of the
currentNLPfield. Allthemodelsincludedinthisthesisarebasedontransformers.
Wefirstprovideanoverviewofthetransformerarchitectureandthenintroducesev-
eral representative model categories, including encoder-only models, decoder-only
models, and sequence-to-sequence models. Finally, we briefly touch upon multi-
modalvision-languagemodels.
2.1.1 TransformerArchitecture
Theoriginallyproposedtransformerarchitecture,asillustratedinFigure2.1,follows
a sequence-to-sequence structure, which takes a sequence as input and outputs a
new sequence. Its key innovation is the self-attention mechanism, which allows
the model to weigh the relevance of each word in a sequence to every other word,
enablingthecaptureoflong-rangedependenciesandcontextintextdata.
At its core, the transformer architecture contains a stack of encoder layers (left)
and decoder layers (right). Both the encoder stack and decoder stack have their
corresponding Embedding layers to process the input, represented by the Input
EmbeddingandOutput EmbeddinglayersinFigure2.1. Withineachlayer,Multi-Head
Attentionisappliedindependentlytoallsubwordsortokensintheinputsequence,
andtheoutputsarepassedthroughFeed Forwardneuralnetworklayers. Intheend,
theoutputofthelastlayerofthedecoderstackisfedtoaLinearlayerandaSoftmax
layertoobtainthefinaloutputsequence.
2.1.1.1 AttentionMechanism
Given a set of key-value pair vectors and a query vector, attention is a technique to
compute a weighted sum of the values, dependent on the association of the query
and the corresponding keys, i.e., the query attends to the values (Vaswani et al.,
2017).
ThecoreinnovationoftheTransformeristheincorporationoftheattentionmech-
anism. Unlike traditional models that linearly process sequences, the self-attention2.1. TheTransformer 11
FIGURE 2.1: The Transformer Model Architecture. It is composed of a stack of
Nencoders(ontheleft)and Ndecoders(ontheright). Eachsublayercomprises
amulti-headattentionlayerandafeed-forwardlayer. LayerNormalisationand
Residualconnectionsareappliedaftereachlayer. Source: Vaswanietal.(2017).
mechanism allows the model to weigh the significance of different words in a se-
quencewhenmakingpredictionsforaparticularword. Thismechanismenablesthe
modeltoconsiderglobalcontextanddependencieswithintheinputwindow.
Self-Attention
Vaswani et al. (2017) introduce the Scaled Dot-Product Attention mechanism, il-
lustrated in the left plot of Figure 2.2. Input vectors (input text embedding + po-
sitional embedding) are passed through three trainable matrices, Q, K, and V, to12 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
ScaledDot-ProductAttention Multi-HeadAttention
FIGURE2.2: IllustrationofMulti-HeadAttention. Inputispassedthroughlearnt
query (Q), key (K), and value (V) matrices to compute attention scores with re-
spect to other tokens in the sequence. Multi-head attention (on the right) en-
ables the calculation of attention individually and projects the input sequence
intodifferentsubspaces. Theoutputsofalltheheadsarethenconcatenatedand
linearlytransformedtoproducethefinaloutput. Source: Vaswanietal.(2017).
compute the Query vector, Key vector, and Value vector. These vectors share the
samedimensionsastheinputvector.
The initial step involves calculating a score that signifies the attention of one
token,token_i,inasequencetoanothertoken,token_j. Thisscoreisdeterminedby
the dot product of the Query vector of token_i and the Key vector of token_j. To
prevent the dot products from becoming too large as the dimensionality increases,
which can lead to gradients becoming too small during backpropagation, a scaling
√
factorof d isappliedtothedotproduct,withd representingthedimensionofK,
k k
theKeymatrix.
Aftercomputingthescoreforeachtoken(includingtoken_iitself),theresultsare
passed through a Softmax operation, which normalises the scores to positive num-
bers and a cumulative sum of 1. The normalised score at each position concerning
the query token determines the level of attention that the query token allocates to
each position. Finally, the output of the attention layer is computed as the sum of
theValuevectorsofeachpositionweightedbythenormalisedscores.
In practice, a set of input vectors is packed into a matrix and simultaneously2.1. TheTransformer 13
multiplied by the Q, K and V matrices. Therefore the attention mechanism can be
representedas:
(cid:112)
Attention(Q,K,V) = softmax(QKT/ d )V
k
Self-attentiondenotesapplyingtheattentionmechanismtotokenswithinthe
samesequence,andisemployedinbothencoderanddecoderlayers. Theonlydis-
tinction in the decoder layer lies in the masked attention mechanism, which only
permits a token to attend to all the tokens that precede it but not those that follow.
Thispreventstheleakageofinformationfromfuturetokensduringtraining.
Cross-Attention
In addition to the self-attention layers, cross-attention is also utilised in the
transformer. Here, Query vectors are from the decoder, while both Key and Value
vectors are derived from the encoder output. Cross-attention enables each position
inthedecoderofasequence-to-sequencemodeltoeffectivelyattendtoallpositions
intheinputsequence.
Multi-HeadAttention
Toenhancetheexpressivepowerofself-attention,thetransformerusesmultipleat-
tentionheads,i.e.,Multi-Head Attention. Insteadofapplyingoneattentionopera-
tionwiththeentireQuery,Key,Valuevectors,Vaswanietal.(2017)findthatdividing
the vectors into multiple chunks with different learned linear projections achieves
more efficient use of the model’s capacity: e.g., for English-to-German translation,
the performance of the model with eight heads is almost 1 BLEU point higher than
that of a model of the same size with single-head attention (Vaswani et al., 2017).
As illustrated in the right plot of Figure 2.2, each head focuses on different parts of
the input sequence, capturing diverse information. Afterwards, the outputs from
multipleheadsareconcatenatedandlinearlytransformed.
2.1.1.2 OtherComponentsintheTransformer
Apart from the multi-head attention mechanism, the following components play
importantrolesinthetransformerarchitecture.14 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
InputEmbeddingandPositionEmbedding
Thetransformerutiliseslearnedembeddingstorepresenttheinput. Inputsequences
arefirsttokenisedintotokensorsubwordsandthenconvertedintohigh-dimensional
vectorsthroughtheinputembeddinglayers. Theembeddinglayersarelineartrans-
formation layers at the bottom of both the encoder and the decoder stack (see Fig-
ure2.1).
As the transformer does not inherently capture the sequential order of tokens,
positional encoding is incorporated into the input embeddings to convey informa-
tion about token positions. Vaswani et al. (2017) employ sine and cosine functions
ofvaryingfrequenciestorepresentpositioninformation,withthepositionalembed-
dings typically sharing the same dimensions as the input embeddings. In addition
toabsolutepositionembeddings,asseeninVaswanietal.(2017),variouspositional
embeddingssuchasrelativepositionalembeddings(Shaw,Uszkoreit,andVaswani,
2018) and fully learnable embeddings (Devlin et al., 2019; Yinhan Liu et al., 2019;
Radfordetal.,2019)havealsobeenproposedintheliterature.
The two embeddings are often added before being fed to the encoder and de-
coder stack. Input text embeddings encapsulate the semantic meaning of the input
tokens,whilepositionalembeddingscapturethesequentialorderofthetokensina
high-dimensionalvectorspace.
Position-wiseFeed-ForwardNetworks
Each encoder and decoder layer contains a fully connected feed-forward network
(FFN)layer,whichprocessestheoutputfromtheattentionmechanismindependently
oneachpositioninthesequence. FFNlayerstypicallyconsistoftwolineartransfor-
mations and a non-linear activation function such as ReLU (Vaswani et al., 2017).
FFN layers are crucial in transformers to capture complex, non-linear relationships
withintheinputsequence.2.1. TheTransformer 15
LayerNormalisationandResidualConnections
Toenhancetrainingstability,eachsub-layer(multi-head attentionandFFNlayer)
withinboththeencoderanddecoderlayerscontainslayernormalisationandaresid-
ualconnection. Layernormalisation(Ba,Kiros,andHinton,2016)playsacrucialrole
in mitigating uninformative variations within the hidden vector values. It achieves
this by normalising the values to a unit mean and standard deviation within each
layer, facilitating more stable and faster training. Residual connection (He et al.,
2016a) is beneficial for easier backpropagation and contributes to a smoother loss
curve. Thesecomponentshelppreventvanishingorexplodinggradients,makingit
easiertotraindeeparchitectures.
FinalOutputSoftmaxLayer
Inthelastoutputlayerofthedecoderstack,alearnedlineartransformationlayeris
used to convert the output vector into logits. This linear layer at the output shares
the same weight matrix with the two input embedding layers of the encoder and
the decoder, with the logits representing the scores against each vocabulary item.
Subsequently,aSoftmaxfunctionisappliedtoconvertthelogitstopredicttheprob-
abilities of the next tokens. Different decoding strategies can then be employed to
samplethetokenstoproducethegeneratedsequence.
2.1.2 TransformerModels
Inthissection,weprovideanoverviewofthreefundamentalcategoriesofthetrans-
formermodels: encoder-onlymodels,decoder-onlymodels,andencoder-decoderor
sequence-to-sequencemodels. Additionally,webrieflyintroducemultimodaltrans-
formers,specificallyvision-languagemodels.
2.1.2.1 Encoder-OnlyModels
Thefirstcategoryofthetransformermodelscomprisesonlytheencoder,withBERT
(Devlinetal.,2019)standingasoneofthemostwidelyadoptedmodelstodate.16 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
BERTistrainedontheEnglishWikipediaandGoogle’sBooksCorpus(Y.Zhuet
al.,2015),focusingontwoprimarypre-trainingobjectives: MaskedLanguageMod-
elling (MLM) and Next Sentence Prediction. Next Sentence Prediction is a binary
classification task that determines whether a given pair of consecutive sentences in
atextcorpusfolloweachotherlogicallyandsemantically,whileMLMenablesbidi-
rectionallearningfromthetextdata. MLMisshowntobemorepotentthaneithera
left-to-rightmodelorashallowconcatenationofleft-to-rightandright-to-leftmod-
els(Devlinetal.,2019).
Initially,thetrainingtextistokenisedusingWordPiece(M.Johnsonetal.,2017),
encompassing a 30K token vocabulary. To prevent the models from directly see-
ing future words, Devlin et al. (2019) propose a masking strategy, which randomly
masks15%ofthetokensinasentenceandtrainsBERTtopredicttheoriginaltoken.
To enhance robustness, the randomly selected tokens to be masked are replaced by
[MASK]80%ofthetime,byarandomtoken10%ofthetime,andleftunchangedthe
remaining10%ofthetime. Thepredictionisoptimisedusingcross-entropy.
MLM has been widely adopted in the training of many models, including pop-
ularonessuchasRoBERTa(YinhanLiuetal., 2019), ALBERT(Lanetal., 2020), and
ELECTRA(K.Clarketal.,2020),amongothers.
Thetokenisedinputtothemodelsisfirstrepresentedastheadditionofthetoken
embedding and the positional embedding, then fed to the stack of transformer lay-
ers,i.e.,theoutputofthelowerlayeristreatedastheinputofthenextlayer,adopt-
ing the same attention mechanism as introduced in Vaswani et al. (2017). BERT is
availableintwodifferentsizes: abasemodelwithatotalof110Mparametersacross
12 transformer layers, and a large model with 340M parameters spanning 24 trans-
formerlayers.
Afterpre-training,BERTcanthenbefine-tunedondownstreamtasks. Thefinal
representation vector in the last layer is used as the encoded representation of the
inputsentence,whichcanlaterbeappliedtovarioustaskssuchastextclassification.
WithBERTsettingthenewstate-of-the-artperformanceinvariousdownstreamNLP
tasks, the pre-training and fine-tuning paradigm has proven to be highly effective
and remains the predominant method for training language models in the current
literature.2.1. TheTransformer 17
2.1.2.2 Decoder-OnlyModels
Thesecondcategoryoftransformermodelsexclusivelyfeaturesthedecoder,where
the input data directly enters the decoder without being transformed into a higher,
moreabstractrepresentationbyanencoder. Representativemodelsinthiscategory
includeGPT(GenerativePre-trainedTransformer)fromOpenAI,alongwithitssuc-
cessiveiterationslikeGPT-2andGPT-3(Radfordetal.,2019;Brownetal.,2020b).
Forinstance,GPT-2usesBytePairEncoding(BPE)(Sennrich,Haddow,andBirch,
2016)totokenisethetextandistrainedontheobjectiveofcausallanguagemodelling
topredictthesubsequenttokeninasequence,typicallywithcross-entropyloss. The
decoder in GPT models employs a specific type of attention mechanism known as
masked self-attention, which allows a token to attend only to previous tokens, pre-
ventingaccesstofuturetokensduringtraining.
Theleft-to-rightdecoder-onlymodelsareeffectiveatgenerationtasksinanAuto-
Regressive (AR) fashion. The process commences by introducing a start token <s>
tothemodel,whichtraversesthroughaseriesoftransformerdecoderlayers,gener-
atingtoken_0conditionedon<s>. Subsequently,token_0becomespartoftheinput
to the model, alongside the start token, used for generating the subsequent tokens.
This sequence continues until the end-of-sequence token <e> is generated, indicat-
ingthecompletionofthesequence. Fine-tunedGPTmodelshavealsodemonstrated
considerablepotentialintaskssuchastranslation,summarisation,andquestionan-
swering(Radfordetal.,2019).
The decoder-only architecture simplifies the model, enhancing its efficiency for
specifictasks,suchaslanguagemodelling. Byeliminatingtheencoder,GPTmodels
canprocessinputdatamoredirectlyandgenerateoutputmorerapidly. Thisdesign
alsoenablesGPTmodelstobetrainedonasubstantialamountofunlabelleddata,a
significantadvantageinNLPwherelabelleddataisoftenlimited. Thankstotheef-
ficiencyandgenerativecapabilities,decoder-onlymodelscontinuetobedeveloped18 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
withimprovedsizeandperformance. MostrecentLLMsadoptdecoder-onlytrans-
former architectures, including GPT-3 (175B),1 OPT (S. Zhang et al., 2022) (125M-
175B),PaLM(Chowdheryetal.,2023)(8B-540B),LLaMA(Touvronetal.,2023a)(7B-
65B),LLaMA2(Touvronetal.,2023b)(7B-70B),tonameafew.
2.1.2.3 Sequence-to-SequenceModels
The next category of the transformer model is the Sequence-to-Sequence (Seq2Seq)
model, also referred to as the encoder-decoder transformer model, which contains
bothanencoderandadecoder. Thisarchitectureishighlyversatileandhasdemon-
stratedeffectivenessinvariousNLPapplications.
BoththeinputandoutputoftheSeq2Seqmodelaresequences. Inputsequences
firstundergoinitialprocessingbytheencoder. Subsequently, thehigh-dimensional
representation is passed on to the decoder through the cross-attention mechanism.
In each decoding step, the decoder attends not only to its previously generated to-
kens but also to the most relevant parts of the input sequence. This is achieved
throughaprocesssimilartoself-attentionbutwithanadditionalstepthatcombines
information from both the history of the decoder and the encoded input, enabling
thegenerationofthenexttokenintheoutputsequence.
Popular models within this category include BART (M. Lewis et al., 2020), T5
(Text-to-Text Transfer Transformer) (Raffel et al., 2020a), among others. BART is
pre-trainedondiversetaskssuchasdenoisingauto-encoding,textinfilling,andtext
generation. This pre-training allows it to excel in various NLP tasks as it learns a
broad range of linguistic patterns. T5, on the other hand, is designed with a text-
to-text framework, unifying diverse NLP tasks into a single system. It showcases
flexibilitybycastingdifferenttasksastextgenerationproblems,allowingaseamless
approachtodifferenttaskswithaunifiedmechanism.
Seq2Seqmodels,arepreferredforhandlingdiverselanguage-relatedtasksbyef-
fectively processing inputs, leveraging cross-attention mechanisms, and generating
high-quality output sequences. Compared to decoder-only models, fewer LLMs
adopt the encoder-decoder transformers, examples include AlexaTM (Soltan et al.,
2022)(20B),Flan-T5(Chungetal.,2022)(80M-11B).
1Numbersinthebracketshowthemodelsizesinthenumberofparameters.2.1. TheTransformer 19
2.1.2.4 MultimodalTransformers
Sincethesuccessofthetransformerarchitectureinlanguageprocessing,ithasbeen
adaptedtoothermodalitiessuchasimageandaudioprocessing.
Vision Transformer (ViT) Transformer Encoder
Class L x +
Bird MLP
Ball Head
Car MLP
...
Norm
Transformer Encoder
+
Patch + Position 0 1 2 3 4 5 6 7 8 9 Multi-Head Embedding Attention
* Extra learnable
[class] embedding Linear Projection of Flattened Patches
Norm
Embedded
Patches
FIGURE 2.3: Vision Transformer. Images are split into fixed-size patches and
linearlyembeddedassequences. Thenpositionembeddingsareaddedandthe
resultingvectorsarefedtoastandardTransformerencoder. Source: Dosovitskiy
etal.(2021).
The Vision Transformer (ViT) model is introduced by Dosovitskiy et al. (2021),
closely following the design principles of the BERT model. An illustration of ViT is
shown in Figure 2.3. Images are split into a sequence of fixed-sized P×P patches,
where P is the patch size. These flattened 2-D image patches are then linearly pro-
jected to match the transformer’s embedding dimension. ViT is pre-trained using
supervisedimageclassificationsuchasImageNet,whichhasdemonstrateditscapa-
bilitytomatchorsurpasspreviousstat-of-the-artCNN-basednetworkslikeResNet
(He et al., 2016b) on common downstream image classification benchmarks, partic-
ularlywhenampledataisavailable(Dosovitskiyetal.,2021).
Buildingonthesuccessofvisiontransformers,theintegrationofmultiplemodal-
ities, such as images and language, has become an active research area. Vision-
languagetransformers,whicharetypicallyencoder-decodertransformers,fusethese
diversemodalities. Intheencodingphase,methodsforvision-languagemodelsare
categorisedintotwoprimaryarchitectures: single-streamanddual-streammodels.
*20 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
Thesingle-streammodelsencodebothmodalitieswithinthesamemodule. Em-
bedded textual input and image features are either concatenated or combined via
cross-modalcross-attention,leveragingQueryvectorsfromonemodalitywhileKey
and Value vectors come from another. Although different vision-language tasks re-
quiredifferentinputformats, single-streamarchitectureisproveneffectiveinmod-
els such as VisualBERT (L. H. Li et al., 2019), V-L BERT (W. Su et al., 2020), OSCAR
(XiujunLietal.,2020),andOFA(P.Wangetal.,2022),etc.
Ontheotherhand,dual-streammodelsemploytwosingle-modalencoderstoin-
dependentlyprocessvisionandtextinformation. Theythenemploystraightforward
methods,suchasshallowattentionlayers(K.-H.Leeetal.,2018)ordotproducts(C.
Jia et al., 2021) to project image and text embeddings into the same semantic space.
Withoutthecomplexcross-attentionmechanisms,thedualencoderstrategyisoften
more efficient in modelling vision-language interactions. Models such as ViLBERT
(Luetal.,2019),andLXMERT(H.TanandBansal,2019)fallintothiscategory.
2.2 Knowledge Types and Sources
Theprecedentsectionsintroducedtheadvancesintransformer-basedlanguagemod-
elsthatsetthenewstate-of-the-artperformanceforawidevarietyofNLPtasks,es-
tablishingthenewparadigmoflearninginformativecontextualisedrepresentations
fortextviatrainingonthelarge-scaleunlabelledcorpus. Whileexistingresearchhas
showcasedtheimplicitstorageofcertainknowledgewithinthelatentparametersof
languagemodels(Petronietal.,2019;Roberts,Raffel,andShazeer,2020;C.Wang,P.
Liu,andYueZhang,2021;B.Caoetal.,2021),PLMsstillfacechallengesinachieving
reliable performance in knowledge-intensive tasks such as fact verification (Thorne
etal.,2018)andquestionanswering(Kwiatkowskietal.,2019),etc.
Addressing this limitation, the research community has actively explored the
integration of explicit knowledge, such as knowledge bases and Wikipedia articles,
into large-scale PLMs. In this section, we provide an overview of knowledge types
andknowledgesources.
Knowledge comprises a wide spectrum of concepts, e.g., Wikipedia states that2.2. KnowledgeTypesandSources 21
Knowledgeisanawarenessoffacts,afamiliaritywithindividualsandsituations,oraprac-
ticalskill.2 InthecontextofNLP,knowledgecanbebroadlyclassifiedintotwocate-
gories: structuredknowledgeandunstructuredknowledge(C.Zhuetal.,2022).
Structured Knowledge includes Knowledge Bases (KB) or knowledge graphs
(KG), dictionaries (W. Yu et al., 2022), syntax-trees (Zhou et al., 2020; Bai et al.,
2021), etc. Knowledge Bases are the most common source of structured knowledge
utilisedinNLP(C.Zhuetal.,2022). Widely-usedKBsincludeWikidata,3 DBpedia,4
WordNet (Miller, 1993), ConceptNet,5 among others. Both Wikidata and DBpedia
are developed around Wikipedia, where structured knowledge is stored as triples,
i.e., <subject, relation, object>, with subject and object being entities. World
knowledge and facts can effectively be represented by such triples, for example,
<United Kingdom, Capital, London>.
German Kepler's laws
Germany is a country in Central … are three scientific laws describing
and Western Europe … the motion of planets around the Sun,
published by Johannes Kepler.
Ethnic group Published by
Johannes Kepler Kepler space telescope
Johannes Kepler was a German launched by NASA … Named
astronomer … best known for after Johannes Kepler.
his laws of planetary motion.
Named after
Operator
Occupation
Astronomer NASA
An astronomer is a scientist in … is an independent agency …
the field of astronomy … for the civilian space program …
FIGURE 2.4: An example of a knowledge graph in Wikidata, which contains
the information of entities, their relations, and the descriptions of the entities.
Source: XiaozhiWangetal.(2021).
Wikidata is a dynamic and collaborative knowledge base that represents and
links entire Wikipedia articles. It provides document-oriented knowledge repre-
sentation, making it a comprehensive source of diverse information. Wikidata also
provides textual descriptions of entities, Figure 2.4 shows an example (source from
2https://en.wikipedia.org/wiki/Knowledge
3https://www.wikidata.org/wiki/Wikidata:Main_Page
4https://www.dbpedia.org/
5https://conceptnet.io/22 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
XiaozhiWangetal.(2021)). DBpediaaimsatextractingstructuredknowledgefrom
Wikipediaandtransformingitintoamachine-readableformat.
WordNet is a lexical database of the English language, which categorises words
intosynsets,makingitparticularlyusefulfortaskssuchaswordsensedisambigua-
tion. ConceptNet is a multilingual knowledge base, representing words, phrases,
and the commonsense relationships between them. The knowledge in ConceptNet
iscollectedfromavarietyofresourcesincludingcrowdsourcing.
Unstructured knowledge takes on various forms, with one prevalent example
beingtextcorporautilisedinlanguagemodelpre-training. Priorresearchhasshown
thatlanguagemodelsstoreormemoriseknowledgeaftertheyaretrained,hencecan
also be treated as an unstructured knowledge source (Petroni et al., 2019; Heinzer-
lingandInui,2021).
Neemanetal.(2023)categoriseunstructuredknowledgeintotwodistinctsources:
parametric knowledge, which is encoded in or memorised by the model parameters,
and contextual knowledge, referring to knowledge encapsulated within external tex-
tualsourcesprovidedtothemodelatinferencetimeofthedownstreamtasks, such
asparagraphsretrievedbasedonthequestionqueryinquestionanswering.
Knowledge is a theme throughout the thesis. In Chapter 3-5, we primarily con-
sider Structured Knowledge, especially the entity and the relations stored in knowl-
edge bases, i.e., WikiData. In Chapters 6 and 7, the exploration of knowledge will
be expanded to more flexible forms, including the unstructured Parametric Knowl-
edge via self-explanation (Chapter 6), and Distilled Knowledge - extraction of knowl-
edgefromrecentLLMstopowersmallertask-specificmodelsviadataaugmentation
(Chapter7). ParticularlyinChapter4,5,and7,wealsostudyzero-shotcross-lingual
transferformultilingualtasks.
2.3 Knowledge-Enhanced Language Models
Nowthatwehavepresentedthefundamentalsoftransformer-basedlanguagemod-
elsandknowledgerepresentation,thissectioncontinuestointroducethetechniques
forcombiningthem. Specifically,wecategoriseknowledgeintegrationmethodsfor
pre-trainedlanguagemodelsinto(i)theincorporationofentityembeddings,(ii)the2.3. Knowledge-EnhancedLanguageModels 23
utilisationofexternalmemory,(iii)theadditionofknowledge-relatedauxiliarypre-
trainingtasks,(iv)theemploymentofadapters,and(v)theaugmentationofretrieval
componentsintothelanguagemodels.
2.3.1 AddingEntityEmbeddings
Thefirstlineofworkinvolvestheidentificationofentitiesinasentence, enhancing
the original input embeddings with entity knowledge represented in triples by in-
corporatingentityembeddings. Entityembeddingscantaketheformofpre-trained
embeddingvectorsorsharetheembeddinglayerofthetransformermodel.
Diverse approaches have emerged to representing entities in knowledge bases
viaentityembeddings. KnowledgegraphembeddingmethodssuchasTransE(Bor-
desetal.,2013)andTransR(Y.Linetal.,2015),employscorefunctionstomeasurethe
distanceoftwoassociatedentitiesbytheirrelation. Theunderlyingintuitionisthat
entitieslinkedbyarelationareproximalinthevectorspace,i.e.,entityembeddings
encapsulate relationship representations from the knowledge graph. Word-entity
co-occurrence methods such as Wikipedia2Vec (Yamada et al., 2020a), concurrently
learnembeddingsofwordsandentitiesbyidentifyingco-occurringwordsarounda
givenentityandplacingsimilarwordsandentitiesincloseproximitywithinacon-
tinuous embedding space. Another category of methods, like BLINK (L. Wu et al.,
2020), utilises entity descriptions encoded by transformer-based PLMs to represent
entities.
Models that integrate knowledge by specific entity embeddings include ERNIE
(ZhengyanZhangetal.,2019),KnowBert(Petersetal.,2019),K-BERT(WeijieLiuet
al.,2020),etc. Thesemodelstypicallyuseanexistingentitylinkerorjointlytrainan
entitylinkertoidentifymentionsofentitiesinknowledgebasesintheinputtext,and
thenretrievethepre-trainedentityembeddingsorembedthecorrespondingentities.
Note that the dimensions of the pre-trained entity embeddings are generally much
lowerthanthoseofthetransformerencoders,whereafusionlayeristypicallyadded
toeffectivelyincorporateentityembeddingsfromadifferentembeddingspaceand
combinecontextandentityinformation.24 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
Token Output Entity Output
e( 1i) e( 2i)
Aggregator
Token Output w1(i) w2(i) w3(i) w4(i) ··· wn(i) e( 1i) e( 2i) Entity Output
Information
Fusion Aggregator
K-Encoder Mx Information Fusion
Multi-Head Multi-Head
Attention Attention
e˜( 1i) e˜( 2i)
Transformer
w˜1(i) w˜2(i) w˜3(i) w˜4(i) ··· w˜n(i) e˜( 1i) e˜( 2i)
Feed Entity Input
Forward Multi-Head Attention Multi-Head Attention
T-Encoder Nx
Multi-Head
Attention
Token Input w1(i 1) w2(i 1) w3(i 1) w4(i 1) ··· w n(i  1) e( 1i 1) e( 2i 1) Entity Input
bob dylan wrote blow 1962 Bob Dylan Blowin’ in the Wind
Token Input Bob Dylan wrote Blowin’ in the Wind in 1962
(a) Model Achitecture (b) Aggregator
FIGURE 2.5: Illustration of ERNIE. ERNIE comprises a conventional text en-
coder and an additional knowledge encoder. An aggregator (on the right) is
applied for the mutual integration of the input tokens and entities. The infor-
mation fusion layer takes both token embedding and the concatenation of the
token embedding and entity embedding as input, and outputs new token em-
beddings and entity embeddings for the next layer. Source: Zhengyan Zhang
etal.(2019).
ERNIE, for instance, employs TAGME (Ferragina and Scaiella, 2010) to link en-
tities to Wikidata. TAMGE identifies entity mentions in the input text, links them
to associated TransE entity embeddings, and fuses them into corresponding posi-
tionsinthetext,asillustratedinFigure2.5. Theknowledge-basedlearningobjective
involvespredictingcorrecttoken-entityalignments, enablingERNIEtooutperform
BERT in tasks including entity typing and relation classification (Zhengyan Zhang
et al., 2019). However, challenges arise from the need for pre-annotated and linked
entities,introducingpotentialnoisethroughentitylinkerslikeTAGME.
KnowBert, on the other hand, extends BERT by jointly training an entity linker
using a knowledge attention and re-contextualisation mechanism, as illustrated in
Figure 2.6. It identifies entity spans in the input text and incorporates an inte-
gratedentitylinkertoretrieveentityembeddingsfromaknowledgebase. Theentity
linkerisresponsibleforentitydisambiguation,considering30entitycandidatesand
using their weighted average embedding. Knowledge-enhanced entity-span rep-
resentations are then re-contextualised with a word-to-entity attention technique.
Whenentity-linkingsupervisionisavailable,themodelistrainedwithanadditional2.3. Knowledge-EnhancedLanguageModels 25
S’e
H i’
7 6
5
+
~
E 4 Prince_(musician)
H’proj Prince_Motor_Company
i + Prince,_West_Virginia
+
Purple_Rain_(album)
Hproj 2 Purple_Rain_(film)
i Purple_Rain_(song)
1
Rain_(entertainer)
H i Rain_(Beatles_song)
Rain_(1932_film)
Prince sang Purple Rain S 3 Se
FIGURE 2.6: Illustration the Knowledge Attention and Recontextualisation
(KAR)componentinKnowBert. BERTwordpiecerepresentations(H )arefirst
i
proj
projectedtoH (1),thenpooledovercandidatementionsspans(2)tocompute
i
S, and contextualized into Se using mention-span self-attention (3). An inte-
gratedentitylinkercomputesweightedaverageentityembeddingsE˜ (4),which
are used to enhance the span representations with knowledge from the KB (5),
computing S′e. Finally, the BERT word piece representations are recontextu-
alised with word-to-entity-span attention (6) and projected back to the BERT
dimension(7)resultinginH′. Source: Petersetal.(2019).
i
knowledge-aware log-likelihood or max-margin objective. KnowBert has demon-
stratedimprovementsinrelationshipextraction, entitytyping, and wordsensedis-
ambiguationoverBERTandERNIE(Petersetal.,2019).
K-BERT expands language representation with adaptable knowledge bases via
aknowledgelayer. Thislayerdetectsandinjectsrelevanttriplesfromaknowledge
baseintotheinputsentence, convertingitintoaknowledge-richsentencetree. The
enrichedsentenceisthenprocessedbytheembeddingslayer. Soft-positionandvis-
ible matrix techniques are employed to control the utilisation of knowledge while
maintaining the semantic meaning of the sentence close to the original input. K-
BERT showcases advantages in knowledge-driven domain-specific tasks, such as
question answering and named entity recognition in fields like law, finance, and
medicine(WeijieLiuetal., 2020). Moreover, K-BERTalsooffersflexibilityinadapt-
ing to different knowledge bases without the need for re-training when changing
theKBintegrated.
2.3.2 UsingExternalMemory
Another category of approaches involves the use of external memory to seamlessly
integrate factual knowledge into language models. This external memory typically26 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
acts as a key-value store, providing access to knowledge base triples or contextual
information. Thismethodologycanbetterfacilitatetheincorporationandupdating
ofknowledgefromknowledgebasesintoPLMs,oftenwithouttheneedforextensive
re-training.
ModelsthatleverageexternalmemoryincludeKGLM(Loganetal.,2019),KNN-
LM (Khandelwal et al., 2020), and others. The key idea of KGLM is to condition
the language model on a knowledge graph. Unlike most of the models discussed
in this thesis, KGLM uses LSTM instead of transformer-based PLMs. In addition
to predicting the next word given the previous words in the sequence, KGLM also
predicts the next entity given the previous words and entities in the sequence. As
KGLMiteratesoverthesequence,itbuildsalocalknowledgegraphwhichisasubset
of the full KG with only entities relevant to the sequence. LSTM is then used to
predictthenextwordaswellasitstype: relatedentity,anewentity,ornotanentity.
KGLMisfoundtooutperformGPT-2onfactcompletiontasks. Qualitatively,KGLM
tendstopredictmorespecifictokenscomparedtoGPT-2.
KNN-LMlearnsthesimilaritiesbetweentextsequencesandstoresallrepresenta-
tionsoftextsequencesinanearestneighbourdatastore. Duringinference,KNN-LM
identifies the k most similar sequences in the data store for a given input, retrieves
thecorrespondingtarget(e.g.,thenextword)fromtheseksequences,andcombines
the KNN probabilities with the probabilities computed by the language model for
thefinalprediction.
2.3.3 AddingKnowledge-RelatedAuxiliaryPre-trainingTasks
The next set of methods focuses on integrating knowledge by designing auxiliary
knowledge-related pre-training tasks, extending beyond traditional training objec-
tives like masked language modelling. These tasks encompass Masked Entity Pre-
diction,EntityPredictionfromDescriptions,Entity-RelationDiscrimination,etc.
Many models in the literature fall within this category. Since the model archi-
tecture is typically not modified, these methods provide flexibility and entail no
additional inference overhead during deployment. Below we review some repre-
sentativemodelsinthiscategory.2.3. Knowledge-EnhancedLanguageModels 27
KE Loss + MLM Loss
h r t
Encoder Embeddings Encoder Encoder
<s> Johannes Kepler was a German astronomer … <s> An astronomer is a scientist in the field of … … Kepler <mask> to have had an epiphany on …
texth
h r t
textt
( Johannes Kepler , O ccupation, Astronomer )
Knowledge Graph Text
FIGURE2.7: IllustrationoftheKEPLERframework. Alanguagemodelisjointly
trainedonknowledgeembedding(KE)andmaskedlanguagemodelling(MLM)
objectives, where three types of entity embeddings are studied for the knowl-
edge embedding task: entity descriptions as embeddings, entity and relation
descriptions as embeddings, and entity embeddings conditioned on relations.
Source: XiaozhiWangetal.(2021).
Xiongetal.(2020)trainamodel,WKLM,usingWikipediaarticles. Thementions
in the text are replaced with different entities of the same type from WikiData to
create negative knowledge statements that are still linguistically correct. An entity
replacementlossisintroducedtotrainWKLMtodistinguishbetweentrueandfalse
knowledge,togetherwiththeMLMobjectiveforpre-training. Sunetal.(2019)adopt
a masking strategy at both the phrase and entity levels during training. This intro-
duces tasks that necessitate factual knowledge for model comprehension. Roberts,
Raffel, and Shazeer (2020) leverage salient span masking introduced by Guu et al.
(2020), to mask out salient spans such as entities and dates. This approach demon-
stratesenhancementsoverT5,particularlyinquestionanswering.
KEPLER(XiaozhiWangetal.,2021),illustratedinFigure2.7,introducesaknowl-
edge embedding objective with supervision from a knowledge base and optimises
jointly with language modelling objectives. KEPLER is specifically trained to en-
codeentitiesfromtheircontextualdescriptions,enhancingtheabilityofPLMstoex-
tractknowledgefromtext. Calixto,Raganato,andPasini(2021)trainamultilingual
model using Wikipedia articles in 100 languages together with BabelNet (Navigli
and Ponzetto, 2012), a multilingual sense-inventory for Word Sense Disambigua-
tion,bypredictingWikipediahyperlinks.
One of our works from this thesis, EntityCS (Whitehouse, Christopoulou, and
Iacobacci, 2022), detailed in Chapter 4, also utilises the entity prediction objectives28 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
Language Pre-train Model task1
... ... ...
TRM TRM TRM TRMM
Pre-train
task2
(a) Multi-Task Learning
Lingustic Knowledge
Gotham Citp ylace of birth date of birt Mh ay 1939
TRM
...
TRM
...
TRM
... L Ta M Rng o Mu d Ma eg le
publisher
Batman
creator
Pre-train
task1
DC Comics
writer-artistis a
Bob Kane KIA ... KIA ... KIAK
Adapter1
Pr te a- str ka 2in
Factual Knowledge ... ... Adapter2
KIA KIA KIAK
(b) K-Adapter
FIGURE 2.8: IllustrationofK-ADAPTER.Comparedtoconventionalmulti-task
learning for injecting knowledge (a) that may result in catastrophic forgetting,
K-ADAPTER (b) injects knowledge by training adapters independently on dif-
ferent pre-train tasks, enabling continual knowledge infusion. When injecting
new kinds of knowledge, the existing knowledge-specific adapters will not be
affected. KIArepresentstheadapterlayerandTRMrepresentsthetransformer
layer. Source: R.Wangetal.(2021).
onacode-switchedtrainingcorpusconstructedfromWikipediaandWikidata.
2.3.4 AddingAdapters
Adaptersaretypicallylight-weightedorsmallertransformerblockswithfewerlay-
ers. Keeping the pre-trained language model parameters frozen, the adapter-based
modelsprovideefficiencyintrainingandshowadvantagesinmitigatingtheriskof
catastrophic forgetting (T. Vu et al., 2022). In addition to the common use case of
adapter for parameter-efficient fine-tuning (Houlsby et al., 2019), researchers have
also developed models utilising adapters to integrate knowledge into PLMs. We
brieflyintroduceseveralsuchmodelsbelow.
K-ADAPTER (R. Wang et al., 2021), as illustrated in Figure 2.8, adds learnable
adapters to RoBERTa that are trained in a multi-task setting on relation prediction
and dependency-tree prediction. Two types of knowledge adapters are developed:
factual knowledge obtained from automatically aligned text triples on Wikipedia
and Wikidata, and linguistic knowledge obtained via dependency parsing. Both
Input
Tokens
Input
Tokens
concatenate
concatenate2.3. Knowledge-EnhancedLanguageModels 29
adapters have demonstrated effectiveness in improving relation classification, en-
tity typing, and question answering (R. Wang et al., 2021). Y. Hou et al. (2022) in-
troduce a lightweight adapter set to enhance multilingual PLMs with cross-lingual
entity alignment and facts from multilingual KB for many languages. Their experi-
ments showcase the benefits of incorporating multilingual factual knowledge, par-
ticularlyforlow-resourcelanguages. Manyworksfocusonenhancingtasksrequir-
ing domain-specific knowledge with adapters, for instance, Biomedical NLP (Lai,
Zhai,andH.Ji,2023),Task-OrientedDialogueSystems(Emelinetal.,2022),aswell
astheexplorationofthemixtureofdomainadapters(Diaoetal.,2023).
2.3.5 Retrieval-AugmentedLanguageModels
Retrieval-augmented approaches have gained popularity for expanding contextual
knowledgewithouttheneedforextensivemodelparametrisation. Itinvolvestrain-
ingaretrieverthatdynamicallyretrievesrelevantknowledge,suchaspassagesfrom
Wikipediaorsub-graphsfromaKBatruntime. Withoutthenecessityofstoringvast
amountsofknowledgewithinthemodel,retrieval-augmentedlanguagemodelsen-
ablemoreefficientandconvenientupdatesoftheevolvingknowledge.
Retrieval-augmentedmodelsareextensivelyappliedtoquestion-answeringand
text-generationtasks. M.Joshietal.(2020)encodequestionsandpassagesalongside
dynamically retrieved textual encyclopedic background knowledge from multiple
documents, particularly focusing on entities mentioned in the text. This method
exhibits effectiveness in tasks emphasising factual reasoning, such as reading com-
prehension.
RAG,Retrieval-AugmentedGeneration,proposedbyP.Lewisetal.(2020),gen-
eratesanswersbyretrievingrelevantspansacrossexternaltextsbasedonpre-trained
sequence-to-sequencemodels. AsillustratedinFigure2.9,givenaquery,RAGlever-
agestheinputsequencetoretrievethetopKrelevantpassagesandgeneratesoutput
byconditioningontheselatentdocumentstogetherwiththeinput. REALM,anex-
tension of RAG proposed by Guu et al. (2020), augments the language model by
retrievingandattendingoverdocumentsfromalargecorpus. Itconsistsoftwokey
components: aneuralknowledgeretrieverimplementedwiththeBERTframework,30 Chapter2. Background: Transformer,Knowledge,Knowledge-EnhancedPLMs
Define "middle ear"(x) The middle ear includes
End-to-End Backprop through q and p θ the tympanic cavity and
Question Answering: the three ossicles. (y)
Question Query EQ ncu oe dry
er
Retriever p
η
Do Ic nu dm exent Generator pθ Q Au ne ss wti eo rn
G
A en ns ew rae tr ii on ng:
(Non-Parametric) (Parametric)
B ba or ra nc k i nO b Ha am wa a iw ia .(s x) q(x) d(z) z4 supports (y)
Fact Verification: Fact Query z3 Margin- Fact Verification:
z2 alize Label Generation
T Ch oe m eD di yv (i xn )e q MIPS z1 p θ T ih si s d i1 v4 it dh e dc e in nt tu or y 3 work
Jeopardy Question sections: "Inferno",
Generation: "Purgatorio" &
Answer Query "Paradiso" (y)
Question Generation
FIGURE 2.9: Illustration of RAG. RAG combines a pre-trained retriever (Query
Encoder+DocumentIndex)withapre-trainedSeq2Seqmodel(Generator)and
fine-tune end-to-end. For query x, Maximum Inner Product Search (MIPS) is
utilised to find the top-K documents z. For the final prediction y, the retrieved
documents are treated as latent variables and marginalised over Seq2Seq pre-
dictionsgivendifferentdocuments. Source: P.Lewisetal.(2020).
responsible for encoding input data and retrieving potentially helpful documents,
andaknowledge-augmentedencoderimplementedwithaTransformer,usedtoin-
fuseentitiesindocumentsandpredictwordsforquestion-answeringtasks.
Retrieval augmentation has also shown advantages in enhancing the few-shot
learning capability of language models. Izacard et al. (2023) introduce a retrieval-
augmentedlanguagemodel,Atlas,whichisdesignedtohandleknowledge-intensive
tasks with very few training examples. Atlas demonstrates robust few-shot perfor-
manceacrossadiverserangeoftasks, includingKILT(Petronietal., 2021), Natural
Questions(Kwiatkowskietal.,2019),etc.
Many new retrieval-augmented approaches have been proposed in the light of
LLMs(B.Wangetal.,2023;Gaoetal.,2023;Asaietal.,2024),amongwhich,self-RAG
(Asaietal.,2024)trainaLMwithreflectiontokens. Thereflectiontokensdetermineif
retrievalwouldbehelpfulandcriticiseitsownoutputtochoosethebestgeneration
pathintermsoffactualityandoverallquality. Thankstotheeffectivenessofextend-
ingnon-parametricanddynamicknowledge,retrieval-augmentedlanguagemodels
maintain their popularity even amid the latest trends of advanced Large Language
Models (J. Liu et al., 2023; Patil et al., 2023), alleviating the need for frequent and
expensivere-trainingofever-largerlanguagemodels.2.4. Summary 31
2.4 Summary
The background section provides an overview of the transformer architecture, rep-
resentative transformer-based models, knowledge types and sources, and various
knowledge-enhanced language models. The subsequent chapters focus on distinct
research aspects, with knowledge as a theme throughout, starting with the next
chapter on the application of knowledge-enhanced language models for fake news
detection.33
Chapter 3
Knowledge-Enhanced Language
Models for Fake News Detection
This chapter focuses on evaluating the effectiveness of knowledge-enhanced lan-
guage models on fake news detection tasks. Specifically, we consider structured
knowledge,suchasentityknowledgerepresentedinknowledgebases.
Themaincontentisanextendedversionofthepaper“EvaluationofFakeNews
Detection with Knowledge-Enhanced Language Models” (Whitehouse et al., 2022),
publishedintheSixteenthInternationalAAAIConferenceonWebandSocialMedia.
3.1 Background and Introduction
This chapter studies the fake news detection task, which includes misinformation,
disinformation, rumours, hoaxes, and other forms of rapid spread and factually in-
accurate information (Sharma et al., 2019a). Due to the wide reach of social me-
dia,fakenewshasbeenobservedtoseverelyimpactpoliticalprocesses(Allcottand
Gentzkow, 2017). Misinformation related to medical issues, such as the COVID-19
pandemic, can cost lives (O’Connor and Murphy, 2020). There is a growing desire
to develop automated methods for fake news detection and mitigation, however, it
remainsatechnicallychallengingproblem(ThorneandVlachos,2018).
We focus on content-based fake news detection: methods that assess the truth-
fulness of news items based only on textual information without using metadata.
State-of-the-artmodelsforthistaskaredrivenbyadvancesinlarge-scalepre-trained
language models (PLMs) (C. Liu et al., 2019; Kaliyar, Goswami, and Narang, 2021),34 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
which are trained on vast amounts of raw web-based text using self-supervised
methods (Rogers, Kovaleva, and Rumshisky, 2020). As discussed in Chapter 2, a
major limitation of these models is the lack of explicit grounding to real-world en-
tities and relations, which makes it difficult to recover factual knowledge (Bender
et al., 2021). On the other hand, knowledge bases (KBs) provide a rich source of
structured and human-curated factual knowledge, often complementary to what is
found in raw text. This has recently led to the development of KB-augmented lan-
guage models (Zhengyan Zhang et al., 2019; Peters et al., 2019). We posit that fake
news detection can particularly benefit from the integration of KBs, making such
modelslessdependentandreliantonsurface-levellinguisticfeatures.
Inthischapter,weempiricallyanalysetheimpactofrecentstate-of-the-artknowl-
edge integration methods, which enhance PLMs with KBs, for content-based fake
newsdetectiontasks. WeevaluateERNIE(ZhengyanZhangetal.,2019),KnowBert
(Petersetal.,2019),KEPLER(XiaozhiWangetal.,2021),andK-ADAPTER,(R.Wang
etal.,2021)ontwodistinctpubliclyavailabledatasets: LIAR(W.Y.Wang,2017),apo-
liticallyorienteddataset,andCOVID-19(Sharmaetal.,2019a),adatasetrelatedtothe
COVID-19 pandemic. We find that integrating knowledge can improve fake news
detectionaccuracy,giventhattheknowledgebasesarerelevantandup-to-date. Our
experimentsarenotdesignedtofindnewstate-of-the-artmodelsforthesedatasets,
buttoinvestigatetheeffectofknowledgebaseintegrationintoPLMs.
Themaincontributionsofthischapterareasfollows:
• WesystematicallyassessvariousKnowledgeBaseintegrationmethodsforfake
newsdetection. Tothebestofourknowledge,thisisthefirststudytowardsthe
effectivenessofknowledge-enhancedlanguagemodelsonfakenewsdetection
tasks.
• Weinvestigatebothmodelanddataaspectsthatmayhindertheeffectiveness
ofKBintegrationorposechallengesinitsaccuratemeasurement.
• We analyse and discuss the potential real-world applications of knowledge-
enhancedlanguagemodelsforfakenewsdetection,includingdynamicadap-
tation,adversarialrobustness,andtheneedforhumanverificationinpractical
deploymentscenarios.3.2. RelatedWork 35
In the following sections, we present a brief overview of four state-of-the-art
methodsthatintegrateKBswiththePLMsstudied. Wethenintroduceandcompare
the datasets, the experiments with different knowledge-enhanced models, and the
effectivenessofentitylinking. Wediscussourfindingswithrespecttothenecessary
conditionsforKBintegrationtobeeffectiveandhowtoassessitseffectinapplication
scenarios. Finally, we discuss the challenges in fake news detection and promising
futuredirections.
3.2 Related Work
A detailed review of related work on knowledge-enhanced language models is in-
cluded in section 2.2. This section presents an overview of related work on fake
newsdetection.
3.2.1 ApproachestoFakeNewsDetection
Fakenews,includingdisinformation,misinformation,rumours,hoaxes,etc. (Sharma
et al., 2019b), poses significant risks to society. The widespread influence of so-
cial media can shape public opinion and manipulate political elections (Allcott and
Gentzkow, 2017), and in the case of misinformation related to medical fields and
health problems, such as the COVID-19 pandemic, even lead to direct loss of life
(O’ConnorandMurphy,2020).
Automated and accurate fake news detection and mitigation represent critical
yettechnicallychallengingproblems(Sharmaetal.,2019a;J.Su,Cardie,andNakov,
2023). Overthepastdecade,variousfakenewsdetectionmethodsusingdeeplearn-
ing techniques have emerged. These methods can be categorised into three types:
content-basedapproaches,userbehaviourorpropagationpatternanalysis,andhy-
bridmodelscombiningboth.
Content-basedMethods
Content-based methods focus on the textual statements of fake news (Oshikawa,
Qian, and W. Y. Wang, 2020) and analyse the language features of the content. For
instance, fakenewsarticlesandpostsoftencontainmorenegativeandexaggerated36 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
words, which can be leveraged to assess the truthfulness of the news (Rubin et al.,
2016).
Themajorityofcurrentcontent-basedmethodsemployneuralnetworksfortext
analysis. For example, Convolutional Neural Networks, widely used in text clas-
sification tasks (Oshikawa, Qian, and W. Y. Wang, 2020), have demonstrated su-
perior results on the LIAR dataset compared to traditional neural networks (W. Y.
Wang,2017). Inrecentyears,larger-scalepre-trainedtransformer-basedmodelslike
BERThavesignificantlyadvancedthestate-of-the-artformanyNLPtasks,including
content-basedfakenewsdetection(Kaliyar,Goswami,andNarang,2021;Farokhian,
Rafe,andVeisi,2023;J.Su,Cardie,andNakov,2023). Mostfakenewsdetectionap-
proaches either combine text with metadata (Ding, Y. Hu, and H. Chang, 2020) or
focussolelyonthesourceofthetext(Nørregaard,B.D.Horne,andAdali,2019).
In terms of the two specific datasets we study, for LIAR, Alhindi, Petridis, and
Muresan (2018) extend the data with evidence sentences in a new dataset LIAR-
PLUS to enhance detection. Chernyavskiy and Ilvovsky (2020) introduce a Deep
Averaging Network to model the discursive structure of the text and use Siamese
models on the extended text data. Additionally, C. Liu et al. (2019) predict labels
at two levels of granularity. In the context of the COVID-19 dataset, results from
theCONSTRAINTS2021workshop(Chakrabortyetal.,2022)showcaseavarietyof
traditionalandneuralNLPmodels. Notably,noneoftheseapproachesincorporates
externalknowledge,suggestingpotentialbenefitsfromknowledgebaseintegration.
UserBehaviour-basedMethods
In contrast to content-based fake news detection, user behaviour-based methods
contend that algorithms focusing on clues from news content are generally less ef-
fective. This is because fake news is often intentionally crafted to mislead users by
mimicking true news (Shu, S. Wang, and Huan Liu, 2019). Instead, these methods
studythesocialcontextduringthenewsdisseminationprocessonsocialmedia,in-
cludinguserprofilesanduserbehaviourssuchaslikesorretweets.
Shu,S.Wang,andHuanLiu(2019)proposeatri-relationshipembeddingframe-
workcalledTriFN,whichsimultaneouslymodelspublisher-newsrelationsanduser-
news interactions for fake news detection. Another approach by Monti et al. (2019)3.3. ModelsandDatasets 37
involvesanautomaticfakenewsdetectionmodelbasedongeometricdeeplearning,
offeringlanguageindependenceandimprovedresiliencetoadversarialattacks.
HybridMethods
Thethirdcategoryoffakenewsdetectionmethodsinvolveshybridmodelsthatcom-
binecontentanduserbehaviouranalysisduringfakenewspropagation.
An example of a hybrid deep model is the CSI model (Ruchansky, Seo, and Yan
Liu, 2017). CSI comprises three modules: Capture, Score, and Integrate, correlating
the characteristics of the news text, user response, and publisher behaviour. The
Capturemodulecapturestemporalpatternsofuserresponsestoaspecificnewsar-
ticle and extracts latent features of the text using RNN. The Score module learns
source characteristics based on user behaviour, scoring publishers based on user
conduct. In the final Integrate module, the model combines response, text, and
sourceinformationtoclassifyeachnewsitemasfakeorreal.
3.3 Models and Datasets
After presenting the background and related work regarding fake news detection,
wenowintroducethemodelsthatweuseinourexperimentsandprovidedetailsof
thedatasetsandourexperimentalsetup.
3.3.1 Knowledge-EnhancedPLMs
In Chapter 2, we have comprehensively reviewed various methods for integrating
knowledge into PLMs, and in this chapter, we specifically focus on the following
fourmodels:
ERNIE enhances BERT (Devlin et al., 2019) by introducing knowledge through
pre-training on both extensive corpora and KBs. While retaining the text encoder
of BERT, ERNIE adds an additional knowledge encoder. The knowledge encoder
followsthestandardtransformerarchitecture, witheachlayerapplyingmulti-head
attentionoverentityembeddingsandtokenembeddings. Afusionlayerthencom-
binestheoutputoftheattentionheads. ERNIEusesTAGME(FerraginaandScaiella,38 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
2010)tolinkentitiestoWikidata. TAMGEidentifiesentitymentionsintheinputtext
and links them to associated TransE (Bordes et al., 2013) entity embeddings, which
arethenfusedintothecorrespondingpositionsofthetext. ApartfromMaskedLan-
guageModellingandNextSentencePredictionpre-trainingtasksasinBERT,ERNIE
alsoadoptsaknowledge-basedlearningobjectivewhichpredictsthecorrecttoken-
entity alignment. ERNIE demonstrate superior performance over BERT in entity
typingandrelationclassification(ZhengyanZhangetal.,2019).
KnowBert incorporates KBs into BERT using knowledge attention and contextu-
alisationmechanism. Itidentifiesentityspansintheinputtextandincorporatesan
integrated entity linker in the model to retrieve entity embeddings from a KB. The
entitylinkerisresponsibleforentitydisambiguation,whichconsiders30entitycan-
didates and uses their weighted average embedding. Knowledge-enhanced entity-
spanrepresentationsarethenre-contextualisedwithaword-to-entityattentiontech-
nique. When entity-linking supervision is available, the model is trained with an
additionalknowledge-awarelog-likelihoodormax-marginobjective. KnowBerthas
shown improvement over BERT in relationship extraction, entity typing and word
sensedisambiguation(Petersetal.,2019).
KEPLER integratesfactualknowledgeintoPLMsbyaddingaknowledgeembed-
ding objective with the supervision from a KB and optimising it jointly with lan-
guage modelling objectives. KEPLER is trained to encode the entities from their
contextual descriptions, which enhances the ability of PLMs to extract knowledge
fromtext. BykeepingtheoriginalstructuresofPLMs, KEPLERcanbeusedingen-
eraldownstreamNLPtaskswithoutadditionalinferenceoverhead. Specifically, af-
ter training on Wikidata5M,1 a large KB-aligned dataset with entity descriptions,
KEPLER shows improved performance over RoBERTa (Yinhan Liu et al., 2019) in
relationshipextraction,entitytypingandlinkprediction(XiaozhiWangetal.,2021).
K-ADAPTER retains the PLMs unchanged, but adds learnable adapter features
that are trained in a multi-task setting on relation prediction and dependency-tree
prediction. Two kinds of knowledge adapters have been developed by R. Wang
1https://deepgraphlearning.github.io/project/wikidata5m3.3. ModelsandDatasets 39
et al. (2021): factual knowledge obtained from automatically aligned text triples on
Wikipedia and Wikidata, and linguistic knowledge obtained via dependency pars-
ing. Bothadaptershavedemonstratedeffectivenessinimprovingrelationclassifica-
tion,entitytyping,andquestionanswering(R.Wangetal.,2021).
3.3.2 FakeNewsDatasets
In our experiments, we use LIAR and COVID-19 to study fake news detection. Both
datasetsconsistofshortstatements,however,theydifferincontent,collectiontime-
lines,andlinguisticfeatures.
LIAR wascollectedin2017fromPolitifact.2 Itincludes12.8khuman-labelledshort
statements about US politics from various contexts, i.e., news releases, TV inter-
views, campaign speeches, etc. Each statement has been rated for truthfulness by
aPolitifacteditorusingasix-gradescale: pants-fire,false,barely-true,half-true,mostly
true,andtrue. LIARalsoprovidesmetadata(e.g.,speaker,context),whichwedonot
use in our experiments. While W. Y. Wang (2017) has been widely cited, we only
found three other results for our specific task (no metadata, six classes) (Alhindi,
Petridis, and Muresan, 2018; C. Liu et al., 2019; Chernyavskiy and Ilvovsky, 2020),
thelatterhasthebestaccuracyof34.5%.
COVID-19 wascollectedin2020aftertheCOVID-19outbreak. Itconsistsof10.5k
posts related to the pandemic which are obtained from different social media sites
including Twitter, Facebook, and Instagram. The fake posts were collected from
variousfact-checkingwebsites,i.e.,PolitifactandNewsChecker,3 andtherealposts
werefromTwitterusingverifiedTwitterhandles. Eachposthasalabel,realorfake. It
wasusedasasharedtaskintheCONSTRAINT2021workshop(Chakrabortyetal.,
2022)withthebest-reportedaccuracyof98.69%.
LinguisticFeatureAnalysis
We perform a linguistic feature analysis following the work in B. Horne and Adali
(2017)toinvestigatethestylisticdifferencebetweenrealandfakenewsinthedatasets.
2https://www.politifact.com
3https://newschecker.in/40 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
Real 10 Real
35
17.5 Fake Fake
30 8
15.0
25
12.5
6
20
10.0
15 4
7.5
10
5.0 2
5
2.5
0
0
LIAR COVID-19 NOUNPROPN VERB ADJ ADV PUNCT NUMS
(A)Wordcountperstatement (B)POS,punctuation,numbersinLIAR
17.5
Real
Fake
15.0
12.5
10.0
7.5
5.0
2.5
0.0
NOUNPROPNVERB ADJ ADV PUNCTNUMSHTTPS
(C)POS,punctuation,numbers,httpsinCOVID-19
FIGURE 3.1: Numberofwords, POStags, punctuation,andnumber-likewords
perstatementinLIARandCOVID-19, aswellasnumberofhttps-linksperstate-
mentinCOVID-19. Themeanvaluesareshownaswhite-filledcirclesintheplots.
WeuseSpaCy4 toparsethestatementsandtoobtainthePart-of-Speech(POS)tags.
For LIAR, we group pants-fire, false, and barely-true as fake and half-true, mostly true,
and true as real. We compare the distribution of various features including word
count,POStags(NOUN,PROPN,VERB,ADJ,ADV),punctuation,andnumber-like
words,ineachstatement. TheresultsarepresentedinFigure3.1.
In terms of statement length, COVID-19 exhibits notable disparities between real
and fake classes, with averages of 32 and 22 words, respectively, as illustrated in
Figure 3.1a. Conversely, LIAR demonstrates a more balanced distribution of state-
ment lengths, with both real and fake statements comprising an average of 18 and
4https://spacy.io/
tnemetats
rep
sdrow
fo
rebmuN
tnemetats
rep
ecnerruccO
tnemetats
rep
ecnerruccO3.4. ExperimentalSetup 41
17words,respectively.
Ingeneral,COVID-19presentsdistinctlinguisticfeaturesbetweenclasseswhereas
LIAR shows more comparable features. Notably, COVID-19 incorporates links, pre-
dominantlyintheformofhttpslinks,formingaseparatecategorywithamarkedly
skeweddistribution,asdepictedinFigure3.1c.
3.4 Experimental Setup
Toassesstheinfluenceofexternalknowledge,wecomparetheperformanceofeach
knowledge-enhancedPLMwiththecorrespondingbaselinemodel. Itisnoteworthy
that ERNIE and KnowBert incorporate entity embeddings linked to the input, al-
lowingustovisualisetheentitiescontributingtofakenewsdetectioninERNIEand
designexperimentstoprobetheimpactofentitydisambiguationinKnowBert.
For the evaluation of model performance in fake news detection, we fine-tune
the knowledge-enhanced PLMs on the training set, employing consistent hyper-
parameter settings. The input text undergoes processing by the PLM, followed by
a dropout (p = 0.1) and a linear layer. The output is then directed to a softmax
layerforclassification. TheoptimisationiscarriedoutusingtheAdamWoptimiser
(LoshchilovandHutter,2019)withalearningrateof5e−6,andcross-entropyserves
as the loss function. We set the maximum input length to 128, and the batch size is
fixedat4. Trainingisconductedfor10epochs,withconvergencetypicallyobserved
afterfiveepochs. Eachexperimentisrunfivetimes,andtheaverageaccuracy,along
withthestandarddeviation,isreported.
Asforthemodelsweuse,ERNIEandKnowBertarebuiltonBERT-base,whereas
KEPLER and K-ADAPTER are enhanced from RoBERTa-base and RoBERTa-large,
respectively. In terms of hardware, a Nvidia GTX 1080 GPU with 12GB of VRAM
is utilised for experiments involving BERT-base, RoBERTa-base, and models based
on them. To maintain a consistent batch size for RoBERTa-large and K-ADAPTER
models,aT4with16GBVRAMisemployed.42 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
3.5 Results and Discussion
We compare ERNIE, three pre-trained KnowBert models with different KBs (Wiki,
WordNet,W+W),5 KEPLER,andK-ADAPTERwiththreeadapters(F,L,F-L)6 inthe
publishedimplementation,tothecorrespondingbaselines. Thissectionpresentsthe
mainresultsanddiscussion.
3.5.1 DetectionAccuracy
MODEL BASE LIAR COVID-19
BERT-Base(BB) - 26.36±0.58 97.51±0.19
RoBERTa-Base(RB) - 26.71±0.93 97.61±0.26
RoBERTa-Large(RL) - 27.36 ±0.79 97.92 ±0.17
ERNIE BB 27.53±0.13 97.30±0.18
KnowBert-Wiki BB 27.64±0.09 97.37±0.09
KEPLER RB 26.77±1.15 97.58±0.15
K-ADAPTER-F RL 28.63 ±0.90∗ 97.92 ±0.10
KnowBert-WordNet BB 26.95±0.45 97.00±0.06
KnowBert-W+W BB 28.95 ±0.64∗ 97.56±0.15
K-ADAPTER-L RL 28.46±0.87∗ 98.07±0.09
K-ADAPTER-F-L RL 27.45±0.78 98.11 ±0.14
TABLE 3.1: Detection accuracy results (average of five runs). The first section
correspondstothebaselinemodels. ModelsinthesecondsectionuseWikidata
KB.ThethirdsectionshowsmodelsusingotherKBsandfeatures. Thebestval-
ueswithineachsectionperdatasetaremarkedinbold. Thesubscriptnumbers
with ± show the standard deviation. Results with ∗ indicate statistically sig-
nificant improvements over the baseline, both for the mean (t-test, one-sided,
p < .05)andmedian(Wilcoxonsignedranktest,one-sided, p < .05).
Thedetectionaccuracyoftheknowledge-enhancedPLMsandtheircorrespond-
ingbaselinesisdetailedinTable3.1.
Todisentangletheeffectivenessofknowledgeintegrationmethods,specificknowl-
edgeresources,andbaselinemodelarchitectures,wecategorisetheresultsintothree
groups: baseline performance (top), models integrated with Wikidata knowledge
base (middle), and models utilizing knowledge bases beyond Wikidata (bottom).
Comparing models with Wikidata knowledge and their baselines, we observe con-
sistentenhancementsontheLIARdataset. However,noimprovementisobservedon
5TheyrefertoWikipedia,WordNet,andWikipedia+WordNetastheknowledgebase.
6TheyrefertoFactual,Linguistic,andFactual+Linguisticadapters.3.5. ResultsandDiscussion 43
the COVID-19 dataset. This discrepancy underscores the positive impact of relevant
knowledge bases on transformer-based models, irrespective of the diverse integra-
tion approaches employed. Conversely, in scenarios where knowledge bases are
outdated and datasets exhibit stylistic imbalances between classes (as outlined in
section3.3.2),externalknowledgebasesmayevendetrimentallyaffectperformance.
Comparing models employing the same knowledge integration approach but
utilising different knowledge sources reveals intriguing findings. Optimal perfor-
mance on both LIAR and COVID-19 datasets is achieved by models incorporating
multipleknowledgesources. Specifically,KnowBERT-W+W,incorporatingbothWiki-
dataandWordNetknowledgebases,exhibitsthemostsubstantialoverallimprove-
ment (a notable increase of +2.59 over BERT-base). Simultaneously, K-ADAPTER,
incorporatinglinguisticadapters,demonstratesthemostsignificantpositiveimpact
on the COVID-19 dataset. This aligns with the observation that COVID-19 exhibits
distinctlinguisticstylisticcuesbetweendifferentclasses.
Overall, acrosstheLIARdataset, allknowledge-enhancedmethodsdemonstrate
improvementsoverthebaseline. Incontrast,ontheCOVID-19dataset,onlythreeout
ofeightmodelsshowimprovement,andtheseimprovementsaremarginal
The computational cost varies across different approaches. KEPLER maintains
thebaselinePLMarchitecture,incurringnoadditionaloverhead. ForK-ADAPTER,
the RoBERTa-large layers are frozen, resulting in a manageable overhead ranging
from 9-23% due to the adapters. However, for KnowBert, the overhead is more
substantial,rangingfrom40-87%,andforERNIE,itisevenhigherat111-131%.
3.5.2 KBLinking
ERNIE and KnowBert establish links between the text and KB entities at runtime,
andthequalityofthislinkingsignificantlyimpactstheoutput. ERNIEutilisesTAGME
andselectsonlyoneentitycandidatepertextspan. InFigure3.2,wepresentthe50
mostfrequentlyselectedKBentitiesforeachdataset.
InthecaseofCOVID-19,themostfrequentlyselectedentitiesdonotappearcontent-
related, such as “https” and “twitter”, while the highly relevant term “COVID-19”
isnotablyabsentinthelinkedentities. Conversely,forLIAR,thelinkedentitiesseem44 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
(A)LinkedEntitiesinCOVID-19
(B)LinkedEntitiesinLIAR
FIGURE 3.2: Word cloud for the 50 most frequent entities linked by ERNIE in
LIARandCOVID-19.
morerelevant. ThisdiscrepancymaybeattributedtothefactthatLIARwascollected
threeyearsearlier,makingitpotentiallybettersuitedfortheentitylinkerandtheKB
used. Another potential factor influencing the effectiveness of KB integration is the
number of linked entities. Unlike ERNIE, KnowBert selects the 30 most probable
entitiespertextspan. Inasensitivitystudy,werestrictKnowBert-W+Wtoonlyone
entity, leading to a reduction in accuracy on LIAR from 28.95% to 27.31%, placing it
belowtheaccuracyofERNIE(27.53%).
3.5.3 Discussion
The consistent improvement in detection accuracy on LIAR achieved by integrating
PLMs with the Wikidata KB demonstrates the potential of knowledge integration,
surpassing results obtained by W. Y. Wang (2017) which integrated multiple types
ofmetadata. However,theimprovements,whilenotable,arenotdramaticforLIAR,
and they lack consistency for COVID-19. Two critical aspects contributing to these
resultsareidentifiedintheeffectiveuseofknowledge-enhancedmodels:3.5. ResultsandDiscussion 45
• Currentness and Relevance of the Knowledge Base: Since COVID-19 was col-
lected after most PLMs were trained, certain terms like “COVID-19” may not
bepresentintheKnowledgeBase.
• QualityoftheDataset: TheCOVID-19datasetcontainsconfoundingfactorsthat
provide strong cues, potentially overshadowing the impact of the knowledge
base. Notably,theprevalenceofhttpslinks,occurringin95.3%ofrealpostsbut
only42.3%offakeposts,canactasashortcuttoderivingthecorrectprediction.
Thereisalsopotentialtoachievemoreexplainabilityandinterpretabilitywithdi-
rectKBintegrationatruntime. Forinstance,inthestatementfromCOVID-19: “DNA
Vaccine: injecting genetic material into the host so that host cells create proteins that are
similar to those in the virus against which the host then creates antibodies”, KnowBert-
W+Wcorrectlyclassifiesitas“real”,whereasBERT-basefails. Weobservethatmost
mentionspansinthestatement,i.e.,“DNA”,“injecting”,“genetic”,“geneticmaterial”,
“host”,“cells”,etc. arecorrectlylinkedtoentities“DNA”,“Injection_(medicine)”,“Ge-
netics”,“Genome”,“Host_(biology)”,“Cell_(biology)”,respectively,suggestingthaten-
titylinksmayhavecontributedtoKnowBert-W+Wforthisclassification. However,
thelevelofexplainabilityremainslimited. Theentitylinkinginbothmodelsisgen-
erallyofmixedqualityaswell,asillustratedintheCOVID-19example.
ApplicationAspects
Theapplicationofautomaticfakenewsdetectioninreal-worldscenariosintroduces
twodynamicaspectsthatarechallengingtotestwithstaticdatasets,ashighlighted
byourexperimentonCOVID-19:
• Dynamic Adaptation: The system needs to adapt to the changing character-
istics of real and fake news (Silva and Almeida, 2021). Knowledge-enhanced
modelsthatutiliseKnowledgeBasesatruntimeprovideanopportunitytoup-
date the KB independently of the model. This approach offers the advantage
ofrecognisingfakenewsascontradictingtheKBevenbeforespecificexamples
offakenewsemerge.46 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
• Adversarial Robustness: Authors of fake news are likely to employ evasive
strategies. Adaptingthetextstyleisrelativelystraightforwardandcanbeau-
tomated,makingthedetectionusingstylisticfeatureschallenging(seeZellers
etal.,2019b;Schusteretal.,2020).
The deployment of fake news detection in social media is likely to necessitate
human verification, especially when users challenge actions taken against them. In
this context, Knowledge Base integration can provide a valuable advantage by of-
feringinsightsintotheknowledgeusedinthedetectionprocess,therebyenhancing
explainability. This transparency could be crucial in addressing user concerns and
buildingtrustinthefakenewsdetectionsystem.
3.6 Conclusion and Future Work
Inthischapter,westudytheeffectivenessofenhancingPLMswithknowledgebases
for fake news detection. The findings underscore that the success of integrating
knowledge with PLMs is contingent upon the availability of suitable KBs and the
qualityofthedataset. Whilebetterperformanceisobservedonastaticdataset,there
is room for improvement on both the modelling and application levels. To the best
ofourknowledge,ourworkisthefirstexaminationofknowledge-enhancedmodels
in the context of fake news detection. The positive results on the LIAR dataset pro-
videinsightsintotheeffectivenessofconsideringentityknowledgeinareasbeyond
traditionaltaskslikeentitylinking.
For practical application, more insight into the specific knowledge utilised dur-
ing the detection process could contribute to more transparent and interpretable
models. Furthermore, the potential for dynamic adaptation of models and KBs to
the evolving characteristics of real and fake news is a promising avenue for explo-
ration. The integration of KBs with PLMs presents an opportunity for more robust
andtimelyfakenewsdetection.
Futureworkcouldalsoinvestigatethedevelopmentofamorereliableevaluation
approach,forexampleinvolvingtestingscenariosthatsimulatedynamicknowledge3.7. Limitations 47
updatesaswellasthechallengesposedbyadversarialandautomaticfakenewsgen-
erators. Overall,asthelandscapeofmisinformationevolves,theadaptabilityandre-
silience of knowledge-enhanced models demonstrate promise in their effectiveness
inreal-worldapplications.
3.7 Limitations
Wefirstdiscussthegenerallimitationsofincorporatingexternalknowledgeresources
for improving Transformer-based models, and then we describe the specific limita-
tionsofthischapter.
GeneralLimitationsofExternalKBsforImprovingTransformerModels
• Scalability and Efficiency: Transformer models are already computationally
intensive, especially for long contexts, due to the quadratic complexity of the
attention calculation within the window length. Integrating external knowl-
edgesourcescanfurtherincreasecomputationalcosts,especiallywhendevel-
opingadaptiveapproachesthatrequireup-to-dateknowledgeondemand.
• Domain Adaptation and Generalisation: External knowledge resources of-
ten originate from specific domains or sources, which may not fully align
with the target task or dataset. This misalignment poses a challenge, par-
ticularly as many knowledge integration approaches require linkage to pre-
defined knowledge sources. As large language models evolve toward greater
general-purposeutility,therearisesagrowingneedforeffectivegeneral-purpose
knowledgefusionapproaches.
• Selective Knowledge Incorporation: Existing strategies for leveraging exter-
nal knowledge in Transformer models often adopt a binary approach, either
consistently integrating external knowledge or entirely disregarding it. How-
ever,asLLMsevolveandaccumulateknowledgewithintheirparameters,the
benefit of external knowledge integration becomes context-dependent. There48 Chapter3. Knowledge-EnhancedLanguageModelsforFakeNewsDetection
are instances where querying external knowledge may hinder model perfor-
mance, given the limitations above (i.e., the computational overhead and po-
tential mismatches between the external knowledge and the task at hand). A
recent work, Self-RAG (Asai et al., 2024), which trains the LM to reflect the
necessity to retrieve external knowledge and critique its own generation con-
ditionedontheexternalknowledge,shinesalightintothisdirection.
LimitationsofthisChapter
• Limited Datasets: The experiments were conducted only on two fake news
datasets, both consisting of short statements. Notably, the COVID-19 dataset
alreadyexhibitedaveryhighbaselineaccuracy. Togainmorecomprehensive
insightsintoknowledge-enhancedlanguagemodels, itwouldbebeneficialto
includemorediverseandcomplexfakenewsdetectiondatasets.
• Limited Knowledge Sources: The evaluation in this chapter focuses on pre-
trained models that integrate with knowledge bases primarily sourced from
WikidataandWordNet. However,forthespecificusecaseofCOVID-19,there
isapotentialvalueinconductingacomparisonthatinvolvesintegratingmed-
icaldomain-specificknowledgebases.49
Chapter 4
Entity-Centric Code-Switching for
Enhanced Cross-lingual Transfer
In this chapter, we focus on utilising structured knowledge in a multilingual setup,
for the improved cross-lingual transferability of pre-trained cross-lingual language
modelsonentity-centrictasks.
The main content of this chapter is based on the paper “ENTITYCS: Improving
Zero-ShotCross-lingualTransferwithEntity-CentricCodeSwitching”(Whitehouse,
Christopoulou,andIacobacci,2022)publishedinFindingsoftheAssociationforCom-
putationalLinguistics: EMNLP2022.
4.1 Background and Introduction
Cross-lingual pre-trained Language Models (XLMs), such as mBERT (Devlin et al.,
2019) and XLM-R (Conneau et al., 2020a), have achieved state-of-the-art zero-shot
cross-lingualtransferabilityacrossdiverseNaturalLanguageUnderstanding(NLU)
tasks. Thesemodelshavebeennotablyenhancedthroughtheincorporationofbilin-
gual parallel sentences, along with alignment methods (Yang et al., 2020; Chi et al.,
2021a; J. Hu et al., 2021; Gritta and Iacobacci, 2021; Feng et al., 2022). However,
acquiringhigh-qualityparalleldataiscostly,especiallyforlow-resourcelanguages.
Therefore, alternative data augmentation approaches have been proposed, one of
whichisCodeSwitching(CS).
Code Switching is a phenomenon in which multilingual speakers alternate be-
tween languages when they speak, a topic that has been studied for many years50 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
(Gumperz, 1977; Khanuja et al., 2020; Dog˘ruöz et al., 2021). Code-switched sen-
tences consist of words or phrases in different languages, capturing finer-grained
cross-lingual expressions compared to parallel sentences. They have been utilised
formultilingualintermediatetraining(Yangetal., 2020)andfine-tuning(Qinetal.,
2020;Krishnanetal.,2021). Nevertheless,manuallycreatinglarge-scaleCSdatasets
is expensive, and only a few natural CS texts exist (Lyu et al., 2015; Barik, Mahen-
dra,andAdriani,2019;R.Xiangetal.,2020;Chakravarthietal.,2020;Loveniaetal.,
2022). Asaresult,researchhasturnedtoautomaticCSdatageneration.
Some of these approaches generate CS data via dictionaries, often ignoring am-
biguity(Qinetal., 2020; Conneauetal., 2020b). Othersrequireparalleldataandan
alignmentmethodtomatchwordsorphrasesbetweenlanguages(Yangetal.,2020;
Rizvi et al., 2021). In both cases, what is switched is chosen randomly, potentially
resulting in syntactically odd sentences or switching to words with little semantic
content(e.g.,conjunctions). Thisisincontrasttoobservationsfrompriorworkthat
have shown in real code-switched data, such as SEAME (Lyu et al., 2015), where
nouns have the highest rate of code-switching (Çetinog˘lu, Schulz, and N. T. Vu,
2016). Theyalsofindthatpeoplemayswitchgrammarruleswhentheycode-switch,
making automatic code-switching by randomly replacing words in a sentence less
feasible.
Ontheotherhand,entitiescontainexternalknowledgeanddonotaltersentence
syntax when replaced with other entities, mitigating the need for parallel data or
word alignment tools. Motivated by this, we propose ENTITYCS, a code-switching
method that focuses on entities. Resources such as Wikipedia and Wikidata offer
rich cross-lingual entity-level information and have shown benefits in XLMs pre-
training (Z. Jiang et al., 2020; Calixto, Raganato, and Pasini, 2021; X. Jiang et al.,
2022). Weusesuchresourcestogenerateanentity-basedCScorpusfortheinterme-
diate training of XLMs. Entities in wikilinks1 are switched to their counterparts in
otherlanguagesretrievedfromtheWikidataKB,thusalleviatingambiguity.
Training models on our synthetic entity-level code-switched data offers several
1https://en.wikipedia.org/wiki/Help:Link#Wikilinks_(internal_links)4.1. BackgroundandIntroduction 51
advantages over using naturally occurring code-switched text. Firstly, our synthe-
sised data allows for greater control over language diversity. While natural code-
switched text often involves only one or two high-resource non-English languages,
our dataset can incorporate a broader range of languages and combinations. Ad-
ditionally, by focusing on entity-centric training objectives, the models are antici-
pated to capture finer-grained semantics shares across languages, which is partic-
ularly beneficial for downstream tasks that require accurate entity understanding.
However, it is important to note that synthetic data may not fully capture the com-
plexitiesofnaturallyoccurringcode-switchedtext,whichofteninvolvessubtleshifts
inlanguageuseinfluencedbycultural,social,andcontextualfactors(Dog˘ruözetal.,
2021).
Using the ENTITYCS corpus, we propose a series of masking strategies that fo-
cus on enhancing Entity Prediction (EP) for better cross-lingual entity representa-
tions. Weevaluatethemodelsonentity-centricdownstreamtasks,includingNamed
Entity Recognition (NER), Fact Retrieval, Slot Filling (SF), and Word Sense Disam-
biguation (WSD). Extensive experiments demonstrate that our models outperform
thebaselineonzero-shotcross-lingualtransfer,witha+2.8%improvementonNER,
surpassing the prior best result that uses large amounts of parallel data, +10.0% on
FactRetrieval,+2.4%onSlotFilling,and+1.3%onWSD.
Themaincontributionsofthischapterinclude:
• Construction of an entity-level CS corpus, ENTITYCS, based on the English
WikipediaandWikidata,mitigatingtheneedforparalleldata,word-alignment
methods,ordictionaries.
• Aseriesofintermediatetrainingobjectives,focusingonEntityPrediction.
• Improvement of zero-shot performance on NER, Fact Retrieval, Slot Filling,
andWSD.
• Furtheranalysisofmodelerrors,thebehaviourofdifferentmaskingstrategies
throughout training, as well as the impact across languages, and demonstra-
tionoftheparticularbenefitofnon-Latinscriptlanguages.52 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
4.2 Related Work
Weintroducetherelatedworkofthischapterinthefollowingthreeaspects:
4.2.1 Cross-LingualPre-Training
Mostexistingcross-lingualpre-trainedlanguagemodelsemployparalleldatatoen-
hance multilingual contextualised word representations for different languages (X.
Ouyang et al., 2021; Luo et al., 2021; Chi et al., 2021b). Adapters have also been
applied to improve zero-shot and few-shot cross-lingual transfer by training only a
smallsetofmodelparameters(Pfeifferetal.,2020;Anselletal.,2021). Additionally,
meta-learning techniques (Nooralahzadeh et al., 2020; Tarunesh et al., 2021) have
proven highly effective for rapid adaptation to new languages (Dou, K. Yu, and
Anastasopoulos,2019). Incomparisontotheseapproaches,ourmethodaimstoen-
hancecross-lingualtransferabilitythroughintermediatetrainingonanentity-based
code-switchingcorpuscreatedfromWikipediawikilinks,withoutrequiringparallel
data.
4.2.2 CodeSwitching
Code Switching methods have shown success in cross-lingual model pre-training
and fine-tuning across various NLU tasks, including NER (Priyadharshini et al.,
2020; L. Liu et al., 2021), Part-of-Speech Tagging (Ball and Garrette, 2018), Machine
Translation (Srivastava and Singh, 2020), Intent Classification, and Slot Filling (Kr-
ishnanetal.,2021). Thesemethodshavealsobeenappliedoncode-switcheddatasets
(RizalandStymne,2020;Prasadetal.,2021).
AsignificantchallengeinstudyingCodeSwitchingisthescarcityoftrainingdata
(D.Gupta,Ekbal,andBhattacharyya,2020). Existingcode-switchedcorporamostly
involve English and one other language (e.g., English-Chinese, English-Spanish,
English-Hindi),extractedfromsocialmediaplatforms(Barik,Mahendra,andAdri-
ani,2019;R.Xiangetal.,2020;Chakravarthietal.,2020;Loveniaetal.,2022).
Methods for generating code-switched data in multiple languages have been
proposed. Qin et al. (2020) and Conneau et al. (2020b) create code-switched data
fromdownstreamtaskdatasetsbyrandomlyswitchingindividualwordstoatarget4.2. RelatedWork 53
language using translations from bilingual dictionaries. However, this introduces
ambiguity errors and is prone to switching words without significant content. Kr-
ishnanetal.(2021)useCodeSwitchingtoimproveIntentClassificationandSlotFill-
ing. Insteadofswitchingindividualwords,theyobtainphraseinformationfromthe
slot labels and generate phrase-level code-switched sentences via automatic trans-
lations. Yang et al. (2020) create code-switched sentences by randomly substitut-
ingsourcephraseswiththeirtargetequivalentsinparallelsentencesafterobtaining
word alignments. Z. Jiang et al. (2020) select a subset of Wikipedia sentences in
four languages that contain multilingual entities from X-FACTR and create code-
switched sentences by switching entities from English to non-English entities and
viceversa,viaWikidatatranslations.
Our proposed EntityCS shares several similarities. The primary distinctions,
particularly in contrast to Z. Jiang et al. (2020), can be summarised in three key as-
pects: (i)Scale: While Z. Jiang et al. (2020) focuses on a limited subset, we create a
substantial corpus, magnifying the scale by a factor of 1000x, spanning across 93
languages rather than just four; (ii) Diversity: In contrast to the 30% sentence code-
switchingandsinglenon-EnglishentityfocusinZ.Jiangetal.(2020),ourapproach
code-switches every sentence to multiple candidate target languages. This modi-
fication enhances the model’s ability to capture cross-lingual information compre-
hensively, resulting in improved cross-lingual transferability; (iii) Entity-Prediction
Objective: A pivotal difference lies in our incorporation of various entity-focused
prediction objectives. This strategic design enhances the entity-awareness of the
intermediate-trained model, which is further shown to contribute to entity-centric
downstreamtasks.
4.2.3 KnowledgeIntegrationintoLanguageModels
AsdetailedinChapter2,Pre-trainedLanguageModelsmaylackexplicitgrounding
toreal-worldentitiesandrelations, makingitchallengingtorecoverfactualknowl-
edge(Benderetal.,2021).
We refer the reader to section 2.3 for techniques of knowledge integration into
PLMs that focus on monolingual models. Integrating multilingual knowledge into54 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
XLMshasalsobeenrecentlyaddressed. X.Jiangetal.(2022)trainamodelwithtwo
knowledge-related tasks: entity prediction and object entailment. They use Wiki-
Datadescriptionembeddingsinonelanguage(Englishandnon-English)topredict
an entity in a target language as a classification task, preserving an entity’s vocab-
ulary. Calixto, Raganato, and Pasini (2021) use Wikipedia articles in 100 languages
togetherwithBabelNet(NavigliandPonzetto,2012),amultilingualsense-inventory
forWSD,bypredictingtheWikiDataIDofeachentity. Anotherworktakingadvan-
tageofentitiesbyRi,Yamada,andTsuruoka(2022)usesdedicatedmultilingualen-
tity embeddings on 24 languages and outperforms word-based pre-trained models
invariouscross-lingualtransfertasks.
4.3 Methodology
Inthissection,weprovideacomprehensiveoverviewoftheENTITYCScorpuscon-
structionanddetailvariousentity-orientedmaskingstrategiesemployedinourex-
periments.
4.3.1 ENTITYCS CorpusConstruction
Wikipedia is a multilingual online encyclopedia available in more than 300 lan-
guages.2 StructureddataofWikipediaarticlesarestoredinWikidata,amultilingual
document-orienteddatabase. Withmorethansixmillionarticles,EnglishWikipedia
has the potential to serve as a rich resource for generating CS data. We use English
Wikipedia and leverage entity information from Wikidata to construct an entity-
basedCScorpus.
Toachievethis,wemakeuseofwikilinksinWikipedia,i.e.,linksfromonepage
toanother. WeusetheEnglishWikipediadump3 andextractrawtextwithWikiEx-
tractor4 while keeping track of wikilinks. Wikilinks are typically surrounded by
square brackets in Wikipedia dump, in the format of [[entity | display text]], where
entityisthetitleofthetargetWikipediapageitlinksto,anddisplaytextcorresponds
2https://en.wikipedia.org/wiki/Wikipedia
3https://dumps.wikimedia.org/enwiki/latest/(Nov2021version)
4https://github.com/attardi/wikiextractor4.3. Methodology 55
to what is displayed in the current article. We then employ SpaCy5 for sentence
segmentation. Tofocusonentity-levelcode-switchedinstances,onlysentencescon-
taining at least one wikilink are retained, and sentences exceeding 128 words are
excludedfromthedataset. ThisprocessresultsinafinalENTITYCScorpuscompris-
ing54.5millionEnglishsentencesand104millionentities.
She was studying [[ computer science ]] and [[ electrical engineering ]] .
computer science Q21198
electrical engineering Q43035
Q21198 Q43035
en: Computer Science en: Electrical Engineering
zh: 计算机科学 zh: 电气工程学
hi: क  ूटर िव ान hi: िवद्युत अिभया  की
fr: Informatique . . .
. . . fr: Électrotechnique
ar: بﻮﺳﺎﺤﻟا ﻢﻠﻋ ar: ﺔﯿﺋﺎﺑﺮﮫﻛ ﺔﺳﺪﻨھ
el: Επιστήμη Υπολογιστών el: Ηλεκτρολογία
She was studying <e>computer science</e> and <e>electrical engineering</e>.
She was studying <e>计算机科学</e> and <e>电气工程学</e>.
She was studying <e>क  ूटर िव ान</e> and <e>िवद्युत अिभया  की</e>.
She was studying <e>Informatique</e> and <e>Électrotechnique</e>.
. . .
FIGURE4.1: IllustrationofgeneratingENTITYCSsentencesfromanEnglishsen-
tenceextractedfromWikipedia. Entitiesindoublesquarebracketsindicatewik-
ilinks.
As depicted in Figure 4.1, our Code Switching process begins with an English
sentence containing wikilinks. Each entity within these links is mapped to its cor-
respondingWikidataID,andtranslationsfortheseentitiesareretrievedfromWiki-
data. TheselectionoftargetlanguagesforCodeSwitchingisbasedontheavailabil-
ityoftranslationsforallentitieswithinagivensentence.
Weconsiderasetof92targetlanguages(non-English),representingtheoverlap
betweenlanguagesavailableinWikidataandthosesupportedbyXLM-R(Conneau
et al., 2020a), the model utilised for intermediate training. To ensure coherence, all
5https://spacy.io/56 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
STATISTIC COUNT
Languages 93
EnglishSentences 54,469,214
EnglishEntities 104,593,076
AverageSentenceLength 23.37
AverageEntitiesperSentence 2
CSSentencesperENSentence ≤5
CSSentences 231,124,422
CSEntities 420,907,878
TABLE 4.1: Statisticsofthe ENTITYCS Corpus.
entitiesinasentencearecode-switchedtothesametargetlanguage, mitigatingpo-
tentialnoisefromintroducingtoomanylanguages.
To manage the size of the corpus, we generate up to five entity code-switched
sentences for each English sentence. Specifically, if fewer than five languages have
translations available for all entities in a sentence, we create ENTITYCS instances
with all available languages. Otherwise, we randomly select five target languages
fromthecandidates. Ifnocandidatelanguagesarefound,weretainthesentencein
theEnglishcorpuswithoutcode-switching.
In the final step, we enclose each entity with entity indicators (<e>, </e>). This
ensuresclearidentificationofentitieswithinthecode-switchedsentences.
The statistical overview of the ENTITYCS corpus is presented in Table 4.1, and
ahistogramdetailingthenumberofsentencesandentitiesperlanguage(excluding
English)inthe ENTITYCS corpusisillustratedinFigure4.2.
4.3.2 MaskingStrategies
To assess the efficacy of intermediate training on the generated ENTITYCS corpus,
we experiment with various training objectives using an existing pre-trained lan-
guagemodel. Initially,weadopttheconventional80-10-10MaskedLanguageMod-
elling(MLM)objective,where15%ofsentencesubwordsortokensserveasmasking
candidates. Amongthese,wereplacetokenswith[MASK]80%ofthetime,with10%
using random tokens (from the entire vocabulary), and the remaining 10% left un-
changed(Same).4.3. Methodology 57
FR
DE Entity
ES Sentence
NL
JA
ZH
IT
RU
PT
CA
SV
PL
AR
UK
FA
NB
FI
HE
KO
CS
DA
ID
HU
TR
EO
VI
RO
SL
EU
EL
SR
GA
BG
SQ
CY
HY
MS
GL
ET
BE
LT
BN
HI
HR
TH
SK
UR
TA
MK
KA
LV
LA
AZ
AF
ML
MR
KK
TL
IS
BR
BS
UZ
TE
SW
KN
FY
JV
PA
GD
KY
NE
MG
SI
KU
GU
YI
MY
MN
SU
OR
AM
SD
PS
HA
SA
SO
UG
AS
KM
LO
OM
XH
5M 10M 15M 20M 25M 30M
FIGURE4.2: NumberofCode-SwitchedEntitiesandSentencesintheENTITYCS
corpus.
egagunaL
tegraT58 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
_Informati que _É lec tro nique
_She _was _study ing [MASK] [MASK] _and _É lec tro nique
(A)WholeEntityPrediction(WEP)
_Informati que _É lec nique
_She _was _study ing [MASK] que _and [MASK][MASK]tro nique
(B)PartialEntityPrediction(PEP)
_was _study _Informati lec nique
_She [MASK] _με ing [MASK] que _and _É [MASK] tro nique
(C)PartialEntityPredictionwithMLM(PEP+MLM)
FIGURE4.3: Illustrationoftheproposedmaskingstrategies. Randomtokensare
chosenfromtheentirevocabularyandthuscanbefromdifferentlanguages. (c)
showsacasewhere“study”isreplacedwithatokeninGreek.
To integrate entity-level cross-lingual knowledge into the model, we introduce
EntityPredictionobjectives, whereweexclusivelymasktokensbelongingtoanen-
tity. By predicting the masked entities in ENTITYCS sentences, we anticipate the
model capturing the semantics of the same entity in different languages. Two dis-
tinct masking strategies are proposed for predicting entities: Whole Entity Predic-
tion(WEP)andPartialEntityPrediction(PEP).
In WEP, inspired by Sun et al. (2019) where whole-word masking is also em-
ployed, we consider all the words (and consequently subwords or tokens) inside
an entity as masking candidates. Subsequently, 80% of the time, we mask every to-
keninsidean entity, leaving20% unchanged. Notably, topredicttheentire masked
entity,werefrainfromreplacingitwithrandomtokens,asitmightintroducenoise,
leadingtothemodelpredictingincorrectentities. Aftermaskingentities,weremove
theentityindicators<e>,</e>fromthesentencesbeforefeedingthemtothemodel.
Figure4.3aprovidesanexampleof WEP.
ForPEP,wealsoconsiderallentitiesasmaskingcandidates. UnlikeWEP,wedo
not enforce tokens belonging to one entity to be either all masked or all unmasked.
Instead, each individual entity token is masked 80% of the time. For the remaining
20%ofmaskingcandidates,weexperimentwiththreedifferentreplacements.
PEP corresponds to the conventional 80-10-10 masking strategy, where 10%
MRS
oftheremainingtokensarereplacedwithrandomtokens,andtheother10%areleft4.4. ExperimentalSetup 59
MASKING ENTITY (%) NON-ENTITY (%)
STRATEGY p MASK RND SAME p MASK RND SAME
MLM 15 80 10 10 15 80 10 10
WEP 100 80 0 20 0 – – –
PEP 100 80 10 10 0 – – –
MRS
PEP 100 80 0 10 0 – – –
MS
PEP 100 80 0 0 0 – – –
M
WEP 50 80 0 20 15 80 10 10
PEP 50 80 10 10 15 80 10 10
MRS + MLM
PEP 50 80 0 10 15 80 10 10
MS
PEP 50 80 0 0 15 80 10 10
M
TABLE 4.2: Summaryoftheproposedmaskingstrategies. p correspondstothe
probabilityofchoosingcandidateitems(entity/non-entitytokens)formasking.
MASK,RND,SAMErepresentthepercentageofreplacingacandidatewithMask,
Random or the Same item. When combining WEP/PEP with MLM (+MLM), we
lower pto50%.
unchanged. In PEP , we remove the 10% random tokens substitution, predicting
MS
onlythe80%maskedtokensand10%Sametokensfromthemaskingcandidates. In
PEP , we further eliminate the 10% Same tokens prediction, essentially predicting
M
only the masked tokens. An example of PEP is illustrated in Figure 4.3b. Previous
workhasdemonstratedtheeffectivenessofcombiningEntityPredictionwithMLM
forcross-lingualtransfer(Z.Jiangetal.,2020). Therefore,weinvestigatethecombi-
nation of Entity Prediction objectives with MLM on non-entity tokens. Specifically,
when combined with MLM, we lower the entity masking probability (p) to 50% to
maintain roughly the same overall masking percentage. Figure 4.3c illustrates an
exampleof PEP combinedwith MLM onnon-entitytokens.
A summary of the masking strategies is presented in Table 4.2, along with the
correspondingmaskingpercentages.
4.4 Experimental Setup
AfterconstructingtheENTITYCScorpus,weproceedtofurthertrainanXLMmodel,
utilising XLM-R-base6 for our experiments. We explore the effectiveness of WEP,
PEP, MLM,andthejointobjectivesinintermediatetraining.
6https://huggingface.co/xlm-roberta-base60 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
WeadoptthesamplingstrategyproposedbyConneauandLample(2019),where
wedown-samplehigh-resourcelanguagesandincreasethesamplingfrequencyfor
low-resource languages. Recognising that semantic features are often emphasised
in higher layers of pre-trained language encoders (Tenney, Das, and Pavlick, 2019;
Rogers,Kovaleva,andRumshisky,2020),werestricttrainingtotheembeddinglayer
and the last two layers of the model. This approach helps prevent catastrophic for-
getting, a phenomenon observed in preliminary experiments when updating the
entirenetwork. Werandomlyselect100sentencesfromeachlanguagetoserveasa
validationset,measuringperplexityevery10Ktrainingsteps.
4.4.1 Datasets
As the ENTITYCS corpus is constructed with Code Switching at the entity level,
we expect our models to mostly improve entity-centric tasks. Thus, we choose the
following datasets: WikiAnn (X. Pan et al., 2017) for NER, X-FACTR (Z. Jiang et
al., 2020) for Fact Retrieval, MultiATIS++ (W. Xu, Haider, and Mansour, 2020) and
MTOP(HaoranLietal.,2021)forSlotFilling,andXL-WiC(Raganatoetal.,2020)for
WSD.7 Thedetailsofthedatasetsareintroducedbelow.
WikiAnn (X. Pan et al., 2017) is a cross-lingual name tagging and linking dataset
based on Wikipedia articles, where named entities are annotated as location (LOC),
organisation (ORG) and person (PER) tags following the IOB2 format. The original
dataset contains 282 languages. We evaluate our models on the 40 languages from
WikiAnnthatareincludedintheXTREMEbenchmark(J.Huetal.,2020).
X-FACTR (Z. Jiang et al., 2020) is a multilingual fact retrieval benchmark similar
to LAMA (Petroni et al., 2019). It probes factual knowledge stored in pre-trained
language models by prompt-based fill-in-the-blank cloze queries, covering 23 lan-
guages. X-FACTRincludesbothsingle-andmulti-tokenentities,andtwodecoding
methods(independentandconfidence-based)areproposed.
7The result reported on the XL-WiC for prior work is our re-implementation based on https://
github.com/pasinit/xlwic-runs.4.4. ExperimentalSetup 61
MultiATIS++ (W. Xu, Haider, and Mansour, 2020) is an expansion of the Multilin-
gual ATIS (Upadhyay et al., 2018) dataset, which includes nine languages (English,
Spanish, German, French, Portuguese, Chinese, Japanese, Hindi and Turkish) from
four language families (Indo-European, Sino-Tibetan, Japonic and Altaic). It con-
tains dialogues in a single domain, Air Travel Information Services. While process-
ing the dataset, we noticed that 14 examples in the test set do not have a matching
numberoftokensandslotlabels,whichweignoredduringtheevaluation.
MTOP (HaoranLietal.,2021)isaMultilingualTask-OrientedParsingdatasetthat
includes six languages from 11 domains that are related to interactions with a per-
sonalassistant. WeusethestandardflatlabelsasreportedinHaoranLietal.(2021).
XL-WiC (Raganatoetal.,2020)isacross-lingualworddisambiguationdataset(Word
inContext),formedasabinaryclassificationproblem. Givenatargetwordandtwo
contexts,thegoalistoidentifyifthewordisusedinthesamesenseinbothcontexts.
The dataset contains both nouns and verbs as target words, covers 12 languages
andwascreatedasanextensiontotheEnglishWiCdataset(PilehvarandCamacho-
Collados,2019).
4.4.2 Hyper-ParameterSettings
We introduce below the detailed setting of our intermediate training and down-
streamtasksfine-tuning.
IntermediateTraining
Weuse8NvidiaV10032GBGPUsfortrainingourmodelsontheENTITYCScorpus,
with the Hugging Face library (Wolf et al., 2020). During fine-tuning, all models
wererunonasingleNvidiaV10032GBGPU.Wesetthebatchsizeto16andgradient
accumulationstepsto2,resultinginaneffectivebatchsizeof256. Forspeedup,we
employ half-precision (fp16) in the experiments. In each batch, we allow examples
frommultiplelanguages,basedonthesamplingstrategyfollowedbyConneauand
Lample (2019). We train for a single epoch with a maximum learning rate 5e−5 and
linear decay scheduler, no warmup or weight decay, gradient clipping equal to 1.0,62 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
MULTIATIS++ MTOP
PARAMETER WIKI-ANN XL-WIC
SF Joint SF Joint
LEARNING RATE 1e−5 3e−5 3e−5 2e−5 3e−5 1e−5
WARMUP RATIO 0.1 0.0 0.0 0.1 0.1 0.0
BATCH SIZE 8 8 8 8 8 8
TABLE 4.3: Besthyper-parametersusedforthedatasets.
andearlystoppingifperplexitydoesnotdropafter20consecutiveevaluations(we
evaluateevery10Ktrainingsteps).
DownstreamTasks
After intermediate training on the ENTITYCS corpus, we evaluate the zero-shot
cross-lingual transfer of the models on downstream tasks by fine-tuning the model
on task-specific English training data. For downstream tasks, we evaluate models
ontheEnglishvalidationsetfivetimesperepochfollowingDodgeetal.(2020).
Forfine-tuningXLM-R-baseonWikiAnn,MultiATIS++,MTOPandXL-WiC,we
fix the number of training epochs to 10, gradient clipping to 1.0, and maximum
sequence length to 128. We select the batch size from {8,32}, learning rate from
{1e−5,2e−5,3e−5,4e−6,5e−6,6e−6},andwarmupratiofrom{0,0.1}.
The best hyper-parameters per task are reported in Table 4.3. We choose the
checkpoints with the best performance on the English validation set. For all exper-
iments except X-FACTR, we fine-tune models with five random seeds and report
averageperformanceandstandarddeviation.
4.4.3 LanguagesforIntermediateTraining
Given the size of the ENTITYCS corpus, we primarily select a subset from the total
93languages,thatcoversmostofthelanguagesusedinthedownstreamtasks. This
subset contains 39 languages, from WikiAnn, excluding Yoruba.8 We train XLM-R-
baseonthissubset,andsubsequentlyfine-tunethenewcheckpointsontheEnglish
trainingsetofeachdataset,withevaluationsspanningallavailablelanguages.
8Yoruba is not included in the ENTITYCS corpus, as we only consider languages that XLM-R is
pre-trainedon.4.5. MainResults 63
NER(F1) FACTRETR.(ACC.) SLOTFILLING(F1,F1/ACC.) WSD(ACC.)
MODEL WIKIANN X-FACTR MULTIATIS++ MTOP XL-WIC
all single multi SF SF/Intent SF SF/Intent
XLM-RPRIOR 61.8 3.5 9.4 2.6 – – – – 58.0
XLM-ROURS 61.60.3 3.5 9.4 2.6 71.82.0 73.00.7/89.11.0 73.2 0.9 72.50.8/86.00.7 59.11.5
MLM 63.50.5 2.5 6.4 1.7 72.12.3 74.00.7/89.61.4 72.80.6 72.70.3/86.3 0.4 59.30.4
WEP 62.40.7 6.1 19.4 3.0 71.61.2 71.70.8/89.71.3 72.20.6 73.0 0.5/86.00.4 60.4 1.0
PEPMS 63.30.7 6.0 15.0 4.3 73.41.7 74.4 0.7/90.0 0.9 71.50.7 72.70.6/86.10.5 60.20.9
PEPMS+MLM 64.4 0.5 5.7 13.9 3.9 74.2 0.4 74.30.8/89.00.9 73.00.3 72.50.6/85.80.8 59.80.8
TABLE4.4: Averageperformanceacrosslanguagesonthetestsetofdownstream
tasks. XLM-RPRIOR corresponds to previous reported results with XLM-R-base,
referringtoChietal.(2021b)forWikiAnn,Z.Jiangetal.(2020)forX-FACTRand
Raganatoetal.(2020)forXL-WiC.XLM-ROURS showsourre-implementedresults
withXLM-R-base. Results(excludingX-FACTR)areaveragedacrossfiveseeds
withstandarddeviationreportedasasubscript.
4.5 Main Results
ThemainresultsarereportedinTable4.4wherewecomparemodelstrainedonthe
ENTITYCS corpus with MLM, WEP, PEP and PEP +MLM masking strategies.
MS MS
For MultiATIS++ and MTOP, we report results of training only Slot Filling (SF), as
wellasjointtrainingofSlotFillingandIntentClassification(SF/Intent).
4.5.1 NamedEntityRecognition
In NER, models with CS intermediate training consistently demonstrate improve-
ment on WikiAnn over the baseline, with PEP +MLM exhibiting a substantial
MS
+2.8% absolute improvement. This outperformance extends to XLM-Align9 (Chi et
al., 2021b), whichemploysa significantamountof paralleldata(seeTable 4.7). The
conventionalMLMobjectiveprovessimilarlyeffectivewith PEP,potentiallydueto
the overlap in entities chosen as masking candidates. However, WEP yields lower
performance,suggestingthatpredictingentireentitiesfromthesurroundingcontext
posesgreaterchallenges.
PerformanceperlanguageforPEP andPEP +MLMisdetailedinTable4.5.10
MS MS
Notably,nearlyalllanguagesbenefitfromtrainingonthe ENTITYCS corpus. Inthe
9https://huggingface.co/microsoft/xlm-align-base
10Per-languageresultsonWikiAnnforothermodelsarereportedinTableA.1inAppendixA.64 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
MODEL AR HE VI ID JV MS TL EU ML TA TE AF NL EN
XLM-ROURS 44.6 51.9 68.3 48.6 59.6 63.3 72.5 61.2 63.2 54.3 49.3 76.3 80.7 83.4
PEPMS 49.6 53.0 70.0 58.5 62.0 64.9 75.7 59.8 63.3 57.7 52.1 76.4 80.9 83.8
PEPMS+MLM 51.5 54.0 70.9 61.1 59.3 69.9 74.6 59.3 66.3 57.6 54.8 77.9 81.5 84.2
DE EL BN HI MR UR FA FR IT PT ES BG RU JA
XLM-ROURS 75.4 74.2 67.9 68.3 61.8 55.8 47.6 78.0 78.2 78.9 76.2 77.3 63.9 22.9
PEPMS 75.1 76.3 72.5 70.1 66.8 61.5 55.6 78.8 78.5 78.6 75.8 78.0 66.4 21.3
PEPMS+MLM 75.5 77.1 74.6 70.7 66.3 65.9 54.2 79.5 78.9 80.1 78.2 79.6 67.7 23.2
KA KO TH SW YO MY ZH KK TR ET FI HU
XLM-ROURS 66.4 48.8 4.3 68.3 45.4 52.7 27.7 44.2 76.9 72.4 75.6 76.9
PEPMS 67.0 50.2 4.6 66.9 44.7 55.2 26.9 48.9 77.4 73.4 76.6 77.8
PEPMS+MLM 68.2 52.1 4.0 66.4 48.4 56.1 29.8 52.0 78.6 71.9 76.8 78.8
TABLE 4.5: F1-scoreperlanguageontheWikiAnntestset. Resultsareaveraged
acrossfiveseeds.
optimalsetting,PEP +MLM,languagesAR,ID,andURexhibitthemostsubstan-
MS
tialimprovement(approximately+10%inARandID).However,EUandSWresult
inlowerperformancecomparedtothebaseline.
Given this, we conduct a detailed analysis of the PEP +MLM model, specifi-
MS
callyfocusingonNERerrorscategorisedintofivetypes: Tag,Span,Tag+Span,Miss-
ing Extraction, and Extra Extraction. In this context, Missing Extraction denotes
instanceswherethemodelfailstoidentifyanentity,whileExtraExtractionrefersto
errors where a non-entity is incorrectly predicted as an entity. For this analysis, we
selectEUandSW(withlowerF1-scoresthanthebaseline),11 aswellasAR,ID,and
UR (languages with the most significant improvement). The delta bar plot in Fig-
ure4.4illustratesthecomparativeanalysis.
Compared to the baseline, AR, ID, and UR consistently improve in Tag+Span
andExtraExtractioncategories. AllexceptIDshowenhancementinSpandetection,
whilemostlanguagesexhibitpoorerperformanceinMissingExtraction. Ontheflip
side,EUandSWresultinslightlyworseSpanandExtraExtractionerrors.
We further investigate the reasons for this behaviour. In ID, we observe that
around80%oftheSpanerrorsareduetoadditionallyidentifyingthetoken“ALIH”
(means “moving”, “changing” in English) as the start of an entity. For example,
for the input [“ALIH”, “Indofood”, “Sukses”, “Makmur”], the gold entity is “ORG:
Indofood Sukses Makmur”, whereas the model predicts “ORG: ALIH Indofood Sukses
11ThaiisexcludedduetoproblematictokenisationinWikiAnn,whereeverythingistokenisedinto
individualcharacters.4.5. MainResults 65
200 1000 50
0
0 0
200
400
1000 50
600
2000 100
AR ID UR
2000 30
1500 20 Tag
Span
1000
10 Tag+Span
500 Missing
0 Extra
0
500 10
EU SW
FIGURE 4.4: Error Delta (lower is better) for different types of errors in the
WikiAnn test set between vanilla XLM-R-base and PEP +MLM. We show er-
MS
ror count differences for AR, ID and UR, the three languages with the largest
F1-scoreimprovement,aswellasEUandSW,thetwolanguagesthatunderper-
formthebaseline.
Makmur”. This pattern is also observed in XLM-R, accounting for 68% of the span
errors. Consultation with a native speaker reveals this may be an inaccuracy of the
dataset(“ALIH”shouldnotappearbeforetheactualentitiesinWikiAnn).
As for EU, a lower overlap between WikiAnn entities and Wikipedia (only 47%
compared to the average of 57% across all WikiAnn languages) might explain the
predictionofadditionalentitiesnotpresentinthedataset.
4.5.2 FactRetrieval
For X-FACTR, models trained with Entity Prediction consistently outperform the
baseline, while MLM performsworsethanvanillaXLM-R,asanticipated. Notably,
WEP achieves the best results for single-token classification, showcasing a remark-
able +10% gain over XLM-R. On the other hand, PEP , which involves masking
MS
part of the entity tokens for prediction, excels when dealing with multi-token en-
tities. Interestingly, models trained on large parallel data, such as InfoXLM12 (Chi
etal.,2021a)andXLM-Align(Chietal.,2021b),demonstratepoorperformancewith
12https://huggingface.co/microsoft/infoxlm-base
atleD
rorrE
atleD
rorrE66 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
LATIN SCRIPT NON LATIN SCRIPT
MODEL
ES DE FR PT TR avg ZH JA HI avg
XLM-ROURS 81.5 79.8 74.8 76.5 43.0 71.1 77.2 56.8 50.6 61.5
MLM 78.8 78.0 74.4 74.6 39.7 69.1 76.4 70.3 61.5 69.4
PEP 79.3 79.7 75.3 76.2 45.3 71.1 77.8 69.0 62.9 69.9
MS
PEP +MLM 81.3 81.4 78.2 76.1 42.1 71.8 78.8 68.8 65.8 71.1
MS
TABLE 4.6: F1-score (average across five seeds) for languages with Latin and
Non-LatinscriptonMultiATIS++testsetwhenusingSF-onlytraining.
single-tokenaccuracyof3%and5%,respectively,andlessthan1%multi-tokenaccu-
racy. This discrepancy can be attributed to their focus on sentence-level alignment.
ResultsperlanguageforX-FACTRareavailableinTableA.2ofAppendixA.
4.5.3 SlotFilling
In the case of SF-only training, the most effective model is PEP +MLM, showcas-
MS
ing a +2.4% gain over XLM-R on MultiATIS++. This performance is competitive
with the best result from XLM-Align (74.4, see Appendix A). Conversely, no im-
provements are observed in MTOP over the baseline. A manual inspection of the
dataset reveals that this disparity can be attributed to domain differences. Multi-
ATIS++ contains entities such as city names, whereas MTOP consists of dialogues
with a personal assistant, involving tasks like setting up reminders, where fewer
entitiesoccur,limitingthebenefitsofentity-centricCStraining.
When jointly optimising SF and Intent Classification, models also demonstrate
improvementsoverthebaselineonSF(+1.4%forMultiATIS++and+0.5%forMTOP),
albeitwithlowergains. Wespeculatethattheadditionalintentlabelsoffercomple-
mentaryinformationtothetask,mitigatingtheimpactofexternalinformation.
We subsequently categorise languages in MultiATIS++ based on whether they
sharethesamescriptasEnglish(Latin)andscrutinisetheirperformanceinSF-only
training. As evident in Table 4.6, models trained on the ENTITYCS corpus exhibit
significantimprovementsinlanguageswithnon-Latinscripts,achievinganaverage
gain of +9.6%. This suggests that entity-focused training enables models to cap-
ture information that proves particularly beneficial for languages featuring scripts4.5. MainResults 67
different from English. Language-specific results on MultiATIS++ can be found in
TableA.3ofAppendixA.
4.5.4 WordSenseDisambiguation
In the case of XL-WiC, we note the modest improvement across tasks, with WEP
yielding the best performance at +1.3% over the baseline. This trend can be at-
tributedtothetask’snature,whereourentity-basedtrainingobjectivesassumethat
disambiguationhasalreadybeenaddressedandistreatedasimplicitexternalinfor-
mation. Notably,intheevaluationofXLM-Align,whichleveragesparalleldata,we
observe no enhancement in disambiguating word-level semantics across languages
(57%accuracy). Per-languageperformanceonWSDisincludedinTableA.4ofAp-
pendixA.
4.5.5 ComparisonwithModelstrainedwithParallelData
WesummarisethecomparisonwithInfoXLMandXLM-AligninTable4.7. Although
theseresultsarenotfaircomparisonsfor ENTITYCS,giventhatInfoXLMandXLM-
Align utilise parallel data, it is evident that ENTITYCS consistently demonstrates
competitiveorsuperiorperformanceacrosstheboard.
NER (F1) FACT RETR. (ACC.) SLOT FILLING (F1) WSD (ACC.)
MODEL WIKIANN X-FACTR MULTIATIS++ MTOP XL-WIC
all single multi SF SF
XLM-ROURS 61.6
0.28
3.5 9.4 2.6 70.6
1.55
72.3
0.98
59.1
1.52
INFOXLM* 62.8 1.1 3.3 0.6 73.9
1.95
74.7
0.30
56.9
0.81
XLM-ALIGN** 63.7 1.5 5.0 1.0 74.4
0.29
74.9
0.36
56.9
1.22
WEP 62.4 0.68 6.1 19.4 3.0 71.6 1.20 73.2 0.89 60.4 0.97
PEP MS 63.3 0.70 6.0 15.0 4.3 73.4 1.70 71.5 0.67 60.2 0.85
PEP MS+MLM 64.4 0.50 5.7 13.9 3.9 74.2 0.43 73.0 0.33 59.8 0.75
TABLE 4.7: Comparison with models using parallel data. Results for InfoXLM
and XLM-Align are obtained from Chi et al. (2021a) and Chi et al. (2021b), re-
spectively.68 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
X-FACTR MULTIATIS++
MODEL WIKIANN
all single multi SF SF/Intent
XLM-ROURS 61.6 3.5 9.4 2.6 70.6 73.0/88.9
EN 61.0 1.1 2.7 0.7 71.5 72.1/89.6
MLM 39 63.5 2.6 6.4 1.7 72.5 73.8/90.2
93 63.3 2.7 6.8 1.8 72.7 73.4/89.6
EN 61.9 3.3 8.5 1.6 71.8 72.2/91.1
WEP 39 62.4 6.1 19.4 3.0 71.1 71.7/89.7
93 59.4 5.8 18.6 2.7 70.4 72.9/90.3
EN 61.2 2.7 6.6 1.6 71.3 72.3/90.7
PEP
MS 39 64.4 5.7 13.9 3.9 73.4 74.4/90.0
+MLM
93 63.6 5.5 13.2 3.8 72.8 72.7/90.8
TABLE 4.8: Results (average over five seeds) with a different number of pre-
traininglanguages.
4.6 Analysis
Weconductfurtheranalysisondifferentmaskingstrategies,examiningtheirimpact
across languages and training steps during intermediate training on the ENTITYCS
corpus. Our primary focus is on WikiAnn, which encompasses the largest number
oflanguagesfromthedatasetsweevaluate.
4.6.1 PerformancevsLanguagesinIntermediateTraining
For WikiAnn, X-FACTR, and MultiATIS++, we conduct additional experiments by
training MLM, WEP,and PEP +MLM withvaryingnumbersoflanguagesinthe
MS
ENTITYCScorpus. Weexplorethreescenarios: usingEnglishonly(noCodeSwitch-
ing), employing the subset of 39 languages (as mentioned in subsection 4.4.3), and
incorporatingall93languages.
AsshowninTable4.8,modelstrainedsolelyonEnglishsentencesdonotexhibit
a noticeable improvement in average performance across languages (except for In-
tent Classification accuracy and WikiAnn with WEP over the baseline). However,
English-only training proves beneficial for English performance, with an average
gain of +23.1% for single-token and +5.6% for multi-token predictions in X-FACTR
overthebaselineXLM-R,using WEP.
When trained with all 93 languages, models employing all masking strategies
demonstrate improved performance compared to XLM-R. However, the prevailing4.6. Analysis 69
trend indicates that training on a more restricted set of languages generally results
inbetterperformance. Thissuggeststhatincorporatingabroaderarrayoflanguages
maynotnecessarilycontributetosuperiorresults,underscoringthenon-trivialchal-
lengeofscalingtotoomanylanguages. Notably,thesubsetof39languagesalready
coversmostlanguagesinthedownstreamtasks. Inscenarioswhereadditionallan-
guages are introduced, an increase in the number of pre-training languages could
potentiallyleadtoimprovedperformance.
4.6.2 PerformancevsTrainingSteps
Figure4.5providesacomparativeanalysisofdifferenttrainingobjectivesontheEN-
TITYCS corpus, showcasing their performance across the number of training steps
on the WikiAnn test set. The figure reveals that most masking strategies reach a
plateau after the middle of training. Notably, objectives involving MLM training
exhibit a clear performance increase across the board, underscoring the benefits of
jointtrainingforentitiesandnon-entities,notonlyintermsofperformancebutalso
in terms of smoother learning curves. Importantly, all objectives consistently out-
performthebaselinethroughoutthetrainingprocess.
It’s worth noting that the observed gains from Code Switching intermediate
training do not stem from additional training data. This is substantiated by Ta-
ble 4.8, where the WikiAnn F1-score trained on English-only sentences (without
CodeSwitching)showsthatadditionalEnglish-onlytrainingdatadoesnotimprove
upontheXLM-Rbaseline(61.6). ThisobservationisfurthersupportedbyFigure4.5
at step 200K, corresponding to the steps required for training on the English-only
sentences. At this point, all models exhibit an F1 score above 62.3, indicating that
the NER performance gain can be attributed to the design of the ENTITYCS corpus
andtheassociatedtrainingobjectives.
4.6.3 RandomandSameTokenPrediction
We further investigate the influence of Random token substitution and Same token
prediction when implementing PEP. By comparing PEP , PEP , and PEP in
MRS MS M
Figure 4.5, a clear trend emerges: incorporating Random token substitution in PEP70 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
WEP PEPM baseline WEP+MLM PEPM+MLM MLM
65.0 PEPMS PEPMRS PEPMS+MLM PEPMRS+MLM
64.5
64.0
63.5
63.0
62.5
62.0
61.5
1 5101520 30 40 50 60 70 80 1 5101520 30 40 50 60 70 80
Training Steps Training Steps
FIGURE4.5: F1-scorecomparisononWikiAnntestset(averageacrossfiveseeds)
as a function of the number of training steps (in ten thousands) with various
maskingobjectives. EP-onlystrategiesareontheleft,andEP+MLMstrategies
areontheright.
leads to inferior performance while omitting the prediction of Same tokens has a
negligibleeffect.
Intuitively,Randomtokensubstitutionintroducestheriskofpredictinganincor-
rect entity, potentially undermining the model’s learning by steering it towards in-
accuratepredictionsfromtherandomlyreplacedtokens. Onthecontrary,predicting
the Same token involves a more straightforward task of replicating the input entity
to the output. Consequently, models appear to neither gain nor lose performance
withorwithouttheSametokenprediction.
These observations align with the findings in Wettig et al. (2023), albeit their
focus on monolingual settings. A similar pattern can also be observed when com-
bining PEP with MLM.
4.6.4 EntityMaskingPercentage
Toassesstheimpactofthepercentageoftheentitymaskingcandidatesduringtrain-
ing,weincrementthemaskingprobability(p)from50%to80%and100%forthebest
model, PEP +MLM,andevaluateitseffectsontheWikiAnntestset.
MS
Notably, we observe that a further increase in the masking percentage leads to
a performance decline, from a 64.4±0.5 F1-score to 63.8±0.7 at 80%, and 64.0±0.4 at4.7. ConclusionandFutureWork 71
100%. We posit that masking too many tokens renders the task of entity prediction
from the remaining context more challenging. However, the minimal performance
differencebetweenpercentagesisnoteworthy,likelystemmingfromtheobservation
that,onaverage,onlytwoentitiesexistpersentence,asdemonstratedinTable4.1.
4.7 Conclusion and Future Work
In summary, this chapter underscores the valuable role of multilingual knowledge
sourced from Wikipedia and Wikidata in enhancing zero-shot transfer learning for
entity-oriented tasks through entity-level Code Switching. The creation of the EN-
TITYCS corpus, leveraging English Wikipedia and Wikidata, facilitates the substi-
tution of entities in wikilinks with their counterparts in multiple languages. By in-
troducingentity-orientedtrainingobjectives,weconsistentlyimproveperformance
across various datasets, including Named Entity Recognition, Fact Retrieval, and
WordSenseDisambiguation,surpassingbaselinemodelsandoutperformingprevi-
ousmethodsrelyingonextensiveparalleldata. Notably,ourapproachdemonstrates
competitiveperformanceinSlotFilling.
Ourfindingshighlightthetask-specificoptimalnatureofdifferentmaskingstrate-
gies, revealing that Whole Entity Prediction excels when emphasising single-token
factualknowledge,whilePartialEntityPredictionisparticularlybeneficialforentity
typing and multi-token factual retrieval. Simultaneously predicting non-entity and
entity tokens proves advantageous for tasks where the entire input context plays a
crucialrole,withanotableimpactonlanguageswithnon-Latinscripts.
Thegenericnatureofourcorpusconstructionprocessallowsscalabilitytoamore
extensive range of languages, underlining the broader applicability of our method-
ology. Further research could explore Code Switching beyond entities, encompass-
ingverbsandphrases. However, itisimportanttoacknowledgethepotentialchal-
lengesassociatedwithautomaticallycode-switchingpartsofsentencesbeyondenti-
ties,particularlyintheabsenceofreadilyavailablealignedcorpora. Wehypothesise
that aspecifically trainedmodule islikely required todiscern suitabletarget candi-
dates,especiallywhenmultipletranslationsareavailable. Thismodulewouldneed72 Chapter4. Entity-CentricCode-SwitchingforEnhancedCross-lingualTransfer
to understand the context of the sentence and ensure the selection of the most ap-
propriateoptionforcode-switchingthatdoesnotalterthesemanticmeaningorthe
syntax of the original sentence. On the other hand, future works may consider in-
corporating additional sources beyond Wikipedia and Wikidata (which requires an
entity linking module), which provide insights into the approach on the more gen-
eraldomains. Overall,ourstudyreinforcesthepotentialofleveragingmultilingual
knowledgesourcesinadvancingcross-lingualtransferlearning.
4.8 Limitations
Thelimitationsofthischapterareoutlinedasfollows:
• MorphologicalInflexionOversight: Beforecode-switchinganentity,itsmor-
phologicalinflexionisnotverified. Thiscanpotentiallyintroduceerrors,asthe
formofthecode-switchedentitymightnotalignwiththesurroundingcontext
(e.g., plural form). Addressing this issue is crucial for future versions of the
corpus.
• Language Diversity Constraint: The diversity of languages in the ENTITYCS
corpusisconstrainedtotheoverlapbetweenWikiDataandXLM-Rpre-training
languages. While this decision facilitates a more robust model comparison,
there is potential to enrich the corpus with additional languages not covered
byXLM-R.
• Task Specificity in Evaluation: The proposed approach’s primary evaluation
iscentredonentity-centrictasks. Theexplorationofbroadernaturallanguage
understanding tasks, such as Natural Language Inference, is feasible, and in-
vestigating the impact on such tasks may provide more insights into the gen-
eralisabilityofthemodels.
• Unidirectional Code-Switching: Code-switching is performed only from En-
glish to other languages while maintaining the context in English. Although
the choice is based on the abundance of English resources for studying cross-
lingualtransfer,exploringbidirectionalcode-switching,particularlyfromnon-
English articles to English, is a promising direction for future research. We4.8. Limitations 73
anticipatethatamodeltrainedusingbidirectionalcode-switchingcouldyield
similarenhancements,asobservedinpriorworkbyZ.Jiangetal.(2020),how-
ever, one limiting factor may be the availability of Wikipedia articles in very
low-resourcelanguages.
• Model Size: The experiments were conducted solely with base-sized models
for speed considerations. Extending these experiments to larger models is a
logical progression that could provide valuable insights into scalability and
performanceimprovements.75
Chapter 5
Faithful and Robust Knowledge
Extraction on the Web
Intheprevioustwochapters,weexploredhowstructuredknowledgeenhancesNLP
applications, including fake news detection and multilingual entity-centric tasks.
Thischapterfocusesoninvestigatingfaithfulandrobustmethodsforextractingin-
formationorstructuredknowledgefromwebtext.
The main content of the chapter is based on the paper “WEBIE: Faithful and
Robust Information Extraction onthe Web” (Whitehouse et al., 2023b) published in
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(ACL2023).
5.1 Background and Introduction
InformationExtraction(IE)isthetaskofextractingstructuredinformationfromun-
structuredtext,typicallypresentedintheformoftriples<subject,relation,object>. It
isessentialformanyNaturalLanguageProcessingapplicationssuchasknowledge
basepopulation,questionanswering,faithfulsummarisation,andfakenewsdetec-
tion (Trisedya et al., 2019; Huguet Cabot and Navigli, 2021; Narayan et al., 2021;
Whitehouseetal.,2022).
Closed IE systems, specifically those extracting triples with predefined entities
and relations from a knowledge base (KB), require two essential pieces of informa-
tion: (i)theentitiesmentionedinthetextand(ii)therelationsbetweeneachpairof
entities. Duetotheexpenseofannotations,mostexistingIEdatasets,likeWikiNRE76 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
(Trisedya et al., 2019) or REBEL (Huguet Cabot and Navigli, 2021), are constructed
using Wikipedia. Entities are linked through wikilinks, and relations are automati-
cally extracted using a distant supervision (DS) approach (Mintz et al., 2009) based
onaKBsuchasWikidata,whichassumesthatiftwoentitiessharearelationinaKB,
sentencesmentioningbothentitiesexpressthatrelation.
Whilemodelstrainedonlyonthisfact-richdomain1 haveshowntobeusefulfor
IEapplications,theyhavelimitedcapacitywhenappliedtoextractinginformationin
otherwebdomains,whichoftencontainsnoisytextortextwithoutanyfactualinfor-
mation. Forexample,AllenAI’sC4dataset,2anopen-sourcedversionofGoogle’sC4
(Raffel et al., 2020b) dataset based on Common Crawl, demonstrate this challenge.
Our analysis using the DS approach reveals that fewer than 15% of the sentences
contain triples (refer to subsection 5.2.3). In other words, the remaining sentences
extracted from C4 either do not contain entities or do not have entities that share a
relation. In such cases, faithful IE models are expected not to output triples. How-
ever, astate-of-the-artgenerativeIEmodel, GenIE(Josifoskietal., 2022), trainedon
REBEL, generates triples for nearly every sentence, leading to a high false positive
rateandhallucinationissues.
To address these challenges and facilitate future work on IE on the web, we
present WEBIE, the first large-scale, entity-linked closed IE dataset collected from
web sources. The WEBIE dataset is collected from the 200 most frequent URL do-
mainsfromtheC4dataset. First,weuseReFinED(Ayoolaetal.,2022),astate-of-the-
artEntityLinking(EL)modeltoidentifymentionspansoftheentitiesandlinkthem
to Wikidata. We then apply the DS approach to extract triples and use a Natural
Language Inference (NLI) model to filter out triples not expressed by the sentence.
We also include negative examples, i.e., sentences without any factual information,
tobetterreflectthedataontheweb. Ourfinaldatasetconsistsof1.6Msentences,and
we annotate a subset of ∼21K triples through crowdsourcing. The annotated set is
exclusivelyusedaspartofthetestsettoallowmorereliableevaluation. Finally,we
introduce mWEBIE, which contains human-corrected translations of the annotated
versionof WEBIE infourlanguages: French,Spanish,Portuguese,andHindi.
1WeusethetermdomaintorefertotheURLdomain.
2Weusethedatasetfromhttps://huggingface.co/datasets/allenai/c4.5.1. BackgroundandIntroduction 77
Previousstudieshavehighlightedthesuperiorityofgenerativemodelsoverdis-
criminative pipelines, which often suffer from accumulative errors due to separate
EntityLinkingandRelationExtraction(RE)steps(Mesquitaetal.,2019; Trisedyaet
al.,2019;Josifoskietal.,2022). Therefore,weprimarilybenchmarkWEBIEwithgen-
erative, transformer-based encoder-decoder models, BART (M. Lewis et al., 2020)
andmBART(Tangetal.,2021). Thelatterisusedforzero-shotcross-lingualtransfer
performanceevaluationonmWEBIE.
Additionally, we propose three training strategies (subsection 5.3.2) that utilise
entitylinkingasanauxiliarytaskforgenerativeIE:jointgenerationwiththelinked-
entity prompt (ENTITY-PROMPT), multi-task learning with distinguished artificial
prompttokens(ARTIFICIAL-PROMPT),andtrainingwithanadditionaltask-specific
language model (LM) head (2LM-HEADS). Our experiments demonstrate that in-
corporatingentity-linkingobjectivesleadstobetterandmorefaithfulIEresults. An
illustrationofthesetrainingstrategiesisprovidedinFigure5.1.
OurexperimentsrevealthatmodelstrainedonWEBIEaremorerobustandgen-
eralisable compared to models trained solely on Wikipedia datasets. These models
achieve a new state-of-the-art performance on REBEL (section 5.5) and competitive
zero-shot performance on WikiNRE. We demonstrate that WEBIE serves as a com-
plementarydatasettoexistingWikipedia-baseddatasetsandemphasisethesignifi-
canceofincludingnegativeexamplestoaddressfalsepositivesingenerativeIE.
Ourmaincontributionsareasfollows:
• Weintroduce(m)WEBIE,thefirstlarge-scale,entity-linkedIEdatasetfromthe
web, with a subset annotated by humans and translated into four other lan-
guages.
• Weproposeandassesstheeffectivenessofusingentitylinkingasanauxiliary
taskforgenerativeIEthroughvarioustrainingstrategies.
• Comprehensive experiments demonstrate that models trained on WEBIE ex-
hibit enhanced generalisability in Information Extraction from the web do-
main,includingcompetitivezero-shotperformanceonIEtasksonWikipedia.78 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
BART
<sub> Barack Obama <rel> place of birth <obj>
Obamawas born in
Honolulu<et> <sub> Barack Obama <rel> place
BART
Honolulu, Hawaii. of birth <obj> Hawaii<et> <sub> Honolulu <rel>
located in <obj> Hawaii<et> …
+ ENTITY-PROMPT
[ENTITY] Obama# Barack Obama | Honolulu#
Honolulu| Hawaii# Hawaii[TRIPLE]<sub> Barack
Obamawas born in
BART Obama <rel> place of birth <obj> Honolulu<et> <sub>
Honolulu, Hawaii.
Barack Obama <rel> place of birth <obj> Hawaii<et>
<sub> Honolulu <rel> located in <obj> Hawaii<et> …
+ ARTIFICIAL-PROMPT
Obama# Barack Obama | Honolulu
<#el#> Obamawas born
# Honolulu| Hawaii# Hawaii
in Honolulu, Hawaii.
BART
<sub> Barack Obama <rel> place of birth
<#tri#> Obamawas <obj> Honolulu<et> <sub> Barack Obama
born in Honolulu, Hawaii. <rel> place of birth <obj> Hawaii<et> <sub>
Honolulu <rel> located in <obj> Hawaii<et> …
+ 2LM-HEADS
LM-Head 1 Obama# Barack Obama | Honolulu
# Honolulu| Hawaii# Hawaii
Obamawas born in
Honolulu, Hawaii. BART LM-Head
2
<sub> Barack Obama <rel> place of birth
<obj> Honolulu<et> <sub> Barack Obama
<rel> place of birth <obj> Hawaii<et> <sub>
Honolulu <rel> located in <obj> Hawaii<et> …
FIGURE 5.1: Illustration of the training strategies. The blue and green text re-
fer to mention span and its corresponding Wikipedia title (used as entity labels).
For standard BART training, the target output is the linearised triples (subsec-
tion 5.3.1). For ENTITY-PROMPT, the target is the EL output (subsection 5.3.2)
concatenated with the linearised triples. In ARTIFICIAL-PROMPT, we prepend
anartificialtokentotheinputtoindicatethedesiredoutput,EL(yellowbox)or
linearisedtriples. For2LM-HEADS,weaddanadditionaltask-specificLMhead
tothedecoderfortheELtask(greybox).
5.2 Related Work
Wereviewtherelatedworkfromthefollowingtwoaspects: datasetsandapproaches
forInformationExtraction.5.2. RelatedWork 79
5.2.1 InformationExtractionDatasets
The term Information Extraction has been used for different tasks in the literature.
Most existing IE datasets are collected from Wikipedia articles aligned with Wiki-
data,includingsentence-levelIEdatasetssuchasREBEL,WikiNRE,FewRel(Hanet
al.,2018),T-REx(Elsaharetal.,2018);document-levelRelationExtraction3 datasets,
e.g., DocRED (Y. Yao et al., 2019), CodRED (Y. Yao et al., 2021). SMiLER (Seganti et
al.,2021)isamultilingualsentence-levelIEdatasetthatisalsobasedonWikipedia,
covering 14 languages and 36 relations. These sentence-level IE datasets typically
donotcontainnegativeexamples. Althoughitispossibletoaugmentthesedatasets
with negative instances sourced from Wikipedia, they do not adequately reflect the
true data distribution on the web. To enable more general information extraction
models beyond solely relying on Wikipedia articles, we argue for the necessity of
datasets that accurately mirror web data – a point later confirmed in our experi-
ments(refertosection5.5).
Datasets such as TACRED (Yuhao Zhang et al., 2017), RE-TACRED (Stoica, Pla-
tanios, and Poczos, 2021), and WebRED (Ormandi et al., 2021) contain negative re-
lation examples, but they lack linkage to knowledge bases. Furthermore, applying
entity linking to a knowledge base in a post-hoc fashion is not trivial, as these rela-
tion extraction datasets like WebRED primarily focus on relation, where the subject
andobjectinatriplecanbeanamesuchasAlice,whichmaynotbelinkabletoaKB.
Therefore, our proposed dataset WEBIE stands out from existing datasets in that it
pertainstothewebdomain,isentity-linked,andincludesnegativeexamples.
5.2.2 InformationExtractionApproaches
IEapproachescanbeclassifiedintotwocategories: pipelinesystemswithdiscrimi-
nativemodels,andsequence-to-sequencesystemswithgenerativemodels. Pipeline
models typically include separate modules for Named Entity Recognition (NER),
EntityLinkingandRelationExtraction(Chagantyetal.,2017;Yamadaetal.,2020b).
3We consider RE dataset as the ones that focus on extracting relations but without entity spans
and/orlinkinginformation.80 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
SystemsthatjointlytrainNER,EL,andRE,havealsobeenexplored, takingadvan-
tage of the information shared among the tasks (B. Ji et al., 2020; Eberts and Ulges,
2020).
Inrecentyears,generativeIEhasgainedalotofattention. NayakandNg(2020)
utilise an LTSM model and propose a pointer network-based decoding. More re-
centapproaches,e.g.,asintroducedinREBELandGenIE,trainatransformer-based
encoder-decoder model with standard maximum-likelihood objectives to convert
sentencestolinearisedoutput. KnowGL(Rossielloetal.,2023)improvesuponREBEL
withadditionalentitytypeinformationaddedtothelinearisedoutput. Ourworkex-
tendsGenIEandexperimentswiththreedifferentapproacheswhereweincorporate
explicitELinformationasanauxiliarytaskwithadaptedconstraintTriedecoding.
5.2.3 WEBIE Collection
In this section, we provide a detailed explanation of the dataset collection process
for(m)WEBIE.
DataPre-processing
We begin the process with the English segment of AllenAI’s C4 dataset, retaining
themostfrequent200URLdomains. Subsequently,onemilliondocumentsareran-
domlysampled,andSpaCy4isemployedforsentencesegmentation. Sentencescon-
tainingfewerthan10wordsareremoved,resultinginapproximately20millionsen-
tences.
EntityLinkingandDSDataset
Next, we run ReFinED (Ayoola et al., 2022), a state-of-the-art EL model on the sen-
tences to identify entity spans and link them to their corresponding Wikidata ID.
Besides named entities, ReFinED also extracts numerical entities that do not have
Wikidata ID. In this work, we only consider numerical entities that express dates
andmapthemtothecorrespondingyearforsimplicity.5 SomeexamplesofReFinED
processedoutputareincludedinTableB.2inAppendixB.
4https://spacy.io/
5Forexample,“October10,2018”willbemappedto“2018”.5.2. RelatedWork 81
After obtaining the entity-linked sentences, we apply the DS paradigm to re-
trieve the set of relations that exist between each pair of entities in each sentence
using Wikidata (September 2022 dump) as our KB and build a DS dataset. After
theabovesteps,weobtain WEBIE DSdatasetconsistingof21.2Mentitiesand4.8M
triples.
EntailmentFiltering
OnemajordrawbackoftheDSapproachisthatthetriplesextractedmayormaynot
be expressed by the source sentence (Riedel, L. Yao, and McCallum, 2010). Follow-
ing previous work on obtaining a cleaner version of the DS dataset (Huguet Cabot
and Navigli, 2021; Vania, Grace Lee, and Pierleoni, 2022), we apply an NLI model,
nli-deberta-v3-large,6 that is trained on SNLI (S. R. Bowman et al., 2015) and
MultiNLI (Williams, Nangia, and S. Bowman, 2018), to filter out triples that do not
entailthesentence. Eachsourcesentenceistreatedasthepremiseandweusemanu-
ally created templates (similar to Vania, Grace Lee, and Pierleoni (2022)) to convert
aDStripletooneormorehypotheses.
Wethenobtaintheentailmentprobabilityscoreforeachpremise-hypothesispair
and take the maximum score for cases with multiple converted hypotheses. We set
the threshold to be 0.7, similar to Huguet Cabot and Navigli (2021), and only keep
tripleswithanentailmentscoreabovethethreshold. Weretain2.1Mtriples(44%of
thepreviousDStriples,seeTable5.1)afterthisfilteringprocess.
NegativeExamples
AftertheDScreationandNLIfilteringsteps,onlylessthan10%oftheoriginalsen-
tencescontaintriples. Totrainmodelsforextractingfactsfromthewebandalleviate
false positives, we include two kinds of negative examples in WEBIE: (i) sentences
with one or zero entities, and (ii) sentences with two or more entities, but without
anyfactualinformation(i.e.,norelationbetweentheentities). Werandomlysample
negative instances covering both cases evenly and add them to WEBIE. In the end,
WEBIEconsistsof1.6Msentences,where50%arenegativeexamples. Asummaryof
6https://huggingface.co/cross-encoder/nli-deberta-v3-large achieved superior results
amongthemodelsweevaluatedinourpreliminaryexperiments.82 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
Dataset Domain Entity Relation Sentence Train† Val† Test† Triple Anno. Negative Language
LinkedType Triple Instance (TestSet)
TACRED Web ✗ 42 106,264 68,124 22,631 15,509 106,264 106,264 79.5% 1
WebRED Web ✗ 523 117,717 – – – 117,717 117,717 65% 1
WikiNRE Wikipedia ✓ 158 255,654 224,881 988 29,785 330,005 0 0 1
REBEL Wikipedia ✓ 1146 3,059,894 2,754,387 152,672 152,835 10,311,293 0 0 1
WebIE Web ✓ 661 1,649,167 1,480,223 82,914 86,030 1,905,205 21,113 50% 5
TABLE 5.1: Statistics of WebIE and comparison with other sentence-level RE
(top two rows) and IE datasets. Sentences correspond to the number of dis-
tinct sentences. We report the publicly available version of WebRED. † shows
the number of examples in each split. Anno. Triple represents the number of
human-annotatedtriples.
thestatisticsof WEBIE withacomparisonwithotherdatasetsisshowninTable5.1.
Thedatasetisrandomlysplitintotrain/validation/testsetsusinga90/5/5split.
5.2.4 HumanAnnotation
Existing IE datasets, such as REBEL, are often automatically annotated using the
DS approach, hence the labels can be noisy. To allow a more reliable evaluation of
WEBIE,werandomlysample∼21Ktriplesfromthemostfrequent200relationsand
annotate them with MTurk. Given a sentence, each HIT (Human Intelligence Task)
is designed to verify if a DS triple is correctly expressed in the sentence.7 First, the
annotators are asked to verify if the head entity (subject) and tail entity (object) are
linked correctly. For each entity, we provide its Wikipedia title and link to its Wiki-
data page as additional context. After that, the annotators are asked to verify if the
triple relation is correctly inferred from the sentence. Here, we provide the relation
descriptionsandexampleusecasesofeachrelation. WeaskthreeMTurkworkersto
annotate each DS triple and take the majority vote as the final label for each triple.
AtripleisconsideredvalidifbothentitiesarelinkedtothecorrectWikidataentities
and the relation is inferred8 by the sentence. An annotation interface is shown in
AppendixB.
To ensure the annotation quality, we set qualifications with additional require-
ments for MTurk workers (see Appendix B for details). The agreement among the
three annotators is high: 99.4% for the head entities, 99.2% for the tail entities, and
7WeensureallDStriplesinaselectedsentenceareannotated.
8We ask for inferred instead of explicit expression since some relations may not be explicitly ex-
pressedinthesentence,e.g.,“locatedin”(London,UK)or“dateofbirth”XX(1986-2022).5.3. GenerativeInformationExtraction 83
76.1% for the relations have all three annotators agreeing on the same label. After
the majority vote, 92.1% of the triples are labelled as inferred and therefore kept as
validtriples.
5.2.5 Multilingual WEBIE
To enable zero-shot cross-lingual transfer evaluation on WEBIE, we further extend
the annotated subset, with additional negative examples, to four other languages:
French, Spanish, Portuguese, andHindi. First, weuseaneuralmachinetranslation
model, thedistilled1.3Bvariant,9 ofNLLB-200(Costa-jussàetal., 2022)totranslate
the English sentences into the target languages. We then use MTurk to verify the
translationandaddentityspaninformationinthetranslatedsentences. Weprovide
theEnglishsentence(withtheentityspanshighlighted)anditstranslation,andfirst,
asktheannotatorstocorrectthetranslation. Afterthat,MTurkworkersareaskedto
markthecorrespondingentityspansinthetargetlanguage. Weasktwoannotators
to complete the aforementioned HIT, and an additional worker to select the better
translation, which is used in our final dataset. To obtain translations with higher
quality,werestricttheregionoftheworkerstocountrieswherethetargetlanguageis
theofficiallanguage.10 ThefinalmWEBIEconsistsof9Kinstancesineachlanguage,
whichcorrespondstoroughly90%ofthe21Kannotatedtriples.
5.3 Generative Information Extraction
Thissectiondescribesthetrainingstrategiesweuseforbenchmarking(m)WEBIE.
5.3.1 Sentence-to-TriplesGeneration
We use BART and mBART for all of our experiments. Given a sentence s as input,
we train the model to autoregressively generate the linearised triples t as an out-
put. Following the practice from Huguet Cabot and Navigli (2021) and Josifoski et
al. (2022), we linearise a triple t by converting it into “<sub> head entity label
i
9https://huggingface.co/facebook/nllb-200-distilled-1.3B
10SeedetailsformWEBIEannotationsinsectionB.3.84 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
<rel> relation <obj> tail entity label <et>”, wherethetagsinbracketsrep-
resent subject, relation, object, and the end of triple, respectively. Head/tail entity
label refers to the Wikipedia title that the mention span in the sentence is mapped
to,whichalsohasaone-to-onecorrespondencewiththeWikidataID.11
Foreachsentence,weorderitslinearisedtriplesaccordingtotheorderinwhich
they appear in the input sentence; first by the order of the appearance of the head
entity, and then by the order of the tail entity (for cases when the head entities are
the same). The conditional probability of generating t is formulated as p(t|s) =
∏N p(t |t ,s). We use the standard cross-entropy loss and maximise the output
t=0 i <i
sequence likelihood with teacher forcing (Sutskever, Vinyals, and Le, 2014). An ex-
ampleofinputandoutputcanbeseenatthetopofFigure5.1.
5.3.2 Entity-LinkingasanAuxiliaryTask
The standard linearised triples output only contains the label of the entity and not
thespan. Asaresult,itmaybedifficulttotracebackfromwhichinputspananentity
isgenerated,especiallyinthecasewhenthemodelhallucinates(e.g.,bygenerating
an entity that is not mentioned in the sentence). To encourage models to generate
faithful and interpretable output, we also experiment with models that are jointly
optimised for generating triples and EL. The goal of the EL task is to identify and
extract entity spans from the input sentence and link them to their corresponding
KBentities. WepositthataddingtheELtaskasanadditionaltrainingobjectivewill
teachthemodeltoattendtotheinputspanswhengeneratingtheoutput.
Weexperimentwiththefollowingthreeapproaches.
ENTITY-PROMPT
Narayan et al. (2021) and Narayan et al. (2022) have shown that generation with
entity-chainplanning,i.e.,generatingthedesiredentitiesfirstbeforetheactualout-
put, is effective in improving the faithfulness and controlling hallucinations in text
generationtaskssuchasabstractivesummarisation. ForgenerativeIEtasks,ELcan
beusedasanintermediateplantogroundthegenerationofthelinearisedtriples. We
11Forexample,amentionspanof“UK”islinkedtoWikipediatitle“UnitedKingdom”andmapped
toQ145inWikidata.5.3. GenerativeInformationExtraction 85
define the Entity-Linking target in the format of “Mention Span # Entity Label
1 1
| Mention Span # Entity Label | ...”,whereweordertheentityspansasthey
2 2
appearinthetext. WethenprependtheEntity-Linkingtargettothelinearisedtriples
target,usingspecialsymbolsasseparators,i.e.,“[ENTITY] Entity-Linking target
[TRIPLE] Linearised Triples Target”. Here “[ENTITY]” is the start symbol be-
foregeneratingtheELoutput,and“[TRIPLE]”isthestartsymbolbeforegenerating
the linearised triples. Given an input sentence, we essentially train the decoder to
first generate the EL chain and then generate the triples, conditioned on both the
inputsentenceandtheELoutput.12
ARTIFICIAL-PROMPT
Artificial Prompt tokens are symbols placed in front of the input sequence, which
has previously been explored in areas such as neural machine translation to distin-
guish the language of the target output translation (M. Johnson et al., 2017). We
adapt this approach for jointly training our models for Entity Linking and genera-
tiveIE.Specifically,weuseanartificialprompttoken<#el#>atthebeginningofthe
inputsentencewhentrainingfortheEntity-Linkingtarget,anduse<#tri#>13forthe
linearised output target. Training instances for both tasks are mixed and randomly
shuffledfortraining.
2LM-HEADS
Finally, inspired by Gontier, Reddy, and Pal (2022), the third approach that we ex-
perimentwithistheadditionofasecondlanguagemodel(LM)headinthedecoder,
whichisinitialisedwiththesameweightsasthefirst(standard)LMhead. Thefirst
LMheadisoptimisedforgeneratingthelinearisedtripleswhilethesecondLMhead
isoptimisedfortheELtask,thuseachtraininginstancehastwodifferenttargetout-
puts. During training, the input sentence is fed to the encoder once, and different
12The EL target only includes mention spans that contribute to valid triples, consistent with the
triplesthatarelatergeneratedconditionedonthelinkedentities.
13Bothartificialprompttokensareaddedasthespecialtokenstothetokenizertoavoidbiasfrom
pre-trainedembeddings,butareintendedtobebiasedtotheassociatedtask.86 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
targetoutputsaregiventothesamedecoder. Eachtask-specificLMheadisthenre-
sponsibleforgeneratingoutputtargetedforit. Thetraininglossisthenformulated
asaweightedsumofthelossesfrombothtasks: L = αL +(1−α)L .
IE EL
5.3.3 InferencewithaConstraintTrie
In addition to standard beam search decoding, we experiment with constraint de-
codingbyrestrictingthegeneratedoutputtobevalidWikipediatitlesandWikidata
relations using a prefix Trie, following the ideas proposed in GENRE (N. D. Cao et
al., 2021) and GenIE (Josifoski et al., 2022). The constraint Trie improves generative
IEbyensuringthatthegeneratedstringsarevalidentitiesorrelations. Weconstruct
two constraint Tries: an entity Trie and a relation Trie, following the same method
as introduced by N. D. Cao et al. (2021). The entity Trie is built using all Wikipedia
titles (as the entity labels), and the relation Trie is built using all Wikidata relation
propertylabels.
Weusefourspecialsymbols,<sub>,<rel>,<obj>and<et>todefinethestateof
the generation. We apply both constraint Tries as follows. We adopt the constraint
Triesothat,intheveryfirstdecodingstate,themodelisallowedtoeither(i)return
an empty string for a negative example, or (ii) generate <sub>, which is the start
symbol for generating a triple. If the <sub> symbol is generated, then we generate
theheadentityusingtheentityTrie,i.e.,onlyvalidentitieswillbeconsidered. Once
thegenerationoftheheadentityiscompleted,themodelproceedstogenerate<rel>
(i.e.,thestartsymbolforgeneratingrelationstring)andthensubsequentlygenerate
allowed tokens from the relation Trie which is built from the relations in Wikidata.
Afterthat, themodelgenerates<obj>andthetailentity, inthesamemanner, using
theentityTrie. Aftergeneratingthefulltriple(indicatedby<et>generatedafterthe
tail entity), the decoder can either stop the generation or start a new iteration for
generatingthenexttriple.
For the ENTITY-PROMPT models, since the entity mention spans are text from
theinputsentencesandusuallyarenotthesameastheentitylabelsinWikidata,we
proposeapartialconstraintgenerationapproach. Specifically, westartthestandard5.4. ExperimentalSetup 87
beamsearchfortheELtargetoutputandonlyactivatetheTrieconstraintsafterthat
whengeneratingthelinearisedtriples.
5.4 Experimental Setup
In this section, we explain the datasets used in the experiments and the detailed
modellingsetup.
5.4.1 Dataset
In addition to our proposed WEBIE dataset, we also use the following datasets for
ourexperiments.
WikiNRE (Trisedya et al., 2019) is an IE dataset based on Wikipedia which is au-
tomatically constructed by aligning Wikipedia sentences to Wikidata triples using
the DS approach. The authors apply a coreference resolution model (K. Clark and
Manning,2016)toobtainsentenceswithimplicitentitynamesanduseaparaphrase
detection model (Ganitkevitch, Van Durme, and Callison-Burch, 2013; Grycner and
Weikum, 2016) to filter out sentences that do not express the DS triples. In our ex-
periments,weonlyuseWikiNREforzero-shotevaluation.
REBEL (Huguet Cabot and Navigli, 2021) is a large-scale IE dataset constructed
automatically from Wikipedia abstracts. Using the Wikipedia hyperlinks in the ab-
stracts, as well as numerical values and dates, they map the entity spans to their
corresponding Wikidata entities. They then use the DS approach to identify triples
in each sentence. To filter out false positives, the authors use an NLI model by con-
catenatingtheentitiesandtherelationasthehypothesis. Inourexperiment, weuse
theREBELdatasetthatissub-sampledbyJosifoskietal.(2022),where857relations
are considered. Both WikiNRE and REBEL do not contain negative examples and
arenotannotatedbyhumans.88 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
5.4.2 Models
WeexperimentwithBARTusingtwosettings: BART withthepre-trainedweights
PLM
from M. Lewis et al. (2020),14 and BART , using the same configuration and ar-
RAND
chitecture but randomly initialised weights. Across the two settings, Josifoski et
al. (2022) find that BART generates better results than BART on REBEL. For
RAND PLM
mWEBIE, we experiment with the mBART-5015 model (for simplicity we refer to it
asmBARTinthisthesis).
To compare models trained on different datasets, we train both BART and
PLM
BART on REBEL (R), WEBIE (W), and both datasets together (R+W). We eval-
RAND
uate the performance of the generated triples by parsing the linearised output to a
list of triples and comparing it to the gold label to calculate precision, recall, and
F1 scores. For WEBIE, we also calculate the accuracy of the prediction of negative
instances,whereapredictionisconsideredcorrectifthemodelaccuratelygenerates
emptystringsratherthanhallucinatingtriples.
For training with EL as an auxiliary task, we primarily experiment with the
BART . We prepare the training instances as described in subsection 5.3.2, and
RAND
train separate models on REBEL and on WEBIE. For the 2LM-HEADS, we conduct
experimentswithdifferentvaluesoftheαparameterinthecombinedlossfunction,
specifically,wesetitto0.5and0.75.
Weuse8GPUs,eachwith32GVRAM,forallexperiments. Wesetthebatchsize
to 8 and the accumulated gradient batches to 32. We follow the hyper-parameters
settingsfromJosifoskietal.(2022)andsetthelearningrateto3e−5,weightdecayto
0.01, and warmup steps to 5K.16 We train for up to 30 epochs with early stopping
(patience 10), validate twice per epoch, and take the last checkpoint for evaluation.
Trainingoneepochtakesapproximately1.5hoursforBARTand2hoursformBART.
5.5 Results and Analysis
This section presents the main results of (m)WEBIE and compare different training
strategies.
14https://huggingface.co/facebook/bart-large
15https://huggingface.co/facebook/mbart-large-50
16ForBARTPLM(W)wefinditisnecessarytousealowerlearningrate5e−6formorestabletraining.5.5. ResultsandAnalysis 89
WEBIE(ALLTEST) WEBIE(ANNO.TEST) REBEL WIKI-NRE
MODEL
P R F1 Acc-N P R F1 Acc-N P R F1 P R F1
BARTRAND(R) 10.83 16.00 12.92 0.00 10.70 13.26 11.84 0.00 64.34 67.90 66.07 15.83 52.09 24.28
BARTPLM(R) 17.58 34.20 23.23 2.28 17.95 30.02 22.47 1.97 63.83 76.66 69.66 18.34 65.04 28.62
BARTRAND(W) 55.06 54.90 54.98 89.67 51.64 44.46 47.78 94.74 22.45 20.42 21.39 10.95 31.49 16.25
BARTPLM(W) 54.81 70.29 61.59 87.59 53.40 62.36 57.53 93.58 28.05 37.28 32.01 15.55 60.45 24.73
BARTRAND(R+W) 51.34 61.22 55.85 86.80 49.64 51.62 50.61 93.15 64.38 69.57 66.87 17.68 65.96 27.89
BARTPLM(R+W) 53.04 75.29 62.23 76.66 53.18 68.41 59.84 82.96 63.49 75.30 68.89 18.93 73.52 30.11
BARTRAND(R) 11.93 18.91 14.63 0.00 11.82 15.63 13.46 0.00 66.89 70.37 68.58 27.61 66.73 39.06
BARTPLM(R) 15.24 39.30 21.96 0.00 15.98 34.92 21.93 0.00 66.28 76.78 71.14 25.39 77.45 38.24
BARTRAND(W) 55.47 57.25 56.35 90.07 52.95 46.60 49.57 95.04 27.47 23.13 25.12 18.98 43.75 26.48
BARTPLM(W) 57.92 74.19 64.91 87.99 57.00 65.91 61.13 94.18 35.81 43.00 39.08 24.30 78.01 37.06
BARTRAND(R+W) 52.79 64.15 57.92 87.45 51.89 54.28 53.06 93.71 66.87 72.24 69.45 29.02 82.35 42.91
BARTPLM((R+W) 54.63 78.43 64.40 76.43 55.22 71.25 62.22 82.59 66.42 78.29 71.87 29.25 86.38 43.70
TABLE5.2: ExperimentresultsforbeamsearchwithandwithoutconstraintTrie.
PandRrefertoprecisionandrecall,respectively,andAcc-Nshowstheaccuracy
of the negative examples. BART corresponds to models with BART config-
RAND
uration but randomly initialised weights. BART are models with pretrained
PLM
weights from M. Lewis et al. (2020). (R), (W), (R+W) refer to models trained on
REBEL,WEBIE,andbothdatasets,respectively. ForWEBIEweshowtheoverall
performance and the accuracy on negative samples. Results in blue shades are
zero-shotperformance.
5.5.1 MainResults
Table 5.2 shows our benchmarking results on WEBIE. We report results with the
constraint Trie in decoding since it overall achieves better results. Contrary to the
findings from Josifoski et al. (2022), we find that BART models with pre-trained
weightsarebetterthanrandomlyinitialisedweights. ConstraintTriedecodingben-
efitsREBEL,WikiNRE,andtherecallperformanceof WEBIE,butmaycompromise
theprecisionsincethemodelsarealsotrainedtohandlenegativeexamples.
ModelstrainedonbothREBELandWEBIE (R+W)obtainoverallbetterF1scores
on the two datasets compared to models trained on each dataset separately. Sim-
ilar performance can also be observed in the zero-shot performance on WikiNRE.
Models trained solely on the REBEL dataset (Wikipedia-domain) show poor gener-
alisability on WEBIE17 and always generate false positives thus resulting in 0% ac-
curacyfornegativeinstancesin WEBIE.ThisindicatesthatWikipedia-domaindata
isnotadequatefortrainingrobustmodelsfortheweb, andtheabsenceofnegative
examplesinthesedatasetsleadstoaprominentissueofhallucinationwhenapplied
totheweb.
17Forpositiveexamplesitonlyachieves20F1points.
DENIARTSNOCNU
EIRTTNIARTSNOC90 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
UNCONSTRAINED CONSTRAINTTRIE
LANGUAGE
P R F1 Empty-Pos% Acc-N P R F1 Empty-Pos% Acc-N
ENGLISH 57.72 61.26 59.43 2.48 95.69 60.29 64.29 62.22 2.63 96.11
FRENCH 43.27 36.13 39.38 11.89 96.19 46.52 40.26 43.16 12.63 96.64
SPANISH 41.93 34.63 37.93 12.34 96.74 45.13 38.89 41.78 12.80 96.97
PORTUGUESE 41.17 32.37 36.24 14.07 96.91 44.15 36.61 40.02 14.82 97.22
HINDI 4.28 1.62 2.35 67.38 98.64 4.23 1.67 2.40 67.55 98.64
TABLE 5.3: PerformanceonmWEBIE withmBART.Resultsfornon-Englishare
zero-shot. Empty-Pos(itive)% shows false negatives percentage, revealing zero-
shotperformancehasahighrateofemptyresultsforpositiveexamples.
BART (R+W) alsoachievesanewstate-of-the-artF1scoreof71.87onREBEL,
PLM
surpassing the performance of 68.93 from GenIE (Josifoski et al., 2022) and 70.74
fromKnowGL(Rossielloetal.,2023),thelatterofwhichtrainswithadditionalinfor-
mation including entity type. The results demonstrate the benefit of WEBIE, which
contributestothegeneralisabilityofthemodels.
5.5.2 Cross-lingualTransferwithmBART
We train mBART on the training set of WEBIE and evaluate the zero-shot cross-
lingualtransferonmWEBIE.Similartopriorexperiments,resultsinTable5.3show
thatconstraintTriedecodingobtainshigherperformancethanstandarddecoding.18
ForEnglish,mBARTachieveshigheroverallperformancethanBART (seeTa-
PLM
ble 5.2). The zero-shot results reveal that Hindi has a significant decline in perfor-
mance compared to the other three non-English languages, French, Spanish, and
Portuguese. As these three languages utilise the Latin script as in English, which
may result in an overlap of entity surface forms. In contrast, the transfer is more
difficult for Hindi as it employs a different writing system. Manual analysis indi-
catesthatmBARTtendstoproduceahighrateoffalsenegativesinHindiexamples,
where the correct extraction mostly occurs when the entities in the sentences share
similarsurfaceformswiththeEnglishcounterparts.
5.5.3 ResultswithAdditionalELTraining
Table 5.4 shows the results of training with Entity-Linking as an auxiliary task. For
REBEL, the best results are achieved with the 2LM-HEADS approach, where the α
18WereportresultsusingENasthesourcelanguagetokenformBART,asitproducesbetterperfor-
mancecomparedtotheactualsourcelanguagetoken.SeemoredetailsinsectionB.1.5.6. ConclusionandFutureWork 91
REBEL WebIE(Anno.)
MODEL UNCONSTRAINED CONSTRAINTTRIE UNCONSTRAINED CONSTRAINTTRIE
P R F1 P R F1 P R F1 P R F1
BARTRAND 64.34 67.90 66.07 66.89 70.37 68.58 51.64 44.46 47.78 52.95 46.60 49.57
ENTITY-PROMPTS 63.30 63.04 63.17 67.91 67.54 67.72 49.64 51.62 50.61 51.90 54.28 53.06
ARTIFICIAL-PROMPT 64.23 68.23 66.17 66.41 70.72 68.50 52.33 46.21 49.08 53.86 48.18 50.86
2LM-HEADS 65.16 68.70 66.88 67.05 70.88 68.91 49.13 47.67 48.39 51.07 49.59 50.32
TABLE 5.4: Comparison of various training with entity linking as an auxiliary
task,andbeamsearchwithandwithoutconstraintTriedecoding. WEBIEresults
areontheannotatedtestset. AllmodelsuseBARTconfigurationwithrandomly
initialised weights. We show in bold the best result among the training objec-
tives.
parameterissetto0.75. For WEBIE withnegativeexamples,allELtrainingmodels
achieve better F1 performance than BART , with ENTITY-PROMPT particularly
RAND
resultinginbetterrecall. ThisshowsthebenefitofjointtrainingwithELtoimprove
the faithfulness of web domain data. ARTIFICIAL-PROMPT achieves the best preci-
sion in WEBIE but does not show significant differences in performance compared
to BART . We also note that the performance on negative examples does not
RAND
show significant variations among the different training approaches. Nevertheless,
allthreeapproachesprovidebetterinterpretability,i.e.,providingtheinformationof
thementionspansinthetextthatcontributestotheIEprediction.
ENTITY-PROMPT and ARTIFICIAL-PROMPT do not require additional architec-
turaladaptationoverthestandardmodel. ENTITY-PROMPT alsodoesnotintroduce
training overhead, whereas the other two models may require twice the training
time. 2LM-HEADS offers the flexibility of adapting the weighted combination of
the main task and the auxiliary task by adjusting α in the joint loss formula, which
allows more emphasis on the main target. Regarding extending to more tasks, the
ARTIFICIAL-PROMPT approach can be easily extended by adding additional artifi-
cial tokens as task identifiers, whereas 2LM-HEADS approach would need to add
moreLM-headstoincludemoretasks.
5.6 Conclusion and Future Work
We present (m)WEBIE, the first large-scale, entity-linked closed IE dataset on the
web. Asubsetofthedatasetisfurtherannotatedbyhumansandtranslatedintofour92 Chapter5. FaithfulandRobustKnowledgeExtractionontheWeb
other languages, French, Spanish, Portuguese, and Hindi, via crowdsourcing. The
maincontributionliesinthedevelopmentoffaithfulandrobustextractionpipelines,
coupledwitheffectivetrainingmethods,forstructuredknowledgeontheweb.
ThehighapprovalrateofthetriplesobtainedfromtheautomaticIEpipeline,as
confirmed by Human annotation, underscores the efficacy of extracting fact triples
through Entity linking, Distance Supervision, and NLI entailment filtering. How-
ever, given the recent surge in powerful Language Model (LLM) capabilities, it is
intriguing to compare the quality of IE datasets obtained using LLM prompting or
viafew-shotin-contextlearning(Y.Maetal.,2023). Whiletheautomaticpipelineex-
celsinefficiency,LLMsoffergreaterflexibilitysincetheydon’tnecessitaterestriction
toasingleknowledgebaseasrequiredbydistancesupervision. Nevertheless, both
distance supervision and LLM-generated triples are prone to false positives, hence
thepost-processingsuchastheNLIfilteringremainsessentialinbothscenarios.
Webenchmark WEBIE withgenerativemodelsandcomparethemodelstrained
onWEBIEandREBEL(Wikipediadomain). Ourresultsshowthatmodelstrainedon
WEBIE have competitive zero-shot performance when applied to REBEL and Wik-
iNRE, whereas models trained only on REBEL have 0% accuracy on the negative
examplesin WEBIE.Thishighlightstheimportanceofincludingnegativeexamples
for training more robust models and reducing hallucination in generative IE on the
web. Models trained on both REBEL and WEBIE achieve the best performance on
bothdatasets,aswellaszero-shotresultsonWikiNRE,positioningWEBIEasacom-
plementarydatasettoexistingWikipedia-centricdatasets.
Our exploration of approaches integrating Entity Linking as an auxiliary task
revealsthattheadditionofatask-specificLMheadachievestheoverallbestperfor-
manceforREBEL.Notably,the ENTITY-PROMPT approachdemonstratessignificant
improvementon WEBIE,particularlyenhancingrecall.
While our primary benchmarking involves transformer-based encoder-decoder
models on WEBIE, future work could also explore pipeline frameworks and larger
language models for few-shot performance. This chapter underscores the impor-
tance of developing a faithful and robust pipeline for extracting structured knowl-
edge on the web, which can then be incorporated into many other knowledge-
intensiveapplications.5.7. Limitations 93
5.7 Limitations
Weidentifyseverallimitationsinthiswork:
• FalseNegatives: Ourcurrentautomatictripleextractionpipelineisbuiltusing
theDSapproachfollowedbyfilteringusinganNLImodel. However,Wikidata
isnotcomplete(Q.Tanetal.,2022). Whilesometriplesmaynotbecompletely
availableinWEBIE,weexpectmodelstrainedonthisdatasetcanstilldiscover
newtriplesthatdonotexistinWikidata.
• Limited Relations in Annotation: The human annotation is only conducted
onthemostfrequent200relations.
• Limited Languages in mWEBIE: As discussed in subsection 5.2.5 and sec-
tion B.3, the languages in mWEBIE are limited to official languages from ge-
ographical regions where there is a reasonable amount of MTurk workers to
acceptthejob.
Analternativesolutionwouldbetouseprofessionaltranslators,especiallyfor
low-resourcelanguages.
• FixedDataset:
Facts might change in the world (and Wikidata). This can lead to a degraded
real-worldperformanceifasystemreliesexclusivelyonWebIEforevaluation
whenthedatasetisnotupdatedaccordingly.95
Chapter 6
Grounded Answer and Explanation
in Knowledge-Intensive VQA
Intheupcomingtwochapters,weshiftourfocustoabroaderformatofknowledge
beyondstructuredknowledge. Thischapter,inparticular,centresaroundknowledge-
intensive Visual Question Answering. We explore better utilisation of the paramet-
ricknowledgeembeddedinpre-trainedmultimodallanguagemodelsthroughself-
explanation.
Themaincontentisanextendedversionofthepaper“TowardsaUnifiedModel
for Generating Answers and Explanations in Visual Question Answering” (White-
house, Weyde, and Madhyastha, 2023) published in Findings of the Association for
ComputationalLinguistics: EACL2023.
6.1 Background and Introduction
The focus of this chapter is on knowledge-intensive Visual Question Answering
(VQA) tasks. Contemporary models for Visual Question Answering (VQA) and
Visual Commonsense Reasoning are typically trained discriminatively to select the
bestanswersfrommultiple-choicequestionsortoclassifysingle-wordanswersfrom
a predetermined vocabulary (Anderson et al., 2018). However, such settings often
havelimitations,suchasencouragingmodelstofindsuperficialcorrelations(Yeand
Kovashka,2021)orpenalisingmodelperformance,evenwhentheanswersareplau-
sible. For example, synonyms, multi-word expressions, and morphological varia-
tionsareoftennotconsideredcorrectanswers.96 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
Image Objects prompt + Question (+ Answer) Answer and/or Explanation
<#AOKA#> What is this place? market
orange carrots,
orange sign, <#AOKE#> What is this place? The man is selling vegetables.
yellow sign, roadside stand, this is because
white van... <#AOKAE#> What is this place? Multimodal marke ist, st eh li ls in i gs vb ee gca eu tas be l eth s.e man
Encoder-
<#A#> What are Person1 and Decoder
Person2, Person1, Person2 doing? Transformer They are having dinner.
Wineglass3, <#E#> What are Person1 and They are sitting at a table with
dining table, Person2 doing? They are food in front of them.
Wineglass2, chair,
bow, white plate, having dinner together, this is Person1 and Person2 are having
white table, green because dinner, this is because they are
bottle, ... <#AE#> What are Person1 and seated at a dining table with
Person2 doing? food in front of them.
FIGURE 6.1: Illustration of UMAE: we train a multimodal encoder-decoder
model on the mix of VQA tasks for jointly optimising answer and explana-
tion, where we distinguish the training instances and target output with arti-
ficial prompt tokens (e.g., <#AOKA#>). The top and bottom examples are from
A-OKVQAandVCR,respectively.
Moreover,mostcurrentexplanationgenerationmodelsaretrainedindependently
of the QA model, and explanations are usually generated after the QA model has
providedananswer. Consequently,theseexplanationmodelslackaccesstothepro-
cessthatgeneratedtheanswer,limitingthegroundingoftheexplanationtothean-
swertext.
We posit that a unified model that simultaneously performs answer prediction
and explanation generation is a more effective and consistent approach for VQA.
Generative models, such as GPT-3 (Brown et al., 2020a), T5 (Raffel et al., 2020b), or
OFA(P.Wangetal.,2022),havedemonstratedsuccessinrapidlyadaptingtodown-
stream tasks and generating high-quality open-ended text, making them suitable
candidatesforthisunifiedapproach.
Toaddressthis,weproposeamultitasklearningapproachfortransformer-based
multimodalencoder-decodermodels,creatingaUnifiedModelforAnswerandEx-
planationgeneration(UMAE).Inadditiontotheprevailingtrendofseparateanswer
predictionandexplanationgenerationbasedontheanswers,ourapproachaddsthe
capability of jointly generating answers and explanations. Inspired by the success
ofartificialprompttokensinNeuralMachineTranslation(NMT)(M.Johnsonetal.,
2017),weextendanddemonstratetheefficacyoftheartificialprompt-basedmethod
forVQAinamultitasksetup. Specifically,weaugmenttraininginstanceswithartifi-
cialprompttokens,enablingthemodeltodistinguishdifferenttaskswhilelearning
sharedsemanticfeatures.6.1. BackgroundandIntroduction 97
After the model is trained, we propose to determine the best option among the
multiple choices by directly ranking the options based on the model’s perplexity
against each one. We find this approach to be more optimal compared to mapping
thegeneratedanswertotheclosestwordembeddingfromtheoptions(D.Schwenk
et al., 2022) or using sentence embedding (concatenating the question and answer)
withBERTScore(T.Zhangetal.,2020). Ourmethodalsoaddressesthesurfaceform
penalisation mentioned earlier (synonyms, multi-word expressions, morphological
variations,etc.) thatinvolvesexactmatchmetrics(Y.Goyaletal.,2017).
Experimentsonacombinationofthreeknowledge-intensiveVQAdatasets,OK-
VQA(Marinoetal.,2019),A-OKVQA(D.Schwenketal.,2022),andVCR(Zellerset
al.,2019a),showthatUMAEmodelsachieveanewstate-of-the-artansweraccuracy
on A-OKVQA, a new state-of-the-art explanation score on VCR, and competitive
out-of-domainperformanceonVQA-X(Parketal.,2018). UMAEsupportsthegen-
erationoftheanswertoaquestion,theexplanationforagivenquestionandanswer,
andbothtogetherjointly,makingthemodelefficientandflexible. Anillustrationof
the training setup is shown in Figure 6.1. To the best of our knowledge, our pro-
posal is the first to unit grounded answer and explanation generation for VQA. We
specificallyfocusonknowledge-intensiveVQAtasksthataredesignedtorequireac-
cessingaknowledgebaseforanswers. Byjointtrainingforanswerandexplanation
generation,wehypothesisethatthemodelcanmoreeffectivelyleverageparametric
knowledge,potentiallyenhancingperformanceevenwithoutexplicitlinkstoexter-
nalknowledgesources.
Insummary,ourmaincontributionsareasfollows:
• The UMAE framework where answers and explanations can be generated by
asingleunifiedmodel(subsection6.3.1).
• Asimpleandefficienttrainingapproachthatusesmultitasklearningwithar-
tificialpromptsanddemonstratesitsabilitytogeneraliseacrossdomains(sec-
tion6.4).
• A method to map generated answers to Multiple-Choice options via evaluat-
ingtheperplexityofthegeneration(subsection6.3.2).98 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
• Newstate-of-the-artresultsbyUMAEparticularlyforexplanationgeneration,
aswellaspromisingout-of-domainperformance(section6.5).
6.2 Related Work
Weintroducetherelatedworktothischapterinthefollowingthreeaspects.
6.2.1 MultimodalTransformer-basedModels
As detailed in Chapter 2, multimodal transformer-based encoder-decoder models
achieve state-of-the-art performance on various vision-language tasks (Y.-C. Chen
etal.,2020;XiujunLietal.,2020;Choetal.,2021;P.Zhangetal.,2021;Z.Wangetal.,
2022). They showcase the possibility of capturing richer multimodal semantic co-
herencethandiscriminativelytrainedmodelsandarefurthercapableofgenerating
self-explanations. Pre-trained on multitask settings with natural language instruc-
tions,e.g.,“whatdoestheregiondescribe?”,modelslikeOFA(P.Wangetal.,2022)are
claimed to have the capability to transfer to unseen tasks and domains via similar
instructions. However,contrarytotheseclaims,weobservethatpre-trainedOFAis
incapableofgeneratingvalidexplanationsthroughsimplenaturallanguageinstruc-
tions(section6.5).
6.2.2 ArtificialPromptTokens
Artificial prompt tokens, introduced by M. Johnson et al. (2017), have primarily
foundapplicationinNeuralMachineTranslation(NMT).IntheirworkonGoogle’s
multilingual NMT, tokens like 2es are added at the beginning of sentences to indi-
catethetargetlanguagefortranslationisSpanish. Byusingdifferentprompttokens,
theyjointlytrainamultilingualNMTmodelonvariouslanguagepairs,allowingthe
model to learn shared semantics among different instances. This approach proves
beneficial for low-resource languages and is effective for zero-shot performance on
previouslyunseenlanguagepairs(M.Johnsonetal.,2017).
BuildingonthesuccessobservedinNMT,ourworkexploitsasimilarapproach
withartificialpromptsforanswerandexplanationgenerationinVQAwithaunited6.3. Methodology 99
model. Thisenablesthemodeltolearnsharedfeaturesamongtasksanddatasetsin
variousdomains.
6.2.3 ExplanationGenerationforVQA
TherehasbeenagrowinginterestinincorporatingexplanationsintoVisualQuestion
Answeringtasks,includingVisualCommonsenseReasoning. Variousdatasetshave
been developed where explanations are provided, e.g., VCR (Zellers et al., 2019a),
whichprovidesexplanationsascandidateoptionsinmultiple-choicequestions,and
A-OKVQA (D. Schwenk et al., 2022), which includes multiple explanations for the
answertoeachquestion.
ExistingVQAdatasetshavealsobeenextendedorcorrectedwithprovidedtex-
tualexplanations,forexample,VQA-X(Parketal.,2018),CLEVR-X(Salewskietal.,
2022), and e-SNLI-VE (Kayser et al., 2021) are developed from VQAv2 (Y. Goyal et
al.,2017),CLEVR(J.Johnsonetal.,2017),andSNLI-VE(Xieetal.,2019),respectively.
Formodellingexplanationgeneration,mostrecentapproachesuseseparatemod-
elstopredictanswersandgenerateexplanations(Dua,Kancheti,andBalasubrama-
nian,2021). J.WuandMooney(2019)introducetheconceptofFaithfulMultimodal
Explanations (FME) for VQA, wherein textual explanations are linked to relevant
imageregionsattendedtobytheunderlyingVQAsystem. Dua,Kancheti,andBala-
subramanian(2021)takeatwo-stepapproach,trainingseparatemodulesforanswer
generation and explanation generation, with the latter based on previously com-
puted answers. Kayser et al. (2021) develop a model called e-UG, which combines
UNITER (Y.-C. Chen et al., 2020) for processing multimodal input and GPT-2 (Rad-
fordetal.,2019)forgeneration. Incontrast,weproposeusingasingleunitedmodel
formoregroundedanswerandexplanationgeneration.
6.3 Methodology
Weintroducethemethodologyinthefollowingtwoaspects.100 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
6.3.1 MultitaskLearningwithArtificialPromptTokens
Weformulatethreegenerationsettings: Q→A:answerprediction;QA→E:explanation
generationconditionedontheanswer;andQ→AE:jointanswerandexplanationgen-
erationforagivenquestion. Wehypothesisethatbytrainingthemodeltogenerate
both the answer and its explanation simultaneously, the result answer and explana-
tionwillbemoregroundedandconsistent.
Weuseapre-trainedmultimodalencoder-decodertransformerasourbasemodel
(herewebuildontheopenlyreleasedversionofOFAasastrongbaseline),andfine-
tunethemodelonamixofVQAdatasetsfromdifferentdomains.
DifferentfromOFA,foreachimageintheVQAdatasets, wefirstextractobjects
andattributesusingabottom-uptop-downattention-basedmodel,whichiscrucial
for open-domain VQA tasks (Anderson et al., 2018). We then add artificial prompt
tokens at the beginning of the textual input to signal the generation task (answer,
explanation, or both) and the dataset.1 For Q→AE, we concatenate answers and ex-
planations with a separator in between. Finally, we mix all training instances, each
consisting of an image (processed in patches), objects and attributes, and textual
inputwithartificialprompts.
6.3.2 PerplexityasMultipleChoiceMetric
TomapthegeneratedoutputtoMultiple-Choiceoptions,inpreviousworkthepre-
dictionsarelooselymatchedwithoptionsorgoldanswersusingembedding-based
methods, such as GloVe embedding similarity (D. Schwenk et al., 2022). In con-
trast to these approaches, we propose to evaluate each option as a text generation
task,byfeedingthemodeltheinformationthatwasusedtogeneratetheansweras
a prompt and calculating the likelihood of each option being generated. Formally,
givenanoptionY = (y ,y ,...,y )withttokens,wecalculatetheprobabilityofeach
1 2 t
token y beinggeneratedbyfeedingtheimage, objects, andquestion, aswellasthe
i
first i−1 tokens from Y to the model p . The perplexity is then calculated with:
θ
1Artificial prompt tokens are added as special tokens to the tokeniser to avoid bias in the pre-
trainedembeddings. However, wenotethatthesetokensmaybebiasedregardingtheirassociation
withspecifictasksaftertraining,whichisanintendedeffect.6.4. ExperimentalSetup 101
PPL(Y) = exp(cid:8) −1 ∑tlogp (y |y )(cid:9) ,whichreflectstheprobabilityofoptionY be-
t i θ i <i
inggeneratedbythemodel. Finally,theoptionwiththelowestperplexityischosen
astheanswer.
Wealsocomparetheperformanceofourapproach,usingperplexityasthemet-
ric,withGloVeembeddingsimilarityforA-OKVQA(seeTable6.1).
6.4 Experimental Setup
WeprimarilyevaluateourproposedUMAEapproachusingpre-trainedOFA2asthe
basemodelonthreeknowledge-intensiveVQAdatasets: OK-VQA,A-OKVQAand
VCR.Thedatasetsareintroducedbelow.
6.4.1 Datasets
OK-VQA (Marinoetal.,2019)isaknowledge-basedVQAdatasetthatrequiresout-
side knowledge beyond the images to answer the questions. It has train and test
splitsofsize9,009and5,046. Eachquestionisprovidedanswersbyfiveannotators.
To use the VQA (Antol et al., 2015) metric, each annotated answer is then repeated
twicetoformagoldanswersetwith10answers. Sincenoexplanationisprovided,
weonlytrainQ→AtaskonOK-VQA.
A-OKVQA (D. Schwenk et al., 2022) is currently a large-scale knowledge-based
VQA dataset split into 17.1K, 1.1K, and 6.7K for train, validation, and test, respec-
tively. Thequestionscoverfourknowledgetypes: visual,commonsense,knowledge
bases,andphysical. Foreachquestion,itprovidesbothmultiple-choiceanswersand
10 free-form answers (annotated by 10 different people), as well as three explana-
tions. Images in both OK-VQA and A-OKVQA are from MSCOCO (T.-Y. Lin et al.,
2014),andanswersinbothdatasetsareinsinglewordsorshortphrases.
VCR (Zellers et al., 2019a) is a large multiple-choice dataset for Visual Common-
sense Reasoning. The train, validation, and test splits have 191.6k, 21.3k, and 26.5k
instances,respectively. Eachquestionhasfouransweroptionsinsentences,andthe
2https://github.com/OFA-Sys/OFA102 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
correctanswerisfurtherprovidedwithfourexplanationoptions. ImagesinVCRare
frommovieclips(Rohrbachetal.,2017). Boundingboxesofentitiesareprovidedas-
sociatedwithmentionssuchasPerson1inquestions,answersandexplanations. We
follow Zellers et al. (2021) and draw coloured highlights around the referenced en-
tityontheimages,whereentitynamesandthecolouredhighlightsareconsistentin
theentiredataset,expectingthemodeltolearntheassociationbetweenthecoloured
boundingboxandtheentity.
VQA-X (Park et al., 2018) contains a subset from the VQAv2 (Y. Goyal et al., 2017)
datasetandfurtherprovidesthreeexplanationsforeachquestion. Theimage-question
pairs are split into train, validation, and test with 29.5k, 1.5k, and 2k instances, re-
spectively. Weonlyusetheoriginaltestsettoevaluatethezero-shotperformanceof
thetrainedmodels.
6.4.2 TrainingSetupandHyper-parameters
Webeginwiththepre-trainedweightsfromtheoriginalOFA-large,3whichistrained
on vision-only tasks including Image Classification, language-only tasks including
Sentence Classification and text Summarisation, as well as various vision-language
tasks including Image Captioning, Visual Question Answering and Visual Entail-
ment(P.Wangetal.,2022).
We split the original train set into train and validation sets (95/5 split) for all
three datasets. Since the test set is not publicly available for A-OKVQA and VCR,
we use the original validation set for experimental analyses. We prepare training
instances as introduced in subsection 6.3.1. Specifically, we add <#OKA#> for OK-
VQA (only answers are available), <#A#>, <#E#>, <#AE#> for VCR, and <#AOKA#>,
<#AOKE#>,<#AOKAE#>forA-OKVQA.Additionally,forVCR,wedrawcolouredhigh-
lighted boxes around the referenced entity on the images as described. To account
for the imbalance in size among the datasets, we up-sample instances in OK-VQA
andA-OKVQA,andshuffleallinstancestotrainamodeldenotedasUMAE .
ALL
3https://github.com/OFA-Sys/OFA6.4. ExperimentalSetup 103
Forablationstudies,wefine-tuneOFAforseparateanswerprediction(OFA Q->A)
and explanation generation conditioned on answers (OFA QA->E). To better under-
stand the impact of mixing datasets from different domains, we also train models
UMAE A-OKVQA and UMAE VCR, focusing on all three answer and explanation genera-
tiontasks butonly usingdata froma singledataset: eitherwith A-OKVQAor with
VCR.Adamisusedastheoptimiserandcross-entropyisthelossfunction.
We set the learning rate to 10e−5, the warm-up ratio to 0.4, and the patch image
size to 480. We shuffle all the training examples and use batch size 16. Due to the
large size of VCR, we train for 30 epochs on models involving VCR (OFA
Q->A
for
VCR, UMAE and UMAE ), and up to 100 epochs for other models. We report
VCR ALL
theempiricalperformancewithcheckpointsthatperformbestonthevalidationset
(the5%splitfromtheoriginaltrainset). ForA-OKVQA,weadditionallyreportthe
answeraccuracyontheoriginaltestset.
6.4.3 NLGEvaluationMetrics
We use beam search for generating answers and additionally experiment with dif-
ferent decoding methods including top-k sampling, Nucleus sampling (Holtzman
et al., 2020), and Typical sampling (Meister et al., 2023), for generating explana-
tions. We evaluate answer accuracy as well as explanation quality with automatic
NLGmetricsande-ViLscores, followingKayseretal.(2021)forbettercomparison.
Specifically, e-ViL scores consist of S (task, i.e., answer accuracy), S (explanation
T E
score), and overall S (product of S and S ), where S is defined by Kayser et al.
O T E E
(2021)astheharmonicmeanofNGRAMScore(theharmonicmeanofn-gramscores
ROUGE-L (C.-Y. Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr
(Vedantam,LawrenceZitnick,andParikh,2015),andSPICE(Andersonetal.,2016))
and additionally the BERTScore (T. Zhang et al., 2020), a learned similarity metric
overcontextualrepresentationsofsentences.
We elaborate and discuss different aspects of these NLG metrics. Each of the n-
gramscorestargetsdistinctaspectsoftheoutput. ROUGE-Lfocusesonassessingthe104 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
overlapofcontiguoussequencesofwordsbetweenthegeneratedtextandtherefer-
encetextbymeasuringtheprecision,recall,andF1scoreofthelongestcommonsub-
sequencesbetweenthegeneratedandreferencetexts. METEORemphasisesseman-
ticsimilarityandincorporatesstemming,synonymy,andwordorderinformationto
computeascorethatbetterreflectsthesemanticequivalencebetweenthegenerated
and reference texts. CIDEr assesses the consensus between human judgments and
machine-generated texts by calculating a score based on the similarity of n-grams
between the generated text and multiple reference texts provided by human anno-
tators. SPICE focuses on the precision and recall of generated linguistic structures
and evaluates the structural similarity between the generated and reference texts
by analysing syntactic and semantic components. On the other hand, BERTScore
isatrainedmetricthatleveragescontextualembeddingsobtainedfrompre-trained
transformermodelslikeBERTtomeasurethesimilaritybetweensentencesatamore
granularlevel.
Tosummarise,higherROUGE-Lscoressuggesthigherdegreesoflexicaloverlap
between the generated and reference texts; higher METEOR scores indicate greater
semanticsimilaritybetweenthegeneratedandreferencetexts; higherCIDErscores
imply better alignment with human judgments and consensus among multiple ref-
erence texts; higher SPICE scores reflect greater precision and recall of generated
linguistic structures; and higher BERTScores signify increased similarity between
sentencesatacontextuallevel. Althoughthesemetricstargetdifferentaspectsofthe
generatedtext,experimentalresultsrevealaconsistenttrendacrossvariousmetrics:
if a generation scores higher in one metric, it often scores higher in others as well
(refertosubsection6.5.2).
However, it is worth noting that none of these metrics directly measure the hu-
man interpretability of the generated explanations. N-gram metrics overlook se-
mantic meanings, coherence, and relevance of the generated text, while BERTScore
isalsobiasedwiththelengthsofthegenerationgreatlydiffering. Morediscussions
onthelimitationoftheNLGmetricareincludedinsubsection6.5.2andsection6.7.6.5. MainResults 105
OK-VQA A-OKVQA VCR
MODEL directanswer multiplechoice directanswer multiplechoice BERTScore
TEST VAL(ppl) VAL(GloVe) TEST VAL TEST VAL(ppl) VAL
OFA* 40.40 24.54 56.19 47.40 48.09 39.77 33.55 64.55
OFAQ->A 49.93 74.32 65.30 61.71 63.00 53.91 54.89 83.85
UMAEALL 51.77 74.59 65.67 63.26 63.29 56.14 56.66 85.97
PRIOR-BEST 54.41 – 60.30 53.70 48.60 40.70 (77.10)† –
TABLE 6.1: Performance of models for answer generation. Better results are in
bold. OFA*referstothepre-trainedOFA.Prior-bestresultsforthethreedatasets
are from Gui et al. (2022), D. Schwenk et al. (2022), Yanan Wang et al. (2023),
respectively. † is from a discriminative model and thus not comparable (see Ye
andKovashka,2021).
6.5 Main Results
In this section, we show the results for generated answers and explanations, com-
paremodelsandanalyseerrors.
6.5.1 AnswerAccuracy
Table6.1presentsourobservationsforansweraccuracyonQ->Ataskoverthethree
datasets. We also evaluate VCR answers using BERTScore as the answers for VCR
areusuallysentences. WeobservethatUMAE ALLoutperformsOFA Q->Aonalldatasets,
improvesthepriorstate-of-the-artonA-OKVQAby10∼15%,andachievescompet-
itiveresultsonOK-VQA.Formodelsthatarefine-tunedonA-OKVQA,wealsosee
a salient improvement (+9%) with the proposed mapping of options by perplexity
inMultiple-Choice,insteadofGloVeembeddingssimilarity.4
QUESTION OBJECTS IMAGES ACCURACY
✓ ✓ original 50.39
✓ ✗ ✗ 39.16
✓ ✗ random 33.48
✓ ✓ ✗ 33.28
TABLE 6.2: Ablation on the modality dependency for answer accuracy of A-
OKVQA.
4PreliminaryexperimentswithNLGmetrics(BERTScoreandBLEU)forselectingtheoptionsgiven
generationweresub-optimal.106 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
e-ViLSCORES N-GRAMSCORES LEARNTSC.
DATASET MODEL
S O S T S E BLEU4 R-L MET. CIDEr SPICE BERTSCORE
OFA* 4.44 56.19 7.90 0.30 4.45 3.26 4.82 4.62 68.64
A-OKVQA
OFAQ->A+OFAQA->E 35.82 74.32 48.29 22.18 48.51 23.56 86.76 22.46 85.96
UMAEA-OKVQA 37.10 73.97 50.15 27.61 52.23 24.06 104.39 22.88 87.86
UMAEALL 37.91 74.59 50.82 27.35 52.56 24.83 101.09 23.33 88.21
e-UG 19.30 69.80 27.60 4.30 22.50 11.80 32.70 12.60 79.00
VCR UMAEVCR 22.57 56.68 39.82 12.25 28.87 16.67 48.14 27.36 81.77
UMAEALL 22.82 56.66 40.27 13.44 29.53 17.54 47.33 26.45 81.91
e-UG 36.50 80.50 45.40 23.20 45.70 22.10 74.10 20.10 87.00
VQA-X
UMAEALL 31.58 77.65 40.67 14.63 35.12 20.29 50.35 19.13 85.40
TABLE 6.3: Explanation Scores. R-L, MET. stand for ROUGE-L, METEOR, respec-
tively. OFA*isthepre-trainedOFA,showingthetransferabilityofOFAforgen-
eratingexplanationswithnaturallanguageinstructions. Resultswithe-UGare
from Kayser et al. (2021). We show the best results of A-OKVQA and VCR in
bold. Thelastrowinblueshadeshowsout-of-domainperformance.
We conducted several ablation studies to investigate the dependency of object
features and images on the performance of our model UMAE for answer accu-
ALL
racyofA-OKVQA,whereweremovedimages,replacedthemwithrandomimages,
andremovedextractedattributesandfeatures. ResultsinTable6.2showthatthevi-
sualencoderiscrucialforperformanceandthatvisualobjectsalonearenotsufficient
foranswerprediction. Usingarandomimagewouldintroducenoiseandtherefore
performs worse than not including the image at all. We did not test removing the
question because we believe the model needs the questions to be able to provide
answers.
6.5.2 ExplanationEvaluation
Table6.3showse-ViLsoresandindividualNLGevaluationresultsforexplanations
usingautomatic NLGmetrics.5 Followingthe samesetup asin Kayseret al.(2021),
an explanation is evaluated only if the answer predicted by the system is correct.6
We observe that pre-trained OFA with natural language prompts, e.g., “what is the
5Nucleussamplingshowsthebestresultsandisreported. Detailedscoreswithdifferentdecoding
methodsareshowninsectionC.1.
6A limitation of evaluating all explanations is that explanations of wrong answers may get high
scoreswithn-grammetrics,eventhoughtheyarejustifyingwronganswersandshouldbepenalised.6.5. MainResults 107
Person1
Question: What is Person1
Question: Which two words were said by Question: What is Person1 going to doing?
Question: What time of year both the person in black and the person do? Answer: Person1 is turning the
was the picture likely taken? in white here? Answer: Person1 is going to lead a wheel.
Answer: fall Answer: i do business meeting.
Ground Truth Explanation:
Ground Truth Explanations: Ground Truth Explanations: Ground Truth Explanation: Person1 has his hands on the
1) The child is wearing a 1) The people got married. Person1 is at the head of a table of wheel and is exerting force.
long sleeve shirt and pants 2) There is a wedding cake. the smiling men in suits.
but no coat. people in the suit and white dress are the Generated Explanations: Generated Explanations:
2) There are brown leaves bride and groom. Beam Search: Person1 is sitting at Beam Search: He has his
on the sidewalk. 3) The photo was obviously taken at a the head of the table, which is hands on the wheel.
3) The time is fall. wedding with the bride and groom at the where leaders sit. Top-k: Person1 is hunched
center of it. it is traditional that they say
Generated Explanations: "i do" when taking their vows. Top-k: Person1 appears to be in over and has his hands on the
Beam Search: The time is charge of the board room. wheel.
fall. Generated Explanations: Nucleus: Person1 is sitting at the Nucleus: Person1 is leaning
Top-k: The leaves are Beam Search: They are at a wedding. head of the table, which is where over the wheel and has his
dropping. Top-k: The words are ""i"".". leaders sit during business hand on it.
Nucleus: The leaves are fall. Nucleus: The person in black and the meetings. Typical: The man is leaning
Typical: The leaves are person in white are in a wedding. Typical: Person1 is sitting at the forward and his hand is on the
brown and dry. Typical: The people are just after getting head of the table, which is often the wheel.
married. place for leaders.
FIGURE6.2: ExamplesofgeneratedexplanationsfromMIXmodelwithdifferent
decodingstrategies. TwoexamplesontheleftarefromA-OKVQAandtheother
twoontherightarefromVCR.
explanationfortheanswer?” or“thisisbecause”performspoorly,asmostgeneratedex-
planations are words (“yes/no”) or short-phrases.7 We compare UMAE models (on
all and individual datasets) with prior best results from e-UG (see section 6.2), and
standard separated trained baselines (OFA Q->A+OFA QA->E). UMAE
ALL
achieves better
results across all datasets, showing the advantage of mixing tasks and datasets in
different domains. For out-of-domain evaluation on VQA-X, UMAE also shows
ALL
mostly competitive results. Examples of explanation generation are shown in Fig-
ure6.2aswellasinsectionC.2inAppendixC.
Since e-ViL only evaluates an explanation if a model generates the correct an-
swer,thesubsetofexplanationsevaluatedvariesbymodel. Tofairlycompareexpla-
nations on the same subset, we propose only using the subset of samples where all
models provide correct answers for explanation prediction. Table 6.4 shows the re-
sultsonA-OKVQAwithsuchasubsetof770candidates,where UMAE showsan
ALL
evenhigherexplanationscore. ThishighlightsthatUMAE generatesexplanations
ALL
thatoverlapsignificantlybetterwithgoldexplanations.
7BERTScoreinnotrepresentativeofthevalidityofoutputsfromOFA*. Wereferthereadertoan
expositionoftheproblemsassociatedwithNLGmetricsinCaglayan,Madhyastha,andSpecia(2020).108 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
MODEL S E BLEU4 R-L MET. CIDEr SPICE BERTSCORE
OFA Q->A+OFA QA->E 42.4 20.0 44.2 19.3 66.7 19.1 85.1
UMAE A-OKVQA 45.8 23.6 47.9 21.7 78.0 20.5 86.9
UMAE 46.8 24.9 49.5 22.3 84.1 20.8 87.3
ALL
TABLE 6.4: ExplanationscoresonthesamesubsetofA-OKVQA.
6.5.3 JointAnswerandExplanationGeneration
WefurtherpresenttheresultsoftheproposedQ→AEtaskwhereanswersandexpla-
nations are jointly generated. We parse the generated sequence to the answer and
theexplanationandusethesamesetsofmetricsastheseparategenerationforeval-
uation. ResultsforanswersinTable6.5andexplanationsinTable6.6. Foranswers,
since the perplexity metric does not directly compare the generation, we show the
Multiple-ChoiceaccuracyusingtheGlovemetricforA-OKVQAandBERTScorefor
VCRanswersentences.
A-OKVQA VCR VQA-X
TASK
MULTIPLE-CHOICE BERTSCORE DIRECT ANSWER
Q->A 65.67 81.91 77.65
Q->AE 65.67 82.30 69.60
TABLE 6.5: Evaluationofanswersgeneratedgivenquestions(Q->A)andjointly
generatedwithexplanations(Q->AE).Thelastcolumnwithablueshadowindi-
cates out-of-domain performance. For simplicity, we use the Glove metric for
selectinganswersforMultiple-ChoicequestionsinA-OKVQA.
DATASET
S
E
NGRAMSCORE BERTSCORE
QA->E Q->AE QA->E Q->AE QA->E Q->AE
A-OKVQA 50.82 47.01 35.69 32.15 88.21 87.39
VCR 40.27 37.02 26.70 24.02 81.91 80.68
VQA-X 40.67 39.67 26.69 25.85 85.40 85.21
TABLE 6.6: Scores of explanations generated given answers (QA->E) and jointly
generatedwithanswers(Q->AE).Thelastrowwithablueshadowindicatesout-
of-domainperformance.
In summary, our experiments demonstrate that the UMAE model leads to im-
provedanswerandexplanationgenerationandallowsfortheflexibilitytogenerate
differenttypesofoutputs,includinganswers,explanations,orboth. Weobservethat6.6. AnalysisandDiscussion 109
UMAE exhibits promising results in jointly generating both the answer and expla-
nation.
6.6 Analysis and Discussion
In this section, we analyse the errors in answer and explanation generation, show-
casetheissuesexistinginthedatasets,anddiscussthelimitationsintheexplanation
evaluationmetrics.
6.6.1 ErrorAnalysis
To better understand the generated answers and errors, we randomly sample 50
errors in OK-VQA and A-OKVQA. Our analysis reveals the following main error
types,wherethefirstthreearerelatedtomodelperformance:
• Knowledge: theimplicitknowledgelearnedbythemodelisinsufficientforan-
sweringsomeoftheknowledge-intensivequestions,suchasquestionsasking
whenacertainsportwasinvented.
• Visual: the model fails to identify the visual attributes correctly, such as ques-
tionsaboutrecognisingobjectshapeormaterial.
• Semanticdisassociation: themodelmisinterpretsquestionsorfailstomatchthe
intended semantic meaning. For example, it may answer what an object is in-
stead of a more complex question such as what is commonly packed in it (e.g.,
answering“suitcase”insteadof“clothes”).
• Metric: the evaluation metric may penalise some of the plausible answers, es-
peciallywhensearchingforexactmatchanswers(mostlyduetothedifference
ofsingular/pluralorphraseswith/withoutspaceinbetween).
• Dataset: errorsduetoissuesinthedatasetsthemselves.
We discuss prominent issues in dataset quality briefly in Appendix C and further
presentthedistributionoferrortypesinFigure6.3.110 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
15 15
Knowledge
10 10 Visual
Semantic
Metric
5 5
Dataset
0 0
OK-VQA A-OKVQA
FIGURE 6.3: Distributionoferrortypesindirectanswersfrom50errorsamples
inOK-VQAandA-OKVQA.
Person3 Question:
Person2
Who is eating a
snack?
Ground Truth Answer:
Person2 and
Person3 are eating.
Person6
Generation:
Person6 is eating a
snack.
FIGURE 6.4: Example of misassociation between coloured highlights and enti-
ties. The model fails to associate purple box with Person2 and green box with
Person3.
We also observe misassociation between bounding boxes and entities in VCR.
Many questions in VCR provide extra context assisting the association, for exam-
ple, questions like “Why are the eyes of Person2 closed?” helps the model to identify
Person2as“thepersonwithclosedeyes”intheimage,butquestionsthatspecifically
need the correct association without context can become problematic. Figure 6.4
showsanexamplerevealingthatthemodelstruggleswiththeassociation. Thismay
berelatedtothelimitationofpatchimageprocessingusedinOFA.Theassumption
thatthemodelwouldlearntheassociationviathecolouredhighlightsintheimages
isnotalwaysvalid.
tnuoC
rorrE6.6. AnalysisandDiscussion 111
6.6.2 DatasetQuality
Weobservethefollowingissuesintheexistingdatasets: (i)wronganswers,(ii)sub-
jectiveorunanswerablequestions,(iii)typosorunclearexpressions,(iv)notrequir-
ingimagesorknowledgetoanswerthequestionasdesigned.
Unlike OK-VQA and A-OKVQA where the ground truth answers are provided
by multiple people, in VCR instead, answers and explanations for a question are
obtainedfromthesamepersonwhoauthoredthequestion. Thismakestheanswers
or explanations contain a severe amount of subjectivity. We find that a significant
number of the examples expect an understanding of the movie plot from which the
image is extracted, rather than requiring commonsense reasoning. While human an-
notators have an implicit understanding of the movies, the dataset itself does not
containrelevantcontextualinformation. Weprovidespecificexamplesoftheissues
insectionC.3inAppendixC.
OK-VQA A-OKVQA
DIRECT ANSWER MULTIPLE-CHOICE DIRECT ANSWER
BEST 80.94 80.74 66.20
AVERAGE 54.98 71.53 57.29
WORST 16.37 59.35 41.46
TABLE 6.7: Human performance on OK-VQA and A-OKVQA measured from
the ground truth answers. For simplicity, we use the Glove metric for selecting
answersforMultiple-ChoicequestionsinA-OKVQA.
We further measure the best, average and worst human performance on OK-
VQAandA-OKVQAbyselectingthemostcommonanswer,arandomanswer,and
the least common answer, respectively, from the 10 ground truth answers for each
question. We calculate the performance using the VQA metric for direct answers,
and the GloVe metric for multiple choices for simplicity. Note that we also remove
the answer selected from the ground truth answers when measuring human per-
formance. FromtheresultsinTable6.7wecanseethattheaverageperformanceon
bothdatasetsisrelativelypoor,whichindicatesthenoiseinthedatasets. Thequality
of the datasets needs to be more carefully inspected so that the model performance
evaluatedonthesedatasetscanbemoremeaningful.112 Chapter6. GroundedAnswerandExplanationinKnowledge-IntensiveVQA
6.6.3 ExplanationEvaluation
CurrentNLGmetricspredominantlyevaluatethen-gramscoresbetweengeneration
andreference. Althoughhumanevaluationmaybetheultimatecriterion(Kayseret
al., 2021), this does not scale. Humans’ judgments on generated explanations are
contextual, especially the images that are predominantly taken into consideration.
However, none of the current widely used metrics considers visual information.
Multimodal evaluation metrics (Madhyastha, J. Wang, and Specia, 2019; M. Jiang
et al., 2019; Hessel et al., 2021) are potentially better approaches to obtain visually
groundedmeasuresforevaluation.
6.7 Conclusion and Future Work
In summary, our proposed Unified Model for Answer and Explanation generation
(UMAE) leverages a multitask learning approach within a multimodal encoder-
decoder framework, incorporating artificial prompt tokens to distinguish distinct
taskswhilelearningsharedsemantics.
Evaluation of our approach on various VQA tasks shows that UMAE outper-
forms prior best models and separately trained baselines in both answer and ex-
planation scores, where we also demonstrate the benefit of using perplexity as the
metricformappinggeneratedanswerstoMultiple-Choiceoptions.
Additionally, UMAE offers flexibility in output and can generate explanations
fordatasetswithoutexplanationsfortraining, e.g., OK-VQA,whilealsoimproving
answerquality. In-depthcasestudiesanderroranalysesundertakeninthischapter
reveal valuable insights for future enhancements, underscoring the importance of
continuousimprovementindatasetquality.
Thischapterhighlightsthebenefitofgroundedanswerandexplanationgenera-
tiontowardsboth,showcasingbetterutilisationoftheparametricknowledgestored
intheparametersofthemultimodallanguagemodels.
6.8 Limitations
Weaddressthelimitationsofourworkinthefollowingtwoaspects:6.8. Limitations 113
• ModelSpecificity: Firstly,theexperimentsconductedwithourproposedframe-
work and fine-tuning approach are primarily on the OFA model. While we
positthatourapproachappliestoanymultimodalgenerativemodel,broader
insightscouldbegainedbyexperimentingwithamorediversesetofmodels.
• EvaluationMethodology: Secondly,intermsofevaluatingourproposedjoint
framework, assessing the generated explanation quality, especially differen-
tiating between explanations generated jointly with answers and those con-
ditioned on the answers, necessitates human judgment. Relying on human
evaluationbecomesessentialforanuancedassessmentcomparedtotheuseof
automaticNLGmetrics.115
Chapter 7
Knowledge Distillation via
LLM-powered Data Augmentation
In the final chapter, we explore the utilisation of knowledge from the latest trend
of powerful large language models, in complex and challenging multilingual com-
monsensereasoningtasks.
Themaincontentofthischapterisbasedonthepaper“LLM-poweredDataAug-
mentationforEnhancedCross-lingualPerformance”(Whitehouse,Choudhury,and
Aji,2023)publishedin Proceedingsofthe2023ConferenceonEmpiricalMethodsinNat-
uralLanguageProcessing: EMNLP2023.
7.1 Background and Introduction
ThesuccessofNLPmodelsgreatlydependsontheavailabilityandqualityoftrain-
ing data. This poses a significant challenge for multilingual NLP, as data for lan-
guagesotherthanEnglishistypicallylimited(Pontietal., 2019; P.Joshietal., 2020;
Whitehouse, Christopoulou, and Iacobacci, 2022). An approach to address the data
scarcity challenge is through zero-shot cross-lingual transfer or multitask training,
in which a model is trained across data of diverse tasks and languages, exhibiting
the capability to handle unseen tasks, particularly in larger models (Artetxe and H.
Schwenk, 2019; Nooralahzadeh et al., 2020; K.-H. Huang et al., 2021). However,
when aiming for task-specific objectives, a smaller, fine-tuned model dedicated to
that particulartask oftenoutperforms larger general-purpose, zero-shot models. In116 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
Train Validation Test
DATASET
English Non-English English Non-English English Non-English
XCOPA 400 0 100 100 500 500
XWinograd 1858 0 233 0 233 424
XStoryCloze 300 300 60 60 1511 1511
TABLE 7.1: Number of examples available in XCOPA, XWinograd, and XSto-
ryCloze per language. Since a validation split is not specified in XStoryCloze,
wetake60randomexamplesfromthetrainsplitforvalidation.
addition,asmallertask-specificmodelismorepracticalandcost-effectivefortrain-
ing and deployment. Nevertheless, developing a powerful task-specific model be-
comeschallengingintheabsenceoftrainingdata(Lauscheretal.,2020).
Conversely, recent powerful Large Language Models (LLMs) excel at handling
general instructions and have shown promise in data generation tasks (Yizhong
Wangetal.,2023). Inthiswork,weleverageLLMstogeneratesyntheticdataforvar-
iousmultilingualcommonsensereasoningtasks,XCOPA(Pontietal.,2020),XWino-
grad(TikhonovandRyabinin,2021), andXStoryCloze(X.V.Linetal.,2022), where
thetrainingdataislimitedevenforEnglish(seeTable7.1). Toaugmentthetraining
data, we provide LLMs with instructions and examples from the original training
data, prompting them to generate new and diverse examples. We explore the gen-
eration of synthetic data in English using different LLMs, including open-source
modelslikeDolly-v21 andStableVicuna2,aswellasChatGPTandGPT-4. Although
theweightsandcapabilitiesofthelattertwomodelsremainundisclosed,weexplore
themastheyextendthecapabilityofgeneratingtextsinlanguagesbeyondEnglish.
We develop task-specific models by fine-tuning multilingual pre-trained lan-
guage models, namely mBERT (Devlin et al., 2019) and XLM-R (Conneau et al.,
2020a),usingthegenerateddata. Wethencomparetheirperformanceagainstmod-
els trained on a limited set of human-created data in the target language whenever
available,andotherwisethroughzero-shottransferlearningfrommanuallycreated
English training data. Our experiments demonstrate that training the models with
relativelylargesyntheticallygenerateddatasetsyieldsbetterperformancethantrain-
ing with limited manually-created datasets. This finding empirically confirms the
1https://github.com/databrickslabs/dolly
2https://github.com/Stability-AI/StableLM7.2. RelatedWork 117
utilityofsyntheticdatageneratedbyLLMsforimprovingdownstreamtask-specific
models.
WeexpandthemultilingualdatasynthesisusingChatGPTandGPT-4onXCOPA
andfindthatgeneratingmultilingualdatasetsgenerallysurpassestheeffectiveness
ofthezero-shotcross-lingualtransfer. Wefurtherassessthequalityofthegenerated
datasetindifferentlanguagesbyaskingnativespeakerstoevaluatethenaturalness
and logical soundness of the generated dataset compared to the human-written ex-
amples. The annotation results reveal that while ChatGPT and GPT-4 successfully
generatenaturaltextinmostlanguages,theystrugglewithgeneratingunderstand-
abletextincertainlanguagessuchasTamil. Moreover,anoticeablegapisobserved
in terms of commonsense coherence when comparing ChatGPT-generated data to
human-constructed data. On the other hand, GPT-4 significantly narrows this dif-
ference.
Tosummarise,ourworkhasthefollowingkeycontributions:
• Leveraging and prompting four LLMs to augment three low-resource, multi-
lingualcommonsensereasoningdatasets.
• Fine-tuning smaller models, mBERT and XLMR, using the synthesised data
andshowcasingthepracticalvalueoftheLLM-generateddata.
• Performing an extensive analysis of the effects of various target languages in
datagenerationandscaling, aswellasahumanevaluationofthenaturalness
andlogicalcoherenceofthedatageneratedinvariouslanguages.
• Releasingthesynthesiseddatasetsforpublicuseandreproducibility.
7.2 Related Work
Wereviewtherelatedworkofthechapterinthefollowingtwoaspects.
7.2.1 MultilingualandLow-ResourceNLP
Recently,therehasbeenincreasedattentiononexpandingNLPbeyondEnglish,in-
cludingthedevelopmentofmultilingualmodels(Devlinetal.,2019;Conneauetal.,118 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
2020a;Xueetal.,2021;Scaoetal.,2022)aswellasthecreationofbenchmarkstoad-
dress multilingual challenges (Conneau et al., 2018; Artetxe, Ruder, and Yogatama,
2020; Adelani et al., 2021; Winata et al., 2023). Among the prevailing challenges
facedacrossvariouslanguages,acommonthemeisthescarcityofavailabledata.
Consequently, when data is lacking, one approach is to employ zero-shot cross-
lingual transfer. Studies conducted by Winata et al. (2023) have demonstrated the
effectivenessofzero-shotcross-lingualtransferforrelatedlanguages. Additionally,
Muennighoffetal.(2023)showthatmodelsfine-tunedonlywithEnglishinstruction
data are capable of understanding multilingual instructions. In this work, we are
tacklingasimilarscenariowheretheavailabilityofdataislimited.
7.2.2 MultilingualDataAugmentation
Lauscher et al. (2020) show that few-shot can drastically increase the cross-lingual
performance of small models, proving that multilingual data augmentation is an
effectivestrategy. Aseriesofworkstrytopredictthecross-lingualaccuracyofmod-
els through measurements and modelling (Xia et al., 2020), and study strategies for
multilingualdataaugmentation, suchaschoosingthetransferlanguages(Y.-H.Lin
etal.,2019),andpredictingmultilingualfew-shotaccuracyleadingforoptimaldata
augmentationapproaches(Srinivasanetal.,2022).
Many works focus on synthetic data augmentation for code-mixing, including
utilisinglinguistictheories(GrandeeLee, Yue, andHaizhouLi, 2019; Pratapaetal.,
2018), machine translation models (Tarunesh, Syamantak Kumar, and Jyothi, 2021),
parallelcorpusandWikipedia(Winataetal.,2019;Whitehouse,Christopoulou,and
Iacobacci, 2022), and employing ChatGPT (H. Dai et al., 2023). To the best of our
knowledge,thisisthefirstworkthatutilisesLLMsformultilingualdataaugmenta-
tion, comparing data generation in English and then translating and generating in
targetlanguages,withaspecialfocusonthechallengingmultilingualcommonsense
reasoningtasks.7.3. DatasetAugmentation 119
7.3 Dataset Augmentation
This section explains the datasets used in the experiments and the detailed instruc-
tionsetup.
OurexperimentsuseXCOPA,XWinograd, andXStoryCloze, whichareselected
due to (i) the limited availability of training data and (ii) commonsense reasoning
datasets present greater challenges for data synthesis. Table 7.1 summarises the
statisticsofthethreedatasets.
XCOPA is a cross-lingual Choice of Plausible Alternatives dataset that translates
andre-annotatesthevalidationandtestsetsofEnglish(EN)COPA(Roemmele,Be-
jan, and Gordon, 2011) into 11 target languages (ET: Estonian, HT: Haitian Creole,
ID: Indonesian, IT: Italian, QU: Quechua, SW: Swahili, TA: Tamil, TH: Thai, TR:
Turkish, VI: Vietnamese, and ZH: Chinese).3 Each instance consists of a premise, a
question(cause/result), andtwoalternatives. Thetaskistopredictthemoreplausi-
blealternative.
XWinograd is expanded from the original English Winograd Schema Challenge
(WSC)(Levesque,Davis,andMorgenstern,2012)tofiveotherlanguages(FR:French,
JA: Japanese, PT: Portuguese, RU: Russian, and ZH),4 which consists of pronoun
resolutionproblemsaimingtoevaluatethecommonsensereasoningabilityofama-
chine. Given a statement with two noun phrases and a pronoun, the challenge of
WSC is to determine the referent of the pronoun, which can only be inferred from
thecontext.
XStoryCloze is collected by X. V. Lin et al. (2022), where the validation split of the
original English StoryCloze dataset (Mostafazadeh et al., 2016) is translated into 10
othertypologicallydiverselanguages(RU,ZH,ES:Spanish,AR:Arabic,HI:Hindi,
ID, TE: Telugu, SW, EU: Basque, and MY: Burmese). Each example consists of a
four-sentencecommonsensestory,acorrectending,aswellasawrongending.
3https://huggingface.co/datasets/xcopa
4https://huggingface.co/datasets/Muennighoff/xwinograd120 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
7.3.1 LLMsforDataGeneration
Ourpreliminaryexperimentsrevealthatlanguagemodelsthatarespecificallyfine-
tuned on downstream NLP tasks, such as BLOOMZ (Scao et al., 2022) and Flan-
T5 (Chung et al., 2022), struggle to follow the complex instructions. Conversely,
more recent LLMs such as Dolly-v2, StableVicuna, ChatGPT, and GPT-4, which are
designed to handle more intricate and general-purpose instructions, have demon-
strated success in following our instructions for data generation. ChatGPT and
GPT-4alsostandoutwiththecapabilityofgeneratingexamplesinnon-Englishlan-
guages.
We explore synthetic data generation with the four aforementioned LLMs, bal-
ancingbetweenopen-accessmodelsandclosedmodels(seesubsection7.5.1). Specif-
ically, we use dolly-v2-12b, which is derived from EleutherAI’s Pythia-12b (Bi-
derman et al., 2023a) and fine-tuned on approximately 15K instructions generated
byDatabricksemployees;andStableVicuna-13B,anRLHF(reinforcementlearning
from human feedback) fine-tuned Vicuna model on various conversational and in-
structionaldatasets-Vicunaisanopen-sourceLLaMAmodel(Touvronetal.,2023a)
fine-tunedonuser-sharedconversationscollectedfromShareGPT.5
7.3.2 InstructionsandResponses
WeutiliseLLMstogeneratesyntheticexamplesforalldatasetsbypromptingthem.
We construct instructions using the descriptions from the dataset papers as a ref-
erence and provide LLMs with some examples, randomly sampled from the train
(+validation) split of the original dataset, then ask LLMs to generate similar data
points. We experiment with various instructions and evaluate the synthesised data
on a smaller scale, update the instructions based on the errors, and then choose the
bestinstructiontogeneratethefinaldatasets.
The final instructions and responses are in Table 7.2. Our data generation pro-
cesscomprises thefollowing keysteps: (i)We establishthedesired totalnumber of
examples to generate. This quantity can be determined by various factors such as
budgetconstraints,afixedratioconcerningtheoriginaldataset,etc. (ii)Weproceed
5https://github.com/lm-sys/FastChat7.3. DatasetAugmentation 121
XCOPA XWINOGRAD XSTORYCLOZE
We are collecting more We are collecting more We are collecting more exam-
examples for the COPA examples for the Wino- ples for the Story Cloze dataset.
datasetwhichwillbeused grad Schema Challenge. Each example consists of a 4-
to test a system’s ability Each example has a short sentence story, one correct end-
of Commonsense Causal sentencethatcontainstwo ingsentencewhichisaplausible
Judgments. The format of noun phrases and one continuationofthestory,andone
thedata: pronoun replaced by “_”, wrong ending sentence which
A premise: a statement of and the challenge is to is logically inconsistent with the
something that happened, determine the referent of context.
andtwochoicesthatcould the pronoun, which can Herearenexamplesofthedata:
plausibly {occur as the only be inferred from the Example 1: Sent-1: Tina is very
result / be the cause} of context. tiredeverysinglemorning. Sent-
the premise. The correct Herearenexamplesofthe 2: Shedoesnotgetenoughsleep
choice is the alternative data: because of her two jobs. Sent-
thatismoreplausiblethan Example 1: Sentence: 3: Tina decides to quit one of
thewrongchoice. Harley hides from the jobs. Sent-4: She now gets
Here are n examples in Dyna because _ is scary. enough sleep to function every-
{language}: Who/What is scary? day. Correctending: Tinaiswell
Example 1: Premise: Correct answer: Dyna. rested. Wrong ending: Tina is
The man wanted to save Wronganswer: Harley. ... more tired than ever before. ...
money. Whathappenedas Examplen:... Examplen:...
a result? Correct choice: Based on the examples Based on the examples above,
He cut back on mak- above, generate m new providemnewsimilarexamples.
ing frivolous purchases. examples. Both noun Requirements:1)thestoryshould
Wrong choice: He with- phrases in each example read like a coherent story, with
drew money from his can be males, females, a specific beginning and ending,
savingsaccount.... Exam- inanimate objects, or where something happens in be-
plen:... groups of people or ob- tween 2) both ending sentences
Based on the examples jects. There should only shouldbeentirelyreasonable,re-
above, generate m new beone“_”inthesentence. alistic and sensible when read in
examplesin{language}. Thecorrectandwrongan- isolation,and3)bothendingsen-
swershouldbeoneofthe tencesshouldfollowupthestory
noun phrases mentioned bysharingatleastoneofthechar-
inthesentence. actersofthestory.
Premise: The politician Sentence: Sam gave Sent-1: Jordan was a high
madeacontroversialstate- Andrewthebookbecause schoolstudentwhowantedtobe-
ment. Whathappenedasa _ had already read it. come a doctor. Sent-2: He spent
result? Correctchoice: The Who/What had already all his free time studying biol-
politician faced criticism readthebook?Correctan- ogy and chemistry. Sent-3: One
from the media. Wrong swer: Sam. Wrong an- day, his school hosted a science
choice: Thepolitician’sap- swer:Andrew. faircompetition. Sent-4: Jordan’s
provalratingsincreased. Sentence: The dog project won first place. Correct
Premise: 我裤子口袋里 chased the cat , but _ was ending: Jordan went on to study
的钥匙不见了。Whatwas too fast. Who/What was medicine in college. Wrong end-
the cause? Correct choice: too fast? Correct answer: ing: Jordangaveuphisdreamof
这个口袋上有一个洞。 the cat. Wrong answer: becomingadoctor.
Wrong choice: 裤子是新 Thedog.
的。
TABLE 7.2: Examples of instructions and LLM-responses for XCOPA, XWino-
grad,andXStoryCloze. WeuseChatGPTfordemonstration.
togenerateexamplesthroughthefollowingiterativeprocess: (a)Toensurediversity,
werandomlysampleasetof n examplesfromthetrainingdatasets. (b)Weappend122 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
Model XCOPA XWinograd XStoryCloze
DOLLY-V2 41.6% 22.4% 41.2%
STABLEVICUNA 36.1% 33.8% 36.1%
CHATGPT 86.4% 43.8% 77.6%
GPT-4 89.7% 85.0% 89.3%
TABLE 7.3: GenerationSuccessRateinEnglish(validexamplesobtained/total
examplesrequested)withdifferentLLMsonthethreedatasets.
these sampled examples to the instructions and prompt the model to generate an
additionalsetof m newexamples. (c)Afterwards, weperformpost-processingand
onlyaddvalidanduniqueexamplestothegeneratedset. Typically, thevaluesof n
andmaresetto5to10.
Wefocusonafixed-budgetscenarioandfirstgenerateatotalof3-4Kdatapoints
for each dataset with LLMs. LLMs tend to generate fewer samples than requested
or inconsistent output in invalid formats. We report the success rate for different
LLMs on the three datasets in Table 7.3, which indicates that GPT-4 has the most
robustness.
Among the datasets, LLMs have the lowest generation success rate for XWino-
grad, which is more challenging. XWinograd requires both answers to be from the
generated sentence, with only one pronoun being replaced. One failed example in
generatedXWinograd: Sentence: ”Thecomputercrashedand_lostalloftheirfiles”.
Correct answer: the user. Wrong answer: the computer. In addition, we observed
pronoun inconsistency in the generated XWinograd data. Despite the requirement
for interchangeable pronouns in the options, models frequently fail to comply. For
example, “The dog bit the mailman because _ entered the yard.” is generated by
ChatGPT with the options “The dog” or “the mailman”, however, “_” in the sen-
tence cannot be replaced by the same pronoun for the given two options, hence it
maymakethetaskeasierandtheexampleisconsideredsub-optimal.
Despite multiple instructions emphasizing the necessity of maintaining consis-
tency between the two phrases to be replaced, all Language Model Models (LLMs)
experimentedwith,failedtoconsistentlyfollowthisrequirement. Thebest-performing
instruction,asillustratedinTable7.2,stillfellshortofachievingperfectconsistency.7.3. DatasetAugmentation 123
FIGURE 7.1: Comparison between the 30 most frequent events in the original
andtheChatGPT-generatedEnglishStoryClozedataset.
We retain these instances within the dataset and further include a human evalua-
tioninsubsection7.6.1. Specifically,wefoundthatamongtheLLMsstudied,GPT-4
demonstrated the highest level of consistency, followed by Chat-GPT, with 76.6%
and48.9%oftheannotatedexamplesexhibitingadherencetotherule, respectively,
asconfirmedbyannotationsfromnativespeakers.
7.3.3 TopicDiversity
AstheStoryClozedatasetcontainsmoresentencesandhasrichercontent,weanal-
yse the diversity of the generation as well as topic coverage and the most frequent
events, following Specifically, an event is counted as any hyponym of “event” or
“process”inWordNet.
ThishelpsustodeterminewhetherLLM-generateddatacancapturethecorpus
distributionbyrandomlysamplingnexamplesfromthedatasetintheinstructions.
ChatGPT-generateddataisusedfordemonstration.
InFigure7.1,wepresenttheresultsofcomparingthegenerateddatapointswith
the original 300 train set used as few-shot examples in the generation instructions.
Wecanseethat23ofthe30mostfrequenteventsintheoriginaldatasetcanalsobe
foundinthe30mostfrequenteventsoftheChatGPT-generateddata.124 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
7.4 Experimental Setup
We first generate synthetic English examples for XCOPA, XWinograd, and XSto-
ryCloze, with Dolly-v2, StableVicuna, ChatGPT, and GPT-4. The size of the final
filteredsynthesiseddataforthethreedatasetsis3.7K,2K,and1.7K,respectively. We
thenfine-tunemBERT,XLMR-base,andXLMR-largewiththesynthesiseddataand
comparethezero-shotcross-lingualtransferperformanceacrossdifferentlanguages,
whereweusetheoriginalvalidationsetintargetlanguages.
ForXCOPA,weadditionallyexperimentwithgeneratingdatapointsdirectlyin
non-English languages, by providing examples in the target language and specify-
ing the language desired for the generated data (see Table 7.2). However, since no
examplesforcauseareincludedinTHandTRtrain/validationdata(theydoappear
inthetestsplit),wedonotgenerateXCOPAforthetwolanguages. WeuseChatGPT
and GPT-4 for multilingual synthetic data generation, as both Dolly-v2 and Stable-
Vicunaexhibitlimitationsineffectivelygeneratingmultilingualtext. Thesizeofthe
multilingualsynthesiseddatais∼3.6Kineachlanguage.
We fine-tune models on all datasets as multiple-choice tasks6 by searching the
best learning rate from {5e−6, 10e−6}, and batch size from {8, 16, 32}. All the fine-
tuning experiments are conducted on a single 40G A100. For generating data with
Dolly-v2 and StableVicuna, we use 2×40G A100. Additionally, for XLMR-large
trainedontheoriginalXWinograddataset,wefoundthatalowerlearningratewas
necessary. Therefore, we performed an additional tuning process with a learning
rateof10e−7.
7.5 Results and Discussion
We now present the main results of fine-tuned models on the three datasets and
compareperformancewithgenerateddataindifferentLLMs,languages,andscales.
6Inourpreliminaryexperiments,wefindthatformulatingXWinogradasabinarytextclassification
resultspoorly,inlinewiththeobservationfromHaokunLiuetal.(2020)thatthetaskformulationis
essentialtotheperformanceofWinograd.7.5. ResultsandDiscussion 125
Finetuned LLMfor XCOPA XWINOGRAD XSTORYCLOZE
Model Generation
ORI GEN O+G ORI GEN O+G ORI GEN O+G
400 3.7k 4.1k 1.8k 2k 3.8k 300 1.7k 2k
Dolly-v2 47.9 53.3↑5.4 54.0↑6.1 52.9 59.6↑6.7 59.3↑6.4 65.0 68.7↑3.7 68.1↑3.1
mBERT
StableVicuna 47.9 52.9↑5.0 54.7↑6.8 52.9 53.7↑0.8 58.5↑5.6 65.0 64.6↓0.4 67.3↑2.3
ChatGPT 47.9 55.0↑7.1 54.1↑6.2 52.9 56.0↑3.1 58.3↑5.4 65.0 64.3↓0.7 68.3↑3.3
GPT-4 47.9 56.4↑8.5 57.2↑9.3 52.9 54.9↑2.0 57.5↑4.6 65.0 68.0↑3.0 69.8↑4.8
Dolly-v2 54.8 58.1↑3.3 58.1↑3.3 53.5 56.5↑3.0 66.3↑12.8 73.0 75.8↑2.8 76.5↑3.5
XLMR- StableVicuna 54.8 57.6↑2.8 59.3↑4.5 53.5 59.0↑5.5 66.0↑12.5 73.0 69.6↓3.4 74.2↑1.2
Base ChatGPT 54.8 58.2↑3.4 59.4↑4.6 53.5 62.7↑9.2 65.9↑12.4 73.0 67.4↓5.6 74.5↑1.5
GPT-4 54.8 62.7↑7.9 63.0↑8.2 53.5 63.3↑9.8 66.9↑13.4 73.0 74.6↑1.6 79.3↑6.3
Dolly-v2 63.0 58.6↓4.4 65.0↑2.0 80.1 76.9↓3.2 83.1↑3.0 85.0 84.8↓0.2 86.4↑1.4
XLMR- StableVicuna 63.0 64.4↑1.4 68.7↑5.7 80.1 68.2↓11.9 82.0↑1.9 85.0 74.6↓10.4 84.8↓0.2
Large ChatGPT 63.0 64.6↑1.6 68.1↑5.1 80.1 73.2↓6.9 83.2↑3.1 85.0 77.3↓7.7 85.8↑0.8
GPT-4 63.0 72.1↑9.1 72.2↑9.2 80.1 76.4↓3.7 83.5↑3.4 85.0 86.0↑1.0 88.4↑3.4
TABLE 7.4: Comparison of Average Accuracy across all languages for mBERT,
XLMR-Base,andXLMR-LargeonXCOPA,XStoryCloze,andXWinograd. Train-
ing datasets include ORI (original EN data), GEN (LLM-generated EN data),
and O+G (both), with the number of examples used for training indicated by
thesubscripts. Thebestresultsobtainedwiththesameamountoftrainingdata
arehighlightedinbold. Greenandredsubscriptsdenoteimprovementandde-
cline in performance compared to the baseline (ORI). See per language results
inAppendixD.
7.5.1 GeneralResult
Table7.4presentstheaverageaccuracyoffine-tunedmBERT,XLMR-Base,andXLMR-
Large models across all languages on the three datasets. The models are trained
using original data (ORI), different LLM-generated data (GEN), as well as a combi-
nationofbothsources(O+G)inEnglish.
Across different datasets, LLMs, and fine-tuned models, consistent improve-
ments are observed when using both original and LLM-generated data. Among
themodels, Dolly-v2performsthebestonXWinogradwhenfine-tunedonmBERT,
while GPT-4 achieves the highest accuracy in other settings. The most significant
improvement is shown in XWinograd with XLMR-Base, where the addition of an
extra2kdatapointsleadstoanaverageaccuracyenhancementof12.8comparedto
thebaseline,acrossallfourLLMs.
We observe the following from the results: (i) In instances where the baseline
performanceisbelow60,exemplifiedbyXCOPAandXWinogradtasksusingsmall
models like mBERT and XLMR-Base, leveraging LLM-generated data either solely
or in conjunction with original data consistently enhances performance across the126 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
board; (ii) As the baseline performance increases, notably exceeding 80 accuracy as
seen in the case of XStoryCloze, relying solely on LLM-generated data might ad-
versely impact performance. However, even in such cases, utilising data generated
solelybyGPT-4stillyieldsimprovementsacrossallfine-tunedmodels;(iii)Combin-
ingoriginaldatawithLLM-generateddataconsistentlydemonstratesenhancements
over using original data alone, irrespective of the baseline performance or the spe-
cificdatasetsandfine-tunedmodelsemployed. Thistrendholdstrueacrossvarious
scenarios, withthe exceptionof XStoryClozewith XLMR-Large, wherethere’s only
amarginal0.2scoredifference.
Overall,wecanseethatLLM-baseddataaugmentationparticularlybenefitssce-
narios where baseline performance is moderate to low, while stronger LLMs show
morepromisesingeneratinghigh-qualitydata,asexemplifiedbyGPT-4.
For smaller models with When using only LLM-generated data, smaller mod-
els like mBERT and XLMR-Base generally outperform the baseline. However, with
XLMR-Large, which achieves stronger baselines, e.g., >80 in XWinograd and XSto-
ryCloze,theaccuracyremainssimilarorevenworsecomparedtousingtheoriginal
data. GPT-4-generated data demonstrates the best robustness but still experiences
a decline in performance in XWinograd when the generated data size is similar to
theoriginaldata. Thishighlightsthechallengesofgeneratingdataatahuman-level
quality.
7.5.2 MultilingualDataGeneration
Weinvestigatewhetherthesyntheticallygeneratedmultilingualdatasetoutperforms
training solely in English. We choose the XCOPA dataset and explore two settings:
synthetic multilingual data byasking LLMs to generate responses in the target lan-
guagesdirectlyandtranslatingtheEnglish-generateddatatotargetlanguageswith
Google Translate API. We exclude Dolly-v2 and StableVicuna due to their limited
effectiveness in generating non-English text. Although GPT-4 exhibits the most
promisingperformance,itissignificantlycostliercomparedtoChatGPT.Therefore,
we also consider ChatGPT as a contrasting experiment under resource-constrained
conditions.7.5. ResultsandDiscussion 127
Finetuned LLM Trainingdata AVG EN ET HT ID IT SW TA VI ZH
Baseline ORI 47.2 53.8 44.2 48.6 47.2 46.2 45.4 48.4 43.6 47.4
GENEN+ORI 54.6 59.6 56.4 53.6 53.8 51.4 51.6 50.4 55.0 59.2
ChatGPT GENXX+ORI 56.8 59.6 58.8 54.6 56.2 61.2 54.6 53.6 52.0 60.2
GENTrans+ORI 58.7 59.6 59.8 58.2 62.8 61.0 52.6 56.8 58.2 59.4
mBERT EN
GENEN+ORI 59.3 72.6 58.8 53.0 62.0 61.0 50.0 54.0 57.6 64.6
GPT-4 GENXX+ORI 61.8 72.6 61.2 58.2 62.2 66.4 57.4 53.4 63.0 61.8
GENTrans+ORI 62.6 72.6 58.6 55.2 65.6 65.4 53.8 62.6 64.6 65.4
EN
Baseline ORI 55.6 57.6 54.6 50.6 59.6 54.8 55.0 53.4 54.8 59.6
GENEN+ORI 59.8 63.8 61.6 51.6 62.6 59.8 51.6 60.4 64.8 62.0
ChatGPT GENXX+ORI 59.9 63.8 60.6 55.0 64.6 59.6 54.6 56.4 59.6 64.8
GENTrans+ORI 61.1 63.8 60.0 58.0 65.0 60.8 53.8 60.2 62.6 66.0
XLMR-Base EN
GENEN+ORI 63.6 69.6 63.8 51.2 67.2 62.4 58.4 63.8 66.8 69.4
GPT-4 GENXX+ORI 64.0 69.6 62.2 56.2 68.6 63.8 57.8 61.2 66.8 70.0
GENTrans+ORI 63.9 69.6 61.6 56.6 68.4 65.2 58.2 60.2 66.0 69.6
EN
Baseline ORI 64.4 71.4 62.8 51.4 69.0 65.8 60.6 62.0 69.4 66.8
GENEN+ORI 69.5 76.4 69.8 48.2 76.0 72.8 63.4 67.8 73.4 77.8
ChatGPT GENXX+ORI 65.2 76.4 62.4 55.2 75.0 62.2 58.2 55.4 66.2 76.2
GENTrans+ORI 67.0 76.4 60.0 59.6 66.2 66.6 59.0 64.8 74.8 75.6
XLMR-Large EN
GENEN+ORI 73.7 84.6 70.4 50.0 80.8 80.2 65.8 72.8 78.4 80.4
GPT-4 GENXX+ORI 74.6 84.6 77.0 56.0 82.2 77.0 65.0 73.8 76.2 80.0
GENTrans+ORI 74.1 84.6 74.2 57.2 82.0 77.4 62.2 75.0 74.4 79.6
EN
TABLE 7.5: Accuracy on XCOPA. ORI corresponds to the original data, GEN
EN
and GEN representsdatageneratedinEnglishandtargetlanguages. Transde-
XX
notes translations of the English-generated data. We show languages that are
available in all settings. Improvement and decline in performance are repre-
sentedwithgreenandredshadows.
Table 7.5 shows the results for the languages that are available for all settings,
excludingTRandTH(unavailableforLLM-generation,refertosection7.4),andQU
(not supported by the Google Translate API). We can see the impact of the gener-
ateddatavariesacrossdifferentfine-tunedmodelsandlanguages,aligningwiththe
findingsofShanuKumar,Dandapat,andChoudhury(2022). TrainingonGPT-4syn-
thesised data displays consistent improvement across all scenarios and languages,
exceptthezero-shotcross-lingualresultonHTwithXLMR-Large.
MorefluctuatingresultscanbeobservedwithChatGPT-generateddata. Acom-
parisonbetweenGEN +ORI andGEN +ORI indicatesthatutilisingdatagener-
EN XX
atedin targetlanguagesgenerallyleads toimprovedperformancewithGPT-4 gen-
erated data, as well as in base models with ChatGPT-generated data. However, for
XLMR-Large,employingChatGPT-generateddataintargetlanguagesmostlyyields
negative outcomes. In languages such as TA and VI, training on generated data in128 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
GEN +ORI GENTrans+ORI
Model EN EN EN EN
3.7K 28.6K 3.7K 28.6K
mBERT 54.3 56.0 58.0 60.1
XLMR-Base 60.1 61.8 61.2 61.7
XLMR-Large 69.7 72.4 67.2 71.4
TABLE 7.6: Accuracy on XCOPA when scaling up the generated data to over
28K with ChatGPT. We report average results on all XCOPA languages excl.
QU,sinceitisnotavailablewiththeGoogleTranslateAPI.
the target languages results in more performance degradation compared to zero-
shot cross-lingual transfer. This suggests that ChatGPT performs worse in those
languagesthanXLMR-Large(Ahujaetal.,2023).
TranslatingtheEnglishdatasetgenerallyshowsoverallbetterresultsthantrain-
ing on the data generated directly in the target languages, except for XLMR-Large
with GPT-4. For SW, both XLMR-Base and XLMR-Large models fined-tuned with
ChatGPT-generated data exhibit performance decline in most cases, even when the
English-generated data benefits all other languages. This observation suggests that
XLMR struggles with SW. In subsection 7.6.1, we select TA, SW, and the two best
languages,IDandZH,alongwithEN,forhumanevaluation.
Furthermore, we explore the effects of incorporating Target Languages in Vali-
dation (TLV). This approach involves training on English examples but evaluating
andtestingonthetargetlanguage. ThedetailedresultsarepresentedinTableD.4in
SectionD.Notably,forsmallermodelstrainedonlimiteddata(e.g.,4.1Kexamples),
integratingtargetlanguagesduringvalidationledtosignificantperformanceboosts
of 3.0 and 0.9 for mBERT and XLMR-Base, respectively. However, when consider-
inglargermodelslikeXLMR-largeandsmallermodelstrainedwithmoreextensive
datasets (e.g., 29K examples), the impact of including target languages during val-
idation was less pronounced. In these cases, we observe only minor variations in
performance. These findings align with those of Ponti et al. (2020), suggesting that
the effectiveness of TLV may vary depending on factors such as model size and
trainingdataavailability.7.5. ResultsandDiscussion 129
Model Ratio XCOPA XWingrad XStoryCloze
1× 64.0 50.2 74.6
2× 64.8 51.9 76.8
mBERT
5× 68.0 57.1 80.6
10× 69.8 65.7 80.3
1× 58.0 45.9 70.7
2× 59.0 53.7 79.7
XLMR-Base
5× 63.0 67.8 81.9
10× 65.8 71.2 84.1
1× 56.0 78.1 81.1
2× 61.2 79.8 90.9
XLMR-Large
5× 81.4 82.0 89.9
10× 85.2 82.8 91.9
TABLE 7.7: PerformanceonEnglishtestexamplestrainingonGPT-4-generated
Englishdataandtheoriginaldata. Originaldatapointsselectedfromthethree
datasetsaresetto200. 1×correspondstousingonlytheoriginaldata,2×means
using200originaldataand200generateddata.
7.5.3 DatasetScalingUp
We now investigate the impact of training on a larger scale of generated data on
model performance. We focus on the XCOPA dataset and expand the generated
data with ChatGPT (more budget-efficient) to 28.6k examples in English. We also
compare the results of zero-shot cross-lingual transfer with translating the English-
generateddatatotargetlanguages.
TheresultsinTable7.6demonstratethepositiveimpactofscalingupthegener-
ateddataonmodelperformance. Particularly,XLMR-Largeexhibitsthemostsignif-
icantimprovement.
7.5.4 FixedRatioDataAugmentation
Weexperimentwithgeneratingdatawithafixedratiooftheoriginaldatasets. Specif-
ically, we compare training with the original English data (200 randomly selected
examples) and augment it with different quantities of English examples generated
byGPT-4,whereweincludeoriginaltraininginstancesinallcases.
TheresultsinTable7.7showcasetheperformanceonEnglishtestexampleswhen
fine-tuningmBERTandXLMRmodelswithtrainingdatasizesthatare1×,2×,5×,
and 10× the size of the original dataset. We can see that performance consistently130 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
improves as we increase the amount of generated data except XStoryCloze, which
has the highest baselines, echoing the previous findings. The relative performance
gainisgenerallymorepronouncedwhenincreasingthedatafrom2× to5× forthe
othertwodatasets.
7.6 Human Evaluation
Tobetterevaluatethequalityofthegenerateddatasetsandcomparethemwiththe
human-created data, we ask native speakers to annotate the multilingual data gen-
eratedbyChatGPTandGPT-4.
For each dataset, we first select 50 generated examples in English, and then re-
quest two annotators to evaluate the examples in two categories: (i) Text Natural-
ness. Theannotatorsareaskedtochooseoneofthefollowingoptionsforeachexam-
ple: “the text sounds natural”, “the text sounds awkward but understandable”, or
“thetextisnotunderstandable”,and(ii)LogicSoundness. Thiscategoryfocuseson
the commonsense aspect of the examples. The annotators are required to select the
most appropriate description from: “the correct option is (clearly) more plausible”,
“bothoptionsareequallyplausible”,“bothoptionsareimplausible”,or“thewrong
optionismoreplausible”. Weonlyasktheannotatorstoevaluatethelogicifthetext
isatleastunderstandable.
For XWinograd, we introduce an additional evaluation criterion. Annotators
are asked to determine whether the two noun phrases in the examples can be re-
placed by the same pronoun (refer to subsection 7.3.2). For XCOPA, we extend
theannotationstonon-Englishlanguages,wherewechoosethetwolanguagesthat
demonstratethemostnotableimprovement, namelyZHandID,aswellasthetwo
languages that exhibit the least improvement or regression in performance with
ChatGPT-generateddata,namelyTAandSW(seeTable7.5). Inadditiontotheorig-
inal examples and the generated examples in the target languages, we include 50
examples that are translated from the same English-generated examples (that were
selectedforannotation).
To ensure impartiality, all the examples are shuffled, and the annotators are not
provided with information regarding the source of the examples (human-created,7.6. HumanEvaluation 131
Original ChatGPT-GenXX ChatGPT-GenT Er Nans Both options equally plausible
50
Both options implausible
40 Wrong option more plausible
30
20
10
0
EN ID ZH SW TA EN ID ZH SW TA EN ID ZH SW TA
Text Naturalness Logic Soundness Logic Issues (ChatGPT)
Original GPT-4-GenXX GPT-4-GenT Er Nans Both options equally plausible
50
Both options implausible
40 Wrong option more plausible
30
20
10
0
EN ID ZH SW TA EN ID ZH SW TA EN ID ZH SW TA
Text Naturalness Logic Soundness Logic Issues (GPT-4)
FIGURE 7.2: Human evaluation of 50 random examples from the original
XCOPA,ChatGPT(top)andGPT-4(bottom)generateddataintargetlanguages,
and translation of English generated data. Examples are annotated by two na-
tive speakers in each language. The subplots in the last column show the logic
issues of the XCOPA data, where the three bars for each language represent
Original,Gen ,andGenTrans (fromlefttoright).
XX EN
LLM-generated,ortranslated).
7.6.1 TextNaturalness
Figure7.2presentstheannotationresultsforXCOPA,averagedfromtwoannotators
foreachlanguage. ForTextNaturalness,wecanseethatinEN,ID,ZH,andSW,both
ChatGPT and GPT-4 achieved higher naturalness than the original dataset. This is
particularly prominent in ID, revealing the fluency issue in the original ID data in
XCOPA,whichisalsoconfirmedbyanativespeaker.
IssueswithTamil
In contrast, the performance of the TA dataset is surprisingly low, with a majority
of examples classified as "not understandable." Upon consulting language experts,
wehaveidentifiedseveralmainissuesinTamil,including(i)theinsertionofredun-
dant words with the same meaning, such as “I will retry to try it again” (ii) verb
agreementerrors,and(iii)thepresenceofuncommonandout-of-contextwords.
ItisworthnotingthatgeneratingTamilusingGPT-4isbothslowandcostly. We
suspect that the tokeniser for Tamil, as well as similar languages like Telugu and
selpmaxE
fo
rebmuN
selpmaxE
fo
rebmuN132 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
Kannada, are poorly trained, resulting in unusable generation in those languages.
While the low quality of the generated data could explain the significant decline
intheperformanceoftheXLMR-LargemodelwhentrainedonChatGPT-generated
datainTamil, intriguingly, modelstrainedonTamildatageneratedbyGPT-4show
improvementoverthebaselines.
To further investigate this issue, we conduct an experiment where we fine-tune
themodelsusingonlyfiveexamplesfromtheTAexamplesgeneratedbyGPT-4that
areidentifiedasnaturalandsoundbytheannotators. TheimprovementonmBERT
under this setting is 50% of the total improvement seen with the entire 3.6K TA ex-
amples. ForXLMR-baseandXLMR-large,15%and3%ofthetotalimprovementcan
beobserved,respectively. Consideringthattheestimatednumberofcorrectsamples
in the 3.6k dataset is around 360, it is plausible that training solely on those exam-
ples could raise the accuracy level, or even surpass, what we observe for the entire
dataset.7 An intriguing question that remains to be investigated in future research
iswhytheremaining3.2kincorrectorunnaturalexamplesdonotnegativelyimpact
themodel’sperformance.
The translated text is typically less natural than the original and generated data
(apart from ID due to issues in the original data). This result affirms that LLMs
generallyexcelingeneratingfluenttextforthelanguagesitsupports.
7.6.2 LogicSoundness
In terms of logic soundness, ChatGPT falls short compared to the original dataset.
We further illustrate the categorised issues in the last column of the plots in Fig-
ure 7.2. We can see that for ChatGPT, the majority of the examples are labelled
as “both options are equally plausible”, only SW has more problematic examples
with “the wrong option is more plausible”. We suspect that this issue arises from
the instruction provided (taken from the description of the original COPA dataset),
which states that “both options could be plausible, but one is more plausible.” In
some cases, ChatGPT generates two choices that are excessively similar in terms of
plausibility. On the other hand, GPT-4 tends to generate options with more clear-
cut differences in plausibility, mirroring the original data. We note that despite the
7Wecouldnotconductthisexperimentastheentiredatasetwasnotmanuallylabelled.7.7. ConclusionandFutureWork 133
description and instruction that both alternatives could happen, both the original
dataset and the data synthesised by GPT-4 tend to present one plausible and one
implausibleoption.
For English XWinograd and XStoryCloze, the majority of the examples in both
original and generated examples are evaluated as natural and logically sound. For
XWinograd, although more than 47 examples are evaluated to exhibit high text
quality and follow commonsense logic, as mentioned in subsection 7.3.2, only 23
ChatGPT-generated examples fulfil the requirement that both noun phrases should
beinterchangeablewiththesamepronoun. GPT-4examplesdemonstratebettercon-
sistency, with 36 following this rule, whereas all original examples are found satis-
factory.
7.7 Conclusion and Future Work
This chapter explores the effectiveness of utilising LLMs for data augmentation in
cross-lingualdatasetswithlimitedtrainingdata. Wespecificallyfocusoncommon-
sense reasoning tasks that are challenging for data synthesis. Our experiments in-
cludingfourLLMsfordatagenerationonthreedatasets,showcaseenhancedcross-
lingualzero-shottransferonsmallerfine-tunedtask-specificlanguagemodels. How-
ever,theimpactvariesacrossdifferentdatasetsandlanguages. Notably,largermod-
els such as XLMR-Large, which have higher baselines, demonstrate more difficulty
inachievingperformanceimprovementswithLLM-generateddata. Amongthefour
LLMs,GPT-4-generateddataexhibitsmostlyconsistentsuperiorperformance.
Expanding data generation directly in target languages also shows general im-
provements compared to cross-lingual zero-shot with the English-generated data.
HumanevaluationofthesynthesisedmultilingualdatasetshowsthattheChatGPT
and GPT-4 generated data demonstrate high naturalness in most languages, even
surpassingtheoriginaldata. However,incertainlanguageslikeTA,bothmodelsfail
to generate natural text. Additionally, when assessing the logical soundness of the
dataset,examplessynthesisedbyChatGPTrevealnotableinconsistenciesregarding
more plausible options compared to the original human-created data. In contrast,
GPT-4exhibitsaperformanceonparwithhuman-writtendata.134 Chapter7. KnowledgeDistillationviaLLM-poweredDataAugmentation
In conclusion, leveraging LLMs for data augmentation shows promise. How-
ever,thechoiceofLLMusedfordatagenerationsignificantlyinfluencesthequality
oftheresultingdata,aswellasitsapplicabilitytothelanguageunderconsideration.
In circumstances where a more advanced model such as GPT-4 cannot be accessed,
othermodelscanbeutilised,thoughthismightresultinperformancedifficultiesin
certain non-English languages, a challenge that also exists for GPT-4, and concerns
regarding logical coherence. A compelling direction for future research could in-
volveexploringtheefficacyofmorerecentinstruction-tunedoralignedopen-source
LLMs,suchasLLaMA2(Touvronetal.,2023b)orTÜLU(ZeqiuWuetal.,2023),in
enhancingdataaugmentation.
7.8 Limitations
Theidentifiedlimitationsinthischapterareasfollows:
• Language Resource Challenges: While LLMs, particularly GPT-4, showcase
promising results in the realm of multilingual commonsense data augmenta-
tion,challengesmayarisewhenappliedtoextremelylow-resourcelanguages.
The effectiveness of these models could be constrained by the availability of
language-specificresources.
• Dependency on Few-Shot Examples: To achieve optimal performance, the
incorporation of few-shot examples in the target language remains necessary
for generating new examples. However, obtaining such examples may pose
challenges,particularlyforlanguageswithlimitedavailableresources.
• Closed Model Accessibility: The usage of closed models like GPT-4 is re-
stricted by licensing limitations, and the reproducibility of results obtained
from these models may be compromised. Despite these constraints, the con-
ductedexperimentsinthischapterillustratethepotentialadvantagesoflever-
agingLLMsformultilingualdatasetaugmentation.
• Black-boxLLMs: Inadditiontothelimitationoftheclosedmodels,currently,
onlyahandfulofmodelsaretrulyopenwithtrainingcode,modelcheckpoints,7.8. Limitations 135
and training data (i.e., BLOOM (Scao et al., 2022), the Pythia suite (Biderman
et al., 2023b), and OLMO (Groeneveld et al., 2024)). All LLMs studied in this
chapterarenottrulyopenorwithdetailedpre-trainingdatareleased. Thislack
of transparency raises concerns regarding the potential biases and shortcom-
ingsembeddedwithinthese"black-box"LLMs. Withoutfulldisclosureoftheir
trainingdatasourcesandmethodologies,thereexiststhepossibilitythatsuch
models were pre-trained on datasets that overlap with the test sets of public
benchmarks,includingthoseusedinourexperiments. Thisscenariocouldin-
troduceaconfoundingfactor,renderingperformanceevaluationsofLLMson
downstreamtasksanddataaugmentationunreliableandpotentiallyskewed.137
Chapter 8
Conclusion
This thesis presents five focused studies that explore knowledge-grounded natural
language understanding and generation, covering knowledge-enhanced fake news
detection, multilingual knowledge-enhanced cross-lingual transfer, faithful and ro-
bustknowledgeextractionfromtheweb,groundedanswerandexplanationgenera-
tionforknowledge-intensiveVQA,andtheemploymentofLLMsfordataaugmen-
tationintasksrequiringmultilingualcommonsenseknowledge.
Throughout this thesis, our focus centres on studying the utilisation and ex-
traction of knowledge, whether in the form of structured knowledge, multilingual
knowledge, parametric knowledge, or knowledge represented as augmented data
distilledfrompowerfulLLMs.
AddressingtheresearchquestionsoutlinedinChapter1, weherebypresentthe
conclusionsderivedfromthisthesis.
ApplicationofStructuredKnowledge
This thesis investigates the broader applicability of structured knowledge beyond
entity-centric tasks. Through our experiments involving four distinct techniques to
incorporate knowledge related to entities across two diverse fake news detection
datasets,wefindthatleveragingstructuredknowledgeimprovesapplicationssuch
asfakenewsdetection,iftheappliedknowledgeisrelevantandcurrent.
MultilingualEntityKnowledge
This thesis broadens the scope of structured knowledge to a multilingual setup.
Specifically, wefindthatutilisingmultilingualentityknowledgevia(i)thecreation138 Chapter8. Conclusion
ofanentity-centriccode-switchedcorpususingdatafromWikipediaandWikidata,
(ii) the intermediate training of a pre-trained multilingual language model, incor-
poratingmaskedlanguagemodellingandentitypredictionobjectives,and(iii)fine-
tuning the intermediate-trained model on downstream tasks, can consistently and
significantlyenhancethecross-lingualtransferability.
KnowledgeExtractionfromWebText
Thisthesisexploresapproachesthatfacilitatetheeffectiveandaccurateextractionof
information of knowledge from the vast and often noisy web text. Upon collecting
a novel mWEBIE dataset and benchmarking against mWEBIE, we conclude that (i)
the inclusion of negative examples within the dataset, and (ii) the integration of
entity-centricauxiliarytasks,arebeneficialtothesuccessfulextractionofstructured
knowledgefromtheinherentlynoisywebtext.
GroundedAnswerandExplanationLearning
This thesis highlights the benefit of multi-task grounded answer and explanation
generation for knowledge-intensive VQA. Notably, we find that even without ex-
plicitlyintroducingexternalknowledge,whichiscommonlydeemedaprerequisite
inthedatasetsexperimented,theadoptionofthismulti-tasklearningapproachcon-
tributestoanotableenhancementinbothansweraccuracyandexplanationquality,
showcasing the advantage of better utilisation of the parametric knowledge stored
withinthemodel’sparameters.
LLM-PoweredDataAugmentation
This thesis leverages the inherent knowledge of recent powerful LLMs to enhance
theperformanceofsmaller,task-specificmodels. Wefindthatdataaugmentationby
promptingLLMssignificantlyimprovescomplexandscarcemultilingualcommon-
sensereasoningtasksforsmallermodels,underscoringpromisingdirectionsindata
augmentationwithLLMsandbroaderknowledgedistillationmethods.8.1. FutureWork 139
8.1 Future Work
Having summarised key insights aimed at optimising the utilisation of knowledge
inNLPapplicationsthroughoutthisthesis,weconcludewithavisionforpromising
futureresearchdirectionsasfollows.
DynamicKnowledgeIntegrationwithEmphasisedGrounding
As the capabilities of LLMs continue to advance, the incorporation of knowledge
emergesasacrucialsupplementtomaintainthegeneralityofLLMswhileensuring
theyremainup-to-dateandfinelyadaptedtodomain-specifictasks.
Current approaches, including the studies in this thesis, often involve single or
pre-defined knowledge sources based on the targeted downstream tasks. For in-
stance, in Chapters 3-5, we employed static knowledge bases for both pre-training
anddownstreamtasks. Althoughthesestrategiesdemonstrateeffectivenessinbench-
mark tasks, they risk obsolescence without re-training. Hence, there is a growing
needformoreadaptiveanddynamicknowledgeintegrationmethodsinreal-world
applications.
ApromisingapproachintroducedinChapter2isretrieval-augmentedlanguage
models for text generation, demonstrating the potential of leveraging dynamic un-
structured knowledge (P. Lewis et al., 2020; Guu et al., 2020; Izacard et al., 2023).
Whileextensiveresearchfocusesoneffectiveretrievalofdocuments(Borgeaudetal.,
2022;X.Maetal.,2023;Soaresetal.,2023)andrankingofthesedocuments(Zerveas,
Rekabsaz, and Eickhoff, 2023), limited attention is given to how these models are
groundedwiththeprovidedcontext. Questionsarise,suchaswhetherthemodelcan
learntodisregardirrelevantretrievedcontext(Yoranetal.,2024),andhowthegener-
ationisimpactedbythecontextretrievedfromend-to-endsystemsversusposthoc
retrieval systems. We posit that a more thorough investigation into the grounding
ofretrieval-augmentedsystemsisneeded.140 Chapter8. Conclusion
KnowledgeIntegrationwithMixture-of-Experts
In addition to dynamic knowledge integration, aligned with the concept of general-
purpose LLMs, there is a compelling need for a general-purpose knowledge augmen-
tation approach, one that does not necessitate the explicit specification of domain
knowledge.
Revisiting the investigation on fake news detection in Chapter 3, we noted the
importance of knowledge base relevance to the target task. Specifically, the in-
tegration significantly enhances the LIAR datasets but only marginally improves
COVID-19. Consequently, it may become desirable to alleviate the need for prede-
terminedknowledgebasesformodeltraining.
A promising direction for such exploration involves extending the adapter ap-
proach (R. Wang et al., 2021) as introduced in Chapter 2. This entails leveraging
differentknowledgesourcestotraindistinctadapters,therebyenhancingtheadapt-
ability of LLMs to various domains. Beyond adapters, broader parameter-efficient
fine-tuningmethods,suchasLow-RankAdaptation(LoRA)(E.J.Huetal.,2022),are
alsopromising. Byretainingthepre-trainedweightsofpowerfulLLMs, itbecomes
possibletotraindifferentversionsofknowledgeadaptersorlow-rankmatrices. Sub-
sequently,acarefullydesignedroutingmechanismbecomescrucialtodeterminethe
appropriateknowledgecomponentforagivencontext.
While related work has demonstrated the advantages of the mixture of experts
(Duetal.,2022;Shenetal.,2024),themixtureofadapters(YaqingWangetal.,2022),
and the mixture of few-shot LoRA learning (C. Huang et al., 2023) for cross-task
generalisation,webelievethereexistsunexploredpotentialforknowledgeaugmen-
tationcombinedwiththemixtureofexperts(Diaoetal.,2023).
FactualityinTextGeneration
Recent advanced LLMs excel at text fluency, typically empowered by large-scale
pre-training and methods that leverage rankings over responses, such as reinforce-
ment learning from human feedback (RLHF) (Ziegler et al., 2019; L. Ouyang et al.,
2022). However, language models are susceptible to producing convincing yet fac-
tuallyinaccurateclaims,commonlyreferredtoas“hallucinations”(Tianetal.,2023).8.2. Summary 141
Thisunderscoresthecriticalneedforadvancementsinfactualityandfaithfulnessin
text generation (Augenstein et al., 2023). An expanded exploration of this thesis on
knowledge grounding, in this context, plays a crucial role in determining what is
factualorfaithful, andincontrollingthegeneration(P.Xuetal., 2020; Rashkinetal.,
2021;Brahmanetal.,2022).
Researchers have made significant efforts in enhancing actuality in text genera-
tion,suchasfine-tuningwithautomaticallygeneratedfactualitypreferencerankings
(Tian et al., 2023), employing factual-nucleus sampling (N. Lee et al., 2022), train-
ing models to self-evaluate for faithfulness (Kadavath et al., 2022), among others.
Despite these advancements, there remains a considerable amount of unexplored
directions, such as the development of better evaluation metrics. Current metrics,
whethern-gram-basedorrelyingontrainedneuralmodelslikeBERTScore(T.Zhang
et al., 2020), often fall short in capturing factuality (E. Clark et al., 2023; Aharoni et
al.,2023). Moreover,commonlyusedentailment-basedNaturalLanguageInference
scores for faithfulness evaluation (Maynez et al., 2020) offer limited interpretability
andmaybebiasedbytheunderlyingpre-trainedmodels.
Therefore, we anticipate that training models for more faithful and grounded
generation,aswellasdevelopingmetricstoevaluatefactualityperformance,willbe
highlyrelevantresearchdirections.
8.2 Summary
In this chapter, we highlight key findings and conclusions of the thesis, along with
excitingandpromisingdirectionsforfutureworkincludingdynamicknowledgein-
tegrationwithemphasisedknowledgegrounding,knowledgeintegrationwithmix-
ture of experts, and knowledge-grounded text generation for enhanced factuality.
Quoting Prof Christopher D. Manning,1 we echo the sentiment of NLP research
“Nothing but blue skies!” as we look forward to the boundless possibilities that
lieaheadintheever-evolvinglanguagetechnology.
1https://2023.emnlp.org/program/keynotes143
Appendix A
Language-Specific Results for
E CS Experiments
NTITY
Wereporttheper-languageresultsoftheexperimentsonENTITYCSinthefollowing
tables. WikiAnn results can be found in Table A.1. X-FACTR results in Table A.2,
MultiATIS++SlotFilling-onlytraininginTableA.3,andXL-WiCinTableA.4.
MODEL AR HE VI ID JV MS TL EU ML TA TE AF NL EN
XLM-ROURS 44.6 51.9 68.3 48.6 59.6 63.3 72.5 61.2 63.2 54.3 49.3 76.3 80.7 83.4
MLM 50.7 53.7 72.7 56.4 59.2 68.4 75.1 58.4 65.1 58.1 53.0 76.3 80.9 84.2
WEP 49.9 52.4 69.8 57.4 60.1 66.7 74.0 60.1 60.8 56.1 48.2 76.5 80.3 83.8
PEPMRS 47.1 52.6 69.8 56.0 60.1 62.4 74.8 56.1 61.6 56.1 50.9 77.9 81.4 83.8
PEPM 47.7 52.9 68.9 59.1 63.1 65.5 76.3 60.0 64.0 57.5 51.6 76.8 80.9 83.9
WEP+MLM 50.3 53.2 69.8 60.8 60.7 69.8 74.5 59.2 64.8 57.2 51.7 76.4 80.9 84.1
PEPMRS+MLM 46.7 53.6 69.6 64.0 60.2 69.2 74.3 57.5 65.6 55.8 52.3 77.5 81.3 84.1
PEPM+MLM 52.4 53.5 70.4 60.3 59.7 69.2 75.5 58.4 66.6 58.1 54.5 77.7 81.2 84.1
DE EL BN HI MR UR FA FR IT PT ES BG RU JA
XLM-ROURS 75.4 74.2 67.9 68.3 61.8 55.8 47.6 78.0 78.2 78.9 76.2 77.3 63.9 22.9
MLM 75.2 76.3 73.9 69.9 64.5 67.0 51.6 79.0 78.6 79.5 77.6 78.6 67.2 22.7
WEP 74.7 74.5 70.8 67.5 61.1 60.7 50.9 77.6 77.7 77.3 74.1 78.7 66.3 20.7
PEPMRS 75.4 74.8 69.6 68.3 64.1 48.7 53.0 78.9 78.6 78.7 77.1 78.6 67.3 21.9
PEPM 75.1 76.5 73.0 69.6 65.8 63.3 55.3 78.5 78.4 78.6 74.3 78.2 67.2 21.0
WEP+MLM 75.4 75.2 72.1 68.9 63.9 58.6 53.4 78.1 78.3 79.0 74.9 78.3 66.6 23.0
PEPMRS+MLM 75.5 76.2 72.2 68.4 64.8 60.1 54.3 79.2 79.1 80.0 76.4 79.1 67.6 23.6
PEPM+MLM 75.3 76.2 74.2 70.0 67.1 64.5 50.0 79.9 78.9 79.7 78.4 79.3 68.2 22.7
KA KO TH SW YO MY ZH KK TR ET FI HU
XLM-ROURS 66.4 48.8 4.3 68.3 45.4 52.7 27.7 44.2 76.9 72.4 75.6 76.9
MLM 66.1 50.8 2.5 65.1 42.9 55.7 29.7 50.7 77.8 71.4 76.2 78.1
WEP 64.8 52.0 2.5 65.8 50.4 52.6 26.1 52.1 75.5 71.9 75.8 76.6
PEPMRS 63.6 51.4 3.7 66.2 45.9 54.6 26.6 49.1 78.0 72.8 77.2 77.7
PEPM 66.7 50.0 5.0 66.8 52.3 56.9 26.7 48.4 77.6 73.2 76.6 77.3
WEP+MLM 65.8 50.4 2.1 63.9 44.9 56.6 29.1 51.0 75.2 71.8 76.9 77.2
PEPMRS+MLM 66.1 50.9 2.4 66.6 41.4 54.5 31.1 51.8 78.4 71.4 77.1 78.7
PEPM+MLM 67.7 51.1 3.3 64.5 43.7 56.6 29.3 51.7 78.2 72.0 76.8 78.6
TABLEA.1: F1-scoreperlanguageontheWikiAnntestset. Resultsareaveraged
acrossfiveseeds.144 AppendixA. Language-SpecificResultsfor ENTITYCSExperiments
MODEL AVG EN FR NL ES RU ZH HE TR KO VI EL MR JA HU BN CEB WAR TL SW PA MG ILO
A 3.5 8.2 4.7 4.4 6.5 5.3 4.6 2.5 3.1 5.1 8.5 6.3 2.7 2.3 0.9 0.1 1.4 1.2 2.8 3.7 0.2 1.9 0.1
S 9.4 15.2 11.3 11.0 13.4 14.4 11.9 12.3 4.0 16.7 14.2 27.3 19.5 9.2 2.2 0.0 1.7 1.3 5.1 5.6 5.8 3.7 0.4
M 2.1 3.3 2.3 2.6 3.3 3.8 4.5 2.2 2.5 2.6 5.1 2.9 1.1 2.1 0.2 0.1 1.0 1.1 1.4 1.9 0.0 1.6 0.0
A 3.3 4.4 2.9 2.7 4.3 5.5 5.3 3.0 3.0 5.6 9.5 7.3 3.4 4.4 0.9 0.1 1.2 1.1 2.3 2.9 0.6 1.8 0.5
S 7.5 5.2 4.4 3.6 4.9 14.2 11.8 11.4 3.9 15.9 12.6 25.6 18.9 8.8 2.0 0.0 1.4 1.4 4.4 4.3 5.8 3.5 0.5
M 2.6 3.9 2.3 2.7 4.2 4.1 5.2 2.7 2.4 3.4 7.0 4.3 2.07 4.2 0.3 0.1 1.0 1.1 1.3 1.9 0.4 1.5 0.5
A 2.3 2.1 3.7 2.9 3.9 2.9 1.9 3.4 1.2 5.0 4.6 4.2 3.6 0.3 0.7 0.0 2.1 1.0 1.4 5.2 0.0 0.0 0.1
S 6.4 5.1 8.7 6.4 9.4 6.0 8.3 8.7 3.1 16.6 9.1 19.3 17.9 2.5 1.8 0.6 2.9 1.1 4.4 8.3 0.3 0.5 0.1
M 1.3 0.9 2.0 2.0 1.9 2.0 1.8 1.9 0.6 2.3 2.4 2.8 2.1 0.2 0.4 0.0 1.8 1.0 0.5 2.2 0.0 0.0 0.1
A 2.5 2.5 3.6 2.9 4.3 2.6 2.0 4.8 1.1 5.7 6.3 5.2 4.2 0.4 0.6 0.1 2.0 1.0 1.2 5.2 0.0 0.0 0.1
S 5.9 4.9 7.6 5.9 9.0 4.4 7.6 7.4 2.5 16.1 8.5 17.2 16.7 2.5 1.6 0.6 2.8 1.1 3.9 7.8 0.3 0.5 0.0
M 1.7 1.8 2.3 2.2 2.6 2.3 1.9 3.4 0.5 3.4 4.6 4.2 2.9 0.3 0.4 0.0 1.7 1.0 0.4 2.4 0.0 0.0 0.1
A 3.3 18.2 6.1 6.0 5.8 1.1 0.4 0.4 1.1 0.5 8.0 3.5 0.4 0.6 3.7 0.0 3.5 0.6 5.0 4.2 0.1 1.7 1.6
S 8.5 38.3 16.4 18.7 14.9 4.4 3.4 1.4 5.6 2.7 16.8 7.3 4.1 2.5 8.5 0.0 6.9 2.6 9.7 10.3 0.0 6.9 5.4
M 1.6 9.4 2.7 2.9 2.9 0.6 0.3 0.4 0.3 0.3 3.5 1.2 0.1 0.5 2.6 0.0 1.5 0.3 1.5 1.9 0.1 1.1 0.5
A 3.1 16.2 6.4 5.6 5.4 1.1 0.3 0.4 1.1 0.5 7.6 3.4 0.4 0.6 3.6 0.0 3.4 0.5 4.5 4.1 0.1 1.3 1.7
S 7.9 35.8 15.9 17.2 13.3 4.5 2.7 1.6 5.4 2.4 15.8 7.3 4.1 2.5 8.2 0.0 6.6 2.5 8.6 7.8 0.0 6.4 5.4
M 1.5 7.5 3.3 2.9 2.9 0.6 0.3 0.4 0.2 0.3 3.6 1.1 0.1 0.5 2.6 0.0 1.5 0.2 1.5 2.0 0.1 1.0 0.6
A 6.1 15.6 9.1 11.5 10.5 2.8 6.7 3.7 3.2 6.7 13.2 7.9 4.0 4.6 6.7 0.9 4.3 2.1 7.4 7.2 0.0 2.3 3.3
S 19.4 36.4 24.1 30.3 25.6 14.3 18.5 34.7 12.2 31.5 23.4 36.0 29.8 17.8 18.5 6.1 8.5 5.0 16.9 21.3 0.0 5.4 9.3
M 3.0 7.2 3.9 4.9 4.6 1.5 6.3 2.6 1.0 2.9 7.3 3.9 1.5 4.1 2.5 0.0 2.2 1.4 1.8 3.3 0.0 1.4 0.6
A 4.9 12.1 8.2 9.6 8.8 2.4 3.1 3.3 2.9 5.9 9.3 7.4 3.5 1.9 5.6 0.8 4.1 1.7 6.8 5.7 0.0 1.8 3.3
S 17.4 32.6 22.9 26.5 23.4 12.2 16.7 32.4 11.2 28.3 19.3 34.3 27.1 15.9 16.0 5.6 8.2 4.7 14.9 17.2 0.0 5.1 9.2
M 2.1 4.6 3.3 3.6 3.0 1.2 2.6 2.3 0.8 2.7 3.9 3.7 1.4 1.5 1.8 0.0 2.1 1.0 1.9 2.0 0.0 1.0 0.7
A 5.8 13.9 7.6 10.1 11.2 2.8 7.2 2.9 2.9 5.8 13.6 8.1 4.4 3.2 7.2 0.6 3.1 2.4 6.8 6.6 1.0 2.5 3.2
S 18.5 34.5 20.0 28.9 25.3 14.0 20.1 26.0 13.0 28.6 25.4 35.0 25.6 17.3 18.2 4.9 6.7 7.2 13.6 17.6 11.6 5.7 8.3
M 2.7 6.6 3.0 4.7 5.2 1.3 6.8 2.3 0.9 2.5 7.6 4.3 2.1 2.7 3.1 0.0 1.8 0.9 1.3 1.4 0.2 1.3 0.4
A 4.6 11.3 6.4 8.6 9.1 2.2 2.7 2.5 2.7 4.9 10.5 7.2 3.5 1.8 6.1 0.6 2.8 2.0 6.2 5.1 0.8 1.4 2.5
S 16.3 31.5 18.4 26.3 22.3 11.8 18.0 24.3 12.2 25.5 22.3 31.3 20.8 14.5 16.4 4.4 6.1 6.6 11.6 15.3 8.9 2.4 7.6
M 1.8 4.7 2.1 3.5 3.4 0.9 2.3 2.0 0.8 2.1 4.9 3.5 1.5 1.4 2.2 0.0 1.6 0.7 1.1 0.9 0.1 0.6 0.4
A 4.7 15.1 6.9 11.0 9.6 5.0 3.8 3.2 2.0 7.3 9.0 5.5 3.0 3.3 1.9 0.2 3.3 1.5 5.9 5.5 0.0 0.7 0.6
S 15.0 35.2 18.6 29.4 22.0 16.7 15.7 19.4 8.7 29.3 19.2 30.2 24.5 19.9 4.6 1.7 6.4 2.5 10.3 12.8 0.0 1.1 1.2
M 2.4 7.1 2.4 4.5 4.2 2.1 3.5 2.6 0.7 3.3 4.4 3.5 0.5 2.7 1.0 0.0 2.0 1.2 2.4 2.8 0.0 0.5 0.4
A 6.0 15.7 8.1 12.5 11.7 5.7 6.9 5.2 2.9 9.2 14.0 6.3 5.1 6.7 3.4 0.4 3.4 1.5 6.4 5.6 0.0 0.5 0.5
S 13.1 31.9 17.1 27.1 19.6 12.1 13.6 17.7 7.6 26.1 16.0 27.1 21.4 16.9 3.9 1.6 5.3 2.2 9.4 9.0 0.0 0.8 1.1
M 4.3 10.0 4.5 7.1 8.6 4.0 6.7 4.7 2.0 6.2 11.8 4.9 3.3 6.4 2.7 0.2 2.4 1.3 3.9 3.8 0.0 0.2 0.3
A 2.6 16.8 5.0 5.2 4.9 1.5 0.2 0.6 0.2 0.6 6.3 3.0 0.4 0.6 1.0 0.0 1.2 0.4 4.5 2.3 0.6 1.8 0.5
S 6.5 35.5 13.4 15.5 12.3 6.2 2.0 4.7 0.5 3.3 13.0 8.7 3.1 3.0 1.9 0.0 2.5 0.5 7.0 5.3 0.7 4.5 0.4
M 1.2 6.6 2.2 2.7 2.3 1.2 0.2 0.5 0.1 0.3 2.8 0.7 0.1 0.5 0.3 0.0 0.6 0.4 1.4 1.0 0.5 1.3 0.5
A 2.7 18.0 5.3 5.1 4.6 1.5 0.6 1.0 0.2 0.9 6.3 3.0 0.6 0.9 0.9 0.0 1.5 0.3 4.6 2.1 0.6 1.6 0.4
S 5.7 33.1 12.0 13.3 10.2 5.8 2.0 4.4 0.5 2.7 11.1 7.3 3.1 3.0 1.7 0.0 1.7 0.3 6.7 4.6 0.6 1.0 0.3
M 1.6 10.4 3.0 3.2 2.6 1.2 0.5 0.9 0.1 0.8 3.8 1.2 0.3 0.8 0.3 0.0 1.1 0.3 1.8 1.1 0.5 1.2 0.5
A 4.9 14.9 9.7 10.5 10.5 7.3 5.5 4.4 1.3 7.0 9.4 5.8 2.0 2.6 1.6 0.0 2.8 0.9 4.5 6.5 0.0 0.3 0.4
S 13.9 34.8 24.2 27.7 23.1 20.2 16.0 17.5 5.8 28.6 19.3 25.2 13.5 15.3 4.0 2.4 5.5 1.7 8.3 11.2 0.0 0.3 0.7
M 2.4 6.3 3.4 4.6 4.2 3.8 5.2 2.7 0.5 3.0 4.6 4.1 0.6 2.1 0.7 0.0 1.7 0.8 1.9 3.1 0.0 0.3 0.2
A 5.7 16.3 10.5 11.0 11.3 7.9 7.0 6.2 1.3 9.2 14.2 6.6 2.8 4.2 1.5 0.3 3.4 1.3 4.1 6.6 0.0 0.3 0.3
S 12.0 30.6 21.6 24.8 20.1 16.6 15.7 16.0 3.0 27.1 15.8 20.8 12.3 12.8 3.4 0.0 4.9 2.0 7.0 9.5 0.0 0.2 0.4
M 3.9 9.4 5.4 6.2 6.7 5.0 6.7 4.5 0.8 6.0 11.8 5.6 1.5 3.8 0.7 0.3 2.8 1.3 2.6 4.5 0.0 0.3 0.2
A 4.5 14.1 8.6 9.1 8.8 5.8 5.0 3.1 0.8 6.3 9.4 5.7 1.8 1.5 1.5 0.1 2.9 1.7 4.0 5.2 0.7 2.3 0.4
S 13.2 32.7 22.0 24.4 20.3 18.6 17.0 12.8 3.8 26.9 19.4 25.9 12.2 12.0 3.4 0.9 5.0 2.1 9.7 9.5 8.1 3.1 0.9
M 2.3 5.9 3.3 4.4 3.8 3.1 4.7 2.6 0.4 2.8 4.4 3.8 0.4 1.1 0.7 0.0 2.2 1.5 1.2 1.3 0.0 1.9 0.2
A 5.5 15.7 9.5 9.8 9.3 6.2 6.9 5.0 0.8 8.1 13.5 7.3 2.8 3.0 1.4 0.1 2.6 1.8 4.4 6.8 2.0 2.6 0.4
S 11.9 31.3 19.2 22.0 16.8 15.7 16.3 12.5 3.5 25.0 16.6 23.9 11.3 8.6 2.8 0.6 4.3 2.0 9.0 9.4 7.5 2.7 0.8
M 3.8 8.7 5.2 6.0 5.5 4.2 6.6 4.5 0.5 5.4 10.9 5.9 1.7 2.7 0.7 0.1 2.3 1.8 2.3 3.7 1.5 2.6 0.2
TABLE A.2: X-FACTRresults.
SRUOR-MLX
MLM
PEW
SMPEP
MLM+
SMPEP
93
NE
93
39
93
NE
93
39
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOC
DNI
FNOCAppendixA. Language-SpecificResultsfor ENTITYCSExperiments 145
MODEL EN ES DE HI FR PT ZH JA TR
XLM-ROURS 95.6
0.15
81.5
0.71
79.8
2.04
50.6
5.35
74.8
1.90
76.5
1.14
77.2
2.06
56.8
4.99
43.0
2.72
MLM 95.6 0.16 78.8 2.88 78.0 2.56 61.5 7.26 74.4 3.18 74.6 1.39 76.4 1.81 70.3 2.00 39.7 4.09
WEP 95.7 0.15 79.9 1.34 80.3 0.58 52.7 4.15 75.6 0.87 76.3 0.63 78.1 1.43 60.7 7.07 40.4 4.40
PEPMS 95.3 0.06 79.3 2.60 79.7 2.28 62.9 2.30 75.3 2.10 76.2 1.60 77.8 1.30 69.0 4.90 45.3 2.50
PEPMS+MLM 95.6
0.10
81.3
1.90
81.4
0.90
65.8
2.20
78.2
0.30
76.1
1.00
78.8
0.60
68.8
3.30
42.1
3.30
TABLE A.3: F1-score (average across five seeds) on MultiATIS++ SF-only train-
ing.
MODEL BG DA ET FA HR JA
XLM-Rours 57.5
1.03
60.6
2.06
61.7
3.23
62.4
1.05
61.7
2.93
54.0
1.56
MLM 59.3 0.85 59.0 0.72 60.6 1.15 63.5 1.41 62.3 1.87 52.6 1.26
WEP 59.0 1.84 61.3 1.13 62.2 0.74 64.9 1.06 63.7 2.40 54.7 2.69
PEPMS 59.4 0.93 60.7 1.03 64.4 1.72 63.5 1.51 64.2 1.85 53.6 2.60
PEPMS+MLM 59.7
1.04
60.9
1.20
63.9
1.02
63.1
1.63
63.8
1.89
53.2
2.18
MODEL KO NL ZH DE FR IT
XLM-Rours 62.4
1.99
61.5
1.94
56.4
3.83
57.7
1.58
56.4
1.40
57.1
1.35
MLM 63.1 1.38 62.3 0.76 52.4 1.03 57.2 0.54 56.6 0.33 58.0 1.09
WEP 64.6 0.74 63.8 0.77 55.2 3.30 59.6 1.04 57.0 0.98 59.1 1.03
PEPMS 64.6 2.88 63.9 0.79 52.8 3.01 59.5 1.79 57.4 0.64 58.8 1.25
PEPMS+MLM 62.1
3.10
63.0
1.01
53.2
2.24
59.0
0.90
57.3
0.58
58.3
1.00
TABLE A.4: XL-WiC test set accuracy (average across five seeds) across lan-
guages.147
Appendix B
Additional Results and Analysis
for W IE
EB
B.1 Additional Results
WeshowinTableB.1theresultsfornon-EnglishlanguagesformWEBIEwhenspec-
ifyingthesourcelanguageandusingthedefault(English)forthemBARTtokenizer.
TheseresultsarefrombeamsearchwithoutconstraintTrie. Wecanseethatspecify-
ing the source language mostly harms the performance (except French), especially
for Portuguese. We hypothesise that due to the model being trained solely on En-
glishasthesourcetoken,mBARTmayhavedifficultyhandlingotherlanguages.
ENasSourceLanguageinmBARTTokenizer XXasSourceLanguageinmBARTTokenizer
LANGUAGE
P R F1 Empty-Pos% Acc-N P R F1 Empty-Pos% Acc-N
FRENCH 43.27 36.13 39.38 11.89 96.19 41.29 37.73 39.43 8.56 94.87
SPANISH 41.93 34.63 37.93 12.34 96.74 40.47 36.57 38.42 8.56 95.82
PORTUGUESE 41.17 32.37 36.24 14.07 96.91 13.81 1.77 3.14 86.33 98.21
HINDI 4.28 1.62 2.35 67.38 98.64 3.69 1.69 2.31 60.62 98.43
TABLEB.1: Comparisonofthezero-shotperformanceonmWEBIEwithmBART
when specifying the source language (XX) and keeping the default setting as
the source language (EN). Results are with standard beam search (without the
constraintTrie).
B.2 Examples of ReFinED Output
We show examples of the sentences processed by ReFinED in Table B.2. For each
input sentence, ReFinED identifies the set of entities in that sentence, and outputs148 AppendixB. AdditionalResultsandAnalysisfor WEBIE
ExampleId Sentence ReFinEDOutput
21464177 On Thursday, [[“Thursday”, None, “DATE”],
British campaigning [“British”, Entity(wikidata_entity_id=Q145,
group the Environ- wikipedia_entity_title=United Kingdom),
mentalInvestigation “GPE”], ["Environmental Investigation
Agency accused Agency", Entity(wikidata_entity_id=Q1345905,
Italy of trying to wikipedia_entity_title=Environmental
sabotage efforts to Investigation Agency), "ORG"],
reformtheEUETS. ["Italy", Entity(wikidata_entity_id=Q38,
wikipedia_entity_title=Italy), “ORG”],
[“EU”, Entity(wikidata_entity_id=Q458,
wikipedia_entity_title=European Union), “ORG”],
[“ETS”, Entity(wikidata_entity_id=Q899383,
wikipedia_entity_title=ETSI),“ORG”]]
1274217 It culminates in the [[“decade-long22”, None, “DATE”],
decade-long debate [“1913”, Entity(parsed_string=[timepoint:
ending in 1913 [“1913”]]), “DATE”], [“Hetch Hetchy”,
to turn the Hetch Entity(wikidata_entity_id=Q1616130,
Hetchy valley in wikipedia_entity_title=Hetch Hetchy),
Yosemite National “GPE”], [“Yosemite National Park”,
Parkintoareservoir Entity(wikidata_entity_id=Q180402,
forSanFrancisco. wikipedia_entity_title=Yosemite Na-
tional Park), “FAC”], [“San Fran-
cisco”, Entity(wikidata_entity_id=Q62,
wikipedia_entity_title=SanFrancisco),“GPE”]]
TABLE B.2: ReFinEDoutputson WEBIE validationexamples.
mentionspan,Wikidataid,andWikipediatitleforeachentity. Forourexperiments,
weusethewikipedia_model_with_numbersmodelwithwikipediaentityset.
B.3 MTurk Annotation Details
Inthissection,wedescribethedetailedsettingsforannotating(m)WEBIEwithMTurk.
WEBIE
Thefirstannotationtask(HIT)istoverifythecorrectnessofthetriplesautomatically
createdfromtheDSapproachandfilteredbytheNLImodel. Theguidanceandthe
interfaceareshowninFigureB.1andFigureB.2,respectively.
In each HIT, we provide a sentence with its entities highlighted (head entity in
blue and tail entity in green) and the URL of the web page which the sentence is
extracted from. For the first EL annotation job, we provide both the links to the
Wikipedia and Wikidata pages. Annotators are asked to choose if the highlighted
spans are linked correctly to the KB. Next, the annotators are asked to verify if aB.3. MTurkAnnotationDetails 149
FIGURE B.1: MTurkHITguidanceentityandrelationlabelling.
FIGURE B.2: MTurkHITuserinterfaceforentityandrelationlabelling.
relation (highlighted in orange) can be inferred from the sentence. We provide the
descriptionoftherelationandanexampleusecasetofacilitatetheannotation.
Each triple is annotated by three workers, and we pay $0.2 per HIT. We hire150 AppendixB. AdditionalResultsandAnalysisfor WEBIE
MTurkworkerswith MastersQualificationandsetadditional requirements includ-
ing(i)havingdone2,000HITsand(ii)havingajobapprovalrate≥99%.
mWEBIE
Figure B.3 and Figure B.4 illustrates the interface for correcting machine-translated
sentenceandidentifyingcorrespondingentitiesinthem. Asitischallengingtofind
qualified crowd workers for the translation task1, we set the geographical regions
for each language to the countries where the language is one of the official lan-
guages. WefindthatonlyIndiaandcountriesinAmericahaveanadequatenumber
of MTurk workers, which highly restricts the options for our target languages. In
the end, the countries we set for the target languages are as follows: Portuguese:
AO,BR,CV,ST,GW,GQ,MZ;Spanish: ES,MX,CO,PE,CL;CAforFrench,andIN
for Hindi2. It was also necessary to remove the Masters Qualification requirement
for MTurk workers (except Hindi) to find adequate annotators. We then conduct
pilot annotations, where we deliberately introduce errors in the reference machine
translationtoverifyiftheworkersunderourrequirementsettingsareabletocorrect
them.
We provide the English sentence paired with the original machine-translated
sentencefortheactualHIT.TheEnglishsentenceishighlightedwithitsentityspans,
andweinstructtheworkerstocorrectthetranslationwhileensuringthattheentities
arecorrectlytranslated. Afterconfirmingthetranslation,workersarethenaskedto
highlight the corresponding entities in the target language (in green). For negative
sentences without entity spans, the longest noun phrases were highlighted instead
topreventworkersfromsimplycopyingthereferencetranslations. Wepay$0.35per
HITforpositivesentencesand$0.25fornegativesentences(sincemostsentencesin
negative examples have only one highlighted entity/noun phrase and it is consid-
eredaneasiertask).
TwoMTurkworkerswereaskedforthetranslationtask,andanadditionalworker
wasaskedtoselectthebettertranslation,forwhich$0.10perHITwaspaid.
1Preliminary results where we include the USA for the mWEBIE annotation task indicate that
MTurkworkerswithlimitedornoknowledgeofthetargetlanguage(orEnglish)stillacceptthejob,
despiteourspecificrequirementforproficiencyinbothEnglishandthetargetlanguage.
2Forthemappingbetweencountrycodesandcountries,pleaserefertohttps://docs.aws.amazon.
com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_LocaleDataStructureArticle.html.B.3. MTurkAnnotationDetails 151
FIGURE B.3: MTurk HIT user interface for correcting the machine-translated
text.
FIGUREB.4: MTurkHITuserinterfaceforentitylabellinginthetargetlanguage.153
Appendix C
Additional Results and Analysis
for UMAE
C.1 Detailed Explanation Scores
ForexplanationgenerationontheVQAtasks,weevaluatetheperformanceofbeam
searchwiththesizeof5,top-ksamplingwithkfrom{50,100,200,...,1000},andNu-
cleus and Typical (Meister et al., 2023) sampling, both with p from {0.1,0.2,...,0.9}.
WeshowthedetailsoftheNLGscoresusingdifferentdecodingstrategiesforexpla-
nationsgeneratedfromQA→EinTableC.1,andQ→AEinTableC.2.
e-ViL N-GRAMSCORES LEARNT
DATASET DECODING
S E B1 B2 B3 B4 R-L MET. CIDEr SPICE BERTSC.
BEAMSEARCH 44.71 52.01 36.69 26.72 19.88 40.39 22.06 68.48 20.94 86.05
TOP-K(k=100) 44.34 52.56 37.06 27.06 19.72 44.45 21.58 73.44 19.38 86.27
A-OKVQA
NUCLEUS(p=0.4) 50.82 58.92 44.66 35.06 27.35 52.56 24.83 101.09 23.33 88.21
TYPICAL(p=0.6) 47.27 54.18 39.39 29.82 22.18 47.78 22.79 84.43 21.47 86.95
BEAMSEARCH 40.23 26.41 20.15 15.95 12.47 29.13 16.82 49.72 27.70 81.84
TOP-K(k=50) 33.19 20.98 14.89 11.18 8.33 23.65 13.72 32.73 21.99 80.31
VCR
NUCLEUS(p=0.1) 40.27 31.42 22.95 17.62 13.44 29.53 17.54 47.33 26.45 81.91
TYPICAL(p=0.4) 35.12 23.42 16.88 12.83 9.64 25.36 14.70 35.85 23.32 80.70
BEAMSEARCH 35.88 37.84 24.91 16.67 10.97 31.32 17.90 38.23 16.23 84.39
TOP-K(k=50) 33.28 38.35 23.11 14.21 8.45 29.15 17.05 32.89 15.26 83.41
VQA-X
NUCLEUS(p=0.1) 40.67 47.56 31.44 21.47 14.63 35.12 20.29 50.35 19.13 85.40
TYPICAL(p=0.5) 36.31 40.85 25.57 16.82 11.14 31.08 18.15 39.71 16.62 83.93
TABLE C.1: ExplanationscoreswithautomaticNLGforgeneratedexplanations
(QA→E) from UMAE model with different decoding strategies. B1 B4 corre-
ALL
spondto BLEU1 BLEU4, R-L means ROUGE-L and MET. means METEOR. Thelast
tworows(withblueshadow)indicateout-of-domainperformance.154 AppendixC. AdditionalResultsandAnalysisfor UMAE
e-ViL N-GRAMSCORES LEARNT
DATASET DECODING
S E B1 B2 B3 B4 R-L MET. CIDEr SPICE BERTSC.
BEAMSEARCH 47.01 54.75 41.39 32.08 24.25 49.75 22.54 86.28 20.68 87.39
A-OKVQA
NUCLEUS(p=0.5) 46.72 55.53 41.63 31.91 23.67 49.16 22.48 82.37 20.67 87.18
BEAMSEARCH 37.02 25.00 18.90 14.87 11.54 27.07 15.66 38.77 25.03 80.68
VCR
NUCLEUS(p=0.1) 35.10 27.41 19.36 14.50 10.73 26.18 15.21 34.99 21.88 80.52
BEAMSEARCH 38.13 39.91 26.30 17.99 12.46 31.69 19.11 42.10 18.15 84.95
VQA-X
NUCLEUS(p=0.1) 39.67 44.92 28.88 19.04 12.55 33.08 20.07 44.28 19.19 85.21
TABLE C.2: ExplanationscoreswithautomaticNLGforgeneratedexplanations
fromQ→AEwithUMAE model. B1 B4correspondtoBLEU1 BLEU4,R-Lmeans
ALL
ROUGE-LandMET. meansMETEOR. Thelasttworows(withblueshadow)indi-
cateout-of-domainperformance.
C.2 Examples of Generated Explanations
Examples of the explanations generated with beam search and Nucleus sampling
forA-OKVQAareshowninFigureC.2,andVCRinFigureC.1.
Ingeneral,Nucleussamplingachievesthebestperformanceacrossthedatasets.
However,top-kandTypicalsamplingdonotshowimprovementoverbeamsearch.
C.3 Examples of Issues in the Datasets
Weshowsomeoftheissuesinthedatasetsdescribedinsubsection6.6.2.
FigureC.3showsexamplesfromVCRthatrequireanunderstandingofthemovie
plot to generate answers. Figure C.4 shows examples from OK-VQA where ques-
tions and answers are subjective or ambiguous. Figure C.5 shows examples from
A-OKVQA and VQA-X that either contain wrong answers, questions that do not
need visual input or typos which severely impact the model generation (“house”
shouldbe“horse”).C.3. ExamplesofIssuesintheDatasets 155
Person2
Person8
Person2
Q wu ae lkst inio gn : a W wah ya t f re ov men ?t are Person2 and Person8 Question: Is Dog2 a living creature? Q peu re fs oti ro mn: a H nco ew ? does Person2 feel about this
Answer: A wedding reception. Answer: No, Dog2 is a stuffed animal. Answer: Person2 is slightly impressed by what
she sees.
Ground Truth Explanation: Ground Truth Explanation:
Everyone is really dressed up, and the lights give Dog2 is immobile, and wouldn't be still with so Ground Truth Explanation:
the area a fairy tale ambiance which is typical of much prey around. Dog2 is also obviously a toy, Person2 has a slight smirk on her face.
a wedding. based on the coloration.
Explanation Generation:
Explanation Generation: Explanation Generation: Beam: Person2 has her arms crossed and a slight
Beam: Person2 is wearing a pink dress and Beam: Teddybear1 and Teddybear2, and frown on her face.
Person8 is wearing a suit. Teddybear5 are stuffed animals, Dog2 is a Nucleus: Person2 has her arms crossed and a
Nucleus: Person2 is wearing a pink dress and stuffed animal. slight frown on her face, which suggest she is not
Person8 is holding hands with her. Nucleus: Dog2 is a stuffed animal, and stuffed impressed.
animals are not alive.
Answer + Explanation Generation: Answer + Explanation Generation:
Beam: Person2 and Person8 are walking away Answer + Explanation Generation: Beam: Person2 does not like what she is hearing,
from a wedding, this is because Person2 and Beam: No, Dog2 is not a living creature, this is this is because Person2 has her arms crossed and
Person8 are holding hands and wearing formal because Dog2 is a stuffed animal. a frown on her face.
clothing. Nucleus: No, Dog2 is not a living creature, this is Nucleus: Person2 is not impressed by it, this is
Nucleus: They are walking away from a wedding, because Dog2 is a stuffed animal, stuffed because Person2 is looking down and frowning,
this is because There are balloons and strings of animals are not alive. and her arms are crossed, which is a sign of
lights above them. disapproval.
Person2
Person1 Person1
Person1
Question: Why does Person1 have a pointy hood
Question: What is Person2 doing? on his cloak?
Answer: Telling Person1 something important. Answer: Person1 is a wizard. Question: Does Person1 drink alcohol?
Answer: Yes Person1 drinks alcohol.
Ground Truth Explanation: Ground Truth Explanation:
P he ar ns do sn 2 to i s e mfo pc hu ase sid z eo n w hP ae tr s to hn e1 y 'a ren d s aw ya inv gin . g their P cho ain rt ay c th eo r.ods are usually used to depict a wizard Person1G hro au sn ad T furu llt h b eE ex rp l ia nn fa rt oio nn t: of him.
Explanation Generation: Explanation Generation:
Beam: Person2 is sitting across from Person1 who Beam: Wizards often wear pointy hoods. Explanation Generation:
is on the phone. Nucleus: Wizards often wear pointy hoods and Beam: Person1 has a cup of beer in front of him.
Nucleus: Person2 is sitting in a chair and is robes. Nucleus: Person1 is drinking from Cup2.
looking at Person1 who is on the phone.
Answer + Explanation Generation: Answer + Explanation Generation:
Answer + Explanation Generation: Beam: Person1 is trying to hide his identity, this Beam: Yes, Person1 drinks alcohol, this is
Beam: Person2 is listening to Person1 talk on the is because Person1 is standing in front of because Person1 has a cup of beer in front of
phone, this is because Person2 is sitting in a Person2 and Person3, and Person4 who are all him.
chair facing Person1 who is on the phone. looking at him. Nucleus: Yes, he does drink, this is because He
Nucleus: Person2 is telling Person1 a story, this is Nucleus: Person1 is a wizard, this is because has Cup2 in front of him and it is full of beer.
because Person2 is sitting in a chair and Person1 Wizards often wear pointy hoods and robes.
is on the phone.
FIGURE C.1: Examples of generated answers and explanations generation for
VCR.156 AppendixC. AdditionalResultsandAnalysisfor UMAE
Question: Why is the woman wearing Question: What time of day is it likely Question: What has caused the Question: What are the umbrellas
goggles? right now? elephants to turn brown? placed in the sand to block?
Answer: protection Answer: morning Answer: dirt Answer: sun
0 p 1 w a e 1nnr) ) )eo cdaT T Tt o reh h ho usce e et nt h rpG i tw s eo er enr o rnoo r io st m su be sw .n ai kca t d t isgn lis onT o o i tgr hgs f u e o g ddt w r flh oee e se wE bsa uy rx nr t ne ip hi s h n sl i ga i s sg lfn ll h a r .wa g o e rt o eo mi o mg mn a g t is na ghl: e dn hes t s sf nuo onr w 0 t b 1 2 th hr) ) )e re I T Y o at t oh uk a ui e gfs b a h cl tG s se i au tmtr , h. no n s eu e n so n y ei w d si et ioT nm lu tr i dhku ot o. eet r wh nlt y l h ii E gn ae ix s hg nrp e t. dtl a i sman dhra oe iet n o i o iff rno .rn u grs i : t in on 0 a 1 t b w 2hr r) ) )aeo o tT T Tmu w eh h hn rns .e e ed es s e eG le i uvn l lr e ee beo p pt s su l he h htn i aep na ad n h n n mT t ca t th er s sn uu e t d ta at sh hn. r r ae eahE t tax c d uvp io isr rel av a tn e l yn b l e .yra e i at nei o ro gn cn tc hs r u: o erl rli in ng g 0 d s 1 2ha) ) ) ay T UT , d hh mea ee n b y f uG d r r o ea m r t mo lr h lbeu ae rn sto eh d un pl e lmT ra a or s sbu vu b rt b inh e de l. o l eaE l a c cx skshp h l sg aoa uin dn v na e ea .t . i co s on u os n: l n ay nd
flying up when skiing.
Explanation Generation: Explanation Generation: Explanation Generation:
Explanation Generation: Beam: The people are using umbrellas Beam: The elephants are in mud. Beam: The umbrellas block the sun.
Beam: The woman needs protection. because it's raining. Nucleus: The elephants are standing in Nucleus: The umbrellas are blocking the
Nucleus: The woman is skiing and Nucleus: The people are using dirt. sun.
needs protection. umbrellas to keep from getting wet.
Answer + Explanation Generation: Answer + Explanation Generation:
Answer + Explanation Generation: Answer + Explanation Generation: Beam: mud, this is because The Beam: sun, this is because The
Beam: protection, this is because The Beam: morning, this is because The sun elephants are muddy. umbrellas block the sun.
woman wants to protect her eyes. is shining through the window. Nucleus: mud, this is because The Nucleus: sun, this is because The
Nucleus: protection, this is because The Nucleus: morning, this is because There elephants are in the mud. umbrellas are placed in the sand to
woman is wearing goggles to protect is light coming through the window. block the sun.
her eyes from the sun.
Question: Why are the people lining up? Question: What is the white appliance Question: Why are the people using Question: The layout of buildings most
Answer: boarding the people are looking in being used to umbrellas? resembles which period?
store? Answer: it's raining Answer: historical european
Answer: food
Ground Truth Explanations:
0) The people are near a plane as if Ground Truth Explanations: Ground Truth Explanations: Ground Truth Explanations:
they want to get on. 0) The appliance is a fridge. 0) There is rain and the people are 0) The cars are on the left side of the
1) The people are boarding. 1) Perishable food is kept in this little being sheltered by umbrellas. road, so it is either the united kingdom
2) The people are lined up facing the refrigerator, which is too small for 1) The people want rain protection. or ireland.
plane. people line up facing many people, but just right for one or 2) They are protecting themselves 1) The high pitched roofs and the
transportation vehicles when they are two. from getting wet from the rain. stone architecture is from then
preparing to board. 2) There are multiple edible items in 2) The layout is from the olden days.
the fridge already.
Explanation Generation: Explanation Generation:
B a N b oe upa aclm l ra e dn: u e sT a:.h T pe h l aep n e epo .ep ole p lea r ae r elin le ind e du p u pto t oboard B Ne ua cm le: u sT :h TeE h x eap pl aa p pn li pa at lni iao cn ne cG eise n ia se r ara e t ffi ro r iin dg: ge era . tor. B b N ue me ua c c bm a le ru: u e s sT le l: ah T se it h ' tesp o e r po ka ep ein o ele pi pn l ea g f r . ra e or meu s u gin s eg i tn t gu inm g b wre el tl .a s B N nie u na c em le t: eu esT : nh T teE h h x e bp c ul eba i nl un d ta ii l ut n di rgo i ynn s .g G a s re aen r e eor a ld ft ri . oo n m: the late
B p N Thee ua o ecm lp e pA l: ue e n sb os :ao w pbra le e o err ad l+ ai r n dp rE e e lax pd bp n l aul oea np a,n e ra t t , dht o i itio n s hbn g ioi s sG a a ie b r sn pd ee b l c atr ea ha nct u e ei ao s .upen sl : a eT nh ee . B a N ap pe ua p pcm ll lei iA a a: u n n nsfs o :c c w fo e eoe d oi ir s s, d + t a a, h E ti f fx s hr rp i iid di sl sa g g in b se ea e . .t b cio ean cu aG s uee s n eTe hr Ta e hti eon: B a r Nare uiea n c m liu enA : us g n i sr n .s :a w g ii tn e 'u s,r m+ t rh a bE i is nrx ep ii nlsl la g abn ,s ea tbt c hi a eo isun c a s iG suee s bn T e ee h r cie ta a t i upi so sen eo: p Tl he e B b N bu ue ua i icl lm ld deA i i: un n n sm g gs : w s sme e d l leo or i e do o+ v ik k eE a vl lx l i i, ak kp le etl ,a h n m mii ssa t e ebii s d do e n i icb e e aG e v vuc a ae a sn l l eue b b r s u uTa e i it hl l i d do T ei in h n n: e g gs s. .
people are using umbrellas to stay dry.
FIGURE C.2: ExamplesofgeneratedanswersandexplanationsforA-OKVQA.C.3. ExamplesofIssuesintheDatasets 157
Person1
Person3
Person3
Question: Why is Person 3 wearing a life jacket? Question: Why did Person1 drop Person3?
Answer: Person1 dropped Person 3 by accident.
Answer Options:
0) The boat has a leak, and Person3 is scared of Explanation Options:
drowning. 0) Person1 can upon Person3 in the woods, and kissed her; she awoke,
1) The boat is sinking and the life jacket will hep them and he dropped her off the bier.
float. 1) Person2 is Person3's mother. Person3 is an infant and can't walk on his own.
2) Person3 is piloting the ship. 2) Person3 is stuck in the toilet as Person1 is pulling her out.
3) Person9 is wearing a life vest in case the ship sinks. 3) Person3 is bent over and appears unsteady. Person1 looks concerned for her.
Generation: Person3 is on a boat. Generation: Person1 is kneeling over the body of Person3.
FIGURE C.3: Questions that require knowledge of the movie plots to generate
theanswersfromVCR.
Question: In which country are the
Question: Is this legal or illegal? transportation regulations loose Question: How long does it take to cook? Question: What nationality is this food?
Ground Truth Answers: enough to allow vehicles like these? Ground Truth Answers: Ground Truth Answers:
legal (6), illegal (4) Ground Truth Answers: 45 minutes (4), 20 minutes (2), 25 american (4), mediteranian (2),
Generation: legal india (8), china (2) minutes (2), minute (2) g reek (2), asian (2)
Generation: england Generation: 1 hour Generation: italian
FIGURE C.4: ExamplesofsubjectivequestionsfromOK-VQA.
Question: What country headquarters this Question: How long does the average Question: What is the brown house
plane company? giraffe live? doing?
Answer: usa Answer: 20-30 years Answer: walking
Ground Truth Explanations:
0) The headquarters are the us. Ground Truth Explanations: Ground Truth Explanations:
1) The company name is virgin atlantic 0) Giraffes can live a long time. 0) it has two legs up and two down
that was founded and has headquarters in 1) 20-30 years is the lifespan. and it is moving.
london england. 2) I looked up this answer on the 1) only two feet are touching the
2) The airplane has virgin atlantic livery. internet since there is no way to tell ground.
this company is based in england. the answer from the picture. 2) he is moving slowly on a mountain
range.
FIGURE C.5: Issues in the datasets that severely impact the model generation:
wrong answers (left, from A-OKVQA), questions that do not need visual input
toanswer(middle,fromA-OKVQA),andtypo(right,fromVQA-X).159
Appendix D
Additional Results for
LLM-powered Data Augmentation
This section includes the following additional results: Table D.1, Table D.2, and Ta-
bleD.3showgenerateddatainEnglishwithdifferentLLMsonXCOPA,XWinograd,
andXStoryCloze. TableD.4andTableD.5showthefullresultonXCOPAwithChat-
GPTandGPT-4.
Fine-tuned TrainData LLM AVG EN ET HT ID IT QU SW TA TH TR VI ZH
Dolly-v2 54.0 63.4 52.0 52.2 54.0 53.8 47.6 48.6 53.4 53.4 52.8 50.4 58.2
StableVicuna 53.5 62.4 51.6 49.2 55.8 55.8 50.0 50.2 50.2 52.6 51.0 50.4 56.0
GEN
ChatGPT 56.0 64.8 54.8 52.6 58.0 57.4 49.8 48.4 55.6 52.8 53.2 53.0 59.0
GPT-4 58.2 69.2 59.2 54.0 60.6 59.2 50.8 48.2 55.0 48.2 53.8 57.6 61.0
mBERT
Dolly-v2 54.4 59.8 52.6 53.2 53.0 56.4 53.8 52.4 50.4 54.8 49.8 52.6 58.8
StableVicuna 55.6 65.2 53.4 50.4 59.0 60.0 51.6 50.4 49.4 52.0 52.4 54.0 58.2
GEN+ORI
ChatGPT 54.6 59.6 56.4 53.6 53.8 51.4 51.4 51.6 50.4 52.6 54.0 55.0 59.2
GPT-4 59.3 72.6 58.8 53.0 62.0 61.0 53.0 50.0 54.0 48.2 52.0 57.6 64.6
Dolly-v2 59.0 64.4 58.8 52.8 60.8 61.0 50.8 55.6 60.4 58.0 57.2 58.6 59.0
StableVicuna 58.5 60.4 59.4 53.6 60.8 56.8 49.2 56.0 61.2 60.4 54.8 59.6 58.6
GEN
ChatGPT 58.8 62.4 56.4 52.4 61.4 58.6 52.2 52.0 63.4 61.2 56.4 59.6 62.8
GPT-4 63.6 67.0 62.4 52.0 68.6 62.6 51.8 58.6 65.4 64.8 63.2 66.6 69.6
XLMR-Base
Dolly-v2 58.7 65.6 57.6 52.2 60.8 58.4 52.4 58.2 57.4 58.0 58.4 58.0 59.8
StableVicuna 61.1 65.0 62.4 49.4 64.2 62.4 46.2 60.4 59.6 58.0 58.0 63.0 63.4
GEN+ORI
ChatGPT 59.8 63.8 61.6 51.6 62.6 59.8 51.2 51.6 60.4 61.6 61.8 64.8 62.0
GPT-4 63.6 69.6 63.8 51.2 67.2 62.4 52.6 58.4 63.8 66.0 64.2 66.8 69.4
Dolly-v2 59.6 62.4 58.6 49.6 64.8 59.2 50.6 56.8 60.8 58.8 57.0 61.0 63.0
StableVicuna 65.7 71.4 66.2 50.4 71.4 70.2 50.0 60.0 64.0 63.6 68.0 68.2 69.8
GEN
ChatGPT 65.2 71.2 64.6 51.6 70.8 66.6 51.0 58.8 66.0 68.2 69.0 68.8 68.8
GPT-4 73.6 83.2 71.2 52.0 81.2 78.2 51.0 62.2 76.6 77.4 75.0 78.4 79.0
XLMR-Large
Dolly-v2 66.4 74.2 62.8 53.0 72.0 70.4 46.2 61.6 65.6 66.2 69.6 67.6 70.6
StableVicuna 69.9 76.0 69.8 51.2 75.0 74.2 51.2 64.4 70.2 71.6 72.2 72.6 75.4
GEN+ORI
ChatGPT 69.5 76.4 69.8 48.2 76.0 72.8 50.8 63.4 67.8 70.8 70.2 73.4 77.8
GPT-4 73.7 84.6 70.4 50.0 80.8 80.2 51.8 65.8 72.8 76.0 74.8 78.4 80.4
TABLE D.1: AccuracyonXCOPAwithdifferentLLM-generatedEnglishdata.160 AppendixD. AdditionalResultsforLLM-poweredDataAugmentation
Fine-tuned Trainingdata LLM AVG EN FR JA PT RU ZH
Dolly-v2 56.47 71.24 53.01 52.45 53.23 54.92 53.97
StableVicuna 53.73 54.94 56.63 50.26 50.57 52.06 57.94
GEN
ChatGPT 56.00 54.94 54.22 54.01 52.09 55.87 64.88
GPT-4 54.90 56.22 56.63 52.55 51.71 52.38 59.92
mBERT
Dolly-v2 59.32 71.24 57.83 53.81 56.65 59.05 57.34
StableVicuna 58.46 57.94 63.86 53.81 57.41 58.41 59.33
GEN+ORI
ChatGPT 58.26 56.65 66.27 53.60 56.27 60.00 56.75
GPT-4 57.48 53.65 62.65 54.43 55.89 57.14 61.11
Dolly-v2 59.63 71.24 57.83 55.79 57.03 57.78 58.13
StableVicuna 58.95 60.09 55.42 57.35 52.47 58.73 69.64
GEN
ChatGPT 62.69 69.10 60.24 61.42 57.03 61.27 67.06
GPT-4 63.32 69.10 61.45 61.52 56.65 60.95 70.24
XLMR-Base
Dolly-v2 66.33 75.54 63.86 65.80 64.26 62.86 65.67
StableVicuna 65.97 64.38 66.27 67.15 63.88 65.71 68.45
GEN+ORI
ChatGPT 65.94 65.24 60.24 68.93 70.72 62.86 67.66
GPT-4 66.88 68.24 67.47 66.94 63.88 63.49 71.23
Dolly-v2 76.86 87.55 67.47 81.02 76.43 74.29 74.40
StableVicuna 68.22 74.25 63.86 68.20 66.16 63.81 73.02
GEN
ChatGPT 73.20 81.97 66.27 73.10 66.92 72.38 78.57
GPT-4 76.37 81.55 74.70 75.91 71.86 75.24 78.97
XLMR-Large
Dolly-v2 83.10 90.56 79.52 85.19 84.03 80.95 78.37
StableVicuna 82.02 83.26 80.72 83.84 86.31 82.22 75.79
GEN+ORI
ChatGPT 83.22 85.84 80.72 87.38 85.93 80.95 78.50
GPT-4 83.52 85.41 81.93 85.92 86.69 80.63 80.56
TABLE D.2: Accuracy on XWinograd with different LLM-generated English
data.
Fine-tuned Trainingdata LLM AVG EN RU ZH ES AR HI ID TE SW EU MY
Dolly-v2 68.7 78.8 71.3 73.6 74.2 67.4 66.9 69.0 65.0 60.9 66.8 62.0
StableVicuna 64.6 71.4 66.8 68.8 68.1 64.3 63.6 66.1 61.2 58.6 63.6 58.4
GEN
ChatGPT 64.3 69.7 66.4 68.1 68.0 64.6 64.5 66.6 59.8 59.2 62.3 58.4
GPT-4 68.0 75.5 70.8 73.3 70.4 67.6 68.2 69.6 63.1 62.3 65.4 62.2
mBERT
Dolly-v2 68.1 75.7 71.2 72.4 73.2 66.4 67.1 68.9 64.5 61.4 67.1 61.0
StableVicuna 67.3 77.0 71.0 70.2 71.4 67.2 66.5 68.4 62.4 60.5 64.3 61.4
GEN+ORI
ChatGPT 68.3 76.4 68.5 72.9 73.0 66.3 68.6 71.1 62.0 62.0 67.4 63.4
GPT-4 69.8 79.5 73.1 75.3 73.4 68.1 69.8 71.9 64.1 62.0 68.9 61.6
Dolly-v2 75.8 81.4 79.2 80.3 78.0 73.6 74.7 80.7 73.0 68.8 72.2 71.7
StableVicuna 69.6 72.3 71.1 71.5 70.4 68.3 70.4 72.1 68.4 65.7 68.0 67.7
GEN
ChatGPT 67.4 69.7 68.9 68.5 68.7 66.1 68.2 68.7 67.0 63.7 65.6 66.6
GPT-4 74.6 78.2 78.0 78.1 77.0 73.5 75.7 77.6 71.7 68.4 73.6 69.2
XLMR-Base
Dolly-v2 76.5 81.5 80.0 80.5 79.4 75.1 75.0 79.6 74.5 71.5 72.3 72.6
StableVicuna 74.2 79.2 77.4 77.8 76.4 74.0 74.5 78.2 70.2 67.6 71.7 69.6
GEN+ORI
ChatGPT 74.5 78.0 76.6 78.8 76.2 72.9 73.9 78.9 71.5 69.6 72.3 71.0
GPT-4 79.3 85.4 83.2 82.6 83.0 78.0 79.9 82.7 75.9 72.9 74.9 74.3
Dolly-v2 84.8 87.4 87.3 87.8 86.6 83.0 84.4 87.1 84.1 81.0 82.9 81.4
StableVicuna 74.6 76.7 75.9 77.4 76.2 72.9 74.5 76.2 74.3 70.8 73.5 72.5
GEN
ChatGPT 77.3 78.6 79.9 78.0 77.9 75.8 77.4 78.0 76.4 73.5 77.1 77.7
GPT-4 86.0 88.5 88.2 88.2 88.0 84.9 85.7 87.8 83.7 81.3 85.6 84.3
XLMR-Large
Dolly-v2 86.4 89.2 87.2 89.5 87.1 85.2 86.7 87.7 85.0 83.0 85.7 83.8
StableVicuna 84.8 88.4 87.6 87.8 86.6 82.9 83.3 87.4 83.7 81.3 83.7 80.0
GEN+ORI
ChatGPT 85.8 88.5 88.0 88.3 87.3 83.7 85.9 87.2 83.7 81.6 85.4 83.8
GPT-4 88.4 92.3 91.5 91.5 90.5 86.4 88.4 91.1 84.8 83.1 87.4 85.2
TABLE D.3: Accuracy on XStoryCloze with different LLM-generated English
data.AppendixD. AdditionalResultsforLLM-poweredDataAugmentation 161
Model TrainingData |Data| AVG EN ET HT ID IT QU SW TA TH TR VI ZH
ORI(BASELINE) 400 47.2 53.8 44.2 48.6 47.2 46.2 50.6 45.4 48.4 49.8 49.8 43.6 47.4
GENEN 3.7k 56.0 64.8 54.8 52.6 58.0 57.4 49.8 48.4 55.6 52.8 53.2 53.0 59.0
GENEN+ORI 4.1k 54.6 59.6 56.4 53.6 53.8 51.4 51.4 51.6 50.4 52.6 54.0 55.0 59.2
GENEN+ORI(TLV) 4.1k 57.6 68.0 55.4 54.0 61.2 59.8 51.8 51.2 55.8 54.4 52.2 53.4 59.2
GENEN 28.6k 57.2 66.2 55.8 50.8 58.6 58.2 53.2 51.2 57.2 53.2 52.0 56.0 61.0
mBERT GENEN+ORI 29k 57.0 66.6 55.4 51.4 59.2 58.6 52.4 50.8 53.6 53.2 50.0 54.8 62.8
GENEN+ORI(TLV) 29k 57.0 66.6 55.4 51.4 59.2 58.6 52.4 50.8 53.6 53.2 50.0 54.8 62.8
GENXX 3.6k/lang 57.5 64.8 57.8 57.4 58.0 60.2 54.6 51.4 53.0 – – 53.0 62.0
GENXX +ORI 4k 56.8 59.6 58.8 54.6 56.2 61.2 53.6 54.6 53.6 – – 52.0 60.2
GENTrans+ORI 4k 58.7 59.6 59.8 59.8 62.8 61.0 – 52.6 56.8 53.4 56.2 58.2 59.4
EN
GENTrans+ORI 29k/lang 60.6 66.6 61.8 57.8 60.8 62.2 – 53.2 58.4 53.2 63.0 60.6 63.8
EN
ORI(BASELINE) 400 55.6 57.6 54.6 50.6 59.6 54.8 46.0 55.0 53.4 56.2 55.2 54.8 59.6
GENEN 3.7k 58.8 62.4 56.4 52.4 61.4 58.6 52.2 52.0 63.4 61.2 56.4 59.6 62.8
GENEN+ORI 4.1k 59.8 63.8 61.6 51.6 62.6 59.8 51.2 51.6 60.4 61.6 61.8 64.8 62.0
GENEN+ORI(TLV) 4.1k 60.7 63.2 61.6 51.4 64.8 61.2 51.2 53.6 62.6 63.0 58.2 61.0 66.6
GENEN 28.6k 60.8 66.4 57.2 56.0 66.4 61.2 53.0 53.8 60.0 61.6 56.6 61.4 64.6
XLMR-Base GENEN+ORI 29k 62.1 64.6 61.8 50.6 66.8 63.6 48.0 55.6 65.8 63.6 57.2 63.2 66.8
GENEN+ORI(TLV) 29k 60.9 66.4 61.8 49.8 66.2 59.8 54.6 53.4 62.4 63.8 58.2 62.8 65.8
GENXX 3.6k/lang 58.8 62.4 57.0 55.6 61.4 59.0 55.6 54.4 56.8 – – 60.6 62.0
GENXX +ORI 4k 59.9 63.8 60.6 55.0 64.6 59.6 52.6 54.6 56.4 – – 59.6 64.8
GENTrans+ORI 4k 61.1 63.8 60.0 58.0 65.0 60.8 – 53.8 60.2 66.2 56.6 62.6 66.0
EN
GENTrans+ORI 29k/lang 62.2 64.6 63.2 57.2 64.8 61.2 – 55.0 61.2 59.2 59.5 64.2 68.4
EN
ORI(BASELINE) 400 64.4 71.4 62.8 51.4 69.0 65.8 52.0 60.6 62.0 64.0 61.2 69.4 66.8
GENEN 3.7k 65.2 71.2 64.6 51.6 70.8 66.6 51.0 58.8 66.0 68.2 69.0 68.8 68.8
GENEN+ORI 4.1k 69.5 76.4 69.8 48.2 76.0 72.8 50.8 63.4 67.8 70.8 70.2 73.4 77.8
GENEN+ORI(TLV) 4.1k 71.9 80.6 71.6 50.8 78.6 77.2 51.8 63.0 69.2 71.2 72.8 77.2 78.8
GENEN 28.6k 71.8 80.6 74.4 51.0 78.4 75.2 51.2 63.4 69.8 70.6 69.8 75.6 77.4
XLMR-Large GENEN+ORI 29k 72.4 81.0 73.8 54.4 80.2 75.2 48.8 61.4 70.4 73.8 70.4 75.6 79.8
GENEN+ORI(TLV) 29k 72.4 81.0 73.8 54.4 80.2 75.2 48.8 61.0 70.4 73.8 70.4 75.6 79.8
GENXX 3.6k/lang 63.4 71.2 62.6 54.2 71.0 65.8 49.4 53.8 56.4 – – 64.0 71.6
GENXX +ORI 4k 65.2 76.4 62.4 55.2 75.0 62.2 54.0 58.2 55.4 – – 66.2 76.2
GENTrans+ORI 4k 67.0 76.4 60.0 59.6 66.2 66.6 – 59.0 64.8 71.2 65.2 74.8 75.6
EN
GENTrans+ORI 29k/lang 71.5 81.0 71.8 57.2 79.8 74.4 – 54.8 71.4 72.6 70.0 77.2 75.6
EN
TABLE D.4: Full results on XCOPA (with ChatGPT-generated data). +TLV cor-
responds to including the original validation set in all Target Languages in the
Validation set. Rows are sorted by the number of instances used in training.
AVG shows average results for languages that are available in all settings (excl.
QU,TH,TR).162 AppendixD. AdditionalResultsforLLM-poweredDataAugmentation
Model TrainingData AVG EN ET HT ID IT QU SW TA TH TR VI ZH
ORI 47.2 53.8 44.2 48.6 47.2 46.2 50.6 45.4 48.4 49.8 49.8 43.6 47.4
GENEN 58.2 69.2 59.2 54.0 60.6 59.2 50.8 48.2 55.0 48.2 53.8 57.6 61.0
GENEN+ORI 59.3 72.6 58.8 53.0 62.0 61.0 53.0 50.0 54.0 48.2 52.0 57.6 64.6
mBERT GENXX 60.2 69.2 59.4 56.2 60.2 63.8 54.4 55.2 54.0 – – 61.2 62.2
GENXX+ORI 61.8 72.6 61.2 58.2 62.2 66.4 54.4 57.4 53.4 – – 63.0 61.8
GENTrans 61.4 69.2 59.2 56.8 65.4 65.2 – 53.4 56.8 52.6 59.6 61.8 65.0
EN
GENTrans+ORI 62.6 72.6 58.6 55.2 65.6 65.4 – 53.8 62.6 53.2 58.8 64.6 65.4
EN
ORI 55.6 57.6 54.6 50.6 59.6 54.8 46.0 55.0 53.4 56.2 55.2 54.8 59.6
GENEN 63.6 67.0 62.4 52.0 68.6 62.6 51.8 58.6 65.4 64.8 63.2 66.6 69.6
GENEN+ORI 63.6 69.6 63.8 51.2 67.2 62.4 52.6 58.4 63.8 66.0 64.2 66.8 69.4
XLMR-Base GENXX 63.2 67.0 60.8 56.4 68.6 62.4 57.4 58.2 60.2 – – 64.6 70.4
GENXX+ORI 64.0 69.6 62.2 56.2 68.6 63.8 56.8 57.8 61.2 – – 66.8 70.0
GENTrans 62.5 67.0 60.0 55.6 66.0 62.4 – 58.0 60.4 64.4 64.6 64.0 68.8
EN
GENTrans+ORI 63.9 69.6 61.6 56.6 68.4 65.2 – 58.2 60.2 68.0 62.6 66.0 69.6
EN
ORI 64.4 71.4 62.8 51.4 69.0 65.8 52.0 60.6 62.0 64.0 61.2 69.4 66.8
GENEN 73.6 83.2 71.2 52.0 81.2 78.2 51.0 62.2 76.6 77.4 75.0 78.4 79.0
GENEN+ORI 73.7 84.6 70.4 50.0 80.8 80.2 51.8 65.8 72.8 76.0 74.8 78.4 80.4
XLMR-Large GENXX 72.8 83.2 75.2 55.2 78.4 76.0 52.4 63.0 68.2 – – 77.8 78.6
GENXX+ORI 74.6 84.6 77.0 56.0 82.2 77.0 56.0 65.0 73.8 – – 76.2 80.0
GENTrans 71.0 83.2 72.4 55.6 79.4 78.2 – 60.6 67.8 77.8 72.6 64.0 77.4
EN
GENTrans+ORI 74.1 84.6 74.2 57.2 82.0 77.4 – 62.2 75.0 75.2 72.8 74.4 79.6
EN
TABLE D.5: Accuracy on XCOPA. GEN
EN
and GEN
XX
represents 3.7K and 3.6K
data in English and target languages generated by GPT-4. AVG shows average
resultsforlanguagesthatareavailableinallsettings(excl. QU,TH,TR).163
Bibliography
DavidIfeoluwaAdelanietal.(2021).MasakhaNER:NamedEntityRecognitionforAfrican
Languages.In:TransactionsoftheAssociationforComputationalLinguistics9.Ed.by
BrianRoarkandAniNenkova,pp.1116–1131.
Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark,
and Mirella Lapata (July 2023). Multilingual Summarization with Factual Consis-
tency Evaluation. In: Findings of the Association for Computational Linguistics: ACL
2023. Ed. by Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. Toronto,
Canada:AssociationforComputationalLinguistics,pp.3562–3591.
KabirAhujaetal.(Dec.2023).MEGA:MultilingualEvaluationofGenerativeAI.In:Pro-
ceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
Ed. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for
ComputationalLinguistics,pp.4232–4267.
TariqAlhindi,SavvasPetridis,andSmarandaMuresan(Nov.2018).WhereisYourEv-
idence: Improving Fact-checking by Justification Modeling. In: Proceedings of the First
WorkshoponFactExtractionandVERification(FEVER).Brussels,Belgium:Associ-
ationforComputationalLinguistics,pp.85–90.
Hunt Allcott and Matthew Gentzkow (May 2017). Social Media and Fake News in the
2016Election.In:vol.31.2,pp.211–36.
PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould(2016).SPICE:
Semantic Propositional Image Caption Evaluation. In: European Conference on Com-
puterVision(ECCV).Springer,pp.382–398.
PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,Stephen
Gould,andLeiZhang(2018).Bottom-upandTop-downAttentionforImageCaption-
ingandCisualQuestionAnswering.In:ProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pp.6077–6086.164 Bibliography
RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,Alexan-
dre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al.
(2023).PaLM2TechnicalReport.In:ArXiv.
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš,
Ivan Vulic´, and Anna Korhonen (Nov. 2021). MAD-G: Multilingual Adapter Gen-
erationforEfficientCross-LingualTransfer.In:FindingsoftheAssociationforCompu-
tational Linguistics: EMNLP 2021. Punta Cana, Dominican Republic: Association
forComputationalLinguistics,pp.4762–4781.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh (2015). Vqa: Visual Question Answering. In:
ProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.2425–2433.
MikelArtetxe,SebastianRuder,andDaniYogatama(July2020).OntheCross-lingual
Transferability of Monolingual Representations. In: Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics. Online: Association for
ComputationalLinguistics,pp.4623–4637.
Mikel Artetxe and Holger Schwenk (2019). Massively Multilingual Sentence Embed-
dingsforZero-ShotCross-LingualTransferandBeyond.In:TransactionsoftheAssoci-
ationforComputationalLinguistics7,pp.597–610.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi (2024).
Self-RAG:LearningtoRetrieve,Generate,andCritiquethroughSelf-Reflection.In:The
TwelfthInternationalConferenceonLearningRepresentations.
Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Gio-
vanniLucaCiampaglia,DavidCorney,ReneeDiResta,EmilioFerrara,ScottHale,
AlonHalevy,etal.(2023).FactualityChallengesintheEraofLargeLanguageModels.
In:ArXiv.
Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and An-
dreaPierleoni(July2022).ReFinED:AnEfficientZero-shot-capableApproachtoEnd-
to-End Entity Linking. In: Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies: Industry Track. Hybrid: Seattle, Washington + Online: Association for Com-
putationalLinguistics,pp.209–220.Bibliography 165
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton(2016).LayerNormalization.
In:ArXiv.
Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yun-
haiTong(Apr.2021).Syntax-BERT:ImprovingPre-trainedTransformerswithSyntax
Trees.In:Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociation
for Computational Linguistics: Main Volume. Ed. by Paola Merlo, Jorg Tiedemann,
andReutTsarfaty.Online:AssociationforComputationalLinguistics,pp.3011–
3020.
Kelsey Ball and Dan Garrette (Oct. 2018). Part-of-Speech Tagging for Code-Switched,
Transliterated Texts without Explicit Language Identification. In: Proceedings of the
2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Brussels,Bel-
gium:AssociationforComputationalLinguistics,pp.3084–3089.
Satanjeev Banerjee and Alon Lavie (June 2005). METEOR: An Automatic Metric for
MT Evaluation with Improved Correlation with Human Judgments. In: Proceedings of
theACLWorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTransla-
tion and/or Summarization. Ann Arbor, Michigan: Association for Computational
Linguistics,pp.65–72.
AnabMaulanaBarik,RahmadMahendra,andMirnaAdriani(Nov.2019).Normaliza-
tion of Indonesian-English Code-Mixed Twitter Data. In: Proceedings of the 5th Work-
shoponNoisyUser-generatedText(W-NUT2019).HongKong,China:Association
forComputationalLinguistics,pp.417–424.
LisaBauerandMohitBansal(Apr.2021).Identify,Align,andIntegrate:MatchingKnowl-
edgeGraphstoCommonsenseReasoningTasks.In:Proceedingsofthe16thConferenceof
the European Chapter of the Association for Computational Linguistics: Main Volume.
Ed. by Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty. Online: Association for
ComputationalLinguistics,pp.2259–2272.
EmilyM.Bender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell
(2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In:
Proceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency,
pp.610–623.
StellaBidermanetal.(July2023a).Pythia:ASuiteforAnalyzingLargeLanguageModels
Across Training and Scaling. In: Proceedings of the 40th International Conference on166 Bibliography
Machine Learning. Ed. by Andreas Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett. Vol. 202. Proceedings
ofMachineLearningResearch.PMLR,pp.2397–2430.
StellaBidermanetal.(July2023b).Pythia:ASuiteforAnalyzingLargeLanguageModels
Across Training and Scaling. In: Proceedings of the 40th International Conference on
Machine Learning. Ed. by Andreas Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett. Vol. 202. Proceedings
ofMachineLearningResearch.PMLR,pp.2397–2430.
AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,andOksana
Yakhnenko (2013). Translating Embeddings for Modeling Multi-relational Data. In:
Advances in Neural Information Processing Systems. Ed. by C.J. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K.Q. Weinberger. Vol. 26. Curran Associates,
Inc.
Sebastian Borgeaud et al. (July 2022). Improving Language Models by Retrieving from
Trillions of Tokens. In: Proceedings of the 39th International Conference on Machine
Learning. Ed. by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepes-
vari, Gang Niu, and Sivan Sabato. Vol. 162. Proceedings of Machine Learning
Research.PMLR,pp.2206–2240.
SamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning
(Sept.2015).ALargeAnnotatedCorpusforLearningNaturalLanguageInference.In:
Proceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLanguageProcess-
ing.Lisbon,Portugal:AssociationforComputationalLinguistics,pp.632–642.
FaezeBrahman,BaolinPeng,MichelGalley,SudhaRao,BillDolan,SnigdhaChaturvedi,
and Jianfeng Gao (Dec. 2022). Grounded Keys-to-Text Generation: Towards Factual
Open-Ended Generation. In: Findings of the Association for Computational Linguis-
tics: EMNLP 2022. Ed. by Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang.
Abu Dhabi, United Arab Emirates: Association for Computational Linguistics,
pp.7397–7413.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,Prafulla
Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,et
al. (2020a). Language Models are Few-shot Learners. In: Advances in neural informa-
tionprocessingsystems33,pp.1877–1901.Bibliography 167
TomBrownetal.(2020b).LanguageModelsareFew-ShotLearners.In:AdvancesinNeu-
ralInformationProcessingSystems.Vol.33.CurranAssociates,Inc.,pp.1877–1901.
OzanCaglayan,PranavaMadhyastha,andLuciaSpecia(Dec.2020).CuriousCaseof
LanguageGenerationEvaluationMetrics:ACautionaryTale.In:Proceedingsofthe28th
International Conference on Computational Linguistics. Barcelona, Spain (Online):
InternationalCommitteeonComputationalLinguistics,pp.2322–2328.
IacerCalixto,AlessandroRaganato,andTommasoPasini(June2021).WikipediaEn-
tities as Rendezvous across Languages: Grounding Multilingual Language Models by
Predicting Wikipedia Hyperlinks. In: Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies.Online:AssociationforComputationalLinguistics,pp.3651–3661.
Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue,
andJinXu(Aug.2021).KnowledgeableorEducatedGuess?RevisitingLanguageMod-
els as Knowledge Bases. In: Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers). Online: Association for Computa-
tionalLinguistics,pp.1860–1874.
Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni (2021). Autore-
gressiveEntityRetrieval.In:InternationalConferenceonLearningRepresentations.
ÖzlemÇetinog˘lu,SarahSchulz,andNgocThangVu(Nov.2016).ChallengesofCom-
putational Processing of Code-Switching. In: Proceedings of the Second Workshop on
Computational Approaches to Code Switching. Ed. by Mona Diab, Pascale Fung,
Mahmoud Ghoneim, Julia Hirschberg, and Thamar Solorio. Austin, Texas: As-
sociationforComputationalLinguistics,pp.1–11.
ArunChaganty,AshwinParanjape,PercyLiang,andChristopherD.Manning(Sept.
2017). Importance Sampling for Unbiased On-demand Evaluation of Knowledge Base
Population. In: Proceedings of the 2017 Conference on Empirical Methods in Natural
LanguageProcessing.Copenhagen,Denmark:AssociationforComputationalLin-
guistics,pp.1038–1048.168 Bibliography
Chakraborty,Tanmoy,Md.ShadAkhtar,KaiShu,H.RussellBernard,MariaLiakata,
Preslav Nakov, and Aseem Srivastava, eds. (May 2022). Proceedings of the Work-
shoponCombatingOnlineHostilePostsinRegionalLanguagesduringEmergencySit-
uations.Dublin,Ireland:AssociationforComputationalLinguistics.
Bharathi Raja Chakravarthi, Vigneshwaran Muralidaran, Ruba Priyadharshini, and
John Philip McCrae (May 2020). Corpus Creation for Sentiment Analysis in Code-
MixedTamil-EnglishText.English.In:Proceedingsofthe1stJointWorkshoponSpoken
Language Technologies for Under-resourced languages (SLTU) and Collaboration and
Computing for Under-Resourced Languages (CCURL). Marseille, France: European
LanguageResourcesassociation,pp.202–210.ISBN:979-10-95546-35-1.
Ting-YunChang,YangLiu,KarthikGopalakrishnan,BehnamHedayatnia,PeiZhou,
and Dilek Hakkani-Tur (Nov. 2020). Incorporating Commonsense Knowledge Graph
in Pretrained Models for Social Commonsense Tasks. In: Proceedings of Deep Learning
InsideOut(DeeLIO):TheFirstWorkshoponKnowledgeExtractionandIntegrationfor
DeepLearningArchitectures.Ed.byEnekoAgirre,MariannaApidianaki,andIvan
Vulic´.Online:AssociationforComputationalLinguistics,pp.74–79.
Xiaojun Chen, Shengbin Jia, and Yang Xiang (2020). A review: Knowledge Reasoning
Over Knowledge Graph. In: Expert Systems with Applications 141, p. 112948. ISSN:
0957-4174.
Yen-ChunChen,LinjieLi,LichengYu,AhmedElKholy,FaisalAhmed,ZheGan,Yu
Cheng, andJingjingLiu (2020).Uniter: Universal Image-TextRepresentation Learn-
ing.In:EuropeanConferenceonComputerVision(ECCV).Springer,pp.104–120.
AlexanderChernyavskiyandDmitryIlvovsky(2020).RecursiveNeuralTextClassifica-
tionUsingDiscourseTreeStructureforArgumentationMiningandSentimentAnalysis
Tasks.In:ISMIS.Springer,pp.90–101.
Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia
Song,Xian-LingMao,HeyanHuang,andMingZhou(June2021a).InfoXLM:An
Information-TheoreticFrameworkforCross-LingualLanguageModelPre-Training.In:
Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. Online: Association
forComputationalLinguistics,pp.3576–3588.Bibliography 169
Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang,
and Furu Wei (Aug. 2021b). Improving Pretrained Cross-Lingual Language Models
via Self-Labeled Word Alignment. In: Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers). Online: Association for
ComputationalLinguistics,pp.3418–3430.
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal (July 2021). Unifying Vision-and-
Language Tasks via Text Generation. In: Proceedings of the 38th International Confer-
ence on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Pro-
ceedingsofMachineLearningResearch.PMLR,pp.1931–1942.
AakankshaChowdheryetal.(2023).PaLM:ScalingLanguageModelingwithPathways.
In:JournalofMachineLearningResearch24.240,pp.1–113.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
YunxuanLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.(2022).
ScalingInstruction-FinetunedLanguageModels.In:ArXiv.
Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aha-
roni,VitalyNikolaev,ThibaultSellam,AdityaSiddhant,DipanjanDas,andAnkur
Parikh (Dec. 2023). SEAHORSE: A Multilingual, Multifaceted Dataset for Summa-
rization Evaluation. In: Proceedings of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing.Ed.byHoudaBouamor,JuanPino,andKalikaBali.
Singapore:AssociationforComputationalLinguistics,pp.9397–9413.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning (2020).
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In:
ICLR.
KevinClarkandChristopherD.Manning(Aug.2016).ImprovingCoreferenceResolu-
tion by Learning Entity-Level Distributed Representations. In: Proceedings of the 54th
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPa-
pers).Berlin,Germany:AssociationforComputationalLinguistics,pp.643–653.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-
laumeWenzek,FranciscoGuzmán,EdouardGrave,MyleOtt,LukeZettlemoyer,170 Bibliography
andVeselinStoyanov(July2020a).UnsupervisedCross-lingualRepresentationLearn-
ingatScale.In:Proceedingsofthe58thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics.Online:AssociationforComputationalLinguistics,pp.8440–
8451.
AlexisConneauandGuillaumeLample(2019).Cross-lingualLanguageModelPretrain-
ing. In: Advances in Neural Information Processing Systems. Vol. 32. Curran Asso-
ciates,Inc.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bow-
man,HolgerSchwenk,andVeselinStoyanov(Oct.2018).XNLI:EvaluatingCross-
lingual Sentence Representations. In: Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing. Brussels, Belgium: Association for Com-
putationalLinguistics,pp.2475–2485.
AlexisConneau,ShijieWu,HaoranLi,LukeZettlemoyer,andVeselinStoyanov(July
2020b).EmergingCross-lingualStructureinPretrainedLanguageModels.In:Proceed-
ingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics.On-
line:AssociationforComputationalLinguistics,pp.6022–6034.
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,
Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al.
(2022). No Language Left Behind: Scaling Human-Centered Machine Translation. In:
ArXiv.
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao,
Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al. (2023). AugGPT: Leveraging
ChatGPTforTextDataAugmentation.In:ArXiv.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (June 2019).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
In: Proceedings of the 2019 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers). Minneapolis, Minnesota: Association for Computational Lin-
guistics,pp.4171–4186.
Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, and Tong Zhang (July 2023).
Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-
trained Language Models’ Memories. In: Proceedings of the 61st Annual Meeting ofBibliography 171
the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Anna
Rogers,JordanBoyd-Graber,andNaoakiOkazaki.Toronto,Canada:Association
forComputationalLinguistics,pp.5113–5129.
Jia Ding,Yongjun Hu, andHuiyou Chang(2020). BERT-Based MentalModel, aBetter
Fake News Detector. In: Proceedings of the 6th International Conference on Comput-
ing and Artificial Intelligence (ICCAI). Tianjin, China: Association for Computing
Machinery,pp.396–400.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and
NoahSmith(2020).Fine-TuningPretrainedLanguageModels:WeightInitializations,
DataOrders,andEarlyStopping.In:ArXiv.
A. Seza Dog˘ruöz, Sunayana Sitaram, Barbara E. Bullock, and Almeida Jacqueline
Toribio (Aug. 2021). A Survey of Code-switching: Linguistic and Social Perspectives
for Language Technologies. In: Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th International Joint Conference on
NaturalLanguageProcessing(Volume1:LongPapers).Online:AssociationforCom-
putationalLinguistics,pp.1654–1666.
AlexeyDosovitskiyetal.(2021).AnImageisWorth16x16Words:TransformersforImage
RecognitionatScale.In:InternationalConferenceonLearningRepresentations.
Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos (Nov. 2019). Investigating Meta-
Learning Algorithms for Low-Resource Natural Language Understanding Tasks. In:
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguis-
tics,pp.1192–1197.
NanDuetal.(July2022).GLaM:EfficientScalingofLanguageModelswithMixture-of-
Experts.In:Proceedingsofthe39thInternationalConferenceonMachineLearning.Ed.
byKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,
and Sivan Sabato. Vol. 162. Proceedings of Machine Learning Research. PMLR,
pp.5547–5569.
Radhika Dua, Sai Srinivas Kancheti, and Vineeth N Balasubramanian (2021). Be-
yond vqa: Generating Multi-Word Answers and Rationales to Visual Questions. In:172 Bibliography
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.1623–1632.
Markus Eberts and Adrian Ulges (2020). Span-based Joint Entity and Relation Extrac-
tionwithTransformerPre-training.In:ECAI,pp.2006–2013.
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon
Hare, Frederique Laforest, and Elena Simperl (May 2018). T-REx: A Large Scale
Alignment of Natural Language with Knowledge Base Triples. In: Proceedings of the
EleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018).
Miyazaki,Japan:EuropeanLanguageResourcesAssociation(ELRA).
DenisEmelin,DanieleBonadiman,SawsanAlqahtani,YiZhang,andSaabMansour
(Dec. 2022). Injecting Domain Knowledge in Language Models for Task-oriented Dia-
logueSystems.In:Proceedingsofthe2022ConferenceonEmpiricalMethodsinNatural
Language Processing. Ed. by Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang.
Abu Dhabi, United Arab Emirates: Association for Computational Linguistics,
pp.11962–11974.
Mahmood Farokhian, Vahid Rafe, and Hadi Veisi (2023). Fake News Detection Using
Parallel BERT Deep Neural Networks. In: Multimedia Tools and Applications. ISSN:
1573-7721.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang
(May2022).Language-agnosticBERTSentenceEmbedding.In:Proceedingsofthe60th
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPa-
pers).Dublin,Ireland:AssociationforComputationalLinguistics,pp.878–891.
PaoloFerraginaandUgoScaiella(2010).TAGME:On-the-flyAnnotationofShortText
Fragments(byWikipediaEntities).In:Proceedingsofthe19thACMinternationalcon-
ferenceonInformationandKnowledgeManagement.
JuriGanitkevitch,BenjaminVanDurme,andChrisCallison-Burch(June2013).PPDB:
TheParaphraseDatabase.In:Proceedingsofthe2013ConferenceoftheNorthAmerican
Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies.Atlanta,Georgia:AssociationforComputationalLinguistics,pp.758–764.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen (Dec. 2023). Enabling Large
Language Models to Generate Text with Citations. In: Proceedings of the 2023 Confer-
enceonEmpiricalMethodsinNaturalLanguageProcessing.Ed.byHoudaBouamor,Bibliography 173
Juan Pino, and Kalika Bali. Singapore: Association for Computational Linguis-
tics,pp.6465–6488.
NicolasGontier,SivaReddy,andChristopherPal(2022).DoesEntityAbstractionHelp
GenerativeTransformersReason?In:TransactionsonMachineLearningResearch.
YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh(July
2017).MakingthevinVQAMatter:ElevatingtheRoleofImageUnderstandinginVi-
sualQuestionAnswering.In:ProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition(CVPR),pp.6904–6913.
Milan Gritta and Ignacio Iacobacci (Aug. 2021). XeroAlign: Zero-shot Cross-lingual
TransformerAlignment.In:FindingsoftheAssociationforComputationalLinguistics:
ACL-IJCNLP 2021. Online: Association for Computational Linguistics, pp. 371–
381.
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind
Tafjord,AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal.
(2024).Olmo:Acceleratingthescienceoflanguagemodels.In:arXivpreprintarXiv:2402.00838.
Adam Grycner and Gerhard Weikum (Nov. 2016). POLY: Mining Relational Para-
phrasesfromMultilingualSentences.In:Proceedingsofthe2016ConferenceonEmpir-
ical Methods in Natural Language Processing. Austin, Texas: Association for Com-
putationalLinguistics,pp.2183–2192.
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk,
andJianfengGao(July2022).KAT:AKnowledgeAugmentedTransformerforVision-
and-Language.In:Proceedingsofthe2022ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics:HumanLanguageTechnologies.Seattle,
UnitedStates:AssociationforComputationalLinguistics,pp.956–968.
JohnJGumperz(1977).TheSociolinguisticSignificanceofConversationalCode-switching.
In:RELCjournal8.2,pp.1–34.
DeepakGupta,AsifEkbal,andPushpakBhattacharyya(Nov.2020).ASemi-supervised
Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer
Learning.In:FindingsoftheAssociationforComputationalLinguistics:EMNLP2020.
Online:AssociationforComputationalLinguistics,pp.2267–2280.
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang(July
2020).RetrievalAugmentedLanguageModelPre-Training.In:Proceedingsofthe37th174 Bibliography
International Conference on Machine Learning. Ed. by Hal Daumé III and Aarti
Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR, pp. 3929–
3938.
Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong
Sun (Oct. 2018). FewRel: A Large-Scale Supervised Few-Shot Relation Classification
Dataset with State-of-the-Art Evaluation. In: Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing. Brussels, Belgium: Association
forComputationalLinguistics,pp.4803–4809.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun(June2016a).DeepResidual
Learning for Image Recognition. In: Proceedings of the IEEE Conference on Computer
VisionandPatternRecognition(CVPR),pp.770–778.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2016b). Deep Residual
Learning for Image Recognition. In: Proceedings of the IEEE conference on computer
visionandpatternrecognition,pp.770–778.
Benjamin Heinzerling and Kentaro Inui (Apr. 2021). Language Models as Knowledge
Bases:OnEntityRepresentations,StorageCapacity,andParaphrasedQueries.In:Pro-
ceedings of the 16th Conference of the European Chapter of the Association for Compu-
tational Linguistics: Main Volume. Ed. by Paola Merlo, Jorg Tiedemann, and Reut
Tsarfaty.Online:AssociationforComputationalLinguistics,pp.1772–1791.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi (Nov.
2021). CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In: Pro-
ceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
Online and Punta Cana, Dominican Republic: Association for Computational
Linguistics,pp.7514–7528.
SeppHochreiterandJürgenSchmidhuber(1997).LongShort-termMemory.In:Neural
computation9.8,pp.1735–1780.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi (2020). The Curious
Case of Neural Text Degeneration. In: International Conference on Learning Represen-
tations.
Benjamin Horne and Sibel Adali (May 2017). This Just In: Fake News Packs A Lot in
Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire Than RealBibliography 175
News. In: Proceedings of the International AAAI Conference on Web and Social Media
11.1,pp.759–766.
Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu, and Mrinmaya
Sachan (Dec. 2022). Adapters for Enhanced Modeling of Multilingual Knowledge and
Text.In:FindingsoftheAssociationforComputationalLinguistics:EMNLP2022.Ed.
byYoavGoldberg,ZornitsaKozareva,andYueZhang.AbuDhabi,UnitedArab
Emirates:AssociationforComputationalLinguistics,pp.3902–3917.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De
Laroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly(June2019).
Parameter-Efficient Transfer Learning for NLP. In: Proceedings of the 36th Interna-
tional Conference on Machine Learning. Ed. by Kamalika Chaudhuri and Ruslan
Salakhutdinov.Vol.97.ProceedingsofMachineLearningResearch.PMLR,pp.2790–
2799.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen (2022). LoRA: Low-Rank Adaptation of Large
LanguageModels.In:InternationalConferenceonLearningRepresentations.
JunjieHu,MelvinJohnson,OrhanFirat,AdityaSiddhant,andGrahamNeubig(June
2021). Explicit Alignment Objectives for Multilingual Bidirectional Encoders. In: Pro-
ceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. Online: Association for
ComputationalLinguistics,pp.3633–3643.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
MelvinJohnson(July2020).XTREME:AMassivelyMultilingualMulti-taskBench-
mark for Evaluating Cross-lingual Generalisation. In: Proceedings of the 37th Interna-
tionalConferenceonMachineLearning.Vol.119.ProceedingsofMachineLearning
Research.PMLR,pp.4411–4421.
Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin
(2023). LoraHub: Efficient Cross-task Generalization via Dynamic Lora Composition.
In:ArXiv.
Kuan-HaoHuang,WasiAhmad,NanyunPeng,andKai-WeiChang(Nov.2021).Im-
proving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. In: Proceed-
ings of the 2021 Conference on Empirical Methods in Natural Language Processing.176 Bibliography
Online and Punta Cana, Dominican Republic: Association for Computational
Linguistics,pp.1684–1697.
Pere-Lluís Huguet Cabot and Roberto Navigli (Nov. 2021). REBEL: Relation Extrac-
tion By End-to-end Language Generation. In: Findings of the Association for Compu-
tational Linguistics: EMNLP 2021. Punta Cana, Dominican Republic: Association
forComputationalLinguistics,pp.2370–2381.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo
Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave
(2023).Atlas:Few-shotLearningwithRetrievalAugmentedLanguageModels.In:Jour-
nalofMachineLearningResearch24.251,pp.1–43.
BinJi,JieYu,ShashaLi,JunMa,QingboWu,YusongTan,andHuijunLiu(Dec.2020).
Span-basedJointEntityandRelationExtractionwithAttention-basedSpan-specificand
ContextualSemanticRepresentations.In:Proceedingsofthe28thInternationalConfer-
ence on Computational Linguistics. Barcelona, Spain (Online): International Com-
mitteeonComputationalLinguistics,pp.88–99.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig (2021). Scaling Up Visual and Vision-
LanguageRepresentationLearningwithNoisyTextSupervision.In:Internationalcon-
ferenceonmachinelearning.PMLR,pp.4904–4916.
Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan,
Jana Diesner, and Jianfeng Gao (Nov. 2019). TIGEr: Text-to-Image Grounding for
Image Caption Evaluation. In: Proceedings of the 2019 Conference on Empirical Meth-
odsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNat-
ural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for
ComputationalLinguistics,pp.2141–2152.
Xiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan Duan (June 2022). XLM-K: Im-
provingCross-LingualLanguageModelPre-trainingwithMultilingualKnowledge.In:
Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 10, pp. 10840–
10848.
Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham
Neubig (Nov. 2020). X-FACTR: Multilingual Factual Knowledge Retrieval from Pre-
trainedLanguageModels.In:Proceedingsofthe2020ConferenceonEmpiricalMethodsBibliography 177
in Natural Language Processing (EMNLP). Online: Association for Computational
Linguistics,pp.5943–5959.
JustinJohnson,BharathHariharan,LaurensvanderMaaten,LiFei-Fei,C.Lawrence
Zitnick, and Ross Girshick (July 2017). CLEVR: A Diagnostic Dataset for Composi-
tionalLanguageandElementaryVisualReasoning.In:ProceedingsoftheIEEEConfer-
enceonComputerVisionandPatternRecognition(CVPR).
MelvinJohnsonetal.(2017).Google’sMultilingualNeuralMachineTranslationSystem:
Enabling Zero-Shot Translation. In: Transactions of the Association for Computational
Linguistics5,pp.339–351.
Mandar Joshi, Kenton Lee, Yi Luan, and Kristina Toutanova (2020). Contextualized
RepresentationsUsingTextualEncyclopedicKnowledge.In:ArXiv.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury
(July2020).TheStateandFateofLinguisticDiversityandInclusionintheNLPWorld.
In: Proceedings of the 58th Annual Meeting of the Association for Computational Lin-
guistics.Online:AssociationforComputationalLinguistics,pp.6282–6293.
Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West
(July 2022). GenIE: Generative Information Extraction. In: Proceedings of the 2022
Conference of the North American Chapter of the Association for Computational Lin-
guistics:HumanLanguageTechnologies.Seattle,UnitedStates:AssociationforCom-
putationalLinguistics,pp.4626–4643.
SauravKadavath,TomConerly,AmandaAskell,TomHenighan,DawnDrain,Ethan
Perez,NicholasSchiefer,ZacHatfield-Dodds,NovaDasSarma,EliTran-Johnson,
etal.(2022).LanguageModels(Mostly)KnowWhatTheyKnow.In:ArXiv.
Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang (2021). FakeBERT: Fake
NewsDetectioninSocialMediawithaBERT-basedDeepLearningApproach.In:Mul-
timediatoolsandapplications80.8,pp.11765–11788.
MaximeKayser,Oana-MariaCamburu,LeonardSalewski,CorneliusEmde,Virginie
Do, Zeynep Akata, and Thomas Lukasiewicz (Oct. 2021). E-ViL: A Dataset and
Benchmark for Natural Language Explanations in Vision-Language Tasks. In: Proceed-
ingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),pp.1244–
1254.178 Bibliography
UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis
(2020).GeneralizationthroughMemorization:NearestNeighborLanguageModels.In:
InternationalConferenceonLearningRepresentations.
Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and
Monojit Choudhury (July 2020). GLUECoS: An Evaluation Benchmark for Code-
SwitchedNLP.In:Proceedingsofthe58thAnnualMeetingoftheAssociationforCom-
putationalLinguistics.Online:AssociationforComputationalLinguistics,pp.3575–
3585.
Jitin Krishnan, Antonios Anastasopoulos, Hemant Purohit, and Huzefa Rangwala
(Nov. 2021). Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Predic-
tion and Slot Filling. In: Proceedings of the 1st Workshop on Multilingual Representa-
tion Learning. Punta Cana, Dominican Republic: Association for Computational
Linguistics,pp.211–223.
Shanu Kumar, Sandipan Dandapat, and Monojit Choudhury (July 2022). ”Diversity
andUncertaintyinModeration”aretheKeytoDataSelectionforMultilingualFew-shot
Transfer.In:FindingsoftheAssociationforComputationalLinguistics:NAACL2022.
Seattle,UnitedStates:AssociationforComputationalLinguistics,pp.1042–1055.
TomKwiatkowskietal.(2019).NaturalQuestions:ABenchmarkforQuestionAnswering
Research.In:TransactionsoftheAssociationforComputationalLinguistics7,pp.452–
466.
TuanManhLai,ChengXiangZhai,andHengJi(2023).KEBLM:Knowledge-Enhanced
Biomedical Language Models. In: Journal of Biomedical Informatics 143, p. 104392.
ISSN:1532-0464.
ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,
andRaduSoricut(2020).ALBERT:ALiteBERTforSelf-supervisedLearningofLan-
guageRepresentations.In:InternationalConferenceonLearningRepresentations.
Anne Lauscher, Vinit Ravishankar, Ivan Vulic´, and Goran Glavaš (Nov. 2020). From
Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual
Transformers.In:Proceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
Language Processing (EMNLP). Online: Association for Computational Linguis-
tics,pp.4483–4499.Bibliography 179
Grandee Lee, Xianghu Yue, and Haizhou Li (2019). Linguistically Motivated Parallel
Data Augmentation for Code-Switch Language Modeling. In: Proc. Interspeech 2019,
pp.3730–3734.
Kuang-HueiLee,XiChen,GangHua,HoudongHu,andXiaodongHe(2018).Stacked
CrossAttentionforImage-textMatching.In:ProceedingsoftheEuropeanconferenceon
computervision(ECCV),pp.201–216.
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad
Shoeybi, and Bryan Catanzaro (2022). Factuality Enhanced Language Models for
Open-EndedTextGeneration.In:AdvancesinNeuralInformationProcessingSystems.
Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh.
Vol.35.CurranAssociates,Inc.,pp.34586–34599.
HectorLevesque,ErnestDavis,andLeoraMorgenstern(2012).TheWinogradSchema
Challenge.In:Thirteenthinternationalconferenceontheprinciplesofknowledgerepre-
sentationandreasoning.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-
hamed,OmerLevy,VeselinStoyanov,andLukeZettlemoyer(July2020).BART:
DenoisingSequence-to-SequencePre-trainingforNaturalLanguageGeneration,Trans-
lation,andComprehension.In:Proceedingsofthe58thAnnualMeetingoftheAssocia-
tionforComputationalLinguistics.Online:AssociationforComputationalLinguis-
tics,pp.7871–7880.
PatrickLewisetal.(2020).Retrieval-AugmentedGenerationforKnowledge-IntensiveNLP
Tasks.In:AdvancesinNeuralInformationProcessingSystems.Ed.byH.Larochelle,
M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin.Vol.33.CurranAssociates,Inc.,
pp.9459–9474.
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar
Mehdad(Apr.2021).MTOP:AComprehensiveMultilingualTask-OrientedSemantic
Parsing Benchmark. In: Proceedings of the 16th Conference of the European Chapter of
theAssociationforComputationalLinguistics:MainVolume.Online:Associationfor
ComputationalLinguistics,pp.2950–2962.
LiunianHaroldLi,MarkYatskar,DaYin,Cho-JuiHsieh,andKai-WeiChang(2019).
VisualBERT:ASimpleandPerformantBaselineForVisionandLanguage.In:ArXiv.180 Bibliography
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al. (2020). Oscar: Object-Semantics
AlignedPre-trainingforVision-LanguageTasks.In:EuropeanConferenceonComputer
Vision(ECCV).Springer,pp.121–137.
Chin-YewLinandFranzJosefOch(July2004).AutomaticEvaluationofMachineTrans-
lation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In:
Proceedingsofthe42ndAnnualMeetingoftheAssociationforComputationalLinguis-
tics(ACL-04).Barcelona,Spain,pp.605–612.
Chu-Cheng Lin, Xinyi Wang, Jonathan H. Clark, Yun Zhu, Han Lu, Chenxi White-
house,andHongkunYu(2024).MultitaskMultilingualModelAdaptationwithFea-
turizedLow-RankMixtures.In:ArXiv.
Yu-Hsiang Lin et al. (July 2019). Choosing Transfer Languages for Cross-Lingual Learn-
ing. In: Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics.Florence,Italy:AssociationforComputationalLinguistics,pp.3125–
3135.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra-
manan, Piotr Dollár, and C Lawrence Zitnick (2014). Microsoft Coco: Common
Objects in Context. In: European Conference on Computer Vision (ECCV). Springer,
pp.740–755.
Xi Victoria Lin et al. (Dec. 2022). Few-shot Learning with Multilingual Generative Lan-
guageModels.In:Proceedingsofthe2022ConferenceonEmpiricalMethodsinNatural
LanguageProcessing.AbuDhabi,UnitedArabEmirates:AssociationforCompu-
tationalLinguistics,pp.9019–9052.
YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu(Feb.2015).Learn-
ingEntityandRelationEmbeddingsforKnowledgeGraphCompletion.In:Proceedings
oftheAAAIConferenceonArtificialIntelligence29.1.
Chao Liu, Xinghua Wu, Min Yu, Gang Li, Jianguo Jiang, Weiqing Huang, and Xi-
angLu(2019).ATwo-StageModelBasedonBERTforShortFakeNewsDetection.In:
KnowledgeScience,EngineeringandManagement.Ed.byChristosDouligeris,Dim-
itrisKaragiannis,andDimitrisApostolou.Cham:SpringerInternationalPublish-
ing,pp.172–183.Bibliography 181
Haokun Liu, William Huang, Dhara Mungra, and Samuel R. Bowman (Nov. 2020).
PreciseTaskFormalizationMattersinWinogradSchemaEvaluations.In:Proceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).
Online:AssociationforComputationalLinguistics,pp.8275–8280.
JiongnanLiu,JiajieJin,ZihanWang,JiehanCheng,ZhichengDou,andJi-RongWen
(2023).RETA-LLM:ARetrieval-AugmentedLargeLanguageModelToolkit.In:ArXiv.
LinlinLiu,BoshengDing,LidongBing,ShafiqJoty,LuoSi,andChunyanMiao(Aug.
2021).MulDA:AMultilingualDataAugmentationFrameworkforLow-ResourceCross-
Lingual NER. In: Proceedings of the 59th Annual Meeting of the Association for Com-
putationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguage
Processing (Volume 1: Long Papers). Online: Association for Computational Lin-
guistics,pp.5834–5846.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping
Wang(Apr.2020).K-BERT:EnablingLanguageRepresentationwithKnowledgeGraph.
In:34.03,pp.2901–2908.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov (2019). RoBERTa: A
RobustlyOptimizedBERTPretrainingApproach.In:ArXiv.
Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh
(July2019).Barack’sWifeHillary:UsingKnowledgeGraphsforFact-AwareLanguage
Modeling. In: Proceedings of the 57th Annual Meeting of the Association for Compu-
tational Linguistics. Florence, Italy: Association for Computational Linguistics,
pp.5962–5971.
Ilya Loshchilov and Frank Hutter (2019). Decoupled Weight Decay Regularization. In:
InternationalConferenceonLearningRepresentations.
Holy Lovenia, Samuel Cahyawijaya, Genta Indra Winata, Peng Xu, Xu Yan, Zihan
Liu, Rita Frieske, Tiezheng Yu, Wenliang Dai, Elham J Barezi, et al. (2022). AS-
CEND:ASpontaneousChinese-EnglishDatasetforCode-switchinginMulti-turnCon-
versation. In: Proceedings of the 13th Language Resources and Evaluation Conference
(LREC).182 Bibliography
JiasenLu,DhruvBatra,DeviParikh,andStefanLee(2019).Vilbert:PretrainingTask-
Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In: Advances
inneuralinformationprocessingsystems32.
Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, and
Luo Si (Aug. 2021). VECO: Variable and Flexible Cross-lingual Pre-training for Lan-
guage Understanding and Generation. In: Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Papers). Online: Association
forComputationalLinguistics,pp.3980–3994.
Dau-ChengLyu,Tien-PingTan,Eng-SiongChng,andHaizhouLi(2015).Mandarin–
English code-switching speech corpus in South-East Asia: SEAME. In: Language Re-
sourcesandEvaluation49.3,pp.581–600. ISSN:1574-0218.
XinbeiMa,YeyunGong,PengchengHe,HaiZhao,andNanDuan(Dec.2023).Query
RewritinginRetrieval-AugmentedLargeLanguageModels.In:Proceedingsofthe2023
Conference on Empirical Methods in Natural Language Processing. Ed. by Houda
Bouamor,JuanPino,andKalikaBali.Singapore:AssociationforComputational
Linguistics,pp.5303–5315.
Yubo Ma, Yixin Cao, Yong Hong, and Aixin Sun (Dec. 2023). Large Language Model
IsNotaGoodFew-shotInformationExtractor,butaGoodRerankerforHardSamples!
In: Findings of the Association for Computational Linguistics: EMNLP 2023. Ed. by
HoudaBouamor,JuanPino,andKalikaBali.Singapore:AssociationforCompu-
tationalLinguistics,pp.10572–10601.
PranavaMadhyastha,JosiahWang,andLuciaSpecia(July2019).VIFIDEL:Evaluat-
ingtheVisualFidelityofImageDescriptions.In:Proceedingsofthe57thAnnualMeet-
ing of the Association for Computational Linguistics. Florence, Italy: Association for
ComputationalLinguistics,pp.6539–6550.
KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi(2019).
OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge.
In: Proceedings of the IEEE/cvf conference on computer vision and pattern recognition,
pp.3195–3204.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald (July 2020).
On Faithfulness and Factuality in Abstractive Summarization. In: Proceedings of theBibliography 183
58th Annual Meeting of the Association for Computational Linguistics. Ed. by Dan
Jurafsky,JoyceChai,NatalieSchluter,andJoelTetreault.Online:Associationfor
ComputationalLinguistics,pp.1906–1919.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell (2023). Locally Typ-
ical Sampling. In: Transactions of the Association for Computational Linguistics 11,
pp.102–121.
FilipeMesquita,MatteoCannaviccio,JordanSchmidek,ParamitaMirza,andDenil-
son Barbosa (Nov. 2019). KnowledgeNet: A Benchmark Dataset for Knowledge Base
Population. In: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language
Processing(EMNLP-IJCNLP).HongKong,China:AssociationforComputational
Linguistics,pp.749–758.
George A. Miller (1993). WORDNET: A Lexical Database for English. In: Human Lan-
guage Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March
21-24,1993.
MikeMintz,StevenBills,RionSnow,andDanielJurafsky(Aug.2009).DistantSuper-
visionforRelationExtractionwithoutLabeledData.In:ProceedingsoftheJointConfer-
enceofthe47thAnnualMeetingoftheACLandthe4thInternationalJointConference
on Natural Language Processing of the AFNLP. Suntec, Singapore: Association for
ComputationalLinguistics,pp.1003–1011.
Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M
Bronstein(2019).FakeNewsDetectiononSocialMediaUsingGeometricDeepLearn-
ing.In:ArXiv.
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Ba-
tra,LucyVanderwende,PushmeetKohli,andJamesAllen(June2016).ACorpus
and Cloze Evaluation for Deeper Understanding of Commonsense Stories. In: Proceed-
ingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociationforCom-
putational Linguistics: Human Language Technologies. San Diego, California: Asso-
ciationforComputationalLinguistics,pp.839–849.184 Bibliography
Niklas Muennighoff et al. (July 2023). Crosslingual Generalization through Multitask
Finetuning. In: Proceedings of the 61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for
ComputationalLinguistics,pp.15991–16111.
ShashiNarayan,GonçaloSimões,YaoZhao,JoshuaMaynez,DipanjanDas,Michael
Collins,andMirellaLapata(May2022).AWell-ComposedTextisHalfDone!Com-
position Sampling for Diverse Conditional Generation. In: Proceedings of the 60th An-
nualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).
Dublin,Ireland:AssociationforComputationalLinguistics,pp.1319–1339.
Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, and
RyanMcDonald(2021).PlanningwithLearnedEntityPromptsforAbstractiveSum-
marization.In:TransactionsoftheAssociationforComputationalLinguistics9,pp.1475–
1492.
RobertoNavigliandSimonePaoloPonzetto(2012).BabelNet:TheAutomaticConstruc-
tion, Evaluation and Application of A Wide-coverage Multilingual Semantic Network.
In:ArtificialIntelligence193,pp.217–250.ISSN:0004-3702.
Tapas Nayak and Hwee Tou Ng (Apr. 2020). Effective Modeling of Encoder-Decoder
ArchitectureforJointEntityandRelationExtraction.In:ProceedingsoftheAAAICon-
ferenceonArtificialIntelligence34.05,pp.8528–8535.
Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and
OmriAbend(July2023).DisentQA:DisentanglingParametricandContextualKnowl-
edgewithCounterfactualQuestionAnswering.In:Proceedingsofthe61stAnnualMeet-
ing of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. Toronto, Canada: As-
sociationforComputationalLinguistics,pp.10056–10070.
FarhadNooralahzadeh,GiannisBekoulis,JohannesBjerva,andIsabelleAugenstein
(Nov.2020).Zero-ShotCross-LingualTransferwithMetaLearning.In:Proceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).
Online:AssociationforComputationalLinguistics,pp.4547–4562.
Jeppe Nørregaard, Benjamin D. Horne, and Sibel Adali (July 2019). NELA-GT-2018:
ALargeMulti-LabelledNewsDatasetfortheStudyofMisinformationinNewsArticles.Bibliography 185
In: Proceedings of the International AAAI Conference on Web and Social Media 13.01,
pp.630–638.
CathalO’ConnorandMichelleMurphy(2020).GoingViral:DoctorsMustTackleFake
NewsintheCovid-19Pandemic.In:Bmj369.10.1136.
Robert Ormandi, Mohammad Saleh, Erin Winter, and Vinay Rao (2021). Webred: Ef-
fectivePretrainingandFinetuningforRelationExtractionontheWeb.In:ArXiv.
Ray Oshikawa, Jing Qian, and William Yang Wang (May 2020). A Survey on Natural
Language Processing for Fake News Detection. English. In: Proceedings of the Twelfth
LanguageResourcesandEvaluationConference.Ed.byNicolettaCalzolarietal.Mar-
seille, France: European Language Resources Association, pp. 6086–6093. ISBN:
979-10-95546-34-4.
LongOuyangetal.(2022).TrainingLanguageModelstoFollowInstructionswithHuman
Feedback. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo,
S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh.Vol.35.CurranAsso-
ciates,Inc.,pp.27730–27744.
XuanOuyang,ShuohuanWang,ChaoPang,YuSun,HaoTian,HuaWu,andHaifeng
Wang (Nov. 2021). ERNIE-M: Enhanced Multilingual Representation by Aligning
Cross-lingualSemanticswithMonolingualCorpora.In:Proceedingsofthe2021Confer-
enceonEmpiricalMethodsinNaturalLanguageProcessing.OnlineandPuntaCana,
DominicanRepublic:AssociationforComputationalLinguistics,pp.27–38.
Jeff Z Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen,
Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissan-
drini,etal.(2023).LargeLanguageModelsandKnowledgeGraphs:Opportunitiesand
Challenges.In:ArXiv.
XiaomanPan,BoliangZhang,JonathanMay,JoelNothman,KevinKnight,andHeng
Ji (July 2017). Cross-lingual Name Tagging and Linking for 282 Languages. In: Pro-
ceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume1:LongPapers).Vancouver,Canada:AssociationforComputationalLin-
guistics,pp.1946–1958.186 Bibliography
DongHukPark,LisaAnneHendricks,ZeynepAkata,AnnaRohrbach,BerntSchiele,
TrevorDarrell,andMarcusRohrbach(June2018).MultimodalExplanations:Justi-
fying Decisions and Pointing to the Evidence. In: Proceedings of the IEEE Conference
onComputerVisionandPatternRecognition(CVPR),pp.8779–8788.
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez (2023). Gorilla:
LargelanguageModelConnectedwithMassiveAPIs.In:ArXiv.
MatthewE.Peters,MarkNeumann,RobertLogan,RoySchwartz,VidurJoshi,Sameer
Singh,andNoahA.Smith(Nov.2019).KnowledgeEnhancedContextualWordRep-
resentations. In: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language
Processing(EMNLP-IJCNLP).HongKong,China:AssociationforComputational
Linguistics,pp.43–54.
FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin,Yux-
iangWu,andAlexanderMiller(Nov.2019).LanguageModelsasKnowledgeBases?
In:Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguagePro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguis-
tics,pp.2463–2473.
Fabio Petroni et al. (June 2021). KILT: a Benchmark for Knowledge Intensive Language
Tasks.In:Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAs-
sociation for Computational Linguistics: Human Language Technologies. Online: As-
sociationforComputationalLinguistics,pp.2523–2544.
JonasPfeiffer,IvanVulic´,IrynaGurevych,andSebastianRuder(Nov.2020).MAD-X:
AnAdapter-BasedFrameworkforMulti-TaskCross-LingualTransfer.In:Proceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).
Online:AssociationforComputationalLinguistics,pp.7654–7673.
MohammadTaherPilehvarandJoseCamacho-Collados(June2019).WiC:theWord-
in-ContextDatasetforEvaluatingContext-SensitiveMeaningRepresentations.In:Pro-
ceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1. Minneapolis,
Minnesota:AssociationforComputationalLinguistics,pp.1267–1273.Bibliography 187
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic´, and
Anna Korhonen (Nov. 2020). XCOPA: A Multilingual Dataset for Causal Common-
senseReasoning.In:Proceedingsofthe2020ConferenceonEmpiricalMethodsinNat-
ural Language Processing (EMNLP). Online: Association for Computational Lin-
guistics,pp.2362–2376.
Edoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak, Ivan Vulic´, Roi Reichart,
Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen (Sept. 2019). Modeling
Language Variation and Universals: A Survey on Typological Linguistics for Natural
LanguageProcessing.In:ComputationalLinguistics45.3,pp.559–601.
Archiki Prasad, Mohammad Ali Rehan, Shreya Pathak, and Preethi Jyothi (Nov.
2021).TheEffectivenessofIntermediate-TaskTrainingforCode-SwitchedNaturalLan-
guageUnderstanding.In:Proceedingsofthe1stWorkshoponMultilingualRepresenta-
tion Learning. Punta Cana, Dominican Republic: Association for Computational
Linguistics,pp.176–190.
Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan
Dandapat, and Kalika Bali (July 2018). Language Modeling for Code-Mixing: The
Role of Linguistic Theory based Synthetic Data. In: Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Melbourne,Australia:AssociationforComputationalLinguistics,pp.1543–1553.
Ruba Priyadharshini, Bharathi Raja Chakravarthi, Mani Vegupatti, and John P. Mc-
Crae (2020). Named Entity Recognition for Code-Mixed Indian Corpus using Meta
Embedding. In: 2020 6th International Conference on Advanced Computing and Com-
municationSystems(ICACCS),pp.68–72.
LiboQin,MinhengNi,YueZhang,andWanxiangChe(July2020).CoSDA-ML:Multi-
LingualCode-SwitchingDataAugmentationforZero-ShotCross-LingualNLP.In:Pro-
ceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
IJCAI-20.InternationalJointConferencesonArtificialIntelligenceOrganization,
pp.3853–3860.
AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever
(2019). Language Models are Unsupervised Multitask Learners. In: OpenAI Technical
Report.188 Bibliography
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,Michael
Matena,YanqiZhou,WeiLi,andPeterJ.Liu(2020a).ExploringtheLimitsofTrans-
fer Learning with a Unified Text-to-Text Transformer. In: Journal of Machine Learning
Research21.140,pp.1–67.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,Michael
Matena,YanqiZhou,WeiLi,andPeterJ.Liu(2020b).ExploringtheLimitsofTrans-
fer Learning with a Unified Text-to-Text Transformer. In: Journal of Machine Learning
Research21.140,pp.1–67.
Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, and Mohammad
Taher Pilehvar (Nov. 2020). XL-WiC: A Multilingual Benchmark for Evaluating Se-
manticContextualization.In:Proceedingsofthe2020ConferenceonEmpiricalMethods
in Natural Language Processing (EMNLP). Online: Association for Computational
Linguistics,pp.7193–7206.
HannahRashkin,DavidReitter,GauravSinghTomar,andDipanjanDas(Aug.2021).
Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features.
In: Proceedings of the 59th Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Natural Language Processing
(Volume1:LongPapers).Ed.byChengqingZong,FeiXia,WenjieLi,andRoberto
Navigli.Online:AssociationforComputationalLinguistics,pp.704–718.
RyokanRi,IkuyaYamada,andYoshimasaTsuruoka(May2022).mLUKE:ThePower
of Entity Representations in Multilingual Pretrained Language Models. In: Proceed-
ingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Vol-
ume 1: Long Papers). Dublin, Ireland: Association for Computational Linguistics,
pp.7316–7330.
Sebastian Riedel, Limin Yao, and Andrew McCallum (2010). Modeling Relations and
Their Mentions without Labeled Text. In: Machine Learning and Knowledge Discov-
ery in Databases. Ed. by José Luis Balcázar, Francesco Bonchi, Aristides Gionis,
andMichèleSebag.Berlin,Heidelberg:SpringerBerlinHeidelberg,pp.148–163.
ISBN:978-3-642-15939-8.Bibliography 189
Arra’Di Nur Rizal and Sara Stymne (May 2020). Evaluating Word Embeddings for
Indonesian–English Code-Mixed Text Based on Synthetic Data. English. In: Proceed-
ings of the The 4th Workshop on Computational Approaches to Code Switching. Mar-
seille, France: European Language Resources Association, pp. 26–35. ISBN: 979-
10-95546-66-5.
MohdSanadZakiRizvi,AnirudhSrinivasan,TanujaGanu,MonojitChoudhury,and
SunayanaSitaram(Apr.2021).GCM:AToolkitforGeneratingSyntheticCode-mixed
Text. In: Proceedings of the 16th Conference of the European Chapter of the Associa-
tion for Computational Linguistics: System Demonstrations. Online: Association for
ComputationalLinguistics,pp.205–211.
Adam Roberts, Colin Raffel, and Noam Shazeer (Nov. 2020). How Much Knowledge
Can You Pack Into the Parameters of a Language Model? In: Proceedings of the 2020
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Online:
AssociationforComputationalLinguistics,pp.5418–5426.
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon (2011). Choice of
Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In: AAAI
springsymposium:logicalformalizationsofcommonsensereasoning,pp.90–95.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky (2020). A Primer in BERTology:
WhatWeKnowAboutHowBERTWorks.In:TransactionsoftheAssociationforCom-
putationalLinguistics8,pp.842–866.
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal,
Hugo Larochelle, Aaron Courville, and Bernt Schiele (2017). Movie Description.
In:InternationalJournalofComputerVision123.1,pp.94–120.
GaetanoRossiello,Md.FaisalMahbubChowdhury,NandanaMihindukulasooriya,
OwenCornec,andAlfioGliozzo(2023).KnowGL:KnowledgeGenerationandLink-
ingfromText.In:ProceedingsoftheAAAIConferenceonArtificialIntelligence.
Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell (June 2016). Fake
News or Truth? Using Satirical Cues to Detect Potentially Misleading News. In: Pro-
ceedingsoftheSecondWorkshoponComputationalApproachestoDeceptionDetection.
Ed. by Tommaso Fornaciari, Eileen Fitzpatrick, and Joan Bachenko. San Diego,
California:AssociationforComputationalLinguistics,pp.7–17.190 Bibliography
NataliRuchansky,SungyongSeo,andYanLiu(2017).Csi:Ahybriddeepmodelforfake
news detection. In: Proceedings of the 2017 ACM on Conference on Information and
KnowledgeManagement,pp.797–806.
Leonard Salewski, A Koepke, Hendrik Lensch, and Zeynep Akata (2022). CLEVR-
X: A Visual Reasoning Dataset for Natural Language Explanations. In: International
WorkshoponExtendingExplainableAIBeyondDeepModelsandClassifiers.Springer,
pp.69–88.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
Gallé, et al. (2022). Bloom: A 176b-Parameter Open-Access Multilingual Language
Model.In:ArXiv.
TalSchuster,RoeiSchuster,DarshJ.Shah,andReginaBarzilay(June2020).TheLim-
itationsofStylometryforDetectingMachine-GeneratedFakeNews.In:Computational
Linguistics46.2,pp.499–510.
DustinSchwenk,ApoorvKhandelwal,ChristopherClark,KennethMarino,andRoozbeh
Mottaghi (2022). A-OKVQA: A Benchmark for Visual Question Answering Using
WorldKnowledge.In:EuropeanConferenceonComputerVision,pp.146–162.
Alessandro Seganti, Klaudia Firla˛g, Helena Skowronska, Michał Satława, and Piotr
Andruszkiewicz (Apr. 2021). Multilingual Entity and Relation Extraction Dataset
and Model. In: Proceedings of the 16th Conference of the European Chapter of the As-
sociationforComputationalLinguistics:MainVolume.Online:AssociationforCom-
putationalLinguistics,pp.1946–1955.
Rico Sennrich, Barry Haddow, and Alexandra Birch (Aug. 2016). Neural Machine
Translation of Rare Words with Subword Units. In: Proceedings of the 54th Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).Ed.
by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computa-
tionalLinguistics,pp.1715–1725.
Karishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan
Liu (May 2019a). Combating Fake News: A Survey on Identification and Mitigation
Techniques.In:10.3,pp.1–42.ISSN:2157-6904.Bibliography 191
Karishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan
Liu (Apr. 2019b). Combating Fake News: A Survey on Identification and Mitigation
Techniques.In:ACMTrans.Intell.Syst.Technol.10.3.ISSN:2157-6904.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani (June 2018). Self-Attention with
Relative Position Representations. In: Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies,Volume2(ShortPapers).Ed.byMarilynWalker,HengJi,andAmanda
Stent.NewOrleans,Louisiana:AssociationforComputationalLinguistics,pp.464–
468.
ShengShenetal.(2024).Mixture-of-ExpertsMeetsInstructionTuning:AWinningCom-
binationforLargeLanguageModels.In:TheTwelfthInternationalConferenceonLearn-
ingRepresentations.
KaiShu,SuhangWang,andHuanLiu(2019).BeyondNewsContents:TheRoleofSocial
Context for Fake News Detection. In: Proceedings of the Twelfth ACM International
ConferenceonWebSearchandDataMining.WSDM’19.MelbourneVIC,Australia:
AssociationforComputingMachinery,pp.312–320.ISBN:9781450359405.
Renato M Silva and Tiago A Almeida (2021). How Concept Drift can Impair the Clas-
sification of Fake News. In: Anais do IX Symposium on Knowledge Discovery, Mining
andLearning.SBC,pp.121–128.
LivioSoares,DanielGillick,JeremyCole,andTomKwiatkowski(Dec.2023).NAIL:
Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. In: Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing. Ed. by
HoudaBouamor,JuanPino,andKalikaBali.Singapore:AssociationforCompu-
tationalLinguistics,pp.2574–2589.
SalehSoltan,ShankarAnanthakrishnan,JackFitzGerald,RahulGupta,WaelHamza,
HaidarKhan,CharithPeris,StephenRawls,AndyRosenbaum,AnnaRumshisky,
etal.(2022).AlexaTM20b:Few-shotLearningUsingaLarge-ScaleMultilingualSeq2seq
Model.In:ArXiv.
Anirudh Srinivasan, Gauri Kholkar, Rahul Kejriwal, Tanuja Ganu, Sandipan Dan-
dapat, Sunayana Sitaram, Balakrishnan Santhanam, Somak Aditya, Kalika Bali,192 Bibliography
andMonojitChoudhury(June2022).LITMUSPredictor:AnAIAssistantforBuild-
ing Reliable, High-Performing and Fair Multilingual NLP Systems. In: Proceedings of
theAAAIConferenceonArtificialIntelligence36.11,pp.13227–13229.
Vivek Srivastava and Mayank Singh (Nov. 2020). PHINC: A Parallel Hinglish Social
MediaCode-MixedCorpusforMachineTranslation.In:ProceedingsoftheSixthWork-
shoponNoisyUser-generatedText(W-NUT2020).Online:AssociationforCompu-
tationalLinguistics,pp.41–49.
George Stoica, Emmanouil Antonios Platanios, and Barnabas Poczos (May 2021).
Re-TACRED:AddressingShortcomingsoftheTACREDDataset.In:Proceedingsofthe
AAAIConferenceonArtificialIntelligence35.15,pp.13843–13850.
Jinyan Su, Claire Cardie, and Preslav Nakov (2023). Adapting Fake News Detection to
theEraofLargeLanguageModels.In:ArXiv.
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai (2020).
VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In: International
ConferenceonLearningRepresentations.
YuSun,ShuohuanWang,YukunLi,ShikunFeng,XuyiChen,HanZhang,XinTian,
Danxiang Zhu, Hao Tian, and Hua Wu (2019). ERNIE: Enhanced Representation
throughKnowledgeIntegration.In:ArXiv.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le (2014). Sequence to Sequence Learning
with Neural Networks. In: Advances in Neural Information Processing Systems. Ed.
by Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger.
Vol.27.CurranAssociates,Inc.
Hao Tan and Mohit Bansal (Nov. 2019). LXMERT: Learning Cross-Modality Encoder
Representations from Transformers. In: Proceedings of the 2019 Conference on Empiri-
calMethodsinNaturalLanguageProcessingandthe9thInternationalJointConference
onNaturalLanguageProcessing(EMNLP-IJCNLP).Ed.byKentaroInui,JingJiang,
Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computa-
tionalLinguistics,pp.5100–5111.
Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied
(Dec. 2022). Revisiting DocRED - Addressing the False Negative Problem in Relation
Extraction. In: Proceedings of the 2022 Conference on Empirical Methods in NaturalBibliography 193
LanguageProcessing.AbuDhabi,UnitedArabEmirates:AssociationforCompu-
tationalLinguistics,pp.8472–8487.
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaud-
hary, Jiatao Gu, and Angela Fan (Aug. 2021). Multilingual Translation from De-
noising Pre-Training. In: Findings of the Association for Computational Linguistics:
ACL-IJCNLP2021.Online:AssociationforComputationalLinguistics,pp.3450–
3466.
IshanTarunesh,SushilKhyalia,VishwajeetKumar,GaneshRamakrishnan,andPreethi
Jyothi (Apr. 2021). Meta-Learning for Effective Multi-task and Multilingual Mod-
elling. In: Proceedings of the 16th Conference of the European Chapter of the Associ-
ationforComputationalLinguistics:MainVolume.Online:AssociationforCompu-
tationalLinguistics,pp.3600–3612.
Ishan Tarunesh, Syamantak Kumar, and Preethi Jyothi (Aug. 2021). From Machine
Translation to Code-Switching: Generating High-Quality Code-Switched Text. In: Pro-
ceedings of the 59th Annual Meeting of the Association for Computational Linguistics
andthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:
LongPapers).Online:AssociationforComputationalLinguistics,pp.3154–3169.
IanTenney,DipanjanDas,andElliePavlick(July2019).BERTRediscoverstheClassical
NLP Pipeline. In: Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics. Florence,Italy: Association forComputational Linguistics,
pp.4593–4601.
James Thorne and Andreas Vlachos (Aug. 2018). Automated Fact Checking: Task For-
mulations, Methods and Future Directions. In: Proceedings of the 27th International
ConferenceonComputationalLinguistics.SantaFe,NewMexico,USA:Association
forComputationalLinguistics,pp.3346–3359.
JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal(June
2018).FEVER:aLarge-scaleDatasetforFactExtractionandVERification.In:Proceed-
ingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforCom-
putational Linguistics: Human Language Technologies, Volume 1 (Long Papers). New
Orleans,Louisiana:AssociationforComputationalLinguistics,pp.809–819.
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea
Finn(2023).Fine-tuningLanguageModelsforFactuality.In:ArXiv.194 Bibliography
Alexey Tikhonov and Max Ryabinin (Aug. 2021). It’s All in the Heads: Using Atten-
tion Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. In:
FindingsoftheAssociationforComputationalLinguistics:ACL-IJCNLP2021.Online:
AssociationforComputationalLinguistics,pp.3534–3546.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,
TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,et
al.(2023a).Llama:OpenandEfficientFoundationLanguageModels.In:ArXiv.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yasmine
Babaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,et
al.(2023b).Llama2:OpenFoundationandFine-tunedChatModels.In:ArXiv.
Bayu Distiawan Trisedya, Gerhard Weikum, Jianzhong Qi, and Rui Zhang (July
2019). Neural Relation Extraction for Knowledge Base Enrichment. In: Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics. Florence,
Italy:AssociationforComputationalLinguistics,pp.229–240.
ShyamUpadhyay,ManaalFaruqui,GokhanTür,Hakkani-TürDilek,andLarryHeck
(2018). (Almost) Zero-Shot Cross-Lingual Spoken Language Understanding. In: 2018
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp.6034–6038.
Clara Vania, Grace Lee, and Andrea Pierleoni (July 2022). Improving Distantly Su-
pervisedDocument-LevelRelationExtractionThroughNaturalLanguageInference.In:
Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Lan-
guageProcessing.Hybrid:AssociationforComputationalLinguistics,pp.14–20.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
N Gomez, Lukasz Kaiser, and Illia Polosukhin (2017). Attention is All you Need.
In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates,
Inc.,pp.5998–600.
RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh(2015).Cider:Consensus-
basedImageDescriptionEvaluation.In:ProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pp.4566–4575.
TuVu,AdityaBarua,BrianLester,DanielCer,MohitIyyer,andNoahConstant(Dec.
2022).OvercomingCatastrophicForgettinginZero-ShotCross-LingualGeneration.In:Bibliography 195
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcess-
ing. Abu Dhabi, United Arab Emirates: Association for Computational Linguis-
tics,pp.9279–9300.
Boxin Wang et al. (Dec. 2023). Shall We Pretrain Autoregressive Language Models with
Retrieval? A Comprehensive Study. In: Proceedings of the 2023 Conference on Empir-
ical Methods in Natural Language Processing. Ed. by Houda Bouamor, Juan Pino,
andKalikaBali.Singapore:AssociationforComputationalLinguistics,pp.7763–
7786.
CunxiangWang,PaiLiu,andYueZhang(Aug.2021).CanGenerativePre-trainedLan-
guage Models Serve As Knowledge Bases for Closed-book QA? In: Proceedings of the
59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thIn-
ternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers).
Online:AssociationforComputationalLinguistics,pp.3241–3251.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
Chang Zhou, Jingren Zhou, and Hongxia Yang (July 2022). OFA: Unifying Ar-
chitectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning
Framework. In: Proceedings of the 39th International Conference on Machine Learn-
ing. Ed. by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato. Vol. 162. Proceedings of Machine Learning Re-
search.PMLR,pp.23318–23340.
RuizeWang,DuyuTang,NanDuan,ZhongyuWei,XuanjingHuang,JianshuJi,Gui-
hong Cao, Daxin Jiang, and Ming Zhou (Aug. 2021). K-Adapter: Infusing Knowl-
edge into Pre-Trained Models with Adapters. In: Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021. Online: Association for Computational
Linguistics,pp.1405–1418.
William Yang Wang (July 2017). “Liar, Liar Pants on Fire”: A New Benchmark Dataset
forFakeNewsDetection.In:Proceedingsofthe55thAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume2:ShortPapers).Vancouver,Canada:Associ-
ationforComputationalLinguistics,pp.422–426.
XiaozhiWang,TianyuGao,ZhaochengZhu,ZhengyanZhang,ZhiyuanLiu,Juanzi
Li, and Jian Tang (2021). KEPLER: A Unified Model for Knowledge Embedding and196 Bibliography
Pre-trainedLanguageRepresentation.In:TransactionsoftheAssociationforComputa-
tionalLinguistics9,pp.176–194.
Xintao Wang, Qianyu He, Jiaqing Liang, and Yanghua Xiao (July 2022). Language
Models as Knowledge Embeddings. In: Proceedings of the Thirty-First International
JointConferenceonArtificialIntelligence(IJCAI-22).MainTrack.InternationalJoint
ConferencesonArtificialIntelligenceOrganization,pp.2291–2297.
Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec
(Oct. 2023). VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural
Networks for Visual Question Answering. In: Proceedings of the IEEE/CVF Interna-
tionalConferenceonComputerVision(ICCV),pp.21582–21592.
YaqingWang,SahajAgarwal,SubhabrataMukherjee,XiaodongLiu,JingGao,Ahmed
HassanAwadallah,andJianfengGao(Dec.2022).AdaMix:Mixture-of-Adaptations
for Parameter-efficient Model Tuning. In: Proceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing. Ed. by Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang. Abu Dhabi, United Arab Emirates: Association for
ComputationalLinguistics,pp.5744–5760.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel
Khashabi, and Hannaneh Hajishirzi (July 2023). Self-Instruct: Aligning Language
Models with Self-Generated Instructions. In: Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). Toronto,
Canada:AssociationforComputationalLinguistics,pp.13484–13508.
Yuxia Wang et al. (Mar. 2024). M4: Multi-generator, Multi-domain, and Multi-lingual
Black-BoxMachine-GeneratedTextDetection.In:Proceedingsofthe18thConferenceof
theEuropeanChapteroftheAssociationforComputationalLinguistics(Volume1:Long
Papers).Ed.byYvetteGrahamandMatthewPurver.St.Julian’s,Malta:Associa-
tionforComputationalLinguistics,pp.1369–1407.
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao
(2022). SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.
In:InternationalConferenceonLearningRepresentations.
AlexanderWettig,TianyuGao,ZexuanZhong,andDanqiChen(May2023).Should
YouMask15%inMaskedLanguageModeling?In:Proceedingsofthe17thConferenceBibliography 197
oftheEuropeanChapteroftheAssociationforComputationalLinguistics.Dubrovnik,
Croatia:AssociationforComputationalLinguistics,pp.2985–3000.
Chenxi Whitehouse, Monojit Choudhury, and Alham Aji (Dec. 2023). LLM-powered
Data Augmentation for Enhanced Cross-lingual Performance. In: Proceedings of the
2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Ed.byHouda
Bouamor,JuanPino,andKalikaBali.Singapore:AssociationforComputational
Linguistics,pp.671–686.
Chenxi Whitehouse, Fenia Christopoulou, and Ignacio Iacobacci (Dec. 2022). Enti-
tyCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switch-
ing.In:FindingsoftheAssociationforComputationalLinguistics:EMNLP2022.Abu
Dhabi,UnitedArabEmirates:AssociationforComputationalLinguistics,pp.6698–
6714.
ChenxiWhitehouse,FantineHuot,JasmijnBastings,MustafaDehghani,Chu-Cheng
Lin, and Mirella Lapata (2023a). Parameter-Efficient Multilingual Summarisation:
AnEmpiricalStudy.In:ArXiv.
ChenxiWhitehouse,ClaraVania,AlhamFikriAji,ChristosChristodoulopoulos,and
Andrea Pierleoni (July 2023b). WebIE: Faithful and Robust Information Extraction
on the Web. In: Proceedings of the 61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for
ComputationalLinguistics,pp.7734–7755.
ChenxiWhitehouse,TillmanWeyde,andPranavaMadhyastha(May2023).Towards
a Unified Model for Generating Answers and Explanations in Visual Question An-
swering. In: Findings of the Association for Computational Linguistics: EACL 2023.
Dubrovnik,Croatia:AssociationforComputationalLinguistics,pp.1648–1660.
Chenxi Whitehouse, Tillman Weyde, Pranava Madhyastha, and Nikos Komninos
(2022). Evaluation of Fake News Detection with Knowledge-Enhanced Language Mod-
els. In: Proceedings of the International AAAI Conference on Web and Social Media.
Vol.16,pp.1425–1429.
AdinaWilliams,NikitaNangia,andSamuelBowman(June2018).ABroad-Coverage
ChallengeCorpusforSentenceUnderstandingthroughInference.In:Proceedingsofthe
2018 Conference of the North American Chapter of the Association for Computational198 Bibliography
Linguistics: Human Language Technologies, Volume 1 (Long Papers). New Orleans,
Louisiana:AssociationforComputationalLinguistics,pp.1112–1122.
Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung (Nov.
2019).Code-SwitchedLanguageModelsUsingNeuralBasedSyntheticDatafromPar-
allelSentences.In:Proceedingsofthe 23rdConferenceonComputationalNaturalLan-
guageLearning(CoNLL).HongKong,China:AssociationforComputationalLin-
guistics,pp.271–280.
Genta Indra Winata et al. (May 2023). NusaX: Multilingual Parallel Sentiment Dataset
for 10 Indonesian Local Languages. In: Proceedings of the 17th Conference of the Eu-
ropeanChapteroftheAssociationforComputationalLinguistics.Dubrovnik,Croatia:
AssociationforComputationalLinguistics,pp.815–834.
Thomas Wolf et al. (Oct. 2020). Transformers: State-of-the-Art Natural Language Pro-
cessing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations. Online: Association for Computational
Linguistics,pp.38–45.
JialinWuandRaymondMooney(Aug.2019).FaithfulMultimodalExplanationforVi-
sual Question Answering. In: Proceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP. Florence, Italy: Association
forComputationalLinguistics,pp.103–112.
Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer
(Nov. 2020). Scalable Zero-shot Entity Linking with Dense Entity Retrieval. In: Pro-
ceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP).Online:AssociationforComputationalLinguistics,pp.6397–6407.
ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,AlaneSuhr,PrithvirajAmmanabrolu,
Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi (2023). Fine-Grained
HumanFeedbackGivesBetterRewardsforLanguageModelTraining.In:Thirty-seventh
ConferenceonNeuralInformationProcessingSystems.
Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, and Graham
Neubig (July 2020). Predicting Performance for Natural Language Processing Tasks.
In: Proceedings of the 58th Annual Meeting of the Association for Computational Lin-
guistics.Online:AssociationforComputationalLinguistics,pp.8625–8646.Bibliography 199
Rong Xiang, Mingyu Wan, Qi Su, Chu-Ren Huang, and Qin Lu (Dec. 2020). Sina
Mandarin Alphabetical Words:A Web-driven Code-mixing Lexical Resource. In: Pro-
ceedingsofthe1stConferenceoftheAsia-PacificChapteroftheAssociationforCompu-
tational Linguistics and the 10th International Joint Conference on Natural Language
Processing. Suzhou, China: Association for Computational Linguistics, pp. 833–
842.
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav (2019). Visual Entailment: A
NovelTaskforFine-grainedImageUnderstanding.In:ArXiv.
Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov (2020). Pre-
trained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model. In:
InternationalConferenceonLearningRepresentations.
Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima
Anandkumar, and Bryan Catanzaro (Nov. 2020). MEGATRON-CNTRL: Control-
lable Story Generation with External Knowledge Using Large-Scale Language Models.
In:Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguagePro-
cessing (EMNLP). Ed. by Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu.
Online:AssociationforComputationalLinguistics,pp.2831–2845.
WeijiaXu,BatoolHaider,andSaabMansour(Nov.2020).End-to-EndSlotAlignment
and Recognition for Cross-Lingual NLU. In: Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP). Online: Association
forComputationalLinguistics,pp.5052–5063.
LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSid-
dhant, Aditya Barua, and Colin Raffel (June 2021). mT5: A Massively Multilin-
gualPre-trainedText-to-TextTransformer.In:Proceedingsofthe2021Conferenceofthe
NorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLan-
guage Technologies. Online: Association for Computational Linguistics, pp. 483–
498.
IkuyaYamada,AkariAsai,JinSakuma,HiroyukiShindo,HideakiTakeda,Yoshiyasu
Takefuji, and Yuji Matsumoto (Oct. 2020a). Wikipedia2Vec: An Efficient Toolkit for
Learning and Visualizing the Embeddings of Words and Entities from Wikipedia. In:200 Bibliography
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing: System Demonstrations. Online: Association for Computational Linguis-
tics,pp.23–30.
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto
(Nov. 2020b). LUKE: Deep Contextualized Entity Representations with Entity-aware
Self-attention.In:Proceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
Language Processing (EMNLP). Online: Association for Computational Linguis-
tics,pp.6442–6454.
Jian Yang, Shuming Ma, Dongdong Zhang, ShuangZhi Wu, Zhoujun Li, and Ming
Zhou(Apr.2020).AlternatingLanguageModelingforCross-LingualPre-Training.In:
Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 05, pp. 9386–
9393.
Yuan Yao, Jiaju Du, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun
(Nov. 2021). CodRED: A Cross-Document Relation Extraction Dataset for Acquiring
KnowledgeintheWild.In:Proceedingsofthe2021ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing.OnlineandPuntaCana,DominicanRepublic:Asso-
ciationforComputationalLinguistics,pp.4452–4472.
Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu,
Lixin Huang, Jie Zhou, and Maosong Sun (July 2019). DocRED: A Large-Scale
Document-LevelRelationExtractionDataset.In:Proceedingsofthe57thAnnualMeet-
ing of the Association for Computational Linguistics. Florence, Italy: Association for
ComputationalLinguistics,pp.764–777.
Keren Ye and Adriana Kovashka (2021). A Case study of the Shortcut Effects in Visual
Commonsense Reasoning. In: Proceedings of the AAAI conference on artificial intelli-
gence.Vol.35,pp.3181–3189.
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant (2024). Making Retrieval-
Augmented Language Models Robust to Irrelevant Context. In: The Twelfth Interna-
tionalConferenceonLearningRepresentations.
Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong
Xu, Michael Zeng, and Meng Jiang (May 2022). Dict-BERT: Enhancing Language
ModelPre-trainingwithDictionary.In:FindingsoftheAssociationforComputationalBibliography 201
Linguistics:ACL2022.Ed.bySmarandaMuresan,PreslavNakov,andAlineVillav-
icencio. Dublin, Ireland: Association for Computational Linguistics, pp. 1907–
1918.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi (June 2019a). From Recog-
nitiontoCognition:VisualCommonsenseReasoning.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pp.6713–6724.
RowanZellers,AriHoltzman,HannahRashkin,YonatanBisk,AliFarhadi,Franziska
Roesner,andYejinChoi(2019b).DefendingAgainstNeuralFakeNews.In:Advances
inNeuralInformationProcessingSystems.Vol.32.CurranAssociates,Inc.
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali
Farhadi, and Yejin Choi (2021). MERLOT: Multimodal Neural Script Knowledge
Models.In:AdvancesinNeuralInformationProcessingSystems.Ed.byM.Ranzato,
A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan.Vol.34.Curran
Associates,Inc.,pp.23634–23651.
George Zerveas, Navid Rekabsaz, and Carsten Eickhoff (Dec. 2023). Enhancing the
RankingContextofDenseRetrievalthroughReciprocalNearestNeighbors.In:Proceed-
ings of the 2023 Conference on Empirical Methods in Natural Language Processing.
Ed. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for
ComputationalLinguistics,pp.10779–10803.
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao (2021). Vinvl: Revisiting Visual Representations in
Vision-Language Models. In: Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.5579–5588.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.(2022).Opt:
OpenPre-trainedTransformerLanguageModels.In:ArXiv.
TianyiZhang,VarshaKishore,FelixWu,KilianQ.Weinberger,andYoavArtzi(2020).
BERTScore: Evaluating Text Generation with BERT. In: International Conference on
LearningRepresentations.
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Man-
ning (Sept. 2017). Position-aware Attention and Supervised Data Improve Slot Fill-
ing.In:Proceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguage202 Bibliography
Processing. Copenhagen, Denmark: Association for Computational Linguistics,
pp.35–45.
ZhengyanZhang,XuHan,ZhiyuanLiu,XinJiang,MaosongSun,andQunLiu(July
2019). ERNIE: Enhanced Language Representation with Informative Entities. In: Pro-
ceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Florence,Italy:AssociationforComputationalLinguistics,pp.1441–1451.
Zhiyuan Zhang, Xiaoqian Liu, Yi Zhang, Qi Su, Xu Sun, and Bin He (Nov. 2020).
Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models.
In: Findings of the Association for Computational Linguistics: EMNLP 2020. Ed. by
Trevor Cohn, Yulan He, and Yang Liu. Online: Association for Computational
Linguistics,pp.259–266.
JunruZhou,ZhuoshengZhang,HaiZhao,andShuailiangZhang(Nov.2020).LIMIT-
BERT:LinguisticsInformedMulti-TaskBERT.In:FindingsoftheAssociationforCom-
putationalLinguistics:EMNLP2020.Ed.byTrevorCohn,YulanHe,andYangLiu.
Online:AssociationforComputationalLinguistics,pp.4450–4461.
ChenguangZhu,YichongXu,XiangRen,BillYuchenLin,MengJiang,andWenhao
Yu(May2022).Knowledge-AugmentedMethodsforNaturalLanguageProcessing.In:
Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguis-
tics: Tutorial Abstracts. Ed. by Luciana Benotti, Naoaki Okazaki, Yves Scherrer,
and Marcos Zampieri. Dublin, Ireland: Association for Computational Linguis-
tics,pp.12–20.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, An-
tonio Torralba, and Sanja Fidler (Dec. 2015). Aligning Books and Movies: Towards
Story-Like Visual Explanations by Watching Movies and Reading Books. In: The IEEE
InternationalConferenceonComputerVision(ICCV).
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario
Amodei,PaulChristiano,andGeoffreyIrving(2019).Fine-tuningLanguageMod-
elsFromHumanPreferences.In:ArXiv.