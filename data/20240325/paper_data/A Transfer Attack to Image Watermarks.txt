A Transfer Attack to Image Watermarks
YuepengHu,ZhengyuanJiang,MoyangGuo,NeilGong
DukeUniversity
yuepeng.hu, zhengyuan.jiang, moyang.guo, neil.gong @duke.edu
{ }
ABSTRACT
Watermark has been widely deployed by industry to detect AI-generated images. The robustness
of such watermark-based detector against evasion attacks in the white-box and black-box settings
is well understood in the literature. However, the robustness in the no-box setting is much less
understood. Inparticular, multiplestudiesclaimedthatimagewatermarkisrobustinsuchsetting.
Inthiswork,weproposeanewtransferevasionattacktoimagewatermarkintheno-boxsetting.Our
transferattackaddsaperturbationtoawatermarkedimagetoevademultiplesurrogatewatermarking
models trained by the attacker itself, and the perturbed watermarked image also evades the target
watermarking model. Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker
doesnothaveaccesstothewatermarkingmodelnorthedetectionAPI.
1 Introduction
GenerativeAI(GenAI)cansynthesizeextremelyrealistic-lookingimages,posinggrowingchallengestoinformation
authenticityontheInternet. Watermarking[1,2,3,4,5,6,7]wassuggestedasakeytechnologytodistinguishAI-
generatedandnon-AI-generatedcontentintheExecutiveOrderonAIsecurityissuedbytheWhiteHouseinOctober
2023. Inwatermark-baseddetection,awatermarkisembeddedintoanAI-generatedimagebeforereleasingit;andan
imageisdetectedasAI-generatedifthesamewatermarkcanbedecodedfromit. WatermarkingAI-generatedimages
hasbeenwidelydeployedinindustry. Forinstance,Google’sSynthIDwatermarksimagesgeneratedbyImagen[8];
OpenAIembedsawatermarkintoimagesgeneratedbyDALL-E[9]; andStableDiffusionenablesuserstoembeda
watermarkintothegeneratedimages[10].
Anattackercanuseevasionattacks[11]toremovethewatermarkinawatermarkedimagetoevadedetection. Specifi-
cally,anevasionattackstrategicallyaddsaperturbationintoawatermarkedimagesuchthatthetargetwatermark-based
detector falsely detects the perturbed image as non-AI-generated. The literature has well understood the robustness
ofwatermark-baseddetectoragainstevasionattacksinthewhite-boxsetting(i.e.,theattackerhasaccesstothetarget
watermarking model) and black-box setting (i.e., the attacker has access to the detection API) [11]. Specifically, in
thewhite-boxsetting,anattackercanfindasmallperturbationforagivenwatermarkedimagesuchthattheperturbed
imageevadesdetectionwhilemaintainingtheimage’svisualquality;andintheblack-boxsetting,anattackercanfind
suchaperturbationoncetheattackercanquerythedetectionAPIforenoughtimes.
However,robustnessofwatermark-baseddetectionintheno-boxsetting(i.e.,theattackerdoesnotevenhaveaccess
tothedetectionAPI)ismuchlessunderstood. Insuchsetting,anattackercanusetransferevasionattacks(ortransfer
attacksforsimplicity)[11,12]. Onetransferattack[11]trainsonesurrogatewatermarkingmodelandusesthewhite-
boxattack[11]togenerateaperturbationforagivenwatermarkedimagebasedonit.Anetal.[12],whichisconcurrent
toourwork,treatawatermark-baseddetectorasaconventionalclassifierandproposeclassifier-basedtransferattacks.
Forinstance,givenasetofwatermarkedandnon-watermarkedimages,anattackercantrainasurrogateclassifierto
distinguishthem. Givenawatermarkedimage, theattackerusesconventionaladversarialexampletechnique[13]to
perturb it such that the surrogate classifier predicts the perturbed image as non-watermarked. Prior studies [11, 12]
found that existing transfer attacks have limited success at evading advanced watermarking methods [2, 3]. Thus, a
commonconclusioninpriorstudiesisthatwatermarkingisrobustintheno-boxsetting.
4202
raM
22
]RC.sc[
1v56351.3042:viXraATransferAttacktoImageWatermarks
Ourwork: Inthiswork,weproposeanewtransferattackintheno-boxsettingtoevadewatermark-baseddetectionof
AI-generatedimages. Unlikeclassifier-basedtransferattacks[12],ourattackdirectlyleveragessurrogatewatermark-
ingmodelsandthusisbettertailoredtowatermark-baseddetection. Specifically,anattackertrainsmultiplesurrogate
watermarkingmodelsusingasurrogatedataset. Thesurrogatewatermarkingmodelsmayhavedifferentneuralnet-
workarchitecturesandwatermarklengths,comparedtothetargetwatermarkingmodel;andthesurrogatedatasetmay
haveadifferentdistributionfromtheoneusedtotrainthetargetwatermarkingmodel.
Givenmultiplesurrogatewatermarkingmodels,akeychallengeishowtofindasmallperturbationforagivenwater-
markedimagesuchthattheperturbedimageevadesthetargetwatermarkingmodel. Weproposeatwo-stepapproach
toaddressthechallenge. Inthefirststep,theattackerpicksawatermark(calledtargetwatermark)foreachsurrogate
watermarking model to guide the perturbation search process. Specifically, the perturbation aims to make each sur-
rogate watermarking model decode the target watermark from the perturbed image. For instance, one way is to flip
eachbitofthewatermarkdecodedbyasurrogatewatermarkingmodelforthewatermarkedimageandtreattheflipped
watermark as the corresponding target watermark. Our intuition is that if the perturbation makes multiple surrogate
watermarking models decode flipped watermarks from the perturbed image, the target watermarking model is also
likelytodecodeaflippedwatermarkfromtheperturbedimage,evadingdetection.
In the second step, given the target watermark for each surrogate watermarking model, we generate a perturbation
by aggregating multiple surrogate watermarking models. For instance, one way is to leverage an existing white-
box attack [11] to find a perturbation based on each surrogate watermarking model and its target watermark; and
then aggregate the multiple perturbations (e.g., take their mean) as the final perturbation. However, such strategy
achieves limited success because the aggregation breaks the perturbation patterns, as shown in our experiments. To
address the challenge, we find a perturbation by ensembling multiple surrogate watermarking models. Specifically,
we formulate an optimization problem, whose objective function is to find a minimum perturbation and constraints
are that each surrogate watermarking model decodes its target watermark from the perturbed image. However, the
optimizationproblemischallengingtosolveduetothehardconstraints. Weproposeseveralstrategiestoreformulate
theoptimizationproblemandapproximatelysolveittofindaperturbation.
We theoretically analyze the transferability of our transfer attack. Specifically, we formally quantify the correlation
betweenthetargetandsurrogatewatermarkingmodels.Basedonsuchquantifiedcorrelation,wederivetheprobability
thatthewatermarkdecodedbythetargetwatermarkingmodelfromthewatermarkedimageisflippedafteraddingthe
perturbation found by our transfer attack to it. Moreover, based on this probability, we further derive both an upper
boundandalowerboundfortheprobabilitythatthewatermarkdecodedbythetargetwatermarkingmodelfromthe
perturbedimagematcheswiththeground-truthwatermarkinthewatermarkedimage. Suchupperandlowerbounds
quantifythetransferabilityofourattack.
WeempiricallyevaluateourtransferattackonimagedatasetsgeneratedbytwoGenAImodels,i.e.,StableDiffusion
andMidjourney. WeuseHiDDeN[2], whichisthestate-of-artimagewatermarkingmethod. Ourresultsshowthat,
with dozens of surrogate watermarking models, our attack successfully evades a watermark-based detector while
maintainingimagequality,evenifthesurrogatewatermarkingmodelsusedifferentneuralnetworkarchitecturesand
watermark lengths from the target one, and are trained using datasets with different distributions. Moreover, our
attacksubstantiallyoutperformsexistingtransferattacks[11,12]. Ourresultsinvalidatethepriorbeliefsthatimage
watermarksarerobustintheno-boxsetting.
Tosummarize,ourcontributionsareasfollows:
• We propose a new transfer attack based on multiple surrogate watermarking models to watermark-based
AI-generatedimagedetector.
• Wetheoreticallyanalyzetheeffectivenessofourattack.
• Weempiricallyevaluateourtransferattackandcompareitwithexistingonesindifferentscenarios.
2 RelatedWork
2.1 ImageWatermarks
Threecomponents: Animagewatermarkingmethodconsistsofthreecomponents: awatermarkthatisabitstring,
anencoderthatembedsawatermarkintoanimage,andadecoderthatdecodesawatermarkfromanimage. Innon-
learning-basedmethods[5,1,14,15,16]thathavebeenstudiedfordecades,theencoderanddecoderarehandcrafted.
Unlike these traditional, non-learning-based methods, learning-based methods [2, 4, 3] leverage deep learning. In
thesemethods,boththeencoderanddecoderareneuralnetworksandtrainedinanend-to-endmannerusinganimage
2ATransferAttacktoImageWatermarks
dataset. The encoder transforms a watermark and an image into feature vectors and combines them to generate a
watermarkedimage;whilethedecoderoutputsawatermarkwhentakingan(unwatermarkedorwatermarked)image
asinput. Whenjointlytrainingtheencoderanddecoder,theobjectivesaretominimizethevisualdifferencebetween
animageanditswatermarkedversionwhileensuringaccuratedecodingofthewatermark.Comparedtonon-learning-
basedones,learning-basedmethodsaremorerobustbecausetheycanleverageadversarialtraining[2]. Therefore,we
focusonlearning-basedmethods.
Adversarial training: Adversarial training [13, 17] is a standard method to train robust classifiers and has been
extendedtotrainrobustwatermarkingmodels[2]. Thekeyideaistoaddapost-processinglayerbetweentheencoder
and decoder. The post-processing layer aims to mimic post-processing that a watermarked image may undergo in
practice. Specifically,thepost-processinglayerpost-processesawatermarkedimagebeforesendingittothedecoder.
Afterjointlytrainingtheencoderanddecoderusingadversarialtraining,thedecodercanstilldecodethewatermarkin
awatermarkedimageevenifitundergoessomepost-processing. Thus,weuseadversarialtraininginourexperiments
totrainencodersanddecoders.
2.2 EvasionAttacks
White-box: Jiangetal.[11]proposedawhite-boxattackwhichassumestheattackerhasaccesstothetargetwater-
markdecoder. Giventhetargetwatermarkdecoderandawatermarkedimage, anattackerfindsasmallperturbation
suchthatthewatermarkdecodedfromtheperturbedimageisclosetoarandomwatermark. Inotherwords,theper-
turbationremovesthewatermarkfromthewatermarkedimageandthustheperturbedimageevadeswatermark-based
detection. Specifically,theattackerfindstheperturbationbysolvinganoptimizationproblemviagradientdescent.
Black-box: In black-box setting, an attacker has access to the API of a target watermark-based detector. Given an
image,thedetectionAPIreturnsabinaryclassificationresult,i.e.,AI-generated(watermarked)ornon-AI-generated
(non-watermarked). Ablack-boxattack[11]leveragesthedetectionAPItoperturbawatermarkedimagetoremove
its watermark. In particular, given a watermarked image, the attacker starts from an initial perturbed image that is
detectedasnon-AI-generatedbytheAPI.Theinitialperturbedimagemayhavealargeperturbationcomparedtothe
watermarkedimage. Then,theattackerrepeatedlyqueriestheAPIandgraduallymovestheperturbedimagetowards
thewatermarkedimagetoreducetheperturbationbasedonthequeryresults. WhenanattackercanquerytheAPIfor
enoughtimes,theattackercanfindaperturbedimagewithasmallperturbationthatevadesdetectionoftheAPI.
No-box: In this setting, an attacker does not even have access to the detection API. In such setting, common post-
processing or transfer attacks can be used to remove watermark from a watermarked image. Specifically, common
post-processing refers to common image editing operations such as JPEG compression, Gaussian noise, and Gaus-
sian blur. Watermarks embedded by non-learning-based methods can be removed by common post-processing, but
learning-basedmethodshavegoodrobustnessagainstcommonpost-processingduetoadversarialtraining[2]. Exist-
ingtransferattacksleverageeitheronlyonesurrogatewatermarkingmodel[11]orclassifier-basedattacks[12](these
attacks are concurrent to ours). For instance, Jiang et al. [11] proposed to train one surrogate encoder and decoder,
and then apply the white-box attack [11] to find a perturbation for a given watermarked image based on the surro-
gate decoder. An example of classifier-based transfer attack [12] is to train a classifier that distinguishes between
watermarked and non-watermarked images; and given a watermarked image, the attacker finds a perturbation such
thattheperturbedimageisclassifiedasnon-watermarkedbytheclassifier. Asshowninpriorworks[11,12]andalso
confirmedinourexperiments,thesetransferattacksachievelimitedsuccessatevadinglearning-basedwatermarking
methods.
2.3 TransferAdversarialExamples
Ensemble-basedtransferadversarialexamplesbyleveragingmultiplesurrogatemodelshavebeenexploredwithinthe
adversarialexamplescommunity[18,19]. However,thesestudiesfocusedonensemble-basedtransferattackstoclas-
sifiers,wherethesurrogateandtargetmodelsareclassifiers. Classifiersarequalitativelydifferentfromwatermarking
decoders. Specifically,aclassifieroutputsalabelforaninput,whileawatermarkingdecoderoutputsabinarystring
comprising multiple bits. Such difference results in a fundamentally different optimization objective for transfer at-
tacks. Furthermore,thebinarynatureofwatermarkbitstringsenablesustocarryoutrigoroustheoreticalanalysison
the effectiveness of our transfer attack. However, it is challenging to conduct such theoretical analysis for transfer
attackstoclassifiers.
3ATransferAttacktoImageWatermarks
3 ProblemFormulation
3.1 Watermark-BasedDetection
AGenAIserviceprovidertrainsawatermarkencoder(calledtargetencoder)anddecoder(calledtargetdecoderand
denotedasT),andusesthemforAI-generatedimagedetection. Duringgeneration,awatermarkwisembeddedinto
each AI-generated image using the target encoder. During detection, a watermark is decoded from a given image x
bythetargetdecoderT. WedenotebyT(x)thedecodedwatermark. Werepresentthebitwiseaccuracybetweentwo
watermarksw andw asBA(w ,w ), whichistheproportionofbitsthatareidenticalinw andw . Watermark-
1 2 1 2 1 2
baseddetectiondetermineswhethertheimagexisAI-generatedbasedonthebitwiseaccuracyBA(T(x),w)between
thedecodedwatermarkT(x)andtheground-truthwatermarkw. Specifically,theimagexisdetectedasAI-generated
ifthebitwiseaccuracyBA(T(x),w)eitherexceedsacertainthresholdτ orfallsbelow1 τ,i.e.,BA(T(x),w)>τ
−
orBA(T(x),w)<1 τ. τ iscommonlysettoavaluesuchthatthefalsedetectionrate,i.e.,theprobabilityoffalsely
−
detecting a non-AI-generated (i.e., non-watermarked) image as AI-generated (i.e., watermarked), does not exceed a
desiredsmallvalueη[11]. Forinstance,whenthewatermarkhas30bitsandη =10−4,τ 0.83.
≈
Thedetectordiscussedaboveisknownasdouble-taildetector[11]. Inasingle-taildetector,theimagexisdetected
as AI-generated if the bitwise accuracy BA(T(x),w) exceeds a threshold τ, i.e., BA(T(x),w) > τ. Double-tail
detector is more robust than single-tail detector, e.g., if a perturbed watermarked image evades double-tail detector,
thenitalsoevadessingle-taildetector,butnotviceversa. Therefore,wefocusondouble-taildetectorinthiswork.
3.2 ThreatModel
Attacker’sgoal: SupposeanattackerusesaGenAIservicetoproduceawatermarkedimagex . Theattacker’sgoal
w
is to introduce a minimal perturbation δ to the watermarked image x , aiming to evade watermark-based detection
w
while preserving image quality. Consequently, the attacker can engage in illicit activities using this image, such as
boostingdisinformationandpropagandacampaignsaswellasclaimingownershipoftheimage.
Attacker’s knowledge: A GenAI service provider’s watermark-based detector includes a target encoder, a target
decoder, a ground-truth watermark w, and a detection threshold τ. We assume that the attacker has no-box access
tothewatermark-baseddetector. Specifically,theattackerdoesnothaveaccesstothetargetencoder,targetdecoder,
ground-truth watermark w, and detection threshold τ. Moreover, the attacker does not know the neural network
architectureofthetargetencoderanddecoder,thelengthoftheground-truthwatermarkw,northeimagedatasetused
totrainthetargetencoder/decoder.SuchthreatmodelariseswhentheGenAIservicesetsitswatermark-baseddetector
privateandrestrictsaccessofthedetectionAPItotrustedcustomers.
Attacker’scapability: WeassumeanattackercanaddaperturbationtoanAI-generated,watermarkedimage. Fur-
thermore,weassumethattheattackerhassufficientcomputationalresourcestotrainmultiplesurrogatewatermarking
models,whereeachsurrogatewatermarkingmodelincludesasurrogateencoderandasurrogatedecoder.
4 OurTransferAttack
4.1 Overview
We propose a transfer attack to evade watermark-based AI-generated image detection. Our transfer attack trains
multiplesurrogatewatermarkingmodels,andthengeneratesaperturbationforagivenwatermarkedimagebasedon
them. Wetrainasetofdiversesurrogatewatermarkingmodelsindependentlyusingadatasetthatmayhaveadifferent
distribution from the one used by the target watermarking model. For a given watermarked image x , we generate
w
aperturbationδ basedonthesurrogatedecoderssuchthatthewatermarkdecodedbyeachsurrogatedecoderforthe
perturbed image x +δ substantially differs from the watermark decoded for the original watermarked image x .
w w
Ourintuitionisthatifmultiplesurrogatedecodersdecodeasubstantiallydifferentwatermarkfortheperturbedimage
x +δ, it is likely that the target watermarking decoder would decode a substantially different watermark, evading
w
detection. Notethatthesamesurrogatedecodersareusedtogenerateperturbationsforallwatermarkedimages.
Next,wedescribethedetailsoftrainingsurrogatemodelsandgeneratingperturbationsbasedonthem.
4.2 TrainSurrogateWatermarkingModels
Transfer attacks require a set of surrogate watermarking models to produce perturbation for a watermarked image.
Toimprovethetransferabilityoftheperturbationproducedinourtransferattack,acrucialchallengeistoencourage
4ATransferAttacktoImageWatermarks
diversity among the surrogate watermarking models. The attacker first collects an image dataset (called surrogate
dataset)totrainthesurrogatewatermarkingmodels. Thesurrogatedatasetmayhaveadifferentdistributionfromthe
datasetusedtotrainthetargetwatermarkingmodel. Toencouragediversity,weproposetoadoptbootstrapping[20],
whichisawidelyusedtechniquetotraindiversemodelsinstatistics. Specifically, theattackerresamplesmsubsets
from the surrogate dataset, each of which is used to train one of the m surrogate watermarking models. Moreover,
theattackercanusedifferentneuralnetworkarchitecturesanddifferentwatermarklengthsforthemsurrogatewater-
markingmodels.
4.3 FormulateanOptimizationProblem
Toevadewatermark-baseddetection,wegenerateaperturbationδforagivenwatermarkedimagex basedonthem
w
surrogatedecoderssincethedetectionprocessonlyinvolvesdecoders. Specifically,ourgoalistoaddaperturbationδ
tothewatermarkedimagex suchthat,foreachsurrogatedecoder,thedecodedwatermarkfromtheperturbedimage
w
x +δ matches an attacker-chosen watermark, which we call target watermark. Our transfer attack faces two key
w
challenges: 1)howtoselectatargetwatermarkforasurrogatedecoder,and2)howtogenerateaperturbationδbased
onthemtargetwatermarksandsurrogatedecoders. Wediscusshowtoaddressthetwochallengesinthefollowing.
Selectatargetwatermarkforasurrogatedecoder: Weusewttodenotethetargetwatermarkfortheithsurrogate
i
decoder,wherei=1,2, ,m. Weconsiderthefollowingapproachestoselectwt.
··· i
Random-Different(RD).Thismethodinvolvesrandomlygeneratingdifferenttargetwatermarksforthemsurrogate
decoders. For the ith surrogate decoder, each bit of wt is sampled from 0,1 uniformly at random. The intuition
i { }
of this method is that, if the m surrogate decoders decode random watermarks from the perturbed image, then the
targetdecoderisalsolikelytodecodearandomwatermarkfromtheperturbedimage. Asaresult,thebitwiseaccu-
racybetweenthewatermarkT(x +δ)decodedbythetargetdecoderfortheperturbedimageandtheground-truth
w
watermarkwisexpectedtoapproach0.5,therebyevadingdetection.
Random-Same(RS).Thismethodrandomlygeneratesonerandomtargetwatermarkwtforallmsurrogatedecoders,
i.e.,wt =wt, i=1,2, ,m. Theintuitionisthatitismorelikelytofindaperturbationδsuchthatthemsurrogate
i ∀ ···
decodersdecodethesametargetwatermarkfromtheperturbedimage,andthusitismorelikelyforthetargetdecoder
todecodewtfromtheperturbedimage. Sincewtispickeduniformlyatrandom,thebitwiseaccuracybetweenwtand
wisexpectedtoapproach0.5anddetectionisevaded.
Inverse-Decode(ID).However,generatingrandomwatermarksastargetwatermarksforsurrogatedecodersmayhave
anissue.Specifically,foreachsurrogatedecoder,approximately50%ofthebitsrequiretobeflippedwhileoptimizing
theperturbation,giventhateachbitofthetargetwatermarkisuniformlysampledfrom 0,1 . Amongthesebitstobe
{ }
flipped,somemaybeveryrobusttopixelchangesandthuslargeperturbationisrequiredtoalterthem.
To address this issue, we propose to use the inverse of the watermark decoded by each surrogate decoder as the
correspondingtargetwatermark. Specifically,weuseS todenotetheithsurrogatedecoder. Forawatermarkedimage
i
x ,S decodesawatermarkfromit,denotedasS (x ).Weusetheinverseofthedecodedwatermark,i.e.,1 S (x ),
w i i w i w
asthetargetwatermarkwtfortheithsurrogatedecoderS ,where1 S (x )meansflippingeachbitofS (− x ). The
i i − i w i w
intuitionofthismethodisthatthewatermarkdecodedbyeachsurrogatedecoderfromtheperturbedimageisreversed,
andthusitislikelythatthewatermarkdecodedbythetargetdecoderfromtheperturbedimageisalsoreversed,i.e.,
T(x +δ)iscloseto1 T(x ).Asaresult,thebitwiseaccuracybetweenT(x +δ)andwisverysmallsinceT(x )
w w w w
−
isclosetow. Wenotethatsuchbitwiseaccuracymaybeevensmallerthan1 τ,whichmeansthatthedouble-tail
−
detectorcanstilldetecttheperturbedimageasAI-generated,watermarkedimage. Weaddressthischallengebyearly
stoppingtheprocessoffindingtheperturbationδ,whichwediscussinSection4.4.
Withtheinversewatermarkastargetwatermark,itmayresultinthosebitsthatarelessrobusttopixelchangesbeing
flipped first during perturbation optimization process. As a result, it can lead to a smaller perturbation compared to
randomtargetwatermarkwhenthesameportionofbitsareflipped,asshowninourexperiments.
Aggregatemsurrogatedecoders: Givenatargetwatermarkwtfortheithsurrogatedecoder,theattackerthenfinds
i
aperturbationδ forthewatermarkedimagex byaggregatingthemsurrogatedecoders. Weconsiderthefollowing
w
twowaystoaggregatethemsurrogatedecoders.
Post-Aggregate(PA).Astraightforwardwayistoapplyanexistingwhite-boxattacktofindaperturbationδ basedon
i
theithsurrogatedecoderandtargetwatermark. Forinstance,giventhewatermarkedimagex ,ithsurrogatedecoder
w
S , andtargetwatermarkwt, theattackercanusethestate-of-the-artwhite-boxattackinJiangetal.[11]tofindthe
i i
perturbationδ .Formally,theattackergetsmperturbations δ m .Then,theattackercanaggregatethemasthefinal
i { i }i=1
perturbationδ,i.e.,δ = aggr(δ , ,δ ),whereaggr isanaggregationmethod. Forinstance,aggr canbeMean
1 m
···
orMedian,whichtakesthemeanormedianofthemperturbationsasthefinalperturbationδ. However,aggregating
5ATransferAttacktoImageWatermarks
theperturbationsgeneratedbythesurrogatedecodersmayruinthepatternsinthemwhichareeffectivetomisleadthe
targetdecoderbeforeaggregation. Asaresult,theobtainedperturbationachieveslimitedtransferabilitytothetarget
decoder,asshowninourexperiments.
Ensemble-Optimization(EO).ToaddressthechallengeofPost-Aggregate,Ensemble-Optimizationconsidersthem
surrogatedecoderswhenoptimizingtheperturbationδ. Specifically,Ensemble-Optimizationaimstofindaminimum
perturbationsuchthatthewatermarkS (x +δ)decodedbyeachsurrogatedecoderfortheperturbedimagex +δ
i w w
isthesameasitscorrespondingtargetwatermarkwt. Formally,weformulateanoptimizationproblemasfollows:
i
min δ
∞
δ ∥ ∥ (1)
s.t.S (x +δ)=wt, i=1,2, ,m
i w i ∀ ···
Ensemble-Optimizationfindsasharedperturbationδsuchthatallthemsurrogatedecodersoutputtheirtargetwater-
marksfortheperturbedimagex +δ. Thegeneralizationabilityofthesharedperturbationforallsurrogatedecoders
w
could enhance the transferability to the target decoder, as shown in our experiments. Note that we use ℓ -norm to
∞
measure the magnitude of the perturbation. Our method is also applicable to other metrics such as ℓ or SSIM, as
2
showninourexperiments.
4.4 SolvetheOptimizationProblem
SolvingtheoptimizationprobleminEquation1getstheperturbationδforthewatermarkedimagex .However,since
w
the constraints of the optimization problem are extremely strict, it is difficult to solve the optimization problem. To
addressthischallenge,werelaxtheconstraintsandreformulatetheoptimizationproblemasfollows:
min δ
∞
δ ∥ ∥ (2)
s.t.l(S (x +δ),wt)<ϵ, i=1,2, ,m,
i w i ∀ ···
wherel(, )denotesametrictomeasurethedistancebetweentwowatermarks. Forinstance,l(, )isthemeansquare
· · · ·
errorinourexperiments.
Thereformulatedoptimizationproblemisstillchallengingtosolveduetothehighnon-linearityoftherelaxedcon-
straints. Toaddressthischallenge,wefurtherreformulatetheoptimizationproblemasfollows:
m
1 ∑︂
min l(S (x +δ),wt)
δ m i w i
i=1
s.t. δ <r, (3)
∞
∥ ∥
m
1 ∑︂
BA(S (x +δ),wt)>1 ϵ,
m i w i −
i=1
where r is a perturbation budget and ϵ (called attack strength threshold) is used to ensure that the bitwise accuracy
between the watermark decoded by each surrogate decoder for the perturbed image and the corresponding target
watermarkishighenoughtoproduceaneffectiveperturbation.
We use projected gradient descent (PGD) [13] to solve the optimization problem in Equation 3. To better control
thestrengthofourtransferattacktoevadedouble-taildetector,weconsiderthesecondconstraintintheoptimization
probleminEquation3asaconditionforearlystopping. Ourdetailedalgorithmtofindtheperturbationδisshownin
Algorithm1inAppendix.Itfirstinitializesδtozeroandthencomputethegradientoftheobjectivefunctionasdefined
inEquation3withrespecttoδ. Thegradientisthenusedtoupdateδ accordingtogradientdescend. Iftheℓ -norm
∞
of the updated δ exceeds r, it is then projected onto the ℓ -norm ball with radius r. The algorithm terminates and
∞
outputsthefinalperturbationδ oncethenumberofoptimizationiterationsortheaveragebitwiseaccuracyexceedsa
predefinedthreshold.
5 TheoreticalAnalysis
We conduct theoretical analysis about the transferability of our transfer attack. Specifically, given the watermarks
decodedbythemsurrogatedecodersfortheperturbedimage,wederivebothanupperboundandalowerboundfor
theprobabilitythatthewatermarkdecodedbythetargetdecoderfortheperturbedimagematcheswiththeground-truth
watermark. AllourproofsareshowninAppendix.
6ATransferAttacktoImageWatermarks
5.1 Notations
WeadoptT todenotethetargetdecoderand S m torepresentthemsurrogatedecoders. w denotestheground-
{ i }i=1
truth watermark and x denotes a watermarked image embedded with w. δ denotes a perturbation that satisfies the
w
constraints of the optimization problem in Equation 3. For simplicity, we assume the target decoder and surrogate
decodersusethesamewatermarklength,whichwedenoteasn. T() andS () denotethejthbitofthewatermarks
j i j
· ·
decodedbyT andS foranimage,respectively. Moreover,weconsiderinversewatermarksasthetargetwatermarks
i
inourtransferattack.
5.2 FormalDefinitionsandAssumptions
To analyze the transferability between surrogate decoders and target decoder, we need to formally quantify their
relationships.Towardsthisgoal,wedefineunperturbedsimilarity,positivetransferringsimilarity,andnegativetrans-
ferringsimilarity. Unperturbedsimilaritymeasurestheprobabilitythatthewatermarksdecodedbythetargetdecoder
andsurrogatedecodersareidenticalforawatermarkedimagex withoutperturbation,whichisformallydefinedas
w
follows:
Definition 1 (Unperturbed similarity). For any watermarked image x and δ satisfying the constraints of the opti-
w
mizationprobleminEquation3,thejthbitofthewatermarkdecodedbyT matcheswiththejthbitofthewatermark
decodedbyS forx withprobabilityk , giventhatthejthbitsofthewatermarksforx +δ andx decodedby
i w ij w w
S areinverse. Conversely,itoccurswithprobabilityk′ giventhatthejthbitsofthewatermarksforx +δandx
i ij w w
decodedbyS areidentical. Formally,fori=1,2, ,mandj =1,2, ,n,wehave:
i
··· ···
Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) )=k ,
w j i w j i w j i w j ij
| − (4)
Pr(T(x ) =S (x ) S (x +δ) =S (x ) )=k′ .
w j i w j | i w j i w j ij
Notethatbothk andk′ canbeestimatedinexperiments. Positivetransferringsimilaritymeasurestheprobability
ij ij
that the watermarks decoded by target decoder and surrogate decoders for a perturbed image x +δ are identical
w
whenthewatermarksdecodedbythesurrogatedecodersareflippedafteraddingtheperturbation,whichisdefinedas
follows:
Definition 2 (Positive transferring similarity). For any watermarked image x and δ satisfying the constraints of
w
the optimization problem in Equation 3, the jth bit of the watermark decoded by T matches with the jth bit of the
watermarkdecodedbyS forx +δwithprobabilitya ,giventhatthejthbitsofthewatermarksdecodedbyT and
i w ij
S areidenticalforx ,andthejthbitsofthewatermarksforx +δandx decodedbyS areinverse. Conversely,
i w w w i
itoccurswithprobabilitya′ giventhatthejthbitsofthewatermarksdecodedbyT andS forx areinverse,and
ij i w
thejthbitsofthewatermarksforx +δ andx decodedbyS arealsoinverse. Formally,fori = 1,2, ,mand
w w i
···
j =1,2, ,n,wehavethefollowing:
···
Pr(T(x +δ) =S (x +δ) T(x ) =S (x ) ,
w j i w j w j i w j
|
S (x +δ) =1 S (x ) )=a ,
i w j i w j ij
− (5)
Pr(T(x +δ) =S (x +δ) T(x ) =1 S (x ) ,
w j i w j w j i w j
| −
S (x +δ) =1 S (x ) )=a′ .
i w j − i w j ij
Both a and a′ can be estimated in experiments. Similarly, we use negative transferring similarity to measure the
ij ij
probabilitythatthewatermarksdecodedbytargetdecoderandsurrogatedecodersforaperturbedimagex +δ are
w
identicalwhentheoutputsofsurrogatedecodersremainunchangedafteraddingtheperturbation,whichisdefinedas
follows:
Definition 3 (Negative transferring similarity). For any watermarked image x and δ satisfying the constraints of
w
the optimization problem in Equation 3, the jth bit of the watermark decoded by T matches with the jth bit of the
watermarkdecodedbyS forx +δwithprobabilityb ,giventhatthejthbitsofthewatermarksdecodedbyT and
i w ij
S areinverseforx ,andthejthbitsofthewatermarksforx +δandx decodedbyS areidentical. Conversely,
i w w w i
itoccurswithprobabilityb′ giventhatthejthbitsofthewatermarksdecodedbyT andS forx areidentical,and
ij i w
thejthbitsofthewatermarksforx +δandx decodedbyS arealsoidentical. Formally,fori=1,2, ,mand
w w i
···
j =1,2, ,n,wehavethefollowing:
···
Pr(T(x +δ) =S (x +δ) T(x ) =1 S (x ) ,
w j i w j w j i w j
| −
S (x +δ) =S (x ) )=b ,
i w j i w j ij
(6)
Pr(T(x +δ) =S (x +δ) T(x ) =S (x ) ,
w j i w j w j i w j
|
S (x +δ) =S (x ) )=b′ .
i w j i w j ij
7ATransferAttacktoImageWatermarks
Both b and b′ can be estimated in experiments. To quantify the magnitude of our transfer attack, we formally
ij ij
define the q-attacking strength which measures the probability that the watermarks for the perturbed image and the
watermarkedimagedecodedbysurrogatedecodersareinverseasfollows:
Definition4(q-attackingstrength). Foranywatermarkedimagex andδsatisfyingtheconstraintsoftheoptimization
w
probleminEquation3,thejthbitofthewatermarkdecodedbyS fortheperturbedimageistheinverseofthejthbit
i
ofthewatermarkdecodedbyS forthewatermarkedimagewithprobabilityq . Formally,wehavethefollowing:
i ij
Pr(S (x +δ) =1 S (x ) )=q . (7)
i w j i w j ij
−
q can also be estimated in experiments. To quantify the performance of the target watermarking model when no
ij
perturbationsareaddedtowatermarkedimages, weintroducethefollowingdefinitiononβ-accuratewatermarking:
assumethebitwiseaccuracybetweenthewatermarkdecodedbythetargetdecoderforthewatermarkedimageandthe
ground-truthwatermarkfollowingPoissonbinomialdistribution. Specifically,wehavethefollowingassumption:
Definition 5 (β-accurate watermarking). For any watermarked image x , the bits of the watermark decoded by T
w
are mutually independent and the probability that the jth bit of the decoded watermark matches with that of the
ground-truthwatermarkwisβ ,where1 j n.
j
≤ ≤
β ,β , ,β can be estimated in experiments. Next, we introduce three assumptions regarding the surrogate de-
1 2 n
···
codersandthetargetdecoder. Forbit-leveldependency,itisassumedthateachbitofthewatermarkdecodedbythe
targetdecoderonlydependsonthecorrespondingbitsofthewatermarksdecodedbythesurrogatedecoders.Formally,
wehavetheassumptionasfollows:
Assumption 1 (Bit-level dependency). For any watermarked image x and δ satisfying the constraints of the opti-
w
mizationprobleminEquation3,eachbitofthewatermarkdecodedbyT fortheperturbedimageonlydependsonthe
correspondingbitsofthewatermarksdecodedbythesurrogatedecodersfortheperturbedimage. Formally,wehave
thefollowing:
Pr(T(x +δ) S (x +δ), ,S (x +δ))
w j 1 w m w
| ··· (8)
=Pr(T(x +δ) S (x +δ) , ,S (x +δ) ), j.
w j 1 w j m w j
| ··· ∀
Additionally, since all the surrogate decoders are trained independently with different subsets of data, we further
assumethatthewatermarksdecodedbythesurrogatedecodersareindependentandconditionallyindependentgiven
thewatermarkdecodedbythetargetdecoderforthewatermarkedorperturbedimage.
Assumption2(Independency). Foranywatermarkedimagex andδ satisfyingtheconstraintsoftheoptimization
w
probleminEquation3,thejthbitsofthewatermarksdecodedbythemsurrogatedecodersfortheperturbedimage
areindependent. Formally,wehavethefollowing:
Pr(S (x +δ) , ,S (x +δ) )
1 w j m w j
··· (9)
=Pr(S (x +δ) ) Pr(S (x +δ) ), j.
1 w j m w j
··· ∀
Assumption 3 (Conditional independency). For any watermarked image x and δ satisfying the constraints of the
w
optimizationprobleminEquation3,thejthbitsofthewatermarksdecodedbythemsurrogatedecodersforx +δ
w
areindependentwhenthejthbitofthewatermarkdecodedbythetargetdecoderforx orx +δisgiven. Formally,
w w
wehave:
Pr(S (x +δ) , ,S (x +δ) T(x ) )
1 w j m w j w j
··· |
=Pr(S (x +δ) T(x ) )
1 w j w j
| ···
Pr(S (x +δ) T(x ) ), j,
m w j w j
| ∀ (10)
Pr(S (x +δ) , ,S (x +δ) T(x +δ) )
1 w j m w j w j
··· |
=Pr(S (x +δ) T(x +δ) )
1 w j w j
| ···
Pr(S (x +δ) T(x +δ) ), j.
m w j w j
| ∀
5.3 DerivingPr(T(x +δ) =1 T(x ) )
w j w j
−
Wefirstderivetheprobabilitythatthewatermarkdecodedbythetargetdecoderisflippedafteraddingtheperturbation
foundbyourtransferattacktothewatermarkimage,conditionedonthatthewatermarkdecodedbyasurrogatedecoder
isflippedornotafteraddingtheperturbation. Formally,wehavethefollowinglemma:
8ATransferAttacktoImageWatermarks
Lemma1. Foranywatermarkedimagex andδsatisfyingtheconstraintsoftheoptimizationprobleminEquation3,
w
the jth bit of the watermark decoded by T for x +δ is the inverse of the jth bit of the watermark decoded by T
w
forx withprobabilitya k +(1 a′ )(1 k′ ),giventhatthejthbitsofthewatermarksforthex +δ andx
w ij ij − ij − ij w w
decodedbyS areinverse. Conversely,itoccurswithprobability(1 b′ )k′ +b (1 k′ )giventhatthejthbitsof
i − ij ij ij − ij
thewatermarksforx +δandx decodedbyS areidentical. Formally,wehave:
w w i
Pr(T(x +δ) =1 T(x ) S (x +δ) =1 S (x ) )
w j w j i w j i w j
− | −
=c ,
ij
(11)
Pr(T(x +δ) =1 T(x ) S (x +δ) =S (x ) )
w j w j i w j i w j
− |
=c′ ,
ij
wherec =a k +(1 a′ )(1 k )andc′ =(1 b′ )k′ +b (1 k′ ).
ij ij ij − ij − ij ij − ij ij ij − ij
Then,wederivetheunconditionalprobabilitythatthewatermarkdecodedbythetargetdecoderisflippedafteradding
theperturbationtothewatermarkedimageasfollows:
Theorem1. Foranywatermarkedimagex andδsatisfyingtheconstraintsoftheoptimizationprobleminEquation3,
w
thejthbitofthewatermarkdecodedbyT fortheperturbedimageistheinverseofthejthbitofthewatermarkdecoded
byT forthewatermarkedimagewithprobabilityc q +c′ (1 q ). Formally,wehavethefollowing:
ij ij ij − ij
Pr(T(x +δ) =1 T(x ) )=e , (12)
w j w j j
−
wheree =c q +c′ (1 q ), i.
j ij ij ij − ij ∀
Thenwederivetheprobabilitythatthewatermarksdecodedbythesurrogatedecodersareflippedwhenthewatermark
decodedbythetargetdecoderisflippedafteraddingtheperturbationasfollows:
Lemma2. Foranywatermarkedimagex andδsatisfyingtheconstraintsoftheoptimizationprobleminEquation3,
w
thejthbitofthewatermarkdecodedbyS fortheperturbedimageistheinverseofthejthbitofthewatermarkdecoded
i
byS forthewatermarkedimagewithprobability cijqij ,giventhatthewatermarksfortheperturbedimage
i cijqij+c′ ij(1−qij)
andthewatermarkedimagedecodedbyT areinverse. Formally,wehavethefollowing:
Pr(S (x +δ) =1 S (x ) T(x +δ) =1 T(x ) )
i w j i w j w j w j
− | −
= c ijq ij . (13)
c q +c′ (1 q )
ij ij ij − ij
Finally,wederivetheprobabilitythatthewatermarkdecodedbythetargetdecoderisflippedafteraddingthepertur-
bationtothewatermarkedimage,giventhewatermarksdecodedbythemsurrogatedecodersfortheperturbedimage.
Formally,wehavethefollowing:
Theorem2. Foranywatermarkedimagex andδsatisfyingtheconstraintsoftheoptimizationprobleminEquation3,
w
thejthbitofthewatermarkdecodedbyT fortheperturbedimageistheinverseofthejthbitofthewatermarkdecoded
byT forthewatermarkedimagewithprobabilityp ,whenthewatermarksfortheperturbedimagedecodedbythem
j
surrogatedecodersaregiven. Formally,wehavethefollowing:
Pr(T(x +δ) =1 T(x ) S (x +δ) , ,S (x +δ) )
w j w j 1 w j m w j
− | ··· (14)
=p ,
j
wherep =min(e ∏︁ cij ∏︁ c′ ij ,
j j i∈Mj1 cijqij+c′ ij(1−qij) i∈Mj2 cijqij+c′ ij(1−qij)
1),M = i S (x +δ) = 1 S (x ) ,andM = i S (x +δ) = S (x ) . M (orM )istheset
j1 i w j i w j j2 i w j i w j j1 j2
{ | − } { | }
ofsurrogatedecoderswhosejthbitsofthedecodedwatermarksflip(ornotflip)afteraddingtheperturbationtothe
watermarkedimage.
5.4 DerivingPr(T(x +δ) =w )
w j j
ThebitwiseaccuracybetweenthedecodedwatermarkT(x +δ)andtheground-truthwatermarkwisusedtodetect
w
whethertheperturbedimagex +δ isAI-generated. Therefore, giventhewatermarksdecodedbythemsurrogate
w
decoders for the perturbed image x +δ, we derive an upper bound and a lower bound of the probability that the
w
watermark decoded by the target decoder matches with the ground-truth watermark after adding the perturbation.
Formally,basedonthetheoremsinSection5.3,wefurtherderivethefollowingtheorem:
9ATransferAttacktoImageWatermarks
Theorem3. Givenanywatermarkedimagex ,δsatisfyingtheconstraintsoftheoptimizationprobleminEquation3,
w
and the watermarks decoded by the m surrogate decoders for the perturbed image, we have an upper bound and a
lowerboundoftheprobabilitythatthejthbitofthewatermarkdecodedbyT fortheperturbedimagematcheswith
thejthbitoftheground-truthwatermarkwasfollows:
Pr(T(x +δ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| ···
max(β p ,0),
j j
≥ − (15)
Pr(T(x +δ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| ···
1 p +β 1.
j j
≤ −| − |
Such upper bound and lower bound quantify the transferability of our transfer attack. Note that we derive both an
upperboundandalowerboundoftheprobabilitybecauseweconsiderdouble-taildetectors.
6 Experiments
6.1 Experimentalsetup
Datasets: In our experiments, we utilize AI-generated images for watermarking model training and testing. For
the target watermarking models, we use two public datasets [21, 22] generated by Stable Diffusion and Midjourney
respectively. FollowingHiDDeN[2], werandomlyselect10,000imagesfromeachdatasettotrainthetargetwater-
markingencodersanddecoders. Fortesting,werandomlysample1,000imagesfromthetestingsetofeachdataset,
embed the ground-truth watermark into each of them using a target encoder, and then find the perturbation to each
watermarkedimageusingdifferentmethods. Totrainthesurrogatewatermarkingmodels,wesample10,000images
fromanotherpublicdataset[23]generatedbyDALL-E2,i.e.,thesurrogatedatasetconsistsofthese10,000images.
Theinputimagesizeofthewatermarkingmodelsis128 128pixels.
×
Target and surrogate watermarking model architectures and watermark lengths: For surrogate watermarking
models,weemploytheconvolutionneuralnetworkarchitecturedescribedinHiDDeNwiththewatermarklengthof30
bits.Weusethesamearchitectureandwatermarklengthforallsurrogatewatermarkingmodelsfortwomainpurposes.
Firstly,itcangiveusaclearerunderstandingofourattack’stransferability,especiallywhenthetargetwatermarking
modelandsurrogatewatermarkingmodelshavedifferentarchitecturesandwatermarklengths. Secondly,itallowsfor
abetteranalysisoftheimpactofthenumberofsurrogatewatermarkingmodelsonourattack’stransferability,without
theconfoundingeffectsofvariationsinarchitectureandwatermarklengthofthesurrogatewatermarkingmodels.
For the target watermarking model, we use different choices of architectures and watermark lengths to analyze the
transferabilityofourtransferattackindifferentscenarios. Specifically,weconsiderthefollowingarchitecturesforthe
targetwatermarkingmodel.
CNN.Thetargetwatermarkingmodelhasconvolutionneuralnetworkarchitecture. Inparticular,theencoderconsists
of4convolutionalblocks,whilethedecoderconsistsof7convolutionalblocks. EachblockintegratesaConvolution
layer,BatchNormalization,andReLUactivation.
ResNet. Tofurtherevaluateourtransferattackwhenthetargetwatermarkingmodel’sarchitectureismoresophisti-
cated than that of the surrogate watermarking models, we also adopt ResNet for the target watermarking model. In
particular,theencoderconsistsof7convolutionalblocksandthedecoderistheResNet-18.
Additionally,wealsoemploydifferentwatermarklengthsforthetargetwatermarkingmodels. Specifically,weeval-
uate our transfer attack on the target watermarking models with watermark lengths of 20 bits, 30 bits, and 64 bits.
Thesevariationsincludewatermarklengthsthatareshorterthan,equalto,andlongerthanthoseusedbythesurrogate
watermarking models, offering a thorough analysis of our transfer attack’s performance across different watermark
lengthsinthetargetwatermarkingmodel.
Common post-processing methods: These methods can be applied in the no-box setting. Therefore, we compare
ourtransferattackwiththem. Specifically,weconsiderthefollowingcommonpost-processingmethods.
JPEG.Itisacommonlyusedcompressionmethodindigitalimaging,whichcanreduceimagefilesizeswhilepreserv-
ingareasonablelevelofimagequality. ThequalityofimagesprocessedthroughJPEGisgovernedbyaqualityfactor
Q. Asthequalityfactordecreases,detectingthewatermarkinthepost-processedimagesbecomesmorechallenging,
althoughthisalsoresultsinalowerimagequality.
10ATransferAttacktoImageWatermarks
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
StableDiffusion StableDiffusion StableDiffusion
0.0 Midjourney 0.0 Midjourney 0.0 Midjourney
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
#ofSurrogateModels #ofSurrogateModels #ofSurrogateModels
1.0 1.0 1.0
StableDiffusion StableDiffusion
0.8 Midjourney 0.8 0.8 Midjourney
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
StableDiffusion
0.0 0.0 Midjourney 0.0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
#ofSurrogateModels #ofSurrogateModels #ofSurrogateModels
(a)20bits (b)30bits (c)64bits
Figure 1: Evasion rate of our transfer attack when the target model uses CNN (first row) or ResNet (second row)
architecture and different watermark lengths (the three columns). The surrogate models use CNN architecture and
watermarklengthof30bits.
Gaussiannoise. ThismethodinvolvesaddingstatisticalnoisethatfollowsaGaussiandistributionwithazeromean
and a standard deviation of σ, which effectively simulates various environmental noise effects encountered in real-
worldscenarios. Alargerσvalueleadstoincreaseddifficultyinwatermarkdetection,butalsoresultsinlowerimage
quality.
Gaussianblur. Thismethodsmoothstheimagebyaveragingpixelvalueswiththeirneighbors. ItappliesaGaussian
filter with kernel size of k k to an image, characterized by a bell-shaped curve with a zero mean and a standard
×
deviation of σ. A larger σ causes more pronounced blurring, which results in lower watermark detection rate and
imagequality. FollowingJiangetal.[11],wesetk =5andvaryσ.
Brightness/Contrast.Thismethodmodifiesthebrightnessandcontrastofanimagebyadjustingpixelvaluesthrough-
outtheimage. Specifically,itoperatesbyeitherincreasingordecreasingthesevaluestomaketheimagebrighteror
darker. Theprocessisregulatedbytwofactors: abrightnessfactorBandacontrastfactorC. Formally,foreachpixel
valuev,themethodtransformsittoCv+B. FollowingJiangetal.[11],wesetB =0.2andvaryC.
Transferattacks: Wecomparewiththefollowingattacks.
WEvade-B-S [11]. This method trains one surrogate watermarking model. During attack, PGD is employed to
perturb a watermarked image such that the watermark decoded by the surrogate decoder for the perturbed image
matcheswithapresetrandomlygeneratedwatermark. Inourexperiments,wetrainthesurrogatewatermarkingmodel
on10,000imagesgeneratedbyDALL-E2. Togiveadvantagestothistransferattack,weassumetheattackerknows
thearchitectureofthetargetwatermarkingmodelandusesitasthearchitectureofthesurrogatewatermarkingmodel.
AdvEmb-RN18[12]. ThismethodusesaResNet-18pretrainedonImageNettogenerateafeatureembeddingfora
watermarkedimage. ThenitusesPGDtoperturbtheimagesuchthattheembeddingoftheperturbedimageliesfar
fromtheoneofthewatermarkedimage.
AdvCls-Real&WM [12]. This method involves training a surrogate classifier using both watermarked and non-
watermarkedimages. ThewatermarkedimagesaregeneratedbythetargetGenAIandarewatermarkedbythetarget
watermarking model, and the non-watermarked images are drawn from a distribution different from the one of the
imagesgeneratedbythetargetGenAI.Duringattack,PGDisemployedtoperturbawatermarkedimagesuchthatitis
misclassifiedbythesurrogateclassifierasanon-watermarkedimage. Inourexperiments,weutilizeimagesgenerated
byDALL-E2asthenon-watermarkedimages.Morespecifically,thetrainingsetforthesurrogateclassifiercomprises
8,000 images generated by Stable Diffusion (or Midjourney) and watermarked by the target watermarking model,
and 8,000 non-watermarked images generated by DALL-E 2. The surrogate classifier is based on the ResNet-18
architecture.
11
etaRnoisavE
etaRnoisavE
etaRnoisavE
etaRnoisavE
etaRnoisavE
etaRnoisavEATransferAttacktoImageWatermarks
0.25 0.25 0.25
StableDiffusion StableDiffusion StableDiffusion
0.20 Midjourney 0.20 Midjourney 0.20 Midjourney
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
0.00 0.00 0.00
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate EvasionRate
0.25 0.25 0.25
StableDiffusion StableDiffusion StableDiffusion
0.20 Midjourney 0.20 Midjourney 0.20 Midjourney
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
0.00 0.00 0.00
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate EvasionRate
(a)20bits (b)30bits (c)64bits
Figure2: AverageperturbationofourtransferattackwhenthetargetmodelusesCNN(firstrow)orResNet(second
row) architecture and different watermark lengths (the three columns). The surrogate models use CNN architecture
andwatermarklengthof30bits.
1.25 1.25 1.25
JPEG JPEG JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
1.00 Gaussianblur 1.00 Gaussianblur 1.00 Gaussianblur
Brightness/Contrast Brightness/Contrast Brightness/Contrast
Ours Ours Ours
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8
EvasionRate EvasionRate EvasionRate
(a)20bits (b)30bits (c)64bits
Figure3:Comparingtheaverageperturbationsofourtransferattackandcommonpost-processingmethodswhenthey
achievethesameevasionrate. ThetargetmodelusesResNetarchitectureanddifferentwatermarklengths(thethree
columns). DatasetisStableDiffusion. ResultsforMidjourneyareshowninFigure9inAppendix.
AdvCls-WM1&WM2[12]. ThismethodisthesameasAdvCls-Real&WMexceptforthetrainingdatausedforthe
surrogateclassifier. Itassumesthattheattackerhasaccesstowatermarkedimagesgeneratedbythetargetwatermark-
ing model with two different watermarks. The surrogate classifier is trained to distinguish the watermarked images
embeddedwithonewatermarkandthoseembeddedwiththeotherwatermark. Inourexperiments,thetrainingsetfor
thesurrogateclassifiercomprises8,000imageswatermarkedbythetargetwatermarkingmodelwithonewatermark,
and8,000imageswatermarkedbythetargetwatermarkingmodelwiththeotherwatermark,wherebothwatermarks
arerandomlypicked.
Evaluationmetrics: Wemainlyusetwometrics: evasionrateandaverageperturbation. Evasionrateisdefinedas
the proportion of the perturbed images that evade the target watermark-based detector. Average perturbation is the
perturbationaddedtoawatermarkedimagemeasuredbyℓ -normaveragedacross1,000watermarkedimagesinthe
∞
testingset. FollowingJiangetal.[11],giventhatthepixelvaluerangeof[0,255]isnormalizedto[-1,1],weadjust
theperturbationbydividingitby2. Thisadjustmentensuresthattheperturbationisexpressedasafractionofthefull
pixelvaluerange[0,255].
Parametersettings: Weuseadversarialtrainingtotrainwatermarkingmodelssinceitenhancesrobustness. Specifi-
cally,forcommonpost-processingmethods,weconsiderthefollowingrangeofparametersduringadversarialtraining
foratargetwatermarkingmodel: Q [10,99]forJPEG,σ [0,0.1]forGaussiannoise, σ [0,2.0]forGaussian
∈ ∈ ∈
12
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
∞
∞
∞
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
∞
∞
∞
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
noitabrutreP
‘egarevA
∞
∞
∞ATransferAttacktoImageWatermarks
blur, and C [1,3] for Brightness/Contrast. For surrogate watermarking models, we adopt a smaller range of pa-
∈
rameters for some common post-processing methods during adversarial training to achieve weaker robustness than
the target watermarking model as follows: Q [50,99] for JPEG, σ [0,0.1] for Gaussian noise, σ [0,1.0] for
∈ ∈ ∈
Gaussianblur,andC [1,3]forBrightness/Contrast.
∈
By default, we set max iter = 5,000, r = 0.25, ϵ = 0.2, and α = 0.1 for our transfer attack. Unless otherwise
mentioned,weuseInverse-Decodetoselectatargetwatermarkforasurrogatedecoder,andEnsemble-Optimizationto
findtheperturbation. αisincreasedwhenthenumberofsurrogatewatermarkingmodelsincreasesinordertosatisfy
theconstraintsofouroptimizationproblemwithin5,000iterations. Thedetailedsettingsforαfordifferentnumber
ofsurrogatemodelsareshowninTable1inAppendix. Moreover,weuseℓ -distanceasthedistancemetricl(, )for
2
· ·
twowatermarks.
Forthedetectionthresholdτ,wesetitbasedonthewatermarklengthofthetargetwatermarkingmodel. Specifically,
we set τ to be a value such that the false positive rate of the watermark-based detector is no larger than 10−4 when
thedouble-taildetectorisemployed. Specifically,τ issettobe0.9,0.83,and0.73forthetargetwatermarkingmodels
withwatermarklengthsof20bits,30bits,and64bits,respectively.
6.2 ExperimentalResults
Ourtransferattackissuccessful: Figure1andFigure2respectivelyshowtheevasionrateandaverageperturbation
ofourtransferattackasthenumberofsurrogatemodelsincreaseswhenthetargetmodelusesdifferentneuralnetwork
architectures and watermark lengths on the two datasets. First, we observe that our transfer attack is successful in
evadingwatermark-baseddetection.Theevasionrateishigherthan85%andtheaverageperturbationisnolargerthan
0.25when100surrogatemodelsareused,nomatterwhatarchitecturesandwatermarklengthsareusedbythetarget
model.
Second, when the target model’s architecture is more complex than the surrogate models’ architecture, our attack
requiresmoresurrogatemodelsandlargeraverageperturbationtosucceed. Forexample,ourattackuses40surrogate
modelsandaverageperturbation0.09toachieveanevasionratearound100%whenthetargetmodelisCNNanduses
30-bit watermarks on Stable Diffusion, while our attack uses 60 surrogate models and average perturbation 0.14 to
achieve a similar evasion rate when the target model is ResNet and uses 30-bit watermarks. Third, when the target
modelusesalonger/shorterwatermarkthanthesurrogatemodels,ourattackrequiresmoresurrogatemodelsandlarger
averageperturbationtosucceed. Forexample, ourattackuses40surrogatemodelsandaverageperturbation0.08to
achieve an evasion rate around 85% when the target model is ResNet and uses 30-bit watermarks, while our attack
uses100surrogatemodelsandaverageperturbation0.15toachieveasimilarevasionratewhenthetargetmodeluses
64-bitwatermarks.
Comparewithcommonpost-processing: Figure3comparestheaverageperturbationsofcommonpost-processing
methods and our transfer attack when they achieve the same evasion rates for different target models on the two
datasets. Given an evasion rate achieved by our transfer attack using a certain number of surrogate models, we set
the parameter of a common post-processing method such that it achieves very close evasion rate. We observe that
ourtransferattackaddsmuchsmallerperturbationsthancommonpost-processingmethodswhenachievingthesame
evasionrate. Notethatforsometargetmodels,theBrightness/Contrastmethodcanonlyachieveabout75%evasion
rateatmost. Thus,thecurvesforBrightness/Contrastareshorterinthecorrespondinggraphs.
To get a more comprehensive comparison with common post-processing methods, Figure 10 and Figure 11 in Ap-
pendixshowthecomparisonresultsusingaverageperturbationmeasuredbyℓ -normandaverageSSIM,respectively.
2
SSIMmeasuresthesimilaritybetweenaperturbedimageandthecorrespondingwatermarkedimagebyconsidering
changesintexture,luminance,andcontrast. AverageSSIMistheSSIMaveragedacross1,000watermarkedimages
andtheirperturbedversionsinthetestingset.Westillobservethatourtransferattackaddsmuchsmallerperturbations
andthusbettermaintainstheimages’visualqualitythancommonpost-processingmethodswhenℓ -normorSSIMis
2
usedtomeasuretheperturbation.
Compare with existing transfer attacks: Figure 4 compares the evasion rates of existing and our transfer attacks
whenthetargetmodelisResNetanduseswatermarkswithdifferentlengths. WenotethatsinceAdvCls-Real&Wm
and AdvCls-WM1&WM2 can only achieve average perturbation around 0.1 no matter how large the perturbation
budget r we set, we add one extra variant of our method with perturbation budget r = 0.1 for comparison. For
other transfer attacks, the perturbation budget r is set to be 0.25. We observe that our transfer attack is much more
effectivethanotherexistingtransferattacks. First,weobservethatexistingtransferattacksachievealmost0evasion
rates except that AdvCls-WM1&WM2 achieves around 10% evasion rates on the Stable Diffusion dataset when the
targetmodel’swatermarklengthis20or30bits. Thisisthereasonthatexistingworksconcludedthatwatermarking
isrobustintheno-boxsetting. However,ourtransferattackachievesmuchhigherevasionratesespeciallywhenthe
13ATransferAttacktoImageWatermarks
1.0
0.8
0.6
0.4
0.2
0.0
20bits 30bits 64bits
AdvEmb-RN18(r=0.25) AdvCls-WM1&WM2 Ours(r=0.1)
AdvCls-Real&WM WEvade-B-S(r=0.25) Ours(r=0.25)
Figure 4: Comparing evasion rates of existing and our transfer attacks when the target model is ResNet and uses
watermarkswithdifferentlengths. DatasetisStableDiffusion. SimilarresultsforMidjourneyareshowninFigure12
inAppendix.
1.0 (cid:15)=0.5
0.100 (cid:15)=0.4
0.8 (cid:15)=0.3
(cid:15)=0.2
0.075 (cid:15)=0.1
0.6
0.050 0.4
(cid:15)=0.5
0.2 (cid:15)=0.4 0.025
(cid:15)=0.3
(cid:15)=0.2
0.0 (cid:15)=0.1 0.000
0 20 40 60 80 100 0.00 0.25 0.50 0.75 1.00
#ofSurrogateModels EvasionRate
(a)Evasionrate (b)ℓ -normperturbation
∞
Figure5: Evasionrateandaverageℓ -normperturbationofourtransferattackwhenusingdifferentϵ.
∞
perturbationbudgetislargeenough(e.g., r = 0.25). WenotethattheaverageSSIMofourtransferattackishigher
than0.97forr = 0.1andhigherthan0.92forr = 0.25,whichmeansthattheimagequalityoftheperturbedimages
ismaintainedwell.
Impact of ϵ: Figure 5 shows the evasion rate and average ℓ -norm perturbation when our transfer attack uses
∞
different ϵ on Stable Diffusion dataset, where the target model is CNN and uses 30-bit watermarks. Figure 13 in
Appendixshowstheresultsonaverageℓ -normperturbationandaverageSSIM.Weobservethatourattackrequires
2
less surrogate models to achieve the same evasion rate when ϵ is smaller. However, when ϵ is too small (e.g., 0.1),
the evasion rate first increases and then decreases as the number of surrogate models increases. This is because the
bitwiseaccuracybetweenthedecodedwatermarkoftheperturbedimageandtheground-truthwatermarkbecomestoo
smallwhenϵistoosmall, andthustheperturbedimageisstilldetectedasAI-generatedbythedouble-taildetector.
Moreover,weobservethat,fordifferentϵ,theaverageperturbation(measuredbyℓ -norm,ℓ -norm,orSSIM)keeps
∞ 2
almostthesamewhenachievingthesameevasionrate. Thisisbecausetheaverageperturbationandtheevasionrate
havestrongcorrelationsinourattack.
Differentvariantsofourtransferattack: Ourattackhastwosteps,eachofwhichhasmultipledesignchoices. We
useRD,RS,andIDtorespectivelydenoteRandom-Different,Random-Same,andInverse-Decodeinthefirststep;and
PAandEOtorespectivelydenotePost-AggregateandEnsemble-Optimizationinthesecondstep. Weconcatenatethe
symbolstorepresentavariant,e.g.,RD-PAmeansusingRDandPAasthetwosteps. Moreover,whenPAisused,we
furtherusesuffix”Mean”or”Median”toindicatetheadoptedaggregationrule. Forinstance,RD-PA-Meanindicates
thatMeanisusedastheaggregationrule.
Figure6showstheevasionrateandaverageℓ -normperturbationwhendifferentvariantsofourtransferattackare
∞
usedonStableDiffusiondataset,wherethetargetmodelisCNNanduses30-bitwatermarks. Figure13inAppendix
showstheresultsonaverageℓ -normperturbationandaverageSSIM.WeobservethatallthethreevariantsusingEO
2
canevadedetectionsuccessfullywhilenoneofthevariantsusingPAcanevadedetection,whichindicatesthatEOis
muchmoreeffectiveataggregatingsurrogatedecodersthanPA.Additionally,withinthethreevariantsusingEO,RD
andRShavesimilarresultsforallmetricsandarelesseffectivethanID.Specifically,ID-EOachieves100%evasion
ratewithlessnumberofsurrogatemodelscomparedtoRD-EOandRS-EO.Forthesame100%evasionrate,ID-EO
14
etaRnoisavE
etaRnoisavE
noitabrutreP
‘egarevA
∞ATransferAttacktoImageWatermarks
1.0 0.25 RD-PA-Mean
RS-PA-Mean
0.8 0.20 ID-PA-Mean
RD-PA-Median
RS-PA-Median
0.6 RD-PA-Mean 0.15 ID-PA-Median
RS-PA-Mean RD-EO
ID-PA-Mean RS-EO 0.4 RD-PA-Median 0.10 ID-EO
RS-PA-Median
0.2 ID-PA-Median 0.05
RD-EO
RS-EO
0.0 ID-EO 0.00
0 20 40 60 80 100 0.00 0.25 0.50 0.75 1.00
#ofSurrogateModels EvasionRate
(a)Evasionrate (b)ℓ -normperturbation
∞
Figure6: Evasionratesandaverageℓ -normperturbationsofdifferentvariantsofourtransferattack.
∞
1.0 1.0
Empirical Empirical
Theoreticallowerbound Theoreticallowerbound
0.8 Theoreticalupperbound 0.8 Theoreticalupperbound
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0 20 40 60 80 100 0 20 40 60 80 100
#ofSurrogateModels #ofSurrogateModels
(a)CNN (b)ResNet
Figure7: Comparingempiricalbitwiseaccuracyandtheoreticalboundsofourtransferattackwhenthetargetmodel
is(a)CNNand(b)ResNet.
achievesamuchloweraverageℓ -normandℓ -normperturbationandamuchhigheraverageSSIMthanRD-EOand
∞ 2
RS-EO.
Theoretical vs. empirical results: Figure 7 shows the empirical average bitwise accuracy between the watermark
decoded by the target decoder for a perturbed image and the ground-truth one, and our theoretical upper and lower
boundsonStableDiffusiondataset. Thetheoreticalboundsareestimatedasfollows:wefirstmeasuretheunperturbed
similarity,transferringsimilarity,andattackingstrengthusingthe1,000testingimages;thenwecomputetheprobabil-
ityp foreachjthbitbasedonTheorem2,followingwhichwecomputetheupperandlowerboundsoftheprobability
j
for each bit of the watermark decoded by the target decoder to match with that of the ground-truth watermark; and
finallyweaveragetheseboundsacrossthebits. Weobservethatourtheoreticalupperandlowerboundsmatchwell
withtheempiricalbitwiseaccuracy. Specifically,theempiricalresultsfallwithinourtheoreticalboundsexceptafew
cases. Suchexceptioncaseshappenbecausethetheoreticalboundsareestimatedusingonly1,000images,resulting
ininaccurateestimations.
7 DiscussionandLimitations
Wefocusontransferattackstolearning-basedtargetwatermarkingmodelsbecauselearning-basedmodelsaremore
robustthannon-learning-basedones. Forinstance, commonpost-processinglikeJPEGcompressioncanalreadyre-
movewatermarksembeddedbyanon-learning-basedmodel,butlearning-basedmodelshavegoodrobustnessagainst
commonpost-processing[11]. However,ourattackcanalsobeappliedtonon-learning-basedmodels. Forinstance,
Figure8showstheevasionrateofourtransferattackwhenthetargetmodelisthewatermarkingmodelusedbyStable
Diffusion [24] as the number of surrogate modelsincreases on the two datasets, where the surrogate models are the
sameasthoseinSection6.Ourresultsshowthatourtransferattackcanstillachievehighevasionrateswhenthetarget
modelisnon-learning-based. Comparedtowhenthetargetmodelislearning-based,ourtransferattackrequiresmore
surrogatemodelstoachievehigh(e.g.,100%)evasionrates. Thisisbecausenon-learning-basedwatermarkingmodel
issubstantiallydifferentfromlearning-based, andthusourattackneedsmoresurrogatemodelstoincreasediversity
andthusenhancetransferability.
15
etaRnoisavE
ycaruccAesiwtiBegarevA
noitabrutreP
‘egarevA
ycaruccAesiwtiBegarevA
∞ATransferAttacktoImageWatermarks
1.0
StableDiffusion
Midjourney
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
#ofSurrogateModels
Figure8: Evasionrateofourtransferattackwhenthetargetmodelisnon-learning-based.
8 ConclusionandFutureWork
Inthiswork,wefindthatwatermark-baseddetectionofAI-generatedimagesisnotrobusttotransferattacksintheno-
boxsetting. Givenawatermarkedimage,anattackercanremovethewatermarkbyaddingaperturbationtoit,where
theperturbationcanbefoundbyensemblingmultiplesurrogatewatermarkingmodels. Ourresultsshowthattransfer
attackbasedonsurrogatewatermarkingmodelsoutperformsthosebasedonsurrogateclassifiersthattreatawatermark-
baseddetectorasaconventionalclassifier.Moreover,leveragingsurrogatewatermarkingmodelsenablesustoperform
arigorousanalysisonthetransferabilityofourattack. Interestingfutureworkincludesextendingourtransferattack
totextandaudiowatermarks,aswellasdesigningmorerobust,especiallycertifiablyrobust,watermarks.
References
[1] NingBi,QiyuSun,DarenHuang,ZhihuaYang,andJiwuHuang. Robustimagewatermarkingbasedonmulti-
bandwaveletsandempiricalmodedecomposition. IEEETransactionsonImageProcessing,2007.
[2] JirenZhu,RussellKaplan,JustinJohnson,andLiFei-Fei.Hidden:Hidingdatawithdeepnetworks.InEuropean
ConferenceonComputerVision,2018.
[3] Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical photographs. In
IEEEConferenceonComputerVisionandPatternRecognition,2020.
[4] Chaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, and In So Kweon. Udh: Universal deep hiding for
steganography,watermarking,andlightfieldmessaging.InAdvancesinNeuralInformationProcessingSystems,
2020.
[5] AliAl-Haj. Combineddwt-dctdigitalimagewatermarking. Journalofcomputerscience,2007.
[6] SaharAbdelnabiandMarioFritz. Adversarialwatermarkingtransformer: Towardstracingtextprovenancewith
datahiding. InIEEESymposiumonSecurityandPrivacy,2021.
[7] JohnKirchenbauer,JonasGeiping,YuxinWen,JonathanKatz,IanMiers,andTomGoldstein. Awatermarkfor
largelanguagemodels. InInternationalConferenceonMachineLearning,2023.
[8] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,KamyarGhasemipour,
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-imagediffusionmodels
withdeeplanguageunderstanding. InAdvancesinNeuralInformationProcessingSystems,2022.
[9] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shottext-to-imagegeneration. InInternationalConferenceonMachineLearning,2021.
[10] RobinRombach.Stablediffusionwatermarkdecoder.https://github.com/CompVis/stable-diffusion/
blob/main/scripts/tests/test_watermark.py,2022.
[11] ZhengyuanJiang,JinghuaiZhang,andNeilZhenqiangGong.Evadingwatermarkbaseddetectionofai-generated
content. InACMConferenceonComputerandCommunicationsSecurity,2023.
[12] Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu,
Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, et al. Benchmarking the robustness of image watermarks.
arXivpreprintarXiv:2401.08573,2024.
16
etaRnoisavEATransferAttacktoImageWatermarks
[13] AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu. Towardsdeep
learningmodelsresistanttoadversarialattacks. InInternationalConferenceonLearningRepresentations,2018.
[14] S.PereiraandT.Pun. Robusttemplatematchingforaffineresistantimagewatermarks. IEEETransactionson
ImageProcessing,2000.
[15] Xiangui Kang, Jiwu Huang, and Wenjun Zeng. Efficient general print-scanning resilient data hiding based on
uniformlog-polarmapping. IEEETransactionsonInformationForensicsandSecurity,2010.
[16] AnuPramila, AnjaKeskinarkaus, andTapioSeppa¨nen. Increasingthecapturingangleinprint-camrobustwa-
termarking. JournalofSystemsandSoftware,2018.
[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXivpreprintarXiv:1412.6572,2014.
[18] HuanranChen,YichiZhang,YinpengDong,andJunZhu. Rethinkingmodelensembleintransfer-basedadver-
sarialattacks. arXivpreprintarXiv:2303.09105,2023.
[19] Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. En-
semble adversarial training: Attacks and defenses. In International Conference on Learning Representations,
2018.
[20] BradleyEfronandRobertJTibshirani. Anintroductiontothebootstrap. CRCpress,1994.
[21] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.
DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. In Annual Meeting of
theAssociationforComputationalLinguistics,2023.
[22] IuliaTurcandGauravNemade. Midjourneyuserprompts&generatedimages(250k). https://www.kaggle.
com/ds/2349267,2022.
[23] DALLE2Images. https://dalle2.gallery,2023.
[24] QingquanWang. Invisiblewatermark. https://github.com/ShieldMnt/invisible-watermark,2020.
17ATransferAttacktoImageWatermarks
Algorithm1FindthePerturbationδ
Require: Watermarkedimagex , msurrogatedecoders S m , mtargetwatermarks wt m , distancemetricl,
w { i }i=1 { i}i=1
perturbationbudgetr,attackstrengththresholdϵ,learningrateα,andmaximumnumberofiterationsmax iter.
Ensure: Perturbationδ
1: Initializeδ 0
←
2: fork =1,2, ,max iterdo
3: g ←∇δm1··∑︁· m i=1l(S i(x w+δ),w it)
4: δ δ α g
← − ·
5: if δ ∞ >rthen
6: ∥ δ∥ δ r
← · ∥δ∥∞
7: endif
8: if m1 ∑︁m i=1BA(S i(x w+δ),w it) ≥1 −ϵthen
9: Returnδ
10: endif
11: endfor
12: Returnδ
A ProofofLemma1
BasedonDefinition1, 2,and 3,wehavethefollowings:
Pr(T(x +δ) =1 T(x ) S (x +δ) =1 S (x ) )
w j w j i w j i w j
− | −
=Pr(T(x +δ) =1 T(x ) ,T(x ) =S (x )
w j w j w j i w j
− |
S (x +δ) =1 S (x ) )
i w j i w j
−
+Pr(T(x +δ) =1 T(x ) ,T(x ) =1 S (x )
w j w j w j i w j
− − |
S (x +δ) =1 S (x ) )
i w j i w j
−
=Pr(T(x +δ) =1 T(x ) T(x ) =S (x ) ,
w j w j w j i w j
− |
S (x +δ) =1 S (x ) )
i w j i w j
−
Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) )
w j i w j i w j i w j
× | −
+Pr(T(x +δ) =1 T(x ) T(x ) =1 S (x ) ,
w j w j w j i w j
− | −
S (x +δ) =1 S (x ) )
i w j i w j
−
Pr(T(x ) =1 S (x ) S (x +δ) =1 S (x ) )
w j i w j i w j i w j
× − | −
=Pr(T(x +δ) =1 T(x ) T(x ) =S (x ) ,
w j w j w j i w j
− |
S (x +δ) =1 S (x ) )
i w j i w j
−
Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) )
w j i w j i w j i w j
× | −
+(1 Pr(T(x +δ) =T(x ) T(x ) =1 S (x ) ,
w j w j w j i w j
− | −
S (x +δ) =1 S (x ) ))
i w j i w j
−
(1 Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) ))
w j i w j i w j i w j
× − | −
=Pr(T(x +δ) =S (x +δ) T(x ) =S (x ) ,
w j i w j w j i w j
|
S (x +δ) =1 S (x ) )
i w j i w j
−
Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) )
w j i w j i w j i w j
× | −
+(1 Pr(T(x +δ) =S (x +δ)
w j i w j
− |
T(x ) =1 S (x ) ,S (x +δ) =1 S (x ) ))
w j i w j i w j i w j
− −
(1 Pr(T(x ) =S (x ) S (x +δ) =1 S (x ) ))
w j i w j i w j i w j
× − | −
=a k +(1 a′ )(1 k )
ij ij − ij − ij
=c .
ij
Similarly,wehavethefollowing:
Pr(T(x +δ) =1 T(x ) S (x +δ) =S (x ) )
w j w j i w j i w j
− |
=Pr(T(x +δ) =1 T(x ) ,T(x ) =S (x )
w j w j w j i w j
− |
S (x +δ) =S (x ) )+Pr(T(x +δ) =1 T(x ) ,
i w j i w j w j w j
−
18ATransferAttacktoImageWatermarks
T(x ) =1 S (x ) S (x +δ) =S (x ) )
w j i w j i w j i w j
− |
=Pr(T(x +δ) =1 T(x ) T(x ) =S (x ) ,
w j w j w j i w j
− |
S (x +δ) =S (x ) )
i w j i w j
Pr(T(x ) =S (x ) S (x +δ) =S (x ) )
w j i w j i w j i w j
× |
+Pr(T(x +δ) =1 T(x ) T(x ) =1 S (x ) ,
w j w j w j i w j
− | −
S (x +δ) =S (x ) )
i w j i w j
Pr(T(x ) =1 S (x ) S (x +δ) =S (x ) )
w j i w j i w j i w j
× − |
=(1 Pr(T(x +δ) =T(x ) T(x ) =S (x ) ,
w j w j w j i w j
− |
S (x +δ) =S (x ) ))
i w j i w j
Pr(T(x ) =S (x ) S (x +δ) =S (x ) )
w j i w j i w j i w j
× |
+Pr(T(x +δ) =1 T(x ) T(x ) =1 S (x ) ,
w j w j w j i w j
− | −
S (x +δ) =S (x ) )
i w j i w j
(1 Pr(T(x ) =S (x ) S (x +δ) =S (x ) ))
w j i w j i w j i w j
× − |
=(1 Pr(T(x +δ) =S (x +δ) T(x ) =S (x ) ,
w j i w j w j i w j
− |
S (x +δ) =S (x ) ))
i w j i w j
Pr(T(x ) =S (x ) S (x +δ) =S (x ) )
w j i w j i w j i w j
× |
+Pr(T(x +δ) =S (x +δ) T(x ) =1 S (x ) ,
w j i w j w j i w j
| −
S (x +δ) =S (x ) )
i w j i w j
(1 Pr(T(x ) =S (x ) S (x +δ) =S (x ) ))
w j i w j i w j i w j
× − |
=(1 b′ )k′ +b (1 k′ )
− ij ij ij − ij
=c′ .
ij
B ProofofTheorem1
BasedonLemma1andDefinition4,wehavethefollowing:
Pr(T(x +δ) =1 T(x ) )
w j w j
−
=Pr(T(x +δ) =1 T(x ) ,S (x +δ)=S (x ))
w j w j i w i w
−
+Pr(T(x +δ) =1 T(x ) ,S (x +δ)=1 S (x ))
w j w j i w i w
− −
=Pr(T(x +δ) =1 T(x ) S (x +δ)=S (x ))
w j w j i w i w
− |
Pr(S (x +δ)=S (x ))
i w i w
×
+Pr(T(x +δ) =1 T(x ) S (x +δ)=1 S (x ))
w j w j i w i w
− | −
Pr(S (x +δ)=1 S (x ))
i w i w
× −
=c′ (1 q )+c q
ij − ij ij ij
=e .
j
C ProofofLemma2
Pr(S (x +δ) =1 S (x ) T(x +δ) =1 T(x ) )
i w j i w j w j w j
− | −
Pr(S (x +δ) =1 S (x ) ,T(x +δ) =1 T(x ) )
= i w j − i w j w j − w j
Pr(T(x +δ) =1 T(x ) )
w j w j
−
=Pr(T(x +δ) =1 T(x ) S (x +δ) =1 S (x ) )
w j w j i w j i w j
− | −
Pr(S (x +δ) =1 S (x ) )
i w j − i w j .
× Pr(T(x +δ) =1 T(x ) )
w j w j
−
BasedonLemma1andTheorem1,wehavethefollowing:
Pr(T(x +δ) =1 T(x ) S (x +δ) =1 S (x ) )
w j w j i w j i w j
− | −
19ATransferAttacktoImageWatermarks
Pr(S (x +δ) =1 S (x ) )
i w j i w j
−
× Pr(T(x +δ) =1 T(x ) )
w j w j
−
c q
= ij ij .
c q +c′ (1 q )
ij ij ij − ij
D ProofofTheorem2
Pr(T(x +δ) =1 T(x ) S (x +δ) , ,S (x +δ) )
w j w j 1 w j m w j
− | ···
=Pr(S (x +δ) , ,S (x +δ)
1 w j m w j
··· |
T(x +δ) =1 T(x ) )
w j w j
−
Pr(T(x +δ) =1 T(x ) )
w j − w j .
× Pr(S (x +δ) , ,S (x +δ) )
1 w j m w j
···
BasedonAssumption2and3,wehavethefollowing:
Pr(S (x +δ) , ,S (x +δ) T(x +δ) =1 T(x ) )
1 w j m w j w j w j
··· | −
Pr(T(x +δ) =1 T(x ) )
w j w j
−
× Pr(S (x +δ) , ,S (x +δ) )
1 w j m w j
···
=Pr(S (x +δ) T(x +δ) =1 T(x ) )
1 w j w j w j
| −
Pr(S (x +δ) T(x +δ) =1 T(x ) )
m w j w j w j
··· | −
Pr(T(x +δ) =1 T(x ) )
w j − w j . (16)
× Pr(S (x +δ) ) Pr(S (x +δ) )
1 w j m w j
···
GiventhatM = i S (x +δ) =1 S (x ) ,wehavethefollowingaccordingtoLemma2:
j1 i w j i w j
{ | − }
Pr(S (x +δ) T(x +δ) =1 T(x ) )
i w j w j w j
| −
=Pr(S (x +δ) =1 S (x ) T(x +δ) =1 T(x ) )
i w j i w w j w j
− | −
c q
= ij ij , i M . (17)
c q +c′ (1 q ) ∀ ∈ j1
ij ij ij − ij
GiventhatM = i S (x +δ) =S (x ) ,wehave:
j2 i w j i w j
{ | }
Pr(S (x +δ) T(x +δ) =1 T(x ) )
i w j w j w j
| −
=1 Pr(S (x +δ) =1 S (x )
i w j i w
− − |
T(x +δ) =1 T(x ) )
w j w j
−
c q
=1 ij ij
− c q +c′ (1 q )
ij ij ij − ij
c′ (1 q )
= ij − ij , i M . (18)
c q +c′ (1 q ) ∀ ∈ j2
ij ij ij − ij
ThenEquation16canbereformulatedasfollows:
Pr(S (x +δ) T(x +δ) =1 T(x ) )
1 w j w j w j
| −
Pr(S (x +δ) T(x +δ) =1 T(x ) )
m w j w j w j
··· | −
Pr(T(x +δ) =1 T(x ) )
w j w j
−
× Pr(S (x +δ) ) Pr(S (x +δ) )
1 w j m w j
···
=Pr(T(x +δ) =1 T(x ) )
w j w j
−
∏︂ Pr(S i(x w+δ) j T(x w+δ) j =1 T(x w) j)
| −
Pr(S (x +δ) )
i w j
i∈Mj1
∏︂ Pr(S i(x w+δ) j |T(x w+δ) j =1 −T(x w) j) .
Pr(S (x +δ) )
i w j
i∈Mj2
20ATransferAttacktoImageWatermarks
Then,basedonDefinition4,Theorem1,Equation17,andEquation18,wehavethefollowing:
Pr(T(x +δ) =1 T(x ) )
w j w j
−
∏︂ Pr(S i(x w+δ) j T(x w+δ) j =1 T(x w) j)
| −
Pr(S (x +δ) )
i w j
i∈Mj1
∏︂ Pr(S i(x w+δ) j T(x w+δ) j =1 T(x w) j)
| −
Pr(S (x +δ) )
i w j
i∈Mj2
=e ∏︂ c ijq ij
j c q2 +c′ (1 q )q
i∈Mj1 ij ij ij − ij ij
∏︂ c′ ij(1 −q ij)
c q (1 q )+c′ (1 q )2
i∈Mj2 ij ij − ij ij − ij
=min(e ∏︂ c ij
j c q +c′ (1 q )
i∈Mj1 ij ij ij − ij
∏︂
c′
ij ,1)
c q +c′ (1 q )
i∈Mj2 ij ij ij − ij
=p .
j
E ProofofTheorem3
Theconditionalexpectationof T(x +δ) T(x ) canberepresentedas:
w j w j
| − |
E(T(x +δ) T(x ) S (x +δ) , ,S (x +δ) )
w j w j 1 w j m w j
| − || ···
=0 Pr(T(x +δ) =T(x )
w j w j
× |
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
+1 Pr(T(x +δ) =1 T(x )
w j w j
× − |
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
=Pr(T(x +δ) =1 T(x )
w j w j
− |
S (x +δ) , ,S (x +δ) ).
1 w j m w j
···
AccordingtoTheorem2,wehave:
Pr(T(x +δ) =1 T(x ) S (x +δ) , ,S (x +δ) )
w j w j 1 w j m w j
− | ···
=p .
j
AccordingtoAssumption3,theconditionalexpectationof T(x ) w canberepresentedas:
w j j
| − |
E(T(x ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
=0 Pr(T(x ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
× | ···
+1 Pr(T(x ) =1 w
w j j
× − |
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
=Pr(T(x ) =1 w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
− | ···
Pr(T(x ) =1 w ,S (x +δ) , ,S (x +δ) )
= w j − j 1 w j ··· m w j
Pr(S (x +δ) , ,S (x +δ) )
1 w j m w j
···
Pr(S (x +δ) , ,S (x +δ) T(x ) =1 w )
= 1 w j ··· m w j | w j − j
Pr(S (x +δ) , ,S (x +δ) )
1 w j m w j
···
Pr(T(x ) =1 w )
w j j
× −
Pr(S (x +δ) T(x ) =1 w )
= 1 w j | w j − j
Pr(S (x +δ) )
1 w j
Pr(S (x +δ) T(x ) =1 w )
m w j w j j
| −
··· Pr(S (x +δ) )
m w j
21ATransferAttacktoImageWatermarks
Table1: Thedetailedsettingsofαfordifferentnumberofsurrogatemodels.
#ofSurrogateModels 1 5 10 20 30 40 60 80 100
α 0.1 1 2 4
Pr(T(x ) =1 w ).
w j j
× −
SincetheflippingbehaviorofsurrogatemodelsisirrelevanttoDefinition5,thenwehavethefollowing:
E(T(x ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
=Pr(T(x ) =1 w ).
w j j
−
BasedonDefinition5,wehave:
E(T(x ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
=1 β .
j
−
Furthermore,theconditionalexpectationof T(x +δ) w canberepresentedas:
w j j
| − |
E(T(x +δ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
=0 Pr(T(x +δ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
× | ···
+1 Pr(T(x +δ) =1 w
w j j
× − |
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
=Pr(T(x +δ) =1 w S (x +δ) , ,S (x +δ) ). (19)
w j j 1 w j m w j
− | ···
Basedonthetriangleinequality,wehavethefollowing:
E(T(x +δ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
min(E(T(x +δ) T(x ) + T(x ) w
w j w j w j j
≤ | − | | − ||
S (x +δ) , ,S (x +δ) ),1)
1 w j m w j
···
=min(1 β +p ,1),
j j
−
andthefollowing:
E(T(x +δ) w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| − || ···
E( T(x +δ) T(x ) T(x ) w
w j w j w j j
≥ || − |−| − |||
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
= p +β 1.
j j
| − |
Therefore,basedonEquation19,wehavethefollowing:
Pr(T(x +δ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| ···
=1 Pr(T(x +δ) =1 w
w j j
− − |
S (x +δ) , ,S (x +δ) )
1 w j m w j
···
1 min(1 β +p ,1)
j j
≥ − −
=max(β p ,0),
j j
−
andthefollowing:
Pr(T(x +δ) =w S (x +δ) , ,S (x +δ) )
w j j 1 w j m w j
| ···
1 p +β 1.
j j
≤ −| − |
22ATransferAttacktoImageWatermarks
1.25 1.25 1.25
JPEG JPEG JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
1.00 Gaussianblur 1.00 Gaussianblur 1.00 Gaussianblur
Brightness/Contrast Brightness/Contrast Brightness/Contrast
Ours Ours Ours
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate EvasionRate
(a)20bits (b)30bits (c)64bits
Figure9:Comparingtheaverageperturbationsofourtransferattackandcommonpost-processingmethodswhenthey
achievethesameevasionrate. ThetargetmodelusesResNetarchitectureanddifferentwatermarklengths(thethree
columns). DatasetisMidjourney.
150 150 150
JPEG JPEG JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
Gaussianblur Gaussianblur Gaussianblur
Brightness/Contrast Brightness/Contrast Brightness/Contrast
100 Ours 100 Ours 100 Ours
50 50 50
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8
EvasionRate EvasionRate EvasionRate
150 150 150
JPEG JPEG JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
Gaussianblur Gaussianblur Gaussianblur
Brightness/Contrast Brightness/Contrast Brightness/Contrast
100 Ours 100 Ours 100 Ours
50 50 50
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate EvasionRate
(a)20bits (b)30bits (c)64bits
Figure10: Comparingtheaverageℓ -normperturbationsofourtransferattackandcommonpost-processingmethods
2
whentheyachievethesameevasionrate. ThetargetmodelusesResNetarchitectureanddifferentwatermarklengths
(thethreecolumns). Firstrow: StableDiffusion. Secondrow: Midjourney.
23
noitabrutreP
∞‘egarevA
noitabrutreP2‘egarevA
noitabrutreP2‘egarevA
noitabrutreP
‘egarevA ∞
noitabrutreP2‘egarevA
noitabrutreP2‘egarevA
noitabrutreP
‘egarevA ∞
noitabrutreP2‘egarevA
noitabrutreP2‘egarevAATransferAttacktoImageWatermarks
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 JPEG 0.4 JPEG 0.4 JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast 0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast 0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast
Ours Ours Ours
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8
EvasionRate EvasionRate EvasionRate
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 JPEG 0.4 JPEG 0.4 JPEG
Gaussiannoise Gaussiannoise Gaussiannoise
0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast 0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast 0.2 G Bra iu gs hs ti na en ssb /l Cur ontrast
Ours Ours Ours
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate EvasionRate
(a)20bits (b)30bits (c)64bits
Figure 11: Comparing the average SSIM of our transfer attack and common post-processing methods when they
achievethesameevasionrate. ThetargetmodelusesResNetarchitectureanddifferentwatermarklengths(thethree
columns). Firstrow: StableDiffusion. Secondrow: Midjourney.
1.0
0.8
0.6
0.4
0.2
0.0
20bits 30bits 64bits
AdvEmb-RN18(r=0.25) AdvCls-WM1&WM2 Ours(r=0.1)
AdvCls-Real&WM WEvade-B-S(r=0.25) Ours(r=0.25)
Figure12: ComparingtheevasionratesofexistingandourtransferattackswhenthetargetmodelisResNetanduses
watermarkswithdifferentlengths. DatasetisMidjourney.
(cid:15)=0.5 1.00
3 (cid:15)=0.4
(cid:15)=0.3
(cid:15)=0.2 0.99
(cid:15)=0.1
2
0.98
0.97
1 (cid:15)=0.5
(cid:15)=0.4
0.96 (cid:15)=0.3
(cid:15)=0.2
0 (cid:15)=0.1
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate
(a)ℓ -normperturbation (b)SSIM
2
Figure13: Averageℓ -normperturbationsandaverageSSIMofourtransferattackwithdifferentϵ. Thetargetmodel
2
usesCNNarchitectureandwatermarklengthof30bits.
24
MISSegarevA
MISSegarevA
noitabrutreP2‘egarevA
etaRnoisavE
MISSegarevA
MISSegarevA
MISSegarevA
MISSegarevA
MISSegarevAATransferAttacktoImageWatermarks
10 RD-PA-Mean 1.0
RS-PA-Mean
8 ID-PA-Mean
RD-PA-Median RS-PA-Median 0.9
6 ID-PA-Median RD-PA-Mean
RD-EO RS-PA-Mean
4 R IDS -- EE OO 0.8 I RD D-P -PA A-M -Me ea dn ian
RS-PA-Median
2 ID-PA-Median
RD-EO
0.7 RS-EO
0 ID-EO
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
EvasionRate EvasionRate
(a)ℓ -normperturbation (b)SSIM
2
Figure 14: Average ℓ -norm perturbations and average SSIM of different variants of our transfer attack. The target
2
modelusesCNNarchitectureandwatermarklengthof30bits.
25
noitabrutreP2‘egarevA
MISSegarevA