A Wasserstein perspective of Vanilla GANs
Lea Kunkel and Mathias Trabs
Karlsruhe Insitute of Technology
The empirical success of Generative Adversarial Networks (GANs) caused an
increasing interest in theoretical research. The statistical literature is mainly
focused on Wasserstein GANs and generalizations thereof, which especially allow
for good dimension reduction properties. Statistical results for Vanilla GANs, the
originaloptimization problem, are still rather limited and require assumptions such
as smooth activation functions and equal dimensions of the latent space and the
ambient space. To bridge this gap, we draw a connection from Vanilla GANs to
the Wasserstein distance. By doing so, existing results for Wasserstein GANs can
be extended to Vanilla GANs. In particular, we obtain an oracle inequality for
Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality
are designed to be satisfied by network architectures commonly used in practice,
such as feedforward ReLU networks. By providing a quantitative result for the
approximationofaLipschitzfunctionbyafeedforwardReLUnetworkwithbounded
Hölder norm, we conclude a rate of convergence for Vanilla GANs as well as
Wasserstein GANs as estimators of the unknown probability distribution.
Keywords: Generativeadversarialnetworks,rateofconvergence,oracleinequality,Wassersteindistance,
distribution estimation
MSC 2020: 62E17, 62G05, 68T07
1 Introduction
Generative Adversarial Networks (GANs) have attracted much attention since their introduction by
Goodfellow et al. (2014), initially due to impressive results in the creation of photorealistic images.
Meanwhile, the areas of application have expanded far beyond this, and GANs serve as a prototypical
example of the rapidly evolving researcharea of generative models.
The Vanilla GAN as constructed by Goodfellow et al. (2014) relies on the minimax game
inf sup E logD(X)+log 1 D(G(Z)) , (1)
G∈GD∈D
(cid:2) (cid:0)
−
(cid:1)(cid:3)
to learn an unknown distribution P∗ of the random variable X. The generator G chosen from a set
G
applied to the latent random variable Z aims to mimic the distribution of X as closely as possible. The
discriminator D, chosen from a set , has to distinguish between real and fake samples.
D
The optimizationproblem(1)ismotivatedby the Jensen-Shannondivergence. Generalizationsofthe
underlying distance have led to various extensions of the originalGAN, such as f-GANs (Nowozinet al.
(2016)). More famously, Wasserstein GANs (Arjovsky et al. (2017)), characterizedby
inf sup E W(X) W(G(Z)) , (2)
G∈GW∈Lip(1)
(cid:2)
−
(cid:3)
are obtained by replacing the Jensen-Shannon divergence by the Kantorovich dual of the Wasserstein
distance. Here, Lip(1) denotes the set of all Lipschitz continuous functions with Lipschitz constant
boundedbyone. This approachcanbe generalizedtoIntegral Probability Metrics (IPM, Mueller(1997))
GANs, see Liang (2021).
The analysis of Wasserstein GANs can exploit the existing theory on the Wasserstein distance. The
latter has a long record of research, particularly in the context of optimal transport (Villani, 2008) but
1
4202
raM
22
]TS.htam[
1v21351.3042:viXraalso in machine learning, see Torres et al. (2021) for an overview. In contrast, Vanilla GANs and the
Jensen-Shannondivergencehave been studied less extensively, andfundamental questions have not been
settled. In particular, all statistical results for Vanilla GANs require the same dimension of the latent
spaceandthetargetspacewhichisinstarkcontrasttocommonpractice. Anotheralgorithmicdrawback
of Vanilla GANs highlighted by Arjovsky & Bottou (2017) is that an arbitrarily large discriminator
class prevents the generator from learning. The Jensen-Shannon divergence between singular measures
is by definition maximal. Therefore, we cannot expect convergence in a dimension reduction setting.
In practice, however, Vanilla GANs have worked in various settings. Thus, using neural networks as a
discriminatorclassmustbeadvantageouscomparedtothesetofallmeasurablefunctions. Thisempirical
fact is supported by the numerical results by Farnia & Tse (2018) who impose a Lipschitz constraint on
the discriminator class.
Our contribution. Our work aims to bridge the gap between Vanilla GANs and Wasserstein GANs
while addressingthe theoreticallimitations ofthe former ones. By imposing aLipschitz conditiononthe
discriminator class in (1), we recover a Wasserstein-like behavior. As a main result, we can derive an
oracleinequalityfortheWassersteindistancebetweenthetruedatageneratingdistributionanditsVanilla
GAN estimate. Inparticular,this allowsus to transferkey features,suchasdimension reduction,known
from the statistical analysis of Wasserstein GANs. We show that the statistical error of the modified
Vanilla GAN depends only on the dimension of the latent space, independent of the potentially much
largerdimensionofthe feature space . Thus Vanilla GANs canavoidthe curse ofdimensionality. Such
X
propertiesarewellknownfrompractice,but cannotbe verifiedbythe classicalJensen-Shannonanalysis.
On the other hand the derived rate of convergence for the Vanilla GAN is slower than for Wasserstein
GANs which is in line with the empirical advantage of Wasserstein GANs.
We then consider the most relevant case where the classes and are parameterized by neural
G D
networks. Using our previous results, we derive an oracle inequality that depends on the network
approximation errors for the best possible generator and the optimal Lipschitz discriminator. To bound
the approximation error of the discriminator, we replace the Lipschitz constraint on the networks with
a less restrictive Hölder constraint. Building on Gühring et al. (2020), we prove a novel quantitative
approximationtheorem for Lipschitz functions using ReLU neural networks with bounded Hölder norm.
As a result we obtain the rate of convergence n−α/2d∗ ,α (0,1), with latent space dimension d∗ 2
∈ ≥
for sufficiently large classes of networks. Additionally, our approximation theorem allows for an explicit
bound on the discriminator approximation error for Wasserstein-type GANs, which achieve the rate
n−α/d∗
,α (0,1).
∈
Related work. The existence and uniqueness of the optimal generator for Vanilla GANs is shown
by Biau et al. (2018) under the condition that the class is convex and compact. They also study
G
the asymptotic properties of Vanilla GANs. Belomestny et al. (2021) have shown a non-asymptotic
rate of convergence in the Jensen-Shannon divergence for Vanilla GANs with neural networks under the
assumptionthatthe density ofP∗ exists andthat the generatorfunctions arecontinuouslydifferentiable.
In practice, however, the Rectifier Linear Unit activation function (ReLU activation function) is
commonly used (Aggarwal, 2018, p.13). The resulting neural network generates continuous piecewise
linear functions. Therefore, the convergencerate of Belomestny et al. (2021) combined with Belomestny
et al. (2023) is not applicable to this class of functions.
The statistical analysis of Wasserstein GANs is much better understood. Biau et al. (2021) have
studied optimization and asymptotic properties. Liang (2021) has shown error decompositions with
respect to the Kullback-Leibler divergence, the Hellinger distance and the Wasserstein distance. The
case where the unknowndistribution lies ona low-dimensionalmanifold is consideredin Schreuder et al.
(2021)aswellasTang&Yang(2022). Thelatteralsoderiveminmaxratesinamoregeneralsettingusing
the Hölder metric. Assuming that the density function of P∗ exists, Liang (2017) has shown a rate of
convergence in Wasserstein distance with ReLU activation function with a factor growing exponentially
in the depth of the network. Theoreticalresults including sampling the latent distribution in additionto
dimensionreductionhavebeenderivedbyHuangetal.(2022),whohavealsoshownarateofconvergence
inaslightlymoregeneralsetting(usingtheHöldermetric)usingReLUnetworkswhoseLipschitzconstant
growsexponentiallyinthe depth. Arateofconvergenceusing the totalvariationmetricandleakyReLU
networks has been shown in Liang (2021).
Convergence rates with respect to the Wasserstein distance have been studied by Chae (2022) and
Chen et al. (2020). Up to a logarithmic factor, optimal rates in the Hölder metric were obtained by
Stéphanovitch et al. (2023) using smooth networks. In a similar setting, Chakraborty & Bartlett (2024)
2discussed several methods for dimension reduction. Recently, Suh & Cheng (2024) have reviewed the
theoretical advances in Wasserstein GANs.
EnsuringLipschitzcontinuityofthediscriminatorclassistheessentialpropertyofWassersteinGANs.
Lipschitz-constrainedneural networks and their empirical success are subject of ongoing research
(Khromov & Singh (2023), in context of GANs see Than & Vu (2021)). Implementations of Wasserstein
GANs have evolved from weight clipping (Arjovsky et al., 2017) to penalizing the objective function
(Gulrajanietal.,2017;Weietal.,2018;Zhouetal.,2019;Petzkaetal.,2017;Miyatoetal.,2018;Asokan
& Seelamantula, 2023), which heuristically leads to networks with bounded Lipschitz constants. Farnia
& Tse (2018) use an objective function that combines Wasserstein and Vanilla GANs.
Outline. In Section 2 we introduce the Vanilla GAN distance, which characterizes the optimization
problem (1). In Section 3, we investigate the relation between the Vanilla GAN distance and the
Wassersteindistance. Weshowthatthedistancesarecompatibletoeachotherwhilenotbeingequivalent.
Using this relation, we derive an oracle inequality for the Vanilla GAN in Section 4, where is an
G
arbitrary set and is a set of Lipschitz functions. We show that Vanilla GANs can avoid the curse
D
of dimensionality. In Section 5 we consider the situation where and consist of neural networks.
G D
Here we relax the Lipschitz condition to a α-Hölder condition and prove a quantitative result for the
approximation of a Lipschitz function by a feedforward ReLU network with bounded Hölder norm. We
thenproveaconvergenceratefor the Vanilla GAN withnetworkgeneratoranddiscriminator. All proofs
are deferred to section 7.
2 The Vanilla GAN distance
Let us first fix some notation. We equip Rd with the ℓ -norm x , 1 p , and denote the number
p p
| | ≤ ≤ ∞
of nonzero entries of a k l matrix A, where k,l N, by A := (i,j): A =0 . For ease of notation
ℓ0 ij
× ∈ | | |{ 6 }|
we abbreviate for x (0, )
∈ ∞
[x]1;1/2 :=max x,√x . (3)
{ }
ForΩ Rd andf: Ω Rwedefine f :=ess sup f(x) :x Ω .Wedenotethesetofbounded
∞,Ω
⊂ → k k {| | ∈ }
Lipschitz functions by
Lip(L,B,Ω):= f: Ω R f B, f(x) f(y) Lx y , x,y Ω .
∞,Ω p
{ → |k k ≤ | − |≤ | − | ∀ ∈ }
The set of unbounded Lipschitz functions is denoted by Lip(L,Ω) := Lip(L, ,Ω). By Rademacher’s
∞
theorem, a Lipschitz function is differentiable almost everywhere. For α (0,1] we define the α-Hölder
∈
norm by
f(x) f(y)
f :=max f , esssup | − | (4)
k kHα(Ω) (cid:26)k k∞ x y α (cid:27)
x,y∈Ω | − |p
and the α-Hölder ball of functions with Hölder constant less or equal than Γ>0 as
α(Γ,Ω):= f: Ω R f <Γ . (5)
Hα(Ω)
H n → (cid:12) k k o
(cid:12)
Inparticular,Lip(L,B,Ω) α(max(L,2B),Ω)forany(cid:12)α (0,1).WeomitthedomainΩinournotation
⊆H ∈
if Ω=(0,1)d.
We observe an i.i.d. sample X ,...,X P∗ with values in := (0,1)d. On another space :=
1 n
(0,1)d∗ , called the latent space, we choose a∼ latent distribution UX . Unless otherwise specified, X Z P∗
∼
and Z U. Throughout, the generator class is a compact nonempty set of measurable functions from
∼ G
to . For G the distribution of the random variable G(Z) is denoted by PG(Z).
Z X ∈G
TypicallythediscriminatorclassconsistsoffunctionsmappingtoRconcatenatedtoasigmoidfunction
that maps into (0,1) to account for the classification task. This is especially the case for standard
classificationnetworks. The most common sigmoidfunction used for this purpose is the logistic function
x (1+e−x)−1, which we fix throughout. Together with a shift by log4, we can rewrite the Vanilla
7→
GAN optimization problem (1) as
inf V (P∗,PG(Z)) (6)
W
G∈G
in terms of the Vanilla GAN distance between probability measures P and Q
1+e−W(X) 1+eW(Y)
V (P,Q):= sup E log log , (7)
W X∼P
W∈W Y∼Qh− (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)i
3where is a set of measurable functions W: R. As long as 0 , we have that V 0.
W
W X → ∈W ≥
To choose the generator Gˆ as the empirical risk minimizer, the unknown distribution P∗ in (6)
n
must be replaced by the empirical distribution P based on the observations X ,...,X . In practice, the
n 1 n
expectation with respect to Z U is replaced by an empirical mean too, which we omit for the sake
∼
of simplicity. Following Huang et al. (2022), the next and all subsequent results easily extend to the
corresponding setting.
The following error bound in terms of the Vanilla GAN distance provides an error decomposition for
the empirical risk minimizer of the Vanilla GAN.
Lemma 2.1. Assume that is chosen such that a minimum exists. Let be symmetric, that is, for
G W
W , W . For
∈W − ∈W Gˆ argminV (P ,PG(Z)) (8)
n W n
∈
G∈G
we have that
n
1
V (P∗,PGˆ n(Z)) minV (P∗,PG(Z))+2 sup W(X ) E[W(X)] . (9)
W W i
≤G∈G
W∈Lip(1)◦W
n
Xi=1(cid:0)
−
(cid:1)
The first term in (9) is the error due to the approximation capabilities of the class . The second
G
termreferstothe stochasticerrordue tothe amountoftrainingdata. As issymmetric,the stochastic
W
errorisnon-negative. Botherrortermsdependonthediscriminatorclass . Largediscriminatorclasses
W
leadtofinerdiscriminationbetweendifferentprobabilitydistributionsandthustoalargerapproximation
errorterm. Similarly,the stochasticerrortermwillincreasewiththe size of . The costofsmallclasses
W
is a less informative loss function on the left side of (9).
W
If is the setofallmeasurablefunctions, theanalysisbyGoodfellowetal.(2014,Theorem1)shows
W
thattheVanillaGANdistanceisequivalenttotheJensen-Shannondistanceshiftedbylog(4). Arjovsky&
Bottou(2017)haveelaboratedonthe theoreticalandpracticaldisadvantagesof this case. Similar to the
total variation distance or the Hellinger distance, the Jensen-Shannon divergence is not compatible with
high-dimensional settings because it cannot distinguish between different singular measures. Therefore,
we need a weaker distance and thus restrict .
W
The key insight of Wasserstein GANs (2) is that this particular drawback of the Jensen-Shannon
distance is can be solved by the Wasserstein distance. The latter is a metric on the space of probability
distributions with finite first moment and meterizes weak convergence in this space (Villani, 2008,
Theorem 6.9). Let P and Q be two probability distributions on the same measurable space (Ω, ),
A
the Wasserstein-1 distance is defined as
W (P,Q):= sup E [W(X) W(Y)]= sup E [W(X) W(Y)]. (10)
1 X∼P X∼P
W∈Lip(1) Y∼Q − W∈Lip(1) Y∼Q −
W(0)=0
Boundsforweakermetrics,suchastheKolmogorovorLevymetric,canbeeasilyderivedfromthebounds
in the Wasserstein metric under weak conditions, see e.g. Gibbs & Su (2002).
Therefore, we choose = Lip(L) for some L 1 in Lemma 2.1. In this case the following result
W ≥
shows that the existence of an empirical risk minimizer is guaranteed as soon as is compact.
G
Lemma 2.2. Assume is compact with respect to the supremum norm. The map T: R ,
≥0
G G →
T(G):=V (P ,PG(z)) is continuous and argminV (P∗,PG(z)) is nonempty.
Lip(L) n Lip(L)
G
3 From Vanilla to Wasserstein and back
Our subsequent analysis builds on the following equivalence result between the Vanilla GAN distance
and the Wasserstein distance with an additional L2-penalty term on the discriminator.
Theorem 3.1. For L>2 and B >0 we have
L(L 1)
sup E[W(X) W(Y)] − E[W(X)2]
W∈Lip(1,B′) n − − 2 o
W(·)>−log(2−2/L)
4eB
V (P,Q) sup E[W(X) W(Y)] E[W(X)2] ,
≤ Lip(L,B) ≤ W∈Lip(L,B) n − − (2eB −1)2 o
W(·)>−log(2)
where B′ =log((1+eB)/2).
Theorem3.1revealsthattheVanillaGANdistanceisindeedcompatiblewiththeWassersteindistance
and will allow us to prove rates of convergence of the Vanilla GAN with respect to the Wasserstein
distance. In doing so, we need to consider the consequences of the penalty term. More precisely, we
can deduce from Theorem 3.1 that the Vanilla GAN distance is bounded from above and below by the
Wasserstein distance or the squared Wasserstein distance, respectively.
Theorem 3.2. Let L>2 and B (0, ]. For two probability measures P and Q on we have
∈ ∞ X
min c W (P,Q),c W (P,Q)2 V (P,Q) LW (P,Q),
1 1 2 1 Lip(L,B) 1
(cid:16) (cid:17)≤ ≤
where c = 1log(2−2/L) and c = 1 1 , setting 1/p=0 if p= .
1 2 d1/p 2 2d2/pL(L−1) ∞
The assumption that L>2 is not very restrictive and takes into account that in practically relevant
cases, such as neural network discriminators, the Lipschitz constant is typically quite large. More
importantly, we observe a gap between W (P,Q)2 in the lower bound and W (P,Q) in upper bound
1 1
when W (P,Q)< 1 which is a consequence of the penalty term in Theorem 3.1. The following example
1
indicates that this loss is unavoidable, by restricting the discriminator class to a subset of Lip(L).
Example 3.3. For ε,γ >0,γ+ε<1 let P= 1(δ +δ ) and Q= 1(δ +δ ). The Wasserstein distance
2 γ γ+ε 2 0 ε
is then given by
W (P,Q)=γ.
1
We consider the Vanilla GAN distance using L-Lipschitz affine linear functions as discriminator,
V (P,Q), with a,b R and a L. Note that the class of affine linear functions can be represented
a·+b
∈ | | ≤
by one layer neural networks (for a definition see Section 5). The optimal b can be calculated explicitly,
the optimal a can be determined numerically. Using the optimal slope a and b we obtain for γ <ε, ε= 1
4
and a>16
W (P,Q)2
1 V (P,Q) a W (P,Q)2.
a·+b 1
2 ≤ ≤ ·
If γ ε, then the optimal a is a=L and
≥
log(2) W (P,Q) V (P,Q) a W (P,Q).
1 a·+b 1
· ≤ ≤ ·
See Appendix A for more details on these calculations.
Wasserstein GANs, where the generator is chosen as the empirical risk minimizer of the Wasserstein
distance(10),achieveoptimalconvergencerates(uptologarithmicfactors)withrespecttotheWasserstein
distance as proved by Stéphanovitch et al. (2023). In view of Theorem 3.2 we cannot hope that Vanilla
GANs achievethe samerate evenifwe use aLipschitz discriminatorclass. This is inline withthe better
performance of Wasserstein GANs in practice. However,Theorem 3.2 allows us to study the behavior of
Vanilla GANs in settings where the dimension of the latent space is smaller than the dimension of the
sample space, a setting that is excluded in all previous work on convergence rates for Vanilla GANs.
4 Oracle inequality for Vanilla GAN in Wasserstein distance
Our aim is to bound the Wasserstein distance between the unknown distribution P∗ and the generated
distribution PGˆ n(Z) using the empirical risk minimizer Gˆ
n
of the Vanilla GAN. The following oracle
inequality shows that imposing a Lipschitz constraint on the discriminator class does circumvents the
theoreticallimitationsofVanillaGANswhichiscausedbythecouplingwiththeJensen-Shannondistance.
Recall notation (3).
Theorem 4.1. Let L > 2 and B (0, ]. For the empirical risk minimizer Gˆ from (8) with =
n
∈ ∞ W
Lip(L,B) we have
W (P∗,PGˆ n(Z)) c inf W (P∗,PG(Z)) 1;1/2 +(1+c)[W (P ,P∗)]1;1/2, (11)
1 1 1 n
≤ G∈G
(cid:2) (cid:3)
for some constant c>0 depending on d,p and L.
5Note that the discriminator class Lip(L,B) admits no finite dimensional parameterization and is
therefore not feasible in practice. We will return to this issue in Section 5. The terms in (11) can be
interpreted analogously to the interpretation of the bound in Lemma 2.1, but here we have an oracle
inequalitywith respectto the Wassersteindistance. The firsttermis the approximationerror. It is large
when is not flexible enoughto provide a goodapproximationof P∗ by PG for some G . The second
G ∈G
term refers to the stochastic error. With a growing number of observations the empirical measure P
n
converges to P∗ in Wasserstein distance, see Dudley (1969), and thus the stochastic error converges to
zero. Together with the bounds on W (P ,P∗) by Schreuder (2020) we conclude the following:
1 n
Corollary 4.2. Let L > 2,B (0, ]. For some constant c > 0 the empirical risk minimizer Gˆ from
n
∈ ∞
(8) with =Lip(L,B) satisfies
W
E[W (P∗,PGˆ n(Z))] inf c[W (P∗,PG∗(Z))]1;1/2+c[inf G G∗ 1;1/2]
1 ≤G∗:Z→Xn 1 G∈Gk − k∞ o
n−1/2d, d>2,
+cn−1/4(logn)1/2, d=2,

n−1/4, d=1.

If there is some G∗ such that P∗ = PG∗(Z), which is commonly assumed in the GAN literature, see
e.g. Stéphanovitch et al. (2023), the first term vanishes and the approximation error is bounded by
inf [ G G∗ 1;1/2]. In the bound of the stochastic error we observe the curse of dimensionality: For
G∈G ∞
k − k
large dimensions d the rate of convergence n−1/2d deteriorates.
To allowfor a dimensionreduction setting, we adopt the miss-specifiedsetting from Vardanyanet al.
(2023, Theorem 1). In this scenario we can conclude statistical guarantees for Vanilla GANs that are
comparable to the results obtained for Wasserstein GANs by Schreuder et al. (2021, Theorem 2). Since
Theorem 3.1 does not show equivalence of the Vanilla GAN distance and the Wasserstein distance, we
lose a square root in the rate.
Theorem 4.3. Let L > 2,B (0, ] and M > 0. The empirical risk minimizer Gˆ from (8) with
n
∈ ∞
=Lip(L,B) satisfies
W
E[W (P∗,PGˆ n(Z))] inf c[W (P∗,PG∗(Z))]1;1/2+c[inf G G∗ 1;1/2]
1 ≤G∗∈Lip(M,Z)n 1 G∈Gk − k∞ o
n−1/2d∗ , d∗ >2
+cn−1/4(logn)1/2, d∗ =2

n−1/4, d∗ =1,

for some constant c depending on the dimension d∗ of the latent space and M.
Z
The Wasserstein distance W (PG∗(Z),P∗) now includes also the error due to the dimension reduction
1
while the stochastic error is determined by the potentially much smaller dimension d∗ < d of the latent
space. Compared to Corollary 4.2, the only price for this improvement is the additional Lipschitz
restriction on G∗. We observe a trade-off in the choice of d∗, since large latent dimensions reduce
the approximation error for P∗, but increase the stochastic error term.
Theorem 4.3 revealswhy Vanilla GANs do perform well in high dimensions. This phenomenon could
notbeexplainedinpreviousworkonVanillaGANs. Belomestnyetal.(2021)andBiauetal.(2018)both
obtain rates in the Jensen-Shannon distance.
5 Vanilla GANs with network discriminator
In practice, both and are sets of neural networks. Our conditions on the generator class are
compactness and gG ood apD proximation properties of some G∗ which is chosen such that PG∗ mG imics
P∗. Since neural networks have a finite number of weights, and the absolute value of those weights is
typically bounded, the compactness assumption is usually satisfied and neural networks enjoy excellent
approximationproperties, cf. DeVore et al. (2021).
The situation is more challenging for the discriminator class. So far, was chosen as the set of
D
Lipschitz functions concatenated to the logistic function. The Lipschitz property is crucial for proof of
Theorem 4.1 and thus for all subsequent results.
6ControllingtheLipschitzconstantwhilepreservingtheapproximationpropertiesisanareaofongoing
research and is far from trivial. Without further restrictions on the class of feedforward networks, the
Lipschitzconstantwouldbeatermthatdependsexponentiallyonthesizeofthenetwork,seeLiang(2017,
Theorem3.2). Bounding the Lipschitz constantof a neuralnetwork is a problem that arises naturally in
the implementation of Wasserstein GANs. Arjovsky et al. (2017)use weight clipping to ensure Lipschitz
continuity. Later, other approachessuch as gradientpenalization(see Gulrajaniet al. (2017),which was
further developed by Wei et al. (2018), Zhou et al. (2019)), Lipschitz penalization (Petzka et al., 2017),
or spectral penalization (Miyato et al., 2018) were introduced and have achieved improved performance
in practice.
To extend the theory from the previous section to neural network discriminator classes, we first
generalizeTheorem4.1from =Lip(L,B)tosubsets Lip(L,B). Asaresultthereisanadditional
W W ⊆
approximationerror term that accounts for the smaller discriminator class.
Theorem 5.1. Let L > 2,B (0, ]. The empirical risk minimizer Gˆ from (8) with Lip(L,B)
n
∈ ∞ W ⊆
satisfies for some constant c>0 that
W (P∗,PGˆ n(Z)) c inf W (P∗,PG(Z)) 1;1/2 +c inf sup W W′ 1;1/2
1 1 ∞
≤ (cid:2)G∈G
(cid:3)
(cid:2)W′∈WW∈Lip(L,B)k − k
(cid:3)
+c W (P ,P∗) 1;1/2 .
1 n
(cid:2) (cid:3)
To apply this result, we must ensure that the Lipschitz constant of a set of neural networks is
W
uniformly bounded by some constant L. Adding penalties to the objective function of the optimization
problemdoes not guarantee a fixed bound on the Lipschitz constant. Approaches such as bounds on the
spectral or row-sum norm of matrices in feedforward neural networks ensure a bound on the Lipschitz
constant,but leadto a lossofexpressivenesswhenconsideringReLUnetworks,eveninverysimple cases
such as the absolute value, see Huster et al. (2019) and Anil et al. (2019). On the other hand, Eckstein
(2020) has shown that one-layer L Lipschitz networks are dense (with respect to the uniform norm)
in the set of all L Lipschitz functions on bounded domains. While this implies that the discriminant
approximationerrorconvergesto zero for growingnetwork architectures,the density statement does not
lead to a rate of convergence that depends on the size of the network.
Aniletal.(2019),motivatedbyChernodub&Nowicki(2016),haveintroducedanadaptedactivation
function, Group Sort, which leads to significantly improved approximation properties of the resulting
networks. They show that networks using the Group Sort activation function are dense in the set of
Lipschitz functions, but there is no quantitative approximation result. A discussion of the use of Group
Sort in the context of Wasserstein GANs can be found in Biau et al. (2021).
Toovercomethisproblem,wewouldliketoapproximatenotonlytheoptimaldiscriminatingLipschitz
function from the Wassersteinoptimization problemin the uniform norm, but also its (weak) derivative.
This would allow us to keep the Lipschitz norm of the approximating neural network bounded. For
networks with regular activation functions Belomestny et al. (2023) have studied the simultaneous
approximation of smooth functions and their derivatives. Gühring et al. (2020) have focused on ReLU
networksandhavederivedquantitativeapproximationboundsinhigherorderHölderandSobolevspaces.
Asanintrinsicinsightfromapproximationtheory,theregularityofthefunctionbeingapproximatedmust
exceedtheregularityorderofthenormusedtoderiveapproximationbounds. Therefore,wecannotexpect
to obtainquantitative approximationresultsfor ReLUnetworksin Lipschitz normwithout assumingthe
continuous differentiability of the approximated function.
Unfortunately, the maximizing function of the Wasserstein optimization problem is in general just
Lipschitz continuous. Since we cannotincreasethe regularityof the targetfunction, we insteadrelax the
Lipschitz assumption of the discriminator in Theorem 5.2 to α-Hölder continuity for α (0,1). This
∈
generalization in the context of Wasserstein GANs has recently been discussed by Stéphanovitch et al.
(2023). Recall the definition of the Hölder ball from (5).
Theorem 5.2. Let L > 2,B > log(3) and Γ > max(L,2B). The empirical risk minimizer Gˆ from (8)
n
with α(Γ) satisfies for any G∗ Lip(M, )
W ⊆H ∈ Z
E[W 1(P∗,PGˆ n(Z))] ≤c Gin ∈f GkG∗ −G kα
∞
1;1/2 +c W 1(P∗,PG∗(Z))α 1;1/2
(cid:2) (cid:3) (cid:2) (cid:3)
+c inf sup W W∗ 1;1/2
∞
(cid:2)W∈WW∗∈Lip(L,B)k − k
(cid:3)
7n−α/2d∗ , 2α<d∗
+cn−1/4(logn)1/2, 2α=d∗

n−1/4, 2α>d∗

for some constant c depending on d∗,M and Γ.
It remains to show that there are ReLU networks that satisfy the assumptions of Theorem 5.2. To
this end, we build on and extend the approximation results by Gühring et al. (2020).
Tofixthe notationwegiveageneraldefinitionoffeedforwardneuralnetworks. Letd,K,N ,...,N
1 K
∈
N. A function Φ: Rd R is a neural network with K layers and N + +N neurons if it results for
1 K
→ ···
an argument x Rd from the following scheme:
∈
x :=x,
0
x :=σ(A x +b ), for k =1,...K 1, (12)
k k k−1 k
−
Φ(x)=x :=A x +b ,
K L K−1 K
where for k 1,...,K , A
k
RNk×Nk−1 and b
k
RNk. σ: R R is an element-wise applied arbitrary
∈ { } ∈ ∈ →
activation function. The number of nonzero weights of all A ,b is given by K (A + b ). We
k k j=1 | j |ℓ0 | j |ℓ0
focus on the ReLU activation function σ(x)=max(0,x). P
Theorem 5.3. Let L,B > 0, and 0 < α < 1. Then there are constants C′,C′′,C′′′ > 0 depending on
d,L,α and B with the following properties: For any ε (0,1/2) and any f Lip(L,B), there is a ReLU
∈ ∈
neural network Φ ε with no more than ⌈C′log 2(ε− 1−1 α)
⌉
layers, ⌈C′′ε− 1−d α log2 2(ε− 1−1 α)
⌉
nonzero weights
and ⌈C′′′ε− 1−d α(log2 2(ε− 1−1 α) ∨log 2(ε− 1−1 α))
⌉
neurons such that
Φ f ε and Φ α max(L,2B)+ε .
ε − ∞ ≤ ε ∈H
(cid:13) (cid:13) (cid:0) (cid:1)
(cid:13) (cid:13)
Since there are many different reasons why controlling the Hölder constant of neural networks is
interesting (with stability probably being the most prominent one), Theorem 5.3 is of interest on its
own. Combining Theorem 5.2 and Theorem 5.3 with a standard approximation result for the generator
approximationerror, such as Yarotsky (2017, Theorem 1), leads to a rate of convergence.
∗
Corollary 5.4. For 0 < α < 1, Γ > 5, M > 0, d∗ > 2α and n > 22 αd choose as the set of ReLU
G
networks withat most c (log(n)+1) layers, c n−α/2(log(n)+1)neurons andc n−α/2(log(n)+1)nonzero
weightsand ′ asthese· tofReLUnetworksw· ithatmost1+c log(n)layers and· c n2(1α −α) log2(n)nonzero
W · ·
weightsandneurons,wherecisaconstantdependingond,d∗,Γandα. Thentheempiricalriskminimizer
Gˆ from (8) with = ′ α(Γ) satisfies
n
W W ∩H
E W P∗,PGˆ n(Z) c n−α/2d∗ +c inf W (P∗,PG∗(Z))α 1;1/2 .
1 1
h (cid:16) (cid:17)i≤ · (cid:2)G∗∈Lip(M,Z) (cid:3)
This corollary shows that Vanilla GANs with a Hölder regular discriminator class are theoretically
advantageous. Since α can be chosen arbitrarily close to one, this reveals why a Lipschitz regularization
asimplemented forWassersteinGANs alsoimprovesthe VanillaGAN.Anempiricalconfirmationcanbe
found in Zhou et al. (2019).
6 Wasserstein GAN
The same analysis can be applied to Wasserstein-type GANs. The constrained on the Hölder constant
can be weakened, as we do not need Theorem 3.2. Note that this does not impact the rate, but the
constant. Define the Wasserstein-type GAN with discriminator class as
W
W (P,Q)= sup E [W(X) W(Y)].
W X∼P
W∈W Y∼Q −
The followingtheoremshowsthatby using Hölder continuousReLUnetworksas the discriminatorclass,
Wasserstein-type GANs can avoid the curse of dimensionality. Furthermore, this avoids the difficulties
arising from the Lipschitz assumption of the neural network, as pointed out by Huang et al. (2022).
8Theorem 6.1. For 0< α<1, Γ> 2d1/p,M > 0 and d> 2α and n> 2αd choose as the set of ReLU
G
networks with at most c (log(n)+1) layers, c nα(log(n)+1) neurons an nonzero weights and ′ as the
set of ReLU networks w· ith at most c log(n) l· ayers and c n(1−α α) log2(n) nonzero weights andW neurons,
· ·
where c is a constant depending on d,d∗,Γ and α . The empirical risk minimizer with = ′ α(Γ)
W W ∩H
Gˆ argminW (P ,PG(Z))
n W n
∈
G∈G
satisfies
E[W 1(P∗,PGˆ n(Z))] min c n−α d + inf W 1(P∗,PG∗(Z)), c n− dα∗ + inf W 1(P∗,PG∗(Z)) .
≤ n · G∗mb. · G∗∈Lip(M,Z) o
Compared to Corollary 5.4 the rate improves to
n−α/d∗
for any α < 1. This upper bound thus
coincides with the lower bound in Tang & Yang (2022, Theorem 1) up to an arbitrary small polynomial
factor. Our rate does not depend exponentially on the number of layers like the results of Liang (2017),
Huang etal.(2022)andwe use non-smoothsimple ReLU networkscomparedto smooth ReQUnetworks
in Stéphanovitch et al. (2023) or group sort networks in Biau et al. (2021). In addition, are able to
evaluate in the Wasserstein metric rather than the α metric.
H
7 Proofs
7.1 Proof for Section 2
Proof of Lemma 2.1. LetX P∗,Xˆ P andZ U. The symmetryof andthe Lipschitz continuity
n
∼ ∼ ∼ W
of x log(1+e−x) yields for any G
7→ ∈G
V (P∗,PGˆ n(Z))
W
1+e−W(X) 1+e−W(Xˆ) 1+e−W(Xˆ) 1+eW(Gˆ n(Z))
= sup E log +log log log
W∈W h− (cid:16) 2 (cid:17) (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)i
sup E log(1+e−W(X))+log(1+e−W(Xˆ)) +V (P ,PGˆ n(Z))
W n
≤ −
W∈W (cid:2) (cid:3)
sup E log(1+e−W(X))+log(1+e−W(Xˆ)) +V (P ,PG(Z))
W n
≤ −
W∈W (cid:2) (cid:3)
= sup E[
log(1+e−W(X))+log(1+e−W(Xˆ))]
−
W∈W
+ sup E[ log(1+e−W(Xˆ))+log(1+e−W(X))]+V (P∗,PG(Z))
W
−
W∈W
2 sup E[W(X) W(Xˆ)]+V (P∗,PG(Z)).
W
≤ −
W∈Lip(1)◦W
The bound for Gˆ from (8) follows since G was arbitrary.
n
∈G
Proof of Lemma 2.2. Let (G ) be a sequence that converges to G . If V (P∗,PG(Z))
n n∈N Lip(L)
∈ G ∈ G ≥
V Lip(L)(P∗,PGn(Z)), then
V (P∗,PG(Z)) V (P∗,PGn(Z))
Lip(L) Lip(L)
−
1+e−W(Gn(Z)) 1+e−W(G(Z))
sup E log log
≤ W∈Lip(L) h (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)i
sup E W(G (Z)) W(G(Z))
n
≤ W∈Lip(L) h − i
L G G .
n ∞
≤ k − k
ThecaseV Lip(L)(P∗,PG(Z))<V Lip(L)(P∗,PGn(Z))canbeboundedanalogously. Therefore,T iscontinuous
and there is at least one minimizer if is compact.
G
97.2 Proofs for Section 3
Before we prove the main results from Section 3 we require an auxiliary lemma:
Lemma 7.1. For X P and Y Q and an arbitrary set of measurable functions we have that
∼ ∼ W
V (P,Q) sup E[ log 1+e−W(X) +log 1+e−W(Y) ].
W
≤ −
W∈W (cid:0) (cid:1) (cid:0) (cid:1)
Proof. Since
log(1+ex)+log(1+e−x) log(4) for all x R,
≥ ∈
we can bound
sup E log 1+e−W(X) log 1+eW(Y)
W∈W h− (cid:0) (cid:1)− (cid:0) (cid:1)i
= sup E[ log 1+e−W(X) +log 1+e−W(Y) log 1+e−W(Y) log 1+eW(Y) ]
− − −
W∈W (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
sup E[ log 1+e−W(X) +log 1+e−W(Y) ] inf E[log 1+e−W(Y) +log 1+eW(Y) ]
≤
W∈W
−
(cid:0) (cid:1) (cid:0) (cid:1)
−W∈W
(cid:0) (cid:1) (cid:0) (cid:1)
sup E[ log 1+e−W(X) +log 1+e−W(Y) ] log(4).
≤ − −
W∈W (cid:0) (cid:1) (cid:0) (cid:1)
Proof of Theorem 3.1. Defining
1+e−x
ψ: R R, ψ(x):= log ,
→ − (cid:16) 2 (cid:17)
we can rewrite
V (P,Q)= sup E[ψ(W(X))+ψ( W(Y))].
Lip(L,B)
−
W∈Lip(L,B)
The function f: [ log(2 2/L), ) R, f(x)=log(2ex 1)is bijective andLipschitz continuouswith
− − ∞ → −
Lipschitz constantL and satisfies ψ( f(x))=x for all x log(2 2/L). Therefore,we obtaina lower
− ≥− −
bound
V (P,Q) sup E[ψ(f(W(X)))+ψ( f(W(Y)))]
Lip(L,B)
≥ −
W∈Lip(1,log((1+eB)/2))
W(·)≥−log(2−2/L)
= sup E[ψ(f(W(X))) W(Y)].
W∈Lip(1,B′) −
W(·)≥−log(2−2/L)
Since f−1 Lip(1,R), we can estimate V from above by
Lip(L,B)
∈
V (P,Q)= sup E[ψ(f(f−1(W(X))))+ψ( f(f−1(W(Y))))]
Lip(L,B)
−
W∈Lip(L,B)
sup E[ψ(f(W(X)))+ψ( f(W(Y)))]
≤ −
W∈Lip(L,B)
W(·)>log(2)
= sup E[ψ(f(W(X))) W(Y)].
−
W∈Lip(L,B)
W(·)>log(2)
A Taylor approximation at zero of the function ψ f(x) = log(2 e−x) yields that for every x
◦ − ∈
( log(2), ) there exists a ξ between x and 0 such that
− ∞
eξ
ψ f(x)=x x2.
◦ − (2eξ 1)2
−
For the lower bound, we thus conclude
L(L 1)
V (P,Q) sup E[W(X) W(Y)] − E[W(X)2].
Lip(L,B) ≥ W∈Lip(1,B′) − − 2
W(·)≥−log(2−2/L)
For the upper bound, we get
eB
V (P,Q) sup E[W(X) W(Y)] E[W(X)2].
Lip(L,B) ≤ − − (2eB 1)2
W∈Lip(L,B) −
W(·)>log(2)
10Note that, using the function g: ( ,log(2 2/L)) R, g(x) = log(2e−x 1), we obtain lower
−∞ − → − −
and upper bounds with a penalty term depending on E[W(Y)2] instead of E[W(X)2].
Proof of Theorem 3.2. We prove the lower bound first. Theorem 3.1 yields
L(L 1)
V (P,Q) sup E[W(X) W(Y)] − E[W(X)2]
Lip(L,B) ≥ W∈Lip(1,B′) − − 2
W(·)>−log(2−2/L)
L(L 1)
sup E[W(X) W(Y)] − E[W(X)2].
≥ − − 2
W∈Lip(1,log(2−2/L))
LetW∗ argmax E[W(X) W(Y)]. ThenδW∗ Lip(1,log(2 2/L))for all δ (0,1] and we
∈ − ∈ − ∈
W∈Lip(1,log(2−2/L))
can conclude
L(L 1)
sup E[W(X) W(Y)] − E[W(X)2]
− − 2
W∈Lip(1,log(2−2/L))
L(L 1)
sup E[δW∗(X) δW∗(Y)] − E[(δW∗(X))2]
≥ δ∈(0,1]n − − 2 o
L(L 1)
= sup δE[W∗(X) W∗(Y)] δ2 − E[(W∗(X))2] ,
δ∈(0,1]n − − 2 o
which is independent from B. In case ∆ := E[W∗(X) W∗(Y)] < L(L 1)E[W∗(X)2] we have for
− −
δ = ∆ (0,1)
E[W∗(X)2]L(L−1)
∈
L(L 1)
sup E[W(X) W(Y)] − E[W(X)2]
− − 2
W∈Lip(1,log(2−2/L))
∆2 ∆2
≥ E[W∗(X)2]L(L 1) − 2E[W∗(X)2]L(L 1)
− −
∆2
=
2E[W∗(X)2]L(L 1)
−
∆2
,
≥ 2log(2 2/L)2L(L 1)
− −
where we used W∗(x) log(2 2/L) in the last step. In case ∆ L(L 1)E[W∗(X)2] we obtain
| |≤ − ≥ −
L(L 1) 1
E[W∗(X) W∗(Y)] − E[W∗(X)2] E[W∗(X) W∗(Y)].
− − 2 ≥ 2 −
Using
∆= sup E[W(X) W(Y)]
−
W∈Lip(1,log(2−2/L))
log(2 2/L))
sup E[W(X) W(Y)]= − W (P,Q),
≥ W∈Lip(log(2−2/L))d−1/p,∞) − d1/p 1
we conclude the claimed lower bound for
1log(2 2/L) 1
c = − , c = .
1 2 dp1 2 2dp2 L(L 1)
−
For the upper bound we use Lemma 7.1 with = Lip(L). Since for W Lip(L) the function
W ∈
log 1+e−W(·) Lip(L) we conclude
− ∈
(cid:0) (cid:1)
V (P,Q) sup E[ψ(W(X))+ψ(W(Y))]
Lip(L,B)
≤
W∈Lip(L)
sup E[ log 1+e−W(X) +log 1+e−W(Y) ]
≤ −
W∈Lip(L) (cid:0) (cid:1) (cid:0) (cid:1)
sup E[W(X) W(Y)]
≤ −
W∈Lip(L)
=L sup E[W(X) W(Y)].
−
W∈Lip(1)
117.3 Proofs for Section 4
Proof of Theorem 4.1. Using Theorem 3.2 and the triangle inequality for the Wasserstein distance, we
deduce for every G and c=max(c−1,c−1/2) that
∈G 1 2
W (P∗,PGˆ n(Z)) W (P∗,P )+W (P ,PGˆ n(Z))
1 1 n 1 n
≤
W (P∗,P )+c V (P ,PGˆ n(Z)) 1;1/2
1 n Lip(L,B) n
≤
W (P∗,P
)+c(cid:2)
V (P ,PG(Z))
1(cid:3);1/2
1 n Lip(L,B) n
≤
W (P∗,P
)+c(cid:2)
L W (P ,PG(Z))
1;1/2(cid:3)
1 n 1 n
≤
(1+cL) W (P∗,(cid:2) P ) 1;1/2 +cL (cid:3) W (P∗,PG(Z)) 1;1/2 .
1 n 1
≤
(cid:2) (cid:3) (cid:2) (cid:3)
As G was arbitrary, we can choose the infimum over .
∈G G
Proof of Corollary 4.2. For every measurable G∗: and any G we have
Z →X ∈G
W (P∗,PGˆ n(Z)) W (P∗,PG∗(Z))+W (PG∗(Z),PG(Z))
1 1 1
≤
=W (P∗,PG∗(Z))+ sup E[W(G∗(Z)) W(G(Z))]
1
−
W∈Lip(1)
W (P∗,PG∗(Z))+E[G∗(Z) G(Z) ]
1 p
≤ | − |
W (P∗,PG∗(Z))+ G∗ G .
1 ∞
≤ k − k
Since G∗ was arbitrary, Theorem 4.1 yields for some constant c
E[W (P∗,PG(Z))] c E[max( W (P ,P∗),W (P ,P∗))]
1 1 n 1 n
≤ ·
+c inp f [W (P∗,PG∗(Z))]1;1/2+[inf G G∗ 1;1/2]
·G∗:Z→Xn 1 G∈Gk − k∞ o
Using Jensen’s inequality, we can bound the stochastic error term by
E[max( W (P ,P∗),W (P ,P∗))] E[ W (P ,P∗)]+E[W (P ,P∗)]
1 n 1 n 1 n 1 n
≤
p p E[W (P ,P∗)]+E[W (P ,P∗)].
1 n 1 n
≤
p
From Schreuder (2020, Theorem 4) we know
n−1/d, d>2
E[W (P∗,P )] c′n−1/2log(n), d=2
1 n ≤ 
n−1/2, d=1.

where c depends only on d. Since (logn)/√n 1, we conclude
≤
n−1/2d, d>2
E[W (P ,P∗)]+E[W (P ,P∗)] 2c′n−1/4(logn)1/2, d=2
p
1 n 1 n ≤ 
n−1/4, d=1.

Proof of Theorem 4.3. Withthe samereasoningasinthe proofofCorollary4.2,thereexistssomec such
that for any measurable G∗: and any G
Z →X ∈G
E[W (P∗,PG(Z))] c E[W (P ,P∗)]+E[W (P ,P∗)]+[W (P∗,PG∗(Z))]1;1/2+[inf G∗ G ]1,1/2
1 1 n 1 n 1 ∞
≤ G∈Gk − k
(cid:0)p (cid:1)
By the triangle inequality
W (P ,P∗) W (P ,PG∗(Z))+W (PG∗(Z),P∗).
1 n 1 n 1
≤
Let Z U be i.i.d. random variables and denote the corresponding empirical measure by U . For
i n
∼
G∗ Lip(M, ) we can then bound the first term by
∈ Z
n
W (P ,PG∗(Z))= sup 1 W(X ) E[W G∗(Z)]
1 n i
n − ◦
W∈Lip(1) Xi=1
12n n
1 1
sup W(X ) W G∗(Z ) + sup W G∗(Z ) E[W G∗(Z)]
i i i
≤ n | − ◦ | n ◦ − ◦
W∈Lip(1) Xi=1 W∈Lip(1) Xi=1
n n
1 1
X G∗(Z ) + sup f(Z ) E[f(Z)] (13)
i i p i
≤ n | − | n −
Xi=1 f∈Lip(M) Xi=1
n
1
= X G∗(Z ) +M W (U ,U)
i i p 1 n
n | − | ·
Xi=1
Hence,
n
E W (P ,PG∗(Z)) 1 E[X G∗(Z ) ]+M E[W (U ,U)].
1 n i i p 1 n
≤ n | − | ·
(cid:2) (cid:3) Xi=1
NotethatE[X G∗(Z ) ]=W (PG∗(Z),P∗)bythe dualityformulaofW usedinthiswork,seeVillani
i i p 1 1
| − |
(2008, Definition 6.2 and Remark 6.5). For E[W (U ,U)], we can exploit the convergence rate for the
1 n
empiricaldistributionasinCorollary4.2,butnowinthed∗ dimensionallatentspace . Therefore,there
Z
exists a c′ such that
n−1/2d∗ , d∗ >2
E[W (P ,P∗)]+E[W (P ,P∗)] c′[W (PG∗(Z),P∗)]1;1/2+c′n−1/4(logn)1/2, d∗ =2
p
1 n 1 n ≤ 1 
n−1/4, d∗ =1.

7.4 Proofs of Theorem 5.1 and Theorem 5.2
Proof of Theorem 5.1. First, we verify that for any two nonempty sets and we have
1 2
W W
V (P,Q) V (P,Q)+2 inf sup W W∗ . (14)
W1
≤
W2
W∈W2W∗∈W1k −
k∞
Indeed, the difference V (P,Q) V (P,Q) is bounded by
W1
−
W2
1+e−W∗(X) 1+eW∗(Y)
inf sup E log log
W∈W2W∗∈W1n h− (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)i
1+e−W(X) 1+eW(Y)
E log log
− h− (cid:16) 2 (cid:17)− (cid:16) 2 (cid:17)io
1+e−W∗(X) 1+e−W(X)
inf sup E log +log
≤W∈W2W∗∈W1n h(cid:12) (cid:12)− (cid:16) 2 (cid:17) (cid:16) 2 (cid:17)(cid:12) (cid:12)i
(cid:12) 1+eW∗(Y) 1+eW(Y) (cid:12)
+E log +log
h(cid:12)− (cid:16) 2 (cid:17) (cid:16) 2 (cid:17)(cid:12)io
inf sup E[W(cid:12) (cid:12) ∗(X) W(X)]+E[W∗(Y) W(Y)](cid:12) (cid:12)
≤W∈W2W∗∈W1(cid:8) | − | | − |
(cid:9)
2 inf sup W∗ W ,
∞
≤ W∈W2W∗∈W1k − k
due to Lipschitz continuity of x log((1+ex)/2).
7→−
From (14) we deduce for Lip(L,B)
W ⊂
V (P,Q) V (P,Q)+2 inf sup W W′ .
Lip(L,B) ≤ W W′∈WW∈Lip(L,B)k − k∞
We abbreviate ∆
W
:= inf W′∈Wsup W∈Lip(L,B)kW −W′ k∞. Now we can proceed as in Theorem 4.1.
In particular, it is sufficient to bound W 1(P n,PGˆ n(Z)). Due to Theorem 3.2 there is some constant c>0
such that for every G
∈G
W (P ,PGˆ n(Z)) c[V (P ,PGˆ n(Z))]1;1/2
1 n Lip(L,B) n
≤
c[V (P ,PGˆ n(Z))+2∆ ]1;1/2
W n W
≤
13c[V (P ,PGˆ n(Z))]1;1/2+2c[∆ ]1;1/2
W n W
≤
c[V (P ,PG(Z))]1;1/2+2c[∆ ]1;1/2
W n W
≤
Because V (P ,PG(Z)) V (P ,PG(Z)) due to Lip(L,B), the rest of the proof is identical
W n Lip(L,B) n
≤ W ⊂
to the proof of Theorem 4.1.
Proof of Theorem 5.2. Since W (P ,P∗) can be estimated as in Theorem 4.3, we only need to bound
1 n
W 1(P n,PGˆ n(Z)). For L = Γ,B = Γ/2 we have Lip(L,B) α(Γ), α (0,1), and the assumptions of
⊂ H ∈
Theorem 3.2 are satisfied. Therefore for every α (0,1)
∈
W (P ,PGˆ n(Z)) c V (P ,PGˆ n(Z)) 1;1/2 c V (P ,PGˆ n(Z)) 1;1/2 .
1 n Lip(L,B) n Hα(Γ) n
≤ ≤
(cid:2) (cid:3) (cid:2) (cid:3)
Now, (14) yields
V Hα(Γ)(P n,PGˆ n(Z)) V W(P n,PGˆ n(Z))+2∆
W
for ∆
W
:= inf sup W∗ W ∞.
≤ W∈WW∗∈Hα(Γ)k − k
Using that Gˆ is the empirical risk minimizer and α(Γ), we thus have
n
W ⊆H
W (P ,PGˆ n(Z)) c V (P ,PGˆ n(Z)) 1;1/2 +c[∆ ]1;1/2
1 n W n W
≤
c(cid:2)
V (P ,PG(Z))
1(cid:3);1/2
+c[∆ ]1;1/2
W n W
≤
c(cid:2) V (P ,PG((cid:3) Z)) 1;1/2 +c[∆ ]1;1/2
Hα(Γ) n W
≤
(cid:2) (cid:3)
To bound the first term, we apply Lemma 7.1 and log(1+e−W(·)) W α(Γ) α(Γ) to obtain
{− | ∈H }⊂H
V (P ,PG(Z)) sup E [ log 1+e−W(Xˆ) +log 1+e−W(G(Z)) ]
Hα(Γ) n
≤ W∈Hα(Γ)
Xˆ∼Pn
− (cid:16) (cid:17) (cid:16) (cid:17)
sup E [W(Xˆ) W(G(Z))]
≤
W∈Hα(Γ)
Xˆ∼Pn
−
sup E [W(Xˆ) W(X)]+ sup E[W(X) W(G(Z))].
≤
W∈Hα(Γ)
Xˆ∼Pn
−
W∈Hα(Γ)
−
For the second term we have by Hölder continuity, Jensens inequality and the duality formula of W as
1
used in the proof of Theorem 4.3 that
sup E[W(X) W(G(Z))] sup E[W(X) W(G∗(Z))]+ sup E[W(G∗(Z)) W(G(Z))]
− ≤ | − | | − |
W∈Hα(Γ) W∈Hα(Γ) W∈Hα(Γ)
ΓE[X G∗(Z)α]+Γ G∗ G α
≤ | − |p k − k∞
ΓW (P∗,PG∗(Z))α+Γ G∗ G α .
≤ 1 k − k∞
Hence, we have for any G and any measurable G∗: for some constant c>0
∈G Z →X
W (P ,PGˆ n(Z)) c sup E [W(Xˆ) W(X)] 1;1/2
1 n
≤
(cid:2)W∈Hα(Γ)
Xˆ∼Pn
−
(cid:3)
+c W (P∗,PG∗(Z))α+ G∗ G α 1;1/2 +c[∆ ]1;1/2.
1 k − k∞ W
(cid:2) (cid:3)
For the remaining stochastic error term, we first note that
sup E [W(Xˆ) W(X)] sup E [W(X ) W(G∗(Z))]
W∈Hα(Γ)
Xˆ∼Pn
− ≤
W∈Hα(Γ)
Xn∼Pn n
−
+ sup E[W(G∗(Z)) W(X)]
−
W∈Hα(Γ)
sup E [W(X ) W(G∗(Z))]+ΓW (P∗,PG∗(Z))α
≤
Xn∼Pn n
−
1
W∈Hα(Γ)
and as in (13) together with Schreuder (2020, Theorem 4) we obtain
E sup E [W(X ) W(G∗(Z))] E sup X G∗(Z)α
hW∈Hα(Γ) Xn∼Pn n − i≤ hW∈Hα(Γ)| − |p i
14n
1
+E sup f(Z ) E[f(Z)]
i
hf∈Hα(M·Γ)n Xi=1 − i
n−α/d∗ , 2α<d∗,
cW (P∗,PG∗(Z))α+cn−1/2ln(n), 2α=d∗,
≤ 1 
n−1/2, 2α>d∗.

For the expectation of the first term we use Jensen’s inequality
E[X G∗(Z )α] E[X G∗(Z ) ]α =W (P∗,PG∗(Z))α.
| i − i |p ≤ | i − i |p 1
7.5 Proof of Theorem 5.3
To prove Theorem 5.3 some additional notation is required. The set of locally integrable functions is
given by
L1 (Ω):= f: Ω R f(x) dx< , for all compact K Ω◦ .
loc
{ → (cid:12)Z K| | ∞ ⊂ }
(cid:12)
A function f L1 (Ω) has a weak deri(cid:12)vative, Dαf, provided there exists a function g L1 (Ω) such
∈ loc w ∈ loc
that
g(x)φ(x)dx=( 1)|α| f(x)φ(α)(x)dx for all φ C∞(Ω) with compact support.
Z − Z ∈
Ω Ω
If such a g exists, we define Dαf :=g. For f L1 (Ω) and k N the Sobolev norm is
w ∈ loc ∈ 0
kf kWk,∞(Ω) := |m α|a ≤x kkD wαf k∞,Ω.
The Sobolev space Wk,∞(Ω) := f L1 (Ω) : f < is a Banach space (Brenner & Scott,
{ ∈ loc k kW pk(Ω) ∞}
2008,Theorem 1.3.2). For f Wk,∞(Ω), define the Sobolev semi norm by
∈
|f |Wk,∞(Ω) := |m α|a =x kkD wαf k∞,Ω.
Note that Lip(L,B,Ω) W1,∞(Ω), since f W1,∞ max(L,B) for any f Lip(L,B,Ω). For two
⊂ k k ≤ ∈
normed spaces (A, ),(B, ) we denote the operator norm of a linear operator T: A B by
A B
k·k k·k →
T :=sup Tx x A, x 1 .
B A
k k k k | ∈ k k ≤
(cid:8) (cid:9)
Theorem 5.3 is very close to Theorem 4.1 by Gühring et al. (2020), which however applies only to
functions f which are at least twice (weakly) differentiable. Our proof can thus build on numerous
auxiliary results and arguments from Gühring et al. (2020). We basically keep the proof structure of
Gühring et al. (2020) which in turn relies on Yarotsky (2017).
Let d,N N. For m 0,...,N d, define the functions φ : Rd R,
m
∈ ∈{ } →
1, x <1,
d
m | |
φ (x)= ψ 3N x ℓ , where ψ(x)=0, x >2,
m ℓY=1 (cid:16) (cid:16) ℓ − N (cid:17)(cid:17) 
2 x,
|
1
|
x 2
−| | ≤| |≤

By definition, we have φ =1 for all m and
m ∞
k k
m 1 m
suppφ x: x k < k =:B ( ). (15)
m
⊂(cid:26) (cid:12)
k
− N (cid:12) N∀ (cid:27)
N1,|·|∞
N
(cid:12) (cid:12)
(cid:12) (cid:12)
Gühring et al. (2020, Lemma C.3 (iv)) have verified that kφ m kW1,∞(Rd) ≤cN for some constant c>0.
A direct consequence of Lemma 2.11, Lemma C.3, Lemma C.5 and Lemma C.6 by Gühring et al.
(2020) is the following approximationresult for the localizing functions φ via ReLU networks:
m
Lemma 7.2. For any ε (0,1/2) and any m 0,...,N d there is a network Ψ with ReLU activation
ε
function, not more than C∈ log (ε−1) layers and∈ n{ o more t} han C (N+1)dlog2(ε−1) nonzero weights and
1 2 2 2
no more than neurons C (N +1)d(log2(ε−1) log (ε−1)) such that for k 0,1
3 2 ∨ 2 ∈{ }
kΨ ε −φ m kWk,∞ ≤cNkε,
15where C ,C ,C and c are constants independent of m and ε. Additionally,
1 2 3
φ (x)=0= Ψ (x)=0,
m ε
⇒
and therefore suppΨ B (m).
ε ⊂ N1,|·|∞ N
Next we approximate a bounded Lipschitz function using linear combinations of the set φ : m
m
{ ∈
1,...,N d . The approximationerror will be measured in the Hölder norm from (4).
{ } }
Lemma 7.3. Let 0 < α < 1. There exists a constant C > 0 such that for any f W1,∞((0,1)d) there
1
∈
are constants c for m 0,...,N d such that
f,m
∈{ }
1 1−α
(cid:13)
(cid:13)f
− m∈{X
0,...,N}dc f,mφ
m
(cid:13) (cid:13)Hα
≤C
1
(cid:16)N (cid:17)
kf kW1,∞.
(cid:13) (cid:13)
The coefficients satisfy for a C >0
2
|c
f,m
|≤C
2
kf˜ kW1,∞(Ωm,N),
where Ω :=B (m) and f˜ W1,∞(R) is an extension of f.
m,N N1,|·|∞ N ∈
Proof. Let E: W1,∞((0,1)d) W1,∞(R) be the continuous linear extension operator from (Stein, 1970,
Theorem 5) and set
f˜:=Ef.→
As E is continuous there exists a C >0 such that
E
kf˜ kW1,∞(Rd) ≤C E kf kW1,∞.
Step 1 (Choice of c ): For each m 0,...,N d we define
f,m
∈{ }
m
c = f˜(y)ρ(y) dy for B :=B
f,m Z Bm,N m,N 43 N,|·| (cid:16)N (cid:17)
and an arbitrary cut-off function ρ supported in B , i.e.
m,N
ρ C∞(Rd) with ρ(x) 0 for all x Rd, suppρ=B and ρ(x)dx=1.
∈ c ≥ ∈ m,N Rn
R
Then
|c
m,f
|=
(cid:12) (cid:12)Z Bm,N
f˜(y)ρ(y) dy
(cid:12)
(cid:12)≤kf˜
k∞,Ωm,N
Z Bm,N
ρ(y) dy = kf˜
k∞,Ωm,N
≤C
E
kf kW1,∞(Ωm,N).
(cid:12) (cid:12)
Step 2 (Local estimates in ): The coefficients c are the averaged Taylor polynomials in the
k·kWk,p m,f
senseofBrenner&Scott(2008,Definition4.1.3)oforder1averagedoverB .AsGühringetal.(2020,
m,N
Proof of Lemma C.4, Step 2) showed, the conditions of the Bramble-Hilbert-Lemma (Brenner & Scott,
2008,Theorem 4.3.8) are satisfied. Hence for k 0,1
∈{ }
2√d 1−k 1 1−k
|f˜ −c m,f |Wk,∞(Ωm,N) ≤C 1
(cid:16)
N
(cid:17)
|f˜ |W1,∞(Ωm;n) ≤C 2
(cid:16)N
(cid:17)
kf˜ kW1,∞(Ωm;n).
Now using φ as defined above, we get
m
1
(cid:13)φ m (cid:16)f˜ −c f,m
(cid:17)(cid:13)∞,Ωm,N
≤kφ m k∞,Ωm,N ·(cid:13)f˜ −c f,m
(cid:13)∞,Ωm,N
≤C 2 Nkf˜ kW1,∞(Ωm,N) (16)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Due tothe productinequalityfor weakderivatives(Gühring etal.,2020,LemmaB.6)there is aconstant
C′ >0 such that the supremum norm of the weak derivative is bounded by
(cid:12)φ m (cid:16)f˜ −c f,m
(cid:17)(cid:12)W1,∞(Ωm,N)
≤C′ |φ m |W1,∞(Ωm,N) ·(cid:13)f˜ −c f,m
(cid:13)∞,Ωm,N
(cid:12) (cid:12) (cid:13) (cid:13)
(cid:12) (cid:12) +C′ kφ m k∞,Ωm,N ·(cid:13) |f˜ −c f,m |(cid:13) W1,∞(Ωm,N)
1
≤C′ ·cN ·C 2 Nkf˜ kW1,∞(Ωm,N)+C′ ·C 3 kf˜ kW1,∞(Ωm,N)
=C
4
kf˜ kW1,∞(Ωm,N). (17)
16Combining (16) and (17) we get
(cid:13)φ
m
(cid:16)f˜ −c
f,m
(cid:17)(cid:13)W1,∞(Ωm,N)
≤C
5
kf˜ kW1,∞(Ωm,N).
(cid:13) (cid:13)
(cid:13) (cid:13)
Step 3 (Global estimate in ): As φ =1, we have that
k·kWk,p m∈{0,...,N}d m
P
f˜(x)= φ (x)f˜(x), for a.e. x (0,1)d.
m
∈
m∈{0X ,...,N}d
As f˜ =f we have for k 0,1
(0,1)d ∈{ }
(cid:12)
(cid:12)
f φ c = f˜ φ c
m f,m m f,m
(cid:13) (cid:13) − m∈{0X ,...,N}d (cid:13) (cid:13)Wk,∞((0,1)d) (cid:13) (cid:13) − m∈{0X ,...,N}d (cid:13) (cid:13)Wk,∞((0,1)d)
(cid:13) (cid:13) (cid:13) (cid:13)
= φ f˜ c
m f,m
(cid:13) (cid:13)m∈{X 0,...,N}d (cid:0) − (cid:1)(cid:13) (cid:13)Wk,∞((0,1)d)
(cid:13) (cid:13)
sup φ f˜ c (18)
m f,m
≤ me∈{0,...,N}d (cid:13) (cid:13)m∈{0X ,...,N}d (cid:0) − (cid:1)(cid:13) (cid:13)Wk,∞(Ωmf,N)
(cid:13) (cid:13)
where the last step follows from (0,1)d
⊂
me∈{0,...,N}dΩ me,N. Now we obtain for each m
∈
{0,...,N }d
using (15), (16) and (17) S
e
φ f˜ c sup φ f˜ c
(cid:13) (cid:13) (cid:13)m∈{X 0,...,N}d m (cid:0) − f,m (cid:1)(cid:13) (cid:13) (cid:13)Wk,∞(Ωm˜,N) ≤ m |m∈ −{0 me,.. |∞.,N ≤} 1d(cid:13) (cid:13) m (cid:0) − f,m (cid:1)(cid:13) (cid:13)Wk,∞(Ωm˜,N)
sup φ f˜ c
≤
m∈{0,...,N}d(cid:13)
m
(cid:0)
− f,m (cid:1)(cid:13)Wk,∞(Ωm,N)
|m−me|∞≤1 (cid:13) (cid:13)
1 1−k
≤C
6
(cid:16)N
(cid:17)
m∈{0s ,u ..p .,N}d,kf˜ kW1,∞(Ωm,N).
|m−me|∞≤1
Plugging this into (18), we obtain for k 0,1
∈{ }
1 (1−k)
(cid:13)
(cid:13)f
− m∈{X
0,...,N}dφ mc f,m
(cid:13) (cid:13)Wk,∞((0,1)d)
≤C 6
(cid:16)N (cid:17)
m˜∈{s 0u ,..p .,N}d(cid:16)m∈{0s ,u ..p .,N}d,kf˜ kW1,∞(Ωm,N)
(cid:17)
(cid:13) (cid:13) |m−me|∞≤1
1 (1−k)
≤C 7
(cid:16)N
(cid:17)
me∈{s 0u ,..p .,N}dkf˜ kW1,∞(Ωmf,N)
1 (1−k)
≤C 8
(cid:16)N
(cid:17)
kf˜ kW1,∞(Rd)
1 (1−k)
≤C
9
(cid:16)N
(cid:17)
kf kW1,∞((0,1)d). (19)
Step4(Interpolation): DefinethelinearoperatorsT : W1,∞((0,1)d) L∞((0,1)d),T : W1,∞((0,1)d)
0 α
→ →
α((0,1)d) and T : W1,∞((0,1)d) W1,∞((0,1)d) via
1
H →
T (f)=f φ c , k 0,α,1
k m f,m
− ∈{ }
m∈{0X ,...,N}d
Note that the linearity follows from the definition of the constants c . Using Lunardi (2018, Theorem
f,m
1.6), for the nontrivial interpolation couple see (Lunardi, 2018, p.11 f.), leads to
T T 1−α T α.
α 0 1
k k≤k k k k
Note that Hα is equivalent to Ws,∞(Ω) in Gühring et al. (2020). Using (19) we conclude
k·k k·k
1 1−α
(cid:13)
(cid:13)f
− m∈{0X
,...,N}dφ mc
f,m
(cid:13) (cid:13)Hα
≤C
10
(cid:16)N (cid:17)
kf kW1,∞.
(cid:13) (cid:13)
17Nowwewanttoapproximatethefunction c φ inHöldernormusingaReLUnetwork.
m∈{0,...,N}d f,m m
P
Lemma 7.4. For any ε (0,1/2) there is a neural network Φ with ReLU activation function such that
ε
∈
for (c ) from Lemma 7.3, there is a constant C >0 such that
f,m m
φ mc
f,m
Φ
ε
C f W1,∞Nαε,
(cid:13) (cid:13)m∈{0X ,...,N}d − (cid:13) (cid:13)Hα ≤ k k
(cid:13) (cid:13)
the number of layers is at most C log (ε−1) , the number of nonzero weights is at most C (N +
1)dlog2(ε−1) and the number of ⌈ neu1 rons2 is at⌉ most C (N +1)d(log2(ε−1) log (ε−1)) , wit⌈ h 2 C ,C
2 ⌉ ⌈ 3 2 ∨ 2 ⌉ 1 2
and C from Lemma 7.2.
3
Proof. FromLemma7.2weknowthatthereareneuralnetworksΨ withatmost C log (ε−1) layers,
C (N+1)dlog2(ε−1) nonzeroweightsand C (N+1)d(log2(ε−1) ε, lm og (ε−1)) neur⌈ ons1 tha2 tappr⌉ oximate
⌈ 2 2 ⌉ ⌈ 3 2 ∨ 2 ⌉
φ such that for k 0,1
m
∈{ }
φ Ψ c′Nkε.
k
m
−
ε,m kWk,∞
≤
Nowweparallelizethesenetworksandmultiplywiththecoefficientsc afterwards. Hereby,weconstruct
f,m
anetworkΦ with1+ C log (ε−1) layers,Nd+ C (N+1)dlog2(ε−1) nonzeroweightsand1+ C (N+
1)d(log2(ε−1ε
) log
(ε⌈ −1)1
)
n2 eurons⌉
such that
⌈ 2 2 ⌉ ⌈ 3
2 ∨ 2 ⌉
Φ = c Ψ . (20)
ε f,m ε,m
m∈{0X ,...,N}d
For each m 0,...,N d denote Ω =B (m) as above. For k 0,1 we get
∈{ } m,N N1,|·|∞ N ∈{ }
Φ c φ = c (Ψ φ )
ε f,m m f,m ε,m m
(cid:13) (cid:13) − m∈{X 0,...,N}d (cid:13) (cid:13)Wk,∞((0,1)d) (cid:13) (cid:13)m∈{0X ,...,N}d − (cid:13) (cid:13)Wk,∞((0,1)d)
(cid:13) (cid:13) (cid:13) (cid:13)
sup c (Ψ φ )
f,m ε,m m
≤ me∈{0,...,N}d(cid:13) (cid:13)m∈{0X ,...,N}d − (cid:13) (cid:13)Wk,∞(Ωmf,N∩(0,1)d)
(cid:13) (cid:13)
3d sup sup c (Ψ φ )
f,m ε,m m
≤ me∈{0,...,N}dm∈{0,...,N}d(cid:13) − (cid:13)Wk,∞(Ωmf,N∩(0,1)d)
(cid:13) (cid:13)
(cid:13) (cid:13)
3d sup sup c (Ψ φ )
f,m ε,m m
≤ me∈{0,...,N}dm∈{0,...,N}d| |(cid:13) − (cid:13)Wk,∞(Ωmf,N∩(0,1)d)
(cid:13) (cid:13)
≤3d me∈{s 0u ,..p .,N}dm∈{s 0u ,..p .,N}dkf˜ kW(cid:13) 1,∞(Ωm,N) kΨ ε,m(cid:13) −φ m kWk,∞(Ωmf,N∩(0,1)d)
CNkε f W1,∞.
≤ k k
The second to last inequality follows from the fact that on Ω me,N is within the support of φ m only for
m m 1. The last inequality follows from (20) and the continuity of the extension operator, see
∞
| − | ≤
(Stein, 1970, Theorem 5). As in Step 4 of Lemma 7.3, we conclude using Lunardi (2018, Theorem 1.6)
e
Φ
ε
φ mc
f,m
CNαε f W1,∞.
(cid:13) (cid:13) − m∈{0X ,...,N}d (cid:13) (cid:13)Hα ≤ k k
(cid:13) (cid:13)
Now we are ready to proof Theorem 5.3.
Proof of Theorem 5.3. Combining Lemma 7.3 and Lemma 7.4 with f W1,∞ B yields for a constant
k k ≤
C >0 for any ε˜ (0,1/2) that
∈
f Φ f c φ + c φ Φ
ε˜ Hα f,m m f,m m ε˜
k − k ≤(cid:13) (cid:13) − m∈{0X ,...,N}d (cid:13) (cid:13)Hα (cid:13) (cid:13)m∈{X 0,...,N}d − (cid:13) (cid:13)Hα
(cid:13) (cid:13) (cid:13) (cid:13)
1 1−α
CB +Nαε˜ , (21)
≤ (cid:16)(cid:16)N (cid:17) (cid:17)
where ε˜determines the approximation accuracy in Lemma 7.4. For
ε −1/(1−α)
N := ,
(cid:24)(cid:16)2CB (cid:17) (cid:25)
18we get for the first term in (21)
1 1−α ε
.
(cid:16)N (cid:17) ≤ 2CB
Choosing
ε˜=
ε ε − 1−1
α +1
−α
(22)
2CB 2CB
(cid:16)(cid:16) (cid:17) (cid:17)
leads to
f Φ ε.
ε˜ Hα
k − k ≤
From Lemma 7.2 we know that there is a ReLU network with no more than 1+ C log (ε˜−1) layers,
Nd+ C (N +1)dlog2(ε˜−1) nonzero weights and 1+ C (N +1)d(log2(ε˜−1) log⌈ (ε˜−1 1))2 neuro⌉ ns with
⌈ 2 2 ⌉ ⌈ 3 2 ∨ 2 ⌉
the required properties. Inserting (22) and assuming CB > 1 yields
2
log 2(ε˜−1) ≤log
2
(cid:16)2C εB
2α
(cid:16)2Cε
B
(cid:17)− 1−α
α (cid:17)≤C′log 2(ε− 1−1 α).
ThusthereareC′,C′′ andC′′′ suchthattheReLUnetworkhasnomorethan1+ ⌈C′log 2(ε− 1−1 α) ⌉layers,
⌈C′′ε− 1−d α log2 2(ε− 1−1 α)
⌉
nonzero weights and 1+ ⌈C′′′ε− 1−d α(log2 2(ε− 1−1 α) ∨log 2(ε− 1−1 α))
⌉
neurons.
Since f Lip(L,B) α(Γ) for Γ=max(L,2B), we conclude
∈ ⊆H
Φ f + Φ f Γ+ε.
ε˜ Hα Hα ε˜ Hα
k k ≤k k k − k ≤
Corollary 5.4 is a straightforwardcombination of Theorem 5.2 and Theorem 5.3.
7.6 Proof of Theorem 6.1
Proof of Theorem 6.1. We get with the same reasoning as in the proof of (21)
W (P∗,PGˆ n(Z)) W (P∗,P )+W (P ,PGˆ n(Z))
1 1 n 1 n
≤
W (P∗,P )+W (P ,PGˆ n(Z))+2 inf sup W W′
1 n W n ∞
≤ W′∈WW∈Lip(1,B)k − k
W (P∗,P )+W (P ,PG(Z))+2 inf sup W W′
1 n W n ∞
≤ W′∈WW∈Lip(1,B)k − k
W (P∗,P )+W (P ,PG(Z))+2 inf sup W W′ .
≤
1 n Hα(Γ) n
W′∈WW∈Lip(1,B)k −
k∞
TheboundonW (P ,PG(Z))dependingontheintrinsicdimensiond∗wasalreadyderivedinTheorem5.2.
Hα n
Along Corollary 4.2 we obtain the rate depending on the ambient dimension d.
Note that the Wasserstein distance is invariant to additive shifts, hence we can add the assumption
W(0)=0 without changing the optimal value. As is bounded, there exists a B >0 depending on the
X
norm used on . In any case B =d is sufficient. To apply Theorem 5.3 we need the Hölder constant of
X
the network to be greater or equal than 2B.
A Calculations for Example 3.3
Forthe WassersteindistancewegetW (P,Q)=γ.TheVanillaGANdistanceusingallLipschitzLaffine
1
functions as discriminator yields in this example V (P,Q)=max f(a,b) for
a·+b a,b∈R,
|a|≤L
1
f(a,b):= log 1+e−aγ−b log 1+e−a(γ+ε)−b log 1+eb log 1+eaε+b +log(4).
2 − − − −
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
Standard calculus yields for fixed a the unique maximizer b∗ = a(ε+γ) and
− 2
f(a,b∗)= log 1+e−a(ε 2+γ) log 1+ea(ε 2−γ) +log(4).
− (cid:16) (cid:17)− (cid:16) (cid:17)
19Since
∂ ε+γ ε γ
f(a,b∗)= − ,
∂a 2(ea(γ 2+ε) +1) − 2(e−a(ε 2−γ) +1)
forε γ,themaximizingaismaximala∗ =L.Thiscoincideswiththeintuitivechoice: asthesupportof
≤
PX and the support of PY can be separated by a single point on R, we expect the optimal discriminator
to be affine linear. Standard calculus yields the linear upper and lower bound for ε= 1.
4
For ε>γ, the unrestricted maximizing a∗ solves the equation
(ε
γ)ea∗ (ε 2+γ) (ε+γ)e−a∗ (ε 2−γ)
=2γ.
− −
Whilethereisnoclosedformsolution,anumericalapproximation(forε= 1)yieldsforγ <εandL>16
4
such that a∗ is feasible
W (PX,PY)2
1 V (PX,PY) a W (PX,PY)2.
a·+b 1
2 ≤ ≤ ·
References
Aggarwal,C. (2018). Neural Networks and Deep Learning: A Textbook. Springer Cham.
Anil, C., Lucas, J., & Grosse, R. (2019). Sorting out Lipschitz function approximation. In International
Conference on Machine Learning (pp. 291–301).: PMLR.
Arjovsky, M. & Bottou, L. (2017). Towards principled methods for training generative adversarial
networks.
Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein generative adversarial networks. In
International conference on machine learning (pp. 214–223).: PMLR.
Asokan, S. & Seelamantula, C. S. (2023). Euler-lagrange analysis of generative adversarial networks.
Journal of Machine Learning Research, 24(126), 1–100.
Belomestny, D., Moulines, E., Naumov, A., Puchkin, N., & Samsonov, S. (2021). Rates of convergence
for density estimation with GANs. arXiv preprint arXiv:2102.00199.
Belomestny, D., Naumov, A., Puchkin, N., & Samsonov, S. (2023). Simultaneous approximation of a
smooth function and its derivatives by deep neural networks with piecewise-polynomial activations.
Neural Networks, 161, 242–253.
Biau, G., Cadre, B., Sangnier, M., & Tanielian, U. (2018). Some theoretical properties of GANs. arXiv
preprint arXiv: 1803.07819.
Biau,G.,Sangnier,M.,&Tanielian,U.(2021). SometheoreticalinsightsintowassersteinGANs. Journal
of Machine Learning Research, 22(119), 1–45.
Brenner, S. C. & Scott, L. R. (2008). The Mathematical Theory of Finite Element Methods, volume 15
of Texts in Applied Mathematics. Springer.
Chae, M. (2022). Rates of convergence for nonparametric estimation of singular distributions using
generative adversarialnetworks. arXiv preprint arXiv:2202.02890.
Chakraborty, S. & Bartlett, P. L. (2024). On the statistical properties of generative adversarial models
for low intrinsic data dimension.
Chen, M., Liao, W., Zha, H., & Zhao, T. (2020). Distribution approximation and statistical estimation
guarantees of generative adversarialnetworks. arXiv preprint arXiv:2002.03938.
Chernodub, A. & Nowicki, D. (2016). Norm-preserving orthogonal permutation linear unit activation
functions (oplu). arXiv preprint arXiv:1604.02313.
DeVore,R. A., Hanin, B., & Petrova,G. (2021). Neuralnetworkapproximation. Acta Numerica, 30,327
– 444.
20Dudley, R. M. (1969). The speed of mean Glivenko-Cantelli convergence. The Annals of Mathematical
Statistics, 40(1), 40 – 50.
Eckstein, S. (2020). Lipschitz neural networks are dense in the set of all Lipschitz functions. arXiv
preprint arXiv:2009.13881.
Farnia, F. & Tse, D. (2018). A convex duality framework for GANs. Advances in neural information
processing systems, 31.
Gibbs,A.L.&Su,F.E. (2002). Onchoosingandbounding probabilitymetrics. International Statistical
Review / Revue Internationale de Statistique, 70(3), 419–435.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &
Bengio, Y. (2014). Generative adversarialnetworks. arXiv preprint arXiv: 1406.2661.
Gühring, I., Kutyniok, G., & Petersen, P. (2020). Error bounds for approximations with deep ReLU
neural networks in Ws,p norms. Analysis and Applications, 18(05), 803–859.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). Improved training of
Wasserstein GANs. Advances in neural information processing systems, 30.
Huang, J., Jiao, Y., Li, Z., Liu, S., Wang, Y., & Yang, Y. (2022). An error analysis of generative
adversarialnetworks for learning distributions. Journal of machine learning research, 23(116), 1–43.
Huster, T., Chiang, C.-Y. J., & Chadha, R. (2019). Limitations of the Lipschitz constant as a defense
againstadversarialexamples.InECMLPKDD2018Workshops: Nemesis2018, UrbReas2018, SoGood
2018, IWAISe2018, andGreenDataMining2018, Dublin,Ireland,September10-14, 2018, Proceedings
18 (pp. 16–29).: Springer.
Khromov, G. & Singh, S. P. (2023). Some fundamental aspects about Lipschitz continuity of neural
network functions. arXiv preprint arXiv:2302.10886.
Liang, T. (2017). How well can generative adversarial networks learn densities: A nonparametric view.
arXiv preprint arXiv:1712.08244.
Liang, T. (2021). How well generative adversarialnetworkslearn distributions. The Journal of Machine
Learning Research, 22(1), 10366–10406.
Lunardi, A. (2018). Interpolation theory.
Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). Spectral normalization for generative
adversarialnetworks. arXiv preprint arXiv:1802.05957.
Mueller, A. (1997). Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2), 429–443.
Nowozin,S.,Cseke,B.,&Tomioka,R.(2016).f-gan: Traininggenerativeneuralsamplersusingvariational
divergence minimization. Advances in neural information processing systems, 29.
Petzka, H., Fischer, A., & Lukovnicov, D. (2017). On the regularization of Wasserstein GANs. arXiv
preprint arXiv:1709.08894.
Schreuder, N. (2020). Bounding the expectation of the supremum of empirical processes indexed by
Hölder classes. Mathematical Methods of Statistics, 29(1), 76–86.
Schreuder,N.,Brunel,V.-E.,&Dalalyan,A.(2021). Statisticalguaranteesforgenerativemodelswithout
domination. Algorithmic Learning Theory, (pp. 1051–1071).
Stein,E.M.(1970). Singular Integrals and Differentiability Properties of Functions (PMS-30). Princeton
University Press.
Stéphanovitch, A., Aamari, E., & Levrard, C. (2023). Wasserstein GANs are minimax optimal
distribution estimators. arXiv preprint arXiv:2311.18613.
Suh, N. & Cheng, G. (2024). A survey on statistical theory of deep learning: Approximation, training
dynamics, and generative models. arXiv preprint arXiv:2401.07187.
21Tang, R. & Yang, Y. (2022). Minimax rate of distribution estimation on unknown submanifold under
adversariallosses. arXiv preprint arXiv:2202.09030.
Than, K. & Vu, N. (2021). Generalization of GANs and overparameterized models under Lipschitz
continuity. arXiv preprint arXiv:2104.02388.
Torres,L.C.,Pereira,L.M.,&Amini,M.H.(2021). Asurveyonoptimaltransportformachinelearning:
Theory and applications. arXiv preprint arXiv:2106.01963.
Vardanyan, E., Minasyan, A., Hunanyan, S., Galstyan, T., & Dalalyan, A. (2023). Guaranteed
optimal generative modeling with maximum deviation from the empirical distribution. arXiv preprint
arXiv:2307.16422.
Villani, C. (2008). Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.
Springer Berlin Heidelberg.
Wei, X., Liu, Z., Wang, L., & Gong, B. (2018). Improving the improved training of Wasserstein GANs.
In International Conference on Learning Representations.
Yarotsky, D. (2017). Error bounds for approximations with deep ReLU networks. Neural Networks, 94,
103–114.
Zhou, Z., Liang, J., Song, Y., Yu, L., Wang, H., Zhang, W., Yu, Y., & Zhang, Z. (2019). Lipschitz
generativeadversarialnets.InInternationalConferenceonMachineLearning(pp.7584–7593).: PMLR.
22