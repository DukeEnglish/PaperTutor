Augmented Reality based Simulated Data (ARSim) with multi-view consistency
for AV perception networks
AqeelAnwar TaeEunChoe ZianWang
NVIDIA NVIDIA NVIDIA/UniversityofToronto
manwar@nvidia.com tchoe@nvidia.com zianw@nvidia.com
SanjaFidler MinwooPark
NVIDIA/UniversityofToronto NVIDIA
sfidler@nvidia.com minwoop@nvidia.com
Abstract Inrecentyears,therehasbeenanotabletrendtowardsthe
developmentof3Dobstacledetectionsystems,leveraging
Detectingadiverserangeofobjectsundervariousdriv- the power of machine learning and camera sensors exclu-
ingscenariosisessentialfortheeffectivenessofautonomous sively[26,28,37,22]. Despitetheseadvancements,akey
driving systems. However, the real-world data collected challengeinperceptionremainsthedetectionofrarely-seen
oftenlacksthenecessarydiversitypresentingalong-taildis- objects/scenarios,essentialforensuringroadsafetyandpre-
tribution. Althoughsyntheticdatahasbeenutilizedtoover- venting accidents. Robust perception models necessitate
comethisissuebygeneratingvirtualscenes,itfaceshurdles substantial-high-qualitytrainingdata,especiallyforrarely
suchasasignificantdomaingapandthesubstantialefforts observedandpotentiallyhazardouseventsliketraffichaz-
requiredfrom3Dartiststocreaterealisticenvironments. To ards,pedestrians,children,bicycles,andmotorbikes. The
overcomethesechallenges,wepresentARSim,afullyauto- scarcityofdataforsuchscenariosposesasignificanthurdle
mated,comprehensive,modularframeworkdesignedtoen- in creating accurate and reliable detection systems. Moti-
hancerealmulti-viewimagedatawith3Dsyntheticobjects vatedbythelimitationsofreal-worlddata,includingscarcity
ofinterest. Theproposedmethodintegratesdomainadap- anddifficultiesinobtainingdiverse,annotateddatasets,re-
tation and randomization strategies to address covariate searchersexplorealternativeapproaches. Additionally,the
shiftbetweenrealandsimulateddatabyinferringessential associatedcostandtimeconstraintsincollectingreal-world
domainattributesfromrealdataandemployingsimulation- datahaveledtotheadoptionofsyntheticdatagenerationas
based randomization for other attributes. We construct a amoreefficientandcost-effectivesolution.
simplified virtual scene using real data and strategically Syntheticdatagenerationusing3Drenderingplatforms
place3Dsyntheticassetswithinit. Illuminationisachieved canbeusedtoreplicatevarioustrafficscenarioswithimpec-
by estimating light distribution from multiple images cap- cablelabels. However,aconsiderabledomaingapbetween
turingthesurroundingsofthevehicle. Cameraparameters syntheticandreal-worlddatacanleadtosub-optimalperfor-
from real data are employed to render synthetic assets in manceinresultingmodels. Whilecomputergraphics(CG)
eachframe. Theresultingaugmentedmulti-viewconsistent artistscanmanuallycrafthigh-qualityphotorealisticscenes,
datasetisusedtotrainamulti-cameraperceptionnetwork thisprocessisoftenexpensiveandlacksscalability. Another
forautonomousvehicles. Experimentalresultsonvarious approachistousethereal-drivedatasetsandinsertsynthetic
AVperceptiontasksdemonstratethesuperiorperformance assetsinthecameraframes. Proposedcopy-pastemethods
ofnetworkstrainedontheaugmenteddataset. [3,10,44,7,6]attempttoaddressdatascarcitybycropping
andpastingexistingsegmented2Dobjectsintonewonesto
simulatedifferentscenarios. However, thesemethodsfall
shortinprovidingmulti-viewconsistentimageeditingand
1.Introduction
3Dgroundtruthlabels,essentialfortrainingmulti-view3D
Autonomousvehicles(AV)heavilydependonperception perceptionmodels.
systemstonavigatesafelyandaccurately. Keytothisnavi- Inthispaper,wepresentanovelmethodforautomatically
gationistheabilitytoperceiveobstacles,enablingvehicles generatinglarge-scaleaugmenteddatatailoredforrarelycap-
todetectandsteerclearofpotentialcollisionsontheroad. turedobjects. Ourapproachdrawsinspirationfromlighting
4202
raM
22
]VC.sc[
1v07351.3042:viXraInput frame ARSim rendered frame
Rendering
ARSim on multi-cameras and label generation
Real Only Real+ARSim
Asset Pool
LDR to HDR
Estimated 360O Light Map
Metric improvements across three different perception task using ARSim
Figure1: Anoverviewoftheproposedapproachanditsimpact. (left)WegenerateanHDRlightmapfromtheinputdataand
positionassetsofinterestin3Daroundtheegocar,subsequentlyrenderingthemwithinthecameraframe,handlingcollision
andocclusionwithexistingrealobjects. Concurrently,thepipelineachievesmulti-viewconsistentframerendering(righttop).
Additionally,integratingARSimdatawithrealdataenhancesperformancemetricsacrossthreecrucialAVperceptiontasks:
obstacledetection,freespacedetection,andparkingdetection(rightbottom),asdemonstratedindetailintheresultssection.
estimationandvirtualobjectinsertiontechniquesemployed ourproposedapproach. InSection3,wedefinetheproblem
inthevisualeffectsandaugmentedrealityindustries. The andintroduceourproposedapproach. Section4presentsthe
objectiveistoseamlesslyandconvincinglycompositevir- modularARSimpipeline,emphasizingthefunctionsandap-
tualobjectsintoreal-worldimages.Weadaptandre-purpose plicationsofitscomponents.Section5offersanexamination
thisimageeditingtechniquetogeneratesimulatedobjectsof ofARSimdatagenerationforthreedistinctautonomousve-
interest. Thismethodaimstoaddressthechallengesposed hicletasks,alongwithathoroughanalysisofthequantitative
by scarce real-world data, enhancing the training process enhancementsresultingfromtheincorporationofARSim
and contributing to the accuracy and reliability of percep- data. In section 6 we carry out a comparative analysis of
tionmodelsinautonomousvehicles. Ourcontributionisas quantitativeimprovementforthetaskof3Dobstacledetec-
follows tionusingpurelysyntheticdataandthe proposedARSim
data.
• We introduce ARSim, a fully automated, end-to-end
frameworkdesignedtogeneratehigh-qualitysynthetic
2.RelatedWork
databyseamlesslyenhancingrealdrivedatasetswith
syntheticobjects,therebyreducingthedomaingap. SyntheticDataGeneration
• We demonstrate the framework’s capability to main- The domain of synthetic data generation for autonomous
tain high-quality realism and multi-view consistency vehicles has experienced considerable growth, marked by
throughaccurateinferenceoflightdistribution,proper thedevelopmentofvarioussimulators,includingDriveSim
shadowcasting,andconsistentrenderingusingcamera [24],CARLA[4],LGSVL[30],rFPro,TORCS[42],We-
parametersacrossallframes. bots,andCoppeliaSim. Thesesimulatorsplayacrucialrole
insupportingthedevelopment,training,andvalidationof
• WeutilizeARSimtoenrichrealdrivedatasetswitha
autonomousdrivingsystems. Theyexhibitdiversityintheir
focusonlong-taildistributionobjectsandshowcasethe
focus,features,andapplications,offeringvaryingdegreesof
augmenteddataset’sefficacyinimprovingperformance
realism,customization,andsensorsupport. Despitetheso-
acrossthreedistinctAVtasks—obstacleperception,
phisticationoftheseplatforms,whichprovidesanenhanced
freespaceperception,andparkingperception—using
degreeofrealism,thereremainsanoticeabledistributiongap
onlyafractionoftheARSim-generateddata.
when compared to real-world datasets. In complement to
The paper is structured as follows. We begin by dis- thesesimulators,machinelearning-basedapproacheshave
cussingrelatedworkinsection2anddrawcomparisonswith emergedasameansofgeneratingsyntheticdata. Generative
2AdversarialNetworks(GANs)[27,1,18,32]havegained renderonlythesyntheticassetineachofthecameraframes
widespreadadoptionforthispurpose,involvingthetraining ontopofrealdrivedata.
ofageneratortoproducerealisticsamplesandadiscrimina-
tortodifferentiatebetweenrealandsyntheticdata. While
ProblemSetup
GANscontributetoimprovingthediversityandqualityof
generated data, challenges persist in ensuring multi-view During the training of an AV perception task, we are
consistencyandgeneratingnecessarygroundtruth. provided with n number of samples, denoted as Si =
i
(x ,y ) ni ( )ni, drawn from the distribution .
{ k k }k=1 ∼ Di Di
DomainAdaptation
Subsequently,aclassifiertrainedonSiundergoestestingon
n samples,representedbySj = (x ,y ) nj ( )nj,
Effortstomitigatethedrawbacksofpurelysyntheticdata saj mpledfrom j. Boththetrainin{ gank dtek st} ink g=1 da∼ taseD tsj can
havebroughtabouttheprominenceofsynthetic-to-realdo- be viewed asD subsets of the larger dataset sampled from
main adaptation techniques. Techniques such as image- the unbalanced distribution . Given that this distribu-
imagetranslationmethods[45,17]andDomainAdversarial tion mirrors real-world condD itions, it exhibits a long-tail
Neural Network (DANN) [8] aim to transfer knowledge distribution for less frequent occurrences. Consequently,
from synthetic to real data domains, thereby reducing the duetothisinherentimbalance,somesamplesorvariations
domaingapandimprovingmodelgeneralization. Additional thereofmaynotexistinthetrainingdata,potentiallylead-
approaches include metric learning [19, 11], deep feature ing to a covariate shift between the two subdomains, i.e.,
alignment[36],anddomainstylization[5]. P (x)=P (x).
Di
̸
Dj
ImageManipulation
DomainRandomization
Thesetechniquesaretypicallydesignedforpurposessuch
Onewaytomitigatethelongtaildistributionistosample
asvisualeffects,artcreation,andaugmentedreality(AR).
from insuchawaythatweoversamplethelessfrequent
An approach closely aligned with our use case involves Di
cases. But this won’t make the distribution any more bal-
lightingestimationandvirtualobjectinsertion[15,14,43,
ancedthanitalreadyis. Theotherapproachistomakeuse
21, 39, 38]. Models in this category estimate scene light-
ofsamplesfromasimulateddatadistribution whichis
ing conditions and produce lighting representations, such Dsim
intentionallygeneratedinsuchawaythatwhenmergedwith
as spherical functions [21], sky models [14, 15, 43], en-
the samples from real data sampled from , will yield a
vironment maps [9, 20], and volumetric representations Di
morebalanceddistribution.Thisdistributionhoweverwillin-
[33,39,38]. Withtheestimatedlightinginformation,virtual
troduceanothercovariateshiftwith ,duetoreducedphoto
objects created by CG artists can be realistically inserted Dj
realismin . Tomitigatethecovariateshiftbetween
intoimages[21],andlightingeffects,suchascastshadows, Dsim Di
and , we propose a hybrid solution combining domain
can be properly handled [38]. Our work distinguishes it- Dj
adaptation and domain randomization. Drawing from the
selfbyaddressinglightingestimationforreal-worlddriving
StructuralCausalModel(SCM)framework[16], thedata
scenarios in an end-to-end manner and facilitating virtual
generation process for simulating images x with ground-
objectinsertionacrossmultiplecameraswithrealisticobject
truth labels y can be expressed as x := f (h ,h ),
placement,andgroundtruthgeneration. X Dsim y
where the generated images are a function of domain at-
tributeh andlabelattributeh . Domainattributesen-
3.ProblemFormulation Dsim y
compassdatasetcharacteristicsdevoidofdirectcausalrela-
Inthispaper,wetargetsimulation-basedsyntheticdata tionshipswiththepredictedoutcome,includingbackground
generationforthetargetapplicationofautonomousvehicles. landscape,brightness/contrast,sharpness,andperspective.
Intheconventionalsimulation-basedsyntheticdatagenera- Conversely,labelattributesdirectlyinfluencethepredicted
tion[29,31,12],agamingengineisusedtocreatecontent outcomeandencompassfeaturessuchasobjectshapeand
andscenariosmimickingtheusecaseunderconsideration. texturewithinanimage.
Suchcompletelysimulatedscenarios,althoughoffermore
control in terms of variations, which is essential in gener-
ProposedApproach
ating a larger distribution of the data, come with a larger
distribution gap due to a limited quantity of 3D environ- For many machine learning models, domain invariance is
mentsandlackingphoto-realisticquality. Weaimtoreduce often overlooked, resulting in a potential spurious corre-
thesecontentandsensordomaingapsbyusingrealdataas lation between the domain attributes h and the label y.
D
backgroundandonlysimulatingwhatisneeded. Insteadof Domain randomization serves to mitigate this correlation
generating the entire scene (foreground, background), we bygeneratingawidevariationofdomainattributesinthe
3Figure2: (Left)AnoverviewofARSim’shigh-levelblockdiagram. (Right)Examplemulti-viewconsistentdatageneratedby
ARSimwithgroundtruthgenerationandmodification.
simulateddata. Inourproposedapproach,wecombinedo- withsignificantcoveragearoundtheegocar(essentialfor
mainadaptationandrandomization,whereaportionofthe accurate light estimation), and the availability of intrinsic
domain attributes (non-object attributes, e.g., background and extrinsic parameters of the camera used to generate
landscapes,lightingconditions,objectshadow)isinferred frames. Thesetwoannotationpiecesareessentialforphysi-
from the real distribution , while simulation-based do- callyviableandphoto-realisticaugmentationtowork. Other
i
D
main randomization is applied to other domain attributes annotationinformationcanbeusefulandincreasethequality
(objectattributes,e.g.,shape,texture,colorvariationsofthe oftheaugmentation.
object). In essence, instead of simulating the entire spec-
trum,wefocusonsimulatingwhatisnecessaryfordomain
SceneRecreation
randomization,therebyimpactingonlyasmallsegmentof
thedistribution. Fortherestofthepaper,wedenotethedata Forproperscene-awareplacementofthe3Dsyntheticassets,
generatedthroughsimulation-baseddomainrandomization additionalannotationsfromtheinputdatacanbeused. The
as VRSim (Virtual Reality-based Simulated data) and the moreinformationwehavefromtheinputscenethericher
data generated through the proposed approach as ARSim this virtual 3D environment. The virtual scene can be as
(AugmentedReality-basedSimulateddata). simple as 3D cuboids around the ego car and as complex
asaNeRFscene(whichrequiressignificantlymoreamount
4.ARSimDataAugmentationPipeline
ofdatacomparedtotheformer). Toshowtheeffectiveness
oftherealisticaugmentationpipeline,weconsidersimple
Inthissection,wepresentthemodularARSimpipeline
annotateddatasetswiththe3Dcuboidanddrivablefreespace
highlightingitskeycomponents. Theinputtothepipelineis
groundtruth. Theextrinsicandintrinsicparametersofthe
arealdrivedataset,whichisthenprocessedforrecreatinga
camerasensorsmountedontheegocarduringdatacollection
minimilast3Dvirtualsceneandinferringlightdistribution.
areusedtocreatecameraobjectswithinthisvirtualscene,
Basedonthelong-taildistributiontobeaddressed,3Dassets
whichwillbeusedtorendertheframefromeachcamera.
are placed in the scene following the real-life constraints
Wesupportbothfisheyeandnon-fisheyecameramodelsfor
(collision, occlusion, ground placement etc). The objects
broadercoverage. Oncethis3Dvirtualsceneisset,thenext
arerenderedacrossthedifferentcamerasandcomposited
importantthingistoassignitalightdistribution.
withtherealimages. Finallythepipelineiscompletedwith
generatingtheassociatinggroundtruthlabels. Thecomplete
pipelinecanbeseeninFig. 2. LightingEstimation
Aspartofthere-created3Dvirtualscene,ourmethodesti-
InputDataSelection
matesanenvironmentmapfromasetofimagescaptured
Thefirststepforcreatingrealisticaugmentationdataistose- bymultiplecameraspositionedaroundtheegovehicle. The
lecttherightinputdata. Therealisticaugmentationpipeline inputimagesoftenprovidesignificantcoverageofthesur-
doesnotchangethedataformat,outputtingthedatainthe roundingviews,butoftenhavealimiteddynamicrangeand
sameformatasthatoftheinputdata. Adatasetcanbeused areunabletocovertheHighDynamicRange(HDR)inten-
asinputfortherealisticaugmentationifithasinputframes sityprofileofoutdoorscenesadequately,whichareessential
4forgeneratinglightingeffectssuchasshadows. Toaddress Dataset Scenes Person Biker
this, we make use of an encoder-decoder neural network
Real(train) 1M 2.2M 290K
trainedwithadatasetofHDRpanoramas. Thenetworkis
Real(test) 306K 266K 29K
designedtotransformaninputLDRpanoramatoitscorre-
ARSim 140K 186K 370K
spondingHDRimage. Specifically, theencodermapsthe
inputimageintothreefeaturevectorsfollowing[38],includ-
ingthepeakdirectionf R3,peakintensityf R3,and Table1: Statisticsofthedatasetusedforobstacledetection
d i
alatentvectorf
R6∈
4.
Thesefeaturevector∈
sarethen
improvement
latent
∈
processedbythedecodertooutputtheHDRpanorama. Dur-
ing inference time, the input images are unprojected onto
objects. This task entails the generation of both 3D and
anequi-rectangularpanoramabasedoncalibratedcamera
2Dboundingboxesalongwiththecalculationofpertinent
intrinsicandextrinsicparametersandthenfusedandfedinto
attributesassociatedwiththeseboundingboxes. Addition-
theencodertopredicttheHDRenvironmentmap. Werefer
ally,itisimperativetoupdatetheexistinginputgroundtruth
totheSupplementformoredetails.
labels in most scenarios due to the augmentation of these
assets. Forinstance,theadditionofasyntheticobjectmay
AssetPlacement
affect the visibility of a real object cuboid. Furthermore,
adjustments may be necessary at the scene level, such as
Theplacementofthesyntheticassetsshouldbeviableand
modifyingfreespaceincertaincases. Uponcompletionof
realistic, asseenbythecameraobjects. Henceconsidera-
thegroundtruthgenerationprocessandtherequisitelabel
tionssuchasallowableregion,objectcollision,andobject
modifications, the dataset becomes available for training
occlusionmustbemade. Basedonthespecificusecase,we
perceptionnetworks.
canaugmenttheassetsbasedontheregionofinterestdefined
bytheuserifthescenestructureallowsit(suchasobjects
alongtheegolanes,far-awayobjects,andmore). Basedon 5.Experiments
theregionofinterest,a3Dlocationisrandomlyselectedfor
It’scrucialtoemphasizethatthegenerationoftheARSim
eachinstance,andanattemptismadetoplacethesynthetic
datasetisindependentofthespecificdownstreamtraining
assetsatthatpositioninthe3Dvirtualscene. Thesynthetic
modelemployed. Thechoiceofthetrainingmodelisincon-
assetplacedischeckedforcollisionwithexistingobjectsin
sequential and can be considered a black box. Instead of
thescene. Theassetisalsocheckedforocclusionasviewed
presentingabsoluteperformancemetrics,weopttodiscuss
from each camera object since it depends not only on the
relativeimprovementasameanstoquantifytheeffective-
asset’s 3D location but also on the individual viewpoints.
nessofincorporatingtheARSimdatasetalongsiderealdata.
Based on the scene information available from the input
Fortheexperimentsoutlinedinthiscontext,weutilizethe
data,werenderthe3Dsyntheticassetforpartialocclusion
NVAutonetmodelandtrainingarchitecturedescribedin[25].
(sayifthedepthmaporsegmentationmapisavailable).
IntheNVAutoNetframework,synchronizedcameraimages
are employed as input for predicting 3D signals, encom-
SceneRendering
passingobstacles,freespaces,andparking. Theinitialstep
Nowthatthevirtual3Denvironmentisset,werenderthe involves utilizing surround view images as input to CNN
framesfromcameraobjectonlyrenderingthesynthetically backbones,extractingessential2Dfeatures. Thesefeatures
placedassets. Avirtualplaneisusedasashadowcatcherfor arethenelevatedandfusedintoacohesiveBird’sEyeView
theseassets,andforeachcameraframe,theobjectandthe (BEV) feature map. The ensuing BEV features undergo
shadowarerenderedseparately. Thisseparationbetweenthe further encoding through a dedicated BEV backbone. Fi-
objectandtheshadowmapoffersmorecontrolforrealism. nally,thesystememploysvarious3Dperceptionheadsto
Post-processingisappliedtothesemapstoadjusttheshadow make predictions for the 3D signals, completing the com-
strength(shadowmap),simulatebasicsensormodels(object prehensiveprocessof3Dsignalforecastingforobjectslike
map),andcreateawidevariationinthesaturationofthese obstacles,freespaces,andparking.
renderedobjects(objectmap). Thepost-processedobject For each of the target applications outlined below, we
and shadow map are then alpha-composited with the real trainNVAutoNetasasingle-taskproblem. Additionally,our
frametogeneratethefinalaugmentedoutput. base dataset consists of an in-house multi-camera dataset
thatincludesessentialskycoveragethroughfisheyecameras.
Notably,otheropen-sourcedatasetslikenuScenes[2]orthe
GroundTruthGenerationandModification
Waymoopendataset[34]donotincorporatefisheyecameras
Theconcludingphaseofthepipelineencompassesthecre- and/orlacka360-degreefieldofview(FOV)coverage. Con-
ationofgroundtruthdatacorrespondingtotheaugmented sequently, theylackthecrucialskycoverageessentialfor
5(b) Spatial distribution of the assets aug-
(a)ExampleobstacleARSimdatawithmulti-viewconsistency. mented
Figure3: ARSimVRUdataforimprovingobstacledetection
Class AveragePrecision Fscore PositionErr(m) YawErr(◦)
↑ ↑ ↓ ↓
Real Real+ARSim Real Real+ARSim Real Real+ARSim Real Real+ARSim
Biker 0.828 0.842 0.82 0.822 0.769 0.739 9.0 7.4
Person 0.807 0.818 0.794 0.797 0.755 0.701 26.9 25.9
Table 2: Improvement in obstacle detection metrics for VRU classes. The arrows ( , ) indicate the desired direction of
↑ ↓
improvementforthemetric
accuratelightestimation. Weaimtoenhancetheperception 306Kscenes. Table2reportstheaverageprecision,F-score,
networksofAVsbyidentifyingspecificareasthatcaterto positionerror,andyawangleerrorforthetargetedclassof
adiverserangeofapplicationsandproblemscenarios. To personandbiker. PositionerroristheEuclideandistance
achievethis,weemploythestrategicimplementationofdata betweenthegroundtruthandthepredicted3Dcuboidcenter
augmentationusingARSimdiscussedbelow. inmeters,andyawerroristheabsolutedifferenceintheyaw
angleofthegroundtruthandthepredicted3Dcuboid. A
5.1.ImprovingObstacleDetection: predictedcuboid3Dissaidtobeatruepositiveiftherelative
radialdistancebetweenitscenterandgroundtruthisless
Detecting3DobjectsiscrucialforAVs,encompassing
than 10% and the cuboid orientation Angle difference is
vehiclesandVulnerableRoadUsers(VRUs)likepedestri-
within2degrees. Table2reportsthenecessaryquantitative
ans,cyclists,andmotorcyclists. Whilethere’sanabundance
metrics. ItcanbeseenthatmakinguseoftheARSimdata
of vehicle data from real drives, VRU data is scarce due
notonlyimprovedthedetectionmetrics(averageprecision,
tosafetyandcostchallenges. ToenhanceVRUdetection,
F-score)butalsoreducedtheposition/orientationerrorsfor
we utilize the ARSim dataset, derived from an in-house
thecuboids. Itisimportanttonotethatthesmallimprove-
realdrivedatasetfeaturing1millionscenes. Werandomly
mentsinclassificationmetricsobservedwhenutilizingthe
sample 140k scenes to generate ARSim data, incorporat-
ARSim data can be attributed to the intricate and diverse
ingover40synthetic3Dobjectsforbikersandpedestrians.
natureofreal-worldhumandata,alongsidethealreadyhigh
These objects are strategically placed around the ego car,
performanceofthespecificquantitativemetricsbeingtar-
withvariationsincolorandpositiontosimulatereal-world
geted.
scenarios. Figure3ashowcasesanexampleARSimscene
withsyntheticVRUs,whileFigure3bdepictsassetspatial
5.2.ImprovingFreespaceDetection
distribution. RelevantstatisticsareoutlinedinTable1.
TwoNVAutonetmodelsweretrainedfortheobstacle3d Freespacereferstotheareawithinroadboundariesde-
task using Real and Real+ARSim data. The performance voidofobstacles,playingapivotalroleinensuringthesafe
of the model was evaluated on real unseen test data with navigationofautonomousvehicles. Muchlike3Dobstacle
6HHaazzaarrdd((RRooII)) HHaazzaarrdd((AAllll)) AAllllCCllaasssseess
5.0%
0.0%
5.0%
−
10.0%
−
15.0%
−
20.0%
−
25.0%
−
30.0%
−
AbsoluteGap( ) RelativeGap( ) SuccessRate( )
↓ ↓ ↑
(b) Improvement in freespace performance metrics, model
(a)ExamplefreespaceARSimdatawithmulti-viewconsistency. trainedonReal+ARSimvsReal
Figure4: ImpactofARSimdataonfreespacedetection
Dataset Abs. Gap(m) Rel. Gap(%) SuccessRate(%) Precision Recall
↓ ↓ ↑ ↑ ↑
Real Only 1.280 29.62 69.49 0.745 0.418
Real+ARSim 1.009 20.68 73.73 0.727 0.471
Table3: ImprovementinfreespaceperformancemetricswithARSimdatasetonrealunseentestdataforhazardclassina
circularregionofradius10maroundegocar.
data,freespacedataalsoexhibitsalong-taildistribution,with realtestdatasetof100kscenes. Theabsolutegap(Abs. gap)
minimalrepresentationfromhazardobjects. Hazardobjects andrelativegap(Rel. Gap)denotesaverageradialdistance
encompassbothroaddebris(strayobjectsfrequentlyfound errors,whilethesuccessrateindicatesthepercentageofsuc-
on the road) and intentionally placed traffic objects. Our cessfullyestimatedangularbins,wheresuccessisdefined
focusisonaddressingthechallengeofclose-rangefreespace byarelativegapoflessthan10%. Thearrowsbesidethe
detectionforhazardobjects. metricindicatethedesireddirectionofimprovement. The
modelsweretargetedforhazarddetectionwithinacircular
Weutilizedanin-houserealdatasetcomprising1.35mil-
regionwitharadiusof10maroundtheegocar.Resultsshow
lionsceneswithassociatedfreespacelabels. Scenesfrom
areductionofover20%and30%inabsoluteandrelative
thisreal-drivedatasetwererandomlysampledtogeneratethe
gaprespectively,withimprovedrecallforfreespacebinclas-
ARSimdataset. Inthisprocess,approximately75instances
sificationtohazardclass,albeitaslightimpactonprecision.
ofsyntheticobjectswereemployed,encompassingadiverse
Fig4billustratestheimprovementachievedbyincorporating
arrayofhazardssuchasboxes,tires,chairs,trashcans,de-
theARSimdatasetacrossdifferentevaluationscenarios.
bris, shoppingcarts, scooters, trafficpoles, cones, barrels,
barriers, triangles, and more. Multiple Synthetic objects
5.3.ImprovingParkingDetectionforgroundlocks
wereplacedwithinarectangularregioncenteredaroundthe
egocar, featuringalongitudinalvariationof 12manda Automaticparkinginvolvesperceptiontaskssuchasde-
±
lateralvariationof 6m. TheARSimdatasetwasgenerated, tectingtheparkingspots,theiravailability,andthenfinally
±
producingatotalof100Kscenesandincorporating226K maneuveringsafelytothespotandcompletingtheparking.
synthetichazardobjects. Fig4ashowsasamplefreespace Insomecountries, theavailabilityofaparkingspotisde-
ARSim data with 4 synthetic objects (barrel, barrier, box, terminedbyparkinggroundlocks(GL),whicharephysical
tirerim)infrontandrightcameras. devicesinstalledintheparkingspots. Whenengaged,these
TwomodelsweretrainedforthefreespaceheadinNVAu- lockscreateaphysicalbarrier,therebyrenderingthepark-
toNet: oneusingonlyrealdataandtheotherusingbothreal ingspotunusableforvehicles. Realdatasetsdonotcontain
andARSimdata. Thefreespaceregionisrepresentedasa enoughparkinggroundlocksfortraining,henceourobjec-
RadialDistanceMap(RDM),composedofequiangularbins tiveistouseARSim-generateddatatoimprovethedetection
aroundthecar,eachcontainingaradialdistancevaluetothe ofvariousgroundlocks.
closestobstacleandasemanticlabel. Table3presentsper- Utilizinganin-housereal-driveparkingdataset,weran-
formancemetricsforthetwomodels,evaluatedonaseparate domlysample 50K scenestoaugmentgroundlocksin
∼
7RReeaall RReeaall++AARRSSiimm
0.8
0.6
0.4
0.2
0.0
PrkmAP PrkF1 PrkPrec. PrkRec. LockF1
(a)Examplesyntheticgroundlockaugmentedonrealsceneswith
multi-viewconsistency. (b)ImprovementinparkingperformancemetricsusingARSim
Figure5: Improvementinperson/bikerclassperformancemetricsusingARSim
Dataset Scenes TotalObjects LockedGLs UnlockedGLs GLs/scene
Real (train) 967K 24.8K 18.5K 6.3K 0.03
ARSim 52K 131K 60.1K 70.8K 2.51
Real+ARSim 1.01M 155.8K 78.7K 77K 0.15
Real (test) 161.5K 2K 1.2K 0.8K 0.01
Table4: Statisticsofthedatasetusedforgroundlockdetectioninparkingmodel
bothlockedandunlockedstatesusing6differentinstancesof Dataset Scenes Person Biker
3Dsyntheticgroundlocks. Thegroundlockswereplacedin
Real (train:all) 2.2M 6.3M 929K
themiddleoftheparkingspot(withaGaussiandistribution
Real (target:NCAP) 48K 15K 3K
forplacementnoise)byestimatingagroundplanethrough
ARSim50k 51K 31K 17K
theseparkingspots. AsamplefromtheparknetARSimdata
VRSim50k 51K 31K 17K
withgroundlockscanbeseeninFig5a.
We trained NVAutoNet’s parking head with Real and
Table 5: Statistics of the dataset used for ARSim/VRSim
Real+ARSimdatasetsandtestedonreal-drivetestdata. The
ablationtest
detailsofthesedatasetscanbeseeninTable4. Fig. 5bplots
theperformancemetricswithandwithoutusingtheARSim
datasetwhenevaluatedontestdata. PrkmAP,PrkF1Prk
Prec. andPrkRec,arethemeanaverageprecision,F1score,
precision, and recall for the parking spot detection. Lock
F1istheF-scoreofabinaryvariablethatsignifieswhether
ceptiontaskforthiscertificationis3Dobstacledetectionfor
thereisalockwithintheparkingspotornot. Itcanbeseen
pre-definedNCAPdummiesasshowninFig6a.
thatthedetectionofgroundlock(LockF1)wasincreased
by96%. Moreover,addinggroundlocksalsoimprovedthe
detectionoftheparkingspotsasitprovidedanadditional Anin-houserealdatasetwith2.2millionscenesisused
signalforparkingspotdetection. as the base dataset. We generate 50K scenes of ARSim
and VRSim datasets using four synthetic assets including
6.AblationTest child,adult,bicycle,andmotorbikeNCAPdummiesshown
inFig6acovering21scenariosdefinedintheEuroNCAP
Inthissectionwecomparetheperformanceofmodels, guideline. ThecameraframefromthegeneratedARSimand
onecompensatedwiththeVRSimdatasetandtheotherwith VRSimdatasetscanbeseeninFig. 6b. VariousNVAutoNet
the proposed ARSim dataset. We consider the target ap- modelsforobstacleheadsweretrained,withdifferentcom-
plicationofEuroNewCarAssessmentPrograms(NCAP) binations of Real, VRSim, and ARSim datasets and were
dummydetection. EuroNCAPisasetofsafetyprotocols evaluated on a target dataset. The details of training and
thatdeterminesthesafetylevelofanewcar. Thekeyper- evaluationdatasetscanbeseeninTab. 5
8
85.0
96.0 76.0 47.0 27.0
77.0
26.0
17.0
52.0
94.0(a)EuroNCAPdummiesusedforaugmen-
tation (b)ComparisonsoftheframesgeneratedfromVRSim(left)andARSim(right)
Figure6: SimdataforNCAPablationstudy
Class AP Fscore improvedtheAPandF-scoreforbothclasses. ARSimtends
toperformslightlybetterinAP,F-score,andrecall,while
VRSim ARSim VRSim ARSim
VRSimisbetterintheprecisionmetric.Usingacombination
Biker 0.02 0.322 0.146 0.441 ofboththeARSimandVRSimdataseemstobethebestfit
person 0.102 0.52 0.167 0.552 toimprovetheperformancemetrics. Moreover,introducing
moresimdata(Real+AR50k+VR50k)improvesthemetrics
Table 6: Metrics evaluation of models on target dataset evenfurther,suggestingroomforimprovement.
trainedwithsim-onlydatasets
7.Conclusion
TrainwithSimonly:
In this paper, we presented a novel augmented reality-
Tab.6showstheaverageprecisionandFscorefortheclasses based method designed to mitigate the covariate shift be-
Biker and Person for two models trained on VRSim and tweenrealandsimulateddatawithinautonomousvehicle
ARSim data only and evaluated on the real NCAP target (AV)perceptionnetworks. Thisapproachintegratesdomain
dataset. It can be seen that the model trained on ARSim adaptation and randomization strategies to effectively ad-
significantly outperforms the model trained with VRSim dressthedisparitiesbetweenthetwodatasets. Specifically,
data. One of the reasons for this significant difference in weinfercrucialdomainattributesfromrealdata,whilelever-
performance is the reduced domain gap between ARSim agingsimulation-basedrandomizationtechniquesforother
and thereal targetdata ascompared toVRSim. Itcan be attributes. Theproposedmethodestimatesthelightingdistri-
seenthateventhoughtheperformanceoftheARSim-trained butionfromtheinputimagesandaugmentsthe3Dsynthetic
modelisrelativelyhigher,itstillisnotgoodenoughtobe objectsofinterestwithmulti-viewconsistencytoaddressthe
usedindependently. Henceacombinationofrealandsim longtaildistributionoftherealdataset. Throughourexper-
datasetsisalwayshelpfulinbridgingthisgap. iments,spanningobstacle,freespace,andparkingtraining
models,wehavedemonstratedsignificantperformanceim-
provementsbyincorporatingbothrealandARSimdatasets.
TrainwithReal+Sim:
Notably, our method outperforms its VRSim counterpart,
Wetrainvariousmodelswithmultiplecombinationsofreal withevenmoreremarkableenhancementsobservedwhen
and sim data and evaluate the performance on the target combiningbothdatasetsfortraining. Thisunderscoresthe
dataset. Fig. 7plotstheAP,Fscore,Precision,andRecallfor synergy between the two approaches and emphasizes the
thetargetedclassesofBikerandPerson. Real+ARxkVRyk effectivenessofouraugmentedreality-basedmethodology.
is a model trained with a combination of real, x thousand Importantly,ourmethodisnotconfinedtoAVapplications
scenesofARSimandythousandscenesofVRSimdata. It butcanbeadaptedtovariousscenariosrequiringdataaug-
can be seen that introducing a small fraction of sim data mentationacrossmultiplecamera
9Real Real+VR50k Real+AR50k Real Real+VR50k Real+AR50k
Real+AR25kVR25k Real+AR50kVR50k Real+AR25kVR25k Real+AR50kVR50k
0.8 0.8
0.7 0.7
0.6 0.6
0.5 0.5
AP Fscore Precision Recall AP Fscore Precision Recall
Person Biker
Figure7: Metricevaluationontargetdatasetwithmodelstrainedonvariouscombinationsofrealandsimdata
References [7] DebidattaDwibedi,IshanMisra,andMartialHebert.
Cut, paste and learn: Surprisingly easy synthesis
[1] Andrew Brock, Jeff Donahue, and Karen Simonyan.
for instance detection. In The IEEE International
Largescalegantrainingforhighfidelitynaturalimage
ConferenceonComputerVision(ICCV),Oct2017. 1
synthesis. arXivpreprintarXiv:1809.11096,2018. 3
[8] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
[2] HolgerCaesar,VarunBankiti,AlexHLang,Sourabh Pascal Germain, Hugo Larochelle, François Lavio-
Vora, Venice Erin Liong, Qiang Xu, Anush Krish- lette, Mario March, and Victor Lempitsky. Domain-
nan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. adversarial training of neural networks. Journal of
nuscenes: Amultimodaldatasetforautonomousdriv- MachineLearningResearch,17(59):1–35,2016. 3
ing. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages11621– [9] Marc-AndréGardner,KalyanSunkavalli,ErsinYumer,
11631,2020. 5 XiaohuiShen,EmilianoGambaretto,ChristianGagné,
and Jean-François Lalonde. Learning to predict in-
[3] TaeEunChoe,JaneWu,XiaolinLin,KarenKwon,and doorilluminationfromasingleimage. arXivpreprint
Minwoo Park. Hazardnet: Road debris detectionby arXiv:1704.00090,2017. 3
augmentationofsyntheticmodels. InProceedingsof
[10] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,
the IEEE/CVF Conference on Computer Vision and
Tsung-YiLin,EkinD.Cubuk,QuocV.Le,andBarret
Pattern Recognition (CVPR) Workshops, pages 161–
Zoph. Simplecopy-pasteisastrongdataaugmentation
171,June2023. 1
methodforinstancesegmentation. InProceedingsof
the IEEE/CVF Conference on Computer Vision and
[4] Alexey Dosovitskiy, German Ros, Felipe Codevilla,
PatternRecognition(CVPR),pages2918–2928,June
AntonioLopez,andVladlenKoltun.Carla:Anopenur-
2021. 1
bandrivingsimulator.InConferenceonrobotlearning,
pages1–16.PMLR,2017. 2
[11] BoqingGong,YuanShi,FeiSha,andKristenGrauman.
Geodesicflowkernelforunsuperviseddomainadapta-
[5] AysegulDundar,Ming-YuLiu,Ting-ChunWang,John
tion.In2012IEEEconferenceoncomputervisionand
Zedlewski, and Jan Kautz. Domain stylization: A
patternrecognition,pages2066–2073.IEEE,2012. 3
strong, simple baseline for synthetic to real image
domainadaptation. arXivpreprintarXiv:1807.09384, [12] YuliangGuo,GuangChen,PeitaoZhao,WeideZhang,
2018. 3 JinghaoMiao,JingaoWang,andTaeEunChoe. Gen-
lanenet: Ageneralizedandscalableapproachfor3d
[6] NikitaDvornik, JulienMairal,andCordeliaSchmid.
lanedetection. InECCV,2020. 3
Modelingvisualcontextiskeytoaugmentingobject
detection datasets. In Proceedings of the European [13] KaimingHe,XiangyuZhang,ShaoqingRen,andJian
ConferenceonComputerVision(ECCV),September Sun. Deep residual learning for image recognition.
2018. 1 CoRR,abs/1512.03385,2015. 13
10
97.0 8.0 8.0
18.0
28.0
87.0 97.0 97.0 97.0
28.0
57.0
28.0
87.0
57.0
38.0
97.0 97.0 97.0
18.0
28.0
56.0
77.0
87.0 97.0
48.0
56.0
57.0
77.0
57.0
8.0
7.0
67.0
47.0
27.0
97.0
16.0
47.0
97.0 87.0
18.0[14] YannickHold-Geoffroy,AkshayaAthawale,andJean- [24] NVIDIA. Drive sim. https://www.
FrançoisLalonde. Deepskymodelingforsingleimage nvidia.com/en-us/self-driving-cars/
outdoor lighting estimation. In CVPR, pages 6927– simulation/. Accessed: 2023-11-08. 2
6935,2019. 3
[25] Trung Pham, Mehran Maghoumi, Wanli Jiang, Bala
[15] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Siva Sashank Jujjavarapu, Mehdi Sajjadi, Xin Liu,
Hadap, Emiliano Gambaretto, and Jean-François Hsuan-ChuLin,Bor-JengChen,GiangTruong,Chao
Lalonde. Deep outdoor illumination estimation. In Fang,etal. Nvautonet: Fastandaccurate360deg3d
CVPR,pages7312–7321,2017. 3 visual perception for self driving. In Proceedings of
theIEEE/CVFWinterConferenceonApplicationsof
[16] Maximilian Ilse, Jakub M Tomczak, and Patrick ComputerVision,pages7376–7385,2024. 5
Forré. Selectingdataaugmentationforsimulatingin-
terventions. InInternationalConferenceonMachine [26] JonahPhilionandSanjaFidler. Lift,splat,shoot: En-
Learning,pages4555–4562.PMLR,2021. 3 codingimagesfromarbitrarycamerarigsbyimplicitly
unprojectingto3d. InECCV,2020. 1
[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and
[27] AlecRadford,LukeMetz,andSoumithChintala. Un-
Alexei A Efros. Image-to-image translation with
supervisedrepresentationlearningwithdeepconvolu-
conditional adversarial networks. In Proceedings of
tionalgenerativeadversarialnetworks. arXivpreprint
the IEEE conference on computer vision and pattern
arXiv:1511.06434,2015. 3
recognition,pages1125–1134,2017. 3
[28] CodyReading,AliHarakeh,JuliaChae,andStevenL.
[18] Tero Karras, Samuli Laine, and Timo Aila. A style-
Waslander. Categorical depth distribution network
basedgeneratorarchitectureforgenerativeadversarial
formonocular3dobjectdetection. InProceedingsof
networks.InProceedingsoftheIEEE/CVFconference
the IEEE/CVF Conference on Computer Vision and
on computer vision and pattern recognition, pages
PatternRecognition(CVPR),pages8555–8564,June
4401–4410,2019. 3
2021. 1
[19] BrianKulis,KateSaenko,andTrevorDarrell. What
[29] Stephan R Richter, Vibhav Vineet, Stefan Roth, and
yousawisnotwhatyouget: Domainadaptationusing
VladlenKoltun. Playingfordata: Groundtruthfrom
asymmetrickerneltransforms. InCVPR2011,pages
computergames. InECCV. 3
1785–1792.IEEE,2011. 3
[30] GuodongRong,ByungHyunShin,HadiTabatabaee,
[20] ChloeLeGendre,Wan-ChunMa,GrahamFyffe,John
QiangLu,SteveLemke,Ma¯rtinšMožeiko,EricBoise,
Flynn,LaurentCharbonnel,JayBusch,andPaulDe- ,
GeehoonUhm,MarkGerow,ShalinMehta,etal.Lgsvl
bevec. Deeplight: Learning illumination for uncon-
simulator: A high fidelity simulator for autonomous
strainedmobilemixedreality. InCVPR,pages5918–
driving. In2020IEEE23rdInternationalconference
5928,2019. 3
onintelligenttransportationsystems(ITSC),pages1–
6.IEEE,2020. 2
[21] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoor-
thi, Kalyan Sunkavalli, and Manmohan Chandraker. [31] German Ros, Laura Sellart, Joanna Materzynska,
Inverserenderingforcomplexindoorscenes: Shape, DavidVazquez, andAntonioMLopez. Thesynthia
spatially-varyinglightingandsvbrdffromasingleim- dataset: Alargecollectionofsyntheticimagesforse-
age. InCVPR,pages2475–2484,2020. 3 manticsegmentationofurbanscenes. InCVPR,2016.
3
[22] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie,
Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. [32] TamarRottShaham,TaliDekel,andTomerMichaeli.
Bevformer: Learning bird’s-eye-view representation Singan: Learning a generative model from a sin-
from multi-camera images via spatiotemporal trans- gle natural image. In Proceedings of the IEEE/CVF
formers. In Computer Vision–ECCV 2022: 17th international conference on computer vision, pages
EuropeanConference,TelAviv,Israel,October23–27, 4570–4580,2019. 3
2022, Proceedings, Part IX, pages 1–18. Springer,
[33] PratulPSrinivasan,BenMildenhall,MatthewTancik,
2022. 1
JonathanTBarron,RichardTucker,andNoahSnavely.
[23] JonathanLong,EvanShelhamer,andTrevorDarrell. Lighthouse: Predictinglightingvolumesforspatially-
Fullyconvolutionalnetworksforsemanticsegmenta- coherent illumination. In CVPR, pages 8080–8089,
tion. InCVPR,pages3431–3440,2015. 13 2020. 3
11[34] PeiSun,HenrikKretzschmar,XerxesDotiwalla,Aure- Wenbo Zhou, Qi Chu, Weiming Zhang, and Neng-
lienChouard,VijaysaiPatnaik,PaulTsui,JamesGuo, hai Yu. X-paste: Revisiting scalable copy-paste for
YinZhou,YuningChai,BenjaminCaine,etal.Scalabil- instance segmentation using CLIP and StableDiffu-
ityinperceptionforautonomousdriving:Waymoopen sion. InAndreasKrause,EmmaBrunskill,Kyunghyun
dataset. InProceedingsoftheIEEE/CVFconference Cho,BarbaraEngelhardt,SivanSabato,andJonathan
on computer vision and pattern recognition, pages Scarlett,editors,Proceedingsofthe40thInternational
2446–2454,2020. 5 Conference on Machine Learning, volume 202 of
Proceedings of Machine Learning Research, pages
[35] AlexandruTelea. Animageinpaintingtechniquebased 42098–42109.PMLR,23–29Jul2023. 1
onthefastmarchingmethod.Journalofgraphicstools,
9(1):23–34,2004. 13 [45] Jun-YanZhu,TaesungPark,PhillipIsola,andAlexeiA
Efros. Unpaired image-to-image translation using
[36] EricTzeng,JudyHoffman,NingZhang,KateSaenko, cycle-consistentadversarialnetworks. InProceedings
and Trevor Darrell. Deep domain confusion: Max- of the IEEE international conference on computer
imizing for domain invariance. arXiv preprint vision,pages2223–2232,2017. 3
arXiv:1412.3474,2014. 3
[37] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua
Lin. Fcos3d: Fullyconvolutionalone-stagemonocular
3dobjectdetection. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV)
Workshops,pages913–922,October2021. 1
[38] ZianWang,WenzhengChen,DavidAcuna,JanKautz,
andSanjaFidler. Neurallightfieldestimationforstreet
sceneswithdifferentiablevirtualobjectinsertion. In
ProceedingsoftheEuropeanConferenceonComputer
Vision(ECCV),2022. 3,5,13
[39] ZianWang,JonahPhilion,SanjaFidler,andJanKautz.
Learning indoor inverse rendering with 3d spatially-
varyinglighting. InICCV,2021. 3
[40] Wikipedia contributors. Fisheye lens —
Wikipedia, the free encyclopedia. https:
//en.wikipedia.org/w/index.php?
title=Fisheye_lens&oldid=1213587243,
2024. [Online;accessed14-March-2024]. 14
[41] Wikipedia contributors. Relative luminance —
Wikipedia,thefreeencyclopedia,2024. [Online;ac-
cessed14-March-2024]. 13
[42] BernhardWymann,EricEspié,ChristopheGuionneau,
Christos Dimitrakakis, Rémi Coulom, and Andrew
Sumner.Torcs,theopenracingcarsimulator.Software
availableathttp://torcs.sourceforge.net,4(6):2,2000.
2
[43] Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-
Geoffroy,SunilHadap,JonathanEisenman,andJean-
FrançoisLalonde. All-weatherdeepoutdoorlighting
estimation. InCVPR,pages10158–10166,2019. 3
[44] Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dong-
dongChen,DongChen,FangWen,LuYuan,CeLiu,
12Supplementary Material
Thisdocumentsupplementsthemainpaperandprovides detailsfromtheLDRpanorama:
additionalimplementationdetails(Sec.A),examplesofgen-
(cid:40)
eratedARSimdata(Sec.B),anddetailsonimprovingdetec- I HDR, ifI LDR 0.9
I = ≥ (2)
tionmetricsusingARSimdataforpre-trainednetworks(No envmap I , Otherwise
LDR
newmodelsweretrained)(Sec.C).
Fig. 8depictsthelightestimationthroughanexample.
A.ImplementationDetails
A.2.Egolightmodeling
A.1.Lightingestimation
Inadditiontothe360-degreeHDRpanoramamentioned
The encoder-decoder architecture of our lighting esti- earlierforilluminatingthescene,weincorporateotherlight
mationmodulefollowstheskymodelingnetworkin[38]. sources specifically for night-time scenes. The ego car’s
The encoder architecture is a ResNet50 [13]. The input headlightsandrearlightsaresimulatedasconicpointlight
totheResNet encoderisa6-channel image, including an sourceswithcorrespondingcolors.Theselightsaretriggered
LDRpanoramaI RH×W×3andapositionalencoding whentheluminanceoftheHDRmap(0.2126 R+0.7152
LDR
I RH×W×3 whe∈ reeachpixelcontainsitsdirectionin G+0.0722 B) [41] falls below a predefi× ned threshol× d
pe
∈ ×
equirectangularprojection. Theencoderoutputsasetofsky of0.5. Figure9displaystheARSimdatacapturedbythe
featurevectorsincludingthepeakdirectionf R3,peak front-facingcamerabothbeforeandaftertheactivationof
d
intensityf R3,andalatentvectorf R64∈ . theegoheadlights. Incorporatingthemodelingoftheego
i latent
∈ ∈
The decoder is a 2D UNet [23] that consumes the sky lightsenhancestherealismofrenderedassetsinnight-time
feature vectors. Before feeding into the decoder, the pre- scenes.
dictedpeakdirectionf isconvertedasa1-channelpanorama
d
RH×W×1encodingthepeakdirectionI
peak
=e100(Ipe·fd−1). B.GeneratedARSimdata
Thepeakintensityf isconvertedasa3-channelpanorama
i B.1.Assetplacement
I RH×W×3:
i
∈
Weemploystratifiedsamplingtoposition3Dsynthetic
(cid:40)
f i, ifI peak 0.98 assetswithinthesceneforrendering. Toensurebalanced
I = ≥ (1)
i 0, Otherwise representationacrosstargetDNNs,thesynthetic3Dassets
areorganizedintocategories. Giventhatsomecategories
Thedecodertakeasinputa7-channelpanoramabyconcate- maycontainmoreinstancesthanothers,uniformsampling
natingI ,I andI andconcatenatelatentvectorf in could result in a skewed distribution. Thus, we establish
peak i pe latent
theUNetlatentspace. TheoutputofthedecoderisanHDR acategoricaldistributionp(n)representingthenumberof
panoramaI RH×W×3. Following[38],wetrainthe syntheticassetsn 1,2,...,N tobeplacedinascene
HDR ∈ ∈ { }
network on the HoliCity dataset and a set of 724 outdoor (e.g.,placing[1,2,3,4]assetswithprobabilities[0.4,0.3,
HDRpanoramas.Thepanoramasarerandomlyrotatedalong 0.2,0.2]),irrespectiveofavailableassetcategories. Subse-
theazimuthasadataaugmentation. quently,wedefineanothercategoricaldistributionforasset
During inference, we unproject the four input fisheye categoriesp(g),wheregdenotestheassetgroup(e.g.,group
imagesontopanoramaswithstandardequi-rectangularpro- of pedestrians), g 1,2,...,G , and G represents the
jectionandmergethemintoonepanoramawithmaxpooling. total number of
ass∈ et{
groups
((cid:80)G}
p(g) = 1). Prior to
g=1
Theinputfisheyeimagestypicallycoverallormostofthe placing an asset, we sample the number of assets n from
panoramas,andweinpaintthepanoramas[35]ifthereare p(n). Thisnisthenallocatedacrossgroupgbasedonp(g)
emptypixels. Theresultingpanoramaisthenfedintothe androundedtothenearestintegertoyieldn (i.e.,thenum-
g
encoder-decoderarchitecturetopredicttheHDRpanorama berofassetsfromassetgroupg tobeplaced). Finally,we
I . ThefinalenvironmentmapI fillssaturatedpix- uniformly sample n instances from group g to position
HDR envmap g
elswithpredictedHDRvaluesandkeepsthehigh-frequency them within the scene. This approach ensures an average
13Figure8: Lightestimation-left,front,rightandrearfisheyecameras(toprow),stitchedpanorama,inpaintedpanorama,HDR
panorama,luminanceofHDRpanorama(bottomrow)
Figure9: Modellingtheegocarheadlights. (left)ARSimgeneratedimagewithoutmodelingtheegocarheadlight. (right)
ARSimgeneratedimageaftermodelingtheegocarheadlights.
ofn
=(cid:80)N
p(n) nassetsperscene,witheachasset otherelementswithinthescene. Theseexamplesalsoillus-
avg n=1 ∗
groupghavingn p(g)occurrencesperscene. tratevariousdaytimeconditions,spanningfromcloudyand
avg
∗
sunnyweathertonighttimeandrainyenvironments,further
B.2.Rangeofscenarios highlightingtheversatilityandapplicabilityofourapproach
acrossdiversescenarios.
ItcanbeseeninFigure10thatourproposedapproach
of ARSim, aimed at enhancing real data by augmenting
C.ImprovingdetectionusingARSimdata
3D synthetic assets with multi-view consistency, encom-
passesabroadspectrumofscenarios. Thiscoverageextends
C.1.3DSyntheticAssets
acrossdifferenttypesofcameras,avastassetpool,accurate
lightingsimulations,anddiversetimesettings. Regarding Figures 11, 12, and 13 display the 3D synthetic assets
camera types, our system supports pinhole, standard, and utilizedingeneratingdataforobstacledetection,freespace
fisheye lens cameras using f-theta lens model [40], with detection,andparkingdetectiontasks. Intheobstacledetec-
thelatteradeptatrendering3Dsyntheticobjectswiththe tiondatagenerationprocess, variousVRUsareemployed
necessarydistortionalongtheouteredges,ensuringrealism (Fig. 11),eachassignedrandomanimationschosenfroma
(as evidenced by the example of the synthetic car in the poolof10differentactionssuchaswalking,jogging,bend-
rearfisheyecamerashowninthelastrowandfirstcolumn). ing, tying shoes, and picking up objects from the ground.
Ourapproachcaterstoacomprehensiverangeof3Dsyn- Thesediverseanimations, combinedwiththedeployment
theticassets,includingVRUs,animals,cars,hazardobjects, of36distinctinstancesof3Dsyntheticpedestrians,result
groundlocks,andNewCarAssessmentPrograms(NCAP) inawidearrayofvariationswithinthedataset. Toenhance
dummies.Importantly,thegeneratedARSimdatashowcases freespacedetection,wecuratedclosed-rangehazarddataby
realisticlightingandshadows,seamlesslyintegratingwith incorporatingadiversearrayof74synthetic3Dassetsdis-
14Figure10: ExamplesofARSimmulti-viewconsistentdata
tributedacross12categories. Thesecategoriesencompassed including trash cans, bags, shopping carts, tree branches,
personalvehicles(suchaselectricscootersandonewheels), tires, boxes, chairs, traffic poles, movable barriers, traffic
aswellasvariousobjectscommonlyencounteredonroads, barriers,andtrafficcones/triangles. Visualrepresentations
15Figure11: Thelistof3DsyntheticassetsusedforimprovingobstacledetectiontargetingVRUs
Figure12: Thelistof3Dsyntheticassetsusedforimprovingfreespacedetectiontargetingclosed-rangehazards
driveinputdataforparknet,parkingspotsaredelineatedby
polygonsintheegocoordinatesystemandassignedvarious
attributessuchasavailabilityandexistinggroundlockpres-
ence. Utilizing these attributes, we deploy a ground lock
withinaparkingspotifitremainsunoccupiedandlacksan
existinggroundlock.
C.2.Visualizingtheimprovement
We visualize the freespace hazard detection results by
inferringonatestscenethemodelstrainedwithandwithout
Figure13:Thelistof3Dsyntheticassetsusedforimproving
theARSimdataasexplainedinthemainpaperbody.Fig.14
parkinglockdetection
showcasesanexampleofcorrectlyclassifiedhazardobjects
fromthetestdata,whichwerepreviouslymisclassifiedas
pedestrianswhentrainedsolelyonrealdata.
of these assets are depicted in Figure 12. Leveraging the
frequencyofoccurrenceofthesecategoriesonroads,weem-
C.3.NCAPAblationStudy: Non-targetedcase
ployedstratifiedsampling,asdescribedearlier,toensurea
controlleddistributionofassetsinthegeneratedARSimdata. In the main body of the paper, we conducted ablation
Forparknetgroundlockdetectionimprovement,weintro- testsforobstacle3ddetectionspecificallyfocusingonNCAP
ducedthreetypesofgroundlocks,eachavailableinlocked dummies. Thesetestsinvolvedcomparingtheperformance
andunlockedstates,asillustratedinFigure13. Unlikethe ofmodelstrainedusingthreedifferentdatasets: Realonly,
placementstrategyforobstacleandfreespacedetectiondata, Real+VRSim,andReal+ARSim. Additionally,thesetrained
the positioning of these ground locks differs. In the real- models underwent evaluation on a separate generic obsta-
16Figure14: Freespacepredictionresultsontherealvalidationdataset. (Left)Inferencewiththemodeltrainedonlyonareal
dataset,thetrafficconeismispredictedasapedestrian. (Right)Freesapacepredictionwithreal+ARSimwiththesametraffic
conecorrectlyclassifiedashazard
RReeaall RReeaall++VVRRSSiimm RReeaall++AARRSSiimm RReeaall RReeaall++VVRRSSiimm RReeaall++AARRSSiimm
1.00 1.00
0.95 0.95
0.90 0.90
0.85 0.85
0.80 0.80
0.75 0.75
Biker Car Person Truck Biker Car Person Truck
AveragePrecision Fscore
Figure15: KPIevaluationonvalidation(Real (val))data
Dataset Scenes Person Biker
Real 2.2M 6.3M 929K
Real (val) 306K 266K 29K
ARSim 51K 31K 17K
VRSim 51K 31K 17K
Table 7: Statistics of the dataset used for ARSim/VRSim
ablationtest
cle3d validation dataset, referred to as Real (target:
NCAP Only). Thisvalidationsetcomprisedclassessuch
asbiker(non-dummyrealbikers),person(non-dummyreal
pedestrians), and truck, as detailed in Table 7. This vali-
dationstepaimedtoensurethattheinclusionofsimulated
datadidnotadverselyaffectperformancemetricsfornon-
targetedcases. Fig. 15presentstheaverageprecisionand
F-scoreresults,revealingthatthedifferencesinperformance
metricswerenegligible.It’scrucialtoemphasizethatFig.15
wasgeneratedusingexistingdata(usedinthemainbodyof
thepaper),andnonewmodelsweretrained.
17
168.0 958.0
178.0
69.0
269.0 569.0
428.0 338.0 628.0
119.0 909.0 819.0
738.0 638.0 448.0
839.0 639.0 49.0
218.0 708.0 608.0
209.0 598.0 209.0