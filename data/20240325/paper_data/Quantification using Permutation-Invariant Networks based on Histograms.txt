QUANTIFICATION USING PERMUTATION-INVARIANT NETWORKS
BASED ON HISTOGRAMS
APREPRINT
OlayaPérez-Mon AlejandroMoreo
ArtificialIntelligenceCenter IstitutodiScienzaeTecnologiedell’Informazione
UniversityofOviedo ConsiglioNazionaledelleRicerche
Gijón,33204,Asturias,Spain Pisa,56124,Italy
UO257319@uniovi.es alejandro.moreo@isti.cnr.it
JuanJosédelCoz PabloGonzález
ArtificialIntelligenceCenter ArtificialIntelligenceCenter
UniversityofOviedo UniversityofOviedo
Gijón,33204,Asturias,Spain Gijón,33204,Asturias,Spain
juanjo@uniovi.es gonzalezgpablo@uniovi.es
March25,2024
ABSTRACT
Quantification,alsoknownasclassprevalenceestimation,isthesupervisedlearningtaskinwhich
amodelistrainedtopredicttheprevalenceofeachclassinagivenbagofexamples. Thispaper
investigatestheapplicationofdeepneuralnetworkstotasksofquantificationinscenarioswhere
itispossibletoapplyasymmetricsupervisedapproachthateliminatestheneedforclassification
asanintermediarystep,directlyaddressingthequantificationproblem. Additionally,itdiscusses
existingpermutation-invariantlayersdesignedforsetprocessingandassessestheirsuitabilityfor
quantification. Inlightofouranalysis,weproposeHistNetQ,anovelneuralarchitecturethatrelieson
apermutation-invariantrepresentationbasedonhistogramsthatisspeciallysuitedforquantification
problems. Our experiments carried outin the only quantification competitionheld to date, show
thatHistNetQoutperformsotherdeepneuralarchitecturesdevisedforsetprocessing,aswellasthe
state-of-the-artquantificationmethods. Furthermore,HistNetQofferstwosignificantadvantagesover
traditionalquantificationmethods: i)itdoesnotrequirethelabelsofthetrainingexamplesbutonly
theprevalencevaluesofacollectionoftrainingbags,makingitapplicabletonewscenarios;andii)it
isabletooptimizeanycustomquantification-orientedlossfunction.
Keywords quantification·prevalenceestimation·deeplearning·deepneuralnetworks
1 Introduction
Inmanyreal-worldapplications[Beijbometal.,2015,Forman,2006,Gonzálezetal.,2019,HopkinsandKing,2010,
MoreoandSebastiani,2022,Diasetal.,2022],predictingtheclassofeachindividualexampleinadatasetisoflittle
concern, since the real interest lies in the aggregate level, i.e., in estimating the prevalence of the classes in a bag
ofexamples. Quantification,alsoknownasclassprevalenceestimation,isthesupervisedlearningtaskthattackles
thisparticularproblem[Gonzálezetal.,2017]. Quantificationhasalreadyprovenusefulinawidevarietyoffields,
providinganswerstoquestionsasforexample: whatisthepercentageofpositive,neutral,andnegativereviewsfora
specificproductofagivencompany? [MoreoandSebastiani,2022]orwhatisthepercentageofplanktonorganisms
belongingtoeachofthephytoplanktonspeciesinthiswatersample? [Gonzálezetal.,2019].
Thislearningproblemcanbeformalizedasfollows. LetY ={c }l betheclassesofinterest,thegoalistolearn
j j=1
a quantifier: q : NX → ∆l−1, i.e., a functional q ∈ Q that, given a test bag B = {x }m in which x ∈ X is a
i i=1 i
4202
raM
22
]GL.sc[
1v32151.3042:viXraQuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
vectoroffeaturesrepresentingadataexample,returnsavectorofclassprevalenceestimationsq(B)∈∆l−1,where
∆l−1 = {(p ,...,p ) | p ∈ [0,1],(cid:80)l p = 1}representstheprobabilitysimplex,i.e.,thedomainofallvectors
1 l j j=1 j
representingprobabilitydistributionsoverY. Wewillusep ∈∆l−1toindicatethetrueprevalencevaluesofabagB,
B
andpˆA ∈∆l−1toindicatetheestimatedprevalencevaluespredictedbythequantificationalgorithmA,sothatp (c )
B B j
andpˆA(c )arethetrueandthepredictedclassprevalence,forclassc ,respectively.
B j j
Atfirstglance, quantificationseemsataskverysimilartoclassificationinspirit. Indeed, themoststraightforward
solutiontothequantificationproblem,calledClassify&Count(CC)intheliterature,comesdowntofirstlearninga
hardclassifierh:X →Y usingatrainingdatasetD ={(x ,y )}n drawnfromX×Y,tothenissuelabelpredictions
i i i=1
forallexamplesinthetestbagB,andfinallycountingthenumberoftimeseachclasshasbeenattributed. However,it
hasbeenobservedthatCCgivesrisetobiasedestimatorsofclassprevalence[Forman,2008]. Thereasonisthathis
biasedtowardsthetrainingprevalenceandthereforetendstounderestimate(resp. overestimate)thetrueprevalenceofa
classwhenthisclassbecomesmoreprevalent(resp. lessprevalent)inthetestbagBthanitwasinthetrainingsetD.
Noticeably,mostquantificationalgorithmsrelyonthepredictionsofaclassifier1whicharesubsequentlypost-processed
usinginformationfromDandB. Thispost-processingisnecessarysince,inquantification,weassumetofaceashiftin
thedatadistribution(i.e.,thattheprevalenceoftheclassesmaydifferbetweenDandB).
Thisparticularshiftisgenerallyknownas“labelshift"or“priorprobabilityshift"[Quionero-Candelaetal.,2009],
accordingtowhichthepriordistributionP(Y)canchangebetweentraininganddeploymentconditions,whilethe
class-conditionaldensitiesP(X|Y)areassumedstationary.ThefactthatCCisnotsuitableforquantificationunderprior
probabilityshiftconditionshasledtothedevelopmentofamyriadofmethodsdesignedspecificallyforquantification,
which is by now recognized as a task on its own right (see, e.g., González et al. [2017], Esuli et al. [2023] for an
overview).
Oneofthemainadvantagesofadoptingdeepneuralnetworkarchitectures(DNNs)forquantificationisthatDNNsallow
thelearningprocesstohandlebagsofexamples(labeledbytheirclassprevalencevalues)insteadofindividualexamples
(labeledbyclass). Followingthisintuition,achangeinthelearningparadigmwithrespecttothetraditionalonewas
firstproposedinQietal.[2021]. Inthispaper,weofferanin-depthexplorationoftheimplicationsofthischangeof
paradigm,byanalyzingthemainadvantagesandlimitationswithrespecttotraditionalapproachestoquantification.
Conversely,traditionalquantificationmethodsadoptanasymmetricapproachinwhichaclassifieristrainedtoinferthe
classoftheindividualexamplesandinwhichthelabelpredictionsareusedtoestimatetheprevalenceoftheclasses
inthebag. Thisway,thetraininglabels(classlabelsattachedtotheexample)andthelabelstobepredicted(class
prevalencevaluesattachedtothebag)arenothomologous. Incontrast,followingtheapproachproposedinQietal.
[2021],wecanreframethequantificationproblemasasymmetricsupervisedlearningtaskinwhichthetrainingset
consistsofacollectionofbagscontainingexampleslabeledattheaggregatelevel(i.e.,withoutindividualclasslabels).
Thisformulationpositsthequantificationproblemasamultivariateregressiontask,inwhichthelabelsprovidedfor
trainingandthelabelsweneedtopredictbecomehomologous. Throughoutthispaper,wewilldemonstratefurther
advantages of this formulation. Among them, and in contrast to traditional quantification methods, the quantifier
becomescapableofoptimizinganyspecificlossfunction.
Withthisaim,ourpaperinvestigatestheapplicationofDNNstothesymmetricquantificationproblem.Thepaperbegins
byaddressingacentralissuethatariseswhenmakingpredictionsforentirebagsratherthanforindividualexamples,
namely,howtorepresentbagsinapermutation-invariantmanner[EdwardsandStorkey,2017,Murphyetal.,2019,
Wagstaffetal.,2019]. TwoinfluentialDNNarchitectureshavebeenproposedforsetprocessing: DeepSets[Zaheer
etal.,2017]andSetTransformers[Leeetal.,2019]. Theformeremploysapoolinglayerlikemax,average,ormedian,
tosummarizeeachbag,whilethelatterusesatransformerarchitecturewithoutpositionalencoding. Theseapproaches
weredesignedasuniversalapproximationfunctionsforset-basedproblems. Here,weproposeanewarchitecture,called
HistNetQ,relyingonhistogram-basedlayers. Therationalewhyhistogramsseempromisingistwo-fold: histograms
arenaturallygearedtowardsrepresentingdensitiesandconveymoreinformationthanplainstatistics(likethemean,or
median). Wewillshowthathistogram-basedlayerscanbeseenasageneralizationofthepoolinglayersproposedin
Zaheeretal.[2017],Qietal.[2021].
The contributions of this paper are three-fold. First, we analyze the symmetric approach of Qi et al. [2021] for
quantification,discussingitsstrengthsandlimitations. Secondly,weempiricallyassessthesuitabilityofpreviously
proposedpermutation-invariantlayerstothequantificationproblem. Finally,weproposeHistNetQ,anewpermutation-
invariantarchitecturebasedondifferentiablehistograms,specificallyusefulforquantificationtasks.
Our experiments show two main results: i) HistNetQ outperforms not only traditional quantification methods and
previousgeneral-purposeDNNarchitecturesforsetprocessingbutalsostate-of-the-artquantification-specificDNN
1Otheralternativesexistwhichinsteadrelydirectlyonthefeaturesoftheexamples(thecovariates)[González-Castroetal.,2013,
Kawakuboetal.,2016];however,theliteraturehasshownthattheseapproachestendtobelesscompetitive.
2QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
methods[Esulietal.,2018,Qietal.,2021]intheLeQua[Esulietal.,2022]competition,theonlycompetitionentirely
devotedtoquantificationheldtodate,ii)HistNetQprovescompetitivealsoundertheasymmetricapproachtoo,thatis,
whenasetoftrainingbagsisnotavailableandmustbegeneratedfromDviasampling.
2 RelatedWork
Thissectionbrieflydescribesthemostimportantquantificationmethodsbasedontheasymmetricapproachaswellas
DNNarchitecturesspecificallydesignedtohandleset-baseddata.
2.1 QuantificationMethods
TheAdjustedClassifyandCount(ACC)method(seeFernandesVazetal.[2019],Forman[2008]), laterrenamed
asBlackBoxShiftEstimation“hard”(BBSE-hard)inLiptonetal.[2018], learnsaclassifierhandthenappliesa
correctionrelyingonthelawoftotalprobability:
(cid:88)
p(h(x)=c )= p(h(x)=c |c )·p(c ), (1)
i i j j
cj∈Y
whichcorrespondstothefollowinglinearsystem:
pˆCC =M ·p, (2)
B h
wherepˆCCaretheprevalenceestimatesreturnedbytheCCmethodforthetestbagBandM isthemisclassification
B h
matrixcharacterizingh, thatis, m istheprobabilitythathpredictsc ifthetrueclassisc . M isunknownbut
ij i j h
canbeestimatedviacross-validation. ACCcomesdowntosolving(2)aspˆACC = Mˆ−1·pˆCC ifM isinvertible;
B h B h
otherwise,thePenrosepseudoinversecanbeused[Bunse,2022].
InBellaetal.[2010],theauthorsproposetwoprobabilisticvariantsofCCandACC,thatconsistofreplacingthehard
classifierhwithasoftclassifiers:X →∆l−1,thusgivingrisetoProbabilisticClassify&Count(PCC):
(cid:80)
s(x)
pˆPCC = x∈B , (3)
B |B|
andProbabilisticAdjustedClassifyandCount(PACC)(alsoknownasBBSE-softinLiptonetal.[2018]):
pˆPACC =Mˆ−1·pˆPCC. (4)
B s B
TheExpectationMaximizationforQuantification(EMQ)[Saerensetal.,2002]methodappliestheEMalgorithmto
adjusttheposteriorprobabilitiesgeneratedbyasoftclassifierstothepotentialshiftinthelabeldistributionbyiterating
overamutuallyrecursivestepofexpectation(inwhichtheposteriorsareupdated)andmaximization(inwhichthe
priorsareupdated)untilconvergence. TheliteraturehasconvincinglyshownthatEMQisa“hardtobeat”quantification
method[Alexandarietal.,2020,Esulietal.,2020]. However,theperformanceofEMQheavilyreliesonthequality
of the posterior probabilities generated by s (i.e., on the fact that these posterior probabilities are well-calibrated).
Forthisreason,differentcalibrationstrategieshavebeenproposedintheliterature;amongthese,theBias-Corrected
TemperatureScaling(BCTS)calibrationprovedthebestofthelot[Alexandarietal.,2020]. Intheexperimentsof
Section5,wewillconsidertwovariantsofEMQ:oneinwhichtheposteriorprobabilitiesarenotrecalibratedand
anotherinwhichweapplyBCTS.
TheHDymethod[González-Castroetal.,2013]usesacombinationofhistogramstorepresentthedistributionsofthe
trainingdataDandthetestbagB,usingtheHellingerDistance(HD)tocomparethem. HDybuildsthehistograms
using the posterior probabilities returned by a soft classifier s. Figure 1 illustrates the inner workings of the HDy
method. Inthetrainingphase,thedistributionsoftheposteriorsreturnedbysforthepositiveandnegativeexamplesin
thetrainingsetDareestimatedusinghistogramsD+andD−respectively. Attesttime,theposteriorsofthetestbag
Barecomputedandrepresentedusingthesameprocedure. HDywillthenreturntheprevalencevaluepˆthatminimizes
theHDbetweenthemixtureandthetestbagdistributions,solvingthefollowingoptimizationproblem:
argminHD(pˆ·D++(1−pˆ)·D−, B). (5)
pˆ∈[0,1]
QuaNetisaDNNarchitectureforbinaryquantification[Esulietal.,2018]. QuaNetsortstheinputsbytheirposterior
probabilitiesandprocessesthesequenceusingabi-directionalLSTMthatlearnsapredictorofclassprevalence. The
3QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
HDy
1.0
D+ Positives distribution
D− Negatives distribution
0.8
B Test bag distribution
Mixture: pD+ + (1-p) D−
0.6
0.4
0.2
0
0 0.25 0.50 0.75 1.00
P(y=+1|x)
Figure1: Inthisexample,weobservethedistributionsofpositivecases(green)andnegativecases(blue)withinthe
trainingdatasetD. Additionally,wecanseethemixturedistribution(magenta)thatprovidesthebestapproximationof
thetestbagdistribution(black).
prevalenceestimationisthencombinedwiththeestimatescomputedwithsomebasequantificationmethods(CC,ACC,
PCC,PACC,andEMQ).QuaNetthengeneratesmanybagsoutofthetrainingdataDtotrainthemodel. However,in
contrasttotherestoftheDNNarchitecturesthatthispaperanalyzes,QuaNetfollowstheasymmetricapproachand
requires(justlikeallquantificationmethodsdiscussedinthissection)theavailabilityofatrainingsetDwithindividual
examplelabels.
A more exhaustive description of these (and other) quantification algorithms can be found in Esuli et al. [2023],
Gonzálezetal.[2017].
2.2 DNNArchitecturesforSets
Inrecentyears,dedicatedDNNshavebeenproposedtohandleset-baseddata. Eventhoughthesearchitectureswerenot
originallydevisedwithclassprevalenceestimationinmind,theyseemaptforthetasksincetheyallconstructontopof
permutation-invariantrepresentations. Quantificationrequirespermutation-invariantlayers,becausetheprevalencesof
BdonotchangeiftheexamplesinBareshuffled.
The first of these architectures is called DeepSets [Zaheer et al., 2017]. DeepSets relies on different permutation-
invariantpoolingoperators,likemax,averageormedian. Poolingoperatorsareappliedtothefeaturesrepresenting
theexamplesinagivenbagB. Anoperatorissaidtobepermutation-invariantwhentheoutputofthelayerisnot
affectedbytheorderinwhichtheexamplesappearinthe(serialized)inputsequenceS. Moreformally,afunctionf is
permutation-invariantiff(S)=f(π(S))foranypermutationfunctionπ. InQietal.[2021],theauthorsusethesame
architectureandpoolinglayersasinDeepSets,proposingitsapplicationtoquantificationproblems. Forthesakeof
clarity,wewillrefertotheuseofsimplepoolinglayers,asmax,averageormedian,asDeepSets.
InLeeetal.[2019]onestepforwardwastakenbyreplacingthesimplepoolingoperatorsofDeepSetswithtransformers,
i.e., with attention-based mechanisms that model complex interactions between the elements in the set. In this
architecture,calledSetTransformers,positionalencodingisnotincludedsincetheorderoftheexamplesinthebag
is unimportant. Instead of modeling the interactions between every possible pair of examples, SetTransformers
incorporatestheconceptofinducingpoints,learnablelatentdatapointsofthevectorspacethatisgivenasinputtothe
self-attentionmechanism. Inthisway,theoriginalO(n2)complexityofSetTransfomerisreducedtoO(nI),wheren
isthebagsizeandI (withI ≪n)thenumberofinducingpoints. Tothebestofourknowledge,SetTransformershave
neverbeenusedinquantification.
3 SymmetricQuantification: ACaseStudyAnalysis
WhileQietal.[2021]pioneeredthesymmetricapproachtothefieldofquantificationlearning,theauthorsdidnotdelve
deeperintotheimplicationsofthenewapproach. Amongotherthings,thissectionaimsatfillingthisgapbyproviding
acomprehensiveanalysisofitsmainadvantagesandlimitations.
4
%QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
Mostpreviousquantificationalgorithms(asforexamplethosedescribedinSection2.1)requireatrainingdatasetD,in
whichlabelsareattachedtoindividualexamples,inordertolearnaquantifierq ∈Q, q :NX →∆l−1 that,givena
testbagB,computesestimatesofclassprevalence. Therefore,thelearningdeviceisoftheformL:(X ×Y)n →Q,
meaning that the quantification problem is posed as an asymmetric task: training labels are defined in Y while
predictionsareprobabilitydistributionsfrom∆l−1. Inordertoreformulatequantificationasasymmetricsupervised
taskthetrainingsetneedstobedefinedasD′ ={(B ,p )}n′ ,withB ∈NX atrainingbaglabeledaccordingtoits
i i i=1 i
classprevalencevaluesp ∈∆l−1. ThelearningdeviceisthusformalizedasL′ :(NX ×∆l−1)n′ →Q,sothatthe
i
labelsprovidedfortrainingandthelabelsweneedtopredictbecomehomologous,i.e.,arebothprobabilitydistributions
in∆l−1.
ThisreformulationpresentssomeadvantagesanddisadvantagesthatwerenotdiscussedinQietal.[2021]. Thefirst
advantageofthenewapproachisthatthequantificationmethodisnolongernecessarilyboundtopriorprobability
shift. This is a major implication, since most previously proposed methods in the quantification literature assume
to be in presence of prior probability shift, and are specifically devised to counter it. In contrast, by adopting the
symmetricapproach,trainingexamplescanpotentiallyexhibitanytypeofshift,towhichthemethodathandwilltryto
developresilienceaspartofthelearningprocedure. Thischaracteristicissignificant,asitconsiderablybroadensthe
applicabilityofthequantificationmethodtoscenariosbeyondpriorprobabilityshift.
Thesecondadvantageisthatthequantificationproblemisaddresseddirectly,andnotviaclassificationasanintermediate
step. ThisshouldbeadvantageousbyvirtueofVapnik’sprinciple,accordingtowhich“Ifyoupossessarestricted
amountofinformationforsolvingsomeproblem,trytosolvetheproblemdirectlyandneversolveamoregeneral
problemasanintermediatestep. Itispossiblethattheavailableinformationissufficientforadirectsolutionbutis
insufficientforsolvingamoregeneralintermediateproblem”. NoticethatallthemethodsdescribedinSection2.1(with
thesoleexceptionofQuaNet)donotdirectlylearnamodelbyminimizingatask-orientedloss(asisrathercustomaryin
otherareasofsupervisedmachinelearning). ThereasonisthatmethodslikeACC,PACC,EMQ,andHDyundertakean
asymmetrictraininginwhichaclassifierislearned,andthenapredefinedpost-processingfunctionisemployedtoyield
prevalenceestimates. Asaresult,mostquantificationmethodsproposedsofarareagnostictospecificquantification
lossfunctions. Incontrast,methodsbasedonthesymmetricapproach(includingHistNetQ)canbespecificallytailored
tominimizeaquantification-orientedlossfunction. Thisisimportantasdifferentapplicationsmaybecharacterizedby
differentnotionsofcriticality;well-designedlossfunctionsplayacrucialroleinaccuratelyreflectingthesenotions,
therebyenablingamethodtobecomeaccurateintermsofapplication-dependentrequirements. Forexample(a)one
mayoptforadoptingtheabsoluteerror(AE)asaneasilyinterpretablemetricingeneralcases; (b)inapplications
relatedtoepidemiology,estimatingtheprevalenceofrarediseasesmightbebetterservedbytherelativeabsoluteerror
(RAE);(c)indifferentcontexts,employingacost-sensitiveerrormeasurecouldhelpweightherelativeimportanceof
differentclasses. See[Sebastiani,2020]forabroaderdiscussiononevaluationmeasuresforquantification.
Thethirdadvantageisawideningoftherangeofproblemstowhichquantificationcanbeapplied.Currentquantification
algorithmscannotbeappliedtoproblemsinwhichlabelsareprovidedattheaggregatelevel(i.e.,datasetsof“type
D′”). Problemsinwhichthesupervisedtrainingdatanaturallyariseintheformofsetslabeledbyprevalencearemany,
andaretheobjectofstudyofresearchareaslikemulti-instancelearning[FouldsandFrank,2010],andlearningfrom
labelproportions(LLP)[deFreitasandKück,2005,Quadriantoetal.,2009]. Examplesoftheseproblemsinclude,for
instance,post-electoralresultsbycensustract,2demographicanalysisinwhichsensibleinformation(e.g.,race,gender)
isanonymizedbutprovidedattheaggregatelevel,orpublicrecordsofproportionsofdiagnoseddiseasesperZIPcode.
Noticethatthesymmetricapproachenablestacklingtheseproblemsdirectly.
However,thesymmetricapproachfacesatleasttwoimportantissues. Thefirstoneisthatthenumberofavailable
trainingbagsinD′maybelimitedinsomeapplications. Thislimitationarisesbecausesupervisedlearningrequires
abundant labeled data. While the previous approach requires labeling individuals (i.e., instances), the symmetric
approachrequireslabelingpopulations(i.e.,bagsofinstances);thelatteriscertainlymoredemandingtoobtain. In
Section3.1wepresentamethodaimedatmitigatingthisissuethatconsistsofgeneratingnewsyntheticbagsfrom
existingones.
Thesecondaspectconcernstheapplicabilityofthesymmetricapproachtocasesinwhichtheonlyavailabletrainingset
isatraditionalone,i.e.,adatasetof“typeD”withindividualclasslabels. However,notethatsuchasetupposesno
reallimitationtothesymmetricapproachessinceadatasetof“typeD′”canbeeasilyobtainedfromadatasetof“type
D”viasampling. Section3.2discussesonesamplinggenerationprotocolthatfulfillthisrequirement;theprotocolis
well-knowninthequantificationliteraturealthoughitismorecommonlyemployedforevaluationpurposes,i.e.,for
generating,outofacollectionoflabelledindividuals,manytestbagsexhibitingdifferentclassdistributionsthatareused
fortestingquantificationalgorithms. Ofcourse,whilefeasibleinprinciple,itremainstobeseenwhetherasymmetric
2See,e.g.,thePUMS(publicusemicrodatasample)oftheU.S.CensusBureauhttps://www.census.gov/data/datasets/
2000/dec/microdata.html
5QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
AE RAE
0.00 0.02 0.04 0.06 0.00 0.05 0.10 0.15 0.20 0.25
0.005 0.010 0.015 0.020 0.0 0.5 1.0 1.5
Figure 2: Distribution of errors produced by EMQ-BCTS and “Mixer” heuristic in terms of Absolute Error (AE)
andRelativeAE(RAE)asevaluatedinLeQuadatasetsT1A(toprow)andT1B(bottomrow)(seemoredetailsin
Section5). EMQ-BCTSwastrainedandoptimizedusing,respectively,thetrainingandvalidationsets,andevaluated
inthecorrespondingtestbags,whileforMixerwerunMontecarlosimulationsgeneratingbagsoutofthetraining
examplesofeachtask.
approachtrainedviaasamplingprotocolperformscomparablyintermsofquantificationaccuracywithrespecttoa
traditionalasymmetricapproachtrainedontheoriginaldataset. Thisaspectwillbeanalyzedintheexperimentsof
Section5.
3.1 BagMixer: dataaugmentationforquantification
Arguably the most important issue the symmetric approach has to face concerns the potential limited size of D′,
somethingthatmighteasilyleadtooverfitting,especiallyifDNNmethodsareused. Thereasonwhyisthattraining
bagsaretheequivalentcounterpartsoftrainingexamplesfromaclassificationproblem.Thismeansthatevenarelatively
highnumberoftrainingbags(e.g.,theLeQuadatasetsweuseintheexperimentsofSection5comprise1000bagseach)
remainsquitelowwhencomparedtoclassificationdatasetscustomarilyusedindeeplearning(thattypicallycomprise
tensorhundredsofthousandsofinstances).
Onepossiblesolutiontothisproblemcomesdowntogeneratingnewbagsoutoftheoriginalonesviasubsampling
andmixing. Ofcourse,whileweareabletogeneratenewbagsoutoftheexamplesinourdataset,wedonotknow
the(gold)trueprevalenceofthenewlygeneratedbags. However,wecanguessitandlabelournewbagswith(silver)
prevalencevaluesinstead. Theheuristicweproposeiscalled“Mixer”andworksasfollows: givenadatasetoftypeD′,
ateachepochwegeneratenewtrainingbags(B,pˆ),fromtheoriginalones,inwhichB =B′(cid:83) B′,whereB′ (resp.
i j i
B′)isarandomsubsetcontaininghalfoftheelementsofB (resp. B )andpˆ =(|B′|p +|B′|p )/(|B′|+|B′|),and
j i j i i j j i j
inwhichbagsB andB arechosenrandomlyfromouroriginaldatasetD′. BagsgeneratedwiththeMixerarefed
i j
intothenetworkalongwithrealbagsfromD′. Theproportionofrealbagsusedforeachiterationiscontrolledbyone
hyperparameter.
Thisheuristiccertainlyintroducessomenoiseinthelabelsofthenewlygeneratedbags. However,wehaveverifiedthat
itistypicallymuchsmallerthantheerrorthatothersurrogatequantifierswouldproduceifemployedinplaceofthe
heuristicforestimatingthebagprevalence(seetheexperimentinFigure2). WeusetheBagMixerfortrainingallDNN
methods.
6
A1T
B1T
STCB-QME
rexiM
STCB-QME
rexiM
STCB-QME
rexiM
STCB-QME
rexiMQuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
3.2 GeneratingacollectionofbagsfromD
Manyexperimentsinquantificationpapersusebenchmarkdatasetsborrowedfromclassificationproblems. Inthese
datasets,testingbagsarenotnaturallyprovidedsowegeneratethemartificiallyfortestingquantificationalgorithms.
Thisway,asamplingprotocolisemployedtogenerateasufficientlylargecollectionoftestingbags,D′={(B ,p )}m′ ,
i i i=1
withB ∈ NX andp ∈ ∆l−1, fromalabeledclassificationtestsetT = {(x ,y )}m . Themostwidelyadopted
i i i i i=1
samplingprotocoliscalledArtificial-PrevalenceProtocol(APP)[Forman,2005]whichisdesignedtosimulateprior
probabilityshift. APPconsistsofdrawingafixednumberofbagsinwhichthebagprevalencep isuniformlydrawn
i
atrandomfromtheprobabilitysimplex∆l−1,andthetestingbagB foreachclassprevalencep isgeneratedfrom
i i
T viarandomsamplingwithreplacement,tryingtomaintainP(X|Y)constant. Inordertodrawprevalencevectors
uniformlyatrandom,weusetheKraemeralgorithm[SmithandTromble,2004]
NotethattheAPPprotocolisalsousefulforgeneratingatrainingdatasetof“typeD′”fromatrainingdatasetof“type
D”;thatis,whentherearenodedicatedtrainingbagsavailablebutwewanttotrainDNN-basedmethodsusingthe
symmetricapproach.
WhileAPPisspecializedingeneratingpriorprobabilityshift,noticethatifwehavesomepriorknowledgeaboutthe
applicationathand,othersamplingprotocolsdesignedforreproducingtheexpectedshiftcouldbeappliedinplace
[Zhangetal.,2013].
4 HistNetQ:DifferentiableHistograms
In this paper, we propose a permutation-invariant layer for quantification that gains inspiration from histograms.
Histogramsrepresentpowerfultoolsfordescribingsetsofvalues: theyaredirectlyalignedwiththeconceptofcounting,
andtheydisregardtheorderinwhichthevaluesarepresented. However,histogramsarenotdifferentiableoperatorsand
hencecannotbedirectlyemployedasbuildingblocksinadeeplearningmodel. Inordertoovercomethisimpediment,
histogramscanbeapproximatedbyusingcommondifferentiableoperationssuchasconvolutionsandpoolinglayers.
Differentrealizationsofthisintuitionhavebeenreportedintheliteratureofcomputervision[Avi-Aharonetal.,2020,
Peeplesetal.,2022,Wangetal.,2016]but,tothebestofourknowledge,noonebeforehasinvestigateddifferentiable
histogramsinquantification.
Previousattemptsfordevisingdifferentiablehistogramsdifferinhowtheseareimplemented. Ontheonehand,Wang
et al. [2016], Peeples et al. [2022] proposed soft variants in which every value can potentially contribute to more
thanonebin,basedonthedistanceofthevaluetothecenterofthebinandthewidththereof. Ontheotherhand,in
Yusufetal.[2020]theauthorsproposeahardvariant, thatis, everyvalueonlycontributestothebininwhichthe
valuefalls. Throughoutpreliminaryexperimentswecarriedoutusingallvariants,wefoundthatthedifferencesin
performancewererathersmall. Thehardvariantprovedslightlybetterinsuchexperiments(intermsofvalidationloss)
andisourvariantofchoicefortheexperimentsofSection5. Otherarchitecturesandtheirresultsarediscussedinthe
supplementarymaterial.
Moreformally,givenabagofndataexamplesB = {x }n ,withx ∈ X,ourgoalistocomputeahistogramfor
i i=1 i
everyfeaturevector{f }z ,wheref ∈ Rn representsthevaluesofthek-thfeatureacrosstheninstancesinthe
k k=1 k
bagB,andwherez isthenumberoffeaturesextracted(i.e.,everyhistogramiscomputedalongadifferentcolumn
froman×z matrixrepresentingB). TheharddifferentiablehistogramlayerproposedinYusufetal.[2020]takes
a user-defined hyperparameter N determining the (fixed) number of bins (we use the same number of bins for all
featurevectors),anddefines{(µ(k),w(k))}N ,thebincentersandwidths,asindependentlearnableparametersfor
b b b=1
eachfeaturevectorf . Thevalueintheb-thbinofthek-thhistogramisdefinedby:
k
n
H(k)(B)= 1 (cid:88) ϕ(f [i];µ(k),w(k)), (6)
b n k b b
i=1
whereϕisdefinedby:
(cid:26) 0, if1.01w−|v−µ| ≤1
ϕ(v;µ,w)= (7)
1, otherwise.
Thevalue1.01inEquation7isjustifiedinYusufetal.[2020]simplyasavaluethatyieldsslightlysmallervalues
than1whentheexponentis<0andslightlybiggervaluesthan1iftheexponentis>0. This,incombinationwitha
thresholdoperation,resultsina(differentiable)mechanismtodetectwhichvaluesfallintowhichbin(seeFigure3fora
graphicalrepresentationofthelayer).
7QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
Feature Features
maps Histograms
Conv. I Conv. II Global
weights: Abs weights: Exp Thres- avg.
fixed 1 fixed -1 hold
pooling
bias: −µ bias: w
Figure3: Learnablehistogramlayerwithhardbinningandlearnablebincentersandwidths. Theindividualcomponents
arecommonoperationsusedinDLframeworksthatweusetocomputeEquation7.
Notethatwecomputedensities(bydividingthecountsbyn)andnotplaincounts,inordertofactorouttheeffectof
thebagsizeinthefinalrepresentation. Notealsothatthetotalnumberofparametersofadifferentiablehistogramlayer
is2Nz. Sincethebincentersandwidthsarelearnable,theoutputcancontaininterval“gaps”(i.e.,intervalsinwhich
valuesarenottakenintoaccount),intervaloverlaps(thusallowingonevaluetocontributetomorethanoneoverlapping
binatthesametime),orevenzero-widthbins. Thismeansthattheoutputofthelayerisnotstrictlyahistogram,but
thisallowsthemodeltocontrolthecomplexityoftherepresentation(shouldN betoohigh,themodelcanwelllearnto
overlapbinsorcreatezero-widthones).
ItisworthnotingthatthequantificationmethodHDy,describedinSection2.1,alsoreliesonhistograms. However,
there are significant differences between HDy and HistNetQ. To begin with, HistNetQ models histograms on the
latentrepresentationsofthe(potentiallyhigh-dimensional)data,whereasHDymodelshistogramsontheposterior
probabilitiesreturnedbyasoftclassifier. Also,asHistNetQusesasymmetricapproachandlearnsdirectlyfrombags,
itdoesnotneedtoimposeanylearningassumption,whileHDyinsteadreliesthepriorprobabilityshiftassumptions.
Lastly, HistNetQ enables the optimization of a specific loss function during the learning process, while this is not
possibleinHDy.
Lemma4.1. Harddifferentiablehistogramlayersarepermutation-invariant.
Proof. Theproofisstraightforward. ThevalueH(k)(B)iscomputedbysummingoverthevaluesreturnedbytheϕ
b
function. Althoughπ(B),withπanypermutationfunction,alterstheorderofthevalueswithinthefeaturevectorsf ,
k
thisorderingdoesnotaffectthefinalcountssince:
n n
1 (cid:88) ϕ(f [i];µ(k),w(k))= 1 (cid:88) ϕ(π(f )[i];µ(k),w(k)),
n k b b n k b b
i=1 i=1
andhenceH(k)(B)=H(k)(π(B)).
b b
Oneoftheclaimsofthepaperisthatpollinglayerslikeaverage,median,ormaxproposedforsetoperations[Zaheer
etal.,2017,Qietal.,2021]canbeseenassimplifiedmodels(orablations)ofourproposalofusinghistogramlayers
(inotherwords,thatahistogramsubsumestheinformationconveyedbythesestatistics). Inordertoverifythis,we
designedatoyexperimentwhereasmallneuralnetworkistrainedtolearneachoftheaggregationfunctions(average,
median,andmax). Tothisaim,weequipournetworkwithasinglehistogramlayerof64bins,followedbyjusttwo
fullyconnectedlayers(sizes32and16). Thenetworkisthentrainedonrandomlygeneratedvectorsof100realvalues
between [0,max], where max is a random number in the range [0,1]. The absolute errors are pretty low: 0.0055
(average),0.0090(median),and0.0219(max)suggestingthathistogramsarericherrepresentationsthantheaverage,
median,ormax. Asthehistogramlayercancapturethedistributionofthedata,itprovidesamorecomprehensiveview
ofthedatabeyondsinglesummarystatistics,somethingthatmakesthemapromisingapproachformachinelearning
tasksthatrequireadensityestimationmethodoversets.
5 Experiments
We have performed two main experiments.3 The most important one was based on the datasets4 provided for the
LeQua2022quantificationcompetition[Esulietal.,2022]. Thesedatasetspermittedustomakeaperfectcomparison
3Thesourcecodeforreproducingtheexperimentsisavailableathttps://github.com/a2032/a2032
4https://zenodo.org/record/5734465
8QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
Featureextractionmodule Quantificationmodule
256 Sigmoid 1024 10Loss(AE,RAE)
14
32 softmax
28 conv2 denseFE denseQ
InputBagB
Permutation
16 invariant
conv1 layer
Figure4: AnexampleofthecommonarchitectureusedforDNNsmethods. Thefeatureextractionlayerandthelayer
sizes correspond to a computer vision problem (Fashion-MNIST dataset). DenseFE and denseQ are sequences of
fully-connectedlayersusedinthefeatureextractionmoduleandinthequantificationmodule,respectively.
betweenasymmetricandsymmetricmethods. TheLeQuacompetitionconsistsoffoursubtasksofproductreviews
quantification: twosubtasks(T2AandT2B)havingtodowithrawtextdocuments,andtwosubtasks(T1AandT1B)
inwhichdocumentswerealreadyconvertedintonumericalvectors(X ⊂ R300)bytheorganizers. Wefocusedon
T1AandT1Bsubtaskssinceweareunconcernedwithtextualfeatureextractioninthispaper. T1Aisabinarytaskof
estimatingtheprevalenceofpositiveversusnegativeopinions. TheorganizersprovidedatrainingdatasetDwith5,000
labeledopinions,avalidationsetD′with1,000bagsof250unlabeledopinionsannotatedbyprevalence,and5,000
testingbagsof250opinionseach. T1Bisamulticlasstaskofestimatingtheprevalenceof28merchandiseproduct
categories,andconsistsofatrainingsetDwith20,000labeledopinions,avalidationsetD′with1,000bagsof1,000
unlabeleddocumentsannotatedbyprevalence,and5,000testingbagsof1,000documents.
WetrainedourDNNmethodsusingthevalidationbagsD′,inlinewiththesymmetricapproach(Section3),while
traditionalquantificationmethods(Section2.1)weretrainedusingthetrainingsetD. Noticethat,ascouldbeexpected
inmostapplicativedomains, thesizeofthelatter(i.e.,thenumberoflabelledinstances)islargerthanthesizeofthe
former(i.e.,thenumberoflabelledbags). Inordertocompensatethisshortageoftrainingbags,weemploytheBag
Mixer(Section3.1)totrainallDNNmethods.
ThetargetlossfunctionoftheLeQuacompetitionwastherelativeabsoluteerror:
RAE(p,pˆ)=
1 (cid:88) |δ(p(c i))−δ(pˆ(c i))|
, (8)
|Y| δ(p(c ))
i
ci∈Y
inwhichδ(p ) = pi+ϵ isthesmoothingfunction, withϵthesmoothingfactorthatwesetto(2|B|)−1 following
i |Y|ϵ+1
Forman[2008],where|B|correspondstothenumberofinstancesinthebagB. Thissectionreportsrelativeerrors
butalsoabsoluteerrors,AE(p,pˆ)= 1 (cid:80) |p(c )−pˆ(c )|,becausebothhavebeenfoundtobebettersuitedfor
|Y| ci∈Y i i
quantificationevaluationthanothermeasures(like,e.g.,KLD),accordingtoSebastiani[2020]. Wehaveoptimizedall
DNNmethodstominimizetheRAEloss,becausethiswastheofficialevaluationmeasure. AsrecalledfromSection2,
mosttraditionalquantificationmethodsrelyonthepredictionsofanunderlyingclassifier. WeuseLogisticRegression
inallcases. Thehyperparametersoftheclassifierwereoptimized,independentlyforeachquantificationmethod,in
termsof RAEinthevalidationbags,eitherbytheLeQuaorganizers5(CC,PCC,ACC,PACC,HDy,QuaNet)orby
ourselves(EMQ-BCTS,EMQ-NoCalib),usingtheQuaPyquantificationlibrary[Moreoetal.,2021].
WealsousetheFashion-MNISTdataset[Xiaoetal.,2017](amorechallengingvariantofthewell-knownMNIST)
for the second experiment. In this case, the goal was to analyze the performance of symmetric approaches when
the training data consists of individual labeled examples. The training set D consists of 60,000 images while the
testsetconsistsof10,000imagesof28x28pixels. Bothsetsarelabeledaccordingto10classes. WeusetheAPP
protocol(Section3.2)for: i)generatingthetrainingbagsD′ forDNNsmethods(500bagswith500examplesfor
each epoch), and ii) evaluating all methods (5,000 test bags, of 500 examples each). To ensure the fairness of the
experiment,allthemethodsusedthesamefeatureextractionmodule(describedinSection5.1). Thequantifierslearned
byDNNmethodswereoptimizedusingAEorRAEdependingonthelossfunctionused. Theclassifieremployed
withquantificationmethodsconsistsofaclassificationheadontopofthefeatureextractionmodule,withasoftmax
activationfunction,optimizedtominimizethecross-entropyloss. Weappliedearlystoppingonavalidationsetinorder
5https://github.com/HLT-ISTI/QuaPy/tree/lequa2022
9
+QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
(a)LeQua-T1A
(b)LeQua-T1B (c)Fashion-MNIST
Figure5:Errordistribution(measuredintermsofRAEonalogarithmicscale)binnedbytheamountofpriorprobability
shift(|p −p |)betweenthetrainingsetandeachtestbag. Thegreenbarsrepresentthedistributionofbagsperbin.
D B
topreventoverfitting. Thevalidationsetwasthenusedtogeneratetheposteriorprobabilitiesonwhichsomemethods
(ACC,PACC)estimatethemisclassificationrates,andinwhichEMQ-BCTSoptimizesthecalibrationfunction.
Table1: ResultsforLEQUA-T1A,LEQUA-T1BandFASHION-MNIST,intermsofAEandRAE.Methodsthatare
notstatisticallysignificantlydifferentfromthebestone(bold),accordingtoaWilcoxonsigned-ranktest,aremarked
with†if0.001 ≤ p-value ≤ 0.05andwith‡ifp-value > 0.05. Missingvaluescorrespondtobinaryquantifiersin
multiclassproblems.
LeQua-T1A LeQua-T1B Fashion-MNIST
AE RAE AE RAE AE RAE
CC 0.0916±0.055 1.0840±4.311 0.0141±0.003 1.8936±1.187 0.0163±0.007 0.5828±0.723
PCC 0.1166±0.070 1.3940±5.621 0.0171±0.003 2.2646±1.416 0.0204±0.008 0.7817±0.974
ACC 0.0372±0.029 0.1702±0.508 0.0184±0.004 1.4213±1.270 0.0082±0.003 0.2226±0.238
PACC 0.0298±0.023 0.1522±0.464 0.0158±0.004 1.3054±0.988 0.0067±0.002 0.1831±0.193
HDy 0.0281±0.022 0.1451±0.456 - - - -
QuaNet 0.0342±0.025 0.3176±1.352 - - - -
EMQ-BCTS 0.0269±0.021 0.1183±0.251 0.0117±0.003 0.9372±0.817 0.0065±0.002 0.1510±0.152
EMQ-NoCalib 0.0236±0.018 0.1088±0.267 0.0118±0.003 0.8780±0.751 0.0132±0.005 0.2549±0.222
DeepSets(avg) 0.0278±0.021 0.1269±0.228 0.0128±0.004 0.9954±0.658 0.0083±0.003 0.3283±0.233
DeepSets(med) 0.0292±0.023 0.1389±0.256 0.0143±0.004 0.8443±0.543 0.0094±0.003 0.7195±0.586
DeepSets(max) 0.0499±0.042 0.2183±0.488 0.0277±0.005 1.4646±1.026 0.0219±0.007 0.3520±0.323
SetTransformers ‡0.0225±0.017 ‡0.1096±0.262 0.0385±0.008 1.6748±1.428 0.0104±0.003 2.2017±1.190
HistNetQ(ours) 0.0224±0.017 0.1071±0.233 0.0107±0.004 0.7574±0.489 0.0060±0.002 ‡0.1592±0.171
5.1 ACommonArchitecture
Inordertoguaranteeafaircomparison,weusedtheexactsamenetworkarchitecture,depictedinFigure4,forall
methods,replacingonlythepermutation-invariantlayer. Thearchitectureisverysimilartotheonespreviouslyproposed
forset-basedproblems[Leeetal.,2019,Qietal.,2021,Zaheeretal.,2017]. Thefirstpartofthenetworkisincharge
ofextractingfeaturesfromtheinputexamples. FortheLeQuadatasets,weusedaseriesoffully-connectedlayers
eachfollowedbyaLeakyReLUactivationfunctionanddropout,whileforFashion-MNISTweusedasimpleCNN
withtwoconvolutionallayersandonefully-connectedlayerasoutput(Figure4). Thesevectorsaregivenasinputto
apermutation-invariantlayerthatgeneratesasinglebagembedding. Theoutputofthislayerispassedthenthrough
a feed-forward module followed by a softmax activation, which finally outputs a vector containing the estimated
prevalencevaluespˆ. Inordertobackpropagatetheerrors,atleastonecompletebagmustbeprocessed.
AllDNNmethodsweretrainedfollowingthesameexactprocedure: thetrainingsetD′wassplitintoanactualtraining
setandavalidationsetusedformonitoringthevalidationloss; weappliedearlystoppingafter20epochswithout
10QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
improvementinvalidation. Hyperparameters(seesupplementarymaterial)weretunedwiththeaidofOPTUNA[Akiba
etal.,2019].
5.2 Resultsanddiscussion
Table1andFigure5reporttheresultsofbothexperiments. Inouropinion,themostimportantresultisthatHistNetQ
outperformsEMQinbothLeQuacompetitions. ThisresultisremarkablebecauseEMQisconsideredoneofthebest
quantificationmethodsintheliterature[Alexandarietal.,2020]andbecauseitwasthewinnerofbothsubtasksinthe
competition[Esulietal.,2022]. TheimprovementobtainedbyHistNetQissubstantialinT1B(morethan13%). We
thinkthatthisisduetotwofactors: (i)T1Bisamulticlassprobleminwhichitisdifficulttoaccuratelyestimatethe
posteriorprobabilities(acrucialelementforEMQ),and(ii)theperformanceofEMQsuffersastheshiftbetweenthe
trainingsetandthetestbagsincreases(seeFigure5b).
Regarding the comparison of DNN methods, the results show that representing bags using histograms (HistNetQ)
brings about better quantification performance than when using SetTransformers or simple aggregation functions
(DeepSets)[Qietal.,2021]acrossthethreedatasets,andthedifferenceinperformanceseemstocorrelatewiththe
complexityoftheproblem,withT1Bstandingoutasthemostchallengingdatasetamongthem. Weconjecturethatthis
improvementcomesfromthefactthathistogram-basedrepresentationsarenaturallygearedtoward“counting”,andthis
turnsbeneficialforquantification. Interestingly,DeepSets(median)obtainsthesecond-bestRAEscoreinT1B.This
maybesurprisingbecauseitusesanapparentlysimplisticpoolinglayer. TheperformanceofSetTransformersiserratic:
itperformssimilarly,inastatisticallysignificantsense,toHistNetQinT1A,butitobtainstheworstresultsfromthe
deeplearninglotinT1B.T1BisundeniablyharderthanT1AandSetTransformers’inducingpointslikelystruggledto
capturetheinteractionsbetweenalltheclasses. WewereunabletomakeSetTranformersconvergetobetterresults
inthiscase,despitetryingmanycombinationsofitshyperparameters(numberofinducingpoints,numberofheads,
etc.). Althoughtransformersarepowerfultoolsinmanycontexts,theyseemnottobethemostadequatesolutionfor
quantificationtaskswheretheorderandtherelationbetweenexamplesinabagarelessimportant(thisisincontrastto
othertypesofdata,suchasinnaturallanguageprocessing,wheretransformersexcelinlearningfromtheorderand
relationsbetweenwords).
YetanotheraspectthatprovedessentialforavoidingoverfittinginallDNNmethodsistheBagMixerheuristic. We
analyzethisinmoredetailinSection5.3. ConcerningHistNetQ,wealsoanalyzedtheextenttowhichthenumberof
binsaffectsperformance(seeTable2). Weobservethatincomplexproblems,likeLeQua-T1B,theperformanceof
HistNetQtendstoimproveasthenumberofbinsincreases,leadingtonetworkswithahighernumberofparameters.
However,thisisnotnecessarilyaruleofthumb,becauseinsimplerproblems,havingtoomanybinsmightleadto
overfitting. Wewouldthereforerecommendtreatingthenumberofbinsjustasanyotherhyperparametertobetuned
foreachspecificproblem.
Table2: ResultsbynumberofbinsinLeQua-T1B
AE RAE
HistNetQ(8bins) 0.0297±0.008 1.2878±1.000
HistNetQ(16bins) 0.0212±0.007 1.0572±0.738
HistNetQ(32bins) 0.0121±0.005 0.7851±0.520
HistNetQ(64bins) 0.0107±0.004 0.7574±0.489
The results on Fashion-MNIST show that EMQ with calibration is the best approach, even while requiring less
computationalresourcesthanDNNmethods. Accordingtotheliterature,theseresultsweretobeexpected,butthe
performanceofHistNetQisrathersimilarandnotsignificantlyworse;itisevenslightlybetterforAE.However,in
thiscase,HistNetQperformsworsewhentheamountofshiftislarge(seeFigure5c). Ontheotherhand,HistNetQ
outperformstherestofthequantificationalgorithms(onlyPACCgetsclose)aswellasDNNmethodsalsointhis
case. Aswitnessedinthefirstexperiment,DeepSetsusingmedianoraveragepollinglayersprovemorestablethan
SetTransformers, especially for RAE. These results seem to suggest that HistNetQ is competitive and should be
consideredevenforproblemsinwhichonlyindividuallabeledexamplesareavailable.
5.3 AblationStudy
AsrecalledfromSection3.1,theBagMixerisadataaugmentationtechniquemeanttoenhancethetrainingdataof
DNNsymmetricquantifiersinordertoavoidoverfitting. Inthissection,weanalyzetheextenttowhichtheBagMixer
impactstheperformanceofeachnetwork. Todoso,wecarryoutadditionalexperimentsinwhichtheBagMixerisnot
11QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
Figure6: TrainingandvalidationlosstrendsofHistNetQinLeQua-T1A,withandwithouttheBagMixerusinga
patiencecriterion(i.e. stoppingthetrainingprocessafteranumberofconsecutiveepochsinwhichthevalidationloss
doesnotimprove). ThetraininglossdecreasesfasterwithouttheBagMixer(leftfigure);however,thevalidationloss
keepsimprovingwiththeBagMixer(rightfigure). ThisisanindicationthattheBagMixerhelpscounteroverfitting.
used(thatis,usingonlythetrainingbagsprovidedinD′),andwecomparetheresultswiththepreviouslyreportedin
Table1.
Forthisexperiment,wehaveusedtheLeQuadatasetsbecausethetrainingbagswiththeircorrespondingprevalence
valuesareprovidedandthuslimited. Incontrast,inFashion-MNIST,trainingbagsaregeneratedwiththeAPPprotocol
usingthelabeledtrainingdatasetDandthereforearepracticallyunlimitedanddifferentineverytrainingiteration.
ForLeQua-T1A,Figure6andTable3showthatthenetworksexhibitlowertrainingerrorswhenoperatingwithjust
1000trainingbags. However,thisreductionintrainingerrorleadstoasignificantdrawback,asthesemodelstendto
performnotablyworseonthevalidationandholdoutdatasetsduetooverfitting.
Table3:LeQua-T1AresultswithoutBagMixer. RelativeerrorvariationwithrespecttowhenusingBagMixer(Table1)
isshowninparenthesis.
AE RAE
Deepsets(avg) 0.0326(+17.3%) 0.1469(+15.8%)
Deepsets(median) 0.0416(+42.5%) 0.1810(+30.3%)
Deepsets(max) 0.0570(+14.2%) 0.2287(+4.8%)
SetTransformers 0.0368(+63.6%) 0.1553(+41.1%)
HistNetQ(ours) 0.0279(+24.6%) 0.1265(+18.1%)
InthecaseofLeQua-T1B,Table4,theadverseeffectsofoverfittingareamplified. Thiswastobeexpected,since
thenumberofclassesismuchhigherinthisdataset(upto28)whilethenumberoftrainingbagsstaysthesame(i.e.,
1000). Inthisscenario,thenetworks,especiallythosewithmorecomplexarchitecturescontainingagreaternumberof
parameters,suchasSetTranformersorHistNetQ,aremorepronetooverfit.
Table4: LeQua-T1BresultswithoutBagMixer. RelativeerrorvariationwithrespecttowhenusingtheBagMixer
(Table1)isshowninparenthesis.
AE RAE
Deepsets(avg) 0.0449(+250.8%) 1.5029(+51%)
Deepsets(median) 0.0215(+50.4%) 1.0991(+30.2%)
Deepsets(max) 0.0200(-27.8%) 1.5740(+7.5%)
SetTransformers 0.0311(-19.2%) 4.2416(+153.3%)
HistNetQ(ours) 0.0445(+315.9%) 1.5108(+99.5%)
12QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
6 Conclusions
This paper introduces HistNetQ, a DNN for quantification that relies on a permutation-invariant layer based on
differentiablehistograms. Wecarriedoutexperimentsusingtwodifferentquantificationproblems(fromcomputer
visionandtextanalysis)inwhichwecomparedtheperformanceofHistNetQagainstpreviouslyproposednetworks
forsetprocessingandalsoagainstthemostimportantalgorithmsfromthequantificationliterature. Theresultsshow
thatHistNetQachievedstate-of-the-artperformanceinbothproblems. Fromaqualitativepointofview,HistNetQ
alsodisplaysinterestingpropertieslikei)theabilitytodirectlylearnfrombagslabeledbyprevalence,whichallows
HistNetQtobeappliedtoscenariosinwhichtraditionalmethodscannot;andii)thepossibilitytodirectlyoptimizefor
specificlossfunctions.
Thisresearchmayhopefullyofferanewviewpointinquantificationlearning,sinceourresultssuggestthatexploiting
datalabeledattheaggregatelevelmightbepreferable,intermsofquantificationperformance,thanexploitingdata
labeled at the individual level. Overall, this study seems to suggest that HistNetQ is a promising alternative for
implementing the symmetric approach in real applications, obtaining state-of-the art results that surpass previous
approaches.
Futureworkmayincludei)studyingthecapabilitiesofHistNetQwhenconfrontedwithtypesofdatasetshiftotherthan
priorprobabilityshift[Tasche,2022,Zhangetal.,2013]andii)exploringpotentialapplicationsofthisarchitectureto
otherproblemsthat,likequantification,requirelearningamodelfromdensityestimatesoversetsofexamples.
References
OscarBeijbom,JudyHoffman,EvanYao,TrevorDarrell,AlbertoRodriguez-Ramirez,ManuelGonzalez-Rivero,and
OveHoeghGuldberg. Quantificationin-the-wild: data-setsandbaselines. arXiv:1510.04811[cs],November2015.
arXiv: 1510.04811.
GeorgeForman. Quantifyingtrendsaccuratelydespiteclassifiererrorandclassimbalance. InProceedingsofthe12th
ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining(KDD2006),pages157–166,
Philadelphia,US,2006. doi:10.1145/1150402.1150423.
PabloGonzález,AlbertoCastaño,EmilyEPeacock,JorgeDíez,JuanJoséDelCoz,andHeidiMSosik. Automatic
planktonquantificationusingdeepfeatures. JournalofPlanktonResearch,41(4):449–463,2019.
DanielHopkinsandGaryKing. Amethodofautomatednonparametriccontentanalysisforsocialscience. American
JournalofPoliticalScience,54(1):229–247,2010.
AlejandroMoreoandFabrizioSebastiani. Tweetsentimentquantification: Anexperimentalre-evaluation. PLOSONE,
17(9):1–23,092022. doi:10.1371/journal.pone.0263449.
FabioFelixDias,MoacirAntonelliPonti,andRosaneMinghim.Aclassificationandquantificationapproachtogenerate
featuresinsoundscapeecologyusingneuralnetworks. NeuralComputingandApplications,34(3):1923–1937,2022.
PabloGonzález, JorgeDíez, NiteshChawla, andJuanJosédelCoz. Whyisquantificationaninterestinglearning
problem? ProgressinArtificialIntelligence,6(1):53–58,2017. ISSN2192-6360. doi:10.1007/s13748-016-0103-3.
George Forman. Quantifying counts and costs via classification. Data Mining and Knowledge Discovery, 17(2):
164–206,October2008. ISSN1573-756X. doi:10.1007/s10618-008-0097-y.
Víctor González-Castro, Rocío Alaiz-Rodríguez, and Enrique Alegre. Class distribution estimation based
on the Hellinger distance. Information Sciences, 218:146–164, January 2013. ISSN 0020-0255.
doi:10.1016/j.ins.2012.05.028.
HidekoKawakubo,MarthinusChristoffelDuPlessis,andMasashiSugiyama. Computationallyefficientclass-prior
estimationunderclassbalancechangeusingenergydistance. IEICETRANSACTIONSonInformationandSystems,
99(1):176–186,2016.
JoaquinQuionero-Candela,MasashiSugiyama,AntonSchwaighofer,andNeilD.Lawrence. DatasetShiftinMachine
Learning. TheMITPress,Cambridge,MA,2009.
PabloGonzález,AlbertoCastaño,NiteshVChawla,andJuanJoséDelCoz. Areviewonquantificationlearning. ACM
ComputingSurveys(CSUR),50(5):1–40,2017.
AndreaEsuli,AlessandroFabris,AlejandroMoreo,andFabrizioSebastiani. LearningtoQuantify. Springer,Cham,
CH,2023. ISBN978-3-031-20466-1. doi:10.1007/978-3-031-20467-8.
Lei Qi, Mohammed Khaleel, Wallapak Tavanapong, Adisak Sukul, and David Peterson. A framework for deep
quantificationlearning. InMachineLearningandKnowledgeDiscoveryinDatabases: EuropeanConference,ECML
PKDD2020,Ghent,Belgium,September14–18,2020,Proceedings,PartI,pages232–248.Springer,2021.
13QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
HarrisonEdwardsandAmosJ.Storkey. Towardsaneuralstatistician. In5thInternationalConferenceonLearning
Representations,ICLR2017,Toulon,France,April24-26,2017,ConferenceTrackProceedings,2017.
RyanL.Murphy,BalasubramaniamSrinivasan,VinayakA.Rao,andBrunoRibeiro. Janossypooling: Learningdeep
permutation-invariantfunctionsforvariable-sizeinputs.In7thInternationalConferenceonLearningRepresentations,
ICLR2019,May6-9,2019,NewOrleans,LA,USA,2019.OpenReview.net.
EdwardWagstaff, FabianFuchs, MartinEngelcke, IngmarPosner, andMichaelAOsborne. Onthelimitationsof
representingfunctionsonsets. InInternationalConferenceonMachineLearning,pages6487–6494.PMLR,2019.
ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,andAlexanderJSmola.
Deepsets. Advancesinneuralinformationprocessingsystems,30,2017.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A
framework for attention-based permutation-invariant neural networks. In International conference on machine
learning,pages3744–3753.PMLR,2019.
AndreaEsuli,AlejandroMoreo,andFabrizioSebastiani. Arecurrentneuralnetworkforsentimentquantification. In
Proceedingsofthe27thACMInternationalConferenceonInformationandKnowledgeManagement(CIKM2018),
pages1775–1778,Torino,IT,2018. doi:10.1145/3269206.3269287.
AndreaEsuli,AlejandroMoreo,FabrizioSebastiani,andGianlucaSperduti. AdetailedoverviewofLeQua@CLEF
2022: Learning to quantify. In Proceedings of the Working Notes of CLEF 2022 - Conference and Labs of the
EvaluationForum,Bologna,Italy,September5th-8th,2022,volume3180ofCEURWorkshopProceedings,pages
1849–1868,Bologna,Italy,2022.CEUR-WS.org.
AfonsoFernandesVaz,RafaelIzbicki,andRafaelBassiStern. Quantificationunderpriorprobabilityshift: Theratio
estimatoranditsextensions. JournalofMachineLearningResearch,20:79:1–79:33,2019.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box
predictors. InInternationalconferenceonmachinelearning,pages3122–3130.PMLR,2018.
Mirko Bunse. On multi-class extensions of adjusted classify and count. In Proceedings of the 2nd International
WorkshoponLearningtoQuantify(LQ2022),pages43–50,Grenoble,IT,2022.
Antonio Bella, Cesar Ferri, José Hernández-Orallo, and María José Ramírez-Quintana. Quantification via Prob-
ability Estimators. In 2010 IEEE International Conference on Data Mining, pages 737–742, December 2010.
doi:10.1109/ICDM.2010.75. ISSN:2374-8486.
Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a pri-
ori probabilities: A simple procedure. Neural Computation, 14(1):21–41, January 2002. ISSN 0899-7667.
doi:10.1162/089976602753284446.
Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-corrected calibration is
hard-to-beatatlabelshiftadaptation. InInternationalConferenceonMachineLearning,pages222–232.PMLR,
2020.
AndreaEsuli,AlessioMolinari,andFabrizioSebastiani. AcriticalreassessmentoftheSaerens-Latinne-Decaestecker
algorithmforposteriorprobabilityadjustment. ACMTransactionsonInformationSystems(TOIS),39(2):1–34,2020.
FabrizioSebastiani. Evaluationmeasuresforquantification: Anaxiomaticapproach. InformationRetrievalJournal,23
(3):255–288,2020. doi:10.1007/s10791-019-09363-y.
JamesR.FouldsandEibeFrank. Areviewofmulti-instancelearningassumptions. Knowl.Eng.Rev.,25(1):1–25,2010.
doi:10.1017/S026988890999035X.
NandodeFreitasandHendrikKück. Learningaboutindividualsfromgroupstatistics. InProceedingsofthe21st
ConferenceinUncertaintyinArtificialIntelligence(UAI2005),pages332–339,Edimburgh,UK,2005.
NoviQuadrianto,AlexanderJ.Smola,TibérioS.Caetano,andQuocV.Le. Estimatinglabelsfromlabelproportions.
JournalofMachineLearningResearch,10:2349–2374,2009.
GeorgeForman. Countingpositivesaccuratelydespiteinaccurateclassification. InProceedingsofthe16thEuropean
ConferenceonMachineLearning(ECML2005),pages564–575,Porto,PT,2005. doi:10.1007/11564096_55.
NoahASmithandRoyWTromble. Samplinguniformlyfromtheunitsimplex. JohnsHopkinsUniversity,Tech.Rep,
29,2004.
KunZhang,BernhardSchölkopf,KrikamolMuandet,andZhikunWang.Domainadaptationundertargetandconditional
shift. InICML,pages819–827,2013.
MorAvi-Aharon,AssafArbelle,andTammyRiklinRaviv. Deephist: Differentiablejointandcolorhistogramlayersfor
image-to-imagetranslation. arXivpreprintarXiv:2005.03995,2020.
14QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
JoshuaPeeples,WeihuangXu,andAlinaZare. Histogramlayersfortextureanalysis. IEEETransactionsonArtificial
Intelligence,3(4):541–552,2022. doi:10.1109/TAI.2021.3135804.
ZheWang,HongshengLi,WanliOuyang,andXiaogangWang. Learnablehistogram: Statisticalcontextfeaturesfor
deepneuralnetworks. InEuropeanConferenceonComputerVision,pages246–262.Springer,2016.
IbrahimYusuf,GeorgeIgwegbe,andOluwafemiAzeez. Differentiablehistogramwithhard-binning. arXivpreprint
arXiv:2012.06311,2020.
AlejandroMoreo,AndreaEsuli,andFabrizioSebastiani. QuaPy: apython-basedframeworkforquantification. In
Proceedingsofthe30thACMInternationalConferenceonInformation&KnowledgeManagement,pages4534–4543,
2021.
HanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist:anovelimagedatasetforbenchmarkingmachinelearning
algorithms. arXivpreprintarXiv:1708.07747,2017.
TakuyaAkiba, ShotaroSano, ToshihikoYanase, TakeruOhta, andMasanoriKoyama. Optuna: Anext-generation
hyperparameteroptimizationframework. InProceedingsofthe25rdACMSIGKDDInternationalConferenceon
KnowledgeDiscoveryandDataMining,2019.
Dirk Tasche. Class prior estimation under covariate shift: No problem? In Proceedings of the 2nd International
WorkshoponLearningtoQuantify: MethodsandApplications(LQ2022),ECML/PKDD,Grenoble(France),2022.
15QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
A OtherTypesofDifferentiableHistogramLayers
Differentiablehistogramscanbeclassifiedasbelongingtothehardorsoftbinningtypes,dependingonwhethereach
valuecontributesonlytothebinitbelongstoorifinstead,eachvaluecontributestomorethanonebin(basedonthe
distanceofthevaluetothebincenteranditswidth),respectively.
Inourexperiments,wehavetestedfourdifferenthistogramsproposedintheliterature,allofthempermutation-invariant
and therefore suitable for set processing. In Section 5, we only reported results for the hard variant that obtained
slightlybetterresults. Thearchitectureoftheremaininghistogramtypes,alongwiththeresultswehaveobtainedinour
experiments,arediscussedinthissection.
A.1 ArchitecturesofOtherDifferentiableHistograms
Feature Features
maps LearnableHistogramLayer(soft) Histograms
ConvolutionI ConvolutionII
Globalavg.
weights: fixed1 Abs weights: w ReLU
pooling
bias:−µ bias: fixed1
Figure7: soft: Learnablehistogramlayerwithsoftbinningandvariablebincentersandwidths
Feature Features
maps LearnableHistogramLayer(softrbf) Histograms
ConvolutionI ConvolutionII Square
Globalavg.
weights: fixed1 weights: w and Exp
pooling
bias: −µ bias: fixed0 negate
Figure8: softrbf: LearnablehistogramlayerwithsoftbinningandvariablebincentersandwidthsusingaRBFfunction
GivenabagofndataexamplesB = {x }n ,withx ∈ X asbefore,ourgoalistocomputeahistogramforevery
i i=1 i
featurevector{f }z ,wheref ∈Rnrepresentsthevaluesofthek-thfeatureacrosstheinstancesx ∈Binalatent
i i=1 k i
spaceRz.
Thesoftdifferentiablehistogramlayers,proposedbyWangetal.[2016],usesoftbinningwithvariablebinsandemploy
convolutionallayerstoapproximatethehistogram(seeFigure7). Thecountsinthesofthistogramsarecomputedby:
(cid:18) (cid:19)
1
ϕ(v;µ,w)=max 0,1− ×|v−µ| , (9)
w
whereµandwarethebincenterandwidthofthebin,andvisoneofthevaluesgeneratedforthek-thfeature. The
rationalebehindthisequationisthatthecloserthevaluevgetstothebincenterµ,thesmallerthemultiplierbecomes,
thusreturningavaluecloseto1. Analogously,asmallvalueforthebinwidthwresultsinlargermultiplicativefactors,
thusmakingthefinalcountcloserto0.
AsshowninFigure7,afirstconvolutionlayerlearnsthebincentersthroughthebiastermwhileasecondconvolutional
layerlearnsthebinwidths.
Inthesoftrbf differentiablehistogramlayersbyPeeplesetal.[2022], anRBFfunctionisusedtoapproximatethe
histogram. Justlikeinthepreviouscase,thehistogramfallsintothecategoryofsoftbinningwithvariablebins. Inthis
case,thecountsarecomputedas:
ϕ(v;µ,w)=e−(v− wµ)2
. (10)
Theparametersofthisfunctionaresimilarlylearnedthroughconvolutions(seeFigure8).
Finally, the sigmoid differentiable histogram layers do not use convolutional layers but two logistic functions to
approximateeachbin. Inthiscase,thetypeofhistogramproducedishardwithfixedbincentersandwidths(thereare
16QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
nolearnableparametersinthelayer). Inthefirststep,thebincentersµandthebinwidthwareinitialized,depending
onthenumberofbinsselected. Then,inasecondstep,twologisticfunctionsareusedtoapproximatewhichvaluesfall
ineachbin(seeFigure9). Thismethodcanbeeasilycomputedusingbasicdifferentiableoperations.
1
σ(v−(µ+w))
2
1−σ(v−(µ−w))
2
0.25 0.5 0.75 1 1.25
Figure9: Histogrambinapproximationusingtwosigmoidfunctionsσ(v)= 1 . Inthisexample,thebincenteris
1+eγv
fixedandequaltoµ=0.75. Binwidthisalsofixedwithw =0.5. γ isaconstantwithahighenoughvaluetomake
sigmoidfunctionssharpandclosertoastepfunction.
B AdditionalResults
Inthissection,weturntoreportadditionalexperimentswehavecarriedoutthatwereomittedfromthepaperforthe
sakeofbrevity. Inparticular,wereportresultsforthedifferentiablehistograms(HistNetQSoft,HistNetQSoftRBF,and
HistNetQSigmoid)discussedintheprevioussection. TheresultsaredisplayedinTable5.
Table5: ResultsforFASHION-MNIST,LEQUA-T1AandLEQUA-T1BintermsofAEandRAE.
Fashion-MNIST LeQua-T1A LeQua-T1B
AE RAE AE RAE AE RAE
CC 0.01634±0.00738 0.58279±0.72314 0.09160±0.05540 1.08400±4.31090 0.01406±0.00295 1.89365±1.18732
PCC 0.02040±0.00796 0.78168±0.97434 0.11664±0.06978 1.39402±5.62123 0.01711±0.00332 2.26462±1.41627
ACC 0.00824±0.00310 0.22256±0.23753 0.03716±0.02936 0.17020±0.50800 0.01841±0.00437 1.42134±1.26971
PACC 0.00673±0.00238 0.18310±0.19252 0.02985±0.02258 0.15218±0.46440 0.01578±0.00379 1.30538±0.98837
HDy - - 0.02814±0.02212 0.14514±0.45621 - -
QuaNet - - 0.03418±0.02528 0.31764±1.35237 - -
EMQ-BCTS 0.00652±0.00246 0.15097±0.15191 0.02689±0.02094 0.11828±0.25065 0.01174±0.00305 0.93721±0.81732
EMQ-NoCalib 0.01324±0.00472 0.25493±0.22246 0.02359±0.01845 0.10878±0.26668 0.01177±0.00285 0.87802±0.75120
DeepSets(avg) 0.00829±0.00254 0.32826±0.23251 0.02779±0.02105 0.12686±0.22817 0.01283±0.00379 0.99542±0.65778
DeepSets(median) 0.00942±0.00288 0.71946±0.58579 0.02919±0.02273 0.13887±0.25631 0.01429±0.00432 0.84427±0.54286
DeepSets(max) 0.02185±0.00699 0.35195±0.32316 0.04991±0.04167 0.21830±0.48828 0.02766±0.00515 1.46464±1.02644
SetTransformers 0.01043±0.00328 2.20175±1.19007 ‡0.02246±0.01717 ‡0.10958±0.26205 0.03847±0.00779 1.67475±1.42750
HistNetQHard(ours) 0.00602±0.00206 0.15923±0.17085 0.02236±0.01709 0.10707±0.23312 0.01070±0.00367 0.75739±0.48891
HistNetQSoft(ours) 0.00842±0.00273 0.16616±0.14238 0.02279±0.01763 †0.10830±0.22461 0.01846±0.00671 0.94806±0.58838
HistNetQSoftRBF(ours) 0.00688±0.00216 0.13601±0.11244 †0.02257±0.01729 ‡0.11250±0.28344 0.02095±0.00688 1.05116±0.66311
HistNetQSigmoid(ours) 0.00758±0.00237 0.69029±0.50995 0.02197±0.01746 0.10728±0.27898 0.01855±0.00630 0.99868±0.62058
These results show that the differences in performance between the histogram-based models are rather small for
Fashion-MNIST (in which SoftRBF seems to work better in terms of RAE), and in LeQua-T1A (in a statistically
significantsense). However,forLeQua-T1B(thehardestproblemintermsofthenumberofclasses),HistNetQHard
clearlystandsoutasthebestofthelot. Itisworthnotingthattheseresults,obtainedinthetestsets,alignwellwith
thetrendseachmethoddisplaysinthevalidationloss. Indeed,ourpreferenceforHistNetQHardovertherestofthe
methodsisbasedontheobservationthatHistNetQHarddisplaysthesmallestvalidationlossoverall–thatistosay,we
havenotsimplypickedthemethoddisplayingthebestresultsintest.
17QuantificationusingPermutation-InvariantNetworksbasedonHistograms APREPRINT
C Hyper-parameterSelection
Table6: Summaryofthemostimportanthyperparametersusedforeachtask. Thelastfourhyperparametersarespecific
forSetTransformers
Hyperparam. Description Fashion-MNIST LeQua-T1A LeQua-T1B
lr Startinglearningrate 0.0003 0.0001 0.0005
optimizer Optimizerusedfortraining AdamW AdamW AdamW
batch_size Numberoffullbagspassedthroughthenet- 2 20 500
workbeforeupdatingtheweights
wd Weightdecay 0 0.00001 0.00001
N Numberofbinsusedinthehistograms 32 32 64
R RealbagsproportionusedbytheBagMixer - 0.9 0.5
ateachepoch
z Outputsizeofthefeatureextractionlayer 256 300 512
FF_Q Number and size of linear layers in the [1024] [2048,2048,2048] [4096]
quantificationhead
dropout Dropoutusedinquantificationmodulelin- 0.1 0.5 0.5
earlayers
O OutputsizeforSetTransfomer 512 512 512
I Number of inducing points in SetTrans- 32 32 128
fomer
H HiddensizeinSetTransfomer 256 256 256
nh NumberofheadsinSetTransfomer 4 4 4
Hyper-parametersforthedifferenttaskswereoptimizedwiththehelpofOPTUNA[Akibaetal.,2019]forthecasesin
whichitwascomputationallyfeasible(Fashion-MNISTandLeQua-T1A).Table6summarizestheconfigurationshown
fortheresultspresentedinourpaper.
18