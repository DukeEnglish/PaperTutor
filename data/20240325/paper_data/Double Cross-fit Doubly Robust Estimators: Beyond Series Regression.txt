Double Cross-fit Doubly Robust Estimators:
Beyond Series Regression
Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, and Larry Wasserman
Department of Statistics & Data Science
Carnegie Mellon University
{alec, siva, edward, larry} @ stat.cmu.edu
Abstract
Doublyrobustestimatorswithcross-fittinghavegainedpopularityincausalinferenceduetotheirfavor-
able structure-agnostic error guarantees. However, when additional structure, such as Ho¨lder smoothness,
is available then more accurate “double cross-fit doubly robust” (DCDR) estimators can be constructed by
splitting the training data and undersmoothing nuisance function estimators on independent samples. We
studyaDCDRestimatoroftheExpectedConditionalCovariance,afunctionalofinterestincausalinference
andconditionalindependencetesting, andderiveaseriesofincreasinglypowerfulresultswithprogressively
stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no
assumptionsonthenuisancefunctionsortheirestimators. Then,assumingthenuisancefunctionsareHo¨lder
smooth,butwithoutassumingknowledgeofthetruesmoothnesslevelorthecovariatedensity,weestablish
thatDCDRestimatorswithseverallinearsmoothersaresemiparametricefficientunderminimalconditions
√
and achieve fast convergence rates in the non- n regime. When the covariate density and smoothnesses
areknown,weproposeaminimaxrate-optimalDCDRestimatorbasedonundersmoothedkernelregression.
√
Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than- n central limit theorem,
√
and that inference is possible even in the non- n regime. Finally, we support our theoretical results with
simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our es-
timator achieves semiparametric efficiency while the usual “single cross-fit” estimator fails, and illustrating
asymptotic normality for the undersmoothed DCDR estimator.
1 Introduction
In causal inference, the researcher’s objective is often to estimate a lower-dimensional functional of the data
generating distribution (e.g., the Average Treatment Effect, the Local Average Treatment Effect, the Average
TreatmentEffectontheTreated,etc.). Dependingonthefunctional,estimatorscanbeconstructedassummary
statistics of combinations of nuisance function estimates (e.g., the propensity score and outcome regression
function). Forthispurpose,doublyrobustestimatorsbasedoninfluencefunctionsandsemiparametricefficiency
theory have become increasingly popular due to their favorable error guarantees [Kennedy, 2022, Tsiatis, 2006,
van der Laan and Robins, 2003]. Doubly robust estimators can be combined with cross-fitting, where the
nuisance function estimators are trained on a separate, independent sample, to avoid imposing Donsker or
other complexity conditions [Chernozhukov et al., 2018, Robins et al., 2008, Zheng and van der Laan, 2010].
This approach, which we refer to as the “single cross-fit” doubly robust (SCDR) estimator, is well-known and
extensivelystudied. Byemployingsamplesplittingandcross-fitting,flexiblemachinelearningestimatorscanbe
usedtoestimatenuisancefunctionswhilestillguaranteeingsemiparametricefficiencyandasymptoticnormality
forthefunctionalestimator,undern−1/4-typerateconditionsonthenuisanceestimators. Infact,Balakrishnan
et al. [2023] showed this estimator is minimax optimal in a particular structure-agnostic model.
1
4202
raM
22
]TS.htam[
1v57151.3042:viXraHowever, despite its favorable structure-agnostic properties, the SCDR estimator may be sub-optimal when
additional structure, such as H¨older smoothness, is present. For instance, when estimating a mixed bias func-
tionalandassumingH¨older(s)smoothnuisancefunctions,theSCDRestimatorissemiparametricefficientwhen
s>d/2, where d is the dimension of the covariates [Rotnitzky et al., 2021]. Notably, this condition is stronger
√
than the minimax lower bound that s > d/4 [Robins et al., 2009]. In the non- n regime, when s < d/4, the
SCDR estimator also fails to attain the lower bound on the minimax rate.
Robins et al. [2008] introduced higher-order estimators as an alternative to the SCDR estimator, using
the higher-order influence function of the target functional to further debias the doubly robust (or, “first-
order”) estimator. These higher-order estimators can be minimax rate-optimal and semiparametric efficient
underminimalconditionsinsmoothnessmodels. However,thepracticalconstructionofhigher-orderestimators
remainschallengingdespiterecentadvances[LiuandLi,2023,Liuetal.,2020,2021,Robinsetal.,2017,vander
Vaart, 2014].
Anotheroption,firstproposedbyNeweyandRobins[2018],isthedoublecross-fit doublyrobust(DCDR)es-
timator,whichcombinesthedoublyrobustestimatorwithundersmoothednuisancefunctionestimatorstrained
on separate, independent samples. Combining undersmoothing with cross-fitting and / or sample splitting for
optimal estimation has been demonstrated in a variety of contexts [Gin´e and Nickl, 2008a, Newey et al., 1998,
Paninski and Yajima, 2008, van der Laan et al., 2022]. Newey and Robins [2018] proposed the DCDR esti-
mator with regression spline nuisance function estimators and showed this estimator can be semiparametric
efficient under minimal conditions in a H¨older smoothness model. Fisher and Fisher [2023] and Kennedy [2023]
extendedthisapproachtoestimateheterogeneouseffects, whileMcGrathandMukherjee[2022]developedmin-
imax rate-optimal plug-in and DCDR estimators, employing series estimators with wavelets to estimate the
nuisance functions.
Inthispaper,weuseaDCDRestimatortoestimatetheExpectedConditionalCovariance(ECC),incorporat-
ingprogressivelystrongerassumptionstoyieldincreasinglypowerfulresults. Webeginwithastructure-agnostic
analysis, presenting a novel asymptotically linear expansion of the DCDR estimator and providing a detailed
analysis of the remainder term. Assuming H¨older smoothness of the nuisance functions, we then establish sem-
√
parametric efficiency under minimal conditions and rates of convergence in the non- n regime. Importantly,
weconsidernearestneighborsandlocalpolynomialregressionestimatorsforthenuisancefunctions,whichhave
not been studied in this context. Furthermore, when both the smoothness levels of the nuisance functions and
√
the covariate density are known, we show that minimax optimal estimation and slower-than- n inference are
feasible.
1.1 Structure of the paper and our contributions
In Section 1.2 we define relevant notation. In Section 2, we describe the ECC, review known lower bounds for
estimatingtheECCoverH¨oldersmoothnessclasses, revisittheexistingliteratureofplug-in, doublyrobustand
higher-order estimators for the ECC, and discuss the motivation for double cross-fitting in more detail.
In Section 3, we provide a new structure-agnostic convergence result for generic DCDR estimators and
analyze its implications, noting that undersmoothing leads to the fastest convergence rate when the nuisance
functionssatisfy acovariance condition— specifically, whenthe covarianceoverthe trainingdata ofan estima-
tor’s predictions at two independent test points scales inversely with sample size.
InSection4,weassumethenuisancefunctionsareH¨oldersmooth,butdonotassumethesmoothnessorthe
covariate density are known, and analyze the DCDR estimator. We show that the DCDR estimator combined
with undersmoothed local polynomial regression is semiparametric efficient under minimal conditions, and
√
achieves a convergence rate in the non- n regime faster than that of the usual SCDR estimator. This faster
convergenceratehasbeenconjecturedtobetheminimaxratewithnon-smoothcovariatedensity[Robinsetal.,
2008]. We also highlight that the DCDR estimator with k-Nearest Neighbors can be semiparametric efficient
2when the nuisance functions are H¨older smooth of order at most one but are sufficiently smooth compared to
the dimension of the covariates (e.g., if the nuisance functions are Lipschitz and the dimension of the covariates
is less than four). However, none of the estimators in Section 4 achieve the minimax rate for smooth or known
covariatedensitybecausetherelevanttuningparameterscanonlyscaleatacertainratetoguaranteetheinverse
Gram matrix exists.
Therefore, in Section 5 we assume the covariate density is known, and use it to allow the tuning param-
eters to scale at more extreme rates and the nuisance function estimators to be further undersmoothed. We
demonstrate minimax optimality of the DCDR estimator when combined with appropriately undersmoothed
covariate-density-adapted kernel regression, which uses the known covariate density. Furthermore, we show
√
asymptotic normality in the non- n regime by undersmoothing the DCDR estimator so its variance dominates
√
its squared bias, but it converges to a normal limiting distribution around the ECC at a slower-than- n rate.
In Section 6, we illustrate our results via simulation. We provide intuition for double cross-fitting and
undersmoothing, demonstrate when our estimator achieves semiparametric efficiency while the usual “single
cross-fit” estimator fails, and illustrate asymptotic normality for the undersmoothed DCDR estimator in the
√
non- n regime. Finally, in Section 7, we conclude and discuss future work.
Thispaperprovidesseveralcontributionstotheliterature. Lemma1andProposition1inSection3presenta
new structure-agnostic analysis of the DCDR estimator for the ECC, which holds for generic nuisance function
estimators. These results can be useful for generic data generating processes and generic nuisance function
estimators. Theorems1and2inSections4and5,respectively,establishsemiparametricefficiencyunderminimal
conditionsandminimaxrate-optimalconvergencefortheDCDRestimatordependingonthesmoothnessofthe
nuisance functions, knowledge of the covariate density, and the nuisance function estimators. While Newey and
Robins [2018] and McGrath and Mukherjee [2022] presented results for series and spline methods, our results
extendtheseanalysestolocalaveragingestimatorssuchaslocalpolynomialregressionandk-NearestNeighbors.
Moreover, Theorem 3 shows asymptotic normality, allowing for inference when both the covariate density and
smoothnessofthenuisancefunctionsareknown. WhileRobinsetal.[2016]establishedasymptoticnormalityof
√
a higher-order estimator of the ECC in the non- n-regime, our result is, to the best of our knowledge, the first
√
limiting distribution result for a cross-fit doubly robust estimator in the non- n regime. Lastly, our simulation
results illustrate efficiency and inference with H¨older smooth nuisance functions. Our code is available at
https://github.com/alecmcclean/DCDR.
1.2 Notation
WeuseEforexpectation,Vforvariance, cov forcovariance,andP (f)=P {f(Z)}= 1 (cid:80)n f(Z )forsample
n n n i=1 i
averages. When x∈Rd we let ∥x∥2 =(cid:80)d x2 denote the squared Euclidean norm, while for generic possibly
j=1 j
random functions f we let ∥f∥2 = (cid:82) f(z)2dP(z) denote the squared L (P) norm and ∥f∥ = sup |f(z)|
P Z 2 ∞ z∈Z
denotethesupremumoff. Iff(cid:98)isanestimatedfunction,thenE∥f(cid:98)∥2
P
istheexpectationof∥f(cid:98)∥2
P
overthetraining
data used to construct f(cid:98). Finally, if A is a square matrix, then λ i(A) refers to the ith eigenvalue of A and
ρ(A)=max {|λ (A)|} is the maximum absolute eigenvalue of A, or spectral radius of A.
i i
We use the notation a ≲ b to mean a ≤ Cb for some constant C, and a ≍ b to mean cb ≤ a ≤ Cb for
some constants c and C, so that a ≲ b and b ≲ a. We use ⇝ to denote convergence in distribution, →p for
a.s.
convergenceinprobability,and−→forconvergencealmostsurely. Weusethenotationa∧banda∨btodenote
the minimum and maximum, respectively, of a and b. We use oP(·) and OP(·) to mean usual convergence in
probability and stochastic boundedness, i.e., if X
n
is a sequence of random variables then X
n
=oP(r n) implies
(cid:12) (cid:12) (cid:16)(cid:12) (cid:12) (cid:17)
(cid:12) (cid:12)X rnn(cid:12) (cid:12)→p 0 and X
n
=OP(r n) implies there exists C <∞ such that P (cid:12) (cid:12)X rnn(cid:12) (cid:12)≥C →0 as n→∞, and use o(1)
and O(1) to denote usual deterministic convergence, i.e., if x is a sequence then x =o(1) implies x →0 as
n n n
n→∞ and x =O(1) implies there exists C <∞ such that x ≤C as n→∞.
n n
When referring to the class of H¨older(s) smooth functions, we mean the class of functions f : Rd → R
that are ⌊s⌋-times continuously differentiable with partial derivatives bounded (where ⌊s⌋ is the largest integer
3strictly smaller than s), and for which
|Dmf(x)−Dmf(x′)|≲∥x−x′∥s−⌊s⌋
for all x,x′ and m=(m ,...,m ) such that (cid:80)d m =⌊s⌋, where Dm = ∂⌊s⌋ is the multivariate partial
1 d j=1 j ∂xm 11...∂xm dd
derivative operator.
In certain places, we denote generic nuisance functions by η. When relevant, we denote datasets of n
observations by D with an appropriate subscript to indicate which dataset. For example, we will refer to the
training data for estimating nuisance function η by D . Further, we denote the covariates of n observations by
η
Xn, and use subscripts in the same way. So, Xn denotes the covariate data in D .
η η
2 Setup and background
Inthissection,wedescribethedatageneratingprocessandtheECC,reviewknownlowerboundsforestimating
the ECC over H¨older smoothness classes, revisit the existing literature on plug-in, doubly robust, and higher-
order estimators, and discuss the motivation for double cross-fitting.
We assume we observe a dataset comprising 3n independent and identically distributed data points {Z }3n
i i=1
drawn from a distribution P. Here, Z is a tuple {X ,A ,Y } where X ∈ Rd are covariates and A ∈ R and
i i i i
Y ∈R. Wedenoteπ(X)=E(A|X)andµ(X)=E(Y |X)andcollectivelyrefertothemasnuisancefunctions.
In causal inference, often A denotes binary treatment status, while Y is the outcome of interest. In that case,
π is referred to as the propensity score and µ as the outcome regression function.
In this paper, we focus on estimating the ECC, denoted by ψ , which is defined as:
ecc
ψ =E{cov(A,Y |X)}=E(AY)−E{π(X)µ(X)}.
ecc
TheECCappearsinthecausalinferenceliteratureinthenumeratorofthevarianceweightedaveragetreatment
effect [Li et al., 2011], as a measure of causal influence [D´ıaz, 2023], and in derivative effects under stochastic
interventions [McClean et al., 2022, Zhou and Opacic, 2022]. Additionally, the ECC has appeared in the
conditional independence testing literature [Shah and Peters, 2020]. Prior work on semiparametric efficient
and minimax optimal DCDR estimators has also focused on the ECC [Fisher and Fisher, 2023, McGrath and
Mukherjee, 2022, Newey and Robins, 2018].
Remark 1. Weassumeweobserve3nobservationsintotalsowehavenobservationsforeachindependentfold.
When estimating the ECC with the DCDR estimator, we split the data into three folds: two for training and
one for estimation. Since our focus is on asymptotic rates, we ignore the constant factor lost from splitting the
data. But, with iid data, one can cycle the folds, repeat the estimation, and take the average to retain full
sample efficiency.
2.1 Assumptions and lower bounds on estimation rates
In this section, we impose two standard conditions on the data generating process. Then, we review the known
lower bounds for estimating the ECC under H¨older smoothness assumptions, although we do not invoke these
smoothness assumptions until Sections 4 and 5. We start with the two assumptions we impose throughout.
Assumption 1. (Bounded first and second moments for A and Y) The regression functions µ(X) and
π(X)satisfy|µ(X)|<∞,|π(X)|<∞,andtheconditionalsecondmomentsofAandY areboundedaboveand
below; i.e, 0<V(A|X =x),V(Y |X =x)<∞ for all x∈X.
We also assume the covariate density f(X) is upper and lower bounded and has bounded support X.
Assumption 2. (Bounded covariate density) The covariates X have support X, a compact subset of Rd,
and the covariate density f(x) satisfies c≤f(x)≤C for all x∈X and 0<c≤C <∞.
4We require no further assumptions until Section 4. In Sections 4 and 5, we analyze the DCDR estimator
when the data generating process satisfies π ∈ H¨older(α) and µ ∈ H¨older(β). In this regime, and when the
covariate density is sufficiently smooth, Robins et al. [2008] and Robins et al. [2009] proved that the minimax
rate satisfies

n−1/2 if α+β >d/4,
in ψ(cid:98)f Psu α,p βE|ψ(cid:98)−ψ ecc|≳
n− 2α2α ++ 2β2 +β d
other2
wise.
(1)
√
The minimax rate exhibits an “elbow” phenomenon, where semiparametric efficiency and n-convergence are
possible when the average smoothness of the nuisance functions is larger than d/4. Outside that regime, the
√
lower bound on the minimax rate is slower than n and depends on the average smoothness of the nuisance
functions and the dimension of the covariates. Importantly, these rates depend on the covariate density being
smooth enough that it does not affect the estimation rate; when the covariate density is non-smooth, minimax
rates for the ECC are not yet known.
2.2 Plug-in, doubly robust, and higher-order estimators
In this section, we describe plug-in, doubly robust, and higher-order estimators. Ultimately, we will focus on
doubly robust estimators due to their simplicity and popularity.
A plug-in estimator for the ECC can be constructed based on the representation
E{cov(A,Y |X)}=E(AY)−E{π(X)µ(X)}
or
E{cov(A,Y |X)}=E(cid:2) A{Y −µ(X)}(cid:3) .
Ineithercase, anestimatorcanbeconstructedaccordingtothe“pluginprinciple”, byplugginginestimatesfor
the relevant nuisance functions and taking the empirical average. These estimators are often intuitive and easy
toconstruct,buttheycaninheritbiasesfromtheirnuisancefunctionestimators. Thishasinspiredanextensive
literature on doubly robust estimators, which are also referred to as “first-order”, “double machine learning”,
or “one-step” estimators.
Doubly robust estimators are based on semiparametric efficiency theory and the efficient influence function
(EIF), which acts like a functional derivative in the first-order von Mises expansion of the functional [Tsiatis,
2006, van der Vaart and Wellner, 1996]. For the ECC, the un-centered EIF is
φ(Z)={A−π(X)}{Y −µ(X)}. (2)
The doubly robust estimator is constructed by estimating the nuisance functions, plugging their values into the
formula for the un-centered EIF, and taking the empirical average:
ψ(cid:98)dr =P n[{A−π (cid:98)(X)}{Y −µ (cid:98)(X)}].
Other doubly robust estimators such as the targeted maximum likelihood estimator are also common in the
literature [van der Laan and Rose, 2011]. They provide similar asymptotic guarantees as the doubly robust
estimator, and are often referred to as “doubly robust” when their bias can be bounded by the product of the
√
root mean squared errors of the nuisance function estimators. They can achieve n-convergence even when
their nuisance function estimators are estimated nonparametrically at slower rates. Furthermore, Balakrishnan
et al. [2023] recently showed that the doubly robust estimator is minimax optimal in a particular structure-
agnosticmodel. However,ifextrastructureisavailable,suchasH¨oldersmoothness,thenstandarddoublyrobust
estimators may not be minimax optimal. This has inspired a growing literature on higher-order estimators.
Higher-order estimators are based on a higher-order von Mises expansion of the functional of interest [Li
5et al., 2011, Robins et al., 2008]. Just as doubly robust estimators correct the bias of plug-in estimators,
higher-order estimators correct the bias of doubly robust estimators. For the ECC, the second-order estimator
is
1 (cid:88)
ψ(cid:98)hoif =ψ(cid:98)dr−
n(n−1)
{A i−π (cid:98)(X i)}b(X i)TΣ(cid:98)−1b(X j){Y
j
−µ (cid:98)(X j)}
i̸=j
where b(X) is a basis with dimension growing with sample size and Σ(cid:98) = P n{b(X)b(X)T} is the Gram matrix.
Higher-orderestimatorscapitalizeontheadditionalstructureavailablewhenthenuisancefunctionsaresmooth,
enabling them to achieve the minimax rate in some settings [Robins et al., 2008, 2009]. Recent research has
developed adaptive and more numerically stable extensions of higher-order estimators [Liu and Li, 2023, Liu
et al., 2021].
2.3 Doubly robust estimation and cross-fitting
In this section, we briefly review doubly robust estimation and cross-fitting and discuss the motivation behind
double cross-fitting. For a more comprehensive discussion, see Newey and Robins [2018].
Single cross-fit doubly robust (SCDR) estimators, which train the nuisance function estimators on a sep-
arate sample from which the functional is estimated, are now relatively well-known in the literature [Cher-
nozhukov et al., 2018, Robins et al., 2008, Zheng and van der Laan, 2010]. When estimating the ECC,
ψ(cid:98)scdr =P n(cid:2) {A−π (cid:98)(X)}{Y −µ (cid:98)(X)}(cid:3) ,withπ (cid:98)andµ (cid:98)trainedonanindependentdatasetfromthatusedtocalcu-
latethesampleaverage. StandardanalysisoftheSCDRestimatorshowsthatitsbiasscaleswiththeproductof
(cid:12) (cid:12)
rootmeansquarederrors(RMSE)ofthenuisancefunctionestimators;i.e.,(cid:12) (cid:12)E(ψ(cid:98)scdr−ψ ecc)(cid:12) (cid:12)≤∥µ (cid:98)−µ∥P∥π (cid:98)−π∥P.
ThisupperboundonthebiasisminimizedifbothnuisancefunctionsareestimatedoptimallyintermsofRMSE.
However, if the nuisance functions are H¨older smooth, the SCDR estimator which minimizes RMSE of its nui-
sance function estimators may not achieve the minimax rate.
Themotivationfordoublecross-fittingarisesfromakeyinsightintothesub-optimalityoftheSCDRestima-
tor. As discussed in Newey and Robins [2018], training the nuisance functions on the same dataset introduces
a dependence between the estimators, and so the bound on the bias of the SCDR estimator is minimized only
when both nuisance function estimators are estimated optimally in terms of RMSE. This intuition motivates
double cross-fitting, where the training data is split and the nuisance function estimators are trained on two
independent folds. Then, the nuisance function estimators are independent, and the bias of the DCDR esti-
mator only depends on the biases of the nuisance function estimators, rather than their RMSEs. And, since
the variance of the DCDR estimator will be diminished via averaging in the estimation fold, it is reasonable to
expect that the nuisance function estimators can be undersmoothed for faster bias convergence rates without
paying a price for the excess variance. We illustrate this phenomenon in subsequent sections when we study
the DCDR estimator with undersmoothed linear smoothers.
Inthenextsection,weformallyoutlinetheDCDRestimatorandderiveastructure-agnosticlinearexpansion
foritserror. InSections4and5,weincorporateprogressivelystrongerassumptionstoproveincreasinglypower-
ful results, including semiparametric efficiency under minimal conditions, minimax optimality, and asymptotic
√
normality in the non- n regime.
3 The DCDR estimator and a structure-agnostic linear expansion
In this section, we derive a structure-agnostic asymptotically linear expansion for the DCDR estimator which
holds with generic nuisance functions and estimators. To the best of our knowledge, this is the first such
structure-agnosticanalysisthatcanallowforimprovedrateswithundersmoothing. Then,weprovideanuisance-
function-agnostic decomposition of the remainder term from the asymptotically linear expansion. Finally, we
discuss, informally, how these results reveal that undersmoothing the nuisance function estimators can lead to
faster convergence rates for the DCDR estimator. First, we formally outline the DCDR estimator.
6Algorithm 1. (DCDR Estimator for the ECC) Let (D ,D ,D ) denote three independent samples of n
µ π φ
observations of Z =(X ,A ,Y ). Then:
i i i i
1. Train an estimator µ for µ on D and train an estimator π for π on D .
(cid:98) µ (cid:98) π
2. On D , estimate the un-centered efficient influence function values φ(Z)={A−π(X)}{Y −µ(X)} using
φ (cid:98) (cid:98) (cid:98)
the estimators from step 1, and construct the DCDR estimator ψ(cid:98)n as the empirical average of φ (cid:98)(Z) over
the estimation data D :
φ
1 (cid:88)
ψ(cid:98)n =P n{φ (cid:98)(Z)}≡
n
φ (cid:98)(Z i).
Zi∈Dφ
Our first result is a structure-agnostic asymptotically linear expansion of the DCDR estimator. It does not
require any assumptions about the nuisance functions or their estimators beyond Assumptions 1 and 2.
Lemma 1. (Structure-agnostic linear expansion) Under Assumptions 1 and 2, if ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1, then
ψ(cid:98)n−ψ
ecc
=(P n−P){φ(Z)}+R 1,n+R
2,n
(cid:32)(cid:114) (cid:33)
E∥φ−φ∥2 +ρ(Σ )
where R
1,n
≤∥b π∥P∥b µ∥P and R
2,n
=OP (cid:98) nP n ,
b ≡b (X)=E{η(X)−η(X)|X} is the pointwise bias of the estimator η, ρ(Σ ) denotes the spectral radius of
η η (cid:98) (cid:98) n
Σ , and
n
(cid:18) (cid:20)(cid:110) (cid:111)T (cid:21)(cid:19)
Σ
n
=E cov (cid:98)b φ(X 1),...,(cid:98)b φ(X n) |X φn
where (cid:98)b φ(X i) = E{φ (cid:98)(Z i)−φ(Z i) | X i,D π,D µ} is the conditional bias of φ
(cid:98)
and X φn denotes the covariates in
the estimation sample.
All proofs are delayed to the appendix. Here, we provide some intuition for the result. Crucially, the proof
of Lemma 1 analyzes the randomness of the DCDR estimator over both the estimation and training data. By
contrast, the analysis of the SCDR estimator is usually conducted conditionally on the training data. The
unconditional analysis of the DCDR estimator allows us to leverage the independence of the training samples,
thereby bounding the bias of the DCDR estimator by the product of integrated biases of the nuisance function
estimators. However,theunconditionalanalysisalsorequiresaccountingforthecovarianceoverthetrainingdata
between summands of the DCDR estimator because, without conditioning on the training data, the nuisance
function estimators are random, and φ(Z )(cid:26)⊥(cid:26)⊥φ(Z ) and cov {φ(Z ),φ(Z )}̸=0. These non-zero covariances
(cid:98) i (cid:98) j i̸=j (cid:98) i (cid:98) j
are accounted for by the new spectral radius term in the second remainder term, ρ(Σ ), which we analyze in
n
further detail in Proposition 1.
Lemma 1 is useful because of its generality, and we use it throughout the rest of the paper. Beyond
Assumptions 1 and 2, Lemma 1 requires no assumptions for the nuisance functions or their estimators. This
is in contrast to previous results, which focus on specific linear smoothers for the nuisance function estimators
[FisherandFisher,2023,Kennedy,2023,McGrathandMukherjee,2022,NeweyandRobins,2018]. InSection4,
we use Lemma 1 to analyze the DCDR estimator with linear smoothers. Before that, we analyze the spectral
radius term in Lemma 1 without assuming any structure on the nuisance functions or their estimators, but
leveraging the specific structure of the ECC.
Remark 2. McGrathandMukherjee[2022]improveduponthebiasterminLemma1usingspecialpropertiesof
waveletestimators,andthebiasoftheirestimatorscalesliketheminimumoftwobiasproducts. Wedemonstrate
that a similar phenomenon occurs for local polynomial regression in Section 5.
Proposition 1. (Spectral radius bound) Under Assumptions 1 and 2, if ψ is estimated with the DCDR
ecc
7estimator ψ(cid:98)n from Algorithm 1, then
ρ(Σ nn) ≤ E∥φ (cid:98) n−φ∥2 P +(cid:0) ∥b2 π∥ ∞+∥s2 π∥ ∞(cid:1)E(cid:104)(cid:12) (cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105)
+(cid:0) ∥b2 µ∥ ∞+∥s2 µ∥ ∞(cid:1)E(cid:104)(cid:12) (cid:12)cov{π (cid:98)(X i),π (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105)
where ∥b2∥ = sup E{η(X)−η(X) | X = x}2 and ∥s2∥ = sup V{η(X) | X = x} are uniform squared
η ∞ x∈X (cid:98) η ∞ x∈X (cid:98)
bias and variance bounds.
Here, we describe Proposition 1 in further detail. The first term on the right hand side comes from the
diagonal of Σ , and is equal to the variance terms already observed in Lemma 1. The second and third terms
n
come from the off-diagonal terms in Σ n. The expected absolute covariance,
E(cid:104)(cid:12)
(cid:12)cov{η (cid:98)(X i),η (cid:98)(X j) | X i,X
j}(cid:12) (cid:12)(cid:105)
,
measuresthecovarianceoverthetrainingdataofanestimator’spredictionsattwoindependent testpoints. For
manyestimators, weanticipatethatE(cid:104)(cid:12) (cid:12)cov{η (cid:98)(X i),η (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105) ≲n−1, andwedemonstratethistobethe
case for several linear smoothers subsequently.
Like Lemma 1, Proposition 1 is useful because of its generality: it applies to any nuisance functions and
nuisance function estimators. Although Proposition 1 relies specifically on the functional being the ECC, we
anticipate that similar results apply for other functionals.
FurtherinvestigationofProposition1revealswhenundersmoothingthenuisancefunctionestimatorswilllead
tothefastestconvergencerate. TheEIFoftheECC,likemanyfunctionals,isLipschitzintermsofitsnuisance
functions,soφ (cid:98)−φ≲|π (cid:98)−π|+|µ (cid:98)−µ|and∥φ (cid:98)−φ∥P ≲∥π (cid:98)−π∥P+∥µ (cid:98)−µ∥P. Moreover,thecompactnessofthesupport
ofX inAssumption2impliesthatthesupremummeansquarederrorsofthenuisancefunctionestimatorsscaleat
thetypicalpointwiserate. Therefore,iftheexpectedcovariancetermscalesinverselywithsamplesizesuchthat
E(cid:104)(cid:12) (cid:12)cov{η (cid:98)(X i),η (cid:98)(X j) | X i,X j}(cid:12) (cid:12)(cid:105) = OP(n−1), then ρ(Σ nn) = OP(cid:16)E∥φ(cid:98)− nφ∥2 P(cid:17) = OP(cid:16) ∥b2 π∥∞+∥s2 π∥∞+ n∥b2 µ∥∞+∥s2 µ∥∞(cid:17) ,
and so
(cid:32)(cid:114) (cid:33)
∥b2∥ +∥s2∥ +∥b2∥ +∥s2∥
R
2,n
=OP π ∞ π ∞
n
µ ∞ µ ∞ . (3)
Balancing R in (3) with the bias R in Lemma 1 requires constructing nuisance function estimators
2,n 1,n
such that ∥b ∥2∥b ∥2 ≍ ∥s2 π∥∞+∥s2 µ∥∞. A natural way to achieve such a balance is by undersmoothing both π
π P µ P n (cid:98)
and µ so their squared bias is smaller than their variance.
(cid:98)
In this section, we have demonstrated a structure-agnostic linear expansion for the DCDR estimator and
presented a nuisance-function-agnostic decomposition of its remainder term. Furthermore, we discussed how,
if the nuisance function estimators satisfy
E(cid:104)(cid:12)
(cid:12)cov{η (cid:98)(X i),η (cid:98)(X j) | X i,X
j}(cid:12) (cid:12)(cid:105)
= OP(n−1), then undersmoothing
the nuisance function estimators will minimize the remainder term. This is as much as we can say without
any assumptions on the nuisance functions or their estimators. In the next section, we assume the nuisance
functionsareH¨oldersmoothandconstructDCDRestimatorswithlocalaveraginglinearsmoothers,andweuse
Lemma 1 and Proposition 1 to demonstrate the DCDR estimator’s efficiency guarantees.
√
4 Semiparametric efficiency under minimal conditions and non- n
convergence
In this section, we assume the nuisance functions are H¨older smooth and construct DCDR estimators without
requiring knowledge of the smoothness or covariate density. When the nuisance functions are estimated with
local polynomial regression, we show the DCDR estimator is semiparametric efficient under minimal conditions
√
and,inthenon- nregime,convergesattheconjecturedminimaxratewithunknownandnon-smoothcovariate
density[Robinsetal.,2008]. Additionally,whenthenuisancefunctionsareestimatedwithk-NearestNeighbors,
we demonstrate that the DCDR estimator is semiparametric efficient when the nuisance functions are H¨older
8smooth of order at most one and are sufficiently smooth compared to the dimension of the covariates. First, we
formally state the H¨older smoothness assumptions for the nuisance functions.
Assumption 3. (H¨older smooth nuisance functions) The nuisance functions π and µ are H¨older smooth,
with π ∈H¨older(α) and µ∈H¨older(β).
We focus on local averaging estimators in this section, and next we review k-Nearest Neighbors and local
polynomial regression. In Appendix H, we review series regression, and establish results like those in this
sectionforregressionsplinesandwaveletestimators. Thoseresultsarealreadyknown[FisherandFisher,2023,
McGrath and Mukherjee, 2022, Newey and Robins, 2018], but we provide them for completeness and because
we use different proof techniques from those considered previously.
4.1 Nuisance function estimators
We define the estimators for µ using D . The estimators for π follow analogously with D , replacing Y by A.
µ π
Estimator 1. (k-Nearest Neighbors) The k-Nearest Neighbors estimator for µ(X)=E(Y |X) is
1 (cid:88) (cid:0) (cid:1)
µ(x)= 1 ∥X −x∥≤∥X (x)−x∥ Y , (4)
(cid:98) k i (k) i
Zi∈Dµ
where X (x) is the kth nearest neighbor of x in Xn.
(k) µ
The k-Nearest Neighbors estimator is simple. However, as we see subsequently, it is unable to adapt to
higher smoothness in the nuisance functions, as in nonparametric regression [Gy¨orfi et al., 2002].
Estimator 2. (Local polynomial regression) The local polynomial regression estimator for µ(X) = E(Y |
X) is
(cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27)
µ (cid:98)(x)= (cid:88) n1 hdb(0)TQ(cid:98)−1b X i h−x K X i h−x Y
i
(5)
Zi∈Dµ
where
Q(cid:98) =
1 (cid:88)
b(cid:18)
X
i−x(cid:19) K(cid:18)
X
i−x(cid:19) b(cid:18)
X
i−x(cid:19)T
,
nhd h h h
Xi∈X µn
b : Rd → Rp where p = (cid:0)d+⌈d/2⌉(cid:1) is a vector of orthogonal basis functions consisting of all powers of each
⌈d/2⌉
covariate up to order ⌈d/2⌉ and all interactions up to degree ⌈d/2⌉ polynomials (see, Masry [1996], Belloni
et al. [2015] Section 3), ⌈d/2⌉ denotes the smallest integer strictly larger than d/2, K : Rd → R is a bounded
kernel with support on [−1,1]d, and h is a bandwidth parameter. If the matrix Q(cid:98) is not invertible, µ (cid:98)(x)=0.
Local polynomial regression has been extensively studied [Fan and Gijbels, 2018, Masry, 1996, Ruppert
and Wand, 1994, Tsybakov, 2009]. There are two notable features to this version of the estimator. First, the
basis is expanded to order ⌈d/2⌉, the smallest integer strictly larger than d/2, rather than the smoothness
of the regression function. Therefore, the estimator does not require knowledge of the true smoothness, but
the expansion of the basis to degree ⌈d/2⌉ still ensures the bias of the DCDR estimator is oP(n−1/2) in the
√
n-regime. Second, the estimator is explicitly defined even when the local Gram matrix, Q(cid:98), is not invertible
— µ (cid:98)(x)=0. This ensures the bias of the estimator is bounded when Q(cid:98) is not invertible.
Unlike k-Nearest Neighbors, local polynomial regression can optimally estimate functions of higher smooth-
ness. In Appendix B, we provide bias and variance bounds for both estimators, which follow from standard
results in the relevant literature [Biau and Devroye, 2015, Gy¨orfi et al., 2002, Kennedy, 2023, Tsybakov, 2009].
However, two nuances arise in this analysis because the bias and variance bounds account for randomness over
thetrainingdata. First,thepointwisevariance,V{η(x)},scalesatthetypicalconditional(onthetrainingdata)
(cid:98)
mean squared error rate; e.g., for local polynomial regression, V{µ(x)} ≲ h−2β + 1 . It may be possible to
(cid:98) nhd
improve this with more careful analysis, but because this will not affect the behavior of the DCDR estimator
9— which uses undersmoothed nuisance function estimators — we leave this to future work. Second, for local
polynomial regression, the local Gram matrix Q(cid:98) may not be invertible. Therefore, it is necessary to show that
non-invertibility occurs with asymptotically negligible probability if the bandwidth h decreases slowly enough,
which is possible using a matrix Chernoff inequality (see, Tropp [2015] Section 5).
Next, we show the covariance terms from Proposition 1,
E(cid:104)(cid:12) (cid:12)cov(cid:8)
η (cid:98)(X i),η (cid:98)(X j) | X i,X
j(cid:9)(cid:12) (cid:12)(cid:105)
, can decrease
inverselywithsamplesizeforbothestimators,anddemonstratetheefficiencyguaranteesoftheDCDRestimator.
4.2 Semiparametric efficiency under minimal conditions
TheefficiencyoftheDCDRestimatordependsonhowquicklytheexpectedabsolutecovarianceE(cid:104)(cid:12)
(cid:12)cov{η (cid:98)(X i),η (cid:98)(X j)|
(cid:12)(cid:105)
X i,X j}(cid:12) decreases. Therefore,first,weshowthatthistermcandecreaseinverselywithsamplesizefork-Nearest
Neighbors and local polynomial regression.
Lemma2. (Covariancebound)SupposeAssumptions1,2,and3hold. Moreover,assumethateachestimator
balances squared bias and variance or is undersmoothed. Then, both k-Nearest Neighbors and local polynomial
regression satisfy
E(cid:104)(cid:12)
(cid:12)cov{η (cid:98)(X i),η (cid:98)(X j)|X i,X
j}(cid:12) (cid:12)(cid:105) =OP(cid:18) n1(cid:19)
(6)
for η ∈{π,µ}.
Lemma 2 demonstrates that the expected absolute covariance can decrease inversely with sample size for
bothk-NearestNeighborsandlocalpolynomialregression. Theresultfollowsfromalocalizationargument—if
the estimation points X and X are well separated, then η(X ) and η(X ) share no training data and therefore
i j (cid:98) i (cid:98) j
their covariance is zero; otherwise, the covariance is upper bounded by the variance. Lemma 2 guarantees that
theexpectedabsolutecovariancedecreasesinverselywithsamplesizeiftheestimatorsbalancesquaredbiasand
varianceorareundersmoothed. Itmaybepossibletoimprovethisresultsothatitalsoappliestooversmoothed
estimators, but because we focus only on undersmoothed nuisance function estimators subsequently, we leave
that to future work.
ThefollowingresultestablishesthattheDCDRestimatorachievessemiparametricefficiencyunderminimal
√
conditions and fast convergence rates in the non- n regime.
Theorem 1. (Semiparametric efficiency) Suppose Assumptions 1, 2, and 3 hold, and ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1.
If the nuisance functions µ and π are estimated with local polynomial regression (Estimator 2) with band-
(cid:98) (cid:98)
(cid:16) (cid:17)−1/d
widths satisfying h ,h ≍ n , then
µ π logn
(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4, and
(7)
E|ψ(cid:98)n−ψ ecc|=OP(cid:16)
lon
gn(cid:17)−α+ dβ
otherwise.
Ifthenuisancefunctionsµandπ areestimatedwithk-NearestNeighbors(Estimator1)andk ,k ≍logn,
(cid:98) (cid:98) µ π
then
(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4 and α,β ≤1, and
(8)
E|ψ(cid:98)n−ψ ecc|≲(cid:16)
lon
gn(cid:17)−(α∧1)+ d(β∧1)
otherwise.
Theorem 1 shows that the DCDR estimator with undersmoothed local polynomial regression is semipara-
metric efficient under minimal conditions. Further, it attains (up to a log factor) the convergence rate n−α+ dβ
√
in probability in the non- n regime. This is slower than the known lower bound for estimating the ECC when
10the covariate density is appropriately smooth, but has been conjectured to be the minimax rate when the co-
variate density is non-smooth [Robins et al., 2009]. A similar but weaker result holds for k-nearest Neighbors
estimators, whereby the DCDR estimator achieves semiparametric efficiency when the nuisance functions are
H¨oldersmoothoforderatmostonebutaresufficientlysmoothcomparedtothedimensionofthecovariates. A
simple example is if the nuisance functions are Lipschitz (i.e., α = β = 1) and the dimension of the covariates
is less than four (d<4).
The DCDR estimator based on local polynomial regression in Theorem 1 is not minimax optimal because
thebandwidthisconstrainedsothatthelocalGrammatrixisinvertiblewithhighprobability, therebylimiting
the convergence rate of the bias of the local polynomial regression estimators and, by extension, the bias of
the DCDR estimator. By replacing the Gram matrix with its expectation (assuming it is known), an estimator
could be undersmoothed even further for a faster bias convergence rate. In the next section we propose such
an estimator — the “covariate-density-adapted” kernel regression. We illustrate that the DCDR estimator
with covariate-density-adapted kernel regression can be minimax optimal. Moreover, we establish asymptotic
√
normality in the non- n regime by undersmoothing the DCDR estimator so its variance dominates its squared
√
bias, but it converges to a normal limiting distribution around the ECC at a slower-than- n rate.
Remark 3. When the DCDR estimator achieves semiparametric efficiency, Slutsky’s theorem and Theorem 1
imply that inference can be conducted for the ECC with Wald-type 1−α confidence intervals, ψ(cid:98)n±Φ−1(1−
(cid:113)
α/2) V(cid:98){φ n(Z)}, where V (cid:98){φ(Z)} is any consistent estimator for V{φ(Z)} (e.g., the sample variance of φ (cid:98)(Z)).
(cid:16) (cid:17)−1/d
Remark 4. There are simple ad-hoc guidelines for scaling the bandwidth like n for local polynomial
logn
regression. For example, one could choose the smallest bandwidth for which the estimator is defined at every
point in the estimation sample.
√
5 Minimax optimality and asymptotic normality in the non- n regime
In this section, we assume the covariate density is known and examine the behavior of the DCDR estimator
with covariate-density-adapted kernel regression estimators for the nuisance functions. For the results in this
section, we require, in addition to previous assumptions, that the covariate density is known and sufficiently
smooth.
Assumption 4. (Known, lower bounded, and smooth covariate density) The covariate density f is
known and f ∈H¨older(γ), where γ ≥α∨β.
Under Assumption 4, we demonstrate the DCDR estimator is minimax optimal. First, we define the
covariate-density-adapted kernel regression estimator:
Estimator 3. (Covariate-density-adapted kernel regression) The covariate-density-adapted kernel re-
gression estimator for µ(X)=E(Y |X) is
(cid:16) (cid:17)
µ(x)=
(cid:88)
K
µ
X hi− µx
Y , (9)
(cid:98) nhdf(X ) i
Zi∈Dµ µ i
where h is the bandwidth and K is a kernel (to be chosen subsequently). The estimator for π(X)=E(A|X)
µ µ
is defined analogously on D .
π
This estimator uses the known covariate density in the denominator of (9). As a result, no constraint on
the bandwidth is required, and the estimator can be undersmoothed more than the local polynomial regression
estimator in Estimator 2. McGrath and Mukherjee [2022] proposed a similar adaptation of the standard
wavelet estimator. As they showed for the wavelet estimator, the known covariate density in Estimator 3 could
be replaced by the estimated covariate density, and our subsequent results would follow if the covariate density
were sufficiently smooth (smoother than in Assumption 4) and its estimator sufficiently accurate. Because the
11properties of such an estimator are not well understood when the covariate density is not sufficiently smooth,
we leave this analysis to future work.
Thesubsequentanalysiscombinestwoversionsofcovariate-density-adaptedkernelregression,withdifferent
kernels.
Estimator3a. (Higher-ordercovariate-density-adaptedkernelregression)Thehigher-ordercovariate-
density-adapted kernel regression has symmetric and bounded kernel K that is of order ⌈α+β⌉ and satisfies
K(x/h)≲1(∥x∥≤h), (cid:82) K(x)dx=1, (cid:82) K(x)2dx≍1, and (cid:82) ∥x∥α+βK(x)dx≲1 [Gy¨orfi et al., 2002, Tsybakov,
2009].
This version of the estimator uses a higher-order localized kernel, which allows it to adapt to the sum
of the smoothnesses of the nuisance functions. See, e.g., Section 5.3, Gy¨orfi et al. [2002] and Section 1.2.2,
Tsybakov [2009] for a review of higher-order kernels and how to construct bounded kernels of arbitrary order.
To complement this estimator, we require a smooth estimator.
Estimator 3b. (Smooth covariate-density-adapted kernel regression) The smooth covariate-density-
adaptedkernelregressionhascontinuousandboundedkernelK
satisfyingK(x/h)≲1(∥x∥≤h),(cid:82)
K(x)dx=1,
(cid:82) K(x)2dx≍1.
Because the kernel in the smooth estimator is localized and continuous, it allows the DCDR estimator to
adapt to the sum of smoothnesses of the nuisance functions through the higher-order kernel estimator. For this
purpose, the smooth kernel must be continuous, but need not control higher-order bias terms. Therefore, a
simple kernel is adequate, such as the Epanechnikov kernel — K(x)=
3(cid:0) 1−∥x∥2(cid:1)
1(∥x∥≤1).
4
5.1 Minimax optimality
The following result shows that the DCDR estimator using covariate-density-adapted kernel regression estima-
tors is minimax optimal.
Theorem 2. (Minimax optimality) Suppose Assumptions 1, 2, 3, and 4 hold. If ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1, one nuisance function is estimated with the smooth covariate-
density-adapted kernel regression (Estimator 3b) with bandwidth decreasing at any rate such that the estimator
isconsistent, andtheothernuisancefunctionisestimatedwiththehigher-ordercovariate-density-adaptedkernel
−2
regression (Estimator 3a) with bandwidth that scales at n2α+2β+d, then

(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4,
(10)
(cid:16) (cid:17)
E|ψ(cid:98)n−ψ ecc|=OP n− 2α2α ++ 2β2 +β d otherwise.
Theorem2establishesthattheDCDRestimatorwithcovariate-density-adaptedkernelregressionestimators
√
is semiparametric efficient under minimal conditions and minimax optimal in the non- n regime. The result
reliesonknowledgeofthesmoothnessofthenuisancefunctions, aswellasshrinkingoneofthetwobandwidths
faster than n−1/d. The proof relies on the smoothing properties of convolutions and an adaptation of Theorem
1 from Gin´e and Nickl [2008a], as well as results from Gin´e and Nickl [2008b] and Chapter 4 of Gin´e and
Nickl [2021]. While Theorem 2 is the first result applied to local averaging estimators such as kernel regression,
McGrath and Mukherjee [2022] proved the same result using approximate wavelet kernel projection estimators
for the nuisance functions. Their result relies on the orthogonality (in expectation) of the wavelet estimator’s
predictions and residuals.
√
Remark 5. To guarantee asymptotic normality in the n-regime, it is necessary that the smooth covariate-
density-adapted estimator is consistent. If one were only interested in convergence rates, as in McGrath and
Mukherjee [2022], one could replace the smooth estimator by any smooth estimator with bounded variance.
Indeed, supposing without loss of generality that µ were the higher-order kernel estimator, one could set π =0
(cid:98) (cid:98)
and use the plug-in estimator for the ECC instead of the DCDR estimator.
12√
5.2 Slower-than- n CLT
√
Inadditiontominimaxoptimality,asymptoticnormalityispossibleinthenon- nregime. TheDCDRestimator
in Theorem 2 balances bias and variance; intuitively, if the DCDR estimator were undersmoothed one might
√
expect it to converge to a Normal distribution centered at the ECC at a sub-optimal slower-than- n rate. We
demonstrate this in the next result. First, we incorporate two further assumptions.
Assumption 5. (Boundedness) There exists M >0 such that |A|<M and |Y|<M.
Assumption 6. (Continuous conditional variance) V(A|X =x) and V(Y |X =x) are continuous in x.
Assumption 5 asserts that A and Y are bounded. Assumption 6 dictates that the conditional variances of
A and Y are continuous in X, which is used to show that the limit of the standardizing variance in (12) exists.
It may be possible to relax these assumptions with more careful analysis. Nonetheless, with them it is possible
to establish the following result.
√
Theorem 3. (Slower-than- n CLT) Under the conditions of Theorem 2, suppose α+β < d and Assump-
2 4
tions 5 and 6 hold. Suppose µ is the undersmoothed nuisance function estimator with bandwidth h scaling at
(cid:98) µ
n− 2α+2+ 2βε +d for 0<ε< 4(α d+β) while π (cid:98) is the smooth consistent estimator. Then,
(cid:114) n
V{φ(Z)|D ,D
}(ψ(cid:98)n−ψ ecc)⇝N(0,1). (11)
(cid:98) π µ
Moreover,
(cid:26)V(A|X)Y2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|D ,D }−a. →s. E E µ , (12)
µ (cid:98) π µ f(X) f(X)
where K is the kernel for µ. If the roles of µ and π were reversed, then (11) holds and
µ (cid:98) (cid:98) (cid:98)
(cid:26)V(Y |X)A2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|D ,D }−a. →s. E E π . (13)
π (cid:98) π µ f(X) f(X)
√
Theorem 3 shows that the DCDR estimator can be suitably undersmoothed in the non- n regime so the
DCDRestimatorissub-optimalbutconvergestoaNormaldistributionaroundtheECC.Moreover, Theorem3
establishesthattheconditionalvariancebywhichtheerrorisstandardizedconvergesalmostsurelytoaconstant
whichcanbeestimatedfromthedata. Therefore,Wald-typeconfidenceintervalsfortheECCcanbeconstructed
√
using(11)and(12)or(13). Asfarasweareaware,thisisthefirstresultdemonstratingslower-than- ninference
for a cross-fit estimator of a causal functional.
Here,wegivesomeintuitionfortheresult,whichmightbestbeunderstoodthroughitsunorthodoxdenomi-
nator in the standardization term in (11): the conditional variance of the estimated efficient influence function.
This denominator is unorthodox both because it includes an estimated efficient influence function and because
it is a conditional variance. The estimated efficient influence function arises because ψ(cid:98)n is undersmoothed to
(cid:16)√ (cid:17)
such an extent that its scaled variance, V nψ(cid:98)n , is growing with sample size. Similarly, V{φ (cid:98)(Z) | D π,D µ}
is also growing at the same rate with sample size, and thus standardizing by this term appropriately concen-
(cid:113)
trates the variance of the standardized statistic, V{φ(cid:98)(Z)n |Dπ,Dµ}(ψ(cid:98)n −ψ ecc). Indeed, (12) demonstrates that
V{φ(Z)|D ,D }isgrowingwithsamplesizebecausenhd →0asn→∞bytheassumptiononthebandwidth.
(cid:98) π µ µ
This result relies on a bound for higher moments of a U-statistic (Proposition 2.1, Gin´e et al. [2000]) which
guarantees control of the sum of off-diagonal terms in V{φ(Z)|D ,D }.
(cid:98) π µ
Meanwhile,theconditional varianceisrequiredsothatanormallimitingdistributioncanbeattained. While
√
thenon- nregimeisoftencharacterizedbynon-normallimitingdistributions,anormallimitingdistributioncan
beestablishedapplyingtheBerry-Esseeninequality(Theorem1.1,BentkusandG¨otze[1996])afterconditioning
on the training data and showing that the standardized statistic satisfies a conditional central limit theorem
almost surely and, therefore, an unconditional central limit theorem.
13This approach — using sample splitting to conduct inference — is an old method which has recently been
examined in several contexts, including, for example, estimating U-statistics [Kim and Ramdas, 2024, Robins
et al., 2016], estimating variable importance measures [Rinaldo et al., 2019], high-dimensional model selection
[Wasserman and Roeder, 2009], and post-selection inference [Dezeure et al., 2015, Meinshausen and Bu¨hlmann,
2010]. Earlier references include Cox [1975], Hartigan [1969], and Moran [1973].
WhilethissectionandprevioussectionshaveestablishedseveraltheoreticalresultsfortheDCDRestimator,
in the next section we investigate and illustrate these properties via simulation.
6 Simulations
In this section, we study the performance of double cross-fit doubly robust (DCDR) estimators compared to
single cross-fit doubly robust (SCDR) estimators and the limiting distributions of both estimators. First, we
provide evidence for why undersmoothing will be optimal with the double cross-fit estimator by showing that
the double cross-fit estimator requires undersmoothed nuisance function estimators to achieve the lowest error.
Then,weconstructH¨oldersmoothnuisancefunctionsandexaminewhenthedistributionofstandardizedSCDR
and DCDR estimates converge to standard Gaussians, and the coverage and power of Wald-style confidence
intervals. Our simulations demonstrate that the undersmoothed DCDR estimator, presented in Theorem 3,
√
enables inference via a central limit theorem in the non- n regime. Further, the DCDR estimator utilizing
local polynomial regression without knowledge of the covariate density, as described in Theorem 1, achieves
semiparametricefficiencyunderminimalconditions. Additionally, weobservethatwhenthenuisancefunctions
exhibit sufficient roughness, the SCDR estimator fails to support inference, whereas the DCDR estimators
continue to do so.
All code and analysis is available at https://github.com/alecmcclean/DCDR
6.1 Intuition for undersmoothing
First,wereinforceourunderstandingforwhydoublecross-fittingleadstoundersmoothingthenuisancefunction
estimators. We consider the data generating process where X is uniform, A=Y, and both nuisance functions
are the Doppler function (shown in Figure 1). Because A=Y, the ECC is the variance of the error noise in A
and Y. Formally, the data generating process is
X ∼Unif(0,1), (14)
(cid:18) (cid:19)
(cid:112) 2.1π
π(X)=µ(X)= X(1−X)sin , (15)
X+0.05
A=Y =π(X)+ε,ε∼N(0,ψ =0.1). (16)
ecc
We chose ψ = 0.1 to give a strong signal to noise ratio for the estimators. The plot of Y against X is
ecc
shown in Figure 1.
Wegenerated500datasetswiththreefoldsofsizes{50,100,200,500,1000,2000}andestimatedeachnuisance
function with k-Nearest Neighbors for k from 1 to 30. We estimated the ECC with the DCDR estimator and
theSCDRestimator; fortheSCDRestimatorwetrainedthenuisancefunctionsonthesamefoldanddiscarded
the unused third fold (see Remark 6). For each k, we computed the average mean squared error (MSE) of the
nuisance function estimators and the DCDR and SCDR estimators over 500 datasets.
To understand when undersmoothing is optimal, we calculated the optimal k corresponding to the lowest
average MSE over 500 datasets for the DCDR, SCDR, and nuisance function estimators. Figure 2 displays the
optimal number of neighbors (y-axis) for each fold size (x-axis), with different colors denoting estimator/esti-
mand combinations. For instance, the green point in the bottom left corner signifies that k =2 gave the lowest
average MSE over 500 repetitions for the DCDR estimator estimating the ECC with datasets with folds of size
14Figure 1: The Doppler function with N(0,0.1) random noise as in (15); this nuisance function was used for
Figure 2.
50. The black points and line represent the optimal k for π estimating π, orange represent µ estimating µ,
(cid:98) (cid:98)
blue represent the SCDR estimator estimating the ECC, and green represents the DCDR estimator estimating
the ECC (blue, orange, and black are the same line for the most part, so the blue line completely obscures
the orange and partially obscures the black). Figure 2 demonstrates the anticipated phenomenon: the optimal
number of neighbors is lower for the DCDR estimator compared to the SCDR estimator and the nuisance
function estimators, and it increases at a slower rate as sample size increases. Equivalently, the optimal k for
the DCDR estimator corresponds to undersmoothed nuisance function estimators while the optimal k for the
SCDR estimator corresponds to optimal nuisance function estimators.
Remark 6. Figure 2 does not describe whether the SCDR estimator or DCDR estimator is more accurate, nor
is that the goal of this analysis. Because we discarded a third of the data available to the SCDR estimator, it
is not possible to compare the estimators directly. Instead, Figure 2 shows that the DCDR estimator requires
undersmoothed nuisance function estimators for optimal accuracy, while the SCDR estimator requires optimal
nuisance function estimators.
6.2 DCDR and SCDR estimators with Ho¨lder smooth nuisance functions
In this section, we demonstrate the improved efficiency and inference possible with the DCDR estimator com-
pared to the SCDR estimator. When the nuisance functions are H¨older smooth, the DCDR estimator with
local polynomial regression can be semiparametric efficient under minimal conditions without knowledge of
the covariate density or the smoothness of the nuisance functions, as in Theorem 1. Meanwhile, the SCDR
estimator can only achieve semiparametric efficiency when the average smoothness is greater than half the
dimension. Furthermore, we illustrate that the undersmoothed DCDR estimator can achieve inference at all
√
non- n smoothness levels when the covariate density and smoothness are known, as in Theorem 3.
To facilitate our analysis, we constructed suitably smooth nuisance functions. Specifically, we consider both
1-dimensional and 4-dimensional covariates uniform on the unit cube, ψ = 10, and π and µ H¨older smooth.
ecc
Throughout, we set both nuisance functions π and µ to be of the same smoothness such that α = β = s,
and we control the smoothness s. To construct appropriately smooth functions, we employed the lower bound
minimax construction for regression (see, Tsybakov [2009], pg. 92). These functions vary with sample size,
and Figure 3 provides an illustration for d = 1, with smoothness levels s ∈ {0.1,0.35,0.6} and dataset sizes
N ∈ {100,1000,5000}. To generate 4-dimensional H¨older smooth functions, we added four functions that are
univariate H¨older smooth in each dimension.
15Figure 2: Fold size (x-axis) versus optimal number of neighbors (y-axis), where optimal is in terms of average
MSE over 500 datasets; black and orange indicate the k-Nearest Neighbors estimators for π(X) and µ(X),
respectively,whileblueindicatestheSCDRestimatorfortheECCandblacktheDCDRestimatorfortheECC.
We generated datasets for fold sizes {100,200,500,1000,2000,5000}. When d=1, we constructed nuisance
functionswithsmoothnesses{0.1,0.35,0.6},andwhend=4withsmoothnesses{0.6,1.5,2.5}. Thefirstsmooth-
√ √
ness level corresponds to the non- n regime, the second smoothness level to the n regime where the SCDR
√ √
estimator fails to achieve n efficiency, and the final level where both estimators are n efficient. For each
fold size-dimension-smoothness combination, we generated 100 datasets and calculated the DCDR estimator
and SCDR estimator with covariate-density-adapted kernel regressions (Estimator 3). For only d = 1, we also
constructed the DCDR estimator with local polynomial regression (Estimator 2).
Figures 4 illustrates our results. Figure 4a shows QQ Plots for the standardized statistics for different
smoothnesses (rows) and fold sizes (columns) for dimension equal to one. The black dots represent the un-
dersmoothed DCDR estimators based on covariate-density-adapted kernel regression where the density and
smoothnesses are known, while the orange dots represent the estimator based on local polynomial regression
where the covariate density and smoothness are unknown. The blue dots represent the SCDR estimator based
onoptimalcovariate-density-adaptedkernelregressionsthatusetheknowncovariatedensityandsmoothnesses,
which have MSE scaling at the optimal rate. The diagonal line is y = x. Figure 4b displays the coverage and
poweroftheassociatedWald-typeconfidenceintervals, withthedimensionandsmoothnessvaryingbycolumn,
and the sample size on the x-axis.
√
The results in Figure 4 confirm that non- n inference is possible, as in Theorem 3. As the sample size
increases(movingacrossthepanelsinFigure4a),thequantilesoftheundersmoothedDCDRestimatesinblack
converge to the quantiles of the standard normal distribution. Additionally, as sample size increases (moving
across the x-axis in Figure 4b), the coverage of the confidence intervals approach appropriate coverage. These
findings align with what was anticipated by the limiting distribution result in Theorem 3. This occurs even
when s<d/4.
Figure 4 also confirms that the DCDR estimator facilitates inference when the SCDR estimator does not.
Theorem 1 dictates that the DCDR estimator with local polynomial regression is semiparametric efficient and
asymptotically normal when d/4<s<d/2 (the middle row). This is demonstrated in Figure 4: in Figure 4a,
when s > d/4, the quantiles of the unknown density DCDR estimator with local polynomial regression, as in
Theorem 1, converge to the quantiles of the standard normal. However, the quantiles diverge when s<d/4, as
shownbytheorangedotsinthetoprow. Similarly,inFigure4b,theconfidenceintervalsachievetheappropriate
95% coverage when s>d/4, and fail otherwise. For the SCDR estimator with asymptotically optimal nuisance
16Figure 3: Example Holder smooth functions (black) of order s ∈ {0.1,0.35,0.6} smoothness for n ∈
{100,1000,5000} observed data points (red) with N(0,10) random noise.
function estimators, Figure 4 illustrates the analogous phenomenon around the s = d/2 threshold. When
s > d/2, the SCDR quantiles in the bottom row of Figure 4a converge closely to the normal quantiles, and
do not converge otherwise. The same phenomenon occurs for the confidence intervals, which do not achieve
appropriate coverage when s < d/2. In summary, these results support the theoretical conclusion that the
DCDR estimators are semiparametric efficient and asymptotically normal in sufficiently non-smooth regimes
(d/4<s<d/2) where the SCDR estimator is not.
7 Discussion
In this paper, we studied a double cross-fit doubly robust (DCDR) estimator for the Expected Conditional
Covariance(ECC).Weanalyzedtheestimatorwithprogressivelystrongerassumptionsandprovedincreasingly
powerful results. We first derived a structure-agnostic error analysis for the DCDR estimator, which holds for
generic data generating processes and nuisance function estimators. We observed that a faster convergence
rate is possible by undersmoothing the nuisance function estimators, provided that these estimators satisfy a
covariancecondition. Weestablishedthatseverallinearsmootherssatisfythiscovariancecondition,andfocused
on the DCDR estimator with local averaging estimators for the nuisance functions, which had not been studied
previously. We showed that the DCDR estimator based on undermoothed local polynomial regression is semi-
parametric efficient under minimal conditions without knowledge of the covariate density or the smoothness
of the nuisance functions. When the covariate density is known, we demonstrated that the DCDR estima-
tor based on undersmoothed covariate-density-adapted kernel regression is minimax optimal. Moreover, we
√
proved an undersmoothed DCDR estimator satisfies a slower-than- n central limit theorem. Finally, we con-
ducted simulations that support our findings, providing intuition for double cross-fitting and undersmoothing,
demonstrating where the DCDR estimator is semiparametric efficient while the usual “single cross-fit” doubly
robustestimatorisnot,andillustratingslower-than-root-nasymptoticnormalityfortheundersmoothedDCDR
√
estimator in the non- n regime.
17(a) QQ Plots for the standardized statistics for different dimensions and smoothnesses (columns) and fold sizes (rows).
BlackdotsrepresenttheundersmoothedDCDRestimatorfromTheorem3withcovariate-density-adaptedkernelregres-
sion (Estimator 3), orange dots represent the DCDR estimator from Theorem 1 based on local polynomial regression
(Estimator 2), and blue dots represent the SCDR estimator based on covariate-density-adapted kernel regression (Esti-
mator 3) with error scaling optimally asymptotically. The diagonal line is y=x.
(b) Points represent the coverage (top rows) and power (bottom row) of 95% confidence intervals over 100 datasets
constructed for different dimensions and smoothnesses (columns) and fold sizes (x-axis). Error bars represent 95%
confidence intervals for the coverage and power of Wald-type confidence intervals. Black represents the undersmoothed
DCDR estimator from Theorem 3 with covariate-density-adapted kernel regression (Estimator 3), orange represents
the DCDR estimator from Theorem 1 based on local polynomial regression (Estimator 2), and blue represents the
SCDR estimator based on covariate-density-adapted kernel regression (Estimator 3) with error scaling at optimally
asymptotically.
Figure 4: Illustrating the efficiency and inferential properties of double cross-fit versus single cross-fit doubly
robust estimators.
18There are several potential extensions of our work. While we focus on the ECC, the principles applied here
generalize. NeweyandRobins[2018]derivedgeneralresultsfortheclassof“averagelinearfunctionals”(Newey
and Robins [2018], Section 3), and similarly general results might be possible for the larger class of “mixed
bias functionals” [Rotnitzky et al., 2021]. Furthermore, DCDR estimators could be used to estimate even more
complexcausalinferencefunctionals, suchasthosebasedonstochasticinterventions, instrumentalvariables, or
sensitivityanalyses. Achievingthiswouldentaildevelopingprincipledapproachesforundersmoothingestimators
of non-standard nuisance functions.
Moreover, even within the context of estimating the ECC there are still unresolved questions. When the
covariate density is unknown and non-smooth, the minimax lower bound is yet unknown. Once a compre-
hensive understanding of the lower bound across all H¨older smoothness classes is obtained, a natural question
arises regarding the feasibility of constructing adaptive and optimally efficient estimators across all smoothness
classes. Finally,similarquestionsregardingefficiencyandinferencecouldbeexploredunderdifferentstructural
assumptionsforthedatageneratingprocess. Forinstance,onecouldconsidernuisancefunctionsthataresparse
or have bounded variation norm and investigate the corresponding estimators.
Acknowledgments
The authors thank Zach Branson, the CMU causal inference reading group, and participants at ACIC 2023 for
helpful comments and feedback.
References
Sivaraman Balakrishnan, Edward H Kennedy, and Larry Wasserman. The fundamental limits of structure-
agnostic functional estimation. arXiv preprint arXiv:2305.04116, 2023.
Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Some new asymptotic theory
for least squares series: Pointwise and uniform results. Journal of Econometrics, 186(2):345–366, 2015.
Vidmantas Bentkus and Friedrich G¨otze. The berry-esseen bound for student’s statistic. The Annals of Proba-
bility, 24(1):491–503, 1996.
G´erard Biau and Luc Devroye. Lectures on the nearest neighbor method. Cham: Springer, 2015.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and
JamesRobins. Double/debiasedmachinelearningfortreatmentandstructuralparameters. TheEconometrics
Journal, 21(1):C1–C68, 2018.
DavidRCox. Anoteondata-splittingfortheevaluationofsignificancelevels. Biometrika,62(2):441–444,1975.
Sanjoy Dasgupta and Samory Kpotufe. Nearest Neighbor Classification and Search, chapter 18, pages 403–423.
Cambridge University Press, Cambridge, 2021.
V´ıctor H de la Pen˜a, Evarist Gin´e, V´ıctor H de la Pen˜a, and Evarist Gin´e. Decoupling of u-statistics and
u-processes. Decoupling: From Dependence to Independence, pages 97–152, 1999.
Ruben Dezeure, Peter Bu¨hlmann, Lukas Meier, and Nicolai Meinshausen. High-dimensional inference: confi-
dence intervals, p-values and r-software hdi. Statistical science, pages 533–558, 2015.
Iv´an D´ıaz. Non-agency interventions for causal mediation in the presence of intermediate confounding. arXiv
preprint arXiv:2205.08000, 2023.
Rick Durrett. Probability: theory and examples. Cambridge university press, Cambridge, UK; New York, NY,
2019.
19Jianqing Fan and Irene Gijbels. Local polynomial modelling and its applications. Routledge, New York, NY,
2018.
Aaron Fisher and Virginia Fisher. Three-way cross-fitting and pseudo-outcome regression for estimation of
conditional effects and other linear functionals. arXiv preprint arXiv:2306.07230, 2023.
Evarist Gin´e and Richard Nickl. A simple adaptive estimator of the integrated square of a density. Bernoulli,
14(1), 2008a.
EvaristGin´eandRichardNickl. Uniformcentrallimittheoremsforkerneldensityestimators. ProbabilityTheory
and Related Fields, 141(3-4):333–387, 2008b.
EvaristGin´eandRichardNickl. Mathematicalfoundationsofinfinite-dimensionalstatisticalmodels. Cambridge
university press, Cambridge, UK, 2021.
Evarist Gin´e, Rafal(cid:32) Latal(cid:32)a, and Joel Zinn. Exponential and moment inequalities for u-statistics. In High
Dimensional Probability II, pages 13–38. Springer, Boston, MA, 2000.
L´aszl´o Gy¨orfi, Michael Kohler, Adam Krzyzak, Harro Walk, et al. A distribution-free theory of nonparametric
regression, volume 1. New York: Springer, 2002.
Bruce E. Hansen. Econometrics. Princeton University Press, Princeton, NJ, 2022.
John A Hartigan. Using subsample values as typical values. Journal of the American Statistical Association,
64(328):1303–1317, 1969.
EdwardHKennedy. Semiparametricdoublyrobusttargeteddoublemachinelearning: areview. arXiv preprint
arXiv:2203.06469, 2022.
Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic
Journal of Statistics, 17(2):3008–3049, 2023.
Ilmun Kim and Aaditya Ramdas. Dimension-agnostic inference using cross u-statistics. Bernoulli, 30(1):683–
711, 2024.
Lingling Li, Eric Tchetgen Tchetgen, Aad van der Vaart, and James M. Robins. Higher order inference on a
treatment effect under low regularity conditions. Statistics & Probability Letters, 81(7):821–828, 2011.
√
LinLiuandChangLi. New n-consistent,numericallystablehigher-orderinfluencefunctionestimators. arXiv
preprint arXiv:2302.08097, 2023.
Lin Liu, Rajarshi Mukherjee, and James M Robins. On Nearly Assumption-Free Tests of Nominal Confidence
Interval Coverage for Causal Parameters Estimated by Machine Learning. Statistical Science, 35(3):518–539,
2020.
Lin Liu, Rajarshi Mukherjee, James M Robins, and Eric Tchetgen Tchetgen. Adaptive estimation of nonpara-
metric functionals. The Journal of Machine Learning Research, 22(1):4507–4572, 2021.
Elias Masry. Multivariate regression estimation local polynomial fitting for time series. Stochastic Processes
and their Applications, 65(1):81–101, 1996.
Alec McClean, Zach Branson, and Edward H Kennedy. Nonparametric estimation of conditional incremental
effects. arXiv preprint arXiv:2212.03578, 2022.
Sean McGrath and Rajarshi Mukherjee. On undersmoothing and sample splitting for estimating a doubly
robust functional. arXiv preprint arXiv:2212.14857, 2022.
20Nicolai Meinshausen and Peter Bu¨hlmann. Stability selection. Journal of the Royal Statistical Society Series
B: Statistical Methodology, 72(4):417–473, 2010.
Patrick AP Moran. Dividing a sample into two parts a statistical dilemma. Sankhy¯a: The Indian Journal of
Statistics, Series A, pages 329–333, 1973.
Whitney K Newey and James R Robins. Cross-fitting and fast remainder rates for semiparametric estimation.
arXiv preprint arXiv:1801.09138, 2018.
WhitneyKNewey,FushingHsieh,andJamesRobins.Undersmoothingandbiascorrectedfunctionalestimation.
1998.
Liam Paninski and Masanao Yajima. Undersmoothed kernel entropy estimators. IEEE Transactions on Infor-
mation Theory, 54(9):4384–4388, 2008.
Alessandro Rinaldo, Larry Wasserman, and Max G’Sell. Bootstrapping and sample splitting for high-
dimensional, assumption-lean inference. The Annals of Statistics, 47(6):3438–3469, 2019.
James Robins, Lingling Li, Eric Tchetgen, and Aad van der Vaart. Higher order influence functions and
minimax estimation of nonlinear functionals. In Institute of Mathematical Statistics Collections, pages 335–
421. Institute of Mathematical Statistics, 2008.
James Robins, Eric Tchetgen Tchetgen, Lingling Li, and Aad van der Vaart. Semiparametric minimax rates.
Electronic Journal of Statistics, 3:1305–1321, 2009.
James M Robins, Lingling Li, Eric Tchetgen Tchetgen, and Aad van der Vaart. Asymptotic normality of
quadratic estimators. Stochastic processes and their applications, 126(12):3733–3759, 2016.
James M. Robins, Lingling Li, Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and Aad van der Vaart. Minimax
estimation of a functional on a structured high-dimensional model. The Annals of Statistics, 45(5), 2017.
Andrea Rotnitzky, Ezequiel Smucler, and James M Robins. Characterization of parameters with a mixed bias
property. Biometrika, 108(1):231–238, 2021.
D. Ruppert and M. P. Wand. Multivariate locally weighted least squares regression. The Annals of Statistics,
22(3):1346–1370, 1994.
David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons,
Hoboken, NJ, 2015.
Rajen D. Shah and Jonas Peters. The hardness of conditional independence testing and the generalised covari-
ance measure. The Annals of Statistics, 48(3):1514–1538, 2020.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015.
Anastasios A Tsiatis. Semiparametric Theory and Missing Data. New York: Springer, 2006.
Alexandre B Tsybakov. Introduction to Nonparametric Estimation. New York: Springer, 2009.
Mark J van der Laan and James M Robins. Unified methods for censored longitudinal data and causality. New
York: Springer, 2003.
Mark J van der Laan, David Benkeser, and Weixin Cai. Efficient estimation of pathwise differentiable target
parameters with the undersmoothed highly adaptive lasso. The International Journal of Biostatistics, 2022.
M.J.vanderLaanandS.Rose. Targeted Learning: Causal Inference for Observational and Experimental Data.
Springer Series in Statistics. New York: Springer, 2011.
21Aad van der Vaart. Higher Order Tangent Spaces and Influence Functions. Statistical Science, 29(4):679–686,
2014.
Aad W van der Vaart and Jon A Wellner. Weak Convergence and Empirical Processes. New York: Springer,
1996.
Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statistics, 37(5A):2178,
2009.
Wenjing Zheng and Mark J van der Laan. Asymptotic theory for cross-validated targeted maximum likelihood
estimation. U.C. Berkeley Division of Biostatistics Working Paper Series, 2010.
Xiang Zhou and Aleksei Opacic. Marginal interventional effects. arXiv preprint arXiv:2206.10717, 2022.
22Appendix
These supplemental materials are arranged into eight sections:
A. In Appendix A, we prove Lemma 1 and Proposition 1 from Section 3.
B. In Appendix B, we prove bias, variance, and covariance bounds for the nuisance function estimators
considered in Section 4 — k-Nearest Neighbors and local polynomial regression.
C. In Appendix C, we use the results from Appendices A and B to prove Lemma 2 and Theorem 1 from
Section 4.
D. In Appendix D, we prove a variety of results for covariate-density-adapted kernel regression, including
conditional and unconditional variance upper and lower bounds.
E. In Appendix E, we prove Theorems 2 and 3 from Section 5, making use of the results in Appendix D.
F. In Appendix F, we prove three technical results regarding properties of the covariate density.
G. InAppendixG,weprovideasimplestronglawoflargenumbersfortriangulararraysofboundedrandom
variables.
H. Finally, in Appendix H, we review series regression nuisance function estimators, and state and prove
several results based on these estimators, which are equivalent to Lemma 2 and Theorem 1 in Section 4
of the paper.
A Section 3 proofs: Lemma 1 and Proposition 1
Lemma 1. (Structure-agnostic linear expansion) Under Assumptions 1 and 2, if ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1, then
ψ(cid:98)n−ψ
ecc
=(P n−P){φ(Z)}+R 1,n+R
2,n
(cid:32)(cid:114) (cid:33)
E∥φ−φ∥2 +ρ(Σ )
where R
1,n
≤∥b π∥P∥b µ∥P and R
2,n
=OP (cid:98) nP n ,
b ≡b (X)=E{η(X)−η(X)|X} is the pointwise bias of the estimator η, ρ(Σ ) denotes the spectral radius of
η η (cid:98) (cid:98) n
Σ , and
n
(cid:18) (cid:20)(cid:110) (cid:111)T (cid:21)(cid:19)
Σ
n
=E cov (cid:98)b φ(X 1),...,(cid:98)b φ(X n) |X φn
where (cid:98)b φ(X i) = E{φ (cid:98)(Z i)−φ(Z i) | X i,D π,D µ} is the conditional bias of φ
(cid:98)
and X φn denotes the covariates in
the estimation sample.
Proof. We first expand ψ(cid:98)n−ψ
ecc
into the term in the statement of the lemma plus two remainder terms, R
1
and R :
2
ψ(cid:98)n−ψ
ecc
=P n{φ (cid:98)(Z)}−E{φ(Z)}
=(P −E){φ(Z)}+E{φ(Z)−φ(Z)}+(P −E){φ(Z)−φ(Z)} (17)
n (cid:98) n (cid:98)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
R1,n R2,n
where E refers to expectation over the estimation and training data. The first term in (17) appears in the
statement of the lemma, so we manipulate it no further.
R and bounding the bias of ψ(cid:98) :
1,n n
23The second term in (17), R 1,n, is the bias of the estimator ψ(cid:98)n. It is not random. A simple analysis shows
E{φ(Z)−φ(Z)}≡E[{A−π(X)}{Y −µ(X)}−{A−π(X)}{Y −µ(X)}]
(cid:98) (cid:98) (cid:98)
=E(cid:2)
{A−π(X)}{µ(X)−µ(X)}+{Y
−µ(X)}{π(X)−π(X)}(cid:3)
(cid:98) (cid:98) (cid:98)
=E(cid:2) {π(X)−π(X)}{µ(X)−µ(X)}(cid:3)
(cid:98) (cid:98)
where the final line follows by iterated expectations. By the independence of the training datasets, we have
E(cid:2) {π (cid:98)(X)−π(X)}{µ (cid:98)(X)−µ(X)}(cid:3) =E(cid:2)E{π (cid:98)(X)−π(X)|X}E{µ (cid:98)(X)−µ(X)|X}(cid:3) ≤∥b π∥P∥b µ∥P
where the inequality follows by Cauchy-Schwarz and the definition of b =E{η(X)−η(X)|X}.
η (cid:98)
R and bounding the variance of ψ(cid:98) :
2,n n
The final term in (17), R , is centered and mean-zero. The statement in Lemma 1 is implied by Chebyshev’s
2,n
inequality after bounding the variance of R . Thus, the rest of this proof is devoted to a bound on V(R ),
2,n 2,n
which must account for randomness across both the estimation and training samples.
Since E{φ(Z)−φ(Z)} is not random, and by successive applications of the law of total variance, we have
(cid:98)
(cid:16) (cid:104) (cid:105)(cid:17) (cid:16) (cid:104) (cid:105)(cid:17)
V[(P −E){φ(Z)−φ(Z)}]=E V P {φ(Z)−φ(Z)}|Xn,D ,D +V E P {φ(Z)−φ(Z)}|Xn,D ,D
n (cid:98) n (cid:98) φ π µ n (cid:98) φ π µ
(cid:16) (cid:104) (cid:105)(cid:17)
=E V P {φ(Z)−φ(Z)}|Xn,D ,D (18)
n (cid:98) φ π µ
(cid:110) (cid:16) (cid:104) (cid:105) (cid:17)(cid:111)
+E V E P {φ(Z)−φ(Z)}|Xn,D ,D |Xn (19)
n (cid:98) φ π µ φ
(cid:110) (cid:16) (cid:104) (cid:105) (cid:17)(cid:111)
+V E E P {φ(Z)−φ(Z)}|Xn,D ,D |Xn (20)
n (cid:98) φ π µ φ
where Xn are the covariates in the estimation data. Expression (18) can be upper bounded using the fact that
φ
the data are iid and V(X)≤E(X2):
(cid:104) (cid:105)
(cid:16) (cid:104) (cid:105)(cid:17) (cid:20) 1 (cid:110) (cid:111)(cid:21) E {φ (cid:98)(Z)−φ(Z)}2
E V P {φ(Z)−φ(Z)}|Xn,D ,D =E V φ(Z)−φ(Z)|Xn,D ,D ≤ .
n (cid:98) φ π µ n (cid:98) φ π µ n
Similarlyexpression(20)canbeupperboundedusinglinearityofexpectation,iiddata,andthatV(X)≤E(X2)
and Jensen’s inequality:
(cid:110) (cid:16) (cid:104) (cid:105) (cid:17)(cid:111) (cid:16) (cid:104) (cid:105)(cid:17)
V E E P {φ(Z)−φ(Z)}|Xn,D ,D |Xn =V E P {φ(Z)−φ(Z)}|Xn
n (cid:98) φ π µ φ n (cid:98) φ
(cid:16) (cid:104) (cid:105)(cid:17)
=V P E{φ(Z)−φ(Z)|Xn}
n (cid:98) φ
(cid:104) (cid:105)
n E {φ(Z)−φ(Z)}2
1 (cid:88) (cid:104) (cid:105) (cid:98)
= V E{φ(Z )−φ(Z )|Xn} ≤ .
n2 (cid:98) i i φ n
i=1
Finally, for expression (19), by linearity of expectation, and the definition of(cid:98)b φ(X i) and Σ n, we have
(cid:34) (cid:40) n (cid:41)(cid:35)
(cid:110) (cid:16) (cid:104) (cid:105) (cid:17)(cid:111) 1 (cid:88)
E V E P n{φ (cid:98)(Z)−φ(Z)}|X φn,D π,D
µ
|X φn =E V
n
(cid:98)b φ(X i)|X φn
i=1
n n
1 (cid:88)(cid:88) (cid:104) (cid:110) (cid:111)(cid:105)
=
n2
E cov (cid:98)b φ(X i),(cid:98)b φ(X j)|X φn
i=1j=1
1
= 1TΣ 1
n2 n
where 1 the n-length vector of 1’s. Since Σ is positive semi-definite and symmetric, Σ = QΛQT where Q is
n n
24the orthonormal eigenvector matrix and Λ=diag(λ ,...,λ ) is the diagonal eigenvalue matrix. Then,
1 n
n n
(cid:88) (cid:88)
1TΣ 1=1TQΛQT1= λ ∥q ∥2 = λ ≤nρ(Σ )
n i i i n
i=1 i=1
where the third equality follows because the q are normalized, and the inequality follows by the definition of
i
the spectral radius. Therefore, 1 1TΣ 1≤ 1ρ(Σ ), and the result follows.
n2 n n n
Proposition 1. (Spectral radius bound) Under Assumptions 1 and 2, if ψ is estimated with the DCDR
ecc
estimator ψ(cid:98)n from Algorithm 1, then
ρ(Σ nn) ≤ E∥φ (cid:98) n−φ∥2 P +(cid:0) ∥b2 π∥ ∞+∥s2 π∥ ∞(cid:1)E(cid:104)(cid:12) (cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105)
+(cid:0) ∥b2 µ∥ ∞+∥s2 µ∥ ∞(cid:1)E(cid:104)(cid:12) (cid:12)cov{π (cid:98)(X i),π (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105)
where ∥b2∥ = sup E{η(X)−η(X) | X = x}2 and ∥s2∥ = sup V{η(X) | X = x} are uniform squared
η ∞ x∈X (cid:98) η ∞ x∈X (cid:98)
bias and variance bounds.
Proof. Since the spectral radius of a matrix is less than its Frobenius norm and the data are iid,
ρ(Σ ) 1 (cid:104) (cid:110) (cid:111)(cid:105) n−1 (cid:104) (cid:110) (cid:111)(cid:105)
nn ≤ nE V (cid:98)b φ(X)|X φn +
n
E cov
i̸=j
(cid:98)b φ(X i),(cid:98)b φ(X j)|X φn .
For the first summand, we have
1 (cid:104) (cid:110) (cid:111)(cid:105) E∥φ−φ∥2
nE V (cid:98)b φ(X)|X φn ≤ (cid:98)
n
P
because V(X) ≤ E(X2). For i ̸= j, we must analyze the covariance term in more detail. Omitting arguments
(e.g., π ≡π(X )),
i i
(cid:104) (cid:110) (cid:111)(cid:105) (cid:110) (cid:104) (cid:105)(cid:111)
E cov (cid:98)b φ(X i),(cid:98)b φ(X j)|X φn =E cov E{φ (cid:98)(Z i)−φ(Z i)|X φn,D π,D µ},E{φ (cid:98)(Z j)−φ(Z j)|X φn,D π,D µ}|X φn
(cid:104) (cid:110) (cid:111)(cid:105)
=E cov (π −π )(µ −µ ),(π −π )(µ −µ )|X ,X
(cid:98)i i (cid:98)i i (cid:98)j j (cid:98)j j i j
(cid:104) (cid:110) (cid:111) (cid:110) (cid:111) (cid:110) (cid:111)(cid:105)
=E E (π −π )(µ −µ )(π −π )(µ −µ )|X ,X −E (π −π )(µ −µ )|X ,X E (π −π )(µ −µ )|X ,X
(cid:98)i i (cid:98)i i (cid:98)j j (cid:98)j j i j (cid:98)i i (cid:98)i i i j (cid:98)j j (cid:98)j j i j
(cid:104) (cid:110) (cid:111) (cid:110) (cid:111)(cid:105)
=E E (π −π )(π −π )|X ,X E (µ −µ )(µ −µ )|X ,X
(cid:98)i i (cid:98)j j i j (cid:98)i i (cid:98)j j i j
−E(cid:8)E(cid:0)
π −π |X
(cid:1)E(cid:0)
µ −µ |X
(cid:1)E(cid:0)
π −π |X
(cid:1)E(cid:0)
µ −µ |X
(cid:1)(cid:9)
(cid:98)i i i (cid:98)i i i (cid:98)j j j (cid:98)j j j
(cid:104)(cid:110) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111)(cid:110) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111)(cid:105)
=E cov π ,π |X ,X +E π −π |X E π −π |X cov µ ,µ |X ,X +E µ −µ |X E µ −µ |X
(cid:98)i (cid:98)j i j (cid:98)i i i (cid:98)j j j (cid:98)i (cid:98)j i j (cid:98)i i i (cid:98)j j j
−E(cid:8)E(cid:0)
π −π |X
(cid:1)E(cid:0)
µ −µ |X
(cid:1)E(cid:0)
π −π |X
(cid:1)E(cid:0)
µ −µ |X
(cid:1)(cid:9)
(cid:98)i i i (cid:98)i i i (cid:98)j j j (cid:98)j j j
(cid:110) (cid:16) (cid:17) (cid:111)
=E cov π ,π |X ,X E(µ −µ |X )E(µ −µ |X ) + (21)
(cid:98)i (cid:98)j i j (cid:98)i i i (cid:98)j j j
(cid:110) (cid:16) (cid:17) (cid:111)
+E cov µ ,µ |X ,X E(π −π |X )E(π −π |X ) (22)
(cid:98)i (cid:98)j i j (cid:98)i i i (cid:98)j j j
(cid:110) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111)
+E cov π ,π |X ,X cov µ ,µ |X ,X (23)
(cid:98)i (cid:98)j i j (cid:98)i (cid:98)j i j
where the first equality follows by definition, the second and third by the definition of φ,φ, and covariance, the
(cid:98)
fourth by the independence of the training datasets, the fifth again by the definition of covariance and because
π ,π ,µ ,µ are not random conditional on X ,X , and the final line by canceling terms.
i j i j i j
For (21),
(cid:110) (cid:16) (cid:17) (cid:111)
E cov π ,π |X ,X E(µ −µ |X )E(µ −µ |X ) ≤
(cid:98)i (cid:98)j i j (cid:98)i i i (cid:98)j j j
(cid:104)(cid:12) (cid:110) (cid:111)(cid:12)(cid:105) (cid:12) (cid:12)
E (cid:12)cov π(X ),π(X )|X ,X (cid:12) sup (cid:12)E{µ(x )−µ(x )}E{µ(x )−µ(x )}(cid:12)
(cid:12) (cid:98) i (cid:98) j i j (cid:12) (cid:12) (cid:98) i i (cid:98) j j (cid:12)
xi,xj∈X
25(cid:104)(cid:12) (cid:110) (cid:111)(cid:12)(cid:105)(cid:26) (cid:12) (cid:12)(cid:27)2
=E (cid:12)cov π(X ),π(X )|X ,X (cid:12) sup(cid:12)E{µ(x)−µ(x)}(cid:12)
(cid:12) (cid:98) i (cid:98) j i j (cid:12) (cid:12) (cid:98) (cid:12)
x∈X
(cid:104)(cid:12) (cid:110) (cid:111)(cid:12)(cid:105)
≤E (cid:12)cov π(X ),π(X )|X ,X (cid:12) supE{µ(x)−µ(x)}2
(cid:12) (cid:98) i (cid:98) j i j (cid:12) (cid:98)
x∈X
(cid:104)(cid:12) (cid:110) (cid:111)(cid:12)(cid:105)
≡E (cid:12)cov π(X ),π(X )|X ,X (cid:12) ∥b2∥
(cid:12) (cid:98) i (cid:98) j i j (cid:12) µ ∞
where the first inequality is H¨older’s inequality, the second because |ab| = |a||b|, the penultimate by Jensen’s
inequality, and the final by the definition of ∥b ∥ . The same result applies for (22) with µ and π swapped.
µ ∞
Next, notice that,
(cid:110) (cid:111) (cid:16)(cid:104) (cid:105)(cid:104) (cid:105) (cid:17)
cov π(X ),π(X )|X ,X =E π(X )−E{π(X )|X ,X } π(X )−E{π(X )|X ,X } |X ,X
(cid:98) i (cid:98) j i j (cid:98) i (cid:98) i i j (cid:98) j (cid:98) j i j i j
(cid:16)(cid:104) (cid:105)(cid:104) (cid:105) (cid:17)
=E π(X )−E{π(X )|X } π(X )−E{π(X )|X } |X ,X
(cid:98) i (cid:98) i i (cid:98) j (cid:98) j j i j
(cid:115)
(cid:18)(cid:104) (cid:105)2 (cid:19) (cid:18)(cid:104) (cid:105)2 (cid:19)
≤ E π(X )−E{π(X )|X } |X E π(X )−E{π(X )|X } |X
(cid:98) i (cid:98) i i i (cid:98) j (cid:98) j j j
(cid:113)
= V{π(X )|X }V{π(X )|X }
(cid:98) i i (cid:98) j j
where the first line follows by definition, the second because π(X ) ⊥⊥ X for X ̸= X , the third by Cauchy-
(cid:98) i j i j
Schwarz, and the fourth by the definition of the variance. Therefore, for (23),
(cid:110) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111) (cid:26)(cid:113) (cid:12) (cid:16) (cid:17)(cid:12)(cid:27)
E cov π ,π |X ,X cov µ ,µ |X ,X ≤E V{π(X )|X }V{π(X )|X }(cid:12)cov µ ,µ |X ,X (cid:12)
(cid:98)i (cid:98)j i j (cid:98)i (cid:98)j i j (cid:98) i i (cid:98) j j (cid:12) (cid:98)i (cid:98)j i j (cid:12)
(cid:113) (cid:110)(cid:12) (cid:16) (cid:17)(cid:12)(cid:111)
≤ sup V{π(x )}V{π(x )}E (cid:12)cov µ ,µ |X ,X (cid:12)
(cid:98) i (cid:98) j (cid:12) (cid:98)i (cid:98)j i j (cid:12)
xi,xj∈X
(cid:110)(cid:12) (cid:16) (cid:17)(cid:12)(cid:111)
=supV{π(x)}E (cid:12)cov µ ,µ |X ,X (cid:12)
(cid:98) (cid:12) (cid:98)i (cid:98)j i j (cid:12)
x
(cid:110)(cid:12) (cid:16) (cid:17)(cid:12)(cid:111)
≡∥s2∥ E (cid:12)cov µ ,µ |X ,X (cid:12)
π ∞ (cid:12) (cid:98)i (cid:98)j i j (cid:12)
where the first line follows by H¨older’s inequality, the second by the argument in the previous paragraph, the
third because |ab|=|a||b|, and the last line follows by definition of ∥s2∥ .
π ∞
The result in Proposition 1 follows by repeating the process in the previous paragraph with the roles of π
and µ reversed. In fact, Proposition 1 can be improved because we can take the minimum rather than the sum
of the variances at the final step so that
ρ(Σ ) E∥φ−φ∥2 (cid:104) (cid:105) (cid:104) (cid:105)
n ≤ (cid:98) P +∥b2∥ E |cov{µ(X ),µ(X )|X ,X }| +∥b2∥ E |cov{π(X ),π(X )|X ,X }|
n n π ∞ (cid:98) i (cid:98) j i j µ ∞ (cid:98) i (cid:98) j i j
+min(cid:16) ∥s2 π∥ ∞E(cid:104)(cid:12) (cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105) ,∥s2 µ∥ ∞E(cid:104)(cid:12) (cid:12)cov{π (cid:98)(X i),π (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:105)(cid:17) .
(24)
Proposition 1 follows because the minimum in (24) is upper bounded by the sum. We will also use (24)
subsequently, referring to it in the proof of Theorems 2 and 3.
B k-Nearest Neighbors and local polynomial regression
In Sections 4, we defined two linear smoother estimators. In this section, we state and prove several results
for each estimator, including bounds on their bias and variance, as well as bounds on their expected absolute
covariance, E[|cov{η(X ),η(X )|X ,X }|]. In the following, we state and prove the results for Y and µ(X).
(cid:98) i (cid:98) j i j
All results also apply to A and π(X).
26B.1 k-Nearest Neighbors
The analysis of the bias of the k-Nearest Neighbors estimator relies on control of the nearest neighbor distance.
The nearest neighbor distance is well understood, and general results can be be found in, for example, Chapter
6ofGy¨orfietal.[2002],Chapter2ofBiauandDevroye[2015],andDasguptaandKpotufe[2021]. Byleveraging
Assumption 2, that the density is upper and lower bounded (which is a stronger assumption than generally
required),weprovideasimpleresultthatissufficientforoursubsequentanalysis,whichusessimilartechniques
to those in the proof of Lemma 6.4 (and Problem 6.7) in Gy¨orfi et al. [2002].
Lemma 3. Suppose we observe {X }n sampled iid from a distribution satisfying Assumption 2. Then, for
i i=1
0<p≤2d and x∈X,
E∥X (x)−x∥p ≲n−p/d. (25)
(1)
Proof. Let B (x) denote a ball of radius r centered at x. Then,
r
(cid:90) ∞
E∥X (x)−x∥p = P(cid:8) ∥X (x)−x∥p >t(cid:9) dt
(1) (1)
0
(cid:90) ∞ (cid:110) (cid:111)
= P ∥X (x)−x∥>t1/p dt
(1)
0
(cid:90) ∞ (cid:110) (cid:111)n
= P ∥X−x∥>t1/p dt
0
(cid:90) ∞(cid:104) (cid:105)n
= 1−P{X ∈B (x)} dt
t1/p
0
where the third line follows because the observations {X }n are iid. Then, by Assumption 2, for all r > 0,
i i=1
P{X ∈ B (x)} ≥ cKrd ∧1, where c is the lower bound on the density and K is a constant arising from the
r
volume of the d-dimensional sphere. Therefore,
(cid:90) ∞(cid:104) (cid:105)n (cid:90) ∞(cid:110)(cid:16) (cid:17) (cid:111)n
1−P{X ∈B (x)} dt≤ 1−cKtd/p ∨0 dt
t1/p
0 0
(cid:90) (cK)−p/d (cid:16) (cid:17)n
= 1−cKtd/p dt
0
(cid:90) (cK)−p/d (cid:16) (cid:17)
≤ exp −cKntd/p dt
0
(cid:90) ∞ (cid:16) (cid:17)
≤ exp −cKntd/p dt.
0
where the penultimate line follows because 1−x≤e−x and the final line because e−x >0.
Next, notice that
(cid:90) ∞ exp(cid:16) −cKntd/p(cid:17) dt=−(cKn)−p/dΓ(p/d,cKntd/p)(cid:12) (cid:12) (cid:12)∞
d/p (cid:12)
0 0
≲n−p/d
wherethefirstlinefollowsfromstandardrulesofintegrationandwhereΓ(s,t)istheincompletegammafunction,
which satisfies Γ(s,x)=(cid:82)∞ ts−1e−tdt, and the second line follows because Γ(p/d,∞)=0 while Γ(p/d,0),d/p,
x
and cK are constants that do not depend on n. Therefore,
E∥X (x)−x∥p ≲n−p/d. (26)
(1)
Thenextresultprovidespointwisebiasandvarianceboundsforthek-NearestNeighborsestimator. Noticethat
the variance scales at the mean squared error rate due to the randomness over the training data .
27Lemma 4. (k-Nearest Neighbors Bounds) Suppose Assumptions 1, 2 and 3 hold. Then, if µ(x) is a k-Nearest
(cid:98)
Neighbors estimator (Estimator 1) for µ(x) constructed on D ,
µ
(cid:16)n(cid:17)−β∧1
sup|E{µ(x)−µ(x)}|≲ d and (27)
(cid:98) k
x∈X
1 (cid:16)n(cid:17)−2(β∧1)
supV{µ(x)}≲ + d . (28)
(cid:98) k k
x∈X
Proof. Weprovetheboundsforgenericx,andthesupremumboundswillfollowbecauseX isassumedcompact
in Assumption 2. Note that, if µ ∈ H¨older(β) for β > 1 then µ ∈ H¨older(1) (in other words, µ is Lipschitz).
For the bias in (27), we have
(cid:12) (cid:12) (cid:40) 1 (cid:88)n (cid:16) (cid:17) (cid:41)(cid:12) (cid:12)
|E{µ(x)−µ(x)}|=(cid:12)E 1 ∥X −x∥≤∥X (x)−x∥ Y −µ(x) (cid:12)
(cid:98) (cid:12) k i (k) i (cid:12)
(cid:12) (cid:12)
i=1
(cid:12) (cid:12)
(cid:12) k (cid:12)
=(cid:12)
(cid:12)
(cid:12)k1 (cid:88) E(cid:2)
µ{X
(j)(x)}−µ(x)(cid:3)(cid:12)
(cid:12)
(cid:12)
(cid:12) j=1 (cid:12)
(cid:12) (cid:12)
(cid:12) k (cid:12)
≲(cid:12)
(cid:12)
(cid:12)k1 (cid:88) E{∥X (j)(x)−x∥β∧1}(cid:12)
(cid:12)
(cid:12)
(cid:12) j=1 (cid:12)
k
1 (cid:88)
≤ E∥X (x)−x∥β∧1
k (j)
j=1
wherethefirstlinefollowsbydefinition,thesecondbyiteratedexpectationsonthetrainingcovariatesandthen
by definition, the first inequality by the smoothness assumption on µ, and the second by Jensen’s inequality.
For k =1, one can invoke Lemma 3 directly, giving
|E{µ (cid:98)(x)−µ(x)}|≤n−β d∧1 . (29)
Otherwise, split the n datapoints into k+1 subsets, where the first k subsets are of size ⌊n/k⌋. Let X(cid:101)j (x)
(1)
denote the nearest neighbor to x in the jth split. Then, the following deterministic inequality holds:
k k
1 (cid:88) 1 (cid:88)
k
E∥X (j)(x)−x∥β∧1 ≤
k
E∥X(cid:101) (j 1)(x)−x∥β∧1.
j=1 j=1
Thus, applying Lemma 3 to E∥X(cid:101)j (x)−x∥β∧1 yields
(1)
|E{µ (cid:98)(x)−µ(x)}|≲(⌊n/k⌋)−β d∧1 ≍(n/k)−β d∧1 . (30)
For the variance in (28), we have
V{µ(x)}=V(cid:2)E{µ(x)|Xn}(cid:3) +E(cid:2)V{µ(x)|Xn}(cid:3)
(cid:98) (cid:98) µ (cid:98) µ
=V(cid:2)E{µ(x)−µ(x)|Xn}(cid:3) +E(cid:2)V{µ(x)|Xn}(cid:3)
(cid:98) µ (cid:98) µ
≤E(cid:2)E{µ(x)−µ(x)|Xn}2(cid:3) +E(cid:2)V{µ(x)|Xn}(cid:3)
(cid:98) µ (cid:98) µ
(cid:16)n(cid:17)−2(β∧1) 1
≲ d +
k k
wherethefirstlinefollowsbythelawoftotalvariance,thesecondbecauseµ(x)isnon-random,thethirdbecause
V(X) ≤ E(X2), the fourth by the bound on the bias, and the final line because {Y ,...,Y } are independent
1 n
conditional on Xn and have bounded conditional variance by Assumption 1.
µ
The supremum bound follows since the proof holds for arbitrary x and X is compact by Assumption 2.
28The final result of this section provides a bound on the covariance term that appears in Proposition 1 and
Lemma 2.
Lemma 5. (k-NN covariance bound) Suppose Assumptions 1 and 2 hold and µ(x) is a k-Nearest Neighbors
(cid:98)
estimator (Estimator 1) for µ(x) constructed on D . Then,
µ
E(cid:2)
|cov{µ(X ),µ(X )|X ,X
}|(cid:3)≲(cid:40) 1 +(cid:16)n(cid:17)−2(β d∧1)(cid:41)(cid:18) k(cid:19)
.
(cid:98) i (cid:98) j i j k k n
Proof. We have
E(cid:2)
|cov{µ(X ),µ(X )|X ,X
}|(cid:3) =E(cid:2)
|cov{µ(X ),µ(X )|X ,X }|1(∥X −X ∥≤∥X −X (X
)∥)(cid:3)
(cid:98) i (cid:98) j i j (cid:98) i (cid:98) j i j i j i (2k) i
≤ sup |cov{µ(x ),µ(x
)}|P(cid:0)
∥X −X ∥≤∥X −X (X
)∥(cid:1)
(cid:98) i (cid:98) j i j i (2k) i
xi,xj
≤
supV{µ(x)}P(cid:0)
∥X −X ∥≤∥X −X (X
)∥(cid:1)
(cid:98) i j i (2k) i
x∈X
(cid:40) (cid:41)
≲ 1 +(cid:16)n(cid:17)−2(β d∧1) P(cid:0) ∥X −X ∥≤∥X −X (X )∥(cid:1)
k k i j i (2k) i
where the first line follows because cov{µ(X ),µ(X ) | X ,X } = 0 when ∥X −X ∥ > ∥X −X (X )∥, the
(cid:98) i (cid:98) j i j i j i (2k) i
second by H¨older’s inequality, and the final line by Lemma 4.
It remains to bound P(∥X −X ∥≤∥X −X (X )∥). We have
i j i (2k) i
P(∥X −X ∥≤∥X −X (X )∥)=E(cid:8)P(∥X −X ∥≤∥X −X (X )∥|X )(cid:9)
i j i (2k) i i j i (2k) i i
2k k
= ≲ .
n+1 n
where the first line follows by iterated expectations. The second line follows because P(∥X −X ∥ ≤ ∥X −
i j i
X (X )∥ | X ) is the probability that X is one of the 2k closest points to X out of X and the n training
(2k) i i j i j
data points. Because X and the training data are iid, X has an equal chance of being any order neighbor to
j j
X , and therefore the probability it is in the 2k closest points is 2k .
i n+1
Therefore, we conclude that
E(cid:2)
|cov{µ(X ),µ(X )|X ,X
}|(cid:3)≲(cid:40) 1 +(cid:16)n(cid:17)−2(β d∧1)(cid:41)(cid:18) k(cid:19)
.
(cid:98) i (cid:98) j i j k k n
B.2 Local polynomial regression
TheproofsinthissubsectionfollowcloselytothoseinTsybakov[2009]. Themaindifferenceisthatwetranslate
the conditional bounds into marginal bounds, like in Kennedy [2023]. Let
(cid:16) (cid:17)
A
n
=1 Q(cid:98) is invertible , (31)
P {1(∥X−x∥≤h)}
ξ := n , and (32)
n hd
(cid:16) (cid:17)
λ
n
:=λ
max
Q(cid:98)−1 . (33)
First, we note that the weights reproduce polynomials up to degree ⌈d/2⌉ by the construction of the estimator
in Estimator 2 (Tsybakov [2009] Proposition 1.12) as long as A
n
=1 (i.e., Q(cid:98) is invertible).
We will state results for the bias and variance of the estimator conditionally on the training covariates,
assumingQ(cid:98) isinvertible,andkeepingλ
n
andξ
n
intheresults. Then,wewillarguethatλ
n
andξ
n
arebounded
29in probability and therefore that (i) Q(cid:98) is invertible with probability converging to one appropriately quickly,
and (ii) the relevant bias and variance bounds hold in probability. Next, we demonstrate that the weights have
the desired localizing properties in the following result (Tsybakov [2009] Lemma 3).
Proposition 2. Suppose Assumptions 1 and 2 hold, µ(x) is a local polynomial regression estimator (Estima-
(cid:98)
tor 2) for µ(x) constructed on D µ, and Q(cid:98) is invertible. Let
(cid:18) (cid:19) (cid:18) (cid:19)
1 X −x X −x
w i(x;X µn)= nhdb(0)TQ(cid:98)−1b i
h
K i
h
.
Then,
λ
sup|w (x;Xn)|≲ n , (34)
i µ nhd
i,x
n
(cid:88)
|w (x;Xn)|≲λ ξ , and (35)
i µ n n
i=1
w (x;Xn)=0 when ∥X −x∥>h. (36)
i µ i
Proof. (36) follows by the definition of the kernel in Estimator 2. For (34),
(cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12)
|w i(x;X µn)|=(cid:12) (cid:12) (cid:12)n1 hdb(0)TQ(cid:98)−1b X i h−x K X i h−x (cid:12) (cid:12)
(cid:12)
(cid:13) (cid:18) (cid:19) (cid:18) (cid:19)(cid:13)
≤ 1 ∥b(0)T∥(cid:13) (cid:13)Q(cid:98)−1b X i−x K X i−x (cid:13) (cid:13)
nhd (cid:13) h h (cid:13)
(cid:13) (cid:18) (cid:19) (cid:18) (cid:19)(cid:13)
≤ λ n (cid:13) (cid:13)b X i−x K X i−x (cid:13) (cid:13)
nhd(cid:13) h h (cid:13)
(cid:13) (cid:18) (cid:19)(cid:13)
≲ nλ hn
d
(cid:13) (cid:13) (cid:13)b X i h−x (cid:13) (cid:13) (cid:13)1(∥X i−x∥≤h)
λ 1(∥X −x∥≤h)
≲ n i
nhd
where the first line follows by definition, the second by Cauchy-Schwarz, the third because ∥b(0)T∥ = 1 and
the definition of λ , the fourth because the kernel is localized by definition in Estimator 2, and the last by
n
Assumption 2 and compact support X. (34) then follows because the indicator function is at most 1. Finally,
for (35),
n n (cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12)
(cid:88) |w i(x;X µn)|=(cid:88)(cid:12) (cid:12) (cid:12)n1 hdb(0)TQ(cid:98)−1b X i h−x K X i h−x (cid:12) (cid:12)
(cid:12)
i=1 i=1
n
≲ λ n (cid:88) 1(∥X −x∥≤h)=λ ξ
nhd i n n
i=1
where the second line follows by the same arguments as before and the definition of ξ .
n
Next, we prove conditional bias and variance bounds (Tsybakov [2009] Proposition 1.13).
Proposition 3. Suppose Assumptions 1, 2, and 3 hold and µ(x) is a local polynomial regression estimator
(cid:98)
(Estimator 2) for µ(x) constructed on D µ. Let A
n
denote the event that Q(cid:98) is invertible, as in (31). Then,
(cid:12) (cid:12)E{µ (cid:98)(x)−µ(x)|X µn,A
n
=1}(cid:12) (cid:12)≲λ nξ nhβ∧⌈d/2⌉ (37)
and
λ2ξ
V{µ(x)|Xn}≲ n n.
(cid:98) µ nhd
30Proof. Notice first that
(cid:40) n (cid:41)
(cid:88)
E{µ(x)−µ(x)|Xn,A =1}=E w (x;Xn)Y −µ(x)|Xn,A =1
(cid:98) µ n i µ i µ n
i=1
n
(cid:88)
= w (x;Xn)µ(X )−µ(x)
i µ i
i=1
n
(cid:88)
= w (x;Xn){µ(X )−µ(x)}
i µ i
i=1
since the weights sum to 1. Let γ =β∧⌈d/2⌉, and consider the Taylor expansion of µ(X )−µ(x) up to order
i
⌊γ⌋:
 
n (cid:90) 1
(cid:12) (cid:12)E{µ (cid:98)(x)−µ(x)|X µn,A n =1}(cid:12) (cid:12)=(cid:88) w i(x;X µn) (cid:88) (1−t)⌊γ⌋−1(cid:8) Dkµ(x+t(X i−x))−Dkµ(x)(cid:9) dt(X i−x)k 
i=1 |k|=⌊γ⌋ 0
n
(cid:88)
≲ w (x;Xn)∥X −x∥γ
i µ i
i=1
n
(cid:88)
≤ |w (x;Xn)|hγ
i µ
i=1
≲λ ξ hγ ≡λ ξ hβ∧⌈d/2⌉
n n n n
where the first line follows by a multivariate Taylor expansion of µ(X )−µ(x) and the reproducing property of
i
local polynomial regression, the second by Assumption 3, the third by (36) and the fourth by (35).
For the variance, we have
n
(cid:88)
V{µ(x)|Xn}= w (x;Xn)2V(Y |X )
(cid:98) µ i µ i i
i=1
n
(cid:88)
≲ w (x;Xn)2
i µ
i=1
n
(cid:88)
≤sup|w (x;Xn)| |w (x;Xn)|
i µ i µ
i,x
i=1
λ2ξ
≲ n n,
nhd
where the second line follows by Assumption 1, and the last line by equations (34) and (35).
In the next result, we provide a bound on the probability that the minimum eigenvalue of Q(cid:98) equals zero,
which informs both an upper bound on λ
n
and a bound on the probability that Q(cid:98) is invertible.
Proposition 4. Suppose Assumption 2 holds, µ(x) is a local polynomial regression estimator (Estimator 2) for
(cid:98)
µ(x) constructed on D . Then, for some c>0
µ
P(cid:110)
λ
min(Q(cid:98))≤c(cid:111) ≲exp(cid:0) −nhd(cid:1)
. (38)
Proof. By the Matrix Chernoff inequality (e.g., Tropp [2015] Theorem 5.1.1),
 (cid:110) (cid:16) (cid:17)(cid:111)  (cid:110) (cid:16) (cid:17)(cid:111)
 λ min E Q(cid:98)  λ min E Q(cid:98)
P λ min(Q(cid:98))≤
2
≲exp
L

 
where L:=maxn ρ(cid:110) 1 b(cid:0)Xi−x(cid:1) K(cid:0)Xi−x(cid:1) b(cid:0)Xi−x(cid:1)T(cid:111) and, as a reminder, ρ(A) denotes the spectral radius
i=1 nhd h h h
31of a matrix A. By the boundedness of b and the kernel, L=O(cid:0) 1 (cid:1) . Meanwhile,
nhd
(cid:16) (cid:17) (cid:40) 1 (cid:18) X−x(cid:19) (cid:18) X−x(cid:19) (cid:18) X−x(cid:19)T(cid:41)
E Q(cid:98) =E b K b
hd h h h
(cid:90)
= b(u)K(u)b(u)Tf(x+uh)du
(cid:90)
= b(u)b(u)Tf(x+uh)du≍I
(d+⌈d/2⌉)
∥u∥≤1 ⌈d/2⌉
where the first line follows by definition and iid data, the second by a change of variables, the third by the
definitionofthekernel,andthefourthbythelowerboundedcovariatedensityinAssumption2andthedefinition
(cid:16) (cid:17)
of the basis. Therefore, E Q(cid:98) is proportional to the identity and thus its minimum eigenvalue is proportional
to 1, and the result follows.
Corollary 1. Suppose Assumption 2 holds, µ(x) is a local polynomial regression estimator (Estimator 2) for
(cid:98)
µ(x) constructed on D . Then,
µ
P(A =0)≲exp(−nhd) (39)
n
and, if nhd →∞ and n→∞, then
λ
n
=OP(1) (40)
Proof. The first result follows because Q(cid:98) is positive semi-definite by the construction of the basis. Therefore,
it is invertible if its minimum eigenvalue is positive, and the bound follows from Proposition 4. Meanwhile, the
second result follows directly from Proposition 4.
Next, we demonstrate that ξ is bounded in probability. This result relies on the bandwidth decreasing
n
slowly enough that nhd →∞ as n→∞ and the upper bound on the covariate density.
Proposition 5. Suppose Assumption 2 holds, µ(x) is a local polynomial regression estimator (Estimator 2) for
(cid:98)
µ(x) constructed on D µ, and nhd →∞ as n→∞. Then, ξ
n
=OP(1).
Proof. Notice that E(ξ ) ≍ 1 and V(ξ ) ≲ 1 by the construction of the kernel, Assumption 2, and Lemma
n n nhd
21. The result follows by the assumption on the bandwidth and Chebyshev’s inequality.
Lemma 6. (Local polynomial regression bounds) Suppose Assumptions 1, 2, and 3 hold, µ(x) is a local poly-
(cid:98)
nomial regression estimator (Estimator 2) for µ(x) constructed on D , and nhd →∞ as n→∞. Then,
µ
(cid:16) (cid:17)
sup|E{µ (cid:98)(x)−µ(x)}|≲OP hβ∧⌈d/2⌉ +exp(−nhd) (41)
x∈X
and
(cid:18) (cid:19)
1
supV{µ (cid:98)(x)}≲OP
nhd
+h2(β∧⌈d/2⌉) +exp(−nhd). (42)
x∈X
Proof. We prove the bounds for generic x, and the supremum bounds will follow because X is compact by
Assumption 2. Starting with (41),
|E{µ (cid:98)(x)−µ(x)}|≤E(cid:2)(cid:12) (cid:12)E{µ (cid:98)(x)−µ(x)|X µn}(cid:12) (cid:12)(cid:3)
(cid:20)
≤E (cid:12) (cid:12)E{µ (cid:98)(x)−µ(x)|X µn,A
n
=1}(cid:12) (cid:12)P(A
n
=1|X µn)
(cid:21)
+(cid:12) (cid:12)E{µ (cid:98)(x)−µ(x)|X µn,A
n
=0}(cid:12) (cid:12)P(A
n
=0|X µn)
(cid:16) (cid:17)
≲E λ ξ hβ∧⌈d/2⌉ +P(A =0)
n n n
(cid:16) (cid:17)
≲OP hβ∧⌈d/2⌉ +exp(−nhd),
32where the first line follows by iterated expectations and Jensen’s inequality, the second by the law of total
probability and the triangle inequality, the third by (37) in Proposition 3 for the first term and because the
bias is bounded in the second term (by the construction of the estimator and Assumption 1) and iterated
expectations again, and the final line by Corollary 1 and Proposition 5.
For (42), we have
(cid:104) (cid:105) (cid:104) (cid:105)
V{µ(x)}=V E{µ(x)|Xn} +E V{µ(x)|Xn}
(cid:98) (cid:98) µ (cid:98) µ
(cid:104) (cid:105) (cid:18) λ2ξ (cid:19)
≲V E{µ(x)|Xn} +E n n
(cid:98) µ nhd
(cid:104) (cid:105) (cid:18) 1 (cid:19)
=V E{µ (cid:98)(x)|X µn} +OP
nhd
,
where the first line follows by the law of total variance, the second by Proposition 3, and the third by Corollary
(cid:104) (cid:105)
1 and Proposition 5. It remains to bound V E{µ(x)|Xn} . We have
(cid:98) µ
(cid:104) (cid:105) (cid:104) (cid:105)
V E{µ(x)|Xn} =V E{µ(x)−µ(x)|Xn}
(cid:98) µ (cid:98) µ
(cid:104) (cid:105)
≤E E{µ(x)−µ(x)|Xn}2
(cid:98) µ
(cid:104)
=E E{µ(x)−µ(x)|Xn,A =1}2P(A =1|Xn)
(cid:98) µ n n µ
(cid:105)
+E{µ(x)−µ(x)|Xn,A =0}2P(A =0|Xn)
(cid:98) µ n n µ
(cid:16) (cid:17)
≲E λ2ξ2h2β∧2⌈d/2⌉ +P(A =0)
n n n
(cid:16) (cid:17)
≲OP h2β∧2⌈d/2⌉ +exp(−nhd),
wherefirstlinefollowsbecauseµ(x)isnotrandom,thesecondlinebecauseV(X)≤E(X2),thethirdlinebythe
law of total probability, the fourth by (37) in Proposition 3 for the first term and because the bias is bounded
in the second term (by the construction of the estimator and Assumption 1) and iterated expectations again,
and the final line by Corollary 1 and Proposition 5.
The supremum bound follows since the proof holds for arbitrary x and X is compact by Assumption 2.
Lemma 7. (Local polynomial regression covariance bound) Suppose Assumptions 1, 2, and 3 hold, µ(x) is a
(cid:98)
local polynomial regression estimator (Estimator 2) for µ(x) constructed on D , and nhd → ∞ as n → ∞.
µ
Then,
(cid:26) (cid:18) (cid:19) (cid:27)
E(cid:2) |cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}|(cid:3)≲hd OP n1
hd
+h2(β∧⌈d/2⌉) +exp(−nhd)
Proof. We have
E(cid:2)
|cov{µ(X ),µ(X )|X ,X
}|(cid:3) =E(cid:2)
|cov{µ(X ),µ(X )|X ,X }|1(∥X −X
∥≤2h)(cid:3)
(cid:98) i (cid:98) j i j (cid:98) i (cid:98) j i j i j
≤ sup |cov{µ(x ),µ(x )}|P(∥X −X ∥≤2h)
(cid:98) i (cid:98) j i j
xi,xj
≤ supV{µ(x)}P(∥X −X ∥≤2h)
(cid:98) i j
x∈X
(cid:26) (cid:18) (cid:19) (cid:27)
1
≲ OP +h2(β∧⌈d/2⌉) +exp(−nhd) hd
nhd
where the first line follows because cov{µ(X ),µ(X ) | X ,X } = 0 when ∥X −X ∥ > 2h, the second by
(cid:98) i (cid:98) j i j i j
H¨older’s inequality, and the last line by Lemmas 6 and 21.
33C Section 4 proofs: Lemma 2 and Theorem 1
Inthissection,weusetheresultsfromAppendicesAandBtoestablishLemma2andTheorem1fromSection4.
Lemma2. (Covariancebound)SupposeAssumptions1,2,and3hold. Moreover,assumethateachestimator
balances squared bias and variance or is undersmoothed. Then, both k-Nearest Neighbors and local polynomial
regression satisfy
E(cid:104)(cid:12)
(cid:12)cov{η (cid:98)(X i),η (cid:98)(X j)|X i,X
j}(cid:12) (cid:12)(cid:105) =OP(cid:18) n1(cid:19)
(6)
for η ∈{π,µ}.
Proof. This follows by Lemmas 5 and 7, and by the conditions on the tuning parameters.
Theorem 1. (Semiparametric efficiency) Suppose Assumptions 1, 2, and 3 hold, and ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1.
If the nuisance functions µ and π are estimated with local polynomial regression (Estimator 2) with band-
(cid:98) (cid:98)
(cid:16) (cid:17)−1/d
widths satisfying h ,h ≍ n , then
µ π logn
(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4, and
(7)
E|ψ(cid:98)n−ψ ecc|=OP(cid:16)
lon
gn(cid:17)−α+ dβ
otherwise.
Ifthenuisancefunctionsµandπ areestimatedwithk-NearestNeighbors(Estimator1)andk ,k ≍logn,
(cid:98) (cid:98) µ π
then
(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4 and α,β ≤1, and
(8)
E|ψ(cid:98)n−ψ ecc|≲(cid:16)
lon
gn(cid:17)−(α∧1)+ d(β∧1)
otherwise.
Proof. By Lemma 1,
ψ(cid:98)n−ψ
ecc
=(P n−P)φ+R 1,n+R
2,n
where
R
1,n
≤∥b π∥P∥b µ∥P
and
(cid:32)(cid:114) (cid:33)
E∥φ−φ∥2 +ρ(Σ )
R
2,n
=OP (cid:98) nP n .
The first term, (P −P)φ, satisfies the CLT in the statement of the result, and also satisfies (P −P)φ =
n n
OP(n−1/2). Therefore, we focus on the two remainder terms in the rest of this proof.
By the conditions on the rate at which the number of neighbors and the bandwidth scale, and by Lemma 2,
(cid:104) (cid:105) 1
E |cov{η(X ),η(X )|X ,X }| ≲ for η ∈{π,µ}.
(cid:98) i (cid:98) j i j n
Therefore, by Proposition 1,
(cid:115) 
E∥φ−φ∥2 +∥b2∥ +∥s2∥ +∥b2∥ +∥s2∥
R
2,n
=OP (cid:98) P π ∞ nπ ∞ µ ∞ µ ∞ .
Because the EIF for the ECC is Lipschitz in the nuisance functions,
E∥φ−φ∥2 ≲E∥π−π∥2 +E∥µ−µ∥2 ≤∥b2∥ +∥s2∥ +∥b2∥ +∥s2∥ ,
(cid:98) P (cid:98) P (cid:98) P π ∞ π ∞ µ ∞ µ ∞
34and, thus,
(cid:32)(cid:114) (cid:33)
∥b2∥ +∥s2∥ +∥b2∥ +∥s2∥
R
2,n
=OP π ∞ π ∞
n
µ ∞ µ ∞ .
Nearest Neighbors:
Next, we consider k-Nearest Neighbors. By Lemma 4, when k ,k ≍logn,
µ π
(cid:18)
n
(cid:19)−(α∧1)+ d(β∧1)
R
1,n
≤∥b π∥P∥b µ∥P ≲
logn
(43)
while
(cid:115) 
(n/logn)−(α d∧1) +(n/logn)−(β∧ d1)
+1/logn
R
2,n
=OP
n
=oP(n−1/2).
The variance term, R , is always asymptotically negligible, while the bias term, R , controls when the
2,n 1,n
√
estimator is semiparametric efficient and the convergence rate in the non- n regime. The convergence rate in
thenon-root-nregimefollowsimmediatelyfrom(43). Forthethresholdatwhichtheestimatorissemiparametric
efficient, notice that
(cid:18)
n
(cid:19)−(α∧1)+ d(β∧1) (cid:18)
n
(cid:19)−α+ dβ
R
1,n
≤
logn
=
logn
=oP(n−1/2)
if and only if α+β >d/4 and α,β ≤1.
2
Local polynomial regression:
(cid:16) (cid:17)−1/d
For local polynomial regression, by Lemma 6, when h ,h ≍ n then
µ π logn
(cid:18)
n
(cid:19)−(α∧⌈d/2⌉)+ d(β∧⌈d/2⌉)
R
1,n
≤∥b π∥P∥b µ∥P =OP
logn
while
(cid:115) 
(n/logn)−(α∧⌈ dd/2⌉) +(n/logn)−(β∧⌈ dd/2⌉)
+1/logn
R
2,n
=OP
n
=oP(n−1/2).
Again, the variance term, R , is always asymptotically negligible, while the bias term, R , controls when
2,n 1,n
√
the estimator is semiparametric efficient and the convergence rate in the non- n regime. When α+β > d there
2 4
are two cases to consider: (1) when α>d/2 or β >d/2, and (2) when α,β <d/2. In the first case, then
(cid:18)
n
(cid:19)−(α∧⌈d/2⌉)+ d(β∧⌈d/2⌉) (cid:18)
n
(cid:19)−⌈d d/2⌉
R
1,n
=OP
logn
=OP
logn
=oP(n−1/2).
In the second case,
(cid:18)
n
(cid:19)−α+ dβ
R
1,n
=OP
logn
=oP(n−1/2),
which follows because α+β >d/2.
When α+β ≤ d/4, it follows that α+β ≤ d/2 =⇒ α,β ≤ ⌈d/2⌉. Therefore, the convergence rate of the
2
DCDR estimator satisfies
(cid:18)
n
(cid:19)−α+ dβ
E|ψ(cid:98)n−ψ|=OP
logn
+oP(n−1/2).
35D Covariate-density-adapted kernel regression
In this section, we establish six results for covariate-density-adapted kernel regression (Estimator 3). The first
result, Lemma 8, establishes upper bounds on the variance and covariance. The second result, Lemma 9,
establishes a lower bound on the unconditional variance. The third result, Lemma 10, establishes an almost
sure limit for the conditional variance while the fourth result, Lemma 11, establishes an upper bound on
the conditional third moment of the estimator. These two results are used in establishing Theorem 3 in
Appendix E. The fifth result, Lemma 12, demonstrates that E{µ(x)} is H¨older smooth when µ is the smooth
(cid:98) (cid:98)
covariate-density-adaptedkernelregression(Estimator3b),whilethesixthresult,Lemma13,demonstratesthis
estimator is bounded if the outcome is bounded.
Lemma 8. (Covariate-density-adapted kernel regression variance and covariance upper bounds) Suppose As-
sumptions 1, 2, 3, and 4 hold, and µ(x) is either a higher-order or smooth covariate-density-adapted kernel
(cid:98)
regression estimator (Estimator 3a or 3b) for µ(x) constructed on D . Then,
µ
1
supV{µ(x)}≲ , and (44)
(cid:98) nhd
x∈X
E(cid:2)
|cov{µ(X ),µ(X )|X ,X
}|(cid:3)≲ 1
(45)
(cid:98) i (cid:98) j i j n
Proof. For the variance upper bound, we have
V{µ(x)}=V(cid:40) (cid:88)n K(cid:0)Xi h−x(cid:1) µ(X i)(cid:41) +E(cid:34) V(cid:40) (cid:88)n K(cid:0)Xi h−x(cid:1) Y
i
|Xn(cid:41)(cid:35)
(cid:98) nhdf(X ) nhdf(X ) µ
i i
i=1 i=1
≲E(cid:40) K(cid:0)Xi h−x(cid:1)2
µ(X
i)2(cid:41) +E(cid:40) K(cid:0)Xi h−x(cid:1)2 (cid:41)
nh2df(X )2 nh2df(X )2
i i
1
≲ ,
nhd
where the first line follows by the law of total variance, the second by iid data and Assumptions 1 and 2, and
the third line follows by the assumption on the kernel that (cid:82) K(x)2dx ≲ 1 and Assumptions 1 and 2. The
uniform bound follows because X is compact.
For the covariance, since the estimator is localized, by the same argument as Lemmas 5 and 7
E(cid:2) |cov{µ(X ),µ(X )|X ,X }|(cid:3) ≤ supV{µ(x)}P(∥X −X ∥≤2h)≲ 1 .
(cid:98) i (cid:98) j i j (cid:98) i j n
x∈X
Lemma 9. (Covariate-density-adapted kernel regression variance lower bounds) Suppose Assumptions 1, 2, 4,
and 5 hold and µ(x) is a either a higher-order or smooth covariate-density-adapted kernel regression estimator
(cid:98)
(Estimator 3a or 3b) for µ(x) constructed on D . Then,
µ
1
inf V{µ(x)}≳ . (46)
x∈X (cid:98) nhd
Proof. We have,
V{µ(x)}=V(cid:2)E{µ(x)|Xn}(cid:3) +E(cid:2)V{µ(x)|Xn}(cid:3)
(cid:98) (cid:98) µ (cid:98) µ
≥0+E(cid:34) V(cid:40) 1 (cid:88)n K(cid:0)Xi h−x(cid:1) Y
i
(cid:12) (cid:12)Xn(cid:41)(cid:35)
n f(X )hd (cid:12) µ
i
i=1
1
(cid:40) K(cid:0)X−x(cid:1)2 (cid:41)
= E h V(Y |X)
nh2d f(X)2
361 (cid:90) (cid:18) t−x(cid:19)2 V(Y |X =t)
= K dt
nh2d h f(t)
t∈R
1 (cid:90) (cid:18) t−x(cid:19)2
≳ K dt
nh2d h
t∈R
1 (cid:90)
= K(u)2du u=(t−x)/h
nhd
u∈R
1
≳ ,
nhd
where the second inequality follows by Assumption 1 and 2 (specifically, because we assume 0 < f(x),V(Y |
x) < C for all x ∈ X), and the final line by the definition of the kernel in Estimator 3a and 3b (specifically,
because (cid:82) K(u)2du≍1). These bounds hold for arbitrary x∈X, and thus hold for the infimum over all x∈X
since X is compact by Assumption 2.
Lemma 10. (Covariate-density-adaptedkernelregressionconditionalvariancelowerbounds)SupposeAssump-
tions 1, 2, 4, 5, and 6 hold and µ(x) is a either a higher-order or smooth covariate-density-adapted kernel
(cid:98)
regression estimator (Estimator 3a or 3b) for µ(x) constructed on D . Then, when nhd ≍ n−α for α > 0 as
µ
n→∞,
(cid:26) Y2 (cid:27) (cid:26) K(X)2(cid:27)
nhdV{µ(X)|D }−a. →s. E E . (47)
(cid:98) µ f(X) f(X)
Proof. We will consider the diagonal variance terms and off-diagonal covariance terms separately
1
(cid:88)n (cid:40) K(cid:0)Xi−X(cid:1) (cid:41)
V{µ(X)|D }= V h Y |X ,Y
(cid:98) µ n2 hdf(X ) i i i
i
i=1
 (cid:16) (cid:17) 
+
1 (cid:88)n (cid:88) covK(cid:0)Xi h−X(cid:1)
Y
,K Xj h−X
Y |X ,Y ,X ,Y

.
n2 hdf(X ) i hdf(X ) j i i j j
 i j 
i=1 j̸=i
For the diagonal terms,
nhd
1 (cid:88)n V(cid:40) K(cid:0)Xi h−X(cid:1)
Y |X ,Y
(cid:41)
=
1 (cid:88)n Y i2 V(cid:26) K(cid:18) X i−X(cid:19)
|X
(cid:27)
.
n2 hdf(X ) i i i n f(X )2hd h i
i i
i=1 i=1
Notice that the right-hand side is an average of non-negative bounded random variables because Y2 is upper
bounded and f(X)2 is lower bounded away from zero by assumption, and because
1
(cid:26) (cid:18)
X
−X(cid:19) (cid:27)
1
(cid:40) (cid:18)
X
−X(cid:19)2 (cid:41)
0≤ V K i |X ≤ E K i |X
hd h i hd h i
1 (cid:90) (cid:18) X −t(cid:19)2
= K i f(t)dt
hd h
X
(cid:90)
= K(u)2f(X −uh)du≍1,
i
X
where the final line follows by a change of variables and because the density is upper and lower bounded and
(cid:82) K(u)2du≍1 by assumption.
Therefore, the diagonal terms, multiplied by nhd, are a sample average of bounded random variables with
common mean. By a strong law of large numbers for triangular arrays of bounded random variables (Lemma
24),
nhd
1 (cid:88)n V(cid:40) K(cid:0)Xi h−X(cid:1)
Y |X ,Y
(cid:41)
−a. →s. lim
E(cid:20) Y i2 V(cid:26) K(cid:18) X i−X(cid:19)
|X
(cid:27)(cid:21)
, (48)
n2 hdf(X i) i i i n→∞ f(X i)2hd h i
i=1
37should the limit on the right-hand side exist. Indeed, this limit exists. First, notice that
(cid:20) Y2 (cid:26) (cid:18) X −X(cid:19) (cid:27)(cid:21)
lim E i V K i |X
n→∞ f(X i)2hd h i
(cid:90) E(Y2 |X =s)(cid:34) (cid:90) (cid:18) s−t(cid:19)2 (cid:26)(cid:90) (cid:18) s−t(cid:19) (cid:27)2(cid:35)
= lim K f(t)dt− K f(t)dt ds
n→∞ f(s)hd h h
X X X
(cid:90) E(Y2 |X =s)(cid:26)(cid:90) (cid:27) (cid:90) E(Y2 |X =s)(cid:26)(cid:90) (cid:27)2
= lim K(u)2f(s+uh)du ds−hd K(u)f(s+uh)du ds.
n→∞ f(s) f(s)
X U X U
(49)
where the second equality follows by a change of variables, linearity of integration, and the symmetry of K. By
the assumed upper bound on Y and lower bound on f(X) and the integrability of K, and because hd n −→ →∞ 0,
the limit of the second summand is zero.
Meanwhile, by the boundedness of Y and f(X), the integrability of K2, and Fubini’s theorem,
(cid:90) E(Y2 |X =s)(cid:26)(cid:90) (cid:27) (cid:90) (cid:90) f(s+uh)
K(u)2f(s+uh)du ds= E(Y2 |X =s)K(u)2 duds.
f(s) f(s)
X U X U
Moreover, by the assumed continuity of f, K(u)2f(s+uh) n −→ →∞ K(u)2f(s) uniformly in u at all s except for
a set of Lebesgue measure-zero on the boundary of X. Indeed, at those points, if u “points” outside X, then
the limit is zero because f(s+uh)=0 for all h. This, combined with the boundedness of Y, f, and K and the
integrability of K2, implies, by the dominated convergence theorem, that
(cid:90) E(Y2 |X =s)(cid:26)(cid:90) (cid:27) (cid:90) (cid:90) f(s+uh)
lim K(u)2f(s+uh)du ds= E(Y2 |X =s)K(u)2 lim duds
n→∞ f(s) n→∞ f(s)
X U X U
(cid:90) (cid:90)
= E(Y2 |X =s)K(u)2duds
X X
(cid:26)(cid:90) (cid:27)(cid:26)(cid:90) (cid:27)
= E(Y2 |X =s)ds K(u)2du
X X
(cid:26) Y2 (cid:27) (cid:26) K(X)2(cid:27)
=E E (50)
f(X) f(X)
Therefore, because the limits of both summands in (49) exist, the limit of the difference is the difference of the
limits. Hence, combining (48) and (50) yields
nhd
1 (cid:88)n V(cid:40) K(cid:0)Xi h−X(cid:1)
Y |X ,Y
(cid:41)
−a. →s.
E(cid:26) Y2 (cid:27) E(cid:26) K(X)2(cid:27)
. (51)
n2 hdf(X ) i i i f(X) f(X)
i
i=1
Next, consider the sum of off-diagonal covariance terms. First, because the kernel is localized, notice that
when the covariates are far apart such that ∥X −X ∥ > 2h, then the two terms inside the covariance do not
i j
sharenon-zerosupportbecauseK(x/h)≲1(∥x∥≤h). Forf(X)andg(X)thatdonotsharenon-zerosupport,
E{f(X)g(X)}=0 and so |cov{f(X),g(X)}|=|E{f(X)}E{g(X)}|. In that case,
(cid:12)  (cid:16) (cid:17) (cid:12) (cid:12)  (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)cov K hd(cid:0) fX (i Xh−X i)(cid:1)
Y
i,K hdfX (j Xh−X
j) Y j |X i,Y i,X j,Y
j (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)=(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)E(cid:40) K hd(cid:0) fX (i Xh−X i)(cid:1)
Y
i(cid:41) E K hdfX (j Xh−X
j) Y
j (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
≲(cid:12) (cid:12)
(cid:12)
1 (cid:90) K(cid:18) X i−x(cid:19) dx(cid:90) K(cid:18) X j −x(cid:19) dx(cid:12) (cid:12)
(cid:12)
(cid:12)h2d h h (cid:12)
=1 (52)
wherethesecondlinefollowsbylowerboundeddensityandupperboundedoutcome,whilethefinallinefollows
(cid:82)
by a change of variables and because K(x)dx=1.
38Otherwise, when the covariates are far apart, the covariance can be upper bounded by the product of
standard deviations by Cauchy-Schwarz, i.e.,
(cid:118)
(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)cov  K hd(cid:0) fX (i Xh−X i)(cid:1)
Y
i,K h(cid:16) dfX (j Xh−X j)(cid:17)
Y j |X i,Y i,X j,Y
j  (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)≤(cid:118) (cid:117) (cid:117) (cid:116)V(cid:40) K hd(cid:0) fX (i Xh−X i)(cid:1)
Y i |X i,Y
i(cid:41)(cid:117) (cid:117) (cid:117) (cid:116)V  K h(cid:16) dfX (j Xh−X j)(cid:17)
Y j |X j,Y
j 

(cid:115) (cid:115)
(cid:26) (cid:18) (cid:19) (cid:27) (cid:26) (cid:18) (cid:19) (cid:27)
1 X −X X −X
≲ V K i |X V K j |X
h2d h i h j
1
≲ , (53)
hd
wherethesecondlinefollowsbecauseY andf(X)areupperandlowerbounded,respectively,byassumptionand
X andX areiid,andthethirdlinefollowsbecauseV(cid:110) K(cid:16) Xj−X(cid:17) |X (cid:111) =hd(cid:82) K(u)2du−h2d(cid:8)(cid:82) K(u)du(cid:9)2 ≲
i j h j
hd by a change of variables because (cid:82) K(u)2du≲1 by assumption.
Then, the sum of off-diagonal covariance terms can be bounded by counting how many covariates are close
and multiplying the count by the upper bound 1 discussed in the previous paragraph. Let P denote (two
hd n
times) the number of close covariate pairs as, i.e.,
n
(cid:88)(cid:88)
P = 1(∥X −X ∥≤2h). (54)
n i j
i=1 j̸=i
Combining (52), (53), and (54), we have
(cid:12)  (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n1 2 (cid:88) i=n 1(cid:88)
j̸=i
cov K hd(cid:0) fX (i Xh−X i)(cid:1) Y i,K hdfX (j Xh−X j) Y j |X i,Y i,X j,Y j (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≲ P nn 2 h1 d +1. (55)
Lemma 22 establishes that Pn −a. →s. 0 under the assumed condition on the bandwidth that nhd ≍n−α for some
n
α>0. Hence,
nhd (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n1 2 (cid:88) i=n 1(cid:88)
j̸=i
cov  K hd(cid:0) fX (i Xh−X i)(cid:1) Y i,K h(cid:16) dfX (j Xh−X j)(cid:17) Y j |X i,Y i,X j,Y j  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≲ P nn +nhd −a. →s. 0. (56)
In conclusion, (51) and (56) and the continuous mapping theorem imply the result.
Lemma 11. (Covariate-density-adaptedkernelregressionthirdmomentupperbound)SupposeAssumptions1,
2, 4, and 5 hold and µ(x) is a either a higher-order or smooth covariate-density-adapted kernel regression
(cid:98)
estimator (Estimator 3a or 3b) for µ(x) constructed on D . Then, when nhd ≍n−α for α>0 as n→∞,
µ
nh3 2dE{|µ (cid:98)(X)|3 |Dn}−a. →s. 0. (57)
Proof. We have
n n n (cid:12) (cid:26) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:27)(cid:12)
E{|µ (cid:98)(X)|3 |Dn}≲ n31 h3d (cid:88)(cid:88)(cid:88)(cid:12) (cid:12) (cid:12)E K X i h−X K X j h−X K X k h−X |X i,X j,X k (cid:12) (cid:12) (cid:12).
i=1j=1k=1
by Assumption 2 and Assumption 5 (bounded density and Y). By the localizing property of the kernel, all
three covariates must be close to share non-zero support, and then the expectation of their product is ≲hd by
the boundedness of the covariate density. Otherwise, E(cid:110) K(cid:0)Xi h−X(cid:1) K(cid:16) Xj h−X(cid:17) K(cid:0)Xk h−X(cid:1) |X i,X j,X k(cid:111) = 0.
Therefore, it suffices to consider the cases when all three covariates are close.
39First, notice that the triple sum can be decomposed as
n n n
(cid:88)(cid:88)(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
= + + + + ,
i=1j=1k=1 i=j=k i̸=j=k i=j̸=k i=k̸=j i̸=j̸=k
i.e., there are n permutations where the indexes are the same, 3 sets of double sums where two indexes are
the same, left-overs are a U-statistic of order 3. Letting P denote twice the number of covariate pairs, as in
n
Lemma 22, and Q
:=(cid:80)n
1(∥X −X ∥≤2h)1(∥X −X ∥≤2h)1(∥X −X ∥≤2h), it follows that
n i̸=j̸=k i j i k j k
n n n
(cid:88)(cid:88)(cid:88)
1(∥X −X ∥≤2h)1(∥X −X ∥≤2h)1(∥X −X ∥≤2h)=n+3P +Q
i j i k j k n n
i=1j=1k=1
because the observations are iid. Hence,
n31
h3d
(cid:88)n (cid:88)n (cid:88)n (cid:12) (cid:12) (cid:12) (cid:12)E(cid:26) K(cid:18) X i h−X(cid:19) K(cid:18) X j h−X(cid:19) K(cid:18) X k h−X(cid:19) |X i,X j,X k(cid:27)(cid:12) (cid:12) (cid:12) (cid:12)≲ n3h hd
3d
(n+3P n+Q n).
i=1j=1k=1
(58)
Therefore,
nh3 2dE{|µ (cid:98)(X)|3 |Dn}≲ nn 3h h3 32d
d
(cid:88)n (cid:88)n (cid:88)n (cid:12) (cid:12) (cid:12) (cid:12)E(cid:26) K(cid:18) X i h−X(cid:19) K(cid:18) X j h−X(cid:19) K(cid:18) X k h−X(cid:19) |X i,X j,X k(cid:27)(cid:12) (cid:12) (cid:12)
(cid:12)
i=1j=1k=1
≲
nh5 2d
(n+P +Q )=
n+P n+Q
n −a. →s. 0,
n3h3d n n n2hd/2
where the convergence results follows by Lemmas 22 and 23, which establish Pn −a. →s. 0 and Qn −a. →s. 0, and the
n n
condition on the bandwidth that ε< 4(α+β), which implies 1 =o(1).
d nhd/2
Our penultimate result shows that the smooth covariate-density-adapted kernel regression, averaged over
the training points, is itself H¨older smooth. Notice that the result relies on the kernel being continuous, which
is a mild assumption, but may not hold for the higher-order kernel.
Lemma 12. (Smooth covariate-density-adapted kernel regression is H¨older smooth) Suppose Assumptions 1,
2, 3, and 4 hold, and µ(x) is a smooth covariate-density-adapted kernel regression estimator (Estimator 3b).
(cid:98)
Then,
E{µ(x)}∈H¨older(β).
(cid:98)
Proof. To establish that E{µ(x)}∈H¨older(β), we will show that (1) it is ⌊β⌋-times continuously differentiable
(cid:98)
withboundedpartialderivatives,and(2)its⌊β⌋orderpartialderivativessatisfytheH¨oldercontinuitycondition.
For x∈X,
(cid:40) 1 (cid:88)n K(cid:0)Xi−x(cid:1) (cid:41) 1 (cid:90) (cid:18) t−x(cid:19) (cid:90)
E{µ(x)}=E h Y = K µ(t)dt= K(u)µ(uh+x)du,
(cid:98) n hdf(X ) i hd h
i
i=1
by the definition of the estimator and substitution. Let Dj denote an arbitrary multivariate partial derivative
operator of order j >0. Then, for j ≤⌊β⌋,
(cid:90) (cid:90)
DjE{µ(x)}=Dj K(u)µ(uh+x)du= K(u)Djµ(uh+x)du,
(cid:98)
wherethesecondequalityfollowsbythecontinuityandintegrabilityassumptionsonK(u)andLeibniz’integral
rule. Because µ ∈ H¨older(β) by Assumption 3, Djµ(uh+x) exists and is continuous. Moreover, for any two
continuous functions f and g, (cid:82) f(x)g(x)dx is continuous, and therefore DjE{µ(x)} exists and is continuous.
(cid:98)
40For boundedness, notice that
(cid:12)(cid:90) (cid:12) (cid:90)
(cid:12) (cid:12)DjE{µ (cid:98)(x)}(cid:12) (cid:12)=(cid:12) (cid:12) K(u)Djµ(uh+x)du(cid:12) (cid:12)≤ |K(u)|(cid:12) (cid:12)Djµ(uh+x)(cid:12) (cid:12)du≲1,
(cid:12) (cid:12)
because µ ∈ H¨older(β) by Assumption 3 and by the integrability of K. Finally, for the H¨older continuity
condition on the ⌊β⌋ derivative, notice that for x,x′ ∈X,
(cid:12) (cid:12) (cid:12)D⌊β⌋E{µ (cid:98)(x)}−D⌊β⌋E{µ (cid:98)(x′)}(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) K(u)D⌊β⌋µ(uh+x)du−(cid:90) K(u)D⌊β⌋µ(uh+x′)du(cid:12) (cid:12) (cid:12)
(cid:12)
=(cid:12) (cid:12) (cid:12)(cid:90) K(u)(cid:110) D⌊β⌋µ(uh+x)−D⌊β⌋µ(uh+x′)(cid:111) du(cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:90) (cid:12) (cid:12)
≤ |K(u)|(cid:12)D⌊β⌋µ(uh+x)−D⌊β⌋µ(uh+x′)(cid:12)du
(cid:12) (cid:12)
(cid:90)
≲ |K(u)|∥x−x′∥β−⌊β⌋du
≲∥x−x′∥β−⌊β⌋,
wherethefirstlinefollowsbythesameargumentasabove,thesecondbylinearityoftheintegral,thepenultimate
line by the H¨older assumption of µ, and the final line by the integrability assumption on the kernel. Therefore,
E{µ(x)} satisfies the conditions of being a H¨older(β) smooth function.
(cid:98)
Ourfinalresultestablishesthatthesmoothcovariate-densityadaptedkernelregressionestimatorisbounded
if the relevant outcome is bounded.
Lemma 13. (Smoothcovariate-density-adaptedkernelregressionisbounded) Suppose Assumptions 1, 2, 3, 4,
and 5 hold, and µ(x) is a smooth covariate-density-adapted kernel regression estimator (Estimator 3b). Then,
(cid:98)
there exists M >0 such that |µ(X)|≤M.
(cid:98)
Proof. This follows immediately because the covariate density and outcome are bounded by assumption, and
the kernel is bounded by construction.
E Section 5 results: proofs of Theorems 2 and 3
For Theorems 2 and 3, we use properties of Sobolev smooth functions. Let L (Rd) denote the space of p-fold
p
Lebesgue-integrable functions, i.e.,
(cid:26) (cid:90) (cid:27)
L (Rd)= f :Rd →R: |f(x)|pdx<∞ .
p
Rd
We will denote the class of Sobolev(s,p) smooth functions as Hs(Rd). For s ∈ N, these classes can be defined
p
as
 
Hs(Rd)=
f ∈L (Rd):Dtf ∈L (Rd)∀
|t|≤s:(cid:18)(cid:90) |f(x)|pdx(cid:19)1/p
+
(cid:88)
∥Dtf∥
<∞
,
p p p p
 Rd 
|s|=t
where Dt is the multivariate partial derivative operator (see Section 1.2). One can also define Sobolev smooth
functions for non-integer s through their Fourier transform (e.g., Gin´e and Nickl [2021] Chapter 4). We will
omit such a definition here because it requires much additional and unnecessary notation, but still use Hs(Rd)
p
to refer to such function classes. Importantly, H¨older(s) = Hs (Rd), and Hs (Rd) ⊆ Hs(Rd) for p ≤ ∞, i.e.,
∞ ∞ p
H¨older classes are contained within Sobolev classes of the same smoothness.
We begin with the following result, Lemma 14, which is used in the proof of Theorem 2. Lemma 14 follows
very closely from Theorem 1 in Gin´e and Nickl [2008a] (also, Lemmas 4.3.16 and 4.3.18 in Gin´e and Nickl
[2021]). The higher order property of the kernel in Estimator 3a allows us to generalize the result to higher
smoothness.
41Lemma 14. Suppose Assumptions 1, 2, 3, and 4 hold, and µ(x) is a higher-order covariate-density-adapted
(cid:98)
kernel regression estimator (Estimator 3a) for µ(x) constructed on D . Let g ∈H¨older(α). Then,
µ
(cid:12) (cid:16) (cid:104) (cid:105) (cid:17)(cid:12)
sup(cid:12)E g(X) E{µ(X)|X}−µ(X) |X =x (cid:12)≲hα+β.
(cid:12) (cid:98) (cid:12) µ
x∈X
Proof. Let h≡h throughout. First note that
µ
 
 (cid:88)
K(cid:0)Xi−x(cid:1)

E{µ(x)}=E h Y
(cid:98) nhdf(X ) i
 i 
Zi∈Dµ
(cid:40) K(cid:0)X−x(cid:1) (cid:41)
=E h µ(X)
hdf(X)
(cid:90) K(cid:0)t−x(cid:1)
= h µ(t)dt.
hd
t∈X
SinceX iscompactinRd,weevaluatethefollowingintegralsoverRd,withtheunderstandingthatoutsidethe
relevantsetstheintegrandevaluatestozero(e.g.,afterthechangeofvariables). Then,lettingg(x)f(x)=gf(x),
h(x)=h(−x), and ∗ denote convolution,
(cid:16) (cid:104) (cid:105) (cid:17) (cid:90) (cid:26)(cid:90) 1 (cid:18) t−x(cid:19) (cid:27)
E g(X) E{µ(X)|X}−µ(X) |X =x = gf(x) K µ(t)dt−µ(x) dx
(cid:98) hd h
x∈Rd t∈Rd
(cid:90) (cid:26)(cid:90) (cid:27)
= gf(x) K(−u)µ(x−uh)du−µ(x) dx u=(x−t)/h
x∈Rd u∈Rd
(cid:90) (cid:26)(cid:90) (cid:27)
= gf(x) K(u)µ(x−uh)du−µ(x) dx
x∈Rd u∈Rd
(cid:90) (cid:20)(cid:90) (cid:21)
= gf(x) K(u){µ(x−uh)−µ(x)}du dx
x∈Rd u∈Rd
(cid:90) (cid:20)(cid:90) (cid:21)
= K(u) gf(x){µ(x−uh)−µ(x)}dx du
u∈Rd x∈Rd
(cid:90) (cid:20)(cid:90) (cid:21)
= K(u) gf(x)µ(uh−x)−gf(x)µ(−x)dx du
u∈Rd x∈Rd
(cid:90)
= K(u){gf ∗µ(uh)−gf ∗µ(0)}du.
u∈Rd
where the first line follows by definition, the second by substitution, the third because K is symmetric, the
(cid:82)
fourth because K =1, the fifth by Fubini’s theorem, and the last two again by definition.
Next, notice that gf ∈H¨older(α)⊆Hα(R) because g ∈H¨older(α) and f ∈H¨older(α∨β) by Assumption 4,
2
and µ∈H¨older(β) =⇒ µ∈H¨older(β)⊆Hβ(R). Therefore, by Lemma 12 and Remark 11i in Gin´e and Nickl
2
[2008b], gf ∗µ∈H¨older(α+β).
The rest of the proof continues by a standard Taylor expansion analysis of higher-order kernels. See, e.g.,
Scott[2015]Chapter6. LetDjf denotethemultivariatepartialderivativeoff oforderj andletη(x)=gf∗µ(x)
for simplicity. Then, we have
(cid:90)
K(u){η(uh)−η(0)}du
u
 
=(cid:90) K(u) (cid:88) Djη(0) (uh)j + (cid:88) ⌊α+β⌋(cid:90) 1 (1−t)⌊α+β⌋−1(cid:8) Dkη(tuh)−Dkη(0)(cid:9) (uh)⌊α+β⌋dtdu
j! k!
u 0
0<|j|<⌊α+β⌋−1 |k|=⌊α+β⌋
(cid:90)
≲ K(u)(h∥u∥)α+β−⌊α+β⌋(h∥u∥)⌊α+β⌋du
u∈Rd
(cid:90)
=hα+β K(u)∥u∥α+βdu≲hα+β,
u∈Rd
42where the first line follows by a Taylor expansion of the difference η(uh)−η(0); the second because (1) η ∈
H¨older(α+β), (2) the kernel is of order at least ⌈α+β⌉, (3) |uk| ≤ ∥u∥k (where ∥·∥ is the Euclidean norm),
and (4) (cid:82)1 (1−t)⌊β⌋−1 = 1 ; and the final line follows again by assumption on the kernel.
0 ⌊β⌋
The supremum over x∈X follows because X is compact by assumption.
Theorem 2. (Minimax optimality) Suppose Assumptions 1, 2, 3, and 4 hold. If ψ is estimated with
ecc
the DCDR estimator ψ(cid:98)n from Algorithm 1, one nuisance function is estimated with the smooth covariate-
density-adapted kernel regression (Estimator 3b) with bandwidth decreasing at any rate such that the estimator
isconsistent, andtheothernuisancefunctionisestimatedwiththehigher-ordercovariate-density-adaptedkernel
−2
regression (Estimator 3a) with bandwidth that scales at n2α+2β+d, then

(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4,
(10)
(cid:16) (cid:17)
E|ψ(cid:98)n−ψ ecc|=OP n− 2α2α ++ 2β2 +β d otherwise.
Proof. Assumewithoutlossofgeneralitythatπ istheconsistentestimatorandµtheundersmoothedestimator,
(cid:98) (cid:98)
with h
µ
≍n− 2α+2 2β+d. Since µ
(cid:98)
and π
(cid:98)
were trained on separate independent samples, the bias satisfies
E(cid:16) ψ(cid:98)n−ψ(cid:17) =E{φ (cid:98)(Z)−φ(Z)}=E(cid:0)(cid:2)E{µ (cid:98)(X)|X}−µ(X)(cid:3)(cid:2)E{π (cid:98)(X)|X}−π(X)(cid:3)(cid:1) .
Lemma 12 demonstratesthat E{π(x)}∈H¨older(α) underthe assumptions given onthe kernel inEstimator 3b.
(cid:98)
Therefore, E{π(x)}−π(x)∈H¨older(α)⊆Hα(Rd). Thus, by Lemma 14,
(cid:98) 2
(cid:12) (cid:16) (cid:17)(cid:12)
(cid:12) (cid:12)E ψ(cid:98)n−ψ (cid:12) (cid:12)≲hα µ+β ≍n− 2α2α ++ 2β2 +β d. (59)
Because φ is Lipschitz in its nuisance functions, and by the same arguments as in Lemma 1 and Proposition 1,
and by (45) in Lemma 8, the remainder term in Lemma 1 satisfies
(cid:32) (cid:33)
∥b2∥ +∥s2∥ +∥b2∥ +∥s2∥
R
2,n
=OP π ∞ π ∞
n
µ ∞ µ ∞ ,
Then, by (44) in Lemma 8,
(cid:18) 1 (cid:19) (cid:16) (cid:17)
R 2,n =OP n2hd =OP n− 2α4α ++ 2β4 +β d . (60)
µ
√
Hence, when α+β >d/4, theCLTtermdominatestheexpansion—asinTheorem1—whereasinthenon- n
2
regime bias and variance are balanced.
√
Theorem 3. (Slower-than- n CLT) Under the conditions of Theorem 2, suppose α+β < d and Assump-
2 4
tions 5 and 6 hold. Suppose µ is the undersmoothed nuisance function estimator with bandwidth h scaling at
(cid:98) µ
n− 2α+2+ 2βε +d for 0<ε< 4(α d+β) while π (cid:98) is the smooth consistent estimator. Then,
(cid:114) n
V{φ(Z)|D ,D
}(ψ(cid:98)n−ψ ecc)⇝N(0,1). (11)
(cid:98) π µ
Moreover,
(cid:26)V(A|X)Y2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|D ,D }−a. →s. E E µ , (12)
µ (cid:98) π µ f(X) f(X)
where K is the kernel for µ. If the roles of µ and π were reversed, then (11) holds and
µ (cid:98) (cid:98) (cid:98)
(cid:26)V(Y |X)A2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|D ,D }−a. →s. E E π . (13)
π (cid:98) π µ f(X) f(X)
43Proof. Theproofreliesonseveralhelperlemmasstatedafterthisproof. Wefocusontheregimewhere α+β < d,
√ 2 4
althoughastandardCLTcouldapplyinthesmootherregime. Inthisnon- nregime,theundersmoothedDCDR
√ √
estimator does not achieve n-convergence and we must instead prove slower-than- n convergence.
We omit Z arguments (e.g., φ(Z) ≡ φ) and let Dn = {D ,D } denote all the training data. First, note
µ π
that by Lemma 15, V(φ | Dn) > 0 almost surely, so that division by V(φ | Dn) is well-defined almost surely.
(cid:98) (cid:98)
Then, by the definition of ψ(cid:98)n,ψ ecc,φ (cid:98), and φ and adding zero and multiplying by one, we have the following
decomposition:
ψ(cid:98)n−ψ
ecc =
P nφ (cid:98)−E(φ (cid:98))
+
E(φ (cid:98)−φ)
(cid:112)V(φ|Dn)/n (cid:112)V(φ|Dn)/n (cid:112)V(φ|Dn)/n
(cid:98) (cid:98) (cid:98)
P φ−E(φ|Dn) E(φ|Dn)−E(φ) E(φ−φ)
= n(cid:98) (cid:98) + (cid:98) (cid:98) + (cid:98)
(cid:112)V(φ|Dn)/n (cid:112)V(φ|Dn)/n (cid:112)V(φ|Dn)/n
(cid:98) (cid:98) (cid:98)
 
P φ−E(φ|Dn)
(cid:115)
V(φ)
E(φ|Dn)−E(φ) E(φ−φ)
= n(cid:98) (cid:98) + (cid:98) (cid:98) (cid:98) + (cid:98)
(cid:124)(cid:112)V(φ (cid:98) (cid:123)| (cid:122)Dn)/n
(cid:125) (cid:124)
V(φ (cid:98) (cid:123)(cid:122)|Dn) (cid:125)(cid:124) (cid:112)V (cid:123)(φ (cid:122)(cid:98))/n
(cid:125)
(cid:124)(cid:112)V (cid:123)(φ (cid:122)(cid:98))/n (cid:125)
CLT T1 T2 T3
where the expectation and variance are over both the test and training data unless otherwise indicated by
conditioning. As the text underneath the underbraces indicates, we will show the limiting result for the first
term — the conditional standardized average. That the unconditional standardized average converges to the
conditional average in probability follows by Lemmas 18, 19, and 20, which establish that T
1
= OP(1), T
2
=
oP(1), and T
3
=o(1), respectively. Therefore,
T 1(T 2+T 3)=OP(1){oP(1)+o(1)}=oP(1).
Returning to the CLT term, let Φ(·) denote the cumulative distribution function of the standard normal. By
iterated expectations and Jensen’s inequality,
(cid:12) (cid:40) (cid:41) (cid:12) (cid:34) (cid:12) (cid:40) (cid:41) (cid:12) (cid:35)
(cid:12) P φ−E(φ|Dn) (cid:12) (cid:12) P φ−E(φ|Dn) (cid:12)
lim sup(cid:12)P n(cid:98) (cid:98) ≤t −Φ(t)(cid:12)≤ lim E sup(cid:12)P n(cid:98) (cid:98) ≤t|Dn −Φ(t)(cid:12)∧1 .
n→∞ t
(cid:12)
(cid:12)
(cid:112)V(φ
(cid:98)|Dn)/n
(cid:12)
(cid:12) n→∞ t
(cid:12)
(cid:12)
(cid:112)V(φ
(cid:98)|Dn)/n
(cid:12)
(cid:12)
(cid:26) (cid:27)
Conditional on Dn, the summands in P √φ(cid:98)−E(φ(cid:98)|Dn) are iid with mean zero and unit variance (almost
n V(φ(cid:98)|Dn)/n
surely). Therefore, by the Berry-Esseen inequality (Theorem 1.1, Bentkus and G¨otze [1996]),
(cid:104) (cid:105)
(cid:12) (cid:12) (cid:40) P φ−E(φ|Dn) (cid:41) (cid:12) (cid:12) E |φ (cid:98)(Z)−E{φ (cid:98)(Z)|D π,D µ}|3 |D π,D µ
sup(cid:12)P n(cid:98) (cid:98) ≤t|Dn −Φ(t)(cid:12)≲ √ −a. →s. 0,
t (cid:12) (cid:12) (cid:112)V(φ (cid:98)|Dn)/n (cid:12) (cid:12) n V{φ (cid:98)(Z)|D π,D µ}3/2
where convergence almost surely to zero follows by Lemma 16. Then, because
(cid:12) (cid:26) (cid:27) (cid:12)
sup (cid:12) (cid:12)P P√nφ(cid:98)−E(φ(cid:98)|Dn) ≤t|Dn −Φ(t)(cid:12) (cid:12)∧1 is uniformly integrable and converges almost surely to zero, conver-
t(cid:12) V(φ(cid:98)|Dn)/n (cid:12)
gence in L1 follows (Theorem 4.6.3, Durrett [2019]), i.e.,
(cid:34) (cid:12) (cid:40) (cid:41) (cid:12) (cid:35)
(cid:12) P φ−E(φ|Dn) (cid:12)
lim E sup(cid:12)P n(cid:98) (cid:98) ≤t|Dn −Φ(t)(cid:12)∧1 =0.
n→∞ t
(cid:12)
(cid:12)
(cid:112)V(φ
(cid:98)|Dn)/n
(cid:12)
(cid:12)
Clearly, (11) is satisfied. Meanwhile, (12) follows from Lemma 15.
Lemma 15. Under the conditions of Theorem 3, suppose without loss of generality that µ is the estimator with
(cid:98)
higher-order kernel K µ and bandwidth scaling as h µ ≍ n− 2α+2+ 2βε +d while π (cid:98) is consistent, smooth, and bounded.
Then,
(cid:26)V(A|X)Y2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|Dn}−a. →s. E E µ . (61)
µ (cid:98) f(X) f(X)
44If the roles of µ and π were reversed, then
(cid:98) (cid:98)
(cid:26)V(Y |X)A2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|Dn}−a. →s. E E π . (62)
π (cid:98) f(X) f(X)
Proof. Unlesstheyarenecessaryforclarity,weomitX andZargumentsthroughoutforbrevity(e.g.,π ≡π(X)).
By definition,
V(φ|Dn)=V{(A−π)(Y −µ)|Dn}
(cid:98) (cid:98) (cid:98)
=V{(A−π)Y |Dn}+V{(A−π)µ|Dn}+2cov{(A−π)Y,(π−A)µ|Dn}. (63)
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
Since µ is the undersmoothed estimator, one might expect the second term in (63) to dominate this expansion
(cid:98)
and scale like V{µ(X)|Dn}. We show this below.
(cid:98)
Starting with the first term in (63), we have
V{(A−π)Y |Dn}=O(1)
(cid:98)
by the boundedness assumption on A and Y in Assumption 5 and because π is bounded by construction
(cid:98)
(Lemma 13). Then, notice that the third term in (63) is upper bounded by the square root of the second term:
by Cauchy-Schwarz and because V{(A−π)Y}=O(1),
(cid:112)
2|cov{(A−π)Y,(π−A)µ|Dn}|≲ V{(π−A)µ|Dn}.
(cid:98) (cid:98) (cid:98) (cid:98)
Hence, demonstrating that the second term in (63) satisfies the almost sure limit when standardized by nhd
µ
ensures it will dominate the expansion.
We have
V{(A−π)µ}=V{(π−π)µ|Dn}+E{V(A|X)µ2 |Dn}. (64)
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
We will show that the first summand, when scaled by nhd, converges to zero almost surely while the second
summand satisfies the result.
For the first summand in (64), we have
nhdV{(π−π)µ|Dn}=
1 (cid:88) Y i2 V(cid:20) {π(X)−π(X)}K(cid:18) X i−X(cid:19)
|X
(cid:21)
+A (65)
µ (cid:98) (cid:98) n f(X )2hd (cid:98) h i n
Dµ i µ µ
a.s.
where A is the off-diagonal covariance terms. A −→ 0 because (π −π) is bounded by Assumption 5 and
n n (cid:98)
Lemma 13, and by the same argument as in Lemma 10.
The diagonal terms in (65) are a sample average of bounded random variables with common mean. Hence,
by the strong law of large numbers for triangular arrays of bounded random variables (Lemma 24) and the
continuous mapping theorem,
(cid:18) Y2 (cid:20) (cid:18) X−X′(cid:19) (cid:21)(cid:19)
nhdV{(π−π)µ|Dn}−a. →s. lim E V {π(X′)−π(X′)}K |X +0, (66)
µ (cid:98) (cid:98) n→∞ f(X)2hd (cid:98) h
µ
should the limit on the right-hand side exist. Indeed, this limit exists, and is zero. Notice that the expectation
is taken over all the training data — both D and D . Therefore,
µ π
(cid:18) Y2 (cid:20) (cid:18) X−X′(cid:19) (cid:21)(cid:19) (cid:32) Y2 (cid:34) (cid:18) X−X′(cid:19)2 (cid:35)(cid:33)
E V {π(X′)−π(X′)}K |X ≤E E {π(X′)−π(X′)}2K |X
f(X)2hd (cid:98) h f(X)2hd (cid:98) h
µ µ µ µ
=E(cid:40) Y2 E(cid:32)
E (cid:2) {π(X′)−π(X′)}2 |D ,X,X′(cid:3)
K(cid:18) X−X′(cid:19)2 |X(cid:33)(cid:41)
f(X)2hd Dπ (cid:98) µ h
µ µ
45≤ sup E (cid:2)
{π(x′)−π(x′)}2(cid:3)E(cid:34) Y2 E(cid:40) K(cid:18) X−X′(cid:19)2 |X(cid:41)(cid:35)
Dπ (cid:98) f(X)2hd h
x′∈X µ µ
=o(1),
where the last line follows because sup E (cid:2) {π(x′)−π(x′)}2(cid:3) =o(1) by Lemma 8 and because the second
x′∈X Dπ (cid:98)
multiplicand in the penultimate line is upper bounded (we added the D subscript to emphasize that this
π
expectation is over the training data for π).
(cid:98)
For the second summand in (64),
nhdE{V(A|X)µ2 |Dn}=
1 (cid:88)n Y i2 E(cid:40) K(cid:18) X i−X(cid:19)2
V(A|X)|X
(cid:41)
+A
µ (cid:98) n f(X )2hd h i n
i=1 i µ µ
where A is the off-diagonal product terms. A −a. →s. 0 because V(A | X) is bounded by Assumption 1 and by
n n
the same argument as in Lemma 10.
Forthediagonalterms,becausetheyareasampleaverageofboundedrandomvariableswithcommonmean,
by a strong law of large numbers for triangular arrays (Lemma 24),
1 (cid:88)n Y i2 E(cid:40) K(cid:18) X i−X(cid:19)2
V(A|X)|X
(cid:41)
−a. →s. lim
E(cid:34) Y2 E(cid:40) K(cid:18) X−X′(cid:19)2 V(A|X′)|X(cid:41)(cid:35)
,
n i=1 f(X i)2hd µ h µ i n→∞ f(X)2hd µ h µ
should the limit on the right-hand side exist. The rest of the proof follows by the same argument as in Lemma
10. We have, by a change of variables and the symmetry of K,
(cid:20) Y2 (cid:26) (cid:18) X−X′(cid:19) (cid:27)(cid:21)
lim E E K V(A|X′)|X =
n→∞ f(X)2hd µ h µ
(cid:90) E(Y2 |s)(cid:26)(cid:90) (cid:27)
lim K(u)2V(A|s+uh)f(s+uh)du ds.
n→∞ f(s)
X U
By the boundedness of Y and f(X), the integrability of K2, and Fubini’s theorem, we can exchange integrals.
Then, by the assumed continuity of f and V(A|x),
K(u)2f(s+uh)V(A|s+uh)n −→ →∞ K(u)2f(s)V(A|s)
uniformlyinuatallsexceptforasetofLebesguemeasure-zeroontheboundaryofX. Indeed,atthosepoints,
if u “points” outside X, then the limit is zero because f(s+uh)V(A | s+uh) = 0 for all h. This, combined
with the boundedness of Y, f, A, and K and the integrability of K2, implies, by the dominated convergence
theorem, that
1 (cid:88)n Y i2 E(cid:40) K(cid:18) X i−X(cid:19)2
V(A|X)|X
(cid:41)
−a. →s.
(cid:90) (cid:90)
E(Y2 |X =s)K(u)2V(A|s)duds
n f(X )2hd h i
i=1 i µ µ X U
(cid:26)E(Y2 |X)V(A|X)(cid:27) (cid:26) K(X)2(cid:27)
=E E . (67)
f(X) f(X)
Then, plugging (67) into (64) and by the continuous mapping theorem,
(cid:26)V(A|X)Y2(cid:27) (cid:26) K(X)2(cid:27)
nhdV{(A−π)µ}−a. →s. E E .
µ (cid:98) (cid:98) f(X) f(X)
The result follows because nhdV{(A−π)µ} dominates the expansion in (63). The same argument follows with
µ (cid:98) (cid:98)
the roles of π and µ reversed, but swapping the roles of Y and A and swapping h and K for h and K .
(cid:98) (cid:98) µ µ π π
46Lemma 16. Under the setup from Theorem 3,
(cid:104) (cid:105)
E |φ(Z)−E{φ(Z)|D ,D }|3 |D ,D
(cid:98) (cid:98) π µ π µ
a.s.
√ −→0. (68)
n V{φ(Z)|D ,D }3/2
(cid:98) π µ
Proof. Assumewithoutlossofgeneralitythatπisthesmoothestimator(Estimator3b)andµisthehigher-order
(cid:98) (cid:98)
kernelestimator(Estimator3a)sothatnhd →0asn→∞,whereh isthebandwidthofthecovariate-density-
µ µ
adapted kernel regression estimator. By Lemma 15, the denominator in (68) satisfies
nh µ3 2d√ nV{φ (cid:98)(Z)|Dn}3/2 =(cid:2) nhd µV{φ (cid:98)(Z)|Dn}(cid:3)3/2 −a. →s. E(cid:26)V(A f(| XX ))Y2(cid:27)3/2 E(cid:26) K f( (X X) )2(cid:27)3/2 . (69)
Meanwhile, the numerator in (68) satisfies
(cid:104) (cid:105) (cid:104) (cid:105)
E |φ(Z)−E{φ(Z)|Dn}|3 |Dn =E |AY −E(AY)+π(X){µ(X)−Y}+µ(X){π(X)−A}|3 |Dn
(cid:98) (cid:98) (cid:98) (cid:98)
(cid:104) (cid:105)
≲E |AY −E(AY)|3 |Dn
(cid:104) (cid:105)
+E |π(X){µ(X)−Y}|3 |Dn
(cid:98)
(cid:104) (cid:105)
+E |µ(X){π(X)−A}|3 |Dn
(cid:98)
(cid:104) (cid:110) (cid:111)(cid:105)
=O 1+E |µ(X)|3 |Dn
(cid:98)
where the first line follows by definition and canceling terms and the last because A, Y, and π are bounded by
(cid:98)
Assumption 5 and construction (Lemma 13). Lemma 11 establishes that
nh µ3 2dE{|µ (cid:98)(X)|3 |Dn}−a. →s. 0. (70)
Therefore, by the continuous mapping theorem,
E(cid:104) |φ (cid:98)(Z)−E{φ (cid:98)(Z)|D π,D µ}|3 |D π,D µ(cid:105) nh µ3 2dE(cid:104) |φ (cid:98)(Z)−E{φ (cid:98)(Z)|D π,D µ}|3 |D π,D µ(cid:105)
a.s.
√ = −→0.
n V{φ (cid:98)(Z)|D π,D µ}3/2 nh µ3 2d√ n V{φ (cid:98)(Z)|D π,D µ}3/2
Lemma 17. Under the conditions of Theorem 3, suppose without loss of generality that µ is the higher-order
(cid:98)
kernel estimator with bandwidth scaling as h µ ≍ n− 2α+2+ 2βε +d while π (cid:98) is the smooth kernel estimator which is
consistent. Then,
1
V{φ(Z)}≍ .
(cid:98) nhd
µ
Proof. Since V{φ(Z)} is a constant by Assumptions 1 and 2. Therefore, if V{φ(Z)−φ(Z)} is increasing with
(cid:98)
sample size then V{φ(Z)}≍V{φ(Z)−φ(Z)}. We have
(cid:98) (cid:98)
V{φ(Z)−φ(Z)}=E[{φ(Z)−φ(Z)}2]−E{φ(Z)−φ(Z)}2.
(cid:98) (cid:98) (cid:98)
By the analysis in Theorem 2,
E{φ(Z)−φ(Z)}2 ≲h2(α+β)
(cid:98) µ
Omitting X arguments,
E[{φ(Z)−φ(Z)}2]=E(cid:104)(cid:8)
(A−π)(µ−µ)+(Y
−µ)(π−π)(cid:9)2(cid:105)
(cid:98) (cid:98) (cid:98) (cid:98)
=E(cid:8) (A−π)2(µ−µ)2(cid:9) +E(cid:8)
(Y
−µ)2(π−π)2(cid:9)
(cid:98) (cid:98) (cid:98)
+2E{(A−π)(Y −µ)(π−π)(µ−µ)}
(cid:98) (cid:98) (cid:98)
47=E(cid:8) (A−π+π−π)2(µ−µ)2(cid:9) +E(cid:2)E(cid:8) (Y −µ)2 |X(cid:9) (π−π)2(cid:3)
(cid:98) (cid:98) (cid:98)
+2E(cid:2)(cid:8)
A(Y −µ)−π(Y
−µ)(cid:9) (µ−µ)(π−π)(cid:3)
(cid:98) (cid:98) (cid:98)
=E(cid:2)(cid:8) (A−π)2+(π−π)2(cid:9) (µ−µ)2(cid:3) +E(cid:16) V(Y |X){π−π}2(cid:17)
(cid:98) (cid:98) (cid:98)
+2E(cid:8)
cov(A,Y
|X)(µ−µ)(π−π)(cid:9)
(cid:98) (cid:98)
=E(cid:8) (π−π)2(µ−µ)2(cid:9)
(cid:98) (cid:98)
(cid:110) (cid:111) (cid:110) (cid:111)
+E V(A|X)(µ−µ)2 +E V(Y |X)(π−π)2
(cid:98) (cid:98)
(cid:110) (cid:111)
+2E cov(A,Y |X)(µ−µ)(π−π)
(cid:98) (cid:98)
where the first line follows by definition; the second by multiplying the square; the third by adding and sub-
tracting π(X) in the first term, iterated expectation on the second term, and multiplying out the third term;
thefourthbymultiplyingoutthesquareinthefirsttermanditeratedexpectationsonX andthetrainingdata,
by definition of V(Y | X) on the second term, and by iterated expectation on X and the training data and
by definition of cov(A,Y | X) on the third term; and the final line follows by iterated expectations on X, the
definition of V(A|X), and rearranging.
Notice that E(cid:8) (π −π)2(µ−µ)2(cid:9) = O(cid:2)E{(µ−µ)2}(cid:3) and E(cid:110) V(Y | X)(π −π)2(cid:111) = O(1) because π and
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
(cid:110) (cid:111)
π are bounded by Assumption 5 and construction (Lemma 13), while 2E cov(A,Y | X)(µ−µ)(π −π) =
(cid:98) (cid:98)
O(cid:104)(cid:112)E{(µ−µ)2}(cid:105)
by Cauchy-Schwarz and Assumption 5. Finally, by Assumptions 1 and 2, and Lemma 9,
(cid:98)
E(cid:8)V(A|X)(µ−µ)2(cid:9)≳ 1
.
(cid:98) nhd
µ
Since 1 is increasing with sample size, this final term then dominates the expression and
nhd
µ
1
V{φ(Z)−φ(Z)}≳ .
(cid:98) nhd
µ
Moreover, because 1 is increasing with sample size, V{φ(Z)}≍V{φ(Z)−φ(Z)}≳ 1 . The upper bound,
nhd (cid:98) (cid:98) nhd
µ µ
V{φ(Z)−φ(Z)} ≲ 1 , follows by the same decomposition as above, but applying the upper bounds from
(cid:98) nhd
µ
Lemma 8.
Lemma 18. Under the conditions of Theorem 3,
V{φ(Z)}
V{φ(Z(cid:98)
)|Dn}
=OP(1).
(cid:98)
Proof. Suppose without loss of generality that µ (cid:98) is the estimator with bandwidth scaling as h µ ≍ n− 2α+2+ 2βε +d
while π is consistent. By Lemma 17,
(cid:98)
nhdV{φ(Z)}≍1.
µ (cid:98)
By Lemma 15,
(cid:26)V(A|X)Y2(cid:27) (cid:26)
K
(X)2(cid:27)
nhdV{φ(Z)|Dn}−a. →s. E E µ .
µ (cid:98) f(X) f(X)
The result follows from these two combined. The same holds if the roles of π and µ were reversed.
(cid:98) (cid:98)
Lemma 19. Under the conditions of Theorem 3,
E{φ(Z)|Dn}−E{φ(Z)}
(cid:98) (cid:98) →p 0.
(cid:112)V{φ(Z)}/n
(cid:98)
Proof. We prove convergence in quadratic mean. The expression in the lemma is mean zero by iterated expec-
48tations,
(cid:34) (cid:35)
E{φ(Z)|Dn}−E{φ(Z)}
E (cid:98) (cid:98) =0.
(cid:112)V{φ(Z)}/n
(cid:98)
Therefore, it suffices to show that the variance of the expression in the lemma converges to zero; i.e.,
nV[E{φ(Z)|Dn}]
(cid:98) →0.
V{φ(Z)}
(cid:98)
By Lemma 17,
1
V{φ(Z)}≍ .
(cid:98) nhd
µ
Consider Z ,Z drawn iid from the same distribution as Z, and which are independent of Dn (like Z). Then,
i j
(cid:104) (cid:105)
V[E{φ(Z)|Dn}]= cov E{φ(Z )|Dn},E{φ(Z )|Dn}
(cid:98) (cid:98) i (cid:98) j
(cid:104) (cid:105)
= cov E{φ(Z )−φ(Z )|Dn},E{φ(Z )−φ(Z )|Dn}
(cid:98) i i (cid:98) j j
(cid:104) (cid:105)
= cov{φ(Z )−φ(Z ),φ(Z )−φ(Z )}−E cov{φ(Z )−φ(Z ),φ(Z )−φ(Z )|Dn}
(cid:98) i i (cid:98) j j (cid:98) i i (cid:98) j j
= cov{φ(Z )−φ(Z ),φ(Z )−φ(Z )}
(cid:98) i i (cid:98) j j
where the first line follows because Z,Z ,Z are identically distributed, the second line because E{φ(Z) | Dn}
i j
is not random because φ does not depend on the training data, the third by the law of total covariance, and
the last because Z and Z are independent. Like in the proof of Lemma 1 in Appendix A, we have
i j
cov{φ(Z )−φ(Z ),φ(Z )−φ(Z )}= cov[E{φ(Z )−φ(Z )|X ,X ,Dn},E{φ(Z )−φ(Z )|X ,X ,Dn}]+0
(cid:98) i i (cid:98) j j (cid:98) i i i j (cid:98) j j i j
=E(cov[E{φ(Z )−φ(Z )|X ,Dn},E{φ(Z )−φ(Z )|X ,Dn}|X ,X ])+0
(cid:98) i i i (cid:98) j j j i j
(cid:104) (cid:110) (cid:111)(cid:105)
≡E cov (cid:98)b φ(X i),(cid:98)b φ(X j)|X i,X
j
by successive applications of the law of total covariance, and where(cid:98)b φ(X i) is defined in Lemma 1. From here,
because X ̸=X , we can use the same argument as in the proof of Proposition 1 (see (24)), and conclude
i j
E(cid:104) cov(cid:110)
(cid:98)b φ(X i),(cid:98)b φ(X j)|X i,X
j(cid:111)(cid:105)
≲
∥b2 π∥ ∞+∥b2 µ∥ ∞+ nmin(∥s2 π∥ ∞,∥s2 µ∥ ∞)
≲
n1
.
where the first inequality follows by Proposition 1 and Lemma 8, and the second by Lemma 8. Therefore,
nV[E{φ(Z)|Dn}]≲1,
(cid:98)
and so
nV[E{φ(Z)|Dn}]
(cid:98) ≲nhd →0 as n→∞,
V{φ(Z)} µ
(cid:98)
where convergence to zero follows because h
µ
≍n− 2α+2+ 2βε +d.
Lemma 20. Under the conditions of Theorem 3,
E{φ(Z)−φ(Z)}
(cid:98) →0.
(cid:112)V{φ(Z)}/n
(cid:98)
Proof. The ratio
E√{φ(cid:98)(Z)−φ(Z)}
is not random because the expectation and variance are over the estimation and
V{φ(cid:98)(Z)}/n
training data. By the analysis in Theorem 2,
E{φ (cid:98)(Z)−φ(Z)}≲h µα+β ≲n−(2 2+ αε +) 2(α β++ dβ)
49Assumewithoutlossofgeneralitythatµistheundersmoothednuisancefunctionestimator,thenbyLemma17,
(cid:98)
1
V{φ(Z)}≍ .
(cid:98) nhd
µ
Therefore,
E (cid:112){φ (cid:98) V(Z {φ) (− Zφ )}( /Z n)} ≲nhd µ/2n−(2 2+ αε +) 2(α β++ dβ) →0 as n→∞
(cid:98)
because h
µ
≍n− 2α+2+ 2βε +d.
F Technical results regarding the covariate density
Below, we state and prove three technical lemmas about the covariates {X }n if their density is bounded
i i=1
above and below as in Assumption 2.
Lemma 21. (Sphere Lemma) Assume X has density f(X) that satisfies Assumption 2 and let B (x) denote a
h
ball of radius h around a fixed point x∈X. Then
P{X ∈B (x)}≍hd (71)
h
Proof. The volume of a ball with radius r in d dimensions scales like rd. The result follows because the density
is upper and lower bounded.
Lemma 22. (Well separated training covariates). Let {X }n be n covariate data points satisfying Assump-
i i=1
tion 2 (bounded density). Let P denote the random variable counting (twice) all pairs of covariates closer than
n
2h where h is a bandwidth scaling with sample size; i.e.,
n n
(cid:88)(cid:88)
P = 1(∥X −X ∥≤2h).
n i j
i=1 j̸=i
If h satisfies nhd ≍n−α for α>0 as n→∞, then
P
n −a. →s. 0. (72)
n
Proof. TheresultfollowsbyamomentinequalityforU-statisticsandtheBorel-Cantellilemma. First,werelate
the un-decoupled U-statistic, P , to the relevant decoupled U-statistic. Let {X(1)}n and {X(2)}n denote
n i i=1 j j=1
two independent sequences drawn from the same distribution as {X }n . Let
i i=1
n n
P′
:=(cid:88)(cid:88) 1(cid:16) ∥X(1)−X(2)∥≤2h(cid:17)
. (73)
n i j
i=1 j̸=i
By Theorem 3.1.1 in de la Pen˜a et al. [1999], for p≥1,
(cid:26)(cid:18)
P
(cid:19)p(cid:27) (cid:26)(cid:18) P′(cid:19)p(cid:27)
E n ≲E n . (74)
n n
Then, by Proposition 2.1 and the right-hand side of (2.2) in Gin´e et al. [2000], for all p>1,
(cid:26)(cid:18) P′(cid:19)p(cid:27)
E n ≲(nhd)p+nhdp+n2−phd. (75)
n
501(cid:16) ∥X(1)−X(2)∥≤2h(cid:17)
This follows because the kernel is i j , which satisfies
n
 (cid:16) (cid:17)p
1 ∥X i(1)−X j(2)∥≤2h  (cid:18) hd(cid:19)p
E ≲ ,
n n
 
 (cid:16) (cid:17) p
1 ∥X i(1)−X j(2)∥≤2h  (cid:18) hd(cid:19)p
E |X ≲ , and
n i n
 
 1(cid:16)
∥X i(1)−X
j(2)∥≤2h(cid:17) p
hd
E  ≲ .
n np
 
Toconclude,weproveaninfinitelysummableconcentrationinequalitydirectly. Letϵ>0. By(75)andMarkov’s
inequality, for all p≥2,
(cid:18) (cid:19)
P
P n ≥ϵ ≲(nhd)p+nhdp+n2−phd ≍n−αp+o(n−(1+α))+o(n−(1+α)), (76)
n
where the right-hand side follows by the conditions on the bandwidth. Hence, for p > 1+δ for any δ > 0,
α
P(cid:0)Pn ≥ϵ(cid:1) =o(n−(1+δ)) for all ϵ>0, and therefore the result follows by the Borel-Cantelli lemma.
n
Lemma 23. (Triply well separated training covariates). Let {X }n be n covariate data points satisfying
i i=1
Assumption2(boundeddensity). LetQ denotetherandomvariablecounting(sixtimes)alltriplesofcovariates
n
closer than 2h where h is a bandwidth scaling with sample size; i.e.,
n
(cid:88)
Q = 1(∥X −X ∥≤2h)1(∥X −X ∥≤2h)1(∥X −X ∥≤2h). (77)
n i j i k j k
i̸=j̸=k
If h satisfies nhd ≍n−α for α>0 as n→∞, then
Q
n −a. →s. 0. (78)
n
Proof. The result follows by the same approach as the previous lemma, but applying a moment inequality for
U-statistics of order 3. First, let {X(1)}n , {X(2)}n , and {X(3)}n denote three independent sequences
i i=1 j j=1 k k=1
drawn from the same distribution as {X }n . Moreover, let
i i=1
n
Q′ :=
(cid:88) 1(cid:16) ∥X(1)−X(2)∥≤2h(cid:17) 1(cid:16) ∥X(1)−X(3)∥≤2h(cid:17) 1(cid:16) ∥X(2)−X(3)∥≤2h(cid:17)
. (79)
n i j i k j k
i̸=j̸=k
Then, by Theorem 3.1.1 in de la Pen˜a et al. [1999] and Proposition 2.1 and the right-hand side of (2.2) in Gin´e
et al. [2000], for all p>1,
(cid:26)(cid:18)
Q
(cid:19)p(cid:27)
E n ≲(nhd)2p+n(nh2d)p+n2hdp+n3−phd. (80)
n
1(cid:16) ∥X(1)−X(2)∥≤2h(cid:17) 1(cid:16) ∥X(1)−X(3)∥≤2h(cid:17) 1(cid:16) ∥X(2)−X(3)∥≤2h(cid:17)
This follows because the kernel is i j i k j k , which satisfies
n
 (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)p
1 ∥X i(1)−X j(2)∥≤2h 1 ∥X i(1)−X k(3)∥≤2h 1 ∥X j(2)−X k(3)∥≤2h  (cid:18) h2d(cid:19)p
E ≲ ,
n n
 
 (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) p
E1 ∥X i(1)−X j(2)∥≤2h 1 ∥X i(1)−X k(3)∥≤2h 1 ∥X j(2)−X k(3)∥≤2h |X(1) ≲(cid:18) h2d(cid:19)p
,
n i n
 
51 (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) p
E1 ∥X i(1)−X j(2)∥≤2h 1 ∥X i(1)−X k(3)∥≤2h 1 ∥X j(2)−X k(3)∥≤2h |X(1),X(2) ≲(cid:18) hd(cid:19)p
, and
n i j n
 
 1(cid:16)
∥X i(1)−X
j(2)∥≤2h(cid:17) 1(cid:16)
∥X i(1)−X
k(3)∥≤2h(cid:17) 1(cid:16)
∥X j(2)−X
k(3)∥≤2h(cid:17) p
hd
E ≲
 
n np
 
Let ϵ>0. Then, by Markov’s inequality, for all p≥3,
(cid:18) (cid:19)
Q
P n ≥ϵ ≲(nhd)2p+n(nh2d)p+n2hdp+n3−phd ≍n−2αp+o(n−(1+α)), (81)
n
where the right-hand side follows by the conditions on the bandwidth. Hence, for p > 1+δ for any δ > 0,
(cid:16) (cid:17) 2α
P Qn ≥ϵ =o(n−(1+δ)) for all ϵ>0, and therefore the result follows by the Borel-Cantelli lemma.
n
G A strong law of large number for a triangular array of bounded
random variables
Thefollowingresultisasimplestronglawoflargenumbersforatriangulararrayofboundedrandomvariables.
Lemma 24. Let {ξ }n i ∼id P for n ∈ N denote a triangular array of random variables which are row-wise
i,n i=1 n
iid. If the random variables satisfy
1. |ξ |<B for all i and n and some B <∞, and
i,n
2. E(ξ )n −→ →∞ µ for some µ∈R,
1,n
then
n
1 (cid:88) a.s.
ξ −→µ. (82)
n i,n
i=1
Proof. The proof follows by a combination of Hoeffding’s inequality and the Borel-Cantelli lemma.
Let t > 0. Because E(ξ ) n −→ →∞ µ, there exists some N ∈ N such that |E(ξ )−µ| < t for all n ≥ N.
1,n 1,n 2
Hence, for n≥N, by the triangle inequality,
(cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:33) (cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:33)
P (cid:12) ξ −µ(cid:12)≥t =P (cid:12) ξ −E(ξ )+E(ξ )−µ(cid:12)≥t (83)
(cid:12)n i,n (cid:12) (cid:12)n i,n 1,n 1,n (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
i=1 i=1
(cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:33)
≤P (cid:12) ξ −E(ξ )(cid:12)+|E(ξ )−µ|≥t (84)
(cid:12)n i,n 1,n (cid:12) 1,n
(cid:12) (cid:12)
i=1
(cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:33)
=P (cid:12) ξ −E(ξ )(cid:12)≥t−|E(ξ )−µ| (85)
(cid:12)n i,n 1,n (cid:12) 1,n
(cid:12) (cid:12)
i=1
(cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) t(cid:33)
≤P (cid:12) ξ −E(ξ )(cid:12)≥ . (86)
(cid:12)n i,n 1,n (cid:12) 2
(cid:12) (cid:12)
i=1
Applying Hoeffding’s inequality to the final line gives
(cid:32)(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:33) (cid:26) 2nt2 (cid:27)
P (cid:12) ξ −µ(cid:12)≥t ≤2exp − . (87)
(cid:12)n i,n (cid:12) 16B2
(cid:12) (cid:12)
i=1
The result then follows because (cid:80)∞ n=1P(cid:0)(cid:12) (cid:12) n1 (cid:80)n i=1ξ i,n−µ(cid:12) (cid:12)≥t(cid:1) <∞ and by the Borel-Cantelli lemma.
52H Series regression
In this section, we consider series regression for the nuisance function estimators, and establish equivalent
results to Lemma 2 and Theorem 1. Series regression is well studied and includes bases such as the Legendre
polynomial series, the local polynomial partition series, and the Cohen-Daubechies-Vial wavelet series [Belloni
et al., 2015, Hansen, 2022]. Here, we focus on regression splines [Fisher and Fisher, 2023, Newey and Robins,
2018]andwaveletestimators[McGrathandMukherjee,2022]. Regressionsplinesareanaturalglobalaveraging
estimatortoconsiderbecause,likethelocalaveragingestimatorsweconsideredinSection4,theydonotrequire
knowledgeofthecovariatedensity. Thewaveletestimatorsareanaturalalternativebecause, likethecovariate-
√
density-adapted kernel regression we considered in Section 5, they can achieve the minimax rate in the non- n
regime. From a technical perspective, our examination of each of these estimators may be of interest because
ourproofsthattheyachievesemiparametricefficiencyorminimaxoptimalityaredifferentfromthoseconsidered
previously.
H.1 Regression splines
First, we review regression splines.
Estimator 4. (Regression Splines) The regression spline estimator for µ(x)=E(Y |X =x) is
µ(x)=
(cid:88) g(x)TQ(cid:98)−1g(X i)
Y (88)
(cid:98) n i
Zi∈Dµ
where g :Rd →Rkµ is a k
µ
order polynomial spline basis, and
1 (cid:88)
Q(cid:98) =
n
g(X i)g(X i)T.
Xi∈X µn
Additionally, the spline neighborhoods are approximately evenly sized (see, Assumption 3 in Fisher and Fisher
[2023]), so that the distance between two points within a neighborhood scales like ≲k−1/d. The regression spline
µ
estimator for π(x)=E(A|X =x) is defined analogously on D .
µ
Theadditionalconditionweimpose,thattheneighborhoodsareapproximatelyevenlysized,canbeenforced
under Assumption 2 that the covariate density and covariate support are bounded. Similar to local polynomial
regression, we require an assumption on the design matrix.
Assumption 7. (Bounded Minimum Eigenvalue) For Estimator 4, there exists λ >0 such that, uniformly
0
over all n,
λ
(cid:2)E(cid:8) g(X)g(X)T(cid:9)(cid:3)
≥λ .
min 0
This assumption requires that the regressors g (X),...,g (X) are not too co-linear, and corresponds to
1 k
ConditionA.2inBellonietal.[2015]andAssumption5inFisherandFisher[2023]. Thisassumptionimplicitly
constrains the number of bases to grow no faster than the sample size, and constrains the convergence rate of
√
the DCDR estimator in the non- n regime.
H.2 Wavelet estimators
Here, we review wavelet estimators. For simplicity, we focus on the case where the covariate density is known
and sufficiently smooth, as in Assumption 4, and propose the same estimator as that considered in McGrath
and Mukherjee [2022].
Estimator 5. (Wavelet estimator) The wavelet estimator for µ(x)=E(Y |X =x) is
µ(x)=
(cid:88) K Vkµ(x,X i)
Y (89)
(cid:98) nf(X ) i
i
Zi∈Dµ
53where K (x,X ) denotes the orthogonal projection kernel onto the linear subspace V as defined in Appendix
Vkµ i kµ
A of McGrath and Mukherjee [2022]. The wavelet estimator for π(x)=E(A|X =x) is defined analogously on
D .
π
H.3 Lemma 2 and Theorem 1 for series regression
First, we state without proof standard bias and variance bounds for regression splines and wavelet estimators.
Lemma 25. Suppose Assumptions 1, 2, and 3 hold. If µ(x) is a regression spline (Estimator 4) and Assump-
(cid:98)
tion 7 holds or µ is a wavelet estimator (Estimator 5) and Assumption 4 holds, then
(cid:98)
sup|E{µ(x)−µ(x)}|≲k−β/d,and (90)
(cid:98) µ
x∈X
k
supV{µ(x)}≲ µ. (91)
(cid:98) n
x∈X
Analogous results hold for π(x) and π(x).
(cid:98)
We can also bound the expected absolute covariance term from Lemma 2 with both regression splines and
wavelet estimators, as in the following result.
Lemma 26. Suppose Assumptions 1, 2, and 3 hold. If µ(x) is a regression spline (Estimator 4) and Assump-
(cid:98)
tion 7 holds or µ is a wavelet estimator (Estimator 5) and Assumption 4 holds, then
(cid:98)
E(cid:2)(cid:12)
(cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X
j}(cid:12) (cid:12)(cid:3)≲ n1
.
Analogous results hold for π(X).
(cid:98)
Proof. For regression splines, the proof follows by the same technique as for local averaging estimators (e.g.,
Lemma 7) because regression splines partition the covariate space into neighborhoods: if X and X are far
i j
enough apart, then they do not share training data. Specifically, let A denote the event that X and X are
ij i j
in the same neighborhood according to the basis g in Estimator 4. Then,
E(cid:2)(cid:12) (cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}(cid:12) (cid:12)(cid:3) =E[|cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X j}|A ij]
≤ sup |cov{µ(x ),µ(x )}|P(A )
(cid:98) i (cid:98) j ij
xi,xj
≲supV{µ(x)}k−1
(cid:98) µ
x
1
≲ .
n
where the first line follows because cov{µ(X ),µ(X ) | X ,X } = 0 when X and X are not in the same
(cid:98) i (cid:98) j i j i j
neighborhood, the second by H¨older’s inequality, the third by the definition of the size of the neighborhoods in
Estimator 4 and Lemma 21, and the final line by Lemma 25.
Forwaveletestimators,theproofisdifferent. ItfollowsbythesameanalysisasinLemma15(i)inMcGrath
and Mukherjee [2022], which we repeat here for completeness. Notice that
 
E{µ (cid:98)(X i)µ (cid:98)(X j)|X i,X j}=E  (cid:88) K Vkµ(X i n,X 2fk () XK V )k fµ (( XX j ),X l)Y kY l |X i,X j
k l
Zk,Zl∈Dµ
=
1 E(cid:34) K Vkµ(X i,X k)K Vkµ(X j,X k)Y k2
|X ,X
(cid:35) +(cid:18)
1−
1(cid:19)
E{µ(X)|X}2
n f(X )2 i j n (cid:98)
k
(cid:32) (cid:34) (cid:35) (cid:33)
=E{µ(X)|X}2+
1
E
K Vkµ(X i,X k)K Vkµ(X j,X k)Y k2
|X ,X −E{µ(X)|X}2 ,
(cid:98) n f(X )2 i j (cid:98)
k
54where the first line follows by definition, the second by iid datapoints, and the third by rearranging. By the
definition of covariance,
cov{µ(X ),µ(X )|X ,X }=E{µ(X )µ(X )|X ,X }−E{µ(X)|X}2
(cid:98) i (cid:98) j i j (cid:98) i (cid:98) j i j (cid:98)
(cid:32) (cid:34) (cid:35) (cid:33)
=
1
E
K Vkµ(X i,X k)K Vkµ(X j,X k)Y k2
|X ,X −E{µ(X)|X}2 .
n f(X )2 i j (cid:98)
k
Therefore,
E(cid:2)(cid:12)
(cid:12)cov{µ (cid:98)(X i),µ (cid:98)(X j)|X i,X
j}(cid:12) (cid:12)(cid:3)≲ n1
,
where the inequality follows by Assumptions 1 and 2 and because K (x,y) is bounded.
Vkµ
By Lemmas 25 and 26, we have an analogous result to Theorem 1, which we state without proof.
Theorem 4. (Series Regression Semiparametric Efficiency) Suppose Assumptions 1, 2, and 3 hold and
ψ
ecc
is estimated with the DCDR estimator ψ(cid:98)n from Algorithm 1. If the nuisance functions µ
(cid:98)
and π
(cid:98)
are
estimated with regression splines (Estimator 4), Assumption 7 holds, and the bases scale like k ,k ≍ n ,
µ π logn
or if the nuisance functions are estimated with wavelet estimators (Estimator 5), Assumption 4 holds, and
k ,k ≍ n , then
µ π logn
(cid:113)
 V{φn (Z)}(ψ(cid:98)n−ψ ecc)⇝N(0,1) if α+ 2β >d/4, and
(92)
E|ψ(cid:98)n−ψ ecc|≲(cid:16)
lon
gn(cid:17)−α+ dβ
otherwise.
This result is optimal for regression splines – to ensure the Gram matrix is invertible, they cannot be
undersmoothed any further, and so the bias of the DCDR estimator cannot be reduced. For wavelet estimators
√
withknowncovariatedensity,thisresultcanbeimprovedinthenon- nregimebyundersmoothingevenfurther
only one of the two nuisance function estimators and carefully analyzing the bias of the DCDR estimator (see,
McGrath and Mukherjee [2022], Proposition 2).
55