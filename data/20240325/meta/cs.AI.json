[
    {
        "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
        "authors": "Yuzhang ShangMu CaiBingxin XuYong Jae LeeYan Yan",
        "links": "http://arxiv.org/abs/2403.15388v1",
        "entry_id": "http://arxiv.org/abs/2403.15388v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15388v1",
        "summary": "Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.",
        "updated": "2024-03-22 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15388v1"
    },
    {
        "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
        "authors": "Kevin XieJonathan LorraineTianshi CaoJun GaoJames LucasAntonio TorralbaSanja FidlerXiaohui Zeng",
        "links": "http://arxiv.org/abs/2403.15385v1",
        "entry_id": "http://arxiv.org/abs/2403.15385v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15385v1",
        "summary": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
        "updated": "2024-03-22 17:59:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15385v1"
    },
    {
        "title": "Can large language models explore in-context?",
        "authors": "Akshay KrishnamurthyKeegan HarrisDylan J. FosterCyril ZhangAleksandrs Slivkins",
        "links": "http://arxiv.org/abs/2403.15371v1",
        "entry_id": "http://arxiv.org/abs/2403.15371v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15371v1",
        "summary": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "updated": "2024-03-22 17:50:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15371v1"
    },
    {
        "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
        "authors": "Ryan TeehanBrenden LakeMengye Ren",
        "links": "http://arxiv.org/abs/2403.15362v1",
        "entry_id": "http://arxiv.org/abs/2403.15362v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15362v1",
        "summary": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.",
        "updated": "2024-03-22 17:26:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15362v1"
    },
    {
        "title": "Collaborative AI Teaming in Unknown Environments via Active Goal Deduction",
        "authors": "Zuyuan ZhangHanhan ZhouMahdi ImaniTaeyoung LeeTian Lan",
        "links": "http://arxiv.org/abs/2403.15341v1",
        "entry_id": "http://arxiv.org/abs/2403.15341v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15341v1",
        "summary": "With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.",
        "updated": "2024-03-22 16:50:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15341v1"
    }
]