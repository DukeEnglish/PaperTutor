[
    {
        "title": "DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data",
        "authors": "Hanrong YeDan Xu",
        "links": "http://arxiv.org/abs/2403.15389v1",
        "entry_id": "http://arxiv.org/abs/2403.15389v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15389v1",
        "summary": "Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.",
        "updated": "2024-03-22 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15389v1"
    },
    {
        "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
        "authors": "Kevin XieJonathan LorraineTianshi CaoJun GaoJames LucasAntonio TorralbaSanja FidlerXiaohui Zeng",
        "links": "http://arxiv.org/abs/2403.15385v1",
        "entry_id": "http://arxiv.org/abs/2403.15385v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15385v1",
        "summary": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
        "updated": "2024-03-22 17:59:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15385v1"
    },
    {
        "title": "Can large language models explore in-context?",
        "authors": "Akshay KrishnamurthyKeegan HarrisDylan J. FosterCyril ZhangAleksandrs Slivkins",
        "links": "http://arxiv.org/abs/2403.15371v1",
        "entry_id": "http://arxiv.org/abs/2403.15371v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15371v1",
        "summary": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "updated": "2024-03-22 17:50:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15371v1"
    },
    {
        "title": "Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks",
        "authors": "Aqeel AnwarTae Eun ChoeZian WangSanja FidlerMinwoo Park",
        "links": "http://arxiv.org/abs/2403.15370v1",
        "entry_id": "http://arxiv.org/abs/2403.15370v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15370v1",
        "summary": "Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.",
        "updated": "2024-03-22 17:49:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15370v1"
    },
    {
        "title": "A Transfer Attack to Image Watermarks",
        "authors": "Yuepeng HuZhengyuan JiangMoyang GuoNeil Gong",
        "links": "http://arxiv.org/abs/2403.15365v1",
        "entry_id": "http://arxiv.org/abs/2403.15365v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15365v1",
        "summary": "Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.",
        "updated": "2024-03-22 17:33:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15365v1"
    }
]