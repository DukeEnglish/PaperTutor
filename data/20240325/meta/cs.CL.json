[
    {
        "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
        "authors": "Yuzhang ShangMu CaiBingxin XuYong Jae LeeYan Yan",
        "links": "http://arxiv.org/abs/2403.15388v1",
        "entry_id": "http://arxiv.org/abs/2403.15388v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15388v1",
        "summary": "Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.",
        "updated": "2024-03-22 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15388v1"
    },
    {
        "title": "Can large language models explore in-context?",
        "authors": "Akshay KrishnamurthyKeegan HarrisDylan J. FosterCyril ZhangAleksandrs Slivkins",
        "links": "http://arxiv.org/abs/2403.15371v1",
        "entry_id": "http://arxiv.org/abs/2403.15371v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15371v1",
        "summary": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "updated": "2024-03-22 17:50:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15371v1"
    },
    {
        "title": "A Transfer Attack to Image Watermarks",
        "authors": "Yuepeng HuZhengyuan JiangMoyang GuoNeil Gong",
        "links": "http://arxiv.org/abs/2403.15365v1",
        "entry_id": "http://arxiv.org/abs/2403.15365v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15365v1",
        "summary": "Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.",
        "updated": "2024-03-22 17:33:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15365v1"
    },
    {
        "title": "Towards Knowledge-Grounded Natural Language Understanding and Generation",
        "authors": "Chenxi Whitehouse",
        "links": "http://arxiv.org/abs/2403.15364v1",
        "entry_id": "http://arxiv.org/abs/2403.15364v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15364v1",
        "summary": "This thesis investigates how natural language understanding and generation\nwith transformer models can benefit from grounding the models with knowledge\nrepresentations and addresses the following key research questions: (i) Can\nknowledge of entities extend its benefits beyond entity-centric tasks, such as\nentity linking? (ii) How can we faithfully and effectively extract such\nstructured knowledge from raw text, especially noisy web text? (iii) How do\nother types of knowledge, beyond structured knowledge, contribute to improving\nNLP tasks?\n  Studies in this thesis find that incorporating relevant and up-to-date\nknowledge of entities benefits fake news detection, and entity-focused\ncode-switching significantly enhances zero-shot cross-lingual transfer on\nentity-centric tasks. In terms of effective and faithful approaches to\nextracting structured knowledge, it is observed that integrating negative\nexamples and training with entity planning significantly improves performance.\nAdditionally, it is established that other general forms of knowledge, such as\nparametric and distilled knowledge, enhance multimodal and multilingual\nknowledge-intensive tasks. This research shows the tangible benefits of diverse\nknowledge integration and motivates further exploration in this direction.",
        "updated": "2024-03-22 17:32:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15364v1"
    },
    {
        "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
        "authors": "Ryan TeehanBrenden LakeMengye Ren",
        "links": "http://arxiv.org/abs/2403.15362v1",
        "entry_id": "http://arxiv.org/abs/2403.15362v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15362v1",
        "summary": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.",
        "updated": "2024-03-22 17:26:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15362v1"
    }
]