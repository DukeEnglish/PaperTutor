[
    {
        "title": "A Wasserstein perspective of Vanilla GANs",
        "authors": "Lea KunkelMathias Trabs",
        "links": "http://arxiv.org/abs/2403.15312v1",
        "entry_id": "http://arxiv.org/abs/2403.15312v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15312v1",
        "summary": "The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.",
        "updated": "2024-03-22 16:04:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15312v1"
    },
    {
        "title": "Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models",
        "authors": "John FischerMarko OrescaninJustin LoomisPatrick McClure",
        "links": "http://arxiv.org/abs/2403.15263v1",
        "entry_id": "http://arxiv.org/abs/2403.15263v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15263v1",
        "summary": "Federated learning (FL) is an approach to training machine learning models\nthat takes advantage of multiple distributed datasets while maintaining data\nprivacy and reducing communication costs associated with sharing local\ndatasets. Aggregation strategies have been developed to pool or fuse the\nweights and biases of distributed deterministic models; however, modern\ndeterministic deep learning (DL) models are often poorly calibrated and lack\nthe ability to communicate a measure of epistemic uncertainty in prediction,\nwhich is desirable for remote sensing platforms and safety-critical\napplications. Conversely, Bayesian DL models are often well calibrated and\ncapable of quantifying and communicating a measure of epistemic uncertainty\nalong with a competitive prediction accuracy. Unfortunately, because the\nweights and biases in Bayesian DL models are defined by a probability\ndistribution, simple application of the aggregation methods associated with FL\nschemes for deterministic models is either impossible or results in sub-optimal\nperformance. In this work, we use independent and identically distributed (IID)\nand non-IID partitions of the CIFAR-10 dataset and a fully variational\nResNet-20 architecture to analyze six different aggregation strategies for\nBayesian DL models. Additionally, we analyze the traditional federated\naveraging approach applied to an approximate Bayesian Monte Carlo dropout model\nas a lightweight alternative to more complex variational inference methods in\nFL. We show that aggregation strategy is a key hyperparameter in the design of\na Bayesian FL system with downstream effects on accuracy, calibration,\nuncertainty quantification, training stability, and client compute\nrequirements.",
        "updated": "2024-03-22 15:02:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15263v1"
    },
    {
        "title": "Double Cross-fit Doubly Robust Estimators: Beyond Series Regression",
        "authors": "Alec McCleanSivaraman BalakrishnanEdward H. KennedyLarry Wasserman",
        "links": "http://arxiv.org/abs/2403.15175v1",
        "entry_id": "http://arxiv.org/abs/2403.15175v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15175v1",
        "summary": "Doubly robust estimators with cross-fitting have gained popularity in causal\ninference due to their favorable structure-agnostic error guarantees. However,\nwhen additional structure, such as H\\\"{o}lder smoothness, is available then\nmore accurate \"double cross-fit doubly robust\" (DCDR) estimators can be\nconstructed by splitting the training data and undersmoothing nuisance function\nestimators on independent samples. We study a DCDR estimator of the Expected\nConditional Covariance, a functional of interest in causal inference and\nconditional independence testing, and derive a series of increasingly powerful\nresults with progressively stronger assumptions. We first provide a\nstructure-agnostic error analysis for the DCDR estimator with no assumptions on\nthe nuisance functions or their estimators. Then, assuming the nuisance\nfunctions are H\\\"{o}lder smooth, but without assuming knowledge of the true\nsmoothness level or the covariate density, we establish that DCDR estimators\nwith several linear smoothers are semiparametric efficient under minimal\nconditions and achieve fast convergence rates in the non-$\\sqrt{n}$ regime.\nWhen the covariate density and smoothnesses are known, we propose a minimax\nrate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover,\nwe show an undersmoothed DCDR estimator satisfies a slower-than-$\\sqrt{n}$\ncentral limit theorem, and that inference is possible even in the\nnon-$\\sqrt{n}$ regime. Finally, we support our theoretical results with\nsimulations, providing intuition for double cross-fitting and undersmoothing,\ndemonstrating where our estimator achieves semiparametric efficiency while the\nusual \"single cross-fit\" estimator fails, and illustrating asymptotic normality\nfor the undersmoothed DCDR estimator.",
        "updated": "2024-03-22 12:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15175v1"
    },
    {
        "title": "Quantification using Permutation-Invariant Networks based on Histograms",
        "authors": "Olaya Pérez-MonAlejandro MoreoJuan José del CozPablo González",
        "links": "http://arxiv.org/abs/2403.15123v1",
        "entry_id": "http://arxiv.org/abs/2403.15123v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15123v1",
        "summary": "Quantification, also known as class prevalence estimation, is the supervised\nlearning task in which a model is trained to predict the prevalence of each\nclass in a given bag of examples. This paper investigates the application of\ndeep neural networks to tasks of quantification in scenarios where it is\npossible to apply a symmetric supervised approach that eliminates the need for\nclassification as an intermediary step, directly addressing the quantification\nproblem. Additionally, it discusses existing permutation-invariant layers\ndesigned for set processing and assesses their suitability for quantification.\nIn light of our analysis, we propose HistNetQ, a novel neural architecture that\nrelies on a permutation-invariant representation based on histograms that is\nspecially suited for quantification problems. Our experiments carried out in\nthe only quantification competition held to date, show that HistNetQ\noutperforms other deep neural architectures devised for set processing, as well\nas the state-of-the-art quantification methods. Furthermore, HistNetQ offers\ntwo significant advantages over traditional quantification methods: i) it does\nnot require the labels of the training examples but only the prevalence values\nof a collection of training bags, making it applicable to new scenarios; and\nii) it is able to optimize any custom quantification-oriented loss function.",
        "updated": "2024-03-22 11:25:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15123v1"
    },
    {
        "title": "Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks",
        "authors": "Benjamin BobbiaMatthias Picard",
        "links": "http://arxiv.org/abs/2403.15108v1",
        "entry_id": "http://arxiv.org/abs/2403.15108v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15108v1",
        "summary": "This paper addresses a new active learning strategy for regression problems.\nThe presented Wasserstein active regression model is based on the principles of\ndistribution-matching to measure the representativeness of the labeled dataset.\nThe Wasserstein distance is computed using GroupSort Neural Networks. The use\nof such networks provides theoretical foundations giving a way to quantify\nerrors with explicit bounds for their size and depth. This solution is combined\nwith another uncertainty-based approach that is more outlier-tolerant to\ncomplete the query strategy. Finally, this method is compared with other\nclassical and recent solutions. The study empirically shows the pertinence of\nsuch a representativity-uncertainty approach, which provides good estimation\nall along the query procedure. Moreover, the Wasserstein active regression\noften achieves more precise estimations and tends to improve accuracy faster\nthan other models.",
        "updated": "2024-03-22 10:51:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15108v1"
    }
]