[
    {
        "title": "DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data",
        "authors": "Hanrong YeDan Xu",
        "links": "http://arxiv.org/abs/2403.15389v1",
        "entry_id": "http://arxiv.org/abs/2403.15389v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15389v1",
        "summary": "Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.",
        "updated": "2024-03-22 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15389v1"
    },
    {
        "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
        "authors": "Yuzhang ShangMu CaiBingxin XuYong Jae LeeYan Yan",
        "links": "http://arxiv.org/abs/2403.15388v1",
        "entry_id": "http://arxiv.org/abs/2403.15388v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15388v1",
        "summary": "Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.",
        "updated": "2024-03-22 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15388v1"
    },
    {
        "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
        "authors": "Kevin XieJonathan LorraineTianshi CaoJun GaoJames LucasAntonio TorralbaSanja FidlerXiaohui Zeng",
        "links": "http://arxiv.org/abs/2403.15385v1",
        "entry_id": "http://arxiv.org/abs/2403.15385v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15385v1",
        "summary": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
        "updated": "2024-03-22 17:59:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15385v1"
    },
    {
        "title": "ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars",
        "authors": "Zhenwei WangTengfei WangGerhard HanckeZiwei LiuRynson W. H. Lau",
        "links": "http://arxiv.org/abs/2403.15383v1",
        "entry_id": "http://arxiv.org/abs/2403.15383v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15383v1",
        "summary": "Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.",
        "updated": "2024-03-22 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15383v1"
    },
    {
        "title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects",
        "authors": "Ruining LiChuanxia ZhengChristian RupprechtAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2403.15382v1",
        "entry_id": "http://arxiv.org/abs/2403.15382v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15382v1",
        "summary": "We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.",
        "updated": "2024-03-22 17:58:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15382v1"
    }
]