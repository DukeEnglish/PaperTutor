[
    {
        "title": "Collaborative AI Teaming in Unknown Environments via Active Goal Deduction",
        "authors": "Zuyuan ZhangHanhan ZhouMahdi ImaniTaeyoung LeeTian Lan",
        "links": "http://arxiv.org/abs/2403.15341v1",
        "entry_id": "http://arxiv.org/abs/2403.15341v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15341v1",
        "summary": "With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.",
        "updated": "2024-03-22 16:50:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15341v1"
    },
    {
        "title": "CACA Agent: Capability Collaboration based AI Agent",
        "authors": "Peng XuHaoran WangChuang WangXu Liu",
        "links": "http://arxiv.org/abs/2403.15137v1",
        "entry_id": "http://arxiv.org/abs/2403.15137v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15137v1",
        "summary": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.",
        "updated": "2024-03-22 11:42:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15137v1"
    },
    {
        "title": "An Agent-Centric Perspective on Norm Enforcement and Sanctions",
        "authors": "Elena YanLuis G. NardinJomi F. HübnerOlivier Boissier",
        "links": "http://arxiv.org/abs/2403.15128v1",
        "entry_id": "http://arxiv.org/abs/2403.15128v1",
        "pdf_url": "http://arxiv.org/pdf/2403.15128v1",
        "summary": "In increasingly autonomous and highly distributed multi-agent systems,\ncentralized coordination becomes impractical and raises the need for governance\nand enforcement mechanisms from an agent-centric perspective. In our conceptual\nview, sanctioning norm enforcement is part of this agent-centric approach and\nthey aim at promoting norm compliance while preserving agents' autonomy. The\nfew works dealing with sanctioning norm enforcement and sanctions from the\nagent-centric perspective present limitations regarding the representation of\nsanctions and the comprehensiveness of their norm enforcement process. To\naddress these drawbacks, we propose the NPL(s), an extension of the NPL\nnormative programming language enriched with the representation of norms and\nsanctions as first-class abstractions. We also propose a BDI normative agent\narchitecture embedding an engine for processing the NPL(s) language and a set\nof capabilities for approaching more comprehensively the sanctioning norm\nenforcement process. We apply our contributions in a case study for improving\nthe robustness of agents' decision-making in a production automation system.",
        "updated": "2024-03-22 11:30:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.15128v1"
    },
    {
        "title": "A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning",
        "authors": "Changmeng ZhengDayong LiangWengyu ZhangXiao-Yong WeiTat-Seng ChuaQing Li",
        "links": "http://arxiv.org/abs/2403.14972v1",
        "entry_id": "http://arxiv.org/abs/2403.14972v1",
        "pdf_url": "http://arxiv.org/pdf/2403.14972v1",
        "summary": "This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate BDoG, achieving state-of-the-art results in\nScience QA and MMBench with significant improvements over previous methods.",
        "updated": "2024-03-22 06:03:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.14972v1"
    },
    {
        "title": "Learning to Change: Choreographing Mixed Traffic Through Lateral Control and Hierarchical Reinforcement Learning",
        "authors": "Dawei WangWeizi LiLei ZhuJia Pan",
        "links": "http://arxiv.org/abs/2403.14879v1",
        "entry_id": "http://arxiv.org/abs/2403.14879v1",
        "pdf_url": "http://arxiv.org/pdf/2403.14879v1",
        "summary": "The management of mixed traffic that consists of robot vehicles (RVs) and\nhuman-driven vehicles (HVs) at complex intersections presents a multifaceted\nchallenge. Traditional signal controls often struggle to adapt to dynamic\ntraffic conditions and heterogeneous vehicle types. Recent advancements have\nturned to strategies based on reinforcement learning (RL), leveraging its\nmodel-free nature, real-time operation, and generalizability over different\nscenarios. We introduce a hierarchical RL framework to manage mixed traffic\nthrough precise longitudinal and lateral control of RVs. Our proposed\nhierarchical framework combines the state-of-the-art mixed traffic control\nalgorithm as a high level decision maker to improve the performance and\nrobustness of the whole system. Our experiments demonstrate that the framework\ncan reduce the average waiting time by up to 54% compared to the\nstate-of-the-art mixed traffic control method. When the RV penetration rate\nexceeds 60%, our technique consistently outperforms conventional traffic signal\ncontrol programs in terms of the average waiting time for all vehicles at the\nintersection.",
        "updated": "2024-03-21 23:00:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.14879v1"
    }
]