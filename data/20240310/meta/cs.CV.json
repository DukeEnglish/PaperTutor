[
    {
        "title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed",
        "authors": "Yifan WangXingyi HeSida PengDongli TanXiaowei Zhou",
        "links": "http://arxiv.org/abs/2403.04765v1",
        "entry_id": "http://arxiv.org/abs/2403.04765v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04765v1",
        "summary": "We present a novel method for efficiently producing semi-dense matches across\nimages. Previous detector-free matcher LoFTR has shown remarkable matching\ncapability in handling large-viewpoint change and texture-poor scenarios but\nsuffers from low efficiency. We revisit its design choices and derive multiple\nimprovements for both efficiency and accuracy. One key observation is that\nperforming the transformer over the entire feature map is redundant due to\nshared local information, therefore we propose an aggregated attention\nmechanism with adaptive token selection for efficiency. Furthermore, we find\nspatial variance exists in LoFTR's fine correlation module, which is adverse to\nmatching accuracy. A novel two-stage correlation layer is proposed to achieve\naccurate subpixel correspondences for accuracy improvement. Our efficiency\noptimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass\nstate-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.\nMoreover, extensive experiments show that our method can achieve higher\naccuracy compared with competitive semi-dense matchers, with considerable\nefficiency benefits. This opens up exciting prospects for large-scale or\nlatency-sensitive applications such as image retrieval and 3D reconstruction.\nProject page: https://zju3dv.github.io/efficientloftr.",
        "updated": "2024-03-07 18:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04765v1"
    },
    {
        "title": "That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation",
        "authors": "Georgi PramatarovMatthew GaddPaul NewmanDaniele De Martini",
        "links": "http://arxiv.org/abs/2403.04755v1",
        "entry_id": "http://arxiv.org/abs/2403.04755v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04755v1",
        "summary": "This paper is about 3D pose estimation on LiDAR scans with extremely minimal\nstorage requirements to enable scalable mapping and localisation. We achieve\nthis by clustering all points of segmented scans into semantic objects and\nrepresenting them only with their respective centroid and semantic class. In\nthis way, each LiDAR scan is reduced to a compact collection of four-number\nvectors. This abstracts away important structural information from the scenes,\nwhich is crucial for traditional registration approaches. To mitigate this, we\nintroduce an object-matching network based on self- and cross-correlation that\ncaptures geometric and semantic relationships between entities. The respective\nmatches allow us to recover the relative transformation between scans through\nweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus\n(RANSAC). We demonstrate that such representation is sufficient for metric\nlocalisation by registering point clouds taken under different viewpoints on\nthe KITTI dataset, and at different periods of time localising between KITTI\nand KITTI-360. We achieve accurate metric estimates comparable with\nstate-of-the-art methods with almost half the representation size, specifically\n1.33 kB on average.",
        "updated": "2024-03-07 18:55:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04755v1"
    },
    {
        "title": "I Can't Believe It's Not Scene Flow!",
        "authors": "Ishan KhatriKyle VedderNeehar PeriDeva RamananJames Hays",
        "links": "http://arxiv.org/abs/2403.04739v1",
        "entry_id": "http://arxiv.org/abs/2403.04739v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04739v1",
        "summary": "Current scene flow methods broadly fail to describe motion on small objects,\nand current scene flow evaluation protocols hide this failure by averaging over\nmany points, with most drawn larger objects. To fix this evaluation failure, we\npropose a new evaluation protocol, Bucket Normalized EPE, which is class-aware\nand speed-normalized, enabling contextualized error comparisons between object\ntypes that move at vastly different speeds. To highlight current method\nfailures, we propose a frustratingly simple supervised scene flow baseline,\nTrackFlow, built by bolting a high-quality pretrained detector (trained using\nmany class rebalancing techniques) onto a simple tracker, that produces\nstate-of-the-art performance on current standard evaluations and large\nimprovements over prior art on our new evaluation. Our results make it clear\nthat all scene flow evaluations must be class and speed aware, and supervised\nscene flow methods must address point class imbalances. We release the\nevaluation code publicly at\nhttps://github.com/kylevedder/BucketedSceneFlowEval.",
        "updated": "2024-03-07 18:46:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04739v1"
    },
    {
        "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
        "authors": "Jielin QiuAndrea MadottoZhaojiang LinPaul A. CrookYifan Ethan XuXin Luna DongChristos FaloutsosLei LiBabak DamavandiSeungwhan Moon",
        "links": "http://arxiv.org/abs/2403.04735v1",
        "entry_id": "http://arxiv.org/abs/2403.04735v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04735v1",
        "summary": "Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.",
        "updated": "2024-03-07 18:38:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04735v1"
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "authors": "Yizhe ZhangHe BaiRuixiang ZhangJiatao GuShuangfei ZhaiJosh SusskindNavdeep Jaitly",
        "links": "http://arxiv.org/abs/2403.04732v1",
        "entry_id": "http://arxiv.org/abs/2403.04732v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04732v1",
        "summary": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "updated": "2024-03-07 18:35:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04732v1"
    }
]