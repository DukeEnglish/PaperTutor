[
    {
        "title": "Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization",
        "authors": "Zhaolin RenNa Li",
        "links": "http://arxiv.org/abs/2403.04764v1",
        "entry_id": "http://arxiv.org/abs/2403.04764v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04764v1",
        "summary": "This paper presents a new approach for batch Bayesian Optimization (BO),\nwhere the sampling takes place by minimizing a Thompson Sampling approximation\nof a regret to uncertainty ratio. Our objective is able to coordinate the\nactions chosen in each batch in a way that minimizes redundancy between points\nwhilst focusing on points with high predictive means or high uncertainty. We\nprovide high-probability theoretical guarantees on the regret of our algorithm.\nFinally, numerically, we demonstrate that our method attains state-of-the-art\nperformance on a range of nonconvex test functions, where it outperforms\nseveral competitive benchmark batch BO algorithms by an order of magnitude on\naverage.",
        "updated": "2024-03-07 18:58:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04764v1"
    },
    {
        "title": "GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks",
        "authors": "Lisa SchneckenreiterRichard FreinschlagFlorian SestakJohannes BrandstetterGünter KlambauerAndreas Mayr",
        "links": "http://arxiv.org/abs/2403.04747v1",
        "entry_id": "http://arxiv.org/abs/2403.04747v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04747v1",
        "summary": "Graph neural networks (GNNs), and especially message-passing neural networks,\nexcel in various domains such as physics, drug discovery, and molecular\nmodeling. The expressivity of GNNs with respect to their ability to\ndiscriminate non-isomorphic graphs critically depends on the functions employed\nfor message aggregation and graph-level readout. By applying signal propagation\ntheory, we propose a variance-preserving aggregation function (VPA) that\nmaintains expressivity, but yields improved forward and backward dynamics.\nExperiments demonstrate that VPA leads to increased predictive performance for\npopular GNN architectures as well as improved learning dynamics. Our results\ncould pave the way towards normalizer-free or self-normalizing GNNs.",
        "updated": "2024-03-07 18:52:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04747v1"
    },
    {
        "title": "SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions",
        "authors": "Ilias DiakonikolasDaniel KaneLisheng RenYuxin Sun",
        "links": "http://arxiv.org/abs/2403.04744v1",
        "entry_id": "http://arxiv.org/abs/2403.04744v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04744v1",
        "summary": "We study the complexity of Non-Gaussian Component Analysis (NGCA) in the\nStatistical Query (SQ) model. Prior work developed a general methodology to\nprove SQ lower bounds for this task that have been applicable to a wide range\nof contexts. In particular, it was known that for any univariate distribution\n$A$ satisfying certain conditions, distinguishing between a standard\nmultivariate Gaussian and a distribution that behaves like $A$ in a random\nhidden direction and like a standard Gaussian in the orthogonal complement, is\nSQ-hard. The required conditions were that (1) $A$ matches many low-order\nmoments with the standard univariate Gaussian, and (2) the chi-squared norm of\n$A$ with respect to the standard Gaussian is finite. While the moment-matching\ncondition is necessary for hardness, the chi-squared condition was only\nrequired for technical reasons. In this work, we establish that the latter\ncondition is indeed not necessary. In particular, we prove near-optimal SQ\nlower bounds for NGCA under the moment-matching condition only. Our result\nnaturally generalizes to the setting of a hidden subspace. Leveraging our\ngeneral SQ lower bound, we obtain near-optimal SQ lower bounds for a range of\nconcrete estimation tasks where existing techniques provide sub-optimal or even\nvacuous guarantees.",
        "updated": "2024-03-07 18:49:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04744v1"
    },
    {
        "title": "A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation",
        "authors": "Ankit Pensia",
        "links": "http://arxiv.org/abs/2403.04726v1",
        "entry_id": "http://arxiv.org/abs/2403.04726v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04726v1",
        "summary": "We study the algorithmic problem of sparse mean estimation in the presence of\nadversarial outliers. Specifically, the algorithm observes a \\emph{corrupted}\nset of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean\n$\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works\nhas developed efficient algorithms for robust sparse mean estimation with\nsample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2\n\\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of\ncontamination. In particular, the fastest runtime of existing algorithms is\nquadratic ($\\Omega(d^2)$), which can be prohibitive in high dimensions. This\nquadratic barrier in the runtime stems from the reliance of these algorithms on\nthe sample covariance matrix, which is of size $d^2$. Our main contribution is\nan algorithm for robust sparse mean estimation which runs in\n\\emph{subquadratic} time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples. We\nalso provide analogous results for robust sparse PCA. Our results build on\nalgorithmic advances in detecting weak correlations, a generalized version of\nthe light-bulb problem by Valiant.",
        "updated": "2024-03-07 18:23:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04726v1"
    },
    {
        "title": "Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration",
        "authors": "Julian RodemannFederico CroppiPhilipp ArensYusuf SaleJulia HerbingerBernd BischlEyke HüllermeierThomas AugustinConor J. WalshGiuseppe Casalicchio",
        "links": "http://arxiv.org/abs/2403.04629v1",
        "entry_id": "http://arxiv.org/abs/2403.04629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04629v1",
        "summary": "Bayesian optimization (BO) with Gaussian processes (GP) has become an\nindispensable algorithm for black box optimization problems. Not without a dash\nof irony, BO is often considered a black box itself, lacking ways to provide\nreasons as to why certain parameters are proposed to be evaluated. This is\nparticularly relevant in human-in-the-loop applications of BO, such as in\nrobotics. We address this issue by proposing ShapleyBO, a framework for\ninterpreting BO's proposals by game-theoretic Shapley values.They quantify each\nparameter's contribution to BO's acquisition function. Exploiting the linearity\nof Shapley values, we are further able to identify how strongly each parameter\ndrives BO's exploration and exploitation for additive acquisition functions\nlike the confidence bound. We also show that ShapleyBO can disentangle the\ncontributions to exploration into those that explore aleatoric and epistemic\nuncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human\nmachine interface (HMI), allowing users to interfere with BO in case proposals\ndo not align with human reasoning. We demonstrate this HMI's benefits for the\nuse case of personalizing wearable robotic devices (assistive back exosuits) by\nhuman-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO\ncan achieve lower regret than teams without.",
        "updated": "2024-03-07 16:13:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04629v1"
    }
]