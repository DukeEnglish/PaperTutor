[
    {
        "title": "DeepSee: Multidimensional Visualizations of Seabed Ecosystems",
        "authors": "Adam CosciaHaley M. SapersNoah DeutschMalika KhuranaJohn S. MagyarSergio A. ParraDaniel R. UtterRebecca L. WipflerDavid W. CaressEric J. MartinJennifer B. PaduanMaggie HendrieSantiago LombeydaHillary MushkinAlex EndertScott DavidoffVictoria J. Orphan",
        "links": "http://dx.doi.org/10.1145/3613904.3642001",
        "entry_id": "http://arxiv.org/abs/2403.04761v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04761v1",
        "summary": "Scientists studying deep ocean microbial ecosystems use limited numbers of\nsediment samples collected from the seafloor to characterize important\nlife-sustaining biogeochemical cycles in the environment. Yet conducting\nfieldwork to sample these extreme remote environments is both expensive and\ntime consuming, requiring tools that enable scientists to explore the sampling\nhistory of field sites and predict where taking new samples is likely to\nmaximize scientific return. We conducted a collaborative, user-centered design\nstudy with a team of scientific researchers to develop DeepSee, an interactive\ndata workspace that visualizes 2D and 3D interpolations of biogeochemical and\nmicrobial processes in context together with sediment sampling history overlaid\non 2D seafloor maps. Based on a field deployment and qualitative interviews, we\nfound that DeepSee increased the scientific return from limited sample sizes,\ncatalyzed new research workflows, reduced long-term costs of sharing data, and\nsupported teamwork and communication between team members with diverse research\ngoals.",
        "updated": "2024-03-07 18:56:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04761v1"
    },
    {
        "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
        "authors": "Adam CosciaLangdon HolmesWesley MorrisJoon Suh ChoiScott CrossleyAlex Endert",
        "links": "http://dx.doi.org/10.1145/3640543.3645142",
        "entry_id": "http://arxiv.org/abs/2403.04760v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04760v1",
        "summary": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
        "updated": "2024-03-07 18:56:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04760v1"
    },
    {
        "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
        "authors": "Adam CosciaAlex Endert",
        "links": "http://dx.doi.org/10.1109/TVCG.2023.3346713",
        "entry_id": "http://arxiv.org/abs/2403.04758v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04758v1",
        "summary": "Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.",
        "updated": "2024-03-07 18:56:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04758v1"
    },
    {
        "title": "Preliminary Guidelines For Combining Data Integration and Visual Data Analysis",
        "authors": "Adam CosciaAshley SuhRemco ChangAlex Endert",
        "links": "http://dx.doi.org/10.1109/TVCG.2023.3334513",
        "entry_id": "http://arxiv.org/abs/2403.04757v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04757v1",
        "summary": "Data integration is often performed to consolidate information from multiple\ndisparate data sources during visual data analysis. However, integration\noperations are usually separate from visual analytics operations such as encode\nand filter in both interface design and empirical research. We conducted a\npreliminary user study to investigate whether and how data integration should\nbe incorporated directly into the visual analytics process. We used two\ninterface alternatives featuring contrasting approaches to the data preparation\nand analysis workflow: manual file-based ex-situ integration as a separate step\nfrom visual analytics operations; and automatic UI-based in-situ integration\nmerged with visual analytics operations. Participants were asked to complete\nspecific and free-form tasks with each interface, browsing for patterns,\ngenerating insights, and summarizing relationships between attributes\ndistributed across multiple files. Analyzing participants' interactions and\nfeedback, we found both task completion time and total interactions to be\nsimilar across interfaces and tasks, as well as unique integration strategies\nbetween interfaces and emergent behaviors related to satisficing and cognitive\nbias. Participants' time spent and interactions revealed that in-situ\nintegration enabled users to spend more time on analysis tasks compared with\nex-situ integration. Participants' integration strategies and analytical\nbehaviors revealed differences in interface usage for generating and tracking\nhypotheses and insights. With these results, we synthesized preliminary\nguidelines for designing future visual analytics interfaces that can support\nintegrating attributes throughout an active analysis process.",
        "updated": "2024-03-07 18:56:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04757v1"
    },
    {
        "title": "QRtree -- Decision Tree dialect specification of QRscript",
        "authors": "Stefano ScanzioMatteo RosaniMattia ScamuzziGianluca Cena",
        "links": "http://arxiv.org/abs/2403.04716v1",
        "entry_id": "http://arxiv.org/abs/2403.04716v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04716v1",
        "summary": "This specification document specifies the syntax and semantics of QRtree,\nwhich is a specific dialect of QRscript particularly suited to represent\ndecision trees without chance nodes. The term dialect identifies one of the\npossible sub-languages that can be encoded inside of an eQR code via QRscript.\nThis specification will describe an intermediate representation of QRtree, made\nthrough a language derived by the three-address code. It will then define the\ntransformation rules from the intermediate representation to a binary code. The\nlatter is a binary representation called eQRtreebytecode. These rules can also\nbe applied inversely to transform the eQRtreeBytecode into the intermediate\nrepresentation. This specification document will pay particular attention to\nthe creation of a compact eQRtreebytecode, as the maximum number of bits that\ncan be stored in a QR code is, at the time of writing, equal to 2953 bytes (in\nthe case of QR code version 40 with a \"low\" error correction level).",
        "updated": "2024-03-07 18:14:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04716v1"
    }
]