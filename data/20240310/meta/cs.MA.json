[
    {
        "title": "Distributed Multi-objective Optimization in Cyber-Physical Energy Systems",
        "authors": "Sanja StarkEmilie FrostMarvin Nebel-Wenner",
        "links": "http://arxiv.org/abs/2403.04627v1",
        "entry_id": "http://arxiv.org/abs/2403.04627v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04627v1",
        "summary": "Managing complex Cyber-Physical Energy Systems (CPES) requires solving\nvarious optimization problems with multiple objectives and constraints. As\ndistributed control architectures are becoming more popular in CPES for certain\ntasks due to their flexibility, robustness, and privacy protection,\nmulti-objective optimization must also be distributed. For this purpose, we\npresent MO-COHDA, a fully distributed, agent-based algorithm, for solving\nmulti-objective optimization problems of CPES. MO-COHDA allows an easy and\nflexible adaptation to different use cases and integration of custom\nfunctionality. To evaluate the effectiveness of MO-COHDA, we compare it to a\ncentral NSGA-2 algorithm using multi-objective benchmark functions from the ZDT\nproblem suite. The results show that MO-COHDA can approximate the reference\nfront of the benchmark problems well and is suitable for solving\nmulti-objective optimization problems. In addition, an example use case of\nscheduling a group of generation units while optimizing three different\nobjectives was evaluated to show how MO-COHDA can be easily applied to\nreal-world optimization problems in CPES.",
        "updated": "2024-03-07 16:12:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04627v1"
    },
    {
        "title": "Cooperative Bayesian Optimization for Imperfect Agents",
        "authors": "Ali KhoshvishkaiePetrus MikkolaPierre-Alexandre MurenaSamuel Kaski",
        "links": "http://dx.doi.org/10.1007/978-3-031-43412-9_28",
        "entry_id": "http://arxiv.org/abs/2403.04442v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04442v1",
        "summary": "We introduce a cooperative Bayesian optimization problem for optimizing\nblack-box functions of two variables where two agents choose together at which\npoints to query the function but have only control over one variable each. This\nsetting is inspired by human-AI teamwork, where an AI-assistant helps its human\nuser solve a problem, in this simplest case, collaborative optimization. We\nformulate the solution as sequential decision-making, where the agent we\ncontrol models the user as a computationally rational agent with prior\nknowledge about the function. We show that strategic planning of the queries\nenables better identification of the global maximum of the function as long as\nthe user avoids excessive exploration. This planning is made possible by using\nBayes Adaptive Monte Carlo planning and by endowing the agent with a user model\nthat accounts for conservative belief updates and exploratory sampling of the\npoints to query.",
        "updated": "2024-03-07 12:16:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04442v1"
    },
    {
        "title": "Cooperative Task Execution in Multi-Agent Systems",
        "authors": "KarishmaShrisha Rao",
        "links": "http://arxiv.org/abs/2403.04370v1",
        "entry_id": "http://arxiv.org/abs/2403.04370v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04370v1",
        "summary": "We propose a multi-agent system that enables groups of agents to collaborate\nand work autonomously to execute tasks. Groups can work in a decentralized\nmanner and can adapt to dynamic changes in the environment. Groups of agents\nsolve assigned tasks by exploring the solution space cooperatively based on the\nhighest reward first. The tasks have a dependency structure associated with\nthem. We rigorously evaluated the performance of the system and the individual\ngroup performance using centralized and decentralized control approaches for\ntask distribution. Based on the results, the centralized approach is more\nefficient for systems with a less-dependent system $G_{18}$, while the\ndecentralized approach performs better for systems with a highly-dependent\nsystem $G_{40}$. We also evaluated task allocation to groups that do not have\ninterdependence. Our findings reveal that there was significantly less\ndifference in the number of tasks allocated to each group in a less-dependent\nsystem than in a highly-dependent one. The experimental results showed that a\nlarge number of small-size cooperative groups of agents unequivocally improved\nthe system's performance compared to a small number of large-size cooperative\ngroups of agents. Therefore, it is essential to identify the optimal group size\nfor a system to enhance its performance.",
        "updated": "2024-03-07 09:58:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04370v1"
    },
    {
        "title": "Generalizing Cooperative Eco-driving via Multi-residual Task Learning",
        "authors": "Vindula JayawardanaSirui LiCathy WuYashar FaridKentaro Oguchi",
        "links": "http://arxiv.org/abs/2403.04232v1",
        "entry_id": "http://arxiv.org/abs/2403.04232v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04232v1",
        "summary": "Conventional control, such as model-based control, is commonly utilized in\nautonomous driving due to its efficiency and reliability. However, real-world\nautonomous driving contends with a multitude of diverse traffic scenarios that\nare challenging for these planning algorithms. Model-free Deep Reinforcement\nLearning (DRL) presents a promising avenue in this direction, but learning DRL\ncontrol policies that generalize to multiple traffic scenarios is still a\nchallenge. To address this, we introduce Multi-residual Task Learning (MRTL), a\ngeneric learning framework based on multi-task learning that, for a set of task\nscenarios, decomposes the control into nominal components that are effectively\nsolved by conventional control methods and residual terms which are solved\nusing learning. We employ MRTL for fleet-level emission reduction in mixed\ntraffic using autonomous vehicles as a means of system control. By analyzing\nthe performance of MRTL across nearly 600 signalized intersections and 1200\ntraffic scenarios, we demonstrate that it emerges as a promising approach to\nsynergize the strengths of DRL and conventional methods in generalizable\ncontrol.",
        "updated": "2024-03-07 05:25:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04232v1"
    },
    {
        "title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
        "authors": "Elizaveta TennantStephen HailesMirco Musolesi",
        "links": "http://arxiv.org/abs/2403.04202v1",
        "entry_id": "http://arxiv.org/abs/2403.04202v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04202v1",
        "summary": "Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.",
        "updated": "2024-03-07 04:12:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04202v1"
    }
]