[
    {
        "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
        "authors": "Boshi WangHao FangJason EisnerBenjamin Van DurmeYu Su",
        "links": "http://arxiv.org/abs/2403.04746v1",
        "entry_id": "http://arxiv.org/abs/2403.04746v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04746v1",
        "summary": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
        "updated": "2024-03-07 18:50:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04746v1"
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "authors": "Yizhe ZhangHe BaiRuixiang ZhangJiatao GuShuangfei ZhaiJosh SusskindNavdeep Jaitly",
        "links": "http://arxiv.org/abs/2403.04732v1",
        "entry_id": "http://arxiv.org/abs/2403.04732v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04732v1",
        "summary": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "updated": "2024-03-07 18:35:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04732v1"
    },
    {
        "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
        "authors": "Chen LiWeiqi WangJingcheng HuYixuan WeiNanning ZhengHan HuZheng ZhangHouwen Peng",
        "links": "http://arxiv.org/abs/2403.04706v1",
        "entry_id": "http://arxiv.org/abs/2403.04706v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04706v1",
        "summary": "Mathematical capabilities were previously believed to emerge in common\nlanguage models only at a very large scale or require extensive math-related\npre-training. This paper shows that the LLaMA-2 7B model with common\npre-training already exhibits strong mathematical abilities, as evidenced by\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\nrespectively, when selecting the best response from 256 random generations. The\nprimary issue with the current base model is the difficulty in consistently\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\nrespectively. We find that simply scaling up the SFT data can significantly\nenhance the reliability of generating correct answers. However, the potential\nfor extensive scaling is constrained by the scarcity of publicly available math\nquestions. To overcome this limitation, we employ synthetic data, which proves\nto be nearly as effective as real data and shows no clear saturation when\nscaled up to approximately one million samples. This straightforward approach\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\nprovide insights into scaling behaviors across different reasoning complexities\nand error types.",
        "updated": "2024-03-07 18:00:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04706v1"
    },
    {
        "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
        "authors": "Ekaterina FadeevaAleksandr RubashevskiiArtem ShelmanovSergey PetrakovHaonan LiHamdy MubarakEvgenii TsymbalovGleb KuzminAlexander PanchenkoTimothy BaldwinPreslav NakovMaxim Panov",
        "links": "http://arxiv.org/abs/2403.04696v1",
        "entry_id": "http://arxiv.org/abs/2403.04696v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04696v1",
        "summary": "Large language models (LLMs) are notorious for hallucinating, i.e., producing\nerroneous claims in their output. Such hallucinations can be dangerous, as\noccasional factual inaccuracies in the generated text might be obscured by the\nrest of the output being generally factual, making it extremely hard for the\nusers to spot them. Current services that leverage LLMs usually do not provide\nany means for detecting unreliable generations. Here, we aim to bridge this\ngap. In particular, we propose a novel fact-checking and hallucination\ndetection pipeline based on token-level uncertainty quantification. Uncertainty\nscores leverage information encapsulated in the output of a neural network or\nits layers to detect unreliable predictions, and we show that they can be used\nto fact-check the atomic claims in the LLM output. Moreover, we present a novel\ntoken-level uncertainty quantification method that removes the impact of\nuncertainty about what claim to generate on the current step and what surface\nform to use. Our method Claim Conditioned Probability (CCP) measures only the\nuncertainty of particular claim value expressed by the model. Experiments on\nthe task of biography generation demonstrate strong improvements for CCP\ncompared to the baselines for six different LLMs and three languages. Human\nevaluation reveals that the fact-checking pipeline based on uncertainty\nquantification is competitive with a fact-checking tool that leverages external\nknowledge.",
        "updated": "2024-03-07 17:44:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04696v1"
    },
    {
        "title": "Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication",
        "authors": "Jacqueline G. CavazosNia Nixon",
        "links": "http://arxiv.org/abs/2403.04671v1",
        "entry_id": "http://arxiv.org/abs/2403.04671v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04671v1",
        "summary": "Collaborative problem-solving (CPS) is a vital skill used both in the\nworkplace and in educational environments. CPS is useful in tackling\nincreasingly complex global, economic, and political issues and is considered a\ncentral 21st century skill. The increasingly connected global community\npresents a fruitful opportunity for creative and collaborative problem-solving\ninteractions and solutions that involve diverse perspectives. Unfortunately,\nwomen and underrepresented minorities (URMs) often face obstacles during\ncollaborative interactions that hinder their key participation in these\nproblem-solving conversations. Here, we explored the communication patterns of\nminority and non-minority individuals working together in a CPS task. Group\nCommunication Analysis (GCA), a temporally-sensitive computational linguistic\ntool, was used to examine how URM status impacts individuals' sociocognitive\nlinguistic patterns. Results show differences across racial/ethnic groups in\nkey sociocognitive features that indicate fruitful collaborative interactions.\nWe also investigated how the groups' racial/ethnic composition impacts both\nindividual and group communication patterns. In general, individuals in more\ndemographically diverse groups displayed more productive communication\nbehaviors than individuals who were in majority-dominated groups. We discuss\nthe implications of individual and group diversity on communication patterns\nthat emerge during CPS and how these patterns can impact collaborative\noutcomes.",
        "updated": "2024-03-07 17:17:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04671v1"
    }
]