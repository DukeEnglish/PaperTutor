[
    {
        "title": "Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization",
        "authors": "Zhaolin RenNa Li",
        "links": "http://arxiv.org/abs/2403.04764v1",
        "entry_id": "http://arxiv.org/abs/2403.04764v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04764v1",
        "summary": "This paper presents a new approach for batch Bayesian Optimization (BO),\nwhere the sampling takes place by minimizing a Thompson Sampling approximation\nof a regret to uncertainty ratio. Our objective is able to coordinate the\nactions chosen in each batch in a way that minimizes redundancy between points\nwhilst focusing on points with high predictive means or high uncertainty. We\nprovide high-probability theoretical guarantees on the regret of our algorithm.\nFinally, numerically, we demonstrate that our method attains state-of-the-art\nperformance on a range of nonconvex test functions, where it outperforms\nseveral competitive benchmark batch BO algorithms by an order of magnitude on\naverage.",
        "updated": "2024-03-07 18:58:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04764v1"
    },
    {
        "title": "BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization",
        "authors": "Amber Yijia ZhengTong HeYixuan QiuMinjie WangDavid Wipf",
        "links": "http://arxiv.org/abs/2403.04763v1",
        "entry_id": "http://arxiv.org/abs/2403.04763v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04763v1",
        "summary": "Bilevel optimization refers to scenarios whereby the optimal solution of a\nlower-level energy function serves as input features to an upper-level\nobjective of interest. These optimal features typically depend on tunable\nparameters of the lower-level energy in such a way that the entire bilevel\npipeline can be trained end-to-end. Although not generally presented as such,\nthis paper demonstrates how a variety of graph learning techniques can be\nrecast as special cases of bilevel optimization or simplifications thereof. In\nbrief, building on prior work we first derive a more flexible class of energy\nfunctions that, when paired with various descent steps (e.g., gradient descent,\nproximal methods, momentum, etc.), form graph neural network (GNN)\nmessage-passing layers; critically, we also carefully unpack where any residual\napproximation error lies with respect to the underlying constituent\nmessage-passing functions. We then probe several simplifications of this\nframework to derive close connections with non-GNN-based graph learning\napproaches, including knowledge graph embeddings, various forms of label\npropagation, and efficient graph-regularized MLP models. And finally, we\npresent supporting empirical results that demonstrate the versatility of the\nproposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel\nOptimization Offers More Graph Machine Learning. Our code is available at\nhttps://github.com/amberyzheng/BloomGML. Let graph ML bloom.",
        "updated": "2024-03-07 18:57:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04763v1"
    },
    {
        "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
        "authors": "Adam CosciaLangdon HolmesWesley MorrisJoon Suh ChoiScott CrossleyAlex Endert",
        "links": "http://dx.doi.org/10.1145/3640543.3645142",
        "entry_id": "http://arxiv.org/abs/2403.04760v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04760v1",
        "summary": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
        "updated": "2024-03-07 18:56:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04760v1"
    },
    {
        "title": "Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing",
        "authors": "Xiaofan YuAnthony ThomasIvannia Gomez MorenoLouis GutierrezTajana Rosing",
        "links": "http://arxiv.org/abs/2403.04759v1",
        "entry_id": "http://arxiv.org/abs/2403.04759v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04759v1",
        "summary": "On-device learning has emerged as a prevailing trend that avoids the slow\nresponse time and costly communication of cloud-based learning. The ability to\nlearn continuously and indefinitely in a changing environment, and with\nresource constraints, is critical for real sensor deployments. However,\nexisting designs are inadequate for practical scenarios with (i) streaming data\ninput, (ii) lack of supervision and (iii) limited on-board resources. In this\npaper, we design and deploy the first on-device lifelong learning system called\nLifeHD for general IoT applications with limited supervision. LifeHD is\ndesigned based on a novel neurally-inspired and lightweight learning paradigm\ncalled Hyperdimensional Computing (HDC). We utilize a two-tier associative\nmemory organization to intelligently store and manage high-dimensional,\nlow-precision vectors, which represent the historical patterns as cluster\ncentroids. We additionally propose two variants of LifeHD to cope with scarce\nlabeled inputs and power constraints. We implement LifeHD on off-the-shelf edge\nplatforms and perform extensive evaluations across three scenarios. Our\nmeasurements show that LifeHD improves the unsupervised clustering accuracy by\nup to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong\nlearning baselines with as much as 34.3x better energy efficiency. Our code is\navailable at https://github.com/Orienfish/LifeHD.",
        "updated": "2024-03-07 18:56:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04759v1"
    },
    {
        "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
        "authors": "Adam CosciaAlex Endert",
        "links": "http://dx.doi.org/10.1109/TVCG.2023.3346713",
        "entry_id": "http://arxiv.org/abs/2403.04758v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04758v1",
        "summary": "Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.",
        "updated": "2024-03-07 18:56:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04758v1"
    }
]