Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed
YifanWang∗ XingyiHe∗ SidaPeng DongliTan XiaoweiZhou†
ZhejiangUniversity
Abstract
ROMA Ours
We present a novel method for efficiently producing semi- 56
AspanFormer
dense matches across images. Previous detector-free
TopicFM Ours (Opt)
matcherLoFTRhasshownremarkablematchingcapability
54 QuadTree
in handling large-viewpoint change and texture-poor
LoFTR
scenarios but suffers from low efficiency. We revisit its
design choices and derive multiple improvements for
52
bothefficiencyandaccuracy. Onekeyobservationisthat
performingthetransformerovertheentirefeaturemapis SP + LG (Opt)
redundant due to shared local information, therefore we 50
proposeanaggregatedattentionmechanismwithadaptive 0 10 20 30 40
Image Pairs Per Second
tokenselectionforefficiency. Furthermore,wefindspatial
varianceexistsinLoFTR’sfinecorrelationmodule,whichis Figure1.MatchingAccuracyandEfficiencyComparisons.Our
adversetomatchingaccuracy.Anoveltwo-stagecorrelation methodoutperformscompetitivesemi-densematchers( )onaccu-
layerisproposedtoachieveaccuratesubpixelcorrespon- racyatasignificantlyhigherspeed.Comparedwithdensematcher
dencesforaccuracyimprovement. Ourefficiencyoptimized ROMA( ),ourmethodis∼7.5×fasterwithcomparableperfor-
modelis 2.5 fasterthanLoFTRwhichcanevensurpass mance.Moreover,ourefficiencyoptimizedmodel( )cansurpass
state-of-t∼ he-ar× t efficient sparse matching pipeline Super- therobustsparsematchingpipeline( )SuperPoint(SP)+Light-
Point+LightGlue. Moreover,extensiveexperimentsshow Glue(LG)onefficiencywithconsiderablybetteraccuracy.
that our method can achieve higher accuracy compared
handcrafted[29]orlearning-basedmatchers[28,41]. These
with competitive semi-dense matchers, with considerable
detector-basedmethodsareefficientbutsufferfromrobustly
efficiency benefits. This opens up exciting prospects for
detectingrepeatablekeypointsacrosschallengingpairs,such
large-scaleorlatency-sensitiveapplicationssuchasimage
asextremeviewpointchangesandtexture-poorregions.
retrieval and 3D reconstruction. Project page: https:
//zju3dv.github.io/efficientloftr/. Recently,LoFTR[48]introducesadetector-freematching
paradigmwithtransformertodirectlyestablishsemi-dense
correspondencesbetweentwoimageswithoutdetectingkey-
1.Introduction points. Withthehelpofthetransformermechanismtocap-
turetheglobalimagecontextandthedetector-freedesign,
Image matching is the cornerstone of many 3D computer LoFTRshowsastrongcapabilityofmatchingchallenging
visiontasks,whichaimtofindasetofhighlyaccuratecor- pairs, especially in texture-poor scenarios. To reduce the
respondencesgivenanimagepair. Theestablishedmatches computationburden,LoFTRadoptsacoarse-to-finepipeline
betweenimageshavebroadusagessuchasreconstructing byfirstperformingdensematchingondownsampledcoarse
the3Dworldbystructurefrommotion(SfM)[1,20,27,45] featuresmaps,wheretransformerisapplied. Then,thefea-
orSLAMsystem[32,33],andvisuallocalization[40,42], ture locations of coarse matches on one image are fixed,
etc. Previousmethodstypicallyfollowatwo-stagepipeline: while their subpixel correspondences are searched on the
theyfirstdetect[39]anddescribe[51]asetofkeypointson other image by cropping feature patches based on coarse
eachimage,andthenestablishkeypointcorrespondencesby match,performingthefeaturecorrelation,andcalculating
expectationoverthecorrelationpatch.
∗Equalcontribution.TheauthorsfromZhejiangUniversityareaffili-
atedwiththeStateKeyLabofCAD&CG.†Correspondingauthor:Xiaowei Despite its impressive matching performance, LoFTR
Zhou. suffers from limited efficiency due to the large token size
1
4202
raM
7
]VC.sc[
1v56740.3042:viXra
5
@
CUA
esoP
evitaleRofperformingtransformerontheentirecoarsefeaturemap, • Anovelaggregatedattentionnetworkforefficientlocal
whichsignificantlybarricadespracticallarge-scaleusages featuretransformation.
suchasimageretrieval[18]andSfM[45]. Alargebunchof • Anoveltwo-stagecorrelationrefinementlayerforaccurate
LoFTR’sfollow-upworks[7,17,34,50,57]haveattempted andsubpixel-levelrefinedcorrespondences.
toimproveitsmatchingaccuracy. However,therearerare
methodsthatfocusonmatchingefficiencyofdetector-free 2.RelatedWork
matching. QuadTreeAttention[50]incorporatesmulti-scale
transformation with a gradually narrowed attention span Detector-BasedImageMatching. Classicalimagematch-
toavoidperformingattentiononlargefeaturemaps. This ing methods [4, 29, 39] adopt handcrafted critics for de-
strategycanreducethecomputationcost,butitalsodivides tectingkeypoints,describingandthenmatchingthem. Re-
asinglecoarseattentionprocessintomultiplesteps,leading centmethodsdrawbenefitsfromdeepneuralnetworksfor
toincreasedlatency. bothdetection[23,39,44]anddescription[13,31,51,52],
In this paper, we revisit the design decisions of the where the robustness and discriminativeness of local de-
detector-free matcher LoFTR, and propose a new match- scriptorsaresignificantlyimproved. Besides,somemethods
ingalgorithmthatsqueezesoutredundantcomputationsfor [10, 12, 30, 36, 53] managed to learn the detector and de-
significantly better efficiency while further improving the scriptortogether.SuperGlue[41]isapioneeringmethodthat
accuracy. As shown in Fig. 1, our approach achieves the firstintroducesthetransformermechanismintomatching,
bestinferencespeedcomparedwithrecentimagematching whichhasshownnotableimprovementsoverclassicalhand-
methodswhilebeingcompetitiveintermsofaccuracy. Our craftedmatchers. Asasideeffect,italsocostsmoretime,
keyinnovationslieinintroducingatokenaggregationmech- especiallywithmanykeypointstomatch. Toimprovetheef-
anismforefficientfeaturetransformationandatwo-stage ficiency,somesubsequentworks,suchas [6,46],endeavor
correlationlayerforcorrespondencerefinement.Specifically, toreducethesizeoftheattentionmechanism,albeitatthe
we find that densely performing global attention over the costofsacrificingperformance. LightGlue[28]introducesa
entirecoarsefeaturemapasinLoFTRisunnecessary,asthe newschemeforefficientsparsematchingthatisadaptiveto
attentioninformationissimilarandsharedinthelocalregion. thematchingdifficulty,wheretheattentionprocesscanbe
Therefore,wedeviseanaggregatedattentionmechanismto stoppedearlierforeasypairs. ItisfasterthanSuperGlueand
performfeaturetransformationonadaptivelyselectedtokens, can achieve competitive performance. However, robustly
whichissignificantlycompactandeffectivelyreducesthe detectingkeypointsacrossimagesisstillchallenging,espe-
costoflocalfeaturetransformation. cially for texture-poor regions. Unlike them, our method
In addition, we observe that there can be spatial vari- focusesontheefficiencyofthedetector-freemethod,which
anceinthematchingrefinementphaseofLoFTR,whichis eliminatestherestrictionofkeypointdetectionandshows
causedbytheexpectationovertheentirecorrelationpatch superiorperformanceforchallengingpairs.
when noisy feature correlation exists. To solve this issue, Detector-Free Image Matching. Detector-free methods
ourapproachdesignsatwo-stagecorrelationlayerthatfirst
directlymatchimagesinsteadofrelyingonasetofdetected
locatespixel-levelmatcheswiththeaccuratemutual-nearest-
keypoints, producing semi-dense or dense matches. NC-
neighbormatchingonfinefeaturepatches,andthenfurther
Net[37]representsallfeaturesandpossiblematchesasa4D
refinesmatchesforsubpixel-levelbyperformingthecorrela-
correlationvolume. SparseNC-Net[38]utilizessparsecor-
tionandexpectationlocallywithintinypatches.
relationlayerstoeaseresolutionlimitations. Subsequently,
Extensiveexperimentsareconductedonmultipletasks, DRC-Net[25]improvesefficiencyandfurtherimprovesper-
including homography estimation, relative pose recovery, formanceinacoarse-to-finemanner.
as well as visual localization, to show the efficacy of our LoFTR [48] first employs the Transformer in detector-
method. Ourpipelinepushesdetector-freematchingtoun- free matching to model the long-range dependencies. It
precedented efficiency, which is 2.5 times faster than showsremarkablematchingcapabilities, however, suffers
∼
LoFTR and can even surpass the current state-of-the-art fromlowefficiencyduetothehugecomputationofdensely
efficient sparse matcher LightGlue [28]. Moreover, our transforming entire coarse feature maps. Many follow-
framework can achieve comparable or even better match- up works further improve the matching accuracy. Match-
ingaccuracycomparedwithcompetitivedetector-freebase- former[57]andAspanFormer[7]performattentiononmulti-
lines[7,14,15]withconsiderablyhigherefficiency. scalefeatures,wherelocalattentionregionsof[7]arefound
Insummary,thispaperhasthefollowingcontributions: with the help of estimated flow. QuadTree [50] gradually
• Anewdetector-freematchingpipelinewithmultipleim- restrictstheattentionspanduringhierarchicalattentionto
provements based on the comprehensive revisiting of relevantareas,whichcanreduceoverallcomputation. How-
LoFTR, which is significantly more efficient and with ever,thesedesignscontributemarginallyorevendecrease
betteraccuracy. efficiency, since the hierarchical nature of multi-scale at-
21. Local Feature Extraction 4. Two-Stage Fine-Level Matching
Crop by M<latexit sha1_base64="xTTC7gEWmbUmOhvgn9A/ddWws90=">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiGzdCBfuAdiiZNNOGZjJjkimUod/hxoUibv0Yd/6NmXYW2nogcDjnXu7J8WPBtXGcb1RYW9/Y3Cpul3Z29/YPyodHLR0lirImjUSkOj7RTHDJmoYbwTqxYiT0BWv749vMb0+Y0jySj2YaMy8kQ8kDTomxktcLiRlRItL7WZ/2yxWn6syBV4mbkwrkaPTLX71BRJOQSUMF0brrOrHxUqIMp4LNSr1Es5jQMRmyrqWShEx76Tz0DJ9ZZYCDSNknDZ6rvzdSEmo9DX07mYXUy14m/ud1ExNceymXcWKYpItDQSKwiXDWAB5wxagRU0sIVdxmxXREFKHG9lSyJbjLX14lrYuqe1mtPdQq9Zu8jiKcwCmcgwtXUIc7aEATKDzBM7zCG5qgF/SOPhajBZTvHMMfoM8f/taSQg==</latexit> c
MNN
Matching
Correlation &
Expectation
Corr.
Input Coarse Fusion F<latexit sha1_base64="DmLwDRjXHC5kYuAf7uOEcMkni7s=">AAACEnicbVDLSsNAFJ34rPUVdekmWAQFKYkUdVkriMsK9gFNGibTSTt08mDmRiwh3+DGX3HjQhG3rtz5N07bLLT1wIXDOfdy7z1ezJkE0/zWFhaXlldWC2vF9Y3NrW19Z7cpo0QQ2iARj0Tbw5JyFtIGMOC0HQuKA4/Tlje8Gvuteyoki8I7GMXUCXA/ZD4jGJTk6sf2AENqA30Az0+vs8y97MLJnFjrgquXzLI5gTFPrJyUUI66q3/ZvYgkAQ2BcCxlxzJjcFIsgBFOs6KdSBpjMsR92lE0xAGVTjp5KTMOldIz/EioCsGYqL8nUhxIOQo81RlgGMhZbyz+53US8C+clIVxAjQk00V+wg2IjHE+Ro8JSoCPFMFEMHWrQQZYYAIqxaIKwZp9eZ40T8vWWblyWylVa3kcBbSPDtARstA5qqIbVEcNRNAjekav6E170l60d+1j2rqg5TN76A+0zx/I8J7P</latexit>ˆt ,Fˆt Fine <latexit sha1_base64="UsYYuJt3gG8SNQylBsouvxjoNus=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2ge0Q8mkmTY0k4xJplCGfocbF4q49WPc+Tdm2llo64HA4Zx7uScniDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqaZkoQptEcqk6AdaUM0GbhhlOO7GiOAo4bQfj28xvT6jSTIpHM42pH+GhYCEj2FjJ70XYjAjm6cOsz/vlilt150CrxMtJBXI0+uWv3kCSJKLCEI617npubPwUK8MIp7NSL9E0xmSMh7RrqcAR1X46Dz1DZ1YZoFAq+4RBc/X3RoojradRYCezkHrZy8T/vG5iwms/ZSJODBVkcShMODISZQ2gAVOUGD61BBPFbFZERlhhYmxPJVuCt/zlVdK6qHqX1dp9rVK/yesowgmcwjl4cAV1uIMGNIHAEzzDK7w5E+fFeXc+FqMFJ985hj9wPn8AFbOSUQ==</latexit> Sl
Image Pair F<latexit sha1_base64="SrmBUY3IsniVOP7jfYIcExDJ+VM=">AAACEnicbVDJSgNBEO2JW4xb1KOXwSAoSJiRoB5jBPEYwSyQCaGnU5M06VnorhHDMN/gxV/x4kERr568+Td2loMmeVDweK+KqnpuJLhCy/oxMkvLK6tr2fXcxubW9k5+d6+uwlgyqLFQhLLpUgWCB1BDjgKakQTquwIa7uB65DceQCoeBvc4jKDt017APc4oaqmTP3GQiy4kDsIjul5yk6adq9MFYqWTL1hFawxznthTUiBTVDv5b6cbstiHAJmgSrVsK8J2QiVyJiDNObGCiLIB7UFL04D6oNrJ+KXUPNJK1/RCqStAc6z+nUior9TQd3WnT7GvZr2RuMhrxehdthMeRDFCwCaLvFiYGJqjfMwul8BQDDWhTHJ9q8n6VFKGOsWcDsGefXme1M+K9nmxdFcqlCvTOLLkgBySY2KTC1Imt6RKaoSRJ/JC3si78Wy8Gh/G56Q1Y0xn9sk/GF+/0xGe1Q==</latexit>˜ A,F˜ BFeatures Network A BFeatures First-Stage Crop Second-Stage {<latexit sha1_base64="xJMIvEo1FYCUGwvPYfe9q/ysG+s=">AAAB+nicbVDLSsNAFL2pr1pfqS7dDBbBVUlE1GXRjRuhgn1AE8JkOmmHTh7MTJQS8yluXCji1i9x5984abPQ1gMDh3Pu5Z45fsKZVJb1bVRWVtfWN6qbta3tnd09s77flXEqCO2QmMei72NJOYtoRzHFaT8RFIc+pz1/cl34vQcqJIujezVNqBviUcQCRrDSkmfWncwJsRoTzLPb3Auc3DMbVtOaAS0TuyQNKNH2zC9nGJM0pJEiHEs5sK1EuRkWihFO85qTSppgMsEjOtA0wiGVbjaLnqNjrQxREAv9IoVm6u+NDIdSTkNfTxYx5aJXiP95g1QFl27GoiRVNCLzQ0HKkYpR0QMaMkGJ4lNNMBFMZ0VkjAUmSrdV0yXYi19eJt3Tpn3ePLs7a7SuyjqqcAhHcAI2XEALbqANHSDwCM/wCm/Gk/FivBsf89GKUe4cwB8Ynz+2HpRO</latexit> Mf }
2. Efficient Coarse Feature Transform <latexit sha1_base64="PIu+Po+RUVXCq6Wk6i8KcOyhAZ8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRiyepYD+gDWWz3bRLN5u4OxFK6J/w4kERr/4db/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rSeujYjVA44T7kd0oEQoGEUrtbsoIm7IXa9ccavuDGSZeDmpQI56r/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFbVr/Gx274ScWKVPwljbUkhm6u+JjEbGjKPAdkYUh2bRm4r/eZ0Uwys/EypJkSs2XxSmkmBMps+TvtCcoRxbQpkW9lbChlRThjaikg3BW3x5mTTPqt5F9fz+vFK7zuMowhEcwyl4cAk1uIU6NICBhGd4hTfn0Xlx3p2PeWvByWcO4Q+czx+u2o+/</latexit> N 3. Coarse-Level Matching
P Eo ns ci oti do in na gl ⇥ F<latexit sha1_base64="mpT7RYbY8drqeZztYBqI+5Q6cAA=">AAACFnicbVDJSgNBEO2JW4zbqEcvg0HwoGFGgnqMEcRjBLNANno6NUmTnoXuGjEM+Qov/ooXD4p4FW/+jZ3loEkeFDzeq6KqnhsJrtC2f4zU0vLK6lp6PbOxubW9Y+7uVVQYSwZlFopQ1lyqQPAAyshRQC2SQH1XQNXtX4/86gNIxcPgHgcRNH3aDbjHGUUttc3TBnLRgaSB8Iiul9wMh+2rFp4skIstbJtZO2ePYc0TZ0qyZIpS2/xudEIW+xAgE1SpumNH2EyoRM4EDDONWEFEWZ92oa5pQH1QzWT81tA60krH8kKpK0BrrP6dSKiv1MB3dadPsadmvZG4yKvH6F02Ex5EMULAJou8WFgYWqOMrA6XwFAMNKFMcn2rxXpUUoY6yYwOwZl9eZ5UznLOeS5/l88WitM40uSAHJJj4pALUiC3pETKhJEn8kLeyLvxbLwaH8bnpDVlTGf2yT8YX78qN6Ch</latexit>˜t ,F˜t M<latexit sha1_base64="xTTC7gEWmbUmOhvgn9A/ddWws90=">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiGzdCBfuAdiiZNNOGZjJjkimUod/hxoUibv0Yd/6NmXYW2nogcDjnXu7J8WPBtXGcb1RYW9/Y3Cpul3Z29/YPyodHLR0lirImjUSkOj7RTHDJmoYbwTqxYiT0BWv749vMb0+Y0jySj2YaMy8kQ8kDTomxktcLiRlRItL7WZ/2yxWn6syBV4mbkwrkaPTLX71BRJOQSUMF0brrOrHxUqIMp4LNSr1Es5jQMRmyrqWShEx76Tz0DJ9ZZYCDSNknDZ6rvzdSEmo9DX07mYXUy14m/ud1ExNceymXcWKYpItDQSKwiXDWAB5wxagRU0sIVdxmxXREFKHG9lSyJbjLX14lrYuqe1mtPdQq9Zu8jiKcwCmcgwtXUIc7aEATKDzBM7zCG5qgF/SOPhajBZTvHMMfoM8f/taSQg==</latexit> c
A B (<latexit sha1_base64="rhB68Oc0gRFMX/esAAnj+TR8xfY=">AAAC0HicjVHLTsJAFD3UF+ILdemmkZjgBluikZUhuGGJRh4JgmnLgA192U6NhBjj1h9wq19l/AP9C++MJVGJ0Wnanjn3njNz7zUDx464pr2mlJnZufmF9GJmaXlldS27vtGI/Di0WN3yHT9smUbEHNtjdW5zh7WCkBmu6bCmOTwW8eY1CyPb9874KGAd1xh4dt+2DE5UJ6/vlXa7xWq30uxWLrI5raDJpU4DPQE5JKvmZ19wjh58WIjhgsEDJ+zAQERPGzo0BMR1MCYuJGTLOMMtMqSNKYtRhkHskL4D2rUT1qO98Iyk2qJTHHpDUqrYIY1PeSFhcZoq47F0Fuxv3mPpKe42or+ZeLnEclwS+5dukvlfnaiFo4+SrMGmmgLJiOqsxCWWXRE3V79UxckhIE7gHsVDwpZUTvqsSk0kaxe9NWT8TWYKVuytJDfGu7glDVj/Oc5p0CgW9IOCdrKfKx8lo05jC9vI0zwPUUYVNdTJ+wqPeMKzcqrcKHfK/Weqkko0m/i2lIcPFsaS1Q==</latexit>1/8)2HBWB
Corr. (<latexit sha1_base64="RnPmSRNHAHVF9/tVkRrMGrDT9yw=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEClJSUXRZdOOygn1AW0qSTuvYNAnJRCilG3fi1h9wqx8k/oH+hXfGKahFdEKSM+fec2buvU7o8VhY1mvKmJmdm19IL2aWlldW17LrG9U4SCKXVdzAC6K6Y8fM4z6rCC48Vg8jZg8cj9Wc/pmM125YFPPAvxTDkLUGds/nXe7agqh2divfFNzrsBEf72t0Pd5rZ3NWwVLLnAZFDXLQqxxkX9BEBwFcJBiAwYcg7MFGTE8DRVgIiWthRFxEiKs4wxgZ0iaUxSjDJrZP3x7tGpr1aS89Y6V26RSP3oiUJnZJE1BeRFieZqp4opwl+5v3SHnKuw3p72ivAbECV8T+pZtk/lcnaxHo4kTVwKmmUDGyOle7JKor8ubml6oEOYTESdyheETYVcpJn02liVXtsre2ir+pTMnKvatzE7zLW9KAiz/HOQ2qB4XiUcG6OMyVTvWo09jGDvI0z2OUcI4yKuQ9wiOe8Gw0jVvjzrj/TDVSWrOJb8t4+ABqzZge</latexit>˜i,˜j)
<latexit sha1_base64="BGOnsDHx3qV1t3PrO8hHL1+N9Dw=">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2gdMh5JJM21oJhmSjFCGfoYbF4q49Wvc+Tdm2llo64HA4Zx7ybknTDjTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkqQttEcql6IdaUM0HbhhlOe4miOA457YaT29zvPlGlmRSPZprQIMYjwSJGsLGS34+xGRPMs4fZoFpz6+4caJV4BalBgdag+tUfSpLGVBjCsda+5yYmyLAyjHA6q/RTTRNMJnhEfUsFjqkOsnnkGTqzyhBFUtknDJqrvzcyHGs9jUM7mUfUy14u/uf5qYmug4yJJDVUkMVHUcqRkSi/Hw2ZosTwqSWYKGazIjLGChNjW6rYErzlk1dJ56LuXdYb941a86aoowwncArn4MEVNOEOWtAGAhKe4RXeHOO8OO/Ox2K05BQ7x/AHzucPjpaRcg==</latexit>
Reduced Transformed Score S Mutual-Nearest
Tokens Self-Attention Cross-Attention Coarse Features Matrix Neighbor (MNN)
Figure2.PipelineOverview.(1)Givenanimagepair,aCNNnetworkextractscoarsefeaturemapsF˜ andF˜ ,aswellasfinefeatures.(2)
A B
Then,wetransformcoarsefeaturesformorediscriminativefeaturemapsbyinterleavingouraggregatedself-andcross-attentionN times,
whereadaptivelyfeatureaggregationisperformedtoreducetokensizebeforeeachattentionforefficiency.(3)Transformedcoarsefeatures
arecorrelatedforthescorematrixS.Mutual-nearest-neighbor(MNN)searchingisfollowedtoestablishcoarsematches{M }.(4)To
c
refinecoarsematches,discriminativefinefeaturesFˆt ,Fˆt infullresolutionareobtainedbyfusingtransformedcoarsefeaturesF˜t ,F˜t
A B A B
withbackbonefeatures.FeaturepatchesarethencroppedcenteredateachcoarsematchM .Atwo-stagerefinementisfollowedtoobtain
c
sub-pixelcorrespondenceM .
f
tentionwillfurtherintroducelatencies. TopicFM[17]first 3.Method
assignsfeatureswithsimilarsemanticmeaningstothesame
topic,whereattentionisconductedwithineachtopicforef- GivenapairofimagesI A,I B,ourobjectiveistoestablisha
ficiency. Sinceitneedstosequentiallyprocesseachtoken’s setofreliablecorrespondencesbetweenthem. Weachieve
featuresfortransformation,theefficiencyimprovementis thisbyacoarse-to-finematchingpipeline,whichfirstestab-
limited. Moreover,performinglocalattentionwithintopics lishes coarse matches on downsampled feature maps and
canpotentiallyrestrictthecapabilityofmodelinglong-range then refines them for high accuracy. An overview of our
dependencies. Compared with them, the proposed aggre- pipelineisshowninFig.2.
gatedattentionmoduleinourmethodsignificantlyimproves
efficiencywhileachievingbetteraccuracy. 3.1.LocalFeatureExtraction
Dense matching methods [14, 15, 54] are designed to
Imagefeaturemapsarefirstextractedbyalightweightback-
estimateallpossiblecorrespondencesbetweentwoimages,
boneforlatertransformationandmatching. UnlikeLoFTR
whichshowstrongrobustness. However,theyaregenerally
andmanyotherdetector-freematchersthatuseaheavymulti-
muchslowercomparedwithsparseandsemi-densemethods.
branchResNet[19]networkforfeatureextraction,wealter-
Unlikethem,ourmethodproducessemi-densematcheswith
natetoalightweightsingle-branchnetworkwithreparam-
competitiveperformanceandconsiderablybetterefficiency.
eterization[11]toachievebetterinferenceefficiencywhile
Transformer hasbeenbroadlyusedinmultiplevisiontasks, preservingthemodelperformance.
includingfeaturematching.Theefficiencyandmemoryfoot- Inparticular,amulti-branchCNNnetworkwithresidual
printofhandlinglargetokensizesarethemainlimitations connectionsisappliedduringtrainingformaximumrepre-
of transformer [56], where some methods [21, 22, 58] at- sentationalpower. Atinferencetime,welosslesslyconvert
tempttoreducethecomplexitytoalinearscaletoalleviate thefeaturebackboneintoanefficientsingle-branchnetwork
theseproblems. Somemethods[9,24]proposeoptimizing byadoptingthereparameterizationtechnique[11],whichis
transformermodelsforspecifichardwarearchitecturesfor achievedbyfusingparallelconvolutionkernelsintoasingle
memoryandrunning-timeefficiency. Theyareorthogonal one. Then,theintermediate1/8down-sampledcoarsefea-
toourmethodandcanbenaturallyadaptedintothepipeline turesF˜ A,F˜
B
andfinefeaturesin1/4and1/2resolutionsare
forfurtherefficiencyimprovement. extractedefficientlyforlatercoarse-to-finematching.
3
noitagerggA elpmaS-pU noitagerggA elpmaS-pU AWAH2)8/1(>tixetal/<==w0SOgEPcIl2i/m0okkqeW/KfHKcrqczKMePqw+JTdNVYUUPwz0Iv9Cj50ol8xKfKrdCOI9WgC0p5cO/jVDlg7uGfDJtyuVKYWT8TWN9exak0kSsqvTUZpwDVsHg7EIhkcxU97V3ERXWWCxsqOiJLgmmGMrS+4oFianflvkud5+SwlcEnLeZ+ro24eKpPm3vxuF0F74qoZchFSeP1YIYrqUDpHHTJq2kyI89Oq1TUr2D4LksHkhRtYKNSqMtMMOLTGJuYCM1RMB0ozGPREQAz+JDEsghjIW85hjw91ZmvKJ5EQPD4UpJDar5IrLWxu03qWx7aXlv/6JU5ED2+td4hx1dAGK4789bPyC1Ye8WwTOmCb6umBkCW7hz5WdjtNHEbUms9THy3NW0iD/IGtv72SdllXamJG9FmfuZnJlm2rp464xDUz7zNjn3njnanW0JGVJM++C9PA/l91qw9h1jjBhN6U291AgLnmgJ4hRJGGuBWZkiulBgjZkmmedLI+FU3DFAJsTLHVjciH0CAAA>"=cCbn+G6dhQVaYYFCAtabcS8yXsA"=46esab_1ahs
tixetal<f<latexit sha1_base64="f4jXYSUiJn4KudYnZNr5TSSFBPI=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit> i
3.2.EfficientLocalFeatureTransformation Add
f<latexit sha1_base64="f4jXYSUiJn4KudYnZNr5TSSFBPI=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit> i
Afterthefeatureextraction, thecoarse-levelfeaturemaps Multi-layer Perceptron
F˜ andF˜ aretransformedbyinterleavingself-andcross- Add
A B
Concat
attention1 ntimestoimprovediscriminativeness. Thetrans-
Multi-layer Perceptron
formedfeaturesaredenotedasF˜t ,F˜t . Up-Sample
A B
Concat
Previousmethodsoftenperformattentionontheentire Cat & Linear
coarse-levelfeaturemaps,wherelinearattentioninsteadof
Cat & Linear
vanillaattentionisappliedtoensureamanageablecomputa- Vanilla Attention Layer h<latexit sha1_base64="YxgTbE4Tg2Dm9Lslt6jOsGRRbvU=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBzEmM5g==</latexit>
tioncost. However,theefficiencyisstilllimitedduetothe LLiinneeaarr AAtttteennttiioonn LLaayyeerr h<latexit sha1_base64="YxgTbE4Tg2Dm9Lslt6jOsGRRbvU=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBzEmM5g==</latexit>
largetokensizeofcoarsefeatures. Moreover,theusageof RoPE
linearattentionleadstosub-optimalmodelcapability. Un- Q<latexit sha1_base64="4XUGxwNURwmnEZdHAYd77F3yrjU=">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Koko9iIUvHhswX5AG8JmO2mXbjZxdyOU0j/hxYMiXv073vw3btsctPXBwOO9GWbmhang2rjut7O2vrG5tV3YKe7u7R8clo6OWzrJFMMmS0SiOiHVKLjEpuFGYCdVSONQYDsc3c389hMqzRP5YMYp+jEdSB5xRo2VOo3bdtCIAh6Uym7FnYOsEi8nZchRD0pfvX7CshilYYJq3fXc1PgTqgxnAqfFXqYxpWxEB9i1VNIYtT+Z3zsl51bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//O6mYmq/oTLNDMo2WJRlAliEjJ7nvS5QmbE2BLKFLe3EjakijJjIyraELzll1dJ67LiXVfcxlW5Vs3jKMApnMEFeHADNbiHOjSBgYBneIU359F5cd6dj0XrmpPPnMAfOJ8/adiPhw==</latexit> =WQfi K<latexit sha1_base64="NFLzNNQxUfcVflQnMcUwpbaP+HI=">AAAB73icbVBNSwMxEJ34WetX1aOXYBE8lV1R7EUoeBF6qWA/oF2WbJptY7PZNckKZemf8OJBEa/+HW/+G9N2D9r6YODx3gwz84JEcG0c5xutrK6tb2wWtorbO7t7+6WDw5aOU0VZk8YiVp2AaCa4ZE3DjWCdRDESBYK1g9HN1G8/MaV5LO/NOGFeRAaSh5wSY6VO/brt10P/wS+VnYozA14mbk7KkKPhl756/ZimEZOGCqJ113US42VEGU4FmxR7qWYJoSMyYF1LJYmY9rLZvRN8apU+DmNlSxo8U39PZCTSehwFtjMiZqgXvan4n9dNTVj1Mi6T1DBJ54vCVGAT4+nzuM8Vo0aMLSFUcXsrpkOiCDU2oqINwV18eZm0zivuZcW5uyjXqnkcBTiGEzgDF66gBrfQgCZQEPAMr/CGHtELekcf89YVlM8cwR+gzx9Y8I98</latexit> =WKfj V<latexit sha1_base64="8HhSfiQ1YEjRG3PALXyrVmoLveQ=">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Koko9iIUvHisYNNCG8Jmu2nXbnbj7kYooX/CiwdFvPp3vPlv3LY5aOuDgcd7M8zMi1LOtHHdb2dldW19Y7O0Vd7e2d3brxwc+lpmitAWkVyqToQ15UzQlmGG006qKE4iTtvR6Gbqt5+o0kyKezNOaZDggWAxI9hYqeNft0M/Dh/CStWtuTOgZeIVpAoFmmHlq9eXJEuoMIRjrbuem5ogx8owwumk3Ms0TTEZ4QHtWipwQnWQz+6doFOr9FEslS1h0Ez9PZHjROtxEtnOBJuhXvSm4n9eNzNxPciZSDNDBZkvijOOjETT51GfKUoMH1uCiWL2VkSGWGFibERlG4K3+PIy8c9r3mXNvbuoNupFHCU4hhM4Aw+uoAG30IQWEODwDK/w5jw6L8678zFvXXGKmSP4A+fzB3q2j5I=</latexit> =WVfj Q<latexit sha1_base64="AHTbE9Tvo2Xf6ZcFbLLCbwYwCdI=">AAAB8HicbVBNSwMxEJ31s9avqkcvwSJ6KrtS1ItQ9OKxBfsh7bJk02wbmmSXJCuUpb/CiwdFvPpzvPlvTNs9aOuDgcd7M8zMCxPOtHHdb2dldW19Y7OwVdze2d3bLx0ctnScKkKbJOax6oRYU84kbRpmOO0kimIRctoOR3dTv/1ElWaxfDDjhPoCDySLGMHGSo+Nm3bQiM4CFpTKbsWdAS0TLydlyFEPSl+9fkxSQaUhHGvd9dzE+BlWhhFOJ8VeqmmCyQgPaNdSiQXVfjY7eIJOrdJHUaxsSYNm6u+JDAutxyK0nQKboV70puJ/Xjc10bWfMZmkhkoyXxSlHJkYTb9HfaYoMXxsCSaK2VsRGWKFibEZFW0I3uLLy6R1UfEuK9VGtVy7zeMowDGcwDl4cAU1uIc6NIGAgGd4hTdHOS/Ou/Mxb11x8pkj+APn8wfQzY/H</latexit> =WQfi0 K<latexit sha1_base64="QRqCx3zXHkosJpCp/V1m6IDyAy0=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRbRU9mVol6Eohehlwr2Q9qlZNNsG5tklyQrlKW/wosHRbz6c7z5b0zbPWj1wcDjvRlm5gUxZ9q47peTW1peWV3Lrxc2Nre2d4q7e00dJYrQBol4pNoB1pQzSRuGGU7bsaJYBJy2gtH11G89UqVZJO/MOKa+wAPJQkawsdJ97bLVq4XHvYdeseSW3RnQX+JlpAQZ6r3iZ7cfkURQaQjHWnc8NzZ+ipVhhNNJoZtoGmMywgPasVRiQbWfzg6eoCOr9FEYKVvSoJn6cyLFQuuxCGynwGaoF72p+J/XSUx44adMxomhkswXhQlHJkLT71GfKUoMH1uCiWL2VkSGWGFibEYFG4K3+PJf0jwte2flym2lVL3K4sjDARzCCXhwDlW4gTo0gICAJ3iBV0c5z86b8z5vzTnZzD78gvPxDb/Zj7w=</latexit> =WKfj0 V<latexit sha1_base64="iaB5eWBby27nEoVDaaEa3ESRSQA=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRbRU9mVol6EohePFey20i4lm2bb2CS7JFmhLP0VXjwo4tWf481/Y9ruQasPBh7vzTAzL0w408Z1v5zC0vLK6lpxvbSxubW9U97d83WcKkKbJOaxaodYU84kbRpmOG0nimIRctoKR9dTv/VIlWaxvDPjhAYCDySLGMHGSvf+ZavnR8e9h1654lbdGdBf4uWkAjkavfJntx+TVFBpCMdadzw3MUGGlWGE00mpm2qaYDLCA9qxVGJBdZDNDp6gI6v0URQrW9KgmfpzIsNC67EIbafAZqgXvan4n9dJTXQRZEwmqaGSzBdFKUcmRtPvUZ8pSgwfW4KJYvZWRIZYYWJsRiUbgrf48l/in1a9s2rttlapX+VxFOEADuEEPDiHOtxAA5pAQMATvMCro5xn5815n7cWnHxmH37B+fgG4bWP0g==</latexit> =WVfj0
likethem,weproposeefficientaggregatedattentionforboth f<latexit sha1_base64="WzTXV7vOUiw0W3iVzQUROJep24k=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbRU0mkqMeiF48VTFtoQ9lsN+3S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5YcKZNq777ZTW1jc2t8rblZ3dvf2D6uFRW8epItQnMY9VN8Saciapb5jhtJsoikXIaSec3OV+54kqzWL5aKYJDQQeSRYxgk0uRecDNqjW3Lo7B1olXkFqUKA1qH71hzFJBZWGcKx1z3MTE2RYGUY4nVX6qaYJJhM8oj1LJRZUB9n81hk6s8oQRbGyJQ2aq78nMiy0norQdgpsxnrZy8X/vF5qopsgYzJJDZVksShKOTIxyh9HQ6YoMXxqCSaK2VsRGWOFibHxVGwI3vLLq6R9Wfeu6o2HRq15W8RRhhM4hQvw4BqacA8t8IHAGJ7hFd4c4bw4787HorXkFDPH8AfO5w+oyo4A</latexit> i0 f<latexit sha1_base64="QQK2x4oOzBzqq4m24yUyBWTTBsM=">AAAB63icbVBNSwMxEJ2tX7V+VT16CRbRU9mVUj0WvXisYD+gXUo2zbaxSXZJskJZ+he8eFDEq3/Im//GbLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW0eJIrRFIh6pboA15UzSlmGG026sKBYBp51gcpv5nSeqNIvkg5nG1Bd4JFnICDaZFJ4PHgflilt150CrxMtJBXI0B+Wv/jAiiaDSEI617nlubPwUK8MIp7NSP9E0xmSCR7RnqcSCaj+d3zpDZ1YZojBStqRBc/X3RIqF1lMR2E6BzVgve5n4n9dLTHjtp0zGiaGSLBaFCUcmQtnjaMgUJYZPLcFEMXsrImOsMDE2npINwVt+eZW0L6tevVq7r1UaN3kcRTiBU7gAD66gAXfQhBYQGMMzvMKbI5wX5935WLQWnHzmGP7A+fwBqk6OAQ==</latexit> j0
efficiencyandperformance. Full Convolution Max-Pooling Salient
Tokens
Tokens
Preliminaries. First,weprovideabriefoverviewofthe f<latexit sha1_base64="f4jXYSUiJn4KudYnZNr5TSSFBPI=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit> i f<latexit sha1_base64="XHptQbu2aN/FQZticJn/tr7CI8o=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaY1NkzCZKKUI/oBb/TTxD/QvvDNOQS2iE5KcOfeeM3Pv9ZMwSKXjvOasufmFxaX8cmFldW19o7i51UjjTDBeZ3EYi5bvpTwMIl6XgQx5KxHcG/khb/rDMxVv3nKRBnF0KccJ74y8QRT0A+ZJoi763ZtuseSUHb3sWeAaUIJZtbj4giv0EIMhwwgcESThEB5Setpw4SAhroMJcYJQoOMc9yiQNqMsThkesUP6DmjXNmxEe+WZajWjU0J6BSlt7JEmpjxBWJ1m63imnRX7m/dEe6q7jenvG68RsRLXxP6lm2b+V6dqkejjRNcQUE2JZlR1zLhkuivq5vaXqiQ5JMQp3KO4IMy0ctpnW2tSXbvqrafjbzpTsWrPTG6Gd3VLGrD7c5yzoHFQdo/KzvlhqXJqRp3HDnaxT/M8RgVV1FAn7wEe8YRnq2pFVmbdfaZaOaPZxrdlPXwAaqWQSA==</latexit> j
commonlyusedvanillaattentionandlinearattention.Vanilla
attentionisacoremechanismintransformerencoderlayer, f<latexit sha1_base64="f4jXYSUiJn4KudYnZNr5TSSFBPI=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit> i f<latexit sha1_base64="XHptQbu2aN/FQZticJn/tr7CI8o=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaY1NkzCZKKUI/oBb/TTxD/QvvDNOQS2iE5KcOfeeM3Pv9ZMwSKXjvOasufmFxaX8cmFldW19o7i51UjjTDBeZ3EYi5bvpTwMIl6XgQx5KxHcG/khb/rDMxVv3nKRBnF0KccJ74y8QRT0A+ZJoi763ZtuseSUHb3sWeAaUIJZtbj4giv0EIMhwwgcESThEB5Setpw4SAhroMJcYJQoOMc9yiQNqMsThkesUP6DmjXNmxEe+WZajWjU0J6BSlt7JEmpjxBWJ1m63imnRX7m/dEe6q7jenvG68RsRLXxP6lm2b+V6dqkejjRNcQUE2JZlR1zLhkuivq5vaXqiQ5JMQp3KO4IMy0ctpnW2tSXbvqrafjbzpTsWrPTG6Gd3VLGrD7c5yzoHFQdo/KzvlhqXJqRp3HDnaxT/M8RgVV1FAn7wEe8YRnq2pFVmbdfaZaOaPZxrdlPXwAaqWQSA==</latexit> j
relyingonthreeinputs: queryQ,keyK,andvalueV.The (a) LoFTR’s Attention Module (b) Our Aggregated Attention Module
resultantoutputisaweightedsumofthevalue,wherethe
Figure3. DetailedTransformerModuleComparison. Unlike
weighted matrix is determined by the query and its corre- LoFTRwhichusesalltokensoffeaturemapstocomputeattention
spondingkey. Formally,theattentionfunctionisdefinedas and resort to linear attention to reduce the computational cost,
follows: theproposedattentionmodulefirstaggregatesfeaturesforsalient
tokens,whichissignificantlymoreefficientforattention. Then
VanillaAttention(Q,K,V)=softmax(QKT)V . (1) thevanillaattentionisutilizedtotransformaggregatedfeatures,
whererelativepositionalencodingisinsertedtocapturethespatial
However, applying the vanilla attention directly to dense
information.Transformedfeaturesareupsampledandfusedwith
localfeaturesisimpracticalduetothesignificanttokensize.
theoriginalfeaturestoformthefinalfeatures.
Toaddressthisissue,previousmethodsuselinearattention
toreducethecomputationalcomplexityfromquadraticto
whereConv2Disimplementedbyastrideddepthwisecon-
linear:
volutionwithakernelsizeofs s,identicaltothatofthe
×
max-pooling layer. Then positional encoding and vanilla
LinearAttention(Q,K,V)=ϕ(Q)(ϕ(K)Tϕ(V)) . (2) attentionarefollowedtoprocessreducedtokens. Positional
encoding (PE) can help to model the spatial location con-
where ϕ() = elu()+1. However, it comes at the cost
· · texts, where RoPE [47] is adopted in practice to account
ofreducedrepresentationalpower,whichisalsoobserved
for more robust relative positions, inspired by [28]. Note
by[5].
that the PE layer is enabled exclusively for self-attention
andskippedduringcross-attention. Thetransformedfeature
AggragatedAttentionModule. Aftercomprehensively
mapisthenupsampledandfusedwithf forthefinalfeature
i
investigating the mechanism of the transformer on coarse
map. Duetotheaggregationandselection,thenumberof
feature maps, we have two observations that motivate us tokensinf andf isreducedbys2,whichcontributesto
i′ j′
to devise a new efficient aggregated attention. First, the
theefficiencyoftheattentionphase.
attention regions of neighboring query tokens are similar,
thuswecanaggregatetheneighboringtokensoff toprevent 3.3.Coarse-levelMatchingModule
i
theredundantcomputation. Second,mostoftheattention
Weestablishcoarse-levelmatchesbasedonthepreviously
weights of each query token are concentrated on a small transformedcoarsefeaturemapsF˜t ,F˜t
. Coarsecorrespon-
numberofkeytokens,hencewecanselectthesalienttokens A B
dencesindicateroughmatchregionsforlatersubpixel-level
off beforeattentiontoreducethecomputation.
Tj
herefore, we propose to first aggregate the f i token matchingintherefinementphase.
Toachievethis,F˜t
A and
utilizingadepth-wiseconvolutionnetwork,andf isaggre-
F˜t
are densely correlated to build a score matrix . The
j B S
gatedbyamaxpoolinglayertogetreducedsalienttokens: softmaxoperatoronboth dimensions(referredtoasdual-
S
softmax)isthenappliedtoobtaintheprobabilityofmutual
f =Conv2D(f ), f =MaxPool(f ) , (3)
i′ i j′ j nearestmatching,whichiscommonlyusedin [37,48,55].
1Wefeedfeatureofoneimageasqueryandfeatureoftheotherimage Thecoarsecorrespondences c areestablishedbyselect-
{M }
askeyandvalueintocross-attention,similartoSG[41]andLoFTR[48]. ingmatchesabovethescorethresholdτ whilesatisfyingthe
4mutual-nearest-neighbor(MNN)constraint. entirecorrespondingfeaturepatchforitsfinematch. How-
ever,thisrefinement-by-expectationswillintroducelocation
variancetothefinalmatch,becauseirrelevantregionsalso
Efficient Inference Strategy. We observe that the dual-
haveweightsandcanaffectresults. Therefore,weproposea
softmax operator in the coarse matching can significantly
noveltwo-stagecorrelationmoduletosolvethisproblem.
restrict the efficiency in inference due to the large token
Ourideaistoutilizeamutual-nearest-neighbor(MNN)
size,especiallyforhigh-resolutionimages. Moreover,we
matchingtogetintermediatepixel-levelrefinedmatchesin
find that the dual-softmax operator is crucial for training,
the first stage, and then refine them for subpixel accuracy
droppingitatinferencetimewhiledirectlyusingthescore
bycorrelationandexpectation. MotivationsarethatMNN
matrix forMNNmatchingcanalsoworkwellwithbetter
S matchingdon’thavespatialvariancesincematchesarese-
efficiency.
lectedbydirectlyindexingpixelswithmaximumscores,but
Thereasonforusingthedual-softmaxoperatorintraining
cannotachievesub-pixelaccuracy. Conversely,refinement-
isthatitcanhelptotraindiscriminativefeatures. Intuitively,
by-expectationcanachievesub-pixelaccuracybutvariance
withthesoftmaxoperation,thematchingscorebetweentwo
exists. Theproposedtwo-stagerefinementcandrawbenefits
pixelscanalsoconditionedonotherpixels. Thismechanism
bycombiningthebestofbothworlds.
forcesthenetworktoimprovefeaturesimilarityoftruecor-
Specifically,torefineacoarse-levelcorrespondence ,
respondences while suppressing similarity with irrelevant Mc
thefirst-stagerefinementphasedenselycorrelatestheirfine
points. With trained discriminative features, the softmax
feature patches to obtain the local patch score matrix .
operationcanbepotentiallyeliminatedduringinference. Sl
MNN searching is then applied on to get intermediate
l
Wedenotethemodelskippingdual-softmaxlayerinin- S
pixel-level fine matches. To limit the overall match num-
ference as efficiency optimized model. Results in Tab. 1
ber,weselectthetop-1finematchforonecoarsematchby
demonstratetheeffectivenessofthisdesign.
sortingthecorrelationscores.
Then,wefurtherrefinethesepixel-levelmatchesforsub-
3.4.Subpixel-LevelRefinementModule
pixel accuracy by our second-stage refinement. Since the
AsoverviewedinFig.2(4),withestablishedcoarsematches matching accuracy has already significantly improved in
,werefinethemforsub-pixelaccuracywithourre- first-stagerefinement,nowwecanuseatinylocalwindow
c
{M }
finementmodule. Itiscomposedofanefficientfeaturepatch forcorrelationandexpectationwithamaximumsuppression
extractorfordiscriminativefinefeatures,followedbyatwo- oflocationvariance. Inpractice,wecorrelatethefeatureof
stagefeaturecorrelationlayerforfinalmatches . eachpointinI witha3 3featurepatchcenteredatits
f A
{M } ×
finematchinI . Thesoftmaxoperatoristhenappliedto
B
getamatchdistributionmatrixandthefinalrefinedmatchis
EfficientFineFeatureExtraction. Wefirstextractdis-
obtainedbycalculatingexpectations.
criminative fine feature patches centered at each coarse
match c by an efficient fusion network for later match 3.5.Supervision
M
refinement. Forefficiency,ourkeyideahereistore-leverage
the previously transformed coarse features
F˜t
,
F˜t
to ob-
Theentirepipelineistrainedend-to-endbysupervisingthe
A B coarseandrefinementmatchingmodulesseparately.
taincross-viewattendeddiscriminativefinefeatures,instead
ofintroducingadditionalfeaturetransformnetworksasin
Coarse-LevelMatchingSupervision. Thecoarseground
LoFTR[48].
Tobespecific,F˜t
A
andF˜t
B
areadaptivelyfusedwith1/4 t br yut wh am rpa it nc ghe gs ri{ dM -levc } elgt pow init th sa froto mta Il nu tom Iber vo iaf dN epa tr he mbu apil st
and1/2resolutionbackbonefeaturesbyconvolutionandup- A B
andimageposesfollowingpreviousmethods[41,48]. The
samplingtoobtainfinefeaturemapsFˆt ,Fˆt
intheoriginal
A B producedcorrelationscorematrix incoarsematchingis
imageresolution. Thenlocalfeaturepatchesarecroppedon S
supervisedbyminimizingthelog-likelihoodlossoverloca-
finefeaturemapscenteredateachcoarsematch. Sinceonly
tionsof :
c gt
shallowfeed-forwardnetworksareincluded,ourfinefeature {M }
fusionnetworkisremarkablyefficient. = 1 (cid:80) log (cid:0)˜i,˜j(cid:1) . (4)
Lc −N (˜i,˜j) ∈{Mc}gt S
Two-Stage Correlation for Refinement. Based on the Fine-LevelMatchingSupervision. Wetraintheproposed
extractedfinelocalfeaturepatchesofcoarsematches, we two-stagefine-levelmatchingmodulebyseparatelysuper-
search for high-accurate sub-pixel matches. To refine a vising the two phases. The first stage fine loss is to
f1
L
coarse match, a commonly used strategy [7, 17, 48] is to minimize the log-likelihood loss of each fine local score
select the center-patch feature of I as a fixed reference matrix basedonthepixel-levelgroundtruthfinematches,
A l
S
point,andperformfeaturecorrelationandexpectationonthe similartocoarseloss. Thesecondstageistrainedby
f2
L
5MegaDepthDataset ScanNetDataset
Category Method Time(ms)
AUC@5◦ AUC@10◦ AUC@20◦ AUC@5◦ AUC@10◦ AUC@20◦
SP+NN 31.7 46.8 60.1 7.5 18.6 32.1 10.8
Sparse SP+SG 49.7 67.1 80.6 17.6 34.9 52.0 42.1
SP+LG 49.9 67.0 80.1 14.8 30.8 47.5 31.9/30.7
DRC-Net 27.0 42.9 58.3 7.7 17.9 30.5 328.0
LoFTR 52.8 69.2 81.2 16.9 33.6 50.6 66.2
QuadTree 54.6 70.5 82.2 19.0 37.3 53.5 100.7
Semi-Dense MatchFormer 53.3 69.7 81.8 15.8 32.0 48.0 128.9
TopicFM 54.1 70.1 81.6 17.3 35.5 50.9 66.4
AspanFormer 55.3 71.5 83.1 19.6 37.7 54.4 81.6
Ours 56.4 72.2 83.5 19.2 37.0 53.6 40.1/34.4
Ours(Optimized) 55.4 71.4 82.9 17.4 34.4 51.2 35.6/27.0
DKM 60.4 74.9 85.1 26.64 47.07 64.17 210.8
Dense
ROMA 62.6 76.7 86.3 28.9 50.4 68.3 302.7
Table1.ResultsofRelativePoseEstimationonMegaDepthDatasetandScanNetDataset.WeusethemodelstrainedontheMegaDepth
datasettoevaluateallmethodsonbothdatasets,whichcanshowtheintra-andinter-datasetgeneralizationabilities.TheAUCofposeerror
atdifferentthresholds,alongwiththeprocessingtimeformatchingimagepairataresolutionof640×480,ispresented. ForSP+LG,
Ours,andOurs(Optimized),therunningtimesofthemodelusingFP32/Mixed-Precisionnumericalprecisionsareshown.
thatcalculatestheℓ lossbetweenthefinalsubpixelmatches changes, aswellasrepetitivepatterns. Wefollowthetest
2
andgroundtruthfinematches . split of the previous method [48] that uses 1500 sampled
f f gt
{M } {M }
The total loss is the weighted sum of all supervisions: pairsfromscenes“SacreCoeur”and“St. Peter’sSquare”
= +α +β . forevaluation. Imagesareresizedsothatthelongestedge
c f1 f2
L L L L
equals1200forallsemi-denseanddensemethods. Follow-
4.Experiments
ing[28],sparsemethodsareprovidedresizedimageswith
longestedgeequals1600.
Inthissection,weevaluatetheperformanceofourmethod
on several downstream tasks, including homography esti- ScanNetdatasetcontains1613sequenceswithground-
mation, pairwise pose estimation and visual localization. truth depth maps and camera poses. They depict indoor
Furthermore,weevaluatetheeffectivenessofourdesignby sceneswithviewpointchangesandtexture-lessregions. We
conductingdetailedablationstudies. usethesampledtestpairsfrom[41]fortheevaluation,where
imagesareresizedto640 480forallmethods.
4.1.ImplementationDetails ×
Baselines. We compare the proposed method with three
WeadoptRepVGG[11]asourfeaturebackbone,andself-
categories of methods: 1) sparse keypoint detection and
andcross-attentionareinterleavedforN =4timestotrans-
matchingmethods,includingSuperPoint[10]withNearest-
formcoarsefeatures. Foreachattention,weaggregatefea-
Neighbor(NN),SuperGlue(SG)[41],LightGlue(LG)[28]
turesbyadepth-wiseconvolutionlayerandamax-pooling
matchers,2)semi-densematchers,includingDRC-Net[25],
layer,bothwithakernelsizeof4 4. Ourmodelistrained
LoFTR[48],QuadTreeAttention[50],MatchFormer[57],
×
ontheMegaDepthdataset[26],whichisalarge-scaleout-
AspanFormer [7], TopicFM [17], and 3) state-of-the-art
door dataset. The test scenes are separated from training
dense matcher ROMA [15] that predict matches for each
data following [48]. The loss function’s weights α and β
pixel.
are set to 1.0 and 0.25, respectively. We use the AdamW
optimizer with an initial learning rate of 4 10 3. The Evaluation protocol. Following previous methods, the
−
networktrainingtakesabout15hourswithaba× tchsizeof16 recovered relative poses by matches are evaluated for re-
on8NVIDIAV100GPUs. Andthecoarseandfinestages flectingmatchingaccuracy. Theposeerrorisdefinedasthe
are trained together from scratch. The trained model on maximumofangularerrorsinrotationandtranslation. We
MegaDepthisusedtoevaluatealldatasetsandtasksinour reporttheAUCoftheposeerroratthresholds(5 ◦,10 ◦,and
experimentstodemonstratethegeneralizationability. 20 ◦). Moreover,therunningtimeofmatchingeachimage
pairintheScanNetdatasetisreportedforcomprehensively
4.2.RelativePoseEstimation
understandingthematchingaccuracyandefficiencybalance.
WeuseasingleNVIDIA3090toevaluatetherunningtime
Datasets. WeusetheoutdoorMegaDepth[26]datasetand
ofallmethods.
indoorScanNet[8]datasetfortheevaluationofrelativepose
estimationtodemonstratetheefficacyofourmethod. Results. AsshowninTab.1,theproposedmethodachieves
MegaDepth dataset is a large-scale dataset containing competitiveperformancescomparedwithsparseandsemi-
sparse3Dreconstructionsfrom196scenes. Thekeychal- densemethodsonbothdatasets. Qualitativecomparisons
lengesonthisdatasetarelargeviewpointsandillumination areshowninFig.4. Specifically,ourmethodoutperforms
6Homographyest.AUC DUC1 DUC2
Category Method Method
@3px @5px @10px (0.25m,2◦)/(0.5m,5◦)/(1.0m,10◦)
D2Net+NN 23.2 35.9 53.6 SP+SG 49.0/68.7/80.8 53.4/77.1/82.4
R2D2+NN 50.6 63.9 76.8 LoFTR 47.5/72.2/84.8 54.2/74.8/85.5
Sparse
DISK+NN 52.3 64.9 78.9 TopicFM 52.0/74.7/87.4 53.4/74.8/83.2
SP+SG 53.9 68.3 81.7 PATS 55.6/71.2/81.0 58.8/80.9/85.5
Sparse-NCNet 48.9 54.2 67.1 AspanFormer 51.5/73.7/86.0 55.0/74.0/81.7
DRC-Net 50.6 56.2 68.3 Ours 52.0/74.7/86.9 58.0/80.9/89.3
Semi-Dense LoFTR 65.9 75.6 84.6
Ours 66.5 76.4 85.5
Table3.ResultsofVisualLocalizationonInLocDataset.
Table 2. Results of Homography Estimation on HPatches
Day Night
Method
Dataset. Our method is compared with sparse and semi-dense
(0.25m,2◦)/(0.5m,5◦)/(1.0m,10◦)
methods.TheAUCofreprojectionerrorofcornerpointsatdiffer-
SP+SG 89.8/96.1/99.4 77.0/90.6/100.0
entthresholdsisreported. LoFTR 88.7/95.6/99.0 78.5/90.6/99.0
TopicFM 90.2/95.9/98.9 77.5/91.1/99.5
PATS 89.6/95.8/99.3 73.8/92.1/99.5
the best semi-dense baseline AspanFormer on all metrics AspanFormer 89.4/95.6/99.0 77.5/91.6/99.5
of the MegaDepth dataset and has lower but comparable Ours 89.6/96.2/99.0 77.0/91.1/99.5
performanceontheScanNetdataset,with 2timesfaster.
∼ Table4.ResultsofVisualLocalizationonAachenv1.1Dataset.
Ouroptimizedmodelthateliminatesthedual-softmaxop-
eratorincoarse-levelmatchingfurtherbringsefficiencyim-
Results. AsshowninTab.2,eventhoughthenumberof
provements,withslightperformancedecreases. Usingthis
matchesisrestricted,ourmethodcanalsoworkremarkably
strategy, our method can outperform the efficient and ro-
well and outperform sparse methods significantly. Com-
bustsparsemethodSP+LGinefficiencywithsignificantly
paredwithsemi-dense,ourmethodcansurpassthemwith
higheraccuracy. DensematcherROMAshowsremarkable
significantlyhigherefficiency. Weattributethistotheeffec-
matchingcapabilitybutisslowforapplicationsinpractice.
tivenessoftwo-stagerefinementforaccuracyimprovement
Moreover,sinceROMAutilizesthepre-trainedDINOv2[35]
andproposedaggregationmoduleforefficiency.
backbone,itsstronggeneralizabilityonScanNetmaybeat-
tributedtothesimilarindoortrainingdatainDINOv2,where 4.4.VisualLocalization
othermethodsaretrainedonoutdoorMegaDepthonly.Com-
paredwithit,ourmethodis 7.5timesfaster,whichhasa DatasetsandEvaluationProtocols. Visuallocalization
∼
goodbalancebetweenaccuracyandefficiency. isanimportantdownstreamtaskofimagematching,which
aims to estimate the 6-DoF poses of query images based
4.3.HomographyEstimation
on the 3D scene model. We conduct experiments on two
commonlyusedbenchmarks,includingInLoc[49]dataset
Dataset. WeevaluateourmethodonHPatchesdataset[3].
andAachenv1.1[43]dataset,forevaluationtodemonstrate
HPatches dataset depicts planar scenes divided into se-
thesuperiorityofourmethod. InLocdatasetiscapturedon
quences. Images are taken under different viewpoints or
indoorsceneswithplentyofrepetitivestructuresandtexture-
illuminationchanges.
lessregions,whereeachdatabaseimagehasacorresponding
Baselines. Wecompareourmethodwithsparsemethods depthmap. Aachenv1.1isachallenginglarge-scaleoutdoor
includingD2Net[12],R2D2[36],DISK[55]detectorswith datasetforlocalizationwithlarge-viewpointandday-and-
NNmatcher,andSuperPoint[10]+SuperGlue[41]. Asfor nightilluminationchanges,whichparticularlyreliesonthe
semi-densemethods,wecomparewithSparse-NCNet[38], robustnessofmatchingmethods. Weadoptitsfulllocaliza-
DRC-Net[25],andLoFTR[48].ForSuperGlueandallsemi- tiontrackforbenchmarking.
densemethods,weusetheirmodelstrainedonMegaDepth Following[7,48],theopen-sourcedlocalizationframe-
datasetforevaluation. workHLoc[40]isutilized. Forbothdatasets,thepercentage
ofposeerrorssatisfyingbothangularanddistancethresholds
Evaluation Protocol. Following SuperGlue [41] and
isreportedfollowingthebenchmarks,wheredifferentthresh-
LoFTR[48],weresizeallimagesformatchingsothattheir
oldsareused. FortheInLocdataset,themetricsoftwotest
smallestedgeequals480pixels. Wecollectthemeanrepro-
scenesincludingDUC1andDUC2areseparatelyreported.
jectionerrorofcornerpoints,andreporttheareaunderthe
AsfortheAachenv1.1dataset,themetricscorrespondingto
cumulativecurve(AUC)under3differentthresholds,includ-
thedaytimeandnighttimedivisionsarereported.
ing3px,5px,and10px. Forallbaselines,weemploythe
sameRANSACmethodasarobusthomographyestimator Baselines. We compare the proposed method with both
forafaircomparison. FollowingLoFTR,weselectonlythe detector-basedmethodSuperPoint[10]+SuperGlue[41]and
top1000predictedmatchesofsemi-densemethodsforthe detector-freemethodsincludingLoFTR[48],TopicFM[17],
sakeoffairness. PATS[34]andAspanformer[7].
7SuperPoint + LightGlue AspanFormer Ours
Figure4.QualitativeResults.OurmethodiscomparedwiththesparsematchingpipelineSuperPoint[10]+LightGlue[28],semi-dense
matcherAspanFormer[7].Imagepairswithtexture-poorregionsandlarge-viewpointchangescanberobustlymatchedbyourmethod.The
redcolorindicatesepipolarerrorbeyond5×10−4(inthenormalizedimagecoordinates).
PoseEstimationAUC 3)ComparedwithusingLoFTR’srefinementthatperforms
Method Time(ms)
@5◦ @10◦ @20◦ expectation on the entire correlation patch, the proposed
OursFull 56.4 72.2 83.5 139.2 two-stagerefinementlayercanbringaccuracyimprovement
1)OursOptimal(w/odual-softmax) 55.4 71.4 82.9 102.0
withneglectablelatency. Weattributethistothetwo-stage
2)ReplaceAgg.AttentiontoLoFTR’sTrans. 54.7 70.5 82.2 171.4
3)Replacetwo-stagerefine.toLoFTR’srefine. 54.7 70.9 82.7 135.3 refinement’spropertythatcanmaximizethesuppressionof
4)Nosecond-stagerefinement 55.8 71.8 83.3 138.1 locationvarianceincorrelationrefinement. 4)Droppingthe
5)ReplaceRepVGGwithResNet 55.4 71.4 82.9 156.2
second refinement stage will lead to degraded pose accu-
Table5. AblationStudies. Thecomponentsofourmethodare racywithminorefficiencychanges,especiallyonthestrict
ablatedontheMegaDepthdatasetforacomprehensiveunderstand- AUC@5 metric. 5)Changingthebackbonefromreparam-
◦
ingofourmethod,whereaveragedrunningtimesforanimagepair eterizedVGG[11]backtomulti-branchResNet[19]leads
withhigh-resolution1200×1200arereported.
todecreasedefficiencywithsimilaraccuracy,whichdemon-
stratestheeffectivenessofourdesignchoiceforefficiency.
Results. Weadheretothepipelineandevaluationsettings
oftheonlinevisuallocalizationbenchmark2 toensurefair- 5.Conclusions
ness. AspresentedinTab.3,theproposedmethodachieves
competitiveresults,takingbothdetector-basedanddetector- Thispaperintroducesanewsemi-denselocalfeaturematcher
freemethodsintoaccount. Beingamethodprimarilygeared basedonthesuccessofLoFTR.Werevisititsdesignsand
towardsefficiency,ourapproachcandeliverresultscompara- proposeseveralimprovementsforbothefficiencyandmatch-
bletothoseofmanyaccuracy-orientedmethods.Asdepicted ingaccuracy.AkeyobservationisthatperformingtheTrans-
inTab.4,ourmethodalsodemonstratesperformanceonpar former on the entire coarse feature map is redundant due
withthebest-performingapproaches. tothesimilarlocalinformation,whereanaggregatedatten-
tionmoduleisproposedtoperformtransformeronreduced
4.5.AblationStudies
tokenswithsignificantlybetterefficiencyandcompetitive
In this part, we conduct detailed ablation studies to ana- performance. Moreover, a two-stage correlation layer is
lyzetheeffectivenessofourproposedmoduleswithresults devisedtosolvethelocationvarianceprobleminLoFTR’s
showninTab.5.1)Withoutdual-softmax,ouroptimalmodel refinementdesign,whichfurtherbringsaccuracyimprove-
canbringhugeefficiencyimprovementinhigh-resolution ments. As a result, our method can achieve 2.5 times
∼
images. 2)Incoarsefeaturetransformation,replacingthe fastercomparedwithLoFTRwithbettermatchingaccuracy.
proposedaggregatedattentionmodulewithLoFTR’strans- Moreover,asasemi-densematchingmethod,theproposed
former can bring significant efficiency dropping, as well methodcanachievecomparableefficiencywiththerecent
as accuracy decrease. Note that the replaced transformer robustsparsefeaturematcherLightGlue[28]. Webelieve
isalsoequippedwithRoPEsameasoursforfaircompari- thisopensuptheapplicationsofourmethodinlarge-scaleor
son. Thisdemonstratestheefficacyoftheproposedmodule latency-sensitivedownstreamtasks,suchasimageretrieval
thatperformingvanillaattentiononaggregatedfeaturescan and 3D reconstruction. Please refer to the supplementary
achievehigherefficiencywithevenbettermatchingaccuracy. materialfordiscussionsaboutlimitationsandfutureworks.
2https://www.visuallocalization.net/benchmark
8
teNnacS
htpeDageMReferences [16] HaoqiFan, BoXiong, KarttikeyaMangalam, YanghaoLi,
ZhichengYan,JitendraMalik,andChristophFeichtenhofer.
[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian
Multiscale vision transformers. ICCV, pages 6804–6815,
Simon,BrianCurless,StevenM.Seitz,andRichardSzeliski.
2021. 11
Buildingromeinaday. ICCV,2009. 1
[17] KhangTruongGiang,SoohwanSong,andSung-GukJo.Top-
[2] ReljaArandjelovic´,PetrGronát,AkihikoTorii,TomásPa-
icfm:Robustandinterpretabletopic-assistedfeaturematch-
jdla,andJosefSivic. Netvlad: Cnnarchitectureforweakly
ing. InAAAI,2023. 2,3,5,6,7,11
supervisedplacerecognition. CVPR,pages5297–5307,2016.
[18] StephenHausler,SouravGarg,MingXu,MichaelMilford,
13
and Tobias Fischer. Patch-netvlad: Multi-scale fusion of
[3] VassileiosBalntas,KarelLenc,AndreaVedaldi,andKrystian
locally-globaldescriptorsforplacerecognition. CVPR,pages
Mikolajczyk. Hpatches: A benchmark and evaluation of
14136–14147,2021. 2
handcraftedandlearnedlocaldescriptors. InCVPR,pages
[19] KaimingHe,X.Zhang,ShaoqingRen,andJianSun. Deep
5173–5182,2017. 7
residuallearningforimagerecognition. CVPR,pages770–
[4] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc
778,2015. 3,8
Van Gool. Speeded-up robust features (surf). CVIU, 110
[20] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing
(3):346–359,2008. 2
Huang,HujunBao,andXiaoweiZhou. Detector-freestruc-
[5] HanCai,ChuangGan,andSongHan. Efficientvit:Enhanced
turefrommotion. InCVPR,2024. 1
linearattentionforhigh-resolutionlow-computationvisual
[21] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,and
recognition. arXivpreprintarXiv:2205.14756,2022. 4
FranccoisFleuret. Transformersarernns:Fastautoregressive
[6] HongkaiChen,ZixinLuo,JiahuiZhang,LeiZhou,Xuyang
transformerswithlinearattention. InICML,2020. 3
Bai, ZeyuHu, Chiew-LanTai, andLongQuan. Learning
[22] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
tomatchfeatureswithseededgraphmatchingnetwork. In
former: Theefficienttransformer. ArXiv,abs/2001.04451,
ICCV,pages6301–6310,2021. 2
2020. 3
[7] HongkaiChen,ZixinLuo,LeiZhou,YurunTian,Mingmin
[23] AxelBarrosoLaguna,EdgarRiba,DanielPonsa,andKrys-
Zhen,TianFang,DavidMckinnon,YanghaiTsin,andLong
tianMikolajczyk.Key.net:Keypointdetectionbyhandcrafted
Quan. Aspanformer: Detector-free image matching with
andlearnedcnnfilters. ICCV,pages5835–5843,2019. 2
adaptivespantransformer. InECCV,pages20–36.Springer,
2022. 2,5,6,7,8,11,12 [24] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,
[8] AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,
Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,
ThomasFunkhouser,andMatthiasNießner. Scannet:Richly-
and Daniel Haziza. xformers: A modular and hackable
annotated 3d reconstructions of indoor scenes. In CVPR,
transformermodellinglibrary. https://github.com/
pages5828–5839,2017. 6
facebookresearch/xformers,2022. 3
[9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and
[25] XinghuiLi,KaiHan,ShudaLi,andVictorPrisacariu. Dual-
ChristopherRé. FlashAttention:Fastandmemory-efficient
resolutioncorrespondencenetworks. InNeurIPS,2020. 2,6,
exactattentionwithIO-awareness. InNeurIPS,2022. 3
7
[10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint:Self-supervisedinterestpointdetection [26] ZhengqiLiandNoahSnavely. Megadepth:Learningsingle-
anddescription. CVPRW,2018. 2,6,7,8,12 viewdepthpredictionfrominternetphotos. InCVPR,pages
2041–2050,2018. 6
[11] XiaohanDing,XiangyuZhang,NingningMa,JungongHan,
GuiguangDing,andJianSun. Repvgg: Makingvgg-style [27] PhilippLindenberger,Paul-EdouardSarlin,ViktorLarsson,
convnetsgreatagain. InCVPR,pages13733–13742,2021. 3, andMarcPollefeys.Pixel-perfectstructure-from-motionwith
6,8,11 featuremetricrefinement. ICCV,pages5967–5977,2021. 1
[12] MihaiDusmanu,IgnacioRocco,TomasPajdla,MarcPolle- [28] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle-
feys,JosefSivic,AkihikoTorii,andTorstenSattler. D2-net: feys. LightGlue:LocalFeatureMatchingatLightSpeed. In
Atrainablecnnforjointdescriptionanddetectionoflocal ICCV,2023. 1,2,4,6,8,12,13
features. InCVPR,pages8092–8101,2019. 2,7 [29] GLoweDavid.Distinctiveimagefeaturesfromscale-invariant
[13] PatrickEbel,AnastasiiaMishchuk,KwangMooYi,Pascal keypoints. IJCV,2004. 1,2
Fua,andEduardTrulls. Beyondcartesianrepresentationsfor [30] Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui
localdescriptors. InICCV,pages253–262,2019. 2 Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan.
[14] JohanEdstedt,IoannisAthanasiadis,MårtenWadenbäck,and Aslfeat: Learning local features of accurate shape and lo-
MichaelFelsberg. Dkm:Densekernelizedfeaturematching calization. InCVPR,pages6589–6598,2020. 2
forgeometryestimation.InCVPR,pages17765–17775,2023. [31] AnastasiiaMishchuk,DmytroMishkin,FilipRadenovic,and
2,3 JiriMatas. Workinghardtoknowyourneighbor’smargins:
[15] JohanEdstedt,QiyuSun,GeorgBökman,MårtenWadenbäck, Localdescriptorlearningloss. NeurIPS,30,2017. 2
andMichaelFelsberg. Roma: Revisitingrobustlossesfor [32] RaulMur-ArtalandJuanD.Tardós. Orb-slam2: Anopen-
densefeaturematching. arXivpreprintarXiv:2305.15404, sourceslamsystemformonocular,stereo,andrgb-dcameras.
2023. 2,3,6 TR,33:1255–1262,2016. 1
9[33] RaulMur-Artal, JoséM.M.Montiel, andJuanD.Tardós. [47] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng
Orb-slam:Aversatileandaccuratemonocularslamsystem. Liu. Roformer: Enhancedtransformerwithrotaryposition
TR,31:1147–1163,2015. 1 embedding. ArXiv,abs/2104.09864,2021. 4,11
[34] JunjieNi,YijinLi,ZhaoyangHuang,HongshengLi,Hujun [48] JiamingSun,ZehongShen,YuangWang,HujunBao,and
Bao,ZhaopengCui,andGuofengZhang. Pats: Patcharea XiaoweiZhou. Loftr: Detector-freelocalfeaturematching
transportation with subdivision for local feature matching. withtransformers. InCVPR,pages8922–8931,2021. 1,2,4,
CVPR,pages17776–17786,2023. 2,7,11 5,6,7,11,13
[35] MaximeOquab,TimothéeDarcet,TheoMoutakanni,HuyV. [49] HajimeTaira,MasatoshiOkutomi,TorstenSattler,Mircea
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Cimpoi,MarcPollefeys,JosefSivic,TomasPajdla,andAk-
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,Russell ihiko Torii. Inloc: Indoor visual localization with dense
Howes,Po-YaoHuang,HuXu,VasuSharma,Shang-WenLi, matchingandviewsynthesis. InCVPR,pages7199–7209,
WojciechGaluba,MikeRabbat,MidoAssran,NicolasBallas, 2018. 7
GabrielSynnaeve,IshanMisra,HerveJegou,JulienMairal, [50] ShitaoTang,JiahuiZhang,SiyuZhu,andPingTan.Quadtree
PatrickLabatut,ArmandJoulin,andPiotrBojanowski. Di- attentionforvisiontransformers. ICLR,2022. 2,6,11
nov2: Learningrobustvisualfeatureswithoutsupervision, [51] YurunTian,BinFan,andFuchaoWu. L2-net:Deeplearning
2023. 7 of discriminative patch descriptor in euclidean space. In
[36] JérômeRevaud,CésarRobertodeSouza,M.Humenberger, CVPR,pages661–669,2017. 1,2
andPhilippeWeinzaepfel. R2d2: Reliableandrepeatable [52] YurunTian, XinYu, BinFan, FuchaoWu, HuubHeijnen,
detectoranddescriptor. InNeurIPS,2019. 2,7 andVassileiosBalntas. Sosnet:Secondordersimilarityreg-
ularization for local descriptor learning. In CVPR, pages
[37] IgnacioRocco,MirceaCimpoi,ReljaArandjelovic´,Akihiko
11016–11025,2019. 2
Torii,TomasPajdla,andJosefSivic. Neighbourhoodconsen-
susnetworks. NeurIPS,31,2018. 2,4 [53] Yurun Tian, Vassileios Balntas, Tony Ng, Axel Barroso-
Laguna,YiannisDemiris,andKrystianMikolajczyk. D2d:
[38] IgnacioRocco,ReljaArandjelovic´,andJosefSivic. Efficient
Keypoint extraction with describe to detect approach. In
neighbourhoodconsensusnetworksviasubmanifoldsparse
ACCV,2020. 2
convolutions. InECCV,pages605–621.Springer,2020. 2,7
[54] PruneTruong,MartinDanelljan,LucVanGool,andRadu
[39] Edward Rosten and Tom Drummond. Machine learning
Timofte. Learningaccuratedensecorrespondencesandwhen
forhigh-speedcornerdetection. InECCV,pages430–443.
totrustthem. InCVPR,pages5714–5724,2021. 3
Springer,2006. 1,2
[55] MichałTyszkiewicz,PascalFua,andEduardTrulls. Disk:
[40] Paul-EdouardSarlin,CésarCadena,RolandY.Siegwart,and
Learninglocalfeatureswithpolicygradient. NeurIPS,2020.
MarcinDymczyk. Fromcoarsetofine:Robusthierarchical
4,7
localizationatlargescale. CVPR,pages12708–12717,2019.
[56] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob
1,7,13
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
[41] Paul-EdouardSarlin,DanielDeTone,TomaszMalisiewicz,
andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,
andAndrewRabinovich.SuperGlue:Learningfeaturematch-
2017. 3,11
ingwithgraphneuralnetworks. InCVPR,2020. 1,2,4,5,6,
[57] QingWang,JiamingZhang,KailunYang,KunyuPeng,and
7,13
RainerStiefelhagen. Matchformer:Interleavingattentionin
[42] Paul-Edouard Sarlin, Ajaykumar Unagar, Måns Larsson, transformersforfeaturematching. InACCV,2022. 2,6,11
HugoGermain,CarlToft,VictorLarsson,MarcPollefeys,
[58] SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,and
Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and
HaoMa. Linformer: Self-attentionwithlinearcomplexity.
TorstenSattler.BacktotheFeature:LearningRobustCamera
ArXiv,abs/2006.04768,2020. 3
LocalizationfromPixelstoPose. InCVPR,2021. 1
[59] WeihaoYu,MiLuo,PanZhou,ChenyangSi,YichenZhou,
[43] TorstenSattler,WillMaddern,CarlToft,AkihikoTorii,Lars XinchaoWang,JiashiFeng,andShuichengYan. Metaformer
Hammarstrand,ErikStenborg,DanielSafari,MasatoshiOku- isactuallywhatyouneedforvision. CVPR,pages10809–
tomi,MarcPollefeys,JosefSivic,etal. Benchmarking6dof 10819,2022. 11
outdoorvisuallocalizationinchangingconditions. InCVPR,
pages8601–8610,2018. 7
[44] NikolaySavinov,AkihitoSeki,LuborLadicky,TorstenSat-
tler,andMarcPollefeys. Quad-networks:unsupervisedlearn-
ing to rank for interest point detection. In CVPR, pages
1822–1830,2017. 2
[45] JohannesL.SchönbergerandJan-MichaelFrahm. Structure-
from-motionrevisited. InCVPR,2016. 1,2
[46] YanShi,Jun-XiongCai,YoliShavit,Tai-JiangMu,Wensen
Feng,andKaiZhang. Clustergnn:Cluster-basedcoarse-to-
finegraphneuralnetworkforefficientfeaturematching. In
CVPR,pages12517–12526,2022. 2
10SupplementaryMaterial PoseEstimationAUC
PositionEncoding Time(ms)
@5 @10 @20
◦ ◦ ◦
A. Insight and Discussion about Aggregated
RoPE 56.4 72.2 83.5 139.2
AttentionModule sinusoidal 55.5 71.5 83.1 137.5
Table6. ImpactofpositionencodingontheMegaDepthdataset,
SomepreviousworksexploredusingpoolinginViTbutare
where averaged running times for an image pair with high-
withdifferentdesignchoicesfromourmethodduetodiffer-
resolution1200×1200arereported.
enttasks. PoolFormer[59]replacesthemulti-headattention
with pooling, which cannot be used for cross-attention in
wherex ,y ,x ,y arethecoordinatesofq andk ,Risa
i i j j i j
matchingthattwoimagesarenotpixel-aligned. MVit[16]
blockdiagonalmatrix:
usespoolingtoreducetokenslikeours,buttheycannotget
high-resfeaturesthatarerequiredformatching. R1(∆x,∆y) 
greD gaif tefe dre fn eatl ty u, rw ese ap nr dop tho ese nt uo pfi sars mt pc lo end bu efc ot ra ett fe en et dio -fn oo rwn aa rg d-
R(∆x,∆y)=
 

R2(∆x,∆y)
...

 

, (6)
Rd/4(∆x,∆y)
network(FFN)forlaterfusionwithinputfeature,asshown
inFig.3. Thisaggregate-and-upsampleblockcanminimize cos(θk∆x) −sin(θk∆x) 0 0 
i in nf fo or rm ma at ti ivo en fl eo as ts ui rn esa ,g wgr he eg ra eti co on ndan ud cte inffi gc uie pn satl my pg le int ghi bg eh f- ore res Rk(∆x,∆y)=  sin(θ 00k∆x) cos(θ 00k∆x)
c so ins ((
θθ0
kk ∆∆ yy )) − cosi sn
(θ(0
θ kk ∆∆ yy
)) 

, (7)
fusioniscrucialtofusesmoothlyinterpolatedmessageswith
whereθ = 1 , k [1,2,...,d/4]encodetheindex
adetailedfeaturemap. AblationisinTab.10(8). k 100004k/d ∈
offeaturechannels.
Moreover,WeperformConvonQvalueinsteadofpool- Comparedtotheabsolutepositionencodingusedinpre-
ingbecausesalienttokensshouldnot representneighbors viousmethods[7,17,34,48,50,57],weutilize2DRoPE
to query attention. The transformer is crucial for enhanc- toallowthemodeltofocusmoreontheinteractionbetween
ingnon-salientfeaturesformatching. PoolingonQcauses featuresratherthantheirspecificlocations,whichbenefits
the attention of texture-less areas dominated by neighbor- capturingthecontextoflocalfeatures. Moreover,relative
ingsalienttokens,reducingtheperformanceasablatedin position encoding is more robust to transformations like
Tab.10(6,7). rotation, translation, and scaling, which is important for
matchinglocalfeaturesindifferentviews.
B.ImplementationDetails
C.MoreExperimentsResults
B.1.LocalFeatureExtraction
C.1.MoreAblationStudies
RepVGG[11]blocksareusedtobuildafour-stagefeature In this part, we conduct more ablation studies on the
backbone. Weuseawidthof64andastrideof1forthefirst MegaDepth and ScanNet dataset to validate the design
stageandwidthsof[64,128,256]andstridesof2forthe choicesofourproposedmodules.
subsequentthreestages. Eachstageiscomposedof[1, 2,
4,14]RepVGGblocksandReLUactivations,respectively.
PositionEncoding Wecomparetheperformanceofour
Theoutputofthelaststagein1/8imageresolutionisusedfor
2DRoPEwiththesinusoidalpositionencoding[56]inTab.6.
efficientlocalfeaturetransformermodulestogetattended
The results show that using 2D RoPE can achieve better
coarsefeaturemaps. Thesecondandthirdstages’feature
performancethansinusoidalpositionencoding.
mapsarein1/2and1/4imageresolutions,respectively,which
areusedforfusingwithtransformedcoarsefeaturesforfine
Aggregation Range We show the performance of our
features.
method with different aggregation range s in Tab. 7. In
ouraggregatedattentionmodule,weusea4 4aggregation
B.2.PositionEncoding ×
rangetoreducetokensizebeforeperformingattention. Us-
ingasmalleraggregationrangeleadstomoretokens,with
Weusethe2DextensionofRotarypositionencoding[47]to
slightperformancechangesbutsignificantlyslowermatch-
encodetherelativepositionbetweencoarsefeaturesinself-
ingspeed. Thisvalidatestheeffectivenessofourparameter
attentionmodules. Giventheprojectedfeaturesqandk,the
choiceintheaggregationattentionmodule.
attentionscorebetweentwofeaturesq andk iscomputed
i j
as:
ImageResolution Wetesttheperformanceofourmethod
a =qTR(x x ,y y )k , (5) with different image resolutions to show the performance
ij i j − i j − i j
11SuperPoint + LightGlue AspanFormer Ours
Figure5.QualitativeResults.OurmethodiscomparedwiththesparsematchingpipelineSuperPoint[10]+LightGlue[28],semi-dense
matcherAspanFormer[7]. Theredcolorindicatesepipolarerrorbeyond5×10−4 onScanNetand1×10−4 onMegaDepth(inthe
normalizedimagecoordinates).Sincenoground-truthposeisavailableonInLocdataset,wecolorthematchwithpredictedconfidence.Red
indicateshigherconfidenceandbluefortheopposite.
PoseEstimationAUC PoseEstimationAUC
aggregationrange Time(ms) Resolution Time(ms)
@5 ◦ @10 ◦ @20 ◦ @5 ◦ @10 ◦ @20 ◦
s=4 56.4 72.2 83.5 139.2 640 640 51.0 67.4 79.8 41.7
s=2 56.2 72.2 83.6 271.1 ×
800 800 53.4 70.0 81.9 58.2
×
Table7. ImpactofaggregationrangeontheMegaDepthdataset, 960 960 54.7 70.7 82.4 81.8
×
1184 1184 56.4 72.2 83.5 139.2
where averaged running times for an image pair with high-
×
1408 1408 56.2 73.1 83.4 223.9
resolution1200×1200arereported. ×
Table8.ImpactoftestimageresolutionontheMegaDepthdataset.
andefficiencychanges. ResultsareshowninTab.8. Com-
paredwiththedefaultresolution1184 1184usedinthe
×
MegaDepth evaluation, using a larger image size leads to Method MegaDepthDataset Time(ms) ScanNetDataset Time(ms)
noticeableaccuracyimprovementwithaslowermatching
AUC@5◦AUC@10◦AUC@20◦ AUC@5◦AUC@10◦AUC@20◦
Full 56.4 72.2 83.5 139.1 19.2 37.0 53.6 34.4
speed.Ourmethodcanstillachievecompetitiveperformance Linear 54.1 70.3 82.1 132.7 16.8 33.2 49.0 36.9
usinglow-resolution640 640imageswiththefastestspeed.
Table 9. Impact of linear attention after aggregation on the
×
Therefore,ourmethodisprettyrobustinimageresolution MegaDepthandScanNetdataset,wheretheresolutionare1200×
choicesforflexiblereal-worldapplications. 1200and640×480,respectively.
LinearAttentionAfterAggregation Usinglinearatten- efficiencygainonhigh-resolutionbutwithaccuracydrop-
tion in our aggregated attention module introduces minor pingasshowninTab.9.
12
teNnacS
htpeDageM
coLnIPoseEstimationAUC Time(ms)
Method Time(ms) Method #Matches
@5◦ @10◦ @20◦
Matching RANSAC
OursFull 19.2 37.0 53.6 34.4
1)OursOptimal(w/odual-softmax) 17.4 34.4 51.2 27.0 SP+SG 43.6 0.53 487
2)ReplaceAgg.AttentiontoLoFTR’sTrans. 17.1 33.2 49.4 41.3 DRC-Net 143.9 2.78 1019
3)Replacetwo-stagerefine.toLoFTR’srefine. 18.1 35.8 52.4 31.8
LoFTR 76.2 1.59 995
4)Nosecond-stagerefinement 18.8 36.7 53.4 32.2
5)ReplaceRepVGGwithResNet 18.6 36.3 52.8 38.1 Ours 45.9/38.6 1.39 997
6)BothConvinAgg.Attention 18.6 35.8 52.5 34.4
7)BothPoolinAgg.Attention 18.3 35.2 51.7 34.1 Table12.RunningtimesofdifferentmethodsonHPatchesdataset.
8)UpsampleafterFFN 17.3 34.6 51.4 32.6
Allimagesareresizedsothattheirshortedgeequals480pixels
Table10.ThecomponentsofourmethodareablatedontheScan- followingSuperGlue[41]andLoFTR[48].ForOurs,therunning
Netdatasetagainforacomprehensiveunderstandingofourmethod, timesofthemodelusingFP32/Mixed-Precisionnumericalpreci-
whereaveragedrunningtimesforanimagepairwithresolution sionsareshown.
640×480arereported. Time(ms)
Method
Time(ms) Aachen InLoc
Process
Full Optimized SP+SG 55.9 83.3
Total 40.1 27.0 LoFTR 83.2 147.6
TopicFM 66.0 119.6
FeatureBackbone 9.1 5.8
PATS 315.8 1148.0
CoarseFeatureTransformation 11.7 12.9
AspanFormer 95.4 164.5
CoarseMatching 8.3 1.7
Ours 40.6/25.5 82.0/46.3
FineFeatureFusion 8.0 4.8
Two-StageRefinement 3.0 2.0
Table 13. Running times of different methods on Aachen and
Table11.Timecostforanimagepairof640×480ontheScanNet InLocdataset. Tomeasuretherunningtime,wesample818and
dataset.TheoptimizedmodelusesMixed-Precisionnumericalac- 356pairsofimagesfromtheNetVLAD[2]’sretrievalresultsfor
curacyanddropsthedual-softmaxoperatorinthecoarsematching AachenandInLoc,respectively.Allimagesareresizedsothattheir
phase. longedgeequals1024pixelsfollowingHLoc[40].ForOurs,the
runningtimesofthemodelusingFP32/Mixed-Precisionnumerical
precisionsareshown.
AdditionalablationstudiesontheScanNetdataset We
further repeat the ablation studies in the main paper and
andRANSAClatencyofourmethodaresmallerthanLoFTR
conductadditionalablationstudiesontheScanNetdatasetto
withasimilarnumberofmatches.
validatethedesignchoicesofourproposedmodules.Results
areshowninTab.10.
E.LimitationsandFutureWorks
C.2.MoreQualitativeResults
We find that our method may fail when strong repetitive
structuresexist,suchasmatchinganimagepairthatdepicts
More qualitative results on the ScanNet dataset, InLoc
differentscenescontainingthesamechair.Wethinkthismay
dataset,andMegaDepthdatasetareshowninFig.5.
beduetothecurrentmodelfocusingmoreonlocalfeatures
for accurate matching, where global semantic context is
D.DetailsAboutTiming
lacking. Therefore, themechanismofhigh-levelcontexts
Therunningtimesevaluationinthepaperareaveragedover canbeaddedtothemodelforperformanceimprovementon
allpairsintestdatasetwithawarm-upof50pairsforac- ambiguousscenes. Moreover,webelievetheefficiencyof
curatemeasurement. Allthemethodsaretestedonasingle ourmethodcanbefurtherimprovedbyadoptingtheearly
NVIDIARTX3090GPUwith14coresofIntelXeonGold stopstrategyofLightGlue[28]sincethecontributionofthe
6330CPU. proposedefficientaggregationmoduleisorthogonaltoit.
Wefurtherreporteachpartrunningtimeofourmethodin
Tab.11,wherebothfullandoptimizedmodelareshown. We
noticedthatskippingthedual-softmaxofCoarseMatching
intheoptimizedmodelcansignificantlyreducetherunning
time. What’smore,withthebenefitofMixed-Precision,the
runningtimeoffeatureextractioncanbefurtherreduced.
The latency on 3 datasets included in Tabs. 2 to 4 are
showninTab.12andTab.13,whereconclusionandspeed
rankings are the same as indicated in Tab. 1. We further
showRANSACtimeonHPatchesdataset. Bothmatching
13