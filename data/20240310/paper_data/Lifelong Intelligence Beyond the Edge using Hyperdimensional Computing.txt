Lifelong Intelligence Beyond the Edge using Hyperdimensional
Computing
XiaofanYu AnthonyThomas IvanniaGomezMoreno
x1yu@ucsd.edu ahthomas@ucsd.edu ivannia.gomez@cetys.edu.mx
UniversityofCaliforniaSanDiego UniversityofCaliforniaSanDiego CETYSUniversity,CampusTijuana
LaJolla,California,USA LaJolla,California,USA Tijuana,Mexico
LouisGutierrez TajanaŠimunićRosing
l8gutierrez@ucsd.edu tajana@ucsd.edu
UniversityofCaliforniaSanDiego UniversityofCaliforniaSanDiego
LaJolla,California,USA LaJolla,USA
ABSTRACT andmemoryresourcesforon-devicetraining[15,34].Nevertheless,
On-devicelearninghasemergedasaprevailingtrendthatavoids theseeffortsoftenrelyonstaticmodelsforinferenceorlackthe
theslowresponsetimeandcostlycommunicationofcloud-based adaptabilitytoaccommodatenewenvironments.
learning.Theabilitytolearncontinuouslyandindefinitelyina Tofundamentallyaddresstheseissues,sensordevicesshouldbe
changingenvironment,andwithresourceconstraints,iscritical capableof"lifelonglearning"[42]:tolearnandadaptwithlimited
forrealsensordeployments.However,existingdesignsareinade- supervisionafterdeployment.On-devicelifelonglearningreduces
quateforpracticalscenarioswith(i)streamingdatainput,(ii)lack theneedforexpensivedatacollection(includinglabels)andof-
ofsupervisionand(iii)limitedon-boardresources.Inthispaper, flinemodeltraining,operatinginadeploy-and-runmanner.This
wedesignanddeploythefirston-devicelifelonglearningsystem approachenablesautonomouslearningsolelyfromtheincoming
calledLifeHDforgeneralIoTapplicationswithlimitedsupervi- sampleswithminimalsupervision,andisthusabletoprovidereal-
sion.LifeHDisdesignedbasedonanovelneurally-inspiredand time decision-making even without a network connection. The
lightweightlearningparadigmcalledHyperdimensionalComput- lifelongaspectisessentialforhandlingdynamicreal-worldenvi-
ing(HDC).Weutilizeatwo-tierassociativememoryorganization ronments,representingthefutureofIoT.
tointelligentlystoreandmanagehigh-dimensional,low-precision Althoughextensiveresearchhasinvestigatedlifelonglearning
vectors,whichrepresentthehistoricalpatternsasclustercentroids. acrossvariousscenarios[42],existingtechniquesfacechallenges
WeadditionallyproposetwovariantsofLifeHDtocopewithscarce that render them unsuitable for real-world deployments. These
labeledinputsandpowerconstraints.WeimplementLifeHDonoff- challengesinclude:
the-shelfedgeplatformsandperformextensiveevaluationsacross (C1) Streamingdatainput.Edgedevicescollectstreamingdata
threescenarios.OurmeasurementsshowthatLifeHDimprovesthe fromadynamicenvironment.Thisonlinelearningwithnon-
unsupervisedclusteringaccuracybyupto74.8%comparedtothe iid data contrasts with the default offline and iid setting
state-of-the-artNN-basedunsupervisedlifelonglearningbaselines wheremultiplepassesontheentiredatasetareallowed[16].
withasmuchas34.3xbetterenergyefficiency.Ourcodeisavailable (C2) Lackofsupervision.Obtainingground-truthlabelsand
athttps://github.com/Orienfish/LifeHD. expertguidanceisoftenchallengingandexpensive.Most
lifelonglearningmethodsrelyonsomeformofsupervision,
KEYWORDS suchasclasslabels[28]orclassshiftboundaries[46],which
aretypicallyunavailableinreal-worldscenarios.
EdgeComputing,LifelongLearning,HyperdimensionalComputing
(C3) Limiteddeviceresources.Neuralnetworks(NN)areknown
fortheirhighresourcedemands[60].Furthermore,themain
1 INTRODUCTION techniquesforlifelonglearningbasedonNN,suchasreg-
ThefusionofartificialintelligenceandInternetofThings(IoT)has ularization[28]andmemoryreplay[35],addextracompu-
becomeaprominenttrendwithnumerousreal-worldapplications, tationalandmemoryrequirementsbeyondstandardNNs,
suchasinsmartcities[10],smartvoiceassistants[55],andsmart makingtheminadequateforedgedevices.
activityrecognition[62].However,thepredominantcurrentap- Real-WorldExample.Toillustratethechallengesfaced,wepresent
proachiscloud-centric,wheresensordevicessenddatatothecloud areal-worldscenarioinFig.1.Consideracameradeployedinthe
forofflinetrainingusingextensivedatasources.Thisapproach wildcontinuouslycollectingdatafromsurroundingenvironment.
faceschallengeslikeslowupdatesandcostlycommunication,in- Ourgoalistotrainanunsupervisedobjectrecognitionalgorithm
volving the exchange of large sensor data and models between ontheedgedevice,purelyfromthedatastream.Weconstructboth
theedgeandthecloud[53].Instead,recentresearchhasshifted iidandsequential(oneclassappearsaftertheother)streamsfrom
towardsedgelearning,wheremachinelearningisperformedon CIFAR-100[6],andadoptthesmallestMobileNetV3model[20]
resource-constrainededgedevicesrightnexttothesensors.While withthepopularBYOLunsupervisedlearningpipeline[16].As
most studies focused on inference-only tasks [32, 33, 50], some seeninFig.1,whilethemodelshowsimprovedaccuracywithiid
recentworkhasinvestigatedtheoptimizationofcomputational streams,ithasasignificantperformancelossundersequentially
4202
raM
7
]GL.sc[
1v95740.3042:viXraUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
OurbasicapproachinLifeHDisfullyunsupervised.However,
inreality,labelsmaybeavailable(orcouldbeacquired)forasmall
numberofexamples.WeintroduceLifeHDsemitoexploitalimited
numberoflabeledsamplesasanextensiontothepurelyunsuper-
visedLifeHD.Additionally,weproposeLifeHDa,whichusesan
adaptiveschemeinspiredbymodelpruning,toadjusttheHDem-
beddingdimensionon-the-fly.LifeHDaallowsustofurtherreduce
resourceusage(powerin-particular),wherenecessary.
Figure1:Real-worldexampleofon-devicelifelonglearningeval-
Insummary,thecontributionsofthispaperare:
uatedusingtheunsupervisedclusteringaccuracymetric[63].The
traininglatencyismeasuredontwotypicaledgeplatforms. (1) WedesignLifeHD,thefirstend-to-endsystemforon-device
unsupervisedlifelongintelligenceusingHDC.LifeHDbuilds
uponHDC’slightweightsingle-passtrainingcapabilityand
ordereddata,highlightingtheNNeffectof“forgetting”inastream- incorporatesournovelclustering-basedmemorydesignto
ingandunsupervisedsetting.Intermsofefficiency,wemeasure addresschallenges(C1)-(C3).
thetraininglatencyofMobileNetV3(small)[20]ontwotypical (2) WefurtherproposeLifeHDsemiasanextensiontofullyuti-
edgeplatforms,RaspberryPi(RPi)4B[2]andJetsonTX2[1]by lizethescarcelabeledsamplesalongwiththestream.We
running10gradientdescentstepsonasinglebatchof32samples. deviseLifeHDathatenablesadaptivepruninginLifeHDto
Evenontheseverycapableedgeplatforms,trainingtakesupto reducereal-timepowerconsumption.
17.4seconds,clearlyunsuitableforreal-timeprocessingunder30 (3) WeimplementLifeHDonoff-the-shelfedgedevicesandcon-
FPS.Therefore,anovelapproachcapableofhandlingnon-iiddata ductextensiveexperimentsacrossthreetypicalIoTscenarios.
andofferingmoreefficientupdatesisnecessarytoaccommodate LifeHDimprovestheunsupervisedclusteringaccuracyupto
thecontinualchangesindata. 74.8%with34.3xbetterenergyefficiencycomparedtolead-
Toaddresschallenges(C1)-(C3),wedrawinspirationfrombiol- ingunsupervisedNNlifelonglearningmethods[13,14,54].
ogy,whereeventinyinsectsdisplayremarkablelifelonglearning (4) LifeHDsemiimprovestheunsupervisedclusteringaccuracy
abilities,anddosousing“hardware”thatrequiresverylittleen- byupto10.25%overtheSemiHD[22]baselineunderlimited
ergy[4].Hyperdimensionalcomputing(HDC)isanemergingpar- labelavailability.LifeHDa limitstheaccuracylosswithin
adigminspiredbytheinformationprocessingmechanismsfound 0.71%usingonly20%ofLifeHD’sfullHDdimension.
in biological brains [24]. In HDC, all data is represented using The rest of the paper is organized as follows. We start by a
high-dimensional,low-precision(oftenbinary)vectorsknownas comprehensivereviewofrelatedworksinSec.2.Wethenintroduce
“hypervectors,”whichcanbemanipulatedthroughsimpleelement- salientbackgroundonHDCinSec.3tohelpunderstanding.We
wiseoperationstoperformtaskslikememorizationandlearning. formally define the unsupervised lifelong learning problem we
HDCiswell-understoodfromatheoreticalstandpoint[56]and targettosolveinSec4.Afterwards,Sec.5describesthedetailsof
sharesintriguingconnectionswithbiologicallifelonglearning[52]. ourmajordesignLifeHD.Sec.6introducesLifeHDsemiandLifeHDa.
Furthermore,itsuseofbasicelement-wiseoperatorsalignswith Sec.7presentstheimplementationandresultsofLifeHD,whilethe
highlyparallelandenergy-efficienthardware,offeringsubstantial evaluationsofLifeHDsemiandLifeHDaarereportedinSec8.We
energysavingsinIoTapplications[11,23,27,65].WhileHDCisre- addthediscussionsandfutureworksinSec.9.Theentirepaperis
portedasapromisingavenue,theliteraturetodatehasnotexplored concludedinSec.10.
weakly-supervisedlifelonglearningusingHDC.
2 RELATEDWORK
Inthiswork,wedesignanddeployLifeHD,thefirstsystemfor
on-devicelightweightlifelonglearninginanunsupervisedanddy- LifelongandOn-DeviceLearning.Lifelonglearning(orcontin-
namicenvironment.LifeHDleveragesHDC’sefficientcomputation uallearning)isalargeandactiveareaofresearchinthebroader
andadvantagesinlifelonglearning,whileeffectivelyhandlingunla- machinelearningcommunity.Catastrophicforgettingisamajor
beledstreaminginputs.Thesecapabilitiesextendbeyondthescope challengeinlifelonglearning,andreferstoacommonlyobserved
ofexistingHDCdesigns,whichhavefocusedoverwhelminglyon empiricalphenomenoninwhichupdatingcertainmachinelearning
thesupervisedsetting[23,27].Specifically,LifeHDrepresentsthe modelswithnewdataseverelydegradestheirabilitytoperform
input ashigh-dimensional, low-precisionvectors, and, drawing previouslylearnedtasks[36].Previousworksproposedtechniques
inspirationfromworkincognitivescience[5],organizesdatainto suchasdynamicarchitecture[31,49],regularizationbypenalizing
atwo-tiermemoryhierarchy:ashort-term“workingmemory”and importantweights[28,66],knowledgedistillationfrompastmod-
along-termmemory.Theworkingmemoryprocessesincoming els[14]andexperiencereplayusingamemorybuffer[35,58].The
dataandsummarizesitintoagroupoffine-grainedclustersthatare lifelonglearningliteraturehasexaminedawiderangeofproblem
representedbyhypervectorscalledclusterHVs.Long-termmemory settings,rangingfromthefullysupervisedcase,inwhichtasks
consolidatesthefrequentlyappearedclusterHVsintheworking andclasslabelsareprovided,andthefullyunsupervisedcasewith-
memory,andwillberetrievedformergingandinferenceoccasion- outanylabelsandpriorknowledge[13,57].However,allofthese
ally. We emphasize that LifeHD is designed to suit a variety of worksarebasedondeepNNsandrequirebackpropagation,which
edgedeviceswithdiverseresourcelevels.Moreefficiencygains isproblematicforresource-constraineddevices.
canbeachievedbyemployingoptimizationssuchaspruningand Neurally-inspired lightweight algorithms have recently been
quantization[15,61],butthisisnotthefocusofourwork. proposed for lifelong learning applications. FlyModel [52] andLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
SDMLP [8] use sparse coding and associative memory for life-
longlearning.However,bothapproachesassumefullsupervision.
STAM[54]isanexpandablememoryarchitectureforunsupervised
lifelonglearning,usinglayeredreceptivefieldsandatwo-tiermem-
oryhierarchy.Itlearnsviaonlinecentroid-basedclusteringpipeline,
noveltydetectionandmemoryupdates.Nevertheless,thememory
inSTAMissolelydedicatedtoimagestorage,whileourLifeHD
additionallyemphasizesmergingpastpatternsintocoarsegroups Figure2:SpatiotemporalHDCencodingfortime-seriesdata.Left:
andshowsmoreeffectivelearningperformance. randomgenerationoflevelhypervectors.Right:thecompleteen-
Recentworksoptimizetheresourceusageofon-devicetraining codingprocess.
viapruningandquantization[34,45],tuningpartialweights[9,47],
memoryprofilingandoptimization[15,61,64],aswellasgrowing andisintuitivelyusedtobuildsets.Thebundlingoperation
theNNonthefly[67].Alltheseworksoptimizetraininggiven isimplementedthroughaddition.
resourceconstraintsanddonotfocusonlifelonglearning.They (3) Permute:𝜌 :H →H.Permutationcanbeusedtoencode
areorthogonaltothecontributionofLifeHDwhichfocuseson sequentialinformationandistypicallyimplementedusinga
adaptiveandcontinualtraining.LifeHDcanbefurtheroptimized cyclicshift.
bycombiningwithsuchtechniques. Theencodingfunction𝜙 :X→H embedsdatafromitsambient
HyperdimensionalComputing.HDChasgarneredsubstantial representationintoHD-space.Ingeneral,encodingshouldpreserve
interestfromthecomputerhardwarecommunityasanenergy- somemeaningfulnotionofsimilaritybetweeninputpointsinthe
efficientandlow-latencyapproachtolearning,andhasbeensuc- sensethat𝜙(𝑥)·𝜙(𝑥′)≈𝑘(𝑥,𝑥′),where𝑘issomesimilarityfunc-
cessfullyappliedtoproblemssuchashumanactivityrecognition[27], tionofinterestonX.Inthispaper,weusespatiotemporalencoding
voicerecognition[23],imagerecognition[11,65],tonameafew. fortimeseriessensordata,andHDnnformorecomplexdata,such
ThelargemajorityofliteratureonHDChasfocusedonusingthe asimages,whichweexplaininthefollowing.
techniquetoperformsupervisedclassificationtasks.Amongthelim- SpatiotemporalEncoding.Thespatiotemporalmethod[38]
itedliteratureforweakly-supervisedlearningwithHDC,HDClus- jointlyencodestheanaloginformationfromeachsensor(spatial)
ter[21]enabledunsupervisedclusteringinHDCwithanewalgo- andateachtimestamp(temporal)toasinglehypervector.Sup-
rithmthatissimilartoK-Means.SemiHD[22]isasemi-supervised posethereare𝑑-differentsensors𝑠 1,...,𝑠 𝑑,eachofwhichproducea
learningframeworkusingHDCwithiterativeself-labeling.Hyper- real-valuedreading𝑥 1,...,𝑥 𝑑,whereuponwemaymodeltheinput
seed[41],C-FSCIL[18]andFSL-HD[65]adoptedHDCorsimilar ataparticularmomentintimebyasetoftuples{(𝑠 𝑖,𝑥 𝑖)}𝑑 𝑖=1.We
vectorsymbolicarchitectures(VSA)forunsupervisedorfew-shot pre-generateasetofbasehypervectorstorepresentthevaluesand
learning.Allaboveworksdidnotconsiderthelifelongaspectand sensorsrespectively.Torepresentarealvaluedfeature𝑥 ∈R,we
usedofflinetrainingonastaticdataset.Tothebestoftheauthors’ quantizethesupportof𝑥intoasetofbinswithcentroids𝑎 1,...,𝑎 𝑄,
knowledge,LifeHDisthefirstworkthatdesignsanddeployslife- andassigneachbinanembedding𝜑(𝑎 𝑖),whichwecalllevelhy-
longlearninginedgeIoTapplicationsespeciallywithzeroormini- pervectors,suchthat𝜑(𝑎 𝑖)·𝜑(𝑎 𝑗)ismonotonicallydecreasingin
malamountoflabels. |𝑎 𝑖 −𝑎 𝑗|.AsshowninFig.2(left),weinitiallygeneratearandom
hypervectorforthefirstlevel.Tomaintainsimilaritybetweenad-
3 BACKGROUNDONHDC jacentlevelhypervectors,foreachsubsequentlevel,werandomly
HyperdimensionalComputing(HDC)isanemergingparadigmfor flipafractionofbitsfromthepreviouslevelasdescribedin[56].
informationprocessingfromthecognitive-neuroscienceliterature
Thefractionofflippingisdenotedas𝑃.Thisprocessisrepeated
[24].InHDC,allcomputationisperformedonlow-precisionand
untilall𝑄levelhypervectorsaregenerated.Torepresentdifferent
distributedrepresentationsofdatathataccordnaturallywithhighly sensor𝑠,weassigneachsensorarandomembedding𝜓(𝑠 𝑖),which
parallelandlow-energyhardware.
wecallIDhypervectors,bysampling𝜓(𝑠 𝑖)∼Unif({±1}𝐷).
ThefirststepinHDCisencoding,whichmapsaninput𝑥 ∈X ThecompletespatiotemporalencodingisvisualizedinFig.2
toadistributedrepresentation𝜙(𝑥)livinginsome𝐷-dimensional (right).Weencodeapair(𝑠,𝑥)via𝜓(𝑠)⊗𝜑(𝑎(𝑥)),where𝑎(𝑥)is
inner-productspaceH,thatwecallthe“HD-space.”Forinstance,
thecentroidofthebinclosestto𝑥.Thispreservesboththeleveland
onemighttakeH ⊂{±1}𝐷 ,orH ⊂R𝐷 .Werefertopointsinthe sensorIDinformation.Toencodethereadingsforallsensorswe
HD-spaceashypervectors.Encodingsofdatacanbemanipulated bundletogethertheirindividualembeddingsandroundtobipolar
soastobuildmorecomplexcompositerepresentationsusingaset
(e.g.{±1})precision:𝜙(𝑥)=Sign(cid:16)(cid:201)𝑑
𝑖=1𝜓(𝑠 𝑖)⊗𝜑(𝑎(𝑥
𝑖))(cid:17)
.Finally,
ofoperatorsdefinedasfollows: to represent a sequence of𝑇 readings:𝑋 = {𝑥 1,...,𝑥 𝑇}, we use
(1) Bind:⊗ : H ×H → H.Bindingtakestwohypervectors permutation:𝜙(𝑋)=(cid:203)𝑇 𝜏=1𝜌𝜏(𝜙(𝑥 𝜏)).
as inputs and returns a hypervector that is dissimilar to HDnnEncoding.Inthisworkweusetherecentlyproposed
bothinputs,andisintuitivelyusedtorepresenttuples.For HDnnstyleencoding[11,65]thatcombinesapretrainedandfrozen
bipolarhypervectors(i.e.,H ⊂{±1}𝑑 ),thebindingoperator NNfeatureextractorwithHDC’sspatiotemporalencodingtoobtain
istypicallyelement-wisemultiplication. stateoftheartaccuracyforsoundandimages.InHDnntheinputs
(2) Bundle:⊕:H×H →H.Bundlingtakestwohypervectors tothespatio-temporalencoding,𝑠 1,...,𝑠 𝑑,areintermediatefeature
asinputandreturnsahypervectorsimilartobothoperands, outputs of the pretrained and frozen NN (Fig. 3). For example,Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
thatpreservestheoverall(im)balancebetweentheclasses.Note,
thatevenwhenoneclasshasnotappearedinthetrainingdata
stream,itisalwaysincludedinE.HenceEisaglobalviewofall
classesthatcanpotentiallyexistintheenvironment.
UnsupervisedClusteringAccuracy.Sincewedonotgiveclass
labelsorthetotalnumberofclassesduringtraining,thepredicted
labelcanbedifferentfromtheground-truthlabel.Therefore,for
Figure3:AnoverviewofsupervisedHDCpipelineincludingencod- evaluationmetric,wecannotadoptthesimplepredictionaccuracy
ing,trainingandinference(similaritycheckforclassification).An thatrequiresexactlabelmatching.Instead,weemployawidely
additionalpretrainedNNisusedasafeatureextractorintheHDnn. usedclusteringmetricknownasunsupervisedclusteringaccuracy
(ACC)[63],whichmirrorstheconventionalaccuracyevaluation
asectionofMobileNetpretrainedonImageNetcreatesfeatures
butwithinanunsupervisedcontext.
whicharethenencodedintoHDhypervectorsforobjectrecognition
Suppose𝜔 𝑘 isthepredictedclusteroftestingsample(𝑋 𝑘,𝑦 𝑘)
tasks.Thisonlymarginallyincreasesthecomputationalcostsas
notrainingisperformedonNN,allthetraininghappensinHD. inE.ACCiscomputedas:𝐴𝐶𝐶 =max𝑚 |E1 | (cid:205) 𝑘|E =1| 1{𝑦 𝑘 =𝑚(𝜔 𝑘)},
SupervisedTrainingandInference.Acommonusecaseof
where𝑚rangesoverallpossibleone-to-onemappingsbetween
HDC,summarizedinFig.3,istofitclassifiers.Inparticular,letus predictedclustersandground-truthclasses.Intuitively,thismetric
supposethatweseeasetof𝑁 labeledsamples{(𝑋 𝑖,𝑦 𝑖)} 𝑖𝑁 =1,where computestheaccuracyunderthe“best”mappingbetweenclusters
𝑥 𝑖 isaninput,and𝑦 𝑖 ∈{𝑐 1,...,𝑐 𝐽}isaclasslabel.Inthetraditional andlabels.ThebiggestadvantageofACCisthatitdoesnotrequire
approachtoclassification,onesimplyrepresentseachclassviathe thenumberofclustersandclassestobeequal.Forinstance,acluster
bundleofitstrainingdata.Thatis:𝜙(𝑐 𝑗) = (cid:201) 𝑖:𝑦𝑖=𝑐𝑗𝜙(𝑋 𝑖).We o laf bp ei ln oe fs ta rn ed esa .Wclu est te rer ao tf sr ue cd hw co lo ud stb eo rit nh gbe relo sn ug ltt ao st ahe vag lr io du ln ed art nru inth
g
storethetrainedclasshypervectorsinanassociativememory.For
outcome,withaconcretevisualizationshowninSec.7.4.
example,inFig.3,wecomputeandstoretheclasshypervectors
of cats and dogs. During inference, we first encode the testing
sample𝑋 𝑞intoaqueryhypervector𝜙(𝑋 𝑞)usingthesameencoding 5 LIFEHD
procedureasfortraining.Wethenpredictthelabelcorrespondingto
Inthissection,wepresentthedesignofLifeHD,thefirstunsuper-
themostsimilarclassasmeasuredbythecosinesimilarity,i.e.,𝑦ˆ=
visedHDCframeworkforlifelonglearningingeneraledgeIoT
argmax𝑗cos(𝜙(𝑋 𝑞),𝜙(𝑐 𝑗))∝argmax𝑗(𝜙(𝑋 𝑞)·𝜙(𝑐 𝑗))/∥𝜙(𝑐 𝑗)∥.
applications. Compared to operating in the original data space,
HDCimprovespatternseparabilitythroughsparsityandhighdi-
4 PROBLEMDEFINITION
mensionality,makingitmoreresilientagainstcatastrophicforget-
Beforedivingintoourmethod,wefirstrigorouslyformulatethe ting[52].LifeHDpreservestheadvantagesofHDCincomputa-
unsupervisedlifelonglearningproblemusingstreamingsources, tionalefficiencyandlifelonglearning,whilehandlingtheinputof
drivenbyreal-worldIoTapplications. unlabeledstreamingdata,whichhasnotbeenachievedinprevious
StreamingData.Torepresentcontinuouslychangingenviron- work[18,21,22,41,65].
ment,weassumeawell-knownclass-incrementalmodelinlifelong
learning,inwhichnewclassesemergeinasequentialmanner[46].
5.1 LifeHDOverview
Wealsoallowdatadistributionshiftwithinoneclass.Thissetting
modelsascenarioinwhichadeviceiscontinuouslysamplingdata Fig.4givesanoverviewofhowLifeHDworks.ThefirststepisHDC
whilethesurroundingenvironmentmaychangeimplicitlyover encodingofdataintohypervectorsasdescribedinSec.3.Training
time,e.g.,theself-drivingvehicleasshowninFig.1.Werequire samples𝑋 areorganizedintobatchesofsize𝑏𝑆𝑖𝑧𝑒andinputinto
thatallsamplesappearonlyonce(i.e.,single-passstreams). anoptionalfixedNNforfeatureextraction(e.g.forimagesand
Formally,weconsiderascenarioinvolving𝑑sensors,eachpro- sound)andtheencodingmodule.Theencodedhypervectors𝜙(𝑋)
ducingarealvaluedreading.Wegroupreadingsintoslidingwin- areinputtoLifeHD’stwo-tiermemorydesigninspiredbycognitive
dowsoflength𝑇,andtreatonesuchbatch𝑋 𝑖 ∈R𝑇×𝑑 asaninput sciencestudies[5],consistingofworkingmemoryandlong-term
sample.Eachinput𝑋 𝑖 isassociatedwithanunknownlabel𝑦 𝑖.Im- memory.Thismemorysystemintelligentlyanddynamicallyman-
portantly,thelabelsarenotmadeavailableduringtraining,nor ageshistoricalpatterns,storedashypervectorsandreferredtoas
theboundariesofclassshift.Thereforetheentireprocessisun- clusterHVs.AsshowninFig.4,theworkingmemoryisdesigned
supervised. We represent the data stream associated with each withthreecomponents:noveltydetection,clusterHVupdateand
classbyD𝑗 ={𝑋 1,𝑋 2,...},andthesetofstreamsforallclassesby clusterHVmerge.𝜙(𝑋)isfirstinputintonoveltydetectionstep(○1).
D ={D 1,...,D𝐽}.Notethattheclass-incrementalstreamscanhave AninsertiontotheclusterHVsismadeifanoveltyflagisraised,
imbalancedclasses,i.e.,|𝐷 𝑖|≠|𝐷 𝑗|,𝑖 ≠ 𝑗,andgradualdistribution otherwise𝜙(𝑋) updatestheexistingclusterHVs(○2).Thethird
shiftwithineachclass. component,clusterHVmerge(○3),retrievestheclusterHVsfrom
LearningProtocol.Ourgoalistobuildaclassificationalgo- long-termmemory,andmergessimilarclusterHVsintoasuper-
rithmthatmapsX→Y.Forevaluation,weusethecommonevalu- clusterviaanovelspectralclustering-basedmergingalgorithm[59].
ationprotocolinstate-of-the-artlifelonglearningworks[13,14,54], Theinteractionbetweenworkingandlong-termmemoryhappens
inwhichweconstructaniiddatasetE = {(𝑋 𝑘,𝑦 𝑘)}forperiodic as commonly encountered cluster HVs are copied to long-term
testing,bysamplinglabeledexamplesfromeachclassinamanner memory,whichwecallconsolidation(○4).Finally,whenthesizeLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
Figure4:Theend-to-endalgorithmflowofLifeHD.
Table1:Listofimportantnotations. HV𝑖:𝜇
𝑖
and𝜎ˆ𝑖,whichrepresentthemeancosinedifferenceand
standarddifferencebetweentheclusterHVanditsassignedinputs.
Symbol Meaning Given𝜙(𝑋),wefirstidentifythemostsimilarclusterHV,denoted
𝑑 Numberofsensorsources by𝑗.LifeHDmarks𝜙(𝑋)as“novel"ifitsubstantiallydiffersfrom
𝑇 Timewindowlengthofoneinputsample𝑋
itsnearestclusterHV.Specifically,thisdissimilarityismeasuredby
𝐷 DimensionoftheHD-space comparingcos(𝜙(𝑋),𝑚 𝑗)withathresholdbasedonthehistorical
𝑄 Numberofquantizedlevelforencoding distancedistributionofclusterHV𝑗:
𝑃 Fractionofrandombitfliptogeneratelevelhypervector
𝜙 HDCencodingfunction
𝜑,𝜓 LevelandIDhypervectorencodingfunction If cos(𝜙(𝑋),𝑚 𝑗) <𝜇 𝑗 −𝛾𝜎ˆ𝑗,thenflagnovel. (1)
𝑏𝑆𝑖𝑧𝑒 Batchsizeofinputsamples
M,L SetofclusterHVsstoredintheworkingandlong-termmemory Thehyperparameter𝛾 fine-tunesthesensitivitytonovelties.
𝑀,𝐿 MaximumnumberofclusterHVsintheworkingandlong-term LifeHDrecognizesnew𝜙(𝑋)asprototypesandinsertstheminto
memory theworkingmemory.Whenreachingitssizelimit𝑀,theworking
𝜇,𝜎ˆ Meansimilarityandstandarddifferenceofbetweeneachcluster
memoryexperiencesforgetting(○5 inFig.4).Theleastrecentlyused
HVanditsassignedinputsintheworkingmemory
ℎ𝑖𝑡 ThenumberoftimesthateachclusterHVishitintheworking (LRU)clusterHV,representedby𝐿𝑅𝑈 =argmin𝑖𝑀 =1𝑝 𝑖,isreplaced.
memory
Here𝑝correspondstothelatestbatchindexwheretheclusterHV
ℎ𝑖𝑡 𝑡ℎ ThehitfrequencythresholdtoconsolidateclusterHVfrom wasaccessed.Asimilarforgettingmechanismisconfiguredforthe
workingtolong-termmemory long-termmemory,wherethelastbatchaccessedismarkedwith𝑞.
𝑝,𝑞 ThemostrecentbatchindexwheneachclusterHVisaccessed,
fortheworkingandlong-termmemoryclusterHVs
5.3 ClusterHVUpdate
𝛾 Hyperparameterfornoveltydetectionsensitivity
𝛼 MovingaverageupdaterateduringclusterHVupdate Ifnoveltyisnotdetected,indicatingthat𝜙(𝑋)closelymatchesclus-
𝑔 𝑢𝑏 ClusterHVmergesensitivity terHV𝑗,weproceedtoupdatetheclusterHVanditsassociated
𝑓𝑚𝑒𝑟𝑔𝑒 ClusterHVmergefrequency information(○2 inFig.4).Thisupdateprocessinvolvesbundling
𝑟 AveragelabelingratioinLifeHDsemi 𝜙(𝑋)withclusterHV𝑚 𝑗,akintohowclasshypervectorsareup-
𝐷𝑎 DimensionofthemaskusedinLifeHDa dated as described in Sec. 3, and updaing 𝜇 𝑗 and 𝜎ˆ𝑗 with their
movingaverage:
limitofeitherworkingorlong-termmemoryisreached,theleast
recentlyusedclusterHVsareforgotten(○5). 𝑚 𝑗 ←𝑚 𝑗 ⊕𝜙(𝑋) (2a)
AllmodulesinLifeHDworkcollaboratively,makingitadaptive
𝜇 𝑗 ←(1−𝛼)𝜇 𝑗 +𝛼cos(𝜙(𝑋),𝑚 𝑗) (2b)
androbusttocontinuouslychangingstreamswithoutrelyingon
anyformofpriorknowledge.Forexample,inscenariosofdistribu- 𝜎ˆ𝑗 ←(1−𝛼)𝜎ˆ𝑗 +𝛼|cos(𝜙(𝑋),𝑚 𝑗)−𝜇 𝑗| (2c)
tiondrift,LifeHDmaygeneratenewclusterHVsuponencountering ℎ𝑖𝑡 𝑗 ←ℎ𝑖𝑡 𝑗 +1,𝑝 𝑗 ←𝑖𝑑𝑥 (2d)
driftedsamplesinitially,whichcanlaterbemergedintocoarseclus-
ters.ThisapproachensuresthatLifeHDcanefficientlycaptureand Thehyperparameter𝛼 adjuststhebalancebetweenhistoricaland
retainhistoricalpatterns. recentinputs,whereahigher𝛼givesmoreweighttorecentsamples.
Inthefollowing,wediscussmoredetailsaboutthemajorcom- Properlymaintaining𝜇 𝑗 and𝜎ˆ𝑗 isvitalfortrackingthe“radius”of
ponentsofLifeHD:noveltydetection(Sec.5.2),clusterHVupdate
eachclusterHV,affectingfuturenoveltydetection.Wealsoincrease
(Sec.5.3),andclusterHVmerging(Sec.5.4).Wesummarizethe thehitfrequencyℎ𝑖𝑡
𝑗
andrefresh𝑝
𝑗
withcurrentbatchindex𝑖𝑑𝑥.
importantnotationsusedinthispaperinTable1. ℎ𝑖𝑡 𝑗 isfurtherusedtocomparedwithapredeterminedthreshold
ℎ𝑖𝑡 𝑡ℎ todecidewhenaworkingmemoryclusterHVappearssuffi-
5.2 NoveltyDetection
cientlyfrequentlytobeconsolidatedtolong-termmemory(○4 in
Theinitialnoveltydetectionstep(○1 inFig.4)iscrucialforidentify- Fig.4).𝑝 𝑗determinesforgettingasdescribedintheprevioussection.
ingemergingpatternsintheenvironment.SupposeM ={𝑚 1,...,𝑚 𝑀} Withthislightweightapproach,LifeHDcontinuallyrecordstempo-
isthesetofclusterHVsstoredintheworkingmemory.Wegauge ralclusterHVsfromtheenvironment,whilethemostprominent
the"radius"ofeachclusterbytrackingtwoscalarsforeachcluster clusterHVsaretransferredtolong-termmemory.Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
computationallatency.Both𝑔 𝑢𝑏and𝑓 𝑚𝑒𝑟𝑔𝑒areanalyzedinSec.7.8,
alongwithotherkeyhyperparametersinLifeHD.
TimeComplexityofMerging.Apotentiallylimitingissue
withspectralclusteringisitstimecomplexity,whichis,intheworst
case𝑂(𝐿3).However,thisisnotaconcerninoursetting.First,the
numberofclusterHVsinlong-termmemory(𝐿)istypicallysmall,
Figure5:AnintuitivevisualizationofclusterHVmerging. around50inpractice,resultinginmodestworst-casecomplexity.
Secondly,worst-caseanalysisisoverlypessimistic,assumingafull
5.4 ClusterHVMerging eigendecompositionofthegraphLaplacian(𝑊).Inpractice,𝑊 is
nearlyalwaysapproximatelylowrank,meaningthatonlythefirst
ClusterHVmerge(○3 inFig.4)hasthedualbenefitofreducing
𝑘 ≪ 𝐿 eigenvectorsareneeded.Insuchcases,fastrandomized
memoryuseandofelucidatingunderlyingsimilaritystructurein
eigendecompositionalgorithmscanreducethetimecomplexity
thedata.Intuitively,agroupofclusterHVscanbemergedifthey
tolinearin𝐿 [17].Thus,whilespectralclusteringissometimes
aresimilartoeachotheranddissimilarfromotherclusterHVs.
colloquiallythoughtofasan“expensive”procedure,thisistrueonly
Forinstance,onemightmergetheclusterHVsforBulldogand
inveryunfavorable“worst-case”settings.Inpractice,itscomplexity
Chihuahuaintoasingle“Dog”clusterHV,thatremainsdistinct
ismodestandacceptableforoursituation,asshowninSec.7.5.
fromtheclusterHVfor“TabbyCat”.
TomergetheclusterHVs,wefirstconstructasimilaritygraph
6 VARIANTSOFLIFEHD
defined over the cluster HVs from the long-term memory. The
cluster HVscorrespond to nodes, and a pairof cluster HVs are WhileLifeHDisdesignedtocatertogeneralIoTapplicationswith
connectedbyanedgeiftheyaresufficientlysimilar.Wethenmerge streaminginputandwithoutsupervision,real-worldscenariosmay
theclusterHVsbycomputingaparticulartypeofcutinthegraph vary.Somescenariosmighthaveafewlabeledsamplesinaddition
inamannersimilartospectralclustering[40].Thisgraphbased totheunlabeledstream,whileothersmayrequireoperationwithin
formalismforclusteringisabletocapturecomplextypesofcluster strictpowerconstraints.LifeHDoffersextensibilitytoaddressthese
geometryandoftensubstantiallyoutperformssimplerapproaches diverseneeds.Inthissection,weintroducetwosoftware-based
likeK-Means[59].WedetailthestepsofclusterHVmergingin extensions:LifeHDsemi,whichaddsaseparateprocessingpathto
LifeHDbelow,whileFig.5offersanillustrativeoverview. managelabeledsamples,andLifeHDa,whichadaptivelyprunes
theHDCmodelusingmaskingtohandlelow-powerscenarios.
Step1:Preprocessing.Giventhesetoflong-termmemoryclus-
terHVsL={𝑙 1,...,𝑙 𝐿},weconstructagraphGusingtheadjacency 6.1 LifeHD
semi
matrix𝐴∈{0,1}𝐿×𝐿 .Here,𝐴 𝑖𝑗 =𝐴 𝑗𝑖 =1[cos(𝑙 𝑖,𝑙 𝑗) ≥𝛽],with𝛽 WhileLifeHDexcelsinunsupervisedscenarios,itdoesnothar-
asanadaptivethreshold.Inotherwords,anedgeconnectscluster
nesslabeleddatawhenavailable.Toaddressthislimitation,we
HVs𝑙
𝑖
and𝑙
𝑗
iftheirsimilarityinHD-spacesurpasses𝛽.Alarger𝛽
introduceLifeHDsemiasanextensiontoenhanceaccuracyutilizing
impliesthatclusterHVsmustbemoresimilartobeconsideredfor
merging.Inpractice,weset𝛽 = 𝑀1 (cid:205) 𝑖𝑀 =1𝜇 𝑖,representingthemean t Fh oe rl eim aci hted inl pa ube tl bs. aI tn chFi 𝑖g 𝑑. 𝑥6 ,, ww eep coro nv si id de eran two ove sr uv bie sw etso :f oL nif eeH laD bese lm edi.
oftheobservedclusterHVs. (𝑋 𝑙,𝑖𝑑𝑥,𝑦 𝑙,𝑖𝑑𝑥)andoneunlabeled𝑋 𝑢𝑙,𝑖𝑑𝑥.Wedenotetheaveragela-
whS et re ep 𝐷2: isD te hc eom diap go os nit ai lo mn. aW tre ixc io nm wpu ht ie chth 𝐷e 𝑖L 𝑖a =pla (cid:205)ci 𝑗a 𝐴n 𝑖𝑊 𝑗.W= e𝐷 t− he𝐴 n, belingratiothroughoutthedatastreamas𝑟 = (cid:205) 𝑖𝑑𝑥(cid:205) |𝑋𝑖𝑑 𝑙,𝑥 𝑖𝑑| 𝑥𝑋 |𝑙 +,𝑖 |𝑑 𝑋𝑥 𝑢| 𝑙,𝑖𝑑𝑥|.
computetheeigenvalues𝜆 1,..,𝜆 𝐿,sortedinincreasingorder,and Sinceobtainingexternalsupervisionisoftenchallengingindy-
eigenvectors𝜈 1,...,𝜈 𝐿of𝑊.
namicenvironments,wefocusoncaseswhere𝑟 ≤0.01.
Step3:Grouping.Weinfer𝑘 =max𝑖∈[𝐿]𝜆 𝑖 ≤𝑔 𝑢𝑏,andmerge LifeHDsemi retains the two-tier memory structure of LifeHD
theclusterHVsbyrunningK-Meanson𝜈 1,...,𝜈 𝑘.Theupperbound butintroducesmodificationstotheworkingmemorycomponents.
𝑔 𝑢𝑏 isahyperparameterthatadjuststhegranularityofmerging, IntheLifeHDsemipipeline,theworkingmemoryundergoesthree
withasmaller𝑔
𝑢𝑏
leadingtosmaller𝑘thusencouragingmerging keysteps.Firstly,labeledsamples(𝑋 𝑙,𝑦 𝑙)updatelabeledclasshy-
pervectorsfollowingtheconventionalHDCmethodsoutlinedin
moreaggressively.
Sec.3.Next,weprocessunlabeledsamples𝑋 𝑢𝑙 throughnovelty
Ourmergingapproachisformallygrounded,asdiscussedin
detectionandHVupdatemodules,mirroringLifeHD.Importantly,
[59].Itisawell-knownfactthattheeigenvectorsof𝑊 encode
informationabouttheconnectedcomponentsofG.WhenGhas
inLifeHDsemi,theseoperationsareappliedtobothlabeledHVsand
𝑘 connectedcomponents,theeigenvalues𝜆 1 =𝜆 2 = ... =𝜆 𝑘 =0.
Torecoverthesecomponents,K-Meansclusteringon𝜈 1,...,𝜈 𝑘 can
beemployed,asexplainedin[59].However,practicalscenarios
mayhaveafewinter-componentedgesthatshouldideallybedis-
tinct.Forinstance,whenthesimilaritythresholdisimprecisely
set,erroneousedgesmayappearinthegraph,causing𝜆 1,...,𝜆 𝑘 to
beonlyapproximatelyzero.Ourmergingapproachisdesignedto
handlethissituationbyintroducing𝑔 𝑢𝑏.TheclusterHVmerging
isevaluatedevery𝑓 𝑚𝑒𝑟𝑔𝑒 batches,where𝑓 𝑚𝑒𝑟𝑔𝑒 isahyperparame- Figure6:AnoverviewofLifeHDsemiwhichisdesignedtohandle
terthatcontrolsthetrade-offbetweenmergingperformanceand scarcelabeledsamples.LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
7 EVALUATIONOFLIFEHD
7.1 SystemImplementation
WeimplementLifeHDwithPythonandPyTorch[43]anddeployit
onthreestandardedgeplatforms:RaspberryPi(RPi)Zero2W[3],
Raspberry Pi 4B [2], and Jetson TX2 module [1]. The selection
Figure7:Left:AnexampleoftheimpactofmaskingonHDC.Teston
ofedgeplatformsrepresentthreetierswithsmall,mediumand
CIFAR-10[29].Right:TheintuitionsbehindthedesignofLifeHDa.
abundantresources.
RPiZero2Whasa1GHzquad-coreCortex-A53CPUand512MB
clusterHVs.Lastly,weintroduceamergingsteptogrouplabeled SDRAM.RPi4Benjoysa1.8GHzquad-coreCortex-A72CPUand
HVsandclusterHVsthatarecloselyrelated.Tohandlelabeled 4GBSDRAM.TheJetsonTXplatformisequippedwithadual-core
HVs, we modify the adjacency matrix𝐴 by making it diagonal NVIDIADenver2CPU,aquad-coreARMCortex-A57MPCore,an
forlabeledentries.Forexample,ifthefirst𝐽 HVscorrespondto NVIDIAPascafamilyGPUwith256NVIDIACUDAcores,and8GB
labeledHVs,weensurethat𝐴 1:𝐽,1:𝐽 = diag([1,...,1]),whilecal- RAM.Wemeasurethetraininglatencyperbatchandtheenergy
culatingtheremainingvaluesfollowingLifeHDprocedures.This consumptionusingtheHioki3334powermeter[19].
strategypreventsthemergingoflabeledHVswitheachother.With WeareawarethatallNN-basedfeatureextractorscanbepruned
theseadjustments,LifeHDsemioffersasolutionthatretainsthecore andquantizedtoattainmoreefficientdeploymentonedgeplat-
elementsofLifeHDwhilehandlingscarcelabeledinputs. forms[11],samefortheNN-basedbaselineswecompareto[13,14].
However,NNmodelcompressionisnottheprimaryfocusofLifeHD.
6.2 LifeHD Existingcompressiontechniques[26,39]canbeapplieddirectlyto
a
thefeatureextractorinLifeHD.WeleaveLifeHDwithacceleration
While HDC computation is typically lightweight, there may be
designandemerginghardwaredeploymentforfutureworks.
instancesofenergyscarcity(e.g.,whenpoweredbyasolarpanel)
thatcallforabalancebetweenaccuracyandpowerefficiency.Sim-
7.2 ExperimentalSetup
ilartoneuralnetworks,oneapproachistoprunetheHDCmodel
usingamask,retainingthemostcrucialHDCdimensionspost- WeconductcomprehensiveexperimentstoevaluateLifeHDon
encoding[25].Dimensionimportancecanbedeterminedbyag- threetypicaledgescenarios.Allthreescenariosincorporatecon-
gregatingallclasshypervectorsintooneandsortingthevalues tinuousdatastreamsandexpectlifelonglearningovertime.We
acrossalldimensions.Notably,directreductionoftheencoding summarizetheexperimentalsetupinTable2.
dimensionshouldbeavoided,asitcandegradeHDC’sexpressive Application#1:PersonalHealthMonitoring.Continuous
capabilityandendupwithcorruption.Fig.7(left)visuallydemon- healthmonitoringhasemergedasapopularusecaseforIoT.We
stratestheimpactofmaskinginsupervisedHDCtasks:retaining utilizetheMHEALTH[6]datasetwhichincludesmeasurements
thetop6000bitsincursonlya3%accuracylosscomparedtousing ofacceleration,rateofturn,andmagneticfieldorientationona
thefull10K-bitprecision. smartwatch.MHEALTHdifferentiates12activitiesindailylives
WhiletheconceptofmaskinghasbeenemployedinpriorHDC andiscollectedfrom10subjects.Notably,MHEALTHemploysraw
studies[25],theyarenotdirectlyapplicabletoLifeHDduetotheir time-seriessignalsratherthanprocessedfrequencycomponentsas
offlinetrainingsettingwithiiddata.Withstreamingnon-iiddata inputs.Weusetimewindowsof2.56s(𝑇 =128)with75%overlapto
inLifeHD,thesetofobservedclusterHVsmayonlyrepresenta generatethesamples.Incontrasttopreviousdatasets,westrictly
subsetofthepotentialclasses,andthelesssignificantbitscould adheretothetemporalorderduringdatacollection.
becomecrucialasnewclassesareintroduced. Application#2:SoundCharacterization.Continuoussound
WeintroduceLifeHDa,whichenhancesLifeHDthroughanadap- detectioncontributestothecharacterizationofurbanenvironments.
tivemaskingapproachappliedtoallclusterHVsinworkingand WechoosetheESC-50[44]datasettoemulatethisscenario.This
long-termmemory.Let𝐷 𝑎representthetargetdimensionforre- datasetcomprises5-second-longrecordingscategorizedinto50
duction.TherationalebehindLifeHDaisdepictedinFig.7(right). semantically diverse classes, including animals, human sounds,
WheneveranoriginalLifeHDdetectsnovelty,wetemporarilyrevert andurbannoises.Weconstructtheclass-incrementalstreamsby
tothefulldimension𝐷for2batches,whichissufficientforLifeHD arrangingthedatainrandomorderwithineachclass.
toconsolidatenewpatternsinitsmemory.Afterthesetwobatches, Application#3:ObjectRecognition.Objectrecognitionisa
weassessthelong-termmemoryclusterHVsbyaggregatingthem commonusecaseforcamera-mountedmobilesystems,e.g.,self-
andrankingthedimensions,andderiveamaskretaining𝐷 𝑎dimen- drivingvehicles.Wesetupaclass-incrementalstreamfromCIFAR-
sionswiththelargestabsolutevalues.Thismaskisthenappliedto 100[29],consistingof32×32RGBimagesof20coarseclasses.We
thefollowingbatchesof𝜙(𝑋)immediatelyafterencoding,upuntil furtherevaluatethecaseofdatadistributiondriftbyexamining
thenextnoveltyisdetected.Noveltydetectionisexecutedwiththe gradualrotationsoccurringwithineachCIFAR-100class.
maskedhypervectors.Importantly,LifeHDacanutilizethesame On MHEALTH, LifeHD is fully dependent on the HDC spa-
noveltydetectionsensitivityasLifeHDsincethemostsignificant tiotemporal encoder to process the raw time-series signals. For
dimensionsdominatethesimilaritycheck.Inotherwords,thesim- ESC-50andCIFAR-100,LifeHDutilizestheHDnnframeworkwith
ilarityresultsinLifeHDausing𝐷 𝑎dimensionaresimilarasusing apretrainedfeatureextractorbeforeHDCencoding,sameasinthe
thefulldimension.LifeHDaoffersanadaptiveHDCmodelpruning state-of-the-artHDCworks[18,52].Specifically,weadaptapre-
interfacewithminimalaccuracylossandoverhead. trainedACDNetwithquantifiedweights[37]forESC-50.ACDNetUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
Table2:ExperimentalsetupofLifeHDacrossalldatasets.
Dataset Application Classes Total TrainingDataOrder HDnn? Pretrained #of
Category (Balanced?) Samples Models Params
MHEALTH[6] Activity 12(N) 9K Temporalorderduringcollection N - -
ESC-50[44] Sound 50(Y) 2K Class-incremental,randomwithinclass Y ACDNet[37] 4.7M
Class-incremental,randomwithinclass MobileNetV2[51]or 2.2M
CIFAR-100[29] Image 20(Y) 60K Y
orgradualrotationwithinclass MobileNetV3small[20] 927K
Table3:ImportanthyperparametersconfigurationofLifeHD.
Dataset HDCEncoding LifeHDDesign
𝐷 𝑄 𝑃 𝑏𝑆𝑖𝑧𝑒 𝑀 𝛾 𝑔
𝑢𝑏
𝑓𝑚𝑒𝑟𝑔𝑒
MHEALTH 1000 5 0.01 32 50 3.0 0.2 25
ESC-50 10000 100 0.02 32 100 1.0 0.1 5
CIFAR-100 10000 100 0.01 32 100 1.0 0.1 150
isacompactconvolutionalneuralnetworkarchitecturedesigned
forsmallembeddeddevices.ForCIFAR-100,weuseaMobileNet
V2[51]foraccuracyevaluationandMobileNetV3small[20]for
efficiencyevaluation,bothpretrainedonImageNet[48].Forall
pretrainedNNs,weremovethelastfullyconnectedlayerusedfor Figure8:Agraphicalexplanationofthepipelinesetup.Greenout-
classificationandkeeptheremainingweightsfrozen. linesdenotethemoduletrainedwiththestreamingdata.Blueout-
Table3summarizesthekeyhyperparametersinLifeHD,which linesdenotetheunsupervisedclassifiertrainedduringtesting.Gray
areselectedbasedonaseparatevalidationset.Weconfigure𝛼 = denoteswhenthemoduleisfrozen&nottrainedfurther.
0.1formoving-averageupdate,ℎ𝑖𝑡 𝑡ℎ =10forlong-termmemory
consolidation.Thelong-termmemorysize𝐿issetto50inallcases. However,sincewedonotassumeawarenessoftaskshifts,
wesimplyfreezethemodelfromthepreviousbatch.
7.3 State-of-the-ArtBaselines • LUMP[13]employsamemorybufferforreplayandmit-
igatescatastrophicforgettingbyinterpolatingthecurrent
We conduct a comprehensive comparison between LifeHD and
batchwithpreviouslystoredsamplesinthememory.
state-of-the-artNN-basedunsupervisedlifelonglearningbaselines,
• STAM[54]isbrain-inspiredexpandablememoryarchitec-
whichcontinuouslytrainaNNforrepresentationlearning.Theloss
tureusingonlineclusteringandnoveltydetection.Weex-
functionsinthesesetupsaredefinedinthefeaturespacewithout
clusivelyapplySTAMtoCIFAR-100duetoitsdemandfor
relyingonlabelsupervision.Duringtesting,wefreezetheneural
intricatedataset-specifictuning(e.g.,numberofreceptive
networkandapplyK-Meansclusteringonthetestingfeatureem-
beddingstogeneratepredictedlabels.𝑘 issetto50whichisthe fields),andbecausetheauthorsonlyreleasedtheimplemen-
tationfortheCIFARdatasets.
samenumberofclusterHVsasinLifeHD.Suchapipelineiswidely
• SupHDC[18,27]isthefullysupervisedHDCpipeline.
usedforlifelonglearningevaluations[46,54].
Fig.8presentsacomparisonofthepipelinesetupusingboth Allbaselinesareadaptedfromtheiroriginalopen-sourcecode.
thebaselinesandLifeHDonHDnnandnon-HDnnframeworks ForCaSSLeandLUMP,weemployBYOL[16]astheself-supervised
respectively.Toensurefaircomparisons,inHDnnframeworkon lossfunctionbecauseithasshowedsuperbempiricalperformance
ESC-50andCIFAR-100,weinitializetheNNwiththesamepre- inlifelonglearningtaskscomparedtootherself-supervisedlearning
trainedweightsforLifeHDandNNbaselines.FortheNNbaselines backbones[14].Weusethememorybuffersizeof256forLUMP
onMHEALTH,werandomlyinitializeaone-layerLSTMof64units whichisthesameasintheoriginalpaper.WeemploytheStochastic
followed by a fully connected layer of 512 units. This architec- GradientDescent(SGD)optimizerwithalearningrateof0.03across
turehasachievedcompetitiveaccuracyastheTransformers-based allmethods,trainingeachbatchfor10steps.Allexperimentsare
designsonMHEALTH[12]. executedfor3randomtrials.
WecompareLifeHDwiththefollowingbaselines,whichinclude
allmainlifelonglearningtechniques: 7.4 LifeHDAccuracy
• FinetuneisanaïvebaselinethatoptimizestheNNmodel ResultsonThreeApplicationScenarios.Fig.9(a)detailsthe
usingthecurrentbatchofdatawithoutanylifelonglearning ACCcurveofallmethodsasstreamingsamplesarereceived.All
techniques. NNbaselinesstartathigheraccuracy,especiallyinESC-50(sounds)
• CaSSLe[14]isadistillation-basedframeworkthatutilizes andCIFAR-100(images),owingtothepresenceofapretrainedNN
self-supervisedlosses.Itleveragesdistillationbetweenthe featureextractorwithintheHDnnframework.Meanwhile,LifeHD
representationsofthecurrentmodelandapastmodel.In beginswithloweraccuracyasboththeworkingandlong-term
theoriginalpaper,thepastmodeliscapturedattheendof memoriesareempty,needingtolearntheclusterHVsandtheop-
theprevioustaskandpriortotheintroductionofanewtask. timalnumberofclusters.Notably,asstreamingsamplescomein,LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(a)TheACCcurveofLifeHDvs.state-of-the-artNNbaselinesonthreescenarios. (b)ThefinalresultsofLifeHDinCIFAR-100withrotation.
Figure9:Theunsupervisedclusteringaccuracy(ACC)resultsofLifeHDonvariousinputdatastreams.
Table4:ThegapofACCsattheendofthestreambetweenLifeHD
andSupervisedHDC[18,27].
Method MHEALTH ESC-50 CIFAR-100
LifeHD 0.75 0.92 0.20
SupervisedHDC[18,27] 0.90 0.95 0.26
Gap -0.15 -0.03 -0.06
allNNbaselinesexperienceadeclineinACC,underscoringthein-
herentchallengesofunsupervisedlifelonglearningwithstreaming
non-iiddataandalackofsupervision.Thisisprimarilyduetothe
demandforextensiveiiddataandmulti-epochofflinetrainingfor
finetuningNNs,whichisnotfeasibleinoursetting.CaSSLe[14]
leadstoforgettingduetoitsinabilitytoidentifysuitablepastmod-
elsfromwhichtodistillknowledge.Similarly,LUMP[13]exhibits
Figure10:TheconfusionmatrixofLifeHDonMHEALTH.The
greenboxhighlightssmallerclusterHVsthatformasinglelarge
reducedACCinESC-50andCIFAR-100,withonlymarginalACC
groundtruthclass,whichisavalidlearningoutcome.Theredbox
improvementinMHEALTH(timeseries),suggestingthatitsmem-
highlights"boundary"clusterHVsthatspanmultipletruelabels,
oryinterpolationstrategymaynotbeuniversallysuitableforall
leadingtolowerACCs.
applications.Whilethememory-baseddesignofSTAM[54]can
mitigateforgetting,itsefficacyindistinguishingpatternsandac- loss.Inourexperiments,LifeHDdemonstratesminimalACCloss
quiringnewknowledgeremainsunsatisfactory.Onthecontrary, evenundersubstantialrotationshifts.
LifeHDdemonstratesincrementalaccuracyacrossallthreediffer- ComparisonwithFullySupervisedHDC.Table4compares
entscenarios,achievingupto9.4%,74.8%and11.8%accuracy theaverageACCsofsupervisedHDCmethod[18,27]andLifeHD.
increaseonMHEALTH(timeseries),ESC-50(sound)andCIFAR- Even without any supervision, LifeHD approaches the ACC of
100(images),comparedwiththeNN-basedunsupervisedlifelong supervisedHDCwithagapof15%,3%and6%onMHEALTH,
learningbaselinesattheend.Suchoutcomecanbeattributedto ESC-50andCIFAR-100.AminimalACCgapconfirmstheeffec-
HDC’slightweightbutmeaningfulencodingandtheeffectivemem- tivenessofLifeHDinseparatingandmemorizingkeypatterns.To
orizationdesignofLifeHD. helpexplainthesmallACClossevenwithoutsupervision,wevi-
ResultsunderDataDistributionDrift.Wefurtherevaluate sualizetheconfusionmatrixofLifeHDonMHEALTHinFig.10.
LifeHD’s performance under drifted data and present the final MHEALTHhas12trueclasses(yaxis),whereasLifeHDmaintains
ACC along with the number of discarded cluster HVs in Fig. 9 23clusterHVsinitslong-termmemory(xaxis).ACCisevaluated
(b).Specifically,weintroducegradualrotationtotheCIFAR-100 bymappingtheunsupervisedclusterHVstotruelabels.Although
sampleswithineachclass,rangingfromnorotationtoasubstantial LifeHDcannotachievepreciselabelmatchingwithtrueclasses,it
rotation angle of 80◦. The other parameter settings remain the canpreservetheessentialpatternsbyusingfiner-grainedclusters.
sameasinTable3.ThenumberofdiscardedclusterHVsaccounts Forexample,thegreenboxinFig.10highlightsavalidlearning
forthosethatareeitherforgottenormerged.FromFig.9(b),we outcome,whereLifeHDusespredictedclusterHVNo.0,9and23
canobservetheremarkableresilienceofLifeHDtodrifteddata, torepresentabiggertrueclassof“Lyingdown”.
withanACClossoflessthan2.3%evenunderasevererotationof
80◦.Thisrobustnessstemsfromthegeneralanduniformdesignof 7.5 TrainingLatencyandEnergy
LifeHDtoaccommodatevarioustypesofcontinuouslychanging Fig.11providescomprehensivelatencyandenergyconsumption
datastreams.Incasesofslightorminimaldistributiondrift,LifeHD resultstotrainonebatchofsamplesonallthreeedgeplatforms.
updatesexistingclusterHVs;ininstancesofseveredrift,newcluster ForCIFAR-100,weusethemostlightweightMobileNetversion,
HVsarecreatedandsubsequentlymergedifdeemedappropriate. V3small[20],asHDnnfeatureextractorandNNbaseline,toas-
However,duetothefinitememorycapacities,moreclusterHVsare sess LifeHD’s efficiency gain over the most competitive mobile
subjecttoforgettingormergingunderlargerdrifts,asshownin computingsetup.OnRPiZero,wereportresultsfortherelatively
Fig.9(b)bythenumberofdiscardedclusterHVs,leadingtoACC lightweightNN-basedbaselines,FinetuneandLUMP[13],usingUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
(a)LatencyonRPiZero (b)LatencyonRPi4B (c)LatencyonJetsonTX2
(d)EnergyonRPiZero (e)EnergyonRPi4B (f)EnergyonJetsonTX2
Figure11:LatencyandenergyconsumptiontotrainonebatchofdatausingLifeHDandallbaselinesonoff-the-shelfedgeplatforms.
themethodsintoNNtraining(Finetune,LUMP[13],CaSSLe[14],
STAM[54])andHDCtraining(SupervisedHDC[18,27]andour
LifeHD).Following[30],wecalculatethepeakmemoryofNNtrain-
ingasthesumofmodel,optimizerandactivationmemories,plus
additionalmemoryconsumptionforlifelonglearning.Specifically,
Figure12:PeakmemoryfootprintofallmethodsonMHEALTH CaSSLe[14]requiresadditionalmemoryfortrainingapredictor
(left)andCIFAR-100(right)withbatchsizeof1.Theresultsare andinferencefromafrozenmodel,LUMP[13]needsextramemory
representativefortimeseriesdataandimagedata. forreplay.ForHDC-basedmethods,eachdimensionofthecluster
HVisrepresentedasasignedintegerandstoredinabyte.Inad-
thesmallestdataset,MHEALTH,whilerunningCaSSLe[14]on ditiontotheworkingandlong-termmemories,wealsoconsider
MHEALTHwouldresultinout-of-memoryerrors.Asshownin thestorageofbipolarlevelandIDhypervectorsforencoding,and
Fig.11,LifeHDisupto23.7x,36.5xand22.1xfastertotrainon thefrozenMobileNetforHDnnencodinginCIFAR-100.Notice
RPiZero,RPi4BandJetsonTX2,respectively,whilebeingupto thatourfocushereisoncomparingfull-precisionmemoryusage,
22.5x,34.3xand20.8xmoreenergyefficientoneach,compared andoptimizationtechniqueslikequantizationcanbeappliedtoall
totheNN-basedunsupervisedlifelonglearningbaselines.Inmost methodsinthefuture.
settings,CaSSLe[14]isthemosttime-consumingbecauseofthe The results in Fig. 12 highlight LifeHD’s memory efficiency.
expensivedistillation.LUMP[13]isslightlymoreexpensivethan LifeHDconserves80.1%-86.2%and84.1%-96.0%ofmemorycom-
Finetuneduetoitsreplaymechanism.STAM[54],implemented paredtoNNtrainingbaselinesonMHEALTH(non-HDnn)and
onlyonCPU,incursthelongesttraininglatencyonJetsonTX2,asit CIFAR-100(HDnn),respectively.Thisremarkableefficiencystems
doesnotuseGPU’saccelerationcapabilities.LifeHDisclearlyfaster fromLifeHD’sHDCdesign,whichdispenseswiththememory-
andmoreefficientthanallNN-basedunsupervisedlifelonglearning intensivegradientdescentsinNNs.STAM[54],withitshierarchical
baselines[13,14,54]duetoLifeHD’slightweightnature.Theover- andexpandablememorystructure,consumes6.3xthememoryof
headofLifeHDalongsidefullysupervisedHDC,SupHDC[18,27], LifeHD,asitstoresrawimagepatchesacrossallhierarchies.Com-
isnegligibleonmorepowerfulplatformslikeRPi4BandJetsonTX2. paredtofullysupervisedHDC,SupHDC[18,27],LifeHDintroduces
Notably,inLifeHD,theclusterHVmergingstepforprocessing amodestmemoryincreasetoaccomplishthechallengingtaskof
about40LTMelementstakes7.4,0.86and0.66secondstorunon organizinglabel-freeclusterHVs.LifeHDprovesadvantageousfor
RPiZero,RPi4BandJetsonTX2,respectively,whichonlyexecutes edgeapplicationswithonly103KBand2.5MBofpeakmemory
onceevery𝑓 𝑚𝑒𝑟𝑔𝑒 batches.Furtherenhancementscanbeachieved requiredforMHEALTHandCIFAR-100.
usingtheaccelerationtechniquesmentionedinSec.5.4.
Fig.11indicatesLifeHDimproveslatencyandenergyefficiency
themostonRPi4B,ascomparedtoRPiZeroandJetsonTX2that 7.7 AblationStudies
representmorelimitedorpowerfuldevices.Thisisbecausethehigh- ThedesignofLifeHDconsistsofseveralkeyelements:thetwo-tier
dimensionalnatureofLifeHDrequiresafairamountofmemory, memoryorganization,noveltydetectionandonlineupdate,and
thus it cannot run efficiently on the highly restricted RPi Zero. clusterHVmergingthatmanipulatespastpatterns.Weconduct
TheGPUresourcesonJetsonTX2boosttheNN-basedbaselines, experimentstoassessthecontributionofeachelement.Usingthe
narrowingthegapbetweenthemandLifeHD.Weexpectmuch configurationinTable3,weevaluatetheperformanceof(i)LifeHD
largerefficiencyimprovementswhenLifeHDisacceleratedusing withoutlong-termmemory,usingonlyasinglelayermemory,(ii)
emergingin-memorycomputinghardware[11,65]. LifeHDwithoutmerging,employingonlynoveltydetection,online
updateandforgetting,and(iii)completeLifeHD.Wepresentthe
7.6 MemoryUsage
ACCandthenumberofclusterHVsinLTMduringMHEALTH
Fig.12providesacomprehensivesummaryofpeakmemoryfoot- training in Fig. 13, chosen as a representative scenario. LifeHD
printforallmethodsonMHEALTHandCIFAR-100.Wecategorize withoutLTM(greendashdotline)forcesclusterHVmergingtoLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(a)Workingmemorysize (b)Noveltythreshold (c)Mergingsensitivity
Figure13:AblationstudyofLifeHDonMHEALTH.Thenumberof
clusterHVsreportedforLifeHDwithoutlong-termmemory(LTM)
isfortheworkingmemory,sincenoLTMisallowed.
(d)Mergefrequency (e)Encodinglevel (f)Encodingflippingratio
takeplaceinworkingmemory,wherethelargenumberoftem- Figure14:SensitivityofvarioushyperparametersinLifeHD,using
poraryclusterHVscreateslessimportantnodesinthegraphand MHEALTHdatasetasarepresentative.
corruptsthegraph-basedmergingprocess,asshowninFig.13(left).
Thisnecessitatesthedesignofthetwo-tiermemoryarchitecture
andmergingwithLTMelements.LifeHDwithoutmerging(blue
similarityinHD-spaceafterencoding.Optimal𝑄dependsonthe
dashedline)consumes1xmorememoryintheLTM,makingit sensorsensitivity,withfiner-grainedsensorsrequiringmorequanti-
unsuitableforresource-constrainededgedevices.Ourdesignof
zationlevels.𝑃determinesthesimilaritybetweenadjacentlevelsof
LifeHD(redsolidline)strategicallycombinessimilarclusterHVs hypervectors.Forpersonalhealthmonitoring,suchasMHEALTH,
withminorlossontheclusteringquality,achievingACCsimilarto
𝑄 =10,𝑃 =0.01usuallygivesthebestresults.
thosewithoutmergingwhileconservingmemorystorage.
8 EVALUATIONOFLIFEHD ANDLIFEHD
semi a
7.8 SensitivityAnalysis
Inthissection,wecompareLifeHDsemiandLifeHDa,ourproposed
Fig. 14 summarizes the sensivitity results of key parameters in extensionsfromLifeHD,withexistingdesignsthataresimilar.
LifeHD,whilethelesssensitiveonessuchas𝛼andℎ𝑖𝑡 𝑡ℎareomitted PerformanceofLifeHDsemi.ToevaluateLifeHDsemiinalow-
duetospacelimitation.ThedefaultsettingisthesameasinTable3. labelscenario,wecompareitwithSemiHD[22],whichisthestate-
WorkingMemorySize.Fig.14(a)showsACCsusingworking of-the-artHDCmethodforsemi-supervisedlearning.Weadapt
memorysizesof20,50,100and200.Ingeneral,alargerworking SemiHD[22]forsingle-passsettings,introducingapseudolabel
memoryallowsmoretemporaryclusterHVsatthecostofhigher assignmentthreshold.Whenthecosinesimilarityofanunlabeled
memoryconsumption.𝑀 = 100producesoptimalresults,while sampletothenearestclasshypervectorsurpassesthethreshold,we
furtherincreasingthememorysizereducesclusteringquality.This assignthatclassasitspseudolabel.Thesampleisthenemployedto
occursbecauseexcessivelylargeworkingmemoryretainsoutdated updatetheclasshypervectorinSemiHD.Weexplorevariousthresh-
prototypes,degradinglifelonglearningperformance. oldvaluesandchoosetheoptimalresultforcomparison.Fig.15
NoveltyThreshold.InFig.14(b),wepresentthefinalACCs (a)comparesLifeHDsemiandSemiHD[22]onESC-50andCIFAR-
fordifferentnoveltydetectionthresholds(𝛾).Alower𝛾 resultsin 100 across various labeling ratios𝑟 < 0.01. The advantages of
morefrequentnoveltydetectionsandincreasedloadsonthework- LifeHDsemiaremostprominentwhenlabelsarelimited,theweakly
ingmemory,whileahigher𝛾 mayleadtooverlookingsignificant supervisedscenarioisLifeHDsemi’smajorfocus.LifeHDsemi im-
changes.Remarkably,LifeHDdemonstratesresiliencetovariations provesACCbyupto10.25%and3.6%onESC-50andCIFAR-100
in𝛾,aphenomenonthatweattributetothecombinedimpactof respectively.Thisoutcomearisesfromtheunsupervisednature
noveltydetectionandmergingprocesses. ofLifeHD,allowingittoautonomouslyorganizeprominentclus-
MergingSensitivity.Fig.14(c)showsACCusingvariousmerg- terHVs,especiallywhenallsamplesfromaclasslacklabels.As
ingthresholds(𝑔 𝑢𝑏).𝑔 𝑢𝑏 determinesthenumberofclusters(𝑘)to thelabelingratioincreases,LifeHDsemi’sadvantageoverSemiHD
mergeintheclusterHVmergingstep(Sec.5.4).Alowvaluefor diminishes,becausemorelabelsbolsterSemiHD’sperformance.
𝑔 𝑢𝑏 resultsinoverlyaggressivemerging,leadingtothefusionof PerformanceofLifeHDa.LifeHDa providesaninterfaceto
dissimilarclusterHVsandadegradedACC.Alarger𝑔 𝑢𝑏 adoptsa trademinimalperformancelossforefficiencygains,byadaptively
conservativemergingstrategyandencouragesfiner-grainedclus- pruningouttheinsignificantdimensions.WecompareLifeHDa
ters,albeitattheexpenseofincreasedresourcedemands. with previous HDC works employing a fixed mask throughout
MergingFrequency.Fig.14(d)showsthefinalACCsfordif- training[25],andtheresultsarepresentedinFig.15(b)forCIFAR-
ferentmergingfrequencies(𝑓 𝑏𝑎𝑡𝑐ℎ).LifeHDshowsitsrobustness 100,includingACCandtraininglatencyperbatchonRPi4B.Fixed
acrossvarious𝑓 𝑏𝑎𝑡𝑐ℎ values,partlyduetothepresenceof𝑔 𝑢𝑏 to masksnegativelyimpactHDClearning,especiallywithsmaller
preventaggressivemerging.Lessfrequentmerging(larger𝑓 𝑏𝑎𝑡𝑐ℎ) dimensions.Suchmasksfailtoadapttonewhypervectorsinclass-
raisestheriskofforgettingimportantpatternsasofmemorycon- incrementalstreams,wherelesssignificantdimensionsmaybecome
straints.Morefrequentmerging(smaller𝑓 𝑏𝑎𝑡𝑐ℎ)increasesthecom- cruciallaterintraining.LifeHDaaddressesthisissuebyadjusting
putationalburdenduetothespectralclustering-basedalgorithm. themaskuponnoveltydetection,leadingtoadegradationofonly
EncodingLevelandFlippingRatioforSpatiotemporalEn- 0.71%inACCand4.5xefficiencygaincomparedtothecomplete
coding.Fig.14(e)and(f)showtheACCsforvariousquantization LifeHD,usingonly20%ofthefullHDdimensionofLifeHD.The
encodinglevels(𝑄)andflippingratios(𝑃)duringthespatiotem- overheadofadaptivelyadjustingthemaskisnegligiblewhennov-
poralencoding.Bothparametersareimportantforpreservingthe eltydetectionoccursinfrequently.Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaŠimunićRosing
10 CONCLUSION
Theabilitytolearncontinuouslyandindefinitelyinthepresenceof
change,andwithoutaccesstosupervision,onaresource-constrained
deviceisacrucialtraitforfuturesensorsystems.Inthiswork,we
designanddeploythefirstend-to-endsystemnamedLifeHDto
learncontinuouslyfromreal-worlddatastreamswithoutlabels.
(a)GainsofLifeHDsemioverSemiHD[22]inlowerlabelingratios. OurapproachisbasedonHyperdimensionalComputing(HDC),
anemergingneurally-inspiredparadigmforlightweightedgecom-
puting.LifeHDisbuiltonatwo-tiermemoryhierarchyincluding
a working and a long-term memory, with collaborative compo-
nentsofnoveltydetection,onlineclusterHVupdateandcluster
HVmergingforoptimallifelonglearningperformance.Wefur-
therproposetwoextensionstoLifeHD,LifeHDsemiandLifeHDa,
(b)LifeHDavs.usingfixedmask(Fixed)[25]undervarious𝐷𝑎. to handle scarce labeled samples and power constraints. Practi-
Figure15:ResultsofLifeHDsemiandLifeHDacomparedtoexisting caldeploymentsontypicaledgeplatformsandthreeIoTscenarios
HDCtechniquesforsimilargoals. demonstrateLifeHD’simprovementofupto74.8%onunsupervised
clusteringaccuracyandupto34.3xonenergyefficiencycompared
tostate-of-the-artNN-basedunsupervisedlifelonglearningbase-
9 DISCUSSIONSANDFUTUREWORKS
lines[13,14,54].
ProblemScale.OnelimitationofLifeHDistherelativesmallprob-
lemscale(e.g.,theimagesizeofCIFAR-100isrestrictedto32x32)
ACKNOWLEDGMENTS
due to the essential difficulty of unsupervised lifelong learning
Theauthorswouldliketothanktheanonymousshepherd,review-
problem,includingsingle-passnon-iiddataandnosupervision.For
ers,andourcolleagueXiyuanZhangfortheirvaluablefeedback.
thesamereason,thereremainsadisparityinaccuracybetween
ThisworkwassupportedinpartbyNationalScienceFoundation
unsupervisedlifelonglearningandfullysupervisedNNs,assub-
underGrants#2003279,#1826967,#2100237,#2112167,#1911095,
stantiatedbypriorresearch[13,54].InordertoscaleLifeHDto
#2112665,andinpartbyPRISMandCoCoSys,centersinJUMP2.0,
morechallengingapplicationssuchasself-drivingvehicles,one
anSRCprogramsponsoredbyDARPA.
possibledirectionistoleveragethepretrainedfoundationmodelas
afrozenfeatureextractorintheHDnnframework,whichweleave
REFERENCES
forfutureinvestigation.
HyperparameterTuning.Whilewerecognizethathyperpa- [1] 2023.JetsonTX2Module.https://developer.nvidia.com/embedded/jetson-tx2.
[Online].
rameterscaninfluencetheperformanceofLifeHD,suchanissueis
[2] 2023.RaspberryPi4B.https://www.raspberrypi.com/products/raspberry-pi-4-
notexclusivetoLifeHD,buthaspersistentlybeenachallengein model-b/. [Online].
machinelearningresearch[7].InLifeHD,theimpactofhyperpa- [3] 2023.RaspberryPiZero2W.https://www.raspberrypi.com/products/raspberry-
pi-zero-2-w/. [Online].
rameterscanbemitigatedthroughpre-deploymentevaluationand
[4] AuroreAvarguès-Weberetal.2012. Simultaneousmasteringoftwoabstract
componentco-design.Forexample,encodingparameterssuchas conceptsbytheminiaturebrainofbees.ProceedingsoftheNationalAcademyof
𝑄,𝑃 canbetunedonsimilarhealthmonitoringdatasourcesprior Sciences109,19(2012),7481–7486.
[5] AlanBaddeley.1992.Workingmemory.Science255,5044(1992),556–559.
todeployment.Meanwhile,thecomponentofclusterHVsmerging [6] GarciaRafaelBanos,OrestiandAlejandroSaez.2014.MHEALTHDataset.UCI
canincreaseLifeHD’sresiliencytothenoveltydetectionthreshold MachineLearningRepository. DOI:https://doi.org/10.24432/C5TW22.
𝛾,asahigherquantityofnovelclusterscanbemergedinlaterstage [7] BerndBischletal.2023.Hyperparameteroptimization:Foundations,algorithms,
bestpractices,andopenchallenges.WileyInterdisciplinaryReviews:DataMining
oflearning. andKnowledgeDiscovery13,2(2023),e1484.
LimitationsofHDC.HDCservesasthefundamentalcoreof [8] TrentonBrickenetal.2023.SparseDistributedMemoryisaContinualLearner.
InInternationalConferenceonLearningRepresentations.
LifeHD.WhileHDCshowspromisewithitsnotablelightweight
[9] HanCaietal.2020. Tinytl:Reducememory,notparametersforefficienton-
design,itisburdenedbyseverallimitationsthatremainactiveareas devicelearning. AdvancesinNeuralInformationProcessingSystems33(2020),
ofresearch.First,forcomplexdatasetslikeaudioandimages,HDC 11285–11297.
[10] NingChenetal.2016.Smarturbansurveillanceusingfogcomputing.In2016
requiresapretrainedfeatureextractor(theHDnnencoding)which IEEE/ACMSymposiumonEdgeComputing(SEC).IEEE,95–96.
maynotexistforcertainapplications.Moreover,akintoanyother [11] ArpanDuttaetal.2022. Hdnn-pim:Efficientinmemorydesignofhyperdi-
mensionalcomputingwithfeatureextraction.InProceedingsoftheGreatLakes
architecture, HDC vectors face capacity limitations determined
SymposiumonVLSI2022.281–286.
bythedimensionofHDspace,encodingmethod,andpotential [12] EhabEssaandIslamRAbdelmaksoud.2023.Temporal-channelconvolutionwith
noiselevelsintheinputdata[56].Duetothesefactors,careful self-attentionnetworkforhumanactivityrecognitionusingwearablesensors.
Knowledge-BasedSystems278(2023),110867.
evaluationandsometimesmanualfeatureengineeringarerequired
[13] DivyamMadaanetal.2022. RepresentationalContinuityforUnsupervised
tosuccessfullydeployHDCfornewapplications. ContinualLearning.InInternationalConferenceonLearningRepresentations.
FutureWorks.AlthoughLifeHDfocusesonsingle-devicelife- [14] EnricoFinietal.2022.Self-supervisedmodelsarecontinuallearners.InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
longlearningforclassificationtasks,themethodcanbeextended [15] InGimandJeongGilKo.2022.Memory-efficientDNNtrainingonmobiledevices.
forothertypesoftasksandlearningsettings,suchasfederated InProceedingsofthe20thAnnualInternationalConferenceonMobileSystems,
ApplicationsandServices.464–476.
learningandreinforcementlearning.Weleavetheinvestigationof
[16] Jean-BastienGrilletal.2020. Bootstrapyourownlatent-anewapproachto
thesetopicsforfuturework. self-supervisedlearning.Advancesinneuralinformationprocessingsystems33LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(2020),21271–21284. [47] HaoyuRen,DarkoAnicic,andThomasARunkler.2021.Tinyol:Tinymlwith
[17] NathanHalko,Per-GunnarMartinsson,andJoelATropp.2011.Findingstructure online-learningonmicrocontrollers.In2021InternationalJointConferenceon
withrandomness:Probabilisticalgorithmsforconstructingapproximatematrix NeuralNetworks(IJCNN).IEEE,1–8.
decompositions.SIAMreview53,2(2011),217–288. [48] OlgaRussakovskyetal.2015.Imagenetlargescalevisualrecognitionchallenge.
[18] MichaelHerscheetal.2022.Constrainedfew-shotclass-incrementallearning.In Internationaljournalofcomputervision115(2015),211–252.
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. [49] Andrei A Rusu et al. 2016. Progressive neural networks. arXiv preprint
9057–9067. arXiv:1606.04671(2016).
[19] Hioki.2023.Hioki3334Powermeter.https://www.hioki.com/en/products/detail/ [50] SwapnilSayanSahaetal.2023.TinyNS:Platform-AwareNeurosymbolicAuto
?product_key=5812. TinyMachineLearning. ACMTransactionsonEmbeddedComputingSystems
[20] AndrewHowardetal.2019. Searchingformobilenetv3.InProceedingsofthe (2023).
IEEE/CVFInternationalConferenceonComputerVision.1314–1324. [51] MarkSandleretal.2018.Mobilenetv2:Invertedresidualsandlinearbottlenecks.
[21] MohsenImanietal.2019.Hdcluster:Anaccurateclusteringusingbrain-inspired InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
high-dimensionalcomputing.InDesign,Automation&TestinEuropeConference 4510–4520.
&Exhibition(DATE).IEEE,1591–1594. [52] YangShen,SanjoyDasgupta,andSaketNavlakha.2021.Algorithmicinsightson
[22] MohsenImanietal.2019. Semihd:Semi-supervisedlearningusinghyperdi- continuallearningfromfruitflies.arXivpreprintarXiv:2107.07617(2021).
mensionalcomputing.InIEEE/ACMInternationalConferenceonComputer-Aided [53] ShunShunhouandYangPeng.2022.AIoTonCloud.InDigitalTransformation
Design(ICCAD).IEEE,1–8. inCloudComputing.CRCPress,629–732.
[23] MohsenImani,DeqianKong,AbbasRahimi,andTajanaRosing.2017.Voicehd: [54] JamesSmithetal.2021. UnsupervisedProgressiveLearningandtheSTAM
Hyperdimensionalcomputingforefficientspeechrecognition.InIEEEInterna- Architecture.InProceedingsoftheThirtiethInternationalJointConferenceon
tionalConferenceonRebootingComputing(ICRC).IEEE,1–8. ArtificialIntelligence,IJCAI-21.2979–2987.
[24] PenttiKanerva.2009.Hyperdimensionalcomputing:Anintroductiontocom- [55] KeSun,ChenChen,andXinyuZhang.2020."Alexa,stopspyingonme!"speech
putingindistributedrepresentationwithhigh-dimensionalrandomvectors. privacyprotectionagainstvoiceassistants.InProceedingsofthe18thconference
CognitiveComputation1(2009),139–159. onEmbeddedNetworkedSensorSystems.298–311.
[25] BehnamKhaleghi,MohsenImani,andTajanaRosing.2020.Prive-hd:Privacy- [56] AnthonyThomas,SanjoyDasgupta,andTajanaRosing.2021. Atheoretical
preservedhyperdimensionalcomputing.InACM/IEEEDesignAutomationCon- perspectiveonhyperdimensionalcomputing. JournalofArtificialIntelligence
ference(DAC).IEEE,1–6. Research72(2021),215–249.
[26] HyejiKim,MuhammadUmarKarimKhan,andChong-MinKyung.2019.Effi- [57] MatteoTiezzietal.2022.StochasticCoherenceOverAttentionTrajectoryFor
cientneuralnetworkcompression.InProceedingsoftheIEEE/CVFconferenceon ContinuousLearningInVideoStreams.InProceedingsoftheThirty-FirstInterna-
computervisionandpatternrecognition.12569–12577. tionalJointConferenceonArtificialIntelligence,IJCAI-22.3480–3486.
[27] YeseongKim,MohsenImani,andTajanaSRosing.2018. Efficienthumanac- [58] RishabhTiwarietal.2022.Gcr:Gradientcoresetbasedreplaybufferselection
tivityrecognitionusinghyperdimensionalcomputing.InProceedingsofthe8th forcontinuallearning.InProceedingsoftheIEEE/CVFConferenceonComputer
InternationalConferenceontheInternetofThings.1–6. VisionandPatternRecognition.99–108.
[28] JamesKirkpatricketal.2017. Overcomingcatastrophicforgettinginneural [59] UlrikeVonLuxburg.2007. Atutorialonspectralclustering. Statisticsand
networks.Proceedingsofthenationalacademyofsciences(2017). computing17(2007),395–416.
[29] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayersoffeatures [60] ErweiWangetal.2019.Deepneuralnetworkapproximationforcustomhardware:
fromtinyimages.(2009). Wherewe’vebeen,wherewe’regoing.ACMComputingSurveys(CSUR)52,2
[30] YoungDKwonetal.2023.LifeLearner:Hardware-AwareMetaContinualLearn- (2019),1–39.
ingSystemforEmbeddedComputingPlatforms.InProceedingsofthe21stACM [61] QipengWangetal.2022.Melon:Breakingthememorywallforresource-efficient
ConferenceonEmbeddedNetworkedSensorSystems. on-devicemachinelearning.InProceedingsofthe20thAnnualInternational
[31] SoochanLeeetal.2020.ANeuralDirichletProcessMixtureModelforTask-Free ConferenceonMobileSystems,ApplicationsandServices.450–463.
ContinualLearning.InInternationalConferenceonLearningRepresentations. [62] GaryMWeissetal.2016. Smartwatch-basedactivityrecognition:Amachine
[32] JiLinetal.2020.Mcunet:Tinydeeplearningoniotdevices.AdvancesinNeural learningapproach.In2016IEEE-EMBSInternationalConferenceonBiomedical
InformationProcessingSystems33(2020),11711–11722. andHealthInformatics(BHI).IEEE,426–429.
[33] JiLinetal.2021.Memory-efficientpatch-basedinferencefortinydeeplearning. [63] JunyuanXie,RossGirshick,andAliFarhadi.2016.Unsuperviseddeepembedding
AdvancesinNeuralInformationProcessingSystems34(2021),2346–2358. forclusteringanalysis.InInternationalConferenceonMachineLearning.PMLR,
[34] JiLinetal.2022.On-devicetrainingunder256kbmemory.AdvancesinNeural 478–487.
InformationProcessingSystems35(2022),22941–22954. [64] DaliangXuetal.2022. Mandheling:Mixed-precisionon-devicednntraining
[35] DavidLopez-PazandMarc’AurelioRanzato.2017.Gradientepisodicmemoryfor withdspoffloading.InProceedingsofthe28thAnnualInternationalConferenceon
continuallearning.Advancesinneuralinformationprocessingsystems30(2017). MobileComputingAndNetworking.214–227.
[36] MichaelMcCloskeyandNealJCohen.1989.Catastrophicinterferenceincon- [65] WeihongXu,JaeyoungKang,andTajanaRosing.2023.FSL-HD:Accelerating
nectionistnetworks:Thesequentiallearningproblem.InPsychologyoflearning Few-ShotLearningonReRAMusingHyperdimensionalComputing.In2023
andmotivation.Vol.24.Elsevier,109–165. Design,Automation&TestinEuropeConference&Exhibition(DATE).IEEE,1–6.
[37] MdMohaimenuzzamanetal.2023.EnvironmentalSoundClassificationonthe [66] JuntingZhangetal.2020.Class-incrementallearningviadeepmodelconsolida-
Edge:APipelineforDeepAcousticNetworksonExtremelyResource-Constrained tion.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Devices.PatternRecognition133(2023),109025. Vision.1131–1140.
[38] AliMoinetal.2021. Awearablebiosensingsystemwithin-sensoradaptive [67] YuZhang,TaoGu,andXiZhang.2020. MDLdroidLite:Arelease-and-inhibit
machinelearningforhandgesturerecognition. NatureElectronics4,1(2021), controlapproachtoresource-efficientdeepneuralnetworksonmobiledevices.
54–63. InProceedingsofthe18thConferenceonEmbeddedNetworkedSensorSystems.
[39] JamesO’Neill.2020.Anoverviewofneuralnetworkcompression.arXivpreprint 463–475.
arXiv:2006.03669(2020).
[40] AndrewNg,MichaelJordan,andYairWeiss.2001.Onspectralclustering:Analysis
andanalgorithm.Advancesinneuralinformationprocessingsystems14(2001).
[41] EvgenyOsipovetal.2022.Hyperseed:Unsupervisedlearningwithvectorsym-
bolicarchitectures.IEEETransactionsonNeuralNetworksandLearningSystems
(2022).
[42] GermanIParisi,RonaldKemker,JoseLPart,ChristopherKanan,andStefan
Wermter.2019. Continuallifelonglearningwithneuralnetworks:Areview.
Neuralnetworks113(2019),54–71.
[43] AdamPaszkeetal.2019.Pytorch:Animperativestyle,high-performancedeep
learninglibrary.Advancesinneuralinformationprocessingsystems32(2019).
[44] KarolJPiczak.2015. ESC:Datasetforenvironmentalsoundclassification.In
Proceedingsofthe23rdACMinternationalconferenceonMultimedia.1015–1018.
[45] ChristosProfentzas,MagnusAlmgren,andOlafLandsiedel.2022. MiniLearn:
On-DeviceLearningforLow-PowerIoTDevices.InInternationalConferenceon
EmbeddedWirelessSystemsandNetworks.
[46] DushyantRao,FrancescoVisin,AndreiRusu,RazvanPascanu,YeeWhyeTeh,and
RaiaHadsell.2019.Continualunsupervisedrepresentationlearning.Advances
inneuralinformationprocessingsystems32(2019).