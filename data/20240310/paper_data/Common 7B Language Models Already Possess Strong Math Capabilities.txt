Common 7B Language Models Already Possess Strong Math Capabilities
ChenLi1,4,WeiqiWang2,4,JingchengHu3,4,YixuanWei3,4,
NanningZheng1,HanHu4,ZhengZhang4*,HouwenPeng4*
1IAIR,Xi’anJiaotongUniversity 2UniversityofScienceandTechnologyofChina
3TsinghuaUniversity 4MicrosoftResearchAsia
edward82@stu.xjtu.edu.cn {v-weiqiwang, t-jingchu, t-yixuanwei, zhez, houwen.peng}@microsoft.com
nnzheng@xjtu.edu.cn ancientmooner@gmail.com
Abstract
Mathematicalcapabilitieswerepreviouslybe-
lievedtoemergeincommonlanguagemodels
onlyataverylargescaleorrequireextensive
math-related pre-training. This paper shows
thattheLLaMA-27Bmodelwithcommonpre-
trainingalreadyexhibitsstrongmathematical
abilities,asevidencedbyitsimpressiveaccu-
racyof97.7%and72.0%ontheGSM8Kand
Figure 1: The orange star markers represent the ac-
MATHbenchmarks,respectively,whenselect-
curacy achieved by selecting the best response from
ing the best response from 256 random gen-
256 random generations of the LLaMA-2 7B model.
erations. The primary issue with the current
The high accuracy on the MATH (left) and GSM8K
basemodelisthedifficultyinconsistentlyelic-
(right) benchmarks (72.0% and 97.7%, respectively)
itingitsinherentmathematicalcapabilities. No-
suggestthattheLLaMA-27Balreadypossessesstrong
tably,theaccuracyforthefirstanswerdropsto
mathematicalcapabilities,althoughthestabilityingen-
49.5% and 7.9% on the GSM8K and MATH
eratingcorrectanswerscouldbeenhanced. Thispaper
benchmarks, respectively. We find that sim-
demonstrates that by scaling synthetic SFT data, the ply scaling up the SFT data can significantly
stabilitycanbesignificantlyimprovedasevidencedby
enhance the reliability of generating correct
thecurves. ThroughthisstraightforwardscalingofSFT
answers. However, the potential for exten-
data,thetop-performingmodelhasexceededanearly
sive scaling is constrained by the scarcity of
GPT-4modelby10.3%ontheMATHbenchmark.
publicly available math questions. To over-
comethislimitation,weemploysyntheticdata,
billion parameters can attain meaningful accu-
whichprovestobenearlyaseffectiveasreal
racy or benefit from chain-of-thought processing
dataandshowsnoclearsaturationwhenscaled
uptoapproximatelyonemillionsamples. This on math problems. A strategy to equip smaller
straightforwardapproachachievesanaccuracy language models with mathematical abilities in-
of 82.6% on GSM8K and 40.6% on MATH volvescreatingmath-specificbasemodelstrained
usingLLaMA-27Bmodels,surpassingprevi- onhundredsofbillionsofmath-relatedpre-training
ousmodelsby14.2%and20.8%,respectively.
data (Lewkowycz et al., 2022; Azerbayev et al.,
Wealsoprovideinsightsintoscalingbehaviors
2023). However, theaccuracyofsuchmodelsre-
acrossdifferentreasoningcomplexitiesander-
mains modest; for example, Llemma-7B (Azer-
rortypes.
bayev et al., 2023) only achieves 36.4% on the
GSM8Kdataset(Cobbeetal.,2021)and18.0%on
1 Introduction
theMATHdataset(Hendrycksetal.,2021).
Mathematicalcapabilitieshavelongbeenconsid- Inthispaper,wedemonstratethatcommonlan-
eredsochallengingthattheyarethoughttoemerge guagemodelsofsmallsize,suchastheLLaMA-2
incommonlanguagemodelsonlyataverylarge 7Bmodel(Touvronetal.,2023b),alreadypossess
scale. Forinstance,studiesby(Weietal.,2022a,b) strongmathematicalcapabilitieswithoutspecific
suggest that only models with size exceeding 50 pre-training on math-related data. Surprisingly,
we find that with supervised fine-tuning on just
*Projectleader. Chen,Weiqi,JingchengandYixuanare
thousandsofmathquestions(notingthattheSFT
internsatMSRA.GitHub:Xwin-MathThisrepositorywillbe
continuallyupdated. stage does not enhance capabilities as stated in
4202
raM
7
]LC.sc[
1v60740.3042:viXraTable 1: Comparison of SFT data scaling with real achieve accuracy nearly on par with that of real
versussyntheticmathquestions. Itrevealsthatsynthetic
questions, highlighting the potential of synthetic
mathquestionsarenearlyaseffectiveasrealones.
SFTmathquestionsforthescalingpurpose.
Leveragingsyntheticdatahasallowedustoscale
Datasize GSM8K-real GSM8K-syn MATH-real MATH-syn
ourSFTdatasignificantly,forinstance,from7.5K
0.94K 26.7 25.9 4.2 3.9
1.88K 32.8 31.9 5.6 4.9 to 960K on GSM8K and from 7.5K to 480K on
3.75K 43.3 42.2 6.6 6.0 MATH.Thisdatascalingshowsnearlyperfectscal-
7.50K 50.2 49.5 8.4 7.9
ing behavior, as drawn in Figure 1. Specifically,
bysimplyscalingtheSFTdata,ourmodelhasbe-
(Baietal.,2022;Ouyangetal.,2022)),themodel comethefirsttoexceed80%and40%accuracyon
can correctly solve 97.7% of GSM8K questions GSM8KandMATH,respectively,usingastandard
and 72.0% of MATH questions, when selecting LLaMA-2 7B base model (achieving 82.6% and
thebestanswerfrom256randomgenerations,as 40.6%respectively)2.
indicatedbytheorangestarmarksinFigure1. It ThestraightforwardsyntheticSFTdataproves
is noteworthy that the accuracy has even outper- effectivefromstrongerbasemodelsaswell,suchas
formedthosereportedfortheGPT-4model,which LLaMA-270B,whichachieves90.6%onGSM8K
achieved92.0%onGSM8Kand42.5%onMATH and52.8%onMATH.Tothebestofourknowledge,
1. Therefore, we conclude that the LLaMA-2 7B thisisthefirstopen-sourcemodeltoexceed90%
modelhasindeeddevelopedstrongmathematical accuracyonGSM8K.Itisalsothefirstopen-source
capabilities. Theprimaryissueisthelackofguar- modeltooutperformGPT-4(i.e.,GPT-4-0314)on
antee that the correct answer will be digged out, theMATHbenchmark,demonstratingtheefficacy
as most generations are incorrect. In fact, the ac- ofoursimplesyntheticscalingmethod.
curacy drops to 49.5% on GSM8K and 7.9% on In addition to the strong results, we have also
MATH if we consider only one random genera- gleaned insights into the effectiveness of our ap-
tionperquestion. Werefertothisastheinstability proach: 1)AsthescaleofSFTdataincreases,the
issue. model’s accuracy tends to plateau when utilizing
Toaddresstheinstabilityissue,wefirstobserve 256attempts;however,thereisamarkedincrease
thattheaccuracyimprovesalmostinlinearoreven using 1 response. This indicates that while the
super-linear with exponentially increased super- model’suppercapabilitylimitremainsfairlycon-
vised fine-tuning (SFT) data. Moreover, we note stant, the performance gains are primarily due to
that the accuracy is far from reaching a plateau enhanced stability in generating correct answers.
when utilizing all available GSM8K and MATH 2)Theaccuracyofsolvingmathproblemsfollows
training data (as shown in Table 1). This obser- apowerlawwithrespecttothenumberofchain-of-
vation encourages us to further scale up the SFT thought(CoT)stepswithdifferentSFTdataquan-
data. However, we face a challenge as there is a tities. AnexpandedSFTdatasetimprovesthereli-
lackofpubliclyaccessiblerealdatatosupportthis abilityofeachreasoningstep. Furtherincreasing
continuousscaling. theproportionoftrainingsampleswithlongerCoT
Toovercomethislimitation,weturntosynthetic stepsthroughresamplingcansignificantlyimprove
data, employing a prestigious language model, theaccuracyofthemodelfordifficultquestions. 3)
namely GPT-4 Turbo, to produce synthetic math Ananalysisoferrortypesduringthescalingpro-
questions. We find that a straightforward “brand- cessrevealsthatcalculationerrorsaremorereadily
new”generationstrategy,whichpromptstheGPT-4 mitigatedcomparedtoreasoningerrors.
Turboto createacompletely new questionbased
onpreferenceonesandthenappliesasimplever- 2 ExamineMathCapabilityofLanguage
ifier (also GPT-4 Turbo based), has been highly Models
effective. Specifically,asindicatedinTable 1,the
Metrics Weemploytwometricstoexaminethe
useofsyntheticallygeneratedmathquestionscan
mathcapabilitiesoflanguagemodels.
1TheaccuracynumbersarereportedintheGPT-4techni-
calreport(OpenAI,2023b).GPT-4modelsarecontinuously 2Concurrently,DeepSeek-MATH-7B(Shaoetal.,2024)
beingimproved.ThelatestGPT-4Turbo(1106)APIhasin- alsosurpasses80%accuracy.However,theirapproachrelies
creasedaccuracyto94.8%onGSM8Kand64.5%onMATH. on a much stronger base model extensively pre-trained on
However,theLLaMA-27Bmodelusingthebestof256gen- math-relatedcorporaandasophisticatedRLalgorithm.Our
erationsstilloutperformsthelatestGPT-4models. resultsarecomplementarytotheirs.ThefirstisaPass@Nmetric In the following, we will present a simple ap-
proachtosignificantlyreducetheinstabilityissue.
Pass@N = E [min(c,1)], (1)
Problems
3 ScalingSFTDatausingSyntheticMath
wherecrepresentsthenumberofcorrectanswers Questions
outofN responses. Thismetricconsidersaques-
In this section, we first demonstrate that scaling
tion to be solved if at least one correct answer is
up the limited real SFT data can significantly al-
producedfromN randomgenerations. Weemploy
leviatetheinstabilityissue. Wealsoobservethat
thismetrictoreflectthepotentialorcapabilityofa
theaccuracyhasnotyetplateauedwhenusingthe
modelinsolvingamathquestion. Toenhancethe
full available GSM8K and MATH training data.
diversityoftheN generations,wesetthetempera-
tureofthegenerationprocessto0.73. WeconsiderfurtherscalingupSFTdatausingsyn-
theticmathquestions. Tothisaim,weintroducea
ThesecondisaPassRatio@Nmetric
straight-forwardmethodforsyntheticdatagenera-
(cid:104) c (cid:105)
PassRatio@N = E , (2) tionutilizingtheGPT-4TurboAPI.Thesynthetic
N
Problems data proves to be as effective as real math ques-
whichmeasuresthepercentageofcorrectanswers tions. Consequently,weboldlyscalethesynthetic
within the N generated answers. This metric is SFTdatato960KonGSM8Kand480KonMATH,
somewhatequivalenttoPass@1,butwithreduced respectively,resultinginnearlyperfectscalingbe-
variance. havior,andreachstate-of-the-artaccuracy.
Observations Based on these two metrics, we ScalingusingRealMathQuestions Webegin
examinetheperformanceoftheLLaMA-2models by examining the scaling behavior of real math
on the GSM8K and the MATH benchmarks4 as questions across the entire GSM8K and MATH
showninFigure1. Toadaptmodelsforthesetwo trainingsets. AsindicatedinTable 1,weobservea
benchmarksininstruction-followingsettings,we consistentaccuracyimprovement,increasingfrom
use their SFT versions, which are trained with a 26.7% to 50.2% on GSM8K, and from 4.2% to
limitedamountofSFTdata(i.e.,7.5K).Asdemon- 8.4%onMATH,withnosignsofsaturation.
strated in (Bai et al., 2022; Ouyang et al., 2022),
SyntheticSFTDataGeneration Sincethereal
the SFT stage does not enhance capabilities (and
data has been exhausted, we contemplate further
mayevenleadtoareduction,asmentionedinthe
scalingupSFTdatausingsyntheticallygenerated
contextof“alignmenttaxes”). Therefore,employ-
mathquestions.
ingtheSFTversionprovidesafairassessmentof
We introduce a straightforward three-step ap-
themodels’mathematicalcapabilities.
proachwiththeassistanceoftheGPT-4TurboAPI:
WefirstobservethatthePass@256metricsfor
theLLaMA-27Bmodelonbothbenchmarksare
• Step 1. Generate a new math question. We
remarkably high: 97.7% on GSM8K and 72.0%
request the GPT-4 Turbo API to generate a
on MATH. This suggests that the LLaMA-2 7B
brand-new question using a reference math
model possesses a strong capability for solving
question as a starting point. To improve the
mathematicalproblems.
validityofthenewquestions,weincorporate
WethennoticethatthePassRatio@256issignif-
three rules into the prompt: Firstly, the new
icantlylowerthanthatofPass@256,being48.2%
questionmustobeycommonknowledge;sec-
onGSM8Kand7.9%onMATH.Thissuggeststhat
ondly,itshouldbesolvableindependentlyof
whilethecorrectanswerstomostmathquestions
theoriginalquestion;andthirdly,itmustnot
arepresentwithin256randomgenerations,there
include any answer responses. Besides, we
is no assurance that the correct answers will con-
havesetspecificformattingrequirementsfor
sistentlybeextracted,aphenomenonwerefertoas
questionsandanswerstailoredtovarioustar-
an"instabilityissue".
getdatasets.
3Itisworthnotingthatmostmathmodelsutilizeagreedy
generationstrategywiththetemperaturesetto0. However, • Step 2. Verify the question. We further en-
theimpactofthisdifferenceisminimal. hancethequalityofthegeneratedquestions
4Following(Lightmanetal.,2023),weutilizeasubsetof
by validating and refining them through at-
500testsamplesfromtheMATHbenchmarkforexperimental
efficiency. temptedsolutions. Byintegratingsolvingandverification steps into a single prompt, we examplesand5,000testcases. Weusethetraining
havefoundthatthisapproachconsistentlyele- examplestogeneratesyntheticdata.
vatesthevalidityofquestionsacrossdifferent SVAMP(Pateletal.,2021). Thisdatasetcomprises
benchmarks. elementary-level math problems. We utilize all
1,000 of its test cases to assess the cross-dataset
• Step 3. Generate chain-of-thought (CoT)
performanceofourmodels.
answers. WerequestGPT-4Turbotoproduce
ASDiv (Miao et al., 2021). This dataset contains
achain-of-thought(CoT)answerresponsefor
asetofmathproblemswithdiverselanguagepat-
eachnewlygeneratedquestion.
ternsandtypesofquestions. Weadoptthetestset
The detailed prompt designs are shown in Ap- of2,305problemsasevaluationbenchmark.
pendixA. Hungarian National High School Exam This
evaluationbenchmarkisfirstintroducedbyGrok-
ComparisonofSyntheticSFTDataversusReal
1 (xAI, 2023), which is designed for evaluating
Data To assess the quality of the synthetically
the out-of-domain capability of math models. It
generatedmathquestions,weevaluatetheireffec-
consistsof33challengingproblems.
tiveness against real questions from the GSM8K
ItisworthnotingthatthefinalanswersofHun-
andMATHtrainingsets,utilizingaLLaMA-27B
garian National High School Exam dataset is an-
model, as detailed in Table 1. The results indi-
notatedbyhuman,whileotherbenchmarksarela-
catethatthesyntheticmathquestionsarenearlyas
belledusingautomaticscripts,similartoprevious
effectiveastherealones.
works(Luoetal.,2023;Gouetal.,2023).
Wealsoexploredvariousothersyntheticmeth-
odsasproposedinpreviousworks(Xuetal.,2023;
4.2 ImplementationDetails
Yu et al., 2023; An et al., 2023). These methods
Indatasynthesis,weutilizetheGPT-4TurboAPI,
alsoprovetobeeffective,thoughmarginallyless
settingthetemperatureto1.0forbothquestionand
sothantheourapproach,asillustratedinFigure6.
answergeneration.
Scaling to about a Million SFT Math Data
Forsupervisedfine-tuning,weemploytheAdam
Consideringtheeffectivenessofthesyntheticap-
optimizerwithacosinelearningrateschedulespan-
proach, wesubstantiallyincreasethescaleofthe
ningatotalof3epochsoftraining. Themaximum
SFTdataforbothGSM8KandMATHproblems,to
learning rate is set 2e-5 (except that 2e-6 for the
960Kand480K,respectively. Figure1presentsthe
Mistral-7bmodel)andthereisa4%linearwarm-
mainreasultsutilizingvarioussizesoftheLLaMA-
up. Themaximumtokenlengthisset2048,andthe
2series. Thestraightforwardscalingstrategyyields
Vicuna-v1.1(Zhengetal.,2023)systempromptis
state-of-the-artaccuracy.
used. Allexperimentsareconductedon8×Nvidia
Itisalsoworthnotingthattheaccuracyhasnot
H100GPUs. Ourmostresource-intensiveexperi-
yetreacheditspeak. Exploringtheeffectsofaddi-
ment,involvinga70Bmodeland960Kdatapoints,
tionalscalingwillbeleftasourfutureresearch.
takes1900H100GPUhours.
Forevaluation,weusethesamepromptasused
4 Experiments
in SFT and set the maximum sequence length to
4.1 DatasetsandEvaluations 2048. The vLLM (Kwon et al., 2023) is used in
answergeneration.
Weconductexperimentson5benchmarkstoevalu-
atetheefficacyoftheproposedmethod.
4.3 MainResultsandComparisonwith
GSM8K (Cobbe et al., 2021). This is a high-
State-of-the-artModels
quality,linguisticallydiversemathdataset,whose
mathknowledgemainlycoversgradeschoollevel. In this comparison, we examine both in-domain
Itincludes7,473trainingexamplesand1,319test benchmarks,GSM8K/MATH,andout-of-domain
cases. In this work, we use its training set as the benchmarks,suchastheHungarianNationalHigh
givenquestionstogeneratenewsyntheticdata. School Exam. For in-domain evaluation of each
MATH(Hendrycksetal.,2021). Thisdatasetfo- benchmark, we utilize data synthesized from its
cusesoncompetitive-levelmathproblemsthatre- respective training samples. For GSM8K, 960K
quireshighlevelsofreasoningabilityandmathe- syntheticdataisemployed,whileforMATH,480K
matical knowledge. It consists of 7,500 training syntheticdataisused. Forout-domainevaluation,Table2:MathreasoningperformancesofvariousLLMs. Table3:Hungariannationalhighschoolexamtestresult
ofvariousLLMs.
Model GSM8K MATH
Model TestScore(%)
Closed-sourcemodels
GPT-4(Achiametal.,2023) 68
GPT-4Turbo(1106) 94.8 64.5
Grok-1(xAI,2023) 59
GPT-4-0314 94.7 52.6
Claude-2(Anthropic,2023) 55
GPT-4(Achiametal.,2023) 92.0 42.5
GPT-3.5Turbo(OpenAI,2023a) 41
Claude-2(Anthropic,2023) 88.0 -
DeepSeek-LLM-67B-Chat(Bietal.,2024) 58
GPT-3.5-Turbo(OpenAI,2023a) 80.8 34.1
Xwin-Math-70B(480KGSM8K) 22
Open-sourcemodelsLLaMA-2-7B Xwin-Math-70B(120KMATH) 51
WizardMath-7B(Luoetal.,2023) 54.9 10.7 Xwin-Math-70B(480KMATH) 59
MuggleMath-7B(Lietal.,2023) 68.4 - Xwin-Math-70B(480KMix) 65
MetaMath-7B(Yuetal.,2023) 66.5 19.8
LEMA-LLaMA-2-7B(Anetal.,2023) 54.1 9.4
Xwin-Math-7B(ours) 82.6 40.6 improvements are +14.1 on GSM8K (compared
Open-sourcemodelsMistral-7B to MuggleMath-13B (Li et al., 2023)) and +22.5
WizardMath-7B-v1.1(Luoetal.,2023) 83.2 33.0 onMATH(comparedtoMetaMath-13B(Yuetal.,
MetaMath-Mistral-7B(Yuetal.,2023) 77.4 28.2 2023)),respectively. OnLLaMA-2-70B,thegains
Xwin-Math-Mistral-7B(ours) 89.2 43.7
are+7.1onGSM8K(comparedtoLEMA-LLaMA-
Open-sourcemodelsLlemma-7B 2-70B(Anetal.,2023))and+26.2onMATH(com-
MetaMath-Llemma-7B(Yuetal.,2023) 69.2 30.0
paredtoMetaMath-70B(Yuetal.,2023)),respec-
Xwin-Math-Llemma-7B(ours) 84.2 47.2
tively.
Open-sourcemodelsLLaMA-2-13B
On a stronger common language model, i.e.,
WizardMath-13B(Luoetal.,2023) 63.9 14.0
Mistral-7B,theimprovementsare+6.0onGSM8K
MuggleMath-13B(Lietal.,2023) 74.0 -
MetaMath-13B(Yuetal.,2023) 72.3 22.4 and +10.7 on MATH (compared to WizardMath-
LEMA-LLaMA-2-13B(Anetal.,2023) 65.7 12.6 7B-v1.1(Luoetal.,2023)),respectively.
Xwin-Math-13B(ours) 88.1 44.9 Onamath-specificbasemodel,suchasLlemma-
Open-sourcemodelsLLaMA-2-70B 7B,thegainsare+15.0onGSM8Kand+17.2on
WizardMath-70B(Luoetal.,2023) 81.6 22.7 MATH(comparedtoMetaMath-Llemma-7B(Luo
MuggleMath-70B(Lietal.,2023) 82.3 -
etal.,2023)),respectively.
MetaMath-70B(Yuetal.,2023) 82.3 26.6
It is also noteworthy that our LLaMA-2-70B
LEMA-LLaMA-2-70B(Anetal.,2023) 83.5 25.0
Xwin-Math-70B(ours) 90.6 52.8 model achieves competitive accuracy with early
versionsofGPT-4onGSM8KandMATH.Toour
wetestmodelstrainedusingGSM8K,MATH,ora knowledge,thisisthefirstLLaMA-basedmodelto
mixedoftwosyntheticsets. outperformGPT-4-0314onMATH.
Forbasemodels,weconsiderbothcommonlan- Theseresultsdemonstratethesignificanteffec-
guagemodels,i.e.,LLaMA-27B/13B/70B/Mistral- tivenessandbroadapplicabilityofscalingsynthetic
7B,andmath-specificmodels,suchasLlemma-7B, mathSFTdata.
toassessthegeneralityoftheproposedapproach.
Out-of-Domain Results We test the models
In-DomainResults Table 2presentsacompar- trainedusingGSM8K,MATH,oramixedoftwo
ison of the proposed approach with the state-of- synthetic sets on an out-of-domain benchmark,
the-artopenandclosed-sourcemodels. Acrossall HungarianNationalHigh-SchoolExamTest,fol-
basemodels,ourmethodsignificantlyoutperforms lowingthepracticein(xAI,2023).
thepreviousbestapproachesthatusethesamepre- Table 3 shows the results. Our model trained
trainedbasemodel. onthemixingdata(240KMATHsyntheticdata+
On LLaMA-2-7B, our approach exceeds the 240KGSM8Ksyntheticdata)rankedasthesecond,
prior best by absolutely +14.2 on GSM8K (com- justbehindtheGPT-4andmuchbetterthanother
paredtoMuggleMath-7B(Lietal.,2023)),andby models. Additionally, we plot the correlation be-
+20.8onMATH(comparedtoMetaMath-7B(Yu tweenGSM8KandHungariannationalhigh-school
etal.,2023)),respectively. Itevensurpassesseveral examscoresinAppendixB.Theresultsshowthat
latest70Bmodelsdedicatedformathcapabilities, thereisnosignificantbenchmarkoverfittinginour
suchasWizardMath-70B(Luoetal.,2023)(82.6 model.
versus81.6onGSM8K).OnLLaMA-2-13B,the Figure.2(Left)presentstheresultsofthemodelFigure2: ComparingtheincreaseinSFTdatascaleusingeitherasingledatasetormixeddatasets.
Figure3: ThePass@256andPassRatio@256curvewithincreasingdatasizeonGSM8KandMATHbenchmark.
trainedonGSM8Ksyntheticdata,whileFigure.2 attempts. Meanwhile, the Pass@256 metric only
(Middle)presentstheresultsofthemodeltrained changed slightly with increasing the amount of
onMATH.Wefindthattheaccuracyofotherbench- useddata. Incontrast,PassRatio@256,whichre-
marks also improves as the amount of data in- flects the stability to generate correct answer, in-
creasesformodelstrainedwitheitherGSM8Kor creasessignificantlywiththeamountofsynthetic
MATHsyntheticdata. Wealsonotethatthegener- data, and its growth trend is similar to that of
alizationbehaviorsdifferforGSM8KandMATH Pass@1. Thisresultconfirmsourhypothesizethat
models: 1)SVAMPandASDivbenefitmorefrom the performance improvements is mainly caused
GSM8K models than from MATH models. 2) bythebetterstabilityinanswergenerationrather
While MATH models perform relatively well on thanstrongerabilitytoanswerthequestion.
theGSM8Kbenchmark,GSM8Kmodelsperform EstimatedSingle-stepReasoningAccuracyBe-
considerablyworseonMATHbenchmarks. causeoftheChain-of-Thought(CoT)areadopted
Figure.2(Right)showstheresultsofmodelsus- ininference,theprocessofanswermathematical
ingamixtureofGSM8KandMATHina1:1ratio. problems is completed by a multi-step reasoning
These models exhibit balanced scaling behaviors process. Therefore, we hypothesize that the in-
inbothin-domainandout-of-domainbenchmarks. creaseinfinalansweraccuracycanbeinterpreted
bytheimprovementinsingle-stepreasoningaccu-
4.4 WhatHappensbehindPerformance
racy. Basedonthisassumption,ifonequestioncan
Improvements? betheoreticallyansweredbysreasoningstepsin
Pass@256v.s. PassRatio@256Todeepentheun- CoT,thefinalansweraccuracycanbeapproximate
derstandingbehindtheperformanceimprovements, bythepowerfunctionofthesingle-stepreasoning
wetrackedPass@NmetricandPassRatio@Nmet- accuracy:
ricunderdifferentdatasize. Theresultsareshown Acc = Accs (3)
final step
inFigure3. Withverylimitedsyntheticdata(e.g.
7.5Ksamples),theXwin-Math-70Bmodelalready With this equation, step accuracy can be esti-
hasveryhighPass@256,indicatingthestrongabil- matedfromthefinalansweraccuracy. Weexperi-
ity to generate correct answers through multiple mentedonGSM8K.ForeachquestioninthetestTable4: Theestimatedsingle-stepreasoningaccuracy
andtheaveragenormalizedfirsterrorpositionbyGPT-4
TurboinXwin-Math-7BonGSM8Kbenchmark.
Datasize EstimatedAccstep Normalizedfirsterrorposition
7.5K 78.9 67.1
120K 89.7 83.9
960K 94.2 90.9
Figure 4: Left: The relationship between the mean
accuracyontheGSM8Kandthenumberofannotated
CoTstepswithdataincreasing. Thesolidlineisfitted
using all seven points, while the dashed line is fitted
using the first four points. Right: Changes in mean
accuracywhenresamplingisusedtoincreasetheCoT
lengehoftrainingdata.
set,wegenerated256responsesandusedthenum-
ber of steps in the GSM8k test set’s CoT annota-
tions as the theoretical CoT steps. We draw the Figure5: Changesintheproportionofcalculationand
curvetoshowtherelationshipbetweenthenumber reasoningmistakeduringdataincreased.
ofCoTreasoningstepsandmeanfinalansweraccu-
racyandshowthefittedcurvebasedonEquation.3. errorpositionofthenormalizationispostponed.
WetestXwin-Math-7Bmodelswithdifferentsyn- TheImprovementintheAccuracyofNumerical
theticdata, andtheresultsareshowninFigure4. Calculations is More Significant than Logical
The solid line is fitted using all seven points and ReasoningTheperformanceofthemodelgradu-
Table 4showstheestimatedsingle-stepaccuracy allyimprovesasthesyntheticdataincreases. For
whenusingdifferentamountsofdatausingalldata adeeperunderstanding,weanalyzetheerrorpro-
points,anditcanbeseenthatthesingle-stepaccu- portion for different types of errors on GSM8K.
racyimprovesignificantlywithmoredata. We categorized errors into two types: reasoning
errorsandcalculationerrors. Reasoningerrorspri-
However, when we fit based on Equation. 3 to
marilyencompassissuessuchaslossofconditions
thefirstfourpoints,asshownindashedlines,we
and concept confusion, while calculation errors
foundthatthelatterthreepointsweresignificantly
includeincorrectanalysisofquantitativerelation-
belowthecurve. Webelievethisphenomenonmay
shipsandnumericalcomputationmistakes. Based
berelatedtothesmallerproportionofmorecom-
ontheexperimentalresultsillustratedinFigure5,
plexproblemsinthetrainingdata. Therefore,we
we observe a gradual decrease in the percentage
resampledthe960Ksyntheticdataaccordingtothe
of calculation errors, suggesting that GSM8K is
number of sentences in CoT solution. As can be
correcting calculation errors at a faster rate than
seenfromFigure4(right),whentheproportionof
reasoningerrors.
complex problems is increased, the accuracy for
simplerproblemsremainsvirtuallyunchanged,but
4.5 AblationsontheDataSyntheticSchema
the accuracy for more complex problems can be
significantly improved. Moreover, the utilization Comparison with Other Data Synthetic Meth-
ofdataresamplingcanincreasethemodel’sPass- ods We compared our approach with following
Ratio@256from71.1to72.8. Thisexperimental commonuseddatasyntheticmethods:
resultprovidesnewinsightsintodataselectionfor AddConstraint. Addingonemoreconstraintothe
mathematicalreasoningtasks. originalquestionwhilekeepingothersunchanged,
In addition, we further used the GPT-4 Turbo whichisusedinWizardMathandMuggleMath.
to find the position where the first step in our an- ChangeNumbers. Changingthenumbersthatap-
swer was wrong and normalized that position by pearintheproblemwhilekeepingthecontextin-
the total number of steps in each answer. As the tact. whichisusedinMuggleMath.
estimatedsingle-stepaccuracygetshigher,thefirst ChangeBackground. Changingthebackgroundinments, with impressive performance on a wide
rangeoftasks. Currently,closed-sourcelargelan-
guagemodels,representedbyGPT(Brownetal.,
2020; Achiam et al., 2023), Gemini (Team et al.,
2023),Grok(xAI,2023),andClaude-2(Anthropic,
2023), are the most advanced models in terms of
performance. However,open-sourcemodels,repre-
sentedbyLLaMA(Touvronetal.,2023a),LLaMA-
2(Touvronetal.,2023b)andMixtral(Jiangetal.,
2024),havealsoprogressedrapidly,andhaveeven
shown competitive performance with the closed-
Figure6: GSM8KandMATHperformanceofdifferent
syntheticmethods. source models on some tasks. Our work, which
Table5: AblationofquestionverificationonMATH. aims to improve the performance of open-source
LLMsonmathematicaltasksbyfine-tuningthem
Model Pass@1(%) onsyntheticdata.
Xwin-Math-70B(7.5Kdata) 28.9
Reasoning Framework for Improving Math-
Xwin-Math-70B(7.5Kdata)w/overification 28.1(-0.8)
ematical Capability Chain-of-thoughts (Wei
Xwin-Math-70B(30Kdata) 37.6
Xwin-Math-70B(30Kdata)w/overification 36.6(-1.0) etal.,2022b)encouragestheLLMsperformmulti-
step reasoning by specific designed prompts and
thequestionwhilekeepingothersthesame.
can improve reasoning performance. Based on
TheCombinationofChangingNumbersandBack-
thiswork,manysubsequentworkssuggestingfur-
ground. Ahybridapproachthatcombineschanging
ther improvements (Fu et al., 2022; Zhang et al.,
bothnumbersandbackground.
2022;Kojimaetal.,2022). Theaboveworksfocus
MetaMathApproach. Thesyntheticmethodspro-
primarilyonhowtoimproveperformancethrough
posed in MetaMath, including answer augmenta-
betterpromptdesignorinferencestrategieswithout
tion,rephrasingquestion,self-verificationquestion
fine-tuningthemodel, whereasourworkfocuses
andFOBARquestion. Inexperiments,wefollow
onhowtoimprovethemodelitself,andthusthese
the implementation of MetaMath but use GPT-4
approachesarecomplementarytoours.
Turbo instead of GPT-3.5 Turbo to generate re-
sponsedatausingtheirreleasedquestions. Fine-tuned LLM for Math Another sort of
The experimental results in the Figure 6 show works (Lightman et al., 2023; Luo et al., 2023;
thatwhenthedatasizeisrelativelysmall,e.g.,7.5k Azerbayevetal.,2023;Yueetal.,2023;Yuetal.,
and30ksamples,theperformancegapbetweenthe 2023; An et al., 2023; Li et al., 2023; Gou et al.,
different methods is negligible. However, as the 2023)trytoimproveperformancedirectlybytrain-
data size increases, our method and the method ingthemodelonmathematicaldata. Adirectway
withaddedconstraintsshowstrongerperformance. istousefine-tuningtoimprovemodels. Onewidely
Thissuggeststhatthechoiceofdatasyntheticstrat- usedmethodistousesyntheticdata,whichisvery
egybecomesmorecriticalasthedatasizeincreases, closetoourapproach: MetaMath(Yuetal.,2023)
andthatsomemethodscanscalethedatamoreef- presents to bootstrap questions to augment data.
ficiently,thusimprovingtheperformance. LeMA(Anetal.,2023)collectsmistake-correction
Effects of Question Verification. The question datapairsbyusingGPT-4asacorrector. AndMug-
verificationisusedtofurtherimprovethegenera- gleMath (Li et al., 2023) augments the GSM8K
tion quality. In our experiments, we found it can datasetbyincorporatingGPT-4withaseriesofpre-
improve the performance on MATH benchmark, defined operations. Compared to these synthetic
theresultsareshowninTable 5,whilewedonot data based efforts, our data synthetic method is
seesignificantlyimpactonGSM8Kdataset. muchsimplerandmorescalableduetointroduce
lesspriorandconstraint.
5 RelatedWorks
SFT Data Scaling Recently, some research ef-
LargeLanguageModels Largelanguagemod- fortshavefocusedonthedatascaleforsupervised
els(Brownetal.,2020;Achiametal.,2023;Tou- fine-tuning. Forinstance,LIMA(Zhouetal.,2023)
vronetal.,2023a,b)havemadesignificantachieve- mentionsthatfine-tuningwith1,000high-qualityinstructionscanyieldimpressiveresultsinvarious Zhangir Azerbayev, Hailey Schoelkopf, Keiran
generaltasks. Otherstudieshaveindicatedthatper- Paster, Marco Dos Santos, Stephen McAleer,
formancescaleswithdatasizeinmathematicaland AlbertQJiang,JiaDeng,StellaBiderman,and
codingtasks(Dongetal.,2023). Recentwork(Bi Sean Welleck. 2023. Llemma: An open lan-
etal.,2024)evenuses1.5milliondataforinstruct guage model for mathematics. arXiv preprint
fine-tuning to obtain top performance. However, arXiv:2310.10631.
theintrinsicreasonsbehindthisscalingeffecthave
YuntaoBai,AndyJones,KamalNdousse,Amanda
notbeenthoroughlyinvestigated.
Askell, Anna Chen, Nova DasSarma, Dawn
6 Conclusion
Drain, Stanislav Fort, Deep Ganguli, Tom
Thisstudyrevealsthatcommon7Blanguagemod- Henighan, Nicholas Joseph, Saurav Kadavath,
els,suchasLLaMA-27B,alreadyexhibitstrong JacksonKernion,TomConerly,SheerEl-Showk,
mathematical capabilities, challenging the previ- NelsonElhage,ZacHatfield-Dodds,DannyHer-
ousbeliefthatadvancedmathematicalreasoningis nandez,TristanHume,ScottJohnston,Shauna
exclusive to larger, more extensively pre-trained Kravec, Liane Lovitt, Neel Nanda, Catherine
models. By significantly scaling up SFT data, Olsson,DarioAmodei,TomBrown,JackClark,
we have markedly improved the stability of the Sam McCandlish, Chris Olah, Ben Mann, and
model’smathematicalproblem-solvingskills. Our JaredKaplan.2022.Trainingahelpfulandharm-
methodologyhasenabledtheXwin-Mathmodels lessassistantwithreinforcementlearningfrom
to reach performance levels comparable to, and humanfeedback.
insomeinstancessurpassing,thoseoftheirlarger
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang
counterparts. Our analysis also indicates that the
Chen, Damai Dai, Chengqi Deng, Honghui
enhancementsareprimarilyattributabletoheight-
Ding, Kai Dong, Qiushi Du, Zhe Fu, et al.
enedaccuracyinsingle-stepreasoningandaextra
2024. Deepseek llm: Scaling open-source lan-
resampling of training data can improve the ac-
guagemodelswithlongtermism. arXivpreprint
curacy of harder questions. Additionally, we see
arXiv:2401.02954.
moresubstantialreductionofcalculationerrorsas
opposedtologicalreasoningerrors. Ourresearch
TomBrown,BenjaminMann,NickRyder,Melanie
contributesvaluableinsightsintothemathematical
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
capabilitiesoflargelanguagemodels.
ArvindNeelakantan,PranavShyam,GirishSas-
try,AmandaAskell,etal.2020. Languagemod-
Acknowledgments
els are few-shot learners. Advances in neural
Chen Li and Nanning Zheng were supported in informationprocessingsystems,33:1877–1901.
partbyNSFCundergrantNo. 62088102. Thank
KarlCobbe,VineetKosaraju,MohammadBavar-
ShengnanAnatIAIR,Xi’anJiaotongUniversity
ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
forhisvaluableadviceonthiswork.
MatthiasPlappert,JerryTworek,JacobHilton,
References ReiichiroNakano,etal.2021. Trainingverifiers
to solve math word problems. arXiv preprint
Josh Achiam, Steven Adler, Sandhini Agar-
arXiv:2110.14168.
wal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Al- GuantingDong,HongyiYuan,KemingLu,Cheng-
tenschmidt, Sam Altman, Shyamal Anadkat, peng Li, Mingfeng Xue, Dayiheng Liu, Wei
et al. 2023. Gpt-4 technical report. arXiv Wang, Zheng Yuan, Chang Zhou, and Jin-
preprintarXiv:2303.08774. gren Zhou. 2023. How abilities in large
language models are affected by supervised
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning fine-tuning data composition. arXiv preprint
Zheng,Jian-GuangLou,andWeizhuChen.2023. arXiv:2310.05492.
Learning from mistakes makes llm better rea-
soner. arXivpreprintarXiv:2310.20689. Yao Fu, Hao Peng, Ashish Sabharwal, Peter
Clark, and Tushar Khot. 2022. Complexity-
Anthropic.2023. Modelcardandevaluationsfor basedpromptingformulti-stepreasoning. arXiv
claudemodels. preprintarXiv:2210.00720.Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao,
Yang,MinlieHuang,NanDuan,WeizhuChen, Jianguang Lou, Chongyang Tao, Xiubo Geng,
et al. 2023. Tora: A tool-integrated reasoning Qingwei Lin, Shifeng Chen, and Dongmei
agentformathematicalproblemsolving. arXiv Zhang.2023. Wizardmath: Empoweringmath-
preprintarXiv:2309.17452. ematical reasoning for large language models
via reinforced evol-instruct. arXiv preprint
Dan Hendrycks, Collin Burns, Saurav Kadavath, arXiv:2308.09583.
Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. 2021. Measuring Shen-YunMiao,Chao-ChunLiang,andKeh-Yih
mathematical problem solving with the math Su. 2021. A diverse corpus for evaluating and
dataset. arXivpreprintarXiv:2103.03874. developingenglishmathwordproblemsolvers.
arXivpreprintarXiv:2106.15772.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris OpenAI.2023a. Gpt-3.5turbofine-tuningandapi
Bamford,DevendraSinghChaplot,Diegodelas updates.
Casas, Emma Bou Hanna, Florian Bressand,
et al. 2024. Mixtral of experts. arXiv preprint OpenAI. 2023b. GPT-4 technical report. CoRR,
arXiv:2401.04088. abs/2303.08774.
TakeshiKojima,ShixiangShaneGu,MachelReid, LongOuyang,JeffWu,XuJiang,DiogoAlmeida,
Yutaka Matsuo, and Yusuke Iwasawa. 2022. CarrollL.Wainwright,PamelaMishkin,Chong
Large language models are zero-shot reason- Zhang,SandhiniAgarwal,KatarinaSlama,Alex
ers. Advancesinneuralinformationprocessing Ray,JohnSchulman,JacobHilton,FraserKel-
systems,35:22199–22213. ton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul Christiano, Jan
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Leike,andRyanLowe.2022. Traininglanguage
Ying Sheng, Lianmin Zheng, Cody Hao Yu, modelstofollowinstructionswithhumanfeed-
Joseph E. Gonzalez, Hao Zhang, and Ion Sto- back.
ica. 2023. Efficient memory management for
largelanguagemodelservingwithpagedatten- ArkilPatel,SatwikBhattamishra,andNavinGoyal.
tion. InProceedingsoftheACMSIGOPS29th 2021. Are nlp models really able to solve
SymposiumonOperatingSystemsPrinciples. simple math word problems? arXiv preprint
arXiv:2103.07191.
Aitor Lewkowycz, Anders Andreassen, David
Dohan, Ethan Dyer, Henryk Michalewski, Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin
Vinay Ramasesh, Ambrose Slone, Cem Anil, Xu,JunxiaoSong,MingchuanZhang,Y.K.Li,
ImanolSchlag,TheoGutman-Solo,YuhuaiWu, Y. Wu, and Daya Guo. 2024. Deepseekmath:
Behnam Neyshabur, Guy Gur-Ari, and Vedant Pushingthelimitsofmathematicalreasoningin
Misra. 2022. Solving quantitative reasoning openlanguagemodels.
problemswithlanguagemodels.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Chengpeng Li, Zheng Yuan, Guanting Dong, YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang RaduSoricut,JohanSchalkwyk,AndrewMDai,
Wang, and Chang Zhou. 2023. Query and re- Anja Hauth, et al. 2023. Gemini: a family
sponseaugmentationcannothelpout-of-domain of highly capable multimodal models. arXiv
math reasoning generalization. arXiv preprint preprintarXiv:2312.11805.
arXiv:2310.05506.
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Hunter Lightman, Vineet Kosaraju, Yura Burda, Xavier Martinet, Marie-Anne Lachaux, Timo-
Harri Edwards, Bowen Baker, Teddy Lee, Jan thée Lacroix, Baptiste Rozière, Naman Goyal,
Leike,JohnSchulman,IlyaSutskever,andKarl EricHambro,FaisalAzhar,etal.2023a. Llama:
Cobbe. 2023. Let’s verify step by step. arXiv Openandefficientfoundationlanguagemodels.
preprintarXiv:2305.20050. arXivpreprintarXiv:2302.13971.Hugo Touvron, Louis Martin, Kevin Stone, Pe- Chunting Zhou, Pengfei Liu, Puxin Xu, Srini
ter Albert, Amjad Almahairi, Yasmine Babaei, Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia
Nikolay Bashlykov, Soumya Batra, Prajjwal Efrat, Ping Yu, Lili Yu, et al. 2023. Lima:
Bhargava,ShrutiBhosale,etal.2023b. Llama Less is more for alignment. arXiv preprint
2: Openfoundationandfine-tunedchatmodels. arXiv:2305.11206.
arXivpreprintarXiv:2307.09288.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel,BarretZoph,SebastianBorgeaud,DaniYo-
gatama,MaartenBosma,DennyZhou,Donald
Metzler,EdH.Chi,TatsunoriHashimoto,Oriol
Vinyals,PercyLiang,JeffDean,andWilliamFe-
dus.2022a. Emergentabilitiesoflargelanguage
models.
Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. 2022b. Chain-of-
thoughtpromptingelicitsreasoninginlargelan-
guagemodels. AdvancesinNeuralInformation
ProcessingSystems,35:24824–24837.
xAI.2023. Grok-1.
Can Xu, Qingfeng Sun, KaiZheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. 2023. Wizardlm: Empowering
largelanguagemodelstofollowcomplexinstruc-
tions. arXivpreprintarXiv:2304.12244.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng
Yu,ZhengyingLiu,YuZhang,JamesTKwok,
Zhenguo Li, Adrian Weller, and Weiyang Liu.
2023. Metamath: Bootstrapyourownmathemat-
icalquestionsforlargelanguagemodels. arXiv
preprintarXiv:2309.12284.
XiangYue,XingweiQu,GeZhang,YaoFu,Wen-
haoHuang,HuanSun,YuSu,andWenhuChen.
2023. Mammoth: Buildingmathgeneralistmod-
els through hybrid instruction tuning. arXiv
preprintarXiv:2309.05653.
Zhuosheng Zhang, Aston Zhang, Mu Li, and
AlexSmola.2022. Automaticchainofthought
prompting in large language models. arXiv
preprintarXiv:2210.03493.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng,
Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang,ZiLin,ZhuohanLi,DachengLi,Eric.P
Xing,HaoZhang,JosephE.Gonzalez,andIon
Stoica. 2023. Judging llm-as-a-judge with mt-
benchandchatbotarena.A SyntheticPromptonGSM8K
Prompt1: QuestionGeneration
Pleaseactasaprofessionalmathteacher.
Yourgoalistocreatehighqualitymathwordproblemstohelpstudentslearnmath.
Youwillbegivenamathquestion. PleasecreateanewquestionbasedontheGivenQuestionandfollowing
instructions.
Toachievethegoal,youhavethreejobs.
#PleasegenerateasimilarbutnewquestionaccordingtotheGivenQuestion.
#Checkthequestionbysolvingitstep-by-steptofindoutifitadherestoallprinciples.
#Modifythecreatedquestionaccordingtoyourcheckingcommenttoensureitisofhighquality.
Youhavefiveprinciplestodothis.
#Ensurethenewquestiononlyasksforonething,bereasonable,bebasedontheGivenQuestion,andcanbe
answeredwithonlyanumber(floatorinteger). Forexample,DONOTask,‘whatistheamountofA,Band
C?’.
#Ensurethenewquestionisinlinewithcommonsenseoflife. Forexample,theamountsomeonehasorpays
mustbeapositivenumber,andthenumberofpeoplemustbeaninteger.
# Ensure your student can answer the new question without the given question. If you want to use some
numbers,conditionsorbackgroundinthegivenquestion,pleaserestatethemtoensurenoinformationis
omittedinyournewquestion.
#PleaseDONOTincludesolutioninyourquestion.
#Ifthecreatedquestionalreadyfollowstheseprinciplesuponyourverification. Justkeepitwithoutany
modification.
GivenQuestion: givenquestion
Youroutputshouldbeinthefollowingformat:
CREATEDQUESTION:<yourcreatedquestion>
VERIFICATION AND MODIFICATION: <solve the question step-by-step and modify it to follow all
principles>
FINALCREATEDQUESTION:<yourfinalcreatedquestion>
Prompt2: AnswerGeneration
Pleaseactasaprofessionalmathteacher.
Yourgoalistoaccuratelysolveamathwordproblem.
Toachievethegoal,youhavetwojobs.
#WritedetailedsolutiontoaGivenQuestion.
#Writethefinalanswertothisquestion.
Youhavetwoprinciplestodothis.
#Ensurethesolutionisstep-by-step.
#Ensurethefinalanswerisjustanumber(floatorinteger).
GivenQuestion: givenquestion
Youroutputshouldbeinthefollowingformat:
SOLUTION:<yourdetailedsolutiontothegivenquestion>
FINALANSWER:<yourfinalanswertothequestionwithonlyanintegerorfloatnumber>
Prompt3: QuestionGenerationw/overification
Pleaseactasaprofessionalmathteacher.
Yourgoalistocreatehighqualitymathwordproblemstohelpstudentslearnmath.
Youwillbegivenamathquestion. PleasecreateanewquestionbasedontheGivenQuestionandfollowing
instructions.
Toachievethegoal,youhaveonejob.
#PleasegenerateasimilarbutnewquestionaccordingtotheGivenQuestion.
Youhavefourprinciplestodothis.
#Ensurethenewquestiononlyasksforonething,bereasonable,bebasedontheGivenQuestion,andcanbe
answeredwithonlyanumber(floatorinteger). Forexample,DONOTask,‘whatistheamountofA,Band
C?’.
#Ensurethenewquestionisinlinewithcommonsenseoflife. Forexample,theamountsomeonehasorpays
mustbeapositivenumber,andthenumberofpeoplemustbeaninteger.
# Ensure your student can answer the new question without the given question. If you want to use some
numbers,conditionsorbackgroundinthegivenquestion,pleaserestatethemtoensurenoinformationis
omittedinyournewquestion.
#Youonlyneedtocreatethenewquestion. PleaseDONOTsolveit.
GivenQuestion: givenquestion
Youroutputshouldbeinthefollowingformat:
CREATEDQUESTION:<yourcreatedquestion>B AdditionalResults
Figure7: Xwin-Math’saggregateperformanceonthesetwobenchmarksissecondonlytoGPT-4,demonstrating
ourmodel’srobustgeneralizationcapabilities.
Hungarian Exam Score vs GSM8K Score
GPT-4
90 Xwin-Math-70B (480K GSM8K)
Claude-2
Xwin-Math-70B (480K Mix)
85
DeepSeek-LLM-67B-Chat
Xwin-Math-70B (480K MATH)
80
Xwin-Math-70B (120K MATH)
75
70
65
Grok-1
60
GPT-3.5 Turbo
20 30 40 50 60 70
Hungarian Exam Score (%)
Table6: Tovalidatebenchmarkdataleakageduringdatageneration,wecompareLMlosson: 1)atrainingsubset,
whichisasmallsubsetwith256samplesfromallsyntheticdata;2)aregeneratedtrainingsubset,wherewemaintain
theoriginalquestionsfromthetrainingsubsetanduseGPT-4Turbotorewriteanswers;3)aregeneratedtestset,
wherewekeepthequestionsfromtheevaluationmetricsunchangedanduseGPT-4Turbotorewriteanswers;4)
areferencetestset, whereweusethetestsetasseedtogeneratenewquestionsandanswersviaGPT-4Turbo.
ReferringtoSkywork,wealsoreporttwokeymetrics: ∆ =L −L ,∆ =L −L ,As
1 test-regen test-ref 2 test-regen train-regen
∆ iscloseto0and∆ issignificantlygreaterthan0intwobenchmarks,webelievethatthereisnoleakageduring
1 2
theprocessofdatasynthesis.
Dataset L L L L ∆ ∆
test-regen test-ref train train-regen 1 2
GSM8K 0.52 0.50 0.11 0.33 0.02 0.19
MATH 0.59 0.58 0.23 0.39 0.01 0.20
)%(
erocS
K8MSGC CaseStudyofSyntheticData
C.1 GSM8K
OriginalQuestion
Randyhas60mangotreesonhisfarm. Healsohas5lessthanhalfasmanycoconuttreesasmango
trees. HowmanytreesdoesRandyhaveinallonhisfarm?
SyntheticQuestionw/oVerification
Mariahas80appletreesinherorchard. Shealsohas4lessthanathirdasmanycherrytreesasapple
trees. HowmanytreesdoesMariahaveinallinherorchard?
VerificationComment
Let’scheckthecreatedquestion:
-Mariahas80appletrees.
-Shehas4lessthanathirdasmanycherrytreesasappletrees. Athirdoftheappletreesis80/3,
whichisapproximately26.67.
-Fourlessthanthatwouldbearound22.67,butsincewecannothaveafractionofatree,weroundit
tothenearestwholenumber,whichis27.
-Themodifiedquestionshouldaskforthenumberofcherrytreeswithoutthedecimal. Soweadjust
thequestiontohaveawholenumberasaresult.
-Since80isnotdivisibleby3,togetawholenumberofcherrytrees,wecanchangethenumberso
thatitisdivisibleby3. Let’suse81appletreesinsteadof80.
SyntheticQuestionwithVerification
Mariahas81appletreesinherorchard. Shealsohas4lessthanathirdasmanycherrytreesasapple
trees. HowmanytreesdoesMariahaveinallinherorchard?
SyntheticSolutionandFinalAnswer
First,let’sdeterminethenumberofcherrytreesMariahas. Wearetoldshehas4lessthanathirdas
manycherrytreesasappletrees.
SinceMariahas81appletrees,let’sfindonethirdofthatnumber:
1/3of81appletrees=81appletrees/3=27appletrees.
Now,sinceshehas4lessthanthisamountincherrytrees,wesubtract4from27tofindthenumber
ofcherrytrees:
27cherrytrees-4=23cherrytrees.
TofindthetotalnumberoftreesMariahasinherorchard,weaddthenumberofappletreestothe
numberofcherrytrees:
81appletrees+23cherrytrees=104trees.
Theansweris 104 .C.2 MATH
OriginalQuestion
Thevalueoflog 28471isbetweentheconsecutiveintegersaandb. Finda+b.
10
SyntheticQuestionw/oVerification
Thevalueoflog 16384isbetweentheconsecutiveintegersxandy. Findx+y.
2
VerificationComment
Toensurethequestioncanbesolved,weneedtocheckifthelogarithmgivencanbeevaluatedand
fallsbetweentwoconsecutiveintegers.
1. Identifythevalueof2n thatequals16384.
2. If16384isapowerof2,thenlog 16384willbeaninteger.
2
3. Checktoensuretheresultisanintegerandfindtheconsecutiveintegersaroundit.
Let’sperformthecheck:
20 = 1,21 = 2,22 = 4,23 = 8,24 = 16
...
214 = 16384
So,log 16384 = 14.
2
Theconsecutiveintegersaround14are(13and14)or(14and15).
Sincethelogarithmgivesanexactinteger, tomakethequestionvalid, wewillmodifyittousea
numberthatisnotanexactpowerof2.
MODIFIEDCREATEDQUESTION:
Thevalueoflog 10000isbetweentheconsecutiveintegersmandn. Findm+n.
2
Now,let’scheckthemodifiedquestion:
1. Since10000isnotapowerof2,log 10000willnotbeaninteger.
2
2. Thegoalistofindtwoconsecutiveintegersbetweenwhichthislogarithmvaluelies.
3. Knowingthat213 = 8192and214 = 16384,log 10000isbetween13and14.
2
Theconsecutiveintegersaroundlog 10000are13and14.
2
SyntheticQuestionwithVerification
Thevalueoflog 10000isbetweentheconsecutiveintegersmandn. Findm+n.
2
SyntheticSolutionandFinalAnswer
Tofindtheconsecutiveintegersbetweenwhichlog 10000lies,weneedtofindthepowersof2that
2
areimmediatelylessthanandgreaterthan10000.
First,let’sfindthelargestpowerof2thatislessthan10000. Startingwith21,22,...andsoon,we
find:
210 = 1024,211 = 2048,212 = 4096,213 = 8192,214 = 16384
Here,weseethat213 < 10000 < 214,whichmeansthatlog 10000isgreaterthan13butlessthan
2
14.
Therefore,theconsecutiveintegersmandnbetweenwhichlog 10000liesarem = 13andn = 14.
2
Nowwewilladdthesetwointegerstofindtherequiredsum:
m+n = 13+14 = 27.
Theansweris 27 .D CaseStudyofModelResponse
D.1 GSM8K
QuestionandGroundTruthSolution
Question: A treasure hunter found a buried treasure chest filled with gems. There were 175
diamonds,35fewerrubiesthandiamonds,andtwicethenumberofemeraldsthantherubies. How
manyofthegemswerethereinthechest?
Solution: There were 175 - 35 = 140 rubies in the treasure chest. There were 140 x 2 = 280
emeraldsinthetreasurechest. So,thetotalnumberofgemsinthetreasurechestis175+140+280
= 595 .
Response–Xwin-Math-7B(7.5kdata)GreedyDecode
First,let’sfindoutthenumberofrubies. Wearetoldthereare35fewerrubiesthandiamonds. Since
thereare175diamonds,wecancalculatethenumberofrubiesasfollows:
Numberofrubies=Numberofdiamonds-35=175-35=140
Next, we need to find out the number of emeralds. We are told there are twice the number of
emeraldsthantherubies. Sowecancalculatethenumberofemeraldsasfollows:
Numberofemeralds=2×Numberofrubies=2×140=280
Now,wecanfindthetotalnumberofgemsbyaddingthenumberofdiamonds,rubies,andemeralds
together:
Total number of gems = Number of diamonds + Number of rubies + Number of emeralds =
175+140+280=605
Theansweris: 605 . Incorrect WrongCalculation
Response–Xwin-Math-7B(7.5kdata)OneSamplefromTemperature=0.7
Let’sfindthenumberofeachtypeofgeminthecheststepbystep.
First,wearegiventhenumberofdiamonds:
Numberofdiamonds=175
Next,wearetoldthereare35fewerrubiesthandiamonds. Tofindthenumberofrubies,wesubtract
35fromthenumberofdiamonds:
Numberofrubies=Numberofdiamonds-35=175-35=140
Then, wearetoldtherearetwicethenumberofemeraldsthantherubies. Tofindthenumberof
emeralds,wemultiplythenumberofrubiesby2:
Numberofemeralds=2*Numberofrubies=2*140=280
Now, we add the number of diamonds, rubies, and emeralds together to find the total number of
gemsinthechest:
Totalnumberofgems=Numberofdiamonds+Numberofrubies+Numberofemeralds=175+
140+280=595
Theansweris: 595 . Correct
Response–Xwin-Math-7B(960kdata)GreedyDecode
Let’sbreakdowntheinformationgivenintheproblemstepbystep.
1. Weknowthereare175diamonds.
2. Thereare35fewerrubiesthandiamonds. Sincethereare175diamonds,wecancalculatethe
numberofrubiesbysubtracting35from175.
Numberofrubies=Numberofdiamonds-35=175-35=140
3. Therearetwicethenumberofemeraldsthantherubies. Wecanfindthenumberofemeraldsby
multiplyingthenumberofrubiesby2.
Numberofemeralds=2×Numberofrubies=2×140=280
Now, tofindthetotalnumberofgemsinthechest, weaddthenumberofdiamonds, rubies, and
emeraldstogether.
Totalnumberofgems=Numberofdiamonds+Numberofrubies+Numberofemeralds=175+
140+280=595
Theansweris: 595 . CorrectD.2 MATH
QuestionandGroundTruthSolution
Question: Iff(x) = 3x−2,whatisthevalueoff(−2)+f(−1)+f(0)? Expressyouranswerasa
x−2
commonfraction.
Solution: f(−2)+f(−1)+f(0) = 3(−2)−2 +3(−1)−2 +3(0)−2 = −8+−5+−2 = 2+5+1 = 14
−2−2 −1−2 0−2 −4 −3 −2 3 3
Response–Xwin-Math-7B(7.5kdata)GreedyDecode
Tofindthevalueoff(−2)+f(−1)+f(0),weneedtoevaluatethefunctionf(x)atthesethree
valuesofx.
First,let’sfindf(−2): f(x) = 3x−2 Pluginx = −2: f(−2) = 3(−2)−2 = −6−2 = −8
x−2 −2−2 −4 −4
Now,let’sfindf(−1): Pluginx = −1: f(−1) = 3(−1)−2 = −3−2 = −5
−1−2 −3 −3
Finally,let’sfindf(0): Pluginx = 0: f(0) = 3(0)−2 = −2 = 1
0−2 −2
Now,wecanfindthesum: f(−2)+f(−1)+f(0) = −8 + −5 +1
−4 −3
Toaddthesefractions,weneedtofindacommondenominator,whichistheleastcommonmultiple
(LCM)ofthe threedenominators(-4,-2,and-3) . TheLCMofthesedenominatorsis-12.
Now,wecanrewriteeachfractionwiththecommondenominator:
−8 = 4 , −5 = 5,1 = 12
−4 3 −3 3 12
Unreasonable
Now,wecanaddthefractions: 4 + 5 + 12 = 4+5+12 = 19
3 3 12 3 3
Theansweris: 19 . Incorrect
3 Wrongcalculation
Response–Xwin-Math-7B(7.5kdata)OneSamplefromTemperature=0.7
Tofindthevalueoff(−2)+f(−1)+f(0),wefirstneedtoevaluateeachfunctionatthesespecific
valuesofx.
Forf(−2): f(x) = 3x−2 Substitutex = −2intotheequation: f(−2) = 3(−2)−2 = −6−2 = −8 =
x−2 −2−2 −4 −4
2
Forf(−1): f(x) = 3x−2 Substitutex = −1intotheequation: f(−1) = 3(−1)−2 = −3−2 = −5 =
x−2 −1−2 −3 −3
5
3
Forf(0): f(x) = 3x−2 Substitutex = 0intotheequation: f(0) = 3(0)−2 = −2 = 1
x−2 0−2 −2
Nowwecanaddthevaluesoff(−2),f(−1),andf(0): f(−2)+f(−1)+f(0) = 2+ 5 +1 = 14
3 3
Theansweris: 14 . Correct
3
Response–Xwin-Math-7B(480kdata)GreedyDecode
Tofindthevalueoff(−2)+f(−1)+f(0),weneedtoevaluatethefunctionf(x)ateachofthe
givenpointsandthensumtheresults.
First,let’sfindf(−2): f(−2) = 3(−2)−2 = −6−2 = −8 = 2
(−2)−2 −2−2 −4
Next,let’sfindf(−1): f(−1) = 3(−1)−2 = −3−2 = −5 = 5
(−1)−2 −1−2 −3 3
Finally,let’sfindf(0): f(0) = 3(0)−2 = 0−2 = −2 = 1
0−2 −2 −2
Now, we can sum these values: f(−2)+f(−1)+f(0) = 2+ 5 +1 To add these, we need a
3
commondenominator,whichis3: 2+ 5 +1 = 2·3 + 5 + 1·3 = 6 + 5 + 3
3 3 3 3 3 3 3
Now,weaddthenumerators: 6 + 5 + 3 = 6+5+3 = 14
3 3 3 3 3
Theansweris: 14 . Correct
3