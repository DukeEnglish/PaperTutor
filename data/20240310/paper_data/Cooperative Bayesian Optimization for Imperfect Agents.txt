Cooperative Bayesian Optimization
for Imperfect Agents
Ali Khoshvishkaie1, Petrus Mikkola1, Pierre-Alexandre Murena1,2, and Samuel
Kaski1,3
1 Department of Computer Science, Aalto University, Helsinki, Finland
2 Hamburg University of Technology, Hamburg, Germany
3 Department of Computer Science, University of Manchester, Manchester, UK
firstname.lastname @aalto.fi
{ }
Abstract. We introduce a cooperative Bayesian optimization problem
for optimizing black-box functions of two variables where two agents
choose together at which points to query the function but have only
control over one variable each. This setting is inspired by human-AI
teamwork,whereanAI-assistanthelpsitshumanusersolveaproblem,in
thissimplestcase,collaborativeoptimization.Weformulatethesolution
as sequential decision-making, where the agent we control models the
userasacomputationallyrationalagentwithpriorknowledgeaboutthe
function. We show that strategic planning of the queries enables better
identification of the global maximum of the function as long as the user
avoids excessive exploration. This planning is made possible by using
BayesAdaptiveMonteCarloplanningandbyendowingtheagentwitha
usermodelthataccountsforconservativebeliefupdatesandexploratory
sampling of the points to query.
1 Introduction
Human-AI cooperation refers to the collaboration between human and artificial
intelligence (AI) driven agents to achieve a common goal [16]. In the cooperative
scenario, the agents work autonomously but interdependently, each leveraging
their unique skills and abilities to collectively reach the shared objective. The
cooperation between a human and an AI agent can be impaired by limitations
intheirinformationprocessingabilitiesandvariousotherfactorssuchasbiases,
heuristics, and incomplete knowledge [10]. It has already been established that
anycooperationismoreeffectivewhentheinvolvedagentshaveatheoryofmind
of the others [7]. It would therefore be helpful if the AI agent could take into
account the human’s information processing capabilities and biases and adapt
to the changing needs and preferences of the human user [19].
Aspecifickindofhuman-AIcooperationiswhenthedecisionisjointlytaken
by two agents for a common goal, and each controls only their part of the de-
cision. An illustrative example is Hand and Brain chess, a team chess variant
in which two players (the Hand and the Brain) play on each side. Each move
is jointly decided by the team, with the Brain calling out a piece and the Hand
4202
raM
7
]GL.sc[
1v24440.3042:viXra2 A. Khoshvishkaie et al.
Fig.1. Interaction scenario between the user and the AI agent in the optimization
task. Unlike a greedy agent (a), the AI agent we propose (b) has a model of the user
and plans its actions by anticipating the user’s behaviour. This results in a more ef-
ficient cooperative exploration of the domain, and therefore avoids getting stuck in a
local optimum. This is visible in the right-hand side plots, showing the corresponding
trajectories of queries to the function f.
being responsible for moving it. In this game, the Brain should essentially con-
sider a move that is understandable for the Hand. Otherwise, the Hand moves
the piece to a strategically bad position, resulting in a disastrous move. If each
player carries out their task without anticipating the other team member, the
teamwillenduptakingasub-optimalaction.Theanticipationisdonebybuild-
ing a model of the partner.
To study this setup in a controlled environment, we propose a cooperative
Bayesian optimization task. The AI agent and human user aim to perform a
sequential black-box optimization task in a 2D space. At each step, the human-
AI team chooses a point to query the function. The choice is made by the AI
agentoptingforthefirstcoordinateandthenthehumanuserselectingtheother
one. In this optimization task, the human user and the AI agent, both with
partial information, cooperatively take part in data acquisition. We formulate
this cooperative data acquisition as a repeated Bayesian game between the user
and the agent played for a finite horizon. The contributions of this paper are:
– We propose a collaborative AI algorithm for settings where the AI agent
plansitsactionbyassessingtheuser’sknowledgeanddecisionprocesswith-
out any prior interaction with the user.Cooperative Bayesian Optimization for Imperfect Agents 3
– Weshowempiricallythatthealgorithmisabletolearntheuser’sbehaviour
in an online setting and use it to anticipate the user’s actions.
– We show empirically that the algorithm helps the team in the optimization
task (measured as the team optimization score) compared to various base-
lines,suchasagreedyalgorithmthatmaximizesitsownbeliefs.Thisisdone
by helping a better exploration of the domain of the function.
2 Cooperative Bayesian Optimization
2.1 Problem Formulation
We consider a problem where a team of two agents, the human user and the
AI agent, aims to maximize a black-box function f : of two parameters
X ×Y
(x,y) . Note here that the function is not necessarily 2-dimensional.
∈ X ×Y
The team explores the domain by acquiring new observations of f. The
X ×Y
exploration consists of a sequence of queries of f at points (x ,y ) .
t t
The outcomes of the query are noisy and we denote by f¯(x,y) the o∈ utcX om× eY of
the query at point (x,y). In this respect, the task of the team is similar to a
Bayesian Optimization (BO) task.
TheteamproceedsbysequentiallyqueryingT points.Ateachstept,theteam
adoptsthefollowingprotocolforthechoiceof(x ,y ),presentedinFigure1.The
t t
AIagentselectsx first.Thehumanuserobservesthevalueofx pickedby
t t
∈X
the AI agent and then selects y . Finally, both agents observe the selected
t
∈ Y
tuple (x ,y ) and the value of f(x ,y ). In this paper, we adopt the point of
t t t t
view of the AI agent and therefore focus on how to optimally select the first
coordinate x . It is important to mention that and are not necessarily
t
X Y
one-dimensional, but can describe any two sets of variables.
The final performance of the optimization process is measured by the opti-
mization score (described in Section 4.2). We view this score as a more under-
standablealternativetothedirectlyrelatedmeasureofsimpleregret,definedas
f f .
∗
−
T∗
2.2 Mathematical Formalization
We address the problem of the AI agent as a repeated Bayesian game, using
the formalism of model-based reinforcement learning, considering the AI agent
as a decision-making agent interacting with an environment made up of the
function f and the human user. In this environment, the agent takes actions
(choice of a coordinate x ) and gets rewarded depending on the action x , the
t t
user’s choice y and the value of the function f(x ,y ).
t t t
We describe the agent’s decision-making problem as Partially Observable
Markov Decision Process (POMDP) = , , ,Ω, , , where the nota-
M ⟨A S T O R⟩
tions are explained in what follows.
Thespace isthespaceoftheactionsavailabletotheagent.Inourcontext,
A
it corresponds to the set of points available to the agent. For this reason, we
X4 A. Khoshvishkaie et al.
will use x (instead of the standard notation a usually used in POMDPs) to
designate the coordinate chosen by the agent. A state s describes a state
∈ S
of the agent’s environment, which is made up of the function f and the user. A
state is then defined as a tuple s = (f AI,θ), where f AI : R is agent’s
X ×Y →
estimation of function f and θ is a parameters set characterizing the user. The
transition (s,x,s)measurestheprobabilityofatransitionfromstatestostate
′
T
s after agent’s action x. By definition, the function f is fixed, and consequently
′
the transition probability can be written as:
T
T(s,x,s ′)=I(f
AI
=f A′I)p(θ
′
|s,x) (1)
where s = (f AI,θ), s ′ = (f A′I,θ ′) and I is the identity function. An observation
ω Ω corresponds to the user’s choice y and the value of f at point (x,y), i.e.
∈
Ω = R. The observation prediction (ω s,x) [0,1] is the probability that
Y× O | ∈
ω Ω is observed after action x has been played within environment state s.
∈
Unlikesomeothersettings,theobservationprediction inourcontextdoesnot
O
dependonthenewstate,butonlyonthestatebeforetheaction.Thisprobability
decomposes as
(ω =(y,z)s,x)=p(y s,x)p(f¯(x,y)=z s,x,y) (2)
O | | |
where the probability p(y s,x) corresponds to user’s decision-making and the
probability p(f¯(x,y) = z| s,x,y) to function sampling. The reward (x,s,ω)
| R
measures the pay-off of agent’s action x in state s after observing ω. The choice
of the reward function in our implementation will be discussed in Section 3.4.
2.3 User Model
InEquations1and2,theprobabilitiesp(θ s,x)andp(y s,x)describetheuser’s
′
| |
behaviour, that is, how the user updates their beliefs and how they make deci-
sions. We note that p(θ s,x) can be decomposed as
′
|
p(θ s,x)= p(θ s,x,y,f(x,y)=z)p(f(x,y)=z s,x,y)p(y s,x)dz (3)
′ ′
| | | |
Zz y
X
where the term p(f(x,y) = z s,x,y) does not depend on the user. Therefore,
|
the user’s behaviour is fully defined by p(θ s,x,y,f(x,y)) and p(y s,x).
′
| |
Inthefollowing,wewillcallthetuple(p(θ s,x,y,f(x,y)),p(y s,x))theuser
′
| |
model. The user model describes the role played by the user within the environ-
ment of the agent. In practice, it will be used to simulate the behaviour of the
user,whichisusefulinparticularwhenplanningfortheactiontoplay.Theuser
model is not necessarily an accurate description of the user’s behaviour, but is
a model used by the agent for making decisions. The choice of this model will
restrict the possibilities of behaviours that the agent will be able to consider. In
thecasewheretheuserishuman,ausefulusermodelshouldbeabletodescribe
computationally rational behaviours [8].Cooperative Bayesian Optimization for Imperfect Agents 5
3 Implementation
In this section, we introduce the practical solution to the Cooperative Bayesian
Optimization problem, considering a minimal user model. This model describes
auserwithpartialknowledgeaboutthefunction,abletoupdatetheirbeliefand
select their actions in a way that balances exploitation and exploration.
3.1 Bayes Adaptive Monte Carlo Planning
In order to solve the POMDP introduced in Section 2.2 and plan the AI agent’s
actions, we rely on a Bayesian model-based Reinforcement learning method.
This method is used to perform a zero-shot planning, where the agent has no
initial information about the user’s behaviour. At each iteration, the model is
updated based on the previous user’s actions, and a zero-shot planning method
is employed to plan for the future.
In order to solve the POMDP, the posterior distribution of the parameters
is estimated using the inference method described in Section 3.3 below. This
posterior distribution is used to plan the actions x by enabling a Monte-Carlo
t
estimation of the value of each action: At each iteration, we run several sim-
ulations with fixed state s sampled from the posterior distribution. In these
t
conditions, having a fixed and known state transforms the POMDP into a sim-
pleMDP:Thismakesitpossibletocomputethevalueoftheactionforthisstate
and,consequently,togetaMonte-Carloestimationofthevalueofanaction.Fi-
nally,theactionthatmaximizestheestimatedvalueischosen.Ithasbeenproven
thatthisprocessconvergestotheBayes-optimalpolicywithinfinitesamples[9].
3.2 User Model Specification
Weproposeasimpleusermodeldescribingacomputationallyrationaluserwith
partial knowledge about the function to be optimized. This user model is an
instantiation of the general form of user models as introduced in Section 2.3.
User’s Knowledge. We represent the user’s partial knowledge of the function f
using a Gaussian Process [18]. A Gaussian Process (GP) is a stochastic pro-
cess over real-valued functions, such that every finite collection of these random
variables has a multivariate normal distribution. We will denote this GP at
step t as f(t). We emphasize that f is not a function, but a prior over func-
um um
tions R. This choice is motivated by the observation that Bayesian
X ×Y →
Optimization based on GPs provides a surprisingly good framework to explain
active function learning and optimization in humans [2].
The user’s GP is assumed to have been initialized based on the observation
of a collection of N points = (xu,yu,f¯(xu,yu)) , using GP re-
u Du { i i i i }i=1,...,Nu
gression. For any unseen function value f(x,y), GP regression models this as a
Gaussian random variable with closed-form mean and variance (see [18], Equa-
tions(2.23)and(2.24)).Theequationsrequirespecifyingthecovariance(kernel)
function, which in this paper is taken to be the squared exponential kernel [18,6 A. Khoshvishkaie et al.
Eq. (2.16)]. The hyperparameters of the kernel function are optimized by maxi-
mizing the marginal likelihood.
Belief Update. The values of the function f sampled during the interaction are
observed by the user and used to sequentially update their GP f . At time t,
um
the user’s GP f(t) is updated by observing = (x ,y ),f¯(x ,y ) . We denote
um t t t t t
D { }
by (f(t) (x ,y ),f¯(x ,y ) ) the GP obtained after Bayes optimal belief
bayes um t t t t
B |{ }
updating, defined as the standard updates (Equations (2.23) and (2.24) in [18]).
However, it has been documented in behavioural studies [6] that humans
deviate from the Bayesian optimal belief update, because of various cognitive
biases [23]. Consequently, in our user model, we consider the conservative belief
updating operator introduced by Kovach [13]:
B
f(t+1) =αf(t) +(1 α) (f(t) (x ,y ),f¯(x ,y ) ), (4)
um um − Bbayes um|{ t t t t }
where α [0,1] represents the degree of conservatism. A low values of α corre-
∈
sponds to an almost Bayes-optimal behaviour, while the case α=1 corresponds
to the user ignoring the new observations and not updating their belief.
Decision-Making. Motivated by the observation of Borji and Itti [2], we model
the user’s choice of an action y as the maximization of an acquisition function
t
y A(x ,y). We consider the UCB acquisition function based on the GP f(t):
t um
7→
A t(y |x t)=E f u(t m)(x t,y) +β V f u(t m)(x t,y) (5)
r
h i h i
whereE[f u(t m)(x t,y)]andV[f u(t m)(x t,y)]arerespectivelythemeanandthevariance
of the GP f(t) at point (x ,y), and β [0,1] is an exploration-exploitation
um t
∈
trade-off parameter.Alowvalueofβ correspondstolessexplorativebehaviour,
exploiting the current belief over f, while a larger value corresponds to more
explorative behaviour, evaluating f at points with larger uncertainty. Given the
AI’s action x , a sensible choice of an action y for the user would consist in
t t
maximizing the acquisition function A (y x ).
t t
|
This choice of y by a maximization can be interpreted as an event of many
t
pairwise comparisons among different actions y : Choosing the action y
t
∈ X
means preferring it to all the others y = y . Inspired by [15], we build a prob-
t
̸
abilistic model of user preferences upon Thurstone’s law of comparative judg-
ment[22]byassumingthattheuser’sactiony givenx iscorruptedbyGaussian
t t
noise,
y =argmax(A (y x )+W(y)), (6)
t t t
y |
whereW isawhiteGaussiannoisewithmeanE[W(y)]=0andauto-correlation
E[W(y)W(y ′)] = σ2 if y = y
′
and 0 otherwise. The likelihood p(y
t
s t,x t) of a
|
single observation y x corresponding to this noise process takes the form
t t
|
m
A (y x ) A (y x )
t i t t t t
p(y s ,x )= 1 [Φ ϕ] | − | , (7)
t t t
| − ∗ σ
i=1(cid:18) (cid:18) (cid:19)(cid:19)
YCooperative Bayesian Optimization for Imperfect Agents 7
where Φ and ϕ are the cumulative and density function of the standard nor-
mal distribution, respectively, and is the convolution operator. To evaluate
∗
the likelihood, f(t) and A (y x ) should be computed recursively by using the
um t t
|
aforementioned equations. For fixed α and β, this is possible given the function
sampling data ( )T . The joint likelihood P (y x )T ,( )T α,β is the
Dt t=1 t | t t=1 Dt t=1
product of the single events y x for t=1,...,T.
t | t (cid:0) (cid:12) (cid:1)
(cid:12)
Summary: Definition of the User Model. The introduced user model is charac-
terizedbythreeparameters:theuser’sknowledgeofthefunctionf ,thedegree
um
of conservatism α and the degree of explorativeness β. Using the notations of
Section 2.2, we can write θ = (f ,α,β). We notice that these parameters are
um
of different natures though: α and β are characteristics of the user, while f
um
corresponds to a mental state, i.e. a description of what the user knows.
Whendefiningthebelief-updatingprobabilityp(θ s,a,y,f(a,y)),weassume
′
|
that the parameters α and β, as characteristics of the user, are stationary and
thereforearenotupdatedduringtheinteraction.Onlytheuser’sGPisupdated,
followingEquation4.Withourdefinitionofthisusermodel,theuser’sdecision-
making p(y s,a) is defined in Equation 7.
|
3.3 Inference of the User Model Parameters
The parameters θ = (f ,α,β) are not observed and need to be estimated on-
um
line during the interaction, based on the user’s actions. We adopt a Bayesian
approach and the inference consists of estimating, at each time step t, the pos-
terior distribution p α,β,f (y x )t ,( )t given the interaction data
(y x )t andthefunctionu sm ampliτ n| gτ daτ t= a1 ( D )τ tτ=1 with =(x ,y ,f¯(x ,y )).
τ | τ τ=1 (cid:0) (cid:12) Dτ τ=1 (cid:1) Dτ τ τ τ τ
For this, we use the following d(cid:12)ecomposition:
p α,β,f (y x )t ,( )t =
um τ | τ τ=1 Dτ τ=1
(cid:0) p α,β(cid:12)
(cid:12)
(y τ |x τ)t τ=1,( Dτ)t τ=1(cid:1) p f um (y τ |x τ)t τ=1,( Dτ)t τ=1,α,β
(cid:0) (cid:12) (cid:1) (cid:0) (cid:12) (cid:1)
Estimation of (α,(cid:12)β). The estimation of (α,β) i(cid:12)s done using Bayesian belief up-
date. The initial prior is chosen to be the uniform distribution over the unit
cube. The posterior distribution is approximated using the Laplace approxima-
tion, which consists in the following. The maximum a posteriori (MAP) esti-
mate (α ,β ) is computed by numerically maximizing the log posterior
MAP MAP
with the BFGS algorithm, which also approximates the Hessian. The posterior
p α,β (y x )t ,( )t is approximated as a Gaussian distribution cen-
τ | τ τ=1 Dτ τ=1
tered on (α ,β ) with the covariance matrix corresponding to the inverse
(cid:0) (cid:12) MAP MAP (cid:1)
of the (cid:12)negative Hessian at the MAP estimate.
Estimation of f . The update of f as given in Equation (4) is determinis-
um um
tic when α is given. Consequently, the term p f (y x )t ,( )t ,α,β
um τ | τ τ=1 Dτ τ=1
is trivial and does not need to be computed during the interaction. Since our
(cid:0) (cid:12) (cid:1)
planning algorithm (described in Section 3.1) relies(cid:12)on sampling from the pa-
rameters (α,β,f ), f is computed from the whole trajectory ( )t using
um um Dτ τ=18 A. Khoshvishkaie et al.
the sampled value of α. For the initialization f(0), we consider that the user has
um
a uniform prior over the function. This interprets as ignoring the fact that the
user has prior knowledge.
3.4 Choice of the Reward Function
The reward for the agent, as introduced in Section 2.2, is designed to be a
compromise of two parts, exposed in the following.
The first part is the expectation of the UCB score over the user’s future
action, calculated with f as estimated in the user model:
um
R1(x,s,ω)=Ey ∼Ausr[UCB(x,y)] (8)
Intuitively, this first part shows how desirable the point (x,y) is for the user
1
R
when the AI selects x. Therefore it values actions x for which the user is able to
find a reasonably good y to query the function. Since is based on the UCB
1
R
score, it also guarantees a trade-off between the exploration and exploitation of
the query point.
When the user’s behaviour is almost uniform over actions (e.g. when the
user is more explorative, because of having little knowledge of f or because
of a high β), reward is close to constant and is not enough to make good
1
R
choices of x . We solve this problem by introducing a second part in the reward
t
definition,thatisbasedontheAIagent’sknowledgeofthefunction.Thisreward
is defined as the average UCB score over the top K promising y values upon the
AI’s knowledge for a chosen action x:
1
(x,s,ω)= UCB(x,y) (9)
2
R K
y ∈toXpK(Aai)
Thisreducestheriskofrelyingtoomuchontheusermodel,whichisnotprefect,
especially at the beginning of the interaction.
Wedefinethe totalrewardas alinearcombinationofthese twocomponents:
(x,s,ω)= (x,s,ω)+C (x,s,ω) (10)
1 2
R R R
where C is a compromising factor between the two terms, a hyperparameter of
the proposed method.
4 Empirical Validation
Inthissection,westudytheperformanceofourmethodintheproposedcooper-
ative Bayesian game (Section 2). We examine scenarios where prior information
is unevenly distributed among agents and when the human user characteristics
vary. In particular, we are interested in how the user’s degree of conservatism
and explorativeness affect the outcome of the Bayesian game.4
4 Implementationofourmethodandsourcecodefortheexperimentsareavailableat
https://github.com/ChessGeek95/AI-assisted-Bayesian-optimization/.Cooperative Bayesian Optimization for Imperfect Agents 9
4.1 Experimental Setup
Domain. We choose as a function f a 3-modal variant of the Himmelblau func-
tion.Itisdefinedon[0,1]2 andhas3minima,locatedrespectivelyat(0.46,0.8),
(0.22,0.44) and (0.74,0.18). The amplitude of the maxima can be adjusted.
Experimental protocol. We consider a synthetic user whose characteristics can
be controlled. The agent follows the specification of a computationally rational
user presented in Section 3.2: we assume a user who follows a Bayesian opti-
mizationroutinebasedontheUCBacquisitionfunctionwithanexplorativeness
parameter β, and a conservative GP-based belief updating with a conservatism
parameter α. We create 2 2 configurations of the human user characteris-
×
tics by considering the possible combinations of the values α 0.1,0.6 and
∈ { }
β 0.2,0.7 . For example, the configuration α = 0.1 and β = 0.7 refers to a
∈ { }
human user who is conservative in belief updating but explorative in decision-
making. These values have been chosen to reflect the extremes, with the user
beingalmostcompletelyconservativeoralmostperfectlyBayesian,andtheuser
being almost exclusively exploitative or almost exclusively explorative.
We study the impact of this prior information by considering 3 3 config-
×
urations of prior information (see Section 2.2) as follows. We provide each of
the two agents with either N = 5 points around local maxima or the global
maximum, or no prior functions evaluations at all. We use the terms “Local”,
“Global”, and “None” to refer to these configurations by considering possible
permutations: (AI’s prior, human’s prior). The points are drawn from a multi-
normal distribution centered on the position of the maximum (local or global).
For example, (Global, Local) refers to the configuration, where the AI agent
hasN =5priorpointsaroundtheglobalmaximum,whilethehumanuseragent
has N =5 points around local maxima of the function.
Fortheexperiments,weconsideradiscretizationofthedomain intoa
X×Y
50 50 grid. Given a simulated user, each experiment consists of 20 interaction
×
steps.Theresultsareaveragedoverasampleof3differentfunctionsf,generated
asdescribedabove,and10differentpriorsamples(initialpointsavailabletoeach
agent before the interaction, see Section 2.3). For our agent, we use the reward
defined in Equation 10 with C =1.
All experiments were run on a private cluster consisting of a mixture of
Intel® Xeon® Gold 6248, Xeon® Gold 6148, Xeon® E5-2690 v3 and Xeon®
E5-2680 v3 processors.
Baselines. To investigate the strengths of our method, we compare it to four
baselines, two of which correspond to a single-agent Bayesian Optimization.
The single-agent BO baselines correspond to one single agent making the
decision, i.e., opting for both coordinates of the point to query, and therefore
correspondtothestandardBOproblem.Thebaselinesillustrateempiricallower
and upper bounds of the optimization performance:
- VanillaBO (random):Single-agentbaseline,queryingpoints(x,y)uniformly
at random on the domain . This is equivalent to two agents querying
X ×Y10 A. Khoshvishkaie et al.
coordinates randomly, which is a lower bound on the performance that any
team should at least achieve.
- VanillaBO (GP-UCB): Single-agent baseline, querying points (x,y) using an
upper confidence bound [4] score upon a Gaussian processes pre-trained on
the prior points. Since the agent has access to all prior data and absolute
control over both coordinates, this is an upper bound on the performance.
The value of β for this agent is chosen to be β = 0.05: it has been chosen
because it gives optimal results compared to other β.
We also compare the performance of our method to two other comparable
multi-agent BO algorithms, corresponding to different strategies for solving the
Cooperative Bayesian Optimization task:
- RandomAI: The AI agent chooses x uniformly at random on the domain .
X
- GreedyAI: The AI chooses x by picking the first coordinate of the UCB
score maximizer. It maximizes its own utility function (UCB score) without
considering the other agent, hence the name. As for the GP-UCB agent, the
value of β for this agent is also chosen to be β =0.05,
4.2 Experiments
Experiment 1: Evolution of the optimization performance. We first
study the efficiency of our algorithm in helping the team in the optimization
task. To do so, we introduce, as a metric, the optimization score. We define
this score as the maximum function value f queried during the cooperative
t∗
game of t rounds. Since the objective function is normalized between 0 and
100,anoptimizationscoreof100denotesmaximumperformance(alsonotethat
simple regret=100 optimization score).
−
The evolution of the optimization performance over the optimization rounds
is presented in Figure 2. It can be seen that our method indeed reaches better
performance compared to the GreedyAI and RandomAI baselines. However, in
the initial rounds, GreedyAI displays much better performances (even better
than the VanillaBO (GP-UCB) agent): this is because GreedyAI exploits prior
informationandthereforeisquicklyabletoguidetheusertowardfindingalocal
maximum. However, once the optimum is found, it does not explore further and
doesnotfindanyglobaloptimum,unlikeourmethod,whichismoreexplorative
from the beginning. We also notice that the RandomAI has initial performance
close to the random VanillaBO baseline, but keeps improving: this is due to the
fact that this agent keeps exploring, but in a sub-optimal way. Finally, we still
notice that the VanillaBO (GP-UCB) baseline is indeed a valuable upper-bound
inthelong-term:eventhoughourStrategicAIhassimilarperformancesonthe
firstrounds,theAInothavingtotalcontrolovertheexplorationendsupmaking
slightly less optimal decisions.
Experiment 2: Impact of the user’s parameters. The results presented
in Figure 2 are averaged over all user parameters. To study the impact of theCooperative Bayesian Optimization for Imperfect Agents 11
Fig.2. Evolution of the optimization performance during the interaction. At the end
oftheinteraction,ouragent(StragicAI)getsbetterperformancethanotherbaselines.
ItperformsslightlyworsethantheVanillaBO (GP-UCB),because,unlikethisbaseline,
the StrategicAI does not have control over the full domain .
X ×Y
user’s conservatism and explorativeness on the optimization performance, we
exploit the possibility offered by a controlled synthetic user to directly interpret
the performance of our method in the case of various user profiles. The final
optimization score for different (α,β) configurations is reported in Table 1. The
scores are averaged over all combined prior knowledge configurations. These re-
sultsconfirmthattheAI’sstrategicplanningsignificantlyimprovesoptimization
performanceinallscenarioswhencomparedtogreedyorrandomstrategies,but
withthehighestmarginforconservativeusers.Thissuggeststhatstrategicplan-
ning is more crucial when users update their beliefs conservatively. In contrast,
the level of user exploration does not significantly affect the size of the margin.
As an addition to this experiment, we performed an ablation study to check
the role played by the choice of the reward (Equation 10), comparing the cases
where C = 1 (used in all other reported experiments) and where C = 0 (which
correspondstoreward introducedinEquation8).Theresultsrevealthatthe
1
R
performance of strategic AI deteriorates with an explorative user, by using
1
R
instead of the full reward (which corresponds to the case C =0).
R
Experiment 3: Impact of the prior knowledge allocation. Table 2 shows
the impact of the prior knowledge allocation on the optimization score when all
the(α,β)configurationsarecombined.TheresultsrevealthattheAI’sstrategic
planningimprovestheoptimizationperformanceregardlessoftheagentandthe
quality of prior knowledge they possess about the function. The only exception
occurs when both agents lack prior knowledge. This may harm the initialization
of the AI’s own Gaussian process belief. In such cases, early-round planning
becomes ineffective. It is worth mentioning that the performance gap between
the strategic AI agent and the greedy AI agent is usually most significant when12 A. Khoshvishkaie et al.
Table 1. Impact of the user’s conservativeness (α) and explorativeness (β) onto the
optimization score.
β =0.2 β =0.7
α=0.1 α=0.6 α=0.1 α=0.6
GP-UCB 88.9 21.4
±
StrategicAI, C =1 (ours) 77.5 24.8 76.2 23.6 79.3 24.7 73.5 23.2
± ± ± ±
StrategicAI, C =0 (ours) 79.6 24.3 77.6 24.7 75.5 25.2 66.6 23.3
± ± ± ±
GreedyAI 71.0 22.6 69.4 22.5 69.5 22.6 67.5 21.6
± ± ± ±
RandomAI 71.0 20.8 64.2 20.3 62.0 21.2 58.7 20.0
± ± ± ±
Random 52.3 9.1
±
the AI agent possesses high-quality prior information, as demonstrated by the
results in rows 1-3 of Table 2.
Experiment 4: User certainty about the global maximum. The opti-
mization score alone may not provide a complete picture of the performance of
collaboration, as the team may achieve a high function value but not “know”
whetheritisindeedclosetotheglobalmaximum.Suchcertaintyrequiresknowl-
edge of the overall domain, which in turn necessitates exploration. To assess the
level of exploration and knowledge, we examine the flatness of the distribution
of the maximum based on the agent’s belief over the function, represented as
p(z f ) := p(z = max f (x,y)). The degree of flatness is measured
∗ belief ∗ (x,y) belief
|
by differential entropy. Specifically, we are interested in how effectively the AI
agentcanincreasetheuser’scertaintyaboutthemaximum,whichwerefertoas
theuser certainty,H(p(z f )),whereH isthedifferentialentropyandf isthe
∗ u u
|
human user’s belief over the objective function. A higher user certainty value
means that the human user has a better understanding of the global maximum.
Table 2. Impact of the agents’ prior knowledge onto the optimization score. The
testedpriorsare:knowledgearoundtheglobaloptimum(G),knowledgearoundalocal
optimum (L) and no prior knowledge (N). Each prior condition is indicated with a
subscript: AI for the AI agent, u for the user.
Prior StrategicAI (ours) GreedyAI
G & G 76.3 23.3 63.6 18.6
AI u
± ±
G & L 75.4 23.7 67.4 23.2
AI u
± ±
G & N 74.0 25.1 61.3 18.6
AI u
± ±
L & G 79.0 23.0 75.8 22.7
AI u
± ±
L & L 82.1 23.8 70.1 25.5
AI u
± ±
L & N 80.0 23.5 75.1 23.6
AI u
± ±
N & N 69.5 23.3 72.1 20.4
AI u
± ±Cooperative Bayesian Optimization for Imperfect Agents 13
Table 3 replicates Experiment 4.2, but instead of presenting the optimiza-
tion score, it shows the user certainty about the global maximum. The results
reveal that the AI using a random strategy is the most effective approach to
reducing the user’s uncertainty about the global maximum, and that there is a
considerable amount of unexplored space left after T =20 rounds when AI acts
strategically or greedily. However, the results also indicate that with strategic
planning, the user’s understanding of the global maximum is slightly improved,
regardless of whether they are conservative or Bayesian users and whether they
are explorative or exploitative. In addition, the observation that strategic plan-
ningenablesuserstoexploremorespaceisalsosupportedbyavisualinspection
of some of the experimental trials, which can be found in the appendix.
Table 3. User certainty about the global maximum at the end of the game.
β =0.2 β =0.7
α=0.1 α=0.6 α=0.1 α=0.6
StrategicAI 1.54 0.13 1.59 0.15 1.56 0.17 1.56 0.19
± ± ± ±
GreedyAI 1.66 0.05 1.67 0.06 1.68 0.06 1.68 0.06
± ± ± ±
RandomAI 1.27 0.17 1.22 0.19 1.16 0.22 1.17 0.22
± ± ± ±
5 Related work
Decomposition-based optimization. The proposed cooperative BO game resem-
bles a decomposition-based optimizer. Decomposed optimization partitions the
dimensions of the optimized function into disjoint subsets and optimizes sepa-
ratelyoverthesepartitions[5].Twopopularfamiliesofdecomposition-basedop-
timizersarecoordinatedescentbasedmethods[11]andcooperativeco-evolutionary
algorithms [17]. Recently, [12] proposed a decomposition-based optimization al-
gorithm for large-scale optimization problems, which is based on Bayesian op-
timization. However, this literature is focused on algorithmic optimization and
does not address the problem from the multi-agent learning perspective.
Multi-agent Bayesian Optimization. TheclosesttoourworkiscollaborativeBO
which considers multiple parties optimizing the same objective function. Still,
the utility from evaluating the function is individual, as [20], where a trusted
mediatorselectsaninputquerytobeassignedtoeachpartywhothenevaluates
the objective function at the assigned input. The main difference with our work
is the absence of this mediator. In other words, in our setting, the parties have
autonomy over their own decisions.
Human-Agent Teaming. The autonomy mentioned above is a crucial charac-
teristic of human–autonomy teams (HATs), where autonomous agents with a14 A. Khoshvishkaie et al.
partial or high degree of self-governance work toward a common goal [14,16].
The HAT literature offers numerous testbeds that enable researchers to design
algorithms and evaluate performance; a selection of these is presented in [16].
One such testbed is the game of Hanabi, which is a cooperative card game of
imperfect information for two to five players [1]. Although the proposed coop-
erative BO game is similar to Hanabi, the crucial difference is that we do not
allow direct communication, which would make collaboration easier and focus
the solution on designing the communication aspects. By contrast, in Hanabi,
players can exchange hints as a means of communication. This idea of commu-
nicationisinherenttothewholefieldofCooperativeGameTheory[3],inwhich
cooperationismadepossiblebyusingbindingagreements.However,thisdomain
mainly focuses on matrix games and not sequential repeated games in extensive
form. Recently, Sundin et al. [21] considered a similar problem for an applica-
tion to molecular design. In this work, the first agent’s action corresponds to a
restrictionofasearchspace,andthesecondagent’sactiontopickingwithinthe
restrictedspace.Thisdiffersfromourworkinthatthefunctiontheyoptimizeis
knownbythesecondagentbutnotobservedbythefirstagent,whileweconsider
a function unknown by both agents and the samples of which are observed.
6 Conclusion
WeintroducedacooperativesetupforBayesianoptimizationofafunctionoftwo
parameters,whereauserandanAIagentsequentiallyselectonecoordinateeach.
The case where the AI agent chooses first is difficult because the agent cannot
know the user’s action. Therefore, we endow the AI agent with a model of the
user,i.e.aprobabilisticdescriptionoftheuser’sbehaviouranddecision-making.
We use this model within a Bayes Adaptive Monte Carlo Planning algorithm
to simulate the user’s behaviour. The AI agent’s strategic planning of actions
enables making choices adapted to the user’s biases and current knowledge of
the domain. We showed empirically that our method, based on a simple user
model, leads to better optimization scores than a non-strategic planner. Even
though our algorithm is, in principle, adapted to be used with human users, the
currentimplementationisyettoocomputationallyexpensivetoworkinreal-time
(calculation time of the order of a minute per action). Alleviating this issue is
an important future work to make our method usable in real-world applications
with real users.
Acknowledgements
This research was supported by EU Horizon 2020 (HumanE AI NET, 952026)
and UKRI Turing AI World-Leading Researcher Fellowship (EP/W002973/1).
Computational resources were provided by the Aalto Science-IT project from
Computer Science IT. The authors would like to thank Prof. Frans Oliehoek
and Dr. Mert Celikok for their help in setting up the project and the reviewers
for their insightful comments.Cooperative Bayesian Optimization for Imperfect Agents 15
References
1. Bard, N., Foerster, J.N., Chandar, S., Burch, N., Lanctot, M., Song, H.F.,
Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., et al.: The hanabi challenge:
A new frontier for ai research. Artificial Intelligence 280, 103216 (2020)
2. Borji,A.,Itti,L.:Bayesianoptimizationexplainshumanactivesearch.In:Burges,
C., Bottou, L., Welling, M., Ghahramani, Z., Weinberger, K. (eds.) Advances in
Neural Information Processing Systems. vol. 26. Curran Associates, Inc. (2013)
3. Chalkiadakis, G., Elkind, E., Wooldridge, M.: Cooperative game theory: Basic
concepts and computational challenges. IEEE Intelligent Systems 27(3), 86–90
(2012)
4. Cox,D.D.,John,S.:Astatisticalmethodforglobaloptimization.In:Proceedings
ofthe1992IEEEInternationalConferenceonSystems,Man,andCybernetics.pp.
1241–1246. IEEE (1992)
5. Duan,Q.,Shao,C.,Qu,L.,Shi,Y.,Niu,B.:Whencooperativeco-evolutionmeets
coordinatedescent:Theoreticallydeeperunderstandingsandpracticallybetterim-
plementations.In:2019IEEECongressonEvolutionaryComputation(CEC).pp.
721–730. IEEE (2019)
6. El-Gamal, M.A., Grether, D.M.: Are people Bayesian? Uncovering behavioral
strategies. Journal of the American Statistical Association 90(432), 1137–1145
(1995)
7. Etel,E.,Slaughter,V.:Theoryofmindandpeercooperationintwoplaycontexts.
Journal of Applied Developmental Psychology 60, 87–95 (2019)
8. Gershman, S.J., Horvitz, E.J., Tenenbaum, J.B.: Computational rationality: A
converging paradigm for intelligence in brains, minds, and machines. Science
349(6245), 273–278 (2015)
9. Guez, A., Silver, D., Dayan, P.: Scalable and efficient bayes-adaptive reinforce-
ment learning based on monte-carlo tree search. Journal of Artificial Intelligence
Research 48, 841–883 (2013)
10. Helander, M.G.: Handbook of human-computer interaction. Elsevier (2014)
11. Hildreth,C.:Aquadraticprogrammingprocedure.NavalResearchLogisticsQuar-
terly 4(1), 79–85 (1957)
12. Jiang,P.,Cheng,Y.,Liu,J.:CooperativeBayesianoptimizationwithhybridgroup-
ing strategy and sample transfer for expensive large-scale black-box problems.
Knowledge-Based Systems 254, 109633 (2022)
13. Kovach, M.: Conservative updating. arXiv preprint arXiv:2102.00152 (2021)
14. Larson, L., DeChurch, L.A.: Leading teams in the digital age: Four perspectives
on technology and what they mean for leading teams. The leadership quarterly
31(1), 101377 (2020)
15. Mikkola, P., Todorovi´c, M., Ja¨rvi, J., Rinke, P., Kaski, S.: Projective Preferential
Bayesian Optimization. In: Proceedings of the 37th International Conference on
Machine Learning. pp. 6884–6892. PMLR (2020)
16. O’Neill, T., McNeese, N., Barron, A., Schelble, B.: Human–autonomy teaming:
A review and analysis of the empirical literature. Human Factors 64(5), 904–938
(2022)
17. Potter, M.A., De Jong, K.A.: A cooperative coevolutionary approach to function
optimization.In:ProceedingsoftheInternationalConferenceonParallelProblem
Solving from Nature. pp. 249–257. Springer (1994)
18. Rasmussen,C.E.,Williams,C.K.I.:Gaussianprocessesformachinelearning.Adap-
tive Computation and Machine Learning, MIT Press (2006)16 A. Khoshvishkaie et al.
19. Sears, A., Jacko, J.A.: Human-computer interaction fundamentals. CRC Press
(2009)
20. Sim,R.H.L.,Zhang,Y.,Low,B.K.H.,Jaillet,P.:CollaborativeBayesianoptimiza-
tion with fair regret. In: Proceedings of the International Conference on Machine
Learning. pp. 9691–9701. PMLR (2021)
21. Sundin, I., Voronov, A., Xiao, H., Papadopoulos, K., Bjerrum, E.J., Heinonen,
M., Patronov, A., Kaski, S., Engkvist, O.: Human-in-the-loop assisted de novo
molecular design. Journal of Cheminformatics 14(1), 1–16 (2022)
22. Thurstone, L.L.: A law of comparative judgment. Psychological Review 101(2),
266 (1994)
23. Tversky, A., Kahneman, D.: Judgment under uncertainty: Heuristics and biases:
Biases in judgments reveal some heuristics of thinking under uncertainty. Science
185(4157), 1124–1131 (1974)Supplementary Material for
Cooperative Bayesian optimization for imperfect
agents
Ali Khoshvishkaie1, Petrus Mikkola1, Pierre-Alexandre Murena1,2, and Samuel
Kaski1,3
1 Department of Computer Science, Aalto University, Helsinki, Finland
2 Hamburg University of Technology, Hamburg, Germany
3 Department of Computer Science, University of Manchester, Manchester, UK
firstname.lastname @aalto.fi
{ }
1 Experiment 1: Impact of a known user model
Table 1 compares our strategic planning performance scores to an unrealistic
case when having a perfect user model for all prior initialization conditions.
Surprisingly, our method is performing comparably well, showing that the user
model is able to capture essential characteristics of the user eventually.
Table 1. Impact of the knowing the user’s prior knowledge and parameters onto the
optimization score. The tested priors are: knowledge around the global optimum (G),
knowledgearoundalocaloptimum(L)andnopriorknowledge(N).Eachpriorcondi-
tion is indicated with a subscript: AI for the AI agent, u for the user.
Prior StrategicAI (ours) StrategicAI+U
G & G 76.3 23.3 72.0 22.2
AI u
± ±
G & L 75.4 23.7 73.9 25.0
AI u
± ±
G & N 74.0 25.1 70.8 24.7
AI u
± ±
L & G 79.0 23.0 81.7 22.4
AI u
± ±
L & L 82.1 23.8 83.7 23.9
AI u
± ±
L & N 80.0 23.5 80.7 23.9
AI u
± ±
N & N 69.5 23.3 71.2 24.7
AI u
± ±
2 Experiment 2: Full Variability Study
Table 2 presents the performance scores for all prior initialization conditions,
and for all values of the parameters of the user model.
4202
raM
7
]GL.sc[
1v24440.3042:viXra2 A. Khoshvishkaie et al.
Table 2. Impact of the agents’ prior knowledge onto the optimization score. The
testedpriorsare:knowledgearoundtheglobaloptimum(G),knowledgearoundalocal
optimum (L) and no prior knowledge (N). Each prior condition is indicated with a
subscript: AI for the AI agent, u for the user.
β =0.2 β =0.7
Prior Method α=0.1 α=0.6 α=0.1 α=0.6
StrategicAI 77.2 ± 21.4 72.6 ± 20.4 80.8 ± 19.1 74.6 ± 18.6
G & G
AI u
GreedyAI 63.0 ± 15.5 64.2 ± 17.3 63.3 ± 13.9 64.0 ± 15.4
StrategicAI 68.3 ± 21.5 76.4 ± 21.4 79.3 ± 21.4 77.8 ± 17.2
G & L
AI u
GreedyAI 66.9 ± 21.3 67.3 ± 20.5 68.4 ± 20.1 67.1 ± 20.4
StrategicAI 80.1 ± 21.3 69.7 ± 21.8 72.9 ± 24.3 73.2 ± 21.4
G & N
AI u
GreedyAI 63.0 ± 15.5 57.9 ± 16.6 64.0 ± 17.4 60.3 ± 13.9
StrategicAI 80.8 ± 19.9 78.1 ± 18.5 81.8 ± 19.3 75.3 ± 19.5
L & G
AI u
GreedyAI 77.2 ± 19.3 75.1 ± 20.4 76.8 ± 19.1 73.9 ± 18.3
StrategicAI 83.5 ± 20.0 84.0 ± 20.3 85.8 ± 18.8 75.2 ± 20.6
L & L
AI u
GreedyAI 71.2 ± 23.5 69.3 ± 22.2 70.9 ± 23.5 69.2 ± 22.5
StrategicAI 80.2 ± 20.8 79.7 ± 17.4 80.4 ± 20.3 79.8 ± 20.6
L & N
AI u
GreedyAI 76.2 ± 20.7 72.2 ± 19.0 77.0 ± 21.3 75.1 ± 20.3
StrategicAI 72.6 ± 22.7 73.2 ± 18.2 74.0 ± 21.7 58.2 ± 18.9
N & N
AI u
GreedyAI 79.6 ± 16.6 79.6 ± 17.0 66.2 ± 17.9 63.1 ± 16.0
3 Acquisition trajectories
In order to illustrate our method, we display here two acquisition trajectories,
i.e. trajectories (x ,y ) in the domain of function f.
t t
{ } X ×YTitle Suppressed Due to Excessive Length 3
Fig.1. Trajectory of query points during an interaction. The first column shows the
user’s knowledge of the function (row 1: initial knowledge, gained from prior observa-
tionsmarkedinpurple;row2and3:finalknowledge),andthirdcolumntheAIagent’s
knowledge of the function. The second column displays real values of the function.
Thesecondandthirdrowsdemonstratethescenariosofinteractionbetweenaspecific
user with GreedyAI and StrategicAI, respectively. The final subplot compares the op-
timization scores during the interaction. The strategic AI obviously outperforms the
GreedyAI,helpingtheuserexploretheenvironmentbetterandfindtheglobaloptima.
However, the the optimization performed with GreedyAI gets stuck at local optima
and get a significantly lower score.4 A. Khoshvishkaie et al.
Fig.2. Trajectory of query points during an interaction. The first column shows the
user’s knowledge of the function (row 1: initial knowledge, gained from prior observa-
tionsmarkedinpurple;row2and3:finalknowledge),andthirdcolumntheAIagent’s
knowledgeofthefunction.Thesecondcolumndisplaysrealvaluesofthefunction.The
secondandthirdrowsdemonstratethescenariosofinteractionbetweenaspecificuser
withGreedyAIandStrategicAI,respectively.Thefinalsubplotcomparestheoptimiza-
tion scores during the interaction. The StrategicAI explores the function better and
outperforms the GreedyAI. However, both fail to find the global optimum.