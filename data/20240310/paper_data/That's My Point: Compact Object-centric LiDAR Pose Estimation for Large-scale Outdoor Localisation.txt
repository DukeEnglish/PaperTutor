That’s My Point: Compact Object-centric LiDAR Pose Estimation
for Large-scale Outdoor Localisation
Georgi Pramatarov, Matthew Gadd, Paul Newman, and Daniele De Martini
Mobile Robotics Group (MRG), University of Oxford
{georgi,mattgadd,pnewman,daniele}@robots.ox.ac.uk
Abstract—This paper is about 3D pose estimation on Li-
DAR scans with extremely minimal storage requirements to
enable scalable mapping and localisation. We achieve this by
clustering all points of segmented scans into semantic objects
and representing them only with their respective centroid and
semantic class. In this way, each LiDAR scan is reduced to
a compact collection of four-number vectors. This abstracts
away important structural information from the scenes, which
is crucial for traditional registration approaches. To mitigate
this, we introduce an object-matching network based on self-
and cross-correlation that captures geometric and semantic
relationships between entities. The respective matches allow us
to recover the relative transformation between scans through
Fig. 1. Method overview. LiDAR scans are represented extremely com-
weighted Singular Value Decomposition (SVD) and RANdom
pactlybyonlythecentroidandsemanticclassofthecorrespondingobjects
SAmple Consensus (RANSAC). We demonstrate that such
inthescene.Sacrificinginformationinthisway,welearnarobustmatching
representationissufficientformetriclocalisationbyregistering
function which leverages the remaining geometry in the object scene
point clouds taken under different viewpoints on the KITTI structureaswellasthesemanticrelationshipsbetweenentities.
dataset, and at different periods of time localising between
same time being human understandable [6]. Each object can
KITTI and KITTI-360. We achieve accurate metric estimates
bedescribednumerically,andscanmatchingandregistration
comparable with state-of-the-art methods with almost half the
representation size, specifically 1.33kB on average. rely on accurate object correspondences. Motivated by these
Index Terms—Localisation, Pose Estimation, Semantic Seg- approaches,thisworktriestoanswerthequestion:howsmall
mentation, Semantic Mapping, Autonomous Vehicles, Robotics can an object descriptor be while still providing enough
information for a reliable and accurate scan registration?
Here, we show that a descriptor composed of the positions
I. INTRODUCTION
of the objects’ centroids and their class – i.e. three floating-
Localisation is crucial for mobile robotics, allowing safe points and one byte – is enough for the task, requiring on
autonomous motion. For this task, LiDAR is popular due to average1.33kBofstorageperLiDARscanagainst1.41MB
itsintrinsicallygeometricreadingsandrobustnesstolighting of raw data. To remedy such compact – and thus feature-
andappearance,thusprovidingaveryinformativeandstable poor – representation, we enhance state-of-the-art feature
representation of its surroundings. embedding and geometric-aware object matching [7], [8]
Acommonapproachtolocalisationiscreatingamapfrom with semantic information, both by encoding it in a network
a previous traversal of the environment and then registering architecture, and by designing a semantically-informed loss
live sensor readings onto the map to extract the metric ego- to guide the training. We further refine the estimates us-
vehicle pose. However, modern LiDAR sensors can collect ing RANdom SAmple Consensus (RANSAC) and Iterative
up to tens of thousands of points per scan, making the ClosestPoint(ICP)ontheobjectmatches,obtainingaverage
representation and storage of compressed but reliable Li- translational errors of about 0.1m and 0.5◦ respectively on
DARobservationscompelling.EspeciallyintheAutonomous KITTI[9].OursystemisoutlinedinFig.1andourprincipal
Driving (AD) domain, maps are prone to being vast and contributions are:
may require a substantial amount of memory to scale [1].
1) Anaccurateregistrationapproachworkingonextremely
This is also critical in distributed settings, where the map
compactobject-centricrepresentationsofLiDARscans;
and observations need to be repeatedly transmitted between
2) A semantic-enhanced neural network architecture and
multiple agents and/or servers [2], [3].
loss function for descriptorless object-matching;
Whilst some methods use pure geometric approaches [1],
3) EvaluationoftheproposedmethodologyonKITTI[10],
[4], semantics and objects [5] provide a tradeoff between
[9] and on a long-term localisation scenario between
emphasisinglocalkeypointsorglobalfeatures–whileatthe
KITTI and KITTI-360 [11]. To the best of our knowl-
edge, this is the first approach that considers long-term
This work was supported by EPSRC Programme Grant “From Sensing
toCollaboration”(EP/V000748/1). cross-dataset registration between these two.
©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
includingreprinting/republishingthismaterialforadvertisingorpromotionalpurposes,creatingnewcollectiveworks,forresaleorredistributiontoservers
orlists,orreuseofanycopyrightedcomponentofthisworkinotherworks.
4202
raM
7
]VC.sc[
1v55740.3042:viXraII. RELATEDWORK Whereas transformers operate on fully-connected graphs,
apurelygraph-basedmethodis,forinstance,SEM-GAT[6],
Classical approaches to LiDAR scan registration rely on
which exploits semantic and morphological features to find
extracting descriptors from two point clouds, discovering
match candidates and compute match attention, leading to
point-to-point correspondences between them and exploit-
introspection capabilities [28]. Similarly, PREDATOR [29]
ing them to regress the displacement. One of the most
designs an overlap-attention module incorporating a Graph
widespread and simple approaches is ICP [12], which uses
Neural Network (GNN) connecting nearest-neighbour key-
cartesian position to describe and match points. As the
points to extract and refine their feature descriptors.
environment changes through different viewpoints or over
time, dense point-to-point correspondences can be replaced In our proposed system, we apply a point encoder and
by more robust keypoint-to-keypoint correspondences. As transformerapproach:weemployaDGCNNmoduleforfea-
such, keypoint detectors have been developed to discover ture extraction, augmented by encoded semantic features to-
robust features in the environment and 3D descriptors to getherwiththegeometryofthescene,andatransformation-
distinguish and match them. Examples of classical, hand- invariant matcher adapted from GeoTransformer [8].
craftedkeypointdetectorsareISS[13]andKPG[14],which
C. Use of Semantics in Point Cloud Registration
selectsalientpointswithlargevariationsintheirlocalneigh-
bourhood. Examples of 3D descriptors include Fast Point Non-learned semantic approaches typically extract and
Feature Histograms (FPFH) [15] and SHOT [16], which describe objects and match them directly. BoxGraph [5],
createhistogram-likedescriptorsbytakingintoconsideration encodes objects with their bounding box achieving a very
the local topology of the neighbouring points. compact scan representation, while GOSMatch [30] encodes
histogram-based descriptors from the distances between the
A. Learning-based Point Cloud Registration Losses
objects in the scene into vertex descriptors for initial pose
Learning-based approaches have replaced classical ones,
estimation and verification. For learned registration tech-
typicallythroughend-to-enddescriptorssupervisedbyadif-
niqueswecandistinguishthreetypesofsegmentationusage:
ferentiable Singular Value Decomposition (SVD) operation
augmentinggeometricinformationwithsemantics,extracting
with ground-truth registration. For instance, Deep Closest
object- or segment-level entities, and its usage in the loss
Point (DCP) [17] adopts a point-based encoder to extract
term. Examples of the first approach are DeepSIR [31]
high-dimensional descriptors and a transformer-based head
and SARNet [32], which learn both geometric and se-
tocomputesoftmatchestofeedintoanSVDmodule.Similar
mantic features to constrain feature matching. Differently,
to us – but to subsample the point clouds to use in a
SegMatch [33], SegMap [34] and SemSegMap [35] par-
DCPsetup–DCPCR[18]integratesacompressionnetwork,
tition point clouds into higher-level segments and extract
trainedtodownsamplethepointscloudswhilepreservingthe
multidimensional descriptors for each segment, and match
local information in the feature representation.
them across scenes. More similar to us, InstaLoc [36] and
A different approach is to directly supervise the matching
SGPR [37] learn to segment and match individual objects to
phase by cross-entropy or contrastive losses. For instance,
the prior scene. In particular, [37] applies a neural network
StickyPillars [19] inspired by the image-based SuperGlue
architecturetoextractvertexfeaturesbasedongraphconnec-
[20], and MDGAT [21] build soft assignments supervised
tivity. Finally, methods like PADLoc [38] leverage panoptic
through ground-truth matches obtained by projection of the
segmentation of point clouds and use such information dur-
keypointsfromonescanintotheother.Empirically,weshow
ing training to aid the convergence to robust descriptors. In
thatamatch-supervision approachtendstoperform betterin
ourapproach,weapplyallthreerulesandextractobject-level
our specific case of few and feature-poor points.
entities,whichwecouplewithsemanticclassinformationas
B. Learning-based Point Cloud Registration Architectures input and as an additional term in the loss. The resulting
object features are orders of magnitude smaller than the
Architectures for point cloud processing for localisation
object descriptors in the above-mentioned methods.
adopt diverse learning architectures. Indeed, whereas some
works exploit the vast experience in image-based Convolu-
III. METHOD
tionalNeuralNetworks(CNNs)[22],[23],mostapplypoint-
and, more recently, transformer- or graph-based approaches. Weaddresstheproblemofrobotposeestimationforlarge-
Although earlier approaches use purely point-based archi- scale outdoor LiDAR localisation with minimal, lightweight
tectures [24], [25], [26], transformer-based architectures are map and query representations. We formulate this as a
recentlyenhancingthembyapplyingself-andcross-attention registration problem between two point clouds where we
in feature space to share context information from within or seek point correspondences to estimate the relative transfor-
across LiDAR scans. One example is GeoTransformer [8], mation.Byrepresentingthepointcloudsassmallsetsof3D
which uses KPConv-FPN [27] and a custom transformer object centroids and their respective semantic types, we ob-
architecture with a keypoint descriptor invariant to rigid tain structures that yield extremely compact, highly-scalable
transformation.Similarly,DCP[17]andDCPCR[18]exploit semantic maps, yet allow very accurate localisation. We
DGCNN [7] and KPConv [27] point encoders respectively, employ a learned approach inspired by GeoTransformer [8].
and a transformer head for attention-based assignments. Still, rather than only relying on geometric features forFig. 2. System diagram. Our method aims to register a semantically labelled query point cloud Ps with a map. It clusters Ps into an object set Os,
keeping only instance centroids and semantic labels. Then, the query and map sets are passed through a semantic embedding and feature extraction
module.TheresultingobjectfeaturesFs andFm arethenpassedthroughageometricself-andfeature-basedcross-attentionmatchingmodule,producing
across-correlationsimilaritymatrixS¯.Asemanticmaskisthenappliedtofiltererroneousmatches,resultinginthefinalobjectcorrespondences.
superpoint matching, we integrate semantic information to semantic label and the inter-object relationships, and learn
obtain descriptive features for object matching as in Fig. 2. any further representations associated with the object.
C. Feature Enhancement
A. Problem Setup
Consider a pair of point clouds, P ={p |p ∈R3} and Indeed,weencodeeachobject’ssemanticclassandneigh-
s i i
P ={p |p ∈R3}, e.g., from a source live sensor stream bouring geometric structure to extract discriminative object
m j j descriptors (③ in Fig. 2). For this, we apply a learnable
of a vehicle and a pre-built target map. We aim to estimate
functionf =h ◦e toeachcluster’ssemanticlabel
therelativetransformationT =[R |t ]whereR ∈ sem sem emb
SO(3) and t ∈ R3 denos, tm e the rs o, tm atios n,m and transls a, tm ion. l i, where e emb : L → Rdemb is a categorical embedding
We do so bys g,m enerating key points, K = {k | k ∈ R3} function and h sem : Rdemb → Rdsem is parametrised by a
s i i
from P and K = {k | k ∈ R3} from P . Crucially, small MLP. We then enhance the features with structural
s m j j m
context o through a three-layer DGCNN-based module
K and K are stable and visible across multiple revisits i
s m
E [7], producing F = {f | f = E(o ∥f (l )),(o ,l ) ∈
of an environment, and |K | ≪ |P | and |K | ≪ |P |. i i i sem i i i
Givenasetofground-truthcs orresponds encesbetwm eenthetm
wo
O,f in ∈ Rdf}, where (·∥·) denotes concatenation. At each
layer, the feature x of the i-th point gets transformed into
sets of key points, T can be obtained using the Kabsch i
s,m the feature x′ through a projection network h and max-
algorithm [39] using SVD. Hence, we aim to obtain an i ec
pooling layer as in:
optimalsetofcorrespondencesC ={(k ,k )|k ∈K ,k ∈
i j i s j
x′ = max h (x ∥x −x )
K m} to produce the final pose estimate. We show that i
j∈kNN(i)
ec i j i (1)
using semantic object-like instances as key points provides
where kNN(i) are i’s k neighbours in feature space. Whilst
a consistent basis for pose estimation even across long-term
x and (x −x ) are not transformation invariant yet, the
i j i
revisits of the same place under variation in object layout.
operationinformstheresultingfeaturesofthelocalstructure.
B. Object Extraction D. Object Similarity and Matching
We follow BoxGraph [5] and SGPR [37], where input The feature vector of each object must be sufficiently
point clouds are segmented via a pre-trained semantic seg- descriptive and discriminative to be effectively matched
mentation network and then clustered into object instances across scans. Since we discard the internal structure of each
based on their semantic labels and Euclidean coordinates. objectinstance,itiscrucialtoexploitthescene’sstructureby
Panoptic segmentation or object detection might yield more incorporating it into each object’s features. Moreover, as the
accurateinstances,yetweoptforthissimplerapproachsince localisation setting is agnostic of the relative transformation
datasets with static object labels are scarce. between the scans, so must be the object descriptors.
In particular, consider a point cloud P (① in Fig. 2) and Since cross-correlation between features across scenes
the set L containing a semantic label for each point in P; benefits the matching task [17], [20], [8], we employ
here, L = {l | l ∈ L}, where L ⊂ N is a finite set the Superpoint Matching Module of GeoTransformer [8],
i i
of semantic classes. We apply the Density-Based Spatial which explicitly models the layout within scenes through
ClusteringofApplicationswithNoise(DBSCAN)algorithm a geometric self-attention module (④ in Fig. 2), and the
to extract a set of object-like clusters. For each cluster we latent feature similarity across scenes through a feature-
keep its centroid o – as the mean coordinate of its points – based cross-attention module (⑤ in Fig. 2). The geometric
and semantic label l. We then end up with with a labelled self-attention module encodes the global structure of the
object set O ={(o ,l )|o ∈R3,l ∈L}, which we use as scene in a transformation-invariant manner by operating on
i i i i
keypoints (② in Fig. 2). Contrary to other approaches that the relative distances between pairs and the relative angles
extractobjectfeaturesbyexploitingapointcloud’sintra-and betweentripletsofobjects.Thefeature-basedcross-attention
inter-object geometric structure, we instead utilise only the module,instead,facilitatesthefeatureexchangebetweenthetwo scenes. Further details can be found in the original loss which focuses the learning on spatially-proximal object
paper [8]. The self- and cross-attention modules are inter- pairs proportionally to their Euclidean distance, and ensures
leaved N times, and, from O , O and the corresponding the positive pairs are of objects of the same semantic class.
t s m
object features above, output the hybrid features H s = Formally,givenananchorobjecto i ∈O s,weconsideran
{hs i |hs i ∈Rdh} and H m ={hm j |hm j ∈Rdh}, suitable for object o j ∈ O m a positive if d i,j := ∥o i−o j∥2 2 < τ match
matching.Following[8],theyproduceanormalisedGaussian and l == l . We set τ := 1m and denote the
i j match
correlationmatrixS¯∈R|Os|×|Om| whichservesasmatching set of anchor objects with A ⊂ O , the set of positive
s s
scores (⑥ in Fig. 2). corresponding objects with pos(i) ⊂ O , and the rest as
m
At this point, we extend [8] and exploit the semantics of negatives with neg(i) ⊂ O . If we let the distance ratio
m
the scene to refine this score further by filtering mismatched be ρ := 1 − di,j ∈ [0,1] for positive matches, we
objects. We mask out the similarity scores of objects with can fi o,j rmulate theτm sa et mch antic distance-aware circle loss with
different labels (⑦ in Fig. 2), i.e. set s¯ i,j := 0, if l is ̸= l jm. respect to A s as:
The optimal set of object correspondences is then obtained 
by Cˆse =lec {t (i ong i,oth je )t |o op iN ∈c Om sa ,t oc jhe ∈s Ow mith ,(t ih ,e j)h ∈igh te os pt ks ii ,m j(i S¯la )r }ity: Ls
DC
= |A1
s|
i(cid:88) ∈Aslog1+ j∈(cid:88) pos(i)eβ ip ,j(hi,j−∆p) ·
(4)
(2) 
(cid:88) eβ in ,k(∆n−hi,k)

E. Pose Estimation and Refinement
j∈neg(i)
To find the relative transformation T s,m = [R s,m|t s,m] where h i,j = (cid:13) (cid:13)hs i −hm j (cid:13) (cid:13)2 2 is the distance in feature space.
between object sets O s and O m given the above correspon- Thepositivepairsandnegativepairsareweightedaccording
dences (⑧ in Fig. 2), we solve to weight factors βp = √ ρ γ(h − ∆ ) and βn =
i,j i,j i,j p i,k
Rˆ s,m,ˆt s,m =m R,i tn (cid:88) S¯ i,j∥Ro i+t−o j∥2 2 (3) γ an( d∆ ∆n−h =i,k 1) ., 4w ah reer te heγ c= or4 re0 spis oa nds ic na glin mg af ra gc into sr
.
a Wn ed d∆ ep fin= e0 th.1
e
(oi,oj)∈Cˆ lossLmn
forthetargetobjectsA analogously,yieldingthe
directly via the weighted SVD algorithm. Alternatively, we DC m
overall loss L as the average of Ls and Lm .
can apply the RANSAC algorithm that iteratively selects DC DC DC
a subset of matches and tries to maximise the number of
inliers up to a distance tolerance for a fixed set of iterations. IV. EXPERIMENTALSETUP
We explore both approaches in the experimental evalua-
A. Datasets
tion; yet, it is important to note that, whereas RANSAC
is computationally expensive for dense point clouds, its We evaluate our method on KITTI [9] and KITTI-360
added complexity is negligible given the low cardinality of [11], which include traversal sequences of different areas in
the object sets O and O . Finally, we find it helpful to Karlsruhe, Germany, by a vehicle equipped with a Velodyne
s m
refine further the relative transformation with ICP, reducing HDL-64 LiDAR sensor. We first explore short-term revisits
the noise in the coarse estimate above, which again adds – applicable to loop closing and SLAM scenarios – on
insignificant computational overhead. KITTI sequence 08, challenging due to reverse revisits. The
second considers long-term revisits in a teach&repeat setup
F. Loss Function where we select KITTI sequence 07 as a map and localise
Deep registration approaches are typically supervised in KITTI-360sequence09,whichhasbeenrecordedtwoyears
two different ways: a regression loss on the final pose esti- apart, with potentially significant changes in the scenes. In
mate that exploits the differentiability of the soft-assignment all our evaluations, we input to our model semantic labels
matrix between source and target point clouds [17], [18], as predicted by a top-performing segmentation network,
[38], or a cross-entropy/contrastive loss which uses ground- Cylinder3D [41], pre-trained on SemanticKITTI [9].
truthmatchestodirectlysupervisethesimilarityscores[20], WeconsidertheKITTIsequencesthatcontainrevisitsand
[19]. Empirically, when key points are sparse, as in our use 00, 05, 06, and 09 for training and 02 for validation,
case, even small ambiguity in the soft assignment matrix by selecting pairs of scans within 3m that are at least 50
introduces large errors in the final estimate; thus, we opt to frames apart, as in [37], [42], [5]. While 3m may be a
supervise the object features in a metric fashion similar to limited threshold for general registration, we argue that a
GeoTransformer [8], [40], which employs an overlap-aware coarse location estimate could be achieved with raw GPS
circle loss on the superpoint features. readingsorplacerecognitionapproachessuchasSGPR[37],
The overlap-aware circle loss aims to bring together the which use a similar compact object representation as ours.
features of positive pairs of objects, i.e. spatially proximal, Therawground-truthposesarenoisy,sowefurtherrefine
and push apart those of negative pairs, i.e., objects far in them via ICP. To register KITTI and KITTI-360, we use
space.Itdoessobyweightingthepositivematchesaccording the raw GPS measurements to select pairs within 3m. Then
to the overlap ratio of their clusters. As we discard object we remove points belonging to dynamic objects and the
geometries, we cannot recover a measure of overlap. For road class using Cylinder3D predictions, and again refine
this reason, we introduce a semantic distance-aware circle with ICP to obtain accurate ground-truth poses, as in Fig. 3.object correspondences N to 15 when using SVD, and 60
c
with RANSAC. This allows SVD to focus on high-quality
matches, while RANSAC has higher outlier tolerance by
design. Segmentation and clustering take 62ms and 80ms,
respectively, while registering a pair of scans takes 20ms
includingrefinement,showingtheefficiencyoftheapproach.
V. RESULTS
A. Map Compactness
Herewe compareour storagerequirements toother meth-
Fig.3. KITTI-360sequence09,registeredonKITTIsequence07. ods. Our representations are extremely compact, producing
B. Baselines onaverage105andmaximum238objects-likeinstancesper
We compare our method with geometric, learning- point cloud, similar to BoxGraph [5]. For each instance, we
based and semantic approaches. 1) RANSAC-based, hand- store the three floating point coordinates of the centroid and
crafted FPFH features as a dense geometric approach – with a byte for the semantic class, which on average requires
downsampled input point clouds with a voxel size of 30cm; 1.33kBofstorage.Withourrepresentation,the3.2km-long
2) BoxGraph [5], a compact hand-crafted semantic method; KITTI sequence 08 can be stored with 4.47MB only.
3) PADLoC [38]1, a learned semantic global localisation BoxGraphstoresthreeadditionalfloatingpointnumbers–
approach;4)superpoint-matchingmoduleofthevanillaGeo- the objects’ bounding boxes – almost doubling the memory
Transformer [8] (here, GeoTF-SP). We re-train the original requirement.Thedifferenceisevenlargercomparedtodense
architecture on our training data and, at test time, replace methods like GeoTranformer [8], which require the full
the fine registration module on the dense point patches with LiDAR scan: given 120k as the number of sampled points,
RANSAC-based matching on superpoint correspondences. common in AD applications, these methods would require
We also refine all methods with ICP for a fair comparison. three floats for point, i.e. 1.4MB. Even when voxelised
representations are used to reduce the point count – reach-
C. Metrics
ing usually 20k points – storage requirements are about
Tomeasuretheperformance,wereporttheRelativeTrans- 234.4kB.Compressedmethodsrequireanintermediatestor-
lation Error (RTE) and Relative Rotation Error (RRE): age space: for instance, OctSqueeze [1] ranges between 2
RTE=(cid:13) (cid:13)ˆt−t(cid:13)
(cid:13)
2,RRE=cos−1(cid:18) Tr(RˆT 2R)−1(cid:19)
(5)
a 2n 1d 9.715 kBb .it Ds Cpe Pr Cp Ro ,in int s( tb ep adp ,), cath nu cs or meq pu reir si sng apfr oo im ntc2 l9 o. u3 dkB 100to
-
folds2, compared to our average ratio of about 1:1000.
between the estimated [Rˆ |ˆt ] and ground-truth
s,m s,m
[R s,m|t s,m]rotationandtranslation,averagedoversuccessful
B. Pose Estimation
registrations. Subscript is removed for brevity. We also
WefirstexploreinTab.Itheposeestimationperformance
report the registration recall that measures the percentage of
of our approach in the short-term setting, where revisits
successfulregistrationssuchthatRTE<τ andRRE<τ .
t R
happen within the same sequence. Both our method and
D. Implementation Details BoxGraphtakesemanticlabelsasinputsexplicitly,soentries
As in [5], we only keep static objects (sidewalk, building, with(GT)representresultswithground-truthsemanticlabels
fence, vegetation, trunk, pole, and traffic sign), stable across fromSemanticKITTI,asopposedtoCylinder3Dpredictions.
revisits. Our semantic embedding dimension is 4, followed Whilst classical RANSAC-based approaches struggle to
by an MLP(32, 64, 128). We use 3 EdgeConv(64, 64) deduce accurate pose, we achieve accurate results with
layers in DGCNN with 30 nearest neighbours, followed by RANSAC and SVD. In particular, our method using Cylin-
an MLP(1024, 512, 256, 256) with ReLU. Normalisation der3D semantic labels estimates the pose within 0.3m and
and dropout deteriorate performance, so we omit them. 1◦ more than 85% of the time. BoxGraph struggles with
GeoTransformer’s matching module is set as in [8], with reverse revisits when using Cylinder3D labels, even after
3 sets of self- and cross-attention layers with 4 attention ICP-refinement.Itsperformanceimprovessignificantlywhen
heads,andoutputdimensionof256.WetrainonanNVIDIA using ground-truth semantics, so we conclude it strongly
RTX 3090 Ti GPU with a batch size of 32 for 50 epochs, depends on the quality of input segmentation. In contrast,
a learning rate of 1·10−3 and ADAM optimizer, halving our method shows greater stability when using ground-
the learning rate whenever the loss has plateaued for 5 truth semantic labels as input. In addition, using the vanilla
epochs. We apply random yaw rotations up to 360◦, ran- GeoTF-SP (with RANSAC) proves ineffective in this set-
domly subsample raw point clouds to 24000 points before ting, even though its deeper features are directly supervised
clustering and add random jitter to object centroids to sim- for matching. This justifies our use of object centroids as
ulate sensor noise. At inference time, we set the number of distinctive and repeatable keypoints.
1PADLoC’spublicweightsaretrainedonaslightlylargersupersetofour 2As DCPCR uses aggregated point clouds; here we use the ratio they
trainingdata,containingpairswithin4m,andincludingsequence07. reportforcomparison.We observe similar trends in the second setting, where Architecture MetricErrors[0.3m/1◦]
Sem.Emb. DGCNN MatchingModule Loss RR RTE RRE
we investigate the long-term localisation performance. We
(1) ✗ ✓ GeoTF L− 36.17 0.14 0.40
see that PADLoC, which incorporates semantics implicitly,
(2) ✗ ✓ GeoTF+ICP
LD −C
68.75 0.11 0.38
DC
struggles to register reliably without ICP refinement. This (3) ✗ ✓ GeoTF LDC 26.59 0.15 0.46
might mean it is difficult to generalize the semantic features
(4) ✓ ✓ GeoTF LSVD 0.31 0.21 0.70
(5) ✓ ✓ GeoTF L− 31.25 0.14 0.40
DC
across long periods. Our method models objects explicitly (6) ✓ ✗ GeoTF LDC 17.01 0.17 0.56
and demonstrates high recall of pose estimates with errors (7) ✓ ✓ KPConv+TF LSVD 16.09 0.13 0.57
within 0.5m and 5◦. Note that neither Cylinder3D, nor our ( (8 9) ) ✓ ✓ ✓ ✓ GeoG Te FoT +F ICP L LD DC C 4 71 5. .0 69 7 0 0. .1 14 1 00 .. 44 12
method are trained on KITTI-360, showing further robust-
TABLEII. AblationonarchitectureonKITTIsequence08,[m|◦].
ness across large temporal intervals. This is in line with
the other explicit object-based method, BoxGraph, whose our design choice. (6) also shows that the DGCNN feature
performance improves. Comparing the different variants of enhancement is crucial for learning discriminative object
our method, we see that weighted SVD achieves lower features. In (7) we replace the GeoTransformer Superpoint
metric errors since it operates on high-quality matches, MatchingModulewithaKPConv-basedfeatureencoderand
while RANSAC indeed maximises the consensus between afeature-basedtransformerheadasin[18].Thedeteriorated
matches, yielding high recall. performance shows the descriptive power of the geometric
self-attentionofGeoTransformer,whichsupportsourdesign.
0.3m/1◦ 0.5m/5◦
Method RR RTE RRE RR RTE RRE
D. Reducing Registration Overlap
FPFH 3.48 0.18 0.63 15.42 0.28 1.59
GeoTF-SP 20.29 0.19 0.52 42.26 0.27 0.79 So far we consider pairs of point clouds with sufficient
GeoTF-SP+ICP 20.49 0.19 0.51 40.83 0.26 0.79
PADLoC 14.24 0.20 0.66 67.78 0.30 1.21 overlap.Wenowbrieflystudytheperformanceofourmethod
PADLoC+ICP 49.28 0.11 0.28 80.02 0.20 0.56 withlesseroverlap.Wefollowacommonsetup[8],[23],[29]
BoxGraph 18.75 0.16 0.60 44.67 0.22 1.16
BoxGraph+ICP 53.28 0.12 0.34 59.38 0.13 0.45 whichaimstoregisterpointcloudpairsthatareatleast10m
Ours-RANSAC 87.50 0.13 0.45 99.80 0.14 0.54
Ours-SVD 85.66 0.12 0.44 95.18 0.12 0.52 apart. We train on KITTI sequences 00-05 and report the
BoxGraph(GT) 51.23 0.12 0.50 77.61 0.16 0.85 averagedresultsonsequences08-10inTab.III.Weseethat
BoxGraph+ICP(GT) 80.64 0.11 0.40 85.09 0.11 0.44 while our performance is limited due to the low quality of
Ours-RANSAC(GT) 88.52 0.11 0.45 99.49 0.13 0.52
Ours-SVD(GT) 89.60 0.09 0.41 96.52 0.10 0.47 object instances we extract, our RANSAC-based approach
FPFH 42.47 0.16 0.48 72.34 0.22 0.93 still obtains more than 90% registration recall within the
GeoTF-SP 38.22 0.17 0.38 56.01 0.23 0.51 commonly used boundaries of τ = 2m and τ = 5◦. This
GeoTF-SP+ICP 39.32 0.13 0.31 54.00 0.20 0.45 t R
PADLoC 0.01 0.23 0.84 5.22 0.38 3.31 is a significant improvement compared to BoxGraph which
PADLoC+ICP 46.93 0.13 0.30 87.00 0.21 0.73
BoxGraph 41.92 0.14 0.44 60.20 0.19 0.74 uses the same instances as our approach, and showcases the
BoxGraph+ICP 62.52 0.12 0.32 71.29 0.15 0.41 potential of learned centroid-only object-based registration.
Ours-RANSAC 78.14 0.11 0.35 94.14 0.15 0.47
Ours-SVD 80.52 0.11 0.30 92.66 0.14 0.39
2m/5◦
TABLEI. Registrationerrors[m|◦]atdifferentthresholdsforshort-term Method RR RTE RRE
revisits on KITTI sequence 08 and long-term localisation on KITTI-360
BoxGraph 10.85 0.61 1.95
sequence09withKITTIsequence07asmap.
Ours-RANSAC 95.53 0.27 1.05
Ours-SVD 67.97 0.33 1.07
C. Ablation Studies and Analysis
BoxGraph(GT) 16.49 0.52 1.84
We analyse the different components of our method to Ours-RANSAC(GT) 92.65 0.26 1.06
gainfurtherinsightintoourdesignchoices.Wereportresults Ours-SVD(GT) 65.90 0.27 0.96
on KITTI sequence 08 and use weighted SVD on N =
c TABLE III. Registration errors [m|◦] for scans at least 10m apart,
60 matches. SVD is more strongly affected by erroneous
averagedoverKITTIsequences08-10.
matches, so this better models the match quality.
Tab. II reports the results, where L denotes the pro-
VI. CONCLUSION
DC
posed distance-aware circle loss and L− ignores semantic We presented a novel approach for global registration
DC
classes and allows mislabelled matches, while L = of LiDAR point clouds with extremely compact object
SVD
||t −ˆt||+||I−Rt Rˆ|| denotes the explicit pose loss esti- representations. We show that using only the 3D centroids
gt gt
matedwithSVDonthefullsimilaritymatrixS¯(⑥inFig.2).
and semantic type of object instances is enough to estimate
Wecanseein(1)-(5)thatperformancedeteriorateswithout accurate poses in challenging conditions, including reverse
the input semantic embeddings and the semantic filtering and long-term revisits of the same place. We employ a
in our loss. We notice that having no input semantics and low-level geometric-attention-based matching module, and
filtering in the loss, (1)-(2), performs better than omitting enhance it with high-level semantics to increase its discrim-
either one of them (3)-(5), potentially meaning that having inative power. The resulting maps have very low storage
semantics at only one end confuses the network while requirements, which enables drastic scaling of the mapped
ignoring them altogether allows it to learn matches purely areas. In addition, working directly on objects can be a
based on spatial proximity. The largest drop in performance subjectforcross-modallocalisationorsemanticanalysisand
happens when replacing the L -loss (4), which justifies reasoning, which is the focus of future work.
DC
80:qeS
70:paM,90:qeSREFERENCES [22] X.Chen,T.La¨be,A.Milioto,T.Ro¨hling,J.Behley,andC.Stachniss,
“Overlapnet: A siamese network for computing lidar scan similar-
ity with applications to loop closing and localization,” Autonomous
[1] L. Huang, S. Wang, K. Wong, J. Liu, and R. Urtasun, “Octsqueeze: Robots.
Octree-structuredentropymodelforlidarcompression,”inIEEE/CVF [23] C. Choy, J. Park, and V. Koltun, “Fully convolutional geometric
conferenceoncomputervisionandpatternrecognition,2020. features,”inInternationalConferenceonComputerVision,2019.
[2] T. Cieslewski, S. Choudhary, and D. Scaramuzza, “Data-efficient [24] Y.Aoki,H.Goforth,R.A.Srivatsan,andS.Lucey,“Pointnetlk:Robust
decentralized visual slam,” in 2018 IEEE International Conference & efficient point cloud registration using pointnet,” in IEEE/CVF
onRoboticsandAutomation(ICRA),2018. conferenceoncomputervisionandpatternrecognition,2019.
[3] B. Ramtoula, R. de Azambuja, and G. Beltrame, “Capricorn: Com- [25] G. D. Pais, S. Ramalingam, V. M. Govindu, J. C. Nascimento,
municationawareplacerecognitionusinginterpretableconstellations R. Chellappa, and P. Miraldo, “3dregnet: A deep neural network for
ofobjectsinrobotnetworks,”in2020IEEEInternationalConference 3d point registration,” in IEEE/CVF conference on computer vision
onRoboticsandAutomation(ICRA),2020. andpatternrecognition,2020.
[4] C. Cao, M. Preda, and T. Zaharia, “3d point cloud compression: A [26] W.Yuan,B.Eckart,K.Kim,V.Jampani,D.Fox,andJ.Kautz,“Deep-
survey,”inInternationalConferenceon3DWebTechnology,2019. gmr: Learning latent gaussian mixture models for registration,” in
[5] G.Pramatarov,D.DeMartini,M.Gadd,andP.Newman,“Boxgraph: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
Semanticplacerecognitionandposeestimationfrom3dlidar,”in2022 UK,August23–28,2020,Proceedings,PartV16,2020.
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems [27] H.Thomas,C.R.Qi,J.-E.Deschaud,B.Marcotegui,F.Goulette,and
(IROS),2022,pp.7004–7011. L.J.Guibas,“Kpconv:Flexibleanddeformableconvolutionforpoint
[6] E.Panagiotaki,D.DeMartini,G.Pramatarov,M.Gadd,andL.Kunze, clouds,” in IEEE/CVF international conference on computer vision,
“Sem-gat: Explainable semantic pose estimation using learned graph 2019.
attention,”inInternationalConferenceonAdvancedRobotics(ICAR), [28] E.Panagiotaki,D.DeMartini,andL.Kunze,“Semanticinterpretation
2023,pp.367–374. andvalidationofgraphattention-basedexplanationsforgnnmodels,”
[7] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and inInternationalConferenceonAdvancedRobotics(ICAR),2023.
J. M. Solomon, “Dynamic graph cnn for learning on point clouds,” [29] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler,
ACM Trans. Graph., vol. 38, no. 5, oct 2019. [Online]. Available: “Predator: Registration of 3d point clouds with low overlap,” in
https://doi.org/10.1145/3326362 IEEE/CVF Conference on computer vision and pattern recognition,
2021,pp.4267–4276.
[8] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, S. Ilic, D. Hu, and
[30] Y.Zhu,Y.Ma,L.Chen,C.Liu,M.Ye,andL.Li,“Gosmatch:Graph-
K.Xu,“Geotransformer:Fastandrobustpointcloudregistrationwith
of-semanticsmatchingfordetectingloopclosuresin3dlidardata,”in
geometric transformer,” IEEE Transactions on Pattern Analysis and
InternationalConferenceonIntelligentRobotsandSystems,2020.
MachineIntelligence,2023.
[31] Q.Li,C.Wang,C.Wen,andX.Li,“Deepsir:Deepsemanticiterative
[9] J.Behley,M.Garbade,A.Milioto,J.Quenzel,S.Behnke,C.Stach-
registrationforlidarpointclouds,”PatternRecognition,2023.
niss,andJ.Gall,“Semantickitti:Adatasetforsemanticsceneunder-
[32] C.Liu,J.Guo,D.-M.Yan,Z.Liang,X.Zhang,andZ.Cheng,“Sarnet:
standingoflidarsequences,”inInternationalConferenceonComputer
Semantic augmented registration of large-scale urban point clouds,”
Vision,2019.
arXivpreprintarXiv:2206.13117,2022.
[10] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
[33] R.Dube´,D.Dugas,E.Stumm,J.Nieto,R.Siegwart,andC.Cadena,
driving?thekittivisionbenchmarksuite,”inConferenceonComputer
“Segmatch: Segment based place recognition in 3d point clouds,” in
VisionandPatternRecognition,2012.
InternationalConferenceonRoboticsandAutomation,2017.
[11] Y. Liao, J. Xie, and A. Geiger, “Kitti-360: A novel dataset and [34] R. Dube´, A. Cramariuc, D. D. H. Sommer, M. Dymczyk, J. N. R.
benchmarks for urban scene understanding in 2d and 3d,” IEEE Siegwart, and C. Cadena, “SegMap: Segment-based mapping and
TransactionsonPatternAnalysisandMachineIntelligence,2022. localization usingdata-driven descriptors,” The International Journal
[12] P.J.BeslandN.D.McKay,“Methodforregistrationof3-dshapes,” ofRoboticsResearch.
inSensorfusionIV:controlparadigmsanddatastructures,vol.1611. [35] A.Cramariuc,F.Tschopp,N.Alatur,S.Benz,T.Falck,M.Bru¨hlmeier,
Spie,1992,pp.586–606. B. Hahn, J. Nieto, and R. Siegwart, “Semsegmap–3d segment-based
[13] Y.Zhong,“Intrinsicshapesignatures:Ashapedescriptorfor3dobject semantic localization,” in International Conference on Intelligent
recognition,”in2009IEEE12thinternationalconferenceoncomputer RobotsandSystems,2021,pp.1183–1190.
visionworkshops,ICCVWorkshops,2009,pp.689–696. [36] L. Zhang, T. Digumarti, G. Tinchev, and M. Fallon, “Instaloc: One-
[14] A. Mian, M. Bennamoun, and R. Owens, “On the repeatability and shotgloballidarlocalisationinindoorenvironmentsthroughinstance
quality of keypoints for local feature-based 3d object retrieval from learning,”Robotics:ScienceandSystems(RSS),2023.
clutteredscenes,”InternationalJournalofComputerVision,2010. [37] X. Kong, X. Yang, G. Zhai, X. Zhao, X. Zeng, M. Wang, Y. Liu,
[15] R.B.Rusu,N.Blodow,andM.Beetz,“Fastpointfeaturehistograms W. Li, and F. Wen, “Semantic graph based place recognition for 3d
(fpfh) for 3d registration,” in International Conference on Robotics point clouds,” in International Conference on Intelligent Robots and
andAutomation,2009. Systems,2020.
[16] F. Tombari, S. Salti, and L. Di Stefano, “Unique signatures of [38] J.Arce,N.Vo¨disch,D.Cattaneo,W.Burgard,andA.Valada,“Padloc:
histogramsforlocalsurfacedescription,”inEuropeanConferenceon Lidar-baseddeeploopclosuredetectionandregistrationusingpanoptic
ComputerVision,2010. attention,”IEEERoboticsandAutomationLetters,2023.
[17] Y. Wang and J. M. Solomon, “Deep closest point: Learning repre- [39] W. Kabsch, “A solution for the best rotation to relate two sets of
sentations for point cloud registration,” in IEEE/CVF international vectors,”ActaCrystallographicaSectionA,1976.
conferenceoncomputervision,2019,pp.3523–3532. [40] Y.Sun,C.Cheng,Y.Zhang,C.Zhang,L.Zheng,Z.Wang,andY.Wei,
“Circleloss:Aunifiedperspectiveofpairsimilarityoptimization,”in
[18] L. Wiesmann, T. Guadagnino, I. Vizzo, G. Grisetti, J. Behley, and
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
C. Stachniss, “Dcpcr: Deep compressed point cloud registration in
2020.
large-scale outdoor environments,” IEEE Robotics and Automation
[41] H. Zhou, X. Zhu, X. Song, Y. Ma, Z. Wang, H. Li, and D. Lin,
Letters,2022.
“Cylinder3d: An effective 3d framework for driving-scene lidar se-
[19] K.Fischer,M.Simon,F.Olsner,S.Milz,H.-M.Gross,andP.Mader,
manticsegmentation,”arXivpreprintarXiv:2008.01550,2020.
“Stickypillars:Robustandefficientfeaturematchingonpointclouds
[42] L. Li, X. Kong, X. Zhao, T. Huang, W. Li, F. Wen, H. Zhang, and
usinggraphneuralnetworks,”inIEEE/CVFConferenceonComputer
Y.Liu,“Ssc:Semanticscancontextforlarge-scaleplacerecognition,”
VisionandPatternRecognition,2021.
inInternationalConferenceonIntelligentRobotsandSystems,2021.
[20] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, “Su-
perglue: Learning feature matching with graph neural networks,” in
IEEE/CVF conference on computer vision and pattern recognition,
2020,pp.4938–4947.
[21] C.Shi,X.Chen,K.Huang,J.Xiao,H.Lu,andC.Stachniss,“Keypoint
matchingfor pointcloudregistration usingmultiplexdynamic graph
attentionnetworks,”IEEERoboticsandAutomationLetters,2021.