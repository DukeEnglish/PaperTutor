[
    {
        "title": "Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States",
        "authors": "Noam RazinYotam AlexanderEdo Cohen-KarlikRaja GiryesAmir GlobersonNadav Cohen",
        "links": "http://arxiv.org/abs/2402.07875v1",
        "entry_id": "http://arxiv.org/abs/2402.07875v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07875v1",
        "summary": "In modern machine learning, models can often fit training data in numerous\nways, some of which perform well on unseen (test) data, while others do not.\nRemarkably, in such cases gradient descent frequently exhibits an implicit bias\nthat leads to excellent performance on unseen data. This implicit bias was\nextensively studied in supervised learning, but is far less understood in\noptimal control (reinforcement learning). There, learning a controller applied\nto a system via gradient descent is known as policy gradient, and a question of\nprime importance is the extent to which a learned controller extrapolates to\nunseen initial states. This paper theoretically studies the implicit bias of\npolicy gradient in terms of extrapolation to unseen initial states. Focusing on\nthe fundamental Linear Quadratic Regulator (LQR) problem, we establish that the\nextent of extrapolation depends on the degree of exploration induced by the\nsystem when commencing from initial states included in training. Experiments\ncorroborate our theory, and demonstrate its conclusions on problems beyond LQR,\nwhere systems are non-linear and controllers are neural networks. We\nhypothesize that real-world optimal control may be greatly improved by\ndeveloping methods for informed selection of initial states to train on.",
        "updated": "2024-02-12 18:41:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07875v1"
    },
    {
        "title": "Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds",
        "authors": "Bastian BollDaniel Gonzalez-AlvaradoChristoph Schnörr",
        "links": "http://arxiv.org/abs/2402.07846v1",
        "entry_id": "http://arxiv.org/abs/2402.07846v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07846v1",
        "summary": "This paper introduces a novel generative model for discrete distributions\nbased on continuous normalizing flows on the submanifold of factorizing\ndiscrete measures. Integration of the flow gradually assigns categories and\navoids issues of discretizing the latent continuous model like rounding, sample\ntruncation etc. General non-factorizing discrete distributions capable of\nrepresenting complex statistical dependencies of structured discrete data, can\nbe approximated by embedding the submanifold into a the meta-simplex of all\njoint discrete distributions and data-driven averaging. Efficient training of\nthe generative model is demonstrated by matching the flow of geodesics of\nfactorizing discrete distributions. Various experiments underline the\napproach's broad applicability.",
        "updated": "2024-02-12 17:56:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07846v1"
    },
    {
        "title": "On Computationally Efficient Multi-Class Calibration",
        "authors": "Parikshit GopalanLunjia HuGuy N. Rothblum",
        "links": "http://arxiv.org/abs/2402.07821v1",
        "entry_id": "http://arxiv.org/abs/2402.07821v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07821v1",
        "summary": "Consider a multi-class labelling problem, where the labels can take values in\n$[k]$, and a predictor predicts a distribution over the labels. In this work,\nwe study the following foundational question: Are there notions of multi-class\ncalibration that give strong guarantees of meaningful predictions and can be\nachieved in time and sample complexities polynomial in $k$? Prior notions of\ncalibration exhibit a tradeoff between computational efficiency and\nexpressivity: they either suffer from having sample complexity exponential in\n$k$, or needing to solve computationally intractable problems, or give rather\nweak guarantees.\n  Our main contribution is a notion of calibration that achieves all these\ndesiderata: we formulate a robust notion of projected smooth calibration for\nmulti-class predictions, and give new recalibration algorithms for efficiently\ncalibrating predictors under this definition with complexity polynomial in $k$.\nProjected smooth calibration gives strong guarantees for all downstream\ndecision makers who want to use the predictor for binary classification\nproblems of the form: does the label belong to a subset $T \\subseteq [k]$: e.g.\nis this an image of an animal? It ensures that the probabilities predicted by\nsumming the probabilities assigned to labels in $T$ are close to some perfectly\ncalibrated binary predictor for that task. We also show that natural\nstrengthenings of our definition are computationally hard to achieve: they run\ninto information theoretic barriers or computational intractability. Underlying\nboth our upper and lower bounds is a tight connection that we prove between\nmulti-class calibration and the well-studied problem of agnostic learning in\nthe (standard) binary prediction setting.",
        "updated": "2024-02-12 17:25:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07821v1"
    },
    {
        "title": "Towards a mathematical theory for consistency training in diffusion models",
        "authors": "Gen LiZhihan HuangYuting Wei",
        "links": "http://arxiv.org/abs/2402.07802v1",
        "entry_id": "http://arxiv.org/abs/2402.07802v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07802v1",
        "summary": "Consistency models, which were proposed to mitigate the high computational\noverhead during the sampling phase of diffusion models, facilitate single-step\nsampling while attaining state-of-the-art empirical performance. When\nintegrated into the training phase, consistency models attempt to train a\nsequence of consistency functions capable of mapping any point at any time step\nof the diffusion process to its starting point. Despite the empirical success,\na comprehensive theoretical understanding of consistency training remains\nelusive. This paper takes a first step towards establishing theoretical\nunderpinnings for consistency models. We demonstrate that, in order to generate\nsamples within $\\varepsilon$ proximity to the target in distribution (measured\nby some Wasserstein metric), it suffices for the number of steps in consistency\nlearning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data\ndimension. Our theory offers rigorous insights into the validity and efficacy\nof consistency models, illuminating their utility in downstream inference\ntasks.",
        "updated": "2024-02-12 17:07:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07802v1"
    },
    {
        "title": "Tuning-Free Stochastic Optimization",
        "authors": "Ahmed KhaledChi Jin",
        "links": "http://arxiv.org/abs/2402.07793v1",
        "entry_id": "http://arxiv.org/abs/2402.07793v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07793v1",
        "summary": "Large-scale machine learning problems make the cost of hyperparameter tuning\never more prohibitive. This creates a need for algorithms that can tune\nthemselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that\ncan match the performance of optimally-tuned optimization algorithms up to\npolylogarithmic factors given only loose hints on the relevant problem\nparameters. We consider in particular algorithms that can match optimally-tuned\nStochastic Gradient Descent (SGD). When the domain of optimization is bounded,\nwe show tuning-free matching of SGD is possible and achieved by several\nexisting algorithms. We prove that for the task of minimizing a convex and\nsmooth or Lipschitz function over an unbounded domain, tuning-free optimization\nis impossible. We discuss conditions under which tuning-free optimization is\npossible even over unbounded domains. In particular, we show that the recently\nproposed DoG and DoWG algorithms are tuning-free when the noise distribution is\nsufficiently well-behaved. For the task of finding a stationary point of a\nsmooth and potentially nonconvex function, we give a variant of SGD that\nmatches the best-known high-probability convergence rate for tuned SGD at only\nan additional polylogarithmic cost. However, we also give an impossibility\nresult that shows no algorithm can hope to match the optimal expected\nconvergence rate for tuned SGD with high probability.",
        "updated": "2024-02-12 16:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07793v1"
    }
]