[
    {
        "title": "Wavefront Randomization Improves Deconvolution",
        "authors": "Amit KohliAnastasios N. AngelopoulosLaura Waller",
        "links": "http://arxiv.org/abs/2402.07900v2",
        "entry_id": "http://arxiv.org/abs/2402.07900v2",
        "pdf_url": "http://arxiv.org/pdf/2402.07900v2",
        "summary": "The performance of an imaging system is limited by optical aberrations, which\ncause blurriness in the resulting image. Digital correction techniques, such as\ndeconvolution, have limited ability to correct the blur, since some spatial\nfrequencies in the scene are not measured adequately (i.e., 'zeros' of the\nsystem transfer function). We prove that the addition of a random mask to an\nimaging system removes its dependence on aberrations, reducing the likelihood\nof zeros in the transfer function and consequently decreasing the sensitivity\nto noise during deconvolution. In simulation, we show that this strategy\nimproves image quality over a range of aberration types, aberration strengths,\nand signal-to-noise ratios.",
        "updated": "2024-02-13 03:00:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07900v2"
    },
    {
        "title": "Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets",
        "authors": "Violet LiuJason ChenAns QureshiMahla Nejati",
        "links": "http://arxiv.org/abs/2402.07895v1",
        "entry_id": "http://arxiv.org/abs/2402.07895v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07895v1",
        "summary": "Amidst growing food production demands, early plant disease detection is\nessential to safeguard crops; this study proposes a visual machine learning\napproach for plant disease detection, harnessing RGB and NIR data collected in\nreal-world conditions through a JAI FS-1600D-10GE camera to build an RGBN\ndataset. A two-stage early plant disease detection model with YOLOv8 and a\nsequential CNN was used to train on a dataset with partial labels, which showed\na 3.6% increase in mAP compared to a single-stage end-to-end segmentation\nmodel. The sequential CNN model achieved 90.62% validation accuracy utilising\nRGBN data. An average of 6.25% validation accuracy increase is found using RGBN\nin classification compared to RGB using ResNet15 and the sequential CNN models.\nFurther research and dataset improvements are needed to meet food production\ndemands.",
        "updated": "2024-02-12 18:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07895v1"
    },
    {
        "title": "MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO",
        "authors": "Shubhabrata MukherjeeCory BeardZhu Li",
        "links": "http://arxiv.org/abs/2402.07894v1",
        "entry_id": "http://arxiv.org/abs/2402.07894v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07894v1",
        "summary": "Low-light conditions and occluded scenarios impede object detection in\nreal-world Internet of Things (IoT) applications like autonomous vehicles and\nsecurity systems. While advanced machine learning models strive for accuracy,\ntheir computational demands clash with the limitations of resource-constrained\ndevices, hampering real-time performance. In our current research, we tackle\nthis challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models\never conceived. YOLO Phantom utilizes the novel Phantom Convolution block,\nachieving comparable accuracy to the latest YOLOv8n model while simultaneously\nreducing both parameters and model size by 43%, resulting in a significant 19%\nreduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages\ntransfer learning on our multimodal RGB-infrared dataset to address low-light\nand occlusion issues, equipping it with robust vision under adverse conditions.\nIts real-world efficacy is demonstrated on an IoT platform with advanced\nlow-light and RGB cameras, seamlessly connecting to an AWS-based notification\nendpoint for efficient real-time object detection. Benchmarks reveal a\nsubstantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB\ndetection, respectively, compared to the baseline YOLOv8n model. For community\ncontribution, both the code and the multimodal dataset are available on GitHub.",
        "updated": "2024-02-12 18:56:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07894v1"
    },
    {
        "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
        "authors": "Soroush NasirianyFei XiaWenhao YuTed XiaoJacky LiangIshita DasguptaAnnie XieDanny DriessAyzaan WahidZhuo XuQuan VuongTingnan ZhangTsang-Wei Edward LeeKuang-Huei LeePeng XuSean KirmaniYuke ZhuAndy ZengKarol HausmanNicolas HeessChelsea FinnSergey LevineBrian Ichter",
        "links": "http://arxiv.org/abs/2402.07872v1",
        "entry_id": "http://arxiv.org/abs/2402.07872v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07872v1",
        "summary": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
        "updated": "2024-02-12 18:33:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07872v1"
    },
    {
        "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
        "authors": "Siddharth KaramchetiSuraj NairAshwin BalakrishnaPercy LiangThomas KollarDorsa Sadigh",
        "links": "http://arxiv.org/abs/2402.07865v1",
        "entry_id": "http://arxiv.org/abs/2402.07865v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07865v1",
        "summary": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization from language, and targeted challenge sets that probe properties\nsuch as hallucination; evaluations that provide calibrated, fine-grained\ninsight into a VLM's capabilities. Second, we rigorously investigate VLMs along\nkey design axes, including pretrained visual representations and quantifying\nthe tradeoffs of using base vs. instruct-tuned language models, amongst others.\nWe couple our analysis with three resource contributions: (1) a unified\nframework for evaluating VLMs, (2) optimized, flexible code for VLM training,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open-source VLMs.",
        "updated": "2024-02-12 18:21:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07865v1"
    }
]