[
    {
        "title": "Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains",
        "authors": "Yasin FindikS. Reza Ahmadzadeh",
        "links": "http://arxiv.org/abs/2402.07752v1",
        "entry_id": "http://arxiv.org/abs/2402.07752v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07752v1",
        "summary": "Tackling multi-agent learning problems efficiently is a challenging task in\ncontinuous action domains. While value-based algorithms excel in sample\nefficiency when applied to discrete action domains, they are usually\ninefficient when dealing with continuous actions. Policy-based algorithms, on\nthe other hand, attempt to address this challenge by leveraging critic networks\nfor guiding the learning process and stabilizing the gradient estimation. The\nlimitations in the estimation of true return and falling into local optima in\nthese methods result in inefficient and often sub-optimal policies. In this\npaper, we diverge from the trend of further enhancing critic networks, and\nfocus on improving the effectiveness of value-based methods in multi-agent\ncontinuous domains by concurrently evaluating numerous actions. We propose a\nnovel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired\nfrom the idea of Q-Functionals, that enables agents to transform their states\ninto basis functions. Our algorithm fosters collaboration among agents by\nmixing their action-values. We evaluate the efficacy of our algorithm in six\ncooperative multi-agent scenarios. Our empirical findings reveal that MQF\noutperforms four variants of Deep Deterministic Policy Gradient through rapid\naction evaluation and increased sample efficiency.",
        "updated": "2024-02-12 16:21:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07752v1"
    },
    {
        "title": "Ensuring trustworthy and ethical behaviour in intelligent logical agents",
        "authors": "Stefania Costantini",
        "links": "http://dx.doi.org/10.1093/logcom/exab091",
        "entry_id": "http://arxiv.org/abs/2402.07547v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07547v1",
        "summary": "Autonomous Intelligent Agents are employed in many applications upon which\nthe life and welfare of living beings and vital social functions may depend.\nTherefore, agents should be trustworthy. A priori certification techniques\n(i.e., techniques applied prior to system's deployment) can be useful, but are\nnot sufficient for agents that evolve, and thus modify their epistemic and\nbelief state, and for open Multi-Agent Systems, where heterogeneous agents can\njoin or leave the system at any stage of its operation. In this paper, we\npropose/refine/extend dynamic (runtime) logic-based self-checking techniques,\ndevised in order to be able to ensure agents' trustworthy and ethical\nbehaviour.",
        "updated": "2024-02-12 10:19:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07547v1"
    },
    {
        "title": "A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?",
        "authors": "Nathan I. N. HenryMangor PedersenMatt WilliamsJamin L. B. MartinLiesje Donkin",
        "links": "http://arxiv.org/abs/2402.07462v2",
        "entry_id": "http://arxiv.org/abs/2402.07462v2",
        "pdf_url": "http://arxiv.org/pdf/2402.07462v2",
        "summary": "The value-loading problem is a significant challenge for researchers aiming\nto create artificial intelligence (AI) systems that align with human values and\npreferences. This problem requires a method to define and regulate safe and\noptimal limits of AI behaviors. In this work, we propose HALO (Hormetic\nALignment via Opponent processes), a regulatory paradigm that uses hormetic\nanalysis to regulate the behavioral patterns of AI. Behavioral hormesis is a\nphenomenon where low frequencies of a behavior have beneficial effects, while\nhigh frequencies are harmful. By modeling behaviors as allostatic opponent\nprocesses, we can use either Behavioral Frequency Response Analysis (BFRA) or\nBehavioral Count Response Analysis (BCRA) to quantify the hormetic limits of\nrepeatable behaviors. We demonstrate how HALO can solve the 'paperclip\nmaximizer' scenario, a thought experiment where an unregulated AI tasked with\nmaking paperclips could end up converting all matter in the universe into\npaperclips. Our approach may be used to help create an evolving database of\n'values' based on the hedonic calculus of repeatable behaviors with decreasing\nmarginal utility. This positions HALO as a promising solution for the\nvalue-loading problem, which involves embedding human-aligned values into an AI\nsystem, and the weak-to-strong generalization problem, which explores whether\nweak models can supervise stronger models as they become more intelligent.\nHence, HALO opens several research avenues that may lead to the development of\na computational value system that allows an AI algorithm to learn whether the\ndecisions it makes are right or wrong.",
        "updated": "2024-02-13 05:21:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07462v2"
    },
    {
        "title": "Learning Optimal Tax Design in Nonatomic Congestion Games",
        "authors": "Qiwen CuiMaryam FazelSimon S. Du",
        "links": "http://arxiv.org/abs/2402.07437v1",
        "entry_id": "http://arxiv.org/abs/2402.07437v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07437v1",
        "summary": "We study how to learn the optimal tax design to maximize the efficiency in\nnonatomic congestion games. It is known that self-interested behavior among the\nplayers can damage the system's efficiency. Tax mechanisms is a common method\nto alleviate this issue and induce socially optimal behavior. In this work, we\ntake the initial step for learning the optimal tax that can minimize the social\ncost with \\emph{equilibrium feedback}, i.e., the tax designer can only observe\nthe equilibrium state under the enforced tax. Existing algorithms are not\napplicable due to the exponentially large tax function space, nonexistence of\nthe gradient, and nonconvexity of the objective. To tackle these challenges,\nour algorithm leverages several novel components: (1) piece-wise linear tax to\napproximate the optimal tax; (2) an extra linear term to guarantee a strongly\nconvex potential function; (3) efficient subroutine to find the ``boundary''\ntax. The algorithm can find an $\\epsilon$-optimal tax with $O(\\beta\nF^2/\\epsilon)$ sample complexity, where $\\beta$ is the smoothness of the cost\nfunction and $F$ is the number of facilities.",
        "updated": "2024-02-12 06:32:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07437v1"
    },
    {
        "title": "Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support",
        "authors": "Igor SvobodaDmytro Lande",
        "links": "http://arxiv.org/abs/2402.07404v1",
        "entry_id": "http://arxiv.org/abs/2402.07404v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07404v1",
        "summary": "Our study presents a new framework that incorporates the Analytic Hierarchy\nProcess (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language\nmodel (LLM), bringing novel approaches to cybersecurity Multiple-criteria\nDecision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous\nagents as virtual experts, we automate the decision-making process, enhancing\nboth efficiency and reliability. This new approach focuses on leveraging LLMs\nfor sophisticated decision analysis, highlighting the synergy between\ntraditional decision-making models and cutting-edge AI technologies. Our\ninnovative methodology demonstrates significant advancements in using AI-driven\nagents for complex decision-making scenarios, highlighting the importance of AI\nin strategic cybersecurity applications. The findings reveal the transformative\npotential of combining AHP and LLMs, establishing a new paradigm for\nintelligent decision support systems in cybersecurity and beyond.",
        "updated": "2024-02-12 04:47:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07404v1"
    }
]