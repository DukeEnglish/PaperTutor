[
    {
        "title": "FAST: Factorizable Attention for Speeding up Transformers",
        "authors": "Armin GeramiMonte HooverPranav S. DulepetRamani Duraiswami",
        "links": "http://arxiv.org/abs/2402.07901v1",
        "entry_id": "http://arxiv.org/abs/2402.07901v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07901v1",
        "summary": "Motivated by the factorization inherent in the original fast multipole method\nand the improved fast Gauss transform we introduce a factorable form of\nattention that operates efficiently in high dimensions. This approach reduces\nthe computational and memory complexity of the attention mechanism in\ntransformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our\nwork presents a linearly scaled attention mechanism that maintains the full\nrepresentation of the attention matrix without compromising on sparsification\nand incorporates the all-to-all relationship between tokens. We explore the\nproperties of our new attention metric and conduct tests in various standard\nsettings. Results indicate that our attention mechanism has a robust\nperformance and holds significant promise for diverse applications where\nself-attention is used.",
        "updated": "2024-02-12 18:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07901v1"
    },
    {
        "title": "A systematic investigation of learnability from single child linguistic input",
        "authors": "Yulu QinWentao WangBrenden M. Lake",
        "links": "http://arxiv.org/abs/2402.07899v1",
        "entry_id": "http://arxiv.org/abs/2402.07899v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07899v1",
        "summary": "Language models (LMs) have demonstrated remarkable proficiency in generating\nlinguistically coherent text, sparking discussions about their relevance to\nunderstanding human language learnability. However, a significant gap exists\nbetween the training data for these models and the linguistic input a child\nreceives. LMs are typically trained on data that is orders of magnitude larger\nand fundamentally different from child-directed speech (Warstadt and Bowman,\n2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our\nresearch focuses on training LMs on subsets of a single child's linguistic\ninput. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in\nthis setting can form syntactic and semantic word clusters and develop\nsensitivity to certain linguistic phenomena, but they only considered LSTMs and\nsimpler neural networks trained from just one single-child dataset. Here, to\nexamine the robustness of learnability from single-child input, we\nsystematically train six different model architectures on five datasets (3\nsingle-child and 2 baselines). We find that the models trained on single-child\ndatasets showed consistent results that matched with previous work,\nunderscoring the robustness of forming meaningful syntactic and semantic\nrepresentations from a subset of a child's linguistic input.",
        "updated": "2024-02-12 18:58:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07899v1"
    },
    {
        "title": "Label-Efficient Model Selection for Text Generation",
        "authors": "Shir Ashury-TahanBenjamin SznajderLeshem ChoshenLiat Ein-DorEyal ShnarchAriel Gera",
        "links": "http://arxiv.org/abs/2402.07891v1",
        "entry_id": "http://arxiv.org/abs/2402.07891v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07891v1",
        "summary": "Model selection for a given target task can be costly, as it may entail\nextensive annotation of the quality of outputs of different models. We\nintroduce DiffUse, an efficient method to make an informed decision between\ncandidate text generation models. DiffUse reduces the required amount of\npreference annotations, thus saving valuable time and resources in performing\nevaluation. DiffUse intelligently selects instances by clustering embeddings\nthat represent the semantic differences between model outputs. Thus, it is able\nto identify a subset of examples that are more informative for preference\ndecisions. Our method is model-agnostic, and can be applied to any text\ngeneration model. Moreover, we propose a practical iterative approach for\ndynamically determining how many instances to annotate. In a series of\nexperiments over hundreds of model pairs, we demonstrate that DiffUse can\ndramatically reduce the required number of annotations -- by up to 75% -- while\nmaintaining high evaluation reliability.",
        "updated": "2024-02-12 18:54:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07891v1"
    },
    {
        "title": "MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning",
        "authors": "Ayesha Siddika NipuSiming LiuAnthony Harris",
        "links": "http://dx.doi.org/10.1109/CoG51982.2022.9893711",
        "entry_id": "http://arxiv.org/abs/2402.07890v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07890v1",
        "summary": "Distributed decision-making in multi-agent systems presents difficult\nchallenges for interactive behavior learning in both cooperative and\ncompetitive systems. To mitigate this complexity, MAIDRL presents a\nsemi-centralized Dense Reinforcement Learning algorithm enhanced by agent\ninfluence maps (AIMs), for learning effective multi-agent control on StarCraft\nMulti-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet\nin MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement\nLearning, MAIDCRL, by incorporating convolutional layers into the deep model\narchitecture, and evaluate the performance on both homogeneous and\nheterogeneous scenarios. The results show that the CNN-enabled MAIDCRL\nsignificantly improved the learning performance and achieved a faster learning\nrate compared to the existing MAIDRL, especially on more complicated\nheterogeneous SMAC scenarios. We further investigate the stability and\nrobustness of our model. The statistics reflect that our model not only\nachieves higher winning rate in all the given scenarios but also boosts the\nagent's learning process in fine-grained decision-making.",
        "updated": "2024-02-12 18:53:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07890v1"
    },
    {
        "title": "Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks",
        "authors": "Giacomo ZonneveldLorenzo PrincipiMarco Baldi",
        "links": "http://arxiv.org/abs/2402.07878v1",
        "entry_id": "http://arxiv.org/abs/2402.07878v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07878v1",
        "summary": "Early detection of network intrusions and cyber threats is one of the main\npillars of cybersecurity. One of the most effective approaches for this purpose\nis to analyze network traffic with the help of artificial intelligence\nalgorithms, with the aim of detecting the possible presence of an attacker by\ndistinguishing it from a legitimate user. This is commonly done by collecting\nthe traffic exchanged between terminals in a network and analyzing it on a\nper-packet or per-connection basis. In this paper, we propose instead to\nperform pre-processing of network traffic under analysis with the aim of\nextracting some new metrics on which we can perform more efficient detection\nand overcome some limitations of classical approaches. These new metrics are\nbased on graph theory, and consider the network as a whole, rather than\nfocusing on individual packets or connections. Our approach is validated\nthrough experiments performed on publicly available data sets, from which it\nresults that it can not only overcome some of the limitations of classical\napproaches, but also achieve a better detection capability of cyber threats.",
        "updated": "2024-02-12 18:44:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07878v1"
    }
]