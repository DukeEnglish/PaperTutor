[
    {
        "title": "A systematic investigation of learnability from single child linguistic input",
        "authors": "Yulu QinWentao WangBrenden M. Lake",
        "links": "http://arxiv.org/abs/2402.07899v1",
        "entry_id": "http://arxiv.org/abs/2402.07899v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07899v1",
        "summary": "Language models (LMs) have demonstrated remarkable proficiency in generating\nlinguistically coherent text, sparking discussions about their relevance to\nunderstanding human language learnability. However, a significant gap exists\nbetween the training data for these models and the linguistic input a child\nreceives. LMs are typically trained on data that is orders of magnitude larger\nand fundamentally different from child-directed speech (Warstadt and Bowman,\n2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our\nresearch focuses on training LMs on subsets of a single child's linguistic\ninput. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in\nthis setting can form syntactic and semantic word clusters and develop\nsensitivity to certain linguistic phenomena, but they only considered LSTMs and\nsimpler neural networks trained from just one single-child dataset. Here, to\nexamine the robustness of learnability from single-child input, we\nsystematically train six different model architectures on five datasets (3\nsingle-child and 2 baselines). We find that the models trained on single-child\ndatasets showed consistent results that matched with previous work,\nunderscoring the robustness of forming meaningful syntactic and semantic\nrepresentations from a subset of a child's linguistic input.",
        "updated": "2024-02-12 18:58:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07899v1"
    },
    {
        "title": "Suppressing Pink Elephants with Direct Principle Feedback",
        "authors": "Louis CastricatoNathan LileSuraj AnandHailey SchoelkopfSiddharth VermaStella Biderman",
        "links": "http://arxiv.org/abs/2402.07896v2",
        "entry_id": "http://arxiv.org/abs/2402.07896v2",
        "pdf_url": "http://arxiv.org/pdf/2402.07896v2",
        "summary": "Existing methods for controlling language models, such as RLHF and\nConstitutional AI, involve determining which LLM behaviors are desirable and\ntraining them into a language model. However, in many cases, it is desirable\nfor LLMs to be controllable at inference time, so that they can be used in\nmultiple contexts with diverse needs. We illustrate this with the Pink Elephant\nProblem: instructing an LLM to avoid discussing a certain entity (a ``Pink\nElephant''), and instead discuss a preferred entity (``Grey Elephant''). We\napply a novel simplification of Constitutional AI, Direct Principle Feedback,\nwhich skips the ranking of responses and uses DPO directly on critiques and\nrevisions. Our results show that after DPF fine-tuning on our synthetic Pink\nElephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms\nLlama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on\nour curated test set assessing the Pink Elephant Problem.",
        "updated": "2024-02-13 18:44:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07896v2"
    },
    {
        "title": "Label-Efficient Model Selection for Text Generation",
        "authors": "Shir Ashury-TahanBenjamin SznajderLeshem ChoshenLiat Ein-DorEyal ShnarchAriel Gera",
        "links": "http://arxiv.org/abs/2402.07891v1",
        "entry_id": "http://arxiv.org/abs/2402.07891v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07891v1",
        "summary": "Model selection for a given target task can be costly, as it may entail\nextensive annotation of the quality of outputs of different models. We\nintroduce DiffUse, an efficient method to make an informed decision between\ncandidate text generation models. DiffUse reduces the required amount of\npreference annotations, thus saving valuable time and resources in performing\nevaluation. DiffUse intelligently selects instances by clustering embeddings\nthat represent the semantic differences between model outputs. Thus, it is able\nto identify a subset of examples that are more informative for preference\ndecisions. Our method is model-agnostic, and can be applied to any text\ngeneration model. Moreover, we propose a practical iterative approach for\ndynamically determining how many instances to annotate. In a series of\nexperiments over hundreds of model pairs, we demonstrate that DiffUse can\ndramatically reduce the required number of annotations -- by up to 75% -- while\nmaintaining high evaluation reliability.",
        "updated": "2024-02-12 18:54:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07891v1"
    },
    {
        "title": "Policy Improvement using Language Feedback Models",
        "authors": "Victor ZhongDipendra MisraXingdi YuanMarc-Alexandre Côté",
        "links": "http://arxiv.org/abs/2402.07876v1",
        "entry_id": "http://arxiv.org/abs/2402.07876v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07876v1",
        "summary": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
        "updated": "2024-02-12 18:41:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07876v1"
    },
    {
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": "Jakub KrajewskiJan LudziejewskiKamil AdamczewskiMaciej PióroMichał KrutulSzymon AntoniakKamil CiebieraKrystian KrólTomasz OdrzygóźdźPiotr SankowskiMarek CyganSebastian Jaszczur",
        "links": "http://arxiv.org/abs/2402.07871v1",
        "entry_id": "http://arxiv.org/abs/2402.07871v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07871v1",
        "summary": "Mixture of Experts (MoE) models have emerged as a primary solution for\nreducing the computational cost of Large Language Models. In this work, we\nanalyze their scaling properties, incorporating an expanded range of variables.\nSpecifically, we introduce a new hyperparameter, granularity, whose adjustment\nenables precise control over the size of the experts. Building on this, we\nestablish scaling laws for fine-grained MoE, taking into account the number of\ntraining tokens, model size, and granularity. Leveraging these laws, we derive\nthe optimal training configuration for a given computational budget. Our\nfindings not only show that MoE models consistently outperform dense\nTransformers but also highlight that the efficiency gap between dense and MoE\nmodels widens as we scale up the model size and training budget. Furthermore,\nwe demonstrate that the common practice of setting the size of experts in MoE\nto mirror the feed-forward layer is not optimal at almost any computational\nbudget.",
        "updated": "2024-02-12 18:33:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07871v1"
    }
]