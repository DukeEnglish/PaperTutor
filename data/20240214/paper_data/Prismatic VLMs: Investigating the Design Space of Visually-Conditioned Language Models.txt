Prismatic VLMs: Investigating the Design Space of
Visually-Conditioned Language Models
SiddharthKaramcheti12 SurajNair2 AshwinBalakrishna2 PercyLiang1 ThomasKollar2† DorsaSadigh1†
§ github.com/TRI-ML/prismatic-vlms ¡ github.com/TRI-ML/vlm-evaluation
Abstract
Visually-conditioned language models (VLMs)
haveseengrowingadoptioninapplicationssuch
as visual dialogue, scene understanding, and
robotic task planning; adoption that has fueled
awealthofnewmodelssuchasLLaVa,Instruct-
BLIP,andPaLI-3. Despitethevolumeofnewre-
leases,keydesigndecisionsaroundimageprepro-
cessing,architecture,andoptimizationareunder-
explored, making it challenging to understand
whatfactorsaccountformodelperformance–a
challengefurthercomplicatedbythelackofob-
jective,consistentevaluations. Toaddressthese
gaps,wefirstcompileasuiteofstandardizedeval-
uationsspanningvisualquestionanswering,ob-
jectlocalizationfromlanguage,andtargetedchal-
lenge sets that probe properties such as halluci-
nation;evaluationsthatprovidecalibrated,fine-
grainedinsightintoaVLM’scapabilities.Second,
werigorouslyinvestigateVLMsalongkeydesign
axes,includingpretrainedvisualrepresentations
and quantifying the tradeoffs of using base vs.
instruct-tunedlanguagemodels,amongstothers. Figure1.PrismaticVLMs.* Throughrigorousexperimentsex-
Wecoupleouranalysiswiththreeresourcecon- ploringthedesignspaceofvisually-conditionedlanguagemodels
tributions: (1) a unified framework for evaluat- (VLMs),weidentifyinsightsthatimprovetraining. Whencon-
trollingfordataandscale,ourmodels(orange)outperformthe
ingVLMs,(2)optimized,flexiblecodeforVLM
state-of-the-artLLaVav1.5(gray;Liuetal.,2023b)across11di-
training, and (3) checkpoints for all models, in-
versetasks,whilesavingmorethan30%thetrainingcompute.
cludingafamilyofVLMsatthe7-13Bscalethat
strictlyoutperformInstructBLIPandLLaVav1.5,
Visually-conditionedlanguagemodels(VLMs)generatenat-
thestate-of-the-artinopen-sourceVLMs.
urallanguageresponsesfromimageinputandtextprompts,
providingageneral,expressiveinterfaceforagrowingspec-
trumofapplications–groundedchat(Lietal.,2023c;Gong
1.Introduction
etal.,2023),visualprogramming(Sur’isetal.,2023;Sub-
Ifyouhavebuiltcastlesintheair,yourworkneednotbelost;that ramanianetal.,2023),roboticcontrol(Driessetal.,2023;
iswheretheyshouldbe.Nowputthefoundationsunderthem. Brohanetal.,2023),etc. Thisbroadadoptionisfueledbya
—HENRYDAVIDTHOREAU recentparadigmshiftinhowwedevelopVLMs;eschewing
thecomplexarchitecturesandtrainingobjectivesofprior
†EqualAdvising1Stanford2ToyotaResearchInstitute *Prismatic(adj)–relatingtoorhavingtheformofaprism.
Correspondenceto: <skaramcheti@cs.stanford.edu> Likeageometricprism,ourVLMsshareacommonstructure,but
arecharacterizedbydifferent“faces”–theindividualdesignaxes
Preprint.Copyright2024bytheauthor(s). weexploreinthiswork.
1
4202
beF
21
]VC.sc[
1v56870.2042:viXraInvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure2.ExploringVLMDesignAxes.WeexplorefourkeydesignaxesfordevelopingVLMs:1)optimizationprocedure,2)image
processingandpretrainedvisualrepresentations,3)languagemodels,and4)scalingpropertiesaroundtrainingtimeanddata(left).To
enablethisexploration,wemakeakeyresourcecontribution:anopen-source,flexiblecodebaseforefficientlytrainingVLMs(right).
work (Tan & Bansal, 2019; Li et al., 2022; 2023b), new First,toprovidefine-grainedinsightintoVLMcapabilities,
VLMsadoptasimpleapproach,treatpatchfeaturesfrom wecompileastandardizedevaluationsuitecomprisedof
pretrainedvisualrepresentations(e.g.,CLIP;Radfordetal., elevenbenchmarksfromthevision-and-languageliterature,
2021)astokensthatcanbeprojectedintotheinputspace including four tasks spanning visual question answering
ofalanguagemodel(LM).This“patch-as-token”approach (Bighametal.,2010;Goyaletal.,2017;Hudson&Manning,
enablestrainingwithasimpleobjective–next-tokenpredic- 2019;Singhetal.,2019),fourtasksspanningobjectlocaliza-
tion–andallowsustoharnesstheecosystemofpowerful tion(Kazemzadehetal.,2014;Yuetal.,2016;Wangetal.,
LMs such as Llama-2 and Mistral (Touvron et al., 2023; 2021),andthreechallengetasksevaluatingfine-grainedspa-
Jiangetal.,2023),alongwiththetoolstoefficientlytrain tialreasoning,robustness,andhallucination(Acharyaetal.,
them (e.g., FSDP; Zhao et al., 2023). This combination 2018;Liuetal.,2022;Lietal.,2023d).Second,wedevelop
has fueled the rapid development and release of models an optimized and modular codebase for VLM training
suchasLLaVav1.5,andPALI-3thatadoptthesameunder- thatemphasizesflexibility,allowinguserstoeasilyswapin
lyingrecipe,whilevaryingindividualingredientssuchas pretrainedcomponents,optimizationprocedures,data,and
thechoiceofpretrainedcomponents,data,oroptimization more (Fig. 2; right). Third, we use these resource contri-
procedure(Liuetal.,2023b;Chenetal.,2023b). butionstoperformtargetedexperimentsexploringfour
keydesignaxes(Fig.2;left): 1)optimizationprocedure,
Unfortunately,existingapproachesonlycoverasliverofthe
2)imageprocessingandvisualrepresentations,3)language
designspacearoundbuildingandtrainingVLMs,without
models,and4)scalingpropertiesaroundtrainingtimeand
thoroughlyevaluatingtheimpactofgivenchoicesondown-
data. We identify a number of insights; for example, we
streamcapabilities. Thismotivatesthekeyquestionofthis
findthatmulti-stagetrainingproceduresadoptedbyexist-
work: whatarethekeydesigndecisionsthatinfluenceVLM
ingworkcanbeeliminatedwithoutimpactonperformance,
capabilitiesanddownstreamuse?Toprovideanswerstothis
reducingcomputecostsby20-25%. Wealsofindthatfused
question,wefirstneedawaytothoroughlyevaluatethe
visualbackbonesthatmergefeaturesfromdifferentback-
strengthsandweaknessesofagivenmodel. Doingthisef-
bones such as CLIP (Radford et al., 2021) and DINOv2
fectivelyrequirescompilingastandardizedevaluationsuite
(Oquabetal.,2023)leadtomoreperformantVLMsacross
comprisedoftasksthatarediverseandobjective;crucially,
the board. Finally, we consolidate our findings and effi-
these tasks should allow for probing specific capabilities
cientlytrainafamilyofopen-sourcemodels–PRISMs–at
suchasspatialreasoning,out-of-distributiongeneralization,
the7B/13Bscalethatstrictlyoutperformstate-of-the-art
andcommonsenseunderstanding,amongstothers. Second,
openVLMssuchasInstructBLIPandLLaVav1.5.1
weneedtorigorouslyexploredifferentVLMdesignaxes,
notonlytobuildaconcretesetofrecommendations,butto 1Wereleaseouroptimizedtrainingcodebase,evaluationsuite,
tieindividualchoicestodownstreamperformance. andcheckpointsforallmodelstrainedaspartofthiswork.
Thisworkaddressestheseaxesthroughfourcontributions. § github.com/TRI-ML/prismatic-vlms
¡ github.com/TRI-ML/vlm-evaluation
2InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
2.Preliminaries handlearbitraryoptimizationprocedures(e.g.,freezingthe
visionbackboneduringtraining). Withtheserequirements,
Togroundouranalysis,werequire1)aVLMmodelarchitec-
weimplementourtrainingcodebaseinPyTorch,usingFully
ture,2)pretrainingdata,and3)atrainingimplementation.
ShardedDataParallel(FSDP;Zhaoetal.,2023)andBF16
Model Architecture. We adopt the general architecture mixedprecision. FSDPletsusspecifyprecisionforindi-
used by many recent VLMs, such as LLaVa, Qwen-VL, vidualmodelcomponents(e.g.,FP16forvisionbackbones,
andPaLI-3(Liuetal.,2023c;Baietal.,2023;Chenetal., BF16forLMs),enablesportabilitytodifferenthardware,
2023b). Thesearchitecturesusea(pretrained)visualback- andprovidesminimalimplementationoverhead. Following
bone to map an input image to a sequence of patch fea- reproducibilitypracticesfrompriorwork(Karamchetietal.,
turesthatarethenprojectedindividuallyintotheembedding 2021;Bidermanetal.,2023),wefixinitializationrandom-
spaceofanLM.Formally,aVLMtakesasinputanimage nessandfixbatchorderduringtraining.WeleverageTIMM
x ∈RH×W andtextprompttokensu witharbitrary (Wightman,2019)andHuggingFaceTransformers(Wolf
img prompt
sequencelengthK. Theseinputsarethenfedtothefollow- etal.,2019)toprovidepretrainedmodels.
ingcomponents: 1)avisualrepresentationbackbone,2)a
Tovalidateourcode,werunanapples-to-applesreproduc-
vision-languageprojector,and3)alanguagemodel.
tionofLLaVav1.5(Liuetal.,2023b)atboththe7Band
Visual Representation. We first process x subject to a 13Bparameterscale. Successfulreproductionresultsarein
img
visualrepresentationbackboneV thatoutputsasequence Fig.4(left). Wefindourimplementationisconsiderably
ω
of features p
img
∈ RL×hvision where p
img
= V ω(x img). As moreefficientthanthereferenceLLaVav1.5trainingim-
an example, p might be the patch features output by a plementation: when benchmarked on the same hardware
img
VisionTransformer(ViT;Dosovitskiyetal.,2021). (an AWS p4de.24xlarge node with 8 A100 GPUs), we
observe20%fastersteptimeswithourFSDP-backedimple-
Vision-Language Projector. Next, we map p to a se-
img mentation,anotablegaingivenLLaValeveragesthewell-
quenceofembeddingse
img
∈RL×htext viaalearnedprojec-
optimizedDeepSpeedZeROlibrary(Rasleyetal.,2020).
torF ,wheree =F (p ).
ψ img ψ img
Wehighlightthisopen-sourcetrainingcodebaseasone
LanguageModel. Finally,weconcatenatethesequencee
img ofthekeycontributionsofthiswork. Unlikeotheropen
withthetextpromptembeddingse =embed(u ),
prompt prompt codebases,weprovideamodularandexpressiveinterface
passing the result to the language model. The language
for easily specifying or adding model components, opti-
modelgeneratesoutputtextu =LM ([e ;e ]).
gen θ img prompt mizationprocedures,anddatawithminimalcodechanges
The composition LM ([F (V (o ));embed(u )]) (Fig.2;right). Inprovidinganefficientandeasilyextensi-
θ ψ ω rgb prompt
then defines a VLM. Given a triple (x ,u ,uˆ ) bleframework,weenablefutureresearcharounddesigning
img prompt gen
during training, we minimize the loss L(ω,ψ,θ) = newevaluations,developingandtrainingnewVLMs,and
−logp(uˆ |x ,u )viagradientdescent. finetuningorotherwiseadaptingexistingmodelsfordiverse
gen img prompt
downstream applications – all while maintaining a high
Pretraining Dataset. We limit our selection of pretrain-
standardofreproducibilityandcontrolledexperimentation.
ingdatatodatasetsthatarefullyopen-source(e.g.,under
permissiveresearchlicenses), andthathavebeenusedin
3.EvaluationSuite
priorwork. Specifically,weusetheLLaVav1.5datamix-
ture,whichconsistsoftwosubsetsusedforamulti-stage
The first contribution of this work is a unified evaluation
trainingpipeline. Thefirstsubsetconsistsofa558Ksam-
suite that offers fine-grained insight into the capabilities
plemixtureofexamplessourcedfromvariouscaptioning
ofagivenVLM.Recent workinevaluatingVLMstends
datasets(e.g.,ConceptualCaptions,LAIONSharmaetal.,
to rely on automated evaluations that use powerful LMs
2018;Schuhmannetal.,2021),whilethesecondconsists
suchasGPT-4(OpenAIetal.,2023)tojudgerelativeand
of 665K multimodal instruct tuning examples comprised
subjectiveperformance(Liuetal.,2023e;Yuetal.,2023),
ofsyntheticdatageneratedinLiuetal.(2023c),aswellas
makingithardtomeasuretheabsoluteimpactofagiven
examplesfromexistingvision-languagetrainingsets(e.g.,
designchange. Instead,wefocusonevaluationswithwell-
GQA,TextCaps;Hudson&Manning,2019;Sidorovetal.,
definedmetrics,spanningthefollowingthreeareas:
2020), andnotably, asampleoflanguage-onlydatafrom
ShareGPT(ShareGPT,2023). Weprovideacomprehensive Open-EndedVisualQuestionAnswering. Weevaluateon
breakdownofthepretrainingdatamixturein§A.1. VizWiz(Bighametal.,2010),VQAv2(Goyaletal.,2017),
GQA (Hudson & Manning, 2019), and TextVQA (Singh
TrainingImplementation&Verification. Toinvestigate
etal.,2019). BothVizWizandVQAv2assessgeneralvi-
thedesignaxesenumeratedin§1,werequirecodeforVLM
sualreasoning;VizWizalsocontainsaseriesofunanswer-
trainingthatisefficientandflexible;critically,weneedthe
able questions. GQA evaluates spatial reasoning, while
ability to easily swap out vision and LM backbones and
3InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure3.EvaluationSuiteOverview.Wecompilemultipleestablishedbenchmarksspanningvisualquestionanswering,localization,
andchallengetasks(e.g.,evaluatingcounting,spatialrelationships,propensitytohallucinate).Thisevaluationsuiteformsthebackbone
forallofouranalysis,givingusfine-grainedinsightintotheimpactofindividualVLMdesignchoices.
TextVQAassessesreasoningaroundtext(e.g.,labels,sig- 4.Experiments–InvestigatingDesignAxes
nage)presentinanimage.
Oursecondcontributionisaseriesoftargetedexperiments
Localization. Part of the pretraining data mixture (from exploringtheVLMdesignspacealongfourkeyaxes: (§4.1)
§2)containsexamplesofpredictingnormalizedbounding optimizationprocedure,(§4.2)imageprocessingandvisual
box coordinates given referring expressions in language. representations,(§4.3)languagemodels,and(§4.4)scaling
As such, we evaluate bounding box prediction accuracy propertiessuchastrainingtimeanddatadiversity.
onRefCOCO,RefCOCO+,andRefCOCOg(Kazemzadeh
ExperimentDesign: Protocols&DrawingConclusions.
et al., 2014; Yu et al., 2016), and on OCID-Ref (Wang
WefirstvalidateourVLMtrainingimplementationbyre-
etal.,2021). RefCOCOfocusesonshortdescriptionswith
producingLLaVav1.5(see§2),adoptingthedesignchoices
spatialanchors,RefCOCO+onstrictlyappearancebased
oftheoriginalwork–thesamechoicesusedbymanyother
descriptions, and RefCOCOg on long, rich descriptions;
recentVLMs: “letterboxpadding”toprocessimages,CLIP
OCID-Refisaroboticsdatasetprobingout-of-distribution
ViT-Largewithapatchsizeof14andinputresolutionof
generalization,withafocusonlocalizingobjectsinclutter.
336px(CLIPViT-L/14@336px;Radfordetal.,2021)asthe
Challenge Sets (Closed-Set Prediction). We evaluate on visualrepresentation,Vicun˜av1.5astheLMbackbone,and
VisualSpatialReasoning(VSR;Liuetal.,2022),TallyQA thetwo-stagetrainingpipelineusingbothdatasubsetsde-
(Acharyaetal.,2018),andPOPE(Lietal.,2023d). VSR scribedin§2. Successfulreproductionresultsatboththe7B
consistsofchallengingTrue/Falsequestionsaboutindivid- and13BscaleareinFig.4(left). Givenboththefidelityof
ualspatialrelationshipsindiversescenes(e.g., “thecake ourreproductionandtheprevalenceofthesedesignchoices,
is at the edge of the dining table”); this is an especially weanchorouranalysesaroundthisparameterization. Crit-
challengingtask,withmostexistingmodelsfailingtoout- ically,eachoftheexperimentsin§4.2,§4.3,and§4.4are
performthemajorityclassbaseline(51%).TallyQAconsists formulatedassingle-stepchangesofthisbasearchitecture,
of questions that assess a VLM’s ability to count objects withallotherchoicesheldconstant.
describedinlanguage,withexpressionsthatrangeincom-
Aseachevaluationin§3usesdifferentmetricswithdiffer-
plexity.Finally,POPEconsistsoftargetedYes/Noquestions
ent scales, direct comparison is challenging. We address
thatassessaVLM’spropensitytohallucinate.
thisbycomputingnormalizedZ-scoresforeachmodeland
WeusethevalidationsetsforallbenchmarksexceptGQA evaluation(usingthemeanandstandarddeviationacross
(whereusetherecommendedthetest-devsplit),VSR(where all models). These scores are used to compute statistical
weusethezero-shottestsplit),andPOPE(wherethereis significance(furtherdetailsin§B.2),andtosettherelative
only a single evaluation split). We provide further detail scalesofeachradarplot(forcompleteness,wealsoprovide
aroundevaluationprotocolsinAppx.B. theabsolutemetricsascoloredandboldedlabels).
4InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure4.ReproducingLLaVav1.5&ExploringOptimizationProcedures. Tovalidateourtrainingcodebase(§2),wereproduce
LLaVav1.5(green),withourmodelsreproducingtheperformancereportedinLiuetal.(2023b)(gray).Wethenrunourfirstexperiment
(§4.1)investigatingtheneedforexpensivemulti-stagetraining(right).Wefindthatsingle-stagetrainingproducesVLMsthatmaintainor
outperformmulti-stagemodels(orange),savingconsiderablecompute;asaresult,wecarrythischangeforwardtoallfutureexperiments.
stage,whereonlythevisualrepresentationisfrozenwhile
boththeprojectionandLMaretrained.
Adoptingmulti-stagetrainingcomplicatesimplementation
andaddstotrainingcost;therefore,asaninitialexperiment,
weevaluatetheneedforthisfirststagethroughatargeted
ablation. We compare the default two-stage training pro-
cedurewithasingle-stageapproachthatskipsdirectlyto
finetuningF andtheLM.Wefind(Fig.4;left)thatinclud-
ψ
ingtheexplicitprojectorpretrainingstageisunnecessary,
withsingle-stagetrainingimprovingaggregateperformance
(p = 0.007). Eliminatingthisfirststagesaves20-25%of
training cost, and removes the need for additional, stage-
specificdata(e.g.,thecaptioningsubsetfrom§2). Asthis
change strictly improves performance and efficiency, we
adoptsingle-stagetrainingforallfollowingexperiments.
Figure5.FullFinetuningthroughVisualBackbones. Weex- FullFinetuningthroughVisualBackbones. Anotherpop-
ploretheimpactoffinetuningthe(conventionallyfrozen)visual ulardesignchoiceinexistingVLMsthatleveragepretrained
backboneinadditiontotheprojectorandlanguagemodelduring visualrepresentationsistoleavethevisualbackbonefrozen
training.Weseethatinboththesingleandmulti-stageparadigms,
during the entirety of training (Liu et al., 2023b; Driess
finetuningthevisionbackbonedramaticallydegradesperformance
etal.,2023;Lietal.,2023b). Suchachoicelimitsthepo-
acrossalmostallbenchmarks–especiallyonlocalizationtasks.
tentialtolearnimprovedvisualrepresentationsconducive
tolanguagegenerationduringthecourseoftraining. Thus,
4.1.OptimizationProcedure
weask–istherepotentialtoimproveVLMperformanceby
Inthissectionwefocusondesignchoicesaroundtheopti- finetuning the full model, including the visual backbone?
mizationprocedureusedtoinitializeandtraineachofthe Wefind(Fig.5)thatthisisnotthecase, andthatfinetun-
threecomponentsdescribedin§2. Specifically,weexam- ingthevisualbackbonesignificantlydegradesperformance
inetheeffectsofmulti-stagetrainingwheredifferentVLM (p = 0.00407), especiallyontasksrequiringfine-grained
componentsarefrozenatdifferentpointsintraining. spatialreasoningsuchasRefCOCOandOCID-Ref.
Multi-StageTraining. Oneoftheprevalentdesignchoices Remark. The degraded performance from full finetuning
adoptedbymanyVLMs(Chenetal.,2023a;Yeetal.,2023) could be for a number of reasons ranging from the scale
istheinclusionofatwo-stagetrainingpipeline:(1)analign- and diversity of the vision-language data we train on to
mentstagetoalignvisionandlanguagefeaturesbytraining languagegenerationasalearningobjective(vs. objectives
therandomlyinitializedprojectorF inisolation,freezing that encourage learning fine-grained perceptual features).
ψ
all other components (Fig. 4, right) and (2) a finetuning Especiallygiventheexistenceofclosed-sourcemodelssuch
5InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure6.ImageProcessing&VisualRepresentations.Weexplorechoicesaroundimageprocessingandvisualrepresentationsin§4.2.
Specifically,weinvestigatetheimpactofdifferentvisualrepresentations(left),howperformancevariesasfunctionofimageprocessing
strategies(middle),andtheimpactofincreasinginputimageresolution(right).
as Fuyu-8B (AI, 2023) that adopt this paradigm to great tionssuchasclassification,croppingoutpartsofanimageis
success,webelievethatidentifyingwaystopreventsuch especiallyharmfulfortasksrequiringfull-scenereasoning.
feature collapse during VLM training (e.g., via auxiliary Inthisexperiment,weevaluatethreedifferentimagepro-
objectives)tobearichdirectionforfuturework. cessingschemes–thedefault“resize&crop”scheme,the
“letterboxpadding”schemeusedbyLLaVav1.5thatpads
4.2.ImageProcessing&VisualRepresentations non-squareimagestosquare,anda“naiveresize”scheme
that warps the original image aspect ratio, squeezing or
Choosing a Pretrained Vision Representation. CLIP
stretchinganimagetosquare. Ourfindings(Fig.6;middle)
(Radford et al., 2021) has become the default choice for
are surprising: while cropping is clearly suboptimal, the
visualrepresentationforalmostallVLMs,despiteawealth
“naiveresize”schemeisthemostperformantforCLIP.For
of visual representations trained on diverse data sources.
SigLIP,both“naiveresize”and“letterboxpadding”perform
Inthisexperiment,weperformahead-to-headcomparison
similarly. Ingeneral,ourresultsfavor“naiveresizing”over
betweenCLIP,SigLIP(Zhaietal.,2023),DINOv2(Oquab
“letterbox padding” but we cannot rule the improvement
etal.,2023),andastandardVisionTransformerpretrained
statisticallysignificant(p=0.0148).
forclassification(onImageNet-21K,finetunedonImageNet-
1K;Dosovitskiyetal.,2021;Steineretal.,2021);forfair Remark. Twospeculativeargumentsfornaivelyresizingan
comparison,weusetheViT-Large2. Wefind(Fig.6;left) imageoverpaddingarethoseofminimizing“deadpixels”
thatthebackbonestrainedwithvision-languagecontrastive anddistributionshift. Animagewitha16:9aspectratiothat
objectives (i.e., CLIP and SigLIP) are significantly more ispaddedtosquareintroducesalargeamountofuninfor-
performantthanalternatives(p=8.88e-8). mativepixels(exceeding40%);warpingtheaspectratiois
possiblylessofashift.Coupledwiththeinnatepatchdimen-
Remark. While the vision-language contrastive objective
sionalityofaVisionTransformer(d=1024fora16×16
is one explanation for the strengths of CLIP and SigLIP,
pixelpatch),naivelyresizinganimagemaypreserveenough
anotherpossibleexplanationisoneoftrainingimagedis-
informationforthedownstreamLM(with7B+parameters)
tribution. BothCLIPandSigLIPcontaininternet-sourced
toextractthepropertiesnecessaryfordownstreamtasks.
images(e.g.,sketches,diagrams,animatedgraphics,etc.)
notinImageNetorintheDINOv2pretrainingdata. ScalingImageResolution. AnothertrendinrecentVLMs
isincreasinginputimageresolutionwiththehopeofcap-
ImageProcessingacrossVisualBackbones. Mostimages
turingfine-graineddetailsthatimprovedownstreamperfor-
haveresolutionsandaspectratiosthatwidelyvary,yetmost
mance (Liu et al., 2023b; Li et al., 2023a). Our findings
visualbackbonesexpectsquareimagesatafixedsize; to
(Fig.6;right)confirmthishypothesis,withscalingto336px
reconcilethis,theoverwhelmingdefaultisto“resize&crop”
or384pxofferingsignificantimprovements(p=5.66e-4).
animagetosize. Whilethistendstoworkwellforapplica-
Remark. While scaling up image resolution seems like
2Toevaluateonanimageresolutioncommontoallrepresenta-
a clear win, we caution that it comes with a significant
tions(224px),weusetheshape-optimizedSigLIPmodel(ViT-SO
increaseincomputecomplexityforVLMsthatprojectin-
Alabdulmohsinetal.,2023)thatisslightlylargerthanaViT-Large
dividualViTpatchesintotheembeddingspaceofanLM.
at400Mparameters(vs307M).
6InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure7.EnsemblingDifferentVisualRepresentions&Basevs. Instruct-TunedLMs. Weexplorefusingvisualfeaturesfrom
DINOv2andCLIP/SigLIPmodels,andfindthatdoingsosignificantlyboostsperformanceonlocalizationandchallengeevaluations
(left).Weadditionallyevaluatethedifferencesbetweenbase(Llama-2;orange)andinstruct-tuned(Vicun˜av1.5;gray)languagemodels
(right);wefindthatLlama-2offerssimilarquantitativeperformance,whilebeinglesspronetohallucination(Fig.11).
Assumingafixedpatchgranularity,doublingtheinputreso- byvision-languagecontrastivemodels. Wenotethatthis
lutionresultsinfourtimesthenumberofinputpatchesfed conclusionmaygeneralizebeyondDINO-stylebackbones;
totheLM.Coupledwiththequadraticcostoftraditional theonlyreasonwedonotevaluatethefusionofImageNet
Transformerattentionasafunctionofsequencelength,this andCLIP/SigLIPbackbonesaswellisduetoamismatch
isasixteen-foldincreaseintimecomplexity(withacompa- inpatchgranularity(theImageNetbackboneusesapatch
rableexplosioninmemoryrequirements). granularityof16×16vs. the14×14granularityusedby
allotherbackbones). Webelievethatfurtherexploringthe
Ensembling Different Visual Representations. A rich
impactonthesetypeoffused,multi-resolutionfeaturesfor
bodyofpriorworkinvisionidentifiesthatdifferenttypes
VLMsisacompellingavenueforfuturework.
of visual representations trained with different inductive
biasescanleadtoimprovedperformanceforabroadspec-
4.3.IntegratingLanguageModels
trumofapplications(Kobayashietal.,2022;Karamcheti
etal.,2023). Motivatedbythis,weaskifthissametrend Base vs. Instruct-Tuned LMs. Instruct tuning (or chat
holdstrueforVLMtraining–specificallywhetherensem- tuning;Ouyangetal.,2022;Chungetal.,2022)isaway
bling DINOv2 features with vision-language contrastive tofinetunebaseLMs(trainedfornext-tokenprediction)to
featuresfromCLIPandSigLIPcanleadtoimprovedperfor- behaveasdialogueagents,offeringanaturalinput/output
mance,followingtheapproachtakeninKerretal.(2023). interfaceforawidespectrumofapplications. Asaresult,
Toimplementthisefficiently,wesimplyconcatenatepatch instructtunedmodelslikeVicun˜a(Zhengetal.,2023)have
featuresfromdifferentbackbonesalongthechanneldimen- become the default backbone for VLMs. Unfortunately,
sionforeachpatch,resultinginthesamenumberofinput instructtuninghasdrawbacks,introducingbiasandregres-
patchembeddings,justwithdoublethefeaturedimension. sionsinperformance(Ouyangetal.,2022). Thus,inthis
Toadjustforthis,wejustincreasetheinputdimensionto experiment we evaluate the impact of instruct-tuned LM
our projector F ψ (a 2-layer MLP) at negligible cost. We backbonesondownstreamVLMperformanceviaahead-
find(Fig.7-left)thatfusingDINOv2andSigLIPfeatures to-headcomparisonbetweenabaseLM(Llama-2;Touvron
providessignificantgainsacrosstheboard(p=0.00162), et al., 2023), and an instruct-tuned variant (Vicun˜a v1.5).
withanotableexceptionfortheDINOv2+CLIPmodels Wefind(Fig.7-right)thatinstruction-tunedLMsyieldno
(p=0.4066),wherecombiningDINOv2featuresseemto statisticallysignificantimprovementinperformanceover
beparticularlyharmfulonTextVQA.Lookingattheremain- baseLMs(p=0.373),butdifferinqualitativeperformance.
ingresults,weseeespeciallyimpressivegainsof5-10%on Specifically, we observe that instruct-tuned LMs lead to
localizationandchallengetasks;ingeneral,theDINOv2+ VLMsthataremoreverbose, pronetohallucination, and
SigLIPfusedrepresentationsarethemostperformantvisual generallylessspecificintheirresponses(Fig.11).
representationswetry,withvirtuallynoaddedparameters.
Co-trainingonLanguage-onlySafetyData. TheLLaVa
Remark. Following the hypotheses in Kerr et al. (2023) v1.5pretrainingdatasetweusefortrainingconsistsof40K
andsimilarwork,webelievethatDINOv2featuresprovide examples of language-only data sourced from ShareGPT
featuresthatcapturelow-levelspatialpropertiesofanimage, (ShareGPT, 2023); this data consists of a diverse set of
augmentingthehigher-level“semantic”propertiescaptured user-uploadedconversationswithOpenAI’sChatGPT;cru-
7InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure8.[Warning–Racism]RemovingLanguage-onlyCo-trainingData.Wefindthatremovinglanguage-onlydataduringtraining
haslittleimpactonbenchmarks(left),itnegativelyimpactsthesafetyofVLMstrainedwithbaseLMs.Inthisexample(right),aVLM
derivedfromLlama-2exhibitsclearracistbehavior,whileco-traininginducessafeguards.
cially many of the examples in this dataset contain toxic, compareperformancewhentrainingatdifferentnumbersof
inappropriate, or otherwise unsafe inputs, and the corre- epochs. Unsurprisingly,wefind(Fig.10;middle)evidence
sponding “guarded” responses from ChatGPT (e.g., “as ofsevereunderfittingwithasingleepoch,withsteadyim-
an AI, I cannot comment on...”). In this experiment, we provement(especiallyfortasksrequiringstructuredoutputs
ablatetheimpactofco-trainingonthislanguage-onlydata such as RefCOCO) until two epochs, when performance
ondownstreamperformance,withagoalofunderstanding plateaus. Wefindthattrainingfortwoepochsyieldsasig-
ifaddinglanguage-onlydataunrelatedtovisualreasoning nificantincreaseinimprovementovertrainingforoneepoch
hurtsperformancerelativetotrainingonmultimodaldata (p=0.00440).
alone. Wefind(Fig.8; left)thatremovinglanguage-only
AddingAdditionalVision-LanguageData.Weinvestigate
dataonlyslightlyimprovesperformance(p=0.194).
addeddataaffectsdownstreamperformance. Weidentify
However, given that the language-only data is the only tworecentlyproposeddatasets: LVIS-Instruct-4V(Wang
source of “safety” data during finetuning, we explicitly etal.,2023), obtainedbypromptingGPT-4Vtogenerate
probeourVLMswithdirectlyoffensiveandtoxicprompts, rich synthetic examples from images sourced from LVIS
to evaluate how important this data is for inducing safe- (Guptaetal.,2019),andLRV-Instruct(Liuetal.,2023a)that
guardsonVLMoutputs. Inouradversarialtesting,wefind specificallyoptimizesforimagediversityrelativetoexisting
that especially for VLMs trained from base LMs such as datasets(addinge.g.,charts,scientificdiagrams,andnews
Llama-2, including this co-training data is important for printings). Wefind(Fig.10;right)thataddingbothdatasets
inducingatleastaminimalsetofsafeguards;Fig.8demon- improvesperformance(p=0.0138),butthatLRV-Instruct
stratestheimportanceofco-trainingonVLMgenerations offersmostoftheresultingperformancegain,anindication
whenpromptedwithquestionswithdirectracistintent. thatsourcingdiverseimageswillbeincreasinglyimportant
toscalingfutureVLMs.
Remark. We focus our probing mostly around unsafe re-
sponsesaroundracism,xenophobia,andgenderbias. These
biases are also prevalent in language, and explicitly rep- 5. PRISM –DistillingKeyInsights
resented in the ShareGPT co-training data. We address
WeidentifyaseriesofindividualinsightsthatsimplifyVLM
VLM-specificharmsinourdiscussionofbroaderimpacts.
trainingandimprovedownstreamperformance:
4.4.ScalingProperties: TrainingTime&Data 1)OptimizationProcedure: Single-stagetrainingreduces
computecostwithoutharmingdownstreamperformance.
Inthissection,weinvestigatehowexistingVLMsscalewith
2) Image Processing and Visual Representations: Fused
trainingtimeandaddeddata;critically,weexaminechoices
DINOv2andSigLIPbackboneswithhighresolutionimages
oftrainingtime(areweundertrainingourmodels),andhow
andnaiveimageresizingyieldstrongperformance.
addingdiversedatasetsimpactsdownstreamperformance.
3) Language Models: Base LMs such as Llama-2 match
AreweUndertraining? Weexploretheimpactoftraining
orexceedtheperformanceofinstruct-tunedLMs,withco-
timeasafunctionoftrainingepochs.UnlikeexistingVLMs
trainingonlanguage-onlydataimportantforsafety.
likePaLIorLLaVathatperformatmostasingleepoch,we
8InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure9.PRISM:CombiningInsightsforTrainingVLMs. Wedistillourexperimentalresultsfrom§4intoaseriesofkeyinsights
fortrainingVLMs. OurresultingfamilyofVLMs–PRISMs–adopt1)single-stagetrainingpipelines,2)fusedDINOv2andSigLIP
representationswithnaiveimageresizing,3)baseLMs,and4)trainonmultipledatasources,fortwoepochs.
4)ScalingProperties: Addingdiversedataandextending abilitytocarryonextendingdialoguesthatflitacrosstopics
trainingtimesignificantlyboostperformance. groundedinavisualcontext. Whilesomeoftheautomated
evaluationsdiscussedin§3provideinitialstepsforevaluat-
Asafinalstep,wecombinetheseinsightstoinformanew
ingtheseopen-endedbehaviors,futureworkwillinvestigate
family of VLMs – PRISMs – at the 7B and 13B param-
howtoextendsuchevaluationstolonger,richercontexts.
eter scale. We present results comparing our DINOv2 +
Related to this are the downstream applications built on
SigLIP PRISM models to InstructBLIP and LLaVa v1.5
topofbroadlycapableVLMs–applicationssuchasusing
inFig.9. Weadditionallyrunahead-to-headcomparison
VLMstolearnroboticcontrolpoliciesorforvisualprogram-
againstLLaVav1.5,trainingamodel–PRISM(Controlled)
ming(Brohanetal.,2023;Sur’isetal.,2023);acompelling
–giventheexactsamedataandtrainingbudget. Bothsets
avenueforfutureworkisunderstandinghowtoco-design
ofPRISMmodelsuniformlyoutperformbothbaselinesby
VLMswiththedownstreamapplicationstoboosttheperfor-
largemarginsacrossourevaluationsuite,withstrongquali-
manceofboththeupstreamVLM,anddownstreamsystem.
tativeperformance(Fig.9;right).
7.Conclusion
6.Limitations&FutureWork
We present a rigorous investigation of the design space
WhilethisworkfocusesonrigorouslystudyingVLMdesign
ofvisually-conditionedlanguagemodels,distillingkeyin-
choices,therearetwokeylimitationsinourapproach. Of
sightsfortrainingfuturemodels. Thisinvestigationisen-
primaryconcernisthegeneralityofourmodelarchitecture;
abledbytwokeyresourcecontributions: 1)anevaluation
whilethethreecomponentarchitecturewedefinein§2is
suitethatenablesfine-grainedinsightintoaVLM’scapa-
reflectiveofthemajorityofexistingVLMs,thereareother
bilities,and2)anoptimized,extensiblecodebasefortrain-
architectureinnovationsandoptimizationproceduresthat
ingVLMswithanemphasisonflexibility–flexibilityover
our study does not currently capture; as a notable exam-
optimizationprocedures,imageprocessingandvisualrep-
ple,wedonotstudythePerceiver-basedarchitecturesused
resentations, language models, and scaling. Our insights
by models such as Flamingo or IDEFICS (Alayrac et al.,
allow us to train a family of VLMs – PRISMs – that out-
2022; Laurenc¸on et al., 2023) for interleaved image-text
performstate-of-the-artopenVLMssuchasInstructBLIP
training. Thoughmanyofourtakeawaysaregeneral(e.g.,
andLLaVa-v1.5. However,thesemodelsaresecondaryto
thesemodelsalsousecontrastivebackbonessuchasCLIP,
thecentralgoalofthiswork–establishingafoundationfor
andautoregressiveLMs),thereremainopenquestionsasto
futureworkintrainingandevaluatingVLMs. Wehopethat
theextentourfindingstransfertodifferentarchitecturesor
ourinvestigationandresourcesserveasastartingpoint;a
atlargerscale(e.g.,70BLMs).
templateforreasoningaboutwhatmattersindevelopingthe
A separate limitation is that of evaluation; we make the nextgenerationofbroadlycapableVLMs.
intentional choice in this work to focus on standardized
evaluations,withobjectivemetrics. Whilethisletsusprobe
fine-grained capabilities, we do not capture the scope of
the dyadic interactions afforded by existing VLMs – the
9InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
BroaderImpacts modelstendto“imagine”objectsorpropertiesofascene
thatarethenreinforcedoversubsequentinteractions. For
We take the established position that building visually-
thisreason,weincludebothVizWiz(Bighametal.,2010)
conditioned language models models in the open – with
andPOPE(Lietal.,2023d)inourevaluationsuite;VizWiz
opendata,open(andefficient)trainingcode,andopeneval-
hasaseriesofcommonsensequestionsandunanswerable
uationcode–isstrictlybeneficialforthebroadermachine
questionsthatareexplicitlyusedtoprobemodelreliability.
learning community and the public (Zellers et al., 2019;
POPEisabenchmarkspecificallycreatedtoevaluatehal-
Touvronetal.,2023). Beingtransparentandensuringthat
lucinationatdifferentdifficulties(e.g.,whenaskedabout
ourworkisaccessibletoallstakeholdersiskeytomitigating
adversarialobjectsthathavestrongco-occurrencewiththe
risksandempoweringthepositiveuseofVLMs.Tothisend
typeofscenedepictedinanimage,generallypopularob-
wediscusstheharmsofourwork,andVLMsmorebroadly
jects,etc.). Wehopethatbyincludingthesetasksaspart
overthefollowingparagraphs,inadditiontomakingseveral
of our evaluation suite, future VLMs move towards mak-
opensourceresoucecontributions: (1)Acodebaseforeffi-
ing designchoices thatlead toreduced hallucinationand
cient,optimizedVLMtraining. (2)Anevaluationsuitefor
improvedreliability(andvice-versa).
evaluatingfine-grainedVLMcapabilities. (3)Thecomplete
setofpretrainedmodelcheckpointsforallVLMstrained
7.2.BenefitsandPotentialOpportunities
inthiswork–includingthosewithknownracistandtoxic
behaviorfromFig.8. In §1 and §6, we discuss applications where VLMs are
alreadymakingapositiveimpact,acceleratingresearchin
7.1.RisksandKnownBiases areassuchasrobotics,visualprogrammingandmore. Here,
we speak specifically as to the benefits and opportunities
Visually-conditionedlanguagemodelsinheritalloftherisks
that our work – specifically our resource contributions –
andbiasesassociatedwithlanguagemodels(Touvronetal.,
provideforthebroaderresearchcommunity.
2023;Brownetal.,2020),aswellaswithunderlyingvision
modelsandcorrespondingpretrainingdatasets(Schuhmann Training and Finetuning Accessibility. One of the key
etal.,2021;Linetal.,2014). benefitsofourVLMtrainingcodebaseisitsefficiency;to
fully train a 7B parameter VLM (e.g., PRISM 7B (Con-
ToxicandUnsafeOutputs. AsshowninFig.8,VLMsare
trolled);Fig.9),takeslessthan9hourson8A100GPUs,
capableofgeneratingtoxicandunsafecontent. Thisistrue
withfinetuningandevaluationpossibleonindividualGPUs
withandwithout“safeguards”inplace(e.g.,safetytuning
(orevenCPU);thisisinsharpcontrasttoexistingcodebases
data).Aswementionin§4.3,ourexplorationinthisworkis
forVLMtrainingthatarefarlessefficient. Reducingthe
cursory,butrevealsthepotentialforgeneratingracist,sexist,
barriertoentryfortryingnewideasaroundVLMdevelop-
abusive,andotherwiseunsafelanguage. Whileincluding
mentiskeytoenablingprogressinriskmitigation,robust
safety-tuningdatainthetrainingmixisonelow-effortway
evaluation, and integration for downstream applications.
topreventtheeaseofgeneratingtoxiccontent(atminimal
Furthermore,theflexibilityofourtrainingcodebaseenables
cost to performance as we show in our work), it is not
swappinginsmaller,morecompute-efficientcomponents
enough. VLMsareespeciallyvulnerabletoadversarialor
(e.g.,newLMsatthe1Bscale).
evenout-of-distributionimageinputsthatmayinadvertently
triggerunsafeoutput(Qietal.,2023;Liuetal.,2023d). We ExtendingourEvaluationSuite. Ourevaluationsuiteis
hopethattheaccessibilityofourtrainingcodeandmodels written in a way that makes it easy to add and evaluate
enablesfutureresearchinmitigatingsuchproblems. new VLMs, as well as add new tasks. It is our plan to
continuallyextendoursuitewithnewevaluations(especially
WesternBias&(American)EnglishBias. Thedataand
those probing for bias, toxicity, hallucination, and other
pretrainedlanguagemodelsusedinthisworkreflectaheavy
unsafeorundesirablebehaviors),astheyarereleased.
biastowardsAmericanEnglishandcorrespondingcultural
norms. WhiletheLMsweuseinthisworkareexposedto
Acknowledgements
somemultilingualdata(withourVLMsshowingsomeabil-
itytohandlesimplephrasesinlanguagessuchasSpanish,
ToyotaResearchInstitute(“TRI”)providedfundstosupport
French,andChinese),akeylimitationisinourvisualdata
thiswork. SiddharthKaramchetiisgratefultobesupported
diversity. Ourpretrainingimagesaresourcedfromdatasets
bytheOpenPhilanthropyProjectAIFellowship.Finally,we
suchasCOCO(Linetal.,2014),sourcedprimarilyfrom
wouldliketothankAdrienGaidon,RaresAmbrus,Achal
(English)subsetsofFlickr.
Dave,BlakeWulfe,MashaItkina,JeanMercat,IgorVasilje-
Factuality, Hallucination, & Reliability. A known lim- vic,SedrickKeh,KushalArora,JohnThickstun,andDavid
itation of both LMs and VLMs is that of factuality and Hallfortheirinsightandadviceduringtheearlydevelop-
hallucination;forVLMsthisisespeciallyproblematic,as mentofthiswork.
10InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
References Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
J.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,
Acharya,M.,Kafle,K.,andKanan,C. TallyQA:Answer-
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
ingcomplexcountingquestions. InAssociationforthe
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.M.,Wu,
AdvancementofArtificialIntelligence(AAAI),2018. 2,4
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
M., Gray, S., Chess, B., Clark, J., Berner, C., McCan-
AI,A. Fuyu-8b: AmultimodalarchitectureforAIagents,
dlish, S., Radford, A., Sutskever, I., and Amodei, D.
2023. 6
Languagemodelsarefew-shotlearners. arXivpreprint
Alabdulmohsin,I.M.,Zhai,X.,Kolesnikov,A.,andBeyer, arXiv:2005.14165,2020. 10
L. Getting ViT in shape: Scaling laws for compute-
Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and
optimalmodeldesign. arXivpreprintarXiv:2305.13035,
Zhao,R. Shikra: Unleashingmultimodalllmsreferential
2023. 6
dialoguemagic. arXivpreprintarXiv:2306.15195,2023a.
5
Alayrac,J.-B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Has-
son, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, Chen,X.,Wang,X.,Beyer,L.,Kolesnikov,A.,Wu,J.,Voigt-
M.,Ring,R.,Rutherford,E.,Cabi,S.,Han,T.,Gong,Z., laender,P.,Mustafa,B.,Goodman,S.,Alabdulmohsin,
Samangooei,S.,Monteiro,M.,Menick,J.,Borgeaud,S., I.M.,Padlewski,P.,Salz,D.M.,Xiong,X.,Vlasic,D.,
Brock,A.,Nematzadeh,A.,Sharifzadeh,S.,Binkowski, Pavetic, F., Rong, K., Yu, T., Keysers, D., Zhai, X.-Q.,
M., Barreira, R., Vinyals, O., Zisserman, A., and Si- andSoricut,R. PaLI-3visionlanguagemodels: Smaller,
monyan, K. Flamingo: a visual language model for faster,stronger. arXivpreprintarXiv:2310.09199,2023b.
few-shot learning. In Advances in Neural Information 2,3
ProcessingSystems(NeurIPS),2022. 9
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Bai,J.,Bai,S.,Yang,S.,Wang,S.,Tan,S.,Wang,P.,Lin, Fedus,W.,Li,E.,Wang,X.,Dehghani,M.,Brahma,S.,
J.,Zhou,C.,andZhou,J. Qwen-vl: Aversatilevision- Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
languagemodelforunderstanding,localization,textread- Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu,
ing,andbeyond. arXivpreprintarXiv:2308.12966,2023. A.W.,Zhao,V.,Huang,Y.,Dai,A.M.,Yu,H.,Petrov,S.,
3 hsinChi,E.H.,Dean,J.,Devlin,J.,Roberts,A.,Zhou,
D.,Le,Q.V.,andWei,J. Scalinginstruction-finetuned
Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, languagemodels.arXivpreprintarXiv:2210.11416,2022.
H.,O’Brien,K.,Hallahan,E.,Khan,M.A.,Purohit,S., 7
Prashanth,U.S.,Raff,E.,etal.Pythia:Asuiteforanalyz-
inglargelanguagemodelsacrosstrainingandscaling. In Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,
InternationalConferenceonMachineLearning(ICML), W.,Li,B.A.,Fung,P.,andHoi,S.C.H. InstructBLIP:
2023. 3 Towardsgeneral-purposevision-languagemodelswithin-
structiontuning. arXivpreprintarXiv:2305.06500,2023.
Bigham,J.P.,Jayant,C.,Ji,H.,Little,G.,Miller,A.,Miller, 19
R.C., Miller, R., Tatarowicz, A., White, B., White, S.,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
andYeh,T. VizWiz: nearlyreal-timeanswerstovisual
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
questions. In User Interface Software and Technology
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
(UIST),pp.333–342,2010. 2,3,10
N. An image is worth 16x16 words: Transformers for
Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Choro- imagerecognitionatscale. InInternationalConference
manski,K.,Ding,T.,Driess,D.,Finn,C.,Florence,P.R., onLearningRepresentations(ICLR),2021. 3,6,18
Fu,C.,Arenas,M.G.,Gopalakrishnan,K.,Han,K.,Haus-
Driess,D.,Xia,F.,Sajjadi,M.S.M.,Lynch,C.,Chowdhery,
man,K.,Herzog,A.,Hsu,J.,Ichter,B.,Irpan,A.,Joshi,
A.,Ichter,B.,Wahid,A.,Tompson,J.,Vuong,Q.H.,Yu,
N.J.,Julian,R.C.,Kalashnikov,D.,Kuang,Y.,Leal,I.,
T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth,
Levine, S., Michalewski, H., Mordatch, I., Pertsch, K.,
D.,Levine,S.,Vanhoucke,V.,Hausman,K.,Toussaint,
Rao,K.,Reymann,K.,Ryoo,M.S.,Salazar,G.,Sanketi,
M.,Greff,K.,Zeng,A.,Mordatch,I.,andFlorence,P.R.
P.R.,Sermanet,P.,Singh,J.,Singh,A.,Soricut,R.,Tran,
Palm-e: Anembodiedmultimodallanguagemodel. In
H., Vanhoucke, V., Vuong, Q. H., Wahid, A., Welker,
InternationalConferenceonMachineLearning(ICML),
S.,Wohlhart,P.,Xiao,T.,Yu,T.,andZitkovich,B. RT-
2023. 1,5
2: Vision-language-action models transfer web knowl-
edgetoroboticcontrol. arXivpreprintarXiv:2307.15818, Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A.,
2023. 1,9 Zhang,W.,Lu,P.,He,C.,Yue,X.,Li,H.,andQiao,Y.J.
11InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Llama-adapterv2: Parameter-efficientvisualinstruction Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
model. arXivpreprintarXiv:2304.15010,2023. 18 Kravitz, J., Chen, S., Kalantidi, Y., Li, L.-J., Shamma,
D.A.,Bernstein,M.S.,andLi,F.-F.Visualgenome:Con-
Gong,T.,Lyu,C.,Zhang,S.,Wang,Y.,Zheng,M.,Zhao,Q., nectinglanguageandvisionusingcrowdsourceddense
Liu,K.,Zhang,W.,Luo,P.,andChen,K. Multimodal- imageannotations. InternationalJournalofComputer
GPT: A vision and language model for dialogue with Vision,123:32–73,2017. 17
humans. ArXiv,0,2023. 1
Laurenc¸on, H., Saulnier, L., Tronchon, L., Bekman, S.,
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Singh,A.,Lozhkov,A.,Wang,T.,Karamcheti,S.,Rush,
Parikh, D. Making the V in VQA matter: Elevating A.M.,Kiela,D.,Cord,M.,andSanh,V. OBELICS:An
the role of image understanding in visual question an- openweb-scalefiltereddatasetofinterleavedimage-text
swering. In Computer Vision and Pattern Recognition documents. InNeuralInformationProcessingSystems
(CVPR),2017. 2,3,17 TrackonDatasetsandBenchmarks(NeurIPSDatasets
andBenchmarks),2023. 9
Gupta,A.,Dolla´r,P.,andGirshick,R.B. LVIS:Adataset
forlargevocabularyinstancesegmentation. InComputer Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., and Liu, Z.
VisionandPatternRecognition(CVPR),2019. 8 Otterhd: Ahigh-resolutionmulti-modalitymodel. arXiv
preprintarXiv:2311.04219,2023a. 6
Hendrycks,D.andGimpel,K. Gaussianerrorlinearunits
(gelus). arXivpreprintarXiv:1606.08415,2016. 18 Li, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: Boot-
strappinglanguage-imagepre-trainingforunifiedvision-
Hudson,D.A.andManning,C.D. GQA:Anewdatasetfor
languageunderstandingandgeneration. InInternational
real-worldvisualreasoningandcompositionalquestion
ConferenceonMachineLearning(ICML),2022. 2,16,
answering. InComputerVisionandPatternRecognition
17
(CVPR),2019. 2,3,17
Li,J.,Li,D.,Savarese,S.,andHoi,S.C.H. BLIP-2: Boot-
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
strappinglanguage-imagepre-trainingwithfrozenimage
Chaplot,D.S.,deLasCasas,D.,Bressand,F.,Lengyel,
encoders and large language models. In International
G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux,
ConferenceonMachineLearning(ICML),2023b. 2,5
M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
Lacroix,T.,andSayed,W.E. Mistral7b. arXivpreprint Li,K.,He,Y.,Wang,Y.,Li,Y.,Wang,W.,Luo,P.,Wang,
arXiv:2310.06825,2023. 2 Y.,Wang,L.,andQiao,Y. Videochat: Chat-centricvideo
understanding. ArXiv,0,2023c. 1
Karamcheti, S., Orr, L., Bolton, J., Zhang, T., Goel, K.,
Narayan,A.,Bommasani,R.,Narayanan,D.,Hashimoto, Li,X.L.andLiang,P.Prefix-tuning:Optimizingcontinuous
T.,Jurafsky,D.,Manning,C.D.,Potts,C.,Re´,C.,and promptsforgeneration.InAssociationforComputational
Liang,P. Mistral-ajourneytowardsreproduciblelan- Linguistics(ACL),2021. 18
guagemodeltraining,2021. 3
Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,androng
Karamcheti,S.,Nair,S.,Chen,A.S.,Kollar,T.,Finn,C., Wen,J. Evaluatingobjecthallucinationinlargevision-
Sadigh,D.,andLiang,P. Language-drivenrepresentation languagemodels. InEmpiricalMethodsinNaturalLan-
learningforrobotics. InRobotics: ScienceandSystems guageProcessing(EMNLP),2023d. 2,4,10
(RSS),2023. 7
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Ramanan, D., Dolla´r, P., and Zitnick, C. L. Microsoft
ReferItGame: Referringtoobjectsinphotographsofnat- COCO:Commonobjectsincontext. InEuropeanCon-
uralscenes. InEmpiricalMethodsinNaturalLanguage ferenceonComputerVision(ECCV),pp.740–755,2014.
Processing(EMNLP),pp.787–798,2014. 2,4,17 10,17
Kerr, J., Kim, C. M., Goldberg, K., Kanazawa, A., and Liu, F., Emerson, G.E.T., andCollier, N. Visualspatial
Tancik,M. LERF:Languageembeddedradiancefields. reasoning. TransactionsoftheAssociationforComputa-
InInternationalConferenceonComputerVision(ICCV), tionalLinguistics(TACL),11:635–651,2022. 2,4
2023. 7
Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and
Kobayashi,S.,Matsumoto,E.,andSitzmann,V. Decom- Wang,L. Mitigatinghallucinationinlargemulti-modal
posingnerfforeditingviafeaturefielddistillation. arXiv models via robust instruction tuning. arXiv preprint
preprintarXiv:2205.15585,2022. 7 arXiv:2306.14565,2023a. 8,16
12InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved base- P.,Makanju,A.A.,Malfacini,K.,Manning,S.,Markov,
lines with visual instruction tuning. arXiv preprint T.,Markovski,Y.,Martin,B.,Mayer,K.,Mayne,A.,Mc-
arXiv:2310.03744,2023b. 1,2,3,5,6,17,18 Grew, B., McKinney, S. M., McLeavey, C., McMillan,
P.,McNeil,J.,Medina,D.,Mehta,A.,Menick,J.,Metz,
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
L.,Mishchenko,A.,Mishkin,P.,Monaco,V.,Morikawa,
tuning. InAdvancesinNeuralInformationProcessing
E.,Mossing,D.P.,Mu,T.,Murati,M.,Murk,O.,Me`ly,
Systems(NeurIPS),2023c. 3,17
D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A.,
Ngo,R.,Noh,H.,Long,O.,O’Keefe,C.,Pachocki,J.W.,
Liu, X., Zhu, Y., Lan, Y., Yang, C., and Qiao, Y. Query-
Paino,A.,Palermo,J.,Pantuliano,A.,Parascandolo,G.,
relevant images jailbreak large multi-modal models.
Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,
arXivpreprintarXiv:2311.17600,2023d. 10
A.,Perelman,A.,deAvilaBelbutePeres,F.,Petrov,M.,
Liu,Y.,Duan,H.,Zhang,Y.,Li,B.,Zhang,S.,Zhao,W., deOliveiraPinto,H.P.,Pokorny,M.,Pokrass,M.,Pong,
Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., andLin, V.H.,Powell,T.,Power,A.,Power,B.,Proehl,E.,Puri,
D. MMBench: Isyourmulti-modalmodelanall-around R.,Radford,A.,Rae,J.,Ramesh,A.,Raymond,C.,Real,
player? arXivpreprintarXiv:2307.06281,2023e. 3 F.,Rimbach,K.,Ross,C.,Rotsted,B.,Roussez,H.,Ry-
der,N.,Saltarelli,M.D.,Sanders,T.,Santurkar,S.,Sastry,
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. G.,Schmidt,H.,Schnurr,D.,Schulman,J.,Selsam,D.,
OK-VQA: A visual question answering benchmark re- Sheppard,K.,Sherbakov,T.,Shieh,J.,Shoker,S.,Shyam,
quiring external knowledge. In Computer Vision and P.,Sidor,S.,Sigler,E.,Simens,M.,Sitkin,J.,Slama,K.,
PatternRecognition(CVPR),2019. 17 Sohl, I., Sokolowsky, B. D., Song, Y., Staudacher, N.,
Such,F.P.,Summers,N.,Sutskever,I.,Tang,J.,Tezak,
Mishra,A.,Shekhar,S.,Singh,A.K.,andChakraborty,A.
N.A.,Thompson,M.,Tillet,P.,Tootoonchian,A.,Tseng,
OCR-VQA:Visualquestionansweringbyreadingtext
E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C.,
in images. In International Conference on Document
Vallone,A.,Vijayvergiya,A.,Voss,C.,Wainwright,C.,
AnalysisandRecognition(ICDAR),2019. 17
Wang,J.J.,Wang,A.,Wang,B.,Ward,J.,Wei,J.,Wein-
mann,C.,Welihinda,A.,Welinder,P.,Weng,J.,Weng,L.,
OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
Wiethoff,M.,Willner,D.,Winter,C.,Wolrich,S.,Wong,
Akkaya,I.,Aleman,F.L.,Almeida,D.,Altenschmidt,J.,
H.,Workman,L.,Wu,S.,Wu,J.,Wu,M.,Xiao,K.,Xu,
Altman,S.,Anadkat,S.,Avila,R.,Babuschkin,I.,Bal-
T.,Yoo,S.,Yu,K.,Yuan,Q.,Zaremba,W.,Zellers,R.,
aji,S.,Balcom,V.,Baltescu,P.,Bao,H.,Bavarian,M.,
Zhang,C.,Zhang,M.,Zhao,S.,Zheng,T.,Zhuang,J.,
Belgum,J.,Bello,I.,Berdine,J.,Bernadett-Shapiro,G.,
Zhuk,W.,andZoph,B. GPT-4technicalreport. arXiv
Berner,C.,Bogdonoff,L.,Boiko,O.,Boyd,M.,Brakman,
preprintarXiv:2303.08774,2023. 3,17
A.-L.,Brockman,G.,Brooks,T.,Brundage,M.,Button,
K.,Cai,T.,Campbell,R.,Cann,A.,Carey,B.,Carlson, Oquab, M., Darcet, T., Moutakanni, T., Vo, H. Q.,
C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Szafraniec,M.,Khalidov,V.,Fernandez,P.,Haziza,D.,
Chen,D.,Chen,S.,Chen,R.,Chen,J.,Chen,M.,Chess, Massa,F.,El-Nouby,A.,Assran,M.,Ballas,N.,Galuba,
B.,Cho,C.,Chu,C.,Chung,H.W.,Cummings,D.,Cur- W.,Howes,R.,Huang,P.-Y.B.,Li,S.-W.,Misra,I.,Rab-
rier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., bat, M. G., Sharma, V., Synnaeve, G., Xu, H., Je´gou,
Deville,D.,Dhar,A.,Dohan,D.,Dowling,S.,Dunning, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski,
S.,Ecoffet,A.,Eleti,A.,Eloundou,T.,Farhi,D.,Fedus, P. DINOv2: Learningrobustvisualfeatureswithoutsu-
L.,Felix,N.,Fishman,S.P.,Forte,J.,Fulford,I.,Gao,L., pervision. TransactionsofMachineLearningResearch
Georges,E.,Gibson,C.,Goel,V.,Gogineni,T.,Goh,G., (TMLR),2023. 2,6
Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,
Greene,R.,Gross,J.,Gu,S.S.,Guo,Y.,Hallacy,C.,Han, Ordonez, V., Kulkarni, G., andBerg, T.L. Im2Text: De-
J.,Harris,J.,He,Y.,Heaton,M.,Heidecke,J.,Hesse,C., scribingimagesusing1millioncaptionedphotographs.
Hickey,A.,Hickey,W.,Hoeschele,P.,Houghton,B.,Hsu, InAdvancesinNeuralInformationProcessingSystems
K.,Hu,S.,Hu,X.,Huizinga,J.,Jain,S.,Jain,S.,Jang,J., (NeurIPS),2011. 17
Jiang,A.,Jiang,R.,Jin,H.,Jin,D.,Jomoto,S.,Jonn,B.,
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
Jun,H.,Kaftan,T.,Kaiser,L.,Kamali,A.,Kanitscheider,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W.,
Ray,A.,Schulman,J.,Hilton,J.,Kelton,F.,Miller,L.E.,
Kim,C.,Kim,Y.,Kirchner,H.,Kiros,J.R.,Knight,M.,
Simens,M.,Askell,A.,Welinder,P.,Christiano,P.,Leike,
Kokotajlo,D.,Kondraciuk,L.,Kondrich,A.,Konstantini-
J.,andLowe,R.J. Traininglanguagemodelstofollow
dis,A.,Kosic,K.,Krueger,G.,Kuo,V.,Lampe,M.,Lan,
instructionswithhumanfeedback. arXiv,2022. 7
I.,Lee,T.,Leike,J.,Leung,J.,Levy,D.,Li,C.M.,Lim,
R.,Lin,M.,Lin,S.,Litwin,M.,Lopez,T.,Lowe,R.,Lue, Qi, X., Huang, K., Panda, A., Wang, M., and Mittal, P.
13InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Visualadversarialexamplesjailbreakalignedlargelan- Sur’is,D.,Menon,S.,andVondrick,C. ViperGPT:Visual
guagemodels. arXivpreprintarXiv:2306.13213,2023. inferenceviaPythonexecutionforreasoning. InInterna-
10 tionalConferenceonComputerVision(ICCV),2023. 1,
9
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, Tan, H. H. and Bansal, M. LXMERT: Learning cross-
J.,Krueger,G.,andSutskever,I. Learningtransferable modalityencoderrepresentationsfromtransformers. In
visual models from natural language supervision. In Empirical Methods in Natural Language Processing
InternationalConferenceonMachineLearning(ICML), (EMNLP),2019. 2
volume139,pp.8748–8763,2021. 2,4,6
Touvron,H.,Martin,L.,Stone,K.R.,Albert,P.,Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Rasley,J.,Rajbhandari,S.,Ruwase,O.,andHe,Y. Deep-
Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C.,
Speed: Systemoptimizationsenabletrainingdeeplearn-
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,
ing models with over 100 billion parameters. In Inter-
J.,Fu,W.,Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,
nationalConferenceonKnowledgeDiscoveryandData
Hartshorn,A.S.,Hosseini,S.,Hou,R.,Inan,H.,Kardas,
Mining(KDD),2020. 3
M.,Kerkez,V.,Khabsa,M.,Kloumann,I.M.,Korenev,
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, A.V.,Koura,P.S.,Lachaux,M.-A.,Lavril,T., Lee,J.,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Liskovich,D.,Lu,Y.,Mao,Y.,Martinet,X.,Mihaylov,T.,
Komatsuzaki,A. LAION-400M:OpendatasetofCLIP- Mishra,P.,Molybog,I.,Nie,Y.,Poulton,A.,Reizenstein,
filtered 400 million image-text pairs. arXiv preprint J.,Rungta,R.,Saladi,K.,Schelten,A.,Silva,R.,Smith,
arXiv:2111.02114,2021. 3,10,17 E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R.,
Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I.,
Schwenk,D.,Khandelwal,A.,Clark,C.,Marino,K.,and Zhang,Y.,Fan,A.,Kambadur,M.,Narang,S.,Rodriguez,
Mottaghi,R. A-OKVQA:Abenchmarkforvisualques- A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2:
tionansweringusingworldknowledge. arXivpreprint Open foundations and fine-tuned chat models. arXiv
arXiv:2206.01718,2022. 17 preprintarXiv:2307.09288,2023. 2,7,10
ShareGPT. ShareGPT,2023. 3,7,17 Wang,J.,Meng,L.,Weng,Z.,He,B.,Wu,Z.,andJiang,Y.-
G. Toseeistobelieve: PromptingGPT-4Vforbettervi-
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Con- sualinstructiontuning. arXivpreprintarXiv:2311.07574,
ceptualcaptions: Acleaned,hypernymed,imagealt-text 2023. 8
datasetforautomaticimagecaptioning. InAssociation
Wang, K.-J., Liu, Y.-H., Su, H.-T., Wang, J.-W., Wang,
forComputationalLinguistics(ACL),2018. 3,17
Y.-S., Hsu, W. H., and Chen, W.-C. OCID-Ref: A 3d
Sidorov,O.,Hu,R.,Rohrbach,M.,andSingh,A. TextCaps: roboticdatasetwithembodiedlanguageforclutterscene
a dataset for image captioning with reading compre- grounding. InAssociationforComputationalLinguistics
hension. InEuropeanConferenceonComputerVision (ACL),2021. 2,4,19
(ECCV),2020. 3,17
Wightman, R. Pytorch image models. https://github.
com/rwightman/pytorch-image-models,2019. 3,18
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,
Batra,D.,Parikh,D.,andRohrbach,M. TowardsVQA
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,
modelsthatcanread. InComputerVisionandPattern
C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,
Recognition(CVPR),2019. 2,3
M., and Brew, J. HuggingFace’s transformers: State-
of-the-art natural language processing. arXiv preprint
Steiner,A.,Kolesnikov,A.,Zhai,X.,Wightman,R.,Uszkor-
arXiv:1910.03771,2019. 3
eit,J.,andBeyer,L.HowtotrainyourViT?data,augmen-
tation,andregularizationinvisiontransformers. Trans- Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang,
actions of Machine Learning Research (TMLR), 2021. J., Hu, A., Shi, P., Shi, Y., Li, C., Xu, Y., Chen, H.,
6 Tian,J.,Qi,Q.,Zhang,J.,andHuang,F. mPLUG-Owl:
Modularization empowers large language models with
Subramanian, S., Narasimhan, M. G., Khangaonkar, K.,
multimodality. arXivpreprintarXiv:2304.14178,2023.
Yang, K., Nagrani, A., Schmid, C., Zeng, A., Darrell,
5,18
T., and Klein, D. Modular visual question answering
viacodegeneration. InAssociationforComputational Yu,L., Poirson,P.,Yang,S., Berg,A.C.,andBerg,T.L.
Linguistics(ACL),2023. 1 Modelingcontextinreferringexpressions. InEuropean
14InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
ConferenceonComputerVision(ECCV),2016. 2,4,17,
19
Yu,W.,Yang,Z.,Li,L.,Wang,J.,Lin,K.,Liu,Z.,Wang,
X., and Wang, L. MM-Vet: Evaluating large multi-
modalmodelsforintegratedcapabilities. arXivpreprint
arXiv:2308.02490,2023. 3
Zellers,R.,Holtzman,A.,Rashkin,H.,Bisk,Y.,Farhadi,
A.,Roesner,F.,andChoi,Y. Defendingagainstneural
fakenews.InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),pp.9054–9065,2019. 10
Zhai,X.,Mustafa,B.,Kolesnikov,A.,andBeyer,L. Sig-
moid loss for language image pre-training. In Interna-
tionalConferenceonComputerVision(ICCV),2023. 6
Zhao, Y., Gu, A., Varma, R., Luo, L., chin Huang, C.,
Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer,
S.,Desmaison,A.,Balioglu,C.,Nguyen,B.,Chauhan,
G.,Hao,Y.,andLi,S. PyTorchFSDP:Experienceson
scalingfullyshardeddataparallel. InVeryLargeData
Bases(VLDB),2023. 2,3,18
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,
H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-
judgewithMT-benchandchatbotarena. arXivpreprint
arXiv:2306.05686,2023. 7
15InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Figure10.AdditionalLM&ScalingResults.Wepresentadditionalresultssupplementingtheinvestigationsin§4.3and§4.4.First,find
thatatthe13Bparameterscale,baseLMsperformcomparablytoinstruct-tunedmodels(left).Next,westudyhowtrainingtimeimproves
modelperformance.Wefindthatevidenceofsevereunderfittingatasingleepoch,withperformanceimprovinguntilwehittwoepochs,at
whichpointperformanceplateaus(middle).Finally,westudytheimpactofscalingupdata;addingdataimprovesaggregateperformance
withtheLRV-Instructdataset(Liuetal.,2023a)havingamoresignificantimpactduetoitsincreasedimagediversity(right).
Figure11.QualitativeExamples–Instruct-Tunedvs.BaseLMs.WefindthatbaseLMs(e.g.,Llama-2)haveslightlybetterqualitative
performancecomparedtoinstruct-tunedLMs(e.g.,Vicun˜av1.5).Unsurprisingly,instruct-tunedLMssometimesgeneratemoreverbose
outputs.Thisverbositycanleadtohallucinations,suchasinthemonkeyexampleontheleftwheretheVicun˜av1.5modelincorrectly
indicatesthatthemanisalsoholdingaknife.WeadditionallyevaluatebothmodelsonanexamplefromtheBLIP-2(Lietal.,2022)paper
(right).WefindthatthebaseLlama-2modelgivesamoreaccurateresponse,suchascorrectlyidentifyingtheGreatWallofChina,going
furthertoprovideadditionalbackgroundinformation.
16InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
A.TrainingVisually-ConditionedLanguageModels
Inthefollowingsections,weprovideadditionaldetailaroundourVLMtrainingprocedure,includinganexpandeddiscussion
oftheLLaVav1.5pretrainingdatasetsweusethroughoutourwork,concreteimplementationdetailsforeachcomponentof
theoverarchingVLMarchitecture,andhyperparametersforVLMtraining. Allofthefollowinginformationisalsomade
explicitinourVLMtrainingcodebase.
A.1.PretrainingDatasetComposition
Asdescribedin§2,weusetheLLaVav1.5(Liuetal.,2023b)pretrainingdatasetsforthemajorityofourexperiments. The
datasetiscomprisedoftwouniquesubsets,witheachsubsetusedforthemulti-stagetrainingproceduredescribedin§4.1;
duringthefirststage(“vision-languagealignment”)onlytheprojectorF istrained, freezingtheweightsofthevisual
ψ
representationandLM.Duringthesecondstage(“multimodalinstructtuning”),bothF andtheLMaretrained.
ψ
Vision-LanguageAlignment.ThefirstsubsetconsistsofimagessourcedfromLAION(Schuhmannetal.,2021),Conceptual
Captions(CC;Sharmaetal.,2018),andSBUCaptions(SBU;Ordonezetal.,2011)augmentedwithsyntheticallygenerated
captionsfromBLIP(Lietal.,2022),anearlyVLMoptimizedforcaptioning. Asthegoalofthisfirststageoftrainingis
simplytoinitializetheprojectorF ,trainingissimple: givensolelytheimageasinput(e.g.,nolanguagepromptu ),
ψ prompt
tryandgeneratethecorrespondingcaption;toupdateF wepropagategradientsthroughtheLM(freezingtheweights). In
ψ
total,thisdatasetconsistsof558K(image,caption)pairs,whereacaptionisnolongerthanasentence.
MultimodalInstructTuning. Thesecondsubsetconsistsof665Kmultimodalinstructtuningexamples. Inordertoinduce
chat-like behavior and enable the VLM to perform specific tasks, Liu et al. (2023b) identify a set of “trigger prompts”
u foreachdatasetinthemixture;thesetriggerpromptstaketheformofaninstruction(e.g.,“Describetheimage.” or
prompt
“Providetheboundingboxcoordinatesfortheregionthissentencedescribes...”) withacorrespondingtargetgeneration. The
multimodalinstructtuningexamplesaresourcedasfollows:
LLaVaSyntheticData(158K).Asyntheticallygenerateddatasetofconversations,fine-graineddescriptions,andquestion-
answeringdatafromLiuetal.(2023c),sourcedbypromptingGPT-4(OpenAIetal.,2023)withimagecaptionsandobject
boundingboxesfromCOCO(Linetal.,2014). Becausethisdatasetwasexplicitlygeneratedfollowingthe“instruct”format
above,thereisnoneedtodefineaseparatetriggerprompt.
StandardVQAData(224K).AcombinationofvisualquestionansweringdatasourcedfromthetrainingsetsofVQAv2
(generalquestionanswering;Goyaletal.,2017),GQA(spatialandcompositionalreasoning;Hudson&Manning,2019),
OK-VQA(reasoningrequiringexternalknowledge;Marinoetal.,2019), andOCR-VQA(reasoningovertext/logosin
images;Mishraetal.,2019). ToencouragetheVLMtogenerateresponsesoftheappropriateformat,LLaVav1.5defines
thefollowingtriggerprompt: “⟨Question⟩? Answer the question using a single word or phrase.”
Multiple Choice VQA Data (50K). Multiple choice visual question answering data sourced from A-OKVQA (requires
diverseexternalknowledge;Schwenketal.,2022). Asthisisamultiplechoicetask,LLaVav1.5definesthefollowing
triggerprompt: “⟨Question⟩? A. ⟨Option A⟩ B. ⟨Option B⟩ ... Answer with the option’s letter from the
given choices directly.”
CaptioningData(22K).ImagesandcaptionssourcedfromTextCaps(imageswithtext/logos;Sidorovetal.,2020). LLaVa
v1.5definesthefollowingtriggerprompt: “Provide a one-sentence caption for the provided image.”
ReferringExpressionData(116K).Referringexpressiongrounding(boundingboxprediction)andregioncaptioningdata
sourcedfromRefCOCO(Kazemzadehetal.,2014;Yuetal.,2016)andVisualGenome(Krishnaetal.,2017). Forbounding
box prediction (localization), the model is tasked with producing normalized bounding box coordinates (as a natural
languagestring). Forthelocalizationtask,LLaVav1.5definesthefollowingtriggerprompt: “⟨Referring Expression⟩
Provide the bounding box coordinates of the region this sentence describes.” Fortheinversetask(region
caption), LLaVav1.5definesaseparatetriggerprompt: “Provide the bounding box coordinate of the region
this sentence describes.”
ShareGPT(Language-Only)(40K).Language-onlyco-trainingdatasourcedfromShareGPT(ShareGPT,2023),comprised
ofuser-uploadedconversationswithChatGPT.SimilartotheLLaVaSyntheticDatadescribedabove,thisdataisalreadyin
theexpected“instruct”format,withnoneedforaseparatetriggerprompt.
17InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
A.2.Implementation–ArchitectureComponents&Optimization
WeimplementourtrainingcodebaseinPyTorch,leveragingitsnativeFullyShardedDataParallel(FSDP;Zhaoetal.,2023)
implementationtodistributetrainingacrossGPUs. WetrainallmodelsinBF16mixedprecision. Inthefollowingsection
weprovideadditionaldetailsaroundeachoftheindividualcomponentsofaVLMasdescribedin§2.
ImageProcessing&VisualRepresentations. Weimplementallimageprocessinglogicusingthedefaultimagetransforms
providedbytorchvisionandPyTorchImageModels(TIMM;Wightman,2019). Inadditiontotheresizinglogicapplied
by the various schemes we evaluate in §4.2, we normalize pixel values using the defaults defined by each pretrained
backbone(oftenthetraditionalImageNetdefaults).
ThedefaultbackboneemployedbyallvisualrepresentationV thatweevaluateinthisworkisaVisionTransformer(ViT;
ω
Dosovitskiyetal.,2021);weextractpatchfeaturesfromthepenultimatelayeroftheViT,followingthepracticedefinedin
LLaVav1/v1.5(Liuetal.,2023b).
Vision-LanguageProjector. WhiletheprojectorF canbeofarbitrarycomplexity,weopttoinitializeasimple2-layer
ψ
GELUMLP(Hendrycks&Gimpel,2016)thatprojectseachpatchindependentlyintotheembeddingspaceoftheLM.
LanguageModels. Tocombineprojectedpatch“embeddings”outputfromF withthelanguagepromptembeddings
ψ
E (u )weperformsimplesequence-wiseconcatenation,insertingthepatchembeddingsonthe“left”oftheprompt
ϕ prompt
embeddings. ThisfollowstheprocessbymanypriorVLMs(Liuetal.,2023b;Yeetal.,2023;Gaoetal.,2023),andisakin
toprefixtuning(Li&Liang,2021),wherepatchembeddingstaketheplaceoftherandomlyinitializedprefixembeddings.
PromptingBasevs. Instruct-TunedLMs. Weusedifferentpromptingtoaccommodateinstruct-tunedLMs(e.g.,Vicun˜a
v1.5)andbaseLMs(e.g.,Llama-2). ForVicun˜av1.5,weusetheexpectedchatformat,consistingofasystempromptand
speciallyformatted“USER”and“ASSISTANT”blocks. WeusethesamesystempromptadoptedinLLaVav1.5–“Achat
betweenacurioususerandanartificialintelligenceassistant. Theassistantgiveshelpful,detailed,andpoliteanswerstothe
user’squestions.” Thetemplateforpromptformattingisthen:
<s>USER: {Input 1} ASSISTANT: {Response} <\s>
ForthebaseLMs(e.g.,Llama-2),weelidethesystempromptentirely. Weformatexamplesasfollows:
<s>In: {Input 1} Out: {Response} <\s>
A.3.TrainingHyperparameters
WeadoptthehyperparametersinTable1foralloursingle-stageexperiments(forboth7Band13B)models. Formulti-stage
pretraining(e.g.,justfortheexperimentsinFig.4)weincreasethebatchsizeto256andlearningrateto1e-3whentraining
theprojectorF forvision-languagealignment;wekeepallotherhyperparametersthesame.
ψ
Table1. TrainingHyperparameters
Hyperparameter Value
BatchSize 128
MaxGradientNorm 1.0
WeightDecay 0.1
LearningRate 2e-5
Optimizer AdamW
Scheduler Warmup&CosineDecay
WarmupRatio 0.03
18InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
B.EvaluationProtocol
We provide additional details around our evaluation procedures, including how we prompt VLMs for evaluation tasks,
howwecomputemetricsforeachevaluationtask,andfinally,providingfurtherdetailaroundhowwecomputestatistical
significancewhendrawingconclusions. Theseproceduresarealsomadeexplicitinourevaluationcodebase.
B.1.EvaluationProcedures
GeneratingResponses. Inordertorundeterministicevaluationsandfairlycomparedifferentmodels,wegenerateoutputs
viagreedydecoding;wenotethatthisensuresconsistency,butmayleadtoworsequalityoutputscomparedtousingother
LMgenerationstrategiessuchasnucleussamplingorbeamsearch.
PromptingVLMsforIndividualTasks. AsevidencedbyAppx.A,different“triggerprompts”inducemodelstoproduce
outputs of a specific structure (e.g., short phrases for visual question answering evaluations such as VQAv2). In our
comparisonsacrossmodels,wemakesuretousethetriggerpromptsdefinedbythepretrainingdatasets,orintheoriginal
works. Specifically,weusethetriggerpromptsinAppx.AwhenevaluatingourmodelsandLLaVav1.5,andthosedefined
inDaietal.(2023)forInstructBLIP.
ComputingEvaluationMetrics. Forallofouropen-endedvisualquestionansweringtasks(VQAv2,TextVQA,GQA,and
TextVQA),wereportaccuracyascomputedbytheofficialevaluationscripts. ForTextVQA,wealsorunavariantofthe
evaluationwhereVLMsareadditionallypromptedwithinputtokensparsedbyanOCR-system. Thesenumbersareonly
reportedattheendoftheappendices(Table2),andonlytomatchtheevaluationproceduresusedintheofficialLLaVa
v1/v1.5andInstructBLIPworks. TheTextVQAinthemainbodyofthepaperarerunonlyassumingaccesstotheimage
andquestion(withouttheOCRsysteminputs).
For our localization tasks, we report accuracy at the specific IoU thresholds defined in the official evaluations; for
RefCOCO/RefCOCO+/RefCOCOgthisis0.5IoU(Yuetal.,2016),whileforOCID-Refthisis0.25IoU(Wangetal.,2021).
Finally,forchallengetasks,weformateachexampleasamultiplechoicequestionandreportaccuracy;forVSRandPOPE
thismeanstwooptions(forTrue/FalseandYes/No,respectively),andforTallyQA,sixteen(thenumbers0-15,inclusive).
B.2.ComparingModelPerformance–SignificanceTesting
Asaddressedin§4,eachevaluationtaskusesdifferentmetrics,withdifferentrelativescales,makingdirectcomparison
challenging. WeaddressthisbycomputingnormalizedZ-scoresforeachmodelandevaluation(usingthemeanandstandard
deviationacrossallmodels),andcomputeglobalscoresbyaveragingacrossall12benchmarks. Todrawconclusionsaround
theimpactofagivendesignchoice,wedefinetwosetsofmodelsforcomparison. Thebasesetisreflectiveofthenull
hypothesiswiththedefaultconfiguration,whilethealternatesetisreflectiveofthenewdesignchoice. Foreachpairof
modelsacrossthebaseandalternatesets,wecomputethenormalizedperformancedifference,andperforma1-sidedFisher
T-testtocomputesignificance(p<0.01).
B.3.ExhaustiveResults
Forcompleteness,wetabulateevaluationresultsforallmodelstrainedinthiswork. Open-endedVQAresultsareinTable2,
LocalizationresultsareinTable3,andChallengeSetresultsareinTable4.
19InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Table2. AllResultsonVQABenchmarks
Model VQAv2 GQA VizWiz TextVQA+OCR TextVQA
OfficialModels
LLaVav1.57B 76.54 61.58 54.24 58.25 46.13
LLaVav1.513B 78.13 63.17 56.66 61.47 48.99
InstructBLIP7B 76.12 48.41 32.02 28.01 33.54
InstructBLIP13B 59.46 42.92 30.65 33.24 27.90
Reproduction&OptimizationProcedure
LLaVav1.57B(Reproduction) 76.80 62.28 51.26 57.91 46.44
LLaVav1.513B(Reproduction) 77.78 62.91 54.83 59.60 48.74
Single-Stage7B 77.09 62.57 54.33 56.87 44.45
Single-Stage13B 77.96 63.17 56.37 59.30 48.03
FrozenViT(Single-Stage) 77.09 62.57 54.33 56.87 44.45
FinetuneViT(Multi-Stage) 74.36 60.08 57.27 56.56 44.40
FinetuneViT(Single-Stage) 73.53 59.65 55.26 53.81 38.33
VisualRepresentation
IN1KViT-L224px 68.26 56.82 49.61 44.54 12.31
DINOv2ViT-L224px 66.29 55.64 48.37 44.70 12.62
CLIPViT-L224px 75.32 61.58 54.52 53.89 36.61
SigLIPViT-SO224px 76.32 62.15 58.82 55.75 40.50
ImagePreprocessing
CLIPViT-L336px(Letterbox) 77.09 62.57 54.33 56.87 44.45
CLIPViT-L336px(ResizeCrop) 77.07 62.29 58.15 58.06 48.83
CLIPViT-L336px(NaiveResize) 77.86 63.48 56.03 59.09 49.66
SigLIPViT-SO384px(Letterbox) 78.61 63.39 56.88 60.33 52.71
SigLIPViT-SO384px(ResizeCrop) 77.57 62.23 58.10 58.40 50.41
SigLIPViT-SO384px(NaiveResize) 78.81 63.60 57.47 61.06 54.87
VisualResolutionScaling
CLIPViT-L224px 75.32 61.58 54.52 53.89 36.61
SigLIPViT-SO224px 76.32 62.15 58.82 55.75 40.50
CLIPViT-L336px 77.09 62.57 54.33 56.87 44.45
SigLIPViT-SO384px 78.61 63.39 56.88 60.33 52.71
EnsemblingVisualFeatures
CLIP336px(NaiveResize) 77.86 63.48 56.03 59.09 49.66
DINOv2+CLIP336px(Letterbox) 75.66 62.89 53.88 46.28 15.16
DINOv2+CLIP336px(NaiveResize) 75.90 63.57 55.31 46.20 15.67
SigLIP384px(NaiveResize) 78.81 63.60 57.47 61.06 54.87
DINOv2+SigLIP384px(Letterbox) 78.66 63.81 59.00 58.77 50.11
DINOv2+SigLIP384px(NaiveResize) 79.18 64.33 61.06 60.31 52.18
Basevs.InstructTunedLMs
Vicun˜av1.57B 77.09 62.57 54.33 56.87 44.45
Vicun˜av1.513B 77.96 63.17 56.37 59.30 48.03
Llama-27B 77.08 62.44 55.98 55.24 44.92
Llama-213B 78.07 63.12 57.55 58.42 47.60
Co-trainingonLanguageSafetyData
Vicun˜av1.57B 77.09 62.57 54.33 56.87 44.45
Vicun˜av1.57B(NoCo-training) 77.08 62.90 44.81 57.59 44.55
Llama-27B 77.08 62.44 55.98 55.24 44.92
Llama-27B(NoCo-training) 77.10 62.94 43.60 56.04 45.45
ScalingTrainTime
1Epoch 77.09 62.57 54.33 56.87 44.45
1.25Epochs 77.30 62.70 57.28 57.22 45.44
1.5Epochs 77.54 62.75 56.37 56.42 45.63
2Epochs 77.79 63.50 55.20 56.12 46.08
3Epochs 77.17 62.96 56.20 54.01 45.69
ScalingData
Base 77.09 62.57 54.33 56.87 44.45
Base+LRV 77.58 63.13 55.76 57.23 45.67
Base+LVIS-4V 77.96 62.43 55.91 57.55 45.99
Base+LVIS-4V+LRV 78.33 63.60 56.01 59.06 46.86
Prism7B
Prism-CLIP7B(Controlled) 77.87 63.65 56.10 58.40 50.31
Prism-CLIP7B 79.67 64.56 53.34 57.72 51.12
Prism-SigLIP7B(Controlled) 79.12 63.98 58.99 60.11 55.79
Prism-SigLIP7B 80.67 64.32 53.70 62.14 58.01
Prism-DINOSigLIP7B(Controlled) 79.05 64.16 59.82 58.69 51.78
Prism-DINOSigLIP7B 80.97 65.27 52.82 59.71 55.64
Prism13B
Prism-CLIP13B(Controlled) 78.83 64.10 57.09 61.10 52.22
Prism-CLIP13B 80.38 65.07 56.47 61.56 53.40
Prism-SigLIP13B(Controlled) 78.52 63.24 57.29 58.50 50.61
Prism-SigLIP13B 80.68 64.56 57.63 60.09 54.28
Prism-DINOSigLIP13B(Controlled) 80.07 65.14 56.61 61.20 54.10
Prism-DINOSigLIP13B 81.66 66.13 58.01 62.89 57.08
20InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Table3. AllResultsonLocalizationBenchmarks
Model RefCOCO RefCOCO+ RefCOCOg OCIDRef
OfficialModels
LLaVav1.57B 55.12 49.47 50.92 35.07
LLaVav1.513B 66.75 61.36 60.85 45.56
InstructBLIP7B N/A N/A N/A N/A
InstructBLIP13B N/A N/A N/A N/A
Reproduction&OptimizationProcedure
LLaVav1.57B(Reproduction) 60.54 54.34 56.31 41.75
LLaVav1.513B(Reproduction) 64.79 59.32 59.33 44.48
Single-Stage7B 64.08 58.19 58.03 44.58
Single-Stage13B 68.98 63.59 61.64 44.89
FrozenViT(Single-Stage) 64.08 58.19 58.03 44.58
FinetuneViT(Multi-Stage) 19.24 17.48 23.12 16.35
FinetuneViT(Single-Stage) 42.56 37.89 41.05 33.42
VisualRepresentations
IN1KViT-L224px 43.24 35.40 36.05 19.58
DINOv2ViT-L224px 28.65 20.72 24.75 8.33
CLIPViT-L224px 59.88 53.69 53.37 37.16
SigLIPViT-SO224px 57.94 51.90 53.31 37.42
ImagePreprocessing
CLIPViT-L336px(Letterbox) 64.08 58.19 58.03 44.58
CLIPViT-L336px(ResizeCrop) 54.31 49.14 49.43 40.82
CLIPViT-L336px(NaiveResize) 65.28 58.79 59.93 44.20
SigLIPViT-SO384px(Letterbox) 63.09 56.24 58.17 45.50
SigLIPViT-SO384px(ResizeCrop) 53.29 47.63 50.18 39.27
SigLIPViT-SO384px(NaiveResize) 61.38 55.76 56.84 41.49
VisualResolutionScaling
CLIPViT-L224px 59.88 53.69 53.37 37.16
SigLIPViT-SO224px 57.94 51.90 53.31 37.42
CLIPViT-L336px 64.08 58.19 58.03 44.58
SigLIPViT-SO384px 63.09 56.24 58.17 45.50
EnsemblingVisualFeatures
CLIPViT-L336px(NaiveResize) 65.28 58.79 59.93 44.20
DINOv2+CLIP336px(Letterbox) 72.44 65.84 64.32 47.41
DINOv2+CLIP336px(NaiveResize) 71.07 64.77 65.26 47.66
SigLIPViT-SO384px(NaiveResize) 61.38 55.76 56.84 41.49
DINOv2+SigLIP384px(Letterbox) 72.10 65.42 64.69 50.37
DINOv2+SigLIP384px(NaiveResize) 73.86 67.29 67.85 52.82
Basevs.InstructTunedLMs
Vicun˜av1.57B 64.08 58.19 58.03 44.58
Vicun˜av1.513B 68.98 63.59 61.64 44.89
Llama-27B 65.24 59.47 58.78 43.89
Llama-213B 68.53 62.97 61.70 45.75
Co-trainingonLanguageSafetyData
Vicun˜av1.57B 64.08 58.19 58.03 44.58
Vicun˜av1.57B(NoCo-training) 63.94 57.51 57.88 44.11
Llama-27B 65.24 59.47 58.78 43.89
Llama-27B(NoCo-training) 64.26 59.30 57.99 42.17
ScalingTrainingTime
1Epoch 64.08 58.19 58.03 44.58
1.25Epochs 67.02 61.37 60.01 46.45
1.5Epochs 68.62 62.81 61.21 47.23
2Epochs 71.23 65.40 63.32 46.32
3Epochs 71.79 66.94 63.87 46.25
ScalingData
Base 64.08 58.19 58.03 44.58
Base+LRV 65.62 59.77 59.82 46.21
Base+LVIS-4V 63.91 58.82 58.91 43.83
Base+LVIS-4V+LRV 64.94 58.91 58.19 43.73
Prism7B
Prism-CLIP7B(Controlled) 66.42 60.14 60.56 44.12
Prism-CLIP7B 71.98 66.96 66.18 44.65
Prism-SigLIP7B(Controlled) 64.74 58.58 60.56 43.63
Prism-SigLIP7B 70.92 65.73 65.46 48.08
Prism-DINOSigLIP7B(Controlled) 73.62 67.85 66.34 50.56
Prism-DINOSigLIP7B 77.78 73.08 71.04 54.12
Prism13B
Prism-CLIP13B(Controlled) 70.92 65.95 65.03 47.32
Prism-CLIP13B 73.37 68.71 69.06 48.98
Prism-SigLIP13B(Controlled) 59.21 53.33 54.66 40.44
Prism-SigLIP13B 69.69 64.99 64.81 44.31
Prism-DINOSigLIP13B(Controlled) 76.64 71.41 70.87 53.60
Prism-DINOSigLIP13B 79.39 75.55 72.73 54.62
21InvestigatingtheDesignSpaceofVisually-ConditionedLanguageModels
Table4. AllResultsonChallengeBenchmarks
Model VSR POPE TallyQA
OfficialModels
LLaVav1.57B 51.47 86.57 62.06
LLaVav1.513B 69.07 87.10 64.83
InstructBLIP7B 58.92 84.30 15.51
InstructBLIP13B 63.91 84.49 49.73
Reproduction&OptimizationProcedure
LLaVav1.57B(Reproduction) 52.95 86.57 60.87
LLaVav1.513B(Reproduction) 65.38 86.94 64.13
Single-Stage7B 51.47 86.57 61.63
Single-Stage13B 70.05 87.00 63.26
FrozenViT(Single-Stage) 51.47 86.57 61.63
FinetuneViT(Multi-Stage) 57.20 82.70 59.15
FinetuneViT(Single-Stage) 51.47 83.82 59.53
VisualRepresentations
IN1KViT-L224px 51.47 82.08 52.95
DINOv2ViT-L224px 51.47 84.84 57.12
CLIPViT-L224px 51.47 85.80 59.09
SigLIPViT-SO224px 51.47 85.07 63.02
ImagePreprocessing
CLIPViT-L336px(Letterbox) 51.47 86.57 61.63
CLIPViT-L336px(ResizeCrop) 51.47 85.42 61.24
CLIPViT-L336px(NaiveResize) 51.47 87.01 62.90
SigLIPViT-SO384px(Letterbox) 51.47 86.78 64.83
SigLIPViT-SO384px(ResizeCrop) 51.47 84.62 62.94
SigLIPViT-SO384px(NaiveResize) 51.47 86.52 65.47
VisualResolutionScaling
CLIPViT-L224px 51.47 85.80 59.09
SigLIPViT-SO224px 51.47 85.07 63.02
CLIPViT-L336px 51.47 86.57 61.63
SigLIPViT-SO384px 51.47 86.78 64.83
EnsemblingVisualFeatures
CLIP336px(NaiveResize) 51.47 87.01 62.90
DINOv2+CLIP336px(Letterbox) 51.47 87.70 63.99
DINOv2+CLIP336px(NaiveResize) 51.47 87.29 65.02
SigLIP384px(NaiveResize) 51.47 86.52 65.47
DINOv2+SigLIP384px(Letterbox) 51.47 87.89 67.19
DINOv2+SigLIP384px(NaiveResize) 51.55 88.30 67.63
Basevs.InstructTunedLMs
Vicun˜av1.57B 51.47 86.57 61.63
Vicun˜av1.513B 70.05 87.00 63.26
Llama-27B 63.67 86.74 59.22
Llama-213B 65.71 86.91 62.54
Co-trainingonLanguageSafetyData
Vicun˜av1.57B 51.47 86.57 61.63
Vicun˜av1.57B(NoCo-training) 53.68 87.27 62.31
Llama-27B 63.67 86.74 59.22
Llama-27B(NoCo-training) 67.18 86.88 57.17
ScalingTrainingTime
1Epoch 51.47 86.57 61.63
1.25Epochs 51.80 86.80 61.69
1.5Epochs 51.55 87.78 61.67
2Epochs 53.93 87.03 62.52
3Epochs 54.99 86.86 62.30
ScalingData
Base 51.47 86.57 61.63
Base+LRV 64.08 86.84 65.54
Base+LVIS-4V 51.47 86.98 62.60
Base+LVIS-4V+LRV 54.91 87.27 63.74
Prism7B
Prism-CLIP7B(Controlled) 66.61 86.83 60.86
Prism-CLIP7B 57.77 87.30 66.00
Prism-SigLIP7B(Controlled) 65.14 87.07 64.54
Prism-SigLIP7B 56.79 87.30 66.46
Prism-DINOSigLIP7B(Controlled) 66.28 88.28 65.07
Prism-DINOSigLIP7B 59.57 88.12 66.70
Prism13B
Prism-CLIP13B(Controlled) 65.96 86.96 65.71
Prism-CLIP13B 71.85 87.23 69.37
Prism-SigLIP13B(Controlled) 62.85 86.82 62.90
Prism-SigLIP13B 64.57 87.50 68.95
Prism-DINOSigLIP13B(Controlled) 71.85 88.50 66.09
Prism-DINOSigLIP13B 72.18 88.07 70.41
22