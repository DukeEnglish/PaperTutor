Suppressing Pink Elephants with Direct Principle Feedback
LouisCastricato1,2,3,NathanLile1,SurajAnand3,HaileySchoelkopf2,
SiddharthVerma4,andStellaBiderman2
1synthlabs.ai2EleutherAI3BrownUniversity4character.ai
Correspondancetolouis@synthlabs.ai
Abstract knownasthe‚ÄúPinkElephanteffect‚Äùinpsychology
(Spiers,2002).
Existingmethodsforcontrollinglanguagemod-
In this paper we study controllable generation
els, such as RLHF and Constitutional AI, in-
volve determining which LLM behaviors are through the lens of the Pink Elephant Problem:
desirable and training them into a language thechallengeofinstructingamodeltonotdiscuss
model. However,inmanycases,itisdesirable anundesired‚ÄúPinkElephant‚Äùentityortopicandin-
forLLMstobecontrollableatinferencetime, steaddiscussanalternativedesired‚ÄúGreyElephant‚Äù
so that they can be used in multiple contexts
asdepictedinFigure1.
withdiverseneeds. Weillustratethiswiththe
PinkElephantProblem: instructinganLLM
toavoiddiscussingacertainentity(a‚ÄúPinkEle-
phant‚Äù),andinsteaddiscussapreferredentity
(‚ÄúGrey Elephant‚Äù). We apply a novel simpli-
fication of Constitutional AI, Direct Princi-
pleFeedback,whichskipstherankingofre-
sponsesandusesDPOdirectlyoncritiquesand
revisions. OurresultsshowthatafterDPFfine-
tuningonoursyntheticPinkElephantsdataset,
our 13B fine-tuned LLaMA 2 model signifi-
cantly outperforms Llama-2-13B-Chat and a
prompted baseline, and performs as well as
GPT-4inonourcuratedtestsetassessingthe
PinkElephantProblem.
1 Introduction
Inrecentyears,general-purposegenerativemodels
suchasLLMsobtainedviatrainingonmassiveun-
labeledcorporahaveachievedgreatsuccess. Train-
ing desirable behaviors into LLMs can improve
theirusefulness; however,controllabilityatinfer-
Figure1: ThischatbotisdesignedtohelpBritishstu-
encetimeremainsdifficult. Currentlanguagemod-
dentsapplytoBritishuniversities,butdoesn‚Äôthaveup-
elsstilloftenfailtoperforminstructionfollowing
to-dateinformationaboutotheruniversities. ‚ÄúAmerican
and reasoning tasks reliably, especially tasks re- Universities‚ÄùisthePinkElephantwhile‚ÄúBritishUni-
quiringcompositionalreasoning,complexandnon- versities‚ÄùistheGreyElephant.
standardinstructionformats(Websonetal.,2023;
Shi et al., 2023a), and logical operations such as To address this challenge, we leverage a novel
negation(McKenzieetal.,2023). Oneparticularly formofReinforcementLearningfromAIFeedback
challenging behavior to teach a language model (RLAIF) (Bai et al., 2022b) that we term Direct
tofollowistoavoidmentioningsometopicwhen PrincipleFeedback. Recentresearchhasshown
instructedtodoso. Paradoxically,mentioningthat thattechniqueslikeReinforcementLearningfrom
topicintheLLM‚Äôspromptmakesthemodelmore AIFeedback(RLAIF)(Baietal.,2022b)cannot
likely to mention it even when instructed not to, onlyimproveamodel‚Äôsabilitytoremainharmless
bearing a resemblance to the what is commonly and helpful (Tunstall et al., 2023a; Ivison et al.,
4202
beF
31
]LC.sc[
2v69870.2042:viXra2023)butcanimproveamodel‚Äôsabilitytoreason anddesiredoutcomesbasedonnoveldeployment
(Shao et al., 2024; Luo et al., 2023a; Lightman scenariosorgoals.
etal.,2023)andreduceamodel‚Äôstendencytohal- Instead, we wish to apply fine-tuning to the
lucinate(Tianetal.,2023). Howeverresultshave modelsothatitgainsthecapabilityofbeingcon-
shown that even with finetuning and prompting, trolledwithoutdirectlyhard-codingourvaluesinto
making a model ‚Äúnot‚Äù discuss a topic remains a themodel. Thishasthebenefitofallowingmore
difficultandopenproblem(McKenzieetal.,2023; tailoredpreferencestobeexpressedbasedonthe
Garc√≠a-Ferreroetal.,2023). deploymentscenario,andallowsmorestakeholders
We find that by applying DPF with high qual- acrosstheAIvaluechaintoparticipateindecision-
itysyntheticdatawecanteachthemodelthetask makingandinclusionoftheirpreferences. Italso
‚Äúdon‚ÄôttalkaboutX‚ÄùwhereXisdeterminedatinfer- provides concrete benefits for the Pink Elephant
encetime. Thisenables Problem: it enables a single high-quality model
tobedeployedindiversecontextswheredifferent
1.1 OurContributions
conceptsortermsshouldbeavoided.
Ourprimarycontributionsare: FormoreclassicalNLPtasks,theshiftfromfine-
tunedtask-specificmodelstosingle,unifiedmodels
1. We present the ‚ÄúPink Elephant Problem‚Äù, a
capable of following arbitrary instructions (Sanh
failuremodeofcurrentlanguagemodels.
etal.,2021;Longpreetal.,2023)hasbroughton
tremendousperformanceimprovementsandvastly
2. We introduce a novel approach for RLAIF,
easierdeploymentduetoasinglemodelbeingcon-
which we term Direct Principle Feedback,
trollablebyasimplenaturallanguageinstruction
whichenablestheuseofCritiquesandRevi-
at inference time. Akin to instruction tuning, the
sionsasnaturallanguageAIFeedback,while
PinkElephantProblempresentsauniquesetofcir-
simplifyingtheRLAIFpipelinesignificantly.
cumstancesthatdrawsanalogiestometa-learning
3. We demonstrate an approach to teach mod- (Iyeretal.,2023).
elstodynamicallyobeynewbehavioralcon- Althoughitwouldbepossibletotrainamodel
straintsatinferencetime,viatrainingourmod- specificallytoavoidmentioningasinglepredeter-
elstoavoidanyPinkElephantentityspecified minedentity,foreverydifferentdeploymentofthe
atinferencetime. modelordifferentPinkElephant,aseparatemodel
would have to be trained, repeating this process.
4. Weshareourapproachesforgeneratingsyn- Generatingaseparatefine-tuningdatasetforevery
theticpreferencedata,aswellasourmodels suchdesiredentitywouldbecost-prohibitive.
andcodebase,inthehopestheywillbehelpful
2.1 SimplifyingRLAIFwithDirectPrinciple
tothecommunity.
Feedback
Additionally,weshareourapproachesforgenerat-
Reinforcement Learning from AI Feedback, as
ingsyntheticpreferencedata,aswellasourmodels
originally presented in Bai et al. (2022b), uses a
andcodebase,inthehopestheywillbehelpfulto
four-stepprocessdepictedinfig.2.
thecommunity.
1. Finetuneamodelonexamplesofhelpfulre-
2 Inference-TimeControllabilityvia questsandoutputs(blue).
DirectPrincipleFeedback
2. Critiqueandrevisethoseoutputstobemore
Existing methods for preference learning fine- desirable,andfine-tuneanewmodelonthose
tuninggenerallyconsistofselectingasetofrules outputs(orange).
orgoodbehavior,knownbeforehandtothedevel-
3. UseyourSupervisedFine-tuning(SFT)model
oper (Bai et al., 2022b), and training a model to
to generate responses to a prompt and have
adhere to preference pairs expressing this behav-
a human or AI system rank those responses
ior. While this is useful from the perspective of
(green).
adeveloperwhohasbehaviortheywanttoassure
in their deployed model, it prevents downstream 4. Feedtherankedresponsesintoanpreference
developers or users of widely distributed models learning algorithm such as PPO or DPO to
frommakingtheirownchoicesaboutmoderation producethefinalmodel(purple).Constitutional AI Bai et al. (2022b), and have further control over
howourpairedpreferencedatapointsdifferwithin
Helpful LLM Response pairs (due to our controllable Revision step), as
comparedtootherRLAIFmethods,whichsimplify
Critique thepipelinebysimplytakingarbitrarymodelout-
putstoapromptandscoringorrankingthemfor
Revision
Gen 1 goodness.
SFT Ranker Concurrently,Huangetal.(2024)replicatethe
ConstitutionalAIpipelineandperformDPOusing
Gen 2
thepre-andpost-revisionpairsbothforSFTand
DPO Aligned for DPO. However, they focus on replicating the
processofBaietal.(2022b),whereaswelookat
Without critique
usingDPFforanovelcontrollabilitysetting.
Gen 1
2.2 ApplyingDPFtothePinkElephant
Helpful LLM Ranker
Problem
Gen 2
Becausehighqualitypairwisepreferencesarein-
DPO Aligned herentlydifficulttogenerateforthePinkElephant
Problem, a natural choice is to turn to revisions
Direct Principle Feedback togenerateahighqualitydatasetwiththedesired
behaviors. Assuch,ThePinkElephantProblemis
Helpful LLM Response aprimecandidateforDPFasopposedtoaranking-
based approach, which would have been much
Critique more difficult to control for specific kinds of nu-
anceddifferentiationsbetweendialoguescontain-
Revision DPO Aligned
ingthePinkElephantasopposedtothosecontain-
ingthedesiredGreyElephant.
Figure2: Ahigh-levelillustrationofConstitutionalAI
(Baietal.,2022b),RLAIFwithoutcritiques,andDPF
3 DatasetGeneration
(ours). DPFstreamlinestheprocesstouseonlyasingle
stepofAIfeedback,andusesrevisionsaccordingtoa Wecuratedadatasetof162Kmulti-turnconversa-
principleasfeedback. Notethatthechoiceofobjective
tions on the Pink Elephant Problem. The conver-
suchasDPOisarbitraryandmaybesubstituted.
sationscover29diversedomainsincludingsports,
health,business,andpolitics. Thedatasettookap-
Previous work has sought to simplify this
proximately 2,000 A100 hours to produce, with
pipeline by excluding the Critique and Revision
roughly20,000-30,000forprototyping.
step and doing pairwise ranking of generations
from the initial model directly (Tunstall et al., 3.1 TopicsGeneration
2023a; Zhu et al., 2023). This has the advantage
ofrequiringonlyasinglepreferencecollectionand
üßë ü§ñ ü§ñ
fine-tuningstep,andiseffectiveforimprovingchat Handwritten GPT-4 SB2-70B
Entertainment Rugby Football Contact sports
qualityorteachingmodelstobehaveasassistants. Sports Sledding Skiing Favorite Food
Thesimplifiedpipelinedoesnotallowforteaching
Tourism Rugby Football Reco Sm pm ore tsnding
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶
behaviorsonmorecomplexprinciples,however,as Seed Topics Pink Elephant Conversation
Pairs Subtopics
donebyBaietal.(2022b).
Figure3: Initialdatasetgenerationstep,asdescribedin
We introduce Direct Principle Feedback
section3.1andsection3.2. Ashortlistoftopicareasis
(DPF),anovelpipelineforRLAIFthatleverages
handwritten,GPT-4ispromptedtogeneratemanycon-
thefactthatthedatapointsbeforeandafterthere-
trastingPinkElephantentitypairs,andStableBeluga2
visionisapplied provideanaturalsourceofpaired
subsequentlypromptedtocreateanumberofplausible
dataforpreference-basedfine-tuning. Byfeeding subtopicsforeachentitypair. Duplicatesareremoved
the (response, revision) pairs into the preference ateachstep.
learning algorithm, we are able to instill a more
complex behavior akin to the principles used in ThefirststepingeneratingahighlydiversesetofPinkElephantpairswasfirsttogenerateahighly 3.3 UnwantedBehaviorGeneration
diverse set of topics that our chatbot would dis-
Theinitialphaseincreatingpairedpreferencedata,
cuss. SimilartoLuoetal.(2023b);Bradleyetal.
which aims to embed an optimal policy instruct-
(2023,2024);Lehmanetal.(2022)webelievethat
ing a chatbot to refrain from discussing the Pink
to produce high quality synthetic data, diversity
Elephant,entailsthegenerationofinappropriatebe-
needstobeamajorconsiderationinthedataset‚Äôs
haviors. Thisincludesdialogueswherethechatbot
construction.
engagesinconversationsaboutthePinkElephant
WeusedGPT-4(Achiametal.,2023)togener- wheninstructednottodoso.
ate a list of 200 diverse possible topics that daily
conversationsareusuallybasedoffof(promptin ü§ñ Us se pr o w rt a tn ot ws aa t cn hew ü§ñ U nes wer : H si p! oI‚Äô rm t lo to ok ing w afo tcr ha .
SB2-70B SB2-70B Chatbot: Interesting! What
appendix D). Then, we manually filtered the top- Rugby Football ‚Ä¶ s ap lro er at ds y ?do you like
ics to ensure that they are in fact common daily + User likes User: I like Football a lot.
Contact sports Football‚Ä¶
conversation topics and topics about which one En St uit by
t
oP pa ii cr + ‚ùårecC o Rh m ua gmtb beo ynt ds ‚Ä¶ C Rh ua gt bb yo
,
t:
w
Y ho icu
h
m isa y
a
le sn oj o ay
couldsensiblyandresponsiblydeployachatbotin Dialogue contact sport!
Plan Dialogue
production. Wetookthelibertytofilterirrelevant
Figure4: Generationofsection3.3dialogueswhichex-
topics,suchas‚ÄúThelegalsystemofextraterrestrial
hibitundesiredbehavior(mentioningthePinkElephant
life.‚Äù
inthefinalturn). Weperformanintermediateplanning
The filtered version of this initial list can be step.
foundintheappendixE.Theycoverawiderange
oftopics,suchastraveling,career,sports,health, Our aim in producing unwanted behavior is to
festivals,religions,andsoon. fosterawidearrayofdialogueswhereinthechat-
boteventuallyreferencesaPinkElephant. Thisre-
quirestheinitialcreationofdiverseconversational
3.2 PinkElephantPairs
themes, designated as Attributes, that logically
leadtothementionofaPinkElephantbythechat-
The next step was to generate a large number of
bot.
PinkElephantPairs(PEPs),approximately2500,
To foster a variety of authentic dialogues that
using GPT-4. Our goal is to generate similar yet
incorporate our PEPs, we commence by creating
contrastivepairsbecausewewantagenerateddi-
50 attributes aimed at facilitating transitions be-
alogue about a Grey Elephant topic to naturally
tweencontrastingPEPs,therebyinitiatingdiscus-
culminateinthementionofaPinkElephant.
sionsthatprogressfromtheGreyElephanttothe
To generate these pairs we used the prompt
PinkElephantwiththeaidofStableBeluga2-70B.
Generate a list of 100 (x, y) pairs
Thesedialoguesserveasinstancesofthebehaviors
that represent [TOPIC] and their top
we seek to avoid, known as Pink Elephant Prob-
alternatives/competitors. Wethenmanually
lemfailures. Ourqualitativeanalysisrevealedthat
check and filter the PEPs to ensure that they fit
incorporating this strategic planning step signifi-
ourdefinitionofPinkElephants. Specifically,we
cantly enhances the naturalness of the dialogues,
verified that they were truly alternatives, yet had
with examples of these plans available in the ap-
specific differentiated qualities that a non-expert
pendix.
couldidentify. Forexample,thepair‚ÄúNike-Adi-
However,despiteemployingtheseattributesand
das‚Äù,twocompetingcompanies,wasgeneratedfor
PEPs,StableBeluga2-70Bwasunabletoproduce
thetopicofsports,andthepair‚ÄúTajMahal-Ellora
conversations that were both fluent and realistic,
Caves‚Äùwasgeneratedasanexampleoftwodiffer-
meeting our predefined standards. Consequently,
ing tourist destinations in India. Examples from
wehaveimplementedaDialoguePlanningphase,
thislistcanbefoundinappendixF.
instructingourmodeltofirstoutlineaconversation
Inthesubsequentphasesofourdatasetcreation betweenauserandthechatbotthattransitionsfrom
process,weprimarilyemployStableBeluga2-70B discussing the Grey Elephant to mentioning the
(Mahanetal.,2023)inconjunctionwiththe"Best- PinkElephant.
of-N"strategy(Stiennonetal.,2022),whereNis Subsequently,weengageStableBeluga2-70Bto
set to 2. The selection criterion is based on the executeaconversationbasedonthedialogueplan
perplexityofthegeneratedoutput. andPEPs,facilitatingaconversationbetweentheuser and chatbot that aligns with the planned se-
Chatbot mentions
quence. Theoriginalplanisthendiscarded,with Rugby before final
turn‚Ä¶
onlythefinaldialoguebeingretained. Wediscov-
e p ar nhe dad s net ah ta m ut rat ah r lke ne ei dn sl st yr ,o tid hmu erc p et ri boo yvn e io nsf ct ra h ep e ar se d inl ii a gm lo ti hn g ea ur e py s ro‚Äôp bl qa aun ban il li i in t tg y y U w ‚Ä¶ As giU w ‚Ä¶ Ate ehs giU w ‚Ä¶ Ar tn‚Ä¶e eh: ts g ir n‚Ä¶: t : e e I h t S :r n ‚Ä¶ I :n u S t :e r n u Iee S e r,d n e u e ‚Ä¶ ,e rd e‚Ä¶h e ,de h ‚Ä¶l ep hl p e lp C mC m pr hh ee e aa nn - tt r tt bb e ii oo oo v nn i tt s s i od ni R ‚Ä¶d R u ug gsn b bto iy ylt l U w ‚Ä¶ As giU w ‚Ä¶ Ate ehs giU w ‚Ä¶ Ar tn‚Ä¶e eh: ts g ir n‚Ä¶: t : e e I h t S :r n ‚Ä¶ I :n u S t :e r n u Iee S e r,d n e u e ‚Ä¶ ,e rd e‚Ä¶h e ,de h ‚Ä¶l ep hl p e lp
Dialogues post-revision‚Ä¶ Filtered
ofsuccessfullyincorporatingamentionofthePink (Paired) Dataset
üíª
Elephant as designed. This innovative planning Cosine sim. ,
Levenshtein dist.
step distinguishes our method from conventional
Figure6: Datafilteringstep,asdescribedinsection3.5.
practicesinRLAIFdatageneration.
We use a number of distance metrics or heuristics to
Weincludeexamplesoftheprompts,dialogues, identify dialogue pairs which wrongly mention the
andanexampledialogueplaninappendixG. PinkElephant(‚ÄòRugby‚Äô)outsidethefinalturnorpost-
revision.
3.4 RevisedBehaviorGeneration
3.5 DataCleaning
Theinitialstageofdataprocessinginvolvedtrun-
U nes wer : H si p! oI‚Äô rm t lo to ok ing w afo tcr ha . ü§ñ SB2-70B ü§ñ SB2-70B U nes wer : H si p! oI‚Äô rm t lo to ok ing w afo tcr ha . catingdialoguesattheinitialinstanceof‚ÄôPinkEle-
C s ap lh ro ea r at tb dso y t ?: I dn ote res yti on ug ! W lh ika et Generate critique The Chatbot Revise tu d ri nalogue C s ap lh ro ea r at tb dso y t ?: I dn ote res yti on ug ! W lh ika et phant‚Äômentionbythechatbot. Subsequentfiltering
User: I like Football a lot. Rr ue gc bo ym , m bue tn id t s is User: I like Football a lot. aimedtoenhancedialoguequalityandadherence
not supposed to.
‚Ä¶ ‚Ä¶
Chatbot: You may enjoy Critique Chatbot: You should keep to predefined criteria. This process entailed the
Rugby, which is also a watching more Football,
contact sport! or try Soccer! exclusionofdialogueswhere:
Dialogue Revised
Dialogue
1. ThechatbotreferencesthePinkElephantprior
Figure 5: Critique and Revision step, as described in to the final utterance, indicating a lapse in
section3.4. Weaskourmodeltorewritethefinaldia-
initialdatacleaning.
logueturntoremovementionsofthePinkElephant.
2. The Pink Elephant is not mentioned in the
finalutterancepre-revision.
Our next step is to generate examples of good
3. The Pink Elephant is mentioned in the final
behavior (i.e., the chatbot successfully avoiding
utterancepost-revision.
discussingthePinkElephant)whichwecanutilize
These exclusions were facilitated using Leven-
forperformingDPOorDPF.
shteindistance,Hammingdistance,andcosinesim-
WeapplyCritiquesandRevisionstoamendour
ilarity measures applied to embeddings. For em-
negativeexamples.
beddings,weusedDistilBERT(Sanhetal.,2019)
First,wepromptStableBeluga2-70Btogenerate
duetoitsnotablespeed,allowingfortheefficient
acritiqueofthechatbot‚Äôsfinalresponseinthedi-
evaluationofdifferentfilteringthresholds. Wees-
alogue, which frequently mentions the undesired
tablished a cosine similarity threshold of 0.8 be-
PinkElephant. Next,weaskthemodeltogenerate
tweenthefinalutteranceembeddingandthePink
arevision: themodelrewritesthisfinalresponse,
Elephantreferenceasourfilteringcriterion. This
removingthementionofthePinkElephantandin-
thresholdwaschosenbecauseitqualitativelyraised
steadredirectingtheconversationbacktotheGrey
thequalityofourdata,strikingabalancebyfilter-
Elephant.
ingoutinsufficientexampleswithoutexcessively
Wefoundthatincludingtheplanincontextfor diminishingthedataset.
therevisionbiasedtherevisionandcritiqueprocess The dataset was then partitioned into training,
tonotactuallyremovethePinkElephant. Assuch, validation, and test sets in a 96%-2%-2% ratio
duringthecritiqueandrevisionprocess,weremove among ‚ÄôPink Elephant Pairs.‚Äô We ensured non-
theplan. overlapping‚ÄôPinkElephantPairs‚Äôacrossthesesub-
Following this step, we now have paired data sets.
pointswheretheoriginalconversationcontainsun-
4 Experiments
desiredbehavior(mentioningthePinkElephant),
andthepost-revisiondatapointcontainsbehavior Here we describe the use of DPF and our gener-
moreinlinewithourbehavioralgoal. ateddatasettosuppressmentionsofPinkElephantentities. Wetriedtoincludeacomparisonbetweenmod-
elstrainedwithDPOandonestrainedwithILQL
4.1 Training (Snell et al., 2022), but were unable to find the
Becauseourdatasetcontainsexamplesofconversa- righthyperparametersforittoconverge;themodel
tionswherethePinkElephantismentionedbythe wouldeitherincorrectlymentionthePinkElephant
userandthechatbotmuststeerawayfromthecon- or produce incoherent language. We conjecture
versation(section3.3, section3.4), ourdatasetis thatILQLwouldrequireadditionalrewardshaping
notdesirableforcloningbehaviorviaSFT.There- to achieve a performant model, and so focus on
fore,wechosetosolelyperformDPOfine-tuning DPO.
withoutanextraSFTstep, viainitializingfroma We additionally attempted prompting the base
modelthathasalreadyundergoneSFTtraining. and DPF-trained OpenHermes models using
Classifier-Free Guidance (CFG) (Sanchez et al.,
WechoosetheOpenHermes7Band13Bmod-
els1 for two reasons: first, they provide a strong 2023; Shi et al., 2023b) with a guidance scale of
1.5forenhancedsensitivitytothesystemprompt.
instruction-tunedmodelasbase. Second,theterms
However,wesawthatCFGhadminimalimpacton
ofLlama2‚Äôslicenseprohibitedusfromusingour
thescoringofourmodels‚Äôgenerations,andsodid
datasetgeneratedbyaLlama2derivativetotrain
notcontinueusingthisasabaseline. Fullresults
anymodelsnotfine-tunedfromLlama2,andthe
withCFGcanbefoundinappendixI.
OpenHermesmodelsarefine-tunedfromLlama-2
asbasemodels.
4.3 QuantitativeEvaluation
We use the codebase from Tunstall et al.
(2023a,b)forourDPOtrainingruns. Wetrainour To perform evaluation at scale on whether the
modelsusingbfloat16precision, aglobalbatch model mentions the Pink Elephant in its genera-
sizeof64,andtheRMSPropoptimizerfollowing tion,weemployGPT-4incombinationwithhuman
Rafailov et al. (2023), and using Flash Attention annotations on a subset of examples for labeling
(Daoetal.,2022;Dao,2023). Becausewewished agreement. Hereisthepromptformatthatweuse
to obtain a model that otherwise performed simi- GPT-4forevaluationofPinkElephants.
larlytoourbaseline,butwiththeaddedabilityto
GPT-4EvaluatorPrompt
avoidPinkElephantswhenspecifiedinitssystem
prompt, we used a relatively large Œ≤ = 0.5 for
A dialogue between a user (USER) and
DPO.Ourfulltrainingconfigurationsareavailable
a chatbot (AGENT) which has been in-
inourpubliccodebase2. SimilarlytoZephyr(Tun-
structed to avoid mentioning the topic of
stalletal.,2023a),wechosetotrainfor3epochs,
<pink_elephant>. Yourjobistodetermine
butchosetouseonly1epochforourfinalmodels
if the AGENT has mentioned this topic.
(appendixC).
DoesAGENTtextdiscussorcontainanydi-
rectorindirectmentionof<pink_elephant
4.2 Baselines
||grey_elephant>?
Wecompareourfine-tuningmethodtoseveraldif- USER:<user_question>
ferentbaselines: AGENT:<agent_response>
.
.
.
1. ThebaseOpenHermesmodels,promptedwith
oursystemprompttoavoidthePinkElephant.
Given this setup, we evaluate on a held out
testsetconsistingof2%oftheoriginalgenerated
2. Llama-2-chat (Touvron et al., 2023) with a
dataset. Specifically, given an example conversa-
systempromptinstructingittoavoidthePink
tion in the test set, we regenerate the response to
Elephant.
thefinaluserquestion(i.e. thefinalturn). Thisisa
3. GPT-4withasystempromptinstructingitto setofuser-chatbotconversationsmeanttomorefre-
avoidthePinkElephant. quentlythanchanceelicitaresponsethatincludes
the Pink Elephant. For some data points this is
1https://huggingface.co/teknium/OpenHermes-7B and anadversarialsetting,wheretheuserasksdirectly
https://huggingface.co/teknium/OpenHermes-13B
aboutthePinkElephant,whileforotherdatapoints,
2https://github.com/EleutherAI/alignment-
handbook/tree/pink-elephants-dpo bringingupthePinkElephantmaybeunnaturalorModel BaseRate‚Üì WithPrompt‚Üì ‚àÜ ‚Üë
OpenHermes-7B 0.33¬±0.010 0.36¬±0.010 ‚àí0.03¬±0.013
w/DPF 0.34¬±0.010 0.17¬±0.008 0.17¬±0.012
OpenHermes-13B 0.34¬±0.010 0.34¬±0.010 0.00¬±0.013
w/DPF 0.34¬±0.010 0.15¬±0.010 0.19¬±0.012
Llama-2-13B-Chat 0.33¬±0.009 0.25¬±0.009 0.08¬±0.013
GPT-4 0.33¬±0.009 0.13¬±0.009 0.20¬±0.013
Table 1: Rate at which the model talks about the Pink Elephant (lower is better). Base Rate ‚Üì is without any
instructionregardingthePinkElephantandrepresentsthemodel‚Äôsnaturaltendencybasedonitstrainingandthe
dialog. WithPrompt‚ÜìistheratewhenspecificallyinstructedtoavoidthePinkElephant. Whileinitiallythe
OpenHermesmodelsbringupthePinkElephantasoftenormoreoftenwhentoldtoavoidit,ourmethodology
decreasestheincidenceofthePinkElephantwhenpromptedtoavoidittoGPT-4-levelperformance.
easilyavoided. Identicaltotraining,themodelis icantlymodifiesthefrequencyofsuccessfulPink
instructedinitssystempromptthatitisnotallowed Elephantavoidancesinthetestset. Notably,base-
to bring up the Pink Elephant in its answers, but lineOpenHermesmodelsexhibitanequivalentor
insteadshouldrespondwithsomethingrelatedto increased tendencytomentionPinkElephantsfol-
theGreyElephant. lowing such directive prompts. In contrast our
Foreachmodel,wereportthefollowingmetrics DPF‚Äôdmodelsadheretothesedirectivesfarmore
onthetestset: effectively,surpassingLlama-2chatincompliance
andachievingperformanceonparwithGPT-4in
‚Ä¢ BaseRate: proportionofexampleswherethe
thissetting.
PinkElephantismentioned,whenthesystem
promptdoesnotmentionthePinkElephant.
To confirm the validity of GPT-4 as our evalu-
‚Ä¢ WithPrompt: proportionofexampleswhere ator at scale, we had two authors assess labeling
the model successfully refrained from men- agreementon200generatedexamples: 50foreach
tioning the Pink Elephant when specifically of OpenHermes-13B, OpenHermes-7B w/ DPF,
instructedtoavoidthePinkElephant. OpenHermes-13Bw/DPF,andGPT-4. Judgements
wereassessedusingonlytheinformationgivento
‚Ä¢ BaseRate-WithPrompt: proportionoftest
GPT-4,andthusblindto(1)whichmodelgenerated
setexamplesforwhichtheinclusionofasys-
theresponse,(2)GPT-4‚ÄôslabelforPinkElephant
tempromptdisallowingthePinkElephantsuc-
mention, and (3) the other annotator‚Äôs label. If
cessfully alters behavior on the model. Our
disagreement was not satisfactorily minimal, fur-
most important metric, this shows the im-
therpromptengineering,ornotusingGPT-4asan
provedeffectivenessofouralgorithmduring
evaluator would have been required. The agree-
inferencetime.
mentbetweenGPT-4andAnnotator1was98.5%,
GPT-4andAnnotator2was94.5%,andAnnotator
Our analysis revealed a consistent Base Rate
1andAnnotator2was95.5%. Therefore,GPT-4is
across models, which aligns with expectations
effectiveforgradingresponsesinthissetup.
giventhatthebaserateisdeterminedbythepreva-
lence of naturally occurring responses with Pink
Elephantsinthetestset. Wealsoseethatourfine- Finally,weadditionallyevaluateon4OpenLLM
tuningdoesnotnegativelyimpactperformancein Leaderboard(Beechingetal.,2023)tasks: MMLU
scenarioswithoutapromptthatexplicitlyforbids (Hendrycks et al., 2021), TruthfulQA (Lin et al.,
PinkElephant. 2022),HellaSwag(Zellersetal.,2019),and(Clark
ThedifferenceinPinkElephantmentionratere- etal.,2018)usingv0.4.1oftheLMEvaluationHar-
vealssubstantialdisparitiesinOpenHermesmodel ness(Gaoetal.,2023)andMT-Bench(Zhengetal.,
performanceswithandwithoutDPF.Thismetric 2023)toillustratequantitativelythatourmodelre-
demonstrateshowtheintroductionofapromptex- tainsitspreviousperformanceafterfine-tuningon
plicitlyprohibitingPinkElephantreferencessignif- thePinkElephantsdataset.6 EthicalConsiderations
Model MT-Bench Leaderboard
OpenHermes-7B 5.19 57.40 While RLAIF offers promising solutions to the
w/DPF(Ours) 5.28 58.12 ‚ÄúPinkElephantproblem‚ÄùinAI,italsoraisesethical
OpenHermes-13B 6.28 61.36 considerations. The reliance on AI for feedback
w/DPF(Ours) 6.09 61.75 loops necessitates a careful design to ensure that
theAI‚Äôsownbiasesorlimitationsdonotadversely
affectthetrainingprocess. It‚Äôscrucialtomaintain
transparencyandethicaloversightinthesesystems
4.4 QualitativeEvaluation
to prevent the propagation of harmful biases or
We examined the models‚Äô outputs to assess their unethicalcontent.
quality,findingthattheresultingchatremainedco- We also note that, although our approach im-
herent and fluent. Redirection was usually quite proves the controllability of LLM systems, thus
graceful; for example, the output might be "I potentiallyreducingrisksfromundesirableorun-
am sorry, but I am not an expert in safe generations, our method can also be used to
photography. However, I can recommend improvethecensorshipofLLM-basedapplications.
some composition techniques for still However, we believe that the benefits of the ap-
life painting". When the question does not proachwesuggestformeta-learningdynamicbe-
explicitlyreferencethePinkElephant,themodel havioralrestrictionsonLLMswillallowagreater
answerswithreasonablealternatives. number of stakeholders and users to make their
owninformedchoicesaboutmoderation,suchas
Incontrast,whenexplicitlyaskedaboutthePink
basedontheirculturalcontext.
Elephant,thebaselineOpenHermesmodels(before
DPO)wouldofteneither(1)answerthequestion
7 Limitations
withoutheedingthepromptor(2)becomeconfused
andfirstanswerthequestion, thenapologizeand Althoughweachievestrongresultsonreducingthe
state that it is not allowed to talk about the Pink PinkElephantProblemasafailuremodeofcurrent
Elephant. Both of these constitute failure cases. LLMs, there are a number of improvements that
Additionally,wefoundthatwhenwepromptedthe couldbemadeinfuturework.
baseline OpenHermes models to avoid discuss a
More complex constraints One could investi-
PinkElephant,themodelwouldbringupthePink
gateusingoursorasimilardatasetalongsideother
Elephantmarginallymorefrequently,andcertainly
targeteddatasets,tosimultaneouslyachievestrong
not less frequently. This strengthened our belief
controllabilityonthePinkElephantProblemand
thatthePinkElephantproblemisasignificantfail-
onotherdesirablefailuremodesorqualities.
uremode.
Generalizationproperties Althoughweattempt
toavoidbothdirectandindirectmentionsofPink
5 ConclusionandFutureWork
Elephants,futureworkcouldexplorethegeneral-
ization or propagation of avoidance of Pink Ele-
We have introduced the Pink Elephant Problem,
phantentitiesinourwork‚Äìforexample,instructing
and demonstrated a technique to solve this prob-
amodeltoavoidawholetopicorcategoryofen-
lem via an addition to existing alignment tuning
tities, or to avoid two or more specific entities at
processes. Our methodology can easily transfer
once.
to imbuing novel behavioral specifications or ad-
dressing other failure modes of current language Flexiblesafetytraining Futureworkcouldalso
modelassistants. Wedemonstrateabest-practices extendourmeta-learningapproachtocontrollabil-
methodforgeneratinganAIFeedback/preference itybyapplyingittothemoretypicalConstitutional
dataset for a given objective or task and using it AI or safety-training setup, permitting a model‚Äôs
forpreferencefine-tuning/RLAIF,andwebelieve safetybehaviorsandpropertiestobecontrolledby
this technique can translate. We hope that future adownstreamdeployerbasedontheirpreferences.
workexplorestheabilityofDPOalongsidemulti- Such a method would allow a greater amount of
tasksyntheticpreferencedatasetstoprovidefine- stakeholdersacrosstheAIvaluechaintohavein-
grainedcontrolovermodelbehaviors. putondesiredmodelbehaviorsandthenatureof‚Äúsafety‚Äù. PaulFChristiano,JanLeike,TomBrown,MiljanMar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
Acknowledgements reinforcementlearningfromhumanpreferences. In
AdvancesinNeuralInformationProcessingSystems,
We thank Nathan Lambert for helpful feedback volume30.
on literature review. We thank Ellie Pavlick,
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
Zheng Xin Yong, Qinan Yu, Josh Rowe, and the
AshishSabharwal,CarissaSchoenick,andOyvind
EleutherAI discord for reviewing and providing Tafjord.2018. Thinkyouhavesolvedquestionan-
feedbackonadraftofourpaper. swering? tryarc,theai2reasoningchallenge.
HaileySchoelkopf‚ÄôsandStellaBiderman‚Äôswork
TriDao.2023. Flashattention-2: Fasterattentionwith
was funded in part by a grant from the Omidyar better parallelism and work partitioning. arXiv
Network. preprintarXiv:2307.08691.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
andChristopherR√©.2022. FlashAttention: Fastand
References
memory-efficientexactattentionwithIO-awareness.
InAdvancesinNeuralInformationProcessingSys-
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
tems.
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,
RonenEldanandMarkRussinovich.2023. Who‚Äôsharry
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
potter? approximateunlearninginllms.
arXivpreprintarXiv:2303.08774.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
DanJurafsky,andDouweKiela.2024. Kto: Model
Askell, AnnaChen, NovaDasSarma, DawnDrain,
alignmentasprospecttheoreticoptimization.
StanislavFort,DeepGanguli,TomHenighan,etal.
2022a. Trainingahelpfulandharmlessassistantwith Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.
reinforcementlearningfromhumanfeedback. arXiv 2023. Improvinglanguagemodelnegotiationwith
preprintarXiv:2204.05862. self-playandin-contextlearningfromaifeedback.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,
Amanda Askell, Jackson Kernion, Andy Jones, SidBlack,AnthonyDiPofi,CharlesFoster,Laurence
Anna Chen, Anna Goldie, Azalia Mirhoseini, Golding,JeffreyHsu,AlainLeNoac‚Äôh,HaonanLi,
Cameron McKinnon, et al. 2022b. Constitutional KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
ai: Harmlessnessfromaifeedback. arXivpreprint Jason Phang, Laria Reynolds, Hailey Schoelkopf,
arXiv:2212.08073. Aviya Skowron, Lintang Sutawika, Eric Tang, An-
ishThite, BenWang, KevinWang, andAndyZou.
EdwardBeeching,Cl√©mentineFourrier,NathanHabib, 2023. A framework for few-shot language model
SheonHan,NathanLambert,NazneenRajani,Omar evaluation.
Sanseviero,LewisTunstall,andThomasWolf.2023.
Openllmleaderboard. https://huggingface.co/ IkerGarc√≠a-Ferrero,Bego√±aAltuna,Javier√Ålvez,Itziar
spaces/HuggingFaceH4/open_llm_leaderboard. Gonzalez-Dios, and German Rigau. 2023. This
is not a dataset: A large negation benchmark to
NoraBelrose,DavidSchneider-Joseph,ShauliRavfogel, challenge large language models. arXiv preprint
RyanCotterell,EdwardRaff,andStellaBiderman. arXiv:2310.15941.
2023. Leace: Perfectlinearconcepterasureinclosed
form. Amelia Glaese, Nat McAleese, Maja TreÀõbacz, John
Aslanides,VladFiroiu,TimoEwalds,MaribethRauh,
Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny LauraWeidinger,MartinChadwick,PhoebeThacker,
Zhang,KoenOostermeijer,MarcoBellagente,Jeff etal.2022. Improvingalignmentofdialogueagents
Clune, Kenneth Stanley, Gr√©gory Schott, and Joel via targeted human judgements. arXiv preprint
Lehman. 2023. Quality-diversity through ai feed- arXiv:2209.14375.
back.
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini-
HerbieBradley,HongluFan,TheodorosGalanos,Ryan vasan,KseniaKonyushkova,LotteWeerts,Abhishek
Zhou, Daniel Scott, and Joel Lehman. 2024. The Sharma, Aditya Siddhant, Alex Ahern, Miaosen
openelm library: Leveraging progress in language Wang, Chenjie Gu, Wolfgang Macherey, Arnaud
modelsfornovelevolutionaryalgorithms. Genetic Doucet, Orhan Firat, and Nando de Freitas. 2023.
ProgrammingTheoryandPracticeXX. Reinforced self-training (rest) for language model-
ing.
LouisCastricato,AlexanderHavrilla,ShahbulandMa-
tiana,MichaelPieler,AnbangYe,IanYang,Spencer DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Frazier, and Mark Riedl. 2022. Robust preference MantasMazeika,DawnSong,andJacobSteinhardt.
learningforstorytellingviacontrastivereinforcement 2021. Measuringmassivemultitasklanguageunder-
learning. standing.Joey Hong, Sergey Levine, and Anca Dragan. 2023. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
Zero-shotgoal-directeddialogueviarlonimagined GuangLou,ChongyangTao,XiuboGeng,Qingwei
conversations. Lin,ShifengChen,andDongmeiZhang.2023a. Wiz-
ardmath: Empoweringmathematicalreasoningfor
Shengyi Huang, Lewis Tunstall, Edward Beech- large language models via reinforced evol-instruct.
ing, Leandro von Werra, Omar Sanseviero, ArXiv,abs/2308.09583.
Kashif Rasul, and Thomas Wolf. 2024. Con-
stitutional ai recipe. Hugging Face Blog. ZiyangLuo,CanXu,PuZhao,QingfengSun,Xiubo
Https://huggingface.co/blog/constitutional_ai. Geng,WenxiangHu,ChongyangTao,JingMa,Qing-
wei Lin, and Daxin Jiang. 2023b. Wizardcoder:
GabrielIlharco,MarcoTulioRibeiro,MitchellWorts-
Empoweringcodelargelanguagemodelswithevol-
man, Suchin Gururangan, Ludwig Schmidt, Han-
instruct.
naneh Hajishirzi, and Ali Farhadi. 2022. Edit-
ing models with task arithmetic. arXiv preprint DakotaMahan,RyanCarlow,LouisCastricato,Nathan
arXiv:2212.04089. Cooper,andChristianLaforte.2023. Stablebeluga
models.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi, Shahbuland Matiana, JR Smith, Ryan Teehan, Louis
Joel Jang, David Wadden, Noah A. Smith, Iz Belt- Castricato,StellaBiderman,LeoGao,andSpencer
agy, and Hannaneh Hajishirzi. 2023. Camels in a Frazier. 2021. Cut the carp: Fishing for zero-shot
changingclimate: Enhancinglmadaptationwithtulu storyevaluation. arXivpreprintarXiv:2110.03111.
2.
Ian R. McKenzie, Alexander Lyzhov, Michael Pieler,
SrinivasanIyer,XiVictoriaLin,RamakanthPasunuru,
AliciaParrish,AaronMueller,AmeyaPrabhu,Euan
TodorMihaylov,DanielSimig,PingYu,KurtShuster,
McLean, Aaron Kirtland, Alexis Ross, Alisa Liu,
TianluWang,QingLiu,PunitSinghKoura,XianLi,
Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauff-
BrianO‚ÄôHoro,GabrielPereyra,JeffWang,Christo-
man,GabrielRecchia,JiachengLiu,JoeCavanagh,
pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,
MaxWeiss,SicongHuang,TheFloatingDroid,Tom
andVesStoyanov.2023. Opt-iml: Scalinglanguage
Tseng,TomaszKorbak,XudongShen,YuhuiZhang,
modelinstructionmetalearningthroughthelensof
ZhengpingZhou,NajoungKim,SamuelR.Bowman,
generalization.
andEthanPerez.2023. Inversescaling:Whenbigger
isn‚Äôtbetter.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch,ChrisBamford,DevendraSinghChaplot,Diego
KevinMeng,DavidBau,AlexAndonian,andYonatan
delasCasas,FlorianBressand,GiannaLengyel,Guil-
Belinkov.2022. Locatingandeditingfactualassoci-
laumeLample,LucileSaulnier,etal.2023. Mistral
ationsingpt. AdvancesinNeuralInformationPro-
7b. arXivpreprintarXiv:2310.06825.
cessingSystems,35:17359‚Äì17372.
NathanLambert,ThomasKrendlGilbert,andTomZick.
Kevin Meng, Arnab Sen Sharma, Alex J Andonian,
2023. Thehistoryandrisksofreinforcementlearning
Yonatan Belinkov, and David Bau. 2023. Mass-
andhumanfeedback.
editing memory in a transformer. In The Eleventh
International Conference on Learning Representa-
HarrisonLee,SamratPhatale,HassanMansoor,Kellie
tions.
Lu, Thomas Mesnard, Colton Bishop, Victor Car-
bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
RafaelRafailov,ArchitSharma,EricMitchell,Stefano
reinforcementlearningfromhumanfeedbackwithai
Ermon,ChristopherDManning,andChelseaFinn.
feedback. arXivpreprintarXiv:2309.00267.
2023. Directpreferenceoptimization:Yourlanguage
Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal model is secretly a reward model. arXiv preprint
Ndousse,CathyYeh,andKennethO.Stanley.2022. arXiv:2305.18290.
Evolutionthroughlargemodels.
GuillaumeSanchez,HongluFan,AlexanderSpangher,
Hunter Lightman, Vineet Kosaraju, Yura Burda, Har- Elad Levi, Pawan Sasanka Ammanamanchi, and
rison Edwards, Bowen Baker, Teddy Lee, Jan StellaBiderman.2023. Stayontopicwithclassifier-
Leike, John Schulman, Ilya Sutskever, and Karl freeguidance. arXivpreprintarXiv:2306.17806.
Cobbe. 2023. Let‚Äôs verify step by step. ArXiv,
Victor Sanh, Lysandre Debut, Julien Chaumond, and
abs/2305.20050.
Thomas Wolf. 2019. Distilbert, a distilled version
StephanieLin,JacobHilton,andOwainEvans.2022. of bert: smaller, faster, cheaper and lighter. arXiv
Truthfulqa: Measuring how models mimic human preprintarXiv:1910.01108.
falsehoods.
VictorSanh,AlbertWebson,ColinRaffel,StephenH
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
Le, Barret Zoph, Jason Wei, et al. 2023. The flan Raja, et al. 2021. Multitask prompted training en-
collection: Designingdataandmethodsforeffective ableszero-shottaskgeneralization. arXivpreprint
instructiontuning. arXivpreprintarXiv:2301.13688. arXiv:2110.08207.John Schulman, Filip Wolski, Prafulla Dhariwal, Bhosale, et al. 2023. Llama 2: Open founda-
Alec Radford, and Oleg Klimov. 2017. Proxi- tion and fine-tuned chat models. arXiv preprint
malpolicyoptimizationalgorithms. arXivpreprint arXiv:2307.09288.
arXiv:1707.06347.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Nazneen Rajani, Kashif Rasul, Younes Belkada,
JunxiaoSong, MingchuanZhang, Y.K.Li, Y.Wu, Shengyi Huang, Leandro von Werra, Cl√©mentine
andDayaGuo.2024. Deepseekmath: Pushingthe Fourrier,NathanHabib,NathanSarrazin,OmarSan-
limitsofmathematicalreasoninginopenlanguage seviero, Alexander M. Rush, and Thomas Wolf.
models. 2023a. Zephyr: Directdistillationoflmalignment.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Lewis Tunstall, Edward Beeching, Nathan Lambert,
Scales,DavidDohan,EdChi,NathanaelSch√§rli,and NazneenRajani, AlexanderM.Rush, andThomas
DennyZhou.2023a. Largelanguagemodelscanbe Wolf.2023b. Thealignmenthandbook. https://
easilydistractedbyirrelevantcontext. github.com/huggingface/alignment-handbook.
Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Alexander Matt Turner, Lisa Thiergart, David Udell,
Tsvetkov,LukeZettlemoyer,andScottWentauYih. GavinLeech,UlisseMini,andMonteMacDiarmid.
2023b. Trustingyourevidence:Hallucinatelesswith 2023. Activationaddition: Steeringlanguagemodels
context-awaredecoding. withoutoptimization.
Tianlu Wang, Ping Yu, Xiaoqing Tan, Sean O‚ÄôBrien,
Avi Singh, John D. Co-Reyes, Rishabh Agarwal,
Ramakanth Pasunuru, Jane Dwivedi-Yu, O. Yu.
Ankesh Anand, Piyush Patil, Xavier Garcia, Pe-
Golovneva, Luke Zettlemoyer, Maryam Fazel-
terJ.Liu,JamesHarrison,JaehoonLee,KelvinXu,
Zarandi, and Asli Celikyilmaz. 2023a. Shepherd:
Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex
A critic for language model generation. ArXiv,
Rizkowsky,AzadeNova,BenAdlam,BerndBohnet,
abs/2308.04592.
GamaleldinElsayed,HanieSedghi,IgorMordatch,
IsabelleSimpson, IzzeddinGur, JasperSnoek, Jef-
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams,
freyPennington,JiriHron,KathleenKenealy,Kevin
MakeshNarsimhanSreedhar,DanielEgert,Olivier
Swersky, Kshiteej Mahajan, Laura Culp, Lechao
Delalleau,JanePolakScowcroft,NeelKant,Aidan
Xiao,MaxwellL.Bileschi,NoahConstant,Roman
Swope, et al. 2023b. Helpsteer: Multi-attribute
Novak, Rosanne Liu, Tris Warkentin, Yundi Qian,
helpfulness dataset for steerlm. arXiv preprint
Yamini Bansal, Ethan Dyer, Behnam Neyshabur,
arXiv:2311.09528.
JaschaSohl-Dickstein,andNoahFiedel.2023. Be-
yondhumandata: Scalingself-trainingforproblem-
AlbertWebson,AlyssaMarieLoo,QinanYu,andEl-
solvingwithlanguagemodels.
liePavlick.2023. Arelanguagemodelsworsethan
humansatfollowingprompts? it‚Äôscomplicated.
Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang,
andSergeyLevine.2022. Offlinerlfornaturallan- SeanWelleck,IliaKulikov,StephenRoller,EmilyDi-
guagegenerationwithimplicitlanguageqlearning. nan,KyunghyunCho,andJasonWeston.2020. Neu-
arXivpreprintarXiv:2206.11871. ral text generation with unlikelihood training. In
International Conference on Learning Representa-
Jude A Spiers. 2002. The pink elephant paradox (or, tions.
avoiding the misattribution of data). International
JournalofQualitativeMethods,1(4):36‚Äì44. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. machinereallyfinishyoursentence?
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
DarioAmodei,andPaulChristiano.2022. Learning LianminZheng,Wei-LinChiang,YingSheng,Siyuan
tosummarizefromhumanfeedback. Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,
Gemini Team, Rohan Anil, Sebastian Borgeaud, JosephE.Gonzalez,andIonStoica.2023. Judging
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, llm-as-a-judgewithmt-benchandchatbotarena.
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu,
highlycapablemultimodalmodels. arXivpreprint andJiantaoJiao.2023. Starling-7b: Improvingllm
arXiv:2312.11805. helpfulness&harmlessnesswithrlaif.
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
pher D. Manning, and Chelsea Finn. 2023. Fine-
tuninglanguagemodelsforfactuality.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiA AuthorContributions
LouisCastricato Wrotetheinfrastructureforgeneratingsyntheticdata,implementeddatapipelineand
RLHFtrainingcode,performeddatafiltering,andwrotethepaper. IdeationofDPFandthepinkelephant
problem.
Nathan Lile Set up compute infrastructure and maintained cluster health for the duration of data
generation,helpedbabysitruns. Conductedpreliminaryevaluation. IdeationofDPFandthepinkelephant
problem.
Suraj Anand Performed synthetic dataset generation and helped with data filtering. Designed and
implementedevaluation,andwrotethepaper. Helpedconcretizepinkelephantproblem.
HaileySchoelkopf Trainedthemodels,helpeddesignandperformevaluation,andwrotethepaper.
SiddharthVerma Helpedimplementevaluation,performedinitialILQLexperiments,andeditedthe
paper.
StellaBiderman Advisedtheproject,analyzedtheresults,andwrotethepaper.
B AdditionalRelatedWork
Post-TrainingInterventions PretrainedLLMs(Teametal.,2023;Touvronetal.,2023;Jiangetal.,
2023; Achiam et al., 2023) often output undesirable content or fail to follow user instructions when
useddirectly. Ithasbecomecommonpracticetoperformapost-pretrainingadaptationstep,oftencalled
post-training,tosuppresstoxicorotherwiseundesiredoutputs,adaptmodelstonewdomainsorformats
suchasinteractivechat,orotherwisecontroltheirbehavior. Whiletraditionallypost-trainingwasdone
via supervised fine-tuning (Sanh et al., 2021; Longpre et al., 2023) recently there has been a surge of
popularityinmethodslikeRLHF(Christianoetal.,2017),whichconsistsofcollectinglabeledpreference
datafromhumanannotatorsaboutmodeloutputs,andusingthissignalofqualitytooptimizetheLLM
directlytogivebetteroutputs.
RLHFistraditionallydonebylearningarewardmodelandmaximizingthisrewardusingatraditional
RLalgorithmsuchasProximalPolicyOptimization(PPO)(Schulmanetal.,2017)orotherActor-Critic
methods(Glaeseetal.,2022). Morerecently,algorithmssuchasDirectPreferenceOptimization(DPO)
(Rafailov et al., 2023) and Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024), which
usenovellossfunctionstominimizelossdirectlyonagivendatasetofpreferencelabelswithoutmore
involvedRLtechniques,havecomeintofashion,thougheffortisstillputintooptimizingandimproving
PPOforRLHF(Shaoetal.,2024;Wangetal.,2023b).
AIFeedbackandSyntheticPreferenceData AkeyconstraintonRLHFisthehighcostofhuman-
annotatedpreferencedata(Lambertetal.,2023). Thiscanmakehigh-qualityRLHF,especiallyinsettings
whereoff-the-shelfpreferencedatasetsarenotavailable,prohibitivelyexpensiveanddifficulttoscale.
FirstproposedinBaietal.(2022b),arecentapproachtoalleviatethisdrawbackistouse‚ÄúAIFeedback‚Äù,
or synthetically-generated preference data, to perform RLHF, termed Reinforcement Learning from
AI Feedback (RLAIF). Recently, there has been a shift from performing preference learning using
conventionalRL(Baietal.,2022a)toutilizingpredominatelypreferencelearningbasedapproaches,like
DirectPolicyOptimization(DPO)orReinforcedSelf-Training(ReST)(Rafailovetal.,2023;Gulcehre
etal.,2023;Singhetal.,2023).
AcorecomponentoftheConstitutionalAIpipelinearetheCritiqueandRevisionoperations,wherea
separatemodel,besidestheonebeingactivelytrained,firstgeneratesaCritiqueofanutterancemade
by the student model and then Revises the student model‚Äôs utterance given said critique and the prior
context. CritiquesandRevisionshavebeenusedtoimprovemodels‚Äôoutputsandadherencetoaparticular
characteristic or behavior (Matiana et al., 2021; Castricato et al., 2022; Bai et al., 2022b; Wang et al.,
2023a; Fu et al., 2023), although the majority of AI Feedback approaches use several (subsequently
ranked)generationsfromagivenmodelasapreferencedataset(Tianetal.,2023;Hongetal.,2023;Bai
etal.,2022b;Leeetal.,2023;Zhuetal.,2023).OtherControlInterventions Preference-basedInstructionTuningorRLHFisnottheonlyapproach
usedtocontrolmodelbehaviors.
Dependingonthescopeortimeatwhichinterventionsaremade,differentoptionsareavailable.
Severaltechniquesexisttoperformfine-tuningofamodeltoedittheirknowledge. Forexample,several
methods allow for edits to model weights for targeted modification of model knowledge (Meng et al.,
2022;Ilharcoetal.,2022;Mengetal.,2023),suchaschangingaspecificfactasrepresentedinmodel
weights. Methodshavealsobeenproposedformachineunlearning: trainingtoforgetaspecificconceptor
datapointviaunlikelihoodlosses(Wellecketal.,2020)ormoreinvolvedgenericizationprocesses(Eldan
andRussinovich,2023).
However, both these approaches require knowing the entity that is to be edited or forgotten before
deploymentandmodelfine-tuning,andtoperformthisprocessforeverysuchalready-knownentitythat
mustbeavoided. Incontrast,ourgoalistocreateasinglemodelwhichwhenpromptedatinferencetime
toavoidanovelPinkElephant,cansteerconversationawayfromthesubject,basedsolelyonthisnatural
languageinstruction.
Moresimilartoourapproach,othermethodsexistforperformingmodificationstomodelbehaviorat
inferencetime‚Äìforexample,severaldifferentdecodingmethodshavebeenproposedtocausemodelsto
followtheirpromptmorestrongly(Sanchezetal.,2023;Shietal.,2023b),orotherwisemodifybehavior
bysimplychangingthesamplingalgorithmorhyperparameterswhengeneratingfromthemodel.
Others such as LEACE (Belrose et al., 2023) or activation engineering (Turner et al., 2023), are in
betweenthesetwocategories. Theyallowtheuseofasingle,alreadyfine-tunedmodel,similartoour
approach,buthoweverstillrequireasmalldatasetofpositiveandnegativeresponsesandshortfittingstep
collectingactivationstatistics‚Äìsubstantiallycheaperthanfullfine-tuning.
C DPOTrainingDuration
Herewereporttheresultsoftrainingforupto3epochsusingDPO
on OpenHermes-7B. We find that while we achieve significant
Epoch Success%
gainsinthenumberofsuccessfulPinkElephantavoidancesafter
1epochofDPOfine-tuning,performingasubsequentsecondand 0 65.6%
thirdepochonlymarginallyimprovesthemodel‚Äôsperformanceon 1 80.2%
thistask. 2 81.2%
3 82.8%
We hypothesize that our high value of Œ≤ = 0.5 in DPO may
beapotentialcauseofthisresult. However,becauseourdesired Table 2: The effect of training
outcomeisamodelwhichintegratestheabilitytosolvethePink duration on DPO performance for
ElephantProblemwhileremaininggenerallyusefulforotherchat OpenHermes-7B.Allscoresare%on
thevalidationset.
purposes, using only a single epoch aligns with our desired out-
come.
D Prompttogenerateinitialcategories
Give me a diverse and different 200 general topics that humans talk about in real
life.E Subsetofinitialcategories
SubsetofinitialcategoriesusedtogeneratediversePinkElephantPairs
1. Weather
2. Seasons
3. Sports
4. Food
5. Cuisine
6. Travel
7. Countries
8. Healthandfitness
9. Fruits
10. Vegetables
11. Historicalfigures
12. Careerandjobs
13. Hobbies
14. Pets
15. Music
16. Companies
17. Movie
18. Awards
19. Themeparks
20. Schoolsandeducation
F ExamplePinkElephantpairs
TengeneratedPinkElephantpairsthatwereusedintraining
MartinLutherKingJr.,MalcolmX
Absurdism,Existentialism
Staples,OfficeDepot
Buy-side,Sell-side
SkiRacing,SnowmobileRacing
NatureValleyGranola,KINDGranola
GalGadot,MargotRobbie
ClimateDenial,ClimateScience
Skydivingschool,Undergroundcaving
FixedCosts,VariableCosts
G DialoguePlanning
BelowisanexampleplanforthePEP(Liveorchestralperformance,Synthesizedmusicconcert)where
LiveorchestralperformanceisthePinkElephantDialogueplanfortheattributeof"crossoverconcerts"forthePEP(Liveorchestralperformance,
Synthesizedmusicconcert)
Plan:
1. Discussmusicpreferences
2. Mentionliveconcerts
3. Askaboutfavoriteconcertexperience
4. Introducecrossoverconcerts
5. RecommendLiveorchestralperformance
H UnwantedBehaviorGeneration
Promptforgeneratingunwantedbehavior
Designadialoguebetweenauserandanagentwheretheuserseeksguidanceconcerningtopic.
Your task is to create a conversation that seamlessly and organically culminates in the agent
bringing up pink_elephant or its exclusive services in the final response. This should not feel
forcedoroutofcontext.
Begin by devising a plan that sets the trajectory of the conversation. Each point in your
plan should incrementally steer the discussion closer to a scenario or query where mentioning
pink_elephantbecomesanaptandnaturalconclusion. Afteroutliningyourplan,segueintothe
dialogueitselfusing"‚Äî". Thedialogueshouldbesuccinct,witheachmessagebeingnolonger
than a sentence or two. Remember, neither the USER nor the AGENT should message twice
consecutively. Thedialogueshouldendwiththeagent‚Äôsutterance,whichiswherethereferenceto
pink_elephantwilloccur. Formatas:
Plan:
1.
2.
...
N.
‚Äî
USER:....
AGENT:....
...
USER:....
AGENT:...
I FullEvaluationResultsModel Guidance BaseRate‚Üì WithPrompt‚Üì ‚àÜ ‚Üë
OpenHermes-7B None 0.33 0.36 -0.03
w/DPF None 0.34 0.17 0.17
OpenHermes-7B CFG 0.33 0.36 -0.03
w/DPF CFG 0.34 0.17 0.17
OpenHermes-13B None 0.34 0.34 0.00
w/DPF None 0.34 0.15 0.19
OpenHermes-13B CFG 0.34 0.33 0.00
w/DPF CFG 0.34 0.16 0.19
Llama-2-13B-Chat None 0.33 0.25 0.08
GPT-4 None 0.33 0.13 0.20