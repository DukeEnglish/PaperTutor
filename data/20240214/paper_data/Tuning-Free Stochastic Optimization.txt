Tuning-Free Stochastic Optimization
Ahmed Khaled1 and Chi Jin1
1Princeton University, Princeton, NJ, USA
February 13, 2024
Abstract
Large-scalemachinelearningproblemsmakethecostofhyperparametertuningevermoreprohibitive.
This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of
“tuning-free” algorithms that can match theperformance of optimally-tuned optimization algorithms up
to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in
particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the
domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by
several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz
functionoveranunboundeddomain,tuning-freeoptimizationisimpossible. Wediscussconditionsunder
whichtuning-freeoptimization ispossibleevenoverunboundeddomains. Inparticular,weshowthatthe
recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently
well-behaved. Forthetask of findingastationary point of asmooth and potentially nonconvexfunction,
we giveavariant of SGDthat matchesthebest-knownhigh-probability convergencerate fortunedSGD
at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no
algorithmcanhopetomatchtheoptimalexpectedconvergenceratefortunedSGDwithhighprobability.
1 Introduction
The hyperparameters we supply to an optimization algorithm can have a significant effect on the run-
time of the algorithm and the quality of the final model (Yang et al., 2021; Sivaprasad et al., 2020). Yet
hyperparameter tuning is costly, and for large models might prove intractable (Black et al., 2022). As
a result, researchers often resort to using a well-known optimizer like Adam (Kingma and Ba, 2015) or
AdamW (Loshchilov and Hutter, 2019) with widely used or default hyperparameters. For example, GPT-
3 (Brown et al., 2020), BLOOM (Workshop et al., 2022), LLaMA (Touvron et al., 2023a), and LLaMA2
(Touvron et al., 2023b) all use either Adam or AdamW with identical momentum parameters and similar
training recipes.
This situationpresents an immense opportunity for algorithmsthat cantune hyperparameterson-the-fly.
Yetsuchalgorithmsandtheir limits arestillpoorlyunderstoodinthe setting ofstochasticoptimization. Let
us make our setting more specific. We consider the minimization problem
minf(x), (OPT)
x
∈X
where f : R is differentiable and lower bounded by f . We assume that we have access to (stochastic)
X → ∗
gradients g(x) that satisfy certain regularity conditions that we shall make precise later.
Our main objects of study are tuning-free algorithms. To make this notion more precise, let be an
A
optimization algorithm that takes in n problem parameters a = (a ,a ,...,a ) and after T (stochastic)
1 2 n
gradient accesses returns a point x such that with high probability
f(x) f Error (f,a,T). (1)
− ∗ ≤ A
The function Error characterizes how well the algorithm minimizes the function f in T steps given the
A A
supplied parameters. Let a = a (f,T) denote the set of parameters that minimizes the right hand side of
∗ ∗
1
4202
beF
21
]CO.htam[
1v39770.2042:viXraequation (1) for a specific function f and number of steps T. In order for an algorithm to find a (f,T), it
∗
muststartsomewhere. Weassumethatwecaneasilyfindlowerandupperboundsontheoptimalparameters:
two sets a and a such that for i=1,2,...,n we have
a a a .
i ≤ ∗i ≤ i
Such hints on problem parameters can often be easily estimated in practice, and are a much easier ask than
the optimal parameters. To be a tuning-free version of , an algorithm has to approximately match the
A B
performance of with optimally tuned parameters given those hints, a definition we make rigorous next.
A
Definition 1.1. (Tuning-free algorithms). We call an algorithm a tuning-free version of if given
B A
hints a,a on the optimal parameters a for a function f it achieves the same error as with the optimal
∗
A
parameters up to only polylogarithmic degradation that depends on the hints and the number of (stochastic)
gradient accesses T. That is, if achieves error f(x) f Error (f,a (f,T),T), then achieves the
∗
A − ∗ ≤ A B
guarantee:
f(x) f ι Error (f,a ,T), ( -error)
∗
− ∗ ≤ · A B
where ι=polylog a1,...,an,T is a polylogarithmic function of the hints.
a
1
an
(cid:16) (cid:17)
Clearly,askingatuning-freealgorithm toachieveexactlythe sameerroras istoomuch: weoughtto
B A
pay some price for not knowing a upfront. On the other hand, if we allow polynomial dependencies on the
∗
hints, then our hints have to be very precise to avoid large errors. This beats the point of being tuning-free
in the first place.
The algorithm that we are primarily concerned with is Stochastic Gradient Descent (SGD). SGD and
A
its variants dominate in practice, owing to their scalability and low memory requirements (Bottou et al.,
2018). We consider three classes of functions: (a) L-smooth and convex functions, (b) G-Lipschitz and
convex functions, and (c) L-smooth and potentially nonconvex functions. We ask for tuning-free algorithms
for each of these function classes. We give precise definitions of these classes and our oracle model later in
Section 1.1.
Inthesettingofdeterministicoptimization,wehaveaverygoodunderstandingoftuning-freeoptimization:
therearemanymethodsthat,givenonlyhintsontheproblemparametersrequiredbySGD,achievethesame
rate as SGD up to only polylogarithmic degradation. We review this case briefly in Section 4. Despite im-
mensealgorithmicdevelopmentsinstochasticoptimization(Duchi et al.,2010;Levy,2017; Levy et al., 2018;
Li and Orabona, 2019; Kavis et al., 2019; Carmon and Hinder, 2022; Ivgi et al., 2023; Cutkosky et al., 2023)
and the related setting of online learning (Orabona and Pa´l, 2016; Cutkosky and Boahen, 2016; Cutkosky,
2019;Mhammedi et al.,2019;Mhammedi and Koolen,2020;Orabona and Cutkosky,2020)wearenotaware
of any algorithms that fit our definition as tuning-free counterparts of SGD for any of the function classes
we consider. The main question of our work is thus:
Can we find tuning-free counterparts for SGD in the setting of stochastic optimization and the
classes of functions we consider (convex and smooth functions, convex and Lipschitz functions,
and nonconvex and smooth functions)?
Our contributions. We answerthe abovequestionin the negativefor the first twofunction classes and
make some progress towards answering it for the third. In particular, our main contributions are:
• For convex optimization: if the domain of optimization is bounded, we highlight results from the
literature showing tuning-free optimization matching SGD is possible. If the domain of optimization
is unbounded, we give an impossibility result that shows no algorithm can be a tuning-free
X
counterpartof SGD for smooth and convex functions (Theorem 2), as well as for Lipschitz and convex
functions (Theorem 3). Additionally if the stochastic gradient noise has a certain large signal-to-
noise ratio (defined in Section 4.2), then tuning-free optimization is possible even when the domain of
optimization isunboundedandcanbeachievedbytherecently-proposedDoG(Ivgi et al.,2023)and
X
DoWG (Khaled et al., 2023) algorithms for both smooth and/or Lipschitz functions (Theorem 4).
2• For nonconvex optimization: We consider two different notions of tuning-free optimization that
correspond to the best-known convergence error bounds for SGD in expectation (Ghadimi and Lan,
2013) and with high probability (Liu et al., 2023). We show tuning-free optimization is impossiblein
the former setting (Theorem 5). On the other hand, for the latter, slightly weaker notion, we give a
positive result and give a tuning-free variant of SGD (Theorem 6).
1.1 Preliminaries
In this section we review some preliminary notions and definitions that we shall make use of throughout the
paper. We say that a function f : R is convex if for any x,y we have
X → ∈X
f(tx+(1 t)y) tf(x)+(1 t)f(y) for all t [0,1].
− ≤ − ∈
We call a function G-Lipschitz if f(x) f(y) G x y for all x,y . All norms considered in this
| − | ≤ k − k ∈ X
def
paper are Euclidean. We let log x = logx+1. A differentiable function f is L-smooth if for any x,y
+ ∈X
we have f(x) f(y) L x y .
k∇ −∇ k≤ k − k
Oracle model. All algorithms we consider shall access gradients through one of the two oracles defined
below.
Definition 1.2. We say that (f) is a deterministic first-order oracle for the function f if given a
O
point x the oracle returns the pair f(x), f(x) .
∈X { ∇ }
If we only allow the algorithm access to stochastic gradients, then we call this a stochastic oracle. Our
main lower bounds are developed under the following noise model.
Assumption 1.1. The stochastic gradient noise is bounded almost surely. That is, there exists some R 0
≥
such that for all x
∈X
g(x) f(x) R.
k −∇ k≤
The stochastic first-oracle that
Definition 1.3. We say that (f,R ) is a stochastic first-order oracle for the function f with bound
f
O
σ if, given a point x , it returns a pair of random variables fˆ(x),g(x) such that (a) the estimates
f
∈ X
are unbiased E[fˆ(x)] = f(x), E[g(x)] = f(x), and (b) the stochasthic gradientsisatisfy Assumption 1.1 with
∇
R=σ .
f
The above oracle restricts the noise to be bounded almost surely. We shall develop our lower bounds
under that oracle. However,for some ofthe upper bounds we develop,we shallrelax the requirementon the
noise from boundedness to being sub-gaussian(see Section 4.2), and we shall make this clear then.
2 Related Work
This section reviews existing approaches in the literature aimed at reducing or eliminating hyperparameter
tuning.
Parameter-free optimization. An algorithm is called parameter-free if it achieves the convergence
A
rate O˜ G kx √0− Tx ∗k givenT stochasticgradientaccessesforanyconvexfunctionf withstochasticsubgradients
bounde(cid:16)d in norm(cid:17)by G, with possible knowledge of G (Orabona, 2023, Remark 1). There exists a vast
literature on such methods, particularly in the setting of online learning, see e.g. (Orabona and Cutkosky,
2020). Parameter-free optimization differs from tuning-free optimization in two ways: (a) the ˜() can
O ·
suppress higher-order terms that are not permitted according to the tuning-free definition, and (b) gives
the algorithm possible knowledge of a parameter like G whereas tuning-free algorithms can only get to see
upper and lower bounds on G. Nevertheless, many parameter-free methods do not need any knowledge of
G(Cutkosky,2019;Mhammedi et al.,2019;Mhammedi and Koolen, 2020). However,Cutkosky and Boahen
(2017b; 2016) give lowerbounds showing that any online learning algorithminsisting ona linear dependence
3on x x (asinoptimallytunedSGD)mustsufferregretfrompotentiallyexponentialregret. Ifwedonot
0
insik st o− n a∗ lk inear dependence on x x , then the best achievable convergence bound scales x x 3,
0 0
k − ∗k k − ∗k
and this is tight (Mhammedi and Koolen, 2020). None of the aforementioned lower bounds apply to the
setting of stochastic optimization, since in general online learning assumes an adversarial oracle, which is
stronger than a stochastic oracle.
Tuning-free algorithms in the deterministic setting. Gradientdescentaugmentedwith line search
(Nesterov, 2014; Beck, 2017) is tuning-free for smooth convex and nonconvex optimization. Bisection search
(Carmon and Hinder, 2022) is tuning-free for both convex and smooth as well as convex and Lipschitz opti-
mization,asisarestartedversionofgradientdescentwithPolyakstepsizes(Hazan and Kakade,2019). Inthe
smooth setting, the adaptive descent method of (Malitsky and Mishchenko, 2020) is also tuning-free. There
are also accelerated methods (Lan et al., 2023), methods for the Lipschitz setting (Defazio and Mishchenko,
2023), methods based on online learning (Orabona, 2023), and others.
Algorithms for the stochastic setting. Observe that because online learning is a more general
setting than the stochastic one, we can apply algorithms from online convex optimization here, like e.g.
(Mhammedi and Koolen, 2020) coupled with an appropriate online-to-batch conversion (Hazan, 2022). In
more recent work (Carmon and Hinder, 2022; Ivgi et al., 2023), we see algorithmic developments specific to
thestochasticsetting. WediscusstheconvergenceratesthesealgorithmsachieveinmoredetailinSection4.1.
Other hyperparameter tuning approaches. In practice, hyperparameters are often found by grid
search, random search, or methods based on Bayesian optimization (Bischl et al., 2023); None of these ap-
proaches come with efficient theoretical guarantees. Another approach is “meta-optimization” where we
have a sequence of optimization problems and seek to minimize the cumulative error over this sequence.
Often, another optimization algorithm is then used to select the learning rates, e.g. hypergradient de-
scent (Baydin et al., 2018). Meta-optimization approaches are quite difficult to establish theoretical guaran-
tees for, and only recently have some theoretical results been shown (Chen and Hazan, 2023). Our setting
inthis paper is different, since ratherthanseek to minimize regretovera sequenceof optimizationproblems,
we have a single function and an oracle that gives us (stochastic) gradient estimates for this function.
3 Tuning-Free Optimization Under a Bounded Domain
We begin our investigation by studying the bounded setting, where we make the following assumption on
the minimization problem (OPT):
Assumption 3.1. The optimization domain is bounded. There exists some constant D > 0 such that
X
x y D for all x,y .
k − k≤ ∈X
We seek a tuning-free version of SGD. Recall that SGD achieves with probability at least 1 δ the
−
following convergence guarantee (Jain et al., 2019; Liu et al., 2023)
DL2 + σD if f is L-smooth,
f(x ) f υ T √T (2)
out − ∗ ≤ ·(√G2+σ2D if f is G-Lipschitz,
√T
where ν = polylog1 and σ is an upper bound on the stochastic gradient noise (per Assumption 1.1). To
δ
achieve the convergence guarantee given by equation (2), we need to know the parameters D,σ, and L in
the smooth case or G in the nonsmooth case. Per Definition 1.1, a tuning-free version of SGD will thus be
given the hints D [D,D], σ [σ,σ], and either L [L,L] in the smooth setting or G [G,G] in the
∈ ∈ ∈ ∈
nonsmooth setting. Given those hints, we then ask the algorithm to achieve the same rate as equation (2)
up to a multiplicative polylogarithmic function of the hints.
Itturnsoutthattuning-freeoptimizationunderaboundeddomainissolvableinmanyways. Manymeth-
odsfromtheonlinelearningliterature,e.g.(Cutkosky and Boahen,2017a;Mhammedi et al.,2019;Cutkosky,
2019) can solve this problem when combined with standard online-to-batch conversion bounds. We give the
details for this construction for one such algorithm in the next proposition:
Proposition 1. Coin betting through Online Newton Steps with Hints (Cutkosky, 2019, Algorithm 1) is
tuning-free in the bounded setting.
4The proof of this result is provided in the appendix, and essentially just combines (Cutkosky, 2019,
Theorem 2) with online-to-batch conversion.
Inthispaper,weshallfocusparticularlyonmethodsthatfitthestochasticgradientdescentparadigm,i.e.
that use updates of the form x =x η g , where g is the stochastic gradient at step k. Two methods
k+1 k k k k
−
that fit this paradigm are DoG (Ivgi et al., 2023) and DoWG (Khaled et al., 2023). DoG uses stepsizes of
the form
t
r
η = t , r =max( x x ,r ), u = g 2, (3)
t t t 0 ǫ t k
√u t k t k − k k k
≤ k=0
X
where r is a parameter that we will always set to D. Similarly, DoWG uses stepsizes of the form
ǫ
r2 t
η = t , r =max( x x ,r ), v = r2 g 2. (4)
t √v t t k t k t − 0 k ǫ t kk k k
≤ k=0
X
The next theorem shows that in the bounded setting, both DoG and DoWG are tuning-free.
Theorem 1. DoG and DoWG are tuning-free in the bounded setting. That is, there exists some ι =
polylog(D,σ,T,δ 1) such that
D σ −
DL2 + σD if f is L-smooth,
f(x ) f ι T √T
out − ∗ ≤ ·(√G2+σ2D if f is G-Lipschitz.
√T
This rate is achieved simultaneously for both classes of functions without prior knowledge of whether f is
smooth or Lipschitz (and thus no usage of the hints L,L,G,G).
This theoremessentiallycomes forfree bymodifying the resultsin(Ivgi et al., 2023; Khaled et al., 2023),
andwhiletheproofmodificationsarequitelengthyweclaimnosignificantnoveltyhere. Wenotefurtherthat
unlike (Cutkosky, 2019, Algorithm 1), both DoG and DoWG are single-loop algorithms– they do not restart
the optimization process or throw away progress. This is a valuable property and one of the reasons we
focus on these algorithms in the paper. Moreover,DoG and DoWG are universal. An algorithmis universal
if it achieves the same rate as SGD for Lipschitz functions and also takes advantage of smoothness when
it exists (Levy, 2017), without any prior knowledge of whether f is smooth. DoG and DoWG enjoy this
property in the bounded domain setting.
4 Tuning-free Optimization Under an Unbounded Domain
We now continue our investigation to the general, unbounded setting where = Rd. Now, the diameter D
X
in Assumption 3.1 is infinite. The convergence of SGD is then characterized by the initial distance to the
optimum D = x x (Liu et al., 2023). We canshow that SGD with optimally-tuned stepsizes achieves
0
∗ k − ∗k
with probability at least 1 δ the convergence rates
−
D L2 + σD if f is L-smooth,
f(x ) f υ
∗T √T∗
(5)
out − ∗ ≤ ·(√G2 √+ Tσ2D
∗
if f is G-Lipschitz,
where υ = polylog 1 and σ is the maximum stochastic gradient noise norm. An algorithm is a tuning-free
δ
version of SGD in the unbounded setting if it can match the best SGD rates given by Equation (5) up to
polylogarithmic factors given access to the hints D,D,σ,σ, and G,G or L,L. This is a tall order: unlike in
the bounded setting, a tuning-free algorithm now has to compete with SGD’s convergence on any possible
initialization.
Deterministicsetting. Whenthereisnostochasticgradientnoise,i.e. σ =0andthealgorithmaccesses
gradients according to the deterministic first-order oracle (Definition 1.2), Tuning-free versions of gradient
descent exist. For example, the Adaptive Polyak algorithm (Hazan and Kakade, 2019), a restarted version
of gradient descent with the Polyak stepsizes (Polyak, 1987) is tuning-free:
5Proposition2(Hazan and Kakade(2019)). TheAdaptivePolyakalgorithmfrom(Hazan and Kakade,2019)
is tuning-free in the deterministic setting.
Thisisfarfromthe onlysolution,andwementionafew othersnext. Parameter-freemethodsaugmented
with normalization are also tuning-free and universal, e.g. plugging in d = D in (Orabona, 2023) gives
0
tuning-free algorithms matching SGD. The bisection algorithm from (Carmon and Hinder, 2022) is also
tuning-free, as is the simple doubling trick. Finally, T-DoG and T-DoWG, variants of DoG and DoWG
which use polylogarithmically smaller stepsizes than DoG and DoWG, are also tuning-free, as the following
direct corollary of (Ivgi et al., 2023; Khaled et al., 2023) shows.
Proposition 3. T-DoG and T-DoWG are tuning-free in the deterministic setting.
T-DoGandT-DoWGusethesamestepsizestructureasDoGandDoWG(giveninequations(3)and(4)),
but divide these stepsizes by running logarithmic factors as follows
r r2
T-DoG: η = t , T-DoWG: η = t .
t √u tlog
+
uu 0t t √v tlog
+
vv 0t
Both methods achieve the same convergence guarantee as in equation (5) up to polylogarithmic factors in
the hints.
4.1 Impossibility Results in the Stochastic Setting
The positive results in the deterministic setting give us some hope to obtain a tuning-free algorithm. Unfor-
tunately, the stochastic setting turns out to be a tougher nut to crack. Our first major result, given below,
slashes any hope of finding a tuning-free algorithm for smooth and stochastic convex optimization.
Theorem 2. For any polylogarithmic function ι:R4 R and any algorithm , there exists a time horizon
→ A
T, an L-smooth and convex function f, and a stochastic oracle (f,σ ), and valid hints L,L,D,D,σ,σ such
f
O
that the algorithm initialized at some x returns with some constant probability a point x satisfying
0 out
A
L D σ LD2 σ D
f
Error =f(x out) f >ι , , ,T ∗ + ∗ ,
A − ∗ L D σ · T √T
(cid:18) (cid:19) (cid:20) (cid:21)
where D = x x is the initial distance to the optimum and σ is the maximum norm of the stochastic
0 f
∗ k − ∗k
gradient noise.
Proof idea. This lower bound is achieved by 1-dimensional functions. In particular, we construct two
one-dimensional quadratic functions f and h with associated oracles (f,σ ) and (h,σ ), and we supply
f h
O O
the algorithm with hints that are valid for both functions and oracles. We show that with some constant
probability, the algorithm observes the same stochastic gradients from both (f,σ ) and (h,σ ) for the
f h
O O
entirerun. Sincethealgorithmcannottellaparteitheroracle,itmustguaranteethatequation(5)holdswith
highprobabilityfor bothf andh ifit is tobe tuning-free. Now,if wechoosef andh further apart,ensuring
that their respective oracles return the same gradients with some constant probability becomes harder. On
the other hand, if we choose f and h too close, the algorithm can conceivably guarantee that equation (5)
holds (up to the same polylogarithmic factor of the hints) for both of them. By carefully choosing f and h
to balance out this tradeoff, we show that no algorithm can be tuning-free in the unbounded and stochastic
setting. The full proof is provided in Section 8.3 in the appendix.
Comparison with prior lower bounds. The above theoremshows a fundamental separationbetween
thedeterministic andstochasticsettings whennotgivenknowledgeofthe problemparameters. The classical
lowerboundsfordeterministicandstochasticoptimizationalgorithms(Nesterov,2018;Woodworth and Srebro,
2016; Carmon et al., 2017) rely on a chain construction that is agnostic to whether the optimization algo-
rithmhasaccesstoproblemparameters. Ontheotherhand,lowerboundsfromtheonlinelearningliterature
show that tuning-free optimization matching SGD is impossible when the oraclecan be adversarial(and not
stochastic), see e.g. (Cutkosky and Boahen, 2017b; 2016). However, adversarial oracles are much stronger
thanstochasticoracles,astheycanchangethefunctionbeingoptimizedinresponsetothealgorithm’schoices.
Our lower bound is closest in spirit to the lower bounds from the stochastic multi-armed bandits literature
that also rely on confusing the algorithm with two close problems (Mannor and Tsitsiklis, 2004).
Our next result shows that tuning-free optimization is also impossible in the nonsmooth case.
6Theorem 3. For any polylogarithmic function ι:R4 R and any algorithm , there exists a time horizon
→ A
T, valid hints L,L,D,D,σ,σ, an G-Lipschitz and convex function f and an oracle (f,σ ) such that the
f
O
algorithm returns with some constant probability a point x satisfying
out
A
G D σ √G2+σ2D
Error =f(x out) f >ι , , ,T ∗ .
A − ∗ (cid:18)G D σ (cid:19)·" √T #
TheprooftechniqueusedforthisresultreliesonasimilarconstructionasTheorem2butusestheabsolute
loss instead of quadratics.
Existing algorithms and upper bounds in the stochastic setting. Carmon and Hinder (2022)
give a restarted variant of SGD with bisection search. If f is G-Lipschitz and all the stochastic gradients
are also bounded by G, their method uses the hint G G and achieves the following convergence rate with
≥
probability at least 1 δ
−
ιG D G D G
f(xˆ) f cι η ǫ G+ + ∗ + ∗ , (6)
− ∗ ≤ T √T T
(cid:18) (cid:18) (cid:19) (cid:19)
wherecisanabsoluteconstant,ιapoly-logarithmicanddouble-logarithmicfactor,andη aninputparameter.
ǫ
If we set η =D/T D , then the guarantee of this method becomes
ǫ
≤
T∗
D G D G
f(xˆ) f cι ∗ + ∗ . (7)
− ∗ ≤ √T T
(cid:18) (cid:19)
Unfortunately,thisdependencedoesnotmeetourbarasthepolynomialdependenceonGisinahigher-order
term. A similar result is achieved by DoG (Ivgi et al., 2023). A different sort of guarantee is achieved by
Mhammedi and Koolen (2020), who give a method with regret
1 T −1 GD GD3
g t,x t x cι ∗ + ∗ , (8)
T h − ∗i≤ √T T
t=0 (cid:18) (cid:19)
X
for some absolute constant c and polylogarithmic factor ι. This result is in the adversarial setting of online
learning,andclearlydoesnotmeetthebarfortuning-freeoptimizationmatchingSGDduetothecubicterm
D3.
∗
4.2 Guarantees Under Benign Noise
Inthelastsubsection,wesawthattuning-freeoptimizationisingeneralimpossible. However,itisclearthat
sometimes it is possible to get within the performance of tuned SGD with self-tuning methods (Ivgi et al.,
2023; Defazio and Mishchenko, 2023). However, the oracles used in Theorems 2 and 3 provide stochastic
gradients g(x) such that the noise is almost surely bounded (i.e. satisfies Assumption 1.1):
g(x) f(x) R.
k −∇ k≤
Soboundednessisclearlynotenoughtoenabletuning-freeoptimization. However,weknowfrompriorresults
(e.g. (Carmon and Hinder, 2022)) that if we can reliably estimate the upper bound R on the noise, we can
adapttounknowndistancetotheoptimumD orthesmoothnessconstantL. Themainissuethattheoracles
∗
in the lower bound of Theorem 2 make it impossible to do that: while the noise n(x) = g(x) f(x) is
−∇
boundedalmostsurelybyR,thealgorithmonlygetstoobservethesamenoisen(x)fortheentireoptimization
run. This foils any attempt at estimating R from the observed trajectory.
A note on notation in this section and the next. Inthepastsectionweusedσ todenote auniform
upper bound on the gradientnoise, while in this sectionand the next we use σ to denote the variance of the
stochastic gradient noise n(x) rather than a uniform upper bound on it. Instead, we use R to denote the
uniform upper bound on the noise.
We will see that for some notion of benign noise, tuning-free optimization matching SGD is possible. We
will developour results under a more generalassumptionon the distribution of the stochastic gradientnoise
g(x) f(x)
−∇
7Assumption 4.1. (Noise with Sub-Gaussian norm). For all x Rd, the noise vector n(x) =g(x) f(x)
∈ −∇
satisfies
• n(x) is unbiased: E[g(x)]= f(x).
∇
• n(x) has sub-gaussian norm with modulus R:
t2
Prob( n(x) t) 2exp − .
k k≥ ≤ 2R2
(cid:18) (cid:19)
• n(x) has bounded variance: E n(x) 2 =σ2 <+ .
k k ∞
h i
This assumption is very general, it subsumes bounded noise (where R = σ ) and sub-gaussian noise.
f
The next definition gives a notion of signal-to-noise that turns out to be key in characterizing benign noise
distributions.
Definition 4.1. Suppose the stochastic gradient noise satisfies Assumption 4.1. We define the signal-to-
noise ratio associated with the noise as
σ
K = 1.
snr
R ≤
To better understand the meaning of K , we consider the following example. Let Y be a random
snr
vector with mean E[Y] = µ and variance E Y 2 = σ2. Suppose further that the errors Y µ are
k k k − k
bounded almost surely by some R. Then Y hµ satisifies the assumptions in Assumption 4.1. Let Y 1,...,Y
b
−
be independent copies of Y. Through standard concentration results (see Lemma 8) we can show that with
high probability and for large enough b
n
1
σˆ d =ef Y µ 2 ≅σ2.
n k − k
i=1
X
Now observe that if the ratio K is small, then we cannot use the sample variance σˆ as an estimator for
snr
the almost-sure bound R. But if the ratio K is closer to 1, then we have σ2 ≅R2 and we can use σˆ as an
snr
estimatorforR. Thisfixestheproblemwehighlightedearlier: nowweareabletogetanaccurateestimateof
R from the observed stochastic gradients. The next proposition gives examples of noise distributions where
K is close to 1.
snr
Proposition 4. Suppose that the noise vectors g(x) f(x) follow one of the following two distributions:
−∇
• A Gaussian distribution with mean 0 and covariance σ2 I , with σ > 0, where I is the d d
d · d ×d d ×d ×
identity matrix.
• A Bernoulli distribution, where [g(x) f(x)] = σφ(x) with equal probability for some φ such that
−∇ ±
φ(x) =1 almost surely.
k k2
Then K = (1).
snr
O
We now give an algorithm whose convergence rate characterized by the signal-to-noise ratio K . We
snr
combine a variane estimation procedure with the T-DoG/T-DoWG algorithms in Algorithms 2 and 3. The
next theorem gives the convergence of this algorithm. This theorem is generic, and does not guarantee any
tuning-freematching ofSGD, but canleadto tuning-freematching ofSGD if the signalto noiseratiois high
enough.
Theorem 4. Suppose we are given access to stochastic gradient estimates g(x) such that the noise vectors
[g(x) f(x)] Rd satisfy Assumption 4.1 with modulus R and signal-to-noise ratio K . If we run T-DoG
snr
−∇ ∈
or T-DoWGwithvariance estimation (Algorithms 2and3)withaminibatch sizeb 2largeenoughtosatisfy
≥
log2bT log2(b ∨d)T
c δ + δ K2 θ,
·s b b ≤ snr−
 
where c is some absolute constant and θ [0,K ] is some known constant. Then Algorithms 2 and 3 with
snr
∈
either option returns a point x such that with probability at least 1 δ,
out
−
8• If f is L-smooth:
LD2b θ 1RD √b
−
f(x out) f cι ∗ + ∗ ,
− ∗ ≤ T total √T total !
where D = x x , c is an absolute constant, ι is a polylogarithmic factor of the hints, and T
0 total
∗ k − ∗k
denotes the total number of stochastic gradient accesses.
• If f is G-Lipschitz:
√G2+θ 2R2D √b
f(x out) f cι − ∗ .
− ∗ ≤ √T total
Note on dimensiondependence in Theorem 4. Wenotethatthelogarithmicdimensiondependence
onthedimensiondcanberemovedif,ratherthanassumingthenormofthenoiseissubgaussian,weassumed
it was bounded.
Onthe surface,itlooks like Theorem4 simply trades offknowledgeofthe absolute boundonthe noise R
with knowledge of some constant θ that lies in the interval [0,K ]. In order to see how Theorem 4 can be
snr
useful, consider the special cases given in Proposition 4. For these noise distributions, we see that choosing
a minibatch size b (log 2dT +1) suffices to ensure Algorithms 2 and 3 converges with the simple choice
≈ O δ
θ = 1. Eventhoughwehadnoaprioriknowledgeofthevarianceσ2 anddidnotassumethenoisedistribution
2
wasstationary,wecouldstilloptimizethe function. Ingeneral,theminibatchsizeb (log 2dT +1)suffices
≈O δ
as long as K is bounded away from zero by some constant. The final cost of running the algorithm is
snr
T = b T = ιT, where ι is some polylogarithmic factor. Therefore, we only pay a logarithmic price for
total
·
not knowing the distribution. Of course, if K is small enough, there can be no optimization– there is not
snr
enoughsignaltodoanyestimationofthesub-gaussianmodulusR. ThedistributionusedinTheorem2does
force K 1.
snr ≤ T
5 Nonconvex Tuning-Free Optimization
Inthis section,weconsiderthe casewherethe optimizationproblem(OPT)is possiblynonconvex. Through-
out the section, we assume that f is L-smooth and lower bounded by some f R. In this setting, SGD
∗ ∈
with a tuned stepsize achieves the following rate in expectation
1 T −1
f(x t) 2 c
L(f(x 0) −f ∗)σ2
+
L(f(x 0) −f ∗)
, (9)
T
t=0
k∇ k ≤ "r T T #
X
forsomeabsoluteconstantc>0. Thisrateisknowntobetightforconvergenceinexpectation(Arjevani et al.,
2019). However, it is not known if it is tight for returning a high probability guarantee. The best-known
high-probability convergence rate for SGD is given by (Liu et al., 2023, Theorem 4.1) and guarantees with
probability at least 1 δ that
−
1 T −1
f(x t) 2 5
L(f(x 0) −f ∗)R2
+
2(f(x 0) −f ∗)L
+
12R2log1
δ. (10)
T k∇ k ≤ T T T
t=0 r
X
We now consider tuning-free algorithms that can match the performance of SGD characterized by ei-
ther equation (9) or equation (1). Per Definition 1.1, an algorithm is given (1) an initialization x , (2) a
0
B
budget of T stochastic gradient accesses, and (3) hints L,L,R,R,∆,∆ on the problem parameters such
total
def
that(a)ifListhesmoothnessconstantoff thenL [L,L],(b)R [R,R],and(c)∆ = f(x ) f [∆,∆].
0
We call strongly tuning-free if it matches the p∈ erformance of S∈ GD characterized by equat− ion∗ (∈ 9) up to
B
polylogarithmicfactors. Alternatively,ifitinsteadmatchestheweakerguaranteegivenbyequation(10)then
we call it weakly tuning-free.
Our first result in this setting shows that we cannot hope to achieve the rate given by equation (9) in
high probability, even given access to hints on all the problem parameters.
9Algorithm 1 Restarted SGD
Require: Initialization y , probability δ, hints R,R,∆,∆,L,L, total budget T
0 total
1: Set η ǫ = 1 and
L
min(L,
5TR2
)
2∆
N =1+ log (11)
  max(L,q 5TR2 )
∆
  q 
 
2: if T total <N then  
3: Return y 0.
4: end if
5: Set the per-epoch iteration budget as T = T total/N .
⌈ ⌉
6: for n=1 to N do
7: η =η ǫ2n
8: Run SGD for T iterations with stepsize η starting from y 0 to get outputs xn 1,...,xn T.
9: Set y n,gˆ n =FindLeader(S,δ,T) (see Algorithm 4).
10: end for
11: Return y =argmin n [N] gˆ n .
∈ k k
Theorem 5. For any polylogarithmic function ι:R4 R and any algorithm , there exists a time horizon
→ A
T, valid hints L,L,∆,∆,σ,σ, an L-smooth and lower-bounded function f and an oracle (f,σ ) such that
f
O
the algorithm returns with some constant probability a point x satisfying
out
A
L ∆ σ L∆σ2 L∆
Error = f(x ) 2 >ι , , ,T + ,
out
A k∇ k (cid:18)L ∆ σ (cid:19)· "r T T #
where ∆=f(x ) f
0
− ∗
Surprisingly, our next theorem shows that the rate given by equation (10) is achievable up to polyloga-
rithmic factors given only access to hints. To achieve this, we use a restarted variant of SGD (Algorithm 6)
combined with a “Leader Finding” procedure that selects a well-performing iterate by subsampling.
Theorem 6. (Convergence of Restarted SGD) Let f be an L-smooth function lower bounded by f and
∗
suppose the stochastic gradient noise vectors satisfy Assumption 4.1. Suppose that we are given the following
def
hints on the problem parameters: (a) L [L,L], (b) R [R,R], and (c) ∆ = f(x ) f [∆,∆]. Then
f f 0
thereexistssomeabsoluteconstantcsuch∈ thattheoutputo∈ fAlgorithm 1satisfiesafter T − ∗ lo∈ g 1 stochastic
total · + δ
gradient evaluations
R2log2dmax(logδ1,N)
L(f(y ) f )R2 (f(y ) f )L
f(y) 2 c δ +c N 0 − ∗ + 0 − ∗ ,
k∇ k ≤ · T
total
· ·s T
total
T
total

 
where c is an absolute constant, N is a polylogarithmic function of the hints defined in equation (11), and d
is the problem dimensionality.
DiscussionofTheorem6. Thistheoremshowsthatinthenonconvexsetting,wepayonlyanadditional
polylogarithmic factor to achieve the same high-probability rate as when we know all parameters. We
emphasize that we do not know if the rate given by equation (10) is tight, but it is the best in the literature.
Finally, the logarithmic dimension dependence on the dimension d can be removed if, rather than assuming
the norm of the noise is subgaussian, we assumed that it was bounded almost surely.
Proof Idea. The proof is an application of the so-called “doubling trick” with a careful comparison
procedure. If we start with a small enough stepsize, we only need to double a logarithmic number of times
until we find a stepsize η such that η η η , where η is the optimal stepsize for SGD on this problem.
′ 2∗
≤
′
≤ ∗ ∗
We therefore run SGD for N epochs with a carefully chosen N, each time doubling the stepsize. At the end
10of every SGD run, we run the FindLeader procedure (Algorithm 4) to get with high probability a point y
n
such that
T 1
f(y ) 2 1 − f(xn) 2,
k∇ n k ≤ T k∇ t k
t=0
X
where xn,...,xn are the SGD iterates from the n-th epoch. Finally, we know that at least one of these N
1 T
pointsy ,...,y hassmallgradientnorm,so wereturnthe pointwith the minimalestimatedgradientnorm
1 N
and bound the estimation error as a function of T. The total number of gradient accesses performed is at
most N(T +MT) T log 1. Therefore, both the restarting and comparisonprocedures add at most a
≈ total · + δ
logarithmic number of gradient accesses.
Related work. ManypapersgivehighprobabilityboundsforSGDorAdaGradandtheirvariantsinthe
nonconvexsetting(Ghadimi and Lan,2013;Madden et al.,2020;Lei and Tang,2021;Li and Orabona,2019;
2020;Faw et al.,2022;Kavis et al.,2022),buttothebestofourknowledgenonegiveatuning-freealgorithm
matching SGD per Definition 1.1. The FindLeader procedure is essentially extracted from (Madden et al.,
2020, Theorem 13), and is similar to the post-processing step in (Ghadimi and Lan, 2013).
Comparison with the convex setting. The rate achieved by Theorem 6 stands in contrast to the
best-known rates in the convex setting, where we suffer from a polynomial dependence on the hints, as
inequation(7). Onepotentialreasonforthis divergenceisthedifficulty oftellingapartgoodandbadpoints.
In the convex setting, we ask for a point y with a small function value f(y). And while the oracle gives us
access to stochastic realizations of f(y), the error in those realization is not controlled. Instead, to compare
betweentwopointsy andy wehavetorelyonstochasticgradientinformationtoapproximatef(y ) f(y ),
1 2 1 2
−
and this seems to be too difficult without apriori control on the distance between y and y . On the other
1 2
hand, in the nonconvexsetting, such comparisonis feasible through sampling methods like e.g. Algorithm 4.
6 Conclusion and Open Problems
We have reachedthe end of our investigation. To summarize: we defined tuning-free algorithms and studied
severalsettingswheretuning-freeoptimizationwaspossible,andseveralwhereweprovedimpossibilityresults.
Yet, many open questions remain. For example, tuning-free optimization might be possible in the finite-sum
setting where we can periodically evaluate the function value exactly. The upper bounds we develop in both
the convexand nonconvexsettings require quite stringentassumptions on the noise (such as boundedness or
sub-gaussian norm), and it is not known if they can be relaxed to expected smoothness (Gower et al., 2019;
Khaled and Richta´rik, 2020) or some variant of it. We leave these questions to future work.
Acknowledgements
We thank Aaron Defazio, Yair Carmon, and Oliver Hinder for discussions during the preparation of this
work.
References
YossiArjevani, YairCarmon,JohnC.Duchi, DylanJ.Foster,NathanSrebro,andBlakeWoodworth. Lower
bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, abs/1912.02365, 2019.
URL https://arXiv.org/abs/1912.02365. (Citedonpage9).
AmitAttiaandTomerKoren.SGDwithAdaGradstepsizes: Fulladaptivitywithhighprobabilitytounknown
parameters, unbounded gradients and affine variance. arXiv preprint arXiv:2302.08783, abs/2302.08783,
2023. URL https://arXiv.org/abs/2302.08783. (Citedonpage26).
Atilim Gunes Baydin, Robert Cornish, David Mart´ınez-Rubio, Mark Schmidt, and Frank Wood. Online
learning rate adaptation with hypergradient descent. In 6th International Conference on Learning Repre-
sentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=BkrsAzWAb. (Citedonpage4).
11Amir Beck. First-order methods in optimization. MOS-SIAM series on optimization. Society for Industrial
and Applied Mathematics ; Mathematical Optimization Society, 2017. ISBN 9781611974997. (Cited on
page4).
Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas,
Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, Difan Deng, and Marius Lindauer. Hyperpa-
rameter optimization: Foundations, algorithms, best practices, and open challenges. WIREs Data Min-
ing and Knowledge Discovery, 13(2), January 2023. ISSN 1942-4795. doi: 10.1002/widm.1484. URL
http://dx.doi.org/10.1002/widm.1484. (Citedonpage4).
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Puro-
hit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: an open-
source autoregressive language model. arXiv preprint arXiv:2204.06745, abs/2204.06745, 2022. URL
https://arXiv.org/abs/2204.06745. (Citedonpage1).
L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale ma-
chine learning. SIAM Review, 60(2):223–311, 2018. doi: 10.1137/16M1080173. URL
https://doi.org/10.1137/16M1080173. (Citedonpage2).
TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
ChristopherBerner,SamMcCandlish,Alec Radford,IlyaSutskever,andDarioAmodei. Languagemodels
are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL
https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
(Citedonpage1).
Yair Carmon and Oliver Hinder. Making SGD parameter-free. In Po-Ling Loh and
Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, vol-
ume 178 of Proceedings of Machine Learning Research, pages 2360–2389. PMLR, 2022. URL
https://proceedings.mlr.press/v178/carmon22a.html. (Citedonpages2,4,6,and7).
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding station-
ary points II: First-order methods. arXiv preprint arXiv:1711.00841, abs/1711.00841, 2017. URL
https://arXiv.org/abs/1711.00841. (Citedonpage6).
Xinyi Chen and Elad Hazan. A nonstochastic control approach to optimization. arXiv preprint
arXiv:2301.07902, abs/2301.07902,2023. URL https://arXiv.org/abs/2301.07902. (Citedonpage4).
Ashok Cutkosky. Artificial constraints and Lipschitz hints for unconstrained online learning. arXiv preprint
arXiv:1902.09013, abs/1902.09013, 2019. URL https://arXiv.org/abs/1902.09013. (Cited on pages 2, 3,
4,5,and18).
Ashok Cutkosky and Kwabena A. Boahen. Online convex optimization with unconstrained domains
and losses. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 748–756,2016. URL
https://proceedings.neurips.cc/paper/2016/hash/550a141f12de6341fba65b0ad0433500-Abstract.html.
(Citedonpages2,3,and6).
Ashok Cutkosky and Kwabena A. Boahen. Stochastic and adversarial online learning with-
out hyperparameters. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wal-
lach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 30: Annual Conference on Neural Information Processing
12Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5059–5067, 2017a. URL
https://proceedings.neurips.cc/paper/2017/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html.
(Citedonpage4).
Ashok Cutkosky and Kwabena A. Boahen. Online learning without prior information. In Satyen Kale and
Ohad Shamir, editors, Proceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam,
The Netherlands, 7-10 July 2017, volume 65 of Proceedings of Machine Learning Research, pages 643–677.
PMLR, 2017b. URL http://proceedings.mlr.press/v65/cutkosky17a.html. (Citedonpages 3and6).
Ashok Cutkosky, Aaron Defazio, and Harsh Mehta. Mechanic: a learning rate tuner. arXiv preprint
arXiv:2306.00144, abs/2306.00144,2023. URL https://arXiv.org/abs/2306.00144. (Citedonpage2).
Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by D-adaptation. arXiv preprint
arXiv:2301.07733, abs/2301.07733, 2023. URL https://arXiv.org/abs/2301.07733. (Cited on pages 4
and7).
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Adam Tauman Kalai and Mehryar Mohri, editors, COLT 2010 - The 23rd
Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 257–269. Omnipress, 2010. URL
http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265. (Citedonpage2).
Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai, and Rachel
Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded gradients and affine vari-
ance. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022,
London, UK,volume178ofProceedings of Machine Learning Research,pages313–355.PMLR,2022. URL
https://proceedings.mlr.press/v178/faw22a.html. (Citedonpage11).
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. doi: 10.1137/120880811. URL
https://doi.org/10.1137/120880811. (Citedonpages 3,11,and40).
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richta´rik. SGD: General analysis and improved rates. In Kamalika Chaudhuri and Ruslan
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, vol-
ume 97 of Proceedings of Machine Learning Research, pages 5200–5209. PMLR, 2019. URL
http://proceedings.mlr.press/v97/qian19b.html. (Citedonpage11).
EladHazan. Introduction to Online Convex Optimization. Bookdraft, MIT UniversityPress,2022. (Citedon
page4).
Elad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv preprint arXiv:1905.00313,
abs/1905.00313,2019. URL https://arXiv.org/abs/1905.00313. (Citedonpages4,5,6,and26).
Maor Ivgi, Oliver Hinder, and Yair Carmon. DoG is SGD’s best friend: a parameter-free
dynamic step size schedule. arXiv preprint arXiv:2302.12022, abs/2302.12022, 2023. URL
https://arXiv.org/abs/2302.12022. (Citedonpages2,4,5,6,7,18,19,20,36,and37).
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of SGD information theo-
retically optimal. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, COLT
2019, 25-28 June2019, Phoenix, AZ, USA,volume99ofProceedings of Machine Learning Research,pages
1752–1755.PMLR, 2019. URL http://proceedings.mlr.press/v99/jain19a.html. (Citedonpage4).
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. A short note on con-
centration inequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736,
abs/1902.03736,2019. URL https://arXiv.org/abs/1902.03736. (Citedonpages33and40).
Ali Kavis, Kfir Y. Levy, Francis R. Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algo-
rithm with optimal guarantees for constrained optimization. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances
13in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pages6257–6266,2019. URL
https://proceedings.neurips.cc/paper/2019/hash/88855547570f7ff053fff7c54e5148cc-Abstract.html.
(Citedonpage2).
AliKavis,KfirYehudaLevy,andVolkanCevher. Highprobabilityboundsforaclassofnonconvexalgorithms
with adagradstepsize. In ICLR. OpenReview.net, 2022. (Citedonpage11).
Ahmed Khaled and Peter Richta´rik. Better theory for SGD in the nonconvex world. arXiv preprint
arXiv:2002.03329, abs/2002.03329,2020. URL https://arXiv.org/abs/2002.03329. (Citedonpage11).
Ahmed Khaled, Konstantin Mishchenko, and Chi Jin. DoWG unleashed: an efficient universal
parameter-free gradient descent method. arXiv preprint arXiv:2305.16284, abs/2305.16284, 2023. URL
https://arXiv.org/abs/2305.16284. (Citedonpages2,5,6,19,and26).
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and
Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arXiv.org/abs/1412.6980.
(Citedonpage1).
Guanghui Lan, Yuyuan Ouyang, and Zhe Zhang. Optimal and parameter-free gradient minimization
methods for smooth optimization. arXiv preprint arXiv:2310.12139, abs/2310.12139, 2023. URL
https://arXiv.org/abs/2310.12139. (Citedonpage4).
Yunwen Lei and Ke Tang. Learning rates for stochastic gradient descent with nonconvex objectives. IEEE
Transactions on Pattern Analysis and Machine Intelligence,43(12):4505–4511,December2021. ISSN1939-
3539. doi: 10.1109/tpami.2021.3068154.URLhttp://dx.doi.org/10.1109/TPAMI.2021.3068154. (Cited
onpage11).
KfirY.Levy.Onlinetoofflineconversions,universalityandadaptiveminibatchsizes.InIsabelleGuyon,Ulrike
vonLuxburg,Samy Bengio,Hanna M. Wallach,RobFergus,S.V. N. Vishwanathan,andRomanGarnett,
editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Infor-
mation ProcessingSystems2017, December4-9, 2017, LongBeach, CA,USA,pages1613–1622,2017.URL
https://proceedings.neurips.cc/paper/2017/hash/ce5140df15d046a66883807d18d0264b-Abstract.html.
(Citedonpages2,5,and18).
Kfir Yehuda Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, univer-
sality and acceleration. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen
Grauman, Nicolo` Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Informa-
tion Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages 6501–6510, 2018. URL
https://proceedings.neurips.cc/paper/2018/hash/b0169350cd35566c47ba83c6ec1d6f82-Abstract.html.
(Citedonpage2).
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adap-
tive stepsizes. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Con-
ference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa,
Japan, volume 89 of Proceedings of Machine Learning Research, pages 983–992. PMLR, 2019. URL
http://proceedings.mlr.press/v89/li19c.html. (Citedonpages 2and11).
Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum. arXiv
preprint arXiv:2007.14294, abs/2007.14294, 2020. URL https://arXiv.org/abs/2007.14294. (Cited on
page11).
Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Lˆe Nguyen. High probability con-
vergence of stochastic gradient methods. arXiv preprint arXiv:2302.14843, abs/2302.14843, 2023. URL
https://arXiv.org/abs/2302.14843. (Citedonpages3,4,5,9,38,and39).
14Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
URL https://openreview.net/forum?id=Bkg6RiCqY7. (Citedonpage1).
Liam Madden, Emiliano Dall’Anese, and Stephen Becker. High-probability convergence bounds for non-
convex stochastic gradient descent. arXiv preprint arXiv:2006.05610, abs/2006.05610, 2020. URL
https://arXiv.org/abs/2006.05610. (Citedonpages11,39,and40).
Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. In Proceed-
ings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
Event, volume 119 of Proceedings of Machine Learning Research, pages 6702–6712. PMLR, 2020. URL
http://proceedings.mlr.press/v119/malitsky20a.html. (Citedonpage4).
ShieMannorandJohnN.Tsitsiklis.Thesamplecomplexityofexplorationinthemulti-armedbanditproblem.
J. Mach. Learn. Res., 5:623–648,2004. ISSN 1532-4435. (Citedonpage6).
ZakariaMhammediandWouterM.Koolen. Lipschitzandcomparator-normadaptivityinonlinelearning. In
JacobD.AbernethyandShivaniAgarwal,editors,Conference on Learning Theory, COLT 2020, 9-12 July
2020, Virtual Event [Graz, Austria],volume125ofProceedings of Machine Learning Research,pages2858–
2887. PMLR, 2020. URL http://proceedings.mlr.press/v125/mhammedi20a.html. (Cited on pages 2, 3,
4,and7).
Zakaria Mhammedi, Wouter M. Koolen, and Tim van Erven. Lipschitz adaptivity with multiple learning
rates in online learning. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory,
COLT2019, 25-28 June2019, Phoenix, AZ,USA,volume99ofProceedings of Machine Learning Research,
pages2490–2511.PMLR,2019. URLhttp://proceedings.mlr.press/v99/mhammedi19a.html. (Citedon
pages2,3,and4).
Yu Nesterov. Universal gradient methods for convex optimization problems. Mathe-
matical Programming, 152(1-2):381–404, 2014. doi: 10.1007/s10107-014-0790-0. URL
https://doi.org/10.1007/s10107-014-0790-0. (Citedonpage4).
Yurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd edition,
2018. ISBN 3319915770. (Citedonpage6).
Francesco Orabona. Normalized gradients for all. arXiv preprint arXiv:2308.05621, abs/2308.05621, 2023.
URL https://arXiv.org/abs/2308.05621. (Citedonpages3,4,and6).
Francesco Orabona and Ashok Cutkosky. ICML 2020 tutorial on parameter-free online optimization. ICML
Tutorials, 2020. URL https://parameterfree.com/icml-tutorial/. (Citedonpages2and3).
Francesco Orabona and D´avid Pa´l. Coin betting and parameter-free online learning. In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 577–585, 2016. URL
https://proceedings.neurips.cc/paper/2016/hash/320722549d1751cf3f247855f937b982-Abstract.html.
(Citedonpage2).
B. T. Polyak. Introduction to Optimization. Translations Series in Mathematics and Engineering. Optimiza-
tion Software Inc., New York, 1987. (Citedonpage5).
Prabhu Teja Sivaprasad, Florian Mai, Thijs Vogels, Martin Jaggi, and Franc¸ois Fleuret. Opti-
mizer benchmarking needs to account for hyperparameter tuning. In Proceedings of the 37th In-
ternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-
ume 119 of Proceedings of Machine Learning Research, pages 9036–9045. PMLR, 2020. URL
http://proceedings.mlr.press/v119/sivaprasad20a.html. (Citedonpage1).
15Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,
BaptisteRozi`ere,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, abs/2302.13971,2023a. URL https://arXiv.org/abs/2302.13971. (Citedonpage1).
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,
Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,
Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xi-
aoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,Robert
Stojnic, Sergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023b. URL http://arxiv.org/abs/2307.09288v2. (Citedonpage1).
Roman Vershynin. High-dimensional probability: An introduction with applications in data science. 2018.
doi: 10.1017/9781108231596. (Citedonpages31and33).
Blake E. Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives.
In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,
editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Infor-
mation Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3639–3647, 2016. URL
https://proceedings.neurips.cc/paper/2016/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html.
(Citedonpage6).
BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel
Hesslow, Roman Castagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, Jonathan Tow,
Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang,
Benoˆıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas
Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz
Suarez, Victor Sanh, Hugo Laurenc¸on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz´alez Ponferrada, Efrat Levkovizh, Ethan Kim,
Eyal Bar Natan, Francesco De Toni, G´erard Dupont, Germ´an Kruszewski, Giada Pistilli, Hady Elsa-
har, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,
Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J¨org Frohberg, Joseph Tob-
ing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber,
Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mun˜oz, Maraim Masoud,
Mar´ıa Grandury, Mario Saˇsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,
Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla
Khamis,Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas,Peter Henderson, PierreColombo,
Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis Lo´pez, Rui Ribeiro,
Salomey Osei, Sampo Pyysalo,Sebastian Nagel, Shamik Bose,Shamsuddeen HassanMuhammad, Shanya
Sharma,ShayneLongpre,SomaiehNikpoor,StanislavSilberberg,SuhasPai,SydneyZink,TiagoTimponi
Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Vio-
lette Lepercq,Vrinda Prabhu,ZaidAlyafeai, ZeerakTalat,Arun Raja,Benjamin Heinzerling,Chenglei Si,
Davut Emre Ta¸sar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea San-
tilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang,
Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful
Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen,
Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish
Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar
16Tojarieh,AdamRoberts,HyungWonChung,JaesungTae,JasonPhang,OfirPress,ConglongLi,Deepak
Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero,Patrick von
Platen, Pierre Cornette, Pierre Franc¸ois Lavall´ee, R´emi Lacroix, Samyam Rajbhandari, Sanchit Gandhi,
ShadenSmith, St´ephaneRequena,SurajPatil,TimDettmers, AhmedBaruwa,AmanpreetSingh,Anasta-
sia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur´elie N´ev´eol, Charles Lovering, Dan Garrette,
Deepak Tunuguntla,EhudReiter, EkaterinaTaktasheva,EkaterinaVoloshina,EliBogdanov,Genta Indra
Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive,
Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton
Cheng,OlegSerikov,OmerAntverg,OskarvanderWal,RuiZhang,RuochenZhang,SebastianGehrmann,
Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena
Rieser,VitalyProtasov,VladislavMikhailov,YadaPruksachatkun,YonatanBelinkov,ZacharyBamberger,
Zdenˇek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana San-
tos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh
HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mun˜oz Ferrandis, Daniel Mc-
Duff, Danish Contractor,David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi
Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani
Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis
Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akin-
lolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani,
Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo
Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, An-
tonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Cl´ementine Fourrier, Daniel Le´on Perin˜a´n, Daniel Molano, Dian Yu, Enrique Manjava-
cas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec,
Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai
Sivaraman, LokeshBulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz,Maiko Takeuchi,
Marc Pa`mies, Maria A Castillo, Marianna Nezhurina, Mario Sa¨nger, Matthias Samwald, Michael Cul-
lan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang,
Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick
Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi
Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon
Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th´eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu,
Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.
BLOOM: a 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
abs/2211.05100,2022. URL https://arXiv.org/abs/2211.05100. (Citedonpage1).
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub
Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter
transfer. InA. Beygelzimer,Y.Dauphin, P.Liang,andJ.WortmanVaughan,editors, Advances in Neural
Information Processing Systems,2021. URLhttps://openreview.net/forum?id=Bx6qKuBM2AD. (Citedon
page1).
17Appendix
7 Proofs for Section 7
Proposition 1. Coin betting through Online Newton Steps with Hints (Cutkosky, 2019, Algorithm 1) is
tuning-free in the bounded setting.
Proof. Inthe boundedsetting, Cutkosky(2019)giveanalgorithmthattakesasparametersǫ,αandachieves
the following regret
T −1
g t,x
t
x ǫ+GD+ x x
0
Glog
kx ∗−x 0 kGexp(α/4G2)
1+
T t=− 01 kg t k2 4.5
t=0 h − ∗i≤ k ∗− k  ǫ P α ! 
X
 
10
+ x x
vT −1
g 2log
T t=− 01 kg t k2 exp(α/2G2) kx ∗k2
+1 .
k ∗− 0 ku ut=0 k t k (cid:16) P (cid:17) ǫ2 
uX
u  
t  
If we set ǫ=D G, α=G2, and use the upper bound x x D and simplify we get the regret
0
· k − ∗k≤
T −1
g ,x x GD+GD+DGlog
DG
1+
G2T 4.5
+D
T −1
g 2
logT10G20D2
t=0 h t t − ∗i≤ "DG (cid:18) G2 (cid:19) # v ut=0k t k s D2G2
X uX
t
Observe that because G G and D D the above can be simplified to
≤ ≤
T −1
g ,x x GDlog
DG
1+
G2T 4.5
+D
T −1
g 2
logT10G20D2
t=0 h t t − ∗i≤ + "DG (cid:18) G2 (cid:19) # v ut=0 k t k s D2G2
X uX
t
Call the maximum of the two log terms ι, then the above rate is
T 1 T 1
− g ,x x GDι+D − g 2√ι. (12)
t=0
h t t − ∗i≤ v
ut=0
k t k
X uX
t
Applying online-to-batchconversionstarting from equation(12) proves the algorithmis tuning-free. For the
smooth setting, it suffices to observe that under a bounded domain we have for any t
g g f(x ) + f(x )
t t t t
k k≤k −∇ k k∇ k
= g f(x ) + f(x ) f(x )
t t t
k −∇ k k∇ −∇ ∗ k
σ+L x x
t
≤ k − ∗k
σ+LD.
≤
Combining this and following online-to-batch conversion as in (Levy, 2017) shows the algorithm considered
is tuning-free in the smooth setting as well.
We will make use of the following two lemmas throughout the upper bound proofs for DoG and DoWG.
Lemma 1. (Ivgi et al., 2023, Lemma 7). Let S be the set of nonnegative and nondecreasing sequences. Let
C and let X be a martingale difference sequence adapted to such that X C with probability
t t 1 t t t t
∈F − F | |≤
1 for all t. Then for all δ (0,1) and Xˆ such that Xˆ C with probability 1, we have that with
t t 1 t t
∈ ∈ F − ≤
probability 1 δ that for all c>0 (cid:12) (cid:12)
− (cid:12) (cid:12)
(cid:12) (cid:12)
t t
y X 8y θ (X Xˆ )2+[c]θ2 +Prob( t T C >c)
(cid:12) (cid:12)Xi=1 i i (cid:12) (cid:12)≤ tv u
u
t,δ Xi=1 i − i t,δ ∃ ≤ | t
(cid:12) (cid:12) t
(cid:12) (cid:12)
(cid:12) (cid:12)
18Lemma 2. (Ivgi et al., 2023, Lemma 3). Let s ,s ,...,s be a positive increasing sequence. Then,
0 1 T
t 1
− s i 1 T
max 1 .
t ≤T Xi=0 s t ≥ e log + s sT 0 − !
Lemma3. (Ivgi et al.,2023,Lemma1). Supposethatf isconvexandhas aminimizer x . Thentheiterates
∗
generated by DoG satisfy for each t:
b 1
−
r
k
g k,x
k
x r b(2d b+r b)√u
b
1.
h − ∗i≤ −
k=a
X
Lemma 4. Suppose that f is convex and has a minimizer x . Then iterates generated by DoWG satisfy for
∗
every t:
t 1 t 1
− −
r2 kh∇f(x k),x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1+ r2 kh∇f(x k) −g k,x
k
−x
∗i
k=0 k=0
X (cid:2) (cid:3) X
Proof. This is a modification of (Khaled et al., 2023, Lemma 3) to account for the case where g = f(x )
k k
6 ∇
(i.e. when the gradients used are not deterministic), following (Ivgi et al., 2023, Lemma 1). We start
d2 x η g x 2
k+1 ≤k k − k k − ∗k
= x x 2+η2 g 2 2η g ,x x .
k k − ∗k kk k k − k h k k − ∗i
Rearranging we get
2η g ,x x d2 d2 +η2 g 2
k h k k − ∗i≤ k− k+1 kk k k
Multiplying both sides by
r2
k we get
2ηk
1r2 r2η
r2 g ,x x k d2 d2 + k k g 2.
kh k k − ∗i≤ 2η
k
k− k+1 2 k k k
(cid:0) (cid:1)
We then follow the same proof as in (Khaled et al., 2023, Lemma 3) to get
t 1
−
r2 khg k,x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1. (13)
k=0
X (cid:2) (cid:3)
We then decompose
t 1 t 1 t 1
− − −
r2 g ,x x = r2 g f(x ),x x + r2 f(x ),x x .
kh k k − ∗i kh k −∇ k k − ∗i kh∇ k k − ∗i
k=0 k=0 k=0
X X X
Plugging back into equation (13) we get
t 1 t 1
− −
r2 kh∇f(x k),x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1+ r2 kh∇f(x k) −g k,x
k
−x
∗i
(14)
k=0 k=0
X (cid:2) (cid:3) X
7.1 Proof of Theorem 1
Theorem 1. DoG and DoWG are tuning-free in the bounded setting. That is, there exists some ι =
polylog(D,σ,T,δ 1) such that
D σ −
DL2 + σD if f is L-smooth,
f(x ) f ι T √T
out − ∗ ≤ ·(√G2+σ2D if f is G-Lipschitz.
√T
This rate is achieved simultaneously for both classes of functions without prior knowledge of whether f is
smooth or Lipschitz (and thus no usage of the hints L,L,G,G).
19Proof of Theorem 1. We first handle the case that T < 4log D. In this case we just return x . If f is
+ D 0
G-Lipschitz, then by convexity we have
2GD D
f(x ) f f(x ),x x f(x ) x x GD log .
0 − ∗ ≤h∇ 0 0 − ∗i≤k∇ 0 kk 0 − ∗k≤ ≤ √T s + D
If f is L-smooth, then by smoothness we have
L LD2 2LD2 D
f(x ) f x x 2 log .
0 − ∗ ≤ 2k 0 − ∗k ≤ 2 ≤ T + D
Therefore in both cases the point we return achieves a small enough loss almost surely. Throughoutthe rest
of the proof, we shall assume that T 4log D.
≥ + D
Part 1: DoG. In the nonsmooth setting, this is a straightforward consequence of (Ivgi et al., 2023,
Proposition 3). In particular, when using DoG with r = D, then Corollary 1 in their work gives that with
ǫ
probability 1 δ there exists some τ [T] and some absolute constant c>0 such that
− ∈
DG 60log6t 2D
f(x ) f c log log ,
τ
− ∗ ≤ · √T δ D
where xˆ t d =ef Pt i=−1 01ri t i=− 01r ix i.
For the smooth setting, we start with Lemma 3 to get
P
t 1 t 1
− −
r
k
f(x k),x
k
x r
t
2d t+r
t
√u
t
1+ r
k
f(x k) g k,x
k
x . (15)
h∇ − ∗i≤ − h∇ − − ∗i
k=0 k=0
X (cid:0) (cid:1) X
Wefollow(Ivgi et al.,2023,Proposition3)andmodifytheproofinastraightforwardmannertoaccommodate
the assumption of bounded noise (rather than bounded gradients). Define
def
τ =min min i r 2r ,T , τ = 0.
k
{ { |
i
≥
τi −1
} }
0
We denote by K the first index such that τ =T. Define
K
x x
X k = g k f(x k), k − ∗ , Xˆ k =0, y k =r kd k.
−∇ d
(cid:28) k (cid:29)
Observethatx isdeterminedby ,andsincer =max ( x x ,r ),itisalsodeterminedby .
k k 1 k t k k 0 ǫ k 1
F − ≤ k − k F −
Therefore
x x
E[X k |Fk −1]=r2 k (cid:28)E[g k −∇f(x k)], k d−
k
∗ (cid:29)=0.
Moreover,observe that
x x
k
X
k
g
k
f(x k) k − ∗k σ.
| |≤k −∇ k d ≤
k
Therefore the X form a martingale. Then we can apply Lemma 1 to get that with probability 1 δ that
k
−
for every t [K]
∈
t 1 t 1
− −
r g f(x ),x x 8d r θ (X )2+σ2
(cid:12) (cid:12)Xk=0
k
h
k
−∇
k k
− ∗i(cid:12) (cid:12)≤
t t t,δv
u uXk=0
k
(cid:12) (cid:12) t
(cid:12) (cid:12) t 1
(cid:12) (cid:12) 8d r θ − g f(x ) 2+σ2
≤
t t t,δv
k
k
−∇
k
k
uk=0
uX
t
8d r θ σ2t+σ2
t t t,δ
≤
16d r θ pσ√T. (16)
t t t,δ
≤
20Now observe that we can use equation (16) to get
τi −1 τi −1 τi −1−1
r g f(x ),x x r g f(x ),x x + r g f(x ),x x
k k k k k k k k k k k k
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)k= Xτi −1 h −∇ − ∗i(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≤≤ 1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)k X 6= d0 τir τih θ t,δσ− √∇ T +16d τi −− 1r τi∗ −i 1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)θ t,δ(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)σ√k X= T0 h −∇ − ∗i(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
32d r θ σ√T. (17)
≤
τi τi t,δ
Now observe that by convexity we have for k τ ,τ +1,...,τ 1
i 1 i 1 i
∈{ − − − }
r
k
0 f(x ) f f(x ),x x f(x ),x x .
k k k k k
≤ − ∗ ≤h∇ − ∗i≤ r τi 1 h∇ − ∗i
−
Summing up from k =τ to k =τ 1 we get
i 1 i
− −
τi −1
1
τi −1
f(x ),x x r f(x ),x x
k k k k k
k= Xτi
−1h∇ − ∗i≤ r
τi −1 k= Xτi −1
h∇ − ∗i
1
τi −1
= r f(x ),x x
k k k
r
τi −1 k= Xτi −1
h∇ − ∗i
1
τi −1
1
τi −1
= r f(x ) g ,x x + r g ,x x . (18)
k k k k k k k
r
τi −1 k= Xτi −1
h∇ − − ∗i r
τi −1 k= Xτi −1
h − ∗i
We now use Lemma 3 to get that
τi 1
−
r
k
hg k,x
k
−x ∗i≤2r
τi
d
τi
+r
τi
√u
τi
−1. (19)
k= Xτi −1 (cid:0) (cid:1)
Plugging in the upper bounds from equations (17) and (19) into equation (18) we get
k=τ Xi τ−
i
−1 1h∇f(x k),x
k
−x
∗i≤
rr τiτ −i
1
h2 (cid:0)d
τi
+r
τi
(cid:1)√u
τi
−1+32d τiθ t,δσ√T i. (20)
Now observe that
g
k
r r + x x =r 1+ k k 2r .
k+1 k t+1 t k k
≤ k − k
(cid:18)
√u k(cid:19)≤
It follows that rτr iτ −i
1
≤2. Moreoverby the definition of the τ i we have that r rτ τi
i
−− 11 ≤2. Therefore
r r r
τi
=
τi τi −1
2 2=4. (21)
r r r ≤ ·
τi −1 τi −1 τi −1
using equation (21) in equation (20) we get
τi 1
−
h∇f(x k),x
k
−x ∗i≤4 2 d
τi
+r
τi
√u
τi
−1+32d τiθ t,δσ√T .
k= Xτi −1 h (cid:0) (cid:1) i
Summing up over the i, we get
T 1 K τi 1
− −
h∇f(x t),x
t
−x
∗i≤
h∇f(x k),x
k
−x ∗i≤4K 2 d
τi
+r
τi
√u
τi
−1+32Kd τiθ t,δσ√T .
Xt=0 Xi=0k= Xτi −1 h (cid:0) (cid:1) i
21Observe that by definition we have
r 2r
T T
K 1+log =log .
≤ r r
0 0
Therefore using the last equation and convexity we have
T 1
− (f(x t) f ) 4log2r T 2 d
T
+r
T
√u
T
1+32d Tθ T,δσ√T .
− ∗ ≤ r 0 −
Xt=0 h (cid:0) (cid:1) i
Note that because the domain is bounded we have max(r ,d ) D, and we used r =D, therefore
T T 0
≤
T 1
− 2D
(f(x t) f ) 4log 4D√u
T
1+32Dθ T,δσ√T . (22)
− ∗ ≤ D −
Xt=0 h i
Observe that by our assumption on the noise and smoothness we have
T 1
u = − g 2
T 1 k
− k k
k=0
X
T 1 T 1
2 − g f(x ) 2+2 − f(x ) 2
k k k
≤ k −∇ k k∇ k
k=0 k=0
X X
T 1
2Tσ2+2 − f(x ) 2
k
≤ k∇ k
k=0
X
T 1
−
2Tσ2+4L (f(x ) f ).
k
≤ − ∗
k=0
X
Using this in equation (22) gives
T 1 T 1
− 2D −
(f(x ) f ) 4log 8Dσ√T +2√LD (f(x ) f )+32Dθ σ√T
t=0 t − ∗ ≤ D  v ut=0 t − ∗ T,δ 
X uX
 t 
T 1
2D − 2D
8log D√L (f(x ) f )+160log θ σD√T. (23)
≤ D v ut=0 t − ∗ D T,δ
uX
t
Observe that if y2 ay+b, then by the quadratic equation and the triangle inequality we have
≤
a+√a2+4b
y .
≤ 2
Squaring both sides gives
1 1
y2 (a+ a2+4b)2 2a2+4b =a2+2b. (24)
≤ 4 ≤ 2
p (cid:0) (cid:1)
Applying this to equation (23) with the following choices
T 1
− 2D 2D
y = f(x ) f , a=8log D√L, b=160log θ σD√T,
v ut=0 t − ∗ D D T,δ
uX
t
then we obtain
T 1
− 2D 2D
(f(x ) f ) 64log2 LD2+320log2 θ σD√T.
t T,δ
− ∗ ≤ D D
t=0
X
22Dividing both sides by T and using Jensen’s inequality we finally get
T 1
1 −
f(xˆ ) f (f(x ) f )
t t
− ∗ ≤ T − ∗
t=0
X
2DLD2 2D σD
64log2 +320log2 θ .
T,δ
≤ D T D √T
This shows DoG is tuning-free in this setting.
Plugging back into equation (15) we get with probability 1 δ that
−
t 1
−
r
k
f(x k),x
k
x r
t
2d t+r
t
√u
t
1+16d tr tθ t,δσ√T.
h∇ − ∗i≤ −
k=0
X (cid:0) (cid:1)
Now we can divide both sides by r to get
t
t 1
− r k f(x k),x
k
x 2d t+r
t
√u
t
1+16d tθ t,δσ√T.
r t h∇ − ∗i≤ −
k=0
X (cid:0) (cid:1)
Part 2: DoWG. By Lemma 4 we have that our iterates satisfy
t 1 t 1
− −
r2 kh∇f(x k),x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1+ r2 kh∇f(x k) −g k,x
k
−x
∗i
k=0 k=0
X (cid:2) (cid:3) X
Define
x x
X k = g k −∇f(x k), k d− ∗ , Xˆ k =0, y k =r2 kd k.
(cid:28) k (cid:29)
Observethatx isdeterminedby ,andsincer =max ( x x ,r ),itisalsodeterminedby .
k k 1 k t k k 0 ǫ k 1
F − ≤ k − k F −
Therefore
x x
E[X k |Fk −1]=r2 k (cid:28)E[g k −∇f(x k)], k d−
k
∗ (cid:29)=0.
Moreover,observe that
x x
k
X
k
g
k
f(x k) k − ∗k σ.
| |≤k −∇ k d ≤
k
Therefore the X form a martingale. Then we can apply Lemma 1 to get that with probability 1 δ that
k
−
for every t [T]
∈
t 1 t 1
− −
r2 g f(x ),x x 8d r2θ (X )2+σ2
(cid:12) (cid:12)Xk=0 kh k −∇ k k − ∗i(cid:12) (cid:12)≤ t t t,δv u uXk=0 k
(cid:12) (cid:12) t
(cid:12) (cid:12) t 1
(cid:12) (cid:12) 8d r2θ − g f(x ) 2+σ2
≤ t t t,δv k k −∇ k k
uk=0
uX
t
8d r2θ σ2t+σ2
≤ t t t,δ
16d r2θ pσ√T.
≤ t t t,δ
Plugging this back into equation (14) we get
t 1
−
r2 kh∇f(x k),x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1+16d tr2 tθ t,δσ√T. (25)
k=0
X (cid:2) (cid:3)
We now divide the proof in two cases:
23• If f is G-Lipschitz: then σ =sup x ∈Rd k∇f(x) −g(x) k≤2G and therefore equation (25) reduces to
t 1
−
r2 kh∇f(x k),x
k
−x ∗i≤2r
t
d t+r
t
√v
t
−1+32d tr2 tθ t,δG√T.
k=0
X (cid:2) (cid:3)
And we have
t 1
v = − r2 g 2 r2G2T.
t −1 kk k k ≤ t
k=0
X
Therefore
t 1
−
r2 f(x ),x x 2r2 d +r G√T +32d r2θ G√T
kh∇ k k − ∗i≤ t t t t t t,δ
k=0
X (cid:2) (cid:3)
34r2 d +r θ G√T
≤ t t t t,δ
≤68r2 tD(cid:2)G√Tθ(cid:3)t,δ.
Using convexity we have
t 1 t 1
− −
r2(f(x ) f ) r2 f(x ),x x 68r2DG√Tθ .
k k − ∗ ≤ kh∇ k k − ∗i≤ t t,δ
k=0 k=0
X X
Dividing both sides by t k− =1 0r2 k and using Jensen’s inequality we get
P t 1
1 −
f(x˜ ) f r2(f(x ) f )
t − ∗ ≤ t k− =1 0r2 k k=0 k k − ∗
X
P
r2
t 68DG√Tθ . (26)
≤ t
k−
=1 0r2
k
t,δ
We now use Lemma 2 to conclude that therePexists some t T such that
≤
r2 e
t (27)
Pt
k−
=1 0r2
k
≤
(cid:18)2logT
+
r rk
ǫ
−1
(cid:19)
Note that by the fact that r D, r = D, and that we assume T 4log D (see the beginning of
T ≤ 0 ≥ + D
this proof) we have
T T T
1 1 .
2log rT − ≥ 2log D − ≥ 4log D
+ r0 + D + D
Plugging this into equation (27) we get
r2 4e D 11 D
t log log .
t
k−
=1 0r2
k
≤ T + D ≤ T + D
Using this in conjunction with eqPuation (26) we thus have that for some t T
≤
748DGθ D
T,δ
f(x˜ ) f log .
t − ∗ ≤ √T + D
24• Iff isL-smooth: Observethatbystraightforwardalgebra,ourassumptiononthenoise,andsmoothness
t 1
v = − r2 g 2
t −1 kk k k
k=0
X
t 1 t 1
2 − r2 g f(x ) 2+2 − r2 f(x ) 2
≤ kk k −∇ k k kk∇ k k
k=0 k=0
X X
t 1
2r2σ2T +2 − r2 f(x ) 2
≤ t kk∇ k k
k=0
X
t 1
−
2r2σ2T +4L r2(f(x ) f ).
≤ t k k − ∗
k=0
X
Using the last line estimate in equation (25) with the triangle inequality we get
t 1 t 1
− −
r2 f(x ),x x 4r d +r r σ√T +√L r2(f(x ) f ) +16d r2θ σ√T.
kh∇ k k − ∗i≤ t t t  t v k k − ∗  t t t,δ
k=0 uk=0
X (cid:2) (cid:3) uX
 t 
By convexity we have
f(x ),x x f(x ) f .
k k k
h∇ − ∗i≥ − ∗
Therefore
t 1 t 1
− −
r2(f(x ) f ) 4r d +r √L r2(f(x ) f )+20r2θ d +r σ√T. (28)
k k − ∗ ≤ t t t v k k − ∗ t t,δ t t
k=0 uk=0
X (cid:2) (cid:3) uX (cid:2) (cid:3)
t
Observe that if y2 ay+b, then we have shown in equation (24) that y2 a2+2b. Applying this to
≤ ≤
equation (28) with a=4r d +r √L and b=20r2θ d +r σ√T gives
t t t t t,δ t t
t 1 (cid:2) (cid:3) (cid:2) (cid:3)
− r2(f(x ) f ) 16r2 d +r 2 L+40r2θ d +r σ√T
k k − ∗ ≤ t t t t t,δ t t
k=0
X (cid:2) (cid:3) (cid:2) (cid:3)
=r2 16 d +r 2 L+40θ d +r σ√T .
t t t t,δ t t
(cid:16) (cid:17)
Dividing both sides by t k− =1 0r2 k and using Jens(cid:2) en’s ine(cid:3) quality we g(cid:2) et (cid:3)
f(xˆ ) f 1 P t −1 r2(f(x ) f ) r2 t 16 d +r 2 L+40θ d +r σ√T ,
t − ∗ ≤ t k− =1 0r2 k Xk=0 k k − ∗ ≤ t k− =1 0r2 k (cid:16) (cid:2) t t (cid:3) t,δ (cid:2) t t (cid:3) (cid:17)
where xˆ t = Pt k−=1 1 0rP 2 k t k− =1 0r2 kx k. We now use LemPma 2 to conclude that there exists some τ ≤ T such
that
P
f(xˆ ) f e 16 d +r 2 L+40θ d +r σ√T .
τ t t τ,δ t t
− ∗ ≤ (cid:18)2logT
+
r rk
ǫ
−1 (cid:19)(cid:16) (cid:2) (cid:3) (cid:2) (cid:3) (cid:17)
By assumption on T we have T 1 T , therefore
2log+ D D − ≥ 4log+ D D
4elog D
f(xˆ ) f + D 16 d +r 2 L+40θ d +r σ√T
τ t t τ,δ t t
− ∗ ≤ T
(cid:16) D(cid:2) LD(cid:3)2 σD (cid:2) (cid:3) (cid:17)
700θ log + ,
≤ T,δ + D · T √T
(cid:18) (cid:19)
where in the last line we used that max(d ,r ) D.
t t
≤
258 Proofs for Section 4
8.1 Proof of Proposition 2
Proposition2(Hazan and Kakade(2019)). TheAdaptivePolyakalgorithmfrom(Hazan and Kakade,2019)
is tuning-free in the deterministic setting.
Proof. By(Hazan and Kakade,2019,Theorem2)wehavethatthepointreturnedbythealgorithmxsatisfies
f(x) f
2G √D T∗ log + f(x G √∗D) T− ∗fˆ 0 if f is G-Lipschitz,
− ∗ ≤ 2L TD ∗2 log
+
f(x L∗D) −2fˆ 0 if f is L-smooth.
T∗
provided that fˆ f , where fˆ is a parameter supplied to the algorithm. To get a valid lower bound on f ,
0 0
≤ ∗ ∗
observe that by the convexity of f we have
f(x ) f f(x ),x x f(x ) x x f(x ) D.
0 0 0 0 0 0
− ∗ ≤h∇ − ∗i≤k∇ kk − ∗k≤k∇ k
It follows that
f f(x ) f(x ) D.
0 0
∗ ≥ −k∇ k
And thus we can use fˆ =f(x ) f(x ) D.
0 0 0
−k∇ k
8.2 Proof of Proposition 3
Proposition 3. T-DoG and T-DoWG are tuning-free in the deterministic setting.
Proof. This is shown in (Khaled et al., 2023, Supplementary material section 7) for DoWG. The proof for
DoG is similar and we omit it for simplicity.
8.3 Proof of Theorem 2
Proof. Let σ >0. Let L=σT. Define the functions
L
f (x)d =ef x2+σx
1
2
L σ
f (x)d =ef x2 x
2
2 − T 1
−
1 1
def
f(x) = f (x)+ 1 f (x)
1 2
T − T
(cid:18) (cid:19)
L
= x2.
2
We shall consider the stochastic oracle (f,σ ) that returns function values and gradients as follows:
f
O
f (x), f (x) with probability 1,
(f,σ )(x)d =ef f (x), f (x) = { 1 ∇ 1 } T
O f { z ∇ z } ( {f 2(x), ∇f 2(x)
}
with probability 1
−
T1.
Clearly we have E[f (x)]=f(x) and E[ f (x)]= f(x). Moreover,
z z
∇ ∇
σ
f f(x) =σ, f (x) f(x) = σ.
1 2
k∇ −∇ k k∇ −∇ k T 1 ≤
−
It follows that σ σ. Therefore (f,σ ) is a valid stochastic first-order oracle. This oracle is similar to
f f
≤ O
the one used by Attia and Koren (2023) in their lower bound on the convergence of AdaGrad-Norm. The
minimizer of the function f is clearly xf =0.
∗
26Let u 0, we shall choose it later. Define
≥
L (T 1)L
h (x)d =ef (x u)2+(σ (T 1)Lu)x+ − u2,
1
2 − − − 2
L σ L
h (x)d =ef (x u)2+Lux x u2,
2
2 − − T 1 − 2
−
L
h(x)d =ef (x u)2.
2 −
with the oracle (h,σ ) given by
h
O
h (x), h (x) with probability 1,
(h,σ )(x)d =ef h (x), h (x) = { 1 ∇ 1 } T
O h { z ∇ z } ( {h 2(x), ∇h 2(x)
}
with probability 1
−
T1.
Observe that
1 L (T 1)L
E[h (x)]= (x u)2+σx (T 1)Lux+ − u2
z
T 2 − − − 2
(cid:20) (cid:21)
T 1 L σx L
+ − (x u)2+Lux u2
T 2 − − T 1 − 2
(cid:20) − (cid:21)
L σx T 1 T 1L T 1 σx T 1L
= (x u)2+ − Lux+ − u2+ − Lux − u2
2 − T − T T 2 T − T − T 2
=h(x).
We can similarly prove that E[ h (x)]=h(x). Moreover,
z
∇
h (x) h(x) = σ (T 1)Lu σ+(T 1)Lu,
1
k∇ −∇ k k − − k≤ −
σ σ
h (x) h(x) = − +Lu +Lu.
2
k∇ −∇ k T 1 ≤ T 1
(cid:13) − (cid:13) −
(cid:13) (cid:13)
It follows that σ σ+(T 1)Lu, therefore (h(cid:13),σ ) is a vali(cid:13)d stochastic oracle. Finally, observe that the
h ≤ − O (cid:13) h (cid:13)
minimizer of h is xh =u.
We fix the initia∗lization x =v >0. Then the initial distance from the optimum for both f and h are:
0
D (f)= v 0 =v, D (h)= v u . (29)
∗ | − | ∗ | − |
And recall that
σ σ, σ σ+(T 1)Lu. (30)
f h
≤ ≤ −
Observe that both f and h share the same smoothness constant L. We supply the algorithm with the
following estimates:
L=L, L=L,
D =min(v, u v ), D =max(v, u v ), (31)
| − | | − |
σ =σ, σ =σ+TLu.
Wenotethatinlightofequations(29)and(30)andthedefinitionsoff andh,thehintsgivenbyequation(31)
are valid for both problems. Now observe the following:
L σ L
h (x)= (x u)2+Lux x u2
2
2 − − T 1 − 2
−
L σ L
= (x2 2ux+u2)+Lux x u2
2 − − T 1 − 2
−
L σ
= x2 x
2 − T 1
−
=f (x).
2
27And by the linearity of expectation we have that h (x) = f (x). Therefore both oracles (f,σ ) and
2 2 f
∇ ∇ O
(h,σ ) return the same stochastic gradient and stochastic function values with probability 1 1.
O h − T
We thus have that over a run of T steps, with probability (1 1)T e 1 the algorithm will only get
− T ≈ −
the evaluations h (x), h (x) from either oracle, and will get the same hints defined in equation (31). In
2 2
{ ∇ }
thissetting,thealgorithmcannotdistinguishwhetheritisminimizinghorminimizingf,andthereforemust
minimizeboth. Thisisthe mainideabehindthisproof: weusethatthealgorithmistuning-free,whichgives
us that the output of the algorithm x satisfies with probability 1 δ
out
−
L σ D 1 LD (h)2 σ D (h)
h
h(x out) −h ∗ ≤c ·poly log + L,log + σ,log + D,log δ,logT ∗ T + √T∗ . (32)
(cid:18) (cid:19)(cid:18) (cid:19)
We shall let ι d =ef poly log L,log σ,log D,log1,logT and note that because all of the relevant param-
+ L + σ + D δ
eters (the hints, the h(cid:16)orizon T, and the probability δ) su(cid:17)pplied to the algorithm are unchanged for h and
f, this ι will be the same for h and f. Continuing from equation (32) and substituting the expressions for
D (h) and σ from equations (29) and (30) we get
h
∗
L(u v)2 (σ+(T 1)Lu) u v
h(x ) h cι − + − | − |
out
− ∗ ≤ T √T
(cid:18) (cid:19)
L(u v)2 σ u v
cι − + | − | +√TLu u v .
≤ T √T | − |
(cid:18) (cid:19)
Using the definition of h and the fact that h =0 we have
∗
L L(u v)2 σ u v
x u 2 cι − + | − | +√TLu u v .
out
2k − k ≤ T √T | − |
(cid:18) (cid:19)
Multiplying both sides by 2 and then using the definition L=σT we get
L
(u v)2 σ u v
x u 2 2cι − + | − | +√Tu u v
out
k − k ≤ T √TL | − |
(cid:18) (cid:19)
(u v)2 u v
=2cι − + | − | +√Tu u v
(cid:18)
T T3
2
| − |
(cid:19)
This gives by taking square roots and using the triangle inequality
x out u √2cι u v T −1 2 + u v T −43 +T41 u u v .
| − |≤ | − | | − | | − |
(cid:16) p p (cid:17)
And finally this implies
x out u √2cι u v T−21 + u v T−3 4 +T41 u u v . (33)
≥ − | − | | − | | − |
(cid:16) p p (cid:17)
Similarly, applying the tuning-free guarantees to f and using that D (f)=v we have
∗
L LD (f)2 σD (f) Lv2 σv
x out 2 =f(x out) f cι ∗ + ∗ =cι +
2k k − ∗ ≤ T √T T √T
(cid:18) (cid:19) (cid:18) (cid:19)
This gives
v2 σv v2 v
x 2 2cι + =2cι + .
k out k ≤ (cid:18)T √TL
(cid:19)
(cid:18)T T3
2(cid:19)
Which gives
v √v
x √2cι + (34)
out ≤ (cid:18)√T T3
4(cid:19)
28Now let us consider the difference between the lower bound on x given by equation (33) and the upper
out
bound given by equation (34),
v v
u −√2cι (cid:16)|u −v |T −21 + p|u −v |T −3 4 +T41 pu |u −v
|
(cid:17)−√2cι
(cid:18)√T
+
T3
4(cid:19)
(35)
Let us put u=v+1 and v =T2, then equation (35) becomes
T2+1 √2cι T−21 +T−43 +T41 T2+1 √2cι T2 −21 +T2 −43 . (36)
− −
(cid:0) (cid:1) (cid:16) p (cid:17) (cid:16) (cid:17)
Now observe that
L D σ+TLu 1
ι=poly log ,log ,log ,log ,logT
+ L + D + σ + δ
(cid:18) (cid:19)
1
=poly log 1,log T2,log (1+T2+T4),log ,log T
+ + + + δ +
(cid:18) (cid:19)
1
=poly log T,log .
+ + δ
(cid:18) (cid:19)
We set δ =
e−1
, therefore we finally get that ι = poly(logT), plugging back into equation (36) we get that
4
the difference between the lower bound of equation (33) and the upper bound of equation (34) is
T2+1 2cpoly(logT) T−21 +T−43 +T41 T2+1 2cpoly(logT) T2 −1 2 +T2 −43 .
− −
(cid:0) (cid:1) p (cid:16) p (cid:17) p (cid:16) (cid:17)
It is obvious that for large enough T, this expression is positive. Moreover, this situation happens with a
positive probability of at least
e−1
since by the union bound
2
1
Prob(Algorithm incorrect for f,h Oracle doesn’t output all h , h ) 2δ+ 1 (1 )T
2 2
∪ { ∇ } ≤ − − T
(cid:18) (cid:19)
e 1
−
/1 .
− 2
By contradiction, it follows that no algorithm can be tuning-free.
8.4 Proof of Theorem 3
Proof. We consider the following functions
f(x)=G x ,
| |
f (x)=G x +Gx,
1
| |
G
f (x)=G x x.
2
| |− T 1
−
We consider the stochastic oracle (f,σ ) that returns function values and gradients as follows:
f
O
f (x), f (x) with probability 1,
(f,σ )(x)d =ef f (x), f (x) = { 1 ∇ 1 } T
O f { z ∇ z } ( {f 2(x), ∇f 2(x)
}
with probability 1
−
T1.
ClearlywehaveE[f (x)]=f(x)andE[ f (x)]= f(x). Itisalsonotdifficulttoprovethat f(x) f (x)
z z z
∇ ∇ k∇ −∇ k≤
G. We define a second function
h(x)=G x u ,
| − |
h (x)=(2 T)G x u (T 1)G x +Gx,
1 − | − |− − | | (37)
G
h (x)=G x x.
2
| |− T 1
−
29And we shall use the oracle (h,σ ) given by
h
O
h (x), h (x) with probability 1,
(h,σ )(x)d =ef h (x), h (x) = { 1 ∇ 1 } T
O h { z ∇ z } ( {h 2(x), ∇h 2(x)
}
with probability 1
−
T1.
By direct computation we have that E[h (x)] = h(x) and E[ h (x)] = h(x). From the definition of the
z z
∇ ∇
functions inequation(37)it isimmediate that allthe gradientsandstochasticgradientsarebounded byGT.
It follows that σ GT. All in all, this shows (h,σ ) is a valid stochastic oracle.
h h
≤ O
We set x = 1, observe that, like in Theorem 2, with some small but constant probability both oracles
0
returnthe same gradientsandfunctionvalues,andthereforethe algorithmcannotdistinguishbetweenthem.
It is therefore forced to approximately minimize both, giving us the guarantee:
G
f(x ) f c ι
out
− ∗ ≤ · · √T
(GT) 1 u
h(x ) h c ι | − | =cι 1 u G√T
out
− ∗ ≤ · · √T | − |
This gives
cι
x (38)
out
| |≤ √T
x u cι 1 u √T
out
| − |≤ | − |
Let us put u=1 1, then
− T
1 cι
x 1
− − T ≤ √T
(cid:12) (cid:18) (cid:19)(cid:12)
(cid:12) (cid:12)
This implies (cid:12) (cid:12)
(cid:12) (cid:12)
1 cι
x 1 (39)
out
≥ − T − √T
And equation (38) implies
cι
x (40)
out
≤ √T
Becauseι=poly(logT)(bydirectcomputation),wehavethatthelowerboundonx givenbyequation(39)
out
exceeds the upper bound on the same iterate given by equation (40) as T becomes large enough, and we get
our contradiction.
9 Proofs for Section 4.2
We have the two following algorithm-independent lemmas:
Lemma 5. Suppose that Y is a sub-exponential random variable (see Definition 9.1) with mean 0 and
sub-exponential modulus R2, i.e. for all t>0
t
Prob(Y t) 2exp .
| |≥ ≤ −R2
(cid:18) (cid:19)
Let Y ,...,Y be i.i.d. copies of Y. Then with probability 1 δ it holds that
1 n
−
n
1 1 2 1 2
Y cR2 log + log ,
i
(cid:12) (cid:12)n
Xi=1
(cid:12) (cid:12)≤ "rn δ n δ #
(cid:12) (cid:12)
where c>0 is an absolute constan(cid:12)t. (cid:12)
(cid:12) (cid:12)
30Proof. By Bernstein’s inequality (Vershynin, 2018, Corollary 2.8.3) we have
1 n t2 t
Prob Y t 2exp cmin , n ,
(cid:12) (cid:12)n
Xi=1
i (cid:12) (cid:12)≥ !≤ (cid:20)− (cid:18)R4 R2
(cid:19) (cid:21)
(cid:12) (cid:12)
for some c>0. Let us set t as foll(cid:12)ows (cid:12)
(cid:12) (cid:12)
R2 1 log2 if 1 log2 <1,
t= cn δ cn δ
(R2q1 log2 if 1 log2 1.
cn δ cn δ ≥
Then (cid:2) (cid:3)
t 1 log2 if 1 log2 <1, t2 1 log2 if 1 log2 <1,
= cn δ cn δ , = cn δ cn δ
R2 (q1 log2 if 1 log2 1. R4 ( 1 log2 2 if 1 log2 1.
cn δ cn δ ≥ cn δ cn δ ≥
By combining the tw(cid:2)o cases(cid:3)above we get (cid:2) (cid:3)
t t2 1 2
min , = log .
R2 R4 cn δ
(cid:18) (cid:19)
Therefore
t2 t
2exp cmin , n =δ.
− R4 R2
(cid:20) (cid:18) (cid:19) (cid:21)
It follows that with probability at least 1 δ we have,
−
1 n R2 1 log2 if 1 log2 <1,
Y cn δ cn δ
i
(cid:12)n (cid:12)≤(R2q1 log2 if 1 log2 1.
(cid:12) Xi=1 (cid:12) cn δ cn δ ≥
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:2) 1 (cid:3)2 1 2
(cid:12) (cid:12) R2 log + log .
≤ "rcn δ cn δ #
Recall the definition of sub-exponential random variables:
Definition 9.1. We call a random variable Y R-sub-exponential if
t
Prob(Y t) 2exp −
| |≥ ≤ R
(cid:18) (cid:19)
for all t 0.
≥
Definition 9.2. We call a random variable Y R-sub-exponential if
t2
Prob(Y t) 2exp −
| |≥ ≤ R2
(cid:18) (cid:19)
for all t 0.
≥
Lemma 6. (Vershynin, 2018, Lemma 2.7.7) A random variable Y is R-sub-gaussian if and only if Y2 is
R2-sub-exponential.
Lemma7. (Vershynin,2018,Exercise2.7.10) IfAisE-sub-exponentialthenA E[A]isc E-sub-exponential
− ·
for some absolute constant c.
Lemma 8. Suppose that X is a random variable that satisfies the assumptions in Definition 4.1 and
X ,...,X are all i.i.d. copies of X. Then with probability 1 δ we have that
1 n
−
n
1 1
( X 2 σ2) c σ2 K 2 nlog +log .
(cid:12)
(cid:12)Xi=1
k
i
k − (cid:12) (cid:12)≤ · ·
s−nr
"r δ δ #
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
31Proof. By assumption we have that X is R-sub-gaussian, therefore by Lemma 6 we have that X 2 is
i i
k k k k
R2-sub-exponential. By Lemma 7 we then have that X 2 σ2 is c R2-sub-exponentialfor some absolute
i 1
k k − ·
constant c. By Lemma 5 applied to Y = X 2 σ2 we have with probability 1 δ that
i i
k k − −
n
1 1 2 1 2
( X σ2) c (c R2) log + log ,
i 2 1
(cid:12) (cid:12)n
Xi=1
k k− (cid:12) (cid:12)≤ · "rn δ n δ #
(cid:12) (cid:12)
where c
2
> 0 is some abso(cid:12) (cid:12)lute constant. Us(cid:12) (cid:12)ing the definition of the signal-to-noise ratio K s−n1
r
= R
σ
we get
that for some absolute constant c
n
1 1 2 1 2
( X σ2) c σ2 K 2 log + log .
(cid:12) (cid:12)n
Xi=1
k
i
k− (cid:12) (cid:12)≤ · ·
s−nr
"rn δ n δ #
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
9.1 Proof of Theorem 4
Themainideaintheproofisthefollowinglemma,whichcharacterizestheconvergenceofthesamplevariance
estimator of b i.i.d. random variables by the number of samples b as well as the signal-to-noise ratio K 1.
s−nr
Lemma9. LetY bearandomvectorinRd suchthatZ =Y E[Y]satisfiestheassumptionsinDefinition4.1.
−
Let Y ,Y ,...,Y be i.i.d. copies of Y. Define the sample mean and variance as
1 2 b
b b
Yˆ = 1 Y , σˆ2 = 1 Y Y 2 .
i i
b b −
i=1 i=1
X X(cid:13) (cid:13)
(cid:13) (cid:13)
Then it holds with probability 1 δ that
−
σˆ2 log2b log2(b ∨d)
1 c K 2 δ + δ ,
σ2 − ≤ ·
s−nr·s
b b 
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)  
(cid:12) (cid:12)
where c is an absolute (non-problem-dependent) constant, b d=d =ef max(b,d), σ2 d =efE Y E[Y] 2 , and
∨ k − k
K 2 is the ratio defined in Definition 4.1. h i
s−nr
Proof. We shall use the shorthand µ=E[Y]. We have
b
1 2
σˆ2 = Y Yˆ
i
b −
Xi=1(cid:13) (cid:13)
(cid:13) (cid:13)
1 b (cid:13) (cid:13) 2
= Y µ+µ Yˆ
i
b − −
Xi=1(cid:13) (cid:13)
(cid:13) (cid:13)
1 b (cid:13) (cid:13) 2
= Y µ 2+ µ Yˆ +2 Y µ,µ Yˆ
i i
b k − k − − −
Xi=1(cid:20) (cid:13) (cid:13) D E(cid:21)
b (cid:13) (cid:13) b
1 (cid:13) (cid:13)2 2
= Y µ 2+ µ Yˆ Y µ,Yˆ µ
i i
b k − k − − b − −
Xi=1 (cid:13) (cid:13) Xi=1D E
(cid:13) (cid:13)
(cid:13) (cid:13)
We have by the triangle inequality
b b
1 2 2
σˆ2 σ2 Y µ 2 σ2 + µ Yˆ + Y µ,Yˆi µ (41)
i i
− ≤(cid:12)b k − k − (cid:12) − (cid:12)b − − (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Xi=1 (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:12) (cid:12) (cid:12) Xi=1D E(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
32By Lemma 8, we may bound the first term on the right hand side of equation (41) as
1 b log1 log2
Y µ 2 σ2 c σ2 K 2 δ + δ . (42)
(cid:12)b k
i
− k − (cid:12)≤ · ·
s−nrs
b b 
(cid:12) Xi=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)  
For the second term on t(cid:12)he right hand side o(cid:12)f equation (41), we apply (Jin et al., 2019, Corollary 7) to
X =µ Y and obtain
i i
−
b
2d 2d
[µ Y ] c bR2log =cR blog .
i
(cid:13) (cid:13)Xi=1 − (cid:13) (cid:13)≤ · r δ r δ
(cid:13) (cid:13)
Squaring both sides we get (cid:13) (cid:13)
(cid:13) (cid:13)
2
b
2d
[µ Y ] c2R2blog
i
(cid:13) − (cid:13) ≤ δ
(cid:13)Xi=1 (cid:13)
(cid:13) (cid:13)
Therefore (cid:13) (cid:13)
(cid:13) (cid:13)
2
1 b c2R2log2d
[µ Y ] δ . (43)
i
(cid:13)b − (cid:13) ≤ b
(cid:13) Xi=1 (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
For the third term on the right hand s(cid:13)ide of equation(cid:13)(41) we have
b b b
1
Y µ,Yˆ µ = Y µ, [Y µ]
i i i
− − * − b − +
Xi=1D E Xi=1 Xj=1
1 1
= Y µ 2+ Y µ,Y µ .
i i j
bk − k b h − − i
j=i
X6
Taking absolute values of both sides and using the triangle inequality we get
b
1 1 1
Y µ,Yˆ µ = Y µ 2+ Y µ,Y µ
i i i j
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)b Xi=1D − − E(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1bk
Y
−
µ
k
2+
1b Xj 6=ih
Y
−
µ,Y
− µi(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) . (44)
i i j
≤ bk − k (cid:12)b h − − i(cid:12)
(cid:12) j=i (cid:12)
(cid:12) X6 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
By our sub-gaussian assumption on Y µ , the first term on(cid:12) the right hand side o(cid:12)f equation (44) can be
k − k
bounded with high probability as
2 2
Y µ c R2log =cR log . (45)
i
k − k≤ δ δ
r r
DefineZ = Y µ,Y µ . Observethatforeachi,wehavethattherandomvectorsZ ,...,Z ,Z , ,Z
i,j i j i,1 i,i 1 i,i+1 i,n
are all indepenh den− t, and− theri efore E[Y ]=0 for i=j. Observe that by the Cauchy-Schwartz ineq− uality ···
i,j
6
Z = Y µ,Y µ Y µ Y µ .
i,j i j i j
| | |h − − i|≤k − kk − k
Observe that each of Y µ and Y µ is sub-gaussianwith modulus R, therefore by (Vershynin, 2018,
i j
k − k k − k
Lemma 2.7.7) their product is sub-exponential with modulus R2. It follows that
t
Prob(Z t) Prob( Y µ Y µ t) 2exp .
| i,j |≥ ≤ k i − kk j − k≥ ≤ −R2
(cid:18) (cid:19)
33Therefore Z is also sub-exponential with modulus R2. By Lemma 5 we then get that for any fixed i, with
i,j
probability at least 1 δ we have
−
1 1 2 1 2
(cid:12) Z i,j(cid:12) c R2 log + log , (46)
(cid:12) (cid:12)b −1
j=1,...,b
(cid:12) (cid:12)≤ · "rb −1 δ b −1 δ #
(cid:12) X j=i (cid:12)
(cid:12) 6 (cid:12)
(cid:12) (cid:12)
for some absolute constan(cid:12) (cid:12)t c>0. Multiply(cid:12) (cid:12)ing both sides of equation (46) by b −b1 and then using straightfor-
ward algebra we get
1 1 2 1 2 b 1
(cid:12) Z i,j(cid:12) cR2 log + log −
(cid:12) (cid:12)b
j=1,...,b
(cid:12) (cid:12)≤ "rb −1 δ b −1 δ #· b
(cid:12) X j=i (cid:12)
(cid:12) 6 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) 1 2 b 1 2 b b 1
(cid:12) (cid:12)=cR2 log + log −
"rb δ · rb −1 b δ · b −1 # b
1 2 b 1 1 2
=cR2 log − + log
b δ b b δ
"r r #
1 2 1 2
cR2 log + log . (47)
≤ "rb δ b δ #
We now use the union bound over all i with the triangle inequality to get
n n
1 1 1 1 1 2b 1 2b
(cid:12) Z i,j(cid:12) (cid:12) Z i,j(cid:12) cR2 log + log . (48)
(cid:12) (cid:12)b
i=1
b
j=1,...,b
(cid:12) (cid:12)≤ b i=1(cid:12) (cid:12)b
j=1,...,b
(cid:12) (cid:12)≤ "rb δ b δ #
(cid:12) X X j=i (cid:12) X(cid:12) X j=i (cid:12)
(cid:12) 6 (cid:12) (cid:12) 6 (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Combining equa(cid:12)tions (45) and (48)(cid:12)we get th(cid:12)at with prob(cid:12)ability 1 δ there exists some absolute constant
−
c >0
′
1 b log2b log2b
Y
i
µ,Yˆ µ c′R2 δ + δ . (49)
(cid:12)b − − (cid:12)≤ s b b 
(cid:12) Xi=1D E(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)  
Combining equations (42), ((cid:12)43) and (49) into equa(cid:12)tion (41) we get
b b
1 2 2
σˆ2 σ2 Y µ 2 σ2 + µ Yˆ + Y µ,Yˆi µ
i i
− ≤(cid:12)b k − k − (cid:12) − (cid:12)b − − (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Xi=1 log2(cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13)log2 (cid:13) (cid:13) (cid:13) R(cid:12) (cid:12) (cid:12)2lXi o= g1 2D d loE g(cid:12) (cid:12) (cid:12)2b log2b
(cid:12) c σ2 K 2 δ(cid:12) + δ +c (cid:12) δ +c R2 (cid:12)δ + δ .
≤
1
· ·
s−nrs
b b 
2
b
3
s b b 
   
For some absolute constants c ,c ,c > 0. Therefore, using the definition K 1 = R and simplifying in the
1 2 3 s−nr σ
last equation we finally get that
log2b log2(b ∨d)
σˆ2 σ2 c σ2K 2 δ + δ ,
− ≤
4
·
s−nrs
b b 
(cid:12) (cid:12)
(cid:12) (cid:12)  
for some absolute constant c >0. Dividing both sides by σ2 yields the statement of the lemma.
4
34Algorithm 2 T-DoG + Variance Estimation
Require: initial point x , initial distance estimate r >0, minibatch size b, θ >0.
0 ǫ
∈X
1: Initialize r ǫ =D, α=84 log(60log(6T)/δ) θ −1.
· ·
2: for t=0,1,2,...,T 1 do
−
3: Update distance estimator: r t max( x t x 0 , r t 1).
4: Sample b stochastic gradients µ← 1 t,µ2 t,...k ,µb t− at xk t and− compute:
b b
µˆ = 1 µi, σˆ2 = 1 µi µˆi 2 , σ2 =maxσˆ2.
t b t t b t− t t k t k
i=1 i=1 ≤
X X(cid:13) (cid:13)
(cid:13) (cid:13)
5: Compute a new stochastic gradient g t evaluated at x t.
6: Update the gradient sum u t =u t 1+ g t 2.
− k k
7: Set the stepsize:
r 1
t
η . (50)
t ← α u t+βσ2 t log2 + 1+ u v0t+ +σ σ2 02 t
p (cid:16) (cid:17)
8: Gradient descent step: x t+1 x t η t f(x t).
← − ∇
9: end for
Algorithm 3 T-DoWG + Variance Estimation
Require: initial point x , initial distance estimate r >0, minibatch size b, θ >0.
0 ǫ
∈X
1: Initialize r ǫ =D, α=84 log(60log(6T)/δ) θ −1.
· ·
2: for t=0,1,2,...,T 1 do
−
3: Update distance estimator: r t max( x t x 0 , r t 1).
4: Sample b stochastic gradients µ← 1 t,µ2 t,...k ,µb t− at xk t and− compute:
b b
µˆ = 1 µi, σˆ2 = 1 µi µˆi 2 , σ2 =maxσˆ2.
t b t t b t− t t k t k
i=1 i=1 ≤
X X(cid:13) (cid:13)
(cid:13) (cid:13)
5: Compute a new stochastic gradient g t evaluated at x t.
6: Update weighted gradient sum: v t ←v t −1+r2 t kg t k2.
7: Set the stepsize:
r2 1
γ t . (51)
t ← α v t+βr2 tσ2 t log2 + 1+ vv 0t+ +r r2 t 2 0σ σ2 t 2
0
p (cid:16) (cid:17)
8: Gradient descent step: x t+1 x t γ t f(x t).
← − ∇
9: end for
35Proof of Theorem 4. First, observe that at every timestep t, conditioned on = σ(g ,x ) we have by
t 1:t 1 1:t
Lemma 9 that with probability 1 δ that the sample variance σˆ2 satisfies foF r some c>− 0
− T t
σˆ t2
1 c K 2
log2b δT
+
log2(b ∨ δd)T
,
σ2(x ) − ≤ ·
s−nr·s
b b 
(cid:12) t (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)  
where c is an absolute const(cid:12) ant and σ2(cid:12) = σ2(x ) denotes the variance of the noise at x (we do not assume
t t t
that the noise distribution is the same for all t). By our assumption on the minibatch size we have that for
some u [0,K2 ]
∈ snr
log2bT log2(b ∨d)T θ
c K 2 δ + δ 1 .
·
s−nr·s
b b ≤ − K2
snr
 
And therefore
σˆ2 θ
t 1 1 .
σ2(x ) − ≤ − K2
(cid:12) t (cid:12) snr
(cid:12) (cid:12)
Which gives (cid:12) (cid:12)
(cid:12) (cid:12)
σˆ2 θ θ
t 1 1 = .
σ2 ≥ − − K2 K2
t (cid:18) snr(cid:19) snr
Multiplying both sides by σ2 we get
t
θ
σˆ2 σ2 R2θ.
t ≥ tK2 ≥
snr
Thereforeσˆ2/θ is, with high probability,an upper bound on any noise norm, and we can use that as normal-
t
izationinT-DoG/T-DoWG.Thisisthekeyideaoftheproof,andit’sentirelyowedtoLemma9. Therestof
the proof follows (Ivgi et al., 2023) with only a few changes to incorporate the variance estimation process.
Following (Ivgi et al., 2023), we define the stopping time
=min t r >3d .
out t 0
T { | }
And define the proxy sequences
η if k < , γ if k < ,
k out k out
η˜ = T γ˜ = T (52)
k k
(0 otherwise. (0 otherwise.
Lemma 10. (Modification of (Ivgi et al., 2023, Lemma 8)) Under the conditions of Theorem 4 both the
DoG (50) and DoWG (51) updates satisfy for all t T
≤
ρ
t
∈σ(g 0,µ1 0,...,µb 0...,g
t
−1,µt 0−1,...,µt b−1),
6d2
ρ g f(x ),x x 0 ,
| t h t −∇ t t − ∗i|≤ 82θ
T,δ
t 9d2
ρ2 g 2 0 ,
kk k k ≤ 84θ
T,δ
k=0
X
t 122d4
(ρ g ,x x )2 0,
k h k k − ∗i ≤ 84θ
T,δ
k=0
X
where ρ stands for either the DoG stepsize proxy η˜ or the DoWG stepsize proxy γ˜ .
t k k
36Proof. The modification of this lemma to account for bounded noise g(x ) f(x ) rather than bounded
k k
−∇
gradients is straightforward,and we omit it for simplicity.
Lemma 11. (Modification of (Ivgi et al., 2023, Lemma 9)) Under the conditions of Theorem 4 both the
DoG (50) and DoWG (51) updates satisfy for all t T with probability at least 1 δ
≤ −
t 1
−
η˜ g f(x ),x x d2.
k h k −∇ k ∗− k i≤ 0
k=0
X
Proof. The modification is straightforwardand omitted.
L d2em form aa ll1 t2. ( TM ,o td hi efi ncation >of T(I .vgi et al.,2023,Lemma10))UndertheconditionsofTheorem4,if kt − =1 0ρ t hg k −∇f(x k),x
∗−
0 ≤ Tout P
Proof. The modification is straightforwardand omitted.
ByLemmaslemmas11and12wegetthatr 3d anditfollowsthatd =max d max r +r
T 0 t k t k k t t 0
≤ ≤ ≤ ≤ ≤
4d . Then, a straightforward modification of Theorem 1 to handle the slightly smaller stepsizes used by T-
0
DoG/T-DoWG shows that both methods are tuning-free. The proof is very similar to Theorem 1 and is
omitted.
10 Proofs for Section 5
10.1 Proof of Theorem 5
Proof. We use the exact same construction from Theorem 2 with the following hints:
L=L, L=L
L L
∆= min(v, u v ), ∆= max(v, u v ).
2 | − | 2 | − |
σ =σ, σ =σ+TLu,
where u > 0 and v > 0 are parameters we shall choose later. Suppose that we have that the algorithm’s
output point x satisfies
L(f(x ) f )σ2 L(f(x ) f )
f(x) 2 cι 0 − ∗ + 0 − ∗ .
k∇ k ≤ "r T T #
We now use the fact that f(x ) f = L(x x )2 to get
0 − ∗ 2 − ∗
L2 x x 2 = f(x) 2
out
k − ∗k k∇ k
cι
L2(x 0 −x ∗)2σ f2 +cιL2(x 0 −x ∗)2
≤ s T T
L x x σ L2(x x )2
0 f 0
=cι | − ∗| +cι − ∗ .
√T T
Dividing both sides by L2 we get
x x σ x x 2
x
out
x 2 cι| 0 − ∗| f +cιk 0 − ∗k .
k − ∗k ≤ L√T T
Taking square roots and using the triangle inequality gives
σ 1 x x
|x
out
−x ∗|≤√cι |x
0
−x
∗| r
Lf
T1 4
+√cι| 0 √−
T
∗|. (53)
p
37Applying equation (53) to the function f with x =v >0, x =0, σ =σ, and L=σ√T we get
0 f
∗
v v
x √cι +√cι .
out
| |≤ T √T
r
Therefore
v v
x √cι +√cι . (54)
out
≤ T √T
r
On the other hand, applying equation (53) to the function h= L(x u)2 (as in the proof of Theorem 2) we
2 −
obtain
(σ+LTu) 1 u v
x u √cι u v +√cι| − |
| out − |≤ | − |
r
L T1
4
√T
p
1 1 u v
=√cι u v +Tu +√cι| − |
| − |s√T T1
4
√T
p
1 1 u v
√cι u v +√Tu +√cι| − |
≤ | − | (cid:20)T1
4
(cid:21)T1
4
√T
p
u v u v
=√cι | − | +√cιT1 4 u u v +√cι| − |.
√T | − | √T
p
p
Therefore
u v u v
x out u √cι | − | +√cιT1 4 u u v +√cι| − | . (55)
≥ −" p√T | − | √T #
p
Combining equations (54) and (55) gives
u v u v v v
u √cι | − | +√cιT1 4 u u v +√cι| − | √cι +√cι .
−" p√T | − | √T #≤ rT √T
p
Dividing both sides by √cι,
v+√v u u v u v
1
| − | +T4 u u v + | − |
√T ≥ √cι −"p√T | − | √T #
p
Put v =T2 and u=T2+1, then we get
T2+1 1 1
√T +1 +T1 4 T2+1+ .
≥ √cι − √T √T
(cid:20) (cid:21)
p
For large enough T, since ι=poly(logT), this inequality does not hold. Therefore we get our contradiction.
10.2 Proof of Theorem 6
Theorem 7. ((Liu et al., 2023), High-probability convergence of SGD in the nonconvex setting). Let f be
L-smooth and possibly nonconvex. Suppose that the stochastic gradient noise is R2-sub-gaussian. Then for
any fixed stepsize η such that ηL 1 we have
≤
1 T −1
f(x t) 2
2(f(x 0) −f ∗)
+5ηR2+
12R2log δ1
.
T k∇ k ≤ ηT T
t=0
X
38Proof. This is a very straightforwardgeneralization of (Liu et al., 2023, Theorem 4.1), and we include it for
completeness. By (Liu et al., 2023, Corollary 4.4) we have that if η L 1 and 0 w η2L 1
t ≤ ≤ t t ≤ 2R2
T η L T T w η2L 1
w η 1 t v f(x ) 2+w ∆ w ∆ + (w w )∆ +3R2 t t +log .
t t t t T T+1 1 1 t t 1 t
t=1(cid:20) (cid:18) − 2 (cid:19)− (cid:21)k∇ k ≤ t=2 − − t=1 2 ! δ
X X X
(56)
Choose η =η and w η2L= 1 , w = 1 .
t t 4R2 t 6R2η
3R2η2(ηL 1)2 (1 ηL)2
v =3R2w2η2(η L 1)2 = − = − .
t t t t − 36R4η2 12R2
Then
η L 1 ηL (1 ηL)2
t
w η 1 v = 1 −
t t − 2 − t 6R2 − 2 − 12R2
(cid:18) (cid:19) (cid:18) (cid:19)
1 ηL (1 ηL)2
= (1 ) −
6R2 − 2 − 2
(cid:20) (cid:21)
1 ηL 1+η2L2 2ηL
= (1 ) −
6R2 − 2 − 2
(cid:20) (cid:21)
1
= 1+ηL η2L2
12R2 −
(cid:2) (cid:3)
The expression 1+x x2 is minimized for x [0,1] at x=1 and has value 1. Therefore
− ∈
η L 1
t
w η 1 v .
t t − 2 − t ≥ 12R2
(cid:18) (cid:19)
Plugging into equation (56) we get
T
1 ∆ 3η 1
f(x ) 2 1 + T +log .
12R2k∇ t k ≤ 6R2η 8 δ
t=1 (cid:18) (cid:19)
X
Therefore
1 T 2∆ 12R2log1
f(x ) 2 1 +5ηR2+ δ.
t
T k∇ k ≤ ηT T
t=1
X
10.3 Restarting SGD
We will use the following lemma from (Madden et al., 2020):
Lemma 13. (Madden et al., 2020, Lemma 33) Let Z =k 1,2,...,K with probability p and K p =
∈{ } k k=1 k
1. Let Z ,...,Z be independent copies of Z. Let Y = (Y ,...,Y ). Let X = (X ,...,X ) be a random
1 m 1 m 1 K
P
vector on the reals independent of Z. Then for any γ >0 we have
K
Prob minX >eη exp( m)+Prob p X >γ
k t k
(cid:18)k ∈Y (cid:19)≤ −
k=1
!
X
39Algorithm 4 FindLeader(S, δ, K)
1: Require: set of points V, desired accuracy δ, and per-point estimation budget K.
2: Set M =log1 and let P = V .
δ | |
3: Construct the set S =(s 1,...,s M) by sampling M points from v 1,...,v P with replacement such that
T
1
Prob(v S) , Prob(v S)=1.
i i
∈ ∝ √i+1 ∈
i=1
X
4: for m=1 to M do
5: Sample K stochastic gradients g 1m,...,g Km evaluated at s m and compute their average
K
1
gˆ = g .
m k
K
k=1
X
6: Compute and store h m = gˆ m .
k k
7: end for
8: Find the point s lead S with the minimal averagestochastic gradient norm:
∈
m =arg min h , s =S .
∗ m lead m
m 1,2,...,M ∗
∈
9: Return s lead and its estimated gradient norm g m .
∗
Theorem 8. (Convergence of FindLeader) If we run Algorithm 4 on a set V of P points v ,v ,...,v , with
1 2 P
sampling budget M and per-point estimation budget K, then the output of the algorithm satisfies for some
absolute constant c>0 and all γ >0
R2log2dM 1 P
Prob f(s ) 2 >eγ+c δ δ+exp( M)+Prob f(v ) 2 >γ .
lead p
k∇ k · K !≤ − P k∇ k !
p=1
X
And
R2log2d
g f(s ) c δ . (57)
m lead
k ∗ −∇ k≤ · K
Proof. The proof of this theorem loosely follows the proofs of (Ghadimi and Lan, 2013, Theorem 2.4) and
(Madden et al., 2020, Theorem 13). First, define the following two sets of true gradients for the iterates in
V and P respectively:
U = f(v ), f(v ),..., f(v ) U = f(s ), f(s ),..., f(s ) .
V 1 2 P S 1 2 M
{∇ ∇ ∇ } {∇ ∇ ∇ }
Lemma 13 gives us
P
1
Prob min f(s ) 2 >eγ exp( M)+Prob f(v ) 2 >γ
m p
(cid:18)m ∈1,2,...,Mk∇ k (cid:19)≤ − P p=1k∇ k !
X
Wenowcomputehowusingtheminimumfromthestochasticestimatesgˆ affectstheerror. Fixm. Observe
m
thatbecausethenormofthestochasticgradientnoise g(x) f(x) issub-gaussianwithmodulusR2,then
using (Jin et al., 2019, Corollary 7) we get with probak bility− at∇ leastk 1 δ that for some absolute constant
− M
c
1
R2log2dM
gˆ f(s ) c δ
m m 1
k −∇ k≤ ·s K
40Squaring both sides gives
R2log2dM
gˆ f(s ) 2 c δ .
m m 1
k −∇ k ≤ · K
Taking a union bound gives us that for all m [M] we have with probability δ that
∈
R2log2dM
max gˆ f(s ) 2 c δ . (58)
m m 1
m [M]k −∇ k ≤ · K
∈
We have by straightforwardalgebra
min gˆ 2 min gˆ f(s )+ f(s ) 2
m m m m
m Sk k ≤m [M] k −∇ ∇ k
∈ ∈ h i
min 2 gˆ f(s ) 2+2 f(s ) 2
m m m
≤m [M] k −∇ k k∇ k
∈ h i
min 2 max gˆ f(s ) 2+2 f(s ) 2
α α m
≤m [M] α [M]k −∇ k k∇ k
∈ (cid:20) ∈ (cid:21)
=2 max gˆ f(s ) 2+2 min f(s ) 2.
m m m
m [M]k −∇ k m [M]k∇ k
∈ ∈
Let m be the argmin. Then
∗
f(s ) 2 2 f(s ) gˆ 2+2 gˆ 2
k∇
m
∗ k ≤ k∇
m
∗ −
sm
∗k k
sm
∗k
2 f(s ) gˆ 2+4 max gˆ f(s ) 2+4 min f(s ) 2
≤ k∇
m
∗ −
sm
∗k m [M]k
m
−∇
m
k m [M]k∇
m
k
∈ ∈
6 max gˆ f(s ) 2+4 min f(s ) 2
m m m
≤ m [M]k −∇ k m [M]k∇ k
∈ ∈
R2log2dM
6c δ +4 min f(s ) 2.
1 m
≤ K m [M]k∇ k
∈
Therefore there exists some absolute constant c such that
R2log2dM 1 P
Prob f(s ) 2 >eγ+c δ δ+exp( M)+Prob f(v ) 2 >γ .
m p
k∇ ∗ k · K !≤ − P k∇ k !
p=1
X
It remains to put s =s .
lead m
∗
Proof of Theorem 6. First, observe that Theorem 7 gives that SGD run for T steps with a fixed stepsize η
such that ηL 1
≤
1 T −1
f(x t) 2
2(f(x 0) −f ∗)
+5ηR2+
12R2log δ1
. (59)
T k∇ k ≤ ηT T
t=0
X
Minimizing the above in η gives
1 2(f(x ) f )
0
η =min , − ∗ .
∗ L r 5TR2 !
We set
1 2∆
η =min , .
0
L r5TR!
41Observe that η η . Now let
0
≤ ∗
η
N ∗ = log ∗
⌈ η ⌉
0
max(L,
5TR2
)
2∆
= log .
  max(L,q 5TR2)
∆
  q 
 
First, if we exit Algorithm 1 at line 4, i.e. ifT
total
<N, then by theL-smoothness of f we have
f(y ) 2 2L(f(y ) f )
0 0
k∇ k ≤ − ∗
2L(f(x ) f )
0
N − ∗
≤ · T
total
max(L,
5TR2
) L(f(x ) f )
2∆ 0
log − ∗ .
≤  max(L,q 5TR2)· T
total
∆
 q 
This fulfills the theorem’s statement. Fromhere on our,we assume that N T . Observethat ourchoice
total
≥
of N guarantees that N N . Let τ be the first n (in the loop on line 2 of Algorithm 1) such that
∗
≥
η
∗ η τ η .
2 ≤ ≤ ∗
Plugging η =η into Equation (59) we get with probability at least δ that
τ
T1 T −1
k∇f(xτ t) k2
≤
2(f(x η0) T−f ∗)
+5η τR2+
12R2 Tlog1
δ
τ
t=0
X
4(f(x ) f ) 12R2log1
0 − ∗ +5η R2+ δ
≤ η T ∗ T
∗
2(f(x ) f ) 12R2log1
2 0 − ∗ +5η R2 + δ
≤ η T ∗ T
(cid:20) ∗ (cid:21)
L(f(x ) f )R2 (f(x ) f )L 12R2log1
13 0 − ∗ + 0 − ∗ + δ. (60)
≤ "r T T # T
We now apply Theorem 8 with the parameters:
V = xτ,xτ,...,xτ ,
0 1 T 1
−
1
M =(cid:8)log , (cid:9)
δ
K =T,
L(f(x ) f )R2 (f(x ) f )L 12R2log1
γ =13 0 − ∗ + 0 − ∗ + δ.
T T T
"r #
The theorem combined with equation (60) gives us that with probability at least 1 4δ
−
L(f(x ) f )R2 (f(x ) f )L 12R2log1 R2log2dM
f(y τ) 2 13 e 0 − ∗ + 0 − ∗ + δ +c δ . (61)
k∇ k ≤ · ·"r T T # T · T
42By straightforwardalgebra
gˆ 2 = min gˆ 2 min gˆ f(y )+ f(y ) 2
r n n n n
k k n [N]k k ≤n [N] k −∇ ∇ k
∈ ∈ h i
min 2 gˆ f(y ) 2+2 f(y ) 2
n n n
≤n [N] k −∇ k k∇ k
∈ h i
min 2 max gˆ f(s ) 2+2 f(y ) 2
α α n
≤n [N] α [N]k −∇ k k∇ k
∈ (cid:20) ∈ (cid:21)
=2 max gˆ f(y ) 2+2 min f(y ) 2.
n n n
n [N]k −∇ k n [N]k∇ k
∈ ∈
Recall that we have r =argmin gˆ 2, then as in the proof of Theorem 8 we have
n [N] n
∈ k k
f(y ) 2 2 f(y ) gˆ 2+2 gˆ 2
k∇
r
k ≤ k∇
r
−
yrk
k
yrk
2 f(y ) gˆ 2+4 max gˆ f(y ) 2+4 min f(y ) 2
≤ k∇
r
−
yrk
n [N]k
n
−∇
n
k n [N]k∇
n
k
∈ ∈
6 max gˆ f(y ) 2+4 min f(y ) 2. (62)
n n n
≤ n [N]k −∇ k n [N]k∇ k
∈ ∈
Observe that because that we passed the budget K = T to the FindLeader procedure, we can use Equa-
tion (57) and the union bound to that with probability 1 δ,
−
R2log2dN
max gˆ f(y ) 2 c δ . (63)
n n
n [N]k −∇ k ≤ · T
∈
And clearly
min f(y ) 2 f(y ) 2. (64)
n τ
n [N]k∇ k ≤k∇ k
∈
Using the estimates of equations (61), (63) and (64) to upper bound the right hand side of equation (62)
gives us that with probability at least 1 5δ
−
R2log2dN L(f(x ) f )R2 (f(x ) f )L 12R2log 1 R2log2dM
f(y r) 2 6c δ +4 13 e 0 − ∗ + 0 − ∗ + δ +c δ .
k∇ k ≤ · T " · ·"r T T # T · T #
Combining the terms and substituting in the definition of T gives the theorem’s statement.
total
43