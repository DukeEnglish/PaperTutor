Towards a mathematical theory for
consistency training in diffusion models
Gen Li ∗† Zhihan Huang ∗‡ Yuting Wei‡
February 13, 2024
Abstract
Consistency models, which were proposed to mitigate the high computational overhead during the
sampling phase of diffusion models, facilitate single-step sampling while attaining the state-of-the-art
empirical performance. When integrated into the training phase, consistency models attempt to train
a sequence of consistency functions capable of mapping any point at any time step of the diffusion
process to its starting point. Despite the empirical success, a comprehensive theoretical understanding
of consistency training remains elusive. This paper takes a first step towards establishing theoretical
underpinnings for consistency models. We demonstrate that, in order to generate samples within ε
proximitytothetargetindistribution(measuredbysomeWassersteinmetric),itsufficesforthenumber
of steps in consistency learning to exceed the order of d5{2{ε, with d the data dimension. Our theory
offers rigorous insights into the validity and efficacy of consistency models, illuminating their utility in
downstream inference tasks.
Keywords: diffusion models, consistency models, non-asymptotic theory, probability flow ODE, denoising
diffusion probabilistic model
Contents
1 Introduction 2
2 Preliminaries 3
2.1 Diffusion-based generative models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Consistency training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 A non-asymptotic convergence theory for consistency training 6
3.1 Assumptions and setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Proof of Theorem 1 8
4.1 Preliminary properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Main analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Discussion 14
∗Thefirsttwoauthors contributed equally.
†Department ofStatistics,TheChineseUniversityofHongKong,HongKong.
‡Department ofStatistics andDataScience, Wharton School,UniversityofPennsylvania, Philadelphia,PA19104, USA.
1
4202
beF
21
]LM.tats[
1v20870.2042:viXraA Proof of auxiliary results 14
A.1 Proof of the recursion (34) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.3 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.4 Proof of Claim (49). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.5 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.6 Proof of Claim (63). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.7 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.8 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.9 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
1 Introduction
Diffusionmodels(Ho et al.,2020;Sohl-Dickstein et al.,2015;Song and Ermon,2019)havegarneredgrowing
interest in recent years due to their impressive capabilities in a wide swath of generative modeling tasks,
such as image synthesis, video generation, and audio synthesis (Dhariwal and Nichol, 2021; Ho et al., 2022;
Kong et al., 2020;Popov et al., 2021;Ramesh et al.,2022; Rombach et al., 2022). In comparisonwith other
deep generative models, such as Generative Adversarial Networks, which oftentimes suffer from training
instability and mode collapse, diffusion models are capable of generating high-fidelity samples based on
learning the gradient of the log-density function or the score function. On a high level, diffusion models
concentrate on two processes: a forward Markov process that gradually degrades data into noise, and a
reverse-time stochastic or deterministic process that starts from pure noise, performs iterative denoising to
generate new data that resemble true data samples in distribution. Interestingly, while the forward process
is straightforwardlyoften designed by progressivelyinjecting more noise into the data samples, it is feasible
toreverttheprocessandensure(almost)matchingmarginalsastheforwardprocess,aslongasfaithfulscore
function estimates are obtainable (Anderson, 1982; Haussmann and Pardoux, 1986).
Nevertheless, given that diffusion models generate new data by implementing a sequence of steps in the
reverseprocess(witheachstepcomputingthescorefunctionbyevaluatingalargeneuralnetwork),theyoften
incursubstantiallyhighercomputationalcostcomparedtoothersingle-stepgenerativemodelingalgorithms,
thereby limiting their sampling efficiency in real-time applications. To remedy this issue, there has been
an explosion of efforts in developing acceleration procedures to speed up the sampling process in diffusion
generativemodeling(e.g.Li et al.(2024);Lu et al.(2022a,b);Luhman and Luhman(2021);Salimans and Ho
(2022); Song et al. (2023); Song and Ermon (2020); Xue et al. (2023); Zhang and Chen (2022); Zhao et al.
(2023)). Amongtheseefforts,training-basedmethods,exemplifiedbyprogressivedistillationandconsistency
models hold greatpromises inproducing samplers thatare computationally efficientand readyfor real-time
implementationwithoutsacrificingsamplingfidelity (Meng et al.,2023;Salimans and Ho,2022; Song et al.,
2023; Sun et al., 2023).
In this paper, our focal point is the consistency model, which was originally proposed by Song et al.
(2023) and claims the state-of-the-art performance. In a nutshell, the consistency model seeks to learn a
functionthatisabletomapanypointatanytime stepofthediffusionprocesstothe process’startingpoint
(the end corresponding to the data distribution). In the sampling phase, the consistency model enables
samplegenerationwithonlyasingleevaluationofthe neuralnetwork. The surprisingefficacyofconsistency
models has been demonstrated in various image datasets, including CIFAR-10, ImageNet 64 64, LSUN
ˆ
256 256, and also video generation (Song et al., 2023; Wang et al., 2023), to name just a few. This
ˆ
approach has received considerable recent attention, covering various extensions (e.g. Kim et al. (2023);
Song and Dhariwal (2023)) as well as applications beyond generative models (e.g. reinforcement learning
(Ding and Jin, 2023)).
Despite the aforementioned mind-blowing empirical successes, however, a theoretical understanding of
consistency models remains elusive even in the most basic setting. In light of the flexibility and versatility
of the consistency model idea (which only requires enforcing some self-consistency conditions), establishing
theoreticalunderpinningsforthesemodelsnotonlyprovidesrigorousjustificationsfortheirvalidity,butalso
yields practicalimplications in downstreaminference tasksby providingtheoreticalbenchmarks to compare
2different training strategies. However, the challenge in establishing theoretical performance guarantees lies
in understanding the role of consistency enforcement in preserving the sampling fidelity.
An overview of our contributions. In this paper, we take a first step towards establishing theoretical
support for consistency models, focusing on consistency training (namely, applying the consistency model
ideafromthetrainingstage). Morespecifically,weconsideraconsistencytrainingparadigmthatrecursively
learns a sequence of functions f , in the hope that the ultimate sampling process can be readily
t 1 t T
completedbyevaluatingf X t wu ithď Xď N 0,I .Ourtheoryrevealsthat: itissufficientforconsistency
T T T d
p q „ p q
training to take a number of steps exceeding the order of
d5 2
{
(1)
ε
uptosomelogarithmicfactorinordertogeneratesamplesthatare2εcloseindistributiontothetargetdata
distribution(measuredbythe Wassersteinmetric). Here,ddenotesthedimensionofthe targetdistribution,
and we omit the logarithm factors and dependence on other universal constants. In other words, it tells us
howmany steps needto be includedin the training stageinorderto enable one-shotsamplingthat achieves
the desirable sampling fidelity.
Notation. We introduce a couple of notation to be used throughout this paper. Given two probability
measures µ and ν on Rd, we denote by C µ,ν the set of all couplings of µ and ν (i.e., all joint distributions
p q
γ x,y whose marginals coincide with µ and ν, respectively). The Wasserstein distance of order q between
p q
these two distributions is defined as
1 q
W µ,ν : inf E x y q { , (2)
q p q “ γ C µ,ν x,y γ } ´ }2
ˆ P p qp q„ ˙
“ ‰
and we often employ W X,Y for random variables X and Y to denote the Wasserstein distance between
q
p q
distributionsofX andY.Inaddition,givenanytwofunctionsf d,T andg d,T ,wewritef d,T g d,T
p q p q p qÀ p q
or f d,T O g d,T (resp. f d,T g d,T ) if there exists some universal constant C 0 such that
1
p q “ p p qq p q Á p q ą
f d,T C g d,T (resp. f d,T C g d,T ) for all d and T. Furthermore, the notation O is defined
1 1
anp alogq ouď slytop O q exceptthp atthq eě logarip thmicq dependency ishidden. GivenamatrixM Rd p d¨q ,wedenote
ˆ
p¨q P
M as the operator norm of M. r
} }
2 Preliminaries
In this section, we introduce the basics of diffusion generative modeling and consistency models. While the
consistencymodelwas originallymotivatedto acceleratethe probabilityflowODE sampleranddistill infor-
mationfromapre-trainedmodel,theideaofpromotingconsistencyalongthetrajectorycanbeincorporated
directly into the training stage, which we focus on in this paper.
2.1 Diffusion-based generative models
Forward process. As briefly mentioned above, in diffusion generative models, one starts from a forward
processandprogressivelyperturbsthedataintopurenoise,wherethenoisedistributionisoftenchosentobe
Gaussian. The forwardprocess is often modeled as solution to an Itô stochastic differential equation (SDE)
dX f X ,t dt g t dW 0 t T , (3)
t t t
“ p q ` p q p ď ď q
where W corresponds to a standard Brownian motion, f ,t : Rd Rd is a vector-valued function that
t
determines the drift of this process, and g :R R is a fp u¨ ncq tion thÑ at adjusts the variance of the injected
noise. We shall adopt the notation q :
p¨Lqaw XÑ
throughout to represent the distribution of X in this
t t t
forward process. In particular, q :
La“w
X
p
is
oq
ur target distribution to generate samples from, and it
0 0
“ p q
is also frequently denoted by pdata. A popular special case that motivates DDPM and DDIM algorithms
3(Ho et al., 2020; Nichol and Dhariwal, 2021; Song et al., 2020) is to take f X ,t 1β t X and g t
p t q “ ´2 p q t p q “
β t forsomefunctionβ (whichcanbeinterpretedasdeterminingthelearningrateschedule). TheSDE
p q p¨q
defined above then reduces to
a
1
dX
t
“´2β pt qX tdt
`
β pt qdW
t
p0 ďt ďT q, X
0
„pdata. (4)
a
Giventhe continuous-time nature ofthe aboveforwardprocess,it wouldoftentimes be helpful to lookat
thediscrete-timecounterpartinstead. Morespecifically,considerthefollowingdiscrete-timerandomprocess:
X
0
pdata, (5a)
„
X 1 β X β W , 1 t T, (5b)
t t t 1 t t
“ ´ ´ ` ď ď
a a
withT representingthetotalnumberofsteps. Here,wedenoteby β 0,1 theprescribedlearningrates
t
t uĎp q
that controlthe strength of the noise injected at each step, and W a sequence of independent noise
t 1 t T
vectors drawn from W
i.i.d.N
0,I . If we further denote
t u ď ď
t d
„ p q
t
α : 1 β , α : α , 1 t T, (6)
t t t k
“ ´ “ ď ď
k 1
ź“
one can write
X ?α X ?1 α W for some W N 0,I . (7)
t t 0 t t t d
“ ` ´ „ p q
In practice, α is oftentimes chosen to be vanishingly small (as long as T is large enough), so as to make
T
sure that the distribution q of X is approximately N 0,I .
T T d
p q
Reverseprocess. Reversingtheaboveprocessintimeleadstoaprocessthattransformsnoiseintosamples
with distribution approximately equal to pdata, which is how diffusion models generate data. In particular,
bythe classicalresults(Anderson,1982;Haussmann and Pardoux,1986), areverse-timeSDEcorresponding
to (4) obeys
1
dYsde β T t Ysde β T t ∇logq Ysde dt β T t dZ 0 t T (8)
t “ 2 p ´ q t ` p ´ q T ´t p t q ` p ´ q t p ď ď q
´ ¯ a
with Y q and Z being a standard Brownian motion, which satisfies Ysde d X . Let p : Law Y be
the
dis0 tr„ ibutT
ion
ofYt
. Evidently, to implement such a process,it
requiresobT t´at in“
ing
ft
aithful
et st“ imatesp ot fq
the
t
score function 1
s X : ∇ logq X . (9)
t X t
p q “ p q
The existence ofthis reverse-timeSDE has formedthe underlyingrationalefor the designofthe popular
sampler called the Denoising Diffusion Probabilistic Model (DDPM).
Another popular sampler, called the Denoising Diffusion Implicit Model (DDIM) (Karras et al., 2022;
Song et al., 2020, 2021), leverages upon the so-called probability flow ODE. More precisely, consider the
following ODE
1
dYode β T t Yode ∇logq Yode dt 0 t T , Yode q , (10)
t “ 2 p ´ q t ` T ´t p t q p ď ď q 0 „ T
´ ¯
which again yields matching marginal distributions for X as
t
ode d
Y X , 0 t T.
T ´t “ t ď ď
1Fornotationalconvenience,wealsoadopttheshorthandnotation∇logqtpXqtodenotethescorefunction(bysuppressing
thedependency onX).
4Generating a data sample from this ODE again only requires reliable estimation of the score function. It
is noteworthy that this deterministic ODE-based approach is often faster than the SDE-based approach
(Song et al., 2021), which has also been justified in theory (Li et al., 2023).
We note that the probability flow ODE considered here in (10) is slightly different from the one in
Song et al. (2023), the latter of which takes the form
dY t∇logq Y dt (11)
t t t
“´ p q
and corresponds to the forward process dX ?2tdW . In particular, if the covariance of X is equal to
t t 0
I , then q is close to a Gaussian
distribution“N
0,T2I (so that the covariance explodes), whereas in the
d T d
p q
process (7), the covariance for X is preservedand equals I throughout the trajectory.
t d
Finally, we note that recent years have witnessed remarkable theoretical advances towards understand-
ing the sampling performance of diffusion models. A highly incomplete list includes Benton et al. (2023a);
Block et al.(2020);Chen et al.(2022a,b,2023b);De Bortoli(2022);De Bortoli et al.(2021);Lee et al.(2023);
Li et al.(2023);Liu et al.(2022);Pidstrigach(2022);Tang(2023);Tang and Zhao(2024). Inparticular,the
recent works Benton et al. (2023a); Chen et al. (2022a, 2023a, 2022b, 2023b); Li et al. (2023) have estab-
lished the convergence rates of both the DDPM and DDIM samplers, as well as their stability against ℓ
2
score estimation errors.
2.2 Consistency training
While the probability flow ODE approach already achieves much faster sampling compared to the DDPM
sampler, it still requires a large number of steps (or equivalently, a large number of neural network evalua-
tions) and does not yet meet the demand for real-time sample generation. This motivates the development
of the consistency model as a means to accomplish sampling in one step (Song et al., 2023).
Specifically, givena solution trajectory x of the probability ODE in (10), a consistency function
t t ǫ,T
is a parametrized function (parameterized bt y θu )Prdessigned to achieve
ideally
f : x ,t x for all t ǫ,T , (12)
θ t ǫ
p q ÝÑ Pr s
which maps a point x at time t back to the desired sample x . Therefore, given a well-trained consistency
t ǫ
modelf ,inthesamplingphase,insteadofrecursivelyapplyingdenoisingfunctionp x x asthereverse
θ θ t 1 t
p ´ | q
diffusion process in diffusion model, it suffices to evaluate f x ,T once to produce an approximation of
θ T
p q
x . By doing so, one forward pass through the consistency model (or one evaluation of the neural network)
ǫ
suffices to generate a sample that mimics the target distribution.
p
When the consistency approachis integrated into the training phase, it entails an iterative procedure to
p
find suitable parameterizationθ. Morespecifically, the idea put forwardby Song et al.(2023) is to minimize
a certain consistency training objective over the parameter θ where
minimize L θ : E λ t dist f X t Z,t ,f X t Z,t , (13)
θ´ n θ 0 n 1 n 1 θ´ 0 n n
θ p q “ p q¨ p ` ` ` q p ` q
” ` ˘ı
where the time horizon ǫ,T is discretized into N 1 sub-intervals, with boundaries t ǫ t ...
1 2
t T.2 Here,dist , ir ssoms edistancemeasurebe´ tweentwovectorsinddimension,λ “ issoă meweă ightină g
N
“ p¨ ¨q p¨q
function, and θ is some moving average of θ during the course of training. The expectation is taken over
´
X
0
pdata, Z N 0,I
d
and n drawn uniformly from 0,1,...,N .
„ „ p q t u
This paper: iterative consistency training. Motivated by the above strategy, the current work pro-
poses to iteratively learn a sequence of consistency functions f with f :Rd Rd, without making
t 1 t T t
explicit the parameterization θ. Here, each f is allowed to bet seu leď ctď ed from a functiÑ on class F (e.g., it can
t
be specified by some large neural network). Taking f x x, we propose to learn f for each 2 t T by
1 t
p q” ď ď
minimizing the following objective:
f argmin E f ?α X ?1 α Z f α X 1 α Z 2 , (14)
t « f F p t 0 ` ´ t q´ t ´1 p t ´1 0 ` ´ t ´1 q 2
P ”› a a › ı
2Theexactformulasforttiu1ďiďN› canbefoundinSongetal.(2023). ›
5where the expectation is taken with respect to Z N 0,I
d
and X
0
pdata. By computing f
t
as in (14)
„ p q „
step by step, we ensure that f X f X X . In the sampling stage, to generate a new
t t t 1 t 1 1
sample from pdata, it is sufficientp toq dr« aw a´ pp oin´ t Xq T« ¨¨N¨ « 0,I
d
and then output f
T
X
T
.
„ p q p q
3 A non-asymptotic convergence theory for consistency training
3.1 Assumptions and setup
Beforedelvingintoourmainresults,letusintroduceseveralnotationandterminologies. Forany0 α 1,
ă ă
we denote
X α ?αX ?1 αZ, and s x ∇logp x , (15)
0 α X α
p q“ ` ´ p q“ p qp q
where X
0
pdata and Z N 0,I
d
. Throughout this paper, we shall abuse the notation and let
„ „ p q
s x : s x ,
t
p q “
αtp
q
with α defined in expression (23).
t
With these definitions in mind, if we define
Φ x : g x,α (16)
t k t k
Ñ p q “ p q
with
g x,α 1
B t p α q “ 2α g t px,α q`s α pg t px,α qq , and g t px,α t q“x, (17)
B ´ ¯
d
then we claim that the probability flow ODE in (10) satisfies Φ X X . In order to prove this
t k t k
relation, it suffices to make use of the ODE given in (10) and the oÑ bsep rvatq io“ n that dα α β dt. In fact,
t t t
d “ ´
property (17) holds true for the entire ODE trajectory in the sense that g X ,α X α . For notational
t t
p q “ p q
simplicity, we shall denote
φ : Φ and Φ : Φ , (18)
t t t 1 t t 1
“ Ñ´ “ Ñ
and it satisfies
Φ x Φ Φ Φ x φ φ φ x .
t k k 1 k t 1 t 2 t t 1 k 1 k 2 t
Ñ p q“ ` Ñ ˝¨¨¨ ´ Ñ´ ˝ Ñ´ p q“ ` p ` p¨¨¨ p q¨¨¨qq
Theconsistencyfunctions f aretrainedaccordingto(14)withoutapre-trainedbackwardprocess.
t 1 t T
t u ď ď
To measure the quality of the output samples which are distributed according to f X , we consider
T T
the Wasserstein distance W f X ,X with X N 0,I . Our main result is estap blisq hed under the
1 T T 1 T d
p q q „ p q
following two assumptions.
`
Assumption 1. Assume that for 1 k t T, Φ is L -Lipschitz continuous such that
t k f
ď ă ď Ñ
Φ x Φ y L x y . (19)
t Ñk p q´ t Ñk p q 2 ď f } ´ }2
Assumption 2. Given a function› ›class F, suppose the› ›re exist ε,εF ą0 such that the functions tf
t u1 ďt ďT
trained according to (14) satisfy
T
E f ?α X ?1 α Z fF ?α X ?1 α Z ε, (20a)
t p t 0 ` ´ t q´ t p t 0 ` ´ t q 2 ď
t ÿ“1 ”› › ı
T › ›
E f tF p?α tX
0
`?1 ´α tZ q´f
t‹
p?α tX
0
`?1 ´α tZ
q 2
ďεF, (20b)
t ÿ“1 ”› › ı
where › ›
fF : argminE f ?α X ?1 α Z f α X 1 α Z 2 , (21a)
t “ f F p t 0 ` ´ t q´ t ´1 p t ´1 0 ` ´ t ´1 q 2
P ”› a a › ı
f t‹ : “E f t ´1 α› t ´1X 0
`
1 ´α t ´1Z ?α tX 0 `?1 ´α tZ . › (21b)
” `a a ˘ˇ ı
ˇ
6Inwords,Assumption 1 requiresthe mappingsfromX to X to be Lipschitzcontinuousfor everyk and
t k
t. Assumption2 is concernedwithtwosourcesoferrorsinthe trainingprocess: (i) ε controlsthe estimation
erroroftheconsistencyfunctions f
t 1 t T
wehaveobtained;(ii)εF correspondstotheapproximationerror
of restricting the consistency funct tiou nsď tď o lie within some fixed function class F, where εF 0 for function
“
classeswith largecapacity(or representationpower)like neuralnetworks. We remarkthat the optimization
step(14)forobtaining f istypicallyaccomplishedthroughpropertrainingoflargeneuralnetworks.
t 1 t T
t u ď ď
Giventhecomplexityofdevelopinganend-to-endtheory,weadoptthecommondivide-and-conquerstrategy
and decouple the training phase with the sampling phase. In the sequel, we shall focus our attention on
quantifying the sampling fidelity, assuming small estimation/optimization errors in the training phase.
Target data distribution. To streamline our main proof, we impose an additional constraint on the
target data distribution, namely,
P X
0 2
TcR 1, (22)
p} } ď q“
where X
0
pdata and c
R
0 is some arbitrarily large constant. This assumption covers a broad family of
„ ą
data distribution with polynomially large support size. We remark that this constraint can be replaced by
some careful assumptions on the tail probability of the target data distribution, and the resulting proof is
expected to be similar.
Learning rate schedule. Finally, let us specify the learning rate schedule α we would like to
t 1 t T
employ during consistency training (14). For some large enough numerical const tanu tsď cď ,c 0, we set
0 1
ą
1
β 1 α ; (23a)
1 “ ´ 1 “ Tc0
c logT c logT t
β 1 α 1 min β 1 1 , 1 , 2 t T (23b)
t t 1
“ ´ “ T ` T ď ď
" *
´ ¯
t
α α , 2 t T. (23c)
t i
“ ď ď
i 1
ź“
Note thatsuchscheduledlearningrateshavebeen employedinthe priorworkLi et al.(2023)to achievethe
desired convergence guarantees. A couple of other useful properties about these learning rates are provided
in Section 4.
3.2 Main results
We are now positioned to state our main theoretical guarantees for consistency training.
Theorem 1. Suppose the learning rates are selected according to (23) and the target distribution satisfies
property (22). Under Assumptions 1 and 2, it obeys
L3d5 2log5T
W
1
f
T
X
T
,X
1
C
1
f { ε εF (24)
p q qă T ` `
`
for some universal constant C 0, where X N 0,I .
1 T d
ą „ p q
Theorem1impliesthat,inordertoachieveasamplingerrorof2 ε εF —inthesensethatW
1
f
T
X
T
,X
1
p ` q p q qď
2 ε εF —it is sufficient for the number of steps in consistency training to exceed
p ` q `
L3d5 2
O f { , (25)
ε εF
´ ` ¯
whereddenotesthedimensionofthetargetdistrribution. Inparticular,ifthe functionclassF isrichenough
L3d5{2
and the approximationerror εF equals zero, then the number of steps required is about the order of f .
ε
This result offers an explicit characterization on the dependence of the Lipschitz constant as well as the
7dimension of the problem. As far as we are aware, this is the first result that theoretically measures the
sampling fidelity ofconsistency models, whichserveas a theoreticaljustification for consistencymodels asa
familyofgenerativemodels. Asalludedtopreviously,comparedtothepopulardiffusionmodels,consistency
models bear the benefit of one-step sampling, requiring only a single function evaluation at the sampling
stage instead of undergoing recursive denoising. Consequently, our theoretical result provides insights into
when one-step sampling is reliable.
We point out that prior results concerning convergence guarantees for diffusion models in terms of the
Wasserstein metric often encounter an exponential dependence on the smoothness parameter of the score
function(e.g.Benton et al.(2023b);Tang and Zhao(2024)). ThisismainlyduetoadirectuseofGrönwall’s
inequality,whichprovidescomparisonstothesolutiontotheinitialvalueproblem. Tacklingthisexponential
dependence is regardedas a challengingopen problem. Our resultis,however,notdirectly comparablewith
these results as the smoothness assumption is imposed instead on the mapping between random variables
along the forward trajectory.
Before concluding, let us take a moment to provide a brief proofoutline for this result; the full technical
details are postponed to Sections 4 and A. In order to prove Theorem 1, we find it helpful to study how
the error f X Φ X propagatesalong the probability flow ODE path. Specifically, we establish the
t t t t 2
} p q´ p q}
following recursive relation for each t, where
E f X Φ X E f X Φ X
t t t t 2 t 1 t 1 t 1 t 1 2
} p q´ p q} ´ } ´ p ´ q´ ´ p ´ q}
Φ
ď
E“ }f
t
pX
t
q´f tF pX
t
q}‰
2
`E“ }f tF pX
t
q´f
t‹
pX
t q}2
`E ‰B t x´1 φ
t
pX
t q
E X
t ´1
|X
t
´φ
t
pX
t q 2
“ ‰ “ 1 Φ ‰ ”› B Φ ` ˘` “ ‰ ˘› ı
E B t ´1 X t 1 γ › B t ´1 φ t X t X t 1 φ t X t dγ .› (26)
` ”ż0 ˆ Bx p ´ p qq´ Bx
`
p q ˘˙
`
´ ´ p q
˘
ı
Here,wedenoteX γ : γX 1 γ φ X . Iftheright-handsideof (26)canbeproperlycontrolled,
t 1 t 1 t t
´ p q “ ´ `p ´ q p q
thenTheorem1canbeeasilyestablishedbyapplyingthisrelationrecursively. Consequently,itboilsdownto
bounding each term on the right-handside separately. Towards this, the first two terms are concerned with
theoptimizationerrorandapproximationerrorintrainingthe consistencyfunction,whichcanbecontrolled
in view of Assumption 2. When it comes to the last two terms, while the Lipschitz property of Φ allows
t k
us to control terms involving derivatives, the main difficulty lies in controlling E X X φ XÑ as well
t 1 t t t
r ´ | s´ p q
asX φ X .AccomplishingthisrequiresacarefulstudyoftheprobabilityflowODEin(17),thedetails
t 1 t t
´ ´ p q
of which are deferred to Sections 4 and A. We would also like to point out that the studies of probability
flow ODE are inspired by the framework established in Li et al. (2023).
4 Proof of Theorem 1
4.1 Preliminary properties
Before diving into our main analysis, we collect several auxiliary facts and properties that shall be used
frequently throughout this proof.
Properties of learning rates. First, we enumerate some of useful properties about the learning rates as
specified by α in (23).
t
t u
c logT 1
α 1 1 , 1 t T (27a)
t ě ´ T ě 2 ď ď
11 α 1 1 α 1 α 4c logT
´ t ´ t ´ t 1 , 2 t T (27b)
21 α ď 2α α ď 1 α ď T ď ď
t t t t 1
´ ´ ´ ´
1 α 4c logT
1 ´ t 1 1 , 2 t T (27c)
ď 1 α ď ` T ď ď
t 1
´ ´
1
α . (27d)
T ď Tc2
In the lastline, c 1000is some largenumericalconstant. All the properties hold providedthat T is large
2
ě
enough. The proof of these properties can be found in Li et al. (2023, Appendix A.2)
8Truncation on typical events. Next, let us introduce the following event:
E : x ,x Rd Rd logp x c dlogT, x x ?α c d 1 α logT , (28)
t
“ p
t t ´1
qP ˆ ´
Xtp t
qď
3
}
t ´1
´
t
{
t }2
ď
4
p ´
t
q
" ˇ a *
ˇ
where c and c are some numˇerical constants to be specified later. Generally speaking, E encompasses a
3 4
typical range of the values of X ,X , and some part of our analysis proceed by seperately considering
t t 1
the points in E and those outsip de E. W´ hq ile truncated on E, there are some nice continuity properties on the
trajectories, and for x ,x Ec, we have
t t 1
p ´ qP
P X ,X E p x p x x dx dx
`p t t ´1 qR ˘“ “ż żp px xt t, ,x xt t´ ´1 1q qR RE EpX Xt t´ ´1 1p pxt t´ ´1 1q
q
X 2t π| pX 1t´ ´11p
α
tt q|
d
{t 2´ e1 xq
p
ˆt ´ ´1 }xt
t ´
2
p1? ´α t αx tt q´1 }2 2 ˙dx
t
´1dx
t
exp c 4dlogT , ` ˘ (29)
ď ´
which can be a high order term` in T when˘c is large enough.
4
On the typical event E, the score and density functions behave regularly, which are clarified by the
following two lemmas from Li et al. (2023).
Lemma 1 (Li et al. (2023), Lemma 1). Consider any x Rd satisfying logp x c dlogT for some
t
P ´
Xtp t
qď
3
large enough constant c . Then it holds that
3
E ?α X x X x d 1 α logT, (30a)
t 0 ´ t 2 t “ t À p ´ t q
E“›?α X x ›2 ˇX x ‰ ad 1 α logT, (30b)
› t 0 ´ t›2ˇ t “ t À p ´ t q
E”› ›?α tX
0
´x t› ›3 2ˇ ˇX
t
“x tı
À
d p1 ´α
t
qlogT 3 {2 . (30c)
”› › ˇ ı ` ˘
Lemma 1 implies that if X ta›king on a “ty›piˇcal” value, then condition on it, the vector ?α X X
t t 0 t
´ “
?1 α W might still follow a sub-Gaussian tail, whose expected norm remains on the same order of that
t t
of a´ n unconditional Gaussian vector N 0, 1 α I .
t d
p p ´ q q
Lemma 2 (Li et al. (2023), Lemma 2). Consider any two points x ,x Rd obeying
t t 1
´ P
1 x
logp x c dlogT, and x t c d 1 α logT (31)
´ Xtp t qď 2 3
›
t ´1 ´ ?α
t›2
ď 4 p ´ t q
› › a
for some large constants c 3,c 4 0. Then we have › › › ›
ą
d 1 α logT
p Xt´1px
q“
ˆ1 `O
´d
p ´
1
´t αq
t
¯˙p Xtpx q,
and for all γ 0,1 ,
Pr s
logp x γ c dlogT. (32)
´
Xt´1 t
p q ď
6
In other words, Lemma 2 ensures that if x fal` ls with˘ in a typical set of X and the point x is not too
t t t 1
far awayfromx
t
?α t, then x
t 1
is alsoa typicalvalue ofX
t
1. Lemma2 here is in a slightlyd´ ifferentform
{ ´ ´
from the original version in Li et al. (2023) due to a different definition of x γ . Notice that using the
t 1
´ p q
inequality (55), the proof of Lemma 2 in Li et al. (2023) remains valid with the new definition of x γ ,
t 1
´ p q
so we keep the original statement of this lemma.
94.2 Main analysis
Throughout this proof, we shall use capital letters to denote random vectors, and lower case letters to
denotetheircorrespondingrealizations,i.e. forsomespecificpointinthesamplespaceω Ω,wecouldwrite
P
x : X ω and x : X ω .
t t t 1 t 1
“ p q ´ “ d ´ p q
First, notice that X Φ X , which gives
1 T T
“ p q
W f X ,X W f X ,Φ X E f X Φ X .
1 T T 1 1 T T T T T T T T 2
p q q“ p q p q ď } p q´ p q}
To control the right h`and side above, let u`s introduce a piec˘e of n“otation ‰
ξ x : f x Φ x , (33)
t t t
p q “ p q´ p q
and we claim that ξ satisfies the following recursive relation with ξ 0:
t 1
“
ξ x E ξ X X x f x fF x fF x f x
t p t q“ t ´1 p t ´1 q| t “ t ` t p t q´ t p t q ` t p t q´ t‹ p t q
Φ
“ B t ´1 φ t x t E X‰ t ` 1 X t x t φ t˘x t ` ˘
` x p q ´ | “ ´ p q
B
` 1 ˘Φ` “ ‰Φ ˘
E B t ´1 X t 1 γ B t ´1 φ t x t X t 1 φ t x t dγ X t x t , (34)
` „ż0 ˆ Bx p ´ p qq´ Bx p q ˙ ´ ´ p q ˇ “ 
where we let ` ˘ ` ˘ ˇ
ˇ
x γ : γx 1 γ φ x (35)
t 1 t 1 t t
´ p q “ ´ `p ´ q p q
for γ 0,1 . We leave its derivation to Section A.1. In addition, let us denote X γ γX 1
t 1 t 1
P r s ´ p q “ ´ `p ´
γ φ X , and the above relation implies that
t t
q p q
E ξ X E ξ X E f X fF X E fF X f X
T p T q 2 ď T ´1 p T ´1 q 2 ` T p T q´ T p T q 2 ` T p T q´ T‹ p T q 2
Φ
“› › › › ‰ “› › E B T ´› ›1 ‰ φ T X“ T› › E X T 1 X T › ›φ‰ T X T“› › › › ‰
` x p q ´ | ´ p q
"› B ›2
›
› ›E
1`
BΦ T
´1˘ X` T“
1 γ
BΦ‰
T ´1 φ T
X˘ T›
› › X T 1 φ T X T dγ (36)
` „›ż0 ˆ Bx p ´ p qq´ Bx p q ˙ ´ ´ p q ›2*
i T › › ` ˘ ` ˘ › ›
pq E f X › fF X E fF X f X ›
ď t p t q´ t p t q 2 ` t p t q´ t‹ p t q 2
t 1
ÿ“ “› › ‰ “› › ‰
› T Φ › › ›
` B t x´1 φ t pX t q E E X t ´1 |X t ´φ t pX t q 2
t ÿ“2" B ` ˘ ”› “ ‰ › ı
1 Φ › Φ ›
`
ż0
E „›B Bt x´1 pX t ´1 pγ
qq´
B Bt x´1 φ t pX t
q
›
X t ´1 ´φ t pX t
q
2dλ
*
T › ` ˘›› ›
ii › ›› ›
p ďqε `εF
`
L fE E›X
t ´1
|X
t
´φ
t
pX
t q 2
›
t ÿ“2" ”› “ ‰ › ı
`su γpE „›BΦ Bt x´1 pX›
t ´1
pγ
qq´
BΦ Bt x´1 φ
t
pX›
t q
›
X
t ´1
´φ
t
pX
t q 2
*,
› ` ˘›› ›
ε εF T 1› T 2, ›› › (37)
“ ` ` ›` ›
where relation i applies inequality (36) recursively and relation ii invokes the triangle inequality and
pq p q
Assumption 1. In the following, we proceed to bound the latter two terms separately.
Control quantity T . Let us start with the term T , where the goal is to control each quantity in the
1 1
summation, which is E E X X φ X 2 . Recalling the backward ODE flow (17) that Φ x :
t ´1 | t ´ t p t q 2 t Ñk p q “
g t x,α k and ”› “ ‰ › ı
p q › ›
g x,α 1
t
B p q g x,α s g x,α , and g x,α x,
α “ 2α t p q` α p t p qq t p t q“
B ´ ¯
10it is easy to check that
1 g x,α
B ?α t p q 1 s g x,α . (38)
´ α ¯ “ 2α23 α p t p qq
B
As a result, we can track the backwardprocess with the score function as:
?α φ X X ?α Φ X Φ X
t t t t t t t 1 t t t t
p q“ `p Ñ´ p q´ Ñ p qq
αt´1 1
X ?α s g X ,α dα
“ t ` t 2α3 2 α p t p t qq
żαt {
1 αt´1 α
X 1 ?α s X t s g X ,α s X dα. (39)
“ t `p ´ t q t p t q` 2 α3 α p t p t qq´ t p t q
żαt c
` ˘
For the remaining term, we first apply the definition of the forward process:
?α E X X ?α E α X 1 α Z X ?α X ?1 α Z
t t 1 t t t 1 0 t 1 t t 0 t
´ | “ ´ ` ´ ´ | “ ` ´
“ ‰ X E”a?α α a?1 α Z X ?α X ?1 α Zı. (40)
t t t t t t 0 t
“ ` ´ ´ ´ | “ ` ´
“` ˘ ‰
The previous work on score matching admits a minimum mean square error (MMSE) form for the score
function (e.g. Chen et al. (2022b); Hyvärinen (2005); Vincent (2011)):
1 2
s : arg min E s ?αX ?1 αZ Z ,
α 0
“ s:Rd Rd p ` ´ q` ?1 α 2
Ñ „› ´ › 
› ›
which leads to an alternative expression by t›he change of variables: ›
›
1
s x E Z ?αX ?1 αZ x . (41)
α 0
p q“ ´ ?1 α ` ´ “
„ ´ ˇ 
ˇ
Plugging equation (41) into (40), we obtain ˇ
?α E X X X 1 α 1 α α α s X , (42)
t t 1 t t t t t t t t
´ | “ ` ´ ´ p ´ qp ´ q p q
“ ‰ ` a ˘
which when combined with (40) yields
?α φ X ?α E X X
t t t t t 1 t
p q´ ´ |
1 ?α 1“ α ‰ 1 α α α s X 1 αt´1 α t s g X ,α s X dα. (43)
“ p ´ t q´p ´ t q` p ´ t qp t ´ t q t p t q` 2 α3 α p t p t qq´ t p t q
´ a ¯
żαt c
` ˘
With equations (39) and (42) in place, we arrive at
1
E φ X E X X 2 E ?α φ X ?α E X X 2
t p t q´ t ´1 | t 2 “ α
t
t t p t q´ t t ´1 | t 2
”› “ ‰› ı ”› “ ‰› ı
› › 1 E › 1 α 1 α α › α 1 ?α s X
t t t t t t t
“ α t « ´p ´ q` p ´ qp ´ q`p ´ q p q
›´ a ¯
›
› 1 αt´1 α t s g X ,α s X dα 2
` 2 żαt cα3 α p t p t qq´ t p t q ›2ff
2 ` 2 ˘ ›
?1 α ?1 α ?α α 1 ?α E s X› 2
ď α ´ ´ t ´ t ´ t ´ t `p ´ t q } t p t q}2
t
´ 1` αt´1 α ˘ ¯ “ 2 ‰
E t s g X ,α s X dα .
` 2α t
”›
›żαt cα3
`
α p t p t qq´ t p t q
˘ ›
›2
ı
› ›
11In view of the Taylor expansion, we can further control the right hand side above as
E ”›φ t pX t q´E “X t ´1 |X t ‰›2 2 ıď α2 tˆ´ ´1 ´ 2α t 1´ 8p α1 p1´
´
5α αt q t2 q¯` ´1 ´ 2α t ` p1 ´ 8α t q2 ¯˙2 E “}s t pX t q}2 2
‰
› › O p ´ t q E s X 2
` α 1 α 5 2 } t p t q}2
´ t p
1
´ t q α{ t´¯
1
“
α
‰
2
E t s g X ,α s X dα
1 α 4
` 2α t
”›
›żαt cα α3
t´`
1α p αt p t qq´ t p t q
˘ ›
›2
ı 2
À pp 1 ´´ α tt qq 2E “}s t pX t q}›2 2 ‰`E
”›
›żαt cα3t `s α pg t pX t,α qq´s t p›X t q ˘dα
›
›2
(ı
4.
4)
› ›
To further control the right hand side of expression (44), we introduce the following Lemma 3 and
Lemma 4, which provide upper bounds for the two expectations in (44) respectively. The proofs of these
lemmas can be found in Sections A.2 and A.3 respectively.
Lemma 3. For X
t
?α tX
0
?1 α
t
Z, where X
0
pdata and Z N 0,I
d
, the second moment of the
„ ` ´ „ „ p q
score function satisfies
d
E s X 2 .
} t p t q}2 ď 1 α
t
´
“ ‰
Moreover, for any 0 α 1, the lemma still holds when replace α with α.
t
ă ă
Lemma 4. For X defined the same as in Lemma 3, pre-selected α and corresponding α , α , we
t i 1 i t t t 1
t u ďď ´
deduce that
αt´1 α 2 1 α 4d3log3T
E
”›
›żαt cα3t `s α pg t pX t,α qq´s t pX t q ˘dα
›
›2 ıÀ p ´ p1t ´q α t q3 .
In view of Lemma 3›and Lemma 4, the right hand side of (›44) is further controlled as
1 α 4d 1 α 4d3log3T
E φ X E X X 2 p ´ t q p ´ t q
t p t q´ t ´1 | t 2 À 1 α
t
3 ` 1 α
t
3
”› › “ ‰› › ı pp 1 ´´ α t qq 4d3log3Tp . ´ q (45)
À 1 α 3
t
p ´ q
Now by properties of the step sizes mentioned in (27b), this upper bound can be simplified as
C d3 2log7 2T
E φ X E X X 2 { { , (46)
t p t q´ t ´1 | t 2 ď T2
”› “ ‰› ı
where C denotes some universal›constant. ›
2
Control quantity T . Now, let us turn our attention to control the term T . We first decompose this
2 2
term by the Cauchy-Schwartzinequality:
Φ Φ
E B t x´1 pX t ´1 pγ qq´ B t x´1 φ t pX t q X t ´1 ´φ t pX t q 2
„› B B › 
ď1 2E› › ›X
t ´1
´φ
t
pX
t q
2
2`
21 E BΦ` t x´1 pX t˘ ´› › ›1› › pγ
qq´
BΦ t x´1 φ› ›
t
pX
t q
2 , (47)
› B B ›
› › › ` ˘›
and we aim to handle the›two component›s respec›tively. ›
› ›
12• Towards bounding the first term in (47), in view of relation (39), we make the observation that
1
E X φ X 2 E ?α X X ?α φ X X 2
t ´1 ´ t p t q 2 “ α
t
p t t ´1 ´ t q´p t t p t q´ t q 2
› › 1 › ›
› › E› ?α α ?1 α Z 1 ?α s› X
t t t t t t
“ α ´ ´ ´ `p ´ q p q
t
›
›` ˘ 1 αt´1 α 2
› t s g X ,α s X dα
` 2 żαt cα3 α p t p t qq´ t p t q ›2
3 E ?α α ?1 α Z 2 1 ?α` s X 2 ˘ › ›
ď α t „ t ´ t ´ ´ t 2` p ´ t q t p t q 2
› ›` 1˘ › ›αt´1›
› α t s g X
,›
› α s X dα 2
` 4 ›żαt cα3 α p t p t qq´ t p t q ›2 
1 α 2 1 α 2d 1› α 4d3log3` T ˘ ›
p ´ t q p ´ t q p ´› t q ›
À 1 α ` 1 α ` 1 α 3
t t t
´ ´ p ´ q
dlog2T
. (48)
À T2
Here, we recall the properties of the learning rates as in (27a) and (27b).
• Whenitcomestothe secondtermin(47), weclaimthatforany x ,x pair,itcanbe decomposed
t t 1
p ´ q
as
Φ Φ t φ φ
B t x´1 px t ´1 pγ qq´ B t x´1 φ t px t q ďL2 f B xk Φ t ´1 Ñk px t ´1 pγ qq ´ B xk Φ t Ñk px t q . (49)
› B B › k 1› B B ›
› ` ˘› ÿ“ › ` ˘ ` ˘›
› › › ›
The›proofofclaim(49)isprovidedino›urSectionA› .5. Weproceedtocontroltherighthands›ideabove
with the aid of the following lemma.
Lemma 5. For 2 k t T, X and X γ defined as above, it holds that
t t 1
ď ă ď ´ p q
E ›B Bφ xk Φ t ´1 Ñk pX t ´1 pγ
qq ´
B Bφ xk Φ t Ñk pX t
q
›2 Àp1 ´α pk 1q ´2 p1 α´
k
q2α pt 1q2 ´L α2 fd
t
q4 2log3T
`
p1 ´ pα 1t ´q4d α4
t
qlo 4g4T .
› ` ˘ ` ˘› (50)
› ›
› ›
We defer the proof of this result to Section A.5. With Lemma 5 in place, we can further derive that
E ›BΦ Bt x´1 px t ´1 pγ
qq´
BΦ Bt x´1 φ t px t
q
›2 ÀL4 fT2 ˆp1 ´α pk 1q ´2 p1 α´
k
q2α pt 1q2 ´L α2 fd
t
q4 2log3T
`
p1 ´ pα 1t ´q4d α4
t
qlo 4g4T
˙
›
›
›
` ˘›
›
›
L6 fd4log8T
. (51)
À T2
Here, again we use the properties of step size in (27b).
Putting expressions (48) and (51) together leads to
E BΦ t x´1 pX
t ´1
pγ
qq´
BΦ t x´1 φ
t
pX
t q
X
t ´1
´φ
t
pX
t q 2 ď
C 1L3 fd T5 { 22log5T . (52)
„› B B › 
› ` ˘›› ›
› ›› ›
In conclusion, ›taking relations (46) and (52) colle›ctively with relation (37), we arrive at
W
1
f
T
pX
T
q,X
1
qďE f
T
pX
T
q´Φ
T
pX
T q 2 ď
C 2d3 {2 Tlo 2g7 {2T ¨T
`
C 1L3 fd T5 { 22log5T ¨T `ε `εF
` “› › ‰ C L3d5 2log5T
› › 1 f { ε εF.
ď T ` `
This thus completes the proof of our advertised result.
135 Discussion
In this work, we have developed a rigorous mathematical framework for analyzing consistency training in
diffusion models. Given a set of consistency functions with sufficiently small training error, we have pinned
down the finite-sample performance for the consistency model in terms of the Wasserstein metric, with
explicit dependencies on the problem parameters. The analysis framework laid out in the current paper
might potentially be applicable to other generative and distillation models, such as the progressive training
procedure in Salimans and Ho (2022).
Moving forward, we highlight several possible directions worthy of future investigation. For instance, it
remainsunclearwhether ourtheoryoffersoptimaldependencies onthe Lipschitzconstantofthe Φ map-
t k
Ñ
pingsandtheambientdimensiond. Canwefurtherrefineourtheoryinordertoobtaintighterdependencies
orestablishmatchinglowerbounds? Inaddition,ourtheorydecouplesthe trainingphasefromthe sampling
phase by assuming a small optimization/estimation error. It would be of great interest to consider whether
one can establish end-to-end results that combine these two phases. Moving beyond consistency models,
it would also be interesting to compare our theory—in terms of sampling efficiency—with other generative
sampling methods, such as accelerated ODE and SDE methods (Lu et al., 2022a; Song and Ermon, 2020).
Acknowledgements
Y. Wei is supported in part by the the NSF grants DMS-2147546/2015447,CAREER awardDMS-2143215,
CCF-2106778,and the Google Research Scholar Award.
A Proof of auxiliary results
A.1 Proof of the recursion (34)
Recalling the definitions of f x and f x yields
t
p q
t‹
p q
f x E Φ X X x E ξ X X x f x fF x fF x f x
t p t q“ t ´1 p t ´1 q| t “ t ` t ´1 p t ´1 q| t “ t ` t p t q´ t p t q ` t p t q´ t‹ p t q
Φ x E Φ X Φ φ x X x E ξ x X x
“t t t 1 t 1 ‰ t“1 t t t t ‰ ` t 1 t 1 t ˘ t` ˘
“ p q` ´ p ´ q´ ´ p q | “ ` ´ p ´ q| “
F F
` f“t px t q´f t px t q ` f`t px t q´˘ f t‹ px t q .‰ “ ‰
` ˘ ` ˘
Invoking the Taylor expansion to obtain
Φ
Φ t 1 x t 1 Φ t 1 φ t x t B t ´1 x γ x t 1 φ t x t dγ
´ p ´ q´ ´ p q “ żγ Bx p p qq ´ ´ p q
` ˘ Φ ` ˘
t 1
B ´ φ t x t x t 1 φ t x t
“ x p q ´ ´ p q
B
` B˘`Φ t ´1 x
t 1
γ ˘ BΦ t ´1 φ
t
x
t
x
t 1
φ
t
x
t
dγ,
` żγˆ Bx p ´ p qq´ Bx p q ˙ ´ ´ p q
` ˘ ` ˘
further leads to
f x Φ x E ξ X X x f x fF x fF x f x
t p t q“ t p t q` t ´1 p t ´1 q| t “ t ` t p t q´ t p t q ` t p t q´ t‹ p t q
Φ
B“t ´1 φ
t
x
t
E X
t
1‰ X t` x
t
φ
t
x
t
˘ ` ˘
` x p q ´ | “ ´ p q
B
` ˘1` “Φ ‰ Φ ˘
E B t ´1 X t 1 γ B t ´1 φ t x t X t 1 φ t x t dγ X t x t . (53)
` „ż0 ˆ Bx p ´ p qq´ Bx p q ˙ ´ ´ p q ˇ “ 
` ˘ ` ˘ ˇ
This thus establishes relation (34). ˇ
14A.2 Proof of Lemma 3
We firstrecallthe definitionofs x , whichis the scorefunction ofX ?α X ?1 α Z. Ifwe letP
t p q t “ t 0 ` ´ t ?αt
be the probability measure of ?α X , and p be the density of ?1 α Z, by definition of the score
function, we can write
t 0 ?1 ´αtZ ´ t
∇ p x ?α x dP ?α x
s
t
x x x0 ?1 ´αtZp ´ t 0 q ?αtX0p t 0 q
p q“´ p x ?α x dP ?α x
xş 0 ?1 ´αtZp ´ t 0 q ?αtX0p t 0 q
xş 0 ?α 1t ´x α0 t´xexp ´ }x 2´ p? 1 ´α αtx tq0 }2 dP ?αtX0p?α tx 0 q
“´ş x0exp ´ ´ }x 2´ p? 1 ´α αtx t0 q}2 dP ?¯ αtX0p?α tx 0 q
E ş ?α´ tX 0 ´x . ¯ (54)
“ X0 |Xt“x 1 α
t
” ´ ı
The second moment of score function thus can be written as
?α X X 2
E }s t pX t q}2 “E Xt E X0 |Xt 1t 0 α´
t
t
2
› ” ´ ı›
ďE Xt› ›E
X0 |Xt
?α 1tX 0 α´ tX t 2 2› ›
” 1 › ´ › ı
“E Xt 1 α t› › 2E X0 |Xt}?α t› ›X 0 ´X t }2 2
”p ´ q ı
d
,
“ 1 α
t
´
where the last line makes use of the expression (7).
A.3 Proof of Lemma 4
Throughoutthisproof,weadoptthetruncationstrategyontothetypicaleventE (definedinexpression(28)).
t
The targeted expectation is then calculated by consideringthe typical event and its complement separately.
On the typical event E . Let us first consider the case when x ,x E . We claim that
t t t 1 t
p ´ qP
dlogT α
s g x ,α 2 c and g x ,α x c d 1 α logT (55)
} α p t p t qq}2 ď 5 1 ´α t
›
t p t q´ cα t t ›2 ď 6
a
p ´ t q
hold for all α
t
α α
t
1. This claim essentially› ›means that every › ›x t,x
t 1
E
t
induces a trajectory on
which all the poď intsď shar´ e similar properties as the definition of E . Inp the fo´ lloq wP ing proof, we shall use α as
t
the variable of integration to differentiate from α, which serves as an argument.
Before proceeding, we isolate some properties obtained with the help of this claim. In particular, if
r
relation (55) holds, then dynamic (38) implies that
x 1 α 1
g x ,α ?α t s g x ,α s x dα
t p t q“ ?α ` 2 α3 α p t p t qq´ t p t q
´
x
t 1żα αtc
1
`
r
˘ ¯
?α t dα O surp s g x ,rα s x
“ ?α ` 2 αr3 ¨ } α p t p t qq´ t p t q}2
´
t żαtc `αtăα ăα
˘¯
α 1 1 r
x O α r r sup s gr x ,α
ď cα
t
t `
ˆ
a
t ´1 ´r?α
t
´ α
t ´1 ¯αtăα
ăα} α p t p t qq}2
˙
α r
x O 1 α sup s g x ,αr r
“ α t ` p ´ t q } α p t p t qq}2
c t ˆ αtăα ăα ˙
r
“
cαα tx t `O ˆdd p1 ´ 1α ´t q α2 r tlogT ˙, r (56)
15where the last line holds using the bound (55). In addition, given the claim (55), according to (161c)
in Li et al. (2023, Appendix C.1), the following inequality holds:
dlogT 3 {2
s g x ,α s x 1 α . (57)
} α p t p t qq´ t p t q}2 Àp ´ t q 1 α
ˆ ´ t˙
Proof of relation (55). We establish the relation (55) by contradiction. If the condition does not hold
along the trajectory, let us define
c dlogT
α: min α: s g x ,α 2 5 or g x ,α α α x c d 1 α logT .
“ } α p t p t qq}2 ą 1 α } t p t q´ { t t }2 ą 6 p ´ t q
t
! ´ a a )
The conptradictionappears if we show both scenarios in the definition of α cannot happen. By virtue of this
definition, it satisfies that for α α α, inequalities (56) and (57) still hold true.
t
ď ă
p
• If for the defined α, we have g x ,α α α x c d 1 α logT, Then, by calculations in
p} t p t q´ { t t }2 ą 6 p ´ t q
expression (56), g x ,α can be written as
t t
p q a a
p
x 1 α 1
g px ,α ?α t s g x ,α s x dα
t p t q“ ´?α t ` 2 żαp tcα3 α p t p t qq´ t p t q
¯
` ˘
p “dp αα tx t `O ˆdd p1 ´r 1α ´t q
α2r
tlogT ˙r r
p
α
x O d 1 α logT .
t t
ďdα
t
` p p ´ q q
p a
which is contradicted with the assumption g x ,α α α x c d 1 α logT.
t t t t 2 5 t
} p q´ { } ą p ´ q
• Otherwise, consider the case that s g x ,α 2 c5dalogT. For α aα α, by inequality (57), we
directly obtain
} α p t p t qq}2 ą 1 ´αt t ď ă
p
p p
dlogT 3 {2 dlogT 1 {2 dlogT 1 {2
s g x ,α O 1 α O O ,
} α p t p t qq}2 ď p ´ t q 1 α ` 1 α “ 1 α
ˆ ˆ ´ t˙ ˙ ˆ ´ t˙ ˆ ´ t˙
where we use the fact that
dlogT
s x 2 , (58)
} t p t q} À 1 α
t
´
whose proof can be found as in (128b) of Li et al. (2023, Appendix B.1.1). We can then make use
of the continuity of s x and trajectory to obtain s g x ,α dlogT 1 2. This result is also
contradicted with
theα dp efiq
nition of α.
} α p t p t qq}2 À p 1 ´αt q {
p
p
Putting everything together, we conclude that α α ,α does not exist, which thus validates the
p P r t t ´1 s
claim (55).
p
On the complement of the typical event Ec. Let us now turn to the case when x ,x Ec. Using
the upper bound in Lemma 3, we integrate overt the tail event of X and X as ineqp uat lityt ´ (1 2q 9)P tot obtain
t t 1
´
E s X 21 X ,X Ec s X 2p x ,x dx dx
Xt,Xt´1 } t p t q}2 p t t ´1 qP t À Ec} t p t q}2 Xt´1,Xtp t ´1 t q t ´1 t
”
`
˘ı ż t
s X 2p x x p x dx dx
À Ec} t p t q}2 Xt´1 |Xtp t ´1 | t q Xtp t q t ´1 t
ż t
d
p x x dx . (59)
À1 ´α t żxt´1: pxt,xt´1 qPE tc Xt´1 |Xtp t ´1 | t q t ´1
16It has been shown in Li et al. (2023, Step 3, Appendix C.1) that
p x x dx exp c dlogT .
żxt´1: pxt,xt´1 qPE tc
Xt´1 |Xtp t ´1
|
t
q
t ´1
À p´
4
q
By virtue of this relation, we can conclude that
E s X 21 X ,X Ec exp c dlogT . (60)
Xt,Xt´1 } t p t q}2 p t t ´1 qP t À p´ 4 q
Here, similar to the proof of L“emma 3, it`holds that ˘‰
d
E s g X ,α 2 E s X α 2 ,
Xt} α p t p t qq}2 “ X pα q} α p p qq}2 ď 1 α
´
d
where use the fact that g X ,α X α . As a result, this inequality enables us to bound the expectation
t t
p q “ p q
of the truncation error in a similar way as in inequality (60):
E s g X ,α 21 X ,X Ec s g X ,α 2p x x p x dx dx
Xt,Xt´1 } α p t p t qq}2 p t t ´1 qP t À Ec} α p t p t qq}2 Xt´1 |Xtp t ´1 | t q Xtp t q t ´1 t
”
`
˘ı ż t
d
p x x dx
À1 ´α t żxt´1: pxt,xt´1 qPE tc Xt´1 |Xtp t ´1 | t q t ´1
exp c dlogT .
4
À p´ q
In summary. Combining the two cases above, we conclude that
αt´1 α 2
E t s g X ,α s X dα
”› ›ż αα tt
´1
c αα3 ` α p t p t qq´ t p t q ˘ › ›2 ı
› tE s g X ,α s X 2›dα
ď α3 } α p t p t qq´ t p t q}2
żαt c
αt´1 α “ ‰
tE s g X ,α s X 2 1 X ,X E 1 X ,X Ec dα
“
żα 1t αc
4α d3
3lo”
g} 3α Tp t p t qq´ t p t q}2
`
`p t t ´1 qP t ˘` `p t t ´1 qP t
˘˘ı
À
p ´ 1t q
α 3
`exp p´c 4dlogT
q
t
p ´ q
1 α 4d3log3T
t
p ´ q ,
À 1 α 3
t
p ´ q
which thus validates the claimed result.
A.4 Proof of Claim (49)
Towards this, let us first make the observation that
Φ Φ φ
B t Ñk x B t ´1 Ñk φ t x B t x
x p q“ x p q x p q
B B B
BΦ t ´2 Ñk` Φ
t
t˘
2
x Bφ t ´1 Φ
t t 1
x Bφ t x
“ x Ñ´ p q x Ñ´ p q x p q
B B B
Φ
` k˘1
φ
` ˘
B k1 Ñk Φ t k1 x B i Φ t i x
“ x Ñ p q x Ñ p q
B i k B
` ˘ź“ ` ˘
t φ
B i Φ t i x ,
“ x Ñ p q
i k 1 B
“ź` ` ˘
17where we recursively apply the definition of Φ φ Φ . In view of the relation above, by some
k1 k k1 k1 1 k
Ñ “ ˝ ´ Ñ
direct algebra, we deduce
Φ Φ t φ t φ
B t x B t y B i Φ t i x B i Φ t i y
x p q´ x p q “ x Ñ p q ´ x Ñ p q
› B B › ›i 2 B i 2 B ›
› › ›ź“ ` ˘ ź“ ` ˘›
›
›
›
›
›
›
t k ´1 Bφ
i Φ
t i
x
Bφ
k Φ
t k
x›
›
Bφ
k Φ
t k
y
t Bφ
i Φ
t i
y
“ x Ñ p q x Ñ p q ´ x Ñ p q x Ñ p q
›k 3ˆi 2 B ˆ B B ˙i k 1 B ˙›
› ÿ“ ź“ ` ˘ ` ˘ ` ˘ “ź` ` ˘ ›
› ›t k ´1 Bφ i Φ
t i
x Bφ k Φ
t k
x Bφ k Φ
t k
y t Bφ i Φ
t i
y › ›
ď x Ñ p q x Ñ p q ´ x Ñ p q x Ñ p q
k 2›i 2 B ›› B B ››i k 1 B ›
ÿ“ ›ź“ ` ˘›› ` ˘ ` ˘›› “ź` ` ˘›
t › Φ ›› φ φ ›› Φ ›
› B k ´1 Φ
t k 1
x›› B k Φ
t k
x B k Φ
t k
y›› B t Ñk y ›
“ x Ñ ´ p q x Ñ p q ´ x Ñ p q x p q
k 2› B ›› B B ›› B ›
ÿ“ › ` ˘›› ` ˘ ` ˘›› ›
›t φ ››φ ›› ›
L2 › B k Φ x ›› B k Φ y . ›› ›
ď f x t Ñk p q ´ x t Ñk p q
k 2› B B ›
ÿ“ › ` ˘ ` ˘›
› ›
where we denote i i´1 pBφ
i
{Bx› q: “1 for saimplicity, and the las›t invokes the Assumption 1 again.
ś
A.5 Proof of Lemma 5
To begin with, let us first provide a more succinct expression for quantity Bφk xpx q. Recall that φ
k
px
q
:
“
g x,α . In view of relation (38), we can write B
k k 1
p ´ q
φ x α
g
k
px,α
k q
αk´1 1
s g x,α dα
k p q“ k ´1 ˜ ?α k ` żαk 2α23 α p k p qq ¸
a
1 1 αk´1 α
x ks g x,α dα . (61)
“ ?α k˜ ` 2 żαk cα3 α p k p qq ¸
By some direct calculations, we arrive at
Bφ
Bk
xpx
q “
?1
α kˆI `
1
2
żαα kk´1 cα
αk 3∇s α pg k px,α
qqBg
k
Bpx x,α
qdα
˙
(62)
where we write ∇s g x,α : ∇ s y . We then proceed to control each term in the above
expression. To do sα op , lk ep t usq iq ntr“ oducy eα tp heq fy o“llg ok wpx i, nα gq two lemmas whose proofs are provided in Section A.7
ˇ
and A.8 respectively. ˇ
Lemma 6. For 2 t T, α α α and x ,x E , the derivative of the score function satisfies
t t 1 t t 1 t
ď ď ď ď ´ p ´ qP
d2 1 α log2T
∇s α pg t px t,α qq´∇s t px t q 2 À p 1´ t αq 2 .
t
p ´ q
› ›
Lemma 7. For 2 t T an› d x ,x E , the stabi› lity of the backward ODE (38) starting at x can be
t t 1 t t
ď ď p ´ qP
bounded as follows:
g x ,α d 1 α logT
B t p t q I p ´ t q .
x ´ À 1 α
› B › ´ t
› ›
› ›
Plugging in the bounds from Lem›ma 6 and Lem›ma 7 to equation (62), we obtain
Bφ
Bk
xpx
q “
?1
α kˆI `
21 żαα kk´1 cα
αk 3
´Bs
Bk
xpx
q `O
´d2 p1
p1´
´α
k αq
kl qo 2g2T
¯¯´I d `O
´d p1
´ 1
´α
k αq
tlogT
¯¯dα
˙
181 1 αk´1 α s x d2 1 α 2log2T
I k B k p qdα O p ´ k q
“ ?α
kˆ
` 2
żαk
cα3 Bx ˙`
ˆ
p1 ´α
k
q2
˙
“ ?1 α kˆI ´ 1 1´ ´? αα kk J k px q ˙`O ˆd2 p1 p´ 1 ´α k αq2
k
qlo 2g2T ˙, (63)
where we denote
1
J x : I E X ?α X X x E X ?α X X x J
k p q “ d ` 1 α k ´ k 0 | k “ k ´ k 0 | k “
´
k"
“ ‰´ “ ‰¯
E X k ?α kX 0 X k ?α kX 0 J X k x . (64)
´ ´ ´ | “
”`
˘` ˘
ı*
The details for deriving expression (63) are included in Section A.6.
equI an lito yrd (e 6r 3)to sup gr go ev se tsL te om sm tua d5 ya tn hd
e
Lco ipp se chw ii tt zh pt rh oe ped ri tff yer oe fnc fue nB cBφ tx ik onΦ Jt ´1 .Ñ Fk op rX tt h´ i1 spγ pq uq rp´ oseB B,φ xk weΦ it nÑ trk opX dut cq e, oi un r-
k
` ˘ ` ˘
final auxiliary result, whose proof is provided in Section A.9.
Lemma 8. For 2 t T and x ,x E , J x is locally Lipschitz continuous with respect to x:
t t 1 t t
ď ď p ´ qP p q
1
J
t
x
t 1
J
t
φ x
t
d3 {2log3 {2T x
t 1
φ x
t
2. (65a)
} p ´ q´ p p qq}À ?1 α t } ´ ´ p q}
´
In addition, for 1 k t 1, the Lipschitz constant along the backward trajectory satisfies
ď ď ´
1
J
k
Φ
t 1 k
x
t 1
γ J
k
Φ
t k
x
t
d3 {2log3 {2T Φ
t 1 k
x
t 1
γ Φ
t k
x
t
. (65b)
´ Ñ p ´ p qq ´ Ñ p q À ?1 α t ´ Ñ p ´ p qq´ Ñ p q 2
› › ´ › ›
› ` ˘ ` ˘› › ›
›To proceed, let us again decompose the› quantity of interest as › ›
φ φ 2
E B k Φ X γ B k Φ X
t 1 k t 1 t k t
x ´ Ñ p ´ p qq ´ x Ñ p q
„› B B › 
› ` ˘ ` ˘› 2
› ›E Bφ k Φ t 1 k X t 1 γ Bφ k Φ t k X› ›t 1 X t,X t 1 E t
“ x ´ Ñ p ´ p qq ´ x Ñ p q p ´ qP
„› B B › 
`› › ›E ` Bφ xk Φ t ´1 Ñk pX t ´1˘ pγ
qq
´` Bφ xk Φ t Ñk p˘ X› › ›t q` 2 1 pX t,X t ´1 q˘ PE tc . (66)
„› B B › 
› ` ˘ ` ˘› ` ˘
We shall control each term› respectively. ›
› ›
The first term. Taking Lemma 8 collectively with expression (63), we obtain
φ φ 2
E B k Φ t 1 k X t 1 γ B k Φ t k X t 1 X t,X t 1 E t
x ´ Ñ p ´ p qq ´ x Ñ p q p ´ qP
„› B B › 
p Ài qp1 1› › ›´α αk kq`2 2E J k Φ t ´1 Ñk pX˘ t ´1 pγ qq` ´J k Φ t Ñ˘ k› › › pX` t q 2 1 pX t,X t ´1˘ qPE t ` d4 p1 ´ 1 α k αq4 klo 4g4T
ii p 1´ α q 2d3” l› ›og3` T ˘ ` ˘› ›2 ` ˘ı d4 1p α´ 4lq og4T
p Àqp 1´ αk kq 2 1› α
t
E Φ t ´1 Ñk pX t ´1 pγ qq´Φ t Ñk pX t q› 21 pX t,X t ´1 qPE t ` p ´ 1 k αq
k
4
iii p 1 ´ α q 2dp 3l´ og3Tq ”› › › › ` d4 1 ˘ı α 4logp 4T´ q
p qp ´ k q E ›L2 X γ φ X 21 X ,X› E p ´ k q . (67)
À 1 α
k
2 1 α
t
f t ´1 p q´ p t q 2 p t t ´1 qP t ` 1 α
k
4
p ´ q p ´ q ” › › ` ˘ı p ´ q
Note that, to ensure inequaliti›es i and ii , on›e invokes Lemma 8 which requires x ,x E . We
k k 1 k
shall verify this relation momentarp ilq y. In p iiq we invoke the Lipschitz continuity of Φp ´ anq dP Φ and
t 1 k t k
d p q ´ Ñ Ñ
the property that Φ X X . To further control the right hand side above, recall that we have
t k t t 1
established the inequaÑ litp y (4q 8)“ when´ x ,x in E . As a result, we conclude that
t t 1 t
p ´ q
φ φ 2
E B k Φ X γ B k Φ X 1 X ,X E
t 1 k t 1 t k t t t 1 t
x ´ Ñ p ´ p qq ´ x Ñ p q p ´ qP
„› B B › 
› ` ˘ ` ˘› ` ˘
› ›
› ›
19p1 ´α k q2 p1 ´α t q2L2 fd4log3T p1 ´α k q4d4log4T . (68)
À 1 α 2 1 α 2 ` 1 α 4
k t k
p ´ q p ´ q p ´ q
It is therefore only left for us to show that x ,x in E , which holds true owing to the Lipschitz
k k 1 k
property of Φ x and Φ x . Specificallyp , for ev´ erq y x ,x in E , by definition, it holds for large
t 1 k t k t t 1 t
enough
constan´ tÑc p thq
at
Ñ p q p ´ q
4
x x ?α c d 1 α logT.
t 1 t t 2 4 t
} ´ ´ { } ď p ´ q
a
d
TheLipschitz continuityofΦ alsoimplies that logp x c dlogT asX Φ X . Asaresult,
t Ñk
´
Xkp k
qď
3 k
“
t Ñk
p
t
q
if we define
E
k1
:
“
px k,x
k ´1
qPRd ˆRd ´logp Xkpx
k
qďc 3dlogT, }x
k ´1
´x
k
{?α
k }2
ďc 4L
f
d p1 ´α
t
qlogT ,
" ˇ a *
ˇ
then one can check that Φ ˇ x ,Φ x E . Notice that E and E share the same form for
every 2 k t T, onlyp
wt
i´
t1
hÑ
ak
p
difft
´
e1
rq
entt
cÑ
ok
np
stat
q nq tP in
tk1
he second
conditk
ion,
wek1
conclude that Lemma 6, 7
ď ă ď
and 8 still hold true with slight different constants. Therefore, we have validated the relation (68).
The second term. When x ,x Ec holds true, it is sufficient to consider a crude upper bound for
t t 1
p ´ qP
2
φ φ
B xk Φ t ´1 Ñk px t ´1 pγ
qq ´
B xk Φ t Ñk px t
q
1 px t,x t ´1 qPE tc .
› B B ›
› ` ˘ ` ˘› ` ˘
› ›
O giw vei sng ustoth φeL xipsch Litz ,›c wo hn id ci htio inn tin urA ns ls eu am dspt ti oon1,weknowthat B›BxΦ
t Ñk
px qďL f. Simplychoosingk “t ´1
Bx t
p qď
f
B
2
φ φ
E B xk Φ t ´1 Ñk pX t ´1 pγ
qq ´
B xk Φ t Ñk pX t
q
1 pX t,X t ´1 qPE tc
„› B B › 
ď4L2 f› › ›P pX` t,X t ´1 qPE tc ˘ ` ˘› ›
›
` ˘
ÀL2 fex`p p´c 4dlogT q. ˘ (69)
Putting relations (68) and (69) together verifies the target result in Lemma 5.
A.6 Proof of Claim (63)
To establish this relation, we first find it useful to write the score function as
1
s x E Z ?α X ?1 α Z x
t t 0 t
p q“ ´ ?1 α ` ´ “
„ ´ t ˇ 
1 ˇ
E x ?ˇα X ?α x ?1 α z x
“´1 α ´ t 0 t 0 ` ´ t “
´ t „ ˇ 
1 ˇ
“´1 ´α
t
żx0px ´?α tx 0ˇ qp X0 |Xtpx 0 |x qdx 0. (70)
As a result, the partial derivative is calculated as
s x 1
Br t p qsi B x ?α x p x x dx
Bx
j
“´1 ´α
t
Bx
j
”żx0p i ´ t 0,i q X0 |Xtp 0 | q 0
ı
1
“´1 ´α
t
”1 ti “j u` żx0px i ´?α tx 0,i q BB x jp X0 |Xtpx 0 |x q ıdx 0
1 p x p x x
“´1 ´α
t
”1 ti “j u` żx0px i ´?α tx 0,i q BB x
j
X0p 0 q p XX tt p| xX q0p | 0 q ıdx 0. (71)
20By noticing the fact that
x ?α x
B
x
jp Xt|X0px |x 0 q“p Xt|X0px |x 0
q¨
j ´
1
αt
t
0,j, (72)
B ´
we can thus rewrite equation (71) as
s x 1 1
Br Bt xp jqsi “´ 1 ´α
t
”1 ti “j u` 1 ´α
t
”żx0px i ´?α tx 0,i qp X0 |Xtpx 0 |x qdx 0 żx0px j ´?α tx 0,j qp X0 |Xtpx 0 |x qdx 0
x ?α x x ?α x p x x dx (73)
´ żx0p
i
´
t 0,i
qp
j
´
t 0,j
q
X0 |Xtp 0
| q
0
ıı
1
J x , (74)
“´1 α r t p qsij
t
´
which leads to equation (63).
A.7 Proof of Lemma 6
The proof of Claim (63) provides an explicit expressionof Bst xpx q via J t px
q
as in expression(73). Organizing
terms of expression (73) gives us B
1
∇s x I
t p t q` 1 α d
t
´
1
x ?α x p x x dx x ?α x p x x dx J
“´ p1 ´α
t
q2 ”żx0p t ´ t 0 q X0 |Xtp 0 | t q 0 ´żx0p t ´ t 0 q X0 |Xtp 0 | t q 0
¯
“:At
looooooooooooooooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooooooooooooooon
x ?α x x ?α x p x x dx
´ żx0p
t
´
t 0
qp
t
´
t 0 qJ X0 |Xtp 0
|
t
q
0
ı
“:Bt
looooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooon
and similarly, it holds that
1
∇s g x ,α I
α p t p t qq` 1 α d
´
1
g x ,α ?αx p x g x ,α dx g x ,α ?αx p x g x ,α dx J
“´ p1 ´α q2 ”żx0p t p t q´ 0 q X0 |Xαp 0 | t p t qq 0 ´żx0p t p t q´ 0 q X0 |Xαp 0 | t p t qq 0
¯
“:Aα
looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
g x ,α ?αx g x ,α ?αx p x g x ,α dx .
´ żx0p
t
p
t
q´
0
qp
t
p
t
q´
0 qJ X0 |Xαp 0
|
t
p
t
qq
0
ı
“:Bα
loooooooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooooooon
In view of these two decompositions, we can bound
1 1 1 1
∇s g x ,α ∇s x A A B B . (75)
α p t p t qq´ t p t q ď 1 α 2 α ´ 1 α 2 t ` 1 α 2 α ´ 1 α 2 t
› › › › ›p ´ q p ´ t q › ›p ´ q p ´ t q ›
We shall› ›proceed by controlling ea› ›ch t› ›erm on the right respectivel› ›y. › › › ›
Controlling the first term. Let us start by bounding the first term. By noticing the basic algebra fact
that for vectors z ,z Rd,
1 2
P
z z z z z z max z , z ,
}
1 1J
´
2 2J }2
ď}
1
´
2 }2
¨ t}
1 }2
}
2 }2
u
we find
1 1
A A
1 α 2 t ´ 1 α 2 α
t
›p ´ q p ´ q ›
› ›
› ›
211 1
x ?α x p x x dx g x ,α ?αx p x g x,α dx
À ›1 ´α
t
żx0p t ´ t 0 q X0 |Xtp 0 | t q 0 ´ 1 ´α żx0p t p t q´ 0 q X0 |Xαp 0 | t p qq 0 ›2¨
› ›
› max s α g t x,α 2, s t x t 2 . ›(76)
t} p p qq} } p q} u
´ ¯
By virtue of the bound (58), we can directly derive
dlogT dlogT dlogT
max s g x,α 2, s x 2 max , . (77)
t} α p t p qq}2 } t p t q}2uÀ 1 α 1 α “ 1 α
t t
! ´ ´ ) ´
It is then sufficient to controlthe first term on the righthand side of inequality (76), which shall be done as
follows. To this end, let us define a set of interest by
E : x: x ?α x c d 1 α logT .
0 t t 2 6 t
“ } ´ } ď p ´ q
a (
We first consider the the following term
x ?α x p x x dx x ?α x p x g x,α dx
›żx0p
t
´
t 0
q
X0 |Xtp 0
|
t
q
0
´ żx0p
t
´
t 0
q
X0 |Xαp 0
|
t
p qq
0
›2
› ›
› p x g x ,α p x x x ?α x dx ›
ď żx0 PE0
ˇ
X0 |Xαp 0 | t p t qq´ X0 |Xtp 0 | t q ˇ¨
›
t ´ t 0 ›2 0
ˇ p x g x ,α p ˇ x› x x › ?α x dx
` żx0 PE 0c
ˇ
X0 |Xαp 0 | t p t qq´ X0 |Xtp 0 | t q ˇ¨
›
t ´ t 0 ›2 0
“ żx0 PE0ˇ
ˇp X0 p| XX 0α p|p Xˇx t0 p| xg 0t |px xxt t, qα gqq
x´
,1
αˇ
ˇ¨p X0 |Xtpx 0 |x t
q¨
›x tˇ ´› ?α tx 0 ›2dx 0›
ˇ ˇ żx0 PE 0cˇ
ˇ
X0 p| XX 0α |p Xt0 p| x 0t |p xt t q ˇ ˇqq ´1 ˇ ˇ¨p X0 |Xtpx 0› |x t q¨ ›x t ´› ?α tx 0 ›2dx 0. (78)
ˇ ˇ › ›
Next, we bound the right haˇnd side above. Towards ˇthis, first recall that in Claim 2 in Li et al. (2023,
Appendix C.1), it has been shown by direct calculations that
p X0 |Xαpx 0 |g t px t,α qq 1 O d p1 ´α t qlogT , if x
0
E 0, (79)
p x x “ ` 1 α P
X0 |Xtp 0 | t q ˆ ´ t ´1 ˙
2
p x g x ,α 16c x ?α x logT
X0 |Xαp 0 | t p t qq exp 1 t ´ t 0 2 , if x
0
E 0. (80)
p X0 |Xtpx 0 |x t
q
ď ˜ ›
›
p1 ´α t qT ›
›
¸ R
Here,weremarkthatwereplace α α x inLi et al.(2023)by g x ,α . This isvalidsincefor x ,x
t t t t t t 1
E , inequality (56) ensures { p q p ´ qP
t
a
α d1 2 1 α log1 2T
›cα tx t ´g t px t,α q ›2 “O
ˆ
{ p p1´ ´αt tq q1 {2 { ˙.
› ›
This approximationonly lead›s to a lower order›term in our final result.
Plugging the relations (79) and (80) into the right hand side of (78) and following the proof of (161c) in
the proof in Li et al. (2023, Appendix C.1), we can obtain
x ?α x p x x dx x ?α x p x g x,α dx
›żx0p
t
´
t 0
q
X0 |Xtp 0
| q
0
´ żx0p
t
´
t 0
q
X0 |Xαp 0
|
t
p qq
0
›2
› d 1 α logT ›
› p ´ t q E ?α X x X x ›
À 1 α ¨ t 0 ´ t 2 t “ t
t 1
´ ´
d3 {2 p1 ´α t qlog3 {2T“› › , › › ˇ ˇ ‰ (81)
À 1 α 1 2
p ´ t q {
22where we apply Lemma 1 to deduce the last inequality. With the same calculations, we can similarly find
x ?α x x ?α x p x x dx x ?α x x ?α x p x g x,α dx
›żx0p
t
´
t 0
qp
t
´
t 0 qJ X0 |Xtp 0
|
t
q
0
´ żx0p
t
´
t 0
qp
t
´
t 0 qJ X0 |Xαp 0
|
t
p qq
0
›
› ›
À
d p1 1´α αt qlogT ¨E ?α tX
0
´x
t
2
2
X
t
“x
t
› ›
t 1
À
d2 p1´ ´α 1t´ qp1 α´α t q” lo› ›g2T Àd2 p1› › ´ˇ ˇ α
t
qlog2Tı . (82)
t 1
p ´ ´ q
With these properties in place, we are ready to prove Lemma 7 by studying the first term in (76). First,
notice that
1 1
x ?α x p x x dx g x ,α ?αx p x g x ,α dx
›1 ´α t żx0p t ´ t 0 q X0 |Xtp 0 | t q 0 ´ 1 ´α żx0p t p t q´ 0 q X0 |Xαp 0 | t p t qq 0 ›2
› ?α ›
› p x g x ,α x ?α x dx p x x x ?α x dx ›
ď ?α
t
p1 ´α
q›żx0
X0 |Xαp 0 | t p t qqp t ´ t 0 q 0 ´
żx0
X0 |Xtp 0 | t qp t ´ t 0 q 0
›2
› ›
?›α 1 ›
`
›
›´?α
t
p› 1 ´α
q
´ 1 ´α
t
¯żx0p X0 |Xtpx 0 |x t qpx t ´?α tx 0 qdx 0
› ›2
›
› 1 α ›
› ` 1 ´α ›żx0p X0 |Xtpx 0 |x t q ˆg t px t,α q´ cα tx t ˙dx 0
›2
›
1 › ›
› ›
ď ?α
t
p1 ´α
t ´1
q› ›żx0p X0 |Xαpx 0 |g t px t,α qqpx t ´?α tx 0 qdx 0 ´› żx0p X0 |Xtpx 0 |x t qpx t ´?α tx 0 qdx 0
›2
1 › 1 ›
› p x x x ?α x dx ›
` ´?α
t
p1 ´α›
t ´1
q
´ 1 ´α
t ¯żx0
X0 |Xtp 0 | t q} t ´ t 0 }2 0 ›
1 α
p x x g x ,α x dx .
` 1 ´α
t ´1 żx0
X0 |Xtp 0 | t q
›
t p t q´ cα
t
t
›2
0
› ›
Now applying the inequality (81), Lemma 1›, and (56) on each›term above separately, we achieve
› ›
1 1
x ?α x p x x dx g x ,α ?αx p x g x ,α dx
›1 ´α t żx0p t ´ t 0 q X0 |Xtp 0 | t q 0 ´ 1 ´α żx0p t p t q´ 0 q X0 |Xαp 0 | t p t qq 0 ›2
› 1 d3 2 1 α log3 2T 1 α ›
› À 1 ´α
t
¨ { p p1´ ´αt tq q1 {2 { ` ?αp
t
p1´ ´t αq
t
q2E ?α tX 0 ´x t 2 X t “x t ›
“› › ˇ ‰
› 1 › ˇ α
sup x g x ,α
` 1 ´α t αtăα ăαt´1 ›cα t t ´ t p t q ›2
d3 2 1 α log3 2T d1 2 1 α log1 2T d1 2 1 α log1 2T › ›
{ t { { t { { t { › ›
p ´ q p ´ q p ´ q
À 1 α 3 2 ` 1 α 3 2 ` 1 α 3 2
p ´ t q { p ´ t q { p ´ t q {
d3 2 1 α log3 2T
{ p ´ t q { . (83)
À 1 α 3 2
p ´ t q {
Finally, plugging inequalities (77) and (83) into expression (76) leads to
1 1 d3 2 1 α log3 2T d1 2log1 2T
1 α 2A α ´ 1 α 2A t À { p 1´ αt q 3 2 { ¨ 1{ α { 1 2
›p ´ q p ´ t q › p ´ t q { p ´ t q {
› › d2 1 α log2T
› › p ´ t q . (84)
À 1 α 2
t
p ´ q
Controlling the second term. With expression (82), we can further control the quantity 1 B
} 1 α 2 α ´
1 B . By similar analysis, we can obtain p ´ q
p1 ´αtq2 t }
1 1
B B
1 α 2 α ´ 1 α 2 t
t
›p ´ q p ´ q ›
› ›
› ›
231
x ?α x x ?α x p x x dx
À α
t
p1 ´α
t
q2 ›żx0p t ´ t 0 qp t ´ t 0 qJ X0 |Xtp 0 | t q 0
› d 1 α logT
›
´
żx0px
t
´?α tx
0
qpx
t
´?α tx
0
qJp
X0
|Xαpx
0
|g
t
px,α qqdx
0 ›`
p p1´ ´t αq
t
q2
d2 1 α log2T ›
p ´ t q . › (85)
À 1 α 2
t
p ´ q
In summary, taking the relations (84) and (85) collectively with inequality (75), we obtain the following
bound
1 1 1 1
∇s g x ,α ∇s x A A B B
α p t p t qq´ t p t q ď 1 α 2 α ´ 1 α 2 t ` 1 α 2 α ´ 1 α 2 t
› › ›
›
› › ›
›
› › ›dp 2 p1´ ´αq t qlog2Tp ,´ t q › › › › › ›p ´ q p ´ t q › › ›
À 1 α 2
t
p ´ q
which leads to the final result.
A.8 Proof of Lemma 7
The proof of this lemma is similar to that of Lemma 4. In particular, we shall prove this result by contra-
diction. Specifically, suppose that there exists α α ,α such that Lemma 7 does not hold. Then, one
t t 1
P r ´ s
can define
g x ,α d 1 α logT
α: “min α Prα t,α t ´1 s: B t p xt q ´I Á p ´ 1 t αq
t
.
! › B › ´ )
With this definition of α, it holds that for all α › α α, one›has
p t ´›1 ě ą ›
g x ,α
p B t p t q 1 O d p1 α t logT . (86)
x “ ` p p ´ q q
› B ›
Now consider the partial deriva›tive of g x›,α at α where
› t p › q
g x,α 1 1 α α g x ,α
B t Bp
x
q ´I d
“ ´?α t
´1 ¯I d `p 2?αp
t żαp
tcα3t∇s α pg t px t,α qqB t p Bxt qdα.
p
The proof in Lemma 4 ensures that
α
g x ,α x c d 1 α logT
t t t 5 t
p q´ α ď p ´ q
c t
› › a
for x ,x E . Thus, the an›alysis in the proof›of (161a) in Li et al. (2023, Appendix C.1) guarantees
t t 1 t › ›
p ´ q P
that
1 α ∇s g x ,α I dlogT, (87)
α t t d
p ´ q p p qq´ À
which directly implies that › ›
› ›
dlogT
∇s g x ,α ,
α p t p t qq À 1 α
t
´
› ›
Combining these results together, we›obtain ›
g x,α 1 1 α α g x ,α
› › ›B t Bp x pq ´I › › ›ď ˇ ˇ ˇ? 1α t ´1 1ˇ ˇ ˇ` 2?α dt lż oα gp t Tcα3t › ›∇ αs t´α 1pg t p αx t t, dα αqq › ›› › ›B t p Bxt q › › ›dα
ď ˇ?α
t
´ ˇ` ›2?α
t
p1 ´α
t qżαt
cα3
›
ˇd 1 α ˇlog›T ›
ˇ p ´ t qˇ › , ›
À 1 α
t
´
which contradicts the definition of α.
p 24A.9 Proof of Lemma 8
Define S: u Rd : u 1 . We first prove that for any u Sd 1 and any x ,x E ,
2 ´ t t 1 t
“t P } } “ u P p ´ qP
1
∇ xu JJ
t
px
t
qu
2 À ?1 α
d3 {2log3 {2T, (88)
t
´
› ›
where ∇ u J x u: ∇ u J x› u . ›
x J t p t q “ x J t p q x “xt
ˇ
Proof of relation (88). Recall thaˇt in Section A.6, we have shown that
1
s x x ?α x p x x dx ,
t p q“´1 ´α t żx0p ´ t 0 q X0 |Xtp 0 | q 0
s x
J t x 1 α t B t p q.
p q“´p ´ q x
B
In view of these two relations and the definition of J , we can write u J x u as
t J t t
p q
u JJ
t
px
t
qu “1
` 1
1
α
E pX
t
´?α tX
0
qJu |X
t
“x
t
2 ´E X
t
´?α tX
0
Ju 2
2
|X
t
“x
t
.
´ t"´ “ ‰¯ ”“` ˘ ‰ ı*
To further control ∇ u J x u, let us consider the two terms on the right hand side separately.
x J t t
p q
• For the first term, one has
2 2
∇ E X ?α X u X x ∇ x ?α x u p x x dx
›
›
xt
´
“p
t
´
t 0 qJ
|
t
“
t
‰¯ ›
›2 “
›
›
xt
´żx0 “p
t
´
t 0 qJ
‰
X0 s|Xt
xp
0
|
t
q
0
¯ ›
›2
› › ›2 1 α t s t x t Ju 1 α t B t p t q ›
“ p ´ qp p q q¨p ´ q x 2
› s xB ›
› ›1 α t s t x t 2 1 α t B t p t q . › › (89)
Àp ´ q} p q} ¨ p ´ q x
› B ›
› ›
By equation (73), we can compute that › ›
s x
1 α t B t p t q
p ´ q x
› 1B ›
› 1 › x ?α x p x x dx x ?α x p x x dx J
› ď ` 1 ´α
t
›żx› 0p t ´ t 0 q X0 |Xtp 0 | t q 0 ´żx0p t ´ t 0 q X0 |Xtp 0 | t q 0
¯
›
´
żx› 0px
t
´?α tx
0
qpx
t
´?α tx
0
qJp
X0
|Xtpx
0
|x
t
qdx
0
›
1 2 ›
1 x ?αx p x x dx ›
À ` 1 ´α t ›żx0p t ´ 0 q X0 |Xtp 0 | t q 0 ›2
1 1 › ›E ?α X x 2 X x d› ›logT. (90)
ď ` 1 α X0 t 0 ´ t 2 t “ t À
t
´ ”› › ˇ ı
Here, in the second inequa› lity, we use t› heˇ fact that for a column vector Z Rd, we have
P
E ZZ J ´E rZ sE rZ sJ “ E Z ´E rZ s Z ´E rZ s J ď E ZZ J ďE ZZ J “E }Z }2 2 ,
and› ›th“ e last‰ line invokesL› ›emm› › ›a” 1` . Now plu˘ g` ging the bo˘ unı› › ›ds in› › in“ equal‰ i› ›ty (58“ )› ›and (› ›9‰ 0) int“ o ineq‰ ual-
ity (89), we obtain
∇
xt
E pX
t
´?α tX
0
qJu |X
t
“x
t
2
2
Àp1 ´α
t
q1 2d3 {2log3 {2T. (91)
›
›
´
“
‰¯ ›
›
› ›
25• When it comes to the second term, some direct calculations give
∇ xtE X
t
´?α tX
0
Ju 2
2
|X
t
“x
t 2
›
›∇
”“`
x ?α x ˘ u‰2 p
xı›
›x dx
“
›› xt
żx0 p
t
´
t 0 qJ X0 |Xtp 0 |› t
q
0
›2
› “ ‰ › 2
ď› ›2 żx0 px t ´?α tx 0 qJu u ¨p X0 |Xtpx 0 |x t qdx 0 ›› 2` ›żx0 px t ´?α tx 0 qJu BB x tp X0 |Xtpx 0 |x t qdx 0 ›2
› “ ‰ › › “ 2 ‰ ›
ď›2 żx0}px t ´?α tx 0 q}2 ¨p X0 |Xtpx 0 |x t qdx 0 ` ›› żx0 p›x t ´?α tx 0 qJu BB x tp X0 |Xtpx 0 |x t qdx 0 ›2 ›
Àp1 ´α t q1 3d1 {2log1 {2T `
›żx0
px t ´?α tx 0 qJ› ›u 2 B“ B x tp X0 |Xtpx 0 |x t q‰ dx 0 ›2, › › (92)
› “ ‰ ›
where we use Lemma 1 to ›obtain the last inequality. To further bound t›he second term in inequality
(92), we repeat the calculations for equations (71) and (72) and deduce that
2
›żx0 px t ´?α tx 0 qJu BB x tp X0 |Xtpx 0 |x t qdx 0 ›2
› 1“ ‰ 2 ›
ď
››
1 ´α t żx0
px
t
´?α tx
0
qJu px
t
´?α tx
0
qp X›
0
|Xtpx
0
|x
t
qdx
0 ›2
› 1“ ‰ 2 ›
› x ?α x u p x x dx x› ?α x p x x dx
` ›1 ´α t żx0 p t ´ t 0 qJ X0 |Xtp 0 | t q 0 ¨ żx0p t ´ t 0 q X0 |Xtp 0 | t q 0 ›2
1 › “ ‰ ›
›E x ?α x 3 X x s x E x ?α x 2 X x . ›(93)
À 1 α t } t ´ t 0 }2| t “ t ` t p t q } t ´ t 0 }2| t “ t 2
ˇ ´ ˇ › ›
ˇ “ ‰ˇ › “ ‰›
ˇ ˇ › ›
Taking colelctively the inequalities (92) and (93), we arrive at
∇ xtE X
t
´?α tX
0
Ju 2
2
|X
t
“x
t
Àp1” ´“` α
t
q1 3d1 {2log1 {˘2T ‰
` 1
1
α
tEı }x
t
´?α tx
0
}3 2|X
t
“x
t `
s
t
px
t
qE }x
t
´?α tx
0
}2 2|X
t
“x
t 2
Àp1 ´α
t
q1 2d3 {2log3 {2T. ˇ ˇ
ˇ
´ “ ‰ˇ ˇ
ˇ
› ›
›
“ ‰› › ›(94)
where last inequality is a direct consequence of Lemma 1. Therefore, combining the two relations (91) and
(94) yields the claimed relation (88).
Next, we shall proceed to show that similar to the relation (88), one also has
1
∇ xu JJ
t
px
t ´1
pγ qqu
2 À ?1 α
td3 {2log3 {2T, (95)
´
› ›
which holds for every 0 γ 1›. ›
ď ď
Proof of inequality (95). Wemaketheobservationthatthederivationsabovetoproverelation(88)only
involves X x which satisfies the first condition in the definition of E , namely, logp x c dlogT.
t
“
t t
´
Xtp t
qď
3
Now,let us provethat logp x γ 2c dlogT for x γ . Similaras inderivinginequality (48),we
´
Xtp t ´1
p qqď
3 t ´1
p q
can deduce
x 1 1 αt´1 α
x φ x x t 1 s x t s g x ,α s x dα
} t ´1 ´ t p t q}2 À
› ›
t d´1 1´ α?α t
lo›
›g2 T`
›
›´? dα t 1´
α¯
2t p lot gq
›
›T2`
›
›2?α t żαt cα3
`
α p t p t qq´ t p t q
˘ ›
›2
› t › › t › › ›
À p ´ q ` p ´ q
a a1 αt´1 α tdα sup s g x ,α s x
` 2?α
t żαt
cα3
´αtăα
ăαt´1} α p t p t qq´ t p t q}2
¯
r
r
26d 1 α logT d 1 α 2logT 1 α 2
dlogT 3 {2
À p ´ t q ` p ´ t q `p ´ t q 1 α
ˆ ´ t˙
a a
d 1 α logT, (96)
t
À p ´ q
a
where we use inequality (57) in the third line. Since x γ : γx 1 γ φ x and inequality (96),
t 1 t 1 t t
´ p q “ ´ `p ´ q p q
we can directly recognize that
x γ φ x d 1 α logT. (97)
t 1 t t 2 t
} ´ p q´ p q} À p ´ q
a
Putting these two relations above together, it is easily seen that
x x γ ?α c d 1 α logT. (98)
t 1 t 1 t 4 t
´ ´ ´ p q{ ď p ´ q
In addition, in view of Lemma 2› ›, we know that for › ›x t,x
t
a
1
E
t
and any γ 0,1 , it holds that
p ´ qP Pr s
d 1 α logT
´logp Xt´1px t ´1 pγ qqď2c 3dlogT and p Xt´1px q“ ˆ1 `O ´d p ´ 1 ´t αq t ¯˙p Xtpx q. (99)
From properties (98) and (99), we conclude x γ ,x E . It thus enables us to apply the same
t 1 t 1 t
analysis as above on J x γ , and draw the cp onc´ lup sioq n th´ atq P
t t 1
p ´ p qq
1
∇ u J x γ u d3 2log3 2T
x J t p t ´1 p qq 2 À ?1 α
t
{ {
´
› ›
for 0 γ 1. We complete the› proof of the inequa› lity (95).
ď ď
In Summary. Based on expression (88), some direct calculations yield
1
J
t
x
t 1
J
t
φ x
t
sup u
J
J
t
x
t 1
J
t
φ x
t
u d3 {2log3 {2T x
t 1
φ x
t
2, (100)
} p ´ q´ p p qq}ď u Sd´1 p ´ q´ p q À ?1 α t } ´ ´ p q}
P ˇ ˇ ´ ` ˘¯ ˇ ˇ ´
whichconcludestheproofofinequˇality(65a). Inaddition,asdˇiscussedaftertheinequality(68),theLipschitz
condition of Φ x allows us to prove x ,x E . Repeating the analysis above, we can conclude
t 1 k k k 1 k
´ Ñ p q p ´ q P
that
1
J Φ x γ J Φ x d3 2log3 2T Φ x γ Φ x ,
k t 1 k t 1 k t k t { { t 1 k t 1 t k t
´ Ñ p ´ p qq ´ Ñ p q À ?1 α t ´ Ñ p ´ p qq´ Ñ p q 2
› › ´ › ›
› ` ˘ ` ˘› › ›
which›thus completes the proof of inequality›(65b). › ›
References
Anderson,B.D.(1982). Reverse-timediffusionequationmodels. StochasticProcesses andtheirApplications,
12(3):313–326.
Benton,J.,DeBortoli,V.,Doucet,A.,andDeligiannidis,G.(2023a).Linearconvergenceboundsfordiffusion
models via stochastic localization. arXiv preprint arXiv:2308.03686.
Benton, J., Deligiannidis, G., and Doucet, A. (2023b). Error bounds for flow matching methods. arXiv
preprint arXiv:2305.16860.
Block, A., Mroueh, Y., and Rakhlin, A. (2020). Generative modeling with denoising auto-encoders and
Langevin sampling. arXiv preprint arXiv:2002.00107.
Chen, H., Lee, H., and Lu, J. (2022a). Improved analysis of score-based generative modeling: User-friendly
bounds under minimal smoothness assumptions. arXiv preprint arXiv:2211.01916.
27Chen,S., Chewi,S., Lee,H.,Li,Y., Lu,J.,andSalim,A.(2023a). The probabilityflowodeis provablyfast.
arXiv preprint arXiv:2305.11798.
Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022b). Sampling is as easy as learning the
score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.
Chen, S., Daras, G., and Dimakis, A. G. (2023b). Restoration-degradation beyond linear diffusions: A
non-asymptotic analysis for DDIM-type samplers. arXiv preprint arXiv:2303.03384.
De Bortoli, V. (2022). Convergence of denoising diffusion models under the manifold hypothesis. arXiv
preprint arXiv:2208.05314.
DeBortoli,V.,Thornton,J.,Heng,J.,andDoucet,A.(2021).DiffusionSchrödingerbridgewithapplications
toscore-basedgenerativemodeling. Advances in Neural Information Processing Systems,34:17695–17709.
Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in neural
information processing systems, 34:8780–8794.
Ding,Z.andJin,C.(2023). Consistencymodelsasarichandefficientpolicyclassforreinforcementlearning.
arXiv preprint arXiv:2309.16984.
Haussmann, U. G. and Pardoux, E. (1986). Time reversal of diffusions. The Annals of Probability, pages
1188–1205.
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M.,
Fleet, D. J., et al. (2022). Imagen video: High definition video generation with diffusion models. arXiv
preprint arXiv:2210.02303.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840–6851.
Hyvärinen, A. (2005). Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(4).
Karras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based
generativemodels. InAdvances in NeuralInformation ProcessingSystems,volume35,pages26565–26577.
Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon,
S. (2023). Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv
preprint arXiv:2310.02279.
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2020). Diffwave: A versatile diffusion model
for audio synthesis. arXiv preprint arXiv:2009.09761.
Lee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory, pages 946–985.
Li,G.,Huang,Y.,Efimov,T.,Wei,Y.,Chi,Y.,andChen,Y.(2024).Acceleratingconvergenceofscore-based
diffusion models, provably.
Li,G.,Wei,Y.,Chen,Y.,andChi,Y.(2023). Towardsfasternon-asymptoticconvergencefordiffusion-based
generative models. arXiv preprint arXiv:2306.09251.
Liu, X., Wu, L., Ye, M., and Liu, Q. (2022). Let us build bridges: Understanding and extending diffusion
generative models. arXiv preprint arXiv:2208.14699.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. (2022a). DPM-Solver: A fast ODE solver for
diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing
Systems, 35:5775–5787.
28Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. (2022b). DPM-Solver++: Fast solver for guided
sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095.
Luhman, E. and Luhman, T. (2021). Knowledge distillation in iterative generative models for improved
sampling speed. arXiv preprint arXiv:2101.02388.
Meng,C.,Rombach,R.,Gao,R.,Kingma,D.,Ermon,S.,Ho,J.,andSalimans,T.(2023). Ondistillationof
guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14297–14306.
Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International
Conference on Machine Learning, pages 8162–8171.
Pidstrigach, J. (2022). Score-based generative models detect manifolds. arXiv preprint arXiv:2206.01018.
Popov,V.,Vovk,I.,Gogoryan,V.,Sadekova,T.,andKudinov,M.(2021).Grad-tts: Adiffusionprobabilistic
model for text-to-speech. In International Conference on Machine Learning, pages 8599–8608.PMLR.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image
generation with CLIP latents. arXiv preprint arXiv:2204.06125.
Rombach,R., Blattmann, A., Lorenz,D., Esser,P., andOmmer,B.(2022). High-resolutionimagesynthesis
with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10684–10695.
Salimans,T. andHo,J. (2022). Progressivedistillationfor fast samplingof diffusionmodels. arXiv preprint
arXiv:2202.00512.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–
2265.
Song, J., Meng, C., and Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502.
Song, Y. and Dhariwal, P. (2023). Improved techniques for training consistency models. arXiv preprint
arXiv:2310.14189.
Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. (2023). Consistency models.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32.
Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. Advances
in neural information processing systems, 33:12438–12448.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Score-based
generativemodeling throughstochasticdifferential equations. International Conference on Learning Rep-
resentations.
Sun, W., Chen, D., Wang, C., Ye, D., Feng, Y., and Chen, C. (2023). Accelerating diffusion sampling
with classifier-basedfeature distillation. In 2023 IEEE International Conference on Multimedia and Expo
(ICME), pages 810–815.IEEE.
Tang, W. (2023). Diffusion probabilistic models. preprint.
Tang,W.andZhao,H.(2024). Contractivediffusionprobabilisticmodels. arXiv preprint arXiv:2401.13115.
Vincent, P. (2011). A connectionbetween score matching and denoising autoencoders. Neural computation,
23(7):1661–1674.
29Wang, X., Zhang, S., Zhang, H., Liu, Y., Zhang, Y., Gao, C., and Sang, N. (2023). Videolcm: Video latent
consistency model. arXiv preprint arXiv:2312.09109.
Xue, S., Yi, M., Luo, W., Zhang, S., Sun, J., Li, Z., and Ma, Z.-M. (2023). SA-Solver: Stochastic Adams
solver for fast sampling of diffusion models. arXiv preprint arXiv:2309.05019.
Zhang,Q.andChen,Y.(2022).Fastsamplingofdiffusionmodelswithexponentialintegrator.arXivpreprint
arXiv:2204.13902.
Zhao,W., Bai,L.,Rao,Y., Zhou,J.,andLu, J.(2023). UniPC: Aunified predictor-correctorframeworkfor
fast sampling of diffusion models. arXiv preprint arXiv:2302.04867.
30