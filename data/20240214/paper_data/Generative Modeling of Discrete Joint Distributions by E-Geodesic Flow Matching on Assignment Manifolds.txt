Generative Modeling of Discrete Joint Distributions by
E-Geodesic Flow Matching on Assignment Manifolds
BastianBoll1 DanielGonzalez-Alvarado1 ChristophSchnörr1
1InstituteforMathematics,ImageandPatternAnalysisGroup,HeidelbergUniversity,Germany
Abstract
samples.Theconcreteformofthetime-variantconditional
measuresgeneratedinthiswaydetermineawholeclassof
flow-matchingapproachesandtheformofthelocalapprox-
This paper introduces a novel generative model imationofthedatadistribution.Eveninthesimplestcaseof
fordiscretedistributionsbasedoncontinuousnor- basicGaussianmeasureswithtime-variantparameters,the
malizingflowsonthesubmanifoldoffactorizing approachsubsumesdiffusionpathsandhenceprovidesan
discretemeasures.Integrationoftheflowgradually attractivealternativetocurrentpracticeofdiffusion-based
assignscategoriesandavoidsissuesofdiscretizing approaches.MoresophisticatedpathsbyoptimalGaussian
the latent continuous model like rounding, sam- measuretransportMcCann[1997],Takatsu[2010]canbe
pletruncationetc.Generalnon-factorizingdiscrete easilyadoptedandfurtherimprovethemethod.
distributionscapableofrepresentingcomplexsta-
Ourapproachiscloselyrelatedtotherecentextensionofthe
tistical dependencies of structured discrete data,
flowmatchingapproachtoRiemannianmanifoldsChenand
canbeapproximatedbyembeddingthesubman-
Lipman[2023].Itprovidesagenerativemodelfordiscrete
ifoldintoathemeta-simplexofalljointdiscrete
orcategorialdata,whichdefinesanotheractivesubareaof
distributionsanddata-drivenaveraging.Efficient
research (cf. Section 2). This seamless combination of a
trainingofthegenerativemodelisdemonstratedby
Riemanniangeometricstructurefortherespresentationand
matchingtheflowofgeodesicsoffactorizingdis-
generationofdiscretedata,andtheflow-matchingapproach
cretedistributions.Variousexperimentsunderline
forefficientandstabletraining,appearstobenewinthelit-
theapproach’sbroadapplicability.
erature.Ourapproachutilizes,inparticular,theembedding
approachofcategorialdistributionsrecentlystudiedbyBoll
1 INTRODUCTION etal.[2023,2024].
Specifically,asillustratedforatoyproblembyFigure1,we
Generative models Kobyzev et al. [2021], Papamakarios
designandlearnadynamicalsystemwhoseflowevolves
etal.[2021]defineanactiveareaofresearch.Theyinclude, onthesubmanifoldofallfactorizingdiscretedistributions.
inparticular,deepdiffusionmodelsSongetal.[2021],Yang This submanifold is embedded into the meta-simplex of
etal.[2023].Thesimplediffusionprocessesinvolved,how- all (possible) discrete distributions of n discrete random
ever,leadtolongtrainingtimesandspecializedalgorithmic variables,eachofwhichtakescvalues.Sincetheextreme
methodsarerequiredforefficientsampling.
pointsoftheembeddedsubmanifoldandtheambientsim-
Asanalternative,therecentpaperLipmanetal.[2023]in- plexcoincide,pushingforwardabasicreferencemeasure–
troducestheflowmatchingapproachtogenerativemodeling asillustratedbyFigure2–to(possiblyasubsetof)these
which enables more stable and efficient training. This is extremepoints,entailsviatheflow-matchingtrainingob-
achievedbytakingaslossfunctiontheexpecteddistance jectiveaconvexcombinationoftheextrempointmeasures,
of a time-variant parametrized vector field to the vector eachofwhichrepresentsahardcategoryassignmenttoeach
fieldwhichgeneratesthetime-variantmarginaldistribution discrete random variable. As a result, we implicitly have
p , t∈[0,1],thatconnectsareferencemeasurep andthe a‘universalityproperty’inthat(theoretically)anydiscrete
t 0
targetdistributionp .Thekeyinsightwhichmakestheap- jointdistributioncanberepresentedandbesampledfrom
1
proach efficient is that the loss function gradients can be usingourapproach.
computed,byaveragingoverthesampleset,thedistanceto
thegeneratingvectorfieldsconditionedonindividualdata
1
4202
beF
21
]GL.sc[
1v64870.2042:viXraFigure1:Thetetrahedronrepresentsinlocalcoordinatesall Figure 2: Visualization of 1000 samples from the target
jointdistributionsw ∈∆4(with4=cn)ofn=2variables distribution(bluepoint;cf.Figure1).Eachsamplecorre-
takingc = 2values.Theembeddedsurfaceistheassign- spondstoanintegralcurveT(W(t))(6)oftheassignment
mentmanifoldW ofallfactorizingdistributionsof2binary flowODE(1)ontheembeddedsubmanifoldoffactorizing
variables.Thebluepointrepresentsatargetjointdistribution distributionsW ⊆ S ,whichcanbecomputedefficiently
4
p(y 1,y 2)= 101 0(45,5,5,45)⊤withstrongstatisticaldepen- bygeometricintegration.Theentireassignmentflowpushes
dency,i.e.itisnotclosetotheanyfactorizingdistribution. forwardastandardGaussianreferencedistributiononthe
Thispaperintroducesagenerativemodelforrepresenting tangentspaceatthebarycenter(redpoint),whichislifted
arbitrary discrete distributions as convex combination of tothesubmanifoldandtransportedtotheextremepoints.
hardcategoryassignmentdistributionscorrespondingtothe Theresulting‘weights’representthebluetargetdistribution
extremepoints.Figure2illustratestherepresentationofthe as convex combination. The parametrized vector field of
targetdistribution(bluepoint). thegenerativemodelistrainedinastableandefficientway
bymatchinge-geodesiccurvesontheassignmentmanifold,
whichrepresentthetrainingdataandcanbecomputedin
Insummary,thispapercontributes closedform.
• anovelcontinuousnormalizingflowmodelofdiscrete
jointdistributions;
2.1 GENERATIVEMODELSONMANIFOLDS
• ageometricRiemannianrepresentationoftime-variant
push-forward measures which is efficient and stable
Our work is inspired by and closely related to Chen and
learnablebasedonmatchinggeodesicflows;
Lipman[2023].Thisworkisbasedontheflow-matching
• anovelapproachtodata-drivenapproximationsofgen- approachtogenerativemodelingintroducedintherecent
eraldiscretedistributionsbysubmanifoldembedding paper Lipman et al. [2023], which enables more stable
andaveraging. andefficienttrainingofcontinuousnormalizingflowsand
henceprovidesanattractivealternativetomaximumlike-
Section2reportsrelatedworkongenerativemodelsonman- lihood learning. The paper Chen and Lipman [2023] ex-
ifoldsandfordiscretedistributions,respectively.Section3 tendstheflow-matchingapproachtoRiemannianmanifolds
specifiesthedynamicalsystemstailoredtodiscretedistribu- anddistinguishestwoscenarios:simplemanifoldsonwhich
tionsandtheembeddingofthesubmanifoldoffactorizing geodesics can be computed in closed form, and general
distributions in the meta-simplex. Our approach is intro- (non-simple)manifoldswhereapproximatedistanceslike
ducedinSection4.Experimentalresultsarediscussedin thetruncateddiffusiondistanceareproposedinstead.
Section5.WeconcludeinSection7andprovidesupplemen-
Our approach adds the statistical (assignment) manifold
talmaterialintheappendix.
W withthecorrespondingFisher-Raogeometrytothelist
of simple manifolds in Chen and Lipman [2023]. In ad-
dition,weproposeagenerativemodelforapproximating
2 RELATEDWORK
joint distributions of discrete random variables using the
flow-matchingapproach,whichessentiallyrestsuponthe
Wedistinguishtwoareasofrelatedresearch:
embedding T(W) ⊂ S of the assignment manifold W
N
inthemeta-simplexS (Figures1and3).Tothebestof
• generativemodelsonmanifolds,and N
ourknowledge,ourgeometricapproachtocombiningthese
• generativemodelsofdiscretedistributions. threeaspectsappearstobenovelintheliterature.
2Sinceourapproachutilizesflow-matchingofe-geodesics Gavves[2021]usesafactorizingdecoder,i.e.thecatego-
(i.e. autoparallel curves with respect to the e-connection; rialvariablesareconditionallyindependentgiventhelatent
cf.AmariandNagaoka[2000]),whichcanbecomputedin variables. Authors concede that this tends to limit model
closedform,itsharestheadvantagesofgenerativemodels expressivity.
fordistributionsonothersimplemanifoldsproposedsofar,
ChenetalChenetal.[2022]proposeagenerativemodel
overrelatedworkusingalternativemethodsChenandLip-
for discrete distributions p(y) = p(y ,...,y ) of n cate-
man[2023]:significantlymorestableandefficienttraining 1 n
gorial variables, each taking c values, based on continu-
incomparisontousingthemaximumlikelihoodcriterion i
ous normalizing flows and quantisation of the generated
(e.g.,Louetal.[2020],MathieuandNickel[2020]),better
probabilitydistributionp(x),x ∈ RnD (cf.,e.g.,Grafand
scalabilitytohighdimensionsthan,e.g.,Ben-Hamuetal.
Luschgy[2000],Gruber[2004])whosegeometryinterms
[2022],Rozenetal.[2021],andnoneedforincorporating
of the Voronoi partition can be learned for each variable.
acostlysimulationsubroutineandapproximationintothe
Anadditionalbenefitofthisapproachisthatforeachcell
training procedure, as required in connection with exten-
a distribution q(x|y ) is learned which enables dequanti-
sions of diffusion-based generative models to manifolds i
zationy (cid:55)→ xafterlearningthediscretedistributionp(y).
(e.g.,Huangetal.[2022],DeBortolietal.[2022]).
ThedimensionD ofthecontinuouslatentdomainranges
from2...6inthepaperandthebestchoiceofDandthe
numbers of Voronoi cells N , i ∈ [n], depending on the
i
2.2 GENERATIVEMODELSOFDISCRETE discrete distribution parameters c , i ∈ [n] and n, appar-
i
DISTRIBUTIONS entlyisopen.TheabilitytochooseasmalldimensionD
independentofthenumberofcategories,isanadvantage.
Discrete distribution means a probability distribution of Ontheotherhand,therepresentationofVoronoicellsby
a discrete random variable, i.e. a random variable taking intersectingraysandlinearinequalityconstraintsisnumer-
values in a finite set, or the joint distribution of several icallysubtleregardingautomaticdifferentiationandtrain-
discreterandomvariables.Machinelearningscenarioswith ing,anddegenerateVoronoicellsmayarisedependingon
discretedistributionshavebeenstudiedfromvariousangles theinitialanchorpointsandthesupportoftheunderlying
intheliterature. continuousdistribution.Furthermore,maximumlikelihood
learningisemployedwhichislessstableandefficientthan
ThepapersMaddisonetal.[2017],Jangetal.[2017]study
theflow-matchingapproachChenetal.[2023](cf.Section
discreterandomvariablesencodedbyunitvectors,thatare
4.1below).
mollified via the softmax function, and sampling via the
argmax operation perturbed by Gumbel-distributed noise The paper Hoogeboom et al. [2021] introduces Argmax
(cf.HazanandJaakkola[2012]),bothmainlymotivatedby FlowsandMultinomialDiffusionwhicharecloserinspirit
applyingthereparametrizationtrickandlarge-scalestochas- to our approach in that learnable descision regions in a
ticgradientviaautomaticdifferentiation,inconnectionwith flat space are used for categorization. On the other hand,
discreterandomvariables.AvariantbasedonGaussiandis- the ELBO variational bound Blei et al. [2017] is applied
tributionshasbeenproposedbyPotapczynskietal.[2020]. together with Gaussian or Gumbel-thresholding for the
Thisdiffersfromourobjectivetorepresentandsamplefrom argmax layer, which contrasts with our approach which
complexdiscretejointdistributions,whichwouldbediffi- usesageodesicflow-matchingobjectivefunctionandlearn-
culttoachieveusingparametricdensitiesontheprobability ablesmoothregionsonthetangentbundledefiningcategory
simplex,liketheGumbel,Dirichletandotherdistributions assignment.
Aitchinson[1982].
Finally,wepointouttheveryrecentpaperChenetal.[2023]
Conversely,Tranetal.[2019]applyadiscretechange-of- whichusesareal-valuedrepresentationofbit-encodeddis-
variable formula and ensure invertibility of the proposed creteorcategorialdata,inconnectionwithdiffusion-based
DiscreteFlowarchitecturebymoduloarithmeticofintegers. generativemodels.Impressiveempiricalresultsarereported.
This creates a highly non-smooth scenario which affects We refer again, however, to the superiority of the flow-
gradient-based training and apparently is not fully under- matching approach relative to diffusion models Lipman
stood. etal.[2023],ChenandLipman[2023].
Accordingly,theauthorsofLippeandGavves[2021]crit- In view of the two paragraphs above, we point out that
icize the limited performance of discrete transformations the generative model introduced in this paper seamlessly
Tranetal.[2019],Hoogeboometal.[2019]regardingvo- combinesaflow-matchingapproachwithaRiemanniange-
cabulary size and gradient approximation. Likewise, de- ometricstructuretailoredtorepresentdiscretedistributions
quantizationtechniquesasproposedbyDinhetal.[2017], andtogeneratediscreteandcategorialdata.
Hoetal.[2019]arelimitedregardingtherepresentationof
multidimensional statistical relations between categories.
The normalizing flow approach proposed by Lippe and
33 BACKGROUND Bothmappings(3)and(4)extendfactor-wisetotheprod-
uct space T = T W analogous to (2). For more details
0 0
3.1 ASSIGNMENTFLOWS oninformationgeometry,werefertoAmariandNagaoka
[2000].
Denote by S := ∆˚ = {p ∈ Rc: p > 0, ⟨1 ,p⟩ =
c c j c
1, ∀j ∈[c]}therelativeinterioroftheprobabilitysimplex
3.2 META-SIMPLEXFLOWEMBEDDING
∆ ,i.e.thesetofdiscreteprobabilitydistributionsofcat-
c
egorial random variables y which take values in the set The assignment manifold W only represents factorizing
[c] = {1,2,...,c}, c ∈ N. S c equipped with the Fisher- distributionswhichformsaveryspecificsubsetofalljoint
Rao metric g w(u,v) = ⟨u,Diag(w)−1v⟩, u,v ∈ T 0, is a distributionsofndiscreterandomvariablesy ,...,y ,each
1 n
Riemannianmanifold,withthetrivialtangentbundleS c×T
0 taking[c]values.Indeed,ageneraldistributionisspecified
andtangentspaceT 0 ={v ∈Rc: ⟨1 c,v⟩=0}. by the combinatorially large number of N := cn values
TheassignmentmanifoldW = S c×···×S
c
⊂ Rn ≥×c (n p(y 1,...,y n)ofthejointprobabilitydistribution,whereas
factors)isacorrespondingproductmanifoldwithtrivialtan- a point W ∈ W on the assignment manifold merely has
gentbundleW×T andtangentspaceT =T ×···×T , n·ccoordinates.Inordertoapproximategeneraldiscrete
0 0 0 0
equippedwiththeproductFisher-Raometricg (U,V)= distributionswhichtypicallyaresupposedtorepresentcom-
W
(cid:80) g (U ,V ), W ∈ W, U,V ∈ T . Each point plexstatisticaldependenciesofdiscretestructuredoutput,
i∈[n] Wi i i 0
weintroducethecorrespondingmeta-simplexofalldiscrete
W = (W ,...,W )⊤ ∈ W representsafactorizingjoint
1 n
distributions
distributionofndiscreterandomvariablesy ,...,y ,each
1 n
takingvaluesin[c]. S =(cid:8) p∈RN: ⟨1 ,p⟩=1, ∀j ∈[N](cid:9) . (5)
N > N
Assignment flows denote a class of dynamicalsystems of
theform Forexample,thejointdistributionoftwobinaryvariables
(n = c = 2) is a point p(y ,y ) ∈ S . By contrast, if
1 2 4
W˙ (t)=R (cid:2) F (cid:0) W(t)(cid:1)(cid:3) , W(0)=W ∈W (1) p(y ,y ) = W ∈ W ⊂ S is a distribution on the as-
W(t) θ 0 1 2 4
signment manifold, then W = (( w1 ),( w2 ))⊤. The
1−w1 1−w2
wherethegeneratingvectorfieldontheright-handsideis correspondingtwo-dimensionalsubmanifoldW embedded
given by a parametrized function F θ: W → Rn×c with inS 4isdepictedbyFigure1.Fromtheviewpointofmathe-
parametersθ,andR W isaT 0-valuedlinearmap matics,suchembeddedsetsareknownasSegrevarietiesat
theintersectionofalgebraicgeometryandstatisticsLinetal.
(cid:0) (cid:1)
R [F (W)] =R F (W), i∈[n] (2a)
W θ i Wi θ;i [2009],Drtonetal.[2009].Thecorrespondingembedding
(cid:10) (cid:11)
=Diag(W )F (W)− W ,F (W) , (2b) mapreads
i θ;i i θ;i
(cid:89)
which represents the inverse metric tensor in the coordi- T: W →T ⊂S , T(W) := W , (6)
natesoftheambientEuclideanspaceRn×c.Giventheinitial
N α i,αi
i∈[n]
pointW ingeneralposition,whichencodesstructureddata
0
inconcreteapplications,lim W(t)=(e ,...,e )⊤ with the multi-index notation α = (α ,...,α ) ∈ [c]n.
t→∞ j1 jn 1 n
convergestoanextremepointoftheclosureofW,which Fortheaboveexamplewithn = c = 2,onehasT(W) =
correspondstoanhardlabelassignmentforeachdiscrete (w w ,w (1−w ),(1−w )w ,(1−w )(1−w ))⊤.The
1 2 1 2 1 2 1 2
variabley =j ∈[c], i∈[n]intermsofthecanonicalunit mapping (6) has been introduced and studied recently in
i i
vectorse ∈Rc.Assignmentflowshavebeenintroducedin Boll et al. [2023, 2024]. Each multi-index α indexes ex-
ji
Åströmetal.[2017]withbasicproperties(well-posedness, actlyoneextremepointe ∈{0,1}N (unitvector,discrete
α
convergence)establishedinZernetal.[2022]andawide Dirac measure) of the meta-simplex S , and we identify
N
rangeofefficientgeometricintegrationschemesforcom- accordingly α with this discrete extreme measure, which
putingW(t)Zeilmannetal.[2020]. representsahardcategoryorlabelassignmenttoeachdis-
creterandomvariabley ,...,y .Wethereforecallαalso
Theexponentialmapwithrespecttothee-connectionreads 1 n
labelconfiguration.
v
p·ep Thecomponentsofapointp∈S in(5)areindexedbyα
Exp (v)= , p∈S , v ∈T , (3) N
p ⟨p,evp⟩ c 0 aswell,analogoustotheembeddedvectors(6).Weusethe
short-hand
wheremultiplication·andtheexponentialfunctionapply
componentwise.Weset p(α):=p(y ,...,y )=p , α∈[c]n. (7)
α1 αn α
exp : T →S , p∈S , (4a)
p 0 c c
Thefollowingpropositionhighlightsthespecificroleofthe
exp :=Exp ◦R . (4b)
p p p embeddedassignmentmanifoldT(W)⊂S N.
4whichapproximates thedatadistribution byamixture of
simpleconditionaldistributionsq(α|β)
(cid:90)
p(α)≈ q(α|β)p(β)dβ =E [q(α|β)]=:p(α).
β∼p (cid:101)
(11)
Comparing(11)and(10)motivatestheAnsatz
q(α|β):=E [T(W) ]≈T(Me ) , (12)
W∼νβ α β α
wherethelatterapproximationholdsforasimpledistribu-
tionν onW concentratedclosetotheextremepointMe
β β
onW,whichcorresponds(cf.Eq.(9))totheextremepoint
e ∈S .Thedistributionq(α|β)definedby(12)issimple,
β n
Figure3:Overviewoftheapproach:ThestandardGaus- becausetheright-handsidemerelyinvolvesafactorizing
sianreferencemeasureN(0,I)ispushedforwardbythe distributionW ∈W ontheembeddedassignmentmanifold,
exponentialmapexp W fromtheflattangentproductspace theembeddingmapT (6)andaveraging.
T totheassignmentmanifoldW,andfurthertothemeta-
0
simplexS (5)bygeometricallyintegratingtheassignment Figure 3, center and top row, illustrates this part of the
N
approach.
flow(1).Sincetheassignmentflowconvergestotheextreme
points of W which agree with the extreme points of S ,
N
anapproximationp (cid:101)(α)ofageneraldiscretetargetmeasure 4.1 RIEMANNIANFLOWMATCHING
p(α)underlyinggivendatacanbeapproximatedbymatch-
ingtheflowofe-geodesics(correspondingtodatasamples) A distribution ν on W can approximately represent un-
andconvexcombinationintermsofembeddedfactorized knowndiscretedatadistributionsp∈S through
N
distributionsT(W), W ∈W andempiricalexpectation.
p ≈E [T(W) ]. (13)
α W∼ν α
Proposition1([Bolletal.,2024,Prop.3.2])ForeveryW ∈ Theunderlyingideaisthateverydistributionp∈S canbe
N
W,thedistributionT(W)∈S N hasmaximumentropy representedasamixtureofDiracdistributionsonextreme
(cid:0) (cid:1) (cid:88) points (10). Chen and Lipman [2023] describe a method
H T(W) =− T(W) logT(W) (8)
α α forlearningcontinuousnormalizingflowsonRiemannian
α∈[c]n manifoldsviaflowmatching.Inourcase,themanifoldin
amongallp∈S subjecttothemarginalconstraintMp= questionistheassignmentmanifoldW andwearelearning
N
W, where the marginalization map M: RN → Rn×c is ameasureν onW torepresentadiscretedatadistribution
givenby pvia(13).AcorecontributionofChenandLipman[2023]
wastoshowhowflowmatchingtolearnν canbeachieved
(cid:88)
(Mp) i,j := p α, ∀(i,j)∈[n]×[c]. (9) by matching conditional probability distributions ν β :=
α∈[c]n:αi=j ν(W|β)fordatasamplesβ ∼ p.Thiscorrespondstothe
distributionν = δ ∈ W intheansatz(12),
Anygeneraldistributionp∈S
N
\W whichisnotinT(W) β ϵ1W+(1−ϵ)Meβ
with0<ϵ≪1andthebarycenter1 ofW.ν isclosely
has non-maximal entropy and hence is more informative W β
concentratedaroundtheextremepointofW corresponding
byencodingadditionalstatisticaldependenciesCoverand
totheextremepointe ∈S .
Thomas[2006].Ourapproachforgeneratinggeneraldistri- β N
butionsp∈S N,bycombiningsimpledistributionsW ∈W Wenowdescribehowν islearnedviaanormalizingflow
via the embedding (6) and assignment flows (1), is intro- and geodesic flow matching, as illustrated by the bottom
ducednext. partofFigure3.Fixϵanddefinetherepresentation
q =ϵ1 +(1−ϵ)Me ∈W (14)
β W β
4 APPROACH
ofsampledataβ ∼ponW.Definethereferencedistribu-
Everyjointdistributionp ∈ S canbewrittenasconvex tion
N
combinationofextremepointmeasures ν =(exp ) N (15)
0 1W ♯ 0
p(α)=(cid:0) (cid:88)
p e
(cid:1)
=E [T(Me )] , (10)
onWaspush-fowardwithrespecttotheexponentialmapof
β β α β∼p β α thestandardGaussianN =N(0,I)centeredinthetangent
β∈[c]n 0
spaceat0∈T ,andthedefineconditionaldistributions
0
where the right-hand side denotes the expectation of the
argumentwithrespecttop.Wewillemployflow-matching, ν (W|q )=ν (W), ν (W|q )=δ (16)
0 β 0 1 β qβ
5asendpointsofaprobabilitypathdefinedbelow,foreach 4.2 LIKELIHOODCOMPUTATION
configurationβ ∈[c]n.Clearly,onehas
Foranyconfigurationα∈[c]n,letr ⊆W denotetheset
E [ν (W|q )]=ν (W), (17a) α
qβ∼ν 0 β 0 ofpointsontheassignmentmanifoldwhichhaveMe αas
E qβ∼ν[ν 1(W|q β)]=E qβ∼ν[δ qβ]=ν, (17b) closestextremalpointofW.Weaimtoboundtheprobabil-
ityofr underthepushforwarddistributionν =(ψ ) ν
duetotheunconditionalreferencemeasureintheformer α t t ♯ 0
frombelow.Tothisend,weconstructasubsetr ⊆T such
caseandtheDiracmeasureinthelatter.Asaconsequence, (cid:101)α 0
that
wecanflow-matchtheprobabilitypath
exp (r )⊆r (24)
1W (cid:101)α α
ν (W)=E [ν (W|q )] (18)
t qβ∼ν t β Letν =(exp−1) ν .Then
by merely flow-matching conditional probability paths
(cid:101)t 1W ♯ t
ν (W|q )withendpoints(16)forindividualdatasamples logp =logP (r )≥logP (r ). (25)
t β α W∼νt α v∼ν(cid:101)t (cid:101)α
β ∼ p.Asimplechoiceistheprobabilitypathgenerated
by pushing ν
0
along e-geodesics connecting each initial Becausewelearntheflowψ tsuchthatitconcentratesproba-
W
0
∈W governedbyν 0withq β. bilitymassclosetopointsq α(see(14)),wecanstrategically
choose r to make importance sampling for this integral
(cid:101)α
DenotealatenttangentvectoronT byu ∼ N andthe
0 0 0 cheap to compute. Let ρ be a proposal distribution with
correspondingpointonW byW = exp (u ).Further,
0 1W 0 supportr (cid:101)α.Then
denotebyu =exp−1(q )∈T thetangentvectorcorre-
β 1W β 0
(cid:90)
spondingtoq .Thenthepathofane-geodesicconnecting
β log P (r )=log ν (v)dv (26a)
W withq reads v∼ν(cid:101)t (cid:101)α (cid:101)t
0 β r(cid:101)α
W tβ =exp W0(texp− W1 0(q β)) (19a) =log(cid:90) ν (cid:101) ρt (( vv )) ρ(v)dv (26b)
=exp (u +t(u −u )) (19b) r(cid:101)α
1W 0 β 0 (cid:104)ν (v)(cid:105)
=logE (cid:101)t (26c)
anddifferentiationgives v∼ρ ρ(v)
W˙ tβ =R Wβ[u
β
−u 0] (20) ≥E v∼ρ[logν (cid:101)t(v)−logρ(v)] (26d)
t
withthelinearmapR givenby(2)andfactor-wiseexten- by Jensen’s inequality and we can thus compute a lower
Wβ
sion.TheRiemannianct onditionalflowmatchingobjective, bound on the log-likelihood of α under our model by ap-
inthepresentcasewiththenorminducedbytheFisher-Rao proximatingtheintegralin(26d).Evaluatingtheintegrand
metric,reads amounts to computing log-likelihood under a continuous
normalizingflow.Thiscanbedonethroughainstantaneous
L (θ)=E (cid:2) ∥R [F (Wβ)]−W˙ β∥2 (cid:3) , (21)
RCFM t,qβ,W0 W tβ θ t t W tβ changeofvariablesGrathwohletal.[2019]andevaluating
thelog-likelihoodundertheproposaldistribution,whichwe
It was shown in Lipman et al. [2023], Chen and Lipman
areabletodoinclosedform.Sinceν istrainedtotransport
[2023],that(21)hasthesamegradientwithrespecttopa- t
thereferencedistributiontoonewhichisconcentratedon
rametersθastheunconditionalRiemannianflowmatching
pointsq ,weexpecttheintegrandin(26a)tohavemostof
objective α
itsmassclosetoq =exp−1(q ),whichmotivatesthefol-
L (θ)=E (cid:2) ∥R [F (ψ )]−U(ψ ))∥2 (cid:3) , (22) lowingchoiceof(cid:101) rα andρ.1 LW etrα beaspherewithdiameter
RFM t,W0 ψt θ t t ψt (cid:101)α (cid:101)α
d>0centeredatq andchoosedsmallenoughtosatisfy
foravectorfieldU whichtransportsν toν andgenerates (cid:101)α
0 (24).Letρbeanisotropicnormaldistributionwithvariance
theassignmentflowψ byintegrating(1).Here,theexpec-
t σ2 andmeanq supportedonlyonthespherer .Thetail
tationistakenwithrespecttot∼U[0,1],q ∼ν,W ∼ν
(cid:101)α (cid:101)α
β 0 0 probabilityofthisdistributionrequiredtonormalizeitto
respectivelyandψ =ψ (W ).
t t 0 probabilitymass1isanalyticallyavailablebecauseρhas
As(20)suggestsinviewof(1),wehavewrittentheparam- rotationalsymmetry(seeAppendixB).
eterized vector field in (21) as an assignment flow vector
fieldwithparameterizedfitnessfunctionF .Substituting
θ 5 EXPERIMENTS
(20)into(21)finallygives
L RCFM(θ)=E t,qβ,W0(cid:2) ∥R W tβ[F θ(W tβ)−(v β −v 0)]∥2 W tβ(cid:3) 5.1 GENERATINGIMAGESEGMENTATIONS
(23)
whichwecandirectlyuseasatrainingobjectivetolearnpa- In image segmentation, a joint assignment of classes to
rametersθ.Learningθamountstolearningaflow(1)which pixels is usually sought conditioned on the pixel values
generatesν throughpushfowardofν andapproximatesthe themselves. Here, we instead focus on the unconditional
0
datadistributionby(13). discrete distribution of segmentations, without regard to
6Figure4:Left:RandomsamplesdrawnfromourmodeltrainedondiscreteCityscapessegmentationdata(c=8classes)at
resolution128×256.Rightwithblueborder:Randomlydrawntrainingdata.
Figure5:Histogramofsamplesfromourmodelfittingthejointdistributionofn=2discreterandomvariables.Leftand
middle:c=91classespervariable.Right:c=2classespervariable.Allthreeplotsshowvaluesofthejointdistribution.
Clearly,themodelisabletofitmulti-modaljointdistributionswhichdonotfactorizeintoindependentmarginals.Theplot
ontherightisthejointdistributionshownasbluedotinFigures1and2.
0.075 3.28 bits / dim 2.92 bits / dim 3.75 bits / dim 3.39 bits / dim 3.84 bits / dim
0.050
0.025 4.10 bits / dim 3.83 bits / dim 3.67 bits / dim 3.48 bits / dim 4.13 bits / dim
0.000
100 101 102 103
#importance samples
Figure7:Upper-boundonthelikelihoodofsamplesfrom
MNIST (in distribution, first row) and Omniglot (out of
Figure6:Convergenceofimportancesamplingtheintegral
distribution,secondrow)underourmodel.Datawithlow
(26d)forourgenerativemodelofMNISTdata.Alreadyfor
likelihood(highbits/dim)canbedetectedasoutliers.Like-
fewsamples,relativeerroriswithinfewpercentagepoints.
lihoodwasboundedbyapproximating(26d)through200
Mean and standard deviation are evaluated over 30 data
importancesamples.
drawnatrandomfromtheMNISTtestset.
7
rorre
evitalertheoriginalpixeldata.Thesediscretedistributionsarevery therelatedworkHoogeboometal.[2021]studiesthesame
high-dimensionalingeneral,withN =cnincreasingexpo- discretelabelingdataset,theysubsampletolowresolution
nentiallyinthenumberofpixels.WeperformRiemannian 32×64inordertosaveoncomputation.Incontrast,Fig-
flow-matching(22)viatheconditionalobjective(21)tolean ure 4 shows convincing samples at resolution 128×256
assignmentflows(1)whichapproximatethisdiscretedis- fromourmodeltrainedwithin3.5hoursonasingledesktop
tribution via (12). To this end, we parametrize F by the GPU.
θ
UNetarchitectureofDhariwalandNichol[2021](detailsin
Currentlimitations.Eventhoughtheimportancesampling
AppendixA)andtrainonthesegmentationsofCityscapes
approachproposedinSection4.2isempiricallyveryeffec-
Cordtsetal.[2016],downsampledtoc=8classesandres-
tive,itstillrequiresmorecomputationalefforttoevaluate
olution128×256,aswellasMNISTLeCunetal.[2010],
comparedtotypicalnormalizingflowmodels.Thisisbe-
regardedasbinaryc = 2segmentationsof28×28pixel
cause the integrand needs to be sampled by performing
images.
instantaneouschangeofvariablesandintegratingtheflow
Figure 4 shows samples from the learned distribution of backwardintimemultipletimestogetapreciseandreli-
Cityscapessegmentationsrandomlydrawnfromourmodel. ableestimate(Fig6).Beyondcomputationalconsiderations,
thereisalsoaJensengapintheestimate(26d)whichcan
not be reduced by drawing more samples. There is some
5.2 NUMERICALLIKELIHOODBOUNDS
potentialtomake(25)anequalityinthefuturebychoosing
r accordingly. In principle, one can also consider direct
In general, sampling integrals over high-dimensional do-
(cid:101)α
simulationof(13)asanalternativewhichestimatesexact
mainsisadifficulttask.Weevaluatetheeffectivenessofour
likelihoodwithouttheJensengap.Anumericallystablepro-
importancesamplingapproachofSection4.2empirically
ceduretoperformthissamplingwassuggestedinBolletal.
byplottingtherelativeerroroflikelihoodboundscomputed
[2023],buttheapproachstillonlyworksforlowdimensions
fromvaryingnumberofimportancesamples.Thereference
inpractice.Thisisbecauseevenformoderaten,thenumber
valuefortheintegraliscomputedfrom3557samples.Fig-
ofconfigurationsN = cn islargeandthusthenumberof
ure6showsthatevenasmallnumberofsamplessuffices
randomsamplesrequiredtoencounteranygivenonegrows
tocomputeareasonableapproximationtotheintegral.This
quicklyingeneral.
is evaluated on MNIST (padded to size 32×32) i.e. the
domainofinterationis(c−1)n=1024dimensional.
7 CONCLUSION
5.3 DETECTINGOUT-OF-DISTRIBUTIONDATA
We have introduced a stable and efficient flow-matching
approachonthestatisticalassignmentmanifoldwhichen-
Wetestiftheboundonlog-likelihood(26d)underourgen-
ablestolearncomplexjointdistributionsofmanydiscrete
erativemodelallowstodetectout-of-distributiondata.To
randomvariables.Whilesomelimitationsrelatedtolikeli-
thisend,wetrainamodel(1)toapproximatetheMNIST
hoodestimationremaintobeadressedinfuturework,our
data distribution and compute the bound (26d) by impor-
approach shows clear promise as an alternative to score-
tance sampling. The Omniglot dataset Lake et al. [2015]
orlikelihood-basedmodellingforthechallengingsetting
containsvisuallysimilarimagestoMNIST,butdisplaysa
of discrete data. This promise is founded on principled
widevarietyofdifferentsymbols.Figure7showsrandom
information-geometricmodellingofdiscretedistributions.
samplesofMNISTandOmniglotaswellastheboundon
log-likelihood of each sample under our learned MNIST
distribution.SamplesfromthismodelcanbefoundinAp- Acknowledgements
pendixA.
This work is funded by the Deutsche Forschungsgemein-
schaft (DFG), grant SCHN 457/17-1, within the priority
6 DISCUSSION
programmeSPP2298:“TheoreticalFoundationsofDeep
Learning”.ThisworkisfundedbytheDeutscheForschungs-
Riemannianflowmatchingontheassignmentmanifoldisa
gemeinschaft(DFG)underGermany’sExcellenceStrategy
remarkablystableandefficientapproachtolearninghigh-
EXC-2181/1-390900948(theHeidelbergSTRUCTURES
dimensional discrete distributions. Because training does
ExcellenceCluster).
notrequiresimulationofflowtrajectories,itrequiresfew
function evaluations compared to established likelihood-
basednormalizingflowtraining.
Tothebestofourknowledge,ourworkisthefirsttodemon-
strateflow-matchingfordiscretedata.TheCityscapesex-
perimentillustratestheresultinggaininefficiency.While
8References PrafullaDhariwalandAlexanderQuinnNichol. Diffusion
modelsbeatGANsonimagesynthesis. InA.Beygelz-
J. Aitchinson. The Statistical Analysis of Compositional imer,Y.Dauphin,P.Liang,andJ.WortmanVaughan,edi-
Data. J.RoyalStatisticalSoc.B,2:139–177,1982. tors,AdvancesinNeuralInformationProcessingSystems,
2021. URLhttps://openreview.net/forum?
S.-I.AmariandH.Nagaoka. MethodsofInformationGe- id=AAWuCvzaVt.
ometry. Amer.Math.Soc.andOxfordUniv.Press,2000.
L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density-
F.Åström,S.Petra,B.Schmitzer,andC.Schnörr.ImageLa- Estimation-Using-Real-NVP. InICLR,2017.
belingbyAssignment. JournalofMathematicalImaging
andVision,58(2):211–238,2017. M. Drton, B. Sturmfels, and S. Sullivant. Lecture on Al-
gebraicStatistics,volume39ofOberwolfachSeminars.
H. Ben-Hamu, S. Cohen, J. Bose, B. Amos, A. Grover, Birkhäuser,2009.
M. Nickel, R. T. Q. Chen, and Y. Lipman. Matching
NormalizingFlowsandProbabilityPathsonManifolds. S.GrafandH.Luschgy. FoundationsofQuantizationfor
InICML,2022. Probability Distributions, volume 1730 of Lect. Notes
Math. Springer,2000.
D.M.Blei,A.Kucukelbir,andJ.D.McAuliffe. Variational
Inference: A Review for Statisticians. Journal of the Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt,
AmericanStatisticalAssociation,112(518):859–877,04 and David Duvenaud. Scalable reversible generative
2017. models with free-form continuous dynamics. In In-
ternational Conference on Learning Representations,
B. Boll, J. Schwarz, D. Gonzalez-Alvarado, D. Sitenko, 2019. URLhttps://openreview.net/forum?
S. Petra, and C. Schnörr. Modeling Large-scale Joint id=rJxgknCcK7.
DistributionsandInferencebyRandomizedAssignment.
P.M.Gruber. OptimumQuantizationanditsApplications.
InL.Calatroni,M.Donatelli,S.Morigi,M.Prato,and
AdvancesinMathematics,186:456–497,2004.
M. Santacesaria, editors, Scale Space and Variational
MethodsinComputerVision(SSVM),number14009in
T.HazanandT.S.Jaakkola. OnthePartitionFunctionand
LNCS,pages730–742.Springer,2023.
RandomMaximumA-PosterioriPerturbations. InICML,
2012.
B. Boll, J. Cassel, P. Albers, S. Petra, and C. Schnörr. A
GeometricEmbeddingApproachtoMultipleGamesand
J.Ho,X.Chen,A.Srinivas,Y.Duan,andP.Abbeel.Flow++:
MultiplePopulations. preprintarXiv:2401.05918,2024.
Improving Flow-Based Generative Models with Varia-
tionalDequantizationandArchitectureDesign. InICML,
R. T. Q. Chen and Y. Lipman. Riemannian Flow Match-
2019.
ingonGeneralGeometries. preprintarXiv:2302.03660,
2023.
E. Hoogeboom, J. W. T. Peters, R. van den Berg, and
M.Welling. IntegerDiscreteFlowsandLosslessCom-
R. T. Q. Chen, B. Amos, and M. Nickel. Semi-Discrete
pression. InNeurIPS,2019.
NormalizingFlowsthroughDifferentiableTesselation.In
NeurIPS,2022. E. Hoogeboom, D. Nielsen, P. Jaini, P. Forré, and
M.Welling. ArgmaxFlowsandMultinomialDiffusion:
T. Chen, R. Zhang, and G. Hinton. Analog Bits: Gener-
LearningCategorialDistributions. InNeurIPS,2021.
ating Discrete Data Using Diffusion Models with Self-
Conditioning. InICLR,2023. C.-W. Huang, M. Aghajohari, A. J. Bose, P. Panangaden,
and A. Courville. Riemannian Diffusion Models. In
MariusCordts,MohamedOmran,SebastianRamos,Timo
NeurIPS,2022.
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke,StefanRoth,andBerntSchiele. Thecityscapes E.Jang,S.Gu,andB.Poole.CategorialReparameterization
datasetforsemanticurbansceneunderstanding. InProc. withGumbel-Softmax. InICLR,2017.
oftheIEEEConferenceonComputerVisionandPattern
Recognition(CVPR),2016. I.Kobyzev,S.J.D.Prince,andM.A.Brubaker.Normalizing
Flows:AnIntroductionandReviewofCurrentMethods.
T. M. Cover and J. A. Thomas. Elements of Information IEEE Trans. Pattern Anal. Mach. Intell., 43(11):3964–
Theory. JohnWiley&Sons,2ndedition,2006. 3979,2021.
V.DeBortoli,E.Mathieu,M.J.Hutchinson,J.Thornton, Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B.
Y. W. Teh, and A. Doucet. Riemannian Score-Based Tenenbaum. Human-level concept learning through
GenerativeModelling. InNeurIPS,2022. probabilistic program induction. Science, 350
9(6266):1332–1338, 2015. doi: 10.1126/science. A.Zeilmann,F.Savarino,S.Petra,andC.Schnörr. Geo-
aab3050. URL https://www.science.org/ metric Numerical Integration of the Assignment Flow.
doi/abs/10.1126/science.aab3050. InverseProblems,36(3):034004(33pp),2020.
YannLeCun,CorinnaCortes,andCJBurges. Mnisthand- A.Zern,A.Zeilmann,andC.Schnörr. AssignmentFlows
written digit database. ATT Labs [Online]. Available: forDataLabelingonGraphs:ConvergenceandStability.
http://yann.lecun.com/exdb/mnist,2,2010. InformationGeometry,5:355–404,2022.
S.Lin,B.Sturmfels,andZ.Xu. MarginalLikelihoodInte-
gralsforMixturesofIndependenceModels. J.Machine
LearningResearch,10:1611–1631,2009.
Y.Lipman,R.T.Q.Chen,H.Ben-Hamu,M.Nickel,and
M.Le.FlowMatchingforGenerativeModeling.InICLR,
2023.
P.LippeandE.Gavves. CategorialNormalizingFlowsvia
ContinuousTransformations. InICLR,2021.
A.Lou,D.Lim,I.Katsman,L.Huang,Q.Jiang,S.-.N.Lim,
andC.M.DeSa. NeuralManifoldOrdinaryDifferential
Equations. InNeurIPS,2020.
C.J.Maddison,A.Mnih,andY.W.Teh. TheConcreteDis-
tribution:AContinuousRelaxationofDiscreteRandom
Variables. InICLR,2017.
E.MathieuandM.Nickel. RiemannianContinuousNor-
malizingFlows. InNeurIPS,2020.
R.J.McCann. AConvexityPrincipleforInteractingGases.
Adv.Math.,128:153–179,1997.
G.Papamakarios,E.Nalisnick,D.J.Rezende,S.Mohamed,
andB.Lakshminarayanan. NormalizingFlowsforProb-
abilisticModelingandInference. J.MachineLearning
Research,22(57):1–64,2021.
A.Potapczynski,G.Loaiza-Ganem,andJ.P.Cunningham.
InvertibleGaussianReparameterization:Revisitingthe
Gumbel-Softmax. InNeurIPS,2020.
N. Rozen, A. Grover, M. Nickel, and Y. Lipman. Moser
Flow:Divergence-basedGenerativeModelingonMani-
folds. InNeurIPS,2021.
Y.Song,J.Sohl-Dickstein,D.P.Kingma,A.Kumar,S.Er-
mon,andB.Poole. Score-BasedGenerativeModeling
Through Stochastic Differential Equations. In ICLR,
2021.
A. Takatsu. On Wasserstein Geometry of Gaussian Mea-
sures. Adv.Stud.PureMath.,57:463–472,2010.
D. Tran, K. Vafa, K. K. Agrawal, L. Dinh, and B. Poole.
DiscreteFlows:InvertibleGenerativeModelsofDiscrete
Data. InNeurIPS,2019.
L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao,
W.Zhang,B.Cui,andM.-H.Yang. DiffusionModels:
AComprehensiveSurveyofMethodsandApplications.
ACMComputingSurveys,56(4),2023.
10Generative Modeling of Discrete Joint Distributions by
E-Geodesic Flow Matching on Assignment Manifolds
(Appendix)
BastianBoll1 DanielGonzalez-Alvarado1 ChristophSchnörr1
1InstituteforMathematics,ImageandPatternAnalysisGroup,HeidelbergUniversity,Germany
A IMPLEMENTATIONDETAILS
FortheCityscapesexperiment,weemploytheUNetarchitectureofDhariwalandNichol[2021]withattention_resolutions
(32,16,8),channel_mult(1,1,2,3,4),4attentionheads,3blocksand64channels.Wetrainedfor250epochsusingAdam
withlearningrate0.0001andcosineannealingscheduler.
ForMNIST,weusethesamearchitecturewithattention_resolutions(16),channel_mult (1,2,2,2),4attentionheads,2
blocksand32channels.Wetrainedfor100epochsusingAdamwithlearningrate0.0005andcosineannealingscheduler..
Wepadtheoriginal28×28imageswithzerostosize32×32tobecompatiblewithspatialdownsamplingemployedbythe
UNetarchitecture.AsFigure8shows,ourmodeldoesnotsimplymemorizethetrainingdata.
ForthesimpledistributionsinFigure5,weemployaneuralnetworkcomposedofbatchnormalization,denselayersand
ReLUactivation.ThesequenceofhiddendimensionsforthemixtureofGaussianandPinwheeldistributionsis(256,256).
Forthecoupledbinaryvariables,weusealinearfunctionF ,withnobatchnormalizationorbias.Wetrainedfor2ksteps
θ
withbatchsize512usingAdamwithlearningrate0.0005.
Inallexperiments,thesmoothingconstantϵof(14)issetto0.01.
Ratherthantheoriginalc=33classes,weonlyusethec=8classcategoriesspecifiedintorchvision.Thesamesubsampling
ofclasseswasusedintherelatedworkHoogeboometal.[2021].Theyadditionallyperformspatialsubsamplingto32×64.
Instead,wesubsamplethespatialdimensions(NEAREST interpolation)to128×256.
All experiments in this paper were run on one of two desktop graphics cards (1x NVIDIA RTX2080ti, 1x NVIDIA
RTX2060super),requiringlessthan100computehoursintotal.
B IMPORTANCESAMPLING
LetQ∈Rc×(c−1)beanorthonormalbasisofthelinearsubspaceT S ⊆Rc.Independentlyforthetangentspaceofevery
0 c
individualsimplexwithindexi∈[n],thechosenproposaldistributionisthenormaldistribution
ρ =Q N(cid:0) Q⊤(q ) ,σ2I (cid:1) (27)
i ♯ (cid:101)α i c−1
onT S ,centeredandsupportedonadiskwithradiusr >0,
0 c
r ={u ∈T S : ∥u −(q ) ∥ ≤r}. (28)
(cid:101)α i 0 c i (cid:101)α i 2
11Figure8:IllustrationofhandwrittendigitsamplestrainedfromtheMNISTdataset.Theleftmostcolumn,highlightedwitha
redframe,displaysrandomsamplesgeneratedbyourmodelthroughintegrationEq.(1)withrandominitializations.The
remainingplotsdepictthefiveclosesttrainingdatatooursamplebasedonpixel-wisedistances.
Foranyu ∈T S ,itholdsQQ⊤u =u andwehave
i 0 c i i
∥u −(q ) ∥2 =⟨u −(q ) ,u −(q ) ⟩ (29a)
i (cid:101)α i 2 i (cid:101)α i i (cid:101)α i
=⟨QQ⊤(u −(q ) ),u −(q ) ⟩ (29b)
i (cid:101)α i i (cid:101)α i
=⟨Q⊤(u −(q ) ),Q⊤(u −(q ) )⟩ (29c)
i (cid:101)α i i (cid:101)α i
=∥Q⊤u −Q⊤(q ) )∥2. (29d)
i (cid:101)α i 2
Thus,u ∈r exactlyifthecoordinatesQ⊤u lieintheball
i (cid:101)α i
r ={x∈Rc−1: ∥x−Q⊤(q ) ∥ ≤r}. (30)
(cid:98)α (cid:101)α i 2
Sincetheproposaldistribution(27)isanormaldistributionwithvarianceσ2 centeredandsupportedonr ,weneedthe
(cid:98)α
probability of r under a normal distribution with full support centered on it, as normalization constant of ρ . By first
(cid:98)α i
shiftingthemean,thiscanbecomputedastheprobabilityofthesphere{x∈Rc−1: ∥x∥ ≤r}.LetX beastandardnormal
2
randomvariableonRc−1,thenthesoughtprobabilityis
(cid:16) r2(cid:17)
P(∥σX∥2 ≤r2)=P ∥X∥2 ≤ . (31)
2 2 σ2
Since X has normal distribution, ∥X∥2 has χ2-distribution and (31) can be computed by evaluating the cumulative
2
distributionfunctionofχ2 withc−1degreesoffreedom.Inpractice,wesetaprobabilitymassof0.8fromtheoutset
and then choose σ2 by inverting (31). A simple geometric argument shows that the largest radius r which satisfies the
requirementsis
(cid:114) c
r =∥q ∥ . (32)
(cid:101)α 2 2(c−1)
12Figure9:IllustrationofCityscapessegmentationsamplesdrawnfromourmodel.Theleftmostcolumn,highlightedwitha
blackframe,displaysrandomsamplesgeneratedbyourmodelthroughintegrationEq.(1)withrandominitializations.The
remainingplotsdepictthefiveclosesttrainingdatatosamplebasedonpixel-wisedistance.
13