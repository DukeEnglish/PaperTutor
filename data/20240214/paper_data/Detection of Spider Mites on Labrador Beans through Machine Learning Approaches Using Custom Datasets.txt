Detection of Spider Mites on Labrador Beans through Machine
Learning Approaches Using Custom Datasets
Violet Liu1∗, Jason Chen1∗, Ans Qureshi1∗, Mahla Nejati1∗∗
Centre for Automation and Robotic Engineering Science,
The University of Auckland, New Zealand 1
hliu665, jche774, aqur476@aucklanduni.ac.nz∗, m.nejati@auckland.ac.nz∗∗
Abstract labour-intensive process requiring trained professionals;
thisresultsinhighlabourcostsforfarmers. Toalleviate
Amidst growing food production demands,
the stress of disease detection, researchers are explor-
earlyplantdiseasedetectionisessentialtosafe-
ing different plant disease detection techniques, includ-
guard crops; this study proposes a visual ma-
ing using various sensors, data collection methods, and
chinelearningapproachforplantdiseasedetec-
machine learning algorithms. Bioforce, a New Zealand
tion,harnessingRGBandNIRdatacollectedin
naturalpestcontrolcompany,fundsthisstudy. Thedata
real-worldconditionsthroughaJAIFS-1600D-
used in this study is collected within their greenhouses.
10GE camera to build an RGBN dataset. A
Thisstudyfocusesontheearlydetectionofspidermites
two-stage early plant disease detection model
in labrador beans using RGB and Near-infrared (NIR)
with YOLOv8 and a sequential CNN was used
data through machine learning algorithms. A two-stage
to train on a dataset with partial labels, which
model is proposed, pipelining a You Only Look Once
showed a 3.6% increase in mAP compared to
version 8 (YOLOv8) segmentation model to a simple 6-
a single-stage end-to-end segmentation model.
layer sequential Convolutional Neural Network (CNN).
The sequential CNN model achieved 90.62%
The study delves into challenges with current plant dis-
validation accuracy utilising RGBN data. An
ease detectiondatasets, dataprocessing techniques, seg-
averageof6.25%validationaccuracyincreaseis
mentationandclassificationmethods. Theprimarycon-
found using RGBN in classification compared
tributions are:
to RGB using ResNet15 and the sequential
CNNmodels. Furtherresearchanddatasetim- • Collecting a customised RGB and Near Infrared
provementsareneededtomeetfoodproduction
(RGBN) dataset for spider mites on labrador beans
demands.
• An examination of how RGBN datasets impact
early disease detection in contrast to RGB datasets
1 Introduction
and the potential of partial transfer learning in
As the global population grows, the demand for food RGBN input layer
and crops also increases. It is estimated that a 70% in-
creaseinfoodwouldbeneededtomeetourpopulation’s • Analysis upon the effects of small datasets on high
needs by 2050 [Fang and Ramasamy, 2015]. In 2021, complexity CNN such as VGG16 and ResNet50
New Zealand’s horticulture industry made 6.68 billion
dollars in exports, which makes 11.1% of the country’s • Investigation on the effectiveness of a two-stage
total exports [Warrington I J, 2021]. Despite efforts to model on datasets with missing labels
enhance yield through automation in pollination [Bar-
nett et al., 2017; Nejati et al., 2019a], pruning [Williams The subsequent sections encompass Related Works
et al., 2023] and harvesting [Nejati et al., 2019b], pests on RGBN-based machine learning, dataset biases, seg-
anddiseasesstillresultin20-40%harvestlosses[Thakur mentation and classification models. The Methodol-
et al., 2022]. Furthermore, it is estimated that billions ogy section delves into design choices, providing ratio-
of dollars are lost yearly just in New Zealand due to nales. Results presents quantitative data, the Discus-
the deterioration of plants caused by disease [Thakur et sion involves in-depth data analysis, and the Conclu-
al., 2022]. The current leading industry method in dis- sion summarises findings. Additionally, it touches upon
easedetectionismanualauditing,atime-consumingand future prospects and acknowledgements.
4202
beF
21
]VC.sc[
1v59870.2042:viXra2 Related Works and above were achieved overall within the studies that
utilise it[Noyan, 2022]. However, it was found that the
Thestudyofplantdiseasedetectionthroughimagepro-
background colours in the dataset create bias. Despite
cessing and machine learning is a prominent and ac-
the dataset containing 38 categories, a validation accu-
tively researched topic. Plant disease detection has
racyof49%wasachievedusingtherandomforestmodel;
many facets, including different data channels, usage of
this is problematic as it proves a correlation between
datasets,andstate-of-the-artmachinelearningmethods.
the background and the classes. Similar tests were con-
2.1 Utilising RGB and NIR Channels in ductedonthe”MNIST”datasetasabenchmark,achiev-
Data ing a validation accuracy of 11.7%; since the ”MNIST”
dataset only has 10 classes, 11.7% is an appropriate
NIRdatahasbeenusedtoclassifytomatoleavesthrough
value for random guesses, showing no bias. Similarly,
colour image analysis and machine learning [Nieuwen-
many papers utilising the ”NaCRRI” Ugandan public
huizenetal.,2020]. Inthisstudyacamerawasplacedin
dataset have consistently achieved a validation accuracy
agreenhousetocaptureimages. Thestudyutilisednear-
of above 98%, which is considered suspicious as it seems
infrared bands; however, it found that Linear Discrimi-
similar to the ”Plant village” bias [Devi et al., 2023;
nant Analysis (LDA) did not have the spatial resolution
Elfatimi et al., 2022; Abed et al., 2021; Singh et al.,
toidentifyearlyspidermitedamagewithinthesebands.
2023]. The ”NaCRRI” Ugandan public dataset tends to
After three months of spider mite damage, a ResNet18
have variations in data collection; for example, the leaf
modelsuccessfullyattaineda90%validationaccuracyin
wouldbeplucked,andatothertimes,itwouldbeonthe
distinguishingbetween”Healthy”and”Damaged”leaves
plant. Itisplausiblethatthesevariationsstrengthenthe
through exclusively RGB data. Nevertheless, this study
dataset and enable the algorithms to achieve a higher
didnotincorporatenear-infrareddataintotheResNet18
validation accuracy. However, whether the dataset has
model due to hardware limitations.
an inherent bias remains unclear, as it is relatively new
RGB and VIS-NIR spectral imaging has been used to
and unexplored. [Elfatimi et al., 2022].
detect ”Apple Scab” disease in an apple orchid [Rouˇs
et al., 2023]. 8 channel based data was collected which Whilstitisproblematicthatthesepublicdatasetsare
potentially biased, collecting and labelling this data is
consisted of, RGB and 5 different NIR frequencies. The
incredibly time-consuming and labour-intensive. Due to
studyachievedaMeanAveragePrecision(mAP)of0.73
the expensive nature of creating customised datasets for
by using only the RGB images with a YOLOv5 classifi-
each study, utilising a public dataset to compare and
cation network; the reason why RGB was used is that it
test the functionalities of proposed models should still
was challenging to balance the light conditions for all 8
be encouraged.
channels in a multispectral camera in an outdoor envi-
ronment.
2.3 State-of-the-art Segmentation Models
A combination of RGB and NIR data channels, or
RGBN channels, was found to improve the classification YOLOv8 is still being heavily applied in agriculture for
of kiwifruit detection by 1.5% to 2.3% [Liu et al., 2020]; detectingplantdiseases. Itshowedrobustnessbyachiev-
this was done by comparing the average precision of a inganF1-scoreof99.2%forsegmentationon”PlantVil-
trained VGG16 model by using different inputs such as lage” and ”PlantDoc” datasets that consisted of 11700
RGB, NIR or a combination of the two through image RGB images with 5 plant species and 8 disease classes
or feature fusion. Although this study is not on plant [Qadri et al., 2023]. Recognising that some datasets do
disease detection and only detects whether a kiwifruit is not faithfully represent the real world is imperative, as
present, it still reveals the potential benefits of utilising ”PlantVillage”dataisfromcontrolledlaboratorycondi-
NIR data in classification and segmentation. tions.
Among these studies, none have addressed the possi-
2.4 State-of-the-art Classification Models
bility of classifying leaves based on stress-related abnor-
malities distinct from disease symptoms. Additionally, VGG16 model remains one of the preferred choices for
none have effectively incorporated RGBN data for early plant disease detection, achieving a validation accuracy
plant disease detection. rate of 95.2% on the ”Plantvillage” dataset. [Alatawi et
al., 2022] covered 19 distinct plant disease classes, span-
2.2 Dataset exploration
ning tomatoes, grapes, apples, and corn crops. [Shah
Beginning with a discussion of publicly available et al., 2022] compared the performance of Inception V3,
datasets, it is found that the popularly used ”Plant vil- VGG16,VGG19,CNN,andResNet50inearlyricedis-
lage” dataset is biased in its labelled classifications due ease detection. In this study, ResNet50 achieved the
to consistent background colors in the images [Bhandari highest validation accuracy of 99.75% , with VGG16
et al., 2023]. High validation accuracies of 90% - 98% close behind and a validation accuracy of 98.47% .Inanotherstudy[Saeedet al.,2021],riceleafdatasets Two models are trained separately and require mak-
obtainedfrom”Kaggle”werecategorizedintomajorand ing some adjustments to the dataset. The segmentation
minor disease groups to understand the diversity of dis- model should exclusively identify leaves, so all consid-
eases better. These datasets were then used to train a eredclassesinthetrainingdatasetmustbeconsolidated
variant of the ResNet152 model, resulting in an accu- into a general ’leaf’ class as shown in Figure 2.
racy of 99.1% for major disease detection and 82.20% Conversely,theclassificationmodelaimstodetermine
for minor disease detection. the health condition of each leaf and should only be
A notable limitation in these studies is their reliance trained on cropped and labelled leaf segments from the
on publicly available datasets, specifically ”PlantVil- original dataset, as depicted in Figure 3
lage” and ”Kaggle”; this limitation poses a challenge The interference workflow of the two-stage model is
when applying their models in real-world scenarios be- shown in Figure 1 (b). It receives an input image, then
cause the ”PlantVillage” and ”Kaggle” datasets con- segments the image into ’leaf’ segments and classifies
sistofphotographscapturedundercontrolledlaboratory their health conditions. After processing all segments,
conditions rather than actual field conditions. it delivers a final output, presenting the detected leaf
Inanotherstudy,ResNet20,whichisarelativelycom- instances and their respective health condition labels.
plex CNN model, exhibited inferior performance com-
3.2 Single-stage Model
paredtoabasicsequentialCNNmodelwhenthedataset
consisted of fewer than 640 images. This was found Another solution for the missing labels is to employ
through the creation of three CNN models with dif- occlusions to conceal these unlabelled instances, ren-
ferent trainable parameter capacities, including one re- dering them black and easily distinguishable from the
ferredtoas”CNN-LowCapacity”,withthelowestnum- background; this approach enables training on a single-
ber of trainable parameters, ”CNN-Medium Capacity,” stage segmentation model to effectively acquire knowl-
”CNN-High Capacity,” and ”ResNet-20,” boasting the edge about the characteristics of the leaves without any
highestnumberoftrainableparameters[BrigatoandIoc- interference from the unlabelled instances.
chi, 2021]. The models were then tested on the sCI- As shown in Figure 4, the leaf instances without
FAR10, sFMNIST, and sSVHN datasets. Interestingly, ground truth in the original dataset are highlighted in
the only state-of-the-art model tested was ResNet, po- orange. These instances are occluded with black pix-
tentially due to other models being very complex. How- els to generate a new dataset, and their labels will be
ever,sinceonlyResNetistestedasanexamplefor”High removed.
number of trainable parameters”, it could be possible
that the inability to perform well with a dataset with 3.3 Segmentation
less than 640 images is exclusive to ResNet. The framework YOLOv8 was chosen to train the seg-
mentation component of the two-stage model due to its
3 Methodology
fastinferencetimeandcomparableaccuracy. Inthecon-
Two models are investigated to streamline early plant text of the two-stage model, inference speed becomes a
disease detection while addressing the challenge of par- pivotalfactorduetotheaddedoverheadassociatedwith
tially labelled data: a single-stage segmentation model forwarding each leaf instance to the classification com-
(Figure 1a) utilising the ”Occluded dataset” and a two- ponent. YOLOv8 is a relatively complex segmentation
staged model (Figure 1b), separating the segmentation model that has a large number of weight parameters.
and classification processes for modularity. These two Training these parameters from scratch is very time-
models will be compared to find the most optimal final consumingandinefficient. Usingtransferlearningtoini-
model. tialize the model weights based on pre-trained features
extracted from RGB channels is more efficient; modify-
3.1 Two-stage Model
ing the input CNN layer of the YOLOv8 model is nec-
The two-stage model comprises a segmentation model essary to segment images with 4 channels of RGBN.
andaclassificationmodel;thesegmentationmodeltakes
3.4 Channel Fusing Techniques
an image of a plant and separates the individual leaves
from the background regardless of their conditions, and TrainingonRGBNdatasetrequiresmodificationsonthe
the classification model then takes each leaf image and input layer of the YOLOv8 model. Since the original
assigns it a label based on the leaf condition, such as YOLOv8 model only accepts inputs with 3 channels in
”Healthy”, ”Stressed” or ”Spidermite”. Combined, the RGB, the input channel parameter of the CNN layer is
twomodelsactlikeastandardsingle-stagemodel,which changed from 3 to 4; this way, the same architecture
segments healthy and diseased leaves from an image in- and weights of the YOLOv8 model can be used for the
put but can be trained on a partially labelled dataset. subsequent layers. Due to the demonstrated increase in(a) Single-Stage Model (b) Two-Stage Model
Figure 1: Comparison of the Single-Stage Model (a) and the Two-Stage Model (b).
Figure 4: Removal of leaf class to obtain a fully labelled
Figure 2: Dataset conversion for segmentation model.
dataset.
transfer, in contrast to the common practice of trans-
ferring weights for RGB models. Therefore, attempts
were made to transfer partial weights from a trained
RGBmodeltothenewmodels. Differentpartialweight-
transferring approaches have been compared, which can
be grouped by two-channel fusing techniques: substitu-
tion and addition.
Substitution technique
The ”substitution” method replaces one of the RGB
Figure 3: Dataset conversion for classification model.
channelswiththeNIRchannelinthedatasetandtrains
the model as a usual RGB model with the new data;
performance through transfer learning, observing simi- this method benefits from transfer learning, but it also
larenhancementsintrainingtheRGBNmodelwouldbe sacrifices the information of the replaced channel, which
highly advantageous. The unavailability of publicly ac- may be important for the object of interest. Moreover,
cessible RGBN datasets also implies that there are no thepre-trainedweightsforthereplacedchannelmaynot
pre-trained RGBN models readily accessible for weight be suitable for the new NIR channel.Addition Technique layer with Softmax activation. All models share iden-
The ”addition” method involves adding the additional tical standard hyperparameters, featuring a consistent
NIR channel along with the RGB channels as an input learningrateof0.001andafixedbatchsizeof32for100
tothemodel;thisrequiresaddingtheNIRpixelvalueto epochs.
the dataset and changing the input layer of the machine
learning model. 4 Results
The shape of the weight tensor for the input layer
4.1 Data Collection
of the segmentation model (CNN) is determined by the
number of filters, input channels, and the size of the In-field data collection mitigates dataset bias, ensuring
filters. For example, if there are k filters, each with size an authentic representation of field-collected data; this
w·h,andtheinputhascchannels,thentheweighttensor is done using a JAI FS-1600D-10GE camera to collect
has the shape k·c·w·h. Modifying the input layer to data from labrador bean leaves from a BioForce green-
includeanadditionalNIRchannelwillincreasethevalue house, as shown in Figure ??. The camera has a res-
of c by 1; this changes the shape of the weight tensor, olution of 1440x1080 pixels and was configured with
preventing direct transfer learning. It remains feasible a 2.8mm focal distance. The camera-to-leaf distance
toleveragethepre-existingweightsfromthepre-trained ranged from 152.2mm to 338.6mm, while the NIR fre-
model by transferring the pre-trained RGB weights to quency fell within a 740nm to 1000nm spectrum. Fig-
the new model while simultaneously introducing a new ure 5 shows a visual representation of the camera setup.
NIR channel without any pre-trained weights. Through in-field data collection, 64 images were taken,
32 in RGB and 32 in NIR. Each image contains around
3.5 Classification
200 leaf instances to be labelled. The bean leaves were
The current state-of-the-art classification algorithms inoculated between 1 to 5 weeks before data collection,
in early plant disease detection are typically those allowing the models to train for early disease detection,
of CNNS, in particular, that of VGG16 and ResNet as the spider mite damage is yet to be visible to the
[Alatawi et al., 2022; Shah et al., 2022; Saeed et al., naked eye.
2021]. The labelled data was cropped into individual
leaf-boundboxesof256x256pixelstotraintheclassifica-
tion models. The leaves were labelled into three classes:
”Healthy”, ”Stressed” and ”Spidermite”. There were 89
images for ”Healthy”, 159 images for ”Spidermite”, and
84 images of ”Stressed” leaves. Data was split into 80%
training, 10% validation and 10% testing. Four models
were compared within this study: VGG16, ResNet15,
ResNet50 and a simple sequential CNN network. These
models were chosen as VGG16 and ResNet are two of
the current state-of-the-art models in plant disease de-
tection, and the simple sequential CNN network was
used as a baseline for the study [Alatawi et al., 2022;
Shah et al., 2022]. A range of data configurations are
explored. These configurations included:
• RGB data using 3 channels.
Figure 5: Visual representation of camera distance to
• RGBN data using 4 channels. plants.
• VGG16 and ResNet50 with transfer learning
One of the main challenges in training the segmen-
• VGG16 and ResNet50 without transfer learning
tation and two-stage models is handling the dataset’s
However, since there is no publically available pre- missing labels. Accurately labelling the datasets poses
trained VGG16 or ResNet50 with NIR data, the pre- a significant challenge, as leaves in the early stages of
trained models will only consider variations involving infection lack visible symptoms, necessitating external
3 channels. The sequential CNN model comprises 6 Polymerase Chain Reaction (PCR) testing for ground
2D convolutional layers separated by max-pooling lay- truth verification. Consequently, the dataset includes
ers. Following this, the data is flattened and passed numerous leaves still undergoing ground truth testing,
throughadenselayerwithRectifiedLinearUnit(ReLU) rendering them unsuitable for model training. Two ba-
activation, and subsequently, it flows into another dense sicapproachesareignoringunlabeledleavesintheimageor training a segmentation model on the remaining la- Table 1: Channel substitution dataset on segmentation
bels. However, these methods are flawed because ignor- model.
ing unlabeled leaves can result in misclassification, and
Dataset channels Epochs mAP50-95
creatinganew’leaf’classforunlabeledleaveswouldhin-
der the model’s ability to differentiate between healthy RGB 100 53.2%
and diseased leaves. Neither of these approaches fully NGB 100 51.1%
addresses the problem. RGN 100 48.7%
Theoriginalimagesinthedatasethavearesolutionof
1440x1080 pixels, which is too large for the training de-
vice’s memory capacity. Therefore, all models discussed pre-trained and are randomly initialized. For instance,
previouslyweretrainedonimagesresizedto480x360pix- RGBR in the table implies that the RGBN model uses
els. However,thismayaffectthesegmentationqualityof thepre-trainedweightsoftheR,GandBchannelsfrom
small or distant objects. The limitation of segmentation the RGB model for the corresponding input CNN layer,
size is resolved by generating a grid dataset, achieved and the fourth NIR channel inherits the weights from
by dividing each image into four equal parts, as shown the red channel of the pre-trained RGB model.
in Figure 6; this way, the models can learn more details
from the original images with larger image sizes. Table 2: Transfer learning on segmentation model.
Transferred weights in CNN Epochs mAP50-95
xxxx 100 48.5%1
RGBx 100 52.2%1
RGBR 100 53.0%
RGBG 100 51.2%
RGBB 100 52.7%
Table 2 shows that training the RGBN model with
pre-trained weights on the RGB channels can train the
modelmoreefficiently,withanaverageincreaseof3%in
Figure 6: Image splitting on the dataset.
mAP within 100 epochs. However, the models have no
significant difference with different transferred weights
4.2 Segmentation Component of of the additional channel.
Two-Stage Model As shown in Table 1, the model with additional NIR
does not outperform the standard RGB segmentation
A pre-trained YOLOv8 model trained on RGB seg-
model with the same configurations, having a best-case
mentation through MS COCO dataset for 500 epochs
mAP of 51.1% and 53.2% respectively. Contrary to ini-
achieves a mAP of 50% by 50 epochs on the collected
tial expectations, the NIR channel does notenhance the
dataset. The identical model, configured with the same
model’s robustness on segmenting leaves.
hyperparameters but without pre-trained weights, only
achieved a mAP of 40.8% after 483 epochs, with no fur-
Grid dataset
ther improvement observed during the last 50 epochs.
Due to graphical memory limitations, a segmentation
Results of Channel Fusing Techniques
modelcanbetrainedwithanimagesizeof704x528with
Table 1 shows the performance of different channel sub- the grid RGB dataset for 50 epochs. Even with fewer
stitution methods to the NIR channel. A model trained trainingepochs,themodeloutperformedtheonetrained
with dataset channels of NGB means that the red chan- on the original dataset, as demonstrated in Table 3. Ta-
nel of the images in the dataset is replaced by the NIR ble 3 indicates that training the segmentation model on
channel. Aftertrainingwith100epochs,themodelwith- the grid dataset is advantageous because it enables the
out channel substitutions in the dataset performs the model to capture finer leaf details, enhancing its ability
best with an mAP of 53.2%. The following Table.2 il- to distinguish between leaves and the background; this
lustrates how different values of transferred weights af- iscurrentlythebest-performingsegmentationmodeland
fect the performance of the additional channel. Trans- will be used as the segmentation component in the two-
ferred weights indicate the degree to which the weights stage model.
oftheRGBNchannelofthenewCNNlayersarederived
fromthepre-trainedweightsoftheRGBmodel. Avalue 1Computed from an average of 3 training attempts since
of ’x’ signifies that the weights on this channel are not some weights are randomly initialised.Table 3: Model training on grid dataset.
Dataset Imagesize Epochs mAP50-95 Inferencetime(ms)
Original 480x360 100 53.2% 5.8
Grid 704x528 50 63.0% 23.5
4.3 Classification Component of
Two-Stage Model
The outcomes of training various models, including
VGG16 with and without transfer learning, ResNet50
with and without transfer learning, ResNet15, and the
sequentialCNNmodelusingthecollectedlabradorbean
leaf dataset, are summarised in Table 4.
Through Table 4, it can be seen that the sequential
CNN model with RGBN performs the best, achieving
a 90.62% validation accuracy at 100 epochs, followed
by pre-trained VGG16 with 87.5% validation accuracy
using only RGB data. Notably, pre-trained ResNet50
models underperformed with a mere 25% validation ac-
curacy. Non-pretrained VGG16 with RGBN data and
ResNet15 with RGB data also scored 25% validation
accuracy. However, ResNet15 improved to 62.5% val-
idation accuracy with 4-channel NIR data. Through
Figures 7a and 7b, it can be seen that the ResNet15
withRGBNandsequentialCNNRGBNbothhaveyetto
(a) ResNet15 with 4 Chan- (b) sequential CNN model
plateau,meaningthattheyhavethepotentialforfurther nels. with 4 Channels.
improvement with extended training if not for hardware
constraints.
Considering that the sequential CNN model has the
highest validation accuracy, it will be the model utilised
for the classification component of the two-stage model.
4.4 Single stage model performance
The training results of the single-stage model on the oc-
cluded dataset are presented in Table 5, which includes
three models: one trained on an RGB occluded dataset,
another on an RGBN dataset with weight transfer from
the red channel of a pre-trained model to the NIR chan-
nel (RGBR), and a third on the RGBN dataset with a
randomly initialised fourth channel (RGBx). The train-
ing results across different models all achieved similar
performances,astheyallreachedasaturationpointwith
no further improvements beyond 32 epochs, leading to
early termination.
4.5 Evaluation on occluded dataset
(c) ResNet15 with 3 Chan- (d) ResNet50 with 3 Chan-
The single-stage and two-staged models are compared nels. nels.
by utilising an occluded dataset. The comparison needs
Figure 7: Validation training on different classification
toberevisedasbothsingle-stageandtwo-stagedmodels
models.
maybeinfluencedbytheirindividualpreparationmeth-
ods. However, the results are still comparable through
careful analysis. The single-stage model is trained using
the occluded dataset, enabling it to learn how to han-
dle occlusions within the validation dataset, including
occluded leaves.Table 4: Results of VGG16, ResNet50, ResNet15 and Sequential CNN model. Each training model was trained on
the Labrador Beans Dataset with three classes, using the following hyperparameters: a learning rate of 0.001, 100
epochs, and a batch size of 32.
Pre-trained? # of Validation Validation
Model RGB NIR
(Y/N) channels Accuracy Loss
VGG16 N Y Y 4 25% 1.1161
VGG16 N Y N 3 40.62% 4.1589
VGG16 Y Y N 3 87.5% 0.3476
Resnet50 N Y Y 4 25% 1.137
Resnet50 N Y N 3 25% 1.136
Resnet50 Y Y N 3 25% 1.1716
Resnet15 N Y Y 4 62.5% 0.7414
Resnet15 N Y N 3 56.25% 0.8752
Sequential N Y Y 4 90.62% 0.2365
Sequential N Y N 3 84.38% 0.5029
The two-stage model approach is also evaluated on 5 Discussion
the same occluded dataset. The segmentation compo-
5.1 Segmentation
nent is the model trained with the RGB grid dataset
Table 1 shows that the substitution channel fusing tech-
with an mAP of 63.0%. The classification component
nique with transfer weights is not always reliable, in
is the sequential CNN model trained with the RGBN
which either replacing the red channel or green channel
dataset,whichachieved90.62%validationaccuracy. Ex-
didnotproducecomparableresultsastheRGBdataset;
perimentsarealsocarriedoutonreplacingthesegmenta-
this proves that the red and blue channels continue to
tioncomponentwiththesingle-stagemodeltrainedwith
offer valuable information for distinguishing leaves, con-
different transferred weights. The segmentation compo-
tributing to the model’s accuracy. One plausible expla-
nentachievedthehighestmAPof23.6%outofallother
nation is that the availability of a large RGB dataset
approaches.
has provided an incomparable advantage when train-
Both the single-stage and two-stage approaches have ing the RGB model, surpassing the potential benefits
varying advantages over the other. The single-stage that a NIR channel might offer in discovering additional
model benefits from directly training from the occluded features. The substitution channel model results sug-
dataset, meaning it would be less likely to miscategorise gestthattransferringinputlayerweightsfromanyRGB
blackpixelsasleaves. Thetwo-stagemodelbenefitsfrom channel in the pre-trained model does not enhance NIR
being tested on non-occluded leaves. However, the first- channel training; this is supported by Table 2, where
stagesegmentationmodelmaydetectslightlydifferently models using RGBN data perform similarly to those
than the ground truth labels on leaf segments and pass without NIR channel transfer learning, indicating that
the leaves to the second-stage classification model with existing RGB channels are unsuitable for pre-training
slight deviation. the NIR channel. Nonetheless, partial transfer learn-
ing’s success is evident compared to models without in-
Thesingle-stagemodelgainsanadvantagebytraining
put layer pre-training, as shown in Table 2.
directlyontheocclusiondataset,allowingittolearnhow
to handle occluded leaves. This knowledge enables the 5.2 Classification
modeltodealeffectivelywithocclusionsinthevalidation
During testing, it became clear that incorporating
dataset,includingoccludedleaves. Thetwo-stagemodel
RGBNdataoutperformsusingonlyRGBdata,asshown
hasyettobetrainedontheocclusiondataset,providing
in Table 4. The improvement is likely attributed to
an advantage to the single-stage model.
theadditionalinformationprovidedbytheNIRchannel,
enabling the model to discern finer distinctions [Liu et
The classification component of the two-stage model
al., 2020; Nieuwenhuizen et al., 2020]. Interestingly, the
faces a challenge as it was initially trained with bound-
sequential CNN model performs best among the three
ingboxdatabutnowreceivesleafmasksasinputswhen
proposed models; this advantage likely results from var-
integrated into the segmentation component pipeline,
ious factors, such as the dataset size, small leaf image
causing a longer processing time.
size, low training epochs, and differences in model com-
Different from the two-stage model with a specialised plexity between VGG16 or ResNet50 and ResNet15 or
classifier, the single-staged model cannot classify leaves sequential CNN. As evidenced by the performance gap
effectively due to a lack of generalisation. between pre-trained and non-pre-trained VGG16 mod-Table 5: Single stage models performance on unseen occluded dataset with 1024x768 images.
Model Epoch mAP(all) mAP(healthy) mAP(spidermite) mAP(stressed) Inferencetime(ms)
RGB 100(32) 17.8% 23% 30.5% 0.00% 90.4
RGBR 100(32) 20% 27.6% 26.9% 5.69% 43.2
RGBx 100(32) 18.55% 18.8% 36.4% 0.545% 68.9
Table 6: Two-stage models performance on unseen occluded dataset.
Segmentation Classification mAP mAP mAP Inference
mAP
model model (all) (spidermite) (stressed) time (ms)
RGBsingle-stagemodel RGBNmodel 15.6% 19.6% 23.2% 4.07% 140
RGBxsingle-stagemodel RGBNmodel 16% 23.4% 16.3% 8.29% 430
RGBRsingle-stagemodel RGBNmodel 16.5% 25% 19.3% 5.27% 240
RGBgridtwo-stagemodel RGBNmodel 23.6% 40% 30.7% 0.00% 290
els,thereducedvalidationaccuracywithoutpre-training additionalleaveslackinggroundtruthinformationabout
canbeattributedtoadenselyconnectednetworkrequir- their conditions. However, this comes with the sacrifice
ing more training epochs when initiated from scratch. of speed as two-stage model relies on passing every seg-
Conversely, this phenomenon is less pronounced with mentstotheclassificationcomponent. Frombothtables
ResNet50, likely due to its larger trainable parameters, difference in inference time per image of the two mod-
necessitating more data and training time to prevent els is found to be magnitude, and the inference time of
overfitting, excessive data analysis and proper training two-stagemodelcanincreaseasthenumberofinstances
[Brigato and Iocchi, 2021]. To further exemplify this detected increases.
hypothesis, ResNet15 was implemented due to it con- Nevertheless, this evaluation result requires careful
taining relatively less trainable parameters than that of interpretation due to the limitations of the occluded
ResNet50. Figure 7c and 7d show the training progress dataset, which only contains 5 training images and 2
of3channelResNet15comparedtothatofthesequential validation images. It is expected that the model trained
CNN model and ResNet50. onthedatasetcannotgeneralizewell,andthevalidation
These trends show that ResNet50 is stagnant within on the dataset would not be comprehensive.
local minima, likely due to its many trainable parame-
ters. While ResNet15 is still gradually learning, its val-
6 Conclusion and Future Works
idation accuracy remains low even after 100 epochs. In
contrast to these relatively complex models, the sequen- Early plant disease detection is vital amid increasing
tialCNNmodelsteadilyincreasesinvalidationaccuracy, globalfooddemand,withpestsanddiseasescausingsig-
supporting the notion that more complex CNN models nificant crop yield losses. This study explores methods
maystruggletolearnwhenexposedtolimiteddata. An- like RGB and NIR channels, machine learning models,
alyzing the validation accuracies of models that did not and specialized datasets for disease detection. NIR data
stagnate at local optima, it becomes evident that incor- showspromiseinenhancingaccuracy,whiledatasetchal-
porating RGBN data results in an average increase of lenges, like biases and variations, are acknowledged. A
6.25% in detection validation accuracy compared to us- two-stagemodelforhandlingmissinglabelsisproposed,
ing only RGB data; this strongly indicates that includ- with a sequential CNN model achieving a strong valida-
ingNIRdataprovidesvaluableinformation,enablingthe tion accuracy of 90.62% with RGBN data. Future work
classificationmodeltodetectdiseasesatanearlierstage should involve advanced hardware, larger datasets, and
thanwhenrelyingsolelyonRGBdata[Rouˇsetal.,2023; more complex models to address global food challenges.
Nieuwenhuizen et al., 2020; Liu et al., 2020]. In future, the studies should be expanded upon by
usingmoreadvancedhardwaretoallowformoretraining
5.3 Model evaluation on occluded dataset
epochs for classification. It would also be beneficial to
The two-stage model, featuring the most accurate leaf collect more data to increase the size of the dataset to
segmentation and disease classification models as indi- allow for the implementation of more complex models.
cated in Table 6, outperforms the single-stage model
presented in Table 5 by achieving an improved mAP of
Acknowledgements
3.6% The experiments replacing the leaf segmentation
model with the single-stage model, as depicted in Table WeextendourthankstoBioforceLimitedandtheirrep-
6, have demonstrated that the leaf segmentation model resentative,ChrisThompson,forsponsoringthisproject
improved its ability to distinguish leaves by training on and supplying resources to assist with the research.References [Nejati et al., 2019b] Mahla Nejati, Nicky Penhall,
HenryWilliams,JamieBell,JongYoonLim,HoSeok
[Abed et al., 2021] Sudad H. Abed, Alaa S. Al-Waisy,
Ahn, and Bruce MacDonald. Kiwifruit detection in
Hussam J. Mohammed, and Shumoos Al-Fahdawi.
challenging conditions. Australasian Conference on
A modern deep learning framework in robot vision
Robotics and Automation, ACRA, 2019-Decem, 2019.
for automated bean leaves diseases detection. Inter-
national Journal of Intelligent Robotics and Applica- [Nieuwenhuizen et al., 2020] A.T. Nieuwenhuizen,
tions, 5:235–251, 6 2021. J. Kool, H.K. Suh, and J. Hemming. Automated
spider mite damage detection on tomato leaves in
[Alatawi et al., 2022] Anwar Abdullah Alatawi,
greenhouses. Acta Horticulturae, pages 165–172, 1
Shahd Maadi Alomani, Najd Ibrahim Alhawiti,
2020.
and Muhammad Ayaz. Plant disease detection using
[Noyan, 2022] Mehmet Alican Noyan. Uncovering bias
ai based vgg-16 model. International Journal of
in the plantvillage dataset. 6 2022.
Advanced Computer Science and Applications, 13,
2022. [Qadri et al., 2023] Syed Asif Ahmad Qadri, Nen-Fu
Huang,TaibaMajidWani,andShowkatAhmadBhat.
[Barnett et al., 2017] J Barnett, M Seabright,
Plant disease detection and segmentation using end-
H Williams, M Nejati, A Scarfe, J Bell, M Jones,
to-endyolov8: Acomprehensiveapproach. pages155–
P Martinson, and P Schare. Robotic Pollination -
160. IEEE, 8 2023.
Targeting Kiwifruit Flowers for Commercial Appli-
cation. International Tri-Conference for Precision [Rouˇs et al., 2023] Robert Rouˇs, Joseph Peller, Ger-
Agriculture, 2017. rit Polder, Selwin Hageraats, Thijs Ruigrok, and
Pieter M. Blok. Apple scab detection in orchards us-
[Bhandari et al., 2023] Mohan Bhandari, Tej Bahadur
ing deep learning on colour and multispectral images.
Shahi, Arjun Neupane, and Kerry Brian Walsh.
2 2023.
Botanicx-ai: Identification of tomato leaf diseases us-
[Saeed et al., 2021] Zubair Saeed, Ali Raza, Ans H
ing an explanation-driven deep-learning model. Jour-
Qureshi, and Muhammad Haroon Yousaf. A multi-
nal of Imaging, 9:53, 2 2023.
crop disease detection and classification approach us-
[Brigato and Iocchi, 2021] Lorenzo Brigato and Luca
ingcnn. In2021InternationalConferenceonRobotics
Iocchi. A close look at deep learning with small data.
and Automation in Industry (ICRAI), pages 1–6.
pages 2490–2497. IEEE, 1 2021.
IEEE, 2021.
[Devi et al., 2023] Nilakshi Devi, Kandarpa Kumar [Shah et al., 2022] Dhruvil Shah, Vishvesh Trivedi,
Sarma, and Shakuntala Laskar. Design of an intelli- Vinay Sheth, Aakash Shah, and Uttam Chauhan.
gentbeancultivationapproachusingcomputervision, Rests: Residual deep interpretable architecture for
iotandspatio-temporaldeeplearningstructures. Eco- plant disease detection. Information Processing in
logical Informatics, 75:102044, 7 2023. Agriculture, 9:212–223, 6 2022.
[Elfatimi et al., 2022] Elhoucine Elfatimi, Recep [Singh et al., 2023] Vimal Singh, Anuradha Chug, and
Eryigit, and Lahcen Elfatimi. Beans leaf diseases Amit Prakash Singh. Classification of beans leaf dis-
classification using mobilenet models. IEEE Access, eases using fine tuned cnn model. Procedia Computer
10:9471–9482, 2022. Science, 218:348–356, 2023.
[Fang and Ramasamy, 2015] Yi Fang and Ramaraja P [Thakur et al., 2022] Poornima Singh Thakur, Pritee
Ramasamy. Current and prospective methods for Khanna,TanujaSheorey,andAparajitaOjha. Trends
plant disease detection. Biosensors, 5(3):537–561, in vision-based machine learning techniques for plant
2015. disease identification: A systematic review. Expert
Systems with Applications, page 118117, 2022.
[Liu et al., 2020] Zhihao Liu, Jingzhu Wu, Longsheng
Fu, Yaqoob Majeed, Yali Feng, Rui Li, and Yongjie [Warrington I J, 2021] Martech Consulting Group Ltd
Cui. Improved kiwifruit detection using pre-trained AitkenAGWarringtonIJ. Freshfacts: Newzealand
vgg16 with rgb and nir information fusion. IEEE Ac- horticultural exports 2021. Fresh Facts, 2021.
cess, 8:2327–2336, 2020. [Williams et al., 2023] Henry Williams, David Smith,
JalilShahabi,TrevorGee,MahlaNejati,HinMcGuin-
[Nejati et al., 2019a] Mahla Nejati, Ho Seok Ahn, and
ness, Mike Duke, Richard Bachelor, and Bruce A.
Bruce MacDonald. Design of a sensing module for a
MacDonald. Modelling wine grapevines for au-
kiwifruit flower pollinator robot. Australasian Con-
tonomousroboticcanepruning. BiosystemsEngineer-
ference on Robotics and Automation, ACRA, 2019-
ing, 235:31–49, 2023.
Decem, 2019.