Policy Improvement using Language Feedback Models
VictorZhong12 DipendraMisra1 XingdiYuan1 Marc-AlexandreCoˆte´1
Abstract efficientlearningthroughpromptingandin-contextlearning
fortextual(Brownetal.,2020)andgroundedproblemssuch
WeintroduceLanguageFeedbackModels(LFMs)
asroboticcontrol(Ahnetal.,2022). However,forinstruc-
thatidentifydesirablebehaviour—actionsthat
tionfollowingingroundedproblems,currentmethodsrely
helpachievetasksspecifiedintheinstruction—
onLLMson-lineduringinference,whichisimpracticaland
forimitationlearningininstructionfollowing. To
expensive.
trainLFMs,weobtainfeedbackfromLargeLan-
guageModels(LLMs)onvisualtrajectoriesver- Wedevelopasample-efficientandcost-effectivetechnique
balized to language descriptions. First, by us- that uses LLMs to train Language Feedback Models
ingLFMstoidentifydesirablebehaviourtoim- (LFMs) for policy improvement in instruction following.
itate, we improve in task-completion rate over Figure1illustratespolicyimprovementusingLFMs. Con-
strongbehaviouralcloningbaselinesonthreedis- siderthetaskofinteractingwithobjectsinakitchentofol-
tinct language grounding environments (Touch- lowinstructionsshowninFigure1(c). First,inFigure1(a),
down, ScienceWorld, and ALFWorld). Sec- givenagroundedenvironmentandabasepolicy(i.e.abe-
ond,LFMsoutperformusingLLMsasexpertsto haviourclonedpolicy),werolloutthebasepolicytocollect
directlypredictactions,whencontrollingforthe asmallsetoftrajectoriesfordifferentinstructions. Next,
numberofLLMoutputtokens. Third,LFMsgen- we verbalize observations in the trajectory by describing
eralizetounseenenvironments,improvingtask- scenes in language. For each instruction and verbalized
completionrateby3.5-12.0%throughoneround trajectorypair,wequeryanLLMtoprovidefeedbackiden-
of adaptation. Finally, LFM can be modified tifying which behaviour in the trajectory is productive to
toprovidehuman-interpretablefeedbackwithout solvingthetaskidentifiedintheinstruction(i.e.answeryes
performanceloss,allowinghumanverificationof orno). Forinstance,givenaninstruction“putacleanslice
desirablebehaviourforimitationlearning. oflettuceintherefridgerator”, GPT-4(OpenAI,2023)is
abletodeducethatkeymilestonesare1)findthelettuce,
2)sliceit3)washitinthesink,and4)putitinthefridge.
1.Introduction Consequently, such an LLM is able to identify when an
agentisexhibitingdesirablebehaviourconducivetosolv-
Sample-efficiencyandgeneralizabilityaretwoprimarychal-
ing tasks outlined in the instruction, for instance by tak-
lengesinlearninginstructionfollowingagentsingrounded
ing the lettuce to the sink, versus undesirable behaviour,
environments(MacMahonetal.,2006;Kollaretal.,2010;
forinstancebycookingthelettuce. AftercollectingLLM
Ahnetal.,2022). First, wewantanagentthatissample-
feedback,wedistillthisworldknowledgeintoasmalland
efficient: itlearnsfromfewdemonstrationsofhowtoact
cost-effectiveLFM.Finally,inFigure1(b),givenapolicyto
accordingtoinstructions. Second, wewantanagentthat
improveonpotentiallynewenvironmentsandinstructions,
is generalizable: it should act successfully in novel envi-
weusethelearnedLFMtoidentifydesirableactionson-line,
ronmentsaccordingtonewinstructionsaftertraining. Re-
thenupdatethepolicytoimitatetheseactions. Crucially,
inforcementlearning(RL;Sutton&Barto(2018))andim-
thistechniqueissample-efficientandcost-effectiveinthat
itationlearning(IL;Schaal(1999),Abbeel&Ng(2004))
itonlyrequiresfewLLMinteractionstocollectanoff-line
aretwotechniquesforlearningagentsforinstructionfol-
dataset during LFM training (i.e. before deployment), as
lowing in grounded environments. These techniques of-
opposedtomanyLLMinteractionson-lineduringpolicy
tenrequirelargenumbersoftrialsanderrorsorexpensive-
improvement(i.e.afterdeployment).
to-obtain expert demonstrations. Recent work show that
pretrainedlargelanguagemodels(LLMs)exhibitsample- Ourfindingsareasfollows: first,throughLFMpolicyim-
provement,onthreegroundedinstructionfollowingbench-
1MicrosoftResearch2UniversityofWaterloo.Correspondence marks, namely Touchdown (Chen et al., 2019), Science-
to:VictorZhong<victor.zhong@uwaterloo.ca>.
World(Wangetal.,2022),andALFWorld(Shridharetal.,
2021b),weobserveconsistentgainsoverstrong,behaviour
1
4202
beF
21
]GL.sc[
1v67870.2042:viXraPolicyImprovementusingLanguageFeedbackModels
parse
 train

query for
 Feedback Feedback
Initial policy Rollouts LLM batched
 feedback

feedback dataset model
feedback model
(a) Learningasmallandcost-effectiveLanguageFeedbackModelfromLLMfeedback.Werolloutaninitialpolicy,thenpromptanLLM
toprovidefeedbackonwhatactionsthepolicytookduringtherolloutwereproductiveinachievingthetaskoutlinedintheinstruction.We
thenusethisdatatotrainafeedbackmodelthatpredictswhetheranactionisproductivegiventheinstruction.
Dataset of
query for
 Feedback step-wise
 imitation
 Improved
Base policy Rollouts desirable

feedback model prediction learning policy
behaviour
(b) Policyimprovementbyimitatingdesirablebehaviouridentifiedbyalearnedfeedbackmodel.Giventheinstruction,werolloutabase
policy,thenidentifyproductiveactionsthathelpachievetasksspecifiedintheinstructionusingthetrainedfeedbackmodel.Finally,we
updatethebasepolicybyimitatingtheidentifieddesirablebehaviour.
Instruction: clean some lettuce and put them in the fridge
(c) ExampleofdesirablebehaviouridentifiedinanexampleenvironmentinALFWorld,akitcheninstructionfollowingbenchmark.
Figure1: Givenanenvironmentandinstructionstofollow,weassumeaccesstoaverbalizationprocedurethatconverts
observationstolanguagedescriptions. PolicyimprovementusingLanguageFeedbackModelinvolves(a)firsttraininga
feedbackmodel,then(b)usingittoidentifydesirablebehaviourforpolicyimprovementviaimitationlearning. Weshowthe
feedbackmodelinyellow,othermodelsinpurple,andgeneratedintermediatedataingreen. AnexampleofLFM-identified
desirablebehaviourisshownin(c).
cloned base policies. Second, using LLMs as feedback theyweredesirable,therebyallowinghumanstoinspectand
modelsoutperformsusingLLMsasexpertpoliciesforim- validateimitationdatausedforpolicyimprovement. Source
itation learning. We compare LFMs against prompting code for our environments and experiments are available
LLMs to directly predict what actions to take, then imi- atanonymous.4open.science/r/language feedback models.
tatingthisLLM-predictedbehaviour. Onallbenchmarks, VideosofLFMfeedbackareavailableatlanguage-feedback-
usingLFMfeedbackoutperformsusingLLMsasexpertsfor models.github.io.
imitationlearning,givenafixedallocationofLLMoutput
tokens. Thisgainisespeciallypronouncedinenvironments
2.Background
withlargeractionspaces,suchasScienceWorld,whereitis
mucheasiertocritiquethantogeneratethecorrectaction. Languagegroundedinstructionfollowing. Inlanguage-
Third, we show that learned feedback models generalize groundedinstructionfollowing,anagentisgivenaninstruc-
tounseenenvironments. AftertrainingLFMsontraining tion x specifying the task to achieve in the environment.
environments,weusethemtoidentifydesirablebehaviour Eachturn,theagentreceivesapotentiallypartialobserva-
ontestenvironments,whichwethenadaptthepolicytoimi- tiono ,andtakesanactiona whichcausestheenvironment
t t
tate. Asingleroundofadaptationachievessignificantgains totransitiontoanewstate. IntheexampleinFigure1(b),
(3.5-12.0%task-completionrate)acrossallenvironments. theagentobservesacounterwithobjectssuchasatoaster,
somelettuce,andaknifeontop. Tofollowtheinstruction
In addition to policy improvement, using LFM feedback
“putacleansliceoflettuceintherefridgerator”,aneffective
offers two advantages over existing techniques such as
agentmaychoosetograbapieceoflettuce. Inthereinforce-
using LLMs as expert policies for imitation learning.
mentlearningsetting,theenvironmentadditionallygivethe
First, LFM improves policies on-line without additional
agentarewardafteradesirable(positivereward)orundesir-
expensivecallstoLLMs. Second,LFMcanofferhuman-
able(negativereward)action(Sutton&Barto,2018).Inthis
interpretablefeedbackwhenidentifyingdesirablebehaviour
work,weconsiderlong-horizonsettingswithonlysparse
toimitate. WeshowinSection5.4thatLFMscanbeeasily
and delayed task-completion rewards. Consequently, we
modifiedtoprovidenotonlydesirablebehaviourbutwhy
focusonimitationlearningfromdemonstrationsasopposed
2PolicyImprovementusingLanguageFeedbackModels
verbalization
Instruction Action Verbalized Observation
straight ahead
Turn so the scaffolding is on your left and go with the Behind you, you see: a white jeep, a large red brick
flow of traffic to the next corner and turn right. When building, the scaffolding, two trees, the next corner. 
you turn there are orange cones in the road to go To your left, you see: a large red brick building, the next
through on both sides. Keep going, through the firs   corner. Straight ahead, you see: that building, a large r  
Figure2: AnexampleverbalizationforTouchdown. WealignCLIPimageembeddingsofpanoramapatchesandlanguage
embeddingsofcommonnoun-phrasestopopulatealanguagetemplate. AppendixAdescribesthisprocedureindetail.
Thebluearrowatthetopindicatetheagent’sorientationwhilethegreenarrowsindicatevaliddirectionstoproceedin.
toreinforcementlearningfromrewards(Schaal,1999). cloneexpertbehaviour(Bain&Sammut,1995;Torabietal.,
2018). BC(orofflineimitationlearning)onlyaskstheex-
Imitationlearning. Inimitationlearningforinstruction perttoperformthetaskN timestocollectN trajectories
following, we are given an expert policy π∗(a|x,o) and {τ }N . Eachτ consistsofM stepsofobservationsand
i i=1 i i
l oe ua trn tha ep po ol li ic cy yπ πθ θ( .a F|x o, ro e) acw hit sh tep par oa ( tm i)e ot fer ts heθ. roW lloe ufi tr τs it ,r wol el was hs eo rc eia ate (id
)
ie sx tp he ert aca tc it oio nn cs h: oτ si en= by[o t( 1 hi e), ea x( 1i p) e, r. t. π., ∗o (a( Mi |) i x, ,a o( M (i i) i )]
)
o πp ∗t (i am |xiz ,e o(θ i)t )o wim hei nta gte ivt eh ne ta hc eti so an ma e( t oi) bc seh ro vs ae tn iob ny s.theexpert g exiv pe en rtt ah cet tio ob ns ,e grv iva eti no tn heo t( si a). mW ee obtr sa ei rn vap to ioli ncy seπ eθ nt bo yi tm heita et xe pt t eh re
t,
t
(cid:104) (cid:16) (cid:17)(cid:105) byminimizingthefollowingobjective:
argminE L π (a|x,o(i)),a(i) (1)
θ
o( ti)∼πθ θ t t
Here,Lisastep-wisecross-entropylossfunctionbetween
t eh xe pep ro tli gc iy v’ es na tc ht eio sn amdis etr oi bb su eti ro vn ata ion nd :theactionchosenbythe
argmin
1
(cid:88)N
1
(cid:88)Mi
L(cid:16)
π
(a|x,o(i)),a(i)(cid:17)
. (3)
N M θ t t
θ i
L(∗)=− (cid:88) 1(cid:104) a′ =a(i)(cid:105) lnπ (a=a′ |x,o(i)). (2) i t
t θ t
a′∈A
Behavioural cloning. Imitation learning in Eq (1) as- ThekeydistinctionbetweenBCandimitationlearningis
sumesanexpertpolicythatcanbeexecutedon-linetopro- that the former optimizes over trajectories under the ex-
duceexpertactions. Forinstance,givenanexpert,imitation pertpolicywhilethelatteroptimizesovertrajectoriesunder
learningassumesthatthisexpertπ∗(a|x,o )providescor- thelearnedpolicy. Consequently,whileBCisofflineand
t
rective actions a as the policy π(a|x,o ) runs. In many easily batchable, it suffers from covariate shift/exposure
t t
cases, this is impractical — a human-in-the-loop expert bias(Rossetal.,2011;Bengioetal.,2015). Likepriorwork
isexpensiveandinconvenientwhileanLLMexpertisex- inlong-horizoninstructionfollowingingroundedenviron-
pensive and, as we show in our experiments, inaccurate. ments (Fried et al., 2018; Chen et al., 2019), we use BC
Alternatively, in behaviour cloning (BC), we instead col- towarm-startastrongbasepolicy(Ash&Adams,2020),
lectanofflinedatasetofexperttrajectoriesfromwhichto whichwethenimproveusingimitationlearning.
3PolicyImprovementusingLanguageFeedbackModels
3.LanguageFeedbackModel Efficientlylearningalanguagefeedbackmodel. While
Eq(4)isareasonableprocedureforusingLLMfeedback
HowcanweleverageworldknowledgeinLLMstomake
toimprovethepolicy,itrequirescallingLLMsateachstep
policy learning more sample-efficient and generalizable?
duringpolicyimprovement. Thisisprohibitivelyexpensive
In this work, we use LLMs to distill a small and cost-
bothintermsofquerycost,becauseLLMscapableofgiv-
effectiveLanguageFeedbackModeltoidentifydesirable
ing desirable feedback are expensive to run, and training
behaviour from a base policy (Figure 1(a)). We then im-
time,becausegeneratingfeedbackusinglargeLLMsisslow.
provethebasepolicybyimitatingthisdesirablebehaviour
InsteadofusingtheLLMateachstep,wemakeamodifi-
throughbatchedimitationlearning,withoutneedforon-line
cationtotheprocedureinEq(4)tocollectLLMfeedback
LLMs(Figure1(b)). AppendixBprovidespseudo-codefor
overlonghorizonsinbatch(Colasetal.,2023)inorderto
theentireprocedureforpolicyimprovementusingLFMs.
trainasmallandcost-effectivelanguagefeedbackmodel.
A natural question is why not directly use LLMs as ex-
pertsforactionprediction. Section5.4showsthattheusing First,forinstructions{x(1),x(2),...}werolloutthebase
LLMstolearnfeedbackmodelsresultsinhigherpolicyim- policy π θ to collect a set of trajectories {τ 1,τ 2,...} con-
provementthanusingLLMsasexpertsforactionprediction. sistingofverbalizedobservationsandactionstaken: τ i =
Moreover,LFMsgeneralizetonewenvironmentsunseen {v(i)π(x(i),v(i)),v(i)π(x(i),v(i)),...}. For each τ , we
1 1 2 2 i
during training, thereby allowing policy improvement on prompt the LLM for feedback on which steps were pro-
newenvironments. ductiveinachievingtheinstructionx(i). Table2’sLFMrow
showsanexampleofrequestingfeedbackfromGPT-4on
3.1.Verbalization arolloutinALFWorld, whichisaninstructionfollowing
benchmarkinverbalized3Dkitchens. ThisLLMfeedback
ToleverageworldknowledgeinLLMs,weconvertrawob-
isthenparsedtoidentifytheprecisestepsinwhichthebase
servationsotolanguagedescriptionsvusingaverbalization
policy π took a productive action towards achieving the
θ
procedureV. Figure2illustratessuchaverbalizationpro-
goals outlined in the instruction. The set of desirable be-
cedureforTouchdown(Chenetal.,2019),wheretheagent haviouriscompiledintoadatasetF.Lety∗ = LLM(x,v,a)
navigatesGoogleStreetViewpanoramaimagesbasedona
denote the feedback given by the LLM for the instruc-
givennaturallanguageinstruction.First,weextractallnoun-
tions x, observations v, and action a. We use the dataset
phrases(NPs)frominstructionsinthedatasetandcompute F ={x(i),v,a,y∗∀v,a∈τ ∀x(i),τ }totrainasmallLan-
i i
theirCLIPlanguageembedding. Givenavisualobservation,
guageFeedbackModelf.
wecomputeCLIPvisualembeddingforeachimagepatch,
and align it with the top matching NP as deemed by the (cid:88)
argmin L(f (y |x,v,a),y∗) (5)
highest cosine similarity between CLIP embeddings. We
θ
θ
(x,v,a,y∗)∈F
thencombinealignedNPswithagentorientationtoformu-
lateanegocentriclanguagedescriptionofthescene. Thisis
Here,Listhecross-entropybetweenoutputdistributionof
describedinmoredetailinAppendixA. thefeedbackmodelf andgoldlabely∗fromtheLLM.
θ
3.2.Learningafeedbackmodel
Learningfromlanguagefeedback. Thenaivelearning
NaivelylearningfromLLMfeedback. Givenaverbal- procedureinEq(4)updatesthepolicyaftereachstepusing
izationprocedureV,aninstructionx,anLLM,andapolicy slowandexpensiveLLMfeedback.Here,weinsteadupdate
π ,wenowdescribeaproceduretousetheLLM’sknowl- thepolicyinroundsusingfastandcost-effectiveLFMfeed-
θ
edgetoimproveπ . First,weprompttheLLMtoprovide back. Inroundk,werolloutthebasepolicyπ(k) anduse
θ
feedbackonwhetheraparticularactiontakenbythepolicy thefeedbackmodelf tocollectadatasetD kofdesirablebe-
π (a|x,v)isproductiveinachievingthetasksoutlinedin
haviour.Leta(k)denotetheactionchosenbypolicyπ(k)(a|
θ t
theinstructionx. Wethenimprovethepolicyπ
θ
byupdat- x,v t). Let DESIRABLE(x,v,a) = f(y =yes|x,v,a) >
ingitsparameterstoimitatedesirablebehaviourdetermined f(y =no|x,v,a), returns whether the feedback model
by the LLM. Let : denote “such that”. Let LLM(x,v,a) predictsthatactionaisdesirable. Wehave
returnyesifandonlyiftheLLMfeedbackindicatesthat
(cid:110)(cid:16) (cid:17) (cid:111)
action a taken in verbalized state v and instruction x is D
k
= x,v t,a( tk) ∀t: DESIRABLE(x,v t,a( tk)) (6)
productive. Given a set of instructions X = {x }N, the
i 1
optimizationprocedureisthen
Wecombinethisdatasetwithpreviouslycollecteddesirable
argminE L(π (a|x,v),a′) (4) behaviourtoupdatethebasepolicyviaimitationlearning.
v,a′,x:LLM(x,v,a′)=yes θ
θ
(cid:88) (cid:16) (cid:17)
whereinstructionxissampledfromX andtheobservations θ∗ =argmin L π(k)(a|x,v ),a (7)
t t
vandactionsa′aresampledfromrolloutsofthepolicyπ . θ
θ vt,at∈∪k i=1Di
4PolicyImprovementusingLanguageFeedbackModels
Inthenextround,wesettheparametersofthebasepolicy etal.(2022)learnrewardmodelsfromhumanpreference,
π(k+1) to be θ∗. Should demonstrations be available, we which is then used to to learn a policy via reinforcement
initializethebasepolicyatk =1totheBCpolicy,andtrain learning(RL).Insteadofusinghumanfeedback,Baietal.
onbothdemonstrationsandidentifieddesirablebehaviour (2022)andLeeetal.(2023)useLLMfeedbacktotraina
during subsequent rounds (i.e. ∪k D where D is the separaterewardmodelforRLfortextualalignment. Huang
i=0 i 0
demonstrationsusedtotrainBC). et al. (2022) and Yao et al. (2023) use LLMs to reason
about potential resolutions to failed actions. Yuan et al.
(2024)useLLMstogeneratenewpromptsandcorrespond-
4.RelatedWork
ingresponses,thenuseanLLMrewardmodeltoidentify
Instruction following in grounded environments. In- good prompt-response pairs for self-improvement in text
structionfollowingingroundedenvironmentshasbeenex- generationalignment. Unliketheseapproaches,wedonot
plored in settings such as navigation (Chen & Mooney, useLLMsduringon-linepolicyimprovement. Wetrainan
2011; Fried et al., 2018; Chen et al., 2019), game- initial small language feedback model from offline LLM
playing(Andreas&Klein,2015;Zhongetal.,2020),and data, then use this small feedback model on-line during
robotics(Blukisetal.,2019;Shridharetal.,2021a;Brohan policy improvement. Additionally, we focus on-line im-
et al., 2023). However, most prior work model environ- provementvialanguagefeedbackforlong-horizon,sparse
ment observations separately from language instructions reward,groundedenvironmentsinsteadoftextgeneration
by using specialized encoders (e.g. RESNET (He et al., alignment. Our procedure for batched, on-line imitation
2015), BERT (Devlin et al., 2019), CLIP (Radford et al., learningissimilarto DAGGER (Rossetal.,2011),which
2021)), then learn from data how to associate raw obser- wecomparetoinAppendixC.However,wecollectbatched
vationswithlanguageinstructions. Insteadofsolelyusing expertfeedbacktoidentifydesirablebehaviourinsteadof
rawobservations,morerecentworkverbalizerawobserva- correctiveactions.
tionstodescribeenvironmentsinlanguage(Shridharetal.,
2021b;Zhongetal.,2021;Schumannetal.,2024). Indoing 5.ExperimentsandAnalysis
so,observationsandinstructionscanbedirectlyjointlyrea-
sonedoverusinglanguagemodelstoachievemoreefficient We evaluate using Language Feedback Model for policy
andgeneralzablelearningthroughlarge-scalepretraining. improvementonthreedistinctlanguagegroundingbench-
Webuildonthislastdirectionbyverbalizingrawobserva- marks. We compare this method against directly using
tionsintolanguagedescriptionstotrainlanguagepolicies. LLMsasanexpertpolicyforimitationlearning. Formally,
However,unlikepriorworkthattrainlanguagemodelsto theenvironmentsfromabenchmarkaredistinctpartially-
predictnextactions,wedeveloplanguagefeedbackmodels observedMarkovDecisionProcessesthatsharesome(orall)
thatcritiqueverbalizedobservationsandbehaviour. oftheenvironmentdynamicsbuthavedifferentinstructions,
observations,and/oractionspace.
LLMagentsinlanguagesettings. LLMsexhibitanarray
of reasoning abilities by pretraining on vast quantities of 5.1.Evaluationbenchmarks
text (Brown et al., 2020; Wei et al., 2022). A number of
Table 1 shows examples of verbalized environments and
recentworkinvestigateusingLLMsaslanguageagentsto
tasksfromeachbenchmark. Eachbenchmarkprovidesdis-
exploit this reasoning ability. Nakano et al. (2022), Yao
tincttrainingandtestenvironmentstotestgeneralization.
etal.(2023)Dengetal.(2023)traininstructionfollowing
In each environment, the agent takes actions to perform
language agents to interact with web browsers to answer
tasks outlined in a language instruction. The task is con-
questionsorinteractwithwebpages.Ahnetal.(2022)show
sideredcompletedifandonlyiftheagentsolvesthetasks
that a language agent can be connected with verbalized
withinthepreallocatednumberofsteps. Weevaluateusing
robotsviaAPIinterfacesforroboticcontrol. Whilepower-
task-completionrateovertestenvironments. Thestatistics
ful,thesepriorworkarelimitedinthattheyrequirequerying
from each benchmark is shown in Appendix A Table 6.
anexpensiveLLMon-line. Incontrast,ourworkexamines
Thesethreebenchmarkssharechallengesinsparse,delayed
settingswhereanLLMisnotavailableon-line. Specially,
reward,partialobservability,andcompositionalgeneraliza-
weuseLLMstocollectasmallsetofoff-linedatafortrain-
tiontounseentasksandenvironments.
ingLFMs. Thesmallandcost-effectiveLFMsarethenused
toidentifieddesirablebehaviourforon-linepolicyimprove- ALFWorldisaverbalizationofALFRED(Shridharetal.,
mentwithoutadditionalinteractionswiththeLLM. 2020),anaturallanguageinstructionfollowingbenchmark
setina3Dsimulatedkitchen. Here,theagentinteractswith
Learning from feedback. An important recent exten- objectsinkitchenstoachievecompositionalgoalssuchas
sion of language agents is to augment them with feed- cleaningthenmicrowavingpotatoes. InALFWorld(Shrid-
back. Ziegleretal.(2020),Stiennonetal.(2020),andBai haretal.,2021b),rawstateinformationfromALFREDare
5PolicyImprovementusingLanguageFeedbackModels
Table1: Examplesofverbalizedenvironments. Forbrevity,weabbreviatelongverbalizedobservationsusing“...”.
Benchmark Context Action
ALFWorld Task:heatsomeeggandputitindiningtable. goto
Observation:Youarriveatloc12.Onthesinkbasin1,yousee... microwave1
T-1Observation:Youareinthemiddleofaroom...Action:gotosinkbasin1
T-2Observation:...
ScienceWorld Task:Yourtaskistofinda(n)livingthing.First,focusonthething.Then,moveitto opendoor
thepurpleboxinthebathroom. tooutside
Observation:Youmovetothekitchen.Thisroomiscalledthekitchen.Init,yousee:
—theagent—asubstancecalledair—achair.Onthechairis...
Inyourinventory,yousee:—anorange...
T-1Observation:Thedoorisnowopen.Action:gotokitchen
T-2Observation...Action:opendoortokitchen
Touchdown Task:Followtheflowoftraffic,withtherowofflowersonyourleftandmakealeftat straight
theintersection.TherewillbeawhiteBillboard... ahead
Observation:behindyou,yousee:therightlaneintersection,alarge,blocky,gray...
T-1Observation:behindyou,slightly...Action:slightlytoyourleft...
usedtopopulatelanguagetemplatesthatdescribeobserva- repeatedlycollectexamples(x,v,a),thentrainthepolicy
tionsinlanguage. usingthiscollecteddataandBCdemonstrations.
ScienceWorldisatextualsimulationbenchmarkforbasic
scienceexperiments(Wangetal.,2022). Theagentinter- LFM: imitationlearningusingfeedbackmodels. We
actswithobjectstoconductexperimentsspecifiedinnatural learnasmallandcost-effectivefeedbackmodeldescribed
language,suchasdeterminingtheboilingtemperatureofa inSection3.2toidentifydesirablebehaviourforimitation
material. ScienceWorldisuniquelychallengingtoduethe learning. First,welearnafeedbackmodelonthetraining
largeamountofvariationsintasktypes(30),andparamet- environments. Second,weusethefeedbackmodeltoiden-
ric variations (10-1400) such as the specific substance to tify desirable behaviour in the training environments for
bemelted. Furthermore,ScienceWorldhasasubstantially policyimprovementviaimitationlearning. TocollectLLM
largeractionspaceandlongerhorizontasks. feedbackfortrainingLFMs,wecollectonerolloutforeach
environmentinabenchmarkandsample10k20-stepwin-
Touchdown is a navigation benchmark where the agent
dowsfromtherollouts. Crucially,welimittheamountof
navigatesGoogleStreetViewimagestofollowlong,com-
feedbackdatacollectedfromtheLLMsuchthatthenumber
positionalinstructions(Chenetal.,2019). Touchdownre-
ofoutputtokensproducedbytheLLMisidenticaltoACT-
quiresjointlyreasoningovernaturalimagesfromGoogle
PRED(weuse100kGPT-2tokensforallbenchmarks). This
Streetviewwithocclusionandmulti-sentencenaturallan-
answerswhetherfeedbackmodelisamorecost-effective
guageinstructionsthatdescribelong-horizongoals. Wein-
thandirectactionpredictionforimitationlearning.
troduceanewverbalizationprocedureforTouchdownbased
on matching noun-phrases and image patches with CLIP
LFMA: one-shot adaptation using feedback models.
embeddingstopopulateegocentriclanguagetemplates. Be-
LFMonlyimitatesdesirablebehaviourintrainingenviron-
haviour cloning using our verbalization, detailed in Ap-
ments. Incontrast,LFMAadaptsthepolicytotestenviron-
pendixA,outperformspriorstate-of-theartresults(Schu-
ments. Givennewtestenvironments,weidentifydesirable
mannetal.,2024).
behaviour using feedback models trained on the training
environments, thenperformoneroundofimitationlearn-
5.2.Methods
ing to adapt to new test environments. This experiment
WetrainBCbaselinepoliciesusingexistingdemonstrations testswhetherlanguagefeedbackmodelsgeneralizetonew
foreachbenchmark. Weexaminethreedifferenttechniques environments, and whether we can use their feedback to
forimprovingtheBCpolicy. Table2showsexamplesof adaptpoliciestonewenvironmentswithoutusingLLMsnor
LLMpromptsusedforeachtechnique. additionaldemonstrations.
5.3.Experimentdetails
ACTPRED: imitationlearningfromLLMexperts. We
comparetodirectlyusingLLMsasexpertstopredictactions We use the GPT-4 LLM (2023-03-15) for action pre-
forimitationlearning. First,weexecutekstepsofthebase diction and feedback. We fine-tune the 770M FLAN-
policy, then query the LLM for the next action a given T5(Chungetal.,2022)toobtainpolicyandfeedbackmod-
the instruction x and the verbalized observations v. We els. Weusedescriptionsofthemostrecent20stepsasthe
6PolicyImprovementusingLanguageFeedbackModels
Table2: LLMpromptsusedtocollectdesirablebehaviourforimitationlearning.ACTPREDusesLLMstodirectlygenerate
theappropriateactionforeachstep,whereas LFM usesLLMstogenerate,inbatch,feedbackthatidentifywhichtaken
actionswereproductive. Forbrevity,weabbreviatelongverbalizedobservationsusing“...”.
ACTPRED
Prompt Yourtaskis:lookatalarmclockunderthedesklamp.
Yousee:youareinthemiddleofaroom.lookingquicklyaroundyou,youseeabed1,adesk1,adrawer17...
whatdoyoudecidetodo?availableactions:examineshelf1,examineshelf2,gotobed...
Youdecideto:gotodesk1.
Yousee:youarriveatdesk1.whatdoyoudecidetodo?availableactions:examinedesk1...
Youdecideto:
LLMOutput examinedesk1
LFM
Prompt Youwillbeshownaplaythroughforsolvingatask.
Task:puttwocandleindrawer.
Before:Youopenthedrawer6.Thedrawer6isopen.Init,youseenothing.
Step21.Youraction:closedrawer6.Result:Youclosethedrawer6...
Step22.Youraction:putcandle3in/ondrawer1.Result:Youputthecandle3in...
Istheplayerontherighttracktosolvethetask?
Answeryesorno.Ifyes,listthehelpfulstepsbythestepnumberinbulletform.
LLMOutput Yes
-Step28
-Step29...
verbalized observation v. All models are trained for 10k Table 3: Task completion rate on three benchmarks. We
stepswithbatchsize20andearlystoppingovervalidation evaluateabehaviourcloningagentBC,animitationlearning
demonstrations. AppendixEshowsdetailsonGPUusage. agentusingLLMastheexpertpolicyACTPRED,andour
proposedmethodLFMwhichimitatesdesirablebehaviour
Feedbackmodeltrainingandinference. Totrainfeed- identifiedbyalanguagefeedbackmodel. Onheld-outevalu-
backmodels,wecollectLLMfeedbackover20-stepwin- ationenvironments,LFMoutperformsothermethodsonall
dows. WethenparseLLMfeedbacktoidentifywhetherthe benchmarks. Furthermore,adaptationtothenewenviron-
actiontakenineachstepwasproductivetosolvingthetasks mentsusingthetrainedlanguagefeedbackmodelsresults
outlined in the instructions. We subsample the feedback insignificantadditionalgains(LFMA).
datatoobtainanevensplitofproductiveandnot-productive
ALFWorld ScienceWorld Touchdown
actions. Thisdataissplitintoa80%train/20%validation
datasettotraintheLFM. BC 62.6 45.8 57.5
ACTPRED 56.0 39.0 58.0
Policytrainingandinference. Totrainpolicies,wefine- LFM 64.1 47.1 59.7
tunelanguagemodelstominimizetoken-wisecross-entropy LFMA 74.6 49.3 62.8
of tokens in the ground-truth verbalized action. During
inferencetime,weconsidera(potentiallyverylarge)setof
ble5showsexamplesofLFM-identifieddesirablebehaviour.
plausibleactionsgivenbytheenvironment. Foreachaction,
ThisshowsthatLFMsareaneffectivemeanstoleveragethe
we evaluate the policy’s language model perplexity, and
knowledgeinpretrainedLLMsforpolicyimprovementin
choose the action with the minimum perplexity averaged
language-groundedenvironments,whichagreewithhuman-
overtokens.
identifieddesirablebehaviour. AppendixDalsocompares
GPT-4totheopen-sourceLLAMA270Bfortrainingfeed-
5.4.Resultsanddiscussion
backmodelsusinghumanevaluation. WefindthatGPT-4
Table 3 shows the performance of the policy behaviour consistentlyoutperformsLLAMA2,whichtendstoidentify
cloned from demonstrations BC, imitation learned from spuriousdesirablebehaviour.
LLMs using action prediction ACTPRED, and imitation
learnedfrom LFM. ForLFMs,weshowzero-shotresults LearningLFMsismorecost-effectivethanusingLLMs
(LFM)aswellasafteroneroundofadaptation(LFMA). foractionprediction. AssumingthesameLLMoutput-
tokenquota,Table3comparesusingLLMstotrainfeedback
LFMs improves policy performance across all bench- models(LFM)tousingLLMstopredictactions(ACTPRED)
marks. Table3showsthatLFMimprovesuponthestrong forpolicyimprovement. Specifically, ACTPRED tendsto
behaviourcloningbaselinepolicyBCinallbenchmarks. Ta- predict spurious actions, especially for complex environ-
7PolicyImprovementusingLanguageFeedbackModels
Table4: FeedbackperformancemeasuredbyF1score. We Table5:Exampleofdetailedlanguagefeedback,whichuses
labelstepstheLLMsconsidertobeproductivetobe“posi- moreLLMinteractiontoprovidehuman-interpretablefeed-
tive”actionsandotherstepsnegativeactions. Wemeasure back. Thepromptandoutputdifferencesbetweensuccinct
theF1scoreofthepositive/negativepredictionsmadeby feedback(Table2)anddetailedfeedbackarehighlighted
thelearnedLFMusingtheLLMpredictionsasgroundtruth. inred. NotethatthesecondrowshowsshowsbatchedLLM
Weobservenosignificantperformancedegradationwhen inference,whichisusedtotrainastep-wisefeedbackmodel.
usingamuchmoredetailedfeedbackmodel(LFMD)that Thestep-wiseinferenceofthefeedbackmodelisshownin
alsoprovidesexplanationsbehindthefeedback,summaries thelastrow. Weleaveprovidingsummariesandsuggestions
ofagentbehaviour,andstrategysuggestions. withthelearnedfeedbackmodeltofuturework.
ALFWorld ScienceWorld Touchdown Prompt
Youwillbeshownaplaythroughforsolvingatask.
LFM 93.2 83.7 43.9 Task:puttwocandleindrawer.
LFMD 92.0 82.5 42.5 Before:Youopenthedrawer6.Thedrawer6is...
Step21.Youraction:closedrawer6.Result:You...
Istheplayerontherighttracktosolvethetask?
mentswithlargeactionsspacessuchasScienceWorld. In
Nextunderheading#Summary,summarizeinone
contrast,thedifficultyinidentifyingproductiveactionsis sentencewhattheplayerdoingatahighlevel.Finally
independentoftheactionspace,andLFMconsistentlyim- underheading#Improvement,describehowtheplayer
proves policy even with large action spaces. This shows canimprovetheirstrategytosolvethetask.
that LFMs is a more cost-effective means use LLMs for LLMoutput
policy improvement compared to using LLMs as expert Yes
policiesforimitationlearning. -Step28:Theplayerlocatesthecandlesbyarrivingat
countertop1,whereacandleisavailable.
-Step29:Theplayersuccessfullytakescandle3from
LFMs generalize to new environments, allowing for
countertop1,demonstratingtheabilitytocollect...
policy adaptation without additional LLM usage nor #Summary
demonstrations. Table 4 shows that LFMs trained dur- Theplayerisgatheringcandlesandplacingthemina
ing language feedback learning can accurately recognize drawer,workingtowardscompletingthetaskofputtingtwo...
#Improvement
desirablebehaviourinnewenvironments. Table3shows
Theplayercanimprovetheirstrategyby:
thatimitatingthisbehaviourobtainssignificantpolicyim-
-Notclosingdrawersunnecessarilyafterchecking...
provementacrossallbenchmarks. ThisshowsthatLFMs -Prioritizingfindingandcollectingthecandlesfirst...
generalize to new environments, which allows for policy Learnedfeedbackmodeloutput
adaptationtonewenvironmentsdespitenothavingdemon- Yes.Theplayersuccessfullylocatesthecandle...
Yes.Theplayercorrectlytakesthecandlefrom...
strationsnorLLMaccess.
pretablefeedbackrequiresmorecostlyLLMusage,itallow
LFMs can provide human-interpretable feedback, al-
forhuman-in-theloopverificationofdesirablebehaviour
lowinghuman-in-the-loopverificationduringpolicyim-
identifiedbytheLFM.Consequently,interpretableLFMs
provement. LFMsimprovepolicyperformancewithsuc-
promotesusertrustinthequalityoftheimitationlearning
cinctfeedback. Here,weextendthemtoadditionallypro-
dataandsubsequentpolicybehaviour.
videdetailedexplanations.Consideraninstruction“turnleft
whenyouseethestopsignthengotothesecondbuilding
ontheright”. Supposethatinthecurrentsteptheagentpro- 6.Conclusion
ceedsstraight,arrivingatthestopsign.Insteadofafeedback
We introduced Language Feedback Models that identify
saying“yes”(i.e.theactionwasproductive),theLFMcan
desirablebehaviourforimitationlearning. Onthreeinstruc-
provideahuman-interpretableexplanationforwhythisac-
tionfollowingbenchmarks,smallandcost-effectiveLFMs
tionwasproductive(i.e. “yesbecauseyoufoundthestop
consistentlyoutperformBCbaselinesandusingLLMsas
signwhereyouaresupposedtoturn”). Table5showsthat
experts for imitation learning, without using LLMs dur-
wecanenhanceLFMtoproducedetailedfeedbackbytrain-
ingpolicyimprovement. Inaddition,LFMsgeneralizeand
ingdetailedfeedbackpromptedfromLLMs. Specifically,
provide significant policy adaptation gains on new envi-
wetrainadetailedLFMDtosimultaneouslyidentifyproduc-
ronments, without using LLMs nor new demonstrations.
tiveactions,summarizeagentintent,andsuggestpotential
Finally, LFMs, can provide detailed human-interpretable
highlevelrecoverystrategies. Table4showsthatsurpris-
feedbackthathumanverificationofimitationdata.Weadvo-
ingly,LFMDthatproducedetailedfeedbackperformsim-
cateforfutureexplorationofhowtoexploitdetailedLFMs,
ilarlytothosethatprovidesuccinctfeedback. Thisshows
suchaslearningdense,subgoal-awarerewardmodelsfor
that Language Feedback Models can be used to provide
RL,andtrustworthypolicieswithhumanverification.
accurate feedback interpretable to humans. While inter-
8PolicyImprovementusingLanguageFeedbackModels
7.BroaderImpact Ash, J. T. and Adams, R. P. On Warm-Starting Neural
NetworkTraining. InNeurIPS,2020.
Thispaperpresentsworkonimprovinginstructionfollow-
ingusingLanguageFeedbackModels. Potentialbeneficial
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
societalconsequencesofthisworkincludethedevelopment
Jones,A.,Chen,A.,Goldie,A.,Mirhoseini,A.,McKin-
ofcost-effectivecomputeragentsthatquicklylearntoac-
non,C.,Chen,C.,Olsson,C.,Olah,C.,Hernandez,D.,
curatelyfollowhumancommands. Themethodwepresent
Drain,D.,Ganguli,D.,Li,D.,Tran-Johnson,E.,Perez,
inthisworklearnsanlanguagefeedbackmodeltrainedby
E.,Kerr,J.,Mueller,J.,Ladish,J.,Landau,J.,Ndousse,
exploitingworldknowledgeinLLMs. Weshowthatthis
K., Lukosuite, K., Lovitt, L., Sellitto, M., Elhage, N.,
technique results in faster and more cost-effective policy
Schiefer,N.,Mercado,N.,DasSarma,N.,Lasenby,R.,
improvement than using LLMs as experts. Furthermore,
Larson,R.,Ringer,S.,Johnston,S.,Kravec,S.,Showk,
weshowthatthefeedbackmodelswelearngeneralizeto
S.E.,Fort,S.,Lanham,T.,Telleen-Lawton,T.,Conerly,
new environments, which results in significant gains for
T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-
newenvironmentsnotseenduringtrainingviapolicyadap-
Dodds,Z.,Mann,B.,Amodei,D.,Joseph,N.,McCan-
tation. Finally, we show that language feedback models
dlish, S., Brown, T., and Kaplan, J. Constitutional AI:
canbeextendedtoprovidedetailedcritiquethatincludeex-
HarmlessnessfromAIFeedback,2022.
planations,agentintentsummaries,andhigh-levelstrategy
recommendations. Learningtoprovidethisdetailedoutput Bain, M. and Sammut, C. A framework for behavioural
resultsinnonoticeableaccuracycost,andcanprovideinter- cloning. InMachineIntelligence,1995.
pretablefeedbackhumanscaninspectandverifytocreate
moretrustworthypolicies. Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N. Sched-
uledSamplingforSequencePredictionwithRecurrent
Potential negative societal consequences of this work in-
NeuralNetworks. InNeurIPS,2015.
cludehallucinationsbyLLMsthatmisleadfeedbackmodel
training.Inthissense,feedbackmodelsmaylearntoencour-
Blukis,V.,Terme,Y.,Niklasson,E.,Knepper,R.A.,and
ageactionsthatdonotachievelanguagegoals(e.g.explor-
Artzi,Y. LearningtoMapNaturalLanguageInstructions
ingthebathroomduringakitchencleaningtask). Further-
toPhysicalQuadcopterControlusingSimulatedFlight.
more,theymayencourageactionsthathelpachievegoals
InCoRL,2019.
butareundesirableinotherways(e.g.unsafelyclimbing
over the table to reach the sofa more quickly). In future
Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Chen,X.,
work,wewillexploreusingtechniquesinLLMalignment
Choromanski,K.,Ding,T.,Driess,D.,Dubey,A.,Finn,
to learn more robust language feedback models, as well
C.,Florence,P.,Fu,C.,Arenas,M.G.,Gopalakrishnan,
asinvestigatelearningfromdetailedfeedbackmodelswith
K.,Han,K.,Hausman,K.,Herzog,A.,Hsu,J.,Ichter,B.,
humanverificationtoimprovethetrustworthinessofdown-
Irpan,A.,Joshi,N.,Julian,R.,Kalashnikov,D.,Kuang,
streampolicies.
Y., Leal, I., Lee, L., Lee, T.-W. E., Levine, S., Lu, Y.,
Michalewski,H.,Mordatch,I.,Pertsch,K.,Rao,K.,Rey-
References mann,K.,Ryoo,M.,Salazar,G.,Sanketi,P.,Sermanet,
P.,Singh,J.,Singh,A.,Soricut,R.,Tran,H.,Vanhoucke,
Abbeel,P.andNg,A.Y.Apprenticeshiplearningviainverse
V.,Vuong,Q.,Wahid,A.,Welker,S.,Wohlhart,P.,Wu,
reinforcementlearning. InICML,2004.
J.,Xia,F.,Xiao,T.,Xu,P.,Xu,S.,Yu,T.,andZitkovich,
B. RT-2: Vision-Language-ActionModelsTransferWeb
Ahn,M.,Brohan,A.,Brown,N.,Chebotar,Y.,Cortes,O.,
KnowledgetoRoboticControl,2023.
David,B.,Finn,C.,Fu,C.,Gopalakrishnan,K.,Hausman,
K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B.,
Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
Irpan,A.,Jang,E.,Ruano,R.J.,Jeffrey,K.,Jesmonth,
J.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,
S., Joshi, N.J., Julian, R., Kalashnikov, D., Kuang, Y.,
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
Lee,K.-H.,Levine,S.,Lu,Y.,Luu,L.,Parada,C.,Pastor,
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.M.,Wu,
P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D.,
J.,Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M.,
Sermanet,P.,Sievers,N.,Tan,C.,Toshev,A.,Vanhoucke,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
V.,Xia,F.,Xiao,T.,Xu,P.,Xu,S.,Yan,M.,andZeng,
S.,Radford,A.,Sutskever,I.,andAmodei,D. Language
A. DoAsICan,NotAsISay: GroundingLanguagein ModelsareFew-ShotLearners. InNeurIPS,2020.
RoboticAffordances,2022.
Chen, D. and Mooney, R. Learning to Interpret Natural
Andreas,J.andKlein,D. Alignment-basedcompositional LanguageNavigationInstructionsfromObservations. In
semanticsforinstructionfollowing. InEMNLP,2015. AAAI,2011.
9PolicyImprovementusingLanguageFeedbackModels
Chen, H., Suhr, A., Misra, D., Snavely, N., and Artzi, Y. Nakano,R.,Hilton,J.,Balaji,S.,Wu,J.,Ouyang,L.,Kim,
Touchdown: NaturalLanguageNavigationandSpatial C.,Hesse,C.,Jain,S.,Kosaraju,V.,Saunders,W.,Jiang,
ReasoninginVisualStreetEnvironments.InCVPR,2019. X., Cobbe, K., Eloundou, T., Krueger, G., Button, K.,
Knight, M., Chess, B., and Schulman, J. WebGPT:
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Browser-assistedquestion-answeringwithhumanfeed-
Fedus,W.,Li,Y.,Wang,X.,Dehghani,M.,Brahma,S., back,2022.
Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, OpenAI. GPT-4 Technical Report.
K.,Valter,D.,Narang,S.,Mishra,G.,Yu,A.,Zhao,V., https://arxiv.org/abs/2303.08774v4,2023.
Huang,Y.,Dai,A.,Yu,H.,Petrov,S.,Chi,E.H.,Dean,
J.,Devlin,J.,Roberts,A.,Zhou,D.,Le,Q.V.,andWei,J. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
ScalingInstruction-FinetunedLanguageModels,2022. Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,
Krueger,G.,andSutskever,I. LearningTransferableVi-
Colas, C., Teodorescu, L., Oudeyer, P.-Y., Yuan, X., and sualModelsFromNaturalLanguageSupervision,2021.
Coˆte´, M.-A. Augmenting autotelic agents with large
languagemodels. InProceedingsofThe2ndConference Ross, S., Gordon, G. J., and Bagnell, J. A. A Reduction
onLifelongLearningAgents,PMLR.PMLR,2023. ofImitationLearningandStructuredPredictiontoNo-
RegretOnlineLearning. InAISTATS,2011.
Deng,X.,Gu,Y.,Zheng,B.,Chen,S.,Stevens,S.,Wang,
B.,Sun,H.,andSu,Y. Mind2Web: TowardsaGeneralist Schaal, S. Is imitation learning the route to humanoid
AgentfortheWeb. InNeurIPS,2023. robots? TrendsinCognitiveSciences,1999.
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT: Schumann, R., Zhu, W., Feng, W., Fu, T.-J., Riezler, S.,
Pre-trainingofDeepBidirectionalTransformersforLan- andWang,W.Y. VELMA:VerbalizationEmbodiment
guageUnderstanding. InNAACL,2019. ofLLMAgentsforVisionandLanguageNavigationin
StreetView. InAAAI,2024.
Fried, D., Andreas, J., and Klein, D. Unified Pragmatic
Models for Generating and Following Instructions. In Shridhar,M.,Thomason,J.,Gordon,D.,Bisk,Y.,Han,W.,
NAACL,2018. Mottaghi, R., Zettlemoyer, L., and Fox, D. ALFRED:
ABenchmarkforInterpretingGroundedInstructionsfor
He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual EverydayTasks. InCVPR,2020.
LearningforImageRecognition,2015.
Shridhar, M., Manuelli, L., and Fox, D. CLIPort: What
Honnibal,M.andMontani,I. spaCy2: Naturallanguage andWherePathwaysforRoboticManipulation. InCoRL,
understanding with Bloom embeddings, convolutional 2021a.
neural networks and incremental parsing. To appear,
2017. Shridhar, M., Yuan, X., Coˆte´, M.-A., Bisk, Y., Trischler,
A.,andHausknecht,M. ALFWorld: AligningTextand
Huang,W.,Xia,F.,Xiao,T.,Chan,H.,Liang,J.,Florence, Embodied Environments for Interactive Learning. In
P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., ProceedingsoftheInternationalConferenceonLearning
Sermanet,P.,Brown,N.,Jackson,T.,Luu,L.,Levine,S., Representations(ICLR),2021b.
Hausman,K.,andIchter,B.InnerMonologue:Embodied
ReasoningthroughPlanningwithLanguageModels. In Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,
CoRL,2022. R., Voss, C., Radford, A., Amodei, D., andChristiano,
P. Learning to summarize from human feedback. In
Kollar,T.,Tellex,S.,Roy,D.,andRoy,N. Towardunder- NeurIPS,2020.
standingnaturallanguagedirections. InHRI,2010.
Sutton,R.S.andBarto,A.G. ReinforcementLearning: An
Lee,H.,Phatale,S.,Mansoor,H.,Mesnard,T.,Ferret,J., Introduction. MITPress,2018.
Lu, K., Bishop, C., Hall, E., Carbune, V., Rastogi, A.,
andPrakash,S. RLAIF:ScalingReinforcementLearning Torabi, F., Warnell, G., andStone, P. Behavioralcloning
fromHumanFeedbackwithAIFeedback,2023. fromobservation,2018.
MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk Wang,R.,Jansen,P.A.,Coˆte´,M.-A.,andAmmanabrolu,P.
thetalk: Connectinglanguage,knowledge,andactionin Scienceworld: Isyouragentsmarterthana5thgrader?
routeinstructions. InAAAI,2006. InEMNLP,2022.
10PolicyImprovementusingLanguageFeedbackModels
Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Ichter,B.,
Xia,F.,Chi,E.,Le,Q.,andZhou,D. Chain-of-Thought
PromptingElicitsReasoninginLargeLanguageModels.
InNeurIPS,2022.
Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
K., and Cao, Y. ReAct: Synergizing Reasoning and
ActinginLanguageModels. InICLR,2023.
Yuan,W.,Pang,R.Y.,Cho,K.,Sukhbaatar,S.,Xu,J.,and
Weston,J. Self-RewardingLanguageModels,2024.
Zhong, V., Rockta¨schel, T., and Grefenstette, E. RTFM:
GeneralisingtoNovelEnvironmentDynamicsviaRead-
ing. InICLR,2020.
Zhong,V.,Hanjie,A.W.,Wang,S.I.,Narasimhan,K.,and
Zettlemoyer,L. SILG:TheMulti-environmentSymbolic
InteractiveLanguageGroundingBenchmark.InNeurIPS,
2021.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Rad-
ford, A., Amodei, D., Christiano, P., and Irving, G.
Fine-TuningLanguageModelsfromHumanPreferences,
2020.
11PolicyImprovementusingLanguageFeedbackModels
Table6:Statisticsfrombenchmarksasmeasuredbytrainingdemonstrations. ThearetheaveragenumberofGPT-2tokensin
theinstruction,verbalizedobservation,andaction;theaveragedemonstrationsteps;theaveragenumberofplausibleactions
inastate;thenumberofuniqueactions,instructions,andobservations;andfinallythenumberoftrainingdemonstrations.
ALFWorld SciWorld Touchdown
Inslen|x| 8.8 64.7 93.4
Obslen|v| 23.9 239.4 284.9
Actlen|a| 4.5 6.0 2.4
Trajlen|τ| 19.7 55.1 34.2
|Actspace| 29.9 1.9k 2.1
#act|{a}| 2.6k 2.4k 8
#ins|{τ}| 1.0k 1.2k 6.5k
#obs|{v}| 18.2k 157k 34.3k
#demos 3.5k 3.6k 6.5k
A.Verbalizationofvisualenvironments
HowcanweleveragetheworldknowledgelearnedbyLLMsfrompretrainingonvastquantitiesoftext? Inmanyinstruction
followingproblems,environmentobservationsareinherentlyvisual. Inthissection,wedescribeaverbalizationprocedure
thatconvertsvisualobservationstolanguagedescriptions,sothatLLMscanmakeinferencesbyjointlyreferringtothe
instructionandenvironmentobservations. Specifically,weuseTouchdownasanexample.
As shown in Figure 2, Touchdown (Chen & Mooney, 2011) is primarily a visual navigation problem. Given a set of
connectedGoogleStreetviewpanoramasthatrepresentneighbourhoodsinManhattan,anagentmustfollowlong,complex
naturallanguageinstructionstonavigatetothecorrectlocation. Crucialtothistaskofnavigationarelandmarksreferred
to by the instructions. For instance, the instruction “turn so the scaffolding is on your left and... to the next corner
andturnright...” referstothelandmarksscaffoldingandnextcorner. PriorworkinverbalizationuseLLMstoidentify
landmarks(Schumannetal.,2024). Inthiswork,wetakethesetofcommonnoun-phrasesinthecorpusofinstructionstobe
landmarks.
Extractingalignednoun-phraseannotationsforvisualpatches First,weidentifyallnoun-phrasesusingSPACY(Hon-
nibal&Montani,2017). Givenavisualscene,wedividethesceneinto300x300pixelnon-overlappingpatches. Foreach
patch,weidentifythenoun-phrasewiththehighestcosinesimilaritybetweenthenoun-phrasetextembeddingandtheimage
patchembedding. WeusetextandvisualencodersfromCLIP(Radfordetal.,2021)toextractembeddingsforeachmodality.
Forpatcheswithnoalignednoun-phrasewithcosinesimilaritygreaterthan0.2,wedonotprovideannotatedanoun-phrase.
Convertingtoverbalizedobservations Toobtainverbalizedobservationsintheformofanegocentricscenedescription,
weconsiderthedirectiontheagentisfacing(showninblue)aswellthedirectionsofpossiblenextsteps(showningreen).
Thenoun-phrasesidentifiedinthescenearethencategorizedinto8directionsin45-degreeincrements,relativetotheagent’s
currentorientation: straightahead(337.5to22.5),slightlytoyourright(22.5to67.5),toyourleft(67.5to112.5),behind
you,slightlytoyourright(112.5to157.5),behindyou(157.5to202.5),behindyou,slightlytoyourleft(202.5to247.5),to
yourleft(247.5to292.5),andslightlytoyourleft(292.5to337.5). Asceneisthenrenderedasfollows:
Straight ahead, you see
- a white van
Slightly to your right, you see
- a red brick building
- a scaffold...
Thisverbalizationachievessignificantlyhighertask-completionaccuracycomparedtopriorstate-of-the-artresults,after
behaviouralcloningfromdemonstrations. Forinstance,weobtain57.5%task-completioncomparedto26.4%bySchumann
etal.(2024).
Statisticsofverbalizedenvironments InAppendixATable6,weshowstatisticsofverbalizedenvironmentsasquantita-
tiveevidenceoftheirchallenges.
12PolicyImprovementusingLanguageFeedbackModels
B.PseudocodeforPolicyImprovementusingLanguageFeedbackModels
In this section we detail, using pseudocode, the procedure for policy improvement using Language Feedback Models.
Algorithm 1 describes learning a model from LLMs. Algorithm 2 describes identifying desirable behaviour that are
productiveforsolvingtasksspecifiedintheinstruction,andthenusingthisbehaviourforimitationlearning. Algorithm3
describestheiterativepolicyimprovementprocedureusingthesetwoalgorithms.
Algorithm1TRAINFEEDBACKMODEL: TrainingaLanguageFeedbackModelusingLLMfeedback.
1: Inputs: initialpolicyπ,LLMLLM,environmentE
2: FeedbackdatasetF ←{}
3: fori=1...N do
4: x← SAMPLEINSTRUCTION
5: τ i ← ROLLOUT(π,E,x)
6: forwindoww j inτ ido
7: y ← QUERYLLMFORFEEDBACK(LLM,w j,x)
8: forverbalizedobservationv k,LLMfeedbacky k ineachstepofydo
(cid:83)
9: F ←F (v k,y k)
10: endfor
11: endfor
12: endfor
13: Feedbackmodelf ← TRAINLM(F)
Algorithm2IMITATEUSINGFEEDBACK: Imitationlearningusingdesirablebehaviouridentifiedbyafeedbackmodel.
1: Inputs: basepolicyπ,environmentE,feedbackmodelf
2: ImitationdatasetG←behaviourcloningdataset
3: fori=1...N do
4: x← SAMPLEINSTRUCTION
5: τ i ← ROLLOUT(π,E,x)
6: forverbalizedobservationv k,actiona k ineachstepofτ ido
7: y k =f(v k)
8: ify k isdesirablethen
(cid:83)
9: G←G (v k,a k)
10: endif
11: endfor
12: endfor
13: Improvedpolicyπ′ ← TRAINLM(G)
Algorithm3PolicyimprovementusingLanguageFeedbackModels.
1: Inputs: basepolicyπ,environmentE
2: Feedbackmodelf ← TRAINFEEDBACKMODEL(π,LLM,E)
3: π 0 ←π
4: fork =1...N do
5: π k ← IMITATEUSINGFEEDBACK(π k−1,E,f)
6: endfor
13PolicyImprovementusingLanguageFeedbackModels
Table7: Taskcompletionrateonevaluationbenchmarks,includingDAGGER.
ALFWorld ScienceWorld Touchdown
BC 62.6 45.8 57.5
ACTPRED 56.0 39.0 58.0
DAGGER 55.2 22.5 50.2
LFM 64.1 47.1 59.7
LFMA 74.6 49.3 62.8
C.Comparisonto DAGGER
OurmainexperimentsinSection5.4illustratethedifficultyofusingLLMsasanexperttopredictactions. Specifically,
we show that when these predictions are used for imitation learning, the resulting policy improvement is worse than
usingLanguageFeedbackModels. Thisperformancedegradationisexacerbatedinenvironmentswithlargeractionspaces,
suchasScienceWorld.
DAGGER(Rossetal.,2011)isanintermediatemethodbetweenLanguageFeedbackModelsandusingLLMsasanexpert
policiesforimitationlearning. Specifically,inDAGGER,wealsouseLLMsasexpertstopredictaction. However,instead
ofusingLLMsduringeachstep, in DAGGER weuseLLMstoprovidebatchedretroactiveactionpredictionsimilarto
how in Language Feedback Models we use LLMs to provide batched retroactive feedback. Here, we apply DAGGER
action prediction to the exact same number of examples as when we collect feedback data for LFMs. In Table 7, we
compare DAGGER performance to those using LLM as an expert (ACTPRED) and using Language Feedback Models
(LFM). WefindthatalthoughDAGGERismoreefficientthanACTPREDinthatitannotatessyntheticexamplesinbatch,it
underperformsACTPRED(andconsequentlyLFM)acrossallbenchmarks.
14PolicyImprovementusingLanguageFeedbackModels
Table8: AgreementbetweenGPT-4andLLAMA2acrossthebenchmarks. Wecollectstepsfromrolloutsonthetraining
environmentswhereeitherGPT-4or LLAMA 2 identifiedaproductiveaction. Thistableshowspercentageofofthose
actionsthatareidentifiedexclusivelybyGPT-4,exclusivelybyLLAMA2,andidentifiedbybothmodels. Thetotalnumber
ofstepsidentfiedare40569forALFWorld,68565forScienceWorld,and90529forTouchdown.
GPT-4only LLAMA2only both
ALFWorld 14.4% 49.3% 36.2%
ScienceWorld 10.2% 62.3% 27.5%
Touchdown 22.3% 67.3% 10.4%
Table9: HumanverificationofLLMfeedbackintermsofpercentageoftruepositivesandfalsepositives. Atruepositive
(TP)isastepthatiscorrectlyidentifiedbytheLLMasbeingproductivetosolvingthetask. Afalsepositive(FP)isastep
thatiswronglyidentifiedbytheLLMasproductive. Wemanuallyevaluate10examplesfromeachbenchmark,eachwith
upto20steps. Support(#ofsteps)isshowninbrackets.
GPT-4 LLAMA2
TP FP TP FP
ALFWorld 100%(22) 0 32%(18) 68%(38)
ScienceWorld 78%(38) 22%(11) 48%(38) 52%(41)
Touchdown 81%(22) 19%(5) 39%(24) 61%(38)
D.QuantitativeandQualitativeAnalysesofLearnedLanguageFeedback
ComparisonofGPT-4toLLAMA270B Howmuchdifferenceistherebetweenlanguagefeedbackobtainedfromthe
open-sourceLLAMA2vsfromGPT-4? Table8showsthat,surprisingly,thereisalargedegreeofdisagreementbetween
GPT4andLLAMA2. Specifically,LLAMA2identifiessignificantlymoreactionsasbeingproductivetoachievingthegoal.
WeperformamanualanalysisoflanguagefeedbackbyGPT-4andLLAMA2tocharacterizequalitativedifferencesbetween
feedbackcollectedbythesetwomodels. First,werolloutBCpolicies,thenaskeachmodelforfeedback. Eachexample
containsasegmentofupto20stepsextractedfromarollout,andtheLLMispromptedtolistproductivesteps. Foreach
steptheLLMidentifiesasproductivetosolvingthetask,wemanuallyverifywhetherthestepisindeedproductive. We
manuallyinspect10examplesfromeachmodelforeachbenchmark,foratotalof10×2×3=60examples. Table9shows
thenumberoftrueandfalsepositivespredictedbybothmodelsinthismanualevaluation. Wefindthatasignificantnumber
ofstepsareincorrectlydeterminedbyLLAMA2asdesirable. WhenwetrainthepolicyonacombinationofLLAMA2data
anddemonstrationsusedtolearntheBCpolicy,weobtainworsetask-completionpercentagethanusingGPT-4dataand
demonstrations. Specially,performancedropfrom64.1%(GPT-4)to56.0%(LLAMA 2)onALFWorld,from47.1%to
47.0%onScienceWorld,andfrom59.7%to56.5%onTouchdown.
Table10showssomeexamplesofstepsidentifiedasproductivebythesemodelsthatillustrate LLAMA 2’stendencyto
identifyspuriousactionsasbeingproductive. IntheALFWorldexamples,forinstance,LLAMA2hasastrongtendency
toidentifyopeningandclosingcabinetsanddrawersasproductive,eventhoughtheyhavenothingtodowithputtinga
cleansoapbaronthecountertop(thefirstinstruction)orputtingacleanspatulaonthesidetable(thesecondinstruction).
SimilarlyinScienceWorld,LLAMA 2identifiesunnecessaryactionssuchasgoingoutside(example1)andgoingtothe
bedroom (example 2) as productive, even when the instruction explicitly details that the aluminum foil is found in the
kitchen(example1)andthattheunknownsubstanceisfoundintheworkshop(example2). Finally,LLAMA2alsotendsto
identifyspuriousactionsasproductiveinTouchdown. Inthelastexample,theinstructionaskstotakealeftafterthefirst
intersection,butLLAMA2rewardstheleftturnduringthefirstturn,beforetheagentevenarrivesatthefirstintersection.
GPT-4,ontheotherhand,correctlyidentifiesStep8,whentheagentfinallyencountersthefirstintersection,asproductive.
WeshowinSection5.4thatsmallandcost-effectiveLanguageFeedbackModelsareabletoreplicateLLMfeedbackthrough
training. OurcomparisonbetweenGPT-4andLLAMA2showthatalesspowerfulmodelsuchasLLAMA2areunableto
providehigh-qualityfeedback. Thesummaryfromthisexperimentarethenthat1)powerfulLLMsarenecessarytoprovide
goodfeedback,butexpensivetorunduringon-linepolicyimprovement3)consequently,learningsmallLFMsisaneffective
solutiontoachievehighfeedbackperformancewhilereducinginferencecostduringpolicyimprovement.
15PolicyImprovementusingLanguageFeedbackModels
E.GPUUsage
Wetrainfeedbackmodelsandpoliciesusing80GBA100GPUs. Toproducerolloutsatinparallel,weuseaclusterof
20032GBV100GPUs. Forallenvironments,feedbackmodeltrainingtakesunder24hoursusingoneA100GPUwhile
inferencecanbeperformedlocallyusinga32GBGPUunder2hours. Policytrainingrequires1dayforALFWorld,2days
forScienceWorld,and3daysforTouchdown. Forallenvironments,policyrolloutovertheentireevaluationenvironments
canbeperformedovertheclusterof20032GBV100GPUsinunder6hours.
16PolicyImprovementusingLanguageFeedbackModels
Table10: ExamplestepsidentifiedasproductivebyGPT-4,LLAMA2,andboth. Touchdownstepsaretruncatedforbrevity.
ALFWorld
Task:cleansomesoapbarandputitincountertop.
Step3Youraction:takesoapbottle2fromtoilet1Result:Youpickupthesoapbottle2fromthetoilet1.
Step6Youraction:gotocountertop1Result:Youarriveatloc11.Onthecountertop1,youseeacandle2,acandle1,asoapbar2...
Step11Youraction:opencabinet1Result:Youopenthecabinet1.Thecabinet1isopen.Init,youseeacandle3,andatoiletpaper1.
Step17Youraction:opencabinet1Result:Youopenthecabinet1.Thecabinet1isopen.Init,youseeacandle3,andatoiletpaper1.
Step19Youraction:gotocabinet3Result:Youarriveatloc12.Thecabinet3isclosed.
Task:putacleanspatulainsidetable.
Step3Youraction:gotosidetable1Result:Youarriveatloc22.Onthesidetable1,youseenothing.
Step10Youraction:gotodrawer3Result:Youarriveatloc19.Thedrawer3isclosed.
Step11Youraction:opendrawer3Result:Youopenthedrawer3.Thedrawer3isopen.Init,youseenothing.
Step14Youraction:opendrawer4Result:Youopenthedrawer4.Thedrawer4isopen.Init,youseenothing.
Step15Youraction:closedrawer4Result:Youclosethedrawer4.
Step19Youraction:gotodrawer6Result:Youarriveatloc27.Thedrawer6isclosed.
Step20Youraction:opendrawer6Result:Youopenthedrawer6.Thedrawer6isopen.Init,youseenothing.
ScienceWorld
Task:Yourtaskistodetermineifaluminumfoiliselectricallyconductive.Thealuminumfoilislocatedaroundthekitchen.First,
focusonthealuminumfoil.Ifitiselectricallyconductive,placeitintheyellowbox.Ifitiselectricallynonconductive,placeitinthe
purplebox.Donotfocusonobjectsnotcentraltocompletingthetask!
Step21Youraction:opendoortogreenhouseResult:Thedoorisnowopen...
Step23Youraction:opendoortooutsideResult:Thedoorisnowopen...
Step25Youraction:opendoortofoundryResult:Thedoorisnowopen...
Step29Youraction:gotooutsideResult:Youmovetotheoutside...
Step31Youraction:gotokitchenResult:Youmovetothekitchen...
Step32Youraction:pickupaluminumfoilResult:Youmovethealuminumfoiltotheinventory...
Step33Youraction:focusonaluminumfoilResult:Youfocusonthealuminumfoil...
Step37Youraction:gotoworkshopResult:Youmovetotheworkshop...
Step38Youraction:movealuminumfoiltoyellowboxResult:Youmovethealuminumfoiltotheyellowbox...
Task: YourtaskistodetermineifunknownsubstanceNiselectricallyconductive. TheunknownsubstanceNislocatedaround
theworkshop. First,focusontheunknownsubstanceN.Ifitiselectricallyconductive,placeitinthebluebox. Ifitiselectrically
nonconductive,placeitintheorangebox.Donotfocusonobjectsnotcentraltocompletingthetask!
Step1Youraction:opendoortoartstudioResult:Thedoorisnowopen...
Step4Youraction:gotohallwayResult:Youmovetothehallway...
Step6Youraction:gotobedroomResult:Youmovetothebedroom...
Step9Youraction:opendoortohallwayResult:Thedoorisalreadyopen...
Step11Youraction:opendoortoworkshopResult:Thedoorisnowopen...
Step12Youraction:gotoworkshopResult:Youmovetotheworkshop...
Step13Youraction:pickupunknownsubstanceResult:YoumovetheunknownsubstanceNtotheinventory...
Step15Youraction:moveunknownsubstancetoblueboxResult:YoumovetheunknownsubstanceNtothebluebox...
Touchdown
Task:Goingwiththeflowoftraffic,proceedtotheintersectionandtakeleft.Thereshouldbesomebluescaffoldingonyourright
handside,takealefthere.Youshouldnowhaveatallsolidgreenfenceonyourighthandside,gostraighttothenextintersectionand
takealeft.There’sabunchofscaffoldingonthelefthandside,gostraight.Passthefirstintersectionyoucomeacross.Youwillpass
mailtrucksparkedonbothsidesoftheroad.
Step1Youraction:slightlytoyourleftResult:behindyou,yousee:parked,whiteboxtrucks,twowidersidewalks,thisnarrowtwo
laneroad,therightsidewalkbuildings.behindyou,sightlytoyourleft,yousee:,threeair-conditioners,threeawning,asmaller
yellowtaxi.toyourleft,yousee:,thetheaterawning,ayellowcabcar,thesecondpurpleawning.slightlytoyourleft,yousee:,a
white-cappedhydrant,ornategraybalconies,thepurplewayfairtruck.straightahead,yousee:,amedianstrip,sometall,brick
buildings,parked,whiteboxtrucks,abluebuslanesign,therightsidewalkbuildings.slightlytoyourright,yousee:,thisnearest
intersection,alightcoloredstonebuilding,ablueandredbusstopsigntoyourright,yousee:,separatebuildingentrances,the
half-mooncurb,thecompany’sname.behindyou,slightlytoyourright,yousee:,,thenastorefront,ornategraybalconies,thefirst
littleweirdintersection.
Step8Youraction:straightaheadResult:behindyou,yousee:,abrownstorefront,surfacestreets,therightsidewalkbuildings,a
parkedblackboxtruck,someunremarkablebrickbuildings.behindyou,sightlytoyourleft,yousee:,thenastorefront,awhite/grey
van,alarge,blocky,graybuilding.toyourleft,yousee:,alargewhitestoresign,aconstructionvehicle,alonggrayandwhite5
storybuildingslightlytoyourleft,yousee:,sometall,brickbuildings,fedexvan,awhite/greyvan.straightahead,yousee:,a
largeredbrickapartmentbuilding,anorangeandwhitetrafficobject,3rdand4thintersections,asmallbluecar,therightsidewalk
buildings,theparkedyellowsuvtaxi.slightlytoyourright,yousee:,constructionawning,alargewhitefedextruck,awhite/grey
van.toyourright,yousee:,thisvanstop,thebuildingedge,alargewhitefedextruck.behindyou,slightlytoyourright,yousee:,
agreen-toppedscaffolding,alargewhitefedextruck,aparkedblackboxtruck.
17