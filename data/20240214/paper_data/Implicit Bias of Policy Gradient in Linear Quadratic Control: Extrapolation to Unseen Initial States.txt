Implicit Bias of Policy Gradient in Linear Quadratic Control:
Extrapolation to Unseen Initial States
NoamRazin†* YotamAlexander†* EdoCohen-Karlik† RajaGiryes† AmirGloberson† NadavCohen†
Abstract objectiveisoftenunderdetermined—i.e.itadmitsmulti-
plesolutions(parameterassignmentsfittingtrainingdata)
Inmodernmachinelearning,modelscanoftenfit
thatdifferintheirperformanceontestdata—andtheex-
trainingdatainnumerousways,someofwhich
tenttowhichalearnedsolutiongeneralizesisdetermined
perform well on unseen (test) data, while oth-
by an implicit bias of the training algorithm (Neyshabur,
ers do not. Remarkably, in such cases gradient
2017; Vardi, 2023). Remarkably, variants of gradient de-
descentfrequentlyexhibitsanimplicitbiasthat
scent frequently converge to solutions with excellent in-
leads to excellent performance on unseen data.
distributiongeneralization(seeZhangetal.(2017)),which
Thisimplicitbiaswasextensivelystudiedinsu-
insomecasesextendstoout-of-distributiongeneralization
pervised learning, but is far less understood in
(cf.Milleretal.(2021)). Theimplicitbiasofgradientde-
optimalcontrol(reinforcementlearning). There,
scenthasaccordinglyattractedvasttheoreticalinterest,with
learningacontrollerappliedtoasystemviagra-
existinganalysesfocusingprimarilyonthebasicframework
dient descent is known as policy gradient, and
ofsupervisedlearning(see,e.g.,Neyshaburetal.(2014);
a question of prime importance is the extent to
Gunasekaretal.(2017);Soudryetal.(2018);Aroraetal.
whichalearnedcontrollerextrapolatestounseen
(2019);Ji&Telgarsky(2019a;b);Woodworthetal.(2020);
initialstates. Thispapertheoreticallystudiesthe
Razin&Cohen(2020);Lyu&Li(2020);Lyuetal.(2021);
implicitbiasofpolicygradientintermsofextrap-
Pesme et al. (2021); Razin et al. (2021; 2022); Frei et al.
olationtounseeninitialstates. Focusingonthe
(2023b;a);Abbeetal.(2023)).
fundamentalLinearQuadraticRegulator(LQR)
problem, we establish that the extent of extrap- Asopposedtosupervisedlearning,littleisknownaboutthe
olationdependsonthedegreeofexplorationin- implicitbiasofgradientdescentinthechallengingframe-
ducedbythesystemwhencommencingfromini- workofoptimalcontrol(seeoverviewofrelatedworkin
tialstatesincludedintraining. Experimentscor- Section 2). In optimal control — which in its broadest
roborateourtheory,anddemonstrateitsconclu- formisequivalenttoreinforcementlearning—thegoalis
sionsonproblemsbeyondLQR,wheresystems tolearnacontroller(alsoknownaspolicy)thatwillsteer
arenon-linearandcontrollersareneuralnetworks. a given system (also known as environment) such that a
We hypothesize that real-world optimal control givencostisminimized(orequivalently,agivenrewardis
maybegreatlyimprovedbydevelopingmethods maximized)(Sontag,2013). Algorithmsthatlearnacon-
forinformedselectionofinitialstatestotrainon. trollerbydirectlyparameterizingitandsettingitsparame-
tersthroughgradientdescentareknownaspolicygradient
methods. Forimplementingsuchmethods,gradientswith
respect to controller parameters are either estimated via
1 Introduction
sampling(Williams,1992),or,incaseswheredifferentiable
Theabilitytogeneralizefromtrainingdatatounseentest formsforthesystemandcostareathand,thegradientsmay
dataisacoreaspectofmachinelearning. Broadlyspeak- becomputedthroughanalyticdifferentiation(see,e.g.,Hu
ing,therearetwotypesofgeneralizationonemayhopefor: etal.(2019);Qiaoetal.(2020);Claveraetal.(2020);Mora
(i)in-distributiongeneralization,wheretestdataisdrawn etal.(2021);Gillen&Byl(2022);Howelletal.(2022);Xu
from the same distribution as training data; and (ii) out- etal.(2022);Wiedemannetal.(2023)).
of-distributiongeneralization,alsoknownasextrapolation,
An issue of prime importance in optimal control (and re-
wheretestdataisdrawnfromadifferentdistributionthan
inforcementlearning)istheextenttowhichalearnedcon-
that of the training data. In modern regimes the training
trollerextrapolatestoinitialstatesunseenintraining. In-
*Equal contribution †Tel Aviv University. Correspondence deed,inreal-worldsettingstrainingisoftenlimitedtofew
to:NoamRazin<noamrazin@mail.tau.ac.il>,YotamAlexander initialstates,andadeployedcontrollerislikelytoencounter
<yotama@mail.tau.ac.il>.
1
4202
beF
21
]GL.sc[
1v57870.2042:viXraImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
initialstatesthatgowellbeyondwhatithasseenintrain- nomenon extends to non-linear systems and (non-linear)
ing(Zhuetal.,2020;Dulac-Arnoldetal.,2021).Theability neuralnetworkcontrollers.
ofthecontrollertohandlesuchinitialstatesisimperative,
Inreal-worldoptimalcontrol(andreinforcementlearning),
particularlyinsafety-criticalapplications(e.g.robotics,in-
contemporarylearningalgorithmsoftenextrapolatepoorly
dustrialmanufacturing,orautonomousdriving).
toinitialstatesunseenintraining(Rajeswaranetal.,2017;
Thecurrentpaperseekstotakefirststepstowardstheoreti- Zhangetal.,2018;2019;Fujimotoetal.,2019;Wittyetal.,
callyaddressingthefollowingquestion. 2021). Our results lead us to believe that this extrapola-
tionmaybegreatlyimprovedbydevelopingmethodsfor
Towhatextentdoestheimplicitbiasofpolicygradient
informedselectionofinitialstatestotrainon. Wehopethat
leadtoextrapolationtoinitialstatesunseenintraining?
ourworkwillencourageresearchalongthisline.
Asatestbedfortheoreticalstudy,weconsiderthefundamen- Paperorganization. Theremainderofthepaperisorga-
talLinearQuadraticRegulator(LQR)problem(Anderson& nizedasfollows. Section2reviewsrelatedwork. Section3
Moore,2007). There,systemsarelinear,costsarequadratic, establishespreliminaries. Section4deliversourtheoretical
anditisknownthatoptimalcontrollersarelinear(Anderson analysis — a characterization of extrapolation to unseen
& Moore, 2007). Learning linear controllers in LQR via initialstatesforlinearcontrollerstrainedviapolicygradi-
policygradienthasbeenthesubjectofvarioustheoretical entinunderdeterminedLQRproblems. Section5presents
analyses(e.g.,Fazeletal.(2018);Maliketal.(2019);Bhan- experimentswiththeanalyzedLQRproblems,aswellas
dari&Russo(2019);Mohammadietal.(2019;2021);Bu with non-linear systems and (non-linear) neural network
etal.(2019;2020);Jinetal.(2020);Gravelletal.(2020); controllers. Lastly,Section6concludes.
Hambly et al. (2021); Hu et al. (2023)). However, these
analysesdonottreatunderdeterminedtrainingobjectives,
2 RelatedWork
thusleaveopenourquestionontheeffectofimplicitbias
onextrapolation. Tofacilitateitsstudy,wefocusonLQR Most theoretical analyses of the implicit bias of gradient
trainingobjectivesthatareunderdetermined. descent(orvariantsthereof)focusonthebasicframework
of supervised learning. Such analyses traditionally aim
Our theoretical analysis reveals that in underdetermined
toestablishin-distributiongeneralization,ortocharacter-
LQRproblems,theextenttowhichlinearcontrollerslearned
ize solutions found in training without explicit reference
via policy gradient extrapolate to initial states unseen in
totestdata(see,e.g.,Neyshaburetal.(2014);Gunasekar
training,dependsontheinterplaybetweenthesystemand
etal.(2017);Soudryetal.(2018);Aroraetal.(2019);Ji
theinitialstatesthatwereseenintraining. Inparticular,it
&Telgarsky(2019a;b);Woodworthetal.(2020);Razin&
dependsonthedegreeofexplorationinducedbythesystem
Cohen (2020); Lyu & Li (2020); Lyu et al. (2021); Azu-
whencommencingfrominitialstatesseenintraining. We
lay et al. (2021); Pesme et al. (2021); Razin et al. (2021;
provethatifthisexplorationisinsufficient, extrapolation
2022);Andriushchenkoetal.(2023);Freietal.(2023b;a);
does not take place. On the other hand, we construct a
Marcotteetal.(2023);Chouetal.(2023;2024)). Amongre-
setting that encourages exploration, and show that under
centanalysesarealsoonescenteringonout-of-distribution
itextrapolationcanbeperfect. Wethenconsideratypical
generalization,i.e.onextrapolation(seeXuetal.(2021);
setting,i.e.oneinwhichsystemsaregeneratedrandomly
Abbeetal.(2022;2023);Cohen-Karliketal.(2022;2023);
andinitialstatesseenintrainingarearbitrary. Inthissetting,
Zhouetal.(2023)). Thesearemotivatedbythefactthatin
weprovethatthedegreeofexplorationsufficesforthereto
manyreal-worldscenarios,trainingandtestdataaredrawn
benon-trivialextrapolation—inexpectation,andwithhigh
fromdifferentdistributions(Shenetal.,2021). Ourworkis
probabilityifthestatespacedimensionissufficientlylarge.
similarinthatitalsocentersonextrapolation,andisalsomo-
Twoattributesofouranalysismaybeofindependentinterest. tivatedbyreal-worldscenarios(seeSection1). Itdiffersin
First,areadvancedtoolsweemployfromtheintersection thatitstudiesthechallengingframeworkofoptimalcontrol.
ofrandommatrixtheoryandtopology. Second,isaresult
Inoptimalcontrol,theoreticalanalysesoftheimplicitbias
bywhichtheimplicitbiasofpolicygradientoveralinear
ofgradientdescentarerelativelyscarce. Forreinforcement
controllerdoesnotminimizetheEuclideannorm,instark
learning, which in a broad sense is equivalent to optimal
contrasttotheimplicitbiasofgradientdescentoverlinear
control: Huetal.(2021)characterizedatendencytowards
predictorsinsupervisedlearning(cf.Zhangetal.(2017)).
high-entropysolutionswithsoftmaxparameterizedpolicies;
Wecorroborateourtheorythroughexperiments,demonstrat- andKumaretal.(2021;2022)revealeddetrimentaleffects
ing that the interplay between a linear system and initial ofimplicitbiaswithvalue-basedmethods. Morerelevantto
statesseenintrainingcanleadalinearcontroller(learned ourworkareZhangetal.(2020;2021);Zhaoetal.(2023),
via policy gradient) to extrapolate to initial states unseen which for different LQR problems, establish that policy
in training. Moreover, we show empirically that the phe-
2ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
gradient implicitly enforces certain constraints on the pa- tosearchingforcontrolsu ,...,u thatminimizethefol-
0 H
rameters of a controller throughout optimization. These lowingquadraticcost:
analyses, however, do not treat underdetermined training
objectives(theypertaintosettingswherethereisaunique
(cid:88)H
x⊤Qx +u⊤Ru ,
h h h h
controller minimizing the training objective), thus leave h=0
openourquestionontheeffectofimplicitbiasonextrap- whereQ∈RD×D andR∈RM×M arepositivesemidefi-
olation. To the best of our knowledge, the current paper
nitematricesthatdefinethecost. Itisknownthatforthis
providesthefirstanalysisoftheimplicitbiasofpolicygra-
problem,optimalcontrolsareattainedbya(state-feedback)
dientforunderdeterminedLQRproblems. Moreover,for
linearcontroller(Anderson&Moore,2007),i.e.bysetting
optimal control problems in general, it provides the first
eachcontrol u tobe acertainlinearfunction ofthecor-
h
analysisoftheextenttowhichtheimplicitbiasofpolicy
responding state x . Accordingly, and in line with prior
h
gradient leads to extrapolation to initial states unseen in
work(e.g.,Fazeletal.(2018);Buetal.(2019);Maliketal.
training.
(2019)), we consider learning a linear controller parame-
AsidefromZhangetal.(2020;2021);Zhaoetal.(2023), terized by K ∈ RM×D, which at time h ∈ {0} ∪ [H]
existing theoretical analyses of policy gradient for LQR assigns the control u h = Kx h.1 The cost attained by K
problemslargelyfallintotwocategories. First, arethose withrespecttoafinitesetX ⊂RD ofinitialstatesis:
proving convergence rates to the minimal cost, typically
1 (cid:88) (cid:88)H
underassumptionsensuringauniquesolution(Fazeletal., J(K;X):= x⊤Qx +x⊤K⊤RKx ,
2018;Maliketal.,2019;Bhandari&Russo,2019;Moham- |X| x0∈X h=0 h h h h
(1)
madi et al., 2019; 2021; Bu et al., 2019; 2020; Jin et al.,
where,foreachx ∈X,thestatesx ,...,x satisfy:2
2020;Gravelletal.,2020;Hamblyetal.,2021;Huetal., 0 1 H
2023). Second,arethoseestablishingsub-linearregretin
x =Ax +BKx =(A+BK)hx . (2)
onlinelearning(Cohenetal.,2018;Agarwaletal.,2019a;b; h h−1 h−1 0
Cassel&Koren,2021;Hazan&Singh,2022;Chenetal.,
2023). Bothlinesofworkdonotaddressthetopicofim- Givena(finite)setS ⊂ RD ofinitialstatesseenintrain-
plicitbias,whichwefocuson. ing,thecontrollerKislearnedbyminimizingthetraining
costJ(·;S). Learningviapolicygradientamountstoitera-
Finally,anempiricalobservationrelatedtoourworkisthat
tivelyupdatingthecontrollerasfollows:
inreal-worldoptimalcontrol(andreinforcementlearning),
contemporarylearningalgorithmsoftenextrapolatepoorly K(t+1) =K(t)−η·∇J(K(t);S) , ∀t∈N, (3)
toinitialstatesunseenintraining(Rajeswaranetal.,2017;
Zhangetal.,2018;2019;Fujimotoetal.,2019;Wittyetal., whereη > 0isapredeterminedlearningrate, andweas-
2021). Thisobservationmotivatedourwork,andourresults sumethroughoutthatK(1) =0.
suggestapproachesforalleviatingthelimitationitreveals
(seeSection6).
3.2 UnderdeterminedLinearQuadraticControl
Existinganalysesofpolicygradientforlearninglinearcon-
3 Preliminaries
trollersinLQR(e.g.,Fazeletal.(2018);Maliketal.(2019);
Notation. Weuse ∥·∥todenotetheEuclideannormofa Bu et al. (2019; 2020); Mohammadi et al. (2019; 2021);
vectorormatrix,[N]todenotetheset{1,...,N},where Bhandari&Russo(2019);Hamblyetal.(2021))typically
N ∈ N, and % to denote the modulo operator. We let assumethatRispositivedefinite,meaningthatcontrolsare
e ,...,e ∈ RD be the standard basis vectors. Lastly, regularized,andthatthesetSofinitialstatesseenintraining
1 D
the subspace orthogonal to X ⊂ RD is denoted by X⊥, spansRD (orsimilarly,whentrainingoveradistributionof
i.e.X⊥ :={v∈RD :v⊥x, ∀x∈X}. initialstates,thatthesupportofthedistributionspansRD).
Undertheseassumptions,thetrainingcostJ(·;S)isnotun-
3.1 PolicyGradientinLinearQuadraticControl derdetermined—itentailsasingleglobalminimizer,which
producesoptimalcontrolsfromanyinitialstate. Thus,our
Weconsideralinearsystem,inwhichaninitialstatex ∈
0
RD evolvesaccordingto: 1SincethehorizonH isfinite,ingeneral,attainingoptimal
controlsmayrequirethelinearcontrollertobetime-varying,i.e.to
x =Ax +Bu , ∀h∈N, implement different linear mappings at different times (cf. An-
h h−1 h−1 derson & Moore (2007)). However, as detailed in Section 3.2,
ouranalysiswillconsidersettingsinwhichatime-invariantlinear
whereA∈RD×D andBD×M arematricesthatdefinethe
controllersuffices.
system,andu h−1 ∈ RM isthecontrolattimeh−1. An 2Ascustomary,weomitfromthenotationofx 1,...,x H the
LQRproblemofhorizonH ∈Noverthissystemamounts dependenceonx andK.
0
3ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
questionontheeffectofimplicitbiasonextrapolation(see the controls they assign to states in U. The performance
Section1)isnotapplicable. ofacontrollerKonstatesinU willquantifyextrapolation
ofKtoinitialstatesunseenintraining. Twomeasureswill
Tofacilitateastudyoftheforegoingquestion,wefocuson
facilitate this quantification. The first, referred to as the
underdeterminedproblems(onesinwhichthetrainingcost
optimalitymeasure,isbasedontheoptimalityconditionin
entailsmultipleglobalminimizers), obtainedthroughthe
Equation (5). Namely, it measures extrapolation by how
followingassumptions: (i)R = 0,meaningthatcontrols
close∥(A+BK)x ∥2istozeroforeveryx ∈U.
are unregularized; (ii) M = D and B ∈ RD×D has full 0 0
Definition1. Theoptimalitymeasureofextrapolationfora
rank,implyingthatthecontroller’sabilitytoaffectthestate
controllerKD×D is:
is not limited; and (iii) the set S of initial states seen in
training does not span RD (note that this implies D ≥ E (K):= 1 (cid:88) ∥(A+BK)x ∥2.
2). For conciseness, in the main text we fix Q to be an opt |U| x0∈U 0
identitymatrix,andassumethatBisanorthogonalmatrix.
Extensions of our results to more general Q and B are Thesecondmeasureofextrapolation,referredtoasthecost
discussedthroughout.
measure,isthesuboptimalityofthecostattainedbyKwith
respecttoU.
Inoursettingofinterest,thecostattainedbyacontrollerK
Definition2. Thecostmeasureofextrapolationforacon-
withrespecttoanarbitrary(finite)setX ⊂ RD ofinitial
trollerKD×D is:
states(Equation(1))simplifiesto:
E (K):=J(K;U)−J∗(U).
cost
J(K;X)= |X1 |(cid:88) x0∈X(cid:88)H h=0(cid:13) (cid:13)(A+BK)hx 0(cid:13) (cid:13)2 , (4)
Theoptimalityandcostmeasuresarecomplementary: the
former disentangles the impact of a controller on initial
withtheglobalminimumofthiscostbeing:
statesfromitsimpactonsubsequentstates,whereasthelat-
J∗(X):=min K∈RD×DJ(K;X)= |X1 |(cid:88) x0∈X∥x 0∥2. t se tar tc eo sn is nid ae tr rs at jh ee cti om ryp .a Bct oo thnb mo et ah si un ri eti sa al rs eta nte os na -nn ed gs au tb ivs ee ,q wue itn ht
lowervaluesindicatingbetterextrapolation. Theirminimal
AcontrollerK∈RD×D attainsthisglobalminimumifand valueiszero,andtheuniquememberofK attainingthis
S
onlyifKx =−B−1Ax forallx ∈X,orequivalently: valueistheperfectlyextrapolatingcontrollerK ∈ K
0 0 0 ext S
definedby:
∥(A+BK)x ∥2 =0 , ∀x ∈X , (5)
0 0 K x =−B−1Ax , ∀x ∈U. (8)
ext 0 0 0
i.e.everyx ∈X ismappedtozerobythestatedynamics
0 Moregenerally,anarbitrarycontrollerhaszerooptimality
thatKinduces.
measureifandonlyifithaszerocostmeasure.Wenotethat,
ToseethatinoursettingthetrainingcostJ(·;S)isindeed asshowninAppendixA,theoptimalityandcostmeasures
underdetermined,noticethat,sincethesetS ofinitialstates arebothinvarianttothechoiceofU,henceweomititfrom
seen in training does not span RD, there exist infinitely theirnotation.
manycontrollersKsatisfying:
Throughoutouranalysis,weshallconsiderasabaselinethe
controllerK ∈K ,definedby:
Kx =−B−1Ax , ∀x ∈S. (6) no-ext S
0 0 0
K x =0 , ∀x ∈U, (9)
no-ext 0 0
Thatis,thereareinfinitelymanycontrollersminimizingthe
whichminimizesthetrainingcostwhileassigningnullcon-
trainingcost. WedenotebyK the(infinite)setcomprising
S
trols to states in U. Aside from degenerate cases, the op-
thesecontrollers,i.e.:
timality and cost measures of K are both positive.3
no-ext
K
S
:=(cid:8) K∈RD×D :J(K;S)=J∗(S)(cid:9) . (7) WhenquantifyingextrapolationforacontrollerK pglearned
viapolicygradient(Equation(3)),wewillcompareitsop-
timality and cost measures to those of K . Namely,
3.3 QuantifyingExtrapolation no-ext
we will examine the ratios E (K )/E (K ) and
opt pg opt no-ext
LetU bean(arbitrary)orthonormalbasisforS⊥(subspace E cost(K pg)/E cost(K no-ext), where a value of one corre-
orthogonal to the initial states seen in training). A con- sponds to trivial extrapolation and a value of zero corre-
troller K is fully determined by the controls it assigns to spondstoperfectextrapolation.
states in S and U. The controllers in K (Equation (7)),
S 3The optimality measure of K is zero if and only if
i.e.thecontrollersminimizingthetrainingcost,allsatisfy no-ext
Ax = 0forallx ∈ U,i.e.ifandonlyifthezerocontroller
0 0
Equation(6), andinparticularagreeonthecontrolsthey minimizesthemeasureaswell.Anidenticalstatementholdsfor
assign to states in S. However, they differ arbitrarily in thecostmeasure.
4ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Figure1: Intuitionbehindourtheoreticalanalysis: inunderdeterminedLQRproblems(Section3.2),theextenttowhichacontroller
learnedviapolicygradientextrapolatestoinitialstatesunseenintraining,dependsonthedegreeofexplorationinducedbythesystem
whencommencingfrominitialstatesthatwereseenintraining.Illustratedarethestatedynamicsinducedbythet’thiterateofpolicy
gradientK(t) (left),bythefinalpolicygradientcontrollerK (middle),andbythenon-extrapolatingcontrollerK definedin
pg no-ext
Section3.3(right). AnarbitrarycontrollerKextrapolatestoinitialstatesunseenintrainingif∥(A+BK)x∥2issmallforx∈S⊥,
i.e.ifthedynamicsinducedbyKsendtowardszerostatesthatareorthogonaltothesetSofinitialstatesseenintraining(seeSection3.3).
Duetothestructureoftrainingcostgradients,thedynamicsinducedbythefinalpolicygradientcontrollerK sendtowardszeroevery
pg
stateencounteredintraining.Accordingly,theextenttowhichK extrapolatesdependsonthedegreeofexploration—theoverlapof
pg
statesencounteredintrainingwithdirectionsorthogonaltoS.Ontheotherhand,thecontrollerK ensuresthatstatesinSaresent
no-ext
tozero(therebyminimizingthetrainingcost),butdoesnothandlestatesinS⊥.Itthusdoesnotextrapolate.
4 AnalysisofImplicitBias words,itimpliesnothingaboutextrapolation.
Thissectiontheoreticallyanalyzestheextenttowhichpolicy Fortunately,itcanbeshownthatthestructureof∇J(·;S)
gradientleadslinearcontrollersinunderdeterminedLQR is such that at every iteration t ∈ N of policy gradient
problemstoextrapolatetoinitialstatesunseenintraining. (Equation(3)),theiteratesK(t)andK(t+1)tendtosatisfy
∥(A+BK(t+1))x(cid:13) (cid:13)2 <(cid:13) (cid:13)(A+BK(t))x∥2foreverystate
4.1 Intuition: ExtrapolationDependsonExploration x along trajectories emanating from S that were encoun-
tered in the iteration, i.e. that have been steered by K(t).
Ouranalysiswillrevealthattheextentofextrapolationto
Consequently,withK beingacontrollertrainedbypol-
pg
initial states unseen in training depends on the degree of icygradient,∥(A+BK )x∥2isrelativelysmallforevery
pg
explorationinducedbythesystemwhencommencingfrom
statexencounteredintraining(i.e.foreveryxbelonging
initialstatesthatwereseenintraining.Beforegoingintothe
toatrajectorythatwasproducedduringtraining). Theex-
formalresults,weprovideintuitionbehindthisdependence.
tent to which K extrapolates therefore depends on the
pg
Per Equations (2) and (4), the training cost attained by a degreeofexploration—theoverlapofstatesencountered
controllerKcanbewrittenasfollows: intrainingwithU,i.e.withdirectionsorthogonaltoS.
J(K;S)=c+ 1 (cid:88) (cid:88)H ∥(A+BK)x ∥2, TheaboveintuitionisillustratedinFigure1. Theremainder
|S| x0∈S h=1 h−1 ofthesectionisdevotedtoitsformalization.
wherec > 0doesnotdependonK,andforeachx ∈ S, 4.2 ExtrapolationRequiresExploration
0
thestatesx ,...,x areproducedbythesystemwhen
1 H−1
Thecurrentsubsectionprovesthatintheabsenceofsuffi-
commencingfromx andsteeredbyK.2Thus,minimizing
0
cient exploration, extrapolation to initial states unseen in
thetrainingcostamountstofindingacontrollerKsuchthat
∥(A+BK)x∥2 =0foreveryxbelongingtoatrajectory trainingdoesnottakeplace.
emanating from S. Notice that it is possible to do so by Recall from Section 3 that we consider a linear system
simplyensuringthat∥(A+BK)x∥2 =0foreveryx∈S. definedbymatricesAandB,asetS ofinitialstatesseenin
Thisisbecause,if(A+BK)x=0forsomex∈S,thenall training,andalinearcontrollerlearnedviapolicygradient,
subsequentstatesinatrajectoryemanatingfromxarezero. whoseiteratesaredenotedbyK(t) fort ∈ N. LetX be
pg
the set of states encountered in training. More precisely,
AsdiscussedinSection3.3,forKtoextrapolatetoinitial
statesunseenintraining,wewouldlike∥(A+BK)x∥2to X pg istheunionoverx 0 ∈S andt∈N,ofthestatesinthe
lengthH trajectoryemanatingfromx andsteeredbyK(t):
besmallforeveryx∈U,whereU isanorthonormalbasis 0
for S⊥ (subspace orthogonal to the set S of initial states
seenintraining). WehaveseeninSection3.3thatmerely X pg := (10)
minimizing∥(A+BK)x∥2foreveryx∈S impliesnoth- (cid:110) (cid:111)
(A+BK(t))hx :x ∈S,h∈{0}∪[H −1],t∈N .
ingaboutthemagnitudeofthistermforx ∈ U. Inother 0 0
5ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Proposition1belowestablishesthatthelearnedcontroller 4.3 ExtrapolationinExploration-InducingSetting
canonlyextrapolatetoinitialstatesspannedbyX . More
pg
Section4.2provedthat,intheabsenceofsufficientexplo-
precisely,foranyt ∈ Nandx ∈ X⊥,thecontrollerK(t)
pg ration,extrapolationtoinitialstatesunseenintrainingdoes
assigns to x a trivial control of zero. This implies that
not take place. We now show that with sufficient explo-
if X ⊆ span(S), meaning no state outside span(S) is
pg
ration,extrapolationcantakeplace. Namely,weconstructa
encounteredintraining,thentheoptimalityextrapolation
systemthatencouragesexplorationwhencommencingfrom
measureofK(t)istrivial,i.e.equaltothatofK (see
no-ext
agiveninitialstate,andshowthatwiththissystemandini-
Section 3.3). Proposition 1 further shows that such non-
tialstate,trainingviapolicygradientleadstoextrapolation,
exploratorysettingsexist—thereexistsystems(matrices
which—dependingoncharacteristicsofthecost—varies
A and B) with which, for any choice of S, it holds that
betweenpartialandperfect.
X ⊆ span(S). Intheexemplifiedsettings, similarlyto
pg
theoptimalityextrapolationmeasure,thecostextrapolation Supposethatwearegivenaninitialstateseenintraining,
measureofK(t)istrivial(andgreaterthanzero). which,withoutlossofgenerality,isthestandardbasisvec-
Proposition1. Foranyiterationt∈Nofpolicygradient, tore 1 ∈RD.4 AssumeforsimplicitythatthehorizonH is
thefollowinghold. divisiblebythestatespacedimensionD.5 Whencommenc-
ingfrome andsteeredbythefirstiterateofpolicygradient,
1
• (Exploration is necessary for extrapolation) For any i.e.byK(1) =0,thesystemproducesthelengthH trajec-
x ∈ X⊥ it holds that K(t)x = 0. Consequently, if toryT :=(e ,Ae ,...,AH−1e ). InlightofSection4.2,
pg 1 1 1
X ⊆span(S)thenE (K(t))=E (K ). forencouragingexplorationwewouldlikethestatesinT to
pg opt opt no-ext
spantheentirestatespace. Asimplechoicethatensuresthis
• (Existenceofnon-exploratorysettings)Thereexistsys- is A = A := (cid:80)D e e⊤. Under this choice,
shift d=1 d%D+1 d
temmatricesAandBsuchthat,foranysetS ofinitial T cyclically traverses through the standard basis vectors
statesseenintraining: X pg ⊆span(S);and: e 1,...,e D.
E (K(t))=E (K )=1, Proposition 2 below establishes that, in the setting under
opt opt no-ext
consideration,theimplicitbiasofpolicygradientleadsto
E cost(K(t))=E cost(K no-ext)=H, extrapolation. Specifically, the learned controller attains
optimalityandcostmeasuresofextrapolationthataresub-
whererecallH isthehorizon.
stantiallylessthanthoseofK (seeSection3.3). This
no-ext
phenomenon is more potent the longer the horizon H is,
Proofsketch(proofinAppendixD.2). Weestablishthatfor
withperfectextrapolationattainedinthelimitH →∞.
any K ∈ RD×D, the rows of ∇J(K;S) are spanned by
Proposition2. AssumethatS ={e },A=A ,andH
statesinthetrajectoriesthatemanatefromS andaresteered 1 shift
byK. SinceK(1) = 0,itfollowsthatforanyt ∈ N,the isdivisiblebyD. Then,policygradientwithlearningrate
rowsofK(t)arespannedbyX ,andsoK(t)x=0forany η
=(cid:0) H2/D+H(cid:1)−1
converges to a controller K pg that:
pg
x∈X⊥. IfX ⊆span(S)thenthisimmediatelyimplies (i) minimizes the training cost, i.e. J(K pg;S) = J∗(S);
pg pg
and(ii)satisfies:
thattheoptimalitymeasureattainedbyK(t)isequaltothat
ofK no-ext. E opt(cid:0) K pg(cid:1) 4(D−1)2
≤ ,
Asforexistenceofnon-exploratorysettings,supposethat E (K ) (H +D)2
opt no-ext
A = B = I, where I is the identity matrix. With arbi-
E (K ) 4(D−1)2
traryS,weprovethatX pg ⊆span(S)byshowingthatthe cost pg ≤ .
statedynamicsinducedbyA,B,andK(t)areinvariantto E cost(K no-ext) (H +D)2
span(S), i.e. (A+BK(t))x ∈ span(S) if x ∈ span(S).
Proofsketch(proofinAppendixD.3). Theanalysisfollows
Then,bythefirstpartoftheproposition,X ⊆ span(S)
pg
impliesthatK(t)x = 0foranyx ∈ U. Thesameistrue fromfirstprinciples,buildingonaparticularlylucidform
forK . Hence,A+BK(t) andA+BK both that∇J(K(1);S)takes. Specifically,wederiveanexplicit
no-ext no-ext
expressionforK(2),andshowthatitminimizesthetrain-
mapanyx ∈ U backtoitself. Usingthisobservation,the
ingcostviatheoptimalityconditionofEquation(5). This
optimalityandcostmeasuresofextrapolationattainedby
K(t)andK arereadilycomputed. impliesthatpolicygradientconvergestoK pg =K(2). Ex-
no-ext
Remark1. ThefirstpartofProposition1(explorationis
4Iftheinitialstateseenintrainingissomenon-zerovectorx
0
thatdiffersfrome , thenthesystemwewillconstructistobe
necessary for extrapolation) extends to the setting where 1
modified by replacing A with P−1AP, where P ∈ RD×D is
B ∈ RD×D is arbitrary and Q ∈ RD×D is any positive
someinvertiblematrixthatmapsx toe .
0 1
semidefinitematrix. TheproofinAppendixD.2accountsfor 5ExtensionoftheanalysisinthissubsectiontoarbitraryH ≥2
thismoregeneralsetting. isstraightforward,butresultsinlessconciseexpressions.
6ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
trapolationintermsoftheoptimalityandcostmeasuresthen 4.4 ExtrapolationinTypicalSetting
followsfromthederivedexpressionforK(2).
Sections4.2and4.3presentedtwoendsofaspectrum. On
Remark 2. Appendix B generalizes Proposition 2 to the oneend,Section4.2provedthat,intheabsenceofsufficient
settingwhereQisanydiagonalpositivesemidefinitema- exploration,extrapolationtoinitialstatesunseenintraining
trix. ThegeneralizedanalysisshedslightonhowQimpacts does not take place. On the other end, Section 4.3 con-
extrapolation. Inparticular, itshowsthatforcertainval- structedanexploration-inducingsetting(namely,asystem
ues of Q, extrapolation can be perfect even with a finite foragiveninitialstateseenintraining),andshowedthat
horizonH. itleadstoextrapolation,which—dependingoncharacter-
isticsofthecost—variesbetweenpartialandperfect. A
4.3.1 IMPLICITBIASINOPTIMALCONTROL̸= naturalquestioniswhatextrapolationmaybeexpectedina
EUCLIDEANNORMMINIMIZATION typicalsetting.
Awidelyknownfactisthatinsupervisedlearning,when Weaddresstheforegoingquestionbyconsideringanarbi-
labels are continuous (regression) and the training objec- trary (non-zero) initial state seen in training x ∈ RD —
0
tive is underdetermined, gradient descent over linear pre- which without loss of generality is assumed to have unit
dictorsimplicitlyminimizestheEuclideannorm. Thatis, norm6 —andarandomlygeneratedsystemmatrixA. For
amongallpredictorsminimizingthetrainingobjective,gra- therandomnessofA,wedrawentriesindependentlyfrom
dientdescentconvergestotheonewhoseEuclideannorm aGauss √iandistributionwithmeanzeroandstandarddevia-
is minimal (cf. Zhang et al. (2017)). A perhaps surpris- tion1/ D. Thischoiceofstandarddeviationiscommon
ingimplicationofProposition2,formalizedbyLemma1 intheliteratureonrandommatrixtheory(Andersonetal.,
andCorollary1below, isthatananalogousphenomenon 2010),andensuresthatwithhighprobability,thespectral
doesnottakeplaceinoptimalcontrol. Infact,the(unique) normofAisroughlyconstant,i.e.independentofthestate
minimalEuclideannormcontroller,amongthoseminimiz- spacedimensionD(cf.Theorem4.4.5inVershynin(2020)).
ingthetrainingcost,isthenon-extrapolatingK . Thus, Whencommencingfromx andsteeredbythefirstiterate
no-ext 0
the extrapolation guarantee of Proposition 2 implies that ofpolicygradient,i.e.byK(1) = 0,thesystemproduces
policygradientoveralinearcontrollerdoesnotimplicitly thelengthDtrajectory(x ,Ax ,...,AD−1x ). Sincex
0 0 0 0
minimizetheEuclideannorm. Thisfindinghighlightsthat isacyclicvectorofAalmostsurely(seeAppendixCfora
conventionalwisdomregardingimplicitbiasinsupervised proofofthisfact),thelattertrajectoryspanstheentirestate
learningcannotbeblindlyappliedtooptimalcontrol. We spacealmostsurely. Thenecessaryconditionforextrapola-
hopeitwillencouragefurtherresearchdedicatedtoimplicit tionputforthinSection4.2isthussupported,implyingthat
biasinoptimalcontrol. extrapolationcouldtakeplace.
Lemma1. Ofallcontrollersminimizingthetrainingcost, Theorem1belowestablishesthatasingleiterationofpolicy
i.e. all K ∈ K S (Equation (7)), the non-extrapolating gradientalreadyleads—inexpectation,andwithhighprob-
K no-extistheuniqueonewithminimalEuclideannorm. abilityifthestatespacedimensionislarge—tonon-trivial
extrapolation,asquantifiedbytheoptimalitymeasure. The
Proofsketch(proofinAppendixD.4). Throughthemethod theoremovercomesconsiderabletechnicalchallenges(aris-
ofLagrangemultipliers,weshowthatiftherowsofsome ingfromthecomplexityofrandomsystems)viaadvanced
K ∈ K S are in span(S), then K is the unique member tools from the intersection of random matrix theory and
of K S whose Euclidean norm is minimal. We then show topology. Thesetoolsmaybeofindependentinterest.
thattherowsofK necessarilyresideinspan(S).
no-ext Theorem1. Letx ∈RD beanarbitraryunitvector. As-
0
Corollary 1. In the setting of Proposition 2, K — the sumethatthesetS ofinitialstatesseenintrainingconsists
pg
controllertowhichpolicygradientconverges, andwhich ofx 0(i.e.S ={x 0}),thattheentriesofAaredrawninde-
minimizesthetrainingcost—satisfies: pendentlyfromaGau √ssiandistributionwithmeanzeroand
standarddeviation1/ D,andthatthehorizonHisgreater
∥K pg∥2−min K∈KS∥K∥2 =(cid:88)D
d=2(cid:18)
1− 2 H(d +−
D1)(cid:19)2
, t wh ha en reon Ne. !!T :h =en N,w (Nith −lea 2r )n (Ning −ra 4t )e ·η ··≤ 3i4 sD tH he(H d− ou11 ) b( l4 eH f− ac1 t) o!! -,
rialofanoddN ∈N,theseconditerateofpolicygradient,
wherenotethattherighthandsideisΩ(D).
i.e.K(2),satisfies:
Proofsketch(proofinAppendixD.5). Wederiveanexpres- E (cid:2) E (cid:0) K(2)(cid:1)(cid:3) H(H −1)
A opt
sion for K to compute its squared Euclidean norm, ≤1−η· ,
no-ext E [E (K )] D
whichbyLemma1isequaltomin ∥K∥2. Then,an A opt no-ext
K∈KS
expressionforK pg,establishedasalemmaintheproofof 6Ifx 0doesnothaveunitnormthentheresultswewillestablish
Proposition2,yieldsthedesiredresult. aretobemodifiedbyintroducingamultiplicativefactorof∥x ∥2.
0
7ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
whereK isthenon-extrapolatingcontrollerdefined and (non-linear) neural network controllers. For concise-
no-ext
in Section 3.3. Moreover, for any δ ∈ (0,1), if D ≥ ness, wedefersomeexperimentsandimplementationde-
|S| + 6|S|H(H−1)(4H−1)!! and η ≤ 1 , tailstoAppendixE.Codeforreproducingourexperiments
δ 8D2H(H−1)(4H−1)!!
thenwithprobabilityatleast1−δoverthechoiceofA: is available at https://github.com/noamrazin/
imp_bias_control.
E
(cid:0) K(2)(cid:1)
H(H −1)
opt
≤1−η· .
E (K ) 4D 5.1 LinearQuadraticControl
opt no-ext
OurtheoreticalanalysisconsideredunderdeterminedLQR
Lastly, the above results hold even if we replace S by an
problemsinthreesettings,respectivelycomprising: (i)sys-
arbitrarysetoforthonormalvectors.
temsthatdonotinduceexplorationfromanyinitialstates
(Section 4.2); (ii) systems with a “shift” transition ma-
Proofsketch(proofinAppendixD.8). Theintuitionbehind trixA,whichinduceexplorationfromcertaininitialstates
theproof(validforH ≥D)isasfollows. Asstatedinthe (Section4.3);and(iii)systemswitharandomlygenerated
discussionregardingexplorationattheopeningofthissub- A, which induce exploration from any initial states (Sec-
section,almostsurely,thelengthDtrajectorysteeredbythe tion4.4). Accordingtoouranalysis,withsystemsthatdo
firstiterateofpolicygradient,i.e.byK(1),spanstheentire
notinduceexplorationfrominitialstatesseenintraining,
statespace. Therefore,almostsurely,statesencounteredin controllers trained via policy gradient do not extrapolate.
trainingoverlapwithU,i.e.withdirectionsorthogonaltoS Ontheotherhand,non-trivialextrapolationoccursunder
(see Section 3.3). The intuitive arguments in Section 4.1 “shift”andrandomsystems. Figure2demonstratesthese
thussuggestthatextrapolationwilltakeplace. findingsempirically,showcasingtherelationbetweenthe
system and extrapolation to initial states unseen in train-
Convertingtheaboveintuitionintoaformalproofentails
ing. Figures 4 and 5 in Appendix E.1 provide additional
considerabletechnicalchallenges. Weaddressthesechal-
experimentsinsettingswith,respectively: (i)alongertime
lengesbyemployingadvancedtoolsfromtheintersection
horizon;and(ii)randomBandQmatrices.
of random matrix theory and topology. Specifically, we
employamethodfromRedelmeier(2014)forcomputing
expectationsoftracesofrandommatrixproducts,through 5.2 Non-LinearSystemsandNeuralNetwork
the topological concept of genus expansion. For the con- Controllers
venience of the reader, a detailed outline of the proof is
The LQR problem is of central theoretical and practical
providedinAppendixD.8.
importanceinoptimalcontrol(Anderson&Moore,2007).
Forexample,itsupportscontrollingnon-linearsystemsvia
Limitations. Despite overcoming considerable technical iterativelinearizations(Li&Todorov,2004). Analterna-
challenges,Theorem1remainslimitedinthreeways:(i)the tiveapproachtocontrollingnon-linearsystemsistotrain
requirementfromthelearningrateη,andtherequirement (non-linear)neuralnetworkcontrollersviapolicygradient.
fromthestatespacedimensionDinthesecond(highprob- Thisapproachislargelymotivatedbythesuccessofneural
ability)result, dependon(4H −1)!!, whichgrowssuper networksinsupervisedlearning,andhasgainedsignificant
exponentiallywiththehorizonH;(ii)extrapolationguar- interestinrecentyears(see,e.g.,Huetal.(2019);Qiaoetal.
antees are provided only for the first iteration of policy (2020);Claveraetal.(2020);Moraetal.(2021);Gillen&
gradient;and(iii)incontrasttotheanalysesofSections4.2 Byl(2022);Howelletal.(2022);Xuetal.(2022);Wiede-
and4.3,extrapolationresultsapplyonlytotheoptimality mannetal.(2023)).
measure,nottothecostmeasure. Experimentsreportedin
OuranalysisofunderdeterminedLQRproblems(Section4)
Section5.1suggestthatallthreelimitationsabovemaybe
impliesthat,whenalinearsysteminducesexplorationfrom
alleviated. Doingsoisregardedasavaluabledirectionfor
initialstatesseenintraining,alinearcontrollertrainedvia
futurework.
policygradienttypicallyextrapolatestoinitialstatesunseen
intraining.Thecurrentsubsectionempiricallydemonstrates
5 Experiments that this phenomenon extends to non-linear systems and
neuralnetworkcontrollers. Experimentsincludetwonon-
Inthissection, wecorroborateourtheory(Section4)via
linearcontrolproblems,inwhichthegoalistosteereither
experiments, demonstrating how the interplay between a
apendulumorquadcoptertowardsatargetstate.
systemandinitialstatesseenintrainingaffectstheextent
to which a controller learned via policy gradient extrapo- The pendulum control problem. A classic non-linear
latestoinitialstatesunseenintraining. Section5.1presents controlproblemisthatofstabilizinga(simulated)pendulum
experimentswiththeanalyzedunderdeterminedLQRprob- atanuprightposition(cf.Hazan&Singh(2022)). Attime
lems,afterwhichSection5.2considersnon-linearsystems steph,thetwo-dimensionalstateofthesystemisdescribed
8ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
LQR Problem
Figure2:InunderdeterminedLQRproblems(Section3.2),theextenttowhichlinearcontrollerslearnedviapolicygradientextrapolateto
initialstatesunseenintraining,dependsonthedegreeofexplorationthatthesysteminducesfrominitialstatesthatwereseenintraining.
WeevaluatedLQRproblemswithstatespacedimensionD =5,horizonH =5,andthreedifferentlinearsystems: (i)an“identity”
systemwithA=I∈RD×D(analyzedinSection4.2);(ii)a“shift”systemwithA=(cid:80)D d=1e d%D+1e⊤
d
(analyzedinSection4.3);√and
(iii)arandomsystem,wheretheentriesofAaresampledindependentlyfromazero-meanGaussianwithstandarddeviation1/ D
(analyzedinSection4.4).Reportedaretheoptimality(Definition1)andcost(Definition2)measuresofextrapolation,normalizedby
therespectivequantitiesattainedbythenon-extrapolatingcontrollerK (seeSection3.3). Avalueofonecorrespondstotrivial
no-ext
(no)extrapolationandavalueofzerocorrespondstoperfectextrapolation.Barheightsstandformedianvaluesover20runsdifferingin
randomseed,anderrorbarsspantheinterquartilerange(25’thto75’thpercentiles).Results:Inagreementwithourtheory(Sections4.2
to4.4),noextrapolationtakesplaceunderthe“identity”system,sinceitdoesnotinduceexplorationfrominitialstatesseenintraining.On
theotherhand,substantial,yetfarfromperfectextrapolationisachievedunderthe“shift”andrandomsystems,whichinduceexploration.
NotethatthisaccordswiththeresultsofSection4.3,astheyindicatethatextrapolationisusuallynotperfect.
bytheverticalangleofthependulumθ ∈Randitsangular when training with initial states of some height, the con-
h
velocityθ˙ ∈ R. Thecontrollerappliesatorqueu ∈ R, trollercanextrapolatenear-perfectlytounseeninitialstates
h h
with the goal of making the pendulum reach and stay at oflowerheight. Theextrapolationisobservedqualitatively,
thetargetstate (π,0). Accordingly, thecost ateachtime in terms of the final states in a trajectory being near the
stepisthesquaredEuclideandistancebetweenthecurrent targetstate,andquantitatively,asevaluatedbythecostmea-
andtargetstates. SeeAppendixE.3.2forexplicitequations surecomparedtoanon-extrapolatingcontroller.7 Moreover,
definingthestatedynamicsandcost. forthequadcoptercontrolproblem,Figures8and10inAp-
pendixE.2demonstratethat,respectively: (i)theextentof
Thequadcoptercontrolproblem. Anothercommonnon-
extrapolationvariesdependingonthedistancefrominitial
linearcontrolproblemisthatofcontrollinga(simulated)
statesseenintraining;and(ii)extrapolationalsoappliesto
quadcopter(cf.Paneratietal.(2021)). Attimesteph,the
unseeninitialstateswithhorizontaldistancefromtheinitial
stateofthesystemx ∈ R12 comprisesthequadcopter’s
h statesseenintraining.
position (x ,y ,z ) ∈ R3, tilt angles (ϕ ,θ ,ψ ) ∈ R3
h h h h h h
(i.e.roll,pitch,andyaw),andtheirrespectivevelocities.The
controllerdeterminestherevolutionsperminute(RPM)for 6 Conclusion
eachofthefourmotorsbychoosingu ∈[0,MAX RPM]4,
h The implicit bias of gradient descent is a cornerstone of
whereMAX RPMstandsforthemaximalsupportedRPM.
modernmachinelearning. Whileextensivelystudiedinsu-
Weconsiderthegoalofmakingthequadcopterreachand
pervisedlearning,itisfarlessunderstoodinoptimalcontrol
stayatthetargetstatex∗ = (0,0,1,0,...,0). Thisisex-
(reinforcement learning). There, learning a controller ap-
pressedbytakingthecostateachtimesteptobeaweighted
pliedtoasystemviagradientdescentisknownaspolicy
squaredEuclideandistancebetweenthecurrentandtarget
gradient,andaquestionofprimeimportance(particularly
states. SeeAppendixE.3.3forexplicitequationsdefining
forsafety-criticalapplications,e.g.robotics,industrialman-
thestatedynamicsandcost.
ufacturing,orautonomousdriving)istheextenttowhich
Results. For both the pendulum and quadcopter control alearnedcontrollerextrapolatestoinitialstatesunseenin
problems, we train via policy gradient a (state-feedback) training. Inthispaperwetheoreticallystudiedtheimplicit
controller, parameterized as a fully-connected neural net- biasofpolicygradientintermsofextrapolationtoinitial
work with ReLU activation. The controls produced by a statesunseenintraining. FocusingonthefundamentalLQR
randomlyinitializedneuralnetworkareusuallynearzero. problem,weestablishedthattheextentofextrapolationde-
Hence, during the first iterations of policy gradient, both
7Thecostextrapolationmeasure(Definition2)isadaptedto
the pendulum and quadcopter fall downwards from their
non-linearcontrolproblemsbytakingU tobeapredeterminedset
respectiveinitialstates,qualitativelyleadingtoexploration.
ofinitialstatesunseenintraining(seeAppendixE.3forfurther
Figure3showsthat,inaccordancewithourtheoryforLQR details).Incontrast,theoptimalitymeasure(Definition1)isnot
problems,inboththependulumandquadcopterproblems, directlyapplicabletonon-linearcontrolproblems.
9ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Figure3:Inthependulumandquadcoptercontrolproblems(seeSection5.2),a(non-linear)neuralnetworkcontrollertrainedviapolicy
gradientoftenextrapolatestoinitialstatesunseenintraining,despitetheexistenceofnon-extrapolatingcontrollers.Left:Initialstates
seenintraining(blue)andinitialstatesunseenintrainingthatareusedforevaluatingextrapolation(red).Middle:Statesatthefinaltime
stepwhencontrollingthesystemusinga(state-feedback)controllerlearnedviapolicygradient. Thecontrollerisparameterizedasa
fully-connectedneuralnetworkwithReLUactivation.Wecarriedoutfivetrainingrunswithdifferentrandomseeds,andchosetodisplay
thecontrollerthatattainedthemediancostmeasureofextrapolation.Right:Statesatthefinaltimestepwhencontrollingthesystem
usinganon-extrapolatingcontroller,i.e.acontrollerthatminimizesthecostforinitialstatesseenintrainingwhileperformingpoorlyon
initialstatesunseenintraining.Weobtainedsuchacontrollerbyemployingamodifiedtrainingobjectivethatencouragessteeringunseen
initialstatestoastatedifferentthanthetargetstate. Results:Sinceanuncontrolledpendulumorquadcopterfallsdownwardsfroma
giveninitialstate,thesystemsqualitativelyinduceexplorationofstateswithlowerheight.ComplyingwithourtheoryforLQRproblems
(Section4),policygradientyieldsnear-perfectextrapolationtounseeninitialstateslowerthatthoseusedfortraining.Inparticular,the
costmeasureofextrapolation,normalizedbythatattainedbythenon-extrapolatingcontroller,isneartheminimalvalueofzero(avalue
ofonestandsfornoextrapolation).FurtherdetailsinAppendixE:Tables1and2fullyspecifytheinitialandfinalstatesdepictedabove,
andFigures6and7presenttheevolutionofstatesthroughtimeunderthepolicygradientandnon-extrapolatingcontrollers.
pendsonthedegreeofexplorationinducedbythesystem Anadditionaldirectionforfutureresearch,whichwehope
whencommencingfrominitialstatesincludedintraining. ourworkwillinspire,isthedevelopmentofpracticalmeth-
Experimentscorroboratedourtheory,anddemonstratedits odsforselectinginitialstateswhoseinclusionintraining
conclusionsonproblemsbeyondLQR,wheresystemsare enhancesextrapolationtoinitialstatesunseenintraining.
non-linearandcontrollersareneuralnetworks. Inreal-worldoptimalcontrol(andreinforcementlearning),
withcontemporarylearningalgorithms,extrapolationtoini-
Futureworkincludesextendingourtheoryinthreeways.
tialstatesunseenintrainingisoftenpoor(Rajeswaranetal.,
First, is to alleviate the technical limitations specified in
2017;Zhangetal.,2018;2019;Fujimotoetal.,2019;Witty
Section4.4. Second,istoaccountfornon-linearsystems
etal.,2021).Webelievemethodsasdescribedbearpotential
andneuralnetworkcontrollerssuchasthoseevaluatedinour
togreatlyimproveit.
experiments. Third,istoaddresssettingswheresystemsare
unknownornon-differentiable,andgradientswithrespect
tocontrollerparametersareestimatedviasampling.
10ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Acknowledgements Caron,R.andTraynor,T. Thezerosetofapolynomial. WSMR
Report,pp.05–02,2005.
WethankYonathanEfroni,EmilyRedelmeier,YuvalPeled,
andAlexanderHockforilluminatingdiscussions,andEs- Cassel, A. B. and Koren, T. Online policy gradie √nt for model
hbalHezroniforaidinpreparingillustrativefigures. This freelearningoflinearquadraticregulatorswith T regret. In
InternationalConferenceonMachineLearning.PMLR,2021.
workwassupportedbyaGoogleResearchScholarAward,
aGoogleResearchGift,theYandexInitiativeinMachine
Chen,X.,Minasyan,E.,Lee,J.D.,andHazan,E. Regretguar-
Learning, the Israel Science Foundation (grant 1780/21), anteesforonlinedeepcontrol. InLearningforDynamicsand
theEuropeanResearchCouncil(ERC)undertheEuropean ControlConference.PMLR,2023.
UnionsHorizon2020researchandinnovationprogramme
Chou, H.-H., Maly, J., andRauhut, H. Moreisless: inducing
(grantERCHOLI819080),theTelAvivUniversityCenter
sparsityviaoverparameterization. InformationandInference:
forAIandDataScience,theAdelisResearchFundforArti-
AJournaloftheIMA,12(3),2023.
ficialIntelligence,LenBlavatnikandtheBlavatnikFamily
Foundation,andAmnonandAnatShashua.NRissupported Chou, H.-H., Gieshoff, C., Maly, J., and Rauhut, H. Gradient
bytheAppleScholarsinAI/MLPhDfellowship. descentfordeepmatrixfactorization: Dynamicsandimplicit
biastowardslowrank. AppliedandComputationalHarmonic
Analysis,68:101595,2024.
References
Clavera,I.,Fu,V.,andAbbeel,P. Model-augmentedactor-critic:
Abbe, E., Bengio, S., Cornacchia, E., Kleinberg, J., Lotfi, A., Backpropagatingthroughpaths. InternationalConferenceon
Raghu, M., and Zhang, C. Learning to reason with neural LearningRepresentations,2020.
networks:Generalization,unseendataandbooleanmeasures.
AdvancesinNeuralInformationProcessingSystems,35,2022. Cohen,A.,Hasidim,A.,Koren,T.,Lazic,N.,Mansour,Y.,and
Talwar,K. Onlinelinearquadraticcontrol. InInternational
Abbe,E.,Bengio,S.,Lotfi,A.,andRizk,K. Generalizationonthe ConferenceonMachineLearning.PMLR,2018.
unseen,logicreasoninganddegreecurriculum.InInternational
conferenceonmachinelearning.PMLR,2023. Cohen-Karlik, E., David, A. B., Cohen, N., and Globerson, A.
Ontheimplicitbiasofgradientdescentfortemporalextrapola-
Agarwal, N., Bullins, B., Hazan, E., Kakade, S., andSingh, K.
tion. InInternationalConferenceonArtificialIntelligenceand
Onlinecontrolwithadversarialdisturbances. InInternational
Statistics.PMLR,2022.
ConferenceonMachineLearning.PMLR,2019a.
Agarwal, N., Hazan, E., and Singh, K. Logarithmic regret for Cohen-Karlik, E., Menuhin-Gruman, I., Cohen, N., Giryes, R.,
online control. Advances in Neural Information Processing andGloberson,A. Learninglowdimensionalstatespaceswith
Systems,32,2019b. overparameterizedrecurrentneuralnetwork. InInternational
ConferenceonLearningRepresentations,2023.
Anderson, B. D. and Moore, J. B. Optimal control: linear
quadraticmethods. CourierCorporation,2007. Dulac-Arnold,G.,Levine,N.,Mankowitz,D.J.,Li,J.,Paduraru,
C.,Gowal,S.,andHester,T.Challengesofreal-worldreinforce-
Anderson,G.W.,Guionnet,A.,andZeitouni,O. Anintroduction
mentlearning:definitions,benchmarksandanalysis. Machine
torandommatrices. Number118.Cambridgeuniversitypress,
Learning,110(9):2419–2468,2021.
2010.
Fazel, M., Ge, R., Kakade, S., and Mesbahi, M. Global con-
Andriushchenko,M.,Varre,A.V.,Pillaud-Vivien,L.,andFlam-
vergenceofpolicygradientmethodsforthelinearquadratic
marion,N. Sgdwithlargestepsizeslearnssparsefeatures. In
regulator. In International conference on machine learning.
InternationalConferenceonMachineLearning.PMLR,2023.
PMLR,2018.
Arora,S.,Cohen,N.,Hu,W.,andLuo,Y. Implicitregularization
indeepmatrixfactorization.InAdvancesinNeuralInformation Frei,S.,Vardi,G.,Bartlett,P.,andSrebro,N.Benignoverfittingin
ProcessingSystems,2019. linearclassifiersandleakyrelunetworksfromkktconditionsfor
marginmaximization. InTheThirtySixthAnnualConference
Azulay, S., Moroshko, E., Nacson, M. S., Woodworth, B. E., onLearningTheory.PMLR,2023a.
Srebro,N.,Globerson,A.,andSoudry,D. Ontheimplicitbias
ofinitializationshape:Beyondinfinitesimalmirrordescent. In Frei,S.,Vardi,G.,Bartlett,P.L.,andSrebro,N.Thedouble-edged
InternationalConferenceonMachineLearning,2021. swordofimplicitbias: Generalizationvs.robustnessinrelu
networks. AdvancesinNeuralInformationProcessingSystems,
Bhandari,J.andRusso,D.Globaloptimalityguaranteesforpolicy
36,2023b.
gradientmethods. arXivpreprintarXiv:1906.01786,2019.
Bu,J.,Mesbahi,A.,Fazel,M.,andMesbahi,M. Lqrthroughthe Fujimoto,S.,Meger,D.,andPrecup,D. Off-policydeepreinforce-
lensoffirstordermethods:Discrete-timecase. arXivpreprint mentlearningwithoutexploration. InInternationalconference
arXiv:1907.08921,2019. onmachinelearning.PMLR,2019.
Bu, J., Mesbahi, A., and Mesbahi, M. Policy gradient-based Gillen, S. and Byl, K. Leveraging reward gradients for rein-
algorithmsforcontinuous-timelinearquadraticcontrol. arXiv forcementlearningindifferentiablephysicssimulations. arXiv
preprintarXiv:2006.09178,2020. preprintarXiv:2203.02857,2022.
11ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Gravell,B.,Esfahani,P.M.,andSummers,T. Learningoptimal Lyu, K. and Li, J. Gradient descent maximizes the margin of
controllersforlinearsystemswithmultiplicativenoiseviapol- homogeneousneuralnetworks. InternationalConferenceon
icygradient. IEEETransactionsonAutomaticControl,66(11), LearningRepresentations,2020.
2020.
Lyu,K.,Li,Z.,Wang,R.,andArora,S. Gradientdescentontwo-
Gunasekar,S.,Woodworth,B.E.,Bhojanapalli,S.,Neyshabur,B., layernets:Marginmaximizationandsimplicitybias. Advances
andSrebro,N. Implicitregularizationinmatrixfactorization. inNeuralInformationProcessingSystems,34,2021.
InAdvancesinNeuralInformationProcessingSystems,2017.
Malik,D.,Pananjady,A.,Bhatia,K.,Khamaru,K.,Bartlett,P.,
Hambly,B.,Xu,R.,andYang,H. Policygradientmethodsfor andWainwright,M. Derivative-freemethodsforpolicyopti-
thenoisylinearquadraticregulatoroverafinitehorizon. SIAM mization:Guaranteesforlinearquadraticsystems. InThe22nd
JournalonControlandOptimization,59(5),2021. internationalconferenceonartificialintelligenceandstatistics.
PMLR,2019.
Hazan, E. and Singh, K. Introduction to online nonstochastic
control. arXivpreprintarXiv:2211.09619,2022. Marcotte,S.,Gribonval,R.,andPeyre´,G. Abidebythelawand
followtheflow:Conservationlawsforgradientflows.Advances
Howell,T.A.,Cleac’h,S.L.,Bru¨digam,J.,Kolter,J.Z.,Schwager, inneuralinformationprocessingsystems,2023.
M.,andManchester,Z. Dojo:Adifferentiablephysicsengine
Metz,L.,Freeman,C.D.,Schoenholz,S.S.,andKachman,T.Gra-
forrobotics. arXivpreprintarXiv:2203.00806,2022.
dientsarenotallyouneed. arXivpreprintarXiv:2111.05803,
Hu, B., Zhang, K., Li, N., Mesbahi, M., Fazel, M., and Bas¸ar, 2021.
T. Towardatheoreticalfoundationofpolicyoptimizationfor
Miller,J.P.,Taori,R.,Raghunathan,A.,Sagawa,S.,Koh,P.W.,
learningcontrolpolicies. AnnualReviewofControl,Robotics,
Shankar,V.,Liang,P.,Carmon,Y.,andSchmidt,L. Accuracy
andAutonomousSystems,6,2023.
ontheline:onthestrongcorrelationbetweenout-of-distribution
Hu,Y.,Liu,J.,Spielberg,A.,Tenenbaum,J.B.,Freeman,W.T., andin-distributiongeneralization. InInternationalConference
Wu, J., Rus, D., and Matusik, W. Chainqueen: A real-time onMachineLearning.PMLR,2021.
differentiable physical simulator for soft robotics. In 2019
Mohammadi, H., Zare, A., Soltanolkotabi, M., and Jovanovic´,
Internationalconferenceonroboticsandautomation(ICRA),
M.R. Global exponentialconvergenceofgradientmethods
pp.6265–6271.IEEE,2019.
overthenonconvexlandscapeofthelinearquadraticregulator.
In2019IEEE58thConferenceonDecisionandControl(CDC).
Hu, Y., Ji, Z., and Telgarsky, M. Actor-critic is implicitly bi-
IEEE,2019.
ased towards high entropy optimal policies. arXiv preprint
arXiv:2110.11280,2021.
Mohammadi, H., Zare, A., Soltanolkotabi, M., and Jovanovic´,
M.R.Convergenceandsamplecomplexityofgradientmethods
Ji, Z. and Telgarsky, M. Gradient descent aligns the layers of
forthemodel-freelinear–quadraticregulatorproblem. IEEE
deeplinearnetworks. InternationalConferenceonLearning
TransactionsonAutomaticControl,67(5),2021.
Representations,2019a.
Mora, M. A. Z., Peychev, M., Ha, S., Vechev, M., and Coros,
Ji,Z.andTelgarsky,M. Theimplicitbiasofgradientdescenton
S. Pods:Policyoptimizationviadifferentiablesimulation. In
nonseparabledata. InConferenceonLearningTheory,2019b.
InternationalConferenceonMachineLearning.PMLR,2021.
Jin, Z., Schmitt, J.M., andWen, Z. Ontheanalysisofmodel-
Munkres,J.R. Elementsofalgebraictopology. CRCpress,2018.
freemethodsforthelinearquadraticregulator. arXivpreprint
arXiv:2007.03861,2020. Neyshabur, B. Implicit regularization in deep learning. arXiv
preprintarXiv:1709.01953,2017.
Kemp,T. Math247a:Introductiontorandommatrixtheory. Lec-
turenotes,2013. Neyshabur,B.,Tomioka,R.,andSrebro,N. Insearchofthereal
inductivebias: Ontheroleofimplicitregularizationindeep
Kingma,D.P.andBa,J.Adam:Amethodforstochasticoptimiza- learning. arXivpreprintarXiv:1412.6614,2014.
tion. InInternationalConferenceonLearningRepresentations,
2015. Panerati,J.,Zheng,H.,Zhou,S.,Xu,J.,Prorok,A.,andSchoellig,
A.P.Learningtofly—agymenvironmentwithpybulletphysics
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Implicit forreinforcementlearningofmulti-agentquadcoptercontrol.In
under-parameterizationinhibitsdata-efficientdeepreinforce- 2021IEEE/RSJInternationalConferenceonIntelligentRobots
mentlearning. InInternationalConferenceonLearningRepre- andSystems(IROS).IEEE,2021.
sentations,2021.
Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,
Kumar,A.,Agarwal,R.,Ma,T.,Courville,A.,Tucker,G.,and G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,L.,etal. Py-
Levine,S. Dr3: Value-baseddeepreinforcementlearningre- torch: An imperative style, high-performance deep learning
quiresexplicitregularization. InInternationalConferenceon library. Advancesinneuralinformationprocessingsystems,32,
LearningRepresentations,2022. 2019.
Li,W.andTodorov,E. Iterativelinearquadraticregulatordesign Pesme,S.,Pillaud-Vivien,L.,andFlammarion,N. Implicitbiasof
fornonlinearbiologicalmovementsystems. InFirstInterna- sgdfordiagonallinearnetworks:aprovablebenefitofstochas-
tionalConferenceonInformaticsinControl,Automationand ticity. AdvancesinNeuralInformationProcessingSystems,34,
Robotics,volume2.SciTePress,2004. 2021.
12ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Qiao,Y.-L.,Liang,J.,Koltun,V.,andLin,M.C. Scalabledif- Xu,K.,Zhang,M.,Li,J.,Du,S.S.,Kawarabayashi,K.-i.,and
ferentiablephysicsforlearningandcontrol. InInternational Jegelka,S. Howneuralnetworksextrapolate: Fromfeedfor-
ConferenceonMachineLearning.PMLR,2020. wardtographneuralnetworks. InInternationalConferenceon
LearningRepresentations,2021.
Rajeswaran,A.,Lowrey,K.,Todorov,E.V.,andKakade,S.M.
Towardsgeneralizationandsimplicityincontinuouscontrol. Zhang,A.,Ballas,N.,andPineau,J. Adissectionofoverfitting
AdvancesinNeuralInformationProcessingSystems,30,2017. andgeneralizationincontinuousreinforcementlearning. In
Internationalconferenceonmachinelearning,2019.
Razin, N.andCohen, N. Implicitregularizationindeeplearn-
ingmaynotbeexplainablebynorms. InAdvancesinNeural Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
InformationProcessingSystems,2020. Understandingdeeplearningrequiresrethinkinggeneralization.
InInternationalConferenceonLearningRepresentations,2017.
Razin, N., Maman, A., and Cohen, N. Implicit regularization
intensorfactorization. InternationalConferenceonMachine Zhang, C., Vinyals, O., Munos, R., and Bengio, S. A study
Learning,2021. onoverfittingindeepreinforcementlearning. arXivpreprint
arXiv:1804.06893,2018.
Razin,N.,Maman,A.,andCohen,N. Implicitregularizationin
hierarchicaltensorfactorizationanddeepconvolutionalneu- Zhang,K.,Hu,B.,andBasar,T. PolicyoptimizationforH 2linear
ralnetworks. InternationalConferenceonMachineLearning, controlwithH ∞robustnessguarantee:Implicitregularization
2022. andglobalconvergence.InLearningforDynamicsandControl.
PMLR,2020.
Redelmeier,C.E.I.Realsecond-orderfreenessandtheasymptotic
Zhang,K.,Zhang,X.,Hu,B.,andBasar,T. Derivative-freepolicy
realsecond-orderfreenessofseveralrealmatrixmodels. Inter-
optimizationforlinearrisk-sensitiveandrobustcontroldesign:
nationalMathematicsResearchNotices,2014(12):3353–3395,
Implicitregularizationandsamplecomplexity. Advancesin
2014.
NeuralInformationProcessingSystems,34,2021.
Shen,Z.,Liu,J.,He,Y.,Zhang,X.,Xu,R.,Yu,H.,andCui,P.
Zhao, F., Do¨rfler, F., and You, K. Data-enabled policy opti-
Towardsout-of-distributiongeneralization: Asurvey. arXiv
mization for the linear quadratic regulator. arXiv preprint
preprintarXiv:2108.13624,2021.
arXiv:2303.17958,2023.
Sontag,E.D. Mathematicalcontroltheory: deterministicfinite
Zhou,H.,Bradley,A.,Littwin,E.,Razin,N.,Saremi,O.,Susskind,
dimensionalsystems,volume6. SpringerScience&Business
J., Bengio, S., and Nakkiran, P. What algorithms can trans-
Media,2013.
formerslearn?astudyinlengthgeneralization. arXivpreprint
arXiv:2310.16028,2023.
Soudry,D.,Hoffer,E.,Nacson,M.S.,Gunasekar,S.,andSrebro,
N. Theimplicitbiasofgradientdescentonseparabledata. The
Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh,
JournalofMachineLearningResearch,19(1),2018.
A.,Kumar,V.,andLevine,S. Theingredientsofreal-world
roboticreinforcementlearning. InInternationalConferenceon
Vardi,G. Ontheimplicitbiasindeep-learningalgorithms. Com-
LearningRepresentations,2020.
municationsoftheACM,66(6),2023.
Vershynin,R. High-dimensionalprobability. UniversityofCali-
fornia,Irvine,2020.
Wiedemann,N.,Wu¨est,V.,Loquercio,A.,Mu¨ller,M.,Floreano,
D.,andScaramuzza,D. Trainingefficientcontrollersviaana-
lyticpolicygradient. In2023IEEEInternationalConference
onRoboticsandAutomation(ICRA).IEEE,2023.
Williams,R.J. Simplestatisticalgradient-followingalgorithms
forconnectionistreinforcementlearning. Machinelearning,8:
229–256,1992.
Witty,S.,Lee,J.K.,Tosch,E.,Atrey,A.,Clary,K.,Littman,M.L.,
andJensen,D. Measuringandcharacterizinggeneralization
indeepreinforcementlearning. AppliedAILetters,2(4):e45,
2021.
Woodworth,B.,Gunasekar,S.,Lee,J.D.,Moroshko,E.,Savarese,
P.,Golan,I.,Soudry,D.,andSrebro,N.Kernelandrichregimes
inoverparametrizedmodels.InConferenceonLearningTheory,
2020.
Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W.,
Garg,A.,andMacklin,M. Acceleratedpolicylearningwith
paralleldifferentiablesimulation. InternationalConferenceon
LearningRepresentations,2022.
13ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
A ExtrapolationMeasuresAreInvarianttotheChoiceofOrthonormalBasis
This appendix establishes that the optimality and cost measures of extrapolation (Definitions 1 and 2 in Section 3.3,
respectively)areinvarianttothechoiceoforthonormalbasisU forS⊥,whereS⊥isthesubspaceorthogonaltothesetS of
initialstatesseenintraining. Thatis,foranytwosuchorthonormalbasesU andU′,therespectivevaluesoftheoptimality
andcostmeasuresarethesame.
Lemma2. ForanycontrollerK∈RD×D,theoptimalityandcostmeasuresofextrapolationareinvarianttothechoiceof
orthonormalbasisU forS⊥.
Proof. LetU beanorthonormalbasisofS⊥,anddenotebyV∈RD×|U|thematrixwhosecolumnsaretheinitialstatesin
U. Furthermore,letZ∈RD×dim(span(S))beamatrixwhosecolumnsformanorthonormalbasisforspan(S). Noticethat
theconcatenatedmatrix[V,Z]∈RD×D isanorthogonalmatrix.
Now,forK∈RD×D,theoptimalitymeasureofextrapolationcanbewritteninamatricizedformasfollows:
1 1 (cid:16) (cid:17)
E (K)= ∥(A+BK)V∥2 = ∥(A+BK)[V,Z]∥2−∥(A+BK)Z∥2 .
opt |U| |U|
SincetheEuclideannormisorthogonallyinvariant,wegetthat:
1 (cid:16) (cid:17)
E (K)= ∥A+BK∥2−∥(A+BK)Z∥2 .
opt |U|
Ascanbeseenintheexpressionabove,theoptimalitymeasureofextrapolationdoesnotdependonthechoiceofU.
Similarly,forK∈RD×D,thecostmeasureofextrapolationcanbewritteninamatricizedformasfollows:
E (K)=J(K;U)−J∗(U)
cost
=
1 (cid:88)H (cid:13) (cid:13)(A+BK)hV(cid:13) (cid:13)2
−1
|U| h=0
=
1 (cid:88)H (cid:13) (cid:13)(A+BK)hV(cid:13) (cid:13)2
|U| h=1
=
1 (cid:88)H (cid:16)(cid:13) (cid:13)(A+BK)h[V,Z](cid:13) (cid:13)2 −(cid:13) (cid:13)(A+BK)hZ(cid:13) (cid:13)2(cid:17)
,
|U| h=1
whereweusedthefactthatJ∗(X)=1foranyfinitesetofunitnorminitialstatesX ⊂RD. Again,sincetheEuclidean
normisorthogonallyinvariant,wegetanexpressionforE (K)thatdoesnotdependonthechoiceofU:
cost
E cost(K)=
|U1 |(cid:88)H h=1(cid:16)(cid:13) (cid:13)(A+BK)h(cid:13) (cid:13)2 −(cid:13) (cid:13)(A+BK)hZ(cid:13) (cid:13)2(cid:17)
.
B ExtensionofAnalysisforExploration-InducingSettingtoDiagonalQ
Inthisappendix,wegeneralizetheanalysisoftheexploration-inducingsettingfromSection4.3tothecasewhereQisa
generaldiagonalpositivesemidefinitematrix(notnecessarilytheidentitymatrixI). Thegeneralizedanalysisshedslighton
howQimpactsextrapolation. Inparticular,itshowsthatforcertainvaluesofQextrapolationcanbeperfectevenfora
finitehorizonH (recallthat,asshowninSection4.3,whenQ=Iperfectextrapolationinthesettingconsideredthereinis
attainedonlywhenH →∞).
LetQ∈RD×D beadiagonalpositivesemidefinitematrixwithdiagonalentriesq ,...,q ≥0,andassumethatq >0for
1 D j
atleastsomej ∈[D](otherwise,theproblemistrivial—thecostforanycontrollerandinitialstateiszero). ForsuchQ,
thecostinanunderdeterminedLQRproblem(Equation(4)),attainedbyacontrollerK∈RD×D overafinitesetX ⊂RD
ofinitialstates,canbewrittenas:
J(K;X)= |X1 |(cid:88)
x0X
(cid:88)H h=0(cid:13) (cid:13)(A+BK)hx 0(cid:13) (cid:13)2 Q, (11)
14ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
where∥v∥ :=(cid:112) v⊤Qvforv∈RD. Theglobalminimumofthiscostis:
Q
J∗(X):=min K∈RD×DJ(K;X)= |X1 |(cid:88) x0∈X∥x 0∥2 Q,
andacontrollerKattainsthisglobalminimumifandonlyif:
∥(A+BK)x ∥2 =0 , ∀x ∈X . (12)
0 Q 0
NotethatthisoptimalityconditiongeneralizesthatofEquation(5).
LetS ⊂RD beafinitesetofinitialstatesseenintrainingandU bean(arbitrary)orthonormalbasisforS⊥. Inouranalysis
of underdetermined LQR problems with Q = I, we quantified extrapolation to initial states unseen in training via the
optimalityandcostmeasuresoverU (Definitions1and2,respectively). Definitions3and4extendtheoptimalityandcost
measurestothecaseofanon-identityQmatrix. AsshowninthesubsequentLemma3,similarlytothethecaseofQ=I,
thegeneralizedmeasuresareinvarianttothechoiceofU.
Definition3. LetQ∈RD×D beapositivesemidefinitematrix. TheQ-optimalitymeasureofextrapolationforacontroller
K∈RD×D is:
EQ (K):= 1 (cid:88) ∥(A+BK)x ∥2 .
opt |U| x0∈U 0 Q
Definition4. LetQ ∈ RD×D beapositivesemidefinitematrix. TheQ-costmeasureofextrapolationforacontroller
K∈RD×D is:
EQ (K):=J(K;U)−J∗(U),
cost
whereJ(·;U)isasdefinedinEquation(11).
Lemma3. ForanycontrollerK ∈ RD×D,theQ-optimalityandQ-costmeasuresofextrapolationareinvarianttothe
choiceoforthonormalbasisU forS⊥.
Proofsketch(proofinAppendixD.6). TheprooffollowsbyargumentssimilartothoseusedforprovingLemma2.
Withthegeneralizedmeasuresofextrapolationinhand,Proposition3belowgeneralizesProposition2fromSection4.3.
Namely,forA=A :=(cid:80)D e e⊤andsetS ={e }ofinitialstatesseenintraining,Proposition3characterizes
shift d=1 d%D+1 d 1
howtheextenttowhichpolicygradientextrapolatesdependsontheentriesofQ.AswasthecaseforQ=I(cf.Section4.3),
the learned controller attains Q-optimality and Q-cost measures of extrapolation that are substantially less than those
attainedbyK (Section3.3). ThisphenomenonismorepotentthelongerthehorizonH is,withperfectextrapolation
no-ext
attainedinthelimitH →∞.
An interesting consequence of considering a diagonal Q, not necessarily equal to the identity matrix, is that it brings
aboutanothersettingunderwhichperfectextrapolationisachieved. Specifically,ifq =···=q =0andq >0,then
2 D 1
for any H divisible by D, the learned controller achieves zero Q-optimality and cost measures. The fact that such Q
matricesleadtoperfectextrapolationcanbeintuitivelyattributedtoa“creditassignment”mechanismofapolicygradient
iteration. Namely, due to the structure of A, the trajectory of states induced by K(1) = 0 when commencing from e
1
consistsofH/Drepetitionsofthecyclee ,e ,...,e ,e . Acostisincurredalongthistrajectoryonlyatatthestartof
1 2 D 1
eachcycle. Thus,thecomponentsof∇J(K(1);S),whichexactlyalignwiththoseofA,willbeofthesamemagnitude,
i.e.∇J(K(1);S) = (cid:80)D β·e e⊤ forsomeβ > 0. Reducingthecostfore viaapolicygradientiterationwill
d=1 d%D+1 d 1
thereforereducethecostforinitialstatesinU bythesameamount. ThisisincontrasttothecaseofQ = I,wherethe
components of ∇J(K(1);S) also aligned with those of A, but have different magnitudes, thereby resulting in varying
degreesofextrapolationtoinitialstatesinU.
Proposition3. AssumethatS ={e },A=A ,H isdivisiblebyD,andthecostmatrixQ∈RD×D hasdiagonalen-
1 shift
triesq ,...,q ≥0,whereq >0foratleastsomej ∈[D]. Furthermore,letα
:=(cid:0) 2(cid:80)d
q
(cid:1) /(cid:0)(cid:0)H +1(cid:1)(cid:80)D
q
(cid:1)
∈
1 D j d j=2 j D j=1 j
[0,1)ford∈[D]. Then,policygradientwithlearningrateη
=(cid:0)H(cid:0)H +1(cid:1)(cid:80)D
q
(cid:1)−1
convergestoacontrollerK
D D j=1 j pg
15ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
that: (i)minimizesthetrainingcost,i.e.J(K ;S)=J∗(S);and(ii)satisfies:
pg
EQ (cid:0) K (cid:1) = (cid:80)D d=2q d%D+1·α d2 ·EQ (cid:0) K (cid:1) ,
opt pg (cid:80)D
q
opt no-ext
d=2 d%D+1
(cid:80)D (cid:80)D−d+1q ·(cid:81)h+d−1α2
EQ (K )= d=2 h=1 (h+d−1)%D+1 d′=d d′ ·E (K ),
cost pg (cid:80)D (cid:80)D−d+1q cost no-ext
d=2 h=1 (h+d−1)%D+1
wherebyconventionif(cid:80)D
q =0thentherighthandsidesofbothequationsabovearezeroaswell.
d=2 d%D+1
Proofsketch(fullproofinAppendixD.7). The proof follows a line identical to that of Proposition 2, generalizing it to
accountforadiagonalpositivesemidefiniteQ(asopposedtoQ=I).
C RandomSystemsGenericallyInduceExploration
Below,weformallystateandprovetheclaimmadeinSection4.4regardingrandomtransitionmatricesgenericallyinducing
exploration.
Lemma4. Givenanon-zerox ∈ RD,supposethatA ∈ RD×D isgeneratedrandomlyfromacontinuousdistribution
whosesupportisRD×D. Then,x,Ax,...AD−1xformabasisofRD almostsurely(i.e.xisacyclicvectorofAalmost
surely).
Proof. DenotebyY ∈ RD×D thematrixwhosecolumnsarex,Ax,...,AD−1x. NotethatxisacyclicvectorofAif
andonlyifthedeterminantofY,whichispolynomialintheentriesofA,isnon-zero. Thezerosetofapolynomialiseither
theentirespaceorasetofLebesguemeasurezero(Caron&Traynor,2005). Hence,itsufficestoshowthatthereexistsan
AsuchthatthedeterminantofYisnon-zero,sincethatimpliesthesetofmatricesforwhichxisnotacyclicvectorhas
probabilityzero. ToseethatsuchAexists,letz ,...,z ∈ RD bevectorscompletingxintoabasisofRD. Wecan
1 D−1
takeAtobeamatrixsatisfyingz =Axandz =Az ford∈[D−2](thewayAtransformsz canbechosen
1 d+1 d D−1
arbitrarily). UnderthischoiceofA,thecolumnsofY,i.e.x,Ax,...,AD−1x,arerespectivelyequaltox,z ,...,z .
1 D−1
Thus,Yisfullrankanditsdeterminantisnon-zero.
D DeferredProofs
Inthisappendix,weprovidefullproofsforourtheoreticalresults.
Additionalnotation. Throughouttheproofs,weuseTr(C)todenotethetraceofamatrixC.
D.1 GradientoftheCostinanLQRProblem
Throughout,wemakeuseofthefollowingexpressionforthegradientofthecostinanunderdeterminedLQRproblem.
Lemma5. ConsideranunderdeterminedLQRproblemdefinedbyA,B∈RD×D,andapositivesemidefiniteQ∈RD×D
(Section3.2). ForanyfinitesetofinitialstatesX ⊂RD,thegradientofthecostJ(·;X)(Equation(4))atK∈RD×D is
givenby:
∇J(K;X)=2B⊤(cid:88)H−1(cid:16)(cid:88)H−h [(A+BK)s−1]⊤Q(A+BK)s(cid:17)
Σ ,
X,h
h=0 s=1
withΣ := 1 (cid:80) x x⊤ = 1 (cid:80) (A+BK)hx [(A+BK)hx ]⊤forh∈{0}∪[H −1].
X,h |X| x0∈X h h |X| x0∈X 0 0
Proof. NoticethatJ(K;X)canbewrittenas:
(cid:68)(cid:88)H (cid:69)
J(K;X)= [(A+BK)h]⊤Q(A+BK)h,Σ .
X,0
h=0
16ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Astraightforwardcomputationshowsthatforany∆∈RD×D:
J(K+∆;X)=J(K;X)
(cid:68)(cid:88)H (cid:88)h−1 (cid:69)
+ [(A+BK)sB∆(A+BK)h−s−1]⊤Q(A+BK)h,Σ
X,0
h=1 s=0
(cid:124) (cid:123)(cid:122) (cid:125)
(I)
(cid:68)(cid:88)H (cid:88)h−1 (cid:69)
+ [(A+BK)h]⊤Q(A+BK)h−s−1B∆(A+BK)s,Σ
X,0
h=1 s=0
(cid:124) (cid:123)(cid:122) (cid:125)
(II)
+o(∥∆∥).
Then,theidentityTr(X⊤Y)=Tr(XY⊤)=⟨X,Y⟩foranymatricesX,Yofthesamedimensions,alongwiththecyclic
propertyofthetrace,leadsto:
(I)=(II)=(cid:68)(cid:88)H (cid:88)h−1 B⊤(cid:2) (A+BK)h−s−1(cid:3)⊤ Q(A+BK)h−sΣ ,∆(cid:69) ,
X,s
h=1 s=0
fromwhichweget:
J(K+∆;X)=J(K;X)+2(cid:68)(cid:88)H (cid:88)h−1 B⊤(cid:2) (A+BK)h−s−1(cid:3)⊤ Q(A+BK)h−sΣ ,∆(cid:69) +o(∥∆∥).
X,s
h=1 s=0
Since∇J(K;X)istheuniquelinearapproximationofJ(·;X)atK,itfollowsthat:
∇J(K;X)=2(cid:88)H (cid:88)h−1 B⊤(cid:2) (A+BK)h−s−1(cid:3)⊤ Q(A+BK)h−sΣ
X,s
h=1 s=0
=2B⊤(cid:88)H (cid:88)h−1(cid:2) (A+BK)h−s−1(cid:3)⊤ Q(A+BK)h−sΣ .
X,s
h=1 s=0
TheproofconcludesbygroupingtermswithΣ ,foreachh∈{0}∪[H −1].
X,h
D.2 ProofofProposition1
Explorationisnecessaryforextrapolation. FromLemma5, thegradientofJ(·;S)atanyK ∈ RD×D takesonthe
followingform:
∇J(K;S)=2B⊤(cid:88)H−1(cid:16)(cid:88)H−h [(A+BK)s−1]⊤Q(A+BK)s(cid:17)
Σ ,
S,h
h=0 s=1
whereΣ := 1 (cid:80) (A+BK)hx [(A+BK)hx ]⊤forh∈{0}∪[H−1]. Thus,ateverypolicygradientiteration
S,h |S| x0∈S 0 0
t∈N,therowsof∇J(K(t);S)areinthespanof{(A+BK(t))hx :x ∈S,h∈{0}∪[H −1]},i.e.inthespanofthe
0 0
statesencounteredwhenstartingfrominitialstatesinS andusingthecontrollerK(t). SinceK(1) =0,ateveryiteration
t∈N,therowsofK(t) =−η(cid:80)t−1∇J(K(i);S)areinthespanofX . Consequently,foranyinitialstatev ∈X⊥ and
i=1 pg 0 pg
t∈NwehavethatK(t)v =0. Ontheotherhand,forthenon-extrapolatingcontrollerK (definedinEquation(9))it
0 no-ext
alsoholdsthatK v =0foranyv ∈X⊥,asX⊥ ⊆S⊥. Thus,ifX ⊆span(S),thenK(t)v =K v =0
no-ext 0 0 pg pg pg 0 no-ext 0
foranyv ∈U,and:
0
E (K(t))= 1 (cid:88) (cid:13) (cid:13)(A+BK(t))v (cid:13) (cid:13)2 = 1 (cid:88) ∥Av ∥2 =E (K ).
opt |U| v0∈U(cid:13) 0(cid:13) |U| v0∈U 0 opt no-ext
Existenceofnon-exploratorysystems. LetA=B=I∈RD×D,whereIistheidentitymatrix.
We first prove that X ⊆ span(S). To do so, it suffices to prove that for all t ∈ N the rows and columns of K(t) are
pg
spanned by the set S of initial states seen in training. Indeed, in such a case A+BK(t) = I+K(t) is invariant to
span(S),i.e.foranyx∈span(S)itholdsthat(I+K(t))x=x+K(t)x∈span(S),fromwhichitreadilyfollowsthat
X ={(I+K(t))hx :x ∈S,h∈{0}∪[H],t∈N}⊆span(S).
pg 0 0
WeprovethattherowsandcolumnsofK(t) arespannedbyS byinductionovert∈N. Thebasecaseoft=1istrivial
sinceK(1) =0. Assumingthattheinductiveclaimholdsfort−1∈N,weshowthatitholdsfortaswell. Accordingto
Lemma5:
∇J(K(t−1);S)=2(cid:88)H−1(cid:16)(cid:88)H−h [(I+K(t−1))s−1]⊤(I+K(t−1))s(cid:17)
Σ(t−1),
h=0 s=1 S,h
17ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
whereΣ(t−1) := 1 (cid:80) (I+K(t−1))hx [(I+K(t−1))hx ]⊤ forh ∈ {0}∪[H −1]. Bytheinductiveassumption,
S,h |S| x0∈S 0 0
therowsandcolumnsofK(t−1) areinspan(S). Hence, bothI+K(t−1) and(I+K(t−1))⊤ areinvarianttospan(S).
Thisimpliesthat(I+K(t−1))hx ∈span(S)and[(I+K(t−1))s−1]⊤(I+K(t−1))h+sx ∈span(S)forallx ∈S,h∈
0 0 0
{0}∪[H −1],ands∈[H −h]. Consequently,∇J(K(t−1);S)isasumofouterproductsbetweenvectorsthatresidein
span(S),andsoitsrowsandcolumnsareinspan(S). Alongwiththeinductiveassumption,wethusconcludethattherows
andcolumnsofK(t) =K(t−1)−η·∇J(K(t−1);S)areinspan(S)aswell.
Wenowturntoprovethat:
E (K(t))=E (K )=1,
opt opt no-ext
E (K(t))=E (K )=H.
cost cost no-ext
Asshownabove,X ⊆span(S),andso,bythefirstpartoftheproof,K(t)v =K v =0foranyv ∈U ⊆X⊥.
pg 0 no-ext 0 0 pg
Thisimpliesthat(I+K(t))v =v foranyv ∈U,fromwhichitfollowsthat:
0 0 0
E (K(t))= 1 (cid:88) (cid:13) (cid:13)(I+K(t))v (cid:13) (cid:13)2 = 1 (cid:88) ∥v ∥2 =1.
opt |U| v0∈U(cid:13) 0(cid:13) |U| v0∈U 0
NoticingthatJ∗(U)=1(e.g.,thisminimalcostisattainedbyK ,definedinEquation(8)),wesimilarlyget:
ext
E (K(t))= 1 (cid:88) (cid:88)H (cid:13) (cid:13)(I+K(t−1))hv (cid:13) (cid:13)2 −1= 1 (cid:88) (cid:88)H ∥v ∥2−1=H.
cost |U| v0∈U h=0(cid:13) 0(cid:13) |U| v0∈U h=0 0
Additionally,bythedefinitionofK (Equation(9)),weknowthat(I+K )v =v foranyv ∈U. Bythesame
no-ext no-ext 0 0 0
computationmadeaboveforK(t),wethusgetthattheoptimalityandcostmeasuresofextrapolationattainedbyK
no-ext
overU areequaltothoseattainedbyK(t).
D.3 ProofofProposition2
WebeginbyderivinganexplicitformulaforK(2)inLemma6,fromwhichitfollowsthatpolicygradientconvergesina
singleiterationtoK =K(2).
pg
Lemma6. Policygradientconvergesinasingleiterationto:
(cid:18) (cid:19)
(cid:88)D 2(d−1)
K =K(2) =−B⊤ 1− ·e e⊤,
pg d=1 H +D d%D+1 d
whichminimizesthetrainingcost,i.e.J(K ;S)=J(K(2);S)=J∗(S).
pg
Proof. ForS ={e },byLemma5,thegradientofthetrainingcostatK(1) =0isgivenby:
1
∇J(0;e
)=2B⊤(cid:88)H−1(cid:16)(cid:88)H−h
[As−1]⊤As
(cid:17)
Σ ,
1 h=0 s=1 shift shift e1,h
whereA =(cid:80)D e e⊤ andΣ :=Ah e [Ah e ]⊤ =e e⊤ forh∈{0}∪[H −1]. Notice
shift d=1 d%D+1 d e1,h shift 1 shift 1 h%D+1 h%D+1
thatA A⊤ =I,i.e.A isanorthogonalmatrix. Hence,[As−1]⊤As =A foralls∈[H]and:
shift shift shift shift shift shift
(cid:88)H−1(cid:88)H−h
∇J(0;e )=2B⊤ A e e⊤
1 shift h%D+1 h%D+1
h=0 s=1
(cid:88)H−1(cid:88)H−h
=2B⊤ e e⊤
(h+1)%D+1 h%D+1
h=0 s=1
(cid:88)H−1
=2B⊤ (H −h)·e e⊤ .
(h+1)%D+1 h%D+1
h=0
RecallingthatH = D·LforsomeL ∈ N,thereareexactlyL = H termsinthesumcorrespondingtoe e⊤,for
D d%D+1 d
eachd ∈ [D]. Focusingonelementsh ∈ {0,D,2D,...,H −D}inthesum,whichsatisfyh%D+1 = 1,thesumof
coefficientsfore e⊤ isgivenbyH +(H −D)+···+D =(H2 +H)·2−1. Moregenerally,ford ∈ [D],thesumof
2 1 D
18ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
coefficientsfore e⊤is(H−d+1)+(H−D−d+1)+···+(D−d+1)=(H2 +H)·2−1−(d−1)H. Thus,
d%D+1 d D D
wemaywrite:
(cid:88)D (cid:18)(cid:18) H2 (cid:19) 2(d−1)H(cid:19)
∇J(0;e )=B⊤ +H − ·e e⊤,
1 d=1 D D d%D+1 d
which,combinedwithK(1) =0andη =(H2/D+H)−1,leadstothesought-afterexpressionforK(2):
(cid:18) (cid:19)
(cid:88)D 2(d−1)
K(2) =K(1)−η·∇J(K(1);e )=−B⊤ 1− ·e e⊤.
1 d=1 H +D d%D+1 d
ToseethatK(2)minimizesthetrainingcost,noticethat:
(cid:18) (cid:19)
(cid:88)D 2(d−1)
(A +BK(2))e =A e −BB⊤ 1− ·e e⊤e =e −e =0,
shift 1 shift 1 d=1 H +D d%D+1 d 1 2 2
where the second equality is due to BB⊤ = I and e⊤e = 0 for d ∈ {2,...,D}. Consequently, J(K(2);S) =
d 1
(cid:80)H ∥(A +BK(2))he ∥2 =∥e ∥2 =1,whichistheminimaltrainingcostJ∗(S)sinceforanyK∈RD×D thecost
h=0 shift 1 1
isasumofH +1non-negativeterms,withtheonecorrespondingtoh=0beingequalto∥e ∥2 =1.
1
Extrapolationintermsoftheoptimalitymeasure. Next,wecharacterizetheextenttowhichK =K(2)extrapolates,as
pg
measuredbytheoptimalitymeasure.AsshownbyLemma2inAppendixA,theoptimalitymeasureisinvarianttothechoice
oforthonormalbasisU forS⊥. Thus,becauseS ={e }wemayassumewithoutlossofgeneralitythatU ={e ,...,e }.
1 2 D
Foranye ∈U,bythedefinitionofK (Equation(9))wehavethat(A +BK )e =A e =e .
d no-ext shift no-ext d shift d d%D+1
Hence:
E (K )= 1 (cid:88)D ∥(A +BK )e ∥2 = 1 (cid:88)D ∥e ∥2 =1. (13)
opt no-ext D−1 d=2 shift no-ext d D−1 d=2 d%D+1
Ontheotherhand,byLemma6foranye ∈U:
d
(cid:88)D
(cid:18) 2(d′−1)(cid:19)
(A +BK )e =e −BB⊤ 1− ·e e⊤e
shift pg d d%D+1 d′=1 H +D d′%D+1 d′ d
(cid:18) (cid:19)
2(d−1)
=e − 1− ·e
d%D+1 H +D d%D+1
2(d−1)
= ·e ,
H +D d%D+1
andso:
E (K )= 1 (cid:88)D ∥(A +BK )e ∥2 = 4(cid:80)D d=2(d−1)2 (14)
opt pg D−1 d=2 shift pg d (D−1)(H +D)2
ThedesiredguaranteeonextrapolationintermsoftheoptimalitymeasurefollowsfromEquations(13)and(14):
E (cid:0) K (cid:1) 4(cid:80)D (d−1)2 4(D−1)2
opt pg = d=2 ≤ .
E (K ) (D−1)(H +D)2 (H +D)2
opt no-ext
Extrapolationintermsofthecostmeasure. Lastly,wecharacterizetheextenttowhichK = K(2) extrapolates,as
pg
quantifiedbythecostmeasure. Asdoneaboveforprovingextrapolationintermsoftheoptimalitymeasure,byLemma2in
AppendixAwemayassumewithoutlossofgeneralitythatU ={e ,...,e }.
2 D
Fixsomee ∈U. WeusethefactthatK =K(2) =−B⊤(cid:80)D (1− d−1 )·e e⊤(Lemma6)tostraightforwardly
d pg d=1 H+D d%D+1 d
computeE (K ). Specifically,recallingthatBB⊤ =I,wehavethatA +BK =(cid:80)D 2(d′−1) ·e e⊤.
cost pg shift pg d′=2 H+D d′%D+1 d′
Now,foranyh∈[H]:
(cid:88)D 2(d′−1)
(A +BK )he =(A +BK )h−1 ·e e⊤e
shift pg d shift pg d′=2 H +D d′%D+1 d′ d
2(d−1)
= ·(A +BK )h−1e .
H +D shift pg d%D+1
19ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Ifh≤D−d+1,unravelingtherecursionfromh−1to0leadsto:
(cid:18)
(cid:89)h+d−1
2(d′−1)(cid:19)
(A +BK )he = ·e .
shift pg d d′=d H +D (h+d−1)%D+1
Ontheotherhand,ifh>D−d+1,then(A +BK )he =0since:
shift pg d
(A +BK )he =(A +BK )h−(D−d+1)(A +BK )D−d+1e
shift pg d shift pg shift pg d
(cid:18)
(cid:89)D
2(d′−1)(cid:19)
= ·(A +BK )h−(D−d+1)e ,
d′=d H +D shift pg 1
and(A +BK )e =(cid:80)D 2(d′−1) ·e e⊤e =0. Altogether,weget:
shift pg 1 d′=2 H+D d′%D+1 d′ 1
J(K pg;{e d})=(cid:88)H h=0(cid:13) (cid:13)(A shift+BK pg)he d(cid:13) (cid:13)2 =(cid:88)D h=− 0d+1(cid:89)h d′+ =d d−1 4 (H(d′ +− D1 )) 22 ,
andso:
1 (cid:88)D (cid:88)D−d+1(cid:89)h+d−1 4(d′−1)2
J(K ;U)= . (15)
pg D−1 d=2 h=0 d′=d (H +D)2
As for the cost attained by K , let e ∈ U. By the definition of K (Equation (9)), for e ∈ U we have that
no-ext d no-ext d′
(A +BK )e = A e = e while(A +BK )e = 0. Thus, (A +BK )he =
shift no-ext d′ shift d′ d′%D+1 shift no-ext 1 shift no-ext d
e forh≤D−d+1and(A +BK )he =0forh>D−d+1. Thisimpliesthat:
(h+d−1)%D+1 shift no-ext d
J(K no-ext;{e d})=(cid:88)H (cid:13) (cid:13)(A shift+BK no-ext)he d(cid:13) (cid:13)2 =D−d+2,
h=0
andso:
1 (cid:88)D
J(K ;U)= (D−d+2). (16)
no-ext D−1 d=2
Finally,noticingthatJ∗(U)=1(e.g.,thisminimalcostisattainedbyK ,definedinEquation(8)),byEquations(15)
ext
and(16)weget:
E (K ) J(K ;U)−J∗(U)
cost pg = pg
E (K ) J(K ;U)−J∗(U)
cost no-ext no-ext
1 (cid:80)D (cid:80)D−d+1(cid:81)h+d−1 4(d′−1)2 −1
D−1 d=2 h=0 d′=d (H+D)2
=
1 (cid:80)D (D−d+2)−1
D−1 d=2
(cid:80)D (cid:80)D−d+1(cid:81)h+d−1 4(d′−1)2
d=2 h=1 d′=d (H+D)2
= .
(cid:80)D
(D−d+1)
d=2
Sincewecanupperboundthenominatorasfollows:
(cid:88)D (cid:88)D−d+1(cid:89)h+d−1 4(d′−1)2 4(D−1)2 (cid:88)D (cid:88)D−d+1 4(D−1)2 (cid:88)D
≤ 1= (D−d+1),
d=2 h=1 d′=d (H +D)2 (H +D)2 d=2 h=1 (H +D)2 d=2
wemayconclude:
(cid:80)D (cid:80)D−d+1(cid:81)h+d−1 4(d′−1)2
E cost(K pg)
=
d=2 1=0 d′=d (H+D)2
≤
4(D−1)2
.
E cost(K no-ext) (cid:80)D (D−d+1) (H +D)2
d=2
20ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
D.4 ProofofLemma1
ConsiderminimizingthesquaredEuclideannormoverthesetofcontrollerswithminimaltrainingcost,i.e.overK :=
S
{K∈RD×D :J(K;S)=J∗(S)}:
min ∥K∥2. (17)
K∈KS
InanunderdeterminedLQRproblem(Section3.2),theminimaltrainingcostJ∗(S)isattainedbyacontrollerKifand
onlyifKx = −B−1Ax forallinitialstatesx ∈ S. Letu ,...,u ∈ RD beabasisofspan(S),whereR ∈ [|S|].
0 0 0 1 R
RequiringthatKx = −B−1Ax forallx ∈ S isequivalenttorequiringtheequalityholdsforthebasisu ,...,u .
0 0 0 1 R
Thus,theobjectiveinEquation(17)isequivalentto:
min K∈RD×D∥K∥2s.t. Ku
r
=−B−1Au
r
, ∀r ∈[R], (18)
whichentailsminimizingastronglyconvexfunctionoverafinitesetoflinearconstraints. Sincethefeasiblesetisnon-empty,
e.g.,itcontainsK (seeitsdefinitioninEquation(9)),thereexistsaunique(optimal)solution,i.e.auniquecontroller
no-ext
thathasminimalsquaredEuclideannormamongthoseminimizingthetrainingcost. Wenowprovethatthisuniquesolution
isK .
no-ext
Denote the d’th row of a matrix C ∈ RD×D by C[d,:] ∈ RD, for d ∈ [D]. We can write the linear constraints
inEquation(18)asR·DconstraintsontherowsofK:
⟨K[d,:],u ⟩=−(cid:10) B−1[d,:],Au (cid:11) , ∀d∈[D],r ∈[R].
r r
SinceK satisfiestheseconstraints,bythemethodofLagrangemultipliers,toprovethatK istheuniquesolution
no-ext no-ext
ofEquation(18)weneedonlyshowthatthereexist{λ ∈R} forwhich:
d,r d∈[D],r∈[R]
(cid:88)R
K [d,:]= λ ·u , ∀d∈[D].
no-ext d,r r
r=1
Thatis,itsufficestoshowthattherowsofK areinspan(u ,...,u )=span(S). Toseethatthisisindeedthecase,
no-ext 1 R
recallthatbythedefinitionofK (Equation(9))itsatisfiesK v =0forallv ∈S⊥. Thisimpliesthattherows
no-ext no-ext 0 0
ofK necessarilyresideinspan(S),concludingtheproof.
no-ext
D.5 ProofofCorollary1
By Lemma 1, K = argmin ∥K∥2. We claim that in the considered setting K = −B−1e e⊤. Indeed,
no-ext K∈KS no-ext 2 1
(A +B(−B−1e e⊤))e = e −e = 0, meaning −B−1e e⊤ satisfies the optimality condition in Equation (5).
shift 2 1 1 2 2 2 1
Furthermore,foranyv ∈U itholdsthat−B−1e e⊤v =0sincev isorthogonaltoe ,meaning−B−1e e⊤satisfies
0 2 1 0 0 1 2 1
Equation(9). Thus,K =−B−1e e⊤and∥K ∥2 =min ∥K∥2 =1(recallBisorthogonal). Ontheother
no-ext 2 1 no-ext K∈KS
hand,asestablishedbyLemma6intheproofofProposition2,K =−B⊤(cid:80)D (1−2(d−1))·e e⊤.Consequently:
pg d=1 H+D d%D+1 d
∥K ∥2 =1+(cid:88)D (cid:16) 1− 2(d−1)(cid:17)2 =argmin ∥K∥2+(cid:88)D (cid:16) 1− 2(d−1)(cid:17)2 .
pg d=2 H +D K∈KS d=2 H +D
SinceH ≥D ≥2itholdsthat:
(cid:88)D (cid:16) 2(d−1)(cid:17)2 (cid:88)D (cid:16) (d−1)(cid:17)2 (cid:88)⌈D/2⌉ 1 ⌈D/2⌉−1
1− ≥ 1− ≥ = ,
d=2 H +D d=2 D d=2 4 4
andso:
∥K ∥2−argmin ∥K∥2 =(cid:88)D (cid:16) 1− 2(d−1)(cid:17)2 =Ω(D).
pg K∈KS d=2 H +D
D.6 ProofofLemma3
LetU beanorthonormalbasisofS⊥,andBbeanorthonormalbasisofspan(S).
21ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Now,forK∈RD×D,theQ-optimalitymeasureofextrapolationcanbewrittenasfollows:
EQ (K)= 1 (cid:88) ∥(A+BK)v ∥2
opt |U| v0∈U 0 Q
1 (cid:88)
= v⊤(A+BK)⊤Q(A+BK)v⊤
|U| v0∈U 0 0
1 (cid:68) (cid:88) (cid:69)
= (A+BK)⊤Q(A+BK), v⊤v⊤ .
|U| v0∈U 0 0
Addingandsubtracting
1 (cid:68) (cid:88) (cid:69)
(A+BK)⊤Q(A+BK), v⊤v⊤
|U| v0∈B 0 0
totherighthandsideoftheequationabove,wehavethat:
1 (cid:68) (cid:88) (cid:69) 1 (cid:68) (cid:88) (cid:69)
EQ (K)= (A+BK)⊤Q(A+BK), v v⊤ − (A+BK)⊤Q(A+BK), v v⊤ .
opt |U| v∈U∪B 0 0 |U| v∈B 0 0
Noticethatsince(cid:80) v v⊤ =I,whereIstandsfortheidentitymatrix,sinceU ∪BisanorthonormalbasisofRD.
v0∈U∪B 0 0
Thus:
EQ (K)= 1 (cid:10) (A+BK)⊤Q(A+BK),I(cid:11) − 1 (cid:68) (A+BK)⊤Q(A+BK),(cid:88) v v⊤(cid:69) .
opt |U| |U| v∈B 0 0
Ascanbeseenintheexpressionabove,theQ-optimalitymeasureofextrapolationdoesnotdependonthechoiceofU.
Similarly,forK∈RD×D,theQ-costmeasureofextrapolationcanbewrittenasfollows:
EQ (K)=J(K;U)−J∗(U)
cost
(cid:18) (cid:19)
= |U1 |(cid:88)
v0∈U
(cid:88)H h=0(cid:13) (cid:13)(A+BK)hv 0(cid:13) (cid:13)2 Q−∥v 0∥2
Q
= |U1 |(cid:88) v0∈U(cid:88)H h=1(cid:13) (cid:13)(A+BK)hv 0(cid:13) (cid:13)2
Q
=
1 (cid:88)H (cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,(cid:88)
v
v⊤(cid:69)
,
|U| h=1 v0∈U 0 0
whereweusedthefactthatJ∗(X)= 1 (cid:80) ∥x ∥2 foranyfinitesetofinitialstatesX ⊂RD. Addingandsubtracting
|X| x0∈X 0 Q
foreachsummandh∈[H]ontherighthandsidetheterm
(cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,(cid:88)
v
v⊤(cid:69)
,
0 0
v0∈B
wehavethat:
EQ (K)= 1 (cid:88)H (cid:16)(cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,(cid:88) v v⊤(cid:69)
cost |U| h=1 v0∈U∪B 0 0
−(cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,(cid:88)
v
v⊤(cid:69)(cid:17)
0 0
v0∈B
=
1 (cid:88)H (cid:16)(cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,I(cid:69)
|U| h=1
−(cid:68)(cid:2) (A+BK)h(cid:3)⊤ Q(A+BK)h,(cid:88)
v
v⊤(cid:69)(cid:17)
,
0 0
v0∈B
whereweagainusedthefactthat(cid:80) v v⊤ =IsinceU∪BisanorthonormalbasisofRD. Ascanbeseenfromthe
v0∈U∪B 0 0
expressionabove,theQ-costmeasureofextrapolationdoesnotdependonthechoiceofU.
22ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
D.7 ProofofProposition3
TheprooffollowsalineidenticaltothatofProposition2(AppendixD.3),generalizingittoaccountforadiagonalQwith
entriesq ,...,q ≥0(asopposedtoQ=I),whereq >0foratleastsomej ∈[D].
1 D j
WefirstprovethatK =K(2) =−B⊤(cid:80)D (1−α )·e e⊤. Thatis,policygradientconvergesinasingleiteration
pg d=1 d d%D+1 d
tothecontroller−B⊤(cid:80)D (1−α )·e e⊤,whichminimizesthetrainingcost. ForS = {e },byLemma5the
d=1 d d%D+1 d 1
gradientofthetrainingcostatK(1) =0isgivenby:
∇J(0;e
)=2B⊤(cid:88)H−1(cid:16)(cid:88)H−h
[As−1]⊤QAs
(cid:17)
Σ ,
1 h=0 s=1 shift shift e1,h
whereA =(cid:80)D e e⊤ andΣ :=Ah e [Ah e ]⊤ =e e⊤ forh∈{0}∪[H −1]. Notice
shift d=1 d%D+1 d e1,h shift 1 shift 1 h%D+1 h%D+1
thatQAs =(cid:80)D q ·e e⊤and[As−1]⊤QAs =(cid:80)D q ·e e⊤,for
shift d=1 (s+d−1)%D+1 (s+d−1)%D+1 d shift shift d=1 (s+d−1)%D+1 d%D+1 d
alls∈[H]. Hence:
(cid:88)H−1(cid:88)H−h(cid:16)(cid:88)D (cid:17)
∇J(0;e )=2B⊤ q ·e e⊤ e e⊤
1 (s+d−1)%D+1 d%D+1 d h%D+1 h%D+1
h=0 s=1 d=1
(cid:88)H−1(cid:16)(cid:88)H−h (cid:17)
=2B⊤ q ·e e⊤
(h+s)%D+1 (h+1)%D+1 h%D+1
h=0 s=1
(cid:88)H−1(cid:16)(cid:88)H (cid:17)
=2B⊤ q ·e e⊤ .
s%D+1 (h+1)%D+1 h%D+1
h=0 s=h+1
RecallingthatH = D·LforsomeL ∈ N,thereareexactlyL = H termsinthesumcorrespondingtoe e⊤,for
D d%D+1 d
each d ∈ [D]. Focusing on elements h ∈ {0,D,2D,...,H −D} in the sum, which satisfy h%D+1 = 1, the sum
of coefficients for e e⊤ is given by H (cid:80)D q +(H −1)(cid:80)D q +···+(cid:80)D q = H (H +1)(cid:80)D q . More
2 1 D j=1 j D j=1 j j=1 j 2D D j=1 j
generally,ford∈[D],therelevantcoefficientsarethosecorrespondingtoh∈{d−1,D+d−1,2D+d−1,...,H −
D+d−1}.
Sinceforeveryl∈[H]itholdsthat(cid:80)H
q
=(cid:80)H
q
−(cid:80)d
q ,thesumof
D s=(l·D+d−1)+1 s%D+1 s=l·D+1 s%D+1 j=2 j
coefficientsfore e⊤isobtainedbysubtracting H (cid:80)d q fromthesumofcoefficientsfore e⊤,i.e.itisequalto
d%D+1 d D j=2 j 2 1
H (H +1)(cid:80)D q − H (cid:80)d q . Wemaythereforewrite:
2D D j=1 j D j=2 j
(cid:18) (cid:18) (cid:19) (cid:19)
(cid:88)D H H (cid:88)D H (cid:88)d
∇J(0;e )=B⊤ +1 q −2 q ·e e⊤,
1 d=1 D D j=1 j D j=2 j d%D+1 d
which,combinedwithK(1) =0andη =(cid:0)H(H +1)(cid:80)D q (cid:1)−1 ,leadstothesought-afterexpressionforK(2):
D D j=1 j
K(2) =K(1)−η·∇J(K(1);e )
1
=−B⊤(cid:88)D (cid:18) 1− 2(cid:80)d j=2q j (cid:19) ·e e⊤
d=1
(cid:0)H +1(cid:1)(cid:80)D
q
d%D+1 d
D j=1 j
(cid:88)D
=−B⊤ (1−α )·e e⊤,
d d%D+1 d
d=1
whereα := 2(cid:80)d j=2qj ∈[0,1]ford∈[D]. ToseethatK(2)minimizesthetrainingcost,noticethat:
d (H D+1)(cid:80)D j=1qj
(cid:88)D
(A +BK(2))e =A e −BB⊤ (1−α )·e e⊤e =e −e =0,
shift 1 shift 1 d d%D+1 d 1 2 2
d=1
wherethesecondequalityisbyBB⊤ = I, e⊤e = 0ford ∈ {2,...,D}, andα = 0. Consequently, J(K(2);e ) =
d 1 1 1
(cid:80)H ∥(A +BK(2))he ∥2 =∥e ∥2 ,whichistheminimaltrainingcostJ∗(e )sinceforanyK∈RD×D thecostis
h=0 shift 1 Q 1 Q 1
asumofH +1non-negativeterms,withtheonecorrespondingtoh=0beingequalto∥e ∥2 .
1 Q
ExtrapolationintermsoftheQ-optimalitymeasure. Next,wecharacterizetheextenttowhichK =K(2)extrapolates,
pg
asmeasuredbytheQ-optimalitymeasure. AsshownbyLemma3inAppendixB,theQ-optimalitymeasureisinvariant
tothechoiceoforthonormalbasisU forS⊥. Thus, becauseS = {e }wemayassumewithoutlossofgeneralitythat
1
U ={e ,...,e }.
2 D
23ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Foranye ∈U,bythedefinitionofK (Equation(9))wehavethat(A +BK )e =A e =e .
d no-ext shift no-ext d shift d d%D+1
Thus:
EQ (cid:0) K (cid:1) = 1 (cid:88)D ∥(A +BK )e ∥2 = 1 (cid:88)D ∥e ∥2 = 1 (cid:88)D q .
opt no-ext D−1 d=2 shift no-ext d Q D−1 d=2 d%D+1 Q D−1 d=2 d%D+1
(19)
Ontheotherhand:
(cid:88)D
(A +BK )e =e −BB⊤ (1−α )·e e⊤e
shift pg d d%D+1 d′ d′%D+1 d′ d
d′=1
=e −(1−α )·e
d%D+1 d d%D+1
=α ·e ,
d d%D+1
andso:
EQ (K )= 1 (cid:88)D ∥(A +BK )e ∥2
opt pg D−1 d=2 shift pg d Q
= 1 (cid:88)D α2·∥e ∥2 (20)
D−1 d=2 d d%D+1 Q
1 (cid:88)D
= α2·q .
D−1 d=2 d d%D+1
ThedesiredguaranteeonextrapolationintermsoftheQ-optimalitymeasurefollowsfromEquations(19)and(20).
ExtrapolationintermsoftheQ-costmeasure. Lastly,wecharacterizetheextenttowhichK = K(2) extrapolates,
pg
asquantifiedbytheQ-costmeasure. AsdoneaboveforprovingextrapolationintermsoftheQ-optimalitymeasure,by
Lemma3inAppendixBwemayassumewithoutlossofgeneralitythatU ={e ,...,e }.
2 D
Fixsomee ∈U. WeusethefactthatK =K(2) =−B⊤(cid:80)D (1−α )·e e⊤,establishedinthebeginningof
d pg d=1 d d%D+1 d
theproof,tostraightforwardlycomputeEQ (K ). Specifically,recallingthatBB⊤ =I,wehavethatA +BK =
cost pg shift pg
(cid:80)D α ·e e⊤ =(cid:80)D α ·e e⊤,wherethesecondequalityisbynoticingthatα =0. Now,forany
d′=1 d′ d′%D+1 d′ d′=2 d′ d′%D+1 d′ 1
h∈[H]:
(cid:88)D
(A +BK )he =(A +BK )h−1 α ·e e⊤e
shift pg d shift pg d′ d′%D+1 d′ d
d′=2
=α ·(A +BK )h−1e .
d shift pg d%D+1
Ifh≤D−d+1,unravelingtherecursionfromh−1to0leadsto:
(cid:16)(cid:89)h+d−1 (cid:17)
(A +BK )he = α ·e .
shift pg d d′ (h+d−1)%D+1
d′=d
Ontheotherhand,ifh>D−d+1,then(A +BK )he =0since:
shift pg d
(A +BK(2))he =(A +BK )h−(D−d+1)(A +BK )D−d+1e
shift d shift pg shift pg d
(cid:16)(cid:89)D (cid:17)
= α ·(A +BK )h−(D−d+1)e ,
d′ shift pg 1
d′=d
and(A +BK )e =(cid:80)D α ·e e⊤e =0. Altogether,weget:
shift pg 1 d′=2 d′ d′%D+1 d′ 1
J(K pg;{e d})=(cid:88)H h=0(cid:13) (cid:13)(A shift+BK pg)he d(cid:13) (cid:13)2
Q
(cid:88)D−d+1(cid:13) (cid:13)(cid:16)(cid:89)h+d−1 (cid:17) (cid:13) (cid:13)2
= (cid:13) α d′ ·e (h+d−1)%D+1(cid:13)
h=0 (cid:13) d′=d (cid:13)
Q
(cid:88)D−d+1 (cid:89)h+d−1
= q · α2 ,
(h+d−1)%D+1 d′
h=0 d′=d
andso:
1 (cid:88)D (cid:88)D−d+1 (cid:89)h+d−1
J(K ;U)= q · α2 . (21)
pg D−1 d=2 h=0 (h+d−1)%D+1 d′=d d′
24ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
As for the cost attained by K , let e ∈ U. By the definition of K (Equation (9)), for e ∈ U we have that
no-ext d no-ext d′
(A +BK )e = A e = e while(A +BK )e = 0. Thus, (A +BK )he =
shift no-ext d′ shift d′ d′%D+1 shift no-ext 1 shift no-ext d
e forh≤D−d+1and(A +BK )he =0forh>D−d+1. Thisimpliesthat:
(h+d−1)%D+1 shift no-ext d
J(K no-ext;{e d})=(cid:88)H h=0(cid:13) (cid:13)(A shift+BK no-ext)he d(cid:13) (cid:13)2
Q
=(cid:88)D h=− 0d+1(cid:13) (cid:13)e (h+d−1)%D+1(cid:13) (cid:13)2
Q
=(cid:88) hD =− 0d+1 q (h+d−1)%D+1,
andso:
1 (cid:88)D (cid:88)D−d+1
J(K ;U)= q (22)
no-ext D−1 d=2 h=0 (h+d−1)%D+1
Finally,noticingthatJ∗(U)= 1 (cid:80)D ∥e ∥2 = 1 (cid:80)D q (e.g.,thisminimalcostisattainedbyK ,definedin
D−1 d=2 d Q D−1 d=2 d ext
Equation(8)),byEquations(21)and(22)weget:
EQ (K )=J(K ;U)−J∗(U)
cost pg pg
1 (cid:88)D (cid:88)D−d+1 (cid:89)h+d−1 1 (cid:88)D
= q · α2 − q
D−1 d=2 h=0 (h+d−1)%D+1 d′=d d′ D−1 d=2 d
1 (cid:88)D (cid:88)D−d+1 (cid:89)h+d−1
= q · α2 ,
D−1 d=2 h=1 (h+d−1)%D+1 d′=d d′
and:
EQ (K )=J(K ;U)−J∗(U)
cost no-ext no-ext
1 (cid:88)D (cid:88)D−d+1 1 (cid:88)D
= q − q
D−1 d=2 h=0 (h+d−1)%D+1 D−1 d=2 d
1 (cid:88)D (cid:88)D−d+1
= q .
D−1 d=2 h=1 (h+d−1)%D+1
ThedesiredresultreadilyfollowsfromtheexpressionsaboveforEQ (K )andEQ (K ).
cost pg cost no-ext
D.8 ProofofTheorem1
Intheproofbelow,wetreatthemoregeneralcasewhereS isanarbitrarysetoforthonormalinitialstatesseenintraining,
which includes the special case of S = {x } for a unit norm x ∈ RD. Furthermore, it will be useful to consider the
0 0
optimalitymeasureofextrapolationforindividualstatesinU,asdefinedbelow.
Definition5. TheoptimalitymeasureofextrapolationforacontrollerK∈RD×D andinitialstatex ∈U is:
0
E (K;x ):=∥(A+BK)x ∥2.
opt 0 0
D.8.1 PROOFOUTLINE
WebeginwithseveralpreliminarylemmasinAppendixD.8.2. Then,towardsestablishingthataniterationofpolicygradient
leadstoextrapolationintermsoftheoptimalitymeasure,weexamine(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)
. Thisinnerproduct
opt
canberepresentedasasumofmatrixtraces,whereeachmatrixisaproductofpowersofAandmatricesthatdependonly
onv andinitialstatesinS. InAppendixD.8.3,weshowthatE
(cid:2)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:3)
≥2H(H −1)/Dvia
0 A opt
basicpropertiesofGaussianrandomvariables.
TheremainderoftheproofconvertsthelowerboundonE
(cid:2)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:3)
intoguaranteesontheoptimality
A opt
measure attained by K(2) = K(1) −η ·∇J(K(1);S). To do so, we employ tools lying at the intersection of random
matrixtheoryandtopology. Namely,attheheartofouranalysisliesamethodfromRedelmeier(2014)forcomputingthe
expectationfortracesofrandommatrixproducts,basedonthetopologicalconceptofgenusexpansion. AppendixD.8.6
providesaself-containedintroductiontothismethod,fortheinterestedreader.
In Appendix D.8.4, we employ the method of Redelmeier (2014) for establishing extrapolation in terms of expected
optimalitymeasure. Specifically,themethodfacilitatesupperboundingE
(cid:2) ∥∇J(K(1);S)∥2(cid:3)
. Alongwiththefactthat
A
E (·)is2-smoothandthelowerboundonE
(cid:2)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:3)
,thisguaranteesareductioninoptimality
opt A opt
25ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
measurecomparedtoK(1)throughanargumentanalogoustothefundamentaldescentlemma. Noticingthattheoptimality
measureattainedbyK(1)andK areequal,concludesthispartoftheproof.
no-ext
InAppendixD.8.5,toestablishextrapolationoccurswithhighprobabilityforsystemswithsufficientlylargestatespace
dimension,wedecompose(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)
intoasumofrandomvariables,whosevariancesweupperbound
opt
by again employing the method of Redelmeier (2014). Chebyshev’s inequality then implies that with high probability
(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)
≥ H(H −1)/D. Lastly, following arguments analogous to those used for establishing
opt
reductionofoptimalitymeasureinexpectationleadstothehighprobabilityguarantee.
D.8.2 PRELIMINARYLEMMAS
Lemma7. LetZ ,...,Z beD-dimensionalindependentGaussianrandomvariables,suchthatZ ∼
N(cid:0)
0,
1I(cid:1)
for
1 K k D
k ∈[K]. Then:
(cid:32)(cid:80)K
∥Z ∥2
(cid:33)
2
Pr k=1 k ≥2 ≤ .
K KD
Proof. Forallk
∈[K],wehavethatE(cid:2)
∥Z
∥2(cid:3)
=1. Furthermore,letzdenotesomeentryofZ . Then:
k k
Var(cid:16)
∥Z
∥2(cid:17) =D·Var(cid:0) z2(cid:1) =D(cid:16) E(cid:2) z4(cid:3) −E(cid:2) z2(cid:3)2(cid:17) =D(cid:18) 3
−
1 (cid:19)
=
2
,
k D2 D2 D
wherethethirdequalityisbythefactthat,foraunivariateGaussianrandomvariabley ∼ N(0,1),wehaveE[y4] = 3.
SinceZ ,...,Z areindependent:
1 K
(cid:32)(cid:80)K
∥Z
∥2(cid:33)
2
Var k=1 k = ,
K KD
andsobyChebyshev’sinequalityweget:
(cid:32)(cid:80)K ∥Z ∥2 (cid:33) (cid:32)(cid:12) (cid:12)(cid:80)K ∥Z ∥2 (cid:12) (cid:12) (cid:33) 2
Pr k=1 k ≥2 ≤Pr (cid:12) k=1 k −1(cid:12)≥1 ≤ .
K (cid:12) K (cid:12) KD
(cid:12) (cid:12)
Lemma8. Foranyv ∈U andx ∈S itholdsthat:
0 0
(cid:68) ∇E (K(1);v ),∇J(K(1);x )(cid:69) =4(cid:88)H−1(cid:88)H−n−1 ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
opt 0 0 0 0 0 0
n=0 k=0
Proof. ForacontrollerK ∈ RD×D,letJ (K;{v }) := ∥v ∥2+∥(A+BK)v ∥2 denotethecost(Equation(4))that
1 0 0 0
it attains over v for a time horizon H = 1. Notice that E (K;v ) = J (K;{v })−1, and so ∇E (K;v ) =
0 opt 0 1 0 opt 0
∇J (K;{v }). Thus,applyingthecostgradientformulaofLemma5,forboth∇E (K(1);v )and∇J(K(1);x ),while
1 0 opt 0 0
recallingthatQ=IandK(1) =0,weobtain:
(cid:68)
∇E (K(1);v ),∇J(K(1);x
)(cid:69) =(cid:28)
2B⊤Av
v⊤,2B⊤(cid:88)H−1(cid:16)(cid:88)H−h (As−1)⊤As(cid:17)
Σ
(cid:29)
,
opt 0 0 0 0
h=0 s=1
x0,h
withΣ := 1 (cid:80) x x⊤ = 1 (cid:80) Ahx [Ahx ]⊤forh∈{0}∪[H −1]. SinceBisanorthogonalmatrix,
x0,h |X| x0∈X h h |X| x0∈X 0 0
bytheidentityTr(X⊤Y)=Tr(XY⊤)=⟨X,Y⟩formatricesX,Yofthesamedimensions,andthecyclicpropertyof
26ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
thetrace,weget:
(cid:68) ∇E (K(1);v ),∇J(K(1);x )(cid:69) =4Tr(cid:18) v v⊤A⊤(cid:88)H−1(cid:88)H−h (As−1)⊤AsΣ (cid:19)
opt 0 0 0 0
h=0 s=1
x0,h
=4(cid:88)H−1(cid:88)H−h Tr(cid:16) v v⊤(As)⊤AsΣ (cid:17)
h=0 s=1
0 0 x0,h
=4(cid:88)H−1(cid:88)H−h Tr(cid:16)
Σ v
v⊤(As)⊤As(cid:17)
h=0 s=1
x0,h 0 0
=4(cid:88)H−1(cid:88)H−h Tr(cid:16)
Ahx x⊤(Ah)⊤v
v⊤(As)⊤As(cid:17)
0 0 0 0
h=0 s=1
=4(cid:88)H−1(cid:88)H−h(cid:10) v ,Ahx (cid:11) ·Tr(cid:16) Ahx v⊤(As)⊤As(cid:17) .
0 0 0 0
h=0 s=1
Thetraceofamatrixanditstransposeareequal. Hence,Tr(cid:0) Ahx v⊤(As)⊤As(cid:1) =Tr(cid:0) (As)⊤Asv (Ahx )⊤(cid:1) . Applying
0 0 0 0
thecyclicpropertyofthetraceoncemore,andintroducingtheindicesn=handk =s−1,concludes:
(cid:68) ∇E (K(1);v ),∇J(K(1);x )(cid:69) =4(cid:88)H−1(cid:88)H−h(cid:10) v ,Ahx (cid:11) ·Tr(cid:16) Ahx v⊤(As)⊤As(cid:17)
opt 0 0 0 0 0 0
h=0 s=1
=4(cid:88)H−1(cid:88)H−n−1 ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
0 0 0 0
n=0 k=0
Lemma9. ThefunctionE (·)is2-smooth. Thatis,foranyK,K′ ∈RD×D itholdsthat∥∇E (K)−∇E (K′)∥≤
opt opt opt
2∥K−K′∥.
Proof. For a controller K ∈ RD×D and v ∈ U, let J (K;{v }) := ∥v ∥2 + ∥(A+BK)v ∥2 denote the cost
0 1 0 0 0
(Equation (4)) that it attains over v for a time horizon H = 1. Notice that E (K;v ) = J (K;{v }) − 1, and
0 opt 0 1 0
so∇E (K;v )=∇J (K;{v }). Thus,applyingthecostgradientformulaofLemma5,weobtain:
opt 0 1 0
∇E (K;v )=2B⊤(A+BK)v v⊤.
opt 0 0 0
ForanyK′ ∈RD×D theaboveformulagives:
∥∇E opt(K;v 0)−∇E opt(K′;v 0)∥=(cid:13) (cid:13)2B⊤B(K−K′)v 0v 0⊤(cid:13) (cid:13)
=(cid:13) (cid:13)2(K−K′)v 0v 0⊤(cid:13) (cid:13),
wherethesecondequalityisbyrecallingthatB⊤B=I. Bysub-multiplicativityofthematrixEuclideannorm,wegetthat:
∥∇E opt(K;v 0)−∇E opt(K′;v 0)∥≤2(cid:13) (cid:13)v 0v 0⊤(cid:13) (cid:13)·∥K−K′∥=2∥K−K′∥.
wherethelastequalityisduetov beingofunitnorm. Finally,wehave:
0
1 (cid:88)
E (K)= E (K;v ),
opt |U| v0∈U opt 0
andthereforeE (·)is2-smooth,beinganaverageof2-smoothfunctions.
opt
D.8.3 LOWERBOUNDONE
A(cid:2)(cid:10)
∇E
opt(K(1)),∇J(K(1);S)(cid:11)(cid:3)
Inthispartoftheproof,weestablishthat:
E
(cid:104)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:105)
≥
2H(H −1)
.
A opt D
Todoso,itsufficestoshowthatforallv ∈U itholdsthatE (cid:2)(cid:10) ∇E (K(1);v ),∇J(K(1);S)(cid:11)(cid:3) ≥ 2H(H−1). Indeed,
0 A opt 0 D
sinceE (K(1))= 1 (cid:80) E (K(1);v ),linearityofthegradientandexpectationthenyieldthedesiredlowerbound.
opt |U| v0∈U opt 0
WebeginbyprovingthattheexpectedinnerproductdoesnotdependonthechoiceoforthonormalinitialstatesinS andU.
27ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Lemma10. Foranyv ∈U,x ∈S,andanytwodifferentstandardbasisvectorse ,e ∈RD:
0 0 i j
(cid:104)(cid:68) (cid:69)(cid:105) (cid:104)(cid:68) (cid:69)(cid:105)
E ∇E (K(1);v ),∇J(K(1);x ) =E ∇E (K(1);e ),∇J(K(1);e ) ,
A opt 0 0 A opt i j
and,inparticular,foranyn∈{0}∪[H −1]andk ∈{0}∪[H −n−1]:
E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) =E (cid:2) ⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) .
A 0 0 0 0 A i j i j
Proof. ByLemma8:
(cid:68) ∇E (K(1);v ),∇J(K(1);x )(cid:69) =4(cid:88)H−1(cid:88)H−n−1 ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
opt 0 0 0 0 0 0
n=0 k=0
Itsufficestoshowthat,foralln∈{0}∪[H −1]andk ∈{0}∪[H −n−1],therandomvariable
⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)
0 0 0 0
isdistributedidenticallyastherandomvariable
⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1) .
i j i j
Indeed,thisimpliesthat,foralln∈{0}∪[H −1]andk ∈{0}∪[H −n−1]:
E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) =E [⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1) ],
A 0 0 0 0 A i j i j
andsofromlinearityoftheexpectation:
E (cid:104)(cid:68) ∇E (K(1);v ),∇J(K(1);x )(cid:69)(cid:105) =E (cid:20) 4(cid:88)H−1(cid:88)H−n−1 ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:21)
A opt 0 0 A 0 0 0 0
n=0 k=0
(cid:20) (cid:21)
=E 4(cid:88)H−1(cid:88)H−n−1 ⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)
A i j i j
n=0 k=0
(cid:104)(cid:68) (cid:69)(cid:105)
=E ∇E (K(1);e ),∇J(K(1);e ) .
A opt i j
Now,fixsomen∈{0}∪[H−1]andk ∈{0}∪[H−n−1]. LetU∈RD×DbeanorthogonalmatrixsatisfyingUe =v
i 0
andUe =x ,andletM:=U⊤AU∈RD×D. Considertherandomvariable
j 0
⟨e ,Mne ⟩·Tr(cid:0) e (Mne )⊤(Mk+1)⊤Mk+1(cid:1) .
i j i j
BythedefinitionsofUandMwehavethat⟨e ,Mne ⟩=e⊤Mne =e⊤U⊤AnUe =v⊤Anx and:
i j i j i j 0 0
Tr(cid:0) e (Mne )⊤(Mk+1)⊤Mk+1(cid:1) =Tr(cid:0) U⊤v (Anx )⊤U(Mk+1)⊤Mk+1(cid:1)
i j 0 0
=Tr(cid:0) v (Anx )⊤U(Mk+1)⊤Mk+1U⊤(cid:1)
0 0
=Tr(cid:0) v (Anx )⊤UU⊤(Ak+1)⊤UU⊤Ak+1UU⊤(cid:1)
0 0
=Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) ,
0 0
wherethesecondequalityisbythecyclicpropertyofthetrace. Thus:
⟨e ,Mne ⟩·Tr(cid:0) e (Mne )⊤(Mk+1)⊤Mk+1(cid:1) =⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
i j i j 0 0 0 0
NoticethattheorthogonalityofUimpliesthattheentriesofMareindependentGaussianrandomvariableswithmean
√
zeroandstandarddeviation1/ D. Thatis,theentriesofMandAareidenticallydistributed. Combinedwiththeequality
above,weconcludethat⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) and⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)
0 0 0 0 i j i j
areidenticallydistributed.
28ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
WithLemma10inplace,wenowlowerboundtheexpectedinnerproductbetween∇E (K(1);v )and∇J(K(1);S),for
opt 0
anyv ∈U asnecessary.
0
Letv ∈U. ByLemma8:
0
H−1H−n−1
(cid:68) ∇E (K(1);v ),∇J(K(1);S)(cid:69) = 4 (cid:88) (cid:88) (cid:88) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
opt 0 |S| 0 0 0 0
x0∈S n=0 k=0
TakingtheexpectationwithrespecttoA,weget:
H−1H−n−1
E (cid:104)(cid:68) ∇E (K(1);v ),∇J(K(1);S)(cid:69)(cid:105) = 4 (cid:88) (cid:88) (cid:88) E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) .
A opt 0 |S| A 0 0 0 0
x0∈S n=0 k=0
(23)
Thesought-afterresultwillreadilyfollowfromthelemmabelow.
Lemma11. Foranyx ∈S,v ∈U,n∈[H −1],andk ∈{0}∪[H −n−1]itholdsthat:
0 0
E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) ≥ 1 . (24)
A 0 0 0 0 D
Proof. AccordingtoLemma10,wemayreplacex andv withanytwodifferentstandardbasisvectorse ∈ RD and
0 0 j
e ∈RD,respectively,since:
i
E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) =E (cid:2) ⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) .
A 0 0 0 0 A i j i j
Thus,inwhatfollowsweshowthat:
E (cid:2) ⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3) ≥ 1 .
A i j i j D
First,letusconsiderthecaseofk =0. Notethatinthiscase,bythecyclicpropertyofthetrace:
⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1) =⟨e ,Ane ⟩·(cid:10) Ae ,An+1e (cid:11) .
i j i j i j i j
Denotingthe(w,z)’thentryofAbya ∈R,forw,z ∈[D],theinnerproductsontherighthandsidecanbewrittenas:
w,z
D
(cid:88)
⟨e ,Ane ⟩= a a ·····a ,
i j i,ti t1,t2 tn−1,j
t1,...,tn−1=1
and:
D
(cid:10) Ae ,An+1e (cid:11) = (cid:88) a ·a a ·····a .
i j l,i l,r1 r1,r2 rn,j
l,r1,...,rn=1
Combiningbothequationsaboveleadsto:
D D
⟨e ,Ane ⟩·(cid:10) Ae ,An+1e (cid:11) = (cid:88) (cid:88) a a ·····a ·a ·a a ·····a . (25)
i j i j i,ti t1,t2 tn−1,j l,i l,r1 r1,r2 rn,j
t1,...,tn−1=1l,r1,...,rn=1
√
SincetheentriesofAareindependentlydistributedaccordingtoazero-meanGaussianwithstandarddeviation1/ D,
basicpropertiesofGaussianrandomvariablesimplythat,foranyw,z ∈[D]andp∈N:
(cid:40)
Dp 2 ·E A(cid:2) ap w,z(cid:3) = ( 0p−1)!!:=(p−1)(p−3)···3 ,
,
i of thp ei rs we iv se en . (26)
Accordingtotheabove,theexpectationofeachsummandontherighthandsideofEquation(25)isnon-negative. Moreover,
for the expectation of a summand to be positive, every entry of A in it needs to have an even power (otherwise, the
29ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
expectationiszero). Wenowdescribeasubsetofindicesforwhichthisoccurs. Considerindicesl,t ,...,t ,r ,...,r
1 n−1 1 n
satisfying:
t =r ,t =r ,...,r =r .
1 2 2 3 n−1 n
Thisimpliesthat:
a =a ,...,a =a ,
t1,t2 r2,r3 tn−1,i rn,i
sothesetermsarepairedup. Weareleftwitha ,a ,a ,a . Requiringthatr =ipairsuptheseremainingterms.
i,r2 l,i l,r1 r1,r2 1
Forallpossibleindexassignmentssatisfyingthespecifiedconstraints,theentriesofAinthecorrespondingsummandhave
evenpowers. Asaresult,byEquation(26)forsuchchoiceofindices:
(cid:2) (cid:3) 1
E a a ·····a ·a ·a a ·····a ≥ ,
A i,ti t1,t2 tn−1,j l,i l,r1 r1,r2 rn,j Dn+1
asthereareoverall2n+2termsintheproduct. Itremainstocountthenumberofindexassignments, forthesumsin
Equation(25),thatsatisfythespecifiedconstraints. WehaveDoptionsforeachoftheunconstrainedindicesl,r ,...,r ,
2 n
andsothereareoverallDnrelevantindexassignments. Thus:
E (cid:2) ⟨e ,Ane ⟩·(cid:10) Ae ,An+1e (cid:11)(cid:3) ≥ Dn = 1 ,
A i j i j Dn+1 D
i.e.,wehaveestablishedEquation(24)forthecaseofk =0.
Next,weshowthatEquation(24)holdsfork ∈ [H −n−1]byreducingthiscasetothecaseofk = 0. Noticethatthe
expressionwithintheexpectationfromEquation(24)canbewrittenas:
⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1) =⟨e ,Ane ⟩·(cid:10) (Ak)⊤AkAe ,An+1e (cid:11) .
i j i j i j i j
DenotingC:=(Ak)⊤Ak andthe(w,z)’thentryofCbyc ,wehavethat:
w,z
⟨e ,Ane ⟩·Tr(cid:0) e (Ane )⊤(Ak+1)⊤Ak+1(cid:1)
i j i j
=⟨e ,Ane ⟩·(cid:10) CAe ,An+1e (cid:11)
i j i j
(27)
D D
(cid:88)D (cid:88) (cid:88)
= a a ·····a ·c ·a ·a a ·····a .
w,z=1
i,ti t1,t2 tn−1,j w,z z,i w,r1 r1,r2 rn,j
t1,...,tn−1=1l,r1,...,rn=1
LetusfocusonthecontributionofCtotheexpressionabove. Forw,z ∈[D],the(w,z)’thentryofCisgivenby:
D
(cid:88)
c = (a a ·····a )·(a a ·····a ).
w,z y1,w y2,y1 yk,yk−1 yk,yk+1 yk+1,yk+2 y2k−1,z
y1,...,y2k−1=1
Asbefore,wewouldliketopairupindicestoachievealowerboundonthenumberofsummandsinEquation(27)inwhich
entriesofAhaveevenpowers. Wethereforerequirethat:
y =y ,y =y ,...,y =y .
1 2k−1 2 2k−2 k−1 k+1
Thispairsupallbuttheleftmostandrightmosttermsofc . GoingbacktoEquation(27)andimposingw = z onthe
w,z
indexassignment,wehavematchedthetermsaddedduetokbeingnon-zero. Fortheremainingindices,wecanapplythe
samematchingschemeusedforthek =0case. Duetokbeingnon-zero,thereare2kadditionalterms,whichaddtothe
powerofD whenlowerboundingtheexpectationofeachsummand,buttheyarecompensatedbyasummationoverk
additionalunconstrainedindices.
30ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Overall,goingbacktoEquation(23)andapplyingLemma11concludestheproof:
H−1H−n−1
E (cid:104)(cid:68) ∇E (K(1);v ),∇J(K(1);S)(cid:69)(cid:105) = 4 (cid:88) (cid:88) (cid:88) E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3)
A opt 0 |S| A 0 0 0 0
x0∈S n=0 k=0
H−1H−n−1
= 4 (cid:88) (cid:88) (cid:88) E (cid:2) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1)(cid:3)
|S| A 0 0 0 0
x0∈S n=1 k=0
H−1H−n−1
4 (cid:88) (cid:88) (cid:88) 1
≥
|S| D
x0∈S n=1 k=0
2H(H −1)
= ,
D
wherethesecondequalityisbynoticingthatforn=0theexpectationiszerosince⟨v ,Anx ⟩=⟨v ,x ⟩=0.
0 0 0 0
D.8.4 OPTIMALITYMEASUREDECREASESINEXPECTATION
Inthispartoftheproof,weestablishthatforanystepsizeη ≤ 1 :
4DH(H−1)(4H−1)!!
E
(cid:2)
E
(cid:0) K(2)(cid:1)(cid:3)
H(H −1)
A opt
≤1−η· .
E [E (K )] D
A opt no-ext
ByLemma9,E (·)is2-smooth. Thus:
opt
E (K(2))≤E (K(1))+(cid:10) ∇E (K(1)),K(2)−K(1)(cid:11) +∥K(2)−K(1)∥2
opt opt opt
(28)
=E (K(1))−η·(cid:10) ∇E (K(1)),∇J(K(1);S)(cid:11) +η2·∥∇J(K(1);S)∥2.
opt opt
AsweprovedinAppendixD.8.3,theexpectedinnerproductbetween∇E (K(1))and∇J(K(1);S)islowerboundedas
opt
follows:
(cid:104)(cid:68) (cid:69)(cid:105) 2H(H −1)
E ∇E (K(1)),∇J(K(1);S) ≥ .
A opt D
TakinganexpectationwithrespecttoAoverbothsidesofEquation(28)thusleadsto:
E
(cid:104)
E
(K(2))(cid:105)
≤E
(cid:104)
E
(K(1))(cid:105)
−η·E
(cid:104)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:105)
+η2·E
(cid:104) ∥∇J(K(1);S)∥2(cid:105)
.
A opt A opt A opt A
(cid:104) (cid:105) 2H(H −1) (cid:104) (cid:105)
≤E E (K(1)) −η· +η2·E ∥∇J(K(1);S)∥2 .
A opt D A
Now, in order to upper bound E
(cid:2) ∥∇J(K(1);S)∥2(cid:3)
, we employ a method from Redelmeier (2014), mentioned in the
A
proofoutline(AppendixD.8.1)andintroducedinAppendixD.8.6,whichfacilitatescomputingexpectedtracesofrandom
matrixproductsthroughthetopologicalconceptofgenusexpansion. Foreaseofexposition,wedefertheupperboundon
E
(cid:2) ∥∇J(K(1);S)∥2(cid:3)
toLemma12inAppendixD.8.4.1below. Specifically,Lemma12showsthat:
A
E A(cid:104)(cid:13) (cid:13)∇J(K(1);S)(cid:13) (cid:13)2(cid:105) ≤4H2(H −1)2(4H −1)!!.
PluggingthisintoourupperboundonE
(cid:2)
E
(K(2))(cid:3)
gives:
A opt
(cid:104) (cid:105) (cid:104) (cid:105) 2H(H −1)
E E (K(2)) ≤E E (K(1)) −η· +4η2·H2(H −1)2(4H −1)!!.
A opt A opt D
Theassumptionthatη ≤ 1 implies:
4DH(H−1)(4H−1)!!
1 2H(H −1)
4η2·H2(H −1)2(4H −1)!!≤ η· .
2 D
31ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Hence:
(cid:104) (cid:105) (cid:104) (cid:105) H(H −1)
E E (K(2)) ≤E E (K(1)) −η· . (29)
A opt A opt D
Note that K(1)v = 0 for any v ∈ U, since K(1) = 0, and similarly K v = 0 by the definition of K in
0 0 no-ext 0 no-ext
Equation(9). Consequently,theexpectedoptimalitymeasuresthattheyattainsatisfy:
E
(cid:2)
E (K
)(cid:3)
=E
(cid:2)
E
(K(1))(cid:3)
=
1 (cid:88)
E
(cid:2)
∥Av
∥2(cid:3)
=1,
A opt no-ext A opt |U| v0∈U A 0
duetov ∈U beingofunitnormandtheentriesofAbeingindependentGaussianrandomvariableswithmeanzeroand
0 √
standarddeviation1/ D. GoingbacktoEquation(29)wemaythereforeconclude:
E
(cid:2)
E
(K(2))(cid:3)
H(H −1)
A opt
≤1−η· .
E [E (K )] D
A opt no-ext
D.8.4.1 UpperBoundonE
(cid:2) ∥∇J(K(1);S)∥2(cid:3)
A
Lemma12. Itholdsthat:
(cid:104) (cid:105)
E ∥∇J(K(1);S)∥2 ≤4H2(H −1)2(4H −1)!!.
A
Proof. ByLemma5andtheidentity⟨X,Y⟩=Tr(X⊤Y),formatricesX,Yofsuitabledimensions,weget:
(cid:13) (cid:13)∇J(K(1);S)(cid:13) (cid:13)2
=(cid:10) ∇J(K(1);S),∇J(K(1);S)(cid:11)
(cid:42) H−1H−h H−1H−h (cid:43)
= 4 B⊤ (cid:88) (cid:88) (Ak−1)⊤Ak (cid:88) Anx (Anx )⊤,B⊤ (cid:88) (cid:88) (Al−1)⊤Al (cid:88) Amy (Amy )⊤
|S|2 0 0 0 0
n=0 k=1 x0∈S m=0 l=1 x0∈S
H−1H−nH−1H−m
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) Tr(cid:16) Anx (Anx )⊤(Ak)⊤Ak−1(Al−1)⊤AlAmy (Amy )⊤(cid:17) .
|S|2 0 0 0 0
x0,y0∈S n=0 k=1 m=0 l=1
whererecallK(1) =0andBisorthogonal. Forconvenience,letuschangethesummationoverktobefrom0toH−n−1,
asopposedtofrom1toH −n,andsimilarlythesummationoverltobefrom0toH −m−1. Alongwiththecyclic
propertyofthetracewemaywrite:
(cid:13) (cid:13)∇J(K(1);S)(cid:13) (cid:13)2
H−1H−n−1H−1H−m−1
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) Tr(cid:16) Anx (Anx )⊤(Ak+1)⊤Ak(Al)⊤Al+1Amy (Amy )⊤(cid:17)
|S|2 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
H−1H−n−1H−1H−m−1
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) Tr(cid:16) (Anx )⊤(Ak+1)⊤Ak(Al)⊤Al+1Amy (Amy )⊤Anx (cid:17)
|S|2 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
= 4 (cid:88) H (cid:88)−1H (cid:88)−n−1H (cid:88)−1H− (cid:88)m−1 y⊤(Am)⊤Anx ·Tr(cid:16) (Anx )⊤(Ak+1)⊤Ak(Al)⊤Al+1Amy (cid:17) (30)
|S|2 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
H−1H−n−1H−1H−m−1
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (Anx )⊤(Ak+1)⊤Ak(Al)⊤Al+1Amy (cid:17)
|S|2 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
H−1H−n−1H−1H−m−1
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (An+k+1)⊤Ak(Al)⊤Am+l+1y x⊤(cid:17) .
|S|2 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
Now,foreachx ,y ,n,k,m,lwewillshowthat:
0 0
E (cid:104) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (An+k+1)⊤Ak(Al)⊤Am+l+1y x⊤(cid:17)(cid:105) ≤(p−1)!!, (31)
A 0 0 0 0
32ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
where p := 2(n+k+m+l+1) ≤ 4H and (p−1)!! := (p−1)(p−3)···3. To that end, we employ the method
fromRedelmeier(2014),whichisbasedonthetopologicalconceptofgenusexpansion. Forcompleteness,AppendixD.8.6
providesaself-containedintroductiontothemethod,andTheorem2thereinlaysouttheresultwhichwewilluse. We
assumebelowfamiliaritywiththenotationandconceptsdetailedinAppendixD.8.6.
ForinvokingTheorem2,letusdefineapermutationγ over[p]viathecycledecomposition:
γ =(1,...,m+n)(m+n+1,...,p),
andamappingϵ:[p]→{−1,1}by:
ϵ(1)=−1,...,ϵ(n)=−1,
ϵ(n+1)=1,...,ϵ(n+m)=1,
ϵ(n+m+1)=−1,...,ϵ(2n+m+k+1)=−1,
ϵ(2n+m+k+2)=1,...,ϵ(2n+m+1+2k)=1,
ϵ(2n+m+2k+2)=−1,...,ϵ(2n+m+2k+l+1)=−1,
ϵ(2n+m+2k+l+2)=1,...,ϵ(p)=1.
Furthermore,defineC ,...,C ∈RD×D by:
1 p
C =I,...,C =I,C =y x⊤,
1 m+n−1 m+n 0 0
C =I,...,C =I,C =y x⊤,
m+n+1 p−1 p 0 0
whereIistheidentitymatrix. Fortheabovechoiceofγ,ϵ,andmatricesC ,...,C itholdsthat:
1 p
E (cid:2) Tr (cid:0) A C ,...,A C (cid:1)(cid:3) =E (cid:104) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (An+k+1)⊤Ak(Al)⊤Am+l+1y x⊤(cid:17)(cid:105) .
A γ ϵ(1) 1 ϵ(p) p A 0 0 0 0
InvokingTheorem2,wemaywriteEquation(39)(fromTheorem2ofAppendixD.8.6)as:
E
(cid:2)
Tr
(cid:0)
A C ,...,A C
(cid:1)(cid:3)
=
(cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ
r (C ,...,C ). (32)
A γ ϵ(1) 1 ϵ(p) p γ−−1δϵπδϵγ+ 1 p
π∈{ρδρ:ρ∈Mp} 2
Noticethat,duetoourchoiceofC ,...,C ,eachsummandontherighthandsideofEquation(32)isnon-negative.
1 p
Now,supposethatx ̸=y . Then,x isorthogonaltoy (recallS isanorthonormalsetofinitialstates),andso:
0 0 0 0
TĎ r(cid:0)
y
x⊤(cid:1) =TĎ r(cid:0)
x
y⊤(cid:1) =TĎ r(cid:0)
(y
x⊤)2(cid:1) =TĎ r(cid:0)
(x
y⊤)2(cid:1)
=0,
0 0 0 0 0 0 0 0
while:
TĎ r(cid:0) x y⊤y x⊤(cid:1) =TĎ r(cid:0) y x⊤x y⊤(cid:1) = 1 .
0 0 0 0 0 0 0 0 D
The only way a summand on the right hand side of Equation (32), corresponding to π = ρδρ, can provide a non-zero
contribution is if a cycle R = (1,...,R) of γ−1δ πδ γ /2 contains either no non-identity matrices, in which case
Ď + ϵ ϵ −
Tr(C ·····C )=1,orifitcontainstwonon-identitymatricesappearingoncetransposedandoncewithouttransposition,
1 R
Ď
inwhichcaseTr(C ·····C )=1/D. Accordingly,thetwonon-identitymatricesamongC ,...,C mustappearin
1 R 1 p
asinglecycleofγ−1δ πδ γ /2forasummandtobenon-zero. ItfollowsthatthesurfaceG(γ,ϵ,ρ)(seeconstructionin
+ ϵ ϵ −
AppendixD.8.6)mustbeconnected. Thus,byProposition6inAppendixD.8.6,theEulercharacteristicofsuchasurface
satisfiesχ(G(γ,ϵ,ρ))≤2. Finally,fromProposition7weknowthatχ(G(γ,ϵ,ρ))=χ(γ,δ πδ ). Asaresult,anon-zero
ϵ ϵ
summandcontributesatmostDχ(γ,δϵπδϵ)−|γ|−1 =D2−2−1 =D−1.
Now,supposethatx =y . Inthiscase:
0 0
TĎ r(cid:0) y x⊤(cid:1) =TĎ r(cid:0) x y⊤(cid:1) =TĎ r(cid:0) (y x⊤)2(cid:1) =TĎ r(cid:0) (x y⊤)2(cid:1) =TĎ r(cid:0) x y⊤y x⊤(cid:1) =TĎ r(cid:0) y x⊤x y⊤(cid:1) = 1 .
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 D
IfC = C = y x⊤ areinthesamecycleofγ−1δ πδ γ /2,thenasinthex = y case,thesurfaceG(γ,ϵ,ρ)is
m+n p 0 0 + ϵ ϵ − 0 0
connected. Thus,byProposition6χ(G(γ,ϵ,ρ))≤2andthecorrespondingsummandcontributesafactorofD2−2−1 =
33ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
D−1.Ontheotherhand,IfC =C =y x⊤arenotinthesamecycleofγ−1δ πδ γ /2,thenthesurfaceG(γ,ϵ,ρ)can
m+n p 0 0 + ϵ ϵ −
havetwoconnectedcomponents(itcannothavemorethantwobecause|γ|=2),andsoχ(G(γ,ϵ,ρ))≤4byProposition6.
Wethereforeobtainafactorof1/D2fromthetracealongγ−1δ πδ γ /2ofC ,...,C ,andthecorrespondingsummand
+ ϵ ϵ − 1 p
contributesatmostD4−2−2 =D0 =1.
Overall,thenumberofsummandsinEquation(32)is|M |=(p−1)!!,i.e.thenumberofpairingsof[p],andwehaveseen
p
thateachsummandcontributesatmost1(forboththex = y andx ̸= y cases). Hence,fromEquation(32)weget
0 0 0 0
Equation(31):
E (cid:104) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (An+k+1)⊤Ak(Al)⊤Am+l+1y x⊤(cid:17)(cid:105) ≤(p−1)!!≤(4H −1)!!.
A 0 0 0 0
GoingbacktoEquation(30)andtakinganexpectationwithrespecttoAconcludes:
E
A(cid:104)(cid:13) (cid:13)∇J(K(1);S)(cid:13) (cid:13)2(cid:105)
H−1H−n−1H−1H−m−1
= 4 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) E (cid:104) Tr(cid:0) (An)⊤Amy x⊤(cid:1) ·Tr(cid:16) (An+k+1)⊤Ak(Al)⊤Am+l+1y x⊤(cid:17)(cid:105)
|S|2 A 0 0 0 0
x0,y0∈S n=0 k=0 m=0 l=0
≤4H2(H −1)2(4H −1)!!.
D.8.5 OPTIMALITYMEASUREDECREASESWITHHIGHPROBABILITY
Inthispartoftheproof,weestablishthatforanyδ ∈(0,1),ifD ≥|S|+6|S|H(H−1)(4H−1)!! andη ≤ 1 ,
δ 8D2H(H−1)(4H−1)!!
thenwithprobabilityatleast1−δoverthechoiceofA:
E
(cid:0) K(2)(cid:1)
H(H −1)
opt
≤1−η· .
E (K ) 4D
opt no-ext
Tothatend,webeginbyconvertingthelowerboundonE
(cid:2)(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)(cid:3)
fromAppendixD.8.3intoa
A opt
boundthatholdswithhighprobability. ByLemma8:
H−1H−n−1
(cid:68) ∇E (K(1)),∇J(K(1);S)(cid:69) = 4 (cid:88) (cid:88) (cid:88) (cid:88) ⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) .
opt |S||U| 0 0 0 0
x0∈Sv0∈U n=0 k=0
Forx ∈S,v ∈U,n∈{0}∪[H −1],k ∈{0}∪[H −n−1],introducingtherandomvariables:
0 0
Z :=⟨v ,Anx ⟩·Tr(cid:0) v (Anx )⊤(Ak+1)⊤Ak+1(cid:1) ,
v0,x0,n,k 0 0 0 0
1 (cid:88) (33)
Y := Z ,
x0,n,k |U| v0∈U v0,x0,n,k
wemaywrite:
H−1H−n−1 H−1H−n−1
(cid:68) (cid:69) 4 (cid:88) (cid:88) (cid:88) 4 (cid:88) (cid:88) (cid:88)
∇E (K(1)),∇J(K(1);S) = Y = Y , (34)
opt |S| x0,n,k |S| x0,n,k
x0∈S n=0 k=0 x0∈S n=1 k=0
where the last transition is by noticing that Y = 0 for n = 0 since ⟨v ,Anx ⟩ = ⟨v ,x ⟩ = 0. Lemma 11 in
x0,n,k 0 0 0 0
AppendixD.8.3hasshownthatE[Z ]≥1/D,andsoE[Y ]≥1/Daswell,forallx ∈S,v ∈U,n∈[H−1],
v0,x0,n,k x0,n,k 0 0
andk ∈{0}∪[H −n−1].
Now, fix some x ∈ S,n ∈ [H − 1], and k ∈ {0} ∪ [H − n − 1]. For upper bounding Var(Z ) and
0 v0,x0,n,k
Cov(Z ,Z ), for v ,v′ ∈ U, we employ a method from Redelmeier (2014), mentioned in the proof
v0,x0,n,k v 0′,x0,n,k 0 0
outline(AppendixD.8.1)andintroducedinAppendixD.8.6,whichfacilitatescomputingexpectedtracesofrandommatrix
34ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
productsthroughthetopologicalconceptofgenusexpansion. Foreaseofexposition,wedeferthesebounds,withwhichwe
upperboundVar(Y ),toAppendicesD.8.5.1andD.8.5.2below. Specifically,Propositions4and5thereinshowthat:
x0,n,k
(4H −1)!! (cid:0) (cid:1) (4H −1)!!
Var(Z )≤ , Cov Z ,Z ≤ ,
v0,x0,n,k D2 v0,x0,n,k v 0′,x0,n,k D3
forallv ̸=v′ ∈U,whereN!!:=N(N −2)(N −4)···3isthedoublefactorialofanoddN ∈N. Theaboveimply:
0 0
(cid:18) (cid:19)
1 (cid:88) (cid:88) (cid:0) (cid:1)
Var(Y )= Var(Z )+ Cov Z ,Z
x0,n,k |U|2 v0∈U v0,x0,n,k v0̸=v 0′∈U v0,x0,n,k v 0′,x0,n,k
(cid:32) (cid:33)
1 |U|(4H −1)!! |U|2(4H −1)!!
≤ +
|U|2 D2 D3
2(4H −1)!!
≤ .
|U|D2
Thus,sinceE[Y ]≥1/D,Chebyshev’sinequalitygives:
x0,n,k
(cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:12) (cid:12) 1 8(4H −1)!!
Pr Y
x0,n,k
≤
2D
≤Pr (cid:12)Y x0,n,k−E[Y x0,n,k](cid:12)≥
2D
≤
|U|
.
Applyingaunionboundoverall|S|H(H −1)/2possibleoptionsforx ∈ S,n ∈ [H −1],k ∈ {0}∪[H −n−1]we
0
arriveat:
(cid:18) (cid:19)
1 4|S|H(H −1)(4H −1)!!
Pr ∃x ∈S,n∈[H −1],k ∈{0}∪[H −n−1]: Y ≤ ≤ .
0 x0,n,k 2D |U|
Since|U|=D−|S|,combinedwithEquation(34)theaboveimpliesthatwithprobabilityatleast1− 4|S|H(H−1)(4H−1)!!:
D−|S|
H−1H−n−1
(cid:68) (cid:69) 4 (cid:88) (cid:88) (cid:88) 4 |S|H(H −1) 1 H(H −1)
∇E (K(1)),∇J(K(1);S) = Y ≥ · · = . (35)
opt |S| x0,n,k |S| 2 2D D
x0∈S n=1 k=0
With the lower bound on
(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)
in place, we turn our attention to establishing that, with high
opt
probability,apolicygradientiterationreducestheoptimalityextrapolationmeasure. ByLemma9,E (·)is2-smooth.
opt
Thus:
E (K(2))≤E (K(1))+(cid:10) ∇E (K(1)),K(2)−K(1)(cid:11) +∥K(2)−K(1)∥2
opt opt opt
=E (K(1))−η·(cid:10) ∇E (K(1)),∇J(K(1);S)(cid:11) +η2·∥∇J(K(1);S)∥2.
opt opt
Ascanbeseenfromtheequationabove,asidefromthelowerboundon(cid:10)
∇E
(K(1)),∇J(K(1);S)(cid:11)
,toshowthatthe
opt
optimalitymeasuredecreasesitisnecessarytoupperbound∥∇J(K(1);S)∥2.Todoso,wecanuseLemma12andMarkov’s
inequality:
(cid:16) (cid:17) 1
Pr ∥∇J(K(1);S)∥2 ≤4DH2(H −1)2(4H −1)!! ≥1− .
D
TogetherwithEquation(35),wehavethatwithprobabilityatleast1− 4|S|H(H−1)(4H−1)!! − 1:
D−|S| D
H(H −1)
E (K(2))≤E (K(1))−η· +η2·4DH2(H −1)2(4H −1)!!.
opt opt D
Sincebyassumptionη ≤ 1 :
8D2H(H−1)(4H−1)!!
1 H(H −1)
η2·4DH2(H −1)2(4H −1)!!≤ η· ,
2 D
35ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
fromwhichitfollowsthat,withprobabilityatleast1− 4|S|H(H−1)(4H−1)!! − 1:
D−|S| D
H(H −1)
E (K(2))≤E (K(1))−η· . (36)
opt opt 2D
Now,recallthatK(1) =0,andsoE (K(1))=E (K )= 1 (cid:80) ∥Av ∥2. Weclaimthatwithhighprobability
opt opt no-ext |U| v0∈U 0
E (K ) ≤ 2. Indeed, sinceU isanorthonormalsetofvectors, {Av : v ∈ U}isasetofindependentrandom
opt no-ext 0 0
variables. Furthermore,theentriesofAv ,forv ∈U,aredistributedindependentlyaccordingtoaGaussiandistribution
0√ 0
withmeanzeroandstandarddeviation1/ D. Hence,Lemma7impliesthatwithprobabilityatleast1− 2 :
D(D−|S|)
E (K(1))=E (K )= 1 (cid:88) ∥Av ∥2 ≤2.
opt opt no-ext |U| v0∈U 0
DividingbothsidesofEquation(36)byE (K ),andapplyingaunionbound,wegetthatwithprobabilityatleast
opt no-ext
1− 4|S|H(H−1)(4H−1)!! − 1 − 2 :
D−|S| D D(D−|S|)
E (K(2)) H(H −1)
opt ≤1−η· .
E (K ) 4D
opt no-ext
Finally,noticethat:
1 1 |S|H(H −1)(4H −1)!! 2 |S|H(H −1)(4H −1)!!
≤ ≤ , ≤ ,
D D−|S| D−|S| D(D−|S|) D−|S|
andthereforetheupperboundaboveholdswithprobabilityatleast1− 6|S|H(H−1)(4H−1)!!. Restatingitintermsofafixed
D−|S|
failureprobabilityδ ∈(0,1),weconcludethatifD ≥|S|+ 6|S|H(H−1)(4H−1)!!,thenwithprobabilityofatleast1−δ:
δ
E (K(2)) H(H −1)
opt ≤1−η· .
E (K ) 4D
opt no-ext
D.8.5.1 UpperBoundonVar(Z )
v0,x0,n,k
Proposition4. Foranyx ∈S,v ∈U,n∈∪[H −1],k ∈{0}∪[H −n−1]:
0 0
(4H −1)!!
Var(Z )≤ ,
v0,x0,n,k D2
whereZ isasdefinedinEquation(33).
v0,x0,n,k
Proof. SinceVar(Z ) = E(cid:2) Z2 (cid:3) −E(cid:2) Z (cid:3)2 ≤ E(cid:2) Z2 (cid:3) ,itsufficestoupperboundthesecond
momentE(cid:2) Z2
v0 (cid:3), ,x w0,n h, ik chupholdv s:0,x0,n,k v0,x0,n,k v0,x0,n,k
v0,x0,n,k
E(cid:2) Z2 (cid:3) =E (cid:104) Tr(cid:0) (An)⊤v x⊤(cid:1)2 ·Tr(cid:0) (An+k+1)⊤Ak+1v x⊤(cid:1)2(cid:105) .
v0,x0,n,k A 0 0 0 0
Letp := 2(n+k+1) ≤ 2H. ToshowthatE(cid:2) Z2 (cid:3) ≤ (p−1)!! weemploythemethodfromRedelmeier(2014),
v0,x0,n,k D2
whichisbasedonthetopologicalconceptofgenusexpansion. Forcompleteness,AppendixD.8.6providesaself-contained
introductiontothemethod,andTheorem2thereinlaysouttheresultwhichwewilluse. Weassumebelowfamiliaritywith
thenotationandconceptsdetailedinAppendixD.8.6.
ForinvokingTheorem2,letusdefineapermutationγ over[p]viathecycledecomposition:
γ =(1,...,n)(n+1,...,p)(p+1,...,p+n)(p+n+1,...,2p),
andamappingϵ:[2p]→{−1,1}by:
ϵ(1)=−1,...,ϵ(2n+k+1)=−1,
36ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
ϵ(2n+k+2)=1,...,ϵ(p)=1,
ϵ(p+1)=−1,...,ϵ(p+2n+k+1)=−1,
ϵ(p+2n+k+2)=1,...,ϵ(2p)=1.
Additionally,defineC ,...,C ∈RD×D asfollows:
1 2p
C =I,...,C =I,C =v x⊤,
1 n−1 n 0 0
C =I,...,C =I,C =v x⊤,
n+1 p−1 p 0 0
C =I,...,C =I,C =v x⊤,
p+1 p+n−1 p+n 0 0
C =I,...,C =I,C =v x⊤,
p+n+1 2p−1 2p 0 0
whereIistheidentitymatrix. Fortheabovechoiceofγ,ϵ,andmatricesC ,...,C itholdsthat:
1 2p
E (cid:2) Tr (cid:0) A C ,...,A C (cid:1)(cid:3) =E (cid:104) Tr(cid:0) (An)⊤v x⊤(cid:1)2 ·Tr(cid:0) (An+k+1)⊤Ak+1v x⊤(cid:1)2(cid:105) =E(cid:2) Z2 (cid:3) .
A γ ϵ(1) 1 ϵ(2p) 2p A 0 0 0 0 v0,x0,n,k
InvokingTheorem2,wemaywriteEquation(39)(fromTheorem2ofAppendixD.8.6)as:
E(cid:2) Z2 (cid:3) =E (cid:2) Tr (cid:0) A C ,...,A C (cid:1)(cid:3) = (cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ r (C ,...,C ).
v0,x0,n,k A γ ϵ(1) 1 ϵ(2p) 2p γ−−1δϵπδϵγ+ 1 2p
π∈{ρδρ:ρ∈M2p} 2
Noticethat,duetothechoiceofC ,...,C ,eachsummandontherighthandsideisnon-negative. Weclaimthatfora
1 2p
non-zerosummandcorrespondingtoπitnecessarilyholdsthatχ(γ,δ πδ )≤4. Meaning:
ϵ ϵ
E(cid:2) Z2 (cid:3) = (cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ r (C ,...,C ).
v0,x0,n,k γ−−1δϵπδϵγ+ 1 2p
π∈{ρδρ:ρ∈M2p},χ(γ,δϵπδϵ)≤4 2
Toseewhythisisthecase,notethat|γ|=4. ItfollowsthatthecorrespondingsurfacesdescribedinAppendixD.8.6are
obtainedbygluingfourfaces. Furthermore,because:
TĎ r(cid:0)
v
x⊤(cid:1) =TĎ r(cid:0)
x
v⊤(cid:1) =TĎ r(cid:16)(cid:0)
v
x⊤(cid:1)2(cid:17) =TĎ r(cid:16)(cid:0)
x
v⊤(cid:1)2(cid:17)
=0,
0 0 0 0 0 0 0 0
TĎ r(cid:0) x v⊤v x⊤(cid:1) =TĎ r(cid:0) v x⊤x v⊤(cid:1) = 1 ,
0 0 0 0 0 0 0 0 D
theonlywayasummandcorrespondingtoπ =ρδρcanbenon-zeroisifacycleR=(1,...,R)ofγ−1δ πδ γ /2contains
Ď + ϵ ϵ −
either no non-identity matrices, in which case Tr(C ,...,C ) = 1, or if it contains two or four such matrices, with
1 R
Ď
non-identitymatricesappearingoncetransposedandoncewithouttransposition,inwhichcaseTr(C ,...,C )=1/D.
1 R
Thus,thefournon-identitymatricesamongC ,...,C mustappearineitheroneortwodifferentcycles. Accordingly,
1 2p
togetanon-zerocontribution,δ πδ musteitherconnectallfourfacesorconnecttwopairsamongthem,i.e.thesurface
ϵ ϵ
G(γ,ϵ,ρ)musthaveeitheroneortwoconnectedcomponents(seeconstructioninAppendixD.8.6). ByProposition6in
AppendixD.8.6,theEulercharacteristicofsuchasurfacesatisfiesχ(G(γ,ϵ,ρ))≤4.
Overall,wehaveestablishedthat:
E(cid:2) Z2 (cid:3) = (cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ r (C ,...,C ).
v0,x0,n,k γ−−1δϵπδϵγ+ 1 2p
π∈{ρδρ:ρ∈M2p},χ(γ,δϵπδϵ)≤4 2
Toconcludetheproof,weshowthateachsummandontherighthandsidecontributesatmost1/D2. Letusexamineall
possiblecasesforπ =ρδρ. Ifallnon-identitymatricesareinasinglecycleofγ−1δ πδ γ /2,then:
+ ϵ ϵ −
Ď 1
Tr (C ,...,C )≤ ,
γ−−1δϵπδϵγ+ 1 2p D
2
andthesurfaceG(γ,ϵ,ρ)isconnected,soχ(G(γ,ϵ,ρ))≤2andthesummandcorrespondingtoπisatmost1/D3. Onthe
otherhand,iftherearetwocyclescontainingnon-identitymatrices,then:
Ď 1
Tr (C ,...,C )≤ ,
γ−−1δϵπδϵγ+ 1 2p D2
2
37ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
andχ(G(γ,ϵ,ρ))≤4,sothesummandcorrespondingtoπisatmost1/D2. Asweshowedabove,thesearetheonlycases
whichgiveanon-zerocontribution. Hence:
E(cid:2) Z2 (cid:3) = (cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ r (C ,...,C )
v0,x0,n,k γ−−1δϵπδϵγ+ 1 2p
π∈{ρδρ:ρ∈M2p},χ(γ,δϵπδϵ)≤4 2
(cid:88) 1
≤
D2
π∈{ρδρ:ρ∈M2p},χ(γ,δϵπδϵ)≤4
(2p−1)!!
≤ ,
D2
wherethelasttransitionisbythenumberofpairingsof[2p]beingequalto|M |=(2p−1)!!. Theproofconcludesby
2p
noticingthat2p=4(n+k+1)≤4H.
D.8.5.2 UpperBoundonCov(Z ,Z )
v0,x0,n,k v 0′,x0,n,k
Proposition5. Foranyx ∈S,v ,v′ ∈U,n∈∪[H −1],k ∈{0}∪[H −n−1]withv ̸=v′:
0 0 0 0 0
(cid:0) (cid:1) (4H −1)!!
Cov Z ,Z ≤ ,
v0,x0,n,k v 0′,x0,n,k D3
whereZ isasdefinedinEquation(33).
v0,x0,n,k
(cid:0) (cid:1) (cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
Proof. NotethatCov Z ,Z =E Z Z −E Z E Z ,where:
v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k
(cid:2) (cid:3)
E Z ,Z
v0,x0,n,k v 0′,x0,n,k
=E (cid:2) Tr(cid:0) (An)⊤v x⊤(cid:1) Tr(cid:0) (An+k+1)⊤Ak+1v x⊤(cid:1) Tr(cid:0) (An)⊤v′x⊤(cid:1) Tr(cid:0) (An+k+1)⊤Ak+1v′x⊤(cid:1)(cid:3) ,
A 0 0 0 0 0 0 0 0
(cid:2) (cid:3) (cid:2) (cid:3)
E Z E Z
v0,x0,n,k v 0′,x0,n,k
=E (cid:2) Tr(cid:0) (An)⊤v x⊤(cid:1) Tr(cid:0) (An+k+1)⊤Ak+1v x⊤(cid:1)(cid:3) E (cid:2) Tr(cid:0) (An)⊤v′x⊤(cid:1) Tr(cid:0) (An+k+1)⊤Ak+1v′x⊤(cid:1)(cid:3) .
A 0 0 0 0 A 0 0 0 0
(cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
Letp := 2(n+k+1) ≤ 2H. WewillshowthatbothE Z Z andE Z E Z canbe
v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k
writtenasasum,inwhicheachsummandisatmost1/D2 andthecoefficientcorrespondingto1/D2 isthesame. Asa
result,thiswillleadtoanupperboundonthecovariancethatdependson1/D3.
(cid:2) (cid:3) (cid:2) (cid:3)
WefirstexamineE Z . TheanalysisbelowappliesequallytoE Z aswell(notethatbyLemma10of
v0,x0,n,k
(cid:2) (cid:3) (cid:2) (cid:3)
v 0′,x0,n,k
AppendixD.8.3weknowthatE Z =E Z ). Below,wemakeuseofthemethodfromRedelmeier(2014),
v0,x0,n,k v 0′,x0,n,k
whichisbasedonthetopologicalconceptofgenusexpansion. Forcompleteness,AppendixD.8.6providesaself-contained
introductiontothemethod,andTheorem2thereinlaysouttheresultwhichwewilluse. Weassumefamiliaritywiththe
notationandconceptsdetailedinAppendixD.8.6.
ForinvokingTheorem2,letusdefineapermutationγ over[p]viathecycledecomposition:
γ =(1,...,m+n)(m+n+1,...,p),
andamappingϵ:[2p]→{−1,1}by:
ϵ(1)=−1,...,ϵ(2n+k+1)=−1,
ϵ(2n+k+2)=1,...,ϵ(p)=1.
Additionally,defineC ,...,C ∈RD×D asfollows:
1 p
C =I,...,C =I,C =v x⊤,
1 n−1 n 0 0
C =I,...,C =I,C =v x⊤,
n+1 p−1 p 0 0
whereIistheidentitymatrix. Fortheabovechoiceofγ,ϵ,andmatricesC ,...,C itholdsthat:
1 p
E (cid:2) Tr (cid:0) A C ,...,A C (cid:1)(cid:3) =E (cid:2) Tr(cid:0) (An)⊤v x⊤(cid:1) Tr(cid:0) (An+k+1)⊤Ak+1v x⊤(cid:1)(cid:3) =E[Z ].
A γ ϵ(1) 1 ϵ(p) p A 0 0 0 0 v0,x0,n,k
38ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
InvokingTheorem2,wemaywriteEquation(39)(fromTheorem2ofAppendixD.8.6)as:
E[Z ]=E
(cid:2)
Tr
(cid:0)
A C ,...,A C
(cid:1)(cid:3)
=
(cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ
r (C ,...,C ). (37)
v0,x0,n,k A γ ϵ(1) 1 ϵ(p) p γ−−1δϵπδϵγ+ 1 p
π∈{ρδρ:ρ∈Mp} 2
Weclaimthat,foranyπcorrespondingtoasummandontherighthandsideoftheequationaboveeither
Ď
Tr (C ,...,C )=0
γ−−1δϵπδϵγ+ 1 p
2
or
Ď 1
Tr (C ,...,C )= .
γ−−1δϵπδϵγ+ 1 p D
2
Toseeitisso,noticethatifγ−1δ πδ γ /2comprisesacyclecontainingthetwonon-identitymatrices,appearingonce
+ ϵ ϵ −
transposedandoncewithouttransposition,thenthenormalizedtraceforthatcycleisequalto1/Dandthenormalizedtrace
fortheremainingcycleis1. Otherwise,oneofthenormalizedtracesforacycleofγ−1δ πδ γ /2isequaltozero. Hence,
+ ϵ ϵ −
foreachsummandontherighthandsideofEquation(37)correspondingtoπ =ρδρ,whosecontributionisnon-zero,the
twofacesofG(γ,ϵ,ρ)(seeconstructioninAppendixD.8.6)inducedbyγ areconnected. ByProposition6,forsuchπwe
getχ(γ,δ πδ )−|γ|=χ(γ,δ πδ )−2≤D0 =1.
ϵ ϵ ϵ ϵ
Overall,theaboveimpliesthateachnon-zerosummandintheexpressionforE[Z ],outofthe|M |=(p−1)!!
v0,x0,n,k
(cid:2)
(cid:3)p
summands, is upper bounded by 1/D. Since, as mentioned above, the same holds for E Z , we get that
E[Z ]E(cid:2) Z (cid:3) can be represented as a sum in which each term is upper
boundedv 0′ b,x y0, 1n /,k
D3 or is equal
v0,x0,n,k v 0′,x0,n,k
to 1/D2. What remains is to show that E[Z Z ] can be written as a sum of (2p−1)!! terms, each up-
v0,x0,n,k v 0′,x0,n,k
per bounded by 1/D3 or equal to 1/D2. Fortunately, we will see that terms equal to 1/D2 cancel out with those of
E[Z ]E[Z ],andasaresultCov(Z ,Z )isupperboundedby(2p−1)!!/D3.
v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k
Fori ∈ Z,letusdenotebyc (cid:0) E[Z Z ](cid:1) andc (cid:0) E[Z ]E[Z ](cid:1) thecoefficientsofDi inthe
i v0,x0,n,k v 0′,x0,n,k i v0,x0,n,k v 0′,x0,n,k
respectiveexpressions. Accordingtothediscussionabove,weneedonlyshowthat:
(cid:0) (cid:1) (cid:0) (cid:1)
c E[Z Z ] =c E[Z ]E[Z ] ,
−2 v0,x0,n,k v 0′,x0,n,k −2 v0,x0,n,k v 0′,x0,n,k
andthatforalli≥−1:
(cid:0) (cid:1)
c E[Z Z ] =0.
i v0,x0,n,k v 0′,x0,n,k
WeapplyagainthemethodfromRedelmeier(2014). Inparticular,weinvokeTheorem2bydefiningthepermutationγ over
[2p]viathecycledecomposition:
γ =(1,...,n)(n+1,...,p)(p+1,...,p+n)(p+n+1,...,2p),
andamappingϵ:[2p]→{−1,1}by:
ϵ(1)=−1,...,ϵ(2n+k+1)=−1,
ϵ(2n+k+2)=1,...,ϵ(p)=1,
ϵ(p+1)=−1,...,ϵ(p+2n+k+1)=−1,
ϵ(p+2n+k+2)=1,...,ϵ(2p)=1.
Furthermore,defineC ,...,C ∈RD×D asfollows:
1 2p
C =I,...,C =I,C =v x⊤,
1 n−1 n 0 0
C =I,...,C =I,C =v x⊤,
n+1 p−1 p 0 0
C =I,...,C =I,C =v′x⊤,
p+1 p+n−1 p+n 0 0
C =I,...,C =I,C =v′x⊤,
p+n+1 2p−1 2p 0 0
39ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
whereIistheidentitymatrix. Fortheabovechoiceofγ,ϵ,andmatricesC ,...,C itholdsthat:
1 2p
(cid:2) (cid:0) (cid:1)(cid:3) (cid:2) (cid:3)
E Tr A C ,...,A C =E Z Z .
A γ ϵ(1) 1 ϵ(2p) 2p v0,x0,n,k v 0′,x0,n,k
Thus,invokingTheorem2leadsto:
E(cid:2)
Z v0,x0,n,kZ
v
0′,x0,n,k(cid:3)
=
(cid:88) Dχ(γ,δϵπδϵ)−|γ|·TĎ
r γ−−1δϵπδϵγ+(C 1,...,C 2p). (38)
π∈{ρδρ:ρ∈M2p} 2
Notice that, due to the choice of C ,...,C , each summand on the right hand side is non-negative. Specifically, the
1 2p
normalizedtracesfordifferentcombinationofthenon-identitymatricesamongC ,...,C satisfy:
1 2p
TĎ r(cid:0)
x
v⊤(cid:1) =TĎ r(cid:0)
v
x⊤(cid:1) =TĎ r(cid:0)
x
v′⊤(cid:1) =(cid:0) v′x⊤(cid:1)
=0,
0 0 0 0 0 0 0 0
TĎ r(cid:16)(cid:0) x v⊤(cid:1)2(cid:17) =TĎ r(cid:16)(cid:0) v x⊤(cid:1)2(cid:17) =TĎ r(cid:16)(cid:0) x v′⊤(cid:1)2(cid:17) =Tr(cid:16)(cid:0) v′x⊤(cid:1)2(cid:17) =TĎ r(cid:0) v x⊤x v′⊤(cid:1) =Tr(cid:0) x v⊤v′x⊤(cid:1) =0,
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
TĎ r(cid:0) v x⊤x v⊤(cid:1) =TĎ r(cid:0) x v⊤v x⊤(cid:1) =TĎ r(cid:0) v′x⊤x v′⊤(cid:1) =Tr(cid:0) x v′⊤v′x⊤(cid:1) = 1 .
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 D
Now,forπ =ρδρcorrespondingtoasummandontherighthandsideofEquation(38),letF ,F ,F ,F bethefacesin
1 2 3 4
G(γ,ϵ,ρ)(seeconstructioninAppendixD.8.6),orderedaccordingtotheirappearanceinγ. Itfollowsthatforthesummand
tobenon-zerothereareonlytwooptions: eitherallfourfacesF ,F ,F ,F areconnected,meaningallfournon-identity
1 2 3 4
matricesareinthesamecycleofγ−1δ πδ γ /2,orF isconnectedtoF andF toF . Anyothersummandwillcontribute
+ ϵ ϵ − 1 2 3 4
zeroduetothetraceidentitiesabove. Weclaimthatthefirstoptiongivesacontributionoforder1/D3. Indeed,forsuch
π =ρδρwehavethat:
Ď 1
Tr (C ,...,C )= ,
γ−−1δϵπδϵγ+ 1 2p D
2
andDχ(γ,δϵπδϵ)−|γ| ≤ D−2 byProposition6(recall|γ| = 4). Asforthesecondoption, notethatanyσ := δ ϵπδ
ϵ
that
connectsF toF andF toF canbefactorizedintoσ =σ σ ,whereσ andσ aretherestrictionsofσtotheelements
1 2 3 4 1 2 1 2
ofF ∪F andF ∪F ,respectively. Wemaysimilarlyfactorizeγ asγ = γ γ . Itfollowsthatthecontributionofthis
1 2 3 4 1 2
summandfactorizesas:
Dχ(γ,σ)−|γ|·TĎ
r (C ,...,C )
γ−−1σγ+ 1 2p
2
(cid:18) (cid:19)(cid:18) (cid:19)
=
Dχ(γ1,σ1)−|γ1|·TĎ
r (C ,...,C )
Dχ(γ2,σ2)−|γ2|·TĎ
r (C ,...,C ) .
γ1− ,−1σ1γ1,+ 1 p γ2− ,−1σ2γ2,+ p+1 2p
2 2
ThisfactorizationcorrespondspreciselytoatermintheexpansionofE[Z ]E[Z ],andvice-versa. Because
v0,x0,n,k v 0′,x0,n,k
allsummandswhichgiveacontributionof1/D2havethisform,wegetthat:
(cid:0) (cid:1) (cid:0) (cid:1)
c E[Z Z ] =c E[Z ]E[Z ] .
−2 v0,x0,n,k v 0′,x0,n,k −2 v0,x0,n,k v 0′,x0,n,k
Toconclude,wehaveshownthatbothE[Z Z ]andE[Z ]E[Z ]canberepresentedasasum
v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k
of non-negative terms, each upper bounded by 1/D3 or equal to 1/D2. Furthermore, the terms equal to 1/D2 are the
(cid:0) (cid:1)
same,forbothE[Z Z ]andE[Z ]E[Z ],andsocanceloutinCov Z ,Z .
v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k v0,x0,n,k v 0′,x0,n,k
Consequently,thecovariancecanbeupperboundedbyasumofatmost|M | = (2p−1)!! ≤ (4H −1)!!terms,each
2p
upperboundedby1/D3. Thus:
(cid:0) (cid:1) (4H −1)!!
Cov Z ,Z ≤ .
v0,x0,n,k v 0′,x0,n,k D3
D.8.6 GENUSEXPANSIONOFGAUSSIANMATRICES
Inthisappendix,weintroducetheconceptofagenusexpansion—aprooftechniquefromrandommatrixtheory,whereby
one expresses the traces of random matrix products as a sum over topological spaces. Specifically, we adapt a result
fromRedelmeier(2014)thatisusedforboundingcertainquantitiesinAppendicesD.8.4andD.8.5.
40ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Additionalnotation. Werequirethefollowingnotation,whichisanadaptationofthatusedinRedelmeier(2014). Given
matricesC ,...,C ∈ RD×D,wedenoteC := C⊤ forn ∈ [N]. ForC ∈ RD×D,weletTĎ r(C) = 1 Tr(C)beits
1 N −n n D
normalizedtrace. WedenotebyM thesetofallpairingsof[N],i.e.thesetofallpermutationswhichhaveN/2cyclesof
N
length2(notethatifN isoddthenthissetisempty). Forapermutationγ :[N]→[N],wedenoteby|γ|thenumberof
cyclesinitscycledecomposition. Lastly,weuseδ :{−N,...,−1,1,...,N}→{−N,...,−1,1,...,N}todenotethe
mappingsatisfyingδ(n)=−n.
TowardsadaptingtheresultofRedelmeier(2014),welayoutseveralpreliminarydefinitions.
Definition 6. For a subset I ⊆ [N], let γ : I → I be a permutation given by the following cycle decomposition:
γ =(z ,...,z )(z ,...,z )···(z ,...,z ),wherez ,...,z ∈I denotetheelementsofI. Wedefineγ
1 n1 n1+1 n2 nk−1+1 nk 1 nk +
tobethepermutationon{−N,...,−1,1,...,N}thatextendsγbyactingastheidentityfori∈/ I. Additionally,wedefine
γ :=δγ δ.
− +
Notethatγ isapermutationwithcycledecomposition:
−
γ =(−z ,...,−z )(−z ,...,−z )···(−z ,...,−z ).
− 1 n1 n1+1 n2 nz−1+1 nk
Definition7. Forasetofnon-zerointegersI ⊂ Z,apermutationπ onI ∪−I,where−I := {−i : i ∈ I},iscalleda
premapifδπδ =π−1andnocycleofπcontainsbothiand−i,foranyi∈I.
Definition 8. For a subset I ⊆ {−N,...,−1,1,...,N}, let γ be a premap given by the cycle decomposition γ =
(z ,...,z )(z ,...,z )···(z ,...,z ). Wedefinethepermutation γ overI asfollows. Foreachcycleofγ,
1 n1 n1+1 n2 nk−1+1 nk 2
ifitssmallestelementinabsolutevalueispositive,thenthecycleisleftunchanged. Otherwise,thecycleisremoved,i.e. γ
2
actsastheidentityfortheelementsintheremovedcycle.
Definition 9. For a subset I ⊆ {−N,...,−1,1,...,N}, let γ be a premap given by the cycle decomposition
γ = (z ,...,z )(z ,...,z )···(z ,...,z ) and C ,...,C ∈ RD×D. We define the trace along γ of
1 n1 n1+1 n2 nk−1+1 nk 1 N
C ,...,C tobe:
1 N
Tr (C ,...,C ):=Tr(C ·····C )·Tr(C ·····C )·····Tr(C ·····C ).
γ 1 N z1 zn1 zn1+1 zn2 zk−1+1 nk
Analogously,wedefinethenormalizedtracealongγ tobe:
Ď Ď Ď Ď
Tr (C ,...,C ):=Tr(C ·····C )·Tr(C ·····C )·····Tr(C ·····C ).
γ 1 N z1 zn1 zn1+1 zn2 zk−1+1 nk
Definition10. LetI ⊂Zbeasetofintegerswhichdoesnotcontainbothiand−i,foranyintegeri∈Z. Furthermore,let
γ beapermutationonI andπapremaponI∪−I :={−i:i∈I}. TheEulercharacteristicof(γ,π)isdefinedby:
χ(γ,π):=(cid:12) (cid:12) (cid:12)γ +−1γ −(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)π(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)γ +−1π−1γ −(cid:12) (cid:12) (cid:12)−(cid:12) (cid:12) (cid:12)I(cid:12) (cid:12)
(cid:12).
(cid:12) 2 (cid:12) (cid:12)2(cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12)
Withthedefinitionsaboveinplace,wearenowinapositiontoimporttheresultofRedelmeier(2014),which,forrandom
Gaussianmatrices,providesaformulafortheexpectationofthenormalizedtracealongapermutationγ.
Theorem2(AdaptationofLemma3.8inRedelmeier(2014)). Letγ beapermutationover[N],andϵ:[N]→{−1,1}.
Furthermore, supposethattheentriesofA ∈ RD×D aresampledindependentlyfromaGaussianwithmeanzeroand
√
standarddeviation1/ D,andC ,...,C ∈RD×D aresomefixed(non-random)matrices. Then:
1 N
E
(cid:2) TĎ
r
(cid:0)
A C ,...,A C
(cid:1)(cid:3)
=
(cid:88) Dχ(γ,δϵπδϵ)−2|γ|·TĎ
r (C ,...,C ), (39)
A γ ϵ(1) 1 ϵ(N) N γ−−1δϵπδϵγ+ 1 N
π∈{ρδρ:ρ∈MN} 2
where δ is a mapping on {−N,...,−1,1,...,N} defined by δ : k (cid:55)→ ϵ(k)k, and we extend ϵ to {−N,...,−1}
ϵ ϵ
symmetrically,i.e.bysettingϵ(k)=ϵ(−k).
To obtain explicit bounds over expected traces along a permutation, based on Theorem 2, we need to bound the Euler
characteristicχ. Forthatpurpose,Redelmeier(2014)makesuseofatopologicalinterpretationofχviatheconceptof
genusexpansion. Specifically,asweshowbelow,eachsummandontherighthandsideofEquation(39)correspondstoa
two-dimensionalsurfacewhosetopologicalpropertiesdeterminethesizeofthesummand. Wefirstgivesomenecessary
backgroundfromtopology.
41ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Definition11. TheEulercharacteristicofasurfaceG isdefinedby:
χ(G):=V(G)+F(G)−E(G),
where V,F,E are the number of vertices, faces, and edges of G, respectively. Strictly speaking, χ(G) is calculated by
constructingaCWcomplexwhichishomeomorphictoG anddeterminingV(G),F(G),andE(G)throughit. Abasic
theoremintopologyshowsthatV(G),F(G),andE(G)areinvariantunderhomotopy,andsothechoiceofCWcomplex
doesnotmatter(cf.Munkres(2018)).
ThefollowingpropositionestablishesbasicpropertiesoftheEulercharacteristicofsurfaces.
Proposition6. ForasurfaceG,theEulercharactersiticχsatisfies:
• ifG hasm∈NconnectedcomponentsG ,...,G ,thenχ(G)=χ(G )+···+χ(G );
1 m 1 m
• andaconnectedsurfaceG satisfiesχ(G)≤2,withequalityholdingifandonlyifG ishomeomorphictoasphere.
Proof. Thesearebasicpropertiesfromthefieldoftopology—seeMunkres(2018).
Now,givenapermutationγon[N],afunctionϵ:[N]→{−1,1},andapairingρ∈M ,weconstructasurfaceG(γ,ϵ,ρ),
N
whosepropertieswillthendeterminethecorrespondingterminthesumofEquation(39).
Let G(γ,ϵ,ρ) be the following (perhaps disconnected) two dimensional surface. Each cycle (z ,...,z ) of γ is asso-
1 m
ciated with the front of an m-gon. The back of this m-gon is associated with the corresponding cycle of γ , i.e. with
−
(−z ,...,−z ). The m-gon will serve as one of the faces of G(γ,ϵ,ρ). For orienting the edges of the face defined
1 m
above,ifC istransposed,i.e.ϵ(z )=−1,thecorrespondingedgeisorientedclockwise,andotherwiseitisoriented
ϵ(zj) j
counterclockwise. AteachvertexofthefaceweplacethematrixC . Wenowconnectfacesdefinedbydifferentcycles
zj
accordingtothefollowingprocedure. Letσ := δ ρδρδ ,whichisapairingof{−N,...,−1,1,...,N}. Foreverypair
ϵ ϵ
(n,σ(n)),wheren∈{−N,...,−1,1,...,N},weglueedgentoσ(n)accordingtotheirrespectiveorientations(where
thesignsof(n,σ(n))determinewhetherwefliptheseorientations,i.e.gluethefrontsorbacksofeachedge). Overall,we
obtainasurfaceG(γ,ϵ,ρ)fromthesegluedfaces.
Finally,Proposition7establishesthattheEulercharacteristicofG(γ,ϵ,ρ)(Definition11),constructedabove,isequaltothe
Eulercharacteristicof(γ,δ ρδρδ )(Definition10).
ϵ ϵ
Proposition7. Givenapermutationγ on[N],afunctionϵ : [N] → {−1,1},andapairingρ ∈ M ,letσ := δ ρδρδ .
N ϵ ϵ
ForthesurfaceG(γ,ϵ,ρ)constructedasspecifiedabove,itholdsthatχ(γ,σ)=χ(G(γ,ϵ,ρ)).
Proof. Recallthatχ(γ,σ)isgivenby(cf.Definition10):
(cid:12) (cid:12) (cid:12)γ +−1γ −(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)σ(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)γ +−1σγ −(cid:12) (cid:12)
(cid:12)−N,
(cid:12) 2 (cid:12) (cid:12)2(cid:12) (cid:12) 2 (cid:12)
andtheEulercharacteristicofG(γ,ϵ,ρ)isgivenby(cf.Definition11):
V(G(γ,ϵ,ρ))+F(G(γ,ϵ,ρ))−E(G(γ,ϵ,ρ)).
Thusitsufficestoshowthatthefollowinghold:
(cid:12) (cid:12) (cid:12)γ +−1γ −(cid:12) (cid:12)
(cid:12)=F(G(γ,ϵ,ρ)) ,
(cid:12) (cid:12) (cid:12)σ(cid:12) (cid:12)
(cid:12)−N =−E(G(γ,ϵ,ρ)) ,
(cid:12) (cid:12) (cid:12)γ +−1σγ −(cid:12) (cid:12)
(cid:12)=V(G(γ,ϵ,ρ)).
(cid:12) 2 (cid:12) (cid:12)2(cid:12) (cid:12) 2 (cid:12)
Thefirstequality(left)followsimmediatelyfromthefactthat|γ +−1γ−|=|γ|,andtheconstructionofG(γ,ϵ,ρ).
Asforthe
2
secondequality(middle),sinceσisapremap,whichisapairingonadomainofsize2N,wehavethat|σ|= |σ| = N. On
2 2 2
theotherhand,byconstructionG(γ,ϵ,ρ)has N edges. Thethirdequality(right)reliesonageneralizationofLemma13.5
2
fromKemp(2013)toaccountfornon-orientablegluings. Specifically,theverticesofG(γ,ϵ,ρ)correspondtothecyclesof
γ −− 11σγ+,i.e.eachvertexofG(γ,ϵ,ρ)correspondstothegluingoftheverticesinsomecycleof γ −− 11σγ+.
Notethatσ,and
2 2
thereforeγ−1σγ ,arepremaps. Thusthedivisionby2leavesuswithapermutationthatactsonasetcontainingexactly
−1 +
oneof{−n,n},foreachn∈[N]. Thiscorrespondstothechoicewhethertoglueeachedgeofthepolygonsfromthefront
ortheback.
42ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
E FurtherExperimentsandImplementationDetails
E.1 FurtherExperimentsWithUnderdeterminedLQRProblems
Figures4and5supplementFigure2(fromSection5.1)byincludinganalogousexperimentswith,respectively: (i)alonger
timehorizonH =8(insteadofH =5);and(ii)randomBandpositivesemidefiniteQmatrices(insteadofB=Q=I).
E.2 FurtherExperimentsWithNeuralNetworkControllersinNon-LinearSystems
Forthequadcoptercontrolproblem,Figures8and10supplementFigure3bydemonstratingthat,respectively: (i)theextent
ofextrapolationvariesdependingonthedistancefrominitialstatesseenintraining;and(ii)extrapolationoccurstoinitial
statesunseenintrainingatdifferenthorizontaldistancesfromtheinitialstatesseenintraining(inadditiontounseeninitial
statesbelowthoseseenintraining).
E.3 FurtherImplementationDetails
We provide implementation details omitted from our experimental reports (Section 5 and Appendices E.1 and E.2).
Source code for reproducing our results and figures, based on the PyTorch (Paszke et al., 2019) framework, can be
foundathttps://github.com/noamrazin/imp_bias_control. TheexperimentswithunderdeterminedLQR
problems (Section 5.1 and Appendix E.1) were carried out on a standard laptop, whereas for experiments with neural
networkcontrollersinnon-linearsystems(Section5.2andAppendixE.2)weusedasingleNvidiaRTX2080TiGPU.
E.3.1 LINEARQUADRATICCONTROL(SECTION5.1)
System. Inallexperiments,exceptforthosewiththe“randomA,B,Q”system(Figure5),wesetB=Q=I∈RD×D.
Initialstates. Wetookthesetofinitialstatesseenintrainingtobethefirstd ∈ [4]standardbasisvectors,andusedthe
remainingstandardbasisvectorsforevaluatingextrapolation.
Optimization. We ran policy gradient over a linear controller for 105 iterations using a learning rate of 10−3. In the
experimentsofFigure2,forallsystemtypes,mediantrainingcostacrossrandomseedswaswithin10−8oftheminimal
possibletrainingcost. IntheexperimentsofFigure4,forallsystemtypeswithH =8,mediantrainingcostwaswithin
2·10−5 oftheminimalpossibletrainingcost. Lastly,intheexperimentsofFigure5,forthe“randomA,B,Q”system
type,mediantrainingcostwaswithin0.02oftheminimalpossibletrainingcost.
E.3.2 THEPENDULUMCONTROLPROBLEM(SECTION5.2)
System. Thetwo-dimensionalstateofthesystemisdescribedbytheverticalangleofthependulumθ ∈Randitsangular
velocityθ˙ ∈R. Attimesteph,thecontrollerappliesatorqueu ∈R,givingrisetothefollowingnon-lineardynamicsfor
h
aunitlengthpendulumwithaunitmassobjectmountedontopofit:
θ =θ +∆·θ˙
h h−1 h−1
, ∀h∈[H], (40)
θ˙ =θ˙ +∆·(cid:0) u −g·sin(θ )(cid:1)
h h−1 h−1 h−1
where∆=0.05isatimediscretizationresolutionandg =10isthegravitationalaccelerationconstant.
Cost. Thegoalofthecontrolleristomakethependulumreachandstayatthetargetstate(π,0). Accordingly,thecostat
eachtimestepisthesquaredEuclideandistancefrom(π,0). Specifically,supposethatwearegivena(finite)setofinitial
statesX ⊂R2. Fora(state-feedback)controllerπ :R2 →R,parameterizedbyw∈RP,thecostisdefinedby:
w
J(w;X):= 1 (cid:88) (cid:88)H (cid:13) (cid:13)(θ ,θ˙ )−(π,0)(cid:13) (cid:13)2 , (41)
H ·|X| (θ0,θ˙ 0)∈X h=0(cid:13) h h (cid:13)
whereθ andθ˙ evolveaccordingtoEquation(40)withu =π (θ ,θ˙ ),forh∈[H]. Inallexperiments,thetime
h h h−1 w h−1 h−1
horizonissettoH =100.
Initial states. For the experiments of Figure 3, Table 1 specifies the initial states used for training and those used for
evaluatingextrapolationtoinitialstatesunseenintraining.
Controllerparameterization. Weparameterizedthecontrollerasafully-connectedneuralnetworkwithReLUactivation.
43ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
The network was of depth 4 and width 50. Parameters were randomly initialized according to the default PyTorch
implementation.
Non-extrapolatingcontroller. Toobtainanon-extrapolatingcontrollerforFigure3, wetrainedthecontrollerusinga
modifiedobjectiveinsteadofthestandardtrainingcost.Inadditiontothecostoverinitialstatesseenintraining,themodified
objectiveincludesan“adversarial”costtermoverinitialstatesunseenintraining,forwhichthetargetstateissettobeeither
(0,0)or(2π,0)(asopposedtotheoriginaltargetstate(π,0)). Specifically,foracoefficientλ=0.1,themodifiedobjective
isgivenby:
J(w;S)+λ· 1 (cid:88) (cid:88)H (cid:13) (cid:13)(θ ,θ˙ )−(cid:0) θ¯ ,0(cid:1)(cid:13) (cid:13)2 ,
H ·|U| (θ0,θ˙ 0)∈U h=0(cid:13) h h θ0 (cid:13)
whereS ⊂R2isthesetofinitialstatesseenintraining,U ⊂R2\Sisthesetofinitialstatesusedforevaluatingextrapolation
toinitialstatesunseenintraining,J(·;S)isdefinedbyEquation(41),θ andθ˙ evolveaccordingtoEquation(40)with
h h
u =π (θ ,θ˙ ),forh∈[H],andθ¯ =0ifθ ≤πandθ¯ =2πifθ >π. Wetrainedfivecontrollerswiththis
h−1 w h−1 h−1 θ0 0 θ0 0
modifiedobjective,usingdifferentrandomseeds,andselectedforFigure3theoneattainingthelowesttrainingcost.
Optimization. The training cost was minimized via policy gradient with learning rate 5·10−4. For training the non-
extrapolatingcontrolleroverthemodifiedobjective(specifiedabove),wefoundtheAdamoptimizer(Kingma&Ba,2015)
tobesubstantiallymoreeffective. Hence,forthatpurpose,weusedAdamwithdefaultβ ,β coefficientsandlearning
1 2
rate3·10−4. Optimizationproceededuntilthetrainingobjectivedidnotimprovebyatleast10−5over5,000consecutive
iterationsor75,000iterationselapsed. Thefinalcontrollerineachrunwastakentobethatwhichachievedthelowest
trainingcostacrosstheiterations.
Computingthenormalizedcostmeasureofextrapolation. Letw ∈RP betheparametersofthenon-extrapolating
no−ext
controller. The normalized cost measure of extrapolation attained by w ∈ RP for a set of initial states unseen in
trainingU ⊂R2\S iscomputedasfollows: (J(w;U)−J˜∗(U))/(J(w ;U)−J˜∗(U)),whereJ(·;U)isdefinedby
no−ext
Equation(41)andJ˜∗(U)isanestimateoftheminimalpossiblecostoverU. WeobtainedtheestimateJ˜∗(U)bytraininga
neuralnetworkcontroller(ofthesamearchitecturespecifiedabove)forminimizingthecostonlyoverU,i.e.forminimizing
J(·;U). Wecarriedoutfivesuchruns,differinginrandomseed,andtookJ˜∗(U)tobetheminimalcostattainedacrossthe
runs.
E.3.3 THEQUADCOPTERCONTROLPROBLEM(SECTION5.2)
System. Thestateofthesystemx=(x,y,z,ϕ,θ,ψ,x˙,y˙,z˙,ϕ˙,θ˙,ψ˙)∈R12comprisesthequadcopter’sposition(x,y,z)∈
R3, tilt angles (ϕ,θ,ψ) ∈ R3 (i.e. roll, pitch, and yaw), and their respective velocities. At time step h, the controller
choosesu ∈ [0,MAX RPM]4,whichdeterminestherevolutionsperminute(RPM)foreachofthefourmotors,where
h
MAX RPM = 21713.71 is the maximal supported RPM. Our implementation of the state dynamics is adapted from
the torchcontrol GitHub repository, which is based on the explicit dynamics given in Panerati et al. (2021). For
completeness,welayoutexplicitlytheevolutionattimesteph∈[H]:
     
x x x˙
h h−1 h−1
y h=y h−1+∆·y˙ h−1
z z z˙
h h−1 h−1

ϕ
 
ϕ
  ϕ˙ 
h h−1 h−1
θ h=θ h−1+∆·θ˙ h−1
ψ
h
ψ
h−1
ψ˙
h−1
,
        
x˙ x˙ 0 0
h h−1 ∆
y z˙ ˙h=y z˙ ˙h−1+
m
·V h−1
k
·∥u0 ∥2− g0
h h−1 f h−1
 ϕ˙   ϕ˙   k√F·l ·(u2 [1]+u2 [2]−u2 [3]−u2 [4])   ϕ˙   ϕ˙ 
h h−1 2 h−1 h−1 h−1 h−1 h−1 h−1
θ˙ h=θ˙ h−1+∆·P−1  k√F 2·l ·(−u2 h−1[1]+u2 h−1[2]+u2 h−1[3]−u2 h−1[4]) −θ˙ h−1×Pθ˙ h−1

ψ˙ h ψ˙ h−1 k T ·(−u2 h−1[1]+u2 h−1[2]−u2 h−1[3]+u2 h−1[4]) ψ˙ h−1 ψ˙ h−1
(42)
44ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
where×standshereforthecrossproductoftwovectors,u [1],...,u [4]aretheentriesofu ,∆=0.02isthe
h−1 h−1 h−1
timediscretizationresolution,g = 9.81isthegravitationalaccelerationconstant,m = 0.027isthequadcopter’smass,
l=0.0397isthequadcopter’sarmlength,k =3.16·10−10,K =7.94·10−12describephysicalconstantsrelatedtothe
F T
conversionofmotorRPMtotorque,and:
 
cos(θ) sin(θ)sin(ϕ) sin(θ)cos(ϕ)
V
h−1
= sin(θ)sin(ψ) −cos(θ)sin(ϕ)sin(ψ)+cos(ϕ)cos(ψ) −cos(θ)cos(ϕ)sin(ψ)−sin(ϕ)cos(ψ)
−sin(θ)cos(ψ) cos(θ)sin(ϕ)cos(ψ)+J(ϕ)sin(ψ) cos(θ)cos(ϕ)cos(ψ)−sin(ϕ)sin(ψ)
 1.4·10−5 0 0 
P= 0 1.4·10−5 0 
0 0 2.17·10−5
arerotationandinertialmatrices,respectively. Forbrevityofnotation,weomittedthesubscripth−1fromϕ,θ,ψinthe
definitionofV .
h−1
Cost. Thegoalofthecontrolleristomakethequadcopterreachandstayatthetargetstatex∗ = (0,0,1,0,...,0). In
accordancewiththetorchcontrolimplementation,thecostateachtimestepisaweightedsquaredEuclideandistance
fromx∗. Specifically,supposethatwearegivena(finite)setofinitialstatesX ⊂R12. Fora(state-feedback)controller
π :R12 →[0,MAX RPM]4,parameterizedbyw∈RP,thecostisdefinedby:
w
J(w;X):= 1 (cid:88) (cid:88)H (cid:88)12 α2·(x [d]−x∗[d])2, (43)
H ·|X| x0∈X h=0 d=1 d h
where x ∈ R12 evolves according to Equation (42) with u = π (x ), for h ∈ [H], the cost weights are
h h−1 w h−1
α = α = α = 1 and α = ··· = α = 0.1, and x [d],x∗[d] denote the d’th entries of x ,x∗, respectively. In
1 2 3 4 12 h h
allexperiments,thetimehorizonissettoH =50.
Initialstates. FortheexperimentsofFigures3,8,and10,Tables2,3,and4specifytheinitialstatesusedfortrainingand
thoseusedforevaluatingextrapolationtoinitialstatesunseenintraining,respectively.
Controllerparameterization. Asinpendulumcontrolexperiments(cf.AppendixE.3.2),weparameterizedthecontroller
asafully-connectedneuralnetworkwithReLUactivation. Thenetworkwasofdepth4andwidth50,anditsparameters
wererandomlyinitializedaccordingtothedefaultPyTorchimplementation. Toconvertthenetwork’soutputsintovalues
within[0,MAX RPM],weappliedthehyperbolictangentactivationandlinearlyscaledtheresult. Thatis,denotingby
z ∈ R4 theoutputofthenetworkforsomestate,thechosencontrolwasu = (tanh(z)+1)· MAXRPM,wheretanhis
2
appliedelement-wiseand1∈R4isthevectorwhoseentriesareallequaltoone.
Non-extrapolating controller. Similarly to the pendulum control experiments (cf. Appendix E.3.2), to obtain a non-
extrapolatingcontrollerbaselinesforFigures3, 8, and10, wetrainedcontrollersusingamodifiedobjectiveinsteadof
thestandardtrainingcost. Inadditiontothecostoverinitialstatesseenintraining, themodifiedobjectiveincludesan
“adversarial”costtermoverinitialstatesunseenintraining,forwhichthetargetstateissettobex¯ =(0,0,0,0,...,0)(as
opposedtotheoriginaltargetstatex∗ =(0,0,1,0,...,0)). Specifically,foracoefficientλ=0.1,themodifiedobjectiveis
givenby:
J(w;S)+λ· 1 (cid:88) (cid:88)H (cid:88)12 α2·(x [d]−x¯[d])2,
|H ·U| x0∈U h=0 d=1 d h
where S ⊂ R12 is the set of initial states seen in training, U ⊂ R12 \S is the set of initial states used for evaluating
extrapolation to initial states unseen in training, J(·;S) is defined by Equation (43), x ∈ R12 evolves according to
h
Equation(42)withu =π (x ),forh∈[H],thecostweightsareα =α =α =1andα =···=α =0.1,
h−1 w h−1 1 2 3 4 12
andx [d],x¯[d]denotethed’thentriesofx ,x¯,respectively. ForeachofFigures3,8,and10,wetrainedfivecontrollers
h h
withthismodifiedobjective,usingdifferentrandomseeds,andselectedtheoneattainingthelowesttrainingcost.
Optimization. Inallexperiments,thetrainingcostwasminimizedviatheAdamoptimizer(Kingma&Ba,2015)with
defaultβ ,β coefficientsandlearningrate3·10−4. Optimizationproceededuntilthetrainingobjectivedidnotimproveby
1 2
atleast10−5over5,000consecutiveiterationsor75,000iterationselapsed. Thefinalcontrollerineachrunwastakentobe
thatwhichachievedthelowesttrainingcostacrosstheiterations.
Computingthenormalizedcostmeasureofextrapolation. Thenormalizedcostmeasureofextrapolationwascomputed
accordingtotheprocessdescribedinAppendixE.3.2forthependulumcontrolexperiments.
45ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
LQR Problem
Figure4:InunderdeterminedLQRproblems(Section3.2),theextenttowhichlinearcontrollerslearnedviapolicygradientextrapolateto
initialstatesunseenintraining,dependsonthedegreeofexplorationthatthesysteminducesfrominitialstatesthatwereseenintraining.
ThisfiguresupplementsFigure2byincludingresultsforanalogousexperimentsoversystemswithalongertimehorizonH =8(instead
ofH =5). Results: Theincreaseintimehorizonimprovedextrapolationtounseeninitialstates,inaccordancewiththeanalysisof
Section4.3.Adrawbackofincreasingthetimehorizon,however,isthatitcanleadtoinstabilitiesduringtraining(cf.Metzetal.(2021)).
Indeed,wewereunabletoconsistentlytraincontrollerswhenthetimehorizonwassubstantiallylongerthanH =8.Thus,techniques
enablingstabletrainingwithlongtimehorizonsmaybeapromisingtoolforimprovingextrapolation.
LQR Problem
Figure5:InunderdeterminedLQRproblems(Section3.2),theextenttowhichlinearcontrollerslearnedviapolicygradientextrapolate
toinitialstatesunseenintraining,dependsonthedegreeofexplorationthatthesysteminducesfrominitialstatesthatwereseenin
training.ThisfiguresupplementsFigure2byincludingresultsforanalogousexperimentsoveranLQRproblemwithrandomA∈RD×D,
B∈RD×D,andpositivesemidefiniteQ∈RD×D (insteadofjustarandomA). Specifically,inthe√“randomA,B,Q”system,the
entriesofAandBweresampledindependentlyfromazero-meanGaussianwithstandarddeviation1/ D.AsforQ√,wefirstsampled
theentriesofamatrixZ ∈ RD×D independently,againfromazero-meanGaussianwithstandarddeviation1/ D. Then,weset
Q=ZZ⊤.Results:Non-trivialextrapolationisachievedunderthe“randomA,B,Q”system,inaccordancewiththefactthatrandom
systemsgenericallyinduceexploration(seediscussioninSection4.4).Theextentofextrapolationissignificantlybettercomparedto
systemswherejustAisrandom(referredtoas“random”inthelegendandanalyzedinTheorem1).Theoreticalinvestigationofthis
phenomenonisleftforfuturework.
46ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Policy Gradient Controller
Time Step 0 Time Step 20 Time Step 40 Time Step 60 Time Step 60 Time Step 100
target state initial state seen in training initial state unseen in training
Non-Extrapolating Controller
Time Step 0 Time Step 20 Time Step 40 Time Step 60 Time Step 60 Time Step 100
target state initial state seen in training initial state unseen in training
Figure6:ForthependulumcontrolexperimentsinFigure3,presentedistheevolutionofstatesthroughtimeunderthepolicygradient
(top)andnon-extrapolating(bottom)controllers.
Policy Gradient Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Non-Extrapolating Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Figure7:ForthequadcoptercontrolexperimentsinFigure3,presentedistheevolutionofstatesthroughtimeunderthepolicygradient
(top)andnon-extrapolating(bottom)controllers.
47ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Figure8: Inthequadcoptercontrolproblem(Section5.2),a(non-linear)neuralnetworkcontrollertrainedviapolicygradientoften
extrapolatestoinitialstatesunseenintraining,despitetheexistenceofnon-extrapolatingcontrollers.ThisfiguresupplementsFigure3by
includingtheresultsofanidenticalexperiment,butwithadditionalunseeninitialstatesthatarefartherawayfromtheinitialstatesseenin
training.SeecaptionofFigure3fordetailsontheexperiment.Results:Asonemightexpect,whiletheextentofextrapolationisstill
highlynon-trivial,itdecaysthefartherawayinitialstatesunseenintrainingarefromtheinitialstatesseenintraining.Furtherdetailsin
AppendixE:Table3fullyspecifiestheinitialandfinalstatesdepictedabove,andFigure9presentstheevolutionofstatesthroughtime
underthepolicygradientandnon-extrapolatingcontrollers.
Policy Gradient Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Non-Extrapolating Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Figure9: Forthepolicygradient(top)andnon-extrapolating(bottom)controllersfromFigure8,presentedistheevolutionofstates
throughtime.
48ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Figure10: Inthequadcoptercontrolproblem(Section5.2),a(non-linear)neuralnetworkcontrollertrainedviapolicygradientoften
extrapolatestoinitialstatesunseenintraining(atdifferenthorizontaldistancesfromthetargetstate), despitetheexistenceofnon-
extrapolatingcontrollers.ThisfiguresupplementsFigure3byincludingtheresultsofananalogousexperiment,wheretheunseeninitial
statesareatdifferenthorizontaldistancesfromtheinitialstatesseenintraining(insteadofbeingatalowerheight). Seecaptionof
Figure3fordetailsontheexperiment.Results:Remarkably,thecontrollertrainedviapolicygradientextrapolateswelltounseeninitial
statesatvarioushorizontaldistancesfromtheinitialstatesseenintraining.Incontrasttounseeninitialstatesbelowthoseusedfortraining,
forwhichextrapolationwasobservedinFigure3,anuncontrolledsystemdoesnotinduceexplorationtostatesatdifferenthorizontal
distances,inthenaivesenseofvisitingthestatealongtrajectoriesemanatingfromtheinitialstatesseenintraining.Hence,theresultsof
thisexperimenthighlighttheimportanceoffindingaquantitativemeasureofexplorationfornon-linearsystems,whichmayfacilitatethe
theoreticalstudyofextrapolationtherein. FurtherdetailsinAppendixE:Table4fullyspecifiestheinitialandfinalstatesdepicted
above,andFigure11presentstheevolutionofstatesthroughtimeunderthepolicygradientandnon-extrapolatingcontrollers.
Policy Gradient Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Non-Extrapolating Controller
Time Step 0 Time Step 10 Time Step 20 Time Step 30 Time Step 40 Time Step 50
target state initial state seen in training initial state unseen in training
Figure11:Forthepolicygradient(top)andnon-extrapolating(bottom)controllersfromFigure10,presentedistheevolutionofstates
throughtime.
49ImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Table1:Target,initial,andfinalstatesforthependulumcontrolexperimentsdepictedinFigure3.Eachstateisdescribedbythevertical
angleofthependulumθ∈Randitsangularvelocityθ˙∈R.
θ θ˙
TargetState 3.14 0
2.64 0.00
InitialStates
3.64 0.00
FinalStatesfor 3.13 0.01
PolicyGradientController 3.15 -0.01
FinalStatesfor 3.14 0.00
Non-ExtrapolatingController 3.15 -0.00
0.00 0.00
0.79 0.00
InitialStates 1.57 0.00
4.71 0.00
5.50 0.00
3.10 0.03
3.10 0.03
FinalStatesfor
3.11 0.02
PolicyGradientController
3.17 -0.03
3.18 -0.04
-0.00 -0.00
0.01 -0.00
FinalStatesfor
0.02 -0.01
Non-ExtrapolatingController
6.27 0.01
6.28 0.01
50
gniniarT
neesnUImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Table2: Target,initial,andfinalstatesforthequadcoptercontrolexperimentsdepictedinFigure3. Eachstateofthesystemx =
(x,y,z,ϕ,θ,ψ,x˙,y˙,z˙,ϕ˙,θ˙,ψ˙)∈R12comprisesthequadcopter’sposition(x,y,z),tiltangles(ϕ,θ,ψ)(i.e.roll,pitch,andyaw),and
theirrespectivevelocities.
x y z ϕ θ ψ x˙ y˙ z˙ ϕ˙ θ˙ ψ˙
TargetState 0 0 1 0 0 0 0 0 0 0 0 0
0.00 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 1.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates 0.25 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.25 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.25 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.25 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.00 1.00 -0.01 -0.00 0.01 -0.01 0.00 0.00 0.00 0.00 0.02
0.00 0.00 1.00 -0.01 -0.00 0.01 0.00 -0.00 -0.01 -0.01 0.00 0.01
-0.00 0.00 1.00 -0.01 0.00 0.01 0.00 -0.00 -0.01 -0.01 -0.00 0.01
FinalStatesfor
-0.03 -0.00 1.00 -0.01 0.07 0.01 -0.10 -0.00 -0.01 -0.02 -0.11 0.01
PolicyGradientController
-0.00 -0.02 1.00 -0.05 -0.00 -0.02 0.00 0.02 0.02 0.11 0.02 0.15
0.03 0.00 1.00 -0.01 -0.07 0.01 0.10 0.01 -0.00 -0.00 0.09 0.02
-0.00 0.02 1.00 0.03 -0.00 0.04 0.01 -0.01 -0.06 -0.12 -0.02 -0.12
0.00 -0.00 1.00 0.02 0.00 -0.02 0.00 0.01 0.01 0.02 0.00 -0.01
0.00 -0.00 1.00 0.02 0.00 -0.02 0.01 0.01 0.01 0.02 -0.00 -0.01
-0.00 -0.00 1.00 0.01 0.00 -0.01 0.01 -0.01 -0.00 0.00 -0.01 -0.03
FinalStatesfor
-0.03 0.00 1.00 0.03 0.09 -0.04 -0.09 0.00 -0.02 0.04 -0.12 -0.05
Non-ExtrapolatingController
-0.00 -0.02 1.00 -0.01 0.00 -0.06 -0.02 0.05 0.00 0.20 0.05 0.15
0.03 0.00 1.01 0.00 -0.08 0.01 0.09 0.00 0.02 -0.00 0.12 0.04
0.00 0.02 1.01 0.05 0.00 0.02 0.02 -0.04 -0.00 -0.17 -0.06 -0.18
0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates 0.00 0.25 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.25 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.00 -0.00 1.00 -0.01 0.00 0.00 0.00 0.01 0.01 -0.00 -0.01 0.02
-0.03 0.01 1.02 0.01 0.07 0.01 -0.08 0.02 -0.03 0.04 -0.12 0.05
FinalStatesfor
-0.01 -0.00 1.04 -0.02 -0.00 0.00 0.05 0.07 -0.06 0.12 -0.08 0.16
PolicyGradientController
0.16 -0.05 0.50 -0.07 -0.71 -0.19 -0.12 0.06 -3.19 0.70 4.80 -0.25
-0.01 0.05 0.88 0.06 -0.01 0.03 -0.02 -0.38 -0.05 -0.68 0.93 -0.44
0.02 0.01 -0.01 0.09 0.02 -0.11 0.14 0.05 0.03 0.17 -0.03 -0.15
-0.03 0.01 0.00 0.10 0.13 -0.11 -0.10 0.02 -0.39 0.02 0.15 0.33
FinalStatesfor
0.02 -0.02 0.00 0.03 0.02 -0.16 0.08 0.05 0.04 0.38 -0.03 0.06
Non-ExtrapolatingController
0.06 0.02 -0.02 0.06 -0.09 -0.06 0.31 0.07 -0.09 0.13 -0.06 -0.10
0.02 0.04 -0.04 0.15 0.02 -0.02 0.11 0.10 -0.57 -0.17 0.04 0.08
51
gniniarT
neesnUImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Table3: Target,initial,andfinalstatesforthequadcoptercontrolexperimentsdepictedinFigure8. Eachstateofthesystemx =
(x,y,z,ϕ,θ,ψ,x˙,y˙,z˙,ϕ˙,θ˙,ψ˙)∈R12comprisesthequadcopter’sposition(x,y,z),tiltangles(ϕ,θ,ψ)(i.e.roll,pitch,andyaw),and
theirrespectivevelocities.
x y z ϕ θ ψ x˙ y˙ z˙ ϕ˙ θ˙ ψ˙
TargetState 0 0 1 0 0 0 0 0 0 0 0 0
0.00 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 1.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates 0.25 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.25 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.25 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.25 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.00 0.00 1.00 0.01 0.00 -0.00 -0.01 0.01 -0.00 0.02 0.02 -0.00
0.00 0.00 1.00 0.01 -0.00 -0.01 -0.00 -0.01 0.00 0.00 0.01 -0.01
0.00 -0.00 1.00 0.01 -0.00 -0.02 0.00 -0.01 -0.00 -0.00 -0.00 -0.02
FinalStatesfor
-0.02 0.00 1.00 0.04 0.09 -0.04 -0.04 0.01 -0.02 0.09 -0.11 0.04
PolicyGradientController
-0.00 -0.02 1.00 -0.01 0.00 -0.07 0.01 0.03 0.01 0.21 -0.01 0.16
0.03 -0.00 1.00 0.02 -0.09 -0.01 0.05 -0.02 0.01 -0.06 0.08 -0.03
0.00 0.02 1.00 0.03 -0.00 0.04 0.01 -0.05 0.02 -0.17 -0.02 -0.18
-0.00 0.00 1.00 0.02 -0.00 -0.02 0.01 0.03 0.01 0.04 -0.01 -0.00
-0.00 0.00 1.00 0.02 -0.00 -0.02 -0.00 0.02 0.00 0.03 0.00 -0.01
-0.00 0.00 1.00 0.02 0.00 -0.02 -0.01 0.03 0.00 0.04 0.02 -0.01
FinalStatesfor
-0.03 -0.00 1.00 0.02 0.10 -0.01 -0.03 -0.02 -0.03 -0.02 -0.16 -0.01
Non-ExtrapolatingController
-0.00 -0.02 1.01 -0.01 -0.01 -0.07 -0.02 0.07 0.06 0.22 0.01 0.12
0.03 0.01 1.00 0.02 -0.10 -0.04 0.05 0.05 0.04 0.03 0.08 -0.03
-0.00 0.02 1.00 0.04 0.01 0.03 0.01 -0.04 -0.05 -0.17 -0.00 -0.15
0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.25 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.25 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates
0.00 0.00 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.25 0.00 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.25 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.25 0.00 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.25 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.01 0.05 1.02 0.01 0.02 0.03 -0.27 0.07 0.02 0.09 0.55 0.01
0.09 0.14 0.64 0.07 0.46 0.23 1.51 0.47 0.00 1.23 -2.99 0.81
0.01 0.02 1.01 -0.00 -0.00 -0.04 -0.15 0.00 0.00 0.15 0.23 0.09
-0.04 0.13 0.91 0.02 0.16 0.15 -0.77 0.21 -0.03 0.10 2.32 0.15
FinalStatesfor 0.10 0.25 0.51 -0.10 0.27 0.58 0.21 0.02 -1.40 1.27 2.83 0.84
PolicyGradientController 0.24 0.11 0.85 -0.57 -0.54 0.28 0.17 0.35 -2.93 1.37 -0.08 2.25
-0.14 0.07 0.70 -0.73 0.84 0.03 -0.10 0.59 -3.11 1.00 2.67 1.19
0.09 0.05 0.74 -0.33 0.11 0.06 -0.15 0.66 -1.42 1.01 3.72 0.83
0.55 0.14 0.95 -1.13 -1.16 0.17 0.89 0.52 -3.04 1.43 -0.90 3.42
0.35 0.22 1.14 -0.74 -0.48 0.34 0.41 0.74 -2.68 0.18 -1.60 2.29
-0.01 -0.02 0.01 -0.00 0.01 0.01 -0.00 -0.10 0.00 -0.24 0.03 0.01
-0.04 -0.01 -0.01 0.04 0.17 -0.02 -0.01 -0.05 -0.09 -0.01 -0.39 0.01
0.00 -0.03 0.01 -0.03 -0.01 -0.05 -0.03 0.05 -0.02 0.32 -0.03 0.16
0.04 -0.02 -0.02 -0.02 -0.16 -0.00 -0.09 -0.06 0.08 -0.03 0.67 -0.16
FinalStatesfor 0.00 0.02 -0.01 0.05 0.01 0.06 -0.06 -0.14 -0.13 -0.27 0.08 -0.25
Non-ExtrapolatingController -0.01 -0.03 -0.00 0.00 0.01 -0.01 -0.01 -0.04 0.02 -0.13 -0.00 0.03
-0.02 -0.01 -0.01 0.03 0.13 -0.02 0.03 -0.06 -0.11 -0.08 -0.42 0.08
0.00 -0.02 0.00 -0.03 -0.02 -0.05 0.02 0.02 0.04 0.19 -0.17 0.24
-0.02 -0.00 -0.01 -0.01 -0.08 0.00 -0.08 -0.05 0.03 0.05 0.35 -0.13
-0.00 0.01 -0.01 0.05 0.01 0.05 0.01 -0.10 -0.16 -0.17 -0.03 -0.23
52
gniniarT
neesnUImplicitBiasofPolicyGradientinLinearQuadraticControl:ExtrapolationtoUnseenInitialStates
Table 4: Target, initial, and final states for the quadcopter control experiments depicted in Figure 10. Each state of the system
x=(x,y,z,ϕ,θ,ψ,x˙,y˙,z˙,ϕ˙,θ˙,ψ˙)∈R12comprisesthequadcopter’sposition(x,y,z),tiltangles(ϕ,θ,ψ)(i.e.roll,pitch,andyaw),
andtheirrespectivevelocities.
x y z ϕ θ ψ x˙ y˙ z˙ ϕ˙ θ˙ ψ˙
TargetState 0 0 1 0 0 0 0 0 0 0 0 0
0.50 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.50 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates -0.50 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 -0.50 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
-0.05 -0.02 1.00 -0.02 0.25 0.01 0.03 -0.06 -0.11 -0.02 -0.05 0.00
-0.01 -0.03 0.99 -0.14 0.01 -0.13 -0.04 0.27 -0.07 0.14 0.06 0.10
FinalStatesfor
0.04 0.00 1.01 0.00 -0.25 -0.03 -0.09 0.04 -0.01 0.12 0.12 0.14
PolicyGradientController
-0.01 0.03 1.01 0.17 0.01 0.13 0.00 -0.32 0.06 -0.09 -0.04 -0.19
0.00 -0.00 1.00 0.01 0.00 -0.01 0.01 0.00 -0.01 0.01 -0.01 -0.01
-0.06 -0.00 1.01 -0.04 0.20 0.02 -0.03 -0.03 -0.06 -0.06 -0.25 -0.07
0.01 -0.04 1.01 -0.13 0.02 -0.09 0.02 0.19 -0.08 0.31 0.04 0.21
FinalStatesfor
0.06 -0.00 1.01 0.04 -0.20 -0.02 -0.02 0.02 0.15 0.09 0.42 0.08
Non-ExtrapolatingController
-0.01 0.04 1.00 0.15 -0.01 0.09 -0.05 -0.24 -0.28 -0.12 0.05 -0.46
-0.00 -0.00 1.00 0.03 -0.00 -0.02 -0.02 -0.02 0.04 -0.08 0.01 -0.07
0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
InitialStates 0.75 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
1.00 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
1.25 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.15 0.01 1.09 0.38 0.90 0.83 2.30 -2.22 -1.55 1.57 1.17 0.80
0.10 -0.13 1.09 -0.11 0.08 -0.00 0.71 -0.55 0.01 -0.95 -0.64 -0.77
FinalStatesfor
-0.10 0.05 0.99 -0.10 0.54 -0.09 0.38 0.77 0.25 0.69 0.10 -0.62
PolicyGradientController
-0.03 0.03 0.97 -0.32 1.02 -0.23 1.72 1.46 -0.64 -0.12 0.90 -1.17
-0.11 -0.06 0.98 -0.41 1.20 -0.12 0.92 1.11 -1.42 -0.53 2.14 -1.16
-0.09 -0.04 0.02 -0.01 0.04 -0.06 -0.04 0.03 0.21 -0.12 0.20 -0.24
-0.13 0.02 0.00 0.01 0.18 0.02 -0.20 -0.05 0.20 -0.00 -0.12 -0.04
FinalStatesfor
-0.07 0.01 -0.01 -0.01 0.23 0.03 -0.34 0.02 -0.14 0.18 -0.51 -0.24
Non-ExtrapolatingController
-0.11 0.00 0.01 -0.05 0.30 -0.01 -0.18 0.08 -0.11 0.15 -0.81 -0.10
-0.16 0.00 0.02 -0.11 0.40 0.01 -0.14 0.07 -0.05 0.21 -0.97 -0.29
53
gniniarT
neesnU