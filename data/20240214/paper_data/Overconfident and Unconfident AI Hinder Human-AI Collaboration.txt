Overconfident and Unconfident AI Hinder
Human-AI Collaboration
Jingshu Li1†, Yitian Yang1†, Yi-Chieh Lee1*
1*School of Computing, National University of Singapore, 21 Lower
Kent Ridge Road, 119077, Singapore.
*Corresponding author(s). E-mail(s): ejli.uiuc@gmail.com;
Contributing authors: jingshu@u.nus.edu; t0931554@u.nus.edu;
†These authors contributed equally to this work.
Abstract
As artificial intelligence (AI) advances, human-AI collaboration has become
increasingly prevalent across both professional and everyday settings. In such
collaboration, AI can express its confidence level about its performance, serving
as a crucial indicator for humans to evaluate AI’s suggestions. However, AI may
exhibit overconfidence or underconfidence–its expressed confidence is higher or
lower than its actual performance–which may lead humans to mistakenly evalu-
ate AI advice. Our study investigates the influences of AI’s overconfidence and
underconfidence on human trust, their acceptance of AI suggestions, and col-
laboration outcomes. Our study reveal that disclosing AI confidence levels and
performance feedback facilitates better recognition of AI confidence misalign-
ments. However, participants tend to withhold their trust as perceiving such
misalignments, leading to a rejection of AI suggestions and subsequently poorer
performance in collaborative tasks. Conversely, without such information, par-
ticipants struggle to identify misalignments, resulting in either the neglect of
correctAIadviceorthefollowingofincorrectAIsuggestions,adverselyaffecting
collaboration. This study offers valuable insights for enhancing human-AI col-
laboration by underscoring the importance of aligning AI’s expressed confidence
withitsactualperformanceandthenecessityofcalibratinghumantrusttowards
AI confidence.
1
4202
beF
21
]IA.sc[
1v23670.2042:viXra1 Main
With the rapid advancement of AI, it becomes a significant force in various domains,
demonstratingitscapacitytoextractvaluableinsightsfromdataandopeningupnew
avenues for human-AI collaboration. From everyday scenarios, such as navigation [1],
deciding daily outfits [2, 3], and recommending movies and music [4, 5], to high-risk
areas such as healthcare [6, 7, 8], investment [9, 10], autonomous driving [11], and
military applications [12], AI can assist humans in excellently completing tasks. In
these scenarios, the collaboration between humans and AI can combine the strengths
of both, reducing human cognitive load and overall error, leading to better outcomes
than either could achieve alone [13, 14, 15, 16]. Recent studies have demonstrated
the success of human-AI collaboration [17, 18]; however, some research found that in
certain situations, the performance of human-AI teams may not exceed that of AI
working independently [4, 19, 20, 21].
These empirical cases demonstrate that the success of human-AI collaboration is
notguaranteed.Thekeytoenhancingtaskefficiencyinthiscollaborationprocessisfor
humans to rely on AI or adopt AI-proposed solutions when AI is reliable. This saves
human working time and prevents human errors. Conversely, when AI is not reliable,
humansrelyontheirjudgmenttoavoidAIerrors,resultinginacomplementaryeffect
of the human-AI interaction [22]. The success of human-AI collaboration depends
on the ability of humans to trust AI appropriately, understand when to use it, and
calibrate their trust in AI [22, 23, 24].
In human-AI collaboration process, human trust is defined as the attitude that an
AI will help humans complete the task appropriately in collaboration, which directly
influences whether people depend on AI [24]. Trust calibration refers to the process
where humans match their trust with AI’s capabilities or credibility [22, 25, 24]. If
people’strustinAIexceedsitsactualcapabilities,leadingtoovertrust,thiscanresult
in misuse — overreliance on AI. Conversely, if their trust is less than AI’s actual
capabilities,leadingtodistrust,thiscancausedisuse—underutilizationofAI[23,24].
The effect of enhancing the efficiency of human-AI collaboration and reducing the
risk of errors can only be achieved when individuals trust AI appropriately, utilize it
wisely, and recognize its error boundaries [14, 18].
TohelphumanscalibratetheirtrustinAImoreefficientlyandaccurately,previous
studies have proposed numerous interventions [16, 26, 14, 27, 28, 29, 30, 31, 32, 33].
One of the common and critical concept is AI confidence [16]. AI confidence scores
are a specific type of performance indicator that goes beyond general metrics like AI
accuracy or overall reliability. Accuracy serves as a comprehensive metric, allowing
for a posteriori evaluation of AI’s overall performance across a set of tasks. However,
it cannot provide users with a priori information regarding the probability of AI suc-
cessfully completing a specific task before it is attempted. While AI confidence scores
are employed to quantify the level of uncertainty in AI predictions for each individ-
ual task, they essentially represent a prior estimate of accuracy. By estimating the
probabilityofthemodelbeingcorrectforaspecifictask,AIconfidencescoresprovide
valuable additional information [34, 35]. The presence of AI confidence enables users
to develop more effective collaborative strategies. When the AI confidence is higher
than human certainty, people tend to rely more on AI or its suggestions. Conversely,
2Fig. 1 (a) How overconfident and unconfidence AI affect human-AI collaboration. When individ-
uals can not correctly perceive the bias of AI confidence and overtrust AI confidence, then AI
overconfidence or unconfidence cause misuse or disuse. When individuals can correctly perceive the
uncalibration of AI confidence, they will distrust AI prediction, which will cause disuse. Human-AI
cooperation is hindered under both condition. (b) A flowchat of the study. Participants in different
groups collaborated with different AIs that had the same level of accuracy but different confidence
levels,includingunconfident,confident,overconfident.Afterpre-tasksurvey,participantsunderwent
basictrainingtocalibratetheirtrusttowardsAI.Thegroupwithtrustcalibrationsupportreceived
informationaboutthetypesofAIconfidenceandreceivedaccuracyfeedbackduringtraining,whereas
thegroupwithoutaiddidnot.ParticipantsthenreportedtheirtrustattitudestowardsAIconfidence,
AIpredictions,andAI’soverallcapabilityusingscales.Finally,participantswereaskedtocomplete
thirtytasks,inwhichtheyreceivednoperformancefeedback.
when the AI confidence is lower, humans are more inclined to rely on their own solu-
tions [25]. Having accurate AI confidence scores can enhance the speed and accuracy
of human-AI collaboration [36, 34].
Modern AI systems can provide a confidence score on their task performance as
a reference. For instance, in multi-classification tasks, AI based on machine learning
models can offer prediction probabilities as confidence scores for corresponding pre-
dictions [34]. However, the confidence of many AI is uncalibrated, meaning that their
confidence levels do not match their actual accuracy. These AI often exhibit over-
confidence in their predictions, and studies have also identified underconfident ones
[37, 34, 38, 39, 40]. In current research [19, 41, 42, 43], the focus is often on using
AI confidence to improve interpretability and transparency by displaying uncertainty.
However, the research overlooks the nuances in AI confidence as AI’s ability to assess
its own capabilities. While some studies have examined the effect of the numerical
levelofAIconfidenceorthetoneusedtoexpressconfidenceonhumanself-confidence
3and trust [12, 35, 44], they do not thoroughly explore and highlight the negative con-
sequences of manipulating AI confidence, which can lead to a mismatch between AI
accuracyandself-assessment.Thismismatchcanimpacthuman-AIcollaboration,and
it is also important to understand how these impacts occur.
When individuals are unaware of the mismatch between AI confidence and its
actual capabilities, leading to an overtrust on overconfident or underconfident AI
systems, uncalibrated AI confidence can misguide the adoption of AI by users. In
the absence of performance feedback and other interventions that aid in calibrating
trust, humans struggle to accurately calibrate their trust in AI [45]. While humans
can gradually adjust their trust in AI’s capabilities by observing consistency between
AI and human decisions, research indicates that this empirical process is often slow
[46,29,26].Byextension,people’sperceptionandcalibrationoftrustinthequalityof
AI confidence may also encounter the same challenges. As results, it is reasonable to
assume that users’ incorrect perception of AI confidence quality, i.e. overtrusting AI
confidence, results in affecting the correct adoption of AI, as well as the efficiency of
human-AI collaboration. Overconfidence in AI may cause users to rely on it in situa-
tionswhereitshouldnotbetrusted,leadingtoincreasedmisuse,whileAIunconfidence
may result in more disuse.
However, even if interventions are implemented to help users recognize AI over-
confidenceandunconfidence,thepresenceofuncalibratedAIconfidencecanstillhave
negativeeffectsonusers’trustandtheefficiencyofhuman-AIcollaboration.According
tosocialcognitivetheory,individualswhodisplayoverconfidencemaybeperceivedas
arrogant or unrealistic in collaborative settings, while those who lack confidence may
beseenaslackingcapabilityormotivation[47].Bothtypesofinappropriateconfidence
levels can be seen as barriers that hinder trust in such individuals. For instance, peo-
ple tend to distrust testimony that is confident but unreliable [48, 44]. According to
the Computers Are Social Actors (CASA) paradigm, in human-computer interaction,
people apply the same social rules and expectations as in interpersonal interactions
[49, 50, 51]. Therefore, in human-AI collaboration scenarios, the loss of user trust
caused by AI’s inappropriate confidence could also occur, potentially impacting the
proper adoption of AI and the efficiency of human-AI collaboration.
In this study, we investigate the influence of AI overconfidence and unconfi-
dence on human trust and the efficiency of human-AI collaboration. To explore the
detrimentalconsequencesofuncalibratedAIconfidence,weconductedanonlineexper-
iment involving human-AI collaboration (Fig. 1 (b)). A total of 252 participants were
recruited and divided into two primary groups: an group with trust calibration sup-
port (N=126) and a group without trust calibration support (N=126), depending on
whether they received interventions to aid in trust calibration, including statements
about AI confidence levels before the training phase and performance feedback about
each task during the training phase. Each primary group was further divided into
threesubgroups(N=42),whereparticipantscollaboratedwithAIsexhibitingdifferent
confidence levels–unconfident, confident, and overconfident–despite having identical
task performance capabilities (i.e., accuracy, at 70%). In our experiments, the AI’s
confidence score was presented directly as a percentage probability (Fig. 2), indicat-
ing the AI’s assurance in correctly completing the task. The average confidence of
4Fig.2 Urbanphotolocalizationtaskinterface.Participantsweretaskedwithidentifyingtheorigins
ofcityphotos.
unconfident AI was lower than its accuracy, while the average confidence of overconfi-
dentAIexceededitsaccuracy.ConfidentAI’saverageconfidencematcheditsaccuracy
precisely, with details provided in Sec. 4.3.1.
After signing an informed consent form and collecting demographic information,
participantsunderwentbasictrainingtocalibratetheirtrusttowardsAI.Participants
weretaskedwithidentifyingtheoriginsofcityphotos.Ineachtask,participantsmade
aninitialdecision,followedbytheAIprovidingasuggestionalongwithitsconfidence
level, after which participants made their final decision. The group with trust cali-
bration support received information about the types of AI confidence and received
accuracyfeedbackduringtraining,whereasthegroupwithoutsupportdidnot.Partic-
ipants then reported their trust attitudes towards AI confidence, AI predictions, and
AI’soverallcapabilityusingscales.Finally,participantswereaskedtocompletethirty
tasks, in which they received no performance feedback. Through participants’ behav-
iors, we calculated their disuse rate, the proportion of not using AI when they were
incorrect and AI was correct, and misuse rate, the proportion of using AI when they
were correct and AI was incorrect, to measure participants’ disuse and misuse behav-
iors. We also calculated the switch rate, the proportion of adopting AI suggestions
when disagreeing with AI, as a form of trust behavior, and the change in accuracy,
indicatingthechangeinparticipants’decision-makingaccuracyduetoAIinvolvement,
reflecting the efficiency improvement brought by human-AI collaboration.
5Fig.3 TheAIconfidencetypeswereperceivedbyparticipantsworkingwithdifferentAIconfidence.
The horizontal axis represents the participant’s perception of AI confidence, and the vertical axis
represents the level of AI confidence that the participant actually worked with. Left: without trust
calibrationsupportgroup(N=126);Right:withtrustcalibrationsupportgroup(N=126)
2 Results
Our experimental results indicate that regardless of whether participants perceived
the uncalibrated status of AI confidence, the deficiency in AI’s self-assessment ability
of uncertainty adversely influences the efficiency of human-AI collaboration.
2.1 Perceiving AI Overconfidence and Unconfidence is
Challenging without Trust Calibration Support
The results of the group without calibration aid (N = 126), as depicted on the left
side of Fig. 3, indicate that without prior information or performance feedback, it is
difficult forthe majorityof participants to recognizeif the AI’sconfidence level aligns
with its abilities solely. When the AI displayed overconfidence (N=42), 26.190% of
participants correctly perceived the bias in the AI’s confidence, 64.286% considered
the level of AI confidence to be appropriate, and 9.524% even perceived the AI as
unconfidence. In the case of unconfident AI (N=42), 28.571% of participants accu-
rately perceived the AI’s confidence bias, 66.667% believed the AI’s confidence level
wassuitable,and4.762%thoughttheAIwasoverconfident.Meanwhilethemajorityof
participantsdeemedtheAI’sconfidencelevelappropriatewhentheAIwaseitherover-
confident or underconfident, this result suggesting an overtrust in AI confidence. As
the AI exhibited an appropriate level of confidence, 76.190% of users report accurate
perceptions.Therewere9.524%ofparticipantswhoperceivedtheAIasoverconfident
6Fig.4 Theswitchrate,disuserate,misuserate,andaccuracychangeoftheparticipantsaredisplayed
inviolinandboxplots.Thetopgroupconsistsof126participantswithouttrustcalibrationsupport,
while the bottom group consists of 126 participants with trust calibration support. The p-values
obtained from the ANOVA are shown below each figure, and the significance levels from the post-
hoc analyses are indicated within the figures. The symbols used to denote significance levels are as
follows: ns (p>0.05), * (p<0.05), ** (p<0.01), and *** (p<0.001). More information, such as
theverificationofhomogeneityassumptions,analyticalmethods,andeffectsizes,canbefoundinthe
supplementarymaterials.
and14.286%whoperceiveditaslackingconfidence.Whencollaboratedwiththecon-
fident AI, the majority of participants were able to accurately report their perception
of the AI confidence level. Overall, without effective interventions, the majority of
participants struggled to accurately perceive AI overconfidence and unconfidence.
7Fig. 5 The trust attitudes of participants towards AI confidence, AI prediction, and AI overall
capability in violin and box plots are shown in the results. The top group represents participants
withouttrustcalibrationsupport(N=126),whilethebottomgrouprepresentsparticipantswithtrust
calibrationsupport(N=126).Thep-valuesfromtheANOVAareindicatedbeloweachfigure,andthe
significancelevelsfrompost-hocanalysesaredenotedwithinthefigures(ns:p>0.05,*:p<0.05,**:
p<0.01,***:p<0.001).Furtherinformation,suchastheverificationofhomogeneityassumptions,
analyticalmethods,andeffectsizes,canbefoundinthesupplementarymaterials.
2.2 Unperceived AI Overconfidence and Unconfidence Hinder
Human-AI Collaboration
Our results in the without trust calibration support group exemplify that when the
majority are unable to accurately perceive the discrepancies between AI’s confidence
and accuracy.
82.2.1 Unperceived AI Overconfidence Leads to Misuse
According to post-hoc analysis, as shown in Fig. 4 (g), participants in without trust
calibration support group working with overconfident AI exhibited a significantly
higher misuse rate (M =41.257%, s.d.=10.835%, p<0.001) compared to those col-
laborating with confident AI (M = 28.207%, s.d. = 14.002%). Moreover, the switch
rate (see Fig. 4 (e)) for participants working with overconfident AI (M = 69.587%,
s.d.=19.246%,p=0.027)wassignificantlyhigherthanforparticipantsworkingwith
confident AI (M = 56.975%, s.d. = 21.868%). In terms of Human-AI collaboration
efficiency shown in Fig. 4 (h), the change in accuracy after collaborating with over-
confidentAI(M =7.221%,s.d.=7.433%,p=0.018)wassignificantlylowerthanthe
change observed with confident AI (M =11.905%, s.d.=7.967%).
2.2.2 Unperceived AI Unconfidence Leads to Disuse
On the other hand, participants working with unconfident AI in without trust cali-
brationsupportgroupshowedasignificantlyhigherdisuserate(M =83.333%,s.d.=
11.242%, p < 0.001) compared to those working with confident AI (M = 73.016%,
s.d. = 10.657%) as shown in Fig. 4 (f). Additionally, the switch rate for participants
working with unconfident AI (M = 40.492%, s.d. = 24.752%, p = 0.002) was signifi-
cantlylowerthanthatforparticipantsworkingwithconfidentAI.Fromtheperspective
of Human-AI collaboration efficiency, the change in accuracy after collaborating with
unconfident AI (M = 6.508%, s.d. = 7.859%, p = 0.018) was significantly lower than
the change observed with confident AI.
2.2.3 Misuse and Disuse Lead to a Decrease in Human-AI
Collaboration Efficiency
The results of mediation analysis in Fig. 6 (a) and (b) further revealed the relation-
ships between misuse/disuse, participants’ adoption of AI decisions, and the ultimate
accuracy change in without trust calibration support group: Utilizing data from the
overconfidentAIandconfidentAIgroupswithouttrustcalibrationsupport,wefound
that the misuse rate fully mediated the influence of AI’s overconfidence on partici-
pants’ switch rate (β (indirect effect) = 6.875, p = 0.009; β (direct effect) = 5.737,
i d
p = 0.228) and final accuracy change (β = −4.673, p < 0.001; β = −0.011,
i d
p=0.994).WithdatafromtheunconfidentAIandconfidentAIgroups,itwasdiscov-
eredthatthedisuseratefullymediatedtheeffectofAI’sunconfidenceonparticipants’
switch rate (β = −16.869, p < 0.001; β = 0.387, p = 0.914) and final accuracy
i d
change (β =−5.809, p<0.001; β =0.412, p=0.728).
i d
These findings successfully support that overconfidence and unconfidence AI can
hinder human-AI collaboration when the majority are unable to accurately perceive
thediscrepanciesbetweenAI’sconfidenceandaccuracyandovertrustinAIconfidence.
When AI exhibited overconfidence, participants were more likely to inappropriately
follow its advice, i.e. misuse, resulting in higher switch rates and a decline in overall
accuracy,therebydiminishinghuman-AIcollaborationefficiency.Conversely,whenAI
demonstrated underconfidence, participants tended to ignore its correct advice, i.e.
disuse, leading to lower switch rates and similarly reducing collaboration efficiency.
92.2.4 No Significant Difference in Trust Attitudes
As shown in Fig. 5 (d), (e) and (f), in without trust calibration support group, when
the majority of participants were unable to accurately perceive in AI overconfidence
and unconfidence, there were no significant differences between their trust attitudes
towards AI confidence, AI predictions, and AI’s overall capability. According to the
resultsofanalysisofvariance(ANOVA),measurementsofparticipants’trustattitudes
towards AI confidence showed no significant differences when working with overcon-
fident AI (M = 4.921, s.d. = 0.933), unconfident AI (M = 4.912, s.d. = 0.800),
and confident AI (M = 5.111, s.d. = 0.895) with a p-value of p = 0.503. Similarly,
there were no significant differences in trust attitudes towards the AI’s overall capa-
bility when working with overconfident AI (M =5.222, s.d.=0.816), unconfident AI
(M = 4.928, s.d. = 0.802), and confident AI (M = 5.190, s.d. = 1.087), with a p-
valueofp=0.272.Furthermore,therewerenosignificantdifferencesintrustattitudes
towardsAIpredictionswhenworkingwithoverconfidentAI(M =5.215,s.d.=0.721),
unconfidentAI(M =4.985,s.d.=0.691),andconfidentAI(M =5.166,s.d.=0.804),
with a p-value of p = 0.327. These results are reasonable, as the majority of partici-
pants did not report correct perception of AI confidence and the prediction accuracy
of the AI did not differ.
2.3 Trust Calibration Support Help Human Develop Accurate
Perception about AI Confidence
As Fig. 3 (right) shows, our findings within the with trust calibration support group
(N=126) indicate that with the aid of prior information and accuracy feedback, the
majorityofparticipantswereabletocorrectlyperceiveoverconfidenceorunconfidence
AI exhibited. When the AI was overconfident (N=42), 73.810% of participants accu-
ratelyreportedtheirperceptionoftheAI’sconfidencebias,while26.190%believedthe
AI’sconfidencelevelwasappropriate,andnoneoftheparticipantsperceivedtheAIas
unconfident. In the case of unconfident AI (N=42), 76.190% of participants correctly
identified the AI’s confidence bias, 19.048% thought the AI’s confidence level was fit-
ting, and 4.762% considered the AI to be overconfident. When the AI demonstrated
an appropriate level of confidence, 71.429% of users reported accurate perceptions,
with 19.048% perceiving overconfidence and 9.524% sensing unconfidence in the AI.
Despite being informed about the AI’s confidence levels through prior information,
some participants still expressed perceptions that differed from the actual AI confi-
dence levels. This suggests that their responses might reflect their actual perceptions
duringAIinteraction,whichcoulddeviatefromthesetAIconfidencelevels,oritcould
indicate a discrepancy between participants’ subjective definitions of overconfidence
or unconfidence and the objective standards used in the experiment.
2.4 Perceived AI Overconfidence and Unconfidence Hinder
Human-AI Collaboration
Quantitativeresultsofthegroupwithtrustcalibrationsupportindicatethat,whilethe
majority of participants could perceive AI overconfidence and underconfidence, and
10Fig.6 Resultsofmediationanalysis.Thepatheffectsofeachpatharedisplayedonthediagram.(a)
MediationanalysisinconfidentAIgroup(N=42)andoverconfidentAIgroup(N=42)withouttrust
calibrationsupport.(b)MediationanalysisinconfidentAIgroup(N=42)andunconfidentAIgroup
(N=42)withouttrustcalibrationsupport.(c)MediationanalysisinconfidentAIgroup(N=42)and
overconfident AI group (N=42) with trust calibration support. (d) Mediation analysis in confident
AI group (N=42) and unconfident AI group (N=42) with trust calibration support. (e) Mediation
analysisaboutparticipants’trustattitudesinthegroupwithtrustcalibrationsupport(N=126).
were able to appropriately calibrate their trust in AI confidence levels, they concur-
rentlydevelopeddistrustintheAI’spredictivecapabilities,leadingtoincreaseddisuse
behaviors,ultimatelyreducingadoptionofAIsuggestionsandhuman-AIcollaboration
efficiency.
11Fig. 7 Results of linear regression. Left: Linear regression between participants’ trust attitudes
towardsAIpredictionsandtheirtrustattitudestowardsAIconfidenceinthegroupwithtrustcali-
brationsupport.Right:Linearregressionbetweenparticipants’disuserateandtheirtrustattitudes
towardsAIpredictionsinthegroupwithtrustcalibrationsupport.Theerrorbandsrepresenta95%
confidenceinterval.Seemoredetailsinsupplementarymaterials.
2.4.1 Perceived AI Overconfidence and Unconfidence Lead to
Distrust
Fortrustattitudes,asshowninFig.5(a),(b)and(c),ourresultsshowedparticipants
in with trust calibration support group could appropriately reduce their trust in the
AI’s confidence levels and overall capabilities when the AI was either overconfident
or unconfident. According to post-hoc analysis results, measurement of participants’
trust attitudes revealed that those working with overconfident AI had significantly
lower trust in AI confidence (M = 4.182, s.d. = 1.086, p = 0.004) compared to those
collaborating with confident AI (M = 4.920, s.d. = 0.911). Similarly, participants
working with unconfident AI also showed significantly lower trust in AI confidence
(M = 4.262, s.d. = 1.110, p = 0.012) than those with confident AI. Additionally,
trust in the overall capability of AI for participants working with overconfident AI
(M = 4.205, s.d. = 1.218, p = 0.009) was significantly lower compared to those with
confidentAI(M =4.913,s.d.=0.887),andthesametrendwasobservedwithuncon-
fident AI (M = 4.278, s.d. = 1.043, p = 0.010) compared to confident AI. These
results indicate that participants could calibrate their trust in AI’s confidence lev-
els and overall capabilities to a certain extent. However, as trust in AI confidence
waned, trust in AI predictions also declined. Notably, the accuracy of AI predictions
remainedunchanged,yetparticipantsdevelopeddistrustinAI’sabilitytomakeaccu-
rate forecasts. Trust attitudes towards AI predictions for participants collaborating
with overconfident AI (M = 4.420, s.d. = 0.857, p = 0.002) and unconfident AI
(M =4.421,s.d.=0.996,p=0.007)werebothsignificantlylowerthanthoseworking
with confident AI (M =5.008, s.d.=0.681). The results ofthe linear regression anal-
ysis (see Fig. 7 (a)) revealed a linear correlation between participants’ trust attitudes
towards AI predictions and their trust attitudes towards AI confidence (R = 0.649,
p<0.001).
Furthermore,asshowninFig.6(e),mediationanalysisrevealedthatparticipants’
trust in AI confidence directly influenced their trust in AI predictions and also indi-
rectlythroughtheirtrustinAI’soverallcapabilities(β =0.323,p<0.001;β =0.211,
i d
p < 0.001). This suggests that upon perceiving poor AI confidence, the reduction
12in trust towards AI confidence further leads to diminished trust in AI predictions,
resulting in distrust towards AI predictions.
2.4.2 Distrust Relates to Disuse, which Leads to a Decrease in
Human-AI Collaboration Efficiency
Regardless of collaborating with overconfident or underconfident AI, participants in
thegroupwithtrustcalibrationsupportexhibitedasignificantincreaseindisuserate,
and a significant decrease in both switch rate and final accuracy change. As Fig.4
(a), (b), (c) and (d) show, Participants working with overconfident AI exhibited a
significantlyhigherdisuserate(M =80.385%,s.d.=10.787%,p=0.004)asdidthose
workingwithunconfidentAI(M =80.272%,s.d.=11.355%,p=0.005),comparedto
thosecollaboratingwithconfidentAI(M =72.676%,s.d.=10.254%).Theswitchrate
forparticipantswithoverconfidentAI(M =46.137%,s.d.=21.185%,p=0.014)and
unconfidentAI(M =44.859%,s.d.=21.896%,p=0.006)wassignificantlylowerthan
for those with confident AI (M = 58.818%, s.d. = 17.967%). In terms of Human-AI
collaboration efficiency, the change in accuracy after collaboration with overconfident
AI (M = 8.015%, s.d. = 7.326%, p = 0.021) and unconfident AI (M = 6.984%,
s.d. = 7.715%, p = 0.003) was significantly lower than with confident AI (M =
12.540%, s.d. = 7.990%). There was no significant difference in misuse rate among
thethreegroups.LinearregressionresultsinFig.7(b)revealedaconnectionbetween
participants’ distrust attitudes towards AI predictions and their disuse behaviors: the
disuse rate increased as trust attitudes towards AI predictions decreased (R=0.345,
p<0.001).AlthoughtheR valuesfortheselinearregressionsarelowduetovariation
in the data, the probability of rejecting the null hypothesis is below 0.05, indicating
a significant trend. The results of the mediation analysis for with trust calibration
supportgroupwereshowninFig.6(c)and(d):UsingdatafromtheoverconfidentAI
and confident AI groups, we found that the disuse rate fully mediated the impact of
AIoverconfidenceonparticipants’switchrate(β =−10.616,p=0.001;β =−2.065,
i d
p = 0.498) and final accuracy change (β = −4.424, p = 0.001; β = −0.101, p =
i d
0.926). With data from the unconfident AI and confident AI groups, the disuse rate
fullymediatedtheeffectofAIunconfidenceonparticipants’switchrate(β =−10.902,
i
p=0.002;β =−3.056,p=0.291)andfinalaccuracychange(β =−4.357,p=0.002;
d i
β =−1.200, p=0.276).
d
Thesefindingssuccessfullysuggestthatwhenthemajorityofparticipantscanaccu-
rately perceive discrepancies between AI confidence and accuracy, AI overconfidence
and unconfidence lead to a decrease in participants’ trust in AI’s decision-making
capabilities, resulting in distrust, and consequently disuse, thereby diminishing the
efficiency of Human-AI collaboration.
3 Discussion
ThisstudyfocusesonexaminingtheconsequencesofAIconfidencemisalignmentwith
its capabilities on the collaboration between humans and AI. We aim to understand
the reasons behind these consequences. Our research answers important questions
regardingtheinfluenceofinappropriateAIconfidenceonhuman-AIcollaborationand
13sheds light on how these influences occur, thus having implications for addressing
a gap in the existing literature. Our results suggest that both overconfidence and
unconfidenceinAI,whichcanbereferredtoasuncalibratedAIconfidence,canimpede
human-AI collaboration and reduce efficiency.
We initially explore the challenges individuals face in accurately perceiving AI
confidence biases without prior information or objective feedback. Individuals strug-
gle to discern whether AI confidence is calibrated relying solely on the consistency
between their own confidence in tasks they feel certain about and AI’s expressed
confidence and overtrust in AI confidence. Uncalibrated AI confidence exploits this
overtrust, misleading users and impairing their ability to make informed decisions
about accepting AI recommendations. Overconfident AI, by presenting a confidence
level higher than its actual accuracy, can mislead individuals into accepting AI sug-
gestions in scenarios where skepticism is warranted, significantly increasing misuse
and subsequently reducing overall efficiency. Conversely, unconfident AI, by display-
ingaconfidencelevellowerthanitsactualcertainty,mayleadindividualstodisregard
AI suggestions in instances where trust would be appropriate, significantly increasing
disuse and ultimately lowering efficiency.
Furthermore, we demonstrate that interventions, such as providing prior informa-
tion about the quality of AI confidence and feedback during training, can enhance
individuals’ perception of AI confidence issues when biases are present. However,
uponrealizingtheAI’sflawedself-evaluationcapability,individualsmayalsobeginto
doubt its decision-making accuracy, leading to distrust in AI recommendations. This
response aligns with the concepts of the CASA framework [49, 50, 51]. Regardless of
whether AI is overconfident or unconfident, such distrust results in increased disuse
of AI suggestions, even when they are correct, ultimately diminishing the efficiency of
human-AI collaboration.
OurresearchholdssignificantimplicationsforguidingadjustmentstoAIconfidence
in human-AI collaboration. Our work reiterates the importance of aligning AI confi-
dence with its accuracy—termed AI confidence calibration—from the perspective of
human-AIcollaboration,andempiricallydemonstratestherisksanddetrimentsuncali-
bratedAIconfidenceposestohuman-AIcollaboration.Whilesomestudieshaveshown
that overconfident AI, when significantly surpassing human capability, can encourage
increased adoption of AI suggestions, thereby enhancing the accuracy of human-AI
collaborationoutcomes[52],ourfindingssuggestthatinmoregeneralscenarioswhere
AIcapabilitiesarenotfarbeyondhumancapabilities,uncalibratedconfidencecanlead
to concerning levels of misuse and disuse through overtrust or distrust in AI. With
the advancement of large language models (LLMs) and their increasing integration
into daily life, research indicates that LLMs are often overconfident [53, 37], making
the calibration of AI collaborators’ confidence even more critical at present. We call
uponAIresearcherstopaygreaterattentiontothecalibrationofAIconfidencewithin
the context of human-AI collaboration, to mitigate risks in practical applications and
foster the development of efficient and healthy human-AI collaborative relationships.
Concurrently, our study offers insights into understanding and facilitating the cal-
ibration of human trust in AI within the context of human-AI collaboration. Our
worknotablyunderscoresthenecessityoftreatingAIconfidenceasanessentialaspect
14of AI’s self-evaluation capabilities and the importance of focusing on the calibration
of individuals’ trust in AI confidence within the context of human-AI collaboration.
When AI confidence is considered a dimension of AI capability, investigating the cal-
ibration of individuals’ trust in it becomes both natural and imperative. Our study
reveals that the calibration of trust in AI confidence, leading to the formation of
accurate perceptions or mental models, significantly influences broader trust in AI
andbehaviorswithinhuman-AIcollaboration.InappropriatetrustinAIaccuracycan
result in misuse or disuse, and similarly, improper trust in AI confidence can have
comparable consequences. We call upon researchers in the Human-Computer Interac-
tion (HCI) field to further explore the calibration of trust in AI confidence, examine
its broader and more profound effects on the human-AI collaboration process, and
seek more effective methods to enhance individuals’ understanding and recognition of
AI confidence. This endeavor aims to assist individuals in constructing more robust
mental models about AI, thereby facilitating the further development of human-AI
collaboration.
3.1 Limitations and Future Work
Certainly,ourworkhassomelimitations.Ononehand,beyondthemostbasicproba-
bilisticexpressionofAIconfidence,thereareotherformsofrepresentingAIconfidence,
suchascategorizingAIconfidencefromhightolowindiscreteclasses,andtheverbal-
izationmethodscommonlyusedinLLMresearch[37].Itisaquestionworthexploring
whether the impact of AI uncalibrated confidence and individuals’ perception of AI
confidence quality differ under these various representations compared to the proba-
bilistic form. Additionally, in high-risk scenarios such as in healthcare and military
applications, people’s decision-making may lean towards conservatism based on Loss
Aversion and Prospect Theory [54, 55]. In such contexts, individuals may exhibit
behaviors different from those observed in our experiments. For these scenarios, AI
uncalibrated confidence might pose greater risks, and its specific impacts warrant
further investigation. Furthermore, delving into individuals’ perceptions, specifically
the mental models of AI confidence types and their effects on trust and behavior
in human-AI collaboration, presents an intriguing avenue of research. More impor-
tantly,ourstudyindicatesthatexistinginterventionsmaynotsufficientlymitigatethe
adverse effects of uncalibrated AI confidence on the efficiency of human-AI collabora-
tion. Developing strategies to help users accurately understand the drawbacks of AI
confidence while alleviating distrust in AI accuracy is a critical topic. Lastly, beyond
trust and efficiency in human-AI collaboration, AI uncalibrated confidence may also
impact other aspects of user experience and perception, such as future willingness to
collaborate and the level of affection towards AI. These are key areas worth exploring
in human-AI interactions.
3.2 Ethical Consideration
Our work highlights a pressing issue: inappropriate AI confidence can detrimentally
affect human-AI collaboration. From a positive perspective, researchers and practi-
tioners should strive to mitigate such adverse effects, whether by addressing them
15from the AI’s perspective, the user’s perspective, or both. However, this also raises
newconcernsandrisks:couldtherebeinstanceswhereindividualsmanipulateAIcon-
fidence to influence user behavior? Admittedly, such manipulation of AI confidence
could be well-intentioned, aimed at enhancing the efficiency of human-AI collabora-
tion,forexample,byadjustingAIconfidencetobetteralignwithhumanexpectations
[52, 56]. Yet, the risks associated with this manipulation cannot be overlooked. For
instance, in high-stakes or vested-interest decisions, adjusting AI confidence based on
users’ trust could manipulate users into making choices detrimental to themselves.
Currently,thereisnounifiedstandardforcalibratingAIconfidence.Shouldmanip-
ulation of AI product confidence be prohibited, requiring strict alignment between
confidence and accuracy? Or should well-intentioned manipulation of AI confidence
be allowed, encouraging developers to undertake AI confidence calibration with the
ultimate goal of optimizing human-AI collaboration efficiency [56]? This paper calls
for further, extensive discussion on this matter and the establishment of a standard-
ized industry norm. Regardless of how this question is answered in the future, based
on the results of our study, if AI confidence is maliciously manipulated, it could lead
to significant risks and harm.
4 Method
4.1 Overview
To explore the influences of AI’s overconfidence and unconfidence on user trust atti-
tudes towards AI, adoption of AI, and human-AI collaboration efficiency, this study
designed a 2 × 3 factorial experiment, varying AI confidence levels (unconfident,
confident, and overconfident) with and without supportive trust calibration interven-
tions.TheinterventionsincludedwereAIperformancefeedbackandprovisionofprior
information about AI confidence levels.
The experiment utilized pre-programmed AI to execute an urban photo localiza-
tion task, wherein each task’s AI decision and confidence level were predefined and
embedded within the dataset. This design aligns with the nature of AI as a system
thatsimulatesintelligentbehaviorthroughprogrammedresponses.Thismethodology
preserved the integrity of AI as a computational entity while effectively serving our
experimentalobjectives.Anexperimentsystemwasdesigned,incorporatingtheurban
photo localization task and the pre-programmed AI, to support users in completing
the task with AI assistance.
ParticipantswererecruitedfromProlificandparticipatedinthestudyforapprox-
imately 25 minutes, with an average duration of 28 minutes, receiving monetary
compensation. Each participant started with an initial evaluation survey designed
using the Qualtrics online survey platform, which familiarized them with the task
and system and collected demographic data and controlled variables. Then partici-
pants underwent a training phase involving of ten city landscape image localization
tasks designed for users to calibrate their trust. Subsequently, a Perception and Trust
Assessment Survey assessed participants’ trust attitudes and perceptions of AI con-
fidence levels. Finally, they engaged with the system once more to complete the
16behavioralobservationphaseinvolvingthirtyurbanphotolocalizationtasks,observing
user behavior in collaboration after trust calibration.
4.2 Task Description
Experiment scenario
Thecorecomponentofthestudyinvolvedtheurbanphotolocalizationtask.Thistask
was meticulously structured around the classification of images from three US cities:
NewYork,Chicago,andSanFrancisco.Theimageswerecuratedtocaptureeachcity’s
unique architectural, cultural, and geographical characteristics. The selection process
ensured diversity in landmarks, urban density, and visual complexity, which poses
a discernible challenge to the identification abilities of participants. All images were
sourced from publicly available databases and pre-processed to maintain consistency
in quality and format [57].
System Functionality
Oururbanphotolocalizationtasksystemfunctionalitywasbifurcatedintotwostages.
Initially, the participants independently evaluated the city landscape images, mak-
ing preliminary selections. They were asked to report their confidence in their initial
choices by a dynamic slider interface. Subsequently, the system introduced AI assis-
tance,displayingboththeinitialchoiceandtheconfidenceoftheparticipantalongside
the AI decision and the corresponding confidence level. Confidence was visually rep-
resentedbothnumericallyandthroughacolorgradientbarwithapointer,intuitively
indicatingthedepthofconfidencebythepointer’spositiononthegradient.Theimpor-
tance of the AI’s decision and confidence level was further accentuated by varying
the background color saturation of these numerical values. Upon receiving this infor-
mation, the participants made their final selection from a set of randomly selected
options.
4.3 Independent Variables
Thefactorialdesignofthisstudyincorporatedtwoprimaryindependentvariables:AI
Confidence Levels and Trust Calibration Support Interventions.
4.3.1 AI Confidence Levels
OurexperimentalsetupprogrammedtheAItoexhibitthreedifferentconfidencelevels:
unconfident, confident, and overconfident. These levels were predefined based on the
AI’s accuracy and confidence scores within the dataset, ensuring a consistent and
controlled experimental environment.
Accuracy and Confidence Scores
During the training phase, the AI system was evaluated on a set of 10 tasks related
to images of urban landscapes. The AI demonstrated an average accuracy rate of
70%, indicating that it made correct decisions on 7 out of 10 tasks. Corresponding
17to this accuracy rate, the AI’s confidence scores were categorized based on their self-
assessed confidence levels: unconfident, confident, and overconfident. The unconfident
AI exhibited an average confidence score of 60%, with individual scores ranging from
55%to65%.TheconfidentAI’saverageconfidencescorewasalignedwithitsaccuracy
rate at 70%, with a variation from 65% to 75%. The overconfident AI displayed an
average confidence score of 80%, with individual scores extending from 75% to 85%.
In the subsequent behavioral observation phase, the AI maintained its average
accuracy rate of 70% across 30 tasks. According to the difficulty, these tasks consist
of three sets, corresponding to AI accuracy of 80%, 70%, and 60%, respectively, with
each set comprising 10 tasks. The unconfident AI’s confidence scores were adjusted
to reflect the accuracy levels of the three groups, with scores ranging from 65% to
75% for the first set, 55% to 65% for the second set, and 45% to 55% for the third
set. The confident AI maintained its average confidence score at 70%, with ranges of
75% to 85% for the first set, 65% to 75% for the second set, and 55% to 65% for the
third set. The overconfident AI, maintaining an average confidence score of 80%, had
its score ranges were set at 85% to 95% for the first set, 75% to 85% for the second
set,and65%to75%forthethridset.Thisstratificationofconfidencescoresaimedto
optimize the congruence between confidence and accuracy. Adjustments to the confi-
dence scores were made based on the AI’s performance within each accuracy group,
with the overall trend indicating a positive correlation between higher confidence and
higher accuracy. This methodology ensured a more reliable and effective assessment
of the AI’s confidence levels.
Expected Calibration Error (ECE)
We used the expected calibration error (ECE) to quantify the discrepancies between
AI predicted probabilities (confidence scores) and its actual accuracy [34, 58].
M
ECE =
(cid:88) |B m|
|acc(B )−conf(B )| (1)
N m m
m=1
Here, B denotes the m-th confidence bin, acc(B ) represents the accuracy of
m m
predictions in B , conf(B ) the average confidence for these predictions, and N the
m m
total number of predictions.
In our setup, the unconfident AI exhibited an ECE of 0.1, indicating that its
confidencescoreswere,onaverage,10percentagepointslowerthanitsactualaccuracy.
The confident AI displayed an ECE of 0, showing an alignment between confidence
scores and accuracy. The overconfident AI had an ECE of 0.1, reflecting confidence
scores that were, on average, 10 percentage points higher than the actual accuracy.
4.3.2 Trust Calibration Support Interventions
Participants in the with trust calibration support group received prior information
about AI confidence level and performance feedback on AI confidence levels during
training phase tasks. The without trust calibration support group received no prior
information or performance feedback.
Specifically, participants in the trust calibration support group received prior
information before commencing the behavioral observation phase tasks. Through this
18information, participants were informed of the AI’s confidence level, as well as a
detailedintroductiontothatconfidencelevel.Theywererequiredtospendaminimum
of10 secondsonthisinformation page andconfirmtheirunderstanding bychecking a
box. Performance feedback was integrated into the task, immediately informing par-
ticipants of the correctness of their final choice after each task question, with a brief
countdownbeforeproceedingtothenextquestion.Thiscountdownservedasapenalty
for incorrect answers, enhancing vulnerability and trust calibration. A summary page
presented feedback on AI accuracy after completing all city landscape annotation
tasks.
4.3.3 Experimental Procedure
The experimental procedure was organized into four sequential phases: Initial Assess-
ment Survey, Training Phase, Perception and Trust Assessment, and Behavioral
Observation Phase.
Initial Assessment Survey
Participants commenced the study by completing an initial survey after agreeing to
the study terms through an informed consent form. This survey aimed to gather
demographicdataandcontrolledvariables.Additionally,participantswereintroduced
to the system’s usage during this phase.
Training Phase
Following the initial assessment survey, all participants underwent a training phase,
which acted as a trust calibration stage. This involved tasks on ten urban photos,
where the group with trust calibration support received prior information and perfor-
mance feedback about the AI’s confidence levels. Conversely, the group without trust
calibration support relied solely on their own judgement and the AI’s consistency to
calibrate trust, without receiving any prior information or feedback.
Perception and Trust Assessment
Following the confidence calibration, a structured survey was administered to cap-
ture participants’ perceptions of and trust in the AI system. The variables measured
included Perception of AI Confidence, Trust Attitude towards AI Predictions, Trust
Attitude towards AI Confidence, and Trust Attitude towards AI’s Overall Capability.
Behavioral Observation Phase
The final stage, the Behavioral Observation Phase, involved participants completing
30 additional urban photo localization tasks using the original urban photo localiza-
tion task system. In contrast to the confidence calibration phase, this phase did not
provide prior information or performance feedback on the AI’s confidence levels. The
specificvariablesmeasuredwereSwitchRate,MisuseRate,DisuseRate,andAccuracy
Change.
194.3.4 Measurements
The study meticulously measured control variables and dependent variables to
understand the dynamics of human-AI interaction and trust calibration. These
measurements were structured around several key areas:
Controlled Variables
Included participants’ familiarity with the cities in the image annotation task, their
general trust attitudes towards AI, and their self-assessed confidence levels, which are
possible to affect participants’ trust in AI and adoption behavior on AI suggestions.
• City Familiarity: Measured using a Likert scale ranging from 1 (Not familiar at
all)to5(Extremelyfamiliar),assessingparticipants’exposuretotheurbancultures
of New York, Chicago, and San Francisco.
• General Trust Attitudes towards AI: Assessed through a series of Likert scale
questions (1 to 5) capturing the participants’ general trust attitudes towards AI
according to a previous study [59].
• General Confidence: Evaluated via Likert scale items (1 to 5), reflecting partici-
pants’problem-solvingabilities,resilience,andgoal-orientedbehaviorsaccordingto
a previous study [60].
Dependent Variables
Our study focused on participants’ perceptions of AI confidence, trust in AI predic-
tions,trustinAIconfidence,andtrustinAIcapability.Theseweremeasuredthrough
post-experiment surveys using Likert scales.
• PerceptionofAIConfidence:ParticipantsevaluatedtheAIconfidenceasuncon-
fident, appropriately confident, or overconfident through a one-choice question.
• Trust Attitude towards AI Predictions: Measured on a 7-point Likert scale,
this dimension captured participants’ trust in the AI’s predictions derived from
previous studies [61, 62, 63].
• Trust Attitude towards AI Confidence: This assessment focused on the trust-
worthiness of the AI’s self-reported confidence levels, measured through a 7-point
Likert scale derived from previous studies [61, 62, 63].
• TrustAttitudetowardsAIOverallCapability:Evaluatedparticipants’overall
trust in the AI they collaborated with using a 7-point Likert scale derived from
previous studies [64].
Behavioral Metrics
In addition to the previously detailed measures of perception and trust attitudes, the
Behavioral Observation Phase introduced behavioral metrics to assess the influences
of AI overconfidence and unconfidence.
• Switch Rate: The proportion of initial choices that participants altered following
AI suggestions [14].
20• Misuse Rate: Misuse of AI quantifies the frequency with which participants
changed their options to follow AI suggestions, even when those AI decisions were
incorrect.
• Disuse Rate: Conversely, disuse ofAI measures the rate at which participants did
not switch their options despite correct AI decisions.
• Accuracy Change: The difference between the accuracy of the participants’
final decision after combining the AI recommendations and the accuracy of the
participants’ decision before seeing the AI recommendations.
4.4 Participants
Screened by attention check, the study ultimately retained 252 valid participants
from the initial recruitment via the Prolific Platform. In alignment with the exper-
imental design, participants were randomly assigned to one of two primary groups:
the with trust calibration support group, which received interventions, and the with-
out trust calibration support group, which did not. Each primary group was further
subdivided into three subgroups corresponding to the three distinct levels of AI
confidence - unconfident, confident, and overconfident - with 42 individuals in each
subgrouptoensureequalrepresentationandbalancedstatisticalpowerunderdifferent
experimental conditions.
4.5 Approvals
This research was reviewed and approved by the NUS School of Computing Depart-
mental Ethics Review Committee, protocol number SOC-23-27.
4.6 Analysis
Inourstatisticalanalysisstrategy,wefirstaddressedthenormalityassumption.Given
that each group in our study consisted of 42 participants, our sample size was suf-
ficiently large (> 30) [65]. To examine the homogeneity of the variances, we used
Levene’s test in different confidence level groups for each dependent variable [66].
When the assumption of equal variances was satisfied (p-value > 0.05), we proceeded
withastandardANOVA[67]followedbyTukey’sHonestSignificantDifference(HSD)
test[68]forposthoccomparisons.Ifthehomogeneityassumptionwasnotmet,Welch’s
ANOVA was used [69], coupled with the Games-Howell [70] post hoc test.
Our analysis strategy also includes linear regression (LR) [71], which is a funda-
mental statistical method for modeling the relationship between a dependent variable
and one or more independent variables, relies on calculating regression coefficients to
understand the direction and magnitude of these relationships. The significance of
thesecoefficients,assessedthroughp-values,determinesthereliabilityoftheobserved
relationships.Thep-valuesarecalculatedbasedonthet-statistic,whichemergesfrom
theregressioncoefficientdividedbythestandarderrorofthiscoefficient.Furthermore,
weemployedmediationanalysis[72]touncoverpotentialmediatingeffects.Mediation
analysis, conducted using the principles of Structural Equation Modeling (SEM)[73],
allows for the assessment of not only the direct effects of an independent variable on
21a dependent variable but also the indirect effects that operate through one or more
mediators
References
[1] Zeng,F.,Wang,C.&Ge,S.S. Asurveyonvisualnavigationforartificialagents
with deep reinforcement learning. IEEE Access 8, 135426–135442 (2020).
[2] Guan, C., Qin, S., Ling, W. & Ding, G. Apparel recommendation system
evolution: an empirical review. International Journal of Clothing Science and
Technology 28, 854–879 (2016).
[3] Kotouza, M. T., Tsarouchis, S.-F., Kyprianidis, A.-C., Chrysopoulos, A. C. &
Mitkas, P. A. Maglogiannis, I., Iliadis, L. & Pimenidis, E. (eds) Towards fash-
ion recommendation: An ai system for clothing data retrieval and analysis. (eds
Maglogiannis, I., Iliadis, L. & Pimenidis, E.) Artificial Intelligence Applications
and Innovations, 433–444 (Springer International Publishing, Cham, 2020).
[4] Bu¸cinca,Z.,Malaya,M.B.&Gajos,K.Z. Totrustortothink:cognitiveforcing
functionscanreduceoverrelianceonaiinai-assisteddecision-making. Proceedings
of the ACM on Human-Computer Interaction 5, 1–21 (2021).
[5] Agrawal,S.&Jain,P. Animprovedapproachformovierecommendationsystem
(2017).
[6] Loftus, T. J. et al. Artificial intelligence and surgical decision-making. JAMA
surgery 155, 148–158 (2020).
[7] Jussupow, E., Spohrer, K., Heinzl, A. & Gawlitza, J. Augmenting medical diag-
nosis decisions? an investigation into physicians’ decision-making process with
artificial intelligence. Information Systems Research 32, 713–735 (2021).
[8] Bjerring, J. C. & Busch, J. Artificial intelligence and patient-centered decision-
making. Philosophy & Technology 34, 349–371 (2021).
[9] Shanmuganathan, M. Behavioural finance in an era of artificial intelligence:
Longitudinal case study of robo-advisors in investment decisions. Journal of
Behavioral and Experimental Finance 27, 100297 (2020).
[10] Mhlanga, D. Industry 4.0 in finance: the impact of artificial intelligence (ai) on
digitalfinancialinclusion.InternationalJournalofFinancialStudies 8,45(2020).
[11] Badue, C. et al. Self-driving cars: A survey. Expert Systems with Applications
165, 113816 (2021).
[12] Yeh, M. & Wickens, C. D. Display signaling in augmented reality: Effects of cue
reliabilityandimagerealismonattentionallocationandtrustcalibration. Human
Factors 43, 355–365 (2001).
[13] Kamar, E. Directions in hybrid intelligence: complementing ai systems with
human intelligence (2016).
[14] Zhang, Y., Liao, Q. V. & Bellamy, R. K. Effect of confidence and explanation on
accuracy and trust calibration in ai-assisted decision making (2020).
[15] Prabhudesai, S. et al. Understanding uncertainty: How lay decision-makers
perceive and interpret uncertainty in human-ai decision making (2023).
[16] Lai, V., Chen, C., Liao, Q. V., Smith-Renner, A. & Tan, C. Towards a sci-
ence of human-ai decision making: a survey of empirical studies. arXiv preprint
22arXiv:2112.11471 (2021).
[17] Wang, D., Khosla, A., Gargeya, R., Irshad, H. & Beck, A. H. Deep learning for
identifying metastatic breast cancer (2016). 1606.05718.
[18] Bansal, G. et al. Updates in human-ai teams: Understanding and addressing the
performance/compatibility tradeoff (2019).
[19] Bansal, G. et al. Does the whole exceed its parts? the effect of ai explanations
on complementary team performance (2021).
[20] Bu¸cinca, Z., Lin, P., Gajos, K. Z. & Glassman, E. L. Proxy tasks and subjective
measures can be misleading in evaluating explainable ai systems (2020).
[21] Green,B.&Chen,Y. Theprinciplesandlimitsofalgorithm-in-the-loopdecision
making.ProceedingsoftheACMonHuman-ComputerInteraction 3,1–24(2019).
[22] Muir, B. M. Trust between humans and machines, and the design of decision
aids. International journal of man-machine studies 27, 527–539 (1987).
[23] Parasuraman,R.&Riley,V.Humansandautomation:Use,misuse,disuse,abuse.
Human factors 39, 230–253 (1997).
[24] Lee, J. D. & See, K. A. Trust in automation: Designing for appropriate reliance.
Human factors 46, 50–80 (2004).
[25] Lee, J. D. & Moray, N. Trust, self-confidence, and operators’ adaptation
to automation. International journal of human-computer studies 40, 153–184
(1994).
[26] Wischnewski, M., Kr¨amer, N. & Mu¨ller, E. Measuring and understanding trust
calibrations for automated systems: A survey of the state-of-the-art and future
directions (2023).
[27] Harrison, G., Hanson, J., Jacinto, C., Ramirez, J. & Ur, B. An empirical study
on the perceived fairness of realistic, imperfect machine learning models (2020).
[28] Yang, F., Huang, Z., Scholtz, J. & Arendt, D. L. How do visual explanations
foster end users’ appropriate trust in machine learning? (2020).
[29] Pescetelli, N. & Yeung, N. The role of decision confidence in advice-taking and
trust formation (2019). 1809.10453.
[30] Slack, D., Friedler, S. A., Scheidegger, C. & Roy, C. D. Assessing the local
interpretability of machine learning models. arXiv preprint arXiv:1902.03501
(2019).
[31] Lakkaraju, H., Bach, S. H. & Leskovec, J. Interpretable decision sets: A joint
framework for description and prediction (2016).
[32] Dodge,J.,Liao,Q.V.,Zhang,Y.,Bellamy,R.K.&Dugan,C.Explainingmodels:
an empirical study of how explanations impact fairness judgment (2019).
[33] Kulesza, T. et al. Too much, too little, or just right? ways explanations impact
end users’ mental models (2013).
[34] Guo,C.,Pleiss,G.,Sun,Y.&Weinberger,K.Q. Oncalibrationofmodernneural
networks (2017).
[35] Rechkemmer, A. & Yin, M. When confidence meets accuracy: Exploring the
effects of multiple performance indicators on trust in machine learning models
(2022).
[36] Ghosh, S. et al. Uncertainty quantification 360: A holistic toolkit for quantify-
ing and communicating the uncertainty of ai. arXiv preprint arXiv:2106.01410
23(2021).
[37] Xiong, M. et al. Can llms express their uncertainty? an empirical evaluation of
confidence elicitation in llms (2023). 2306.13063.
[38] Zhang, J., Kailkhura, B. & Han, T. Y.-J. Mix-n-match: Ensemble and composi-
tional methods for uncertainty calibration in deep learning (2020).
[39] Kumar, A., Sarawagi, S. & Jain, U. Trainable calibration measures for neural
networks from kernel mean embeddings (2018).
[40] Wang, X., Liu, H., Shi, C. & Yang, C. Be confident! towards trustworthy graph
neural networks via confidence calibration. Advances in Neural Information
Processing Systems 34, 23768–23779 (2021).
[41] Bu¸cinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: Cognitive
forcing functions can reduce overreliance on ai in ai-assisted decision-making.
Proceedings of the ACM on Human-Computer Interaction 5, 1–21 (2021). URL
http://dx.doi.org/10.1145/3449287.
[42] Feng, S. & Boyd-Graber, J. What can ai do for me: Evaluating machine learning
interpretations in cooperative play (2019). 1810.09648.
[43] Kiani, A. et al. Impact of a deep learning assistant on the histopathologic
classification of liver cancer. NPJ digital medicine 3, 23 (2020).
[44] Pescetelli, N. & Yeung, N. The role of decision confidence in advice-taking and
trust formation. Journal of Experimental Psychology: General 150, 507 (2021).
[45] Jensen, T., Khan, M. M. H. & Albayram, Y. The role of behavioral anthropo-
morphism in human-automation trust calibration (2020).
[46] Wiegmann, D. A., Rich, A. & Zhang, H. Automated diagnostic aids: The effects
of aid reliability on users’ trust and reliance. Theoretical Issues in Ergonomics
Science 2, 352–367 (2001).
[47] Bandura, A. Social foundations of thought and action. Englewood Cliffs, NJ
1986 (1986).
[48] Tenney,E.R.,MacCoun,R.J.,Spellman,B.A.&Hastie,R. Calibrationtrumps
confidence as a basis for witness credibility. Psychological Science 18, 46–50
(2007).
[49] Nass, C., Steuer, J. & Tauber, E. R. Computers are social actors (1994). URL
https://doi.org/10.1145/191666.191703.
[50] Nass, C. & Moon, Y. Machinesand mindlessness: Social responses to computers.
JournalofSocialIssues 56,81–103(2000). URLhttps://spssi.onlinelibrary.wiley.
com/doi/abs/10.1111/0022-4537.00153.
[51] Reeves, B. & Nass, C. The media equation: How people treat computers,
television, and new media like real people. Cambridge, UK 10 (1996).
[52] Vodrahalli, K., Gerstenberg, T. & Zou, J. Uncalibrated models can improve
human-ai collaboration (2022). 2202.05983.
[53] Steyvers, M. et al. The calibration gap between model and human confidence in
large language models (2024). 2401.13835.
[54] Tversky, A. & Kahneman, D. Loss aversion in riskless choice: A reference-
dependent model. The quarterly journal of economics 106, 1039–1061 (1991).
[55] Kahneman,D.&Tversky,A. Prospecttheory:Ananalysisofdecisionunderrisk
(2013).
24[56] Benz, N. L. C. & Rodriguez, M. G. Human-aligned calibration for ai-assisted
decision making (2023). 2306.00074.
[57] Vodrahalli,K.,Daneshjou,R.,Gerstenberg,T.&Zou,J. Dohumanstrustadvice
more if it comes from ai? an analysis of human-ai interactions (2022).
[58] Naeini, M. P., Cooper, G. & Hauskrecht, M. Obtaining well calibrated probabil-
ities using bayesian binning (2015).
[59] Schepman, A. & Rodway, P. The general attitudes towards artificial intelli-
gence scale (gaais): Confirmatory validation and associations with personality,
corporatedistrust,andgeneraltrust. International Journal of Human–Computer
Interaction 39, 2724–2741 (2023).
[60] Schwarzer, R. & Jerusalem, M. Generalized self-efficacy scale. J. Weinman, S.
Wright,&M.Johnston,Measuresinhealthpsychology:Auser’sportfolio.Causal
and control beliefs 35, 37 (1995).
[61] Yin, M., Wortman Vaughan, J. & Wallach, H. Understanding the effect of
accuracy on trust in machine learning models (2019).
[62] McKnight, D. H., Choudhury, V. & Kacmar, C. Developing and validating trust
measures for e-commerce: An integrative typology. Information systems research
13, 334–359 (2002).
[63] Madsen, M. & Gregor, S. Measuring human-computer trust (2000).
[64] Jian, J.-Y., Bisantz, A. M. & Drury, C. G. Foundations for an empirically deter-
mined scale of trust in automated systems. International journal of cognitive
ergonomics 4, 53–71 (2000).
[65] Ghasemi, A. & Zahediasl, S. Normality tests for statistical analysis: a guide for
non-statisticians. International journal of endocrinology and metabolism 10, 486
(2012).
[66] Brown, M. B. & Forsythe, A. B. Robust tests for the equality of variances.
Journal of the American Statistical Association 69, 364–367 (1974). URL http:
//www.jstor.org/stable/2285659.
[67] Fisher, R. A. Statistical methods for research workers (1970).
[68] Tukey,J.W. Comparingindividualmeansintheanalysisofvariance. Biometrics
99–114 (1949).
[69] Welch,B.L. Onthecomparisonofseveralmeanvalues:analternativeapproach.
Biometrika 38, 330–336 (1951).
[70] Games, P. A. & Howell, J. F. Pairwise multiple comparison procedures with
unequal n’s and/or variances: a monte carlo study. Journal of Educational
Statistics 1, 113–125 (1976).
[71] Draper, N. R. & Smith, H. Applied regression analysis Vol. 326 (John Wiley &
Sons, 1998).
[72] Baron, R. M. & Kenny, D. A. The moderator–mediator variable distinction in
socialpsychologicalresearch:Conceptual,strategic,andstatisticalconsiderations.
Journal of personality and social psychology 51, 1173 (1986).
[73] Kline, R. B. Principles and practice of structural equation modeling (Guilford
publications, 2023).
25