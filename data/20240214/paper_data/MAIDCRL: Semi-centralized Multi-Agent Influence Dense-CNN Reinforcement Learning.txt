MAIDCRL: Semi-centralized Multi-Agent Influence
Dense-CNN Reinforcement Learning
Ayesha Siddika Nipu Siming Liu Anthony Harris
Department of Computer Science Department of Computer Science Department of Computer Science
Missouri State University Missouri State University Missouri State University
Springfield, U.S. Springfield, U.S. Springfield, U.S.
Nipu62@MissouriState.edu SimingLiu@MissouriState.edu Anthony999@MissouriState.edu
Abstract—Distributed decision-making in multi-agent systems individual agent’s perspective no longer exists in MAS due to
presents difficult challenges for interactive behavior learning the dynamic activities of other agents. These non-stationary
in both cooperative and competitive systems. To mitigate this
states lead to significant stability issues to MARL in the
complexity, MAIDRL presents a semi-centralized Dense Rein-
learningprocess.Inaddition,MASthemselvesintroduceextra
forcementLearningalgorithmenhancedbyagentinfluencemaps
(AIMs), for learning effective multi-agent control on StarCraft cooperative and competitive learning tasks to achieve team
Multi-Agent Challenge (SMAC) scenarios. In this paper, we objectives for individual agent decision-making. A popular
extendtheDenseNetinMAIDRLandintroducesemi-centralized approach is to provide MARL with complete information for
Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL,
training agents, commonly named centralized learning.
byincorporatingconvolutionallayersintothedeepmodelarchi-
Our prior work introduced agent influence maps (AIM), ag-
tecture,andevaluatetheperformanceonbothhomogeneousand
heterogeneousscenarios.TheresultsshowthattheCNN-enabled gregated into a global multi-agent influence map (MAIM),
MAIDCRL significantly improved the learning performance which is then used in addition to local agent observations
and achieved a faster learning rate compared to the existing for fine-grained decision-making. We combined MAIRL and
MAIDRL, especially on more complicated heterogeneous SMAC
the DenseNet model architecture, which defined Multi-Agent
scenarios. We further investigate the stability and robustness of
Influence Dense Reinforcement Learning (MAIDRL), and we
ourmodel.Thestatisticsreflectthatourmodelnotonlyachieves
higherwinningrateinallthegivenscenariosbutalsobooststhe evaluated MAIDRL’s performance on StarCraft Multi-Agent
agent’s learning process in fine-grained decision-making. Challenge (SMAC) scenarios in a real-time strategy (RTS)
IndexTerms—Deepreinforcementlearning,convolutionalneu- game,StarCraftII(SC2)[2].Byextractingadescriptiverepre-
ral network, multi-agent system, StarCraft II, MAIDRL
sentationfromthecompleteglobalinformationandcombining
it with the DenseNet architecture, MAIDRL demonstrated
I. INTRODUCTION
a significant improvement in centralized, decentralized, and
Artificial Intelligence (AI) has advanced significantly in hybridized methods. In this study, we extended MAIDRL
manyaspectsofourlivesinrecentyears.Therapidprogressin with the use of a Convolutional Neural Network (CNN) and
AI has reached the human-level or even outperformed human introduce Multi-Agent Dense-CNN Reinforcement Learning
champions in a wide variety of tasks including autonomous (MAIDCRL) for solving MAS problems. This reformulation
driving,gameplaying,proteinfolding,androbotics.However, ofextractingspatialfeaturesfromMAIMbyutilizingmultiple
most of these achievements of AI are limited in single-agent CNN layers further improved the learning performance of
systems where interaction among agents is not considered. MAIDCRL in a variety of SMAC scenarios. In order to
Since there are a large number of applications that involve evaluate how the influence map (IM) affects the multi-agent
cooperation and competition between multiple agents, we are learning performance of MAIDCRL, we therefore performed
interested in AI techniques that work not only on single- a rigorous analysis of the agent’s behavior and found several
agent systems but also multi-agent systems (MAS). Recently, fascinating behavioral attributes determined by the agent in
deep Reinforcement Learning (DRL) has been considered for the testing scenarios.
some of the most effective AI techniques to solve problems
in many domains, i.e. AlphaGo and AlphaStar [1]. Extending
II. RELATEDWORK
DRL to enable interaction and communication among agents Extensiveresearchhasbeenconductedonapplyingdifferent
is critical to building artificially intelligent systems in multi- variants of RL algorithms for controlling agents in a wide
agentenvironments.Oneofthemainchallengeofmulti-agent variety of cooperative, competitive, and mixed MAS. Nair
RL (MARL) is that the canonical RL algorithms including et al. introduced parallel learning with a distributed network
Q-Learning and policy gradient algorithms do not generalize and a shared memory replay to split the learning tasks across
well to MAS due to the exponential growth of the number of multiple instances of simulations, effectively increasing the
statesasthenumberofagentsincreases.Thesecondchallenge explorationspeedatwhichagentslearn[3].Liuetal.modeled
in MARL is that the stationary Markovian property from an the problem of MARL for a constrained, partially observable
4202
beF
21
]IA.sc[
1v09870.2042:viXraFig. 1: Outline of MAIDCRL architecture.
MDP, where the agents need to maximize a global reward oftheenvironmentandperformanactionbasedontheagent’s
function subject to both peak and average constraints. They observation. S × {A ,...,A } → S′, r denotes the state
1 N
proposed a novel algorithm, CMIX, to enable centralized transition from S to S′ in a Markov game where each agent
training and decentralized execution under those constraints performs action following a policy π in each environmental
[4],[5].Rashidetal.introducedQMIX,anetworkfortraining stepandreceivesasharedrewardr.Thetheoreticalmaximum
decentralized policies based on joint-action properties, which reward in each episode is scaled to a non-negative value
improvedtheperformanceofSC2significantly[6].Therehave between 0 to 20, as defined in SMAC. The reward is a
alsobeenseveralresearchonintegratingConvolutionalNeural shared reward to the whole team instead of individual agents
Networks into DRL applications. Stanescu et al. presented considering damage dealt to the enemy, points on a unit kill,
a CNN for RTS game state evaluation that goes beyond and a bonus for victory.
commonly used material-based evaluations by also taking
spatial relations between units into account [7]. A Deep A. Experimental Features
convolutional neural network (DCNN) has been developed
Our experiments are performed using Python, NumPy, and
by Kondo and Matsuzaki [8] where they adopted supervised
Tensorflow as the framework for all the RL training and eval-
learning with multiple layers of convolutional networks and
uation. For statistical results, we evaluate the CNN-enabled
found exceptional scores. Other notable applications which
MAIDCRLon31randomseedsindifferentSMACscenarios.
enabled us to further investigate the broad spectrum of CNN
Each experiment runs for a total number of 1600 episodes
are checking electricity prices, multi-microgrid co-operative
consideringalargenumberoftuningparametersintroducedin
systems,mimickingGoExperts[9]etc.Intheaboveresearch,
CNN architecture. MAIDCRL is built on top of the standard
they mainly focused on the application of CNN whereas, in
Advantage Actor-Critic (A2C) algorithm with separate neural
our experiment, we are addressing the state representation
network(NN)modelsforbothactorandcritic.Weintroduced
and global information abstraction through the combination
a hybrid ε-greedy, softmax approach called ε-soft that starts
of MAIM and DCNN to provide shared goals and encourage
with an initial value (ε ) of 1 and decreases throughout the
collaborative behavior learning among agents. 0
overall running process.
III. METHODOLOGY
B. Multi-AgentInfluenceDense-CNNReinforcementLearning
In this research, we have evaluated the multi-agent learning
performance of CNN-enabled MAIDCRL on SMAC plat- In our A2C RL configuration, we utilize two separate NN
form.Weconductedourexperimentsinselectedhomogeneous components without sharing neural layers between the actor
scenarios including 3m, 8m, 25m where the numeric value and the critic networks. Each network contains one of the
denotes the number of active marines in each team, and one DenseNet grouping layers with three 256-neuron dense layers
heterogeneous scenario 2s3z in which two stalkers and three in the dense block. Our A2C design contains one single
zealotsworktogethertodefeattheequalnumberofopponents controller that manages each agent individually based on the
at the beginning of each game. The SMAC game episodes agent’s observation, and the resulting NN allow for faster
are usually modeled as a Markov game, which is a multi- learning because the parameters in the neural network are
agent extension of MDPs [10]. A Markov game containing updated based upon the parallel exploration of each agent. At
N agents comprises a set of states S that shows the status the terminal state of each episode, the parameters θ of the
A
of the agents and the environment, and a set of actions actor network are updated using the gradient ascent equation
A 1,A 2,...,A N and observations O 1,O 2,...,O N for each of as shown in Equation 1 [11].
the N agents. For an individual agent in the Markov game,
we model the surrounding friendly and enemy units as a part ∇ log π(a |s )(Qπ(a ,s ;θ )−Vπ(s ;θ )) (1)
θA t t t t A t C(a)AverageoftheRunningAverageEpisodeRewardon25m. (a)TotalNumberofWinningAmongAllSeeds
(b)AverageoftheRunningAverageEpisodeRewardon2s3z. (b)AverageNumberofEpisodesforFirstWinning
Fig. 2: Results of MAIDCRL on different scenarios. Fig. 3: Robustness of MAIDCRL
TABLE I: Performance Comparison between MAIDCRL and
IV. RESULTSANDDISCUSSION
MAIDRL on Extended Scenarios
WeevaluatedtheMAIDCRLperformancefromthreediffer-
entaspects:theaveragerunningepisodereward,theaverageof
Scenario Method Min Max Avg Std
3m MAIDRL 4.29 17.14 11.01 4.19 totalnumberofwinnings,andtheaveragenumberofepisodes
MAIDCRL 5.01 16.98 14.84 3.12 for achieving the first win. These criteria are used to evaluate
8m MAIDRL 5.01 16.93 14.77 3.77
the overall performance, overall robustness, and the speed of
MAIDCRL 6.82 18.79 15.65 3.81
25m MAIDRL 6.45 13.59 11.82 0.94 the learning algorithms respectively.
MAIDCRL 9.22 16.09 12.95 1.25
2s3z MAIDRL 5.32 13.70 10.33 3.43 A. MAIDCRL Learning Performance
MAIDCRL 5.12 18.88 12.14 2.25
Fig. 2 shows the results received from MAIDCRL and
MAIDRLoncomplex25mand2s3zscenarios.Themaximum
In order to provide a shared goal and encourage collab- running average is improved by 18.39% on 25m and 37.81%
orative behavior learning among agents, we use a spatial on 2s3z. Fig. 2a and Fig. 2b illustrate that the learning
informationtechnique,AgentInfluenceMap(AIM),toextract is slow at the beginning for complicated SMAC scenarios.
and filter useful features from the local information of each However, MAIDCRL outperformed MAIDRL after 600 and
agent. We aggregate the AIMs from all the agents on the map 800 episodes on average for 25m and 2s3z respectively.
and generate a Multi-Agent Influence Map (MAIM), and we Table I shows the detailed results of MAIDCRL and
selected the dimensionality of 64 × 64 as it outperformed MAIDRLcomparisononallscenariosincluding3m,8m,25m
other dimensions on MAIDCRL. We extended the existing and 2s3z. Note that the boldly marked values indicate the
MAIDRL model architecture by incorporating multiple con- best performance in the given scenarios. Our CNN-enabled
volutionallayersthataccepttheMAIMasaninput.Theinput MAIDCRL achieved a higher running average score over all
oftheinfluencemapisconcatenatedwithaCNNlayerforeach the testing scenarios. MAIDRL seems to achieve maximum
ofthethreegroupsinthedenseblocks.Multipleconvolutional reward on 3m scenario, whereas MAIDCRL surpassed by
layers have been incorporated in the new architecture with 32 18.79, 16.09, and 18.88 on 8m, 25m, and 2s3z respectively.
filtersforeachlayer,astriderateof1,andakernelsizeof3to
B. Robustness of MAIDCRL Architecture
extractspatialfeaturesonMAIM.Wealsousedeluactivation
function,amax-poolingof2×2,anddifferentdropoutsranges To evaluate the robustness of the presented approach, we
[0.1, 0.5]. We explored and compared the outcome of MAID- further analyzed the total number of winning instances over
CRL representation with MAIDRL and collected statistics the total of 31 seeds of both MAIDCRL and MAIDRL
for several different combinations. Fig. 1 demonstrates the on scenarios 3m, 8m and 2s3z. Figure 3a illustrates that
detailed architecture of our proposed MAIDCRL model. MAIDCRL won all runs on 3m, 28 on 8m, and 18 on 2s3zFig. 4: Learned Behavior on Different SMAC Scenarios
whereasMAIDRLcouldn’tbeatourproposedmodelinanyof V. CONCLUSIONANDFUTUREWORK
the challenging scenarios. We further had a close observation
In this study, we extended a semi-centralized RL model
on the learning speed of our solution. Figure 3b shows that
MAIDRL and introduced a new CNN-enabled MAIDCRL to
CNN-enabledMAIDCRLfoundthefirstwinningstrategyafter
solve various MARL systems. We evaluate the performance
770,950,and1053episodesonaveragefor3m,8mand2s3z
of MAIDCRL in homogeneous and heterogeneous SMAC
scenarios respectively. It takes 800, 1200, and 1240 episodes
scenarios of varying complexity for the statistical results.
for MAIDRL to win on 3m, 8m, and 25m respectively.
MAIDCRL demonstrated observable improvements in the
Therefore, CNN-enable MAIDCRL learned faster compared
overall performance, overall robustness, generalizability, and
to MAIDRL in all the given homogeneous and heterogeneous
peakperformanceshowninselectedscenarios.Thesecompar-
scenarios. Note that none of these two models achieved a
isonscouldleadtonewideasonhowtodesignbetterinfluence
winning strategy in 25m, thus we have skipped 25m scenario
mapsanddiscovernewfeaturesinMulti-Agentreinforcement
from this evaluation.
learning algorithms. Further investigation is required to test
our model in a wider range of heterogeneous environments
C. Learned Behavior Analysis
containing more complicated maps.
A qualitative comparison of learned behaviors is also an-
REFERENCES
alyzed to the best of our interest. The best performing RL
[1] J. Jumper et al., “Highly accurate protein structure prediction with
modelsoneachscenarioareselectedasthetrainedcontrollers
alphafold,”Nature,vol.596,no.7873,pp.583–589,2021.
in the test runs. While observing the episodes played by the [2] A. Harris and S. Liu, “Maidrl: Semi-centralized multi-agent reinforce-
MAIDCRL controller, we have noticed two major strategies, ment learning using agent influence,” in 2021 IEEE Conference on
Games(CoG). IEEE,2021,pp.01–08.
oneisprioritizingcollaborativeattackandanotheroneisrepo-
[3] A. Nair et al., “Massively parallel methods for deep reinforcement
sitioning with minimal movement after damage. The behavior learning,”arXivpreprintarXiv:1507.04296,2015.
specific characteristics and their impact is illustrated in the [4] S. Liu, S. J. Louis, and M. Nicolescu, “Comparing heuristic search
methods for finding effective group behaviors in rts game,” in 2013
following subsections. When we loaded our pre-trained RL
IEEECongressonEvolutionaryComputation. IEEE,2013,pp.1371–
model on 8m scenario, a collective movement was noticed 1378.
among the agents even though all the agents made their [5] ——,“Usingcigarforfindingeffectivegroupbehaviorsinrtsgame,”in
2013IEEEConferenceonComputationalInteligenceinGames(CIG).
decisions in a completely decentralized way. Therefore they
IEEE,2013,pp.1–8.
were more successful to win than randomly selecting their [6] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and
targetunits.Figure4adepictsthedominanceoftheMAIDCRL S. Whiteson, “Qmix: Monotonic value function factorisation for deep
multi-agent reinforcement learning,” in International Conference on
model over the SC2 AI agents where four controlled agents
MachineLearning. PMLR,2018,pp.4295–4304.
remained alive and worked in two groups targeting only one [7] M.Stanescu,N.A.Barriga,A.Hess,andM.Buro,“Evaluatingreal-time
enemyunit.Fromourobservationof25m,wefoundthatsome strategygamestatesusingconvolutionalneuralnetworks,”in2016IEEE
Conference on Computational Intelligence and Games (CIG). IEEE,
of the frontliners adjusted their position in a way so that they
2016,pp.1–7.
can reorganize their position with minimal movement in case [8] N. Kondo and K. Matsuzaki, “Playing game 2048 with deep convo-
ofhealingdamages.Figure4breflectsthattheagentsaremore lutional neural networks trained by supervised learning,” Journal of
InformationProcessing,vol.27,pp.340–347,2019.
reluctant than SC2 AI while positioning each of the units. As
[9] I. Sutskever and V. Nair, “Mimicking go experts with convolutional
a result, they actively focus on dealing with the damages of neural networks,” in International Conference on Artificial Neural
opponent’s attack. On the heterogeneous scenario 2s3z, we Networks. Springer,2008,pp.101–110.
[10] P.Pengetal.,“Multiagentbidirectionally-coordinatednets:emergence
observed that prioritizing the target played a significant role
of human-level coordination in learning to play StarCraft combat
while there are multiple types of units in the enemy team. games,”arXivpreprintarXiv:1703.10069,2017.
Figure 4c shows that our melee unit zealots moved passed [11] V.Mnihetal.,“Asynchronousmethodsfordeepreinforcementlearning,”
inInternationalConferenceonMachineLearning. PMLR,2016,pp.
enemy zealots in the front line and focused fire on enemy
1928–1937.
stalkers in the back first.