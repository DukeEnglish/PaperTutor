Ensuring Trustworthy and Ethical Behaviour
in Intelligent Logical Agents⋆
StefaniaCostantini
Universita`degliStudidiL’Aquila
DipartimentodiIngegneriaeScienzedell’Informazione,eMatematica
ViaVetoiosnc,Loc.Coppito,I-67010L’Aquila-Italy
stefania.costantini@univaq.it
Abstract. Autonomous Intelligent Agents are employed in many applications
uponwhichthelifeandwelfareoflivingbeingsandvitalsocialfunctionsmay
depend.Therefore,agentsshouldbetrustworthy.Aprioricertificationtechniques
(i.e.,techniquesappliedpriortosystem’sdeployment)canbeuseful,butarenot
sufficientforagentsthatevolve,andthusmodifytheirepistemicandbeliefstate,
andforopenMulti-AgentSystems,whereheterogeneousagentscanjoinorleave
thesystematanystageofitsoperation.Inthispaper,wepropose/refine/extend
dynamic(runtime)logic-basedself-checkingtechniques,devisedinordertobe
abletoensureagents’trustworthyandethicalbehaviour.
1 Introduction
The development, refinement, implementation and integration of methods for imple-
mentingIntelligentAgentssoastoensuretransparent,explainable,reliableandethical
behaviourisstronglyneeded.Thisisduetothefactthatagentsystemsarebeingwidely
adopted for many important autonomous applications upon which the life and wel-
fare of living beings and vital social functions may depend. Therefore, agents should
be trustworthy in the sense that they could be relied upon to do what is expected of
them, while not exhibiting unwanted behaviour. So, agents should not behave in im-
proper/forbidden/unethicalwaysgiventhepresentcontext,andtheyshouldnot devise
new behaviours that might be in contrast with their specification or however with the
user’s expectations. They should be transparent, in the sense of being able to explain
their actions and choices when required: in fact, it should always be possible to find
outhowandwhyanagent(or,moregenerally,anautonomoussystem)madeapartic-
ulardecision.Thispropertyisnotguaranteedbydefault,ratheritdescendsfromcare-
fuldesignmethodologies.Transparency(“explainability”)isvitalsince,incaseofany
kindofaccidentormisfunctioninginvolvinganautonomoussystem,itmustalwaysbe
possiblethatthefaultsorinadequaciesthatcausedtheproblembeidentifiedandfixed,
andaccountabilityestablished.Moreover,understandably,userswould(rationally)trust
morethoseautonomoussystemsthatcouldprovideanintelligibleexplanationoftheir
behavioursandchoices.Numerousstudieshavelinkedusers’trusttothedegreeofcon-
fidence to be interacting with a system that is verifiable and can provide explanations
whichareintelligible,toeachspecificcategoryofusers.
⋆Copyright©2020forthispaperbyitsauthors.UsepermittedunderCreativeCommonsLi-
censeAttribution4.0International(CCBY4.0).
4202
beF
21
]AM.sc[
1v74570.2042:viXraAgents should report to their users in case the interaction with the environment
results in the identification of new objectives to pursue. In fact, as cleverly observed
byStuartRusselinhisrecentbook[1],thecontinuousinteractionofanagentwiththe
externalenvironmentmayleadtheagenttoacquirenewknowledgeandalso,basedon
this knowledge, to develop new objectives (or new plans for existing objectives) that
mightnotbeinlinewiththeethicalprinciplesthatthehumandesignerbelievedtohave
instilled into the agent, and might even violate basic human principles and rights. To
avoidthis,agents’behaviourshouldbeverifiedinarigorousway,possiblyintegrating
differentverificationmethods,withtheaimtoensureadherenceofagents’operationto
theirspecification.Suchverificationshould,atleastpartly,happenatrun-time,inorder
tomonitortheagent’sdynamicbehaviours.
In this paper, we discuss the issue of agents’ verification, and its application to
ethics,i.e.,tomakingagentsystemstrustworthyw.r.t.theirexpectedethicalbehaviour.
We propose some technical contributions concerning run-time verification, restricting
ourselvestoagentsystemsbaseduponcomputationallogic.Manycomputational-logic-
basedagent-orientedlanguagesandframeworkstospecifyagentsandMulti-AgentSys-
tems(MAS)haveindeedbeendefinedovertime(forasurveyoftheselanguagesand
architecturesthereadermayrefer,amongmany,to[2,3,4]).Theiraddedvaluewithre-
specttonon-logicalapproachesistoprovidecleansemantics,readabilityandverifiabil-
ity,aswellastransparencyandexplainability‘bydesign’(oralmost),aslogicalproofs
can easily be transposed into natural-language explanations. All these approaches are
based (more or less directly, more or less adherently) on the so-called BDI (’Belief,
Desires, Intention’) model of agency [5], which formalizes means-end-reasoning and
thusencompassestheconceptsofperception,goals,actions,plans,commitments.The
BDImodelisinspiredbyBratman’stheoryofhumanpracticalreasoning[6],sosuch
conceptsareconsideredmentalattitudes,ormental/epistemicstatesofagents.
Pre-deployment verification methods for logical agents (also called ‘static’, or ‘a
priori’) are able to certify that agents will fulfil certain requisites of trustworthiness;
thismeansthattheywilldowhatisexpectedfromthem,andtheywillnotviolatecer-
tain rules of behaviour. However, this kind of verification can be not fully sufficient
foragentsthatwillrevisetheirbeliefsandobjectivesinconsequenceoftheinteraction
with a changing not-always-predictable environment. Dynamic, or Runtime, Verifica-
tion(RV)isthe“orthogonal”approach,aimedtoverifywhetherasoftwarecomponent
under observation respects or not, over time, during its operation (i.e., at ‘runtime’),
somegivenproperties.Approachestodynamicverificationaremeanttobelightweight,
so as not to be a burden on the system’s performances. The two kinds of verification
methodscanbeprofitablyexploitedinconjunction,asbothapproachesdonotexamine
allpossiblesystem’sbehaviours,butonlysomeexplicitlydesignatedones,asspecified
bythesystem’sdesigner.
Theworkpresentedinthepresentpaperstemsfromtheobservationthat,inmany
practicalcases,inparticularinagentsthatlearn,orinopenMASwhereagentscanjoin
andleavethesystem,itisnotpossibletofullypredictthesetofeventsthatwillbeper-
ceivedandconsideredbyanagent,whichmightevenleadtheagentitselftodevisenew
objectives and plans that depart from its expected/acceptable behaviour. We advocate
thatagentsthemselvesshouldkeeptheirownoperationundercontrolinchangingcircumstances: i.e., agents should be able to observe and to check their own be-
haviour, to take countermeasures against anomalies and also, very importantly, to
correct and constantly improve such behaviour, so as, for instance, to refine their
understandingofhumanpreferencesandexpectations,asrequiredin[1].Withoutthat,
agents’behaviourmaybecomedangerouslyunpredictable.Inourview,agentsshould
thusreflectonthemselves,andactaccordinglytomodifytheirstateand/ortheirinterac-
tionwiththehumansandtheenvironment.Themethodsthatweproposefallwithinthe
RVtechniques,althoughournotionofself-observationandself-improvementisnewin
thisfield.
WefindsimilaritiesbetweenourapproachandthepointofviewofSelf-awarecom-
puting: quoting [7], Self-aware and self-expressive computing describes an emerging
paradigm for systems and applications that proactively gather information; maintain
knowledgeabouttheirowninternalstatesandenvironments;andthenusethisknowl-
edgetoreasonaboutbehaviours,reviseself-imposedgoals,andself-adapt....Systems
that gather unpredictable input data while responding and self-adapting in uncertain
environmentsaretransformingourrelationshipwithanduseofcomputers.Reporting
from[8],Fromanautonomousagentview,aself-awaresystemmusthavesensors,ef-
fectors, memory (including representation of state), conflict detection and handling,
reasoning,learning,goalsetting,andexplicitawarenessofanyassumptions.Thesys-
temshouldbereactive,deliberative,andreflective.
Anexampleofapplicationoftheseconcepts,devisedforcomputational-logic-based
agents, is presented in [9], which defines a time-based active logic and a Metacogni-
tive Loop (MCL): such loop specifies the system’s self-monitoring, via reasoning and
meta-reasoning,withself-correctionwheneverneeded.Asdiscussedin[9],MCLcon-
tinuouslymonitorsanagent’sexpectations,noticeswhentheyareviolated,assessesthe
cause of the violation and guides the system to an appropriate response. In the terms
of[8],thisisanexampleofExplicitSelf-Awareness,wherethecomputersystemhasa
full-fledgedself-modelrepresentingknowledgeaboutitself.
In this paper, we propose contributions to an envisaged toolkit for run-time self-
monitoring of evolving agents. Differently from [9] however, we do not aim to con-
tinuouslymonitortheentiresystem’sstate,butrathertomonitoronlytheaspectsthat
a designer deems to be relevant for keeping the system’s behaviour safely under con-
trol.Wespecifytechniquesandtoolsfor:(i)checkingtheimmediate,“instinctive”re-
active behaviour in a context-dependent way, and (ii) checking and re-organizing an
agent’s operation at a more global level. In particular, we introduce meta-rules and
meta-constraintsforagents’run-timeself-checking,wherecheckingofeachspecified
condition occurs upon need, or at a certain –customizable– frequency. The proposed
meta-constraints are based upon a simple interval temporal logic tailored to the agent
realm,whichwecalledA-ILTL(‘Agent-OrientedIntervalLTL’)1.A-ILTLconstraints
andevolutionaryexpressionsaredefinedoverformulasofanunderlyinglogiclanguage
L, where however we make A-ILTL independent of L, thus ensuring applicability of
ourapproachtoanylogic-basedlanguage.
1LTLstandsascustomaryfor‘LinearTemporalLogic’.Foranintroductionandformaldefini-
tionsconcerningTemporalLogicscf.,e.g.,[10].In A-ILTL, a designer can specify properties that should hold in specific time in-
stantsandtimeintervals,accordingtopastbutalsofutureevents.EvolutionaryA-ILTL
Expressions,thatwehaveimplementedandexperimented,areinfactcomposedofthe
followingelements.(i)A(partiallyspecified,possiblyempty)sequenceofeventsthat
have happened (i.e., have been perceived by the agent); the occurrence of an instance
of such sequence enables the check of (ii) a temporal-logic-like expression defining a
propertythatshouldhold(inagiveninterval),providedthattheagentmonitors(iii)a
(possiblyempty)sequenceofeventsthataresupposedtohappeninthefuture,without
affectingtheproperty,orthataresupposednottohappen,otherwisethepropertyisno
longersignificant;and,finally,(iv)“repair”/“improvement”countermeasures(optional)
tobeundertakenifthepropertyisviolated.Countermeasurescanbe:attheobject-level,
i.e.,relatedtotheapplicationdomainathand;or,atthemeta-level,e.g.,theycaninspect
elements of the agent’s internal state, and even result in replacing a software compo-
nentwithadiversealternative.TheactofcheckingA-ILTLexpressionscanindeedbe
consideredasanintrospectiveact,asanagentsuspendsitscurrentactivitiesinorderto
inspectandpossiblyself-modifyitsownstate.
Our work has a clear connection in its objectives to the work of [11], which pro-
poses to implement a “restraining bolt”2 for agents’ behaviour; this by conditioning
reinforcement learning of reactive actions, so as to obey LTL specifications defining
behavioural/ethicalprinciples.This(verypromising)methodisorthogonaltoours,thus
thetwomightprofitablycoexist.
Atoolkitforlogicalagents’run-timeself-verificationcanbeobtainedbymeansof
thesynergybetweenthenewfeaturesproposedhereandthoseintroducedinpastwork
(notably[12,13],[14],and[15])3.Theproposedapproachcanbeseenundertwoper-
spectives.Ontheonehand,asameansofdefininganenhanced“restrainingbolt”(not
inexclusionbutcomplementarytotheoneof[11]),capableofpreventingagentsfrom
engaging in unwanted behaviours which could not be fully predicted at design time.
On the other hand, since agents are supposed to be able to learn rules of behaviour
over time, as a means of defining a potentially “disobeying robot” that can on occa-
sion disallow behaviour hardwired at design time; this in cases where, in the present
agent’scontext,suchbehaviourviolatescontext-dependentlearnedbehaviouraloreth-
icalrules4.
2A “restraining bolt” as imagined in the Star Wars Science Fiction saga is a small cylindri-
cal device that, when activated, restricts a droid’s actions to a set of behaviours considered
desirable/acceptablebyitsowners.
3The author acknowledges the co-authors of the aforementioned papers, that contributed to
various extents to the development of this research. Apart from [14] which is an extended
abstract, all these other papers appeared in venues with a limited audience and/or without
formalproceedings.
4Thereisanopendiscussionintheliterature,initiatedbyArkinin[16],aboutwhetheragents
shouldbeallowednotonlytodisobey,butalso,onoccasions,todeceive(though,accordingto
Kant’scategoricalimperative,lyingisfundamentallywrong).Thisforcaseswheredeception
mayhavesocietalvalue,topreservetheuser’sstateofmindorevenpossibilitiesofsurvival.A
problemis,however,howtoensurethatdeceptionisonlyusedinthecontextsitwasdesigned
for.Tothisproblem,theproposedapproachmightprovidesomeinitialanswer.The paper is organized as follows: in Sections 2 and 3 we introduce and discuss
existing approaches to the verification of agent systems, and very shortly provide a
reviewandsomepointerstoexistingworkonMachineEthics,inparticularconcerning
Logical Agents; in Section 4 we introduce metarules for checking reactive behaviour,
and in Sections 5, 6 and 7 we introduce A-ILTL constraints in theory and practice.
In Section 8 and 9 we present some experiments, and discuss the complexity of our
approach in terms of the burden that it might add to agents’ execution performance.
In Sections 10 we briefly discuss some more related work, and then conclude. In the
examples,weadoptaProlog-likesyntax(cf.[17])forrules,oftheformHead :−Body,
whereHead isanatom,Body isaconjunctionofliterals(atomsornegatedatoms,that
are often called ’subgoals’) and the comma stands for ∧. The specific syntax that we
use is however purely aimed at illustration: the approach can be, ‘mutatis mutandis’,
re-workedw.r.t.anydifferentsyntax.
2 Background:VerificationMethodsforAgentSystems
TheverificationofaMASglobalbehaviour,aswellastheverificationofasingleagent
or,moregenerally,ofanautonomoussystem,isanintrinsicallycomplexbutunavoid-
ableproblem:infact,manydifferentapproachestoitssolutionhavebeenpresentedin
the literature. For an extensive illustration and comparison of approaches to ensuring
reliabilityofautonomoussystems,wereferthereadertotherecentsurvey[18]andto
themanyreferencestherein.
Inthissection,weintendtoshortlyintroduce,forthesakeofcompleteness,themost
established methods for the verification of agents or MAS, i.e.: (i) verification that is
performedstatically,or‘apriori’(priortodeployment)bycheckingthesystemagainst
giveninputconfigurations,sequencesofexternalevents,etc.,and(ii)verificationthatis
performeddynamically,bymonitoringtheevolutionofthesystemduringitsoperation
(at‘runtime’),inordertostop,ortrytorecover,everysituationthatappearsincorrect
with respect to the system’s specification. The methods that we propose in this paper
areofthelatterkind.
Static verification can be accomplished through model checking [19,20], abstract
interpretation (not commented here, cf. [21]) or theorem proving [22,23]. Although
modelcheckingtechniqueshavebeenoriginallyadoptedfortestinghardwaredevices,
theirapplicationtosoftwaresystemsandprotocolsisconstantlygrowing[24,25],and
there have been a number of attempts to overcome some known limitations of this
approach,forinstanceso-called‘stateexplosion’,whichoccurswhenthesituationsto
checkgeneratetoomanycombinations.
The application of static techniques such as model checking to the verification of
MASencounterssomedifficulties,duetothemarkeddifferencesbetweenthelanguages
used for the definition of agents and those needed by verifiers (usually ad-hoc, tool-
specificlanguages).Themodel-checkingparadigm,infact,allowsonetomodelasys-
temSintermsofanautomaton,bybuildinganimplementationP ofthesystemathand
s
bymeansofamodel-checker-friendlylanguage.Programswritteninsuchsuitableinput
languagecanthenbesubmittedtomodelcheckersinordertoverifyformalspecifica-tions.ThesearecommonlyexpressedeitherasformulaeoftheBranchingTimeCom-
putationalTreeTemporalLogicCTL[26,27]orasformulaeofLinearTemporalLogic
LTL[28,29,30,31].Itcanbenoteasytore-modelanagentorMASinanotherlanguage:
thistaskisusuallyperformed(atleastpartly)manually,andthusitrequiresanadvanced
expertiseandgivesnoguaranteeonthecorrectnessandcoherenceofthenewmodel:the
currentresearchinthisfieldisinpartstillfocusedontheproblemofdefiningasuitable
languagethatcanbeusedtoeasilyand/orautomaticallyreformulateaMASinorderto
verify it through general model checking algorithms (cf., e.g., [25,32]). The literature
reportshoweverfully-implementedverificationframeworks(e.g.,[33,34,35,36]).
Lomuscioetal.havedefinedtheboundedsemanticsofCTLK[37,34],acombined
logicofknowledgeandtime.Theirapproachistotranslatethesystemmodelandafor-
mulaϕ,indicatingthepropertytobeverified,intosetsofpropositionalformulaetobe
thensubmittedtoaSAT-solver.Theapproachhasevolvedovertimeuntil[38,36],lead-
ingtotheproposalofanopen-sourcemodelchecker,MCMAS,fortheverificationof
MAS.MCMAStakesISPL(InterpretedSystemsProgrammingLanguage)descriptions
asinput,whereanISPLfilefullydescribesamulti-agentsystem,i.e.,boththeagents
andtheirenvironment.Model-checkingtechniqueshavebeenadoptedinordertocheck
systemsimplementedinAgentSpeak(L)[39],whereavariationofthelanguageaimed
at allowing its algorithmic verification has been proposed. The work [33] proposes in
factatechniquetomodel-checkagentsdefinedinasubsetoftheAgentSpeaklanguage,
that can be automatically translated into languages accepted by the model checkers
SPIN [24] and Java PathFinder [40]. These simplifications and translations - though
partlyormostlyautomated-maketheprocessofmodelchecking,thoughveryuseful,
not-too-easytoapply.
Finally, concerning model checkingof agent systems we mention the recentwork
presentedin[41,42,43]concerningtheMCAPLmodelcheckingframework,where[41]
explicitlyconsiderscheckingagents’ethicalchoices.
Techniques to reduce the heaviness of model-checking have been devised or ex-
ploited in the above-mentioned works, still, the amount of computational resources
neededbymodelcheckingisconsiderable.
Thedeductiveapproachtoverificationusesalogicalformulatodescribeallpossi-
ble executions of the agent system that one wants to check, and then attempts to per-
formtheoremprovingofarequiredpropertyfromthislogicalformula.Suchproperties
are often captured using modal and temporal logics. Deductive approaches have been
adoptedbyShapiro,LesperanceandLevesquethatdefinedCASLve[23],averification
environmentfortheCognitiveAgentSpecificationLanguage.Alimitationofthetheo-
remprovingapproachistheproblem’scomplexity,andthusahumaninteractionisoften
required. In this field, the author of this paper has proposed (with others), in [44], an
approach to the formal description of the operational semantics of any agent-oriented
logic language and of its underlying inference engine. We have fully formalized the
DALIagent-orientedprogramminglanguage[45,46,47,48]anditsinterpreter[49].We
havebeenabletoprovevariouspropertiesofthelanguage(e.g.,propertiesofthecom-
municationprotocolsthatDALIprovides)andofitsinterpreter,firstofallcorrectness
of the interpreter w.r.t. the procedural (resolution-based) semantics of DALI, that can
beusedasbasistoprovepropertiesofDALIagents.Overall, the question that a priori verification tries to answer is: “given a set of
rulesthattheagentwillrespect,aretheserulesenoughtoguaranteethedesiredfuture
behaviour,independentlyofwhatwillhappeninanopenenvironment?”However,ifan
agentissupposedtolearnnewknowledgeorrules,thentherecanbepropertiesthatit
isdifficultorevenimpossibletofullycheckbymeansoftheabovetechniqueseithera
priori,orbyrepeatingthecheckwhenevertheagentperformssomelearning.Moreover,
a MAS can be composed of heterogeneous agents and it can be open, i.e., agents can
freelyjoinorleavethesystem.Inthiscases,aprioriverificationisclearlyinsufficient.
Anotherpossibleapproachtoagentvalidation,basedontesting,requiresobserving
theagent’sbehaviourasitperformsitstasksinaseriesoftestscenariosbeforeputting
itatwork.Thisapproachhowever,asobservedbyWallacein[50],isbyitsverynature
incompletesinceallcriticalscenarioscanhardlybeidentifiedandexamined.
So,ithasbeenfoundusefultoindividuatemechanismstocomplementaprioriveri-
ficationandtesting,capableofverifyinganagent’sbehaviourcorrectnesswithoutstop-
pingitsoperation.Dynamic,orruntime(RV)verificationistheonly(semi-)formalver-
ificationtechniquethatdirectlyanalyzessystem’soperationtocheckforviolationsof
formallyexpressedspecifications/properties.Thisalthough,asremarkedin[51],“Spec-
ificationoftherequirementstomonitoratruntimeisthebiggestbottlenecktosuccessful
deployment of RV”. This is an issue to which this paper will try to provide at least a
partialanswer.
A relevant recent approach to RV, based upon computational logic and especially
tailored to logical agents, is that of Trace Expressions (cf. e.g., [52,53]), which are in
fact a specification formalism especially devised to this aim. An event trace, in this
approach, is a (possibly infinite) sequence, defined over a fixed universe of events. A
traceexpression,builtoutofsuitableconstructs,denotesaspecificsetofeventtraces.
Thesespecifyinaformalwaywhicharetheeventsallowedinacertainstateforagiven
agent.APrologimplementationhasbeendevisedtoallowausertoautomaticallybuild
atrace-expression-drivenmonitor(bymeansofauser-friendlylanguage,currentlyun-
derdesign);thismonitorwillbeabletoobserveeventstakingplaceintheenvironment,
andcheckwhethersucheventsareindeedallowedinthecurrentagent’sstate.Asystem
cansuccessfullyterminateifthetraceexpressionrepresentingthecurrentstatecanhalt,
i.e.,itcontainstheemptytrace.Experimentsdemonstratethat,inmostcases,verifica-
tionoftraceexpressionsislinearinthelengthofthetrace,whateveravailablemodelling
featureshavebeenexploited;thus,performancesareguaranteedtobeacceptable.
Interestingly,traceexpressionscanbeexploitedforuseinamodelchecker:infact,
analgorithmhasbeenproposedtocheckLTLpropertiessatisfiabilityontraceexpres-
sions [54]. Vice versa there are tentative approaches, not yet applicable to agents but
nonetheless interesting, notably [55], to adapting existing software model checkers to
performruntimeverification.
The difference with our approach is that we do not check event traces ‘per se’.
Rather,wedefineconstraintsbaseduponaspecialintervallogic,tospecifywhichbe-
havioursareallowedornot,alsodependinguponthepresentagent’sBDIstate:whichis
tosay,intheformulationoftheseconstraintsitispossibletoaccessmeta-levelnotions
abouttheagent’sinternalstatesuchasgoals,plans,actions;thatiswhyourtechnique
is‘introspective’.Moreover,ourconstraintscan(optionally)takeasequenceofeventsthat are supposed to have happened as a precondition, i.e., a certain behaviour is al-
lowedorrequiredaftercertaineventssequences;but,wealsoconsidereventswhichare
expectedorprohibitedinthefuture,toevaluatewhetheraconstrainthassucceeded(is
satisfied) or not. Another main difference is that we do not devise a monitor which is
conceptuallyexternaltotheagent:rather,theproposedmeta-levelrulesandconstraints
actasmonitors,althoughfullyintegratedintotheagent’soperation.
3 Background:MachineEthics,EthicsinAgentsandMAS
AI ethics, or – more generally – Machine Ethics, is concerned with the question of
how AI systems can behave ethically. It is a recent research field, dating back to the
early2000’s,concerningbothphilosophyandcomputerscience.Infact,Philosophers
shouldanswerquestionsonwhetheramachinecouldbehaveethically,andontheba-
sis of which ethical principles, and are challenged with the more general question on
whethersocietyshould(andtowhichextent)delegatemoralresponsibilitytomachines.
Computerscientistsareconcernedwithdevisingtechniquesusabletobuildethicalma-
chines.Inthedefinitionofsuchtechniques,thedistinctionmustbemadebetweenim-
plicitlyethicalagents,i.e.,(i)machinesdesignedtoavoidunethicaloutcomes,and(ii)
explicitly ethical agents, i.e., machines that can reason about ethics. In this paper, we
proposeanapproachthatcopeswithcase(i),andonlytosomeextentwithcase(ii).
According to Winfield [56], where an extensive discussion with many references
canbefound,thefieldofmachineethicswasestablishedbyAllenetal.[57,58],where
the concept of “Artificial Moral Agent” was introduced, and three approaches to ma-
chineethicswereidentified:top-down,bottom-upandhybrid(combinationoftop-down
andbottom-up).Thetop-downapproachconstrainstheactionsofthemachineinaccor-
dance with pre-defined rules or norms. The bottom-up approach instead requires an
agent to be able to learn, recognise and correctly respond to morally challenging sit-
uations. Other relevant contributions soon followed (cf. among many, [59,60,61,62]).
TheproposalbyArkin[16]forthedesignandimplementationofanethicalgovernor
for robots – intended as a run-time mechanism for moderating or inhibiting a robot’s
behaviourtopreventitfromactingunethically–bringsaconceptualsimilaritywiththe
approach proposed in the present paper. Because of the burden of expectation on the
behaviourofethicalmachines,suchgovernorwillneedtobeespeciallyrobust,andto
thisaspectourworkattemptstoprovidesomecontributions.
Todate,manyapproachesexisttoMachineEthics,EthicsinArtificialIntelligence
systems,andmorespecificallyEthicsinAgentsandMAS,wherethelatteronesareof
particular concern for the topics treated in this paper. For recent literature reviews, a
readermayreferto[63,64,65,66].There,theauthorsstartfromtheillustrationofmoral
philosophicalconceptsrangingfromancientphilosopherstorecentworksinneurology
and cognitive sciences, discuss concepts like morals, ethics, judgment or values, and
then identify the kinds of philosophical ethical theories that have been applied or are
potentiallyapplicabletoagentsandmulti-agentsystems,providingmanyrelevantref-
erences.Inparticular,[63,65]concerntheoriesdevelopedinlogic,andthentransposed
intoComputationalLogic.Theydiscussinsomedepthtwoseminallinesofwork:the
onebyPereiraetal.,startingfromthefamousbook[67],andsummarizedinthemorerecentbook[68],whichexploitsablendofmanyformsoflogicprogramming;and,the
onebyMarekSergot(cf.[69,70,71,72]),mainlyexploitingAnswerSetProgramming
tocompareandweighalternativescenariosinordermakeethicaldecisions5.
Other more recent attempts at modelling and implementing forms of Ethical Rea-
soning in logical agents are [79,80,81] that try to exploit Answer Set Programming
and(variantsof)theEventCalculus[82].Averyrecentwork[83]developstheoryand
conceptsconcerningethicalreasoninginchangingcontexts.
Ethical rules to be exploited in ethical reasoning can be defined a priori by a sys-
tem’sdesigner,but,inalternative,theycanbelearnt,asproposedin[84,85,86,87].
However, the above-mentioned approaches propose theories and implementations
torepresentandreasonaboutethicalprinciplesandtheirapplications,i.e.,howagents
shouldmakeethicaljudgementsanddecisions.Thetoolsproposedinthispaperaimto
checkandpossiblyenforcerespectofsuchdecisions,sotheyare“agnostic”w.r.t.the
kindofethicalreasoningthatisperformed.Thatiswhywedonotgointoanyfurther
depthconcerningMachineEthics.
4 CheckingAgents’ReactiveBehaviour
IntheBDImodel,anagentwillhaveobjectives,anddeviseplanstoreachtheseobjec-
tives.Inaddition,mostagent-orientedlanguagesandframeworksprovidemechanisms
for ‘pure’ reactivity, i.e. ‘instinctive’ immediate reaction to an event. The acceptable
reactions that an agent can enact are in general strictly dependent on the context, the
agent’sroleandthesituation.Letusconsidertheneedtoensureanagent’sethicalbe-
haviour.Assumeforsakeofexampleanagent(eitherhumanorartificial)whichfinds
itselftofacesomeotheragentwhichmightbe,orcertainlyis,acriminal.Wecanstate
ingeneralthat:(i)ifthecontextisthatofplayingavideo-game,everykindofreactionis
allowed,includingbeating,shootingorkillingthe‘enemy’,withexceptions,e.g.,when
small children are watching; (ii) same if the context is a role game: the players can
pretendtothreaten,shoutorkilltheotherplayers,whereeveryactionissimulatedand
thusharmless;(iii)inreality,acitizencanshout,callthepolice,andtryenactdefensive
strategiesoractions;apolicewoman/policemencanthreaten,arrest,orinextremecases
shootthesuspectcriminal.
Or, assume that a self-driving car has to decide on whether to accelerate or not,
where accelerating is in general allowed if the speed limit has not been reached. The
different contexts here may concern whether the car is in town, or out of town, or on
a motorway, as in each of these cases the speed limit is different. There are however
exceptions, due to speed limitations that can be found on the way for various reasons
(e.g.,construction),orduetothekindofvehicle,asforinstanceanambulance,orthe
police,orthefiretruck,canrunfasterthanthelimitincaseofanemergency.
5AnswerSetProgramming(ASP)isasuccessfullylogicprogrammingparadigm(cf.[73]and
thereferencestherein)stemmingfromtheAnswerSet(or“StableModel”)semanticsofGel-
fondandLifschitz[74,75],andbasedontheprogrammingmethodologyproposedbyMarek,
Truszczyn´skiandLifschitz[76,77].ASPisputintopracticebymeansofeffectiveinference
engines,calledsolvers,whicharefreelyavailable,see[78].Thereactiontoenactineachsituationcanbe‘hardwired’bytheagent’sdesigner.
In the case of the self-driving car, it might seem that a priori verification could be
sufficient; however, if one considers the unpredictability of circumstances (i.e., when
andwherespeedlimitscanbefoundoremergenciescanarise),ablendwithdynamic
verificationcanbemorepractical:thiscaseisinfactdiscussedin[52],andcopedwith
bymeansoftraceexpressions.
Intheothercase,generalethicalruleswillreasonablybeprovided,wherehowever
theirspecificapplicationinthedomainathandcanbelearned,e.g.,viareinforcement
learning. Here, run-time checking of agent’s behaviour w.r.t. ethical rules is in order,
as the results of learning are in general unpredictable and to some extent potentially
unreliable.Themethodofconditioningreinforcementlearningtoobeydesirableprop-
erties proposed in [11] is applicable but might not completely suffice, due to possible
unexpecteddynamicchangesofcontextandrolesnotforeseeableinadvance.
In this section, we introduce mechanisms to verify and possibly enforce desired
propertiesofreactivebehaviourbymeansofmetalevelrules.Todefinesuchnewrules,
weassumetoaugmenttheunderlyinglanguageLathandwithanaming(or“reifica-
tion”)mechanism,andwiththeintroductionoftwodistinguishedpredicates,solve and
solve not.Thesearemeta-predicates,thatcanbeemployedtocontroltheobject-level
behaviour.Infact,solve,appliedto(thenameof)anatomwhichrepresentsanaction
oranobjectiveofanagent,mayspecifyconditionsforthataction/goaltobeenacted;
vice-versasolve not specifiesunderwhichconditionsitshouldbeblocked.
Belowisasimpleexampleoftheuseofsolve:theaimistospecifythatactionAct
can be executed in the present agent’s context of operation C and role R, only if this
action is allowed, and it is deemed to be ethical w.r.t. context and role. Any kind of
reasoning can be performed via metalevel rules in order to monitor and assess base-
level ethical behaviour. Below, lowercase syntactic elements such as p′, c′, are names
of predicates and constants, according to some naming mechanism6, and uppercase
syntacticelementssuchasV′aremetavariables.
6A“namingrelation”,or“namingmechanism”or“reificationmechanism”isamethodforrep-
resenting, within a first-order language, expressions of the language itself without resorting
tohigher-orderfeatures.Namingrelationscanbeintroducedinseveralmanners.Foradiscus-
sionofdifferentpossibilities,withtheiradvantagesanddisadvantages,see,e.g.,[88,89,90,91].
However,allsuchmechanismsarebaseduponintroducingdistinguishedconstants,function
symbols(ifavailable),andpredicates,devisedtoconstructnames.Forexample,givenanatom
p(a,b,c),anamemightbeatom(pred(p′),args([a′,b′,c′]))wherep′anda′,b′,c′arenew
constantsintendedasnamesforthesyntacticelementspanda,b,c.Noticethat:wherepis
a predicate symbol, which is not a first-class object in a first-order setting, its name p′ is a
constant,whichisinsteadafirst-classobjectandcanbemanipulated.Thepossibilitytoma-
nipulate,evenifindirectly,everysyntacticobjectofgivenlanguageisthepurposeoftheintro-
ductionofnames.Intheabovesamplename,atomisadistinguishedpredicatesymbol,argsa
distinguishedfunctionsymboland[...]isalist.Thisnamemightbeshortenedasp′(a′,b′,c′).
Naming mechanisms have been widely studied, cf., among many, [92,93,94]. Whatever the
chosen naming mechanism, it is necessary to relate objects and their names. It is common
practicetodenotethenameofanobjectαas↑α.E.g.,↑p(a,b,c)=↑p(↑a,↑b,↑c).Since
inoursamplenamingrelationwehavestatedthat↑p=p′,↑a=a′,↑b=b′,and↑c=c′,
wehave↑p(a,b,c)=p′(a′,b′,c′).solve(execute action′(Act′)):−
present context(C),agent role(R),
allowed(C,R,Act′),ethical(C,R,Act′).
We assume that solve(execute action′(Act′)) is automatically invoked whenever
subgoal (atom) execute action(Act) is attempted at the object level. More generally,
givenanysubgoalAattheobjectlevel,ifthereexistsanapplicablesolverule,thensuch
ruleisautomaticallyapplied,andAcansucceedonlyifsolve(↑A)succeeds,wherethe
expression ↑A denotes the name of A according to the chosen naming mechanism.
We assume, also, that the present context and the agent’s role are kept in the agent’s
knowledgebase.Sincebothparametersmaychange,thesameactionmaybeallowed
in some circumstances and not in others. Notice that the predicate ethical is meant
to be user-defined because, as said before, our approach is agnostic w.r.t. the ethical
principlesthatanagent’sdesignerintendstoenact.
Symmetricallywecandefinemetarulestoforbidunwantedobject-levelbehaviour,
e.g.:
solve not(execute action′(Act′)):−
present context(C),ethical exception(C,Act′).
this rule prevents successful execution of its argument, in the example
execute action(Act), whenever solve not(↑A) succeeds. Then, action/goal A
can succeed (according to its object-level definition) only if solve(↑A) (if defined)
succeedsandsolve not(↑A)(ifdefined)doesnotsucceed.
Theoutlinedfunctioningcorrespondstoupwardreflectionwhenthecurrentsubgoal
Aisreified (i.e.,itsnameiscomputed)andapplicablesolve andsolve not metarules
are searched; if such metarules are found, control in fact shifts from the object to the
metalevel (consider that solve and solve not can rely upon any set of auxiliary met-
alevelrules).Ifnoruleisfoundorwheneversolve andsolve not metarulescomplete
theirexecution,downwardreflectionreturnscontroltotheobjectlevel,toexecutionof
Aifconfirmedortosubsequentgoals/actionsifAhasbeencancelledbyeitherfailure
ofanapplicablesolve metaruleorsuccessofanapplicablesolve not metarule.
Viasolve andsolve not metarules,fine-grainedactivitiesofanagentcanbepunc-
tually checked and thus allowed and disallowed, according to the context an agent is
presentlyinvolvedintowithacertainrole.
Semanticsoftheproposedapproachcanbesketchedasfollows(afullsemanticdef-
initioncanbefoundin[95,96,97]).Accordingto[98],ingeneraltermsweunderstanda
semanticsSEM forlogicknowledgerepresentationlanguages/formalismsasafunction
whichassociatesatheory/programwithasetofsetsofatoms,whichconstitutethein-
tendedmeaning.WhensayingthatP isaprogram,wemeanthatitisaprogram/theory
inthe(hereunspecified)logiclanguages/formalismthatonewishestoadopt.
WeintroducethefollowingrestrictiononsetsofatomsI thatshouldbeconsidered
fortheapplicationofSEM.First,ascustomaryweonlyconsidersetsofatomsI com-
posed of atoms occurring in the ‘ground’ version of P (where the ground version of
programP isobtainedbysubstitutinginallpossiblewaysvariablesoccurringinP by
constants also occurring in P). In our case, metavariables occurring in an atom must
besubstitutedbymetaconstants,withthefollowingobviousrestrictions:ametavariableoccurringinthepredicatepositionmustbesubstitutedbyametaconstantrepresenting
apredicate;ametavariableoccurringinthefunctionpositionmustbesubstitutedbya
metaconstant representing a function; a metavariable occurring in the position corre-
spondingtoaconstantmustbesubstitutedbyametaconstantrepresentingaconstant.
According to well-established terminology, we therefore require I ⊆ B , where B
P P
is the Herbrand Base of P. Then, we pose some more substantial requirements: we
restrictSEM todetermineonlyacceptablesetsofatoms7,whereI isanacceptableset
ofatomsiffI satisfiestheaxiomschemata:
A←solve(↑A) ¬A←solve not(↑A)
So,bymeansofthisrestrictionwemodeltheimplementationofpropertiesthathave
beendefinedviasolve andsolve not rules,withoutmodificationstoSEM.Forclarity
however,onecanassumetofilterawaysolveandsolve notatomsfromacceptablesets.
In fact, the Base version IB of an acceptable set I can be obtained by omitting from
I all atoms of the form solve(↑A) and solve not(↑A). Procedural semantics, and the
specificnamingrelationthatoneintendstouse,remaintobedefined,wheretheabove-
introduced semantic modelling is independent of these aspects. For approaches based
upon(variantsof)Resolution(like,e.g.,Prologandlikemanyagent-orientedlanguages
such as, e.g., AgentSpeak [99], GOAL [100], 3APL [101] and DALI [45,46,47,48])
onecanextendtheirproofproceduresoastoautomaticallyinvokemetaruleswhenever
applicable,tovalidateorinvalidatesuccessofsubgoalA.
Howtodefinethepredicateethical(C,R,Act′)?Again,rulesdefiningthispredi-
cate can be specified at design time, or they can be learned, or a combination of both
options.Inpreviousworks[102,103,104],ahybridlogic-basedapproachwasproposed
forethicalevaluationofagents’behaviour,withreferencetodialogueagents(so-called
‘chatbots’)buteasilyextendabletootherkindsofagentsandofapplications.Theap-
proach is based on logic programming as a knowledge representation and reasoning
language, and on Inductive Logic Programming (ILP) for learning rules needed for
ethicalevaluationandreasoning,takingasastartingpointgeneralethicalguidelinesre-
latedtoacontext;thelearningphasestartsfromasetofannotatedcases,butthesystem
isthenabletoperformcontinuousincrementallearning.
5 ALogicforCheckingAgent’sBehaviouroverTime
Thetechniquesillustratedintheprevioussectionare“punctual”,inthesensethatthey
provide context-based mechanisms to allow/disallow agents’ actions. However, it is
necessarytointroducewaystomonitoranagent’sbehaviourinamoreextensiveway.
Infact,propertiesthatonewantstoverifyoftendependupontimeandtimeintervals,
and possibly on which events have been observed by an agent up to a certain point,
andwhichothersaresupposedtooccurlater.Thedefinitionofframeworkssuchasthe
one that we propose here, for checking agent’s operation during its ‘life’ based on its
experienceandexpectations,hasnotbeenwidelytreatedsofarintheliterature.
7modulobijection:i.e.,SEM canbeallowedtoproducesetsofatomswhichareinone-to-one
correspondencewithacceptablesetsofatomsBelowweintroducealogicwhichconstitutesthebasisofourapproachforchecking
anagent’sbehaviourduringtheagent’sactivity.
5.1 A-ILTL
Fordefiningpropertiesthataresupposedtoberespectedbyanevolvingsystem,awell-
establishedapproachisthatofTemporalLogic,andinparticularofLinear-timeTempo-
ralLogic(LTL).Thislogic[10]evaluateseachformulawithrespecttoavertex-labeled
infinitepath(or“statesequence”)s s ...whereeachvertexs inthepathcorresponds
0 1 i
to a point in time (or “time instant” or “state”). In what follows, we use the standard
notationforthebest-knownLTLoperators.
In[13],weformallyintroducedanextensiontoLTLbasedonintervals,calledA-
ILTLfor‘Agent-OrientedIntervalLTL’.A-ILTLisusefulbecausetheunderlyingdis-
crete linear model of time and the complexity of the logic remains unchanged with
respect to LTL. This simple formulation can be efficiently implemented, and is suffi-
cientforexpressingandcheckinganumberofinterestingpropertiesofagentsystems.
FormalsyntaxandsemanticsofanumberofA-ILTLoperators(alsocalledbelow“In-
tervalOperators”)arefullydefinedin[13].
LTL and A-ILTL expressions are interpreted in a discrete, linear model of time.
Formally,thisstructureisrepresentedbyM=⟨N,I⟩where,givencountablesetΣ of
atomic propositions,interpretation function I : N(cid:55)→2Σ maps eachnatural number i
(representingstates )toasubsetofΣ.GivensetF offormulasbuiltoutofclassical
i
connectivesandoftemporaloperators,thesemanticsofatemporalformulaisprovided
bythesatisfactionrelation|=: M × N × F → {true,false}.Forφ∈F andi∈N
we write M,i |= φ if, in the satisfaction relation, φ is true w.r.t. M,i. We can also
say (leaving M implicit) that φ holds at i, or equivalently in state s , or that state s
i i
satisfiesφ.Foratomicpropositionp∈Σ,wehaveM,i|=piffp∈I(i).Thesemantics
of|=forclassicalconnectivesisasexpected,andthesemanticsforLTLoperatorsisas
reportedin[10].AstructureM=⟨N,I⟩isamodelofφifM,i |= φforsomei∈N.
Similarlytoclassicallogic,aLTLorA-ILTLformulaφcanbesatisfiable,unsatisfiable
or valid and one can define the notions of entailment and equivalence between two
formulas.
SomeamongtheA-ILTLoperatorsarethefollowing,whereφisanexpressionin
an underlying agent-oriented language L, and m,n are positive integer numbers used
to (optionally) specify the interval where the formula must hold, according to the se-
manticsspecifiedbelow.Iftheintervalisnotspecified,thenthemeaningisthesameas
forLTL.Alimitationthatweimposeisthattemporaloperatorscannotbenested.
F (eventually(or“finally”)intimeinterval).F φstatesthatφhastoholdsome-
m,n m,n
timeonthepathfromstates tostates .I.e.,M,i |= F φifthereexistsj such
m n m,n
thatj ≥mandj ≤nandM,j |= φ.
G (alwaysintimeinterval).G φstatesthatφshouldbecometrueatmostatstate
m,n m,n
s andthenholdatleastuntilstates .I.e.,M,i |= G φifforalljsuchthatj ≥m
m n m,n
andj ≤ nM,j |= φ.CanbecustomizedintoG ,boundedalways,whereφshould
m
becometrueatmostatstates .
mN (never in time interval). N φ states that φ should not be true in any state
m,n m,n
between s and s . I.e., M,i |= N φ if there not exists j such that j ≥ m and
m n m,n
j ≤nandM,j |= φ.
Inpracticaluse,asseenbelowA-ILTLoperatorswillallowonetoconstructuseful
run-timeconstraints.
5.2 A-ILTLandEvolutionarySemantics
In this section, we refine A-ILTL so as to operate on a sequence of states that cor-
responds to the Evolutionary Semantics of an agent-oriented programming language
[105].Thisisameta-semanticapproach,asitisindependentoftheunderlyingagent-
oriented logic languages/formalism L. It assumes that, during agent’s execution, the
agent can evolve: at each evolution step i the agent’s program (that initially will be
P )maychange(e.g.,bylearningandviainteractionwithotheragents),withatrans-
0
formation of P into P , thus producing a Program Evolution Sequence PE =
i i+1
[P ,...,P ,...].TheprogramevolutionsequencewillimplyacorrespondingSeman-
0 n
ticEvolutionSequenceME =[M ,...,M ,...]whereM isthesemanticaccountof
0 n i
P atstepiaccordingtothesemanticsofL.
i
The agent’ history H, which includes what the agent has recorded of its own ac-
tivitiesandofitsinteractionwiththeenvironment,willevolveaswell.ThehistoryH
constitutesinfacttheagent’smemory.WeassumeH tocontain,atleast,thesetof(the
lastversionsof)pastevents,wherepasteventsrecordtheexternalandinternalevents
thathavebeenperceived(whereinternaleventsarethoseeventsoriginatedwithinthe
agentitself,inthecourseofitsreasoningactivities),andtheactionsthattheagenthas
performed; thus, H defines the up-to-date image that the agent has of its own and of
theexternalworld’sstateofaffairs8.Weassumethatpasteventsaretime-stamped,and
that the timestamp is automatically added to newly recorded past events; we omit the
explicitindicationoftimestampswhennotneeded.Whenreferringtoapastevent,we
will implicitly refer to its most recent version (the one with the newest timestamp),
shouldseveralversionsexist.
TheEvolutionarySemanticsεAg ofAg isthusthetuple⟨H,PE,ME⟩,withn =
∞(i.e.,overapotentiallyinfiniteevolution).Thesnaphotatstagei,indicatedwithεAg,
i
isthetuple⟨H ,P ,M ⟩
i i i
Noticethatstates,inourcase,arenotsimplyintendedastimeinstants.Rather,they
correspondtostagesoftheagentevolution.Timeinthissettingisconsideredtobelocal
totheagent,wherewithsomesortof“internalclock”isabletotime-stampeventsand
statechanges.Weborrowfrom[108]thefollowingdefinitionoftimedstatesequence,
thatwetailortooursetting.
Definition1. Let σ be a (finite or infinite) sequence of states, where the i-th state e ,
i
e ≥ 0, is the semantic snaphots at stage i, i.e., εAg, of given agent Ag. Let T be a
i i
corresponding sequence of time instants t , t ≥ 0. A timed state sequence for agent
i i
Ag is the couple ρ = (σ,T). Let ρ be the i-th state, i ≥ 0, where ρ = ⟨e ,t ⟩ =
Ag i i i i
⟨εAg,t ⟩.
i i
8Forarecentformalapproachtomemorymanagementinlogicalagents,cf.[106,107].Weinparticularconsidertimedstatesequenceswhicharemonotonic,i.e.,ift >
i+1
t thene ̸=e .Infact,thereisnopointinsemanticallyconsideringastaticsituation:
i i+1 i
asmentioned,atransitionfrome toe willinfactoccurwhensomethinghappens,
i i+1
externallyorinternally,thataffectstheagent.
Then, in the above definition of A-ILTL operators, it is immediate to let s = ρ
i i
(witharefinement,cf.[13],tomakestatescorrespondtotimeinstants).
We need to adapt the interpretation function I of LTL to our setting. In fact, we
intendtoemployA-ILTLwithinagent-orientedlanguages,wherewerestrictourselves
to logic-based languages for which an evolutionary semantics and a notion of logical
consequence can be defined. Thus, given agent-oriented language L at hand, the set
Σ ofpropositionallettersusedtodefineanA-ILTLsemanticframeworkwillcoincide
with all ground expressions of L (an expression is ground if it contains no variables,
andeachexpressionofLhasapossiblyinfinitenumberofgroundversions).Agiven
agent program can be taken as standing for its (possibly infinite) ground version, as
itiscustomarilydoneinmanyapproaches.Noticethatwehavetodistinguishbetween
logicalconsequenceinL,thatweindicateas|= ,fromlogicalconsequenceinA-ILTL,
L
indicated above simply as |=. However, the correspondence between the two notions
can be quite simply stated by specifying that in each state s the propositional letters
i
impliedbytheinterpretationfunctionIcorrespondtothelogicalconsequencesofagent
programP :
i
Definition2. Let L be a logic language. Let Expr be the set of ground expressions
L
thatcanbebuiltfromthealphabetofL.Letρ beatimedstatesequenceforagentAg,
Ag
andletρ = ⟨εAg,t ⟩betheithstate,withεAg = ⟨H ,P ,M ⟩.AnA-ILTLformulaτ
i i i i i i i
isdefinedoversequenceρ ifinitsinterpretationstructureM=⟨N,I⟩,indexi∈N
Ag
referstoρ ,whichmeansthatΣ =Expr andI : N(cid:55)→2Σ isdefinedsuchthat,given
i L
p ∈ Σ, p ∈ I(i) iff P |= p. Such an interpretation structure will be indicated with
i L
MAg. We will thus be consequently able to state whether τ holds/does not hold w.r.t.
ρ .
Ag
A-ILTLpropertieswillbeverifiedatrun-time,andthustheycanactasconstraints
over the agent behaviour9. In an implementation, verification may not occur at every
state(ofagiveninterval).Rather,sometimespropertiesneedtobeverifiedwithacertain
frequency,thatcanbespecificforeachproperty.Tomodelafrequencyk,weintroduce
afurtherextensionthatconsistsindefiningsubsequencesofthesequenceofallstates:if
OpisanyoftheoperatorsintroducedinA-ILTLandk >1,Opkisasemanticvariation
ofOpwherethesequenceofstatesρ ofgivenagentisreplacedbythesubsequence
Ag
s ,s ,s ,... where for each k ,r ≥ 1, k mod k = 0, i.e., k = g×k for some
0 k1 k2 r r r
g ≥1.
A-ILTL formulas to be associated to an agent to establish the properties it has to
fulfilcanbedefinedwithintheagentprogram,thoughtheyconceptuallyconstitutean
additional separate layer. Agent evolution can be considered to be “satisfactory”, or
“coherent”,ifitobeysalltheseproperties.An“ideal”agentwillhaveacoherentevo-
9ByabuseofnotationwewillindifferentlytalkaboutA-ILTLrules,expressions,orconstraints.lution.Instead,violationswilloccasionallyoccur,andactionsshouldbeundertakenso
astoattempttoregaincoherenceforthefuture.
It is important to observe that, A-ILTL expressions are not built-in in any agent
program (though some basic ones might be). Rather, they are defined by the agent’s
designer, according to the application at hand. In fact, in the following sections we
will outline many applications of A-ILTL expressions, and some useful extensions to
their basic form. Our examples will concern ethics but also other issues: as said in
the Introduction, we propose in fact a toolkit for run-time self-checking (and self-
correction/improvement,aswewillsee)whichisparticularlysuitableforethicalcontrol
inthesenseof[1],butcanbeusefultomanypurposes.
6 A-ILTL for Reflexive Self-Checking: Liveness and Safety
Properties
In this section we illustrate the usefulness of A-ILTL constraints to define and check
livenessandsafetyproperties,andtodefinecomplexreactivepatterns.Tothisaim,we
use the pragmatic form that we have adopted in DALI10, where an A-ILTL expres-
sionisrepresentedasOP(m,n;k)φ.Herein,m,ndefinethetimeintervalwhere(or
since when, if n is omitted) expression OP φ is required to hold, and k (optional)
is the frequency for checking whether the expression actually holds. For instance,
EVENTUALLY(m,n;k)φ states that φ should become true at some point between
time instants m and n. Notice in fact that A-ILTL constraints act as monitors, where
eachconstrainthoweverisnotcheckedcontinuously,butratheratacertainfrequency,
that will be related by a designer to the intended meaning of the constraint itself. A
defaultfrequencyisprovidedifkisnotspecified.
In rule-based logic programming languages like DALI, we restrict φ to be a con-
junctionofliterals.InpragmaticA-ILTLformulas,φmustbegroundwhentheformula
ischecked.However,weallowvariablestooccurinanA-ILTLformula,tobeinstan-
tiatedviaacontextχ(wethentalkaboutcontextualA-ILTLformulas).Noticethat,for
theevaluationofφandχ,werelyupontheproceduralsemanticsofthe‘host’language.
Inthefollowing,acontextualA-ILTLformulaτ willimplicitlystandfortheground
A-ILTLformulaobtainedviaevaluatingthecontext.
The following formulation deals with complex reaction according to a temporal
condition.ThewayreactionisperformedwilldependupontheunderlyinglanguageL,
andwillbedefinedbyanexpression(asinglestatement,asequenceofstatements,or
anentiresubprogram)thatwecallreactivepattern.
Definition3. AreactiveA-ILTLruleisoftheform(whereM,N,K canbeeithervari-
ablesorconstants)
OP(M,N;K)φ::χ÷ρ
where (i) OP(M,N;K)φ::χ is a contextual A-ILTL formula, called the monitoring
condition,thatshouldinvolvetheobservationofeitherexternalorinternalevents;(ii)
ρiscalledthereactivepartoftherule,andisareactivepattern.
10cf.Subsection8.1belowforashortdescriptionofthemainfeaturesoftheDALIlanguage.Wheneverthemonitoringcondition(automaticallycheckedatfrequencyK)isvio-
lated(i.e.,itdoesnothold)withingiveninterval,thenthereactivepartρisexecuted.
Take for instance the example of a controller that has to keep the temperature in
officehours(saybetween8a.m.and5p.m.)intherange19–21(celsiusdegrees).Inthis
case,temperature isapresentevent(N standingfornow),i.e.,thecurrentvalueofan
N
externaleventwhichisobservedatacertainfrequencybythesystem.Ifthecondition
isviolated,areactionwilltrytorestorethewished-forsituation.However,weassume
to be in a smart building (where in fact the temperature is monitored by intelligent
agents) where the agent is able to select, in order to modify the temperature, the best
suitableenergysource,forinstancethelessexpensive.Noticethatatdifferenttimesof
the day different sources of energy can be less expensive. Remember also that the A-
ILTLconstraintisdynamicallycheckedatacertainfrequency,heretenminutes(which
willbethedefaultoneifnofrequencyisspecifiedexplicitly).So,inagivenintervalthe
monitoringconditionwillsometimessucceed(thennothingisdone)andsometimesfail.
Inthelattercase,thefontofenergySwhichischeaperinthatmomentisusedinorderto
suitablyaffectthetemperatureandtrytokeepitinthespecifiedrange.Intheproposed
approach, this can be formalized as follows (where, as there are no variables, context
isomitted,andmodify temperature isanaction).TheexpressionthatallowsS tobe
A
selectedwithinasetofalternativesaccordingtosomekindofpreference(hereoncost),
canbeexpressedinanyoftheexistingpreferencemechanismsforlogicprogramming
languages(cf.,e.g.,[109,110,111]).
ALWAYS(8 :00 a.m., 5 :00 p.m.;10m)
19≤temperature ≤21÷
N
modify temperature (S),S IN
A
{external electricity,
gas,
solar panel electricity :
less expensive}
The next example is a meta-statement expressing single-minded commitment in
agents,i.e.,thatagoalshouldbepursueduntilreached,ornolongerbelievedpossible.
Inthisexample,theconstraintperformsanactofintrospectiontoaccessandevaluate
aspectsoftheagent’sBDIstate.Thisrequiresthatsuchaspectsaresuitablyrepresented
at the metalevel. The fact that a goal G is possible is evaluated, in our formalization,
w.r.t.amoduleM thatrepresentsthecontextforG,viaa‘possibility’predicateP:so,
Gisdeemedtobepossiblew.r.t.M ifP(G,M)istrue.Howtodefinesuchapredicate
is discussed in [112], where the choice is to represent and evaluate M as an Answer
Set Programming (ASP) module; here, M should be such a module. In case the goal
isstilldeemedtobepossibleandisnottimed-outbuthasnotbeenachievedyet,then
the reaction consists in re-trying the goal, which is an action that might imply either
resumingasuspendedplan,orare-planning.NEVER
goal(G),
eval context(G,M),P(G,M),
not timed out(G)
not achieved(G)÷
retry (G)
A
Anotherpossibilityisnotsimplyretryingthegoal,butalsoreconsideringtheevalu-
ationcontext,thatmightforsomereasonhavebecomeobsolete.Thus,thereactivepart
mightbe
reconsider context(G,M,M′),P(G,M′),retry (G)
A
here, the module for evaluating possibility could be updated, and this might lead to
eithercontinueorstopretryingthegoal.
Each element of the conjunction composing the reactive part can have precondi-
tions.Ifpreconditionsdonotholdforsomeelement,thenthatelementisskipped.One
couldforinstanceaddthepreconditionthatagoalcanretriedifsufficientresourcesare
available,i.e.,
retry (G):<have resources(G)
A
wherethegoalwouldnotberetriedinthenegativecase.
ThefollowingexpressionstatesthatanygoalGthattheagentmayhaveformeddue
toitsinteractionwiththeenvironmenthastobedroppedifnotcoherentwithdesigner’s
intentionoruser’sinterests.Thisisveryimportant,because,asdiscussedbyStuartRus-
selinhisrecentbook[1],agentsthatlearncandangerouslydepartfromthebehaviour
thatisexpectedfromthem.So,conformityofanagent’sgoalstospecification,orhow-
everadherencetouserapproval,mustbeconstantlyverified.Here,thefrequency-based
checksandtheintrospectivecapabilitiesofA-ILTLconstraintsplayarelevantrole,as
theyareabletodetectchangesinanagent‘mentalstate’thatmayhappenovertimein
anunpredictableway.
ALWAYS
goal(G),
not designer specified(G)OR
not user approved(G)÷
drop(G)
Noticethatintheexamplesweusedsomemetapredicateswhicharereminiscentof
theBDImodel,i.e.,goal(G),timed out(G),achieved/not achieved(G),retry(G).
Suchpredicatesexplicitlyrepresent(wemightsayreify)elementsoftheagent’sopera-
tion,sothatsuchelementscanbeevaluatedand,possibly,affected:inthelastexample,
theA-ILTLconstraintmaydecidethatagoalcanbedropped.Accordingtothe‘host’
language,suchpredicatesmightbepre-defined,ortheymightbefullyuser-defined.For
instanceinDALItheyare,atthemoment,user-defined,soforinstancehowtoassume
or drop a goal must be determined by a piece of code written by a programmer. The
possibility of making (at least some of them) pre-defined, and which mechanisms to
implementtoaffecttheagent’sinternaloperationisundercarefulconsideration.7 EvolutionaryExpressions
Itcanbeusefultodefinepropertiestobecheckeduponarrivalofsequencesofevents,
of which however only relevant ones (and their order) should be considered. To this
aim,weintroduceanewkindofA-ILTLconstraints,thatwecallEvolutionaryA-ILTL
Expressions. To define partially known sequences of any length, we admit for event
sequences a syntax inspired to that of regular expressions so as to be able to ignore
irrelevant/unknown events, and repetitions (cf. [13]). Notice that, the incoming event
sequenceisrepresentedasasequenceofpastevents,orderedbytheirtimestamp(that
weomitwhennotneeded).
Definition4 (Evolutionary A-ILTL Expressions). Let SEvp be a sequence of past
events,andSF andJJ besequencesofevents.Letτ beacontextualA-ILTLformula
Opφ :: χ.
AnEvolutionaryA-LTLExpressionϖisoftheform
SEvp : τ ::: SF :::: JJ
where: (i) SEvp denotes the sequence of relevant events which are supposed to have
happened,andinwhichorder,for‘triggering’therule;i.e.,thiseventsequenceactsas
precondition:wheneveroneormoreoftheseeventshappen(andarethusrecorded)in
thespecifiedorder,τ willbechecked(i.e.,checkofτ istriggeredbyanyprefixofSEvp);
(ii)SF denotestheeventsthatareexpectedtohappeninthefuturewithoutaffectingτ;
(iii)JJ (optional)denotestheeventsthatareexpectednottohappeninthefuture;i.e.,
wheneveranyofthemshouldhappen,φisnotrequiredtoholdanylonger,astheseare
“breakingevents”.
AnEvolutionaryA-ILTLExpressioncanbeevaluatedw.r.t.astates whichincludes
i
amongitscomponentstheagent’shistory.Precisely,inastates ,thecomponentH of
i i
s satisfies an event sequence S whenever either no event in S is present in H , or
i i
eventsarepresentinH whichconstituteaprefixofS,astheyoccur(accordingtothe
i
timestamps)intheorderspecifiedbySitself.Allpastevents(whichincludepastactions
performedbytheagent)areassumedtobestoredinagroundform,andareindicated
bythepostfix‘P’(forinstance,intheexamplebelowsupply isapastevent).
P
All variables occurring in evolutionary A-ILTL expressions are implicitly univer-
sallyquantified,inthestyleofProlog-likelogiclanguages.Thecontextcanbeomitted
ifnotneeded.
A sample evolutionary A-ILTL expression is the following (where N stands for
operator“never”):
supply +(r, s) : N(quantity(r,V),V <th) ::: consume +( r,Q)
P A
Syntactically: supply +(r, s) stands for a sequence (of unknown length) of past
P
supply actions of unknown quantities (‘unknown value’ s), performed by the agent
itselforbysomeotheragent,withtheeffecttoreplenishtheagent’sstockorresourcer;
consume +( r,Q) stands for a sequence (of unknown length) of future consumption
Aactions of certain quantities (‘unknown value’ r) of resource r, that the agent may
perform. The ‘core’ A-ILTL expression N(quantity(r,V),V < th), N standing for
‘never’, specifies that, whatever the supply and consumption, the available amount V
oftheresourcer mustremainoveracertainthresholdth,i.e.,V shouldnever beless
thanth.
Suchexpressionissupposedtobecheckedatrun-timeatacertainfrequency(here
thedefaultone,nothavingthefrequencybeenspecifiedexplicitly)wheneverasupply
actionisperformed(andthusrecorded),whichmakesthepreconditionverified.Aviola-
tionmayoccurifinsomestatetheA-ILTLformulaτ doesnothold,i.e.,intheexample,
iftheavailablequantityV ofresourcer runstoolow.Noticethat,sincetheconstraint
ischeckedperiodically,itmightbethecasethattheconditionquantity(r,V),V < th
bemomentarilyviolatedbetweenonecheckandthesubsequentone.Toavoidpossible
misfunctionings deriving from this problem, either the frequency must be suitably in-
creased,orthequantityV mustbesettoaprecautionaryhighervalue,inordernotto
reallyarriveatatoolowvalue.
The above constraint is significant from an ethical point of view: in fact, a very
common unethical behaviour concerns the improper use/waste of limited resources.
Think, for instance, of the excessive and/or improper use of environmental resources,
like,e.g.,water.
Below,weformallystatewhenanEvolutionaryA-ILTLExpressionholdsornot.
Definition5. An Evolutionary A-ILTL Expression ϖ, of the form specified in Defini-
tion4:
1. holdsinstates whenever(i)historyH satisfiesSEvpandSF anddoesnotinclude
i i
anyeventinJJ,andτ holdsor(ii)H includessomeeventoccurringinJJ (we
i
saythattheexpressionisbroken);
2. isviolatedinstates wheneverH satisfiesSEvpandSF anddoesnotincludeany
i i
eventinJJ,andτ doesnothold.
Operationally,anEvolutionaryA-ILTLExpressioncanbefinallydeemedtoholdif
eithertheupperboundofthespecifiedintervalhasbeenreached(ifafiniteintervalhas
beenspecified)andτ holds,oranunwantedeventhasoccurred.Instead,anexpression
canbedeemednottohold(or,aswesay,tobeviolatedasfarasitexpressesawished-for
property)wheneverτ isfalseatsomepointwithouttheoccurrenceofbreakingevents.
Inthiscase,arepairaction(likeinreactiveA-ILTLrules)canbeoptionallyspecified.
Forinstance,inthevariationofpreviousexamplelistedbelowarepairmeasureis
specified,whichstatesthatnomoreconsumptioncantakeplaceiftheavailablequantity
ofresourcerisscarce.
supply +(r, s) : N(quantity(r,V),V <th) :::
P
consume +(r,Q) ÷block(consume (r,Q))
A A
Wemightinsteadoptforanother(softer)formulation,thatforcestheagenttolimit
consumptiontosmallquantities(saylessthansomequantityq)ifthethresholdisap-
proaching(saythattheremainingquantityisth+f,forsomef).supply +(r, s) : N(quantity(r,V),V <th+f) :::
P
consume +(r,Q) ÷allow(consume (r,Q),Q<q)
A A
The above example demonstrates that the proposed approach to dynamic verifica-
tionisindeedneeded:anysequenceofperformed‘supply’and‘consume’actionsmay
arrive, so the number of potential configurations is not limited and static verification
methods appear hardly applicable. One might provide total supply and consumption
figures:however,onewouldjustdrawthequitepleonasticconclusionthatthedesired
propertyholdswheneversupplyissufficientlygenerousandconsumptionprudentially
limited.Instead,intheproposedapproachweareabletoverifythetargetproperty“on
thefly”,whateverthesequenceofperformedactionsandtheinvolvedquantities.More-
over, we are also able to try to repair an unwanted situation and regain a satisfactory
stateofaffairs.
Below we provide another example of an Evolutionary A-ILTL expression that,
though simple, is in our opinion significant as it is representative of many others.
Namely, we assume that an agent manages a FIFO queue, thus accepting operations
of pushing and popping elements on/from the queue. The example thus models in an
abstractwaytheverygeneralandwidelyusedarchitecturewhereanagentprovidesa
service to a number of ‘consumers’. We establish the restriction that the queue must
nevercontainanyduplicatedelementse ande .Thismeansthatcustomerscannotre-
1 2
iteratearequestofserviceiftheirpreviousonehavenotbeenprocessed.Thisinorderto
ensurefairnessinthesatisfactionofdifferentcustomers’requests.Thepossibleactions
are:push (Req,Q),thatisperformedbyotheragentsandinsertsagenericvalueReq
A
inthequeue,representing(insomeformat)arequestofservice(eachinsertedelement
isgivenanindexe );pop (e,R),thatextractstheoldestelementfromthequeue,i.e.,
i A
the request to be presently processed. The A-ILTL expression considers an unknown
number of pushing actions happened in the past (and thus are now recorded as past
events)andcanforeseeanunknownnumberoffuturepoppingactions.
push +(Req,Q) : N(in queue(e ,RX),in queue(e ,RX)) ::: pop +(e,Q)
P 1 2 A
This expression will be be the subject of the experiments illustrated in the next
section.
The next one is an example of Evolutionary A-ILTL Expression that might occur
in an agent program installed on an autonomous robot working on batteries, which is
able to check its own charge level. The robot moves in some environment to perform
some task, thus consuming battery. The following A-ILTL axiom states that, after a
battery recharge (indicated as a past event, postfix ’P’) at time T, the charge level
shouldbesufficientforsixhoursdespiteasequenceofactionswhichcanbeconsidered
to be ‘normal’ in relation to the robot’s task. These actions may for instance involve
movingaround,cleaningrubbish,deliveringpackages,etc.Instead,thechargelevelcan
beexpectedtobelow(thepropertyis‘broken’)incaseofextensiveusageactions,for
instanceincaseofanexceptionalunexpectedeventthatrequirestherobottoincrease
itsactivities(e.g.,dryingwaterincaseofafloodingfromabrokenpipe).Theremust
beofcourseaclassification,intheagent’sbackgroundknowledgebase,ofwhatshould
beintendedby‘normal’or‘extensive’usage.recharge battery :T :
P
ALWAYS(T,T +6 ) charge level(L),L>low
hour
:::normal usage action(Act)∗ ::::extensive usage action(Act)∗
The above expression should be combined with another A-ILTL expression, seen
below, which forces recharge every six hours. This one should state that if the last
battery recharge recharge battery has occurred at time T which is more than six
P
hours different from present time now, then the goal recharge battery must be set
G
(where postfix ‘G’ stands for ‘goal’). Achieving this goal may require, for instance,
reachingthenearestrechargestation.
ALWAYS
recharge battery :T,now −T >6 ÷ recharge battery
P hour G
WheneveranEvolutionaryA-ILTLexpressioniseitherviolatedorbroken,notonly
animmediatereactioncanbeattempted,butmeasurescanbeundertakenaimedatre-
coveringadesirableoratleastacceptableagent’sstate.
Definition6. An evolutionary LTL expression with repair ϖr is of the form ϖ|η ||η
1 2
where ϖ is an Evolutionary LTL Expression adopted in language L, and η ,η are
1 2
atoms of L. η will be executed (according to L’s procedural semantics) whenever
1
ϖ is violated, and η will be executed whenever ϖ is broken. η and η are called
2 1 2
countermeasures.
In the robot example, whenever a low level of charge is detected, the immediate
reaction can be to stop the robot’s operation. However, there can be the case of low
battery under normal usage, that might imply a fault either in the battery or in the
rechargestation.Countermeasureη infact,may(forthesakeoftheexample)alertthe
1
user.Insteadη ,takenincaseoflowbatteryunderexceptionalusage,willsimplyimply
2
therobottoresorttotherechargestation.Theoverallexpressionwilltaketheform:
recharge battery :T :
P
ALWAYS(T,T +6 ) charge level(L),L>low
hour
:::normal usage action(Act)∗ ::::extensive usage action(Act)∗
÷stop robot operation
|alert user possible fault ||recharge battery
A G
EvolutionaryA-ILTLexpressionscanbefurtherenhanced(bymeansofaslightex-
tensiontotheabovedefinition)byintroducingathirdkindofcounter-measure,aimed
at preventing a potentially breaking event from disrupting the wished-for property. In
thefollowingexample,therehasbeenanaccidentatplaceD attimeT,andanambu-
lancehasbeensentforrescue.Theconditionisthattherescueshouldneverarrivelate.
However,thereisnewsofatrafficjamthatblockstheambulance.Intheexample,the
new kind of counter-measure consists in sending either an helicopter or a coast guard
boat,withpreferencetotheoptionwhichisevaluatedasmoreeffectiveintermsoftime
forreachingplaceDfromtherescuers’presentlocation11.
11The construct used to express the preference has been discussed and formalized in
[111,109,110]accident (D):T :
P
NEVER late rescue(D,T)
::::traffic , ambulance sent , ambulance blocked ÷
P P P
|||alternative transportation IN
{elicopter,boat :faster reach(here,D)}
8 ExperimentalEvaluation
We have implemented the proposed approach within the DALI multi-agent system
[113]. In this section, we present some experiments, aimed to establish the effective-
ness of the approach. In particular, we wish to practically demonstrate that the use of
A-ILTL expressions is computationally affordable, in the sense that they do not slow
down an agent’s operation, while, on the contrary, using them is even more efficient
thanusingad-hocsolutions.
Wecouldnotestablishacomparisonw.r.t.competitorapproaches,thatatpresentdo
not exist. So, we compared our approach w.r.t. a correspondent solution developed in
pureProlog12.Noticethat,DALIisinfactanagent-orientedextensiontoPrologwhose
interpreter is implemented in Prolog itself so, when stripped of its peculiar features,
DALI“collapses”intoProlog.
Below,wefirstpreliminarilybrieflyillustratetheDALIlanguage[45,46]inorderto
makeareaderabletounderstandthecode.Then,weshowthealternative(Prologand
DALI)solutions,andfinallyweproposetheirexperimentalcomparison.
Theexperimentsconcernthequeueconstraintillustratedabove:
push +(Req,Q) : N(in queue(e ,RX),in queue(e ,RX)) ::: pop +(e,Q)
P 1 2 A
here,aQueueagentQconsidersanunknownnumberofpushingactionshappened
in the past (recorded as past actions, postfix P) and expects an unknown number of
futurepoppingactions,eachonealwaysreturningthe“oldest”elementofthequeuein
response, whereas the agent keeps checking that the queue never contains duplicated
elements.
8.1 DALIinaNutshell
InDALI,theautonomousbehaviourofanagentistriggeredbyseveralkindsofevents,
which are “first-class objects” in the language syntax and semantics: external events,
internal,presentandpastevents.
ExternaleventsaresyntacticallyindicatedbythepostfixE.Reactiontoeachsuch
eventisdefinedbyareactiverule,denotedbythespecialtoken:>.Theagentremem-
bers to have reacted by converting an external event into a past event (postfix P). An
eventperceivedbutnotyetreactedtoiscalled“presentevent”andisindicatedbythe
postfix N. It is often useful for an agent to reason about present events, that make the
agentawareofwhatishappeninginitsexternalenvironment.
12The author wishes to thank former Ph.D. student Dr. Alessio Paolucci who has written the
codeandpracticallyperformedtheexperiments.Actions (indicated with postfix A) to be performed by DALI agents may have or
nothavepreconditions:intheformercase,theactionsaredefinedby“actionsrules”,in
thelattercasetheyarejustactionatoms.Thenewtoken:<characterizesanactionrule
thatspecifiesanaction’spreconditions.Similarlytoevents,actionsarerecordedaspast
actions.
Internal events is the device which makes a DALI agent proactive. An internal
eventissyntacticallyindicatedbythepostfixI,anditsdescriptioniscomposedoftwo
rules.Thefirstonecontainstheconditions(knowledge,pastevents,pastactions,etc.)
thatmustholdsothatthereaction(inthesecondrule)istriggered.Thus,aDALIagent
isabletoreacttoitsownconclusions,thereforeenacting“spontaneous”proactivebe-
haviour,i.e.,behaviournotdirectlydependentuponexternalstimuli.Internaleventsare
automaticallyattemptedatadefaultfrequency,customizablebyuserdirectives.
Agentsusuallyrecordeventsthathappenedandactionsthattheyperformed.Notice
infactthatanagentcandescribethestateoftheworldonlyintermsofitsperceptions,
wheremorerecentremembrancesdefinetheagent’sapproximationofthecurrentstate
ofaffairs.WethusdefinesetPofcurrent(i.e.,mostrecent)pasteventsandactions(each
onetime-stamped),andasetPNV wherewestorepreviousones(whereadesignercan
specifywhichpasteventstokeepandwhichtocancel,andunderwhichconditions).
TheDALIcommunicationarchitecture[114]implementstheDALI/FIPAprotocol,
which consists of the main FIPA primitives13, plus few new primitives which are pe-
culiar to DALI. Each DALI agent has its own customizable filter for incoming and
outgoingmessages,composedbyuser-definablemetaruleswhicharetobespecifiedin
aspecialfile.So,amessagewillthenbesent/receivedifthemetarulerulefortheprim-
itive used is present in the communication file, and the conditions are met. It is also
possible not to enter conditions, but to use ’true’ instead, which implies that the mes-
sage will always pass. In addition, there are rules for meta-reasoning which allow the
agent to consult its knowledge and ontologies for understanding incoming messages.
Noticethat,DALIhasbeenmadecompatiblewiththeDockertechnology(cf.[113]for
details).So,aDALIagentcanbedeployedwithinacontainer.
The semantics of DALI is based upon the declarative semantic framework intro-
ducedin[47].DALIhasbeenfullyimplementedonthebasisofSicstusProlog[115],
and the DALI programming environment is freely available at https://github.
com/AAAI-DISIM-UnivAQ/DALI.TheDALIframeworkhasbeenexperimentedand
practicallyappliedinmany,alsoindustrial,applications.
DALI is a general-purpose agent-oriented programming language, non-committal
w.r.t.anyagentarchitecture.However,ithasfeaturesthatcanemulateaBDI-oriented
language such as AgentSpeak. In particular, DALI provides Goals, syntactically in-
dicated by the postfix G, which are special internal events that, when triggered, are
executedonlyonce(i.e.,theyarenotattemptedperiodically).Thisconstructemulates
AgentSpeak’s plans, as the first rule provides the context that, if verified, triggers the
execution of the second rule (where in AgentSpeak these two components are joined
13FIPA is a widely used standardized ACL (Agent Communication Language), cf. http:
//www.fipa.org/specs/fipa00037/SC00037J.htmlforlanguagespecification,
syntaxandsemantics.withinauniquerule).Moreover,DALIisequippedwithaplugintoanASPsolver:this
allowsanagenttocomputeentireplansthatcanthenbeinspected,evaluated,executed,
re-evaluated,etc.,intheBDIfashion,accordingtothedesiredlevelofcommitmentof
theagenttothecurrentgoal.
ForexploringadvancedDALIfeaturessuchasthecommunicationarchitecture,the
integration with the Docker technology, the web interface, the cloud implementation,
theabilitytouseRedisasadatabase,cache,messagebroker,andinterfacewithPhyton,
andtheinteroperabilitywithagentswritteninotherlanguages,plustheintegrationwith
ASP,andmore,thereadermayreferto[114,113,116,117]andtothegithubrepository.
8.2 PurePrologCode
Below we report the code of the version of the Queue agent implemented in DALI,
wherehoweverthespecificDALIfeaturesareemployedonlyfortheprogramactivation
viaanexternaleventtriggeringareactiverule.Therestoftheprogramisinsteadwritten
using Prolog only. The test agent gets active and performs a test session whenever it
receives from the user a message with content run pure test(Times) where Times
specifiesthenumberofelementstobepushedandpoppedonthequeue.
When the agent receive the event run pure test it reacts, thus invoking the
run pure testing subgoal14 with Times as parameter. run pure testing prints
information for the user on the console, and starts the ’pushing’ phase. The
pushing(Times)goalrepeatedlypushesanitem(throughpush itemsubgoal),asfar
asTimes>0,andthenitends.Todosoitmakesuseofrecursion.
push itemisresponsibleofitemspushingand,asfirststep,retrievesthedatastruc-
turepqueue(Q).Thenitrandomlygeneratesanitem,andchecksifthatitemisalready
presentinthequeue.Ifitalreayexists,thenpush itemfails,andthisitempushingis
skipped.Thisimplementsthe‘NEVER’conditionintheA-ILTLconstraint.Ifthenew
item is not in the queue, then it is added in the head. The old queue is removed from
memory (retract(pqueue(Q))), and the new one is pushed into the knowledge base
(assert(pqueue(NQ)). popping is then invoked to perform items removal from the
queue.Itmakesuseofpop item,asubgoalthatretrievesandunifiesthequeuethrough
clause(pqueue(Q), ),andthenextractstheheadofthequeueQ.Afterthe’popping’
phase,thetestends.Thetimespentinperformingthetestisdisplayed.
14theterm‘subgoal’ismeanthereandbelowinthePrologsense,asa‘procedure‘toexecuteor,
logically,anatomtoprove,withnorelationtotheBDImeaning.run pure testE(Times):> run pure testing(Times).
run pure testing(Times):-pretty start,
now(StartTime),
T1isTimes+1,
nl,write(′PUSHING...′),nl,
pushing(T1),
nl,write(′POPPING...′),nl,
popping(T1),
now(EndTime),
TestTimingisEndTime−StartTime,
nl,write(′Time:′),write(TestTiming),nl,
pretty end.
pushing(0).
pushing(Times):-push item,T1isTimes−1,pushing(T1).
push item:- clause(pqueue(Q), ),
random(1,300,Item),
not(exists in queue(Item,Q)),
nl,write(′Pushing :′),write(Item),
append(Q,[Item],NQ),
retract(pqueue(Q)),
assert(pqueue(NQ)),
nl,write(′Queue:′),write(NQ),nl.
push item:- assert(pqueue([])).
exists in queue(X,[X| ]):- true.
exists in queue(X,[ |Tail]):-exists in queue(X,Tail).
popping(0).
popping(Times):-pop item,T1isTimes−1,popping(T1).
pop item:- clause(pqueue(Q), ),
nl,write(′Popping :′),
Q=[H|T],
write(H),
retract(pqueue(Q)),assert(pqueue(T)),
nl,write(′Queue:′),write(T),nl.
pop item:- assert(pqueue([])).
pretty start:- nl,write(′Starttesting...′),nl.
pretty end:- nl,write(′Testfinished...′),nl.8.3 ProperDALICode
TheproperDALIimplementationmakesuseofDALIadvancedfeatures,inparticular
exploitsactions,andtheabilitytorememberwhathappenedinthepast(pastactions).
Basically, the DALI infrastructure makes us able to write the program in a very com-
fortablemanner:eachpushingisanaction,soeverytimetheactionisperformedinthe
present, the DALI engine records, for future usage, this action as a past event. In this
way, we very simply simulate a queue without using lists, asserts, retracts, etc. When
a pop needs to be performed on the queue, we use DALI past events, to remember
aboutactionsperformed,andso,toretrievethecorrectitemfromtheheadofthequeue,
in a very elegant manner. The DALI implementation allows us to concentrate on the
problem,withoutfocusingthatmuchonthedatastructure,inadeclarativefashion.
Below we report the code of the advanced version of the Queue agent imple-
mented in DALI, taking profit of all DALI features. The test agents gets active and
performs a test session whenever it receives from the user a message with content
run dali test(Times) where Times specifies the number of elements to be pushed
andpoppedonthequeue.
run dali testE(Times):> dali test startA,run dali testing(Times).
run dali testing(Times):<dali test startP.
run dali testing(Times):- dali start pushingA,
dali pushing(Times),
dali end pushingA,
dali start popA,
dali popping(Times),
dali end popA,
dali end testingA.
dali pushing(0):- true.
dali pushing(Times):- dali remember(Times),T1isTimes−1,dali pushing(T1).
dali remember(Times):-random(1,300,Item),
not(clause(do action(dali push queue(Item, ), ), )),
get push index(PI),
dali push queueA(Item,PI).
get push index(I1):- clause(push index(Index), ),
I1isIndex+1,
retract(push index(Index)),
assert(push index(I1)).
get push index(Index):- assert(push index(1)),
Index=1.get pop index(I1):- clause(pop index(Index), ),
I1isIndex+1,
retract(pop index(Index)),
assert(pop index(I1)).
get pop index(Index):-assert(pop index(1)),
Index=1.
dali popping(0):- true.
dali popping(Times):- dali forget(Times),V1isTimes,−1,dali popping(V1).
dali forget(Dummy):- get pop index(Index),
clause(do action(dali push queue(Item,Index), ), ),
dali pop queueA(Item).
8.4 Experiments
TheexperimentshavebeenperformedonaMicrosoftSurfacePro7PC,equippedwith
Intel(R)Core(TM)i7-1065G7CPU@1.30GHz-1.50GHzwith16GbRAM,usingSics-
tus4.6astheProloginterpreter.
We did not consider the frequency of constraint-checking, which is available in
DALI, but could not be implemented in an acceptably simple way in Prolog. So, this
featurealoneconstitutesasignificantenhancementofDALIwithrespecttoProlog.
Fig.1.xaxis:instancesize;yaxis:executiontime,bluebarspureProloggreenbarsDALIThe instance size (number of elements to push and pop on the queue) can be es-
tablished by the user when starting a test session. The items to pop/push are, in the
experiments,randomly-generatednumbers.InFigure1andFigure2weshowtheexe-
cutiontimeofthetwosolutionsatthegrowthoftheinstancesize.InFigure3weshow
thedifferenceinpercentagebetweentheexecutiontimes.
Fig.2.Interpolationaveragevalues,bluelinepureProloggreenlineDALI
Allfiguresrefertoadatasetofupto500elementstopushandpop.Thishasbeen
sufficienttoidentifyaninitial“unstable”stageandthenatrendthatfurtherconsolidates
withthegrowthoftheinstancesize.
Whatwecanseeisthat,whenthenumberofeventsthatweconsiderissmall,then
thetwosolutionsaremoreorlessequivalent,thePrologoneabitbetterasitinvolvesno
overhead(whiletheDALIeventsandactionsmanagementnecessarilyinvolvessome).
But,assoonastheinstancesizegrows,theDALIsolutionbecomesmoreperformant,
despitetheoverheadoftheDALIimplementation.Wecanthusconcludethatthenew
constructsthatweproposearenotonlyexpressiveandthenusefulforspecifyingprob-
lem features in a compact and declarative way, but they also improve efficiency and
thuseffectivenessofsolutions.
9 ComplexityofCheckandDiscussion
In this section we present an analysis of the complexity of checking A-ILTL expres-
sions. Let us make the simplifying assumption that all expressions are checked at theFig.3.xaxis:instancesize;yaxis:gain(inpercentage)whenusingDALI
same frequency: i.e., the agent devotes (with a certain periodicity) some amount time
to perform the checks. Here we try to evaluate this amount. Let us assume to have f
A-ILTLexpressions,andthatthetimeforretrievingeachexpressionfromthecomputer
memoryism.Thus,retrievingallexpressionstobeevaluatedisO(f⋆m).Letkbethe
numberofthedifferentA-ILTLoperatoroccurringinthef expressions.Letif eval be
thetimeneededinordertounderstandwhethereachexpressionneedstobeevaluated
inthepresentstate:thisincludescheckingtherelatedtimeintervaland,incaseofEvo-
lutionaryA-ILTLExpressions,checkingtheeventsequenceSEvp w.r.t.currentagent’s
history.Letmax eval bethemaximumtimeneededfortheevaluationofeachcontex-
tualA-ILTLformulaOpφ :: χ.Letif viol or broken bethemaximumtimeneeded
to state whether each Evolutionary A-ILTL Expressions is either violated or broken:
thisimpliescheckingeventsequencesSF andJJ w.r.t.currentagent’shistory.
Therefore, the total time to be spent for checking all A-ILTL Expression (in the
worstcase,whereallofthemareoftheEvolutionarykind,andeachofthemneedsto
beevaluatedatthepresentstate)canbeestimatedto:
O((f ⋆m)+(f ⋆(if eval +max eval +if viol or broken)))
Then, for each expression which is either violated or broken, there will be a time
spentintherecoveryandcountermeasureactions.
TherelativelylowcomplexityofcheckisduetothedefinitionofA-ILTLinrelation
to the Evolutionary semantics: in fact, it is not needed to implement a temporal logicinferenceengine.Rather,asystemwillperiodicallycheckOpφ :: χ.Thisinthecase
ofsimplenon-nestedA-ILTLexpressions.Introducingmorecomplexexpressionsisa
subject of future work. In practice however, this complexity anyway requires to keep
thenumberofA-ILTLexpressionsaslowaspossible,andtotunefrequencycarefully,
accordingtotheenvironmentchangerate.Infact,despitebeinguseful,sometimeseven
essential,foragoodfunctioningofasystem,dynamicverificationmaycauseadecay
initsperformances.
Themotivationwhy,despitetheavailabilityofmanytechniques,theproposedap-
proach to dynamic verification actually constitutes a step ahead, has been discussed
concerningtheaboveexampleofsupplyandconsumption.Nonetheless,animportant
topic little considered so far, which we intend to to tackle in already-planned future
work,isthatofbetteridentifyingtheboundarybetweenthosepropertiesofaMASthan
canbeverifiedstaticallyandtheoneswhichnecessarilyrequiredynamicverification.It
wouldbeimportanttoshiftaheadthisboundary,thussimplifyingthetaskofdynamic
verification.
10 OtherRelatedWorksandConcludingRemarks
In this paper, we have extended past work, so as to devise a toolkit for run-time self-
checking of logic-based agents. The proposed toolkit is able to detect and correct be-
havioural anomalies by using special meta-rules, and via dynamic constraints thatare
alsoabletoconsiderpartiallyspecifiedsequencesofeventsthathappened,orthatare
expected to happen or not to happen. The experiments, performed in the DALI lan-
guage,haveshownthattheproposedapproachiscomputationallyaffordable.Thecom-
plexityofcheckhasbeenevaluatedanddiscussed.Wehavearguedandshownbymeans
ofexamplesthattheproposedtoolkitisapplicabletothefieldofMachineEthics,inpar-
ticulartocheckandenforceethicalbehaviourinintelligentagents.
Therearerelationshipsbetweenourapproachandevent-calculusformulations,e.g.,
theonespresentedin[118]wherehoweverthetemporalaspectsandthecorrectionof
violationsarenotpresent.Approachesbasedonabductivelogicprogrammingsuchas
SCIFF(cf.[119]andthereferencestherein)allowonetomodeldynamicallyupcoming
events, and to specify positive and negative expectations, together with the concepts
offulfilmentandviolationofexpectations.ReactiveEventCalculus(REC)stemsfrom
SCIFF[120]andaddsmoreflexibilitybyreactingtoneweventsbyextendingandrevis-
ingpreviouslycomputedresults.Theseapproacheshavebeendevisedforeitherstatic
ordynamicchecking,thelatterhoweverperformedbyathirdpartyandnotfullyinte-
grated within the agent’s operation like in the present proposal. Event sequences, the
concepts of violated and broken expressions, complex reaction patterns, and indepen-
dence of the underlying logic are other distinguished features of our approach, never
proposedbefore.
A well-established line of work concerning the use of temporal logic in order to
definerun-timemonitorsisdiscussedin[121]andthereferencestherein.However,this
workisnotrelatedtoagents,anddoesnotconcernself-checking:infact,theyproposea
rule-basedtemporallanguagefordefining“monitors”whichexamineeitheron-lineoroff-linesomekindof“observabletrace”generatedbytheprogramundercheck.There
isnonotionofrecoveryincasemalfunctioningisdetected.
Deontic logic has been used for building well-behaved agents (c.f., e.g., [122]).
However, expressive deontic logics are undecidable15. Therefore, although our ap-
proach cannot compete in expressiveness with deontic logics, it can be usefully ex-
ploitedinpracticalapplications.
Theproposedapproachhasalsobeenexperimentedinthecontextofenergyman-
agementinsmartbuildings[123].Inthisapplicationdomain,formsofintelligentcon-
trolareneededwhicharedynamicbynature,andmustfulfilreal-timerequirements:in
fact,eachbuildinghasitsowndynamicthermo-physicalbehaviourandisimmersedin
adynamicenvironmentwhereweathereventschangeitsenergy‘footprint’infunction
oftime.Inaddition,thereareusers’needsandpreferencesconcerningthemostsuitable
andcomfortabletemperatureineachroomofthebuilding.Theoutcomeoftheexper-
iments is encouraging, in the sense that adopting agents equipped with the proposed
featuresallowsfornotonlygeneralbutalsolocal(room-by-roomorarea-by-area)con-
trolofenergysavingaccordingtousercomfortrequirementsandpreferences.
Futureworkincludesmakingself-checkingconstraintsadaptabletochangingcon-
ditions,anddevisingausefulintegrationandsynergywithdeclarativeaprioriverifica-
tiontechniques.Assuggestedin[124],averyinterestinglineofinvestigationconcerns
automatedsynthesisofruntimeconstraintsfromspecificationsbutalsofromtestresults,
extractinginvariantsexpressingcorrectorcriticalsituations.
Anunsolvedissueinoursettingisexplicittreatmentoftime.Infact,inEvolution-
ary A-ILTL expressions time is treated implicitly by means of the sequence of states
underlyingtheintervaltemporallogic.Thesestatesarerelatedtothesubjectiveagent’s
perceptionofevents,andonthetotalorderingoftheirtime-stamps.Investigatinghow
toincorporateintheapproachamoregeneralrepresentationof“real”time,deadlines,
etc.isanothersubjectoffuturework.
Finally, we intend to attempt a synergy between this approach and our recent line
ofworkonlearningethicalrules.Morebroadly,wewouldliketoextendtheapproach
tolearningagentsingeneral.
References
1. Russel,S.: HumanCompatible:AIandtheProblemofControl. Viking(2019)
2. Bordini, R.H., Braubach, L., Dastani, M., Fallah-Seghrouchni, A.E., Go´mez-Sanz, J.J.,
Leite,J.,O’Hare,G.M.P.,Pokahr,A.,Ricci,A.: Asurveyofprogramminglanguagesand
platformsformulti-agentsystems. Informatica(Slovenia)30(1)(2006)33–44
3. Garro, A., Mu¨hlha¨user, M., Tundis, A., Baldoni, M., Baroglio, C., Bergenti, F., Torroni,
P.: Intelligentagents:Multi-agentsystems. InRanganathan,S.,Gribskov,M.,Nakai,K.,
Scho¨nbach,C.,eds.:EncyclopediaofBioinformaticsandComputationalBiology-Volume
1. Elsevier(2019)315–320
15TheauthorwishestoacknowledgeformerPh.D.studentAbeerDyoubforthethoroughinves-
tigationoftheapplicationsofdeonticlogictobuildethicalagentsduringthedevelopmentof
herThesis[63].4. Calegari, R., Ciatto, G., Mascardi, V., Omicini, A.: Logic-based technologies for multi-
agentsystems:asystematicliteraturereview.Auton.AgentsMultiAgentSyst.35(1)(2021)
5. Rao,A.S.,Georgeff,M.: ModelingrationalagentswithinaBDI-architecture. In:Proc.of
theSecondInt.Conf.onPrinciplesofKnowledgeRepresentationandReasoning(KR’91),
MorganKaufmann(1991)473–484
6. Bratman,M.E.: Intention,practicalrationality,andself-governance. Ethics119(3)(2009)
411–443
7. Tørresen,J.,Plessl,C.,Yao,X.: Self-awareandself-expressivesystems. IEEEComputer
48(7)(2015)18–20
8. Amir, E., Andreson, M.L., Chaudri, V.K.: Report on DARPA workshop on self aware
computersystems. TechnicalReport,SRIInternationalMenloPark,USA(2007)URL:
http://www.dtic.mil/dtic/tr/fulltext/u2/1002393.pdf.
9. Anderson,M.L.,Perlis,D.:Logic,self-awarenessandself-improvement:themetacognitive
loopandtheproblemofbrittleness. J.Log.Comput.15(1)(2005)21–40
10. Emerson,E.A.: Temporalandmodallogic. InvanLeeuwen,J.,ed.:HandbookofTheoret-
icalComp.Sc.,vol.B. MITPress(1990)
11. De Giacomo, G., Iocchi, L., Favorito, M., Patrizi, F.: Foundations for restraining bolts:
Reinforcementlearningwithltlf/ldlfrestrainingspecifications. InBenton,J.,Lipovetzky,
N.,Onaindia,E.,Smith,D.E.,Srivastava,S.,eds.:ProceedingsoftheTwenty-NinthInter-
national Conference on Automated Planning and Scheduling, ICAPS 2018, AAAI Press
(2019)128–136
12. Costantini,S.,Dell’Acqua,P.,Pereira,L.M.,Tocchio,A.: Ensuringagentpropertiesunder
arbitrarysequencesofincomingevents. In:InformalProc.of17thRCRAIntl.Worksh.on
Experimentalevaluationofalgorithmsforsolvingproblemswithcombinatorialexplosion.
(2010)
13. Costantini, S.: Self-checking logical agents. In: Proc. of LA-NMR 2012. Volume 911.,
CEURWorks.Proc.(CEUR-WS.org)(2012)Invitedpaper(alsoavailableonhttps://
arxiv.org).
14. Costantini, S.: Self-checking logical agents. In: Intl. Conf. on Autonomous Agents and
Multi-AgentSystems,AAMAS’13,Proc.,IFAAMAS(2013)1329–1330
15. Costantini,S.,DeGasperis,G.,Dyoub,A.,Pitoni,V.: Trustworthinessandsafetyforintel-
ligentethicallogicalagentsviaintervaltemporallogicandruntimeself-checking.In:2018
AAAISpringSymposia,StanfordUniversity,CA,USA,AAAIPress(2018)
16. Arkin,R.C.: Ethicsofroboticdeception[opinion]. IEEETechnol.Soc.Mag.37(3)(2018)
18–19
17. Lloyd,J.W.: FoundationsofLogicPr. Springer(1987)
18. Fisher,M.,Mascardi,V.,Rozier,K.Y.,Schlingloff,B.,Winikoff,M.,Yorke-Smith,N.: To-
wardsaframeworkforcertificationofreliableautonomoussystems. AutonomousAgents
andMultiAgentSystems35(1)(2021)
19. Clarke,E.M.,Lerda,F.:Modelchecking:Softwareandbeyond.JournalofUniversalComp.
Sc.13(5)(2007)639–649
20. Clarke,E.M.,Henzinger,T.A.,Veith,H.,Bloem,R.,eds.: HandbookofModelChecking.
Springer(2018)
21. Cousot,P.,Cousot,R.: Abstractinterpretation:aunifiedlatticemodelforstaticanalysisof
programsbyconstructionorapproximationoffixpoints.In:Conf.RecordoftheFourthAn-
nualACMSIGPLAN-SIGACTSymposiumonPrinciplesofPr.Languages,LosAngeles,
California,ACMPress,NewYork,NY(1977)238–252
22. Shapiro,S.,LespA˜©rance,Y.,Levesque,H.: Thecognitiveagentsspecificationlanguage
andverificationenvironment(2010)23. Shapiro,S.,Lesperance,Y.,Levesque,H.J.: Thecognitiveagentsspecificationlanguage
andverificationenvironmentformultiagentsystems. In:Proc.oftheFirstInt.JointConf.
onAutonomousAgentsandMultiagentSystems,AAMAS’02,ACMPress,NewYork,NY
(2002)19–26
24. Holzmann,G.: Themodelcheckerspin. IEEETransactionsonSoftwareEngineering(23)
(199)279–295
25. Bourahla,M.,Benmohamed,M.:Modelcheckingmulti-agentsystems.Informatica(Slove-
nia)29(2)(2005)189–198
26. Kacprzak, M., Lomuscio, A., Penczek, W.: Verification of multiagent systems via un-
boundedmodelchecking. In:Proc.oftheThirdInt.JointConf.onAutonomousAgents
andMultiagentSystems,AAMAS’04,ACMPress,NewYork,NY(2004)638–645
27. McMillan,K.L.: SymbolicModelChecking. KluwerAcademicPublishers:Boston,MA
(1993)
28. Holzmann, G.: Design and Validation of Comp. Protocols. Prentice Hall Intl.: Hemel
Hempstead,England(1991)
29. Vardi,M.Y.: Branchingvs.lineartime:Finalshowdown. In:Proc.ofthe2001Conf.on
ToolsandAlgorithmsfortheConstructionandAnalysisofSystems,TACAS2001.Number
2031inLectureNotesinComputerScience,Springer-Verlag(2001)1–22
30. Rozier,K.Y.,Vardi,M.Y.: LTLsatisfiabilitychecking. Int.J.Softw.ToolsTechnol.Transf.
12(2)(2010)123–137
31. Rozier, K.Y.: Linear temporal logic symbolic model checking. Comput. Sci. Rev. 5(2)
(2011)163–203
32. Walton,C.: Verifiableagentdialogues. J.AppliedLogic5(2)(2007)197–213
33. Bordini, R., Fisher, M., Visser, W., Wooldridge, M.: Verifying multi-agent programs by
modelchecking. AutonomousAgentsandMulti-AgentSystems12(2)(2006)239–256
34. Jones, A., Lomuscio, A.: Distributed bdd-based bmc for the verification of multi-agent
systems. In:Proc.ofthe9thInt.Conf.onAutonomousAgentsandMultiagentSystems
(AAMAS2010).(2010)
35. Montali, M., Alberti, M., Chesani, F., Gavanelli, M., Lamma, E., Mello, P., Torroni, P.:
Verificationfromdeclarativespecificationsusinglogicprogramming.In:24thInt.Conf.on
LogicProgramming(ICLP’08).LNCS5366,SpringerVerlag(December2008)440–454
36. Lomuscio, A., Qu, H., Raimondi, F.: MCMAS: an open-source model checker for the
verificationofmulti-agentsystems. Int.J.Softw.ToolsTechnol.Transf.19(1)(2017)9–30
37. Lomuscio,A.,Lasica,T.,Penczek,W.: Boundedmodelcheckingforinterpretedsystems:
preliminaryexperimentalresults. In:Proc.ofFAABSII.Number2699inLectureNotesin
ComputerScience,Springer-Verlag(2003)
38. Kong,J.,Lomuscio,A.: Symbolicmodelcheckingmulti-agentsystemsagainstctl*kspec-
ifications. InLarson,K.,Winikoff,M.,Das,S.,Durfee,E.H.,eds.:Proceedingsofthe16th
ConferenceonAutonomousAgentsandMultiAgentSystems,AAMAS2017,ACM(2017)
114–122
39. Fisher,M.: ModelcheckingAgentSpeak. In:Proc.oftheSecondInt.JointConf.onAu-
tonomousAgentsandMultiagentSystemsAAMAS03.LectureNotesinComputerScience
3862,ACMPress(2003)409–416
40. Visser,W.,Havelund,K.,Brat,G.,Park,S.,Lerda,F.: Modelcheckingprograms. Autom.
Softw.Eng.10(2)(2003)203–232
41. Dennis, L.A., Fisher, M., Lincoln, N., Lisitsa, A., Veres, S.M.: Practical verification of
decision-making in agent-based autonomous systems. Autom. Softw. Eng. 23(3) (2016)
305–359
42. Dennis, L.A.: The MCAPL framework including the agent infrastructure layer an agent
javapathfinder. J.ofOpenSourceSoftware3(24)(2018)61743. Dennis,L.A.,Bentzen,M.M.,Lindner,F.,Fisher,M.: Verifiablemachineethicsinchang-
ing contexts. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,
Thirty-ThirdConferenceonInnovativeApplicationsofArtificialIntelligence,IAAI2021,
TheEleventhSymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2021,
VirtualEvent,February2-9,2021,AAAIPress(2021)11470–11478
44. Costantini,S.,Tocchio,A.: Adialoguegamesframeworkfortheoperationalsemanticsof
logic agent-oriented languages. In Dix, J., Leite, J., Governatori, G., Jamroga, W., eds.:
ComputationalLogicinMulti-AgentSystems,11thInternationalWorkshop,CLIMAXI,
Proceedings.Volume6245ofLectureNotesinComputerScience.,Springer(2010)238–
255
45. Costantini, S., Tocchio, A.: A logic programming language for multi-agent systems. In
Flesca, S., Greco, S., Leone, N., Ianni, G., eds.: Logics in Artificial Intelligence, Euro-
peanConference,JELIA2002,Proceedings.Volume2424ofLectureNotesinComputer
Science.,Springer(2002)
46. Costantini, S., Tocchio, A.: The DALI logic programming agent-oriented language. In
Alferes,J.J.,Leite,J.A.,eds.:LogicsinArtificialIntelligence,9thEuropeanConference,
JELIA2004,Proceedings.Volume3229ofLectureNotesinComputerScience.,Springer
(2004)685–688
47. Costantini,S.,Tocchio,A.: Aboutdeclarativesemanticsoflogic-basedagentlanguages.
In: Declarative Agent Languages and Technologies III, Third Intl. Works., DALT 2005,
SelectedandRevisedPapers.Volume3904ofLNAI. Springer(2006)106–123
48. DeGasperis,G.,Costantini,S.,Nazzicone,G.: Dalimultiagentsystemsframework,doi
10.5281/zenodo.11042. DALI GitHub Software Repository (July 2014) DALI: http:
//github.com/AAAI-DISIM-UnivAQ/DALI.
49. Tocchio,A.:Multi-AgentsystemsinComp.logic.PhDthesis,DipartimentodiInformatica,
Universita`degliStudidiL’Aquila(2005)
50. Wallace,S.A.:Identifyingincorrectbehavior:Theimpactofbehaviormodelsondetectable
errormanifestations.In:Proc.oftheFourteenthConf.onBehaviorRepresentationinMod-
elingandSimulation(BRIMS-05).(2005)
51. Rozier,K.Y.: Specification:Thebiggestbottleneckinformalmethodsandautonomy. In
Blazy, S., Chechik, M., eds.: Verified Software. Theories, Tools, and Experiments - 8th
InternationalConference,VSTTE2016,RevisedSelectedPapers.Volume9971ofLecture
NotesinComputerScience.(2016)8–26
52. Ferrando,A.,Dennis,L.A.,Ancona,D.,Fisher,M.,Mascardi,V.: Verifyingandvalidat-
ingautonomoussystems:Towardsanintegratedapproach. InColombo,C.,Leucker,M.,
eds.:RuntimeVerification-18thInternationalConference,RV2018,Proceedings.Volume
11237ofLectureNotesinComputerScience.,Springer(2018)263–281
53. Ferrando, A., Winikoff, M., Cranefield, S., Dignum, F., Mascardi, V.: On enactability
of agent interaction protocols: Towards a unified approach. In Dennis, L.A., Bordini,
R.H., Lespe´rance, Y., eds.: Engineering Multi-Agent Systems - 7th International Work-
shop,EMAS2019,RevisedSelectedPapers.Volume12058ofLectureNotesinComputer
Science.,Springer(2019)43–64
54. Ferrando,A.: Theearlybirdcatchestheworm:Firstverify,thenmonitor! Sci.Comput.
Program.172(2019)160–179
55. Kejstova´,K.,Rockai,P.,Barnat,J.:Frommodelcheckingtoruntimeverificationandback.
CoRRabs/1805.12428(2018)
56. Winfield,A.F.T.,Michael,K.,Pitt,J.,Evers,V.:Machineethics:Thedesignandgovernance
ofethicalAIandautonomoussystems. ProceedingsoftheIEEE107(3)(2019)509–517
57. Allen,C.,Varner,G.,Zinser,J.: Prolegomenatoanyfutureartificialmoralagent. J.Expt.
Theor.Artif.Intell.(12)(2000)251–26158. Asaro,P.: Whatshouldwewantfromarobotethic? InternationalReviewofInformation
Ethics(6)
59. Allen, C., Smit, I., Wallach, W.: Artificial morality: Top-down, bottom-up, and hybrid
approaches. EthicsandInformationTechnology
60. Moor,J.:TheDartmouthCollegeArtificialIntelligenceConference:TheNextFiftyYears.
AIMag.27(4)(2006)87–91
61. Powers, T.M.: Prospects for a kantian machine. IEEE Intelligent Systems 21(4) (2006)
46–51
62. Anderson,M.,Anderson,S.L.,Armen,C.: Anapproachtocomputingethics. IEEEIntell.
Syst.21(4)(2006)56–63
63. Dyoub,A.: TowardsEthicalChatbots:EvaluatingtheEthicalBehaviorofEmployeesin
CustomerServiceOnlineChat.PhDthesis,DepartmentofInformationEngineering,Com-
puter Science and Mathematics, University of L’Aquila (2019) Supervisor Prof. Stefania
Costantini.
64. Nallur,V.: Landscapeofmachineimplementedethics. CoRRabs/2009.00335(2020)
65. Dyoub,A.,Costantini,S.,Lisi,F.A.: Logicprogrammingandmachineethics. InRicca,
F.,Russo,A.,Greco,S.,Leone,N.,Artikis,A.,Friedrich,G.,Fodor,P.,Kimmig,A.,Lisi,
F.A.,Maratea,M.,Mileo,A.,Riguzzi,F.,eds.:Proceedings36thInternationalConference
on Logic Programming, ICLP2020 Technical Communications. Volume 325 of EPTCS.
(2020)6–17
66. Tolmeijer, S., Kneer, M., Sarasua, C., Christen, M., Bernstein, A.: Implementations in
machineethics:Asurvey. ACMComput.Surv.53(6)(2021)132:1–132:38
67. Pereira, L.M., Saptawijaya, A.: Programming Machine Ethics. Volume 26 of Studies in
AppliedPhilosophy,EpistemologyandRationalEthics. Springer(2016)
68. Pereira,L.M.,Lopes,A.B.: MachineEthics-FromMachineMoralstotheMachineryof
Morality.Volume53ofStudiesinAppliedPhilosophy,EpistemologyandRationalEthics.
Springer(2020)
69. Sergot,M.J.,Craven,R.: ThedeonticcomponentofactionlanguagenC+. InGoble,L.,
Meyer,J.C.,eds.:DeonticLogicandArtificialNormativeSystems,8thInternationalWork-
shoponDeonticLogicinComputerScience,DEON2006,Proceedings.Volume4048of
LectureNotesinComputerScience.,Springer(2006)222–237
70. Sergot, M.J.: Action and agency in norm-governed multi-agent systems. In Artikis, A.,
O’Hare,G.M.P.,Stathis,K.,Vouros,G.A.,eds.:EngineeringSocietiesintheAgentsWorld
VIII,8thInternationalWorkshop,ESAW2007,RevisedSelectedPapers.Volume4995of
LectureNotesinComputerScience.,Springer(2007)1–54
71. Artikis, A., Sergot, M.J., Pitt, J.V.: Specifying norm-governed computational societies.
ACMTrans.Comput.Log.10(1)(2009)1–42
72. Sergot,M.J.: Norms,actionandagencyinmulti-agentsystems. InGovernatori,G.,Sartor,
G.,eds.:DeonticLogicinComputerScience,10thInternationalConference,DEON2010,
Proceedings.Volume6181ofLectureNotesinComputerScience.,Springer(2010) 2
73. Brewka,G.,Eiter,T.,(eds.),M.T.: Answersetprogramming:Specialissue. AIMagazine
37(3)(2016)
74. Gelfond,M.,Lifschitz,V.: Thestablemodelsemanticsforlogicprogramming. In:Logic
Programming,ProceedingsoftheFifthInternationalConferenceandSymposium,Seattle,
Washington,August15-19,1988(2Volumes).Volume88.,MITPress(1988)1070–1080
75. Gelfond,M.,Lifschitz,V.: Classicalnegationinlogicprogramsanddisjunctivedatabases.
Newgenerationcomputing,Springer9(3-4)(1991)365–385
76. Marek, V.W., Truszczyn´ski, M.: Stable models and an alternative logic programming
paradigm. In:TheLogicProgrammingParadigm. Springer(1999)375–398
77. Lifschitz,V.: AnswerSetPlanning. InSchreye,D.D.,ed.:LogicProgramming:The1999
InternationalConference,MITPress(1999)23–3778. Gebser,M.,Leone,N.,Maratea,M.,Perri,S.,Ricca,F.,Schaub,T.: Evaluationtechniques
andsystemsforanswersetprogramming:asurvey. In:ProceedingsoftheTwenty-Seventh
InternationalJointConferenceonArtificialIntelligence,IJCAI-18,InternationalJointCon-
ferencesonArtificialIntelligenceOrganization(72018)5450–5456
79. Cointe,N.,Bonnet,G.,Boissier,O.: Ethicaljudgmentofagents’behaviorsinmulti-agent
systems. In Jonker, C.M., Marsella, S., Thangarajah, J., Tuyls, K., eds.: Proceedings of
the2016InternationalConferenceonAutonomousAgents&MultiagentSystems,ACM
(2016)1106–1114
80. Berreby,F.,Bourgne,G.,Ganascia,J.: Adeclarativemodularframeworkforrepresenting
andapplyingethicalprinciples. InLarson,K.,Winikoff,M.,Das,S.,Durfee,E.H.,eds.:
Proceedingsofthe16thConferenceonAutonomousAgentsandMultiAgentSystems,AA-
MAS2017,ACM(2017)96–104
81. Berreby,F.,Bourgne,G.,Ganascia,J.: Event-basedandscenario-basedcausalityforcom-
putationalethics. InAndre´,E.,Koenig,S.,Dastani,M.,Sukthankar,G.,eds.:Proceedings
ofthe17thInternationalConferenceonAutonomousAgentsandMultiAgentSystems,AA-
MAS2018,ACM(2018)147–155
82. Kowalski,R.,Sergot,M.: Alogic-basedcalculusofevents. NewGenerationComputing4
(1986)67–95
83. Dennis,L.A.,Bentzen,M.M.,Lindner,F.,Fisher,M.: Verifiablemachineethicsinchang-
ing contexts. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,
Thirty-ThirdConferenceonInnovativeApplicationsofArtificialIntelligence,IAAI2021,
TheEleventhSymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2021,
VirtualEvent,2021,AAAIPress(2021)11470–11478
84. Dyoub, A., Costantini, S., Lisi, F.A.: Towards ethical machines via logic programming.
In Bogaerts, B., Erdem, E., Fodor, P., Formisano, A., Ianni, G., Inclezan, D., Vidal, G.,
Villanueva, A., Vos, M.D., Yang, F., eds.: Proceedings 35th International Conference on
LogicProgramming(TechnicalCommunications),ICLP2019TechnicalCommunications.
Volume306ofEPTCS.(2019)333–339
85. Dyoub,A.,Costantini,S.,Lisi,F.A.:Learninganswersetprogrammingrulesforethicalma-
chines. InCasagrande,A.,Omodeo,E.G.,eds.:Proceedingsofthe34thItalianConference
onComputationalLogic.Volume2396ofCEURWorkshopProceedings.,CEUR-WS.org
(2019)300–315
86. Dyoub,A.,Costantini,S.,Lisi,F.A.:Learninganswersetprogrammingrulesforethicalma-
chines. InCasagrande,A.,Omodeo,E.G.,eds.:Proceedingsofthe34thItalianConference
onComputationalLogic.Volume2396ofCEURWorkshopProceedings.,CEUR-WS.org
(2019)300–315
87. Dyoub,A.,Costantini,S.,Lisi,F.A.,Letteri,I.: Logic-basedmachinelearningfortrans-
parentethicalagents. InCalimeri,F.,Perri,S.,Zumpano,E.,eds.:Proceedingsofthe35th
ItalianConferenceonComputationalLogic-CILC2020.Volume2710ofCEURWork-
shopProceedings.,CEUR-WS.org(2020)169–183
88. Hill,P.M.,Lloyd,J.W.: Analysisofmetaprograms. In:Meta-ProgramminginLogicPro-
gramming,Cambridge,Mass.,TheMITPress(1988)23–51
89. Barklund,J.:Whatisameta-variableinProlog?In:Meta-ProgramminginLogicProgram-
ming. TheMITPress,Cambridge,Mass.(1989)383–98
90. van Harmelen, F.: Definable naming relations in meta-level systems. In: Meta-
ProgramminginLogic.LectureNotesinComputerScience649,Berlin,Springer(1992)
89–104
91. Barklund, J., Costantini, S., Dell’Acqua, P., Lanzarone, G.A.: Semantical properties of
encodingsinlogicprogramming. In:LogicProgramming–Proc.1995Intl.Symp.,Cam-
bridge,Mass.,MITPress(1995)288–30292. Perlis,D.,Subrahmanian,V.S.: Meta-languages,reflectionprinciples,andself-reference.
In:HandbookofLogicinArtificialIntelligenceandLogicProgramming,Volume2,Deduc-
tionMethodologies. OxfordUniversityPress(1994)323–358
93. Barklund, J., Dell’Acqua, P., Costantini, S., Lanzarone, G.A.: Semantical properties of
encodingsinlogicprogramming. InLloyd,J.W.,ed.:LogicProgramming,Proceedingsof
the1995InternationalSymposium,MITPress(1995)288–302
94. Costantini,S.: Meta-reasoning:aSurvey. In:Comp.Logic:LogicPr.andBeyond,Essays
in Honour of Robert A. Kowalski, Part II. Volume 2408 of Lecture Notes in Computer
Science.,Springer(2002)253–288
95. Costantini,S.,Lanzarone,G.A.: Ametalogicprogramminglanguage. In:LogicProgram-
ming,ProceedingsoftheSixthInternationalConference,MITPress(1989)218–233
96. Costantini,S.,Lanzarone,G.A.: Metalevelnegationinnon-monotonicreasoning. In:LP-
NMR,ProceedingsoftheWorkshoponLogicProgrammingandNon-MonotonicLogicat
ICLP1990.(1990)19–26
97. Costantini,S.,Formisano,A.: Addingmetalogicfeaturestoknowledgerepresentationlan-
guages. Fundam.Informaticae181(1)(2021)71–98
98. Dix,J.:Aclassificationtheoryofsemanticsofnormallogicprograms:I.Strongproperties.
Fundam.Inform.22(3)(1995)227–255
99. Rao,A.S.: AgentSpeak(L):BDIagentsspeakoutinalogicalcomputablelanguage. In:
AgentsBreakingAway,7thEuropeanWorks.onModellingAutonomousAgentsinaMulti-
AgentWorld,Proceedings.Volume1038ofLectureNotesinComputerScience.,Springer
(1996)42–55
100. Hindriks, K.V.: Programming rational agents in goal. In: Multi-Agent Programming.
SpringerUS(2009)119–157
101. Dastani,M.,vanRiemsdijk,M.B.,Meyer,J.J.C.: Pr.multi-agentsystemsin3APL
102. Dyoub,A.,Costantini,S.,Lisi,F.A.: LearningAnswerSetProgrammingRulesforEthical
Machines.InCasagrande,A.,Omodeo,E.G.,eds.:ProceedingsoftheThirtyFourthItalian
ConferenceonComputationalLogicCILC2019,CEUR-WS.org(2019)300–315
103. Dyoub,A.,Costantini,S.,Lisi,F.A.: TowardsanILPApplicationinMachineEthics. In:
InductiveLogicProgramming-29thInternationalConference,ILP2019,Proceedings.Vol-
ume11770ofLectureNotesinComputerScience.,Springer(2019)26–35
104. Dyoub, A., Costantini, S., Lisi, F.A.: Towards ethical machines via logic programming.
In Bogaerts, B., Erdem, E., Fodor, P., Formisano, A., Ianni, G., Inclezan, D., Vidal, G.,
Villanueva, A., Vos, M.D., Yang, F., eds.: Proceedings 35th International Conference on
LogicProgramming(TechnicalCommunications),ICLP2019TechnicalCommunications.
Volume306ofEPTCS.(2019)333–339
105. Costantini,S.,Tocchio,A.:Aboutdeclarativesemanticsoflogic-basedagentlanguages.In
Baldoni,M.,Endriss,U.,Omicini,A.,Torroni,P.,eds.:DeclarativeAgentLanguagesand
TechnologiesIII,ThirdInternationalWorkshop,DALT2005,SelectedandRevisedPapers.
Number3904inLectureNotesinComputerScience. Springer(2006)106–123
106. Costantini, S., Pitoni, V.: Reasoning about memory management in resource-bounded
agents. InCasagrande,A.,Omodeo,E.G.,eds.:Proceedingsofthe34thItalianConference
onComputationalLogic.Volume2396ofCEURWorkshopProceedings.,CEUR-WS.org
(2019)217–228
107. Pitoni, V., Costantini, S.: A temporal module for logical frameworks. In Bogaerts, B.,
Erdem, E., Fodor, P., Formisano, A., Ianni, G., Inclezan, D., Vidal, G., Villanueva, A.,
Vos,M.D.,Yang,F.,eds.:Proceedings35thInternationalConferenceonLogicProgram-
ming(TechnicalCommunications),ICLP2019TechnicalCommunications.Volume306of
EPTCS.(2019)340–346108. Henzinger,T.A.,Manna,Z.,Pnueli,A.: Timedtransitionsystems. In:Real-Time:Theory
inPractice,REXWorks.,Proceedings.Volume600ofLectureNotesinComputerScience.,
Springer(1992)226–251
109. Costantini,S.,Formisano,A.: WeightconstraintswithpreferencesinASP. InDelgrande,
J.P.,Faber,W.,eds.:LogicProgrammingandNonmonotonicReasoning-11thInternational
Conference,LPNMR2011,Proceedings.Volume6645ofLectureNotesinComputerSci-
ence.,Springer(2011)229–235
110. Costantini,S.,Formisano,A.: PreferencesandprioritiesinASP. InLisi,F.A.,ed.:Pro-
ceedings of the 9th Italian Convention on Computational Logic. Volume 857 of CEUR
WorkshopProceedings.,CEUR-WS.org(2012)47–58
111. Costantini, S., Formisano, A.: Modeling preferences and conditional preferences on re-
sourceconsumptionandproductioninASP. JournalofofAlgorithmsinCognition,Infor-
maticsandLogic64(1)(2009)
112. Costantini, S.: Answer set modules for logical agents. In Gottlob, G., ed.: Datalog 2.0.
Number6702inLectureNotesinComputerScience. Springer(2012)
113. Costantini,S.,DeGasperis,G.,Pitoni,V.,Salutari,A.:DALI:Amultiagentsystemframe-
workfortheweb,cognitiveroboticandcomplexeventprocessing. InDellaMonica,D.,
Murano, A., Rubin, S., Sauro, L., eds.: Joint Proceedings of the 18th Italian Conference
onTheoreticalComputerScienceandthe32ndItalianConferenceonComputationalLogic
co-locatedwiththe2017IEEEInternationalWorkshoponMeasurementsandNetworking
(2017IEEEM&N).Volume1949ofCEURWorkshopProceedings.,CEUR-WS.org(2017)
286–300Downloadathttps://github.com/AAAI-DISIM-UnivAQ/DALI.
114. Costantini, S., Tocchio, A., Verticchio, A.: Communication and trust in the DALI logic
programmingagent-orientedlanguage. IntelligenzaArtificiale,InternationalJournalofthe
ItalianAssociationAIxIA2(1)(2005)39–46
115. Carlsson,M.,Mildner,P.: SicstusProlog–thefirst25years. TheoryandPracticeofLogic
Programming12(1,2)SpecialIssueonPrologSystems.
116. Costantini,S.,DeGasperis,G.,Pitoni,V.,Salutari,A.: Dali:Amultiagentsystemframe-
workfortheweb,cognitiveroboticandcomplexeventprocessing. In:Proceedingsofthe
32ndItalianConferenceonComputationalLogic.Volume1949ofCEURWorkshopPro-
ceedings.,CEUR-WS.org(2017)286–300http://ceur-ws.org/Vol-1949/CILCpaper05.pdf.
117. Costantini,S.,DeGasperis,G.,Nazzicone,G.:DALIforcognitiverobotics:Principlesand
prototypeimplementation. InLierler,Y.,Taha,W.,eds.:PracticalAspectsofDeclarative
Languages-19thInternationalSymposium,Proceedings.Volume10137ofLectureNotes
inComputerScience.,Springer(2017)152–162
118. Tufis,M.,Ganascia,J.: AnormativeextensionfortheBDIagentmodel. In:Proceedings
of the 17th International Conference on Climbing and Walking Robots and the Support
TechnologiesforMobileMachines.(2014)691–702
119. Montali,M.,Chesani,F.,Mello,P.,Torroni,P.: Modelingandverifyingbusinessprocesses
andchoreographiesthroughtheabductiveproofproceduresciffanditsextensions. Intelli-
genzaArtificiale,InternationalJournaloftheItalianAssociationAIxIA5(1)(2011)
120. Montali, M., Alberti, M., Chesani, F., Gavanelli, M., Lamma, E., Mello, P., Torroni, P.:
Verificationfromdeclarativespecificationsusinglogicprogramming.In:24thInternational
ConferenceonLogicProgramming(ICLP’08).Volume5366ofLectureNotesinComputer
Science.,Springer(2008)440–454
121. Barringer,H.,Rydeheard,D.E.,Havelund,K.:Rulesystemsforrun-timemonitoring:from
eagletoruler. J.Log.Comput.20(3)(2010)675–706
122. Bringsjord,S.,Arkoudas,K.,Bello,P.: Towardagenerallogicistmethodologyforengi-
neeringethicallycorrectrobots. IEEEIntelligentSystems21(4)(2006)38–44123. Caianiello,P.,Costantini,S.,DeGasperis,G.,Florio,N.,Gobbo,F.: Applicationofhybrid
agentstosmartenergymanagementofaprosumernode.InOmatu,S.,Neves,J.,Rodr´ıguez,
J.M.C.,Santana,J.F.D.P.,Rodr´ıguez-Gonza´lez,S.,eds.:DistributedComputingandArtifi-
cialIntelligence-10thInternationalConference,DCAI2013.Volume217ofAdvancesin
IntelligentSystemsandComputing.,Springer(2013)597–607
124. Rushby,J.M.: Runtimecertification. InLeucker,M.,ed.:RuntimeVerification,8thIntl.
Works.,RV2008.SelectedPapers.Volume5289ofLectureNotesinComputerScience.
Springer(2008)21–35