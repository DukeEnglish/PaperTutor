A systematic investigation of learnability from single child linguistic input
YuluQin1 (yq810@nyu.edu)
WengtaoWang1 (ww2135@nyu.edu)
BrendenM.Lake1,2 (brenden@nyu.edu)
1CenterforDataScience,2DepartmentofPsychology,NewYorkUniversity
Abstract onlytheirowninput—theycannotshareandaggregateinput
withothers—andthusthisisthesettingwefocusonhere.
Languagemodels(LMs)havedemonstratedremarkableprofi-
ciencyingeneratinglinguisticallycoherenttext,sparkingdis- Here, we use a recent article by Wang et al. (2023) as a
cussions about their relevance to understanding human lan- launchpadforournewlearnabilitystudiesbasedonasingle
guagelearnability. However,asignificantgapexistsbetween
child’sinput. Wangetal.(2023)appliedtwoneurallanguage
the training data for these models and the linguistic input a
child receives. LMs are typically trained on data that is or- models,ContinuousBag-of-Words(CBOW;Mikolov,Chen,
ders of magnitude larger and fundamentally different from Corrado, & Dean, 2013) and Long Short-Term Memory
child-directed speech (Warstadt & Bowman, 2022; Warstadt
(LSTM;Hochreiter&Schmidhuber,1997),totheSAYCam-
et al., 2023; Frank, 2023a). Addressing this discrepancy,
our research focuses on training LMs on subsets of a sin- S dataset, a longitudinal collection of transcribed linguis-
gle child’s linguistic input. Previously, Wang, Vong, Kim, tic inputs to a single child aged 6 to 25 months (Sullivan,
and Lake (2023) found that LMs trained in this setting can
Mei, Perfors, Wojcik, & Frank, 2021). Wang et al. (2023)’s
form syntactic and semantic word clusters and develop sen-
sitivitytocertainlinguisticphenomena,buttheyonlyconsid- studyrevealedthatthesemodelssuccessfullyrecoveredlexi-
eredLSTMsandsimplerneuralnetworkstrainedfromjustone calclassesthatreflectkeysyntacticandsemanticdistinctions,
single-childdataset. Here,toexaminetherobustnessoflearn- including nouns, verbs, animals, body parts, etc., from the
abilityfromsingle-childinput,wesystematicallytrainsixdif-
ferentmodelarchitecturesonfivedatasets(3single-childand process of learning to predict the next word in transcribed
2baselines). Wefindthatthemodelstrainedonsingle-child child-directed utterances. Additionally, they employed the
datasetsshowedconsistentresultsthatmatchedwithprevious
Zorro test suite to evaluate the models’ grammatical knowl-
work,underscoringtherobustnessofformingmeaningfulsyn-
tacticandsemanticrepresentationsfromasubsetofachild’s edgethroughacceptabilityjudgments(Huebneretal.,2021).
linguisticinput. However, these promising findings are based on two model
Keywords: learnability; single-child; distributionallearning; architectures trained on only one single child’s data, thus
robustness;languagemodels limiting the generalizability of their results. Our research
builds upon this groundwork by investigatingthe robustness
Introduction
of Wang et al. (2023)’s learnability results from one child’s
Young children are remarkably efficient language learners, inputacrossdifferentsettings,includingmultipledatasetsand
yetthemechanismsbehindlanguageacquisitionremainasci- different model architectures, to see which combinations of
entific puzzle. Meanwhile, important advances in language datasetsandarchitecturescanproducesuccessfullearners.
models (LMs) for natural language processing provide us Specifically, inthisstudy, weexamined6modelarchitec-
withnew,powerfulcomputationaltoolstoinvestigatefunda- tures(3modelclassesand2sizeseach)trainedon5datasets:
mentalquestionsregardinglanguageacquisitionanditsrela- 3datasetsrepresentinginputtoindividualchildrenand2oth-
tionship with human cognition (Warstadt & Bowman, 2022; ers representing meaningful baselines for comparison. Each
Frank,2023b). Trainedontrillionsofwrittenwords,contem- combinationofarchitectureanddatasetwasanalyzedthrough
porary Transformer-based Large Language Models (LLMs) linguistic acceptability tests, visualizations of word embed-
canproducecoherenttextwithaproficiencythatfarexceeds dings,andclozetests. Acrosseachofthesesettings,wefind
the predictions of experts in the field from a decade ago thattheresultsarerobustandsimilartoWangetal.(2023)’s.
(Chang & Bergen, 2023), raising important questions about Wediscussthedetailsinthesectionsthatfollow.
the degree to which strong inductive biases and language-
specificmechanismsareneededtoacquirelanguagebeyond Methods
more general distributional learning mechanisms (Landauer,
Datasets
Foltz, & Laham, 1998; Elman, 1990). To improve the rele-
vanceoflanguagemodelsascognitivemodelsofhumanlan- We explored 5 datasets, three that capture child-directed
guage acquisition, previous efforts trained models on aggre- speech at the level of a single child, one aggregating child-
gatedlinguisticinputacrossmultiplechildren(Warstadtetal., directed speech from multiple children, and one with an
2023; Huebner, Sulem, Cynthia, & Roth, 2021). As in sev- equivalentamountoftextfromtheweb.
eralworks(Wangetal.,2023; Vong,Wang,Orhan,&Lake, SAYCam-S, Sarah and Ellie. These are three different
2024;Abend,Kwiatkowski,Smith,Goldwater,&Steedman, single-child datasets in our experiments. SAYCam-S is the
2017; Waterfall, Sandbank, Onnis, & Edelman, 2010), we single child dataset used in Wang et al. (2023). The other
trainmodelsonsubsetsofthelinguisticinputthatjustasin- twochild-directeddatasetsaretwosetsoftranscribedspeech
glechildwasexposedto. Childrenmustlearnlanguagefrom from CHILDES (MacWhinney, 2000), each directed to one
4202
beF
21
]LC.sc[
1v99870.2042:viXraTable 1: Dataset Statistics. SAYCam-S, Sarah, and Ellie are three single-child datasets. Note that all datasets except CHILDES have a
similarnumberoftrainingtokens.
SAYCam-S Sarah Ellie Wikipedia CHILDES
Numberofutterances 26,255 32,965 38,140 10,504 1,151,816
Mean(SD)utterancelength 8.07(5.46) 6.71(3.32) 6.29(3.14) 24.81(14.60) 7.09(4.19)
Numberoftokens 211,947 221,211 239,807 260,580 8,163,820
Out-of-vocabularyrate 1.85% 1.26% 1.74% 9.69% 4.11%
Vocabularysize 2350 2333 2780 8833 15,762
Numberofutterances 1460 1786 2269 588 64,254
Mean(SD)utterancelength 7.96(5.46) 6.79(3.50) 6.03(3.00) 25.50(14.63) 7.16(4.09)
Numberoftokens 11,621 12,119 13,676 14,995 459,787
Out-of-vocabularyrate 2.21% 2.24% 3.58% 12.04% 3.98%
individualchild:Sarah(agerangingfrom2;3to5;1)fromthe Table2: ModelArchitectures. #oftrainableparametersarebased
ontheSAYCam-Sdataset,withslightvariationacrossdatasetsdue
Browncorpus(Brown,1973)andEllie(agerangingfrom0;9
todifferencesinvocabularysize.
to5)fromtheSakalicorpus(Beaupoil-Hourdel,2015).These
two datasets, respectively sourced from the North American Model #ofparameters
English and the British English sections of the CHILDES LSTM(1-layer) 3.3M
database,capturelongitudinalrecordingsinnaturalisticcon- LSTM(2-layer) 5.4M
texts. AsshowninTable1,thesethreedatasetspresentsimi- GPT-2(2-layer) 7.8M
larstatisticsintermsofvocabularysize,lengthofutterances GPT-2(8-layer) 26.7M
andnumberoftokens. BabyBERTa(2-layer) 7.8M
Wikipedia. As a comparison, we also have a randomly BabyBERTa(8-layer) 26.8M
sampled Wikipedia dataset with a parallel amount of text
tokens to Ellie, the child dataset that contains the most to-
kens. (After filtering sentences with fewer than 2 words, styleTransformerscalledBabyBERTa1(Radfordetal.,2019;
as discussed below in Data Preprocessing, the final token
Liuetal.,2019;Huebneretal.,2021),inadditiontoLSTMs
counts varied slightly.) Notably, with its longer average ut-
(Hochreiter&Schmidhuber,1997). Wetesttwomodelsizes
terancelengthandmorecomplexcontent,thisWikipediaset
ofeachmodelclass. Thecomprehensivelistofmodelarchi-
has fewer sentences but a larger vocabulary than the afore-
tecturesusedisdetailedinTable2.
mentioned child-directed datasets. Detailed statistics can be
Training objectives. All models were trained from
foundinTable1.
scratch. For LSTMs and GPT-2-based Transformers, the
CHILDES. Finally, as a reference, we incorporated the models aimed to predict the next token in a short utterance,
NorthAmericanPortionoftheCHILDEScorpus. Itcontains using cross-entropy loss for training. For the BabyBERTa-
aggregated child-directed data with a nearly 4× larger vo- based Transformer, the model was trained to predict ran-
cabularyandapproximately30×moretokensthanthesingle domly masked tokens, where 15% of the tokens in each ut-
childdatasets. SeethedetailedstatisticsinTable1. terance were masked, and the masks were chosen anew for
eachbatch.
DataPreprocessing
Modelconfigurations. Wetrained2architecturesoflarge
and small sizes for each model class, resulting in a total
BuiltontopofYedetore,Linzen,Frank,andMcCoy(2023)’s
of 6 architectures. These include uni-directional LSTMs (1
datapreprocessingprocedure,weexcludedchildren’sownut-
layerand2layers),aswellasGPT-2-basedandBabyBERTa-
terances to replicate data as similar as possible to the sen-
based Transformers (2 layers and 8 layers), as listed in Ta-
tenceschildrenreceiveandreplacedtokensthatappearfewer
ble2. Subsequently,weperformedanextensivehyperparme-
than 3 times with an <unk> token. We split approximately
tersearch. Wetunedandidentifiedthebesthyperparameters
90%ofeachdatasettotraining,5%tovalidation,and5%to
basedonvalidationperplexityforeachofourfivedatasets.2
testing. Wealsofilteroutsentencesthatcontainfewerthan2
wordsduringtrainingandvalidation.Detailsofdatasetstatis-
1Prior research has shown that a scaled-down version of
ticsfortrainingandvalidationcanbeseeninTable1.
RoBERTa-base termed BabyBERTa, achieves grammatical knowl-
edgecomparabletothefullRoBERTa-basewhentrainedonchild-
ModelArchitecturesandTraining directeddata(Huebneretal.,2021). Weappliedtheirinsightsand
will refer to our RoBERTa-based Transformer as a BabyBERTa-
basedTransformerinthefollowingsections.
Wangetal.(2023)investigatedn-grammodels,CBOWsand
2Detailed configurations for each model architecture can
LSTMs. In contrast, our evaluation expands to 6 different
be seen in https://github.com/yuluqinn/single-child
model architectures, including GPT-2-style and RoBERTa- -robustness.
gniniarT
noitadilaVTable3:ValidationPerplexity.
Model SAYCam-S Sarah Ellie Wikipedia CHILDES
LSTM(1-layer) 18.01 18.45 23.86 102.00 23.45
LSTM(2-layer) 18.47 18.40 23.59 98.70 23.74
GPT-2(2-layer) 18.74 18.97 23.93 127.58 20.81
GPT-2(8-layer) 18.42 18.46 23.94 130.54 20.15
BabyBERTa(2-layer) 10.41 10.96 16.24 74.38 10.39
BabyBERTa(8-layer) 9.25 10.67 14.94 65.10 10.35
Table4:ZorroTestAccuracies(%).
Model SAYCam-S Sarah Ellie Wikipedia CHILDES
LSTM(1-layer) 66.43 68.98 66.45 59.44 78.28
LSTM(2-layer) 69.18 68.25 64.59 61.64 81.49
GPT-2(2-layer) 68.22 68.70 65.40 57.47 86.40
GPT-2(8-layer) 65.76 70.49 66.45 61.88 87.83
BabyBERTa(2-layer) 69.57 70.23 66.28 59.02 84.63
BabyBERTa(8-layer) 65.45 66.42 64.46 59.54 81.65
For the hyperparameter search, we standardized all model Results
embedding and hidden sizes to 512 and all FFN intermedi-
Weanalyzeeachtrainedmodelthroughlinguisticacceptabil-
ate sizes for Transformer-based models to 2048. We used
itytestsforlinguisticknowledge,visualizationsofwordem-
ReduceOnPlateaulearningrateschedulerinPyTorch,which
beddingsforsyntacticandsemanticcategorystructures, and
reducesthelearningratebyafactorof10afterthevalidation
cloze tests for noun-verb distinction within context. In each
loss plateaus for 2 consecutive epochs. We used early stop-
analysis,wefindrobustresultssimilartoWangetal.(2023)
pingtoselectthecheckpointwiththebestvalidationloss.
acrossallmodelswithdifferentconfigurations.
Wetunedotherhyper-parametersbasedonvalidationper-
formance,including: LinguisticAcceptabilityTests
Following Wang et al. (2023), we tested models’ sensitivity
• learningrate∈{1×10−4, 3×10−4, 1×10−3, 3×10−3}
tolinguisticknowledgesuchassubject-verbagreementonthe
• batchsize∈{8,16,32} Zorrotestsuite(Huebneretal.,2021). Thistestsuiteevalu-
ates 13 grammatical phenomena on 23 tests, each contain-
• weightdecay∈{0.01, 0.05, 0.1, 0.15, 0.24}
ing2000minimalsentencepairs. Toavoidout-of-vocabulary
• dropoutrate∈{0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5} words,Wangetal.(2023)filteredoutallminimalpairscon-
taining tokens outside of their SAYCam-S vocabulary, left
• numberofattentionheads(forTransformer-basedmod-
with 15 tests, each containing fewer than 700 pairs. In this
els)∈{8, 16, 32}
work, we regenerated Zorro based on the original linguistic
Performanceforaparticularconfigurationisaveragedacross templates and the intersected vocabulary of our 5 datasets,
3runswithdifferentrandomseeds. Asameasureofgeneral-
resultinginafull23tests.4
ization, thevalidationperplexityscore3 forLSTM(1-layer), Test accuracy. From Table 4, we can see average Zorro
GPT2 (8-layer) and BabyBERTa (8-layer), is shown in Ta- test accuracies over 3 different random seeds are consistent
ble3. among 3 single-child datasets (Sarah, Ellie, and SAYCam-
S), nearly all of which reached over 65% correct (chance is
Tokenizer 50%). Among all single-child-directed datasets, the Sarah
dataset trained models with the best Zorro accuracy in all
We used the Hugging Face Tokenizers library to construct a
model architectures except LSTM (2-layer). Comparatively,
word-levelsimpletokenizerforeachdataset.Forconsistency,
acrossall5datasetsstudied,modelstrainedontheWikipedia
we specify an <sos>, an <eos>, an <unk>, a <mask> and a
datasetexhibitthelowestZorroaccuracy,whilethosetrained
<pad>tokenforalldatasetsandmodels. RefertoTable1for
on the CHILDES dataset achieve the highest. Furthermore,
thevocabularysizeforeachdataset.
foreachspecificlinguistictest,modelstrainedonsinglechild
3The perplexity is the exponentiation of the validation cross- datasets give consistent performances as seen in Figure 1.
entropy loss, defined as: perplexity = exp(H(X)), H(X) = Thefirstrowillustratesfourlinguistictestswheremostmod-
− N1∑N i=1logP(xi),whereHistheentropy,andX isarandomvari-
abledenotingatoken.Weuseditasamorestraightforwardmeasure 4The regenerated Zorro test suite can be found in https://
ofmodelperformanceonnext-wordpredictiontasks. github.com/wwt17/Zorro.case quantifiers argument structure determiner noun agreement
subjective pronoun existential there dropped argument between neighbors
(the baby gave she/she gave the there are (every books/many books) the purple bear gave her ./give her
these (dog/dogs) can be brown .
baby) my book . about big stars . the purple bear .
100%
90%
80%
70%
60%
50%
40%
30%
20%
subject verb agreement subject verb agreement subject verb agreement subject verb agreement
in simple question in question with aux across prepositional phrase across relative clause
the (flower/flowers) by the bears the (friends/friend) that you know
where is the (girls/girl) ? how (do/does) the song fit in here ?
are cool . was smart .
100%
90%
80%
70%
60%
50%
40%
30%
20%
sarah ellie saycam wiki childes sarah ellie saycam wiki childes sarah ellie saycam wiki childes sarah ellie saycam wiki childes
Dataset Dataset Dataset Dataset
LSTM (1-layer) LSTM (2-layer) GPT-2 (2-layer) GPT-2 (8-layer) BabyBERTa (2-layer) BabyBERTa (8-layer)
Figure 1: Zorro test accuracies across different settings. We tested 6 model architectures on 23 linguistic tests in Zorro. Each model
architecture,trainedwith3seeds,yielded18accuracydatapointsperdataset. Ourscatterplotsshowresultsfor8selectedtests,withthe
test name and an example sentence pair (unacceptable/acceptable) highlighted above each. For example, models evaluate which is more
acceptableinthe“case–subjectivepronoun”test: “thebabygaveshemybook.” or“shegavethebabymybook.” Wefoundmodelstrained
onsingle-childdatasetsexcelinspecifictestsbutstruggleinothers,likesubject-verbagreement. Fourhigh-performingtestsareshownin
thefirstrow,andfourlower-performingtests,particularlyforsubject-verbagreement,areinthesecondrow.Chanceisthedottedline.Runs
with3seedsshowvariability,similartopreviousfindings(Sellametal.,2022;Yedetoreetal.,2023).
elstrainedonsingle-childdatasetsperformwell,whereasthe corresponding to syntactic categories such as nouns, tran-
secondrowshowsmodelsperformpoorlyonthesubject-verb sitive verbs, and intransitive verbs, and semantic categories
agreementphenomenon.5 suchasfood,animalsandbodyparts. Toanalyzethecluster
In particular, all models trained on child-directed datasets structures of word embeddings in their trained models, they
exhibit high performance on the “quantifiers–existential visualizedtheembeddingsbyt-SNE(VanderMaaten&Hin-
there” test and perform near chance levels on the “subject- ton,2008)andclusterdendrograms.
verb agreement–across relative clause” test, which aligns to
To test the robustness of Wang et al.’s findings, our study
Wangetal.(2023)conclusionfrompreviousevaluations. As
expands these visualizations to all models we mentioned
a comparison, models trained on CHILDES achieve higher
above. As for syntactic distinctions, we found all models
testaccuracythanmodelstrainedonotherdatasets,yetthere
consistently exhibited clustering patterns in t-SNE plots and
is a noticeable variance in their accuracy as shown in the
dendrograms across various datasets. We first analyze word
bottom right plot of Figure 1. This variability underscores
embeddingsoffoursyntacticcategories(nouns,verbs,adjec-
thechallengeofmasteringthesyntacticknowledgerequired
tives, and adverbs) using t-SNE, as illustrated in Figure 2.
for subject-verb agreement tests, despite the more enriched
Focusing specifically on the three single-child datasets, we
linguistic context CHILDES provides. More generally, the
observeadistinctseparationbetweennouns(markedinred)
CHILDEScorpus, whichismuchlargerthanotherdatasets,
and verbs (marked in blue). Although some overlap exists,
alsoyieldedthebestperformanceinmanyothertests.
clustersofadjectivesandadverbsarestilldiscernible. Mod-
els trained on CHILDES and Wikipedia datasets displayed
VisualizationsforSyntacticandSemantic
more distinct clustering, likely due to their broader vocabu-
Categories
lariescomparedtosingle-childdatasets.
Intheirstudy,Wangetal.(2023)followedaplanofanalysis
As for semantic categorization, we use the same 8 child-
from Elman’s pioneering work (Elman, 1989, 1990, 1991),
directed semantic categories in Wang et al. (2023), which
demonstrating that CBOW and LSTM models when trained
was derived from WordBank (Frank, Braginsky, Yurovsky,
solelyontheSAYCam-Sdatasetcanformemergentclusters
& Marchman, 2016). Due to differences in vocabulary, we
cannotusethesamesetofwordsacrossalldatasets. There-
5A complete plot for model performances on all tests
fore,foreachdataset,weadaptthesetofwordsineachcat-
canbefoundinhttps://github.com/yuluqinn/single-child
-robustness. egory, enabling visualization of the six most frequent words
ecnamrofreP
ecnamrofrePnoun
verb
adjective
adverb
SAYCam-S SAYCam-S SAYCam-S Sarah Ellie CHILDES Wikipedia
LSTM (2-layer) BabyBERTa (8-layer) GPT-2 (8-layer) GPT-2 (8-layer) GPT-2 (8-layer) GPT-2 (8-layer) GPT-2 (8-layer)
Figure2:Clusteringdifferentmodels’wordembeddingsforsyntacticcategories. Werunt-SNEtovisualizeembeddingsofallwordsin
thevocabularythatarecategorizedintooneofthefoursyntacticcategories: noun,verb,adjective,andadverb. t-SNEuses1−cos(u,v)as
thedistancemetric.Weshowsevenvisualizationsherefromvarioustrainingdatasetsandmodelarchitectureslabeledbelowtheplots.Nouns
andverbsformtwolargesalientclusters,whileadjectivesandadverbsaremostlyclusteredtogether.
(a)GPT-2(2-layers)t-SNE (b)GPT-2(2-layer)Dendrogramclustering
(c)BabyBERTa(2-layer)t-SNE (d)LSTM(1-layer)t-SNE
Figure3: Clusteringwordembeddingsforsemanticcategories. Herewevisualizewordembeddingsofthreearchitecturestrainedonthe
Sarahdataset: (a, b)GPT-2(2-layer), (c)BabyBERTa(2-layer), (d)LSTM(1-layer). Again, t-SNEanddendrogramplotsusethecosine
measure in Figure 2. We present the 6 most frequent words from 8 different categories. Many distinct clusters correspond to semantic
categories,suchasbodyparts,clothing,animals,andplaces.
per category. Figure 3displays three modelsand revealsvi- ClozeTests
sually identifiable clusters such as body parts, clothing and Inadditiontoexaminingemergentlexicalclassesintherepre-
animals.6
sentationspace,wewantedtofurthertestifmodelscanprop-
6All models’ t-SNE and dendrogram plots for syntactic and semantic categories can be found in https://github.com/
yuluqinn/single-child-robustness.Table5:Clozeteststatisticsandaccuracies(%)ofdifferentiatingnounvs.verb.Webuildandevaluatetheclozetestsfromthevalidation
setforeachdatasetindependently.Chanceperformanceis50%.
SAYCam-S Sarah Ellie Wikipedia CHILDES
Numberofclozes 2412 1763 1801 343 74266
Ratioofnounclozes 35.16% 34.66% 38.87% 69.97% 38.76%
LSTM(1-layer) 97.89 96.48 94.23 93.88 96.66
LSTM(2-layer) 97.76 96.14 94.95 95.04 96.64
GPT-2(2-layer) 98.09 95.92 94.39 93.88 97.23
GPT-2(8-layer) 97.97 96.31 94.11 92.13 97.40
BabyBERTa(2-layer) 96.93 95.07 93.78 93.59 97.22
BabyBERTa(8-layer) 97.51 94.55 93.73 94.75 96.33
erly identify the syntactic category of a missing word based ingly, we observed comparable patterns in our study, even
onitssurroundingcontext. Therefore,followingWangetal. though we used a much smaller dataset comprising single-
(2023), we apply cloze tests (Taylor, 1953) to provide fur- childlinguisticinputandacorrespondingWikipediadataset.
ther evidence for syntactic category structures, specifically Specifically, we found that the Wikipedia dataset struggled
the noun-verb distinction. We use clozes such as “we are with tests such as dropped argument for ditransitive verb
going to here”, where this cloze expects either a noun or andlocalattractorinquestionwithauxiliaryverb, whilethe
a verb.7 We follow the same process as Wang et al. (2023) single-childdatasetsconsistentlyoutperformedintheseareas.
to generate and evaluate the clozes for each dataset. Cloze ThiscloselymirrorsthefindingsfromHuebneretal.(2021)’s
teststatisticsandaccuraciesareshowninTable5. Allofour studyusingaggregateddatasourcesandlargerdataquantity.
modelsachieveover90%accuracy,consistentlydemonstrat- Our results suggest that even limited data can be indicative
ingtheirabilitytocontextuallydifferentiatenounsandverbs. of differences between datasets and, potentially, that child-
directed speech may better equip models with the necessary
GeneralDiscussion
linguisticabilitiesforcertaintests.
InordertostudytherobustnessofWangetal.(2023)’slearn- The second key contribution of our study is an in-depth
abilityresultsfromonechild’slinguisticinput,wesystemati- examination of the robustness of the findings by Wang et
callytrained6modelarchitectureson3differentsingle-child al. (2023), which were originally based on one single-child
datasets. Wefoundalltrainedmodelsachievedconsistentre- dataset: SAYCam-S. We expanded this investigation to in-
sults in distinguishing syntactic and semantic categories of cludefivedifferentdatasetsandsixmodelarchitectures,sig-
words, as well as sensitivity to several linguistic phenom- nificantly broadening the scope. Additionally, we enhanced
ena. We observed high performance on linguistic tests such themethodologyforlinguisticevaluationusingtheZorrotest
as quantified existential “there” constructions, case of sub- suite (Huebner et al., 2021). Wang et al. previously lim-
jectivepronouns,anddroppedargumentforditransitiveverb. itedtheiranalysistosentencepairsfromZorrothatmatched
Butthesemodelsconsistentlyfailedonmorecomplicatedlin- SAYCam-S’s vocabulary, which resulted in a reduced test
guistic tests, such as subject-verb agreement across relative scope covering only 15 out of 23 tests and fewer than 700
clause. sentencepairspertest.Thislimitedsizepotentiallyweakened
Unlike other work considering the importance of the do- thevalidityoftheirconclusions. Incontrast,weregenerated
main of child-directed speech for learnability, this paper fo- the Zorro test suite to align with the intersected vocabulary.
cusesspecificallyontheroleofinputtoasinglechild. This Ourmodelswerethentestedoncomprehensivenew23tests
approach offers a more realistic baseline than methods that encompassing all 13 linguistic phenomena, with 2,000 sen-
trainmodelsonlarger,aggregateddatasources. Withasim- tencepairsineachtest. Thisapproachhasyieldedmorero-
ilargoal,BabyLMchallenge(Warstadtetal.,2023)explores bustandreliableresults.
learning under limited data conditions. However, even the
Ourstudydemonstratesthatmodelswithdifferentconfig-
smallest data track in the BabyLM challenge contains about
urationscanconsistentlylearntodistinguishseveralsyntactic
40timesmoredata(10millionwordtokens)thanoursingle-
andsemanticcategoriesandaresensitivetocertainlinguistic
childdataset.Similarly,inthestudybyHuebneretal.(2021),
testsbasedsolelyonthelinguisticinputfromasinglechild.
a RoBERTa-based Transformer was trained on 5 million to-
However,weacknowledgeseverallimitations. Firstly,while
kensfromanage-orderedversionofCHILDES(Huebner&
modelsdemonstratetheabilitytoformsyntacticandseman-
Willits, 2020) and an equivalent amount from a Wikipedia
tic clusters distinguishing lexical classes, it remains unclear
dataset.Theiranalysisofthemodel’sperformanceacrossvar-
how they acquire this representation and whether their un-
iouslinguisticphenomenawasconductedonZorro. Intrigu-
derstandingofthesecategoriesalignswithhumancognition.
7SimilartothecategorydistinctiontestinKimandSmolensky Secondly,ourevaluationmethods,thoughinsightful,arenot
(2021). exhaustive. ThebehavioraltestsusingZorroarevaluableforassessing responses to grammatical variations in sentences. child-directed language. In Proceedings of the 25th con-
However,itisimportanttonotethatZorrohasitslimitations ference on computational natural language learning (pp.
(Va´zquezMart´ınez,LeaHeuser,Yang,&Kodner,2023),and 624–646).
we still lack more systematic semantic evaluations. Lastly, Huebner,P.A.,&Willits,J.A.(2020).Ordermatters:Devel-
our models are exclusively trained on transcribed speech. opmentally plausible acquisition of lexical categories. In
Wang et al. (2023) and Warstadt et al. (2023) suggest that Cogsci.
integratingmultiplemodalitiesgivenrealisticexperienceisa Kim, N., & Smolensky, P. (2021, February). Testing for
significantchallengeinlanguagelearning,althoughtherehas grammaticalcategoryabstractioninneurallanguagemod-
beenrecentprogress(Vongetal.,2024). Weseemulti-modal els. In Proceedings of the society for computation in lin-
learningasapromisingmeansofenhancingmodeldataeffi- guistics2021(pp.467–470).Online:AssociationforCom-
ciency and realism by better capturing the learning problem putationalLinguistics.
facedbyayoungchild. Landauer,T.K.,Foltz,P.W.,&Laham,D. (1998). Anintro-
duction to latent semantic analysis. Discourse processes,
Acknowledgments
25(2-3),259–284.
WethankWaiKeenVong,CaraLeong,CindyLuoandSolim
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ...
LeGris for helpful feedback on earlier drafts. This work
Stoyanov, V. (2019). Roberta: A robustly optimized bert
wassupportedbyNSFAward1922658NRT-HDR:FUTURE
pretrainingapproach. arXivpreprintarXiv:1907.11692.
Foundations, Translation, and Responsibility for Data Sci-
MacWhinney, B. (2000). Thechildesproject: Toolsforan-
ence.
alyzingtalk(3rded.). LawrenceErlbaumAssociatesPub-
lishers.
References
Mikolov, T., Chen, K., Corrado, G.,&Dean, J. (2013). Ef-
Abend, O., Kwiatkowski, T., Smith, N. J., Goldwater, S., &
ficient estimation of word representations in vector space.
Steedman,M. (2017). Bootstrappinglanguageacquisition.
arXivpreprintarXiv:1301.3781.
Cognition,164,116–143.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Beaupoil-Hourdel, P. (2015). Multimodal acquisition and
Sutskever, I., et al. (2019). Language models are unsu-
expression of negation. analysis of a videotaped and lon-
pervisedmultitasklearners. OpenAIblog,1(8),9.
gitudinal corpus of a french and an english mother-child
Sellam, T., Yadlowsky, S., Tenney, I., Wei, J., Saphra, N.,
dyad(Unpublisheddoctoraldissertation). Ph.D.Disserta-
D’Amour, A. N., ... Pavlick, E. (2022). The multiberts:
tion,SorbonneNouvelleUniversity,Paris.
Bertreproductionsforrobustnessanalysis.InInternational
Brown,R. (1973). Afirstlanguage: Theearlystages. Har-
conferenceonlearningrepresentations(iclr).
vardUniversityPress.
Sullivan,J.,Mei,M.,Perfors,A.,Wojcik,E.,&Frank,M.C.
Chang, T. A., & Bergen, B. K. (2023). Language
(2021). Saycam: Alarge,longitudinalaudiovisualdataset
model behavior: A comprehensive survey. arXiv preprint
recordedfromtheinfant’sperspective. Openmind,5,20–
arXiv:2303.11504.
29.
Elman,J.L. (1989). Representationandstructureinconnec-
Taylor, W. L. (1953). “Cloze Procedure”: A New Tool for
tionist models. Center for Research in Language, Univer-
Measuring Readability. Journalism & Mass Communica-
sityofCalifornia,SanDiego.
tionQuarterly,30,415-433.
Elman, J. L. (1990). Finding structure in time. Cognitive
Van der Maaten, L., & Hinton, G. (2008). Visualizing data
science,14(2),179–211.
usingt-sne. Journalofmachinelearningresearch,9(11).
Elman, J. L. (1991). Distributed representations, simple
recurrent networks, and grammatical structure. Machine Va´zquezMart´ınez,H.,LeaHeuser,A.,Yang,C.,&Kodner,
learning,7,195–225. J.(2023,December).Evaluatingneurallanguagemodelsas
Frank,M.C.(2023a).Bridgingthedatagapbetweenchildren cognitive models of language acquisition. In Proceedings
andlargelanguagemodels. TrendsinCognitiveSciences, ofthe1stgenbenchworkshopon(benchmarking)general-
27(11),990-992. isationinnlp(pp.48–64).
Frank, M. C. (2023b). Large language models as models Vong,W.K.,Wang,W.,Orhan,A.E.,&Lake,B.M. (2024).
ofhumancognition. PsyArXiv. Retrievedfromhttps:// Grounded language acquisition through the eyes and ears
psyarxiv.com/wxt69 ofasinglechild. Science,383,504–511.
Frank, M. C., Braginsky, M., Yurovsky, D., & Marchman, Wang, W., Vong, W. K., Kim, N., & Lake, B. M. (2023).
V.A. (2016). Wordbank: anopenrepositoryfordevelop- Findingstructureinonechild’slinguisticexperience. Cog-
mental vocabulary data*. Journal of Child Language, 44, nitivescience,47(6),e13305.
677-694. Warstadt,A.,&Bowman,S.R. (2022). Whatartificialneu-
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term ralnetworkscantellusabouthumanlanguageacquisition.
memory. Neuralcomputation,9(8),1735–1780. AlgebraicStructuresinNaturalLanguage,17–60.
Huebner, P. A., Sulem, E., Cynthia, F., & Roth, D. (2021). Warstadt,A.,Mueller,A.,Choshen,L.,Wilcox,E.,Zhuang,
Babyberta: Learning more grammar with small-scale C., Ciro, J., ... others (2023). Findings of the babylmchallenge: Sample-efficient pretraining on developmen-
tallyplausiblecorpora. InProceedingsofthebabylmchal-
lengeatthe27thconferenceoncomputationalnaturallan-
guagelearning.
Waterfall, H. R., Sandbank, B., Onnis, L., & Edelman, S.
(2010). An empirical generative framework for computa-
tional modeling of language acquisition. Journal of child
language,37(3),671–703.
Yedetore, A., Linzen, T., Frank, R.,&McCoy, R.T. (2023,
July). How poor is the stimulus? evaluating hierarchical
generalizationinneuralnetworkstrainedonchild-directed
speech. In Proceedings of the 61st annual meeting of the
associationforcomputationallinguistics(volume1: Long
papers)(pp.9370–9393).