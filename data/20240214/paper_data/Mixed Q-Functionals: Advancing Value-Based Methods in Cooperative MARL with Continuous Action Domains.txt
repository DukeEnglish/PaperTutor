Mixed Q-Functionals: Advancing Value-Based Methods in
Cooperative MARL with Continuous Action Domains
YasinFindik∗ andS.RezaAhmadzadeh∗
Abstract spacesreflectingconcurrentdecisionsofallagentsposessev-
eralchallengessuchas: thecurseofdimensionality[Shoham
Tackling multi-agent learning problems efficiently etal.,2007],non-stationarity[Busoniuetal.,2008],andthe
isachallengingtaskincontinuousactiondomains. needforaglobalexplorationprocess[Matignonetal.,2012].
While value-based algorithms excel in sample ef-
Thesechallengesarefurtheramplifiedinscenarioswithcon-
ficiency when applied to discrete action domains, tinuous action spaces, where calculating each action’s value
theyareusuallyinefficientwhendealingwithcon-
becomesinfeasible.
tinuous actions. Policy-based algorithms, on the
CurrentMARLapproachesforcontinuousactiondomains
other hand, attempt to address this challenge by
primarilyutilizepolicy-based methods,suchasMulti-Agent
leveraging critic networks for guiding the learn-
Proximal Policy Optimization (MAPPO) [Yu et al., 2021],
ingprocessandstabilizingthegradientestimation.
andMulti-AgentDeepDeterministicPolicyGradient(MAD-
Thelimitationsintheestimationoftruereturnand
DPG) [Lowe et al., 2017]. These policy-based methods –
fallingintolocaloptimainthesemethodsresultin
mirroring their single-agent counterparts like Proximal Pol-
inefficient and often sub-optimal policies. In this
icyOptimization(PPO)[Schulmanetal.,2017],DeepDeter-
paper, we diverge from the trend of further en-
ministicPolicyGradient(DDPG)[Lillicrapetal.,2015]–are
hancing critic networks, and focus on improving
designedtooptimizethepolicyparametersformultipleinter-
theeffectivenessofvalue-based methodsinmulti-
actingagentsbasedonexpectedreturns,stabilizingthelearn-
agentcontinuousdomainsbyconcurrentlyevaluat-
ing process. Nevertheless, since the policy-based methods
ing numerous actions. We propose a novel multi-
directly optimize policies through function approximation,
agentvalue-basedalgorithm,MixedQ-Functionals
the inherent assumptions and approximations in these meth-
(MQF), inspired from the idea of Q-functionals,
odsoftenleadtosampleinefficiency,particularlywhencon-
thatenablesagentstotransformtheirstatesintoba-
trastedwithvalue-basedmethods[Papoudakisetal.,2020].
sis functions. Our algorithm fosters collaboration
Meanwhile, recent advances in the development of inno-
among agents by mixing their action-values. We
vativeneuralnetworkstructuresforsingle-agentRLdomains
evaluatetheefficacyofouralgorithminsixcooper-
haveenabledthevalue-basedconcepttobeapplicableincon-
ativemulti-agentscenarios. Ourempiricalfindings
tinuous action spaces. The resulting single-agent algorithm,
revealthatMQFoutperformsfourvariantsofDeep
known as Q-functionals [Lobel et al., 2023], has shown to
DeterministicPolicyGradientthroughrapidaction
surpass the performance of leading policy-based methods.
evaluationandincreasedsampleefficiency.
Essentially,Q-functionalsconvertastateintoafunctionthat
operates over the action space, facilitating the simultaneous
1 Introduction evaluationofQ-valuesforvariousactions. Thisparallelpro-
cessingcapabilityallowsefficientassessmentofmultipleac-
In multi-agent reinforcement learning (MARL), the primary
tionsperstate,leadingtoamoreeffectivesamplingtechnique
goal is to formulate and discover policies that empower
incontinuousenvironmentswithasingle-agent.
agents to maximize collective or individual rewards in a
In this paper, we introduce Mixed Q-Functionals, a novel
shared environment. MARL algorithms either incorporate
value-based approach tailored to address the sample in-
policy gradient [Sutton et al., 1999] methods for discover-
efficiency in policy-based methods for cooperative multi-
ing an optimal policy directly or learn action-value func-
agent tasks in continuous action spaces. Unlike frameworks
tions[WatkinsandDayan,1992]forestimatingtheexpected
like MADDPG, which rely on a policy network, Mixed Q-
rewardsperactionineachstate,asinsingle-agentreinforce-
Functionals (MQF) enable agents to employ Q-functionals,
mentlearning. Yetthepresenceofmulti-dimensionalaction
compute individual action-values efficiently, and mix those
∗PeARL Lab, Richard Miner School of Computer and Infor- to enhance cooperation among multiple agents. We evalu-
mation Sciences, University of Massachusetts Lowell, MA, USA atedourapproachusingsixexperimentsacrosstwodifferent
yasin findik@student.uml.edu, reza@cs.uml.edu environments,eachcharacterizedbycooperativemulti-agent
4202
beF
21
]AM.sc[
1v25770.2042:viXratasks. In summary, the key contributions of this study are: learn a value function together [Claus and Boutilier, 1998].
(a)WeintroduceMixedQ-Functionals(MQF)andtwobase- However, this method is computationally expensive and can
linemethods(IndependentQ-FunctionalsandCentralizedQ– become intractable since the observation and action space
Functionals), as novel value-based approaches for multi-a- growexponentiallywiththenumberofagents.
genttaskswithcontinuousactiondomains. (b)Ourresearch
Value Function Factorization. Value function factoriza-
is the first, to the best of our knowledge, to demonstrate the
tionmethodsareproposedasanefficientapproachforlearn-
advantagesofvalue-based overpolicy-based methodsinco-
ingajointaction-valuefunction,whosecomplexityincreases
operative MARL with continuous action spaces, revealing
exponentially with the number of agents. These methods
new possibilities for value-based strategies. (c) Our com-
adhere the centralized training with decentralized learning
parative analyses reveal that MQF consistently outperforms (CTDE)[Oliehoeketal.,2008]paradigm,enablingagentsto
DDPG-based methods in similar settings, achieving optimal
executetheiractionsindependentlywhileacentralizedmech-
solutions and exhibiting faster convergence in six different
anism integrates their strategies. Thus, they effectively ad-
scenarios.
dress the non-stationarity problem of IQL through central-
ized training, and overcome the scalability issue of central-
2 BackgroundandRelatedWork
izedlearningthroughdecentralizedexecution.
Reinforcement learning (RL) focuses on how an agent can VDN [Sunehag et al., 2017] and QMIX [Rashid et al.,
maximize its cumulative future reward, by learning from 2020b]aretwoexemplarymethodsforfactorizingvaluefunc-
its interactions within an environment. This concept is tions. Both methods maintain a separate action-value func-
commonly modeled, in MARL, as a multi-agent exten- tionQˆ iforeachagenti∈{1,...,N}. Theirdifferenceliesin
sion of Markov Decision Process (MDP), called Decentral- howtheycomputethecentralactionvalueQ tot. Specifically,
ized Partially Observable MDP (DEC-POMDP) [Oliehoek VDNsumsQ istoobtainQ tot,as
et al., 2016]. DEC-POMDP is characterized as a tuple
N
⟨S,A,O,R,T,N,γ⟩wheres∈S indicatesthetruestateof Qˆ :=(cid:88) Qˆ (s,a ), (2)
tot i i
theenvironment,A,OandRarethesetofagents’action,ob-
i=1
servationandreward,respectively.T(s,A,s′): S×A×S (cid:55)→
whileQMIXmixesthemusingacontinuousmonotonicfunc-
[1,0]isthedynamicsfunction,definingthetransitionproba-
bility, where s′ is the next state. N denotes the number of tionwhichisstate-dependent,asfollows:
agentsandγ ∈[0,1)isthediscountfactor. Qˆ :=f (Qˆ (s,a ),...,Qˆ (s,a )), (3)
tot s 1 1 n N
Q-Learning & Deep Q-Networks. Q-Learning, intro-
ducedin [WatkinsandDayan,1992],andDeepQ-Networks where ∂∂ Qf ˆs
i
≥ 0,∀i ∈ {1,...,N}. These value function fac-
(DQN),proposedby [Mnihetal.,2015],aretwofundamen- torization methods most commonly utilize DQN to approxi-
talapproachesinRL.Q-Learningutilizesanaction-valueQπ matetheactionvaluefunction. Theyminimizesloss(1)ina
forapolicyπ,definedas centralized manner, by substituting Qˆ with Qˆ , and r with
tot
r whichisthesumofagents’rewards.
Qπ(s,a)=E[G|st =s,at =a], team
Value-based methodsareeffectiveinderivingrobustpoli-
whereGdenotesthereturnandtisthetime-step. Thisfunc- ciesforagentsinenvironmentswithafinitenumberofactions
tioncanbeexpressedrecursivelyas (i.e. discreteactions). Yet,thesemethods,includingtheiren-
hanced versions [Findik et al., 2023], often struggle to gen-
Qπ(s,a)=E [R(s,a)+γE [Qπ(s′,a′)]],
s′ a′∼π erate viable policies in continuous environments, where the
withRdenotingtherewardfunction. DQNaimstolearnthe action space is represented by continuous vectors, resulting
action-valuefunctionQˆassociatedwiththeoptimalpolicyby inaninfinitearrayofpossibleactions[Limetal.,2018].
minimizingtheloss:
Policy Gradient (PG). Policy Gradient (PG) methods, as
L(w PN)=E s,a,r,s′[(r+γmax(Qˆ(s′,a′;w TN))−Qˆ(s,a;w PN))2], (1) describedin[Suttonetal.,1999],arepivotalinRLwithcon-
a′ tinuousactionspaces. Theydirectlyoptimizethepolicypa-
wherew andw aretheparametersofthepredictionand rameters(denotedasθ),aimingtomaximizetheexpectedre-
PN TN
targetnetworks,respectively. Thetargetnetworkparameters turn. Thecoreprincipleinvolvesadjustingθ inthedirection
w are periodically updated with w to stabilize learning. ofthepolicy’sgradient,expressedas:
TN PN
Toimprovestabilityofthemodelsfurther,DQNemploysan
∇ J(θ)=E [∇ logπ (a|s)Qπ(s,a)], (4)
experiencereplaybuffer,storingtuplesof⟨s,a,r,s′⟩. θ s∼pπ,a∼πθ θ θ
The direct application of Q-Learning to multi-agent sce- where pπ represents the state distribution under policy π.
narios might involve each agent i independently learning Diverse algorithms have emerged from PG, each character-
an optimal function Qˆ∗, known as Independent Q-Learning ized by unique approaches to estimating Qπ. The REIN-
i
(IQL) [Tan, 1993]. Yet, as each agent adjusts its pol- FORCEalgorithm,forinstance,employsasamplereturncal-
icy autonomously over time, the environment becomes non- culation G = (cid:80)∞ γk−1G as its foundational mech-
t k=1 t+k
stationary from the perspective of other agents, disrupting anism [Williams, 1992]. Meanwhile, temporal-difference
thestationarityconditionsnecessaryfortheQ-Learningalgo- methods are employed in actor-critic algorithms to learn an
rithmtoreliablyconverge. Anotherapproachisfullycentral- action-value function approximation Qˆπ(s,a), which serves
ized learning, where all agents share a single controller and asthecritic[KondaandTsitsiklis,1999].ThePGtheoremcanalsobeadaptedfordeterministicpoli- 3 ProposedMethod
cies(DPG) [Silveretal.,2014],denotedasµ :S (cid:55)→Awith
θ Intheprecedingsection,wediscussedthechallengesandad-
apolicyvectorofnparametersθ ∈ Rn. Thegradientofthe
vantages of policy-based and value-based methods in multi-
expectedreturncanbeformulatedas:
agentsettings.Policy-basedmethodsareeffectiveincontinu-
∇ J(µ )=E [∇ µ (s)∇ Qµ(s,a)| ]. (5) ousactionspaces,butoftenconvergetosuboptimalsolutions
θ θ s∼pµ θ θ a a=µθ(s)
orshowslowconvergenceduetosampleinefficiency. Toad-
DeepDeterministicPolicyGradient(DDPG)[Lillicrapetal., dresstheseproblems,weproposeanovelvalue-basedMARL
2015], avariationofDPG,usesdeepneuralnetworkstoap- algorithm, called Mixed Q-Functionals (MQF). MQF lever-
proximatethepolicyµandthecriticQˆµ. ages sample efficiency of Q-functionals [Lobel et al., 2023]
for continuous environments, and employs CTDE paradigm
Multi-Agent DDPG (MADDPG). MADDPG [Lowe et
to tackle issues of scalability and non-stationarity. Addi-
al., 2017] is an actor-critic policy-based method built upon
tionally, we introduceIndependentQ-Functionals (IQF)and
DDPG,anddesignedtooperatewithmultipleagents. Inthis
CentralizedQ-Functionals(CQF)todemonstratethesuperior
approach, agent policies parameterized by θ = [θ ,...,θ ]
1 n performanceofMQFinmulti-agentcontexts.
and denoted as π = [π ,...,π ]. The gradient of the ex-
1 N SinceourproposedMARLalgorithmsbuilduponthefoun-
pectedreturn,shownin(4),foreachagentisre-expressedas:
dation of QF, we offer a brief overview of it before delving
intothespecificsofIQF,CQF,andMQF.
∇ J(θ )=E [∇ logπ (a |o )Qˆπ(o,a)],
θi i x∼pµ,ai∼πi θi i i i i
where Qˆπ(o,a) is a centralized action-value function that 3.1 Q-functionals
i
takes state information o = [o ,...,o ] and the actions of Q-functionals[Lobeletal.,2023]relyonthemainideathatif
1 N
allagentsa=[a ,...,a ]asinputandoutputstheQ-value. thevaluesofnumerousactionscanbecalculatedquickly,then
1 N
Next,theideaisextendedtoworkwithdeterministicpolicies value-basedmethods, knownfortheirsampleefficiency, be-
µ=[µ ,...,µ ],and(5)isupdatedasfollows: comesuitableforcontinuousactionspaces. Thisfundamen-
θ1 θN
tal idea allows to address the shortcomings of policy-based
∇ θiJ(µ θi)=E o∼pµ,a∼µ[∇ θiµ θi(a i|o i)∇ aiQˆµ i(o,a)| ai=µθi(oi)]. methods effectively for single-agent setups. The concept of
Q-functionals modifies the representation of Q-functions to
LikeDQN,MADDPGalsousesreplaymemory,whichkeeps
enablethesimultaneousevaluationofmultipleaction-values
theexperiencesofallagents,andtargetnetworkstoincrease
foraspecificstate. TraditionaldeepQ-functions,denotedby
thestabilityofthepolicynetworks. Includingthesefeatures,
aneuralnetwork,mapstate-actionpairsdirectlytoR,asin
theloss,utilizedtoupdatethecentralizedaction-valuefunc-
tionQµ i ,becomesasfollows: Qˆ(s,a):(S×A)(cid:55)→R. (6)
L(θ)=E [(r +γQµ′ (o′,a′)| −Qµ(o,a))2], However, Q-functionalsredefinethismappingbyseparating
i o,a,r,o′∼D i i a′=µ′(o′) i
thestateandaction,wherethestatealoneismappedtoafunc-
where D is the replay memory that contains the tuples tionthatsubsequentlymapsactionstoR:
⟨o,a,r,o′⟩, and µ′ = [µ ,...,µ ] is target policies
θ′ θ′
whoseparametersareregularly1 updatedN withθ(fordetailex- QˆF(s,a):S (cid:55)→(A(cid:55)→R). (7)
planation, see [Lowe et al., 2017]). Although MADDPG-
based algorithms perform state-of-the-art by maximizing The standard architectures, as described by (6), require
agents’ return for higher rewards, in complex environments twice the computational effort to evaluate action-values for
(i.e. environmentswithlargestate,andmotionspace),there- two actions in the same state compared to just one. Yet,
sults of training tend to fall into local optimal solutions due Q-functionals transform each state into a set of parame-
toinefficientsamplingandtheirsmallscopeofexploration. ters defining a function over the action space. This means
each state is encoded as a function, constructed by learning
Recentstudieshavedevotedeffortstoenhancingthesam-
the coefficients of basis functions within the action space.
ple efficiency and overall performance of actor-critic based
These functions, representing states, enable quick evalua-
PG methods. Notably, advancements such as FACtored
Multi-Agent Centralized policy gradients (FACMAC) [Peng tion of various actions through matrix operations between
et al., 2021] and Value Function Search (VFS) [Marchesini the action representations and learned coefficients. Hence,
andAmato,2023]haveemergedassignificantcontributions. Q-functionals efficiently handle environments with continu-
ousactionspacesforsingle-agentsettingswithoutsacrificing
FACMAC, for instance, incorporates a factored critic and a
sampleefficiency.
centralizedgradientestimatortofacilitatelearningincontin-
uouscooperativetasks. VFSintroducesanovelapproachby
3.2 IndependentQ-Functionals(IQF)
periodicallygeneratingasuiteofperturbedcriticsthrougha
two-scale perturbation noise, specifically designed to refine We introduce a straightforward application of Q-functionals
value predictions for a given Deep PG agent, which utilizes in multi-agent context, wherein each agent i independently
actor-criticnetworks. However,ourresearchdivergesbyfo- aims to learn an optimal functional, QˆF∗ in (7), similar to
i
cusing on applying the value-based concept in continuous the concept underlying IQL [Tan, 1993]. However, this ap-
MARL tasks, addressing the inherent inefficiencies in sam- proach also cannot handle the non-stationarity that emerges
plingcommonlyassociatedwithpolicy-basedmethods. duetotheevolvingpoliciesofthelearningagents.Therefore,Model Training (b)
Episode Generation (a)
Learners' Prediction Blocks Learners' Target Blocks
E Agent n – QFn Agent n – QFT n
N V DiU stn ri if bo ur tm ion Agent i – QFi Q PrM ei dx ie cr ti on TM ai rx ge er t Q' C No ee tfA wficg oiee rknntt i – QFT i DiU stn ri if bo ur tm ion
Text MOR
N
EI r ,
a
s '
Explo Era xt
pD
io
loiU ns ratn r tDi i iof bo niU Au sr ctm tni tr io i oif bn no u sr tm ion FA uBg nae csn
ti is
ot n1 C– CN
o
oQ eee ffF tf icwf1 i ic
e
o nie r tsk sn 1t prob pa ac gk a- tioQ ntot Q'tot s'1C No Cee otC f ewfN i fo c fo ice Cie e r io eA tf knwf e nig fc t tfo sicei C ie r en Fok nn et t uBsf t f1 ni ac ci e sF– tn
i ius
t osQ
n
ncF F QtDT iu
oA
11 iU n ncs tc itn ortD nii i aof b soiU 1An s ur ctn tm tr iioi i of b no nsAu r ctm tiioonns Text
N T Exploration Q1 a1 Replay r TD-Error Q'1Q a1 '1 a1
< s1, a1, r1, s'1 > Memory
Figure1:OverviewoftheMQFArchitecture:(a)EpisodeGeneration,(b)ModelTraining.Thebluearrowsdepictthetargetnetworkupdates
formixerandlearners’predictionblocks,whiletheredarrowsindicatethedirectionofthebackpropagation.
it lacks convergence guarantees, even with unlimited explo- spaces.Thisfunctionality,combinedwiththemixingofcom-
ration.Inourstudy,weuseIndependentQ-Functionals(IQF) puted action-values, promotes cooperative behavior among
as a baseline, evaluating the efficacy of Q-functionals varia- agents in such settings. The effectiveness of MQF is under-
tionagainstDDPG-basedmethods. scoredintheresultssection,whereitisbenchmarkedagainst
DDPG variants, specifically Independent DDPG (IDDPG),
3.3 CentralizedQ-Functionals(CQF)
CentralizedDDPG(CDDPG),andMADDPG.
In our study, we also present another variant of the Q-
As illustrated in Fig. 1, our proposed framework entails
functionals for multi-agent settings through the introduction
threemaincomponents: generatingepisodes, traininglearn-
of a single Centralized Q-Functionals (CQF), denoted as
ers’predictionblocks,andupdatinglearners’targetblocks.
QˆF , which integrates the state functions through learned
cent
coefficients over the joint action space of all agents. This
Generating Episodes. The generation of an episode in-
approachcanbeexpressedbymodifying(7)asfollows:
volves agents engaging with the environment through ac-
QˆF (s,a):S (cid:55)→(A(cid:55)→R), (8) tions derived from their prediction blocks, as illustrated in
cent
Fig. 1(a). Concisely, the process starts with the agent’s co-
wherea = [a ,...,a ]ands = [s ,...,s ]representac-
1 N 1 N efficient neural network predicting the basis function coeffi-
tions and states of all agents, respectively. Nevertheless, as
cients for the current state. This is followed by uniformly
the number of agents grows, the size of the state and action
sampling and evaluating k actions, which involves calculat-
spacesincreasesexponentially,leadingtoariseinthemodel’s
ingthematrixmultiplicationofthepredictedcoefficientsand
time complexity. Consequently, maintaining sampling effi-
the sampled actions’ representations. The action associated
ciencybecomesmorechallenging.
withthehighestQ-valueis,then,selected.Toenhancetheex-
3.4 MixedQ-Functionals(MQF) ploration and sample efficiency, in our experiments, we em-
ploy Gaussian exploration, by adding Gaussian-distributed
Inresponsetothenon-stationaritychallengesinherentinIQF
noisetoagents’calculatedactions,orε-greedy,whereagents
andthescalabilityissuesfacedbyCQF,weintroduceMixed
may take random actions with ε probability. Notably, these
Q-Functionals(MQF).ThisnovelmethodmixesQ-valuesde-
exploration techniques are deactivated during agent evalua-
rivedfromeachagent’sindividualQ-functionals. Itutilizesa
tion. Uponreceivingavectorofactions,a,fromtheagents’
centralized loss function to train the agents’ coefficient net-
prediction blocks, the environment provides feedback in the
works,leveragingtheconceptofvaluefunctionfactorization.
formofr ands,representingtheagents’rewardsandstates,
The mixing function in MQF is versatile, ranging from ad-
respectively. Thesevectorsaresubsequentlyallocatedtothe
ditive forms as in (2) to monotonic functions like in (3), or
corresponding agents, and stored in their replay memories,
morecomplexmethods,similartothosedescribedin[Sonet
D ,astuples⟨s ,a ,r ,s′⟩.
al.,2019][Rashidetal.,2020a]. Thecomplexityofthetasks i i i i i
being addressed plays a crucial role in determining the spe-
Training Learners’ Prediction Blocks. The training pro-
cificformofthemixingfunctiontobeemployed.
cess for the MQF begins with the selection of a batch of
Notably,MQFdifferentiatesitselffromothervalue-based
transition tuples ⟨s ,a ,r ,s′⟩, each of size b, from the re-
MARL methods, described in current literature on mixing i i i i
play memory D of each agent. These tuples are utilized to
functions, through its adept handling of continuous action i
calculatetheagents’individualQ values. Subsequently,we
spaces. It also capitalizes on the sample efficiency charac- i
employtwonetworks: oneformixingtheagents’currentQ
teristicofvalue-basedconcept,enhancingagentperformance
relativetopolicy-based methods. MQF1 frameworkenables values to derive QF tot, and another for mixing the next-state
Q′ valuestoobtainQ′F. Fig.1illustratesthesenetworksas
agents to represent their states with basis functions, facili- tot
themixerpredictionandthemixertarget, respectively. This
tatingitsapplicationinenvironmentswithcontinuousaction
procedureisfollowedbythecalculationofthetemporaldif-
1Thepseudo-codeisdetailedinSupplementaryMaterialA.1. ferenceerrorbetweenthepredictionandtargetvalues,whichcanbeencapsulatedbythefollowingformulation: Predator-Prey(PP): Inthisscenario, weadaptthesimple
tagenvironment,initiallyintroducedby[Loweetal.,2017],
b∼D
e = (cid:88) [(QˆF (s,a;w )−y(r,s′))2], (9) to a variation of the classic predator-prey game. This setup
TD tot PN features three slower agents (predators) aiming to catch a
s,a,r,s′ faster prey in a 2D space with two large landmarks serving
wherey(r,s′)isdefinedas: as obstacles. The prey’s movements, based on a hard-coded
heuristic,aimtomaximizedistancefromthenearestpredator
N ateachstep.Agentshaveaccesstosensoryinformation:their
y(r,s′)=(cid:88) r +γmax(QˆF (s′,a′;w )| ). (10) own velocity and position, relative position of landmarks,
i tot TN a′∼U
a′ otheragents,theprey,andtheprey’svelocity. Twovariants,
i=1
differentiatedbytheirrewardstructures,areimplemented:
The bold symbols, in (9) and (10), are vectors denoting the
• Standard Predator-Prey (S-PP): Success is achieved
set of corresponding values for all agents and U signifies a
uniform distribution. The function QˆF , resembling forms when at least two predators collide with the prey, with
tot each predator involved gaining 10 points. A distance-
such as those in (2) and (3), serves as a mixing mechanism
based penalty (proportional to the Euclidean distance
fortheagents’Q-values. Theoptimizationoftheagents’Q-
between predator and prey) is applied otherwise. This
functionals aims to minimize the temporal difference error,
variant, however, leads predators to unintentionally co-
e in(9), thusfacilitatingthecoordinationofagentactions
TD
operate to collide with the prey due to distance-based
to maximize collective rewards. This training is conducted
centrally using QˆF , while the determination of individual penalization.
tot
agent actions is guided by their QˆF, enabling decentralized • Increased Cooperation Predator-Prey (IC-PP): This
i
executioninlinewiththeCTDEparadigm. Thecomprehen- variantintroducesafieldofviewmechanicforpredators,
siveschematicofthetrainingprocessisdepictedinFig.1(b). promoting more strategic and intentional cooperation.
A successful capture necessitates at least one predator
Updating Learners’ Target Blocks. Our framework, de-
physically colliding with the prey. Predators that have
signed for multi-agent environments with continuous action
thepreywithintheirfieldofviewatthemomentofcap-
spaces, diverges from standard value-based methods in up-
turegain10points,encouragingthemtotimethecapture
dating the target networks. It employs a soft update mecha-
tomaximizeteamreward.
nism at each time-step, proven more effective in continuous
action domains instead of periodic updates [Lillicrap et al., Multi-WalkerEnvironment(MWE)
2015]. This method is characterized by incremental adjust-
The MWE, a multi-agent continuous control task empha-
ments, formalized as w TN = τw PN +(1−τ)w TN where τ sizing locomotion, was initially introduced in [Gupta et al.,
is a small factor (τ ≪ 1), ensuring improved stability and 2017]. It consists of walkers, each controlling two joints in
effectivenessinsuchenvironments.
aleg,taskedwithtransportingobjectsatopthem. Originally,
each walker is considered a single agent, focusing on coor-
4 Experiments dinatingmovementstobalanceandtransportapackage. Our
study, however, introduces a novel modification: splitting a
4.1 Environments
walker into two agents, each controlling one leg, to include
WeevaluateourmethodintwodistinctMARLenvironments, additional cooperative behaviors. This change requires each
Multi-Agent Particle Environment [Mordatch and Abbeel, leg of a walker to not only coordinate internally but also to
2017] and Multi-Walker Environment [Terry et al., 2021], cooperatewithagentscontrollingthelegsofothers.
acrossvariousscenarios2. Each agent (i.e., a leg) in this modified environment con-
trolstwotorquevalues,rangingfrom[−1,1]. Whiletheorig-
Multi-AgentParticleEnvironment(MPE)
inal observation space per agent includes a 32-dimensional
UtilizingOpenAI’sMPEpackage[Loweetal.,2017],wede-
vector with information about nearby walkers and LiDAR
vise two novel scenarios: landmark capturing and predator-
sensordata,ourversionmodifiesitbyincludingglobal(hull’s
prey,eachwithtwovariants. Theactionspace,inthesesce-
angle, angular velocity, package angle, velocities in x and y
narios, is two-dimensional governing both vertical and hori-
directions) and local (ground contact, hip and knee angles,
zontalmovements,withinarangeof[−1,1].
andvelocities)informationaboutthewalkerandleg,respec-
Landmark Capturing: This scenario involves N agents tively. Agents lose 100 points for falls but gain 5 points for
andN landmarks,withagentscooperatingtocoverallland- successful forward movement. The environment terminates
marks. Anagent’srewardise−d2/c,wheredistheEuclidean an episode under any of the following conditions: a walker
distancetothenearestlandmarkwithinathresholdd ,orthe or package falls, reaches the edge of the terrain, or a prede-
th
sumofdistancestoalllandmarksotherwise,leadingagentsto fined time-step is met. We utilize two configurations: one
strategizewhichlandmarkistobecapturedbywhom. Each walkerpartitionedintotwoagents(2A1W)andtwowalkers
agent’sobservationincludesitsvelocityandlandmarks’rela- partitionedintofouragents(4A2W).
tivelocation. Weexploretwoconfigurations: 2agentswith2
4.2 ModelsandHyperparameters
landmarks(2A2L)and5agentswith5landmarks(5A5L).
OuralgorithmapproximatestheQ-functionalsforeachagent
2SupplementaryMaterialA.2includesimagesofthescenarios. usingaMulti-LayerPerceptron(MLP)withavariablenum-ber of layers. These networks take a state as input and out-
putanumberofcoefficientsdeterminedbythecombinatorial
formula
(cid:0)r+d(cid:1)
, where d is the action size and r represents
d
theorderofbasisfunctionusedforstaterepresentation. We
usedpolynomialbasisfunctionsofrankr =2forCQF,IQF,
andMQFinourexperiments. ForMADDPG,weutilizedits
original implementation3. We determined optimal hyperpa-
rameters4throughtestingacrosstwodifferentseeds.
Our results are presented through figures – Fig. 2, Fig. 3
and Fig. 4 – that display mean test rewards of agents, in-
(a) (b)
dicated by solid lines, with shaded regions representing the
95% confidence interval calculated over multiple runs. Test Figure2:LandmarkCapturingResults:(a)2A2L,(b)5A5L.
rewards are obtained by evaluating agents’ rewards using
a greedy strategy, with assessments conducted every 10000
performancesinthe2A2Lscenariocomparedto5A5L,they,
trainingstepsover10testepisodes. Comprehensivenumeri-
nonetheless,underperformrelativetootherapproaches.
cal results are summarized in Table 1 and Table 2, based on
theaverageof10000testepisodesaftertraining. Theimple- On a different note, the two MADDPG variants and in-
mentationandexperimentalsetupareavailableonGitHub5. dependent learning models, IQF and IDDPG, demonstrate
slowerconvergencetoanysolutionthanMQFinboth2A2L
and5A5Lsetups,asdemonstratedinFig.2(a)andFig.2(b),
4.3 Results
respectively. Asolutionisconsideredoptimalifalltheland-
We evaluate the performance of our method on six distinct marks are captured by the agents, and otherwise subopti-
tasks, as explained in Section 4.1. In each of these tasks, mal. While all algorithms exhibit comparable performances
weassesstheperformanceofourproposedalgorithm, MQF in the 2A2L (i.e., converging to an optimal solution); in the
(Section 3.4), along with its simpler extensions – CQF and 5A5L setup, a noticeable difference is observed. DDPG-
IQF (Sections 3.3 ands 3.2, respectively) – against estab- based algorithms tend to converge to suboptimal solutions,
lished benchmarks: Centralized DDPG (CDDPG), Indepen- likelyduetosampleinefficiency,whereasvalue-basedmeth-
dentDDPG(IDDPG),andMulti-AgentDDPG(MADDPG). ods, including IQF, show a tendency to converge to optimal
ItshouldbenotedthatweemploytwovariantsoftheMAD- solutions. Notably,IQFexhibitschallengesinaddressingthe
DPG: MADDPG-Team, trained with team-based rewards, non-stationarity issues of independent learning. This is par-
andMADDPG-Ind,usingindividualagentrewards. ticularlyevidentfromtheperformancefluctuationsobserved
In our experimental analysis, we begin with the MPE, around the 2000th episode mark, although its performance
specifically chosen as the MADDPG algorithm was origi- doesshowimprovementwithextendedtraining. Incontrast,
nallyevaluatedwithit, providingadirectbasisforcompari- MQF which utilizes CTDE paradigm to tackle these prob-
son.ThefirstexperimentswithinMPEfocusonthelandmark lems,rapidlyconvergestoanoptimalsolution. Furthermore,
capturingscenarios(2A2Land5A5L).Theobservationsde- wedefinedasuccessmetricfortheseexperiments:anepisode
rived from Fig. 2 reveal that regardless of the agent count, issuccessfulifalllandmarksarecapturedbyitsend. Asde-
thecentralizedmethods,CQFandCDDPG,exhibitedsubpar pictedinTable1,MQFoutperformsinbothrewardandsuc-
performances. Thiscanbeattributedtotwoprimaryfactors: cessrate,provingitseffectivenessinthesescenarios.
(i)thecurseofdimensionalityresultingfromtheconcatena- In the second phase of MPE experiments, we concen-
tionofagentobservations,and(ii)thelimitednatureofagent trate on predator-prey scenarios, specifically S-PP and IC-
observations,aspreviouslydiscussedinSection4.1. Specifi- PP. As demonstrated in Fig. 3(a) and detailed in Table 1,
cally,agentsonlyperceivetheirvelocityandtherelativeloca- the S-PP scenario showcases the superiority of the IQF al-
tionoflandmarks,withoutawarenessoftheirownpositions, gorithmoverothers,includingMQF,withanaveragereward
leading to an ineffective distribution of landmarks among of47.85±0.47.Interestingly,despitethefactthattheagents’
agents. Notably, while CQF and CDDPG exhibit improved collective reward per capture increases when three agents
concurrently catch the prey, none of the algorithms consis-
3https://github.com/openai/maddpg tently converge to this behavior. This is because securing
4HyperparametersaredetailedinSupplementaryMaterialA.3. multiple captures within an episode yields a higher cumula-
5Linktobeprovideduponacceptance. tive reward in the long term. To elucidate, a single capture
Methods 2-Agent&2-Landmarks 5-Agents&5-Landmarks StandardPredator-Prey IncreasedCooperationPredator-Prey
TeamReward SuccessRate TeamReward SuccessRate TeamReward Total#captures 3-Agent#captures TeamReward Total#captures 3-Agent#captures
CQF 32.64±4.26 0.09±0.01 6.22±0.54 0.00±0.00 27.44±0.66 1.82±0.03 0.00±0.00 65.27±1.60 4.80±0.11 0.37±0.03
CDDPG 25.86±4.98 0.00±0.00 5.89±0.32 0.00±0.00 20.13±1.08 1.51±0.05 0.00±0.00 72.77±5.35 4.30±0.11 1.04±0.21
IQF 74.98±2.13 0.69±0.07 207.67±0.49 0.94±0.02 47.85±0.47 2.73±0.02 0.10±0.00 97.61±1.13 4.91±0.06 2.03±0.12
IDDPG 77.38±0.29 0.83±0.01 169.55±5.93 0.13±0.11 36.69±0.48 2.22±0.03 0.07±0.00 86.43±0.30 4.19±0.02 1.91±0.01
MQF 80.46±0.57 0.88±0.03 210.69±0.32 0.98±0.01 33.97±0.58 2.02±0.03 0.07±0.00 104.02±0.96 4.65±0.05 2.71±0.04
MADDPG-Ind 75.05±3.08 0.82±0.08 176.28±5.09 0.29±0.16 34.11±0.11 2.05±0.00 0.06±0.00 81.92±0.86 3.78±0.05 2.11±0.03
MADDPG-Team 73.90±3.17 0.82±0.08 166.25±6.64 0.02±0.01 38.13±0.28 2.25±0.01 0.07±0.00 78.87±0.79 3.75±0.70 1.93±0.01
Table1:Averagemetricswith95%confidenceintervalsformultiplerunsonMPEwithdifferentscenariosupontrainingcompletion.2-Agents(1-Walker) 4-Agents(2-Walkers)
TeamReward BalanceRate TeamReward BalanceRate
CQF 50.76±13.09 0.72±0.07 -59.51±8.15 0.46±0.03
CDDPG -0.26±20.10 0.60±0.16 43.23±14.22 1.00±0.00
IQF 78.18±2.43 0.85±0.02 30.66±22.62 0.73±0.09
IDDPG -7.27±12.41 0.39±0.08 30.26±6.83 0.79±0.03
MQF 94.61±1.89 0.95±0.02 91.31±5.80 0.96±0.02
MADDPG-Ind -54.57±18.05 0.195±0.09 -69.84±13.37 0.13±0.09
MADDPG-Team -20.78±18.71 0.339±0.12 10.03±13.35 0.80±0.08
Table2:Averagemetricswith95%confidenceintervalsformultiple
(a) (b)
runsonMWEupontrainingcompletion.
Figure3:Predator-PreyResults:(a)S-PP,(b)IC-PP.
trained models exhibit multiple behavioral patterns, varying
involving three agents yields a team reward of 30. How- intheirdegreeofoptimality: (a)learningmerelytoholdthe
ever, this strategy is less time-efficient compared to captur- package without falling, (b) focusing on throwing the pack-
ing with two agents, which results in a reward of 20 but al- age as far as possible, (c) mastering balance and forward
lows for more frequent captures. Evidently, as the number movementwithsmallsteps,and(d)successfullycarryingthe
ofthree-agentcapturesrises,thetotalnumberofcapturesdi- packagetotheterrain’sendbeforetheepisodeconcludes.Be-
minishes, leading to a decrease in the overall episode team haviors(a)to(c)areindicativeofthealgorithmsbeingstuck
reward. Theadditional10rewarddoesnotsufficientlyincen- in suboptimal policies. Analysis of early-trained models re-
tivizecollaboration,suggestingthatthisscenarioismoreef- vealsaninitialtendencytowardsbehavior(b), whichgradu-
fectively resolved by agents operating independently rather allytransitionstobehaviors(a)and(c)astrainingprogressed.
than increasing complexity through mixing agents action- While independent learning algorithms, IQF and IDDPG,
values. This experiment underscores that independent tasks initiallyseemcomparabletoMQFinFig.4,theirlimitations
withinamulti-agentsettingcanbeeffectivelyaddressedus- become apparent upon deeper analysis. These models ex-
ingQ-functionals,asevidencedbyitsperformancerelativeto hibit a notable drop in average reward and stability with an
DDPG-basedapproaches. increasing number of test episodes. In particular, although
However, in the IC-PP scenario, we demonstrate MQF’s these algorithms, unlike MQF, rarely achieve the optimal
effectiveness in promoting cooperative behaviors among behavior (d), their performance lack consistency, as shown
agents. This scenario rewards agents not only for capturing by the average team rewards and balance rates (indicator of
preybutalsoforobservingitduringcapture,encouragingco- avoiding falls or package drops) presented in Table 2. CD-
operative capture strategies. As shown in Table 1, although DPG, however, despite recording a high balance rate in the
IQF achieves a higher total number of prey captures, MQF 4A2W scenario, often fails to transport the package within
outperformsbothIQFandDDPGextensionsintermsofav- theepisode’smaximumsteps. Incontrast,MQFconsistently
erageteamreward. ThisisduetoMQF’sproficiencyinfacil- delivers higher returns in both scenarios. It is worth noting
itatingthree-agentcaptures,leadingtoenhancedcooperative thatextendingtrainingtimeorincreasingthemaximumstep
behaviorsandhighercollectiverewards.Figure3(b)indicates count per episode could potentially allow other methods to
MADDPG-Team and MADDPG-Ind converge more rapidly reach optimal solutions, a hypothesis supported by MQF’s
buttolocaloptima,makingthemlesseffectivethanIQFand enhancedsampleefficiencyandoveralleffectiveness.
MQFinmaximizingteamrewards,ashighlightedinTable1.
Finally,weevaluatethealgorithms’performanceinMWE 5 ConclusionandFutureWork
variations,specifically2A1Wand4A2Wconfigurations.The
Weintroducedanovelalgorithmthatleveragesthestrengths
of value-based strategies to address sample inefficiency in
policy-based methods, particularly in the context of cooper-
ativemulti-agenttaskswithincontinuousactionspaces. Our
approachenablesagentstocharacterizeeachstateusingaba-
sis function, and optimize its coefficients. To further facili-
tate cooperation among agents, we employ a mixer network
that combines their action-values effectively. Our empirical
findings are twofold: (i) our method consistently achieves
optimal solutions, surpassing policy-based methods which
frequently converge to suboptimal cases; (ii) in instances
wherepolicy-basedmethodsalsoattainoptimalsolutions,our
methoddemonstratesafasterrateofconvergence.
Lookingahead,ourprimaryobjectiveistoenhancethesta-
(a) (b) bility of the agents’ learning model. Although our method
already exhibits greater stability compared to existing ap-
Figure4:Multi-WalkerResults:(a)2A1W,(b)4A2W. proaches,weobservedoccasionalfluctuationsinsometrials,suggestingpotentialinstability. Toaddressthesechallenges, value function search. arXiv preprint arXiv:2302.10145,
future work can consider incorporating additional stability 2023.
measures,suchastargetpolicysmoothingin[Fujimotoetal.,
[Matignonetal.,2012] LaetitiaMatignon,GuillaumeJLau-
2018], and adapting more sophisticated mixer networks, es-
rent,andNadineLeFort-Piat. Independentreinforcement
peciallyasthecomplexityoftheenvironmentsescalates.
learnersincooperativemarkovgames: asurveyregarding
coordination problems. The Knowledge Engineering Re-
References
view,27(1):1–31,2012.
[Busoniuetal.,2008] LucianBusoniu,RobertBabuska,and
[Mnihetal.,2015] Volodymyr Mnih, Koray Kavukcuoglu,
BartDeSchutter. Acomprehensivesurveyofmultiagent
DavidSilver,AndreiARusu,JoelVeness,MarcGBelle-
reinforcement learning. IEEE Transactions on Systems,
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
Man,andCybernetics,PartC(ApplicationsandReviews),
land,GeorgOstrovski,etal. Human-levelcontrolthrough
38(2):156–172,2008.
deepreinforcementlearning. nature,518(7540):529–533,
[ClausandBoutilier,1998] Caroline Claus and Craig 2015.
Boutilier. The dynamics of reinforcement learning in
[MordatchandAbbeel,2017] Igor Mordatch and Pieter
cooperative multiagent systems. AAAI/IAAI, 1998(746-
Abbeel. Emergence of grounded compositional lan-
752):2,1998.
guage in multi-agent populations. arXiv preprint
[Findiketal.,2023] Yasin Findik, Paul Robinette, Kshitij arXiv:1703.04908,2017.
Jerath,andSRezaAhmadzadeh. Impactofrelationalnet-
[Oliehoeketal.,2008] Frans A Oliehoek, Matthijs TJ
worksinmulti-agentlearning:Avalue-basedfactorization
Spaan, and Nikos Vlassis. Optimal and approximate q-
view. In 2023 62nd IEEE Conference on Decision and
valuefunctionsfordecentralizedpomdps. JournalofAr-
Control(CDC),pages4447–4454.IEEE,2023.
tificialIntelligenceResearch,32:289–353,2008.
[Fujimotoetal.,2018] Scott Fujimoto, Herke Hoof, and
[Oliehoeketal.,2016] Frans A Oliehoek, Christopher Am-
DavidMeger. Addressingfunctionapproximationerrorin
actor-critic methods. In International conference on ma- ato, et al. A concise introduction to decentralized
chinelearning,pages1587–1596.PMLR,2018. POMDPs,volume1. Springer,2016.
[Guptaetal.,2017] Jayesh K Gupta, Maxim Egorov, and [Papoudakisetal.,2020] Georgios Papoudakis, Filippos
MykelKochenderfer. Cooperativemulti-agentcontrolus- Christianos, Lukas Scha¨fer, and Stefano V Albrecht.
ing deep reinforcement learning. In Autonomous Agents Benchmarking multi-agent deep reinforcement learn-
and Multiagent Systems: AAMAS 2017 Workshops, Best ing algorithms in cooperative tasks. arXiv preprint
Papers, Sa˜o Paulo, Brazil, May 8-12, 2017, Revised Se- arXiv:2006.07869,2020.
lectedPapers16,pages66–83.Springer,2017. [Pengetal.,2021] Bei Peng, Tabish Rashid, Christian
[KondaandTsitsiklis,1999] Vijay Konda and John Tsitsik- Schroeder de Witt, Pierre-Alexandre Kamienny, Philip
lis. Actor-criticalgorithms. Advancesinneuralinforma- Torr, Wendelin Bo¨hmer, and Shimon Whiteson. Fac-
tionprocessingsystems,12,1999. mac: Factored multi-agent centralised policy gradients.
Advances in Neural Information Processing Systems,
[Lillicrapetal.,2015] TimothyPLillicrap,JonathanJHunt,
34:12208–12221,2021.
AlexanderPritzel,NicolasHeess,TomErez,YuvalTassa,
David Silver, and Daan Wierstra. Continuous con- [Rashidetal.,2020a] TabishRashid,GregoryFarquhar,Bei
trol with deep reinforcement learning. arXiv preprint Peng, and Shimon Whiteson. Weighted qmix: Expand-
arXiv:1509.02971,2015. ingmonotonicvaluefunctionfactorisationfordeepmulti-
[Limetal.,2018] Sungsu Lim, Ajin Joseph, Lei Le, agent reinforcement learning. Advances in neural infor-
mationprocessingsystems,33:10199–10210,2020.
YangchenPan,andMarthaWhite. Actor-expert:Aframe-
work for using q-learning in continuous action spaces. [Rashidetal.,2020b] Tabish Rashid, Mikayel Samvelyan,
arXivpreprintarXiv:1810.09103,9,2018. ChristianSchroederDeWitt,GregoryFarquhar,JakobFo-
[Lobeletal.,2023] Samuel Lobel, Sreehari Rammohan, erster, and Shimon Whiteson. Monotonic value function
Bowen He, Shangqun Yu, and George Konidaris. Q- factorisationfordeepmulti-agentreinforcementlearning.
functionals for value-based continuous control. In Pro- The Journal of Machine Learning Research, 21(1):7234–
ceedingsoftheAAAIConferenceonArtificialIntelligence, 7284,2020.
volume37,pages8932–8939,2023. [Schulmanetal.,2017] John Schulman, Filip Wolski, Pra-
[Loweetal.,2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox-
Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi- imal policy optimization algorithms. arXiv preprint
agentactor-criticformixedcooperative-competitiveenvi- arXiv:1707.06347,2017.
ronments. Advancesinneuralinformationprocessingsys-
[Shohametal.,2007] Yoav Shoham, Rob Powers, and
tems,30,2017.
Trond Grenager. If multi-agent learning is the answer,
[MarchesiniandAmato,2023] Enrico Marchesini and whatisthequestion? Artificialintelligence, 171(7):365–
ChristopherAmato. Improvingdeeppolicygradientswith 377,2007.[Silveretal.,2014] DavidSilver,GuyLever,NicolasHeess,
Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministicpolicygradientalgorithms. InInternational
conference on machine learning, pages 387–395. Pmlr,
2014.
[Sonetal.,2019] Kyunghwan Son, Daewoo Kim, Wan Ju
Kang,DavidEarlHostallero,andYungYi. Qtran: Learn-
ingtofactorizewithtransformationforcooperativemulti-
agentreinforcementlearning. InInternationalconference
onmachinelearning,pages5887–5896.PMLR,2019.
[Sunehagetal.,2017] PeterSunehag,GuyLever,Audrunas
Gruslys, WojciechMarianCzarnecki, ViniciusZambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z
Leibo, Karl Tuyls, et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296,2017.
[Suttonetal.,1999] Richard S Sutton, David McAllester,
Satinder Singh, and Yishay Mansour. Policy gradient
methodsforreinforcementlearningwithfunctionapprox-
imation. Advances in neural information processing sys-
tems,12,1999.
[Tan,1993] Ming Tan. Multi-agent reinforcement learning:
Independentvs.cooperativeagents. InProceedingsofthe
tenthinternationalconferenceonmachinelearning,pages
330–337,1993.
[Terryetal.,2021] J Terry, Benjamin Black, Nathaniel
Grammel,MarioJayakumar,AnanthHari,RyanSullivan,
LuisSSantos,ClemensDieffendahl,CarolineHorsch,Ro-
drigo Perez-Vicente, et al. Pettingzoo: Gym for multi-
agent reinforcement learning. Advances in Neural Infor-
mationProcessingSystems,34:15032–15043,2021.
[WatkinsandDayan,1992] Christopher JCH Watkins and
Peter Dayan. Q-learning. Machine learning, 8:279–292,
1992.
[Williams,1992] Ronald J Williams. Simple statistical
gradient-followingalgorithmsforconnectionistreinforce-
mentlearning. Machinelearning,8:229–256,1992.
[Yuetal.,2021] Chao Yu, Akash Velu, Eugene Vinitsky,
Yu Wang, Alexandre Bayen, and Yi Wu. The surpris-
ingeffectivenessofppoincooperative,multi-agentgames.
arXivpreprintarXiv:2103.01955,2021.A SupplementaryMaterial A.2 Environments
A.1 MQFAlgorithm
Algorithm1:MixedQ-Functionals
input :predictionnetwork,QˆF ;targetnetwork,QˆF ;relational
prediction target
graph,G;batchsize,b;updatefrequency,stepupdate;actionrange,
[amin,amax];actionrepresentationcalculator,Φ;orderofthebasis
function,o;
1 foreachepisodedo
2 Initializes ▷s∈RN
3 foreachstepofepisodedo
4 arand←U([amin,amax]) ▷randomactionselection
5 Cs←QˆF prediction(s) ▷statefunction’scoefficients
(a) (b)
6 Vs←Φ(o,arand) ▷actionrepresentations
7 Q←CsVs ▷calculateactionvaluesusingmatrixmultiplication
8 abest←argmax(Q) ▷bestactionforeachagent
9 a←abest+ϵ ▷ϵ∼N(0,0.1)
10 Takeactions,a,observer,s′
11 Stores,a,r,s′inmemory
12 s←s′
13 ifmod(step,stepupdate)is0then ▷networkupdateprocess
14 S,A,R,S′←samplechunk,sizeofb,frommemory
15 Cs←QˆF prediction(S)
16 Vs←Φ(o,A)
17 Qprediction←CsVs (c) (d)
18 Qp ter ae mdiction←mix(Qprediction) ▷mixingactionvalues
19 arand←U([amin,amax])
20 Cs←QˆF target(S′)
21 Vs←Φ(o,arand)
22 Qtarget←CsVs
23 Qtarget←argmax(Qtarget)
best
24 Qt ta er ag met←mix(Qprediction) ▷mixingactionvalues
(d) (f)
25 L←use(9)withR,Qt ta er ag met,Qp ter ae mdiction
26 BackpropagateLtotheparametersofQˆprediction Figure5: Environments: (a)2A2L,(b)5A5L,(c)S-PP,(d)IC-PP,
27 UpdateweightsofQˆF targetwiththoseofQˆF prediction (e)2A1W,and(f)4A2W.A.3 ExperimentalDetails
2Agents&2Landmarks(2A2L) 5Agents&5Landmarks(5A5L)
hyperparameters Algorithms hyperparameters Algorithms
CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team
maxepisode 10000 10000 10000 10000 10000 10000 10000 maxepisode 10000 10000 10000 10000 10000 10000 10000
nbruns 5 5 5 5 5 5 5 nbruns 5 5 5 5 5 5 5
gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99 gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99
exploration e-greedy ornstein-uhlenbeck e-greedy ornstein-uhlenbeck e-greedy – – exploration e-greedy ornstein-uhlenbeck e-greedy ornstein-uhlenbeck e-greedy – –
learningstartssteps 10000 10000 10000 10000 10000 10000 10000 learningstartssteps 10000 10000 10000 10000 10000 10000 10000
buffersize 500000 500000 500000 500000 500000 500000 500000 buffersize 500000 500000 500000 500000 500000 500000 500000
samplemethod uniform – uniform – uniform – – samplemethod uniform – uniform – uniform – –
samplesize 1000 – 1000 – 1000 – – samplesize 1000 – 1000 – 1000 – –
stepsperupdate 1 10 1 50 1 50 50 stepsperupdate 1 10 1 50 1 50 50
batchsize 512 512 512 512 512 512 512 batchsize 512 512 512 512 512 512 512
optimizer Adam Adam Adam Adam Adam Adam Adam optimizer Adam Adam Adam Adam Adam Adam Adam
lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001 lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001
lrcritic – 0.00025 – 0.00025 – 0.001 0.001 lrcritic – 0.00025 – 0.00025 – 0.001 0.001
targetnetworklr 0.005 0.005 0.005 0.005 0.005 0.01 0.01 targetnetworklr 0.005 0.005 0.005 0.005 0.005 0.01 0.01
nblayers 3 2 3 2 3 3 3 nblayers 3 2 3 2 3 3 3
nbneurons 256 256 256 256 256 256 256 nbneurons 256 256 256 256 256 256 256
activationactor tanh tanh tanh tanh tanh relu relu activationactor tanh tanh tanh tanh tanh relu relu
activationcritic – relu – relu – relu relu activationcritic – relu – relu – relu relu
Table3:Algorithmichyperparametersfor2A2L. Table4:Algorithmichyperparametersfor5A5L.
Figure6: Detailedresultsfor2A2LacrossCQF,CDDPG,IQF,ID- Figure7: Detailedresultsfor5A5LacrossCQF,CDDPG,IQF,ID-
DPG,MQF,andMADDPG-Team. DPG,MQF,andMADDPG-Team.StandardPredator-Prey(S-PP) IncreasedCooperationPredators-Prey(IC-PP)
hyperparameters Algorithms hyperparameters Algorithms
CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team
maxepisode 120000 120000 120000 120000 120000 120000 120000 maxepisode 40000 40000 40000 40000 40000 40000 40000
nbruns 3 3 3 3 3 3 3 nbruns 3 3 3 3 3 3 3
gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99 gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99
exploration gaussian ornstein-uhlenbeck gaussian ornstein-uhlenbeck gaussian – – exploration gaussian ornstein-uhlenbeck gaussian ornstein-uhlenbeck gaussian – –
learningstartssteps 10000 10000 10000 10000 10000 10000 10000 learningstartssteps 10000 10000 10000 10000 10000 10000 10000
buffersize 500000 500000 500000 500000 500000 500000 500000 buffersize 500000 500000 500000 500000 500000 500000 500000
samplemethod uniform – uniform – uniform – – samplemethod uniform – uniform – uniform – –
samplesize 1000 – 1000 – 1000 – – samplesize 1000 – 1000 – 1000 – –
stepsperupdate 50 10 50 50 50 50 50 stepsperupdate 50 50 50 10 50 50 50
batchsize 512 512 512 512 512 512 512 batchsize 512 512 512 512 512 512 512
optimizer Adam Adam Adam Adam Adam Adam Adam optimizer Adam Adam Adam Adam Adam Adam Adam
lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001 lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001
lrcritic – 0.00025 – 0.00025 – 0.001 0.001 lrcritic – 0.00025 – 0.00025 – 0.001 0.001
targetnetworklr 0.005 0.005 0.005 0.005 0.005 0.01 0.01 targetnetworklr 0.005 0.005 0.005 0.005 0.005 0.01 0.01
nblayers 3 2 3 2 3 3 3 nblayers 3 2 3 2 3 3 3
nbneurons 256 256 256 256 256 256 256 nbneurons 256 256 256 256 256 256 256
activationactor tanh tanh tanh tanh tanh relu relu activationactor tanh tanh tanh tanh tanh relu relu
activationcritic – relu – relu – relu relu activationcritic – relu – relu – relu relu
Table5:AlgorithmichyperparametersforS-PP. Table6:AlgorithmichyperparametersforIC-PP.
Figure8: DetailedresultsforS-PPacrossCQF,CDDPG,IQF,ID- Figure9: DetailedresultsforS-PPacrossCQF,CDDPG,IQF,ID-
DPG,MQF,MADDPG-Team. DPG,MQF,MADDPG-Team.MultiWalkerEnvironment–2Agents(2A1W) MultiWalkerEnvironment–4Agents(4A2W)
nwalkers 1 nwalkers 2
sharedreward False sharedreward False
terrainlength 100 terrainlength 100
maxcycles 500 maxcycles 500
positionnoise 0 positionnoise 0
anglenoise 0 anglenoise 0
forwardreward 5 forwardreward 5
Table7:Environmentalhyperparametersfor2A1W. Table9:Environmentalhyperparametersfor4A2W.
hyperparameters Algorithms hyperparameters Algorithms
CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team CQF CDDPG IQF IDDPG MQF MADDPG-Ind MADDPG-Team
maxepisode 20000 20000 20000 20000 20000 20000 20000 maxepisode 30000 30000 30000 30000 30000 30000 30000
nbruns 10 10 10 10 10 10 10 nbruns 5 5 5 5 5 5 5
gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99 gamma 0.99 0.99 0.99 0.99 0.99 0.99 0.99
exploration gaussian ornstein-uhlenbeck gaussian ornstein-uhlenbeck gaussian – – exploration gaussian ornstein-uhlenbeck gaussian ornstein-uhlenbeck gaussian – –
learningstartssteps 10000 10000 10000 10000 10000 10000 10000 learningstartssteps 10000 10000 10000 10000 10000 10000 10000
buffersize 500000 500000 500000 500000 500000 500000 500000 buffersize 500000 500000 500000 500000 500000 500000 500000
samplemethod uniform – uniform – uniform – – samplemethod uniform – uniform – uniform – –
samplesize 10000 – 10000 – 10000 – – samplesize 10000 – 10000 – 10000 – –
stepsperupdate 50 10 50 10 50 50 50 stepsperupdate 50 10 50 10 50 50 50
batchsize 512 512 512 512 512 512 512 batchsize 512 512 512 512 512 512 512
optimizer Adam Adam Adam Adam Adam Adam Adam optimizer Adam Adam Adam Adam Adam Adam Adam
lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001 lractor 0.001 0.000025 0.001 0.000025 0.001 0.001 0.001
lrcritic – 0.00025 – 0.00025 – 0.01 0.01 lrcritic – 0.00025 – 0.00025 – 0.01 0.01
targetnetworklr 0.01 0.005 0.01 0.005 0.01 0.01 0.01 targetnetworklr 0.01 0.005 0.01 0.005 0.01 0.01 0.01
nblayers 2 2 2 2 2 3 3 nblayers 2 2 2 2 2 3 3
nbneurons 256 256 256 256 256 256 256 nbneurons 256 256 256 256 256 256 256
activationactor tanh tanh tanh tanh tanh tanh tanh activationactor tanh tanh tanh tanh tanh tanh tanh
activationcritic – relu – relu – relu relu activationcritic – relu – relu – relu relu
Table8:Algorithmichyperparametersfor2A1W. Table10:Algorithmichyperparametersfor4A2W.
Figure 10: Detailed results for 2A1W across CQF, CDDPG, IQF, Figure 11: Detailed results for 4A2W across CQF, CDDPG, IQF,
IDDPG,MQF,MADDPG-Team. Sinceeachjoint(agent)receives IDDPG,MQF,MADDPG-Team. Sinceeachjoint(agent)receives
thesamerewardasthecorrespondingwalker,onlytherewardsfor thesamerewardasthecorrespondingwalker,onlytherewardsfor
thewalkerisdisplayed. thewalkersaredisplayed.