1
MODIPHY: Multimodal Obscured Detection for IoT
using PHantom Convolution-Enabled Faster YOLO
Shubhabrata Mukherjee, Cory Beard, Zhu Li
School of Science and Engineering, University of Missouri Kansas City
Kansas City, USA
smpw5@umsystem.edu; beardc@umkc.edu; zhu.li@ieee.org
Abstract—Low-lightconditionsandoccludedscenariosimpede acclaim [10]–[12]. While deploying neural network models
object detection in real-world Internet of Things (IoT) appli- on edge devices like UAVs presents challenges [13], our
cations like autonomous vehicles and security systems. While
work bridges this gap by enhancing the YOLOv8 nano [14]
advanced machine learning models strive for accuracy, their
architecture, giving rise to the “YOLO Phantom.” Tailored
computational demands clash with the limitations of resource-
constrained devices, hampering real-time performance. In our for resource-constrained IoT devices, this adaptation ensures
currentresearch,wetacklethischallenge,byintroducing“YOLO compatibilitywithsmall-scalehardware,achievingsubstantial
Phantom”, one of the smallest YOLO models ever conceived. speedimprovementsforreal-timeobjectdetectioninlow-light
YOLO Phantom utilizes the novel Phantom Convolution block,
conditions. Our presentation delves into these enhancements,
achieving comparable accuracy to the latest YOLOv8n model
scrutinizes their performance gains, and showcases their ef-
while simultaneously reducing both parameters and model size
by43%,resultinginasignificant19%reductioninGigaFloating- fectiveness in pertinent low-light object detection tasks.
Point Operations (GFLOPs). YOLO Phantom leverages transfer In the context of our current research, we have developed
learning on our multimodal RGB-infrared dataset to address a compact yet efficient multimodal object detection model
low-light and occlusion issues, equipping it with robust vision
optimized for effective low light object detection. Leveraging
underadverseconditions.Itsreal-worldefficacyisdemonstrated
the FLIRV2 dataset [15], we crafted a multimodal dataset
on an IoT platform with advanced low-light and RGB cameras,
seamlessly connecting to an AWS-based notification endpoint to train our own YOLO Phantom model, complemented by
for efficient real-time object detection. Benchmarks reveal a integratingaNoIRcameraModule[16]withRaspberryPifor
substantial boost of 17% and 14% in frames per second (FPS) exceptionalperformanceinlow-lightscenarios.Thedistinctive
for thermal and RGB detection, respectively, compared to the
contributions of this paper include:
baselineYOLOv8nmodel.Forcommunitycontribution,boththe
code and the multimodal dataset are available on GitHub.1. • Creation of a compact YOLO model: Designing one
of the smallest YOLO models, by using only 50% of
Index Terms—Low light object detection, Multimodal fusion,
the kernels in three of the layers, demonstrating superior
IoT, YOLOv8, Phantom Convolution
speed without compromising accuracy.
• YOLOPhantomformultimodaldetection:Introducing
I. INTRODUCTION
YOLOPhantom,capableofdetectingmultiplemodalities
Robust object detection under low light and occlusion is suchasRGBandthermal.Themodelunderwenttraining
crucial for various applications, including traffic monitoring, using various modalities and demonstrates effectiveness
publicsafety,andenvironmentalmonitoring.Instancessuchas in scenarios characterized by low light conditions and
Tesla’s autopilot oversight and GM’s Cruise pedestrian crash occlusion.
underscore safety concerns in autonomous vehicles [1-2]. The • Innovative convolution block: Proposing the novel
challenges extend to facial recognition systems, facing accu- “Phantom Convolution” block, by using Depthwise Sep-
racy drops in inadequate lighting, necessitating advancements arable Convolution [17] and Group Convolution [18]
in object detection technologies [3-4]. AI plays a pivotal role layers we created a faster and more efficient version of
inaddressingtheselow-lightchallenges,employinginnovative Ghost Convolution [19].
approaches like integrating infrared sensors to perceive ther- • Realization of an end-to-end notification system: Re-
mal signatures in darkness. The early fusion [5] of thermal alizing the new model through a practical end-to-end
andRGBdatasetsinmultimodalimagingemergeasapowerful object detection framework. This involves integrating a
solution,particularlyinovercomingocclusion.Fusingthermal, RaspberryPiwithRGBandNoIRcamerasforsimultane-
visiblelight,LiDAR,andradardistancescanningdatathrough ous detection, along with an AWS cloud-based real-time
machine learning significantly improves the performance of notification system.
object detection and classification systems.
Recent strides in low-light object detection, utiliz-
II. IMPLEMENTATIONSTRATEGIES
ing deep learning and fusion techniques [6]–[9], exhibit
Customized detection systems power diverse applications,
promise. YOLO-based methods, in particular, have gained
from self-driving cars to surveillance, tailoring to specific
1Thecodeanddatasetareavailableathttp://tinyurl.com/46y5twku environments and tasks. Key types include:
4202
beF
21
]VC.sc[
1v49870.2042:viXra2
• Aerial Detection System (ADS): ADS is utilized in models. Performance evaluation revealed comparable or su-
air defense, public safety, disaster recovery [20] and perior performance in different scenarios compared to other
vegetation monitoring, with ground-based installations state-of-the-art (SOTA) smaller models, encompassing both
strategically placed and air-based systems integrated into RGB and thermal modalities. YOLO Phantom’s lightweight
manned or unmanned aerial vehicles (UAVs) for en- design makes it highly suitable for real-time object detection
hanced situational awareness. on edge devices.
• Marine Detection System (MDS): MDS is designed
to detect and identify threats to maritime security, using
radar,sonar,acousticsensors,andsmartcamerastotrack
vessels and activities in surrounding waters [21]. AI-
based software and ML algorithms analyze sensor data
for threat identification, with examples like an Auto-
maticIdentificationSystem(AIS)forvesseltrackingand
Long-RangeIdentificationandTracking(LRIT)forlong-
distance identification.
• Ground-based Detection System (GDS): GDS focuses
on detecting, classifying, or tracking objects in terrestrial
environments, employing technologies like ground-based
radar,RGBorinfrared-sensor-basedcameras,andmotion
Fig.2. Size,Parameters,andGFLOPcomparisonofsmallerYOLOmodels
sensors to identify people, vehicles, or object move-
ment[22].AI-basedinfrastructureanalyzescollecteddata
for threat identification, including perimeter intrusion IV. OURAPPROACH:YOLOPHANTOM
detection systems and object detection with facial recog-
We have modified the architecture of the smallest version
nition using smart cameras and machine learning.
of the Ultralytics YOLOv8 model, YOLOv8n, to enable it to
detect both RGB and thermal modality images. Furthermore,
westrategicallyrestructuredandresizedselectnetworkblocks
tofacilitateexpeditedmodelinference.And,finally,wetrained
it using transfer learning with a multimodal RGB and thermal
imagedatasetasdescribedlater.ThisenabledYOLOPhantom
to detect multimodal objects much faster than the Ultralytics
YOLOv8, with similar or slightly higher accuracy (mAP), as
seen in Section V.
A. Ultralytics YOLOv8 architecture
The Ultralytics YOLOv8 architecture is primarily divided
into three main parts: the Backbone, Neck, and Head. The
Backbone is responsible for generating feature maps and in-
Fig. 1. Detection on a rainy, obscured evening with severe occlusion using
amultimodalYOLOmodelandaNoIRcameraonaRaspberryPiplatform corporates the following components: C2f, a modified version
of Darknet53 with Cross-Stage Partial (CSP) connections for
improved feature re-usability and gradient flow, convolutional
III. ROLEOFRESOURCEOPTIMIZATION
layers with varying kernel sizes. The output of the Backbone
Limited resources in remote areas often constrain resilient is feature maps at multiple scales, capturing both low-level
object detection in IoT systems. Optimized resource usage details and high-level semantic information. Fig. 3 shows the
becomes vital for their extended, reliable operation in such implementationandourchangesinYOLOv8backbone,which
environments, enhancing lifespan, accuracy, and overall effi- includes the C2f replaced by improved C2f (“C2fi”) and new
cacy. In particular, resource optimization is focused on the Phantom Convolution blocks. Ultralytics YOLOv8 backbone
efficient utilization of computation resources such as power, also consists of SPPF (Spatial Pyramid Pooling - Fast) which
memory, and temperature control. Limited power in devices enhances the receptive field and captures multi-scale features
like Unmanned Aerial Vehicle (UAV) or surveillance cars through parallel pooling branches with different kernel sizes
hinderstrainingandrunningcomputation-intensiveAImodels, and concatenation of pooled features.
compromising efficiency, longevity, and overall performance. Fig. 4 shows the Ultralytics YOLOv8 implementation of
Through significant improvements, particularly the incorpo- the neck and head, along with our introduced Phantom Con-
ration of our newly proposed phantom convolution block, volution block. Upsample blocks are used to increase the
we successfully developed one of the most compact YOLO feature resolution and match with C2fi (C2f with ShortCut
models to date. As illustrated in Fig. 2, our model boasts the False)blocksusingConcatblocks.TheoutputsfromC2figoes
fewest parameters, minimal GFLOPs, and the smallest size to Convolution blocks as well as detection heads, used for
when compared to some of the most recent compact YOLO detecting small, medium, or large objects. It uses a decoupled3
Fig.3. ModifiedYOLOv8Backbone[23]
Fig.4. ModifiedYOLOv8NeckandDecoupleddetectionHead[23]
head to perform various tasks like detection, classification,
b) Depth-wise separable Convolution: Depth-wise Sep-
segmentation separately. YOLOv8 also uses SiLU (Swish
arableconvolution[17]isaCNNvariantthatsplitsastandard
Linear Unit) as default activation function, Mosaic augmen-
convolution into two steps: depth-wise convolution, where
tation, anchor-free detection, batch normalization for enriched
each input channel is convolved independently, and point-
training,andflexibleprediction.PleasenotethatinsidetheUl-
wise convolution, combining results across channels with 1x1
tralyticsrepository,inthemainarchitecturefileforYOLOV8,
filters. This separation significantly reduces computational
the head and neck sections have been collectively referred to
complexity and parameters compared to traditional convolu-
as ‘head’.
tions, making it widely adopted in mobile and edge devices
for efficiency without compromising expressiveness. It strikes
abalancebetweencomputationalefficiencyandmodelperfor-
B. Architecture improvement for faster and better inference
mance, making it suitable for resource-constrained environ-
In the development of YOLO Phantom, we undertook
ments.
modifications to accommodate both RGB and thermal im-
• Input tensor: X ∈RN×C×H×W
age detection, enhancing the model’s versatility. Ultralytics
• Depthwise convolution filter: D ∈RC×1×Kd×Kd
YOLOv8trainingwasconductedontheCOCOdataset[24],a
• Pointwise convolution filter: P ∈RC′×C×1×1
predominantly RGB dataset. It was necessary for us to make
• Bias term: b∈RC′×1
adjustments to architectural elements and parameter scaling
• Output tensor: Y ∈RN×C′×H′×W′
by directing our efforts toward achieving comparable mean
Depthwise Convolution:
average precision (mAP) performance with reduced model
complexity and improved speed. Various novel convolution
methodshavebeenadaptedbyresearcherstoimproveaccuracy
Y =DepthwiseConv(X,D)//Y
∈RN×C×H′×W′
(2)
dw dw
performance (better mAP) or make the model faster (higher
FPS) to perform different real-time computer vision tasks. Pointwise Convolution:
We experimented with quite a few of them for our training;
we have mentioned our observations from using them in our
Y =PointwiseConv(Y ,P)+b//Y
∈RN×C′×H′×W′
(3)
model architecture. dw
a) Group Convolution: Group convolution [25] divides Now, if we combine above two operations:
filters into groups, each operating on a subset of input
channels. Unlike traditional convolution connecting to all
input channels, group convolution segregates filters, reducing
(cid:88)C (cid:88)Kd (cid:88)Kd
Y = X ·D
n,c′,i,j n,c,i×S+k′,j×S+l′ c,1,k′,l′
computational costs and enabling parallelization. It enhances
c=1k′=1l′=1
efficiencywithoutcompromisingrepresentationalpower,espe-
C
cially in models with many channels. The group convolution +(cid:88) Y ·P +b (4)
dw,n,c,i,j c′,c,1,1 c′
operation can be expressed as below:
c=1
• Input tensor: X ∈RN×C×H×W c) GhostConvolution: GhostConvolution[19]enhances
• Weight tensor: W ∈RG×C G×K×K convolutional neural networks (CNNs) by introducing a par-
• Bias term: b∈RG×1 allel “ghost branch” alongside the main convolutional layers.
• Output tensor: Y ∈RN×G×C G×H′×W′ Theghostbranch,createdbysubsamplinginputchannels,pro-
cessesareducedinputversionandcombinesitsoutputwiththe
mainbranch.Thistechniquebalancescomputationalefficiency
C
(cid:88)G and model expressiveness. By utilizing fewer channels in the
Y = X ·W +b (1)
n,g,i,j n,(g−1)C G+c,i′,j′ g,c,k′,l′ g ghost branch, Ghost Convolution achieves resource savings
c=14
while retaining essential features. It is particularly valuable
in scenarios with limited computational resources like mobile
devices where efficiency is crucial.
Fig.6. DetectionandNotificationpipeline
filter, making it ideal for low-light detection. To establish
connectivity between the Raspberry Pi and the cloud, we
leveraged an AWS IoT Core, thereby constructing an event
notification framework. This integration empowered us to dis-
patchcustomizednotificationsbasedondetectedevents;inour
experimentation, we validated this functionality by triggering
notifications upon the detection of a person. Furthermore, we
used AWS Simple Notification Service (SNS), to augment the
Fig.5. PhantomConvolutionandC2fiblockarchitecture AWSIoTCoreindisseminatingalerts.Thisfeaturefacilitated
thetransmissionofnotificationstodesignatedemailaddresses,
Various strategies, culminating in adopting Ghost Convolu-
phone numbers, or mobile applications, enhancing the versa-
tion, were employed to enhance feature extraction with fewer
tility and accessibility of our alerting mechanism.
parameters. As seen in Fig. 5, the Ghost Convolution block
underwentre-architecting,withthefirstlayertransformedinto
a group convolution (group size 4) for increased operational
speed and larger kernel sizes (5x5) to expand the receptive
field. The second layer utilized Depth-wise Separable Convo-
lutionforefficiency,leadingtoacompactblockwithimproved
computational efficiency, named “Phantom Convolution”.
The C2f blocks were modified by eliminating forward pass
connections, introducing “C2fi” or “C2f improved” blocks
for a smaller, faster, and more efficient model. These blocks,
withshortcutconnectionssettofalse,replacedtraditionalC2f
blocks throughout the architecture. These changes allowed a
reduction in the number of filters in deeper layers, halving
the number of filters in layer 7 and in the head of the
backbone.Notably,theseventhlayerincorporatedthePhantom
Fig.7. DetectionsetupusingRaspberryPiandRGBcamera
Convolutionblock,andthefinalconvolutionblockinthehead
(layer19)wasreplacedwithPhantomConvolutiontoenhance To bolster the core detection framework, we integrated
efficiency and expedite the detection process. supplementary safety controls. A temperature monitoring sys-
tem for the Raspberry Pi automatically halts detection when
exceeding a conservative 60°C threshold. Additionally, an
C. Experimental Setup
audio alert system triggers upon person detection, offering
In this study, we devised an end-to-end detection and no-
redundancy beyond visual notifications. These enhancements
tification system utilizing IoT-based hardware in conjunction
increase responsiveness and provide layered alerts for en-
with Amazon Web Services (AWS) for cloud-based notifica-
hanced situational awareness and threat mitigation.
tions (Fig. 7). Specifically, we employed a Raspberry Pi [26]
version4,modelB(CanaKitExtreme,128GB,8GB,BullsEye
D. Dataset description and Training
OS), as the hosting and execution platform for the object
detection model. An external RGB camera was interfaced Utilizing the Teledyne FLIR ADAS Thermal Dataset
with the USB port of the Raspberry Pi to capture relevant v2 [15], we curated a multimodal dataset comprising both
data. We also connected a NoIR Camera Module V2 (Fig. 6) RGB and thermal imagery. The capability to perceive thermal
to the Raspberry Pi using Camera Serial Interface (CSI). infrared radiation offers both complementary and distinct
This camera module can see in low-light without an infrared advantages compared to conventional sensor technologies like5
Fig.8. Variousoccludedandlow-lightdetectionscenariocomparisonwithsmallerYOLOmodels(lefttoright)usingNoIRorRGBcameras
visiblecameras,Lidar,andradarsystems.TeledyneFLIRther- (NMS) 0.5 threshold. Figure 8 showcases YOLO Phan-
mal sensors excel in detecting and classifying objects under tom’scompetitiveperformance,matchingorexceedingsmaller
challenging conditions, including total darkness, fog, smoke, models like YOLOv4-tiny, YOLOv7-tiny, YOLOv8n, and
adverse weather, and glare. The primary dataset comprises YOLOv5nu, despite having 4-2x fewer parameters.
26,442 annotated images spanning 15 categories. From the
training and validation sets, we selected 10,478 thermal and
10,053 RGB images for the training set, and a combination
of 1,128 thermal and 1,069 RGB images for validation to
B. Performance on different modality data
construct the multimodal dataset for training. Focusing on the
four most represented classes (person, car, traffic light, street
sign), we conducted experiments. Additionally, a separate In this section, we conducted a benchmark analysis of
thermal test dataset of 3,749 images and an RGB test dataset YOLO Phantom against recent pre-trained models, including
of 3,493 images were constructed from the main FLIR V2 YOLOv5nu, Ultralytics YOLOv8n, and YOLOv8n trained on
dataset’s test section for benchmarking purposes. multimodal data without any architecture modifications. This
comparativeassessmentwasexecutedontwodistinctmodality
YOLO Phantom was trained using our multimodal dataset
datasets:RGBandthermaldata.Thekeyperformancemetrics
on a CentOS cluster with two “NVIDIA RTX A6000” GPUs
considered for benchmarking were accuracy (mAP50-95(B))
and64CPUsfor100epochs.Imageswereresizedto640x640
and frames per second (FPS). The evaluation encompassed
pixels. The “yolov8n.pt” pre-trained weights served as the
five different platforms: PyTorch, TorchScript, ONNX, Open-
starting point for the training process.
VINO,andncnn.Thebenchmarkingprocesswasconductedon
a M1 MAC OS CPU. As depicted in Fig. 9, YOLO Phantom
V. RESULTSANDANALYSIS demonstrates a significantly superior performance compared
to pre-trained models such as YOLOv5nu and YOLOv8n. It
A. Out of sample testing
exhibits a slightly higher accuracy than the YOLOv8n fusion
This section presents a performance comparison of YOLO model trained on our multimodal data. Moreover, YOLO
Phantom against other state-of-the-art (SOTA) smaller YOLO Phantom consistently achieves higher FPS across various
models. The evaluation employed a diverse image set encom- model variants, surpassing both pre-trained smaller models
passing low-light and occlusion scenarios (thermal imaging, andtheYOLOv8nfusionmodel.Similartrendswereobserved
tunnel interiors, cloudy evenings, foggy mornings, nighttime). in Fig. 10 during the thermal data benchmarking. YOLO
Both RGB and NoIR cameras captured the images. All the Phantom outperforms its competitors in mAP and achieves
detection has been performed with a non-max-suppression superior speed due to its smaller size.6
Next, we conducted a cross-modality mAP performance
comparison on the thermal dataset. As seen in Table III,
in this case also the reverse modality model (RGB-trained)
performed poorly compared to the same modality (thermal-
trained)YOLOv8nmodel.However,thethermal-trainedmodel
performed marginally better compared to YOLO Phantom on
this occasion.
Fig.9. mAPandFPSperformancecomparisononRGBdatawithpretrained TABLEIII
YOLOV5nu,pretrainedYOLOv8n,YOLOv8ntransferlearnedonmultimodal CROSS-MODALITYMAPPERFORMANCEONTHERMALDATA
data,andYOLOPhantom
Format RGBTrained ThermalTrained YOLOPhantom
PyTorch 7.21 25.21 24.82
TorchScript 6.81 24.9 24.46
ONNX 6.81 24.9 24.46
OpenVINO 2.81 24.58 23.79
ncnn 3.87 15.26 14.94
Finally, we performed a cross-modality FPS comparison
on the thermal dataset. As observed in Table IV, RGB, and
thermal modality trained models performed nearly similarly,
Fig. 10. mAP and FPS performance comparison on thermal data with
pretrainedYOLOV5nu,pretrainedYOLOv8n,YOLOv8ntransferlearnedon but YOLO Phantom shows much better FPS performance
multimodaldata,andYOLOPhantom compared to both.
C. Cross-modality performance TABLEIV
CROSS-MODALITYFPSPERFORMANCEONTHERMALDATA
Thissectionevaluatestheperformanceofmultimodalmod-
Format RGBTrained ThermalTrained YOLOPhantom
elsagainstunimodalmodelsusingreversemodalitydata.This
PyTorch 17.76 16.54 16.65
approach demonstrably highlights the added value of multi- TorchScript 15.54 15.01 15.09
modal learning. Table I showcases performance (mAP) on ONNX 19.16 18.48 19.32
RGBdatacomparingYOLOv8ntrainedonthermal,YOLOv8n OpenVINO 22.92 22.38 24.22
ncnn 23.98 24.03 26.96
trained on RGB, and YOLO Phantom trained multimodally.
While the thermal-trained YOLOv8n struggled, the RGB-
trained one excelled. Notably, YOLO Phantom achieved sim-
ilar or better performance in most cases, demonstrating its
multimodal advantage. VI. CONCLUSIONANDFUTUREWORK
YOLO Phantom opens the door to many new real-world
TABLEI
CROSS-MODALITYMAPPERFORMANCEONRGBDATA object detection tasks, particularly in resource-constrained
environments. This lightweight model exhibits a remarkable
Format ThermalTrained RGBTrained YOLOPhantom
equilibrium between rapid execution, elevated accuracy, and
PyTorch 9.29 19.65 19.72
TorchScript 9.19 19.57 19.51 resource efficiency – the three most critical factors for IoT
ONNX 9.19 19.57 19.51 devices. The innovative Phantom Convolution block seam-
OpenVINO 8.43 10.24 16.65 lessly integrates into any framework, unlocking superior fea-
ncnn 5.22 11.99 11.95
ture extraction with reduced computational burden. Its novel
architecture transcends specific tasks, demonstrating excep-
In the next case, we performed a similar comparison, but tional performance across diverse, resource-scarce detection
instead of accuracy (mAP) we measured speed (FPS). As scenarios. To further strengthen an implementation’s practi-
observed in Table II, in this case, YOLO Phantom performed cal viability for resource-constrained IoT deployments, user-
similar or better than thermal-trained or RGB-trained models. definedtemperaturemonitoringwasaddedtosafeguardagainst
thermalexcursions,whileoptionalaudioalertsofferadditional
redundancyforenhancedsituationalawarenessandthreatmit-
TABLEII
CROSS-MODALITYFPSPERFORMANCEONRGBDATA igation. This advancement holds promise for broader perfor-
mance gains across computer vision, encompassing tasks like
Format ThermalTrained RGBTrained YOLOPhantom
PyTorch 17.50 17.14 17.53 classification, segmentation, and even generative modeling.
TorchScript 16.35 15.77 15.50 The impact of YOLO Phantom extends beyond just enhanced
ONNX 20.12 20.45 21.92 road safety and refined surveillance applications, paving the
OpenVINO 22.86 23.28 25.86
way for pioneering applications in environmental monitoring,
ncnn 23.77 23.85 27.15
wildlife tracking, and beyond.7
REFERENCES [23] RangeKing. (2024) Github homepage. [Online]. Available: https:
//github.com/RangeKing
[1] A. Press. (2023, January) Ap: Tesla driver killed after plowing into [24] T.Lin,M.Maire,S.J.Belongie,L.D.Bourdev,R.B.Girshick,J.Hays,
firetruckonfreeway.[Online].Available:http://tinyurl.com/3bejc9cy P. Perona, D. Ramanan, P. Doll’a r, and C. L. Zitnick, “Microsoft
[2] Reuters. (2023, November) Gm’s cruise to recall 950 driverless COCO:commonobjectsincontext,”CoRR,vol.abs/1405.0312,2014.
cars after accident involving pedestrian. [Online]. Available: http: [Online].Available:http://arxiv.org/abs/1405.0312
//tinyurl.com/5fj6mre5 [25] Z.Su,L.Fang,W.Kang,D.Hu,M.Pietika¨inen,andL.Liu,“Dynamic
[3] R.AlSobbahiandJ.Tekli,“Comparingdeeplearningmodelsforlow- group convolution for accelerating convolutional neural networks,” in
lightnaturalsceneimageenhancementandtheirimpactonobjectdetec- Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
tionandclassification:Overview,empiricalevaluation,andchallenges,” UK, August 23–28, 2020, Proceedings, Part VI 16. Springer, 2020,
SignalProcessing:ImageCommunication,p.116848,2022. pp.138–155.
[4] M. Ahmed, K. A. Hashmi, A. Pagani, M. Liwicki, D. Stricker, and [26] Raspberry Pi Foundation. (2024) Raspberry pi. [Online]. Available:
M.Z.Afzal,“Surveyandperformanceanalysisofdeeplearningbased https://www.raspberrypi.org/
objectdetectioninchallengingenvironments,”Sensors,vol.21,no.15,
p.5116,2021.
[5] R.O.Chavez-GarciaandO.Aycard,“Multiplesensorfusionandclas-
sificationformovingobjectdetectionandtracking,”IEEETransactions
onIntelligentTransportationSystems,vol.17,no.2,pp.525–534,2016.
[6] W.ChenandT.Shah,“Exploringlow-lightobjectdetectiontechniques,”
arXivpreprintarXiv:2107.14382,2021.
[7] R. Kvyetnyy, R. Maslii, V. Harmash, I. Bogach, A. Kotyra, Z. Gradz,
A.Zhanpeisova,andN.Askarova,“Objectdetectioninimageswithlow
light condition,” in Photonics Applications in Astronomy, Communica-
tions,Industry,andHighEnergyPhysicsExperiments2017,vol.10445.
SPIE,2017,pp.250–259.
[8] L. Ye and Z. Ma, “Llod: a object detection method under low-light
conditionbyfeatureenhancementandfusion,”in20234thinternational
seminar on artificial intelligence, networking and information technol-
ogy(AINIT). IEEE,2023,pp.659–662.
[9] X.Wang,D.Wang,S.Li,S.Li,P.Zeng,andX.Liang,“Low-lighttraffic
objectsdetectionforautomatedvehicles,”in20226thCAAInternational
ConferenceonVehicularControlandIntelligence(CVCI). IEEE,2022,
pp.1–5.
[10] W.Liu,G.Ren,R.Yu,S.Guo,J.Zhu,andL.Zhang,“Image-adaptive
yoloforobjectdetectioninadverseweatherconditions,”inProceedings
oftheAAAIConferenceonArtificialIntelligence,vol.36,no.2,2022,
pp.1792–1800.
[11] X.Yin,Z.Yu,Z.Fei,W.Lv,andX.Gao,“Pe-yolo:Pyramidenhance-
mentnetworkfordarkobjectdetection,”inInternationalConferenceon
ArtificialNeuralNetworks. Springer,2023,pp.163–174.
[12] Z.Wang,Z.Cai,andY.Wu,“Animprovedyoloxapproachforlow-light
andsmallobjectdetection:Ppeontunnelconstructionsites,”Journalof
ComputationalDesignandEngineering,vol.10,no.3,pp.1158–1175,
2023.
[13] S.Y.Nikouei,Y.Chen,S.Song,R.Xu,B.-Y.Choi,andT.R.Faughnan,
“Real-timehumandetectionasanedgeserviceenabledbyalightweight
CNN,” in 2018 IEEE International Conference on Edge Computing
(EDGE). IEEE,2018,pp.125–129.
[14] G.Jocher,A.Chaurasia,andJ.Qiu,“UltralyticsYOLO,”https://github.
com/ultralytics/ultralytics,version8.0.0,releasedJanuary2023.License:
AGPL-3.0.
[15] T. FLIR, “Free teledyne flir thermal dataset for algorithm training,”
2023, accessed: 2024-01-30. [Online]. Available: https://www.flir.com/
oem/adas/adas-dataset-form/
[16] Raspberry Pi Foundation. (2024) Raspberry pi camera module
2 noir. [Online]. Available: https://www.raspberrypi.com/products/
pi-noir-camera-v2/
[17] F.Chollet,“Xception:Deeplearningwithdepthwiseseparableconvolu-
tions,”inProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,2017,pp.1251–1258.
[18] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification
withdeepconvolutionalneuralnetworks,”Advancesinneuralinforma-
tionprocessingsystems,vol.25,2012.
[19] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet:
Morefeaturesfromcheapoperations,”inProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,2020,pp.1580–
1589.
[20] S.Mukherjee,O.Coudert,andC.Beard,“Unimodal:Uav-aidedinfrared
imagingbasedobjectdetectionandlocalizationforsearchanddisaster
recovery,” in 2022 IEEE International Symposium on Technologies for
HomelandSecurity(HST),2022,pp.1–6.
[21] M.Zhang,S.Xu,W.Song,Q.He,andQ.Wei,“Lightweightunderwater
object detection based on yolo v4 and multi-scale attentional feature
fusion,”RemoteSensing,vol.13,no.22,p.4706,2021.
[22] X.Tang,Z.Zhang,andY.Qin,“On-roadobjectdetectionandtracking
based on radar and vision fusion: A review,” IEEE Intelligent Trans-
portationSystemsMagazine,vol.14,no.5,pp.103–128,2022.