Label-Efficient Model Selection for Text Generation
♠♡ ♠ ♠♢ ♠
ShirAshury-Tahan ,BenjaminSznajder ,LeshemChoshen ,LiatEin-Dor ,
♠ ♠
EyalShnarch andArielGera
♠ ♡ ♢
IBMResearch, Bar-IlanUniversity, MIT
1 Abstract ingnuancedjudgmentsofthequalityofgenerated
texts (Celikyilmaz et al., 2020); often, this can
Model selection for a given target task can be
onlybedonebyexperthumanannotators(vander
costly,asitmayentailextensiveannotationofthe
Lee et al., 2021), or possibly by powerful LLMs
quality of outputs of different models. We intro-
(e.g.,GPT-4,Zhengetal.,2023),bothofwhichare
duce DiffUse, an efficient method to make an in-
costlytoapplyatscale. Moreover,asthenumberof
formeddecisionbetweencandidatetextgeneration
modelsandtasksincreases,conductingtheseeval-
models. DiffUse reduces the required amount of
uations becomes prohibitively expensive (Perlitz
preferenceannotations,thussavingvaluabletime
etal.,2023a).
andresourcesinperformingevaluation.
Ourgoalistoaddressthecostsassociatedwith
DiffUse intelligently selects instances by clus-
evaluating model outputs in text generation, by
teringembeddingsthatrepresentthesemanticdif-
reducing the burden on the oracle. Specifically,
ferences between model outputs. Thus, it is able
we focus on the use case of directly comparing
to identify a subset of examples that are more in-
twocandidatemodels,wheretheoracleisaskedto
formative for preference decisions. Our method
makepreferencejudgementsbetweentheoutputs
ismodel-agnostic, andcanbeappliedtoanytext
generated by the two models. Our focus is on
generation model. Moreover, we propose a prac-
comparativejudgmentsandnotabsolutescores,as
ticaliterativeapproachfordynamicallydetermin-
these are considered more reliable for evaluating
ing how many instances to annotate. In a series
textgeneration(Callison-Burchetal.,2007;Sedoc
ofexperimentsoverhundredsofmodelpairs,we
etal.,2019;Lietal.,2019;Liangetal.,2020).
demonstratethatDiffUsecandramaticallyreduce
Inthiswork,weproposeamethodthatsubstan-
therequirednumberofannotations–byupto75%
tially reduces the number of examples that must
–whilemaintaininghighevaluationreliability.
beannotatedbytheoracle,whileyieldingamore
reliableestimateofthepreferredmodelforthetask.
2 Introduction
Ourapproach-DiffUse-selectspairsofmodelout-
Modelevaluationisaprerequisiteforinformedde- putsthatontheonehandarerepresentativeofthe
cisions–predominantly,choosingtherightmodel spaceofdifferencesbetweenmodelbehaviorson
for the task. As such, an essential requirement is agiventask,andontheotherhandaremoreinfor-
theabilitytocomparemodelsbasedonhowwell mative, showing clearer preference. Specifically,
theyperform. wecalculateembeddingvectorsthatrepresentthe
Comparing model performance generally re- semanticdifferencebetweentheoutputsofthetwo
quiressomesortoforacle–ahumanannotatoror models; then, by partitioning these embeddings
LLM-basedevaluator–thatcanjudgemodelout- intoclusters,wecanintelligentlyselectadiverse
putsandpreferoneoutputoveranother. However, informativesubsetofinstancesforannotation.
depending on the nature of the oracle, such judg- DiffUse is inherently generic and does not as-
ments can incur significant costs, particularly in sume anything about the models, tasks, or unla-
termsofannotationbudgets(Ein-Doretal.,2020b; beledtestdata. Ourresults(§6)demonstrateitssta-
van der Lee et al., 2019) and computational re- bilityandeffectivenessfordifferenttextgeneration
quirements (Liang et al., 2022; Biderman et al., tasks,acrosshundredsofpairsofgenerativemod-
2023; Perlitz et al., 2023a). Specifically for text els,andacrossabroadrangeofannotationbudgets.
generationtasks,theoracleisburdenedwithmak- Wealsoproposeaniterativereal-worldsolutionfor
4202
beF
21
]LC.sc[
1v19870.2042:viXrapractitioners(§6.2),whichenablesmakingreliable underagivenannotationbudget,bywiselyselect-
andcost-efficientchoicesbetweencandidatemod- ing only a subset of examples Dobserved ⊆ D
test test
els. Wefindthismethodtobebetterinallofour fromthetestsettobelabeledbytheoracle.
experiments,achievingareductioninannotations Anaivebaselineforestimatingthetestwinning
ofupto75%comparedtorandomsampling. modelistouniformlysampleN testinstances,la-
Furthermore,weconductacomprehensiveanal- bel them, and compute the winning model over
ysis (§7) of the components of our method. Our theseinstances.
findingssuggestthatourmethodtendstoselectex-
amplesfromregionsintheoutput-differencespace 4 Method
thataredominatedbythepreferredmodel.
Our algorithm, DiffUse, is simple and effective,
3 DefinitionsandProblemFormulation and relies solely on the outputs generated by the
models. ThefullflowisdescribedinFigure1.
Inthiswork,wefocusoncomparativeevaluationof Weaimtorepresentexamplesinamannerthat
models. Giventwotextgenerationmodels,wewish capturesthedistributionofmodelmismatchingbe-
toevaluatewhichmodelisstrongerwithrespectto haviors,i.e.,varioustypesofdifferencesbetween
agivengenerationtask,basedonpreferencelabels modeloutputs. Tothisend,wefirstembedmodel
ofanoracleoverthemodeloutputs. outputsintoasemanticvectorspace(usingoff-the-
For an input instance x and a pair of mod- shelfmethods). Subsequently,wegeneratediffer-
elsM A,M B withcorrespondingoutputsy A,y B,a encevectorsbysubtractingtheembeddingsofone
preference label y pref ∈ {M A,M B,T} indicates modelfromtheembeddingsoftheother,foreach
whethery A isbetterthany B (M A),worsethany B exampleinthetestset. Then,weclusterthesedif-
(M B)orsimilartoy B (T). ferencevectorsandselectonerepresentativefrom
eachclustertobelabeledbytheoracle.
Test winning model The model for which the
Giventheconstructionofthedifferencevectors,
outputsoverthetestsetD aremorefrequently
test weexpectthisvectorspacetolargelycarryinfor-
preferredbytheoracle. Formally:
mationaboutsemanticdifferences,i.e.,thenature
⎧ ⎪
⎪ ⎪
⎪M
A
ifP tM estA > P tM estB o exf ad mis pa lg er fe re om men et as chbe ct lw ue ste en rm eno sd ue rels s. thC ah to to hs ein seg ta on
f
W test = ⎪ ⎨ ⎪M B ifP tM estA < P tM estB selected examples is representative of this space;
⎪
⎪ ⎪ ⎪ ⎩T ifP tM estA = P tM estB hence,theseexamplesareexpectedtobeinforma-
tiveforestimatingwhichisthepreferredmodel.
where
5 Experiments
1
Pm = ∑ 1 (1)
test ∣D ∣ {y pref=m} 5.1 TheData
test (x,y pref)∈D
test
Throughoutourexperiments,weutilizedatafrom
is the test winning probability of model m ∈
theHELMbenchmark(Liangetal.,2022,version
{M A,M B},and1
{y
pref=m}istheindicatorfunction 0.2.22
). We rely on data from its core scenarios,
thattakesthevalue1ify = mand0otherwise.1 which encompass inputs, outputs, and scores for
pref
variousmodels,datasets,andtasks.
Testwinningdistance Theabsolutedifference
TheHELMscoresserveastheground-truthdata,
betweenthetestwinningprobabilitiesofthetwo
such that the test winning model (§3) for a given
models,∣PM
A
−PM
B∣.
test test scenarioandmodelpairistheonethatreceiveda
higherscoreforagreaternumberoftestinstances.
Problemformulation Calculatingthetestwin-
Note that the scores within HELM reflect the re-
ning model requires preference labels for every
sultofanautomatedreference-basedmetric. Thus,
pointinthetestset. However,thisisoftencostly
while this “oracle” is not a human, it does rely
andimpractical. Thus,ourgoalistomaximizethe
ontheexistenceofannotatedhumanreferencean-
probabilityof identifying thetestwinning model
swers. Moreover, for each scenario we report re-
1Pm isitselfanunbiasedestimatorofPm,i.e.,the(un-
test
known)winningprobabilityoverallpossibleinputinstances 2https://crfm.stanford.edu/helm/v0.2.2/?group=
fromthesamedistribution. core_scenarios🤖
Oracle
Model A OutputsA Embeddings A
Labels
Test set Difference
Vectors
🤖
Clusters Representatives
Model B
Outputs B Embeddings B
Figure1: DiffUseflow. Ourmethodconsistsof5steps: performinginferencewiththemodelsonthetestset,
encodingthegeneratedoutputs,performingpairwisesubtraction,clusteringtheresultingvectors,andselecting
representativesforevaluation. Acomprehensivedescriptionisprovidedin§4.
sultsforseveralautomatedmetrics,hencesimulat- andGurevych,2019)all-MiniLM-L6-v2encoderto
3
ingarangeofdifferentkindsoforacles. embedtheoutputs ,andsubtracttheresultingem-
Our experimental setup includes 6 text genera- beddingstoobtaindifferencevectors. Forcluster-
tionscenarios,withresultsof666uniquepairsof ingthevectors,weoptforHierarchicalAgglomer-
models (paired comparisons of 37 different mod- ativeClustering(Müllner,2011)withWardlink-
4
els)foreachscenario. Thetasksweexperimented age andEuclideandistance.
with are summarization and question answering For a given budget of N examples to be anno-
(i.e., the text generation tasks in HELM), as de- tatedbytheoracle,weselectthembypartitioning
tailedinAppendixTable1. AsshowninFig.2,the thevectorsintoN clusters. Then,fromeachcluster
winningdistancesbetweenmodelpairsinHELM weselectasingleexample,andspecificallytheone
span a large range, but are often small; in other whoseembeddingisclosest(incosinedistance)to
words,HELMshowcasesdiversebehaviorsbutde- thecenterofthecluster.
terminingthewinningmodelisusuallynottrivial. Note that while we found this setup to work
particularly well, opting for a different choice of
clustering algorithm, or for a different approach
XSum
400
CNN/DailyMail ofselectingexamplesgiventheclusters,doesnot
350 NarrativeQA dramaticallyaffecttheresults(§7.1).
NaturalQuestions - Closed-book
300
NaturalQuestions - Open-book
5.3 ExampleSelectionExperiments
250 QuAC
200 Ourmainexperimentsexaminethesuccessrateof
anexampleselectionmethod,definedasfollows.
150
Foragivendataset,abudgetofsizeN,andapair
100
ofgenerativemodels,weuseaselectionmethodto
50
selectN examplesforannotation. Thissampleis
0
thenannotatedbytheoracle,andusedtodetermine
0.0 0.2 0.4 0.6 0.8
Test Winning Distance thesamplewinningmodel. Anexampleselection
runissucccessfulwhenthesamplewinningmodel
Figure2: Distributionoftestwinningdistance(§3)in
equalsthetestwinningmodel. Thesebinaryresults
HELMbetweenpairsofgenerativemodels. Evidently,
mostmodelsarecloselypositionedtoeachother. are then aggregated across several random seeds
andacrossallgenerativemodelpairstodetermine
thesuccessrateoftheexampleselectionmethod.
5.2 ExampleSelectionMethod
DiffUseiscomparedtothe(strong)baselineof
Asoutlinedabove(§4),DiffUseconsistsofcalcu- randomselection,wheretheN examplesaresam-
latingdifferencevectorsthatrepresentthemodel
3https://huggingface.co/sentence-transformers/
output behaviors, clustering them, and sampling
all-MiniLM-L6-v2
examplesbasedontheresultingclusters. 4https://docs.scipy.org/doc/scipy/reference/
Specifically, we use Sentence-BERT (Reimers generated/scipy.cluster.hierarchy.linkage.html
sriap
fo
rebmuNanthropic_stanford-online-all-v4-s3
openai_text-davinci-003
vs. cohere_command-medium-beta
vs. ai21_j1-jumbo
100
75
70
95
65
90
60
55
85
50
80
DiffUse 45 DiffUse
Random Random
40
75
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples
Figure3: Comparingexampleselectionmethods. Successrates(±standarderror)inidentifyingthebestoftwo
competinggenerativemodels(listedintheplottitle),intermsoftheirperformanceoverCNN/DailyMail(using
Rouge-2astheoracle).
pledi.i.d. fromthedataset. To give a broader and quantitative picture,
Tobetterestimatetherobustnessoftheselection Figure 4 depicts the aggregated results for the
methods,foreachexperimentalrun(seed)wesam- CNN/DailyMail summarization data, averaged
plealargesubsetofthefulldata(800outof1000 acrossall666modelpairs. Theplotdemonstratesa
scenarioexamplesinHELM)andtreatthissubset clearadvantageofourapproachoverrandomselec-
asifitwerethefulltestset. tion,arrivingatthecorrectdecisionmoreoftenand
Intotal,foreachofthe6HELMscenarios,we usingfewerexamples. Thus,usingDiffUsethere
report results across 666 unique model pairs, 10 isamuchlowerriskofchoosingthewrongmodel.
runs(seeds)foreach,andvaryingN between5to Thispatternisquiteconsistentacrossthedifferent
200examplestobeannotated. datasetstested,ascanbeseeninAppendixA.1.
6 Results CNN/DailyMail (rouge-2)
WestartbycomparingthesuccessrateofDiffUse
90
tothatoftheRandomselectionbaseline.
Figure 3 illustrates two cases of such compari-
85
son,eachforaspecificpairofmodels. Ascanbe
seen,successratescanvarygreatlybetweencases
80
wherethereisarelativelylargeperformancediffer-
encebetweenthegenerativemodels(leftpanel)and
75
those with a small performance difference (right
DiffUse
panel). As for the latter, estimating the preferred
Random
70
model is harder and requires more annotated in-
0 25 50 75 100 125 150 175 200
stances. Naturally,themodelpreferenceestimation Number of annotated examples
becomesmoreaccurateasthebudgetN increases
Figure4: Modelchoicesuccessrate. Aggregatedex-
andthepreferencedecisionreliesonalargersetof
ampleselectionresultsfortheCNN/DailyMaildataset,
examplesannotatedwithoraclepreference.
across all 666 model pairs (×10 repetitions for each
In the two cases presented in Fig. 3, DiffUse
pair). DiffUse demonstrates a clear advantage in cor-
achieveshighersuccessratesatidentifyingthebet- rectlydeterminingthestrongermodel,basedonasmall
tergenerativemodel,incomparisontorandomsam- numberoforacle-annotatedexamples.
pling. These results showcase that with DiffUse
onecanreachthecorrectdecisionwithasmaller Note that while our approach demonstrates
numberofexamplestobeannotatedbytheoracle. a clear advantage, its effect does vary across
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuSdatasets,andacross“oracles”(inourcase,different Winning Distance Bias (XSum)
reference-basedmetrics). Thisislikelyconnected 0.15 DiffUse
tothenatureofthetasks. Whereataskhaslongand Random
0.10
diverseoutputs,thedifferencevectorscontainrich
semantic information; in contrast, where outputs
0.05
are short and highly constrained (e.g., extractive
QA),thesemanticdifferencesbetweenoutputsare 0.00
lessinformativeandDiffUsemaybelesseffective.
0.05
6.1 EvaluatedWinningDistance
0.10
Ourfocusisonmakingaccuratepreferencechoices
betweenmodels,i.e. choosingthebetterperformer. 0 100 200 300 400 500 600 700 800
Number of annotated examples
However,anotherfacetofmodelevaluationisthe
sizeoftheperformancegap,whichwerefertoas Figure5: Deviationfromtestwinningdistance. The
the “winning distance” (§3). Thus, when using a figuredepictsthedifferencebetweentheestimatedwin-
ningdistanceandthetestwinningdistance,aggregated
small set of examples to estimate the relation be-
across all model pairs over XSum. Shaded areas de-
tweenmodels,anotherinterestingquestionishow
notestandarderror(averagedacrosspairs). Clearly,our
theestimate(e.g.,modelBwonby18%)compares
methodfavorsthetestwinningmodel,providingabi-
totheground-truthperformancegap.
asedestimateinitsfavor. Thebiasdissipatesasmore
Figure 5 depicts the relation between the esti- examplesareselected,onlyconvergingtothetruedis-
matedandactualperformancedistances. Random tanceforthefullsetofexamples.
selection, being an unbiased estimator, naturally
has an average deviation of 0 from the real dis-
5 Tothisend,weproposeaniterativemethodfor
tance . In contrast, the figure demonstrates that
selectingexamples. Inthisapproach,thenumberof
ourmethodprovidesanestimateddistancethatis
examplessenttotheoracleisincreasedgradually,
biasedtowardthewinningmodel. Thisbias,which
untilapredefinedreliability-orientedthresholdis
isparticularlylargewherethenumberofannotated
met. Hierarchicalclusteringnaturallylendsitself
examplesissmall,explainshowthemethodisable
toaniterativesolution: supposewehaveclustered
to outperform random selection at binary prefer-
thedifferencevectorsintoN clusters,andtheor-
ence choices - being biased on average towards
acle has annotated the N selected examples, yet
thewinner,therewouldalsobefewercaseswhere
we suspect that the preference estimation is not
thelosingmodelisaccidentallyselected(notethe
sufficientlyreliable. Inthiscase,wecannowclus-
lowerboundsoftheshadedareasinFig.5).
terthevectorsintok+1clusters;thiswillfurther
6.2 PracticalIterativeSelectionAlgorithm partition one of the previous clusters, providing
6
two new examples to be labeled by the oracle .
Accuracyinestimatingthewinningmodelcanvary
Witheachpartitioningstep,theamountofinforma-
widely, dependingonthenumberofexamplesas
tionincreases,andthisprocedureisrepeateduntil
well as the actual performance gap (Fig. 3). In a
reachingthethreshold/stoppingcriterion.
practicalscenario,however,userspresumablydo
not know in advance the size of the performance Thefulliterativeselectionflowisdescribedin
gapbetweenthemodelstheycompare;moreover, Algorithm1. Forthestoppingcriterion,wepropose
afterannotatingsomeexampleswiththeoracleand touseareliabilitythresholdbasedonthehyperge-
estimatingthewinningmodel,userswillnotknow ometricdistribution(seeAppendixA.2fordetails).
whethertheestimationisinfactcorrect. Thethresholdisaheuristicthatapproximatesthe
Thus, in order to apply oracle effort minimiza- levelofrisk,whereachosenthresholdof0.1,for
tionstrategiesinpractice,thereisaneedforaprac- example,looselycorrespondstoalikelihoodofup
ticalapproachthatdeterminestherequirednumber to 10% of choosing the wrong model. The risk
of examples dynamically, and provides some ap- thresholdissetinadvance,andreflectsapreferred
proximationofthereliabilityoftheestimate.
6Partitioningaclustermeansselectingtwonewexamples,
5Thisdoesnotimplythatasingleestimationusingrandom inadditiontotheoneoriginallyannotatedforthecluster;we
selectionislikelytobeaccurate; rather,thatacrossalarge discardtheoriginalexample(e inAlg.1)fromthepreference
c
numberofestimations,theexpectedvalueofthedeltaiszero. decision,asitispresumedtobelessinformativeatthispoint.
ecnatsid
gninniw
eurt
morf
ecnereffiDAlgorithm1:IterativeSelectionAlgorithm-Risk-basedThreshold
Input: Twomodels{M ,M },datasetD,andoracleO
A B
Parameters: Thresholdp ∈ (0,1),Minimumnumberofannotationsn,Maximumbudgetb
Output: Winningmodel(orinconclusive)
CalculatethedifferencevectorsV(D,M ,M ),asdescribedinSection4.
A B
ClusterV intonclusters.
ChooserepresentativesE = {e ,...,e },onefromeachcluster.
n 1 n
GettheoracletagsT =O(E ),andcalculatetheprobabilitysf (T )(seeApp.A.2)
n n hypergeom n
Initializek = n+1,T = T
n
whilesf (T) > pandlabeled < bdo
hypergeom
Findthenextclusterctobesplit(1 ≤ c < k),andsplitit.
ChooserepresentativesE
k
= {e k,e k+1},onefromeachoftwosplits.
GettheoracletagsT =O(E ).
k k
k = k+1,T = (T − {e })∪T
n c k
ReturnthewinningmodelaccordingtoT.
200
DiffUse
175 Random
DiffUse
150
125
100
75
Random
50
25
0 0 20 40 60 80 100
NarrativeQ NA atu Ora pl eQ nu -be ost oi kons XSum CNN/DailyMail QuAC Natu Clr oal sQ edu -e bs oti oo kns Success Outc Io nm coe n c(% lu) sive Error
Figure6: Iterativeselectionresults. Theplotsdepicttheresultsofapplyingiterativeselection(Algorithm1;with
p=0.2,n=5,andb=200),comparingDiffUsetorandomsampling. Resultsareaggregatedacross666model
pairs. Theleftpaneldepictsthemeannumberofexamplesannotatedbytheoraclebeforereachingthestopping
criterion. Therightpaneldepictstheproportionofoutcomesoftheiterativeselectionexperiments–i.e.,wasa
winningmodeldetermined,andwasthisdecisioncorrect–aggregatedacrossalldatasets. SeealsoApp.Table2,3.
pointonatrade-off: betweentheuser’stolerance thisdatasetarequitesmall(cf. Fig.2),makingit
for error, and the amount of examples the oracle difficulttoconclusivelydeterminethewinner.
willneedtoannotate.
Resultsfortheiterativealgorithmareshownin 7 Analysis
Figure 6. Clearly, DiffUse provides a significant
7.1 MethodParameters
advantage over random selection, increasing the
likelihoodofsuccessfullydeterminingthewinner Next,weexamine3choicesintheflowofDiffUse
(rightpanel),whilesignificantlyreducingthenum- (Fig.1): therepresentationsofexamples,theclus-
berofexamplessenttotheoracle(leftpanel). teringalgorithmandtheclusterrepresentative
Notethatthenumberofannotationsinpractice selection.
varieswidely,andislinkedtotheperformancegap Our method relies on difference vectors (i.e.,
between the models. For instance, in the Closed- subtractionofoutputembeddings)torepresentex-
BookversionofNaturalQuestions,alargenumber amples. Anaiveapproachwouldbetoclusterthe
ofexamplesisannotated,andtheoutcomeisusu- embeddings of inputs, akin to some methods in
allyinconclusive(leftpanelofFig.6,App. Tab.2); activelearning(Zhangetal.,2022). However,we
this is because the distances between models in find that this approach does not consistently out-
selpmaxe
detatonna
fo
rebmuN
dohteMperform random sampling (App. Figure 14). We
alsocomparevariousapproachesforaggregating
thesemanticembeddingsofthetwooutputs. We
findthatusingdifferencevectors,i.e.,subtracting
thetwoembeddings,outperfomsotheraggregation
methods,suchasconcatenationoraddition.
In contrast, we find that the choices of cluster-
ingalgorithmandrepresentativeselectionareless
significant, and performance differences are not
dramatic(AppendixA.3). Notethatallconfigura-
tionssignificantlyoutperformtherandombaseline. Figure7: Example2-Dprojection. At-SNE(vander
MaatenandHinton,2008)projectionofthedifference
7.2 Whichexamplesareselected? vectors from a randomly selected pair of models in
XSum. Theobservedbehavior,wheremostvectorsare
Asshownabove,thesuccessofourmethodhinges
centered around zero, and the distribution is sparser
ontheuseofoutputdifferencevectors. Next,we awayfromit,isconsistentacrossmodelpairs.
performseveralanalysestobetterunderstandhow
clusteringthesevectorsenablesselectingexamples
Clustering of Difference Vectors (50 clusters)
thatareinformativefortheoracle.
Thedifferencevectorsrepresentvarianceinthe 1.4
outputs, and thus in the models’ behavior for a
given task. Assuming an ideal semantic encoder, 1.2
highlydistinctoutputsshouldyielddifferencevec-
1.0
tors with high norms, signifying pronounced dis-
similarities. Conversely,similaroutputswouldre- 0.8
sultinlowernorms,indicatingsubtledifferences.
0.6
7.2.1 ClusterSizesandDifferenceNorms
0.4
Sincethedistancebetweentwovectorsisbounded
0 10 20 30 40 50 60 70
bythesumoftheirnorms,differencevectorswith % of Examples in Cluster
smaller norms have a higher tendency to be clus-
Figure8: Clustersizevs. averagevectornorm. Hi-
teredtogether. ThisisnicelydemonstratedinFig-
erarchical clustering results of the difference vectors,
ure7,whichdepictsanexampletwo-dimensional fortheXSumdatasetwhenpartitioninginto50clusters.
projectionofdifferencevectorsforapairofmod- Eachpointrepresentsasinglecluster;intotal,theplot
els. The projection reveals a densely populated depicts∼ 33Kpoints(666modelpairs×50resulting
regionclosetozero,correspondingtocaseswhere clustersperpair). Thex-axisreflectsthepercentageof
all examples that are in the cluster, and the y-axis is
themodeloutputsshowmoresubtledifferences.
theaveragevectornormwithinthecluster. Resultsare
Figure8illustratestherelationbetweenthesizes
characterizedbyafewverylargeclusterswithasmall
ofclustersandtheaveragenormofdifferencevec-
averagenorm(bottomright);thispatternisconsistent
tors within the cluster. Evidently, clustering the acrossdifferentnumbersofclusters(App. Fig.15)
differencevectorstendstoresultinasmallnumber
oflargeclusters,whichhavealowaveragenorm
7.2.2 NormsandWinningModel
(bottom-rightareaofFig.8),alongsidealargenum-
berofsmallclusterswithhighernormvalues. Of- We have demonstrated that our method over-
ten,overhalfofthevectorsareassignedtoasingle represents difference vectors with a higher norm.
clusterwithsmallnorms. AsDiffUseselectsone This leads to the question of how this tendency
examplefromeachcluster,thesub-populationof relatestomodelpreference.
examples with small difference norms is under- Figure10depictstherelationbetweenthenorm
representedinthesetofselectedexamples. ofdifferencevectorsandestimationofthetestwin-
Figure9directlydepictsthenormsizedistribu- ningmodel. Ascanbeseen, thepreferencelabel
tionoftheselectedexamples. Again, weseethat ofinstanceswithhigherdifferencenormsismore
DiffUseisbiasedtowardhigh-norminstances. likelytoalignwiththetestwinningmodel. Thisis
retsulC
ni
mroN
egarevAinlinewiththewinner-biasshowninFig.5. Preference Estimation by Norm Bin (NarrativeQA)
A possible explanation for this observation is
thatlargersemanticdifferencesbetweenthemod-
80
els’outputsareexpectedbeassociatedwithlarger
quality gaps; meanwhile, the chances that the
60
weaker model will beat the stronger model’s out-
putbyalargemarginarelow. Thus,thelowerthe
40
differencenorm,thehighertheprobabilityofthe
preferencelabeltobe“erroneous”,namelyforthe
20
weakermodeltobepreferredbytheoracle.
Giventheinformativenessofhigh-normoutput
0
pairs,asimpleapproachwouldbetoforgocluster- 10 20 30 40 50
ing,andsimplyselecttheinstanceswiththehighest Bin of Difference Vector Norms
norm for annotation. However, this results in in-
Figure10:Normsandpreferenceestimation.Theplot
ferior performance (Appendix A.4). This is not depictsthesuccessrateatestimatingthebetteroftwo
surprising; selecting by norm alone can result in models,basedonsub-populationswithvaryingvector
outliers,andmaynotberepresentativeofthespace norms. Foreachmodelpair,thedifferencevectorswere
ofdifferencevectors. partitionedbasedontheirnormsizesinto50equal-count
bins,andeachbinofinstanceswasusedtoestimatethe
testwinningmodel. Theplotpresentsanaggregation
Selected examples (NarrativeQA)
acrossallmodelpairsoverNarrativeQA.
20000 DiffUse
Random
17500
15000 Otherpriorworkshaveexaminedmethodsofin-
12500 telligentlyselectingsubsetsofexamplesforevalua-
10000 tion,aimingtofindsetsofexamplesthataremore
7500 informativethanrandomlysampledinstances.
5000 Rodriguezetal.(2021);Vaniaetal.(2021)look
2500 at the problem of selecting examples for evaluat-
0 ing new models, given fully-annotated question
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
answeringdataforanexistingsetofmodels. They
Norm of Difference Vector
showthatsomeselectionstrategies,basedonitem
Figure9: Normsizedistributionofexamples. These
responsetheory(Lordetal.,1968),outperformthe
histogramsdepictthenormofdifferencevectorsforthe
randomselectionbaselineforrankingnewmodels
output pairs selected for annotation (across all Narra-
onaquestionansweringtask.
tiveQAselectionruns). Comparedtorandomsampling,
DiffUseselectsexampleswithhighervectornorms. Severalworkshaveaddressedlabel-efficientas-
sessmentinthecontextofclassifierperformance.
Katariyaetal.(2012)proposealabel-efficiental-
To sum, clustering difference vectors over-
gorithmtogainbetteraccuracyestimatesofclassi-
representsoutputpairswithalargenorm(§7.2.1).
fiers,byselectingexamplestolabelbasedonstrat-
These, in turn, are more strongly associated with
ified sampling. Ji et al. (2021) suggest an active
theoverallwinningmodel(§7.2.2). Thus,ouranal-
Bayesianapproachthatusesinferreduncertaintyto
ysesillustratehowDiffUseisabletocorrectlydeter-
guideselectionofinstances. Inspiredbyworkson
minethewinningmodelusingfewerannotations.
activelearning,Kossenetal.(2021)proposemeth-
8 RelatedWork ods based on a stochastic acquisition process, to
avoidunwantedandhighlyproblematicbiasesin-
Inlightofthesoaringcostsoflanguagemodeleval- volvedinactiveselectionoftestsetexamples. They
uation,evenwhenusingautomaticmetrics,some showtheeffectivenessoftheiracquisitionfunctions
recentworks(Perlitzetal.,2023a;Maynezetal., on both classification and regression models. Ha
2023)havestudiedtheeffectsofreducingthesize et al. (2021) suggest an iterative method that uti-
ofevaluationsets–viarandomsampling–onthe lizesasurrogatemodeltoestimatethemetricsof
reliablerankingofmodels. interestovertheunlabeledtestset,andlabelsexam-
tnuoC
)%(
etar
sseccuSplesthatleadtomaximaluncertaintyreductionof datafortrainingrewardmodels,andmore.
themetricestimation. Withasimilarspirittoour
Limitations
work,Viveketal.(2023)findanchorexamplesin
classificationdatasetsthatrepresenthowconfident
Asourapproachreliesonobtainingrepresentations
differentmodelsareoverthoseinputexamples.
ofmodeloutputs,itincursthenon-trivialcomputa-
Ourworkdiffersfromtheseprioreffortsinthat tionalcostofperforminginferenceoverthesetof
wetailorourapproachtothenatureoftextgenera- examplestobeclustered,intherangeofhundreds
tion,donotassumea“trainset”ofmodelannota- ofexamples. Thus,ourmethodisonlysuitedfor
tions,andfocusonmakinganinformedpreference the(verycommon)scenariowherethecostofap-
decisionbetweentwocandidatemodels. plying the oracle is significantly greater than the
The current work also draws inspiration from costofperforminginferenceonasomewhatlarger
worksfindingpatternsinmodelbehaviour,forin- setofexamples. Thisisthecaseforexamplewhen
stancethatmodelsseemtolearninasimilarorder theoracleisapaidAPIorahumanannotator.
andmakethesamemistakes(Choshenetal.,2022; Asnotedin§6.1,DiffUseisabiasedapproach
Hacohenetal.,2020),andthatevaluationtrained thattendstoover-representsubpopulationsofthe
on one model can work well over another (Wan of examples. Here we show empirically – across
etal.,2022)orevenworkontheinputlevelalone modelpairsandacrossdatasets–thatthismethod
(Don-Yehiyaetal.,2022). Moreclosely,ourwork providessignificantandconsistentgainsinrelation
relatestoeffortsaimedatreducingannotationcosts torandomselection. However,asalsomentionedin
duringtraining,namelyiterativeactivelearningap- App.A.2,foragivenattemptatmodelcomparison
proaches(Zhangetal.,2022;Ein-Doretal.,2020a; thereisnotheoreticalorstatisticalguaranteeofthe
Perlitzetal.,2023b). probabilityofmakingthecorrectchoice.
Our study is motivated by the fact that obtain-
9 Discussion
ingalargeamountofhumanqualityorpreference
judgments for a target generation task and set of
Wehaveshownthatourmethod,DiffUse,provides
modelsisprohibitivelyexpensive. Ironically,this
significant cost savings in obtaining preference
alsomeansitisnottrivialtoobtainlarge-scaleman-
judgementsfromanoracle. Usingadynamicalgo-
uallyannotateddatathatcanbeusedforevaluating
rithmsuchastheoneproposedhere(§6.2),practi-
theaccuracyofouroracleminimizationapproach
tionerscanobtainaminimalnumberofjudgements
(existing preference datasets, e.g. for RLHF, are
whilemaintaininghighevaluationreliability.
extremelydiverseanddonothaveawell-defined
Inthisworkweprovideasomewhatnovelview
notion of target tasks). Hence, here we rely on
intothenotionofbias. Biasinanexampleselection
the automated metrics in HELM to simulate dif-
methodisoftenseenasaproblem,onethatshould
ferenttypesoforacles. Thisisalimitationofour
be avoided or corrected (Kossen et al., 2021). In
experimentsaswedonotdirectlydemonstrateour
contrast, in our setting the power of DiffUse is
methodonexpensivereal-worldoracles.
preciselythefactthatitisbiasedinaveryparticular
direction(Fig.5,§7.2),reducingthelikelihoodfor
errorinthecontextofbinarypreferencedecisions. References
Hereweexaminedtheproblemofselectingbe-
Stella Biderman, USVSN Sai Prashanth, Lintang
tween a pair of candidate models. We leave to Sutawika, Hailey Schoelkopf, Quentin Anthony,
futureworkthescenarioofpickingfromalarger Shivanshu Purohit, and Edward Raf. 2023. Emer-
gentandpredictablememorizationinlargelanguage
set of candidates. This may entail adapting our
models. arXiv:2304.11158.
method to a multi-model scenario, or combining
our pairwise approach with an efficient method ChrisCallison-Burch,CameronFordyce,PhilippKoehn,
for limiting the number of pairwise comparisons ChristofMonz, andJoshSchroeder.2007. (meta-)
evaluationofmachinetranslation. InProceedingsof
(e.g.,MohankumarandKhapra,2022).
theSecondWorkshoponStatisticalMachineTransla-
Whilethecurrentworkdealswithmodelselec- tion,pages136–158,Prague,CzechRepublic.Asso-
tion,ourapproachofmodelingdifferencesbetween ciationforComputationalLinguistics.
outputscanpotentiallybeapplicableforotherpur-
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
posesaswell. Thiscanincludequalitativeassess-
2020. Evaluation of text generation: A survey.
mentofmodelbehaviours,collectionofpreference arXiv:2006.14799.EunsolChoi,HeHe,MohitIyyer,MarkYatskar,Wen- JannikKossen,SebastianFarquhar,YarinGal,andTom
tauYih, YejinChoi, PercyLiang, andLukeZettle- Rainforth. 2021. Active testing: Sample-efficient
moyer.2018. QuAC:Questionansweringincontext. modelevaluation. InProceedingsofthe38thInter-
In Proceedings of the 2018 Conference on Empiri- nationalConferenceonMachineLearning,volume
calMethodsinNaturalLanguageProcessing,pages 139ofProceedingsofMachineLearningResearch,
2174–2184,Brussels,Belgium.AssociationforCom- pages5753–5763.PMLR.
putationalLinguistics.
TomKwiatkowski, JennimariaPalomaki, OliviaRed-
Leshem Choshen, Guy Hacohen, Daphna Weinshall, field,MichaelCollins,AnkurParikh,ChrisAlberti,
andOmriAbend.2022. Thegrammar-learningtra- DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
jectoriesofneurallanguagemodels. InProceedings tonLee,KristinaToutanova,LlionJones,Matthew
of the 60th Annual Meeting of the Association for Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
ComputationalLinguistics(Volume1: LongPapers), Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
pages8281–8297. ralquestions: Abenchmarkforquestionanswering
research. TransactionsoftheAssociationforCompu-
Shachar Don-Yehiya, Leshem Choshen, and Omri tationalLinguistics,7:452–466.
Abend.2022. PreQuEL:Qualityestimationofma-
chinetranslationoutputsinadvance. InProceedings MargaretLi,JasonWeston,andStephenRoller.2019.
of the 2022 Conference on Empirical Methods in Acute-eval: Improved dialogue evaluation with
NaturalLanguageProcessing,pages11170–11183, optimized questions and multi-turn comparisons.
AbuDhabi,UnitedArabEmirates.Associationfor arXiv:1909.03087.
ComputationalLinguistics.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
LiatEin-Dor,AlonHalfon,ArielGera,EyalShnarch,
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
LenaDankin,LeshemChoshen,MarinaDanilevsky,
Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu-
Ranit Aharonov, Yoav Katz, and Noam Slonim.
mar, et al. 2022. Holistic evaluation of language
2020a. Active Learning for BERT: An Empirical
models. arXiv:2211.09110.
Study. In Proceedings of the 2020 Conference on
EmpiricalMethodsinNaturalLanguageProcessing
WeixinLiang,JamesZou,andZhouYu.2020. Beyond
(EMNLP),pages7949–7962,Online.Associationfor
userself-reportedLikertscaleratings: Acomparison
ComputationalLinguistics.
modelforautomaticdialogevaluation. InProceed-
ingsofthe58thAnnualMeetingoftheAssociation
Liat Ein-Dor, Eyal Shnarch, Lena Dankin, Alon Hal-
forComputationalLinguistics,pages1363–1374,On-
fon,BenjaminSznajder,ArielGera,CarlosAlzate,
line.AssociationforComputationalLinguistics.
MartinGleize,LeshemChoshen,YufangHou,etal.
2020b. Corpuswideargumentmining—aworking
FMLord,MRNovick,andAllanBirnbaum.1968. Sta-
solution. InProceedingsoftheAAAIConferenceon
tisticaltheoriesofmentaltestscores.
ArtificialIntelligence,volume34,pages7683–7691.
Joshua Maynez, Priyanka Agrawal, and Sebastian
Huong Ha, Sunil Gupta, Santu Rana, and Svetha
Gehrmann. 2023. Benchmarking large language
Venkatesh.2021. ALT-MAS:Adata-efficientframe-
model capabilities for conditional generation. In
work for active testing of machine learning algo-
Proceedings of the 61st Annual Meeting of the As-
rithms. arXiv:2104.04999.
sociationforComputationalLinguistics(Volume1:
Long Papers), pages 9194–9213, Toronto, Canada.
GuyHacohen,LeshemChoshen,andDaphnaWeinshall.
AssociationforComputationalLinguistics.
2020. Let’sagreetoagree: Neuralnetworksshare
classificationorderonrealdatasets. InInternational
AkashKumarMohankumarandMiteshKhapra.2022.
ConferenceonMachineLearning,pages3950–3960.
Activeevaluation:EfficientNLGevaluationwithfew
PMLR.
pairwise comparisons. In Proceedings of the 60th
Disi Ji, Robert L. Logan, Padhraic Smyth, and Mark AnnualMeetingoftheAssociationforComputational
Steyvers.2021. Activebayesianassessmentofblack- Linguistics (Volume 1: Long Papers), pages 8761–
boxclassifiers. ProceedingsoftheAAAIConference 8781,Dublin,Ireland.AssociationforComputational
onArtificialIntelligence,35(9):7935–7944. Linguistics.
NamitKatariya,ArunIyer,andSunitaSarawagi.2012. DanielMüllner.2011. Modernhierarchical,agglomera-
Activeevaluationofclassifiersonlargedatasets. In tiveclusteringalgorithms. arXiv:1109.2378.
2012IEEE12thInternationalConferenceonData
Mining,pages329–338. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Çag˘lar Gulçehre, and Bing Xiang. 2016. Abstrac-
TomášKocˇiský,JonathanSchwarz,PhilBlunsom,Chris tivetextsummarizationusingsequence-to-sequence
Dyer,KarlMoritzHermann,GáborMelis,andEd- RNNs and beyond. In Proceedings of the 20th
wardGrefenstette.2018. TheNarrativeQAreading SIGNLLConferenceonComputationalNaturalLan-
comprehensionchallenge. TransactionsoftheAsso- guage Learning, pages 280–290, Berlin, Germany.
ciationforComputationalLinguistics,6:317–328. AssociationforComputationalLinguistics.Shashi Narayan, Shay B. Cohen, and Mirella Lapata. practiceguidelines. ComputerSpeech&Language,
2018. Don’tgivemethedetails,justthesummary! 67:101151.
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018 ChrisvanderLee,AlbertGatt,EmielvanMiltenburg,
Conference on Empirical Methods in Natural Lan- SanderWubben,andEmielJ.Krahmer.2019. Best
guageProcessing,pages1797–1807,Brussels,Bel- practicesforthehumanevaluationofautomatically
gium.AssociationforComputationalLinguistics. generatedtext. InInternationalConferenceonNatu-
ralLanguageGeneration.
Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Ar-
viv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Laurens van der Maaten and Geoffrey Hinton. 2008.
Michal Shmueli-Scheuer, and Leshem Choshen. Visualizingdatausingt-SNE. JournalofMachine
2023a. Efficientbenchmarking(oflanguagemodels). LearningResearch,9(86):2579–2605.
arXiv:2308.11696.
Clara Vania, Phu Mon Htut, William Huang, Dhara
Mungra, Richard Yuanzhe Pang, Jason Phang,
Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer,
HaokunLiu,KyunghyunCho,andSamuelR.Bow-
DafnaSheinwald,NoamSlonim,andLiatEin-Dor.
man.2021. Comparingtestsetswithitemresponse
2023b. Activelearningfornaturallanguagegener-
theory. In Proceedings of the 59th Annual Meet-
ation. In Proceedings of the 2023 Conference on
ingoftheAssociationforComputationalLinguistics
EmpiricalMethodsinNaturalLanguageProcessing,
andthe11thInternationalJointConferenceonNatu-
pages9862–9877,Singapore.AssociationforCom-
ralLanguageProcessing(Volume1: LongPapers),
putationalLinguistics.
pages1141–1158,Online.AssociationforComputa-
NilsReimersandIrynaGurevych.2019. Sentence-bert: tionalLinguistics.
Sentenceembeddingsusingsiamesebert-networks.
RajanVivek,KawinEthayarajh,DiyiYang,andDouwe
InConferenceonEmpiricalMethodsinNaturalLan-
Kiela.2023. Anchorpoints: Benchmarkingmodels
guageProcessing.
withmuchfewerexamples. arXiv:2309.08638.
Pedro Rodriguez, Joe Barrow, Alexander Miserlis
YuWan,DayihengLiu,BaosongYang,HaiboZhang,
Hoyle, John P. Lalor, Robin Jia, and Jordan Boyd-
Boxing Chen, Derek F Wong, and Lidia S Chao.
Graber.2021. Evaluationexamplesarenotequally
2022. Unite: Unifiedtranslationevaluation. arXiv
informative: How should that change NLP leader-
preprintarXiv:2204.13346.
boards? In Proceedings of the 59th Annual Meet-
ingoftheAssociationforComputationalLinguistics
ZhisongZhang,EmmaStrubell,andEduardHovy.2022.
andthe11thInternationalJointConferenceonNatu-
Asurveyofactivelearningfornaturallanguagepro-
ralLanguageProcessing(Volume1: LongPapers),
cessing. InProceedingsofthe2022Conferenceon
pages4486–4503,Online.AssociationforComputa-
EmpiricalMethodsinNaturalLanguageProcessing,
tionalLinguistics.
pages6166–6190,AbuDhabi,UnitedArabEmirates.
AssociationforComputationalLinguistics.
João Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai
Thirani,LyleUngar,andChrisCallison-Burch.2019.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
ChatEval: Atoolforchatbotevaluation. InProceed-
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ingsofthe2019ConferenceoftheNorthAmerican
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Chapter of the Association for Computational Lin-
JudgingLLM-as-a-judgewithMT-benchandchatbot
guistics(Demonstrations),pages60–65,Minneapo-
arena. arXiv:2306.05685.
lis,Minnesota.AssociationforComputationalLin-
guistics.
Ozan Sener and Silvio Savarese. 2018. Active learn-
ing for convolutional neural networks: A core-set
approach. InInternationalConferenceonLearning
Representations.
Akim Tsvigun, Ivan Lysenko, Danila Sedashov, Ivan
Lazichny,EldarDamirov,VladimirKarlov,Artemy
Belousov,LeonidSanochkin,MaximPanov,Alexan-
der Panchenko, Mikhail Burtsev, and Artem Shel-
manov. 2022. Active learning for abstractive text
summarization. In Findings of the Association for
Computational Linguistics: EMNLP 2022, pages
5128–5152,AbuDhabi,UnitedArabEmirates.As-
sociationforComputationalLinguistics.
ChrisvanderLee,AlbertGatt,EmielvanMiltenburg,
and Emiel Krahmer. 2021. Human evaluation of
automaticallygeneratedtext: CurrenttrendsandbestA Appendix sufficiently reliable; if not, the sample size n is
increasedandadditionalexamplesarelabeled.
A.1 FullResults
Notethatweusethisprobability-basedthreshold
Resultsforthe6textgenerationscenarios(datasets) merelyasaheuristic,orproxy,fortherealproba-
in HELM, with 3 different metrics for each sce- bility. Inpractice,theassumptionsofthehypergeo-
nario,arepresentedinFigure11. metricdistributionareviolatedinourcase. Most
importantly,thisdistributiondescribesrandomse-
A.2 IterativeSelectionThreshold
lection,whereasDiffUseisnon-random,andinfact
Asdescribedin§6.2andAlgorithm1,wepropose
hasadistinctbiastowardsselectingcertainkinds
aniterativealgorithmforannotatingexamplesby
ofexamples(§6.1,§7). Moreover,evenforrandom
theoracleandchoosingthewinningmodel.
selection, the approach does not precisely match
Weoptforareliability-orientedstoppingcrite-
themodelcomparisonsetting;forinstance,ifthere
rionthatisbasedonthehypergeometricdistribu-
isalargenumberofexampleswherethereisatie
tion. Thisdistributiondescribestheprobabilityof
betweenthetwomodels,anullhypothesisofa50%
‘success’whensamplingwithoutreplacement,and
win-rateisinfactoverlyconservative. Thus,while
is parameterized by a population size N, sample
thethresholdchosenbytheuserservesasagood
size n, number of successes in the population K
proxyfortheestimatederrorrate,andisthussuit-
andnumberofsuccessesinthesamplek.
ableasastoppingcriterion,itdoesnotguarantee
Specifically,welookatthehypergeometricdis-
theactualerrorratevalue. Inourempiricalexperi-
tribution survival function, sf hypergeom(k − 1),
ments,foralldatasetstheerrorratewaslowerthan
whichdescribestheprobabilityofgettingkormore
thechosenriskthreshold(cf. Tables2,3).
successesbychance. Inamodelcomparisonsce-
When opting for higher risk thresholds, there
nario, n corresponds to the number of examples
is a large impact to the initial number of labeled
annotated by the oracle, and k to the number of
examples, because wins that are based on a very
votesreceivedbythewinningmodelwithinthisset.
small sample (e.g., 3 out of 3) are avoided, even
Wedefinethenullhypothesisasonewherethewin-
thoughtheymaymeettheriskthreshold.
ningmodelisthewinnerin50%oftheinstancesin
thefulltestset,i.e.,whereK = N/2. Usingthis A.3 ClusteringMethodsandRepresentative
valueforK,Theresultsf(k−1)thusreflectshow Selection
likelyorunlikelyitistogetavalueofk orhigher
We conducted selection experiments employing
givenaground-truth50%winrate.
various clustering algorithms. We found that the
For instance, say we select examples out of a
majorityofthesealgorithmsproducedresultsthat
pool of 500 unlabeled examples. The oracle is
exceededthoseofrandomsampling.
given a total of 10 examples to label, and deter-
Below,weprovidedetailsregardingthecluster-
minesthatmodelAwasthewinnerin8ofthem:
ingmethodsweexplored:
sf(k−1,N,K,n) = sf(7,500,250,10) = 0.0529
1. HierarchicalClustering
Thus,inthisexample–giventhenullhypothesis (a) EuclideanDistance: Hierarchicalclus-
andassumingahypergeometricdistribution–there teringwithEuclideandistancemeasures
isonlya≈ 5%probabilityofgettingsuchahigh dissimilaritybetweendatapointsbased
win rate – or a higher one – by chance. In other ontheirspatialcoordinates. Itfacilitates
words,asituationwheremodelAisthewinnerin cluster creation by iteratively merging
just50%ofthefulltestset,andan8/10resultwas data points to minimize within-cluster
obtained,isrelativelyunlikely. Asituationwhere variance.
modelAisthewinnerinunder50%ofthetestset
(b) Cosine Distance: Hierarchical cluster-
isevenlesslikely. Thismeansthattheusercanbe
ingusingcosinedistancemeasuressimi-
fairlyconfidentthatthecorrectwinnerwaschosen.
laritybetweendatapointsviathecosine
Thus,whenapplyingtheiterativealgorithm,the
oftheanglebetweenvectors. Cosinedis-
user sets an acceptable risk level – say, 10% – in
tanceswereemployedduringthemerg-
advance; at each iteration, sf is calculated using
ingprocess.
thecurrentvaluesofnandk;ifthevalueofsf is
lower than the risk level, the result is considered 2. K-Means Clustering: K-Means clusteringTask Scenario Description
NarrativeQA The NarrativeQA benchmark for reading comprehension
QuestionAnswering
overnarratives(Kocˇiskýetal.,2018)
NaturalQuestions The NaturalQuestions (Kwiatkowski et al., 2019) bench-
(closed-book) markforquestionansweringbasedonnaturally-occurring
queriesthroughGoogleSearch. Theinputdoesnotinclude
theWikipediapagewiththeanswer.
NaturalQuestions The NaturalQuestions (Kwiatkowski et al., 2019) bench-
(open-book) markforquestionansweringbasedonnaturally-occurring
queries through Google Search. The input includes the
Wikipediapagewiththeanswer.
QuAC (Ques- TheQuACbenchmarkforquestionansweringinthecontext
tion Answering ofdialogues(Choietal.,2018).
inContext)
XSUM The XSUM benchmark for text summarization of BBC
Summarization
newsarticles(Narayanetal.,2018)
CNN/DailyMail The CNN/DailyMail benchmark for text summarization
(Nallapatietal.,2016).
Table1: TheHELMscenariosweusedforourexperiments,whichincludeshortandlongtextoutputtasks.
#Annotations↓ Error(%)↓ Success(%)↑ Inconclusive(%)↓ AverageDistance Avg.Dist. Avg.Dist. Avg.Dist.
Dataset Method (Error)↓ (Success) (Inconcl.)↓
CNN/DailyMail DiffUse 15.90 11.97 86.50 1.53 0.27 0.06 0.30 0.09
Random 22.47 16.77 80.32 2.91 0.27 0.10 0.31 0.07
NarrativeQA DiffUse 28.77 4.20 89.64 6.16 0.31 0.06 0.34 0.05
Random 118.69 0.72 43.03 56.25 0.31 0.11 0.55 0.13
NaturalQuestionsClosed-book DiffUse 189.05 0.20 6.40 93.41 0.15 0.12 0.32 0.13
Random 191.74 0.11 4.37 95.53 0.15 0.10 0.32 0.14
NaturalQuestionsOpen-book DiffUse 85.25 0.93 62.81 36.26 0.20 0.03 0.28 0.06
Random 160.97 0.17 21.79 78.05 0.20 0.15 0.44 0.13
QuAC DiffUse 86.91 7.75 58.24 34.01 0.15 0.07 0.20 0.08
Random 128.93 4.37 35.93 59.70 0.15 0.09 0.25 0.10
XSum DiffUse 44.25 6.86 79.38 13.75 0.30 0.09 0.35 0.08
Random 59.39 6.05 72.45 21.50 0.30 0.10 0.37 0.09
Table 2: Iterative selection results (p = 0.2). The table depicts the results of applying iterative selection
(Algorithm1;withp=0.2,n=5,andb=200),comparingDiffUsetorandomsampling. Resultsareaggregated
across666modelpairs.Thetabledetailstheamountofannotationsperformedbeforereachingthestoppingcriterion,
andtheoutcomesoftheselectionexperiments(Success/Error/Inconclusive). Inaddition, itdetailstheaverage
winningdistance(§3)betweenmodelpairs,brokendownbytheexperimentoutcomes. ↓: Lowerisbetter.
Wheretheexperimentresultisinconclusiveorthewrongwinningmodelischosen,theperformancegapbetween
modelsisquitesmall;Thus,evenwheretheuserisunabletocorrectlydeterminethebetter-performingmodel,the
costofthisfailureisrelativelylimited.NarrativeQA (exact-match) NarrativeQA (quasi-exact-match) NarrativeQA (f1-score)
95
95
95
90
90
85 90
85
80 85
80
75
80
70 DiffUse 75 DiffUse DiffUse
Random Random Random
75
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
QuAC (exact-match) QuAC (quasi-exact-match) QuAC (f1-score)
90
90 90
85 85 85
80 80 80
75 75 75
70 70 70
DiffUse DiffUse DiffUse
65
Random 65 Random 65 Random
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Closed-book (exact-match) NaturalQuestions - Closed-book (quasi-exact-match) NaturalQuestions - Closed-book (f1-score)
95 95 95
90 90 90
85 85
85
80 80
80
75 75
70 70 75
65 D Raif nfU dose m 65 D Raif nfU dose m 70 D Raif nfU dose m
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Open-book (exact-match) NaturalQuestions - Open-book (quasi-exact-match) NaturalQuestions - Open-book (f1-score)
95
95 95
90
90 90
85
85 85
80
80 80
75
75
75
70 DiffUse DiffUse DiffUse
Random 70 Random Random
70
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
CNN/DailyMail (rouge-2)
CNN/DailyMail (BERTScore-F) CNN/DailyMail (summac)
90 95.0 95.0
92.5 92.5
85 90.0 90.0
87.5
87.5 80 85.0
85.0
82.5
75 82.5
80.0
DiffUse DiffUse 80.0 DiffUse
70 Random 77.5 Random Random
77.5
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
XSum (rouge-2) XSum (BERTScore-F) XSum (summac)
95 96 85
94 80
90 92
75 90
85 88 70
86
80 84 65
DiffUse 82 DiffUse 60 DiffUse
75 Random Random Random
80
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
Figure11: Fullresults. Plotsdepictsuccessratesofmodelpreferenceestimation,aggregatedover666unique
modelpairs. Eachpaneldepictsadifferentcombinationofdatasetand"oracle"(reference-basedevaluationmetric).
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS#Annotations↓ Error(%)↓ Success(%)↑ Inconclusive(%)↓ AverageDistance Avg.Dist. Avg.Dist. Avg.Dist.
Dataset Method (Error)↓ (Success) (Inconcl.)↓
CNN/DailyMail DiffUse 34.54 7.13 86.44 6.43 0.27 0.05 0.30 0.07
Random 51.98 8.65 79.32 12.03 0.27 0.07 0.32 0.07
NarrativeQA DiffUse 47.80 1.35 84.53 14.11 0.31 0.05 0.36 0.05
Random 132.75 0.05 37.69 62.27 0.31 0.05 0.59 0.14
NaturalQuestionsClosed-book DiffUse 195.37 0.00 3.30 96.70 0.15 NaN 0.37 0.14
Random 197.35 0.00 1.65 98.35 0.15 NaN 0.40 0.14
NaturalQuestionsOpen-book DiffUse 111.02 0.23 52.87 46.91 0.20 0.02 0.30 0.07
Random 172.34 0.02 16.71 83.27 0.20 0.05 0.49 0.14
QuAC DiffUse 129.20 2.96 43.26 53.78 0.15 0.06 0.23 0.09
Random 158.26 1.14 25.11 73.75 0.15 0.08 0.29 0.11
XSum DiffUse 71.18 2.16 73.21 24.62 0.30 0.07 0.37 0.08
Random 88.84 2.09 63.90 34.01 0.30 0.08 0.41 0.10
Table 3: Iterative selection results (p = 0.1). The table depicts the results of applying iterative selection
(Algorithm1;withp=0.1,n=5,andb=200),comparingDiffUsetorandomsampling. Resultsareaggregated
across666modelpairs.Thetabledetailstheamountofannotationsperformedbeforereachingthestoppingcriterion,
andtheoutcomesoftheselectionexperiments(Success/Error/Inconclusive). Inaddition, itdetailstheaverage
winningdistance(§3)betweenmodelpairs,brokendownbytheexperimentoutcomes. ↓: Lowerisbetter.
Wheretheexperimentresultisinconclusiveorthewrongwinningmodelischosen,theperformancegapbetween
modelsisquitesmall;Thus,evenwheretheuserisunabletocorrectlydeterminethebetter-performingmodel,the
costofthisfailureisrelativelylimited.
partitionsdatainto’k’clustersbyiteratively andtasks,asdemonstratedinFig.16. Thisisnot
assigningdatapointstothenearestclustercen- surprising; selecting by norm alone can result in
terandupdatingcentersbasedonthemeanof outliers,andmaynotberepresentativeofthespace
assigned points. Our approach incorporated ofdifferencevectors.
"greedy k-means++" for centroid initializa-
tion,leveraginganempiricalprobabilitydis-
tribution of points’ contributions to overall
inertia.
Themodelpreferencesuccessratesfordifferent
clusteringalgorithms,selectingasinglerepresen-
tative from each cluster based on distance to the
clustercenter,areshowninFigure12.
Wealsoexploredvariousmethodsforselecting
a representative from each cluster. These meth-
odsencompassedrandomselection,choosingthe
examplenearesttothecentroid(employingeither
Euclidean or cosine distances), and selecting the
examplewiththemaximumnorm. AsseeninFig-
ure13,thechoiceofrepresentativesdidnotsignifi-
cantlyimpacttheoutcomes.
Here we focus on clustering algorithms as the
approachforsamplingfromthevectordistribution.
However,otherselectionapproaches,suchascore-
set(SenerandSavarese,2018)orIDDS(Tsvigun
etal.,2022),mayalsoproveeffective.
A.4 NormofDifferenceVectors
Weexploredthenormofthedifferencevectorsas
a signal for selecting examples. While we exper-
imented with various binning scenarios, the best
outcomeswereobtainedbydirectlyselectingthe
vectors with the maximal norm. However, even
thisapproachprovedinconsistentacrossdatasetsNarrativeQA (f1-score) QuAC (f1-score) NaturalQuestions - Closed-book (f1-score)
95
90
95
90
85
90 85
80
85 75 80
H-cosine - centers-cos H-cosine - centers-cos 75 H-cosine - centers-cos
80 H-ward - centers-cos 70 H-ward - centers-cos H-ward - centers-cos
random random 70 random
75 K-means - centers-cos 65 K-means - centers-cos K-means - centers-cos
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Open-book (f1-score) CNN/DailyMail (rouge-2) XSum (rouge-2)
95
95
90
90 90
85
85
85
80
80
H-cosine - centers-cos H-cosine - centers-cos 80 H-cosine - centers-cos
75 H-ward - centers-cos 75 H-ward - centers-cos H-ward - centers-cos
random random random
70 K-means - centers-cos 70 K-means - centers-cos 75 K-means - centers-cos
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
Figure12:Comparingclusteringalgorithms.Plotsdepictsuccessratesofmodelpreferenceestimation,aggregated
over666uniquemodelpairs. Eachpaneldepictsadifferentdataset. Forallclusteringmethods,asingleexample–
closestincosinedistancetotheclustercenter–isselectedfromeachcluster.
NarrativeQA (f1-score) QuAC (f1-score) NaturalQuestions - Closed-book (f1-score)
100 95
90
95 90
85
85
90 80
80
85 H-ward - centers 75 H-ward - centers 75 H-ward - centers
H-ward - centers-cos H-ward - centers-cos H-ward - centers-cos
80 H-ward - max_norm 70 H-ward - max_norm 70 H-ward - max_norm
H-ward - random H-ward - random H-ward - random
random 65 random 65 random
75
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Open-book (f1-score) CNN/DailyMail (rouge-2) XSum (rouge-2)
95
95
90
90 90
85
85 85
80
80 H-ward - centers H-ward - centers 80 H-ward - centers
H-ward - centers-cos H-ward - centers-cos H-ward - centers-cos
75 H-ward - max_norm 75 H-ward - max_norm 75 H-ward - max_norm
H-ward - random H-ward - random H-ward - random
70 random 70 random 70 random
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
Figure13:Comparingrepresentativeselectionmethods.Plotsdepictsuccessratesofmodelpreferenceestimation,
aggregated over 666 unique model pairs. Each panel depicts a different dataset. For all non-random methods,
hierarchicalclusteringwithWardlinkagewasusedtopartitionthedifferencevectors;theplotscompareapproaches
forselectingasinglerepresentativefromeachcluster.
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuSNarrativeQA (f1-score) QuAC (f1-score) NaturalQuestions - Closed-book (f1-score)
95 90 95
90
85
90
85 80
85 80
75
75
80 random 70 random random
H-cosine - centers-cos - inputs H-cosine - centers-cos - inputs 70 H-cosine - centers-cos - inputs
H-ward - centers-cos - inputs 65 H-ward - centers-cos - inputs H-ward - centers-cos - inputs
75
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Open-book (f1-score) CNN/DailyMail (rouge-2) XSum (rouge-2)
95
95
90
90
90
85 85
85
80 80
80
75 random 75 random random
H-cosine - centers-cos - inputs H-cosine - centers-cos - inputs H-cosine - centers-cos - inputs
70 H-ward - centers-cos - inputs 70 H-ward - centers-cos - inputs 75 H-ward - centers-cos - inputs
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
Figure14: Input-basedclusteringresults. Plotsdepictsuccessratesofmodelpreferenceestimation,aggregated
over666uniquemodelpairs. Eachpaneldepictsadifferentdataset.
Clustering of Difference Vectors (10 clusters) Clustering of Difference Vectors (50 clusters) Clustering of Difference Vectors (100 clusters)
1.4 1.4 1.4
1.2
1.2 1.2
1.0
1.0
1.0
0.8
0.8
0.8 0.6
0.6 0.4
0.6
0.4 0.2
0 20 40 60 80 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60
% of Examples in Cluster % of Examples in Cluster % of Examples in Cluster
Figure15: Clustersizevs. averagevectornorm. Theplotdescribestheresultsofhierarchicalclusteringofthe
differencevectors,fortheXSumdatasetwhenpartitioningintodifferentnumbersofclusters. Eachpointrepresents
asinglecluster;intotal,eachpaneldepictsbetween6.7Kand67Kpoints(666modelpairs×thenumberofclusters
perpair). Thex-axisreflectsthepercentageofallexamplesthatareinthecluster,andthey-axisistheaverage
vector norm within the cluster. The results are characterized by very large clusters with a small average norm
(bottomrightoftheplots).
)%(
etar
sseccuS
)%(
etar
sseccuS
retsulC
ni
mroN
egarevA
)%(
etar
sseccuS
)%(
etar
sseccuS
retsulC
ni
mroN
egarevA
)%(
etar
sseccuS
)%(
etar
sseccuS
retsulC
ni
mroN
egarevANarrativeQA (f1-score) QuAC (f1-score) NaturalQuestions - Closed-book (f1-score)
90 95
95
85 90
90 80 85
85 75 80
80 DiffUse 70 DiffUse 75 DiffUse
max_norm max_norm max_norm
Random 65 Random 70 Random
75
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
NaturalQuestions - Open-book (f1-score) CNN/DailyMail (rouge-2) XSum (rouge-2)
95
95
90
90 90
85
85
85 80
80
80
75 DiffUse 75 DiffUse DiffUse
max_norm max_norm max_norm
70 Random 70 Random 75 Random
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of annotated examples Number of annotated examples Number of annotated examples
Figure16: Max-normbaselineresults. Plotsdepictsuccessratesofmodelpreferenceestimation,aggregatedover
666uniquemodelpairs. Eachpaneldepictsadifferentdataset.
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS
)%(
etar
sseccuS