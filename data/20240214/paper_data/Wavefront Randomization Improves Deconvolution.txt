Wavefront Randomization Improves Deconvolution
Amit Kohli*, Anastasios N. Angelopoulos∗, Laura Waller
University of California, Berkeley
February 14, 2024
Abstract
The performance of an imaging system is limited by optical aberrations, which cause blurriness in
the resulting image. Digital correction techniques, such as deconvolution, have limited ability to correct
the blur, since some spatial frequencies in the scene are not measured adequately (i.e., ‘zeros’ of the
system transfer function). We prove that the addition of a random mask to an imaging system removes
itsdependenceonaberrations,reducingthelikelihoodofzerosinthetransferfunctionandconsequently
decreasing the sensitivity to noise during deconvolution. In simulation, we show that this strategy
improvesimagequalityoverarangeofaberrationtypes,aberrationstrengths,andsignal-to-noiseratios.
1 INTRODUCTION
Aberrations describe the deviations of an imaging system from ideal, diffraction-limited imaging. Even
well-designed optics have inherent aberrations; they are often the limiting factor in optical space-bandwidth
product. Correcting aberrations usually involves complex sequences of optical elements—like those in a
microscope objective—to achieve diffraction-limited imaging across the target field-of-view (FoV). Alterna-
tively,adaptiveopticalsystemsuseaprogrammablephasemodulatorinaclosed-looptodynamicallycorrect
aberratedwavefrontsinreal-time. Bothofthesehardware-basedsolutionsaregenerallyexpensiveandbulky.
For a simpler and less expensive alternative, many users turn to computational post-processing—for exam-
ple, deconvolution, whereby images captured with poorly-corrected optics are digitally processed to remove
aberration effects. Deconvolution requires knowing or measuring the system point spread function (PSF),
then implementing an image reconstruction algorithm to deconvolve it from the captured image. However,
deconvolution is limited in use as it often fails in the case of low-quality and/or noisy images.
This manuscript explores a new computational imaging approach to correcting aberrations via a simple
andinexpensivehardwaremodificationcombinedwithstandarddeconvolution. Givenanaberratedimaging
system,weshowthatwavefrontrandomization (e.g. byinsertingarandomphasemaskinthepupilplane)can
resultinimproveddeconvolution(seeFig.1). Itmaybesurprisingtothinkthataddingarandomscattering
element to the system could improve image quality. Indeed, the captured images from the randomized
system will initially look worse than the original aberrated images. However, randomization better encodes
thescene’sspatialfrequencyinformation—i.e., itimprovesitstransferfunction, resultinginahigher-quality
imageafterdeconvolution. Wavefrontrandomizationcanbeimplementedbyasimplephasemaskordiffuser
in the pupil plane, whose PSF can be experimentally measured for use in deconvolution.
Ourapproachisenabledbyanewdiscovery: addingauniformlyrandomphasemasktoanimagingsystem
makes it invariant to its initial aberrations. When a system is aberrated, it induces structured wavefront
distortionsthatoftencausezerosinthesystem’stransferfunction. However,whenthewavefrontdistortions
are uniformly randomized, they are no longer correlated with the original aberrations and thus lose their
structure, resulting in a transfer function with no zeros. Consequently, this random but improved transfer
function makes deconvolution more tolerant to noise, which would have normally overcome the signal null
frequencies.
∗equalcontribution
1
4202
beF
31
]VC.sc[
2v00970.2042:viXraMTF PSF Image Reconstruction
Figure 1: Simulation of a spherically aberrated imaging system with and without wavefront randomization.
With no randomization (top row) the system has an MTF with severe nulls and a large blob-like point
spread function (PSF). The image is a blurry, noisy version of the scene and the deconvolved image has
noise-inducedpatternedartifacts. Witharandommask(bottomrow),thewavefrontisrandomized,causing
the modulation transfer function (MTF) to become flatter, with no nulls. The corresponding point spread
function (PSF) is a speckle pattern with small features. The image is random, but the deconvolved image is
much closer to the ground truth. Noise is white, additive Gaussian.
Aberration correction methods using phase masks in conjunction with computation have been explored
beforeandcanbedividedintononrandomdesigns[1,2,3]whichoptimizeforaspecificaberrationtype,and
random designs which trade off optimality for robustness to unknown aberrations. These random methods
have performed aberration correction in the context of extending the depth of field [4], correcting sample-
inducedaberrations[5],heuristictransferfunctiondesign[6],stellarinterferometrywithlowqualityoptics[7],
andsparseapertureimaging[8]. Thismanuscriptprovidesrigorousstatisticalanalysesforrandomizedimag-
ing under arbitrary, unknown aberrations, accompanied by comprehensive simulations of transfer function
distributions/means and deconvolutions against aberration type and noise. The theorems we prove are new,
and give exact analytical expressions for the random transfer functions under consideration.
2 Background
For the purpose of convenience, although imaging systems are generally 2-dimensional, the theory in this
manuscript is done in a 1-dimensional discrete-time setting. All optical fields will be described as discrete,
periodic complex-valued sequences with period N. This mathematical setup is very common in the devel-
opment of imaging algorithms [9]. All sequences will henceforth be defined on {0,...,N −1}, with the
understanding that they can be naturally extended to Z by periodicity.
The pupil function
P =A eiϕn
n n
is composed of two real-valued sequences: the transmittance of the system aperture A , and the deviation
n
ϕ from an ideal wavefront. These ϕ are the undesirable, unknown aberrations that we wish to correct.
n n
Fortheremainderofthismanuscript, wewillsetA =0forn=⌊N⌋,...,N−1andA =1otherwise. The
n 2 n
modulation transfer function (MTF) H is the real-valued magnitude of the discrete autocorrelation of the
n
pupil function,
(cid:12) N−1 (cid:12) (cid:12) N−1 (cid:12)
H n =(cid:12) (cid:12) (cid:12)||P1 ||2 (cid:88) P mP m∗ −n(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12)||P1 ||2 (cid:88) A meiϕmA m−ne−iϕm−n(cid:12) (cid:12) (cid:12). (1)
n n
m=0 m=0
The MTF describes the amount of each spatially frequency in the scene captured by the system. Clearly,
the aberrations ϕ affect the value of the MTF. In fact, an application of the Cauchy-Schwarz inequality
n
2
ksam
oN
ksaMto (1) gives a pointwise upper bound on the MTF:
(cid:12) N−1 (cid:12) (cid:12) (cid:12)
(cid:12) 1 (cid:88) (cid:12) (cid:12) |n| (cid:12)
H n ≤(cid:12) (cid:12)||P ||2 A mA m−n(cid:12) (cid:12)=(cid:12) (cid:12)1− ⌊N/2⌋(cid:12) (cid:12).
n
m=0
Equalityoccurswhenϕ =0,or,inotherwords,aberrationscanonlyworsenadiffraction-limitedsystem.[1]
n
Our main tool will be the use of a phase mask to change the aberration profile. If W is a sequence
n
representing the phase profile of a phase mask, the pupil becomes
P˜ =A ei(ϕn+Wn), (2)
n n
andtheresultingMTFfollowsfromautocorrelationasper(1)(seeAppendixC).Thispaperisaboutchoosing
W strategicallyinordertoremovethedependenceofthetransferfunctionontheaberrations—anoutcome
n
we call aberration invariance.
The benefit of aberration invariance is to improve the effectiveness of digital post-processing. Although
theoreticallyexactrecoveryispossibleforanypositivetransferfunction,inreality,anyzerosornear-zerosin
theMTFarehighlysusceptibletonoise. Thesenoisyfrequenciesareboostedbytheinversefilter,leadingto
noticeable artifacts in the deconvolution. The top row of Fig. 1 shows an example of a spherical aberration
that pushes the MTF below the noise floor at several null frequencies, leading to a systematic, patterned
artifacts corresponding to those frequencies. Achieving aberration invariance will allow us to provably avoid
this type of deconvolution artifact.
3 Theory
We now describe how wavefront randomization via random masks provides aberration invariance.
3.1 Random Masks
Our main discovery is that aberration independence can be provably achieved by wavefront randomization,
without any knowledge about the aberrations whatsoever. Herein we do so by inserting a random phase mask
into the pupil plane with phase profile W , which is a real-valued random variable whose distribution we
n
can design.
The theoretical results in this section provide characterizations of the transformed MTF under two
differentmodelsforW : inthefirstandsimplestmodel,W ∼Unif(0,2π). Inthesecond,W ∼πBern(0.5).
n n n
Inbothcases,thetheorywillshowaberrationindependencearisingfrominsertingthemask. Figure2shows
accompanying simulations of the MTFs and supports the theoretical claims.
3.1.1 Uniform mask
In this section, we consider the case where W ∼Unif(0,2π). It will be immediately clear that the resulting
n
pupil function is entirely independent of aberrations and has a known and exact distribution.
Theorem 1 (Aberration invariance: uniform mask). Consider a masked pupil function P˜ as in (2) with
n
i.i.d.
arbitrary aberrations ϕ and W ∼ Unif(0,2π). Then,
n n
P˜ =d A eiWn
n n
and
1 (cid:12) (cid:12)
H =d (cid:12)1⊤ ei∆n(W)(cid:12),
n ⌊N/2⌋(cid:12) C(N,n) (cid:12)
where C(N,n) = ⌊N/2⌋−n and the ∆ : RN → RC(N,n) function computes the vector ∆ (w) = (w −
n n n
w ,...,w −w ,...,w −w ).
0 j j−n N−1 N−n−1
3No mask
Mask
Cycles per μm
Aberration severity
Figure 2: Simulation of MTFs with and without a uniform random mask. Each row represents a different
aberration type, and each column represents a different aberration strength. Within each individual plot
is the MTF of the system with no mask (red), the empirical distribution of MTFs from many draws of a
uniform mask (green), and the average MTF of those draws (blue). As expected by Theoreom 1, the MTF
distribution and average from uniform masks do not change with aberration type or strength, whereas the
MTF without a mask does so drastically. Also note how the MTF distribution is concentrated around the
average, signifying that the MTF is reliably null-free.
Intuitively, by uniformly randomly shifting the phase at each point in the pupil wavefront, the mask
makes it so that the wavefront itself is uniformly random, regardless of what the initial aberrations were.
The proof of this theorem relies on the unique fact that uniform random phasors are invariant to constant
shifts (this fact is also critical to the study of random speckle patterns [10]). Figure 2 confirms this theorem
andalsoshowsthattheresultingrandomMTFconcentratesarounditsmean,avoidingnullfrequencieswith
essentially probability one.
3.1.2 Binary mask
In this section, we consider the case where W ∼πBern(0.5), i.e. W is an (appropriately scaled) Bernoulli
n n
random variable with probability p = 0.5. This binary mask can be easily fabricated or represented with
an adaptive element like a deformable mirror. An exact characterization of the MTF is possible, but it is
aberration-dependent; nonetheless, a lower-bound on the expectation can be derived that is independent of
the aberrations.
Theorem 2 (Approximate aberration invariance: binary mask). Consider a pupil function P˜ as in (2)
n
i.i.d.
with arbitrary aberrations ϕ and W ∼ πBern(0.5). Then,
n
(cid:112) (cid:115)
C(N,n) 2a⊤U
H¯ =d 1+ ,
n ⌊N/2⌋ C(N,n)
where U is uniformly distributed on (cid:28) : the C(N,n)-dimensional hypercube with vertices {−1,1} along each
n
dimension and a is the C(N,n)-dimensional vector
a =(|cos(ϕ −ϕ −ϕ +ϕ )|),∀j ∈{n,...,N −1},k ∈{j,...,N −1}. (3)
jk j j−n k k−n
4
lacirehpS
sucofeD
msitamgitsA
edutilpma
goL
Probabilitya) b)
No mask
Mask
Noise power Signal-to-noise ratio
c) 0.90 0.76 0.62 d) 0.90 0.71 0.54
0.78 0.78 0.76 0.79 0.78 0.77
Sphere Astigmatism
Figure3: SimulationofimagereconstructionsandtheirSSIMscoresfordifferentnoiselevelsandaberration
strengths/types. a) Deconvolutionswith (bottom)andwithout(top)a uniform random maskforincreasing
noise power. The masked case degrades more slowly. b) A plot of SSIM scores on the deconvolution from
a) against SNR; the masked deconvolution degrades more gradually. c) Deconvolutions with and without a
mask for increasing levels of spherical aberration and d) astigmatism. Deconvolution with the mask is less
sensitive to noise for large aberrations. SSIM scores are shown on each image reconstruction.
Furthermore, the expectation of the MTF is
(cid:115) (cid:115)
E[H¯ ]=D(N,n) (cid:88) 1+ 2a⊤u ≥D(N,n) (cid:88) (cid:18) 1+ 21⊤U (cid:19) ,
n C(N,n) C(N,n)
u∈(cid:28)
n
u∈(cid:28)
n
+
where (·) + =x1{x≥0} and D(N,n)= √ 1 .
⌊N/2⌋2 C(N,n)(C(N,n)−1)
Above, the MTF is expressed as a function of the aberrations through the vector a and the random
variable U. The dependence is mild and the MTF is approximately independent of the aberrations (see
Appendix A). Furthermore, the expectation of the MTF is lower-bounded by a quantity that is aberration
invariant.
4 Imaging Simulations
The theoretical results in the previous section show that wavefront randomization can remove the MTF’s
dependence on aberrations. Moreover, the resulting random MTFs are concentrated and rarely have nulls,
makingthemamenabletopost-processing. Itremainstoperformdeconvolutionstodetermineifthisstrategy
actually improves image quality in the presence of noise.
To that end, we simulate imaging with and without a random mask for a variety of different conditions:
aberration types, aberration strengths, and signal-to-noise ratios. The simulation is done by first generating
5
ksam
oN
ksaM
ksam
oN
ksaM
msitamgitsA
MISSa pupil function with an aberration profile determined by Seidel aberration coefficients; the size of the
coefficients correspond to aberration strength. This aberrated pupil is then masked with a random mask—
drawn only once at the beginning of the experiment—generated by sampling from a uniform distribution.
Then, by Fourier transform of the pupil, we obtain the system point spread function (PSF) and convolve it
with the cameraman image to obtain the measurement. Finally, we add Gaussian noise to the measurement
and PSF, and use a Wiener filter to deconvolve the noisy image with the noisy PSF.
The simulation is repeated with and without the mask, using sphere and astigmatism as the base aber-
rations. The aberration strength and noise levels are varied from low to high. The results are displayed in
Fig.3alongwithSSIMscores. Themaintakeawayofthisexperimentisthatthereconstructionwithoutthe
mask is heavily dependent on the aberration-noise combination, whereas the reconstruction with the mask
is solely dependent on the noise, regardless of the aberration level. Thus, for a fixed noise level, the recon-
struction with the mask is nearly identical and has similar SSIM scores for all aberration levels and types.
This is in stark contrast to the reconstruction without the mask, which degrades severely with aberration
level.
5 Discussion
The primary contribution of this manuscript is the discovery that wavefront randomization can remove
the dependence of an imaging system on its aberrations. Specifically, by using a random pupil mask, the
transfer function of an aberrated system is transformed into a random transfer function whose distribution
is independent of the aberrations. Moreover, this random transfer function has desirable properties for use
indeconvolutionsuchasbeingconcentratedarounditsmeanandrarelyhavingzeros. Withinacertainnoise
regime, these random transfer functions allow for better deconvolution, even with severe aberrations.
The next logical step in this inquiry is to specify a practical imaging regime in which wavefront random-
ization is beneficial. Real-life experiments are needed to capture a variety of variables beyond the scope of
simple simulations and determine whether this method has utility.
On the theoretical side, there are still many open questions about the properties of the random transfer
functions and whether there are superior mask distributions for particular imaging conditions. Further,
providing analytic high-probability lower bounds on the MTF, would be of interest; since the exact MTF
distribution is known, it may also be possible to design better recovery algorithms by leveraging these
statistics in a nonparametric maximum likelihood model. The resampling and combination of multiple
randommasksinordertoimproveimagereconstructionisalsoaninterestingtopic. Additionally,assumption
ofshift-invarianceplayedamajorroleinboththeoryandsimulationssinceitprovidesthesimplerelationships
betweenthepupil,MTF,andPSF;itistruehoweverthatmanyhighly-aberratedsystemsareactuallyshift-
varying. But, by dividing a shift-varying system into isoplanatic patches—a common existing strategy—the
theory can be easily extended to shift-varying systems as well.
It is also worth noting that the theory in this manuscript is done in 1-dimensional discrete-time, but the
results can be extended to 2-dimensional and even continuous time in a straightforward manner. Finally,
we believe the randomization of other optical elements, and the analysis of their effect on the distribution
of the resulting reconstruction, is an important avenue for future research under the paradigm of wavefront
randomization.
ACKNOWLEDGMENTS
A.K. was funded by the Berkeley Fellowship for Graduate Study. A.N.A. was supported by the Berkeley
FellowshipforGraduateStudyandtheNationalScienceFoundationGraduateResearchFellowshipProgram
under Grant No. DGE 1752814. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily reflect the views of the National Science
Foundation. ThismaterialisbaseduponworksupportedbytheAirForceOfficeofScientificResearchunder
award number FA9550-22-1-0521. This publication has been made possible in part by CZI grant DAF2021-
225666 and grant DOI https://doi.org/10.37921/192752jrgbn from the Chan Zuckerberg Initiative DAF, an
advised fund of Silicon Valley Community Foundation (funder DOI 10.13039/100014989).
6References
[1] B. Roy Frieden. How well can a lens system transmit entropy? J. Opt. Soc. Am., 58(8):1105–1112,
1968.
[2] W.ThomasCathey,B.RoyFrieden,WilliamT.Rhodes,andCraigK.Rushforth. Imagegatheringand
processing for enhanced resolution. J. Opt. Soc. Am. A, 1(3):241–250, 1984.
[3] Andrew R Harvey, Gonzalo D Muyo, and Tom Vettenburg. Control of optical aberrations with coded
apertures. In Unconventional Imaging, Wavefront Sensing, and Adaptive Coded Aperture Imaging and
Non-Imaging Sensor Systems, volume 8165, pages 310–317. SPIE, 2011.
[4] Zeev Zalevsky and Alex Zlotnik. Axially and transversally super-resolved imaging and ranging with
random aperture coding. Journal of Optics A: Pure and Applied Optics, 10(6):064014, 2008.
[5] NizanMeitav,ErezNRibak,andShyShoham. Pointspreadfunctionestimationfromprojectedspeckle
illumination. Light: Science & Applications, 5(3):e16048–e16048, 2016.
[6] BryanJ.StosselandNicholasGeorge. Multiple-pointimpulseresponses: controlledblurringandrecov-
ery. Optics Communications, 121(4):156–165, 1995.
[7] J.C.Dainty. Diffraction-limitedimagingofstellarobjectsusingtelescopesoflowopticalquality. Optics
Communications, 7(2):129–134, 1973.
[8] Nicholas J. Miller, Matthew P. Dierking, and Bradley D. Duncan. Optical sparse aperture imaging.
Appl. Opt., 46(23):5933–5943, 2007.
[9] RichardGPaxman,TimothyJSchulz,andJamesRFienup. Jointestimationofobjectandaberrations
by using phase diversity. JOSA A, 9(7):1072–1085, 1992.
[10] Joseph W Goodman. Speckle phenomena in optics: theory and applications. Roberts and Company
Publishers, 2007.
A Binary mask
This section will expand on the theoretical development initialized in Sec. 3.1.2. Accompanying simulations
i.i.d.
of binary mask MTFs are displayed in Fig. 4 As a brief review, insertion of a mask, W ∼ πBern(0.5),
n
into the Fourier plane of an imaging system yields a MTF
(cid:112) (cid:115)
C(N,n) 2a⊤U
d
H = 1+ ,
n ⌊N/2⌋ C(N,n)
where C(N,n) = ⌊N/2⌋−n and a contains the aberrations as per Eq. (3). Though this quantity is not
completelyaberrationindependent,ithasaweakdependenceonaberrations. Thisisanempiricalobservation
illustrated in Fig. 4. There is also theoretical motivation for such a claim: the second moment of H¯ is
n
aberrationindependent. Toseethis,firstconsiderthe2ndmomentofH¯ foragenerallyBernoulliparameter
n
(not necessarily 0.5).
Theorem 3. Consider the pupil function P˜, which has been masked by a Bernoulli-p phase mask. Then
j
for n=0,...,⌊N/2⌋−1 its autocorrelation H has a 2nd moment
n
7No mask
Mask
Cycles per μm
Aberration severity
Figure 4: Simulation of MTFs with and without a binary random mask. Each row represents a different
aberration type, and each column represents a different aberration strength. Within each individual plot is
theMTFofthesystemwithnomask(red),theempiricaldistributionofMTFsfrommanydrawsofabinary
mask, and the average MTF of those draws (blue). The MTF distribution and average from binary masks
do change with aberration type and strength, but very little. Moreover, the binary mask MTF distribution
is concentrated around the average, signifying that the MTF is reliably null-free.
(cid:34)(cid:18) (cid:19)
1 n
E[H2]=1{n=0}+1{n>0} −
n ⌊N/2⌋ ⌊N/2⌋2
 
(cid:18) (1−2p)2(cid:19) ⌊N/2 (cid:88)⌋−n−1 ⌊N (cid:88)/2⌋−1
+  ei(2ϕj−ϕj−n−ϕj+n)+ ei(ϕj−2ϕj−n+ϕj−2n)
⌊N/2⌋2
j=n j=2n
(cid:18) (1−2p)4(cid:19)(cid:18)⌊N (cid:88)/2⌋−1
(cid:88)
(cid:19)(cid:35)
+ ei(ϕj−ϕj−n−ϕk+ϕk−n) .
⌊N/2⌋2
j=n k̸=j
k̸=j±n
Now, when p=0.5, we see that all aberration dependent terms vanish.
Corollary 1 (Aberration invariance: squared MTF of binary mask). Consider the setting of Theorem 3
with p=0.5. Then,
E(cid:2) H2(cid:3)
=
C(N,n)
.
n ⌊N/2⌋
The 2nd moment of H or the average squared MTF is a constant and independent of any aberrations,
n
signifying that the concentration properties of the MTF about its expectation are favorable.
B Proofs of Main Results
Belowareproofsofthetheoremsshowninthemainmanuscript,refertoAppendixCforsupportinglemmas.
8
lacirehpS
sucofeD
msitamgitsA
edutilpma
goL
ProbabilityProof of Theorem 1. The first statement directly follows from Lemma 2. By Lemma 1,
(cid:12) (cid:12)
(cid:12)N−1 (cid:12)
H n =
⌊N1 /2⌋(cid:12)
(cid:12)
(cid:12)(cid:88) ei(ϕj−ϕj−n+Wj−Wj−n)(cid:12)
(cid:12)
(cid:12)
(cid:12)j=n (cid:12)
where C(N,n)=⌊N/2⌋−n. Two applications of Lemma 2 gives
(cid:12) (cid:12)
(cid:12)N−1 (cid:12)
H n
=d ⌊N1 /2⌋(cid:12)
(cid:12)
(cid:12)(cid:88) ei(Wj−Wj−n)(cid:12)
(cid:12) (cid:12),
(cid:12)j=n (cid:12)
which is succinctly written in the theorem statement. This completes the proof.
Proof of Theorem 2. We only consider the MTF for n=1,...,⌊N⌋−1 since it is symmetric. We can write
2
the unnormalized MTF as
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)N−1 (cid:12) (cid:12)N−1 (cid:12) (cid:12)⌊N/2⌋−1 (cid:12)
H¯ n =(cid:12) (cid:12) (cid:12)(cid:88) P˜ jP˜ j∗ −n(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12)(cid:88) A2 nei(ϕj−ϕj−n)eiπBje−iπBj−n(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
(cid:88) ei(ϕj−ϕj−n)R jR j−n(cid:12) (cid:12) (cid:12),
(cid:12)j=0 (cid:12) (cid:12)j=0 (cid:12) (cid:12) j=n (cid:12)
where now the Bernoulli B random variables have probability 0.5 and by Lemma 3 the corresponding R
j j
are Rademacher random variables.
Thus, we can write the MTF as
(cid:118)
(cid:117)(cid:12) (cid:12)2
(cid:117)(cid:12)⌊N/2⌋−1 (cid:12)
H¯ n =(cid:117) (cid:116)(cid:12) (cid:12) (cid:88) ei(ϕj−ϕj−n)R jR j−n(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) j=n (cid:12)
(cid:118)
(cid:117)
(cid:117) ⌊N (cid:88)/2⌋−1 (cid:88)
=(cid:117) (cid:116)(⌊N/2⌋−n)+ R jR j−nR kR k−nei(ϕj−ϕj−n−ϕk+ϕk−n)
j=n k∈{n,...,j−1,j+1,⌊N/2⌋−1}
(cid:32)(cid:115) (cid:33)
(cid:112) 1
= C(N,n) 1+ Z ,
C(N,n)
whereC(N,n)=⌊N/2⌋−nandZ =(cid:80)⌊ jN =n/2⌋−1(cid:80) k∈{n,...,j−1,j+1,⌊N/2⌋−1}R jR j−nR kR k−nei(ϕj−ϕj−n−ϕk+ϕk−n).
LetthesummandsofZ beZ j,k =R jR j−nR kR k−nei∆ϕ j,k where∆ϕ j,k =ϕ j−ϕ j−n−ϕ k+ϕ k−n, anddefine
the index set
J ={(j,k):j ∈{n,...,⌊N/2⌋}} and k ∈{n,...,⌊N/2⌋}\{j}.
Consider the distribution of the summand with index (j,k) ∈ J, i.e., z . Because j ̸= k, R ⊥⊥ R , so
j,k j k
R R is equal in distribution to a Rademacher random variable. There are two further cases.
j k
(1) j̸=k−n and k̸=j−n In this case, R R R R ∼Rad because all are clearly independent.
j k j−n k−n
(2)j=k−nork=j−n Inthefirstsub-case,case,R R =1(deterministically),soR R R R =
j k−n j k j−n k−n
R R ∼Rad by independence. The second sub-case is similar.
k j−n
The conclusion of the above cases is that Z j,k
=d
R
j,kei∆ϕ
j,k for some Rademacher random variable R j,k.
What is the dependency structure of these Rademachers? The following cases illuminate the question:
(1) (j′,k′)=(k,j) . We have that R =R ; they are deterministically equal.
j,k k,j
9(2) j=k−n and not (1) It can be verified exhaustively that R | R ∼ Rad, which implies
j,k j′,k′
independence.
Knowing this dependence structure, we can re-express Z as
⌊N/2⌋−1
Z =d (cid:88) (cid:88) R j,k(cid:16) ei∆ϕ j,k +e−i∆ϕ j,k(cid:17) =2 (cid:88) R j,kcos(cid:16) ∆ϕ j,k(cid:17) .
j=n k>j (j,k)∈I>
Above,the>intheindexofthesumsandthesplittingoftheexponentialareduetothefactthatR =R
j,k k,j
andei∆ϕ
j,k
=e−i∆ϕ
k,j. TheindexsetI >isallpairsappearingintheearliersums;|I >|=C(N,n)(C(N,n)−1).
It is now worth noting that Z has gone from being complex-valued to real-valued, and that the R ∈ I
j,k >
are independent.
First, consider the case n=⌊N/2⌋−1. Here, Z a =.s. 0, which implies E[H¯ ]=(cid:112) C(N,n)=1.
n
Now consider a vector U uniformly distributed over the vertices of the hypercube
(cid:28)=±1 .
|I>|
We can write, by Lemma 4,
Z =d 2a⊤U,
(cid:16)(cid:12) (cid:12)(cid:17)
where a is the |I |-length vector (cid:12)cos(∆ϕ )(cid:12) . Thus, the full distribution of the (normalized) MTF
> (cid:12) j,k (cid:12)
√ (j,k)∈I>
(cid:113)
is H =d C(N,n) 1+ 2a⊤U , which proves the first theorem statement.
n ⌊N/2⌋ C(N,n)
(cid:104)(cid:113) (cid:105)
Now we are ready to calculate E 1+ Z manually. The expectation is equal to
C(N,n)
(cid:115) (cid:115)
(cid:88) 2a⊤u 1 (cid:88) 2a⊤u
1+ P(U =u)= 1+ .
C(N,n) 2|I>| C(N,n)
u∈(cid:28) u∈(cid:28)
As a sum of concave functions, this is concave (and non-constant) in a. Thus, its minimum occurs at a
corner point; but which one?
We can see that, for all q ∈[|I |], letting I−q =I \{q},
> > >
(cid:118) (cid:118) 
(cid:115) (cid:117) 2a⊤ u (cid:117) 2a⊤ u
(cid:88) 1+ 2a⊤u = (cid:88) (cid:117) (cid:116)1+ I >−q + 2a q +(cid:117) (cid:116)1+ I >−q − 2a q .
C(N,n)  C(N,n) C(N,n) C(N,n) C(N,n)
u∈(cid:28) u∈(cid:28)
I>−q
Each summand in the above display is of the form
(cid:115) (cid:115)
2a 2a
1+γ+ q + 1+γ− q .
C(N,n) C(N,n)
But the minimizing value of a over the domain [0,1], uniformly over all feasible values of γ, is a = 1.
q q
Similarly, the maximizer is a =0. (This can be verified by taking derivatives, or alternatively, by plotting
q
this function.) Thus,
(cid:118) (cid:118) 
(cid:117) 2a⊤ u (cid:117) 2a⊤ u
argmin (cid:88) (cid:117) (cid:116)1+ I >−q + 2a q +(cid:117) (cid:116)1+ I >−q − 2a q 
aq  C(N,n) C(N,n) C(N,n) C(N,n)
u∈(cid:28)
I>−q
(cid:115) (cid:115)
2a 2a
=argmin 1+γ+ q + 1+γ− q =1.
aq C(N,n) C(N,n)
This directly implies that a=1 minimizes the sum, since it is a feasible point.
|I>|
10In summary, the expected value of the normalized MTF can be calculated as
 √ (cid:113)
 C(N,n) (cid:80) 1+ 2a⊤u n=1,...,⌊N⌋−2
E[H ]= ⌊N/2⌋2|I>| u∈(cid:28) C(N,n) 2 .
n
 1 n=⌊N⌋−1
⌊N/2⌋ 2
It can furthermore be lower-bounded by
 √ (cid:114)(cid:16) (cid:17)
 C(N,n) (cid:80) 1+ 21⊤u n=1,...,⌊N⌋−2
E[H n]≥ ⌊N/2⌋2|I>| u∈(cid:28) C(N,n) + 2 ,
 1 n=⌊N⌋−1
⌊N/2⌋ 2
where (x) =x1{x≥0}.
+
Proof of Theorem 3. Recall our setting: we want to find the expected square magnitude of the autocorrela-
tion of
P˜ =A eiϕjeiπBj.
j j
Since the autocorrelation sequence is N periodic and symmetric about n = 0, we will solve it only for
n=0,...,⌊N⌋−1. For convenience, we will omit the square of the normalization factor 1 till the end.
2 ⌊N/2⌋
Writing out the unnormalizaed autocorrelation sequence for these values gives
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)N−1 (cid:12) (cid:12)N−1 (cid:12) (cid:12)⌊N/2⌋−1 (cid:12)
H¯ n =(cid:12) (cid:12) (cid:12)(cid:88) P˜ jP˜ j∗ −n(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12)(cid:88) A2 nei(ϕj−ϕj−n)eiπBje−iπBj−n(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
(cid:88) ei(ϕj−ϕj−n)R jR j−n(cid:12) (cid:12) (cid:12),
(cid:12)j=0 (cid:12) (cid:12)j=0 (cid:12) (cid:12) j=n (cid:12)
where the R
j
=eiπBj are i.i.d. as per Lemma 3. Next, we take the expected square of this expression to get
(cid:12) (cid:12)2
(cid:12)⌊N/2⌋−1 (cid:12) ⌊N/2⌋−1 ⌊N/2⌋−1
E(cid:12) (cid:12) (cid:88) ei(ϕj−ϕj−n)R jR j−n(cid:12) (cid:12) = (cid:88) E[(R jR j−n)2]+ (cid:88) (cid:88) E[R jR j−nR kR k−n]ei(ϕj−ϕj−n−ϕk+ϕk−n),
(cid:12) (cid:12)
(cid:12) j=n (cid:12) j=n j=n k̸=j
where we utilize the expanded form of a product of sums and apply linearity of expectation. Since each
R canonlybe1or−1,R R2 =1deterministically. Thus,E[(R R )2]=E[1]=1,andsothefirstterm
j j j−n j j−n
simplifies to
⌊N/2⌋−1
(cid:88)
E[(R R )2]=⌊N/2⌋−n.
j j−n
j=n
Now we will decompose the second term (the cross terms) into a variety of cases based on the sum
indices. Doingsowillallowustocomputealloftheexpectations. Notethat,thenormalizedautocorrelation
sequence is normalized by it’s value at n = 0 such that H˜ = 1. Consequently, we will only consider the
0
following cases when n>0 and remedy this in the final expression.
Case 1: k ̸= j ±n. In this case, none of the indices in the expectation overlap, which allows us to invoke
independenceandseparatetheterms. TheexpectationofR ,fromlemma3,isE[R ]=(1−p)−p=(1−2p).
j j
Combining these facts gives
E[R R R R ]=E[R ]E[R ]E[R ]E[R ]=(1−2p)4.
j j−n k k−n j j−n k k−n
Thus, the full expression of the second term for terms under this case becomes
⌊N/2⌋−1
(cid:88) (cid:88)
(1−2p)4 ei(ϕj−ϕj−n−ϕk+ϕk−n).
j=n k̸=j
k̸=j±n
Case 2: k =j+n. Considering only terms for which k =j+n, yields the following expression
⌊N/2⌋−n−1 ⌊N/2⌋−n−1
(cid:88) (cid:88)
E[R2]E[R R ]ei(2ϕj−ϕj−n−ϕj+n) =(1−2p)2 ei(2ϕj−ϕj−n−ϕj+n)
j j−n j+n
j=n j=n
11Case 3: k =j−n. Similarly, considering only terms for which k =j−n, yields the following expression
⌊N/2⌋−1 ⌊N/2⌋−1
(cid:88) (cid:88)
E[R2 ]E[R R ]ei(ϕj−2ϕj−n−ϕj−2n) =(1−2p)2 ei(ϕj−2ϕj−n−ϕj−2n)
j−n j j−2n
j=2n j=2n
Havingcoveredallcases, wecanassembleoursresultalongwiththesquarednormalizationfactor 1
⌊N/2⌋2
to arrive at the final expression.
C Technical lemmas
Lemma 1. In the setting of (2), we have that
(cid:12) (cid:12)
(cid:12)N−1 (cid:12)
H n =
⌊N1 /2⌋(cid:12)
(cid:12)
(cid:12)(cid:88) ei(ϕj−ϕj−n+Wj−Wj−n)(cid:12)
(cid:12) (cid:12).
(cid:12)j=n (cid:12)
Proof. This can be verified by directly plugging into (1).
Lemma 2 (Translationinvarianceofuniformphasors.). Let U ∼Unif(0,2π) and consider any ϕ∈R. Then
ei(ϕ+U) =d eiU.
Proof. Define U′ ∼Unif(ϕ,2π+ϕ). Note that U′ mod 2π =d U. Thus ei(ϕ+U) =d ei(U′ mod(2π)) =d eiU.
Lemma 3. Let B be a Bernoulli random variable with probability parameter p. Then random variable
R=∆ eiπB =e−iπB is specified by
(cid:40)
1 w.p. 1−p
R=
−1 w.p. p
Note that R here is a Rademacher random variable when p=0.5.
Proof. SinceB isBernoulli,itcanonlybeeither1or0. Inthecasethatitis1,R=eiπB =eiπ =−1,which
happens with probability p. In the case that it is 0, R = eiπB = e0 = 1, which happens with probability
1−p.
 
R R
n 0
 R n+1R 1 
Lemma4. LetR 0,...,R M bejointlyindependentRademacherrandomvariables. Thenforn>0  . .  
 . 
R R
M M−n
is uniformly distributed on the hypercube.
 
R R
n 0
 R n+1R 1 
Proof. For convenience, set M =⌊N/2⌋−1. Let U = . . We will proceed by induction on M for
 . 
 . 
R R
M M−n
n=1,...,M−1. Thecaseofn=mfollowssinceU willonlyhaveasingleelementR R whichisuniform
M 0
on the hypercube because P(R R =±1)=0.5. For the base case, let M =2, then the only relevant case
M 0
(cid:20) (cid:21)
R R
is n=1 for which U = 1 0 . Note that
R R
2 1
P(U =u)=P(U =u |U =u )P(U =u )=P(R R =u |R R =u )P(R R =u )
2 2 1 1 1 1 2 1 2 1 0 1 1 0 1
=P(R R =u |R R =u )0.5.
2 1 2 1 0 1
Now R R will only take values 1 or −1 and it will do so with equal probability regardless of the value
2 1
of R R because R is unaffected. Thus P(R R = ±1 | R R = u )0.5 = 0.52 and U is uniform on the
2 1 2 2 1 1 0 1
hypercube.
12Now let’s assume that for an arbitrary M and n∈{1,...M −1} we have
P(U =u )=P(R R =u )P(R +1R =u )...P(R R =U )=0.5M−n−1.
1:M−n−1 1:M−n−1 n 0 1 n 1 2 M−1 M−n−1 M−n−1
Then, using the law of total probability, we can write
P(U =u)=P(U =u |U =u )P(U =u )
M−n M−n 1:M−n−1 1:M−n−1 1:M−n−1 1:M−n−1
=P(R R =u |U =u )0.5M−n−1.
M M−n M−n 1:M−n−1 1:M−n−1
Now by a similar logic as the base case, fixing all of the elements of U will not effect the outcome of
1:M−n−1
R . Thus we have
M
P(R R =±1|U =u )=0.5=P(R R =±1),
M M−n 1:M−n−1 1:M−n−1 M M−n
and so
P(R R =u |U =u )0.5M−n−1 =0.5M−n
M M−n M−n 1:M−n−1 1:M−n−1
which means that that U is uniform on the hypercube.
13