[
    {
        "title": "Innovation Resistance Theory in Action: Unveiling Barriers to Open Government Data Adoption by Public Organizations to Unlock Open Data Innovation",
        "authors": "Anastasija NikiforovaAntoine ClarinvalAnneke ZuiderwijkDaniel RudmarkPetar MilicKatrin Rajamäe-Soosaar",
        "links": "http://arxiv.org/abs/2407.10883v1",
        "entry_id": "http://arxiv.org/abs/2407.10883v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10883v1",
        "summary": "Open Government Data (OGD) plays a pivotal role in fostering data-driven\ninnovation and sustainability across various sectors. Despite its potential,\nmany public organizations are reluctant to share their data openly. While\nexisting research has explored factors impacting the public organizations\nintention to share OGD, there is a paucity of research applying theoretical\nmodels to investigate the resistance by public organizations to making\ngovernment data publicly available. This study addresses the gap by developing\nan Innovation Resistance Theory (IRT) model tailored to OGD that allows\nidentifying predictors of resistance among public agencies. We develop an\ninitial model based on literature and refine it through interviews with 21\npublic agencies across six countries. The final model describes 39 barriers\nrelated to usage, value, risks, tradition, and image. The findings contribute\nto the literature by adapting IRT to the context of OGD, an area where its\napplication has been notably limited. As such, this study addresses the growing\ndemand for novel theoretical frameworks to examine OGD adoption barriers.\nPractical insights are provided to support policymakers in creating data\necosystems that encourage data openness and address challenges in OGD adoption.",
        "updated": "2024-07-15 16:35:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10883v1"
    },
    {
        "title": "Random Channel Ablation for Robust Hand Gesture Classification with Multimodal Biosignals",
        "authors": "Keshav BimbrawJing LiuYe WangToshiaki Koike-Akino",
        "links": "http://arxiv.org/abs/2407.10874v1",
        "entry_id": "http://arxiv.org/abs/2407.10874v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10874v1",
        "summary": "Biosignal-based hand gesture classification is an important component of\neffective human-machine interaction. For multimodal biosignal sensing, the\nmodalities often face data loss due to missing channels in the data which can\nadversely affect the gesture classification performance. To make the\nclassifiers robust to missing channels in the data, this paper proposes using\nRandom Channel Ablation (RChA) during the training process. Ultrasound and\nforce myography (FMG) data were acquired from the forearm for 12 hand gestures\nover 2 subjects. The resulting multimodal data had 16 total channels, 8 for\neach modality. The proposed method was applied to convolutional neural network\narchitecture, and compared with baseline, imputation, and oracle methods. Using\n5-fold cross-validation for the two subjects, on average, 12.2% and 24.5%\nimprovement was observed for gesture classification with up to 4 and 8 missing\nchannels respectively compared to the baseline. Notably, the proposed method is\nalso robust to an increase in the number of missing channels compared to other\nmethods. These results show the efficacy of using random channel ablation to\nimprove classifier robustness for multimodal and multi-channel biosignal-based\nhand gesture classification.",
        "updated": "2024-07-15 16:23:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10874v1"
    },
    {
        "title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM",
        "authors": "Keshav BimbrawYe WangJing LiuToshiaki Koike-Akino",
        "links": "http://arxiv.org/abs/2407.10870v1",
        "entry_id": "http://arxiv.org/abs/2407.10870v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10870v1",
        "summary": "Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.",
        "updated": "2024-07-15 16:18:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10870v1"
    },
    {
        "title": "Interactive Public Transport Infrastructure Analysis through Mobility Profiles: Making the Mobility Transition Transparent",
        "authors": "Yannick MetzDennis AckermannDaniel A. KeimMaximilian T. Fischer",
        "links": "http://arxiv.org/abs/2407.10791v1",
        "entry_id": "http://arxiv.org/abs/2407.10791v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10791v1",
        "summary": "Efficient public transport systems are crucial for sustainable urban\ndevelopment as cities face increasing mobility demands. Yet, many public\ntransport networks struggle to meet diverse user needs due to historical\ndevelopment, urban constraints, and financial limitations. Traditionally,\nplanning of transport network structure is often based on limited surveys,\nexpert opinions, or partial usage statistics. This provides an incomplete basis\nfor decision-making. We introduce an data-driven approach to public transport\nplanning and optimization, calculating detailed accessibility measures at the\nindividual housing level. Our visual analytics workflow combines\npopulation-group-based simulations with dynamic infrastructure analysis,\nutilizing a scenario-based model to simulate daily travel patterns of varied\ndemographic groups, including schoolchildren, students, workers, and\npensioners. These population groups, each with unique mobility requirements and\nroutines, interact with the transport system under different scenarios\ntraveling to and from Points of Interest (POI), assessed through travel time\ncalculations. Results are visualized through heatmaps, density maps, and\nnetwork overlays, as well as detailed statistics. Our system allows us to\nanalyze both the underlying data and simulation results on multiple levels of\ngranularity, delivering both broad insights and granular details. Case studies\nwith the city of Konstanz, Germany reveal key areas where public transport does\nnot meet specific needs, confirmed through a formative user study. Due to the\nhigh cost of changing legacy networks, our analysis facilitates the\nidentification of strategic enhancements, such as optimized schedules or\nrerouting, and few targeted stop relocations, highlighting consequential\nvariations in accessibility to pinpointing critical service gaps.",
        "updated": "2024-07-15 15:09:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10791v1"
    },
    {
        "title": "XEQ Scale for Evaluating XAI Experience Quality Grounded in Psychometric Theory",
        "authors": "Anjana WijekoonNirmalie WiratungaDavid CorsarKyle MartinIkechukwu Nkisi-OrjiBelen Díaz-AgudoDerek Bridge",
        "links": "http://arxiv.org/abs/2407.10662v1",
        "entry_id": "http://arxiv.org/abs/2407.10662v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10662v1",
        "summary": "Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users' need for holistic \"multi-shot\" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced \"Seek\"\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.",
        "updated": "2024-07-15 12:25:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10662v1"
    }
]