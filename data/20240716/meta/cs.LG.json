[
    {
        "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
        "authors": "Bocheng ZouMu CaiJianrui ZhangYong Jae Lee",
        "links": "http://arxiv.org/abs/2407.10972v1",
        "entry_id": "http://arxiv.org/abs/2407.10972v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10972v1",
        "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.",
        "updated": "2024-07-15 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10972v1"
    },
    {
        "title": "Walking the Values in Bayesian Inverse Reinforcement Learning",
        "authors": "Ondrej BajgarAlessandro AbateKonstantinos GatsisMichael A. Osborne",
        "links": "http://arxiv.org/abs/2407.10971v1",
        "entry_id": "http://arxiv.org/abs/2407.10971v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10971v1",
        "summary": "The goal of Bayesian inverse reinforcement learning (IRL) is recovering a\nposterior distribution over reward functions using a set of demonstrations from\nan expert optimizing for a reward unknown to the learner. The resulting\nposterior over rewards can then be used to synthesize an apprentice policy that\nperforms well on the same or a similar task. A key challenge in Bayesian IRL is\nbridging the computational gap between the hypothesis space of possible rewards\nand the likelihood, often defined in terms of Q values: vanilla Bayesian IRL\nneeds to solve the costly forward planning problem - going from rewards to the\nQ values - at every step of the algorithm, which may need to be done thousands\nof times. We propose to solve this by a simple change: instead of focusing on\nprimarily sampling in the space of rewards, we can focus on primarily working\nin the space of Q-values, since the computation required to go from Q-values to\nreward is radically cheaper. Furthermore, this reversion of the computation\nmakes it easy to compute the gradient allowing efficient sampling using\nHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlo\nmethod based on this insight - and illustrate its advantages on several tasks.",
        "updated": "2024-07-15 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10971v1"
    },
    {
        "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
        "authors": "Hongyu WangShuming MaRuiping WangFuru Wei",
        "links": "http://arxiv.org/abs/2407.10969v1",
        "entry_id": "http://arxiv.org/abs/2407.10969v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10969v1",
        "summary": "We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. The key results from this\nwork are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs\nwhile being much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.",
        "updated": "2024-07-15 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10969v1"
    },
    {
        "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning",
        "authors": "Haohong LinWenhao DingJian ChenLaixi ShiJiacheng ZhuBo LiDing Zhao",
        "links": "http://arxiv.org/abs/2407.10967v1",
        "entry_id": "http://arxiv.org/abs/2407.10967v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10967v1",
        "summary": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.",
        "updated": "2024-07-15 17:59:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10967v1"
    },
    {
        "title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations",
        "authors": "Walter SimonciniSpyros GidarisAndrei BursucYuki M. Asano",
        "links": "http://arxiv.org/abs/2407.10964v1",
        "entry_id": "http://arxiv.org/abs/2407.10964v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10964v1",
        "summary": "This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of vision encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These are\nprojected to a lower dimension and then concatenated with the model's\nembedding. The resulting features are evaluated on k-nearest neighbor\nclassification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification and image retrieval, and that they significantly\nimprove the retrieval-based in-context scene understanding abilities of\npretrained models, for example improving upon DINO by +17% for semantic\nsegmentation - without any training.",
        "updated": "2024-07-15 17:58:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10964v1"
    }
]