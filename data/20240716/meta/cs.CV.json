[
    {
        "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
        "authors": "Bocheng ZouMu CaiJianrui ZhangYong Jae Lee",
        "links": "http://arxiv.org/abs/2407.10972v1",
        "entry_id": "http://arxiv.org/abs/2407.10972v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10972v1",
        "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.",
        "updated": "2024-07-15 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10972v1"
    },
    {
        "title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations",
        "authors": "Walter SimonciniSpyros GidarisAndrei BursucYuki M. Asano",
        "links": "http://arxiv.org/abs/2407.10964v1",
        "entry_id": "http://arxiv.org/abs/2407.10964v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10964v1",
        "summary": "This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of vision encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These are\nprojected to a lower dimension and then concatenated with the model's\nembedding. The resulting features are evaluated on k-nearest neighbor\nclassification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification and image retrieval, and that they significantly\nimprove the retrieval-based in-context scene understanding abilities of\npretrained models, for example improving upon DINO by +17% for semantic\nsegmentation - without any training.",
        "updated": "2024-07-15 17:58:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10964v1"
    },
    {
        "title": "InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models",
        "authors": "Nirat SainiNavaneeth BodlaAshish ShrivastavaAvinash RavichandranXiao ZhangAbhinav ShrivastavaBharat Singh",
        "links": "http://arxiv.org/abs/2407.10958v1",
        "entry_id": "http://arxiv.org/abs/2407.10958v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10958v1",
        "summary": "We introduce InVi, an approach for inserting or replacing objects within\nvideos (referred to as inpainting) using off-the-shelf, text-to-image latent\ndiffusion models. InVi targets controlled manipulation of objects and blending\nthem seamlessly into a background video unlike existing video editing methods\nthat focus on comprehensive re-styling or entire scene alterations. To achieve\nthis goal, we tackle two key challenges. Firstly, for high quality control and\nblending, we employ a two-step process involving inpainting and matching. This\nprocess begins with inserting the object into a single frame using a\nControlNet-based inpainting diffusion model, and then generating subsequent\nframes conditioned on features from an inpainted frame as an anchor to minimize\nthe domain gap between the background and the object. Secondly, to ensure\ntemporal coherence, we replace the diffusion model's self-attention layers with\nextended-attention layers. The anchor frame features serve as the keys and\nvalues for these layers, enhancing consistency across frames. Our approach\nremoves the need for video-specific fine-tuning, presenting an efficient and\nadaptable solution. Experimental results demonstrate that InVi achieves\nrealistic object insertion with consistent blending and coherence across\nframes, outperforming existing methods.",
        "updated": "2024-07-15 17:55:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10958v1"
    },
    {
        "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
        "authors": "Yaoting WangPeiwen SunDongzhan ZhouGuangyao LiHonggang ZhangDi Hu",
        "links": "http://arxiv.org/abs/2407.10957v1",
        "entry_id": "http://arxiv.org/abs/2407.10957v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10957v1",
        "summary": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.",
        "updated": "2024-07-15 17:54:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10957v1"
    },
    {
        "title": "Can Textual Semantics Mitigate Sounding Object Segmentation Preference?",
        "authors": "Yaoting WangPeiwen SunYuanchao LiHonggang ZhangDi Hu",
        "links": "http://arxiv.org/abs/2407.10947v1",
        "entry_id": "http://arxiv.org/abs/2407.10947v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10947v1",
        "summary": "The Audio-Visual Segmentation (AVS) task aims to segment sounding objects in\nthe visual space using audio cues. However, in this work, it is recognized that\nprevious AVS methods show a heavy reliance on detrimental segmentation\npreferences related to audible objects, rather than precise audio guidance. We\nargue that the primary reason is that audio lacks robust semantics compared to\nvision, especially in multi-source sounding scenes, resulting in weak audio\nguidance over the visual space. Motivated by the the fact that text modality is\nwell explored and contains rich abstract semantics, we propose leveraging text\ncues from the visual scene to enhance audio guidance with the semantics\ninherent in text. Our approach begins by obtaining scene descriptions through\nan off-the-shelf image captioner and prompting a frozen large language model to\ndeduce potential sounding objects as text cues. Subsequently, we introduce a\nnovel semantics-driven audio modeling module with a dynamic mask to integrate\naudio features with text cues, leading to representative sounding object\nfeatures. These features not only encompass audio cues but also possess vivid\nsemantics, providing clearer guidance in the visual space. Experimental results\non AVS benchmarks validate that our method exhibits enhanced sensitivity to\naudio when aided by text cues, achieving highly competitive performance on all\nthree subsets. Project page:\n\\href{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}",
        "updated": "2024-07-15 17:45:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10947v1"
    }
]