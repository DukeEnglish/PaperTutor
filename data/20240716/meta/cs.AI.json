[
    {
        "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
        "authors": "Yongyuan LiangTingqiang XuKaizhe HuGuangqi JiangFurong HuangHuazhe Xu",
        "links": "http://arxiv.org/abs/2407.10973v1",
        "entry_id": "http://arxiv.org/abs/2407.10973v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10973v1",
        "summary": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks.",
        "updated": "2024-07-15 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10973v1"
    },
    {
        "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
        "authors": "Bocheng ZouMu CaiJianrui ZhangYong Jae Lee",
        "links": "http://arxiv.org/abs/2407.10972v1",
        "entry_id": "http://arxiv.org/abs/2407.10972v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10972v1",
        "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.",
        "updated": "2024-07-15 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10972v1"
    },
    {
        "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
        "authors": "Yaoting WangPeiwen SunDongzhan ZhouGuangyao LiHonggang ZhangDi Hu",
        "links": "http://arxiv.org/abs/2407.10957v1",
        "entry_id": "http://arxiv.org/abs/2407.10957v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10957v1",
        "summary": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.",
        "updated": "2024-07-15 17:54:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10957v1"
    },
    {
        "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
        "authors": "Ruisheng CaoFangyu LeiHaoyuan WuJixuan ChenYeqiao FuHongcheng GaoXinzhuang XiongHanchong ZhangYuchen MaoWenjing HuTianbao XieHongshen XuDanyang ZhangSida WangRuoxi SunPengcheng YinCaiming XiongAnsong NiQian LiuVictor ZhongLu ChenKai YuTao Yu",
        "links": "http://arxiv.org/abs/2407.10956v1",
        "entry_id": "http://arxiv.org/abs/2407.10956v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10956v1",
        "summary": "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.",
        "updated": "2024-07-15 17:54:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10956v1"
    },
    {
        "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
        "authors": "Dilara SoyluChristopher PottsOmar Khattab",
        "links": "http://arxiv.org/abs/2407.10930v1",
        "entry_id": "http://arxiv.org/abs/2407.10930v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10930v1",
        "summary": "Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai",
        "updated": "2024-07-15 17:30:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10930v1"
    }
]