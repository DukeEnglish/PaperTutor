[
    {
        "title": "A unified theory and statistical learning approach for traffic conflict detection",
        "authors": "Yiru JiaoSimeon C. CalvertSander van CranenburghHans van Lint",
        "links": "http://arxiv.org/abs/2407.10959v1",
        "entry_id": "http://arxiv.org/abs/2407.10959v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10959v1",
        "summary": "This study proposes a unified theory and statistical learning approach for\ntraffic conflict detection, addressing the long-existing call for a consistent\nand comprehensive methodology to evaluate the collision risk emerged in road\nuser interactions. The proposed theory assumes a context-dependent\nprobabilistic collision risk and frames conflict detection as estimating the\nrisk by statistical learning from observed proximities and contextual\nvariables. Three primary tasks are integrated: representing interaction context\nfrom selected observables, inferring proximity distributions in different\ncontexts, and applying extreme value theory to relate conflict intensity with\nconflict probability. As a result, this methodology is adaptable to various\nroad users and interaction scenarios, enhancing its applicability without the\nneed for pre-labelled conflict data. Demonstration experiments are executed\nusing real-world trajectory data, with the unified metric trained on\nlane-changing interactions on German highways and applied to near-crash events\nfrom the 100-Car Naturalistic Driving Study in the U.S. The experiments\ndemonstrate the methodology's ability to provide effective collision warnings,\ngeneralise across different datasets and traffic environments, cover a broad\nrange of conflicts, and deliver a long-tailed distribution of conflict\nintensity. This study contributes to traffic safety by offering a consistent\nand explainable methodology for conflict detection applicable across various\nscenarios. Its societal implications include enhanced safety evaluations of\ntraffic infrastructures, more effective collision warning systems for\nautonomous and driving assistance systems, and a deeper understanding of road\nuser behaviour in different traffic conditions, contributing to a potential\nreduction in accident rates and improving overall traffic safety.",
        "updated": "2024-07-15 17:55:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10959v1"
    },
    {
        "title": "Enhancing Stochastic Optimization for Statistical Efficiency Using ROOT-SGD with Diminishing Stepsize",
        "authors": "Tong ZhangChris Junchi Li",
        "links": "http://arxiv.org/abs/2407.10955v1",
        "entry_id": "http://arxiv.org/abs/2407.10955v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10955v1",
        "summary": "In this paper, we revisit \\textsf{ROOT-SGD}, an innovative method for\nstochastic optimization to bridge the gap between stochastic optimization and\nstatistical efficiency. The proposed method enhances the performance and\nreliability of \\textsf{ROOT-SGD} by integrating a carefully designed\n\\emph{diminishing stepsize strategy}. This approach addresses key challenges in\noptimization, providing robust theoretical guarantees and practical benefits.\nOur analysis demonstrates that \\textsf{ROOT-SGD} with diminishing achieves\noptimal convergence rates while maintaining computational efficiency. By\ndynamically adjusting the learning rate, \\textsf{ROOT-SGD} ensures improved\nstability and precision throughout the optimization process. The findings of\nthis study offer valuable insights for developing advanced optimization\nalgorithms that are both efficient and statistically robust.",
        "updated": "2024-07-15 17:54:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10955v1"
    },
    {
        "title": "SLIP: Securing LLMs IP Using Weights Decomposition",
        "authors": "Yehonathan RefaelAdam HakimLev GreenbergTal AvivSatya LokamBen FishmanShachar Seidman",
        "links": "http://arxiv.org/abs/2407.10886v1",
        "entry_id": "http://arxiv.org/abs/2407.10886v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10886v1",
        "summary": "Large language models (LLMs) have recently seen widespread adoption, in both\nacademia and industry. As these models grow, they become valuable intellectual\nproperty (IP), reflecting enormous investments by their owners. Moreover, the\nhigh cost of cloud-based deployment has driven interest towards deployment to\nedge devices, yet this risks exposing valuable parameters to theft and\nunauthorized use. Current methods to protect models' IP on the edge have\nlimitations in terms of practicality, loss in accuracy, or suitability to\nrequirements. In this paper, we introduce a novel hybrid inference algorithm,\nnamed SLIP, designed to protect edge-deployed models from theft. SLIP is the\nfirst hybrid protocol that is both practical for real-world applications and\nprovably secure, while having zero accuracy degradation and minimal impact on\nlatency. It involves partitioning the model between two computing resources,\none secure but expensive, and another cost-effective but vulnerable. This is\nachieved through matrix decomposition, ensuring that the secure resource\nretains a maximally sensitive portion of the model's IP while performing a\nminimal amount of computations, and vice versa for the vulnerable resource.\nImportantly, the protocol includes security guarantees that prevent attackers\nfrom exploiting the partition to infer the secured information. Finally, we\npresent experimental results that show the robustness and effectiveness of our\nmethod, positioning it as a compelling solution for protecting LLMs.",
        "updated": "2024-07-15 16:37:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10886v1"
    },
    {
        "title": "Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data",
        "authors": "Victor Churchill",
        "links": "http://arxiv.org/abs/2407.10854v1",
        "entry_id": "http://arxiv.org/abs/2407.10854v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10854v1",
        "summary": "We present a computational technique for modeling the evolution of dynamical\nsystems in a reduced basis, with a focus on the challenging problem of modeling\npartially-observed partial differential equations (PDEs) on high-dimensional\nnon-uniform grids. We address limitations of previous work on data-driven flow\nmap learning in the sense that we focus on noisy and limited data to move\ntoward data collection scenarios in real-world applications. Leveraging recent\nwork on modeling PDEs in modal and nodal spaces, we present a neural network\nstructure that is suitable for PDE modeling with noisy and limited data\navailable only on a subset of the state variables or computational domain. In\nparticular, spatial grid-point measurements are reduced using a learned linear\ntransformation, after which the dynamics are learned in this reduced basis\nbefore being transformed back out to the nodal space. This approach yields a\ndrastically reduced parameterization of the neural network compared with\nprevious flow map models for nodal space learning. This primarily allows for\nsmaller training data sets, but also enables reduced training times.",
        "updated": "2024-07-15 16:06:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10854v1"
    },
    {
        "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
        "authors": "Changhun KimTaewon KimSeungyeon WooJune Yong YangEunho Yang",
        "links": "http://arxiv.org/abs/2407.10784v1",
        "entry_id": "http://arxiv.org/abs/2407.10784v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10784v1",
        "summary": "In real-world applications, tabular data often suffer from distribution\nshifts due to their widespread and abundant nature, leading to erroneous\npredictions of pre-trained machine learning models. However, addressing such\ndistribution shifts in the tabular domain has been relatively underexplored due\nto unique challenges such as varying attributes and dataset sizes, as well as\nthe limited representation learning capabilities of deep learning models for\ntabular data. Particularly, with the recent promising paradigm of test-time\nadaptation (TTA), where we adapt the off-the-shelf model to the unlabeled\ntarget domain during the inference phase without accessing the source domain,\nwe observe that directly adopting commonly used TTA methods from other domains\noften leads to model collapse. We systematically explore challenges in tabular\ndata test-time adaptation, including skewed entropy, complex latent space\ndecision boundaries, confidence calibration issues with both overconfident and\nunder-confident, and model bias towards source label distributions along with\nclass imbalances. Based on these insights, we introduce AdapTable, a novel\ntabular test-time adaptation method that directly modifies output probabilities\nby estimating target label distributions and adjusting initial probabilities\nbased on calibrated uncertainty. Extensive experiments on both natural\ndistribution shifts and synthetic corruptions demonstrate the adaptation\nefficacy of the proposed method.",
        "updated": "2024-07-15 15:02:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10784v1"
    }
]