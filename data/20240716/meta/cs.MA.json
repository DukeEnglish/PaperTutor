[
    {
        "title": "GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic Signal Control Agents",
        "authors": "Haoyuan JiangXuantang XiongZiyue LiHangyu MaoGuanghu SuiJingqing RuanYuheng ChengHua WeiWolfgang KetterRui Zhao",
        "links": "http://arxiv.org/abs/2407.10811v1",
        "entry_id": "http://arxiv.org/abs/2407.10811v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10811v1",
        "summary": "Currently, traffic signal control (TSC) methods based on reinforcement\nlearning (RL) have proven superior to traditional methods. However, most RL\nmethods face difficulties when applied in the real world due to three factors:\ninput, output, and the cycle-flow relation. The industry's observable input is\nmuch more limited than simulation-based RL methods. For real-world solutions,\nonly flow can be reliably collected, whereas common RL methods need more. For\nthe output action, most RL methods focus on acyclic control, which real-world\nsignal controllers do not support. Most importantly, industry standards require\na consistent cycle-flow relationship: non-decreasing and different response\nstrategies for low, medium, and high-level flows, which is ignored by the RL\nmethods. To narrow the gap between RL methods and industry standards, we\ninnovatively propose to use industry solutions to guide the RL agent.\nSpecifically, we design behavior cloning and curriculum learning to guide the\nagent to mimic and meet industry requirements and, at the same time, leverage\nthe power of exploration and exploitation in RL for better performance. We\ntheoretically prove that such guidance can largely decrease the sample\ncomplexity to polynomials in the horizon when searching for an optimal policy.\nOur rigid experiments show that our method has good cycle-flow relation and\nsuperior performance.",
        "updated": "2024-07-15 15:26:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10811v1"
    },
    {
        "title": "Communication- and Computation-Efficient Distributed Decision-Making in Multi-Robot Networks",
        "authors": "Zirui XuSandilya Sai GarimellaVasileios Tzoumas",
        "links": "http://arxiv.org/abs/2407.10382v1",
        "entry_id": "http://arxiv.org/abs/2407.10382v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10382v1",
        "summary": "We provide a distributed coordination paradigm that enables scalable and\nnear-optimal joint motion planning among multiple robots. Our coordination\nparadigm contrasts with current paradigms that are either near-optimal but\nimpractical for replanning times or real-time but offer no near-optimality\nguarantees. We are motivated by the future of collaborative mobile autonomy,\nwhere distributed teams of robots will coordinate via vehicle-to-vehicle (v2v)\ncommunication to execute information-heavy tasks like mapping, surveillance,\nand target tracking. To enable rapid distributed coordination, we must curtail\nthe explosion of information-sharing across the network, thus limiting robot\ncoordination. However, this can lead to suboptimal plans, causing overlapping\ntrajectories instead of complementary ones. We make theoretical and algorithmic\ncontributions to balance the trade-off between decision speed and optimality.\nWe introduce tools for distributed submodular optimization, a diminishing\nreturns property in information-gathering tasks. Theoretically, we analyze how\nlocal network topology affects near-optimality at the global level.\nAlgorithmically, we provide a communication- and computation-efficient\ncoordination algorithm for agents to balance the trade-off. Our algorithm is up\nto two orders faster than competitive near-optimal algorithms. In simulations\nof surveillance tasks with up to 45 robots, it enables real-time planning at\nthe order of 1 Hz with superior coverage performance. To enable the\nsimulations, we provide a high-fidelity simulator that extends AirSim by\nintegrating a collaborative autonomy pipeline and simulating v2v communication\ndelays.",
        "updated": "2024-07-15 01:25:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10382v1"
    },
    {
        "title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding",
        "authors": "Chang LeiHuan Lei",
        "links": "http://arxiv.org/abs/2407.10279v1",
        "entry_id": "http://arxiv.org/abs/2407.10279v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10279v1",
        "summary": "Artificial intelligence for card games has long been a popular topic in AI\nresearch. In recent years, complex card games like Mahjong and Texas Hold'em\nhave been solved, with corresponding AI programs reaching the level of human\nexperts. However, the game of Dou Di Zhu presents significant challenges due to\nits vast state/action space and unique characteristics involving reasoning\nabout competition and cooperation, making the game extremely difficult to\nsolve.The RL model DouZero, trained using the Deep Monte Carlo algorithm\nframework, has shown excellent performance in DouDiZhu. However, there are\ndifferences between its simplified game environment and the actual Dou Di Zhu\nenvironment, and its performance is still a considerable distance from that of\nhuman experts. This paper modifies the Deep Monte Carlo algorithm framework by\nusing reinforcement learning to obtain a neural network that simultaneously\nestimates win rates and expectations. The action space is pruned using\nexpectations, and strategies are generated based on win rates. This RL model is\ntrained in a realistic DouDiZhu environment and achieves a state-of-the-art\nlevel among publicly available models.",
        "updated": "2024-07-14 17:32:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10279v1"
    },
    {
        "title": "Learning to Steer Markovian Agents under Model Uncertainty",
        "authors": "Jiawei HuangVinzenz ThomaZebang ShenHeinrich H. NaxNiao He",
        "links": "http://arxiv.org/abs/2407.10207v1",
        "entry_id": "http://arxiv.org/abs/2407.10207v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10207v1",
        "summary": "Designing incentives for an adapting population is a ubiquitous problem in a\nwide array of economic applications and beyond. In this work, we study how to\ndesign additional rewards to steer multi-agent systems towards desired policies\n\\emph{without} prior knowledge of the agents' underlying learning dynamics. We\nintroduce a model-based non-episodic Reinforcement Learning (RL) formulation\nfor our steering problem. Importantly, we focus on learning a\n\\emph{history-dependent} steering strategy to handle the inherent model\nuncertainty about the agents' learning dynamics. We introduce a novel objective\nfunction to encode the desiderata of achieving a good steering outcome with\nreasonable cost. Theoretically, we identify conditions for the existence of\nsteering strategies to guide agents to the desired policies. Complementing our\ntheoretical contributions, we provide empirical algorithms to approximately\nsolve our objective, which effectively tackles the challenge in learning\nhistory-dependent strategies. We demonstrate the efficacy of our algorithms\nthrough empirical evaluations.",
        "updated": "2024-07-14 14:01:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10207v1"
    },
    {
        "title": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights",
        "authors": "Xinyu-ChenYanwen-ZhuYang-HouLianzhen-Zhang",
        "links": "http://arxiv.org/abs/2407.10064v1",
        "entry_id": "http://arxiv.org/abs/2407.10064v1",
        "pdf_url": "http://arxiv.org/pdf/2407.10064v1",
        "summary": "In various industrial fields of human social development, people have been\nexploring methods aimed at freeing human labor. Constructing LLM-based agents\nis considered to be one of the most effective tools to achieve this goal.\nAgent, as a kind of human-like intelligent entity with the ability of\nperception, planning, decision-making, and action, has created great production\nvalue in many fields. However, the bridge O\\&M field shows a relatively low\nlevel of intelligence compared to other industries. Nevertheless, the bridge\nO\\&M field has developed numerous intelligent inspection devices, machine\nlearning algorithms, and autonomous evaluation and decision-making methods,\nwhich provide a feasible basis for breakthroughs in artificial intelligence in\nthis field. The aim of this study is to explore the impact of AI bodies based\non large-scale language models on the field of bridge O\\&M and to analyze the\npotential challenges and opportunities it brings to the core tasks of bridge\nO\\&M. Through in-depth research and analysis, this paper expects to provide a\nmore comprehensive perspective for understanding the application of\nintelligentsia in this field.",
        "updated": "2024-07-14 03:31:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.10064v1"
    }
]