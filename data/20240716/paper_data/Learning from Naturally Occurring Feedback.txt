Learning from Naturally Occurring Feedback
ShacharDon-Yehiya1 LeshemChoshen2,3 OmriAbend1
1TheHebrewUniversityofJerusalem,2MIT,3MIT-IBMWatsonAILab
{first.last}@mail.huji.ac.il
Abstract
Humanfeedbackdataisacriticalcomponent üë© How do you plot the numbers on
lineplot in seaborn?
indevelopinglanguagemodels. However,col-
lecting this feedback is costly and ultimately
not scalable. We propose a scalable method
forextractingfeedbackthatusersnaturallyin- The seaborn.lineplot() function in ü§ñ
Seaborn is used to create a line
cludewheninteractingwithchatmodels,and
plot, which is a type of chart used
leveragingitformodeltraining. Wearefurther to visualize the relationship
motivatedbypreviousworkthatshowedthere between two continuous variables
are also qualitative advantages to using natu- ‚Ä¶
ralistic(ratherthanauto-generated)feedback,
suchaslesshallucinationsandbiases. Weman-
ually annotated conversation data to confirm üë© Yes, but how do I also plot the
actual numbers above the line?
the presence of naturally occurring feedback
in a standard corpus, finding that as much as
‚àº30% of the chats include explicit feedback. Ask for Classification
Weapplyourmethodtoover1Mconversations
ü§ñ
To plot the actual numbers above
to obtain hundreds of thousands of feedback
‚Ä¶
samples. Trainingwiththeextractedfeedback
showssignificantperformanceimprovements
over baseline models, demonstrating the effi-
üë© It works! Great! Now, how do I
cacyofourapproachinenhancingmodelalign-
add a title?
menttohumanpreferences.
Positive Feedback
1 Introduction
Humanfeedbackisavaluableresourceformodel Figure1: Naturallyoccurringfeedbackexample. The
user asks the model a question, and responds to its
development. Thecurrentstandardmodeltraining
answer with an ‚ÄúAsk For Clarification‚Äù feedback and
processincludesapretrainingphase(Radfordetal.,
laterwith‚ÄúPositiveFeedback‚Äù.
2019),followedbyanalignmentphase,wherethe
modelisusuallyfine-tunedandtrainedwithrein-
forcementlearningonhumanpreferencedata(Bai
canoccurinmanyforms(e.g.,theusercontinues
etal.,2022),ofteniteratively(Touvronetal.,2023).
to the next question once they are satisfied with
The more data at hand, the better the model (Ka-
the model‚Äôs response), in this work we focus on
plan et al., 2020; Roberts et al., 2023). However,
unambiguousandexplicitcues,suchaswhenthe
collectingsuchdatausuallyrequirescostlyhuman
user directly refers to the quality of the model‚Äôs
labor,limitingitsscalability.
response (e.g., ‚Äúthank you!‚Äù, or ‚Äúthat‚Äôs wrong‚Äù)
Humansneverthelessdonotneedcommentators
orrephrasesandasksthesamequestionagain(cf.
toknowthattheirconversationpartnerissatisfied.
¬ß3.1).
Rather,theyinferitfromthecommunicationitself.
Wesuggestemployingasimilarrationalewithlan- Withtheintroductionofgeneralassistantmodels
guage models (LMs), and extract natural human likeChatGPT(OpenAIetal.,2024)andOpenAsis-
feedback(seeFig.1). Whilenaturalisticfeedback tant(K√∂pfetal.,2024),human-modelinteractions
4202
luJ
51
]LC.sc[
1v44901.7042:viXrahavebecomeveryprominent,notonlyamongma- aredoingtheranking(Cuietal.,2023;Leeetal.,
chinelearningexpertsbutalsoamongthegeneral 2023; Zhu et al., 2023) at the expense of intro-
public. Thus, there are huge amounts of data po- ducingnoiseandbiases(Zhengetal.,2024). For
tentiallyavailable. example, it was shown that LLMs tend to prefer
Unlike model as a judge methods (Liu et al., longerresponsesregardlessoftheirquality(Saito
2023b; Zheng et al., 2023b), naturally occurring etal.,2023).
feedbackisanchoredinthehumanresponse,and Anotherlineofworkcollectsdatasampleson-
thereforeislessproneto‚Äúhallucinations‚Äù(Lewis line during the interaction, by eliciting free-text
et al., 2021) and biases (Saito et al., 2023), and feedbackfromtheuser. Thisfeedbackisthenused
easiertoexplainandverify. in various ways for training (Shi et al., 2022; Jin
Anotheradvantageofthisapproachisthatthis etal.,2023;Scheureretal.,2022). Hancocketal.
formoffeedbackispotentiallyclosertothefeed- (2019)suggestedestimatingusersatisfactionand
back given by two human interlocutors (Bassiri, onlyifitislow,toelicitfeedbackfromtheusers.
2011;Wertsetal.,1995),possiblycontainingrele- Wefocusonnaturallyoccurringfeedback,i.e.,
vantinformationforbetteralignment. spontaneousunsolicitedfeedback. Whentwohu-
We manually annotate and show that naturally manstalk,theydonotscoreeachother‚Äôsresponses
occurringfeedbackisindeedprevalentinconversa- norexplicitlyaskforfeedback(atleastnotoften).
tiondata(¬ß3.2). Furthermore,wefindthatnaturally Rather,theinterlocutorsactivelysignaltheirunder-
occurringfeedbackismorecommoninrecentlycol- standingandagreementthroughtheuseofverbal
lecteddatathaninolderdata,possiblyduetousers and visual responses, such as ‚Äúhmm‚Äù, ‚Äúyeah‚Äù or
raisingtheirexpectationsandbeingabletoconduct facialexpressions,headnods,etc. (Vranjesetal.,
amore‚Äúhuman-like‚Äùconversationwiththemodel 2018;BavelasandGerwing,2011).
(¬ß4). This further underscores the importance of We show that also in a human-model textual
ever-growing data resources, over static datasets. conversation,suchfeedbacksignalsexist. Finding
Modelskeepimprovingandthereforethedataused them,ideallyautomatically,willallowustoextract
toalignthemshouldevolvetoo. freely annotated training examples. Employing
We introduce a method to automatically ex- suchextractionon anendlessstream ofnewcon-
tractthenaturallyoccurringfeedbackfromhuman- versationdata(Don-Yehiyaetal.,2023b)hasthe
modelinteractions(¬ß5). Wevalidateourmethod, potentialtogrowandimproveunboundedly.
bothquantitativelyandqualitatively,findingitman-
agestocorrectlyextractthefeedbacktoareason- 3 NaturallyOccurringFeedback
able degree. We use our method to obtain over
170k feedback samples from 1M non-annotated We begin by defining a taxonomy for naturally
human-model conversations. We release it as a occurring feedback. We then manually annotate
dataset(¬ß5.3).1 conversationstoaccountforthestatisticsofsuch
We use the extracted data to train a model to feedbacktypesinconversations.
better align with human preferences. Our model Throughout our discussion when we consider
demonstratedsuperiorperformance,outperforming feedbackwereferto(apartof)ahumanresponse
thepretrainedmodelinupto78%ofthetestcases thatrefersto(apartof)thelastmodel‚Äôsresponse.
(¬ß6).
3.1 FeedbackTaxonomy
2 Background
Wedefinethefollowingcategories,splitintofour
negativefeedbackcategoriesandonepositive:
Tocompileapreferencedataset,humanannotators
areaskedtorank/scorethegeneratedresponsesof
1. RepeatorRephrase(rephrase): Theuserre-
large language models (LLMs) at the time of the
peatsorrephrasestheirlastresponse,explain-
interaction (Chiang et al., 2024), or in retrospect
ingagainwhattheywants.
(Baietal.,2022;Ethayarajhetal.,2022). Tosave
thiscostlyhumaneffort,sometimesothermodels
2. Make Aware with Correction (aware + cor-
1Code and data: https://github.com/shachardon/ rect): Theuserpointstothemodelthatitwas
naturally_occurring_feedback, https://huggingface.
wrong, and provides information regarding
co/datasets/shachardon/naturally_occurring_
feedback theerror/howtofixit. E.g.,No,Iwanted...3. Make Aware Without Correction (aware -
correct): Theuserpointstothemodelthatit
waswrong,withoutprovidinganyadditional 35
information. E.g.,That‚Äôsincorrect 30
25
4. AskforClarification(clarify): Theuserasks
20
for additional resolution that was expected
to be in the the previous response, but was 15
missing. E.g.,Wasitlikethat? 10
5
5. Positive Feedback (positive): The user con-
0
firmsthatthemodeldidagoodjob,possibly rephrase aware aware clarify positive
+correct -correct
thankingit. E.g.,Thankyou!
Figure 2: The distribution of feedback categories for
Wenowturntomotivatingthissetofcategories. the first 300 conversations in the dataset, as deemed
by manual annotation. The most frequent categories
The two main design features are simplicity and
are‚ÄúRepeatandRephrase‚Äùand‚ÄúAskforClarification‚Äù.
text-anchoredness,i.e.,thefeedbackshouldbedi-
Thereareonly9casesof‚ÄúPositiveFeedback‚Äù.
rectlyandexplicitlyderivedfromthetext,without
requiringcomplexsubjectiveinterpretation.
Following this line, the feedback type that ap- useful, but still can be used as a strictly negative
pears the most explicitly in the text is probably example(incontrasttoAskforClarification).
‚ÄúPositive Feedback‚Äù. Although we found it to be ‚ÄôRepeatorRephrase‚Äôisuniquecomparedtothe
lesscommon(see¬ß5.3),positivefeedbackcanusu- other feedback forms as it does not leverage the
allyberecognizedatthevocabularylevel. Theuser model‚Äôsabilitytoprocessmulti-turninteractions.
thanks the model for its response (e.g., ty), says Instead,theuserignoresthepreviousresponseand
itdidagoodjob(e.g.,great!) orthatitwasright rephrases again what they want, as if it was the
(e.g.,that‚Äôscorrect). beginningoftheconversation. Assumingthatthe
Thenegativefeedbackcases,ontheotherhand, following model‚Äôs response would be better, the
aremuchmorediverse. Therearevocabulary-level twoone-turnuser-modelinteractionscanbeused
feedbackcases(that‚Äôswrong),butalsomoreseman- as a preference pair for training. However, it is
ticallycomplexinstances("actually,Iwasasking importanttonotethatthecontextofthefullconver-
about..."). Thus,usingthefeedbackpatternsfrom sationiscrucialtorecognizethisfeedbackform.
Petraketal.(2023),webreakthenegativefeedback Intheirtaxonomy,Petraketal.(2023)alsohave
cases into finer categories to avoid too general a an‚ÄúIgnoreandContinue‚Äùcategory,wheretheuser
definition. Also,moredetailedcategoriesprovide ignoresanerror. Weleaveitoutasitdoesnotcon-
additional information that can be used later for tainfeedback,butratherimpliesthatnonewasleft
bettertraining/inference. despiteanerror. Itisonlymeaningfulwhenaccom-
Wefoundthe‚ÄúAskforClarification‚Äùcategoryto paniedbyanannotatederrorinthepreviousmodel
besomewhatinthemiddleintermsofsentiment response,whichwedonothaveinoursetting.
andfeedbacknaturebetweenthePositiveFeedback
3.2 ManualAnnotation
and the rest of the negative categories. The user
asks for more information or confirmation, indi- To get an initial impression of the distribution of
cating that the model‚Äôs response was in the right categoriesinthistaxonomy,oneoftheauthorsman-
direction,sonotentirelywrong,butstillprovides uallyannotatedthefirst300conversationsfromthe
somesubtlefeedback. Thiscategoryisverycom- LMSYS-Chat-1Mdataset(Zhengetal.,2023a,see
mon(see¬ß5),andweexpectthesecasestobeeven ¬ß5.1). Afterfilteringoutnon-Englishconversations
morefrequentasmodelsimprove. and offensive/unsettling conversations, we were
Anotherdistinctionwefoundusefulisbetween leftwith223conversations. Wefind77conversa-
‚ÄúMakeAwarewithCorrection‚Äùand‚ÄúMakeAware tionswithatotalof101feedbackcases: 37Repeat
withoutCorrection‚Äù. Thefirstholdsclearpotential orRephrase,18MakeAwarewithCorrection,13
for training/inference, as the user provides infor- MakeAwarewithoutCorrection,24AskforClari-
mationregardingtherequiredfix. Thelatterisless ficationand9PositiveFeedback(seeFig.2). The
tnuoCfactthat‚àº 30%ofconversationsincludefeedback
is an encouraging result. As the percentage is so
There are five different patterns in user responses
highitislikelythatsimplemethodswouldalready subsequent to errors in assistant utterances:
Repeat or Rephrase (UR1) - The user repeats or
sufficetoextractnotableamountsoffeedbackdata
rephrases their concern, e.g., Actually, I wanted‚Ä¶
oreasilyfilterforspecificqualitydata.
Make Aware with Correction (UR2) - The user makes
the system aware of the error and provides
To validate our manual annotation, we ask an
information to address what is missing or wrong in its
in-houseannotatortore-annotatethefirst100con- utterance, e.g., No. I wanted you to‚Ä¶
versations,ofwhich68passthefiltering. Wegeta Make Aware without Correction (UR3) - The user
makes the system aware of the error without
Cohen‚Äôskappaof0.65forthebinarytaskoffeed- providing additional information, e.g., You‚Äôre wrong.
backrecognition. Ofthefeedbackcasesthatboth Ask for Clarification (UR4) - The user asks for
annotators agreed upon, they also agreed on the clarification, e.g., Are you sure? Is it really that‚Ä¶
categoryin0.79ofthecases. Positive Feedback (UR5) - The user confirms that the
assistant did a good job by directly saying so or
thanking it, e.g., Thank you
4 Up-to-DateFeedback Given these guidelines, please recognize such user
responses in the following dialogue. Please use the
format:
{
Comparingthemodelsoftwoyearsagowiththose "User Response Pattern": [Insert User Response Pattern],
"User Response Text": [Insert User Response Text]
oftodayseemslikecomparingapplestooranges,
}
andevenatashortertimescale,thestate-of-the-art
If there is no feedback, use the following format:
advances rapidly (Beeching et al., 2023). In the
{
interim, as models get better, users expect more. "User Response Pattern": "No Feedback",
"User Response Text": ""
Usersusethemodelsfornewscenariosthatwere
}
not possible before (Zhao et al., 2024) and do so
in a more natural way (except in extreme cases
Figure3: ExtractionPrompt. Wedescribethefivefeed-
(Don-Yehiyaetal.,2023a)). Weexpectthatwith
backcategoriesandaskthemodeltorecognizespans
morefluentanddiverseconversationscomesmore
ofhumanresponsesinthechat,thatcontainsuchfeed-
feedback. back.WeaskthemodeltooutputitsresponseinaJSON
format,containingboththetextspanandcategory.
Wemeasurethatempiricallybycomparingthe
new annotation of current models to annotation
efforts of earlier models. Out of the six datasets
5 AutomaticallyExtractingFeedback
that were annotated by Petrak et al. (2023), only
the Self-Feeding Chatbot dataset (Hancock et al., Given that natural feedback is already present in
2019)isbothhuman-modelandopendomain,and current datasets and ongoing human-model con-
thuscomparable. TheSelf-Feedingdatasetwascre- versations,weproposeamethodtoautomatically
atedin2019,andsoisthemodelthatwasusedto extractthisfeedback.
generateit. Only11feedbackinstanceswerefound Basedonthefivefeedbackcategories(¬ß3.1),we
withinarandomsampleof100conversations. This instruct an LLM to recognize spans ‚Äì part of the
is less than half the feedback frequency found in humanresponsesthatcontainfeedbackinagiven
thenewerLMSYS-Chat-1Mdataset(omittingthe conversation and classify them. We then use a
positivefeedbackcategoryasitwasintroducedby Pythonscripttoparsethegeneratedresponseand
us). Wenotethatthereare48annotatederrorsin extractallthefeedbackinstances. Wediscussthe
the100Self-Feedingdatasetsample,andhenceit implementationdetailsnext.
isunlikelythatitwasalackoferrorsthatcaused
theuserstogivelessfeedback. 5.1 ExtractionImplementationDetails
Ourfindingssuggestthatmorethanever,natu- Data. We use the LMSYS-Chat-1M dataset
rally occurring feedback can serve as a valuable (Zheng et al., 2023a), a collection of real-world
resource for feedback data. We believe in the fu- conversationswith25state-of-the-artLLMs. We
ture not only would models be better, continuing selectthisdatasetforitssizeandvarietyofstate-of-
the above trend, but natural feedback itself may the-art models and conversation topics. We filter
becomeaknownresource,onewhichusersexpect outconversationswithlessthantwoturns,asthere
themodelstouse. is no human feedback in a one turn conversation14
rephrase 15 0 0 5 0
0.4
12
aware 3 3 0 2 0
+correct 10
0.3
aware 2 1 0 2 0 8 -correct
6 0.2
clarify 5 0 0 13 0
4
0.1
2
positive 0 0 0 0 7
0
rephrase aware aware clarify positive 0.0
+correct -correct rephrase aware aware clarify positive
Predicted label +correct -correct
Figure4: ConfusionMatrixfortheExtractedFeedback. Figure5: Automaticallyextractedfeedbackdistribution.
Outofthe101manuallyannotatedfeedbackcases,our Theautomaticextractionandmanualannotation(Fig.2)
automaticmethodmanagedtofind58,andtocorrectly agreeonwhichfeedbacktypesaremorecommon: ‚ÄúAsk
classifytocategories38. Thereisnoconfusionbetween For Clarification‚Äù and ‚ÄúRepeat or Rephrase‚Äù. ‚ÄúMake
‚ÄúPositiveFeedback‚Äùandtherestofthecategories. AwarewithoutCorrection‚Äùand‚ÄúPositiveFeedback‚Äùare
therarest.
(oneuserqueryfollowedbyonemodelresponse).
5.2 ExtractionEvaluation
Model. We use Mixtral-8x7B-Instruct-v0.1
Toevaluatemodelperformanceincorrectlyidenti-
(Jiang et al., 2024) with 4-bit quantization to fit
fyingandclassifyingthenaturallyoccurringfeed-
our GPUs. See App.¬ßA for the full generation
back,weusethe300manuallyannotatedconver-
parameters. Duringdevelopment,wealsoexperi-
sations from ¬ß3.2. We analyze the two abilities
mentedwithYi-34B-Chat(Youngetal.,2024)and
separately;extractionoffeedbackspanandclassi-
GPT-3.5, but found that Mixtral surpasses them.
fyingitintothecorrectcategory. Afeedbackspan
It is reasonable to assume that the non-quantized
isconsideredcorrectifitisasub-stringofaman-
version of the model or other stronger models
uallyannotatedfeedbackspanandisatleasthalf
would allow more accurate feedback extraction.
as long. We report both precision and recall. We
Our experiments here are intended as a POC,
definethespan-precisionasthepercentageofthe
wherethemodelandotheraspectsdescribedbelow
correctlyidentifiedfeedbacksoutofthetotalnum-
can be substantially improved if necessary for
berofidentifiedfeedbacks. Correspondingly,the
practicaluses(see¬ß9).
span-recallistheratioofthosecorrectlyidentified
feedbacks,butdividedbythetotalnumberofman-
Prompt. After experimenting with a couple of
uallyannotatedfeedbacks. Wedefinethecategory
versions,wefoundthepromptinFig3toperform
precision/recall as the number of feedbacks that
best. Onekeypointisaskingthemodeltoprovide
werebothidentifiedandclassifiedcorrectlytothe
its output in JSON format, to make parsing eas-
rightcategory,dividedbythetotalnumberofiden-
ier. Usingfew-shotexamplesseemstoconfusethe
tified/manuallyannotatedfeedbacksrespectively.
model,probablybecauseofthelengthofthecon-
While our manual annotation found 101 feed-
versations and the difficulty to separate different
backcases,ourautomaticmethodfound134feed-
conversations.
backcases, outofwhich58arecorrect. Employ-
Parsing. If the generated text contains the ingbootstrapwith1000repetitions,thisresultsto
prompt,wedeletetheprompt. Wethenextractall 0.43¬±0.05precisionand0.58¬±0.06recall. Tak-
JSONobjectsandconfirmtheycontainthe‚ÄúUser ingthecategoriesintoaccount,weget0.28¬±0.04
ResponsePattern‚Äùand‚ÄúUserResponseText‚Äùfields. precisionand0.38¬±0.06recall. Fig4presentsthe
For each of the JSON objects we verify that the confusion matrix for the categories. We can see
‚ÄúUser Response Text‚Äù is indeed contained in one that‚ÄúRepeatandRephrase‚Äùand‚ÄúMakeAwarewith
oftheuserresponsesandthatthecategoryisvalid Correction‚Äù are the most frequent categories for
(oneofthe5possibilities). Ifanyofthesedonot bothmanualannotationandtheautomaticmethod,
hold,wediscardtheexample. andthattheautomaticmethoddidnotpredictthe
lebal
eurT
egatnecreP‚ÄúMakeAwarewithoutCorrection‚Äùcategoryatall. If response, i.e., starting from the second turn only.
wereduceittobinarycategories,i.e.,positive/neg- Theaveragefeedbackturnis3.1,andtheaverage
ative,wecanseethatthereisnoconfusionbetween lengthofthefeedbackspanis52.5tokens.
thepositivecategoryandthenegativecategories.
6 TrainingontheExtractedFeedback
Examiningthefalsepositives,weseethatmany
ofthemaredebatable. Forexample,theuserstarted Todemonstratetheusefulnessoftheextracteddata,
bytellingthemodel‚ÄúIaminterestedtoknowhow weuseittotrainLLMsandshowtheimprovement.
youwork‚Äù. Themodelthenrespondedwith‚ÄúIam Ourdatacontainsbothpositiveandnegativeex-
anAIlanguagemodelthatusesmachinelearning amples. We start by using the positive examples
algorithmstounderstandandgeneratehuman-like only,tofinetunethemodels. Wethenpresentsome
text. Iamtrainedonalargedatasetoftext... Ifyou initialresultsforpreferencetraining,withbothpos-
haveanyspecificquestionsabouthowIwork,feel itiveandnegativeexamples.
free to ask!‚Äù. The user then asked ‚ÄúCan I make
you on my own computer?‚Äù and our extraction 6.1 TrainingDetails
methodmarkedthistextasa‚ÄúAskforClarification‚Äù Werandomlysplitthepositiveexamplesto80/20%
feedbackcase. Wedidnotannotatethisresponse fortrain/valdata,remainingwith8448trainingex-
asfeedbackasweconsideredittobeanewrequest amples. Weusethreemodels: EleutherAI/pythia-
and not a clarification of the previous one. How- 1.4b, EleutherAI/pythia-2.8b (Biderman et al.,
ever,wedoseewhyitcouldbepointingtomissing 2023),andmistralai/Mistral-7B-v0.1(Jiangetal.,
informationintheoriginalquestion. 2023). FormoredetailsseeApp.¬ßA.
Usingbettermodelsfortheextractionorfinding
6.2 ModelPerformanceEvaluation
betterinstructionpromptswouldprobablyimprove
results. Still,giventhatitseemsthatalsothefalse Tomeasuretheimprovementofthemodelsgiven
positivesencodearelevantsignal,wehypothesize thenewdata,weusethevalidationsplitoftheOpe-
thatthisextractedfeedbackdataalreadywouldbe nAssistantdataset(K√∂pfetal.,2024). Wegenerate
beneficialforpreferencetraining. thelastresponsewithbothourtrainedmodelsand
thecorrespondingpretrainedmodels. SeeApp.¬ßA
5.3 TheNaturalFeedbackDataset
forthegenerationparameters.
Usingthemanualannotationasatestset,andour
Human Evaluation. We perform human eval-
extraction method to acquire more feedback, we
uation of the model outputs to acquire a reliable
create a large Natural Feedback Dataset. We run
evaluation. Todosoanin-househumanannotator
the described extraction method on all 1M con-
wasaskedtoratenotconsistentlyorderedpairsof
versationsoftheLMSYS-Chat-1Mdataset. After
100modelresponsesforeachofthemodels,with-
filteringouttwoturnconversations(see¬ß5.1),we
outknowingwhatmodelcreatedwhichresponse
areleftwith334,319conversations. Weapplyour
(the pretrained baseline or the finetuned version).
methodandendupwith173,859feedbackexam-
Ourtrainedmodelswon69%/81.5%/77%over
ples from 115,312 different conversations. See
theircorrespondingpretrainedversions.
Fig.5forthecategorydistribution. Intermsofpos-
itive/negative examples, we have about 15 times EvaluationbyOpenModels. Inadditiontoman-
morenegativeexamples,similartotheratiowehad ual evaluation, we perform automatic evaluation,
inthemanualannotation(¬ß3.2). Notethatthisratio which allows more flexibility in the analysis. To
is not surprising, as correcting a model is poten- prioritize replicable science, we also explore the
tiallybeneficialfortheuser(helpingthemodelto usabilityofopenmodelsasevaluatorsinoursce-
helpme),whilethankingitislesspractical. nario. The RewardBench leaderboard (Lambert
We examine the statistics of the conversations et al., 2024) evaluates the capabilities of models
thatwerefoundtocontainfeedback. Theaverage in the task of rating model responses, and its top
numberofturnsinaconversationintheLMSYS- modelsoutperformsomeclosedmodelsthatarefre-
Chat-1Mdatasetis2,whiletheaveragenumberof quentlyusedasjudges. Basedontheleaderboard,
turns in a conversation that contains feedback is wetakeopenbmb/Eurus-RM-7b(Yuanetal.,2024)
5.5. Thisisnotsurprisingastheminimumnumber andsfairXC/FsfairX-LLaMA3-RM-v0.1(leaders
ofturnsinconversationthatcontainsfeedbackis2 oftheleaderboard,whenconductingtheseexperi-
asuserfeedbackcanappearafteratleastonemodel ments),andcomparethepretrainedtothefinetunedmodelsresponses. However,weweredisappointed ¬ß6.1). Totestthebenefitsofthenegativeexamples
tofindthatthesemodelsdoapoorjobcomparing wetryothertrainingmethods.
theoutputsofthesmallermodels. openbmb/Eurus- The positive and negative examples are not ar-
RM-7b reported 31% and 38% wins for the 1.4B rangedinpairsthatusethesamepromptandhence
and2.8Bmodels,andsfairXC/FsfairX-LLaMA3- arenot suitableforDPO training(Rafailov etal.,
RM-v0.1reported48%and60%. Weassumethis 2024). Instead, we use KTO (Ethayarajh et al.,
is due to the distribution of the data they were 2024), which is capable of handling non-paired
trained on, which only represents stronger mod- preference data. As mentioned in ¬ß5.3, there are
els. Theresultsforthe7Bmodelontheotherhand manymorenegativeexamplesthanpositiveones.
arecomparabletothosewegotinthehuman(and Tobalancethis,weuseonlythe"MakeAwarewith
GPT-4) evaluation. openbmb/Eurus-RM-7b and Correction"and"MakeAwarewithoutCorrection"
sfairXC/FsfairX-LLaMA3-RM-v0.1reported70% categories,andontopofthatwedown-sample. We
and72%winsforthetrainedmodelrespectively. chosethesecategoriesasweassumetheir‚Äônegative‚Äô
signal is the strongest. For the hyperparameters,
GPT as a Judge. To complete the picture, we
seeApp.¬ßA.
run a GPT-4 as a judge evaluation (Zheng et al.,
Werunthisexperimentwiththe7bmodelonly,
2023b). WeusetheRewardBench(Lambertetal.,
asKTOisnotbeneficialforsmallermodels(Etha-
2024)implementationtoinstructGPT-4tocompare
yarajhetal.,2024). Westartfromthepreviously
responsepairs. Ourtrainedmodelswon65%/74%
finetunedmodelandthentrainitwithKTO.
/78%overtheircorrespondingpretrainedversions.
Weevaluatethemodel. openbmb/Eurus-RM-7b
reports74%wins,sfairXC/FsfairX-LLaMA3-RM-
Conductingabinominaltestontheseresults,we
v0.1 reports 75% wins, and GPT-4 reports 79%
findthatallabovereportedresultsaresignificant
wins. Thesescoresarealloverwhelminglybetter
with p < 10e‚àí9. We conclude that our automat-
thanthepretrained,andsomewhatbetterthanthose
ically extracted training data is indeed beneficial.
wehadforthefinetunedmodel(about1-3points
Training on about 8k positive examples yields a
improvement). Weconcludethatournegativedata
significant improvement for all our tested model
(oratleastsomeofthecategories)isindeeduseful
sizes,moreasthemodelislarger.
fortraining.
6.3 RandomChatsBaseline
7 AblationExperiments
Asanadditionalbaseline,wereplaceourextracted
We analyze different aspects of our extraction
positive examples with a random sample of chat
method, including our choice of feedback taxon-
examples from the LMSYS-Chat-1M dataset of
omy.
thesamesize. Theseexamplesarenotnecessarily
positive,buttheyareinachatformatandofrela- 7.1 TaxonomyEffectontheExtraction
tivelywell-performingmodelsandthereforemight
Hereweexaminetheeffectofthefeedbacktaxon-
be useful for knowledge distillation nonetheless
omyonthesuccessofthemodelinaccuratelyex-
(Honovichetal.,2023). Wewanttotestwhether
tractingthefeedbackspansfromtheconversations,
training on our extracted data has any advantage
findingthatourmodelbenefitsfromourtaxonomy
over this randomly sampled data. We finetune
designdecisions.
the 7B model on them, and evaluate their per-
Weexamineseveraltaxonomyalternatives. We
formance. openbmb/Eurus-RM-7b reports 64%
evaluateeachbycalculatingtheprecisionandrecall
wins,sfairXC/FsfairX-LLaMA3-RM-v0.1reports
relative to the 300 manually annotated conversa-
68% wins, and GPT-4 reports 75% wins, all out-
tionsfrom¬ß3.2.
performed by our main results. This strengthens
ourpreviousconclusionthatourextracteddatais 7.1.1 NoCategories
beneficial.
Ourtaxonomyintroduces5differentfeedbackcat-
egories(see¬ß3.1). Hereweexaminewhetherthere
6.4 KTOTraining
isevenaneedforanytaxonomyatall.
Sofar, wehaveshownpromisingresultsforfine- Wechangeourpromptsuchthatitwillnotcon-
tuning. Whilefinetuningisaperformantwaytouse tainanycategorydefinition. Weinstructthemodel
feedback,itonlytrainsonpositiveexamples(see torecognizespansoftextthatareinformitiveastothesatisfactionoftheuser,andratethemonascale the model to provide a ‚ÄúConfidence Level (1-5)‚Äù
of1-5. SeeApp.¬ßCfortheprompt. field. SeeApp.¬ßCfortheprompt.
Running the model in this setting, the model Lookingatthedistributionofconfidencescores
found 693(!) text spans, while not even one of themodelassigned,wefindthatover96%ofthe
themmatchesthemanuallyannotatedfeedbackex- feedback cases received a 5 score. The other 4%
amples. Manually looking at a few of them, it are mostly ‚ÄúNo Feedback‚Äù or hallucinations that
seemsthatthemodelfailsmiserablyatidentifying areautomaticallyremovedattheparsingstage(see
relevanttextspans. Forexample,itoftensuggests ¬ß5.1). Weconcludethatthismethodisineffective.
seeingtheoriginaluser‚Äôsrequestsasanindication
ofusersatisfaction(e.g.,‚ÄúShowmehowtoimple- 8 RelatedWork
mentatoyversionofarelationaldatabase. Begin
InadditiontotheLMSYS-Chat-1Mdatasetwhich
bywritingatoyqueryplannerthatconvertSQL...‚Äù),
we used due to its size and inclusion of multiple
which is of course not a valid user feedback as it
models,thereareotherrecent,publicconversation
precedesthemodelresponse.
datasetssuchasWildChat(Zhaoetal.,2024),Col-
We conclude that an overly general extraction
lective Cognition ChatGPT Conversations,2 and
promptisharderforthemodeltohandle,andthat
PRISM (Kirketal.,2024).
a detailed taxonomy is helpful for the automatic
Petraketal.(2023)investigatedthetypesofer-
extractionprocess.
rorsanduserresponsesin6differentdatasets. Al-
7.1.2 LimitedCategorization thoughweadoptandmodifytheirfeedbacktaxon-
omy,wetaketwostepsfurther. Wefocusonuser
We examine the effect of using fewer feedback
responsesandextractthemautomatically,andwe
categoriesontheextractionprocess. Thisfollowed
showtheimportanceofusingup-to-dateconversa-
the hypothesis that focusing on a smaller set of
tiondata(¬ß4),contrarytotheirconclusion.
categorieswouldallowforbetterprecision.
WeoptedforKTOtotrainamodelonourdata,
Welimitourselvestothe‚ÄúRepeatandRephrase‚Äù
buttherearemanymoreoptionsfortrainingonnon-
and‚ÄúPositiveFeedback‚Äùcategories,aswerecog-
positiveexamples(Christianoetal.,2017). Ouyang
nize that they are both relatively easier for the
etal.(2022)andShietal.(2022)suggestedtocre-
modeltodistinguishfromtheothercategories(see
atepossiblecorrectionsfornegativeexamplesand
4). Weinstructthemodeltoextractthesetwotypes
trainonthem. Othermethodsusepairsofpositive
forfeedbackonly. SeeApp.¬ßCfortheprompt.
and negative examples and train on them both to
Forthe‚ÄúPositiveFeedback‚Äùcategory,themodel
predicttheirscores(Liuetal.,2023a). Pengetal.
managestoachieve0.5precisionforbothspanand
(2024) and Wu et al. (2024) showed the possible
categoryprecision,i.e.,allpositivecasesthatthe
gainoffine-grainedfeedback.
modelfoundwereclassifiedcorrectly.
Another set of related works are ones creating
For the ‚ÄúRepeat or Rephrase‚Äù category, the
syntheticdatafromdatasets(Yehudaietal.,2024)
modelmanagedtoachieve0.43text-spanprecision
or augmenting user feedback (Sudalairaj et al.,
and0.17categoryprecision. Giventhatthereare
2024). Wenotetheirsimilarityinbetterutilizing
only two possible categories, there is a relatively
human effort for creating data samples for train-
largegapbetweenthespanandcategoryprecision.
ing. Those, however, differ from our work in the
Lookingattheextractedexamples,weseethatthe
problemtheyaddress. Sucheffortsrelyonboost-
modeltendstoinventnewcategories,forexample
ingexistingtrainingsignals,whetherfoundinthe
‚ÄúAskingforAssistance‚Äùor‚ÄúAskforExamples‚Äù.
humanexplicitannotations,themodel,orboth. In
We conclude that focusing on fewer feedback
contrast,ourapproachaimstoidentifysignalsina
categoriesisnotnecessarilyeasierforthemodel.
scalablefashion. Infact,theoutputofourmethod
7.2 ConfidenceLevel canbeusedastheirinput(Bartolom√©etal.,2024).
We want to examine the usefulness of asking the
9 DiscussionandFutureWork
model to generate a ‚Äúconfidence level‚Äù value, to
better filter the extracted feedback samples such Thispaperadvocatestheuseofnaturallyoccurring
thatwewillgetahigherprecisionscore. feedbackandintroducesamethodtoextractit. We
Todoso,inadditiontothe‚ÄúUserResponsePat-
2https://huggingface.co/datasets/
tern‚Äùand‚ÄúUserResponseText‚Äùfields,weinstruct CollectiveCognition/chats-data-2023-10-16?row=11findthatnaturallyoccurringfeedbackiscommonin emphasize that our extraction method itself does
human-modelchats. Weuseourmethodtoextract notuseGPToranyproprietarymodels.
over170k feedbacksamplesandtrainmodelson
EthicsStatement
them,demonstratingtheirusefulness.
Ourmethodcanbeimprovedwithabettermodel,
This work has been approved by the IRB of our
prompt, and more sophisticated extraction algo-
institution. Weabidebythetermsandconditions
rithm. Nonetheless,wehopeourresultsencourage
of the LMSYS-Chat-1M dataset (see the license
moreworkonnaturallyoccurringfeedback. here 3. As mentioned by the LMSYS-Chat-1M
Weobservedin¬ß4thatnewerconversationdata datasetauthors,theLMSYS-Chat-1Mdatasetcon-
tendstocontainmorenaturallyoccurringfeedback. tains unsafe conversations that may be perceived
Buschmeier and Kopp (2018) showed the impor- as offensive or unsettling. The provided OpenAI
tanceof‚Äúlistenerfeedback‚Äù(subtleverbalsignals, moderation API tag can be used to filter it. We
headgestures,andfacialexpressions)fortheability informedourannotatorsofthisandinstructedthem
tocommunicate. Theyshowedthatthisfeedback toskiptheseconversations.
encourageshumaninterlocutorsinteractingwitha
modeltoprovidemorefeedbackbythemself,and AcknowledgmentsandThanks
to rate the conversation as more helpful. There-
WethankJohn(autoMeta)Cook,RamonAstudillo
fore,weexpectvoiceassistantconversationdatato
andBenBurtenshawforthedeepdiscussionsthat
containevenmorefeedback.
helpedusconvergewithourthoughts.
Another interesting line of future work is the
incorporation of feedback into chats in real-time,
withinteractivereinforcementlearningforexam- References
ple,oratleastinamannerthatwoulddirectlyaffect
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
futureuserconversations,makinggivingfeedback
Askell, AnnaChen, NovaDasSarma, DawnDrain,
morebeneficialfortheuser. StanislavFort,DeepGanguli,TomHenighan,etal.
2022. Trainingahelpfulandharmlessassistantwith
Limitations reinforcementlearningfromhumanfeedback. arXiv
preprintarXiv:2204.05862.
Although the original LMSYS-Chat-1M dataset
√ÅlvaroBartolom√©,GabrielMart√≠n-Bl√°zquez,Agust√≠n
containssomenon-Englishconversations,wefilter
Piqueres-Lajar√≠n,andDanielVila-Suero.2024. Disti-
thoseoutduringevaluation,asourannotatorsare label: AnAIfeedback(AIF)frameworkforbuilding
notfamiliarwiththeselanguages. datasetswithandforLLMs.
As mentioned a couple of times along the pa-
MohammadAminBassiri.2011. Interactionalfeedback
per, our automatic extraction method can be im- andtheimpactofattitudeandmotivationonnoticing
proved further to achieve better precision and re- l2form. EnglishLanguageandLiteratureStudies,
call. Webelievethatthefactthateventhecurrent 1(2):61.
relatively low precision data managed to achieve
JanetBeavinBavelasandJenniferGerwing.2011. The
good training results underscores the importance listenerasaddresseeinface-to-facedialogue. Inter-
andpotentialofnaturallyoccurringfeedback. With nationalJournalofListening,25(3):178‚Äì198.
theabundanceofdata,futureworkmightseekbet-
EdwardBeeching,Cl√©mentineFourrier,NathanHabib,
terprecisionorkeephighrecalldependingontheir
SheonHan,NathanLambert,NazneenRajani,Omar
goals. Onecouldtrainonalotoflowqualitydata, Sanseviero,LewisTunstall,andThomasWolf.2023.
focusonspecificsubsetsofinterest(e.g.,adomain) Openllmleaderboard. https://huggingface.co/
spaces/HuggingFaceH4/open_llm_leaderboard.
orfocusonqualityannotationthrowingalotand
stillnoendingupwanting,eachrequiringdifferent StellaBiderman,HaileySchoelkopf,QuentinGregory
precision-recalltradeoffs. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hal-
lahan,MohammadAflahKhan,ShivanshuPurohit,
We used GPT as a judge and other models for
USVSN Sai Prashanth, Edward Raff, et al. 2023.
evaluating the trained model. This approach is
Pythia: A suite for analyzing large language mod-
bothcostlyandknowntohavebiases(e.g.,Dubois els across training and scaling. In International
et al., 2024; Panickssery et al., 2024). Therefore,
3https://huggingface.co/
weuseitonlytocomplementthehumanevaluation
datasets/lmsys/lmsys-chat-1m#
andtheopen-modelsevaluation. Wewouldliketo lmsys-chat-1m-dataset-license-agreementConferenceonMachineLearning,pages2397‚Äì2430. OrHonovich,ThomasScialom,OmerLevy,andTimo
PMLR. Schick. 2023. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. In
HendrikBuschmeierandStefanKopp.2018. Commu- Proceedings of the 61st Annual Meeting of the As-
nicativelistenerfeedbackinhuman-agentinteraction: sociationforComputationalLinguistics(Volume1:
Artificialspeakersneedtobeattentiveandadaptive. LongPapers),pages14409‚Äì14428,Toronto,Canada.
InProceedingsofthe17thinternationalconference AssociationforComputationalLinguistics.
onautonomousagentsandmultiagentsystems,pages
1213‚Äì1221. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch,ChrisBamford,DevendraSinghChaplot,Diego
Wei-LinChiang,LianminZheng,YingSheng,Anasta- delasCasas,FlorianBressand,GiannaLengyel,Guil-
siosNikolasAngelopoulos,TianleLi,DachengLi, laumeLample,LucileSaulnier,etal.2023. Mistral
HaoZhang,BanghuaZhu,MichaelJordan,JosephE 7b. arXivpreprintarXiv:2310.06825.
Gonzalez,etal.2024. Chatbotarena: Anopenplat-
formforevaluatingllmsbyhumanpreference. arXiv Albert Q. Jiang, Alexandre Sablayrolles, Antoine
preprintarXiv:2403.04132. Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
PaulFChristiano,JanLeike,TomBrown,MiljanMar- Casas, Emma Bou Hanna, Florian Bressand, Gi-
tic, Shane Legg, and Dario Amodei. 2017. Deep anna Lengyel, Guillaume Bour, Guillaume Lam-
reinforcementlearningfromhumanpreferences. Ad- ple, L√©lio Renard Lavaud, Lucile Saulnier, Marie-
vancesinneuralinformationprocessingsystems,30. AnneLachaux,PierreStock,SandeepSubramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang,
WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,and Timoth√©eLacroix,andWilliamElSayed.2024. Mix-
MaosongSun.2023. Ultrafeedback: Boostinglan- tralofexperts. Preprint,arXiv:2401.04088.
guage models with high-quality feedback. arXiv
preprintarXiv:2310.01377. DiJin,ShikibMehri,DevamanyuHazarika,Aishwarya
Padmakumar, Sungjin Lee, Yang Liu, and Mahdi
Shachar Don-Yehiya, Leshem Choshen, and Omri Namazifar.2023. Data-efficientalignmentoflarge
Abend.2023a. Humanlearningbymodelfeedback: languagemodelswithhumanfeedbackthroughnatu-
Thedynamicsofiterativepromptingwithmidjour- rallanguage. arXivpreprintarXiv:2311.14543.
ney. InProceedingsofthe2023ConferenceonEm-
pirical Methods in Natural Language Processing, JaredKaplan,SamMcCandlish,TomHenighan,TomB
pages4146‚Äì4161,Singapore.AssociationforCom- Brown,BenjaminChess,RewonChild,ScottGray,
putationalLinguistics. AlecRadford,JeffreyWu,andDarioAmodei.2020.
Scaling laws for neural language models. arXiv
Shachar Don-Yehiya, Leshem Choshen, and Omri preprintarXiv:2001.08361.
Abend. 2023b. Sharelm: Crowd-sourcing human
feedback for open-source llms together. https:// HannahRoseKirk,AlexanderWhitefield,PaulR√∂ttger,
sharelm.github.io/. (Accessedon04/15/2024). AndrewBean,KaterinaMargatina,JuanCiro,Rafael
Mosquera, Max Bartolo, Adina Williams, He He,
YannDubois,Bal√°zsGalambosi,PercyLiang,andTat- BertieVidgen,andScottA.Hale.2024. Theprism
sunori B Hashimoto. 2024. Length-controlled al- alignment project: What participatory, representa-
pacaeval: Asimplewaytodebiasautomaticevalua- tiveandindividualisedhumanfeedbackrevealsabout
tors. arXivpreprintarXiv:2404.04475. thesubjectiveandmulticulturalalignmentoflarge
languagemodels. Preprint,arXiv:2404.16019.
Kawin Ethayarajh, Yejin Choi, and Swabha
Swayamdipta. 2022. Understanding dataset Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte,
difficultywithV-usableinformation. InProceedings Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,
of the 39th International Conference on Machine Abdullah Barhoum, Duc Nguyen, Oliver Stan-
Learning, volume 162 of Proceedings of Machine ley, Rich√°rd Nagyfi, et al. 2024. Openassistant
LearningResearch,pages5988‚Äì6008.PMLR. conversations-democratizinglargelanguagemodel
alignment. AdvancesinNeuralInformationProcess-
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, ingSystems,36.
DanJurafsky,andDouweKiela.2024. Kto: Model
alignmentasprospecttheoreticoptimization. arXiv Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
preprintarXiv:2402.01306. LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
NouhaDziri,SachinKumar,TomZick,YejinChoi,
Braden Hancock, Antoine Bordes, Pierre-Emmanuel et al. 2024. Rewardbench: Evaluating reward
Mazare, and Jason Weston. 2019. Learning from models for language modeling. arXiv preprint
dialogueafterdeployment: Feedyourself,chatbot! arXiv:2403.13787.
InProceedingsofthe57thAnnualMeetingoftheAs-
sociationforComputationalLinguistics,pages3667‚Äì HarrisonLee,SamratPhatale,HassanMansoor,Kellie
3684,Florence,Italy.AssociationforComputational Lu, Thomas Mesnard, Colton Bishop, Victor Car-
Linguistics. bune, and Abhinav Rastogi. 2023. Rlaif: Scalingreinforcementlearningfromhumanfeedbackwithai AnnaMakanju,KimMalfacini,SamManning,Todor
feedback. arXivpreprintarXiv:2309.00267. Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer,AndrewMayne,BobMcGrew,ScottMayer
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio McKinney, Christine McLeavey, Paul McMillan,
Petroni,VladimirKarpukhin,NamanGoyal,Hein- Jake McNeil, David Medina, Aalok Mehta, Jacob
richK√ºttler, MikeLewis, WentauYih, TimRock- Menick, Luke Metz, Andrey Mishchenko, Pamela
t√§schel, Sebastian Riedel, and Douwe Kiela. 2021. Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Retrieval-augmented generation for knowledge- Mossing,TongMu,MiraMurati,OlegMurk,David
intensivenlptasks. Preprint,arXiv:2005.11401. M√©ly,AshvinNair,ReiichiroNakano,RajeevNayak,
ArvindNeelakantan,RichardNgo,HyeonwooNoh,
HaoLiu,CarmeloSferrazza,andPieterAbbeel.2023a.
LongOuyang,CullenO‚ÄôKeefe,JakubPachocki,Alex
Chainofhindsightalignslanguagemodelswithfeed-
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
back. arXivpreprintarXiv:2302.02676.
tistaParascandolo,JoelParish,EmyParparita,Alex
Passos,MikhailPavlov,AndrewPeng,AdamPerel-
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
man,FilipedeAvilaBelbutePeres,MichaelPetrov,
RuochenXu, andChenguangZhu.2023b. G-eval:
Henrique Ponde de Oliveira Pinto, Michael, Poko-
NLGevaluationusinggpt-4withbetterhumanalign-
ment. In Proceedings of the 2023 Conference on rny,MichellePokrass,VitchyrH.Pong,TollyPow-
EmpiricalMethodsinNaturalLanguageProcessing, ell, Alethea Power, Boris Power, Elizabeth Proehl,
RaulPuri,AlecRadford,JackRae,AdityaRamesh,
pages2511‚Äì2522,Singapore.AssociationforCom-
CameronRaymond,FrancisReal,KendraRimbach,
putationalLinguistics.
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal, der,MarioSaltarelli,TedSanders,ShibaniSanturkar,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale- GirishSastry,HeatherSchmidt,DavidSchnurr,John
man,DiogoAlmeida,JankoAltenschmidt,SamAlt- Schulman, Daniel Selsam, Kyla Sheppard, Toki
man,ShyamalAnadkat,RedAvila,IgorBabuschkin, Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
SuchirBalaji,ValerieBalcom,PaulBaltescu,Haim- Shyam,SzymonSidor,EricSigler,MaddieSimens,
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir- JordanSitkin,KatarinaSlama,IanSohl,Benjamin
wanBello,JakeBerdine,GabrielBernadett-Shapiro, Sokolowsky, Yang Song, Natalie Staudacher, Fe-
ChristopherBerner,LennyBogdonoff,OlegBoiko, lipePetroskiSuch,NatalieSummers,IlyaSutskever,
MadelaineBoyd,Anna-LuisaBrakman,GregBrock- Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
man, Tim Brooks, Miles Brundage, Kevin Button, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
TrevorCai,RosieCampbell,AndrewCann,Brittany PrestonTuggle,NickTurley,JerryTworek,JuanFe-
Carey, Chelsea Carlson, Rory Carmichael, Brooke lipeCer√≥nUribe,AndreaVallone,ArunVijayvergiya,
Chan,CheChang,FotisChantzis,DerekChen,Sully ChelseaVoss,CarrollWainwright,JustinJayWang,
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben AlvinWang,BenWang,JonathanWard,JasonWei,
Chess,ChesterCho,CaseyChu,HyungWonChung, CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-
Dave Cummings, Jeremiah Currier, Yunxing Dai, ayiWeng,LilianWeng,MattWiethoff,DaveWillner,
Cory Decareaux, Thomas Degry, Noah Deutsch, Clemens Winter, Samuel Wolrich, Hannah Wong,
Damien Deville, Arka Dhar, David Dohan, Steve Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Dowling,SheilaDunning,AdrienEcoffet,AttyEleti, Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qim-
TynaEloundou,DavidFarhi,LiamFedus,NikoFelix, ingYuan,WojciechZaremba,RowanZellers,Chong
Sim√≥nPosadaFishman, JustonForte, IsabellaFul- Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
ford,LeoGao,ElieGeorges,ChristianGibson,Vik Zheng, Juntang Zhuang, William Zhuk, and Bar-
Goel,TarunGogineni,GabrielGoh,RaphaGontijo- ret Zoph. 2024. Gpt-4 technical report. Preprint,
Lopes, Jonathan Gordon, Morgan Grafstein, Scott arXiv:2303.08774.
Gray,RyanGreene,JoshuaGross,ShixiangShane
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Gu,YufeiGuo,ChrisHallacy,JesseHan,JeffHarris,
CarrollWainwright,PamelaMishkin,ChongZhang,
YuchenHe,MikeHeaton,JohannesHeidecke,Chris
SandhiniAgarwal,KatarinaSlama,AlexRay,John
Hesse,AlanHickey,WadeHickey,PeterHoeschele,
Schulman,JacobHilton,FraserKelton,LukeMiller,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Maddie Simens, Amanda Askell, Peter Welinder,
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
PaulFChristiano,JanLeike,andRyanLowe.2022.
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Traininglanguagemodelstofollowinstructionswith
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
humanfeedback. InAdvancesinNeuralInformation
woo Jun, Tomer Kaftan, ≈Åukasz Kaiser, Ali Ka-
ProcessingSystems,volume35,pages27730‚Äì27744.
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
CurranAssociates,Inc.
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ArjunPanickssery,SamuelR.Bowman,andShiFeng.
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
2024. Llmevaluatorsrecognizeandfavortheirown
≈Åukasz Kondraciuk, Andrew Kondrich, Aris Kon-
generations. Preprint,arXiv:2404.13076.
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Andi Peng, Yuying Sun, Tianmin Shu, and David
Leike, Jade Leung, Daniel Levy, Chak Ming Li, Abel.2024. Pragmaticfeaturepreferences: Learn-
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz ingreward-relevantpreferencesfromhumaninput.
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Preprint,arXiv:2405.14769.Dominic Petrak, Nafise Moosavi, Ye Tian, Nikolai ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Rozanov,andIrynaGurevych.2023. Learningfrom Melanie Kambadur, Sharan Narang, Aurelien Ro-
free-texthumanfeedback‚Äìcollectnewdatasetsor driguez,RobertStojnic,SergeyEdunov,andThomas
extend existing ones? In Proceedings of the 2023 Scialom.2023. Llama2: Openfoundationandfine-
Conference on Empirical Methods in Natural Lan- tunedchatmodels. Preprint,arXiv:2307.09288.
guageProcessing, pages16259‚Äì16279, Singapore.
AssociationforComputationalLinguistics. JelenaVranjes,GeertBr√¥ne,andKurtFeyaerts.2018.
Dualfeedbackininterpreter-mediatedinteractions:
AlecRadford,JeffreyWu,RewonChild,DavidLuan, Ontheroleofgazeintheproductionoflistenerre-
DarioAmodei,IlyaSutskever,etal.2019. Language sponses. JournalofPragmatics,134:15‚Äì30.
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9. Margaret G. Werts, Mark Wolery, Ariane Holcombe,
andDavidL.Gast.1995. Instructivefeedback: Re-
RafaelRafailov,ArchitSharma,EricMitchell,Christo- view of parameters and effects. Journal of Behav-
pherDManning,StefanoErmon,andChelseaFinn. ioralEducation,5(1):55‚Äì75.
2024. Directpreferenceoptimization:Yourlanguage
ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,Alane
modelissecretlyarewardmodel. AdvancesinNeu-
Suhr,PrithvirajAmmanabrolu,NoahASmith,Mari
ralInformationProcessingSystems,36.
Ostendorf, and Hannaneh Hajishirzi. 2024. Fine-
Adam Roberts, Hyung Won Chung, Gaurav Mishra, grainedhumanfeedbackgivesbetterrewardsforlan-
Anselm Levskaya, James Bradbury, Daniel Andor, guagemodeltraining. AdvancesinNeuralInforma-
SharanNarang,BrianLester,ColinGaffney,Afroz tionProcessingSystems,36.
Mohiuddin,etal.2023. Scalingupmodelsanddata
Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv,
with t5x and seqio. Journal of Machine Learning
Nathaniel Mills, Assaf Toledo, Eyal Shnarch, and
Research,24(377):1‚Äì8.
LeshemChoshen.2024. Genie: Achievinghuman
parityincontent-groundeddatasetsgeneration. arXiv
KeitaSaito,AkifumiWachi,KokiWataoka,andYouhei
preprintarXiv:2401.14367.
Akimoto. 2023. Verbosity bias in preference la-
beling by large language models. arXiv preprint
Alex Young, Bei Chen, Chao Li, Chengen Huang,
arXiv:2310.10076.
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu,JianqunChen,JingChang,KaidongYu,Peng
J√©r√©myScheurer,JonAnderCampos,JunShernChan,
Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shim-
AngelicaChen, KyunghyunCho, andEthanPerez.
ingYang,TaoYu,WenXie,WenhaoHuang,Xiao-
2022. Traininglanguagemodelswithlanguagefeed-
hui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie,
back. Preprint,arXiv:2204.14146.
Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai,
Weiyan Shi, Emily Dinan, Kurt Shuster, Jason We- ZhenyuGu,ZhiyuanLiu,andZonghongDai.2024.
ston, and Jing Xu. 2022. When life gives you Yi: Open foundation models by 01.ai. Preprint,
lemons,makecherryade: Convertingfeedbackfrom arXiv:2403.04652.
bad responses into good labels. arXiv preprint
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,
arXiv:2210.15893.
XingyaoWang,JiaDeng,BojiShan,HuiminChen,
Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen
ShivchanderSudalairaj,AbhishekBhandwaldar,Aldo
Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.
Pareja, Kai Xu, David D Cox, and Akash Srivas-
2024. Advancingllmreasoninggeneralistswithpref-
tava.2024. Lab: Large-scalealignmentforchatbots.
erencetrees. Preprint,arXiv:2404.02078.
arXivpreprintarXiv:2403.01081.
WentingZhao,XiangRen,JackHessel,ClaireCardie,
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
YejinChoi,andYuntianDeng.2024. Wildchat: 1m
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
chatgptinteractionlogsinthewild. arXivpreprint
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
arXiv:2405.01470.
Bhosale,DanBikel,LukasBlecher,CristianCanton
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
and Minlie Huang. 2024. Large language models
CynthiaGao,VedanujGoswami,NamanGoyal,An-
are not robust multiple choice selectors. Preprint,
thonyHartshorn,SagharHosseini,RuiHou,Hakan
arXiv:2309.03882.
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura, LianminZheng,Wei-LinChiang,YingSheng,Tianle
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di- Li,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
anaLiskovich,YinghaiLu,YuningMao,XavierMar- Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gon-
tinet,TodorMihaylov,PushkarMishra,IgorMoly- zalez, Ion Stoica, and Hao Zhang. 2023a. Lmsys-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- chat-1m: Alarge-scalereal-worldllmconversation
stein,RashiRungta,KalyanSaladi,AlanSchelten, dataset. Preprint,arXiv:2309.11998.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- LianminZheng,Wei-LinChiang,YingSheng,Siyuan
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,ZhuohanLi,DachengLi,EricP.Xing,HaoZhang, Therearefivedifferentpatternsinuserresponses
Joseph E. Gonzalez, and Ion Stoica. 2023b. Judg- subsequenttoerrorsinassistantutterances: Repeat
ingllm-as-a-judgewithmt-benchandchatbotarena.
orRephrase(UR1)-Theuserrepeatsorrephrases
Preprint,arXiv:2306.05685.
their concern, explaining again what they want.
Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Make Aware with Correction (UR2) - The user
2023. Judgelm: Fine-tuned large language points to the model that it was wrong, and pro-
models are scalable judges. arXiv preprint
videsinformationregardingtheerror/howtofixit.
arXiv:2310.17631.
"No,Iwanted..."MakeAwarewithoutCorrection
A ModelsandParameters (UR3) - The user points to the model that it was
wrong,withoutprovidinganyadditionalinforma-
All the models we used were released with a tion. "That‚Äôsincorrect"AskforClarification(UR4)
apache-2.0 license, except to llama3 which is re- - The user asks for additional resolution that was
leasedwiththellama3licenseandOpenAImodels expected to be in the the previous response, but
withtheirowntermsofuse. wasmissing. "Wasitlikethat?‚Äù,‚ÄúCanyouprovide
Torunourextractionprocess,werunthemodel acodesolutionforthis?‚ÄùPositiveFeedback(UR5)
with 0.2 temperature, 256 maximum new tokens, -Theuserconfirmsthatthemodeldidagoodjob,
top-p0.95,and1.0repetitionpenalty. Overall,the possiblythankingit. "Thankyou!",‚ÄúGreat!‚ÄùGiven
modelprocessedapproximatelyoneconversation these guidelines, please recognize such user re-
per10secondsonanNVIDIA40AGPU. sponsesinthefollowingdialogue. Notethatthere
Weusealearningrateof5e‚àí7andRMSprop mightbeconversationsthatcontainunsafe,offen-
optimizer. WeuseNVIDIARTX6000forthe1.4B sive or unsettling content. If you encounter such
model, NVIDIA A40 for the 2.8B and 2 A40 for conversation, please skip it. To speed up the an-
the 7B models. To fit our GPUs we restrict the notation,youcanstartbygoingthroughtheuser‚Äôs
maximum input length to 1024, and accumulate responsesonly,andthenifthereisafeedbackcan-
gradients to achieve a batch size of 32. We run didate,lookattheentireconversationcontext.
training for up to 20 epochs, and select the best
B.2 HumanEvaluationAnnotation
modelaccordingtoitsperformanceonthevalida-
tionset. Forthehumanevaluationannotationtaskweused
FortheKTOtraining,weusethesamehyperpa- thefollowingguidelines:
rametersasinthefinetuningexperiment,andtake Eachexampleincludesaprompt,andtwopossi-
the ones specific to KTO from an existing KTO bleassistantresponses: response_aandresponse_b.
implementation4. Your task is to choose the better response (‚ÄòA‚Äô or
Training each model took up to five days, de- ‚ÄòB‚Äô).Iftheresponsesseemsaboutthesame,oryou
pendingonitssizeandtheGPUused. areunsureaboutwhichonetopick,pick‚ÄòTIE‚Äô.
Toevaluatethemodels,weusethesamegenera- Notethatsomeofthepromptsalreadycontain
tionparametersasabove: 0.2temperature,256max someassistantresponses,astheyarethebeginning
newtokens,0.95topp,and1.0repetitionpenalty. ofamulti-trunconversation. Inthatcase,yourtask
WeusedNVIDIARTX6000forbothgenerating istochoosethebetternextassistantresponse.
theoutputsandforrunningtheopenrewardsmod- Example:
els. Generatingtheoutputstookuptotwodaysfor prompt:
the7Bmodelsandmuchlessforthesmallerones. # user: Describe the impact of climate change
Runningtherewardsmodelstookupto15minutes oncoastalcities.
foreach. UsingGPT-4asajudgecostus70$. #assistant: response_a:
# user: Describe the impact of climate change
B AnnotatorsInstructions
oncoastalcities.
# assistant: Climate change leads to rising sea
Herewedescribetheannotatorsguidelines.
levels, which can cause flooding in coastal cities.
B.1 ManuallyFeedbackAnnotation Italsoincreasesthefrequencyofextremeweather
events,damaginginfrastructureanddisplacingres-
Forthefeedbackannotationtask,theannotatorwas
idents. Efforts to mitigate these impacts include
giventhefollowingguidelines:
buildingseawallsanddevelopingbetterdrainage
4https://github.com/ContextualAI/HALOs systems. response_b:Given a conversation between a user and an assistant
model, please recognize spans of text that can inform
about the satisfaction of the user. There are two different patterns in user responses
These can be explicit/implicit expressions of satisfaction subsequent to errors in assistant utterances:
or dissatisfaction in the user responses.
Repeat or Rephrase (UR1) - The user repeats or
rephrases their concern, e.g., Actually, I wanted‚Ä¶
Given these guidelines, please recognize such user
Positive Feedback (UR5) - The user confirms that the
responses in the following dialogue. Please use the
assistant did a good job by directly saying so or
format:
thanking it, e.g., Thank you
{ Given these guidelines, please recognize such user
"User Response Text": [Insert User Response Text] responses in the following dialogue. Please use the
format:
"User Response Satisfaction (1-5)": [Insert Satisfaction
{
Level]
"User Response Pattern": [Insert User Response Pattern],
} "User Response Text": [Insert User Response Text]
}
If there is no feedback, use the following format:
If there is no feedback, use the following format:
{
{
"User Response Text": "" "User Response Pattern": "No Feedback",
"User Response Satisfaction (1-5)": -1, "User Response Text": ""
}
}
Figure7:ExtractionPromptfortheLimitingCategories
Figure6: ExtractionPromptfortheNoCategoriesset-
setting.
ting.
# user: Describe the impact of climate change
oncoastalcities.
#assistant: Climatechangeaffectscoastalcities There are five different patterns in user responses
subsequent to errors in assistant utterances:
by causing sea levels to rise and increasing the
Repeat or Rephrase (UR1) - The user repeats or
frequency of severe storms. These changes re-
rephrases their concern, e.g., Actually, I wanted‚Ä¶
sult in frequent flooding, infrastructure damage,
Make Aware with Correction (UR2) - The user makes
andforcedrelocationofcommunities. Strategies the system aware of the error and provides
information to address what is missing or wrong in its
tocombattheseeffectsincludeconstructingbarri-
utterance, e.g., No. I wanted you to‚Ä¶
erslikeseawalls,enhancingurbandrainage,and
Make Aware without Correction (UR3) - The user
implementingcomprehensivecoastalmanagement makes the system aware of the error without
providing additional information, e.g., You‚Äôre wrong.
plans.
Ask for Clarification (UR4) - The user asks for
Answer: ‚ÄòB‚Äô clarification, e.g., Are you sure? Is it really that‚Ä¶
Positive Feedback (UR5) - The user confirms that the
C AlternativeExtractionPrompts assistant did a good job by directly saying so or
thanking it, e.g., Thank you
Forthe‚ÄúNoCategory‚Äùsetting,weusetheprompt
Given these guidelines, please recognize such user
inFig.6. responses in the following dialogue. Please use the
format:
For the ‚ÄúLimiting Categories‚Äù setting, we use
{
thepromptinFig.7. "User Response Pattern": [Insert User Response Pattern],
"User Response Text": [Insert User Response Text],
For the Confidence Level setting, we use the
"Confidence Level (1-5)": [Insert Confidence Level]
promptinFig.8.
}
D AiAssistantsInResearchOrWriting If there is no feedback, use the following format:
{
"User Response Pattern": "No Feedback",
Weusedcopilotforwritingcodescripts,andalso
"User Response Text": "‚Äù,
usedChat-GPTalittleforsentencerephrasing.
"Confidence Level (1-5)": [Insert Confidence Level]
}
Figure8: ExtractionPromptfortheConfidenceLevel
setting.