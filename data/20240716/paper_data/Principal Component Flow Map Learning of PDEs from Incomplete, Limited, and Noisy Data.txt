PRINCIPAL COMPONENT FLOW MAP LEARNING OF PDES
FROM INCOMPLETE, LIMITED, AND NOISY DATA
VICTOR CHURCHILL∗∗
Abstract. Wepresentacomputationaltechniqueformodelingtheevolutionofdynamicalsys-
tems in a reduced basis, with a focus on the challenging problem of modeling partially-observed
partialdifferentialequations(PDEs)onhigh-dimensionalnon-uniformgrids. Weaddresslimitations
of previous work on data-driven flow map learning in the sense that we focus on noisy and limited
datatomovetowarddatacollectionscenariosinreal-worldapplications. Leveragingrecentworkon
modeling PDEs in modal and nodal spaces, we present a neural network structure that is suitable
for PDE modeling with noisy and limited data available only on a subset of the state variables or
computational domain. In particular, spatial grid-point measurements are reduced using a learned
lineartransformation,afterwhichthedynamicsarelearnedinthisreducedbasisbeforebeingtrans-
formedbackouttothenodalspace. Thisapproachyieldsadrasticallyreducedparameterizationof
theneuralnetworkcomparedwithpreviousflowmapmodelsfornodalspacelearning. Thisprimarily
allowsforsmallertrainingdatasets,butalsoenablesreducedtrainingtimes.
Keywords. Data-drivenneuralnetworkmodeling,partially-observedPDEs,noisylimiteddata
1. Introduction. Data-driven scientific machine learning has shown promise
for modeling time-dependent physical phenomena that do not have a known set of
governing equations (e.g. a physics-based model). Such phenomena arise frequently
in one-of-a-kind or custom systems, large and small, in applications relevant to our
societal well-being and national security as wide ranging as climate modeling, disease
preventation,andweaponscombustion. Usingpartialsystemobservationsintimeand
space, state-of-the-art methods train a predictive parameterized model, e.g. a neural
network (NN), that can explain unknown dynamics and aid in accelerated simulation
for optimization and control, e.g. in digital twin development.
1.1. Motivation. Whenmodelinghigh-dimensionalsystems,e.g. systemsmod-
eled traditionally by partial differential equations (PDEs) with one or more spatial
variables observed on a fine spatial grid, NN models require many parameters to ac-
countfordifferentsystembehaviors. Inmovingfromtoyproblemstorealapplications,
asignificantchallengethroughoutthefieldisthecurseofdimensionality,whichisthe
phenomenon that multiple system observations (training data samples) are required
to accurately learn each model parameter without over-fitting. The more complex
and high-dimensional a system is, the more parameters are needed in the model, and
hencethemoresystemobservationsareneededfortrainingdata. Ontheotherhand,
real-world data collection scenarios limit the spatial grid on which measurements are
made, the time at which observations are made, the state variables that are actually
observed, and the amount of data collected. If only a few noisy data are available to
train a highly parameterized model, accuracy and generalizability suffer. Therefore,
this paper addresses the immediate need for methods that require fewer parameters
to enable efficient and scalable data-driven modeling of unknown high-dimensional
dynamicswhilemaintainingpredictionaccuracyandstability,especiallywhenpartial
observations demand memory (i.e. a time history) of inputs. This paper focuses on
learningflowmapstoreducethenumberofmodelparameterssothatlimitedtraining
data can be used productively.
∗Department of Mathematics, Trinity College, Hartford, CT 06119, USA.
Email:victor.churchill@trincoll.edu.
1
4202
luJ
51
]LM.tats[
1v45801.7042:viXra1.2. Literature Review. Data-driven learning of unknown partial differential
equations(PDEs)hasbeenaprominentareaofresearchforthebetterpartofthelast
decade now. One approach is governing equation discovery (see e.g. [22, 23, 24],),
whereby a map from the state variables and their spatial derivatives to their time
derivatives is constructed. This technique can explicitly reconstruct the governing
equations of unknown PDEs from a large dictionary of partial derivative operators
using sparsity-promoting regression on solution data collected from the system. In
certain circumstances, exact equation recovery is even possible. While the sparsity
approach uses data advantageously, it does not apply to equations that are not well-
represented by a sparse number of elements in the prescribed dictionary. E.g., a
particular fractional derivative may not be in the dictionary, or a term in the PDE
may only be approximated by the sum of a large number of terms in the dictionary.
Another approach which has received significant attention is operator learning.
Recent network architecture advances such as DeepONet [16], neural operators [12,
13], and transformer operators [1, 14], seek to approximate infinite-dimensional op-
erators in a variety of problems related to PDEs such as continuous-time solution
operators that map a time t or spatial grid point x to the relevant PDE solution at
thatpoint. Becauseoftheirgeneralapproach,thesemethodsdonotexploittheprob-
lem of learning the evolution of an autonomous system of PDEs, e.g. to economize
the parameterization, and are highly data-intensive. While proven universal approx-
imation theorems suggest economical parameterizations, actual implementations are
quitelarge. Additionally,thesenetworksoftenhavedifficultywithaccuratelong-term
prediction accurately when extrapolating time t outside the training set.
ThispaperfocusesontheFlowMapLearning(FML)technique,whichconstructs
an approximate flow map between two discretized system states separated by a short
time. If an accurate approximation to the true flow map is constructed, it allows for
the definition of a predictive model for the unknown system that marches new initial
conditionsforwardintime. Thisapproachhasbeenshowntobeparticularlyeffective
when NNs, specifically residual networks (ResNet [9]), are used to approximate the
flow map. Originally designed for ODEs [6, 20], recent advances were also made with
respecttosystemstraditionallymodeledbyPDEs,[3,4,26]. In[26],FMLofPDEsis
conducted in a modal space (i.e. a generalized Fourier basis) by first representing the
observationsinareducedbasisandthenlearningtheflowmapofanassociatedODE.
In [3], FML of PDEs is conducted in nodal space (i.e. on a spatial grid) using an
architecture resembling a forward Euler scheme with finite difference approximations
topartialderivatives. ThroughouttheFMLframework, fully-connectednetworksare
used to accommodate complex interactions on non-uniform grids. This induces large
parameterizations for high-dimensional state vectors in both modal and nodal FML
implementations, requiring even larger training data sets to achieve generalizability
and high accuracy over a variety of initial conditions. Perhaps counterintuitively,
FMLmodelsbasedonfully-connectedNNspenalizehigh-dimensionalobservationsin
this way. The method we propose later is more ambivalent to the observation grid
size, and avoids that issue by conducting the learning in a reduced basis. Broadly,
there is a need to deal with limited data and non-trivial noise levels.
Of particular importance to this article within the FML framework is [4], which
extended [3] to incomplete or partially-observed PDE systems requiring memory (i.e.
a time history). The method there represented a generalization of [3] and [7], which
covered NN modeling of partially-observed ODE systems motivated by the Mori-
2Zwanzig formulation.1 The main drawback to the memory-based nodal PDE FML
frameworkisitshugeparameterization,whichrequireslargeamountsoftrainingdata
to avoid overfitting, and long training and prediction times.
Finally,twootherpapers,[15,21],havesimilargoalsoflearningflowsofdynamical
systems in a reduced basis, albeit in different contexts. In [21], authors consider a
partially-observedheatequationbutobservedatonly3pointsthroughoutthedomain,
with little need for reduction of high-dimensional measurements and focusing more
on reducing the dynamics. In [15], the authors consider a generalized Koopman
framework similarlyframedin thecontext oftheMori-Zwanzigformulation, butonly
consider a long (107 time steps), noiseless trajectory from the Lorenz ’96 ODE.
1.3. Ourfocusandcontributions. Muchlikein[4],thispaperconsidersdata-
driven flow map learning of PDEs from incomplete solution data. In other words, we
consider the case of modeling an unknown PDE system using measurable data that
represent partial information of the system. It is important to consider incomplete
solution data because in real applications it is often difficult, if not impossible, to
collectdataofallthestatevariablesofanunknownsysteminallareasoftherelevant
domain. Itmayevenbedifficulttoidentify allstatevariablesinanunknownsystem.
Examples in [4] used up to 10,000 noise-less training samples to train networks
with up to hundreds of thousands of parameters with high accuracy. This demon-
stratedthenot-so-surprisingresultthatwithnearlyunlimitedtrainingtimeanddata
generated from densely packed randomized initial conditions, arbitrary accuracy is
possible. However, the question now moves to the limited data scenario. As training
datacomesatacostinapplications,itisdesirabletominimizethenumberofparam-
eters in the model while still capturing accurate system dynamics. In what follows,
we limit data to no more than 100 samples in all examples, and add noise. This com-
bination of factors causes the nodal memory FML models of [4] to fail, motivating
this exploration.
To reduce the amount of data required to accurately generalize a NN flow map
model, the key factor is reducing the size of the model parameterization.2 This
depends on both the dimensionality of the observations, i.e. the grid size and the
numberofobservedstatevariables, aswellasthestructureofthenetworkitself. The
approach of this paper is to:
• Reduce the high-dimensional spatial measurements to a few important com-
ponentsbylearningcommonreducedrepresentationsfortrainingtrajectories;
• Reduce the dimensionality of the model (i.e. network parameterization) di-
rectly by conducting the dynamics learning in the reduced basis, which re-
quires a lower complexity network architecture due to its ODE form.
Therefore, the main contribution of the paper is a memory-based framework for flow
map learning of PDE in a learned lower-dimensional modal space from incomplete,
limited, and noisy data. In particular, to reduce the measurements, we introduce
unconstrained,constrained,andfixed(orpre-trained)approachestolearningcommon
reducedbasesforthetrainingtrajectoriesmotivatedbyprincipalcomponentanalysis
(PCA). To reduce the network parameterization, we derive the existence of an ODE
flowmapmodelandadditionallyproposeamatrixformtofurtherreducethenetwork
1The same modeling principle was first proposed in [25], in the context of closure for reduced
ordermodelingusinganLSTMstructure.
2Whilethereisagrowingliteratureonhighover-parameterizationtodealwithover-fittingwhen
facedwithlimiteddata(seee.g. thedoubledescentcurvein[1]),thepropertyhasnotbeenobserved
inthiscontext. Indeed,inexamplesbelowhighover-parameterizationstillresultsinover-fitting.
3parameterization. Finally, an adjustment to the overall ResNet structure radically
improves the FML strategy’s robustness to noise. In a set of numerical examples, we
compare these options against each other as well as the existing nodal model from [4]
to demonstrate that our method is indeed highly effective and accurate.
2. Preliminaries. Inthissection,webrieflyreviewtheexistingNN-basedframe-
workforlearningtheevolutionofunknownPDEsinmodalspace,aswellaspartially-
observed PDEs in nodal space. Throughout this paper our discussion will be over
discrete time instances with a constant time step ∆t,
t <t <··· , t −t =∆t, ∀n. (2.1)
0 1 n+1 n
Wewillalsousesubscripttodenotethetimevariableofafunction,e.g.,u =u(x,t ).
n n
2.1. FML of PDEs in Modal Space. The flow map learning approach was
first applied to modeling PDEs in [26]. The idea was that when the solutions of a
PDE can be expressed using a fixed basis, the learning can be conducted in modal
space. There, data was generated as modal or generalized Fourier space coefficients,
such that
n
(cid:88)
u(x,t)= uˆ (t)ϕ (x) (2.2)
j j
j=1
where u (t) are the coefficients and the ϕ (x) represent a pre-chosen set of basis
j j
functions in a finite-dimensional subspace of the infinite-dimensional space where the
solutions live. Letting
uˆ(t)=(uˆ (t),...,uˆ (t))∈Rn, (2.3)
1 n
the authors sought to model the flow map of the resulting autonomous ODE system
duˆ
(t)=f(uˆ(t)), (2.4)
dt
where f :Rn →Rn represents the unknown governing equations for the coefficients.
Per the FML literature on ODEs [6, 20], the flow map of the system is a function
that describes the evolution of the solution. The flow map of (2.4) depends only on
the time difference but not the actual time, i.e., uˆ =Φ (uˆ ). Thus, the solution
n tn−ts s
over one time step satisfies
uˆ =Φ (uˆ )=uˆ +Ψ (uˆ ), (2.5)
n+1 ∆t n n ∆t n
where Ψ = Φ −I, with I as the identity operator. When data for the state
∆t ∆t
variablesuˆ overthetimestencil(2.1)areavailable,theycanbegroupedintosequences
separated by one time step
{uˆ(l)(0), uˆ(l)(∆t),...,uˆ(l)(K∆t)}, l=1,...,L, (2.6)
where L is the total number of such data sequences and K +1 is the length of each
sequence3. These data sequences form the training data set. Inspired by basic one-
stepnumericalschemesforsolvingODEs,onecanthenmodeltheunknownevolution
operator using a residual network (ResNet [9]) of the form
yout =[I+N](yin), (2.7)
3ThedatasequencelengthKisassumedtobeconstantfornotationalconvenience. Ifinfactdata
areofdifferentlengths,chunksofaminimumsequencelengthcanbetakenfromlongertrajectories.
4where N:Rn →Rn stands for the mapping operator of a standard feedforward fully
connectedneuralnetwork. Thenetworkisthentrainedbyusingthetrainingdataset
and minimizing the recurrent mean squared loss function
1 (cid:88)L (cid:88)K (cid:13) (cid:13)2
(cid:13)uˆ(l)(k∆t)−[I+N]k(uˆ(l)(0))(cid:13) , (2.8)
L (cid:13) (cid:13) 2
l=1k=1
where [I+N]k indicates composition of the network function k times. This recurrent
loss permits the incorporation of longer data lengths and is used to increase the
stability of the network approximation over long term prediction. Once the network
is trained to satisfactory accuracy, the trained network thus accomplishes
uˆ(l)(k∆t)≈[I+N]k(uˆ(l)(0)), ∀l=1,...,L, k =1,...,K,
and it can be used as a predictive model
uˆ =uˆ +N(uˆ ), n=0,1,2,..., (2.9)
n+1 n n
for initial condition uˆ(t ). If desired, nodal approximations can be formed via (2.2).
0
2.2. FML of Partially-Observed PDEs in Nodal Space. Whendataofthe
PDE solutions are available as nodal values over a set of unstructured grid points in
physical space, its NN learning is more involved. In [4], the NN structure for nodal
FML of PDEs from [3] was extended to partially-observed systems.
Decomposingthestatevectoru=(v;w)intoobservablesvonanN -dimensional
O
grid and missing information w,4 the authors took inspiration from memory-based
FML for ODEs [7], as well as the Mori-Zwanzig formulation [17, 27], and derived an
approximate flow map for v only using a finite length of time history or memory.
Similar to [3], the NN structure of [4] is based on a simple numerical scheme for
solvingthePDE,andconsistsofasetofspecializedlayersresemblingthecombination
of differential operators involved in the unknown PDE. Specifically, the NN model
defines the following mapping,
v =v +A(F (v ,...,v ),...,F (v ,...,v )), (2.10)
n+1 n 1 n n−(NM−1) J n n−(NM−1)
where F ,...,F are the operators for the disassembly channels and A the operator
1 J
fortheassemblylayer,andN ≥1isthenumberofmemorytermsinthemodel. The
M
NNmodelingapproachwasshowntobehighlyflexibleandaccuratetolearnavariety
ofPDEs. See[3,4]formoredetailsonthisstructureanditsmathematicalproperties.
Since this configuration can also be viewed as a residual network application, i.e.
v =[I+N](v ,...,v ) where N=A◦(F ,...,F ), if data sequences of
n+1 n n−(NM−1) 1 J
length N +K
M
{v(l) ,...,v(l) ,v(l),v(l) ,...,v(l) }, l=1,...,L, (2.11)
n−(NM−1) n−1 n n+1 n+K
are available, then we can directly apply a recurrent loss scheme
1 (cid:88)L (cid:88)K (cid:13)
(cid:13)v(l)
−[I+N]k(cid:16)
v(l),...,v(l)
(cid:17)(cid:13) (cid:13)2
, (2.12)
L (cid:13) n+k n n−(NM−1) (cid:13)
l=1k=1
4Thisgeneralformulationconsidersmissingstatevariablesormissingareasofthedomain.
5where[I+N]k indicatescompositionofthenetworkfunctionktimes,usingpredictions
as inputs for k > 1. The special case of N = 1 corresponds to the standard model
M
from [3] for modeling PDEs without missing information (thus no need for memory).
Asaforementioned,themainissuewiththisconfigurationisthelargeparameterization
whichinturnisverydatathirsty. Figure2.1showsthestructure. Thistechniquewill
serve as the baseline to which we will compare the several proposed models below.
Disassembly 𝒟
Input: N MN O N O
N O Output: N O
vn−(NM−1)
J Assembly 𝒜
vn+1
vn−1 Na
Nc
vn Nd
Fig. 2.1: FML model for nodal PDEs with missing information.
3. Principal Component Flow Map Learning (PC-FML). In this main
section, we describe a new method for approximating the flow map of partially-
observed systems of PDEs using NNs. We first set up the problem being considered
with appropriate notation, then move on to motivate and discuss the method.
3.1. Problem Setup and Data Collection. Consider an autonomous time-
dependent system of PDEs,

u =L(u), (x,t)∈Ω×R+,
 t
B(u)=0, (x,t)∈∂Ω×R+, (3.1)
u(x,0)=u (x), x∈Ω¯,
0
whereu,thevectorofstatevariables,hasN
var
elements,Ω⊂RNd,N
d
=1,2,3,isthe
physical domain, and L and B stand for the PDE operator and boundary condition
operator, respectively, of the entire system. We assume that the PDE is unknown.
We assume that solution data of the PDE are only partially observable. First,
let u = (v;w), where v contains the N state variables with available data and
obs
w contains the N −N unobserved subset of state variables. Furthermore, we
var obs
assume that v is observed at a set of points,
X ={x ,...,x }⊂Ω. (3.2)
Ngrid 1 Ngrid
The set X is quite a general grid that may in fact be a discretization of some
Ngrid
proper subset of Ω such that Conv(X ) ⊊ Ω where Conv(X ) is the convex
Ngrid Ngrid
hull of X . We emphasize that the difference can be nontrivial and also represent
Ngrid
(cid:12) (cid:12)
an incomplete or partially-observed measurement, i.e. (cid:12)Ω\Conv(X Ngrid)(cid:12) = O(1)
where |·| represents the Lebesgue measure.
6Using vector notation, we concatenate
V(t)=(V 1(t),...,V Nobs(t))T ∈RNobsNgrid, (3.3)
where each component of the subset of state variables is given by
V j(t)=(v j(x 1,t),...,v j(x Ngrid,t))∈RNgrid, j =1,...,N obs. (3.4)
These state variables are observed at discrete time instances (2.1) such that the set
(cid:110) (cid:111)
V(l) ,...,V(l),V(l) ,...,V(l) , l=1,...,N , (3.5)
n−(Nmem−1) n n+1 n+Nrec traj
represents all available data similar to (2.11) where N ≥1 is the memory length,
mem
N is the recurrent loss parameter, and N is the total number of trajectories
rec traj
available. Note that we continue to use the compact notation, V = V(t ). Ac-
n n
counting for each of the N training samples with length of each vector N N
traj obs grid
observed over the memory and recurrent length N +N , the dataset is of size
mem rec
N ×(N N )×(N +N ). From the data (3.5), our goal is to model the
traj obs grid mem rec
flow map
V(x,t+∆t)=Φ (V(x,t)) (3.6)
∆t
for the reduced system of observed variables V on the grid x. This construction is
fundamentally different than [26], which sought to model the modal coefficient flow
map (2.9) whereas this study models the nodal flow map (3.6).
Additionally, we highlight that the data collection scenario is different than in
[26]. There,nodaldataweregeneratedfrommodalspacecoefficientsbyfirstchoosing
a basis {ϕ (x)}n as in (2.2), implying a known invertible map between modal and
j j=1
nodal measurements. Here the spatial measurement data are directly collected from
physical or nodal space with no knowledge of ideal modal spaces, with finding an ap-
propriatebasisoneofthepremisesofthisexploration. Neithertheexistenceofnorthe
form of particular basis functions of a map between nodal and modal measurements
are known and therefore must be discovered.
3.2. Mathematical Motivation.
3.2.1. Reduced basis for nodal measurements. We begin reducing the pa-
rameter space of the NN model by reducing the dimension of the observations. For
ease of presentation, we define N = N N and consider the observable state
full obs grid
vector V
n
∈ RNfull of an unknown PDE at time t
n
such as in (3.1), recalling that
N such vectors represent the entire input.
mem
We assume that there exists a set of basis functions in the physical domain Ω,
{b
(x)}Nfull,
(3.7)
j j=1
such that solutions V admit an accurate N -term series approximation
n red
V ≈V˜
=N (cid:88)red
Vˆ (t)b (x), (3.8)
n n j j
j=1
with N <N , incurring error
red full
(cid:13) (cid:13)2
δ(l) =(cid:13)V(l)−V˜ (cid:13) , (3.9)
n (cid:13) n n(cid:13)
2
7for l = 1,...,N and n = 1,...,N +N as in (3.5). Generally, if N were
traj mem rec red
fixed, we might seek an optimal basis of the form (3.7)
N (cid:88)trajNme(cid:88)m+Nrec
{b∗(x)}Nred =arg min δ(l). (3.10)
j j=1 n
{bj(x} jN =r 1ed l=1 n=1
that accurately approximates the solutions at all trajectories and time steps in the
training data. We note that an optimal basis may be significantly different from
thosegeneratingeachtraininginstance’sexactfinite-termseriesrepresentationofthe
finite-dimensional grid measurements.
More simply put, we assume that the solutions to the unknown PDE admit a
common linear approximation. As will be demonstrated in the numerical examples,
this need not be a particularly accurate approximation and at times (especially with
noisy measurements) higher values for N are not advantageous. This assumption
red
limitsthestudytofunctionswell-approximatedbyacommonlinearbasis. Ofcourse,
there are solutions to PDEs that do not admit accurate linear reduced order approxi-
mations, e.g. traveling waves with discontinuous initial conditions or linear transport
[8]. The viability of solutions to PDEs having accurate linear reduced order approxi-
mations can be quantified by the decay of the Kolmogorov n-width [19]. We plan to
consider more general bases in future work.
Similar to the definition of V in (3.3), let the concatenation of the modal expan-
sion coefficients be
(cid:16) (cid:17)T
Vˆ = Vˆ (t ),...,Vˆ (t ) . (3.11)
n 1 n Nred n
Considering these definitions, we define two linear transformations
Π
in
∈RNred×Nfull, such that Π inV
n
=Vˆ n,
(3.12)
Π
out
∈RNfull×Nred, such that Π outVˆ
n
=V˜ n,
which transform the nodal measurements to the reduced modal space and back out.
A focus of the ensuing technique is on finding matrices Π and Π , that bal-
in out
ance minimization of the error δ(l) and the reduced dimension N over the entire
n red
training dataset (3.5). As described in Section 3.5, several approaches are taken, mo-
tivatedbyaprincipalcomponentanalysis(PCA),alsoknownasprincipalorthogonal
decomposition (POD), approach. E.g., perhaps the most straightforward approach
is to use fixed transformations based on the PCA weight matrix which can be pre-
computed from the training data. This would enforce an accurate approximation of
V by Π Π V along with Π =ΠT and Π having orthogonal columns.
n out in n in out out
3.2.2. Partial reduced dynamics. Motivated by the modal approximation
(3.8), there exists an unknown formal system of ODEs for the modal expansion coef-
ficients Vˆ ∈RNred of the form,
dVˆ
=f(Vˆ(t)), (3.13)
dt
the solution of which has an approximate correspondence to the solution of the PDE
in V. This permits learning the evolution of the system in RNred instead of RNfull.
8When the governing PDE is unknown, the system for Vˆ is unknown as well. Since V
represent partial observations of the entire PDE system, Vˆ represent partial observa-
tions of some entire system, say for U˜, the modal coefficients of some approximation
of the full system.
We seek to discover the dynamics for only the observables Vˆ in this reduced
space, the evolution of which is non-autonomous per the Mori-Zwanzig formalism
[18, 28], and follows a generalized Langevin equation. Referring to [7] for details, a
time-discretized finite memory approximation models the coefficient flow map as
(cid:16) (cid:17)
Vˆ =Vˆ +M Vˆ ,...,Vˆ . (3.14)
n+1 n n n−(Nmem−1)
whereN
mem
≥1isthenumberofmemorytermsinthemodel5,andM:RNmemNred →
RNred is a fully-connected NN.
Since data is collected in the nodal space, all of the modal input terms in (3.14)
can be viewed as nodal terms by factoring out Π per (3.12), retrieving
in
(cid:0) (cid:1)
Π V =Π V +M Π V ,...,Π V . (3.15)
in n+1 in n in n in n−(Nmem−1)
Furthermore, we can transform the model outputs to RNfull by applying Π out,
(cid:0) (cid:1)
Π Π V =Π Π V +Π M Π V ,...,Π V . (3.16)
out in n+1 out in n out in n in n−(Nmem−1)
This suggests the following model for the nodal space flow map as
V¯ =Π Π V¯ +Π M(cid:0) Π V¯ ,...,Π V¯ (cid:1) , (3.17)
n+1 out in n out in n−1 in n−(Nmem−1)
where M approximates the modal space coefficient flow map, and the bar notation
denotes approximate quantities. After extensive numerical experimentation, we find
thatthetransformationofV¯ intoandoutofthereducedspaceisessentialtodealing
n
with noisy nodal measurements. Essentially, Π Π functions as a denoiser. If
out in
measurements are noise-free, it can be removed.
3.3. Numerical Approach. Per the motivation above, we propose to model
the approximate flow map (3.17) using NNs. That is, network operations will learn
the operators Π , Π and M. The overall network operation, shown in Figure 3.1,
in out
consists of:
• Input reduction: each of the N -length state vectors in the N -length
full mem
memory period are reduced to modal space via a matrix P
in
∈ RNred×Nfull,
after which they are concatenated into a N N -length vector.
mem red
• Flow map network: a fully-connected neural network M : RNmemNred →
RNred maps the reduced time history to the modal residual.
• Residual expansion: expands the modal residual to a nodal residual via a
matrix P
out
∈RNfull×Nred.
• Skip connection:addsthecurrentnodaltimestephavingbeenprojectedinto
the reduced space.
Together, the network operation is:
V¯ =P P V¯ +P M(cid:0) P V¯ ,P V¯ ,...,P V¯ (cid:1) (3.18)
n+1 out in n out in n in n−1 in n−(Nmem−1)
5ThenumberofmemorystepsNmem isproblem-dependentonboththelengthofthetime-step
andthedynamics. Weplantoevaluateitsapriorioptimizationinafutureinvestigation.
9Input: N memN full
Modal Input: N memN red
Flow Map Network: M
Vn−2 P in
V̂ n−2 Modal Residual: N red Nodal Residual: N full Output: N full
P
Vn−1
P
in V̂ n−1
Nc out
ΔV̂
n+1
ΔVn+1 Vn+1
P in V̂ n Nd
Vn
P P
out in
Fig. 3.1: The proposed PC-FML network structure, with N =3.
mem
We will refer to this as Principal Component Flow Map Learning (PC-FML) model.
Generally,thecomponentsP ,P ,andMrepresentthetrainableparametersinthe
in out
network. However,Section3.5providesseveraloptionsfortheirspecification. Wenote
that compared with the partially-observed PDE flow map learning of [4], modeling
via (3.18) has the potential to significantly reduce the number of parameters in the
model. E.g., P and P represent 2N N parameters as they are matrices,
in out full red
while at minimum M represents N N2 parameters. Meanwhile, in [4], networks
mem red
had at least a multiple (typically ≥5 times) of N N2 parameters.
mem full
3.4. Determining reduced order dimension. Given the N × N ×
traj full
(N +N )datatensorin(2.6),thePC-FMLapproachrequiresspecificationofthe
mem rec
modaldimensionN . AssembleandreshapethedataintoanN (N +N )×
red traj mem rec
N matrix, sayD. Thismatrixrepresentsinrowsthetotalnumberofobservations
full
in the training dataset and in columns the dimension of the observations. Compute
its singular value decomposition (SVD): D = UΣVT. Looking at the decay of the
singular values, we can determine an appropriate reduced order dimension N (or
red
if one exists). By comparing D and D = U Σ VT , the associated rank-N
red red red red red
truncated SVD approximation, we can further examine if N is appropriate. E.g.,
red
wemaycheckthelargestabsolutedeviationoftheapproximationoveralltrajectories.
Different values of N can be tested for accuracy as a small pre-computational cost
red
which has no requirements on the amount of training data.
Figure 3.2 shows the decay of the singular values in each of the three forthcom-
ing numerical experiments with and without noise. We see, e.g., in Example 3 the
noiselesssingularvaluesofthetrainingdatadropoffgraduallyafterN =13, while
red
noise is dominant after N = 5 in the noisy observations. Additionally, this SVD
red
computationexposespotentialbasiselementsb (x)forj =1,...,N inthecolumns
j red
of V , which can be used to explain the decomposition and interpret the dynamics.
red
Figures 3.3 and 3.4 show the 13- and 5-dimensional bases for the noiseless and noisy
cases,respectively,inExample3. Weseesimilarshapesforthefirst5elements,while
we also notice noise creeping into the b (x) and b (x) in the noisy measurements.
4 5
This demonstrates why after this point, including extra basis elements is not advan-
tageous as they will have a large noise component that may actually be detrimental
10100 100 100
10-1 10-2 10-2
10-2 10-4 10-4
10-3 10-6 10-6
10-4 10-8 10-8
10-5 10-10 10-10
10-6 10-12 10-12
10-7 10-14 10-14
10-8 10-16 10-16
10-9 100 101 102 10-18 100 101 10-18 100 101 102 103
Fig. 3.2: Exs. 1, 2, 3 (left to right): Determining reduced order dimension.
to reconstructing the unknown dynamics.
Fig. 3.3: Ex. 3 (noiseless): Reduced basis elements b (x) for Fixed PC-FML.
j
Fig. 3.4: Ex. 3 (noisy): Reduced basis elements b (x) for Fixed PC-FML.
j
3.5. Model Training. With N fixed, we consider three options for training
red
the network, which correspond to options for choosing P
in
∈RNred×Nfull and P
out
∈
11RNfull×Nred. In all cases, the network operation is
(cid:0) (cid:1) (cid:0) (cid:1)
N V ,...,V ;Θ,P ,P =P P V +P M P V ,...,P V ;Θ
n n−(Nmem−1) in out out in n out in n in n−(Nmem−1)
(3.19)
To ease the notational burden, let the main component of the recurrent loss be
L K
L(Θ,P ,P )= 1 1 (cid:88)(cid:88) ∥Nk(V(l),...,V(l) ;Θ)−V(l) ∥2, (3.20)
in out LK n n−(Nmem−1) n+k 2
l=1k=1
wherethepowerk denotescompositionofthenetworkwithitselfk timesinthemode
of (3.24). We consider the following options:
• Fixed: One of the main inspirations for this new method was to use a fixed
linear transformation such as principal component analysis (PCA, see e.g.
[10]) to construct the maps P and P . Therefore, consider P and P
in out in out
non-trainable and fixed in the following “offline” way. As in determining an
appropriate reduced order dimension N as in Section 3.4, compute the
red
rank-N truncated SVD of the data matrix D, D = U Σ VT . Fix
red red red red red
P =VT and P =V and train the NN model (3.17) with the dataset
in red out red
(3.5) by minimizimg the recurrent mean squared loss:
min(cid:8) L(Θ,VT ,V )(cid:9) . (3.21)
red red
Θ
This fixed option allows us to control the modal basis.
• Constrained: Consider P and P to be trainable, but constrained to be
in out
transposes of one another. Furthermore, impose regularization to encourage
orthogonality,thatP P ≈I . Inthiscase,wetraintheNNmodel
in out Nred×Nred
(3.17) with the dataset (3.5) by minimizing the recurrent mean squared loss
balanced with a regularization penalty:
(cid:26) (cid:27)
Θ,Pm ini ,n
Pout
L(Θ,P in,P out)+ λ
2
(cid:13) (cid:13)P inP iT n−I NR×NR(cid:13) (cid:13)2
2 (3.22)
subject to P =PT.
out in
In numerical examples below, we fix λ = 10−2. This constrained option
lets the data choose the appropriate modal bases subject to constraints that
mimic those obtained in the SVD approach.
• Unconstrained: Consider P and P to be unconstrained matrices of
in out
trainableweights. Inthiscase,wetraintheNNmodel(3.17)withthedataset
(3.5) by minimizing the recurrent mean squared loss:
min {L(Θ,P ,P )}. (3.23)
in out
Θ,Pin,Pout
This unconstrained option relies completely on the data to choose an appro-
priate modal basis.
Essentially,theoptionscorrespondtotheamountoffreedomwegivethedataandthe
network to learn the transformation. Indeed the inspiration for the general method
wastheFixedregime,whichcanberelaxedtotheConstrainedregime’sorthogonality
and symmetry or completely dropped in the Unconstrained regime.6 Notably, in all
6WeadmitthatthenetworkmayfindabetterFMLbasis!
12cases the loss is measured in the nodal domain. This is significant, and differs from
[2,26],sincethenetworkparametersaretrainedtominimizelossinthenodaldomain.
If instead training is only executed in the modal domain, expanding back out could
cause larger errors in the nodal domain, which is the focus.
3.6. ModelPrediction. Oncetrained,i.e. onceΘ∗,P∗,P∗ havebeenchosen,
in out
we obtain a predictive model over the grid set X for the underlying unknown
Ngrid
PDE,suchthatgivenanewinitialcondition(V ,...,V )T overX , wehave
Nmem 1 Ngrid
(cid:40)
(V¯ ,...,V¯ )=(V ,...,V ),
Nmem 1 Nmem 1 (3.24)
V¯ =N(V¯ ,...,V¯ ;Θ∗,P∗,P∗ ), k =1,2,...
Nmem+k Nmem+k k in out
where again the bar denotes the model’s approximation.
4. Computational Studies. Inthissection, wepresentseveralchallengingnu-
merical examples to demonstrate the PC-FML approach for learning PDEs from in-
complete information. In each, the true governing PDEs are in fact known, but are
used only to generate training and testing data. Their knowledge does not facili-
tate the PC-FML model construction. In other words, this is a purely data-driven
approach. Full specification of the data generation as well as network structure pa-
rameters are provided in each example.
In an effort to avoid hand-picking results, all examples share the following prop-
erties and parameters:
• Time-step: fixed at ∆t=10−2.
• Memory: fixed at N =20 steps.
mem
• Recurrent loss: fixed at N =10 steps.
rec
• Trajectories: limited to N =100 training trajectories.
traj
• Noise: additive zero-mean Gaussian noise is considered at two noise levels,
with standard deviations σ = 0 and σ = 10−1, referred to generically as
noiseless and noisy.
• PC-FML network: networks M have 3 hidden layers with either 10 (Ex. 1)
or 15 (Exs. 2 and 3) nodes per layer. Hyperbolic tangent is used as the
activation function.
• Nodal FML network size: comparison networks from [4] have 5 disassembly
channels with 1 hidden layer with either N (Exs. 1 and 2) or N nodes
grid red
per layer. The assembly layer has the same number of nodes. Hyperbolic
tangent is used as the activation function.
• Optimization: in all cases, the Adam optimizer [11] is used to train the
networks for 10,000 epochs using a fixed learning rate of 10−3.
• Ensembleprediction: bothPC-FMLandnodalFMLuseensembleprediction
[5], which sequentially uses an average of 10 trained models as the input to
predict the next time step.
4.1. Example 1: 1D heat equation on incomplete non-uniform grid. We
first consider the heat equation
π2u =u (4.1)
t xx
onthedomainΩ=[0,1]withzeroDirichletboundaryconditions. Theoverallbehav-
ior of the PDE is characterized by dissipation.
13Problem Model N σ N
red params
Unconstrained PC-FML 2 0,10−1 1052
Constrained PC-FML 2 0,10−1 852
Example 1
Fixed PC-FML 2 0,10−1 652
Nodal FML N/A 0,10−1 1051036
Unconstrained PC-FML 5 0,10−1 2575
Constrained PC-FML 5 0,10−1 2325
Example 2
Fixed PC-FML 5 0,10−1 2075
Nodal FML N/A 0,10−1 263036
Unconstrained PC-FML 13 0 44565
Constrained PC-FML 13 0 24584
Fixed PC-FML 13 0 4603
Nodal FML N/A 0 2105791
Example 3
Unconstrained PC-FML 5 10−1 17445
Constrained PC-FML 5 10−1 9760
Fixed PC-FML 5 10−1 2075
Nodal FML N/A 10−1 814671
Table 4.1: Comparison of FML model parameterizations for Exs 1, 2, 3.
To generate training data, first we solve the PDE from N = 100 randomized
traj
initial conditions of the form
u(x,0)=α sin(πx)+α sin(2πx) (4.2)
1 2
where α ,α ∼U[−1,1], until T =2 (200 time steps). One chunk of length N +
1 2 mem
N is selected randomly from each trajectory to form the training data set. The
rec
solutions are observed on a randomly-chosen non-uniform grid of N =100 points
grid
representing roughly the middle 50% of the domain, [.2399,.7577]⊂Ω.
ThemethodfromSection3.4isusedtodetermineanappropriatereducedbasisof
dimensionN =2forbothσ =0,10−1. Figure3.2(left)showsthedetails. Asindi-
red
catedbythetrainingdatageneration,b (x)=sin(πx)andb (x)=sin(2πx)represent
1 2
optimal basis functions, however this fact is not used in the PC-FML approach.
Validation testing also uses 100 trajectories drawn similarly to (4.2). On the left
and center of Figures 4.1 (noiseless) and 4.2 (noisy), an example test trajectory is
shown at various prediction times, and on the right the average absolute ℓ error
2
the test trajectories is shown over 500 time steps. We see that the Fixed and Uncon-
strainedPC-FMLstrategiesstaystableandsmoothovertheentirepredictionoutlook,
while the nodal FML deteriorates rapidly. Interestingly, we see the Constrained PC-
FML approach successful in the noisy case, while it appears to get “stuck” after a
short time in the noiseless experiment. We hypothesize that perhaps without a good
initialization(nospecialsettingisusedhere),theConstrainedapproachfailstofindan
appropriate basis that abides by its constraints. Additionally, the denoising behavior
of PC-FML approach is evident.
WenotethesuccessofthesemethodsoutuntilT =10despitehavingonly“seen”
dynamicsuptoT =2inthetrainingdata. Table4.1showsthatthefailingnodalFML
model uses 151036 parameters, while the more successful PC-FML models use just
528,328,and128fortheUnconstrained,Constrained,andFixedoptions,respectively.
140.24 t=1 0.1 t=4 100 average absolute l2 error over 100 trajectories
Reference
Nodal FML
0.22 U Cn oc no sn trs at ir na ein de Pd C P -FC M-F LML
Fixed PC-FML
0.2 0.05
0.18
10-1
0.16 0
Reference
0.14 N Uo nd coa nl F stM raL ined PC-FML
Constrained PC-FML
Fixed PC-FML
0.12 -0.05
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
x x
10-2
1 t=7 8 t=10
Reference
0.8 Nodal FML
0.6 6 U Cn oc no sn trs at ir na ein de Pd C P -FC M-F LML
Fixed PC-FML
0.4 4
0.2 10-3
0 2
-0.2
-0.4 0
-0.6 R N Ue o nf d ce oar nle F sn tMc rae L ined PC-FML -2 N Uo nd coa nl F stM raL ined PC-FML
-0.8 Constrained PC-FML Constrained PC-FML
Fixed PC-FML Fixed PC-FML
-1
0 0.2 0.4 0.6 0.8 1
-4
0 0.2 0.4 0.6 0.8 1
10-4
0 1 2 3 4 5
x x t (seconds)
Fig.4.1: Ex. 1(noiseless): Exampletrajectory(left,center)andaverageerror(right).
0.6 t=1 0.25 t=4 average absolute l2 error over 100 trajectories
Reference Nodal FML
Nodal FML 0.2 6 Unconstrained PC-FML
0.5 Unconstrained PC-FML Constrained PC-FML
C Fio xn es dt r Pa Cin -e Fd M P LC-FML 0.15 5.5 Fixed PC-FML
0.4 0.1 5
0.3 0.05 4.5
0
0.2 -0.05 4
0.1 -0.1
-0.15 R Ne of de ar le Fn Mce L 3.5
0 Unconstrained PC-FML
-0.2 Constrained PC-FML
-0.1 -0.25 Fixed PC-FML 3
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
x x
2.5
0.25 t=7 0.25 t=10
0.2 0.2 2
0.15 0.15
0.1 0.1
0.05 0.05
1.5
0 0
-0.05 -0.05
-0.1 -0.1
-0.15 R Ne of de ar le Fn Mce L -0.15 R Ne of de ar le Fn Mce L
Unconstrained PC-FML Unconstrained PC-FML
-0.2 Constrained PC-FML -0.2 Constrained PC-FML
-0.25 Fixed PC-FML -0.25 Fixed PC-FML 1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5
x x t (seconds)
Fig. 4.2: Ex. 1 (noisy): Example trajectory (left, center) and average error (right).
4.2. Example 2: 1D wave system with missing variables. Next we con-
sider the system
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)
v 0 1 v
= , (4.3)
w 1 0 w
t x
15on the domain Ω = [−1,1] with periodic boundary conditions. We consider the case
when only data on the solution component v are available (i.e. N =1) and aim at
obs
constructing an accurate NN predictive model for the evolution of v.
To generate training data, first we solve the PDE analytically until T = 1 (100
time steps) from N =100 randomized initial conditions given by
traj
(cid:20) v(cid:21) (cid:34)
a
+(cid:80)Nb
(a cos(nx)+b
sin(nx))(cid:35)
(x,0)= 0 n=1 n n , (4.4)
w c +(cid:80)Nb (c cos(nx)+d sin(nx))
0 n=1 n n
with a ,c ∼U[−1,1] and a ,b ,c ,d ∼U[−1, 1], and N =2. Then, one random
0 0 2 2 n n n n n n b
chunkoflengthN +N pertrajectoryfromv onlyarerecordedintoourtraining
mem rec
data. The solutions are observed on a uniform grid of N =50 points.
grid
The method from Section 3.4 is used to determine an appropriate reduced basis
of dimension N = 5 for both the noiseless and noisy cases. Figure 3.2 (center)
red
shows the details. As in Ex. 1, indicated by the training data generation, there are
an obvious set of 5 optimal basis functions, however this fact is not used.
Validation testing also uses 100 trajectories drawn similarly to (4.4). On the left
and center of Figures 4.3 (noiseless) and 4.4 (noisy), an example test trajectory is
shown at various prediction times, and on the right the average absolute ℓ error
2
the test trajectories is shown over 150 and 100 time steps, respectively. We see that
thePC-FMLstrategiesstayrelativelystableandsmoothoverthepredictionoutlook,
while the nodal FML deteriorates almost immediately. Particularly in the noisy case,
deviations here are similar to those observed in numerical methods, whereby small
errors in initial conditions accumulate and while the solution exhibits “wave-like”
behavior,itis“displaced”fromtheappropriatetrajectory. Additionally,thedenoising
behavior of PC-FML approach is evident.
Table 4.1 shows that the nodal FML model uses 263036 parameters, while the
PC-FML models use just 2575, 2325, and 2075 for the Unconstrained, Constrained,
and Fixed options, respectively.
4.3. Example 3: 2D wave equation on non-uniform grid with missing
information. Finally, we consider the two-dimensional wave equation
u =∆u (4.5)
tt
on the domain Ω = [−1,1]2 with zero Dirichlet vertical boundary conditions (i.e.
x = −1,1) and zero Neumann horizontal boundary conditions (i.e. y = −1,1). We
note that in this form the PDE in fact does not conform to (3.1) since it is second
order in time. This is therefore indeed a problem of a “missing variable” (i.e. u ),
t
such that observing u alone is partial information for the model.
To generate training data, first we solve the PDE until T = 4 (400 time steps)
from N =100 randomized initial conditions given by
traj
u(x,0)=arctan(α cos((π/2)x))
1
(4.6)
u (x,0)=α sin(πx)exp(α sin((π/2)y))
t 2 3
withα ,α ∼U[−1,1]andα ∼U[−3,3]. Then,onerandomchunkoflengthN +
1 3 2 mem
N per trajectory from u are recorded into the training data. The solutions are
rec
observed on a non-uniform grid of N =1537 points shown in Figure 4.5.
grid
The method from Section 3.4 is used to determine an appropriate reduced basis
of dimension N = 13 for the noiseless case (σ = 0) and N = 5 for the noisy
red red
161 t=0.25 1.5 t=0.50 average absolute l2 error over 100 trajectories
0.5 1 101
0.5
0 Reference Reference
Nodal FML Nodal FML
Unconstrained PC-FML 0 Unconstrained PC-FML
Constrained PC-FML Constrained PC-FML
-0.5 Fixed PC-FML Fixed PC-FML
-0.5
-1 -1 100
-1.5 -1.5
-1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1
x x
1 t=1.00 1.5 t=1.50
10-1
0.5 1
0.5
0
0
-0.5
-0.5
10-2
-1 R N Ue o nf d ce oar nle F sn tMc rae L ined PC-FML -1 R N Ue o nf d ce oar nle F sn tMc rae L ined PC-FML N Uo nd coa nl F stM raL ined PC-FML
Constrained PC-FML Constrained PC-FML Constrained PC-FML
Fixed PC-FML Fixed PC-FML Fixed PC-FML
-1.5 -1.5
-1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 0.2 0.4 0.6 0.8 1 1.2 1.4
x x t (seconds)
Fig.4.3: Ex. 2(noiseless): Exampletrajectory(left,center)andaverageerror(right).
0.2 t=0.25 1 t=0.50 average absolute l2 error over 100 trajectories
0
0.5
-0.2
-0.4 Reference 0 Reference
Nodal FML Nodal FML
-0.6 Unconstrained PC-FML Unconstrained PC-FML
Constrained PC-FML Constrained PC-FML
-0.8 Fixed PC-FML -0.5 Fixed PC-FML
-1
-1
-1.2
-1.4 -1.5
-1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1
x x
1 t=0.75 1 t=1.00
0.5 0.5
0 0
100
-0.5 -0.5
-1 R Ne of de ar le Fn Mce L -1 R Ne of de ar le Fn Mce L Nodal FML
Unconstrained PC-FML Unconstrained PC-FML Unconstrained PC-FML
Constrained PC-FML Constrained PC-FML Constrained PC-FML
Fixed PC-FML Fixed PC-FML Fixed PC-FML
-1.5 -1.5
-1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
x x t (seconds)
Fig. 4.4: Ex. 2 (noisy): Example trajectory (left, center) and average error (right).
case (σ = 10−1). Figure 3.2 (right) shows the details. In particular, we see that
the noiseless singular values of the training data drop off gradually after N = 13,
red
while noise is dominant after N = 5 in the noisy observations. Unlike Exs. 1
red
and 2, indicated by the training data generation, there are not obvious optimal basis
functions for the initial conditions. The Fixed PC-FML approach provides insight,
whereby Figures 3.3 and 3.4 expose the 13- and 5-dimensional bases b (x) for j =
j
171
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
-1 -0.5 0 0.5 1
x
Fig. 4.5: Ex. 3: Non-uniform measurement grid.
1,...,N in the columns of V for the noiseless and noisy cases, respectively. We
red red
see similarities in the first basis 5 elements, but also notice noise creeping into the
b (x) and b (x) in the noisy measurements. This demonstrates why after this point,
4 5
including extra basis elements is not advantageous as they will have a large noise
component that may actually be detrimental to reconstructing and explaining the
unknowndynamics. ThebasesdiscoveredbytheUnconstrainedandConstrainedcan
be analyzed in the same way.
Validation testing also uses 100 trajectories drawn similarly to (4.6). On the
left and center of Figures 4.6 (noiseless) and 4.7 (noisy), an example test trajectory
is shown at various prediction times, and on the right the average absolute ℓ er-
2
ror the test trajectories is shown over 1000 and 200 time steps, respectively. We
see that the PC-FML strategies stay relatively stable and smooth over the predic-
tion outlook, while the nodal FML noisily deteriorates almost immediately and never
actually exhibits “wave-like” behavior. Additionally, the denoising behavior of PC-
FML approach is evident. Generally, the results show that despite its simplicity, the
SVD-motivated PC-FML approach yields accurate approximation to nonlinear data.
Table4.1showsforthenoiselesscase,thenodalFMLmodeluses2105791parame-
ters,whilethePC-FMLmodelsusejust44565,24584,and4603fortheUnconstrained,
Constrained,andFixedoptions,respectively. Inthenoisycase,thenodalFMLmodel
uses 814671 parameters, while the PC-FML models use just 17445, 9760, and 2075
for the Unconstrained, Constrained, and Fixed options, respectively.
5. Conclusion. WehavepresentedacomputationaltechniqueintheFlowMap
Learning (FML) family for modeling the evolution of unknown PDEs from limited,
noisy, and incomplete solution data using a novel NN architecture with significantly
reduced parameterization, notably enabling hundredfold lower training data require-
ments. In particular, the architecture achieves this drastic decrease in parameters
by first learning a common reduced linear approximation to reduce high-dimensional
spatial measurements to a few important components, and then conducting the dy-
namics learning in this reduced basis which requires a lower complexity network due
to its ODE form. Future work will focus on efficient learning nonlinear reduced rep-
resentations in the context of data-driven FML of PDEs.
18
yFig.4.6: Ex. 3(noiseless): Exampletrajectory(left,center)andaverageerror(right).
Fig. 4.7: Ex. 3 (noisy): Example trajectory (left, center) and average error (right).
REFERENCES
[1] S.Cao,Chooseatransformer: Fourierorgalerkin,Advancesinneuralinformationprocessing
systems,34(2021),pp.24924–24940.
[2] J. Chen and K. Wu,Deep-osg: Deep learning of operators in semigroup,JournalofCompu-
tationalPhysics,493(2023),p.112498.
[3] Z. Chen, V. Churchill, K. Wu, and D. Xiu, Deep neural network modeling of unknown
19partialdifferentialequationsinnodalspace,JournalofComputationalPhysics,449(2022),
p.110782.
[4] V. Churchill, Y. Chen, Z. Xu, and D. Xiu, Dnn modeling of partial differential equations
with incomplete data,JournalofComputationalPhysics,493(2023),p.112502.
[5] V. Churchill, S. Manns, Z. Chen, and D. Xiu, Robust modeling of unknown dynamical
systems via ensemble averaged learning,arXivpreprintarXiv:2203.03458,(2022).
[6] V.ChurchillandD.Xiu,Flowmaplearningforunknowndynamicalsystems: Overview,im-
plementation,andbenchmarks,JournalofMachineLearningforModelingandComputing,
4(2023).
[7] X. Fu, L.-B. Chang, and D. Xiu, Learning reduced systems via deep neural networks with
memory,J.MachineLearningModel.Comput.,1(2020),pp.97–118.
[8] C.GreifandK.Urban,Decayofthekolmogorovn-widthforwaveproblems,AppliedMath-
ematicsLetters,96(2019),pp.216–222.
[9] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in
Proceedings of the IEEE conference on computer vision and pattern recognition, 2016,
pp.770–778.
[10] I. T. Jolliffe and J. Cadima, Principal component analysis: a review and recent devel-
opments, Philosophical transactions of the royal society A: Mathematical, Physical and
EngineeringSciences,374(2016),p.20150202.
[11] D.KingmaandJ.Ba,Adam: Amethodforstochasticoptimization,arXiv:1412.6980,(2014).
[12] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and
A. Anandkumar, Neural operator: Learning maps between function spaces with appli-
cations to pdes,JournalofMachineLearningResearch,24(2023),pp.1–97.
[13] Z. Li, N. B. Kovachki, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anand-
kumar, et al., Fourier neural operator for parametric partial differential equations, in
InternationalConferenceonLearningRepresentations,2020.
[14] Z.Li,K.Meidani,andA.B.Farimani,Transformerforpartialdifferentialequations’operator
learning,arXivpreprintarXiv:2205.13671,(2022).
[15] Y.T.Lin,Y.Tian,D.Livescu,andM.Anghel,Data-drivenlearningforthemori–zwanzig
formalism: Ageneralizationofthekoopmanlearningframework,SIAMJournalonApplied
DynamicalSystems,20(2021),pp.2558–2601.
[16] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear operators
via deeponet based on the universal approximation theorem of operators,NatureMachine
Intelligence,3(2021),pp.218–229.
[17] H. Mori, Transport, collective motion, and brownian motion, Progress of theoretical physics,
33(1965),pp.423–455.
[18] H. Mori, Transport, collective motion, and brownian motion, Progress of theoretical physics,
33(1965),pp.423–455.
[19] A. Pinkus, N-widths in Approximation Theory, vol. 7, Springer Science & Business Media,
2012.
[20] T.Qin,K.Wu,andD.Xiu,Datadrivengoverningequationsapproximationusingdeepneural
networks,J.Comput.Phys.,395(2019),pp.620–635.
[21] F.Regazzoni,L.Dede,andA.Quarteroni,Machinelearningforfastandreliablesolution
of time-dependent differential equations, Journal of Computational physics, 397 (2019),
p.108852.
[22] S.Rudy,A.Alla,S.L.Brunton,andJ.N.Kutz,Data-drivenidentificationofparametric
partial differential equations, SIAM Journal on Applied Dynamical Systems, 18 (2019),
pp.643–660.
[23] S.H.Rudy,S.L.Brunton,J.L.Proctor,andJ.N.Kutz,Data-drivendiscoveryofpartial
differential equations,Scienceadvances,3(2017),p.e1602614.
[24] H.Schaeffer,Learningpartialdifferentialequationsviadatadiscoveryandsparseoptimiza-
tion,ProceedingsoftheRoyalSocietyofLondonA:Mathematical,PhysicalandEngineer-
ingSciences,473(2017).
[25] Q.Wang,N.Ripamonti,andJ.Hesthaven,Recurrentneuralnetworkclosureofparametric
POD-Galerkin reduced-order models based on the Mori-Zwanzig formalism, J. Comput.
Phys.,410(2020),p.109402.
[26] K.WuandD.Xiu,Data-drivendeeplearningofpartialdifferentialequationsinmodalspace,
J.Comput.Phys.,408(2020),p.109307.
[27] R.Zwanzig,Nonlineargeneralizedlangevinequations,JournalofStatisticalPhysics,9(1973),
pp.215–220.
[28] R.Zwanzig,Nonlineargeneralizedlangevinequations,JournalofStatisticalPhysics,9(1973),
pp.215–220.
20