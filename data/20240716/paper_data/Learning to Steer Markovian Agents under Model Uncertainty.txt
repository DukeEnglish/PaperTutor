Learning to Steer Markovian Agents
under Model Uncertainty
JiaweiHuang VinzenzThoma
ETHZurich ETHAICenter
jiawei.huang@inf.ethz.ch vinzenz.thoma@ai.ethz.ch
ZebangShen HeinrichH.Nax
ETHZurich UniversityofZurich
zebang.shen@inf.ethz.ch heinrich.nax@uzh.ch
NiaoHe
ETHZurich
niao.he@inf.ethz.ch
Abstract
Designing incentives for an adapting population is a ubiquitous problem in a
wide array of economic applications and beyond. In this work, we study how
to design additional rewards to steer multi-agent systems towards desired poli-
cieswithout priorknowledgeoftheagents’underlyinglearningdynamics. We
introduceamodel-basednon-episodicReinforcementLearning(RL)formulation
foroursteeringproblem. Importantly,wefocusonlearningahistory-dependent
steeringstrategytohandletheinherentmodeluncertaintyabouttheagents’learn-
ingdynamics. Weintroduceanovelobjectivefunctiontoencodethedesiderata
of achieving a good steering outcome with reasonable cost. Theoretically, we
identifyconditionsfortheexistenceofsteeringstrategiestoguideagentstothe
desiredpolicies. Complementingourtheoreticalcontributions,weprovideempiri-
calalgorithmstoapproximatelysolveourobjective,whicheffectivelytacklesthe
challengeinlearninghistory-dependentstrategies. Wedemonstratetheefficacyof
ouralgorithmsthroughempiricalevaluations1.
1 Introduction
Manyreal-worldapplicationscanbeformulatedasMarkovGames(Littman,1994)wheretheagents
repeatedlyinteractandupdatetheirpoliciesbasedonthereceivedfeedback. Inthiscontext,different
learningdynamicsandtheirconvergencepropertieshavebeenstudiedextensively(see,forexample,
Fudenberg and Levine (1998)). Because of the mismatch between the individual short-run and
collectivelong-runincentives,orthelackofcoordinationindecentralizedsystems,agentsfollowing
standardlearningdynamicsmaynotconvergetooutcomesthataredesirablefromasystemdesigner
perspective,suchastheNashEquilibria(NE)withthelargestsocialwelfare. Aninterestingclassof
gamesthatexemplifytheseissuesareso-called“StagHunt”games(seeFig.1-(a)),whichareused
tostudyabroadarrayofreal-worldapplicationsincludingcollectiveaction,publicgoodprovision,
socialdilemma,teamworkandinnovationadoption(Skyrms,2004)2. StagHuntgameshavetwo
1Thecodeofalltheexperimentsinthispapercanbefoundinhttps://github.com/jiaweihhuang/
Steering_Markovian_Agents.
2WedeferaconcreteandpracticalscenariowhichcanbemodeledbytheStagHuntgametoAppx.C.1
WorkshoponAligningReinforcementLearningExperimentalistsandTheorists(ARLET2024).
4202
luJ
41
]GL.sc[
1v70201.7042:viXrapure-strategy NE, one of which is ‘payoff-dominant’, that is, both players obtain higher payoffs
in that equilibrium than in the other. Typical algorithms may fail to reach the payoff-dominant
equilibriumpH,Hq(LHSFig.1-(b)). Indeed,theotherequilibriumpG,Gqistypicallyselectedwhenit
isrisk-dominant(Young,1993;Newton,2021)3.
Thispaperfocusesonsituationswhenanexter-
nal “mediator” exists, who can influence and H G
steertheagents’learningdynamicsbymodify- H (5,5) (0,4)
ingtheoriginalrewardsviaadditionalincentives. G (4,0) (2,2)
FollowingtheworksofZhangetal.(2023)and
Canyakmazetal.(2024)(seemorediscussionin (a)PayoffMatrixoftheTwo-Player“StagHunt”Game.
HandGstandfortwoactionsHuntandGather.Both
Sec.1.1),thiskindofmediatorcanbeconcep-
(H,H)and(G,G)areNEand(H,H)ispayoff-dominant.
tualizedinvariousways. Inparticular,wecan
think of a social planner who provides mone- 1.0 No Steering 1.0 With Steering
taryincentivesforjointventuresorforadoption
of an innovative technology via individual fi-
0.5 0.5
nancialsubsidies. AsillustratedontheRHSof
Fig.1-(b),withsuitablesteering,agents’dynam-
icscanbedirectedtothebestoutcome. Inthis
0.0 0.5 1.0 0.0 0.5 1.0
paper,westudytheproblemfromthemediator 1(H) 1(H)
perspective, and focus on developing steering (b)DynamicsofAgentsPolicieswithout/withSteering.
strategies for Markovian agents whose policy Agentsfollownaturalpolicygradient(replicatordynam-
ics)forpolicyupdate. xandyaxescorrespondtothe
learningdynamicsonlydependontheircurrent
probabilitytotakeactionHbytherowandcolumnplay-
policyandontheir(modified)rewardfunction.
ers.Redcurvesrepresentthedynamicsofagents’poli-
Ourprimaryobjectiveistosteertheagentsto
ciesstartingfromdifferentintializations(blackdots).
somedesiredpolicies,thatis,tominimizethe
steering gap vis-a-vis the target outcome. As Figure1: Example: The“StagHunt”Game
asecondaryobjective, thepaymentstoagents
regardingthesteeringrewardsshouldbereasonable,thatis,thesteeringcostshouldbelow.
Inpractice,learningtherightsteeringstrategiesencounterstwomainchallenges. First,theagents
maynotdisclosetheirlearningdynamicsmodeltothemediator. Asaresult,thiscreatesfundamental
modeluncertainty,whichwewilltacklewithappropriateReinforcementLearning(RL)techniquesto
trading-offexplorationandexploitation. Second,itmaybeunrealistictoassumethatthemediator
isabletoforcetheagentsto“reset”theirpoliciesinordertogeneratemultiplesteeringepisodes
with the same initial state. This precludes the possibility of learning steering strategies through
episodictrial-and-error. Therefore,themostcommonly-considered,fixed-horizonepisodicRL(Dann
andBrunskill,2015)frameworkisnotapplicablehere. Instead,wewillconsiderafinite-horizon
non-episodicsetup,wherethemediatorcanonlygenerateonefinite-horizonepisode,inwhichwe
havetoconductboththemodellearningandsteeringoftheagentssimultaneously. Motivatedby
theseconsiderations,wewouldliketoaddressthefollowingquestioninthispaper:
HowcanwelearndesiredsteeringstrategiesforMarkovianagents
inthenon-episodicsetupundermodeluncertainty?
Weconsideramodel-basedsettingwherethemediatorcangetaccesstoamodelclassF containing
theagents’truelearningdynamicsf˚.WesummarizeourmainresultsinTable1andbrieflyhighlight
ourkeycontributionsasfollow:
• ConceptualContributions: InSec.3,weformulatesteeringasanon-episodicRLproblem,and
proposeanoveloptimizationobjectiveinObj.(1),whereweexplicitlytackletheinherentmodel
uncertainty by learning history-dependent steering strategies. As we show in Prop. 3.3, under
certainconditions,evenwithoutpriorknowledgeoff˚,theoptimalsolutiontoObj.(1)achieves
notonlylowsteeringgap,butalso“ParetoOptimality”intermsofbothsteeringcostsandgaps.
• TheoreticalContributions: InSec.4,weprovidesufficientconditionsunderwhichthereexists
steeringstrategiesachievinglowsteeringgap. Theseresultsinturnjustifyourchosenobjective
andproblemformulation.
3Riskdominanceandpayoffdominance, asintroducedbyHarsanyiandSelten(1988), areequilibrium
selectioncriteriabasedon,respectively,Paretooptimalityandsizeoftheequilibrium’sbasinofattraction.
2
)H(2 )H(2Table1: ASummaryofMainResults
ANewObjectiveFunctioninObj.(1)
Settings Existence(Sec.4) Algorithms(Sec.5)
F “tf˚u
Thm.4.2 AnyRL/ControlAlgorithm
(Specialcase: knownf˚)
|F|issmall Alg.4(Amodelbelief-statemethod)
Prop.4.5
|F|islarge Proc.2(TheFETEframework)
• AlgorithmicContributions: Learningahistory-dependentstrategypresentschallengesduetothe
exponentialgrowthinthehistoryspace. Weproposealgorithmstoovercometheseissues.
– Whenthemodelclass|F|issmall,inSec.5.1,weapproachourobjectivefromtheperspective
oflearninginaPartiallyObservableMDP,andproposetotolearnapolicyoverthemodel
beliefstatespaceinsteadofoverthehistoryspace.
– Forthecasewhen|F|islarge,exactlysolvingObj.(1)canbechallenging. Instead,wefocus
onapproximatesolutionstotrade-offoptimalityandtractability. InSec.5.2,weproposea
First-Explore-Then-Exploit(FETE)framework. Undersomeconditions,wecanstillensure
thedirectedagentsconvergetothedesiredoutcome.
• EmpiricalValidation:InSec.6,weevaluateouralgorithmsinvariousrepresentativeenvironments,
anddemonstratetheireffectivenessundermodeluncertainty.
1.1 CloselyRelatedWorks
Wediscusstheworksmostcloselyrelatedtooursinthissection,anddefertheotherstoAppx.C.2.
SteeringLearningDynamics The‘steeringproblem’aswestudyitinthispaperwasfirstintro-
ducedbyZhangetal.(2023). Theyconsiderthecaseofno-regretlearnerswhomaypossiblyknow
themediator’ssteeringstrategyandcanbearbitrarilyadversarial. Theirfocusisonsteeringagents
suchthattheaveragepolicyconvergestothetargetNEwhiletheaccumulativebudgetissublinear.
When the desired NE is not pure, Zhang et al. (2023) further require the mediator to be able to
“giveadvice”totheplayers. Bycontrast,wefocusonabroaderclassofMarkovianagentswitha
finite-horizonsteeringsetup,withfocusonthesteeringgapoftheterminalpolicyandthecumulative
steeringcost. Besides,ourmediatorcansteeronlybymodifyingtheagents’rewardfunctions.
PerhapstheclosesttooursisaconcurrentworkbyCanyakmazetal.(2024). Theyexperimentally
investigatetheuseofcontrolmethodstodirectgamedynamicstowardsdesiredoutcomes,inparticular
allowingformodeluncertainty. Ourworkcomplementstheirempiricalresultsinthreemainways.
First,wehandlethemodeluncertaintyinasomewhatmoreprincipledwaybyproposingaconcrete
non-episodicRLformulationforthesteeringproblemandasuitablelearningobjective(Obj.(1)),
whereweexplicitlylearnahistory-dependentsteeringstrategy. Second,wedevelopnoveltheory
regardingtheexistenceofstrategieswithlowsteeringgap.Third,onthealgorithmiclevel,Canyakmaz
etal.(2024)consideratwo-phase(exploration+exploitation)frameworkcalledSIAR-MPCwhich
is similar to our FETE framework for large model sets. However, they employ random noise-
basedexploration,whileweconsideramoreadvancedexplorationstrategy(Sec.5.2)thatresultsin
significantlyhigherexplorationefficiencyinexperiments(Sec.6). Besides,whenthemodelsetis
small,wecontributeabelief-statebasedalgorithmthatcanexactlysolveourObj.(1). Assuggested
byProp.3.3,itissuperiortotheirtwo-phaseframeworkintermsofthesteeringcostwhileachieving
alowsteeringgap.
OpponentShaping IntheRLliteraturealineofworkfocusontheproblemofopponentshaping,
whereagentscaninfluenceeachotherslearningbyhandingoutrewards(Foersteretal.,2018;Yang
et al., 2020; Willi et al., 2022; Lu et al., 2022; Willis et al., 2023; Zhao et al., 2022). Although
thewaysofinfluencingagentsaresimilartooursetting,westudytheproblemofamediatorthat
actsoutsidetheMarkovGameandsteersalltheagentstowardsdesiredpolicies,whileinopponent
shapingtheagentsthemselveslearntoinfluenceeachotherfortheirowninterests.
Episodic RL and Non-Episodic RL Most of the existing RL literature focus on the episodic
learningsetup,wheretheentireinteractionhistorycanbedividedintomultipleepisodesstarting
3fromthesameinitialstatedistribution(DannandBrunskill,2015;Dannetal.,2017). Comparing
withthissetting,ourfinite-horizonnon-episodicsettingismorechallengingbecausethemediator
cannotsimplylearnfromrepeatedtrial-and-error. Therefore,thelearningcriterions(e.g. no-regret
(Azaretal.,2017;Jinetal.,2018)orsamplecomplexity(DannandBrunskill,2015))inepisodicRL
settingisnotsuitableinourcase,whichtargetsatfindinganear-optimalpolicyinmaximizingreturn.
Thismotivatesustoconsiderthenewobjective(Obj.(1)).
To our knowledge, most of the previous works use “non-episodic RL” to refer to the learning in
infinite-horizonMDP.Onepopularsettingistheinfinite-horizonMDPswithstationarytransitions,
wherepeopleconsiderthediscounted(Schulmanetal.,2017;Dongetal.,2019)oraveragereturn
(Aueretal.,2008;Weietal.,2020). Theinfinit-horizonsettingwithnon-stationarydynamicsis
knownasthecontinualRL(Khetarpaletal.,2022;Abeletal.,2024),wherethelearners“neverstops
learning”andcontinuetoadapttothedynamics. Sincewefocusonthesteeringproblemwithfixed
andfinitehorizon,themethodologyinthoseworkscannotbedirectlyappliedhere.
Mostimportantly,wearealsothefirstworktomodelthesteeringproblemasaRLproblem.
2 Preliminary
Inthe following, we formallydefinethe finite-horizonMarkovGamethat wewillfocus on. We
summarizeallthefrequentlyusednotationsinthispaperinAppx.B.
FiniteHorizonMarkovGame Afinite-horizonN-playerMarkovGameisdefinedbyatuple
G :“ tN,s ,H,S,A :“ tAnuN ,P,r :“ trnuN u,whereN :“ t1,2,...,Nuistheindicesof
1 n“1 n“1
agents,s isthefixedinitialstate,H isthehorizonlength,S isthefinitesharedstatespace,Anisthe
1
finiteactionspaceforagentn,andAdenotesthejointactionspace. Besides,P:“tP u with
h hPrHs
P :SˆAÑ∆pSqdenotesthetransitionfunctionofthesharedstate,andrn :“trnu with
h h hPrHs
rn :SˆAÑr0,1sdenotestherewardfunctionforagentn. Foreachagentn,weconsiderthenon-
h
stationaryMarkovianpoliciesΠn :“tπn “tπn,...,πnu|@hPrHs, πn :S Ñ∆pAnqu.Wedenote
1 H h
Π:“Π1ˆ...ˆΠN tobethejointpolicyspaceofallagents. Givenapolicyπ :“tπ1,...,πNuPΠ,
atrajectoryisgeneratedby: @h P rHs, @n P rNs, an „ πnp¨|s q, rn Ð rnps ,a q, s „
h h h h h h h`1
P p¨|s ,a q,wherea :“ tanu denotesthecollectionofallactions. Givenapolicyπ,we
h h h h h nPrNs ř
definethevaluefunctionsby: Qn,πp¨,¨q :“ E r H rnps ,a q|s “ ¨,a “ ¨s, Vn,πp¨q :“
ř h|r π h1“h h1 h1 h1 h h h|r
E r H rnps ,a q|s “¨s,whereweuse|rtospecifytherewardfunctionassociatedwiththe
vaπ luefh u1 n“ ch tioh n1 s.h In1 thh e1 resh tofthepaper,wedenoteAn,π “Qn,π´Vn,π tobetheadvantagevalue
|r |r |r
function,anddenoteJnpπq:“Vn,πps qtobethetotalreturnofagentnw.r.t. policyπ.
|r 1|r 1
3 TheProblemFormulationoftheSteeringMarkovianAgents
WefirstintroduceourdefinitionofMarkovianagents. Informally,thepolicyupdatesofMarkovian
agentsareindependentoftheinteractionhistoryconditioningontheircurrentpolicyandobserved
rewards. This subsumes a broader class of popular policy-based methods as concrete examples
(Giannouetal.,2022;Dingetal.,2022;Xiao,2022;Daskalakisetal.,2020).
Definition3.1(MarkovianAgents). GivenagameG,afiniteandfixedT,theagentsareMarkovian
iftheirpolicyupdaterulef onlydependsonthecurrentpolicyπ andtherewardfunctionr:
t
@tPrTs, π „fp¨|π ,rq.
t`1 t
Hereweonlyhighlightthedependenceonπ andr,andomitotherdependence(e.g. thetransition
t
functionofG). Itisworthtonotethatwedonotrestrictwhethertheupdatesofagents’policiesare
independentorcorrelatedwitheachother,deterministicorstochastic. WeassumeT isknowntous.
Inthesteeringproblem,themediatorhastheabilitytochangetherewardfunctionrviathesteering
rewardu,sothattheagents’dynamicsaremodifiedto:
@tPrTs, u „ψ p¨|π ,u ,...,π ,u ,π q, π „fp¨|π ,r`u q,
t t 1 1 t´1 t´1 t t`1 t t
Hereψ :“tψ u denotesthemediator’s“steeringstrategy”togenerateu . Weconsiderhistory-
t tPrTs t
dependentstrategiestohandlethemodeluncertainty,whichwewillexplainlater. Besides,u :“
t
4tun u , where un : S ˆA Ñ r0,U s is the steering reward for agent n at game
t,h hPrHs,nPrNs t,h max
horizonhandsteeringstept. U ă `8denotestheupperboundforthesteeringreward. For
max
practicalconcerns,wefollowZhangetal.(2023)andconstrainthesteeringrewardtobenon-negative.
The mediator has a terminal reward function ηgoal and a cost function ηcost. First, ηgoal : Π Ñ
r0,η sassesseswhetherthefinalpolicyπ alignswithdesiredbehaviors—thisencapsulates
max T`1
ourprimarygoalofalowsteeringgap. Notethatweconsiderthegeneralsettinganddonotrestrict
the maximizer of ηgoal to be a Nash Equilibrium. For instance, to steer the agents to a desired
policy π˚, we could choose ηgoalpπq :“ ´}π ´π˚} 2. Altřernatively, in scenarios focusing on
maximizingutility,ηgoalpπqcouldbedefinedasthetotalutility Jnpπq.Forηcost :ΠÑR ,
nPrNs |r ě0
iřtisusedtoquantifythesteeringcostincurredwhilesteering. Inthispaper,wefixηcostpπ,uq :“
Jnpπnqtobethetotalreturnrelatedtoπandthesteeringrewardu. Notethatwealways
nPrNs |u
have0ďηcostpπ,uqďU NH.
max
Steering Dynamics as a Markov Decision Process (MDP) Given a game G, the agents’ dy-
namics f and pηcost,ηgoalq, the steering dynamics can be modeled by a finite-horizon MDP.
M :“ tπ ,T,Π,U,f,pηcost,ηgoalqu with initial state π , horizon length T, state space Π, action
1 1
spaceU :“r0,U sHN|S||A|,stationarytransitionf,runningrewardηcostandterminalrewardηgoal.
max
Forcompleteness,wedefertoAppx.C.3foranintroductionoffinite-horizonMDP
SteeringunderModelUncertainty Inpractice,themediatormaynothavepreciseknowledgeof
agentslearningdynamicsmodel,andtheuncertaintyshouldbetakenintoaccount.Wewillonlyfocus
onhandlingtheuncertaintyinagents’dynamicsf,andassumethemediatorhasthefullknowledgeof
Gandtherewardfunctionsηgoalandηcost. Weconsiderthemodel-basedsettingwherethemediator
onlyhasaccesstoafinitemodelclassF (|F|ă`8)satisfyingthefollowingassumption:
AssumptionA(Realizability). f˚ PF.
A Finite-Hoziron Non-Episodic Setup and Motivation As motivated previously, we formu-
late steering as a finite-horizon non-episodic RL problem. To our knowledge, in contrast to our
finite-horizonsetting,mostofthenon-episodicRLsettingsconsidertheinfinite-horizonsetupwith
stationaryornon-stationarytransitions,andtherefore,theyarealsonotsuitablehere. Weprovide
morediscussioninSec.1.1.
Definition3.2(FiniteHorizonNon-EpisodicSteeringSetting). Themediatorcanonlyinteractwith
therealagentsforoneepisodetπ ,u ,...,π ,u ,π u,whereπ „ f˚p¨|π ,u q@t P rTs.
1 1 T T T`1 t`1 t t
Nonetheless,themediatorcangetaccesstothesimulatorsforallmodelsinF,anditcansample
arbitrarytrajectoriesanddoepisodiclearningwiththosesimulatorstodecidethebeststeeringactions
u ,u ,...,u todeploy.
1 2 T
The Learning Objective Motivated by the model-based non-episodic setup, we propose the
followingobjectivefunction,wherewesearchoverthesetofallhistory-dependentstrategies,denoted
byΨ,tooptimizetheaverageperformanceoverallf PF.
1
ÿ ” ÿT ı
ψ˚ Ðargmax E β¨ηgoalpπ q´ ηcostpπ ,u q , (1)
ψPΨ |F| ψ,f T`1 t t
fPF t“1
Here we use E r¨s :“ Er¨|@t P rTs,u „ ψ p¨|tπ ,u ut´1,π q,π „ fp¨|π ,r `u qs to
ψ,f t t t1 t1 t1“1 t t`1 t t
denotetheexpectationovertrajectoriesgeneratedbyψandf PF;β ą0isaregularizationfactor.
Next,weexplaintherationaletoconsiderhistory-dependentstrategies. AsintroducedinDef.3.2,
weonlyintereactwiththerealagentsonce. Therefore, themediatorneedstousetheinteraction
historywithf˚todecidetheappropriatesteeringrewardstodeploy,sincethehistoryisthesufficient
informationsetincludingalltheinformationregardingf˚availaletothemediator.
Wewanttoclarifythatinoursteeringframework,wewillfirstsolveObj.(1),andthendeployψ˚
tosteerrealagents. Thelearningandoptimizationofψ˚ inObj.(1)onlyutilizessimulatorsofF.
Besides,afterdeployingψ˚torealagents,wewillnotupdateψ˚withthedatageneratedduringthe
interactionwithrealagents. Thisisseeminglydifferentfromcommononlinelearningalgorithms
whichconductthelearningandinteractionrepeatedly(DannandBrunskill,2015). Butwewantto
highlightthat,giventhefactthatψ˚ishistory-dependent,itisalreadyencodedinψ˚howtomake
decisions(orsay,learning)inthefaceofuncertaintyaftergatheringdatafromrealagents. Inother
words,onecaninterpretthat,inObj.(1),wearetryingtooptimizean“onlinealgorithm”ψ˚which
5can“smartly”decidethenextsteeringrewardtodeploygiventhepastinteractionhistory. Aswewill
justifyinthefollowing,ourObj.(1)canindeedsuccessfullyhandlethemodeluncertainty.
ř
Justification for Objective (1) We use C pfq :“ E r T ηcostpπ ,u qs and ∆ pfq :“
ψ,T ψ,f t“1 t t ψ,T
E rmax ηgoalpπq´ηgoalpπ qsasshortnotesofthesteeringcostandthesteeringgap(ofthe
ψ,f π T`1
terminalpolicyπ ),respectively. Besides,wedenoteΨε :“tψ PΨ|max ∆ pFqďεu4to
T`1 fPF ψ,T
bethecollectionofallsteeringstrategieswithε-steeringgap. Basedonthesenotations,weintroduce
twodesiderata,andshowhowanoptimalsolutionψ˚ofObj.(1)canachievethem.
Desideratum1(ε-SteeringGap). Wesayψhasε-steeringgap,ifmax ∆ pfqďε.
fPF ψ,T
Desideratum 2 (Pareto Optimality). We say ψ is Pareto Optimal if there does not exist another
ψ1 P Ψ, suchthat(1)@f P F, C pfq ď C pfqand∆ pfq ď ∆ pfq; (2)Df1 P F, s.t.
ψ1,T ψ,T ψ1,T ψ,T
eitherC pf1qăC pf1qor∆ pf1qă∆ pf1q.
ψ1,T ψ,T ψ1,T ψ,T
Proposition3.3. [JustificationforObj.(1)]BysolvingObj.(1): (1)ψ˚isParetoOptimal;(2)Given
anyε,ε1 ą0,ifΨε{|F| ‰Handβ ě UmaxNHT|F|,wehaveψ˚ PΨε`ε1;
ε1
Next,wegivesomeinterpretation. Asourprimarydesideratum,weexpecttheagentsconvergeto
somedesiredpolicythatmaximizesthegoalfunctionηgoalafterbeingsteeredforT steps,regardless
ofthetruemodelf˚. Therefore,werestricttheworstcasesteeringgaptobesmall. Asstatedin
Prop.3.3,foranyaccuracylevelεą0,aslongasε{|F|-steeringgapisachievable,bychoosingβ
largeenough,wecanapproximatelyguaranteeψ˚hasε-steeringgap. Forthesteeringcost,although
itisnotourprimaryobjective,Prop.3.3statesthatatleastwecanguaranteetheParetoOptimality:
competingwithψ˚,theredoesnotexistanotherψ1,whichcanimproveeitherthesteeringcostorgap
forsomef1 PF withoutdeterioratinganyothers.
Giventheabovediscussion,onenaturalquestionisthat: whenisΨε non-empty,orequivalently,
whendoesastrategyψwithε-steeringgapexist? InSec.4,weprovidesufficientconditionsand
concreteexamplestoaddressthisquestionintheory. Notably,wesuggestconditionswhereΨε is
non-emptyforanyεą0,sothattheconditionΨε{|F| ‰HinProp.3.3isrealizable,evenforlarge
|F|. Afterthat,inSec.5,weintroducealgorithmstosolveourObj.(1).
4 ExistenceofSteeringStrategywithε-SteeringGap
Inthissection,weidentifysufficientconditionssuchthatΨεisnon-empty. InSec.4.1,westartwith
thespecialcasewhenf˚isknown,i.e. F “tf˚u. Theresultswillserveasbasiswhenwestudythe
generalunknownmodelsettinginSec.4.2.
4.1 Existencewhenf˚isKnown: NaturalPolicyGradientasanExample
Inthissection,wefocusonapopularchoiceoflearningdynamicscalledNaturalPolicyGradient
(NPG)dynamics(Kakade,2001;Agarwaletal.,2021)(a.k.a. thereplicatordynamics(Schusterand
Sigmund,1983))withdirectpolicyparameterization. NPGisaspecialcaseofthePolicyMirror
Descent(PMD)(Xiao,2022). Forthereadability,westicktoNPGinthemaintext,andinAppx.E.1,
we formalize PMD and extend the results to the general PMD, which subsumes other learning
dynamics,liketheonlinegradientascent(Zinkevich,2003).
Definition4.1(NaturalPolicyGradient). Foranyn P rNs,t P rTs,h P rHs,s P S, thepolicy
h
is updated by: πn p¨|s q 9 πn p¨|s
qexppαApn,πt
ps ,¨qq. Here
Apn,πt
is some random
t`1,h h t,h h h|rn`un h h|rn`un
estimationfortheadvantagevalueAn h|, rπ nt
`un
withE
πnrApt
n h|, rπ nt `unps h,¨qs“0.
t
t t
p
WeuseAπ (andAπ )todenotetheconcatenationofthevaluesofallagents,horizon,states
|r`u |r`u
andactions. WeonlyassumeApπt iscontrollableandhaspositivecorrelationwithAπt butcould
|r`u |r`u
bebiased,whichwecallthe“generalincentivedriven”agents.
AssumptionB(GeneralIncentiveDrivenAgents).
@tPrTs, xErApπt s,Aπt yěλ }Aπt }2, }Apπt }2 ďλ2 }Aπt }2,
|r`ut |r`ut min |r`ut 2 |r`ut 2 max |r`ut 2
4Infact,besidesε,Ψε alsodependsonotherparameterslikeT,U ,F andtheinitialpolicyπ . For
max 1
simplicity,weonlyhighlightthosedependenceifnecessary.
6ForNPG,notethatthepolicyisalwaysboundedawayfrom0. WewilluseΠ` :“tπ|@n,h,a ,s :
h h
πnpa |s qą0utodenotesuchfeasiblepolicyset. Westateourmainresultbelow.
h h h
Theorem4.2(Informal). SupposeηgoalisLipschitzinπ,givenanyinitialπ PΠ`,foranyεą0,
1
iftheagentsfollowDef.4.1underAssump.B,ifT andU arelargeenough,wehaveΨε ‰H.
max
Ourresultisstronginindicatingtheexistenceofasteeringpathforanyfeasibleinitialization. The
proofisbasedonconstruction. Thebasicideaistodesigntheu sothatAπt 9logπ˚,forsome
t |r`ut πt
targetpolicyπ˚ PΠ`(approximately)maximizingηgoal,thenwecanguaranteetheconvergenceof
π towardsπ˚underAssump.B.Themainchallengeherewouldbethedesignofu . Wedeferthe
t t
detailsandtheformalstatementstoAppx.E.
4.2 Existencewhenf˚isUnknown: theIdentifiableModelClass
r
Intuitively,whenf˚isunknown,ifwecanfirstuseafewsteeringstepsT ăT toexploreandidentify
r
f˚,andthensteertheagentsfromπr tothedesiredpolicywithinT ´T stepsgiventheidentified
T
f˚,wecanexpectΨε ‰H. Motivatedbythisinsight,weintroducethefollowingnotion.
Definition 4.3 (pδ,Tδq-Identifiable). Given δ P p0,1q, we say F is pδ,Tδq-identifiable, if
F F
max min E rIrf “ f ss ě 1 ´ δ, where IrEs “ 1 if E is true and otherwise 0;
ψ fPF ψ,f ř MLE
Tδ
f :“argmax F logfpπ |π ,u q.
MLE fPF t“1 t`1 t t
Intuitively,F ispδ,Tδq-identifiable,ifDψ,s.t. afterTδ steeringsteps,thehiddenmodelf canbe
F F
identifiedbytheMaximalLikelihoodEstimation(MLE)withhighprobability. Next,weprovidean
exampleofpδ,Tδq-identifiablefunctionclasswithTδ upperboundedforanyδ Pp0,1q.
F F
Example 4.4. [One-Step Difference] If @π P Π, there exists a steering reward u P U, s.t.
π
min H2pfp¨|π,r `u q,f1p¨|π,r `u qq ě ζ, for some universal ζ ą 0, where H is the
f,f1PF π π
Hellingerdistance,thenforanyδ Pp0,1q,F ispδ,Tδq-identifiablewithTδ “Opζ´1logp|F|{δqq.
F F
BasedonDef.4.3,weprovideasufficientconditionwhenΨεisnon-empty.
r
Theorem4.5. [ASufficientConditionforExistence]Givenanyεą0,ΨεpF;π q5 ‰H,ifDT ăT,
T 1
s.t.,(1)F isp 2ηmε ax,Tr q-identifiable,(2)Ψε T{ ´2 TrpF;π Trq‰Hforanypossibleπ Tr generatedatstep
r
T duringthesteering.
Weconcludethissectionbynotingthat,byThm.4.2,theabovecondition(2)isrealisticforNPG(or
moregeneralPMD)dynamics. TheproofsforallresultsinthissectionaredeferredtoAppx.F.
5 Learning(Approximately)OptimalSteeringStrategy
Inthissection,weinvestigatehowtosolveObj.(1). ComparingwiththeepisodicRLsetting,the
mainchallengeistolearnahistory-dependentpolicy. Sincethehistoryspacegrowsexponentiallyin
T,directlysolvingObj.(1)canbecomputationallyintractableforlargeT. Therefore,themainfocus
ofthissectionistodesigntractablealgorithmstoovercomethischallenge.
Asaspecialcase,whenthemodelisknown,i.e. F “ tf˚u,bytheMarkovianproperty,Obj.(1)
reducestoanormalRLobjective, andastate-dependentsteeringstrategyψ : Π Ñ U isalready
enough. Forcompleteness,weincludethealgorithmbutdefertoAlg.3inAppx.C.4. Intherestof
thissection,wefocusonthegeneralcase|F|ą1. InSec.5.1,weinvestigatethesolutionswhen|F|
issmall,andinSec.5.2,westudythemorechallengingcasewhen|F|islarge.
5.1 SmallModelClass: DynamicProgrammingwithModelBeliefState
APartiallyObservableMDPPerspective Infact,wecaninterpretObj.(1)aslearningtheoptimal
policyinaPOMDP,inwhichthehiddenstateispπ ,fq,i.e. atuplecontainingthepolicyandthe
t
hiddenmodelf uniformlysampledfromF,andthemediatorcanonlypartiallyobservethepolicy
π . Itiswell-knownthatanyPOMDPcanbeliftedtothebeliefMDP,wherethestateisthebelief
t
5Herewehighlightthedependenceoninitialpolicy,model,andtimeforclarity(seeFootnote4)
7state of the original POMDP. Then, the optimal policy in the belief MDP is exactly the optimal
history-dependentpolicyintheoriginalPOMDP(Ibe,2013). Inourcase,foreachsteptPrTs,the
beliefstateispπ ,b q,whereb :“rPrpf|tπ ,u ut ,π qs isthe“modelbeliefstate”defined
t t t t1 t1 t1“1 t fPF
tobetheposteriordistributionofmodelsgiventhehistoryofobservationsandactions. When|F|is
small,themodelbeliefstateb PR|F|islowdimensionalandcomputable. Learningψ˚istractable
t
byrunninganyRLalgorithmontheliftedMDP.InProc.1,weshowhowtosteerinthissetting. We
deferthedetailedalgorithmoflearningsuchbelief-statedependentstrategytoAlg.4inAppx.C.5.
Procedure1:TheSteeringProcedurewhen|F|isSmall
1 Input: ModelSetF;TotalstepT;
2 SolvingObj.(1)bylearningabeliefstate-dependentstrategyψ B˚ eliefbyAlg.4withF andT.
3 Deployψ B˚ elieftosteertherealagentsforT steps.
5.2 LargeModelClass: AFirst-Explore-Then-ExploitFramework
When|F|islarge,themethodinSec.5.1isinefficientsincethebeliefstateb ishigh-dimensional. In
t
fact,theabovePOMDPinterpretationimpliestheintractabilityofObj.(1)forlarge|F|: thenumber
ofhiddenstatesofthePOMDPscaleswith|F|. Therefore,insteadofexactlysolvingObj.(1),we
turntotheFirst-Explore-Then-Exploit(FETE)frameworkasstatedinProcedure2.
r
ThefirstT ă T stepsaretheexplorationphase,wherewelearnanddeployanexplorationpolicy
ψExplore maximizingtheprobabilityofidentifyingthehiddenmodelwiththeMLEestimator. The
r
remainingT ´T stepsbelongtotheexploitationstage. WefirstestimatethetruemodelbytheMLE
withtheinteractionhistorywithrealagents. Next, welearnanexploitationstrategytosteerreal
r r
agentsfortherestT ´T stepsbysolvingObj.(1)withF “ tf u,timeT ´T andtheinitial
MLE
policyπ Tr `1,asiff MLEisthetruemodel.
JustificationforFETE WecannotguaranteethatDesiderata1&2areachievable,becausewedo
not exactly solve Obj. 1. However, if F is pδ{|F|,Tδ{|F|q-identifiable (Def. 4.3) and we choose
F
Tr ě Tδ{|F|,wecanverifyPrpf “ f˚q ě 1´δ inProc.2. Therefore,wecanstillexpectthe
F MLE
exploitationpolicyψExploitsteertheagentstoapproximatelymaximizeηgoalpπ qwithreasonable
T`1
r
steeringcostfortherestT ´T steps.
Procedure2:TheSteeringProcedurewhen|F|isLarge(TheFETEFramework)
r
1 Input: ModelSetF;TotalstepT;ExplorationhorizonT;
2 /*—————————————-ExplorationPhase—————————————-*/
3 Learnanexplorationstrategy
1
ÿ ÿTr
ψExplore Ðargmax E rIrf “argmax logf1pπ1 |π1,u1qss.
ψ |F| fPF π 11,u1 1,...,π T1 Ă`1„ψ,f f1PF t“1 t`1 t t
DeployψExploretosteertherealagentsandcollecttπ 1,u 1,...,π Tr,u Tr,π Tr `1u
4 /*—————————————-ExploitationPhase—————————————-*/
ř
r
5 Estimatef MLE Ðargmax fPF T t“1logfpπ t`1|π t,u tq ř
r
6 DeployψExploit Ðargmax ψE ψ,fMLErβ¨ηgoalpπ T´Tr `1q´ T t“´ 1T ηcostpπ t,u tq|π 1 “π Tr `1s.
WeconcludethissectionbyhighlightingthecomputationaltractabilityofFETE.Notethatwhen
computingψExploit, wetreatf asthetruemodel, soanhistory-independentψExploit isenough.
MLE
Therefore,theonlypartwhereweneedtolearnahistory-dependentstrategyisintheexploration
r
stage,andthemaximalhistorylengthisatmostT,whichcanbemuchsmallerthanT. Moreover,in
somecases,itisalreadyenoughtojustlearnahistory-independentψExploretodotheexploration(for
example,themodelclassinExample4.4).
86 Experiments
Inthissection,wediscussourexperimentalresults. Formoredetailsofallexperimentsinthissection
(e.g. experimentsetupandtrainingdetails),wedefertoAppx.H.Thesteeringhorizonissettobe
T “500,andalltheerrorbarshows95%confidencelevel. Wedenoterxs` :“maxt0,xu.
6.1 LearningSteeringStrategieswithKnowledgeoff˚
Normal-FormStagHuntGame InFig.1-(b),wecomparetheagents’dynamicswith/without
steering,wheretheagentslearntoplaytheStagHuntGameinFig.1-(a). Wereporttheexperiment
p
setuphere. BothagentsfollowtheexactNPG(Def.4.1withAπ “ Aπ)withfixedlearningrate
α“0.01. Forthesteeringsetup,wechoosethetotalutilityasηgoal,andusePPOtotrainthesteering
strategy(onecanchooseotherRLorcontrolalgorithmsbesidesPPO).Wealsoconductexperiments
inarepresentativezero-sumgame‘MatchingPennies’,whichwedeferthedetailstoAppx.H.2.
Grid World Stag Hunt Game: Learning Steering Strategy with Observations on Agents’
Behaviors In the previous experiments, we consider the direct parameterization and the state
space X “ Π Ă R4 has low dimension. In real-world scenarios, the policy space Π can be
extremely rich and high-dimensional if the agents consider neural networks as policies. In ad-
dition, the mediator may not get access to the agents’ exact policy π because of privacy issues.
Thismotivatesustoinvestigatethepossibility
ofsteeringagentswithobservationsonagents’
4.0
behavior only (e.g. trajectories of agents in a
3.0
gameG),insteadofthefullobservationofπ.
2.0
InAppx.G,wejustifythissetupandformalizeit 1.0 with steering
asapartiallyobservableextensionofourcurrent no steering
framework.Weconsidertheevaluationinagrid- 0 5 10 15
Steering Step t
worldversionoftheStagHuntGameasshown
(a) (b)
inFig.2-(a). Inthissetting,thestatespacein
gameGbecomespixel-basedimages,andboth Figure2: Grid-WorldVersionofStagHuntGame.
agents(blueandred)willadoptConvolutional Left: Illustration of game. Right: The perfor-
Neural Networks (CNN) based policies with mance of agents with/without steering. Without
thousandsofparametersandupdatewithPPO. steering,theagentsconvergetogoforhares,which
We train a steering strategy, which only takes hassub-optimalutility. Underourlearnedsteering
theagents’recenttrajectoriesasinputtoinfer strategy, the agents converge to a better equilib-
the steering reward. As shown in Fig. 2-(b), riumandchasethestag.
without direct usage of the agents’ policy, we
canstilltrainasteeringstrategytowardsdesired
solution.
6.2 LearningSteeringStrategieswithoutKnowledgeoff˚
Small Model Set |F|: Belief State Based Steering Strategy In this part, we evaluate Proc. 1
designedforsmallF. Weconsiderthesamenormal-formStagHuntgameandsetupasSec.6.1,
whiletheagentsupdatebytheNPGwitharandomlearningrateα“rξs`,whereξ „Npµ,0.32q.
Herethemeanvalueµisunknowntothemediator,andweconsideramodelclassF :“tf ,f u
0.7 1.0
includingtwopossiblevaluesofµPt0.7,1.0u. WereportourexperimentalresultsinTable2.
Firstly,wedemonstratethesuboptimalbehaviorifthemediatorignoresthemodeluncertaintyandjust
randomlydeploystheoptimalstrategyoff orf . Todothis,wetrainthe(history-independent)
0.7 1.0
optimalsteeringstrategybyAlg.3,asifweknowf˚ “ f (orf˚ “ f ),whichwedenoteas
0.7 1.0
ψ˚ (orψ˚ ). TomeetwithourDesideratum1,wefirstsettheaccuracylevelε“0.01,andsearch
0.7 1.0
theminimalβ sothatthelearnedsteeringstrategycanachieveε-steeringgap(seeAppx.H.3.1).
Becauseofthedifferenceinµ,wehaveβ “70andβ “20intrainingψ˚ andψ˚ ,respectively,
0.7 1.0
andempirically,weobservethatψ˚ requiresmuchlargersteeringrewardthanψ˚ . Aswemarked
0.7 1.0
inredinTable2-(a)and(b),becauseofthedifferenceinthesteeringsignal,ψ˚ consumesmuch
0.7
highersteeringcosttoachievethesameaccuracylevelinf ,andψ˚ mayfailtosteeragentswith
1.0 1.0
f tothedesiredaccuracy. Next,wetrainanotherstrategyψ˚ viaAlg.4,whichpredictsthe
0.7 Belief
9
ytilitU
latoTsteeringrewardbasedonboththeagents’policyπandthebeliefstateofthemodel. Aswecansee,
ψ˚ canalmostalwaysachievethedesiredε-steeringgapwithreasonablesteeringcost.
Belief
Table2: EvaluationforProc.1(Averagedover25differentinitialπ ,seeAppx.H.1).
1
(a)Performanceinf (b)Performanceinf
0.7 1.0
pp∆ ďεq C pp∆ ďεq C
ψ,T ψ,T ψ,T ψ,T
ψ˚ 0.99˘0.01 10.6˘0.3 ψ˚ 1.00˘0.00 8.2˘0.2
0.7 0.7
ψ˚ 0.13˘0.02 7.6˘0.2 ψ˚ 1.00˘0.00 5.6˘0.2
1.0 1.0
ψ˚ 0.87˘0.05 10.5˘0.4 ψ˚ 0.99˘0.01 6.1˘0.3
Belief Belief
Large Model Set |F|: The FETE Framework In this part, we evaluate the FETE framework
(Proc.2inSec.5.2). WeconsiderN-playernormal-formcooperativegameswithtwoactionsAandB.
Eachagentwillreceivethesamerewardifallofthemtakethesameactions,andnorewardotherwise.
TherewardsforactionAandBare2and1,respectively.
In this experiment, we consider
“opportunistic”agentswhotendto
0.02 30
increasethelearningratesifthead- 1.0 Oracle Oracle
FETE 25 FETE
vantageofoneactionovertheother 20
isclear. Moreconcretely,thelearn- 0.6 0.01 15
ingrateofagentnisα “ rξ s` 10
n n
withξ „Np1.0`r|Qn,π pAq´ 0.2 Random Exploration 5
Qn |r, `π un pBq| ´ λns`,0.5| 2r q` ,u where 0 30 St1 e0 e0 riO nur gs 2 S0 t0
ep
t300 0 fA TrufB
e
ModfC
els
fD 0 fA TrufB
e
ModfC
els
fD
λn is the unknown threshold pa-
Figure3: EvaluationforProc.2. Left: Probabilityofidentify-
rameter. In our experiments, we ingf˚byf . Ourscanachievenear100%successratewith
MLE
setN “10,andforeachnPrNs,
30steeringsteps,whiletherandomexplorationtakesmorethan
λn P t0.5,1.0,1.5,`8u, which 300steps. MiddleandRight: ComparisonbetweenFETEand
resultsinanextremelylargemodel Oracles. FETEcanachievecompetitiveperformanceinboth
class F with |F| “ 410. In each steeringgapandcost.
step t P rTs, the mediator can
observeonelearningratessample
tα u ofagents,andweneedtoestimatethetruetλnu fromthosesamples. Bydesign,
n nPrNs nPrNs
the true model can be quickly identified if the steering reward u can lead to a large value gap
|Qn,π pAq´Qn,π pBq|. WeevaluateourProc.2andreporttheresultsinFig3. Weconsiderthe
|r`u |r`u
fixedinitialpolicywith@nPrNs, πnpAq“1´πnpBq“1{3. Fortheexploration,comparingwith
1 1
thenoise-basedrandomexploration(Canyakmazetal.,2024),wecanseetheclearadvantageofour
strategytrainedbyrewardsignalsreflectingexplorationefficacy6. Besides,wealsocomparethesteer-
inggapandcostbetweenourFETEmethodandtheOraclesteeringstrategy–ifthemediatorknows
r
f˚ inadvance(bysolvingObj.(1)withF “ tf˚u). WechooseexplorationhorizonT “ 30sug-
gestedbythepreviousexplorationexperiment,andreportfourrealizationsoff˚ Ptf ,f ,f ,f u.
A B C D
Forf ,f andf ,theagentsshareλn “0.5,1.5,`8,respectively. f hasadiversesetupwhere
A B C D
λn “0.5for1ďnď5andλn “1.5for5ănď10. Aswecansee,thesteeringgapofourFETE
remainslowcomparingwiththeinitialgapmax ηgoalpπq´ηgoalpπ q«2.0,andthesteeringcost
π 1
iscompetitive.
7 Conclusion
In this paper, we introduce the problem of steering Markovian agents to desired game outcomes
undermodeluncertainty. Weprovidetheoreticalfoundationsforthisproblembyformulatinganovel
optimizationobjectiveandprovidingexistenceresults. Moreover, wedesignseveralalgorithmic
approachessuitableforvaryingdegreesofmodeluncertaintyinthisproblemclass. Wetesttheir
performancesindifferentexperimentalsettingsandshowtheireffectiveness. Ourworkopensup
avenuesforcompellingopenproblemsthatmeritfutureinvestigation. Firstly,futureworkcouldaim
toidentifysuperioroptimizationobjectivesthatguaranteestrictlybetterperformancesintermsof
6Empirically,weuseposteriorprobabilityastrainingsignal,seeAppx.H.3.2formoreexplanation.
10
)*f=ELMf(rP
T, T,Csteeringgapandcostthanours. Secondly,whenapplyingourstrategiesinreal-worldapplications,
constraintsonthesteeringrewardbudgetcouldbeadded.Finally,theframeworkcouldbegeneralized
topermitnon-Markovianagents.
AcknowledgmentsandDisclosureofFunding
TheworkissupportedbyETHresearchgrantandSwissNationalScienceFoundation(SNSF)Project
Funding No. 200021-207343 and SNSF Starting Grant. VT is supported by an ETH AI Center
DoctoralFellowship. HHNissupportedbytheSNSFEccellenzaGrant‘MarketsandNorms’.
References
Abel,D.,Barreto,A.,VanRoy,B.,Precup,D.,vanHasselt,H.P.,andSingh,S.(2024). Adefinition
ofcontinualreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,36.
Agarwal,A.,Kakade,S.M.,Lee,J.D.,andMahajan,G.(2021). Onthetheoryofpolicygradient
methods:Optimality,approximation,anddistributionshift.JournalofMachineLearningResearch,
22(98):1–76.
Akin,E.andLosert,V.(1984). Evolutionarydynamicsofzero-sumgames. Journalofmathematical
biology,20:231–258.
Auer,P.,Jaksch,T.,andOrtner,R.(2008). Near-optimalregretboundsforreinforcementlearning.
Advancesinneuralinformationprocessingsystems,21.
Azar,M.G.,Osband,I.,andMunos,R.(2017). Minimaxregretboundsforreinforcementlearning.
InInternationalconferenceonmachinelearning,pages263–272.PMLR.
Bai,Y.,Jin,C.,andYu,T.(2020). Near-optimalreinforcementlearningwithself-play. Advancesin
neuralinformationprocessingsystems,33:2159–2170.
Balcan, M.-F., Blum, A., andMansour, Y.(2013). Circumventingthepriceofanarchy: Leading
dynamicstogoodbehavior. SIAMJournalonComputing,42(1):230–264.
Baumann,T.,Graepel,T.,andShawe-Taylor,J.(2020). Adaptivemechanismdesign: Learningto
promotecooperation. In2020InternationalJointConferenceonNeuralNetworks(IJCNN),pages
1–7.IEEE.
Cai,Y.,Luo,H.,Wei,C.-Y.,andZheng,W.(2024). Near-optimalpolicyoptimizationforcorrelated
equilibriumingeneral-summarkovgames. arXivpreprintarXiv:2401.15240.
Canyakmaz,I.,Sakos,I.,Lin,W.,Varvitsiotis,A.,andPiliouras,G.(2024). Steeringgamedynamics
towardsdesiredoutcomes. arXivpreprintarXiv:2404.01066.
Chakraborty, S., Bedi, A. S., Koppel, A., Manocha, D., Wang, H., Wang, M., and Huang, F.
(2023). Parl: Aunifiedframeworkforpolicyalignmentinreinforcementlearning. arXivpreprint
arXiv:2308.02585.
Chen, S., Yang, D., Li, J., Wang, S., Yang, Z., andWang, Z.(2022). Adaptivemodeldesignfor
markovdecisionprocess. InInternationalConferenceonMachineLearning,pages3679–3700.
PMLR.
Curry,M.,Thoma,V.,Chakrabarti,D.,McAleer,S.,Kroer,C.,Sandholm,T.,He,N.,andSeuken,S.
(2024). Automateddesignofaffinemaximizermechanismsindynamicsettings. Proceedingsof
theAAAIConferenceonArtificialIntelligence,38(9):9626–9635.
Dann, C. and Brunskill, E. (2015). Sample complexity of episodic fixed-horizon reinforcement
learning. AdvancesinNeuralInformationProcessingSystems,28.
Dann,C.,Lattimore,T.,andBrunskill,E.(2017). Unifyingpacandregret: Uniformpacboundsfor
episodicreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,30.
11Daskalakis,C.,Fishelson,M.,andGolowich,N.(2021). Near-optimalno-regretlearningingeneral
games. AdvancesinNeuralInformationProcessingSystems,34:27604–27616.
Daskalakis,C.,Foster,D.J.,andGolowich,N.(2020). Independentpolicygradientmethodsfor
competitivereinforcementlearning. Advancesinneuralinformationprocessingsystems,33:5527–
5540.
Deng,Y.,Schneider,J.,andSivan,B.(2019). Strategizingagainstno-regretlearners. InWallach,H.,
Larochelle,H.,Beygelzimer,A.,d'Alché-Buc,F.,Fox,E.,andGarnett,R.,editors,Advancesin
NeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.
Ding,D.,Wei,C.-Y.,Zhang,K.,andJovanovic,M.(2022).Independentpolicygradientforlarge-scale
markovpotentialgames: Sharperrates,functionapproximation,andgame-agnosticconvergence.
InInternationalConferenceonMachineLearning,pages5166–5220.PMLR.
Dong, K., Wang, Y., Chen, X., andWang, L.(2019). Q-learningwithucbexplorationissample
efficientforinfinite-horizonmdp. arXivpreprintarXiv:1901.09311.
Fiez, T., Chasnov, B., and Ratliff, L. (2020). Implicit learning dynamics in stackelberg games:
Equilibriacharacterization,convergenceanalysis,andempiricalstudy. InIII,H.D.andSingh,A.,
editors,Proceedingsofthe37thInternationalConferenceonMachineLearning,volume119of
ProceedingsofMachineLearningResearch,pages3133–3144.PMLR.
Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. (2018).
Learningwithopponent-learningawareness. InProceedingsofthe17thInternationalConference
onAutonomousAgentsandMultiAgentSystems,pages122–130.
Fudenberg,D.andLevine,D.K.(1998). TheTheoryofLearninginGames,volume1ofMITPress
Books. TheMITPress.
Gerstgrasser,M.andParkes,D.C.(2023). Oraclesandfollowers: Stackelbergequilibriaindeep
multi-agentreinforcementlearning. InKrause,A.,Brunskill,E.,Cho,K.,Engelhardt,B.,Sabato,
S.,andScarlett,J.,editors,Proceedingsofthe40thInternationalConferenceonMachineLearning,
volume202ofProceedingsofMachineLearningResearch,pages11213–11236.PMLR.
Giannou, A., Lotidis, K., Mertikopoulos, P., and Vlatakis-Gkaragkounis, E.-V. (2022). On the
convergenceofpolicygradientmethodstonashequilibriaingeneralstochasticgames. Advances
inNeuralInformationProcessingSystems,35:7128–7141.
Gong,L.,Yao,W.,Gao,J.,andCao,M.(2022). Limitcyclesanalysisandcontrolofevolutionary
gamedynamicswithenvironmentalfeedback. Automatica,145:110536.
Guo,X.,Li,L.,Nabi,S.,Salhab,R.,andZhang,J.(2023). Mesob: Balancingequilibria&social
optimality.
Harsanyi,J.C.andSelten,R.(1988). Ageneraltheoryofequilibriumselectioningames. MITPress
Books,1.
Harsanyi,J.C.andSelten,R.(1992). Ageneraltheoryofequilibriumselectioningames. TheMIT
PressClassics.TheMITPress,CambridgeMass,[2ndprinting]edition.
Hernandez-Leal,P.,Kaisers,M.,Baarslag,T.,andDeCote,E.M.(2017). Asurveyoflearningin
multiagentenvironments: Dealingwithnon-stationarity. arXivpreprintarXiv:1707.09183.
Huang,J.,He,N.,andKrause,A.(2024a). Model-basedrlformean-fieldgamesisnotstatistically
harderthansingle-agentrl. arXivpreprintarXiv:2402.05724.
Huang,J.,Yardim,B.,andHe,N.(2024b). Onthestatisticalefficiencyofmean-fieldreinforcement
learningwithgeneralfunctionapproximation.InInternationalConferenceonArtificialIntelligence
andStatistics,pages289–297.PMLR.
Ibe,O.(2013). Markovprocessesforstochasticmodeling. Newnes.
12Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is q-learning provably efficient?
Advancesinneuralinformationprocessingsystems,31.
Jin,C.,Liu,Q.,Wang,Y.,andYu,T.(2021). V-learning–asimple,efficient,decentralizedalgorithm
formultiagentrl. arXivpreprintarXiv:2110.14555.
Kakade,S.M.(2001). Anaturalpolicygradient. Advancesinneuralinformationprocessingsystems,
14.
Khetarpal,K.,Riemer,M.,Rish,I.,andPrecup,D.(2022). Towardscontinualreinforcementlearning:
Areviewandperspectives. JournalofArtificialIntelligenceResearch,75:1401–1476.
Leonardos,S.,Overman,W.,Panageas,I.,andPiliouras,G.(2021). Globalconvergenceofmulti-
agentpolicygradientinmarkovpotentialgames. arXivpreprintarXiv:2106.01969.
Li,J.,Yu,J.,Nie,Y.M.,andWang,Z.(2020). End-to-endlearningandinterventioningames.
Littman,M.L.(1994). Markovgamesasaframeworkformulti-agentreinforcementlearning. In
Machinelearningproceedings1994,pages157–163.Elsevier.
Liu,B.,Li,J.,Yang,Z.,Wai,H.-T.,Hong,M.,Nie,Y.,andWang,Z.(2022). Inducingequilibria
viaincentives: Simultaneousdesign-and-playensuresglobalconvergence. AdvancesinNeural
InformationProcessingSystems,35:29001–29013.
Lu, C., Willi, T., De Witt, C. A. S., and Foerster, J. (2022). Model-free opponent shaping. In
InternationalConferenceonMachineLearning,pages14398–14411.PMLR.
Luo,Z.-Q.,Pang,J.-S.,andRalph,D.(1996). MathematicalProgramswithEquilibriumConstraints.
CambridgeUniversityPress.
Mertikopoulos,P.,Papadimitriou,C.,andPiliouras,G.(2018). Cyclesinadversarialregularized
learning. InProceedingsofthetwenty-ninthannualACM-SIAMsymposiumondiscretealgorithms,
pages2703–2717.SIAM.
Monderer,D.andTennenholtz,M.(2004). K-implementation. J.Artif.Int.Res.,21(1):37–62.
Newton,J.(2021). Conventionsunderheterogeneousbehaviouralrules. TheReviewofEconomic
Studies,88(4):2094–2118.
Paarporn,K.,Eksin,C.,Weitz,J.S.,andWardi,Y.(2018). Optimalcontrolpoliciesforevolutionary
dynamicswithenvironmentalfeedback.In2018IEEEConferenceonDecisionandControl(CDC),
pages1905–1910.
Paccagnan,D.andGairing,M.(2021). Incongestiongames,taxesachieveoptimalapproximation. In
Proceedingsofthe22ndACMConferenceonEconomicsandComputation,EC’21,page743–744,
NewYork,NY,USA.AssociationforComputingMachinery.
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. (2021). Stable-
baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning
Research,22(268):1–8.
Ratliff,L.J.,Dong,R.,Sekar,S.,andFiez,T.(2019). Aperspectiveonincentivedesign: Challenges
andopportunities. AnnualReviewofControl,Robotics,andAutonomousSystems,2(1):305–338.
Riehl,J.,Ramazi,P.,andCao,M.(2018). Asurveyontheanalysisandcontrolofevolutionarymatrix
games. AnnualReviewsinControl,45:87–106.
Roughgarden, T. and Tardos, É. (2004). Bounding the inefficiency of equilibria in nonatomic
congestiongames. GamesandEconomicBehavior,47(2):389–403.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy
optimizationalgorithms. arXivpreprintarXiv:1707.06347.
13Schuster,P.andSigmund,K.(1983).Replicatordynamics.Journaloftheoreticalbiology,100(3):533–
538.
Shen,H.,Yang,Z.,andChen,T.(2024). Principledpenalty-basedmethodsforbilevelreinforcement
learningandrlhf. arXivpreprintarXiv:2402.06886.
Skyrms,B.(2004). Thestaghuntandtheevolutionofsocialstructure. CambridgeUniversityPress.
Thoma,V.,Pasztor,B.,Krause,A.,Ramponi,G.,andHu,Y.(2024). Stochasticbileveloptimization
withlower-levelcontextualmarkovdecisionprocesses. arXivpreprintarXiv:2406.01575.
Wang,J.,Song,M.,Gao,F.,Liu,B.,Wang,Z.,andWu,Y.(2023). Differentiablearbitratinginzero-
summarkovgames. InProceedingsofthe2023InternationalConferenceonAutonomousAgents
andMultiagentSystems,AAMAS’23,page1034–1043,Richland,SC.InternationalFoundation
forAutonomousAgentsandMultiagentSystems.
Wang,K.,Xu,L.,Perrault,A.,Reiter,M.K.,andTambe,M.(2022). Coordinatingfollowerstoreach
betterequilibria: End-to-endgradientdescentforstackelberggames. ProceedingsoftheAAAI
ConferenceonArtificialIntelligence,36(5):5219–5227.
Wei, C.-Y., Jahromi, M.J., Luo, H., Sharma, H., andJain, R.(2020). Model-freereinforcement
learningininfinite-horizonaverage-rewardmarkovdecisionprocesses.InInternationalconference
onmachinelearning,pages10170–10180.PMLR.
Willi, T., Letcher, A. H., Treutlein, J., and Foerster, J. (2022). Cola: consistent learning with
opponent-learningawareness. InInternationalConferenceonMachineLearning,pages23804–
23831.PMLR.
Willis,R.,Du,Y.,Leibo,J.,andLuck,M.(2023). Resolvingsocialdilemmasthroughrewardtransfer
commitments. AdaptiveandLearningAgentsWorkshop;Conferencedate: 29-05-2023Through
30-05-2023.
Xiao, L. (2022). On the convergence rates of policy gradient methods. The Journal of Machine
LearningResearch,23(1):12887–12922.
Yang,J.,Li,A.,Farajtabar,M.,Sunehag,P.,Hughes,E.,andZha,H.(2020). Learningtoincentivize
otherlearningagents. InProceedingsofthe34thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’20,RedHook,NY,USA.CurranAssociatesInc.
Yang,J.,Wang,E.,Trivedi,R.,Zhao,T.,andZha,H.(2022). Adaptiveincentivedesignwithmulti-
agentmeta-gradientreinforcementlearning. InProceedingsofthe21stInternationalConference
onAutonomousAgentsandMultiagentSystems,AAMAS’22,page1436–1445,Richland,SC.
InternationalFoundationforAutonomousAgentsandMultiagentSystems.
Yardim,B.,Cayci,S.,Geist,M.,andHe,N.(2023).Policymirrorascentforefficientandindependent
learninginmeanfieldgames. InInternationalConferenceonMachineLearning,pages39722–
39754.PMLR.
Young, H. P. (1993). The evolution of conventions. Econometrica: Journal of the Econometric
Society,pages57–84.
Zhang,B.H.,Farina,G.,Anagnostides,I.,Cacciamani,F.,McAleer,S.M.,Haupt,A.A.,Celli,A.,
Gatti,N.,Conitzer,V.,andSandholm,T.(2023). Steeringno-regretlearnerstooptimalequilibria.
arXivpreprintarXiv:2306.05221.
Zhang,K.,Yang,Z.,andBas¸ar,T.(2021). Multi-agentreinforcementlearning: Aselectiveoverview
oftheoriesandalgorithms. Handbookofreinforcementlearningandcontrol,pages321–384.
Zhao,S.,Lu,C.,Grosse,R.B.,andFoerster,J.(2022). Proximallearningwithopponent-learning
awareness. AdvancesinNeuralInformationProcessingSystems,35:26324–26336.
14Zhong,H.,Yang,Z.,Wang,Z.,andJordan,M.I.(2024). Canreinforcementlearningfindstackelberg-
nashequilibriaingeneral-summarkovgameswithmyopicallyrationalfollowers? J.Mach.Learn.
Res.,24(1).
Zinkevich,M.(2003). Onlineconvexprogrammingandgeneralizedinfinitesimalgradientascent. In
Proceedingsofthe20thinternationalconferenceonmachinelearning(icml-03),pages928–936.
15Contents
1 Introduction 1
1.1 CloselyRelatedWorks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminary 4
3 TheProblemFormulationoftheSteeringMarkovianAgents 4
4 ExistenceofSteeringStrategywithε-SteeringGap 6
4.1 Existencewhenf˚isKnown: NaturalPolicyGradientasanExample . . . . . . . 6
4.2 Existencewhenf˚isUnknown: theIdentifiableModelClass. . . . . . . . . . . . 7
5 Learning(Approximately)OptimalSteeringStrategy 7
5.1 SmallModelClass: DynamicProgrammingwithModelBeliefState . . . . . . . . 7
5.2 LargeModelClass: AFirst-Explore-Then-ExploitFramework . . . . . . . . . . . 8
6 Experiments 9
6.1 LearningSteeringStrategieswithKnowledgeoff˚ . . . . . . . . . . . . . . . . . 9
6.2 LearningSteeringStrategieswithoutKnowledgeoff˚ . . . . . . . . . . . . . . . 9
7 Conclusion 10
A DiscussiononLimitationsandSocietalImpact 18
B FrequentlyUsedNotations 18
C MissingDetailsintheMainText 19
C.1 AReal-WorldScenariothatCanbeModeledasaStagHuntGame . . . . . . . . . 19
C.2 AdditionalRelatedWorks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.3 ABriefIntroductiontoMarkovDecisionProcess . . . . . . . . . . . . . . . . . . 20
C.4 AlgorithmforLearningOptimal(History-Independent)Strategywhenf˚isKnown 20
C.5 AlgorithmforLearningBelief-StateDependentSteeringStrategy. . . . . . . . . . 20
D MissingProofsinSection3 20
E MissingProofsforExistencewhentheTrueModelf˚isKnown 21
E.1 MoreDetailsaboutPolicyMirrorDescent . . . . . . . . . . . . . . . . . . . . . . 21
E.2 ProofsfortheExistenceofDesiredSteeringStrategy . . . . . . . . . . . . . . . . 23
E.2.1 SpecialCase: PMDwithExactAdvantage-Value . . . . . . . . . . . . . . 23
E.2.2 TheGeneralIncentiveDrivenAgentsunderAssump.B . . . . . . . . . . . 25
F MissingProofsforExistencewhentheTrueModelf˚isUnknown 26
G GeneralizationtoPartialObservationMDPSetup 28
16G.1 POMDPBasics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
G.2 SteeringProcessasaPOMDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
H MissingExperimentDetails 29
H.1 AboutInitializationinEvaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . 29
H.2 ExperimentsforKnownModelSetting . . . . . . . . . . . . . . . . . . . . . . . . 29
H.2.1 ExperimentDetailsinNormal-FormStagHuntGame. . . . . . . . . . . . 29
H.2.2 ExperimentDetailsinGrid-WorldVersionofStagHuntGame . . . . . . . 30
H.2.3 ExperimentsinMatchingPennies . . . . . . . . . . . . . . . . . . . . . . 31
H.3 ExperimentsforUnknownModelSetting . . . . . . . . . . . . . . . . . . . . . . 31
H.3.1 DetailsforExperimentswithSmallModelSetF . . . . . . . . . . . . . . 31
H.3.2 DetailsforExperimentswithLargeModelSetF . . . . . . . . . . . . . . 32
H.4 ASummaryoftheComputeResourcesbyExperimentsinthisPaper . . . . . . . . 33
17A DiscussiononLimitationsandSocietalImpact
DiscussiononLimitations ThemainlimitationofthispaperisthatweonlyfocusonMarkovian
agents,whosedynamicsf onlydependsontheircurrentpolicyprofilesandthe(modified)reward
function. Althoughthismodelcanalreadycapturelotsoflearningdynamics,notallthetheoretical
resultsandmethodologycanbegeneralizedtonon-Markoviansettings.
Besides,wecannotguaranteehowtheoptimalsteeringstrategyregardingourObjective(1)performs
intermsofthesteeringcost. Weleaveittothefutureworks.
Moreover, for the large model class setting, the strategy obtained by running proposed FETE
frameworkmaynothaveguaranteesonthesteeringcost. Itisalsouncleariftherearebetteralgorithm
designsforthissetting.
Discussion on Societal Impact Although this work are not directly applicable to real-world
applications,butwebelieveitmayhavepositivesocietalimpact. Thesteeringproblemwestudy
matches the requirement of many real-world scenarios (e.g. the social planner may incentivize
technologycompaniesbyindividualfinancialsubsidies). Webelievethetheoreticalandalgorithmic
contributionsinthispapercanprovideusefulinsightsforthoseapplications.
B FrequentlyUsedNotations
Notation Description
G Afinite-horizongeneral-sumMarkovGame
N Thenumberofagents
S,A StatespaceandactionspaceofthegameG
H ThehorizonofthegameG
P TransitionfunctionofthegameG
r RewardfunctionofthegameG
π Theagents’policy(collectionofpoliciesofallagents)
π Theinitialpolicy
1
M Afinite-horizonMarkovDecisionProcess(thesteeringMDP)
X,U StatespaceandactionspaceofM
T ThehorizonofM (i.e. thehorizonofthesteeringdynamics)
T (Stationary)TransitionfunctionofM
ηcost ThesteeringcostfunctionofM
ψ Thehistory-dependentsteeringstrategybymediator
u(oru foraspecifichorizont) Thesteeringrewardfunction
t
U Theupperboundforsteeringreward
max
f Agentslearningdynamics(T“f inthesteeringMDP)
ηgoal ThegoalfunctionofM
F Themodelclassofagentsdynamics(withfinitecandidates)
β RegularizationcoefficientinObj.(1)
ř
C pfq ThetotalexpectedsteeringcostE r T ηcostpπ ,u qs
ψ,T ψ,f t“1 t t
∆ pfq Thesteeringgap: E rmax ηgoalpπq´ηgoalpπ qs
ψ,T ψ,f π T`1
Ψ Thecollectionofallhistorydependentpolicies
ΨεasashortnoteofΨε pF;π q tψ PΨ|E rmax ηgoalpπq´ηgoalpπ q|π sďεu
T,Umax 1 ψ,f π T`1 1
Qn,π ,Vn,π ,An,π TheQ-value,V-valueandadvantagevaluefunctionsforagentn
h|r`u h|r`u h|r`u
f TheMaximalLikelihoodEstimator(introducedinDef.4.3)
MLE
b ModelbeliefstaterPrpf|tπ ,u ut ,π qs PR|F|
t t1 t1 t1“1 t fPF
ψExplore{ψExploit Theexploration/exploitationpolicyinFETEframework.
r r r Ă
Op¨q,Ωp¨q,Θp¨q,Op¨q,Ωp¨q,Θp¨q StandardBig-Onotations,p¨qomitsthelogterms.
18C MissingDetailsintheMainText
C.1 AReal-WorldScenariothatCanbeModeledasaStagHuntGame
As a real-world example, the innovation adaption can be modeled as a (multi-player) Stag Hunt
game. Considerasituationinvolvingacoordinationproblemwherepeoplecanchoosebetweenan
inferior/unsustainablecommunicationortransportationtechnologythatischeap(theGatheraction)
andasuperiortechnologythatissustainablebutmoreexpensive(theHuntaction). Ifmoreandmore
peoplebuyproductsbythesuperiortechnology,theincreasingprofitscanleadtothedevelopment
ofthattechnologyandthedecreaseofprice. Eventually,everyonecanaffordthepriceandbenefit
fromthesustainabletechnology. Incontrast,ifpeoplearetrappedbytheproductsoftheinferior
technologyduetoitslowprice,thelong-runsocialwelfarecanbesub-optimal. Themediator’sgoal
istosteerthepopulationtoadoptthesuperiortechnology.
C.2 AdditionalRelatedWorks
LearningDynamicsinMulti-AgentSystems Inmulti-agentsetting,itisanimportantquestion
todesignlearningdynamicsandunderstandtheirconvergenceproperties(Hernandez-Lealetal.,
2017). Previousworkshasestablishednear-optimalconvergenceguaranteestoequilibra(Daskalakis
et al., 2021; Cai et al., 2024). When the transition model of the multi-agent system is unknown,
manypreviousworkshavestudiedhowtoconductefficientexplorationandlearnequilibriaunder
uncertainty(Jinetal.,2021;Baietal.,2020;Zhangetal.,2021;Leonardosetal.,2021;Yardimetal.,
2023;Huangetal.,2024b,a). However,mostoftheseresultsonlyhaveguaranteesonsolvingan
arbitraryequilibriumwhenmultipleequilibriaexists,anditisunclearhowtobuildalgorithmsbased
onthemtoreachsomedesiredpoliciestomaximizesomegoalfunctions.
MathematicalProgrammingwithEquilibriumConstraints(MPEC) MPECgeneralisesbilevel
optimizationtoproblemswherethelowerlevelconsistsofsolvinganequilibriumproblem(Luo
etal.,1996). (Lietal.,2020;Liuetal.,2022;Wangetal.,2022,2023;Yangetal.,2022). These
worksconsidervariantsofanMPECandpresentgradientbasedapproaches,mostofwhichrelyon
computinghypergradientsviatheimplicitfunctiontheoremandthusstrongassumptionsonthelower
levelproblem,suchasuniquenessoftheequilibrium. Mostgamesfailtosatisfysuchconstraints. In
contrast,ourworkmakesnoassumptionsontheequilibriumstructureandinsteadmildassumptions
onthelearningdynamics.
GameTheoryandMechanismDesign InGameTheory,asetupsuchasourscanbemodelled
as a Stackelberg game. Several works have considered finding Stackelberg equilibria using RL
(GerstgrasserandParkes,2023;Zhongetal.,2024)orgradient-basedapproaches(Fiezetal.,2020).
Dengetal.(2019)showedhowagentscanmanipulatelearningalgorithmstoachievemorereward,as
iftheywereplayingaStackelberggame. Relatedproblemsareimplementationtheory(Monderer
andTennenholtz,2004)andequilibriumselection(HarsanyiandSelten,1992). Moreover,thefield
of mechanism design has been concerned with creating economic games that implement certain
outcomesastheirequilibria. SeveralrecentworkshaveconsideredmechanismdesignonMarkov
Games(Curryetal.,2024;Baumannetal.,2020;Guoetal.,2023). Inthecaseofcongestiongames,
mechanismshavebeenproposedtocircumventthepriceofanarchy(Balcanetal.,2013;Paccagnan
andGairing,2021;RoughgardenandTardos,2004),i.e. equililbriawithlowsocialwelfare.
Thereisalsoalineofworkhasfocusedoncontrolstrategiesforevolutionarygames(Gongetal.,
2022;Paarpornetal.,2018). However,thegameandlearningdynamicsdiffersignificantlyfromour
setting. Forafullsurveyofcontrol-theoreticapproaches,wereferthereadertoRatliffetal.(2019);
Riehletal.(2018).
BilevelReinforcementLearning BilevelRLconsiderstheproblemofdesigninganMDP—by
for example changing the rewards—with a desireable optimal policy. Recently, several works
havestudiedgradient-basedapproachestofindsuchgoodMDPconfigurations(Chenetal.,2022;
Chakrabortyetal.,2023;Shenetal.,2024;Thomaetal.,2024). Whilesimilarinsomeregards,in
thissettingweassumethelowerlevelisaMarkovGameinsteadofjustanMDP.Moreover,ouraim
isnottodesignagamewithadesireableequilibriumfromscratch,buttotakeagivengameand
agentdynamicsandsteerthemwithminimaladditionalrewardstoadesiredoutcomewithinacertain
19amountoftime. Thereforeourupper-levelproblemisastrategicdecision-makingproblem,solvedby
RLinsteadofrunninggradientdescentonsomeparameterspace.
C.3 ABriefIntroductiontoMarkovDecisionProcess
Afinite-horizonMarkovDecisionProcessisspecifiedbyatupleM :“tx ,T,X,U,T,pη,ηtermqu,
1
where x is the fixed initial state, T is the horizon length, X is the state space, U is the action
1
space. Besides, T :“ tT u with T : X ˆ U Ñ ∆pXq denoting the transition function7,
t tPrTs t
η :“tη u withη :X ˆU Ñr0,1sisthenormalrewardfunctionandηterm :X ˆU Ñr0,1s
t tPrTs t
denotes the additional terminal reward function. In this paper, without further specification, we
will consider history dependent non-stationary policies Ψ :“ tψ :“ tψ ,...,ψ u|@t P rTs,ψ :
1 T t
pX ˆUqt´1 ˆX Ñ ∆pUqu. Givenaψ P Ψ, anepisodeofM isgeneratedby: @t P rTs, u „
t
ψ p¨|tx ,u ut´1,x q, η Ðη px ,u q, x „T p¨|x ,u q; ηterm Ðηtermpx q;
t t1 t1 t1“1 t t t t t t`1 t t t T`1
C.4 AlgorithmforLearningOptimal(History-Independent)Strategywhenf˚isKnown
Algorithm3:LearningwithKnownSteeringDynamics
1 Input: ModelSetF :“tf˚u;Initialsteeringstrategyψ1 :“tψ t1u tPrTs;Regularization
coefficientβ;IterationnumberK;
2 fork “1,2,...,K do
3 Agentsinitializewithpolicyπ 1k.
4 Sampletrajectorieswithψ ζk,@tPrTs:
uk „ψkp¨|πkq, πk „f˚p¨|πk,r`ukq, ηk “´ηcostpπk,ukq.
t t t t`1 t t t t t
5 Updateψk`1 ÐRLAlgorithmpψk,tπ tk,uk t,η tkuT t“1Ytβ¨ηgoalpπ Tk `1quq.
6 end
p
7 Outputψ˚ Ðψ ζK.
C.5 AlgorithmforLearningBelief-StateDependentSteeringStrategy
Algorithm4:SolvingObj.(1)byLearningBeliefState-DependentStrategy
1 Input: ModelSetF;Regularizationcoefficientβ;Initialsteeringstrategyψ1 :“tψ t1uT t“1;
IterationnumberK;
2 fork “1,2,...,K do
3 Samplef „UniformpFq;Initializeπ 1k “π 1.
4 Sampletrajectorieswithψk fromsimulatoroff:
5 @tPrTs bk t :“Prp¨|π 1k,uk 1,...,π tk ´1,uk t´1,π tkq, uk t „ψ tkp¨|bk t,π tkq,
6 π tk `1 „fp¨|π tk,r`uk tq, η tk Ð´ηcostpπ tk,uk tq
7 Updateψk`1 ÐRLAlgorithmpψk,tpπ tk,bk tq,uk t,η tkuT t“1Ytβ¨ηgoalpπ Tk `1quq.
8 end
p
9 returnψ˚ :“ψK “tψ tKuT t“1
D MissingProofsinSection3
Proposition3.3. [JustificationforObj.(1)]BysolvingObj.(1): (1)ψ˚isParetoOptimal;(2)Given
anyε,ε1 ą0,ifΨε{|F| ‰Handβ ě UmaxNHT|F|,wehaveψ˚ PΨε`ε1;
ε1
7Inthispaper,wefocusonstationarytransitionfunction,i.e.T
1
“...“T T.
20Proof. SupposeΨε{|F|isnon-empty,wedenoteψε{|F|asoneoftheelementsinΨε{|F|.Bydefinition,
sincemax ηgoalpπqisfixed,wehave:
π
ÿ
1
ψ˚ Ðargmax ´β∆pψ,f,Tq´Cpψ,f,Tq.
ψ |F|
fPF
Ifβ ě UmaxNHT|F|,bydefinition,
´ ÿε1 ¯ ´ ÿ ¯
1 1
0ď ´β∆pψ˚,f,Tq´Cpψ˚,f,Tq ´ ´β∆pψε{|F|,f,Tq´Cpψε{|F|,f,Tq
|F| |F|
fPF fPF
ÿ ´ ¯
1
ď β ∆pψε{|F|,f,Tq´∆pψ˚,f,Tq `U NHT
|F| max
fPF
(thesteeringrewarduPr0,U s)
max
ÿ ´ ¯
1 ε
ď β ´∆pψ˚,f,Tq `U NHT (ψε{|F| PΨε pFq)
|F| |F| max T,Umax
fPF
´ ÿ ¯
U NHT ε 1
ď max ´ ∆pψ˚,f,Tq `U NHT
ε1 |F| |F| max
fPF
ř
Asadirectobservation,ifE r∆pψ˚,f,Tqs“ 1 ∆pψ˚,f,Tqą ε`ε1,theRHSwill
f„UnifpFq |F| fPF |F|
bestrictlylessthan0,whichresultsincontradiction. Therefore,wemusthave
@f PF, ∆pψ˚,f,Tqď|F|¨E r∆pψ˚,f,Tqsďε`ε1.
f„UnifpFq
whichimpliesψ˚ PΨε`ε1.
Next,weshowtheParetoOptimality. Ifthereexistsψandf suchthat
• Forallf1 PF withf ‰f1,Cpψ˚,f,TqěCpψ,f,Tqand∆pψ˚,f,Tqě∆pψ,f,Tq;
• Forf,eitherCpψ˚,f,TqąCpψ,f,Tqand∆pψ˚,f,Tqě∆pψ,f,TqorCpψ˚,f,Tqě
Cpψ,f,Tqand∆pψ˚,f,Tqą∆pψ,f,Tq.
Therefore,wemusthave:
ÿ ÿ
1 1
β∆pψ,f,Tq´Cpψ,f,Tqă β∆pψ˚,f,Tq´Cpψ˚,f,Tq,
|F| |F|
fPF fPF
whichconflictswiththeoptimalityconditionofObj.(1). ˝
E MissingProofsforExistencewhentheTrueModelf˚ isKnown
Inthissection,westudythePolicyMirrorDescentasaconcreteexample. InAppx.E.1,weprovide
more details about PMD. Then, we study the PMD with exact updates and stochastic updates in
Appx.E.2.1andE.2.2,respectively. ThetheoremsinSec.4.1willbesubsumedasspecialcases.
E.1 MoreDetailsaboutPolicyMirrorDescent
DefinitionE.1(PolicyMirrorDescent). ForeachagentnPrNs,theupdatesatsteptPrTsfollows:
@hPrHs,s PS, θn p¨|s qÐθn p¨|s q`αApn,πt ps,¨q, (Updateinthemirrorspace)
h t`1,h h t,h h h|rn`un
t
zn p¨|s qÐp∇ϕnq´1pθn p¨|s qq (Mapθbacktotheprimalspace)
t`1,h h t`1,h h
π tn `1,hp¨|s hqÐargminD ϕnpz,z tn `1,hp¨|s hqq, (Projection)
zP∆pAnq
SimilartoDef.4.1,hereApn,πt issomerandomestimationfortheadvantagevalueAn,πt
h|rn`un h|rn`un
with E πnrApn h|, rπ nt `unps h,¨qs “ 0. t Besides, θ tn
,h
P R|S||A| denotes the variable in the dual spact e.
ϕn : dompϕnq Ñ
Rt
isafunctionsatisfyingAssump.Cbelow,whichgivesthemirrormap∇ϕn;
p∇ϕnq´1istheinversemirrormap;D ϕnpz,zrq:“ϕnpzq´ϕnpzrq´x∇ϕnpzrq,z´zryistheBregman
divergenceregardingϕn.
21AssumptionC. Weassumeforalln P rNs,ϕn isµ-stronglyconvexandessentiallysmooth,i.e.
differentiableand}∇ϕnpzkq}Ñ`8foranysequencezk Pdompϕnqconvergingtoapointonthe
boundaryofdompϕnq.
ByPythagoreanTheoremandthestrictlyconvexityofD ,theprojectionπinDef.E.1isunique.
ϕn
LemmaE.2. GivenaconvexsetC andafunctionϕwhichisµ-stronglyconvexonC,wehave
1
}argminD pz,p∇ϕq´1pθ qq´argminD pz,p∇ϕq´1pθ qq}ď }θ ´θ } .
zPC ϕ 1 zPC ϕ 2 µ 1 2 2
Proof. Given any dual variables θ and θ , and their projection z :“
1 2 1
argmin D pz,p∇ϕq´1pθ qq and z :“ argmin D pz,p∇ϕq´1pθ qq, by the first or-
zPC ϕ 1 2 zPC ϕ 2
deroptimalitycondition,wehave:
@z PC, x∇ϕpz q´θ ,z´z yě0,
1 1 1
x∇ϕpz q´θ ,z´z yě0
2 2 2
Ifwechoosez “z inthefirstequationandz “z inthesecondequation,andsumtogether,we
2 1
have:
xθ ´∇ϕpz q`∇ϕpz q´θ ,z ´z yě0,
1 1 2 2 1 2
Bystronglyconvexityofϕ,theaboveimplies:
xθ ´θ ,z ´z yěx∇ϕpz q´∇ϕpz q,z ´z yěµ¨}z ´z }2
1 2 1 2 1 2 1 2 1 2
Therefore,
µ}z ´z }ď}θ ´θ },
1 2 1 2
andwefinishtheproof. ˝
Next,wediscusssomeconcreteexamples.
ExampleE.3(NaturalPřolicyGradient). IfweconsiderthemirrormapandBregmanDivergence
generated by ϕnpzq :“ zpanqlogzpanq, we have Dnpz ,z q “ KLpz }z q, and recover
anPAn ϕ 1 2 1 2
theNPGinDef.4.1. Notethatϕn is1-stronglyconvexontheconvexset∆pAnq, Assump.Cis
satisfiedwithµ“1.
ExampleE.4(OnlineGradientAscent(Zinkevich,2003)). IfweconsidertheEuclideandistance
generatedbyl -normϕnpzq“ 1}z}2,werecovertheprojectedgradientascent
2 2 2
DefinitionE.5. ForeachagentnPrNs,theupdatesatsteptPrTsfollows:
@hPrHs,s PS, πn p¨|s qÐProj pπn p¨|s q`αApn,πt ps,¨qq,
h t`1,h h ∆pAnq t,h h h|rn`un
t
NotethattheprojectionwithEuclideandistanceis1-Lipschitz,Assump.Cissatisfiedwithµ“1.
OtherNotationsandRemarks Inthefollowing,weuseΠ` tobethe“feasiblepolicyset”(for
NPGinDef.4.1,Π` referstobesetofpoliciesboundedawayfrom0),suchthatforanyπ PΠ`,
thereexistsadualvariableθcorrespondingtoπ,i.e.,
@nPrNs,hPrHs,s
h
PS, π hnp¨|s hqÐargminD ϕnpz,p∇ϕnq´1pθ hnp¨|s hqqq.
zP∆pAnq
InthefollowingLem.E.6,weshowthatconstantshiftinθn p¨|s qdoesnotchangetheprojection
t,h h
result. Therefore,whenwesaythedualvariableθassociatedwithagivenpolicyπ,weonlyconsider
thoseθsatisfyingE
an h„π
hnrθ hnpan h|s hqs“0.
Lemma E.6 (Constant Shift does not Change the Projection). For any n P rNs, regularizer ϕn
satisfyingconditionsinAssump.C,andanyθ PR|An|,considertheconstantvectorc1,wherecPR
isaconstantand1“t1,1,...,1uPR|An|,wehave:
argminD ϕnpz,p∇ϕnq´1pθqq“argminD ϕnpz,p∇ϕnq´1pθ`c1qq
zP∆pAnq zP∆pAnq
22Proof.
arg min D ϕnpz,p∇ϕnq´1pθ`c1qq
zP∆pAnq
“arg min ϕnpzq´xθ`c1,zy
zP∆pAnq
“arg min ϕnpzq´xθ,zy`c (wehaveconstraintsthatz P∆pAnq)
zP∆pAnq
“arg min ϕnpzq´xθ,zy
zP∆pAnq
“argminD ϕnpz,θq.
zP∆pAnq
˝
E.2 ProofsfortheExistenceofDesiredSteeringStrategy
WefirstformallyintroducetheLipschitzconditionthatThm.4.2requires.
AssumptionD(ηgoalisL-Lipschitz). Foranyπ,π1 PΠ,|ηgoalpπq´ηgoalpπ1q|ďL}π´π1} .
2
p
Inthefollowing,inAppx.E.2.1,asawarm-up,westartwiththeexactcasewhentheestimationAπ
isexactlythetrueadvantagevalueAπ (whichcanberegardedasaspecialcaseofAssump.B).Then,
inAppx.E.2.2,westudythegeneralsettingandproveThm.4.2asaspecialcaseofPMD.
E.2.1 SpecialCase: PMDwithExactAdvantage-Value
LemmaE.7(ExistenceofSteeringPathbetweenFeasiblePolicies). Considertwofeasiblepolicies
π,πr which are induced by dual variables tθn u and tθr nu , respectively.
1,h hPrHs,nPrNs h hPrHs,nPrNs
If the agents follow Def. E.1 with exact Q value and start with π “ π, as long as U ě
1 max
2H` α2 Tpmax
n,h,sh,an
h|θr hnpan h|s hq´θ hnpan h|s hq´E
a¯n h„π tn ,hp¨|sn
hqrθr hnpa¯n h|s hq´θ hnpran h|s hqs|q,there
existsa(history-independent)steeringstrategyψ :“tψ u withψ :Π` ÑU,s.t.,π “πr.
t tPrTs t T`1
Proof. ForagentnPrNs,givenaπ ,weconsiderthefollowingsteeringrewardfunctions
t
un t,hps h,an hq“ν tn ,hps h,an hq´A hn |, rπ ntps h,an hq´E arn„π tn ,hp¨|shqrν tn ,hps h,ran hq´An h|, rπ ntps h,ran hqs
´ s¯m h,i a¯n
n
htν tn ,hps¯ h,a¯n hq´An h|, rπ ntps¯ h,a¯n hq´E arn„π tn ,hp¨|shqrν tn ,hps¯ h,ran hq´An h|, rπ ntps¯ h,ran hqsu,
whereνn :SˆAn ÑRwillbedefinedlater. Byconstruction,wehave:
t,h
E an h„π tn ,hp¨|shqrun t,hps h,an hqs (2)
“´ s¯m h,i a¯n
n
htν tn ,hps¯ h,a¯n hq´A hn |, rπ ntps¯ h,a¯n hq´E arn„π tn ,hp¨|shqrν tn ,hps¯ h,ran hq´An h|, rπ ntps¯ h,ran hqsu, (3)
whichisaconstantandindependentw.r.t. s ,an. Besides, bydefinition, wecanensurethenon-
h h
negativityofun . Asaresult,
t,h
@tPrTs, Qt,πt ps ,anq
h|rn`un h h
t
“At,πtps ,anq`un ps ,anq`C ps q (Eq.(3))
h|rn h h t,h h h h h
“νn ps ,anq`C1ps q. (4)
t,h h h h h
where we use C ps q and C1ps q to denote some state-dependent but action-independent value.
h h h h
AccordingtoLem.E.6,undertheabovesteeringrewarddesign,thedynamicsofπ ,...,π ,...,π
1 t T
canbedescribedbythefollowingdynamics:
@tPrTs, @nPrNs,hPrHs,s PS : θn p¨|s qÐθn p¨|s q`ανn ps ,anq (5)
h t`1,h h t,h h t,h h h
π tn `1,hp¨|s hqÐargminD ϕnpz,θ tn `1,hp¨|s hqq, (6)
zP∆pAnq
23Nowweconsiderthefollowingchoiceofνn :
t,h
r
θnpan|s q´θnpan|s q
νn ps ,anq“ h h h h h h ,
t,h h h αT
whichimpliesθ “ θr ,andtherefore,π “ πr. Besides,thesteeringrewardfunctioncanbe
T`1 T`1
upperboundedby:
un t,hps h,an hqď2 s¯m h,a a¯x
n
h|ν tn ,hps¯ h,a¯n hq´An h|, rπ ntps¯ h,a¯n hq´E arn„π tn ,hp¨|shqrν tn ,hps¯ h,ran hq´An h|, rπ ntps¯ h,ran hqs|
ď2H ` 2 p max |θrnpan|s q´θnpan|s q|q,
αT n,h,sh,an
h
h h h h h h
whichimpliestheappropriatechoiceofU .
max
˝
TheoremE.8. UnderAssump.D,giventheinitialπ :“ π P Π`, foranyT ě 1andε ą 0, if
1
theagentsfollowDef.4.1withexactQvalue,thenΨε ‰ Hifthefollowingconditionsare
T,Umax
satisfied:
• Thereexistsfeasibleπr PΠ`suchthatηgoalpπrqěmax ηgoalpπq´ε
π
• Denoteθandθr asthedualvariablesassociatedwithπandπr,respectively. WerequireU ě
max
r
2H ` α2 Tpmax n,h,sh,an h|θ hnpan h|s hq´θ hnpan h|s hq|q
Proof. TheproofisadirectlyapplicationofLem.E.7. ˝
NPGasaSpecialCase ForNPG,wehavethefollowingresults.
LemmaE.9. Given@π,πr PΠ`,T ě1,iftheagentsfollowDef.4.1withexactadv-valueandstart
fromπ “π,bychoosingU appropriately,thereexistsa(history-independent)steeringstrategy
1 max
ψ :“tψ u withψ :Π` ÑU,s.t.,π “πr.
t tPrTs t T`1
TheoremE.10. UnderAssump.D,givenanyinitialπ PΠ`,foranyT ě1andεą0,iftheagents
1
followDef.4.1withexactQvalue,bychoosingU appropriately,wehaveΨε ‰H.
max
ProofforLem.E.9andThm.E.10 TheproofisbydirectlyapplyingLem.E.7andThm.E.8since
NPGisaspecialcaseofPMDwithKL-DivergenceasBregmanDivergence. Foranyπ,πr PΠ`,we
r
considerthedualvariablesθ,θsuchthat:
θ hnp¨|s hq“logπ hnp¨|s hq´E an„πnrlogπ hnpan h|s hqs, θr hnp¨|s hq“logπr hnp¨|s hq´E an„πnrlogπr hnpan h|s hqs.
h h h h
(7)
Choice of U in Lem. E.9 By applying Lem. E.7 and Thm. E.8, we consider the following
max
choiceofU
max
2 πrnps ,anq πrnps ,ranq
U max ě2H ` αTp n,hm ,sa hx ,ah|log πh hnpsh h,ah n hq ´E arn h„π hnrlog πh hnpsh h,rah n hqs|q. (8)
ChoiceofU inThm.E.10 Wedenoteπ˚ Pargmax ηgoalpπqRΠ`.
max πPΠ
Whenπ˚ PΠ`,wecandirectlyapplyThm.E.8withπr Ðπ˚,andchoosingU correspondingly
max
followingEq.(8).
However, in some cases, π˚ R Π` because it takes deterministic action in some states. In that
case, sinceηgoal isL-Lipschitzinπ, wecanconsiderthemixturepolicyπr :“ p1´Opεqqπ˚ `
L
Opεqπ , where π is the uniform policy. As a result, we have πr P Π` as well as
L Uniform Uniform
ηgoalpπrqěmax ηgoalpπq´ε. ThentheU canbechosenfollowingEq.(8).
πPΠ max
24E.2.2 TheGeneralIncentiveDrivenAgentsunderAssump.B
TheoremE.11(FormalVersionofThm.4.2forthegeneralPMD). UnderAssump.DandAssump.C,
giventheinitialπ :“ π P Π`,foranyε ą 0,iftheagentsfollowDef.4.1undertheAssump.B,
1
thenΨε ‰Hifthefollowingconditionsaresatisfied:
T,Umax
• Thereexistsfeasibleπr PΠ`suchthatηgoalpπrqěmax ηgoalpπq´ ε
π 2
• Denoteθandθr asthedualvariablesassociatedwithπandπr,respectively. WerequireU ě
max
2pH ` αλ λm
2
mi an xp1` λλ mm ain xqT}θr ´θ} 2qandT “Θpλ λ2 m
2
ma inx logL}θr µ´ εθ}2q.
RemarkE.12. InThm.E.2.2,ourboundforU hereisjustaworst-caseboundtohandlethe
max
r
noisyupdatesintheworstcase. Withhighprobability,thedualvariableθ willconvergetoθandthe
t
steeringrewarddoesnothavetobeaslargeasU .
max
Proof. Givenaπ ,weconsiderthefollowingsteeringrewardu :
t t
un t,hps h,an hq“ν tn ,hps h,an h,π tq´An h|, rπ ntps h,an hq´E arn„π tn ,hp¨|shqrν tn ,hps h,ran h,π tq´An h|, rπ ntps h,ran hqs
´ s¯m h,i a¯n
n
htν tn ,hps¯ h,a¯n h,π tq´An h|, rπ ntps¯ h,a¯n hq´E arn„π tn ,hp¨|shqrν tn ,hps¯ h,ran h,π tq´An h|, rπ ntps¯ h,ran hqsu,
r r
Here we choose νn ps ,an,π q :“ 1 ¨pθnpan|s q´θn pan|s qq, where θ denotes the dual
t,h h h t γ h h h t`1,h h h
variable of policy πr and γ will be determined later. Comparing with the design in the proof of
Thm.E.8,herethe“driventerm”νnneedtodependonπ becauseoftherandomnessinupdates.
h t
Aswecansee,un ps ,anqě0,andforeacht,wehave:
t,h h h
Er}θr ´θ }2s“Er}θr ´θ }2s´2Erxθr ´θ ,θ ´θ ys`Er}θ ´θ }2s
t`1 2 t 2 t t`1 t t`1 t 2
“Er}θr ´θ }2s´2αErxθr ´θ ,Apπt ys`Er}Apπt }2s
t 2 t |r`ut |r`ut 2
ďp1´2λ α `λ2 α2 qEr}θr ´θ }2s,
minγ maxγ2 t 2
whichimplies
Er}θr ´θ }2sďp1´2λ α `λ2 α2 qT}θr ´θ}2.
T`1 2 minγ maxγ2 2
Weconsiderthechoiceγ “ λ2 maxα,whichimplies,
λmin
Er}θr ´θ }2sďp1´
λ2
minqT}θr ´θ}2.
T`1 2 λ2 2
max
WhenT “2c 0λ λ2 m 2 ma inx log2L} µθr ´ εθ}2 ěc 0log 1´ λλ
2
m2 m ain xp 2L2ν }2 θrε ´2 θ}2 2qforsomeconstantc 0,wehave:
Er}θr
´θ } sď
µε
,
T`1 2 2L
whichimplies,
Erηpπ˚q´ηgoalpπ qsď ε `LEr}πr ´π } sď ε ` L Er}θr ´θ } s“ε.
T`1 2 T`1 2 2 µ T`1 2
Next,wediscussthechoiceofU ,byAssump.B,weknow,
max
}θr
´θ }
“}θr
´θ
´αApπt
}
ď}θr
´θ }
`α}Apπt
}
t`1 2 t |r`ut 2 t 2 |r`ut 2
ď}θr ´θ } `αλ }Aπt }
t 2 max |r`ut 2
ďp1` λ min q}θr ´θ }
λ t 2
max
25whereweusethefactthat}Aπτ } “ 1}θr ´θ } andourchoiceofγ. Therefore,foralltPrTs,
}θr ´θ t} 2 ďp1` λλ mm ain xqT}θr ´|r θ` }u 2τ . T2 oenγ sureourτ d2 esignofun t,hisfeasible,weneedtoset:
U “2pH ` 1 p1` λ min qT}θr ´θ} q
max γ λ 2
max
“2pH ` λ min p1` λ min qT}θr ´θ} q.
αλ2 λ 2
max max
˝
ProofforThm.4.2 AswediscussinExample.E.3,Assump.Cissatisfiedwithµ“1. Theproof
isadirectapplicationofThm.E.8withthesamechoiceofdualvariablesasEq.(7).
F MissingProofsforExistencewhentheTrueModelf˚ isUnknown
Inthefollowing,weestablishsometechnicallemmasforthemaximallikelihoodestimator. Givena
steeringdynamicsmodelclassF andthetruedynamicsf˚ „p andasteeringstrategyψ :ΠÑU,
0
weconsiderasteeringtrajectoryτ :“tπ ,u ,...,π ,u ,π ugeneratedby:
T0 1 1 T0 T0 T0`1
@tPrT s, u Ðψpπ q, π „f˚p¨|π ,u q, (9)
0 t t t`1 t t
whereπ isindependentw.r.t. π fort1 ătconditioningonπ . Inthefollowing,wewilldenote
t`1 t1 t
τ :“tπ ,u ,...,π ,u ,π utobethetrajectoryuptostept.
t 1 1 t t t`1
Foranyf PF,wedefine:
źT0
p pτ q:“ fpπ |π ,u q. (10)
f T0 t`1 t t
t“1
Given τ , we use τ¯ to denote the “tangent” trajectory tpπ ,u ,π¯ quT0 where π¯ „
T0 T0 t t t`1 t“1 t`1
f˚p¨|π ,u q is independently sampled from the same distribution as π conditioning on the
t t t`1
sameπ andu .
t t
Lemma F.1. Let l : Π ˆ U ˆ Π Ñ R be a real-valued loss function. Define Lpτ q :“
ř ř T0
T0 lpπ ,u ,π qandLpτ¯ q:“ T0 lpπ ,u ,π¯ q. Then,forarbitrarytPrT s,
t“1 t t t`1 T0 t“1 t t t`1 0
ErexppLpτ q´logE rexppLpτ¯qq|τ sqs“1.
t τ¯T0 t t
Proof. WedenoteEi :“E rexpplpπ ,u ,π¯ qq|π ,u ,f˚s. Bydefinition,wehave:
π¯i`1 i i i`1 i i
ÿt źk
E rexpp lpπ ,u ,π¯ qq|τ s“ Ei.
τ¯t i i i`1 t
i“1 i“1
Therefore,
E rexppLpτ q´logE rexppLpτ¯ qq|τ sqs
τT0 T0 τ¯T0 řT0 T0
expp T0 lpπ ,u ,π qq
“E rE r řt“1 t t t`1 |τ Ytπ ,u uss
τT0´1YtπT0,uT0u πT0`1 E rexpp T0 lpπ ,u ,π qq|τ s T0´1 T0 T0
τ¯T0ř t“1 t t t`1 T0
expp T0 lpπ ,u ,π qq
“E rE r t“ś1 t t t`1 |τ Ytπ ,u uss
τT0´1YtπT0,uT0u πT0`1 T0 Et T0´1 T0 T0
ř t“1
expp T0´1lpπ ,u ,π qq lpπ ,u ,π q
“E r t“ś1 t t t`1 ¨E r T0 T0 T0`1 |τ Ytπ ,u uss
τT0´1YtπT0,uT0u T0´1Et πT0`1 ET0 T0´1 T0 T0
ř t“1
expp T0´1lpπ ,u ,π qq
“E r t“ś1 t t t`1 s“...“1.
τT0´1 T0´1Et
t“1
˝
26Lemma F.2. [Property of the MLE Estimator] Under the condition in Prop. 4.4, given the
true model řf˚ and any deterministic steering strategy ψ : Π Ñ U, define f
MLE
Ð
argmax T0 logfpπ |π ,u q,wherethetrajectoryisgeneratedby:
fPF t“1 t`1 t t
@tPrT s, u Ðψpπ q, π „f˚p¨|π ,u q,
0 t t t`1 t t
then,foranyδ Pp0,1q,w.p. atleast1´δ,wehave:
ÿT0
|F|
H2pf p¨|π ,u q,f˚p¨|π ,u qqďlogp q.
MLE t t t t δ
t“1
Proof. Givenamodelf PF,weconsiderthelossfunction:
#
1log fpπ1|π,uq , iff˚pπ1|π,uq‰0
l pπ,u,π1q:“ 2 f˚pπ1|π,uq
M
0, otherwise
ConsideringtheeventE:
|F|
E :“t´logE rexpL pτ¯ q|τ sď´L pτ q`logp q, @f PFu.
τ¯T0 M T0 T0 M T0 δ
ř ř
where we define L pτ q :“ T0 l pπ ,u ,π q and L pτ¯ q :“ T0 l pπ ,u ,π¯ q.
M T0 t“1 M t t t`1 M T0 t“1 M t t t`1
Besides,byapplyingLem.F.1onl definedaboveandapplyingMarkovinequalityandtheunion
M
boundoverallf PF,wehavePrpEqě1´δ. OntheeventE,wehave:
´logE rexpL pτ¯ q|τ s
τ¯T0 fMLE T0 T0
|F|
ď´L pτ q`logp q
fMLE T0 δ
|F|
ďl pf˚q´l pf q`logp q
MLE MLE MLE δ
|F|
ďlogp q. (f maximizesthelog-likelihood)
δ MLE
Therefore,
d
|F|
ÿT0
fpπ¯ |π ,u q
logp qě´ logE r t`1 t t |π ,u ,f˚s
δ
t“1
τ¯T0
d
f˚pπ¯ t`1|π t,u tq t t
ÿT0
fpπ¯ |π ,u q
ě 1´E r t`1 t t |π ,u ,f˚s (´logxě1´x)
π¯t`1 f˚pπ¯ |π ,u q t t
t“1 t`1 t t
ÿT0
“ H2pfp¨|π ,u q,f˚p¨|π ,u qq.
t t t t
t“1
˝
Example 4.4. [One-Step Difference] If @π P Π, there exists a steering reward u P U, s.t.
π
min H2pfp¨|π,r `u q,f1p¨|π,r `u qq ě ζ, for some universal ζ ą 0, where H is the
f,f1PF π π
Hellingerdistance,thenforanyδ Pp0,1q,F ispδ,Tδq-identifiablewithTδ “Opζ´1logp|F|{δqq.
F F
Proof. Considerthesteeringstrategyψpπq“u . Givenanyf PF,andthetrajectorysampledby
π
ψandf,byLem.F.2,w.p. 1´ δ ,wehave:
|F|
|F|
ÿT0
2logp qě H2pfp¨|π ,u q,f p¨|π ,u qqěT ζ.
δ t t MLE t t 0
t“1
Byunionbound,ifT “r4log|F|s`1,withprobabilityatleast1´δ,
0 ζ δ « ff
ÿTδ
maxE rIrf “f ss“maxE Irf “argmax logf1pπ |π ,u qs ě1´δ.
f,ψ MLE f,ψ t`1 t t
fPF fPF f1PF t“1
˝
27r
Theorem4.5. [ASufficientConditionforExistence]Givenanyεą0,ΨεpF;π q8 ‰H,ifDT ăT,
T 1
s.t.,(1)F isp 2ηmε ax,Tr q-identifiable,(2)Ψε T{ ´2 TrpF;π Trq‰Hforanypossibleπ Tr generatedatstep
r
T duringthesteering.
Proof. Wedenoteψ :“tψ u tobetheexplorationstrategytoidentifyf˚. Givena
Explore Explore,t tPrTs
π Tr,wedenoteψ πε{ TĂ2 :“tψ πε{ TĂ2 ,tu
tPrTs
PΨε T{ ´2 Trpπ Trqtobeoneofthesteeringstrategywithε-optimal
gapstartingfromπr.
T
r
We consider the history-dependent steering strategy ψ :“ tψ u , such that for t ď T, ψ “
t tPrTs t
ψ ,andforalltąTr ,wehaveψ “ψε{2 .
Explore,t t π TĂ,t
Asaresult,foranyf PF,thefinalgapwouldbe:
ε
∆ pfq“Prpf “fq¨ `Prpf ‰fq¨η ďε,
ψ,T MLE 2 MLE max
whichimpliesψ PΨεpF;π q. ˝
T 1
G GeneralizationtoPartialObservationMDPSetup
G.1 POMDPBasics
PartialObservationMarkovDecisionProcess A(finite-horizon)Partial-ObservationMarkov
DecisionProcess(withhiddenstates)canbespecifiedbyatupleM :“ tν ,T,X,U,O,T,η,Ou.
1
Hereν istheinitialstatedistribution,Listhemaximalhorizonlength,X isthehiddenstatespace,
1
U istheactionspace,Oistheobservationspace. Besides,T:X ˆU ÑX denotesthestationary
transitionfunction,O : X Ñ ∆pOqdenotesthestationaryemissionmodel,i.e. theprobabilityof
someobservationconditioningonsomestate. WewilldenoteH :“ O ˆU ...ˆO tobethe
h 1 1 h
historyspace,anduseτ :“ to ,u ,...,o utohistoryobservationuptosteph. Weconsiderthe
h 1 1 h
historydependentpolicyψ :“tψ ,...,ψ uwithψ :H Ñ∆pUq. Startingfromtheinitialstate
1 H h h
x ,thetrajectoryinducedbyapolicyψisgeneratedby:
1
@hPrHs, o „Op¨|x q, u „ψ p¨|τ q, η „η po ,u q, x „Tp¨|x ,u q.
h h h h h h h h h h`1 h h
G.2 SteeringProcessasaPOMDP
GivenagameG,weconsiderthefollowingMarkovianagentdynamics:
@tPrTs, τ „π , π „fp¨|π ,τ ,rq,
t t t`1 t t
whereτ :“tst,k,at,k,,...,st,k,at,kuK isseveraltrajectoriesgeneratedbythepolicyπ .
t 1 1 H H k“1 t
Ineachstept,weassumetheagentsfirstcollecttrajectoriesτ withpolicyπ ,andthenoptimizetheir
t t
policiesfollowingsomeupdaterulefp¨|π ,τ ,rq. ComparingwiththeMarkoviansetupinSec.3,
t t
heref hasadditionaldependenceonthetrajectoriesτ .
t
Basedonthisnewformulation,thedynamicsgiventhesteeringstrategyisdefinedby:
@tPrTs, τ „π , u „ψ p¨|τ ,u ,...,τ ,u ,π q, π „fp¨|π ,r`u q,
t t t t 1 1 t´1 t´1 t t`1 t t
InFig.4,weillustratethesteeringdynamicsbyProbabilisticGraphicalModel(PGM).Herewetreat
thejointofπ andτ asthehiddenstateatstept,andthetrajectoryτ isthepartialobservationo
t t t t
receivedbythemediator. Next,weintroducethenotionofdecodablePOMDP,wherethehidden
stateisdeterminedbyashorthistory.
Definition G.1 (m-Decodable POMDP). Given a POMDP M, we say it is m-decodale, if there
existsadecoderϕ,suchthat,x “ϕpo ,u ,...o ,u ,o q,
h h´m h´m h´1 h´1 h
Inoursteeringsetting,ifforanyf PF,f ism-decodable,wejustneedtolearnasteeringstrategy
ψ :“pOˆUqmˆO ÑU,whichpredictsthesteeringrewardgiventhepastm-stephistory. This
8Herewehighlightthedependenceoninitialpolicy,model,andtimeforclarity(seeFootnote4)
28Steering Reward Steering Reward
𝑢"∼𝜓"(⋅|𝑜") 𝑢!∼𝜓!(⋅|𝑜",𝑢",𝑜!)
𝑢 𝑢
! "
𝑜 𝑜
! "
Observation Emission Observation Emission
𝑜"∼𝕆(⋅|𝑥") 𝑜 !∼𝕆(⋅|𝑥 ")
…
𝑥 𝑥 𝑥
! # #
𝝅 𝜏 State Transition 𝝅 𝜏 State Transition 𝝅 𝜏
! ! " " # #
𝜋!∼𝑓(⋅|𝜋",𝜏",𝑟+𝑢") 𝜋#∼𝑓(⋅|𝜋#,𝜏!,𝑟+𝑢!)
𝜏!∼𝐺(⋅|𝜋!) 𝜏#∼𝐺(⋅|𝜋#)
Figure 4: Probabilistic Graphic Model (PGM) of the POMDP formulation of the steering
process. Startingwiththeinitialstatex :“pπ ,τ q,foralltě1,themediatorreceivesobservation
1 1 1
o „ Op¨|x qandoutputthesteeringrewardgiventhehistoryu „ ψp¨|o ,u ,...,o q. Theagents
t t t 1 1 t
thenupdatetheirpoliciesfollowingthedynamicsf andthemodifiedrewardfunctionr`u .
t
is the motivation for our experiment setup in the Grid World Stag Hunt game in Sec. 6.1. More
concretely,weassumetheagentstrajectoriesinthepastfewstepscanbeusedassufficientstatistics
for the current policy, and use them as input of the steering strategy (see Appx. H.2.2 for more
details).
H MissingExperimentDetails
H.1 AboutInitializationinEvaluation
Insomeexperiments,wewillevaluateoursteeringstrategieswithmultipledifferentinitialpolicyπ ,
1
inordertomakesureourevaluationresultsarerepresentative.
Hereweexplainhowwechoosetheinitialpoliciesπ . Wewillfocusongameswithtwoactions
1
which is the only case we use this kind of initialization. For each player, given an integer i, we
construct an increasing sequence with common difference Seq :“ p1, 3,...,2i´1q. Then, we
i 2i 2i 2i
considertheinitialpoliciesπ suchthatπ1pa1q“1´π1pa2qPSeq ,π2pa1q“1´π2pa2qPSeq .
1 i i
Inthisway,weobtainasetofinitialpoliciesuniformlydistributedingridswithcommondifference
1. Asaconcreteexample,theinitialpointsinFig.1-(b)markedincolorblackisgeneratedbythe
i
aboveprocedurewithi“10.
H.2 ExperimentsforKnownModelSetting
H.2.1 ExperimentDetailsinNormal-FormStagHuntGame
WeprovidethemissingexperimentdetailsforthesteeringexperimentsinFig.1-(b).
Choiceofηgoal Wřeconsiderthetotalutilityasthegoalfunction. Butforthenumericalstability,we
chooseηgoalpπq“ Jnpπq´10whereweshifttherewardviathemaximalutilityvalue10.
nPrNs |r
TheSteeringStrategy Thesteeringstrategyisa2-layerMLPwith256hiddenlayersandtanhas
theactivationfunction. Givenatimesteptandthepolicyπ :“tπ1,π2uwithπnpHq`πnpGq“1
t t t t t
fornPt1,2u,theinputofthesteeringstrategyis
d d d d
π1pHq π1pHq π2pHq π2pHq T ´t
plog t ,´log t ,log t ,´log t , q. (11)
π1pGq π1pGq π2pGq π2pGq 100
t t t t
Herethefirst(second)twocomponentscorrespondtothe“dualvariable”ofthepolicyπ1pHqand
t
π1pGq (π2pHq and π2pGq), respectively; the last component is the time embedding because our
t t t
steeringstrategyistime-dependent.
2910
4
3
2 5
1
0 0
=10 =25 =100 =10 =25 =100
Figure 5: Trade-off between Steering Gap (Left) and Steering Cost (Right). (averaged over 5x5
uniformlydistributedgridsasinitializationsofπ ,seeAppx.H.1).
1
Thesteeringstrategywilloutputavectorwithdimension4,whichcorrespondstothesteeringrewards
fortwoactionsoftwoplayers. Notethatherethesteeringrewardfunctionu1 : S ˆA1r0,U s
max
(foragent1)andu2 :“ S ˆA2 Ñ r0,U s(agent2)isdefinedonthejointofstatespaceand
max
individualactionspace. Thiscanberegardedasaspecializationofthesetupinourmaintext,where
weconsiderun :SˆAÑr0,U s@nPrNs,whichisdefinedonthejointofstatespaceandthe
max
entireactionspace.
TrainingDetails ThemaximalsteeringrewardU issettobe10,andwechooseβ “25. We
max
usethePPOimplementationofStableBaseline3(Raffinetal.,2021). Thetraininghyper-parameters
canbefoundinourcodesinoursupplementalmaterials.
During the training, the initial policy is randomly selected from the feasible policy set, in order
toensurethegoodperformanceingeneralizingtounseeninitializationpoints. Anotherempirical
trick we adopt in our experiments is that, we strengthen the learning signal of the goal function
byincludingηgoalpπ qforeachstept P rTs. Inanotherword,weactuallyoptimizethefollowing
t
objectivefunction:
1
ÿ ” ÿT ı
ψ˚ Ðargmax E β¨ηgoalpπ q` β¨ηgoalpπ q´ηcostpπ ,u q . (12)
ψPΨ |F| ψ,f T`1 t t t
fPF t“1
The main reason is that here T “ 500 is very large, and if we only have the goal reward at the
terminalstep,thelearningsignalisextremelysparseandthelearningcouldfail.
OtherExperimentResults InFig.5,weinvestigatethetrade-offbetweensteeringgapandthe
steering cost when choosing different coefficients β. In general, the larger β can result in lower
steeringgapandhighersteeringcost.
H.2.2 ExperimentDetailsinGrid-WorldVersionofStagHuntGame
WerecalltheillustrationinLHSofFig.2. Weconsidera3x3gridworldenvironmentwithtwoagents
(blueandred). Atthebottom-leftandup-rightblocks,wehave‘stag’and‘hares’,respectively,whose
positionsarefixedduringthegame. Atthebeginningofeachepisode,agentsstartfromtheup-left
andbottom-rightblocks,respectively.
ForeachtimestephPrHs,everyagentcantakefouractions{up,down,left,right}tomove
totheblocksnexttotheircurrentblocks. Butiftheagenthitsthewallaftertakingtheaction(e.g. the
agentlocatesatthemostrightcolumnandtakestheactionright),itwillnotmove. Aslongasone
agentreachestheblockwitheitherstagorhare,theagentswillreceiverewardsandberesettothe
initialposition(up-leftandbottom-rightblocks). Therewardisdefinedbythefollowing.
• Ifbothagentsreachtheblockwithstagatthesametime,eachofthemreceivereward0.25.
• Ifbothagentsreachtheblockwithharesatthesametime,eachofthemreceivereward0.1.
• Ifoneagentreachestheblockwithhares,itwillgetreward0.2andtheothergetreward0.
• Inothercases,theagentsreceivereward0.
WechooseH “ 16. Thebeststrategyisthatalltheagentsmovetogethertowardstheblockwith
Stag,sowithinoneepisode,theagentscanreachtheStag16/2=8times,andthemaximaltotal
returnwouldbe8*0.25=4.0.
30
)3-e1()*f(T,
)*f(T,CInthefollowing,weintroducethetrainingdetails. Ourgrid-worldenvironmentandthePPOtraining
algorithmisbuiltbasedontheopensourcecodefrom(Luetal.,2022).
Agents Learning Dynamics The agents will receive a 3x3x4 image encoding the position of
allobjectstomakethedecision. TheagentsadoptaCNN,andutilizePPOtooptimizetheCNN
parameterswithlearningrate0.005.
SteeringSetupandDetailsinTrainingSteeringStrategy OursteeringstrategyisanotherCNN,
whichtakestheagentsrecenttrajectoriesasinput. Moreconcretely,foreachsteeringiterationt,we
asktheagentstointeractandgenerated256episodeswithlengthH,andconcatenatethemtogether
to a tensor with shape [256 * H, 3, 3, 4]. The mediator takes that tensor as input and output an
8-dimensionsteeringrewardvector. Herethesteeringrewardscorrespondstotheadditionalrewards
giventotheagentswhenoneofthemreachtheblockswithstagorhares(wedonotprovideindividual
incentivesforstatesandactionsbeforereachingthoseblocks). Tobemoreconcrete,the8rewards
correspondtotheadditionalrewardforblueandredagentsforthefollowing4scenarios: (1)both
agentsreachstagtogether(2)bothagentsreachharestogether(3)thisagentreachstagwhilethe
otherdoesnotreachstag(4)thisagentreachhareswhiletheotherdoesnotreachhares.
ThesteeringstrategyisalsotrainedbyPPO.Wechooseβ “25andlearningrate0.001. Weconsider
thetotalutilityasthegoalfunction, andweadoptthesimilarempiricaltrickasthenormal-form
version,whereweincludeηgoalintotherewardfunctionforeverytPrTs(Eq.(12)). Theresultsin
Fig.2istheaverageof5steeringstrategiestrainedbydifferentseedsfor80iterations. Thetwo-sigma
errorbarisshown.
H.2.3 ExperimentsinMatchingPennies
MatchingPenniesisatwo-playerzero-sumgamewithtwoactionsH=HeadandT=Tailanditspayoff
matrixispresentedinTable3.
Table3: PayoffMatrixofTwo-PlayerGameMatchingPennies. TwoactionsHandTstandforHead
andTail,respectively.
H T
H (1,-1) (-1,1)
T (-1,1) (1,-1)
Choice of ηgoal In this game, the unique Nash Equilibrium is the uniform policy πNE with
πn,NEpHq “ πn,NEpTq “ 1 for all n P t1,2u. We consider the distance with πNE as the goal
2
function,i.e. ηgoal “´}π´πNE} .
2
ExperimentSetups WefollowthesamesteeringstrategyandtrainingsetupsforStagHuntGame
inAppx.H.2.1. TheagentsfollowNPGtoupdatethepolicieswithlearningrateα“10.
ExperimentResults AsshowninFig.6-(a),wecanobservethecyclingbehaviorwithoutsteering
guidance (Akin and Losert, 1984; Mertikopoulos et al., 2018). In contrast, our learned steering
strategycansuccessfullyguidetheagentstowardsthedesiredNash. InFig.6-(b),wealsoreportthe
trade-offbetweensteeringgapandsteeringcostwithdifferentchoiceofβ.
H.3 ExperimentsforUnknownModelSetting
H.3.1 DetailsforExperimentswithSmallModelSetF
TheresultsinTable2isaveragedover5seedsandtheerrorbarsshow95%confidenceintervals.
TrainingDetailsforψ˚ andψ˚ Thetrainingofψ˚ andψ˚ followthesimilarexperiment
0.7 1.0 0.7 1.0
setupasAppx.H.2.2,exceptheretheagentsadoptrandomlearningrates. Forthechoiceofβ,we
traintheoptimalsteeringstrategywithβ Pt10,20,30,40,50,60,70,80,90,100uforbothf and
0.7
f ,andchoosetheminimalβ suchthattheresultingsteeringstrategycanachievealmost100%
1.0
31No Steering With Steering
1.0 1.0
10
0.5 0.5 5 0.5
0.1
0 0
0.0 0.5 1.0 0.0 0.5 1.0 =10 =25 =100 =10 =25 =100
1(Head) 1(Head)
(a)DynamicsofAgentsPolicies. (b)Trade-offbetweenAccuracyandCostbyβ.
Figure 6: Experiments in MatchingPennies. (a) x and y axes correspond to the probability that
agentstakeHead. Blackdotsmarktheinitialpolicies, andredcurvesrepresentsthetrajectories
ofagentspolicies. Thesteeringstrategytoplotthefigureistrainedwithβ “25. (b)Wecompare
β “10,25,100. Errorbarshows95%confidenceintervals. (averagedover5x5uniformlydistributed
gridsasinitializationsofπ ,seeAppx.H.1)
1
accuracy(i.e. ∆ ďεforalmostall5x5uniformlydistributedinitialpoliciesgeneratedbyprocess
ψ,f
inAppx.H.1). Aswereportedinthemaintext,weobtainβ “70forf andβ “20forf .
0.7 1.0
Training Details for ψ˚ For the training of ψ˚ , the input of the steering strategy is the
Belief Belief
originalstate(Eq.(11))appendedbythebeliefstateofthemodel. IneachsteeringsteptPrTs,we
assumethemediatorcanobservealearningratesampleα,anduseittoupdatethemodelbeliefstate
correspondingly. Theregularizationcoefficientβ forthetrainingofψ issettobetheexpected
Belief
regularizationcoefficientoverthebeliefstateβ “bpf q¨70`bpf q¨20. Inanotherword,we
0.7 1.0
usethesumofthecoefficientoftwomodelsweightedbythebeliefstate. Thisisreasonablebythe
definitionoftherewardfunctioninthebeliefstateMDPliftedfromtheoriginalPOMDP.ψ˚ is
Belief
trainedthePPOalgorithm.
Duringthetrainingofψ˚ ,wefindthatthetrainisnotverystable,possiblybecausethechosen
Belief
β fortwomodelsarequitedifferent. Therefore,wekeeptrackingthesteeringgapofthesteering
strategyduringthetrainingandsavethemodelaslongasitoutperformsthepreviousonesinsteering
gap. Ourfinalevaluationisbasedonthatmodel.
H.3.2 DetailsforExperimentswithLargeModelSetF
We first highlight here that exploring and identifying λn is necessary. Because the mediator can
usethesameincentivetodrivethose“opportunistic”agentswithsmallλnforalargerupdatestep
towardsthedesiredpolicy,comparingwiththose“steady”oneswithlargerλn.
WesetU “ 2.0,andtherandomexplorationstrategy(redcurveintheleftsub-plotinFig.3)
max
willsamplethesteeringrewarduniformlyfromtheintervalr0,U s. WeusethePPO(Raffinetal.,
max
2021)totrainofexplorationpolicyandalsothesteeringstrategygivenhiddenmodel.
For the training of exploration policy, although the learning signal Irf “ f s in Proc. 2
MLE
is supported by theory, it contains much less information than the posterior probability
rPrpf|π ,u ,...,π ,u ,π qs . Therefore,empirically,weinsteadtrainahistory-independent
1 1 T T T`1 fPF
steeringstrategytomaximizetheposteriorprobabilityoff:
ÿ ÿ
1
ψExplore Ðargm ψax
|F|
E ψ,fr Prpλn|π 1,u 1,...,π Tr,u Trπ Tr `1qs.
fPF nPrNs
HereweusethesumofposteriorsofλnssincetheλnsareindependentforallnPrNs. Weobserveit
resultsinbetterperformance,anditisdoablebykeeptrackingthemodelbeliefstateofeachagent.Be-
sides,similartoStagHuntgames,weobservethatusingtheposteriorPrpλn|π ,u ,...,π ,u ,π q
1 1 t t t`1
as rewards inthe non-terminal steps t ă T increase the performance, and we use the same trick
(Eq.(12)).
Toplottheresultsinthemiddleandrightsub-plotsinFig.3,foreachmodelf˚ Ptf ,f ,f ,f u,
A B C D
wetraintwosteeringstrategies(withthesamestatedesigninEq.(11)). Thefirstoneistheoracle
strategy,whichstartswithπ andsteeringforT “ 500steps. Thesecondoneistheexploitation
1
policy paired with the exploration policy. Because we do not know what πr is after executing
T
32
)daeH(2 )daeH(2
)3-e1()*f(T,
)*f(T,Cr
theexplorationpolicyforT steps. Duringthetrainingoftheexploitationpolicy,weuserandom
initialization,andsetT “470,whichsubtracttheexplorationsteps.
During the evaluation, for FETE, we steer with the exploration policy for the first 30 steps, and
executetheexploitationpolicyfortherest. Theresultsareaveragedover5seedsandtwo-sigmaerror
barisshown.
H.4 ASummaryoftheComputeResourcesbyExperimentsinthisPaper
Experiments on Two-Player Normal-Form Games For the experiments in ‘Stag Hunt’ and
‘Matching Pennies’ (illustrated in Fig. 1, 5, 6), we only use CPUs (AMD EPYC 7742 64-Core
Processor). Ittakeslessthan5hourstofinishthetraining.
ExperimentsonGrid-WorldVersionof‘StagHunt’ Fortheexperimentsingrid-world‘Stag
Hunt’(illustratedinFig.2),weuseoneRTX3090andlessthan5CPUs(AMDEPYC774264-Core
Processor). Thetraining(perseed)takesaround48hours.
ExperimentsonN-PlayerNormal-FormCooperativeGames Fortheexperimentsincooperative
games(illustratedinFig.3),weonlyuseCPUs(AMDEPYC774264-CoreProcessor). Ittakesless
than10hourstofinishthetraining.
33