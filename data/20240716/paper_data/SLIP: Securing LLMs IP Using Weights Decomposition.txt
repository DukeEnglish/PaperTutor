SLIP: Securing LLM’s IP Using Weights Decomposition
Yehonathan Refael∗1, Adam Hakim1, Lev Greenberg1, Tal Aviv1, Satya Lokam1, Ben
Fishman1, and Shachar Seidman2
1Microsoft
July 16, 2024
Abstract
Large language models (LLMs) have recently seen widespread adoption, in both academia
and industry. As these models grow, they become valuable intellectual property (IP), reflecting
enormous investments by their owners. Moreover, the high cost of cloud-based deployment has
driveninteresttowardsdeploymenttoedgedevices,yetthisrisksexposingvaluableparametersto
theft and unauthorized use. Current methods to protect models’ IP on the edge have limitations
intermsofpracticality,lossinaccuracy,orsuitabilitytorequirements. Inthispaper,weintroduce
a novel hybrid inference algorithm, named SLIP, designed to protect edge-deployed models from
theft. SLIP is the first hybrid protocol that is both practical for real-world applications and
provably secure, while having zero accuracy degradation and minimal impact on latency. It
involves partitioning the model between two computing resources, one secure but expensive, and
another cost-effective but vulnerable. This is achieved through matrix decomposition, ensuring
that the secure resource retains a maximally sensitive portion of the model’s IP while performing
a minimal amount of computations, and vice versa for the vulnerable resource. Importantly, the
protocol includes security guarantees that prevent attackers from exploiting the partition to infer
the secured information. Finally, we present experimental results that show the robustness and
effectiveness of our method, positioning it as a compelling solution for protecting LLMs.
1 Introduction
Large Language Models (LLMs) are increasingly being commercialized due to their superior per-
formance and vast applications [83]. Alongside this trend, many researchers have been tackling
challenges related to their deployment – balancing costs, latency, performance and security. Compa-
niesdevelopingLLMsinvestsubstantialcapitaltocreatethesemodels, mostlyinextensivecomputing
resources for training, which could reach hundreds of millions of dollars [54], but also on high-end
talent, datasets and labeling, proprietary training procedures and unique network architecture.
Consequently, the parameters of a pre-trained foundational model can be considered highly valuable
intellectual property (IP) for its owner, who would therefore be highly motivated to secure it from
theft. Hosting the models on cloud services may offer considerable security against various theft
∗Author worked on this research during his internship at Microsoft.
0Email addresses: Yehonathan Refael (t-yrefael@microsoft.com), Adam Hakim (adamhakim@microsoft.com),
Lev Greenberg (levgreenberg@microsoft.com), Tal Aviv (talaviv@microsoft.com), Shachar Sei-
dman (v-sseidman@microsoft.com), Ben Fishman (ben.fishman@microsoft.com), Satya Lokam
(satya.lokam@microsoft.com).
1
4202
luJ
51
]RC.sc[
1v68801.7042:viXraand extraction attacks, but the costs associated with these services can become exorbitant with the
exponential growth in demand [50]. As a result, there is a growing financial incentive to offload
some, or even most, of the computational work from secure computing resources, like the cloud, to
more cost-effective, but less secure edge devices, such as servers, laptops, and smartphones. However,
model owners seeking to utilize this affordable and accessible computing resource need a security
solution that ensures their valuable intellectual property remains safe.
Recently, hybrid inference has been emerging as a framework that aims to address various
challenges related to deployment of LLM’s [12]. In this framework, tokens are generated by two
computing resources that differ in certain properties, with most strategies focusing on cost and
performance [17, 1, 35], or on privacy [28, 10, 59], rather than on model security. Few have aimed
to protect model IP; For example, in [63] they propose securing final layers of DNNs, but their
approach offers only a single cutoff point in the model, supports feed-forward DNNs only, degrades
performance and requires retraining which cannot guarantee information leakage. [85] uses RL for
model splitting but, but their obfuscated parameters are inefficiently spread across the model, and
they lack security guarantees against recovery through observing inferences. [41] employs weight
permutation and an authorization module on a trusted execution environment (TEE), but is not
provably secure, vulnerable to hardware attacks and impractical for consumer devices due to the
cost of TEEs.
In this paper, we propose a novel approach based on hybrid inference, which we name SLIP 1,
that addresses this problem and enables utilization of a cheap and low-security computing resource
in tandem with an expensive resource that has higher security guarantees. our hybrid inference
approach provides a versatile and economical solution for leveraging the full capabilities of LLMs
while accommodating various budget constraints, latency and security standards, with no loss in
model accuracy. We achieve this by offloading the majority of computations to the cost-effective but
less secure computing resource, while protecting the most valuable yet computationally undemanding
information on the secure and expensive resource. In particular, we make the following new
contributions.
• We propose decomposing strategically selected weight matrices across the DNN, to
pinpoint a minimal set of maximally information-dense components (as observed in [47, 73, 65])
that embody the model’s IP, which must be safeguarded on the secure resource. Consequently, the
remainder of the model performs most of the computations, but it becomes devoid of value and can
therefore be exposed to a compromised computing resource. Since both computing resources must
participate in each model inference, it incurs costs in latency and introduces new potential attacks
that could exploit the communication between them.
• Thus, our paper introduces a protocol to mitigate security risks created by this inference
hybridization. An Attacker could try to extract or approximate the valuable model information
stored on the secure resource, through algebraic, denoising, or query-based attacks that exploit
its clean outputs. Thus, Our protocol does not reveals clean outputs to the low-security resource,
by masking the secure resource’s output using pre-computed noise, that can be cheaply removed
later without degradation. The output is manipulated to ensure it is uniformly distributed and
independent of the output, and thereby perfectly secured.
• Lastly, we provide experimental evidence for the robustness of our approach. We show
that popular LLMs can be partitioned to create an exposed portion with most of the computations
but insignificant utility, even when regions, where decomposition occurred, are bypassed. Moreover,
we demonstrate that when fine-tuning the exposed portion of the model, in an attempt to restore its
original performance, the recoverability is limited and can be traded off with reduced offload.
2Figure 1: SLIP Framework: The figure illustrates the decomposition of the original Large Language
Modelbetweentwoentities: thetrusted/securedparty,positionedontheright,andtheuntrusted/non-
secured edge device, on the left. During inference, the intermediary layer outputs are exchanged
between these parties. The model weights stored with the secured party which embodies the model
the intellectual property is safeguarded by the SLIP method, preventing theft or deduce of those
weights, by tring to leverage the parties data exchanged.
1.1 Background and related works
Threat Model. In our setting, we assume an attacker is motivated to obtain the complete model
parameters, i.e., its IP, for any purpose [52], while knowing the model’s architecture. To achieve the
highest security guarantee in our protocol, we assume the attacker has indefinite physical access and
full admin privileges in the low-security compute resource. This means this resource is adversarial,
as if an attacker has take possession of it, bypassed any existing security measures, gained direct
control over any exposed model parameters and can freely query the secure resource. Conversely,
we assume the secure resource is a completely trusted entity, inaccessible to an attacker except
through the query API. Additionally, we assume the attacker has full knowledge of our protocol
and any associated hyper-parameters [55]. Thus, our protocol focuses on protecting the portion of
the model assigned to the secure resource against attacks aiming to infer it, and on ensuring that
the exposed model portion is devoid of any value, even after attempts to restore it through model
recovery techniques [14, 45].
Model Extraction and Stealing. Model extraction attacks are a type of security threat where
attackers aim to create a duplicate or approximation of a machine learning model by using its public
API ([9, 64, 71, 37] see [79, 33] for review). Attackers manipulate the model’s input-output pairs to
learn about its structure and functionality, allowing them to create an approximation of the model.
This can enable them to use the model for their own purposes, sidestep any usage restrictions or fees,
or even uncover sensitive information about the original training data. Hence, this type of attacks
could compromise compute resources at any security level, whether the inference system is hybrid or
not, as it exploits the intended use of the service. In contrast, model stealing entails unauthorized
access to and replication of trained models, by circumventing the security measures [52]. Therefore,
this type of attack is more likely to occur on a low-security resource involved in an hybrid inference
protocol, making it the focus of our paper.
Protecting DNNs IP from Theft. Several existing methods strive to secure neural network
parameters for computation on low-security resources. One approach is termed Model Obfuscation,
the process of intentionally obscuring or hiding the critical information of a neural network while
retainingitsfunctionality. Theseincludeusingknowledgedistillation[75]orneuralarchitecturesearch
[86], intentional model poisoning [22], increasing model sensitivity to parameters perturbation [68],
locating model tampering for user authorization [30] and many more [84]. However, many of these
approaches degrade model accuracy, or cannot provide provable security guarantees. Additionally,
3there are passive protection techniques, such as watermarking [72, 5, 77, 78, 81] (see [42] for survey)
or Attestation [11], which help verify ownership and copyrights, but they cannot effectively prevent
unauthorized access and usage to demotivate attackers. Cryptographically secure obfuscation offers
provable guarantees but remains impractical for complex functions and unsuitable for scaling to even
small ML models [80]. Lastly, cryptographic techniques have been extensively applied in the highly
active area of Privacy Preserving Machine Learning (PPML). Such techniques include Homomorphic
Encryption (e.g., [4], [53, 40]), Secure Multiparty Computation (e.g., [38, 16, 15, 44, 24]), and
Differential Privacy (e.g., [2]). However, in PPML, the emphasis is on user data privacy or training
data rather than protecting model weights, and existing cryptographic frameworks are not designed
for this purpose, or do not account for the computational trade-offs between asymmetric parties
with varying costs and security levels [61, 34, 31, 59].
2 Framework and settings
2.1 Model decomposition and properties
Here we describe a general setting for the model decomposition we suggest for offloading most of
the inference computation to an untrusted entity while simultaneously protecting the model during
inference. To that end, for simplicity, we denote the trusted entity by Charlie and the untrusted
entity by David. For example, C could be a trusted cloud or a trusted server, whereas D could be an
untrusted edge device or an untrusted server, or C could be a trusted NPU while D could be an
untrusted NPU on the same server.
Definition 1 (Neural network decomposition). Consider a model ϕ parameterized by Θ, and some
Θ
two sets of tensors weights Θ , and Θ . If Θ = Θ ⊕Θ , where ⊕ stands for the sums of tensors
1 2 1 2
weights then, (ϕ ,ϕ ) is a called a neural network decomposition of ϕ .
Θ1 Θ2 Θ
EachoneofthepartiesCharlieandDavidisgivenadifferentsetoftensorsΘ andΘ ,respectively.
C D
Thus, for simplicity, we denote the decomposition of the given model ϕ , by (ϕ ,ϕ ). We quantify
Θ ΘC ΘD
the utility of a given decomposition by the following properties definitions.
Definition 2 (Neural network decomposition properties). Consider a decomposition (ϕ ,ϕ ) of
ΘC ΘD
a given model ϕ parameterized by Θ. Let P(x,y) be a data distribution from which the training
Θ
data are sampled. Accordingly, the empirical risk of ϕ over P is given by E [R(ϕ (x,y))],
(x,y)∼P Θ
where R is the model loss (e.g., accuracy).
• Usefulness. Let ϕ , be a random initialized model, parameterized by the random set of
Θrand
tensors Θrand. A decomposition (ϕ ,ϕ ) is said to be κ-effective, where κ is the true-risk
ΘC ΘD
ratio between the models ϕ , and ϕ , namely
0⊕ΘD Θrand
E [R(ϕ (x,y))]
κ = E
(x,y)∼P 0⊕ΘD
,
Θrand
E [R(ϕ (x,y))]
(x,y)∼P Θrand
where 0 is the zeros set of tensors corresponding to Θ dimensions.
C
• Confidentiality. A decomposition (ϕ ,ϕ ) of ϕ is said to be confidential, if, for every
ΘC ΘD Θ
adversary D , given Θ and black box access to Θ, recovers ϵ -approximation of the true risk
1 D 1
of ϕ , namely ϵ ·E [R(ϕ (x,y))], (w.r.t., e.g., accuracy on a test distribution), in time
Θ 1 (x,y)∼P Θ
T, there is adversary D that can construct ϵ -approximation of the true risk of ϕ using only
2 2 Θ
black box queries to Θ (i.e., without using Θ ) for some ϵ ≈ ϵ , in time T. See Figure 2 for
D 2 1
an illustration.
4• Information dense. A decomposition (ϕ ,ϕ ) is considered as η-informational densed,
ΘC ΘD
where η =
|ΘC|
.
|Θ|
Figure 2: Confidentiality of a Model Decomposition
The Usefulness property of a decomposition implies how the exposed decomposed part of model
Θ exhibits poor performance (e.g., random-guess accuracy). Therefore in a complementary way, it
D
implies how well Θ quantifies the model’s secret IP. The more useful Θ is, the more secret IP is
C C
embodied in Θ . Accordingly, the efficiency property points out that a lower fraction of parameters
C
|Θ | in C implies a reduced memory space requirement and fewer operations during inference in C.
C
2.1.1 Model decomposition via SVD
In this section, we begin by describing our decomposition method. for fully connected multilayer
perceptrons (MLPs). Further, in Remark 1, we describe the generalization of this decomposition to
more complex models (i.e. attention head). To decompose an MLP, we strategically identify specific
critical linear layers within the network. The criteria for selecting these layers are documented in
Appendix B.2. The decomposition process for these identified layers is outlined subsequently.
Let W
i
∈ Rdi+1×di be the weights matrix of i-th layer, deemed critical, and where d i,d
i+1
∈ N
are the widths of the i-th and (i+1)-st layers, respectively. To avoid cluttering notation, we will
omit the subscript in what follows and denote m := d and n := d . Singular Value Decomposition
i+1 i
(SVD) of W is given by W = UΣV⊤ = (cid:80)r σ u v⊤, where, rank(W) =: r ≤ min{m,n},
j=1 j j j
Σ = diag(σ ,...,σ ) is a rectangular diagonal matrix with σ the j-th singular value of W, and
1 r j
{u ∈ Rm} and {v ∈ Rn} are left and right (resply.) singular vectors of W.
j j∈[m] j j∈[n]
We then define, WC – the sensitive part to be held by C – to be the part of W defined by its top
k singular components, where 1 < k ≪ rank(W) is a parameter we call the sensitivity rank (that
depends on the model and the layer). The remaining part WD of W is offloaded to the untrusted
entity D . More precisely,
k n
(cid:88) (cid:88)
WC = σ u v⊤ and WD = σ u v⊤. (1)
j j j j j j
j=1 j=k+1
In the notation of the previous subsection, the decomposition (MLP ,MLP ) for an MLP
ΘC ΘD Θ
is then defined as follows. For each critical layer of Θ, apply (1) to its weight matrix W and include
the matrix WC and the next nonlinear layer (such as Relu) in Θ and include WD in Θ . For each
C D
non-critical layer of include the entire weight matrix and the next nonlinear layer in Θ (without
D
adding anything to D.
The following insights imply that the suggested technique for model decomposition is optimal,
namely, simultaneously holding both the effectiveness and the efficiency properties.
5GPT-2 Phi-2 Llama-2-7B
0.06 0.09 c_fc
0.05 0 0. .0 08 7 0.02 c W_ qproj
0.04 0 0. .0 06 5 0.015 W Wk v
0.03 0.04 0.01 Wo
0.02 0.03
0.02 0.005
0.01 0.01
0 0 0
0 5 Singu10larV1 a5lueI2 n0dex25(tota3 l0768)35 0 5 Singu10larVa15lueIn20dex(2 t5otal230560)35 0 5 Singu10larVa15lueIn20dex(2 t5otal430096)35
Figure 3: Singular values of the weight matrices of each model, sorted from largest to smallest.
Information decay. Intuition for the decomposition in (1) comes from the plots in Figure 3.
They show that several popular models exhibit an exponential decay of their singular values for
important linear layers. To the extent that singular components capture the distribution of “essential
information” of a weight matrix – a claim that we will further justify in Section 4 – this shows that
a value k ≪ rank(W) can be chosen and weight matrix W can be split as in (1) so that much of the
“secret” information is retained in WC by C . We consider any suitable k > 1 since in Appendix E.0.1
we show an attack that proves that if C holds just the top singular component k = 1 of W and
offloads the rest of W to an untrusted D , then D can extract WC by solving a system of linear
equations formed from a small number of inference queries.
2.2 Securing protocol framework
In the real world, to secure the two parties C and D run an interactive protocol Π to run inference
on ϕ . At the beginning of the protocol, Π, C and D both receive the input x, and at the end of
Θ
executing Π, they should both obtain the output of the protocol Π(x), which must be equal ϕ (x).
Θ
Even after several executions of Π, D should not be able to learn C ’s part Θ . 1
C
We elaborate on the last two points above with the informal definitions below to help quantify
the utility of such decompositions.
Definition 3 (Efficiency). For a protocol Π as described above, let T (Π) be time complexity
Θ
computation of the model under the protocol. Analogously, let T (Π), and T (Π) be the time
ΘC ΘD
complexity computation of local computation of C and D, respectively. A decomposition (ϕ ,ϕ )
ΘC ΘD
of ϕ is said to be ϵ−efficient if there is for a protocol Π such that, for a small ϵ > 0,
Θ
T (Π) ≤ ϵ·T (Π). (2)
ΘC Θ
Definition 4 (Safety). A decomposition (ϕ ,ϕ ) of ϕ is called safe if an associated protocol Π
ΘC ΘD Θ
exists such that Π is both efficient and confidential.
The main hypothesis we put forth in this paper is that all LLMs have safe decompositions. We
give supporting empirical and theoretical evidence to support this hypothesis. Specifically, this is
done by applying Singular Value Decomposition on certain critical layers of the model to split a
model between a trusted entity and an untrusted entity. We then use cryptographic protocols that
are safe: they can provably, under cryptographic assumptions, resist a natural reconstruction strategy
by an adversarial untrusted entity that tries to recover the trusted part of the model layer by layer.
There are tradeoffs between the efficiency protocols and strength of guarantees against this class of
1More generally, we could insist that D not be able to learn a functionally (even approximately) equivalent model
to Θ except by essentially training such a model from scratch.
6
dezilamroNeulaVattacks giving a spectrum of choices for the protocols. Protecting against arbitrary reconstruction
attacks by an adversarial D appears to be a long term research challenge.
3 Hybrid inference protocol
In this section, we introduce a protocol designed to secure the decomposed two MLP sequential
weights WC and WC , which constitute critical intellectual property (IP) of the model, against theft
i i+1
by an attacker with access to an edge device. Simply utilizing singular value decomposition (SVD)
to split the layers does not adequately protect against potential threats, as attackers might still
deduce the hidden components of the weights using methods such as denoising, query-based attacks,
algebraic techniques, or model recovery strategies. We demonstrate that the proposed protocol
perfectly secures against these attacks. Furthermore, in Section 4, we illustrate that applying this
protocol to (only a few) select sequential layers can adequately secure the model IP.
Additionally, for the validity and practicality of the following operations and claims, we consider
all parameters to be integers with general precision denoted by q (i.e Int8, Int32), such that
W ,W ∈ Zdi+1×di. Typically, widely employed quantization techniques convert floating-point
i i+1 q
weights into an integer format; accordingly, our method is also applicable. It is important to note
that deployed neural networks in real-world scenarios often operate using low-precision weights.
Therefore, we evaluate our solution under these conditions.
Pre-deployment noise generation. Ahead of the model deployment, where the model is
stored only on the C, it generates a large batch of i.i.d random vectors (i.e much larger than the
dimension of the model layers ∼ N = 232) per any layer on which the protocol would be applied,
that are the size of the layer input. Those vectors will serve later, in inference time, to generate the
noise that will be required to mask outputs from C to D. These vectors will be periodically refreshed
according to the used protocol and required security level, where in the basic protocol each random
vector will be used only once. We denote these vectors by z 1,i,z 2,i,...,z
N,i
∼ Uniform[0,L]di, where
we assume that Z iZ⊤
i
∈ Zd qi×di is of full rank (the noise lies on full dimension field), and L ∈ Z
q
is a
predefined number (to be chosen according to the numbers ranges in the problem, namely the data,
and weight values). The C shell keeps those vectors in its storage, but not in its memory (for efficient
memory consumption). In addition, any multiplication ∀[i,j],WDz is computed in advance and
i j,i
stored in the C storage, as well, this will be an essential tool in the method being used to cancel the
propagated noise. For clarity, we denote z˜ = WDz . In subsection 3.3 we suggest a costly-efficient
j,i i j,i
sophisticated alternative version.
Remark 1. For clarity, without loss of generality, our approach is applied to MLP layers. However,
the methods apply to other models that include linear operators. For example, convolution layers
can be represented as a linear operator with a restricted weights structure matrix. As an additional
example, a detailed description of the protocol for the attention head layer appears in Appendix ??.
3.1 Protocol overview
In this subsection, we present an overview of the protocol, as described in Figure 4, and subsequently,
in Subsection 3.2, we establish the correctness of the protocol and demonstrate its capability to
secure data under reasonable assumptions. This protocol is designed to ensure the security of hybrid
inference processes involving two consecutive layers of indexes i and i+1, within the range [1,m],
where m represents the total number of layers in the model. For a given inference prompt x, the
input is the output activation a from the preceding layer. Notably, if i = 1, the protocol input
i−1
7a = x which is the initial input to the model. The protocol has three stages in layer i and two in
0 t
layer i+1. The protocol unfolds as follows.
Figure 4: Efficient and perfectly secured protocol for hybrid inference
• A- Computation: parallely D and C compute their cross-product of the respective decomposed
weight matrix with the input, namely aD = WDa , and aC = WCa . Then D sends its result,
i i i−1 i i i−1
the vector aD to C .
i
• B- Reproducing: C combines both products of the previous stage, to get the original i’s
layer output, the one that would be calculated under regular non-decomposed model inference
(one-party setting). It obtained by simply applies the activation function σ on the sum of both
terms: a = σ(cid:0) aC +aD(cid:1) = σ(cid:0)(cid:0) WC +WD(cid:1) a (cid:1) = σ(W a ). The activation output a indeed
i i i i i i−1 i i−1 i
could be the input to the next layer i+1, however, it embodies an analytical relation to WC, a fact
i
that could leveraged to infer the layer’s secret, and therefore it could not be sent and shared with D
as-is, and must be securely masked before.
• C- Obfuscation: C is masking the activation a as follows, it first create the noise ∆ =
i i
n (·), where n (·) is a noise creation function that can be implemented is several ways. Then
create create
it combines the noise with the activation to get a(noisy) = n (a ,∆ ), where n (·) is a dedicated
i add i i add
function for that purpose and a(noisy) is the noisy activation that can be securely downloaded to D .
i
In our case we implemented these functions as follows:
a(noisy) = n (a ,∆ ) ≜ mod(a +∆ ,L),2 (3)
i add i i i i
∆ = n (·) ≜ z (4)
i create i
In subsection 3.3 we provide generalized version for ∆ . Layer i+1 of the protocol has an additional
i
2 stages:
• D- Computation: this is exactly the same as stage A but now with the noisy input
a(noisy) which is the output of stage C from the previous layer i. Therefore the output will be
i
aD(noisy) = WD a(noisy). Then D send it back to C .
i+1 i+1 i
• E- Reproducing: C first removing the noise from aD(noisy) by applying a dedicated noise
i+1
(cid:16) (cid:17)
removal function aD = n aD(noisy) , and now it holds two "clean" decomposed weight
i+1 remove i+1
matrices aC and aD so it can calculate the a layer’s output same as in stage B.
i+1 i+1 i+1
In our case we implemented the noise removal function based on lemma 1, by applying
(cid:16) (cid:17) (cid:16) (cid:17)
aD = n aD(noisy) ≜ mod aD(noisy) −∆˜ ,L . (5)
i+1 remove i+1 i+1 i
2a(noisy) ∼U[0,L] by Lemma 2, implying that the device learn nothing about the C weights.
i
8Incasethenextlayer(i+2th)isalsodecomposed,theprotocolwillcontinuetostageC:Obfuscation
by adding mask noise to the resulting activation a of the current i+1th layer.
i+1
3.2 Theoretical guarantees
The following lemma shows the effective removal of the noise propagated at E:Reproducing 3.1.
Lemma 1. The noise reduction equation 5 satisfies
mod(WD a ,L) = mod(aD(noisy) −∆˜ ,L),
i+1 i i+1 i
namely, if the original result fits L, ∥WD a ∥ < L, then aD = WD a .
i+1 i ∞ i+1 i+1 i
The outcome of the noise cancellation on D is equivalent to the results that would be achieved
in the absence of noise masking (assuming it fits L). Consequently, the performance in the C is
comparable to that in a single-party setup, without any degradation in accuracy.
Hypothetically,sendingactivationa toDbytheC wouldresultinexposingthesecretdecomposed
i
weights to D. Indeed, given a strong analytical relation between the activation and the C decomposed
weights, the adversary may be able to reconstruct the layer’s weights. The following lemma shows
that the proposed masking algorithm perfectly secures the activation, which in turns ensure perfect
protection of the decomposed layer’s weights.
Theorem 2 (Perfectly secure masking). Given a discrete random variable s ∈ Zd and random noise
n ∼ U[0,L−1]d, we define a masked variable as s = mod(s+n,L). Then s ∼ U[0,L−1]d, and
n n
s and s are independent, namely the masking is perfectly secured.
n
A visual illustration of the making effect is given in the following Figure .
𝑁𝑜𝑖𝑠𝑒~𝑈0,5
𝑚𝑜𝑑 𝑁𝑜𝑖𝑠𝑒+𝑆𝑒𝑐𝑟𝑒𝑡,5~𝑈0,5
Cloud Secret Noise Secret+ Noise Mod(Secret+ Noise,5)
10 10 10 10
𝑚𝑜𝑑
=
5 5 5 5
0 0 0 0
0 5 10 0 5 10 0 5 10 0 5 10
Edge Mod(Secret+ Noise,5) Transformation Cloud Transformed Noise Output
10 10 10 10
𝑊𝑒∙𝑎𝑛𝑜𝑖𝑠𝑦 𝐷𝑒𝑛𝑜𝑖𝑠𝑒
5 5 5 5
0 0 0 0
0 5 10 0 5 10 0 5 10 0 5 10
Figure 5: Efficient and perfectly masking a given secret.
Example 1 (Non-feasible inference-averaging attack). Consider a sophisticated attacker, exposed
to D enters the LLM in the same inference batch, denoted by x. Therefore, in the i-th layer, an
intermediate inference a +∆ will be obtained. Still, we will remember that the masking would result
i i
in mod(a +∆ ,L). Therefore it will not be possible to minimize (almost nullifying) the masking that
i i
is allegedly expressed by noising the true signal, so that the clean signal, namely a , will be revealed
i
and so the decomposed weights of The i-th layer would be exposed.
93.3 Masking generalization: cost efficient
The computation of noise cancellation vectors z˜ might be costly; therefore, to decrease the
t,i
operationalexpensesassociatedwithpreparingthesevectorsinthebackground,severalenhancements
are suggested herein. Remember that generating the vectors z˜ necessitates storing the decomposed
t,i
WD in the C and computing z˜ = WDz . This process is for new vectors z that are continually
i t,i i t,i t,i
regenerated in the background to augment the pre-existing, extensive set of random vectors created
prior to deployment. The most straightforward extension would be to use the z from the pre-
j,i
existing set of vectors and the corresponding z˜ multiple time, where the index j will be selected in
j,i
random.
Initially, to reduce the seemingly risk that an adversary would be able to learn the available
noise vectors in Z by sampling it multiple times, the pre-computed batch size N of Z should be
i i
selected to be a large enough (e.g., N > 10e6) along with a limited number (e.g., 2) the same vector
z can be re-used. In principal, to mitigate the aforementioned risk, a linear combination of a small
j,i
number ℓ of randomly selected noise vectors from Z will be used instead of a single vector from Z
i i
(cid:88)
∆ = mod( α z ,L),
i j j,i
j∈Si
where S is a set of size ℓ of random indexes [1,N], α ∈ Z are co-prime scalars with L randomly
i j L
generated in inference time t (real-time) . The resulting ∆ still U[0,L−1] as shown in Lemma 3.
i
Lemma 3. Given n ∼ U[0,L−1]d and α ∈ Z co-prime to L for i = 1..l, n = mod((cid:80)l α n ,L)
i i L s i=1 i i
is distributed U[0,L−1]d.
The corresponding noise cancellation vector is computed as following ∆˜ = (cid:80) α z˜ . The
i j∈Si j j,i
computation of ∆
i
and ∆˜
i
requires only O(ℓd i) operation using ℓ pre-computed Z
i
∈ Zdi selected in
S . The conditions of Lemma 1 still holds, so the same equation 5 is used to cancel the noise. The
i
advantage of this extension is ensuring that the chance to get S = S for t > t is negligible.
i,t1 i,t2 2 1
Example 2. for N = 106 and l = 50, there are ∼ 10235 possible distinct combinations of z , so
j,i
the chance to get the same combination at least twice after 1015 trials is ∼ 10−206, which make it
practically impossible, these even without taking in account an additional randomization provided by
α . Please refer to appendix D to a detailed analysis.
j
4 Experiments
Experimental Setup. In this section, we explore how to obtain a neural network decomposition
with the desired properties. This is examined through benchmarking various decompositions with
the WikiText dataset [48] and assessing their effectiveness against fine-tuning attacks. We conducted
tests on LLaMA2-7B [70], Phi-2 [49], and GPT-2 Small [58], using EleutherAI’s evaluation code [19]
to measure perplexity on a word completion task, where higher scores indicate lower performance.
We used NVIDIA A100 GPU for all of our experiments.
Decomposition effectiveness. We aimed to show it is possible to decompose a model into
a small, secured and information-rich portion and a large, exposed and valueless portion that is
devoid of IP. Thus, for each of the three models, we evaluated multiple distinct decompositions, each
defined by choosing a specific decoder block and a particular layer type within the block (MLP layers
(c_fc, c_proj) or attention layers (W ,W , W , W ), and a specific number of singular values to
q k v o
remove from the layer’s weight matrix. Each decomposition is represented by a single dot in the
10Algorithm 1 Sequential Masking Protocol Via Random Stream
Input: Previous layer output a ; Inference number t; Two decomposed sequential layers
i−1
W
i
∈ Rdi+1×di, and W
i+1
∈ Rdi+2×di+1; pre-computed random noise z 1,i,z 2,i,...,z N,i, and their
corresponding z˜ ,z˜ ,...,z˜ ; (Optional) Masking matrices R ,R .
1,i 2,i N,i i i+1
Parallel compute of D and C Parties:
The D party:
The C party:
aD i ← W iDa i−1 aC i ← W iCa i−1
S ← UniformSample({1,2,...,N},ℓ)
i
(cid:80)
∆ ← α z
i j∈Si j j,i
(noisy)
a ← a +∆ .
i i i
D computes: aD(noisy) ← WD a(noisy) {≡ aD +(cid:80) α z˜ }
i+1 i+1 i i+1 j∈Si j j,i
D sends aD(noisy) to C
i+1
C compute n ← (cid:80) α z˜ {propagated noise}
i j∈ (cid:16)Si j j,i
(cid:17)
C compute a ← σ aC +aD(noisy) −n {≡ σ(cid:0) aC +aD (cid:1) }
i+1 i+1 i+1 i i+1 i+1
Return a .
i+1
figure, representing an almost full LLM, missing only a number of singular values (indicated on the
x axis), in a single layer (indicated by the trace) and a single block. To emphasise the effectiveness
between layer types only, we averaged the results across all the blocks of an LLM for the same
layer type. Results in figure 6 show that for each model examined, there are at least a few layer
types from which removing even a small amount of singular vectors (< 10) in a single block can
significantly degrade model perplexity. Model scores plummeted from a baseline perplexity of around
10 to thousands, indicating that these vectors hold significant IP.
GPT-2 Phi-2 Llama-2-7B
1
11
0
1
00 1
0
0
0M M
k
k
0
55555 2222 11
0
10
0
0k
0
0
555 22 11
0
10
0
0k
0
0
5555 222 c c
W
W
W
W_ _
q
k
v
of pc roj
100 5 22 102 10 52 Baseline
012341020406080100120140160180200220240260280300 0123410661332002663334004665336006667338008669331000 01234101062133204265336407468539601061 6171 3281 0381 6491 3600
NumberofExcisedSingularValues NumberofExcisedSingularValues NumberofExcisedSingularValues
Figure 6: Perplexity scores for all three models, after removing various numbers of singular values
from different layer types in a single decoder block. Results are averaged across all possible blocks.
Block effectiveness. We explored the impact of bypassing different sequences of decoder blocks
on the effectiveness of an exposed model, to identify optimal blocks for decomposition. Testing the
three LLMs, we bypassed 1, 3, 5, or 7 consecutive blocks and recorded the subsequent perplexity (see
figure 7). Bypassing early blocks severely degraded performance, indicating they hold substantial IP
and are crucial for decomposition. This is consistent with findings [23] that show shallow blocks are
vital for knowledge retention. Bypassing the last blocks also reduced performance, but less so.
Exposed Model recovery. We investigated the consequences of fine-tuning [25] using public
datasets to restore the compromised portion of the phi-2 model. Using the Alpaca dataset [69]
and LoRA [29], we fine-tuned for 10 epochs across five decomposition configurations and one with
randomized model parameters. Each configuration is characterized by an index (1-6), percentage
11
ytixelprePGPT-2 Phi-2 Llama-2-7B
1
1
10 11
0
1
00 0 10 1
0
0
0M M MB B
k
k
0
1 10 1
0
10 0
0
0k k
0
0
5555 2222 1 10 1
0
10 0
0
0k k
0
0
55555 2222 1 3 5
7
Baseline
10 10
0
10 52 10 52
0 2 4 6 8 10 0 4 8 12 16 20 24 28 0 4 8 12 16 20 24 28
BlockIndex BlockIndex BlockIndex
Figure 7: Performance degradation of the LLMs when bypassing windows of 1, 3, 5, or 7 consecutive
blocks. The x-axis represents the center block of each window, while the y-axis shows the perplexity.
of offload (calculated as defined by the efficiency property), and amount of excised singular values,
and the number of blocks in which all layer types were decomposed at the beginning (+) and end
(-) of the model. After decomposition, all exposed models showed random performance, but after
fine-tuning we observed that as less computation is offloaded and more of the model is exposed,
recovery becomes easier. Configurations offloading 92% to 97.5% remained robust against recovery,
as depicted in figure 8.
Idx Offload[%] ExcisedSV Blocks Recovered(KChanges)
100M Partitioned(KChanges)
Rand 0 0 All 10M Recovered(BlocksChange
1 85 380 All 1M Partitioned(BlocksChange
2 92 200 All Baseline
3 95 200 ±10 100k Randomized
4 97.5 200 ±5 10k RecoveredRandomized
5 99.4 50 ±5 1000
6 99.9 10 ±5 100
10
Offload[%]
Figure 8: Impact of fine-tuning on various decomposition configurations, detailed in the left table.
Compute & latency analysis. We simulated the latency of a neural network decomposition
between an edge device and a cloud server, considering the computational capacities of GPUs,
network bandwidth and delay. Our simplified analysis, based on a version of Llama2 [70] with only
feed-forward layers, decomposed in a third of its layers, shows edge computation dominates latency
(150.34 ms), followed by data transfer (71.31 ms), with cloud computation being minimal (0.66 ms).
Our approach leaves a mere 1.5% of operations to the cloud (see details in C).
5 Discussion
In this paper, we presented SLIP, a novel hybrid inference technique designed to safeguard DNNs,
especially LLMs, against theft when offloaded to low-security resources like edge devices, leaving
minimal computations on the secure resource, such as the cloud or trusted execution environments.
To our knowledge, SLIP is the first such technique that is both practical for real-world use and
provably secure, without compromising model accuracy or introducing significant latency. The
technique’s security is multifaceted: it leverages the rapid decay of singular values in LLM weight
matrices to effectively decompose them and store only the most critical singular vectors securely,
while the remaining model is useless and robust against restoration attempts with fine-tuning.
The secret model is protected from an adversarial compute resource through our secure inference
protocol, which is provably secure and efficient. Practically, SLIP introduces minimal computational
12
ytixelpreP
ytixelpreP
85 92 95 97.5 99.4 99.9overhead at inference, with latency primarily influenced by network delays, which can be mitigated
in scenarios with ample bandwidth, such as when using a TEE. Additionally, the technique does
not cause degradation in accuracy since decomposition retrains the original results, and the noise
added is canceled completely. These properties make SLIP highly promising for industry application,
enabling cost-effective model deployment on edge devices, and providing a foundation for the research
community to enhance security and efficiency in a hybrid inference framework.
Limitations & Future Work. Our protocol’s current limitations present opportunities for
further research, including testing on additional models and tasks, and quantifying latency and
FLOPs in practical implementations. Additionally, future work could focus on developing algorithms
to efficiently determine the optimal model decomposition that balances security, latency, and offload
according to the model owner’s preferences. Moreover, this framework can be expanded to secure
user prompts for privacy, obfuscate model architecture, and enhance the efficiency of noise generation,
retrieval, and removal processes.
13References
[1] A. Anonymous. Crayon: Customized on-device llm via instant adapter blending and edge-server
hybrid inference. ACL 2024, Under Review. Paper under review.
[2] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep
learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on
computer and communications security, pages 308–318, 2016.
[3] M. Abdalla, F. Bourse, A. De Caro, and D. Pointcheval. Simple functional encryption schemes
forinnerproducts. InIACR International Workshop on Public Key Cryptography, pages733–751.
Springer, 2015.
[4] A. Acar, H. Aksu, A. S. Uluagac, and M. Conti. A survey on homomorphic encryption schemes:
Theory and implementation. ACM Computing Surveys (Csur), 51(4):1–35, 2018.
[5] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet. Turning your weakness into a strength:
Watermarking deep neural networks by backdooring. In 27th USENIX Security Symposium
(USENIX Security 18), pages 1615–1631, 2018.
[6] A. Beimel. Secret-sharing schemes: A survey. In International conference on coding and
cryptology, pages 11–46. Springer, 2011.
[7] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
[8] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
[9] L. Birch, W. Hackett, S. Trawicki, N. Suri, and P. Garraghan. Model leeching: An extraction
attack targeting llms, 2023.
[10] G. Chaopeng, L. Zhengqing, and S. Jie. A privacy protection approach in edge-computing based
on maximized dnn partition strategy with energy saving. Journal of Cloud Computing, 12(1):29,
2023.
[11] H. Chen, C. Fu, B. D. Rouhani, J. Zhao, and F. Koushanfar. Deepattest: An end-to-end
attestation framework for deep neural networks. In Proceedings of the 46th International
Symposium on Computer Architecture, pages 487–498, 2019.
[12] J. Chen and X. Ran. Deep learning with edge computing: A review. Proceedings of the IEEE,
107(8):1655–1674, 2019.
[13] M. Chen and M. Wu. Protect your deep neural networks from piracy. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, pages 1–7, 2020.
[14] T. Chen, T. Ding, B. Yadav, I. Zharkov, and L. Liang. Lorashear: Efficient large language
model structured pruning and knowledge recovery, 2023.
[15] K. Cheng, N. Xi, X. Liu, X. Zhu, H. Gao, Z. Zhang, and Y. Shen. Private inference for deep
neural networks: A secure, adaptive, and efficient realization. IEEE Transactions on Computers,
72(12):3519–3531, 2023.
14[16] A. Dalskov, D. Escudero, and M. Keller. Secure evaluation of quantized neural networks.
Proceedings on Privacy Enhancing Technologies, 2020(4):355–375, Aug. 2020.
[17] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruhle, L. V. S. Lakshmanan, and
A. H. Awadallah. Hybrid llm: Cost-efficient and quality-aware query routing, 2024.
[18] L. Fan, K. Ng, and C. S. Chan. Rethinking deep neural network ownership verification:
Embedding passports to defeat ambiguity attacks. In Proceedings of the Annual Conference on
Neural Information Processing Systems, pages 4716–4725, 2019.
[19] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu,
A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds,
H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A
framework for few-shot language model evaluation, 12 2023.
[20] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning, volume 1. MIT Press, 2016.
[21] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT Press,
2016.
[22] M. Grailoo, Z. U. Abideen, M. Leier, and S. Pagliarini. Preventing distillation-based attacks on
neural network ip, 2022.
[23] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts. The unreasonable
ineffectiveness of the deeper layers, 2024.
[24] K. Gupta, N. Jawalkar, A. Mukherjee, N. Chandran, D. Gupta, A. Panwar, and R. Sharma.
Sigma: Secure gpt inference with function secret sharing. Cryptology ePrint Archive, Paper
2023/1269, 2023. https://eprint.iacr.org/2023/1269.
[25] Z. Han, C. Gao, J. Liu, S. Q. Zhang, et al. Parameter-efficient fine-tuning for large models: A
comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.
[26] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural
Computation, 18:1527–1554, 2006.
[27] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural
Computation, 18:1527–1554, 2006.
[28] X. Hou, J. Liu, J. Li, Y. Li, W. jie Lu, C. Hong, and K. Ren. Ciphergpt: Secure two-party gpt
inference. Cryptology ePrint Archive, Paper 2023/1147, 2023.
[29] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rank adaptation of large language models, 2021.
[30] L. Huang, G. Zhao, and C. Qin. Recoverable active protection framework for neural network
models. In 2023 IEEE International Workshop on Information Forensics and Security (WIFS),
pages 1–6, 2023.
[31] Z. Huang, W.-j. Lu, C. Hong, and J. Ding. Cheetah: Lean and fast secure {Two-Party} deep
neural network inference. In 31st USENIX Security Symposium (USENIX Security 22), pages
809–826, 2022.
15[32] T. Ishiyama, T. Suzuki, and H. Yamana. Highly accurate cnn inference using approximate
activation functions over homomorphic encryption. In 2020 IEEE International Conference on
Big Data (Big Data), pages 3989–3995. IEEE, 2020.
[33] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot. High accuracy and high
fidelity extraction of neural networks. In 29th USENIX security symposium (USENIX Security
20), pages 1345–1362, 2020.
[34] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. {GAZELLE}: A low latency framework
for secure neural network inference. In 27th USENIX security symposium (USENIX security
18), pages 1651–1669, 2018.
[35] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang. Neurosurgeon:
Collaborative intelligence between the cloud and mobile edge. In Proceedings of the Twenty-
Second International Conference on Architectural Support for Programming Languages and
Operating Systems, ASPLOS ’17, page 615–629, New York, NY, USA, 2017. Association for
Computing Machinery.
[36] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang. Neurosurgeon:
Collaborative intelligence between the cloud and mobile edge. SIGARCH Comput. Archit. News,
45(1):615–629, apr 2017.
[37] S. Kariyappa, A. Prakash, and M. K. Qureshi. Maze: Data-free model stealing attack using
zeroth-order gradient estimation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 13814–13823, June 2021.
[38] B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten.
Crypten: Secure multi-party computation meets machine learning. Advances in Neural Infor-
mation Processing Systems, 34:4961–4973, 2021.
[39] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical
Report 001, University of Toronto, 2009.
[40] J.-W. Lee, H. Kang, Y. Lee, W. Choi, J. Eom, M. Deryabin, E. Lee, J. Lee, D. Yoo, Y.-S. Kim,
et al. Privacy-preserving machine learning with fully homomorphic encryption for deep neural
network. iEEE Access, 10:30039–30054, 2022.
[41] Q. Li, Z. Shen, Z. Qin, Y. Xie, X. Zhang, T. Du, and J. Yin. Translinkguard: Safeguarding
transformer models against model stealing in edge deployment, 2024.
[42] Y. Li, H. Wang, and M. Barni. A survey of deep neural network watermarking techniques.
Neurocomputing, 461:171–193, 2021.
[43] N. Lin, X. Chen, H. Lu, and X. Li. Chaotic weights: A novel approach to protect intellectual
property of deep neural networks. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, 2020.
[44] J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network predictions via minionn
transformations. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’17, page 619–631, New York, NY, USA, 2017. Association for
Computing Machinery.
16[45] X. Ma, G. Fang, and X. Wang. Llm-pruner: On the structural pruning of large language models.
In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances
in Neural Information Processing Systems, volume 36, pages 21702–21720. Curran Associates,
Inc., 2023.
[46] S. Meftah, B. H. M. Tan, C. F. Mun, K. M. M. Aung, B. Veeravalli, and V. Chandrasekhar.
Doren: toward efficient deep convolutional neural networks with fully homomorphic encryption.
IEEE Transactions on Information Forensics and Security, 16:3740–3752, 2021.
[47] F. Meng, Z. Wang, and M. Zhang. Pissa: Principal singular values and singular vectors
adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024.
[48] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models, 2016.
[49] Microsoft Research. Phi-2: The surprising power of small language models. Language model
with outstanding reasoning and language understanding capabilities, 2023.
[50] A. Mok. ChatGPT could cost over $700,000 per day to operate, 2023. [Accessed 22-05-2024].
[51] S.Obla,X.Gong,A.Aloufi,P.Hu,andD.Takabi.Effectiveactivationfunctionsforhomomorphic
evaluation of deep neural networks. IEEE access, 8:153098–153112, 2020.
[52] D. Oliynyk, R. Mayer, and A. Rauber. I know what you trained last summer: A survey on
stealing machine learning models and defences. ACM Comput. Surv., 55(14s), jul 2023.
[53] G. Onoufriou, P. Mayfield, and G. Leontidis. Fully homomorphically encrypted deep learning
as a service. Machine Learning and Knowledge Extraction, 3(4):819–834, 2021.
[54] R. Perrault and J. Clark. Artificial intelligence index report 2024. 2024.
[55] F. A. Petitcolas. Kerckhoffs’ principle. In Encyclopedia of Cryptography, Security and Privacy,
pages 1–2. Springer, 2023.
[56] B. Pulido-Gaytan, A. Tchernykh, J. M. Cortés-Mendoza, M. Babenko, G. Radchenko,
A. Avetisyan, and A. Y. Drozdov. Privacy-preserving neural networks with homomorphic encryp-
tion: Challengesandopportunities. Peer-to-Peer Networking and Applications, 14(3):1666–1691,
2021.
[57] A. Pyone, M. Maung, and H. Kiya. Training dnn model with secret key for model protection.
In 2020 IEEE 9th Global Conference on Consumer Electronics, pages 818–821, 2020.
[58] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[59] D. Rathee, M. Rathee, N. Kumar, N. Chandran, D. Gupta, A. Rastogi, and R. Sharma.
Cryptflow2: Practical 2-party secure inference. In Proceedings of the 2020 ACM SIGSAC
Conference on Computer and Communications Security, pages 325–342, 2020.
[60] M. Ribeiro, K. Grolinger, and M. A. M. Capretz. MLaaS: Machine learning as a service. In
Proceedings of the 14th IEEE International Conference on Machine Learning and Applications,
pages 896–902, 2015.
17[61] B. D. Rouhani, M. S. Riazi, and F. Koushanfar. Deepsecure: scalable provably-secure deep
learning. In Proceedings of the 55th Annual Design Automation Conference, DAC ’18, New
York, NY, USA, 2018. Association for Computing Machinery.
[62] A. Sanyal, M. Kusner, A. Gascon, and V. Kanade. Tapas: Tricks to accelerate (encrypted)
prediction as a service. In International conference on machine learning, pages 4490–4499.
PMLR, 2018.
[63] A. Schlögl and R. Böhme. ennclave: Offline inference with model confidentiality. In Proceedings
of the 13th ACM Workshop on Artificial Intelligence and Security, pages 93–104, 2020.
[64] A. Shamir, I. Canales-Martinez, A. Hambitzer, J. Chavez-Saab, F. Rodrigez-Henriquez, and
N. Satpute. Polynomial time cryptanalytic extraction of neural network models, 2023.
[65] P. Sharma, J. T. Ash, and D. Misra. The truth is in there: Improving reasoning in language
models with layer-selective rank reduction, 2023.
[66] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The german traffic sign recognition
benchmark: Amulti-classclassificationcompetition. InThe 2011 International Joint Conference
on Neural Networks, pages 1453–1460. IEEE, 2011.
[67] Z. Sun, R. Sun, L. Lu, and A. Mislove. Mind your weight (s): A large-scale study on insufficient
machine learning model protection in mobile apps. In 30th USENIX security symposium
(USENIX security 21), pages 1955–1972, 2021.
[68] K. Szentannai, J. Al-Afandi, and A. Horváth. Preventing neural network weight stealing via
network obfuscation. In Intelligent Computing: Proceedings of the 2020 Computing Conference,
Volume 3, pages 1–11. Springer, 2020.
[69] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.
Stanford alpaca: An instruction-following llama model, 2023.
[70] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023.
[71] J.-B.Truong, P.Maini, R.J.Walls, andN.Papernot. Data-freemodelextraction. InProceedings
of the IEEE/CVF conference on computer vision and pattern recognition, pages 4771–4780,
2021.
[72] Y. Uchida, Y. Nagai, S. Sakazawa, and S. Satoh. Embedding watermarks into deep neural
networks. In Proceedings of the 2017 ACM on international conference on multimedia retrieval,
pages 269–277, 2017.
[73] X. Wang, Y. Zheng, Z. Wan, and M. Zhang. Svd-llm: Truncation-aware singular value
decomposition for large language model compression. arXiv preprint arXiv:2403.07378, 2024.
[74] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking
machine learning algorithms. arXiv:1708.07747, 2017.
[75] H.Xu,Y.Su,Z.Zhao,Y.Zhou,M.R.Lyu,andI.King. Deepobfuscation: Securingthestructure
of convolutional neural networks via knowledge distillation. arXiv preprint arXiv:1806.10313,
2018.
18[76] M. Xue, Z. Wu, Y. Zhang, J. Wang, and W. Liu. Advparams: An active dnn intellectual
property protection technique via adversarial perturbation based parameter encryption. IEEE
Transactions on Emerging Topics in Computing, 2022.
[77] Y. Yan, X. Pan, M. Zhang, and M. Yang. Rethinking {White-Box} watermarks on deep learning
models under neural structural obfuscation. In 32nd USENIX Security Symposium (USENIX
Security 23), pages 2347–2364, 2023.
[78] P. Yang, Y. Lao, and P. Li. Robust watermarking for deep neural networks via bi-level
optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 14841–14850, 2021.
[79] Y. Yao, J. Duan, K. Xu, Y. Cai, Z. Sun, and Y. Zhang. A survey on large language model
(llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing,
4(2):100211, 2024.
[80] D. Ye, P. Liu, and J. Xu. How fast can we obfuscate using ideal graded encoding schemes.
Cryptology ePrint Archive, Paper 2017/321, 2017. https://eprint.iacr.org/2017/321, see
also https://www.esat.kuleuven.be/cosic/blog/program-obfuscation/.
[81] J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin, H. Huang, and I. Molloy. Protecting
intellectual property of deep neural networks with watermarking. In Proceedings of the 2018 on
Asia conference on computer and communications security, pages 159–172, 2018.
[82] L. Zhang, C. Li, Q. Hu, J. Lang, S. Huang, L. Hu, J. Leng, Q. Chen, and C. Lv. Enhancing
privacy in large language model with homomorphic encryption and sparse attention. Applied
Sciences, 13(24), 2023.
[83] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,
Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie,
and J.-R. Wen. A survey of large language models, 2023.
[84] M. Zhou, X. Gao, J. Wu, J. C. Grundy, X. Chen, C. Chen, and L. Li. Model obfuscation for
securing deployed neural networks. openreview, 2022.
[85] T. Zhou, Y. Luo, S. Ren, and X. Xu. NNSplitter: An active defense solution for DNN model via
automated weight obfuscation. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato,
and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning,
volume 202 of Proceedings of Machine Learning Research, pages 42614–42624. PMLR, 23–29
Jul 2023.
[86] T. Zhou, S. Ren, and X. Xu. Obfunas: A neural architecture search-based dnn obfuscation
approach. In Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided
Design, pages 1–9, 2022.
[87] I. Zimerman, M. Baruch, N. Drucker, G. Ezov, O. Soceanu, and L. Wolf. Converting trans-
formers to polynomial form for secure inference over homomorphic encryption. arXiv preprint
arXiv:2311.08610, 2023.
19A Proofs
Recall Lemma 1. 1
For ∥We a ∥ < L, the noise reduction equation 5 satisfies
i+1 i ∞
(cid:16) (cid:17)
mod(WD a t,L) = mod aD(noisy) t−∆˜ ,L ,
i+1 i i+1 i,t
namely, aD t = WD a t.
i+1 i+1 i
Proof.
(cid:16) (cid:17)
aD t = mod aD(noisy) t−∆˜ ,L
i+1 i+1 i,t
(cid:16) (cid:17)
= mod WD a(noisy) t−WD ∆ ,L
i+1 i i+1 i,t
= mod(cid:0) WD mod(a t+∆ ,L)−WD ∆ ,L(cid:1)
i+1 i i,t i+1 i,t
= mod(cid:0) WD (a t+∆ )−WD ∆ ,L(cid:1)
i+1 i i,t i+1 i,t
(cid:124)(cid:123)(cid:122)(cid:125)
(1)
= mod(cid:0) WD a t,L(cid:1)
i+1 i
= WD a t
i+1 i
(cid:124)(cid:123)(cid:122)(cid:125)
(2)
where, (1) follows from properties of modulo operator and (2) follows form the assumption that
∥We a ∥ < L.
i+1 i ∞
□
Now, we show that discrete uniform distribution U[0,L−1] is closed to a constant addition
modulo L.
Lemma 4 (Uniform Distribution Closeness). Given a constant s ∈ Zd, random variable n ∼
U[0,L−1]d, and a random variable x = mod(s+n,L), it holds x ∼ U[0,L−1]d.
Proof. We first prove it for one-dimensional case d = 1. It is enough to show
P(x = a) = P(mod(s+n,L) = a) = 1/L.
We separate analysis to two distinct cases: (1) mod(s,L) ≤ a, and (2) mod(s,L) > a:
1. For mod(s,L) ≤ a, the equation mod(s+n,L) = a holds iff n = a−s. In this case,
P(mod(s+n,L) = a) = P(n = a−s) = 1/L.
2. Similarly, for mod(s,L) > a, equation mod(s+n,L) = a holds iff n = L−(mod(s,L)−a).
So also this case,
P(mod(s+n,L) = a) = P(n = L−(mod(s,L)−a)) = 1/L.
20Finally, for any dimension d > 1, by repeating the same analysis for each dimension , we get
d
(cid:89)
P(x = a) = P(x == a ) = 1/Ld,
i i
i=1
where x and a are i-th coordinate of d-dimensional vectors x and a. □
i i
Recall Theorem 2.
Given a constant s ∈ Zd, random variable n ∼ U[0,L−1]d, and a random variable x = mod(s+n,L),
it holds x ∼ U[0,L−1]d.
Proof. First, we prove that s ∼ U[0,L−1]d, for v ∈ Zd
n 1 L
(cid:88)
P(s = v ) = P(s = v |s = v )P(s = v )
n 1 n 1 3 3
v3∈Zd
(cid:88) 1
= P(s = v )
Ld 3
v3∈Zd
1 (cid:88)
= P(s = v )
Ld 3
v3∈Zd
1
= .
Ld
Next, we prove the independence by showing that P(s = v ,s = v ) = P(s = v )P(s = v ),
n 1 2 n 1 2
where v ∈ Zd and v ∈ Zd.
1 L 2
1
P(s = v ,s = v ) = P(s = v |s = v )P(s = v ) = P(s = v ) = P(s = v )P(s = v ).
n 1 2 n 1 2 2 Ld 2 n 1 2
Note that we used P(s |s) = 1/Ld from Lemma 4 in both equations. □
n
Recall Lemma 3.
Given n ∼ U[0,L−1]d and α ∈ Z co-prime to L for i = 1..l, n = mod((cid:80)l α n ,L) is distributed
i i L s i=1 i i
U[0,L−1]d.
Proof. We start from the observation that multiplication of Z Ld by co-prime of L modulo L creates
permutation of Z d. Permutation mapping on Z d preserves the probability of each element of Z d,
L L L
so α n is also distributed U[0,L−1]d.
i i
Next, use 2, by applying it with n = α n and s = (cid:80)l α n . □
1 1 i=2 i i
B Practical Considerations
B.1 Pseudo-random numbers generation
In the main paper, we analyzed our protocol assuming true randomness from uniform distributions.
However, in practice, we would use a Cryptographically Secure Pseudorandom Number Generator
(CSPRNG).A CSPRNGhasthe propertythatno polynomial timeadversarycandistinguish between
true uniform distribution and the output of such a generator except with a negligible probability.
By using such a generator, our security proofs will continue to hold. Otherwise, an adversary that
breaks security of our schemes can be used to construct a distinguisher that breaks the security of a
CSPRNG.
21B.2 Model partitioning
Model Partitioning MP receives a list of hyper-parameter triplets that define the how to perform a
model partition:
[[block,layer_type,K] ,...,[block,layer_type,K] ]
1 n
where block is any decoder block within an LLM, layer_type is any layer out of the possible
layers in a decoder block, and K is the number of top singular vectors to excise. Each triplet defines
where singular vectors should be excised and how many. Therefore, using the list, MP partitions a
given LLM into a secret version, comprised of the components detailed in the list, and an exposed
version, comprised of all remaining components. Finding the optimal list with which to perform the
partition is an optimization problem, that should consider trade-offs between the three following
metrics: model security level, compute offload, and latency. For example, increasing security, given
by excising a larger amount of singular vectors, comes at a cost of less compute offload. Additionally,
choosing more layers on which to perform partition results in additional networking latency. In
this paper, evaluated several partitions, taking into account experimental findings in Section 4. For
example, we realised it is useful the focus on first and latter blocks of the decoder backbone, and that
it is enough to focus on a small amount of singular vectors. However, optimized partitioned could be
reached by incorporating Reinforcement Learning, Genetic Algorithms or additional heuristics.
C Compute & latency analysis details
In this analysis, we evaluate the inference latency of a neural network model that is partitioned
between an edge device and the cloud according to the protocol suggested in the paper. This
setup necessitates considering both computational latency and data transfer latency between the
two compute resources. To accurately measure the total latency, we break down the protocol into
distinct phases and calculate the computations in each, which contribute to the overall latency. Key
parameters influencing latency include the bandwidth between the edge and cloud, the number of
layers, the processor FLOPs capacity, and the size of the data being transferred. The formulas used
for latency approximation are given by:
Computation Time (T ): The time required to perform the necessary computations
Compute
on the edge or cloud.
FLOPs
T = (1)
Compute HardwareFLOP/s×Utilization
Data Transfer Time (T ): The time required to transfer data between the edge and
Transfer
the cloud.
input_size×b×s
T = λ + (2)
Transfer a Bandwidth
Where λ is the network latency between the edge and the cloud, b is the batch size, and s is the
a
size of each activation value in bytes.
Note that we chose to keep the SVD-decomposed weight matrix W decomposed in the cloud
ci
as U Σ V . This decision is based on the assumption that k ≪ m,n, which makes the amount
ci ci ci
of computation lower than performing a full matrix multiplication. Conversely, we decided to
reconstruct W after decomposition for use on the edge device because in this scenario, using
ei
the reconstructed matrix on the edge results in less computation compared to operating with the
decomposed form.
22C.1 Total latency calculation
Since the processing of each phase is sequential and cannot be parallelized, the total latency is the
sum of the individual latency of each phase. This ensures that the computation and data transfer
steps are accounted for in a linear sequence, reflecting the actual flow of operations during inference.
The total latency (T ) for the purposed protocol, where the model is partitioned between
Total
an edge device and a cloud server, can be divided into three main components: edge computations
(Pe), cloud computation (Pc), and data transfer (cid:0) Pt(cid:1).
Edge Computation are given by
FLOPSedge = (l−l )·Pe +l ·Pe = 2mnb·l+nb·(l−l ) (3)
d non-dec d compute d
Where l represents the decomposed layers and l represents the total layers in the model. With
d
regards to the full model on the edge, we find that
FLOPSfull = 2mnb·l+nb·l (4)
So the cloud offload consists of n·l activation operations.
d
Cloud Computation are given by
(cid:16) (cid:17)
FLOPScloud = l · Pc +Pc +Pc +Pc
d compute composed noise_gen noise_add (5)
= 2l ·b·(mk+nk+2nl +1.5n)
d v
Data Transfer is given by
Ntransfer = Pt +l ·(cid:0) Pt +Pt (cid:1) = nb(2l +1) (6)
input d upload download d
Total Latency is given by
T = T (cid:0)FLOPSedge(cid:1) +T (cid:0)FLOPScloud(cid:1) +T (cid:0) Ntransfer(cid:1) (7)
Total Compute Compute Transfer
C.2 Selecting parameter values
Next, we will select specific values for the parameters used in the formulas and analyze the results.
This will help illustrate the computational and data transfer latency in our protocol, when the
weights are split between an edge device and a cloud server. We based many of our numbers on
Llama2 model, treating it as if it were a regular feedforward (FF) network to simplify the analysis
and make the estimations more straightforward. Additionally, we assumed a single token generation
and chose the one of the decomposition strategies mentioned in the Experiments section as an
efficient and robust option.
C.3 Results
The results of our calculations are as follows, and are discussed in the paper itself in section 5.
23D Probabilistic analysis precomputed vector repeating
To analyze the analyze the chance to get the same selection of z , we first compute C(N,ℓ) the
j,i
total number of possible combination to selecting ℓ vectors out of N,
N!
C(N,ℓ) = .
(N −ℓ)!ℓ!
The chance to get the same combination of z equals
j,i
P(∃t ,t such that t < t < n, so S = S )
1 2 1 2 i,t1 i,t2
=1−P(∄t ,t such that t < t < n, so S = S )
1 2 1 2 i,t1 i,t2
C(N,ℓ)!
=1−
C(N,ℓ)n(C(N,ℓ)−n)!
For N = 106, ℓ = 50 and n = 1015, C(106,50) = 3.28E235. Applying the above equation
directly is not practical because for a very large C(106,50) value, so instead we can use the following
approximation
C(N,ℓ)! C(N,ℓ)−n
1− <1−( )n
C(N,ℓ)n(C(N,ℓ)−n)! C(N,ℓ)
n
=1−(1− )n
(C(N,ℓ)
n2
∼1−(1− )
(C(N,ℓ)
n2
=
C(N,ℓ)
∼10−206
E Illustration of secret masking
E.0.1 Minimal number of Singular Vectors that must be decomposed
The following lemma shows that under the use of SVD decomposition, more than one singular vector
is needed to be removed from the model and stored separately on the cloud, noted by W .
c
Lemma 5 (The minimal number of required singular vectors to be hidden). The matrix V should
c
contain more than one singular vector, in order to ensure that an attacker cannot reconstruct W
c
using only the singular vector available to them on the edge in W , or that they are combinatoricaly
e
complex for the user to reconstruct.
Proof. Let us denote
W = S V D =
e e e e
     t
−− S −− σ 0 0 ··· 0 −− D −−
1 1 1
 −− S 2 −−   0 σ 2 0 ··· 0   −− D 2 −− 
     
 −− S 3 −− · 0 0 σ 3 ··· 0 · −− D 3 −− 
 .   . . . .   . 
 . .   . . . . . . . .   . . 
     
−− S −− 0 0 0 ··· σ −− D −−
n n n
24Let us suppose that one singular vector (without the loss of generally the largest one), namely S ,
1
should be enough to be designated for the matrix W , ensuring W is unrecoverable to an attacker..
c c
Then, in the edge device, we have n−1 orthonormal vectors in {S ,S ,...,S }. Since all vectors in
2 3 n
it are orthonormal it could be leveraged to construct the following n−1 equations,
St ·S = 0
2 1
St ·S = 0
3 1
.
.
.
St ·S = 0,
n 1
which results in only one degree of freedom. Now, simply applying the Gram-Schmidt process (or
any other orthonormalization techniques) would result in the exact S singular vector. This is in
1
contradiction to the assumption. □
Remark 2 (Finding the corresponding missing singular vector). The sum of the singular values
of a matrix equals the trace of that matrix, meaning trace(W ) = λ +λ +...+λ +λ . Given
e 1 2 n−1 n
λ ,λ ,...,λ , the first singular value could be computed by
2 3 n−1
λ = trace(W )−(λ +λ +...+λ )
1 e 2 3 n
Corollary 5.1. Directly by lemma 5, separating n >> k ≥ 2 singular values and their corresponding
singular vectors into their cloud designated matrices S ,V ,D , results in k equations with n unknown
c c c
parameters. Thus, in this case, the level of freedom that can not be retrieved (by linear algebraic
operations) can given by n−k+1.
F Equivalence of Convolutional Layer and Fully Connected Layer
Convolutional Layer
Consider the input tensor X of shape (H,W,C), the convolutional kernel K of shape (kH,kW,C,N),
and the output tensor O of shape (H ,W ,N). For simplicity, assume stride s = 1 and no padding.
o o
The output at position (i,j) for filter n is given by:
C kH kW
(cid:88)(cid:88)(cid:88)
O = X ·K
i,j,n i+u−1,j+v−1,c u,v,c,n
c=1u=1v=1
Fully Connected Layer
The equivalent fully connected layer will have an input vector x and a weight matrix W. The input
tensor X is unrolled into a vector x of size H ·W ·C.
Unrolling Process
1. Flatten the Input:
x = vec(X)
2. Construct the Weight Matrix: The weight matrix W for the fully connected layer will be
of shape (H ·W ·N,H ·W ·C). Each row of W corresponds to a specific position of the kernel
o o
applied to the flattened input.
25For each output position (i,j) and filter n:
W = K
n,index(i,j,c) u,v,c,n
where index(i,j,c) maps the 3D position (i,j,c) in the input tensor to the corresponding position in
the flattened vector.
Analytical Formulation
For each output position (i,j) and filter n:
C kH kW
(cid:88)(cid:88)(cid:88)
O = X ·K
i,j,n i+u−1,j+v−1,c u,v,c,n
c=1u=1v=1
Unroll X to x and define the corresponding weight vector w in W:
(i,j,n)
w = vec(K )
(i,j,n) :,:,c,n
The output O is obtained by matrix multiplication:
o = W·x
where o is the flattened version of O.
26Table 1: Description of phases and operations
Phase Op Type Frequency Description FLOPs/Data Size
Upload Input Transfer Once Transfer input data X of n×b
size n to the cloud
Edge-Only
Compute Compute Each Compute all non- 2·m·n·b+n
non- decomposed operations ·b
decomposed for matrices W
ei
layer
Edge-Partial
Compute Compute Each Weights Matrix 2·m·n·b
decomposed Multiplication
layer W a
ei+1 inois
Cloud-Partial
Compute Compute Each Decomposed Matrices 2·k·b·(m
decomposed Multiplication U Σ V a +n)
c c c i
layer
Upload Edge
to Cloud Transfer Each Transfer edge activation n×b
decomposed a data to the
e,i+1noisy
layer cloud
Cloud
Activation
Function Compute Each Remove Noise + Vector 2·n·b·(L
v
decomposed addition + activation +1)
layer a = σ(cid:16) a +a −∆˜ (cid:17)
i+1 ci ei i
Cloud Noise
Vector
Generation Compute Each Sample L noise vectors 2·n·b·L
v v
decomposed and create ∆
i
layer
Cloud Noise
Addition Compute Each Vector addition a = n·b
inoisy
decomposed a +∆
i i
layer
Activation Data
Download Transfer Each Transfer final activation n×b
decomposed data to edge a
inoisy
layer
27Table 2: Summary of model parameters, hardware specifications, and network details
Parameter Value Description
Model Parameters
l : Total Layers 224 7 layers (4 attn + 3 mpl) per block
Llama2 contains 32 blocks
l : Decomposed Layers 70 7 layers (4 attn + 3 mpl) per block
d
10 decomposed blocks in configuration X4
n,m : Matrix Dimensions 4096, 4096 Attention Layer
b : batch_size 32 Average input length
k : Secret Singular Values 50 X4 configuration
l : Sampled Noise Vectors 50
v
s : Activation size in bytes 4b FP32
Hardware Specifications
Edge GPU FLOP/sec 4 TFlops/sec Nvidia Jetson Nano spec (estimation for
FP32)
Cloud GPU FLOP/sec 14 TFlops/sec Nvidia V100 spec
Utilization 40% Realistic estimation
Network Details
B : Network Bandwidth 25 MB/s Average bandwidth in the US
λ : Network Delay 35 ms Average latency in the US
a
Global IP Network Latency (att.net)
Table 3: Performance
Metric Value
FLOPs Full Model 240.547 GFLOPs
FLOPs Edge (Hybrid) 240.538 GFLOPs
FLOPs Cloud (Hybrid) 3.697 GFLOPs
Input Transfer 1.848 M
Compute Edge Latency 150.34 ms
Compute Cloud Latency 0.66 ms
Transfer Latency 71.31 ms
Total Latency 222.31 ms
28