GuideLight: ‚ÄúIndustrial Solutions‚Äù Guidance for More Practical Traffic
Signal Control Agents
Haoyuan Jiang1‚Ä†, Xuantang Xiong2‚Ä†, Ziyue Li3‚Ä†‚àó, Hangyu Mao4, Guanghu Sui4,
Jingqing Ruan2, Yuheng Cheng2, Hua Wei5, Wolfgang Ketter3, Rui Zhao4 ;
Abstract‚ÄîCurrently, traffic signal control (TSC) methods North
1 2 3 4 5 6 7 8
based on reinforcement learning (RL) have proven superior to conflict
traditionalmethods.However,mostRLmethodsfacedifficulties of 1 and 2 1
2
when applied in the real world due to three factors: input,
output, and the cycle-flow relation. The industry‚Äôs observable 38 5 TR hig roh ut gla hn le ane 3
inputismuchmorelimitedthansimulation-basedRLmethods. West Leftlane 6 2 Leftlane East 4
Throughlane 1
For real-world solutions, only flow can be reliably collected, Rightlane 47 5 A B
whereascommonRLmethodsneedmore.Fortheoutputaction, non-conflict: 6 C D PhaseB combining
most RL methods focus on acyclic control, which real-world movement 2 and 5 7 E F
signal controllers do not support. Most importantly, industry 8 G H
standards require a consistent cycle-flow relationship: non- South
(a) A standard intersection with four (b) Conflict matrix for eight
decreasing and different response strategies for low, medium, approaches and three lanes (right, movements, with conflict in grey
and high-level flows, which is ignored by the RL methods. To
through, left) within each approach and available phase indexed A-H
narrow the gap between RL methods and industry standards,
we innovatively propose to use industry solutions to guide Fig. 1: Illustration of a standard intersection and phases. A
the RL agent. Specifically, we design behavior cloning and
phaseisstandardizedasacombinationoftwonon-conflicting
curriculum learning to guide the agent to mimic and meet
industryrequirementsand,atthesametime,leveragethepower movements, e.g., phase A is movement 1 and 5 combined.
of exploration and exploitation in RL for better performance. In industry, a four-phase cycle control, i.e., A‚ÜíD‚ÜíE‚ÜíH,
Wetheoreticallyprovethatsuchguidancecanlargelydecrease is common, representing EW-Though ‚Üí EW-Left ‚Üí SN-
the sample complexity to polynomials in the horizon when
Through ‚Üí SN-Left.
searching for an optimal policy. Our rigid experiments show
that our method has good cycle-flow relation and superior
performance. The code is available here.
I. INTRODUCTION ‚Ä¢ R1: Input (State): The RL-based methods usually need
multiple environmental observations, which are rather chal-
Trafficsignalcontrol(TSC)isacriticalchaptertomaintain
lenging to obtain in the real world. Specifically, they need a
trafficefficiencyandsafetyinsmartsustainablemobility[1].
combinationofqueuelength,trafficflow,waitordelaytime,
Recently, employing reinforcement learning (RL) to solve
car position, and so on as the observation space [2], [3], [4],
TSC has become a research hotspot since it does not require
[5], [6], [7]. However, traffic flow is the only available and
too much artificial priori and can improve performance
stable index in most cities. SCATS, SCOOT [8], and other
significantly through the paradigm of trial and error. Despite
traditional industrial solutions thus only use traffic flow to
all efforts in this field, there is no real-world deployment of
provide stable and good enough performance.
an actual RL-based TSC to date. In fact, it has not yet been
‚Ä¢ R2: Output (Action): The learned RL policies do not
proven that RL is even applicable as TSC in a real-world
align with the industry practice. Firstly, Fig. 1 shows how
setting. Every investigation so far has been done without
traffic signals are usually controlled with the basic concept
considering real-world deployment requirements. In this pa-
of ‚Äúphase‚Äù, i.e., a combination of two non-conflicting traffic
per, we aim for a practical RL-based traffic signal control
movements(moredetailsinSecIII).AsshowninFig.2(a1),
method, considering the following industry requirements:
thereal-worldsolutionistodirectlydeterminethecycletime
1HaoyuanJiangwasaresearcheratSenseTime,thisworkwasdonethere, for each of the fixed-order four phases, e.g., A-D-E-H, and
nowinBaiduInc.jianghaoyuan@zju.edu.cn then repeat, known as ‚ÄúCyclic Control‚Äù. (Details in Sec III.)
2Xuantang Xiong, Jingqing Ruan, and Yuheng Cheng were research In cyclic control, the task is to determine the phase duration
interns at SenseTime Research. They are PhD candidates at the Chinese
for each phase of A, D, E, and H within a cycle. Thus, we
AcademyofSciencesandtheChineseUniversityofHongKong.
3Ziyue Li and Wolfgang Ketter are with the Information canalsosumuptogetthewholecyclelength.Cycliccontrol
Systems Department at the University of Cologne, Germany. isknowntobefriendlyandintuitivefordriverstofollow.As
zlibn@wiso.uni-koeln.de
4HangyuMao,GuanghuSui,andRuiZhaoarewithSenseTimeResearch. depictedinFig.2(b1),almostalltheRLmodels[4],[5],[9],
maohangyu@sensetime.com,zhaorui@sensetime.com [7], [10], [11], [12], [13], [14], [15], [16], [17], [18] follow
5HuaWeiisanassistantprofessorintheSchoolofComputingandAug- an ‚ÄúAcyclic Control‚Äù, which is still conceptual in simulation
mentedIntelligenceatArizonaStateUniversity.hua.wei@asu.edu
systemsandnotvalidatedinreallife:theactionistochoose
‚Ä†Theseauthorscontributeequallytothiswork.
‚àóThecorrespondingauthor. one phase from 8 available phases (with fixed 10 seconds
4202
luJ
51
]AM.sc[
1v11801.7042:viXra
enalthgiR
enalhguorhT
enaltfeL
Leftlane Throughlane RightlaneCyclic Control: appliedüá¶üá∫üá∫üá∏üá®üá≥üá¨üáßüá©üá™üáØüáµ‚ãØ Acyclic Control: conceptual considering All Phase configurations) [5], and MetaGAT
A 100 s A B C D ‚ãØ H
[10], have quite unreasonable cycle-flow relations, heavily
10s 10s 10s 10s 10s
60s H D 70 s confusing the drivers.
10s 10s 10s
E 120 s ùë°! ùë°" ‚Ä¶ In this paper, we present a practical RL model to tackle
(a1) Industry controls cyclically (b1) Academia controls acyclically
all three criteria above. We will use the traffic flow as state
Cycle Time Cycle Time
Max CT (1) Logistic Model (2) SCATS input and output cyclic phases as action so the trained RL
Stretch CT model is campatible with industry practice. Moreover, we
Alt Min 2 Ours also aim to maintain a rigid and rule-based-like cycle-flow
MetaGAT
Alt Min 1 DDPG relation in Fig. 2 by combining the traditional solution with
Min CT FRAP
Traffic Traffic the RL method, to obtain a solution that meets industrial
Non-Peak Climbing Peak Flow Flow
(a2) Non-decreasing relation in industry (b2) RL solutions break the relation needs and has good performance at the same time. In this
(a) Industrial solutions and requirements (b) Common RL-based academic methods
paper, we propose, GuidedLight, an RL-based agent guided
Fig. 2: The gaps between industry requirements and RL- by the ‚Äúindustrial solutions‚Äù. Specifically, we use Behaviour
based academic methods: Industrial requirements: (a1) Cloning (BC) to encourage the agent to learn from the
theyfollowcycliccontrol,e.g.,thecycleofphasesA-D-E-H teacher,e.g.,SCATS,andwealsoletthelearningprocessbe
(details in Fig. 1), and decide the duration for each phase; gradual and progressive: we adopt curriculum learning [23],
(a2) The cycle time should be non-decreasing with traffic whichteachestheagentfromeasytoadvanced.Insummary,
flow, encouraging stable drivers‚Äô behaviors. It also has three the contributions are three-fold:
stages: non-peak, climbing, and peak, with different stages ‚Ä¢ OurGuideLightlaysasignificantfoundationforthe
featuring various controlling patterns. Academic Solutions: practicalimplementationofRL-basedsolutionstobe
(b1) Common RL-based methods in academia control cycli- adopted in practice. Technically, our RL framework
cally,i.e.,tochooseonephaseoutofeightphases(A-H)for is purely designed in a way that matches the industry
the next time interval, with each chosen phase running for standards in terms of state, action, rewards, and cycle-
a fixed interval, e.g., 10 seconds; (b2) The cycle time-traffic flow relation.
flow relation is also diverse and unstable, disrupting drivers ‚Ä¢ To guarantee the rigid cycle-flow relation, we innova-
negatively. Our solution is the first RL-based cyclic TSC tively propose to use industrial rule-based solutions to
agent, which perfectly follows the non-decreasing relations, guide the agent via behavior cloning and curriculum
with also clear three stages. learning.Moreover,wealsotheoreticallyprovethatwith
guidancefromthetraditionalmethods,ourmethodguar-
antees a polynomial sample complexity in the horizon.
‚Ä¢ Experimentsshowthattheproposedmethodnotonlyre-
to release) for every 15 seconds. Thus, the resulting phase
spects the cycle-flow relations, but also achieves higher
sequencecanbequiterandomandunstable,causingdrivers‚Äô
performance,thankstothe‚Äúindustrial‚Äùguidanceandthe
confusion and even potential traffic accidents.
agent‚Äôs own exploration and exploitation.
‚Ä¢ R3: Cycle-Flow Relation: Most importantly, the industry
The rest of the paper is organized as follows: In Section
requiresthatforeachintersection:1)thetrendsofthetraffic
II, we will review the traditional methods and the RL-based
flow and the cycle time should be positively synchronized,
methodsfortrafficsignalcontrol;InSectionIII,wewillgive
whichmeansthecycle-flowcurveshowninFig.2(a2)should
the key background information and preliminaries; Section
always be non-decreasing, between the required minimal
IV will officially design the proposed ‚ÄúGuideLight‚Äù and
and maximal cycle time. A straightforward explanation is
specifically, Section IV-D will prove that such guidance can
that as the traffic flow regulated by this phase increases,
largely decrease the sample complexity to polynomials in
the duration required to fully clear this phase of the traffic
the horizon when searching for an optimal policy. Section
flow will also extend. Otherwise, the start time and stop
V will present the rigid experiments, Section VI discusses
time of the unreleased vehicle will be lost; 2) moreover, the
generalization, and Section VII will conclude.
relation between the traffic flow and the cycle time should
be three-stage [19]: as shown in Fig. 2(a2), both logistic II. RELATEDWORK
model [20] and SCATS [21] show that when the flow is
A. Traditional methods
medium (yellow region, climbing up to the peak), cycle
time should response very sensitively, and the other regions Traditional methods are widely used in the real world,
such as the purple region where the flow is reaching the most of them are cyclic control, which is friendly and con-
traffic capacity, the cycle time change should be smooth, sistent to drivers; however, they rely on strong assumptions
to avoid further delay. Mechanisms of R2 and R3 cultivate and manually specified rules. Fixed-time control [24] is one
stable behaviors and response patterns from the driver to of the earliest cycle-based methods: it configures the traffic
the traffic lights and avoid confusion and even potential signal plan as a fixed cycle length, which is inflexible and
trafficaccidents.Incontrast,asshowninFig.2(b2),theRL- cannot automatically adapt its policy to the actual situation.
based methods such as DDPG (Deep Deterministic Policy Actuated control model [25], [26] instead used pre-defined
Gradients) [22], FRAP (short for Flipping and Rotation and sets of rules to decide whether to adjust the current phase.Later on, methods with analytic solutions were proposed: thedurationofindividualphases.However,whenconfronted
Webster[27]directlycalculatesacyclelengthusingtheflow with a limited variety of observations, e.g., only flow is
ratioasinputtominimizethedelaytime:C = 1.5¬∑L+5 where available and a preferred rigid cycle-flow relationship, this
1‚àíY
Listhelosstime,andY isthesumofthecriticalflowratio. method may be incompetent.
Logisticmethod[27]usesthelogisticcurvetobetterrespond
III. PRELIMINARIES
to different types of traffic flow. SCATS [21] is the most
used one worldwide, which does not use a mathematical Some necessary domain information is given here.
model but a set of heuristic rules: it uses a set of simple Definition 1. Intersection is where two or more roads
algebraic expressions to describe the traffic characteristics cross. A 4-arm intersection is shown in Fig. 1, where each
and operating rules of the current road network. As shown arm (N, S, W, E) has entering lanes and exiting lanes.
inFig.2(a2)‚Äôsredcurve,SCATShandles(1)non-peaktraffic Definition2.TrafficMovementisdefinedasthedirection
flowwith‚Äústair-like‚Äùrules,withindexessuchasMinCT,Alt inwhichavehiclecrossesfromanenteringlanetoanexiting
Min 1, Alt Min 2 to be specified, (2) climbing traffic with lane, including go-through, turn-right, and turn-left in each
rapid response up to Stretch CT, and (3) peak traffic with arm.InFig1,thenumbers1-8representtheeightmovements
a flatter curse, capped by Max CT. All of these traditional controlled by the signal (right movements are usually free).
methods are simple and interpretable, but their assumption Definition 3. Phase is a combination of two non-
dependencies are relatively straightforward and inflexible. conflicting movements that could be released together. For
Actual situations may not conform to them, so traditional instance, movements 1 and 2 conflict, thus not being able
methods often cannot guarantee optimal results. to form a phase; but movements 1 and 5 can form W-E-
through phase, denoted as phase-A. In reality, not all the
B. RL methods.
intersections have all four legs with all movements and
In recent years, numerous RL-based TSC methods [28], phases; in that case, zero-padding will be used to mask the
[29] have achieved significant outcomes. As mentioned be- missing movement or phase.
fore, most RL-based methods are acyclic control, whose Definition 4. Cyclic Control In real-world solutions like
actionistodecidewhichphasetochoose.In[30],[12],[31], SCATS, four phases are usually controlled in a cyclic way:
[2], a single agent is employed to manage all intersections A ‚Üí D ‚Üí E ‚Üí H and then repeat, where in each phase,
within a given scenario. However, due to the joint of their thephasedurationneedstobedecided.Thecycletimeisthe
statespaceoractionspace,theseoftenencounterissuessuch summation of all phase durations plus red and yellow time.
as the curse of dimensionality [13] and instability [10]. A Our RL method follows the four-phase cyclic control.
different approach is taken by [14], [4], [32], which utilizes
IV. METHOD
image-basedstatesfordecentralizedintersectioncontrol.[5],
furtherproposedFRAP,awidelyfollowedmethodforacyclic Sec.IV-Awillintroducethestatespace,actionspace,and
control. It inputs the traffic features of all eight movements, rewarddesign.Sec.IV-Bintroducesthenetworkdesign.Sec.
combines two non-conflicting movements into a phase (8 IV-C introduces the training based on behavior cloning and
phasesobtainedintotal),andusesdeepQ-learningtodecide curriculum learning. Sec. IV-D will theoretically prove the
whichphasewillbeselectedforthenexttimestep.Building improved sample complexity with horizon H.
upon this, [9] further presents a General Plug-In (GPI)
A. RL Setup
module, where any arbitrary and various intersections can
be mapped into the standard one, thus having a unified state Generally, the single-agent TSC problem is formulated
andactionspaceforvariousintersections.Thisfacilitatesthe as a Markov decision process ‚ü®S,A,r,Œ≥,œÄ‚ü©. Every agent
co-training of large-scale scenarios. controls an intersection independently.
Nonetheless, this acyclic control has quite a wide gap to Toperfectlymatchourproposedalgorithmwithreal-world
real-world applications, and a major reason is the hardware: situations, the state, action, and reward are all carefully
most traffic signal controllers only support cyclic phase designed.
plans. A few methods have been tried to solve it. For State S: Only traffic flow and last step action-based
instance, [33] used DDPG [22] to determine each phase observations(e.g.,sampledevery5minor15min)arechosen
durationinthenextcycle.Nonetheless,thismethodrequires since commonly they are the only available index; Thus, the
the total cycle time to be fixed. Alternatively, [34] uses state space is continuous.
a strategy of deciding whether to remain in the present ActionA:Insteadofchoosingthenextphaseintheacyclic
phaseortransitiontothesubsequentphaseduringeachshort control, our action is designed to directly output the phase
interval. However, this approach neglects various practical duration for each phase in a cyclic control. Besides, there
constraints, such as ensuring that the discrepancy between are two more constraints: the total cycle time should change
the durations of two consecutive cycles is not excessively smoothly and should stay within the minimal and maximal
large. An innovative strategy is presented in [35], which thresholds. Thus, we translate the action into 3 categories,
modifies one phase‚Äôs duration during each timestep. This thus being discrete actions, which are adding/cutting 5 sec-
approach accommodates certain constraints and allows for onds (+5s/-5s) or non-changing (0s) for each phase: such a
dynamicadjustmentsinboththecompletecycledurationand design frees us from the computation burden of the originalModel Structure
Movementlevel Actor and Critic Loss
Embeddings
PhaseTraffic DecisionPolicyùúã
embeddings +5s
ùíÜ!,# MLP1 ¬±-5 0s s
I fln op wu st : i nA e s ai cn hg l me i on vt ee mrs ee nc tt sion with (a) ùíÜ!,$ FRAP sum LSTM MLP2 ùëé !"
MLP3
ùíÜ!,&
ùíÜ! ùíë!,# ùíë! MLP4 Agent‚Äôs CL
ùíÜ!,%
Phaselevelembeddings
(b) ùíë!,$
Laststep Greenlight Utilization
duration utilization balance
Intersectioni Behavior Cloning + Curriculum Learning
‚ÄúEntry-level Teacher‚Äù ‚ÄúAdvanced Teacher‚Äù
(c)
Behavior Cloning Loss
Flow Teacher‚Äôs CL
Fig. 3: Illustration of our proposed GuidedLight. One agent controls one intersection. (a) For each intersection, it will
observe movement-level features related to traffic dynamics and then use FRAP‚Äôs aggregating module to aggregate two
non-conflicting movements‚Äô embeddings into one phase of traffic embedding. (b) Combining phase traffic embedding with
other phase-level features, we get a wholesome embedding for a phase and then input them into LSTM and Actor-Critic.
(c) Teachers such as linear controller and industrial controller SCATS are adopted, via Behavior Cloning and Curriculum
Learning, to guide the agent‚Äôs cycle length (CL) in mimicking the teacher‚Äôs CL.
purely continuous action space, and the small change of 5s time for a vehicle to pass the intersection. Green imbalance
on each phase ensures the smoothness of cycle time. To gi evaluates the standard deviation of all the phases‚Äô gr:
consider the constraint of the total cycle time staying within
gi=standard deviation({gr} ) (4)
the minimal and maximal thresholds, once the cycle time allphases
hits the min (max) threshold, the action of -5s (+5s) will be It is worth mentioning that in the reward, we use metrics
masked. Thus, we have: besidetrafficflow,suchasqueuelength,thereasonis:during
traininginthesimulation,thequeuelengthisobtainableand
(cid:40) it is beneficial to be part of the reward to give the agent
{+5s,‚àí5s,0s}, if CT‚àà[min-threds,max-threds]
a= better-informedfeedback,sothatwhenbeingdeployedinthe
masked, otherwise
real world, even only traffic flow is observable, but agent‚Äôs
(1)
action can still tend not to cause long-queue.
Rewardr(s,a):wedefinetherewardbasedonthroughput Optimal policy œÄ‚àó(a|s): At each intersection i and each
v(‚Üë,thehigherthebetter),queuelengthl(‚Üì),andtwofactors
timesteptthegoaloftheagentistofindapolicyœÄ(a|s)that
thatmatterintheindustry,i.e.,green-lightutilizationrategr maximizes the expected return G := (cid:80) E[Œ≥tri], where Œ≥
t t t
(‚Üë), and green imbalance gi (‚Üì). Our reward function is the
is the discount factor.
weighted sum of the four factors, i.e.,
(cid:88)
œÄ‚àó(a|s)=argmax E[Œ≥tri] (5)
t
œÄ
r =w ¬∑v+w ¬∑l+w ¬∑gr+w ¬∑gi (2) t
v l gr gi
B. Network design
withweightstunedandspecifiedinSec.V-C.Throughput
The model is shown in Fig. 3. When deciding for t+1,
v is the number of vehicles passing through the intersection
we utilize the movement-level and the phase-level features
per unit of time. Queue length measures the length of the
from t. Then the fused feature is fed into LSTM [36] to
vehicle queue at the intersection. Greenlight utilization rate
capture long-term decision dynamics, and finally, the actor-
measures the portion of greenlight that is really used to
critic network makes the action [9].
releasevehicles,whichisinferredasfollowsforeachphase:
For the movement-level feature: the i-th intersection in
the scenario, the m-th movement (m‚àà{1,...,8}) observes
v√ó2.5s
gr = (3) K features. In our case, K = 3 includes only traffic
phase duration
dynamics, i.e., traffic flow, and two additional static indices,
wherevisthethrough-putofthisphaseand2.5seconds(a i.e., road capacity and indicator of whether the movement
commonsettingintrafficcontrol manual [27])isusuallythe exists. For any traffic movement, we get its embedding,
‚Ä¶
‚Ä¶where ||,Sigmoid(¬∑),MLP(¬∑) are concatenation, activation C. Training
function, and multilayer perceptron, respectively.
Ourexperimentsshowthatsolelyoptimizingtheagentus-
ingtheRLrewardingmechanismonlyyieldedunsatisfactory
e =||K Sigmoid(MLP (s )), (6)
i,j k=1 k i,m,k results regarding the industry requirement (consistent cycle-
flow relation). The reasons are two-fold: (1) the agent only
Then, we use part of FRAP to first aggregate two non-
perceives the traffic flow from the dynamic road network.
conflictingmovements‚Äôembeddingsintoaphaseembedding,
This restricted input hampers the agent‚Äôs ability to explore
and then also to embed the phase conflict information into
and identify a superior policy. (2) moreover, the reward is
the phase embeddings. (1) Add: FRAP adds the embedding
hard to align with the stringent requirements of the industry.
of movements as a phase representation. For the p-th phase
Behavior Cloning: To address this predicament, we em-
whichconsistsofmovementsj,j‚Ä≤,p‚àà[A,B,...,H]:¬Øe =
i,p
ploy BC to steer the agent‚Äôs actions: namely, we employ the
e +e , where the two movements j,j‚Ä≤ ‚àà [1,2,...,8].
i,j i,j‚Ä≤
traditional and market-proved solutions to guide our agent.
(2) Phase-pair representation: Once obtaining the phase
This approach substantially narrows the agent‚Äôs exploration
embedding, a phase-pair representation is constructed to
domain, facilitating faster optimization [37].
capturethepairwiserelationsforcompetition:eÀÜ =¬Øe ‚à•¬Øe .
i i,l i,l‚Ä≤
However, the well-developed solutions such as SCATS
(3) Phase competition: To avoid phase conflict, pairwise
directly output a continuous phase duration, and ours is a
competition scores [5] are learned as a competition mask,
discrete action of +5s/-5s/0s. How can we design a loss to
denoted as ‚Ñ¶. Thus, the phase-pair representations eÀÜ will
i
penalize the difference between the two? We translate them
go through 1√ó1 convolution and multiply with the phase
into logits. The action of the p-th phase from our agent‚Äôs
competitionmasktoyieldthemaskedphase-pairembedding:
decision policy œÄ can be sampled from below logits:
e = Conv (eÀÜ )‚äó‚Ñ¶, and ‚äó is Kronecker product. We
i 1√ó1 i
denote the whole FRAP module as FRAP(¬∑) [5]: lt =MLP (ht),where lt ‚ààR3, (13)
i,p p i i,p
e i =FRAP(e i,1,...,e i,8),where e i ‚ààR4√ó4√ódf, (7) The label of BC guidance can be generated as:
e i is 4-phase embedding tensor (phase A-D-E-H), with em- ltÀÜ =Ô£± Ô£≤ 10 EE i tt ,p +‚àí 55 ‚â§‚â• TT i tt , ‚àí‚àí p 11
(14)
beddingdimensionasd f.Wesumitalongtherowdirection i,p
Ô£≥ 2
i,p otherwisei,p
and get the phase embedding with traffic flow information.
whereEt isthephasedurationdecidedbytheexpertmodel
p i,f =sum(e i),where p i,f ‚ààR4√ódf (8) ofattimei,p t,theTt‚àí1isthedurationtimeatlasttimestep,the
i,p
0/1/2 represents the ground truth is +5s/-5s/0s respectively.
For the phase-level feature: sÀÜ denotes the phase-level
Lastly, we employ the Cross Entropy as the BC loss:
raw observations with KÀÜ features in these observations. In
our case, KÀÜ = 3, including more contexts of each phase‚Äôs L =CrossEntropy(lt ,ltÀÜ ), (15)
BC i,p i,p
durationatthelastcycletime,greenlightutilizationrate,and
green imbalance. We use each MLP to embed them. Curriculum Learning: Delving deeper into the BC pro-
cess, we adopt the Curriculum Learning approach to ensure
p =||KÀÜ MLP(sÀÜ ), (9) the agent‚Äôs progressive learning trajectory. We gradually
i,c kÀÜ=1 kÀÜ
introduce teachers from easy to advanced to generate the
Weconcatenatep andp astheintersectioni‚Äôsfeature: label in Eq. (14).
i,f i,c
‚Ä¢ Easy guide: we choose a linear model that posits a
p =||(p ,p ) (10)
i i,f i,c straightforward linear correlation between cycle dura-
tion and traffic flow. Phase duration‚âà0.35√óv, where
Lastly, we adopt the LSTM to perceive long-term historical
0.35isroughlyestimatedfromregressionbasedonlarge
states, so that the agent can use historical trends to achieve
amounts of offline data.
better consistency between phase duration and traffic flow.
We take the phase-level feature pt at this time step t of the ‚Ä¢ Medium guide: logistic curve based on [27] could be
i used, where the cycle time and traffic flow follow a
intersection i and last time step LSTM hidden state ct‚àí1 to
logistic relation between the Max CT and Min CT, as
the LSTM and get the ht and ct.
i shown in Fig. 2(a2).
ht,ct =LSTM(p ,ct‚àí1), (11) ‚Ä¢ Advanced guide: In contrast, SCATS exhibits a more
i i
intricate behavior, with the three-stage cycle-flow re-
For the decision policy œÄ, it receives the ht as input and lations, as shown in Fig. 2(a2), thus being the most
i
outputs the action at. advanced guide.
i
TotalLoss:TheBehaviorCloningandCurriculumLearn-
at i =œÄ(¬∑|ht i). (12) ingarecombinedwiththeActor-Critictoformulatethetotal
training loss:
As mentioned in Sec IV-A about action, at is defined as
i
+5s/-5s/0s for each phase. L=Œ±L +Œ≤L +Œ∫L . (16)
Actor Critic BCAlgorithm 1 GuideLight training process Assumption 2. We assume that there is a guarantee for
Input: A set of target intersections I; training episodes E, a the exploratory policy. Within the confines of an online
set of teacher algorithms B; contextual bandit scenario characterized by a stochastic
Output:OptimizedpolicyœÄ; contexts‚àºp andarewardfunctionr(s,a)spanning[0,R],
0
1: for episode=1, ...,E do anexploratorypolicyexistsineveryround,suchthatthetotal
2: Initialize LSTM hidden state c0 for each intersection regret is bounded:
and clean buffer D ‚Üê‚àÖ.
T
(cid:88)
3: for each time step t do E [r(s,œÄ‚ãÜ(s))‚àír(s,œÄt(s))]‚â§f(T,R)
s‚àºp0
4: Recordeachrawobservationofeachintersectionto t=1
D;
This exploratory policy guarantee is a prevailing assump-
5: Get the current time step intersection feature ac- tion in extensive literature, whether in tabular methods [42]
cordingtoEq.(10)anduseLSTMtoperceivelong-
or in methods similar to ours using general function approx-
term historical information according to Eq. (11);
imation [43], [44].
6: Get and take action at i according to Eq. (12); BasedonthePerformanceDifferenceLemmaproposedby
7: Get the rewards ri t and the next time step raw Kakade et al. [45], we can establish a relationship between
observation;
the iterations I and the discrepancy between the optimal
8: end for value function V‚ãÜ and the current value function VœÄ.
0 0
9: for each step in training steps do
10: sample minibatch data from D; E s0‚àºd0[V 0‚ãÜ(s 0)‚àíV 0œÄ(s 0)]
11: SelectoneteacherfromB accordingtothetraining I
(cid:88)
processandintroducetheteacherlabelaccordingto = E s‚àºd‚ãÜ i[QœÄ i(s,œÄ i‚ãÜ(s))‚àíQœÄ i(s,œÄ i(s))] (17)
Eq. (14) ; i=0
12: ComputerLaccordingEq.(16)andupdateNetwork Consequently, we can derive an upper bound for regret.
parameter; Let T denote the total exploration steps and E represent the
13: end for number of epochs after which the policy is updated. Hence,
14: end for the final number of policy iterations is T/(EH).
E [V‚ãÜ(s )‚àíVœÄ(s )] (18)
s0‚àºd‚ãÜ 0 0 0 0
where Œ±,Œ≤,Œ∫ are tuning parameters. The Actor loss and the T (cid:88)/EH
Critic loss are the same as Proximal Policy Optimization = E s‚àºd‚ãÜ i[Q i(s,œÄ i‚ãÜ(s))‚àíQ i(s,œÄ i(s))] (19)
(PPO) [38]. i=0
T/EH
(cid:88)
D. Theoretical Analysis ‚â§C¬∑ E s‚àºdg[Q i(s,œÄ i‚ãÜ((s)))‚àíQ i(s,œÄ i(s))] (20)
i
Will introducing such guidance even speed up learning? i=0
Weprovethatourapproach,undertheassumptionofguided T (cid:88)/EH
‚â§C f(H,R) (21)
policyefficacy,cansignificantlyreducethesamplecomplex-
ity from exponential in the horizon to polynomial. i=0
Theorem 1. For 0-initialized œµ-greedy, where initial esti- In the derivation, inequality (20) arises from Assumption
mates for action probabilities start at zero, there exists an 1, while inequality (21) is predicated on Assumption 2.
MDP instance where the sample complexity scales exponen- Drawing on the results presented [46], it can be inferred
tially with the total time horizon H to identify a policy with that under the setting of asymptotic functions, f(H,R) =
a suboptimality less than 0.5. [39] R¬∑(AE F(H))1/2, where E is a polynomial function. Con-
Assumption1.AssumethattheoptimalpolicyasœÄ‚ãÜ,with sequently, the rate of our algorithm is up to a factor of
its visited state distribution as d‚ãÜ. The policy updated based C¬∑poly(H) [43].
on the guidance loss is denoted as œÄÀÜg with its visited state
V. EXPERIMENT
distribution as dgÀÜ. We make the assumption that œÄÀÜg cover
A. Towards Real-world Dataset and Simulation
the states visited by œÄ‚ãÜ subject to an upper limit:
Several efforts have been made to ensure that our eval-
d‚ãÜ(s)
sup ‚â§C uation scenario is as consistent as possible with the real-
s dgÀÜ(s)
world scenario. (1) Road Network: we built one simulation
This ratio, also referred to as the distribution mismatch scenarioinSUMObasedonareal-worldcase:FenglinWest
coefficient, is used in gradient descent methods [40]. Road, Shaoxing, Zhejiang, as shown in Fig. 4, which is a
We explain the advantage of guidance based on an online corridor with 10 intersections to be signal-controlled. More
contextual bandit scenario [41], and we look at regret, a details are in [9]; (2) Traffic flow: most SUMO scenarios
measureofthedifferencebetweentherewardthatcouldhave only support 1-hour-long flow data, which is not enough to
been obtained by always taking the best possible action and train agents to learn to respond to all levels of flow, i.e.,
the reward that was actually obtained by the policy used. the 3-stage cycle-flow relations shown in Fig. 2. Thus, wetraffic flow refers to the volume of vehicles passing through
the intersection per unit of time. Green-light utilization
10
measures the proportion of effective time when vehicles
(a) Fenglincorridor in Shaoxing city
pass through the intersection during the green light phase.
1 2 3 4 5 6 7 8 9 10 8 Utilization balance evaluates the level of balance among the
(b) Fenglinscenario in SUMO 2 (c) Flow in theLo c Ia nlt tim ee rsection 3 four phases of the intersection. Queue length measures the
length of the vehicle queue at the intersection.
Fig. 4: Illustration of Fenglin scenario. Additionally, we visualize the synchronization between
cycle time and traffic flow. In practice, this is an important
indicator of the TSC system. Cycle time tends to increase
collect the real 24-h flow data, e.g., shown in Fig. 4(c), within a bounded range as traffic flow increases.
and based on the real-world traffic flow distribution, we
generated 1400 different flow patterns for each intersection, E. Results
maximally enhancing the agents‚Äô generalizability; (3) More
1) Main Results: Quantitative metrics are displayed in
configurations: more detailed designs such as aligning the
Table I, demonstrating that our method consistently attains
SUMO‚Äôsflowsamplingratewiththereal-worldsensors(e.g.,
the highest overall scores, showing substantial advantages
every 5min or 15min), following the pre-defined 4-phase
over other algorithms. Particularly, in the crucial metric of
plan (i.e., Phase A-D-E-H), supporting the output of phase
green-lightutilizationrate,whichishighlyimportantinreal-
durationfromagent(insteadofchoosingthenextphase),and
world applications, our algorithm markedly surpasses both
soon,havebeenmade.Thedatasetisopen-sourcedherefor
the SCATS and FTC methods. Although our approach only
the community.
achieves the second-best throughput and green imbalance,
B. Benchmark Methods thegapbetweenourperformanceandthebestresultsinthese
WecompareourGuideLightwithbothtraditionalandRL- categoriesisrelativelyminor.Detailedanalysisisasfollows:
based methods: Behavior Cloning and Curriculum Learning helps
Traditional Methods: GuidedLight learn. Via the delicately designed heuristics,
suchasthethreestagesandparameterspecifications,SCATS
‚Ä¢ Fixed Time Control (FTC) [24] with random offset
achieves the highest throughput, being the most efficient
executes each phase within a loop, utilizing a pre-
controller. Thanks to our Behavior Cloning and Curriculum
defined phase duration.
Learning,GuidedLightsuccessfullymimicsSCATS‚Äôsbehav-
‚Ä¢ SCATS [21] The most widely used conventional traffic
ior and achieves the second. As illustrated in Fig. 2(b2),
control algorithms based on real-time traffic data, road
our GuidedLight also achieves a non-decreasing cycle time-
characteristics,andmanuallydesignedoperationalrules.
traffic flow relation, and good-shaped three-stage controlling
RL-based Methods:
behavior for non-peak, climbing, and peak traffic.
‚Ä¢ DDPG [33] A robust RL model widely employed
RL-based agent boosts GuidedLight‚Äôs performance
across diverse problem domains and settings, which is
in other metrics. Through the Actor-Critic-based agent,
meticulously tailored to suit our setting.
GuidedLight further optimizes industrial-concerned metrics
‚Ä¢ FRAP‚àó [5]: It is the ground-breaking acyclic RL based
such as green utilization rate and green imbalance, as well
on phase competition. We implement it in a cyclic way
as the queue length, crowning our GuidedLight as the best
based on ours without BC.
overall performer.
‚Ä¢ MetaGAT‚àó [10] combines contextual meta-
Academic solutions based on RL have rather dis-
reinforcement learning based on GRU to improve
counted performance and confusing cycle time-traffic
generalization and GAT for cooperation. We also
flow relations. The well-recognized top performers, FRAP
implement it in a cyclic way.
and MetaGAT, when applied in practical industrial settings,
‚Ä¢ GuidedLight Our proposed method in this paper. We
suffered performance drops and cannot beat industrial solu-
used the linear guide and SCATS guide.
tions such as SCATS and ours. This is because their states,
C. Implementation details of GuidedLight actions, and rewards are all derailed from being practical.
Furthermore, as illustrated in Fig. 2(b2), they even have
The implementation details of GuidedLight are summa-
decreasing cycle time-traffic flow relation, meaning higher
rizedinTableII.Theparametersarefine-tunedmainlybased
traffic demand can trigger shorter cycle time and vice versa,
on grid search.
which can be rather confusing for drivers.
D. Evaluation metrics
2) Visualization of Synchronization: We visualize the
We use two metrics to evaluate these methods. Firstly, synchronization of cycle time and traffic flow in Fig. 5
we evaluate the results based on traffic flow, queue length, based on two intersections. It shows that our method and
and two factors that matter in the industry, i.e., green-light the SCATS system achieve notable synchronization; namely,
utilization rate, and utilization balance. These factors form the cycle time co-varies responsively with fluctuations in
ourrewardfunction,whichouragentreceivesasinput.Here, trafficflow.Onthecontrary,theFTCandDDPGapproaches
)snim5/selcihev.oN(wolFTABLE I: Main Results. The best boldfaced and second best underlined. Our method always achieves the top 2.
FTC SCATS DDPG FRAP‚àó MetaGAT‚àó GuideLight (Ours)
Throughput 155.95¬±0.08 161.86¬±0.31 158.34¬±0.17 113.15¬±0.03 141.24¬±0.05 159.63 ¬±0.28
Green Utilisation 87.57¬±0.01 92.19¬±0.02 117.74¬±0.01 103.02¬±0.02 98.32¬±0.01 121.80¬±0.02
Green Imbalance -52.60¬±0.01 ‚àí57.04¬±0.01 ‚àí58.57¬±0.01 ‚àí71.57¬±0.01 ‚àí60.67¬±0.01 -56.03¬±0.01
Queue Length ‚àí82.82¬±0.02 ‚àí61.30¬±0.22 ‚àí71.05¬±0.12 ‚àí265.32¬±0.13 -58.91¬±0.12 -55.66¬±0.29
All 108.09¬±0.09 135.71¬±0.33 149.47¬±0.20 ‚àí120.33¬±0.15 119.98¬±0.15 169.74¬±0.39
Intersection 1
(a1) FTC (b1) DDPG (c1) SCATS (d1) Ours w/o BC (e1) Ours
Intersection 10
(a2) FTC (b2) DDPG (c2) SCATS (d2) Ours w/o BC (e2) Ours
Fig. 5: The synchronization between cycle time and traffic flow in Fenglin dataset. As in (e2), GuidedLight outputs the
control policies with cycle time perfectly following the flow and thus meets industry requirements. Fig 2(b2) summarizes
the overall cycle time-traffic flow relation.
TABLE II: Implementation details of GuidedLight (a)Behavior Cloning Loss (b)Total Reward
Linear Guide LogisticGuide
Items Details SCATS Guide
Learning rate 5e-4
Actor loss coefficient Œ± 1
Critic loss coefficient Œ≤ 1 Linear LogisticSCATS guide
Behavior cloning loss coefficient Œ∫ 0.5 Logistic Linear Linear Logistic
SCATS guide SCATS
Weight of Green-light utilization rate in Reward w 1
gr
Weight of Green imbalance in Reward w -1
gi
Weight of Throughput in Reward w v 4e-2 Fig. 6: Training process under BC: first linear guide, then
Weight of Queue length in Reward w -1e-3
l logistic guide, and lastly SCATS guide. After introducing
the linear guide and the logistic guide, the training loss
continuously drops. When transitting to the SCATS guide,
remain indifferent to such dynamics. Synthesizing the afore- therewasaninitialincreaseinBClossduringtheswitch,but
mentioned results, it becomes evident that RL algorithms the reward still had an upward trend throughout the training
tend to outshine traditional methods in terms of quantitative process.Further,theBClosssteadilydecreased.Allofthese
evaluationscores.However,theconventionalSCATSmethod prove the effectiveness of our curriculum learning.
demonstrates superior synchronicity of cycle time and flow.
Our method, underpinned by BC guidance, excels in quanti-
tativescoringandsynchronicity,thusdeliveringanoptimized Thus,linearandlogisticguidesconstantlyhelptoreducethe
performance. BC loss further and increase the reward; when we introduce
3) Visualization of Curriculum Training Process: Fig. 6 the advanced SCATS guide, the loss and reward get worse
alsoshowsthetrainingprocessofthecurriculumlearning.As first for a while, mainly due to the SCATS guide being non-
wecanobserve,introducingthelinearandlogisticguideshas smooth, but then, the training gets immediately improved
quite similar and non-distinguishable effects on the training since SCATS is a well-tailored industrial solution for traffic
process, and the main reason is that both linear and logistic signal control, being configured perfectly for the task.
guides are smooth (in an extreme case, a logistic function 4) AblationStudies: Ablationstudiesareconductedtoas-
with a tiny slope is almost equivalent to a linear function ). sess the impact of each component within our GuidedLight.
)s(
)s(
)snim5/selcihev.oN(wolF
)snim5/selcihev.oN(wolF
)s(
)s(
)snim5/selcihev.oN(wolF
)snim5/selcihev.oN(wolF
)s(
)s(
)snim5/selcihev.oN(wolF
)snim5/selcihev.oN(wolF
)s(
)s(
)snim5/selcihev.oN(wolF
)snim5/selcihev.oN(wolF
)s(
)s(
)snim5/selcihev.oN(wolF
)snim5/selcihev.oN(wolF(a) Throughput (b) Greenlight Utilization Rate ods‚Äô applicability in the real world. Several contributions
have been made. From the model‚Äôs perspective: (1) the RL
settingisstrictlydesignedfollowingtheindustrystandardsin
terms of input, output, evaluation, and so on; (2) moreover,
to pursue the standard cycle-flow relation, we innovatively
incorporate traditional yet effective methods such as SCATS
as the guidance and use behavior cloning and curriculum
ours w/o L w/o S w/o BC ours w/o L w/o S w/o BC learning to teach the agents. From the data‚Äôs perspective,
(c) Green Imbalance (d) Queue Length our simulation scenario is strictly designed to replicate the
real-world case. We theoretically prove that the guided RL
can guarantee a polynomial sample complexity, and rigid
experiments confirm that our GuidedLight could achieve
higher performance and uphold industry requirements.
Future work: we will deploy the GuildedLight model in
real cities and consider utilizing multi-agent reinforcement
ours w/o L w/o S w/o BC ours w/o L w/o S w/o BC learning to facilitate collaborative coordination among mul-
tiple intersections, further improving traffic efficiency while
Fig. 7: Ablation study. Except for the fact that our solution
ensuring alignment with industry needs.
is slightly worse than without BC in Green-light utilization
rate, our solution has achieved the best results in another 3
metrics, proving the effectiveness of our design. REFERENCES
[1] W. Ketter, K. Schroer, and K. Valogianni, ‚ÄúInformation systems
As shown in Fig. 7, ‚Äúw/o BC‚Äù denotes the model without research for smart sustainable mobility: A framework and call for
action,‚ÄùInformationSystemsResearch,vol.34,no.3,pp.1045‚Äì1065,
guidance at all, ‚Äúw/o L‚Äù is without guidance from the linear
2023.
andlogisticmodel(sincethetwohavesimilareffects,sowe [2] E.VanderPolandF.A.Oliehoek,‚ÄúCoordinateddeepreinforcement
do not distinguish them), and ‚Äúw/o S‚Äù is without guidance learners for traffic light control,‚Äù Proceedings of Learning, Inference
andControlofMulti-agentSystems(atNIPS2016),vol.1,2016.
from SCATS.
[3] S. S. Mousavi, M. Schukat, and E. Howley, ‚ÄúTraffic light control
FromFig.7(a)-(b),theentry-levelguide,thelinearmodel, using deep policy-gradient and value-function-based reinforcement
seems to be more critical to achieving higher throughput learning,‚ÄùIETIntelligentTransportSystems,vol.11,no.7,pp.417‚Äì
and green utilization rate. It may be because the linear 423,2017.
[4] H. Wei, G. Zheng, H. Yao, and Z. Li, ‚ÄúIntellilight: A reinforcement
model follows a straight rule that more cars need longer
learningapproachforintelligenttrafficlightcontrol,‚ÄùinProceedings
cycle time, thus allowing more cars to release, resulting of the 24th ACM SIGKDD International Conference on Knowledge
in higher throughput and utilization rates. From Fig. 7 (c)- Discovery&DataMining,2018,pp.2496‚Äì2505.
[5] G.Zheng,Y.Xiong,X.Zang,J.Feng,H.Wei,H.Zhang,Y.Li,K.Xu,
(d), the advanced guide, SCATS, seems to affect the green
andZ.Li,‚ÄúLearningphasecompetitionfortrafficsignalcontrol,‚Äùin
imbalance and queue length more: these two indexes are Proceedingsofthe28thACMInternationalConferenceonInformation
usually more complexly correlated with other traffic factors, andKnowledgeManagement,2019,pp.1963‚Äì1972.
[6] H.Jiang,Z.Li,L.Bai,R.Zhao,etal.,‚ÄúAgeneralscenario-agnostic
andanadvancedguideisbetter.Straightforwardly,themodel
reinforcementlearningfortrafficsignalcontrol,‚Äù2022.
without any guidance has the worst performance. [7] J. Lu, J. Ruan, H. Jiang, Z. Li, H. Mao, and R. Zhao, ‚ÄúDualight:
Enhancing traffic signal control by leveraging scenario-specific and
VI. DISCUSSION scenario-sharedknowledge,‚ÄùarXivpreprintarXiv:2312.14532,2023.
Choices of Phase Plan: This paper uses a 4-phase plan [8] P.Hunt,D.Robertson,R.Bretherton,andM.C.Royle,‚ÄúTheSCOOT
on-line traffic signal optimisation technique,‚Äù Traffic Engineering &
as an example, which covers most intersections. It can also
Control,vol.23,no.4,1982.
handle 3-phase plans by adding masks in state and action. [9] H. Jiang, Z. Li, L. Bai, Z. Li, and R. Zhao, ‚ÄúA general scenario-
For the 5-phase plan, state and action need to be extended. agnostic reinforcement learning for traffic signal control,‚Äù 2023.
[Online].Available:https://openreview.net/forum?id=RKMbC8Tslx
In addition, due to the integrated features of FRAP [5], our
[10] Y.Lou,J.Wu,andY.Ran,‚ÄúMeta-reinforcementlearningformultiple
method can also handle various types of intersections. trafficsignalscontrol,‚ÄùinProceedingsofthe31stACMInternational
Generalization to Different Intersection Structures: Conference on Information & Knowledge Management, 2022, pp.
4264‚Äì4268.
Given different intersections can have quite different struc-
[11] X. Zang, H. Yao, G. Zheng, N. Xu, K. Xu, and Z. Li, ‚ÄúMetalight:
tures, with different numbers of lanes, movements, legs, and
Value-basedmeta-reinforcementlearningfortrafficsignalcontrol,‚Äùin
shapes (each as T-shape, Y-shape), for a more generalized ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.34,
solution, we recommend using a general plug-in module no.01,2020,pp.1153‚Äì1160.
[12] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, ‚ÄúMultiagent rein-
fromGESA[9],whichmapsvariousintersectionstoaunified
forcement learning for integrated network of adaptive traffic signal
4-leg intersection. controllers (MARLIN-ATSC): Methodology and large-scale applica-
tionondowntownToronto,‚ÄùIEEETransactionsonIntelligentTrans-
VII. CONCLUSIONS portationSystems,vol.14,no.3,pp.1140‚Äì1150,2013.
[13] A.Wong,T.Ba¬®ck,A.V.Kononova,andA.Plaat,‚ÄúDeepmultiagent
ThispaperproposedGuidedLight,anRL-basedmodelfor
reinforcement learning: Challenges and directions,‚Äù Artificial Intelli-
TSC. GuidedLight is delicate to further improve RL meth- genceReview,vol.56,no.6,pp.5023‚Äì5056,2023.
eulav
eulav
eulav
eulav[14] H. Wei, C. Chen, G. Zheng, K. Wu, V. Gayah, K. Xu, and Z. Li, [37] J. D. Chang, K. Brantley, R. Ramamurthy, D. Misra, and W. Sun,
‚ÄúPresslight:Learningmaxpressurecontroltocoordinatetrafficsignals ‚ÄúLearning to generate better than your llm,‚Äù arXiv preprint
in arterial network,‚Äù in Proceedings of the 25th ACM SIGKDD arXiv:2306.11816,2023.
International Conference on Knowledge Discovery & Data Mining, [38] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
2019,pp.1290‚Äì1298. ‚ÄúProximal policy optimization algorithms,‚Äù arXiv preprint
[15] J.Ruan,Z.Li,H.Wei,H.Jiang,J.Lu,X.Xiong,H.Mao,andR.Zhao, arXiv:1707.06347,2017.
‚ÄúCoslight: Co-optimizing collaborator selection and decision-making [39] S. Koenig and R. G. Simmons, ‚ÄúComplexity analysis of real-time
to enhance traffic signal control,‚Äù arXiv preprint arXiv:2405.17152, reinforcementlearning,‚ÄùinAAAI,vol.93,1993,pp.99‚Äì105.
2024. [40] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan, ‚ÄúOn the
[16] H. Jiang, Z. Li, H. Wei, X. Xiong, J. Ruan, J. Lu, H. Mao, and theory of policy gradient methods: Optimality, approximation, and
R. Zhao, ‚ÄúX-light: Cross-city traffic signal control using transformer distributionshift,‚ÄùTheJournalofMachineLearningResearch,vol.22,
on transformer as meta multi-agent reinforcement learner,‚Äù arXiv no.1,pp.4431‚Äì4506,2021.
preprintarXiv:2404.12090,2024. [41] C.TekinandM.VanDerSchaar,‚ÄúDistributedonlinelearningviaco-
[17] X.Du,Z.Li,C.Long,Y.Xing,S.Y.Philip,andH.Chen,‚ÄúFelight: operativecontextualbandits,‚ÄùIEEEtransactionsonsignalprocessing,
Fairness-awaretrafficsignalcontrolviasample-efficientreinforcement vol.63,no.14,pp.3700‚Äì3714,2015.
learning,‚Äù IEEE Transactions on Knowledge and Data Engineering, [42] J.LangfordandT.Zhang,‚ÄúTheepoch-greedyalgorithmforcontextual
2024. multi-armed bandits,‚Äù Advances in neural information processing
[18] Z.Li,Q.Zeng,Y.Liu,J.Liu,andL.Li,‚ÄúAnimprovedtrafficlights systems,vol.20,no.1,pp.96‚Äì1,2007.
recognitionalgorithmforautonomousdrivingincomplexscenarios,‚Äù [43] D. Simchi-Levi and Y. Xu, ‚ÄúBypassing the monster: A faster and
InternationalJournalofDistributedSensorNetworks,vol.17,no.5, simpler optimal algorithm for contextual bandits under realizability,‚Äù
p.15501477211018374,2021. MathematicsofOperationsResearch,vol.47,no.3,pp.1904‚Äì1931,
2022.
[19] H. C. Manual, ‚ÄúHighway capacity manual,‚Äù Washington, DC, vol. 2,
[44] A.Krishnamurthy,J.Langford,A.Slivkins,andC.Zhang,‚ÄúContextual
no.1,2000.
banditswithcontinuousactions:Smoothing,zooming,andadapting,‚Äù
[20] P. Lloyd, ‚ÄúAmerican, german and british antecedents to pearl and
TheJournalofMachineLearningResearch,vol.21,no.1,pp.5402‚Äì
reed‚Äôslogisticcurve,‚ÄùPopulationStudies,vol.21,no.2,pp.99‚Äì108,
5446,2020.
1967.
[45] S.KakadeandJ.Langford,‚ÄúApproximatelyoptimalapproximatere-
[21] P. Lowrie, ‚ÄúSCATS, Sydney co-ordinated adaptive traffic system: A
inforcementlearning,‚ÄùinProceedingsoftheNineteenthInternational
trafficresponsivemethodofcontrollingurbantraffic,‚Äù1990.
ConferenceonMachineLearning,2002,pp.267‚Äì274.
[22] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
[46] I. Uchendu,T. Xiao,Y. Lu, B.Zhu, M.Yan, J. Simon,M. Bennice,
D.Silver,andD.Wierstra,‚ÄúContinuouscontrolwithdeepreinforce-
C. Fu, C. Ma, J. Jiao, et al., ‚ÄúJump-start reinforcement learning,‚Äù in
mentlearning,‚ÄùarXivpreprintarXiv:1509.02971,2015.
International Conference on Machine Learning. PMLR, 2023, pp.
[23] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and 34556‚Äì34583.
P.Stone,‚ÄúCurriculumlearningforreinforcementlearningdomains:A
frameworkandsurvey,‚ÄùTheJournalofMachineLearningResearch,
vol.21,no.1,pp.7382‚Äì7431,2020.
[24] R. P. Roess, E. S. Prassas, and W. R. McShane, Traffic engineering.
Pearson/PrenticeHall,2004.
[25] M. Fellendorf, ‚ÄúVISSIM: A microscopic simulation tool to evaluate
actuated signal control including bus priority,‚Äù in 64th Institute of
TransportationEngineersAnnualMeeting,vol.32. Springer,1994,
pp.1‚Äì9.
[26] P.MirchandaniandL.Head,‚ÄúAreal-timetrafficsignalcontrolsystem:
architecture, algorithms, and analysis,‚Äù Transportation Research Part
C:EmergingTechnologies,vol.9,no.6,pp.415‚Äì432,2001.
[27] P.KoonceandL.Rodegerdts,‚ÄúTrafficsignaltimingmanual.‚ÄùUnited
States.FederalHighwayAdministration,Tech.Rep.,2008.
[28] K.-L.A.Yau,J.Qadir,H.L.Khoo,M.H.Ling,andP.Komisarczuk,
‚ÄúAsurveyonreinforcementlearningmodelsandalgorithmsfortraffic
signalcontrol,‚ÄùACMComputingSurveys(CSUR),vol.50,no.3,pp.
1‚Äì38,2017.
[29] H. Wei, G. Zheng, V. Gayah, and Z. Li, ‚ÄúA survey on traffic signal
controlmethods,‚ÄùarXivpreprintarXiv:1904.08117,2019.
[30] L.PrashanthandS.Bhatnagar,‚ÄúReinforcementlearningwithaverage
costforadaptivecontroloftrafficlightsatintersections,‚Äùin201114th
InternationalIEEEConferenceonIntelligentTransportationSystems
(ITSC). IEEE,2011,pp.1640‚Äì1645.
[31] L.-H. Xu, X.-H. Xia, and Q. Luo, ‚ÄúThe study of reinforcement
learningfortrafficself-adaptivecontrolundermultiagentmarkovgame
environment,‚Äù Mathematical Problems in Engineering, vol. 2013,
2013.
[32] J.Gao,Y.Shen,J.Liu,M.Ito,andN.Shiratori,‚ÄúAdaptivetrafficsignal
control:Deepreinforcementlearningalgorithmwithexperiencereplay
andtargetnetwork,‚ÄùarXivpreprintarXiv:1705.02755,2017.
[33] N. Casas, ‚ÄúDeep deterministic policy gradient for urban traffic light
control,‚ÄùarXivpreprintarXiv:1703.09035,2017.
[34] B. Xu, Y. Wang, Z. Wang, H. Jia, and Z. Lu, ‚ÄúHierarchically and
cooperatively learning traffic signal control,‚Äù in Proceedings of the
AAAI Conference on Artificial Intelligence, vol. 35, no. 1, 2021, pp.
669‚Äì677.
[35] X.Liang,X.Du,G.Wang,andZ.Han,‚ÄúAdeepreinforcementlearning
networkfortrafficlightcyclecontrol,‚ÄùIEEETransactionsonVehicular
Technology,vol.68,no.2,pp.1243‚Äì1253,2019.
[36] S.HochreiterandJ.Schmidhuber,‚ÄúLongshort-termmemory,‚ÄùNeural
computation,vol.9,no.8,pp.1735‚Äì1780,1997.