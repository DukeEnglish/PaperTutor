BECAUSE:
Bilinear Causal Representation for Generalizable
Offline Model-based Reinforcement Learning
HaohongLin1,WenhaoDing1,JianChen1,LaixiShi2,JiachengZhu3,BoLi4,DingZhao1
1CMU, 2Caltech, 3MIT, 4UChicago&UIUC
{haohongl, wenhaod, dingzhao}@andrew.cmu.edu
Abstract
Offlinemodel-basedreinforcementlearning(MBRL)enhancesdataefficiencyby
utilizingpre-collecteddatasetstolearnmodelsandpolicies,especiallyinscenarios
where exploration is costly or infeasible. Nevertheless, its performance often
suffersfromtheobjectivemismatchbetweenmodelandpolicylearning,resultingin
inferiorperformancedespiteaccuratemodelpredictions. Thispaperfirstidentifies
the primary source of this mismatch comes from the underlying confounders
presentinofflinedataforMBRL.Subsequently,weintroduceBilinEarCAUSal
rEpresentation(BECAUSE),analgorithmtocapturecausalrepresentationforboth
statesandactionstoreducetheinfluenceofthedistributionshift,thusmitigating
theobjectivemismatchproblem. Comprehensiveevaluationson18tasksthatvary
indataqualityandenvironmentcontextdemonstratethesuperiorperformanceof
BECAUSEoverexistingofflineRLalgorithms. Weshowthegeneralizabilityand
robustnessofBECAUSEunderfewersamplesorlargernumbersofconfounders.
Additionally,weoffertheoreticalanalysisofBECAUSEtoproveitserrorbound
andsampleefficiencywhenintegratingcausalrepresentationintoofflineMBRL.
1 Introduction
OfflineReinforcementLearning(RL)hasshowngreatpromiseinlearningdirectlyfromhistorically
collected datasets, especially in scenarios where active interaction is expensive or infeasible [1].
Specifically,offlinemodel-basedreinforcementlearning(MBRL)[2,3,4],learningpolicieswithan
estimatedworldmodel,generallyperformbetterthantheirmodel-freecounterpartsinlong-horizon
taskssuchasself-drivingvehicles[5],robotics[6],andhealthcare[7]. However,offlineRLsuffers
fromdistributionshiftsincetherolloutdatacouldsamplefromsomeunknownbehaviorpoliciesthat
aresub-optimalorfromslightlydifferentenvironmentscomparedtothedeploymenttime[8].
Althoughidentifyingdistributionshiftissues,many Datacollectionwithbehavior policy
ofthecurrentofflineMBRLworksfailtomodelthe (Unknown)
shiftinenvironmentdynamics,whichisubiquitous DataBuffer Model Policy test Environment
andcouldcausecatastrophicfailureoftrainedpolicy {𝑠,𝑎,𝑠!,𝑟} 𝑇((𝑠!|𝑠,𝑎) 𝜋(𝑎|𝑠) 𝑇(𝑠′|𝑠,𝑎)
ataslightlydifferentdeploymentstage. Furthermore,
Objective Mismatch
sincethelearningobjectivesoftheworldmodelsand
lowmodelloss≠ success lowmodelloss≈ success
policies are isolated from each other, a significant Success Ourmethod
challengeinofflineMBRLisobjectivemismatch[9, Failure
10]problem(showninFigure1):modelsthatachieve Model Loss Model Loss
a lower training loss are not necessarily better for
controlperformance. Forexample,inlong-horizon Figure1: Theobjectivemismatchproblem.
planningtasks,therewardissparseyetthepredictionaccuracyofthemodelmaydecayasthehorizon
Preprint.Underreview.
4202
luJ
51
]GL.sc[
1v76901.7042:viXraenlargesandcompoundingerroraccumulates. Previousworks[9,10]haveattemptedtoreducesuch
objectivemismatchbyjointlylearningthemodelandpolicy. However,theperformanceissuboptimal
intheabsenceoftheunderlyingcauseofobjectivemismatch[8].
Inthiswork,weidentifythattheobjectivemismatchbetweenmodelestimationandpolicylearning
comesfromtwosourcesofdistributionshiftinofflineMBRL:(1)shiftbetweentheonlineoptimal
policyandofflinesub-optimalbehaviorpolicies,and(2)shiftbetweenthedatacollectionenvironment
andonlinetestingenvironments. Unlikehumans,whomakedecisionsbasedonreasoningovertask-
relevant factors, models in offline RL memorize correlations without learning the causality. The
sub-optimalbehaviorpoliciesintroducespuriouscorrelations[11]betweenactionsandstates,making
themodelmemorizespecificactions.Whenonlinetestingenvironmentsdifferfromthedatacollection
environment,themodelcouldoverfitspuriouscorrelationsinthestateandfailtogeneralizetounseen
states. Basedontheanalysisoftheabovemismatch,ourworkdiffersfrompreviousworkofcausal
model-basedRL[12,13]inthatwemodelcausalityinbothmodelandpolicylearning. Weaimto
avoidspuriouscorrelationbydiscoveringunderlyingstructuresbetweenabstractedstatesandactions.
Toalleviateobjectivemismatchandgeneralizewell,weintroducetheBilinEarCAUSalrEpresenta-
tion(BECAUSE)thatintegratesthecausalrepresentationinbothworldmodellearningandplanning
ofMBRLagents. InspiredbypreliminaryworksthatusebilinearMDPstocapturethestructural
representationinMBRL[14],wefirstapproximatethecausalrepresentationtocapturethelow-rank
structureintheworldmodel,thenusethislearnedrepresentationtofacilitateplanningbyquantifying
theuncertaintyofsampledtransitionpairs. Consequently,wefactorizethespuriouscorrelationsand
learnaunifiedrepresentationforboththeworldmodelandplanner.
Insummary,thecontributionofthispaperisthreefold:
• WeformulateofflineMBRLintothecausalrepresentationlearningproblem,highlightingthetight
connection between structural causal models and low-rank structures in MDPs. To the best of
ourknowledge, thisisthefirstworkthatsystematicallyrevealstheconnectionbetweencausal
representationlearningandBilinearMDPs.
• WeproposeBECAUSE,anempiricalcausalrepresentationframework,basedontheaboveformu-
lation. BECAUSEfirstlearnsacausalworldmodel,thenfostersthegeneralizabilityofofflineRL
agentsbyquantifyingtheuncertaintyofthestatetransition,whichfacilitatesconservativeplanning
tomitigatetheobjectivemismatch.
• Weprovideextensiveempiricalstudiesandperformanceanalysisintasksofmultipledomainsto
demonstratethesuperiorityofBECAUSEoverexistingbaselines,whichillustratesitspotentialto
improvethegeneralizabilityandrobustnessofofflineMBRLalgorithms.
2 ProblemFormulation
Toalleviatetheobjectivemismatchproblemandthedegradedperformancecausedbythespurious
correlation,wefirstprovideournovelformulationoflearningtheunderlyingcausalstructuresof
MarkovDecisionProcess(MDP)underthebilinearMDPsetting,thenintroducethecausaldiscovery
forMDPwithconfounders.
2.1 Preliminary: MDPandBilnearMDP
(cid:8) (cid:9)
Wedenoteanepisodicfinite-horizonMDPbyM = S,A,T,H,r ,whichiscomposedofstate
spaceS,actionspaceA,asetoftransitionfunctionsT,planninghorizonH andrewardfunctionr
associatedwithtaskpreferences. Withoutlossofgeneralityinmanyreal-worldpractices,weassume
thattherewardfunctionisboundedbyr ∈ [0,1],∀h ∈ [H]. Specifically,weareinterestedina
h
goal-conditionedrewardsetting,where∀g ∈S,r(s,a;g)=1ifandonlyifs=g.
Given a policy π and the state-action pair (s,a) ∈ S ×A, we then define the state-action value
(cid:104) (cid:105)
functioninthetimestephasQπ(s,a)=E (cid:80)H r (s ,a )|s =s,a =a ,andthevaluefunc-
h π i=h i i i h h
(cid:104) (cid:105)
tionVπ(s)=E (cid:80)H r (s ,a )|s =s . TheexpectationE hereisintegratedintorandomness
h π i=h i i i h π
throughoutthetrajectory,whichisessentiallyinducedbytherandomactionofthepolicya ∼π(·|s )
i i
andthetime-homogeneoustransitiondynamicsoftheenvironments ∼T(·|s ,a ),∀i∈[h,H].
i+1 i i
Intheofflinedataset,thedatarolloutscanbeseenasgeneratedbysome(mixed)behaviorpolicyπ ,
β
resultinginadatasetDwithintotalnsamples{s ,a ,s′,r } .
i i i i 1≤i≤n
2Definition1(BilinearMDP[14]). Foreach(s,a) ∈ S ×A,s′ ∈ S, wehavethecorresponding
featurevectorϕ(·,·):R|S|×R|A| →Rd,µ(·):R|S| →Rd′. WithsomecorematrixM ∈Rd×d′,
wecanrepresentthetransitionfunctionkernelT(·|·,·)as
∀s,a,s′ ∈S×A×S, T(s′|s,a)=ϕ(s,a)TMµ(s′), (1)
whereϕ(s,a)andµ(s′)areembeddingfunctionsthatmaptheoriginalstateandactiontothelatent
space,M isthecorematrixthatmodelsthetransitionrelationshipbetweentheprevioustimestep
andnexttimestepinthelatentspace. Suchalineardecompositioninthetransitiondynamicsallows
ustoembedstructuresofthetransitionmodelwithoutthelossofgeneralfunctionapproximation
capabilitiestoderivestateandactionrepresentations.
2.2 ActionStateConfoundedMDP
We consider the existence of confounders in Causal Path State Nodes Confounders
the MDP to represent the offline data collec- Confounder Path Action Nodes
tionprocess,anddefineaction-stateconfounded 𝑠 𝑠′ 𝑢 𝑢 !
MDP(ASC-MDP):
𝑠 𝑠′ 𝑠 𝑠′
Definition2(ASC-MDP). Besidesthecompo-
(cid:8) (cid:9)
nentsinstandardMDPsM= S,A,T,H,r ,
𝑎 𝑢 𝑎 𝑢 𝑎 𝑢 ′
" !
weintroduceasetofunobservedconfounders
u. InASP-MDP,confoundersarefactorizedas (a) Confounded MDP (b) SC-MDP (c) ASC-MDP (Ours)
u = {u ,u } , where u ∈ U denotes Figure2: ComparisonofourASC-MDPwithtwoex-
π c 1≤h≤H π
the confounders between s and a ∼ π (s) in- istingformulations.
β
ducedbybehaviorpolicies,andu ∈U denotestheconfounderswithinthestate-actionpairsofthe
c
environmenttransition,thatis,theinherentstructurebetween(s,a)ands′. Hereweassumeatime-
invariantconfounderdistributionu∼P (·),∀h∈[H],whichisacommonassumption[15,16,17]
u
The resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from
theoriginalMDP,ASC-MDPisdifferentfromtheConfoundedMDP[18]andState-Confounded
MDP(SC-MDP)[19]inthatitmodelsboththespuriouscorrelationbetweenthecurrentstatesand
thecurrentactiona,aswellasthosebetweenthenextstates′and(s,a). Yet,confoundedMDPand
SC-MDPonlymodelpartofthepossibleconfoundersbetweenstatesandactions. Thefactorization
oftheconfounderinASC-MDPalignswiththesourceofspuriouscorrelationinofflineMBRL.
3 ProposedMethod: BECAUSE
We propose BECAUSE, our core methodology for modeling, learning, and applying our causal
representations for generalizable offline MBRL. Section 3.1 models the basic format of causal
representationsandanalyzestheirproperties. Section3.2givesacompactwaytolearnthecausal
representationϕ(s,a)andµ(s′),aswellasthecoremaskestimationM. Section3.3utilizesthese
learnedcausalrepresentationsinbothworldmodellearningandMBRLplanningfromofflinedatasets.
3.1 CausalRepresentationforASC-MDP
Inthepresenceofahiddenconfounderu,wemodeltheconfounderbehindthetransitiondynamics
asalinearconfoundedMDP[18]:
T(s′|s,a,u)=ϕ(cid:101)(s,a,u)Tµ(s′), (2)
whereu ∼ P u(·). InspiredbytheBilinearMDPinDefinition1, wedecomposeϕ(cid:101)(s,a,u)intoa
confounder-awarecorematrixM(u)andafeaturemappingϕ(s,a),whichfactorizetheinfluenceof
theconfounders. Giventhefactorizationofconfounderu={u ,u }inDefinition2,wederivevia
c π
d-separationinthegraphicalmodelinFigure2thats′ ⊥⊥u |{s,a,u }. Asaresult,weonlyneedto
π c
considertheconfounderu fromtheenvironmentwhendecomposingthetransitionmodel:
c
T(s′|s,a,u)=T(s′|s,a,u )=ϕ(s,a)TM(u )µ(s′). (3)
c c
(cid:20) 0d×d M (cid:21)
Definition3(ConstructionofcausalgraphG). InASC-MDP,G= . forall(sparse)
0d′×d 0d′×d′
corematrixM,thecausalgraphGisbipartite,thus∀G,G∈DAG.
3Definition3revealstheconnectionbetweenthecorematrixM andcausalgraphG,asisformulated
in the ASC-MDP. To reduce the influence of u and estimate the unconfounded transition model
T(s′|s,a),onewayistoidentifythecausalstructuresinducedbytheconfounderu forthetransition
c
dynamics[20]. Existingmethodsindifferentiablecausaldiscovery[21,22,23]transformcausal
discoveryonsomecausalgraphG,intoamaximumlikelihoodestimation(MLE)withregularization:
G(cid:98) =argmaxlogp(D;ϕ,µ,G)−λ|G| =⇒ M(cid:99)=argmaxlogp(D;ϕ,µ,M)−λ|M|, (4)
G∈DAG M
Sinceinourcase,M isasub-matrixofthecausalgraphG. GiventheDefinition3,Gautomatically
satisfies the formulation of ASC-MDP, thus discovering G is essentially estimating the sparse
submatrixM withoutDAGconstraints: M(cid:99)=argmax logp(D;ϕ,µ,M)−λ|M|. Weelaborate
M
Definition3andshowtherelationshipbetweencorematrixM andcausalgraphGinAppendixA.2.
WealsomaketheassumptionthatthesparseGandM remaininvariantwithdifferentenvironment
confoundersintheofflinetrainingandonlinetesting.
Assumption1(Invariantcausalgraph). WedenotethecausalgraphGunderconfounderuasG(u).
ThegeneralizationproblemthatweaimtosolvesatisfiestheinvarianceinthecausalgraphG(u ),
c
whereG(u )=G(u′),M(u )=M(u′),u andu′ aretheconfoundersintrainingandtesting.
c c c c c c
Remark1. Theassumption1canalsobeinterpretedastaskindependencein [24],invariantstate
representation[25],orinvariantactioneffectin[26].
3.2 LearningCausalRepresentationfromOfflineData
WefirstlearnthecausalworldmodelT(s′|s,a) Uncertainty
start
···
inthepresenceofconfoundersuintheoffline
datasets. AsformulatedinASC-MDP2,there Breakcorrelation Planning … ···
are two sets of confounders: u and u . To ··· goal
π c 𝑠,𝑎
estimateanunconfoundedtransitionmodeland 𝜙(⋅,⋅)
removetheeffectofconfounder,wefirstremove
𝑢
theimpactofu cwhichcomesfromthedynam- O Buffl fi fn ee
r
𝑀)
ics shift by estimating a batch-wise transition 𝑇"(𝑠!|𝑠,𝑎) 𝐸!(s,a)
matrixM(u ),thenweapplyareweightingfor- 𝑠′ World Model UncertaintyQuantifier
c 𝜇(⋅)
mulatodeconfoundu inducedbythebehavior
π
policiesandmitigatethemodelobjectivemis- Figure 3: BECAUSElearnsacausality-awarerepre-
match. sentationfromthebufferandusesitinboththeworld
model and uncertainty quantification to obtain a pes-
As discussed in Definition 3, we only need to
simisticplanningpolicy.
optimizethepartoftheparametersofthecausal
graph G, i.e. M. Thus, we can remove the constraints in (4), then transform the original causal
discoveryproblemintoaregularizedMLEproblemasfollows:
minL (M)=min(−logp(D;ϕ,µ,M)+λ|M|)
mask
M M
=min(cid:0)E ∥µT(s′)K−1−ϕT(s,a)M∥2 + λ∥M∥ (cid:1) . (5)
(s,a,s′)∈D µ 2 0
M (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
WorldModelLearning SparsityRegularization
whereK :=(cid:80) µ(s′)µ(s′)T isaninvertiblematrix. Thederivationofequation(5)iselaborated
µ s′∈S
in Appendix A.3. In practice, we use the χ2-test for discrete state and action space and the fast
ConditionalIndependentTest(CIT)[27]forcontinuousvariablestoestimateeachentryinthecore
matrixM. WeregularizethesparsityofM bycontrollingthep-valuethresholdinCITandprovidea
moredetailedimplementationinAppendixC.1.
Estimatingthecoremaskprovidesamoreaccuraterelationshipbetweenstateandactionrepresen-
tations,andwefurtherrefinethestateactionrepresentationfunctionϕandµtohelpcapturemore
accuratetransitiondynamics. Weoptimizethembysolvingthefollowingproblem,accordingtothe
transitionmodellossandspectralnormregularization[28]tosatisfytheregularityconstraintsofthe
featureinAssumption3:
minL rep(ϕ,µ)=minE (s,a,s′)∈D ∥µT(s′)K µ−1−ϕT(s,a)M∥2 2+λ ϕ∥ϕ∥ 2+λ µ∥µ∥ 2. (6)
ϕ,µ ϕ,µ
TheworldmodellearningprocessisillustratedinFigure3. Theestimationofindividual M(u )
c
mitigatesthespuriouscorrelationbroughtbyu . Tofurtherdealwiththespuriouscorrelationin
c
4u inducedbythebehaviorpolicyπ (a|s,u ),weutilizetheconditionalindependencepropertyin
π β π
theASC-MDPshowninequation(3). Thefollowingequationshowsthatthetrueunconfounded
transitionT canberewritteninthereweightingformulas. ThisreweightingprocessinmaskM serves
asthesoftinterventionapproach[18,29]toestimatethetreatmenteffectinthetransitionfunctionT
inanunconfoundedway:
E [ϕ(s,a)TM(u )µ(s′)·π (a|s,u )]
T(s′|s,a)= pu c β π
E π (a|s,u )
pu β π
(7)
(cid:20)E
[M(u )π (a|s,u
)](cid:21)
=ϕ(s,a)T pu c β π µ(s′)≜ϕ(s,a)T M(u)µ(s′).
E π (a|s,u )
pu β π
The derivation of equation (7) is illustrated in Appendix A.4. equation (7) basically shows
a re-weighting process given the empirical estimation of M(u ) in every batch of trajectories:
c
M(u)= E pu[M(uc)πβ(a|s,uπ)]. ComparedtothegeneralreweightingstrategiesinpreviousMBRL
E puπβ(a|s,uπ)
literatures[18,29]whichreweightstheentirevaluefunction,thisre-weightingprocessisconducted
onlyontheestimatedmatrix,whiletherepresentationϕ(s,a)andµ(s′)aresubsequentlyregularized
by weighted estimation of M. The pipeline of causal world model learning is described in the
first part of the Algorithm 1. We discuss more details of the implementation and experiment in
AppendixC.
3.3 CausalRepresentationforUncertaintyQuantification
ToavoidenteringOODstatesinthe
Algorithm1:BECAUSETrainingandPlanning
onlinedeployment,wefurtherdesign
Input: OfflinedatasetD,causaldiscoveryfrequencyk
apessimisticplanneraccordingtothe
uncertaintyofthepredictedtrajecto- Output: CausalmaskM,featurefunctionϕ(cid:98),µ (cid:98),policyπ (cid:98)
riesintheimaginationrolloutstepto // Causal world model learning
mitigateobjectivemismatch. M ←[1]d′×d
0
fori∈[K]do
We use the feature embedding from
bilinearcausalrepresentationtohelp Updateϕ(cid:98)n,µ (cid:98)nbyL rep(ϕ,µ)in(6)
quantify the uncertainty, denoted as ifi mod k=0then
E θ(s,a). As we have access to the UpdateM(cid:99)nwithL mask(M)in(5)
offline dataset, we learn an Energy- WeightedaverageM with(7)
n
based Model (EBM) [30, 31] based // Uncertainty quantifier learning
ontheabstractedstaterepresentation FitE (s,a)withL (8)
θ EBM
ϕandcorematrixM. Ahigherout-
InitializeV(cid:98)H+1(s)=0,∀(s,a)
putoftheenergyfunctionE (·,·)in-
θ // Pessimistic planning
dicatesahigheruncertaintyinthecur-
whileh<H do
rentstateastheyarevisitedbythebe-
EstimatetheuncertaintywithscoreE (s,a)
θ
haviorpoliciesπ lessfrequently. In
β ComputeQ (s,a)with(9)
practice,theenergy-basedmodelusu- h
allysuffersfromahigh-dimensional
V(cid:98)h(s,a)=max aQ(s,a)
dataspace[32]. Tomitigatethisover- a←argmax aQ(s,a)
head of training a good uncertainty s′,r ←env.step(a,g)
quantifier, we first embed the state
samplesthroughtheabstractrepresentationµ(s′),andthestateactionpairviaϕ(s,a).
L (θ)=E E [µ(s+)|ϕ(s,a)]−E E [µ(s−)|ϕ(s,a)]+λ ∥θ∥ , (8)
EBM T(cid:98)(·|s,a) θ q(s,a) θ EBM 2
where µ(s)+ refers to the positive samples from the approximated transition dynamics T(cid:98)(·|s,a),
andµ(s−)referstothelatentnegativesamplesviatheLangevindynamics[30]. Additionally,we
regularizetheparametersofEBMtoavoidoverfittingissues. Weattachmoretrainingdetailsand
results of EBMs in Appendix C.2 The learned energy function E (s,a) is used to quantify the
θ
uncertaintybasedontheofflinedata.
Duringtheonlineplanningstage,weusethelearnedEBMtoadjusttherewardestimationbasedon
ModelPredictiveControl(MPC)[33]. Attimesteph,webasicallysubtracttheoriginalstepreturn
estimationr (s,a)byitsuncertaintyE (s,a):
h θ
(cid:88)
Q h(s,a)=Q(cid:98)h(s,a)−E θ(s,a)=r h(s,a)−E θ(s,a)+ T(cid:98)(s′|s,a)V(cid:98)h+1(s′).
(9)
(cid:124) (cid:123)(cid:122) (cid:125)
s′∈S
AdjustedReturn
53.4 TheoreticalAnalysisofBECAUSE
ThenwemoveontodevelopthetheoreticalanalysisfortheproposedmethodBECAUSE.Based
on two standard Assumption 2 and 3 on the feature’s existence and regularity, we achieve the
finite-samplecomplexityguarantee—anupperboundofthesuboptimalitygapasfollows,whose
proofispostponedtoAppendixB.
Theorem1(Performanceguarantee). Considerany0<δ <1andanyinitialstates∈S. Under
(cid:101)
theAssumption2, 3andthatthetransitionmodelT isanSCM(definedin4),foranyaccuracylevel
0≤ξ ≤1,withprobabilityatleast1−δ,theoutputpolicyπofBECAUSE(Algorithm1)basedon
(cid:80)
thehistoricaldatasetDwithn= n(s,a)samplesgeneratedfromabehaviorpolicyπ
(s,a)∈S×A β
satisfies:
H (cid:34)(cid:115) (cid:35)
V∗(s)−Vπ(s)≲min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9)(cid:88) E log(1/δ) |s =s ,
1 (cid:101) 1 (cid:101) 1 ξ s 0 π∗ n(s ,a ) 1 (cid:101)
h h
h=1
whereC ,C aresomeuniversalconstants,σisSCM’snoiselevel(seeDefinition4),andM ∈Rd×d′
1 s
istheoptimalgroundtruthsparsetransitionmatrixtobeestimated.
Theerrorboundshrinksastheofflinesamplesizenoverallstate-actionpairsincrease. Italsogrows
proportionallytotheplanninghorizonH,SCM’snoiselevelσ,andtheℓ normofthegroundtrue
0
causalmaskM,whichdescribestheintrinsiccomplexityoftheworldmodel.
Consequently,withProposition1intheAppendix,wecanachieveξ-optimalpolicy(V∗(s)−Vπ(s)≤
1 (cid:101) 1 (cid:101)
ξ)aslongasthehistoricaldatasetsatisfiesthefollowingconditions: ∀0≤ξ ≤1,
min(cid:8) C2log2(cid:0)∥M∥0(cid:1) |S|,C2σ2∥M∥ (cid:9) ·H2log(1/δ)
min E (cid:2) n(s ,a )|s =s(cid:3)≳ 1 ξ s 0 .
(s,a,h)∈S×A×[H] π⋆ h h 1 (cid:101) ξ2
4 ExperimentResults
Inthissection, weconductacomprehensiveempiricalevaluationofBECAUSE’sgeneralization
performanceinadiversesetofenvironments,coveringdifferentdecision-makingproblemsinthe
gridworld,manipulation,andautonomousdrivingdomains,showninAppendixFigure8.
4.1 ExperimentSetting
EnvironmentDesign Wedesign18tasksin3representativeRLenvironments. Agentsneedto
acquirereasoningcapabilitiestoreceivehigherrewardsandachievegoals.
• Lift: ObjectmanipulationenvironmentinRoboSuite[34]. Wedesignedthisenvironmentforthe
agenttoliftanobjectwithaspecificcolorconfigurationonthetabletoadesiredheight. Inthe
OODenvironmentLift-O,thereisaninjectedspuriouscorrelationbetweenthecolorofthecube
andthepositionofthecubeinthetrainingphase. Duringthetestingphase,thecorrelationbetween
colorandpositionisdifferentfromtraining.
• Unlock: WedesignedthisenvironmentfortheagenttocollectakeytoopendoorsinMinigrid[35].
IntheOODenvironmentUnlock-O,therewillbeadifferentnumberofgoals(doorstobeopened)
inthetestingenvironmentsfromthetrainingenvironments.
• Crash: Safety is critical in autonomous driving, which is reflected by the collision avoidance
capability. WeconsiderariskyscenariowhereanAVcollideswithajaywalkerbecauseitsviewis
blockedbyanothercar[36]. Wedesignsuchacrashscenariobasedonhighway-env[37],where
thegoalistocreatecrashesbetweenapedestrianandAVs. IntheOODenvironmentCrash-O,the
distributionofreward(numberofpedestrians)isdifferentinonlinetestingenvironments.
Forallthreedifferentenvironments,wesetaspecificsubsetofthestatespaceasthegoalg ∈S,and
therewardisdefinedasthegoal-reachingrewardr(s,a,g)=I(r =g). Whentheepisodeendsin
thegoalstatewithinthetaskhorizonH,theepisodeisconsideredasuccess. Wethenusetheaverage
successrateasthegeneralevaluationmetricsforourBECAUSEandallbaselines.
Ineachenvironment,wecollectthreetypesofofflinedata: random,medium,andexpertbasedon
thedifferentlevelsofu inthebehaviorpolicies. InUnlockenvironments,wecollect200episodes
π
6Lift-Random Lift-Medium Lift-Expert Unlock-Random Unlock-Random
Unlock-Random Unlock-Medium Unlock-Expert Unlock-Medium Unlock-Medium
Crash-Random Crash-Medium Crash-Expert Unlock-Expert Unlock-Expert
(a) (b) (c)
Figure4: ResultsofBECAUSEandbaselinesindifferenttasks. (a)Averagesuccessrateindistri-
butionandoutofdistribution. (b)Averagesuccessratew.r.t. ratioofofflinesamples. (c)Average
successratew.r.t. spuriouslevelintheenvironments. Weevaluatethemeanandstandarddeviationof
thebestperformanceamong10randomseedsandreporttask-wiseresultsinAppendixTable3.
fromeachlevelofbehaviorpoliciesastheofflinedemonstrationdata,andthenumberofepisodesis
1,000intheenvironmentsLiftandCrash,whichallhavecontinuousstateandactionspace. Amore
detailedviewoftheenvironmenthyperparametersandbehaviorpolicydesignisinAppendixC.5.
Baselines WecompareourproposedBECAUSEwithseveralofflinecausalRLorMBRLbaselines.
ICIL[38]learnsadynamic-awareinvariantcausalrepresentationlearningtoassistageneralizable
policylearningfromofflinedatasets. CCIL[39]conductsasoftinterventioninourofflinesetting
byjointlyoptimizingpolicyparametersandmasksoverthestate. MnM[9]unifiestheobjectiveof
jointlytrainingthemodelandpolicy,whichallocateslargerweightsinthestatepredictionlossinthe
high-rewardregion.Delphic[40]introducesdelphicuncertaintytodifferentiatebetweenuncertainties
causedbyhiddenconfoundersandtraditionalepistemicandaleatoricuncertainties. TD3+BC[41]
isanofflinemodel-freeRLapproachthatcombinestheTwinDelayedDeepDeterministicPolicy
Gradient (TD3) algorithm with Behavior Cloning (BC) to adopt both the actor-critic framework
andsupervisedlearningfromexpertdemonstrations. MOPO[2]isanofflineMBRLapproachthat
usesflatlatentspaceandcount-baseduncertaintyquantificationtomaintainconservatisminonline
deployment. GNN[42]isaGNN-basedbaselineusingaRelationalGraphConvolutionalNetworkto
modelthetemporaldependencyofstate-actionpairsinthedynamicmodelwithmessagepassing.
CDL[24]usescausaldiscoverytolearnatask-independentworldmodel. DenoisedMDP[12]and
IFactor[13]conductcausalstateabstractionbasedontheircontrollabilityandrewardrelevance.
Thelastthreemethodsaredesignedforonlinesettings,soweonlyimplementtheirmodellearning
objectives. WeattachmoredetailsofthebaselineimplementationinAppendixC.6.
4.2 ExperimentResultsAnalysis
Weempiricallyanswerthefollowingresearchquestions.
• RQ1:HowisthegeneralizabilityofBECAUSEintheonlineenvironments(whichmaybeunseen)?
Specifically,howdoesBECAUSEperformunderdiversequalitiesofdemonstrationdata(different
levelofu ),anddifferentenvironmentcontexts(differentu )?
π c
• RQ2: HowdoesthedesigninBECAUSEcontributetotherobustnessofitsfinalperformance
underdifferentsamplesizesorspuriouslevels?
• RQ3:HowdoesBECAUSEachievetheaforementionedgeneralizabilitybymitigatingtheobjective
mismatchprobleminofflineMBRL?
7Unlock-random Unlock-medium Unlock-expert
Negative Reward
Positive Reward
Example Policies
Figure5: Evaluationofthedifferencebetweenthedistributionofepisodicmodellossforsuccess
andfailuretrajectories. Thehigherdifferenceindicatesareductioninmodelmismatchissues. An
exampleoffailuremodeistryingtoopenthedoorwithouthavingthekey.
ForRQ1,inFigure4(a),weevaluatethesuccessrateintheonlineenvironmentagainstdifferent
baselines. Theresultshowsthatunderdifferentenvironmentsanddifferentqualitiesofbehavior
policiesπ (differentu ),BECAUSEconsistentlyachievesthebestperformancein8outof9for
β π
boththein-distribution(I)andout-of-distribution(O)forallthedemonstrationdataquality(different
levelofu ). WhereOhereindicatesthetasksunderunseenenvironmentwithconfounderu′ ̸=u
π c c
differentfromofflinetraining. Anotherfindingisthatmodel-basedapproachesgenerallyperform
betterthanmodel-freeapproachesatvariouslevelsofofflinedata,whichshowstheimportanceof
worldmodellearningforgeneralizableofflineRL.WeattachthedetailedresultsintheAppendix
Table3,4andthecausalmasksdiscoveredineachenvironmentinAppendixFigure9forreference.
ForRQ2,wecomparedifferentaspectsof Figure6: TheablationstudiesbetweenBECAUSEand
BECAUSE’srobustnesswithMOPOwith- itsvariants. WereporttheoverallSuccessrate(%)over
outcausalstructures[2]. Wecomparetheir 9in-distribution(I)and9out-of-distribution(O)tasks,
performancewithdifferentratiosoftheen- respectively. Boldisthebest.
tireofflinedatasetandillustratethesuccess
ratesinFigure4(b). Theresultshowsthat, Variants BECAUSE Optimism Linear Full
for any selected number of samples, BE-
Overall-I 73.3±4.5 64.4±6.4 57.9±6.1 39.3±6.3
CAUSEconsistentlyoutperformsMOPO
Overall-O 43.0±4.9 32.4±3.3 33.2±5.2 25.2±3.9
withaclearmargin. WealsoevaluateBE-
CAUSEperformanceathigherspuriouslevelsinFigure4(c). Weaddupto8×oftheoriginalnumber
ofconfoundersintheenvironmentstotesttherobustnessoftheagent’sperformance. BECAUSE
consistentlyoutperformsMOPOandthemarginenlargesasthespuriouslevelgrowshigher.
ForRQ3,weaimtounderstandwhetherBECAUSEachieveshigherperformancebyresolvingthe
objectivemismatchproblem. Wefirstcollecttwogroupsoftrajectories: τ andτ ,eachwith
pos neg
positive reward (success) and negative reward (failure) in Unlock task with sparse goal-reaching
reward. We want to have a model whose loss is informative for discriminating control results,
that is, we wish L (τ ) < L (τ ). According to our visualization in Figure 5, in
model pos model neg
Unlock-Expert andUnlock-Medium, theratioofτ ismuchhigherinBECAUSEthanMOPO
pos
amongthetrajectorieswithlowmodelloss. InUnlock-Random, themismatchofthemodeland
controlobjectiveismoresignificant,sincethedemonstrationispoorinstatecoverage. MOPOcannot
succeedevenwhenthemodellossislow,whereasourmethodscan. Weperformahypothesistest
withH :L (τ )<L (τ ). InBECAUSE,thisdesiredpropertyismoresignificant
0 model pos model neg
than MOPO attributed to the causal representation we learn, indicating a reduction of objective
mismatch. WeattachdetaileddiscussionsforthemismatchevaluationinAppendixC.3andTable2.
4.3 AblationStudies
We conducted ablation studies with three variants of BECAUSE and report the average success
rateacrossninein-distributionandnineout-of-distributiontasksinTable6. TheOptimismvariant
conductsoptimisticplanninginsteadofpessimisticplanninginequation(9),whichusesuniform
samplingintheplannermodule. TheLinearvariantassumesafullconnectiontothecausalmatrix
M,thendirectlyuseslinearMDPtoparameterizethedynamicsmodelT,whichremovesthecausal
discoverymoduleinBECAUSE.TheFullvariantlearnsfromthefullbatchofdatatoestimatethe
8
OPOM
ESUACEBcausalmaskwithoutiterativeupdate. Wereporttheresultsofthetask-wiseablationwithconfidence
intervalandsignificanceinAppendixTable5and6.
5 RelatedWorks
ObjectiveMismatchinMBRL TheobjectivemismatchinMBRL[43,44]referstothefactthat
pureMLEestimationoftheworldmodeldoesnotalignwellwiththecontrolobjective. Previous
works[29,45]proposereweightingduringmodeltrainingtoalleviatethismismatch, [46]proposes
agoal-awarepredictionbyredistributingmodelerroraccordingtotheirtaskrelevance. Theseworks
essentiallyreweightlossfortheentiremodeltraining, whileourworkconductsreweightingjust
overtheestimatedcausalmaskmoreefficiently. Morerecently, [9,10]proposedajointtraining
betweentheworldmodelandpolicies. Althoughjointoptimizationimprovesperformance,theydo
notaddressthegeneralizabilityofthelearnedmodelunderthedistributionshiftsetting. Intheoffline
setting,Model-basedRL[2,3,4]employsmodelensemble,pessimisticpolicyoptimizationorvalue
iteration[47,48],andanenergy-basedmodelforplanning[49]toquantifyuncertaintyandimprove
testperformance. Tothebestofourknowledge,nopreviousworkexploredormodeledtheimpactof
distributionshiftontheobjectivemismatchprobleminMBRL.
Causal Discovery with Confounder Most of the existing causal discovery methods [50] can
be categorized into constraint-based and score-based. Constraint-based methods [51] start from
a complete graph and iteratively remove edges with statistical hypothesis testing [52, 53]. This
type of method is highly data-efficient but not robust to noisy data. As a remedy, score-based
methods[54,55]usemetricssuchasthelikelihoodorBIC[56]asscorestomanipulateedgesin
the causal graph. Recently, researchers have extended score-based methods with RL [57], order
learning [58] or differentiable discovery [22, 59, 60]. To alleviate the non-identifiability under
hiddenconfounders,activeinterventionmethodshavebeenexplored[61],aimingtobreakspurious
correlationsinanonlinefashion. Withextraassumptionsonconfounders,somerecentworksdetect
suchcorrelations[62,63,64]sothatmodelscaneffectivelyidentifyelusiveconfounders.
Causal Reinforcement Learning Recently, many RL algorithms have incorporated causality
to improve reasoning capability [65] and generalizability. For instance, [66] and [67] explicitly
estimatecausalstructureswiththeinterventionaldataobtainedfromtheenvironmentinanonline
setting. These structures can be used to constrain the output space [19] or to adjust the buffer
priority[68]. Buildingdynamicmodelsinmodel-basedRL[24,69,70]basedoncausalgraphsis
widelystudied. MostexistingcausalMBRLworksfocusonestimatingthecausalworldmodelby
predicting transition dynamics and rewards. Existing methods learn this causal world model via
sparsityregularization[23,71],conditionalindependencetest[24,69,72],variationalinference[73,
12],counterfactualdataaugmentation[74,75],hierarchicalskillabstraction[76,77],uncertainty
quantification[40],rewardredistribution[24,78],causalcontextmodeling[79,80]andstructure-
awarestateabstraction[12,13,81,82]basedonthecontrollabilityandtaskorrewardrelevance.
However,thepresenceofconfoundersduringdatacollectioncanskewthelearnedpolicy,makingit
susceptibletospuriouscorrelations. Deconfoundingsolutionshavebeenproposedeitherbetween
actionsandstates[39,83,84]oramongdifferentdimensionsofstatevariables[19,85].
6 Conclusion
Inthispaper,westudyhowtomitigatetheobjectivemismatchprobleminMBRL,especiallyunder
the offline settings where distribution shift occurs. We first propose ASC-MDP and the bilinear
causalrepresentationassociatedwithit. Basedontheformulation,weproposedhowtolearnthis
causalabstractionbyalternatingbetweencausalmasklearningandfeaturelearninginfittingthe
worlddynamics. Intheplanningstage,weappliedthelearnedcausalrepresentationtoanuncertainty
quantification module based on EBM, which improves the robustness under uncertainty in the
online planning stage. We theoretically justify BECAUSE’s sub-optimality bound induced by
thesparsematrixestimationproblemandofflineRL.Comprehensiveexperimentson18different
tasksshowthatgivenadiverselevelofdemonstrationastheofflinedataset,BECAUSEhasbetter
generalizabilitythanbaselinesindifferentonlineenvironments,anditrobustlyoutperformsbaselines
underdifferentspuriouslevelsorsamplesizes. WeempiricallyshowthatBECAUSEmitigatesthe
objectivemismatchwithcausalawarenesslearnedfromofflinedata. OnelimitationofBECAUSE
liesinitssimplifiedassumptionoftime-homogeneouscausalstructure,whichmaynotalwaysholdin
long-horizonornon-stationarysettings. Besides,thecurrentimplementationisstillbasedonvector
observations. Itwillbeinterestingtoscaleupthecausalreasoningframeworkintohigh-dimensional
observationstodiscoverconceptfactorsinlong-horizonvisualRLsettings.
9References
[1] SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning:
Tutorial,review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
[2] TianheYu,GarrettThomas,LantaoYu,StefanoErmon,JamesYZou,SergeyLevine,Chelsea
Finn,andTengyuMa. Mopo: Model-basedofflinepolicyoptimization. AdvancesinNeural
InformationProcessingSystems,33:14129–14142,2020.
[3] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:
Model-basedofflinereinforcementlearning. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.
Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,
pages21810–21823.CurranAssociates,Inc.,2020.
[4] TianheYu,AviralKumar,RafaelRafailov,AravindRajeswaran,SergeyLevine,andChelsea
Finn. Combo: Conservative offline model-based policy optimization. Advances in neural
informationprocessingsystems,34:28954–28967,2021.
[5] ZeyuZhuandHuijingZhao. Asurveyofdeeprlandilforautonomousdrivingpolicylearning.
IEEETransactionsonIntelligentTransportationSystems,23(9):14043–14065,2021.
[6] AviralKumar,AnikaitSingh,FrederikEbert,MitsuhikoNakamoto,YanlaiYang,ChelseaFinn,
andSergeyLevine. Pre-trainingforrobots: Offlinerlenableslearningnewtasksfromahandful
oftrials. arXivpreprintarXiv:2210.05178,2022.
[7] ShengpuTangandJennaWiens. Modelselectionforofflinereinforcementlearning: Practical
considerationsforhealthcaresettings. InMachineLearningforHealthcareConference,pages
2–35.PMLR,2021.
[8] LaixiShiandYuejieChi. Distributionallyrobustmodel-basedofflinereinforcementlearning
withnear-optimalsamplecomplexity. arXivpreprintarXiv:2208.05767,2022.
[9] BenjaminEysenbach,AlexanderKhazatsky,SergeyLevine,andRussRSalakhutdinov. Mis-
matchednomore: Jointmodel-policyoptimizationformodel-basedrl. AdvancesinNeural
InformationProcessingSystems,35:23230–23243,2022.
[10] ShentaoYang,ShujianZhang,YihaoFeng,andMingyuanZhou. Aunifiedframeworkforalter-
natingofflinemodeltrainingandpolicylearning. AdvancesinNeuralInformationProcessing
Systems,35:17216–17232,2022.
[11] JonasPeters,DominikJanzing,andBernhardSchölkopf. Elementsofcausalinference: founda-
tionsandlearningalgorithms. TheMITPress,2017.
[12] TongzhouWang, SimonS Du, AntonioTorralba, PhillipIsola, Amy Zhang, andYuandong
Tian. Denoised mdps: Learning world models better than the world itself. arXiv preprint
arXiv:2206.15477,2022.
[13] Yu-RenLiu,BiweiHuang,ZhengmaoZhu,HonglongTian,MingmingGong,YangYu,andKun
Zhang. Learningworldmodelswithidentifiablefactorization. arXivpreprintarXiv:2306.06561,
2023.
[14] LinYangandMengdiWang. Reinforcementlearninginfeaturespace: Matrixbandit,kernels,
and regret bound. In International Conference on Machine Learning, pages 10746–10756.
PMLR,2020.
[15] WeitongZhang,JiafanHe,DongruoZhou,QGu,andAZhang.Provablyefficientrepresentation
selectioninlow-rankmarkovdecisionprocesses: fromonlinetoofflinerl. InUncertaintyin
ArtificialIntelligence,pages2488–2497.PMLR,2023.
[16] FanFengandSaraMagliacane. Learningdynamicattribute-factoredworldmodelsforefficient
multi-objectreinforcementlearning. arXivpreprintarXiv:2307.09205,2023.
[17] AngelaZhou. Reward-relevance-filteredlinearofflinereinforcementlearning. arXivpreprint
arXiv:2401.12934,2024.
[18] LingxiaoWang,ZhuoranYang,andZhaoranWang. Provablyefficientcausalreinforcement
learning with confounded observational data. Advances in Neural Information Processing
Systems,34:21164–21175,2021.
[19] WenhaoDing,LaixiShi,YuejieChi,andDingZhao. Seeingisnotbelieving: Robustreinforce-
mentlearningagainstspuriouscorrelation. arXivpreprintarXiv:2307.07907,2023.
10[20] JunzheZhangandEliasBareinboim. Markovdecisionprocesseswithunobservedconfounders:
Acausalapproach. Technicalreport,Technicalreport,TechnicalReportR-23,PurdueAILab,
2016.
[21] KarrenYang,AbigailKatcoff,andCarolineUhler. Characterizingandlearningequivalence
classesofcausaldagsunderinterventions. InInternationalConferenceonMachineLearning,
pages5541–5550.PMLR,2018.
[22] Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and
AlexandreDrouin. Differentiablecausaldiscoveryfrominterventionaldata. AdvancesinNeural
InformationProcessingSystems,33:21865–21877,2020.
[23] ZizhaoWang,XuesuXiao,YukeZhu,andPeterStone.Task-independentcausalstateabstraction.
WorkshoponRobotLearning: Self-SupervisedandLifelongLearning,NeurIPS,2021.
[24] ZizhaoWang,XuesuXiao,ZifanXu,YukeZhu,andPeterStone. Causaldynamicslearningfor
task-independentstateabstraction. arXivpreprintarXiv:2206.13452,2022.
[25] AmyZhang,ClareLyle,ShagunSodhani,AngelosFilos,MartaKwiatkowska,JoellePineau,
Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International
ConferenceonMachineLearning,pages11214–11224.PMLR,2020.
[26] WenxuanZhu,ChaoYu,andQiangZhang. Causaldeepreinforcementlearningusingobserva-
tionaldata. arXivpreprintarXiv:2211.15355,2022.
[27] KrzysztofChalupka,PietroPerona,andFrederickEberhardt. Fastconditionalindependence
testforvectorvariableswithlargesamplesizes. arXivpreprintarXiv:1804.02747,2018.
[28] YuichiYoshidaandTakeruMiyato. Spectralnormregularizationforimprovingthegeneraliz-
abilityofdeeplearning. arXivpreprintarXiv:1705.10941,2017.
[29] NathanLambert,BrandonAmos,OmryYadan,andRobertoCalandra. Objectivemismatchin
model-basedreinforcementlearning. arXivpreprintarXiv:2002.04523,2020.
[30] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.
AdvancesinNeuralInformationProcessingSystems,32,2019.
[31] YezhenWang,BoLi,TongChe,KaiyangZhou,ZiweiLiu,andDongshengLi. Energy-based
open-worlduncertaintymodelingforconfidencecalibration. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages9302–9311,2021.
[32] BoPang,TianHan,ErikNijkamp,Song-ChunZhu,andYingNianWu. Learninglatentspace
energy-basedpriormodel. AdvancesinNeuralInformationProcessingSystems,33:21994–
22008,2020.
[33] EduardoFCamachoandCarlosBordonsAlba. Modelpredictivecontrol. Springerscience&
businessmedia,2013.
[34] YukeZhu,JosiahWong,AjayMandlekar,RobertoMartín-Martín,AbhishekJoshi,Soroush
Nasiriany,andYifengZhu. robosuite: Amodularsimulationframeworkandbenchmarkfor
robotlearning. InarXivpreprintarXiv:2009.12293,2020.
[35] MaximeChevalier-Boisvert,LucasWillems,andSumanPal. Minimalisticgridworldenviron-
mentforopenaigym. https://github.com/maximecb/gym-minigrid,2018.
[36] ZennaTavares,JamesKoppel,XinZhang,RiaDas,andArmandoSolar-Lezama. Alanguage
forcounterfactualgenerativemodels. InInternationalConferenceonMachineLearning,pages
10173–10182.PMLR,2021.
[37] EdouardLeurent.Anenvironmentforautonomousdrivingdecision-making.https://github.
com/eleurent/highway-env,2018.
[38] IoanaBica,DanielJarrett,andMihaelavanderSchaar. Invariantcausalimitationlearningfor
generalizablepolicies. AdvancesinNeuralInformationProcessingSystems,34:3952–3964,
2021.
[39] PimDeHaan,DineshJayaraman,andSergeyLevine. Causalconfusioninimitationlearning.
AdvancesinNeuralInformationProcessingSystems,32,2019.
[40] AlizéePace,HugoYèche,BernhardSchölkopf,GunnarRatsch,andGuyTennenholtz. Delphic
offline reinforcement learning under nonidentifiable hidden confounding. In The Twelfth
InternationalConferenceonLearningRepresentations,2023.
11[41] ScottFujimotoandShixiangShaneGu.Aminimalistapproachtoofflinereinforcementlearning.
Advancesinneuralinformationprocessingsystems,34:20132–20145,2021.
[42] MichaelSchlichtkrull,ThomasNKipf,PeterBloem,RiannevandenBerg,IvanTitov,andMax
Welling. Modelingrelationaldatawithgraphconvolutionalnetworks. InEuropeansemantic
webconference,pages593–607.Springer,2018.
[43] Amir-massoudFarahmand,AndreBarreto,andDanielNikovski. Value-awarelossfunctionfor
model-basedreinforcementlearning. InArtificialIntelligenceandStatistics,pages1486–1494.
PMLR,2017.
[44] YupingLuo,HuazheXu,YuanzhiLi,YuandongTian,TrevorDarrell,andTengyuMa. Algo-
rithmicframeworkformodel-baseddeepreinforcementlearningwiththeoreticalguarantees.
arXivpreprintarXiv:1807.03858,2018.
[45] PierlucaD’Oro,AlbertoMariaMetelli,AndreaTirinzoni,MatteoPapini,andMarcelloRestelli.
Gradient-awaremodel-basedpolicysearch. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pages3801–3808,2020.
[46] SurajNair,SilvioSavarese,andChelseaFinn. Goal-awareprediction: Learningtomodelwhat
matters. InInternationalConferenceonMachineLearning,pages7207–7219.PMLR,2020.
[47] ChiJin,ZhuoranYang,ZhaoranWang,andMichaelIJordan. Provablyefficientreinforcement
learningwithlinearfunctionapproximation. InConferenceonLearningTheory,pages2137–
2143.PMLR,2020.
[48] MasatoshiUeharaandWenSun. Pessimisticmodel-basedofflinereinforcementlearningunder
partialcoverage. arXivpreprintarXiv:2107.06226,2021.
[49] YilunDu, ShuangLi, JoshuaTenenbaum, andIgorMordatch. Learningiterativereasoning
throughenergyminimization. InInternationalConferenceonMachineLearning,pages5570–
5582.PMLR,2022.
[50] ClarkGlymour,KunZhang,andPeterSpirtes. Reviewofcausaldiscoverymethodsbasedon
graphicalmodels. Frontiersingenetics,10:524,2019.
[51] PeterSpirtes,ClarkNGlymour,RichardScheines,andDavidHeckerman.Causation,prediction,
andsearch. MITpress,2000.
[52] KarlPearson. X.onthecriterionthatagivensystemofdeviationsfromtheprobableinthecase
ofacorrelatedsystemofvariablesissuchthatitcanbereasonablysupposedtohavearisenfrom
randomsampling. TheLondon,Edinburgh,andDublinPhilosophicalMagazineandJournalof
Science,50(302):157–175,1900.
[53] KunZhang,JonasPeters,DominikJanzing,andBernhardSchölkopf. Kernel-basedconditional
independencetestandapplicationincausaldiscovery. arXivpreprintarXiv:1202.3775,2012.
[54] DavidMaxwellChickering. Optimalstructureidentificationwithgreedysearch. Journalof
machinelearningresearch,3(Nov):507–554,2002.
[55] Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional
markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning
Research,13(1):2409–2464,2012.
[56] AndrewANeathandJosephECavanaugh. Thebayesianinformationcriterion: background,
derivation, and applications. Wiley Interdisciplinary Reviews: Computational Statistics,
4(2):199–203,2012.
[57] ShengyuZhu,IgnavierNg,andZhitangChen. Causaldiscoverywithreinforcementlearning.
arXivpreprintarXiv:1906.04477,2019.
[58] DezhiYang,GuoxianYu,JunWang,ZhengtianWu,andMaozuGuo. Reinforcementcausal
structurelearningonordergraph. InProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume37,pages10737–10744,2023.
[59] NanRosemaryKe,OlexaBilaniuk,AnirudhGoyal,StefanBauer,HugoLarochelle,Bernhard
Schölkopf,MichaelCMozer,ChrisPal,andYoshuaBengio. Learningneuralcausalmodels
fromunknowninterventions. arXivpreprintarXiv:1910.01075,2019.
[60] YunzhuLi,AntonioTorralba,AnimaAnandkumar,DieterFox,andAnimeshGarg. Causal
discoveryinphysicalsystemsfromvideos.AdvancesinNeuralInformationProcessingSystems,
33:9180–9192,2020.
12[61] NinoScherrer,OlexaBilaniuk,YashasAnnadani,AnirudhGoyal,PatrickSchwab,Bernhard
Schölkopf,MichaelCMozer,YoshuaBengio,StefanBauer,andNanRosemaryKe. Learning
neuralcausalmodelswithactiveinterventions. arXivpreprintarXiv:2109.02429,2021.
[62] ChristopherClark,MarkYatskar,andLukeZettlemoyer.Don’ttaketheeasywayout:Ensemble-
basedmethodsforavoidingknowndatasetbiases. arXivpreprintarXiv:1909.03683,2019.
[63] DivyanshKaushik,EduardHovy,andZacharyCLipton. Learningthedifferencethatmakesa
differencewithcounterfactually-augmenteddata. arXivpreprintarXiv:1909.12434,2019.
[64] MeikeNauta,RickyWalsh,AdamDubowski,andChristinSeifert. Uncoveringandcorrecting
shortcutlearninginmachinelearningmodelsforskincancerdiagnosis. Diagnostics,12(1):40,
2021.
[65] PrashanMadumal,TimMiller,LizSonenberg,andFrankVetere. Explainablereinforcement
learningthroughacausallens. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume34,pages2493–2500,2020.
[66] SurajNair,YukeZhu,SilvioSavarese,andLiFei-Fei.Causalinductionfromvisualobservations
forgoaldirectedtasks. arXivpreprintarXiv:1910.01751,2019.
[67] SergeiVolodin,NevanWichers,andJeremyNixon. Resolvingspuriouscorrelationsincausal
modelsofenvironmentsviainterventions. arXivpreprintarXiv:2002.05217,2020.
[68] MaximilianSeitzer,BernhardSchölkopf,andGeorgMartius. Causalinfluencedetectionfor
improvingefficiencyinreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,34,2021.
[69] Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. Offline
reinforcementlearningwithcausalstructuredworldmodels. arXivpreprintarXiv:2206.01474,
2022.
[70] MircoMutti,RiccardoDeSanti,EmanueleRossi,JuanFelipeCalderon,MichaelBronstein,and
MarcelloRestelli. Provablyefficientcausalmodel-basedreinforcementlearningforsystematic
generalization. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,
pages9251–9259,2023.
[71] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-
stationary reinforcement learning. Advances in Neural Information Processing Systems,
35:31957–31971,2022.
[72] JiahengHu,ZizhaoWang,PeterStone,andRobertoMartin-Martin. Elden: Explorationvia
localdependencies. arXivpreprintarXiv:2310.08702,2023.
[73] WenhaoDing,HaohongLin,BoLi,andDingZhao. Generalizinggoal-conditionedreinforce-
mentlearningwithvariationalcausalreasoning. AdvancesinNeuralInformationProcessing
Systems,35:26532–26548,2022.
[74] LarsBuesing,TheophaneWeber,YoriZwols,NicolasHeess,SebastienRacaniere,ArthurGuez,
andJean-BaptisteLespiau. Woulda,coulda,shoulda: Counterfactually-guidedpolicysearch. In
InternationalConferenceonLearningRepresentations,2018.
[75] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based
counterfactualdataaugmentation. arXivpreprintarXiv:2210.11287,2022.
[76] TabithaELee,JialiangAlanZhao,AmritaSSawhney,SiddharthGirdhar,andOliverKroemer.
Causalreasoninginsimulationforstructureandtransferlearningofrobotmanipulationpolicies.
In2021IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages4776–4782.
IEEE,2021.
[77] TabithaEdithLee,ShivamVats,SiddharthGirdhar,andOliverKroemer. Scale: Causallearning
anddiscoveryofrobotmanipulationskillsusingsimulation. InConferenceonRobotLearning,
pages2229–2256.PMLR,2023.
[78] YudiZhang,YaliDu,BiweiHuang,ZiyanWang,JunWang,MengFang,andMykolaPech-
enizkiy. Interpretable reward redistribution in reinforcement learning: A causal approach.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[79] BiweiHuang,FanFeng,ChaochaoLu,SaraMagliacane,andKunZhang. Adarl: What,where,
andhowtoadaptintransferreinforcementlearning. InInternationalConferenceonLearning
Representations,2021.
13[80] PeideHuang,XilunZhang,ZiangCao,ShiqiLiu,MengdiXu,WenhaoDing,JonathanFrancis,
BingqingChen,andDingZhao.Whatwentwrong?closingthesim-to-realgapviadifferentiable
causaldiscovery. InConferenceonRobotLearning,pages734–760.PMLR,2023.
[81] XiangFu,GeYang,PulkitAgrawal,andTommiJaakkola. Learningtaskinformedabstractions.
InInternationalConferenceonMachineLearning,pages3480–3491.PMLR,2021.
[82] BiweiHuang,ChaochaoLu,LiuLeqi,JoséMiguelHernández-Lobato,ClarkGlymour,Bern-
hardSchölkopf,andKunZhang. Action-sufficientstaterepresentationlearningforcontrolwith
structuralconstraints. InInternationalConferenceonMachineLearning,pages9260–9279.
PMLR,2022.
[83] ZhihongDeng,ZuyueFu,LingxiaoWang,ZhuoranYang,ChenjiaBai,ZhaoranWang,andJing
Jiang. Score: Spuriouscorrelationreductionforofflinereinforcementlearning. arXivpreprint
arXiv:2110.12468,2021.
[84] MaximeGasse,DamienGrasset,GuillaumeGaudron,andPierre-YvesOudeyer. Usingcon-
foundeddatainlatentmodel-basedreinforcementlearning. TransactionsonMachineLearning
Research,2023.
[85] JunzheZhang,DanielKumor,andEliasBareinboim. Causalimitationlearningwithunobserved
confounders. Advancesinneuralinformationprocessingsystems,33:12263–12274,2020.
[86] YingJin,ZhuoranYang,andZhaoranWang. Ispessimismprovablyefficientforofflinerl? In
InternationalConferenceonMachineLearning,pages5084–5096.PMLR,2021.
[87] ThomasBlumensathandMikeEDavies. Iterativehardthresholdingforcompressedsensing.
Appliedandcomputationalharmonicanalysis,27(3):265–274,2009.
[88] AmirBeckandMarcTeboulle. Afastiterativeshrinkage-thresholdingalgorithmforlinear
inverseproblems. SIAMjournalonimagingsciences,2(1):183–202,2009.
[89] PeterAuer,ThomasJaksch,andRonaldOrtner. Near-optimalregretboundsforreinforcement
learning. Advancesinneuralinformationprocessingsystems,21,2008.
[90] RyanTibshiraniandLarryWasserman. Sparsity,thelasso,andfriends. Lecturenotesfrom
“StatisticalMachineLearning,”CarnegieMellonUniversity,Spring,2017.
[91] Patrick E McKnight and Julius Najab. Mann-whitney u test. The Corsini encyclopedia of
psychology,pages1–1,2010.
14A AuxiliaryDetailsofBECAUSEFramework
A.1 NotationSummary
WeillustrateallthenotationsusedinthemainpaperandappendixinTable1.
Table1: Notationsusedinthispaperandtheircorrespondingmeanings.
Notation Explanation
A,a Actionspace,action
S,s Statespace,state
r Reward
γ Discountfactor
T(·|·,·) Transitiondynamics
h,H Timesteph,HorizonH
π∗ Optimalpolicyintheonlineenvironments
π Behaviorpolicygeneratingtheofflinedatasets
β
V(·) Statevaluefunction
Q(·,·) Action-statevaluefunction
D Offlinedatasets
n(s,a) Numberofsamplesfor(s,a)pairsinofflinedatasets
B Bellmanoperator
h
ϕ(·,·) Featurerepresentationofstateandaction
ϕ(cid:101)(·,·) Featureofstate,actionandconfounderinequation(2)
µ(·) Featurerepresentationofnextstate
d,d′ Dimensionsoffeaturesforϕ(·,·)andµ(·)
K Featurematrixexpandedfromµinequation(5)
µ
C ,C ,C Featureregularity
ϕ µ β
C Sparsity-relatedconstant
s
κ Restrictiveeigenvalueconstant
X FeatureKroneckerproduct
u,u ,u Confounders
c π
M,M(u) Binarytransitionmatrix(undercertainconfounders)
G Causalgraph
M(cid:99),M(u) Estimatedcausalmatrix
βM,β(cid:98)M Optimal/estimatedparametersincausalmatrix
PAG(·) ParentalnodeinthecausalgraphG
ϵ ExogenousnoiseinSCMbyDefinition4
λ,λ ,λ Spectrumregularizerweightinequation(6)
ϕ µ
σ StandarddeviationofexogenousnoiseinSCM
ξ Accuracylevelofthepolicy
K IterativeupdatestepsinBECAUSE
E Energy-basedmodel
θ
δ Levelof‘highprobability’
Γ(·,·) Uncertaintyquantificationfunction
E δ-uncertaintyquantifierset
λ Regularizerweightfortheℓ norminEBM
EBM 2
A.2 DerivationofDefinition3
(cid:20) 0d×d M (cid:21)
ThenodeofthiscausalgraphG = containstwogroupsofentities: (1)Thestate
0d×d′ 0d×d
actionabstractionϕ(s,a), and(ii)thenextstateabstractionµ(s′). Wedenoteϕ(·,·)(i) astheith
factorintheabstractedstateactionrepresentations,andµ(·)(j) forthejth factorintheabstracted
staterepresentations.
15ThesourcenodeofallthenodesGisϕ(s,a)(i),whichistheabstractedstate-actionrepresentation,
andthesinknodeofalltheedgesinGisthetoµ(s)′(i).
T(s′|s,a)=(cid:2) ϕ(s,a)T µ(s′)T(cid:3)(cid:20) 0d×d M (cid:21)(cid:20) ϕ(s,a)(cid:21) =ϕ(s,a)TMµ(s′), (10)
0d×d′ 0d×d µ(s′)
Therefore, G is a bipartite graph, since there will be no edges between ϕ(s,a)(i),ϕ(s,a)(j), or
µ(s)(i),µ(s)(j).
Consequently,weshowthatG∈DAG.
A.3 Derivationofequation(5)
Definition 4 (Structured Causal Model). An SCM θ := (S,E) consists of a collection S of d
functions[11],
s :=f (PAG(s ),ϵ ), j ∈[d], (11)
j j j j
wherePAG ⊂{s ,...,s }\{s }arecalledparentsofx intheDirectedAcyclicGraph(DAG)G,
j 1 d j j
andE = {ϵ }d arejointlyindependent. Forinstance, incontinuousstateandactionspace, we
i i=1
parameterizetheworldmodelwithjointGaussianDistribution,i.e. ϵ∼N(0,σI ).
dd′
WethenusebilinearMDPtoapproximatetheoriginallikelihoodfunctioninequation(4),i.e.
(cid:89)
p(D;ϕ,µ,M)∝ exp(−∥µT(s′)K−1−ϕT(s,a)M∥2),
µ 2 (12)
(s,a,s′)∈D
whereK :=(cid:80) µ(s′)µ(s′)T isaninvertiblematrix. ThenwecanapplyanMLEinequation(5).
µ s′∈S
InourBECAUSEalgorithm,theoptimizationofthecausalworldmodelisconductedbysolving
theregularizedMLEprobleminequation(13). ThebiggestdifferencebetweenBECAUSEandthe
offlineversionof[14,15]isthatitaimstoapplyℓ regressioninsteadofridgeregressiontoestimate
0
matrixM:
M =argmax[logp(D;ϕ,µ,M)−λ|M|]
n
M
(cid:88)
=argmin ∥µT(s′)K µ−1−ϕT(s,a)M∥2 2+ λ∥M∥ 0 . (13)
M (cid:124) (cid:123)(cid:122) (cid:125)
(s,a,s′)∈D
SparsityRegularization
(cid:124) (cid:123)(cid:122) (cid:125)
WorldModelLearning
A.4 Proofofequation(7)
Thederivationdependsonthefollowingre-weightingformulain[18]:
E T(s′|s,a,u)π (a|s,u )
T(s′|s,a)= pu β π . (14)
E π (a|s,u )
pu β π
Thenweapplyequation(14)tothedecompositioninequation(2)andequation(3),whichyields
E (cid:2) T(s′|s,a,u)π (a|s,u )(cid:3)
T(s′|s,a)= pu β π
E π (a|s,u )
pu β π
E (cid:2) ϕ(s,a)TM(u )µ(s′)π (a|s,u )(cid:3)
=
pu c β π
E [π (a|s,u )]
pu β π (15)
(cid:34) E (cid:2) M(u )π (a|s,u )(cid:3)(cid:35)
=ϕ(s,a)T pu c β π µ(s′)
E [π (a|s,u )]
pu β π
=ϕ(s,a)T M(u)µ(s′),
wherethelastequalityholdsbylettingM(u):= E pu[M(uc)πβ(a|s,uπ)].
E pu[πβ(a|s,uπ)]
16B ProofofTheorem1
Inthissection,weprovidetheproofofthesub-optimalityupperboundinTheorem1. Wefirstshow
someusefuldefinitionsandlemmasinSectionB.1. Armedwiththem,weprovidethetheoretical
resultstailoredforthecausaldiscoverysettinginSectionB.2. Furthermore,wegiveadetailedproof
oftheuncertaintysetforminourcausaldiscoveryproblemsinSectionB.3.
B.1 Preliminary
Inthissubsection,wefirstdefinetheδ-uncertaintyquantifierΓ,thenwerefertothelemmasinthe
previousliteraturetoconstructasuboptimalityboundbasedonthedefineduncertaintyquantifierΓ.
First,wedefinetheBellmanoperatorB ,forsomevaluefunctionV :S (cid:55)→R,theBellmanoperator
h
canbedefinedas:
(B V)(s,a)=E[r (s ,a )+V(s )|s =s,a =a]. (16)
h h h h h+1 h h
Similarly,wedenotetheapproximateBellmanoperatoroftheempiricalMDPconstructedfromthe
offlinedatasetDasB (cid:98)hforanyh∈[H].
Definition5(δ-UncertaintyQuantifier). Welet{Γ }H ,Γ : S ×A (cid:55)→ Rtobeaδ-uncertainty
h h=1 h
quantifierwithrespecttodatadistributionP iftheevent:
D
(cid:110) (cid:111)
E = |(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|≤Γ h(s,a),∀(s,a,h)∈S×A×[H]
satisfiesP (E)≥1−δ.
D
Asweconsidertheofflinemodellearningandplanning,wedefinethemodelevaluationerrorateach
steph∈[H]as
∀(s,a)∈S×A: ι h(s,a)=(B hV(cid:98)h+1)(s,a)−Q(cid:98)h(s,a), (17)
whereι istheerrorinducedbytheapproximateBellmanoperator, especiallythetransitionker-
h
nel based on D. We then identify the source of sub-optimality in our offline MBRL setting by
decomposingthesub-optimalityerrorinLemma1.
Lemma1(DecompositionofSuboptimality[86]).
H H
(cid:88) (cid:88)
∀s∈S : V∗(s)−Vπ(s)=− E [ι (s ,a )|s =s]+ E [ι (s ,a )|s =s]
h h π h h′ h′ h π∗ h′ h′ h′ h
h′=h h′=h
H
(cid:88)
+ E π∗[⟨Q(cid:98)h′(s h′,·),π∗(·,s h′)−π (cid:98)(·,s h′)⟩ A|s
h
=s],
h′=h
(18)
whereπisanylearnedpolicy,π∗istheoptimalpolicythatmaximizesthecumulativereturnasbelow:
H
π∗ =argmaxE (cid:104) (cid:88) γh′ r(s ,a )|s (cid:105) .
π h′ h′ h
π
h′=1
Basedonthisdecomposition,wewillgetthebasicformofsub-optimalityerrorboundforgeneral
offlineRLsettingsinLemma2:
Lemma 2 (Suboptimality in standard MDP [86]). Suppose we have {Γ }H as δ-uncertainty
h h=1
quantifier. UnderE definedinequation(5),thesuboptimalityerrorboundbyconservativeplanning
satisfies:
H
(cid:88)
∀s∈S : V∗(s)−Vπ(s)≤2 E [Γ (s ,a )|s =s].
h h π∗ h′ h′ h′ 1
h′=h
Thebasicformofsub-optimalityboundinLemma2involvesanuncertaintyquantifierΓ ,whichin
h
ourcasewillbefurtherreplacedbyanexactboundinoursparsematrixestimationproblemofcausal
discoveryalgorithms.
17B.2 ProofofTheorem1
Themainresultsholdunderthefollowingtwoassumptions:
Assumption2(Existenceofacorematrixgiventhefeatureembedding). Foreach(s,a)∈S×A,
featurevectorsϕ(s,a)∈Rd,µ(s)∈Rd′ areapproximatedasapriori. Givenaspecificconfounder
setu,thereexistsanunknownmatrixM(u)∗ ∈Rd′×dsuchthat,
T(s′|s,a,u)=ϕ(s,a)TM(u)µ(s′). (19)
Assumption3(Featureregularity). Weassumefeatureregularity[14,15]forthefollowingcompo-
nentsoftheconfoundedbilinearMDP:
• ∀u,∥M(u)∥2 ≤C d,
F M
• ∀(s,a)∈S×A,∥ϕ(s,a)∥2 ≤C d,
2 ϕ
• ∀s′ ∈R|S|,∥µTs′∥ ≤C ∥s′∥ ,∥µK−1∥ ≤C′,
2 µ ∞ µ 2,∞ µ
• ∀s,a,s′ ∈S×A×S,∥ϕ(s,a)µ(s′)T∥ ≤C .
1 µ
whereC ,C ,C ,C′ aresomeuniversalconstants.
M ϕ µ µ
(cid:113)
Here,foranymatrixX,∥X∥ :=max (cid:80) X2 representstheoperator2(cid:55)→∞norm.
2,∞ i j ij
Proofpipeline. Armedwiththeaboveassumptions,weturntothebilinearMDPsetting,whichthis
workfocuseson. Weshalldevelopthefinite-sampleanalysisbyspecifyingthemainerrorterm—-
δ-uncertaintyquantifierΓ(seeLemma2)forourtime-homogeneouscorematrixestimationproblem
inthefollowinglemma.
Lemma3(UncertaintyboundforBilinearCausalRepresentation). UndertheAssumption2, 3and
thatT isanSCM(definedin4),fortheBECAUSEalgorithm,fortheξ-optimalpolicy(V∗(s)−
1 (cid:101)
Vπ(s)≤ξ),∀0≤ξ ≤1,wehavetheδ-uncertaintysetas:
1 (cid:101)
(cid:110)
E
BECAUSE
= |(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|
(cid:115)
≲min(cid:8)
C
log(cid:0)∥M∥ 0(cid:1)(cid:112)
|S|,C
σ(cid:112)
∥M∥
)(cid:9) log(1/δ) ,∀(s,a,h)∈A×S×[H](cid:111)
,
1 ξ s 0 n(s,a)
whereC issomeuniversalconstants.
1
Armedwiththeabovelemma,wecompletetheproofofTheorem1byshowingthat
H
(cid:88)
V∗(s)−Vπ(s)≤2 E [Γ (s ,a )|s =s]
1 (cid:101) 1 (cid:101) π∗ h′ h′ h′ 1 (cid:101)
h′=1
H (cid:34) (cid:115) (cid:35)
≲2(cid:88) E min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9) log(1/δ) |s =s
π∗ 1 ξ s 0 n(s ,a ) 1 (cid:101)
h h
h=1
(20)
ThisconcludestheproofofTheorem1.
B.3 ProofofLemma3
ThekeytoprovingTheorem1istoproveLemma3. TheproofpipelineofLemma3isillustrated
below. In Step 1, we derive the estimation of the causal transition matrix M in BECAUSE as a
sparsityregressionproblem. InStep2,wedecomposetheerrortermswithinδ-uncertaintysetinto
two parts: (a) error due to the under-explored dataset, (b) error due to optimization error in the
structuredcausalmodel. ThenweboundbotherrortermsinStep3andStep4,respectively. Finally,
inStep5,wesumupalltheresultsandderivetheformofδ-uncertaintyquantifierwhichwillleadto
ourfinalresultsinTheorem1.
18Step1: derivingtheoutputmodelofBECAUSE. Recallingtheoriginaloptimizationproblemin
equation(13)toestimatethecorematrix:
M(cid:99)=argmax[logp(ϕ,µ,M)−λ|M|]
M
(cid:88)
=argmin ∥µ(s′)TK µ−1−ϕ(s,a)TM∥2 2+ λ∥M∥ 0 . (21)
M (cid:124) (cid:123)(cid:122) (cid:125)
(s,a,s′)∈D
SparsityRegularization
(cid:124) (cid:123)(cid:122) (cid:125)
ModelLearning
This part of derivation aims to transform the above estimation problem into a linear regression
problem, withtheregressiondatapairs(X,T)andsomeunknownparametersβ associatedwith
maskM tobeestimated. Eventually,we’llderivetherepresentationofeachpartofβM,X,T,and
eventuallyreachthefollowingform:
(cid:88)
min [∥T (s′ |s ,a )−X βM∥2+λ∥βM∥ ]. (22)
βM
πβ i i i i 2 0
(si,ai,s′ i)∈D
Wedefineeachcomponentofthistargetformofℓ regressionasfollows:
0
• ForunknownparametersβM: WefirstdefineβM ∈[0,1]dd′ asacolumndimensionalvector
consistingofalltheentriesintime-homogenouscausalmatrixM,whereβM denotesthei-th
i
entryofβM. Besides,wedefineβM asthetruecorematrixgivensomeofflinedatasetD and
D
correspondingdatapairsT ,X thatsatisfiesT =βMX +ϵ.
data data data D data
• FordatasetD: RecallthetransitionpairsintheofflinedatasetD ={s ,a ,s′} . Here,n
i i i 1≤i≤n
representsthesamplesizeovercertainstate-actionpairsintherolloutdatabysomebehavior
policyπ . Forsimplicity,wedenoten≜n(s,a)inthefollowingderivation,whichismentioned
β
inSection2.1.
• ForregressiontargetT : Then,weintroducethefollowingtransitiontargetsT inducedby
πβ πβ
theofflinedatasetDsampledwithbehaviorpolicyπ :
β
(cid:80) 1(s =s,a =a,s′ =s′)
T (s′|s,a):= (si,ai,s′ i)∈D i i i
πβ (cid:80) 1(s =s,a =a)
(si,ai,s′ i)∈D i i
(23)
1 (cid:88)
= 1(s =s,a =a,s′ =s′).
n(s,a) i i i
(si,ai,s′ i)∈D
Underthenfinitesamplesintheofflinedataset,weassumethatT ∼N(E[T ],σ2I ). The
πβ πβ n
above definition specifies the regression target in the ℓ regression problem, and we denote
0
T =[T (s′|s ,a ),··· ,T (s′ |s ,a )]T ∈Rn astheempiricaltransitionprobabilitiesof
πβ πβ 1 1 1 πβ n n n
certaintransitionpairsintheofflinedataD ={s ,a ,s′} .
i i i 1≤i≤n
• ForregressiondataX: Next, weneedtospecifythedataX intheregressionproblem. We
denote the i-th row of X as the i-th sample in the offline transition pairs X ∈ D, which is a
i
vectorofKroneckerproductbetweenϕ(s i,a i)∈Rdandnormalized µ C(s µ′ i) ∈Rd′ (withoutloss
ofgenerality,weassumeC =1andonlyneedtonormalizeµ(s′)byC ):
ϕ i µ
µ(s′)
X =ϕ(s ,a )⊗ i
i i i C
µ
(24)
1
= [ϕ(s ,a )(1)µ(s′)(1),ϕ(s ,a )(1)µ(s′)(2),··· ,ϕ(s ,a )(d)µ(s′)(d′)]T
C i i i i i i i i i
µ
Asaresult, X ∈ Rdd′, sincethereareinallnsamplesinofflinedataset, X ∈ Rn×dd′ isthe
i
dataset-dependent matrix with all n rows of samples, and d and d′ are the latent dimension
of ϕ and µ, respectively. Based on the feature regularity criteria in Assumption 3, we have
∥X ∥ ≤∥X ∥ ≤1,∥X∥ ≤1.
i 2 i 1 ∞
The prior work [14] estimate the transition kernel of a bilinear MDP using the following ridge
regression:
minE ∥µ(s′)TK−1−ϕ(s,a)TM∥2+λ∥M∥ . (25)
(s,a,s′)∈D µ 2 2
M
19Inthispaper,inordertopromotethesparsityofthematrixM,weintroducetheℓ regularization
0
termandarriveatthefollowingoptimizationproblem:
(cid:88)
min [∥µ(s′)TK−1µ(s′)−ϕ(s ,a )TMµ(s′)∥2+λ∥βM∥ ]
i µ i i i i 2 0
βM
(si,ai,s′ i)∈D
(26)
(cid:88)
→m βMin [∥T πβ(s′
i
|s i,a i)−X iβM∥2 2+λ∥βM∥ 0]=:β(cid:98) DM,
(si,ai,s′ i)∈D
wherewedenotethesolutionassociatedwiththeofflinedatasetDasβ(cid:98)M. Here,weusetheempirical
D
versionconstructedbythefinitesamplesinofflinedatasetD.
Giventhegoal-conditionedrewardsetting,forasingleepisodes∼τ,r(s,a;g)=1ifandonlyif
s=g,otherwiser(s,a;g)=0,asisspecifiedinSection2.1. Sinceweareessentiallypredictingthe
(cid:80)
probabilities(normalizedtoasumof1)ofwhetherthenextstateisthegoalstate,i.e. V(cid:98)(s)=1.
s∈S
Therefore,wehave∥V(cid:98)(·)∥
1
≤1.
Asisdenotedbyequation(23),forthespecificofflinedatasetcollectedbysomebehaviorpoliciesπ ,
β
wehavetheregressiontargetT πβ(s′|s,a)= (cid:80) ( (cid:80)si, (a si i, ,s a′ i i) ,∈ sD
′
i)∈1 D(s 1i= (ss i, =ai s= ,aa i, =s′ i a= )s′) ,andthecorresponding
featuresinducedbythedatasetX
i
=ϕ(s i,a i)⊗ µ C(s µ′ i). Wehavethefollowingequationshold:
T =XβM +ϵ, (27)
πβ D
whereβ isthetrueunderlyingtransitionmaskgiventheofflinedatasetD,ϵ∼N(0,σ·I )issome
D n
exogenousnoiseofthetransitionmodel.
Specifically,fortheregressionproblem,wetransformtheoriginaltrajectorydatasetDas[X,T ],
πβ
as the representation of transition pairs rolled out by the behavior policy π . Similarly, we can
β
definesome’well-exploreddataset’D∗,whichisaninfinitedatasetrolloutbythebehaviorpolicy
π , D∗ = {s ,a ,r }∞ , similar to the definition of regression target T in equation (23) and
β i i i i=0
representationdataX inequation(24),wehave[X,E[T ]]astheregressionpairswithaccesstothe
πβ
truetransitionprobabilitydistributionforallthestateactionpairs(s,a). HereX ∈ Rn×dd′ isthe
regressiondatadefinedinequation(22). Recallingthedefinitioninequation(23),wecanfurther
buildtheregressiontargetunderwell-exploreddatasetD∗as:
(cid:80) 1(s =s,a =a,s′ =s′)
E[T (s′|s,a)]= (si,ai,s′ i)∈D∗ i i i
πβ (cid:80) (si,ai,s′ i)∈D∗1(s i =s,a i =a) (28)
=E [1(s =s,a =a,s′ =s′)]
s′∼T(·|s,a) i i i
We denote E[T ] = (cid:2)E[T (s′|s ,a )],···E[T (s′ |s ,a )](cid:3)T ∈ Rn. In practice, with finite
πβ πβ 1 1 1 πβ n n n
samplesizen,wehaveE[T (s′|s,a)]=T(s′|s,a)+ϵ=XβM +ϵ. Inaddition,wealsointroduce
πβ D∗
avectorformT∈R|S||A|×|S|sothat∀(s,a)∈S ×A,wehave
T(·|s,a)=(cid:2) T(s′|s,a) T(s′|s,a) ··· T(s′ |s,a)(cid:3)T ∈R|S|,
1 2 |S|
where the state space is denoted as S = {s′,··· ,s } are all possible states. Similar to the
1 |S|
KroneckerproductwedefineforX inequation(26),wedefineXinamatrixformforanystate-action
pair(s,a):
X(·|s,a)=(cid:2)
ϕ(s,a)⊗
µ(s′ 1)
,···ϕ(s,a)⊗
µ(s′ |S|) (cid:3)T ∈R|S|×dd′
.
C C
µ µ
Inaddition,weletX(s′|s,a) ∈ R1×dd′ denotethes′-throwofX(·|s,a)associatedwiththestate
s′ ∈S. Consequently,theestimatedtransitionkernelcanbeexpressedasfollows:
T(cid:98)(·|s,a)=ϕT(s,a)M(cid:99)µ(·)=X(·|s,a)β(cid:98)M.
D
Step 2: decomposing the term of interest. To begin with, recalling the definition of Bellman
operatorB inequation(16)andapplyingHölder’sinequality,thetermofinterestforanytimestep
h
201≤h≤H andstate-actionpair(s,a)∈S×Acanbecontrolledas
|(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|≤|⟨T(cid:98)(·|s,a)−T(·|s,a),V(cid:98)h+1⟩|
≤∥T(cid:98)(·|s,a)−T(·|s,a)∥ ∞∥V(cid:98)h+1∥
1
(29)
≤∥T(cid:98)(·|s,a)−T(·|s,a)∥ ∞,
wherethefirstinequalityisheldgivenourgoal-conditionedrewardformulationinsection2.1. To
continue,wehave
|(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|≤∥T(cid:98)(·|s,a)−T(·|s,a)∥
∞
(30)
=∥X(·|s,a)β(cid:98) DM −X(·|s,a)βM∥
∞
Here,werecallβ(cid:98)M representstheparametervectorintheestimatedcausalmasksbasedontheoffline
D
datasetD sampledbyπ β. Similarly,wedenoteβ(cid:98) DM
∗
astheestimatedcausalmaskoutputtedfrom
equation(26)basedontheinfinitedatasetD∗ generatedbythebehaviorpolicyπ . Then,wecan
β
furthercontrolequation(30)as
∥X(·|s,a)β(cid:98) DM −X(·|s,a)βM∥
∞
=∥X(·|s,a)β(cid:98) DM −X(·|s,a)β(cid:98) DM
∗
+X(·|s,a)β(cid:98) DM
∗
−X(·|s,a)βM∥
∞
≤∥X(·|s,a)[β(cid:98) DM −β(cid:98) DM ∗]∥ ∞+∥X(·|s,a)[β(cid:98) DM
∗
−βM]∥
∞
≤∥X(·|s,a)∥ ∞∥β(cid:98) DM −β(cid:98) DM ∗∥ ∞+∥X(·|s,a)∥ ∞∥β(cid:98) DM
∗
−βM∥
∞
≤∥β(cid:98) DM −β(cid:98) DM ∗∥ ∞+∥β(cid:98) DM
∗
−βM∥
∞
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(a) (b)
(31)
Herethelastinequalitycomesfromthefactthat
(cid:88)
∥X(·|s,a)∥ =max |X(·|s,a) |=max∥X(·|s,a) ∥
∞ ij i 1
i∈|S| i∈|S|
j∈[dd′]
C
=max∥ϕ(s,a)µ(s′)T∥ ≤ µ =1
i∈|S| i 1 C µ
basedonthedefinitionofX inequation(24)andassumption3. Here(a)comesfromthemismatch
errorbetweenthedemonstratedofflinedatasetandsomeoptimalrolloutdatasets. And(b)comes
from the error of the ℓ optimization of causal masks given the existence of exogenous noise σ
0
definedbySCMinDefinition4. Wewillcontrolthemseparatelyinthefollowing.
Step 3: Controlling term (a). We need to consider the optimization process in the original
regressionprobleminequation(22)tofullyunderstandthedifferencebetweenβ(cid:98)M andβ(cid:98)M,where
D D∗
theonlydifferenceisthatthelatterusesaperfectdatasetwithinfinitesamples. Theoptimization
problemwetarget(cf.equation(26))canbesolvedbytheiterativehardthresholdingalgorithm(IHT)
proposedby[87]IHToffersaniterativesolutionfortheℓ regressionproblem,armedwithahard
0
thresholdingoperatorasbelow:
(cid:26)
max{0,β −λ} ifβ >λ
[g (β)] = j j (32)
λ j 0 ifβ ≤λ, j =1,··· ,dd′.
j
We denote β(cid:98)M(i) as the estimated causal mask parameters after i-th iterations with dataset D.
D
Similarly,wedenoteβ(cid:98)M(i)astheestimationafteri-thiterationswithdatasetD∗. Weinitializethe
D∗
graphtobeafullgraphregardlessofthedatasets(D∗orD)usedintheoptimizationprocess,leading
toβ(cid:98)M(0)=β(cid:98)M(0)=1∈Rdd′.
D∗ D
RecallthatX ∈Rn×dd′,T
πβ
∈Rn,E[T πβ]∈Rnandβ(cid:98) DM is[0,1]dd′ basedonthedefinitioninthe
originalℓ optimizationprobleminequation(22),equation(23)andequation(28). Theupdaterules
0
ofusingeitherthedatasetDorD∗canbewrittenas:
β(cid:98) DM(i)=g λ(β(cid:98) DM(i−1)+ηXT[T
πβ
−Xβ(cid:98) DM(i−1)])
(33)
β DM ∗(i)=g λ(β(cid:98) DM ∗(i−1)+ηXT[E[T πβ]−Xβ(cid:98) DM ∗(i−1)]).
21Notethatthedifferencebetweenthetwoparametersβ(cid:98)M andβ(cid:98)M essentiallyreliesonthedifference
D D∗
between two pairs of transition kernel estimation datasets [X,E[T ]] and [X,T ]. It is easily
πβ πβ
verified that the hard-thresholding operator [g (·)] defined in equation (32) is L = 1-Lipschitz.
λ i
Accordingto[88],wecansetthelearningrateη ≤ 1 =1here. UsingtheLipschitzproperty,ateach
L
iterativeupdatestepi,wecancontrolthedifferencebetweentheestimatedparametersobtainedby
usingDorD∗as
∥β(cid:98) DM(i)−β(cid:98) DM ∗(i)∥
2
=∥g λ(cid:0) β(cid:98) DM(i−1)+ηXT[T
πβ
−Xβ(cid:98) DM(i−1)](cid:1)
−g λ(cid:0) β(cid:98) DM ∗(i−1)+ηXT[E[T πβ]−Xβ(cid:98) DM ∗(i−1)](cid:1) ∥
2
≤∥(cid:0) β(cid:98) DM(i−1)+ηXT[T
πβ
−Xβ(cid:98) DM(i−1)](cid:1) −(cid:0) β(cid:98) DM ∗(i−1)+ηXT[E[T πβ]−Xβ(cid:98) DM ∗(i−1)](cid:1) ∥
2
≤∥(I
dd′
−ηXTX)[β(cid:98)D(i−1)−β(cid:98)D∗(i−1)]+ηXT[T
πβ
−E[T πβ]]∥
2
≤∥I
dd′
−ηXTX∥ 2∥β(cid:98)D(i−1)−β(cid:98)D∗(i−1)∥ 2+η∥X∥ 2∥T
πβ
−E[T πβ]∥
2
≤∥β(cid:98)D(i−1)−β(cid:98)D∗(i−1)∥ 2+η∥T
πβ
−E[T πβ]∥ 2,
(34)
Here, I represent the identity matrix of size dd′ ×dd′, the last inequality holds based on the
dd′
factthat∥X∥ ≤ 1definedinequation(24). Sinceη ≤ 1 = 1,asaresult,∥I −ηXTX∥ =
2 L dd′ 2
max(1−ηλ(XTX))≤1. WeperformIHTforsufficientK >0iterationsandoutputthelaststep
estimation as our solutions for either the offline dataset D (we use for practical optimization) or
theperfectdatasetD∗. Inpractice, foranydatasetsuchasD andanyaccuracylevel0 ≤ ξ ≤ 1,
we have the output of IHT gradually converges to the optimal solution β(cid:98)M of the problem in
D
(cid:16) (cid:17)
equation(26). Namely,afteratmostK ≃log ∥M∥0 steps[87,Corollary1],theoutputsatisfies
ξ
(cid:13) (cid:13)
(cid:13)β(cid:98)M −β(cid:98)M(K)(cid:13) ≤ ξ. Similarly, based on the perfect dataset D∗, the output of IHT gradually
(cid:13) D D (cid:13) 4
2
convergestoβ(cid:98)M atthesamerate. Consequently,thetermofinterest(a)canbeboundedrecursively
D∗
as:
(cid:13) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13)
∥β(cid:98) DM −β(cid:98) DM ∗∥
∞
=(cid:13) (cid:13)β(cid:98) DM(K)−β(cid:98) DM ∗(K)+ β(cid:98) DM −β(cid:98) DM(K) + β(cid:98) DM
∗
−β(cid:98) DM ∗(K) (cid:13)
(cid:13)
∞
(cid:13) (cid:13) ξ ξ
≤(cid:13)β(cid:98)M(K)−β(cid:98)M(K)(cid:13) + +
(cid:13) D D∗ (cid:13) ∞ 4 4
(cid:13) (cid:13) ξ ξ
≤(cid:13)β(cid:98)M(K)−β(cid:98)M(K)(cid:13) + +
(cid:13) D D∗ (cid:13) 2 4 4
(35)
ξ
≤∥β(cid:98) DM(0)−β(cid:98) DM ∗(0)∥ 2+Kη∥T
πβ
−E[T πβ]∥ 2+
2
ξ
=Kη∥T −E[T ]∥ +
πβ πβ 2 2
≲ηlog(cid:0)∥M∥ 0(cid:1) ∥T −E[T ]∥ + ξ ,
ξ πβ πβ 2 2
wherethesecondinequalityholdsbyrecursivelyapplyingequation(34)toK,K−1,··· ,0iterations,
the last equality is due to the fact that β(cid:98)M(0) = β(cid:98)M(0), and the last inequality holds by setting
D D∗
K =log(∥M∥2).
ξ
Nowtheremainingoftheproofwillfocusoncontrolling∥T −E[T ]∥ . Recallthatwehave
πβ πβ ∞
definedtheregressiontargetinequation(23)andequation(28):
n(s,a)
1 (cid:88)
T (s′|s,a)= 1(s =s,a =a,s′ =s′),
πβ n(s,a) i i i
i=1
E[T (s′|s,a)]=E [1(s =s,a =a,s′ =s′)].
πβ s′∼T(·|s,a) i i i
Proposition1(Well-exploreddataset). WiththeofflinedatasetDofintotalnsampleswithn(s,a)
samplesgeneratedindependentlyconditionedonany(s,a). Forany0<δ <1,withprobabilityat
22least1−δ,onehas
(cid:115)
(cid:18) (cid:19)
|S| |S||A|
∀(s,a)∈S×A: ∥T (·|s,a)−E[T (·|s,a)]∥ ≤C log ,
πβ πβ 2 β n(s,a) δ
forsomeuniversalconstantC .
β
Theabovepropositioncanbedirectlyprovedbyapplying[89,Lemma17]overall(s,a)∈S×A:
(cid:115)
(cid:18) (cid:19)
14|S| 2|S||A|
max ∥T (·|s,a)−E[T (·|s,a)]∥ ≤ log . (36)
(s,a)∈S×A πβ πβ 1 n(s,a) δ
Asaresult,wecanfurtherextendtheresultsinequation(35)toboundtheterm(a)asfollows:
ξ
∥β(cid:98) DM −β(cid:98) DM ∗∥
∞
≤Kη∥T
πβ
−E[T πβ]∥ 2+
2
(cid:115)
(cid:18) (cid:19)
≲ηC C log(cid:0)∥M∥ 0(cid:1) |S| log |S||A| + ξ
β µ ξ n(s,a) δ 2
(cid:115) (37)
(cid:18) (cid:19)
≤C C
log(cid:0)∥M∥ 0(cid:1) |S|
log
|S||A|
+
ξ
β µ ξ n(s,a) δ 2
(cid:115)
(cid:18) (cid:19)
≜C log(cid:0)∥M∥ 0(cid:1) |S| log |S||A| + ξ ,
1 ξ n(s,a) δ 2
whereC = C C issomeconstantthatisrelatedtothefeatureregularity,ξ istheleveloferror
1 β µ
toleranceinthecumulativevaluereturns,inoursetting,0≤ξ ≤1.
Step4: Controllingterm(b). For(b)inequation(31),whichis∥β(cid:98) DM
∗
−βM∥ ∞,weareinterested
inwhatistheoptimizationerrorgivenfinitewell-exploredofflinedatasetD∗. Heretheoptimization
errormainlyoriginatesfromtheGaussiannoiseintheSCMformulationinDefinition4. Yetour
causaldiscoverymodule,i.e. anℓ estimator,willalwaysencountersomeestimationerrorinduced
0
bytheexogenousnoise.
Firstly,wehavetheboundedrelationshipbetweenℓ andℓ norm:
2 ∞
∥β(cid:98) DM
∗
−βM∥
∞
≤∥β(cid:98) DM
∗
−βM∥ 2,
thenwecananalyzetheerrorboundforℓ regressioninthesenseofℓ norm.
0 2
Thederivationbelowgenerallyfollowstheℓ regularizedlinearregressionboundin[90]. Basedon
0
Assumption2and3,thereexistsM,wedenotethisoptimalsolutioninthevectorformasβM.
Besidestheaforementionedoptimizationerrorintheiterativethresholdingupdateprocess,according
totheSCMinDefinition4andAssumption2andProposition1,wedenoteafinitesubsetofobserved
transitionprobabilitiesofthewell-exploreddataE[T ]withsizen: T ∈Rn. Bydefinitionabove,
πβ obv
wecanthenassumethefinite-sampleregressiontargetT isgeneratedbycausalfeaturesoftransition
pairs(denotedbyX =X =X ,X ∈Rn×dd′)andtheground-truthcausalmask(representedby
π∗ πβ
βM ∈Rdd′)withthefollowingequation:
T ≜E[T ]=T +ϵ=XβM +ϵ. (38)
obv πβ
Hereϵ∼N(0,σI )istheindependentexogenousnoisedefinedinDefinition4. We’llthenusethe
n
aboveequationtoboundterm(b)inequation(31).
Specifically,wesolvedthisℓ regressionproblemwithitsboundedformasfollows:
0
min∥T −XβM∥2, s.t.∥βM∥ ≤s. (39)
obv 2 0
βM
InBECAUSE,weselectthesparsitylevels≈∥βM∥ =∥M∥ ≤dd′.
0 0
23Forsimplicity,wedenotetheapproximatesolutionβ(cid:98)M inequation(39)asβ(cid:98)M. Bythevirtueof
D∗
optimalityofthesolutionβM inequation(39),wefindthat
∥T obv−XβM∥2
2
≤∥T obv−Xβ(cid:98)M∥2 2, (40)
thenbyexpandingandshiftingtheterms, wecanderivethebasicinequalityfortheℓ estimator
0
above:
∥T obv−XβM∥2
2
≤∥(T obv−XβM)+(XβM −Xβ(cid:98)M)∥2
2
=( =38 ⇒) ∥ϵ∥2 ≤∥ϵ+(XβM −Xβ(cid:98)M)∥2
2 2
=∥ϵ∥2+∥Xβ(cid:98)M −XβM∥2+2⟨ϵ,Xββ∗ −Xβ(cid:98)M⟩
2 2
Sincebothβ(cid:98)M andβM ares-sparse,thevectorβ(cid:98)M −βM isatmost2s-sparse. Wedenotetheset
of all 2s-sparse dd′-dimensional vector set as Tdd′(2s), we denote v = I(β(cid:98)M −βM = 0) as an
indicatorvector,v
i
=0ifandonlyifβ(cid:98) iM −β iM,∀i∈[dd′].
Byshiftingtheterms,weusethesub-GaussianassumptionandfurtheruseHölder’sinequalitytoget
thefollowingresults:
1 2
∥Xβ(cid:98)M −XβM∥2 ≤ ⟨XTϵ,β(cid:98)M −βM⟩
n(s,a) 2 n(s,a)
2
≤ n(s,a)∥β(cid:98)M −βM∥
2
sup ⟨v,XTϵ⟩
v∈Tdd′(2s)
(XTϵ) (41)
≤2∥β(cid:98)M −βM∥
2
sup ∥ n(s,a)S∥
2
|S|=2s
(cid:115)
2slog(dd′/δ)
≤2∥β(cid:98)M −βM∥ 2σ
n(s,a)
,
whereϵ∼N(0,σ2I )istheexogenousnoisevariableintheSCMinDefinition4.
n
Thenbysimplyapplyingrestrictedeigenvalue(RE)condition,forsomeκ(S)>0,wehave
(cid:115)
1 2slog(dd′/δ)
κ(S)∥β(cid:98)M −βM∥2
2
≤ n(s,a)∥X(β(cid:98)M −βM)∥2
2
≤2∥β(cid:98)M −βM∥ 2σ
n(s,a)
Therefore,wehave:
√ (cid:114) (cid:115)
2σ 2s log(dd′/δ) ∥M∥ log(dd′/δ)
∥β(cid:98)M −βM∥2
2
≤
κ(S) n
≜C sσ 0
n(s,a)
(42)
withprobabilityatleast1−δ,whichboundsterm(b)inequation(31).
Step5: Summinguptheresults Summarizingbothboundsforterms(a)and(b)inequation(31),
wewillgetthefollowingboundswithprobability1−δ:
|(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|≤∥β(cid:98) DM
∗
−βM∥ ∞+∥β(cid:98) DM −β(cid:98) DM ∗∥
∞
(cid:115) (cid:115)
≤C
log(cid:0)∥M∥ 0(cid:1) |S| log(cid:18) |S||A|(cid:19)
+C
σ(cid:112)
∥M∥
log(dd′/δ)
+
ξ
1 ξ n(s,a) δ s 0 n(s,a) 2
(cid:115)
≲min(cid:8)
C
log(cid:0)∥M∥ 0(cid:1)(cid:112)
|S|,C
σ(cid:112)
∥M∥
)(cid:9) log(1/δ)
1 ξ s 0 n(s,a)
(43)
Therefore,wecompletetheproofofLemma3byshowingthatforall(s,a)∈A×A,h∈[H],
(cid:40)
E
BECAUSE
|(B (cid:98)hV(cid:98)h+1)(s,a)−(B hV(cid:98)h+1)(s,a)|
(44)
(cid:115) (cid:41)
≲min(cid:8)
C
log(cid:0)∥M∥ 0(cid:1)(cid:112)
|S|,C
σ(cid:112)
∥M∥
)(cid:9) log(1/δ)
1 ξ s 0 n(s,a)
24Thefinalboundoftheterm(b)includesadependencyofO(√1 )andlogarithmofdimensionalitydd′
n
intheestimatedtransitionmatrix. ItalsoincursadependencyoferrortoleranceξorSCM’snoise
√ (cid:112)
levelσsquarerootofsparsitylevel s= ∥M∥ .
0
Sofar,weprovethefollowingboundintheorem1:
H (cid:34)(cid:115) (cid:35)
V∗(s)−Vπ(s)≤min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9)(cid:88) E log(1/δ) |s =s ,
1 (cid:101) 1 (cid:101) 1 ξ s 0 π∗ n(s ,a ) 1 (cid:101)
h h
h=1
(45)
Inordertoachieveξ-optimalpolicysuchthatV∗(s)−Vπ(s)≲ξ,theRHSneedstosatisfy:
1 (cid:101) 1 (cid:101)
H (cid:34)(cid:115) (cid:35)
min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9)(cid:88) E log(1/δ) |s =s ≲ξ (46)
1 ξ s 0 π∗ n(s ,a ) 1 (cid:101)
h h
h=1
(cid:113)
Wefirstmultiply min E (cid:2) n(s ,a )|s =s(cid:3) ,andthentakethesquareonboth
(s,a,h)∈S×A×[H] π⋆ h h 1 (cid:101)
sides. Wehave:
(cid:115) 
min(cid:8) C 1log(cid:0)∥M ξ∥ 0(cid:1)(cid:112) |S|,C sσ(cid:112) ∥M∥ 0)(cid:9)(cid:88)H E π∗ log(1/δ)min (s, na, (h s)∈ ,S a×A )×[H]n(s h,a h)(cid:3) |s 1 =s (cid:101)
h h
h=1
(cid:114)
≲ξ min E (cid:2) n(s ,a )|s =s(cid:3)
π⋆ h h 1 (cid:101)
(s,a,h)∈S×A×[H]
(47)
Giventhedefinition,LHSinequation(47)satisfies:
H
LHS ≲min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9)(cid:88) E (cid:104)(cid:112) log(1/δ)|s =s(cid:105) (48)
1 ξ s 0 π∗ 1 (cid:101)
h=1
Toensurethesatisfactionofξ-optimalpolicy,wethuswouldliketheRHSofequation(47)satisfies:
(cid:114)
RHS =ξ min E (cid:2) n(s ,a )|s =s(cid:3)
π⋆ h h 1 (cid:101)
(s,a,h)∈S×A×[H]
H
≳min(cid:8) C log(cid:0)∥M∥ 0(cid:1)(cid:112) |S|,C σ(cid:112) ∥M∥ )(cid:9)(cid:88) E (cid:104)(cid:112) log(1/δ)|s =s(cid:105) (49)
1 ξ s 0 π∗ 1 (cid:101)
h=1
=min(cid:8)
C
log(cid:0)∥M∥ 0(cid:1)(cid:112)
|S|,C
σ(cid:112)
∥M∥
)(cid:9) H(cid:112)
log(1/δ)
1 ξ s 0
Consequently,wecanshiftthetermsandgetthesamplecomplexityboundfortheξ-optimalpolicy,
∀0≤ξ ≤1:
min(cid:8) C2log2(cid:0)∥M∥0(cid:1) |S|,C2σ2∥M∥ (cid:9) ·H2log(1/δ)
min E (cid:2) n(s ,a )|s =s(cid:3)≳ 1 ξ s 0 ,
(s,a,h)∈S×A×[H] π⋆ h h 1 (cid:101) ξ2
(50)
C AdditionalExperimentsDetails
Inthissection,weprovideadditionalexperimentresultsandalgorithmimplementationdetails.
C.1 ImplementationofCausalDiscovery
Weimplementthecausaldiscoveryprimarilybasedonequation(5). However,inpractice,howto
controlthecoefficientbeforethesparsityregularizationtermsiscrucialtothefinalperformance. In
25practice,insteadofcontrollingλ,weusep-valueasathresholdtodeterminethefollowingconditional
independence:
ϕ(s,a)(i) ⊥⊥µ(s′)(j)|ϕ(s,a)−(i). (51)
Hereϕ(·,·)(i)meansthatthiselementistheithfactorintheabstractedstateactionrepresentation,
similartoµ(·,·)(j),ϕ(s,a)−(i) meansalltheotherfactorsintherepresentationexceptfortheith
factor.
If the p-value based on the above conditional independence test is less than a threshold, we can
remove the edge by setting M = 0. Please refer to the Appendix Table 10 for the selection of
ij
thresholdineachenvironment.
C.2 TrainingDetailsofEnergy-basedModel
WetraintheEBMaccordingtothemarginlossinequation(8). Inpractice,weattachTanh()tothe
outputlayertocliptheunnormalizedscorebetween-1and+1. Theenergynetworkstakeinboth
conditionsandsamples,thenconcatenatethemtogetherandsentitintoMLPencoders. Thedetailed
hyperparametersofEBMarelistedinTable9.
InthevanillaEBM,peoplefollowtheLangevindynamicstoeffectivelysamplethenegativesamples.
Here,aswediscoverthecausalmaskandidentifythecausalrepresentationinthemodellearning
stage,wefindthatwecanusetheserepresentationsinboththeenergynetworksandthesampling
processtogetsomeeffectivenegativesamples,whichissimilartothepracticeofaugmentationof
causality-guidedcounterfactualdata[75].
Theinterestingtrickweemployhereisthewaytogetournegativesamplesbymixingthelatent
factorsfromofflinedata. Forexample,forapositivesamplearray

µ(s′)(1) µ(s′)(2) ···
µ(s′)(d′)
1 1 1
x+ = µ(s′ 2)(1) µ(s′ 2)(2) ··· µ(s′ 2)(d′) , (52)
 ··· ··· ··· ··· 
µ(s′ )(1) µ(s′ )(2) ··· µ(s′ )(d′)
n n n
withconditions:
 
ϕ(s ,a )(1) ϕ(s ,a )(2) ··· ϕ(s ,a )(d)
1 1 1 1 1 1
ϕ(s ,a )(1) ϕ(s ,a )(2) ··· ϕ(s ,a )(d)
y = 1 1 1 1 1 1 . (53)
 ··· ··· ··· ··· 
ϕ(s ,a )(1) ϕ(s ,a )(2) ··· ϕ(s ,a )(d)
1 1 1 1 1 1
Here,s ,a ,s′ denotesthetimestepoftheofflinesamples. ϕ(·,·)(i) meansthiselementistheith
i i i
factorintheabstractedstateactionrepresentation,similartotheµ(·,·)(i)
aswealreadygetthecorrespondingcausalrepresentationµ(s′)thatissemanticallymeaningful,
wecanmixthecolumnstocreateusefulcounterfactualnegativesamples
 µ(s′)(1) µ(s′)(2) ··· µ(s′ )(d′)
1 2 n−1
x− = µ(s′ 2)(1) µ(s′ 1)(2) ··· µ(s′ n)(d′)  . (54)
countefactual  ··· ··· ··· ··· 
µ(s′ )(1) µ(s′)(2) ··· µ(s′)(d′)
n 2 1
Thesecounterfactualnegativesamplesseemtoeffectivelyusethecausalrepresentationandspeedup
thetraining,asweshowinFigure7.
C.3 AdditionalMismatchAnalysis
Toevaluatethesignificanceoftheobjectivemismatcheffectatthreedifferentlevelsofofflinedatasets
inUnlock environments, wecollect5,000episodesineachoftheUnlockenvironmentsforeach
method. Thenweevaluatethemismatchviathefollowingtwometrics.
26T=0 T=100 T=200 T=300 T=400
T=500 T=600 T=700 T=800 T=900
Next State
GT
Figure7: ComparisonoftheconvergencespeedinEBMtraining. Comparedtorandomnegative
samples,ourapproachenjoysahigherrateofconvergenceempirically.
• WeconducthypothesistestingviaMann-WhitneyUTest[91],withNullhypothesis
H :L (τ )<L (τ ),
0 model pos model neg
• Tounderstandtheexactdifferenceinmodellossbetweentwogroupsofsamples,wecompute
theirWasserstein-1distanceintheepisodicmodellossbetweenthetrajectorieswithpositiveand
negativerewards,i.e. W (τ ∥τ ).
1 pos neg
Wereporttheresultsofp-valueandtheW distanceoftwogroupsofmodellosssamplesinthe
1
followingtable.
Table2: Thecomparisonresultsofthep-valueandtheW distance(×10−4)betweenMOPOand
1
BECAUSE.Boldmeansthebetter.
Unlock-Expert Unlock-Medium Unlock-Random
Methods
p-value(↓) W Dist(↑) p-value(↓) W Dist(↑) p-value(↓) W Dist(↑)
1 1 1
MOPO 6.5×10−5 0.7 1.0 1.1 1.0 N.A.
Ours ≈0 3.1 ≈0 3.0 0.9 <0.1
C.4 AdditionalExperimentResults
Wereporttheresultsofthetask-wiseperformanceofallbaselinesinthemainexperimentandvariants
intheablationstudiesinTable3,4,5,and 6.
27
modnaR
MBE
lasuaC
modnaR
MBE
lasuaCTable3: Successrate(%)for18tasksinthreedifferentenvironments. Weevaluatethemeanand
95%confidenceintervalgivenbythet-testofthebestperformanceamong10randomseeds,aswell
asthep-valuebetweentheoverallperformance. Boldisthebest.
Env ICIL CCIL TD3+BC MOPO GNN CDL Denoised IFactor MnM Delphic Ours
Lift-I-R 14.3±9.9 10.0±11.4 9.7±5.4 24.3±2.9 22.1±3.6 33.8±5.0 20.0±4.0 24.0±2.8 16.3±2.8 20.2±3.1 19.2±0.6
Lift-O-R 8.5±4.7 0.0±0.0 1.3±1.0 10.2±1.6 13.3±2.3 16.0±4.7 15.5±3.8 21.2±3.0 14.2±2.2 17.7±2.8 21.4±3.9
Unlock-I-R 3.4±1.1 2.2±0.8 4.39±0.9 21.5±1.9 11.7±2.1 6.6±0.5 6.9±0.7 8.1±1.1 8.6±1.3 14.4±1.1 32.7±2.8
Unlock-O-R 11.6±4.0 13.5±3.7 13.3±3.0 16.6±1.3 12.1±1.5 7.6±1.0 7.0±0.8 8.0±1.1 9.0±1.0 12.2±1.1 27.6±2.0
Crash-I-R 11.4±4.3 19.5±10.5 9.1±6.6 32.7±4.9 11.7±0.8 39.7±4.6 32.0±2.8 31.3±4.4 16.0±2.2 17.4±3.6 59.4±6.1
Crash-O-R 2.8±1.8 5.7±3.0 0.9±0.6 10.0±2.0 3.7±0.4 10.8±1.9 10.8±2.1 11.0±3.6 4.1±0.6 5.4±1.1 19.7±1.4
Lift-I-M 54.0±13.3 44.0±20.8 26.6±14.8 39.8±5.3 27.5±5.3 32.9±5.7 35.3±4.8 41.0±5.7 28.7±1.8 24.0±2.3 59.5±4.4
Lift-O-M 46.8±15.2 20.0±21.9 13.0±1.1 31.9±2.8 25.8±1.8 26.0±4.2 27.0±3.2 30.2±2.7 24.3±2.3 18.3±2.6 32.3±4.9
Unlock-I-M 4.8±0.9 4.7±1.3 5.4±1.0 84.8±5.1 17.5±2.6 29.7±4.4 12.9±1.5 37.7±5.5 37.6±4.9 74.1±1.5 98.0±4.9
Unlock-O-M 12.9±2.8 14.1±2.0 19.2±2.5 39.5±4.7 16.2±1.8 20.5±3.9 10.8±0.8 21.5±2.7 27.0±3.7 51.7±2.3 68.8±1.5
Crash-I-M 35.5±9.9 24.5±12.2 16.3±9.0 47.7±7.3 11.5±1.1 63.5±4.0 63.8±4.0 45.5±5.0 18.4±2.9 58.2±2.2 90.4±1.8
Crash-O-M 11.7±3.5 7.9±4.4 5.6±3.1 17.3±2.3 3.8±0.4 20.0±2.3 20.3±1.6 16.0±2.0 6.3±1.9 22.2±1.7 20.3±1.9
Lift-I-E 73.8±17.0 86.7±15.7 44.3±12.2 82.4±6.7 63.3±0.0 71.0±5.5 74.2±5.5 98.0±3.7 33.3±3.0 53.5±6.3 92.8±1.2
Lift-O-E 54.1±26.1 80.0±18.9 41.6±16.6 72.4±1.9 60.8±0.6 63.1±5.1 64.0±4.4 91.7±5.7 29.0±4.1 49.5±3.1 93.7±5.9
Unlock-I-E 11.6±3.2 13.7±3.1 15.6±2.2 88.8±4.6 15.3±1.6 73.2±2.8 50.3±3.0 59.3±2.6 60.7±2.2 83.2±1.2 97.4±1.0
Unlock-O-E 22.4±5.0 35.4±6.0 41.6±6.2 39.9±4.4 13.8±1.4 41.3±4.1 35.7±2.3 29.5±3.5 38.7±2.0 54.4±1.9 82.1±6.5
Crash-I-E 35.7±9.8 26.8±7.2 26.0±14.4 58.5±4.7 11.2±0.9 63.7±2.8 69.3±3.9 52.0±5.3 10.2±1.1 57.0±1.2 95.3±1.3
Crash-O-E 11.3±3.8 12.3±3.5 6.2±1.8 14.8±2.1 3.5±0.9 18.8±1.8 20.7±2.2 15.6±2.3 3.9±0.6 16.5±1.7 20.7±3.2
Overall-I 27.2 25.8 17.5 53.4 21.0 44.7 40.5 44.1 25.5 44.7 73.3
Overall-O 20.2 19.9 14.6 28.1 17.0 24.9 23.5 27.2 17.4 27.5 43.0
Table4: p-valuesofdifferentmethods(eachhas10randomtrials)againstBECAUSEinvarious
environments. Underthesignificancelevel0.05,wemarkallthebaselineresultsthataresignificantly
lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly
outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of
experiments).
Env ICIL TD3+BC MOPO GNN CDL Denoised IFactor CCIL MnM Delphic
Lift-I-random 0.001 0.000 0.001 0.000 0.000 0.000 0.001 0.001 0.000 0.000
Lift-O-random 0.000 0.000 0.000 0.001 0.033 0.013 0.464 0.000 0.001 0.052
Unlock-I-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Lift-I-medium 0.198 0.000 0.000 0.000 0.000 0.000 0.000 0.067 0.000 0.000
Lift-O-medium 0.966 0.000 0.438 0.009 0.022 0.032 0.209 0.125 0.003 0.000
Unlock-I-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-medium 0.000 0.000 0.018 0.000 0.412 0.500 0.001 0.000 0.000 0.943
Lift-I-expert 0.017 0.000 0.004 0.000 0.000 0.000 0.993 0.203 0.000 0.000
Lift-O-expert 0.004 0.000 0.000 0.000 0.000 0.000 0.297 0.027 0.000 0.000
Unlock-I-expert 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-expert 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-expert 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-expert 0.000 0.000 0.002 0.000 0.133 0.500 0.005 0.000 0.000 0.011
28Table5: Successrate(%)for18tasksinthreedifferentenvironments. Weevaluatethemeanand
95%confidenceintervalofthetestperformanceamong10randomseeds. Boldmeansthebest.
Env BECAUSE BECAUSE-Optimism BECAUSE-Linear BECAUSE-Full
Lift-I-random 33.8±5.0 23.2±3.1 16.5±1.6 22.2±6.6
Lift-O-random 21.4±3.9 15.3±1.5 8.9±4.8 18.2±5.3
Unlock-I-random 32.7±2.8 31.2±2.4 20.7±1.7 10.7±0.8
Unlock-O-random 27.6±2.1 26.3±1.7 24.0±2.7 9.3±0.6
Crash-I-random 59.4±6.2 49.9±9.2 54.3±5.4 36.1±7.3
Crash-O-random 19.7±1.4 14.2±0.5 14.8±1.5 10.0±2.2
Lift-I-medium 59.5±4.5 46.8±2.1 24.4±5.3 36.4±6.7
Lift-O-medium 32.3±5.0 24.5±2.5 16.4±2.3 28.9±4.3
Unlock-I-medium 98.0±4.9 92.7±5.8 91.0±1.8 29.9±1.9
Unlock-O-medium 68.8±1.5 58.7±2.2 60.0±2.0 18.7±1.3
Crash-I-medium 90.4±1.8 82.8±9.9 66.7±7.4 60.8±1.8
Crash-O-medium 20.3±1.9 17.5±0.0 15.9±2.8 24.6±0.9
Lift-I-expert 92.8±1.2 68.6±4.7 75.6±10.7 78.1±6.1
Lift-O-expert 93.7±6.0 58.3±7.9 66.1±8.0 71.9±6.9
Unlock-I-expert 97.4±1.0 93.1±1.9 94.0±2.0 29.3±1.3
Unlock-O-expert 82.1±6.6 64.6±3.1 65.8±3.2 20.2±1.7
Crash-I-expert 95.3±1.4 91.0±2.2 77.9±2.8 50.3±7.6
Crash-O-expert 20.7±3.2 12.5±0.0 26.9±6.1 25.0±1.8
Overall-I 73.3 64.4 57.9 39.3
Overall-O 43.0 32.4 33.2 25.2
Table6: p-valuesofdifferentmethods(eachhas10randomtrials)againstBECAUSEinvarious
environments. Underthesignificancelevel0.05,wemarkallthebaselineresultsthataresignificantly
lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly
outperforms3variantsin18tasks83.3%oftheexperiments(45outoftotal54pairsofexperiments).
Env BECAUSE-Optimism BECAUSE-Linear BECAUSE-Full
Lift-I-random 0.001 0.000 0.003
Lift-O-random 0.003 0.000 0.146
Unlock-I-random 0.189 0.000 0.000
Unlock-O-random 0.145 0.015 0.000
Crash-I-random 0.036 0.090 0.000
Crash-O-random 0.000 0.000 0.000
Lift-I-medium 0.000 0.000 0.000
Lift-O-medium 0.004 0.000 0.130
Unlock-I-medium 0.067 0.006 0.000
Unlock-O-medium 0.000 0.000 0.000
Crash-I-medium 0.061 0.000 0.000
Crash-O-medium 0.005 0.005 1.000
Lift-I-expert 0.000 0.003 0.000
Lift-O-expert 0.000 0.000 0.000
Unlock-I-expert 0.000 0.002 0.000
Unlock-O-expert 0.000 0.000 0.000
Crash-I-expert 0.001 0.000 0.000
Crash-O-expert 0.000 0.969 0.990
29C.5 AdditionalEnvironmentDescription
Weprovideamoredetaileddescriptionoftheenvironmentsweuseintheexperiment,asshownin
Table7.
Lift The Lift environment based on the robosuite [34] contains 33 dimensions of state space,
includingtheendeffectorpose,jointpose,jointvelocity,cubeposeaswellasitsrelativeposition,
cubecolor,andacontactflag. Itcontains4dimensionsofhybridactionspacethatusesOperation
SpaceControl(OSC)tocontrolthe3Dpositionandthe1Dgrippermovement. Thetaskiscounted
asasuccesswhentheassignedblockisliftedfromthetableover0.1m. Thegeneralizationsetting
intheLiftenvironmentistouseanunseencombinationofpositionandcolorduringonlinetesting.
Thisenvironmentcanbeabstractedinto15dimensionsoffactorizablestatespaceand4dimensions
offactorizableactionspace. ThecausalgraphofthisenvironmentisrecordedinFigure9(a).
Unlock TheUnlockenvironmentsbasedontheMiniGridworld[35]contain110dimensionsof
discretestatespace,with3of36-dimensionalvectorinputsrepresentingthecurrentpositionofthe
agent,key,anddoorina6x6gridworld. Therest2dimensionsinthestatespacememorizethestate
ofwhethertheagenthasthekeyinhand. Theactionspaceisalsodiscrete(witheightdimensions)
todeterminethemovement(up/down/left/right)andthepick-key,open-dooractions. Anepisode
willbecountedasasuccesswhentheagentholdsthekeyandusesittoopenthedoorintheright
position. ThegeneralizationsettingintheUnlockenvironmentistochangethepositionofthedoor
andincreasethenumberoftotalgoalsintheenvironment. Theagentwillonlysuccessfullyfinishone
episodebyopeningallthedoors. ThecausalgraphofthisenvironmentisrecordedinFigure9(b).
Crash TheCrashenvironmentsarebasedontheHighwayenvironment[37]whichcontains22
dimensions of continuous state space, with four vector inputs representing the current position,
velocity, and orientation of the surrounding vehicles and ego vehicles. There are two additional
dimensionsofstatememorizingthecollisiontypebetweentheegovehiclesandsurroundingvehicles
orpedestrians. The8-dimensionalactionspaceiscontinuoustodeterminetheaccelerationinthe
x−ydirectionsoftheegoandsurroundingagents. ThegeneralizationoftheCrashenvironmentis
toadddifferentnumbersofpedestriansthatmaycausethecrash. Anepisodewillonlyendwhenthe
egovehicleshaveanear-misswithbothofthepedestriansatthescene. Wevisualizethecausalgraph
ofthisenvironmentinFigure9(c).
AllthreeenvironmentsarevisualizedinFigure8. WelisttheirbasicconfigurationsinTable7.
Table7: Environmentconfigurationsusedinexperiments
Environment
Parameters
Lift Unlock Crash
Maxstepsize 30 15 30
Statedimension 33 110 22
Actiondimension 4 8 8
Actiontype Hybrid Discrete Hybrid
Intrinsicstaterank 15 4 6
Intrinsicactionrank 4 3 4
Figure8: Threeenvironmentsusedinthispaper.
30(a) Lift Environment (b) Unlock Environment (c) Crash Environment
Figure9: UnderlyingcausalgraphGinall3environmentswithexpertdemonstration.
C.6 AdditionalBaselineInformation
Wecollectdataontheabove3differentenvironments,thusforming9groupsofofflinedatasets.
Table8: Bahaviorpoliciesusedtocollectofflinedataindifferentenvironments.
Environment Behavior #Episodes SuccessRate AdditionalDescription
Random 1000 0.24 Randomactionsafterafewstepsofinitialization.
Lift Medium 1000 0.60 Randomactionsbeforethegoal-reachingexpert.
Expert 1000 1.00 Queryexpertpolicyforalltimesteps.
Random 200 0.21 Randomnavigationwithhighrandomness
Unlock Medium 200 0.46 Targetedsearchingingoaldirections
Expert 200 0.87 ShortestpathplanningviaA∗
Random 1000 0.14 Fixedego,randompedestrians
Crash Medium 1000 0.35 Plannedego,randompedestrians
Expert 1000 0.66 Planninginbothegoandpedestrians
Aftercollectingthedatausingscriptedpoliciesindifferentenvironments,wetrainallagentsaswell
asBECAUSEunder10differentrandomseeds. Thenwereportthebestperformanceofeachtrial
andcomputethemeanandstandarddeviationover10seedsforeachtaskintheAppendix3.
Werefertothefollowingcodebasetoimplementallthebaselinesweuse:
• InvariantCausalImitationLearning(ICIL,[38]):https://github.com/ioanabica/Invariant-
Causal-Imitation-Learning,MITLicense.
• CausalConfusionImitationLearning(CCIL,[39]):referencelinktothepaper.
• TD3 with Behavior Cloning (TD3+BC, [41]): https://github.com/sfujim/TD3_BC, MIT
License.
• Model-based Offline Policy Otimization (MOPO, [2]): https://github.com/junming-
yang/mopo.git,MITLicense.
• RelationalGraphNeuralNetwork(GNN,[42]):https://github.com/MichSchli/RelationPrediction.git,
MITLicense.
• CausalDynamicsLearning(CDL,[24]):https://github.com/wangzizhao/robosuite/tree/cdl,
MITLicense.
• DenoisedMDP(Denoised,[12]): https://github.com/facebookresearch/denoised_mdp.git,
CCBY-NC4.0.
• MismatchNoMore(MnM,[9]):referencelinktothepaper.
• Worldmodelwithidentifiablefactorization(IFactor,[13]), referencelinktothepaper.
• DelphicOfflineRL(Delphic,[40]): referencelinktothepaper.
31ThedetailedhyperparametersweuseinBECAUSEandotherbaselinesarelistedinTable9and
Table10:
C.7 ExperimentSupport
Ourcodeisavailableattheanonymousrepo:https://anonymous.4open.science/r/BECAUSE-NeurIPS
Computingresources Theexperimentsarerunonaserverwith2×AMDEPYC754232-Core
ProcessorCPU,2×NVIDIARTX3090graphicsand2×NVIDIARTXA6000graphics,and252GB
memory. Foronesingleexperiment,ittakesBECAUSEandotherbaselinesabout1.5hourswith
100,000iterationstotraintheworldmodeland1,000,000stepstotraintheenergy-basedmodels.
C.8 BroaderImpact
Thisworkincorporatescausalityintoreinforcementlearningmethods,whichhelpshumansunder-
standtheunderlyingmechanismofalgorithmsandcheckthesourceoffailures. However,thelearned
causalworldmodelmaycontainhuman-readableprivateinformationabouttheenvironmentandthe
dataset. Tomitigatethispotentialnegativesocietalimpact,thecausalworldmodelshouldonlybe
accessibletotrustworthyusers.
Table9: Hyper-parametersofmodelsusedinexperimentsofBECAUSEandbaselines(PartI)
Environment
Models Parameters
Lift Unlock Crash
Learningrate 0.0001 0.001 0.0001
SizeofdataD 15000 4000 15000
Epochperiteration 20 5 10
Batchsize 256 256 256
PlanninghorizonH 15 10 20
Planningpopulation 1500 100 1000
BECAUSE
Rewarddiscountγ 0.99 0.99 0.99
Spectralnormregularizerλ 10−4 10−4 10−4
ϕ
Spectralnormregularizerλ 10−4 10−4 10−4
µ
Causaldiscoveryp 10−8 10−4 10−6
thres
Encoderhiddens 256 64 128
EBMhidden 256 64 128
EBMnegativebuffer 5000 1000 5000
EBMtrainingsteps 1000 1000 1000
EBMregularizerλ 10−4 10−4 10−4
EBM
MLPhiddens 256 64 128
MOPO* MLPlayers 2 2 2
Ensemblenumber 5 5 5
Initializedmaskcoef. 1.0 1.0 1.0
CDL* MLPhiddens 256 64 128
Sparsityregularizer 0.001 0.001 0.001
GNNhiddens 256 64 128
GNN*
GNNlayers 3 1 3
* UsethesameplanningparametersasBECAUSE.
32Table10: Hyper-parametersofmodelsusedinexperimentsofbaselines(Continued)
Environment
Models Parameters
Lift Unlock Crash
LearningrateofMINE 0.0001 0.0001 0.0001
MINEhiddens 256 64 128
MLPhiddens 256 64 128
LearningrateofEBM 0.01 0.01 0.01
ICIL
SizeofbufferofEBM 1000 1000 1000
EBMtrainingsteps 1000 1000 1000
EBMhiddens 256 64 128
Koflangevinrollout 60 60 60
λ oflangevinrollout 0.01 0.01 0.01
Var
LearningrateofCritic 0.0003 0.003 0.0003
Critichiddens 256 64 128
LearningrateofActor 0.0003 0.001 0.0001
Actorhiddens 256 64 128
TD3+BC
Targetupdaterate 0.005 0.001 0.0001
Policynoise 0.2 0.2 0.2
Balancecoefficientα 1.0 2.5 2.5
xbeliefsize 256 64 128
ybeliefsize 256 64 128
zbeliefsize 0 0 0
xstatesize 33 110 22
DenoisedMDP
ystatesize 33 110 22
zstatesize 0 0 0
embeddingsize 256 64 128
Learningrate 0.0001 0.001 0.0001
Allhiddendim 256 64 128
IFactor Disentangledprioroutputsize 19 7 10
Learningrate 0.0001 0.001 0.0001
Allhiddendim 256 64 128
MnM Discriminatorlearningrate 0.0001 0.001 0.0001
Discriminatorclipnorm 0.25 0.25 0.25
Regweight 0.0001 0.001 0.0001
CCIL Initialmaskprobability 0.95 0.95 0.95
Learningrate 0.0001 0.001 0.0001
Ensemblemodelsize 5 5 5
Uncertaintypenaltyweight 0.0001 0.0001 0.00005
Delphic
KLweight 0.0001 0.0001 0.00005
33