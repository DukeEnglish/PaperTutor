No Train, all Gain: Self-Supervised Gradients Improve
Deep Frozen Representations
WalterSimoncini1,∗ SpyrosGidaris2 AndreiBursuc2 YukiM.Asano1
1QUVALab,UniversityofAmsterdam2valeo.ai,Paris,France
Abstract
ThispaperintroducesFUNGI,FeaturesfromUNsupervisedGradIents,amethod
to enhance the features of vision encoders by leveraging self-supervised gra-
dients. Our method is simple: given any pretrained model, we first compute
gradientsfromvariousself-supervisedobjectivesforeachinput. Thesearepro-
jectedtoalowerdimensionandthenconcatenatedwiththemodel’sembedding.
Theresultingfeaturesareevaluatedonk-nearestneighborclassificationover11
datasetsfromvision,5fromnaturallanguageprocessing,and2fromaudio. Across
backbonesspanningvarioussizesandpretrainingstrategies,FUNGIfeaturespro-
videconsistentperformanceimprovementsovertheembeddings. Wealsoshow
that using FUNGI features can benefit linear classification and image retrieval,
and that they significantly improve the retrieval-based in-context scene under-
standing abilities of pretrained models, for example improving upon DINO by
+17% for semantic segmentation – without any training. Code is available at
https://github.com/WalterSimoncini/fungivision.
1 Introduction
The k-nearest neighbor algorithm (kNN) (Fix, 1985) is a fundamental non-parametric machine
learningtool,andcanbescaledtodatasetswithbillionofexamplesthankstoadvancesinquantization
(Jegouetal.,2010;Guoetal.,2020)andefficientGPUimplementations(Johnsonetal.,2019). This
simpleandversatilealgorithmhasshownpotentialinmultipleapplicationswellbeforedeepneural
networksbecamerelevant(Efros&Leung,1999;Hays&Efros,2008;Torralbaetal.,2008). Its
recentapplicationsincludefastandrobustimageclassificationwithVisionTransformers(Caronetal.,
2021;Chen&He,2021),unlabeleddataselection(Yalnizetal.,2019),relevanttext-retrieval(Lewis
etal.,2020),andvisualin-contextlearning(Balazevicetal.,2024),whereacontextofdatasamples
withtheirannotations(e.g.,asemanticsegmentationmap)areusedtomakedensepredictions.
Devisingpowerfulandexpressivefeaturesforrecognitionandimageunderstandinghasalonghistory
incomputervision. Featureengineeringstrategiesrangefromsimplelocalfeatures(Lowe,2004;
Dalal&Triggs,2005;VanDeSandeetal.,2009)extractinggradient,boundaryorcolorinformation,
to various mid-level (Boureau et al., 2010) or global pooling (Oliva & Torralba, 2001; Sivic &
Zisserman,2003;Jégouetal.,2010)techniques. Itisalsopossibletocoupleoff-the-shelfpretrained
backbonesasfeatureextractorswithsuchpoolingstrategies(Gongetal.,2014;Kulkarnietal.,2015;
Gidarisetal.,2020)toimproveperformances. Whiletheseapproachesdemonstratetheutilityof
usinganeuralnetwork’slearnedembeddingspace,theystillrequirespecificexpertiseandtuningfor
eachbackboneandtaskwithonlylimitedguidancefromthedataitself.
∗Workpartiallydoneasaninternatvaleo.ai.DatasetsweresolelydownloadedandevaluatedbytheUniversity
ofAmsterdam.walter@ashita.nl
Preprint.Underreview.
4202
luJ
51
]VC.sc[
1v46901.7042:viXraProject &
Concat
Gradient
Enhanced
Features
Database
FUNGI
Figure1: Gradient-augmentedfeatures: givenapretrainedbackbonef anditsembeddings,we
θ∗
applyafamilyofSSLlosses,extracttheirgradients,andprojectandconcatenatethem. Thesenew
featuresareusedtobuildak-nearestneighborindex,whichcanbeusedforclassificationorretrieval.
We depart from this line of work and aim to attain strong representations without training and
feature engineering, yet still exploiting information cues from data. In particular, we suggest to
enhance the neural network’s embeddings by incorporating FUNGI (Features from Unsupervised
Gradients). FUNGI are obtained from self-supervised loss functions, as these do not require any
humanannotationsandallowforasimpleenhancementtoembedding-onlykNN.Thelossesare
computedontopofpretrainedbackbones(withrandomlyinitializedlinearlayersifneeded),which
permitsourmethodtobe“plug-and-play”andbenefitfromthediversesetofpretrainingobjectives
putforthbythecommunity.
We explore gradients from various learning objectives, such as contrastive learning (Chen et al.,
2020a) and self-distillation (Caron et al., 2021) thereby integrating complementary information
that mitigates the weaknesses of individual losses. The gradients are obtained from late hidden
layers,makingthemcomputationallycheap. Finally,theseareprojectedtosmallerdimensions,and
concatenatedwiththeneuralnetworkembeddings,toyieldnewinputstotheclassickNNalgorithm.
UsingkNNwithFUNGIcanberegardedasanon-parametrictransferlearningapproach: thegradient
informationencodesfirst-orderlearningsignalsthatarespecifictothedownstreamdata,yetnoparam-
etersneedtobeupdated. Despitethissimplicity,weachieveconsistentperformanceimprovements
acrossmultiplemodelsandbenchmarks.
Overall,ourmaincontributionsaresummarizedasfollows:
• WeintroduceFUNGI,anovelmethodthatcombinesneuralnetworkfeaturesandgradients
toenhancerepresentationsforthek-nearestneighboralgorithm.
• Wedemonstratethatthegradientsfromself-supervisedlosseshavepredictiveabilitiesand
offercomplementaryinformationtomodelembeddings.
• Wevalidatethegeneralityandutilityofourmethodbyachievingconsistentgainsacross11
image,5text,and2audioclassificationbenchmarks,plus2in-contextimagesegmentation
and2imageretrievaltasks,utilizingatotalof16backbones.
2 RelatedWork
FastAdaptation Thereisabroadrangeofapproachestoquicklyadaptmodelstonewlyspecified
tasksanddata. Inspiredbyearlylearning-to-learnwork(Hochreiteretal.,2001),meta-learning
methods(Finnetal.,2017;Nicholetal.,2018)learntoinitializetheparametersofalearnersuch
that it becomes faster to fine-tune with a small number of gradient steps and data. Alternative
approachesleverageexternalmemorymodulestostorerelevanttrainingsamplestolearntomatch
queryexamples(Santoroetal.,2016;Vinyalsetal.,2016),learntoproduceandcompareclass-based
prototypes(Snelletal.,2017)orlearntogeneratetheweightsofaclassifier(Gidaris&Komodakis,
2018)orevenofanentireneuralnetwork(Bertinettoetal.,2016)fromonlyafewlabeledexamples.
The advent of Vision Transformers (Dosovitskiy et al., 2020) enable new parameter- and data-
efficientstrategiestoadaptpretrainedmodelsthroughvisualprompts(Jiaetal.,2022)andin-context
learning(Zhangetal.,2023).Incontrasttothislineofworks,ourmethoddoesnotrequirespecialized
trainingandcanbeappliedtoanyfrozenpretrainedbackbone. FUNGIcanalsoberelatedtotest-time
2
{CKA (Flowers) CKA (ESAT) CKA (Cars) Per-Class Accuracy Deltas (Flowers 102)
1
Emb. 100.090.3 82.3 46.6 100.092.9 84.3 43.7 100.095.2 85.6 51.5
KL 90.3100.083.5 48.9 92.9100.085.2 45.1 95.2100.086.6 54.2
0
DINO 82.3 83.5100.053.5 84.3 85.2100.047.9 85.6 86.6100.058.0 0.5
SimCLR 46.6 48.9 53.5100.0 43.7 45.1 47.9100.0 51.5 54.2 58.0100.0 0.0
0.5
Pair Accuracy (Flowers) Pair Acc. (ESAT) Pair Acc. (Cars)
0.5
Emb. 56.9 60.4 64.0 68.0 90.7 92.0 93.2 94.4 32.3 33.2 36.8 37.8 0.0
0.5
KL 60.4 61.4 65.4 70.2 92.0 91.6 94.1 94.6 33.2 32.7 37.0 38.7
DINO 64.0 65.4 65.3 70.2 93.2 94.1 93.1 94.5 36.8 37.0 35.8 38.6 0.5
0.0
SimCLR 68.0 70.2 70.2 67.0 94.4 94.6 94.5 93.6 37.8 38.7 38.6 34.9 0.5
Emb. KL DINO SimCLR Emb. KL DINO SimCLR Emb. KL DINO SimCLR
Figure3:
GradientsCla ess
n
In cd oex
dedifferentinforma-
Figure2: Combiningdiversefeaturesleads tion. The plots show the delta in per-class ac-
tolargeimprovements. Thetopplotsdisplay curacyofk-nearestneighbormodelsfittedwith
thepairwiseCKAsimilaritybetweeneachpair gradientsfromdifferentobjectivescomparedto
offeatures,andthebottomshowsthekNNac- amodelfittedusingtheembeddings,indicated
curaciesforcombiningthese. as“Emb.” intheplot.
training (Sun et al., 2020; Hardt & Sun, 2023), where the parameters of a predictive model are
updatedovertestsampleswithaself-supervisedobjectivetoreducethegapbetweenthetrainingand
testdistributions. Whilewealsouseself-supervisedobjectivesandgradients,ourapproachdoesnot
updatemodelparametersandisnotlimitedtopredictivemodels,asitcanbeappliedtoanytaskthat
canbesolvedwithretrieval.
Self-SupervisedLearningObjectives Inrecentyears,self-supervisedlearning(SSL)hasmade
tremendousprogressincomputervision. SSLaimstolearngoodrepresentationsfromunlabeled
databyleveragingsupervisionfromdifferentsignalsinthedataitselfviapretextobjectives,thus
foregoinghumansupervision. Modelspretrainedwithself-supervisionaresubsequentlyfinetunedto
downstreamtasksofinterestwithfewlabeledsamples. ThecruxofSSLisinthepretextlearning
objective. Awideanddiversecollectionofpretextobjectiveshavebeenproposedinthecommunity
relyingoncontrastivelearning(Chenetal.,2020a;Heetal.,2020;Chenetal.,2020b),clustering
(Caronetal.,2018;Asanoetal.,2020;Caronetal.,2020),self-distillation(Caronetal.,2021;Grill
etal.,2020;Chen&He,2021;Gidarisetal.,2021),feature(Zhouetal.,2022;Assranetal.,2023)or
inputreconstruction(Heetal.,2022). Wehypothesizethatthegradientsinducedbytheseobjectives
encapsulatedifferentinformationfromtheinputdata,andthatthisinformationcanbecombinedto
producemoreinformation-richrepresentations. Here,wedonotuseself-supervisionintheusualway,
i.e.,topretrainanencoder,butratherfocusonpretextobjectivesanddataaugmentationstrategiesto
computerepresentationsfromafrozenpretrainedmodel.
FeatureEngineering Along-standingresearchareaforpatternrecognitionandimageunderstand-
ingbeforetheadventofdeepneuralnetworksthatbroughttheparadigmofend-to-endrepresentation
learning. In contrast, classic feature extraction methods are devised without labeled data and of-
ten from only a few data samples. They range from local features, such as SIFT (Lowe, 2004),
HOG(Dalal&Triggs,2005),toglobalpooling,suchasGIST(Oliva&Torralba,2001),Bag-of-
Visual-Words (Sivic & Zisserman, 2003), Fisher vectors (Perronnin et al., 2010), VLAD (Jégou
etal.,2010),selectivematchkernels(Toliasetal.,2013),etc. Thesepoolingstrategiescanbeeasily
pluggedtointermediateoroutputneuralnetworkactivations(Gongetal.,2014;Kulkarnietal.,2015;
Gidaris et al., 2020), harnessing data-driven learned representations. Other modern examples of
featureengineeringincludekNN-prompting(Xuetal.,2023)thatusesthenexttokenprobabilities
of a language model to perform few shot nearest neighbor classification, LOST (Siméoni et al.,
2021)whichusespatchfeaturesfromself-supervisedvisiontransformersforobjectlocalizationand
FroFA(Bäretal.,2024),whichappliesdataaugmentationtodeepfeatures. FUNGIcanalsobeseen
asaformoffeatureengineering: bymakinguseofdifferentpretexttasks, weareabletoextract
multiplecomplementaryfeaturesfromapretrainedbackbone.
3
CKA
Accuracy
.bmE
LK
ONID
RLCmiS(2) Compute Loss & Backpropagate
Attract
(1) Patchify and Forward Repel
(3) Extract
Per-Sample
Gradients (4) Project Fixed Negative Batch
Figure 4: Gradients extraction using a SimCLR loss. Given a pretrained backbone f and a
randomlyinitializedprojectionheadh,wefirstpatchifyanimage,obtainthelatentrepresentationsof
patches(1),calculatetheSimCLRlossbymaximizingthepairwisecosinesimilarityofpatches,and
minimizingtheirsimilaritytoafixednegativesbatchandbackpropagate(2),extracttheper-sample
gradients(3)andfinallyprojectthegradientstothesamedimensionalityastheembeddings(4).
3 WhatGradientstoCombine?
Thecoreassumptionofourmethodisthatgradientsencodecomplementaryinformationtothemodel
embeddings. WeshowthatthisholdsinFigure2,wherewecomparethecenteredkernelalignment
(CKA)(Kornblithetal.,2019)scoreofdifferentfeaturepairstotheaccuracyink-nearestneighbor
classificationoftheircombination. Theplotshowsthatfeaturecombinationsalwaysperformbetter
thanasinglefeatureandthatcombiningmorediversefeaturesleadstobetterdownstreamperformance.
WefurtherhighlightthedifferencebetweengradientsandembeddingsinFigure3,whichshowsthat
performingk-nearestneighborclassificationwithgradientsyieldsdifferentpredictionscompared
totheembeddingsandthattheper-classaccuracydistributionchangesacrossthetypeofgradient-
featuressuggestingthattheyarecomplementarytobothembeddingsandeachother.
4 Method
Ourmethod,FUNGI,enhancesk-nearestneighborsearchbyincorporatingfeaturesfromunsupervised
gradients. Weextractgradientsfromself-supervisedlossfunctions,projectthemtosmallerdimen-
sions, and concatenate them with neural network embeddings. The extraction of self-supervised
gradientsisillustratedinFigure4,whileFigure1showshowFUNGIfeaturesareconstructed.
Definitions Throughoutthissection,wedefineL normalizationasz′ =z/||z|| ,avisionbackbone
2 2
asf,alinearprojectionheadashandvectorizationasvec(·).
4.1 FUNGI: FeaturesfromUnsupervisedGradients
GradientsExtraction. Givenanarbitraryvisionbackbonef,whichinourcaseisavisiontrans-
former(ViT)(Dosovitskiyetal.,2020),weattacharandomlyinitializedlinearprojectionheadh
andobtainalatentrepresentationz =h(f′(x))oftheinputimages,whichweusetocomputethe
lossforoneofourself-supervisedobjectives. Wethenrunbackpropagationandextractthegradients
withrespecttotheweightsandbiasesofanarbitraryhiddenlinearlayerwithinf. Unlessspecified
otherwise, we use the attention output projection of the last transformer block as our gradient’s
source.
FromGradientstoRetrieval-FriendlyFeatures. Gradientsarehighdimensionalandthusim-
practical for nearest-neighbor retrieval due to speed and storage considerations and the curse of
dimensionality. Totackletheseissues,wedownsamplethegradientstothedimensionalityoforiginal
modelembeddingsdusingtherandomprojectionsintroducedbyAchlioptas(2003). Forthis,we
firstvectorizethegradientsbyflatteningthemtoam-dimensionalvectorandthenmultiplythem
byamatrixR∈{−1,1}d,mwhoseentriesaretherealizationsofaBernoullirandomvariablewith
p=0.5. Thegradientg withrespecttoalossL isthendefinedas
β β
g (x)=Rvec(∇L (x)). (1)
β β
CombiningwithEmbeddings Toproduceour FUNGI featuresϕ, weconcatenateoneormore
gradientstothemodelembeddings. Asgradientmagnitudescanvarywidelyacrosslosses,andwe
4wantgradientstobeequallyconsideredastheembeddings,weL -normalizeeachgradient,aswell
2
theembeddingsandcompute
ϕ(x)=cat(cid:2) g′ (x),g′ (x),...,f′(x)(cid:3) , (2)
β1 β2
wherecatdenotesconcatenation. Finally,wereducethedimensionalityofthesecombinedfeatures
viaPCAtoad-dimensionalvector. Thisallowsthecombinationofmultiplelossesatiso-storagecost.
OurfinalFUNGIfeaturesforasamplexarethusobtainedas:
ϕ (x)=PCA (ϕ(x)). (3)
PCA d
4.2 Self-SupervisedObjectives
Weconsiderlossesrepresentingthreefamiliesofself-supervisedobjectives: DINO(Caronetal.,
2021), SimCLR (Chen et al., 2020a) and a KL-divergence based loss inspired by the out-of-
distributiondetectionliterature(Huangetal.,2021). Inthissectionwebrieflydescribetheobjectives
andouradjustmentstothem,andinAppendixA.4,wealsobrieflydiscussclusteringandmasked
imagemodeling-basedlosses.
DINO.DINOisadistillationandimplicitclustering-basedlearningmethod. Weusethestandard
DINOloss,which,givenanimage,enforcesglobalandlocalcropcorrespondencebetweenteacher
andstudentmodelsusingacross-entropyloss. Inourcase,bothmodelssharethesameparameters,
but have independent heads h and h for student and teacher respectively, thus we have z =
s t i
h (f′(x)),i∈{s,t}. TheDINOobjectivecanbeexpressedas:
i
L =Cross-Entropy(z ,z ). (4)
DINO s t
SimCLR.SimCLRisanoise-contrastivemethod. Givenabatchofimages,SimCLRgeneratestwo
viewsforeachimageandaimstominimizethedistancebetweenviewsbelongingtothesameimage
andmaximizetheirdistancetoallotherviews. Instead,wegenerateasetof49overlappingpatches
foreachimage,whichwecallthepositiveset. Thissetisthencontrastedagainstafixedcomparison
batchof49×256negativeexamples. Ourobjectiveistheexpectationofthepair-wiseInfoNCE
(Oordetal.,2018)lossforeachpairofpositiveviews. Ifwedefinethepositivesetoflatentview
representationsasZ,wherez ∈Z =h′(f(x ))foraviewx ,andthecomparisonbatchsizeasN
i i i
theL objectiveisthendefinedas:
SimCLR
exp(sim(z ,z )/τ)
L =E [ℓ ] ℓ =−log i j . (5)
SimCLR (zi,zj)∼Z,zi̸=zj zi,zj zi,zj (cid:80)49(N+1)1
exp(sim(z ,z )/τ)
k=1 [k̸=i] i k
KLDivergence. TheKLobjectiveiscalculatedastheKLdivergencebetweenthesoftmaxedlogits
ofthelatentsandauniformdistributionU:
L =KL(softmax(z)||U). (6)
KL
Wehypothesizetworeasonsasforwhythislossproducespredictivegradients: first,itreceivesa
non-augmentedimage,withahighersignal-to-noiseratiocomparedtootherobjectives,andsecond,
if we assume that similar images (e.g., the ones that belong to the same class) produce similar
activations,thenmaximizingtheirentropybyforcingtheoutputdistributiontomatchanuniform
shouldproducesimilarintra-classgradientsandhelpseparability. Thishypothesisissupportedby
thefactthatwhiletheKLgradientsarediscriminative,theyhavechanceperformanceinothertasks,
suchasin-contextsceneunderstanding.
4.3 In-ContextSceneUnderstanding
Balazevicetal.(2024)introducedamethodforretrieval-basedin-contextsceneunderstanding,where,
forsemanticsegmentation,theyfirstbuildamemorybankcontainingtrainingimagepatchesandtheir
labels,andattesttime,foreachimagepatch,retrieveitsnearestneighborsandusethemtopredictits
labelusinganattentionmechanism. Imagesarefirstresizedto512×512,andthenencodedasaset
of322 =1024patchfeaturesusingaViTwithpatchsize16.
We enhance the patch features using SimCLR gradients, obtained by contrasting the input patch
tokensagainsttheirnearestneighborsfromasupportindexbuiltwithScaNN(Guoetal.,2020). We
usethereproductionofthisevaluationprotocolbyParizaetal.(2024)torunourexperiments.
5Full Dataset Few Shot
0.90 0.60
0.85 +2.0% +1.0% 0.55 E FUm Nb Ge Iddings +4.8% +1.4%
0.80 +4.0% +1.8% 0.50 +5.3% +2.9%
0.75
00 .. 67 50 +3.5% +5.8% +2.6% +4.9% +1.9% +2.9% +1.9% 00 .. 44 05 +2.8% +3.4% +3.7% +4.6% +2.1% +3.0% +1.2%
0.60 0.35
0.55 0.30
0.50 0.25
AR VI iTN -1 S/K 16 D VieI T-T B/32 Mo ViC To -v B/3 16 D VieI T-T B/16 AR I ViN T-2 S1 /K 16 AR I ViN T-2 B1 /K 16 D VI iN T-O B/16 C ViLI T-P B/16 AR I ViN T-2 L1 /K 16 DI VN iTO -v B/2 14 EVA Vi TC -L BI /P 16 AR VI iTN -1 S/K 16 D VieI T-T B/32 Mo ViC To -v B/3 16 D VieI T-T B/16 AR I ViN T-2 S1 /K 16 AR I ViN T-2 B1 /K 16 D VI iN T-O B/16 C ViLI T-P B/16 AR I ViN T-2 L1 /K 16 DI VN iTO -v B/2 14 EVA Vi TC -L BI /P 16
Figure5: FUNGI worksacrossbackbones. Accuracyink-nearestneighborclassificationusing
embeddings and FUNGI features from various ViT backbones, both for full dataset and few shot
setups,averagedover11datasets. FortheFUNGIfeatureswechosethebestperformingcombination
acrossdatasets. “AR”indicatesbackbonestrainedwiththeAugRegstrategy(Steineretal.,2021).
Table1: FUNGIfeaturesarebetteronseveraldatasets.AccuracyofembeddingsandFUNGIfeatures
inkNNclassificationover11datasets,fortwoAugReg(Dosovitskiyetal.,2020;Steineretal.,2021)
ViT-B/16modelsfromtimm(Wightman,2019)pretrainedonIN1KandIN21K.
Pretrain Cars CUB DTD ESAT C100 C10 Pets Food IN1K FGVC Flowers Mean
Fulldataset
Embeddings IN1K 21.3 42.0 54.3 89.0 66.3 89.4 87.3 52.3 77.2 17.9 53.8 59.2
FUNGI IN1K 27.2 50.1 58.6 93.4 69.7 90.7 89.5 58.9 78.8 21.4 61.6 63.6↑4.4
Embeddings IN21K 21.0 74.0 58.4 91.8 58.4 82.9 83.6 70.6 72.1 23.0 95.0 66.4
FUNGI IN21K 25.1 74.2 65.0 94.7 63.5 85.7 85.7 73.4 74.5 24.3 96.6 69.3↑2.9
5-Shot
Embeddings IN1K 9.4 23.7 32.5 38.6 36.9 48.8 57.5 20.1 55.7 8.3 41.2 33.9
FUNGI IN1K 11.4 26.6 33.9 42.2 38.6 50.2 59.4 24.1 58.6 9.2 49.8 36.7↑2.8
Features IN21K 7.6 50.0 33.7 47.7 23.2 39.7 53.3 32.0 40.3 10.7 86.2 38.6
FUNGI IN21K 9.2 48.5 36.3 54.5 28.2 41.7 51.0 37.8 45.4 12.2 85.8 41.0↑2.4
5 Experiments
Inthissection,weevaluatetheperformanceofFUNGIink-nearestneighborimageandtextclassi-
ficationandretrieval-basedin-contextsceneunderstanding. Furtherexperiments,includingimage
retrievalandaudioclassification,areprovidedinAppendixA.
5.1 ImageClassification
FollowingCaronetal.(2021),weevaluateourFUNGIfea-
tures using the task of kNN classification. To show the 0.8
generalizabilityofourmethod,weevaluateourfeatures 0.7
acrossViTbackbones(Dosovitskiyetal.,2020)withvary- 0.6
ingmodelsizesandpretrainingstrategies,includingboth 0.5
supervisedandself-supervisedmethods. 0.4
Embeddings
Weconductourexperimentson11diversedownstream 0.3 KL + SimCLR Gradients
datasets,describedinAppendixC.Unlessotherwisespec- 0.2
2 3 4 5 6 7 8 9 10
ified,wereporttheaverageaccuracyacrossthesedatasets. Shots
WeevaluateourfeaturesusingthekNNimplementationof Figure6: Betterdata-efficiency. kNN
scikit-learn(Pedregosaetal.,2011)withmajorityvoting accuracyofembeddingsandFUNGI (us-
over20neighbors,forfulldatasetandfewshotscenarios, ingonlyKLandSimCLRgradients)on
the latter using five examples per class, to analyze the ImageNet-100usingaDeIT-B/16back-
efficacyofourapproachinlow-datascenarios. bonewhenonlykshotsareused.
Ourresults,presentedinFigure5,showthatFUNGIconsistentlyimprovesthekNNperformanceof
allViTmodels,regardlessofmodelsizeorpretrainingstrategy,bothforthefulldatasetandinfew
shotscenarios.
6
ycaruccA
ycaruccATable2: Performanceimprovesasmoregradientsareused. Accuracyinimageclassification
usingkNNwithembeddingsandFUNGI,averagedacross11datasetsfor7backbones,forstandard
andfewshotsetups. ResultsforadditionalbackbonesareshowninTable8. “K”,“D”and“S”stand
forKL,DINOandSimCLR,respectively.
DINOv2 DINO DeIT MoCov3 DeIT AugRegIN1K AugRegIN1K
ViT-B/14 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/32 ViT-S/16 ViT-B/16
FullDataset
Embeddings 79.9 69.0 65.3 63.2 61.7 60.8 59.2
+K 80.6↑0.7 69.4↑0.4 66.3↑1.0 63.4↑0.2 63.3↑1.6 60.3↓0.5 58.9↓0.3
+K+D 81.3↑1.4 70.1↑1.1 68.1↑2.8 64.7↑1.5 65.7↑4.0 62.6↑1.8 61.1↑1.9
+K+D+S 81.7↑1.8 70.9↑1.9 70.1↑4.8 65.8↑2.6 67.3↑5.6 64.3↑3.5 63.6↑4.4
FewShot
Features 47.6 39.3 38.4 34.7 36.2 34.9 33.9
+K 48.1↑0.5 39.4↑0.1 38.7↑0.3 35.8↑1.1 36.6↑0.4 34.2↓0.7 33.5↓0.4
+K+D 49.1↑1.5 39.7↑0.4 39.1↑0.7 36.6↑1.9 37.6↑1.4 35.0↑0.1 34.5↑0.6
+K+D+S 50.3↑2.7 40.5↑1.2 41.1↑2.7 38.2↑3.5 39.0↑2.8 36.5↑1.6 36.7↑2.8
Table3: FUNGI featuresimprovein-contextsemanticsegmentation. mIoUforretrieval-based
semanticsegmentationonPascalVOC2012,comparingaDINObaselineagainstFUNGIfeaturesand
theself-supervisedHummingBirdmodel. ResultsfromBalazevicetal.(2024)aremarkedwith‡.
Weresizeeachimageto512×512andextract322 =1024patchfeatures.
MemoryBankSize
Backbone Features 1024×102 1024×103 1024×104
DINOViT-S/16 Embeddings 37.2 43.1 46.6
DINOViT-S/16 FUNGI 50.7↑13.5 56.3↑13.2 58.0↑11.4
DINOViT-B/16 Embeddings 44.9 50.8 55.7
DINOViT-B/16 FUNGI 62.1↑17.2 66.1↑15.3 67.0↑11.3
HummingBirdViT-B/16‡ Embeddings - - 70.5
Figure7: FUNGIproducessharperandmorecompletesegmentationmasks. Segmentationmasks
producedvianearestneighborretrievalusingDINOfeatures(left),FUNGI(center)andtheground
truth(right). Bothmethodsuseamemorybankof1024×104patches.
Wefurtherinvestigatedata-efficientsettingsinFigure6,whereFUNGIshowsasignificantimprove-
mentwhen3to6shotsareused,highlightingthepotentialofFUNGIinlow-dataregimes.
InTable1,weshowthat,withsomeexceptions, FUNGIprovidesconsistentimprovementsacross
datasetsfortwoAugReg(Steineretal.,2021)ViT-B/16backbones,pretrainedonIN1KandIN21K,
withFUNGIprovidingbetterresultsontheformer. WefurtherdiscusstheseresultsinSection6.
Lastly,inTable2weshowthatperformanceimprovesasmoregradientsfromdifferentself-supervised
objectivesareused.
5.2 In-ContextSceneUnderstanding
Inthissection,weassesstheeffectivenessofourapproachinthetaskofretrieval-basedsemantic
segmentationonPascalVOC2012(Everinghametal.,2010)andADE20K(Zhouetal.,2019,2017).
WeusethetrainaugandtrainsplitstobuildthememorybanksforVOCandADE20K,respectively,
andreportthemeanintersectionoverunion(mIoU)onthevalidationset.
We apply FUNGI to the DINO ViT-S/16 and ViT-B/16 models. Our results, presented in Table 3
andTable7,demonstratethatFUNGIsignificantlyenhancesDINO’sperformanceacrossallmemory
7Table4:Data-efficientsemanticsegmentation.mIoUscoresfordata-efficientretrieval-basedseman-
ticsegmentationonPascalVOC2012andADE20K,usingDINObackbonesandtheirFUNGIfeatures
andembeddings. WealsocompareFUNGItoend-to-endfine-tuningandfindourmethodtoperform
bestforVOC.ResultsfromBalazevicetal.(2024)aremarkedwith‡.
DatasetSize
PascalVOC ADE20K
Backbone Features Decoder 1/128(n=83) 1/64(n=165) 1/128(n=158) 1/64(n=316)
ViT-B/16‡ - E2EFT 36.1 44.3 11.7 14.4
ViT-S/16 Emb. NN 26.3 31.8 8.8 10.0
ViT-S/16 FUNGI NN 29.1↑2.8 34.0↑2.2 10.2↑1.4 12.3↑2.3
ViT-B/16 Emb. NN 32.2 39.0 9.3 11.3
ViT-B/16 FUNGI NN 38.0↑5.8 46.8↑7.8 11.7↑2.4 13.7↑2.4
Table5: FUNGIfeaturesareusefulforthetextmodality. Top-1accuracyinkNNtextclassification
forthefulldatasetandfewshotsetups. “K”and“S”standforKLandSimCLR,respectively.
TREC Banking-77 SST(FineGrained) AGNews Tweet-Eval
Full 5-shot Full 5-shot Full 5-shot Full 10-shot Full 5-shot
BERTBase
Features 83.6 20.0 55.4 14.5 40.0 20.4 88.8 45.8 23.8 13.6
+K 85.6↑2.0 27.6↑7.6 67.1↑11.7 22.2↑7.7 40.7↑0.7 23.2↑2.8 91.0↑2.2 61.4↑15.6 24.4↑0.6 13.8↑0.2
+K+S 86.8↑3.2 23.0↑3.0 67.9↑12.5 23.8↑9.3 41.8↑1.8 18.4↓2.0 89.6↑0.8 61.9↑16.1 24.8↑1.0 14.5↑0.9
T5Small
Features 88.6 25.6 29.7 5.2 30.0 25.9 71.8 37.4 23.4 8.4
+K 88.6 23.6↓2.0 33.3↑3.6 5.6↑0.4 32.7↑2.7 24.1↓1.8 74.3↑2.5 40.6↑3.2 24.2↑0.8 9.5↑1.1
+K+S 88.4↓0.2 23.6↓2.0 29.1↓0.6 6.1↑0.9 32.0↑2.0 24.2↓1.7 74.8↑3.0 41.0↑3.6 24.4↑1.0 9.9↑1.5
banksizes,withthemostsubstantialimprovementsobservedinsmallermemorybanksforVOC.
Qualitatively,FUNGIproducessharperandmorecompletesegmentationmasks,asshowninFigure7.
Notably,theDINOViT-B/16model,whenenhancedwithourFUNGIapproach,achievescompetitive
results against the current state-of-the-art HummingBird model (Balazevic et al., 2024), with a
differenceofonly3.5%onVOCand3.1%onADE20K,withoutanytraining. Thisisaparticularly
promisingresult,asHummingBirdemploysaself-supervisedpretrainingstrategythatisspecialized
forretrieval-baseddensepredictiontasks,whicharethefocusofourevaluationinthisstudy.
In addition, we evaluate the efficacy of FUNGI in a data-efficient setup, and report the results in
Table 4. Our findings indicate that our method outperforms DINO in this scenario, even when
comparedtoend-to-endfine-tuningofDINOonthedownstreamtaskforPascalVOC.
5.3 OtherModalities
Naturallanguage. Weusefivedatasets,describedinAppendixC,andtwotransformerlanguage
models: BERTbaseuncased(Devlinetal.,2018)andT5-small(Raffeletal.,2020). WeusetheL
KL
andL lossesandobtaintheSimCLRviewsbyrandomlydeletingwordswitha10%probability.
SimCLR
TheresultsarepresentedinTable5,andshowthatFUNGIachievesimprovementsinthetextdomain.
However,SimCLRgradientsstrugglewithsomedatasets. Differentdataaugmentationstrategies,
such as back-translation (Sennrich et al., 2016), or language-specific self-supervised losses, e.g.,
maskedlanguagemodeling(Devlinetal.,2018),mayyieldmorediscriminativegradients. Weleave
thisinvestigationforfuturework.
Audio. WedemonstrategainsfortheaudiomodalityinTable11,whereweimprovetheESC-50
kNNclassificationaccuracyfrom42.8%to47.0%andSpeechCommandsfrom27.4%to29.9%with
anSSASTbackbone(Gongetal.,2022). FurtherdetailsareprovidedinAppendixA.2.
5.4 AblationStudies
Projection head. To compute our self-supervised losses, we first L -normalize the model em-
2
beddings(exceptforSimCLR)andthenprojectthemusingarandomlyinitializedlinearhead. We
8Table6: Impactoftheprojectionheadconfiguration. Top-1accuracyofgradientsonImageNet-
100ink-nearestneighborclassificationversustheprojectionheadconfigurationforKL,DINOand
SimCLRgradients. “norm”indicateswhetherthefeaturesareL -normalizedbeforebeingprojected.
2
AsfeaturesarealwaysL -normalizedfortheSimCLRobjective,the“empty”headconfigurationis
2
notapplicable. Thedefaultsetupismarkedin cyan.
∇ ∇ ∇
KL DINO SimCLR
Norm Projection Acc. Norm Projection Acc. Norm Projection Acc.
88.3 79.3 N/A
✓ 87.3 ✓ 87.8 ✓ 88.7
✓ 88.8 ✓ 84.7 ✓ 88.8
✓ ✓ 89.1 ✓ ✓ 90.1 ✓ ✓ 88.7
motivatethischoiceempiricallybyablatingthesecomponents,andtheresultsinTable6showthat
thisconfigurationproducesthemostpredictivegradientsforImageNet-100.
KL DINO SimCLR
1.0
0.8
0.6 embeddings
attn.qkv
0.4
attn.proj
0.2 mlp.fc1
mlp.fc2
0.0
1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
Layer # Layer # Layer #
Figure8: Gradientsfromdeeperlayersaremorepredictive. Top-1accuracyofgradientsobtained
fromeverylayerofasupervisedDeITViT-B/16ink-nearestneighborclassificationonImageNet-100
fortheKL,DINO,andSimCLRobjectives. Thedefaultsetup(lastlayers)ismarkedin cyan.
.
Gradientssourcelayer. Throughoutthepaper,weextractgradientsfromtheself-attentionoutput
projectionofthelasttransformerblock. Intuitively,deeperlayersprovidemorepredictivefeatures,
and thus, their gradient should display the same behavior. This assumption is confirmed by our
resultsinFigure8,where,foralllosses,deeperlayersconsistentlyproducemorepredictivegradients.
Regardingthechoiceoflayerwithinatransformerblock,forshallowerblocks,thesecondMLPlayer
issignificantlymorepredictive,buttheperformancegapbecomesinsignificantaswemovetowards
deeperblocks,favoring(byasmallmargin)theattentionoutputprojection,Thislayerisalsomore
memoryefficient,asitusesfewerparameterscomparedtootherlayers.
6 DiscussionandConclusion
Broaderimpact. OurmethodimprovesthefeaturesusedforthekNNalgorithm. Assuch,itisa
fundamentalcontributiontoMachineLearning. GiventheubiquitoususeofkNN,ourmethodcould
havepositiveconsequences, suchasimprovingreliabilityandfactuality inRetrievalAugmented
Generation(RAG)systems,whereLanguageModelsaregroundedinretrievedpiecesoftextbefore
generatingananswer. Wedonotforeseeanydirectnegativeconsequencecausedbyourmethod.
Impact of pretraining dataset. Our method works with various backbones, model sizes, and
pretraining strategies. However, we have observed that the benefits diminish as the size of the
pretraining dataset increases: in Table 1, FUNGI provides a smaller relative improvements on a
backbonepretrainedwithIN21KcomparedtoonepretrainedonIN1K,andsimilarly,inTable14the
relativeimprovementoverEVA-CLIP(Sunetal.,2023)issmallercomparedtoCLIP(Radfordetal.,
2021),astheyarepretrainedon2Band400Mtext-imagepairsrespectively.
Computationalefficiency. ComputingFUNGIfeaturesintroducesanoverhead,whichwemeasure
inTable22bycomparingthethroughputofaDeITViT-B/16whenextractinggradientsandembed-
dings. TheDINOandSimCLRlosseshavethelargestoverhead,astheyforward12and49viewsper
9
ycaruccAimage,respectively. AsshowninAppendixB.1,thisnumbercanbereduced,ataperformancecost.
However,thankstoourdimensionalityreduction,thespeedofkNNretrievalisnotimpacted.
7 Conclusion
Wehaveshownthatgradientsfromself-supervisedobjectiveshavepredictiveabilitiesandencode
complementaryinformationtothemodelembeddings. Buildingonthosefindings,weintroduced
FUNGI,whicheffectivelycombinesembeddingsandgradientsintopowerfulfeaturesforretrieval-
basedtasks. Specifically,wehaveshownthatFUNGIenhancetheperformanceofkNN-basedimage
andtextclassificationacrossmodels,pretrainingstrategies,anddownstreamdatasets,bothforfull
datasetandfewshotsetups.Moreover,wehaveshownthatFUNGIsignificantlyboosttheperformance
ofDINOfeaturesforretrieval-basedsemanticsegmentationtasks.
Acknowledgements. WeacknowledgetheuseoftheDutchnationalsupercomputerSnelliusfor
runningtheexperimentspresentedinthispaper.
References
DimitrisAchlioptas. Database-friendlyrandomprojections: Johnson-lindenstrausswithbinarycoins.
JCSS,2003. 4
YukiM.Asano,ChristianRupprecht,andAndreaVedaldi. Self-labellingviasimultaneousclustering
andrepresentationlearning. InICLR,2020. 3
MahmoudAssran,QuentinDuval,IshanMisra,PiotrBojanowski,PascalVincent,MichaelRabbat,
YannLeCun,andNicolasBallas. Self-supervisedlearningfromimageswithajoint-embedding
predictivearchitecture. InICCV,2023. 3
IvanaBalazevic,DavidSteiner,NikhilParthasarathy,ReljaArandjelovic´,andOlivierHenaff.Towards
in-contextsceneunderstanding. NeurIPS,2024. 1,5,7,8,16,20,21
AndreasBär,NeilHoulsby,MostafaDehghani,andManojKumar. Frozenfeatureaugmentationfor
few-shotimageclassification. arXivpreprintarXiv:2403.10519,2024. 3
Francesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis Espinosa-Anke, Miguel
Ballesteros,ValerioBasile,VivianaPatti,andHoracioSaggion. Semeval2018task2: Multilingual
emojiprediction. InIWSE,2018. 23
FrancescoBarbieri,JoseCamacho-Collados,LuisEspinosaAnke,andLeonardoNeves. TweetEval:
Unifiedbenchmarkandcomparativeevaluationfortweetclassification. InEMNLP,2020. 23
Luca Bertinetto, João F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning
feed-forwardone-shotlearners. InNeurIPS,2016. 2
LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecompo-
nentswithrandomforests. InECCV,2014. 22,23
Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for
recognition. InCVPR,2010. 1
MathildeCaron,PiotrBojanowski,ArmandJoulin,andMatthijsDouze. Deepclusteringforunsuper-
visedlearningofvisualfeatures. InECCV,2018. 3,18
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervisedlearningofvisualfeaturesbycontrastingclusterassignments. NeurIPS,2020. 3
MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InICCV,2021. 1,2,
3,5,6,24
IñigoCasanueva, TadasTemcˇinas, DanielaGerz, Matthew Henderson, andIvan Vulic´. Efficient
intentdetectionwithdualsentenceencoders. arXivpreprintarXiv:2003.04807,2020. 22,23
10TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastivelearningofvisualrepresentations. InICML,2020a. 2,3,5
XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning. InCVPR,2021. 1,
3
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastivelearning. arXivpreprintarXiv:2003.04297,2020b. 3
XinleiChen,SainingXie,andKaimingHe. Anempiricalstudyoftrainingself-supervisedvision
transformers. InICCV,2021. 24
MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi. De-
scribingtexturesinthewild. InCVPR,2014. 22,23
NavneetDalalandBillTriggs. Histogramsoforientedgradientsforhumandetection. InCVPR,
2005. 1,3
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018. 8,
24
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020. 2,4,6
AlexeiAEfrosandThomasKLeung. Texturesynthesisbynon-parametricsampling. InICCV,1999.
1
M.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zisserman. Thepascalvisualobject
classes(voc)challenge. IJCV,2010. 7
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationof
deepnetworks. InICML,2017. 2
EvelynFix.Discriminatoryanalysis:nonparametricdiscrimination,consistencyproperties,volume1.
USAFschoolofAviationMedicine,1985. 1
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
CVPR,2018. 2
Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord. Learning
representationsbypredictingbagsofvisualwords. InCVPR,2020. 1,3
SpyrosGidaris,AndreiBursuc,GillesPuy,NikosKomodakis,MatthieuCord,andPatrickPérez.
Obow: Onlinebag-of-visual-wordsgenerationforself-supervisedlearning. InCVPR,2021. 3
Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc.
Interspeech2021,pp.571–575,2021. doi: 10.21437/Interspeech.2021-698. 17,18,24
YuanGong,Cheng-ILai,Yu-AnChung,andJamesGlass. Ssast: Self-supervisedaudiospectrogram
transformer. InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume36, pp.
10699–10709,2022. 8,17,18,24
YunchaoGong,LiweiWang,RuiqiGuo,andSvetlanaLazebnik. Multi-scaleorderlesspoolingof
deepconvolutionalactivationfeatures. InECCV,2014. 1,3
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya,CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. InNeurIPS,2020. 3
AntonioGulli.Ag’scorpusofnewsarticles.http://groups.di.unipi.it/~gulli/AG_corpus_
of_news_articles.html,2005. Accessed: 2020-05-21. 23
11RuiqiGuo,PhilipSun,ErikLindgren,QuanGeng,DavidSimcha,FelixChern,andSanjivKumar.
Acceleratinglarge-scaleinferencewithanisotropicvectorquantization. InICML,2020. 1,5,21
MoritzHardtandYuSun. Test-timetrainingonnearestneighborsforlargelanguagemodels. arXiv
preprintarXiv:2305.18466,2023. 3
JamesHaysandAlexeiAEfros. Im2gps: estimatinggeographicinformationfromasingleimage. In
CVPR,2008. 1
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervisedvisualrepresentationlearning. InCVPR,2020. 3
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencodersarescalablevisionlearners. InCVPR,2022. 3,16,24
PatrickHelber,BenjaminBischke,AndreasDengel,andDamianBorth. Eurosat: Anoveldatasetand
deeplearningbenchmarkforlanduseandlandcoverclassification. JST-AEORS,2019. 22,23
SeppHochreiter,AStevenYounger,andPeterRConwell. Learningtolearnusinggradientdescent.
InICANN,2001. 2
RuiHuang,AndrewGeng,andYixuanLi. Ontheimportanceofgradientsfordetectingdistributional
shiftsinthewild. NeurIPS,2021. 5
HerveJegou,MatthijsDouze,andCordeliaSchmid. Productquantizationfornearestneighborsearch.
TPAMI,2010. 1
HervéJégou,MatthijsDouze,CordeliaSchmid,andPatrickPérez. Aggregatinglocaldescriptors
intoacompactimagerepresentation. InCVPR,2010. 1,3
MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,BharathHariharan,and
Ser-NamLim. Visualprompttuning. InECCV,2022. 2
JeffJohnson, MatthijsDouze, andHervéJégou. Billion-scalesimilaritysearchwithgpus. TBD,
2019. 1
SimonKornblith,MohammadNorouzi,HonglakLee,andGeoffreyHinton. Similarityofneural
networkrepresentationsrevisited. InICML,2019. 4
Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of
fine-grainedcars. 2013. 22,23
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
22,23
Praveen Kulkarni, Joaquin Zepeda, Frederic Jurie, Patrick Perez, and Louis Chevallier. Hybrid
multi-layerdeepcnn/aggregatorfeatureforimageclassification. InICASSP,2015. 1,3
PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,
HeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,etal. Retrieval-augmentedgenera-
tionforknowledge-intensivenlptasks. InNeurIPS,2020. 1
XinLiandDanRoth. Learningquestionclassifiers. InCOLING,2002. 22,23
DavidGLowe. Distinctiveimagefeaturesfromscale-invariantkeypoints. IJCV,2004. 1,3
JulienMairal. Cyanure: Anopen-sourcetoolboxforempiricalriskminimizationforpython,c++,
andsoonmore. arXivpreprintarXiv:1912.08165,2019. 17
S.Maji,J.Kannala,E.Rahtu,M.Blaschko,andA.Vedaldi. Fine-grainedvisualclassificationof
aircraft. Technicalreport,2013. 22,23
AlexNichol,JoshuaAchiam,andJohnSchulman. Onfirst-ordermeta-learningalgorithms. arXiv
preprintarXiv:1803.02999,2018. 2
12Maria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumber
ofclasses. InICVGIP,2008. 22,23
AudeOlivaandAntonioTorralba. Modelingtheshapeofthescene: Aholisticrepresentationofthe
spatialenvelope. IJCV,2001. 1,3
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictive
coding. arXivpreprintarXiv:1807.03748,2018. 5
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. TMLR,2023. 24
Valentinos Pariza, Mohammadreza Salehi, and Yuki Asano. Hummingbird evaluation for vision
encoders,42024. URLhttps://github.com/vpariza/open-hummingbird-eval. 5
OmkarMParkhi,AndreaVedaldi,AndrewZisserman,andCVJawahar. Catsanddogs. InCVPR,
2012. 22,23
F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Pretten-
hofer,R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,and
E.Duchesnay. Scikit-learn: MachinelearninginPython. JMLR,2011. 6
FlorentPerronnin,JorgeSánchez,andThomasMensink. Improvingthefisherkernelforlarge-scale
imageclassification. InECCV,2010. 3
JamesPhilbin,OndrejChum,MichaelIsard,JosefSivic,andAndrewZisserman. Objectretrieval
withlargevocabulariesandfastspatialmatching. In2007IEEEconferenceoncomputervision
andpatternrecognition.IEEE,2007. 16,17
JamesPhilbin,OndrejChum,MichaelIsard,JosefSivic,andAndrewZisserman.Lostinquantization:
Improvingparticularobjectretrievalinlargescaleimagedatabases. In2008IEEEconferenceon
computervisionandpatternrecognition.IEEE,2008. 16,17
KarolJ.Piczak. ESC:DatasetforEnvironmentalSoundClassification. InProceedingsofthe23rd
AnnualACMConferenceonMultimedia,pp.1015–1018.ACMPress,2015. ISBN978-1-4503-
3459-4. doi: 10.1145/2733373.2806390. URL http://dl.acm.org/citation.cfm?doid=
2733373.2806390. 17,23
FilipRadenovic´,AhmetIscen,GiorgosTolias,YannisAvrithis,andOndˇrejChum. Revisitingoxford
andparis: Large-scaleimageretrievalbenchmarking. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,2018. 16
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021. 9,16,24
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. JMLR,2020. 8,24
OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal. Imagenetlargescalevisualrecognition
challenge. IJCV,2015. 22,23
AdamSantoro,SergeyBartunov,MatthewBotvinick,DaanWierstra,andTimothyLillicrap. Meta-
learningwithmemory-augmentedneuralnetworks. InICML,2016. 2
RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneuralmachinetranslationmodels
withmonolingualdata. InACL,2016. 8
Oriane Siméoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick
Pérez,RenaudMarlet,andJeanPonce. Localizingobjectswithself-supervisedtransformersand
nolabels. InBMVC,2021. 3
13SivicandZisserman. Videogoogle: Atextretrievalapproachtoobjectmatchinginvideos. InICCV,
2003. 1,3
JakeSnell,KevinSwersky,andRichardZemel.Prototypicalnetworksforfew-shotlearning.NeurIPS,
30,2017. 2
RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherD.Manning,AndrewNg,and
ChristopherPotts. Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.
InEMNLP,2013. 22,23
AndreasSteiner,AlexanderKolesnikov,XiaohuaZhai,RossWightman,JakobUszkoreit,andLucas
Beyer. Howtotrainyourvit? data,augmentation,andregularizationinvisiontransformers. arXiv
preprintarXiv:2106.10270,2021. 6,7,16,19,24
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training
techniquesforclipatscale. arXivpreprintarXiv:2303.15389,2023. 9,16,24
YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritzHardt. Test-timetraining
withself-supervisionforgeneralizationunderdistributionshifts. InICML,2020. 3
GiorgosTolias,YannisAvrithis,andHervéJégou. Toaggregateornottoaggregate: Selectivematch
kernelsforimagesearch. InICCV,2013. 3
AntonioTorralba,RobFergus,andWilliamTFreeman. 80milliontinyimages: Alargedatasetfor
nonparametricobjectandscenerecognition. TPAMI,2008. 1
HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHervé
Jégou.Trainingdata-efficientimagetransformers&distillationthroughattention.InICML.PMLR,
2021. 24
KoenVanDeSande,TheoGevers,andCeesSnoek. Evaluatingcolordescriptorsforobjectandscene
recognition. TPAMI,2009. 1
OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal. Matchingnetworksforone
shotlearning. NeurIPS,2016. 2
CatherineWah,SteveBranson,PeterWelinder,PietroPerona,andSergeBelongie. Thecaltech-ucsd
birds-200-2011dataset. 2011. 22,23
PeteWarden. Speechcommands:Adatasetforlimited-vocabularyspeechrecognition. arXivpreprint
arXiv:1804.03209,2018. 17,23
Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models,2019. 6,16,24
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
vonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,
MariamaDrame,QuentinLhoest,andAlexanderM.Rush. Transformers: State-of-the-artnatural
languageprocessing. InEMNLP,2020. 24
Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. k
nnprompting: Beyond-contextlearningwithcalibration-freenearestneighborinference. arXiv
preprintarXiv:2303.13824,2023. 3
IZekiYalniz,HervéJégou,KanChen,ManoharPaluri,andDhruvMahajan. Billion-scalesemi-
supervisedlearningforimageclassification. arXivpreprintarXiv:1905.00546,2019. 1
XiangZhang,JunboJakeZhao,andYannLeCun. Character-levelconvolutionalnetworksfortext
classification. InNeurIPS,2015. 23
YuanhanZhang,KaiyangZhou,andZiweiLiu. Whatmakesgoodexamplesforvisualin-context
learning? InNeurIPS,2023. 2
14BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBarriuso,andAntonioTorralba. Scene
parsingthroughade20kdataset. InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,2017. 7
BoleiZhou,HangZhao,XavierPuig,TeteXiao,SanjaFidler,AdelaBarriuso,andAntonioTorralba.
Semanticunderstandingofscenesthroughtheade20kdataset. InternationalJournalofComputer
Vision,127(3):302–321,2019. 7
JinghaoZhou,ChenWei,HuiyuWang,WeiShen,CihangXie,AlanYuille,andTaoKong. ibot:
Imagebertpre-trainingwithonlinetokenizer. InICLR,2022. 3,18
15A AdditionalExperimentalResults
Inthissectionwefirstillustrateadditionalresultsthatsolidifythefindingsdiscussedinthemain
paper,followedbyevaluationsofFUNGIinimageretrieval,audioclassification,linearclassification
of image features and a brief investigation of the performance of gradients from clustering and
maskedimagemodelingself-supervisedobjectives.
In Table 8, we report the performance of embeddings and FUNGI features for some additional
backbones, includingCLIPandEVA-CLIPmodels, forwhich, asexplainedinthemaintext, we
experiencediminishingreturnsasthepretrainingdatasetsizegrows. Moreover,forbothmodels,the
SimCLRlossdoesnotproducepredictivegradients,whichwehypothesizeisduetomodelsbeing
saturatedtoacontrastiveloss,astheyuseasimilarobjectiveforpretraining.
In Table 9, we report the mean accuracy and one standard deviation (computed via numpy.std)
acrossthreeseedsforasubsetofourdatasets,usingaDeITViT-B/16backbone,showingthatthe
performanceimprovementsofFUNGIareconsistentacrossseeds.
InFigure11weevaluatetheeffectivenessofFUNGIacrossdifferentViTmodelsizes. Thefindings
showthatFUNGIimprovestheresultsforallthreeViTmodels(ViT-S,ViT-B,andViT-L),withthe
biggestimprovementsobservedintheViT-Bmodel.
InTable7,wereporttheperformanceofFUNGIforin-contextretrieval-basedsemanticsegmentation
onADE20K,andshowthatourmethodoutperformsDINOacrossallmemorybanksizesandis
competitiveagainstHummingBird.
Table7: FUNGIfeaturesimprovein-contextsemanticsegmentationonADE20K.Wereportthe
mIoUforretrieval-basedsemanticsegmentationonADE20K,comparingaDINObaselineagainst
FUNGIfeaturesandtheself-supervisedHummingBirdmodel. ResultsfromBalazevicetal.(2024)
aremarkedwith‡. Weresizeeachimageto512×512andextract322 =1024patchfeatures.
MemoryBankSize
Backbone Features 1024×102 1024×103 1024×104
DINOViT-S/16 Embeddings 11.4 14.5 17.0
DINOViT-S/16 FUNGI 16.1↑4.7 20.0↑5.5 22.3↑5.3
DINOViT-B/16 Embeddings 14.5 18.3 20.8
DINOViT-B/16 FUNGI 19.2↑4.7 23.5↑5.2 25.2↑4.4
HummingBirdViT-B/16‡ Embeddings - - 28.3
Table8: Additionalbackbones. AverageaccuracyofembeddingsandFUNGIfeaturesink-nearest
neighborclassificationacross11datasetsforCLIP(Radfordetal.,2021;Sunetal.,2023),AugReg
(Steineretal.,2021)fromtimm(Wightman,2019),andmaskedautoencoder(Heetal.,2022)models.
“K”,“D”and“S”standforKL,DINOandSimCLR,respectively.
EVACLIP AugRegIN21K CLIP AugRegIN21K AugRegIN21K MAE
ViT-B/16 ViT-L/16 ViT-B/16 ViT-B/16 ViT-S/16 ViT-B/16
FullDataset
Features 83.2 74.7 73.7 66.4 65.6 24.0
+K 83.8↑0.6 75.0↑0.3 76.9↑3.2 67.1↑0.7 65.3↓0.3 44.4↑20.4
+K+D 84.1↑0.9 76.2↑1.5 77.7↑4.0 68.6↑2.2 67.1↑1.5 45.4↑21.4
+K+D+S 83.4↑0.2 76.5↑1.8 74.6↑0.9 69.3↑2.9 67.6↑2.0 38.8↑14.8
FewShot
Features 53.1 47.7 43.0 38.6 37.4 7.5
+K 54.0↑0.9 48.3↑0.6 47.2↑4.2 40.2↑1.6 37.7↑0.3 18.5↑11.0
+K+D 54.1↑1.0 48.5↑0.8 47.9↑4.9 40.3↑1.7 38.7↑1.3 19.2↑11.7
+K+D+S 53.7↑0.6 50.2↑2.5 44.4↑1.4 41.0↑2.4 39.5↑2.1 14.0↑6.5
A.1 ImageRetrieval
Weevaluatetheperformanceof FUNGI featuresinimageretrievalusingtherevisited(Radenovic´
etal.,2018)Oxford(Philbinetal.,2007)andParis(Philbinetal.,2008)landmarksdatasets. We
reportthemeanaverageprecision(mAP)forboththemedium(M)andhard(H)splits.
16Table9: TheperformanceofFUNGIisconsistentacrossseeds. Averageaccuraciesink-nearest
neighborclassificationandonestandarddeviationforFUNGIfeatureson8datasets,measuredacross
threeseedsusingaDeITViT-B/16backbone. “K”,“D”and“S”standforKL,DINOandSimCLR,
respectively.
Cars CUB DTD ESAT Pets Food FGVC Flowers
Embeddings 32.3 56.0 58.6 90.7 90.8 60.5 23.5 56.9
+K 33.5±0.2 57.9±0.1 60.4±0.2 91.6±0.2 91.3±0.2 61.5±0.1 22.9±0.1 60.7±0.6
+K+D 36.2±0.2 60.9±0.2 63.1±0.3 93.4±0.1 91.8±0.0 65.2±0.2 24.2±0.4 64.3±0.5
+K+D+S 39.3±0.3 63.6±0.3 64.1±0.2 95.0±0.2 92.1±0.2 67.3±0.1 28.3±0.6 69.3±0.5
Forthistask,weuseFUNGIfeaturesbuiltwithDINOandKLgradients,astheSimCLRgradients
didnotresultingoodretrievalperformance. TheresultsaredisplayedinTable10andshowthat
FUNGI features improve the retrieval abilities of all backbones, except DINOv2. Our method is
particularlyeffectivewhenappliedonCLIPbackbones: ontheParishardsplit,weimproveby12.4%
and7.2%forCLIPandEVA-CLIP,respectively.
Table10: FUNGI improvesimageretrieval. Meanaverageprecision(mAP)ofembeddingsand
FUNGI forretrievalontheParis(Philbinetal.,2008)andOxford(Philbinetal.,2007)landmarks
datasets,forbothmedium(M)andhard(H)splits.“K”and“D”standforKLandDINO,respectively.
DINOv2 DINO CLIP EVACLIP DeIT
ViT-B/14 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16
Oxford M H M H M H M H M H
Embeddings69.7 42.0 39.2 11.0 31.4 10.8 36.7 12.7 36.6 12.6
+K 70.4↑0.7 42.6↑0.6 38.6↓0.6 11.6↑0.6 40.4↑9.0 14.5↑3.7 39.9↑3.2 13.9↑1.2 36.9↑0.3 12.6↑0.0
+D 69.4↓0.3 41.2↓0.8 40.4↑1.2 12.9↑1.9 41.4↑10.0 15.3↑4.5 40.9↑4.2 14.8↑2.1 38.8↑2.2 12.7↑0.1
+K+D 70.1↑0.4 42.0↑0.0 39.8↑0.6 13.0↑2.0 42.6↑11.2 14.7↑3.9 41.2↑4.5 14.9↑2.2 39.1↑2.5 12.8↑0.2
Paris M H M H M H M H M H
Embeddings89.4 77.5 63.8 37.6 64.6 40.4 69.8 46.7 63.0 37.2
+K 88.7↓0.7 76.2↓1.3 64.5↑0.7 38.4↑0.8 74.7↑10.1 52.8↑12.4 75.2↑5.4 53.9↑7.2 63.6↑0.6 37.7↑0.5
+D 89.0↓0.4 76.2↓1.3 65.6↑1.8 39.0↑1.4 72.0↑7.4 47.8↑7.4 72.9↑3.1 50.2↑3.5 65.7↑2.7 40.2↑3.0
+K+D 88.7↓0.7 75.9↓1.6 65.8↑2.0 39.5↑1.9 75.1↑10.5 52.5↑12.1 74.9↑5.1 53.0↑6.3 65.6↑2.6 40.0↑2.8
A.2 AudioClassification
WeevaluateFUNGIontheaudiomodalityusingSSAST(Gongetal.,2021,2022),aself-supervised
audio spectrogram transformer trained for audio and speech classification, as the backbone. We
constructFUNGIfeaturesusing∇ KLand∇ SimCLR,andtesttheirperformanceink-nearestneighbor
classificationonESC50(Piczak,2015)andSpeechCommandsV2(Warden,2018).
We use the same formulation as in the vision experiments for the L and L objectives.
KL SimCLR
However,forthelatter,weobtain16viewsofthesameclipbyaddinguniformnoisefollowingGong
etal.(2022). Ifwedefinethefilterbankofanaudioclipasc∈Rh,w,thenoise-augmentedclipcˆis
computedas:
x ·x
cˆ=c+ 1 2 x ∼U(0,1)h,w x ∼U(0,1). (7)
10 1 2
Finally, cˆis shifted by a factor sampled from a discrete uniform distribution U(−10,10). The
completelistofhyperparametersusedfortheaudioclassificationexperimentsisreportedinTable12.
TheresultsarereportedinTable11,andshowthatFUNGIfeaturesbuiltusing∇ KLyieldpromising
results, improving by up to 4.2% on the baseline. On the other hand, using ∇ does not
SimCLR
consistentlyyieldimprovements. Itratheroftencausesaperformancedropwhencombinedwith
∇ . Aswithtextclassification,furtherresearchisneededtodeterminetheoptimalself-supervised
KL
objectivesanddataaugmentationtoextractpredictivegradients.
A.3 LinearClassificationofImageFeatures
WeevaluateFUNGIfeaturesinlogisticregression,usingtheimplementationfromthecyanurelibrary
(Mairal,2019).Wetraineachclassifierforamaximumof300epochs(30inthecaseofImageNet-1K)
usingL regularization. Foreachdatasetandfeaturecombination(i.e.,embeddings,embeddings+
2
∇ ,etc.),wepickthebestregularizationparameterbetween5linearlyspacedvaluesintheinterval
KL
17Table11: FUNGI worksforaudio. Top-1accuraciesink-nearestneighboraudioclassificationof
embeddingsandFUNGIfeaturesobtainedfromaSSASTbackbone(Gongetal.,2022,2021). “K”
and“S”standforKLandSimCLR,respectively.
ESC50 SpeechCommandsV2
Full 5-shot Full 5-shot
Features 42.8 20.0 27.4 5.3
+K 47.0↑4.2 21.2↑1.2 29.9↑2.5 6.1↑0.8
+S 45.2↑2.5 19.0↓1.0 25.4↓2.0 5.5↑0.2
+K+S 45.8↑3.0 21.0↑1.0 27.3↓0.1 5.8↑0.5
Table 12: Audio classification experimental details. Parameters used to extract audio encoder
gradientsfortheL (left)andL (right)objectives.
KL SimCLR
Parameter Value
Parameter Value PositiveViews 16
NegativeViews 2
Temperature 15
ProjectionDim 768
ProjectionDim 768
NegativesBatchSize 64×2
Temperature 0.07
[5×10−6,5×10−4]usingthevalidationset. Fordatasetswithoutavalidationset,wegenerateone
usingan80/20stratifiedsplit. Thefinalmodelistrainedusingtheentiretrainingdataset.
WereporttheresultsinTable13andTable14andfindthat,inlinearclassification,FUNGIfeatures
aremosteffectiveforbackbonespretrainedusingasupervisedobjective. Incontrast,self-supervised
backbones do not benefit as much. This is especially evident for DINO and DINOv2, where
FUNGI often yields worse results, especially in a few shot scenarios. Contrary to the k-nearest
neighbor classification results, the best features combination is backbone specific. In Figure 9,
we show that significant performance improvements can be achieved by picking the best feature
combinationforeachbackbone.
A.4 AdditionalGradients
Inthissection,westudytheperformanceofgradientsobtainedbytwoadditionalself-supervised
objectives, DeepCluster (Caron et al., 2018) and iBOT (Zhou et al., 2022) in k-nearest neighbor
classificationonImageNet-100,usingaDeITViT-B/16backbone. DeepClusterisaself-distillation
andexplicitclustering-basedself-supervisedmethodthatalternatesbetweenclusteringimagefeatures
andtrainingamodeltopredictclusterassignments. iBOTisanextensionofDINOthatcombines
imageandpatch-levelobjectives,thelatterimplementedasalatent-spacemaskedimagemodeling
(MIM)objective,wherealearnablepatchtokenreplacesasubsetofpatches,andthemodelmust
reconstructthem.
The results are displayed in Figure 10, and show that the objectives used in this work achieve
performancesonparwiththemodelembeddings, evensurpassingtheminthecaseofDINO.At
the same time, iBOT and DeepCluster instead produce gradients with relatively poor predictive
performance. Fortheformer,apossiblereasonisthatitincorporatesadenseloss,whosegradients
maynothelptodiscriminateexamplesontheimagelevel. RegardingDeepCluster,modelspretrained
using this strategy had worse performance in retrieval tasks compared to supervised pretraining
(Caronetal.,2018),whichmayexplainthepoorretrievalabilitiesofitsgradients.
A.5 AdditionalAblations
DINOdataaugmentationandhead. Tomaximizethesignal-to-noiseratio,weonlyuselocal
and global crops for the DINO data augmentation. We validate this choice empirically, and the
resultsinTable15showthatrandomcropsproducemorediscriminativegradientscomparedtothe
standarddataaugmentationpolicy. Moreover,wealsoempiricallyvalidatethechoiceofusingtwo
18Full Dataset Few Shot
1.00 0.90
0.95 0.85 E FUm Nb Ge Iddings
0.90 +0.8% +0.5% 00 .. 78 50 +0.9% 0.0%
0.85 +2.0% +0.4% 0.70 +4.1% +1.2%
0.80 +3.3% +1.2% +1.1% +2.1% +2.4% +0.2% 00 .. 66 05 +3.0% +1.6% +2.2% +1.1% +2.5% -0.5%
0.75 +1.6% +1.6%
0.55
0.70 0.50
0.65 0.45
AR VI iTN -1 S/K 16 D VieI T-T B/32 AR I ViN T-2 S1 /K 16 AR I ViN T-2 B1 /K 16 Mo ViC To -v B/3 16 D VieI T-T B/16 D VI iN T-O B/16 C ViLI T-P B/16 AR I ViN T-2 L1 /K 16 EVA Vi TC -L BI /P 16 DI VN iTO -v B/2 14 AR VI iTN -1 S/K 16 D VieI T-T B/32 AR I ViN T-2 S1 /K 16 AR I ViN T-2 B1 /K 16 Mo ViC To -v B/3 16 D VieI T-T B/16 D VI iN T-O B/16 C ViLI T-P B/16 AR I ViN T-2 L1 /K 16 EVA Vi TC -L BI /P 16 DI VN iTO -v B/2 14
Figure9: FUNGIworksacrossbackbonesforlinearprobing. Accuracyinlogisticregression-based
image classification of embeddings and FUNGI features on various ViT backbones, both for full
datasetandfewshotsetups,averagedover11datasets. FortheFUNGIfeatures,wechosethebest
performingcombinationacrossdatasets. “AR”indicatesAugRegbackbones(Steineretal.,2021).
Table13: Thebestgradientsforlinearprobingarebackbone-specificforthemainbackbones.
Averageaccuracyacross11datasetsforlogisticregression-basedimageclassificationofembeddings
andFUNGIfeatures,constructedusingmultiplegradientcombinations,forbothstandardandfew
shotsetupsusing7backbones. “K”,“D”and“S”standforKL,DINOandSimCLR,respectively.
DINOv2 DINO DeIT MoCov3 DeIT AugRegIN1K AugRegIN1K
ViT-B/14 ViT-B/16 ViT-B/16 ViT-B/32 ViT-B/32 ViT-B/16 ViT-S/16
FullDataset
Features 88.3 81.0 78.2 77.3 73.1 71.6 71.4
+K 88.3↑0.0 80.4↓0.6 78.5↑0.3 77.7↑0.4 73.4↑0.3 70.9↓0.7 70.5↓0.9
+K+D 88.8↑0.5 81.2↑0.2 80.7↑2.5 79.4↑2.1 76.2↑3.1 73.0↑1.4 73.0↑1.6
+K+D+S 88.7↑0.4 80.7↓0.3 80.5↑2.3 78.7↑1.4 76.4↑3.3 73.1↑1.5 73.0↑1.6
FewShot
Features 76.7 62.9 61.0 58.0 57.2 54.8 54.4
+K 76.3↓0.4 62.2↓0.7 61.7↑0.7 57.6↓0.4 57.8↑0.6 54.4↓0.4 53.5↓0.9
+K+D 76.7↑0.0 62.4↓0.5 63.5↑2.5 59.2↑1.2 60.2↑3.0 56.5↑1.7 55.9↑1.5
+K+D+S 76.6↓0.1 61.6↓1.3 63.4↑2.4 58.7↑0.7 60.1↑2.9 57.0↑2.2 56.0↑1.6
independentheadsfortheDINOlossinTable15,showingthatthischoiceisbeneficialforkNN
classification.
PCA. WeusePrincipalComponentAnalysis(PCA)tocombinedatafrommultiplelossesatan
iso-storageandretrievalspeedcost. GivenadatasetofFUNGIfeatures,wefitthePCAonthetraining
splitanduseittotransformbothtrainingandtestsplits. Table16liststhePCAdimensionalitiesused
foreachmodelarchitectureandshowsthattheydonotcauseadecreaseinperformancebutrather
provideaminorimprovement.
B ExperimentalDetails
B.1 VisionNearestNeighborClassificationExperimentalDetails
Hyperparameters. Weusethreelossestoextractgradientsfromvisionbackbones: L ,L
KL DINO
andL . TheexactparametersforeachlossareshowninTable17. Thissetofparametersis
SimCLR
used consistently across backbones and datasets. While L and L are robust to the choice
KL DINO
ofhyperparameters,L isparticularlysensitivetothenumberofpositiveviews,asshownin
SimCLR
Figure12,whereperformanceincreasesinalogarithmicfashionasmorepositiveviewsareused,at
thecostofgradientextractionspeed. Whilethisbehaviorisconsistentacrossdatasets,ithasthemost
significantimpactindatasetswithmanyclasses,e.g.,Flowers102.
SimCLRdataaugmentationandlossdetails. Givenanimage,wepatchifyitin49overlapping
viewsasfollows: wefirstresizetheinputimageto(224,224),andthenextract49patchesofsize
112×112,usingastridecorrespondingto1/6ofthepatchsize. Nootherstyleorcoloraugmentation
isused. Asthenumberofpatchesincreases,sodoesthememoryrequiredtocomputethelossandthe
19
ycaruccATable14: Thebestgradientsforlinearprobingarebackbone-specificfortheadditionalback-
bones. Average accuracy across 11 datasets for logistic regression-based image classification of
embeddingsandFUNGIfeatures,constructedusingmultiplegradientcombinations,forbothstandard
and few shot setups using 6 backbones. “K”, “D” and “S” stand for KL, DINO and SimCLR,
respectively.
EVACLIP AugRegIN21K CLIP AugRegIN21K AugRegIN21K MAE
ViT-B/16 ViT-L/16 ViT-B/16 ViT-B/16 ViT-S/16 ViT-B/16
FullDataset
Features 86.9 82.9 81.8 76.8 75.6 38.6
+K 87.2↑0.3 82.0↓0.9 82.6↑0.8 76.1↓0.7 74.4↓1.2 63.4↑24.8
+K+D 87.8↑0.9 83.3↑0.4 83.9↑2.1 77.9↑1.1 76.6↑1.0 66.2↑27.6
+K+D+S 87.7↑0.8 83.2↑0.3 83.0↑1.2 77.8↑1.0 76.8↑1.2 63.1↑24.5
FewShot
Features 78.0 68.9 66.2 58.4 57.6 23.9
+K 78.6↑0.6 69.0↑0.1 69.3↑3.1 59.1↑0.7 57.6↑0.0 36.0↑12.1
+K+D 78.9↑0.9 70.1↑1.2 70.4↑4.2 60.7↑2.3 59.3↑1.7 37.3↑13.4
+K+D+S 77.5↓0.5 69.5↑0.6 65.9↓0.3 59.8↑1.4 58.6↑1.0 32.3↑8.4
Gradients Accuracy on ImageNet-100 Improvement vs Model Size
0.94 features
0.76 Embeddings +1.87%
0.92 FUNGI
0.74
0.90
0.88 0.72
0.86 0.70 +2.95%
0.84 +1.93%
0.68
0.82
0.66
0.80
Deep DINO KL iBOT iBOT SimCLR
Cluster (No MIM) (MIM) ViT S ViT B ViT L
Figure10: Notallobjectivesproducegood Figure11: Gainsacrossbackbonesizes. Ac-
predictive gradients. Top-1 accuracy in k- curacy in k-nearest neighbor image classifi-
nearest neighbor classification of gradients cationaveragedacross11datasetsusingthe
obtained from different self-supervised ob- model embeddings and FUNGI features ex-
jectives using a DeIT ViT-B/16 backbone. tractedfromAugRegbackbonesofincreasing
“MIM”standsformaskedimagemodeling. size.
gradients. Thisproblemcanbepartiallytackledbyprecomputingthenegativebatch,whichinour
experimentsispickedrandomlyfromthetrainingdatasetandkeptconstantforeveryinput. Moreover,
wecanobservethattheSimCLRlossisonlydefinedforpositivepairs,soweonlyneedtocompute
thesimilarityofpositivepairstoallotherpairs,significantlyreducingthesizeofthesimilarities
matrixandmemoryusage.
B.2 In-ContextSceneUnderstandingExperimentalDetails
For the evaluation of the in-context scene understanding abilities of FUNGI features we closely
replicatethesetupdescribedbyBalazevicetal.(2024),andusethesameparametersforboththefull
andfewshotevaluations,withtwominorexceptions: (1)weuseasingleaugmentationepochfor
thefulldatasetevaluationand(2)weuseananisotropicquantizationthresholdof0.2forthenearest
neighborindex,asthisparameterwasnotspecifiedinthepaper. Thefullsetofparametersforthe
evaluation,losscomputationanddataaugmentationisreportedinTable18. Asfordataaugmentation,
weusethesamepolicyofBalazevicetal.(2024),andapplyeachaugmentationindependently.
InordertoconstructFUNGIfeaturesforthistask,weimplementaSimCLRlossthatcontrastspatch
tokensfromaninputimagetotheirnearestneighborsretrievedfromasupportingmemorybank. In
practice,we:
20
ycaruccA ycaruccATable15: DINOheadconfigurationanddataaugmentation. Top-1accuracyonImageNet-100in
k-nearestneighborclassificationfortheDINOgradientsusingsharedorindependentteacherand
studentheads(left)andwithrespecttothedataaugmentationpolicy(right).
IndependentHeads Accuracy DataAugmentation Accuracy
✗ 88.4 DINO 88.9
✓ 89.1 RandomCrops 90.1
Table 16: PCA does not degrade performance. PCA output dimensionalities with respect to
thebackbonearchitecture(left)anditsimpactonk-nearestneighborimageclassificationaccuracy
averagedacross11datasetsusingaDeITViT-B/16backbone(right).
Architecture PCADim NoPCA PCA
ViT-S/16 384 Embeddings 65.1 65.3↑0.2
ViT-B/16,ViT-L/16 512 +KL 66.0 66.3↑0.3
BERT,T5 512 +KL+DINO 67.8 68.1↑0.3
SSAST 512 +KL+DINO+SimCLR 69.8 70.1↑0.3
• Constructamemorybankofimagepatchesofthesamesizeastheoneusedforevaluation
anditsnearestneighborindexwithScaNN(Guoetal.,2020)followingtheprocedureby
Balazevicetal.(2024). Wecallthisoursupportindex.
• Then,foreachimage,we:
1. Resizeitto512×512,computeits[CLS]andpatchtokensandprojectthemwitha
linearhead. Excludingthe[CLS]token,eachimageismappedto322 =1024features,
asallourbackbonesusepatchesofsize16.
2. Foreachtoken,retrieveitstwonearestneighborsfromthesupportindexandrandomly
drop50%ofthem.
3. ComputetheSimCLRloss,wherethepatchtokensconstitutethepositivesetandthe
neighborsthenegativebatch. Thisallowsustocomputeaper-patchgradient.
4. Dropthe[CLS]token,asitdoesnotcorrespondtoarealimagepatch.
5. ConstructFUNGIfeaturesasinEquation8,wheref(x)mapsanimagetopatchesof
dimensiond,L normalizationisdefinedasz′ =z/||z|| andcatindicatesconcate-
2 2
nation.
F =cat′(∇′L ,f′(x)) f(x):R3×512×512 →R1024×d (8)
SimCLR
B.3 TextClassificationExperimentalDetails
The parameters used to extract gradients from text encoders for L and L are shown in
KL SimCLR
Table19. Thegradientsourcelayerisalwaystheattentionoutputprojectionofthelasttransformer
Table17: Imagegradientssetup. Dataaugmentationandlossparametersusedtoextractgradients
fromvisionencodersforL ,L andL (lefttoright).
KL SimCLR DINO
Parameter Value
ProjectionDim 2048
Parameter Value GlobalCrops 2
Parameter Value Pos. Views 49 GlobalCropScale 0.25,1.0
Neg. Views 49 GlobalCropSize 224×224
ProjectionDim 768
ProjectionDim 96 LocalCrops 10
Temperature 15
Neg. Batch 256×49 LocalCropScale 0.05,0.25
Temperature 0.07 LocalCropSize 224×224
TeacherTemp. 0.07
StudentTemp. 0.1
21SimCLR Accuracy vs Number of Patches SimCLR Num Patches vs Images/s
67 Accuracy 70 Images/s
60
66
50
65
40
64
30
63
20
62 10
4 9 16 25 36 49 4 9 16 25 36 49
Number of Patches Number of Patches
Figure12: SimCLRissensitivetothenumberofviews. TheSimCLRgradientsmean-per-class
accuracyonFlowers102withrespecttothenumberofpatches(left)andtheimages/sversusthe
numberofpatches(right)usingasupervisedDeITViT-B/16backbone.
Table18: In-contextsceneunderstandingsetup. Parameters(left)anddataaugmentation(right)
usedforthein-contextsceneunderstandingtaskforbothfulldatasetandfewshotsetups. Forthe
computation of L we use 1025 views as we include the [CLS] token, which is discarded
SimCLR
afterwards. The retrieved negatives indicate the number of neighbors retrieved from the support
index,whilethelossnegativesthenumberofneighborsusedforthelosscomputation. Thecolordata
augmentationsareappliedindependently,intheordershownhere.
Parameter FullDataset Few-Shots
MemoryBankSize SeeTable3 2048×104
NearestNeighborsk 30 90
Temperature 0.02 0.1
AugmentationEpochs 1 8 Parameter Value
ScaNNIndex Randomcropp 1.0
NumLeaves 512 512 Scalefactor [0.5,2.0]
NumLeavestoSearch 32 256 Brightnessjitterp 0.5
ReorderingNumNeighbors 120 1800 Contrastjitterp 0.5
DimensionsperBlock 4 4 Saturationjitterp 0.5
AnisotropicQuantization 0.2 0.2 Huejitterp 0.5
L Maxbrightness∆ 0.1
SimCLR
Maxcontrast∆ 0.1
SupportIndexSize SeeTable3 2048×104
Maxsaturation∆ 0.1
ProjectionDim 96 96
Maxhue∆ 0.1
PositiveViews 1025 1025
NegativesBatchSize 1025 1025
SimCLRTemperature 0.07 0.07
RetrievedNegativesperPatch 2 2
LossNegativesperPatch 1 1
encoderblock. Weusethesameparametersacrossbackbones. Nodataaugmentationisusedforthe
L ,whileforL theviewsareobtainedbyrandomlydeletingwordsindependently,where
KL SimCLR
eachwordhasa10%probabilityofbeingdeleted.
C DataandModels
Weinvestigatetheperformanceofourgradient-enhancedfeatureson11imageclassificationdatasets,
namely CIFAR 10 and CIFAR 100 (Krizhevsky et al., 2009), Oxford Flowers 102 (Nilsback &
Zisserman,2008),Food101(Bossardetal.,2014),ImageNet-1K(Russakovskyetal.,2015),FGVC
Aircraft(Majietal.,2013),CUB200-2011(Wahetal.,2011),Oxford-IITPets(Parkhietal.,2012),
StanfordCars(Krauseetal.,2013),DTDTextures(Cimpoietal.,2014)andEuroSAT(Helberetal.,
2019), 5 text classification datasets: TREC (Li & Roth, 2002) in its coarse version, banking-77
(Casanuevaetal.,2020),StanfordSentimentTreebank(SST)(Socheretal.,2013)initsfine-grained
22
ycaruccA s/segamiTable19: Textclassificationexperimentaldetails. Parametersusedtoextracttextencodergradients
fortheL (left)andL (right)objectives.
KL SimCLR
Parameter Value
PositiveViews 12
Parameter Value NegativeViews 2
Temperature 15 ProjectionDim 256
ProjectionDim 768 NegativesBatchSize 256×2
Temperature 0.07
WordDeletionp 0.1
version,AGnews(Zhangetal.,2015;Gulli,2005)andtweeteval(emoji)(Barbierietal.,2018,2020)
and2audioclassificationdatasets: ESC50(Piczak,2015),anenvironmentalsoundclassification
dataset,andSpeechCommandsV2(Warden,2018),akeywordspottingtask,wherethegoalisto
classifyutterancesintoapredefinedsetofwords. Thedatasets,theirlicenseandcitationsarealso
listedinTable20.
We follow the evaluation protocol for each individual dataset and report the top-1 accuracy for
CIFAR10and100,Food101,ImageNet-1K,StanfordCars,EuroSAT,DTDTextures,CUB200-2011,
TREC,banking-77,SST,AGnews,tweeteval(emoji),ESC50andSpeechCommandsV2,andthe
mean-per-classaccuracyforFlowers102,FGVCAircraftandOxford-IITPets.
Weusethedefaultsplitsdefinedbytorchvisionorthedatasetauthorswherepossible. AsEuroSAT
doesnotexplicitlydefineatestsplit,weusean80/20stratifiedsplitasindicatedbythedatasetpaper.
Wealwaysreportmetricsonthetestsplits,withtheexceptionofImageNet,forwhichweusethe
validationsplit.
Table20: Datasets. Summarytableofalldatasetsusedinthispaper,theirlicenseandcitation.
Dataset Type License Citation
CIFAR10 Image Unknown Krizhevskyetal.(2009)
CIFAR100 Image Unknown Krizhevskyetal.(2009)
StanfordCars Image Custom(NonCommercial) Krauseetal.(2013)
DTDTextures Image Custom(ResearchOnly) Cimpoietal.(2014)
EuroSAT Image MIT Helberetal.(2019)
CUB200(2011) Image Custom(ResearchOnly,NonCommercial) Wahetal.(2011)
Oxford-IITPets Image CCBY-SA4.0 Parkhietal.(2012)
Food101 Image Unknown Bossardetal.(2014)
FGVCAircraft Image Custom(ResearchOnly,NonCommercial) Majietal.(2013)
Flowers102 Image Unknown Nilsback&Zisserman(2008)
ImageNet1K Image Custom(ResearchOnly,NonCommercial) Russakovskyetal.(2015)
ImageNet100 Image Custom(ResearchOnly,NonCommercial) Russakovskyetal.(2015)
TREC Text Unknown Li&Roth(2002)
Banking-77 Text CCBY4.0 Casanuevaetal.(2020)
SST Text Unknown Socheretal.(2013)
AGNews Text Custom(NonCommercial) Zhangetal.(2015);Gulli(2005)
TweetEval Text Unknown Barbierietal.(2018,2020)
ESC50 Audio CCBY-NC3.0 Piczak(2015)
SpeechCommandsV2 Audio CCBY4.0 Warden(2018)
WeevaluateFUNGIfeaturesacrossseveralarchitectures,pretrainingstrategiesandmodelsizes. These
arelistedinTable21,alongsidetheirlicense,datatypeandcitation.
D ComputeResources
AllourexperimentswererunonasingleNVIDIAA100GPUwith40GBofVRAM.Considering
theinferencetimeslistedinTable22,replicatingthek-nearestneighborimageclassificationresults
wouldrequireapproximately27GPUhoursperbackboneusingfloat16,whichweusethroughoutall
ourexperiments. Asweevaluateourmethodacross13backbones,reproducingtheseresultswould
require351GPUhours. Asforthetextandaudioclassificationexperiments,theyrequirearound3
GPUhoursperbackbone,foratotalof9hours.
23Table 21: Models used in the paper. Summary table of all architectures/pretraining strategies
evaluatedinthepaper,alongwiththeirlicense,citation,andimplementation,ifapplicable.
Model Type License Citation
MaskedAutoencoders Image CCBY-NC4.0 Heetal.(2022);Wightman(2019)
AugReg Image Apache2.0 Steineretal.(2021);Wightman(2019)
DeIT Image Apache2.0 Touvronetal.(2021)
DINO Image Apache2.0 Caronetal.(2021)
DINOv2 Image Apache2.0 Oquabetal.(2023)
CLIP Image MIT Radfordetal.(2021);Wightman(2019)
EVA-CLIP Image MIT Sunetal.(2023);Wightman(2019)
MoCov3 Image CCBY-NC4.0 Chenetal.(2021)
BERT Text Apache2.0 Devlinetal.(2018);Wolfetal.(2020)
T5 Text Apache2.0 Raffeletal.(2020);Wolfetal.(2020)
SSAST Audio BSD3-Clause Gongetal.(2022,2021)
Table 22: FUNGI introduces a speed overhead. Embeddings and gradients extraction speed
measuredinimages/secondonanNVIDIAA100GPUforaDeITViT-B/16backbone. Thegradients
speedincludetherandomprojectionstep. Theperformancecolumnreportstheaccuracyaveraged
across11datasetsforthecombinationofasinglegradientwiththemodelembeddings. †indicates
k-nearestneighborinferenceonCPU.
FeaturesSource Images/s InferenceSpeed(samples/s)† Performance
Embeddings 479 2700 67.3
∇ 344 2700 68.2↑0.9
KL
∇ 32 2700 70.1↑2.8
DINO
∇ 12 2700 70.9↑3.6
SimCLR
Thein-contextsceneunderstandingexperimentsarerelativelyfast,requiringlessthanoneGPUhour
onaverage,resultinginanupperboundof40GPUhours.
Finally,ourablationstudiesrequiredapproximately38hours,whilethepreliminaryexperimentsfor
thispaperrequiredanegligibleamountofcompute.
E Algorithm
Algorithm1providespytorch-stylepseudocodeforthecomputationofL ,thegradientextraction,
KL
andthecomputationofFUNGIfeatures(withoutPCA).
Algorithm1PyTorchpseudocodefortheKLFUNGIfeatures.
# model, head: the vision backbone and the projection head
# feat_dim, grad_dim: the model features and gradients (as vectors) dimensionality
# projection: the random projection used to downsample gradients
projection = (torch.rand(feat_dim, grad_dim) - 0.5) > 0
uniform = torch.ones(feat_dim) / feat_dim
for x in dataset:
# Extract the feature and its projection
y = model(x)
z = head(y)
kl_div(log_softmax(z), softmax(uniform)).backward() # Calculate the loss and backpropagate
layer = model.blocks.11.attn.proj # Select the target layer
# Extract and project the gradients
gradients = torch.cat([layer.weight.grad, layer.bias.grad.unsqueeze(dim=-1)], dim=-1)
gradients = projection @ gradients.view(-1)
y, gradients = normalize(y), normalize(gradients) # L2 normalize features and gradients independently
feature = torch.cat([y, gradients], dim=-1) # Build the final feature
24