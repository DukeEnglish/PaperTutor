Fine-Tuning and Prompt Optimization:
Two Great Steps that Work Better Together
DilaraSoylu ChristopherPotts OmarKhattab
StanfordUniversity
Abstract these LMs to accurately conduct their easier sub-
tasksandtocommunicateeffectivelywithinmulti-
NaturalLanguageProcessing(NLP)systems
stagepipelines,thiscouldgreatlyexpandthescope
areincreasinglytakingtheformofmulti-stage
ofreliableNLPsystemswecanbuild.
pipelinesinvolvingmultipledistinctlanguage
models(LMs)andpromptingstrategies. Here To this end, Khattab et al. (2023) recently in-
we address the question of how to fine-tune troduced the DSPy framework for defining and
suchsystemstoimprovetheirperformance.We automatically optimizing LM Programs. In it, a
castthisasaproblemofoptimizingtheunder- programisdefinedasafunctionΦthatcomposes
lyingLMweightsandthepromptingstrategies
asetofstages,whichwewillrefertoaslanguage
together,andconsiderachallengingbuthighly
modules M = ⟨M ,...,M ⟩, into a pipeline.
realisticscenarioinwhichwehavenogoldla- 1 |M|
EachmoduleM specifiesafuzzynatural-language
belsforanyintermediatestagesinthepipeline. i
transformation (e.g., generating a summary of a Toaddressthischallenge,weevaluateapproxi-
mateoptimizationstrategiesinwhichweboot- supplieddocument)thatneedstobelearned. Todo
straptraininglabelsforallpipelinestagesand so, each module learns a particular prompt (tem-
use these to optimize the pipeline’s prompts plate) π to make a call to a particular LM with
andfine-tuneitsweightsalternatingly. Inex- weights θ. The optimization problem is then de-
periments with multi-hop QA, mathematical
finedasmaximizingtheexpectedperformance(per
reasoning,andfeature-basedclassification,we
a downstream metric µ) of the program Φ over a
findthatsimpleapproachesforoptimizingthe
setofinputsbyupdatingeachmodule’sπ andθ.
prompts and weights together outperform di-
rectly optimizing weights alone and prompts Existingwork(Khattabetal.,2023;Opsahl-Ong
alonebyupto65%and5%,respectively,onav- et al., 2024) has studied optimizing the discrete
erageacrossLMsandtasks.Wewillreleaseour stringpromptofeachmoduleandhasconsidered
newoptimizersinDSPyathttp://dspy.ai.
simple approaches for fine-tuning each module’s
LMweights. Inthisempiricalstudy,weinvestigate
1 Introduction
updating each module’s prompt and LM weights
While the capabilities of language models (LMs) togethertomaximizeadownstreammetriconthe
continuetogrow,recentworkhasshownthepoten- final output of the program. Doing this is chal-
tialofbuildingmorepowerfulNaturalLanguage lenging as Φ is not generally differentiable and
Processing (NLP) systems by composing multi- itsmodulesM generallylacklabeledoutputsand
i
pleskillsofLMsintopipelines. Examplesofthis exhibitsophisticateddependencies. Moreover,in
include systems for retrieval-augmented genera- realistic settings, the training set is usually very
tion (Guu et al., 2020; Lewis et al., 2020), multi- small and only a small number of LM calls are
hopreasoning(Qietal.,2021;Khattabetal.,2021), possiblefortrainingandinference.
informationextraction(PourrezaandRafiei,2023; To address this challenge, we propose to alter-
D’Oosterlincketal.,2024),andothersophisticated natebetweenoptimizingpromptsandfine-tuning
pipelines(Dohanetal.,2022;Khattabetal.,2022; LM weights and evaluate approximate optimiza-
Beurer-Kellneretal.,2023;Schlagetal.,2023). tion strategies in which we bootstrap training la-
Such LM Programs offer much more control bels for all pipeline modules. In experiments
for designing NLP systems, as they break down withmulti-hopQA(HotPotQA),mathematicalrea-
problemsintomodular,moremanageablesub-tasks soning (GSM8K), and feature-based classification
that can be assigned to LMs. If we could teach (Iris), we show that these tandem strategies
1
4202
luJ
51
]LC.sc[
1v03901.7042:viXraare highly effective across three different LMs, maximizethefollowingobjective:
leading to 5–78% gains for HotPotQA, 2.5–10%
1 (cid:88)
gains for GSM8K, and -5.9–136% gains for Iris argmax µ(Φ (x),m)
|X| ⟨Θ,Π⟩
againstpromptsonlyandweightsonlystrategies, Θ,Π (x,m)∈X
averaged across mistral-7b-instruct-v0.2,
ResearcherstuningLMpipelinesareineffectseek-
llama-2-7b-chat,andllama-3-8b-instruct.
ingtoachievethisobjective. Itisalsoaverylarge
subspaceoftheoptimizationproblemintheDSPy
2 ProblemStatement framework1 forLMprograms. Unfortunately,this
problemisintractable: wedon’thavegradientsor
intermediate output labels to optimize each mod-
We are given an LM program Φ, which operates
ule, so we seek approximate strategies for such
likeablackboxfunctionΦ : X → Y,inwhichX
optimization.
andY aretypicallyinnaturallanguage(e.g.,ques-
tionsandtheirprogramgeneratedanswers,respec- 3 AlternatingPromptandWeight
tively). For example, we may have a program Φ OptimizationStepsforLMPrograms
foransweringcomplexquestionswithshortfactoid
We now introduce the BetterTogether algorithm,
answers. In the course of its execution, Φ makes
which simply alternates prompt and weight opti-
one or more calls to each of |M| ≥ 1 language
mizationstepsforLMprograms. Wehypothesize
modules,M = ⟨M ,...,M ⟩.
1 |M|
that, when an LM is used to teach itself how to
For example, the program may implement a
tacklethetaskdefinedbyanLMprogram,optimiz-
multi-hop,retrieval-augmented pipelineforques-
ingpromptsandfine-tuningLMweightsareboth
tion answering. This common pipeline (Qi et al.,
essentialtoachievethehighestquality. Inparticu-
2021;Khattabetal.,2021;Pressetal.,2023;Khat-
lar,weexpectthat(1)promptoptimizationbefore
tab et al., 2022) breaks down the input into sub-
fine-tuningcanleadtomoresuccessfuldatapoints
questionsthatareusedtoiterativelyfindrelevant
for fine-tuning and (2) prompt optimization after
passages(e.g.,fromacorpuslikeWikipedia)until
fine-tuningcanmakeadjustmentstothebehaviorof
thequestioncanbefaithfullyanswered. Ingeneral
theLMprogramthatleadtohigherquality. Consid-
terms, each module M : X → Y is a declara-
i i i eringthatfine-tuningisoftenperceivedasamore
tiveLMinvocationthatdefines,ininherentlyfuzzy
powerful tool, this can be surprising, especially
natural-languageterms,aninputX domain(likea
i when both forms of optimization are ultimately
user-suppliedquestionandasetofretrievedpas-
appliedoverthesamesetoftraininginputsX.
sages)andanoutputY co-domain(likeasearch
i
querytofindadditionalrelevantpassages). Algorithm1BetterTogether: OptimizingLMpro-
Weseektoimplementeachlanguagemoduleas gramsbyalternatingpromptandweightoptimiza-
somespecific,well-tunedstrategyforinvokingan tionsteps,instantiatedinAlgorithm2
underlyinglanguagemodelLM. Concretely, we Input: ProgramΦ =Φ ⊙Φ ,
⟨Θ,Π⟩ Θ Π
assumethatamoduleM iwillbefullyimplemented withmoduleweightsΘ=[θ 1,...,θ |Φ|]
andmodulepromptsΠ=[π ,...,π ]
by specifying (1) the string prompt π in which 1 |Φ|
i TrainingSetXandMetricµ
the module inputs X i are plugged in to decode 1: functionBETTERTOGETHER(Φ ⟨Θ,Π⟩,X,µ)
the module outputs Y
i
and (2) the floating-point 2: Π′ ←OPTIMIZEPROMPTS(Φ ⟨Θ,Π⟩,X,µ)
weightsθ assignedtotheparametersofLMinthe 3: Θ′ ←FINETUNEWEIGHTS(Φ ⟨Θ,Π′⟩,X,µ)
courseofi
thismodule. WerefertotheversionofΦ
4: Π′′ ←OPTIMIZEPROMPTS(Φ ⟨Θ′,Π⟩,X,µ)
5: returnΦ ⟨Θ′,Π′′⟩
inwhichthepromptsandLMweightsareassigned 6: endfunction
explicitlytoΠandΘ,respectively,asΦ .
⟨Θ,Π⟩
Accordingly, the general optimization frame-
Given nothing but a small training set X =
workforouralgorithmisdefinedinAlgorithm1.
{(x ,m ),...,(x ,m )} of inputs x ∈ X
1 1 |X| |X|) i
GivenaprogramΦ,thealgorithmbeginsbyopti-
and optional metadata like output labels or other
mizingΦ’sprompts,thenfine-tuningitssetofLM
hintsm ∈ Mthatcanbeusedfordeterminingthe
i
weights,andfinallyoptimizingitspromptsagain.
correctness of a given program run, and a metric
µ : Y×M → R,ourgoalistooptimizeΦ,thatis, In principle, each of these steps could be treated
configureitsmodules’promptsandLMweightsto 1http://dspy.ai
2asoptional. Thiswilldefinethedifferentpossible timization, we use BootstrapFewshotRS (BFRS)
combinationsthatwewillseektoevaluateinSec- ofDSPy,whichself-generatespotentialfew-shot
tion4. Specifically, weareinterestedinthequal- examples of every module and applies a form of
ityof(1)thevanillaprogramΦwithsimpleuser- random search (RS) to select the specific gener-
supplied instructions as the prompts and no fine- atedfew-shotexamplesthatareusedforprompting.
tuning of LM, (2) optimizing the prompts only, Overall,BFRSfirstdividesX intoatrainingsplitT
(3)optimizingtheweightsonly,(4)optimizingthe andavalidationsplitV (Line2). Itthenexecutes
promptstwice,i.e. usingtheprompt-optimizedΦ theprovidedΦ onthetraininginputs,collect-
⟨Θ,Π⟩
as a starting point for a second round of prompt ing input–output pairs for every module in Φ for
optimization,(5)optimizingtheweightstwice,(6) eachx ∈ T. Thisiscalledatraceτ,andwekeep
i
optimizingthepromptsthentheweights,(7)vice onlythetracesassignedhighscoresbyµ(Line4).
versa, and (8) optimizing the prompts, weights, Given all of these traces, BFRS samples multiple
thenprompts. Overall,weexpectthefinalthreeto differentsubsetsofafewtracesτ′(Line6),eachof
consistentlyoutperformthefirstfive. themcontainingapotentialfew-shotexamplefor
For our algorithm in Algorithm 1 to be com- eachmoduleinΦ,andultimatelyselectsthesub-
plete,weneedtoinstantiateLines1–3withspecific setthat,whenusedtoconstructfew-shotprompts
approachesforpromptoptimizationandLMfine- (Line7)achievesthehighestscore(Line8). This
tuning. Forthis,wechoosetheBootstrap-∗family simple search strategy is known to consistently
of algorithms from Khattab et al. (2023), which leadstolargequalityimprovementsinprompting
workbyexecutinganinitialversionoftheprogram LM programs (Khattab et al., 2023; Opsahl-Ong
oninputexamples(x ,m ) ∈ X andrecordingthe etal.,2024),oftenoutperformingmanuallyorauto-
i i
inputs/outputsobservedateachmodulewhenthe maticallyoptimizingpromptinstructionsorwriting
finaloutputis“correct”,i.e.,µ(Φ(x ),m ) ≥ λfor examplesbyhand.
i i
some threshold λ (e.g., 1.0 for binary accuracy). Forfine-tuning,weextendBootstrapFinetune
Thisisimportanttonote: inlinewithourformula- (BFT)ofDSPy,whichself-generatesalargenum-
tion,ourpromptandweightoptimizationregimes berexamplesforeverymoduleandcombinesthem
arenotsimplytrainingonhand-labeleddatabuton intoonedatasettofinetunetheLMweightswithan
self-generatedprogramtraces. implicitmulti-taskobjective,wherethesub-tasks
are the modules’ roles. Existing work has only
Algorithm2InstantiatingAlgorithm1’sprompt& considered BFT in a very narrow setting for LM
weightoptimizerswithbootstrappingalgorithms programs: onHotPotQA,Khattabetal.(2023)train
a T5-Large model using traces from a few-shot
Input: TrainingSetXandMetricµ
1: functionBOOTSTRAPFEWSHOTRS(Φ ⟨Θ,Π⟩,X,µ)
Llama2-13bprogram,withoutconsideringgetting
2: T,V ←SPLITINTOTRAINANDVALIDATION(X) anLMtoteachitselfviaBFTnorconsideringarole
3: τ ←BOOTSTRAPTRACES(Φ ⟨Θ,Π⟩,T)
for BFRS in the fine-tuned program. In this work,
4: τ ←FILTERTRACES(τ,µ)
5: InitializeattemptslistA←{} wefocusonallowingmodelstoteachthemselves
6: forτ′ ∈SAMPLEFEWSHOTSUBSETS(τ)do and self-improve. We propose for the first time
7 8 9: :
:
Π σ Ex′ ← t← end|VC 1 A|O (cid:80)N wS
i⟨
tT
x
hR
i,
(U
m
σC
i
,⟩T Π∈F
V
′E )W µ(S ΦH ⟨O ΘT ,ΠP ′R ⟩(O xM i)P ,T mS( iτ )′) c no am tiob nin ti ong get the ths etr sa at meg eie Ls Mof tB oF tR eS aca hn id tsB eF lfT fv ai ra ba el tt te er r-
10: endfor thaneitherpromptorweightoptimizationinisola-
11: returnΠ ,A’shighest-scoringpromptssequence
max tion. (Wecouldalsotestsimilarideasinscenarios
12: endfunction
13: wherealargermodelsdoesthebootstrappingfora
14: functionBOOTSTRAPFINETUNE(Φ ⟨Θ,Π⟩,X,µ) smallerLM.Thismayleadtoevenhigherresults
15: τ ←BOOTSTRAPTRACES(Φ ⟨Θ,Π⟩,X)
butisoutsideourscope.)
16: τ ←FILTERTRACES(τ,µ)
17: Θ′ ←TRAINLM(τ)
18: returnΘ′ 4 ExperimentalEvaluation
19: endfunction
20:
21: SetOPTIMIZEPROMPTSasBOOTSTRAPFEWSHOTRS We now seek to evaluate our hypothesis on the
22: SetFINETUNEWEIGHTSasBOOTSTRAPFINETUNE importance of optimizing both prompts and LM
weights of LM programs. We conduct our eval-
Algorithm 2 shows the instantiations for uation across three datasets that span different
Lines 1–3 of our Algorithm 1. For prompt op- tasks (and thus LM programs) each. In partic-
3mistral-7b-instruct-v0.2 llama-2-7b-chat llama-3-8b-instruct
Strategy
HotPotQA GSM8K Iris HotPotQA GSM8K Iris HotPotQA GSM8K Iris
VanillaZero-shot 17.2 40.3 20.0 13.2 24.0 0.0 31.6 72.7 34.0
PromptOptimization(Π) 33.8 46.4 52.0 33.3 26.0 56.0 46.9 77.9 78.7
WeightOptimization(Θ) 22.9 40.7 28.7 12.2 24.0 – 34.8 75.1 31.3
Π→Π 33.8 47.7 64.0 32.6 24.7 64.0 46.5 77.6 77.3
Θ→Θ 24.0 42.8 31.3 13.0 24.1 – 34.4 44.1 30.7
Π→Θ 36.3 47.3 24.7 32.7 27.3 29.3 42.8 77.6 34.7
Θ→Π 33.0 48.3 65.3 34.2 26.6 – 43.6 78.9 83.3
Π→Θ→Π 37.6 46.8 57.3 34.8 26.3 49.3 46.7 77.0 79.3
Table 1: Main Results. Percentage accuracies of strategies consisting of prompt optimization (Π), weight
optimization(Θ),andtheirpermutationsonHotPotQA,GSM8K,andIrisevaluatedonmistral-7b-instruct-v0.2,
llama-2-7b-chat,llama-3-8b-instruct. Reportedareaverageperformanceofthreerunsonheld-outtestsets
usingdifferentrandomseeds. Settingsthatincludeweightoptimizationasthefirststeprelyonthedata-points
bootstrappedusingthe“VanillaZero-shot”setting. Sincethereweren’tanydata-pointsthatwereansweredcorrectly
byllama-2-7b-chatontheIrisdatasetusingthe‘VanillaZero-shot”setting,thereweren’tanybootstrapped
examplesforweightoptimizationeither. Settingsthatweren’tpossibletorunduetothisaremarkedwith“–”.
ular, we use HotPotQA (Yang et al., 2018) for 2022)retriever. Accuracyismeasuredusingtheex-
multi-hop reasoning, GSM8K (Cobbe et al., 2021) actmatchscoreoftheanswerwiththegroundtruth
forarithmeticreasoning,andIris(Fisher,1988) answer for the given question, after normalizing
forclassification. Unlessotherwisespecified,we case,strippingsurroundingwhitespacecharacters,
use1000trainingsetand500developmentsetex- andremovingpunctuation. Weuseaheld-outset
amples for each dataset. We conduct our main of1500examplesfromtheofficialdevelopmentset
experiments using the same model for prompt toreportourfinalresults,sincetheofficialtestset
optimization, bootstrapping training traces, and isnotpublic.
fine-tuning. We experiment with three mod-
ArithmeticReasoning GSM8Kisapopularbench-
els: mistral-7b-instruct-v0.2 (Jiang et al.,
mark consisting of grade school math problems.
2023), llama-2-7b-chat (Touvron et al., 2023),
WeimplementitasanLMprogramwithasingle
llama-3-8b-instruct(MetaAI,2024).
moduleusingCoTprompting,wheretheLMgen-
Weimplementallofourprogramsandoptimiz-
erates a reasoning string followed by an answer.
ersasextensionstotheDSPyframework. Allevalu-
We report our final results on the entire held-out
ationresultsaretheaverageofthreerandomseeds,
testsetofGSM8K,with1319examples.
which are used to shuffle our training sets before
optimization. Full text for programs is shared in Classification Irisisaclassicclassificationtask
AppendixA.AppendicesBandCreportthelicense in machine learning, where the task is to classify
informationforallLMsanddatasetsusedaswellas species of Iris flowers. We use a single-module
ourimplementationdetails(e.g.,hyperparameters CoTDSPyprogramforIris withthegoalofas-
andsoftware),respectively. sessingwhetheritbeingafeature-basedclassifica-
Multi-hop Reasoning HotPotQA (in the “full- tiontaskgivesalargeadvantagetomethodsbased
entirely on gradient descent (fine-tuning). This
wiki”setting)isaquestionansweringtaskinwhich
teststheextrapolationofourhypothesistoavery
systemsmustfindtwoWikipediapagesviasearch
different setting from the other two tasks. We re-
andusethemtoanswerafactoidquestion. There-
portourresultsonatestsetof50examplesdueto
foreitcanbeimplementedasaprogramthathas
thesizeoftheIrisdataset.
three LM modules: the first two for generating
search queries (i.e., hops) and the last one for
5 Results&Discussion
generating an answer. Each module uses Chain-
of-Thought(CoT;Weietal.2022)togenerateits Table1reportshoweachofthestrategiesdescribed
outputs, producing a reasoning string before the in Section 3 perform on the held-out test sets of
search query or the answer. Search queries are ourdatasets. Reportedvaluesareaveragedacross
passed to a frozen ColBERTv2 (Santhanam et al., threerunswithuniquerandomseeds. AppendixD
4separatelyreportstheresultsfromeachrun. Human-CenteredArtificialIntelligence(HAI),and
In 7 out of the 9 dataset and LM pairs, we ob- bytheHAIHoffman–YeeGrant“DendriticCom-
servethatthebest-performingstrategiesarealways putationforKnowledgeSystems”.
strategies that utilize prompt (Π) and weight (Θ)
optimization steps together, although there is no
References
clearwinneramongthethreemethodsthatoptimize
both. Overall,optimizingpromptsisessentialon LucaBeurer-Kellner,MarcFischer,andMartinVechev.
allthetasks,butoptimizingpromptsandweights 2023. Promptingisprogramming: Aquerylanguage
togetherleadstostronggainsoverthebestsetting for large language models. Proc. ACM Program.
Lang.,7(PLDI).
thatonlyoptimizesoneofthetwo.
Insummary,wehaveproposedtoalternatebe- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
tween prompt optimization and fine-tuning LM MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
weights. In experiments with multi-hop QA
Nakano, Christopher Hesse, and John Schulman.
(HotPotQA),mathematicalreasoning(GSM8K),and
2021. Training verifiers to solve math word prob-
feature-based classification (Iris), we show that lems. Preprint,arXiv:2110.14168.
ourstrategiesarehighlyeffectiveforgettinganLM
David Dohan, Winnie Xu, Aitor Lewkowycz, Ja-
toteachitselftoperformanLMprogramviaboot-
cob Austin, David Bieber, Raphael Gontijo Lopes,
strapping, leading to 5–78% gains for HotPotQA,
Yuhuai Wu, Henryk Michalewski, Rif A. Saurous,
2.5–10%gainsforGSM8K,and-5.9–136%gainsfor
JaschaSohl-dickstein, KevinMurphy, andCharles
Iris. Sutton.2022. Languagemodelcascades. Preprint,
arXiv:2207.10342.
6 Limitations
Karel D’Oosterlinck, Omar Khattab, François Remy,
ThomasDemeester,ChrisDevelder,andChristopher
While this short paper presents strong evidence
Potts.2024. In-contextlearningforextrememulti-
fromninecasestudiesintotal,spanningthreetasks
labelclassification. Preprint,arXiv:2401.12178.
(andtheircorrespondingLMprograms)andthree
LMs, it is possible that other tasks, programs, or RonaldA.Fisher.1988. Iris. UCIMachineLearning
Repository.
LMswillchangethepatterninunforeseenways. In
particular,wehaveonlyexperimentedwithweight
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,
optimization in the form of LoRA fine-tuning of and Mingwei Chang. 2020. Retrieval augmented
pre-trainedmodels. Itisinprinciplepossiblethat languagemodelpre-training. InProceedingsofthe
37thInternationalConferenceonMachineLearning,
someotherfine-tuningstrategywouldbesopow-
volume 119 of Proceedings of Machine Learning
erfulandcost-effectiveastoremovetheneedfor
Research,pages3929–3938.PMLR.
promptoptimization.
Inaddition,thoughweexpectourfindingstoin- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
formmanyresearchersandpractitionersinterested
WeizhuChen.2021. LoRA:Low-rankadaptationof
inoptimizingLMprograms,andencouragethem
largelanguagemodels. Preprint,arXiv:2106.09685.
toexploreoptimizingpromptsandfine-tuningLM
weights together, we do not yet understand why HuggingFace.2023. Textgenerationinference.
bothareimportant. Theroleofpromptoptimiza-
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
tionandtheroleoffine-tuningmulti-stageLMpro-
sch,ChrisBamford,DevendraSinghChaplot,Diego
gramsarebothnew,andtherelativelackofdeep delasCasas,FlorianBressand,GiannaLengyel,Guil-
understandingoftheserolesintheemerginglitera- laumeLample,LucileSaulnier,LélioRenardLavaud,
Marie-AnneLachaux,PierreStock,TevenLeScao,
turecouldposerisksinunanticipatedinteractions
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
between these components, compared with stan-
andWilliamElSayed.2023. Mistral7b. Preprint,
dardgradientdescentforneuralnetworks,which arXiv:2310.06825.
hasbeenstudiedfordecades.
Omar Khattab, Christopher Potts, and Matei Zaharia.
2021. Baleen: Robustmulti-hopreasoningatscale
Acknowledgments
viacondensedretrieval. InThirty-FifthConference
onNeuralInformationProcessingSystems.
D.S.issupportedbyRaviFamilyGraduateFellow-
ship. This work was partially supported by IBM
Omar Khattab, Keshav Santhanam, Xiang Lisa
asafoundingmemberoftheStanfordInstitutefor Li, David Hall, Percy Liang, Christopher Potts,
5and Matei Zaharia. 2022. Demonstrate-search- ImanolSchlag,SainbayarSukhbaatar,AsliCelikyilmaz,
predict: Composing retrieval and language mod- Wen tau Yih, Jason Weston, Jürgen Schmidhuber,
els for knowledge-intensive nlp. arXiv preprint andXianLi.2023. Largelanguagemodelprograms.
arXiv:2212.14024. Preprint,arXiv:2305.05364.
Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Zhiyuan Zhang, Keshav Santhanam, Sri Vard- bert, Amjad Almahairi, Yasmine Babaei, Nikolay
hamanan,SaifulHaq,AshutoshSharma,ThomasT. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Joshi,HannaMoazam,HeatherMiller,MateiZaharia, Bhosale,DanBikel,LukasBlecher,CristianCanton
and Christopher Potts. 2023. DSPy: Compiling Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
declarativelanguagemodelcallsintoself-improving JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
pipelines. Preprint,arXiv:2310.03714. CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
tus, Fabio Petroni, Vladimir Karpukhin, Naman IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Goyal,HeinrichKüttler,MikeLewis,Wen-tauYih, Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Tim Rocktäschel, Sebastian Riedel, and Douwe anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Kiela. 2020. Retrieval-Augmented Generation for tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Knowledge-Intensive NLP Tasks. In Advances in bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
NeuralInformationProcessingSystems33: Annual stein,RashiRungta,KalyanSaladi,AlanSchelten,
ConferenceonNeuralInformationProcessingSys- Ruan Silva, Eric Michael Smith, Ranjan Subrama-
tems 2020, NeurIPS 2020, December 6-12, 2020, nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
virtual. lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Dirk Merkel. 2014. Docker: Lightweight Linux con- Melanie Kambadur, Sharan Narang, Aurelien Ro-
tainersforconsistentdevelopmentanddeployment. driguez,RobertStojnic,SergeyEdunov,andThomas
LinuxJournal,2014(239):2. Scialom.2023. Llama2: Openfoundationandfine-
tunedchatmodels. Preprint,arXiv:2307.09288.
MetaAI.2024. Metallama3.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
KristaOpsahl-Ong,MichaelJRyan,JoshPurtell,David Bosma,brianichter,FeiXia,EdH.Chi,QuocVLe,
Broman,ChristopherPotts,MateiZaharia,andOmar and Denny Zhou. 2022. Chain of thought prompt-
Khattab.2024. Optimizinginstructionsanddemon- ing elicits reasoning in large language models. In
strationsformulti-stagelanguagemodelprograms. AdvancesinNeuralInformationProcessingSystems.
Preprint,arXiv:2406.11695.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,
Mohammadreza Pourreza and Davood Rafiei. 2023. WilliamCohen,RuslanSalakhutdinov,andChristo-
DIN-SQL: Decomposed in-context learning pher D. Manning. 2018. HotpotQA: A dataset for
of text-to-SQL with self-correction. Preprint, diverse, explainablemulti-hopquestionanswering.
arXiv:2304.11015. In Proceedings of the 2018 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
OfirPress,MuruZhang,SewonMin,LudwigSchmidt, 2369–2380,Brussels,Belgium.AssociationforCom-
NoahSmith,andMikeLewis.2023. Measuringand putationalLinguistics.
narrowingthecompositionalitygapinlanguagemod-
els. InFindingsoftheAssociationforComputational
Linguistics: EMNLP2023,pages5687–5711,Singa-
pore.AssociationforComputationalLinguistics.
PengQi,HaejunLee,TgSido,andChristopherMan-
ning. 2021. Answering open-domain questions of
varyingreasoningstepsfromtext. InProceedingsof
the2021ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing,pages3599–3614,Online
andPuntaCana,DominicanRepublic.Association
forComputationalLinguistics.
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2022. Col-
BERTv2: Effective and efficient retrieval via
lightweight late interaction. In Proceedings of the
2022ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics: Hu-
manLanguageTechnologies,pages3715–3734,Seat-
tle, United States. Association for Computational
Linguistics.
6Appendices
A Programs
TheDSPyprogramsforHotPotQA,GSM8K,andIrisaresharedinSnippets1,2,3,respectively.
class HotPotQAProgram(dspy.Module):
1
def __init__(self, passages_per_hop=3):
2
super().__init__()
3
4
self.retrieve = dspy.Retrieve(k=passages_per_hop)
5
self.generate_query = [dspy.ChainOfThought("context, question ->
6
search_query") for _ in range(2)]
self.generate_answer = dspy.ChainOfThought("context, question -> answer")
7
8
def forward(self, question):
9
context = []
10
11
for hop in range(2):
12
search_query = self.generate_query[hop](context=context, question=
13
question).search_query
passages = self.retrieve(search_query).passages
14
context = dsp.utils.deduplicate(context + passages)
15
16
return self.generate_answer(context=context, question=question).copy(context
17
=context)
Snippet1: DSPyprogramforHotPotQA.
class CoTProgram(dspy.Module):
1
def __init__(self):
2
super().__init__()
3
self.prog = dspy.ChainOfThought("question -> answer")
4
5
def forward(self, question):
6
return self.prog(question=question)
7
Snippet2: DSPyprogramforGSM8K.
class IrisSignature(dspy.Signature):
1
"Given the petal and sepal dimensions in cm, predict the iris species."
2
3
petal_length = dspy.InputField()
4
petal_width = dspy.InputField()
5
sepal_length = dspy.InputField()
6
sepal_width = dspy.InputField()
7
answer = dspy.OutputField(desc='setosa, versicolour, or virginica')
8
9
10
class IrisProgram(dspy.Module):
11
def __init__(self):
12
self.pred = dspy.ChainOfThought(IrisSignature)
13
14
def forward(self, petal_length, petal_width, sepal_length, sepal_width):
15
return self.pred(petal_length=petal_length, petal_width=petal_width,
16
sepal_length=sepal_length, sepal_width=sepal_width)
Snippet3: DSPyprogramforIris,providedtousbytheDSPyteam.
B AssetInformation
We share the associated licenses for the models and datasets we used below. For models, we list the
specificHuggingFacemodelidweusedtoretrievetherespectiveweights.
1. mistralai/Mistral-7b-Instruct-v0.2: Apache License 2.0
2. meta-llama/Llama-2-7b-chat-hf: Meta Llama 2 Community License at https://ai.meta.
com/llama/license/
73. meta-llama/Meta-Llama-3-8B-Instruct: Meta Llama 3 Community License at https://
llama.meta.com/llama3/license/
4. HotPotQA:Apache License 2.0
5. GSM8K:MIT License
6. Iris: Creative Commons Attribution 4.0 International (CC BY 4.0)
AlltheLMsusedinthisworkareintendedforuseinEnglish.
C ImplementationDetails
In this section, we share the implementation details as it pertains to sizes of the splits, LM sampling,
fine-tuning,andcomputerequirements. Wealsosharethedetailsforhowwecomputethegainsreported
throughoutthepaper.
Split Sizes For optimizing prompt templates with BootstrapFewshotRandomSearch (BFRS), we sub-
sample100examplesfromthetrainingsetforBFRStrainingsetand250examplesforitsvalidationset.
We allow BFRS to use up to 3 boostrapped as well as 3 labeled in-context-examples to search over 6
candidatefew-shotprompts.
TheoriginalIrisdatasethasatotalof150examplesacrossallthesplits. Were-splitallthedata-points
into train, development, and test sets, each with 50 examples. We use this test set to report our final
numbers. Fromthetrainingsplit,weusea15/35sub-splitforinternalprompt-optimizationtrainingand
validation,respectively.
Sampling For sampling, we host our models in Docker (Merkel, 2014) instances through
HuggingFace’s text-generation-inference (HuggingFace, 2023) toolkit. We keep the sampling
parametersthesameacrossallexperiments,usingTopKsamplingwithatemperatureof0.1,andtop_k
of0.97,untilthemodeleithergeneratesastoppingstringoratotalof1024tokens(includingthetokens
intheprompt,ifsupplied).
Fine-tuning Forfine-tuning,weuseLowRankAdaptation(LoRA)(Huetal.,2021)totrainthequery
andkeyself-attentionlayersofourmodels,usingaLoRArankof32,alphaof64,withnodropout. We
fine-tuneallofourmodelsfor5epochsusingbfloat16precision,withalearningrateof1e−5andan
effectivebatchsizeof8. Weusegradient accumulation stepslargerthan1inordertoeffectively
usealargebatchsize,withouthavingtofitallthebatchinmemoryatonce.
ComputeRequirement WeuseA100GPUstorunourexperiments. Thetotaltimeittakestorunthe
experiments varies based on the strategy, LM and dataset. Total approximate GPU hours to produce
Table1was≈75hours.
D ExtendedResults
The results shared in 1 are the average of three runs. Tables 2, 3, and 4 show the breakdown of the
individualrunsforHotPotQA GSM8K andIris respectively.
8mistral-7b-instruct-v0.2 llama-2-7b-chat llama-3-8b-instruct
Strategy
Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg
VanillaZero-shot 17.2 17.2 17.2 17.2 13.2 13.2 13.2 13.2 31.6 31.6 31.6 31.6
PromptOptimization(Π) 32.7 34.7 34.0 33.8 33.3 33.3 33.4 33.3 45.7 47.4 47.5 46.9
WeightOptimization(Θ) 22.0 23.1 23.5 22.9 12.4 11.8 12.3 12.2 34.9 35.3 34.3 34.8
Π→Π 31.7 36.0 33.7 33.8 31.7 33.1 33.1 32.6 47.3 45.4 46.7 46.5
Θ→Θ 24.1 23.9 23.9 24.0 12.4 13.5 13.3 13.0 35.1 34.1 34.1 34.4
Π→Θ 34.9 39.1 34.9 36.3 32.8 32.3 33.1 32.7 40.6 42.1 45.7 42.8
Θ→Π 29.3 33.8 35.8 33.0 36.0 33.4 33.1 34.2 44.5 40.9 45.3 43.6
Π→Θ→Π 34.9 40.7 37.2 37.6 34.7 34.5 35.3 34.8 46.5 47.1 46.4 46.7
Table 2: Results of HotPotQA Runs. Percentage accuracies of strategies consisting of prompt optimization
(Π), weight optimization (Θ) and their permutations for HotPotQA evaluated on mistral-7b-instruct-v0.2,
llama-2-7b-chat,llama-3-8b-instruct. Reportedaretheperformanceofthreerunsonheld-outtestsetsusing
differentrandomseedsandtheiraverage.
mistral-7b-instruct-v0.2 llama-2-7b-chat llama-3-8b-instruct
Strategy
Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg
VanillaZero-shot 40.3 40.3 40.3 40.3 24.0 24.0 24.0 24.0 72.7 72.7 72.7 72.7
PromptOptimization(Π) 45.0 47.2 47.1 46.4 27.3 25.1 25.5 26.0 76.9 77.9 78.9 77.9
WeightOptimization(Θ) 40.8 40.0 41.2 40.7 23.7 24.2 24.0 24.0 75.7 74.8 74.8 75.1
Π→Π 46.3 47.2 49.6 47.7 28.4 24.0 21.8 24.7 76.5 80.1 76.1 77.6
Θ→Θ 42.9 41.8 43.8 42.8 24.0 24.3 24.0 24.1 52.2 36.6 43.4 44.0
Π→Θ 46.4 47.3 48.2 47.3 27.8 28.1 25.9 27.3 77.6 75.4 79.8 77.6
Θ→Π 50.1 46.0 48.8 48.3 26.8 26.1 27.0 26.6 78.5 79.8 78.4 78.9
Π→Θ→Π 44.9 48.5 47.1 46.8 27.1 25.9 25.9 26.3 77.6 75.4 77.8 77.0
Table3: ResultsofGSM8KRuns. Percentageaccuraciesofstrategiesconsistingofpromptoptimization(Π),weight
optimization(Θ)andtheirpermutationsforGSM8Kevaluatedonmistral-7b-instruct-v0.2,llama-2-7b-chat,
llama-3-8b-instruct. Reportedaretheperformanceofthreerunsonheld-outtestsetsusingdifferentrandom
seedsandtheiraverage.
mistral-7b-instruct-v0.2 llama-2-7b-chat llama-3-8b-instruct
Strategy
Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg Run1 Run2 Run3 Avg
VanillaZero-shot 20.0 20.0 20.0 20.0 0.0 0.0 0.0 0.0 34.0 34.0 34.0 34.0
PromptOptimization(Π) 50.0 56.0 50.0 52.0 42.0 56.0 70.0 56.0 82.0 64.0 90.0 78.7
WeightOptimization(Θ) 26.0 28.0 32.0 28.7 – – – – 32.0 30.0 32.0 31.3
Π→Π 74.0 54.0 64.0 64.0 62.0 74.0 56.0 64.0 86.0 72.0 74.0 77.3
Θ→Θ 28.0 32.0 34.0 31.3 – – – – 32.0 30.0 30.0 30.7
Π→Θ 22.0 26.0 26.0 24.7 30.0 28.0 30.0 29.3 36.0 32.0 36.0 34.7
Θ→Π 60.0 68.0 68.0 65.3 – – – – 76.0 80.0 94.0 83.3
Π→Θ→Π 40.0 54.0 78.0 57.3 62.0 32.0 54.0 49.3 92.0 86.0 60.0 79.3
Table4: ResultsofIrisRuns. Percentageaccuraciesofstrategiesconsistingofpromptoptimization(Π),weight
optimization(Θ)andtheirpermutationsforIrisevaluatedonmistral-7b-instruct-v0.2,llama-2-7b-chat,
llama-3-8b-instruct. Reportedaretheperformanceofthreerunsonheld-outtestsetsusingdifferentrandom
seedsandtheiraverage.Settingsthatincludeweightoptimizationasthefirststeprelyonthedata-pointsbootstrapped
using the “Vanilla Zero-shot” setting. Since there weren’t any data-points that were answered correctly by
llama-2-7b-chatusingthe‘VanillaZero-shot”setting,thereweren’tanybootstrappedexamplestoforweight
optimizationeither. Settingsthatweren’tpossibletorunduetothisaremarkedwith“–”.
9