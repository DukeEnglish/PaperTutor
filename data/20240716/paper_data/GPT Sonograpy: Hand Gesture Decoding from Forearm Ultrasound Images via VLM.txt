IEEETRANSACTIONSANDJOURNALSTEMPLATE 1
GPT Sonograpy: Hand Gesture Decoding from
Forearm Ultrasound Images via VLM
Keshav Bimbraw, Ye Wang, Senior Member, IEEE, Jing Liu, Toshiaki Koike-Akino, Senior Member, IEEE
Abstract—Large vision-language models (LVLMs), such as
the Generative Pre-trained Transformer 4-omni (GPT-4o), are
emerging multi-modal foundation models which have great po-
tential as powerful artificial-intelligence (AI) assistance tools for
a myriad of applications, including healthcare, industrial, and
academicsectors.Althoughsuchfoundationmodelsperformwell
in a wide range of general tasks, their capability without fine-
tuning is often limited in specialized tasks. However, full fine-
tuningoflargefoundationmodelsischallengingduetoenormous
computation/memory/dataset requirements. We show that GPT-
4o can decode hand gestures from forearm ultrasound data
evenwith nofine-tuning,andimproves withfew-shot,in-context
learning.
IndexTerms—GPT,AI,LMM,LLM,VLM,UltrasoundImag-
ing, Human-Machine Interface, Gesture Recognition
I. INTRODUCTION
LARGE language models (LLMs) [1], such as genera-
tive pre-trained transformers (GPTs) [2], have recently
emerged as powerful general assistance tools and exhibited
tremendouscapabilitiesinawiderangeofapplications.LLMs
are often configured with billions of parameters to capture
linguistic patterns and semantic relationships in natural lan-
guage processing, enabling text generation, summarization,
translation, reasoning, question-answering, etc.
More recently, large multi-modal models (LMMs) [3] with
the capability to understand both natural language and other
modalities, such as images and sounds, have offered new
opportunities for biomedical applications. For example, it was
demonstratedthatlargevision-languagemodels(LVLMs)such
asGPT-4o[4]andLLaVa[5]couldbeaviabletoolformedical
applications[6],includingsurgicaloncology[7]andradiology
diagnosis[8]–[10].WeexaminethecapabilitiesofGPT-4ofor
sonography [11], to analyze and decode ultrasound images. Fig.1:ConversationwithGPT-4othatmotivatedustousethe
Musculoskeletal ultrasound is a non-invasive and non- VLM for ultrasound image decoding.
radiative imaging technique that uses ultrasound waves to
visualizemuscles,tendons,ligaments,andjoints.Forinstance,
ultrasoundmeasurementscanbeusedtovisualizetheanatomi-
that the use of LVLMs like GPT-4o to classify ultrasound
calaspectsoftheforearm,toestimatehandgestures[12],[13].
images can provide a lot more information through human
This is applicable to several domains, such as control of pros-
readable explanations of the model’s predictions, which aids
thetic hands [14], teleoperation of robotic grippers [15], and
understanding of the reasoning behind gesture recognition. In
controllingvirtualrealityinterfaces[16].Inparticular,modern
addition, contextual information can be potentially leveraged
deep learning methods have shown improved performance to
to improve the classification performance.
estimate different hand gestures [17]. It is highly expected
Although the pre-trained LVLMs work well for a general
task, its performance is often limited for specialized tasks
K.B is with Worcester Polytechnic Institute, Worcester, MA 01609, USA
(e-mail:kbimbraw@wpi.edu). such as biomedical dataset. Given such a dataset, fine-tuning
The authors are with Mitsubishi Electric Research Laboratories (MERL), can greatly improve the performance for downstream tasks
201 Broadway, Cambridge, MA 02139, USA (e-mail: {bimbraw, yewang,
in general. Nevertheless, fine-tuning LVLMs is challenging
jiliu,koike}@merl.com).
ThisworkwasconductedwhileK.BwasaninternatMERL. due to the substantial amount of labelled data required [18].
4202
luJ
51
]VC.sc[
1v07801.7042:viXraIEEETRANSACTIONSANDJOURNALSTEMPLATE 2
Fig. 2: Hand gestures (a through e) and the corresponding forearm ultrasound image (f through j) from subject 1. (a) and (f):
Index flexion; (b) and (g): all pinch; (c) and (h) hand horns; (d) and (i) fist; (e) and (j): open hand.
Additionally, it demands significant computational resources it could infer some additional information when it is given
and time. Therefore, it is more practical and cost-effective to some context. To this end, a follow-up question was asked:
consider using the pre-trained LVLMs without fine-tuning but “This is forearm ultrasound data. Can you tell me what the
with prompt tuning [19] or in-context learning (ICL) [20]. hand might be doing while this data was acquired?”. The
ICLdoesnotmodifythepre-trainedLVLMs,butinsteadadds LVLMgavesomemoreinformationaboutphysiologyofhand
some task-specific examples to the input context to improve movement and how different hand movements would lead to
the performance of generating the desired responses. differentultrasoundimages.Thefullconversationcanbeseen
In this work, we show that we can leverage GPT-4o to in Fig. 1.
classify ultrasound images using a few-shot ICL strategy. This motivated us to experiment with GPT-4o to see if
We demonstrate that providing some labelled examples to it could classify forearm ultrasound images corresponding to
the LVLM significantly improves its performance for forearm differenthandmovements.Wearealsointerestedinevaluating
ultrasound-based gesture recognition. This opens up exciting its performance while varying the amount of data and context
applicationsforLVLMsinmedicalimaging.Thecontributions that it is exposed to.
of this paper are summarized as follows.
• We examine the capability of LVLMs for sonography III. METHODOLOGY
diagnosis.
Forthisstudy,ultrasounddatawasacquiredfrom3subjects.
• WeuseGPT-4otoanalyzeforearmultrasoundimagesfor
The study was approved by the institutional research ethics
hand gesture decoding.
committee (IRB reference number 23001). Written informed
• We demonstrate that GPT-4o can achieve high accuracy
consent was given by the subjects before data acquisition. Per
of over 70% for cross-subsession experiments to classify
subject, data was acquired for 5 hand gestures as shown in
hand gesture even without any fine-tuning.
Figs. 2: (1) index flexion; (2) all pinch; (3) hand horns; (4)
• We show that the few-shot ICL strategy is substantially
fist; and (5) open hand. These are based on activities of daily
effective to improve the classification accuracy.
living and the chosen gestures are a subset of the dataset in
• We provide some discussions on cross-subject transfer,
[12].
prompt engineering, and image augmentations.
II. MOTIVATION A. Data Acquisition
LVLMs have the capability to handle tasks that involve TheultrasounddatawasacquiredusingaSonostar4Llinear
both images and texts. They have proven to be useful for palm Doppler ultrasound probe [21]. A custom-designed 3D-
understanding medical image data, especially with extensive printed wearable was strapped onto the subject’s forearm.
fine-tuning [8]. Since full fine-tuning of LVLMs requires The data from the probe was streamed to a Windows system
substantial computational resources, we first examined to see over Wi-Fi, and screenshots of the ultrasound images were
how GPT-4o would perform without fine-tuning. GPT-4o was captured using a custom Python script. The 4L linear probe
provided a forearm ultrasound image, and asked a simple has 80 channels of ultrasound data, and the post-processed
question “What can you tell me about this image?”. The beamformed B-mode data is obtained, from which 350×350-
LVLM was able to identify that it is an ultrasound image, pixel images are acquired.
andgavesomeadditionalinformationaboutgenericultrasound For each subject, 5 sessions of data were collected. In each
imagesandtheirvisualproperties.Wethenexaminedwhether session, subjects performed a sequence of 5 gestures. WithinIEEETRANSACTIONSANDJOURNALSTEMPLATE 3
B. Large Vision-Language Model (LVLM)
We use GPT-4o [4] as one of state-of-the-art LVLMs.
GPT-4o is a multi-modal generative pre-trained transformer
designed by OpenAI. It is said that GPT-4o uses more than
175 billion parameters. GPT-4o integrates texts and images in
asinglemodel,enablingittohandlemultipledatatypessimul-
taneously. This multi-modal approach enhances accuracy and
responsivenessinhuman-computerinteractions.Forinference,
Azure OpenAI module within OpenAI’s Python library was
used [22]. Azure cloud computing was used within a Linux
system with Python 3.11 for scripting.
The image data needs to be converted to a text format so
that GPT-4o can understand it. For this study, the ultrasound
image data was encoded to base64 using the Python base64
library, resulting in a text-based representation suitable for
transmission or embedding [23]. The ultrasound image is
represented as a long string of text upon encoding.
C. GPT-4o Prompts
The conversation flow we use is described in Fig. 3. To
effectively utilize GPT-4o, we designed the conversation as
follows.
1) System Message: We began with a system message to
set context and guidelines for the conversation. GPT-4o was
informed that it would serve as a helpful research assistant
and will assist in classifying hand gestures using forearm
ultrasound data.
2) In-Context Learning (ICL): We used an ICL strategy
which provides training examples in contexts. We use a few
forearm ultrasound image samples along with the class labels
for the in-context examples to assist GPT-4o for specialized
classificationtasks.NotethatICLdoesnotinvolveany‘learn-
ing’proceduresuchasfine-tuning,adaptation,orpost-training.
3) QueryforClassification: TheGPT-4owasthenaskedto
predict the hand gesture class based on the given ultrasound
image. It was explicitly instructed to provide just the class
number, which can be saved for further analysis.
IV. EXPERIMENTALSETUP
The performance was evaluated with few-shot in-context
strategies: 0-shot; 1-shot; 2-shot; and 3-shot ICL. Two exper-
iments were carried out: within-session analysis and cross-
session analysis. For the former, for a given subject, of the
40 images per class in session 1, the last sub-session (last
10 images) were used for evaluation, and the remaining were
used for training. For the latter, the last sub-session of session
5 was used for evaluation, while the remaining data was used
Fig. 3: Conversation with GPT-4o for forearm ultrasound as ICL training samples. For the three different experiments,
classification based on 1-shot learning. different data was used for training and evaluation. For 0-shot
strategy, the LVLM was shown images in the test set directly
and asked what class out of the 5 it belonged to.
each session, this sequence was repeated 4 times, resulting in
A. Within-Session Analysis
20 sub-sessions. For our study, we analyzed 10 frames per
sub-session, resulting in a total of 1000 images (i.e., 20 sub- Forthe1,2,and3-shotstrategies,thedata-splitisdescribed
sessions, 10 frames/sub-session, 5 gestures) per subject. below.IEEETRANSACTIONSANDJOURNALSTEMPLATE 4
Fig.4:Confusionmatricesforwithin-session(a–d),cross-session(e–h),andrandomizedcross-session(i–l)experimentssummed
over the three subjects for: 0-shot (a, e, and i), 1-shot (b, f, and j), 2-shot (c, g, and k), and 3-shot (d, h, and l) strategies.
1) 1-Shot: The first image per class from sub-session 1 2) 2-Shot: The first image per class (sub-session 1) from
was shown to the model along with the class label before sessions1and2wereshowntothemodelalongwiththeclass
asking the question. This leads to a total of 5 images and labels, leading to a total of 10 images shown.
their corresponding class-labels shown. This can be seen in 3) 3-Shot: The first image from sub-session 1 per class
Fig. 3. fromsessions1,2,and3wereshowntothemodelalongwith
2) 2-Shot: The first two images per class from sub-session the class label, leading to a total of 15 images shown.
1wereshowntothemodelalongwiththeclasslabels,leading
to a total of 10 images shown.
C. Evaluation Metrics
3) 3-Shot: The first image per class from sub-sessions 1,
2, and 3 were shown to the model along with the class label, To evaluate the performance, the predicted class labels
leading to a total of 15 images shown. from GPT-4o were compared to the true values. Classification
accuracywasusedasametricforevaluatingtheperformance.
Confusionmatriceswereusedtovisualizetheperformancefor
B. Cross-Session Analysis
different scenarios. Precision, recall and F1 scores were also
Forthe1,2,and3-shotstrategies,thedata-splitisdescribed
calculated for each confusion matrix.
below.
1) 1-Shot: The first image per class from sub-session 1
V. RESULTS
was shown to the model along with the class label before
askingthequestion.Thisleadstoatotalof5imagesandtheir This section provides the results for within-session and
corresponding class-labels shown. The training data shown in cross-session experiments for 0-shot, 1-shot, 2-shot, and 3-
similar to the within-session experiment. shot ICL strategies.IEEETRANSACTIONSANDJOURNALSTEMPLATE 5
TABLE I: Within-session experiment results TABLEIII:Averageaccuracycomparisonsforwithin-session,
cross-session, and randomized cross-session experiments
Accuracy Precision Recall F1Score
0-shot 0.193 — 0.193 — Within-Session Cross-Session Randomized
1-shot 0.600 0.817 0.600 0.618 0-Shot 0.193(±0.012) 0.200(±0.000) 0.213(±0.031)
2-shot 0.740 0.826 0.753 0.756 1-Shot 0.600(±0.159) 0.333(±0.167) 0.420(±0.106)
3-shot 0.720 0.846 0.720 0.731 2-Shot 0.740(±0.120) 0.513(±0.155) 0.453(±0.050)
3-Shot 0.720(±0.160) 0.613(±0.223) 0.433(±0.042)
TABLE II: Cross-session experiment results
Accuracy Precision Recall F1Score
While the performance with in-context learning was better
0-shot 0.200 — 0.200 —
1-shot 0.333 — 0.333 — than 0-shot case, it was worse than non-randomized case.
2-shot 0.513 0.588 0.513 0.479 Increasing the number of training samples did not clearly
3-shot 0.613 0.688 0.613 0.605
improve the average classification across subjects. For 0-shot
strategy, the classification accuracy was 21.3% (±3.0%). For
A. Within-Session Experiment 1-shot strategy, the average classification accuracy was 42.0%
(±10.6%).For2-shotstrategy,theaverageclassificationaccu-
The confusion matrix, summed over the three subjects for
racywas45.3%(±5.0%).Andfor3-shotstrategy,theaverage
the within-session experiment can be seen in Fig. 4(a)–(d) for
classification accuracy was 43.3% (±4.2%). The results are
0, 1, 2, and 3-shot strategies respectively.
summarized in Fig. 6.
Theclassificationaccuracy,alongwiththeprecision,recall,
and F1 scores are summarized in table I.
Table III shows the classification accuracy averaged over
VI. DISCUSSION
three subjects for within-session experiment. For 0-shot strat- Several additional experiments were carried out for within-
egy, the average classification accuracy was 19.3% (±1.0%). session data from subject 1 to understand GPT-4o’s perfor-
For 1-shot, 2-shot and 3-shot strategies, we achieved 60.0% mance and reasoning. All these experiments were done for
(±15.9%), 74.0% (±12.0%), and 72.0% (±16.0%) respec- a 1-shot strategy. The baseline confusion matrix is shown in
tively. It clearly demonstrates that in-context examples can Fig. 5(a). For this case, the accuracy is 86%, with the macro
significantly improve the classification accuracy even without average precision, recall, and F1 scores being 0.9, 0.86, and
fine-tuning the pre-trained LVLM. A slight decline of 2 0.85, respectively.
percentage points is observed when the training examples
increase from 2 to 3 per class. It may be within a statistical
A. Results with different prompts
fluctuation due to the small number of test samples.
WewantedtoseehowGPT-4owouldperformwithprompts
less and more descriptive than the prompts shown in Fig. 3.
B. Cross-Session Experiment
1) Less descriptive information: For this experiment, we
The confusion matrix, summed over the three subjects for did not provide the system message. And for training, we
the cross-session experiment can be seen in Figs. 4(e)–(h) for only stated the class label with the image. As the question,
0, 1, 2, and 3-shot strategies respectively. The classification we just asked ‘What class does the image belong to? Only
accuracy, along with the precision, recall, and F1 scores are give the class number.’ With this minimal information, the
summarized in table II. confusion matrix obtained is shown in Fig. 5(b). For this
For 0-shot case in Fig. 4(a), the classification accuracy is case, the accuracy is 82%, with the macro average precision,
comparabletoarandomguessbecauseof5classes.For1-shot recall, and F1 scores being 0.86, 0.82, and 0.82, respectively.
strategyinFig.4(b),itwas52%.For2-shotinFig.4(c),itwas It was interesting to see that there was only a decline of 4%
56%, which increased to 70% for 3-shot case as in Fig. 4(d). in the classification accuracy from the baseline of Fig. 5(a),
This trend is encouraging since increasing the number of in- meaning that we can provide it a lot less information without
context samples can improve the performance of GPT-4o to compromising significantly on the accuracy.
classifyforearmultrasoundimagestopredictthehandgestures 2) More descriptive information: For this experiment, we
they correspond. provided a lot more contextual information to GPT-4o both
This was repeated for subjects 2 and 3. Table III shows in the system message, as well as in the final question.
the classification results averaged over the three subjects. We mentioned that it should focus on the arrangement of
For 0-shot strategy, the average classification accuracy was regions with different brightness. We also mentioned that
20.0% (±0.0%). For 1-shot, 2-shot and 3-shot strategies, it the anatomical and physiological properties visualized in the
was obtained to be 33.3% (±16.7%), 51.3% (±15.5%), and ultrasound image are distinct for different hand gestures. The
61.3% (±22.3%) respectively. These results show a clear confusion matrix is shown in Fig. 5(c). For this case, the
improvement in the classifier performance for an increasing accuracy is 80%, with the macro average precision, recall,
number of in-context samples. It was interesting to observe and F1 scores being 0.87, 0.8, and 0.8, respectively.
that the standard deviation increases sharply as the number of It was interesting to see that providing so much extra
training examples increases from 2 to 3 per class. information did not really help improve the performance.
The results for the case where the input samples were Rather, it decreased the performance compared to the less
picked randomly from the training data is shown in Table III. descriptive information case by 2%.IEEETRANSACTIONSANDJOURNALSTEMPLATE 6
(a) Baseline (Accuracy: 86%) (b) Low-descriptive (Accuracy: 82%) (c) High-descriptive (Accuracy: 80%)
Fig. 5: Confusion matrices with different prompts (within-session, subject 1, 1-shot).
that VLMs like GPT-4o can be used to understand better why
it made a particular prediction. More effective conversations
with contextual clues may improve its performance.
C. Different input formats
Radiologists often look at stacked medical images to un-
derstand medical image data. This is done especially with
time-varying data to visualize how the physiological features
change with time [25]. We wanted to see how GPT-4o would
performfordifferentstacksofultrasoundimages.Fig.8shows
a stacked image sample with 4 ultrasound image frames.
1) Two images as input: Using two stacked ultrasound
frames as input for 1-shot strategy, instead of one image per
Fig. 6: Average classification accuracy for within-session and
class, 1 image with two ultrasound frames corresponding to
cross-session experiments, visualizing results in Table III.
the class were shown. This can be visualized in the top row
of Fig. 8. The classification results are shown in Fig. 9(a).
B. Reasoning ability For this case, the accuracy is 78%, with the macro average
precision, recall, and F1 scores being 0.83, 0.78, and 0.77,
With the flow shown in Fig. 3, we wanted to understand
respectively.
whyGPT-4omadethatparticularestimation.Fig.7showsthe
2) Fourimagesasinput: Using4stackedultrasoundframes
useraskingquestionstoGPT-4o,anditansweringwhyitmade
as input for 1-shot strategy, instead of one image per class,
thatparticularestimationcomparedtotheotherclasses.Based
1 image with 4 ultrasound frames corresponding to the class
on this conversation, we can make the following conclusions.
wereshown.ThiscanbevisualizedinFig.8.Theclassification
1) Logical Coherence: GPT-4o demonstrates a structured
results are shown in Fig. 9(b). For this case, the accuracy is
approachtoreasoning,witheachsuccessivesteplogicallyfol-
72%, with the macro average precision, recall, and F1 scores
lowing the previous one. This indicates an ability to maintain
being 0.84, 0.72, and 0.68, respectively.
logical consistency.
Although more training samples are provided by stacking
2) ContextualUnderstanding: Themodelincorporatescon-
frames, the classification accuracy was degraded. It may be
text into its reasoning, ensuring that decisions are relevant to
becausetheimageformatisdifferentforthetestingimageand
the given scenario. It takes into consideration the information
therelativeimageresolutionislowerwhenstacked.Webelieve
provided during training, as well as in the system message.
that the performance can be improved by better designing
3) Decision-Making: GPT-4o was able to express why the
prompts.
image does not belong to the other classes. It provides a clear
delineation between the the different classes, such as for class
D. Future work
5 (open hand), it stated that there is a different distribution of
bright and dark areas with more spread out experience, and We conducted experiments to understand capabilities of
hence, the image does not belong to class 5. GPT-4o for hand gesture classification based on forearm
While the model’s reasoning is not fully trustworthy and ultrasound data. We explored some interesting features of
VLMsarepronetohallucinations[24],itisencouragingtosee usingVLMsforthistask.FutureworkwouldincludeextensiveIEEETRANSACTIONSANDJOURNALSTEMPLATE 7
Fig. 8: Stacked ultrasound images for class 1 with ultrasound
image frames taken at different times.
VII. CONCLUSIONS
In this work, we show that we can use a large vision-
languagemodel(LVLMs),GPT-4oasapowerfulAIassistance
tool for understanding and interpreting forearm ultrasound
data.Weshowthatbyprovidingsomeexamplesofultrasound
images, we can improve its performance for hand gesture
classification based on forearm ultrasound data. For within-
session performance, we show that the average gesture classi-
ficationaccuracyreached74.0%for5handgestureswithjust2
trainingsamples,andforcross-sessionperformance,itreached
61.3% for just 3 training samples per class. Our approach
can be used in cases where full-fine tuning of these models
is challenging because of enormous compute/memory/dataset
requirements. This research opens up exciting avenues for
research in utilizing large vision-language models for medical
imaging.
REFERENCES
REFERENCES
[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican
Dong, et al. A survey of large language models. arXiv preprint
arXiv:2303.18223,2023.
[2] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.
Improvinglanguageunderstandingbygenerativepre-training. 2018.
[3] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-
Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale
multi-modal pre-trained models: A comprehensive survey. Machine
IntelligenceResearch,20(4):447–482,2023.
[4] Sakib Shahriar, Brady D Lund, Nishith Reddy Mannuru, Muham-
Fig.7:ConversationwithGPT-4oasafollowuptothe1-shot mad Arbab Arshad, Kadhim Hayawi, Ravi Varma Kumar Bevara,
conversationinFig.3todemonstrateitsreasoningcapabilities. AashrithMannuru,andLaibaBatool. PuttingGPT-4otothesword:A
comprehensive evaluation of language, vision, speech, and multimodal
proficiency. 2024.
[5] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved
baselineswithvisualinstructiontuning.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages26296–
26306,2024.
[6] NanZhang,ZaijieSun,YuchenXie,HaiyangWu,andChengLi. The
cross validation analysis, in addition to acquiring data from latest version ChatGPT powered by GPT-4o: what will it bring to the
more subjects. More rigorous prompt engineering should be medicalfield? InternationalJournalofSurgery,pages10–1097,2024.
[7] Ning Zhu, Nan Zhang, Qipeng Shao, Kunming Cheng, and Haiyang
consideredaswell.WearealsointerestedinexploringVLM’s
Wu. OpenAI’sGPT-4oinsurgicaloncology:revolutionaryadvancesin
cross-subject generalizability for medical image datasets. In generativeartificialintelligence. EuropeanJournalofCancer,2024.
addition, the comparison to retrieval augmented generation [8] Yuki Sonoda, Ryo Kurokawa, Yuta Nakamura, Jun Kanzawa, Mariko
Kurokawa, Yuji Ohizumi, Wataru Gonoi, and Osamu Abe. Diagnostic
(RAG) [26] and parameter efficient fine-tuning (PEFT) [27]
performances of GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro in
methods should follow. radiology’sdiagnosispleasecases. medRxiv,pages2024–05,2024.IEEETRANSACTIONSANDJOURNALSTEMPLATE 8
IEEEEngineeringinMedicine&BiologySociety(EMBC),pages4753–
4757.IEEE,2020.
[16] Keshav Bimbraw, Jack Rothenberg, and Haichong Zhang. Leveraging
ultrasound sensing for virtual object manipulation in immersive envi-
ronments. In2023IEEE19thInternationalConferenceonBodySensor
Networks(BSN),pages1–4.IEEE,2023.
[17] Keshav Bimbraw and Haichong K Zhang. Mirror-based ultrasound
system for hand gesture classification through convolutional neural
networkandvisiontransformer. InMedicalImaging2024:Ultrasonic
ImagingandTomography,volume12932,pages218–222.SPIE,2024.
[18] YuexiangZhai,HaoBai,ZipengLin,JiayiPan,ShengbangTong,Yifei
Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-
tuning large vision-language models as decision-making agents via
reinforcementlearning. arXivpreprintarXiv:2405.10292,2024.
[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale
forparameter-efficientprompttuning.arXivpreprintarXiv:2104.08691,
2021.
[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
Kaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,Girish
Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901,
(a) Stacked 2-frame (Accuracy: 78%) 2020.
[21] Sonostar. 4L linear palm Doppler ultrasound probe, 2024. http:
//sonostarmed.com/PalmUS/839.html.
[22] GitHub - openai/openai-python: The official Python library for the
OpenAI API — github.com. https://github.com/openai/openai-python.
[Accessed03-07-2024].
[23] https://platform.openai.com/docs/guides/vision/
uploading-base-64-encoded-images. [Accessed03-07-2024].
[24] HanchaoLiu,WenyuanXue,YifeiChen,DapengChen,XiutianZhao,
Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. A survey
on hallucination in large vision-language models. arXiv preprint
arXiv:2402.00253,2024.
[25] Frank Gaillard. Stacks — Radiology Reference Article — Radiopae-
dia.org — radiopaedia.org. https://radiopaedia.org/articles/stacks?lang=
us. [Accessed05-07-2024].
[26] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,
YiDai,JiaweiSun,andHaofenWang. Retrieval-augmentedgeneration
forlargelanguagemodels:Asurvey. arXivpreprintarXiv:2312.10997,
2023.
[27] Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-
efficient fine-tuning for large models: A comprehensive survey. arXiv
preprintarXiv:2403.14608,2024.
(b) Stacked 4-frame (Accuracy: 72%)
Fig. 9: Confusion matrix given stacked ultrasound images.
[9] Tatsushi Oura, Hiroyuki Tatekawa, Daisuke Horiuchi, Shu Matsushita,
Hirotaka Takita, Natsuko Atsukawa, Yasuhito Mitsuyama, Atsushi
Yoshida, Kazuki Murai, Rikako Tanaka, et al. Diagnostic accuracy
of vision-language models on Japanese diagnostic radiology, nuclear
medicine, and interventional radiology specialty board examinations.
medRxiv,pages2024–05,2024.
[10] Turay Cesur, Yasin Celal Gunes, Eren Camur, and Mustafa Dagli.
EmpoweringradiologistswithChatGPT-4o:Comparativeevaluationof
largelanguagemodelsandradiologistsincardiaccases.medRxiv,pages
2024–06,2024.
[11] FrederickWKremkau.Sonographyprinciplesandinstruments.Elsevier
HealthSciences,2015.
[12] KeshavBimbraw,ChristopherJNycz,MatthewSchueler,ZimingZhang,
andHaichongKZhang.Simultaneousestimationofhandconfigurations
andfingerjointanglesusingforearmultrasound. IEEETransactionson
MedicalRoboticsandBionics,5(1):120–132,2023.
[13] JessMcIntosh,AsierMarzo,MikeFraser,andCarolPhillips. Echoflex:
Handgesturerecognitionusingultrasoundimaging. InProceedingsof
the 2017 CHI Conference on Human Factors in Computing Systems,
pages1923–1934,2017.
[14] Zongtian Yin, Hanwei Chen, Xingchen Yang, Yifan Liu, Ning Zhang,
Jianjun Meng, and Honghai Liu. A wearable ultrasound interface
for prosthetic hand control. IEEE journal of biomedical and health
informatics,26(11):5384–5393,2022.
[15] KeshavBimbraw,ElizabethFox,GilWeinberg,andFrankLHammond.
Towardssonomyography-basedreal-timecontrolofpoweredprosthesis
graspsynergies. In202042ndAnnualInternationalConferenceofthe