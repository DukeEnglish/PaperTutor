InVi: Object Insertion In Videos Using Off-the-Shelf
Diffusion Models
NiratSaini*1 NavaneethBodla2 AshishShrivastava2 AvinashRavichandran2
XiaoZhang2 AbhinavShrivastava1 BharatSingh2
1UniversityofMaryland,CollegePark
2CruiseLLC
Abstract
WeintroduceInVi,anapproachforinsertingorreplacingobjectswithinvideos
(referredtoasinpainting)usingoff-the-shelf,text-to-imagelatentdiffusionmodels.
InVi targets controlled manipulation of objects and blending them seamlessly
into a background video unlike existing video editing methods that focus on
comprehensivere-stylingorentirescenealterations.Toachievethisgoal,wetackle
twokeychallenges.Firstly,forhighqualitycontrolandblending,weemployatwo-
stepprocessinvolvinginpaintingandmatching. Thisprocessbeginswithinserting
theobjectintoasingleframeusingaControlNet-basedinpaintingdiffusionmodel,
andthengeneratingsubsequentframesconditionedonfeaturesfromaninpainted
frame as an anchor to minimize the domain gap between the background and
the object. Secondly, to ensure temporal coherence, we replace the diffusion
model’s self-attention layers with extended-attention layers. The anchor frame
featuresserveasthekeysandvaluesfortheselayers,enhancingconsistencyacross
frames. Ourapproachremovestheneedforvideo-specificfine-tuning,presenting
anefficientandadaptablesolution. ExperimentalresultsdemonstratethatInVi
achievesrealisticobjectinsertionwithconsistentblendingandcoherenceacross
frames,outperformingexistingmethods.
1 Introduction
Theemergenceofimageandvideogenerationalgorithmshasopenedupexcitingnewpossibilitiesfor
utilizinggenerateddataacrossvariousdomains,includingmediaproduction,AR/VR,andsynthetic
dataformodeltrainingRombachetal.[2022],Guoetal.[2023b],PNVRetal.[2023],Rameshetal.
[2022],Esseretal.[2023],Shrivastavaetal.[2017]. However,unconstrainedtext-to-image/video
generationsufficesonlyinalimitedsetofscenarios. Inpractice,thereisoftenaneedforenhanced
controloverimage/videogenerationprocesses,encompassingaspectssuchascharacterconsistency,
pose,andbeyond. Thisneedhaspromptedthedevelopmentofnumerousalgorithmsintheimage
generationdomain,includinginpaintingLugmayretal.[2022],Rombachetal.[2022],LoRARuiz
etal.[2023],Huetal.[2022],andControlNetZhangetal.[2023]. Thesetechniquesensurethatthe
generatedimagesadheretoconstraintssuchasbackground,style,andpose. Intherealmofvideo
generation,algorithmssuchasGeyeretal.[2023],Caoetal.[2023],Wuetal.[2023]haveaddressed
thedemandforcontrol,butmanypredominantlyfocusoncomprehensiverestylingofentirevideos
ratherthanthenuancedtaskofinsertingorreplacingspecificobjectswithinthevideo–aprocess
commonlyknownasinpainting. Furthermore,whilesomeapproachestackleobjectmanipulation,
they often extend changes to the entire scene’s background rather than solely concentrating on
modifyingthesubject. Inthiswork,wefocusonthetasksofaddingandreplacingobjectsinavideo
(Figure1). UnlikerecenttechniquessuchasthosepresentedinGeyeretal.[2023],Wuetal.[2023],
we choose text-to-image diffusion models instead of text-to-video diffusion models, as the latter
*workdoneduringinternshipatCruise.Correspondingemail:nirat@umd.edu.
Preprint.Underreview.
4202
luJ
51
]VC.sc[
1v85901.7042:viXraOff-the-Shelf
Diffusion Model
Background Video Foreground Mask Control Image Generated Video
Figure1: InViinsertsobjectsintoabackgroundvideousingaforegroundmask,acontrolsignal(e.g.,
pose,canny,depthmap),andatextpromptbyleveragingoff-the-shelfdiffusionmodels. Itensures
thattheinsertedobje+ctalignssemantically+withthetex+t,istemporallycoherentintime,andalso
conformsspatiallytothecontrolsignal.
Background Video Foreground Mask Control Image Generated Video
necessitatesignificantmodificationsforourspecifictask. Moreover,bybuildingupontext-to-image
models,wecircumventtherequirementfortrainingonextensivevideodatasetsandcanleverage
awidearrayofestablishedtext-to-imagemodelsspanningvariousdomains,includinganime,art,
photography,autonomousdriving,andmore. Thisstrategicchoiceenablesustotakeadvantageof
pre-trainedconditionalmodelslikeinpaintingRombachetal.[2022],LoRARuizetal.[2023],Hu
etal.[2022],ControlNetZhangetal.[2023],andseamlesslyintegratethemintoouralgorithm.
Existingapproachesforvideoeditingexhibitshortcomings,suchasnotgeneratingalltheframes
Geyer et al. [2023] or requiring expensive per-video fine-tuning Wu et al. [2023]. Methods like
TokenflowGeyeretal.[2023],whichoptforajointsynthesisapproach,however,generatesonlya
subsetoftherequiredframesandrelyonopticalflowtogeneratetheremainingones. Thislimitation
arisesfromthechallengeofsynthesizingallframesjointly,whichbecomesincreasinglychallenging
due to GPU memory limitations, leading to performance degradation as the number of frames
increases. Ontheotherhand,methodslikeTune-a-VideoWuetal.[2023]requireadditionaltemporal
layersandfine-tuningonthetargetvideo,leadingtosignificantlatency.
To tackle these challenges, we introduce InVi, a novel method for inpainting objects in videos.
Leveragingoff-the-shelftext-to-imagelatentdiffusionmodels,ourapproachseamlesslyappliesto
videos of any duration, eliminating the requirement for individual fine-tuning for each video. In
addressingobjectinpaintinginvideos,ourmethodaddressestwoprimarychallenges: (1)Ensuring
realisticblendingoftheinsertedobjectinthetargetvideo,avoidingaresemblancetoitsappearance
inthesourceimage. (2)Ensuringconsistencyacrossframesduringthevideosynthesisprocess.
Toachieveaseamlessintegrationofthesourceimageintothetargetimage,InViintroducesatwo-step
inpaintandmatchprocess. Initially,theobjectisinsertedintoasinglevideoframe,leveragingthe
effectivenessofimage-basedinpainting. Subsequently,theinpaintedframeservesasthereference
forgeneratingsubsequentframes,ensuringthatvideosynthesisisconditionedonfeatureswithinthe
domainofthetargetvideoratherthanthesourceimagealone. Tomaintaincoherenceacrossframes,
InViemploysanauto-regressivearchitecturewithextended-attentiontoincorporatefeaturesfrom
theprecedingframewhilegeneratingthecurrentframe. Throughexperimentsconductedonseveral
videosfromtheDAVISdatasetandourowntestset,whichincludesnovelobjectinsertionscenarios,
weobservethatInVioutperformsothermethodsbymorethan40pointsinbackgroundconsistency
metricsandisthepreferredchoiceinnearly70%ofthevideosinouruserstudy.
2 RelatedWorks
Conditionalvideogenerationandediting: Basedontheprogressingeneratingimagesfromtext
withdiffusionmodelsSahariaetal.[2022],Rameshetal.[2022],Rombachetal.[2022],therehas
beenanincreaseinworksthataddressvideogeneration Guoetal.[2023a],Chenetal.[2023],Wu
et al. [2023]. This has facilitated the creation of videos from textual descriptions, which can be
furtherrefinedtoachievevideo-to-videogenerationbyusingattributesderivedfrominitialvideo
inputs. Forinstance,Gen-1Esseretal.[2023]utilizesestimateddepthasaconditioningfactor,while
VideoComposerWangetal.[2023]usesabroaderarrayofinputssuchasdepth,motionvectors,and
sketches. However,mostofthesemethodsneedexplicittrainingonvideosforlearningmotionGuo
et al. [2023a], Chen et al. [2023], and ensuring that these models generalize to arbitrary motion
patterns requires access to carefully curated large video datasets, which are relatively fewer (or
2
tpmorP
tpmorP
txeTnon-existent) than those available for images Schuhmann et al. [2022]. Additionally, substantial
computationalresourcesarerequiredforthedevelopmentofthesemodelsandtheirderivativesfor
conditional generation. To the best of our knowledge, there does not exist a text to video model
whichistrainedend-to-end,whichsupportsinpaintingobjectsinvideos,whileprovidingsupportfor
usingauxiliaryconditionslikepose,depth,edgemaps,etc.,asiscommonlyavailableforimages. To
overcomethechallengesassociatedwithtrainingsuchcomplexmodelsonvideos,someapproaches
resorttosingle-imageediting,subsequentlyextendingthesemodificationsacrossvideosequencesby
identifyingandapplyingeditstocorrespondingpixelsthroughouttheframesandtheirefficacyhinges
onrobusttracking. VariousmethodsYangetal.[2023],Guetal.[2023]haveemployedtechniques
suchasopticalflow,keypointtracking,orotherformsofmotiondetectiontoaddressthischallenge.
However,thesetechniquesarehardtoscaletolongvideos,forconsistentappearancechangesin
objects.
AdaptingImageModelsforVideotoVideotasks: Manymethodshaveextendedimage-to-image
modelsforswappingobjectsinvideos. Forinstance,Khachatryanetal.[2023]modifiesself-attention
mechanismsindiffusionmodels,whileWuetal.[2023]conductsper-videofine-tuningandemploys
inversion-denoisingtechniquesforeditingpurposes. MasaCtrlCaoetal.[2023],originallydeveloped
forimageeditingtasks,hasbeenextendedtovideogenerationtasksandleveragesthefirstframe
generatedasareferencetosynthesizesubsequentframesinthevideosequence.
Liuetal.[2023a]andFate-ZeroQietal.[2023]adaptimage-to-imagepipelinesHertzetal.[2022],
Tumanyan et al. [2023], Brooks et al. [2023] for video editing by introducing modifications to
cross-frameattentionmodules,incorporatingnull-textinversion,andmore. However,mostexisting
methodsarelimitedtogeneratingveryshortvideoclips. TokenFlowGeyeretal.[2023]produces
keyframesandemploysanearest-neighborfieldondiffusionfeaturestoextendkeyframeattributesto
remainingframes. However,asthevideolengthincreases,interpolationperformancemaydegrade
duetoaccumulatedinterpolationerrorsovertime. Incontrast,ourmodelenhancesspatio-temporal
attentionKhachatryanetal.[2023],Liuetal.[2023a],Qietal.[2023]withanchor-basedcross-frame
attention,enablingthegenerationoflongvideoswithanydesirednumberofframes. Ourworkalso
differsfromTokenFlowGeyeretal.[2023]initssupportforinpainting. Geyeretal.[2023]doesnot
supportin-painting,asitistailoredtopreservethestructureandmotionoftheoriginalvideoand
cannothandleeditslikechangingthesize,shape,poseormotionpatternsofobjects. Weusesimilar
ideasoflatentinversionofthesourcevideo,buttheycanbeofavideofromadifferentdomain,and
wecanuseit’sposeorcannyfeaturestoinpaintasimilarobjectinnewvideos. Thisensuressharp
andconsistentobjectinsertioninnewvideos,whileGeyeretal.[2023]failsinmaintainingsharpness
ofanewobject,duetoitsopticalflowpropagationinthelatentspace.
3 InVi
WebuildupontheconceptsofLatentDiffusionModelsHoetal.[2020],Rombachetal.[2022],
DDIMinversionRombachetal.[2022],Geyeretal.[2023]andLoraHuetal.[2022]. Readersare
encouragedtorefertothemethodsorappendixforamorein-depthdetails. GivenaninputvideoI =
[I1,...,In]comprisingnframes,atextpromptP describingthedesirededitandacontrolsequence
C = [C1,...,Cn], InVi generates an edited video I˜ = [˜I1,...,˜In]. As in LDM Rombach et al.
[2022],thevideoframesareconvertedtolatentfeatureusinganencoder,E,andthecorresponding
encodedfeaturesaredenotedby[x1,...,xn]. Similarly,theencodedfeaturesoftheeditedvideoare
denotedby[x˜1,...,x˜n]. TheeditedvideoalignsspatiallywiththecontrolsequenceCandconforms
tothesemanticconstraintsoutlinedinP. Thetextprompt,P,offersgenericsemanticguidance,
influencingfactorssuchasobjectappearance. Alternatively,thedesirededit’sappearancecanbe
specifieddirectlyasanimageinsteadofthetextprompt, forwhich, weleverageLoRAHuetal.
[2022]. Incontrast,thecontrolsequenceCprovidesmorenuancedcontrol,suchasposeorobject
shape. Variousmethodsexistforprovidingspatialcontrol,denotedbyC,suchasdepthmaps,edge
maps,andnormalmapsforgenericobjects,orhumanposesiftheobjectisapersonZhangetal.
[2023]. Next,wewilldescribeeachofthestepsinourpipelineinmoredetail.
3.1 Generatingthefirst-frameandpre-processing
First, given the object’s location in each frame via bounding boxes, we extract a region of fixed
resolutionbyexpandingtheseboundingboxes,asillustratedinFigure2(a). Wetheninsertanobject
3Inpainting Diffusion
Create foreground Model with ControlNet
masks and crop
Prompt
(a) (b)
Legend Inpainting Diffusion Model with
Anchor image pipeline Anchor based Cross- Attention
Target image pipeline
Text Prompt
Neural network
Operations
Summation
Insert the in-painted
object back in the video
DDIM-Inv
Prompt
ControlNet
(c)
Figure2: InViinferencepipeline:(a)Givenavideoandobjectboundingboxes,first,wecroparegion
aroundtheboundingboxwhichisinpainted.(b)Next,weuseaControlNet-basedinpaintingdiffusion
model to inpaint the cropped region in the first frame. (c) To ensure temporal consistency when
inpaintingsubsequentframes,weemploythepreviousframeasananchorimage. Thisisachievedby
adaptingtheself-attentionblockofthedenoisingU-Netwithextendedattention. Specifically,we
augmenttheKeysandValuesofthecurrentframebeinginpaintedwiththoseoftheanchorframe,
allowingforconsistentappearance. Finally,theinpaintedcropisseamlesslyintegratedbackintothe
inputvideo.
intothefirstframe,forwhichwerelyonaControlNet-basedinpaintingdiffusionmodel. Thiscanbe
anyoff-the-shelftexttoimageinpaintingmodelforprompt-basededitingandpersonalizedmodel
usingLoRARuizetal.[2023]forreferenceimagebasedediting. Onceoneframeiseditedusing
imageinpaintingpipeline,weusethisgeneratedimageasan“anchor”denotedasIancandeditthe
remainingframes.
Topreparetheinputsforgeneratingsubsequentframes,wefirstpassthemaskedimagethrougha
VAEencoder,asdoneinpriorworkRombachetal.[2022],compressingitintoalower-dimensional
input(64×64×4inourexperiments). Thisinputisthenconcatenatedwithasuitablydownsampled
mask of identical dimensions (64 × 64), indicating the area to be inpainted. In contrast to the
inpaintingpipelineinRombachetal.[2022],whichcombinestheseinputswithGaussiannoise(sized
64×64×4)duringinpainting,weutilizetheoutputafterDDIMinversiononbackgroundframes
asinputfortheinpaintingmodel. Thisstepiscrucialformaintainingvideoconsistency,asDDIM
inversion on the background frame ensures a consistent noise pattern across frames. Additional
conditionssuchasposeordepth-mapareprovidedtoControlNet,asoutlinedinZhangetal.[2023].
AcomprehensivewirediagramdetailingallinputsforourpipelineisillustratedinFigure2(c).
3.2 TemporallyConsistentFrameInpainting
TopropagateinformationfromtheeditedanchorframeIanctoanothervideoframe,weproposeto
usecross-frameattentionmechanisms,circumventingconventionalmethodssuchasopticalflowor
explicitpointtracking. Givenananchorframe,Ianc,weincorporateitasanadditionalinputtothe
diffusionmodelandreplacetheself-attentionmechanisminthemodelwithcross-frameattention.
Specifically,weusetheanchorframefeaturestoaugmentkeys,denotedbyK,andvalues,denoted
byV,withintheattentionlayersofthediffusionmodel. Wedenotethekeyandvaluematricesofthe
ith frameasK andV ,respectively,wherelisthelayerindexofthediffusionmodelandt
i,l,t i,l,t
isthediffusionstep. Similarly,wedenotethekeyandvaluematricesofthemodelobtainedwhen
4
elpmasnwoD
noitanetacnoC
rof
rohcnA
emarf
rehtonatheanchorframeispassedtothemodelasKanc andVanc*,respectively. Toediti-thframeIi,we
l,t l,t
modifytheself-attentionmoduletoacross-frameattentionusingthekeyandvaluevectorsofanchor
framesasfollows:
(cid:32) (cid:33)
Q [K ,Kanc]T
lthlayerfeature=Softmax i,l,t √i,l,t l,t [V ,Vanc], ∀l,∀t∈[1,...,T].
i,l,t l,t
d
Note that this augmentation does not change the network architecture and does not require any
learning of new parameters. Our method, as shown in Figure 2(c), utilizes softmax-generated
attentionscorestointegrateVancfeaturesfromtheanchorframe. Thisprocesseffectivelyenforces
thetemporalcorrespondencebetweenthecurrentframeandtheanchorframe,andfacilitatesthe
propagationofvaluefeaturesfromtheanchorframestothecurrentframethroughthemultiplication
of attention scores with Vanc. By substituting the self-attention module with an anchor-based
cross-frameattentionmechanism,weachievetemporalconsistencyacrosstheeditedvideoframes.
Wecoulduseoneanchorframefortheentirevideo,however,thisisnotidealasthebackground
appearance and the pose of an object gradually evolves over time. Therefore, once we generate
a frame i, it serves as the anchor for generating the next frame i+1. This sequential process is
describedinAlgorithm1.
Algorithm1InVi: ObjectInsertioninVideos
Input:
X=[x1,...,xn] ▷Backgroundvideoinlatentspace
b b
M=[M1,...,Mn] ▷Downsampledinputmask
X =[x1 ,...,xn ] ▷Maskedbackgroundinlatentspace
bm bm bm
C=[C1,...,Cn] ▷Conditionalinputs
P,ϕ ▷Targettextprompt,ControlNet-basedinpaintingmodel
{xi}T ←DDIM-Inv[xi] ∀i∈[1,...,n], ∀t∈[1,...,T]
t t=1 b
Fort=T,...,1do
x˜1 =ϕ(x1,x1 ,M1,C1)
t t bm
Kanc,Vanc ←K ,V ∀l ▷savefirstframefeaturesinacache
l,t l,t 1,l,t 1,l,t
Fori=2,...,ndo
Fort=T,...,1do
loadKanc,Vancfromcache
l,t l,t
x˜i ←ϕ(xi,xi ,Mi,Ci,Kanc,Vanc) ▷inpainti-thframewithanchorfeatures
t t bmt t t t t
saveK ,V
i,l,t i,l,t
Kanc,Vanc ←K ,V ▷Updatecachewithi-thframefeatures
l,t l,t i,l,t i,l,t
Output:X˜ =[x˜1,...,x˜n] ▷Latentsforinpaintedframesatt=1
1 1
3.3 Post-processing
After inpainting the object within the Region of Interest (RoI), an occasional subtle halo effect
emerges,resemblingaflickeringsquare,inthevicinityoftheinsertedobject. Inthecaseofhigh-
resolutionvideos,duetothelimitedtrainingofbasediffusionmodelsonsuchresolutions(andan
orderofmagnitudehigherinferencetime),objectinpaintingcanonlybeperformedwithinasmall
RoI. The subtle differences which result from VAE based reconstruction are not very prominent
(although noticeable) when the inpainted RoI is composed with the original frame but this gets
amplifiedinavideoastheobjectmoves. Consequently,toachieveseamlessandefficientblendingfor
highresolutionvideos,weadoptamulti-stepapproach. Initially,weextractthemaskoftheinserted
objectusinggrounding-DINOLiuetal.[2023b](fordetectingarbitraryclasses)andSAMKirillov
etal.[2023](gettingobjectmasksinsideboundingboxes). Oncethemaskisobtained,weemploy
dilationtoexpanditsboundary. Subsequently,weutilizeLamaSuvorovetal.[2022]toinpaintthe
pixelswithinthisboundary,ensuringsmoothblendingthroughoutthevideosequenceasshownin
Figure3. Thiscomprehensivestrategyenhancesvisualcoherenceandminimizesanyartifactsor
discrepanciesresultingfromtheobjectinsertionprocess. Notethatforlowresolutionvideoswhere
theentireframecanbeinpainted,wedonotrequirethisstep.
*Forbrevity,wewillomitthesubscriptslandtwherecontextmakesitclear.
5Figure3:Post-processingtoremoveflickeringsquareartifacts.a)Backgroundimage.b)Initialimage
generatedfromourpipeline. c)Zoomed-inviewrevealingartifactsaroundtheinsertedobject. d)A
trimapisgeneratedtofacilitateseamlessblendingoftheobjectintothebackground.e)Post-processed
frameshowcasingthefinalresultafterblendingtheinsertedobjectwiththebackground.
Table1:QuantitativeResultsforobjectswapping(ontheleft)andobjectinsertion(ontheright).Eval-
uationforbackgroundconsistency,temporalappearanceconsistency,andalignmentwithprompts.
FateZero Tune-a-Video TokenFlow InVi Frm+Inp TokenFlow+Inp InVi
CLIP-Text 0.30 0.31 0.32 0.33 CLIP-Text 0.24 0.26 0.28
CLIP-Temp 0.95 0.95 0.96 0.97 CLIP-Temp 0.96 0.97 0.98
Back-L1 35.66 100.98 42.26 6.40 LPIPS 0.07 0.05 0.02
4 Experiments
Ourmethodisevaluatedacrossdiversedatasets,includingvideosfromtheDAVISdatasetwhichare
usedinpriorworkPerazzietal.[2016],aselectionofvideosfromtheVIRATsurveillancedatasetOh
etal.[2011],aswellashuman-centricvideossourcedfromYouTube. Additionally,wecurateour
ownvideofootagefeaturingcars,trafficcones,fallingballs,andvariousmovingobjectstofurther
assesstherobustnessandefficacyofourmethod. Toreplicatesyntheticassetssuitableforinsertion
intoa3Dsceneusingsimulationenginesforapplicationslikesurveillance,AR/VR,andautonomous
driving, we adopt two approaches. Firstly, we gather videos with a static camera over extended
durations,enablingtheextractionofconditionalinputsfromearliertimeframes. Alternatively,we
employobjectremovalsoftware(RunwayML)toartificiallyremoveobjectsfromscenes,allowingus
toutilizeconditionalinputsfromtheoriginalvideo.Weattachexamplesoftheseinthesupplementary
material.Thespatialresolutionofourvideos(aftercropping)is384×672or512×512pixels,andthey
consistofanywherefrom24to200frames. Ourevaluationdatasetcomprisesof30text-videopairs.
WhentrainingaLoRA-Dreamboothmodel,wetrainfor1200iterationswitharankof96,usinga
singlereferenceimage,withoutsettinganyregularization. Inourexperiments,weusetheinpainting
versionofRealisticVision5.0,whichisbasedonStableDiffusion1.5Rombachetal.[2022]. Our
computationaloverhead(apartfromDDIMinversion)isminimalcomparedtotheper-framebaseline
asweonlydoubletheFLOPs,andmemoryintheself-attentionblocksofthetransformerlayers,
whileeverythingelseremainsthesame.
4.1 Baselines
WebenchmarkInViagainstseveralvideoeditingmethodsthatswapobjectswhilepreservingtheir
structure. Theseinclude: (1)Fate-ZeroQietal.[2023],azero-shottext-basedvideoeditingmethod;
(2)Tune-a-VideoWuetal.[2023],whichfine-tunesthetext-to-imagemodelonthegiventestvideo;
and(3)TokenFlowGeyeretal.[2023],whicheditsselectedanchorframesandpropagatestheimplicit
flow from the keyframes to the rest of the video using an off-the-shelf propagation method. We
employPnP-DiffusionTumanyanetal.[2022]basededitingwithTokenFlow. Thesemethodsalter
theentireframeanddonotpreservethebackground. Sincetherearenoexistingvideoinpainting
methodsthatutilizeoff-the-shelfdiffusionmodels,weincludetwoadditionalbaselinestoevaluatethe
inpaintingperformance: (1)Per-framediffusion-basedimageinpaintingbaselineusingControlNet;
and(2)aControlNetZhangetal.[2023]basedinpaintingpipelineforTokenFlow.
4.2 QuantitativeEvaluation
Following previous workGeyer et al. [2023], Qi et al. [2023], we use several metrics to evaluate
variousaspectsofourobjecteditingandinpaintingtechniques. Firstly, wecomputeCLIP-Text,
whichrepresentstheaverageCLIPfeaturesimilaritybetweenthegeneratedframesandthetarget
prompt,servingasanindicatorofvideo-textalignment. Forassessingtemporalconsistencywithin
6Text Alignment Background Consistency Visual Quality Text Alignment Temporal Consistency Visual Quality
8%4% 11% 1% 9% 1% 9% 3% 16% 4% 16%
18% 11% 19% 15%
70% 77% 71% 76% 81% 80%
FateZero Tune-a-Video TokenFlow InVi Frm+Inp TokenFlow+Inp InVi
(a) Object Swapping (b) Object Insertion
Figure4: UserPreferenceStudy: InViOutperformsBaselineMethodsintextalignment,background
andtemporalappearanceconsistencyandoverallvideoquality.
"Spiderman sliding on the railing" "An orange car driving on the road"
"Daniel Craig as James Bond playing with a basketball in a tuxedo" "A traffic cone in the middle of the road"
"A basketball jumping on the road"
"A woman walking to college with a black backpack"
Figure 5: Qualitative results. The first image is a background frame from the video undergoing
inpainting. Subsequentframesdepictthevideowiththeinsertedobject.
thevideo,weutilizeCLIP-Temp,whichmeasuresthesimilarityofconsecutiveframesandaverages
theresultsacrossthegeneratedvideo. Giventheimportanceofmaintainingbackgroundconsistency
while editing a specific object in videos, we use a background mask to evaluate Back-L1 which
istheaverageL1distancebetweeneachpixelacrosscorrespondingframesoforiginalvideoand
editedvideo. Videoeditingismorecommontask,hencewecomparewithexistingbaselineswhich
operateoff-the-shelfwithoutanytraining,forafaircomparisonwithourmethod. Forinsertingnew
objectsinavideo, allbaselinesandourmethodinpaintonlytheobject, thebackgroundremains
consistent. Hence,insteadofBack-L1weuseaverageLPIPSZhangetal.[2018],whichispatch
basedperceptualsimilarityscoreacrossconsecutiveframesofthevideo. LowerLPIPSmeansmore
similarityacrossframes. Finally,inadditiontoobjectivemetrics,weconductauserstudytogauge
the alignment of the edited video quality with human preferences, covering aspects such as text
alignment,backgroundchanges,temporalconsistency,andoverallimpressions.
4.3 UserStudy
Videoeditingandinpaintingisasubjectivetask,wherequalityofresultscannotbeevaluatedwith
quantitative metrics alone. Hence, we also conduct a user preference study (with 15 users, 195
questionresponses),whereusersareshownvideosofbaselinesandourmethod,andareaskedtopick
thevideowithbesttextalignment,backgroundconsistency(foreditedvideo),temporalconsistency
(iftheinpaintedobjectisconsistentinappearanceacrossframes)andoverallvisualquality(least
blurrinessandextraartifacts). Figure4showsthatuserspreferInViacrossallquestions∼75%times.
WhileTokenflowGeyeretal.[2023]ispreferred∼15%oftimesacrossallthequalitativecategories.
Moredetailscanbefoundinsupplementalmaterials.
74.4 QualitativeEvaluation
AsdepictedinFigure7,weconductacomparativeanalysisofInViagainstprominentbaselines. Our
approach,representedinthebottomrow,demonstratessuperiorperformancebycloselyadhering
to editing instructions and ensuring temporal coherence in the edited videos. Conversely, other
techniquesoftenstruggletoachievebothobjectivessimultaneously. Tune-A-VideoWuetal.[2023]
expandsa2Dimagemodelintoavideomodelandfine-tunesittofollowthevideo’smovementclosely.
Whileeffectiveforshortclips,itencounterschallengesinaccuratelycapturingmovementinlonger
videos,resultinginvisualartifactssuchascartoonishappearancesintheeditedvideos,asobservedin
thecarexample. Similarly,fate-zeroalsoexhibitsartifactsanddeviatesfromtheeditingtext-prompt
closely. AlthoughTokenFlowGeyeretal.[2023]yieldsreasonableresultsoverall,itfailstoperform
well for inpainting. While it effectively edits rigid objects using flow, it struggles with inserting
articulatedmovingobjectslikewalkingpeople. Moreover,allbaselinesexhibitinconsistenciesin
maintainingthebackgroundconsistentwiththesourcevideo,oftenmodifyingthebackgroundalong
withtheobjecttobeedited. Throughacomprehensiveuserstudyandqualitativeassessments,as
showninTable1andFigure5,wedemonstratethatInViexcelsinpreservingbackgroundconsistency
whileinsertingnewobjectsintothescene.
“A red suv driving down a curvy road in the countryside”
Figure 6: Ablation experiments: We make simple changes to the baseline methods. Frm+Inp
conductsframe-wiseinpaintingusingaconstantseedandprompt. Tokenflowpreservestheexact
structureoftheoriginaljeep(likepreservesthegrillsandmostlychangesthecolor). TokenFlow+Inp
combinesControlNetalongwithaninpaintingmethod,servingasabaselineforinpainting,butleads
toblurryresults. TokenFlow+Inp(NoFlow)removesthenearest-neighborfieldcomputationfrom
Tokenflow,andkeepstheslidingwindowbasedinpaintingof2framesatatime. Finally,InVi,which
surpasses these methods in terms of clarity, consistency, and sharpness, establishing itself as the
preferredchoiceforinpaintingtasks.
8
pnI+wolFnekoT
oediV
ecruoS
pnI+mrF
wolFnekoT
pnI+wolFnekoT
iVnI
)wolF
oN(Figure7: Inourqualitativecomparison,wecontrasttheperformanceofInViwiththreebaseline
methods: FateZero,TokenFlow,andTune-a-video. FateZerofrequentlydivergesfromtheediting
prompt, as seen in the woman running example. Meanwhile, both TokenFlow and Tune-a-video
unintentionallymodifythebackground. InVi,however,consistentlyyieldsresultsthatcloselyalign
withtheeditingpromptwhilepreservingthebackground.
4.5 Ablation
4.5.1 AdvancingBeyondTokenFlowforInpaintingTasks
TokenFlowGeyeretal.[2023]primarilyworksforvideoeditingtasks,andreliesonopticalflowin
thelatentspace. However,asseeninFigure6(Row3),itresultsincolorleakageineditedobjects
andunwantedcolorsaturationchangesinbackgroundcolors. Tocomparewithaninpaintingpipeline,
we modified TokenFlow to include a 9-channel inpainting based UNet for inference alongside
ControlNetZhangetal.[2023]. Thismitigatesthecolorleakageissuesandenhancesbackground
consistency,asseeninRow4ofFigure6,butleadstoblurryandunrealisticvideooutputs.TokenFlow
reliesontwomaincomponents: (i)Extended-attention,whichselectsandedits5-6framessparsely
fromthevideo,ensuringconsistentappearanceacrossallframes,and(ii)Flowpropagationacross
otherframes,whichcomputeslatentsforuneditedframesthroughinterpolationinthelatentspace
basedoneditedframes. OurhypothesissuggeststhattheblurrinessobservedinTokenFlowresults
fromflowcomputation. Weexperimentwithusingonlyextended-attentionwithaslidingwindow,
whichresultsinsharperinpaintedobjects(Row5).
InVieditsoneframeandrecursivelyusethegeneratedframesforeditingtheremainingvideo,which
ensures consistency in appearance throughout the video. Because of the recursive approach, we
donotneedtojointlygenerateK framessampledacrossthevideo,butweonlyusethepreviously
generatedframewhilegeneratingthenextframe. Hence, ourmemoryusageonlyincreasesbya
factorof2comparedtoTokenflowGeyeretal.[2023].
5 ConclusionandFutureWork
Wepresentedanewapproachtousetext-to-imagemodelsforvideoinpaintingtasks,usingoff-the-
shelf models which operates without the need for video-specific training. By harnessing DDIM
inverted latents extracted from the source video and incorporating the structural information of
9new objects via conditional ControlNet inputs, InVi seamlessly inpaints new objects into scenes.
Utilizinganchor-framebasedextended-attentionforeditingframes,InViensuresbothconsistencyin
appearanceandstructureoftheinsertedobject. Ourmethodsurpassesexistingbaselines,showcasing
significantenhancementsintemporalconsistencyandvisualfidelity. Moreover,unlikepriormethods,
InViefficientlyhandleslongervideoswithlimitedGPUmemoryandenablestheinsertionofdynamic
objectswithoutrequiringanexplicitmotionmodule. Onelimitationofourworkisthatourmethod
relieson2Dboundingboxesineachframe,whichcaneitherbeprovidedbytheuserorestimated
usingthegeometryofthescene. Infuturework,weplantoautomatethegenerationoftheseboxes
usingGPTbasedlayoutgenerationtechniques,sothatitcanbemorebroadlyapplicable. Asourwork
buildsuponexistingimagegenerationmethods,weinheritboththepositiveandnegativesocietal
impactofsuchmethods.
References
TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix: Learningtofollowimage
editinginstructions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18392–18402,2023.
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiangZheng. Masactrl:
Tuning-freemutualself-attentioncontrolforconsistentimagesynthesisandediting. arXivpreprint
arXiv:2304.08465,2023.
Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang-Jin
Lin. Control-a-video: Controllable text-to-video generation with diffusion models. ArXiv,
abs/2305.13840,2023. URLhttps://api.semanticscholar.org/CorpusID:258841645.
PatrickEsser,JohnathanChiu,ParmidaAtighehchian,JonathanGranskog,andAnastasisGermanidis.
Structure and content-guided video synthesis with diffusion models. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages7346–7356,2023.
MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel. Tokenflow: Consistentdiffusionfeatures
forconsistentvideoediting. arXivpreprintarxiv:2307.10373,2023.
Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu,
DavidJunhaoZhang,MikeZhengShou,andKevinTang. Videoswap: Customizedvideosubject
swappingwithinteractivesemanticpointcorrespondence. ArXiv,abs/2312.02087,2023. URL
https://api.semanticscholar.org/CorpusID:265609343.
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,Y.Qiao,DahuaLin,andBoDai. Animated-
iff: Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. ArXiv,
abs/2307.04725,2023a. URLhttps://api.semanticscholar.org/CorpusID:259501509.
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725,2023b.
AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or. Prompt-
to-promptimageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International
ConferenceonLearningRepresentations,2022.
LevonKhachatryan,AndranikMovsisyan,VahramTadevosyan,RobertoHenschel,ZhangyangWang,
ShantNavasardyan,andHumphreyShi. Text2video-zero: Text-to-imagediffusionmodelsare
zero-shotvideogenerators. arXivpreprintarXiv:2303.13439,2023.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXivpreprint
arXiv:2304.02643,2023.
10ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia. Video-p2p: Videoeditingwith
cross-attentioncontrol. arXivpreprintarXiv:2303.04761,2023a.
ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. Groundingdino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023b.
AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,andLucVanGool.
Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages11461–11471,2022.
SangminOh,AnthonyJ.Hoogs,A.G.AmithaPerera,NareshP.Cuntoor,Chia-ChihChen,JongTaek
Lee,SaurajitMukherjee,JakeK.Aggarwal,HyungtaeLee,LarryS.Davis,EranSwears,Xiaoyang
Wang, Qiang Ji, Kishore K. Reddy, Mubarak Shah, Carl Vondrick, Hamed Pirsiavash, Deva
Ramanan, Jenny Yuen, Antonio Torralba, Bi Song, Anesco Fong, Amit K. Roy-Chowdhury,
and Mita Desai. A large-scale benchmark dataset for event recognition in surveillance video.
CVPR2011,pages3153–3160,2011. URLhttps://api.semanticscholar.org/CorpusID:
263882069.
Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and
AlexanderSorkine-Hornung. Abenchmarkdatasetandevaluationmethodologyforvideoobject
segmentation. 2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages
724–732,2016. URLhttps://api.semanticscholar.org/CorpusID:1949934.
Koutilya PNVR, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, and David Jacobs. Ld-znet: A
latentdiffusionapproachfortext-basedimagesegmentation. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages4157–4168,October2023.
ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,YingShan,andQifeng
Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. 2023 IEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages15886–15896,2023. URLhttps:
//api.semanticscholar.org/CorpusID:257557738.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. URL https:
//api.semanticscholar.org/CorpusID:248097655.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pages10684–10695,2022.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMedicalImageComputingandComputer-AssistedIntervention–MICCAI
2015: 18thInternationalConference,Munich,Germany,October5-9,2015,Proceedings,PartIII
18,pages234–241.Springer,2015.
NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InPro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
22500–22510,2023.
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding.AdvancesinNeuralInformation
ProcessingSystems,35:36479–36494,2022.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b: An
open large-scale dataset for training next generation image-text models. Advances in Neural
InformationProcessingSystems,35:25278–25294,2022.
AshishShrivastava,TomasPfister,OncelTuzel,JoshuaSusskind,WendaWang,andRussellWebb.
Learningfromsimulatedandunsupervisedimagesthroughadversarialtraining. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,2017.
11RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,ArseniiAshukha,
AlekseiSilvestrov,NaejinKong,HarshithGoka,KiwoongPark,andVictorLempitsky. Resolution-
robustlargemaskinpaintingwithfourierconvolutions. InProceedingsoftheIEEE/CVFwinter
conferenceonapplicationsofcomputervision,pages2149–2159,2022.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfor
text-driven image-to-image translation. 2023 IEEE/CVF Conference on Computer Vision and
PatternRecognition(CVPR),pages1921–1930,2022. URLhttps://api.semanticscholar.
org/CorpusID:253801961.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfor
text-driven image-to-image translation. 2023 IEEE/CVF Conference on Computer Vision and
PatternRecognition(CVPR),pages1921–1930,2023. URLhttps://api.semanticscholar.
org/CorpusID:253801961.
AshishVaswani,NoamM.Shazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeuralInformationProcessing
Systems,2017. URLhttps://api.semanticscholar.org/CorpusID:13756489.
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. arXivpreprintarXiv:2306.02018,2023.
JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,WynneHsu,
YingShan,XiaohuQie,andMikeZhengShou. Tune-a-video: One-shottuningofimagediffusion
modelsfortext-to-videogeneration. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages7623–7633,2023.
ShuaiYang,YifanZhou,ZiweiLiu,andChenChangeLoy. Rerenderavideo: Zero-shottext-guided
video-to-videotranslation. arXivpreprintarXiv:2306.07954,2023.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. 2023IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages
3813–3824,2023. URLhttps://api.semanticscholar.org/CorpusID:256827727.
RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition(CVPR),June2018.
12A Preliminaries
Weintroduceconceptsthatarerequiredtounderstandourmethods. Readersareencouragedtorefer
tothemethodsforamorein-depthtreatment.
DiffusionmodelsHoetal.[2020]graduallyintroduceGaussiannoisetoasamplex ∼q(x )over
0 0
T steps,yieldingnoisysamplesx ,t=1,...,T. Thedistributionofthesenoisysamplesisgoverned
√ t
byq(x |x )=N(x ; α x ,β I),whereβ denotesthenoisevarianceatadiffusionsteptand
t t−1 t t t−1 t t
α = 1−β . Eventually,thisforwardprocessleadstox ∼ N(0,I),renderingtheimagex as
t t T T
whitenoise. Conversely,thereverseprocessinverselyappliestheaforementionedprocedurethrough
the θ-parameterized Gaussian distribution: p (x |x ) = N(x ;µ (x ,t),β I). The learning
θ t−1 t t−1 θ t t
involvesestimatingµ tobeabletogenerateadatasamplefromnoiseinT reverseprocesssteps.
θ
Latent Diffusion Models (LDM) Rombach et al. [2022] improved the learning and generation
processbyshiftingitfromtheimagespacetoalatentspace. Theimageisencodedintothelatent
spaceusinganencoder,E,andboththeforwardandreversediffusionprocessesoccurinthislatent
space. Thelatentspacesamplesareconvertedbackintoanimagesamplesusingadecoder,D. The
denoisingmodel,basedontheU-NetarchitectureRonnebergeretal.[2015],iscomposedofself-
attentionlayersVaswanietal.[2017]andcross-attentionlayersVaswanietal.[2017]toseamlessly
integrate textual conditions. These models are also referred to as text-to-image models as a text
promptcanbeconvertedintotokensandusedwithincrossattentionlayersoftheU-Netmodel.
DreamBooth-LoRAbasedfine-tuningRuizetal.[2023],Huetal.[2022]helpspersonalizethe
diffusion model by creating a unique prompt and “binding” it with a specific image. To achieve
this“binding”,first,wegenerateapromptwithauniqueidentifier: “a[V][classnoun]”,where[V]
denotesauniqueidentifierlinkedtothesubjectand[classnoun]representsacoarseclassdescriptor
ofthesubject(e.g. boy, horse, etc.). Next, weconditionthediffusionmodelonthispromptand
fine-tune it using the LoRA Hu et al. [2022] technique, ensuring that the prompt aligns with the
providedimage.LoRAinvolvescreatingaduplicatesetoftheoriginaldiffusionweights,representing
themwithlow-rankmatrices,andexclusivelytrainingtheselow-rankmatriceswhilemaintainingthe
originalnetwork’sfrozenstate. Thetraininglow-rankmatricesarethenmergedwiththeoriginal
frozenweights,preservingthearchitectureandkeepingtheinferencetimeidenticaltotheoriginal
model. Thisapproachisthesameforinpaintingbaseddiffusionmodels.
DDIMInversion(DDIM-Inv)convertsacleansamplex toitsnoisyversioninreversesteps:
√ 0
z
=√
α
z t− 1− √α tϵ θ(z t,t,p) +(cid:112)
1−α ϵ (z ,t,p), t=0,...,T −1.
t+1 t+1 α t+1 θ t
t
Thedifferencebetweentheforwarddiffusionprocess(FDP)andDDIM-Invisinthenoisegeneration
mechanism. IntheFDP,noiseissampledfromaGaussiandistribution,whereasinDDIM-Inv,the
noiseistheoutputoftheU-Netmodel.
B UserStudy
Weevaluateourapproachwithauserstudy,for13text-videopairsand15participants. Theusers
wereshownsourcevideo,and3-4methods,randomized(withInViincluded),andareexpectedto
answer3questions. Therearetwotypesofvideos: (a)videosforObjectswappingand(b)videosfor
objectinsertion. Inobjectswappingvideo,thesourcevideoalsohastheobjects,whicharemodified
withaprompt. InObjectinsertion,thesourcevideodonothavetheobject,andusingconditioned
control images, we insert a new object in the scene. Moreover, for object swapping videos, we
use existing video editing methods are baselines: FateZero Qi et al. [2023], Tune-A-Video Wu
etal.[2023]andTokenFlowGeyeretal.[2023]. Forobjectinsertion,therearenovideoinpainting
pipelines using text-to-image pre-trained models. Hence we use baselines Framewise inpainting
(Frm+Inp) and TokenFlow with Controlnet and inpainting pipeline (Tokenflow+Inp). For object
swapping,weaskusersthefollowingquestions:
• Whichvideodemonstratesthehighestconsistencywithrespecttothesourcevideo?
ChoosethemethodwhichisBESTatpreservingthebackgroundfromthesourcevideo.
• Whichvideoalignsmostaccuratelywiththeprovidedtextprompt?
ChoosethemethodwhichBESTcapturesthedetailsinthe prompt(givenon topofthe
video).
13Figure8: SurveyPreviewforObjectswappingvideos. Theusersareshown4videosalongwith
sourcevideoandpromptusedforediting,toanswerquestionsaboutvisualquality,textalignment
andbackgroundconsistency.
• Whichvideodemonstratesthehighestvisualquality?
ChooseavideowhichhastheLEASTamountofextraartifacts(jitter,unwantedblobs),
blurriness,unrealisticlightingandflickering.
Forobjectinsertion,weaskusersthefollowingquestions:
• Whichvideodemonstratesthehighesttemporalconsistencyacrossnewobjectappearance?
Choose the video with the BEST appearance consistency overtime for the new inserted
object.
• Whichvideoalignsmostaccuratelywiththeprovidedtextprompt?
ChoosethemethodwhichBESTcapturesthedetailsinthe prompt(givenon topofthe
video).
• Whichvideodemonstratesthehighestvisualquality?
ChooseavideowhichhastheLEASTamountofextraartifacts(jitter,unwantedblobs),
blurriness,unrealisticlightingandflickering.
14Figure9:SurveyPreviewforObjectInsertionvideos.Theusersareshown3videosalongwithsource
videoandpromptusedforediting,toanswerquestionsaboutoverallvisualquality,textalignment
andtemporalconsistency.
15