MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test
with Open-domain Information Extraction Large Language Models
ChengguangGan1*,QingyuYin2,XinyangHe3,HanjunWei4,YunhaoLiang3,
YounghunLim1,ShijianWang5,HexiangHuang6,QinghaoZhang7,
ShiwenNi8†,TatsunoriMori1†
1YokohamaNationalUniversity,2ZhejiangUniversity,3UniversityofChineseAcademyofSciences,
4ChengduInstituteofComputerApplications,ChineseAcademyofSciences,
5SoutheastUniversity,6UniversityofTsukuba,7PusanNationalUniversity,
8ShenzhenInstituteofAdvancedTechnology,ChineseAcademyofSciences
Abstract Sentence
Classification Named Entitiy Recognition
Nature Giant pandas are mammals, endemic to China.
TheMutualReinforcementEffect(MRE)repre-
sentsapromisingavenueininformationextrac-
Animal Name pandas Nation China
tionandmultitaskingresearch. Nevertheless,
itsapplicabilityhasbeenconstrainedduetothe Part-of-Speech Sentiment Analysis
Mutual
exclusiveavailabilityofMREmixdatasetsin Negative Reinforcement The food in this restaurant was very bad and I will never return.
Effect
Japanese,therebylimitingcomprehensiveex-
Negative bad plorationbytheglobalresearchcommunity.To
addressthislimitation, weintroduceaMulti-
Relation Extraction
lingualMREmixdataset(MMM)thatencom- Sports Soccer striker Kylian Mbappe won the 2018 World Cup.
passes 21 sub-datasets in English, Japanese,
and Chinese. In this paper, we also propose Kylian Mbappe Position striker
a method for dataset translation assisted by
Event Extraction
Large Language Models (LLMs), which sig- Politics Rishi Sunak lost the UK general election in 2024.
nificantlyreducesthemanualannotationtime
required for dataset construction by leverag- Rishi Sunak lost UK general election 2024
ing LLMs to translate the original Japanese
Figure1:TheMutualReinforcementEffectbetweenthe
datasets. Additionally, we have enriched the
labelsofWord-levellabelsandtext-levellabelwithina
datasetbyincorporatingopen-domainNamed
sametext.
EntityRecognition(NER)andsentenceclassi-
ficationtasks. Utilizingthisexpandeddataset,
wedevelopedaunifiedinput-outputframework
2023;XiangandWang,2019). Traditionally,these
to train an Open-domain Information Extrac-
tionLargeLanguageModel(OIELLM).The IEsubtaskshavebeensegregatedintodistinctcate-
OIELLM model demonstrates the capability goriesforprocessing. Inconventionalmulti-taskIE
to effectively process novel MMM datasets, (Sunetal.,2023;Zhaoetal.,2020),datasetsfrom
exhibitingsignificantimprovementsinperfor- varioustasksaretypicallymergedandsubsequently
mance.
fine-tunedusingaunifiedmodel. Thisprocesscul-
minatesintheextractionofinformationfrommul-
1 Introduction
tiplesubtasks,eachdirectedbytask-specificoutput
Informationextraction(IE)Sarawagietal.(2008) heads. Whilethismethodeffectivelyleveragesthe
isasignificantareaofresearchwithinnaturallan- internalknowledgeofthemodelacrossdifferentIE
guage processing (NLP). This field has evolved tasks,itdoesnotaddressthepotentialinterconnec-
toencompassavarietyofsubtasks,includingsen- tionsamongthetasksthemselves. Thisomission
tenceclassification(),textclassification(),Named highlightsagapinunderstandinghowthesetasks
EntityRecognition(NER)(Quetal.,2023;Nadeau mightbenefitfromexploringtheirmutualrelation-
andSekine,2007;Lampleetal.,2016),sentiment ships.
analysis (Tan et al., 2023; Medhat et al., 2014; TheMutualReinforcementEffect(MRE)Gan
Rodríguez-Ibánezetal.,2023),relationshipextrac- etal.(2023b)representsanemergingresearchdi-
tion(Wadhwaetal.,2023;Mintzetal.,2009;Et- rectioninmultitaskingInformationExtraction(IE).
zionietal.,2008),andeventextraction(Gaoetal., It primarily explores the interconnections among
individualsubtasks,therebyenhancingtheperfor-
*gan-chengguang-pw@ynu.jp
*†Correspondingauthor mance of each task. MRE categorizes IE sub-
4202
luJ
51
]LC.sc[
1v35901.7042:viXratasksintotwoprincipalgroups: text-leveltasksand
word-leveltasks. Forinstance,conventionaltasks
suchassentenceclassification,textclassification,
and text sentiment analysis fall under text-level
tasks. Conversely,taskslikeNER,whichinvolve
classifying words, are categorized as word-level
tasks.
Figure1illustratesaspecificexampleofMRE
applied to IE subtasks. The left side of the fig-
uredepictslabelsforsentenceclassification,while
therightsidedisplayswordsalongsidetheircorre-
spondinglabelsextractedfromthesentence. Thus,
the left side corresponds to text-level tasks, and
the right side to word-level tasks. Notably, each
sentencefeaturesbothatext-levellabelandword-
Figure 2: Multilingual Mutual Reinforcement Effect
levellabelentitiespair. ThisdiffersfromtypicalIE
Mix Datasets Names of all sub-datasets. (The image
multitasking,whichextractsinformationfromvari-
doesnotrepresentapercentageoftheactualsubdataset
oussub-tasksacrossdifferenttexts. Instead,MRE
size.)
simultaneously performs text-level classification
andword-levellabel-entitiespairingonthesame
text. lated Multilingual Mutual Reinforcement Effect
WecanelucidatetheMREwithspecificexam- Mix (MMM)Datasets. Notably, the SCPOSsub-
ples. ConsiderthesentencefromFigure1: ’Giant datasetwithintheoriginalJapanesedatasetissub-
pandasaremammals,endemictoChina.’ Thissen- stantiallylargerthanothers,whichiswhythefigure
tenceiscategorizedunder’nature,’anditincludes does not depict the proportional sizes of the sub-
twoannotatedlabel-entitiespairs: ’AnimalName: datasets;instead,itmerelylabelseachsubdataset
pandas’and’Nation: China.’ Thisexemplifiesthat within the MMM dataset. This dataset encom-
whenasentenceisclassifiedasrelatingtonature, passesthreelanguagesandincludessevendistinct
itlikelycontainstermsassociatedwithnaturalel- subdatasetsforeachlanguage,eachcorresponding
ements, such as species names and geographical toaspecificinformationextractiontask.
locations. Similarly,whenasentenceisclassified Among these, SCNM denotes the Sentence
as nature, there is a high probability that the sen- ClassificationandNamedEntityRecognitionMix
tence as a whole describes nature. The interplay Dataset. SCPOS, which stands for Sentiment
betweenthesetwolevelsoftextlevelclassification ClassificationandPart-of-SpeechDataset,involves
and word-level entity recognition illustrates the comprehensivesentimentpolarityclassificationat
MRE,aseachlevelmutuallyreinforcesunderstand- both the text and word levels. This subdataset is
ing and classification accuracy. Expanding upon furthersegmentedintofourcategories: sentiment-
this,MREisapplicabletovarioussubtasksinIE. relatedwords,adjectivesandnouns,solelyadjec-
Forinstance,insentimentanalysis,thepresenceof tives,andsolelynouns. Thiscategorizationfacili-
numerouswordswithpositivesentimenttypically tatesanexaminationofhowwordswithdifferent
indicates that the text overall conveys a positive lexical properties influence the overall sentiment
sentiment. Conversely,ifatextisoverallnegative, classificationofatext.
it likely contains words with negative sentiment. Additionally, the TCREE dataset is dedicated
Integrating tasks at both the text and word levels to Text Classification and Relation & Event Ex-
notonlyaidsinthemodel’scomprehensionofthe traction,leveragingmixeddatatypesforenhanced
contextbutalsoimprovesperformanceacrossboth analyticaldepth. Lastly,TCONERrepresentsthe
tasks. ThismethodologyprobeswhetherLLMscan Open-Domain NER and Text Classification, uti-
interpret texts similarly to human understanding, lizinganexistingopen-domaindatasetforannota-
byassessingthesemanticsofindividualwordsand tions. This structure allows for a nuanced explo-
thensynthesizingtheseinsightstoclassifythetext rationoftextclassificationandentityrecognition
asawhole(Ganetal.,2023c). acrossvariousdomains.
Figure2illustratesthecompositionofthetrans- Thisworkbeginsbyintroducinganoveldatasettranslation framework. Initially, six MRE mix (2023b) dataset in Japanese, followed by the SC-
datasets were translated, and subsequently, the POS (Gan et al., 2023d) and TCREE Gan et al.
TCONERdatasetwasexpanded. Furthermore,to (2023a) datasets. However, the exclusive use of
address the suboptimal performance of existing theJapaneselanguageacrossthesedatasetsposes
large language models LLMs on IE tasks, we re- significantchallengesforresearchersattemptingto
fined the training methods. We propose a new, furtherexploretheMRE.Moreover,therehasbeen
streamlined,andefficientinput-outputschemethat agrowinginterestinemployingLLMsfordataset
standardizes the handling of all tasks, enhancing construction(Tanetal.,2024;Wadhwaetal.,2023;
themodel’strainingprocess. TheLLMwasthen Li et al., 2023; Laskar et al., 2023). Pioneering
trained using the MMM dataset. The outcomes studiesHuangetal.(2023)havedemonstratedthe
indicate that the optimized LLM, referred to as efficacyofLLMsindataannotation,whereLLM-
OIELLM, surpasses the original training method annotateddatasetshaveoutperformedmanuallyan-
inperformanceacrossvariousdatasets. Moreover, notated counterparts. For instance, LLMs have
expanding the use of more MRE mix datasets is beenutilizedtogeneratedatasetsformathematical
showntofurtherenhancetheknowledgeutilization problemsLinetal.(2024)andtodevelopdataset
withintheLLM,leadingtoimprovedperformance labelingframeworks,suchasFreeAL(Xiaoetal.,
acrossalltasks. 2023a),wherethedataisinitiallylabeledbyLLMs
Thispaperpresentsseveralcontributionsarede- and subsequently refined by smaller models be-
tailedasfollows: foreundergoingafinal,moreaccuratelabelingby
LLMsagain.
1. Development of a Novel Framework for These methodologies leverage instructional
Dataset Translation: We propose a new learning and in-context learning to guide LLMs
framework that leverages existing LLMs to to respond to specific queries and from these re-
translate datasets for underrepresented lan- sponses,extractannotatedlabels,therebycreating
guages. This framework facilitates the con- afullylabeleddataset. Distinctfrompreviousef-
structionofMultilingualMREMixDatasets, forts, the MMM dataset represents the inaugural
significantly reducing the reliance on exten- initiativetotranslatedatasetsfromlesser-usedlan-
sivehumanlaborandaddressingtheissueof guagesintomorewidelyspokenlanguages, such
datasetscarcityinlinguisticresearch. as English and Chinese. Furthermore, the newly
developed TCONER dataset addresses a critical
2. ExpansionofMREMixDatasets: Wehave
gapbyprovidingthefirstopen-domainNamedEn-
successfullyexpandedtheMREMixdatasets
tityRecognition(NER)datasetwithintheexisting
toincludeanewlyconstructedopen-domain
frameworkoftheMREmixdataset.
NER dataset. This addition serves as a criti-
LLMinInformationExtraction. Sincethein-
calcomponentinenhancingthecomprehen-
troductionofPretrainedLanguageModels(PLMs),
sivenessandutilityoftheMREMixdatasets,
sequential-to-sequential(seq2seq)basedIEmod-
effectivelyplacingthelastpieceinthepuzzle
elshavegainedprominence. Thesedevelopments
foropen-domainapplications.
rangefromtheinitialUIELuetal.(2022)tolater
modelssuchasUSMLouetal.(2023)andMirror
3. OptimizationofInputandOutputSchemes
(Zhu et al., 2023). All these models are genera-
for IE LLMs: Our research optimizes the
tive in nature, enabling them to handle multiple
traditional input and output mechanisms of
word-level IE tasks—such as NER, Relation Ex-
informationextractionLLMs. Byemploying
traction,andEventExtractionsimultaneously. The
the specially constructed MMM dataset, we
primaryadvantageofthesegenerativeIEmodels
notonlyrejectthestandardmodelstrainedon
istheirgeneralizability;theyeliminatetheneedfor
genericdatabutalsosignificantlyenhancethe
task-specificfine-tuningacrossdifferenttasks. In-
performance of LLMs within the domain of
stead,asinglemodelcanaddressallIEsubtasksby
informationextraction.
standardizingtheformatofinputsandoutputsfor
varioustasks. Themodelistrainedacrossdifferent
2 RelatedWork
IEsubtasksusingtheseunifiedformats,aimingto
Datasets. To begin, the MRE mix dataset pri- equipasinglemodelwiththecapabilitytomanage
marily originates from the SCNM Gan et al. multipletaskseffectively.Input Translated Dataset Samples
Translated Text-level &
Translate the following Japanese data set into English. There are two
Word-level Label Part requirements. The first requirement is that the output extraction sequence
Instruction must be formatted in accordance with the input and the example. The
second is to ensure that the words in the extracted sequence after
translation can be found in the same field in the text. Here is an example.
Manual
Rule-base
Matching Calibration
Input: text: 13日、約7年半ぶりにJリーグ復帰を果たした横浜F・マリノス
In-context リ所 ー属 グ・ ;中 13村 日俊 :/n輔 t。 rane sv le an tet de x ot ura tpc uti to : n te 抽 xt出 : O序 n 列 th： e 1 : 3中 th村 , j俊 un輔 su; kre ei n ns at ka ate mm ue ran t o;J f
Untranslated Text-level & Learning yokohama F · marinos returned to the j. league after about seven and a
Word-level Label Part half years. event extraction extracted sequence: :shunsuke
nakamura;reinstate sm eqe un et; nJ c l ee :a {g }u /ne t; rT ah ne sl a1 t3 ioth n: o/n u tI pn up tu :t: text: {} extracted Rule-base
Filter
Datasets
Translated Text-level & GPT-3.5
Word-level Label Part
Text and Entities Part
Figure3: Theoverviewofdatasettranslationframework.
With the advent of LLMs, new approaches to 3.1 DatasetTranslationFramework
IE have emerged, which can be broadly divided
First, it is essential to understand the format of
into two categories. The first involves direct in-
theMultilingualMutualReinforcementEffectMix
teractionwithLLMsusingpromptsinazero-shot
(MMM) dataset. As depicted in Figure 4, the
orfew-shotmanner,wherethemodeloutputsthe
MMMdatasetcomprisesinputsandoutputs. The
desiredentitieseitherthroughmulti-rounddialog-
input section, highlighted in blue, includes both
style prompts or through single-command-based
text and a task instruction word, such as "NER."
promptsthatextractentitiesinonego(Wangetal.,
In the output section, shown in green, the ini-
2023; Wei et al., 2023). The second approach in-
tial output is a text-level classification label, fol-
volvesfine-tuningLLMsusingspecializeddatasets
lowed by the task instruction word "NER." The
(Zhouetal.,2023;Xiaoetal.,2023b).
labeling follows the start and end symbols (i.e.,
Our research distinguishes itself by focusing
":", ";") used in the original MRE mixed dataset.
more intensively on the MRE. We go beyond
This format allows for consistent generation of
merely aggregating existing IE sub-datasets for
label-entitypairsregardlessofquantity(e.g.,":la-
model training. Instead, we develop specialized
bel1;entities1:label2;entities2..."). Thus, the task
MRE-enhanced datasets, through which we not
instruction word guides the model in producing
only demonstrate but also apply the efficacy of
variousword-levelextractedinformationalongside
MREinenhancinginformationextractioncapabili-
thetext-levelclassificationlabel.
ties.
Figure3presentsaflowchartoftheentiredataset
translationframework. Theprocessbeginsonthe
3 MultilingualMutualReinforcement
leftmost side, where six sub-datasets are initially
EffectMixDatasets
processedusingarule-basedmatchingmethod,ac-
cordingtotheirclassifications. Thelabelsatboth
In this chapter we will explain how to translate
textandwordlevelsaresystematicallytranslated
MREmixdatasetsinsmalllanguagesintootherlan-
intoEnglishandChinese. Giventheconsistentla-
guages. AndhowtoconstructTCONERdatasets.
belingacrossdatasets,thistranslationcanproceed
Andhowyoucanminimizetheuseofmanuallabor
directlybasedonpredefinedrules. Forinstance,the
withguaranteedquality. Japaneselabel"ポジティブ"isdirectlytranslated
as"positive."Employingarule-basedapproachfor
Input Giant pandas are mammals, endemic to China. NER labeltranslationisnotonlyquickandprecisebut
also simplifies the subsequent translation of text
Output Nature NER :Animal Name;pandas:Nation;China and entities. Furthermore, these translated labels
areinputintoaLLMalongwiththeuntranslated
Figure4: TheformatofMMMdatasets. text and entities, serving an auxiliary role in the
translationprocess.The process involves two main inputs to the tions,thisframeworkcanbeadaptedfortranslating
LLM, GPT-3.5-Turbo Ouyang et al. (2022): the datasetsforothertasks,effectivelyaddressingthe
partwithtranslatedlabelsandthepartwithuntrans- scarcityofdatasetsinlesser-usedlanguages.
latedtextandentities. Weemploybothinstruction-
basedandin-contextlearning(ICL)methodologies
3.2 ConstructionofTCONER
forthistranslationtask. Asdepictedinthecentral
portionofFigure3,theselectionoftheinstruction
In the original MRE mix datasets, relation and
template was refined through multiple iterations.
eventextractiontasksareopen-domain,implying
Initially, a simple instruction such as "Translate
that the labels are not predefined. However, the
thefollowingJapanesedatasetintoEnglish."failed
labelsetislimitedtoonlyadozenoptions. Given
toproducesatisfactorytranslations. Consequently,
thiscontext,weconstructedanewdataset,termed
we introduced several constraints to enhance the
TCONER,basedonanopen-domainNamedEntity
output quality. These include stipulating that the
Recognition (NER) dataset* (Zhou et al., 2023).
model’soutputformatmustalignwiththeexample
ThelabelsatthetextlevelintheTCONERdataset
providedbelow,withacriticalrequirementbeing
are also open-domain. To annotate this dataset,
the accurate translation of entities, ensuring they
we initially employed the GPT-3.5-Turbo model
corresponddirectlytotermsfoundintheoriginal
to assign open-domain text-level labels. Subse-
Japanesetext. Additionalconstraintswereapplied
quentmanualverificationandannotationwerecon-
specifically for Japanese-to-Chinese translations,
ductedtoensureaccuracyandconsistency,result-
suchasinformingthemodelthatlabelshavebeen
ing in the finalized TCONER dataset. Similarly,
pre-translated and only text and entities require
we translated the constructed English TCONER
translation. Wealsoinstructedthemodeltoensure
dataset using the dataset translation framework.
comprehensive translation into Chinese. Further-
TheTCONERdatasetwastranslatedintoJapanese
more, a one-shot example of ICL was provided
andChinese.
to demonstrate the desired outcome, guiding the
modeltogeneratetranslationsstrictlyadheringto
thespecifiedformat. 3.3 ResultsofDatasetsConstruction
Finally,weobtainedthetranslateddataset. How-
ever,duetotheinherentunpredictabilityofLLM Appendix A Table 2 presents the statistics of the
outputs,itisnotalwaysguaranteedthattheoutputs finaltranslationresults. Duetothehighcostsasso-
will conform to the expected format, even when ciatedwiththeuseofapremiumAPI,welimited
the inputs are consistent. To address this, we im- ourstudyto10,000samplesfromeachofthreesub-
plemented a dual-component rule-based filtering datasetswithinSCPOSandtheTCONERdataset,
mechanism. Thefirstcomponentinvolvesremov- whichcontains180,000entries. These10,000sam-
ingsamplescontaininganyresidualJapanesechar- ples,retainedpost-translation,provedtobeanam-
actersfromthetranslateddata. Thesecondcompo- pletestset. Itwasobservedthattherewasagreater
nententailsverifyingwhetherthetranslatedentities datalosswhentranslatingintoChinesecompared
exactlymatchwordsinthetext. Samplesthatdo toEnglish. Thisdiscrepancymaybeattributedto
notmeetthiscriterionareexcluded. Additionally, thetrainingdatapredominanceofEnglishinOpe-
thisstepassesseswhetherthepairingsoflabelsand nAI’sGPT-3.5-Turbomodel,resultinginsuperior
entities adhere to the formatting standards of the performanceinEnglish-relatedtasks. Forinstance,
MMMdataset. intheSCNMandTCREEdatasets,theJapaneseto
Despitethesubstantialreductionindatasetsize Englishtranslationaccuracyexceeded80%. Con-
resultingfromthefirsttwosteps—translationand versely,thetranslationresultsfromEnglishtoChi-
filtering—the remaining data exhibit exception- neseintheTCONERdatasetweremarkedlybetter
allyhightranslationquality. Thefinaldatasetun- thanthosefromEnglishtoJapanese. Thisfurther
dergoes a manual review and correction process, confirms that GPT-3.5-Turbo exhibits enhanced
whichensuresmaximumaccuracywhileminimiz- effectiveness with major languages compared to
ing the reliance on manual labor. This approach lesser-usedones.
outlinesourtailoreddatasettranslationframework,
designedtoaccommodatethespecificcharacteris-
*https://huggingface.co/datasets/
ticsoftheMMMdataset. Withminimalmodifica- Universal-NER/Pile-NER-type?row=0Input
Task Insturct Word
Output
/NER
Text-level Word-level
Text1 /Sentiment Relation Word Labels Task Insturct Word Label-entities Pairs
Label1 /NER/ :Label1;Entities1...
/Sentiment Adj and N
Text2 OIELLM
/Sentiment Adj Label2 /Sentiment Adj and N/ :Label1;Entities1...
Text3
/Sentiment N
Label3 /Relation Extraction/ :Entities1;Label1;Entities2:
/Relation Extraction
/Event Extraction
Figure5: TheinputandoutputofOpen-domainInformationExtractionLargeLanguageModel(OIELLM).
4 Open-domainInformationExtraction traction. However, thisresearchadoptsadistinct
LargeLanguageModel methodology. Thisdepartureismotivatedbyear-
lier foundational studies in generic generative IE
In this chapter, we outline methodologies to en-
with PLMs, where dialogue models were not uti-
hancetheperformanceofexistingmodelsandtech-
lized. Instead,thesestudiesimplementedageneric
niques for processing MRE mix datasets, aiming
framework. Accordingly, we too will employ a
to surpass previous benchmarks. Before delving
modifiedinputandoutputschemetailoredforthe
intothespecificsoftheOpen-domainInformation
MMM dataset, diverging from the conventional
ExtractionLargeLanguageModel(OIELLM),it
dialogue-basedapproaches.
isimperativetojustifythenecessityforadistinct
Figure5illustratestheinputandoutputformats
modeltailoredtoMMMdatasets.
ofourenhancedOIELLM.Thefundamentalunit
Firstly, MRE mix datasets differ significantly
of analysis in both input and output is words, re-
from traditional IE tasks as they require simulta-
flectingourunderstandingofthetokenizationprin-
neous output of text-level labels and word-level
cipleutilizedbyLLMs,whichtypicallyfocuseson
label-entitypairs. Consequently,standardsequence
wordsorphrases. Byomittingthedialogprompt,
labelingmodelsareinadequateforhandlingthese
wedonotcompromisetheLLM’scomprehension
demandsdirectly. Furthermore,existinggenerative
ofthetask. Thisadjustmentnotonlyreducesthe
IEmodelsandmethodologieshavesolelyfocused
input-outputlengthbutalsosimplifiestheLLM’s
onproducingword-levellabel-entities,neglecting
processing,therebyenhancingoperationalspeed.
text-levellabelsaltogether.
Eachtextprocessedisprefixedwithtask-specific
The primary objective of MRE mix datasets is
instructionwords,whichdefinethetasktypeand
toinvestigatetheinterplaybetweentext-leveland
guide the model’s subsequent output generation.
word-levelannotations. Byleveragingthissyner-
Inourformat,alltaskinstructionwordsinthein-
gisticrelationship,weaimtoconcurrentlyenhance
putareintroducedbyaspecialsymbol"/",which
the performance of both tasks. This model im-
serves to delineate the task words from the main
provestextualunderstandingbylearningbothtasks
text. Thisseparationiscrucialfordistinguishingbe-
intandem. Additionally,theMREframeworkcan
tweentext-levellabelsandword-levellabel-entity
contributetomodelinterpretability,drawinginspi-
pairsintheoutput.
rationfromcognitiveprocessesthatmimichuman
reasoning. The combined text and task instruction words
In summary, this study proposes the develop- arethenfedintotheOIELLM,withtheoutputcom-
ment of a novel model specifically designed for prisingbothtext-levellabelsandword-levellabel-
processing the MMM dataset. Furthermore, we entitypairs. Ourlabelingconventionadherestothe
aimtoexperimentallyinvestigatewhethertheMRE formatusedinthepreviousMREmixdatasets,uti-
positivelyinfluencesvariousIEsubtaskswhenus- lizing":"and";"toensureconsistencyandclarity.
ing LLMs. Traditionally, IE tasks have been ap- Insummary,bystandardizingtheinputandout-
proachedthroughtheuseofQAdialoguesforex- putstructuresandclearlydefiningtaskinstructionJapanese SCNM SCPOS:RW SCPOS:Adj&N
Model TL WL ALL TL WL ALL TL WL ALL
USA-7B - - - 53.27 40.80 7.67 91.33 81.68 9.63
GIELLM-13B-jp 85.47 84.46 54.2 86.01 66.61 17.39 93.23 47.35 0.20
OIELLM-8B 84.73 88.53 61.93 86.50 54.76 12.40 89.13 14.88 0.40
OIELLM-8B* 87.30 89.28 64.00 88.20 53.79 12.30 89.63 15.84 0.73
OIELLM-13B 89.00 86.33 57.70 94.60 52.36 11.90 95.20 11.94 0.20
Japanese SCPOS:Adj SCPOS:N TCREE
Model TL WL ALL TL WL ALL TL WL ALL
USA-7B 91.43 45.51 51.77 92.03 81.30 9.73 - - -
GIELLM-13B-jp 93.67 45.06 55.67 92.83 46.42 0.33 97.47 79.01 77.89
OIELLM-8B 87.13 74.96 53.07 87.77 22.92 0.50 95.07 74.92 83.69
OIELLM-8B* 89.93 75.33 54.93 90.63 23.69 0.63 96.98 74.42 84.19
OIELLM-13B 94.00 60.69 42.50 94.70 18.07 0.60 97.08 73.82 84.19
English SCNM SCPOS:RW SCPOS:Adj&N
Model TL WL ALL TL WL ALL TL WL ALL
OIELLM-8B 82.30 81.36 52.53 72.17 49.60 11.82 76.57 18.00 1.67
OIELLM-8B* 85.43 82.38 55.43 74.75 49.93 12.81 79.77 19.28 2.27
OIELLM-13B 84.80 80.68 50.60 95.07 46.64 12.19 94.30 18.59 3.20
English SCPOS:Adj SCPOS:N TCREE
Model TL WL ALL TL WL ALL TL WL ALL
OIELLM-8B 75.47 51.85 32.33 76.10 28.67 1.27 80.87 21.77 33.67
OIELLM-8B* 76.60 51.95 33.17 78.67 27.45 0.73 80.23 25.90 22.37
OIELLM-13B 94.40 50.56 38.40 95.30 28.36 0.60 89.90 23.50 22.60
Chinese SCNM SCPOS:RW SCPOS:Adj&N
Model TL WL ALL TL WL ALL TL WL ALL
OIELLM-8B 84.90 71.90 46.40 89.29 45.75 9.93 92.33 8.75 0.33
OIELLM-8B* 86.33 69.97 46.77 92.27 46.20 10.60 94.50 8.46 0.40
OIELLM-13B 87.70 68.12 41.60 95.03 43.32 8.72 94.90 8.42 0.50
Chinese SCPOS:Adj SCPOS:N TCREE
Model TL WL ALL TL WL ALL TL WL ALL
OIELLM-8B 93.73 60.96 53.00 92.63 28.32 0.63 91.73 58.12 56.41
OIELLM-8B* 95.80 64.51 57.63 94.97 28.91 1.30 95.06 59.54 58.83
OIELLM-13B 96.00 60.68 54.90 95.20 27.77 1.00 95.26 56.91 56.00
TCONER English Japanese Chinese
Model TL WL ALL TL WL ALL TL WL ALL
OIELLM-8B 24.80 21.12 0.20 27.70 13.83 0.20 33.73 18.87 0
OIELLM-8B* 37.13 23.05 0.30 41.40 14.24 0.17 48.27 18.06 0.17
OIELLM-13B 40.30 19.23 0.30 43.40 13.02 0 47.70 15.72 0.30
Table1: TheF1scoreofMMMdatasets.TL：Text-Level.WL:Word-level.ALL:TLandWLarecorrectsimultaneously.words,ourmodifiedOIELLMeffectivelyprocesses ":",";"). EachLabel-entitypairwastreatedasan
allsub-datasetswithintheMMMframework. individualelementwithintheset. TheF1scorewas
segmented into three categories: Text-level (TL),
5 Experiment Word-level (WL), and ALL. These represent the
F1scoresatrespectivelevelsandtheaggregateF1
In this chapter, we detail specific experimen-
scorewhenbothlevelsareaccuratelypredictedin
tal procedures, including dataset sizes for the
anoutput. Fordetailedmethodologies,including
MMMdatasetandmethodologiesfortrainingthe
codesandformulas,pleaserefertoAppendixC.
OIELLM model, along with the evaluation tech-
niquesused.
6 Results
5.1 DetailsofOIELLMTraining
Table1presentstheexperimentalresultsofthree
We began by selecting baselines: USA-7B (IL + OIELLMmodelstrainedon21MMMsub-datasets.
ICL)† and GIELLM-13B-jp‡, previously utilized Notably, the model designated with an asterisk,
forprocessingtheMREmixeddatasets,servedas OIELLM-8B,wastrainedusingtheLLaMA3-8B-
comparative models. For the foundational archi- Instruct framework, whereas the remaining mod-
tecture of OIELLM, we chose the latest Instruct els were based on the LLaMA3-8B-Base frame-
andBaseversionofLLaMA3-8B§.SinceLLaMA3 work. These results demonstrate the enhanced
doesnotoffera13Bversion,weincorporatedthe performance of OIELLM in handling Japanese
LLaMA2-13BTouvronetal.(2023)modelaswell. data after incorporating multilingual capabilities.
WeattemptedtoevaluatetheMMMdatasetus- Impressively, OIELLM’s performance surpassed
ingtheGPT-3.5-Turbomodel;however,thismodel that of GIELLM-13B-jp on half of the datasets,
failedtoproducetheexpectedinformationandwas despite GIELLM-13B-jp being a model specifi-
unabletomaintainaconsistentformat,despitebe- callytailoredforJapanese. Thisobservationsup-
ingprovidedwithanadequatenumberoffew-shot portsthehypothesisthatintegratingmultilingual-
examplesfortraining. TheresultingF1-scorewas ismandmultitaskingcanmoreeffectivelyleverage
nearzero. Consequently,wedecidednottoselect theknowledgeembeddedinthepre-trainingofmul-
theGPT-3.5-Turbomodelforfurthertestinginour tilingualLLMs.
study. However, OIELLM’s performance on the
OIELLMwasfine-tunedusingfullparameters TCONERtaskwassuboptimal,whichweattribute
based on these three models. Training was con- to insufficient training data. Given that open-
ductedatBF16precision,whileinferencewasper- domaintasksrequireextensiveanddiversedatasets
formed at FP16. The training spanned 3 epochs comparedtodomain-specifictasks,thelimiteddata
withalearningrateof1e-5,utilizingcomputational mayhavehinderedthemodel’sperformance. This
resources including three A800 80GB and three areawillbeafocusofourfutureresearch,aiming
RTX 6000 Ada 48GB GPUs, with training dura- tounderstandandimprovethedatadependencies
tionsrangingfrom12to20hours. Forthetraining ofOIELLMinopen-domaincontexts.
andtestsets,Comprehensivestatisticsonthetrain-
7 ConclusionandFutureWork
ingandtestsetsareavailableinAppendixB.
Inthisstudy,weintroduceaframeworkthatutilizes
5.2 Evaluation
LLMstotranslatedatasets,therebyremovinglan-
WeemployedtheF1scoreasourprimarymetric
guagebarriersforresearchinless-representedlan-
for evaluation. Initially, the model’s output was
guages. Toaddressthedeficiencyofopen-domain
bifurcated into two segments based on the task-
IEtasksintheMREmixdataset, weconstructed
specificinstructword: theText-levelLabelandthe
theTCONERdataset. Additionally,wetrainedthe
Label-entitiespairs. Subsequently,Label-entities
OIELLM model using the newly created MMM
pairsweredelimitedusingstart-endsymbols(i.e.,
dataset.
†https://huggingface.co/ganchengguang/ FutureworkwillfocusonemployingtheMMM
USA-7B-instruction-incontext-learning dataset to further explore the Mutual Reinforce-
‡https:
ment Effect. We will also continue to enhance
//huggingface.co/ganchengguang/GIELLM-13B-jpllm
the performance of the OIELLM model in open-
§https://huggingface.co/meta-llama/
Meta-Llama-3-8B-Instruct domaininformationextractiontasks.8 Limitations textsummarization. InFindingsoftheAssociation
forComputationalLinguistics: EMNLP2023,pages
Duetoresourceconstraints,wewereunabletoem- 10245–10255.
ploythehigher-performingGPT-4-TurboOpenAI
Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan,
(2023) model as the base for our dataset transla-
NancyChen,ZhengyuanLiu,andDiyiYang.2023.
tion framework. Consequently, this model was
CoAnnotating: Uncertainty-guidedworkallocation
also not utilized during the testing phase on the betweenhumanandlargelanguagemodelsfordata
dataset. Infuturework,weaimtoleverageamore annotation. InProceedingsofthe2023Conference
advancedmodel,suchastheGPT-4-Turbo,toeval- onEmpiricalMethodsinNaturalLanguageProcess-
ing, pages 1487–1505, Singapore. Association for
uatetheMMMdataset,providedthatthenecessary
ComputationalLinguistics.
resourcesbecomeavailable.
QingwenLin,BoyanXu,ZhengtingHuang,andRuichu
Cai. 2024. From large to tiny: Distilling and re-
References fining mathematical expertise for math word prob-
lems with weakly supervision. arXiv preprint
OrenEtzioni,MicheleBanko,StephenSoderland,and
arXiv:2403.14390.
Daniel S Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
JieLou,YaojieLu,DaiDai,WeiJia,HongyuLin,Xi-
51(12):68–74.
anpei Han, LeSun, andHua Wu.2023. Universal
informationextractionasunifiedsemanticmatching.
ChengguangGan,QinghaoZhang,andTatsunoriMori.
Preprint,arXiv:2301.03282.
2023a. Giellm: Japanesegeneralinformationextrac-
tionlargelanguagemodelutilizingmutualreinforce-
Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu
menteffect. arXivpreprintarXiv:2311.06838.
Lin,XianpeiHan,LeSun,andHuaWu.2022. Uni-
ChengguangGan,QinghaoZhang,andTatsunoriMori. fied structure generation for universal information
2023b. Sentence-to-labelgenerationframeworkfor extraction. InProceedingsofthe60thAnnualMeet-
ingoftheAssociationforComputationalLinguistics
multi-task learning of japanese sentence classifica-
tionandnamedentityrecognition. InInternational (Volume1: LongPapers),pages5755–5772,Dublin,
ConferenceonApplicationsofNaturalLanguageto Ireland.AssociationforComputationalLinguistics.
InformationSystems,pages257–270.Springer.
Walaa Medhat, Ahmed Hassan, and Hoda Korashy.
ChengguangGan,QinghaoZhang,andTatsunoriMori. 2014. Sentiment analysis algorithms and applica-
2023c. Think from words (tfw): Initiating human- tions: A survey. Ain Shams engineering journal,
like cognition in large language models through 5(4):1093–1113.
think from words for japanese text-level classifica-
tion. arXivpreprintarXiv:2312.03458. MikeMintz,StevenBills,RionSnow,andDanJuraf-
sky.2009. Distantsupervisionforrelationextraction
ChengguangGan,QinghaoZhang,andTatsunoriMori. withoutlabeleddata. InProceedingsoftheJointCon-
2023d. Usa: Universal sentiment analysis model ferenceofthe47thAnnualMeetingoftheACLand
& construction of japanese sentiment text clas- the 4th International Joint Conference on Natural
sification and part of speech dataset. Preprint, Language Processing of the AFNLP, pages 1003–
arXiv:2309.03787. 1011.
JunGao,HuanZhao,ChanglongYu,andRuifengXu.
DavidNadeauandSatoshiSekine.2007. Asurveyof
2023. Exploringthefeasibilityofchatgptforevent
namedentityrecognitionandclassification. Lingvis-
extraction. arXivpreprintarXiv:2303.03836.
ticaeInvestigationes,30(1):3–26.
JiaxinHuang,ShixiangGu,LeHou,YuexinWu,Xuezhi
OpenAI. 2023. Gpt-4 technical report. Preprint,
Wang, Hongkun Yu, and Jiawei Han. 2023. Large
arXiv:2303.08774.
languagemodelscanself-improve. InProceedings
ofthe2023ConferenceonEmpiricalMethodsinNat-
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
uralLanguageProcessing,pages1051–1068,Singa-
CarrollWainwright,PamelaMishkin,ChongZhang,
pore.AssociationforComputationalLinguistics.
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
GuillaumeLample,MiguelBallesteros,SandeepSub- 2022. Training languagemodelsto followinstruc-
ramanian,KazuyaKawakami,andChrisDyer.2016. tions with human feedback. Advances in Neural
Neural architectures for named entity recognition. InformationProcessingSystems,35:27730–27744.
arXivpreprintarXiv:1603.01360.
Xiaoye Qu, Yingjie Gu, Qingrong Xia, Zechang Li,
Md Tahmid Rahman Laskar, Mizanur Rahman, Israt ZhefengWang,andBaoxingHuai.2023. Asurvey
Jahan,EnamulHoque,andJimmyHuang.2023. Can onarabicnamedentityrecognition: Past,recentad-
largelanguagemodelsfixdataannotationerrors? an vances, and future trends. IEEE Transactions on
empiricalstudyusingdebatepediaforquery-focused KnowledgeandDataEngineering.MargaritaRodríguez-Ibánez,AntonioCasánez-Ventura, A chat-enhanced instruction tuning framework for
FélixCastejón-Mateos,andPedro-ManuelCuenca- universal information extraction. arXiv preprint
Jiménez.2023. Areviewonsentimentanalysisfrom arXiv:2312.15548.
socialmediaplatforms. ExpertSystemswithAppli-
cations,223:119862. HeZhao,LongtaoHuang,RongZhang,QuanLu,and
HuiXue.2020. Spanmlt: Aspan-basedmulti-task
Sunita Sarawagi et al. 2008. Information extraction. learningframeworkforpair-wiseaspectandopinion
Foundations and Trends® in Databases, 1(3):261– termsextraction. InProceedingsofthe58thannual
377. meetingoftheassociationforcomputationallinguis-
tics,pages3239–3248.
KaiSun,RichongZhang,SamuelMensah,YongyiMao,
and Xudong Liu. 2023. Learning implicit and ex- Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen,
plicitmulti-taskinteractionsforinformationextrac- andHoifungPoon.2023. Universalner: Targeteddis-
tion. ACM Transactions on Information Systems, tillationfromlargelanguagemodelsforopennamed
41(2):1–29. entityrecognition. arXivpreprintarXiv:2308.03279.
Kian Long Tan, Chin Poo Lee, and Kian Ming Lim. TongZhu,JunfeiRen,ZijianYu,MengsongWu,Guo-
2023. Asurveyofsentimentanalysis: Approaches, liangZhang, XiaoyeQu, WenliangChen, Zhefeng
datasets, and future research. Applied Sciences, Wang, Baoxing Huai, and Min Zhang. 2023. Mir-
13(7):4550. ror: Auniversalframeworkforvariousinformation
extractiontasks. arXivpreprint. ArXiv:2311.05419
ZhenTan,AlimohammadBeigi,SongWang,Ruocheng [cs].
Guo,AmritaBhattacharjee,BohanJiang,Mansooreh
Karami,JundongLi,LuCheng,andHuanLiu.2024. A Statisticalresultsofthetranslated
Largelanguagemodelsfordataannotation: Asurvey.
MMMdataset
arXivpreprintarXiv:2402.13446.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Dataset SCNM SCPOS:RW SCPOS:
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
RW Adj&N
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open founda- Japanese 5343 2000 187528
tion and fine-tuned chat models. arXiv preprint English 4449 1312 4801
arXiv:2307.09288.
Chinese 3177 1406 3937
Somin Wadhwa, Silvio Amir, and Byron C Wallace.
2023. Revisitingrelationextractionintheeraoflarge Dataset SCPOS: SCPOS: TCREE
languagemodels. InProceedingsoftheconference. Adj N
AssociationforComputationalLinguistics.Meeting,
Japanese 187528 187528 2000
volume2023,page15566.NIHPublicAccess.
English 9132 5027 1910
XiaoWang,WeikangZhou,CanZu,HanXia,Tianze Chinese 7413 3920 1491
Chen, Yuansen Zhang, Rui Zheng, Junjie Ye,
QiZhang,TaoGui,etal.2023. Instructuie: Multi- Language English Japanese Chinese
taskinstructiontuningforunifiedinformationextrac-
TCONER 45888 6791 9047
tion. arXivpreprintarXiv:2304.08085.
XiangWei,XingyuCui,NingCheng,XiaobinWang, Table 2: Statistical results of the translated MMM
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, dataset. (Duetoresourceconstraints,weextractedonly
Yufeng Chen, Meishan Zhang, et al. 2023. Zero- 10,000samplesastranslationobjectsfromeachofthe
shotinformationextractionviachattingwithchatgpt. threeSCPOSsub-datasetsandtheTCONERdataset.)
arXivpreprintarXiv:2302.10205.
WeiXiangandBangWang.2019. Asurveyofeventex-
B StatisticalResultsofTrainandTest
tractionfromtext. IEEEAccess,7:173111–173137.
DatasetinOIELLM
RuixuanXiao, YiwenDong, JunboZhao, RunzeWu,
MinminLin,GangChen,andHaoboWang.2023a. As shown in Tables 3 and 4, the statistics for the
FreeAL:Towardshuman-freeactivelearninginthe
completetrainingandtestsetsoftheMMMdataset.
eraoflargelanguagemodels. InProceedingsofthe
The MMM dataset was segmented into 21 sub-
2023ConferenceonEmpiricalMethodsinNatural
Language Processing, pages 14520–14535, Singa- datasets. Training set sizes were assigned based
pore.AssociationforComputationalLinguistics. onthesizesofthesesub-datasets,categorizedinto
threegroups: 500,1000,and2000samples. Sam-
XinglinXiao,YijieWang,NanXu,YuqiWang,Hanx-
ples beyond these numbers were allocated to the
uan Yang, Minzheng Wang, Yin Luo, Lei Wang,
Wenji Mao, and Daniel Zeng. 2023b. Yayi-uie: testsets.Dataset SCNM SCPOS:RW SCPOS:
RW Adj&N
Japanese 1000 1000 1000
English 1000 500 1000
Chinese 1000 500 1000
Dataset SCPOS: SCPOS: TCREE
Adj N
Japanese 1000 1000 1000
English 1000 1000 500
Chinese 1000 1000 500
Language English Japanese Chinese
TCONER 2000 2000 2000 Algorithm1ParseTextLabelandEntityPairs
1: procedure PARSE_OUTPUT(output, in-
Table3: StatisticalresultsoftrainsetsofOIELLM.
struct_word,is_tcree)
Dataset SCNM SCPOS:RW SCPOS: 2: Input: output (String), instruct_word
(String),is_tcree(Boolean)
RW Adj&N
Japanese 4343 1000 186528 3: Output: text_label (String), entity_pairs
(SetofTuples)
English 3449 812 3801
Chinese 2177 906 2937 4:
5: instruct_word ←instruct_word
Dataset SCPOS: SCPOS: TCREE 6: ifinstruct_word ∈/ outputthen
Adj N 7: return(””,{})
Japanese 186528 186528 1000 8: endif
English 8132 4027 1410 9: text_label,entity_pairs ← out-
Chinese 6413 2920 991 put.split(instruct_word,1)
10: text_label ← text_label.strip()
Language English Japanese Chinese 11: ifis_tcreethen
TCONER 43888 4791 7047 12: entity_pairs ←
[entity_pairs.strip()]
Table4: Statisticalresultsoftestsets.
13: else
14: entity_pairs ← [pair.strip() for
C CalculateDetailofF1Score pair inentity_pairs.split(” : ”)ifpair]
precision×recall 15: endif
F 1 = 2× (1) 16: entity_pairs ← [tuple(pair.split(”;”))
precision+recall
forpair inentity_pairs]
|Real∩Generated|
precision = (2) 17: return(text_label,set(entity_pairs))
|Generated|
18: endprocedure
|Real∩Generated|
recall = (3)
|Real|