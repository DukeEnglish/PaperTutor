Q-Sparse: All Large Language Models can be
Fully Sparsely-Activated
HongyuWang∗ ShumingMa∗ RuipingWang FuruWei⋄
https://aka.ms/GeneralAI
Abstract
We introduce, Q-Sparse, a simple yet effective approach to training sparsely-
activatedlargelanguagemodels(LLMs). Q-Sparseenablesfullsparsityofacti-
vationsinLLMswhichcanbringsignificantefficiencygainsininference. This
is achieved by applying top-K sparsification to the activations and the straight-
through-estimatortothetraining. Thekeyresultsfromthisworkare,(1)Q-Sparse
can achieve results comparable to those of baseline LLMs while being much
more efficient at inference time; (2) We present an inference-optimal scaling
law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings,
includingtraining-from-scratch,continue-trainingofoff-the-shelfLLMs,andfine-
tuning;(4)Q-Sparseworksforbothfull-precisionand1-bitLLMs(e.g.,BitNet
b1.58[WMD+23]). Particularly,thesynergyofBitNetb1.58andQ-Sparse(can
beequippedwithMoE)providesthecornerstoneandaclearpathtorevolutionize
theefficiency,includingcostandenergyconsumption,offutureLLMs.
LLaMA LLM BitNet b1.58
Q-Sparse 1.58bit Q-Sparse
300M 3B 7B 300M 3B 7B
# Activated Params # Activated Params
-1 0 1 -1 0
Output 5 0 -1 1 2
-1 0 1 -1 0
1 0 1 -1 1
1 0 1 -1 1
-3 2
Index Select
Sparse Input -3 0 0 2 0 0
0
TopK Sparsification
-1 0 1 -1 0
Input
-3 1 0 2 -1 0 0 1 -1 0 1
0
-1 -1 1 0 1
Weight
1 0 1 -1 1
0 -1 1 0 0
Forward
Backward -1 1 1 0 -1
Figure1: Q-Sparseachievesasuperiorinference-optimalscalinglawthanthedensemodels. Itsaves
significantcomputeofmatrixmultiplicationbytop-K sparsificationoftheactivations.
∗Equalcontribution.⋄Correspondingauthor.S.Ma,F.WeiarewithMicrosoftResearch.H.WangandR.
WangarewithUniversityofChineseAcademyofSciences.
4202
luJ
51
]LC.sc[
1v96901.7042:viXra
ssoL ssoL1 FullySparsely-ActivatedLLMs
Largelanguagemodels(LLMs)haveachievedremarkableperformanceonawiderangeofnatural
language processing (NLP) tasks. However, the deployment of LLMs in real-world applications
is challenging due to their high computational cost and memory footprint, especially during the
inferencestage. Toaddressthischallenge,recentworks[MWM+24,WMD+23,SXZ+24,XGZC23,
LKM23] have focused on improving the efficiency of LLMs with various approaches, including
quantization[MWM+24,WMD+23,FAHA23],pruning[XGZC23],distillation[GDWH23],better
decoding[LKM23],andsoon. Onepromisingapproachistousesparsitytoreducethenumberof
activatedparametersinLLMs.
SparsitycontributestwofactorstotheefficiencyofLLMs. First,sparsitycanreducetheamountof
computationofthematrixmultiplicationaszeroelementsarenotcomputed. Second,sparsitycan
reducetheamountofinput/output(I/O)thattransferstheparametersbetweenthememoryandthe
computationunits. TheI/OtransferservesasthemajorbottleneckintheinferencestageofLLMs.
OnecommonapproachtosparsityinLLMsistouseweightsparsity,whichprunesthemodelweights
tosavethecomputation. However,unstructuredweightsparsityisdifficulttoparallelizeinGPU
devices,whilestructuredweightsparsityhasalargeimpacttotheaccuracyofthemodel.
Another approach is to use activation sparsity, which reduces the number of activated elements
in the activation tensors. Activation sparsity can be achieved by using the mixture-of-experts
(MoE)mechanism[LLX+21,FZS21],modifyingtheactivationfunction[MAM+23,SXZ+24],or
predicting the position to be sparsed [LWD+23]. However, these approaches do not enable full
sparsity of activations in LLMs, which can limit the efficiency gains during the inference stage.
Moreover,comparedtothedensemodels,thescalinglawsforthesparsely-activatedLLMshavenot
beenwellstudied.
To explore the full potential of sparsity in LLMs, we introduce Q-Sparse, a simple yet effective
approach to enable full sparsity of activations in LLMs. The major modification on LLMs is in
thelinearprojection(i.e.,matrixmultiplication). AsshowninFigure1,foreachlinearprojection,
ithasatop-Ksparsificationfunctionthatselectsthetop-Kactivationsintheinputtensor. Forthe
backprogation,weusethestraightthroughestimatortocomputethegradientsoftheactivations. We
alsointroduceasquaredReLUfunctionforthefeed-forwardlayerstofurtherimprovethesparsityof
theactivations. Q-Sparsecanbeusedwithbothfull-precisionandquantizedLLMs. Tostudythe
scalinglawofsparsely-activatedLLMs,weconductaseriesofscalingexperimentsandderivean
inference-optimalscalinglawforsparsely-activatedLLMs. Wesummarizethefindingsfromthe
scalingexperimentsandtheimplicationsofthescalinglawasbelow:
• Theperformanceofthesparsely-activatedmodelsisbetterthanthedensebaselineswiththe
sameinferencecomputebudget(i.e.,activatedparametersorFLOPs).
• AstheparametersN scales,theperformancegapbetweenthesparsely-activatedmodels
andthedensebaselinesdecreases.
• Theperformanceofthesparsely-activatedmodelswitharound40%sparsityratiocanmatch
theperformanceofthedensebaselineswiththesamemodelsizeandtrainingtokens.
• Given the same inference budget N , a sparsely-activated full-precision model with a
a
sparsityratioof45.58%(or1.84N parameters)canachievethebestperformance. Forthe
a
1.58-bitmodels,theoptimalsparsityratiois61.25%.
WealsoconductexperimentstoevaluatetheeffectivenessofQ-Sparseindifferentsettings,includ-
ingtraining-from-scratch,continue-trainingofoff-the-shelfLLMs,andfinetuning. Weshowthat
Q-SparsecanachieveresultscomparabletothoseofbaselineLLMswiththesametrainingcostwhile
beingmuchmoreefficientatinferencetime.
2 Q-Sparse
2.1 Architecture
TheQ-SparsearchitectureisbasedontheTransformerarchitecture[VSP+17,TLI+23]withmodifi-
cationstoenablesparsityintheactivations.
2Top-KSparsity
TheTransformerarchitectureusesnn.Lineartoperformtheprojectioninbothattentionandfeed-
forwardlayers,whichcanbewrittenas:
Y =X·WT (1)
whereX ∈ RN×D istheinputtensor,W ∈ RM×D istheweighttensor,andY ∈ RN×M isthe
outputtensor. Thenn.Linearoperationisequivalenttothematrixmultiplicationoperation.
Weintroduceatop-Ksparsityfunctionontopofthematrixmultiplicationoperation. Thetop-K
sparsityfunctionisdefinedas:
Y =(X⊙M)·WT (2)
M=Top (|X|) (3)
k
whereM∈RN×D isthemasktensorthatindicatesthetop-KactivationsintheinputtensorXin
termsoftheabsolutevalues,⊙istheelement-wisemultiplicationoperation,andTop isthefunction
k
thatselectsthetop-Kelementsinthetensors.
Toreducetheintervalaroundzero,were-scalethetensorbyitsL normafterperformingthetop-K
2
sparsityfunction.
QuantizedTop-KSparsity
Recentworks[WMD+23]haveshownthatquantizationcanbeusedtoreducethememoryfootprint
andcomputationalcostofLLMswithoutthelossofperformance. Weintroduceaquantizedversion
ofthetop-Ksparsityfunction. Thequantizedtop-Ksparsityfunctionisdefinedas:
Y =(Q(X)⊙M)·WT (4)
whereQ(·)isthequantizationfunctionthatquantizestheinputtensorXtoa8-bitrepresentation:
X
Q(X)=RoundClip( ,−128,127) (5)
γ+ϵ
γ =max(|X|) (6)
RoundClip(X,a,b)=min(max(round(X),a),b) (7)
whereϵisasmallconstanttoavoiddivisionbyzero,andγ isthemaximumabsolutevalueinthe
inputtensorX.
Q-Sparse can be used with both full-precision and quantized LLMs. Specifically, the quantized
versionofQ-Sparseiscompatiblewith1-bitLLMs,suchasBitNetb1.58[WMD+23]. Whenusing
Q-Sparsewith1-bitLLMs,thequantizationfunctionisperformedontheweighttensorW:
Y =(Q(X)⊙M)·Q (W)T (8)
w
whereQ (·)isthequantizationfunctionthatquantizestheweighttensorWtoa1.58-bitrepresenta-
w
tion:
W
Q (W)=RoundClip( ,−1,1) (9)
w α+ϵ
whereαisthemeanabsolutevalueintheweighttensorW:
α=mean(|W|) (10)
SquaredReLU
3Dense baseline
25 Q-Sparse (w/ STE)
Q-Sparse (w/o STE)
20
15
10
5
0
0 5 10 15 20
# Layers
Figure2: Theaveragemagnitudeofeachprojection’sgradientofdensebaseline,Q-Sparsewithand
withoutSTEacrossdifferentlayers. Thevisualizationisconductedwith300Mmodelsizeonasubset
ofthevalidsetofC4[RSR+19]. ItshowsthatthegradientvanisheswithoutSTE.
Tofurtherimprovethesparsityoftheactivations,weusethesquaredReLUfunction[SML+21]for
thefeed-forwardlayers. ThesquaredReLUfunctionisdefinedasReLU(X)2.
FollowingtheLLaMAarchitecture,weusethegatedlinearunit(GLU)forthefeed-forwardlayers.
The squared ReLU function is applied with the GLU function into a ReLU2GLU function. The
ReLU2GLUfunctionisdefinedas:
ReLU2GLU(X)=XWT ⊙ReLU2(XWT ) (11)
up gate
2.2 Training
Mostoftheexistingworks[MAM+23]ontrainingsparsely-activatedmodelsusethevanillaback-
propagationalgorithmtocomputethegradientthroughthesparsityfunction:
∂Y ∂Y
= ⊙M (12)
∂X ∂(X⊙M)
whereMisthemasktensorthatindicatesthetop-KactivationsintheinputtensorX,and⊙isthe
element-wisemultiplicationoperation.
Thevanillaback-propagationalgorithmhasalimitation.Itzero-outsthegradientsofthenon-activated
elements,whichcanleadtothevanishinggradientproblem,especiallywhenthesparsityratiois
high. Inthiswork,weproposetousethestraight-throughestimator[BLC13]toback-propagatethe
gradientsthroughthesparsityfunction. Inthisway,thegradientsarepassedthroughthesparsity
functionwithoutbeingzeroed-out. Thestraight-throughestimatorisdefinedas:
∂Y ∂Y
= (13)
∂X ∂(X⊙M)
Wevisualizetheaveragel2normofeachprojection’sgradientacrossdifferentlayersfordensemodel,
Q-SparsewithandwithoutSTE.Weadopttop-Kas50%forQ-Sparse. WithoutSTE,thegradient
ismuchsmalleratthebottomlayers,whileSTEcanpreservethemagnitudeofthegradients. As
showninFigure2,STEestimatorsignificantlyeasestheissueofgradientvanishing,especiallyatthe
bottomofthelayers. WepresentmorevisualizationsforeachcomponentsintheAppendixA.
4
mroN
tneidarG2.3 Q-SparseforContinue-TrainandFinetuningSettings
Q-Sparsecanbeusedindifferentsettings,includingtraining-from-scratch,continue-training,and
finetuning. Inthecontinue-trainandfinetuningsettings,weusethesamearchitectureandtraining
procedureasinthetraining-from-scratchsetting. Theonlydifferenceisthatweinitializethemodel
withthepre-trainedweightsandcontinuetrainingwiththesparsityfunctionenabled.
Forthepre-trainedmodelsthatdonothavethesquaredReLUfunctioninthefeed-forwardlayers,we
applythetop-Ksparsityfunctionaftertheactivatedfunction(e.g.,SiLU)inthefeed-forwardlayers.
Itcanimprovethesparsityoftheactivationswithoutchangingthemodelarchitecture.
3 ScalingLaws
RecentworkonlargelanguagemodelshasshownthattheperformanceofLLMsscaleswiththe
modelsizeandtheamountoftrainingdata. [HBM+22]arguesthattheconvergedperformanceofa
denseTransformermodelwithN parametersfollowsapower-lawscalinglaw,whichcanbewritten
as:
A
L(N)≜E+ (14)
Nα
whereL(N)istheperformanceofthemodelwithN parameters,E istheperformanceofthemodel
withinfiniteparameters, Aisaconstant, andαisthescalingexponent. Notethatthenumberof
trainingtokensarefixedinthissetting,whichispartoftheconstantE.
Inthiswork,weinvestigatethescalinglawofsparsely-activatedLLMs. Wefindthattheperformance
ofsparsely-activatedLLMsalsofollowsapower-lawscalinglaw,whichcanbewrittenas:
A(S)
L(N,S)≜E+ (15)
Nα
β
A(S)=B+Cexp( ) (16)
1−S
whereL(N,S)istheperformanceofthesparsely-activatedmodelwithN parametersandasparsity
ratioofS,andαandβ arethescalingexponents.
Inthefollowingpart,wewillintroducehowwederivethescalinglawandthecorrespondingfindings.
3.1 ScalingExperimentsandFindings
Todeterminetheformofthescalinglawofsparse-activatedLLMs,webeginwithaseriesofscaling
experiments. In the experiments, we train a series of language models with Q-Sparse of various
scales,rangingfrom300Mto7B.ThemodelsaretrainedontheRedpajamadataset[Com23]. We
usetheSentencepiecetokenizerfromLLaMAtopreprocessdata. BesidesQ-Sparse,wealsotrain
thedensebaselineswiththesamedatasetsandsettings. MoredetailscanbefoundintheAppendixB.
Theobservedlossesofthesparsely-activatedmodelsandthedensebaselinesareshowninFigure3.
Wesummarizethefindingsasbelow:
• Theperformanceofthesparsely-activatedmodelsscaleswiththemodelsizeandthesparsity
ratio.
• GivenafixedsparsityratioS,theperformanceofthesparsely-activatedmodelsfollowsa
power-lawscalinglawwithregardstothemodelsizeN.
• GivenafixedparametersN,theperformanceofthesparsely-activatedmodelsfollowsan
exponential-lawscalinglawwithregardstothesparsityratioS.
• AstheparametersN scales,theperformancegapbetweenthesparsely-activatedmodels
andthedensebaselinesdecreases.
Accordingtothesefindings,ourmainhypothesisisthattheperformanceofthesparsely-activated
modelsfollowsacombinationofapower-lawscalinglawwithregardstothemodelsizeN andan
exponential-lawscalinglawwithregardstothesparsityratioS.
53.9
Dense Baseline 3.9 300M
3.8 Top 70% Activated 700M
3.7 Top 50% Activated 3.8 1.3B
Top 40% Activated
3.6
3.7
3.5
3.6
3.4
3.3 3.5
300M1.3B 3B 7B 20% 40% 60%
Model Size (N) Sparsity Ratio (S)
Figure3: Thescalingcurvesofthesparsely-activatedmodelsregradingtothemodelsizegivena
fixedsparsityratioS (Left),andregradingtothesparsityratiogivenafixedmodelsizeN (Right).
3.2 PowerLawintheModelSizeN
WithafixedsparsityratioS,thescalinglawshouldfollows[KMH+20]’sscalinglaw,whichcanbe
writtenas:
A(S)
L(N,S)≜E+ (17)
Nα(S)
whereα(S)isthescalingexponent,andthescalingfactorA(S)isafunctionofthesparsityratioS.
GivenanymodelsizeN,thefunctionL(N,S)shouldfollowtheLipschitzcontinuitywithregards
tothesparsityratioS. Therefore,thescalingexponentα(S)shouldbeanon-decreasingfunction.
GivenanymodelsizeN,thefunctionL(N,S)isincreasingwiththesparsityratioS,soα(S)should
beanon-increasingfunction. Aboveall,thescalingexponentα(S)shouldbeaconstant,andthe
scalingfunctioncanbewrittenas:
A(S)
L(N,S)≜E+ (18)
Nα
3.3 ExponentialLawintheSparsityRatioS
According to the above finding, the performance of the sparsely-activated models follows an
exponential-law scaling law with regards to the sparsity ratio S. Therefore, the scaling factor
A(S)shouldalsofollowanexponentiallaw. Besides,givenanymodelsizeN,thescalingfunction
isincreasingwiththesparsityratioS. Therefore,thescalingfactorA(S)shouldbeanon-decreasing
function. ThescalingfactorA(S)canbewrittenas:
β
A(S)=B+Cexp( ) (19)
1−S
whereB isthescalingfactorforextremelysparseLLMs,C isthescalingfactorfordenseLLMs,
andβ isthescalingexponentofthescalingfactorA(S)withregardstothesparsityratioS.
3.4 FittingtheParameters
Wefittheparametersofthescalinglawtotheobservedlossesofthesparsely-activatedmodels. We
usetheL-BFGSalgorithm[Noc80]tominimizetheHuberloss[Hub92]betweenthepredictedand
observedlogloss.
min (cid:88) Huber (cid:16) logLˆ(N ,S )−logL (cid:17) (20)
δ i i i
E,B,C,β,α
Runsi
Following[HBM+22],δ issetas10−3. Weselectthebestfitfromagridofinitialisationsaround
possiblelocaloptimas.E,B,C,αandβareestimatedas1.86,0.01,1.89,0.10and0.05,respectively.
6
ssoL ssoL3.5
70%
10% Sparsity
60%
45% Sparsity 3.4
70% Sparsity 45.55 80 %%
3.62 3.59 3.55 3.52 3.48 3.45 3.41 3.38 3.34 3.31 3.28 3.24 3.21
40%
3.3 30%
20%3.66
3.2 10%
300M 3B 7B 300M 700M 1.3B 7B
# Activated Params (Na) # Activated Params (Na)
3.5
70%
30% Sparsity
61.25%
60% Sparsity 3.4 80% Sparsity 45 00 %% 3.723.693.66 3.62 3.59 3.55 3.52 3.48
3.45 3.41
3.38
3.34
3.31 3.28 3.24
3.3 30%3.76
20%3.79
3.2 10%
300M 3B 7B 300M 700M 1.3B 7B
# Activated Params (Na) # Activated Params (Na)
Figure4: Theinference-optimalscalingcurvesofthesparsely-activatedmodelswithfull-precision
(Top)and1.58-bit(Bottom)weight. Itshowsthatasparistyof45.58%forfull-precisionmodels
and61.25%for1.58-bitmodelscanachievethebestperformancewiththesameinferencecompute
budget(i.e.,activatedparametersorFLOPs).
3.5 DiminishingGapbetweenSparsely-ActivatedModelsandDenseBaselines
Giventheabovescalinglaw,wecanderivetheperformanceofthesparsely-activatedmodelsandthe
densebaselineswiththesamemodelsizeN andthesamesparsityratioS. Theperformancegap
betweenthesparsely-activatedmodelsandthedensebaselinesdecreasesasthemodelsizeN scales.
Theperformancegapcanbewrittenas:
A(S) A(0)
L(N,S)−L(N,0)= − (21)
Nα(S) Nα(0)
A(0) A(S)
= ( −1) (22)
Nα A(0)
Sinceαisaconstantthatsatisfiesα>0,theperformancegapdecreasesasthemodelsizeN scales.
ItmeansthatgivenalargeenoughmodelsizeN,theperformanceofthesparsely-activatedmodels
caneventuallymatchtheperformanceofthedensebaselineswiththesamemodelsize.
3.6 Inference-OptimalScalingLaw
ThescalinglawcanalsobetransformedintoaformthatisdependentontheactivatedparametersN ,
a
whichreflectstheeffectivecompute(i.e.,FLOPs)ofthemodelduringinference:
1−S
L(N ,S)≜E+A(S)( )α (23)
a N
a
whereN isthenumberofactivatedparametersinthemodel,whichisequaltoN ×(1−S). Since
a
A(S)isanincreasingfunctionand(1−S)α isadecreasingfunction,thereexistsasparsityratio
S∗ >0thatminimizesthelossofthesparsely-activatedmodels. Thisleadstotheinference-optimal
scalinglawofthesparsely-activatedmodels:
1−S∗
L(N )≜E+A(S∗)( )α (24)
a N
a
It shows that the performance of the sparsely-activated models is better than the dense baselines
withthesameinferencecomputebudget. WefurthersolvetheoptimalsparsityratioS∗,findingthat
7
ssoL
ssoL
oitaR
ytisrapS
oitaR
ytisrapS4.50
4.4 Dense Baseline Dense Baseline
Q-Sparse 4.25 Q-Sparse
4.2
4.00
4.0
3.75
3.8
3.50
3.6
3.25
3.4 3.00
3.2 2.75
3.0 2.50
0 10B 20B 30B 40B 50B 0 10B 20B 30B 40B 50B
#Tokens #Tokens
(a)700Mmodelsize (b)7Bmodelsize
Figure5: ThetraininglosscurveofQ-Sparseandthebaselinewithfull-precision. Weadopttop-K
as70%forQ-Sparse,resultingin40%overallsparsity.
4.50
4.4 BitNet b1.58 BitNet b1.58
1.58bit Q-Sparse 4.25 1.58bit Q-Sparse
4.2
4.00
4.0
3.75
3.8
3.50
3.6
3.25
3.4 3.00
3.2 2.75
3.0 2.50
0 10B 20B 30B 40B 50B 0 10B 20B 30B 40B 50B
#Tokens #Tokens
(a)700Mmodelsize (b)7Bmodelsize
Figure6: ThetraininglosscurveofQ-Sparseandthebaselinewith1.58-bitweight. Weadopttop-K
as70%forQ-Sparse,resultingin40%overallsparsity.
S∗ ≈45.58%. Itmeansthatasparsely-activatedmodelwithasparsityratioof45.58%(or1.84N
a
parameters)canachievethebestperformancewiththesameinferencebudgetN . Wefollowthe
a
sameprocesstoestimatetheinference-optimalscalinglawfor1.58-bitQ-Sparsemodels.Wefindthat
theoptimalsparsityratiois61.25%(or2.58N parameters). Figure4showstheinference-optimal
a
scalingcurvesofthesparsely-activatedmodelswithfull-precisionand1.58-bitweight. Itshowsthat
withthesameperformance,thesparsely-activatedmodelscanachieveasignificantreductioninthe
numberofactivatedparametersorFLOPsduringinference.
The inference-optimal scaling law shows that the performance of the sparsely-activated models
can be optimized by adjusting the sparsity ratio S. It can be used to guide the training of the
sparsely-activatedmodelsandtooptimizetheperformanceofthemodelsduringinference.
4 Experiments
WeconductexperimentstoevaluatetheeffectivenessofQ-Sparseindifferentsettings, including
training-from-scratch,continue-trainingofoff-the-shelfLLMs,andfinetuning.
8
ssoL
ssoL
ssoL
ssoL20 65%
TopK (w/ STE) TopK (w/ STE)
TopK (w/o STE) TopK (w/o STE)
18 ReLU ReLU
60%
16
55%
14
50%
12
10 45%
0 10B 20B 30B 40B 50B 0 10B 20B 30B 40B 50B
#Tokens #Tokens
Figure7: Thetraininglosscurves(Left)andtheoverallsparsityratio(Right)ofdifferentsparsity
functions. Allmodelsaretrainedwith300Msizeand50Btokens.
4.1 Training-from-Scratch
Setting WetrainaseriesoflanguagemodelswithQ-Sparseinbothfull-precisionand1.58bits.
Themodelsaretrainedwith50BtokensontheRedpajamadataset[Com23]. WecompareQ-Sparse
withthedensebaselineswiththesamedatasetsandsettings.
Results Theobservedlossesofthesparsely-activatedmodelsandthedensebaselinesareshownin
Figure5. ItshowsthatQ-Sparsewith40%sparsityratiocanmatchtheperformanceofthedense
baselineswiththesamemodelsizeandtrainingtokens.
BitNetb1.58+Q-Sparse WefurtherevaluatetheeffectivenessofQ-Sparseon1-bitLLMs. We
train a series of BitNet b1.58 models with Q-Sparse of various scales. We plot the training loss
curvesofbothQ-SparseandtheBitNetb1.58baseline. Figure6showsthattheperformanceofthe
sparsely-activatedBitNetb1.58modelsisbetterthanthedensebaselineswiththesameinference
computebudget. ItdemonstratesthatQ-Sparseiscompatibleto1-bitLLMsandtheirsynergycanbe
usedtooptimizetheperformanceofthemodelsduringinference.
AblationStudyoftop-KSparistyandSTE Toevaluatetheeffectofthetop-Ksparsityfunction,
wecomparetheperformanceofthesparsely-activatedmodelswiththetop-Ksparsityfunctionand
theReLUsparsityfunction. Moreover,westudytheeffectoftheSTEbycomparingthemodelswith
andwithoutSTE.Figure7illustratestheresults. ItshowsthateitherremovingSTEorreplacing
withReLUfunctionsignificantlyhurttheperformance. Besides, thesparsityratioofthemodels
withtheReLUfunctiondecreasesasthetrainingprocesses. Inconstrast,thesparistyratioremains
unchangedwiththetop-Ksparistyfunction. AsshowninFigure8,webreakdownthecontribution
ofthesparsityratiofromdifferentcomponents,findingthatthedecreasingsparistyismainlyfromthe
QKVprojection,thegatingprojectionandtheupprojectionofthefeed-forwardlayers. Thisproves
thesuperioroftop-KoverReLUfunction.
4.2 Continue-Training
Setting Wecontinue-traintheMistral7Bmodel[BBC+23]for40BtokensontheFineWeb-Edu
dataset[LBAvWW24]. WeusetheSentencepiecetokenizerfromMistraltopreprocessdata. Weuse
thebatchsizeof4Mtokensandthelearningrateof5e-5. WeusetheAdamoptimizerwiththeweight
decayof0.01. MoretrainingdetailscanbefoundinAppendixB.
Results Forafaircomparison,wecontinue-traintheMistral7Bmodelwiththesamerecipeasthe
densebaseline. WecompareQ-SparsewiththeReLUfication[MAM+23]anddReLUSparsifica-
tion[SXZ+24]methods,whichsparsifythemodelbychangingtheactivationfunction. Following
theoriginpaper[MAM+23],weadoptatwo-stagetrainingstrategythatfirstreplacesthenon-ReLU
activationandthenaddstheReLUfunctions. ForthedReLUSparsificationmethod,weimplement
9
LPP
oitaR
ytisrapS100%
50%
90%
45% 80%
70%
40% TopK (w/ STE) - Output
TopK (w/ STE) - QKV TopK (w/ STE) - Down
TopK (w/ STE) - Up/Gate 60% ReLU - Output
ReLU - QKV ReLU - Down
35%
ReLU - Up/Gate 50%
0 10B 20B 30B 40B 50B 0 10B 20B 30B 40B 50B
#Tokens #Tokens
Figure8: Thesparsityratioofeachmodel’scomponentofdifferentsparsityfunctions.
Models Activated ARC HS MMLU WG TQA Avg.
DenseBaseline 7.0B 61.8 81.4 59.8 77.5 42.7 64.6
ReLUfication[MAM+23] 5.0B 57.2 78.8 54.7 74.7 38.8 60.8
dReLUSparsification[SXZ+24] 5.4B 59.2 78.0 54.0 75.8 38.3 61.0
2.9B 59.0 79.0 55.6 74.0 41.0 61.7
Q-Sparse(thiswork)
3.8B 60.5 80.7 58.0 75.9 43.5 63.7
Table1: Theresultsofthecontinue-trainingforQ-Sparseandthebaselinesontheendtasks.
thedReLUsparsificationmethodfollowingtheoriginpaper[SXZ+24]. Weevaluatethesemodels
on a range of language tasks, including ARC-Challenge [YBS19], HellaSwag [ZHB+19], Wino-
grande [SBBC20], MMLU [HBB+21] and TruthfulQA [LHE22]. Results are shown in Table 1.
ItshowsthatQ-Sparseachievescomparableperformancetothedensebaselinewhilebeingmuch
more efficient at inference time. Moreover, Q-Sparse outperforms the ReLUfication and dReLU
Sparsificationmethodsintermsoftheperformanceandthesparsityratio.
Tobreakdownthesparsityofeachcomponentinthemodel,wepresentthesparsityratioofthequery,
key,value,output,up,down,andgatetensorsinTable2. ItshowsthatQ-Sparseachievesahigher
sparsityratiothantheReLUficationanddReLUSparsificationmethods. Thesparsityratioofthe
query,key,value,output,up,anddowntensorsishigherthan40%,andthesparsityratioofthegate
tensorishigherthan60%. ItdemonstratesthatQ-Sparsecanachievefullsparsityofactivationsin
LLMs.
4.3 SupervisedFinetuning
Setting We finetune the base model of Mistral 7B [JSM+23] and Qwen1.5 7B [BBC+23] on
Open-Orcadataset[LGP+23]forboththedensebaselinesandQ-Sparse. Thebatchsizeissetas128.
Thelearningratesareselectedfrom{3e-6,5e-6,7e-6}. Allmodelsaretrainedwith1epochforafair
comparison. Thehyper-parametersaredetailedinAppendixB.Weconducttheevaluationforthese
modelsonarangeoflanguagetasks,includingARC-Challenge[YBS19],HellaSwag[ZHB+19],
Winogrande[SBBC20],MMLU[HBB+21]andTruthfulQA[LHE22].
Results TheresultsareshowninTable3. ItshowsthatQ-Sparsewith3.6Bactivatedparameters
achievessignificantbetterperformancethantheQwen1.54Bdensemodel. Moreover,Q-Sparsewith
around4BactivatedparametersachievescomparableperformancetotheMistral7Bmodelandthe
Qwen1.57Bmodel. ItdemonstratesthatQ-Sparsecanbeusedtofinetuneadensepretrainedmodel
toamuchmoreefficientsparsemodelwithalmostnolossataccuracy.
10
oitaR
ytisrapS
oitaR
ytisrapSModels Activated QKV Out Up Gate Down Overall
DenseBaseline 7.0B 0.0 0.0 0.0 0.0 0.0 0.0
ReLUfication[MAM+23] 5.0B 12.3 0.0 10.3 10.3 79.3 28.3
dReLUSparsification[SXZ+24] 5.4B 0.1 0.0 0.1 0.1 85.5 23.0
2.9B 51.4 50.0 50.0 50.0 80.0 58.2
Q-Sparse(thiswork)
3.8B 42.0 40.0 40.0 40.0 60.4 45.7
Table2: Theactivatedparametersandthesparsityratioofthecontinue-trainingforQ-Sparseandthe
baselinesonthetestsetofWikitext2.
Models Activated ARC HS MMLU WG TQA Avg.
Qwen1.5-4B 3.2B 42.8 68.2 53.6 67.1 47.9 55.9
Qwen1.5-7B 6.5B 47.7 74.6 61.5 71.4 50.7 61.2
3.6B 46.3 72.6 59.1 67.5 50.3 59.2
Q-Sparse
4.1B 47.9 73.2 59.2 69.4 51.1 60.1
Mistral-7B 7.0B 62.5 82.6 61.2 77.6 50.3 66.8
3.8B 60.5 81.5 60.0 77.1 50.5 65.9
Q-Sparse
4.3B 61.4 81.6 60.6 77.6 50.7 66.4
Table3: Theresultsofthesupervisedfine-tuningforQ-Sparseandthedensebaselinesontheend
tasks.
5 DiscussionandFutureWork
ScalingBitNetb1.58+Q-Sparse+YOCO
We have shown promising results of combining 1-bit LLMs (i.e., BitNet b1.58) and fully sparse
activations(i.e.,Q-Sparse). Weareworkingonscalingupthetrainingintermsofbothmodelsize
andtrainingtokens. Furthermore,wewillincorporateYOCO[SDZ+24]toaddresstheissueofKV
cacheforLLMinference. TheintegrationofBitNet,Q-Sparse,andYOCOprovidesacomprehensive
approachtooptimizingalldatatypesinLLMinferenceanddeployment,whichincludessystematic
optimizationofmodelweights,activations,andKVcache.
Q-Sparse+MoE
Mixture-of-ExpertshasbeenthemostwidelymethodtoachievesparseactivationsinLLMs.Q-Sparse
isorthogonalandcanbeseamlesslyintegratedwithMoE.
Q-SparseinBatchMode
ThecurrentQ-Sparseimplementationisnotfriendlytobatchtrainingandinference. Weareworking
onmakingQ-Sparsecompatiblewithbatchmodewithinnovationsfrombothmodelingandsystem
implementation.
References
[BBC+23] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,
WenbinGe,YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,Runji
Lin, DayihengLiu, GaoLiu, ChengqiangLu, KemingLu, JianxinMa, RuiMen,
XingzhangRen,XuanchengRen,ChuanqiTan,SinanTan,JianhongTu,PengWang,
ShijieWang,WeiWang,ShengguangWu,BenfengXu,JinXu,AnYang,HaoYang,
JianYang,ShushengYang,YangYao,BowenYu,HongyiYuan,ZhengYuan,Jianwei
Zhang,XingxuanZhang,YichangZhang,ZhenruZhang,ChangZhou,JingrenZhou,
XiaohuanZhou,andTianhangZhu. Qwentechnicalreport. CoRR,abs/2309.16609,
2023.
11[BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or prop-
agatinggradientsthroughstochasticneuronsforconditionalcomputation. CoRR,
abs/1308.3432,2013.
[Com23] TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,
2023.
[FAHA23] EliasFrantar,SalehAshkboos,TorstenHoefler,andDanAlistarh. OPTQ:accurate
quantizationforgenerativepre-trainedtransformers. InTheEleventhInternational
ConferenceonLearningRepresentations,2023.
[FZS21] WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scalingto
trillionparametermodelswithsimpleandefficientsparsity. CoRR,abs/2101.03961,
2021.
[GDWH23] YuxianGu,LiDong,FuruWei,andMinlieHuang. Knowledgedistillationoflarge
languagemodels. arXivpreprintarXiv:2306.08543,2023.
[HBB+21] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,Dawn
Song,andJacobSteinhardt. Measuringmassivemultitasklanguageunderstanding.
In9thInternationalConferenceonLearningRepresentations,ICLR2021,Virtual
Event,Austria,May3-7,2021.OpenReview.net,2021.
[HBM+22] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,Trevor
Cai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,
AidanClark,TomHennigan,EricNoland,KatieMillican,GeorgevandenDriessche,
BogdanDamoc,AureliaGuy,SimonOsindero,KarenSimonyan,ErichElsen,JackW.
Rae, OriolVinyals, andLaurentSifre. Trainingcompute-optimallargelanguage
models. CoRR,abs/2203.15556,2022.
[Hub92] Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in
statistics: Methodologyanddistribution,pages492–518.Springer,1992.
[JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-
vendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,
GuillaumeLample, LucileSaulnier, LélioRenardLavaud, Marie-AnneLachaux,
PierreStock,TevenLeScao,ThibautLavril,ThomasWang,TimothéeLacroix,and
WilliamElSayed. Mistral7b. CoRR,abs/2310.06825,2023.
[KMH+20] JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,
RewonChild,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scaling
lawsforneurallanguagemodels. CoRR,abs/2001.08361,2020.
[LBAvWW24] AntonLozhkov,LoubnaBenAllal,LeandrovonWerra,andThomasWolf. Fineweb-
edu,May2024.
[LGP+23] WingLian,BleysGoodson,EugenePentland,AustinCook,ChanvichetVong,and
"Teknium". Openorca: An open dataset of gpt augmented flan reasoning traces.
https://https://huggingface.co/Open-Orca/OpenOrca,2023.
[LHE22] StephanieLin,JacobHilton,andOwainEvans. Truthfulqa: Measuringhowmod-
els mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin,Ireland,May22-27,2022,pages3214–3252.AssociationforComputational
Linguistics,2022.
[LKM23] YanivLeviathan,MatanKalman,andYossiMatias. Fastinferencefromtransformers
viaspeculativedecoding. InInternationalConferenceonMachineLearning,ICML
2023,23-29July2023,Honolulu,Hawaii,USA,2023.
[LLX+21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,
YanpingHuang,MaximKrikun,NoamShazeer,andZhifengChen. Gshard: Scaling
giantmodelswithconditionalcomputationandautomaticsharding. InICLR2021,
2021.
12[LWD+23] ZichangLiu,JueWang,TriDao,TianyiZhou,BinhangYuan,ZhaoSong,Anshumali
Shrivastava,CeZhang,YuandongTian,ChristopherRé,andBeidiChen. Dejavu:
Contextualsparsityforefficientllmsatinferencetime. InAndreasKrause,Emma
Brunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett,
editors, International Conference on Machine Learning, ICML 2023, 23-29 July
2023, Honolulu, Hawaii, USA,volume202ofProceedingsofMachineLearning
Research,pages22137–22176.PMLR,2023.
[MAM+23] ImanMirzadeh,KeivanAlizadeh,SachinMehta,CarloC.DelMundo,OncelTuzel,
GolnooshSamei,MohammadRastegari,andMehrdadFarajtabar. Relustrikesback:
Exploiting activation sparsity in large language models. CoRR, abs/2310.04564,
2023.
[MWM+24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan
Huang,LiDong,RuipingWang,JilongXue,andFuruWei. Theeraof1-bitllms:
Alllargelanguagemodelsarein1.58bits. CoRR,abs/2402.17764,2024.
[Noc80] JorgeNocedal. Updatingquasi-newtonmatriceswithlimitedstorage. Mathematics
ofcomputation,35(151):773–782,1980.
[RSR+19] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer
learningwithaunifiedtext-to-texttransformer. CoRR,abs/1910.10683,2019.
[SBBC20] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Wino-
Grande: anadversarialwinogradschemachallengeatscale. InTheThirty-Fourth
AAAIConferenceonArtificialIntelligence,pages8732–8740,2020.
[SDZ+24] YutaoSun,LiDong,YiZhu,ShaohanHuang,WenhuiWang,ShumingMa,Quanlu
Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder
architecturesforlanguagemodels. CoRR,abs/2405.05254,2024.
[SML+21] David R. So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and
QuocV.Le. Primer: Searchingforefficienttransformersforlanguagemodeling.
CoRR,abs/2109.08668,2021.
[SXZ+24] YixinSong,HaotongXie,ZhengyanZhang,BoWen,LiMa,ZeyuMi,andHaibo
Chen. Turbosparse: Achievingllmsotaperformancewithminimalactivatedparam-
eters. arXivpreprintarXiv:2406.05955,2024.
[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample.
LLaMA:openandefficientfoundationlanguagemodels. CoRR,abs/2302.13971,
2023.
[VSP+17] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.
Gomez,LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvances
inNeuralInformationProcessingSystems30: AnnualConferenceonNeuralInfor-
mationProcessingSystems2017,December4-9,2017,LongBeach,CA,USA,pages
5998–6008,2017.
[WMD+23] HongyuWang,ShumingMa,LiDong,ShaohanHuang,HuaijieWang,LingxiaoMa,
FanYang,RuipingWang,YiWu,andFuruWei. Bitnet: Scaling1-bittransformers
forlargelanguagemodels. CoRR,abs/2310.11453,2023.
[XGZC23] MengzhouXia,TianyuGao,ZhiyuanZeng,andDanqiChen. Shearedllama: Accel-
eratinglanguagemodelpre-trainingviastructuredpruning. CoRR,abs/2310.06694,
2023.
[YBS19] VikasYadav,StevenBethard,andMihaiSurdeanu. Quickand(notso)dirty: Un-
supervisedselectionofjustificationsentencesformulti-hopquestionanswering. In
KentaroInui,JingJiang,VincentNg,andXiaojunWan,editors,EMNLP-IJCNLP,
2019.
13[ZHB+19] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi.HellaSwag:
canamachinereallyfinishyoursentence? InProceedingsofthe57thConferenceof
theAssociationforComputationalLinguistics,pages4791–4800,2019.
A Visualizations
4.0
4.0 Dense baseline Dense baseline
3.5 Q-Sparse (w/ STE) 3.5 Q-Sparse (w/ STE)
Q-Sparse (w/o STE) Q-Sparse (w/o STE)
3.0 3.0
2.5 2.5
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0 5 10 15 20 0.0 0 5 10 15 20
# Layers # Layers
(a)Queryprojection (b)Keyprojection
50 Dense baseline Dense baseline
Q-Sparse (w/ STE) Q-Sparse (w/ STE)
Q-Sparse (w/o STE) 40 Q-Sparse (w/o STE)
40
30
30
20 20
10 10
0 0 5 10 15 20 0 0 5 10 15 20
# Layers # Layers
(c)Valueprojection (d)Outputprojection
30 D Qe -Sn ps ae r sb ea s (e wl /in Se TE) 25 D Qe -Sn ps ae r sb ea s (e wl /in Se TE) 30 D Qe -Sn ps ae r sb ea s (e wl /in Se TE)
25 Q-Sparse (w/o STE) 20 Q-Sparse (w/o STE) 25 Q-Sparse (w/o STE) 20 20
15 15 15
10 10 10
5 5 5
0 0 5 10 15 20 0 0 5 10 15 20 0 0 5 10 15 20
# Layers # Layers # Layers
(e)Gateprojection (f)Upprojection (g)Downprojection
Figure9: Thegradientmagnitudeofeachlinearprojectionofdensebaseline,Q-Sparsewithand
withoutSTEestimatoracrossdifferentlayers.
B Hyperparameters
Size HiddenSize GLUSize #Heads #Layers SeqLength
300M 1024 2730 16 24 2048
700M 1536 4096 24 24 2048
1.3B 2048 5460 32 24 2048
7B 4096 11008 32 32 2048
Table4: ModelconfigurationsforthescalingexperimentsofbothBitNetb1.58andLLaMALLM
withQ-Sparse.
14
mroN
tneidarG
mroN
tneidarG
mroN
tneidarG
mroN
tneidarG
mroN
tneidarG
mroN
tneidarG
mroN
tneidarGModel Size LearningRate WeightDecay BatchSize Adamβ
300M 1.8×10−3 →1.5×10−3 0.1→0 0.5M (0.9,0.95)
700M 1.5×10−3 →1×10−3 0.1→0 0.5M (0.9,0.95)
BitNetb1.58
1.3B 1.2×10−3 →8×10−4 0.1→0 0.5M (0.9,0.95)
7B 1×10−3 →6×10−4 0.1→0 0.5M (0.9,0.95)
300M 6.0×10−4 0.1 0.5M (0.9,0.95)
700M 2.5×10−4 0.1 0.5M (0.9,0.95)
LLaMALLM
1.3B 2.0×10−4 0.1 0.5M (0.9,0.95)
7B 1.5×10−4 0.1 0.5M (0.9,0.95)
Table5: Hyper-parametersforthescalingexperimentsofbothBitNetb1.58andLLaMALLMwith
Q-Sparse.
Hyperparameters Value
Trainingupdates 10K
Tokenspersample 4M
Adamβ (0.9,0.95)
Learningrate 5e-5
Endlearningrate 1e-6
Learningrateschedule Polynomialdecay
Warmupupdates 375
Gradientclipping 2.0
Dropout ✗
Attentiondropout ✗
Weightdecay 0.01
Table6: Hyper-parametersforthecontinue-trainingofMistral7BwithQ-SparseonFindwebEdu
dataset.
Hyperparameters Value
Trainingepoch 1
BatchSize 128
Adamβ (0.9,0.95)
Learningrate {3e-6,5e-6,7e-6}
Learningrateschedule Cosinedecay
Warmupratio 0.03
Dropout ✗
Attentiondropout ✗
Weightdecay ✗
Table 7: Hyper-parameters for the supervised fine-tuning of Mistral 7B and Qwen-1.5 7B with
Q-SparseonOpenOrcadataset.
15