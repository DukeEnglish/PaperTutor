UnderreviewasaconferencepaperatICLR2024
ADAPTABLE: TEST-TIME ADAPTATION FOR TABULAR
DATA VIA SHIFT-AWARE UNCERTAINTY CALIBRATOR
AND LABEL DISTRIBUTION HANDLER
ChanghunKim1,2∗,TaewonKim1∗,SeungyeonWoo1,JuneYongYang1,EunhoYang1,2
1KoreaAdvancedInstituteofScienceandTechnology(KAIST),2AITRICS
{changhun.kim, maxkim139, oox1987, laoconeth, eunhoy}@kaist.ac.kr
ABSTRACT
Inreal-worldapplications,tabulardataoftensufferfromdistributionshiftsdue
totheirwidespreadandabundantnature,leadingtoerroneouspredictionsofpre-
trainedmachinelearningmodels.However,addressingsuchdistributionshiftsin
thetabulardomainhasbeenrelativelyunderexploredduetouniquechallengessuch
asvaryingattributesanddatasetsizes,aswellasthelimitedrepresentationlearning
capabilitiesofdeeplearningmodelsfortabulardata.Particularly,withtherecent
promisingparadigmoftest-timeadaptation(TTA),whereweadapttheoff-the-shelf
modeltotheunlabeledtargetdomainduringtheinferencephasewithoutaccessing
thesourcedomain,weobservethatdirectlyadoptingcommonlyusedTTAmeth-
odsfromotherdomainsoftenleadstomodelcollapse.Wesystematicallyexplore
challengesintabulardatatest-timeadaptation,includingskewedentropy,complex
latentspacedecisionboundaries,confidencecalibrationissueswithbothoverconfi-
dentandunder-confident,andmodelbiastowardssourcelabeldistributionsalong
withclassimbalances.Basedontheseinsights,weintroduceAdapTable,anovel
tabulartest-timeadaptationmethodthatdirectlymodifiesoutputprobabilitiesby
estimating target label distributions and adjusting initial probabilities based on
calibrateduncertainty.Extensiveexperimentsonbothnaturaldistributionshiftsand
syntheticcorruptionsdemonstratetheadaptationefficacyoftheproposedmethod.
1 INTRODUCTION
Tabulardataisoneofthemostabundantformsofdataintheindustrialworld,coveringdiversefields
suchashealthcare(Johnsonetal.,2016;2021),finance(Shahetal.,2022),andmanufacturing(Hein
et al., 2017) to name a few. However, due to their ubiquity and vast quantity, tabular data in the
wildfrequentlyexhibitsdistributionshifts(Malininetal.,2021;Üstevetal.,2013).Forinstance,
electronichealthrecord(EHR)datacollectedfromasenior-majoritycohortmaydifferindistribution
comparedtoajunior-majoritycohort.Undersuchdatadistributions,machinelearningmodelstrained
ondatafromtheseniorcohortmaynotfunctionasintendedondatafromthejuniorcohortattesttime.
Suchdistributionshiftsposeasignificantproblemindeployinglearnedmodelsastheyundermine
theirintegrityattesttime.
Despitetheimportanceofaddressingsuchprevalentdistributionshiftsintabulardata,therehave
beenlimitedstudiesondomainadaptationfortabulardatacomparedtootherdomainslikecomputer
vision(Zhuetal.,2023;Wangetal.,2021a),naturallanguageprocessing(Farajianetal.,2017;Dou
etal.,2019)andspeechprocessing(Mai&Carson-Berndsen,2022;Kimetal.,2023).Thedifficulties
ofdevelopingdomainadaptationstrategiesforthetabularrealmstemfromtwoprimaryreasons.First,
tabulardatadisplaysuniqueandheterogeneousattributes,includingbothnumericalandcategorical
featuresacrosscolumns,wheretheseattributesexhibitsubstantialvariations.Second,deeptabular
learningmodelsoftenexhibitlimitationsintheirrepresentationlearningschemesintabulardata,
whichimposeconstraintsondomainadaptationmethodsthatrelyonutilizingthefeaturespaceof
themodel.Thesefactorsareintricatelyintertwined,makingtabulardomainadaptationexceptionally
challenging.
* Equalcontribution.
1
4202
luJ
51
]GL.sc[
1v48701.7042:viXraUnderreviewasaconferencepaperatICLR2024
Figure1:Comparisonofentropydistributionhistogramsfortabular(a)andimage(b)datasetsusing
thesameMLPmodelarchitecture,alongwithagradientnormvs.entropyplotforthetabulardataset
(c).
Inparallel,test-timeadaptation(TTA)(Wangetal.,2021a;Liuetal.,2021;Niuetal.,2023;Sunetal.,
2020;Limetal.,2023;Mirzaetal.,2023;Boudiafetal.,2022;Zhouetal.,2023;Parketal.,2023)
hasbeenrecentlyproposedasanemergingparadigmindomainadaptation–aimingtoadaptthe
off-the-shelfmodeltoanunlabeledtargetdomainduringinferencephasewithoutaccessingthesource
domain.TTAmethodsareappealingduetotheirsuitabilityinscenarioswhereaccessingsourcedata
isimpractical,eitherduetoprivacyorstoragelimitations,orincaseswheretheprecisetargetdomain
duringtestingisunknown.Inthecontextoftabulardata,wherethereisagrowingdemandforTTA,
weobservethatnaivelyapplyingexistingapproachesdesignedforotherdomainstendstodegrade
theperformanceofthesourcemodelinmostscenarios.Inparticular,wesystematicallyexaminethe
failureofentropyminimization-basedfullytest-timeadaptationmethods,whichareattheforefront
of the TTA paradigm for other domains, applied to tabular data. We discover that the prediction
entropyofthemodelconsistentlyexhibitsastrongbiastowardtheunder-confidentregiondueto
theircomplexdecisionboundarieswithinthelatentspace,whichareknowntotriggerlargegradients
withinthecontextoftheentropyminimizationobjective,leadingtomodelcollapse.Inaddition,we
findthatthepredictionsofthemodelsarehighlybiasedtothesourcelabeldistributionasinother
domains (Berthelot et al., 2020; Wu et al., 2021; Park et al., 2023). This bias also contributes to
theshiftofthemodeltowardsthemajorclassofthesourcedomain,ultimatelyleadingtomodel
instability.
Motivated by these observations, we propose an Adaptation for Table (AdapTable) framework,
whichisthefirsttest-timeadaptationstrategytailoredforthetabulardomain.Confirmingthatusing
unsupervisedobjectivesduringtheinferencephasetoadaptthemodelishighlyunstableforatabular
domain,AdapTablecircumventstheneedtotunethemodelparameterusingunsupervisedobjectives
andfocusesinsteadoncorrectingtheoutputdistribution.Withthefindingsthatthemodel’sprediction
probabilitiesarepoorlycalibratedandbiasedtotheunder-confidentarea,weproposeashift-aware
post-hocuncertaintycalibrator,whichpredictsper-sampletemperaturescalingfactortocalibrate
predictions, in consideration of shift information of columns and their relationships using graph
neural networks. Armed with the hypothesis that the source model is highly biased towards the
sourcelabeldistribution,wesuggestalabeldistributionhandler,whereweestimatethetargetlabel
distributionofthecurrentbatch,andcorrectthetargetlabeldistributiongroundedinBayes’theorem.
Here,weinterpolatetheoriginallabeldistributionandtheestimatedtargetlabeldistributionbasedon
themargincalculatedbythecalibrator.Throughextensiveexperiments,weverifythatourmethod
achieves state-of-the-art performance across various datasets including natural distribution shift
scenarios,andsyntheticcorruptionswiththreerepresentativedeeptabularlearningarchitectures.
Tosummarize,ourcontributionisthreefold:
• Wethoroughlyanalyzethefailureofexistingentropyminimization-basedtest-timeadapta-
tionstrategiesontabulardataintermsofthemodel’sentropydistribution,labeldistribution,
confidencecalibration,anddecisionboundaryinthelatentspace.
• Withtheseobservationsinmind,weproposeatest-timeadaptationmethodthatdirectly
adjustsoutputpredictionprobabilitiesutilizingshift-awarepost-hocuncertaintycalibration
and label distribution handler on the tabular domain for the first time, to the best of our
knowledge.
• Throughoutextensiveexperiments,weverifythattheproposedmethodachievesstate-of-
the-artresultsondifferentmodelarchitecturesandout-of-distributionbenchmarksincluding
naturaldistributionshiftsandsyntheticcorruptions.
2UnderreviewasaconferencepaperatICLR2024
Figure 2: Comparison of reliability diagrams for both overconfident (a) and under-confident (b)
predictionprobabilitiesinthesametabulardomain,alongwiththecomparisonofthelatentspacesof
tabular(c)andimagedata(d)usingt-SNEwithinthesameMLPmodelarchitecture,wheredifferent
colorsindicatedifferentclasses.
2 DESIGN PRINCIPLES ON TEST-TIME ADAPTATION FOR TABULAR DATA
Inthissection,toestablishthedesignprinciplesofTTAontabulardata,weconductacomprehensive
analysisoftheshortcomingsobservedinexistingtest-timeadaptationstrategies,includingentropy
minimization-basedapproaches,asoutlinedinSection2.1.Furthermore,wediscusstheissueof
labeldistributionshiftbetweensourceandtargetdomains,alongwithaddressingtheclassimbalance
probleminSection2.2.
2.1 FAILUREOFEXISTINGTEST-TIMEADAPTATIONMETHODSONTABULARDATA
Domainadaptationreferstomethodsusedinadaptingmachinelearningmodelstoreal-worlddata
that may differ in distribution from their training data. There are two main branches: traditional
supervised/unsupervised domain adaptation (Ben-David et al., 2006; Sun et al., 2017; Ganin &
Lempitsky, 2015; Khurana et al., 2021), which often require both source and target data during
training,andtest-timeadaptation(Sunetal.,2020;Gandelsmanetal.,2022;Boudiafetal.,2022;Niu
etal.,2023;Zhouetal.,2023;Parketal.,2023),anovelapproachthatadaptsthemodelusingonly
unlabeledtargetdataduringtheinferencephase.Indeed,developingtest-timeadaptationstrategiesfor
tabulardataisparticularlypromisingduetotheirapplicability;makingoff-the-shelfmodelsadaptable
duringdeployment,whichmaysufferfromdistributionshiftsduringtesting,allthewhileevading
privacyconcernsfromviewingsourcedatainhindsight,acrucialissueinfieldssuchashealthcare
andfinance.
However,weobservethatdirectapplicationentropyminimizationmethodsandtheirvariations,most
abundant forms of fully test-time adaptation in other modalities like image (Wang et al., 2021a)
andspeech(Kimetal.,2023),failtoshowtheirefficacyinthetabulardomain.Wefindanumber
ofgroundsforthis.First,wedemonstratethatthedistributionsofpredictionentropyconsistently
exhibitastrongbiastowardsthehighregionacrossvarioustabulardatasetsandmodelarchitectures,
includingMLP(Murtagh,1991),TabNet(Arik&Pfister,2021),andFT-Transformer(Gorishniyetal.,
2021).AsshowninFigure1(a)and(b),suchphenomenonisuniquetorealtabulardatasetssuchas
CMC(Bischletal.,2021),notlinearizedimageslikeOptdigits(Alpaydin&Kaynak,1998).This
alignswiththefindingsinthevisiondomainNiuetal.(2022;2023),wheresampleswithhighentropy
generatesignificantgradientsleadingtomodelcollapse.TheseworksNiuetal.(2022;2023)bypass
thisbyfilteringsampleswithhighentropies.Nevertheless,predictionsfromdeeplearningmodelsin
thetabulardomaintendtoexhibitanunder-confidenttendency,whereapplyingthistechniquetothe
tabulardomainresultsthatdiscardingsampleswithhighentropyleavingveryfewsamplesformodel
update(Figure1(c)).Intermsofprobabilitycalibration,incontrasttothegenerallyoverconfident
natureofcomputervisionmodels,deeplearningmodelstrainedontabulardatashowoverallpoor
calibration.However,it’sworthnotingthatthetendencytowardsunder-confidenceoroverconfidence
variedacrossdatasets(Figure2(a)and(b)).
Wealsoconductacomparisonbetweenthelatentspaceofmodelstrainedontabulardataandthose
trainedonlinearizedimages,asshowninFigure2(c)and(d).Throughthisanalyticalexploration,
itisdiscernedthatthedecisionboundarywithinthelatentspaceoftabulardataismarkedlymore
complex;attributabletotheabsenceofclearclassseparationwithinthelatentspace.Thisobservation
providesanotherindicationofthelimitationsofentropyminimization,whichheavilyreliesonthe
clusterassumption–lowerdatadensityresultsinacloserdecisionboundary,leadingtoincreased
uncertainty.WealsofindthatthemajorityofotherTTAmethods,suchastest-timetrainingmethods–
3UnderreviewasaconferencepaperatICLR2024
Figure3:Labeldistributionshiftbetweenthesourcedomain(a)andthetargetdomain(b)inthe
tabulardataset–HELOC,biasofmodelpredictionstowardsthesourcelabeldistribution(c),andthe
labeldistributionofapplyingourlabeldistributionhandlertowardsthetargetlabeldistribution(d).
TTT++(Liuetal.,2021)andTTT-MAE(Gandelsmanetal.,2022)–andoutputoptimizationmethods
–LAME(Boudiafetal.,2022)andODS(Zhouetal.,2023)–heavilyrelyonrepresentationlearning
abilitiesofdeepneuralnetworks.Specifically,LAME(Boudiafetal.,2022)andODS(Zhouetal.,
2023),directlyaimtooptimizetheprobabilitiesofinstancesproximatewithintherepresentation
space,ultimatelybringingthemcloser.However,thesemethodsencounterlimitationsintabulardata
becausetheclusterassumptiondoesnothold.Thesefindingsunderscoretheneedforuncertainty
calibration,drivenbythemodel’spoorcalibration.Conversely,theyalsohighlightthechallenges
associatedwithimprovingtherepresentationspacefortabulartest-timeadaptation.
2.2 LABELDISTRIBUTIONSHIFTANDCLASSIMBALANCEPROBLEM
Labeldistributionshift,whichisamismatchinoutputclasslabeldistributionbetweenthesource
domainandtargetdomain,hasbeenasignificantchallengeinvariousmachinelearningtasks(Ganin
& Lempitsky, 2015; Wang et al., 2021b; Khurana et al., 2021). In situations of distribution shift
intabulardata,labeldistributionshiftcommonlyoccursbecauseoffactorssuchasdatacollection
patterns,cohortdifferences,andspatialandtemporalchanges.Thesechallengescansignificantly
impactmodelperformance,emphasizingtheimportanceofconsideringtheminthecontextoftabular
test-timeadaptation.
Recently,inthefieldoftest-timeadaptationforimageclassification,methodshavebeenproposedto
addresssuchissueoflabeldistributionshifts(Parketal.,2023;Zhouetal.,2023).However,these
methodsmaketheimpracticalassumptionthatthesourcemodelistrainedonperfectlyclass-balanced
data.ThisassumptionmightholdforstandardimageclassificationbenchmarkslikeMNIST(Deng,
2012)orCIFAR-10(Krizhevskyetal.,2009)butishardtobesatisfiedinthetabulardomain.Under
the class imbalanced scenario of the source domain, we find that models tend to produce biased
predictionsduringtestingbasedontheclassdistributionobservedinthetrainingdata(Figure3).
These findings pose an additional challenge of addressing both label distribution shift and class
imbalance,thushighlightingtheneedfordevelopinganoveltest-timeadaptationmethodforthe
tabulardomain.
Beforedelvingintotheintricaciesofourapproach,wehighlighta Table1:Pivotalfindingsthat
pivotalfinding:uncertaintycalibrationplaysacrucialroleinmitigat- uncertainty calibration bene-
ingthelabeldistributionshifts.Ourmethodfundamentallyestimates fitsthelabeldistributionhan-
the average label distribution of the current batch and it corrects dler.
predictionsbymaintainingconfidentpredictionswhileadjustingun-
Method HELOC ANES
certainpredictionstowardstheestimatedaveragelabeldistribution
Unadapted 47.6 79.3
of the current batch. As shown in Table 1, we observe that if the AdapTable 63.7 79.6
sourcemodelisperfectlycalibratedbyincreasingtheconfidencefor AdapTable(Oracle) 90.1 84.7
correctsampleswhiledecreasingtheconfidenceforincorrectsamples,ourlabeldistributionhandler
leadstoaremarkableimprovementinperformance.Thishighlightsthecriticalroleofdevelopingan
effectiveuncertaintycalibratorforthelabeldistributionhandler.
3 ADAPTABLE
Inthissection,we introduceanAdaptationfor Table(AdapTable)framework,whichisthefirst
tabulartest-timeadaptationstrategy.AdapTablefocusesoncorrectingtheoutputprobabilityand
circumventstheneedtotunethemodelparameterusingunsupervisedobjectives.Tothisend,weaim
tomaintainpredictionsforconfidenttestsampleswhileheavilycorrectingpredictionsforuncertain
testsamples.Toestimatethemodel’suncertaintyincasesofpoorlycalibratedsourcemodels(as
4UnderreviewasaconferencepaperatICLR2024
Figure4:TheoverallpipelineoftheproposedAdapTableframework.AdapTablefocusesoncor-
rectingtheoutputprobabilityandcircumventstheneedtotunethemodelparameters.Towardsthis,
AdapTableestimatesthetargetlabeldistribution(Section3.3)andadjuststheoriginalprobability
basedoncalibrateduncertainty(Section3.2).
investigatedinSection2.1),wederiveashift-awareuncertaintycalibrator,whichpredictsper-sample
temperature scaling factor to calibrate model predictions in Section 3.2. After that, a novel label
distributionhandlerisintroducedinSection3.3.Theoverallframeworkoftheproposedmethodis
depictedinFigure4andinAlgorithm1.
3.1 TEST-TIMEADAPTATIONSETUPFORTABULARDATA
Wefirstdefinetheproblemsettingoftest-timeadaptationforthetabulardomain.LetF(·|θ):RD →
RC beapre-trainedclassifieronthelabeledsourcetabulardomainD ={(xs,ys)} inpairsofa
s i i i
tabularinputanditsoutputclasslabel,whichtakesarowx ∈RD fromatableandreturnsoutput
i
logitF(x |θ).Here,DandC arethenumberofinputfeaturesandoutputclasses,respectively.We
i
focusontabularclassification,andsuggesttest-timeadaptationmethodsfortabularclassification
aimtoadaptF(·|θ)totheunlabeledtargettabulardomainD ={xt} duringthetestphasewithout
t i i
accesstoD .UnlikemostTTAapproacheswhichdirectlyfine-tuneθwithunsupervisedobjectives,
s
wecorrecttheoutputpredictionF(xt|θ)directly.
i
3.2 SHIFT-AWAREUNCERTAINTYCALIBRATOR
SincesourcetabularmodelsarepoorlycalibratedasdiscussedinSection2.1anduncertaintycalibra-
tionishelpfulforhandlinglabeldistributionshifts,givenatestbatch{xt}N inthetargetdomain,we
i i=1
proposeashift-awarepost-hocuncertaintycalibratorG(·,·|ϕ):RNC ×RD →Rthatincorporates
shiftinformationandsimultaneouslyaddressesbothoverconfidenceandunder-confidence,departing
fromtraditionalapproachesthatprimarilyrelyonoutputlogitsonly(Guoetal.,2017)andfocusonly
oneitheroverconfidence(Guoetal.,2017;Pollastrietal.,2021)orunder-confidence(Wangetal.,
2021c;Hsuetal.,2022).Here,G takesnotonlytheoutputlogitF(xt|θ)ofeachtestinstancext
i i
butalsotheshiftinformationofcurrentbatchstasadditionalinformationandreturnsaper-sample
temperature scaling factor T = G(F(xt|θ),st|ϕ). We define shift information st of a specific
i i u
columnindexuasfollows:
st =(cid:16) xt
−(cid:0)| (cid:88)Ds|
xs/|D |(cid:1) (cid:17)N .
u iu i′ s u i=1
i′=1
Here,wefocusonthedistributionshiftofeachcolumn,whichisatabular-specificinputcovariate
shift,andformashift-awaregraph,whereeachnoderepresentsaspecificcolumn,andeachedge
representstherelationshipbetweendifferentcolumns.Specifically,weformanodefeatureh(0)of
u
5UnderreviewasaconferencepaperatICLR2024
eachnodeindexuandadjacencymatrixA∈[0,1]D×D asfollows:
h(0) =st for1≤u≤D
u u
A =ρ(cid:0) (xt )N ,(xt )N (cid:1) for1≤u,v ≤D,
uv iu i=1 iv i=1
whereρ(X,Y)denotestheabsolutevalueofcorrelationcoefficientbetweenX andY.Suchnode
embeddingsarereasonableforthetabulardomain,whereeachcolumnhasadistinctandindependent
meaning,andtheseembeddingsconsidertheshiftsandrelationshipsbetweenthem.InourG,wehave
columnwiseprojectionlayerl (formatchinginputdimensionsbetweennumericalandcategorical
u
columns–weassumethatallcolumnsarenumericalforthesimplicityofnotation,butinsituations,
wherecategoricalcolumnsarepresent,st forcategoricalcolumnindexubecomesamatrixinstead
u
ofasinglevectorandapplylinearprojectiononitundertheone-hotencodingscheme),singlegraph
convolutionlayerGCN(·,·,·),andfinaldecoderDec(·)thatreturnsper-sampletemperaturescaling
factor T . After forming a shift-aware graph, we apply l and GCN to get hidden representation
i u
(cid:16) (cid:17)
h(1) =GCN l h(0),{l h(0),∀v ∈N },A ,whereN denotestheindexsetoftheneighborhoods
u u u v v u u
ofu-thcolumn(i.e.,allothercolumnsdefinedbyA).Wethenaverageh(1)toextractacomprehensive
u
representationh˜ =(cid:80)D h(1)/Dtocapturetheshiftinformationacrossallcolumnswithincurrent
u=1 u
(cid:16) (cid:17)
testbatch,andfinallygetper-sampletemperaturescalingfactorT =Dec F(xt|θ),h˜ .Wetrain
i i
G byoptimizingϕusingtrainingsetagainaftertrainingF withlossunctionL=L +λ L
FL CAL CAL
motivatedbyunder-confidencebehaviorinFigure1(a)andFigure2(b),whereL denotesfocal
FL
loss(Linetal.,2017b),L denotesaregularizationtermsuggestedinCaGCN(Wangetal.,2021c),
CAL
andλ denotestheweightofregularizationterm.Suchpost-trainingstrategyiscommonlyused
CAL
intest-timeadaptationmethods(Limetal.,2023;Parketal.,2023),andthedetailedpost-training
strategyisfullydescribedinSectionC.
3.3 LABELDISTRIBUTIONHANDLER
Thissectionproposesalabeldistributionhandler,forcorrectlyestimatingthetargetlabeldistribution
onthecurrenttestbatch.Formally,ifweestimatetheclassprobabilitygivenxtinthetargetdomain
i
p (y|xt) = σ (F(xt|θ)) with Softmax function σ , using Bayes’ theorem, this can be
t i Softmax i Softmax
representedasfollows:
p (y)p (xt|y)
p (y|xt)= t t i .
t i p (xt)
t i
Basedontheobservationthatthedistributionofthemodelinthetargetdomainisbiasedtowardsthe
labeldistributionofthesourcedomainasdiscussedinSection2.2andFigure3,wecaninferthat
p (y)ishighlybiasedtop (y)=(cid:0)(cid:80)|Ds|1 /|D |(cid:1)C withanindicatorfunction1.Onesimple
t s i=1 [j=y is] s j=1
solutiontocorrectthisbiasedprobabilityistomultiplyp (y)/p (y),i.e.,usep (y|xt)·p (y)/p (y)
t s t i t s
asourfinalprediction.Weassumethatwecanaccessthemarginallabeldistributionofthesource
domainsimilartoTTT++(Liuetal.,2021).Toestimatethemarginallabeldistributionofthetarget
domainp (y)withoutlabels,wederiveanonlinetargetlabelestimator toleveragethetemporal
t
localityoflabeldistributioninreal-worldscenarios,whereclasslabelsoftestsamplescollectedat
closetimeintervalstendtoexhibitrelativesimilarity.Additionally,weintroduceadebiasedtarget
labelestimatortomitigatebiastowardsthesourcelabeldistribution.Atfirst,weinitializeonline
targetlabelestimatorpoe(y)=(1/C)C asauniformdistribution.Givenatestbatch{xt}N ,we
t j=1 i i=1
predictthedebiasedtargetlabelestimatorpde(y|xt)andestimatethecurrenttargetlabeldistribution
t i
p (y)asfollows:
t
(cid:16) (cid:17)
pde(y|xt)=σ p (y|xt)/p (y)
t i L1 t i s
N
(cid:88)
p (y)=(1−α)· pde(y|xt)/N +α·poe(y),
t t i t
i=1
where the interpolate cumulative online target label estimator and the average probability of the
current debiased target label estimator with L normalization function σ . Now, we utilize the
1 L1
shift-aware uncertainty calibrator defined in Section 3.2. Given a current test batch {xt}N , we
i i=1
calculatest andgetper-sampletemperatureT = G(F(xt|θ),st|ϕ).Wedefinetheuncertaintyϵ
i i i
6UnderreviewasaconferencepaperatICLR2024
Table 2: The average accuracy (%) and their corresponding standard errors for both supervised
modelsandTTAbaselinesarereportedacrossthreerepresentativebackbonearchitecturesandthree
datasetsfromTableShiftbenchmark,whichincludenaturaldistributionshifts.Theresultsarereported
overthreedifferentrandomseeds.
Method
HELOC ANES DIABETESREADMISSION
Acc. bAcc. F1 Acc. bAcc. F1 Acc. bAcc. F1
K-NN 47.7±0.0 62.0±0.0 40.3±0.0 74.8±0.0 76.9±0.0 71.1±0.0 57.4±0.0 57.7±0.0 56.9±0.0
LR 49.9±0.0 63.5±0.0 44.2±0.0 78.7±0.0 80.2±0.0 76.2±0.0 60.2±0.0 61.4±0.0 58.9±0.0
RF 44.1±0.7 58.2±7.6 32.2±1.5 74.3±0.3 81.7±0.1 68.4±0.7 53.7±0.5 64.4±0.5 42.1±1.2
XGBOOST 48.0±3.1 57.6±7.2 39.9±4.9 78.5±0.2 80.5±0.2 75.8±0.4 62.2±0.2 63.1±0.1 61.3±0.4
CATBOOST 54.7±0.0 65.4±0.0 51.7±0.0 79.1±0.0 80.4±0.0 76.8±0.0 62.6±0.3 63.4±0.0 61.8±0.5
+ADAPTABLE 65.6±0.0 65.5±0.0 65.4±0.0 79.9±0.0 79.6±0.0 78.6±0.0 62.9±0.2 63.3±0.0 62.5±0.3
Unadapted 47.0±1.8 53.2±1.5 38.2±3.5 79.3±0.2 76.5±0.5 77.3±0.4 61.3±0.1 61.1±0.1 60.2±0.3
PL 45.3±1.2 51.8±1.0 34.9±2.3 78.9±0.2 75.6±0.5 76.6±0.5 60.7±0.1 60.5±0.1 58.9±0.3
TTT++ 47.0±1.6 53.2±1.5 38.2±3.6 79.5±0.1 76.8±0.5 77.6±0.2 61.3±0.1 61.1±0.1 60.2±0.3
TENT 44.6±1.5 51.2±1.2 33.2±2.6 78.0±0.5 74.0±0.6 74.9±0.6 60.2±0.2 60.2±0.1 58.3±0.3
EATA 47.0±1.8 53.2±1.5 38.2±3.6 79.3±0.2 76.5±0.5 77.3±0.4 61.3±0.1 61.1±0.1 60.2±0.4
SAR 43.1±0.0 50.0±0.0 30.1±0.0 69.4±0.9 62.0±1.2 59.4±1.6 57.5±1.0 57.1±1.1 51.3±2.2
LAME 43.1±0.0 50.0±0.0 30.1±0.0 63.5±0.3 54.6±0.5 46.8±1.0 55.3±0.4 54.9±0.5 46.9±1.0
ADAPTABLE 64.5±0.7 65.8±0.6 64.5±0.3 79.6±0.1 78.4±0.3 78.6±0.0 61.7±0.0 61.7±0.0 61.7±0.0
Unadapted 43.1±0.0 50.0±0.0 30.2±0.1 72.0±3.5 67.0±4.1 66.8±5.1 53.9±3.5 53.4±3.6 42.2±10.1
PL 43.1±0.0 50.0±0.0 30.2±0.1 71.6±3.5 66.4±4.0 66.0±5.2 53.9±3.5 53.5±3.6 42.8±10.1
TTT++ 43.1±0.0 50.0±0.0 30.2±0.1 71.6±4.1 65.9±5.2 64.6±6.8 53.0±2.4 52.5±2.7 40.3±6.1
TENT 43.1±0.0 50.0±0.0 30.2±0.1 73.7±2.5 68.5±2.2 68.6±3.3 53.1±2.4 52.5±2.5 41.4±6.1
EATA 43.1±0.0 50.0±0.0 30.2±0.1 74.4±1.5 69.4±2.1 69.7±2.5 53.0±2.4 52.5±2.4 40.3±7.5
SAR 43.1±0.0 50.0±0.0 30.1±0.0 72.5±3.1 68.5±4.0 68.3±5.1 50.6±0.1 50.0±0.4 34.4±0.9
LAME 43.1±0.0 50.0±0.0 30.1±0.0 67.8±5.1 60.8±10.1 55.6±12.1 50.6±0.0 50.0±0.0 33.9±0.3
ADAPTABLE 51.5±5.2 55.6±4.0 46.1±7.1 75.2±1.3 73.0±3.2 72.8±2.5 56.1±3.5 50.2±0.0 55.8±3.4
Unadapted 43.4±0.5 50.3±0.2 30.9±1.0 79.1±0.2 75.9±0.5 76.8±0.3 62.5±0.2 62.4±0.2 61.9±0.2
PL 43.2±0.2 50.0±0.0 30.3±0.3 78.8±0.3 75.3±0.6 76.3±0.7 62.2±0.3 62.0±0.3 61.3±0.4
TTT++ 43.5±0.5 50.3±0.5 31.0±1.2 79.3±0.3 76.6±0.4 77.4±0.3 62.6±0.0 62.5±0.0 62.2±0.1
TENT 43.2±0.3 50.1±0.1 30.4±0.3 78.5±0.3 74.9±0.7 75.9±0.8 62.2±0.3 62.0±0.4 61.2±0.3
EATA 47.9±6.1 53.4±3.3 39.4±8.3 79.1±0.4 75.9±0.5 76.8±0.4 62.5±0.2 62.4±0.2 61.9±0.2
SAR 43.3±0.4 50.2±0.3 30.6±0.5 74.2±1.5 68.4±1.8 68.2±2.4 60.2±0.8 59.8±0.8 57.0±1.3
LAME 47.7±5.1 53.3±3.3 38.9±10.1 77.2±0.6 72.9±0.9 73.7±0.8 52.1±0.5 51.6±0.6 41.3±2.3
ADAPTABLE 60.3±0.6 60.3±0.6 55.9±1.7 77.8±0.2 77.8±2.4 78.2±0.0 61.7±0.0 60.6±2.1 60.5±1.8
of F(xt|θ) as a reciprocal of margin of calibrated probability distribution σ (F(xt|θ)/T )
i Softmax i i
(SectionC).Wethenmeasurequantilesofeachinstancex usingϵ withincurrentbatch,andadjust
i i
originalprobabilitywithc asp (y|xt)′ =σ (c ·F(xt|θ))withascalingconstantc defined
i t i Softmax i i i
likebelow:
 T if ϵ ≤Q(cid:0) {ϵ }N ,q (cid:1)
 i i′ i′=1 low
c = 1 if Q(cid:0) {ϵ }N ,q (cid:1) <ϵ <Q(cid:0) {ϵ }N ,q (cid:1) (1)
i i′ i′=1 low i i′ i′=1 high
1/T if ϵ ≥Q(cid:0) {ϵ }N ,q (cid:1) ,
i i′ i′=1 high
whereQ(X,q)denotesthequantilefunctionwhichgivesthevaluecorrespondingtothelowerq
quantileinX,T =1.5ρwithρ=max p (y) /min p (y) denotestemperaturescalinghyperpa-
j s j j s j
rameter,andq /q denotelow/highquantiles,respectively.Finally,wegetthefinalprediction
low high
pˆ(y)withself-ensembling(Gaoetal.,2023)andupdatesonlinetargetlabelestimatorpoe(y)as
i t
follows:
(cid:16) (cid:17)
pˆ(y)=p (y|xt)′/2+σ p (y|xt)′p (y)/p (y) /2
i t i L1 t i t s
N
(cid:88)
poe(y)←(1−α)· pˆ(y)/N +α·poe(y).
t i t
i=1
ThemoredetailedexplanationofAdapTableandthealgorithmtablecanbefoundinSectionCand
Algorithm1,respectively.
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
Source Tabular Models and Datasets To verify the proposed method under various tabular
models,threerepresentativedeeptabularlearningmodels–MLP(Murtagh,1991),TabNet(Arik
&Pfister,2021),andFT-Transformer(Gorishniyetal.,2021)–areusedassourcetabularmodels.
Todemonstrateourapproachacrossvarioustest-timedistributionshifts,wetestourmethodunder
7
desivrepuS
PLM
teNbaT
remrofsnarT-TFUnderreviewasaconferencepaperatICLR2024
Table3:Theaverageaccuracy(%)andtheircorrespondingstandarderrorsofbothsupervisedmodels
and TTA baselines with MLP backbone architecture across three datasets from OpenML-CC18
benchmark(Bischletal.,2021),eachcorruptedwith6differentsyntheticcorruptions:Gaussiannoise,
uniformnoise,randomcolumndrop,randommissing,numericalcolumnshifts,categoricalcolumn
shifts.Theresultsarereportedoverthreedifferentrandomseeds.
CMC MFEAT-PIXEL DNA
Method
Acc. bAcc. F1 Acc. bAcc. F1 Acc. bAcc. F1
K-NN 47.9±1.7 45.4±1.3 44.0±1.3 96.2±0.5 96.1±0.5 95.8±0.6 79.3±1.6 77.0±1.1 77.8±1.2
LR 49.0±1.4 46.3±1.3 42.8±1.3 96.8±0.5 96.6±0.5 96.6±0.5 89.4±2.0 87.9±1.7 87.8±2.1
RF 51.4±1.5 48.5±1.3 44.5±1.4 96.5±0.3 96.3±0.4 96.3±0.4 91.1±2.0 90.7±1.6 89.2±2.1
XGBOOST 52.9±2.0 49.7±1.7 47.1±1.7 94.2±0.6 94.0±0.8 93.9±0.8 91.7±2.1 91.1±1.8 90.2±3.2
CATBOOST 51.9±2.0 49.0±1.8 47.4±1.8 96.5±0.4 96.3±0.5 96.3±0.5 91.8±2.0 90.7±1.8 90.4±2.1
+ADAPTABLE 52.4±1.6 49.3±0.6 48.6±0.7 96.7±0.5 96.6±0.5 96.5±0.5 94.6±1.1 92.9±1.8 93.8±1.5
Unadapted 53.5±1.6 47.9±1.4 47.9±1.5 96.4±0.3 96.4±0.4 96.3±0.4 91.4±1.5 90.1±1.5 90.0±1.5
PL 54.1±2.0 47.8±1.5 47.8±1.5 96.4±0.3 96.5±0.3 96.3±0.4 90.9±1.5 89.5±1.8 89.4±1.5
TTT++ 52.7±1.7 47.2±1.4 47.2±1.8 95.8±0.4 95.6±0.5 95.5±0.5 89.5±1.3 86.6±1.5 87.6±1.5
TENT 54.1±2.0 47.5±1.4 47.5±1.5 96.5±0.4 96.5±0.3 96.4±0.3 90.3±1.4 88.7±1.9 88.6±1.7
EATA 53.8±1.7 48.1±1.4 48.1±1.5 96.4±0.4 96.5±0.4 96.3±0.4 91.4±1.4 90.1±1.7 90.0±1.5
SAR 48.0±2.0 37.7±0.7 37.7±0.7 65.6±5.0 63.8±5.1 62.8±5.8 62.6±3.1 59.8±2.8 55.2±3.5
LAME 49.6±4.4 40.3±3.2 40.3±3.1 95.9±0.7 95.8±0.7 95.7±0.7 74.9±5.1 63.6±5.8 63.8±9.1
ADAPTABLE 55.7±2.4 50.3±1.8 50.2±1.7 97.8±0.4 97.5±0.4 97.4±0.4 95.0±0.6 89.9±1.8 92.1±0.9
threenaturaldistributionshiftsdatasets– HELOC (Brownetal.,2018), ANES (Studies,2022),
andDIABETESREADMISSION(Cloreetal.,2014)inTableShiftbenchmark(Gardneretal.,2023).
Furthermore, we also verify our method by randomly splitting three datasets – CMC, MFEAT-
PIXEL, and DNA – within the OpenML-CC18 benchmark (Bischl et al., 2021), and injecting
sixtypesofsyntheticcorruptions–Gaussiannoise,uniformnoise,randommissing,columndrop,
numericalcolumnshift,andcategoricalcolumnshift–onthem.Thesedatasetsareselectedwiththe
followingcarefulconsiderations:i)consistofbothnumericalandcategoricalfeatures,ii)consistonly
ofcategoricalornumericalfeatures,andiii)differenttasktypesincludingbothbinaryandmulti-way
classification. More details about the datasets are described in Section E.1 of the supplementary
material.
BaselinesandImplementationDetails WecompareAdapTablewithseventesttimeadaptation
baselines–PL(Lee,2013),TTT++(Liuetal.,2021),TENT(Wangetal.,2021a),SAM(Foretetal.,
2021)(withTENT),EATA(Niuetal.,2022),SAR(Niuetal.,2023),andLAME(Boudiafetal.,
2022)–intheotherdomain.Forthepurposeofperformancereference,wealsoreporttheaccuracy
of classical machine learning algorithms: k-nearest neighbors (K-NN), logistic regression (LR),
randomforest(RF),XGBOOST(Chen&Guestrin,2016),andCATBOOST(Dorogushetal.,2017).
Weemployfixedbatchsize64forallexperiments,acommonsettingamongbaselines(Schneider
etal.,2020;Wangetal.,2021a).Othertest-timehyperparametersaretunedwithrespecttonumerical
columnshiftwithintheCMCdataset,foreachbackbonearchitecture.Thehyperparametersarethen
fixedthroughoutallofthedatasets.Moredetailsaboutthebaselinesandimplementationsareshown
inSectionF.2andSectionF.3ofthesupplementarymaterial.
4.2 MAINRESULTS
ResultonNaturalDistributionShifts Table2showstheresultonnaturaldistributionshifts.We
observethatourmethodconsistentlyoutperformsbaselinesinmostsettingsacrossdifferentdatasets
andbackbonearchitecturetypes.Furthermore,itisworthnotingthatourmethod,integratedwith
CatBoost(Dorogushetal.,2017),canalsoeffectivelyadaptinthesesettings,wherebaselineTTA
approachescannotbeapplicable.Asdiscussedin 2.1,entropyminimizationvariantsfailundermost
datasets,aswellasthebaselinesthatdependonclusterassumptionsonlatentspace-TTT++(Liu
etal.,2021),EATA(Niuetal.,2022)andLAME(Boudiafetal.,2022).
Result on Synthetic Corruptions We further evaluate the efficacy of AdapTable on different
datasetsofOpenML-CC18benchmark(Bischletal.,2021)onsyntheticcorruptions,depictedin
Table3.Pleasenotethatwereporttheaverageperformanceacrosssixsyntheticcorruptions.The
overalltrendissimilartoreal-worlddistributionalshifts–showingstate-of-the-artperformanceon
mostdatasetsandevaluationmetrics,showingthatourmethodcaneffectivelyadaptundervarious
8
desivrepuS
PLMUnderreviewasaconferencepaperatICLR2024
Figure5:Reliabilitydiagrambefore(a)andafter(b)applyingshift-awareuncertaintycalibratorfor
CMCdataset,andJensen-ShannonDivergencebetweenestimatedtargetlabeldistributionbeforeand
afterapplyinglabeldistributionhandleruponHELOCdataset(c).
datasetswithbothnumericalandcategoricalcolumns(CMC),withonlynumerical(MFEAT-PIXEL)
andcategoricalcolumns(DNA).Itisalsoworthhighlightingthatourapproachcanbecombined
withCatBoostinthissettingaswellforthesesyntheticcorruptions.
4.3 FURTHERANALYSIS
AblationStudy Toproveeachcomponentof Table4:AblationstudyofAdapTabledecomposed
theproposedmethodtowardsperformanceim- intothecomponents,whereSUC indicatesshift-
provement, we conduct an ablation study. As awareuncertaintycalibrator.
showninTable4,webelievethattheproposed
CMC
shift-awareuncertaintycalibratoraswellasthe Method MFEAT-PIXEL HELOC
NUMERICALCATEGORICAL
graphconvolutionlayerwithiniteffectivelyin-
creasestheaccuracyunderbothnaturaldistribu- Unadapted 59.7±1.6 58.1±0.8 96.0±0.1 47.0±1.6
tionshifts(HELOC)andsyntheticcorruptions AdapTable
61.9±0.7 61.4±1.2 97.2±0.1 61.7±2.2
(w/oSUC)
(MFEAT-PIXEL,CMC).
AdapTable
64.0±0.9 62.6±1.3 97.4±0.1 62.5±0.6
(w/MLPSUC)
EfficacyofShift-AwareUncertaintyCalibra- AdapTable 65.6±1.6 64.2±2.4 97.8±0.2 64.5±0.3
tor Toverifytheproposedshift-awareuncer-
taintycalibratorsuggestedinSection3.3,wecomparethereliabilitydiagrambeforeandafterapplying
shift-awareuncertaintycalibratorinFigure5(a)and(b).Theproposedmethod’sabilitynotonly
tobringthereliabilitydiagramclosertothey =xlinebutalsotoreduceconfidenceandincrease
uncertaintyforshiftedtestsetsisindeednoteworthy.
EfficacyofLabelDistributionHandler Figure5(c)comparesJensen-ShannonDivergencevalue
oftestlabeldistributionandpredictiononeachbatch,betweenbeforeandafteradaptationusinglabel
distributionhandler(LDH)forHELOC(Brownetal.,2018)dataset.Thefigurerepresentsalarge
amountofdeclineofdivergence,whichimplieslabeldistributionhandler3.3canindeedreducea
gapbetweenshiftedtargetlabeldistributionandsource-orientedmodelprediction.
5 CONCLUSION
In this paper, we have introduced AdapTable, a tabular-specific test-time adaptation strategy for
thefirsttime.Wehavesystematicallyinvestigatedthechallengesrelatedtotest-timeadaptationfor
tabulardataandhaveproposedtest-timeadaptationstrategiestoovercomesuchproblemswithtwo
key components: a shift-aware uncertainty calibrator, which utilizes information about covariate
shiftsamongcolumnstocorrectpoorconfidencecalibrationinthemodel,andalabeldistribution
handler, which estimates the label distribution of the current test batch in real-time and uses the
improveduncertaintytoadjusttheoutputdistribution.Extensiveexperimentshavedemonstrated
thattheproposedmethodachievesstate-of-the-artperformanceacrossdifferentdatasetswithboth
naturaldistributionshiftsandsyntheticcorruptionsandthreerepresentativedeeptabularlearning
architectures.AdapTableprovidesafoundationalphilosophyfordesigningtest-timeadaptationin
thecontextoftabulardataandshedslightonthedomainadaptationfieldfortabulardata,offering
valuableinsightsanddirectionsforfutureresearch.
9UnderreviewasaconferencepaperatICLR2024
ETHICS STATEMENT
Tabulardataoftencontainssensitivepersonalinformation,suchasmedicaltestresultsinhealthcare
dataorfinancialdetailsineconomicdata.Therefore,itiscrucialtohandlesuchdatawithutmostcare
andconsiderationforprivacyandsecurityconcerns.Inthisregard,AdapTable,whichenablesthe
adaptationofamodeltothetargetdomainwithoutrequiringaccesstosensitivesourcedomaindata,
holdssignificantappeal.Webelievethattheproposedmethodhasthepotentialtoaddressprivacy
andsecurityconcernsassociatedwithsensitivesourcedomaindatawhilestillachievingeffective
adaptationtothetargetdomain.
REPRODUCIBILITY STATEMENT
Toensurereproducibility,wewillreleasethesourcecodesoon.
REFERENCES
E.AlpaydinandC.Kaynak. OpticalRecognitionofHandwrittenDigits. UCIMachineLearning
Repository,1998.
SercanÖArikandTomasPfister. TabNet:Attentiveinterpretabletabularlearning. InAAAI,2021.
ShaiBen-David,JohnBlitzer,KobyCrammer,andFernandoPereira. Analysisofrepresentationsfor
domainadaptation. InNIPS,2006.
DavidBerthelot,NicholasCarlini,EkinDCubuk,AlexKurakin,KihyukSohn,HanZhang,andColin
Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation
anchoring. InICLR,2020.
BerndBischl,GiuseppeCasalicchio,MatthiasFeurer,FrankHutter,MichelLang,RafaelG.Man-
tovani, Jan N. van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. In NeurIPS,
2021.
Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online
test-timeadaptation. InCVPR,2022.
KyleBrown,DerekDoran,RyanKramer,andBradReynolds. HELOCapplicantriskperformance
evaluation by topological hierarchical decomposition. In NIPS Workshop on Challenges and
OpportunitiesforAIinFinancialServices:theImpactofFairness,Explainability,Accuracy,and
Privacy,2018.
TianqiChenandCarlosGuestrin. XGBoost:Ascalabletreeboostingsystem. InKDD,2016.
JohnClore,KrzysztofCios,JonDeShazo,andBeataStrack. Diabetes130-UShospitalsforyears
1999-2008. UCIMachineLearningRepository,2014.
LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch. IEEESignal
ProcessingMagazine,2012.
Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. CatBoost: gradient boosting with
categoricalfeaturessupport. InNIPSWorkshoponMLSystems,2017.
Zi-YiDou,JunjieHu,AntoniosAnastasopoulos,andGrahamNeubig. Unsuperviseddomainadapta-
tionforneuralmachinetranslationwithdomain-awarefeatureembeddings. InEMNLP,2019.
MAminFarajian,MarcoTurchi,MatteoNegri,andMarcelloFederico. Multi-domainneuralmachine
translationthroughunsupervisedadaptation. InProceedingsoftheSecondConferenceonMachine
Translation,2017.
PierreForet,ArielKleiner,HosseinMobahi,andBehnamNeyshabur. Sharpness-awareminimization
forefficientlyimprovinggeneralization. InICLR,2021.
10UnderreviewasaconferencepaperatICLR2024
YossiGandelsman,YuSun,XinleiChen,andAlexeiEfros. Test-timetrainingwithmaskedautoen-
coders. InNeurIPS,2022.
YaroslavGaninandVictorLempitsky. Unsuperviseddomainadaptationbybackpropagation. In
ICML,2015.
JinGao,JialingZhang,XihuiLiu,TrevorDarrell,EvanShelhamer,andDequanWang. Backtothe
source:Diffusion-drivenadaptatidontotest-timecorruption. InCVPR,2023.
JoshGardner,ZoranPopovic,andLudwigSchmidt. Benchmarkingdistributionshiftintabulardata
withtableshift. InNeurIPS,2023.
YuryGorishniy,IvanRubachev,ValentinKhrulkov,andArtemBabenko. Revisitingdeeplearning
modelsfortabulardata. InNeurIPS,2021.
LéoGrinsztajn,EdouardOyallon,andGaëlVaroquaux. Whydotree-basedmodelsstilloutperform
deeplearningontabulardata?arxiv2022. InNeurIPS,2022.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. InICML,2017.
DanielHein,StefanDepeweg,MichelTokic,SteffenUdluft,AlexanderHentschel,ThomasA.Runk-
ler,andVolkmarSterzing. Abenchmarkenvironmentmotivatedbyindustrialcontrolproblems. In
SSCI,2017.
HansHao-HsunHsu,YuesongShen,ChristianTomani,andDanielCremers. Whatmakesgraph
neuralnetworksmiscalibrated? InNeurIPS,2022.
AlistairJohnson,LucasBulgarelli,TomPollard,StevenHorng,LeoAnthonyCeli,andRogerMark.
Mimic-iv,2021.
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad
Ghassemi,BenjaminMoody,PeterSzolovits,LeoAnthonyCeli,andRogerGMark. Mimic-iii,a
freelyaccessiblecriticalcaredatabase. Scientificdata,2016.
SameerKhurana,NikoMoritz,TakaakiHori,andJonathanLeRoux.Unsuperviseddomainadaptation
forspeechrecognitionviauncertaintydrivenself-training. InICASSP,2021.
ChanghunKim,JoonhyungPark,HajinShim,andEunhoYang. SGEM:Test-timeadaptationfor
automaticspeechrecognitionviasequential-levelgeneralizedentropyminimization. InINTER-
SPEECH,2023.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Toronto,ON,Canada,2009.
Meelis Kull, Telmo Silva Filho, and Peter Flach. Beta calibration: a well-founded and easily
implementedimprovementonlogisticcalibrationforbinaryclassifiers. InAISTATS,2017.
MeelisKull,MiquelPerelló-Nieto,MarkusKängsepp,TelmodeMenezeseSilvaFilho,HaoSong,
andPeterA.Flach. Beyondtemperaturescaling:Obtainingwell-calibratedmulticlassprobabilities
withdirichletcalibration. InNeurIPS,2019.
Dong-HyunLee. Pseudo-label:Thesimpleandefficientsemi-supervisedlearningmethodfordeep
neuralnetworks. InICMLWorkshoponChallengesinRepresentationLearning,2013.
HyesuLim,ByeonggeunKim,JaegulChoo,andSunghaChoi. Ttn:Adomain-shiftawarebatch
normalizationintest-timeadaptation. InICLR,2023.
Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDollár. Focallossfordenseobject
detection. InICCV,2017a.
Tsung-YiLin,PriyaGoyal,RossB.Girshick,KaimingHe,andPiotrDollár. Focallossfordense
objectdetection. InICCV,2017b.
11UnderreviewasaconferencepaperatICLR2024
ZacharyC.Lipton,Yu-XiangWang,andAlexanderJ.Smola. Detectingandcorrectingforlabelshift
withblackboxpredictors. InICML,2018.
YuejiangLiu,ParthKothari,BastienVanDelft,BaptisteBellot-Gurlet,TaylorMordan,andAlexandre
Alahi. Ttt++:Whendoesself-supervisedtest-timetrainingfailorthrive? InNeurIPS,2021.
LongMaiandJulieCarson-Berndsen. Unsuperviseddomainadaptationforspeechrecognitionwith
unsupervisederrorcorrection. InINTERSPEECH,2022.
AndreyMalinin,NeilBand,AlexanderGanshin,GermanChesnokov,YarinGal,MarkJ.F.Gales,
AlexeyNoskov,AndreyPloskonosov,LiudmilaProkhorenkova,IvanProvilkov,VatsalRaina,Vyas
Raina,MariyaShmatova,PanosTigas,andBorisYangel. Shifts:Adatasetofrealdistributional
shiftacrossmultiplelarge-scaletasks. InNeurIPS,2021.
M Jehanzeb Mirza, Inkyu Shin, Wei Lin, Andreas Schriebl, Kunyang Sun, Jaesung Choe, Horst
Possegger,MateuszKozinski,InSoKweon,Kun-JinYoon,etal. Mate:Maskedautoencodersare
online3dtest-timelearners. InICCV,2023.
FionnMurtagh. Multilayerperceptronsforclassificationandregression. Neurocomputing,1991.
ShuaichengNiu,JiaxiangWu,YifanZhang,YaofoChen,ShijianZheng,PeilinZhao,andMingkui
Tan. Efficienttest-timemodeladaptationwithoutforgetting. InICML,2022.
ShuaichengNiu,JiaxiangWu,YifanZhang,ZhiquanWen,YaofoChen,PeilinZhao,andMingkui
Tan. Towardsstabletest-timeadaptationindynamicwildworld. InICLR,2023.
SunghyunPark,SeunghanYang,JaegulChoo,andSungrackYun. Labelshiftadapterfortest-time
adaptationundercovariateandlabelshifts. InICCV,2023.
JohnPlatt. Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularizedlikeli-
hoodmethods. InAdvancesinLargeMarginClassifiers,2000.
FedericoPollastri,JuanMaroñas,FedericoBolelli,GiuliaLigabue,RobertoParedes,RiccardoMag-
istroni,andCostantinoGrana. Confidencecalibrationfordeeprenalbiopsyimmunofluorescence
imageclassification. InICPR,2021.
JoaquinQuinonero-Candela,MasashiSugiyama,AntonSchwaighofer,andNeilDLawrence.Dataset
shiftinmachinelearning. MitPress,2008.
SteffenSchneider,EvgeniaRusak,LuisaEck,OliverBringmann,WielandBrendel,andMatthias
Bethge. Improving robustness against common corruptions by covariate shift adaptation. In
NeurIPS,2020.
RajSanjayShah,KunalChawla,DheerajEidnani,AgamShah,WendiDu,SudheerChava,Natraj
Raman,ChareseSmiley,JiaaoChen,andDiyiYang. Whenfluemeetsflang:Benchmarksand
largepretrainedlanguagemodelforfinancialdomain. InEMNLP,2022.
AmericanNationalElectionStudies. Anestimeseriescumulativedatafile[datasetanddocumenta-
tion].september16,2022version,2022. URLwww.electionstudies.org.
MarioStylianouandNancyFlournoy. Dosefindingusingthebiasedcoinup-and-downdesignand
isotonicregression. InBiometrics,2002.
SiningSun,BinbinZhang,LeiXie,andYanningZhang. Anunsuperviseddeepdomainadaptation
approachforrobustspeechrecognition. InNeurocomputing,2017.
YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritzHardt. Test-timetraining
withself-supervisionforgeneralizationunderdistributionshifts. InICML,2020.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNIPS,2017.
DequanWang,EvanShelhamer,ShaotengLiu,BrunoOlshausen,andTrevorDarrell. Tent:Fully
test-timeadaptationbyentropyminimization. InICLR,2021a.
12UnderreviewasaconferencepaperatICLR2024
HuiWang,JianTian,SongyuanLi,HanbinZhao,QiTian,FeiWu,andXiLi. Unsuperviseddomain
adaptationforimageclassificationviastructure-conditionedadversariallearning. InTPAMI,2021b.
XiaoWang,HongruiLiu,ChuanShi,andChengYang. Beconfident!towardstrustworthygraph
neuralnetworksviaconfidencecalibration. InNeurIPS,2021c.
RuihanWu,ChuanGuo,YiSu,andKilianQWeinberger. Onlineadaptationtolabeldistribution
shift. InNeurIPS,2021.
ZhiZhou,Lan-ZheGuo,Lin-HanJia,DingchuZhang,andYu-FengLi. ODS:Test-timeadaptation
inthepresenceofopen-worlddatashift. InICML,2023.
JinjingZhu,HaotianBai,andLinWang. Patch-mixtransformerforunsuperviseddomainadaptation:
Agameperspective. InCVPR,2023.
YunusÜstev,OzlemIncel,andCemErsoy. User,deviceandorientationindependenthumanactivity
recognitiononmobilephones:Challengesandaproposal. InUbiComp,2013.
13UnderreviewasaconferencepaperatICLR2024
APPENDIX
A RelatedWorks 14
B AdditionalObservations 15
B.1 EntropyDistributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.2 LatentSpaceVisualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.3 ReliabilityDiagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C DetailedAlgorithmofAdapTable 16
D FurtherExperiments 18
D.1 ComputationalEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D.2 HyperparameterSensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
D.3 EfficacyofLabelDistributionHandler . . . . . . . . . . . . . . . . . . . . . . . . 19
E DatasetDetails 19
E.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.2 SyntheticCorruptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
F BaselineDetails 21
F.1 DeepTabularLearningArchitectures . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.2 SupervisedBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.3 Test-TimeAdaptationBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G HyperparameterDetails 22
G.1 SupervisedBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.2 Test-TimeAdaptationBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
G.3 AdapTable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
H LimitationsandBroaderImpacts 23
H.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
H.2 BroaderImpacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A RELATED WORKS
DeepTabularLearning Thetabulardomain,distinctfromimageandlanguagedata,hasseen
limitedexplorationindeeplearning,primarilyduetotheabsenceofspatialandsemanticrelation-
shipsthatconvolutionalorrecurrentneuralnetworksrelyon.Althoughattention-basedarchitectures
likeTabNet(Arik&Pfister,2021),FT-Transformer(Gorishniyetal.,2021)havebeenintroduced
for tabular data, they often require large datasets, resulting in computational overhead that sur-
passesperformancegains.Asaresult,multi-layerperceptrons(MLPs)remaindominantinpractical
applications.
Test-Time Adaptation In recent studies, a novel approach called test-time adaptation (TTA)
hasemergedasasolutiontoaddresslimitationsfoundintraditionalunsuperviseddomainadapta-
14UnderreviewasaconferencepaperatICLR2024
tion(UDA)methods(Ganin&Lempitsky,2015;Wangetal.,2021b).Test-timeadaptationstrate-
gies (Wang et al., 2021a; Liu et al., 2021; Niu et al., 2023; Zhou et al., 2023), aim to adapt the
pre-trainedmodelfromasourcedomaintoatargetdomain,withoutaccesstosourcedata.While
these TTA methods have shown promising performance, the test time adaptation scheme for the
tabulardomainstillremainsunder-exploredregimesincethetabulardomainposesuniquechallenges
duetoitsownnature.Therefore,thereisastrongmotivationtoexploreanddevelopTTAmethods
tailoredtothetabularrealm.
UncertaintyCalibration Uncertaintycalibrationisacrucialtechniqueforenhancingthereliability
ofdeeplearningmodeloutputs.Itinvolvesestimatingthemodel’sconfidenceinitspredictionsby
examiningtheprobabilitiesassignedtopredictedclasses.Conventionaltrainingmethodsoftenresult
inunwarrantedoverconfidenceinmodeloutputs,promptingresearchintouncertaintycalibration
methods,suchasPlattscaling(Platt,2000)andisotonicregression(Stylianou&Flournoy,2002)
predatetheeraofdeeplearning,whilemorerecentmethods,includingbetacalibration(Kulletal.,
2017)andDirichletcalibration(Kulletal.,2019)rootedinprobabilitydistributions,haveemerged.
Temperaturescaling,akintoPlattscalingbutsimpler,directlyimpactsuncertaintywhilekeeping
modelpredictionsintact.
Label Distribution Shift In the domain adaptation area, the assumption that there is no label
distributionshiftcanberisky,assuchshiftscanoccurreadilyandsignificantlyaffectmodelper-
formance(Quinonero-Candelaetal.,2008).Variousapproacheshavebeendevelopedtotacklethis
issueincludingReMixMatch(Berthelotetal.,2020),onlinelabeladaptation(Wuetal.,2021),black
boxshiftestimation(BBSE)(Liptonetal.,2018).Recently,inthecomputervisiondomain,some
works(Parketal.,2023;Zhouetal.,2023)havebeenproposedtoconsiderlabeldistributionshift
underthetest-timeadaptationsetting.Inourwork,weleveragethemodel’spredictionsandtraining
setstatisticstoadjusttheoutput.Additionally,weintegratealabeladaptertoeffectuatesubstantive
modificationstotheoutput.
B ADDITIONAL OBSERVATIONS
B.1 ENTROPYDISTRIBUTIONS
ToshowthegeneralizabilityoftheobservationinSection2.1,whereinpredictionentropyofthemodel
consistentlyexhibitsastrongbiastowardtheunder-confidentregion,weadditionallyprovideentropy
distributionhistogramsfortestinstancesacrosssixdifferentdatasetsandthreerepresentativedeep
tabularlearningarchitectures.Here,wecanobservedistinctpatternsbetweentheupperfourrows
(HELOC,ANES,DIABETES READMISSION,CMC)andthelowertworows(MFEAT-PIXEL,
DNA). In the upper four rows, the entropy is consistently high, indicating a skew towards the
under-confidentregion.However,inthelowertworows,thisisnotthecase.Thisdiscrepancyarises
from the fact that the two datasets below are characterized by homogeneity in all columns, one
beingalinearizedimagedataset(MFEAT-PIXEL)andtheotheraDNAstringsequencedataset
(DNA).Thiscomprehensiveanalysisshowcasestheuniquecharacteristicsoftabulardata–biased
entropydistributionstowardtheunder-confidentregion–comparedtootherdomains.Asdiscussed
inSection2.1,applyingentropyminimizationwithsamplesofhighentropyoftencausegradient
explodingandmodelcollapse(Niuetal.,2023).
B.2 LATENTSPACEVISUALIZATIONS
Wefurthervisualizelatentspacesfortestinstancesusingt-SNEacrosssixdifferentdatasetsandthree
representativedeeptabularlearningarchitectures,toshowthecommonbehavioroftheobservation
inSection2.1,whereintabulardataexhibitsextremelycomplexdecisionboundarywithinthelatent
spacecomparedtothatofotherdomains.Again,bycomparingupperforrowswithtabulardataset
andlowertworowswithlinearizedimagedataset(MFEAT-PIXEL)andhomogeneousDNAstring
sequencedataset(DNA),itisobviousthatthedecisionboundarywithinthelatentspaceoftabular
domainisparticularlycomplexcomparedtootherdomainsexceptforthecaseofDNAdatasetwith
TabNetmodel.AsshowninSection2.1,thisalsoprovidesanotherindicationofthelimitationsof
existingTTAmethods(Sunetal.,2020;Gandelsmanetal.,2022;Liuetal.,2021;Gandelsmanetal.,
2022;Boudiafetal.,2022;Zhouetal.,2023),whichheavilyrelyontheclusterassumption.
15UnderreviewasaconferencepaperatICLR2024
Algorithm1AdapTable
1: Input:Pre-trainedtabularclassifieronthesourcedomainF(·|θ):RD →RC,post-trainedshift-
awareuncertaintycalibratorG(·,·|ϕ),indicatorfunction1,quantilefunctionQ,SoftmaxandL
1
normalizationfunctionsσ (·),σ (·),tabulardatainthesourcedomainD ={(xs,ys)} ,
Softmax L1 s i i i
currenttabularbatchinthetargetdomain{xt}N
i i=1
2: Parameters:Smoothingfactorα,Low/highquantilesq /q
low high
3: p s(y), T ←(cid:0)(cid:80) i|D =s 1|1 [j=y is]/|D s|(cid:1)C j=1, 3max jp s(y) j/2min jp s(y) j
4:
if{xt}N
isthefirsttestbatchthen
i i=1
5:
poe(y)←(cid:0) 1/C(cid:1)C
▷Initializeonlinetargetlabelestimator
t j=1
6: endif
7: foru=1toDdo
8: st
u
←(cid:0) xt iu−((cid:80)| iD ′=s 1|xs i′/|D s|) u(cid:1)N
i=1
▷Calculateshiftinformationofu-thcolumn
9: endfor
10: fori=1toN do
11: p t(y|xt i)←σ Softmax(cid:0) F(xt i|θ)(cid:1)
11 32 :: p
T
id te ←(y| Gx (cid:0)t i) F←
(xt
iσ |L θ)1 ,(cid:0) sp tt |( ϕy (cid:1)|xt i)/p s(y)(cid:1) ▷D▷ eP ter re mdi ic nt ed pe eb ri -a ss ae md pta lerg te et mla pb ee ral te us rt eim ofat xo
t
ir
14: j∗, j∗∗ ←argmax 1≤j≤Cp t(y|xt i) j, argmax 1≤j≤C,j̸=j∗p t(y|xt i)
j
15: ϵ i ←1/(cid:0) σ Softmax(cid:0) F(xt i|θ)/T i(cid:1) j∗ −σ Softmax(cid:0) F(xt i|θ)/T i(cid:1) j∗∗(cid:1) ▷Defineuncertaintyofxt i
16: endfor
17: p t(y)=(1−α)·(cid:80)N i=1pd te(y|xt i)/N +α·po te(y) ▷Estimatecurrenttargetlabeldistribution
18: fori=1toN do
19: ifϵ i ≤Q(cid:0) {ϵ i′}N i′=1,q low(cid:1) then
20: c i ←T
21: elseifQ(cid:0) {ϵ i′}N i′=1,q low(cid:1) <ϵ i <Q(cid:0) {ϵ i′}N i′=1,q high(cid:1) then
22: c i ←1 ▷Measuretemperaturec iusinguncertaintyϵ i
23: else
24: c i ←1/T
25: endif
26: p t(y|xt i)′ ←σ Softmax(cid:0) c i·F(xt i|θ)(cid:1) ▷Adjustoriginalprobabilitywithc i
27: pˆ i(y)←p t(y|xt i)′/2+σ L1(cid:0) p t(y|xt i)′p t(y)/p s(y)(cid:1) /2 ▷Performself-ensembling
28: endfor
29: po te(y)←(1−α)·(cid:80)N i=1pˆ i(y)/N +α·po te(y) ▷Updateonlinetargetlabelestimator
30: Output:Finalpredictionsof{pˆ i(y)}N
i=1
B.3 RELIABILITYDIAGRAMS
Wealsoprovidefurtherreliabilitydiagramsacrosssixdifferentdatasetsandthreerepresentative
deeptabularlearningarchitectures,toshowthattabulardataoftenexhibitsbothoverconfidentand
under-confidentpatternscomparedtoconsistentoverconfidentbehaviorinimagedomain(Stylianou
&Flournoy,2002),andunder-confidentbehavioringraphdomain(Wangetal.,2021c).Incasesof
overconfidentconfidenceisevident,whereasinHELOC,(CMC,TabNet),and(DNA,TabNet),under-
confidentconfidenceisobserved.Thisunderscoresthenecessityforthepropositionofuncertainty
calibratorstailoredspecificallytothetabulardomain.
C DETAILED ALGORITHM OF ADAPTABLE
TheoverallprocedureoftheproposedAdapTableisminutelysummarizedinAlgorithm1.Givenapre-
trainedtabularclassifierF(·|θ) : RD → RC onthesourcedomainD = {(xs,ys)} ,andcurrent
s i i i
tabular batch in the target domain
{xt}N
, we first post-train shift-aware uncertainty calibrator
i i=1
G(·,·|ϕ)byoptimizingϕusingtrainingsetagainaftertrainingF withlossfunction
L=L +λ L .
FL CAL CAL
16UnderreviewasaconferencepaperatICLR2024
Figure6:Computationalefficiencycomparisonbetweentest-timeadaptationbaselinesandAdapT-
able.WereporttheaverageadaptationtimecalculatedoveralltestinstancesinCMCdatasetcorrupted
byGaussiannoise.
Here,wecalculateshiftinformationofu-thcolumnfor1≤u≤Das
ss =(cid:0) xs
−(| (cid:88)Ds|
xs/|D |) (cid:1)N ,
u iu i′ s u i=1
i′=1
withλ =0.1denotestheweightofregularizationloss.Giventheoriginalprobabilityp (y|xs)=
CAL s i
σ (cid:0) F(xs|θ)(cid:1) ,andcalibratedpredictionprobabilityp =σ (cid:0) F(xs|θ)/T (cid:1) withper-sample
Softmax i i Softmax i i
tempperatureT =G(cid:0) F(xs|θ),ss|ϕ(cid:1) ,j∗ =argmax p ,andj∗∗ =argmax p ,
i i 1≤j≤C ij 1≤j≤C,j̸=j∗ ij
thefocallossL (Linetal.,2017a)andtheregularizationlossL (Wangetal.,2021c)aredefined
FL CAL
asfollows:
C
L (xs,ys)=(cid:88) 1 (cid:0) 1−p (cid:1)γ log(cid:0) p (cid:1) ,
FL i i [j=ys] ij ij
i
j=1
L (xs,ys)=1 (cid:0) 1−p +p (cid:1) +1 (cid:0) p −p (cid:1) .
CAL i i [j∗=ys] ij∗ ij∗∗ [j∗̸=ys] ij∗ ij∗∗
i i
Here,L alienatesp andp forcorrectlypredictedsampleswhereasitattractsthemforthe
CAL ij∗ ij∗∗
otherones,andγ =2.Foreachepoch,weapplycosineannealingscheduler.
Afterpost-trainingG,wecalculateshiftinformationofu-thcolumnfor1≤u≤Dasst =(cid:0) xt −
u iu
((cid:80)|Ds|xs/|D |) (cid:1)N .Then,wepredictper-sampletemperatureofxt asT =G(cid:0) F(xt|θ),st|ϕ(cid:1)
i′=1 i′ s u i=1 i i i
anddefineuncertaintyϵ ofxtasthereciprocalofthemarginofthecalibratedprobabilitylikebelow:
i i
ϵ =1/(cid:0) σ (cid:0) F(xt|θ)/T (cid:1) −σ (cid:0) F(xt|θ)/T (cid:1) (cid:1) .
i Softmax i i j∗ Softmax i i j∗∗
Withpreviouslycalculatedonlinetargetlabelestimator,wepredictthedebiasedtargetlabelestimator
pde(y|xt)andestimatethecurrenttargetlabeldistributionp (y)with
t i t
(cid:17)
pde(y|xt)=σ ‘ig(p (y|xt)/p (y)
t i L1 t i s
N
(cid:88)
p (y)=(1−α)· pde(y|xt)/N +α·poe(y).
t t i t
i=1
Afterthat,wequantilerelativeuncertaintyϵ among{ϵ }N withincurrentbatch,andperform
i i′ i′=1
temperaturesharpeningforcertainsamples,whereasweperformtemperaturesmoothingforuncertain
sampleswithp (y|xt)′ =σ (cid:0) c ·F(xt|θ)(cid:1) ,wherec isdefinedasinEquation1.UsingBayes’
t i Softmax i i i
theorem, we adjust the predicted probability of each instance x as σ (cid:0) p (y|xt)′p (y)/p (y)(cid:1) ,
i L1 t i t s
andusingself-ensembling(Gaoetal.,2023),wegetthefinalpredictionofpˆ(y)=p (y|xt)′/2+
i t i
σ (cid:0) p (y|xt)′p (y)/p (y)(cid:1) /2,andupdateonlinetargetlabelestimatorasfollows:
L1 t i t s
pˆ(y)=p (y|xt)′/2+σ (cid:0) p (y|xt)′p (y)/p (y)(cid:1) /2
i t i L1 t i t s
N
(cid:88)
poe ←(1−α)· pˆ(y)/N +α·poe.
t i t
i=1
17UnderreviewasaconferencepaperatICLR2024
Table5:Post-trainingtimeanalysisofuncertaintycalibratorinAdapTableunderdifferentscales,
encompassingsmall-scale(MLP+CMC),medium-scale(FT-Transformer+HELOC),andlarge-scale
(TabNet+DIABETESREADMISSION).
Setting TrainingTimeofUncertaintyCalibrator(sec)
CMC,MLP 4.46
HELOC,FT-Transformer 9.27
DIABETESREADMISSION,TabNet 281.16
Figure7:HyperparametersensitivityanalysisoftheproposedAdapTableusingMLPunderHELOC
datasetwithrespecttosmoothingfactorα,lowuncertaintyquantileq ,andhighuncertaintyquantile
low
q .
high
Figure 8: Hyperparameter sensitivity analysis of TENT (Wang et al., 2021a) using MLP under
HELOCdatasetwithrespecttolearningrateγ,numberofadaptationstepsn.
Figure9:Jensen-ShannonDivergencebetweenestimatedtargetlabeldistributionbeforeandafter
applyinglabeldistributionhandleruponANESandDIABETESREADMISSIONdataset.
D FURTHER EXPERIMENTS
D.1 COMPUTATIONALEFFICIENCY
InordertoshowtheefficiencyoftheproposedAdapTable,weperformacomputationalefficiency
comparisonbetweentest-timeadaptationbaselinesandAdapTableinFigure6.Theaveragedadapta-
18UnderreviewasaconferencepaperatICLR2024
Table6:Numberoftotalcolumns,numberofnumericalandcategoricalcolumnsalongwiththeir
classesperdatasetsused.
Property HELOC ANES DIABETESREADMISSION CMC MFEAT-PIXEL DNA
#Columns 22 54 46 9 240 180
#Numerical 20 8 12 2 240 0
#Categorical 2 46 34 7 0 180
#Classes 2 2 2 3 10 3
tiontimeiscalculatedbyaveragingadaptationtimeoveralltestinstancesinCMCdatasetcorrupted
by Gaussian noise. We find that the adaptation time of AdapTable ranks third among eight TTA
methods,byshowcasingitscomputationaltractability.Furthermore,weobservethatourapproach
requiressignificantlylessadaptationtimecomparedtoTTAbaselinessuchasTTT++(Liuetal.,
2021), SAM (Foret et al., 2021), EATA (Niu et al., 2022), and SAR (Niu et al., 2023), despite
constructing shift-aware graph and incorporating a single forward pass for GNN with negligible
extracostofadjustingoutputlabelestimationarerequired.Thiscanbeattributedtothefactthat
thegraphwegenerateplaceseachcolumnasanode,resultinginagraphofaverysmallscale–
typicallyrangingfromtenstohundredsofnodes.Thisminimizesthecostofmessagepassingin
GNNforwardprocess,whileotherbaselinesiteratethroughmultipleadaptationstepswithforward
andbackwardprocesses,leadingtoincreasedcomputationalexpenses.Furthermore,wealsoprovide
GNNpost-trainingtimeofAdapTableunderdifferentscales,encompassingsmall-scale(CMC,MLP),
medium-scale(HELOC,FT-Transformer),andlarge-scale(DIABETES READMISSION,TabNet).
GNNpost-trainingrequiresonlyafewsecondsforsmall-andmedium-scalesettings,andnotably,it
remainsnegligible,eveninourlargestexperimentalsetting.
D.2 HYPERPARAMETERSENSITIVITY
ToassesstherobustnessofAdapTableagainsthyperparameterconfigurations,weconductanex-
haustivehyperparametersensitivityanalysiscoveringalltest-timeparameters,includingthesmooth-
ing factor α, low uncertainty quantile q , and high uncertainty quantile q . Specifically, in
low high
our experiments utilizing MLP on HELOC dataset, we perform hyperparameter optimization
by varying one parameter while keeping the others fixed at the identified optimal setting, i.e.,
(α,q ,q )=(0.1,0.25,0.75).Notably,asFigure7exhibits,ourfindingsrevealthattheadapta-
low high
tionperformanceremainsinsensitivetoalterationsinallthreetypesofhyperparameters,particularly
whenvaryingq ,demonstratingminimalperformancefluctuations.Furthermore,forthesmoothing
low
factorαandhighquantileq ,wepinpointsweetspotsat[0,0.2]and[0.5,0.6],respectively.This
high
observationunderscorestheadaptabilityofourapproach,allowingflexiblehyperparameterselection
anddemonstratinggeneralizabilityacrossdiversetestconditions.Thisstandsinstarkcontrasttothe
hyperparametersensitivityexhibitedbythetent,asdepictedinFigure8.Notably,regardlessofan
extensivehyperparametersearchinthetabulardomainforthetent,theperformancepost-adaptation
failstosurpasstheunadaptedperformance,asevidencedbyourmaintableexperimentresultsin
Table2andTable3.
D.3 EFFICACYOFLABELDISTRIBUTIONHANDLER
Figure9reportstheJensen-ShannonDivergence(JSDivergence)ofeachtestbatchbetweenground
truthlabeldistributionandprediction,comparingbeforeandafteradaptationusinglabeldistribution
handler for ANES (Studies, 2022) and DIABETES READMISSION (Clore et al., 2014) datasets,
furtherfromSection4.3.Thefigureconsistentlyexhibitsadecreaseindivergenceafteradaptation,
whichsolidifiestheefficacyofthelabeldistributionhandler3.3.
E DATASET DETAILS
E.1 DATASETS
Inourexperiment,weverifyourmethodacrosssixdifferentdatasets.Amongthem,threedatasets
(HELOC,ANES,andDIABETESREADMISSION)includenaturaldistributionshiftsbetweentraining
19UnderreviewasaconferencepaperatICLR2024
andtestdata,whiletheotherones(CMC,MFEAT-PIXEL,andDNA)doesnothavesuchshifts,
andthuswesyntheticallyinjectnoises(SectionE.2)onthemtomimicplausibledistributionshift
scenarios. In our experiments, each dataset is partitioned as follows: 60% for training, 20% for
validation,and20%fortesting.Foralldatasets,thenumericalfeaturesarenormalized–subtraction
ofmeananddivisionbystandarddeviation,whilecategoricalfeaturesareone-hotencoded.Wefind
thatdifferentencodingtypesdonotplayasignificantroleintermsofaccuracy,asnotedin(Grinsztajn
etal.,2022).DetailedspecificationsofeachdatasetarelistedinTable6
• HELOC:HomeEquityLineofCredit(HELOC)(Brownetal.,2018)datasetisthedataset
topredictwhethertheapplicantwillrepaytheirHELOCaccountwithintwoyears;whichis
alineofcredittypicallyofferedbyabankasapercentageofhomeequity.Dataissplitwith
respecttoexternalriskestimationvalue;loweronesareusedfortestdata.
• ANES:AmericanNationalElectionStudies(ANES)(Studies,2022)provideclassification
task of U.S. presidential election participation. Domain shift is given by the geographic
regionofsurveyees.
• DIABETESREADMISSION:DiabetesReadmission(Cloreetal.,2014)representstenyears
(1999-2008)ofclinicalcareat130UShospitalsandintegrateddeliverynetworks.Eachrow
concernshospitalrecordsofpatientsdiagnosedwithdiabetes,whounderwentlaboratory,
medications,andstayedupto14days.Thegoalistodeterminetheearlyreadmissionofthe
patientwithin30daysofdischarge.Admissionsourcesaredifferentbetweentrainandtest
data.
• CMC:ContraceptiveMethodChoice(CMC)isasubsetofContraceptivePrevalenceSurvey
conductedinIndonesia.Thegoalistopredictthecurrentcontraceptivemethodchoice–
betweenno-use,long-termmethods,orshort-termmethods,withrespecttothewoman’s
demographicandsocio-economiccharacteristics.Thetrainandtestdataweresplitwith
respecttothemostimportantcolumn’svalues.Sinceitcontainsbothnumericalandcategor-
icalfeatures,weobtainedtwodifferentsplitsbyselectingonemostimportantcolumnfrom
thenumericalcolumns,andonefromthecategoricalcolumns.
• MFEAT-PIXEL:MultipleFeaturesDataset–Pixel(MFEAT-PIXEL)isahandwrittendigit
recognitiondataset.ItsgoalistoclassifyhandwrittennumeralsextractedfromDutchutility
maps.Theinputisdigitized,andallthepixelvaluesareinbinaryform,0correspondingto
blackand1correspondingtowhite.Thetrainandtestdataweresplitwithrespecttothe
mostimportantcolumn’svalues.
• DNA:PrimateSplice-JunctionGeneSequencesconsistofsplicejunctionofDNA,described
by180indicatorvariables.Thegoalistorecognizethe3classes–1.boundariesbetween
exons(retainedaftersplicing),2.introns(removedaftersplicing),or3.noneoftheabove.
ThedatasetstemsfromIrvinedatabase,butwithmajordifferencesincludingtheprocessing
ofsymbolicvariablesrepresentingthenucleotides,andthenamesofeachexample.The
trainandtestdataweresplitwithrespecttothemostimportantcolumn’svalues.
E.2 SYNTHETICCORRUPTIONS
Letx=[x ,··· ,x ]beasingletablerowwithDcolumns,whereµ andσ denotethemeanand
1 D i i
thestandarddeviationoftheempiricalmarginaldistributionofthei-thcolumncalculatedbythe
trainingset.Weinjectfoursyntheticcorruptionstomimicaleatoricuncertainty,andtwonatural-shift
orientedsyntheticshiftstomimicnaturaldistributionshifts.
• Gaussian Noise: For each column x , we add a Gaussian noise ϵ with x ← x +ϵ·σ
i i i i
independently,whereϵ∼N(0,0.12).
• Uniform Noise: For each column x , we add a uniform noise ϵ with x ← x +ϵ·σ
i i i i
independently,whereϵ∼U(−0.1,0.1).
• RandomMissing:Foreachcolumnx ,wemaskandreplaceitbyusingarandommask
i
m and a random sample x¯ with x ← (1−m )·x +m ·x¯ , where m ∼ Ber(0.2),
i i i i i i i i
P(x¯ = k) = (cid:80)ns 1 /n fork ∈ R.n isthenumberoftraininstances,andxs
i i=j [xs j,i=k] s s j,i
denotesthei-thcolumnofthej-thtrainsample.Weassumethatwehaveknowledgeof
whichcolumnsaremissing.
20UnderreviewasaconferencepaperatICLR2024
• RandomColumnMissing:Thisissimilartotherandommissingcorruption,exceptforthe
factthatalltestinstancesacrossmultiplebatcheshavethesamecommoncolumnsmissing.
Foreachcolumnx ,wemaskandreplaceitwitharandomsampleusingarandommask
i
m and a random sample x¯ with x ← (1−m )·x +m ·x¯ , where m ∼ Ber(0.2),
i i i i i i i i
P(x¯ = k) = (cid:80)ns 1 /n fork ∈ R.n isthenumberoftraininstances,andxs
i i=j [xs j,i=k] s s j,i
denotesthei-thcolumnofthej-thtrainsample.Wealsoassumethatwehaveknowledgeof
whichcolumnsaremissing.
• Numerical Column Shift: This shift mimics natural domain shifts, we extract the most
importantnumericalcolumnbasedonpre-trainedXGBoost(Chen&Guestrin,2016)and
sortallinstancesinthedatasetaccordingtothemostimportantnumericalcolumn,andthe
top80%ofthedataispredominantlyallocatedtothetrainingandvalidationsets,whilethe
lower20%isprimarilyassignedtothetestset.
• Categorical Column Shift: This shift also mimics natural domain shifts, we extract the
mostimportantcategoricalcolumnbasedonpre-trainedXGBoost(Chen&Guestrin,2016)
andsplitthetraintestdatasetaccordingly.Instancesbelongingtothecategorythatismost
frequentlyrepresentedwithinthetop80%arepredominantlyassignedtothetrainingand
validationsets.Conversely,instancesassociatedwiththecategorythathastheleastfrequent
occurrenceswithinthelower20%aremainlyallocatedtothetestset.
F BASELINE DETAILS
F.1 DEEPTABULARLEARNINGARCHITECTURES
• MLP: Multi-layer perceptron (MLP) (Murtagh, 1991): is a foundational deep learning
architecture characterized by multiple layers of interconnected nodes, where each node
applies a non-linear activation function to a weighted sum of its inputs. In the tabular
domain,MLPisoftenemployedasadefaultdeeplearningmodel,witheachinputfeature
correspondingtoanodeintheinputlayer.
• TabNet:TabNet(Arik&Pfister,2021)introducesauniqueblendofdecisiontreesandneural
networks.Itutilizesanattentionmechanismtoselectivelyfocusoninformativefeaturesat
eachdecisionstep,makingitparticularlywell-suitedforhandlingtabulardatawithamixof
categoricalandcontinuousfeatures.
• FT-Transformer:FT-Transformer(Gorishniyetal.,2021),shortforfeaturetokenizeralong
withTransformer(Vaswanietal.,2017),representsastraightforwardmodificationofthe
Transformer architecture tailored for tabular data. In this model, the feature tokenizer
componentplaysacrucialrolebyconvertingallfeatures,whethercategoricalornumerical,
intotokens.Subsequently,aseriesofTransformerlayersareappliedtothesetokenswithin
theTransformercomponent,alongwiththeadded[CLS]token.Theultimaterepresentation
ofthe[CLS]tokeninthefinalTransformerlayeristhenutilizedfortheprediction.
F.2 SUPERVISEDBASELINES
• K-NN:k-NearestNeighbors(k-NN)isawidelyusedmodelintabularlearning,thatmeasures
distancebetweendatapointsusingachosenmetrictoidentifyitsk-nearestneighbors,and
makes predictions through majority voting for classification, or weighted averaging for
regression.kisauser-definedhyperparameter,influencingthesensitivityofthemodel.
• LR: Logistic Regression (LR) is a linear classification algorithm for tabular data that
models the probability of an instance belonging to a particular class. Using a logistic
functiontosquashthelinearcombinationofinputfeaturesintoarangeof[0,1].Withthe
appropriate regularization techniques, it has shown its capability to be comparable with
SOTAarchitecturesinthetabulardomain.
• RF: Random Forest (RF) is an ensemble learning (bagging) algorithm that constructs
multipledecisiontreestoenhanceaccuracyandmitigateoverfitting.Itexcelsinhandling
non-linearpatterns,providinghighaccuracyandrobustnessagainstoutliers.
• XGBOOST:ExtremeGradientBoosting(XGBoost)(Chen&Guestrin,2016)isanensemble
learning(boosting)algorithmbuildingasequenceofweaklearners,usuallydecisiontrees,
21UnderreviewasaconferencepaperatICLR2024
Table7:Hyperparametersearchspaceofsupervisedbaselines.#Neighborsdenotesthenumberof
neighbors,#Estimdenotesthenumberofestimators,Depthdenotesthemaximumdepth,andLR
denotesthelearningrate,respectively.
Baseline SearchSpace
K-NN #Neighbors:{2-12}
RF #Estim:{50-200},Depth:{2-12}
XGBOOST #Estim:{50-200},Depth:{2-12},LR:{0.01-1},Gamma:{0-0.5}
CATBOOST #Estim:{50-200},Depth:{5-40}
tocorrecterrorsofthepreviousmodel.Itstandsoutforitshighpredictiveperformance,
abilitytohandlecomplexrelationshipsandregularizationfeatures.
• CATBOOST:CatBoost(Dorogushetal.,2017),similartoXGBoost,isaboostingensem-
blealgorithm.Itefficientlyhandlescategoricalfeatureswithoutextensivepre-processing,
makingitadvantageousforreal-worlddatasets.Itsbenefitsincludehighperformance,butit
comesatacomputationalcost.Additionally,parametertuningmaybenecessaryforoptimal
results.
F.3 TEST-TIMEADAPTATIONBASELINES
• PL:Pseudo-labeling(PL)(Lee,2013)usesapseudo-labelingstrategytoupdatethemodel
weightsduringtest-time.
• TTT++: Test-time training (TTT++) (Liu et al., 2021) tries to mitigate deterioration of
test-time adaptation performance through feature alignment strategies, regularizing the
adaptation,withouttheneedtore-accesssourcedata.
• TENT:Testentropyminimization(TENT)(Wangetal.,2021a)updatesthescaleandbias
parameterswithinthebatchnormalizationlayerwithentropyminimizationduringtest-time,
withagiventestbatch.
• SAM:Sharpness-awareminimization(SAM)(Foretetal.,2021)althoughnotamethod
devisedfortest-timeadaptation,hasshownitseffectivenesscombinedwithTENTthrough
updatingparametersthatlieinneighborhoodshavinguniformlylowloss.
• EATA:Efficientanti-forgettingtest-timeadaptation(EATA)(Niuetal.,2022)pointsout
that samples with high entropy may lead to unreliable gradients that disrupt the model.
EATAfiltersthesehigh-entropysamplesalongwithutilizingafisherregularizertoconstrain
importantmodelparametersduringadaptation.
• SAR:Sharpness-awareandreliableoptimization(SAR)(Niuetal.,2023)improvesupon
SAM,armedwiththeobservation–sampleswithlargeentropyleadstomodelcollapse
duringtest-time,andfiltersthesamplesforadaptationwithapre-definedthreshold.
• LAME:Laplacianadjustedmaximum-likelihoodestimation(LAME)(Boudiafetal.,2022)
isanewapproachtowardstest-timeadaptation,adaptingwithoutparameteroptimization,
butonlycorrectstheoutputprobabilitiesofaclassifierratherthantweakingthemodel’s
innerparameters.
G HYPERPARAMETER DETAILS
G.1 SUPERVISEDBASELINES
Fork-nearestneighbors(K-NN),logisticregression(LR),randomforest(RF),XGBOOST(Chen&
Guestrin,2016),andCATBOOST(Dorogushetal.,2017),optimalparametersaresearchedforeach
datasetsusingrandomsearchof100iterations,foreachdataset.Thesearchspaceforeachmethodis
specifiedinTable7.
22UnderreviewasaconferencepaperatICLR2024
Table8:Hyperparametersearchspaceoftest-timeadaptationbaselines.Here,weonlydenotethe
commonhyperparameters,wheremethodspecifichyperparametersarespecifiedinSectionG.2.
Hyperparameter SearchSpace
LearningRate {1e-3,1e-4,1e-5,1e-6}
AdaptationSteps {1,5,10,15,20}
Episodic {True,False}
Table9:Selectedhyperparametersoftest-timeadaptationbaselines.Inthistableweonlydenotethe
commonhyperparameters,wheremethodspecifichyperparametersarespecifiedintext.
Baseline LearningRate AdaptationSteps Episodic
PL 1e-4 1 True
TTT++ 1e-5 10 True
TENT 1e-4 1 True
SAM 1e-3 1 True
EATA 1e-5 10 True
SAR 1e-3 1 True
LAME N/A N/A N/A
G.2 TEST-TIMEADAPTATIONBASELINES
Entropyminimization-basedmethods,namelyTENTWangetal.(2021a),SAMForetetal.(2021),
andSARNiuetal.(2023),require2mainhyperparameters–learningrate,numberofadaptationsteps
perbatch,andwhethertoresetthemodelafterbatch(i.e.,episodicadaptation).Additionally,SARNiu
etal.(2023)requiresathresholdhyperparametertofiltersampleswithhighentropy.ForTENT,we
setthelearningrateas0.0001with1adaptationstepandepisodicupdate.ForSAMForetetal.(2021)
andSARNiuetal.(2023),thelearningrateis0.001with1adaptationstepandepisodicupdate.For
PLLee(2013),wesetthelearningrateas0.0001with1adaptationstepandepisodicupdates.For
TTT++(Liuetal.,2021),EATA(Niuetal.,2022)andLAME(Boudiafetal.,2022),wefindthatthe
author’shyperparameterchoicesareoptimal,asspecifiedintheirpaperandofficialcode,exceptfor
theirlearningrateandadaptationsteps.ForTTT++(Liuetal.,2021)andEATA(Niuetal.,2022),the
learningrateissetto0.00001with10adaptationstepsperbatchandepisodicupdates.LAMEBoudiaf
etal.(2022)onlycorrectstheoutputlogits,thusnotrequiringhyperparametersrelatedtogradient
updates. For all previous baselines, we find that their hyperparameter choice did not vary across
differentarchitectures,namelyMLP,TabNet(Arik&Pfister,2021)andFT-Transformer(Gorishniy
et al., 2021). As noted in the main paper, all hyperparameters for the corresponding method and
backbonearchitecturepairaretunedwithrespecttonumericalshiftonCMCdatasetfromOpenML-
CC18(Bischletal.,2021).Anoverviewofthehyperparametersearchspace,alongwithselected
hyperparametersofeachmethodisprovidedinTable8andTable9,respectively.
G.3 ADAPTABLE
AdapTablerequiresthreekeytest-timehyperparameters:smoothingfactorα,andlow/highuncertainty
quantilesq /q .TheparametersforeachbackbonearchitecturearedescribedinTable10.
low high
H LIMITATIONS AND BROADER IMPACTS
H.1 LIMITATIONS
Similartoothertest-timetraining(TTT)methods,AdapTableincorporatesanadditionaltraining
procedureduringthesourcemodel’strainingphase.Thiscontrastswithfullytest-timeadaptation
methods,whichrefrainfrommakingassumptionsduringtest-timeexecution.Specifically,AdapTable
necessitatesanextrapost-trainingstepforshift-awareuncertaintycalibratortoadjustthemodel’s
predictions.Whilefullytest-timeadaptationmethods,suchasSAR(Niuetal.,2023),areapplicable,
23UnderreviewasaconferencepaperatICLR2024
Table10:SelectedhyperparametersofAdapTable.Threemajorhyperparameters–smoothingfactor
α,lowquantileq ,highquantileq arespecifiedbelowperarchitecture.Thehyperparameters
low high
werefixedthroughoutdatasets.
Architecture α q q
low high
MLP 0.1 0.25 0.75
TabNet 0.0 0.25 0.9
FT-Transformer 0.0 0.25 0.9
theirperformanceimprovementsinthetabulardomainarelimited,oftenfailingtoaddresscertainco-
variate/labelshifts,asevidencedinourevaluations.AdapTabledemonstratessubstantialperformance
gainsinthemajorityofevaluationscenarios,althoughoccasionalshortcomingspersistinspecific
datasetsandmodelspecifications.
H.2 BROADERIMPACTS
Our research represents an initial effort to address the challenge of domain shift, a significant
impedimentinthepracticaldeploymentofmachinelearningmodelsfortabulardata.Despitethe
prevalenceoftabulardatainindustrialapplications,investigationsintodomainadaptationspecific
tothisdatatypehavebeencomparativelylimited,especiallywhencontrastedwithdomainssuchas
computervision,naturallanguageprocessing,andspeechprocessing.
Through comprehensive examinations, we have identified that the straightforward application of
TTAmethodologiesfromotherdomains,particularlythoserelyingonentropyminimization,which
currentlyconstitutesthemostprevalentformofTTA,encounterssubstantialchallengesinthecontext
oftabulardata.Notably,thesechallengesarisefromthehighuncertaintyofpredictionentropyfor
tabulardata,potentiallyleadingtomodelcollapse,asexemplifiedinSAR(Niuetal.,2023),and
theinadequacyoftheclusterassumptionwithinthelatentspaceofmodelstrainedontabulardata.
Moreover,mostTTAmethodologiesaretailoredexclusivelytodeeplearningmodels,anassumption
oftenoverlookedindomainswheredeeplearningmodelshavesurpassedtraditionalmachinelearning
approaches.However,thisassumptioncannotbedismissedinthetabulardomain,whereclassical
machinelearningmethods,suchasdecisiontrees,formacompetitivebaseline.
Incontrast,AdapTable,byrefiningonlytheoutputprobabilitiesandcircumventingnoisybackpropa-
gationfromhigh-entropy-pronedata,avoidsfailure.Additionally,ournovelshift-awareuncertainty
calibratorleveragestheheterogeneouscharacteristicsofcolumns,enablingourmethodtoeffectively
addressdomainshift.Wepositthatourworkempowersfutureresearcherstoadeptlyconfrontthe
crucialchallengeofmitigatingdomainshiftintabulardata–anarenawheretheapplicationofprior
methodsfromotherdomainsisnotstraightforwardduetotheaforementionedissues.
24UnderreviewasaconferencepaperatICLR2024
Figure10:Entropydistributionhistogramsfortestinstancesacrosssixdifferentdatasetsandthree
representativedeeptabularlearningarchitectures.
25UnderreviewasaconferencepaperatICLR2024
Figure11:Latentspacevisualizationsfortestinstancesusingt-SNEacrosssixdifferentdatasetsand
threerepresentativedeeptabularlearningarchitectures.
26UnderreviewasaconferencepaperatICLR2024
Figure12:Reliabilitydiagramsfortestinstancesacrosssixdifferentdatasetsandthreerepresentative
deeptabularlearningarchitectures.
27