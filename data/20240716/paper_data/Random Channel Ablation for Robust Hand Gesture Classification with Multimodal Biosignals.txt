Random Channel Ablation for Robust Hand Gesture
Classification with Multimodal Biosignals
Keshav Bimbraw∗†, Jing Liu†, Ye Wang†, and Toshiaki Koike-Akino†
∗Worcester Polytechnic Institute, Worcester, MA 01605, USA
†Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA
kbimbraw@wpi.edu, {jiliu, yewang, koike}@merl.com
Abstract—Biosignal-based hand gesture classification is an improve the gesture classification performance compared to
important component of effective human-machine interaction. usingjustonemodality[15].Weexploreusingultrasoundand
For multimodal biosignal sensing, the modalities often face data FMG in a multimodal fashion for gesture recognition in this
lossduetomissingchannelsinthedatawhichcanadverselyaffect
paper.
the gesture classification performance. To make the classifiers
robusttomissingchannelsinthedata,thispaperproposesusing With the advancements in machine learning, there is a
Random Channel Ablation (RChA) during the training process. growing interest in using biosignals for hand gesture clas-
Ultrasound and force myography (FMG) data were acquired sification not just in academia but also in industry [16].
from the forearm for 12 hand gestures over 2 subjects. The
The robustness of the machine learning models is of prime
resulting multimodal data had 16 total channels, 8 for each
importance for effective hand gesture classification. Though
modality. The proposed method was applied to convolutional
neuralnetworkarchitecture,andcomparedwithbaseline,impu- combiningmultiplemodalitiesandmultiplechannelscanoften
tation, and oracle methods. Using 5-fold cross-validation for the leadtomoreaccuratepredictions,italsoincreasesthefragility
two subjects, on average, 12.2% and 24.5% improvement was of the system, as in practice there are often missing channel
observed for gesture classification with up to 4 and 8 missing
and missing modality issues due to the unreliability of the
channels respectively compared to the baseline. Notably, the
sensors and communications, as well as partial occlusions
proposed method is also robust to an increase in the number
of missing channels compared to other methods. These results and other disruptions. A common practice is to impute the
show the efficacy of using random channel ablation to improve missingvalueswithzerosorthemean[17].However,suchad-
classifierrobustnessformultimodalandmulti-channelbiosignal- hoc approaches have no robustness guarantee and still suffer
based hand gesture classification.
fromsignificantperformancedegradationwhenthenumberof
IndexTerms—BiomedicalSignalProcessing,BiomedicalImag-
missing channels increases. In this paper, we aim to improve
ing, Image Processing, Biomedical Sensors, Deep Learning, Ro-
bustness, Multimodal, Multi-Channel, Ultrasound, FMG the robustness of multimodal and multi-channel signal pro-
cessing applied to biosignal based hand gesture classification.
I. INTRODUCTION WeleveragetheRandomizedAblationtechnique[18]fromthe
Adversarial Machine Learning literature, which was proposed
Hand gesture recognition using biosignals has been a fo-
todefendagainstsparseadversarialattacks,wheresomepixels
cus of research for designing pipelines for effective human-
of an RGB image are adversarially perturbed.
machine interfaces. For this, several biosignal modalities have
In this paper, we generalize Randomized Ablation to multi-
been explored such as surface electromyography (sEMG) [1],
channel and multimodal settings, which we call Random
ultrasound[2],forcemyography(FMG)[3],photoplethysmog-
Channel Ablation (RChA). We apply this to biosignal pro-
raphy (PPG) [4], mechanomyography [5], inertial measure-
cessing, with a focus on robust hand gesture prediction in the
ment unit (IMU) [6], and electrical impedance tomography
presence of missing channels. We use multimodal ultrasound
(EIT) [7]. Forearm ultrasound can help with visualization
and FMG data for the classification of 12 hand gestures with
of the musculature which can be used to estimate hand
a convolutional neural network (CNN). Section II describes
gestures [8]. Recent research has shown that it can be used to
the data acquisition, model, gesture classification workflow,
estimate different hand configurations [9], finer finger move-
and random channel ablation technique. Section III describes
ments [2] and forces [10]. With FMG, piezoelectric sensors
theexperimentalsetupfordataacquisition,processing,experi-
around the forearm are used to acquire pressure information
mental design, and evaluation metrics. Finally, the results and
corresponding to muscle contraction, from which hand ges-
discussion sections describe the obtained results and major
turescanbeinferred.Haetal.showedanaverageclassification
takeaways in addition to future research directions stemming
accuracy of over 80% for estimating hand gestures and pros-
from this work.
thetic hand control [11]. Multimodal biosignal-based gesture
classification has been explored primarily with sEMG [12], in
II. METHODSANDEXPERIMENTALDESIGN
addition to modalities like ultrasound [13], FMG [14] among
Ultrasound and FMG data were simultaneously acquired
others. Multimodal data based systems have been shown to
from 2 subjects. The data acquisition procedure and the 12
∗ThisworkwasperformedwhileK.B.wasaninternatMERL. hand gestures were explained to the subjects. The study
4202
luJ
51
]CH.sc[
1v47801.7042:viXraFig. 1. Data acquisition: (a) Hand gesture; (b) Linear ultrasound probe
strappedtotheforearmusingacustom-designedattachment;(c)BioXFMG
armbandstrappedtotheforearm.
was approved by the institutional research ethics committee
at Mitsubishi Electric Research Laboratories (IRB reference
number 23001), and written informed consent was given by
the subjects before the data acquisition.
A. Data Acquisition
The ultrasound data was acquired using a Sonostar 4L
linear palm Doppler ultrasound probe. A custom-designed
3D-printed wearable was strapped onto the subject’s forearm.
The data from the probe was streamed to a Windows system
over Wi-Fi, and screenshots of the ultrasound images were
captured using a custom Python script. The 4L linear probe
has 80 channels of ultrasound data, and the post-processed
beamformed B-mode data is obtained, from which 350×350-
Fig. 2. The model architecture to classify hand gestures from multimodal
pixel image data is acquired. The FMG data was acquired
data.
using a BioX AAL-Band 2.0. The data were streamed to
the Windows system over Bluetooth, and saved along with
the ultrasound frames using the Python script to ensure that
to capture hierarchical features through convolutional and
the ultrasound and FMG data were synchronized. The FMG
pooling operations. The network has 4 cascaded convolution
band gives 8 channels of FMG data. Fig. 1 shows the data
layers followed by batch normalization and pooling layers
acquisition from the ultrasound and FMG.
compared to the referenced architecture which has 5 cascaded
Thedatawereacquiredfor12handgesturesselectedbased
convolution layers and a different number of parameters per
on activities of daily living, as described in detail in [19].
layerbecauseofdifferentinputdimensions.The2-dimensional
These hand gestures consist of 4 finger flexions, 4 pinch
convolution layers have 16 channels as the output channels,
configurations, hand wolf, fist, hook, and open hand gestures.
and a kernel size of (3, 3), with a stride of 1. These are
100framesofdatawereacquiredwhilethesubjecthadafixed
followed by a fully connected linear layer which flattens
hand gesture Per subject, 24,000 frames of data were used for
out the output from the fourth max-pooling layer. After
the study which resulted from 20 sessions of data acquisition
another batch normalization, a dropout operation is applied
(20sessions,12handgestures,100framespergesture:24,000
with dropout probability of 0.5. This is followed by a final
frames). The average frame rate of data acquisition was 18
fullyconnectedlayerwithasoftmaxactivationwhichleadsto
Hz. The experiments were run on a desktop with an Intel i7-
the probabilities of the 12 hand gestures based on the input
13700K CPU, 128GB RAM, and an NVIDIA GeForce RTX
data.Eachconvolutionlayerhasarectifiedlinearunit(ReLU)
4090 GPU.
activation. The first fully connected (dense) layer has a ReLU
B. Model Architecture, Parameters and Metrics activation,whiletheoutputlayerhasasoftmaxactivation.The
network is visualized in Fig. 2.
A CNN was used in this paper designed using PyTorch
based on a network used to classify hand gestures from Cross Entropy Loss was used for evaluating the loss and
ultrasound images from the forearm [2], [19]. The network is the Adam optimizer was used for optimizing the network. A
tailoredforimagecompression,anditsarchitectureisdesigned learning rate of 0.0001 and a batch size of 10 were used. TheFig. 3. Data pre-processing and ablation. (a) The 350×350-pixel forearm ultrasound image was acquired using a linear ultrasound probe. (b) 8-channel
FMGdatawassimultaneouslyacquired.(c)Theultrasoundimagewasdownsizedbyafactorof2.Thenewimagedimensionswere175×176.Anadditional
columnofzeroswasaddedtomakethecolumnwidthdivisibleby8.(d)UltrasoundandFMGdatawereindependentlynormalizedandthenappendedper
frame. The FMG data was expanded to span the rows of the ultrasound image. The dimensions of each sample were 175×184 with the last 8 columns
belongingtoFMGandtheremainingtothedownsizedultrasoundimage.(e)Theoutputafterrandomchannelablationwithchannels0,6,8,and15ablated.
First,thenumberofchannelstobeablatedwaschosenrandomlywhichturnedouttobe4.Then,4channelswererandomlychosenandablatedfromthe16
channels.
evaluationwasdoneforthreedifferentrandomseedvalues:0, to the ultrasound data as shown in Fig. 3(d). The dimension
1, and 11, and the results were averaged over these seeds. of this image is 175 × 184 pixels with the last 8 columns
Classification accuracy percentage (Acc) was used as the representing data obtained from FMG and the remaining 176
accuracymetric,whichisthefractionofcorrectclassifications columns belonging to the ultrasound image.
over the total number of classifications multiplied by 100.
5-fold cross-validation was used to assess the robustness
D. Robust Training for Hand Gesture Classification
and generalization performance of our method. The dataset
waspartitionedintofivesubsets,trainingthemodelonfourof It has been previously shown that randomly ablating image
thesesubsets,andevaluatingitsperformanceontheremaining pixels can help make a classifier robust to sparse adversarial
subset. This meant, that per subject and fold, the training perturbations that only corrupt very few number of image
data had 19,200 samples and the testing data had 4,800 pixels [18]. This is because the fraction of corrupted pixels
samples. This process was repeated five times, each time is very small and the vast majority of the pixels are benign.
usingadifferentsubsetforevaluation.Throughouttrainingand There is a high probability that the randomly retained (non-
evaluation,carefulattentionwasgiventomaintainingtemporal ablated) pixels are not corrupted. Further, it is possible to rec-
independence and preventing any overlap between the train ognize images with partial observation (i.e., retained pixels).
and test sets. The results were averaged to provide a more If a classifier is trained deliberately with randomly ablated
reliable estimate of the model’s performance and reduce the information (i.e., partial observation), using the Randomized
impact of variability in a single train-test split. Ablation strategy can be certifiably robust to such sparse
adversarialperturbations.Inourwork,insteadofanadversary,
C. Data Preprocessing
we only consider randomly missing channels, and our robust
Fig. 3 shows the data pre-processing for the ultrasound and classifier is trained on randomly observed/retained channels.
FMG data. The 350×350-pixel ultrasound data and 1×8 Algorithm 1 describes the robust training procedure of
FMG data are obtained per frame. The ultrasound image is the proposed Random Channel Ablation method. The pre-
downsized by a factor of 2. Let I be the original image processed data is first combined for both modalities. The
with dimensions d×d. The downsized image, I downsized, with maximum number of ablated channels K, the batch size M
dimensions d × d, is expressed as follows: for training, and the learning rate η are specified. Each batch
2 2
X ,y in X ,y is then used for training the
I (i,j)=I(2i,2j). (1) batch batch train train
downsized neural network and its parameters θ are then updated. The
Here, i and j are pixel indices in the downsized image, parameters are updated based on the gradient ∇ θ of the loss
and 2i and 2j are the corresponding pixel indices in the function w.r.t. the model parameters θ. The loss function
original image. The downsized image is shown in Fig. 3(c). ℓ(f θ(X ablate),y batch) is based on the loss obtained from the
Thedimensionofthedownsizedultrasoundimageis175×175 predictions f θ(X ablate) and the true labels y batch.
pixels. A zeros column was added to the downsized image to Fig. 3(e) shows the multimodal data sample after randomly
make the columns perfectly divisible by 8, with the updated ablating4channels.First,thenumberofchannelstobeablated
dimensions being 175×176 pixels. was set as 4. Then, 4 channels were randomly chosen and
To have consistency in the FMG and ultrasound data, the ablated from the total 16 channels. For FMG, each column
data from both modalities were independently normalized. corresponds to one channel, and for ultrasound, each channel
Following this, the FMG data was expanded to have the same is 22-pixel wide column. This was done so that the ablation
number of rows as the ultrasound data and then appended was consistent across the modalities.Algorithm 1 Robust training via Random Channel Ablation
TABLEII
of T epochs. The neural network f is parameterized by θ. CLASSIFICATIONACCURACYFORDIFFERENTMETHODSWHENSAMPLES
1: Input X train = append(X us,X fmg), y train, max # of
ARERANDOMLYMISSINGUPTO50%CHANNELS
ablated channels K, batch size M, learning rate η Baseline Imputation Proposed Oracle
2: for t=1,...,T do Subj1 56.1% 75.1% 83.8% 87.3%
Subj2 73.9% 92.2% 95.1% 97.1%
3: for {X batch,y batch} in {X train,y train} do
Average 65.0% 83.7% 89.5% 92.2%
4: X ablate ← ablate(X batch,K)
5: θ =θ−η∇ θℓ(f θ(X ablate),y batch)
6: end for
7: end for
8: return neural network parameters θ
9:
10: Subroutine X ablate ← ablate(X batch,K) :
11: Initialize X ablate =X batch
12: Randomly choose # of ablated channels k ∈[0,...,K]
13: if k > 0, then randomly choose k channels of X ablate
and set them to zero
14: end if
TABLEI
CLASSIFICATIONACCURACYFORDIFFERENTMETHODSWHENSAMPLES
ARERANDOMLYMISSINGUPTO25%CHANNELS
Baseline Imputation Proposed Oracle
Subj1 70.0% 81.9% 86.2% 87.3%
Subj2 87.3% 95.6% 95.5% 97.1%
Average 78.7% 88.8% 90.9% 92.2%
E. Evaluation Fig.4. Classificationaccuracyforafixednumberofchannelsbeingmissing.
Duringtesting,theindexofmissingchannelsischosenuni-
formly at random for each sample. For missing channels, the
Table II shows the gesture classification accuracy for the
proposed method fills zero to the missing channels, and then
Baseline, Imputation, Proposed, and Oracle methods when
applies robust classifier trained in Algorithm 1 for prediction.
the fraction of missing channels is up to 50%. This leads
The proposed method is compared with the Baseline
to a maximum of 8 channels being missing. On average,
method,Imputationmethod,andOracle,wheretheclassifierof
our proposed method’s classification accuracy was 89.5%
these 3 methods is trained on all channels. The Oracle has all
compared to 83.7% for the Imputation method and 65.0% for
channels available during testing as well, and its performance
the Baseline method.
canbeviewedasanupperboundforthemissing-channelcase.
Fig. 4 shows the results where a fixed number of channels
To deal with missing channels at testing, the Baseline method
aremissingpersampleforthedifferentmethods.For4missing
simply fills in with zero values. While for the Imputation
channels, on average, our proposed method’s classification
method, the missing channel is imputed using the overall
accuracy was 90.5% compared to 86.5% for the Imputation
sample mean of that channel from the training samples.
method and 69.8% for the Baseline method. For 8 missing
channels, on average, our proposed method’s classification
III. RESULTS
accuracy was 87.7% compared to 73.0% for the Imputation
Upon averaging the hand gesture classification results over method and 41.6% for the Baseline method. It is also worth
the two subjects, the average Oracle case hand gesture clas- noting that, when there is no missing channel, our robustly
sification accuracy was 92.2%. Table I shows the gesture trained classifier performs almost the same as the Baseline
classificationaccuracyfortheBaseline,Imputation,Proposed, method (which is also the Oracle method since there are no
and Oracle methods when the fraction of missing channels is missing channels).
up to 25%. Since the multimodal data comprises 16 channels,
this leads to a maximum of 4 channels being missing (i.e.,
IV. DISCUSSION
the number of missing channels of each testing sample is In this section, we discuss the effect of random channel ab-
randomly chosen from 1 to 4). On average, our proposed lation,andtheeffectofthenumberofmissingchannelsonthe
method’s classification accuracy was 90.9%, compared to classification performance. We also highlight the advantages
88.8% for the Imputation method and 78.7% for the Baseline of our method in making it robust to missing channels and
method. some future research directions stemming from this work.A. Effect of Random Channel Ablation delivergoodgestureclassificationperformance[20].Formerg-
ing data from multimodal ultrasound and FMG information,
AscanbeseeninbothTablesIandII,theproposedmethod
alternative learning based approaches can be explored such as
outperforms the Baseline and the Imputation methods. For
in [21] and [22]. Within the scope of the system described
a maximum of 4 (25%) channels being missing, there is an
in the paper, these approaches would entail learning a low-
improvementof12.2%comparedtotheBaselinemethod,and
dimensional embedding of the ultrasound data which would
2.1% compared to the Imputation method. The classification
then be fused with the 1D FMG signal.
accuracyis1.3%lowerthantheOraclecase,whichisexpected
Additionally, the current study focused on data acquired
since the proposed method only has partial observation of
from 2 subjects. To ensure the generalizability and applica-
the channels during testing. Similar trends were observed
bility of multimodal and multi-channel biosignal based hand
with a maximum of 8 (50%) channels being missing with an
gesture recognition using our proposed random channel abla-
improvement of 24.5% from the Baseline method and 5.8%
tion approach, future work will focus on acquiring data from
from the Imputation method. The classification accuracy for
multiple subjects. The expanded set of subjects will enable an
the proposed method is only 2.7% less than the Oracle case.
evaluation of the proposed approach for subjects with varying
This shows that random channel ablation can help make the
characteristics, making it more versatile.
classifier robust to missing channels, while still maintaining
very high utility.
V. CONCLUSION
B. Effect of Number of Missing Channels In this study, we introduced our random channel ablation
With a higher number of missing channels in the test set, it technique aimed at enhancing the robustness of deep learning
is expected to see a significant drop in classification accuracy, classifiers used for hand gesture recognition when faced with
as can be seen for the Baseline method in Fig. 4. For 4 missing channels in multimodal and multi-channel biosignal
channels missing, the proposed method’s accuracy is 20.7% data. Our approach demonstrated notable resilience to up
better than the Baseline method and 4.0% better than the to 50% missing channels, surpassing the performance of
Imputation method. For 8 channels missing, the proposed the Baseline and Imputation methods. We observed minimal
method’s accuracy is 46.1% better than the Baseline method degradation in classifier performance compared to the Oracle
and 14.7% better than the Imputation method. Overall, the case. The efficacy of our method unveils promising prospects
drop in the classification accuracy from 0 channels missing for its application in diverse multimodal and multi-channel
to 8 channels missing is a mere 4.1% for the proposed biosignal training scenarios. This technique holds practical
method compared to 19.2% for the Imputation method and implications for real-world data acquisition and deep learning
50.6%fortheBaselinemethod,demonstratingclearlysuperior model performance, particularly in scenarios where certain
performance compared to the other methods. channelsmaybeabsent.Weillustratedthecapabilitytobolster
modelrobustnesswhichcanbeutilizedforseveralapplications
C. A Universal All-in-One Classifier and across several modalities.
Fortheproposedrandomchannelablationmethod,onlyone
VI. ACKNOWLEDGMENTS
universal classifier needs to be trained, and it can be applied
to all situations with up to K out of n channels missing. In TheauthorsaregratefultotheMitsubishiElectricResearch
contrast, one could train a specific classifier for the specific Laboratories(MERL)researchersfortheirinputandfeedback.
missing-channel situation. For example, one could train a
specific classifier for the situation where only the second
REFERENCES
channel is missing. However, this would scale poorly when [1] M.Zheng,M.S.Crouch,andM.S.Eggleston,“Surfaceelectromyog-
the number of total channels n and the maximum number of raphyasanaturalhuman–machineinterface:Areview,”IEEESensors
missing channels K is large, since one would have to train Journal,vol.22,no.10,pp.9198–9214,2022.
(cid:80)K (cid:0)n(cid:1)
classifiers in total, which would not be practical.
[2] K. Bimbraw, C. J. Nycz, M. J. Schueler, Z. Zhang, and H. K. Zhang,
k=0 k “Prediction of metacarpophalangeal joint angles and classification of
hand configurations based on ultrasound imaging of the forearm,” in
D. Future Research 2022 International Conference on Robotics and Automation (ICRA),
2022,pp.91–97.
This work focused on showcasing the effectiveness of our [3] X. Jiang, L.-K. Merhi, and C. Menon, “Force exertion affects grasp
random channel ablation technique for multimodal and multi- classification using force myography,” IEEE Transactions on Human-
MachineSystems,vol.48,no.2,pp.219–226,2018.
channel biosignals through ultrasound and FMG modalities.
[4] D.Li,P.Kang,K.Zhu,J.Li,andP.B.Shull,“Feasibilityofwearable
For future work, we aim to diversify the data acquisition PPGforsimultaneoushandgestureandforcelevelclassification,”IEEE
by incorporating additional modalities, such as surface elec- SensorsJournal,vol.23,no.6,pp.6008–6017,2023.
[5] M.-K. Liu, Y.-T. Lin, Z.-W. Qiu, C.-K. Kuo, and C.-K. Wu, “Hand
tromyography, inertial measurement unit gloves, and cameras,
gesture recognition by a MMG-based wearable device,” IEEE Sensors
among others. Considering the temporal aspects of the input Journal,vol.20,no.24,pp.14703–14712,2020.
datacanbeusedtoimprovetheclassificationofhandgestures, [6] O.Makaussov,M.Krassavin,M.Zhabinets,andS.Fazli,“Alow-cost,
IMU-basedreal-timeondevicegesturerecognitionglove,”in2020IEEE
and it can also help leverage some additional modalities in a
International Conference on Systems, Man, and Cybernetics (SMC),
multimodal setting such as sEMG which has been proven to 2020,pp.3346–3351.[7] B. Ben Atitallah, Z. Hu, D. Bouchaala, M. A. Hussain, A. Ismail,
N. Derbel, and O. Kanoun, “Hand sign recognition system based on
EIT imaging and robust CNN classification,” IEEE Sensors Journal,
vol.22,no.2,pp.1729–1737,2022.
[8] K. Bimbraw, J. Rothenberg, and H. Zhang, “Leveraging ultrasound
sensingforvirtualobjectmanipulationinimmersiveenvironments,”in
2023 IEEE 19th International Conference on Body Sensor Networks
(BSN),2023,pp.1–4.
[9] K. Bimbraw, E. Fox, G. Weinberg, and F. L. Hammond, “Towards
sonomyography-based real-time control of powered prosthesis grasp
synergies,”in202042ndAnnualInternationalConferenceoftheIEEE
EngineeringinMedicine&BiologySociety(EMBC),2020,pp.4753–
4757.
[10] K.BimbrawandH.K.Zhang,“Estimatingforceexertedbythefingers
based on forearm ultrasound,” in 2023 IEEE International Ultrasonics
Symposium(IUS),2023,pp.1–4.
[11] N. Ha, G. P. Withanachchi, and Y. Yihun, “Performance of forearm
FMGforestimatinghandgesturesandprosthetichandcontrol,”Journal
ofBionicEngineering,vol.16,pp.88–98,2019.
[12] Q.Gao,J.Liu,andZ.Ju,“Handgesturerecognitionusingmultimodal
data fusion and multiscale parallel convolutional neural network for
human–robot interaction,” Expert Systems, vol. 38, no. 5, p. e12490,
2021.
[13] S. Wei, Y. Zhang, and H. Liu, “A multimodal multilevel converged
attention network for hand gesture recognition with hybrid sEMG and
A-modeultrasoundsensing,”IEEETransactionsonCybernetics,2022.
[14] S.Jiang,Q.Gao,H.Liu,andP.B.Shull,“Anovel,co-locatedEMG-
FMG-sensingwearablearmbandforhandgesturerecognition,”Sensors
andActuatorsA:Physical,vol.301,p.111738,2020.
[15] M. Abavisani, H. R. V. Joze, and V. M. Patel, “Improving the perfor-
manceofunimodaldynamichand-gesturerecognitionwithmultimodal
training,” in Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,2019,pp.1165–1174.
[16] R. Tchantchane, H. Zhou, S. Zhang, and G. Alici, “A review of hand
gesture recognition systems based on noninvasive wearable sensors,”
AdvancedIntelligentSystems,vol.5,no.10,p.2300207,2023.
[17] A. R. T. Donders, G. J. van der Heijden, T. Stijnen, and K. G.
Moons, “Review: A gentle introduction to imputation of missing
values,” Journal of Clinical Epidemiology, vol. 59, no. 10, pp.
1087–1091, 2006. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0895435606001971
[18] A. Levine and S. Feizi, “Robustness certificates for sparse adversarial
attacksbyrandomizedablation,”inProceedingsoftheAAAIConference
onArtificialIntelligence,vol.34,no.04,2020,pp.4585–4593.
[19] K. Bimbraw, C. J. Nycz, M. Schueler, Z. Zhang, and H. K. Zhang,
“Simultaneousestimationofhandconfigurationsandfingerjointangles
usingforearmultrasound,”IEEETransactionsonMedicalRoboticsand
Bionics,vol.5,no.1,pp.120–132,2023.
[20] M.Sima˜o,P.Neto,andO.Gibaru,“Emg-basedonlineclassificationof
gestures with recurrent neural networks,” Pattern Recognition Letters,
vol.128,pp.45–51,2019.
[21] C.Li,Y.Hou,W.Li,Z.Ding,andP.Wang,“Dfn:Adeepfusionnetwork
forflexiblesingleandmulti-modalactionrecognition,”ExpertSystems
withApplications,vol.245,p.123145,2024.
[22] T.Yang,T.Xu,Y.Cheng,Z.Tang,S.Su,andY.Cao,“Afusionmethod
based on 1d vibration signals and 2d images for detection of railway
surface defects,” in 2023 3rd International Conference on Neural Net-
works,InformationandCommunicationEngineering(NNICE). IEEE,
2023,pp.282–286.