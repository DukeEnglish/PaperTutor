Spider2-V: How Far Are Multimodal Agents From
Automating Data Science and Engineering Workflows?
RuishengCao∗12 FangyuLei1 HaoyuanWu1 JixuanChen1 YeqiaoFu1 HongchengGao1
XinzhuangXiong1 HanchongZhang2 YuchenMao1 WenjingHu1 TianbaoXie1HongshenXu2
DanyangZhang12 SidaWang RuoxiSun3 PengchengYin4 CaimingXiong5 AnsongNi6
QianLiu7 VictorZhong8 LuChen2 KaiYu2 TaoYu1
1TheUniversityofHongKong 2ShanghaiJiaoTongUniversity
3GoogleCloudAIResearch 4GoogleDeepMind 5SalesforceResearch
6YaleUniversity 7SeaAILab 8UniversityofWaterloo
Abstract
Datascienceandengineeringworkflowsoftenspanmultiplestages,fromware-
housingtoorchestration,usingtoolslikeBigQuery,dbt,andAirbyte. Asvision
languagemodels(VLMs)advanceinmultimodalunderstandingandcodegenera-
tion,VLM-basedagentscouldpotentiallyautomatetheseworkflowsbygenerating
SQLqueries,Pythoncode,andGUIoperations. Thisautomationcanimprovethe
productivityofexpertswhiledemocratizingaccesstolarge-scaledataanalysis. In
thispaper,weintroduceSpider2-V,thefirstmultimodalagentbenchmarkfocusing
onprofessionaldatascienceandengineeringworkflows,featuring494real-world
tasksinauthenticcomputerenvironmentsandincorporating20enterprise-level
professionalapplications. Thesetasks,derivedfromreal-worldusecases,evaluate
the ability of a multimodal agent to perform data-related tasks by writing code
andmanagingtheGUIinenterprisedatasoftwaresystems. Tobalancerealistic
simulationwithevaluationsimplicity,wedevotesignificantefforttodeveloping
automaticconfigurationsfortasksetupandcarefullycraftingevaluationmetrics
foreachtask. Furthermore,wesupplementmultimodalagentswithcomprehensive
documentsoftheseenterprisedatasoftwaresystems. Ourempiricalevaluation
revealsthatexistingstate-of-the-artLLM/VLM-basedagentsdonotreliablyauto-
matefulldataworkflows(14.0%success). Evenwithstep-by-stepguidance,these
agentsstillunderperformintasksthatrequirefine-grained,knowledge-intensive
GUIactions(16.2%)andinvolveremotecloud-hostedworkspaces(10.6%). We
hopethatSpider2-Vpavesthewayforautonomousmultimodalagentstotransform
theautomationofdatascienceandengineeringworkflow. Ourcodeanddataare
availableathttps://spider2-v.github.io.
1 Introduction
Data science and engineering pipelines usually rely on professional data software systems such
as BigQuery, dbt, and Airbyte to acquire, process, and orchestrate large-scale data. Utilizing
theseenterprisesystemsinvolveswritingSQLandPythoncode,aswellasfrequentandrepetitive
graphical user interface (GUI) controls, which can be complex even for experienced data scien-
tists and engineers. With rapid advances in large language models (LLMs) and vision language
models(VLMs),LLM/VLM-basedautonomousagentshavethepotentialtoautomatethesework-
∗ WorkdonewhileinterningattheUniversityofHongKong.
Preprint.Underreview.
4202
luJ
51
]IA.sc[
1v65901.7042:viXraTask1:Load data underthecurrentGoogle Drive folder into anewtable“data1”oftheopenedBigQuerydataset.
GUI control
on BigQuery UI
(createnewtable)
GUI control
GUI control on popup panel
on Drive UI (fillininformation)
(copydatalink)
Task2:Savetop 20 dramatic movies since 2000 fromSnowflakedatabaseIMDB intofile"top20movies.csv"onDesktopwith
detailedrequirementsintheopened.txtfile.
GUI control
on Snowflake UI
(createnewworksheet)
GUI control
cross apps
(rename output file)
write SQL
datawarehousing dataingestion datatransformation datavisualization dataorchestration
Figure1: Spider2-Visamultimodalagentbenchmarkspanningacrosscompletedatascienceand
engineeringworkflows(e.g.,twotaskexamplesintheFigureabove). Itinvolvesvariousprofessional
enterprise-levelapplicationsandincludesintensiveGUIcontrolsapartfromcodewritingthroughout
thereal-timemulti-turninteractionwithanexecutablecomputerenvironment.
flows[37,32],enhancingproductivityfordatascientistsandengineers[38,16]whiledemocratizing
accesstolarge-scaledata[15,40].
Previousstudiesondataagentsfocusedmainlyondailylifedataprocessingandanalysisbygenerating
codeorAPIcalls[42,9,4],neglectingothercrucialstagesofdatascienceandengineering(e.g.,data
ingestionandintegration)usingenterpriseapplications(e.g.,Snowflake,Airflow,andDagster).
Additionally,tocompletedataworkflows,datascientistsandengineersoftenneedtonavigatemultiple
professionaldatasystems,combiningcodewritingwithintensiveGUIcontrols,suchasnavigating
webpagesandclickingbuttons[5,45]. However,thereiscurrentlynobenchmarkthatintegratesboth
codegenerationandGUIcontrolsforprofessionaldatascienceandengineering.
Toaddressthisgap,weproposeSpider2-V,thefirstmultimodalagentbenchmarkcoveringtheentire
data science and engineering workflow, involving 494 real-world tasks in a real-time executable
computerenvironmentand20professionalenterprisedatasoftware. Spider2-Vaimstoevaluatea
multimodalagent’sabilitytoperformprofessionaldata-relatedtasksbywritingcodeandmanagingthe
GUIinenterprisedatasoftwaresystems,includingdatawarehousing(e.g.,BigQuery),dataingestion
andintegration(e.g.,Airbyte),datatransformation(e.g.,dbt),dataanalysisandvisualization(e.g.,
Superset),anddataorchestration(e.g.,Dagster).Thesetasksarederivedfromreal-worldpractices,
suchasofficialtutorialsonprofessionalapplicationsandopen-sourcedataengineeringprojects(with
two task examples presented in Figure 1). We also supplement retrieval-augmented agents with
officialdocumentationandtutorialsofthesesoftwaresystemstoassesstheircapabilitytogeneralize
andlearnfromtheseresources.
Each task in Spider2-V is defined within an executable computer environment based on OS-
WORLD [34], which allows multimodal agents to simulate human actions (e.g., typing code or
clickingbuttons)inarealisticsetting. Specifically,amultimodalagentcanobservereal-timeimage-
style screenshots and text-style accessibility tree of professional data applications in the current
workflowandexecuteitspredictedactionsindynamicmulti-roundinteractionwiththecomputer.This
environmentisconnectedtothereal-worldInternet,allowingtheinclusionofprofessionalsoftware
requiringauthenticuseraccounts(e.g.,Snowflake). Toensurereproducibleandreliableexperiments
withthisenterprisedatasoftware, 10authorswithcomputersciencebackgroundsdeveloped170
automatictasksetupconfigurationsand151customizedevaluationmetricsintotal.
Weexperimentwithstate-of-the-artLLMsandVLMsincludingclosed-sourceonesGPT-4series[21],
Gemini-Pro-1.5[26],Claude-3-Opus[2],QWen-Max[3]andopen-sourcerepresentativesMixtral-
28x7B[11]andLlama-3-70B[20]. Performancesrevealthateventhetop-tierVLM(GPT-4V[1])
achievesonly14.0%successrate. Inthemostchallengingsubset,withactionstepsexceeding15,
theperformancedropsto1.2%. Andforthoseopen-sourceLLMs,thesuccessrateislessthan2%.
This indicates that existing LLMs or VLMs are still far away from achieving full data workflow
automation. Evenprovidedwithanoraclestep-by-stepplan,theoverallperformanceonlyincreases
to16.2%. Thisobservationuncoversthepoorcapabilityofactiongrounding(e.g.,identifyingthe
precisecoordinatesofelementsinthecurrentfocusedapplicationwindow)formultimodalagents.
Furthermore,extensiveanalysis(§4.3)onSpider2-Vdemonstratethatthesestrategiesremarkably
promotethefinalperformance,whichincludeenhancingthealignmentbetweendifferentobservation
modalities,introducingfeedbackonactionexecution,integratingretrieveddocumentcontextand
enlargingthehistorytrajectorylength. Thesefindingslaythegroundworkfordevelopingpractical
multimodalagentsthatcanrevolutionizetheautomationofdatascienceandengineeringworkflows.
2 ExecutableComputerEnvironmentofSpider2-V
Inthissection,weintroducethereal-timeexecutablecomputerenvironmentofSpider2-V,whichis
builtuponvirtualmachines(VMs)andadaptedfromOSWORLD[34].
2.1 TaskDefinition
Generally, an autonomous data agent is modeled as a partially observable Markov decision pro-
cess(POMDP).Giventhecurrentobservationo ∈Owhichincludesanaturallanguageinstruction
t
andascreenshot, accessibilitytree(a11ytree), ortheircombination, anagentgeneratesanexe-
cutable action a ∈ A. This action can be clicking on a certain pixel of the screen (CLICK(560,
t
200)),orwritingcodethroughkeyboard(TYPE("ls -lh")). Theexecutionofa resultsinanew
t
state s ∈ S (e.g., the updated computer state) and a new partial observation o ∈ O. The
t+1 t+1
a11ytree is a text-style representation of the desktop environment, which describes the status,
position,andtextcontentofeachelement(e.g.,windows,buttons,andinputboxes). Theinteraction
looprepeatsuntilanactionthatmarkstermination(DONEorFAIL)isgeneratedortheagentreaches
themaxnumberofsteps. SeeApp.Dformoredetailsabouttheobservationspaceandactionspace.
2.2 EnvironmentSetup
emptycloud
database
OR data APIcalls
local cloud uploading
resource
management
(a)FileTransfer (b)ApplicationLaunch (c)RemoteAPICalls
(d)ScriptExecution (e)PlaywrightAutomation
Figure2: Fivecommonoperationstoresettheinitialenvironment.
Toensurethatanagentstartsfromaconsistentinitialstate,weinvokeaseriesoffunctioncallsbased
onapre-storedvirtualmachine(VM)snapshottoresettheenvironment. Thesefunctioncallsvary
among tasks. And we summarize 5 universal categories with their functionalities (see Figure 2),
namely: 1)FileTransfer: transferfilesorprojectarchives(eitherfromlocalorcloudstorage)intothe
VM;2)ApplicationLaunch: opensoftwareonthedesktop,e.g.,VisualStudioCodeandChromium;
3)RemoteAPICalls: invoketool-specificAPIcallsforprofessionalapplications,especiallythose
requiring authentic user accounts, to reset and configure cloud workspaces; 4) Script Execution:
executeashellscriptinVMtosetuptheinitialstate,e.g.,runaDockercontainertostartalocalhost
webserverforSuperset;5)PlaywrightAutomation: runwebbrowsersimulationwithPlaywright,
e.g.,signintoanaccountorclickaspecificbuttonandredirecttothetargetwebpage.
32.3 Task-specificEvaluation
1 manuallytriggerAirflowDAG
Instruction:Help me change the scheduleofthe
Airbyteconnectionto6:00 p.m.every day. now=$(date -u +"%Y-%m-%dT%H:%M:%S.%N%:z")
astro run ${DAG_ID}>/dev/null 2>&1
API: 2 waitforexecution
POST /v1/connections/get
curl–XPOST\
-H "Content-Type:application/json"\
groundtruth http://.../v1/connections/get\
-d'{"connectionId":"xxx-xxx"}'
"scheduleType": "cron",
"scheduleData": { 3 checkrunningstatus/logs
"cron": {
compare_csv "cronE "x 0p r 0e s 1s 8i o *n " * : ?", s t a t u s = $ ( a s t r o -d - -e - sdv t a ar g r-u tin - d dd a$a t{g eDs A $l G {i _ ns I ot D w- } }r \u \ns \
"cronTimeZone": "UTC" | grep -m 1 "manual" \
... ... }} i f [
e
" cs ht
o
a "t Du As G" = $|
{
" Da s Aw u Gk c
_
c I' e D{ }sp sr r"i un ] nt ; s$ t u3 h c} e c' n e)
ed"
fi
(a)File-basedComparison (b)Information-basedValidation (c)Execution-basedVerification
Figure3: Threegenericmethodsfortaskevaluation.
Aftertheinteractionterminates,weonlyhaveaccesstotheopen-endedresultingstateofthecomputer.
Thus,tomeasurewhetherthegoalofeachtaskisaccomplished,wewritetask-specificfunctionsto
retrievethedesiredresultfromtheopen-endedresultingstateandreturnthesuccessflag(0/1). In
total,Spider2-Vcontains170initialstateconfigurationsand151evaluationscripts,respectively. And
weclassifyallevaluationmethodsinto3genericcategories,alsoshowninFigure3:
a) File-basedcomparison: thismethodfindsandcopiesthetargetfilesfromVMtothehost,and
resortstofile-typebasedmetrics(e.g.,.json,.csv,etc.) tocomparethespecifiedaspectofthe
generatedfilewithgroundtruth. Sometimes,thegroundtruthmaybeupdatedovertime. Inthis
case,wewillfetchthelatestlabelsfromtheInternetduringevaluation.
b) Information-based validation: this scheme is usually utilized to extract and check desired
informationfromthecomputer. Forexample,inFigure3(b),wewanttoconfirmwhetherthetime
scheduleofthedatatransportationiscorrectlyconfiguredinAirbyte. WecaninvokeAirbyte
APIstoretrieve,orChromiumPlaywrighttolocatethetargetvalue.
c) Execution-basedverification:toverifywhetheranexpectedgoalisachieved,wemayalsoneedto
firstexecuteacomplicatedShellscriptinthefinalVM.Forexample,inFigure3(c),wemanually
triggerthetargetAirflowDAG2andchecktheeventualstatusthroughrunninglogs.
3 BenchmarkConstruction
Inthissection,weintroducethegeneralannotationpipeline,documentwarehouseconstruction,and
datasetstatisticsforSpider2-V.Forconcreteexamples,refertoApp.F.
3.1 AnnotationPipeline
Toconstructtasksindifferentcategories,wefindthatofficialtutorialsofenterpriseapplicationsserve
asanexcellentstartingpoint. The6-stepannotationpipelineisillustratedinFigure4(a),andwe
elaborateitwithaconcreteandrealexample“OrchestratedbtCorejobswithAirflowandCosmos”3:
1) Collecttutorials: firstly,wefindtutorialsfromofficialwebsitesforeachprofessionaltoolin
Figure5. Intotal,10annotatorscollected217sourceURLs. Notethatthesetutorialsmayutilize
otherprofessionalsoftware,e.g.,MySQL.AllinvolvedprofessionaltoolsarelistedinApp.B.
2) Learntutorials: theannotatorselectsonetutorial,learnsandrealizesitintheVM.Afterthat,
theycansummarizekeyknowledgepointsfromthistutorial. Forexample,inFigure4(b),fivekey
stepsinintegratingadbtprojectintoanAirflowtaskareextracted.
2ADAGinAirflowisdefinedasacollectionoftaskstorun,andDAG_IDisusedtouniquelyidentifyit.
3TheselectedAirflowtutorialURL:https://www.astronomer.io/docs/learn/airflow-dbt
4𝒊).configuretheAstroproject Ia b ws at ntr a tc ot ini tn es grt ar tu e c at ni o exn i: sting dbt
... 𝒊𝒊).prepareadbtproject e sn ev ti ur ponment p t hr a ao s sj k e bc g et r e o nu ` p cj . oa nf T ff h il e ge u- c rs o eh n do n .p e ` c C t oi i un o lt n do t ya o on u A P hi o er s lf t pl g o r mw e es D SB QT L
finish the remaining work? Name the target
1 collecttutorials 𝒊𝒊𝒊).createanAirflowconnection D rA uG n a` tj af 1f 0l :e 0_ 0s h ao .p m_ .d a eg ve` r, y da an yd .schedule it to
𝒊𝒗).writetheAirflowDAG
chosento verboseinstruction:
selectedtutorialfromAirflow 𝒗).unpausetheDAGonWebUI constructtask T Ao i ri fn lt oe wg .r a Lt ee t 'sa n foe lx li os wt i tn hg e sd eb t stp er po sj :ect into
2 learntutorials 1 t. h eC l li ec fk t t ah pe p liV ci as tu ia ol n St mu ed ni uo bC ao rd .e icon on
2 learntutorials(andidentifykeysteps) 2. Click and open the code file with path
3 4 w swI Ba r ereb sws i tiiat t utdnr e peeta sc ,tt e fio: d
d
n un… e
e
v ns… f
f
i ct gp cpr tr ea oats mso iu _s psv n oc1 2 3 r ae . . . e rm ntr s eo t c e si u (b p y l l pnoo e p i t ,s tn e c n se k g(s: ).… … :.… .): ba manuena.s a axos.t lpwt.a iE = o r t l rw$r o uv yh(t s fa d =ir lDa $lu s fl tAt (en u oa aGe u rc i
w
$ls_a c l-{otI i uDorD e et g
t
$= Apo s d Ci j +G gs
a
{.a"_ d Leo
s
D.f%I e I.fYD v rn
k
A-} l ：% #e r
s
G
> m_ u D _c - w/s n A % adh
l
Ih d ieo d G
i
D-|||e T tvp a -% i/_ g
s
}c sggaH nnd s t ttrrw: gua k aaeek %lg l rppl fMl i s
\
'to: si k-"-{r%2 td m$ p -S>s a { rrr.& vt t 1D iuu%1 e A n nnN i:
a
"G t# s% a$ m_ c : i{ ai $m -oz
r
-n nd 3a -m" .o u} }) dnp
f
ow a" 'aul s
l
} )l gae \ " h-lt
o
p\ ili
w
l\dyo a n at $ n
d
ir { Cdi D
a
ng A
L g
g G Ioe _
s
$r I ：b
{
Di } s
d
Dt e
e
A\ r
t
Gv
a
_e
i Il
Do
s
}u t \puts a b ucac " rpndsu
m
oz tr a -si rl -- -ri tp oa -Hd ufr gi - dX f rrf q " aae 'l ed i p Dl P C div {o sa n l oo a "Oo mr"wg c u cw -si ,Sn ifcs l g k- j/ u i e p tr Tt nlop a d n r "raf e :onr f e s f h sorl n awno f / / i -t cjto t dej l l/ - pt h/w mce e- - – T rp eits p n y o: mnih r o p j/ aoo - #.o e/ "np b zj :l :_/ . cr
i
o iz ro pac "dji ew pa "ap as pl :fte lh c d fer io " p lcso b # Aoeatp t is_t:s rtsi8y t fgho0p a lron8r oep/0r. "t wsj/oz ,_saD ccopji "o ooniep pc nn"/ ok nnvc \"re e1t( ,tr c/ "tc)c c ":ioo o c on 5n o nnn i 4t net n 3a nc 2i _ta t ,n tii o e yo .r pnn .s esi V ." \ }:n M ’ g \ e s cx t re a ec r au t tt e` 3 a ` T ` 4 5 p e D. p ` h . . ad j p ga a` e o aI e P eg f N c nsn n r s f. c e kh d e i/ l. o x . ee At s nj e. d t .h s a _ e .c , rl ie f s o d G l r3 f hd e os “ w m fcol oe t on e oC lospe pa gi rte_ _i lp s erb oncwns dl ep wle wtrreh ae e i+l s aido gd Ct tSo t i ip cp hcw e` ” t_ rh pc p w nt oed o so y io w t e na mtd t ln i o r ntg eoe h l t o i. o t h s n eonp tnh af c b casy he vi r rc t tnte el e ol h` c id r e a wa e. o ri t, t ssd A oug h e ese i W nch et rs r e tt y a . f bD cp lb. ip oe ot. D U oa d wT. A I nn ea aG se n s` W .l fd k` e .c: i G` b .a lr l eo Ul .u Ie p d .
5 w er vi at le uatt ia os nk-s fp ue nc ci tf ii oc ns Gro isu _n pd aut ser du =t fh al: se c
cron=“010* * *”
succeed?
verify task list is verify status/schedule
6 cross-validatef oa nil? VM 5exa wc rt il ty e t th ae s kd -b st p ep cr io fj ie cct evas la ut ai ts if oy nu fs ue nr cti in ot ne sntion 4 writeenl va iu rn oc nh mea np tpl si ec ta ut pio fn us nctions d u ss ie mulP al ta eywr ui sg eh rt lot go in
Figure4: GeneralannotationpipelinewithoneselecteddemonstrationfromtheofficialAirflow
tutorial: OrchestratedbtCorejobswithAirflowandCosmos.
3) Writeinstructions: sincethechosentutorialisextremelycomplicated,theannotatorcanselect
a few key points to construct the task instruction. In Figure 4, we only select key steps iv)
andv)towritetwoversionsofinstructions,abstractandverbose,indicatingdifferentlevelsof
proficiency. Notethat,toavoidpotentialdatacontaminationandmakethetaskmorerealistic,
weasktheannotatortointroduceatleasttwomodificationstotherawtutorial. Inthisexample,
wea)replacetheoriginal“my_simple_dbt_project”intoanopen-sourcedbtprojectcalled
“jaffle-shop”4,andb)addoneextrarequirementonthetimeschedule(10:00a.m. daily).
4) Write environment setup functions: the next step is to write initialization functions using
operationsdefinedin§2.2. Intheexampleabove,weneedto: a)UploadanunfinishedAirflow
projectintotheVM.b)ExecuteaShellscripttolaunchthewebserver(viaDockercontainers)for
Airflowundertheprojectfolder. c)Openallrelevantapplicationsonthedesktoptosimulate
realuserscenarios. d)UsePlaywrighttoauto-logintothedefaultAirflowaccount.
5) Writetask-specificevaluationfunctions: Inthisstep,annotatorsarerequiredtoprogrammati-
callyobtainresultsfromtheopen-endedstatesofVMandassesswhetherthetaskiscompleted
using methods in § 2.3. In this example, the evaluator contains: a) manually run the target
AirflowDAGandverifythefinalstatusis“success”;b)usingAirflowCLIstoretrievedetails
ofthetargetAirflowDAG,andcomparedbtsub-tasks,statusandschedulewithgroundtruth.
6) Cross-validate on VM: to ensure correctness, we go through strict cross-validation. Each
annotated task is sent to two other annotators to check: a) whether the chosen task reflects a
real-worldusecase;b)whetherverboseinstructionaccuratelyfulfillsthetaskanditsrequirements
intheabstractinstruction;c)whethertheenvironmentcanberesettothesamestateindifferent
trials; d) whether the evaluation is robust when we exactly follow the verbose instruction or
modifysomeinconsequentialsteps;e)whethertheevaluationscoreis0ifwedeliberatelymake
somemistakes(red-teaming). Thetaskispreservedonlyifitwithstandsallthesetests.
Onaverage,theannotationofonetask(includingcross-validation)costsroughly4hours.
3.2 DocumentWarehouse
Evenseniordatascientistsqueryofficialdocumentationofprofessionalapplicationswhencompleting
acomplicateddataengineeringtask. Tocompensateforthedeficienciesofthedataagentsinutilizing
enterpriseprofessionalsoftware(e.g.,unawareofcodingspecificationsorAPIs),webuildadocument
warehouseforSpider2-V.Concretely,werecursivelycrawlthewebpagesfromtherootwebsitesof
theprofessionalapplicationsinFigure5. Afterpre-processingthroughheuristics(refertoApp.C),
4URLofopen-sourcedbtproject“jaffle-shop”:https://github.com/dbt-labs/jaffle-shop
5raw HTML web pages are convert into 3 different formats for retrieval, namely a) pure text, b)
markdown,and3)simplifiedHTML.Eventually,weobtain11,231documentsintotal.
3.3 DatasetStatistics
Table1: StatisticsofSpider2-V.
Statistics Number
TotalTasks 494(100%)
-PureCLI 28(5.7%)
-PureGUI 186(37.7%)
-CLI+GUI 280(56.7%)
-w.AuthenticUserAccount 170(34.4%)
-w/o.AuthenticUserAccount 324(65.6%)
Level(ActionSteps)
-Easy(≤5) 98(19.8%)
-Medium(6∼15) 310(62.8%)
-Hard(>15) 86(17.4%)
Avg.ActionSteps 4.0/9.6/22.0
Avg.LengthofAbstractInstructions 37.1
Figure5: Taskcategorieswithprofessionaltools. Avg.LengthofVerboseInstructions 191.5
Avg.NumberofUsedAppsPerTask 2.5
0.10
0.025 abstract 0.5
verbose
0.08 0.020 0.4
0.06 0.015 0.3
0.04 0.010 0.2
0.02 0.005 0.1
0.00 0.000 0.0
0 10 20 30 40+ 0 200 400 600+ 0 2 4 6 8
Steps Words Number of Related Apps
Figure6: Distributionofactionsteps,instructionlength,andrelatedapplicationspertask.
Tasks We classify all 494 tasks in Spider2-V into 7 categories and 11 software sub-categories
withmainstatisticsinFigure5andTable1. Specifically,most(280tasks,56.7%)involveCLIand
GUIoperations. And34%examplesrequestregisteringauthenticsoftwareaccounts. Sinceeach
taskisassociatedwithadetailed,step-by-steptutorial(verboseinstruction),theentiretasksetcan
becategorizedintothreedistinctlevelsbasedonthenumberofactionsintheseinstructions. The
proportionofeasy,medium,andhardtasksisapproximately1:2:1. Accordingtotherightmost
distribution depicted in Figure 6, most tasks necessitate the coordinated utilization of multiple
professionalapplications,therebyestablishingSpider2-Vasaparticularlychallengingbenchmark.
Comparison with existing benchmarks In Table 2, we compare Spider2-V with other agent
benchmarks. Spider2-V incorporates generic computer control commands into the field of data
science and engineering and is distinguished by these salient features: 1) a real-time executable
environment. Insteadofprovidingstaticinput-outputpairs,Spider2-Visequippedwithadynamic
computerdesktopsuchthatagentscanproactivelyexploreit;2)multipleenterprisesoftware. We
integrate20professionalapplicationsintothebenchmark,whichincludenotonlytoolsinstalledon
localhostsbutalsocloud-basedenterpriseservices;3)intensiveGUIoperations. Unliketraditional
codingordatasciencedomains,experienceddatascientistsfrequentlymanipulatetheUIsofthose
professional software to simplify the data workflow (e.g., enabling a specific function on the UI
pageorvisualizingthegraphviewofdatainputs). Insummary,Spider2-Vfocusesontheuseof
professionalenterprisesoftwarewithvisualinterfaceinaninteractivecomputerenvironment.
4 ExperimentsandAnalysis
In this section, we introduce the experiment settings, experimental results, and ablation study to
assesstheproficiencyofcurrentLLMorVLMbasedagentsonSpider2-Vbenchmark.
6
ycneuqerF ycneuqerF ycneuqerFTable2: Comparisonwithexistingagentbenchmarks. Columnsincludetheresearchfield(Field),
whetheranexecutableenvironmentisprovided(Exec. Env.?),whetherenterpriseserviceisutilized
(Ent. Serv.?),whetherGUIactionsaresupported(GUISupport?) andsomeotherstatistics.
Exec. Ent. GUI #Apps/ #Exec.-based
Benchmark Field #Tasks
Env? Serv.? Support? Sites Eval.Func.
Spider[42] Text-to-SQL (cid:37) (cid:37) (cid:37) 1 0 1034
DS-1000[15] DataScience (cid:37) (cid:37) (cid:37) 1 0 1000
Arcade[40] DataScience (cid:37) (cid:37) (cid:37) 1 0 1082
MLAgentBench[10] MachineLearning ✓ (cid:37) (cid:37) 4 13 13
SWE-Bench[12] SoftwareEngineering (cid:37) (cid:37) (cid:37) 12 1 2294
Mind2Web[5] Web (cid:37) (cid:37) ✓ 137 0 2000
WEBLINX[19] Web (cid:37) (cid:37) ✓ 155 0 2337
WorkArena[6] Web ✓ ✓ ✓ 1 7 29
AndroidWorld[25] Android ✓ (cid:37) ✓ 20 6 116
WebArena[45] Web ✓ (cid:37) ✓ 5 5 812
OSWorld[34] ComputerControl ✓ (cid:37) ✓ 9 134 369
Spider2-V DataScience&Engineering ✓ ✓ ✓ 20 151 494
w/ComputerControl
4.1 EnvironmentSettings
Agentbaselines Thebaselinemethodincludes3schemesinzero-shotpromptlearning: 1)Set-
of-Mark (SoM, [36]): following OSWORLD [34] and VisualWebArena [14], we adopt heuristic
methodstoretrievecoordinatesofvisibleelementsfroma11ytree(atext-formatobservationtype)
and draw indexed bounding box for these elements on the screenshot. We further insert these
indexesintothepruneda11ytreetoenhancethealignmentbetweenscreenshotanda11ytree. 2)
ExecutionFeedback(EF,[28]): weappendexecutionfeedbackmessagesofthoseactionswhich
failedtobegroundedintheenvironmentduetounexpectederrors. Thetwotechniquesmentioned
aboveareelaboratedinApp.D.3.1. 3)Retrieval-AugmentedGeneration(RAG,[8]): weleverage
thetaskinstructionasthequeryvector, bge-large-en-v1.5[33]astheembeddingmodel, and
LlamaIndex[18]frameworkastheretrievaltogeneratedocumentcontextforeachtaskexample.
Documentsarepre-chunkedintosegmentswithmaximumlength512andtokensoverlappingsize20.
Top4segmentsareselectedasadditionalcontextinthetaskprompt(detailedinApp.G.3).
LLMsandVLMs Weexperimentwithstate-of-the-artLLMsandVLMs,includingopen-source
representativessuchasMixtral-8x7B[11]andLlama-3-70B[20],andclosed-sourceonesincluding
Qwen-Max[3],Gemini-Pro-1.5[26],Claude-3-Opus[2]andGPT[1]families(GPT-4oandGPT-
4V 5). With respect to the two open-source LLMs and QWen-Max, we utilize pure text-format
a11ytree as the observation type on account of their incapability of image processing. For the
remaining4VLMswhichsupportvisioninput,weusealignedtextandimage(thatisSet-of-Mark)as
theobservationtypeinmainexperiments. Unlessotherwisespecified,wesetthetemperatureto0.5
andtop_pto0.9,thehistorytrajectorywindowsizeto3,themaximumlengthofa11ytreeto5000
tokens,andthemaximumoutputtokensto1500ineachturn. Heuristically,werequiretheagentto
completethetaskswithinboth15interactionturnsandonehour,whichsufficesformosttasks6.
4.2 MainResults
In Table 3, we compare performances of different LLMs and VLMs. All results above integrate
techniques of both execution feedback (EF) and retrieval-augmented generation (RAG) in § 4.1.
Accordingly,wecansummarizethat:
1) Existingdataagentsarefarfromsatisfactoryincompletingreal-worlddatascienceand
engineering tasks. Even state-of-the-art VLMs (GPT-4o and GPT-4V) perform terribly on
Spider2-V, achieving at best 14.0% overall success rate. As for their strongest competitors,
5Weutilizetheversiongpt-4o-2024-05-13forGPT-4oandgpt-4-1106-vision-previewforGPT-4V.
6Althoughsometasksrequiremorethan15actions,weencouragethemultimodalagenttopredictmultiple
actionsinoneresponseinordertosavethebudgetinthepromptdesign(seeApp.G.1.2).
7Table3: SuccessratesofbaselineagentsonSpider2-Vgroupedby7taskcategories(seeFigure5),
namelydatawarehousing(ware.),transformation(trans.),ingestion(inges.),visualization(visual.),
orchestration(orche.),traditionaldataprocessing(proc.),andITservicemanagement(manag.). For
thefirstthreeLLMs,sincetheydonotsupportvisualinformation,weonlyutilizethetext-based
a11ytreeastheobservation. FortheremainingfourVLMs,weadoptSet-of-Mark(see§4.1).
SuccessRate(%)
LLM/VLM Observation
ware. trans. inges. visual. orches. proc. serv. Overall
Mixtral-8x7B 1.2 0.0 0.0 0.0 2.6 0.9 0.0 0.8
Llama-3-70B a11ytree 2.4 0.0 0.0 2.5 3.9 2.8 0.0 2.0
Qwen-Max 1.2 0.0 0.0 0.0 2.6 0.0 0.0 0.6
Claude-3-Opus 2.4 2.5 10.4 15.0 11.5 3.8 12.1 8.1
Gemini-Pro-1.5 3.6 2.5 14.6 15.0 10.3 2.8 19.0 9.1
Set-of-Mark
GPT-4o 7.2 7.5 24.0 14.1 19.8 10.1 13.8 13.8
GPT-4V 10.8 10.0 12.0 25.0 18.4 8.5 12.1 14.0
Gemini-Pro-1.5[26]andClaude-3-Opus[2],theyattainworseperformances,evenlessthan10%
percents. Thereisstillampleroomforimprovementinfuturework.
2) Closed-sourcemodelsaremuchmoresuperiorthanopen-sourceones. Forthoseopen-source
LLMs,thesuccessrateisexceedinglylow,withsomecategoriesapproachingzero. Ononehand,
itcanbeattributedtothefactthatclosed-sourceVLMsarepre-trainedandfine-tunedondataof
higherquality. Ontheotherhand,closed-sourceVLMssupportinputswithlongercontextsand
integratebothvisionandtextmodalities(furtheranalyzedin§4.3).
3) Performancesofdataagentsexhibithighvariance,especiallyincategories“dataingestion”
and“datavisualization”. ThemajorityofthesetwopartitionsarepureGUItasks,whichmeans
agentsmostlyinteractwiththeenvironmentthroughtime-dependentGUIoperations. However,a
minorerrorinoneintermediatestepcanbeamplified,resultingintheentiresequenceofactions
beingwasted. Througherroranalysisontrajectories,wediscoverthatonceagentsmispredictthe
coordinatesofthecorrectbutton,theywillopenthewrongwindowandbecometrappedinthe
incorrectarea,unabletoreturn.
4) Across7datacategories,thepartitions“datawarehousing”and“traditionaldataprocessing”
areextremelychallenging. Thereasonsforthisobservationaretwo-fold: a)datawarehousing
tasks mostly involve authentic user accounts (e.g., BigQuery and Snowflake). Compared to
othertaskswhichcanbeaccomplishedinalocalhost,thesedynamicreal-worldscenariosincur
extraburdenondataagents,suchasnetworkconnectiondelayandpop-upwindows. Multimodal
agentsneedtodealwiththeseunexpectedsituationsinreal-timeinteractionwiththecomputer.
b)Asfortraditionaldataprocessing,thebottleneckisthatspreadsheetsinExcelcontainmany
cells,anditisparticularlydifficultfordataagentstoaccuratelylocatethecoordinatesofcells.
Forexample,applyingthesamemathformulatotheentirecolumnrequestsmultimodalagentsto
firstlypinpointtherightcornerofaspecificcell,waitforthemousetobecomeacross,pressand
dragthemousetowardsthetargetcell. Thisseriesofactionsrequirespreciseandfine-grained
GUIcontrolswhicharedifficulttoimplement.
4.3 Analysis
Inthissection,wedelveintodifferentfactorswhichinfluencetheeventualsuccessrates,andanalyze
theunderlyinglogics. ThefollowinganalysesarebasedonouragentbaselinewithVLMGPT-4o
unlessotherwisespecified. Firstly,wesplittheoverallresultsintodifferentsubsetsinTable4.
1) Taskswithmoreinherentactionstepsaremoredifficult. Eachtaskisassociatedwithone
verbosetaskinstructionwhichgivesastep-by-stepguidanceonhowtocompleteit. Wecountthe
numberofactionsintheverboseinstructionandsplittheentiretasksetinto3difficultylevels:
≤5steps(Easy),5∼15steps(Medium),and>15steps(Hard). Notsurprisingly,asthenumber
ofintrinsicactionstepsincreases,theaverageperformancedecreasessignificantly. Andforthose
extremelytoughtasks,existingVLM-baseddataagentscanhardlyaccomplishthegoal.
8Table4: SuccessrateofGPT-4owithagentbase-Table5:Ablationstudyonactionspace,observa-
lineSoM+EF+RAGacrossdifferentpartitions. tiontypesand3tricksin§4.1onatasksubset.
TaskSplits Ratio(%) SR(%) Action Observation
SR(%)
Easy 19.8 38.8 Space Types
Medium 62.8 9.7 JSONdict 4.2
Hard 17.4 1.2 pyautogui screenshot 4.2
w/oaccount 66.0 15.6 JSONdict 10.5
a11ytree
w/account 34.0 10.6 pyautogui 12.6
CLI 5.7 7.1 screenshot+a11ytree 11.4
GUI 37.7 20.1 w/Set-of-Mark 15.6
CIL+GUI 56.7 10.6 pyautogui w/exec.feedback 13.6
Abstract 50 11.3 w/retrievalaug. 14.4
Verbose 50 16.2 w/alltricks 16.3
2) Tasksinvolvingauthenticuseraccountsaremuchmorechallenging. Onesalientfeatureof
Spider2-Vistheintegrationofprofessionalapplicationsthatrequireauthenticuseraccounts. We
alsosplittheentiretasksetaccordingly(w/oorw/account). Notably,dataagentsstruggleto
completetasksinvolvingauthenticuseraccounts(10.6%successrate). Thesetasksdealwith
real-world scenarios and incorporate cloud-hosted enterprise services. Compared with Web
serverswhicharelaunchedlocallyintheVM(e.g.,fromDockercontainers),thecloudWebUIs
1)generallyintegratemorecomprehensivefunctionalitiesoroptionsintheirmenupanel, and
2)potentiallysufferfromemergencysituation,suchasextendednetworkresponsedelaydueto
bandwidthlimitationorserveroverload. Weconjecturethesetwocausescollectivelycontributeto
theinferiorperformances.
3) IncorporatingGUIoperationstypicallyleadtoimprovedperformances. Wesplitthetaskset
byinterfaces. IfthetaskcanbecompletedwithpureCLIs(e.g.,codeeditororbashterminal),
weclassifyitascli. IfthetaskonlyrequirestheagenttomanipulatetheGUI(usuallyonthe
Webpage),weclassifyitintogui. Fortheremainingcases(cli+gui),anagentmustwritecode
orscripts,andcontroltheUIscreen. Weobservethatpureguitasksaremucheasierthancli
tasks. Thisconclusioncanbeexplainedbythefollowingtworeasons: 1)GUIsofprofessional
applicationsaredesignedtosimplifytheoriginalcodingtask.Clickingbuttonsortypingvalueson
UIscanavoidhandlingtherigorousandcomplexcodingspecification. 2)Bothobservationtypes,
namelythescreenshotanda11ytree,arenaturallyproposedforGUItasks. Forpureclitasks,
dataagentsmustperformextraactionstolocateandswitchtothetargetpanelbeforewritingcode.
4) Providingastep-by-stepguidelineintaskinstructionsresultsinremarkableperformance
gains. Thekeydifferencebetweenabstractandverboseinstructions(thethirdstepin§3.1)is
whethera detailedstep-by-stepguidanceis offered. Withsuchstepwise oracletutorials, data
agentsdonotneedtoreasonandplan,thusdramaticallysimplifyingtheoriginaltask. Andthe4.8
pointsimprovementinTable4consolidatesthishypothesis. Nevertheless,thelowsuccessrate
withverboseinstructions(16.2%)indicatesthatcurrentVLMsstillyieldunsatisfactoryresults
when purely grounding actions in real-world contexts. And significant potential remains for
furtherenhancement.
InTable5,weanalyzetheinfluenceofdifferentcombinationsofactionspace,observationtypes,
andthe3techniquesdescribed§4.1. Thefindingsinclude: 1)Regardingactionspace,pyautogui
code slightly outperforms self-customized JSON dict (12.6% v.s. 10.5%). This can be at-
tributed to the advantage that agents can also generate functional Python code like file traversal
apart from the limited GUI control operations using the first action space. And it improves the
efficiency of action grounding. 2) As for observation types, single screenshot leads to very
lowperformances(4.2%)onaccountoftheagent’sfailureinpinpointingconcreteelements.
When inserting a11ytree into the observation which contains precise coordinates, the agent ca-
pability of locating target pixels is remarkably promoted. 3) All 3 tricks we integrate into the
agent baseline (namely SoM, EF and RAG) will boost eventual performances. It is interest-
ing that if we do not adopt Set-of-Mark (that is, enhancing the alignment between two modal-
ities of observations), the result of screenshot+a11ytree is even worse than that using pure
a11ytree. Thisemphasizesthesignificanceofmodalalignmentwhenhandlingstateobservations.
9Amoderatetemperatureandlonger
history window size improve per-
formances. In Figure 7, we inves-
tigate the influences of two hyper-
parameters on a task subset: 1) The
top-ranked performance is achieved
with sampling temperature 0.5. 2)
Withthehistorywindowsizeenlarges, Figure7: Ablationstudyonhyper-parameters.
from 0 (no history, only the current
observation)to3,theperformancein-
creasesstably. However,duetoconstraintsoninputlengthandconsiderationsofcost-effectiveness,
weareunabletoextendthehistorytrajectoriesanyfurther. Thisalsopointsoutthattheinteraction
efficiencyisaseriousissueandpromisingresearchdirection.
5 RelatedWork
Benchmarksfordatascienceandengineering Inthefieldofdatascienceandengineering,several
recentworksproposenovelbenchmarkstoevaluatethecapabilitiesofLLMagentsinmanipulating
Excelspreadsheets[16,4],commondatasciencelibraries(e.g.,SQLandpandas)[42,15,9,40],
machinelearning[10]orsoftwareengineering[16]projects. Theyareusuallyconfinedtoasingle
stagewithintheentiredatapipeline,predominantlydataprocessingandanalysis,thusoverlooking
otherstagessuchasdatawarehousingandorchestrationfromabroaderperspective. Besides,like
othercoding-relateddatasets[38,29,41],theymerelyfocusonthecommandlineinterface,neglecting
thefactthatenterprisesoftwareusuallyhasrichgraphicaluserinterfaces(GUIs). Anddatascientists
oftencombinecodeprogrammingwithintensiveGUIoperationstofulfilladataworkflow. Tothis
end,Spider2-Visproposedasthefirst-of-its-kindmultimodalagentbenchmarkinthefieldofdata
scienceandengineering,whichcoverstheentiredataworkflowandintegratesvisualinterfaces.
Benchmarksformultimodalagents ExistingworksonGUIinteractionmainlyencompassweb
navigation[27,17,39,5,14],mobiledevice[43,44,24,25,30],andcomputerdesktop[34,32,7,13].
Onetrendofrecentadvancedbenchmarksistoprovideanexecutablesimulationenvironment. Multi-
modal agents can explore and interact with this platform through keyboard, mouse, gesture and
touchscreenactionsinamorerealisticandcomplexscenario. However,previousliteraturemostly
focusesondailylifeapplications(e.g.,Webbrowserandcalendar) [35,23]orworkflowsofnon-
specializedbusinesstasks[31]. Fewworks[6,34,31]investigatethecapabilityofmultimodalagents
tomanipulateenterprise-levelsoftware. GUIsofprofessionalapplicationsoftencontainabundant
domain-specific terminologies (e.g., “materialization” in Dagster), which requires multimodal
agentstounderstandthespecializedknowledge. Spider2-Vincorporates20professionaltoolsinto
areal-timecomputerenvironmenttotesttheproficiencyofagentsindatascienceandengineering.
Furthermore,wesupplementalargevolumeofdocumentsforretrievaltocompensatefordeficiencies
ofagentsindomainknowledge.
6 Conclusion
Inthiswork,weproposeSpider2-V,thefirstdatascienceandengineeringbenchmarkwhichintegrates
enterpriseprofessionalapplicationsandsupportsintensiveGUIoperationsbesidescodewritingacross
thefulldatapipeline. Itcontains494tasks,involves20professionaltools,andprovidesareal-time
executable computer environment. The most advanced VLM (GPT-4V) still performs poorly on
Spider2-V(achieving14.0%successrate),renderingitaverychallengingbenchmark. Although
currentmultimodalagentsarestillfarfromautomatingdataworkflows,Spider2-Vpresentsaneasily
accessiblebenchmarkandlaysthefoundationforfutureresearch.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
10technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-
cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,
2024.
[3] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,
GaoLiu,ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,
ChuanqiTan,SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,ShengguangWu,
BenfengXu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,BowenYu,
HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,YichangZhang,ZhenruZhang,
ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwentechnicalreport. arXiv
preprintarXiv:2309.16609,2023.
[4] YibinChen,YifuYuan,ZeyuZhang,YanZheng,JinyiLiu,FeiNi,andJianyeHao. Sheetagent:
Ageneralistagentforspreadsheetreasoningandmanipulationvialargelanguagemodels. arXiv
preprintarXiv:2403.03636,2024.
[5] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamuelStevens,BoshiWang,HuanSun,and
YuSu. Mind2web: Towardsageneralistagentfortheweb. arXivpreprintarXiv:2306.06070,
2023.
[6] AlexandreDrouin,MaximeGasse,MassimoCaccia,IssamHLaradji,ManuelDelVerme,Tom
Marty,LéoBoisvert,MeghThakkar,QuentinCappart,DavidVazquez,etal. Workarena: How
capablearewebagentsatsolvingcommonknowledgeworktasks? InICLR2024Workshopon
LargeLanguageModel(LLM)Agents,2024.
[7] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu,
WeichenZhang,PeiyiWang,XiangwuGuo,etal. Assistgui: Task-orienteddesktopgraphical
userinterfaceautomation. arXivpreprintarXiv:2312.13108,2023.
[8] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,
andHaofenWang. Retrieval-augmentedgenerationforlargelanguagemodels: Asurvey. arXiv
preprintarXiv:2312.10997,2023.
[9] XueyuHu,ZiyuZhao,ShuangWei,ZiweiChai,GuoyinWang,XuwuWang,JingSu,Jingjing
Xu,MingZhu,YaoCheng,etal. Infiagent-dabench: Evaluatingagentsondataanalysistasks.
arXivpreprintarXiv:2401.05507,2024.
[10] QianHuang,JianVora,PercyLiang,andJureLeskovec. Benchmarkinglargelanguagemodels
asairesearchagents. arXivpreprintarXiv:2310.03302,2023.
[11] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
[12] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthik
Narasimhan.Swe-bench:Canlanguagemodelsresolvereal-worldgithubissues? arXivpreprint
arXiv:2310.06770,2023.
[13] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem
Alshikh,andRuslanSalakhutdinov.Omniact:Adatasetandbenchmarkforenablingmultimodal
generalistautonomousagentsfordesktopandweb. arXivpreprintarXiv:2402.17553,2024.
[14] JingYuKoh,RobertLo,LawrenceJang,VikramDuvvur,MingChongLim,Po-YuHuang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena:
Evaluatingmultimodalagentsonrealisticvisualwebtasks. arXivpreprintarXiv:2401.13649,
2024.
[15] YuhangLai,ChengxiLi,YimingWang,TianyiZhang,RuiqiZhong,LukeZettlemoyer,Wen-
tauYih,DanielFried,SidaWang,andTaoYu. Ds-1000: Anaturalandreliablebenchmark
fordatasciencecodegeneration. InInternationalConferenceonMachineLearning, pages
18319–18345.PMLR,2023.
11[16] HongxinLi,JingranSu,YuntaoChen,QingLi,andZHAO-XIANGZHANG. Sheetcopilot:
Bringingsoftwareproductivitytothenextlevelthroughlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,36,2024.
[17] EvanZheranLiu,KelvinGuu,PanupongPasupat,TianlinShi,andPercyLiang. Reinforcement
learningonwebinterfacesusingworkflow-guidedexploration.arXivpreprintarXiv:1802.08802,
2018.
[18] Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.
Accessed: 2024-05-08.
[19] XingHanLù,ZdeneˇkKasner,andSivaReddy. Weblinx: Real-worldwebsitenavigationwith
multi-turndialogue. arXivpreprintarXiv:2402.05930,2024.
[20] MetaAI. IntroducingmetaLlama3: ThemostcapableopenlyavailableLLMtodate,April
2024. URLhttps://ai.meta.com/blog/meta-llama-3/. Accessed: 2024-04-18.
[21] ROpenAI. Gpt-4technicalreport.arxiv2303.08774. ViewinArticle,2:13,2023.
[22] KeunhongPark,UtkarshSinha,JonathanT.Barron,SofienBouaziz,DanBGoldman,StevenM.
Seitz,andRicardoMartin-Brualla. Nerfies: Deformableneuralradiancefields. ICCV,2021.
[23] YujiaQin,ShengdingHu,YankaiLin,WeizeChen,NingDing,GanquCui,ZheniZeng,Yufei
Huang,ChaojunXiao,ChiHan,etal. Toollearningwithfoundationmodels. arXivpreprint
arXiv:2304.08354,2023.
[24] ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap. Android
inthewild: Alarge-scaledatasetforandroiddevicecontrol. arXivpreprintarXiv:2307.10088,
2023.
[25] ChristopherRawles,SarahClinckemaillie,YifanChang,JonathanWaltz,GabrielleLau,Mary-
bethFair,AliceLi,WilliamBishop,WeiLi,FolawiyoCampbell-Ajala,DanielToyama,Robert
Berry,DivyaTyamagundlu,TimothyLillicrap,andOrianaRiva. Androidworld: Adynamic
benchmarkingenvironmentforautonomousagents,2024.
[26] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXiv
preprintarXiv:2403.05530,2024.
[27] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of
bits: Anopen-domainplatformforweb-basedagents. InInternationalConferenceonMachine
Learning,pages3135–3144.PMLR,2017.
[28] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
Reflexion:Languageagentswithverbalreinforcementlearning.AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[29] HongjinSu,ShuyangJiang,YuhangLai,HaoyuanWu,BoaoShi,CheLiu,QianLiu,andTaoYu.
Arks: Activeretrievalinknowledgesoupforcodegeneration. arXivpreprintarXiv:2402.12317,
2024.
[30] JunyangWang,HaiyangXu,JiaboYe,MingYan,WeizhouShen,JiZhang,FeiHuang,and
JitaoSang. Mobile-agent:Autonomousmulti-modalmobiledeviceagentwithvisualperception.
arXivpreprintarXiv:2401.16158,2024.
[31] Michael Wornow, Avanika Narayan, Ben Viggiano, Ishan S Khare, Tathagat Verma, Tibor
Thompson,MiguelAngelFuentesHernandez,SudharsanSundar,ChloeTrujillo,KrrishChawla,
etal. Domultimodalfoundationmodelsunderstandenterpriseworkflows? abenchmarkfor
businessprocessmanagementtasks. arXivpreprintarXiv:2406.13264,2024.
[32] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu
Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with
self-improvement. arXivpreprintarXiv:2402.07456,2024.
12[33] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighoff. C-pack: Packagedresources
toadvancegeneralchineseembedding,2023.
[34] TianbaoXie,DanyangZhang,JixuanChen,XiaochuanLi,SihengZhao,RuishengCao,TohJing
Hua,ZhoujunCheng,DongchanShin,FangyuLei,YitaoLiu,YihengXu,ShuyanZhou,Silvio
Savarese,CaimingXiong,VictorZhong,andTaoYu. Osworld: Benchmarkingmultimodal
agentsforopen-endedtasksinrealcomputerenvironments,2024.
[35] QiantongXu,FengluHong,BoLi,ChangranHu,ZhengyuChen,andJianZhang. Onthetool
manipulationcapabilityofopen-sourcelargelanguagemodels,2023.
[36] JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v.arXivpreprintarXiv:2310.11441,
2023.
[37] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik
Narasimhan,andOfirPress. Swe-agent: Agent-computerinterfacesenableautomatedsoftware
engineering. arXivpreprintarXiv:2405.15793,2024.
[38] JohnYang,AksharaPrabhakar,KarthikNarasimhan,andShunyuYao. Intercode:Standardizing
andbenchmarkinginteractivecodingwithexecutionfeedback. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[39] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan. Webshop: Towardsscalable
real-worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformation
ProcessingSystems,35:20744–20757,2022.
[40] PengchengYin,Wen-DingLi,KefanXiao,AbhishekRao,YemingWen,KensenShi,Joshua
Howland,PaigeBailey,MicheleCatasta,HenrykMichalewski,OleksandrPolozov,andCharles
Sutton. Naturallanguagetocodegenerationininteractivedatasciencenotebooks. InAnna
Rogers, JordanBoyd-Graber, andNaoakiOkazaki, editors, Proceedingsofthe61stAnnual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
126–173, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.
18653/v1/2023.acl-long.9. URLhttps://aclanthology.org/2023.acl-long.9.
[41] TaoYu,RuiZhang,KaiYang,MichihiroYasunaga,DongxuWang,ZifanLi,JamesMa,Irene
Li,QingningYao,ShanelleRoman,ZilinZhang,andDragomirRadev. Spider: Alarge-scale
human-labeleddatasetforcomplexandcross-domainsemanticparsingandtext-to-SQLtask.
InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages3911–3921,Brussels,Belgium,October-November2018.AssociationforComputational
Linguistics. doi: 10.18653/v1/D18-1425.
[42] TaoYu,RuiZhang,KaiYang,MichihiroYasunaga,DongxuWang,ZifanLi,JamesMa,Irene
Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for
complexandcross-domainsemanticparsingandtext-to-sqltask. InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3911–3921,2018.
[43] ChiZhang,ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGang
Yu. Appagent: Multimodal agents as smartphone users. arXiv e-prints, pages arXiv–2312,
2023.
[44] Danyang Zhang, Lu Chen, and Kai Yu. Mobile-env: A universal platform for training and
evaluationofmobileinteraction. arXivpreprintarXiv:2305.08144,2023.
[45] ShuyanZhou,FrankFXu,HaoZhu,XuhuiZhou,RobertLo,AbishekSridhar,XianyiCheng,
YonatanBisk,DanielFried,UriAlon,etal. Webarena:Arealisticwebenvironmentforbuilding
autonomousagents. arXivpreprintarXiv:2307.13854,2023.
13A RelevantURLs
Datawarehousing Helpme …
Upload this GoogleSheet to the
'census' datasets in BigQuery and
name it 'population'.
Dataingestionandintegration actionspace documents
I want to transfer data from Faker to
the target databaseSnowflake. Could
you help me setup the source?
Datatransformation
Separate the logic of model "customers" GUI
out into two staged models,
"stg_customers" and "stg_orders".
Dataanalysisandvisualization
Use dataset "game_sales" to draw a line CLI
chart,whichshould reflect the trend of
the average global sales per year.
interfaces
Dataorchestration
I just built a 3-step Dagster pipeline.
Couldyouscheduleittorunregularly
everyhourtokeepallassetsuptodate?
fulldatapipeline observationspace professionaltools
Figure8: OverviewofSpider2-V,whichincludestaskexamplesacrossthefulldatapipeline,an
executablecomputerenvironment,andadocumentwarehouseforagentretrieval.
GithubRepository Thetaskexamples,environment,documents,codeandexperimentsarepublicly
availableinGithubrepositoryhttps://github.com/xlang-ai/Spider2-VunderApache-2.0
license. Boththeenvironmentandtaskexampleswillbemaintainedbytheauthorscontinuously.
Concretely,theenvironmentcodeisadaptedfrompreviousworkOSWORLD[34],whichisreleased
under Apache-2.0 license. A non-exhaustive list of artifacts (or task examples) used in Spider2-
V includes: 1) SheetCopilot [16] which is released under GPL-3.0 license, 2) WorkArena [6]
which is distributed under Apache-2.0 license, and 3) official tutorials or guides on professional
applications (e.g., dbt, Airflow, Dagster, Superset, etc.). These tutorials are free to use and
publiclyavailable. Forthoseenterpriseapplicationswhichrequirerealaccounts,namelyBigQuery,
Snowflake, dbt-cloud and ServiceNow, we only exploit their sandbox functions or free-trials
withoutintroducinganyextracostorprivacyissues.
ProjectWebsite Wealsobuildaprojectwebsitehttps://spider2-v.github.io/basedon
Nerfies [22] template which is free-to-use and licensed under a Creative Commons Attribution-
ShareAlike4.0InternationalLicense.Onthiswebsite,weprovideahigh-leveloverviewofSpider2-V,
theleaderboardofthebenchmarkandmoreconcretedynamictaskdemonstrations.
Theauthorsdeclarethatthebenchmarkcollectionandusagestrictlyobeytheaforementionedlicenses.
B ChecklistofAllProfessionalSoftwareinSpider2-V
InTable6,welistallprofessionaltoolsincorporatedintheSpider2-Vbenchmark,aswellastheir
categoriesanddescriptions.
C DetailsofDocumentWarehouse
C.1 DocumentWebsitesforProfessionalTools
Table8liststheofficialdocumentationwebsitescorrespondingtodifferentsoftware. Wecrawledonly
theEnglishdocumentationfromeachofficialwebsiteandselecteddocumentsmatchingtheversion
installedinourtestingenvironmentfordownload. WeusedHTTrack7,afreeandeasy-to-useoffline
browserutility,todownloadtheHTMLfilestoalocaldirectory,buildingalldirectoriesrecursively.
7https://www.httrack.com/
14Table6: SummaryofallapplicationsinSpider2-V(label♡meansarealaccountisneeded).
Category Software Description
Fully-managedenterprisedatawarehouseserviceoffered
BigQuery♡ byGoogleCloudPlatform(GCP).Itenablesrapidprocess-
ingandanalysisoflargedatasetsusingSQL-likequeries.
Cloud-baseddatawarehousingandanalyticsplatformfor
Snowflake♡ large-scaledatastorageandprocessing,providingservices
toload,store,query,andanalyzedatasetsatscale.
High-performanceandscalablerelationaldatabaseman-
MySQL agementsystem(RDBMS)thatiswidelyusedandsuited
DataWarehousing
forfastdataretrieval.
RDBMStostoreandmanagelargeamountsofdatawith
PostgreSQL
extensiveadditionalfeatures.
Self-contained,serverlessRDBMSwithcolumn-storear-
DuckDB
chitectureforfastanalyticalqueries.
AnotherlightweightandserverlessRDBMSthatoptimizes
SQLite
queriesortransactionsonindividualrows.
DataIngestionand Buildconnectionstoextract,transform,andloaddatafrom
Airbyte
Integration multiplesourcestovariousdestinations.
Framework to transform, test, and deploy data in ware-
dbt houses. Withdbt,usersmaydefinedatamodels,transform
Data rawdata,andprovidedataqualitychecks.
Transformation
Cloud-based platform to model, transform and analyze
dbt-cloud♡
datainascalableandcollaborativemanner.
Business intelligence tool to create custom dashboards,
Metabase reports,andanalytics. Itprovidesasimpleandintuitive
interfacetoaskquestionsandcreatevisualizations.
DataAnalysisand
Visualization Enablesuserstomakeinteractivedashboards. Itcancon-
Superset nect to various data sources and create visualizations to
exploreandanalyzethedata.
Platform for building, deploying, and scheduling data
Dagster pipelines. Itintegratesdatafromvarioussourcesandman-
agesdatatransformationjobswithdependencies.
DataOrchestration
Programmaticallyscheduleandmonitorworkflowsinthe
Airflow
formofDirectedAcyclicGraphs(DAGs).
Interactivewebenvironmentforcodeandvisualizations.
JupyterLab Itdealswithnotebookscontaininglivecodeandnarrative
text.
TraditionalData
Processing Spreadsheetsoftwarethatallowsuserstocreateandedit
data in tables, charts, and formulas. We use the open-
Excel
sourceLibreOfficeCalcinsteadofMicrosoftExcelinour
environment.
Cloud-based IT service management platform that pro-
ITService vides a suite of tools and features to streamline inci-
ServiceNow♡
Management dentmanagement,servicecatalog,assetmanagement,and
workflowautomation.
DailyApplications Docker,Chromium,VisualStudioCode,BashTerminal
15Wealsoretainedthedirectorystructureofeachwebsite,aswebelievethepathofeachdocument
can,tosomeextent,representthedocument’spurpose. Forexample,theHTMLfilesunderthepath
“docs.getdbt.com/docs/deploy”areaboutdeployingdbtinproductionorstagingenvironments. This
crawlingstepresultedinatotalof21,239HTMLfiles.
C.2 FilteringofHTMLpages
WefurtherfilteredthecrawledHTMLpagesbasedontwocriteria: irrelevantcontenttosoftware
usage and pages containing invalid content. For the former, we mainlyjudged whether the page
containedcontentrelatedtosoftwareusagebasedonitspathandmanuallyconfirmedit. Forexample,
pages under "author" on the website often relate to the website developer or development team
rather than software usage. Additionally, we removed category-type pages that only contained
navigationinformation. Furthermore,wefilteredoutpagesbasedonthenumberoftokensobtained
bywhitespacetokenization. Wemainlyremovedpageswithtokencountslessthan100,aswefound
thatthesepagespredominantlycontainedinvalidinformationsuchasaccessfailures,invalidlinks,
orwebpageredirections. Forexample,theofficialwebsiteofDagstercontainednumerouslinksto
unreleasedversionsofdocuments,allofwhichresultedinaccessfailures. Therefore,afterremoval,
thenumberofvalidpagescorrespondingtoDagsterdecreasedfrom10,065to332. Finally,We
obtained11,231filteredHTMLfiles(seeTable8).
C.3 HTMLPreprocessing
HTMLfilescontainasignificantamountofcontentunrelatedtotheactualcontentofthewebpage,
such as “<script>”, “<style>” tags, tag attributes, and developer comments. These parts may
provideaestheticstothepagebutareirrelevanttothedocument-levelinformation. Additionally,
theyoftenoccupyalargeportionoftheHTMLfile,makingitexcessivelylongforLLMstoinput.
To perform Retrieval-Augmented Generation (RAG) more efficiently and to help models better
understandsoftwaredocumentation,wepreprocessedtheseHTMLfilesinthreeformats: plaintext,
HTML,andMarkdown. ThesethreeformatsofdataandtheoriginalHTMLfileswillbereleasedto
facilitatefutureresearch. ThetokenstatisticsofalldataformatsareshowninTable9. Wedescribe
thepreprocessingdetailsbelow:
PlainText: WeusedBeautifulSoup48 toextractthetextualelementsfromtheHTMLDOM9
treeandconnectedtheseelementsusing“\n”. ThismethodallowsustoobtaintheHTMLcontent
inthesimplestmanner,butlosingthestructuralinformationoftheHTMLmayaffectthemodel’s
understandingofthewebpagecontent.
Simplified HTML: We remove all sub-trees of the HTML DOM which do not contain textual
elements. We also filter out all headers, footers, copyrights, forms, and iFrames. We removed
allHTMLtagattributessincetheymostlydonotcontainactualcontentorsemanticinformation.
Additionally,whenanodeintheHTMLDOMtreehasonlyonechildnode,weremovethatnode
anddirectlyconnectitschildnodetoitsparentnode. Thiseffectivelysimplifiesthestructureand
depthoftheHTML.ThesimplifiedHTMLpreservesboththestructureandcontentinformationof
theoriginalHTMLwithfewertokens.
Markdown: Wefurtherusedthemarkdownify10tooltoconvertthesimplifiedHTMLintoMark-
downformat. Markdownformatusesfewertokenstorepresentstructuralinformationcomparedto
HTML,strikingagoodbalancebetweenHTMLandplaintextformats. Moreover,sincepuretext
includesasubstantialnumberofnewlinecharactersusedtoconcatenatetextelementsandsomeparts
ofthetextcontentinmarkdownfilesaredirectlyconcatenatedwithoutthesenewlines,thisresultsin
asmalleraveragenumberoftokensinmarkdownfilescomparedtothepuretextformat.
Concreteexamplesofthesethreeformatsaredetailedinthetaskprompts(seeApp.G.3). Inour
pilotexperiments(seeTable7),wecomparetheperformancesusingdifferentformatsofretrieved
documentsonasubset(130tasksamples)ofSpider2-V.Andpuretextformatoutperformstheothers.
8https://beautiful-soup-4.readthedocs.io/en/latest/
9TheDocumentObjectModel(DOM)isaninterfacethatrepresentsanHTMLdocumentasatreestructure,
whereeachnodeisanobjectcorrespondingtoapartofthedocument.
10https://github.com/matthewwithanm/python-markdownify
16Table7: PerformanceswithdifferentformatsofretrieveddocumentsonasubsetofSpider2-V.
RAGFormat SuccessRate(%)
PureText 16.92
MarkdownSyntax 15.38
SimplifiedHTML 15.38
Table8: Summaryofsoftwaredocumentation. OrigPageNum: Thenumberofallwebpageswe
crawledfromthedocumentationwebsite. FilteredPageNum: Thenumberofwebpagesobtainedafter
filteringoutirrelevantorinvalidpages.
Software DocumentationWebsite OrigPageNum FilteredPageNum
dbt/dbt-cloud https://docs.getdbt.com/ 1192 1102
https://release-1-7-2.dagster.
Dagster 10065 332
dagster-docs.io/
Airflow https://docs.astronomer.io/ 493 489
https://docs.airbyte.com/
https://airbyte.com/tutorials/
Airbyte 958 859
https://
airbyte-public-api-docs.s3.
us-east-2.amazonaws.com/
rapidoc-api-docs.html
https://superset.apache.org/
Superset 120 68
docs/
https://www.metabase.com/docs/
v0.49/
Metabase 404 384
https://www.metabase.com/learn/
Snowflake https://docs.snowflake.com/en/ 4436 4431
https://cloud.google.com/
Bigquery 1330 1328
bigquery/docs/
https://jupyterlab.readthedocs.
Jupyter 2241 2238
io/en/4.1.x/
Total 21239 11231
D DetailsofExecutableEnvironmentinSpider2-V
Inthissection,webrieflyintroduceOSWORLD[34]andhowweadaptittomeetourrequirements.
D.1 Overview
Spider2-VformalizestheinteractionwithaUbuntudesktopasapartiallyobservableMarkovdecision
process(POMDP)(S,O,A,T,R)withstatespaceS,observationspaceO,actionspaceA,state
transition function T : S ×A → S and reward function R : S ×A → R. Given the current
observationo ∈Ofromthedesktop,theagentneedstopredictactiona ∈Aforthenextstep.
t t+1
Anadmissibleactionincursachangeinthelatentstatespaces ∈S,andtheenvironmentfeedback
t+1
o . Theinteractionlooprepeatsuntilaspecial“DONE”or“FAIL”actionisissued,whereinthetask
t+1
episodeendsandarewardr =R(s )∈{0,1}iscomputed,with1indicatingtasksuccess.
T
The executable computer environment (a Ubuntu operating system) is built upon virtual ma-
chines(VMs). Byusingthe“snapshot”functionalityofVM,thelocalhostenvironmentstatecanbe
17Table9: Averagenumberofpagetokensofdifferentdocumentationformats. WeusedTikToken,a
fastBPEtokenizerforusewithOpenAI’smodels,tocalculatethetokencountforgpt-3.5-turbo.
Software OrigHTML PlainText SimpHTML Markdown
dbt/dbt-cloud 17954 1669 2963 1510
Dagster 131777 2615 4704 2290
Airflow 35011 2007 3885 1829
Airbyte 30124 2448 4328 2329
Superset 10798 1398 2389 1415
Metabase 33523 2288 4690 2333
Snowflake 105155 1750 3342 1595
Bigquery 103748 6245 11777 5718
Jupyter 224153 11240 19917 6743
Total 109119 4273 7789 3212
Agent
onestep
delay
Task
Eval
Metadata
(1)pyautoguicode
Controller
Action
Grounding
ENV Final
Reset State
(2)JSONdict
(a)environment (b)actionspace
Figure9: OverviewoftheexecutableenvironmentofSpider2-Vandtwotypesofactionspace.
completelyrecoveredtoastoredhistorystate. Thissnapshotwithtask-specificsetupfunctions(see
§ 2.2) serve as the initial state s ∈ S for different tasks. And a core controller is responsible
0
for grounding action a (see App. D.2) into the VM desktop and obtaining observations o (see
t t
App.D.3)fromtheresultingstateofVM.Aftertheagentissuesaspecialaction“DONE”or“FAIL”,
the controller will invoke the customized evaluation function for the current task (see § 2.3) and
reportthemetricscore. TheentireprocedureisshowninFigure9(a).
D.2 ActionSpace
ForgenericactionsthatsupportbothCLIandGUI,weintroducetwodifferentactionspaces:
pyautoguicode Thisactionspaceacceptsarbitraryexecutablepythoncode. Particularly, code
snippets that using python library “pyautogui” to control the mouse and keyboard are strongly
recommended. Generally,mouse-basedactions(e.g.,clickandscroll)directlymanipulatetheGUI
screen,whilekeyboard-basedactions(e.g.,typewriteandhotkey)interactwiththeCLIsuchasthe
bashterminalandcodeeditor(e.g.,VisualStudioCode).
18JSONdict Inspiredbythe“pyautogui”library,wesummarize7actionstosimplifytheaction
space. ThissmallsetcancoverallCLIandGUIactionsneededonthedesktop. Foreachaction
anditsparameters,wefurtherencapsulateitintoaJSONdicttorestricttheoutputformat. TheAPI
specificationandusecasesareformallydescribedinpromptmessages(seeApp.G.1.2). Andthe
checklistofall7actionsispresentedinFigure9(b).
D.3 ObservationSpace
<desktop-frame name=“main”coord=“(0,0)”size=“(1920,1080)”>
<applicationname=“Chromium”coord=“(70,64)”size=“(1442,814)”>
<framename=“Graph: file_sizes_job -Chromium”showing=“true”visible=“true”>
<push-buttonname=“Minimize”enabled=“true”>...</push-button>
<push-buttonname=“Maximize”enabled=“true”>...</push-button>
<push-buttonname=“Close” enabled=“true”>...</push-button>
<entryname=“Addressandsearchbar”>http://127.0.0.1:3000/...</entry>
...#otherelements
</frame>
</application>
<applicationname=“gnome-terminal-server”>
<framename=“user@ubuntu: ~/file-ops-and-jobs/”coord=“(70,27)”size=“(1442,851)”>
<fillername=“”coord=“(70,64)”size=“(1442,814)”>
<menuname=“File”coord=“(70,64)”size=“(40,25)”selectable=“true”>...</menu>
<menuname=“Edit”coord=“(110,64)”size=“(43,25)”selectable=“true”>...</menu>
<menuname=“View”coord=“(153,64)”size=“(49,25)”selectable=“true”>...</menu>
<menuname=“Help”coord=“(337,64)”size=“(47,25)”selectable=“true”>...</menu>
...#otherelements
</filler>
</frame>
</application>
...#otherapplicationwindows
</desktop-frame>
(1)screenshot (2)accessibilitytree
Figure10: Twoobservationtypes: screenshotandaccessibilitytree(a11ytree).
Withrespecttoobservations,therearetwowidelyusedalternatives(seeFigure10): 1)image-style
screenshotoftheentiredesktop,and2)text-formataccessibilitytree(a11ytree). Theaccessibility
tree, obtained from the Assistive Technology Service Provider Interface (ATSPI) library 11, is a
text-formatabstractionoftheentirecomputerdesktopwhichdescribesthename,type,status(e.g.,a
menubaris“selectable”),position(e.g.,inFigure10(2),theattributes“coord”and“size”together
definetherectangleposition),andtextcontentembeddedineachelement(e.g.,windows,panels,
buttons,andinputboxes). Weextracta11ytreeusingpythonlibrarypyatspiandconvertitinto
theXMLformat. ItfunctionssimilartoDOM(DocumentObjectModel)treeforwebsites.
D.3.1 Twotricks: Set-of-MarkandExecutionFeedback
. . . . . .
Figure11: Screenshotwithboundingboxes. Figure12: Convertedtableofa11ytree.
Figure13: Illustrationofthealignedobservationtypeset-of-mark(SoM).
Set-of-Mark(SoM) Theoriginaltext-styleaccessibilitytree(a11ytree)andimage-stylescreen-
shotdonotalignwitheachother. Tocompensateforthisdeficiency,wefollowOSWORLD[34]and
WebArena[45]todrawboundingboxesforelementsofinterestinthescreenshotandlabelthese
elementswithnumericindexes. Theaccuratecoordinatesoftheseboundingboxesareextractedfrom
thea11ytree. Furthermore,were-organizethea11ytreeintoatable(eachleafnodeina11ytree
isconvertedintoonerow)andinsertanotherattribute/column“index”foreachnodeinthetree. The
valueofattribute“index”isexactlythenumericlabelofthecorrespondingelementinthescreenshot.
Thealignedscreenshotanda11ytree(a.k.a.,set-of-mark,SoM[36])areillustratedinFigure13.
11https://docs.gtk.org/atspi2/
19ExamplesofExecutionFeedbackMessages
Here are failed actions with their error messages in your last response:
# Action 1
import pyautogui
index_34 = (23, 43)
pyautogui.click(index_343)
# Execution error:
Traceback (most recent call last):
NameError: name 'index_343' is not defined
# Action 2
import pyautogui
import time
pyautogui.write('USE DATABASE IMDB\n\\n')
# Execution error:
File "<string>" line 3
pyautogui.write('USE DATABASE IMDB
^
SyntaxError: unterminated string literal
ExecutionFeedback Wealsoincorporateanothertypeofinformationastheobservation,namely
theexecutionfeedbackofactions(seemessagesabove). Wenoticethat,somepredictedactionsmay
beparsederroneouslyorfailtobeexecuted. Inthiscase,thetwoobservationtypesmentionedbefore
arenotchangedatall. Andtheagentrepeatedlyurgestoconductthesameincorrectaction. Toinform
theagentofexecutionerrors,weincludethisexecutionfeedbackasthethirdobservationtype.
E FormatofTaskExamples
Inthissection,webrieflyintroducetheformatoftaskexamples. FollowingOSWORLD[34],each
taskinstanceisrepresentedasaJSONdictionarywhichcontainsthefollowingfields: (seeFigure14)
• id: globallyuniqueidofthecurrenttaskexample.
• instruction: thetaskinstructionwhichindicatesthetaskgoal.
• source: alistofreferencedtutoriallinkstoconstructthecurrenttask.
• config:alistofdictionarieswhichdefinetheoperationstoinitializeandresetthecomputer
desktop.Eachdictionarycontainsthefunctionname(the“type”key)anditsparameters(the
“parameters”key)indicatingoneenvironmentsetupfunction. Forexample,inFigure14,
wedefine3environmentresetfunctions,namely1)“bigquery_init”toclearthecloud
workspaceofGoogleproject“bigquery-project”,2)“google_chrome_browser”to
launchtheGoogleChromeapplication,and3)“bigquery_login”tosimulatetheGoogle
accountloginoperationwithplaywright.
• related_apps: alistofapplicationnameswhichshouldbeusedinthecurrenttask.
• tags: alistoftagsdenotingdifferentcategories.
• evaluator: adictionarycontaining3fields: func,result,expected. Itdefineshowto
evaluate thefinal results oncetask completion. Concretely, the“func” fielddefines the
nameofourcustomizedfunction(ormetric)whichisusedtocomparethepredictedresult
andtheexpectedgoldenresult. The“result”fielddefineshowtoextractthepredicted
resultfromthefinalenvironmentstates. Andthe“expected”fielddefineshowtoobtain
thegoldenresult. Forexample,inFigure14,weutilizethefunction“compare_csv”to
comparethepredictedfile“/home/user/Downloads/answer.csv”inthevirtualmachine
andthegoldenfile“answer_gold.csv”inlocalhost.
20Figure14: Theformatofasimpletaskexample(.jsonconfigurationfile).
21F TaskExamples
Inthispart,wepresentdiverseexamplesinSpider2-V.
Table10: RealtaskexamplesfromSpider2-V.
Related
Instruction ScreenshotAfterInitialization
App(s)
Ihaveadbtproject"jaffle_shop". Please
integratethisprojectintodagsterandadda
Dagster
dagsterasset"customers"accordingtothe
dbt
schemaprovidedbythefile"~/dbt-dagster-
Chromium
project/jaffle_shop/customers_schema.yml".
VSCode
Materializetheassetintheopeneddagster
UI.
IhavejustuploadeddataaboutAmeraican
babiesintotable‘names_2014‘. Iamcuri-
ousaboutthetopfivenamesforUSbabies
BigQuery
thatwereassignedmaleatbirthinthatyear.
Chromium
Pleasesavethe‘name‘and‘count‘intoan-
othertable‘top5_male_2014‘inthesame
datasetforme.
I have defined an Airflow DAG. Please
Dagster helpmemigrateittoDagsterbasedonthe
Airflow requirements in "README.md". Remem-
MySQL bertolaunchtheDagsterwebserverfrom
Chromium "dagster_migration.py"andstarttheDAG
VSCode schedule. TestthescheduleonDagsterUI
Terminal Launchpadandmakesurethejobcansuc-
ceed.
I want to have a stack bar chart out of
SampleDatabaseinmetabase. Couldyou
helpmevisualizethedataofProductstable
Metabase andsummarizethedataofSumofpriceby
Chromium ProductCategoryandCreatedAt-Quarter.
Thenstackthevisualizedchart. Pleasehelp
me download the visualization as a PNG
file,andrenameitto"stack_chart.png".
IwanttouseLogisticRegressiontopredict
whetherastudentwillbeadmittedtoacol-
lege or not, and have now built the code
Jupyter
framework in this open jupyter notebook.
Chromium
Pleasereadtheframeworkcodeandcom-
pleteallthe#TODOsections. Finally,you
needtorunthecodeandsaveit.
Continuedonnextpage
22Table10–continuedfrompreviouspage
Related
Instruction ScreenshotAfterInitialization
App(s)
Addanewcolumnnamed"Profit"andcal-
Excel culatetheprofitforeachweekbysubtract-
ing"COGS"from"Sales"inthatcolumn.
Help me create a rolling mean line chart
fortable flightsto seethe trend ofthe av-
Superset
erage cost per day. The rolling period
Chromium
shouldbe7andsavethechartasthename
"rolling_mean".
Help me set up the destination of data
Airbyte transfer to a local JSON file in the Air-
Chromium byte local UI. The target file path is /lo-
cal/json_destination.
I’ve created an empty dbt cloud project
named"test_connection". Couldyouhelp
dbt-cloud me set up the connection to a BigQuery
Chromium GCP?Youdon’tneedtoconfiguretherepos-
itoryfortheproject,andthecredentialfile
isprovidedatdesktop.
IhavedefinedtwoDAGstofetchandpro-
cess data from TheCocktailDB. I hope to
Airflow
changethescheduleoftheconsumerDAG
Docker
suchthateachtimetheresultingfilesofthe
VSCode
producerareupdated,theconsumerDAG
Chromium
is triggered. Can you help me with this
data-awarescheduling?
ModifythecurrentDagstermachinelearn-
ingpipelinebyaddingtwofeatures"Age"
Dagster and"Fare"totheLogisticRegressionmodel
Chromium fromthedata(youshouldfillintheNaNval-
VSCode uesbythemeanofthecolumn). Launcha
runofthejob"sklearn_job",andschedule
ittorunateveryhouronweekdays.
Continuedonnextpage
23Table10–continuedfrompreviouspage
Related
Instruction ScreenshotAfterInitialization
App(s)
I heard there are many free to download
datasets on Snowflake marketplace. And
Snowflake I am really curious about worldwide ad-
Chromium dresses. Could you help me get one
database about it? Name it ‘WORLD-
WIDE_ADDRESSES‘.
Go to the hardware store and order 8
ServiceNow "iPad mini" with configuration {’Choose
Chromium thecolour’: ’Purple’,’Choosethestorage’:
’256’}
Load the data from the Google drive Spi-
BigQuery
der002folderintoBigquery’s’data1’table
Chromium
of’information’datasets.
Metabase
Help me finish the metabase login setup
Postgresql
withinformationshowninsetup.json.
Chromium
Ijustbuilta3-stepDagsterpipeline. Now,
Dagster I want to run it regularly to keep all as-
Chromium sets up to date. Name the target job
VSCode ‘hacker_news_pipeline‘andscheduleitto
runeveryhour.
Install dbt-cloud-cli from GitHub and ex-
dbt-cloud tract the binary to the same folder as the
Chromium dbtproject"analytics". Followtheinstruc-
Terminal tion"Step1:Install"specifiedintheopened
accountprofilepage.
24G PromptsforMulti-modalAgents
Multi-modal agent baseline involves complex prompt engineering. The following sections will
introducethesystemprompt,taskprompt,andretrievedcontextaugmentedprompt.
G.1 SystemPrompt
Theentiresystempromptconsistsoftheenvironmentprompt,observationspaceprompt,actionspace
prompt,andgeneraltips. Differentaction/observationtypeshavedifferentprompts. Inthissection,
wewillintroduceeachoneinturnandpresenttheoverallsystempromptatlast.
G.1.1 ObservationSpacePrompt
The four different observation space settings, namely 1) screenshot, 2) a11ytree, 3) screen-
shot+a11ytree,and4)SoM,eachhasadifferentprompt.
ScreenshotSetting
After each action step, you will get an image-style observation,
which is the screenshot of the computer screen. And you need to
(cid:44)→
predict the next action on the computer based on this image.
(cid:44)→
AccessibilityTreeSetting
After each action step, you will get a text-style observation, which
is extracted and pruned from the accessibility tree based on
(cid:44)→
AT-SPI library. The accessibility tree describes the elements
(cid:44)→
(e.g., panels, icons, buttons, frames, windows, applications) on
(cid:44)→
the computer desktop, as well as its embedded text content,
(cid:44)→
status and positions. For simplicity, we prune the original tree
(cid:44)→
and only extract useful information into a tabular format for you.
(cid:44)→
Here is a quick glance on the observation:
(cid:44)→
TAG, NAME, POSITION (top-left x & y), SIZE (width & height), TEXT
menu, Visual Studio Code, (99, 0), (184, 27), ''
push-button, Chromium Web Browser, (0, 33), (70, 64), ''
terminal, Terminal, (70, 74), (1430, 832), '(base)
user@ubuntu:~/projects/$'
(cid:44)→
... more rows ...
, where `TAG` / `NAME` is the element type / name respectively.
`POSITION` and `SIZE` together describe the square position of
(cid:44)→
this element on the computer screen. For example, if you want to
(cid:44)→
click one button, you can click any point in the square area
(cid:44)→
defined by `POSITION` and `SIZE`. Assume that the position of
(cid:44)→
this button is (100, 200), and the size is (40, 40), the CENTER
(cid:44)→
of this button is (120, 220), which is calculated by x = 100 + 40
(cid:44)→
/ 2 = 120, y = 200 + 40 / 2 = 220. `TEXT` refers to the text
(cid:44)→
content embedded in the element, e.g., the bash terminal output
(cid:44)→
or texts in an editable input box.
(cid:44)→
And you will predict the next action of the computer based on the
accessibility tree.
(cid:44)→
25Screenshot+AccessibilityTreeSetting
The observation space is a combination of two sources: 1) image-style
screenshot of the desktop, and 2) text-style accessibility tree
(cid:44)→
derived from AT-SPI library.
(cid:44)→
### Screenshot
After each action step, you will get a image-style observation, which
is the screenshot of the computer screen. And you need to predict
(cid:44)→
the next action on the computer based on this image. You can use
(cid:44)→
this image to locate the elements on the screen or check the
(cid:44)→
status of the computer, especially whether the previous action is
(cid:44)→
successful or not.
(cid:44)→
### Accessibility Tree
The accessibility tree describes the elements (e.g., panels, icons,
buttons, frames, windows, applications) on the computer desktop,
(cid:44)→
as well as its embedded text content, status and positions. For
(cid:44)→
simplicity, we prune the original tree and only extract useful
(cid:44)→
information into a tabular format for you. Here is a quick glance
(cid:44)→
on the observation:
(cid:44)→
TAG, NAME, POSITION (top-left x & y), SIZE (width & height), TEXT
menu, Visual Studio Code, (99, 0), (184, 27), ''
push-button, Chromium Web Browser, (0, 33), (70, 64), ''
terminal, Terminal, (70, 74), (1430, 832), '(base)
user@ubuntu:~/projects/$'
(cid:44)→
... more rows ...
, where `TAG` / `NAME` is the element type / name respectively.
`POSITION` and `SIZE` together describe the square position of
(cid:44)→
this element on the computer screen. For example, if you want to
(cid:44)→
click one button, you can click any point in the square area
(cid:44)→
defined by `POSITION` and `SIZE`. Assume that the position of
(cid:44)→
this button is (100, 200), and the size is (40, 40), the CENTER
(cid:44)→
of this button is (120, 220), which is calculated by x = 100 + 40
(cid:44)→
/ 2 = 120, y = 200 + 40 / 2 = 220. `TEXT` refers to the text
(cid:44)→
content embedded in the element, e.g., the bash terminal output
(cid:44)→
or texts in an editable input box.
(cid:44)→
You can use the accessibility tree to accurately locate positions of
useful elements on the screen and check the concrete textual
(cid:44)→
contents of elements.
(cid:44)→
By combining the screenshot and accessibility tree, you should be
intelligent to predict the next feasible and meaningful action.
(cid:44)→
26SoMSetting
The observation space is a combination of two sources: 1) image-style
screenshot of the desktop with interact-able elements marked with
(cid:44)→
numerical indexes, and 2) text-style accessibility tree derived
(cid:44)→
from AT-SPI library.
(cid:44)→
### Labeled Screenshot
After each action step, you will get a image-style observation, which
is the screenshot of the computer screen. For ease of locating
(cid:44)→
positions of elements, we extend the original screenshot with
(cid:44)→
index marks. That is, some salient elements which can be
(cid:44)→
interacted with (e.g., a button or editable input box) are marked
(cid:44)→
with line boudaries and numeric indexes. You can use this image
(cid:44)→
to locate the elements on the screen or check the status of the
(cid:44)→
computer, especially whether the previous action is successful or
(cid:44)→
not.
(cid:44)→
### Accessibility Tree
The accessibility tree describes the elements (e.g., panels, icons,
buttons, frames, windows, applications) on the computer desktop,
(cid:44)→
as well as its embedded text content, status and positions. For
(cid:44)→
simplicity, we prune the original tree and only extract useful
(cid:44)→
information into a tabular format for you. Here is a quick glance
(cid:44)→
on the observation:
(cid:44)→
INDEX, TAG, NAME, POSITION(top-left x & y), SIZE(width & height),TEXT
1, menu, Visual Studio Code, (99, 0), (184, 27), ''
2, push-button, Chromium Web Browser, (0, 33), (70, 64), ''
3, terminal, Terminal, (70, 74), (1430, 832), (base)
user@ubuntu:~/projects/$'
... more rows ...
, where `INDEX` indicates exactly the numeric label for each element
marked in the screenshot. You can use this alignment information
(cid:44)→
to simplify your predicted action. For example, you can use
(cid:44)→
`pyautogui.click(index_2)` to represent clicking the CENTER of
(cid:44)→
the element with index 2 on the screenshot. We will automatically
(cid:44)→
perform the position calculation and substitution for you. `TAG`
(cid:44)→
/ `NAME` is the element type / name respectively. `POSITION` and
(cid:44)→
`SIZE` together describe the square position of this element on
(cid:44)→
the computer screen. For example, if you want to click one button,
(cid:44)→
you can click any point in the square area defined by `POSITION`
(cid:44)→
and `SIZE`. Assume that the position of this button is (100, 200),
(cid:44)→
and the size is (40, 40), the CENTER of this button is (120, 220),
(cid:44)→
which is calculated by x = 100 + 40 / 2 = 120, y = 200 + 40 / 2 =
(cid:44)→
220. `TEXT` refers to the text content embedded in the element,
(cid:44)→
e.g., the bash terminal output or texts in an editable input box.
(cid:44)→
You can use the accessibility tree to accurately locate positions of
useful elements on the screen and check the concrete textual
(cid:44)→
contents of elements.
(cid:44)→
By combining the screenshot and accessibility tree, you should be
intelligent to predict the next feasible and meaningful action.
(cid:44)→
27G.1.2 ActionSpacePrompt
Asforthepromptofactionspace,weprovidetwochoices: 1)pyautoguicode,and2)JSONdict.
pyautoguiCode
You are required to use `pyautogui` to perform the action grounded to
the observation. And the action space includes two types:
(cid:44)→
1. Python code block using pyautogui wrapped by 3 backticks, e.g.,
```python
# you python code here, e.g.,
pyautogui.hotkey('ctrl', 'c')
```
2. Three pre-defined special actions: [WAIT, FAIL, DONE]
- When you think you have to wait for some time, return ```WAIT```;
- When you think the task can not be done, return ```FAIL```, don't
easily say ```FAIL```, try your best to do the task;
(cid:44)→
- When you think the task is done, return ```DONE```.
These 3 actions also need to be wrapped by 3 backticks.
### REMEMBER THAT:
0. We will import libraries `pyautogui` and `time` automatically for
you, but if you use other python libraries, PLEASE IMPORT THEM
(cid:44)→
FIRST ALTHOUGH THIS IS DISCOURAGED;
(cid:44)→
1. DONOT use the `pyautogui.locateCenterOnScreen` function to locate
the element you want to operate with, since we have no image of
(cid:44)→
the element you want to operate with;
(cid:44)→
2. DONOT use the `pyautogui.screenshot` function to make screenshot;
3. For time efficiency, you can return one line or multiple lines of
python code to perform continuous actions in one response. For
(cid:44)→
example, your response may contain the following code block:
(cid:44)→
```
pyautogui.moveTo(100, 210)
pyautogui.dragTo(500, 200, button='left', mouseDownUp=True)
pyautogui.rightClick()
```
4. When predicting multiple lines of code, make some small delay like
`time.sleep(0.5)` interval, such that the machine can response
(cid:44)→
correctly. And it is STRONGLY RECOMMENDED that, for one action
(cid:44)→
which may influence the environment significantly (e.g., click
(cid:44)→
the button of one application to open it, or click a web link
(cid:44)→
which navigates to a new page), it is better to predict this
(cid:44)→
action without follow-ups in order to observe the changes in
(cid:44)→
environment states first;
(cid:44)→
5. Each time when you predict code, neither variables nor function is
shared acrossed different code blocks. In other words, each code
(cid:44)→
block will be executed in isolation;
(cid:44)→
6. For coordinates (x, y), please speculate or calculate by yourself
based on the observation of previous interaction turn. BE CAREFUL
(cid:44)→
to ensure the coordinates are feasible.
(cid:44)→
7. Please pay attention that, code wrapped by 3 backticks ``` will be
recognized as an action in the action space. Therefore, when you
(cid:44)→
output non-action code, please use other symbols like '''
(cid:44)→
instead.
(cid:44)→
28JSONDict(truncated)
Firstly, we use json dict to describe the types and parameters for
each action we allowed (`required=true` means this argument must
(cid:44)→
be provided). Then, we demonstrate use cases, and precautions.
(cid:44)→
### Specification for All Actions
ACTION_LIST = [
{
"action_type": "MOVE_TO",
"note": "move the cursor to a specified position (x, y)",
"parameters": {
"x": {
"type": float,
"range": [0, MAX_SCREEN_WIDTH],
"required": true,
},
"y": {
"type": float,
"range": [0, MAX_SCREEN_HEIGHT],
"required": true,
}
}
},
... more action dicts ...
]
### Use Cases
- For MOVE_TO, you need to predict the x and y coordinate of the
mouse cursor, the left top corner of the screen is (0, 0).
(cid:44)→
Use case: move the mouse to position (56.1, 65.0)
```json
{
"action_type": "MOVE_TO",
"x": 56.1,
"y": 65.0
}
... more use cases ...
### Precautions
1) The output action MUST BE CHOSEN and CAN ONLY BE CHOSEN from the
action space (json dict) defined above, otherwise your action
(cid:44)→
will be considered as invalid and you will get a penalty. For
(cid:44)→
example, bash, sql, or python code WILL NOT be executed;
(cid:44)→
2) For each action dict, STRICTLY OBEY THE FORMAT, which must contain
the `action_type` field and required parameters. Optional
(cid:44)→
parameters will be set to default values if not provided. NEVER
(cid:44)→
RETURN ME ANYTHING ELSE WHICH IS NOT DEFINED;
(cid:44)→
3) For efficiency, you CAN predict multiple actions in one response,
but REMEMBER TO WRAP EACH ACTION DICT SEPARATELY using backticks
(cid:44)→
```json and ```.
(cid:44)→
29G.1.3 OverallSystemPrompt
You are an intellignet agent who is expert in completing data
science/engineering tasks using professional tools on computer. You
have deep understanding of computer basics and data
(cid:44)→
science/engineering knowledge.
(cid:44)→
Now, you will interact with a real desktop environment, which is an
Ubuntu operating system that has access to the Internet. You
(cid:44)→
should strictly follow the user instruction, communicate with the
(cid:44)→
environment and try your best to complete the given data-related
(cid:44)→
task successfully. Generally, you will communicate with the
(cid:44)→
environment in this interactive and continuous manner:
(cid:44)→
1) In each iteration, you should take one action to control the
keyboard or mouse in the desktop environment given the actions
(cid:44)→
and observations from a few previous steps;
(cid:44)→
2) Then, you will obtain new observations from the environment after
the action is grounded (you do not need to worry about the
(cid:44)→
execution, we will perform it for you);
(cid:44)→
3) Repeat steps 1) and 2) until you think the work is done.
Here are the details of the action spaces (including usage and
precautions) and observation spaces:
(cid:44)→
{{action_prompt}}
{{observation_prompt}}
Besides, here are some important tips for you to better complete the
task:
(cid:44)→
1. My computer's password is 'password', feel free to use it when you
need sudo rights.
(cid:44)→
2. The screen size for the running desktop is: ({screen_width},
{screen_height}).
(cid:44)→
3. Some action may need time to reflect in the environment (e.g.,
code execution and web page loading), please be patient and refer
(cid:44)→
to the WAIT action.
(cid:44)→
4. Try to complete the task in as few steps as possible, we are on a
tight budget.
(cid:44)→
5. Try to use the applications we opened for you as possible, e.g.,
use the opened gnome-terminal instead of the embedded one in
(cid:44)→
Visual Studio Code.
(cid:44)→
6. For critical actions (e.g., opening an application or clicking a
button), ensure the action succeeds before predicting or
(cid:44)→
proceeding to the next one. That is, DO NOT be greedy to predict
(cid:44)→
all actions all at once in one response without confirming the
(cid:44)→
observation of a significant action.
(cid:44)→
7. When you try to write codes or texts, please ensure you have
focused on the right window or input panel. If the input panel
(cid:44)→
already has some texts, be careful that you may need to clear or
(cid:44)→
selecting them before overwritting.
(cid:44)→
8. DO NOT be stubborn to complete the task in one step. You can break
down the task into several steps and complete them one by one.
(cid:44)→
9. DO NOT be stupid to repeat the same actions without any progress.
If you find that the action is not effective in the observation,
(cid:44)→
try another one.
(cid:44)→
10. RETURN ME ONLY THE ACTION DEFINED IN ACTION SPACES. NEVER EVER
RETURN ME ANYTHING ELSE. THIS IS CRITICAL!!!
(cid:44)→
30G.2 TaskPrompt
The task instruction for Spider2-V has two forms. The abstract instruction describes the overall
goalofataskwithoutastep-by-stepsolution, thustestingbothplanningandgroundingabilities.
Theverboseinstructionprovidesadetailedtutorial-likesolutiontothetask,primarilyvalidatingthe
groundingability.
G.2.1 ExampleofTaskPromptforAbstractInstructions
Now, let's start the task!
You are asked to complete the following task: I want to build an
airflow project connecting to a local postgres database. Could
(cid:44)→
you install docker, astro and postgresql for me. The sudo
(cid:44)→
password is 'password' (' not included). By the way, configure
(cid:44)→
docker and postgresql to auto-start on boot, and allow me to
(cid:44)→
prevent typing sudo when using docker each time.
(cid:44)→
G.2.2 ExampleofTaskPromptforVerboseInstructions
Here is a step-by-step tutorial from an expert instructing you how to
complete it:
(cid:44)→
Now we want to upload data from xlang_gcs/google_ads/ in google cloud
storage to my dataset google_ads. To do this:
(cid:44)→
1. Click the "+ ADD" button next to the "Explorer" panel.
2. Click the "Google Cloud Storage" panel on the pop-up window.
3. In the input box "Google Cloud Storage", enter the
'xlang_gcs/google_ads/account_history_data.csv' in the second
(cid:44)→
windows. This window is labeled 'Select file from GCS bucket or
(cid:44)→
use a a URI pattern'.
(cid:44)→
4. Destination Part, set Dataset to 'my_google_ads'
5. In Destination Part, set Table to 'account_history_data'
6. In Schema part, Mark the check mark in front of Auto detect.
7. Then, click the blue `CREATE TABLE` button at the bottom.
8. After page loading, click the "+ ADD" button next to the
"Explorer" panel again.
(cid:44)→
9. Click the "Google Cloud Storage" panel on the pop-up window.
10. In the input box "Google Cloud Storage", enter the
'xlang_gcs/google_ads/account_stats_data.csv' in the second
(cid:44)→
windows. This window is labeled 'Select file from GCS bucket or
(cid:44)→
use a a URI pattern'.
(cid:44)→
11. Destination Part, set Dataset to 'my_google_ads'
12. In Destination Part, set Table to 'account_stats_data'
13. In Schema part, Mark the check mark in front of Auto detect.
14. Click the `CREATE TABLE` button at the bottom left in the pop-up
window.
(cid:44)→
Eventually, we have completed this task.
You can exactly follow the detailed plan above or proactively tackle
the task based on the real-time environment interaction by
(cid:44)→
yourself.
(cid:44)→
31G.3 ExampleofRetrievedContextAugmentedTaskPrompt
WealsointroduceaRAGsetting,wherewecollectandcleantheofficialdocumentsoftheprofessional
tools as the retrieval corpus. We select top k (k may depend on the constraint on input length)
chunks(eachchunkisatokensequencewithmaximumlength512)andinsertthemintotheprompt
input. Herearethreedemonstrationsofdifferentformatsoftheretrievedcontext.
PureTextFormat
We also retrieve relevant documentation from the web to help you with
the task:
(cid:44)→
Documentation Source:
release-1-7-2.dagster.dagster-docs.io/integrations/dagstermill/using-
notebooks-with-dagster.html
(cid:44)→
Documentation Title:
Using Jupyter notebooks with Papermill and Dagster Tutorial
Documentation Content:
The page will display the notebook asset in the Asset Graph.
If you click the notebook asset, a sidebar containing info about the
asset will slide out from the right side of the page. In the
(cid:44)→
Description
(cid:44)→
section of the panel is a View Source Notebook button:
This button allows you to view the notebook directly in the UI. When
clicked, Dagster will render the notebook - referenced in the
(cid:44)→
notebook_path parameter - that'll be executed when the
iris_kmeans_jupyter asset is materialized:
(cid:44)→
Click the Materialize button. To view the execution as it happens,
click the View button in the alert that displays.
(cid:44)→
After the run completes successfully, you can view the executed
notebook in the UI. Click the asset again and locate the View
(cid:44)→
Notebook button in the Materialization in Last Run section of the
(cid:44)→
sidebar:
(cid:44)→
Click the button to display the executed notebook - specifically, the
notebook that was executed and written to a persistent location:
(cid:44)→
Step 5: Add an upstream asset #
While our iris-kmeans notebook asset now materializes successfully,
there are still some improvements we can make. The beginning of
(cid:44)→
the notebook fetches the Iris dataset, which means that every
(cid:44)→
time the notebook is materialized, the data is re-fetched.
(cid:44)→
To address this, we can factor the Iris dataset into its own asset.
This will allow us to:
(cid:44)→
Use the asset as input to additional notebooks.
This means all notebooks analyzing the Iris dataset will use the same
source data, which we only have to fetch once.
(cid:44)→
Materialize notebooks without fetching data for each materialization.
Instead of making potentially expensive API calls, Dagster can fetch
the data from the previous materialization of the Iris dataset
(cid:44)→
and provide that data as input to the notebook.
(cid:44)→
32MarkdownSyntaxFormat
We also retrieve relevant documentation from the web to help you with
the task:
(cid:44)→
Documentation Source:
release-1-7-2.dagster.dagster-docs.io/integrations/dagstermill/using-
notebooks-with-dagster.md
Documentation Title:
Using Jupyter notebooks with Papermill and Dagster Tutorial
Documentation Content:
When clicked, Dagster will render the notebook - referenced in the
`notebook_path`parameter - that'll be executed when the
(cid:44)→
`iris_kmeans_jupyter`asset is materialized:
(cid:44)→
!Click the **Materialize**button. To view the execution as it happens,
click the **View**button in the alert that displays.
(cid:44)→
After the run completes successfully, you can view the executed
notebook in the UI. Click the asset again and locate the **View
(cid:44)→
Notebook**button in the **Materialization in Last Run**section of
(cid:44)→
the sidebar:
(cid:44)→
!Click the button to display the executed notebook - specifically,
the notebook that was executed and written to a persistent
(cid:44)→
location:
(cid:44)→
!Step 5: Add an upstream asset#
------------------------------
While our `iris-kmeans`notebook asset now materializes successfully,
there are still some improvements we can make. The beginning of
(cid:44)→
the notebook fetches the Iris dataset, which means that every
(cid:44)→
time the notebook is materialized, the data is re-fetched.
(cid:44)→
To address this, we can factor the Iris dataset into its own asset.
This will allow us to:
(cid:44)→
**Use the asset as input to additional notebooks.**This means all
notebooks analyzing the Iris dataset will use the same source
(cid:44)→
data, which we only have to fetch once.
(cid:44)→
**Materialize notebooks without fetching data for each
materialization.**Instead of making potentially expensive API
(cid:44)→
calls, Dagster can fetch the data from the previous
(cid:44)→
materialization of the Iris dataset and provide that data as
(cid:44)→
input to the notebook.
(cid:44)→
In this step, you'll:
Create the Iris dataset assetProvide the Iris dataset as input to the
notebookModify the notebook
(cid:44)→
33SimplifiedHTMLFormat
We also retrieve relevant documentation from the web to help you with
the task:
(cid:44)→
Documentation Source:
release-1-7-2.dagster.dagster-docs.io/integrations/dagstermill/using-
notebooks-with-dagster.html
Documentation Title:
Using Jupyter notebooks with Papermill and Dagster Tutorial
Documentation Content:
If you execute these cells, several plots of the Iris dataset will be
created:
(cid:44)→
<p>Next, we conduct our K-means analysis:</p>
<code>estimator
<span>=</span>sklearn<span>.</span>cluster<span>.</span>KMeans
(cid:44)→
<span>(</span>n_clusters<span>=</span><span>3</span><span>)</span>
estimator<span>.</span>fit<span>(</span>iris<span>[</span>
<span>[</span><span>"Sepal length (cm)"</span><span>,</span>
<span>"Sepal width (cm)"</span><span>,</span>
<span>"Petal length (cm)"</span><span>,</span>
<span>"Petal width (cm)"</span>
<span>]</span><span>]</span><span>)</span>
</code>
<p>Lastly, we plot the results of the K-means analysis. From the
plots, we can see that one species of Iris is separable from the
(cid:44)→
other two, but a more sophisticated model will be required to
(cid:44)→
distinguish the other two species:</p>
(cid:44)→
<p>Like many notebooks, this example does some fairly sophisticated
work, including producing diagnostic plots and a statistical
(cid:44)→
model. For now, this work is locked away in the
(cid:44)→
<code>.ipynb</code>format, only reproducible using a complex
(cid:44)→
Jupyter setup, and only programmatically accessible within the
(cid:44)→
notebook context. We'll address this in the remainder of the
(cid:44)→
tutorial.</p>
(cid:44)→
<h2>Step 2: Create a Dagster asset from the Jupyter
Notebook<span>#</span></h2>
(cid:44)→
<p>By creating a Dagster asset from our notebook, we can integrate
the notebook as part of our data platform. This enables us to
(cid:44)→
make its contents more accessible to developers, stakeholders,
(cid:44)→
and other assets in Dagster.</p>
(cid:44)→
<p>To create a Dagster asset from a Jupyter notebook, we can use the
<code>define_dagstermill_asset</code>function.
(cid:44)→
34