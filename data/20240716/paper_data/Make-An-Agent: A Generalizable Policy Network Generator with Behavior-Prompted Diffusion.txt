Make-An-Agent: A Generalizable Policy Network
Generator with Behavior-Prompted Diffusion
YongyuanLiang1 TingqiangXu2 KaizheHu2 GuangqiJiang3
FurongHuang1 HuazheXu245
1UniversityofMaryland,CollegePark 2TsinghuaUniversity
3UniversityofCalifornia,SanDiego 4ShanghaiQiZhiInstitute 5ShanghaiAILab
Abstract
Can we generate a control policy for an agent using just one demonstration of
desiredbehaviorsasaprompt,aseffortlesslyascreatinganimagefromatextual
description? Inthispaper,wepresentMake-An-Agent,anovelpolicyparameter
generatorthatleveragesthepowerofconditionaldiffusionmodelsforbehavior-to-
policygeneration. Guidedbybehaviorembeddingsthatencodetrajectoryinforma-
tion,ourpolicygeneratorsynthesizeslatentparameterrepresentations,whichcan
thenbedecodedintopolicynetworks. Trainedonpolicynetworkcheckpointsand
theircorrespondingtrajectories,ourgenerationmodeldemonstratesremarkablever-
satilityandscalabilityonmultipletasksandhasastronggeneralizationabilityon
unseentaskstooutputwell-performedpolicieswithonlyfew-shotdemonstrations
asinputs. Weshowcaseitsefficacyandefficiencyonvariousdomainsandtasks,in-
cludingvaryingobjectives,behaviors,andevenacrossdifferentrobotmanipulators.
Beyond simulation, we directly deploy policies generated by Make-An-Agent
ontoreal-worldrobotsonlocomotiontasks.
1 Introduction
Policylearningtraditionallyinvolvesusingsampledtrajectoriesfromareplaybufferorbehavior
demonstrationstolearnpoliciesortrajectorymodelsmappingfromstatestoactiona,modelinga
narrowbehaviordistribution. Inthispaper,weconsiderashiftinparadigm: movingbeyondtraining
apolicy,canwereverselypredictoptimalpolicynetworkparametersusingsuboptimaltrajectories
fromofflinedata? Thisapproachwouldobviatetheneedtoexplicitlymodelbehaviordistributions,
allowingustolearntheunderlyingparameterdistributionsintheparameterspace,thusrevealingthe
implicitrelationshipbetweenagentbehaviorsforspecifictasksandpolicyparameters.
Using low-dimensional demonstrations (such as agent behavior) to guide the generation of high-
dimensionaloutputs(policyparameters)isachallengingproblem. Whendiffusionmodels(12;19)
havedemonstratedhighlycompetitiveperformanceonvarioustasksincludingtext-to-imagesynthesis,
weareinspiredtoapproachpolicynetworkgenerationasaconditionaldenoisingdiffusionprocess.
Byprogressivelyrefiningnoiseintostructuredparameters,thediffusion-basedgeneratorcandiscover
variouspoliciesthatarenotonlysuperiorinperformancebutalsomorerobustandefficientthanthe
demonstrationinthepolicyparameterspace.
Whilepriorworksonhypernetworks(10;1;17)exploretheconceptoftrainingahypernetworkto
generateweightsforanotherneuralnetwork,theyprimarilyusehypernetworksasaninitialization
networkofmeta-learning(7)andthenadapttospecifictasksettings. Ourapproachdivergesfromthis
paradigmbyleveragingagentbehaviorsasdirectpromptsortogenerateoptimalpolicieswithinthe
0Code,datasetandvideoarereleasedinhttps://cheryyunl.github.io/make-an-agent/.
1Correspondingto:cheryunl@umd.edu,huazhe_xu@mail.tsinghua.edu.cn
Preprint.
4202
luJ
51
]IA.sc[
1v37901.7042:viXraparameterspace,withouttheneedforanydownstreampolicyfine-tuningoradaptationwithgradient
updates. Since behaviors - as the observable manifestation of deployed policies - from different
tasksoftenshareunderlyingskillsorenvironmentalinformation,ourpolicygeneratorcanexploit
thesepotentialcorrelationsintheparameterspace, suchassharedparametersforsimilarmotion
patterns,whichleadstoenhancedcross-taskone-shotgeneralizability. Whatweneedisanend-to-end
behavior-to-policygenerator,notasharedbasepolicy.
Toachievethis,weintroduceMake-An-Agent,featuringthreekeytechnicalcontributions: (1)We
propose an autoencoder that encodes policy networks into compact latent representations based
on their network layers, which can also effectively reconstruct the original policy from its latent
representation. (2) We leverage contrastive learning to capture the mutual information between
long-termtrajectoriesandtheirsuccessorfuturestates. Thisapproachyieldsanovelandefficient
behaviorembedding. (3)Weutilizeasimpleyeteffectivediffusionmodelconditionedonthelearned
behaviorembeddings, togeneratepolicyparameterrepresentations, whicharethendecodedinto
deployablepoliciesusingthepretraineddecoder. (4)Weconstructapretraineddatasetofpolicy
networkparametersandcorrespondingdeployedtrajectoriestotrainourproposedmethodology.
ToinvestigatethegenerationperformanceofMake-An-Agent,weevaluateourapproachinthree
continuouscontroldomainsincludingdiversetabletopmanipulationandreal-worldlocomotiontasks.
Duringtesttime,wegeneratepoliciesusingtrajectoriesfromthereplaybufferofpartially-trainedRL
agents.Thepoliciesgeneratedbyourmethoddemonstratesuperiorperformancecomparedtopolicies
producedbymulti-task(26;22)ormetalearning(7;25)andotherhypernetwork-basedgeneration
methods(1). Ourgeneratoroffersseveralkeyadvantages:
‚Ä¢ Versatility: Make-An-Agentexcelsingeneratingeffectivepoliciesforawiderangeof
tasksbyconditioningonagentbehaviorembeddings. Sincewetraintheparametergenerator
forlatentparameterrepresentations,itcangeneratepolicynetworksofvaryingsizeswithin
thelatentspace,demonstratingscalability.
‚Ä¢ Generalizability: Ourdiffusion-basedgeneratordemonstratesrobustgeneralization,yield-
ing proficient policies even for unseen behaviors or unseen embodiments in unfamiliar
tasks.
‚Ä¢ Robustness: Ourmethodcangeneratediversepolicyparameters,exhibitingresilientper-
formanceunderenvironmentalrandomnessfromsimulatorsandreal-worldenvironments.
Notably,Make-An-Agentcansynthesizehigh-performingpolicieswhenfedwithnoisy
trajectories,highlightingtherobustnessofourmodel.
2 Preliminaries
Policy Learning. Reinforcement Learning (RL) is structured within the formation of Markov
Decision Processes (MDPs) (2), which is defined by the tuple M = ‚ü®S,A,P,R,Œ≥‚ü©. Here, S
signifiesthestatespace,Atheactionspace,P thetransitionprobabilities,Rtherewardfunction
andŒ≥ thediscountfactor. RLaimstooptimizeanagent‚ÄôspolicyœÄ :S ‚ÜíA,whichoutputsaction
a basedonstates ateachtimestep,tomaximizecumulativerewards. Theoptimalpolicycanbe
t t
expressedas:
(cid:34) ‚àû (cid:35)
(cid:88)
œÄ‚àó =argmaxE Œ≥tr , (1)
z‚àºœÄ t
œÄ
t=0
wherezrepresentsatrajectorygeneratedbyfollowingpolicyœÄ.IndeepRL,policiesœÄarerepresented
usingneuralnetworkfunctionapproximations(21),parameterizedbyŒ∏ ,facilitatingthelearningof
œÄ
intricatebehaviorsacrosshigh-dimensionalstateandactionspaces.
DiffusionModels. DenoisingDiffusionProbabilisticModels(DDPMs)(12)aregenerativemodels
thatframedatagenerationthroughastructureddiffusionprocess,whichinvolvesiterativelyadding
noisetothedataandthendenoisingittorecovertheoriginalsignal. Givenasamplex ,theforward
0
diffusionprocesstoobtainx ,x ,...,x ofincreasingnoiseintensityistypicallydenotedby:
1 2 T
(cid:112)
q(x |x )=N(x , 1‚àíŒ≤ x ,Œ≤ I), (2)
t t‚àí1 t t t‚àí1 t
whereqistheforwardprocess,N isGaussiannoise,andŒ≤ ‚àà(0,1)isisthenoisevariance.
t
2Thedenoisingprocess,whichisthereverseoftheforwarddiffusion,canbeformulatedas:
p (x |x )=N (x |¬µ (x ,t),Œ£ ), (3)
Œ∏ t‚àí1 t t‚àí1 Œ∏ t Œ∏
wherep denotesthereverseprocess,¬µ andŒ£ arethemeanandvarianceoftheGaussiandistribution
Œ∏ Œ∏ Œ∏
respectively,whichcanbeapproximatedbyanoisepredictionneuralnetworkparameterizedbyŒ∏.
Diffusion models aim to learn reverse transitions that maximize the likelihood of the forward
transitions at each time step t. The noise prediction network Œ∏ is optimized using the following
objective,asthefunctionmappingfromœµ (x ,t)to¬µ (x ,t)isaclosed-formexpression:
Œ∏ t Œ∏ t
‚àö ‚àö
L (Œ∏):=E [||œµ‚àíœµ ( Œ±¬Ø x + 1‚àíŒ±¬Ø œµ,t)||2], (4)
DM x0‚àºq,œµ‚àºN(0,1),t Œ∏ t 0 t
‚àö ‚àö
Here,œµ‚àºN(0,I),isthetargetGaussiannoise,Œ±¬Ø
:=(cid:81)t
1‚àíŒ≤ ,and Œ±¬Ø x + 1‚àíŒ±¬Ø œµisthe
t s=1 s t 0 t
estimateddistributionofx fromtheclosed-formrelation.
t
Althoughdiffusionmodelsaretypicallyusedforimagegenerationthroughthereverseprocess,the
variablexcanbegeneralizedtorepresentdiverseentitiesforgeneration. Inthispaper,weadaptxto
representtheparametersŒ∏ ofthepolicynetworkinpolicylearning.
œÄ
3 Methodology
Inference process Behavior Generated
embeddings deployable
Agent‚Äôs trajectory policy
Conditional
inputs
denoising decode
‚Ä¶ Latent Diffusion ‚Ä¶
Model
Gaussian noise noising encode vectorize
Latent parameter
Forward process representations from training
dataset
Figure1: Overview: Intheinferenceprocessofpolicyparametergeneration,conditioningonbehavior
embeddingsfromtheagent‚Äôstrajectory,thelatentdiffusionmodeldenoisesrandomnoiseintoalatentparameter
representation,whichcanthenbereconstructedasadeployablepolicyusingtheautoencoder. Theforward
processforprogressivelynoisingthedataisalsoconductedonthelatentspaceafterencodingpolicyparameters
aslatentrepresentations.
Autoencoder Long trajectory with ùëõsteps After the first
success step ùêæ
encoding s a ‚Ä¶ s a s a s ‚Ä¶ s
vectorized 0 0 n-1 n-1 n n K K+m
‚Ä¶
Projection ùùì Projection ùùã
Policy Contrastive
network Latent h Loss v
parameters Representations
decoding
Behavior embeddings
Figure2:Autoencoder:Encodingpolicyparameters Figure3:Contrastivebehaviorembeddings:Learn-
intothelatentspaceanddecodinglatentparameterrep-inginformativebehaviorembeddingsfromlongtrajec-
resentationsintopolicynetworks. torieswithcontrastiveloss.
Overview. AnoverviewofourproposedmethodologyisillustratedinFigure1. Toachievethis,
weaddressseveralkeychallenges: (1)Developinglatentrepresentationsofhigh-dimensionalpolicy
parameters that can be effectively reconstructed into well-functioned policies. (2) Learning an
embeddingofbehaviordemonstrationsthatservesasaneffectivediffusioncondition. (3)Traininga
conditionaldiffusionmodelspecificallyforpolicyparametergeneration.
Parameterrepresentation. WeuseanMLPwithmlayersasthecommonpolicyapproximator.
Consequently,whenthefullparametersofapolicyareflattened,theyformahigh-dimensionalvector.
3
redocnE Decoder
AutoencoderToenablegenerationwithlimitedcomputationalresourceswhileretainingefficacy,andtosupportthe
generationofpoliciesfordifferentdomainswithvaryingstateandactiondimensions,wecompress
thepolicynetworkparametersintoalatentspace.
Basedonthepolicynetworkarchitecture,weunfoldtheparametersfollowingthearchitectureofthe
policynetwork,representedasx=[x ,x ,...,x ],wherex denotestheflattenedparameters
0 1 m‚àí1 i
fromeachlayer. TheencoderE encodeseachx asz ,resultinginaparameterlatentrepresentation
i i
denotedasz =[z ,z ,...,z ],whereeachz inthelatentspacehasthesamedimension,while
0 1 m‚àí1 i
thedecoderDcandecodezintox.Toimprovetherobustnessofthisprocedure,weintroducerandom
noiseaugmentationinbothencodinganddecodingduringtraining. Giveneachvectorizedparameter
asx,weminimizetheobjectiveas,
L=MSE(x,D(E(x+Œæ )+Œæ )), (5)
D E
where z = E(x+Œæ ), and Œæ and Œæ represent the augmented noise. The architecture of the
D E D
autoencoderisshowninFigure2.
Foreachdomain,theautoencoderforparameterrepresentationonlyneedstobetrainedoncebefore
parameter generation, which can handle policy parameters from different tasks. To facilitate the
generalizabilityofthepolicygeneratoracrossdomains,wedesignthelatentparameterrepresentations
tohavethesamedimensionsfordifferentdomains.
Behaviorembedding. Sinceourgoalinlearningbehaviorembeddingsisnottomodelthedistri-
butionofstatesandactions,buttoprovideconditionalinformationforpolicyparametergeneration,
weaimforthemtoencapsulatebothcrucialenvironmentaldynamicsandthekeyinformationofthe
taskgoal. Theprinciplebehindourbehaviorembeddingsistolearnthemutualinformationbetween
precedingnsteptrajectoriesandsubsequentstateswithsuccesssignals.
I=I(s ;{s ,a }n ) (6)
success i i i=0
We propose a novel contrastive method to train behavior embeddings. In Figure 3, we present a
designdemonstrationofourcontrastiveloss. ForalongtrajectoryœÑ,wedecoupleitastheninitial
state-actionpairsœÑn =(s ,a ,s ,a ,...,s ,a )andthemstatesafterthefirstsuccesstimeK as
0 0 1 1 n n
œÑÀÜ=(s ,s ,...,s ). Givenabatchoftrajectorysequences{œÑ }N whichcanbepresented
K K+1 K+m i i=1
as{œÑn,œÑÀÜ}N ,weoptimizethecontrastiveobjective(15;29)as:
i i i=1
L(œï ,œà ,W)=‚àí
1 (cid:88)N
log
h‚ä§
i
Wv
i (7)
Œ∏ Œ∏ N (cid:80)N h‚ä§Wv
i=1 j=1 i j
whereh = œï (œÑn)andv = œà (œÑÀÜ)areembeddingsfromdifferentpartsofthelongtrajectoryœÑ
i Œ∏ i i Œ∏ i i
andW isalearnablemetricthatmeasuresthesimilaritybetweenembeddingsh andv .
i i
ForeachtrajectoryœÑ,weobtainasetofembeddingsœÑ ={h ,v }. Inpractice,thechoiceofspecific
e i i
embeddingscanbetailoredtothecharacteristicsofdifferenttasksandtrajectories. Weuse(h ,v )as
i i
theconditionalinputinourexperiments.
Flexibility. Withtheconsiderationthatinmanyscenarios,rewardsareoftensparseornon-existent,
whereassuccesssignalsserveasamoredirectindicatorofwhetherapolicyhasachieveditsobjective.
Wethereforeuseoriginaltrajectoriesthatexcluderewardinformationbutincludesuccessinformation.
Fortaskswithoutexplicitsuccesssignals, suchaslocomotion, wesegmentlongtrajectoriesinto
multipleshortertrajectories.Foreachsegment,weusethelastmstatesasœÑÀÜandthe0‚àínstate-action
pairsasœÑn. Theinformativebehaviorembeddingsofalongtrajectoryareconcatenatedfromthe
embeddingsofallthetrajectorysegments.
Thisembeddingapproachstrivestocapturetheessentialinformationforgeneratingbehavior-specific
policyparameters,includingenvironmentaldynamicsandtaskgoals,usingthemostconciserepre-
sentationpossiblefromlongtrajectoriesandprioritizingflexibilityandefficiency.
Conditionalpolicygenerator. Aftertrainingtheparameterautoencoderandbehaviorembeddings,
forpolicyparameterxandthecorrespondingtrajectorygdeployedbypolicyx,wecantransferx
aslatentparameterrepresentationzwiththeautoencoderE andtrajectoryœÑ asbehaviorembedding
œÑ . Theconditionaldiffusiongeneratoristrainedonlatentrepresentationz,conditioningonœÑ . We
e e
optimizetheconditionallatentdiffusionmodelviathefollowinglossfunction:
L (Œ∏):=E (cid:2) ‚à•œµ‚àíœµ (z ,œÑ ,t)‚à•2(cid:3) , (8)
LDM z,œµ‚àºN(0,1),t Œ∏ t e 2
4wheretheneuralbackboneœµ (z ,œÑ ,t)isimplementedasa1DconvolutionalUNet(18)parameterized
Œ∏ t e
byŒ∏ andtisuniformlysampledfrom{1,...,T}. Theoutputsofourparametergeneratorcanbe
encoded by D as deployable policies. During training the diffusion model, both the parameter
autoencoder and behavior embedding layers are frozen, which ensures the training stability and
efficiency.
Dataset. Webuildadatasetcontainingtensofthousandsofpolicyparametersandtrajectoriesfrom
deployingthesepolicies. ThedatasetisobtainedfrommultipleRLtrainingacrossarangeoftasks.
Weutilizedthedatasettotrainboththeautoencoderandbehaviorembeddingmodels. Thenweuse
theencodedparameterrepresentationsandbehaviorembeddingsderivedfromthecollectedtrajectory
totraintheconditionaldiffusionmodelforpolicyparametergeneration.
4 Experiments
MetaWorld Robosuite Quadrupedal Locomotion
Figure4:VisualizationofMetaWorld,Robosuite,andrealquadrupedallocomotion.
WeconductextensiveexperimentstoevaluateMake-An-Agent,answeringthefollowingproblems:
‚Ä¢ Howdoesourmethodcomparewithothermulti-taskorlearning-to-learnapproachesfor
policylearning,intermsofperformanceonseentasksandgeneralizationtounseentasks?
‚Ä¢ Howscalableisourmethod,andcanitbefine-tunedacrossdifferentdomains?
‚Ä¢ Doesourmethodmerelymemorizepolicyparametersandtrajectoriesofeachtask,orcanit
generatediverseandnewbehaviors?
Benchmarks. Weincludetwomanipulationbenchmarksforsimulatedexperimentsandreal-world
robottaskstoshowtheperformanceandcapabilitiesofourmethodasvisualizedinFigure4.
MetaWorld. MetaWorld(27)isabenchmarksuiteforrobotictabletopmanipulation,consistingof
adiversesetofmotionpatternsfortheSawyerroboticarmandinteractionswithdifferentobjects.
Weselected10tasksfortrainingasseentasksand8forevaluationasunseendownstreamtasks.
DetaileddescriptionsofthesetaskscanbefoundinAppendixB.1. ThestatespaceofMetaWorld
consistsof39dimensionsandtheactionspacehas4dimensions. Thepolicynetworkarchitecture
usedforMetaWorldisa4-layerMLPwith128hiddenunits,containingatotalof22,664parameters.
Robosuite. Robosuite(31),asimulationbenchmarkdesignedforroboticmanipulation,supports
variousrobotssuchastheSawyerandPandaarms. Wetrainmodelsonthreemanipulationtasks:
BlockLifting,DoorOpeningandNutAssembly,usingthesingle-armPandarobot. Evaluationsare
conductedonthesametasksusingtheSawyerrobot. Thisexperimentaldesignaimstovalidatethe
practicalityofourapproachbyassessingwhetherthegeneratedpolicycanbeeffectivelyutilizedon
differentrobots. IntheRobosuiteenvironment,thestatespacecomprises41dimensions,andthe
actionspaceconsistsof8dimensions. Thepolicynetworkemployedforthisdomaincontains23,952
parameters.
Quadrupedallocomotion. ToevaluatethepoliciesgeneratedbyMake-An-Agentinthereal
world,weutilizewalk-these-ways(13)totrainpoliciesonIsaacGymanduseourmethodtogenerate
actornetworksconditioningontrajectoriesfromIsaacGymsimulationwiththepretrainedadaptation
modules.Then,wedeploythegeneratedpolicyonrealrobotsinenvironmentsdifferfromsimulations.
Thepoliciesgeneratedforreal-worldlocomotiondeploymentcomprise50,956parameters.
Dataset.Wecollect1500policynetworksforeachtaskinMetaWorldandRobosuite.Thesenetworks
aresourcedfrompolicycheckpointsduringSAC(11)training. Thecheckpointsaresavedevery
55000trainingstepsoncethetestsuccessratereaches1. Duringthetrainingstage,wefixtheinitial
locationsofobjectsandgoalsandtrainthepoliciesusingdifferentrandomseeds. Foreachtask,we
requireanaverageof8SACtrainingruns,approximately30GPUhours.
Forevaluation, thetrajectoriesusedasgenerationconditionsaresampledfromtheSACtraining
buffer within the first 0.5 million timesteps, which can be highly sub-optimal, under the same
environmentalinitialization. Duringtesting,thegeneratedpoliciesareevaluatedin5randominitial
configurations,thoroughlyassessingtherobustnessofpoliciesgeneratedusingtrajectoriesfromthe
fixedenvironmentsettings.
InRoboSuiteexperiments,duetotheinconsistencyinpolicynetworks,weretraintheautoencoderand
finetunethediffusiongeneratortrainedonMetaWorlddata. TheexperimentalsetupforRoboSuiteis
almostidenticaltothatofMetaWorld,withtheonlydifferencebeingtherobotusedduringtesting.
Forreal-worldlocomotiontasks,wesave10,000policynetworkcheckpointsusingwalk-these-ways
(WTW)(13)trainedonIsaacGym,requiringatotalof200GPUhours. The100trajectoriesusedas
generationconditionsaresourcedfromthefirst10,000trainingiterationsofWTW.
Baselines. WecompareMake-An-Agentwithfourbaselines,includingmulti-taskimitationlearn-
ing(IL),multi-taskreinforcementlearning(RL),meta-RLwithhypernetworks,andmeta-ILwith
transformers. Theserepresentstate-of-the-artmethodsformulti-taskpolicylearningandadaptation.
Forafaircomparison,eachbaselineusesthesametestingtrajectorydatafordownstreamadaptation.
Multi-task BC (23): We train behavior cloning policies using 100 trajectories from the optimal
policiesinourtrainingdatasetforeachtask,andthenfinetunethemwithtesttrajectories,whichare
treatedastrajectorieswithsparserewards.
Multi-taskRL,CARE(22): WetrainCAREoneachtaskfor2millionsteps. ForRLtraining,we
trainthealgorithmindenserewardenvironmentsandfinetunethemodelusingtesttrajectorieswith
sparserewards,wherefeedbackisonlyprovidedattheendofatrajectory.
Meta-RL with hypernetworks (1): We train a hypernetwork with our training data with dense
rewards,whichcanadapttodifferenttask-specificpoliciesduringtestingwithtesttrajectories.
MetaImitationLearningwithdecisiontransformer(DT)(4)Wetrainthepre-trainedDTmodel
usingthetrainingtrajectoriesinourdataset,thenusethetesttrajectoriesfromreplaytoadaptitto
testtasks.
4.1 PerformanceAnalysis
Figure5:Evaluationofseentaskswith5randominitializationsonMetaWorldandRobosuite.Ourmethod
generatepoliciesusing5/10/50/100testtrajectories.Baselinesarefinetuned/adaptedbythesametesttrajectories.
Resultsareaveragedovertrainingwith4seeds.
Byusingtesttrajectoriesasconditions,ourpolicygeneratorcanproduceanequivalentnumberof
policyparameters. Comparedwithbaselines,wereportboththebestresultamongthegenerated
policiesandtheaverageperformanceofthetop5policies. Allalgorithmsusethesametask-specific
replay trajectories. The difference is that we use them as generation conditions, whereas other
methodsusethemforadaptation.
Wedefinepoliciesachievinga100%successrateduringevaluationasqualifiedpolicies. Theanalysis
ofqualificationratesforpoliciesgeneratedbyourmodelispresentedinAppendixB.2.
6Figure6:Evaluationof8unseentaskswith5randominitializationsonMetaWorldandRobosuite.Our
methodgeneratespoliciesusing50/100testtrajectorieswithoutanyfinetuning.Baselinesareadaptedusingthe
sametesttrajectories.Averageresultsarefromtrainingwith4seeds.
Adaptabilitytoenvironmentalrandomnessonseentasks. Figure5demonstratesthesignificant
advantage of our algorithm over other methods on seen tasks. This is attributed to the fact that,
despitetesttrajectoriesoriginatingfromthesameenvironmentinitialization,thegeneratedpolicy
parametersaremorediverse,thuspossessingastrongabilitytoadapttoenvironmentalrandomness.
Incontrast,otheralgorithms,whenadaptedusingsuchsingulartrajectories,exhibitmorelimited
adaptability in these scenarios. Our experimental design aligns with practical requirements, as
real-worldrandomnessisinherentlymorecomplex.
Generalizabilitytounseentasks. Figure6showcasesthesuperiorperformanceofouralgorithm
onunseentasks. Testtrajectoriesoriginatefromthesameenvironmentsettingforeachtask,while
evaluationoccursinrandomlyinitializedenvironments. Ourpolicygenerator,withoutfine-tuning,
directlyutilizestesttrajectoriesasinput,demonstratingaremarkableabilitytogenerateparameters
that work on unseen tasks. The agent‚Äôs behavior in unseen tasks exhibits similarities to seen
taskbehaviors,suchasarmdynamicsandthepathtogoals. Byeffectivelycombiningparameter
representationsrelatedtothesefeatures,thegenerativemodelsuccessfullygenerateseffectivepolicies.
Incontrast,baselinemethodsstruggletoadaptinenvironmentalrandomness.
Theseresultsstronglysuggestthatouralgorithm,comparedtootherpolicyadaptationmethods,may
offerasuperiorsolutionforunseenscenarios. Tofurtherinvestigaterobustnessingeneralization,we
addedGaussiannoisewithastandarddeviationof0.1toactionsintesttrajectoriesusedforpolicy
generationoradaptationonunseentasks. Figure7demonstratesthatourmethodremainsresilient
tonoisyinputs,whiletheperformanceofthebaselinesissignificantlyimpacted. Webelievethisis
becauseourbehaviorembeddingsonlyneedtocapturekeydynamicinformationasconditionsto
generatepolicies,withoutdirectlylearningstate-actionrelationshipsfromtrajectories,resultingin
betterrobustness.
Trajectorydifference. Tocomparethedifferencebetweenusingtesttrajectoriesasconditionsand
thetrajectoriesobtainedbydeployingthegeneratedpolicies,wevisualizethetrajectoriesduring
unseentaskevaluations.AsshowninFigure9,ourdiffusiongeneratorcansynthesizevariouspolicies,
whichissignificantlydifferentfrompolicylearningmethodsthatlearntopredictactionsorstates
fromtrajectorydata. Webelievethatthisphenomenonfullyillustratesthevalueofourproposed
policyparametergenerationparadigm.
Parameterdistinction. Beyondtrajectorydifferences,wealsoinvestigatethedistinctionbetween
synthesizedparametersandRLpolicyparameters. WecalculatethecosinesimilaritybetweentheRL
policiesusedtoobtainthetesttrajectoriesandtheparametersgeneratedfromthesetrajectories. As
abenchmark,weincludetheRLpoliciesafter100stepsoffinetuningwiththetestdata. Fortasks
seenduringtraining,theparametersgeneratedbyourapproachdemonstratesignificantlygreater
diversitycomparedtotheRLparametersafterfine-tuning,indicatingthatourgeneratordoesnot
simplymemorizetrainingdata. Onunseentasks,thesimilaritybetweenourgeneratedparameters
andthoselearnedbyRLisalmostnegligible,withmostsimilaritiesfallingbelow0.2. Thisfurther
highlightsthediversityandnoveltyofthepolicyparametersgeneratedbyourmethod.
Real-worldEvaluation Wefurtherdeploypoliciesgeneratedfromsimulationtrajectoriesontoa
quadrupedrobot,instructingittocompletetasksasillustratedinFigure10. Oursynthesizedpolicies
7Figure7: EvaluationofunseentasksonMetaWorld Figure8:Ablationstudiesaboutusingdifferentem-
usingnoisedtrajectories. beddingsasconditionsinpolicygenerationonMeta-
World5unseentasks.(Top5models)
Window close Faucet open Handle press side Door lock
Figure9:Trajectorydifference:trajectoriesasconditionalinputsv.s.trajectoriesfromsynthesizedpoliciesas
outputsonMetaWorld4unseentasks.
exhibitsmoothandeffectiveresponseswhenfacedwiththesechallengingtasks,whichhighlightsthe
stabilityofthegeneratedpoliciesunderthedynamicsrandomnessofreal-worldenvironments.2
Making agile turns to avoid stepping on a Navigating to circumvent the goal and
bouquet while moving across a mat ball while swiftly moving backward.
Figure10:Real-worldlocomotiontasks,includ- Figure11:ParameterSimilarity: Parametercosine
ingturning,fastbackwardmovement,andobstacle similaritybetweenRL-trainedpoliciesandourgener-
avoidanceonamat. atedpoliciesorfine-tunedpolicies.
4.2 AblationStudies
Tobetterinvestigatetheimpactofeachdesignchoiceinourmethodonthefinalresults,weconduct
aseriesofcomprehensiveablationstudies. AllablationstudiesreportaverageresultsoftheTop5
generationmodelsonMetaWorld.
Choiceofbehaviorembeddings. Regardingthechoiceofconditionalembeddings,asillustrated
in Figure 3, we concatenate h and v as generation conditions to maximally preserve trajectory
information. Figure8showsthatutilizingeitherembeddingindividuallyalsoachievescomparable
performanceduetoourcontrastiveloss,ensuringefficientcaptureofdynamicsinformation. Our
contrastivebehaviorembeddingssignificantlyoutperformabaselinethataddsanembeddinglayerin
thediffusionmodeltoencodetrajectoriesasinput. Theseablationresultsunderscoretheeffectiveness
ofourbehaviorembeddings.
Choiceoftrajectorylength. Thetrajectorylengthnusedinbehaviorembeddingscanalsoimpact
experimental results. Figure 12a demonstrates that overly short trajectories lead to performance
degradation,probablyduetotheabsenceofcrucialbehaviorinformation. However,beyond40steps,
trajectorylengthminimallyimpactspolicygeneration,indicatingthatourmethodisnotsensitiveto
thelengthoftrajectories.
2WethankKunLeiandQingweiBenfortheirhelpandsupportinreal-robotapplications.
8Impactofpolicynetworksize. Theimpactofpolicynetworksizeongeneratedparametersisalso
worthdiscussing, asthenetwork‚Äôshiddensizeinfluencesthedimensionalityofparameterstobe
generated. Figure12bsuggeststhatahiddensizeof128isasuitablechoice. Smallernetworksmay
hinderpolicyperformance,whilelargeronesincreaseparameterreconstructioncomplexity.
Impactofparameternumberusedintraining. Westudytheimpactofthenumberofpolicy
checkpointsincludedpertaskinthetrainingdataset,asshowninFigure12c.Insufficienttrainingdata
(<=1000)leadstoasignificantperformancedeclineacrossalltasks. Withmorethan1000parameters,
thereisnonotableimprovementinperformance.
Impactoflatentrepresentationsize. Additionally,Figure12dillustratestheimpactofvarying
thesizeofthelatentparameterrepresentation. Largerlatentrepresentationscannegativelyaffect
theperformanceofthegenerativemodel. Conversely,whenthesizeofparameterrepresentationsis
toosmall,itmayhindertheautoencoder‚Äôscapacitytodecoderepresentationstodeployablepolicies.
Thisunderscorestheinfluenceoftheparameterautoencoderontheoveralleffectivenessofthepolicy
networkgenerator.
(a)TrajectoryLength (b)PolicyModelSize (c)ParameterNumber (d)RepresentationSize
Figure12:AblationstudiesofourtechnicaldesignsonMetaWorldwith50testtrajectories(Top5models).
5 RelatedWorks
ParameterGeneration. Learningtogenerateneuralnetworkshaslongbeenacompellingareaof
research. SincetheintroductionofHypernetworks(10)andthesubsequentextensions(3),several
studieshaveexploredneuralnetworkweightprediction. Hypertransformer(30)utilizesTransformers
togenerateweightsforeachlayerofconvolutionalneuralnetworks(CNN)usingtasksamplesfor
supervisedandsemi-supervisedlearning. Additionally,previouswork(20)employsself-supervised
learningtolearnhyperrepresentationsofneuralnetworkweights. Inthecontextofusingdiffusion
models for parameter generation, G.pt (16) trains a diffusion transformer to generate parameters
conditionedonlearningmetricssuchastestlossesandpredictionerrors,enablingtheoptimization
ofunseenparameterswithasingleupdate. Similarly,p-diff(24)proposeadiffusion-basedmethod
to generate the last two normalization layers without any conditions for classification tasks. In
contrasttothesepriorworks,ourfocusisonpolicylearningproblems. Wedevelopalatentdiffusion
parametergeneratorthatismoregeneralizableandscalable,basedonagents‚Äôbehaviorsasprompts.
LearningtoLearnforPolicyLearning. Whendiscussinglearningtolearninpolicylearning,the
conceptofmeta-learning(7)hasbeenwidelyexplored. Thegoalofmeta-RL(7;6;9;14)istolearn
apolicythatcanadapttoanynewtaskfromagiventaskdistribution. Duringthemeta-trainingor
meta-testingprocess,priormeta-RLmethodsrequirerewardsassupervisionforpolicyadaptation.
Meta-imitationlearning(8;5;25)addressesasimilarproblembutassumestheavailabilityofexpert
demonstrations. Diffusionmodelshavealsobeenusedinmeta-learning. Metadiff(28)modelsthe
gradientdescentprocessfortask-specificadaptationasadiffusionprocesstoproposeadiffusion-
basedmeta-learningmethod. Ourworkdepartsfromtheselearning-to-learnworks. Instead,weshift
thefocusawayfromdatadistributionsacrosstasksandsimplyleveragebehaviorembeddingsas
conditionalinputsforpolicysynthesisintheparameterspace.
6 Conclusion
Inthispaper,weintroducedanovelpolicygenerationmethodbasedonconditionaldiffusionmodels.
Targetingthegenerationofpoliciesinhigh-dimensionalparameterspaces,weemployanautoencoder
to encode and reconstruct parameters, incorporating a contrastive loss to learn efficient behavior
embeddings. Bypromptingwiththesebehaviorembeddings,ourpolicygeneratorcaneffectively
producediverseandwell-performingpolicies. Extensiveempiricalresultsacrossvariousdomains
9demonstrate the versatility of our approach in multi-task settings, the generalization ability on
unseen tasks, and the resilience to environmental randomness. Our work not only introduces a
freshperspectiveonpolicylearning,butalsoestablishesanewparadigmthatdelvesintothelatent
connectionsbetweenagentbehaviorsandpolicyparameters.
Limitation. Duetothevastnumberofparametersinvolved,wehavenotyetexploredlargerand
morediversepolicynetworks. Additionally,thecapabilitiesoftheparameterdiffusiongenerator
arelimitedbytheparameterautoencoder. Webelievethereissubstantialroomforfutureresearch
toexploremoreflexibleparametergenerationmethods. Itwouldalsobeinterestingtoapplyour
proposedgenerationframeworktogenerateotherstructures,furtherfacilitatingexplorationinpolicy
learningwithintheparameterspace.
References
[1] JacobBeck,MatthewThomasJackson,RistoVuorio,andShimonWhiteson. Hypernetworks
inmeta-reinforcementlearning. InKarenLiu,DanaKulic,andJeffIchnowski,editors,Pro-
ceedingsofThe6thConferenceonRobotLearning,volume205ofProceedingsofMachine
LearningResearch,pages1478‚Äì1487.PMLR,14‚Äì18Dec2023. 1,2,6
[2] RichardBellman. Amarkoviandecisionprocess. Journalofmathematicsandmechanics,pages
679‚Äì684,1957. 2
[3] VinodKumarChauhan,JiandongZhou,PingLu,SoheilaMolaei,andDavidAClifton. Abrief
reviewofhypernetworksindeeplearning. arXivpreprintarXiv:2306.06955,2023. 9
[4] LiliChen,KevinLu,AravindRajeswaran,KiminLee,AdityaGrover,MishaLaskin,Pieter
Abbeel,AravindSrinivas,andIgorMordatch. Decisiontransformer: Reinforcementlearning
viasequencemodeling. Advancesinneuralinformationprocessingsystems,34:15084‚Äì15097,
2021. 6
[5] YanDuan,MarcinAndrychowicz,BradlyStadie,OpenAIJonathanHo,JonasSchneider,Ilya
Sutskever,PieterAbbeel,andWojciechZaremba. One-shotimitationlearning. Advancesin
neuralinformationprocessingsystems,30,2017. 9
[6] YanDuan,JohnSchulman,XiChen,PeterL.Bartlett,IlyaSutskever,andPieterAbbeel.Rl$ÀÜ2$:
Fastreinforcementlearningviaslowreinforcementlearning. CoRR,abs/1611.02779,2016. 9
[7] ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadap-
tationofdeepnetworks. InInternationalconferenceonmachinelearning,pages1126‚Äì1135.
PMLR,2017. 1,2,9
[8] ChelseaFinn,TianheYu,TianhaoZhang,PieterAbbeel,andSergeyLevine. One-shotvisual
imitationlearningviameta-learning. InConferenceonrobotlearning,pages357‚Äì368.PMLR,
2017. 9
[9] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised
meta-learningforreinforcementlearning. arXivpreprintarXiv:1806.04640,2018. 9
[10] DavidHa,AndrewDai,andQuocVLe. Hypernetworks. arXivpreprintarXiv:1609.09106,
2016. 1,9
[11] TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine. Softactor-critic: Off-policy
maximumentropydeepreinforcementlearningwithastochasticactor. InICML,volume80of
ProceedingsofMachineLearningResearch,pages1856‚Äì1865.PMLR,2018. 5
[12] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inneuralinformationprocessingsystems,33:6840‚Äì6851,2020. 1,2
[13] GabrielBMargolisandPulkitAgrawal. Walktheseways: Tuningrobotcontrolforgeneraliza-
tionwithmultiplicityofbehavior. ConferenceonRobotLearning,2022. 5,6,14
[14] RussellMendonca,AbhishekGupta,RosenKralev,PieterAbbeel,SergeyLevine,andChelsea
Finn. Guidedmeta-policysearch. AdvancesinNeuralInformationProcessingSystems,32,
2019. 9
10[15] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastive
predictivecoding. arXivpreprintarXiv:1807.03748,2018. 4
[16] WilliamPeebles,IlijaRadosavovic,TimBrooks,AlexeiEfros,andJitendraMalik. Learningto
learnwithgenerativemodelsofneuralnetworkcheckpoints. arXivpreprintarXiv:2209.12892,
2022. 9
[17] SahandRezaei-Shoshtari,CharlotteMorissette,Fran√ßoisRobertHogan,GregoryDudek,and
DavidMeger. Hypernetworksforzero-shottransferinreinforcementlearning. InAAAI,pages
9579‚Äì9587.AAAIPress,2023. 1
[18] DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagation
andapproximateinferenceindeepgenerativemodels. InInternationalconferenceonmachine
learning,pages1278‚Äì1286.PMLR,2014. 5
[19] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684‚Äì10695,2022. 1
[20] Konstantin Sch√ºrholt, Boris Knyazev, Xavier Gir√≥-i Nieto, and Damian Borth. Hyper-
representationsasgenerativemodels: Samplingunseenneuralnetworkweights. Advancesin
NeuralInformationProcessingSystems,35:27906‚Äì27920,2022. 9
[21] DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-
che,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mas-
teringthegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484‚Äì489,
2016. 2
[22] ShagunSodhani,AmyZhang,andJoellePineau.Multi-taskreinforcementlearningwithcontext-
basedrepresentations. InInternationalConferenceonMachineLearning,pages9767‚Äì9779.
PMLR,2021. 2,6
[23] FarazTorabi,GarrettWarnell,andPeterStone. Behavioralcloningfromobservation. arXiv
preprintarXiv:1805.01954,2018. 6
[24] KaiWang,ZhaopanXu,YukunZhou,ZelinZang,TrevorDarrell,ZhuangLiu,andYangYou.
Neuralnetworkdiffusion,2024. 9
[25] Mengdi Xu, Yuchen Lu, Yikang Shen, Shun Zhang, Ding Zhao, and Chuang Gan. Hyper-
decisiontransformerforefficientonlinepolicyadaptation. arXivpreprintarXiv:2304.08487,
2023. 2,9
[26] RuihanYang,HuazheXu,YiWu,andXiaolongWang. Multi-taskreinforcementlearningwith
softmodularization. AdvancesinNeuralInformationProcessingSystems,33:4767‚Äì4777,2020.
2
[27] TianheYu,DeirdreQuillen,ZhanpengHe,RyanJulian,KarolHausman,ChelseaFinn,and
SergeyLevine. Meta-world:Abenchmarkandevaluationformulti-taskandmetareinforcement
learning. InConferenceonRobotLearning(CoRL),2019. 5
[28] Baoquan Zhang, Chuyao Luo, Demin Yu, Xutao Li, Huiwei Lin, Yunming Ye, and Bowen
Zhang.Metadiff:Meta-learningwithconditionaldiffusionforfew-shotlearning.InProceedings
oftheAAAIConferenceonArtificialIntelligence,volume38,pages16687‚Äì16695,2024. 9
[29] RuijieZheng,YongyuanLiang,XiyaoWang,ShuangMa,HalDaum√©III,HuazheXu,John
Langford, Praveen Palanisamy, Kalyan Shankar Basu, and Furong Huang. Premier-TACO
isafew-shotpolicylearner: Pretrainingmultitaskrepresentationviatemporalaction-driven
contrastiveloss. InForty-firstInternationalConferenceonMachineLearning,2024. 4
[30] Andrey Zhmoginov, Mark Sandler, and Maksym Vladymyrov. Hypertransformer: Model
generationforsupervisedandsemi-supervisedfew-shotlearning. InICML,volume162of
ProceedingsofMachineLearningResearch,pages27075‚Äì27098.PMLR,2022. 9
[31] YukeZhu,JosiahWong,AjayMandlekar,RobertoMart√≠n-Mart√≠n,AbhishekJoshi,Soroush
Nasiriany,andYifengZhu. robosuite: Amodularsimulationframeworkandbenchmarkfor
robotlearning. arXivpreprintarXiv:2009.12293,2020. 5
11Appendix
A ImplementationDetails
AllexperimentswereconductedonNVIDIA A40GPUs.
Autoencoder. Theautoencoderimplementationconsistsofathree-layerMLPencoderandadecoder.
Priortotraining,eachlayerofthepolicynetworkisflattenedandencodedseparately. Thefinalmean
andstdlayersareconcatenatedwiththemiddlelayerforencoding.
The hyperparameters used for the autoencoder are detailed in Table 1. On average, training an
autoencoderrequires5GPUhours.
Behaviorembedding.
Thebehaviorembeddingmodelconsistsoftwothree-layerMLPembeddings. Duringtraining,we
concatenatethestateandactionsequencesfromthefirstn=60steps(eachsequencehavingalength
of3)toformtheinputforthehembeddinglayer.Subsequently,weconcatenatethem=3statesafter
successasinputsforthevembeddinglayer. Bothembeddinglayersoutput128-dimensionalvectors.
Whenutilizingtheseembeddingsasconditionalinputs,weconcatenatethehandvembeddingsas
256-dimenionalconditions.
AllhyperparametersaboutthetrainingofthebehaviorembeddingscanbefoundinTable2. Asingle
trainingfortheembeddingsrequireslessthan1GPUhour.
Conditionaldiffusiongenerator. Ourdiffusionmodelemploysa1DconvolutionalU-Netarchitec-
tureasitsbackbone,utilizingbehaviorembeddingsasglobalconditions. Itoutputslatentparameter
representationswiththesamedimensionalityastheautoencoder‚Äôsoutput.
Trainingasinglediffusiongeneratorrequiresonly4GPUhours. Allrelevanthyperparametersare
detailedinTable3.
Hyperparameters Weconductallexperimentswiththissinglesetofhyperparameters.
Table1:HyperparametersforAutoencoder
Hyper-parameter Value
Table2:HyperparametersforBehaviorEmbedding
backbone MLP
Hyper-parameter Value
inputdim 22664
backbone MLP
hiddensize 1024
trajectorydim 1020
outputdim [2,1024]
successstatedim 117
encoderdepth 1
hiddensize 1024
decoderdepth 1
outputdim 128
inputnoisefactor 0.0001
batchsize 16
outputnoisefactor 0.001
optimizer AdamW
batchsize 32
learningrate 1e-4
optimizer AdamW
weightdecay 1e-4
learningrate 1e-3
trainingepoch 300
weightdecay 5e-3
lrscheduler CosineAnnealingWarmRestarts
trainingepoch 3000
lrscheduler CosineAnnealingWarmRestarts
B Experiments
B.1 TaskDescription
MetaWorld Descriptionsoftasksandrandominitialization:
Seentasks(Training):
12Table3:HyperparametersforDiffusionModel
Hyper-parameter Value Hyper-parameter Value
behaviorembeddingshape [256] kernelsize 3
parametershape [2,1024] noisescheduler DDIM
numinferencesteps 10 batchsize 128
embeddingdimindiffusionsteps 128 optimizer AdamW
learningrate 2.0e-4 beta [0.95,0.999]
eps 1.0e-8 weightdecay 5.0e-4
trainingepoch 1000 lr_scheduler cosine
lrwarmupsteps 500
‚Ä¢ windowopen: Pushandopenawindow. Randomizewindowpositions
‚Ä¢ dooropen: Openadoorwitharevolvingjoint. Randomizedoorpositions
‚Ä¢ draweropen: Openadrawer. Randomizedrawerpositions
‚Ä¢ dialturn: Rotateadial180degrees. Randomizedialpositions
‚Ä¢ faucetclose: Rotatethefaucetclockwise. Randomizefaucetpositions
‚Ä¢ buttonpress: Pressabutton. Randomizebuttonpositions
‚Ä¢ doorunlock: Unlockthedoorbyrotatingthelockclockwise. Randomizedoorpositions
‚Ä¢ handlepress: Pressahandledown. Randomizethehandlepositions
‚Ä¢ plateslide: Slideaplateintoacabinet. Randomizetheplateandcabinetpositions
‚Ä¢ reach: reachagoalposition. Randomizethegoalpositions
Unseentasks(Downstream):
‚Ä¢ windowclose: Pushandcloseawindow. Randomizewindowpositions
‚Ä¢ doorclose: Closeadoorwitharevolvingjoint. Randomizedoorpositions
‚Ä¢ drawerclose: Openadrawer. Randomizedrawerpositions
‚Ä¢ faucetopen: Rotatethefaucetcounter-clockwise. Randomizefaucetpositions
‚Ä¢ buttonpresswall: Bypassawallandpressabutton. Randomizethebuttonpositions
‚Ä¢ doorlock: Lockthedoorbyrotatingthelockclockwise. Randomizedoorpositions
‚Ä¢ handlepressside: Pressahandledownsideways. Randomizethehandlepositions
‚Ä¢ coffee-button: Pushabuttononthecoffeemachine. Randomizethepositionofthebutton
‚Ä¢ reachwall: Bypassawallandreachagoal. Randomizegoalpositions
Robosuite Descriptionsoftasks,robots,andrandominitialization:
Tasks:
‚Ä¢ Door: Adoorwithahandleismountedinfreespaceinfrontofasinglerobotarm. The
robotarmmustlearntoturnthehandleandopenthedoor. Thedoorlocationisrandomized
atthebeginningofeachepisode.
‚Ä¢ Lift: Acubeisplacedonthetabletopinfrontofasinglerobotarm. Therobotarmmustlift
thecubeaboveacertainheight. Thecubelocationisrandomizedatthebeginningofeach
episode.
‚Ä¢ NutAssembly-Single: Twocoloredpegs(onesquareandoneround)aremountedonthe
tabletop,andtwocolorednuts(onesquareandoneround)areplacedonthetableinfrontof
asinglerobotarm. Thegoalistoplaceeitheroneroundnutoronesquarenutintoitspeg.
Robots:
13‚Ä¢ Panda: Pandaisa7-DoFandrelativelynewrobotmodelproducedbyFrankaEmika,and
boasts high positional accuracy and repeatability. The default gripper for this robot is
thePandaGripper,aparallel-jawgripperequippedwithtwosmallfingerpads,thatcomes
shippedwiththerobotarm.
‚Ä¢ Sawyer:SawyerisRethinkRobotic‚Äôs7-DoFsingle-armrobot.Sawyer‚ÄôsdefaultRethinkGrip-
permodelisaparallel-jawgripperwithlongfingersandusefulforgraspingavarietyof
objects.
B.2 MoreResults
Inadditiontoreportingtheaverageperformanceofthetop5generatedresultsinthemainpaper,we
rigorouslydefine"qualifiedpolicies"asthoseachievinga100%successrateinthetestenvironment.
Table4presentstheproportionofqualifiedpoliciesamong100policyparametersgeneratedfrom
100trajectories. Notably,wemaintainanaveragequalificationrateofover30%onseentasks.
Furthermore, even on unseen tasks, we can generate high-performing policies using an average
ofonly20trajectories. Consideringthatourmethoddoesnotrelyonexpertdemonstrations, the
qualityandsuccessrateofourgeneratedpoliciessignificantlyenhancethesampleefficiencyofpolicy
learning.
Table4:QualifiedrateandsuccessrateofTop5/10modelsfromthegeneratedpoliceswith100trajectorieson
MetaWorld
SeenTasks windowopen dooropen draweropen dialturn plateslide buttonpress handlepress faucetclose
QualifiedRate 0.33¬±0.02 0.27¬±0.04 0.42¬±0.03 0.23¬±0.02 0.45¬±0.04 0.32¬±0.14 0.5¬±0.08 0.45¬±0.13
GeneratedTop5 1.0¬±0.0 1.0¬±0.0 1.0¬±0.0 0.82¬±0.09 1.0¬±0.0 0.87¬±0.10 1.0¬±0.0 0.98¬±0.01
GeneratedTop10 1.0¬±0.0 0.87¬±0.06 1.0¬±0.0 0.73¬±0.06 0.94¬±0.05 0.80¬±0.05 0.96¬±0.02 0.97¬±0.01
UnseenTasks drawerclose faucetopen buttonpress coffeebutton handlepress reachwall doorlock windowclose
wall side
QualifiedRate 0.55¬±0.04 0.16¬±0.06 0.11¬±0.01 0.08¬±0.03 0.04¬±0.03 0.13¬±0.05 0.13¬±0.04 0.10¬±0.01
GeneratedTop5 1.0¬±0 1.0¬±0 0.97¬±0.02 0.74¬±0.21 0.45¬±0.17 0.94¬±0.03 1.0¬±0.0 0.92¬±0.05
GeneratedTop10 1.0¬±0 0.85¬±0.13 0.95¬±0.02 0.64¬±0.23 0.30¬±0.12 0.72¬±0.11 0.85¬±0.05 0.85¬±0.08
B.3 DetailsofReal-worldRobots
Inthissection,wedetailthereal-worldrobotapplicationsofourmethod. Wedeploysynthesized
policiesontheUnitreeGo2quadruped,designingdiversereal-worldtestingenvironmentstoevaluate
twokeyaspectsofagentperformance: (1)stabilityduringhigh-speedturningandbackwardlocomo-
tion,and(2)robustnessofmovementsonuneventerrain(mats). Ourdeploymentprocessconsistsof
fourkeysteps:
‚Ä¢ ObtainactornetworkparametersandcorrespondingtesttrajectoriesfromIsaacGymsimula-
tions,wheretheactorsaretrainedusingwalk-these-ways(13).
‚Ä¢ TrainMake-An-Agentusingtheacquiredtrainingdata.
‚Ä¢ GenerateactornetworksfromrandomlysampledIsaacGymtrajectories,coveringavariety
oftrainingperiods.
‚Ä¢ Equip the Unitree Go2 quadruped with the generated actors and a pretrained adaptor,
enablingittocompletedesigned,challenginglocomotiontasks.
14