Can Textual Semantics Mitigate Sounding Object
Segmentation Preference?
Yaoting Wang1† , Peiwen Sun2† , Yuanchao Li3 , Honggang Zhang2 , and
Di Hu(cid:66)1,4
1 Gaoling School of Artificial Intelligence, Renmin University of China, China
yaoting.wang@outlook.com
dihu@ruc.edu.cn
2 Beijing University of Posts and Telecommunications, Beijing, China
{sunpeiwen,zhhg}@bupt.edu.cn
3 University of Edinburgh, Scotland, UK
yuanchao.li@ed.ac.uk
4 Engineering Research Center of Next-Generation Search and Recommendation
Abstract. The Audio-Visual Segmentation (AVS) task aims to seg-
ment sounding objects in the visual space using audio cues. However,
in this work, it is recognized that previous AVS methods show a heavy
reliance on detrimental segmentation preferences related to audible ob-
jects, rather than precise audio guidance. We argue that the primary
reason is that audio lacks robust semantics compared to vision, espe-
cially in multi-source sounding scenes, resulting in weak audio guidance
overthevisualspace.Motivatedbythethefactthattextmodalityiswell
exploredandcontainsrichabstractsemantics,weproposeleveragingtext
cuesfromthevisualscenetoenhanceaudioguidancewiththesemantics
inherent in text. Our approach begins by obtaining scene descriptions
through an off-the-shelf image captioner and prompting a frozen large
language model to deduce potential sounding objects as text cues. Sub-
sequently,weintroduceanovelsemantics-drivenaudiomodelingmodule
withadynamicmasktointegrateaudiofeatureswithtextcues,leading
to representative sounding object features. These features not only en-
compass audio cues but also possess vivid semantics, providing clearer
guidance in the visual space. Experimental results on AVS benchmarks
validate that our method exhibits enhanced sensitivity to audio when
aided by text cues, achieving highly competitive performance on all
three subsets. Project page: https://github.com/GeWu-Lab/Sounding-
Object-Segmentation-Preference
Keywords: Audio-Visual Segmentation · Textual Guidance · Multi-
modal learning
† Equal contribution.
(cid:66) Corresponding author.
4202
luJ
51
]VC.sc[
1v74901.7042:viXra2 Y. Wang et al.
Fig.1: The previous methods (left) achieve satisfactory results in receiving normal
audio input. However, even when the sound is completely silent, they still segment
vast pixels to represent the guitar regarding the segmentation preference of audible
objects built during the training. In contrast, our approach (right) utilizes a frozen
LLM to reason from scene descriptions that the male is not playing the guitar, as his
hands are off the strings. Guided by this semantic information, our method produces
more precise segmentation with finer audio guidance.
1 Introduction
In human perception, visual and auditory senses are closely related, and au-
dio can provide vision with additional and dynamic scene information to en-
hance visual understanding. In the context of audio-visual understanding tasks,
Audio-Visual Localization (AVL) enables the localization of sounding objects
within visual scenes using audio references [15,36]. However, the coarse local-
ization of sounding objects is no longer sufficient to meet the practical demands
of complex scene understanding, such as in autonomous driving [12,49] and
augmented reality [33]. To address this challenge, tasks are gradually shifting
from bounding-box or rough heat-map localization to finer-grained pixel-level
segmented localization, which is known as Audio-Visual Segmentation (AVS).
The existing works on AVS can be broadly categorized into fusion-based
methods [8,16,20,21,29,47] and prompt-based methods [30,34,40]. The for-
mer primarily focuses on localizing sounding objects by fusing audio and visual
features, while the latter emphasizes constructing effective audio prompts for
the visual foundation model. However, most of these methods fail to establish
compact audio-visual correlations to achieve clear audio guidance in the visual
space. As depicted in the left of Fig. 1, previous methods could achieve satis-
factory results with normal audio input. Nevertheless, when the audio is silent,Sounding Object Segmentation Preference 3
these methods still segment many pixels to represent potential sounding objects
based on the segmentation preferences of audible objects1.
Furthermore, from the experiment with manually muted audio input, we
observethatmostofthebehaviorofAVSbaselinesdependsonthesegmentation
preferenceestablishedduringthetrainingprocessratherthanrelyingonreliable
audio-visual correlation. For instance, these models tend to take a shortcut by
segmenting out the muted guitars because the audio guidance is weak, and they
simplylearnedwiththeeasy-to-learnvisualfeaturealone(i.e.,shortcutlearning)
thatguitars areoften associatedwith sound-emittingduring thetraining phase.
The unreliable audio-visual correlation can be attributed to two main factors.
Firstly,thescarcityoftrainingdataforAVSisasignificantchallengeduetothe
demanding nature of pixel-level annotation. Secondly, the audio modality itself
presents inherent complexity and ambiguity [16,21,29], especially in scenarios
involving multiple audio sources that may be intertwined.
Given the challenges discussed above, our approach aims to build a robust
audio-visual correlation by integrating the text modality, which inherently pro-
vides meaningful and robust semantic information about objects and their in-
teractions. Firstly, in the Scene Describing phase, we utilize an off-the-shelf
image captioner (LLaVA-1.5 [28]) to generate detailed dense captions for scene
understanding. Then, in the Sounding Objects Reasoning phase, we instruct
the frozen large language models (LLMs, e.g., LLaMA2 [39]) as a text cues cap-
turer to collect potential sounding objects from the scene description as text
cues.Notably,weintroduceanelaboratefew-shotprompttemplatetoguidethe
text cues capturer to reason with Chain-of-Thought (CoT) instructions. After
that, in the Semantics-Driven Audio Modeling (SeDAM) module, we project
the audio feature into latent audio features. However, instead of directly us-
ing these latent features to interact with visual features for mask decoding, we
leverage these latent features to interact with text cues. Specifically, the latent
features are attended to and collected to form sounding object features with
semantics provided by the text cues through the crossmodal transformer. We
further propose the Prompting Mask Queries with Semantics (PMQS) module
tointroducethesoundingobjectfeaturesintopre-trainedmaskqueriesforlater
segmentation. Finally, we use simple but effective bottleneck adapters in the
Audio-Prompted Decoding phase for better segmentation quality. The exper-
imental results demonstrate that our proposed method improves audio-visual
grounding by incorporating guidance from both audio and text cues.
Experiments in Sec. 4.4 reveal the advantage of our method, as verified by
its increased sensitivity to the changes in audio input compared to networks
withoutenhancingaudioguidancewithtextcues.Insummary,ourcontributions
are threefold:
– We recognize the behavior of AVS model is influenced by the preference
segmentationphenomenon,whichcannotbesolvedeasilybypreviousaudio-
1 Tobeclear,wedefineaudibleobjectsasobjectscapable ofproducingsound,while
sounding objects are defined as objects that are currently producing sound.4 Y. Wang et al.
visual based methods. A novel strategy introduces textual semantics as the
bridge into the AVS task, establishing a better audio-visual correlation.
– We introduce the SeDAM module to form sounding object features with
audio and text cues. We also suggest a PMQS module to prompt the pre-
trainedmaskquerieswiththesesoundingobjectfeaturesforsoundingobject
segmentation using a visual foundation model.
– Our experiments show that our method can be more sensitive to the audio
guidance, manifested in its high sensitivity to the changes in audio input.
2 Related Works
2.1 Audio-Visual Localization and Segmentation
TraditionalAVLtasks[15,35,36]predictthepositionsofsoundingobjectsusing
bounding-boxorcoarseheat-mapsthroughunsupervisedlearningofaudio-visual
correlation. In recent years, driven by the growing demand for more precise lo-
calization in industries such as autonomous driving [12,49] and augmented real-
ity[33],theAVLtaskhasevolvedtowardsafiner-grainedAVStaskthatlocalizes
sounding objects at a pixel-level. The existing AVS works can be broadly cat-
egorized into fusion-based [8,16,20,21,29,43,47] and prompt-based [30,32,40]
The pioneering AVS work [47] is fusion-based and adopts a multi-stage strategy
to integrate audio with multi-scale visual features. Liu et al. [29] address the
issues of inadequate fusion of audio-visual features with an audio-aware query-
enhancedtransformer(AuTR).Incomparison,AVSegFormer[8]directlydecodes
thefusedfeaturewiththeaudioquery.Wanget al.[40]innovativelypromptthe
visualfoundationmodelwithaudioinput,harnessingtheabundantvisualbefore
achievinggeneralizableAVSinzero-shotandfew-shotscenarios.However,these
methods have not effectively established a reliable audio-visual correlation, pre-
ventingthemodelsfromperceivingrobustaudioguidance.Onthecontrary,our
experimentsinSec.4.4alsorevealthatsomeAVSbaselines’competitiveperfor-
mance may stem from segmentation preferences formed during the training.
2.2 Text Aided Scene Understanding
BeforetheexplosionofLLM,textualsemanticinformationhadbeenwidelyused
for visual understanding tasks. Zhan et al. [38] and Sharma et al. [37] improve
Visual Question Answering by using the extra semantic information of image
captions, while Hur and Park [17] utilize the image captioner to help zero-shot
image classification. Wang et al. [41] incorporating text with audio and visual
cues to solve reference AVS task.
In recent years, the community has witnessed an emergent interest in strong
off-the-shelf capabilities in open-world visual understanding [24–26,45]. Notable
examples, such as GPT4(V) [44] and LLaVA [27] have showcased remarkable
linguisticandvisualcapabilitiesinzero-shotsceneunderstanding[7,42]without
any training requirement. Innovatively, in the field of AVS, BAVS [23] replacesSounding Object Segmentation Preference 5
Fig.2:OverallpipelineoftheproposedTeSOmethod.Weutilizeanoff-the-shelfimage
captionerfordensescenedescribingandemployafrozenLLMtoreasonoutpotential
soundingobjectsastextcues.Thesesemantictextcuesarethenaggregatedwithaudio
featuresintheSeDAMmoduletoformthesoundingobjectfeatures.Subsequently,we
introduce the sounding object features into pre-trained mask queries in the PMQS
module. Finally, we use adapters to tune the visual-only mask decoder for AVS in
theAudio-promptedDecodingphase.“MSDA” isthemulti-scaledeformableattention
proposed by Zhu et al. [48].
audio input with textual audio tags from a large pre-trained vision (UniDif-
fuser [2]) and audio (BEATs [3]) foundation model, establishing a reliable AVS
system with foundation knowledge. However, due to the significant gap in class
distributionbetweenthepre-trainingdatasetandtheAVSdataset,aswellasthe
presence of multi-source scenarios in the AVS task, the extracted audio tags are
notidealtoprovideaccuratesemantics.Incontrast,weutilizethetextmodality
withrobustsemanticfeaturestoenhanceratherthanreplacetheaudioguidance
and address the segmentation preference issue.
3 Text-guided Sounding Object Segmentation
As shown in Fig. 2, we introduce our Text-guided Sounding Object Segmenta-
tion (TeSO) method in this section. Our experiments about muted audio input
imply that audio does not exert reasonable guidance over the visual space for
some AVS baselines, resulting in the models relying more on the segmenta-
tion preferences established during the training process. In essence, they learn
which objects are most likely audible during training and take shortcuts during
inference. To address the above issue, our method aims to enhance the audio-
visual correlation by leveraging the text modality, which inherently possesses
robust semantic information, to obtain finer-grained audio guidance. We begin
byacquiringdetailedscenedescriptionsthroughanoff-the-shelfimagecaptioner.
Subsequently,afrozenLLMworksasthetextcuescapturer,collectingpotential
soundingobjectsastextcuesfromthescenedescriptionswithCoTinstructions.6 Y. Wang et al.
Finally,weintroduceanovelSeDAMmodulewithadynamicmasktoseamlessly
integrate audio features and text semantics through a crossmodal transformer,
providing finer audio guidance with text semantic cues.
3.1 Multimodal Representation
Visual:Followingpreviousworks[40,47],wesampleframesfromthevideoat1-
secondintervals.Mask2Former[4]servesasourvisualfoundationmodel,andwe
extractvisualfeaturesF
V
∈RdV×H×W fromapre-trainedSwintransformer[6].
We use simple two-layer MLP adapters to tune the visual encoder for better
visual representations.
Audio:Similartothevideoprocessing,wesplittheaudiointoclipsat1-second
intervals. We extract audio features F
As
= {F A1,F A2,...,F AT} ∈ RT×dA using
VGGish [9,13], where T represents the length of the audio in seconds, match-
ing the number of video frames. Therefore, each video frame corresponds to
F =F [i]. We freeze the pre-trained parameters for a better comparison with
A As
previous works.
Text: For each video frame, we complement the captured text cues into a sen-
tence using a template (“This is a {_}.”) to extract text features F
T
∈RNT×dT
with ImageBind [10], where N is the number of text cues.
T
3.2 Text Cues Generation
To obtain more precise text cues, we use a two-stage inference flow. Firstly, we
employ an off-the-shelf image captioner to generate dense scene descriptions.
Secondly, we ask a text cues capturer to collect potential sounding objects from
the generated descriptions.
Scene Describing. Asshowninthetop-leftofFig.3,inthisphase,weemploy
the commonly used and straightforward prompt: “Please carefully understand
the image content and provide as rich a description as possible.” This instructs
theimagecaptionertogenerateadetaileddescriptionofthevisualscene,atask
commonly referred to as dense captioning [18]. This process enables the image
captioner to produce comprehensive and informative descriptions that not only
describe the objects like “a human and guitar in the image” but also capture
their interactions and relationships.
Sounding Object Collecting. Toextractpotentialsoundingobjectsfromthe
generated dense descriptions, we design a task-specific CoT prompt template
with few-shot demonstrations, as illustrated in Fig. 3. Specifically, we carefully
craftexamplesthatcovervariousscenariosandprovidereasoningapproachesfor
eachexample.Theprocessbeginsbyidentifyingtheobjectspresentinthedense
scene descriptions, followed by step-by-step common-sense reasoning according
to the CoT instructions, as depicted in the lower right of Fig. 3. During the
CoT reasoning, the text cues capturer first categorizes the sample objects andSounding Object Segmentation Preference 7
Fig.3: “P.S.O” stands for “Potential Sounding Object”. A frozen LLM reasoner works
asatextcuescapturerbyconsideringtheinteractionofaudibleobjects.Forinstance,
apersonmaysingasongwhileplayingtheguitar,buthewouldnotsingalongwitha
saxophone. In contrast, a noun parser simply captures any nouns that are present.
assesseswhetherobjectsinthatcategoryarelikelymakingsounds.Subsequently,
the capturer analyzes which group of potential sounding objects can produce
sounds simultaneously. Finally, the group with higher confidence is captured as
the text cues of length N and encoded in F for further guidance. The right
T T
partofFig.3alsoillustrateswhyweusethefrozenLLMasatextcuescapturer
insteadofusinganaivenounparser.ItisimportanttonotethattheuseofLLM
is solely for generating high-quality text cues, the LLM itself is not engaged in
the training process for a fairness concern.
3.3 Semantic-Driven Audio Modeling
After obtaining potential sounding objects, we can enhance the semantics of
audio features with these text cues in this SeDAM module. To allow semantic
text cues to capture different aspects of audio features F , we project F into
A A
distinct latent features F
AL
= {a i}N i=L
1
∈ RNL×dAL, where N
L
is the number
of latent features. It is worth noting that, we incorporate an auxiliary loss term
L aimsatensuringthedistinctionamonglatentaudiofeatures.Thisway,
infoNCE
we project audio features into the latent space, where each unique a represents
i
a latent audio feature to be captured by semantic text cues, forming clear and
loud sounding object features with robust semantics for finer guidance.
Toincorporatethesemanticsoftextcues,weemploytextcuesasthequeryto
attend and combine the relevant latent features through crossmodal multi-head
attention, formulated as:
F =MultiHead(F ,F ,F ), (1)
A(cid:101)L T AL AL8 Y. Wang et al.
whereF ∈RNT×dA istheattendedlatentaudiofeaturesandwillbecombined
A(cid:101)L
with text cues F as follows:
T
F =FC(LN(F +F )), (2)
AT T A(cid:101)L
where F
AT
∈RNT×dA is the combined feature with both audio information and
semantic text cues, and FC and LN for linear layer and layer normalization.
Note that the LLM reasoner may generate a different number of text cues, and
we either pad with a zero matrix or truncate to N . It is noteworthy that we
T
propose task-specific CoT instructions to prompt the text cues capturer to infer
more accurate potential sounding objects as the text cues. Additionally, we also
introduce a dynamic mask based on attention score to mitigate the potential
noisy information.
We first compute the average pooling attention scores ζ ∈RNT for text cues
on the attention head and key dimension:
(cid:32) (cid:33)
F FT
ξ =Pool T AL , (3)
(cid:112)
d
AL
(cid:112)
where d isthescalingfactorfordotproduction.Thenweformtheattention
AL
mask M={m ,m ,m ,...,m } as follows:
1 2 3 NT
(cid:40)
0 if ξ >0
M = i , (4)
i
−∞ otherwise
where M ∈ RNT, will be used as a key padding mask for the PMQS module
in the next section to ensure that the attention weight of redundant text cues
approaches zero.
3.4 Prompting Mask Queries with Semantics
Current segmentation foundation models [4,5,19] adopt transformer-based de-
coderwithmaskqueriesasthemainstreamapproach.Topromptsuchfoundation
models, we propose the PMQS module, aiming to introduce semantic informa-
tion to mask query features F
Q
= {q 1,q 2,...,q NQ}, where F
Q
∈ RNQ×dV, and
N is the number of mask queries in crossmodal attention. We formulate this
Q
updated expression as follows:
(cid:32) (cid:33)
F FT
ϕ (F ,F ,F )=Softmax Q AT +M F , (5)
ca Q AT AT (cid:112)
d
AT
AT
where M is added to the attention score to mask the redundant text cues for
each query. Then we obtain the updated mask query features F(cid:101)Q ∈RNQ×dV as:
F(cid:101)Q =F Q+ϕ ca(F Q,F AT,F AT). (6)Sounding Object Segmentation Preference 9
3.5 Audio-Prompted Decoding
As shown on the right side of Fig. 2, the updated mask query features F(cid:101)Q are
forwarded into the transformer-based decoder, generating mask queries M ∈
Q
RNQ×H×W and class queries C
Q
∈ RNQ×NC, where N
C
is the number of class
queriespre-definedinthesegmentationfoundationmodel.Subsequently,wecan
obtain the final segmentation M
pred
∈RNC×H×W:
M
pred
=Einsum(C Q,F(cid:101)Q), (7)
where Einsum(·) is the Einstein Summation Convention.
Notably,toenhancethequalityofthesegmentation,weusetwo-layerbottle-
neckMLPadapters[14]tolightlyfine-tunetheoutputprojectionofthefollowing
attentions: a) Swin Window Attention (W-MSA) [31], b) Deformable Attention
in pixel decoder [48] and c) Masked Attention in transformer decoder [4].
3.6 Learning Objectives
Segmentation Loss.Duringthemodeltrainingprocess,weemploythebinary
cross-entropy loss and dice loss to optimize the mask quality:
L =λ ·L +λ ·L . (8)
mask bce bce dice dice
Further, we adopt the mask classification loss [5] to compose the final segmen-
tation loss:
L =L +λ ·L . (9)
seg mask cls cls
Latent Component Loss.Thislosstermaimstoexplicitlyguidethemodelto
learn distinct representations for each latent audio component, helping prevent
redundancy and overlapping information across the latent components.
(cid:32) (cid:33)
exp(sim(a,a+))
L =−log , (10)
infoNCE (cid:80)NL
exp(sim(a,a ))
i=1 i
where a+ and sim(·) denote the positive example and the cosine similarity.
Total Loss The final loss function is composed of the weighted sum of the two
kinds of losses above:
L=L +λ ·L . (11)
seg infoNCE infoNCE
4 Experiments
4.1 Implementation Details
Dataset.OurproposedmethodisevaluatedontheAVSBenchmarks[47],which
contains three subsets. Firstly, the single-source subset (V1S) contains 4932
videos over 23 categories, covering various sounds, such as humans, animals,10 Y. Wang et al.
vehicles, and musical instruments. For videos in the training split of this sub-
set,onlythefirstsampled frameisannotated.Secondly,themulti-sourcesubset
(V1M) contains 424 videos that include two or more categories from V1S, and
all sounding objects are visible in the frames. Finally, the AVSBench-semantic
(AVSS) subset, an extension of V1S and V1M, contains 12,356 videos. These
newly collected videos are trimmed to 10 seconds, differing from the 5 seconds
in V1S and V1M.
Setting. We conduct training and evaluation using the VGGish backbone pre-
trained on Youtube-8M [1] and Swin-base Transformer backbone pretrained on
semantic-ADE20K [46] by Mask2Former [4]. The number N of the crossmodal
transformerlayersissetto4,andtheparametersλ ,λ ,λ ,λ inthe
bce dice cls infoNCE
loss are set to 5,5,2,1. The AdamW optimizer is adopted with a learning rate
of 1e-4 for the visual encoder adapters and 1e-3 for other learnable parameters,
and the training epoch is roughly set to 60.
Metrics. To conduct a comprehensive evaluation of our model, we carry out
testsusingmeanIntersectionoverUnion(mIoU)andF-scoreastheperformance
metrics, following previous works [8,47].
4.2 Comparison Results
When compared to methods that do not incorporate text as additional seman-
tic information, our method, which leverages captioning and reasoning, demon-
strates strong competitiveness. As illustrated in Sec. 4.2, our method achieves
comparable results across all V1S, V1M, and AVSS-binary subsets, showcas-
ing average performance improvements of up to 1.25% in mIoU and 2.7% in
F-score, respectively. Furthermore, qualitative analysis indicates that our pre-
dicted masks exhibit superior quality, as demonstrated in sub-figure (1) of Fig.
4 (more examples in supplementary materials).
It is necessary to mention that our proposed TeSO does not show signifi-
cant improvements on V1S over certain existing methods, such as BAVS [23],
which also utilizes text semantic information. Apart from marginal effects, this
phenomenon can also be attributed to the singular sound source and simplistic
natureofthedatascenariospresentinV1S.OnV1M,however,ourperformance
significantly surpasses that of other methods, including BAVS.
In addition to the AVS task, we also conduct experiments on the Audio-
Visual Semantic Segmentation (AVSS) task, as shown in Tab. 2. Our model
exceeds the best performance in previous models by 5.37% on mIoU, which
further demonstrates that our model is capable of establishing a better audio-
visual correlation.
4.3 Bottlenecks and Fairness
Our method (TeSO) collects text cues from dense image descriptions at REC
#1
(80.41%) and a filter with a dynamic mask at REC (80.95%), indicating a
#2
robust startup for the text-guided AVS framework, in comparison with another
two-stage model, BAVS.Sounding Object Segmentation Preference 11
Table1:PerformanceonAVS-Benchmarks.Theunderscoreisusedtoindicatesuboptimalresults.BAVS
useslargepre-trainedvisualandaudiofoundationmodelstoreplaceaudioinputwithtexttag.Incom-
parison,TeSO(ours)enhancesratherthanreplacesaudioguidancebasedontherobustsemanticsoftext.
ItisevidentthatourmethodhasagoodF-scoreresultwhileensuringmIoU.
V1S V1M AVSS-binary
Method Audio-backboneVisual-backbone
mIoU(%)F-scoremIoU(%)F-scoremIoU(%)F-score
AVSBench[47] VGGish PVT-v2 78.70 0.879 54.00 0.645 62.45 0.756
AVSegFormer[8] VGGish PVT-v2 82.06 0.899 58.36 0.693 64.34 0.759
AVSC[22] VGGish PVT-v2 81.29 0.886 59.50 0.657 - -
AVS-BG[11] VGGish PVT-v2 81.71 0.904 55.10 0.668 - -
AQFormer[16] VGGish PVT-v2 81.60 0.894 61.10 0.721 - -
†CATR[20] VGGish PVT-v2 81.40 0.896 59.00 0.700 - -
AV-SAM[34] ResNet18 ViT-Base 40.47 0.566 - - - -
SAMA[30] VGGish ViT-Huge 81.53 0.886 63.14 0.691 - -
GAVS[40] VGGish ViT-Base 80.06 0.902 63.70 0.774 67.70 0.788
AuTR[29] VGGish Swin-Base 80.40 0.891 56.20 0.672 - -
MUTR[43] VGGish Video-Swin-Base 81.60 0.897 64.00 0.735 - -
BAVS[23] ‡BEATs Swin-Base 82.68 0.898 59.63 0.659 55.45 0.640
TeSO(ours) VGGish Swin-base 83.27 0.933 66.02 0.801 68.53 0.813
†:Tomakeafaircomparison,theresultsofCATRherearewithoutsupplementedannotationofthe
trainingset.
‡:BEATsisastrongeraudiobackbone,trainedonAudioSetdatasetwith90Mparameters.Incomparison,
VGGishistrainedonYoutube-8Mwith70Mparameters.
Table 2: PerformanceonAVSSdataset. Table 3: Potential sounding objects
Our method showcases a substantial recognition(REC)performanceatdiffer-
improvement in the semantic subset, ent model stages and the corresponding
achieved by leveraging robust text se- segmentation results. Experiments are
mantics to enhance audio guidance. conducted on AVS-V1M test set.
AVSS REC REC SEG
Method Backbone Method (#1) (#2)
mIoU(%)F-Score AP(%) AP(%) mIoU(%)
AVSBench PVT-v2 29.80 0.352 BAVS 24.22 79.69 59.63
CATR PVT-v2 32.80 0.385 TeSO (ours) 77.96 78.33 63.87
p
BAVS Swin-base 33.59 0.375 TeSO (ours) 77.82 78.27 63.31
n
TeSO(Ours)Swin-base 38.96 0.451 TeSO(ours) 80.41 80.95 66.02
In addition, we develop several variants of TeSO to investigate its perfor-
mance sources and bottlenecks, as well as its fairness in comparison with other
methods. TeSO denotes the variant using one-shot prompts, while TeSO de-
p n
notesthevariantemployinganounparserinsteadofafrozenLLMfortextcues
collection. As shown in Tab. 3, the results demonstrate that both the number
(shots) of demonstrations and the method of text cues collection significantly
impact the model’s ability to accurately identify audio semantics and perform
segmentation (SEG). Thus, utilizing few-shot prompts and off-the-shelf founda-
tion models are both essential for ensuring the acquisition of high-quality text
cues (i.e., the bottleneck).
However, we would like to emphasize that, even without using LLM to
capture text cues, our method variant (TeSO ) still fairly achieves competitive
n
performance (63.31% mIoU) compared with BAVS (59.63% mIoU) and state-
of-the-art method GAVS (63.70% mIoU) in the complex multi-source scenario
(AVS-V1M), and significantly outperforms other methods. This suggests that12 Y. Wang et al.
Table 4: Impact on mIoU and F-score by muted audio or noise-only audio on AVS-
V1M test set. TeSO (ours) exhibits the most intensive attenuation up to 48.8%. It is
noteworthy that the other models still generate “satisfactory” results even in silent or
purelynoisyenvironments.Thisobservationsuggeststhatthemodelsmightrelymore
onthesegmentationpreferencesformedduringthetrainingratherthantrulylearning
efficient audio guidance.
Mute WGN-10dB WGN-40dB
Method mIoU%(∆m%)F-score(∆f%)mIoU%(∆m%)F-score(∆f%)mIoU%(∆m%)F-score(∆f%)
AVS-Bench 49.15(6.6) 0.622(4.9) 46.76(11.1) 0.618(5.5) 47.46(9.8) 0.610(6.7)
GAVS 42.53(29.2) 0.695(8.4) 41.51(34.8) 0.692(10.6) 41.87(30.3) 0.696(8.4)
BAVS 45.06(26.1) 0.644(5.8) 43.27(27.4) 0.601(8.8) 42.93(31.6) 0.595(9.7)
TeSO(ours) 37.89(42.6) 0.671(16.2) 34.20(49.6) 0.661(17.5) 34.74(47.4) 0.656(18.1)
leveraging text-enhanced audio for visual control with robust semantics is a
viable and promising direction.
4.4 Audio Control
Based on our observations, we notice a distinct shape of the predicted mask in
sub-figure (2) of Fig. 4 when some popular AVS baselines are used for infer-
ence. Interestingly, this shape remains distinguishable even when the inference
is conducted with muted audio. This suggests that the network can predict
rough masks without relying on audio input, indicating that the audio-visual
grounding is not effectively guided by audio cues. As a result, this undesirable
phenomenon can be attributed to the segmentation preference, which is a result
of these models lacking proper audio control.
To evaluate the influence of audio control on segmentation, various models
are subjected to inference using muted audio and noise-only audio (at 10 dB
and 40 dB) instead of the corresponding audio signals. It should be noted that
the visual features remained unchanged throughout the evaluation. As shown
in Tab. 4, the disparity ∆ (and ∆ ) refers to the difference in mIoU (and F-
m f
score) values before and after the audio substitution is applied as an evaluation
metric for the sensitivity of audio changes. A larger drop in mIoU and F-score
indicates a greater degree of audio control over the visual space. In addition to
demonstrating strong segmentation capabilities in the AVS task, TeSO (ours)
exhibits the highest level of attenuation, reaching up to 47.4%, when subjected
to muted and noise-only audio. Additionally, qualitative analysis revealed that
thepredictedmasksremainedpredominantlyblank,asdepictedinsub-figure(2)
ofFig.4(moreexamplesinsupplementarymaterials).Thisobservationindicates
that our model has better audio guidance over the visual space, enabling it to
play a crucial role in audio-visual grounding.
4.5 Ablation Study
As shown in Tab. 5, we conduct ablation experiments targeting the designed
TeSO modules and the guidance from different modalities. To explore the guid-
ing ability, we progressively remove either the audio or text guidance for TeSO.Sounding Object Segmentation Preference 13
Fig.4: Examples of the impact of normal audio input and all-mute audio on popular
methods.Innormalscenarios,ourmethodshowsbettermasksthanpreviousmethods.
Inall-mutescenarios,ourapproachexhibitsstrongsensitivitytowardsaudioinputs,as
it is capable of generating blank masks for silent audio clips.
Theexperimentsindicatethatcontrolfrombothaudioandtextinputsforvisual
space can establish a strong audio-visual correlation. Furthermore, by utilizing
our proposed TeSO, we achieve enhanced audio guidance and reach a 66.02%
mIoU while incorporating precise semantic information from the text modality.
Moreover, through the utilization of our proposed TeSO method that incorpo-
ratesrobustsemanticinformationfromthetextmodality,weachievedimproved
audio guidance and attained a mIoU of 66.02%.
As shown in Tab. 6, we also implement a baseline that directly filters the in-
ferred semantic labels during the model evaluation process by intersecting them
with the potential sounding object collected by the frozen LLM capturer. This
implies that no text cues are utilized to enhance the audio guidance for better
Table 5: Ablation study on AVS- Table6:Comparisonofourproposedbase-
V1M.Ourmethodbenefitsfromusing line and TeSO. The baseline method em-
bothaudioandtextastheguidingin- ploys LLM to filter the predicted results
formation for sounding objects. but does not enhance the audio guidance
with text cues. Experiments are conducted
Control mIoU(%)F-score on AVS-V1M test set.
TeSO 66.02 0.801
(-)LinfoNCE 65.34 0.793
Normal Mute WGN-40dB
(-)DynamicMask 64.78 0.781
(-)PMQS 62.90 0.774
mIoU%mIoU%(∆m%)mIoU%(∆m%)
Baseline 64.15 43.37(32.4) 43.09(32.8)
(-)SeDAM 62.22 0.760
TeSO 66.02 37.89(42.6) 34.74(47.4)
(-)Text 62.28 0.768
(-)Audio 42.11 0.692
(-)Text&Audio 37.35 0.65614 Y. Wang et al.
controloverthevisualspace.Asexpected,thesensitivity(∆ )ofmodelperfor-
m
mancetochangesinaudioinputhasdecreasedcomparedwithTeSO,indicating
the effectiveness of our method.
5 Discussion
Noisy text cues. Besides dense image captions for collecting semantic text
cues, we also constructed a dynamic mask in Eq. (4) to further select effective
semantics based on audio correspondence. We use audio recognition results to
determinetheaccuracyoftextcues.PleaserefertoTab.3tocomparetheaudio
recognition results based on text cues collection stage (REC ) and dynamic
#1
mask filtering stage (REC ). Our approach achieves higher audio recognition
#2
performance, compared with using the audio foundation model in BAVS. This
indicates the reliability and accuracy of our strategy for obtaining text cues.
Prompt with CoT. As shown in the Fig. 3, we utilize task-specific prompt
templates with multiple scenario demonstrations and CoT instructions to as-
sist the frozen LLM in collecting more accurate potential sounding objects as
text cues. For the detailed prompt templates with CoT instructions and related
experimental information, please refer to the Prompt Template part and Text
Assistance part in the supplementary materials.
Versatility.WeexploreanalternativevisualfoundationmodelliketheSegment
Anything Model [19] for segmentation. Conducting experiments using a ViT-
based backbone with minimal adjustments, our model achieves an impressive
65.36%mIoUonV1M,stillgettingcomparableperformance.Thisdemonstrates
theversatilityandeffectivenessofourmodel.Additionalexperimentdetailsand
analysis can be found in the supplementary materials.
6 Conclusion
This study represents a pioneering effort to incorporate text cues into the AVS
task. Unlike previous approaches that solely rely on audio assistance, our TeSO
method establishes a stronger audio-visual correlation by harnessing the robust
semantics unique to the text modality By integrating CoT instructions and
common-sense reasoning, we enhance the ability to capture potential sounding
objects from detailed scene descriptions, thereby optimizing the utilization of
text cues. Furthermore, we have developed a dynamic mask based on attention
scores to effectively handle noisy information present in the text cues. Our com-
prehensive experiments highlight the effectiveness of our approach, demonstrat-
ing comparative performance across all AVS Benchmarks. In particular, more
observation experiments validate that enhancing audio with text cues grants
superior audio guidance. In summary, our work provides a new direction for
building reliable audio-visual correlation with textual semantics, and we hope
that it can help the community construct more robust audio-visual systems.Sounding Object Segmentation Preference 15
Acknowledgements
This research was supported by National Natural Science Foundation of China
(NO.62106272), and Public Computing Cloud, Renmin University of China.
References
1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark.
arXiv preprint arXiv:1609.08675 (2016)
2. Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., Zhu,
J.: One transformer fits all distributions in multi-modal diffusion at scale. arXiv
preprint arXiv:2303.06555 (2023)
3. Chen,S.,Wu,Y.,Wang,C.,Liu,S.,Tompkins,D.,Chen,Z.,Wei,F.:Beats:Audio
pre-training with acoustic tokenizers. arXiv preprint arXiv:2212.09058 (2022)
4. Cheng, B., Choudhuri, A., Misra, I., Kirillov, A., Girdhar, R., Schwing, A.G.:
Mask2former for video instance segmentation (2021)
5. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.1290–1299
(2022)
6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)
7. Espejel, J.L., Ettifouri, E.H., Alassan, M.S.Y., Chouham, E.M., Dahhane, W.:
Gpt-3.5, gpt-4, or bard? evaluating llms reasoning ability in zero-shot setting and
performance boosting through prompts. Natural Language Processing Journal 5,
100032 (2023)
8. Gao, S., Chen, Z., Chen, G., Wang, W., Lu, T.: Avsegformer: Audio-visual seg-
mentation with transformer (2023)
9. Gemmeke,J.F.,Ellis,D.P.,Freedman,D.,Jansen,A.,Lawrence,W.,Moore,R.C.,
Plakal,M.,Ritter,M.:Audioset:Anontologyandhuman-labeleddatasetforaudio
events. In: 2017 IEEE international conference on acoustics, speech and signal
processing (ICASSP). pp. 776–780. IEEE (2017)
10. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra,
I.: Imagebind: One embedding space to bind them all. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.15180–
15190 (2023)
11. Hao,D.,Mao,Y.,He,B.,Han,X.,Dai,Y.,Zhong,Y.:Improvingaudio-visualseg-
mentation with bidirectional generation. arXiv preprint arXiv:2308.08288 (2023)
12. Van der Heiden, R.M., Janssen, C.P., Donker, S.F., Hardeman, L.E., Mans, K.,
Kenemans, J.L.: Susceptibility to audio signals during autonomous driving. PloS
one 13(8), e0201963 (2018)
13. Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C.,
Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for
large-scaleaudioclassification.In:2017IEEEinternationalconferenceonacoustics,
speech and signal processing (ICASSP). pp. 131–135. IEEE (2017)16 Y. Wang et al.
14. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-
mundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp.
In: International Conference on Machine Learning. pp. 2790–2799. PMLR (2019)
15. Hu, D., Wei, Y., Qian, R., Lin, W., Song, R., Wen, J.R.: Class-aware sounding
objectslocalizationviaaudiovisualcorrespondence.IEEETransactionsonPattern
Analysis and Machine Intelligence 44(12), 9844–9859 (2021)
16. Huang,S.,Li,H.,Wang,Y.,Zhu,H.,Dai,J.,Han,J.,Rong,W.,Liu,S.:Discovering
sounding objects by audio queries for audio visual segmentation (2023)
17. Hur, C., Park, H.: Zero-shot image classification with rectified embedding vectors
using a caption generator. Applied Sciences 13(12), 7071 (2023)
18. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization
networksfordensecaptioning.In:ProceedingsoftheIEEEconferenceoncomputer
vision and pattern recognition. pp. 4565–4574 (2016)
19. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything.
arXiv:2304.02643 (2023)
20. Li, K., Yang, Z., Chen, L., Yang, Y., Xun, J.: Catr: Combinatorial-dependence
audio-queried transformer for audio-visual video segmentation. arXiv preprint
arXiv:2309.09709 (2023)
21. Ling,Y.,Li,Y.,Gan,Z.,Zhang,J.,Chi,M.,Wang,Y.:Heartosegment:Unmixing
the audio to guide the semantic segmentation (2023)
22. Liu, C., Li, P., Qi, X., Zhang, H., Li, L., Wang, D., Yu, X.: Audio-visual segmen-
tation by exploring cross-modal mutual semantics (2023)
23. Liu,C.,Li,P.,Zhang,H.,Li,L.,Huang,Z.,Wang,D.,Yu,X.:Bavs:Bootstrapping
audio-visual segmentation by integrating foundation knowledge. arXiv preprint
arXiv:2308.10175 (2023)
24. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning
(2023)
25. Liu,H.,Li,C.,Li,Y.,Li,B.,Zhang,Y.,Shen,S.,Lee,Y.J.:Llava-next:Improved
reasoning,ocr,andworldknowledge(January2024),https://llava-vl.github.
io/blog/2024-01-30-llava-next/
26. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)
27. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023)
28. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural
information processing systems 36 (2024)
29. Liu, J., Ju, C., Ma, C., Wang, Y., Wang, Y., Zhang, Y.: Audio-aware query-
enhanced transformer for audio-visual segmentation (2023)
30. Liu, J., Wang, Y., Ju, C., Zhang, Y., Xie, W.: Annotation-free audio-visual seg-
mentation. arXiv preprint arXiv:2305.11019 (2023)
31. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows.In:Proceedings
of the IEEE/CVF international conference on computer vision. pp. 10012–10022
(2021)
32. Ma,J.,Sun,P.,Wang,Y.,Hu,D.:Steppingstones:Aprogressivetrainingstrategy
foraudio-visualsemanticsegmentation.IEEEEuropeanConferenceonComputer
Vision (ECCV) (2024)
33. Majumder,S.,Al-Halah,Z.,Grauman,K.:Move2hear:Activeaudio-visualsource
separation. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision. pp. 275–285 (2021)Sounding Object Segmentation Preference 17
34. Mo,S.,Tian,Y.:Av-sam:Segmentanythingmodelmeetsaudio-visuallocalization
and segmentation. arXiv preprint arXiv:2305.01836 (2023)
35. Park, S., Senocak, A., Chung, J.S.: Marginnce: Robust sound localization with a
negativemargin.In:ICASSP2023-2023IEEEInternationalConferenceonAcous-
tics, Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023)
36. Senocak, A., Oh, T.H., Kim, J., Yang, M.H., Kweon, I.S.: Learning to localize
soundsourceinvisualscenes.In:ProceedingsoftheIEEEConferenceonComputer
Vision and Pattern Recognition. pp. 4358–4366 (2018)
37. Sharma, H., Jalal, A.S.: Image captioning improved visual question answering.
Multimedia tools and applications 81(24), 34775–34796 (2022)
38. Shi,Z.,Zhou,X.,Qiu,X.,Zhu,X.:Improvingimagecaptioningwithbetteruseof
captions. arXiv preprint arXiv:2006.11807 (2020)
39. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov,N.,Batra,S.,Bhargava,P.,Bhosale,S.,Bikel,D.,Blecher,L.,Ferrer,C.C.,
Chen,M.,Cucurull,G.,Esiobu,D.,Fernandes,J.,Fu,J.,Fu,W.,Fuller,B.,Gao,
C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kar-
das,M.,Kerkez,V.,Khabsa,M.,Kloumann,I.,Korenev,A.,Koura,P.S.,Lachaux,
M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov,
T.,Mishra,P.,Molybog,I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,R.,Sal-
adi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang,
B.,Taylor,R.,Williams,A.,Kuan,J.X.,Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,
A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom,
T.: Llama 2: Open foundation and fine-tuned chat models (2023)
40. Wang, Y., Liu, W., Li, G., Ding, J., Hu, D., Li, X.: Prompting segmenta-
tion with sound is generalizable audio-visual source localizer. arXiv preprint
arXiv:2309.07929 (2023)
41. Wang,Y.,Sun,P.,Zhou,D.,Li,G.,Zhang,H.,Hu,D.:Ref-avs:Referandsegment
objects in audio-visual scenes. IEEE European Conference on Computer Vision
(ECCV) (2024)
42. Wu,W.,Yao,H.,Zhang,M.,Song,Y.,Ouyang,W.,Wang,J.:Gpt4vis:Whatcan
gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732 (2023)
43. Yan, S., Zhang, R., Guo, Z., Chen, W., Zhang, W., Li, H., Qiao, Y., He, Z., Gao,
P.: Referred by multi-modality: A unified temporal transformer for video object
segmentation. arXiv preprint arXiv:2305.16318 (2023)
44. Yang,Z.,Li,L.,Lin,K.,Wang,J.,Lin,C.C.,Liu,Z.,Wang,L.:Thedawnoflmms:
Preliminaryexplorationswithgpt-4v(ision).arXivpreprintarXiv:2309.174219(1),
1 (2023)
45. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual lan-
guage model for video understanding (2023)
46. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Se-
manticunderstandingofscenesthroughtheade20kdataset.InternationalJournal
of Computer Vision 127(3), 302–321 (2019)
47. Zhou, J., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong,
L.,Wang,M.,Zhong,Y.:Audio-visualsegmentation.In:EuropeanConferenceon
Computer Vision (2022)
48. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable
transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159
(2020)
49. Zürn,J.,Burgard,W.:Self-supervisedmovingvehicledetectionfromaudio-visual
cues (2022)Can Textual Semantics Mitigate Sounding Object
Segmentation Preference?
(Supplementary Material)
Yaoting Wang1† , Peiwen Sun2† , Yuanchao Li3 , Honggang Zhang2 , and
Di Hu(cid:66)1,4
1 Gaoling School of Artificial Intelligence, Renmin University of China, China
yaoting.wang@outlook.com
dihu@ruc.edu.cn
2 Beijing University of Posts and Telecommunications, Beijing, China
{sunpeiwen,zhhg}@bupt.edu.cn
3 University of Edinburgh, Scotland, UK
yuanchao.li@ed.ac.uk
4 Engineering Research Center of Next-Generation Search and Recommendation
A Detailed Prompts
Our detailed prompt template with CoT instructions for LLMs to reason the
potential sounding objects is shown in Fig. A1. To enhance the comprehensive-
nessofinstructionexamples,wegenerateexamplesbasedonvariousquantitative
andscenariosettings.Theseincludescenariosrangingfromnopossiblesounding
object to multiple sounding objects, as well as simple sounding scenarios with
static objects to complex sounding scenarios with multiple sound sources.
B Qualitative Segmentation Results
To demonstrate the effectiveness of our model, we showcase its performance
undervariousconditionsbypresentingqualitativesegmentationexamples.These
include the results on the AVS task, depicted in Fig. B2, as well as the results
ontheAVSStask,showninFig.B3.Duetotheunavailabilityofpublishedwork
on AVS, which hinders result reproduction and comparison, we still employ
AVSBench as our baseline for comparison.
Comparison on AVS: From Fig. B2, it is evident that our segmentation re-
sults show a significant improvement compared to AVSBench, demonstrating a
substantial enhancement of audio-visual grounding.
Comparison on AVSS:FromFig.B3,wecanobservethatourmodelachieves
† Equal contribution.
(cid:66) Corresponding author.Sounding Object Segmentation Preference 19
I will provide you with a scene description that includes both visual and auditory elements.
You need to analyze the semantics and grammar of the given sentences step-by-step, to identify
which objects may emit sounds. Note, your reasoning should follow the sub-problems outlined
below. Which objects exist in the description? What are categories of the objects? Which categories
can make sounds? Which group of sounding objects is able to make sounds simultaneously in
description? Which group of objects is more possible?
Example 1: Inside a white room a boy is playing the trumpet.
Question: Which objects may make sounds?
Reasoning: The objects are boy, trumpet, and room. A boy is a human, and humans can make sounds
by speaking and singing; A trumpet is an instrument, and the instrument can make sounds; A room
is a static object, and the static object cannot make sounds; Boy and trumpet are unable to make
sounds simultaneously; The trumpet is the most possible group to make sounds according to the
description.
Answer: ['Trumpet'].
Example 2: A woman is walking with a dog in a garden.
Question: Which objects may make sounds?
Reasoning: The objects are woman, dog, and garden. A woman is a human, and humans can make
sounds by speaking and singing; A dog is an animal, and animals can make sounds; A garden is an
environment, and an environment cannot make sounds; Woman and dog are able to make sounds
simultaneously. Woman and dog are the most possible group to make sounds according to the
description.
Answer: ['woman', 'dog'].
Example 3: Several cars driving on the road, the sky is blue, and the distant mountains are blurred.
Question: Which objects may make sounds?
Reasoning: The objects are car, road, sky, and mountain. A car is a vehicle, and the vehicle can make
sounds during driving; A road is an environment, and the environment cannot make sounds; The
sky is an environment, and the environment cannot make sounds; The mountain is an environment,
and the environment cannot make sounds. Cars are able to make sounds simultaneously. Cars are
the most possible group to make sounds according to the description.
Answer: ['car'].
Example 4: A man is playing the piano and another is playing the guitar.
Question: Which objects may make sounds?
Reasoning: The Objects are man, people, piano, and guitar. A man is a human, and humans can make
sounds by speaking and singing; A piano is an instrument, and the instrument can make sounds; A
guitar is an instrument, and the instrument can make sounds; Piano and guitar are able to make
sounds simultaneously. Two men are able to make sounds simultaneously. Guitar and man are able
to make sounds simultaneously. Piano and guitar are the most possible group to make sounds
according to the description.
Answer: ['piano', 'guitar'].
Example 5: A pencil box is put on the bench.
Question: Which objects may make sounds?
Reasoning: The Objects are: pencil box, and bench. A pencil box is a static object, and the static
object cannot make sounds; A bench is a static object, and the static object cannot make sounds;
Answer: [].
Fig.A1: Few-shot prompt with CoT instructions. We feed these prompts to LLMs
before each time we generate the reasoning results. This template generates the best
result of Tab. G3.20 Y. Wang et al.
Fig.B2: Qualitative comparison on AVS task. TeSO shows brilliant audio-visual
grounding compared with the baseline.
great performance gains on AVSS. Besides the noticeable improvements in se-
mantic segmentation results, it is crucial to remind that our approach tries to
mitigate the segmentation preference of audible objects.
C Effectiveness of Audio Control
More examples of muted or noise-only audio are provided here to show the
effectiveness of audio control. From Fig. J4, it is evident that our segmentation
resultsachievedbyTeSOexhibitalmostentirelyblankmasks,indicatingeffective
“guidance” by the muted or noise-only audio. As we mentioned in Sec. 4.4, our
experimentsrevealthatthecomparableresultsofsomemethodsmaytoacertain
extent rely on the segmentation preference of audible objects built during the
trainingbecausetheystillsegmentaudibleobjects(asshowninFig.J4)andget
comparable results even with pure Gaussian white noise.
D Effectiveness of Text Guidance
AsshowninFig.J5,wepresentthequalitativeresultsofintroducingtextcuesto
enhance the audio-visual correlation using a heatmap instead of the final binary
mask, to show the degree of the model’s attention on every pixel. By employing
language as a bridge, we elevate audio-visual correlation by leveraging text cues
as guidance, thereby enabling the model to better perceive the audio guidance.Sounding Object Segmentation Preference 21
Fig.B3: QualitativecomparisononAVSStask.TeSOhasbettersemanticperception
capability compared with other method.
E Ablation of Captioner and Reasoner
AsshowninTab.D1,(a)and(b)indicatethatusingatext-onlyLLMLLaMA-2
as a reasoner outperforms a multimodal LLM LLaVA-1.5. (a) and (c) demon-
stratethatLLaVA-1.5ismoreeffectiveasaframe-leveldensecaptioner.(a)and
(d) show that benefits from the reasoning ability, LLM reasoner outperforms
a naive NLTK noun parser. (a) and (e) suggest that our frame-level reasoning
approach is more accurate than using manual video-level noisy labels.
F Ablation of Audio Backbone.
We use the BEATS as an alternative to extract audio features. As shown in
iter3
Tab.F2,weachievedamodestimprovementof0.24%.However,weuseVGGish
for a fair comparison with most of the other methods.
TableD1:CaptionerandreasonerablationonV1M-Test.ThecooperationofLLaVA-
1.5 and LLaMA-2 (the first column) works best.
(a) (b) (c) (d) (e)
CaptionerLLaVA-1.5-7bLLaVA-1.5-7bVideoLLaMA-7bLLaVA-1.5-7b /
Reasoner LLaMA-2-7b LLaVA-1.5-7b LLaMA-2-7b NLTK Manualvideo-levelnoisylabel
mIoU 66.02 65.41 64.27 63.87 65.53
F-score 0.801 0.790 0.788 0.783 0.79222 Y. Wang et al.
TableF2:AblationofaudiobackboneonV1M-Test.ModelwithBEATsaudiofeature
gets better performance.
Method Backbone mIoU(%) Fscore
BAVS BEATs 59.63 0.659
Ours VGGish 66.02 0.801
Ours BEATs 66.26 0.803
Table G3: Textassistanceandprompttemplate.Weimplementdiverseprompttem-
platesonLLaMA-2-7b-Q4togeneratesemantictextlabelswithvaryinglevelsofquality
in V1M. Subsequently, we assess the text’s capability to control the segmentation re-
sults when subjected to different quality conditions of semantic text labels.
No.Category CoTFewshotTemplate mIoU(%)F-score
1 ✔ few-shot N×templatesshowninFig.3. 66.02 0.801
2 ✔ one-shot 1×templateshowninFig.3. 65.33 0.792
3 ✔ zero-shot“Let’sthinkstepbysteptoobtainsoundingobjectsinthecaption.”64.76 0.787
instructive
4 ✘ few-shot N×questionsofsoundingobjectswithdirectanswers. 64.80 0.785
5 ✘ one-shot 1×questionofsoundingobjectswithdirectanswer. 64.08 0.773
6 ✘ zero-shot“Pleasetellmethesoundingobjectsinthecaption.” 63.83 0.775
7 ✘ zero-shot“Pleasetellmethemostpossiblesoundingobjectinthecaption.” 62.98 0.753
8 misleading ✘ zero-shot“Pleasetellmeanyobjectsinthecaption.” 63.39 0.765
9 ✘ zero-shot“Pleasetellmethebackgroundobjectsinthecaption.” 61.52 0.740
10 irrelevant ✘ zero-shot“Tellmeanyrandomobject.” 60.86 0.722
G Ablation of Different Prompts
We experienced with different prompt templates to explore the best prompt-
ing method. The alternative methods are divided into three categories based
on instructive (No.1-6), misleading (No.7-9), and irrelevant (No.10) effects on
segmentation, as shown in Tab. G3.
The inductive approach involves employing various prompt methods to ac-
quire informative guidance. By examining the impact of this instructive infor-
mation on segmentation, we can analyze the significance of incorporating CoT
within the method. Utilizing a few-shot template with CoT can lead to a per-
formance improvement of up to 1.22% (No.1 vs. No.4). Furthermore, we devise
an effective few-shot (No.1) template that yields a performance improvement of
up to 1.25% compared to the zero-shot (No.3) inference process. Finally, our
method observes that the combination of few-shot templates and CoT leads to
optimal performance.
In a similar context, we conduct experiments to investigate the impact of
misleadingtextlabelsonthemodel.Specifically,weexploretheeffectsofmissing
(No.7), ambiguous (No.8), and erroneous (No.9) text semantic information on
themodel’sperformance.Thepresenceofsuchproblematicsemanticinformation
results in a significant 4.50% decrease in mIoU. Moreover, the introduction of
irrelevantinformation(No.10)causesthelargestdropinperformance,amounting
to 5.16% performance.Sounding Object Segmentation Preference 23
Table H4: Performance on AVS-V3 for testing the generalization on unseen objects.
0-shot 1-shot 3-shot 5-shot
Method mIoU(%)F-scoremIoU(%)F-scoremIoU(%)F-scoremIoU(%)F-score
AVSBench 53.00 0.707 56.11 0.754 63.22 0.767 63.87 0.783
AVSegFormer 54.26 0.715 58.30 0.764 64.19 0.774 65.17 0.785
GAVS 54.71 0.722 62.89 0.768 66.28 0.774 67.75 0.795
TeSO (ours) 61.06 0.743 65.41 0.775 69.29 0.791 72.15 0.810
H Multi-instance Scenario
The multi-instance scenario (e.g., two men in the same frame) is a challenging
problem in the field of AVS, especially since the existing mono-channel audio
input cannot provide sufficient audio spatial information to the model. While
ourfocusinthisworkisonthesegmentationpreferenceproblemcausedbyweak
audio guidance, we believe that our method has the potential to address such
issues because visual scene descriptions can provide guidance that is difficult
to obtain through audio features. For example, “a man in red clothes on the
right is singing a song while the man in black is smiling” can help eliminate
the ambiguity in multi-instance scenes. In future work, we hope to explore the
multi-instance problem in AVS using our framework.
I Generalization
BenefitingfromthestronggeneralizationofLLMs,itispossibletoperformvisual
sceneunderstandingonalmostanysceneandinferpotentialsoundingobjectsas
textcues.Therefore,weexplorewhetherthemodelcanexhibitbettersegmenta-
tion generalization performance for unseen object categories with the assistance
ofgeneralizabletextcuesfromLLMs.AsshowninTab.H4,ourexperimentson
the AVS-V3 dataset demonstrate that our approach can significantly improve
the generalization performance compared to other methods. This improvement
may be attributed to the generalizable text cues reasoned by LLMs from scene
descriptions, establishing a better audio-visual correlation.
J Versatility
Ourtext-guidedmethodisgenerallyapplicabletovariousvisualfoundationmod-
els with a transformer-based decoder. As depicted in Tab. J5, we present the
outcomes obtained by employing two different popular visual foundation mod-
els, namely SAM (Segment Anything Model) and Mask2Former, in their base
versions. Even when using SAM as the foundation model, our method remains
effective and produces comparable results.24 Y. Wang et al.
Table J5: PerformanceofTeSOonAVS-Benchmarkswithdifferentvisualfoundation
models. TeSO-M2F uses Mask2Former as the visual foundation model while TeSO-
SAM uses SAM as the visual foundation model.
V1S V1M AVSS-binary
Method Audio-backboneVisual-backbone
mIoU(%)F-scoremIoU(%)F-scoremIoU(%)F-score
AVSBench VGGish PVT-v2 78.70 0.879 54.00 0.645 62.45 0.756
BAVS Beats Swin-Base 82.68 0.898 59.63 0.659 55.45 0.640
TeSO-SAM(ours) VGGish ViT-Base 82.68 0.903 64.97 0.794 68.16 0.808
TeSO-M2F(ours) VGGish Swin-Base 82.84 0.917 66.02 0.801 68.53 0.813
Fig.J4:ComparisonofaudiocontrolonAVSresultswithmutedand40dBpureWhite
GaussianNoise(WGN)audioinput.Thegroundtruthlabelsareallblankbecausethe
audio input is manually set wrong with muted or noise-only audio.
Fig.J5: Comparison of whether text cues are used in TeSO. TeSO enhances audio-
visual correlation by using text cues as a bridge between audio and visual modalities.
Brighter (or redder) colors indicate the model’s increased attention to these pixels.