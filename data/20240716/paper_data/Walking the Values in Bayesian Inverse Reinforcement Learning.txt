Walking the Values in Bayesian Inverse Reinforcement Learning
OndrejBajgar1 AlessandroAbate1 KonstantinosGatsis2 MichaelA.Osborne1
1UniversityofOxford
2UniversityofSouthampton
Abstract
(IRL)addressesthisissuebyinsteadlearningtheunderlying
rewardfunctionfromexpertdemonstrations.
A key challenge in IRL is that the reward function is of-
ThegoalofBayesianinversereinforcementlearn-
ten underdetermined by the available demonstrations, as
ing(IRL)isrecoveringaposteriordistributionover
multiple reward functions can lead to the same optimal
reward functions using a set of demonstrations
behaviour. This can be solved by picking a criterion for
fromanexpertoptimizingforarewardunknown
choosingamongtherewardfunctionscompatiblewiththe
tothelearner.Theresultingposterioroverrewards
demonstrations–maximummargin[NgandRussell,2000,
canthenbeusedtosynthesizeanapprenticepolicy
Ratliffetal.,2006]andmaximumentropy[Ziebartetal.,
thatperformswellonthesameorasimilartask.
2008]arethemostprominentexamples.Asanalternative,
AkeychallengeinBayesianIRLisbridgingthe
BayesianIRLexplicitlytrackstheuncertaintyinthereward
computationalgapbetweenthehypothesisspaceof
usingaprobabilitydistribution.Thisnotonlyaccountsfor
possiblerewardsandthelikelihood,oftendefined
theissueofunderdeterminacybutalsoprovidesprincipled
intermsofQvalues:vanillaBayesianIRLneedsto
uncertaintyestimatestoanydownstreamtasks,whichcan
solvethecostlyforwardplanningproblem–going
beused,forinstance,forthesynthesisofsafepoliciesorfor
fromrewardstotheQvalues–ateverystepofthe
activelearning.
algorithm,whichmayneedtobedonethousands
of times. We propose to solve this by a simple Whilehavingtheseattractiveproperties,BayesianIRLis
change:insteadoffocusingonprimarilysampling computationallychallenging.Whileinferenceisdoneover
inthespaceofrewards,wecanfocusonprimarily thespaceofrewardfunctions(intermsofwhichtheprior
workinginthespaceofQ-values,sincethecom- isalsoexpressed),thelikelihoodisusuallyformulatedin
putationrequiredtogofromQ-valuestoreward terms of Q values (or is otherwise linked to the distribu-
is radically cheaper. Furthermore, this reversion tionoftrajectories),andgoingfromtheformertothelatter
ofthecomputationmakesiteasytocomputethe mayrequiresolvingthewholeforwardplanningproblem
gradientallowingefficientsamplingusingHamilto- at each iteration (as is case in the original Bayesian IRL
nianMonteCarlo.WeproposeValueWalk–anew algorithm[RamachandranandAmir,2007]),whichisex-
MarkovchainMonteCarlomethodbasedonthis pensiveinitselfandmayfurtherneedtobedonethousands
insight – and illustrate its advantages on several oftimesduringIRLinference.Toavoidthis,weproposeto
tasks. useasimpleinsight:whilegoingfromrewardstoQ-values
isexpensive,theinversecalculationcanbemuchsimpler.
Thus,weproposetoperformtheinferenceasifitweredone
1 INTRODUCTION primarily over the space of Q-values, computing reward
estimatesbesideit,resultinginamuchcheaperalgorithm.
Reinforcementlearning(RL)hasshownimpressiveperform- A related formulation appeared already in the variational
anceacrossawidevarietyoftasks,rangingfromrobotics methodofChanandvanderSchaar[2021],whichwas,how-
togameplaying.However,oneofthemainchallengesin ever,learningonlyapointestimateoftheQ-functionthus
applyingRLtoreal-worldproblemsisspecifyinganappro- sacrificingBayesianismfromthecentreofthealgorithm.
priaterewardfunctionbyhand,whichisoftendifficultand
We instead propose a new method that provides a full
canresultinrewardfunctionsthatareonlyimperfectprox-
BayesiantreatmentoftheQvalues,alongwiththerewards,
iesfordesigners’intentions.Inversereinforcementlearning
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).
4202
luJ
51
]GL.sc[
1v17901.7042:viXraandisabletoprovidesamplesfromthetrueposterior,be- our method is applicable in both this setting and the one
ingbasedonMarkovchainMonteCarlo(MCMC)asop- includinganenvironmentsimulator,thoughmostoftheex-
posedtovariationalinference,whichneedstopre-specify perimentsarerunintheformersettingfollowingthemain
afamilyofdistributionswithinwhichtoapproximatethe baseline method, AVRIL). Instead, we have a model of
posterior.Furthermore,sincethecomputationrequiredat how the expert policy is linked to the reward and, in the
eachstepismuchsimplerthaninpriorMCMC-basedmeth- caseofBayesianIRL,alsoapriordistributionoverreward
ods [Ramachandran and Amir, 2007, Michini and How, functions, p (which is, in general, a multi-dimensional
R
2012],whichinitselfmakesourmethodmoreefficient,we stochasticprocess,thatforanysetofstate-actionpairsre-
can also easily calculate the gradient, which allows us to turnsajointprobabilitydistributionoverthecorresponding
useHamiltonianMonteCarlo[Duaneetal.,1987]granting setofreal-valuedrewards).Commonlyusedexpertmodels
furthergainsinefficiency. includeBoltzmannrationalitymodelssuchas
The contributions of this paper are the following: (1) we eαQ∗(ϕ(st),ai)
p shro av pi ede fot rhe thfi ers pt oM steC riM orC )- ab la gs oe rd ith(a mnd foth ru cs oa ng tin no us ot uic s-t so path ce
e
P[a i|ϕ(s t)]= (cid:80) a′∈AeαQ∗(ϕ(st),a′) (1)
Bayesianinversereinforcementlearning;(2)weshowthatit [RamachandranandAmir,2007,ChanandvanderSchaar,
scalesbetterondiscrete-spacecasesthantheMCMC-based 2021]whereQ∗(s,a)istheexpected(discounted)returnif
baseline,PolicyWalk;and(3)weshowthatweoutperform actionaistakeninstates,andtheoptimalpolicyissub-
the previous state-of-the-art algorithm for Bayesian IRL sequently followed, and α is a rationality coefficient; the
on continuous state-spaces, AVRIL, better capturing the maximumentropyapproach[Ziebartetal.,2008],wherethe
posterioroverrewardsandperformingbetteronimitation probabilityofeachtrajectoryisassumedtobeproportional
learningtasks. totheexponentialofthetrajectory’sreturn;orsparsebeha-
viournoisemodels[Zhengetal.,2014],wheretheexpertis
Thepaperisorganizedasfollows:Section2providesback-
assumedtobehaverationallyexceptforsparsedeviations.
groundoninversereinforcementlearningandMarkov-chain
Besidetheseapproximatelyrationalmodels,variousmodels
MonteCarloandsummarizesrelatedwork.Section3intro-
ofirrationalitycanalsobeconsidered[Evansetal.,2015].
ducesourproposedalgorithmcalledValueWalk.Section4
TheBayesianIRLframeworkisflexiblewithrespecttothe
compares our approach to an MCMC-based predecessor,
choiceofexpertmodel,eachsuchmodeljustresultingina
PolicyWalk[RamachandranandAmir,2007],theprevious
differentlikelihoodfunction,andcanalsobeextendedto
state-of-the-artscalablemethodforBayesianIRL,AVRIL
thecasewherethemodelisnotfullyknown.
[ChanandvanderSchaar,2021],and2imitationlearning
baselinesonseveralcontroltasks. In this article, we adopt the Boltzmann rationality model
(1).WewillassumethatconditionalontheQvalues,the
actionschosenbytheexpertareindependent,yieldingthe
2 BACKGROUND likelihood
(cid:89)
eαQ∗(ϕ(st),at)
2.1 BAYESIANINVERSEREINFORCEMENT p(D|r)= p(s |s ,a )
LEARNING st,at,st+1∈D
(cid:80) a′∈AeαQ∗(ϕ(st),a′) t+1 t t
(2)
The goal of Bayesian inverse reinforcement learning foradiscreteactionspaceA(theexpressioncanreadilybe
is recovering a posterior distribution over reward func- adaptedtoacontinuoussettingbyreplacingthesumbyan
tions based on observing a set of demonstrations integral).Giventhislikelihoodtogetherwiththepriorover
D = {(ϕ(s 1),a 1),...,(ϕ(s n),a n)} from an expert act- rewardsp R,wecancalculatetheposteriorusingtheBayes
ing in a Markov decision process (MDP) M = Theoremasp(r|D) = p(D|r)p R(r)/p(D).Generally,we
(S,A,p,r,γ,t ,ρ )whereS,Aarethestateandaction cannotcalculatethisposterioranalytically,soinpractice,
max 0
spaces respectively, ϕ : S → Φ is a feature function rep- weneedtoresorttoapproximatemethods.Inthisarticle,
resentingstatesinafeaturespaceΦ,p : S ×A → P(S) weuseMarkovchainMonteCarlosampling.
isthetransitionfunctionwhereP(S)isasetofprobability
WhenperformingBayesianinferenceoverthereward,the
measures over S, r : Φ×A → R is a reward function,
transitionprobabilitieswillbeconsideredfixed(exceptfor
γ ∈ (0,1)isadiscountrate,t ∈ N∪{∞}isthetime
max Appendix A, which discusses the extension of Bayesian
horizon,andρ ∈P(S)istheinitialstatedistribution.
0 inferencealsototransitionprobabilities).Thuslookingat
InIRL,weknowallelementsoftheMDPexceptforthe thelikelihoodasafunctionofthereward,wecanwrite
rewardfunctionand,possibly,thetransitionfunction(the
(cid:89)
eαQ∗(ϕ(st),at)
settingwithouttheknowledgeoftransitiondynamics–or p(D|r)=c =:cL(D|r).
other form of access to the environment or its simulator
st,at∈D
(cid:80) a′∈AeαQ∗(ϕ(st),a′)
– is sometimes called strictly batch [Jarrett et al., 2020]; (3)
2(cid:82) (cid:82)
Sincep(D) = p(D|r)dp (r) = c L(D|r)dp (r),the mapping, first learns an estimate of the reward function,
R R
constant transition term cancels out in the posterior, and, whichcanthenbeusedtosynthesizeapolicy.Thiscanoffer
goingforward,wecanusethepartiallikelihoodLinreward better generalization, but usually requires a model of the
posteriorinference.Furthermore,MCMCalgorithmsgener- environment or access to it in order to run reinforcement
allydependonlyontheunnormalizeddistribution,thuswe learning,andgenerallyincursahighercomputationalcost.
canalsodroptheremainderofthemarginalp(D)fromour
WebuildontheparadigmofBayesianIRLintroducedby
calculation.
Ramachandran and Amir [2007]. While the Bayesian ap-
proach is attractive thanks to its principled treatment of
2.2 MARKOV-CHAINMONTECARLO(MCMC) uncertaintyinlightofthelimiteddemonstrationdata,the
keydownsiderelativetoothermethodshasbeenitsscalabil-
MarkovchainMonteCarlo(MCMC)methodsformaclass itytohigher-dimensionalsettings.MichiniandHow[2012]
ofalgorithmswidelyusedforsamplingfromcomplexprob- trytoimproveefficiencyuponRamachandranbyfocusing
abilitydistributions.MCMCmethodsrelyonconstructing computation into regions of the state space close to the
Markovchainswhosestationarydistributionisthedistribu- expertdemonstration,stillusingMCMC,whileChanand
tionofinterest.Usuallyanewcandidatesampleinthechain van der Schaar [2021] try to improve efficiency by using
isproposedandthenacceptedorrejectedwithprobability an approximate variational distribution to model the pos-
proportionaltotheoneunderthetargetdistribution–inour terior, as well as an additional neural network that tracks
casetheposterioroverrewards. the Q function, which avoids the need for a costly inner-
loopsolver.Mandyametal.[2023]hasrecentlyusedkernel
In simpler MCMC methods, such as Metropolis-
densityestimationasanalternativemethodforapproximate
Hastings[Metropolisetal.,1953,Hastings,1970],which Bayesianinference.1
werealsousedinsomepreviousarticlesonBayesianIRL
[RamachandranandAmir,2007,MichiniandHow,2012], Asopposedtorecentworkexperimentingwithotherapprox-
thenewstepisproposedasarandomjumpinthesampling imationtechniques,wereturntoMCMC,withitsgreater
space.However,thisoftenleadstoahighrejectionrate,if expressivity,whileatthesametimeadaptingittobeused
thejumpsarelarge,ortightlycorrelatedsamples,ifthejump withcontinuousstatespaces,whichwouldnotbefeasible
issmall,bothofwhichcanmakethealgorithminefficient. withpriorMCMC-basedmethods.
Thus,weinsteadusethepopularHamiltonian(orhybrid)
Monte Carlo (HMC; Duane et al. [1987]) with the no-U- 3 METHOD
turn(NUTS)sampler[HoffmanandGelman,2014],which
usesthegradientoftheposteriordensityandHamiltonian- Similarly to early work in Bayesian IRL [Ramachandran
likedynamicstoproposesamplesthatarefarapartbutstill andAmir,2007,MichiniandHow,2012],weuseMarkov
likelyundertheposterior,keepingahighacceptancerate, chainMonteCarlosamplingtoproducesamplesfromthe
thusimprovingtheefficiencyofthealgorithm. posteriordistributionoverrewardsgivenapriorandexpert
demonstrations.Ourkeyinnovationisinthewaywecalcu-
latetheposterior.AteachstepoftheMarkovchain,these
2.3 RELATEDWORK
previousmethodsgenerally(1)proposedanewreward(2)
usedsomemethodofforwardplanning,suchaspolicyiter-
Inversereinforcementlearningismostoftenusedasacom-
ation,todeducethecorrespondingoptimalQfunctionand
ponentinimitationlearning:themoregeneraltaskoflearn-
then(3)usedtheQfunctiontoevaluatethelikelihoodand
inganapprenticepolicyfromexpertdemonstrations(see
therewardtoevaluatetheprior.
Zare et al. [2023] for a good recent survey). Beside IRL,
theothermajorfamilyofmethodswithinimitationlearn- We suggest proceeding the other way round: our method
ing is behavioural cloning [Pomerleau, 1991, Ross et al., proposes a set of new parameters of the Q function and
2011],which,initsvanillaform,aimstolearnthepolicyvia thenusesittodeducethecorrespondingrewards,whichis
supervisedlearningdirectlyfromtheexpert’sobservation- generallyamucheasiercalculationthangoingfromrewards
actionpairs.Thesupervisedlearningapproachhasanad- toQfunctions.Themethodthenusestherewardtocalculate
vantageoflowercomputationalcost,butfacesthechallenge the prior and the Q value to evaluate the likelihood, and
of covariate-shift, since the training states are distributed combinesthetwotocalculatetheunnormalizedposterior
accordingtotheexpertpolicy,notthatofthelearneragent, density. This value can then be used for calculating the
thoughmultiplemethodstrytomitigatethisbyencouraging acceptance probability in any chosen MCMC algorithm.
thelearnerpolicytostayclosetotheexpertone[Dadashi
etal.,2020,Reddyetal.,2019,Brantleyetal.,2019]. 1The evaluation in this paper focuses on an offline setting
withoutaccesstoenvironmentdynamics,whilethelastmentioned
Inverse reinforcement learning represents an alternative methodfundamentallydependsonhavingaccesstotheenviron-
which,insteadofdirectlylearningtheobservation-action mentdynamicssoweomititfromthecomparisoninthispaper.
3Also thanks to the calculation being simple (rather than Algorithm1:Calculationoftheunnormalizedposterior
involvingaRL-likeinner-loopproblem)anddifferentiable, forfiniteS andAandknowntransitionprobabilitiesP
we can also calculate the gradient, which we can use for (performedineachstepofHMC).Theresultingcandid-
efficientproposalsusingHMC+NUTS.Sinceweconstruct aterewardsampleR¯isthenaccepted/rejectedtogether
the random chain in the space of Q values instead of the withthecorrespondingQ.
space of rewards, used by previous methods, we call our
Data:acandidatevectorofQvalues,setofexpert
newmethodValueWalk.
demonstrationsD,prioroverrewardsp
R
1 fors,s′ ∈S,a,a′ ∈Ado
3.1 FINITESTATEANDACTIONSPACES 2 πQ(a′|s′)=I[a′ =argmax a′′Q(s′,a′′)];
3
P¯(s,a;s′,a′)=p(s′|s,a)π(a′|s′);
Letusfirstoutlinethealgorithmforthecaseoffinitestate 4 end
andactionspacessincethecalculationcanbeperformedex- 5 R=(I−γP¯)QwhereR¯,Q¯ ;
actlyinthiscase,andthelatercontinuousalgorithmbuilds 6 p Q(Q)=p R(R)det(I−γP¯);
onthisbasecase.Weconcentratehereonthecalculation 7 L(D|Q)=
oftheposteriorprobabilitycorrespondingtoasinglepro- (cid:81) exp(αQ(s,a))/(cid:80) exp(αQ(s,a′));
(s,a)∈D a′∈A
posedsetofQvalues(whichisperformedateachstepof Result:p(Q|D)∝p (Q)L(D|Q);candidatesample
Q
theHMCtrajectory)andotherwiseemploystandardHMC. R
Note that here, we assume the knowledge of the environ-
mentdynamicsP,sincethisfinitesettingisclosetothatof
PolicyWalk[RamachandranandAmir,2007],whichalso Theabovepriortermcanbecombinedwiththelikelihood
assumesthisknowledge.However,themethodcaneasily
(cid:89) (cid:88)
beextendedtothestrictlybatchsettingusingstepsanalog- L(D|Q)= exp(αQ(s,a))/ exp(αQ(s,a′))
oustotheonestakeninthenextsubsectiononcontinuous (s,a)∈D a′∈A
spaces,orcanbecombinedwithinferenceovertransition
tocalculatetheunnormalizedposteriordensityp(Q|D)∝
probabilities(seeAppendixA).
p (Q)L(D|Q)whichweuseinthestandardHMC+NUTS
Q
In this finite case, we perform inference over a vector algorithm to produce samples from the posterior over Q
Q ∈ R|S||A| representing the optimal Q-value for each values,and,asabyproduct,alsosamplesfromtheposterior
state-actionpair.Thefirstthingtonoticeisthatgivensucha overrewards(aswouldbeexpectedfromanIRLalgorithm).
vector,wecancalculatethecorrespondingrewardvectorof Algorithm 1 summarizes the whole posterior probability
thesamedimensionalityasQusingtheBellmanequation calculation,andTheorem1inAppendixBformallyshows
as thatthesecondaryMarkovchainoverrewardsproducedby
thealgorithmalsosatisfiesthedetailedbalancecondition
(cid:88) (cid:88)
R(s,a)=Q(s,a)−γ p(s′|s,a) π (a′|s′)Q(s′,a′)
Q withrespecttotheposterioroverrewardsandthusconsti-
s′∈S a′∈A tutesavalidMCMCalgorithmforsamplingfromthereward
(4)
posterior.
with either πQ(a′|s′) = I[a′ = argmax Q(s′,a′′)] or a
a′′
softmaxapproximation(whichweusesinceithasthead- SeeSection4.1foranexampleofthisfinite-casealgorithm
vantageofbeingdifferentiableusinganinversetemperat- appliedtoagridworldenvironment.Notethatifthereward
urecoefficientα¯ toregulatethesoftnessoftheapproxim- is known to depend only on the state, the sampling can
ation).Equation(4)canalsobewritteninvectorformas instead be performed over state-values V. Similarly, if it
R=(I−γP¯)QwhereP¯isa|S||A|×|S||A|matrixwhose dependsonthefullstate,action,nextstatetriple,itshould
valuesaredefinedasP¯(s,a;s′,a′)=P(s′|s,a)πQ(a′|s′). be performed over state-action-state values to maintain a
Inthatcase,givenapriorp overrewards,wecancalculate matchinthedimensionalityoftherewardandvaluespaces.
R
thepriorofQas
p (Q)=p ((I−γP¯)Q)det(I−γP¯), 3.2 CONTINUOUSSTATEREPRESENTATIONS
Q R
where p and p are the prior probability densities of Q For continuous or large discrete spaces, it is generally
Q R
and R respectively. Since P¯ is a stochastic matrix and no longer possible or practical to maintain a separate Q-
0 < γ < 1, the determinant is always strictly positive. functionparameterforeachstate,soweneedtoresortto
Notethatthedeterminantneedstoberecalculatedonlyif approximation.Thus,fromnowon,ourinferencewillcentre
the optimal policy changes and otherwise can be cached aroundparametersθ
Q
∈RnQ ofaQfunctionapproximator
between steps of HMC eliminating the associated costly Q :Φ×A→RwhereΦisthespaceoffeaturerepresenta-
θ
calculation.Furthermore,wefoundthatinpractice,there- tionsofthestates.Whilethemethodisagaincentredaround
coveredsamplesdonotdiffersignificantlyifthedeterminant the Q function, the algorithm can also produce samples
termisomitted. from the reward posterior at any set of evaluation points
4of interest, D eval. Furthermore, a method such as warped Algorithm2:Calculationoftheunnormalizedposterior
Gaussianprocesses[Snelsonetal.,2003]canthenbeused probabilitywithcontinuousstaterepresentationsfora
togeneralizetherewardposteriorfromD eval tonewparts singleproposedparametervalueθ Q(performedineach
ofthestate-actionspace. stepofMCMC).Thereturnedcandidaterewardsamples
areacceptedorrejectedbytheouterMCMCalgorithm
Thelikelihoodcalculationremainsverysimilartothedis-
togetherwiththecandidateparametersθ .
cretecase: Q
Data:candidateparametersoftheQ-functionθ ,aset
L(D|θ )=
(cid:89) exp(αQ θQ(ϕ(s),a))
(5)
ofexpertdemonstarationsD,asetofevaQ
luation
Q (cid:80) exp(αQ (ϕ(s),a′))
(s,a)∈D a′∈A θQ locationsD eval,prioroverrewardsp R
1 InitializeemptysequenceR candofcandidatereward
(assumingAtobebounded).Whatconcernstheevaluation
samples;
oftheprior,therewardcorrespondingtogivenQ-function
2 for(s,a)∈D evaldo
parameterscanbeexpressedusingthecontinuousBellman
3 Sampleasetofsuccessorstates
equationas S ={s′′ ∼pˆ(·|s,a)};
succ
R(s,a)=Q θQ(cid:0) ϕ(s),a(cid:1) −γE s′,a′|s,a(cid:104) Q θQ(cid:0) ϕ(s′),a′(cid:1)(cid:105) 4 R γ(s, 1a)= (cid:80)Q θQ(ϕ( ms) a, xa)−
Q (s′,a′);
|Ssucc| s′∈Ssucc a′∈A θQ
onanysubsetofstatesandactions. 5 AppendR ttoR cand;
In general, the integral in E [Q (ϕ(s′,a′)] = 6 end
(cid:82) p(s′|s,a)max Q (ϕs (′, sa ′′ )|s ,, aa ′) nθQ eeds to be ap- 7 Usesamplestoevaluatethepriorp R(D eval,R cand);
ps ro′∈ xS imated,forwhica h′∈ aA nyoθ fQ
anumberofnumericalmeth-
8 UsedemonstrationstoevaluatethelikelihoodL(D|θ Q)
perequation(5);
odscanbeused,fromgridsamplingtoMonteCarlometh-
Result:unnormalizedapproximateposterior
ods,tomoresophisticatedtechniqueslikeprobabilisticnu-
p(θ |D)∝p (D ,R )p(D|θ );
merics[Hennigetal.,2022].Formostsuchmethods,we Q R eval cand Q
candidaterewardsamplesR .
theintegralisapproximatedusingadiscretesetofcandid- cand
(cid:8) (cid:9)
atesuccessorstatesS (s,a)= s∼q(·|s,a) sampled
succ
fromsomeproposaldistributionqas
1 (cid:88) p(s′|s,a) maxQ (ϕ(s′),a′). (6) Thecorrespondingcontinuousversionofthealgorithmis
|S succ| q(s′|s,a)a′∈A θQ presentedinAlgorithm2.
s′∈Ssucc
We can store both the Q function parameters θ and the
Q
The variant of the approximation we choose depends of
correspondingrewardsamplesdependingondownstream
whatinformationwehaveatourdisposal:
needs. We can then fit a warped Gaussian process to the
posterior reward samples to get a posterior reward distri-
• Ifwehaveaccesstoaprobabilisticmodelpˆoftheen-
bution over the whole state space. This can then be used
vironment(whichcaneitherrepresentthetrueenviron-
togetherwithanalgorithmforRL(orsafeRLinparticular)
mentdynamics,ifweknowthem,orourbestinferred
tofindanapprenticepolicyfromthereward.Alternatively,
modelofthedynamicsincludinganyepistemicuncer-
asashortcut,theposterioroverQ-functionscanbeusedto
tainty)thatwecansamplefrom,wecansimplysample
defineanapprenticepolicydirectly.
S (s,a)={s′ ∼pˆ(·|s,a)}anddroptheimportance
succ
weight.
• If we can evaluate the density pˆwe can directly use
the importance sampling equation 6 with q being a
proposaldistributionideallyclosetopˆ. 3.3 CONTINUOUSACTIONS
• IfallwehaveisastaticsetoftrajectoriesD –either
+
justtheexpertonesD,oralsoadditionalonessampled Thealgorithmcanbeextendedtocontinuousactions,repla-
fromanother,possiblyrandom,policy–wecancrudely cingthesumintheBoltzmannlikelihood(5)byanintegral,
approximatetherewardforatransitions,a,s′ ∈ D and again, in turn, approximating it by a discrete set of
+
usingasingletonS (s,a)={s′}.Thisisanapprox- samplesfromtheactionspace.Simplediscretizations(such
succ
imationmadebythebaselineAVRILalgorithm,soto asuniformsampling)canworkwellforlow-dimensional
match, we use it for the experiments in Section 4.2. action spaces (as we illustrate in our safe navigation ex-
In that case we require that D ⊆ D , and for perimentinthenextsection)butsufferfromthecurseof
eval +
s,a,s′ ∈ D we can define an empirical transition dimensionality,soamoresophisticatedschemewouldbe
+
model pˆ(s′′|s,a) = δ (s′′) to be used within the al- neededforhigher-dimensionalactionspaces.Weleavethat
s′
gorithm. forfuturework.
5Ground-truth rewards ValueWalk posterior reward samples AVRIL (blue) and MB-AVRIL (red) densities
0.3 0.08 0.04 0.3 1.5 1.5
-1.0 -30.0 10.0 0.2 00 .. 00 46 00 .. 00 23 0.2 1.0 1.0
0.1 0.02 0.01 0.1 0.5 0.5
0.0 0.00 0.00 0.0 0.0 0.0 20 0 20 20 0 20 20 0 20 5 0 5 5 0 5 5 0 5
0.15 0.6 0.4
0.2 0.2 0.4
-1.0 -1.0 -1.0 0.10 0.4 0.2
0.1 0.05 0.1 0.2 0.2
0.0 0.00 0.0 0.0 0.0 0.0
20 0 20 20 0 20 20 0 20 5 0 5 5 0 5 5 0 5
0.15 0.3
-1.0 -1.0 -1.0 0.2 0.2 0.10 0.4 0.2 11 .. 05
0.1 0.1 0.05 0.2 0.1 0.5
0.0 0.0 0.00 0.0 0.0 0.0
20 0 20 20 0 20 20 0 20 5 0 5 5 0 5 5 0 5
Reward Reward
Figure1:Left:Illustrative3x3gridworld.Theagentalwaysstartsinthetopleftcorner.Thetoprightcorneryieldsareward
of 10 and is terminal. The top centre tile represents an unsafe state that should be avoided and yields a reward of -30.
Centre:HistogramsofthesamplesfromtheposterioroverrewardsrecoveredbyourValueWalkalgorithmcorresponding
tothe9statesofthegridworld.Theredlineindicatesthemean.Right:Densityfunctionsoftheposterioroverrewards
recoveredbyAVRILanditsmodel-basedversion,MB-AVRIL.Notethemuchnarrowerrangeoftherewardaxisrelativeto
thehistograms.
4 EXPERIMENTS alsoranPolicyWalk,PolicyWalk-HMCandValueWalkona
6x6and12x12versionofthegridworldtoexaminehowthe
Wetestedourmethodongridworlds(forillustrationandto computetimesoftheseMCMC-basedmethodsscale.
comparethespeedtoPolicyWalk[RamachandranandAmir,
2007],whichourmethodbuildsuponbutwhichisrestric- Table1:Speedcomparison.Timepereffectivesample(in
tedtosuchsmallfinite-spacesettings)andon3simulated seconds)producedbyPolicyWalk,PolicyWalk-HMC,and
controltaskswithcontinuousstates. ValuWalkona3x3,6x6,and12x12gridworldrespectively.
States PolicyWalk PolicyWalk-HMC ValueWalk
4.1 GRIDWORLD
9 0.86 0.80 0.20
36 9.00 4.18 0.71
Foranillustrationofthemethodwitheasilyinterpretable
144 246.43 18.44 0.77
andvisualizablefeatures,wefirsttestitonasimplegrid-
worldenvironmentshowninFigure1.Wehavegenerateda
fixedsetof50demonstrationstepsintheenvironmentand
usedourmethod,ValueWalk(includingtheenvironmentdy- 4.1.1 Results
namics),theoriginalPolicyWalk[RamachandranandAmir,
2007],asped-upversionofPolicyWalkusingHMC,which Both PolicyWalk and ValueWalk (our algorithm) resul-
wedenotebyPolicyWalk-HMC(seeAppendixC.1),and ted in matching posterior reward samples as expected
AVRIL[ChanandvanderSchaar,2021](whichdoesnot (Kolmogorov-Smirnov did not reveal any significant dif-
useenvironmentdynamics,makingthecomparisonunfair ferenceswithallp-values>0.2oneachofthe9dimensions
but illustrative of inherent limitations of such model-free ofthereward).Thehistogramsofthesamplesareshownin
methods)aswellasamodel-basedversionofAVRIL,which themiddleplotinFigure1.Thespeedcomparisonofthe
wemarkasMB-AVRIL(seeAppendixC.2),torecovera twomethodscanbefoundinTable1,showingValueWalk
posterioroverrewardsfromanindependentnormalprior indeedrunsfasterthanthebaselinePolicyWalkalgorithmin
withmean0andstandarddeviationof10. bothvariants,withtheadvantagegrowingwithanincreas-
ingsizeoftheenvironmentandacorrespondinglygrowing
WithValueWalkandPolicyWalk-HMC,wecollectedatotal
numberofrewardparameters.
of 1,000 MCMC samples using HMC+NUTS with 100
warm-upsteps,whichleadtoRˆ ≤1.01oneachdimension Theposteriortendstoconcentratearoundthegroundtruth
(whereRˆisthepotentialscalereductionfactor[Gelmanand value,exceptintheterminaltop-rightstate,whichshows
Rubin,1992], acommonlyusedindicator thatthechains that the data are consistent with both positive values and
havemixedwell).ForvanillaPolicyWalk,wecollected1M mildly negative ones (since it is a terminal state, the fact
samples(sincethosearemuchmorecorrelated).Wethen that the expert heads there can equally well be explained
6
ytisned
ytilibaborP
ytisned
ytilibaborPby escaping from negative-reward states as by trying to 4.2.1 Results
incurapositivereward.Thisisconfirmedifwelookatthe
correlationbetweenvariousrewardsshowninFigure4in The results are plotted in Figure 2. While both agents do
AppendixE.1whichshowsthattherewardintheterminal closetoexpert-levelwhenprovidedwith15experttraject-
statecanbenegativeonlyifotherstates’rewardsarealso ories, our algorithm reaches this level with fewer expert
negative). demonstrations.Wehypothesizethatthisisduetotreating
the Q-function in a Bayesian way, as opposed to a point
WealsoranAVRILonthissimplegridworld(whichtook
estimate in AVRIL, leveraging the advantages of a fully
43stoconverege).Intermsoftheresultingposterior,there
Bayesiantreatmentinthelowdataregime.
are3thingstonote(seeFigure1centreandright).Firstly,
theposteriorsofsomestatesaremuchtighter–thex-axisis Tosupportthis,wecanlookattheloglikelihoodsoftheac-
zoomedinabout4xrelativetotheValueWalkhistograms. tionpredictionsonahold-outsetof100testtrajectoriesand
ThisisduetothefactthatAVRILdoesnotmodeltheuncer- theentropiesofthepredictiveposteriorshowninFigure3.
taintyintheQ-function,insteadlearningonlyapointestim- ForValueWalk,theloglikelihoodincreasesasthemethod
ate.TherewardposterioristhenpeggedtothisQ-function isgivenmoretrajectories,whilethepredictionentropyde-
pointestimatethussignificantlyreducingitsvariance.As creasesaswewouldexpectfromaBayesianmethodgiven
aresult,boththerewardoftheobstacleandofthegoalare increasingamountsofinformation.Ontheotherhand,we
extremelyunlikelyundertheposterior. donotconsistentlyseesimilarbehaviourinAVRIL.Thetest
loglikelihoodconsistentlyincreasesonlyinthecaseofthe
Secondly,wecanobservethattheposteriorrewardforthe
LunarLanderenvironment,whereit,however,startsfrom
obstacle is not any lower than that for most other states.
extremelylowlevels(theinitialmeanlogprobabilityof-10
Thisisbecausethisstateisnevervisitedindemonstrations,
wouldcorrespondtoaprobabilityof5∗10−5,suggesting
and AVRIL – not taking the environment dynamics into
the method has been putting practically 0 probability on
account – consequently does not update this value. This
actionstakenbytheexpertamongonly4possibleactions).
illustratesanimportantdownsidesfacedbymethodswithout
Also,thepredictionentropyofAVRILtendstoincreasewith
anenvironmentmodel.(Notethatthemodel-freeversionof
seeingmoretrajectories.ThatsuggeststhatAVRILmaybe
ValueWalkwouldfacethesameissue.)Thisdefectisfixed
exhibiting overfitting behaviour in the low data regimes,
inthemodel-basedversionofAVRIL,AVRIL-MB.
whichBayesianmethodsshouldgenerallyavoid.
Finally, we can see that while the true posterior differs
The ValueWalk experiments on the control environments
fromnormal(seeespeciallythestrongskewofthenegative-
were run for 10,000 sample steps with 4,000 warm-up
reward top middle cell), AVRIL is limited by its normal
stepsonLunarLanderand5,000samplestepswith2,000
variationaldistribution.Whileintheory,AVRILcouldbe
warm-upstepsonCartpoleandAcrobot.Thetrainingtakes
usedwithanyvariationalfamily,wefirstneedtodetermine
between2and19hoursofwalltimeonasingleNvidiaRTX
whichfamilymaybesuitable,forwhichanMCMC-based
3090GPU2whereAVRILtakes1-5minutestoconverge.
methodsuchasoursisausefulinstrument.
5 DISCUSSION
4.2 CLASSICCONTROLENVIRONMENTS
We presented a method that allows us to apply MCMC-
To allow for direct comparison, we also evaluated Value- basedBayesianinversereinforcementlearningtocontinu-
Walkonthreeclassiccontrolenvironmentsthatwereused ousenvironments.Themethodmaintainstheattractiveprop-
toevaluateAVRILbyitsauthors:CartPole,wherethegoal erties of MCMC methods: it is agnostic to the shape of
istobalanceaninvertedpendulumbycontrolingacartun- theposterior(wherevariationalmethodsassumeapartic-
derneathit,Acrobot,wherethegoalistoswingupadouble ularparameterizeddistributionfamily)andgivenenough
pendulumusinganactuatedjoint,andLunarLander,where compute, produces samples from the true posterior. This
thegoalistosafelylandasimulatedlanderonthesurfaceof comesatalargecomputationalcostrelativetocheapermeth-
themoon.WeusedthesamesetupaswasusedforAVRILto ods,suchasvariationalinference.However,westillthink
studytheperformanceofanapprenticeagentasafunction MCMC-methodsdohavearoletoplayintheBayesianIRL
ofthenumberofdemonstrationtrajectoriesfor1,3,7,10, ecosystem.
and15trajectories.Theapprenticeagentwasevaluatedon
Firstly,wehaveshownthatstayingtruetotheBayesianpos-
300testepisodesandthemeanrewardisreported.Wealso
teriordoesbringbenefitsintermsofsuperiorperformance
compareagainstenergy-baseddistributionmatching(EDM;
onimitationlearningtasks.Furthermore,thecomputational
Jarrettetal.[2020])–asuccessfulmethodforstrictlybatch
costispaidinthelearningphase,withinferenceatdeploy-
imitationlearning–andplainbehaviouralcloning(BC)as
mentbeingfast(submillisecondperstepinallcases,which
asimplebaseline.BaselineresultsweretakenfromChan
andvanderSchaar[2021]. 2ExperimentswithfewertrajectorieswererunonaCPU.
7Acrobot-v1 CartPole-v1 LunarLander-v2
0 500 300
100 400 200
200 300 100
300 ValueWalk (ours) 200 ValueWalk (ours) 0 ValueWalk
AVRIL AVRIL AVRIL
EDM EDM EDM
400 BC 100 BC 100 BC
Random policy Random policy Random policy
Expert Expert Expert
500 0 200
1 3 7 10 15 1 3 7 10 15 1 3 7 10 15
Number of demonstration trajectories Number of demonstration trajectories Number of demonstration trajectories
Figure 2: The test performance of an apprentice agent for ValueWalk and 3 baseline methods for different numbers of
demonstrationtrajectories.TheValueWalkapprenticeagenttakestheactionthatmaximizesthemedianoftheposterior
Q-valuesamples.Thelineshowsmeanperformanceacross5runswithdifferentsetsofexpertdemonstrations;theshaded
regionshowsmean±std.
Acrobot-v1 CartPole-v1 LunarLander-v2
0.2 0.50 0.600 0
1.2
0.575
0.45 0.42 2
0.4 0.550 1.0
0.40 4
0.6 0.44 ValueWalk logprobs 0.525 0.8
AVRIL logprobs
0.35 0.500 6
ValueWalk entropy 0.6
0.8 0.46 AVRIL entropy 0.475
0.30 8 0.4
0.450
1.0 0.25 0.48 10 0.2
0.425
0.20 0.400 12 0.0
1 3 7 10 15 1 3 7 10 15 1 3 7 10 15
Number of demonstration trajectories Number of demonstration trajectories Number of demonstration trajectories
Figure3:Theloglikelihoodonahold-outsetof100testdemonstrationsandtheentropyoftheactionpredictionsproduced
byValueWalkandAVRIL.Theplotshowsthemeanandthe90%confidenceintervalonthevalueofthemeancalculated
usingthebootstrap.
8
drawer
naeM
ytilibaborp
gol
noitca
naeM
ytilibaborp
gol
noitca
naeM
drawer
naeM
yportne
noitcA
drawer
naeM
ytilibaborp
gol
noitca
naeM
yportne
noitcA
yportne
noitcAwouldbesufficientforreal-timecontrolinmostpossible W. K. Hastings. Monte Carlo sampling methods using
usecasesandcouldbefurtheroptimized). Markov chains and their applications. Biometrika, 57
(1):97–109, April 1970. ISSN 0006-3444. doi: 10.
Secondly, we think that having a method that can draw
1093/biomet/57.1.97. URLhttps://doi.org/10.
samplesfromthetrueposteriorcanbeextremelyimportant
1093/biomet/57.1.97.
intheprocessofdevelopingother,fasteroreasiertoscale
methods,sinceitallowsustoassesshowtheirapproxima- PHennig,MAOsborne,andHPKersting. Probabilistic
tiondeviatesfromthetrueposteriorandhowitimpactstheir Numerics. CambridgeUniversityPress,2022.
performance.Also,variationalmethodsinparticularrequire
apre-specifiedfamilyofdistributionsoverwhichtheoptim- MatthewDHoffmanandAndrewGelman. TheNo-U-Turn
izationissubsequentlyrun.ValueWalkcanbeusedinan Sampler:AdaptivelySettingPathLengthsinHamiltonian
exploratoryphasetodeterminewhatfamilyofdistributions MonteCarlo. JournalofMachineLearningResearch,15:
maybeappropriatefortheproblemathand,beforepossibly 1593–1623,2014.
usingtheadvantagesofvariationalmethodstoscaleup.
Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar.
Thus, despite their steep computational cost, we think Strictlybatchimitationlearningbyenergy-baseddistri-
MCMC methods have their place in Bayesian inverse re- butionmatching. AdvancesinNeuralInformationPro-
inforcementlearning,andourmethodisasizablestepin cessingSystems,33:7354–7365,2020.
extendingthemuptoawiderrangeofsettings.
Aishwarya Mandyam, Didong Li, Diana Cai, Andrew
Jones, and Barbara E. Engelhardt. Kernel Dens-
References
ity Bayesian Inverse Reinforcement Learning, March
2023. URL http://arxiv.org/abs/2303.
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz 06827. arXiv:2303.06827[cs].
Obermeyer,NeerajPradhan,TheofanisKaraletsos,Rohit
Singh,PaulSzerlip,Paul Horsfall,andNoah D.Good- NicholasMetropolis,AriannaW.Rosenbluth,MarshallN.
man. Pyro:DeepUniversalProbabilisticProgramming. Rosenbluth,AugustaH.Teller,andEdwardTeller. Equa-
JournalofMachineLearningResearch,2018. tionofStateCalculationsbyFastComputingMachines.
TheJournalofChemicalPhysics,21(6):1087–1092,June
Kiante Brantley, Wen Sun, and Mikael Henaff.
1953. ISSN 0021-9606, 1089-7690. doi: 10.1063/
Disagreement-Regularized Imitation Learning. In
1.1699114. URL http://aip.scitation.org/
InternationalConferenceonLearningRepresentations,
doi/10.1063/1.1699114.
2019.
Bernard Michini and Jonathan P. How. Improving the
AlexJChanandMihaelavanderSchaar. ScalableBayesian
efficiency of Bayesian inverse reinforcement learning.
InverseReinforcementLearning. ICLR2021,2021.
In 2012 IEEE International Conference on Robotics
Robert Dadashi, Leonard Hussenot, Matthieu Geist, and and Automation, pages 3651–3656, May 2012. doi:
OlivierPietquin. PrimalWassersteinImitationLearning. 10.1109/ICRA.2012.6225241. ISSN:1050-4729.
InInternationalConferenceonLearningRepresentations,
Andrew Ng and Stuart Russell. Algorithms for Inverse
October2020.
ReinforcementLearning. InInternationalConferenceon
Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Machine Learning, 2000. URL http://www.eecs.
Duncan Roweth. Hybrid Monte Carlo. Physics Let- harvard.edu/cs286r/courses/spring06/
tersB,195(2):216–222,September1987. doi:10.1016/ papers/ngruss_irl00.pdf.
0370-2693(87)91197-X.
DeanA.Pomerleau. EfficientTrainingofArtificialNeural
OwainEvans,AndreasStuhlmueller,andNoahD.Good- NetworksforAutonomousNavigation. NeuralCompu-
man. Learning the Preferences of Ignorant, Inconsist- tation,3(1):88–97,March1991. ISSN0899-7667. doi:
entAgents. AAAI2016,December2015. URLhttp: 10.1162/neco.1991.3.1.88.
//arxiv.org/abs/1512.05832.
DeepakRamachandranandEyalAmir. BayesianInverse
Andrew Gelman and Donald B. Rubin. Inference
ReinforcementLearning. IJCAI2007,page6,2007.
from Iterative Simulation Using Multiple Sequences.
Statistical Science, 7(4), November 1992. ISSN Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
0883-4237. doi: 10.1214/ss/1177011136. URL Zinkevich. Maximummarginplanning. InICML2006,
https://projecteuclid.org/journals/ pages 729–736, New York, NY, USA, 2006. Associ-
statistical-science/volume-7/issue-4/ ationforComputingMachinery. ISBN1-59593-383-2.
Inference-from-Iterative-Simulation-Using-dMoui:l1t0i.1p1l4e5-/1S1e4q3u84e4n.c11e4s3/936. URLhttps://doi.
10.1214/ss/1177011136.full. org/10.1145/1143844.1143936.
9Siddharth Reddy, Anca D. Dragan, and Sergey Levine.
SQIL: Imitation Learning via Reinforcement Learning
with Sparse Rewards. In International Conference on
LearningRepresentations,September2019.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A
ReductionofImitationLearningandStructuredPredic-
tiontoNo-RegretOnlineLearning. InProceedingsofthe
FourteenthInternationalConferenceonArtificialIntel-
ligenceandStatistics,pages627–635.JMLRWorkshop
andConferenceProceedings,June2011.
Edward Snelson, Zoubin Ghahramani, and Carl
Rasmussen. Warped Gaussian Processes. In
Advances in Neural Information Processing Sys-
tems, volume 16. MIT Press, 2003. URL https:
//papers.nips.cc/paper/2003/hash/
6b5754d737784b51ec5075c0dc437bf0-Abstract.
html.
Maryam Zare, Parham M. Kebria, Abbas Khosravi, and
SaeidNahavandi. ASurveyofImitationLearning:Al-
gorithms,RecentDevelopments,andChallenges,Septem-
ber2023.
Jiangchuan Zheng, Siyuan Liu, and Lionel M. Ni. Ro-
bust Bayesian Inverse Reinforcement Learning with
Sparse Behavior Noise. Proceedings of the AAAI
Conference on Artificial Intelligence, 28(1), June
2014. ISSN 2374-3468. doi: 10.1609/aaai.v28i1.
8979. URL https://ojs.aaai.org/index.
php/AAAI/article/view/8979. Number:1.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and
AnindKDey. MaximumEntropyInverseReinforcement
Learning. AAAI2008,page6,2008.
10Walking the Values in Bayesian Inverse Reinforcement Learning
(Supplementary Material)
OndrejBajgar1 AlessandroAbate1 KonstantinosGatsis2 MichaelA.Osborne1
1UniversityofOxford
2UniversityofSouthampton
A UNKNOWNTRANSITIONPROBABILITIES
Section3.1presentsaversionoftheValueWalkalgorithmforfinitestateandactionspacesthatassumesknowntransition
probabilities.However,thekeytrickusedinValueWalkextendstounknowntransitionprobabilitiesaswell.
Onesimplifiedoptiontohandleunknowntransitions,alsoemployedinthecontinuous-statecaseinSection3.2matchingthe
settingusedbyAVRIL,isreplacingthetransitionprobabilitieswiththeirempiricalestimatepˆ(s′|s,a)=ξ(s,a,s′)/ξ(s,a)
where ξ(s,a,s′),ξ(s,a) are the numbers of occurrences in the set of demonstration set of the transition (s,a,s′) and
state-actionpair(s,a).Inthefinite-state,thiswouldmeanlimitingtheevaluationofthepriorinAlgorithm3toonlythose
state-actionpairsthatdooccurinthedata(i.e.replacingvectorsandmatricesonlines3-6bytheappropriatesub-vectors
andsub-matrices).
AmoreprincipledBayesianalternativeisofcourseusingfullBayesianinferencealsoovertransitions–inthatcase,wecan
performtheMCMCsamplingjointlyoverboththetransitionsandtheQfunctionparameters,recoveringsamplesfromthe
fulljointposterior.Thechangesneededare(1)treatingparametersofthetransitionmodelasinputsinthealgorithm,(2)
addingaprioroverthoseparameters(sothejointpriorwillbeaproductoftheQ-parameterpriorandthetransition-parameter
prior),and(3)includingtransitionprobabilitiesinthelikelihood.Hereistheadaptationofthefinite-spacealgorithmtothis
caseofunknownprobabilities:
Algorithm 3: Calculation of the unnormalized posterior for finite S and A with unknown transition probabilities
(performedineachstepofHMC).TheresultingcandidaterewardsampleR¯isthenaccepted/rejectedtogetherwiththe
correspondingQandP.
Data:acandidatematrixofQvalues,acandidatetransitionmatrixP,setofexpertdemonstrationsD,priorover
rewardsp ,priorovertransitionsp
R P
1 fors,s′ ∈S,a,a′ ∈Ado
2
π(a|s)=exp(α¯Q(s,a))/(cid:80) a′∈Aexp(α¯Q(s,a′));
3
P¯(s,a;s′,a′)=P(s′|s,a)π(a′|s′);
4 end
5 R¯ =(I−γP¯)Q¯ whereR¯,Q¯ areflattenedvectorversionsoftherewardandQ-valuematrices;
6 p Q(Q)=p R(R¯)det(I−γP¯);
7
p(D|Q)=(cid:81) (s,a,s′)∈DP(s′|s,a)exp(αQ(s,a))/(cid:80) a′∈Aexp(αQ(s,a′));
Result:p(Q,P|D)∝p (P)p (Q)p(D|Q,P);candidatesampleR¯
P Q
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).B PROOFOFSOUNDNESSOFTHEALGORITHM
Theorem1. Assumethatthetransitionkernelq satisfiesthedetailedbalancecondition
Q
q (Q′|Q) p (Q′|D)
Q = Q
q (Q|Q′) p (Q|D)
Q Q
withrespecttotheposterioroverQvaluesdefinedinAlgorithm1.ThentheassociatedimplicitMarkovchainoverrewards
alsosatisfiesthedetailedbalanceconditionwithrespecttotheposteriorp (R|D).
R
Proof. Letq bethetransitionkerneloverQ-valuesthatsatisfiesthedetailedbalanceconditionwithrespecttotheposterior
Q
p (Q|D)asassumedinthetheoremstatement.
Q
Theimplicittransitionkernelq overrewardsinducedbyq canbeexpressedas
R Q
q R(R′|R)=q
Q(Q(R′)|Q(R))(cid:12)
(cid:12) (cid:12)
(cid:12)det(cid:18)
∂Q ∂R(R
′′)(cid:19)(cid:12)
(cid:12) (cid:12)
(cid:12)
(7)
whereQ(R) = (I −γP¯)−1RistheQ-valuecorrespondingtorewardRasusedinAlgorithm1.Thedeterminantterm
accountsforthechangeofvariablesfromQtoR.
TheposterioroverrewardscanbeexpressedintermsoftheposterioroverQ-valuesas
(cid:12) (cid:18) (cid:19)(cid:12)
p R(R|D)=p
Q(Q(R)|D)(cid:12)
(cid:12) (cid:12)det
∂Q ∂R(R) (cid:12)
(cid:12)
(cid:12)
=p
Q(Q(R)|D)(cid:12) (cid:12)det(I−γP¯)−1(cid:12)
(cid:12). (8)
Nowconsidertheratiooftheimplicittransitionkernel:
(cid:12) (cid:16) (cid:17)(cid:12) (cid:12) (cid:16) (cid:17)(cid:12)
q R(R′|R)
=
q Q(Q(R′)|Q(R))(cid:12) (cid:12) (cid:12)det (cid:16)∂Q ∂R(R ′′) (cid:17)(cid:12)(cid:12) (cid:12)
=
p Q(Q(R′)|D)(cid:12) (cid:12) (cid:12)det (cid:16)∂Q ∂R(R ′′) (cid:17)(cid:12)(cid:12) (cid:12)
=
q R(R|R′) q Q(Q(R)|Q(R′)) (cid:12)det ∂Q(R) (cid:12) p Q(Q(R)|D) (cid:12)det ∂Q(R) (cid:12)
(cid:12) ∂R (cid:12) (cid:12) ∂R (cid:12)
p (R′|D)det((I−γP¯′)−1)det(I−γP¯′) p (R′|D)
R = R (9)
p (R|D)det((I−γP¯)−1) det(I−γP¯) p (R|D)
R R
wherethesecondequalityfollowsfromtheassumeddetailedbalanceconditiononq ,thelastequalityfollowsfromthe
Q
expressionforp (R|D)derivedabove,andP¯′arethejointstate-actiontransitionscorrespondingtoQ′.Thus,theimplicit
R
Markov chain over rewards induced by the transition kernel q satisfies detailed balance with respect to the posterior
Q
p (R|D),asclaimed.
R
The theorem establishes an important property of the ValueWalk method, namely that the implicit Markov chain over
rewardsinducedbytheHMC-basedsamplingofQ-valuessatisfiesdetailedbalancewithrespecttothetrueposteriorover
rewardsgiventhedemonstrations,p (R|D).Thispropertyiscrucialforthesoundnessofthemethod.
R
DetailedbalanceisasufficientconditionfortheMarkovchaintohaveastationarydistributionequaltothetargetdistribution,
inthiscasep (R|D).Thismeansthat,assumingthechainisergodic,thesamplesofrewardsobtainedfromtheValueWalk
R
methodwillasymptoticallyfollowthetrueposteriordistribution,regardlessoftheinitialdistribution.Inotherwords,the
theoremguaranteesthat,givenenoughsamples,ValueWalkwillcorrectlycharacterizetheposterioruncertaintyoverrewards,
whichisakeygoalofBayesianinversereinforcementlearning.
C VARIATIONSOFTHEBASELINEMETHODS
C.1 POLICYWALK-HMC
WeproposedthatValueWalkbeusedwithHamiltonianMonteCarlo(HMC)treatingtheunderlyingparametersasfully
continuous.Bycontrast,PolicyWalk,asoriginallyproposed,samplesthenextproposedvalueoftherewardparameters
fromneighboursofthecurrentpointonadiscretizedgrid.Toisolatethespeed-upeffectofourQ-spacetrickfromthe
12speed-upduetoHMC,wealsoimplementedaversionofPolicyWalkwithHMC(denotedbyPolicyWalk-HMCinthe
paper.Thisinvolvescalculatingthegradientoftheposteriorwithrespecttotherewardparameters.Todothat,weusethe
matrix-multiplicationcomputedQ-values.Weomitthedependenceofthecombinedtransition-policymatrixtothegradient,
sincethederivativeoftheoptimalpolicywithrespecttotherewardiszeroalmosteverywhere.
C.2 MODEL-BASEDAVRIL
Inthegridworldexperiments,bothPolicyWalkandValueWalkareleveragingtheenvironmentdynamics,whichAVRIL
doesnotuse.Forfairercomparison,wearethusincludingalsoamodel-basedversionofAVRIL,whichdiffersfromthe
original(model-free)AVRILinthatitevaluatestheKLdivergencefromtheprioracrossallstates(thegridworldisusing
state-onlyrewards),andtheTDtermiscalculated(1)overstateactionpairsand(2)thenext-statevaluecanbeestimated
usingtheactualexpectation,insteadofjustusingtheQ-valueofthenextempiricalstate-actionpairfromthedemonstrations.
Theremainderofthealgorithmremainsthesame.
D EXPERIMENTDETAILS
Forthegridworldexperiments,weusedaversionofAVRILlearningaQ-valueforeachstate-actionpairandameanand
variancevaluefortherewardineachstate.ForPolicyWalk,weraninferenceoverarewardvectorcontainingarewardvalue
pereachstate.ForValueWalk,weraninferenceoverthestate-valuevector.
Inthecontinuousstatespaceenvironments,forthe3continuousbaselinemethods,wematchthesetupfromChanand
vanderSchaar[2021]anduseneuralnetworkmodelswith2hiddenlayersof64unitsandanELUactivationfunction.For
ourexperiments,wescaleupthenetworksizewiththecomplexityoftheproblem:weuseonehiddenlayerwith8units
forCartpole,1layerof16unitsforAcrobot,and2layersof24unitsforLunarLander.Ineachcase,wealsotriedrunning
AVRILwithamatchingnetworksizebutineachcaseitperformedsimilarlyorusuallyworsethanthedefault2x64setupfor
whichresultsarereported.
ForPolicyWalkandValueWalk,weusethePyro[Binghametal.,2018]implementationofHMC+NUTS.Forthecontrol
environmentexperiments,weranwith2,000warm-upstepsand10,000inferencestepsforLunarLanderand2,000warm-up
stepswith5,000inferencestepsforCarpoleandAcrobot(sinceweareinferringfewernetworkparametersthere).We
automaticallytunethestepsizeduringwarm-upbutdonottunethemassmatrix.
In the continuous environments, we use a Gaussian process prior with an RBF kernel with fixed scale of 1 and fixed
lengthscaleof0.2forCartpoleandAcrobotand0.03forLunarLander(chosenmanuallybasedonthedistributionoffeatures
inthedemonstrationsforeachenvironment,wherethelengthscaleroughlycorrespondstothestdofone-stepchangeineach
feature).
InCartpole,Acrobot,andLunarLander,wereusethedemonstrationsetsprovidedbytheauthorsofAVRIL.Eachcontains
1000demonstrationtrajectories,fromwhichwerandomlychoseasetof100testtrajectoriesandthensplittheremaining
examplesinto5trainingsplits.Wethenre-raneachexperimentforeachnumbern =1,3,7,10,15oftrajectoriesonthe
traj
firstn trajectoriesofeachofthe5splitsandevaluatedtheresultingapprenticeagenton300episodesoftheenvironment.
traj
Wereportthemeanandstdacrossthesplitsandevaluations.
Unlessotherwisestated,weuseaBoltzmannrationalitycoefficientof3.
E ADDITIONALDETAILSOFRESULTS
E.1 GRIDWORLDEXPERIMENTS
Figure4shows2-Dhistogramsofpairwisejointposteriorsoverrewardsofthe9statesofthegridworld.Twoaspectsofthe
expert’sbehaviourarecapturedbythisplotandmaynotbeobviousfromthesimplehistogramsinFigure1.Firstly,the
agentheadingtotheterminaltoprightcornercanbeexplainedeitherbytherewardtherebeingpositive,orbytherewardin
otherstatesbeingnegative,andthustheagentusingtheterminalstateasawaytoescapeincurringfurthernegativerewards.
Secondly,notethatpracticallyalloftheprobabilitymassisplacedontherewardoftheobstacletilebeinglowerthanthatof
thetwotilesbelow,thusexplainingtheexpertavoidingtheobstacletile.
Theplotalsoclearlyshowsthattheposteriorisnon-Gaussian(noteespeciallythesharpedgeexpressinghighconfidence
13that the ratio of the two values does not cross a certain threshold) and thus could not be captured by the Gaussian-
assumingvariationalprior.Also,therewardsofdifferentstatesare,sometimesverytightly,correlated,somodellingthemas
independentwouldagainbeinappropriate.
1420 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20 20 20 20
10 10 10 10 10 10 10 10 10
0 0 0 0 0 0 0 0 0
10 10 10 10 10 10 10 10 10
20 20 20 20 20 20 20 20 20
30 30 30 30 30 30 30 30 30
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
Figure 4: 2-D histograms representing the joint posteriors of the rewards associated with the 9 states of the gridworld
(enumeratedleft-to-right,top-to-bottom,sostate3isthegoalstateinthetoprightcorner.
15