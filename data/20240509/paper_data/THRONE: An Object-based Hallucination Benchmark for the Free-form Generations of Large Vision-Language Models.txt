THRONE: An Object-based Hallucination Benchmark for
the Free-form Generations of Large Vision-Language Models
PrannayKaul1* ZhizhongLi2† HaoYang2 YonatanDukler2
AshwinSwaminathan2 C.J.Taylor2 StefanoSoatto2
VGG,UniversityofOxford1 AWSAILabs2
prannay@robots.ox.ac.uk {lzhizhon,haoyng,dukler,swashwin,taylorcj,soattos}@amazon.com
Abstract prehension.
LVLMs are largely built on LLMs and therefore inherit
Mitigatinghallucinationsinlargevision-languagemod- boththeiradvantagesandtheirdisadvantages. LLMshave
els(LVLMs)remainsanopenproblem. Recentbenchmarks been shown to produce hallucinations [47, 51], generated
do not address hallucinations in open-ended free-form re- textresponsesthatarecoherentandplausiblebutfactually
sponses, which we term “Type I hallucinations”. Instead, incorrect. LVLMs echo this behavior with generated text
they focus on hallucinations responding to very specific contradicting with the visual or text input [53]. Halluci-
question formats—typically a multiple-choice response re- nations prevent the use of LVLMs in safety-critical situa-
garding a particular object or attribute—which we term tionsandthereforeevaluatingandmitigatinghallucinations
“Type II hallucinations”. Additionally, such benchmarks in LVLMs is crucial for their deployment in such settings.
often require external API calls to models which are sub- Determining the presence and cause of hallucinations in
ject to change. In practice, we observe that a reduction in LVLMsremainsanopenquestion[45,53].
TypeIIhallucinationsdoesnotleadtoareductioninType We divide LVLM hallucinations into two types. Type
I hallucinations but rather that the two forms of halluci- Ihallucinationsoccurinresponsetoopen-endedquestions
nations are often anti-correlated. To address this, we pro- with a very large set of possible responses —e.g. What is
poseTHRONE,anovelobject-basedautomaticframework happening in this image?. TypeIIhallucinationsarein-
forquantitativelyevaluatingTypeIhallucinationsinLVLM correct responses to a factual question regarding a spe-
free-formoutputs. Weusepubliclanguagemodels(LMs)to cific concept about the image with a fixed set of options
identifyhallucinationsinLVLMresponsesandcomputein- suchasyes/no—e.g.Is there a traffic light in this
formativemetrics. Byevaluatingalargeselectionofrecent image? Fig. 3 illustrates the difference between these two
LVLMsusingpublicdatasets,weshowthatanimprovement types of hallucination. Reducing hallucinations in both
inexistingmetricsdonotleadtoareductioninTypeIhal- cases is required for useful, multi-purpose LVLMs. How-
lucinations,andthatestablishedbenchmarksformeasuring ever, later in Fig. 4 we observe that the same LVLM can
TypeIhallucinationsareincomplete. Finally,weprovidea givecontradictinganswerswhenbeingevaluatedforTypeI
simple and effective data augmentation method to reduce vs. TypeIIhallucinations. Thisimpliesthatmeasuringand
TypeIandTypeIIhallucinationsasastrongbaseline. reducingonetypedoesnotnecessarilyreducetheother.
Existing works to evaluate LVLMs often avoid direct
1 Introduction quantification of hallucinations and instead develop com-
prehensive benchmarks that judge various other desirable
Thispaperproposesabenchmarktoevaluatehallucinations
abilitiessuchas: opticalcharacterrecognition,fine-grained
by large vision-language models (LVLMs) when generat-
recognition and attribute detection [14, 22, 32]. The ex-
ing free-form responses, specifically detailed descriptions,
tent of hallucinations in these benchmarks is obfuscated,
basedonagivenimage.
since it is only one of many factors influencing other met-
The rapid advancement in large language models
rics. It requires human effort to inspect individual predic-
(LLMs) [52] has pushed the development of large vision-
tions. Therearetwomajorestablishedworkswhichspecif-
languagemodels(LVLMs).[1,6,7,17,24,25,30,36,44,
ically develop a benchmark for evaluating hallucinations
48, 54] LVLMs take input text and images and generate
in vision-language models: POPE [26] and CHAIR [40],
text responses to enable multi-modal perception and com-
whichwediscussindetailinSec.2. However,weobserve
theybothhaveshortcomingsineffectivelyevaluatinghallu-
*WorkconductedduringaninternshipatAmazon
†Correspondingauthor cinations:
4202
yaM
8
]VC.sc[
1v65250.5042:viXraLarge The image features a green 1. Is there a person? Yes, there is
D te hs ic sr i ib me a ge Vision ned xo btu u b stl .oe - Tad h e erc ek gde r r ed eob nuu bs bl uep - sa d r e ik c se k d e r 2 3 4. . . I I Is s s t t th h he e er r re e e a a a c b ta u rr s u? ? ck? ( Ip so s ti ht ei rv ee ) a dog a n ed xo tg ts oi t tt hi en g Yes
in detail. Language positioned in front of the 5. ... in this image? person in
red bus, and both buses are this image.
Model parked on the side of a Large
road. The green bus is quite External
large, occupying a Vision Response
significant portion of the Language
scene. Language Parsing
There are several people in Model
the image, with one person Model
standing near the green bus
and two others closer to the 1. Yes
red bus. Additionally, there 2. Yes (negative) No, there is
is a car parked further back 3. Yes Is there a cat not a cat in No
in th se i ds ec e on fe , t ho en rt oh ae d .right 4 5. . N .o .. in this image? this image
Figure1. THRONE(Ours):LVLMsarepromptedwithaconcept Figure 2. POPE: Questions with specific concepts prompt an
neutralinstruction.AnexternalLMperformsabstractiveQAonthe LVLM directly to evaluate Type II hallucinations [26]. Hand-
responsetoestablishtheexistenceofTypeIhallucinations. craftedrulesparseLVLMresponsestogiveyes/nolabels.
POPE [26] is a recent work addressing Type II halluci- second, we evaluate a number of current LVLMs on
nations with respect to object classes. However, we find THRONE and demonstrate that observed progress in re-
Type I and Type II hallucinations are disconnected, and ducing Type II hallucinations do not translate to a corre-
thatPOPEgivesanincompletepictureonLVLMhallucina- spondingreductioninTypeIhallucinations;third,weshow
tions. Moreover, POPEsystematicallyunder-samplesneg- a recent method, POPE, is inaccurately capturing the ex-
ativeobjectcategoriesleadingtoalargeunderestimationof tent of Type II hallucination discovery due to its sampling
TypeIIhallucinations(seeSec.5.4). strategy; we mitigate this issue in our implementation of
CHAIR [40] does address Type I hallucinations— THRONE while presenting results for a complete version
establishing object category hallucination in short image ofPOPE;andfourth,weprovideasimpleaugmentationof
captions using simple text matching. However, CHAIR is visualinstructiontuningdatawhichsignificantlyimproves
notsuitedtocurrentLVLMsbecausethesimpletextmatch- performanceonTypeIhallucinationswhilemaintainingor
ingitemployscannotcomprehendabstractorhypothetical improvingTypeIIhallucinationperformance.
concepts present in today’s LVLMs (see Fig. 4). Further,
hand-crafted rules for each set of classes are required for 2 RelatedWork
usabletextmatching,andtrivialmodelanswerscanattaina
Hallucination Benchmarks for LVLMs. In response
perfectCHAIRscore.
to the development of LVLMs, few evaluation bench-
To address the issues of current LVLM hallucina-
marks focusing on hallucinations have been introduced.
tionbenchmarks,wepropose THRONE (Text-from-image
CHAIR[40],oneofthefirstworkstoassesshallucinations,
Hallucination Recognition with Object-probes for open-
is designed for short image captions. CHAIR uses a fixed
eNdedEvaluation). THRONE leverageslanguagemodels
set of object classes (extended with their synonyms) as a
(LMs)toevaluateTypeIhallucinationsinfree-form,open-
set of text strings to find predicted object classes in im-
endedimagedescriptionswithrespecttoapre-definedob-
age captions via exact text matching. Subsequently, each
jectvocabularyofinterest. ByutilizingLMs,THRONEis
classmatchedinthecaptioniscomparedtotheCOCO[27]
abletoaccuratelyjudgewhetheranobjectmentionedinan
captions and bounding box annotations to establish ob-
LVLM response is implied to exist in the image or is ab-
jecthallucinations. CHAIRwasdevelopedpriortocurrent
stractly mentioned with no implication about its existence
instruction-tunedLVLMswhichproducelongfree-formre-
(seeFig.4).
sponses (10× longer than image captions) with a diverse
Moreover, in THRONE,weprovideeasyaccesstoour vocabulary,limitingitsapplicabilitytomodernLVLMs. As
benchmark,byleveragingopen-sourceLMsthatcanrunon showninFig.3,exacttextmatchinginCHAIRisproneto
common GPUs, instead of relying on closed-source com- incorrectly matching the vocabiliary classes with abstract
mercialmodels[35]thataresubjecttoarbitrarychange,as concepts in the free-form response and the and synonyms
doneinotherworks[4,22,32]. Throughcombiningmulti- oftheclasslistmustbemanuallyselectedtopreventconfu-
pleopen-sourceLMs,wemitigateanysingle-modelbiases sion during evaluation. For example, in CHAIR the word
in judging hallucinations when calculating Type-I halluci- “chair” will match all responses for the phrase “toilet
nationscoreswithTHRONE. seat”,becausetheexacttextmatchingclassifies“seat”as
We make four contributions: first, we establish an ac- the COCO class “chair”. Finally, CHAIR metrics com-
curate and accessible benchmark to quantitatively evalu- pare the overall number of predicted objects to the overall
ate object hallucinations in free-form responses, leverag- number of predicted objects judged to be hallucinations—
ing LMs to judge the existence of Type I hallucinations— ignoring the recall of ground-truth objects and the distri-
quantitativelyshowinghalfthejudgementerrorsofCHAIR; bution of object classes. This means that a single correctpredictionacrosstheentireevaluationdatasetalongwitha Image Prompt Response
genericresponseotherwisee.g.A natural scene,achieves
In the image, a
a perfect score (0hallucinatedobjects = 0.0). See the Supple- man is hanging
1predictedobjects What is clothes on a
mentary Material for a full overview of CHAIR. In con- happening in clothesline that
this image? is attached to
trast,THRONEusespre-trainedLMsthatgobeyonddirect the back of a
moving car.
synonymmatching,toautomaticallyjudgetheexistenceof
Yes, there is a
concepts and hallucinations in free-form responses. In ad- traffic light in
dition, our method considers both recall and precision to Is there a the image, and
traffic light in the man is
yield a holistic benchmark and does not require any man- this image? standing on the
back of a car
ual curation of synonyms. THRONE and CHAIR both near it.
evaluate Type I hallucinations—hallucinations in response
Figure 3. Type I vs. Type II Hallucinations: (Top) LVLMs
to concept-neutral prompts e.g. Describe this image in
prompted with concept-neutral instructions produce Type I hal-
detail.
lucinations. (Bottom) Instructions specifying a concept produce
POPE[26]isarecentlyproposedbenchmarktoevaluate TypeIIhallucinations.ExamplesfromLLaVA-v1.5[29].
object hallucinations in LVLMs—specifically Type II hal-
lucinations, in which an LVLM is directly queried with a anearlyworkfine-tuningthevisionencodertodynamically
yes-no question regarding the existence of a particular ob- prefixtheprompttoafrozenLLM.Flamingo[1]combines
ject of the form: Is there {a/an} {objectclassname} visualandlanguagefeaturesusingcross-attentionlayersin
in the image?. TheLVLMresponseisparsedusingsim- an otherwise frozen LLM. BLIP-2 [25] uses a frozen im-
ple rules to determine whether a Type II hallucination has age encoder to learn a Querying-Transformer (Q-Former)
occurred. Precision and recall metrics are compiled using on image-text pairs that is used as the connector. This ar-
the parsed LVLM responses and the ground-truth annota- chitecture is adapted in [7] for dialogue via training with
tion data. Despite focusing on measuring hallucinations, visual instruction tuning. LLaVA [30] uses COCO [27]
POPE only queries an LVLM with 3 positive and 3 sam- annotations and GPT-4 [35] to generate visual instruction
plednegativequestionsperCOCOimagei.e.theevaluation tuningdatainaplain-textpipeline. Combiningthisgener-
isartificiallybalanced. Thismeansmanypotentialhalluci- ateddatawithstandardVQAdatasets(VQAv2etc.) further
nations with respect to the COCO categories are not cap- boostsperformance[29]. Differentworksmodifythetrain-
tured by their method. In Sec. 5.4, we show POPE dra- ingapproachbyusingefficientadaptation[16,48]multiple
maticallyunderestimatesTypeIIhallucinationsandpresent training stages [3] or introduce the use of discrete tokens
theresultsofacompleteversion. Ourmethod, THRONE, for localization [36]. We note however, that most of these
evaluatestheprevalenceofTypeIhallucinations,whichwe worksevaluateperformanceontraditionalvision-language
observearedisjointtoTypeIIhallucinations. datasets like VQAv2 [2], which do not consider the extent
ofhallucinations,aknownproblemwithLLMs[47,51].
Comprehensive Benchmarks for LVLMs have recently
grown in number. MMBench [32] and MM-Vet [49] as-
3 THRONE
sess various aspects of LVLM performance such as: color
perception, celebrity recognition, and numerical calcula- Recap of existing methods. POPE [26] and other bench-
tion.HowevermanyoftheseworksintegrateevolvingAPIs marks(MME[14],MMBench[32])directlyqueryLVLMs
whicharemodifiedoften(orevendiscontinued)andarein- with a restricted desired answer space, e.g. yes-no (MME,
herentlystochastic. Overtimethisgreatlyreducesthecon- POPE) and A-B-C-D multiple choice (MMBench), as
sistency of these benchmarks. Exceptions are MME [14] shown in Fig. 2. These benchmarks only consider such
andSEED-Bench[22],buttheimpactofTypeIIhallucina- short answer formats, whereas a key quality of LVLMs is
tions on final metrics is conflated with a number of other in their ability to generate free-form coherent text. More-
aspectsofmodelperformance. Ourmethod,THRONE,di- over,POPE,whichaddressesTypeIIhallucinations,under-
rectly addresses Type I hallucinations, only making use of samples negative classes meaning hallucinations are dra-
open-sourcelanguagemodelsanddatasets. maticallyunderestimated(seeSec.5.4andFig.7). Incon-
Large Vision Language Models (LVLMs) have rapidly trast, weskipclasssubsamplingandenumerateallclasses
developed by harnessing advancements in large language foreveryimage,ensuringafullevaluationofTypeIhallu-
models (LLM) [5, 39, 43] and by directly integrating pre- cinationsoftheground-truthclasses.
trainedLLMsintotheirarchitectures. Incontrasttoearlier CHAIR [40] also evaluates Type I hallucinations, but
vision-languagemodelssuchasCLIP[18,38],LVLMsare wasdevelopedwhentypicalvision-languagemodelscould
generally comprised of a pretrained LLM and image en- onlygenerateshortandsimplecaptionssimilartothosein
coder, aligned with a connector module of varying com- COCOCaptions[12]. Moreover,itlacksaccuratecompre-
plexity. Some works are highlighted here. Frozen [44] is hension of responses (see the right side of Fig. 4) and ig-Type I Hallucination Evaluation
Instruction Response
Describe this image in The image showcases a fruit stand at a grocery
detail. store, featuring a variety of fruits on
display. There are several bunches of bananas,
with some placed in the foreground and others
in the background. The bananas are arranged in
different sections, creating an appealing
◼ - GT Class
presentation for customers. In addition to the
◼ - Type II bananas, there are also apples and oranges on
Hallucination display. The apples are located towards the
Type II Hallucination Evaluation (POPE) ◼ - Hypothetical left side of the image, while the oranges are
Content (not a placed in the middle and right side of the
Instruction [GT] Response [Prediction] Hallucination) stand. The fruits are well-organized and
presented in an attractive manner, making it
Is there...
an inviting sight for shoppers.
a person [No] No, there is no person in the image. [No] Type I Hallucinations Present and Found
a banana [Yes] Yes, there is a banana in the image. [Yes] MSCOCO Object Prediction from Description
an apple [No] No, there is not an apple in the image. [No] Human CHAIR THRONE (Ours)
an orange [No] No, there is no orange in the image. [No]
banana banana person banana
...in the image? No Type II Hallucinations Found apple orange apple orange apple orange
Figure4.AComparisonofPOPE,CHAIRandTHRONE: DirectlyqueryingLVLMsforobjectexistence(person,bananaetc.)using
concept-specific instructions, as in POPE (bottom left), does not produce the same hallucinations as using concept-neutral instructions
(right). WehighlighttheTypeIhallucinationsinorange. CHAIRreliesonexacttextmatchingtoafixedsetofobjectsandsynonyms,
thusincorrectlylabels“customers”and“shoppers”ashallucinations,highlightedinred. THRONE isdesignedfortherichvocabulary
andthefree-formgenerationsofmodernLVLMsbyharnessingLMstoestablishobjectexistence.ByusinganLMtopassjudgement,our
evaluationcorrectlycaptures“customers”and“shoppers”ashypotheticalcontentinthefree-formgeneration.
norestherecallofground-truthobjects. InSec.5.5,wede- dictedlabels:
scribequantitativeevaluations,usingahumanoracle,which Yˆ ∈{0,1}|I|×|C| (1)
demonstrateTHRONEhalvestherateofhallucinationmis-
judgement in CHAIR. See Sec. 2 and the Supplementary where0/1indicatesanegative/positiveexistencejudgement
MaterialformoredetailsonCHAIRanditsshortcomings. bytheLMwithrespecttotherelevantLVLMresponse.
Fig.4showsanoverviewofthethreeaforementionedmeth- Similarly using the ground-truth data for I, an array of
ods: POPE,CHAIRandourmethod,THRONE. ground-truthlabelscanbeconstructed:
Y ∈{0,1}|I|×|C| (2)
3.1 EvaluatingHallucinationswithTHRONE
To address these limitations, we propose a framework, Using these two arrays, we calculate four metrics: (1)
THRONE, shown in Fig. 1, to evaluate the prevalence of Overall Precision, P ALL; (2) Overall Recall, R ALL; (3)
Type I hallucinations in LVLM responses conditioned on Class-wise Precision, P CLS; and (4) Class-wise Recall,
animageandaneutraltextprompt. R CLS. Overall metrics are calculated in a class-agnostic
Foreachimageinalabeleddataset,I,addressingasetof manner. Class-wise metrics are calculated in a class-
classes,C,theLVLMisqueriedwiththesameinstruction: conscious manner by computing precision and recall for
Describe this image in detail., regardless of image each category separately and then averaging. This follows
content. TheLVLMresponse,whichisexpectedtobelong commonpracticeinobjectdetectionandinstancesegmen-
free-formtextcontaininganimagedescription,isgenerated tation[10,27].
and stored. Next, a publicly available, open-source, exter- FalsepositivesinLVLMsreduceprecisionandaredom-
nallanguagemodel(LM)performsabstractivequestionan- inated by hallucinations—precision indicates the extent of
swering (AQA) using the LVLM response as context and TypeIhallucinationsinLVLMresponses. Therecallmet-
a question of the form: Please answer yes or no. Is rics inform the level of class coverage by an LVLM when
there {a/an} {object class name} in this image? or producingimagedescriptions. Theclass-wisemetricsgive
similar, for every class in C (right side of Fig. 1). By se- a general measure of performance as the overall metrics
lectinganappropriateLMandusingasimpleprompttem- are skewed towards the most common categories. A com-
plate (see Sec. 4 for specific details), we ensure the AQA monwaytocombineprecision,P,andrecall,R,metricsis
response is either yes or no—our method does not require throughthegeneralizedFscore,F β:
any additional parsing. This is in contrast to other works
P ·R
which require added parsing or interpretation by a closed- Fβ =(1+β2)· (3)
(β2·P)+R
sourcemodel.
After performing AQA on each response generated by Prior work such as POPE [26], use the common balanced
the LVLM for every class in C, we obtain an array of pre- F-score(orF1-score)whichequallyweightsprecisionandrecall. However, given that THRONE is concerned with
The image features a FLAN-T5-Base no
measuringhallucinationsandisthereforeparticularlyinter- d di en li in cg i ot ua sb l se l iw ci et h o fa
ested in precision, we choose β = 0.5 or the F0.5-score, c w ah h co i cc t oo e ml pa p at l ne a i t ec e da . k be T y h o e an c ga a l k ae s si s P Il se a ts he e ra en s aw e kr n iy fe es io nr tn ho i. s image?
of beer, which is placed
thus weighing precision twice as important as recall. The on the table as well.
F0.5-scoreiscommonlyusedinpandemicmisinformation T t ch h ae e kr ee t a aa b nr l de e , tt hw o eo n e ob to n hw e el a rs r o tn h e FLAN-T5-Large yes
further away. A fork is
filters [37], recommender systems [34], active stock selec- also visible on the Please answer yes or no.
tion [41] and other areas where false positives are costlier t f Ta o hb r el e e s, n c j er o ne y ea i d n cy g r et t ao h t e eb se c a au k s e ce . od z y D ao e ks n it fh ee bt ee ix nt g g pi rv ee s ee nv ti ?dence for
and inviting atmosphere
thanfalsenegatives. Fromthiswecancalculatetheoverall for a delightful dessert
and beverage experience. FLAN-T5-Large no
and class-wise F0.5-scores, F0.5 and F0.5, respectively.
ALL CLS Figure5. AQAEnsemblinginEvaluation:UsingdifferentLMs
To mitigate against class imbalance issues, we use F0.5
CLS ordifferentpromptswhenrunningAQAonLVLMgeneratedre-
as the principle metric of comparison between LVLMs in sponses can produce opposing answers to identical prompts or
THRONE. identicalLMs.ToensureTHRONEisrobusttothis,weensemble
multipleLMsandmultiplepromptsinourevaluationpipeline.
3.2 EnsuringRobustnessviaEnsembling
4 Implementation
Any LM used for AQA in THRONE may misjudge Type
Weprovidedetailsofourframeworkregardingtheselection
II hallucinations—no LM is perfect. Fig. 5 (top) shows
ofpublicdatasets,publicLMsandLMprompts.
twodifferentvariantsfromthesamemodelfamily(FLAN-
4.1 Dataset
T5[33]),yieldingoppositeresponseswhenpromptedwith
the same response and question. Moreover, an LM may Any proper evaluation of object hallucinations (a type of
yielddifferentanswerstosemanticallyidenticalquestions, falsepositiveerror)requiresknowing,withcertainty,which
despiteconditioningonthesameresponse—showninFig.5 classes are absent in an image. In our benchmark, we use
(bottom). ToensureTHRONEisrobusttospuriousperfor- COCO [27] for a number of reasons: (1) its annotations
mance by any one LM, we ensemble various LMs and se- of 80 categories are exhaustive at an image level (image-
manticallyequivalentquestionformats. TheuseofN dis- level recall≈99% [27])—if there are many book instances
tinctLMsandM distinctquestionformatsyieldsNM an- inaCOCOimage,atleastoneisannotatedwithbounding
swers for each (LVLM response, class) combination. This boxes; (2) many LVLMs are partly trained on COCO data
setofNM answersiscombinedbasedonequalvotingby andsoshouldbefamiliarwiththesetofcategories; (3)its
each (LM, question) pair to “elect” the predicted answer. imagesgenerallycontaincomplexscenessuitableforgener-
Continuing the notation from Eq. (1), stacking predictions atinglongfree-formdescriptionsunlikeimagerecognition
from each (LM, question) pair yields a 3D array of pre- datasetslikeImageNet[8].
dicted labels: Y¯ ∈ {0,1}|IOURS|×|COURS|×NM. To combine WeutilizethevalidationsetofCOCO2017,whichcon-
the answers from each (LM, question) pair via voting, we tains |I| = 5000 images and |C| = 80 categories. Us-
require agreement between at least k answers. Where suf- ing the single LVLM text prompt Describe this image
ficient agreement does not exist we introduce an “ignore” in detail., we generate 5000 responses. As we query
label. Mathematically,theelementsinYˆ (fromEq.(1))are each LVLM response for each category in C, a single
calculatedas: LM performs AQA |I| × |C| = 400k times across the
LVLM responses—one instance of AQA per (image re-
 0, (cid:80)NMY¯ ≤(NM −k) sponse,class)pair.InSec.5.3,wealsopresentresultsusing
 k=1 i,j,k
Yˆ = 1, (cid:80)NMY¯ ≥k Objects365[42]whichisrarelyusedinLVLMtraining.
i,j k=1 i,j,k
−1,
otherwise 4.2 LanguageModels
To assess Type I hallucinations in an LVLM response us-
wherean“ignore”labelexistsinY¯,itisremovedfromthe ing THRONE, we require a language model (LM) which
calculationsofP ,R ,P ,andR ,aswecannot can answer questions on the existence of object categories
ALL ALL CLS CLS
beconfidentintheAQAprocessforthatparticular(LVLM based on the LVLM response. MMBench [32] makes use
response, class) combination. The choice of k reflects the ofaChatGPTmodeltoidentifymultiplechoiceanswerse-
desired level of confidence. We make use of a unanimous lectionsandstillreportsmistakes. Inourexperience,some
voting mechanism (k = NM)—use the prediction only if LMs give rather incoherent judgements when used to as-
all (LM, question) pairs agree, otherwise ignore. Human sess hallucinations when the prompt is changed (see sup-
evaluation of our benchmark and choice of voting mech- plemental material). Therefore for THRONE, we choose
anism is found in the Supplementary Material. Once we FLAN-T5 models [33, 39]. We make this choice because
have applied this voting mechanism, we can calculate the FLAN-T5 model family: (1) have undergone instruction
metricsdescribedattheendofSec.3.1. tuningwiththousandsoftasks[33];(2)areopen-sourceandModel L PALL RALL F A1 LL F A0. L5 L PCLS RCLS F C1 LS F C0 L.5 S 5.1 Models
Adapter-v2[15] 514 63.6 73.3 68.1 65.3 68.2 70.6 69.4 68.7
Adapter-v2.1[15] 512 63.8 73.7 68.4 65.5 67.4 71.2 69.3 68.1 For fair comparison between existing LVLMs, each pub-
InstructBLIP[7] 525 70.8 74.3 72.5 71.5 77.2 71.9 74.5 76.1 licly available model we evaluate uses an LLM with ∼7B
Otter-Image[23] 257 33.0 31.2 32.1 32.7 25.2 16.9 20.2 22.9
MiniGPT4[54] 473 81.7 59.8 69.0 76.1 79.9 61.8 69.7 75.5 parameters. TheLVLMsgenerallyhavedifferentsizedim-
MiniGPT-v2[6] 381 79.0 66.6 72.3 76.2 77.6 67.0 71.9 75.2
mPLUG-Owl[48] 555 55.5 71.9 62.6 58.1 66.3 68.3 67.3 66.7 ageencoders–e.g.LLaVA[30]usesaCLIPViT-L/14[9,38]
LRV-Instruction-v2[28] 103 82.0 56.7 67.0 75.3 78.4 58.8 67.2 73.5
with an input resolution of 336 × 336, while Instruct-
LLaVA-v1.3*[30] 532 80.5 65.2 72.1 76.9 79.9 65.3 71.9 76.5
LLaVA-v1.5[29] 509 68.1 61.0 64.4 66.6 69.9 56.4 62.5 66.8 BLIP [7] uses a ViT-g/14 [50] trained with EVA [13] and
LLaVA-Mistral[19,31] 524 86.8 71.8 78.3 83.6 84.4 64.2 70.8 77.5
aninputresolutionof224×224. Notethatimageencoder
Table 1. THRONE Results with COCO for a selection of size and resolution is not something we can easily control
instruction-tuned LVLMs. We select F0.5 as the principal met- in a pre-trained model. Each model we consider contains
CLS
ricforevaluationinourbenchmarktobalanceacrossclassesand instruction tuning in the final training phase—instruction
toprioritizeprecision(whichreflectstheextentofhallucination) tuned models provide free-form descriptions; THRONE
overrecall.Bestandsecond-bestperformancearedenotedbyblue
focusesonmodelsthatcangeneratefree-formdescriptions.
andred,respectively. *Ourimplementationusingofficialcodeto
E.g. we leave out BLIP-2 [25] (response median length
enable fair comparison. L corresponds to the median response
31 characters) in favor of InstructBLIP (median response
length(measuredin#ofcharacters).
length525).
5.2 THRONEResultsonCOCO
thereforeaccessibletothecommunity;(3)canfitlocallyon
asingleGPUforthemodelsweconsider;(4)followuser’s Results are shown in Tab. 1. The principal metric that we
instructiontoonlyrespondyesorno;and(5)areoptimized
usetojudgemodelperformanceistheclasswiseF0.5-score
for use in the free and public Text-Generation-Inference (highlightedgray). We also report all the metrics outlined
API[11]foracceleration. AsdescribedinSec.3.2,weuti- in Sec. 3.1, (P, R,F1,F0.5) for overall (left) and class-
lizeN LMstoensureourmethodisrobust.Specifically,we wiseaveraging(right), utilizingtheunanimousvotingpre-
useN = 3variantsofFLAN-T5,namely: FLAN-T5-Base sentedinSec.3.2. SeetheSupplementaryMaterialforre-
(250M parameters), FLAN-T5-Large (780M parameters), sultsandananalysisofdifferentvotingmechanisms. These
andFLAN-T5-XL(3Bparameters). results demonstrate that improvements on other bench-
marks (POPE, MME, MMBench etc.) may be orthogo-
4.3 PromptEnsembling nal and potentially at odds with improved performance on
THRONE.Usingtheresultsofthe11LVLMsthatweeval-
To guarantee each of these FLAN-T5 variants faithfully
uate,THRONEandPOPE,whichmeasureTypeIandType
produce responses of either yes or no only during AQA,
IIhallucinations,respectively,haveaSpearman’srankcor-
weusethefollowinginputtemplatetoeachLM,reflecting
theformatusedwhentrainingFLAN-T51: relation coefficient of just 0.2, and THRONE vs POPE-
C has just 0.4—the relationship between performance on
Text: {LVLM Response} Read the text
POPEandTHRONEonthesamedatasetisfarfrommono-
about an image and answer the question.
Question: Please answer yes or no. tonic. Forclass-wiseprecision—P ,thebestperforming
CLS
{Question}
models hallucinate ∼20% of the objects. We show in the
WeuseM =3semanticallyidenticalquestions: SupplementaryMaterialthatthevastmajorityoffalsepos-
• Is there a/an {classname} in this image? itive objects in the free-form image descriptions evaluated
• Does the text imply a/an {classname} is in the
image? aredirecthallucinationsratherthanmisclassificationsofvi-
• Does the text explicitly mention a/an sually similar objects (e.g. mistaking a squash racket for a
{classname} is in the image?
tennis racket). These results demonstrate that much work
As outlined in Sec. 3.2, we use a unanimous voting
stillremainstoadequatelysuppressTypeIhallucinationsin
mechanism to combine the answers from each (LM, ques-
LVLMs.
tion)pairandsok =3×3=9.
5.3 THRONEResultsonObjects365
5 EvaluationResults
Many LVLMs train on COCO directly or indirectly, thus
In this section, we: (1) outline our LVLM selection and
to demonstrate generality we apply THRONE to the Ob-
reasoning; (2)present THRONE onCOCOforevaluating
jects365 dataset [42]. Like COCO, Objects365 aims to be
TypeIhallucinations; (3)extend THRONE toObjects365
exhaustive in its image-level class labeling (it aims to la-
(containing a larger vocabulary); (4) analyze and extend
belatleastoneinstanceforeachclasspresent),butithasa
POPE to enable improved evaluation of Type II halluci-
largerobjectvocabularyandisnotusedastrainingdatafor
nations; and(5) highlightresults fromourablation studies
theLVLMsthatweevaluate. Togatheramanageablesub-
foundintheSupplementalMaterial.
setoftheObjects365validationset(80kimages),weusethe
1https://tinyurl.com/5n6nexze naturalsamplingalgorithmfrom[21],resultingin5110im-Model PALL RALL F A1
LL
F A0 L.5
L
PCLS RCLS F C1
LS
F C0 L.5
S
Adapter-v2[15] 46.7 33.9 39.3 43.4 48.9 28.5 36.0 42.8
Adapter-v2.1[15] 46.8 34.0 39.4 43.5 48.8 28.8 36.2 42.8
InstructBLIP[7] 54.5 37.2 44.2 49.8 53.7 33.6 41.3 48.0
Otter-Image[23] 21.4 12.7 16.0 18.8 9.5 4.4 6.0 7.7
MiniGPT4[54] 53.0 32.9 40.6 47.2 49.9 31.9 39.0 44.9
MiniGPT-v2[6] 54.5 36.0 43.4 49.4 51.3 34.6 41.3 46.8
mPLUG-Owl[48] 43.7 33.4 37.8 41.2 48.2 29.0 36.2 42.6
LRV-Instruction-v2[28] 57.5 26.7 36.5 46.7 51.4 26.6 35.1 43.3
LLaVA-v1.3*[30] 57.6 32.9 41.9 50.1 52.6 30.5 38.6 45.9
LLaVA-v1.5[29] 54.0 39.5 45.6 50.3 53.9 34.3 41.9 48.4 Figure7. InstabilityofPOPEtocompleteevaluationofType
LLaVA-Mistral[19,31] 58.3 39.1 46.9 53.1 57.8 35.9 44.3 51.5 IIHallucinations. ExtendingPOPEtoanexhaustiveanalysison
all COCO images and classes (POPE-C) leads to a dramatic re-
Table 2. THRONE Evaluation with Objects365. Evaluation ductioninperformanceacrossall11models.POPEissensitiveto
resultsforaselectionofinstruction-tunedLVLMs,weuseasubset thesamplingmechanism,andbyundersamplingnegativeclasses,
of Objects365 for the THRONE evaluation. Best and second- POPEseverelyunderestimatesTypeIIhallucinationsinLVLMs.
bestperformancearedenotedbyblueandred,respectively. *Our
implementationusingofficialcodetoenablefaircomparison. The above reasons make the evaluation of Type II halluci-
nationsusingPOPEinsufficient(seeSec.2formoredetails
Spearman's rank correlation: 0.912
onPOPE).Tocorrectthis,wecompletePOPEbyusingall
0.54
images to exhaustively query each LVLM for every class
0.52 in the COCO vocabulary, as done in THRONE. We name
this POPE-Complete (POPE-C). Fig. 7 shows the extreme
0.50
differenceinevaluationbetweenPOPEandourexhaustive
0.48 version,POPE-C—reportingF1-score(POPEdoesnotuti-
lize F0.5-score). As POPE only evaluates at most 9 nega-
0.46
tiveclasses, onlyasmallsubsetofpotentialhallucinations
0.44 of COCO classes are evaluated, thereby heavily underesti-
matingtheextentofTypeIIhallucination. ForeachLVLM
0.42
analyzed,weobservealarge—inmanycasesanextreme—
0.40 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 reductioninprecisionandthereforeinF1-score.
Classwise F0.5-Score (COCO Classes)
Figure 6. Comparing THRONE on COCO and Objects365. OurevaluationcontainsthreepairsofLVLMsinwhich
We observe that despite variations in the data distribution, oneisthefollow-upworktotheother, whereeachfollow-
THRONE metrics, which measure Type I hallucinations gener- upworkgenerallytrainsonmoredataformoretaskswith
alizeandhaveastrongSpearman’srankcorrelationofr=0.900. a more advanced language model. Comparing the right
Onemodel(redcircle)designatedasanoutlierandignoredwhen handsideofFig.7toTab.1,weobservethatthesefollow-
calculatingrankingcorrelation.
up works generally show an improvement in POPE (and
POPE-C),butsurprisinglyindicateasmallreductioninper-
ages(theCOCOvalidationsethas5kimages). Wepresent
formanceonTHRONEwithCOCO.Thisobservationsug-
theresultsforTHRONEonObjects365inTab.2. Figure6
geststhatprogressinreducingTypeIIhallucinationscanbe
showsthestrongcorrelationinTHRONEperformancebe-
orthogonaltoreducingTypeIhallucinations.
tween evaluating on COCO and Objects365. This demon-
strates that measuring Type I hallucinations in an LVLM
5.5 Ablations
usingTHRONEwitharelativelysmalldatasetlikeCOCO,
IntheSupplementaryMaterialwepresentthreekeyablation
is indeed indicative of the intrinsic level of Type I halluci-
experimentsandgiveanexecutivesummaryofresultshere.
nationinagivenLVLM.
First, after subsampling COCO images and LVLM re-
5.4 CompletingPOPEforTypeIIHallucinations
sponses, we replace the LMs in THRONE with human
Our experiments have used THRONE to evaluate the judgement as an oracle for Type I hallucination occur-
prevalenceofTypeIhallucinationsinLVLMresponseson rence. When comparing THRONE and CHAIR with hu-
COCO. POPE evaluates Type II hallucinations on COCO, man judgements, we estimate using THRONE improves
butwefindPOPEislargelyincomplete. First, POPEonly theprecisionofjudgingTypeIhallucinationsto96%versus
evaluates on 500 COCO validation set images. Second, 91%whenusingCHAIR—thisreducesthefalsediscovery
for each image only a subset of classes (at most 12) are rate by more than 50%. Note that we find most estimated
evaluated—eachimageisonlyqueriedwith15%ofpossi- errors in THRONE arise from the particular class defini-
ble questions. Finally, POPE artificially balances evalua- tionsinCOCO,e.g.theCOCOclasstvincludescomputer
tionquestionsbetweenpositivesandnegatives,despiteob- monitors,whichtheoraclejudgementisawareof.
jectclassexistenceinimagesbeinginherentlyimbalanced. Second, we apply the same class (and image) sampling
)sessalC
563-stcejbO(
erocS-5.0F
esiwssalCstrategy as in POPE to THRONE and show this sampling Object THRONE POPE POPE-C
overestimatesF0.5 byanaverageof12.3pointscompared Model EnumerationData PCLS RCLS F C0 L.5 S P R F1 P R F1
tothecompleteC uL seS ofclassesinTHRONE(Tab.1). ✗ 79.9 65.3 76.5 58.0 98.4 73.0 7.7 99.2 14.3
LLaVA-v1.3 COCO 83.2 68.8 79.9 73.2 88.2 80.0 9.8 69.4 17.2
Finally, we vary the choice of k i.e. the voting mecha- COCO+VG 86.2 67.0 81.5 83.0 82.5 82.8 13.8 50.4 21.7
nism used to combine answers from multiple (LM, ques- ✗ 69.9 56.4 66.8 81.9 90.8 86.1 58.7 85.7 69.7
LLaVA-v1.5 COCO 87.2 76.6 84.9 88.6 85.3 87.0 58.9 87.5 70.4
tion)pairs. Weusetheunanimousvotingmechanism(k =
COCO+VG 86.1 77.0 84.1 89.8 83.7 86.7 64.5 86.1 73.7
9) in THRONE to minimize the false discovery rate and
find the valid alternatives of simple majority (k = 5) or Table3. ImprovedBaselineviaObjectEnumeration: Adding
all-but-one (k = 8) voting mechanisms have strong corre- ourobjectenumerationtasktoLLaVAtrainingandinferenceleads
lationsandrankcorrelationsacrossallcomputemetrics,in tolargeimprovementsonTHRONEparticularlyintermsofclass-
wiseprecision, P , overstandardLLaVAmodels, demonstrat-
THRONE,of>0.99and>0.94,respectively. CLS
ingareductioninTypeIhallucinations,aswellassmallreductions
6 ImprovedBaselines inTypeIIhallucinationsjudgedbyPOPEandPOPE-C.
Much needs to be done to study Type I hallucinations.
COCOimageswhereavailable. OnTHRONE,weobserve
As a first step to their mitigation, we demonstrate a base-
alargeincreaseinclasswiseprecisionandthereforeF0.5,
line method to augment the visual instruction tuning data CLS
particularly for LLaVA-v1.5, demonstrating the ability of
for LLaVA models [29, 30], yielding improvements on
our method to reduce Type I hallucinations. Moreover, on
THRONE while maintaining similar performance regard-
POPE and POPE-C, using our object enumeration yields
ingTypeIIhallucinationsonPOPE.
small improvements in precision, indicating reduced Type
6.1 VisualInstructionTuningDataAugmentation IIhallucinationsaswell.IntheSupplementaryMaterial,we
Similar to chain-of-thought learning [46], during instruc- ablatethesamplingofnegativesduringobjectenumeration
tion tuning, we augment all visual instruction tuning sam- trainingandtheeffectofremovingtheobjectenumeration
plesconstructedbyLLaVA[30]byprependingthetaskof taskduringinference.
enumeratingalistofobjects(presentandabsent)andindi- 7 Conclusion
catingapproximatelocations,ifapplicable. Otherthanthis,
the LLaVA data and training pipeline remains unchanged. We establish a novel benchmark, THRONE, for evaluat-
To generate the new data for this object enumeration task, ing hallucinations generated by LVLMs in free-form im-
weusethesameCOCOboundingboxannotationsusedto age descriptions i.e. Type I hallucinations. Our bench-
generate the vision instruction tuning data in LLaVA. The mark utilizes multiple LMs and prompt formats with a
simpletext-onlyformat(nospecialtokens)weuseis: simple voting mechanism to yield an accurate evaluation
of Type I hallucinations in LVLM responses. We ensure
Instruction: <image> Give a list of objects and locations
that THRONE is broadly accessible by utilizing open-
in the image.
Response: {class_name_1} [{location_1}/absent] source LMs capable of running on a single commercial
...
GPU. Using THRONE, we benchmark 11 publicly avail-
{class_name_N} [{location_N}/absent]
able LVLMs on two datasets, COCO and Objects365, and
where locationi is a plain text indicator representing demonstratethatlimitedprogresshasbeenmadeinaddress-
thelocationofthecenterpointoftherelevantobjectinthe ingTypeIhallucinations. Moreover, weshowhowthees-
imageona3×3grid(e.g. bottom left). Toprovideneg- tablished benchmark, POPE, underestimates Type II hal-
ativesinthetrainingdata,ifclassnameiisnotpresent,we lucinations, which occur in response to specific questions
usetheplaintextindicatorabsent. Priorwork[53]shows e.g. yes-no questions. We present results for a completed
thatclassesthatfrequentlyco-occurinthetrainingdataare version(POPE-C)toenableacomparisonofTypeIhalluci-
themostcommonhallucinations,thereforewebiasourneg- nationsthroughTHRONEandTypeIIhallucinationsusing
ativesamplingtowardsclasspairsthatfrequentlyco-occur POPE-C. Finally, we propose a simple data augmentation
usingacorrelationmatrix. Wedetailandablatethischoice for LVLM training that can result in a large reduction in
intheSupplementaryMaterial. TypeIhallucinationswhilstmaintainingorimprovingType
6.2 ImprovedBaselineResults IIhallucinationperformance.
LimitationsandEthicalConsiderationsarediscussedin
Tab. 3 shows the result of evaluating our improved base-
theSupplementaryMaterial.
linemethodonTHRONE,POPE,andPOPE-C.Duringin-
ference,weapproximateourtrainingdataaugmentationby
first prompting the LVLM to perform the object enumera-
tiontasksandthengeneratingaresponsetothepromptfrom
therelevantbenchmark.Weadditionallyshowresultsofuti-
lizingVisualGenomeboundingboxannotations[20]ontheReferences [12] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Sri-
vastava,LiDeng,PiotrDolla´r,JianfengGao,XiaodongHe,
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
MargaretMitchell,JohnCPlatt,etal. Fromcaptionstovi-
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
sual concepts and back. In Proceedings of the IEEE Con-
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
ferenceonComputerVisionandPatternRecognition,2015.
visual language model for few-shot learning. Advances in
3
Neural Information Processing Systems, 35:23716–23736,
[13] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
2022. 1,3
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Cao. Eva:Exploringthelimitsofmaskedvisualrepresenta-
Mitchell,DhruvBatra,CLawrenceZitnick,andDeviParikh.
tionlearningatscale.InProceedingsoftheIEEEConference
Vqa:Visualquestionanswering.InProceedingsoftheInter-
onComputerVisionandPatternRecognition,2023. 6
nationalConferenceonComputerVision,pages2425–2433,
[14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
2015. 3
MengdanZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,
[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
XiawuZheng,KeLi,XingSun,andRongrongJi. Mme: A
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
comprehensive evaluation benchmark for multimodal large
Zhou. Qwen-vl: Aversatilevision-languagemodelforun-
language models. arXiv preprint arXiv:2306.13394, 2023.
derstanding, localization, text reading, and beyond. arXiv
1,3
preprintarXiv:2308.12966,2023. 3
[15] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
[4] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
WanrongZhu, AnasAwadalla, JoshGardner, RohanTaori,
angyuYue,HongshengLi,andYuQiao. Llama-adapterv2:
andLudwigSchmidt. Visit-bench:Abenchmarkforvision-
Parameter-efficientvisualinstructionmodel. arXivpreprint
language instruction following inspired by real-world use,
arXiv:2304.15010,2023. 6,7
2023. 2
[16] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
LoRA: Low-rank adaptation of large language models. In
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
Proceedings of the International Conference on Learning
guagemodelsarefew-shotlearners. AdvancesinNeuralIn-
Representations,2022. 3
formationProcessingSystems,33:1877–1901,2020. 3
[17] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
[6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, OwaisKhanMohammed,BarunPatra,QiangLiu,KritiAg-
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. garwal,ZewenChi,JohanBjorck,VishravChaudhary,Sub-
Minigpt-v2: large language model as a unified interface hojit Som, Xia Song, and Furu Wei. Language is not all
for vision-language multi-task learning. arXiv preprint youneed:Aligningperceptionwithlanguagemodels. arXiv
arXiv:2310.09478,2023. 1,6,7 preprintarXiv:2302.14045,2023. 1
[7] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat [18] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom
Fung, and Steven Hoi. Instructblip: Towards general- Duerig. Scaling up visual and vision-language representa-
purposevision-languagemodelswithinstructiontuning. In tionlearningwithnoisytextsupervision. InProceedingsof
AdvancesinNeuralInformationProcessingSystems,2023. the International Conference on Machine Learning, pages
1,3,6,7 4904–4916,2021. 3
[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- [19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
Fei. Imagenet: A large-scale hierarchical image database. Chris Bamford, Devendra Singh Chaplot, Diego de las
InProceedingsoftheIEEEConferenceonComputerVision Casas,FlorianBressand,GiannaLengyel,GuillaumeLam-
andPatternRecognition,2009. 5 ple, Lucile Saulnier, et al. Mistral 7b. arXiv preprint
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, arXiv:2310.06825,2023. 6,7
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- KenjiHata,JoshuaKravitz,StephanieChen,YannisKalan-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
isworth16x16words: Transformersforimagerecognition Connectinglanguageandvisionusingcrowdsourceddense
at scale. Proceedings of the International Conference on image annotations. International Journal of Computer Vi-
LearningRepresentations,2021. 6 sion,123:32–73,2017. 8
[10] MarkEveringham,LucVanGool,ChrisK.I.Williams,John [21] Kibok Lee, Hao Yang, Satyaki Chakraborty, Zhaowei
Winn,andAndrewZisserman. ThePASCALVisualObject Cai,GurumurthySwaminathan,AvinashRavichandran,and
Classes (VOC) challenge. International Journal of Com- Onkar Dabeer. Rethinking few-shot object detection on a
puterVision,2010. 4 multi-domainbenchmark. InProceedingsof the European
[11] Hugging Face. Text generation inference. https:// ConferenceonComputerVision,2022. 6
github.com/huggingface/text-generation- [22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
inference,2023. Accessed:November10,2023. 6 iao Ge, and Ying Shan. Seed-bench: Benchmarking mul-timodalllmswithgenerativecomprehension. arXivpreprint [36] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
arXiv:2307.16125,2023. 1,2,3 Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
[23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, ingmultimodallargelanguagemodelstotheworld. arXiv
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal preprintarXiv:2306.14824,2023. 1,3
model with in-context instruction tuning. arXiv preprint [37] Tina D Purnat, Paolo Vacca, Christine Czerniak, Sarah
arXiv:2305.03726,2023. 6,7 Ball, Stefano Burzo, Tim Zecchin, Amy Wright, Supriya
[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Bezbaruah, Faizza Tanggol, E`ve Dube´, Fabienne Labbe´,
Blip: Bootstrappinglanguage-imagepre-trainingforunified MaudeDionne,JayaLamichhane,AvichalMahajan,Sylvie
vision-languageunderstandingandgeneration. InProceed- Briand, and Tim Nguyen. Infodemic signal detection dur-
ingsoftheInternationalConferenceonMachineLearning, ingthecovid-19pandemic: Developmentofamethodology
pages12888–12900.PMLR,2022. 1 foridentifyingpotentialinformationvoidsinonlineconver-
[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. sations. JMIRInfodemiology,1(1):e30971,2021. 5
BLIP-2: Bootstrapping language-image pre-training with [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
frozenimageencodersandlargelanguagemodels. InPro- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
ceedingsoftheInternationalConferenceonMachineLearn- AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
ing,2023. 1,3,6 ingtransferablevisualmodelsfromnaturallanguagesuper-
[26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin vision. In Proceedings of the International Conference on
Zhao,andJi-RongWen. Evaluatingobjecthallucinationin MachineLearning,pages8748–8763.PMLR,2021. 3,6
largevision-languagemodels.InProceedingsoftheConfer- [39] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
enceonEmpiricalMethodsinNaturalLanguage,2023. 1, SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and
2,3,4,13 PeterJ.Liu. Exploringthelimitsoftransferlearningwitha
[27] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, unifiedtext-to-texttransformer. JournalofMachineLearn-
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence ingResearch,2020. 3,5
Zitnick. Microsoft coco: Common objects in context. In [40] AnnaRohrbach,LisaAnneHendricks,KayleeBurns,Trevor
Proceedings of the European Conference on Computer Vi- Darrell, and Kate Saenko. Object hallucination in image
sion,2014. 2,3,4,5 captioning. InProceedingsoftheConferenceonEmpirical
[28] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser MethodsinNaturalLanguage, pages4035–4045, 2018. 1,
Yacoob, and Lijuan Wang. Aligning large multi-modal 2,3,14
model with robust instruction tuning. arXiv preprint [41] GiulianoRossi,JakubKolodziej,andGurvinderBrar.Arec-
arXiv:2306.14565,2023. 6,7 ommendersystemforactivestockselection. Computational
[29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. ManagementScience,17,2020. 5
Improved baselines with visual instruction tuning. arXiv [42] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang
preprintarXiv:2310.03744,2023. 3,6,7,8 Yu,XiangyuZhang,JingLi,andJianSun. Objects365: A
[30] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. large-scale,high-qualitydatasetforobjectdetection.InPro-
Visualinstructiontuning.InAdvancesinNeuralInformation ceedings of the International Conference on Computer Vi-
ProcessingSystems,2023. 1,3,6,7,8 sion,2019. 5,6
[31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im- Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
provedreasoning,ocr,andworldknowledge,2024. 6,7 Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
[32] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang Llama: Open and efficient foundation language models.
Zhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe, arXivpreprintarXiv:2302.13971,2023. 3
Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your [44] MariaTsimpoukelli,JacobLMenick,SerkanCabi,S.M.Ali
multi-modal model an all-around player? arXiv preprint Eslami,OriolVinyals,andFelixHill. Multimodalfew-shot
arXiv:2307.06281,2023. 1,2,3,5 learningwithfrozenlanguagemodels.InAdvancesinNeural
[33] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, InformationProcessingSystems,pages200–212,2021. 1,3
HyungWonChung,YiTay,DennyZhou,QuocVLe,Barret [45] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,
Zoph, Jason Wei, and Adam Roberts. The flan collection: Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji
Designingdataandmethodsforeffectiveinstructiontuning. Zhang,JihuaZhu,JitaoSang,andHaoyuTang. Evaluation
InProceedingsoftheInternationalConferenceonMachine andanalysisofhallucinationinlargevision-languagemod-
Learning,2023. 5 els. arXivpreprintarXiv:2308.15126,2023. 1
[34] HweeTouNg,SiewMeiWu,TedBriscoe,ChristianHadi- [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
winoto, RaymondHendySusanto, andChristopherBryant. Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
TheCoNLL-2014sharedtaskongrammaticalerrorcorrec- Chain-of-thought prompting elicits reasoning in large lan-
tion. InProceedingsoftheEighteenthConferenceonCom- guagemodels. AdvancesinNeuralInformationProcessing
putationalNaturalLanguageLearning: SharedTask,2014. Systems,2022. 8
5 [47] HongbinYe,TongLiu,AijiaZhang,WeiHua,andWeiqiang
[35] OpenAI. Gpt-4technicalreport,2023. 2,3 Jia. Cognitive mirage: A review of hallucinations in largelanguage models. arXiv preprint arXiv:2309.06794, 2023.
1,3
[48] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178,2023. 1,3,6,7
[49] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
KevinLin, ZichengLiu, XinchaoWang, andLijuanWang.
Mm-vet: Evaluatinglargemultimodalmodelsforintegrated
capabilities. arXivpreprintarXiv:2308.02490,2023. 3
[50] XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLu-
casBeyer.Scalingvisiontransformers.InProceedingsofthe
IEEEConferenceonComputerVisionandPatternRecogni-
tion,2022. 6
[51] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yu-
long Chen, et al. Siren’s song in the ai ocean: A survey
on hallucination in large language models. arXiv preprint
arXiv:2309.01219,2023. 1,3
[52] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,Xiaolei
Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang,ZicanDong,etal. Asurveyoflargelanguagemod-
els. arXivpreprintarXiv:2303.18223,2023. 1
[53] YiyangZhou,ChenhangCui,JaehongYoon,LinjunZhang,
Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu
Yao. Analyzingandmitigatingobjecthallucinationinlarge
vision-languagemodels,2023. 1,8
[54] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. arXiv
preprintarXiv:2304.10592,2023. 1,6,7A VotingMechanismAblation
Model k P R F1 F0.5 P R F1 F0.5 Ignore%
ALL ALL ALL ALL CLS CLS CLS CLS
9 63.6 73.3 68.1 65.3 68.2 70.6 69.4 68.7 2.4
Adapter-v2 5 61.8 75.0 67.7 64.0 65.7 72.0 68.7 66.9 0.0
8 63.4 73.8 68.2 65.2 68.0 70.8 69.4 68.5 1.4
9 63.8 73.7 68.4 65.5 67.4 71.2 69.3 68.1 2.4
Adapter-v2.1 5 61.7 75.3 67.8 64.0 64.7 72.5 68.4 66.1 0.0
8 63.6 74.1 68.5 65.5 67.2 71.5 69.3 68.0 1.5
9 70.8 74.3 72.5 71.5 77.2 71.9 74.5 76.1 2.5
InstructBLIP 5 68.2 77.2 72.4 69.8 73.2 74.3 73.7 73.4 0.0
8 70.6 75.1 72.8 71.4 76.8 72.3 74.5 75.9 1.5
9 33.0 31.2 32.1 32.7 25.2 16.9 20.2 22.9 8.5
Otter-Image 5 25.6 34.7 29.5 27.1 16.4 20.1 18.0 17.0 0.0
8 32.4 31.8 32.1 32.3 23.9 17.2 20.0 22.2 4.8
9 81.7 59.8 69.0 76.1 79.9 61.8 69.7 75.5 2.9
MiniGPT-4 5 74.8 64.9 69.5 72.6 73.0 65.4 69.0 71.3 0.0
8 80.8 61.1 69.6 75.9 79.1 62.4 69.8 75.1 1.8
9 79.0 66.6 72.3 76.2 77.6 67.0 71.9 75.2 2.8
MiniGPT-v2 5 73.6 71.4 72.5 73.1 72.1 70.5 71.3 71.7 0.0
8 78.4 67.8 72.7 76.0 76.9 67.7 72.0 74.8 1.8
9 86.8 71.8 78.3 83.6 84.4 64.2 70.8 77.5 2.7
LLaVA-Mistral 5 82.8 75.9 78.3 81.2 78.5 68.3 71.2 77.4 0.0
8 86.3 73.1 78.5 83.3 83.7 65.0 71.2 77.4 1.6
9 55.5 71.9 62.6 58.1 66.3 68.3 67.3 66.7 2.4
mPLUG-Owl 5 54.3 73.9 62.6 57.3 63.7 69.9 66.6 64.8 0.0
8 55.5 72.6 62.9 58.2 66.2 68.6 67.4 66.7 1.4
9 82.0 56.7 67.0 75.3 78.4 58.8 67.2 73.5 3.6
LRV-Instruction-v2 5 77.5 60.8 68.1 73.5 74.6 61.9 67.7 71.7 0.0
8 81.7 57.9 67.8 75.5 78.0 59.4 67.4 73.4 2.0
9 80.5 65.2 72.1 76.9 79.9 65.3 71.9 76.5 2.4
LLaVA-v1.3 5 76.4 68.7 72.4 74.7 75.6 68.0 71.6 73.9 0.0
8 80.0 66.3 72.5 76.9 79.4 65.8 72.0 76.3 1.4
Table4. ComparisonofVotingMechanismsinTHRONE.Wecomparethreedifferentvotingmechanisms: unanimous,k =9;simple
majority, k = 5; and jury majority, k = 8. Moreover in each case we report the number of ignore labels as a result of each voting
mechanism. Thenumberoflabelsis400,000foragivenLVLM.Wefindthatthenumberofignorelabelsislowisalmostallcasesand
metricsarestronglycorrelated(> 0.99). In THRONE,weuseaunanimousvotingmechanism(k = 9)tominimizethelikelihoodof
hallucinationjudgementerrors.B THRONE vs. POPESampling
Model Sampling P R F1 F0.5 P R F1 F0.5
ALL ALL ALL ALL CLS CLS CLS CLS
THRONE 63.6 73.3 68.1 65.3 68.2 70.6 68.3 68.0
Adapter-v2 POPE 77.3 73.2 75.2 76.5 84.6 70.0 76.9 81.2
∆ (-13.7) 0.2 (-7.1) (-11.1) (-16.3) 0.5 (-8.6) (-13.2)
THRONE 63.8 73.7 68.4 65.5 67.4 71.2 68.1 67.5
Adapter-v2.1 POPE 79.2 73.2 76.1 77.9 85.4 71.8 76.3 80.6
∆ (-15.4) 0.5 (-7.7) (-12.4) (-18.0) (-0.6) (-8.2) (-13.2)
THRONE 70.8 74.3 72.5 71.5 77.2 71.9 73.1 75.2
InstructBLIP POPE 82.2 74.4 78.1 80.5 85.0 70.0 77.9 82.6
∆ (-11.4) 0.0 (-5.5) (-9.0) (-7.8) 1.9 (-4.8) (-7.4)
THRONE 33.0 31.2 32.1 32.7 25.2 16.9 18.7 21.5
Otter-Image POPE 66.5 34.9 45.7 56.3 70.7 17.3 35.6 49.2
∆ (-33.5) (-3.7) (-13.7) (-23.6) (-45.5) (-0.4) (-17.0) (-27.7)
THRONE 81.7 59.8 69.0 76.1 79.9 61.8 67.6 73.6
MiniGPT-4 POPE 89.1 58.7 70.8 80.7 88.2 60.0 70.5 78.9
∆ (-7.5) 1.1 (-1.7) (-4.7) (-8.2) 1.8 (-2.9) (-5.4)
THRONE 79.0 66.6 72.3 76.2 77.6 67.0 70.4 74.0
MiniGPT-v2 POPE 88.3 65.8 75.4 82.7 89.8 68.5 76.3 82.6
∆ (-9.3) 0.8 (-3.1) (-6.5) (-12.2) (-1.5) (-5.9) (-8.6)
THRONE 74.7 77.2 75.9 75.2 78.0 76.1 76.3 77.1
LLaVA-Mistral POPE 83.8 76.6 80.0 82.2 87.8 74.6 80.0 84.4
∆ (-9.1) 0.7 (-4.1) (-7.1) (-9.8) 1.5 (-3.7) (-7.3)
THRONE 55.5 71.9 62.6 58.1 66.3 68.3 65.2 65.3
mPLUG-Owl POPE 72.9 71.2 72.0 72.6 82.0 64.5 74.1 78.6
∆ (-17.4) 0.7 (-9.4) (-14.4) (-15.6) 3.7 (-8.9) (-13.3)
THRONE 82.0 56.7 67.0 75.3 78.4 58.8 65.0 71.5
LRV-Instruction-v2 POPE 88.6 54.8 67.7 78.9 85.0 56.2 68.8 77.4
∆ (-6.6) 1.9 (-0.7) (-3.6) (-6.6) 2.6 (-3.7) (-5.9)
THRONE 80.5 65.2 72.1 76.9 79.9 65.3 70.4 75.2
LLaVA-v1.3 POPE 85.5 61.6 71.6 79.3 87.9 61.7 72.6 80.8
∆ (-4.9) 3.6 0.5 (-2.4) (-8.0) 3.6 (-2.2) (-5.5)
THRONE 68.1 61.0 64.4 66.6 69.9 56.4 62.2 66.8
LLaVA-v1.5 POPE 81.5 64.4 72.0 77.4 86.2 59.2 70.4 78.8
∆ (-13.4) (-3.4) (-7.6) (-10.9) (-16.2) (-2.7) (-8.2) (-12.0)
Table 5. Balanced Sampling (POPE) vs. Exhaustive Sampling (THRONE): Applying POPE sampling to THRONE leads to an
underestimationoftheprevalenceofTypeIhallucinationsregardlessofLVLM.
In the main paper, we demonstrated how the sampling method used in POPE [26] leads to an underestimation of Type
IIhallucinationsandoutlineacompleteversionofPOPE(POPE-C),whichshowsthetrueextentofTypeIIhallucinations
inLVLMs. Inthissection,weperformtheopposite—weapplyPOPEtypesamplingto THRONE andcomparetheresults
to the complete sampling method used in THRONE as outlined in the main paper. Tab. 5 shows the results of applying
POPE style sampling to THRONE, once again, applying POPE style sampling leads to a large underestimation of Type I
hallucinations. POPEstylesamplingappliedtoTHRONEleadstoameanunderestimationofF0.5 by10.9pointscompared
CLS
tocompletesampling,whichisthedefaultinTHRONE.C CHAIROverview
WepresentadetaileddescriptionoftheCHAIRevaluationpresentedin[40]below. ThemethodofCHAIR,likeTHRONE,
attempts to capture the extent of hallucinations in free-form generated text pertaining an image, however, focuses on more
traditionalimagecaptioners. SimilarlytoTHRONE,CHAIRdoesnotuseconcept-focusedprompts(i.e.insteadisfocused
on“Type-I”)andisintendedtobeusedonlyincaptioningtasks. CHAIRdefinesamanualpipelineon-topoftheannotated
MSCOCOimagedatasettoproducealistofgroundtruthobjectspresentinthescene.
Givenasetofgroundtruthobjectsinanimageandamodel-generatedimagecaption,CHAIRextractstheobjectspresent
inthesceneviaatraditionalhard-ruleextractionmethodandthenattemptstomapeachofthepredictedobjectsintooneofthe
80classcategoriesofMSCOCO.ThemappingoftheextractedobjectsfromthecaptionintotheclasssetofMSCOCOusesa
pre-definedsynonymdictionaryforeachobjectcategory. Oncetheobjectsofthepredictedcaptionareextractedandmapped
tooneoftheMSCOCOcategories, CHAIRevaluatesfor“false-positive”predictions(i.e.hallucinations). Inparticularthe
authorsintroducetwovariantsofCHAIR,thefirstvariantCHAIR quantifiestheextentofhallucinatedinstancesas,
i
|{hall. object}|
CHAIR = .
i |{pred. object}|
Inthissetting,hallucinationswouldbeobjectsextractedfromthemodel-generatedcaptionsthatafterbeingmapped,arestill
notpresentintheground-truthobjectlistforthecorrespondinginstance. WenotethatCHAIR canbeviewedasmeasuring
i
the “false discovery rate” (FDR) that is 1 − P where P is the precision. Using Precision as the main metric, however
is limiting as it does not take into account the “False Negative Rate” (FNR) which is 1−R where R is the recall. This
implies that by lacking recall measurements, CHAIR may assign high scores to short and incomplete captions which are
i
not comprehensive in detailing the image. This is in stark contrast with the new generation of LVLM models powered by
LLMs which are designed to be more exhaustive and detailed and makes the use of CHAIR problematic when evaluating
i
with LVLMs. We note that CHAIR is the main metric used when people report “CHAIR scores”, and typically reported
i
numberscorrespondtothemeanCHAIR scoreacrosstheMSCOCOvalidationsetofimages.
i
ThesecondvariationofCHAIRistheCHAIR whichsimplymeasurethenumberofsentences(predictions)thatinclude
s
atleast1hallucinationascomparedwithallsentencesconsidered,
|{sentencesw/hall. object}|
CHAIR = . (4)
s |{allsentences}|
NotethatCHAIR doesnotmeasuretheextentofhallucinationwithinasentence,justtheexistenceofatleastonehalluci-
s
nation. Thisisproblematicasitdoesnotcapturetheextentofhallucinationinthesentenceespeciallyforlong-formtextand
doesnotelucidateifacaptioncontainsmanyorasinglehallucination.
Producing ground truth in CHAIR To create a list of ground truth objects from MSCOCO annotations, the authors of
CHAIRharnesstwoannotationtypestoproducethemostexhaustivelistofgroundtruthobjects.Firsttheauthorsdirectlyuse
alloftheinstancesegmentationlabelsforeachimage,whichtheyaggregateintoauniquelistofobjectsexistingintheimage.
Next the authors use the 5 human-labelled captions of each image in MSCOCO and use the same extraction and mapping
pipelineappliedtothepredictionstoproduceanadditionalsetofobjectsthatarepresentinthecaptions. Bothobjectslists
are combined and the authors note that captioning ground truth and instance segmentation ground truth objects are often
complementaryastheyfollowdifferentstyles. Thereforecombiningobjectsfrombothtypesofobjectlistsisbeneficialfor
themostexhaustivefinalgroundtruthlist.
D QualitativeEvaluationof THRONE
D.1 EvaluationMethod
We include a self-contained file (THRONEqualevalresults.html) in the Supplementary Material, which shows the qual-
itative evaluation and comparison of THRONE and CHAIR. For each LVLM evaluated, we sample 10 COCO images at
randominwhich THRONE andCHAIRdisagreeand5COCOimagesinwhich THRONE andCHAIRagree. Therefore,
wequalitativelyevaluate165responses. TheseresultsaresummarizedinTab.6
BycalculatingerrorratesforeachofthesecasesandnotingtheproportionofresponsesinwhichTHRONEandCHAIRMethod CHAIR==THRONE? #Responses(%) #ResponsesEvaluated #Judgements #JudgementErrors ErrorRate
CHAIR/THRONE ✓ 38350(69.7%) 55 157 4 2.5%
THRONE ✗ 16650(30.3%) 110 376 30 8.0%
CHAIR ✗ 16650(30.3%) 110 477 111 23.3%
Table6. SummaryofQualitativeEvaluation: Ourqualitativeresultsshowthatforresponsesinwhich THRONE andCHAIRdiffer,
thereisalargedifferenceintheerrorrate.WhenTHRONEandCHAIRagree,theerrorrateissmall.
#ResponsesAnalyzed #FalsePositivesIdentified #Hallucinations #Misclassifications
90 71 69 2
Table7. HallucinationsDominateFalsePositives: Humanevaluationestablishesthevastmajority(69 ≈ 97%)offalsepositiveobject
71
classesinLVLMresponsesaretruehallucinationsratherthanplausiblemisclassificationsofobjects.
disagreewecanestimatetheoverallerrorrateofeachmethodusingaweightedsum.
MethodErrorRate=(AgreementProportion)×(ErrorRateinAgreementCase)
+(DisagreementProportion)×(ErrorRateinDisagreementCase)
CHAIRErrorRate=0.697×0.025+0.303×0.233=0.088=8.8%
THRONEErrorRate=0.697×0.025+0.303×0.080=0.043=4.3%
D.2 Discussion
WefindthatthepluralityoferrorsmadeinTHRONErelatetoamismatchbetweentheLMdefinitionofacertainclassand
thedefinitoninCOCO.ThemostclearexampleisinthetvCOCOclass. InCOCO,thisclassincludescomputermonitors,
whereas for an LM, the implication of the existence computer monitors in an LVLM response does not lead to a “yes”
response when asked Is there a tv in this image? or similar. When doing an manual evaluation our human oracle is
awareoftheparticularCOCOclassdefinitionsandanswersaccordingly. Usingahandcraftedrulefortvandothersimilar
COCOclasses,wewouldexpecttheerrorrateofTHRONEtoreducesignificantly,butinTHRONEwedeliberatelyavoid
theuseofhandcraftedrules.
As mentioned in the main paper, the errors in CHAIR are more fundamental and result due to simple text matching of
synonymsnotbeingabletodiscriminatebetweenabstractconceptsalludedtoinaresponseanddirectobjectsimpliedtoexist
intheimagebasedontheresponse.
Tab. 7 shows results for human analysis of false positives. We analyze 90 responses, the 15 samples for mPLUG-Owl,
MiniGPT-v2, MiniGPT-4, LLaVA-7b-v1.5, LLaVA-7b-v1.3andInstructBLIP—thefinal90responsesintheself-contained
file: THRONEqualevalresults.htmlintheSupplementaryMaterial.
E ImprovedBaselineImplementationDetails
In the main paper, we introduced a simple method to augment the LLaVA visual instruction tuning data with an object
enumerationtasktoreduceTypeIandTypeIIhallucinationswhenusedtotrainLLaVAmodels. Theformatusedforobject
enumerationis:
Instruction: <image> Give a list of objects and locations
in the image.
Response: {class_name_1} [{location_1}/absent]
...
{class_name_N} [{location_N}/absent]
wherelocationirepresentsthelocationoftheboundingboxcenterpointona3×3grid.
Wegiveadditionaldetailsontheconstructionoftheobjectenumerationtaskhere.
E.1 ObjectEnumerationImplementationDetails
The LLaVA visual instruction tuning data contains 157712 samples applied to 81479 images from the COCO training set
(someimagescorrespondtomultiplesamples). Weensuretheabsolutecharacterlengthofourobjectenumeratetaskforasingle sample is not exceedingly long—we do not want the visual instruction tuning data to be pushed outside the context
lengthoftheLLaVAmodel. Thisisdonebylimitingthenumberofinstancesperclassinasampleto3.
For each sample we construct an object enumeration task using bounding box data as follows: first, sort bounding box
annotationsforagivenimagebyboxareaindescendingorder;second,loopoverthesortedannotationsaddingtheinstance
(classnamei, locationi)totheobjectenumerationtaskiftherearelessthan3instancesofclassnameinthetask;third,
sample6negativeclassesandappendthemtotheobjectenumerationtaskusingabsentasthelocationstring.
Thesamplingofnegativeclassesisdetailednext.
E.2 NegativeSamplingImplementationDetails
Tosamplenegativesintheobjectenumerationtaskwefirstbuildaco-occurencematrixfromtheboundingboxannotations.
Thepseudocodeforbuildingthismatrixisasfollows:
from pycocotools.coco import COCO
import numpy as np
train_dset = COCO(instances_path)
num_cats = len(train_dset.getCatIds())
co_occur = np.array((num_cats, num_cats))
cat_id2cont_id = {x: i for i, x in sorted(enumerate(train_dset.getCatIds()))}
for iid in train_dset.getImgIds():
anns = train_dset.loadAnns(train_dset.getAnnIds(imgIds=iid))
pres_cats = [coco_cid2cont_cid[x[’category_id’]] for x in anns]
pres_cats = np.unique(pres_cats)
for r in pres_cats:
for c in pres_cats:
if r != c:
co_occur[r, c] += 1
Afterbuildingthisco-occurencematrixnegativeclassesaresampledinamannerwhichisawareoftheclassespresentin
agivenimage. Thepseudocodeisasfollows(usingsomevariablesfromtheabovepseudocode):
present_cat_ids: List[int] # list of category ids present in the image
present_cont_ids = [cat_id2cont_id[x] for x in present_cat_ids
# combine co-occurence across present categories
# ensuring present categories can not be sampled
present_co_occur = co_occur[present_cont_ids].copy()
present_co_occur[:, present_cont_ids] = 0
present_co_occur = present_co_occur / present_co_occur.sum(axis=1, keepdims=True)
present_co_occur = present_co_occur.sum(axis=0)
present_co_occur = present_co_occur / present_co_occur.sum()
# sharpen distribution
present_co_occur = present_co_occur ** 10
present_co_occur = present_co_occur / present_co_occur.sum()
rng = np.random.RandomState(iid)
neg_ids = rng.choice(
sorted(train_dset.getCatIds()),
size=6,
p=present_co_occur,
replace=False
)
Thismethodofsamplingyieldsnegativeclasseswhichcommonlyco-occurwithpositiveclassesinagivenimage. There-
fore,theobjectenumerationtasktrainstheLVLMtodistinguishindividualobjectsandclassesratherthanrelyingonglobal
context.E.3 ObjectEnumerationDataDetails
InTable3ofthemainpaper,wepresentresultsonTHRONE,POPEandPOPE-Cwhentrainingwithourobjectenumeration
task using COCO or COCO and VisualGenome as object enumeration data. Approximately 33000 of the 81479 COCO
images in the LLaVA visual instruction tuning data are contained in the VisualGenome dataset. When using COCO and
VisualGenome data, we construct the object enumeration task for an image from VisualGenome data when possible and
COCOotherwise—wedonotcombineCOCOandVisualGenomeannotationsforanyimage.
E.4 InferenceDetails
InTable3ofthemainpaper,wepresentresultsonTHRONE,POPEandPOPE-Cwhentrainingwithourobjectenumeration
taskandperformingtheobjectenumerationtaskatinference. Inthenextsectionweshowtheeffectofnotperformingobject
enumerationduringinferenceonTHRONEandPOPE,insteaddirectlyaddressingtherelevanttask.
E.5 AblationResultsonImprovedBaseline
Obj.Enum. Obj.Enum. Obj.Enum. THRONE POPE
Model Data Negatives Inference PCLS RCLS F C0 L.5
S
P R F1
✗ N/A N/A 79.9 65.3 76.5 58.0 98.4 73.0
COCO ✗ ✓ 82.4 69.2 79.4 64.8 95.2 77.1
LLaVA-v1.3 COCO+VG ✗ ✓ 85.8 60.4 79.1 66.0 95.3 78.0
COCO ✓ ✓ 83.2 68.8 79.9 73.2 88.2 80.0
COCO+VG ✓ ✓ 86.2 67.0 81.5 83.0 82.5 82.8
✗ N/A N/A 69.9 56.4 66.8 81.9 90.8 86.1
LLaVA-v1.5 COCO+VG ✓ ✗ 79.3 76.1 78.6 83.2 86.4 84.8
COCO+VG ✓ ✓ 86.1 77.0 84.1 89.8 83.7 86.7
Table8.EffectofNegativesandInference:IncludingnegativesinourobjectenumerationtaskimprovesperformanceonTHRONEand
POPEintermsofprecisionandF-score. PerformingtheobjectenumerationtaskatinferencetimeimprovesperformanceonTHRONE
andPOPE,buthampersinferencetimeastheobjectenumerationtaskcangeneratelongsequences.
F Limitations
InthispaperwepresentTHRONEwhichisasteptowardsmeasuringandmitigatinghallucinationsinLVLMs,nonetheless,
ourworkhasfewkeylimitationswhichwelistbelow.
1. THRONE is concerned with only measuring hallucinations in LVLM predictions in the form of a false existence of an
objectinaclosedsetofclasses. AsobservedinLLMs, hallucinationsaremuchmoremultifacetedandincludenotjust
objects outside a pre-defined vocabulary, but also many abstract concepts such as wrong reasoning relating to a visual
sceneaswellaswrongattributesofaparticularobjectsorperson. Theseadditionalhallucinationsarenotpossibletobe
measuredwithTHRONEwithoutmodifications.
2. The presented method of THRONE only focused on “Type-I hallucinations” which does not paint a complete picture
of the hallucinating behavior of an LVLM. Indeed we present POPE-C in Fig. 7 to extend hallucination measurements
in both Type-I and Type-II hallucinations. We present POPE-C as an extension of POPE since we observe that POPE
severelyundercountshallucinationsinType-IIform.
3. Due to lack of general and exhaustive ground truth object label lists for a given image, our method relies on curated
datasets such as MCOCO or Object365 that have detailed annotations that are complete on an image level, which are
neededforourevaluation.
4. OurmethodfocusesonlyonthehallucinationbiasofLVLMsbutdoesnotincludemeasurementsofothertypesofbiasof
LVLMgenerations(e.g.relatedtoconceptsoffairnessingeneration)whichweleaveforfuturework.
G EthicalConsiderations
We present THRONE which is a general evaluation pipeline for measuring hallucinations (specifically “Type-I” halluci-
nations) in Large-Vision-Language Models (LVLMs). Overall we believe that our contribution is ethically positive as it
measures and shows that existing public LVLMs are not yet ready to be deployed in mission critical applications, as we
observe that they still suffer from hallucinating objects to a large extent. In addition we believe our presented evaluation
frameworkalsoprovidesfora“north-star”inmeasuringevaluationandcanaidthefieldandpractitionersalikeinmeasuring
andmakingprogresstowardsreducingevaluationsinLVLMsaswellaselectingtouseoneLVLMoveranother.Wenotethat
measuringsocietalbiasinLVLMsishighlyimportantpre-requisitebeforetheirdeployment,howeverthisisnotinvestigated
inthecurrentwork.