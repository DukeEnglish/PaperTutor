Open Source Language Models Can Provide Feedback: Evaluating
LLMs’ Ability to Help Students Using GPT-4-As-A-Judge
CharlesKoutcheme NicolaDainese SamiSarsa∗
charles.koutcheme@aalto.fi nicola.dainese@aalto.fi sami.j.sarsa@jyu.fi
AaltoUniversity AaltoUniversity UniversityofJyväskylä
Espoo,Finland Espoo,Finland Jyväskylä,Finland
ArtoHellas JuhoLeinonen PaulDenny
arto.hellas@aalto.fi juho.2.leinonen@aalto.fi paul@cs.auckland.ac.nz
AaltoUniversity AaltoUniversity TheUniversityofAuckland
Espoo,Finland Espoo,Finland Auckland,NewZealand
ABSTRACT helpfulsuggestionsornext-stephints[13].Recentadvancements
Largelanguagemodels(LLMs)haveshowngreatpotentialforthe inlargelanguagemodels(LLMs)havedemonstratedpromisingca-
automaticgenerationoffeedbackinawiderangeofcomputing pabilities,providingrapid,human-likefeedbackthatcouldimprove
contexts.However,concernshavebeenvoicedaroundtheprivacy supportinprogrammingcourses[5].
andethicalimplicationsofsendingstudentworktoproprietary DespitethepromiseofLLM-basedfeedbackapproaches,wesee
models.Thishassparkedconsiderableinterestintheuseofopen twokeyissueslimitingtheirwide-scaleadoption.Firstly,thefeed-
sourceLLMsineducation,butthequalityofthefeedbackthatsuch backprovidedmustbeaccurateandreliable–itshouldidentify
openmodelscanproduceremainsunderstudied.Thisisaconcern issuescorrectly,notintroduceconfusion,andguidestudentsto-
asprovidingflawedormisleadinggeneratedfeedbackcouldbe wardssolvingproblemsindependently.Althoughstate-of-the-art
detrimentaltostudentlearning.Inspiredbyrecentworkthathas LLMsexhibitimpressiveabilitiesintaskssuchasbugdetection
utilisedverypowerfulLLMs,suchasGPT-4,toevaluatetheout- andcoderepair[27,34],theyarenotinfallibleandpresentingstu-
putsproducedbylesspowerfulmodels,weconductanautomated dentswithmisleadingfeedbackcouldbedetrimentaltolearning.
analysisofthequalityofthefeedbackproducedbyseveralopen Secondly,concernsabouttheprivacyandethicalimplicationsof
sourcemodelsusingadatasetfromanintroductoryprogramming sendingstudentworktoproprietaryLLMshaveledtocallsfor
course.First,weinvestigatetheviabilityofemployingGPT-4asan greateruseofopenmodels[43].Open-sourceLLMsarebecoming
automatedevaluatorbycomparingitsevaluationswiththoseofa aviableoption,yettheevaluationofopen-sourcealternativesis
humanexpert.WeobservethatGPT-4demonstratesabiastoward underexplored,especiallyinthecontextofcomputingeducation.
positivelyratingfeedbackwhileexhibitingmoderateagreement Inthiswork,ouroverarchinggoalistoevaluatethequalityof
withhumanraters,showcasingitspotentialasafeedbackevaluator. feedbackonstudent-writtenprogramsproducedbyLLMs,inpartic-
Second,weexplorethequalityoffeedbackgeneratedbyseveral ulartoidentifywhetheropen-sourcemodelscanbeacompetitive
leadingopen-sourceLLMsbyusingGPT-4toevaluatethefeedback. alternativetoproprietarymodels.Wefocusonfeedbackthatiden-
Wefindthatsomemodelsoffercompetitiveperformancewithpop- tifiesmistakesinstudentcode,suchasthosethatleadtocompiler
ularproprietaryLLMs,suchasChatGPT,indicatingopportunities errorsortestfailures,asthesearethemostcommontypesofissues
fortheirresponsibleuseineducationalsettings. identifiedbyexistingautomatedfeedbacktools[13].
Our work begins by comparing evaluations of programming
CCSCONCEPTS feedbackgeneratedbyGPT-4withthoseofexperthumanraters,
similartopriorworkcomparingGPT-4andhumanjudgmentsof
•Socialandprofessionaltopics→Computingeducation.
quality[22,37].Afterestablishingthevalidityofthisapproachfor
evaluatingprogrammingfeedback,weapplyitatscaletoevaluate
KEYWORDS
thequalityoffeedbackgeneratedbyarangeofsmalleropen-source
opensource,largelanguagemodels,generativeAI,LLMs,automatic
models.Ourresearchquestionsare:
feedback,automaticevaluation,programmingfeedback,LLM-as-a-
judge,Zephyr,CodeLlama,GPT-4 RQ1 Howeffectivelycanlargelanguagemodels,suchasGPT-
4,assessthequalityofprogrammingfeedbackgenerated
1 INTRODUCTION bylanguagemodelsrelativetoexperthumanjudgment?
Feedbackisessentialforstudentsuccess,yetdeliveringprompt RQ2 Howdoopen-sourcelargelanguagemodelscompare
high-qualityfeedbackisachallenge.Thisisespeciallytrueinlarge againstproprietarymodelsingeneratinghigh-qualityfeed-
computingclasseswheredemandcontinuestorise[23,24].Auto- backonprogrammingquestionsaskedbystudents?
matedfeedbacktools,incorporatinganalysistechniquesandtesting
Toanswerourfirstresearchquestion,weuseanexistingdataset
frameworks,arethusincreasinglypopular[12,25].Althoughthese
fromasinglecoursewherefeedbackonstudenthelprequestshad
toolstypicallyidentifycodeerrors,theyoftenfallshortinoffering
beengeneratedbyanLLM(GPT-3.5).Thisdatasetcontainshuman
∗AlsoaffiliatedwithAaltoUniversity. expertevaluationsfortheLLM-generatedfeedbackwithrespectto
4202
yaM
8
]LC.sc[
1v35250.5042:viXraCharlesKoutchemeetal.
completeness,perceptivityandselectivity.WethenuseGPT-4,ap- GPT-4canreachover80%agreementwithhumans,matchingthe
plyingthesamerubricastheexperthumanrater,tore-evaluatethe levelofinter-agreementbetweenhumans.
qualityofthefeedback.OurresultsestablishthatGPT-4canreliably Ourworkisthusparticularlyclosetoeffortsinvestigatingwhether
identifylow-qualityfeedback,butmightbeoverlyoptimisticin ChatGPTmodelscaneffectivelyactasajudgeinmorespecificdo-
general.Toansweroursecondresearchquestion,wegeneratenew mainssuchaslaw[37].Inthiswork,weuseGPT-4asasingle
feedbacktothesamestudentrequestsforhelpfromtheoriginal answergradingjudge,whereweasktheLLMtodirectlyassigna
dataset,thistimeusingmultipleopen-sourcemodelsfromtheCode scoretothefeedbackgeneratedbyanotherLLM(GPT-3.5).
LlamaandZephyrfamilies.Weevaluatethequalityofthisfeedback Whilerecentwork[32,45]hasalreadyusedGPT-4aspartof
automaticallyusingGPT-4asajudge. theirevaluationstrategies,alsoineducationalsettings[8,21],the
questionofhoweffectiveLLMs(orinparticularGPT-4)canbein
assessingthequalityofthefeedbackgeneratedbyotherLLMshas
2 BACKGROUND
notbeenstudiedexplicitly.
2.1 AutomaticFeedbackinComputing
Education 3 METHODS
Generatingautomatedfeedbackisoneofthelong-standingchal- Inthissection,wedescribeourmethodologytoanswerourtwo
lengesincomputingeducationandhasintriguedresearchersfor researchquestions.Wefirststartbydescribingthedataatthesource
decades[13].Inasystematicliteraturereviewofautomatedfeed- ofourexperiments.Then,giventhatthemaininterestofthispaper
backgenerationforprogrammingexercises,Keuningetal.[13] isaroundtheevaluationofopenlanguagemodels,wedescribe
foundthatautomaticfeedbackmostlyfocusesontellingstudents firstthemethodologyaroundRQ2(howweevaluatevariousopen
the mistakes present in their solutions, while giving formative sourcelanguagemodels),beforedescribingthemethodsforRQ1
feedbacktohelpstudentsovercomeobstaclesisrare.Thus,large (howtheevaluationofsuchmodelscanbestreamlinedwithGPT-4).
languagemodelsprovideanexcitingopportunityinpotentially
fillingthisgap.
3.1 Data
Indeed,therehasbeengreatinterestingeneratingautomated
Weacquiredthedatafromtheauthorsof[6]forourstudy,following
feedbackusinglanguagemodelsinthepastyear[1,6,14,19,20,
thenationalprotocolsforethicalresearch.Thedatacomesfroman
26, 28, 29, 31]. Most research to date has suggested that LLMs
openonlineintroductoryprogrammingcourseorganizedbyAalto
can be used for automatic feedback generation, but have many
UniversityinFinlandthatusesDartastheprogramminglanguage.
limitations.Forexample,Balseetal.[1]foundthattherewashigh
Thecourseprovidesremotehelpfunctionality,wherestudentscan
variabilityinthequalityoffeedbackgeneratedbyGPT-3,andit
askforhelponprogrammingexercisesthatfailautomatedtests
wouldsometimesgenerateincorrectandinconsistentfeedback.Ina
withthepressofabutton.Thedatahastenrandomlysampled
similarvein,Hellasetal.[6]foundthatwhileGPT-3.5wouldoften
helprequestsforthetop15programmingexerciseswiththemost
findactualissuesinstudentcodeandprovideappropriatefeedback,
helprequests(150helprequestsintotal).Theoriginalstudy[6]
itonlysometimesdetectedalltheissuespresentinthecode,and
evaluatedlargelanguagemodelsforautomaticallygeneratingre-
wouldalsooftenhallucinateissuesnotpresentinthecodeatall.
sponsestohelprequests.Theautomaticallygeneratedresponses
Kiesleretal.[14]foundthatChatGPTwouldworkbetterforsome
werequalitativelyanalyzedandannotatedbyahumanevaluator
typesoferrorsthanothers;forexample,itwouldprovidegood
withextensiveexperienceinteachingprogrammingusingmultiple
feedbackoncompilationerrors,butperformedmorepoorlyfor
criteria.Forthepresentstudy,wetookthequalitativeanalysisas
logicandsemanticerrors,orwhenmultipleerrorswerepresent
a starting point, focusing on the following criteria for the help
simultaneouslyinstudentcode.Thisisinlinewiththefindingsof
requestresponseevaluations:
Leinonenetal.[19]andPhungetal.[28]whobothfoundthatLLMs
couldbeusedtogivefeedbackonsyntaxerrors.Currentstate-of- completeness Identifiesandmentionsallactualissues.
the-artmodelssuchasGPT-4struggletomatchhumanperformance perceptivity Identifiesandmentionsatleastoneactualissue.
ingeneratingfeedbackonprogrammingexercises[29]. selectivity Doesnotidentifynon-existentissues.
Thefirsttwocriteriaconsiderhowwellissuesincodearead-
2.2 UsingLanguageModelsasJudges dressedbythefeedback,whilethelastoneindicateshowwellthe
Language Models such as ChatGPT have started to reach near- feedbackavoidsgivingoutmisleadinginformation.Forthepresent
humanperformanceinmanytasks[9],whichhassparkedinterest study,welimitedourselvestotheresponsesfromthebestmodel
inusingthemforevaluatingtheoutputofotherLLMs. oftheoriginalstudy(GPT-3.5),aswellastheannotationsforthe
Theideaofusingalargelanguagemodeltojudgetheoutput GPT-3.5producedresponsesfromthehumanevaluator.
ofotherLLMs–LLMs-as-judges–wasfirststudiedinthework Inourpreliminaryevaluations,wealsostudiedthepossibility
ofZhengetal[44].Theauthorsproposedthreevariationsofthe of verifying whether the response had unwanted, duplicate, or
LLMs-as-judgesparadigm:(1)pairwisecomparison(i.e.,selecting repetitivecontent(e.g.GPT-3.5providingamodelsolutionthatthe
whichoftwoLLMsoutputisthebest),(2)singleanswergrading promptexplicitlyaskednottoprovide)asconductedintheoriginal
(i.e.,scoringanLLMsingleanswer),and(3)reference-basedgrading study.GPT-4wasperfectatjudgingtheseandonlyselectedthe
(i.e.,singleanswergradingwithrespecttoareferencesolution). criteriathatfocusedonthecontentofthefeedback(notthestyle).
Across the three scenarios, results showed that models such as Weomitdiscussingthisaspectduetopagelimitations.OpenSourceLanguageModelsCanProvideFeedback
3.2 GeneratingFeedback Wegradedthequalityofthefeedbackgeneratedforeachin-
Prompting. Usingtheincorrectprogramsofthedatasetdescribed correctprogramforeachmodelusingGPT-4asajudgewiththe
inSec3.1,wezero-shotpromptedarangeoflanguagemodelsto promptshowninFigure2.Theexperimentswereperformedwith
generatefeedbackexplainingalltheissuesinanincorrectstudent theopenlanguagemodelsusingHuggingface’sTransformersli-
program.Weiterativelyrefinedourfeedbackprompt[3];Figure1 brary[41].GPT-3.5andGPT-4werequeriedusingusingOpenAI’s
showsthefinalpromptusedintheseevaluations.Followingprior PythonAPI.
work[6,36],wegenerateasinglefeedbackusinggreedydecoding
(i.e.,selectingthetokenwiththehighestprobabilityasthenext 3.3 AutomaticFeedbackEvaluation
elementinthesequence). OuraimregardingRQ2wastoestimatethecapabilityofGPT-4
asajudgeinassessingautomaticfeedbackquality.Forthis,we
taskedGPT-4toannotatethe150originalGPT-3.5generatedfeed-
PromptingLLMsforFeedback backaccordingtothethreecriteriaoutlinedabove(completeness,
perceptivityandselectivity).Wetreatedthefeedbackannotation
Youareacomputerscienceprofessorteachingintroductoryprogram-
taskasthreedistinctbinaryclassificationproblems(oneproblem
mingusingDart. 1
foreachcriterion,oneinstanceforeachhelprequest),wherethe
Belowisaproblemdescriptionaccompaniedbyanexamplesolution. correctlabelsforeachclassarethehuman-annotatedonesfromthe
Youarealsoprovidedwithanincorrectprogram(i.e.itdoesnotpassall originalstudy.FollowingpriorworkinusingLLMsandjudges[44],
unittests)writtenbyastudent.Yourtaskistoprovidesuggestionson wesampledasingleanswerfromGPT-4usinggreedydecoding,
howthestudentcouldfixtheircodesothatitfulfilsalltherequirements leavingallotherhyperparametersatdefaultvalues.
in the problem description. Your suggestions should only improve
thefunctionalcorrectnessoftheincorrectprogram,soyoucanleave
stylisticsuggestionsaside.Donotincludecodeinyourfeedback. 2 PromptingGPT-4togradeGPT-3.5generatedfeedback
##Problemdescription: Youareacomputerscienceprofessorteachingintroductoryprogram-
<handout> mingusingDart. 1
##Modelsolution: 3 Belowisaproblemdescription,amodelsolution,andanincorrect
<sample_solution> programwrittenbyastudent.Youarealsoprovidedwiththefeedback
generated by GPT-3.5. Your task is to evaluate the quality of the
##StudentCode: feedback(bysayingyesorno)toensureitadherestothemultiple
<submitted_code> criteriaoutlinedbelow.Foreachcriterion,provideyouranswerin
aseparatelinewiththeformat'(CRITERIA_NUMBER):Yes/No'.Do
notprovidecomments,butbeattentivetotheproblemdescription
requirements. 2
Figure1:Feedbackprompttemplate.Weprovide(1)asystem
promptspecifyingthebehaviourofthemodel,(2)adescrip- ##Problemdescription:
<handout>
tionofthefeedbacktask,and(3)contextualinformation.
##Modelsolution:
<sample_solution>
Models. Weevaluatedfivepowerfulopen-sourcemodels.The
firstthreemodelsCodeLlama-7B[35],CodeLlama-13B[35],and
##StudentCode:
CodeLlama-34B[35],areLlama2[38]languagemodelsreleased <submitted_code>
byMetaandhaverespectively7,13,and34billionparameters.The
3
lasttwomodelsareZephyr-7B-𝛼[39]andZephyr-7B-𝛽[39],two ##Feedback:
versionsofa7BparametersMistral[10]modelfurtherinstruction- <feedback>
tunedwithDirectPreferenceOptimization[32]byHuggingFace1.
Wechosethesemodelsbecauseoftheirextensivedocumentation, ##Criteria:
communityadoption,respectiveperformanceoncodeandlanguage
(1)Identifiesandmentionsallactualissues
reasoningbenchmarks(HumanEval[2]andMMLU[7])andability
(2)Identifiesandmentionsatleastoneactualissue
to follow instructions. Discussing the details of these language
(3)Doesnotidentifynon-existentissues
modelsisoutofthescopeofthispaper,andweinvitethereaderto
checktheoriginalpapersformoreinformation.Wealsoevaluated
GPT-3.5andGPT-4onourfeedbacktask.Sincethepromptused
Figure2:Judgingprompttemplate.Weprovide(1)asystem
inourworkslightlydiffersfromtheoriginalstudyfromHellaset
promptspecifyingGPT-4’sbehaviour,(2)adescriptionofthe
al.[6](i.e.,weprovidedamodelsolutionasadditionalcontextual
gradingtask,and(3)contextualinformation.
information[30]),wereranthefeedbackexperimentforGPT-3.5.
Figure2outlinesthepromptusedtogradetheLLMfeedback,
1TheexactmodelcodenamesonHuggingFacearecodellama/CodeLlama-Instruct-
{7b/13b/34b}-hfandHuggingFaceH4/zephyr-7b-{alpha/beta} whichportraysaformofrubricgrading[42]toallowthemodeltoCharlesKoutchemeetal.
considerallproblemssimultaneously.Wenotethatinthiscase,the unlikeprecisionandF0.5,considerthepossibilitythatagreement
orderinwhichthecriteriaareoutlinedisimportant.Wepurposely occursbychance.Thekappascoresarehighestoncompleteness
promptedthemodeltoprovideitsanswerinthegivenorder(com- (0.48) and selectivity (0.40) indicating moderate agreement and
pleteness,toselectivity),asananswertothecompletenesscriteria somewhatloweronperceptivity(0.21)wherethescoreindicates
influencesselectivity.Similartotheoriginalstudy,student-written fairagreement.Tofurtherelaboratetheissueofskewininterpreting
helprequestmessagesareexcluded,astheyoftendidnotprovide thescores,whencomparedagainstadummymodelthatpredicts
muchmeaningfulcontextualinformation[6]. everythingasthemajoritylabel,theGPT-4perceptivenessshows
WeuseprecisionandF0.5-score2asourmainmetricsinevalu- onlyasmallimprovementof2percentagepointsforbothprecision
atingthejudge,sincebothofthesemetricsemphasizeamodel’s andF0.5.Incontrast,incompletenessandselectivity,wherethe
abilitytominimisefalsepositives,which,inourcontext,translate dataismorebalanced,GPT-4scoresare10percentagepointshigher
tomisleadingfeedback.Wealsoreportrecall,F1score,andCohen’s thanthoseofthedummymodel(weomitthedummymodelscores
kappaforcompleteness. forthesakeofbrevity,thescorescanbecomputedfromTable1).
Nonetheless,lookingatallthemetrics,wecanobservethatthe
4 RESULTSANDDISCUSSION judgemaintainsahighrecallacrossallcriteria(i.e.,classification
WefirstpresentourresultsregardingtheuseofGPT-4asanauto- tasks),whilemaintainingreasonableprecisionandaccuracy.
maticjudge,i.e.,evaluatorofquality(RQ1),in4.1.Then,wepresent
Sourcesofmisjudgment. Ourresultsareencouraging,although
theresultsregardingtheabilityofgivingfeedbackoflargelanguage
there still is a significant margin of improvement in the judge
models(RQ2),in4.2.
model’sperformance.Thismaybeduetoeithertheperformance
of the LLM used as a judge, or the human-LLM agreement on
4.1 AutomaticFeedbackEvaluation
whatistobeconsideredamistake.Theabilityofajudgelanguage
Table1showsthegroundtruth(fromthehumanexpert)andthepre-
modeltogradetheoutputofanotherLLMstronglydependson
dicted(fromthejudge)annotations.Accordingtothegroundtruth
itsabilitytosolvethespecificproblemathand[44].Thatis,for
annotations,repeatingtheobservationsoftheoriginalstudy[6],
GPT-4todecidewhetherGPT-3.5identifiedallissuesinaprogram
onlyabitmorethanhalfofthefeedbackarecomplete,whilemost
(completeness)wouldrequireGPT-4itselftoidentifytheminthe
ofthemareperceptive.Manyofthefeedbackalsocontainsome
firstplace.Similarly,askingGPT-4toidentifywhetherGPT-3.5
misleadingcontent.Whenlookingintothepredictedannotations,
ishallucinatingelementsofitsfeedback(selectivitycriterion)is
wenoticethatthejudgetendstogradethegeneratedfeedback
relatedtoGPT-4nothallucinatingthesameissue.Someevidence
morepositivelycomparedtotheexpertannotator,indicatingsome
fortheshortcomingsofGPT-4isprovidedintheworkofPhung
degreeofpositivebias.
etal.[29],whichshowsthatalthoughthemodelisquitegoodat
providingprogrammingfeedback,thegeneratedfeedbackarenot
Table1:HumanandJudgeannotationstatistics. voidofmistakes.Thismaybeevenmoretrueinourstudywhere
weuseDart,aprogramminglanguageinwhichGPT-4ismostlikely
Groundtruth Predicted lessproficientasinPython.Theobservedperformanceoutcomes
True False True False mayalsobeaffectedbyadiscrepancyinthewayalanguagemodel
completeness 82 68 113 37 perceiveserrorscomparedtoahuman.Itisalsopossiblethereare
perceptivity 127 23 127 23 caseswherethejudgeiscorrectandtheoriginallabelisincorrect.
selectivity 78 72 106 44
total 283 167 246 104 Researchandpracticalimplications. Wefeelitisimportantto
highlightthatourresultsdonotprovidea"per-feedback"guarantee
ofthequalityofthejudgement,butrather,astatisticaloverviewof
Table2:GPT-4judgingclassificationperformanceresultsfor thecurrentabilityofGPT-4toappropriatelyjudgesuchfeedback.
eachindependentcriterion. Theseresultssuggestthatautomatedevaluationoffeedbackisnot
yetreadytorelyexclusively onLLMevaluation–humanjudge-
precision recall f0.5 f1 accuracy kappa mentisstillnecessary.Inparticular,webelievethatLLMjudge
completeness 0.70 0.95 0.74 0.81 0.75 0.48 evaluationshouldnotbeusedasawaytoassessthequalityofa
perceptivity 0.84 1.00 0.87 0.91 0.85 0.22 singlegeneration(beforebeinggiventoastudent).However,an
selectivity 0.65 0.94 0.69 0.77 0.71 0.40 LLMjudgecouldpotentiallybeanassistanttoahumanevaluator,
andanLLMevaluationcouldalsobeusedasacomparativemethod.
Forinstance,itcouldbeusedinevaluatingthequalityofprompting
Table2showstheclassificationresultsforGPT-4.Lookingatour
techniques and generation parameters, or exploring alternative
mainmetrics,precisionandF0.5,weseethatthejudgeperforms
languagemodels(e.g;opensourceonces)acrossalargenumber
reasonablywellinclassifyingcompleteness,andalittleworsein
ofgenerations.Still,eveninthesescenarios,ideally,wewouldnot
selectivity. The judge achieves higher scores in perceptiveness,
beusingGPT-4asajudgeforresearchfullyonitsown,butasa
althoughthiscanbepartlyattributedtoskewinthedata.Theeffect
complementtohumanevaluations[8].
ofskewinthemainmetricsisreflectedinthekappascoreswhich,
2F0.5-scoreistheharmonicmeanofprecisionandrecallthatweighsprecisionmore Open-sourcejudges. UsingGPT-4asajudgehasshownpromises,
heavily.ItisavariationofthecommonlyusedF1-scorewhichweighsbothequally. inparticularinresearch[44].However,theproprietarynatureofOpenSourceLanguageModelsCanProvideFeedback
thestate-of-the-artLLMhasledtotheriseofjudgesbasedonopen- recentZephyr-7Bmodels(releasedOctober2023)outperformeven
sourceLLMs.PandaLM[40]isanopen-sourceLLMthathasbeen thelargestCodeLlamamodel(34B)despitethelatterbeingalmost
fine-tunedforselectingthebestresponsetoan(instruction,input) 5timeslarger.Ontopofthis,weseethatthesestrongopen-source
pairandprovidinganexplanationtogetherwithitsdecision.In modelsarecompetitivewithproprietarymodels,achievingaperfor-
similarworks,JudgeLM[46]focusesonincreasingtheperformance mancecomparablewithGPT-3.5.Forinstance,wereportfeedback
ofthe‘judge’LLM,whilePrometheus[15]emphasizesachieving generatedbyGPT-3.5andbyZephyr-7B-𝛽inFigure4:GPT-3.5’s
granularfeedbackaccordingtoacustomscorerubric. feedbackismoredetailed,butZephyr-7B-𝛽’sfeedbackcontains
Alltheseworksfollowacommonscheme:theycreateahigh- alltheinstructionsneededtofixtheprogramaswell.However,
qualityimitationdataset,wheretheinputsarehuman-curatedand thereisstillagapinperformancebetweenthemandGPT-4.In
high-performanceclosedmodelslikeGPT-4providethedesired particular,whileGPT-4canprovidecomprehensivefeedbackin
outputs;then,theyfine-tuneoneormoreopen-sourcemodelson 99%ofthecases,theperformancedropsto75%forZephyr-7B-𝛼
thedataset.Althoughtheirresultsarepromising,closedmodels andto70%forZephyr-7B-𝛽.OnedoeswelltonotealsothatGPT-4
suchasGPT-4remainthestate-of-the-artmodel,whichiswhywe isthejudgeandthejudgeislikelytofavouritself(andperhapsits
chosetouseGPT-4forourstudy. oldersiblingGPT-3.5too)throughincorporatingthesametendency
tohallucinateasthejudged[44].
4.2 Feedbackgeneration Insummary,ourresultssuggestthat(1)morerecent,smaller
Comprehensiveandinsightfulfeedback. Forreal-lifeapplications, languagemodelscanoutperformsomeofthebiggerones,(2)open-
educatorsaremostlyinterestedincomprehensivefeedbackwhich sourceLLMsarebecomingcompetitivewithproprietarymodels,
respectsthethreecriteria(i.e.,feedbackthatidentifiesallissues, and(3)open-sourcemodelsarenotreachingyetthestate-of-the-art.
withoutprovidingnon-truthfulinformation).Whileprovidingstu-
dentswithallusefulinformation,thistypeoffeedbackleaveslittle Opensourcevsclosedsourcemodels. Open-sourcelanguagemod-
roomforthestudentstofigureouthowtoprogressthemselves. elshaveoftenfallenshortoftheperformanceofproprietarymodels.
Althoughamodelmightnotalwaysbegoodatidentifyingallis- Ourresultsconfirmthatnotallopenlanguagemodels(evenrecent
sues,themodelcouldbegoodatrecognisingoneissuethatwould ones)performaswellastheirnon-permissivecounterparts(for
"unstuck"astudentandleavethemtofigureouttherest.Such instance,CodeLLamamodelsarenotcompetitivewithGPT-3.5).On
insightfulfeedback(i.e.,perceptiveandselective)couldbeprovided topofthis,abarriertotheuseofopen-sourceLLMsineducation
ashintstostudents.Thesetwotypesoffeedback(comprehensive usedtobetheneedtohavecustomcomputationalresources(e.g.
and insightful) is the main focus of our first analysis of the re- customGPUs)duetotheirsize.Forinstance,runningCodeLlama
sults.Figure3showstheproportionsoffeedbackgeneratedbyeach with34Bparametersrequired2GPUs.
languagemodelwhichfallunderthetwofeedbacktypes. Nonetheless,ourresultsalsohighlightverypositiveoutcomes.
Zephyr-7B-𝛼,a7billionparametersopen-sourcelanguagemodel
performedaswellasGPT-3.5.Whatmakesourresultparticularly
Fractions of Comprehensive and Insightful feedback
interestingisthatthislanguagemodelisrelativelysmall,which
for different models
makesrunningitoncustomresourcescheaper.Inotherdomains,
Comprehensive
1.0 Insightful 0.990.99 evensmallerlanguagemodelsarereachingimpressiveperformance,
andresearchintohowtomakeopenlanguagemodelsreachbet-
terperformanceforeducationalpurposesisontherise[16–18].
0.8 0.75 0.74 NewmethodssuchasquantizationorCPUaccelerationallowsuch
0.70 0.70 0.69 modelstorunonmodestconsumerlaptopssuchastheonesofedu-
0.65
cators.Ontopofthis,LLMdeploymentisbecominglessofabarrier
0.6
duetoopen-sourcehostingservicessuchasHuggingFace[41].
0.4 0.380.39
0.350.35 5 CONCLUSIONS
Inthispaper,weevaluated(1)towhatextentGPT-4couldbeusedto
0.2 0.180.19 assessautomaticallygeneratedprogrammingfeedback,and(2)how
welldifferentlargelanguagemodels,includingopen-sourceones,
perform in generating feedback on student code (automatically
0.0
code code code zephyr zephyr gpt-3.5 gpt-4 evaluatedusingGPT-4).OurfindingssuggestthatGPT-4canbe
llama-7b llama-13bllama-34b alpha-7b beta-7b turbo
Models quitereliableinassessingthequalityofautomaticallygenerated
feedbackandthatopen-sourcelanguagemodelscanbeusedto
Figure3:Fractionof‘comprehensive’(i.e.,satisfyingallthree generateprogrammingfeedback.AsLLM-generatedfeedbackcan
criteria)and‘insightful’(i.e.,perceptiveandselective)feed- be generated on demand directly in the learning environments
backforalllanguagemodels. thatstudentsuse,itcouldbealow-costandlow-barrierscaffoldto
helpstudentswhiletheyarelearningtoprogram.Thiscouldleave
Whencomparingthemodelsintermsofperformance,wecan moretimefortheinstructorandteachingassistantstofocusonthe
makethefollowingobservations.First,wenoticethatthemore morecomplexcaseswhereLLMsmightcurrentlystruggletohelp
smelborp
fo
noitcarFCharlesKoutchemeetal.
students.Asanadditionalcontribution,wealsoreleasethecode
Problemdescriptionandincorrectcodesolution
usedforconductingourexperiments3.
Writeaprogramthataskstheuserfortheirfirstandlastname.The Althoughtherewillalwaysbesituationswherestudentschoose
programthenprintsa"JamesBond"-likegreeting.Forexample,with touseclosedsourcemodelstoseekfeedbackoranswerquestions,
firstname‘James‘andlastname‘Bond‘,theoutputoftheprogram
awiderangeofneweducationaltoolspoweredbyLLMsarebeing
shouldbeasfollows:
developedandintegratedintocomputingclassrooms[4,11,20].The
modelsthatsuchtoolstypicallyuseisdeterminedbythedeveloper,
“‘Enterfirstname.
and our work shows that developers may increasingly rely on
Enterlastname.
MynameisBond,JamesBond. smalleropensourcemodelswithoutsacrificingperformanceor
“‘ withconcernsaroundprivacy.Additionally,openmodelsprovide
1 main() { greatercertaintyintermsofbeingaccessibleandfine-tunablefor
2 print('Type␣in␣text.');
3 var input = stdin.readLineSync(); specialisedpurposes.Theyarealsopotentiallymorecosteffective.
4 print('You␣typed␣in␣$input'); Ourworkhaslimitations.Forthispreliminaryanalysis,weonly
5 }
takealookatasmallsubsetofdatacomingfromoneinstitution,and
FeeedbackgeneratedbyGPT-3.5 wewillneedmoretomakeourresultsevenmorerobust.SinceDart
ishardlythemostpopularprogramminglanguage,wehypothesize
Tofixyourcode,youneedtomodifyittoasktheuserfortheirfirst
thatGPT-4asajudgecouldexhibitstrongerevaluationperformance
andlastname,andthenprintthegreetingintherequiredformat.
foralanguagewithmorepresencesuchasPython.Moreover,we
Herearethestepsyoucanfollowtofixyourcode:
only evaluated the ability of GPT-4 to judge the quality of the
1.Replacetheline‘print(’Typeintext.’);‘with‘print(’Enterfirstname.’);‘
feedbackgeneratedbyasingleotherlanguagemodel(GPT-3.5),for
toprompttheusertoentertheirfirstname.
2.Replacetheline‘varinput=stdin.readLineSync();‘with‘varfirstName whichGPT-4mightbebiasedtowardsgivingpositiveresults[33].
=stdin.readLineSync();‘tostoretheuser’sfirstnameinthevariable However,thiswouldthusmeanthatGPT-4mightbemorereliable
‘firstName‘. forotherlanguagemodelsnotofitsfamily(e.g.open-sourceones).
3.Replacetheline‘print(’Youtypedin$input’);‘with‘print(’Enterlast Aspartofourfuturework,wearefocusingontwodistinctareas:
name.’);‘toprompttheusertoentertheirlastname. improvingthejudgmentsfromLLMsonthequalityoffeedback,and
4.Addanewline‘varlastName=stdin.readLineSync();‘tostorethe evaluatingopen-sourceLLMstoserveasthejudges.Inthiswork,
user’slastnameinthevariable‘lastName‘.
wezero-shotpromptedthemodeltoprovidefeedbackonallthree
5.Replacetheline‘print(’Mynameis$lastName,$firstName$last-
qualitycriteriaatonce.Byimprovingthepromptingstrategy(e.g.
Name.’);‘with‘print(’Mynameis$lastName,$firstName$lastName.’);‘
usingzero-shotchainofthought),theperformanceoftheLLMasa
toprintthegreetingintherequiredformat.
judgecouldpotentiallyincrease.Theotherareawearecurrently
FeeedbackgeneratedbyZephyr-7B-𝛽 workingonisevaluatinghowwellopen-sourcelanguagemodels
couldworkasthejudgeoffeedbackquality.Thiscouldmakeit
Tofixthestudent’scodeandmakeitfulfiltherequirementsofthe
problemdescription,youcansuggestthefollowingchanges: easierforeducatorstousethemodelsbothduetocost-effectiveness
1.Changetheprogram’snamefrom‘main()‘to‘voidmain()‘tofollow andprivacyconcerns.
Dart’snamingconventionsforfunctions.
2.Changetheprogram’sprompttomatchtheproblemdescription: ACKNOWLEDGMENTS
"Enterfirstname."and"Enterlastname."
ThisresearchwaspartiallysupportedbytheResearchCouncilof
3.Createvariables‘firstName‘and‘lastName‘tostoretheuser’sinput
Finland(AcademyResearchFellowgrantnumber356114).
using‘stdin.readLineSync()‘.
4.Concatenatetheuser’slastname,firstname,andlastnameagainto
createthedesiredgreeting,andstoreitinavariablecalled‘myName‘. REFERENCES
5.Printthegreetingusing‘print()‘. [1] RishabhBalse,BharathValaboju,ShreyaSinghal,JayakrishnanMadathilWar-
Here’sthefixedcode: riem,andPrajishPrasad.2023.InvestigatingthePotentialofGPT-3inProviding
1 void main() { FeedbackforProgrammingAssessments.InProceedingsofthe2023Conference
2 print('Enter␣first␣name.'); onInnovationandTechnologyinComputerScienceEducationV.1.292–298.
3 var firstName = stdin.readLineSync(); [2] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondede
4 print('Enter␣last␣name.'); OliveiraPinto,etal.2021. EvaluatinglanguageModelsTrainedonCode.
5 var lastName = stdin.readLineSync();
arXiv:2107.03374[cs.LG]
6 var myName = '$lastName,␣$firstName␣$lastName';
7 print('My␣name␣is␣$myName.'); [3] PaulDenny,VirajKumar,andNasserGiacaman.2023.ConversingwithCopilot:
8 } ExploringPromptEngineeringforSolvingCS1ProblemsUsingNaturalLanguage.
InProc.ofthe54thACMTechnicalSymposiumonComputerScienceEducationV.
1.ACM,NewYork,NY,USA,1136–1142.
[4] PaulDenny,JuhoLeinonen,JamesPrather,AndrewLuxton-Reilly,Thezyrie
Amarouche,etal.2024.PromptProblems:ANewProgrammingExerciseforthe
Figure4:Examplesofgeneratedfeedback.Wepresentaprob-
GenerativeAIEra.InProc.ofthe55thACMTechnicalSymposiumonComputer
lemdescriptiontogetherwiththeincorrectcodesolution, ScienceEducationV.1.ACM,NewYork,NY,USA,296–302.
thefeedbackgeneratedbyaproprietarymodel(GPT-3.5)and [5] PaulDenny,JamesPrather,BrettA.Becker,JamesFinnie-Ansley,ArtoHellas,
JuhoLeinonen,AndrewLuxton-Reilly,BrentN.Reeves,EddieAntonioSantos,
thefeedbackgeneratedbyaopen-sourcemodel(Zephyr-7B).
andSamiSarsa.2024.ComputingEducationintheEraofGenerativeAI.Commun.
Problemdescriptions,andstringsandvariablenamesinthe ACM67,2(Jan2024),56–67. https://doi.org/10.1145/3624720
examplesshownhavebeentranslatedfromtheoriginallan-
guage(Finnish);LLMfeedbackwasinEnglishandthusnot
3https://github.com/KoutchemeCharles/iticse24
translated.OpenSourceLanguageModelsCanProvideFeedback
[6] ArtoHellas,JuhoLeinonen,SamiSarsa,CharlesKoutcheme,LiljaKujanpää, Comput.Educ.22,3,Article34(2022),40pages.
andJuhaSorva.2023. ExploringtheResponsesofLargeLanguageModelsto [26] MaciejPankiewiczandRyanS.Baker.2023.LargeLanguageModels(GPT)for
BeginnerProgrammers’HelpRequests.InProceedingsofthe2023ACMConference automatingfeedbackonprogrammingassignments. arXiv:2307.00150[cs.HC]
onInternationalComputingEducationResearch-Volume1.ACM,93–105. [27] HammondPearce,BenjaminTan,BaleeghAhmad,RameshKarri,andBren-
[7] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul danDolan-Gavitt.2023.ExaminingZero-ShotVulnerabilityRepairwithLarge
Arora,etal.2021. MeasuringCodingChallengeCompetenceWithAPPS. LanguageModels.In2023IEEESymposiumonSecurityandPrivacy.2339–2356.
arXiv:2105.09938[cs.SE] [28] TungPhung,JoséCambronero,SumitGulwani,TobiasKohn,RupakMajumdar,
[8] YannHicke,AnmolAgarwal,QianouMa,andPaulDenny.2023.AI-TA:Towards etal.2023.GeneratingHigh-PrecisionFeedbackforProgrammingSyntaxErrors
anIntelligentQuestion-AnswerTeachingAssistantusingOpen-SourceLLMs. usinglanguageModels. arXiv:2302.04662[cs.PL]
arXiv:2311.02775[cs.LG] [29] TungPhung,Victor-AlexandruPădurean,JoséCambronero,SumitGulwani,To-
[9] FanHuang,HaewoonKwak,andJisunAn.2023.IsChatGPTbetterthanHuman biasKohn,etal.2023.GenerativeAIforProgrammingEducation:Benchmarking
Annotators?PotentialandLimitationsofChatGPTinExplainingImplicitHate ChatGPT,GPT-4,andHumanTutors.Int.J.ofManagement21,2(2023),100790.
Speech.InCompanionProceedingsoftheACMWebConference2023.ACM. [30] TungPhung,Victor-AlexandruPădurean,AnjaliSingh,ChristopherBrooks,José
[10] AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,Deven- Cambronero,etal.2023.AutomatingHumanTutor-StyleProgrammingFeedback:
draSinghChaplot,etal.2023.Mistral7B. arXiv:2310.06825[cs.CL] LeveragingGPT-4TutorModelforHintGenerationandGPT-3.5StudentModel
[11] MajeedKazemitabaar,JustinChow,CarlKaToMa,BarbaraJ.Ericson,David forHintValidation. arXiv:2310.03780[cs.AI]
Weintrop,andToviGrossman.2023.StudyingtheeffectofAICodeGenerators [31] JamesPrather,PaulDenny,JuhoLeinonen,BrettA.Becker,IbrahimAlbluwi,
onSupportingNoviceLearnersinIntroductoryProgramming.InProc.ofthe etal.2023.TheRobotsareHere:NavigatingtheGenerativeAIRevolutionin
2023CHIConf.onHumanFactorsinComputingSystems.ACM,NewYork,NY, ComputingEducation. InProceedingsofthe2023WorkingGroupReportson
USA,Article455,23pages. InnovationandTechnologyinComputerScienceEducation.108–159.
[12] HiekeKeuning,JohanJeuring,andBastiaanHeeren.2016.TowardsaSystematic [32] RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.
ReviewofAutomatedFeedbackGenerationforProgrammingExercises.InPro- Manning,andChelseaFinn.2023.DirectPreferenceOptimization:YourLanguage
ModelisSecretlyaRewardModel. arXiv:2305.18290[cs.LG]
ceedingsofthe2016ACMConferenceonInnovationandTechnologyinComputer
ScienceEducation.ACM,41–46. [33] Nazneen Rajani, Nathan Lambert, Sheon Han, Jean Wang, Osvald Nit-
[13] HiekeKeuning,JohanJeuring,andBastiaanHeeren.2018.ASystematicLiter- ski, et al. 2023. Can foundation models label data like humans?
atureReviewofAutomatedFeedbackGenerationforProgrammingExercises. https://huggingface.co/blog/llm-v-human-data.
ACMTrans.Comput.Educ.19,1,Article3(2018),43pages. [34] FranciscoRibeiro,JoséNunoCastrodeMacedo,KanaeTsushima,RuiAbreu,
[14] NatalieKiesler,DominicLohr,andHiekeKeuning.2023.ExploringthePotential andJoãoSaraiva.2023.GPT-3-PoweredTypeErrorDebugging:Investigating
ofLargeLanguageModelstoGenerateFormativeProgrammingFeedback.arXiv theUseofLargeLanguageModelsforCodeRepair.InProc.ofthe16thACM
preprintarXiv:2309.00029(2023). SIGPLANInt.Conf.onSoftwareLanguageEngineering.ACM,111–124.
[15] SeungoneKim,JaminShin,YejinCho,JoelJang,ShayneLongpre,etal.2023. [35] BaptisteRozière,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,etal.2023.
Prometheus:InducingFine-grainedEvaluationCapabilityinLanguageModels. CodeLlama:OpenFoundationModelsforCode. arXiv:2308.12950[cs.CL]
arXiv:2310.08491[cs.CL] [36] JaromirSavelka,AravAgarwal,MarshallAn,ChrisBogart,andMajdSakr.2023.
[16] CharlesKoutcheme.2022.TowardsOpenNaturalLanguageFeedbackGeneration ThrilledbyYourProgress!LargeLanguageModels(GPT-4)NoLongerStruggle
forNoviceProgrammersUsingLargeLanguageModels.InProc.ofthe22ndKoli toPassAssessmentsinHigherEducationProgrammingCourses.InProc.ofthe
CallingInt.Conf.onComputingEducationResearch.ACM. 2023ACMConf.onInt.ComputingEducationResearch-Volume1.ACM,78–92.
[17] CharlesKoutcheme.2023.TrainingLanguageModelsforProgrammingFeedback [37] JaromirSavelka,PaulDenny,MarkLiffiton,andBradSheese.2023. Efficient
UsingAutomatedRepairTools.InArtificialIntelligenceinEducation.Springer ClassificationofStudentHelpRequestsinProgrammingCoursesUsingLarge
NatureSwitzerland,830–835. LanguageModels. arXiv:2310.20105[cs.CY]
[18] CharlesKoutcheme,SamiSarsa,JuhoLeinonen,ArtoHellas,andPaulDenny. [38] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
2023.AutomatedProgramRepairUsingGenerativeModelsforCodeInfilling.In Lachaux,etal.2023.LLaMA:OpenandEfficientFoundationLanguageModels.
ArtificialIntelligenceinEducation.SpringerNatureSwitzerland,798–803. arXiv:2302.13971[cs.CL]
[19] JuhoLeinonen,ArtoHellas,SamiSarsa,BrentReeves,PaulDenny,etal.2023. [39] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,
UsingLargeLanguageModelstoEnhanceProgrammingErrorMessages.InProc. etal.2023.Zephyr:DirectDistillationofLMAlignment.arXiv:2310.16944[cs.LG]
ofthe54thACMTechnicalSymposiumonComputerScienceEducationV.1.ACM, [40] YidongWang,ZhuohaoYu,ZhengranZeng,LinyiYang,CunxiangWang,etal.
NewYork,NY,USA,563–569. 2023. PandaLM:AnAutomaticEvaluationBenchmarkforLLMInstruction
[20] MarkLiffiton,BradESheese,JaromirSavelka,andPaulDenny.2024.CodeHelp: TuningOptimization. arXiv:2306.05087[cs.CL]
UsingLargeLanguageModelswithGuardrailsforScalableSupportinProgram- [41] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDe-
mingClasses.InProc.ofthe23rdKoliCallingInt.Conf.onComputingEducation langue,etal.2020.HuggingFace’sTransformers:State-of-the-artNaturalLan-
Research.ACM,NewYork,NY,USA,Article8,11pages. guageProcessing. arXiv:1910.03771[cs.CL]
[21] HunterMcNichols,WanyongFeng,JaewookLee,AlexanderScarlatos,Digory [42] MikeWu,M.Mosse,NoahD.Goodman,andC.Piech.2019.ZeroShotLearning
Smith,etal.2024. AutomatedDistractorandFeedbackGenerationforMath forCodeEducation:RubricSamplingwithDeepLearningInference.InAAAI.
Multiple-choiceQuestionsviaIn-contextLearning. arXiv:2308.03234[cs.CL] https://doi.org/10.1609/aaai.v33i01.3301782
[22] StevenMoore,HuyA.Nguyen,TianyingChen,andJohnStamper.2023. As- [43] LixiangYan,LeleSha,LinxuanZhao,YuhengLi,RobertoMartinez-Maldonado,
sessingtheQualityofMultiple-ChoiceQuestionsUsingGPT-4andRule-Based etal.2023.PracticalandEthicalChallengesofLargeLanguageModelsinEduca-
Methods.InResponsiveandSustainableEducationalFutures.SpringerNature tion:ASystematicScopingReview.BritishJournalofEducationalTechnology
Switzerland,229–245. (2023).
[23] NationalAcademiesofSciences,Engineering,andMedicine.2018.Assessingand [44] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,
respondingtothegrowthofcomputerscienceundergraduateenrollments.National et al. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.
AcademiesPress. arXiv:2306.05685[cs.CL]
[24] AndyNguyen,ChristopherPiech,JonathanHuang,andLeonidasGuibas.2014. [45] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,etal.2023.LIMA:
Codewebs:ScalableHomeworkSearchforMassiveOpenOnlineProgramming LessIsMoreforAlignment. arXiv:2305.11206[cs.CL]
Courses.InProc.ofthe23rdInt.Conf.onWorldWideWeb.ACM,491–502. [46] LianghuiZhu,XinggangWang,andXinlongWang.2023.JudgeLM:Fine-tuned
[25] JoséCarlosPaiva,JoséPauloLeal,andÁlvaroFigueira.2022.AutomatedAssess- LargeLanguageModelsareScalableJudges. arXiv:2310.17631[cs.CL]
mentinComputerScienceEducation:AState-of-the-ArtReview.ACMTrans.