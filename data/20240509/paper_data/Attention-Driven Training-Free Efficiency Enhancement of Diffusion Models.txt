Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models
HongjieWang1*,DifanLiu2,YanKang2,YijunLi2,ZheLin2,NirajK.Jha1,YuchenLiu2†
1PrincetonUniversity,2AdobeResearch
Abstract generatingasingle512pximagerequires90seconds[19].
Toaddressthisissue,numerousapproachesgearedatef-
DiffusionModels(DMs)haveexhibitedsuperiorperfor-
ficient DMs have been introduced, which can be roughly
manceingeneratinghigh-qualityanddiverseimages.How-
categorized into two regimes: (1) efficient sampling strat-
ever, thisexceptionalperformancecomesatthecostofex-
egy [24, 30] and (2) efficient model architecture [19, 38].
pensive architectural design, particularly due to the atten-
Whileefficientsamplingmethodscanreducethenumberof
tionmoduleheavilyusedinleadingmodels. Existingworks
denoising steps, they do not reduce the memory footprint
mainlyadoptaretrainingprocesstoenhanceDMefficiency.
and compute cost for each step, making it still challeng-
Thisiscomputationallyexpensiveandnotveryscalable. To
ingtouseondeviceswithlimitedcomputationalresources.
this end, we introduce the Attention-driven Training-free
On the contrary, an efficient architecture reduces the cost
EfficientDiffusionModel(AT-EDM)frameworkthatlever-
of each step and can be further combined with sampling
agesattentionmapstoperformrun-timepruningofredun-
strategiestoachieveevenbetterefficiency. However, most
dant tokens, without the need for any retraining. Specifi-
priorefficientarchitectureworksrequireretrainingofthe
cally,forsingle-denoising-steppruning,wedevelopanovel
DM backbone, which can take thousands of A100 GPU
ranking algorithm, Generalized Weighted Page Rank (G-
hours. Moreover, due to different deployment settings on
WPR),toidentifyredundanttokens,andasimilarity-based
variousplatforms,differentcompressionratiosoftheback-
recoverymethodtorestoretokensfortheconvolutionoper-
bonemodelarerequired,whichnecessitatemultipleretrain-
ation. In addition, we propose a Denoising-Steps-Aware
ingrunslater. Suchretrainingcostsareabigconcerneven
Pruning (DSAP) approach to adjust the pruning budget
forlargecompaniesintheindustry.
across different denoising timesteps for better generation
quality. Extensive evaluations show that AT-EDM per- Tothisend,weproposetheAttention-drivenTraining-
forms favorably against prior art in terms of efficiency free Efficient Diffusion Model (AT-EDM) framework,
(e.g.,38.8%FLOPssavingandupto1.53×speed-upover which accelerates DM inference at run-time without any
Stable Diffusion XL) while maintaining nearly the same retraining. To the best of our knowledge, training-free ar-
FID and CLIP scores as the full model. Project webpage: chitecturalcompressionofDMsisahighlyunchartedarea.
https://atedm.github.io. Onlyonepriorwork,TokenMerging(ToMe)[1],addresses
thisproblem. WhileToMedemonstratesgoodperformance
on Vision Transformer (ViT) acceleration [2], its perfor-
1.Introduction mance on DMs still has room to improve. To further en-
richresearchontraining-freeDMacceleration,westartour
Diffusion Models (DMs) [9, 29] have revolutionized com-
study by profiling the floating-point operations (FLOPs)
puter vision research by achieving state-of-the-art perfor-
of the state-of-the-art model, Stable Diffusion XL (SD-
mance in various text-guided content generation tasks, in-
XL) [26], through which we find that attention blocks are
cluding image generation [28], image editing [12], super
thedominantworkload. Inasingledenoisingstep,wethus
resolution[17],3Dobjectsgeneration[27],andvideogen-
propose to dynamically prune redundant tokens to accel-
eration[10].Nonetheless,thesuperiorperformanceofDMs
erate attention blocks. We pioneer a fast graph-based al-
comesatthecostofanenormouscomputationbudget. Al-
gorithm, Generalized Weighted Page Rank (G-WPR), in-
though Latent Diffusion Models (LDMs) [28, 34] make
spiredbyZero-TPrune[35],anddeployitonattentionmaps
text-to-image generation much more practical and afford-
in DMs to identify superfluous tokens. Since SD-XL con-
able for normal users, their inference process is still too
tains ResNet blocks, which require a full number of to-
slow. For example, on the current flagship mobile phone,
kens for the convolution operations, we propose a novel
similarity-basedtokencopyapproachtorecoverprunedto-
*WorkwaspartlydoneduringaninternshipatAdobe.
†CorrespondingAuthor. kens,againleveragingtherichinformationprovidedbythe
1
4202
yaM
8
]VC.sc[
1v25250.5042:viXraFigure1.ExamplesofapplyingAT-EDMtoSD-XL[26].Comparedtothefull-sizemodel(toprow),ouracceleratedmodel(bottomrow)
hasaround40%FLOPsreductionwhileenjoyingcompetitivegenerationqualityatvariousaspectratios.
attention maps. This token recovery method is critical to 2.RelatedWork
maintainingimagequality. Wefindthatnaiveinterpolation
Text-to-Image Diffusion Models. DMs learn to reverse
or padding of pruned tokens adversely impacts generation
the diffusion process by denoising samples from a normal
quality severely. In addition to single-step token pruning,
distributionstepbystep.Inthismanner,thediffusion-based
we also investigate cross-step redundancy in the denoising
generativemodelsenablehigh-fidelityimagesynthesiswith
process by analyzing the variance of attention maps. This
varianttextprompts[4,9].However,DMsinthepixelspace
leadsustoanovelpruningschedule,dubbedasDenoising-
suffer from large generation latency, which severely lim-
Steps-AwarePruning(DSAP),inwhichweadjusttheprun-
its their applications [36]. The LDM [28] was the first to
ing ratios across different denoising timesteps. We find
trainaVariationalAuto-Encoder(VAE)toencodethepixel
DSAPnotonlysignificantlyimprovesourmethod,butalso
space into a latent space and apply the DM to the latent
helps improve other run-time pruning methods like ToMe
space. Thisreducescomputationalcostsignificantlywhile
[1]. Compared to ToMe, our approach shows a clear im-
maintaining generation quality, thus greatly enhancing the
provementbygeneratingclearerobjectswithsharperdetails
application of DMs. Subsequently, several improved ver-
andbettertext-imagealignmentunderthesameacceleration
sionsoftheLDM,calledStableDiffusionModels(SDMs),
ratio. Insummary,ourcontributionsarefour-fold:
have been released. The most recent and powerful open-
sourceversionisSD-XL[26],whichoutperformsprevious
• We propose the AT-EDM framework, which leverages
versionsbyalargemargin. SD-XLisourdefaultbackbone
rich information from attention maps to accelerate pre-
inthiswork.
trainedDMswithoutretraining.
EfficientDiffusionModels. Researchershavemadeenor-
• We design a token pruning algorithm for a single de-
mouseffortstomakeDMsmoreefficient. Existingefficient
noising step. We pioneer a fast graph-based algo-
DMscanbedividedintotwotypes:
rithm,G-WPR,toidentifyredundanttokens,andanovel
(1) Efficient sampling to reduce the required number of
similarity-based copy method to recover missing tokens
denoising steps [22, 30–32]. A recent efficient sampling
forconvolution.
work[24]managedtoreducethenumberofdenoisingsteps
• Inspiredbythevariancetrendofattentionmapsacrossde-
to as low as one. It achieves this by iterative distillation,
noisingsteps,wedeveloptheDSAPschedule,whichim-
halvingthenumberofdenoisingstepseachtime.
provesgenerationqualitybyaclearmargin.Theschedule
(2)Architecturalcompressiontomakeeachsamplingstep
alsoprovidesimprovementsoverotherrun-timeacceler-
moreefficient[11,19,36,38]. Arecentwork[13]removes
ationapproaches,demonstratingitswideapplicability.
multipleResNetandattentionblocksintheU-Netthrough
• WeuseAT-EDMtoaccelerateatop-tierDM,SD-XL,and
distillation. Although these methods can save computa-
conductbothqualitativeandquantitativeevaluations.No-
tionalcostswhilemaintainingdecentimagequality,theyre-
ticeably,ourmethodshowscomparableperformancewith
quireretrainingoftheDMbackbonetoenhanceefficiency,
anFIDscoreof28.0with40%FLOPsreductionrelative
needing thousands of A100 GPU hours. Thus, a training-
tothefull-sizeSD-XL(FID27.3),achievingstate-of-the-
free method to enhance the efficiency of DMs is needed.
artresults. VisualexamplesareshowninFig.1.
Notethatourproposedtraining-freeframework,AT-EDM,
2
LX-DS
MDE-TA
sPOLFT
7.6
@
sPOLFT1.4@isorthogonaltotheseefficiencyenhancementmethodsand
canbestackedwiththemtofurtherimprovetheirefficiency. Conv+Res 1623
We provide corresponding experimental evidence in Sup-
Attn 5108
plementaryMaterial.
Training-Free Efficiency Enhancement. Training-free U-Net 6731
(i.e., post-training) efficiency enhancement schemes have
0 1000 2000 3000 4000 5000 6000 7000 8000
been widely explored for CNNs [14, 33, 39] and ViTs
GFLOPs
[2, 7, 15, 35]. However, training-free schemes for DMs Figure2.U-NetFLOPsbreakdownofSD-XL[26]measuredwith
arestillpoorlyexplored. Tothebestofourknowledge,the 1024pximagegeneration.AmongcomponentsofU-Net(convolu-
onlypriorworkinthisfieldisToMe[1]. Itusestokenem- tionblocks,ResNetblocks,andattentionblocks),attentionblocks
bedding vectors to obtain pair-wise similarity and merges costthemost.
similar tokens to reduce computational overheads. While
itiscompatiblewithanytokenpruningscheme. DSAPcan
ToMeachievesadecentspeed-upwhenappliedtoSD-v1.x
potentiallybemigratedtoexistingefficientDMstohelpim-
and SD-v2.x, we find that it does not help much when ap-
provetheirimagequality.
plied to the state-of-the-art DM backbone, SD-XL, whilst
ourmethodachievesaclearimprovementoverit(seeexper-
3.Methodology
imentalresultsinSection4). Thisismainlydueto(1)the
significantarchitecturalchangeofSD-XL(seeSupplemen- We start our investigation by profiling the FLOPs of the
tary Material); (2) our better algorithm design to identify state-of-the-art DM, SD-XL, as shown in Fig. 2. Notice-
redundanttokens. ably,amongcompositionsofthesamplingmodule(U-Net),
Exploiting Attention Maps. We aim to design a method attention blocks, which consist of several consecutive at-
that exploits information present in pre-trained models. tention layers, dominate the workload for image genera-
ToMe only uses embedding vectors of tokens and ignores tion. Therefore, we propose AT-EDM to accelerate atten-
the correlation between tokens. We take inspiration from tion blocks in the model through token pruning. AT-EDM
recentimageeditingworks[3,5,8,25],inwhichattention containstwoimportantparts: asingle-denoising-steptoken
mapsclearlydemonstratewhichpartsofageneratedimage pruning scheme and the DSAP schedule. We provide an
aremoreimportant. Thisinspiresustousethecorrelations overviewofthesetwopartsandthendiscussthemindetail.
and couplings between tokens indicated by attention maps
3.1.Overview
toidentifyunimportanttokensandprunethem.Specifically,
we can convert attention maps to directed graphs, where
Fig.3illustratesthetwomainpartsofAT-EDM:
nodesrepresenttokens,withoutinformationloss. Basedon
thisidea,wedeveloptheG-WPRalgorithmfortokenprun- PartI:Tokenpruningschemeinasingledenoisingstep.
inginasingledenoisingstep. Step 1: We obtain the attention maps from an attention
Non-Uniform Denoising Steps. Various existing works layerintheU-Net. Wecanpotentiallyobtaintheattention
[6, 18, 21, 37] demonstrate that denoising steps contribute maps from self-attention or cross-attention. We compare
differentlytothequalityofgeneratedimages;thus,itisnot thetwochoicesandanalyzethemindetailthroughablation
optimum to use uniform denoising steps. OMS-DPM [21] experiments.
buildsamodelzooandusesdifferentmodelsindifferentde- Step 2: We use a scoring module to assign an importance
noisingsteps. Ittrainsaperformancepredictortoassistin score to each token based on the obtained attention map.
searchingfortheoptimalmodelschedule. DDSM[37]em- We use an algorithm called G-WPR to assign importance
ploysaspectrumofneuralnetworksandadaptstheirsizes scorestoeachtoken. ThisisdescribedinSection3.2.
to the importance of each denoising step. AutoDiffusion Step3:Wegeneratepruningmasksbasedonthecalculated
[18] employs evolutionary search to skip some denoising importancescoredistribution. Currently,wesimplyusethe
stepsandsomeblocksintheU-Net. Diff-Pruning[6]uses top-kapproachtodeterminetheretainedtokens,i.e.,prune
aTaylorexpansionoverprunedtimestepstodisregardnon- tokenswithlessimportancescores.
contributorydiffusionsteps. Allexistingmethodseitherre- Step4: Weusethegeneratedmasktoperformtokenprun-
quireanintensivetraining/fine-tuning/searchingprocessto ing. We do this after the feed-forward layer of attention
obtainanddeploythedesireddenoisingscheduleorarenot layers. Wemayalsoperformpruningearlybeforethefeed-
compatiblewithourproposedG-WPRtokenpruningalgo- forward layers. We provide ablative experimental results
rithm due to the U-Net architecture change. On the con- foritinSupplementaryMaterial.
trary,basedonourinvestigationofthevarianceofattention Step5: WerepeatSteps1-4foreachconsecutiveattention
mapsacrossdenoisingsteps,weproposeDSAP.Itssched- layer.Notethatwedonotapplypruningtothelastattention
ulecanbedeterminedviasimpleablationexperimentsand layerbeforetheResNetlayer.
3Attention Block
①Get the ④Run-time ⑤Repeat ①-④for ⑥Similarity-based copy to fill pruned
attention map pruning consecutive layers tokens before passing to ResNet layers
②Calculate importance scores via ③Generate pruning masks
G-WPR
Strong Weak Retained token Pruned token Filled token
Graph Signal
Prune Less
... ...
Prune More
Denoising
𝑁 ... 𝑁−𝜏+1 𝑁−𝜏 ... 0 timestep 𝑡
Figure3. OverviewofourproposedefficiencyenhancementframeworkAT-EDM.Single-Denoising-StepTokenPruning: (1)Weget
theattentionmapfromself-attention.(2)WecalculatetheimportancescoreforeachtokenusingG-WPR.(3)Wegeneratepruningmasks.
(4)Weapplythemaskstotokensafterthefeed-forwardnetworktorealizetokenpruning.(5)WerepeatSteps(1)-(4)foreachconsecutive
attentionlayer.(6)BeforepassingfeaturemapstotheResNetblock,werecoverprunedtokensthroughsimilarity-basedcopy.Denoising-
Steps-AwarePruningSchedule: Inearlysteps,weproposetoprunefewertokensandtohavelessFLOPsreduction. Inlatersteps,we
prunemoreaggressivelyforhigherspeedup.
Step 6: Finally, before passing the pruned feature map to umn. A can be thought of as the adjacency matrix of a
the ResNet block, we need to fill (i.e., try to recover) the directedgraphintheG-WPRalgorithm. Inthisgraph, the
pruned tokens. A simple approach is to pad zeros, which setofnodeswithinput(output)edgesisreferredtoasΦ
in
meanswedonotfillanything.Themethodthatwecurrently (Φ ). NodesinΦ (Φ )representKey(Query)tokens,
out in out
use is to copy tokens to corresponding locations based on i.e., Φ = {k }N (Φ = {q }M ). Let st (st ) de-
in j j=1 out i i=1 K Q
similarity. ThisisdescribedindetailinSection3.2. notethevectorthatrepresentstheimportancescoreofKey
(Query)tokensinthet-thiterationoftheG-WPRalgorithm.
Part II: DSAP schedule. Attention maps in early de-
In the case of self-attention, Query tokens are the same as
noising steps are more chaotic and less informative than
Key tokens. Specifically, we let {x }N denote the N to-
those in later steps, which is indicated by their low vari- i i=1
kensandsdenotetheirimportancescoresinthedescription
ance.Thus,theyhaveaweakerabilitytodifferentiateunim-
ofourtokenrecoverymethod.
portant tokens [8]. Based on this intuition, we design the
DSAPschedulethatprunesfewertokensinearlydenoising TheG-WPRAlgorithm. WPR[35]usestheattentionmap
steps. Specifically, we select some attention blocks in the asanadjacencymatrixofadirectedcompletegraph. Ituses
up-samplinganddown-samplingstagesandleavethemun- a graph signal to represent the importance score distribu-
pruned, since they contribute more to the generated image tion among nodes in this graph. This signal is initialized
qualitythanotherattentionblocks[19].Wedemonstratethe uniformly. WPR uses the adjacency matrix as a graph op-
scheduleindetailinSection3.3. erator, applying it to the graph signal iteratively until con-
vergence. Ineachiteration,eachnodevotesforwhichnode
3.2.PartI:TokenPruninginaSingleStep
is more important. The weight of the vote is determined
Notation. Suppose A(h,l) ∈ RM×N is the attention map by its importance in the last iteration. However, WPR, as
of the h-th head in the l-th layer. It reflects the correla- proposedin[35],constrainstheusedattentionmaptobea
tionsbetweenM QuerytokensandN Keytokens. Were- self-attentionmap. Basedonthis, weproposetheG-WPR
fer to A(h,l) as A for simplicity in the following discus- algorithm,whichiscompatiblewithbothself-attentionand
sion. Let A denote its element in the i-th row, j-th col- cross-attention, as shown in Algorithm 1. The attention
i,j
4
petS-gnisioneD-elgniS
-spetS-gnisioneD
gninurP
nekoT
gninurP
erawA
reyaL
noitnettA
noitnettA-ssorC noitnettA-fleS
NFF
eludoM
gninurP
reyaL
noitnettA
eludoM
gninurP
reyaL
noitnettA
eludoM
gnillifeR
reyaL
teNseRfrom Query q i to Key k j weights the edge from q i to k j Token pruning is not natively compatible Similarity-based copy resolvesthe
with ResNet incompatibility
inthegraphgeneratedbyA. Ineachiterationofthevanilla
1 2 3 4 1 2 3 4
WPR, by multiplying with the attention map, we map the Token pruning Token pruning
importanceofQuerytokensst totheimportanceofKeyto- 1 2 3 4 Not compatible due to 1 2 3 4 Similarity-
kensst K+1,i.e.,eachnodeinΦQ outvotesforwhichΦ innode
1
2Resha ×pe th Re en so Nn- es tquare shape 1 2 R3 esha4
pe
basedCopy
is more important. For self-attention, st+1 = st+1 since 3 4 Layer
Q K 1 2 √ ResNet
Query and Key tokens are the same. For cross-attention, Retained token Pruned token Filled token 3 4 Layer
Query tokens are image tokens and Key tokens are text Key Similarity-based Copy
1 2 3 4 1 2 3 4 1 2 3 4
prompt tokens. Based on the intuition that important im- 1 1 √ 2: 1 1 4 agetokensshoulddevotealargeportionoftheirattentionto 2 3 2 3 4 √ 3: 4 1 2 3 4
importanttextprompttokens,wedefinefunctionf(A,s ) 4 ③Find the
K highestattention ④Get the most ⑤Copy retained
thatmapsst+1tost+1. Oneentropy-basedimplementation ①Attention map ②Delete rows received for each similar token of tokens to fill
K Q averaged cross heads of prunedtokens prunedtoken prunedtokens prunedtokens
is
Figure4. Oursimilarity-basedcopymethodfortokenrecovering
st+1(q )=f(A,st+1)= (cid:80)N j=1A i,j·st K+1(k j) (1) resolves the incompatibility between token pruning and ResNet.
Q i K −(cid:80)N A ·lnA Token pruning incurs the non-square shape of feature maps and
j=1 i,j i,j
thusisnotcompatiblewithResNet.Toaddressthisissue,wepro-
where A is the attention from Query q to Key k . This pose similarity-based copy to recover the pruned tokens. It first
i,j i j
is the default setting for cross-attention-based WPR in the averages the attention map across heads and deletes the rows of
following sections. We discuss and compare other imple- pruned tokens to avoid selecting them as the most similar one.
mentations in Supplementary Material. Note that for self- Then,itfindsthesourceofthehighestattentionreceivedforeach
attention, f(A,st+1) = st+1. The G-WPR algorithm has pruned token and copies the corresponding retained tokens for
K K recovery. After recovering, the tokens can be translated into a
an O(M × N) complexity, where M (N) is the number
spatially-completefeaturemaptoserveasinputtoResNetblocks.
of Query (Key) tokens. We employ this algorithm in each
head and then obtain the root mean square of scores from
(II)Interpolation. Interpolationmethods, suchasbicubic
differentheads(torewardtokensthatobtainveryhighim-
interpolation,arenotsuitableinthiscontext. Tousethein-
portancescoresinafewheads).
terpolation algorithm, we first pad zeros to fill the pruned
tokens and form a feature map of size N ×N. Then we
Algorithm1TheG-WPRalgorithmforbothself-attention downsampleitto N × N andupsampleitbacktoN ×N
andcross-attention 2 2
withtheinterpolationalgorithm. Wekeepthevaluesofre-
Require: M,N > 0isthenumberofnodesinΦ ,Φ ;A ∈
out in tained tokens fixed and only use the interpolated values of
RM×N;s ∈ RM,s ∈ RN;f(A,s )mapstheimportance
Q K k prunedtokens.Duetothehighpruningrates(usuallylarger
ofKeytothatofQuery
than 50%), most tokens that represent the background get
Ensure: s∈RMrepresentstheimportancescoreofimagetokens
pruned,leadingtolotsofprunedtokensthataresurrounded
s0 ← 1 ×e
Q M M byotherprunedtokensinsteadofretainedtokens. Interpo-
t←0
while(|st −st−1|>ϵ)or(t=0)do lationalgorithmsassignnearlyzerovaluestothesetokens.
Q Q
st+1 ←AT ×st (III) Direct copy. Another possible method is to use the
K Q
st+1 ←f(A,st+1) correspondingvaluesbeforepruningisapplied(i.e.,before
Q K
st+1 ←st+1/|st+1| being processed by the following attention layers) to fill
Q Q Q
t←t+1 the pruned tokens. The problem with this method is that
endwhile thevaluedistributionchangessignificantlyafterbeingpro-
s←st
Q cessed by multiple attention layers, and copied values are
far from the values of these tokens if they are not pruned
Recovering Pruned Tokens. We have fewer tokens after andareprocessedbythefollowingattentionlayers.
token pruning, leading to efficiency enhancement. How- To avoid the effect of distribution shift, we propose the
ever, retained tokens form irregular maps and thus cannot similarity-based copy technique, as shown in Fig. 4. In-
beusedforconvolution,asshowninFig.4. Weneedtore- steadofcopyingvaluesthatarenotprocessedbyattention
covertheprunedtokenstomakethemcompatiblewiththe layers, we select tokens that are similar to pruned tokens
followingconvolutionaloperationsintheResNetlayer. fromtheretainedtokens. Weusetheself-attentionmapto
(I) Padding Zeros. One straightforward way to do this is determine the source of the highest attention received for
topadzeros. However,tomaintainthehighqualityofgen- eachprunedtokenandusethatasthemostsimilarone.This
eratedimages,wehopetorecovertheprunedtokensaspre- isbasedontheintuitionthatattentionfromtokenx toto-
a
ciselyaspossible,asiftheywerenotpruned. kenx ,A ,isdeterminedbytwofactors: (1)importance
b a,b
5
yreuQRegion I Region II Region III Region IV We present all the related ablative experimental results in
Section 4.4. A detailed description of the less aggressive
pruning schedule is provided in Supplementary Material.
To further consolidate our intuitions, we also investigate a
more aggressive pruning schedule in early denoising steps
and find it is inferior to our current approach (see Supple-
Step mentaryMaterial).
Figure5. Varianceofattentionmapsindifferentdenoisingsteps.
Wedividethedenoisingstepsintofourtypicalregions: (I)Very- 4.ExperimentalResults
early steps: Variance of attention maps is small and increases
rapidly. (II)Mid-earlysteps: Varianceofattentionmapsislarge Inthissection,weevaluateAT-EDMandToMeonSD-XL.
and increases slowly. (III) Middle steps: Variance of attention We provide both visual and quantitative experimental re-
mapsislargeandalmostconstant.(IV)Lastseveralsteps. sultstodemonstratetheadvantagesofAT-EDMoverToMe.
oftokenx b,i.e.,s(x b),and(2)similaritybetweentokenx a 4.1.ExperimentalSetup
and x . If we observe the attention that x receives, i.e.,
b b
compare {A } , since s(x ) is fixed, index i = η that Common Settings. We implement both our AT-EDM
i,b i∈N b
maximizes {A } is the index of the most similar to- methodandToMeontheofficialrepositoryofSD-XLand
i,b i∈N
ken, i.e., x . Finally, wecopythevalueoftokenx tofill evaluatetheirperformance.Theresolutionofgeneratedim-
η η
(i.e.,recover)theprunedtokenx . agesis1024×1024pixelsandthedefaultFLOPsbudgetfor
b
eachdenoisingstepisassumedtobe4.1T,whichis38.8%
3.3.PartII:Denoising-Steps-AwarePruning smallerthanthatoftheoriginalmodel(6.7T)unlessother-
wisenoted. ThedefaultCFG-scaleforimagegenerationis
Earlydenoisingstepsdeterminethelayoutofgeneratedim-
7.0unlessotherwisenoted. Wesetthetotalnumberofsam-
agesand, thus, arecrucial. Onthecontrary, latedenoising
pling steps to 50. We use the default sampler of SD-XL,
stepsaimatrefiningthegeneratedimage,nativelyincluding
i.e.,EulerEDMSampler.
redundantcomputationssincemanyregionsoftheimagedo
AT-EDM. For a concise design, we only insert a pruning
notneedrefinement.Inaddition,earlydenoisingstepshave
layer after the first attention layer of each attention block
aweakerabilitytodifferentiateunimportanttokens,andlate
and set the pruning ratio for that layer to ρ. To meet the
denoisingstepsyieldinformativeattentionmapsanddiffer-
FLOPs budget of 4.1T, we set ρ = 63%. For the DSAP
entiateunimportanttokensbetter. Tosupportthisclaim,we
setting,wechoosetoleavethefirstattentionblockineach
investigatethevarianceoffeaturemapsindifferentdenois-
down-stageandthelastattentionblockineachup-stageun-
ing steps, as shown in Fig. 5. It indicates that attention
pruned. Weusethisprune-lessscheduleforthefirstτ =15
mapsinearlystepsaremoreuniform. Theyassignsimilar
denoisingsteps.
attentionscorestobothimportantandunimportanttokens,
making it harder to precisely identify unimportant tokens ToMe. The SD-XL architecture has changed significantly
andprunetheminearlysteps.Basedontheseintuitions,we compared to previous versions of SDMs (see Supplemen-
proposeDSAPthatemploysaprune-lessscheduleinearly tary Material). Thus, the default setting of ToMe does not
denoisingstepsbyleavingsomeofthelayersunpruned. leadtoenoughFLOPssavings. TomeettheFLOPsbudget,
ThePrune-LessSchedule.InSD-XL,eachdown-stagein- it is necessary to use a more aggressive merging setting.
cludestwoattentionblocksandeachup-stageincludesthree Therefore,weexpandtheapplicationrangeoftokenmerg-
attention blocks (except for stages without attention). The ing(1)fromattentionlayersatthehighestfeatureleveltoall
mid-stage also includes one attention block. Each atten- attentionlayers,and(2)fromself-attentiontoself-attention,
tionblockincludes2-10attentionlayers. Inourprune-less cross-attention, and the feedforward network. We set the
schedule, we select some attention blocks to not perform mergingratior =50%tomeettheFLOPsbudgetof4.1T.
tokenpruning. Sincepreviousworks[13,19]indicatethat Evaluations. We first compare the generated images with
the mid-stage contributes much less to the generated im- manually designed challenging prompts in Section 4.2.
agequalitythantheup-stagesanddown-stages, wedonot Then, we report FID and CLIP scores of zero-shot image
select the attention block in the mid-stage. Based on the generation on the MS-COCO 2017 validation dataset [20]
ablation study, we choose to leave the first attention block inSection4.3. Testedmodelsgenerate1024×1024pxim-
ineachdown-stageandthelastattentionblockineachup- agesbasedonthecaptionsof5kimagesinthevalidationset.
stageunpruned.Weusethisprune-lessscheduleforthefirst Weprovideablativeexperimentalresultsandanalyzethem
τ denoisingsteps. Weexploresettingτ indifferentregions in Section 4.4 to justify our design choices. We provide
shown in Fig. 5 and find τ = 15 is the optimal choice. moreimplementationdetailsinSupplementaryMaterial.
6
ecnairaV“Ultra realistic illustration of an old man cyborg, cyberpunk, sci-fi fantasy”
“close up of mystic dog, like a phoenix, red and blue colors digital”
“15mm wide-angle lens photo of a rapper in 1990 New York holding a kitten up to the camera”
“A single beam of light enters the room from the ceiling. The beam of light is illuminating an easel. On the
easel there is a Rembrandt painting of a raccoon.”
w/o DSAP w/ DSAP
(a) SD-XL
(b) ToMe (c) Ours (d) ToMe (e) Ours (AT-EDM)
Figure6.ComparingAT-EDMtothestate-of-the-artapproach,ToMe[2].Whilethefull-sizeSD-XL[26](Col.a)consumes6.7TFLOPs,
we compare the accelerated models (Col. b-e) at the same budget of 4.1 TFLOPs. Compared to ToMe, we find that AT-EDM’s token
pruningalgorithmprovidesclearergeneratedobjectswithsharperdetailsandfinertextures, andabettertext-imagealignmentwhereit
better retains the semantics in the prompt (see the fourth row). Moreover, we find that DSAP provides better structural layout of the
generated images, which is effective for both ToMe and our approach. AT-EDM combines the novel token pruning algorithm and the
DSAPschedule(Col.e),outperformingthestateoftheart.
4.2.VisualExamplesforQualitativeAnalysis ample, AT-EDMpreservesthemainobject, thefaceofthe
old man, much better than ToMe does. AT-EDM’s strong
We use manually designed challenging prompts to evalu-
ability to preserve the main object is also exhibited in the
ateToMeandourproposedAT-EDMframework. Thegen-
secondexample.ToMeloseshigh-frequencyfeaturesofthe
erated images are compared in Fig. 6. We compare more
mainobject,suchastextureandhair,whileAT-EDMretains
generatedimagesinSupplementaryMaterial. Visualexam-
themwell,evenwithoutDSAP.Thethirdexampleagainil-
ples indicate that with the same FLOPs budget, AT-EDM
lustrates the advantage of AT-EDM over ToMe in preserv-
demonstrates better main object preservation and text-
ingtherapper’sface. Thefourthexampleusesarelatively
imagealignmentthanToMe. Forinstance, inthefirstex-
740 Table1.DeployingToMeandAT-EDMinSD-XLunderdifferent
SD-XL@6.7TFLOPs FLOPs budgets. We generate all images with the CFG-scale of
38
ToMe@4.1TFLOPs 7.0,exceptforSD-XL†,forwhichweuseaCFG-scaleof4.0.
36 AT-EDM@4.1TFLOPs
AT-EDM†@4.5TFLOPs
34 Model FID CLIP TFLOPs
32 SD-XL 31.94 0.3284 6.7
SD-XL† 27.30 0.3226 6.7
30
ToMe-a 58.76 0.2954 2.9
28
AT-EDM-a 52.00 0.2784 2.9
26 ToMe-b 40.94 0.3154 3.6
AT-EDM-b 29.80 0.3095 3.6
24
0.28 0.29 0.3 0.31 0.32 0.33 ToMe-c 35.27 0.3198 4.1
CLIP score (Open Clip ViT-g14)
AT-EDM-c 28.04 0.3209 4.1
Figure7. FID-CLIPscorecurves. TheusedCFGscalesare[1.0,
ToMe-d 32.46 0.3235 4.6
1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0, 12.0, 15.0]. Thisfigure
AT-EDM-d 27.23 0.3245 4.5
is zoomed in to the bottom-right corner to show the comparison
betweenthebesttrade-offpoints.AT-EDMoutperformsToMeby
aclearmargin.SeecompletecurvesinSupplementaryMaterial. scores than AT-EDM. When the FLOPs saving is 30-40%,
AT-EDMachievesnotonlybetterimagequality(lowerFID
complex prompt that describes relationships between mul- scores) but also better text-image alignment (higher CLIP
tipleobjects. ToMemisunderstands”aRembrandtpainting scores) than ToMe. Note that under the same CFG-scale,
ofaraccoon”asbeingarandompaintingontheeaseland AT-EDM achieves a lower FID score than the full-size
apaintingofaraccoononthewall. Onthecontrary,theim- model while reducing FLOPs by 32.8%. In the case that
agegeneratedbyAT-EDMunderstandsandpreservesthese ittradestext-imagealignmentforimagequality(viareduc-
relationships very well, even without DSAP. As a part of ing the CFG scale to 4.0), AT-EDM achieves not only a
ourAT-EDMframework,DSAPisnotonlyeffectiveinAT- lower FID score but also a higher CLIP score than the
EDMbutalsobeneficialtoToMeinimprovingimagequal- full-sizemodelwhilereducingFLOPsby32.8%.Wepro-
ity and text-image alignment. When we deploy DSAP in videmorevisualexamplesundervariousFLOPsbudgetsin
ToMe,weselectcorrespondingattentionblockstonotper- SupplementaryMaterial.
formtokenmerging,whilekeepingtheFLOPscostfixed. LatencyAnalysis. SD-XLusestheFusedOperation(FO)
library,xformers[16],toboostitsgeneration. TheCurrent
4.3.QuantitativeEvaluations
Implementation(CI)ofxformersdoesnotprovideattention
FID-CLIP Curves. We explore the trade-off between the maps as intermediate results; hence, we need to addition-
CLIPandFIDscoresthroughvariousClassifer-FreeGuid- allycalculatetheattentionmaps. Wediscussthesampling
ance (CFG) scales. We show the results in Fig. 7. AT- latency for three cases: (I) without FO, (II) with FO un-
EDM† does not deploy pruning at the second feature level derCI,and(III)withFOundertheDesiredImplementation
(see Supplementary Material). It indicates that for most (DI),whichprovidesattentionmapsasintermediateresults.
CFG scales, AT-EDM not only lowers the FID score but Table 2 shows that with FO, the cost of deploying prun-
alsoresultsinhigherCLIPscoresthanToMe,implyingthat ing at the second feature level exceeds the latency reduc-
imagesgeneratedbyAT-EDMnotonlyhavebetterquality tion it leads to. Hence, AT-EDM† is faster than AT-EDM.
butalsobettertext-imagealignment. Specifically,whenthe Fig.8showstheextralatencyincurredbydifferentpruning
CFG scale equals 7.0, AT-EDM results in [FID, CLIP] = steps shown in Fig. 3. With a negligible quality loss, AT-
[28.0,0.321],whichisalmostthesameasthefull-sizeone EDMachieves52.7%,15.4%,17.6%speed-upinterms
([27.3, 0.323], CFG scale=4.0). For comparison, ToMe oflatencyw/oFO,w/FOunderCI,w/FOunderDI,re-
resultsin[35.3,0.320]withaCFGscaleof7.0. Thus,AT- spectively, whichoutperformsthestate-of-the-artworkby
EDMreducestheFIDgapfrom8.0to0.7. a clear margin. We present the memory footprint of AT-
VariousFLOPsBudgets. WedeployToMeandAT-EDM EDMinSupplementaryMaterial.
onSD-XLundervariousFLOPsbudgetsandquantitatively
4.4.AblationStudy
compare their performance in Table 1. The FLOPs cost in
this table refers to the average FLOPs cost of a denoising Self-Attention (SA) vs. Cross-Attention (CA). G-WPR
step. Table 1 indicates that AT-EDM achieves better im- canpotentiallyuseattentionmapsfromself-attention(SA-
agequalitythanToMe(lowerFIDscores)underallFLOPs basedWPR)andcross-attention(CA-basedWPR).Wepro-
budgets. When the FLOPs budget is extremely low (less vide a detailed comparison between the two implementa-
than 50% of the full model), ToMe achieves higher CLIP tions. Wevisualizetheirpruningmasksandprovidegener-
8
erocs
DIFTable2.Comparisonbetweensamplinglatencyindifferentcases.
†meansnotdeployingpruningatthesecondfeaturelevel.
Model SD-XL ToMe AT-EDM AT-EDM†
Ave.FLOPs/step 6.7T 4.1T 4.1T 4.5T
w/oFO 31.0s 21.0s 20.3s 22.1s
w/FOunderCI 18.0s 17.7s 18.3s 15.6s
w/FOunderDI 18.0s 17.7s 16.3s 15.3s
(c) Bicubic (e) Similarity-based
(a) SD-XL (b) Padding Zeros (d) Direct Copy
Interpolation Copy
Step 1
Step 2 Figure10. Differentmethodstorecovertheprunedtokens. Zero
Step 3 AT-EDM
padding (Col. b), bicubic interpolation (Col. c), and direct copy
Step 4 AT-EDM†
Step 6 (Col.d)canhardlyrecoverprunedtokensandresultinnoticeable
0 500 1000 1500 2000 image degradation with blurry background (incomplete moon).
Latecncy (ms) Ontheotherhand,similarity-basedcopy(Col.e)providesbetter
Figure 8. Latency incurred by different pruning steps shown in
imagequalityandkeepsthecompletemoonintheoriginalimage.
Fig.3.Measuredw/FOunderCI.NotethatunderDI,thelatency
Betterviewedwhenzoomedin.
ofStep1(gettheattentionmap)iseliminated.
(a) SD-XL (b) 0 Step (c) 5 Steps (d) 15 Steps (e) 30 Steps (f) 45 Steps
Figure11.Comparisonbetweendifferentnumbersofearlyprune-
Pruning Mask lessstepswhere0stepisthesameaswithoutDSAP.Wefindthat
Generated Image
(Black: Pruned Tokens) pruninglessonthefirst15stepsachievesthebestquality.
(a) SD-XL (b) CA-WPR (c) SA-WPR (d) CA-WPR (e) SA-WPR
Figure 9. Comparison between different implementations of G-
WPR:CA-basedWPRandSA-basedWPR.Ingeneral,CA-based kens,providingsignificantlyhigherimagequality.
WPRmayremovetoomanybackgroundtokens,makingtheback-
Denoising-Steps-AwarePruning.Weexploredifferentde-
groundnotrecoverable,whileSA-basedWPRpreservestheimage
signchoicesforDSAP.
qualitybetter.
(1)Theprune-lessscheduleselectsoneattentionblockfrom
each down-stage and up-stage in the U-Net and skips the
atedimageexamplesforavisualcomparisoninFig.9.This
token pruning in it. According to ablation results shown
figureindicatesthatSA-basedWPRoutperformsCA-based
in Supplementary Material, F-L (First-Last) appears to be
WPR.ThereasonisthatCA-basedWPRprunestoomany
thebestone,i.e.,leavingthefirstattentionblockofdown-
background tokens, making it hard to recover the back-
stagesandthelastattentionblockofup-stagesunprunedin
groundviasimilarity-basedcopy.
earlydenoisingsteps.
Similarity-basedCopy. Weprovidecomparisonsbetween
differentmethodstofilltheprunedpixelsinFig.10,which (2)Wethenexplorehowthenumberofearlyprune-lessde-
demonstrate the advantages of our similarity-based copy noisingstepsaffectsthegeneratedimagequalityinFig.11.
method. Images generated by bicubic interpolation are Note that we keep the FLOPs budget fixed and adjust the
quite similar to those generated by padding zeros because pruning rate accordingly when we change the number of
interpolationusuallyassignsnear-zerovaluestoprunedto- prune-less steps. This figure shows that the setting of 15
kens that are surrounded by other pruned tokens and can earlyprune-lessstepsprovidesthebestimagequality. Note
hardly recover them. Direct copy means directly copying thatthesettingofzeroprune-lessstepisidenticaltotheset-
correspondingtokenvaluesbeforethefirstpruninglayerin tingwithoutDSAP,and5, 15, 30, 45prune-lessstepsrep-
theattentionblocktorecovertheprunedtokens,wherethe resentssettingtheboundaryinRegionI,II,III,IVofFig.5,
followingattentionlayersdonotprocessthecopiedvalues. respectively. The results indicate that placing the bound-
Thus, the copied values cannot recover the information in arybetweentheprune-lessandnormalscheduleinRegion
pruned tokens and even negatively affect the retained to- II performs best. This meets our expectation because the
kens. Onthecontrary,similarity-basedcopyusesattention varianceofattentionmapsbecomeshighenoughtoidentify
mapsandtokensthatareretainedtorecovertheprunedto- unimportanttokenswellinRegionII.
9
pam
erutaeF
egami
detareneG5.Conclusion References
Inthisarticle,weproposedAT-EDM,anovelframeworkfor [1] DanielBolyaandJudyHoffman.Tokenmergingforfaststa-
acceleratingDMsatrun-timewithoutretraining. AT-EDM blediffusion. InProceedingsoftheIEEE/CVFConference
has two components: a single-denoising-step token prun- on Computer Vision and Pattern Recognition, pages 4598–
4602,2023.
ingalgorithmandacross-steppruningschedule(DSAP).In
the single-denoising-step token pruning, AT-EDM exploits [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman. To-
attention maps in pre-trained DMs to identify unimportant
ken merging: Your ViT but faster. arXiv preprint
tokens and prunes them to accelerate the generation pro-
arXiv:2210.09461,2022.
cess. To make the pruned feature maps compatible with
[3] MingdengCao,XintaoWang,ZhongangQi,YingShan,Xi-
the latter convolutional blocks, AT-EDM again uses atten-
aohuQie,andYinqiangZheng. MasaCtrl: Tuning-freemu-
tion maps to reveal similarities between tokens and copies
tualself-attentioncontrolforconsistentimagesynthesisand
similar tokens to recover the pruned ones. DSAP further
editing. arXivpreprintarXiv:2304.08465,2023.
improvesthegenerationqualityofAT-EDM.Wefindsuch
[4] PrafullaDhariwalandAlexanderNichol. Diffusionmodels
a pruning schedule can also be applied to other methods
beatGANsonimagesynthesis. AdvancesinNeuralInfor-
likeToMe. Experimentalresultsdemonstratethesuperior- mationProcessingSystems,34:8780–8794,2021.
ityofAT-EDMwithrespecttoimagequalityandtext-image [5] DaveEpstein, AllanJabri, BenPoole, AlexeiAEfros, and
alignment compared to state-of-the-art methods. Specifi- Aleksander Holynski. Diffusion self-guidance for control-
cally,onSD-XL,AT-EDMachievesa38.8%FLOPssaving lable image generation. arXiv preprint arXiv:2306.00986,
andupto1.53×speed-upwhileobtainingnearlythesame 2023.
FIDandCLIPscoresasthefull-sizemodel,outperforming [6] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Struc-
priorart. tural pruning for diffusion models. arXiv preprint
arXiv:2305.10924,2023.
Acknowledgment [7] Mohsen Fayyaz, Soroush Abbasi Koohpayegani,
Farnoush Rezaei Jafari, Sunando Sengupta, Hamid
This work was supported in part by an Adobe summer RezaVaeziJoze,EricSommerlade,HamedPirsiavash,and
internship and in part by NSF under Grant No. CCF- Ju¨rgen Gall. Adaptive token sampling for efficient vision
2203399. transformers. In Proceedings of the European Conference
onComputerVision,pages396–414.Springer,2022.
[8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626,2022.
[9] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
sionprobabilisticmodels. AdvancesinNeuralInformation
ProcessingSystems,33:6840–6851,2020.
[10] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,
Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben
Poole, Mohammad Norouzi, David J. Fleet, et al. Imagen
video:Highdefinitionvideogenerationwithdiffusionmod-
els. arXivpreprintarXiv:2210.02303,2022.
[11] JonathanHo,ChitwanSaharia,WilliamChan,DavidJ.Fleet,
MohammadNorouzi,andTimSalimans.Cascadeddiffusion
models for high fidelity image generation. The Journal of
MachineLearningResearch,23(1):2249–2281,2022.
[12] BahjatKawar,ShiranZada,OranLang,OmerTov,Huiwen
Chang,TaliDekel,InbarMosseri,andMichalIrani. Imagic:
Text-basedrealimageeditingwithdiffusionmodels.InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages6007–6017,2023.
[13] Bo-KyeongKim,Hyoung-KyuSong,ThibaultCastells,and
Shinkook Choi. On architectural compression of text-to-
imagediffusionmodels. arXivpreprintarXiv:2305.15798,
2023.
[14] Woojeong Kim, Suhyun Kim, Mincheol Park, and Geun-
seokJeon. Neuronmerging: Compensatingforprunedneu-
10rons. AdvancesinNeuralInformationProcessingSystems, [27] BenPoole,AjayJain,JonathanT.Barron,andBenMilden-
33:585–595,2020. hall. DreamFusion: Text-to-3D using 2D diffusion. arXiv
[15] WoosukKwon,SehoonKim, MichaelWMahoney,Joseph preprintarXiv:2209.14988,2022.
Hassoun, Kurt Keutzer, and Amir Gholami. A fast post- [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
training pruning framework for transformers. Advances in Patrick Esser, and Bjo¨rn Ommer. High-resolution image
Neural Information Processing Systems, 35:24101–24116, synthesis with latent diffusion models. In Proceedings of
2022. theIEEE/CVFConferenceonComputerVisionandPattern
[16] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Recognition,pages10684–10695,2022.
Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, [29] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Surya Ganguli. Deep unsupervised learning using
and Daniel Haziza. xFormers: A modular and hackable nonequilibriumthermodynamics. InProceedingsoftheIn-
transformermodellinglibrary. https://github.com/ ternationalConferenceonMachineLearning, pages2256–
facebookresearch/xformers,2022. 2265.PMLR,2015.
[17] HaoyingLi,YifanYang,MengChang,ShiqiChen,Huajun [30] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Feng,ZhihaiXu,QiLi,andYuetingChen. SRDiff: Single Denoising diffusion implicit models. arXiv preprint
image super-resolution with diffusion probabilistic models. arXiv:2010.02502,2020.
Neurocomputing,479:47–59,2022. [31] YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,Ab-
[18] LijiangLi,HuixiaLi,XiawuZheng,JieWu,XuefengXiao, hishekKumar,StefanoErmon,andBenPoole. Score-based
RuiWang,MinZheng,XinPan,FeiChao,andRongrongJi. generative modeling through stochastic differential equa-
AutoDiffusion:Training-freeoptimizationoftimestepsand tions. arXivpreprintarXiv:2011.13456,2020.
architecturesforautomateddiffusionmodelacceleration. In [32] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
ProceedingsoftheIEEE/CVFInternationalConferenceon Sutskever. Consistency models. arXiv preprint
ComputerVision,pages7105–7114,2023. arXiv:2303.01469,2023.
[19] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, [33] Suraj Srinivas and R. Venkatesh Babu. Data-free param-
YunFu,YanzhiWang,SergeyTulyakov,andJianRen.Snap- eter pruning for deep neural networks. arXiv preprint
Fusion: Text-to-image diffusion model on mobile devices arXiv:1507.06149,2015.
withintwoseconds.arXivpreprintarXiv:2306.00980,2023. [34] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
[20] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, generativemodelinginlatentspace. AdvancesinNeuralIn-
PietroPerona,DevaRamanan,PiotrDolla´r,andC.Lawrence formationProcessingSystems,34:11287–11302,2021.
Zitnick. MicrosoftCOCO:Commonobjectsincontext. In [35] Hongjie Wang, Bhishma Dedhia, and Niraj K. Jha. Zero-
Proceedings of the European Conference on Computer Vi- TPrune: Zero-shottokenpruningthroughleveragingofthe
sion,pages740–755.Springer,2014. attentiongraphinpre-trainedtransformers. InProceedings
[21] EnshuLiu,XuefeiNing,ZinanLin,HuazhongYang,andYu oftheIEEE/CVFConferenceonComputerVisionandPat-
Wang.OMS-DPM:Optimizingthemodelschedulefordiffu- ternRecognition,2024.
sionprobabilisticmodels. arXivpreprintarXiv:2306.08860, [36] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tack-
2023. ling the generative learning trilemma with denoising diffu-
[22] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo sionGANs. arXivpreprintarXiv:2112.07804,2021.
numericalmethodsfordiffusionmodelsonmanifolds.arXiv [37] Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and
preprintarXiv:2202.09778,2022. Yingcong Chen. Denoising diffusion step-aware models.
[23] Simian Luo, Yiqin Tan, LongboHuang, Jian Li, andHang arXivpreprintarXiv:2310.03337,2023.
Zhao. Latent consistency models: Synthesizing high- [38] XingyiYang,DaquanZhou,JiashiFeng,andXinchaoWang.
resolution images with few-step inference. arXiv preprint Diffusionprobabilisticmodelmadeslim. InProceedingsof
arXiv:2310.04378,2023. theIEEE/CVFConferenceonComputerVisionandPattern
[24] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Recognition,pages22552–22562,2023.
Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. [39] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and
Ondistillationofguideddiffusionmodels. InProceedings KevinBailly. Red: Lookingforredundanciesfordata-free
oftheIEEE/CVFConferenceonComputerVisionandPat- structuredcompressionofdeepneuralnetworks. Advances
ternRecognition,pages14297–14306,2023. inNeuralInformationProcessingSystems,34:20863–20873,
[25] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch- 2021.
Elor, and Daniel Cohen-Or. Localizing object-level shape
variations with text-to-image diffusion models. arXiv
preprintarXiv:2303.11306,2023.
[26] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
Robin Rombach. SDXL: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952,2023.
11Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models
Supplementary Material
TheSupplementaryMaterialisorganizedasfollows.We
UP1
first provide more implementation details of AT-EDM in
Section A, including a detailed illustration of the SD-XL UP2
backbone. Then, we provide a more comprehensive com- UP3
parisonwiththestate-of-the-artmethod,ToMe[1],inSec- Mid
tionB,includingananalysisofwhyToMeperformsworse Down3
on SD-XL [26] than on previous versions of Stable Diffu-
Down2
sionModels(SDMs). Weprovidemoreablationresultsin Conv+Res Attn
Down1
Section C to justify our design choices in the main article.
We analyze the memory footprint of AT-EDM in Section 0 500 1000 1500 2000 2500 3000
GFLOPs
D.AT-EDMisorthogonaltovariousefficientDMmethods,
suchassamplingdistillation,thuscanfurtherboosttheiref-
Figure 12. The FLOPs breakdown of SD-XL. Measured with
ficiency. Tosupportthisclaim, wedeployAT-EDMinthe
1024×1024pximagegeneration.
distilled version of SD-XL, SDXL-Turbo1, and show cor-
responding experimental results in Section E. We discuss
limitationsandtrade-offsofAT-EDMinSectionFandpo-
tentialnegativesocialimpactsofAT-EDMinSectionG. 512x512 Conv+Res Attn
A.ImplementationDetails
768x768
Inthissection,weprovidemoredetailsoftheimplementa-
1024x1024
tionofAT-EDM.Wefirstintroducethearchitectureofour
SD-XLbackboneasbackgroundmaterialandthendescribe
0 1000 2000 3000 4000 5000 6000 7000 8000
our single-step and cross-step pruning schedules in detail. GFLOPs
We describe details of the evaluation and our calibration
blockforFLOPsmeasurementintheend. Figure13.TheFLOPsbreakdownofResNetblocksandattention
blocksinSD-XLatdifferentimageresolutions.
A.1.TheSD-XLBackbone
The state-of-the-art version of SDM is SD-XL. Compared
with previous versions of SDM, it increases the quality of attention block dominates the cost at all resolutions. Note
generated images significantly. Thus, we select SD-XL as thattheFLOPscostofattentionblocksdoesnotscalemuch
thebackbonemodelinthisarticle. Specifically,wedeploy faster than that of ResNet blocks when the generation res-
AT-EDM and ToMe on SDXL-base-0.9. The architec- olutionincreases. Webelievethisisduetotheelimination
turehastwomaindifferencesfromthatofpreviousSDMs, ofattentionblocksatthehighestfeaturelevelandtheaddi-
such as SD-v1.5 and SD-v2.1: (1) attention blocks at the tion of attention layers at the lowest feature level, making
highestfeaturelevel(i.e.,withthemosttokens)aredeleted; the cost of feed-forward layers, which scales linearly with
(2) attention blocks can potentially include multiple atten- an increment in token numbers, a huge part of the cost of
tionlayers(anattentionlayeriscomposedofself-attention, attentionlayers.
cross-attention,andfeed-forwardnetwork),suchasA2(in-
cludes 2 attention layers) and A10 (includes 10 attention
A.2.PruninginaSingleDenoisingStep
layers).
Tovalidatetheconclusionthatthecostofattentionlay-
For a concise design, we always insert the pruning layer
ersdominatesthesamplingcost,weinvestigatetheFLOPs
after the first attention layer of each attention block. All
costofSD-XL.ItsFLOPsprofileisshowninFig.12. This
theotherattentionlayersinthisattentionblockcanbenefit
figureindicatesthattheattentionblockdominatesthecom-
from the reduction in token numbers. We may also insert
putationalcostofallstagesthatincludeattention. Wealso
multiplepruninglayersatvariouslocationsinanattention
investigate the scaling law of SD-XL at different genera-
block, which prunes tokens gradually. However, this re-
tion resolutions, as shown in Fig. 13. We observe that the
quires a more thorough hyperparameter search to ensure a
1https://huggingface.co/stabilityai/sd-turbo goodbalancebetweenFLOPscostandimagequality.
124 ×H ×W 4 ×H ×W
C C
A Attention Block with
320 ×H ×W R ResNet Block C Convolution Block N NAttention Layers 320 ×H ×W
Down Down Stage
R R C Up Up Stage Unpruned blocks in early steps R R R
Mid Mid-Stage
! #
320 × × 640 ×H ×W
" "
A A A A A R 2 R 2 C R 2 R 2 R 2 C
F L F M L
! # ! #
640 × × 1280 × ×
$ $ " "
Mid
A A A A A A R 10 R 10 R 10 R R 10 R 10 R 10 C
! # ! #
1280 × × 1280 × ×
F L $ $ $ $ F M L
Figure14. TheU-NetarchitectureofSD-XL.Residualconnectionsarenotshownhereforbrevity. Theexampleinthisfiguregenerates
a8H ×8W pixelimage. Theinput/outputsizeofeachstageisshownintheC×H ×W format,whereC isthenumberofchannels;
H andW representtheresolution. Therearetwoattentionblocks{F(First), L(Last)}ineachdownsamplingstageandthree{F(First),
M(Middle), L(Last)} in each upsampling stage. In the prune-less schedule, we do not apply pruning to attention blocks in the gray
rectangles. Downsamplingstage1,2,and3isatthefirst,second,andthirdfeaturelevel,respectively. AT-EDM†doesnotapplypruning
toattentionblocksatthesecondfeaturelevel.
A.3.ThePrune-LessSchedule whenwegenerateimagesforvisualcomparisonandquanti-
tativeanalysis. WerunallexperimentsonasingleNVIDIA
Earlydenoisingstepsdeterminethelayoutofthegenerated
A100-40GBGPU.
images and have a weaker ability to differentiate between
unimportant tokens [8]. Thus, we need heterogeneous de- A.5.CalibrationBlockforFLOPsMeasurement
noising steps and, hence, use a less aggressive pruning
The popular library for FLOPs measurement, fvcore4,
scheduleforsomeoftheearlydenoisingsteps.
is not natively compatible with SDMs. Thus, we use
In the normal pruning setting, when we target 4.1
the THOP5 library instead to measure the FLOPs cost of
TFLOPs for each sampling step, we use a pruning rate of
SDMs. However, we found it does not correctly compute
63% (i.e., retain 37% tokens) after the first attention layer
the FLOPs cost of self-attention. The FLOPs cost of sam-
of A2 and A10; in the prune-less schedule, we do not ap-
plingstepsgivenbythislibraryscaleslinearlyasthenum-
plypruningtoattentionblocksinthegrayrectanglesshown
berofimagetokens. Thisisunreasonablebecausethecost
in Fig. 14. We validate the choice of not deploying prun-
ofself-attentioninsamplingstepsscalesquadraticallywhen
ingthroughablativeexperimentalresultsshowninthemain
the number of tokens increases (other parts of a sampling
article.
step scale linearly). After a thorough investigation of the
A.4.DetailsofEvaluation behavior of THOP, we found it basically does not take the
costofself-attentionintoaccount. Thus, wedesignacali-
When measuring the FID and CLIP scores on MS-COCO
brationblocktosupplementthemissedtermofFLOPscost
2017 [20], we deduplicate captions to make sure each im-
foreachattentionblock:
agecorrespondstoasinglecaption. Wecentercroppedim-
ages in the validation set, resize them to 1024×1024 px,
F =4×B×N ×(HW)2×C (2)
and use the clean-fid library2 to calculate FID scores. cali a
WeusetheViT-G/14modelofOpen-CLIP3tocalculatethe where B is the batch size; N is the number of attention
a
CLIPscoresofgeneratedimages. Wesetthebatchsizeto3 layersinthisattentionblock; HW isthenumberofimage
2https://github.com/GaParmar/clean-fid/tree/main 4https://github.com/facebookresearch/fvcore
3https://github.com/mlfoundations/openclip 5https://github.com/Lyken17/pytorch-OpCounter
13
1
nwoD
2
nwoD
3 nwoD Up
3
Up
2
Up
1160
SD-XL@6.7TFLOPs
140
ToMe@4.1TFLOPs
120
AT-EDM@4.1TFLOPs
100 AT-EDM†@4.5TFLOPs
80
60
40
20
0.12 0.17 0.22 0.27 0.32
CLIP score (Open Clip ViT-g14)
Figure 15. Complete FID-CLIP score curves. The used CFG
scalesare[1.0,1.5,2.0,2.5,3.0,4.0,5.0,6.0,7.0,9.0,12.0,15.0].
tokens; and C is the number of channels. The factor 4 is
due to the fact that (1) there are two images processed at
the same time for each generated image in a batch (one is
guidedbytheprompt,andanotherisnot);(2)therearetwo
Matrix-MatrixMultiplications(MMMs)inself-attention.
B.ComprehensiveComparisonwithToMe
Inthissection,wefirstanalyzewhyToMecannotreplicate
onSD-XLitsgoodperformanceonpreviousSDMsinSec-
tion B.1. Then, we provide complete FID-CLIP curves to (a) SD-XL (b) ToMe (c) AT-EDM
compare AT-EDM with ToMe in Section B.2. In the end,
we present cases in which both AT-EDM and ToMe per- Figure16. ExamplesonwhichbothAT-EDMandToMeperform
formwellandvisuallycompareAT-EDMandToMeunder well. Eachrowofthisfigurecorrespondstothefollowingtypical
cases:(1)simplesinglemainobjectwithasimplebackground;(2)
variousFLOPsbudgetsinSectionB.3.
multiplemainobjects; (3)complexsinglemainobject; (4)com-
B.1.DeployingToMeonSD-XL plexscenewithoutamainobject.
For SD-v1.x and SD-v2.x, ToMe maintains the generated
imagequalityquitewellaftertokenmerging. However, as thattomeetthe4.1TFLOPsbudgetforeachsamplingstep,
we demonstrate in the main article, ToMe incurs obvious we set the merging ratio to 50% for ToMe under the ex-
qualitydegradationonSD-XLaftertokenmerging. pandedmergingrange.
InthedefaultsettingofToMe,itonlymergestokensfor
B.2.CompleteFID-CLIPCurves
attention blocks at the highest feature level and their self-
attention. However, SD-XL eliminates attention blocks at Weexplorethetrade-offbetweentheCLIPandFIDscores
the highest abstraction level and native ToMe does not do through various CFG scales. We show the complete FID-
anything to this backbone. Thus, it is necessary to expand CLIPcurvesinFig.15. AT-EDM†doesnotdeploypruning
its merging range to attention blocks at all feature lev- at the second feature level (as mentioned in the caption of
els. In addition, since SD-XL adds a lot more attention Fig. 14). This figure illustrates that for most CFG scales,
layers at the lowest feature level, where tokens are signif- AT-EDM not only lowers the FID score but also results in
icantlyfewerthanathigherfeaturelevels,self-attentionno higherCLIPscoresthanToMe,implyingthatimagesgener-
longer dominates the cost of attention layers. Given that atedbyAT-EDMnotonlyhavebetterqualitybutalsobetter
themergingratioofToMehasanupperlimitof75%, itis text-imagealignment.
notenoughtoonlymergetokensforself-attentiontomeet
B.3.MoreImagesfromAT-EDMandToMe
the 4.1 TFLOPs budget. Thus, it is necessary to expand
itsmergingrangetoCross-Attention(CA),Self-Attention Insomecases, ToMeperformsfairlywellandhasitsmer-
(SA), and the Feed-Forward (FF) network. We believe its.WepresentseveraltypicalexamplesinFig.16.Thefirst
the expanded deployment range of token merging leads to exampleinthefirstrowrepresentsthecaseofasimplemain
the relatively poor performance of ToMe on SD-XL. Note objectwithasimplebackground. BothToMeandAT-EDM
14
erocs
DIF“A plate is filled with broccoli and noodles.”
SD-XL
“Three birds walking around a dry grass field.”
SD-XL
(a) 6.7 TFLOPs (b) 2.9 TFLOPs (c) 3.6 TFLOPs (d) 4.1 TFLOPs (e) 4.5-4.6 TFLOPs
Figure17.ComparisonbetweenAT-EDMandToMeunderdifferentFLOPsbudgets.NotethatforCol.e,theaveragecostofeachsampling
stepforAT-EDM(ToMe)is4.52(4.56)TFLOPs.PromptsareselectedfromtheMS-COCO2017validationdataset.
preservethemainobjectquitewell. Thesecondrowrepre- tooutperformit.
sentsamorecomplexcaseinwhichtherearemultiplemain WealsoprovidevisualexamplesofToMeandAT-EDM
objectsinthegeneratedimage. AlthoughToMelosessome underdifferentFLOPsbudgetsinFig.17. Itindicatesthat
texture details, it preserves the overall layout quite well. AT-EDMoutperformsToMeunderanyFLOPsbudget. We
Thethirdrowisthecaseofatypicalcomplexmainobject, alsoobservethatAT-EDMneedsatleast3.6TFLOPsbud-
a human face. In this example, ToMe preserves the face gettoensureanacceptableimagequality.
without artifacts. The last row of this figure demonstrates
thecaseofgeneratingacomplexscenewithoutamainob-
C.MoreAblationExperiments
ject. Inthiscase,bothToMeandAT-EDMcanmaintainthe
layoutwellwhilesupplementingsomedetails.Theseexam-
Inthissection,wesupplementablationexperimentstoval-
plesshowthatToMeisastrongbaselineanditisnon-trivial
idate our design choices. We first discuss the deployment
15
AT-EDM
ToMe
AT-EDM
ToMetration on a few tokens. Then, when designing f(A,s ),
K
we need to (1) reward the similarity between the attention
distribution(i.e.,eachrowofA)andtheimportancedistri-
bution(i.e.,s );(2)penalizeuniformattentiondistribution.
K
Basedonthesepoints,weobtainseveralimplementationsof
f(A,s ). Wehadmentionedanentropy-basedimplemen-
K
tationinthemainarticle,whichrewardssimilaritythrough
thedot-productandpenalizesuniformdistributionthrough
entropy. Weprovideadditionalimplementationshere:
(I)Hard-clip-basedimplementation
N
(cid:88)
st+1(x )=f(A,st+1)= ϵ(A −η)·st+1(x ) (3)
(a) SD-XL (b) After-FF (c) Before-FF Q i K i,j K j
j=1
Figure18. Comparisonbetweeninsertingthepruninglayerafter
where ϵ(x) = 1 if x ≥ 0, ϵ(x) = 0 if x < 0; η is the
theFFandbeforetheFFlayer.
thresholdofattention(wesetitto0.2asthedefaultsetting);
A istheattentionfromQueryq toKeyk .
i,j i j
(II)Soft-clip-basedimplementation
location for run-time pruning and then compare different
implementationsofthemappingfunctionf(A,s )forCA-
K
basedWPR.NotethatCA-basedWPRandSA-basedWPR
N
are two implementations of G-WPR and we mainly focus st+1(x )=f(A,st+1)=(cid:88) Sig(A −η)·st+1(x ) (4)
Q i K i,j K j
onCA-basedWPRinthissection. Wealsoinvestigatethe
j=1
schedulethatprunesmoreinearlydenoisingstepsandver-
ifyourintuitionofpruninglessinearlysteps. whereSig(x)= 1 .
1+e−x
C.1.DeploymentLocationforRun-TimePruning
(III)Power-basedimplementation
Inourdefaultsetting,weusegeneratedmasksaftertheFF
layer to perform token pruning. Another option is to per-
N
formpruningearlybeforetheFFlayers,whichresultsina st+1(x )=f(A,st+1)=(cid:88) (β·st+1(x ))α·Ai,j (5)
Q i K K j
littlebitofextraFLOPsavingsatthecostofimagequality.
j=1
We provide several visual examples in Fig. 18. Note that
here,wesimplychangethepruninglayerinsertionlocation where α and β are scaling factors to ensure that β ·
withoutkeepingthetotalFLOPscostfixed,whichisdiffer- st+1(x )>1andα·A >1forlargest+1(x )andA .
K j i,j K j i,j
entfromwhatwedointheablationexperimentsinthemain Here, we let α = 5 and β = N 2t, where N
t
denotes the
article. We find that inserting the pruning layer before the numberofKeytokens.
FFlayerindeedhurtsimagequality(althoughslightly). For We compare these implementations visually in Fig. 19.
example, theplantinthefirstexampleandtheUFOinthe We find that among these implementations, the hard-clip-
secondexamplebecomeworse. Giventhatpruningbefore based implementation performs the worst. Although the
the FF layer only results in marginal extra FLOPs savings entropy-basedimplementationandthepower-basedimple-
(reduces the cost from 4.1 TFLOPs to 4.0 TFLOPs), we mentation are better than other implementations for CA-
choose to prune after the FF layer to obtain better image basedWPR,noneofthemcanoutperformSA-basedWPR.
quality. Thus, we use SA-based WPR as our default setting in AT-
EDM.
C.2.ImplementationsofCA-basedWPR
C.3.Prune-LessScheduleforEarlyDenoisingSteps
To generalize WPR to cross-attention, we need to design
a function f(A,s ) that maps the importance of Key to- The prune-less schedule selects one attention block from
K
kenstothatofQuerytokens. Theintuitionbehinddesign- each down-stage and up-stage in the U-Net and skips the
ing this function is that vital Query tokens should devote token pruning in it. We generate images with the same
muchoftheirattentiontoimportantKeytokens. Thus,the prompts and different selections, as shown in Fig. 20. It
desired attention distribution should satisfy: (1) similarity indicatesthatF-Lappearstobethebestchoice. F-Listhe
to the importance distribution of Key tokens; (2) concen- schedulethatweshowinFig.14.
16CA-WPR
(a) SD-XL (b) SA-WPR
(c) Entropy (d) Hard Clip (e) Soft Clip (f) Power
Figure19. ComparisonbetweendifferentimplementationsofCross-Attention-basedWPR.NoneofthemcanoutperformSelf-Attention-
basedWPR.
(a) SD-XL (b) F-F (c) F-M (d) F-L (e) L-F (f) L-M (g) L-L
Figure20. Comparisonbetweendifferentprune-lesssettings. Therearetwoattentionblocks{F(First),L(Last)}thatareleftunprunedin
thedownsamplingstagesandthree{F(First),M(Middle),L(Last)}intheupsamplingstages.ResultsindicatethatF-Listhebestschedule.
C.4.TheNumberofPrune-LessSteps thefirst15denoisingsteps. Thissupportsourhyperparam-
eterchoice.
The intuitions that we use to design the prune-less sched-
uleintheearlydenoisingstepsare(1)earlydenoisingsteps C.5.PruneMoreinEarlyDenoisingSteps
determinethelayoutofgeneratedimagesandthusarecru-
cial; (2)earlydenoisingstepshaveaweakerabilitytodif- In AT-EDM, we design a cross-step pruning schedule that
ferentiate unimportant tokens. The first intuition prohibits is less aggressive in early denoising steps. This is based
usfrompruningmoretokensintheearlysteps(seeSection ontheintuitionthat(1)earlydenoisingstepsdeterminethe
C.5). Thesecondintuitionguidesustochoosethenumber layoutofgeneratedimagesandthusareveryimportant;(2)
ofprune-lesssteps. Thevarianceofattentionmapsreflects theabilityofearlydenoisingstepstodifferentiatebetween
theirabilitytodifferentiateunimportanttokenssincetheat- unimportant tokens is weaker than that of later steps. To
tention score of unimportant tokens deviates significantly verifyourintuition,weinvestigatetheschedulethatprunes
fromthatofnormaltokens. Weshowthevarianceofatten- more in early denoising steps. Note that for symmetry,
tionmapsgivenbydifferentdenoisingstepsinFig.5. The “prunemoreinthefirst15steps”selectscorrespondingat-
figure indicates that the variance is more than 1.0E-5 after tention blocks in the last 35 steps for not pruning tokens
17Turbo, which is a distilled version of SD-XL. Our exper-
imental results show that although SDXL-Turbo reduces
around95%FLOPscostofSD-XL,AT-EDMcanfurther
reducetheFLOPscostofSDXL-Turboby33.4%while
reducingFIDby14.5%andonlyincurring2.1%CLIP
reduction on MSCOCO-2017. AT-EDM works as a regu-
larizerandslightlyimprovesthequalityofimages.
F.LimitationsandTrade-Offs
AT-EDMdemonstratesstate-of-the-artresultsforaccelerat-
ingDMinferenceatrun-timewithoutanyretraining. How-
ever,asamachinelearningalgorithm,itinevitablyhassome
limitations.
Prune Less Prune More
(a) SD-XL
(b) 15 Steps (c) 15 Steps (d) 30 Steps (1)AT-EDMrequiresapre-trainedDM;sinceitsavescom-
putation to accelerate the model, its performance is inher-
Figure21. ComparisonbetweendifferentDSAPschedules. Ex-
ently upper-bounded by the full-sized one. While most of
amplesindicatethatpruningmoretokensinearlydenoisingsteps
the time, AT-EDM matches the performance of the pre-
changesthelayoutofgeneratedimagessignificantly.
trainedmodel,bothquantitativelyandqualitatively(seeex-
perimental results in the main article), with around 40%
FLOPsreduction,thereexistsomesampleswherethefull-
while keeping the total FLOPs cost fixed. We provide vi-
sizedmodeloutperformsAT-EDM(seeFig.17). Nonethe-
sual examples in Fig. 21 for comparison. These examples
less, AT-EDM outperforms prior art by a clear margin. In
clearlysupportourintuitionthatpruningmoreinearlyde-
addition, AT-EDM is differentiable. We will fine-tune the
noisingstepsnotonlyaffectsthelayoutofgeneratedimages
prunedmodeltofurtherimprovequalityinthefuture.
butalsohurtsimagequality.
(2) AT-EDM leverages the rich information stored in the
D.MemoryFootprintofAT-EDM attentionmaps,whichcouldbeinaccessiblewithoutincur-
ringoverheadduetotheopen-sourcednatureoftheimple-
Since we need to obtain the attention map from the first
mentation. For instance, SD-XL [26] adopts an efficient
attention layer, AT-EDM cannot reduce the peak memory
attention library, xFormers [16], which fuses computation
footprint. However, benefiting from the significantly re-
to directly obtain succeeding tokens without providing in-
duced number of tokens in the following attention lay-
termediateattentionmaps. AsshowninTable2,inthecase
ers, AT-EDM reduces the average memory footprint sig-
thatFusedOperation(FO)isnotused,usingAT-EDMleads
nificantly. Since PyTorch does not automatically release
to significant latency savings. With FO under the Current
theredundantassignedmemorywhenthememoryrequire-
Implementation(CI),AT-EDMdoesnotresultinahugela-
ment reduces in the later layers, we theoretically estimate
tency saving due to the cost of calculating attention maps.
the average footprint of AT-EDM, assuming the redundant
Reusing attention maps across steps and obtaining an ap-
occupiedmemorywillbereleasedinthelayerswithfewer
proximation for them could alleviate this issue. With FO
tokens. We believe this is practical when the implementa-
undertheDesiredImplementation(DI)thatprovidesatten-
tionisgoodenough. Thepeakandtheoreticalaveragefoot-
tionmaps,AT-EDM’spotentialisfullyunlockedandleads
print of full-size SD-XL (AT-EDM) are 19.5GB (19.5GB)
toadecentspeedup.
and 18.8GB (12.6GB), respectively. This indicates that if
wehaveafine-grainedpipelineschedule,AT-EDMallows AT-EDMisespeciallygoodatgeneratingobject-centric
us to run 49.2% more generation tasks with the given images, such as a portrait. It can employ a high pruning
VRAMrestriction. rate without hurting the main object. Generating complex
scenes or tens of objects is relatively tricky for AT-EDM
E.StackwithSamplingDistillation sinceitmaylosesomedetailsincornercases. Insomerare
corner cases where the texture details are not significant,
Methods like consistency distillation [23, 32] can greatly ToMe might perform slightly better, as our algorithm may
reducethecostofDMs. NotethatAT-EDMdoesnotcon- prunetoomanytokensinthatsmallregion. ToMeisindeed
tradict these methods and can be deployed to speed them astrongbaseline,butitisremarkablethatourAT-EDMstill
upfurther. Tosupportthis, wedeployAT-EDMinSDXL- outperformsitinmostcases.
18G.PotentialNegativeSocialImpacts
Text-to-image generative models like SD-XL have signif-
icantly advanced the field of AI and digital art creation.
However,theymayalsopotentiallyhavenegativesocialim-
pact. For example, they can create highly realistic images
that may be indistinguishable from real photographs. As
the technology can be used to create convincing but false
images,thiscanpotentiallyleadtoconfusionandmisinfor-
mationspread.Inaddition,theuseofthesemodelstocreate
inappropriateorharmfulcontent,suchasrealisticimagesof
violence,hatespeech,orexplicitmaterial,raisessignificant
ethicalquestions. Thereisalsothepotentialforperpetuat-
ingbiasesiftheAImodelistrainedonbiaseddatasets.
19