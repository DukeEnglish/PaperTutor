Mitigating Negative Side Effects in Multi-Agent Systems Using Blame Assignment
PulkitRustagi and SandhyaSaisubramanian
OregonStateUniversity
{rustagip,sandhya.sai}@oregonstate.edu
Abstract of agents’ joint actions in the environment, such as a nar-
rowcorridorbeingblockedforhumanaccesswhenmultiple
Whenagentsthatareindependentlytrained(orde- robots simultaneously move large shelves through it. Thus,
signed) to complete their individual tasks are de- evenwhentheagentsareadeptatcompletingtheirtasksand
ployedinasharedenvironment, theirjointactions producenoNSEswhenactinginisolation,theirjointactions
mayproducenegativesideeffects(NSEs). Astheir mayhaveundesirableeffects.
trainingdoesnotaccountforthebehaviorofother Mitigating NSEs in multi-agent settings is challenging
agents or their joint action effects on the environ-
because: (1) NSEs are often discovered after deployment,
ment, the agents have no prior knowledge of the
sincetheydependontheenvironment,agentinteractionsand
NSEs of their actions. We model the problem of
their assigned tasks; (2) NSEs and corresponding penalties
mitigating NSEs in a cooperative multi-agent sys-
are defined over joint actions, introducing a dependency be-
temasaLexicographicDecentralizedMarkovDe-
tween agents; and (3) the computational complexity of mit-
cision Process with two objectives. The agents
igating NSEs, without significantly affecting task comple-
must optimize the completion of their assigned
tion, increases with the number of agents in the system.
taskswhilemitigatingNSEs. Weassumeindepen-
Prior research on NSEs primarily target single-agent set-
denceoftransitionsandrewardswithrespecttothe tings [Krakovna et al., 2020; Saisubramanian et al., 2020;
agents’ tasks but the joint NSE penalty creates a Saisubramanian et al., 2022; Zhang et al., 2018]. It is not
form of dependence in this setting. To improve
straightforwardtoapplythemtomulti-agentsettingsasthey
scalability, the joint NSE penalty is decomposed
donotaccountfortheagentinteractionsthatproduceNSEs.
intoindividualpenaltiesforeachagentusingcredit Recently, [Alamdari et al., 2022] proposed an approach to
assignment, which facilitates decentralized policy
avoidNSEsbyoptimizingthefuturereturnsbyotheragents
computation.Ourresultsinsimulationonthreedo-
in the environment. However, it cannot avoid NSEs that re-
mainsdemonstratetheeffectivenessandscalability
quirepolicyupdateformultipleagentsanddonotfullyscale
ofourapproachinmitigatingNSEsbyupdatingthe
tolargesettings.
policiesofasubsetofagentsinthesystem.
We formulate the problem of mitigating NSEs in a multi-
agent system as a decentralized Markov decision process
(DEC-MDP) with two objectives and a lexicographic order-
1 Introduction
ingoverthem. Theprimaryobjectiveforeachagentistoop-
Many real-world settings involve the operation of multiple timizeitsassignedtaskandthesecondaryobjectiveistomin-
autonomous agents that do not require coordination for the imize the NSEs. We assume independence of transition and
completion of their individual assigned tasks in an environ- rewards with respect to the agents’ assigned tasks [Becker
ment,suchasrobotswithdistinctareasofoperationinfactory et al., 2004]. Each agent’s reward for task completion is
warehouses[D’Andrea,2012]. Suchagentsareoftentrained determined by its local state and actions and the overall re-
in isolation and their training does not account for potential ward for the system is the sum of individual agent’s re-
negativeinteractionswithotheragentsortheircumulativeef- wards,andagents’statetransitionsareaffectedonlybytheir
fects on the environment. Consequently, the agents may act own actions. We use a lexicographic formulation since it is
inwaysthatarecollectivelyundesirablewhendeployed,such moreintuitiveandmanyreal-worldproblemshaveaninher-
as producing negative side effects (NSEs) [Alamdari et al., ent ordering over objectives. It has been also been shown
2022; Saisubramanian et al., 2020]. This paper focuses on that lexicographic MDPs can be converted to a constrained
mitigating NSEs in cooperative multi-agent settings where MDP[Pinedaetal.,2015].
the agents produce no (or negligible) NSEs when executing The agents do not have prior knowledge about NSEs but
theirpolicyinisolation,buttheirjointpolicyresultsinNSEs. incurajointpenaltyforthemwhenexecutingcertainactions
Consider warehouse robots, operating alongside humans, together.Thus,whileagentscanindependentlycomputepoli-
that optimize moving shelves between two locations. Each cies to complete their tasks, mitigating NSEs requires ad-
robot’s model provides the necessary information, including dressingaformofdependencecreatedbythejointpenalty.
reward and transition dynamics, to complete its task opti- Wepresentametareasoning[CarlinandZilberstein,2012;
mally. The models may lack information about the effects Zilberstein,2011]approachtodetectandmitigateNSEs.The
4202
yaM
7
]AM.sc[
1v20740.5042:viXraFigure1:Overviewofoursolutionapproach.Theagentsindependentlycomputeoptimalpoliciestocompletetheirassignedtasksdescribed
byRi (Naivepolicy). TheNSEMonitorcomputestheNSEpenaltyassociatedwiththejointpolicy⃗π. TheBlameResolverthenperforms
1
blameassignmenttodecomposethepenaltyineachjointstate-actionpairintopenaltiesassociatedwitheachagent’slocalstate-actionpair,
whichyieldsRi . Theagentsfinallyrecomputetheirpoliciesbysolvingthebi-objectiveproblemusinglexicographicvalueiteration(LVI)
N
withRi ≻Ri .
1 N
metareasonerisacentralizedentitythatmonitorstheagents’ 2 RelatedWorks
behaviors and has two components: (1) NSE Monitor that
NegativeSideEffects Priorworkshaveconsidereddiffer-
estimatestheNSEsassociatedwithagents’ jointpolicy, and
ent definitions of NSEs such as hindering the completion of
outputsthecorrespondingpenalty,and(2)aBlameResolver
futuretasksortheoperationofotheragents[Alamdarietal.,
thatdecomposesthejointpenaltyintoindividualagentpenal-
2022; Klassen et al., 2022; Krakovna et al., 2020; Turner et
ties, using our algorithm Reward Estimation using Counter-
al.,2020],andmodelincompletenessintheformofmissing
factualNeighbors(RECON).ItisassumedthattheNSEmon-
informationorincompleterewardfunction[Hadfield-Menell
itorhasaccesstoamodelofNSEsandtheassociatedpenalty,
etal.,2017;Saisubramanianetal.,2022;Zhangetal.,2018;
either provided as part of its design or acquired through hu-
Zhangetal.,2020]. Oursettingissimilartothelatterworks
man feedback. The blame resolver, using RECON, per-
inthattheagentshaveincompleteinformationaboutthecu-
forms blame (credit) assignment to determine each agent’s
mulative effects of their joint actions. A common approach
relative contribution towards the NSE, based on which the
in such settings is to modify the existing reward function
joint NSE penalty is decomposed into local penalties for
or to consider a secondary reward function associated with
eachagent[BilbaoandEdelman,2000;Nguyenetal.,2018;
NSEs[Alamdarietal.,2022;Klassenetal.,2022;Krakovna
ProperandTumer,2012]. Thisdecompositionfacilitatesup-
etal., 2018;Saisubramanianetal., 2020]andsolveamulti-
dating the model of each agent with NSE information via a
objectiveproblem.TheinformationaboutNSEsmaybegath-
penaltyfunction,therebyenablingdecentralizedpolicycom-
ered using human feedback [Saisubramanian et al., 2020;
putationtomitigateNSEs.
Zhang et al., 2018] . However, these approaches have been
presented for a single agent setting or consider other agents
Oursolutionframeworkusesafour-stepapproachtomit-
aspartoftheenvironment,anditisnotstraightforwardtoex-
igate NSEs (Figure 1): (1) the agents first calculate optimal
tendthemtomulti-agentsettingswheremultipleagentpoli-
policiestocompletetheirassignedtasks(referredtoasnaive
ciesmustbeupdated.Recently,adistributedcoordinationap-
policies);(2)theNSEmonitorestimatestheNSEpenaltyas-
proachhasbeenproposedformitigatingNSEsinmulti-agent
sociated with the joint policy of the agents; (3) the blame
settings [Choudhury et al., 2024]. They target NSEs aris-
resolver then decomposes the joint penalty into individual
ing due to local agent interactions and formulate it as a dis-
penaltiesforeachagent,usingblame(credit)assignment;and
tributedconstraintoptimizationproblemandtheagentslearn
(4) the agents recompute their policies by solving a decen-
a policy to avoid NSEs using coordinated Q-learning. Their
tralized,bi-objectiveproblem,withtheprescribedrewardfor
approach is not scalable as distributed constraint optimiza-
theirtaskandtheestimatedlocalpenaltyforNSE,usinglex-
tion can become intractable for large problems [Fioretto et
icographicvalueiteration[Wrayetal.,2015].
al., 2018]. Wepresentametareasoningapproachthatscales
tolargemulti-agentsystems.
Ourprimarycontributionsare:(1)formalizingtheproblem
ofmitigatingNSEincooperativemulti-agentsettingsasade- Credit Assignment. Credit assignment is a popular ap-
centralized, bi-objective problem; (2) presenting a metarea- proach to measure the contribution of an agent to team per-
soningapproachtoupdateagentmodelswithapenaltyfunc- formance, which is used to convert a joint reward into in-
tioncorrespondingtoNSEs; (3)introducinganalgorithmto dividual agent rewards [Nguyen et al., 2018]. Difference
decompose joint NSE penalty into individual penalties, us- Reward [Proper and Tumer, 2012] and its variants such as
ingcounterfactual-basedcreditassignment;and(4)empirical D++[Rahmattalabietal.,2016],WonderfulLifeUtilityand
evaluationonthreedomainsinsimulation. AristrocraticUtility[Nguyenetal.,2018],performcreditas-Supportsdecentralized Compatiblewith CangenerateNSE-
CreditAssignmentTechnique Scalable
planning heterogeneousagents specificcounterfactuals
ShapleyValue (cid:37) (cid:33) (cid:33) (cid:37)
DifferenceReward (cid:33) (cid:33) (cid:33) −
ActionNotTaken (cid:33) (cid:33) (cid:33) (cid:37)
D++ (cid:37) (cid:33) (cid:33) (cid:37)
WonderfulLifeUtility (cid:37) (cid:37) (cid:37) (cid:37)
AristrocraticUtility (cid:37) (cid:37) (cid:37) (cid:37)
ValueDecomposition (cid:37) (cid:37) (cid:33) (cid:37)
OurApproach (cid:33) (cid:33) (cid:33) (cid:33)
Table 1: Overview of different credit assignment techniques for mitigating side effects. A technique is scalable if it is computationally
inexpensive to generate counterfactuals for problems with a few hundreds of agents. “−” indicates that the approach cannot be applied
directlybutcanbemodifiedtomeettherequirementsofoursetting.
signmentbycomparingthejointrewardsbeforeandafterthe Wray et al., 2015]. An LMDP is denoted by the tuple
agent is removed from the system. Another type of credit M = ⟨S,A,T,RRR,o⟩ with finite set of states S, finite set of
assignment uses Shapley value and involves computing a actionsA,transitionfunctiondenotedbyT:S×A×S→[0,1]
valuefunctionforeachagentbyconsideringallcombinations and a vector of reward functions RRR = [R ,...,R ]T with
1 k
of possible agent interactions [Bilbao and Edelman, 2000; R :S×A→R, andodenotesthestrictpreferenceordering
i
Meng,2012;Shapley,1953;SundararajanandNajmi,2020]. over the k objectives. The set of value functions is denoted
Table 1 summarizes the characteristics of different credit byVVV =[V ,...,V ]T,withV correspondingtoo ,
1 k i i
assignment techniques. Blame assignment for NSEs must (cid:88)
VVVπ(s)=RRR(s,π(s))+γ T(s,π(s),s′)VVVπ(s′),∀s∈S.
be based on counterfactuals generated by considering NSE-
relevantfeaturesonly. Theexistingmethodscalculatecoun- s′∈S
terfactualsbyconsideringallstatefeatureswhichleadstoin- A slack∆∆∆=⟨δ ,...,δ ⟩ with δ ≥0, denotes the acceptable
1 k i
correctblameassignmentforNSEs. deviation from the optimal expected reward for objective o
i
soastoimprovethelowerpriorityobjectives. Objectivesare
3 Preliminaries processed in the lexicographic order. The set of restricted
actions for o is A (s) = {a ∈ A| max Q (s,a′) −
Decentralized Markov Decision Process (Dec-MDP). A i+1 i+1 a′∈Ai i
Dec-MDPiswidelyusedtomodeldecentralizedmulti-agent Q (s,a) ≤ η whereη =(1−γ)δ ,γ ∈ [0,1). Refer[Wray
i i i i
decision-makingproblems[GoldmanandZilberstein,2004]. etal.,2015]foradetailedbackgroundonLMDP.
A Dec-MDP is defined by the tuple ⟨A,S,A,T,R⟩ with A
denoting the finite set of k agents in the system; S =Sˆ × 4 ProblemFormulation
1
...×Sˆ denoting the joint state space, where Sˆ denotes the
k i ProblemSetting. Considerasettingwheremagentsarein-
statespaceofagenti; A = Aˆ 1×...×Aˆ k denotingthejoint dependently performing their assigned tasks which is their
actionspace,whereAˆ denotesagenti’sactions;T :S×A× primary objective o = {o1,...,om}. The agents operate
i 1 1 1
S → [0,1] denoting the transition function; and R denoting based on a transition and reward independent decentralized
therewardfunction. Ajointpolicy⃗π=(π ,...,π )isasetof Markovdecisionprocess(Dec-MDP),denotedbyM [Becker
1 k
policies,oneforeachagentinthesystem. et al., 2004]. A meta-level process, metareasoner, moni-
Transitionandrewardindependence. ADec-MDPwith tors and controls the agents performance (object-level pro-
transitionindependenceandrewardindependence[Beckeret cess),byinterveningifagentactionshaveundesirableconse-
al., 2004] is a class of problems in which agents operate in- quences[Zilberstein,2011;Svegliatoetal.,2022].
dependently but are tied together through a reward structure WhilethemodelM containsalltheinformationnecessary
that depends on all of their execution histories. A transition tooptimizeo ,itmaynotdescribethejointeffectsofagents’
1
andreward-independentMDPsatisfiesthefollowing: actions, since it is irrelevant to task completion in this set-
ting. Due to the limited fidelity of M, negative side effects
T(sˆ′|s,a,s′ )=T (sˆ′|sˆ,aˆ ),∀i∈A
i −i i i i i (NSEs)occurwhenagentsexecutetheirpoliciesjointly. The
R(s,a,s′)=(cid:88) R (sˆ,aˆ ,sˆ′). agents incur a penalty for NSEs, determined by a function
i i i i R whichisknowntothemetareasoner(eitherbydesignor
i∈A N
learnedusinghumanfeedback)butunknowntotheagents.
Atransitionandreward-independentDec-MDPcanbesolved
We make the following assumptions about the NSEs: (1)
asksingleagentMDPs[GoldmanandZilberstein,2004].
there are no (or negligible) NSEs when agents execute their
Lexicographic MDP (LMDP). LMDPs are particularly policy in isolation but their joint policy produces NSEs that
convenient to model problems with potentially compet- must be mitigated to the extent possible; (2) agents have no
ing objectives with an inherent lexicographic ordering priorknowledgeaboutNSEsofthejointactions,besidesre-
over them, such as ours where task completion is priori- ceivingapenaltyassignedbythemetareasoner;and(3)NSEs
tized over NSE mitigation [Saisubramanian et al., 2020; do not interfere with the agents’ task completion—they areundesirablebutarenotcatastrophic.Wetargetsettingswhere 5 PenaltyDecompositionviaBlame
the completion of the agents’ assigned tasks (o 1) are priori- Assignment
tizedoverminimizingNSEs(o ),o ≻o .
2 1 2
Our algorithm for NSE penalty decomposition, Reward Es-
MASE-MDP. The problem of mitigating NSEs in a coop- timation using Counterfactual Neighbors (RECON), is out-
erativemulti-agentsystemisformulatedasaDec-MDPwith lined in Algorithm 1. RECON first initializes each agent’s
twoobjectivesandalexicographicorderingoverthem. penaltyfunctionRi tozero(Line1). Aftertheagentshave
N
calculatedtheirnaivepoliciesindependently,theNSEMoni-
Definition1. Amulti-agentsideeffectsMDP(MASE-MDP)
torcomponentinthemetareasonercalculatesthejointpenalty
isabi-objectiveDec-MDPwithlexicographicorderingover
⃗r for NSEs incurred from joint policy ⃗π (Line 2). If the
theobjectives,denotedbyM =⟨A,S,A,T,R,o⟩,where: N
penalty exceeds a pre-defined NSE tolerance threshold η,
• A={1,...,m}isafinitesetofagentsinthesystem; then the Blame Resolver decomposes the joint penalty into
• S =Sˆ ×...×Sˆ isthejointstatespace; local penalties for each agent, based on their relative con-
1 m
• A=Aˆ ×...×Aˆ isthejointactionspace; tributiontoNSE,calculatedusingblame(credit)assignment
1 m
(Lines 3-5). The penalty decomposition resolves the depen-
• T :S×A×S →[0,1]isthetransitionfunction;
• R=[R ,R ]istherewardfunctionwithR :S×A→R dencyinducedbythejointpenaltyandenablesdecentralized
1 N 1
policy computation to optimize task completion while mini-
denotingtherewardfortaskperformanceandR :S×
N
A→RdenotingthepenaltyfunctionforNSEs;and mizingNSEs.
• o = [o ,o ]denotestheobjectives,whereo isthepri- Blame Estimation. Blame (credit) assignment estimates an
1 2 1
mary objective denoting agents’ assigned tasks and o agent’s contribution to a global penalty (or reward) using
2
isminimizingNSEs,witho ≻o . counterfactualstates. TheblameB (Eqn.1)iscalculatedfor
1 2 i
each agent i in the joint state s, based on the difference be-
We consider independence of transition function and task
tweenthecurrentNSEpenaltyandtheminimumNSEpenalty
completion reward R in the MASE-MDP, meaning each
1 that could have been achieved by the agent. The existing
agent’s transitions and task reward depend only on its lo-
credit assignment techniques calculate counterfactual states
cal state and action. Hence, each agent can indepen-
by considering all state features. In our setting, NSE oc-
dently compute a policy for its assigned task by solving an
currence is determined only by the dynamic global features
MDPs[Beckeretal.,2004;GoldmanandZilberstein,2004].
(f⃗ ). Therefore,thecounterfactualsmustbecalculatedonly
However,theagentsincurajointpenaltyfortheNSEs,which gd
introduces a form of dependence and prevents decentralized overf⃗ gd. GeneratingcounterfactualstatesforNSEsbycon-
computation of individual agent policies. In Section 5, we sideringallthestatefeaturescouldresultinincorrectattribu-
present an approach to decompose joint penalties into indi- tion. Consider multiple warehouse robots, with some carry-
vidualpenalties,therebyfacilitatingdecentralizedplanning. inglargeshelves, navigatinginanarrowcorridor. TheNSE
ofblockedpathsforhumanaccessisdeterminedbynumber
LocalandGlobalStateFeatures. Weconsiderafactored
of agents carrying large shelves. Generating counterfactual
staterepresentation.LetF denotethesetoffeaturesintheen-
states traditionally will involve changing agent location but
vironment, which are categorized into local features F and
l that does not provide information on NSE associated with
global features F , F = F ∪F . The local features of an
g l g multiple agents carrying large shelves. Hence, counterfac-
agent i are denoted by Fi. Local features are agent-specific
l tualsmustbegeneratedoverNSE-relevantstatefeatures. We
features that are controlled by the agent’s actions and affect
presentamodifiedformulationofcreditassignmentthatuses
its performance (e.g. an agent’s x,y location). Global fea-
counterfactualneighborsinsteadofcounterfactualstates.
turesaresharedamongagentsanddenotetheoverallstateof
thesystem. Theyarefurtherdividedintostaticanddynamic Definition4. Thecounterfactualneighborsofajointstates,
global features, denoted by F
gs
and F
gd
respectively, based denotedbys c,isthesetofallstatesthatvaryonlyintheval-
on whether they are exogenous or can be modified by agent ues of dynamic global features (f⃗ ) while all other feature
gd
actions. Anagent’sstatesˆisdescribedbyf⃗=f⃗i∪f⃗ ∪f⃗ . valuesaresameasins.
l gd gs
Counterfactualneighborsarethereforeasubsetofcounter-
Definition 2. Static global features F are exogenous fac-
gs factual states. Since the NSE penalty depends only on the
torsthataffectallagentsandareobservabletoallagentsbut
state in our formulation, we calculate counterfactual neigh-
notchangedbytheagents’actions(e.g. oceancurrents).
borsoverthestatevariables. Thesetofcounterfactualneigh-
Definition3. DynamicglobalfeaturesF describeproper- borsthatarereachablefromthestartstateintheenvironment
gd
tiesoftheenvironmentthataffectagentoperationdirectlyor arereferredtoasvalidcounterfactualneighborsandarede-
indirectly, and can be modified by agent actions (e.g. loca- notedbysv.
c
tionsandsizesoftheshelvesmovedbytheagents).
Definition 5. Agent-specific counterfactual neighbors, de-
In this work, the penalty for NSE associated with a joint notedbysi c⊂s c,arestatesthatdifferinthosedynamicglobal
actionismodeledasafunctionofthedynamicglobalfeatures featuresthatcanbecontrolledbyagenti,whileotherfeature
of the joint state, R (s) = Ω(f⃗ ). However, the proposed valuesaresameasinthecurrentjointstates.
N gd
frameworkandapproachcanbeextendedtohandlepenalties Agent-specificcounterfactualsallowustoestimateanindi-
definedoverjointstatesandactions. vidualagent’scontributiontoNSE.NotethatthisisatypicalAlgorithm1RECON systems with large number of agents and eliminates the re-
Input: NSEToleranceη,⃗πforprimaryassignedtask quirementofhavinganiterativeapproachwhereagentscould
1: InitializeR Ni (sˆ)=0,∀sˆ∈Sˆ i,∀i∈A d wi osc rkov be ur tn ie tw inN cuS rE si an hu in gs hee cn os mta pt ues ta. tA ion nait le ora vt eiv rhe eR aE dC aO ndN dw oi el sl
2: Calculatejointpenaltyr⃗π for⃗π
N notguaranteeamonotonicimprovementinpolicywitheach
3: ifr N⃗π >ηthen iteration.
4: ComputeblameB i(s)usingEqn.1,∀i∈A,∀s∈S
5: R Ni (sˆ i)←B i(s),∀i∈A,∀sˆ i ∈s,∀s∈S 6 ExperimentalSetup
6: endif
7: return [R1 ,...,Rm] We evaluate our approach in simulation using three proof-
N N
of-conceptgrid-worlddomains(salp,overcooked,andware-
house). Figure2showssampleinstancesfromeachdomain.
formofcounterfactualstategenerationforcreditassignment, All algorithms are implemented in Python and the simula-
inthattheagentcontributionisestimatedbyeitherremoving tionswererunonanUbuntumachinewith32GBRAM.The
it or replacing its actions, while the other agents’ behaviors MASE-MDPproblemissolvedusinglexicographicvalueit-
arefixed[ProperandTumer,2012;Nguyenetal.,2018]. eration[Wrayetal.,2015]andresultsareaveragedoverfive
Counterfactual neighbors vs state counterfactuals. Let instances in each domain. We test with zero NSE tolerance
si,v ⊂svbethesetofvalid(reachable)counterfactualneigh- η=0toevaluatetheextentofNSEmitigationusingRECON,
bc orsforc
agenti. TheblameB foragentiinthejointstates
andusingϵ=10−4forrescalingtheblamevalues.
i
iscalculatedas,
Baselines We compare the performance of RECON with
four baselines. First is a Naive policy that is optimal for
b (s)
B (s)= i ·R (s), with (1) agent task completion and does not optimize NSE mitiga-
i (cid:80) b (s) N
i∈A i tion, providing an upper bound on NSE penalty. Second
(cid:18) (cid:18) (cid:19)(cid:19)
1 is performing credit assignment using the Difference Re-
b (s)= R∗ +ϵ+ R (s)− min R (s′) , (2)
i 2 N N s′∈si c,v N w bla ar md ete bc yhn ri eq mu oe v[ inP grop the er aa gn ed ntTu frm omer, th2 e01 s2 y] stt eh mat
,
c Balc (u s)late =s
where R is the joint NSE penalty function, R∗ = i
N N R (s) − R (s ). A direct comparison with this tech-
max R (s)denotesthemaximumjointNSEpenaltypos- N N −i
s∈S N niqueisnotfeasiblesinceitcalculatescounterfactualsbased
sible which is used to rescale blame in the range [0,R∗ ] to
N on assumptions that do not hold in our setting. Therefore,
avoidimpracticalvaluesthatmightbenegativeorexceedthe
wemodifytheapproachtocalculatecounterfactualsoverdy-
jointNSEpenaltyitself. ϵisasmallfixedvaluetoavoidsin-
namic global features of the agents, and calculate blame as
gularitiesduringnormalizationandrescaling,andb (s)isan
i B (s) = R (s)−max R (s′). Weusemaxoverall
intermediatevalueusedtocalculateB i(s). Theaboveequa- i N s′∈si c,v N
validcounterfactualneighborstogetanaggressiveblameas-
tionassignsblameproportionaltotheagent’sabilitytomiti-
signment,producingconservative(safer)behavior. Third,we
gateNSEs.Thisensuresthatagentsalreadymakingtheirbest
compare with a variant of considerate reward inspired from
effortsarepenalizedlesscomparedtootheragents.
[Alamdari et al., 2022], where they augment agents’ reward
The metareasoner’s Blame Resolver compiles a local
penaltyfunctionRi foreachagentiasfollows: functionswithvaluefunctionsofsubsequentlyactingagentto
N avoidtheirdefinitionofNSE.Sinceinoursetting,agentsop-
Ri (sˆ)=B (s),∀sˆ ∈s,∀s∈S,i∈A. (3) eratesimultaneouslyandNSEisdefinedformultipleagents,
N i i i
weaugmentagents’prescribedrewardswithblamecontribu-
The agents then solve the MASE-MDP, with the pre-
tionsfromtherestoftheagentsasshowninEquation4.
scribedRi correspondingtotheirassignedtaskandRi pro-
1 N
vided by the metareasoner, using lexicographic value itera- Ri(sˆ) R (s)−B (s)
tion(LVI)[Wrayetal.,2015]. R ri(sˆ i)=α
1
1 R∗i +α
2
N
R∗
i (4)
GeneralizingRi ThepenaltyfunctionRi obtainedfrom 1 N
N N
RECONinAlgorithm1,iscalculatedbasedontheblameval- whereR 1∗ =max sˆ∈Sˆ iR 1(sˆ)isthemaximumpossiblereward
uescorrespondingto⃗πanddoesnotprovideanyinformation forassignedtaskforagenti,α andα aretheselfishandcare
1 2
aboutpotentialNSEsthatmayoccuriftheagentsfolloweda coefficientsrespectively. R∗ andR∗ areusedtonormaliza-
1 N
differentjointpolicy. Asaresult,NSEsmaypersistwhenthe tionsothattheirrelativescalesarenotaninherentfactor,buta
agents update their policy by solving the MASE-MDP with controlledone(usingα ,α ). Fourth,wecomparewithgen-
1 2
Ri . To overcome this limitation, we consider a supervised eralized RECON w/o counterfactual data, which uses only
N
learning approach to generalize the NSE penalty to unseen theRi valuesfromRECONtogeneralizethepenaltysitua-
N
situations by using the Ri , based on the initial ⃗π, as train- tionsusingsupervisedlearningonglobalfeaturesofthelocal
N
ingdata. Thepredictionaccuracycanfurtherbeimprovedby statesandattributedblamevalues. Finally,wecomparewith
including the counterfactuals as part of the training [Zerbel generalizedRECONw/counterfactual(cf)data, whichuses
and Tumer, 2023]. In our experiments, learning to general- bothcounterfactualsandRi valuestogeneralizeRi toun-
N N
izeusingcounterfactualsimprovestheperformanceinsome seensituationsusingsupervisedlearningsimilartotheprevi-
settings,comparedtogeneralizingwithoutusingcounterfac- ousbaseline. Thetechniquesareevaluatedonthefollowing
tualinformation. Generalizationhelpsscalethealgorithmto domains.(a)Salpagents ,inspiredfrom[Suther- (b)Overcookedenvironment,inspiredfrom[Carroll (c) Warehouse environment, inspired
et al., 2019], shows 4 agents tasked with soup or- from [Papoudakis et al., 2021], shows
land and Madin, 2010; Sutherland and
Weihs, 2017], are tasked with collecting ders. The kitchen shown above has tomatoes , agents taskedwithprocessingshelves
physical samples from sites , , and onions , cooking pots , clean dishes , and ,atcounter . Thenarrowcorridors
depositing them at lab facility sur- a serving counter , dirty dishes , and garbage withhumanworkers thatareincon-
venienced around multiple robots carry-
roundedbycorals pronetodamage. bins forwaste.
ingshelves.
Figure2:Exampleprobleminstancesfromdomainsusedinourexperiments.
Sample Collection using Salps Salps are marine organ- status of its assigned task. Agents can move forward in all
ismsthatcanformchainsandcoordinatemovements[Suther- directions, and interact with objects in the kitchen. Inter-
land and Madin, 2010]. We consider a setting where salp- actions vary from picking and putting down ingredients and
inspired underwater robots must collect chemical samples dishes, to using the cooking pots and dumping garbage in
from different locations in the seabed. Each agent is tasked bins. Agent’smoveactionssucceedwithaprobabilityof0.8
withcollectingaspecifictypeofsampleanddepositingitto orslidetoanadjacentlocationwith0.2probability.Theinter-
adesignatedlocationforanalysis.Anagent’sstateisdenoted actactionssucceedwithprobabilityof0.8orfailandremain
by ⟨x,y,sample,coral,status⟩, where x,y denote agent’s inthesamestatewithprobability0.2. Featuresusedforgen-
location, sample indicates the sample type, coral indicates eralizingRi are⟨object,bin⟩. Wetestwithfiveinstancesof
N
presenceofcoralatx,y,andstatusisabinaryvariableindi- 15×15 grids that vary in the locations of the garbage bins.
catingifthesamplehasbeendepositedatthedestination. In NSE:Thegarbagebinsemitbadodorsandattractflies. Any
ourexperiments,samplescanbeoftypeA,BorXindicating object involved in soup preparation must therefore be kept
nosample. Agentscanmoveinallfourdirections,andpick awayfromthegarbagebinsorthewastemustbedisposedat
anddropsamples,eachwitharewardof−1. Therewardfor afartherbin,otherwisethefoodqualityisaffected.
task completion is +100. The move actions succeed with a
probabilityof0.8orfailandslidetoanadjacentlocationwith Warehouse Inventory Management In the multi-robot
a probability of 0.2. Features used for training when gener- warehouseenvironment [Papoudakisetal.,2021],agentsare
alizingRi are⟨sample,coral⟩. Wetestwithfiveinstances tasked with getting the requested shelves of different sizes
N
of 20×20 grids that vary in coral locations. This allows us (bigorsmall)thatwillbeprocessedatthecounterandmust
toevaluatethedifferentapproachestomitigateNSEs, while be returned to their location. We consider the setting where
keepingotherconditionsfixed. theserobotagentsareassignedspecificshelvesthattheyneed
NSE:Thesalp-likerobotsmayhavechemicalresiduesfloat- to locate, pick up, transport, process, and bring back, in or-
ing around them when transporting samples. A joint NSE der to complete their task. Each agent receives a reward
occurs when multiple robots carrying chemical samples are of +100 for returning its processed shelf, and −1 for each
inimmediatevicinityofcorals,potentiallydamagingit. step before completion. An agent’s state is represented as
⟨x,y,shelfsize,shelfstatus,corridor,done⟩, where x,y de-
Overcooked Inthisdomain,agentsaretaskedwithprepar- noteitslocation,shelfsizeisthesizeoftheshelftransported
ing and serving food [Carroll et al., 2019]. The agents bytheagentandcanbeoneofbig, small, orX fornoshelf.
worktogethertodeliverafixednumberoftomatoandonion shelf status denotes the current processing stage of the as-
soup orders, while keeping the kitchen clean. Each agent signed shelf which could be one of {picked up, processed,
is assigned a cleaning or a cooking task, and it receives delivered}, corridor denotes presence of a narrow corridor
a reward of +100 upon task completion. Actions in non- atagivenlocationx,y,anddoneindicatesthestatusoftask
goal state have a reward of −1. In each problem instance, completion. The corridors in the warehouse are structural
20% agents are assigned to cleaning and the rest are as- features unrelated to shelves and do not impact the robots’
signed the cooking task. An agent’s state is represented as delivery tasks. Robots can fulfill their tasks with or without
⟨x,y,dir,object,bin,status⟩, where x,y denote its loca- goingthroughacorridor;shelvesareorganizedseparatelyin
tion, dir denotes its orientation, bin indicates the presence another part of the warehouse. The robots can move in all
of garbage bins at x,y, and status indicates the completion directions and toggle load that allows them to pick up and(a)Salpdomain (b)Overcookeddomain (c)Warehousedomain
Figure3:AverageNSEpenaltyandstandarddeviationforvarying%ofagentsundergoingpolicyupdateineachdomainwith25agents.
dropshelves. Ashelfislabeledasprocessedifitisdropped 7 ResultsandDiscussion
at the counter. Agent’s move actions succeed with a proba-
Wecomparetheeffectivenessofdifferenttechniquesinmit-
bilityof0.8andslidetoanadjacentlocationwith0.2proba-
igating NSEs, along multiple dimensions such as the num-
bility. Thetoggleload actionsucceedswithaprobabilityof
berofagentsundergoingpolicyupdatetomitigatetheimpact
0.8orfailswithaprobabilityof0.2,inwhichcasetheagent
produced,NSEavoidancebygeneralizingRi tounseensit-
remains in the same state. Features used to generalize Ri N
N uations,andruntimeofthetechniques.
are ⟨shelfsize,shelfstatus,corridor⟩. We test with five in-
stances of 15×15 grids maintaining the same location for Effect of number of agents undergoing policy update
shelves and counters, but changing the location of corridors First, we evaluate the effectiveness of our approach in mit-
acrossthewarehouse. igatingNSEs,byvaryingthenumberofagentswhosepolicy
NSE: When multiple robots carrying shelves are simultane- mustbeupdatedafterRi iscalculated. Foreachdomain,we
N
ously in the same narrow corridor, it inconveniences human consider 25 agents in the environment and vary the percent-
workerstonavigateandaccessthearea.TheNSEpenaltyde- age of agents undergoing policy update to minimize NSEs,
pendsonshelfsizeandthenumberofagentscarryingshelves from 10% to 100%. We select agents to update policies by
inthesamecorridor. ranking them in the decreasing order of their blame values.
The results, in Figure 3, show naive policy as a straight line
NSEPenalty WeemployalogarithmicNSEpenaltyfunc-
in all the domains, since it is indifferent to NSEs. For the
tion(Equation5)forthedomainsdescribedearlier. TheNSE
salpsamplecollectiondomain,Figure3ashowsthattheNSE
penaltycalculatedas,
penalty plateaus at the 50% mark, where the difference re-
(cid:88) (cid:88) ward baseline shows its best performance against the naive
R (s)= β ·log(α N +1), (5)
N k d k policy. For the warehouse domain, in Figure 3c, we see a
d∈Fgdk∈d
similar trend of NSE penalty plateauing at the 50% mark
where F is the set of dynamic global features in the envi- however, difference reward fails to mitigate NSE and per-
gd
ronment,disafeatureinF ,andkisthevalueoffeatured. forms same as the naive policy. In case of the overcooked
gd
N isthenumberofagentswithdynamicglobalfeaturevalue domain, Figure 3b shows a steady decline of NSE penalty
k
d=kinthejointstates. β isascalingfactorthatisdirectly usingRECONanditsgeneralizedversionsbutsimilartothe
k
relatedtotheNSEpenaltyforaspecificfeaturevalued=k. warehousedomainresults,differencerewardfailstomitigate
Essentially,higherNSEpenaltiescorrespondtohighervalues NSEandperformssameasthenaivepolicy.
ofβ ,reflectingthatmitigatingmorecriticalfeaturesyieldsa Note that in some cases, NSE may not be avoided even
k
moresubstantialreductioninNSEpenalty,aligningwiththe when we update the policies of 100% of the agents in the
nuanced dynamics of real-world environments. α (> 0) is system. Thisisbecauseweprioritizecompletingthetaskop-
d
asensitivityparameterforNSEassociatedwithfeatured. A timallyoverminimizingNSEs. Henceinsomecases,itmay
largerα denotesthattheNSEpenaltyismoresensitivetoan beimpossiblefortheagentstoavoidNSEs,whileoptimally
d
increasingnumberofagentswithaspecificfeaturevalue. completing their tasks. This is a problem characteristic and
Weusealogarithmicfunctiontoeffectivelymodelscenar- notalimitationofRECON.
ioswheretheNSEimpactplateauswithacertainnumberof Overall, the results show that RECON is able to mitigate
agents performing undesirable actions [Gemp et al., 2022; NSEs, without updating the policies of a large number of
Zhu et al., 2018]. For example, there may be no significant agents. Based on these results, we update the policies of
differenceintheNSEimpactcausedby10agentsblockinga 50%ofagentsinallthefollowingexperiments,acrossalldo-
corridorversus11agentsblockingacorridor. Alogpenalty mains, since we consistently see a plateau and drop in the
willleadtoprioritizingthemitigationofsevereNSEs,byup- NSE penalty at 50%. For some settings, updating all the
dating the policy of a subset of agents. However, note that agents’policiesmaybeeffectivebutitisnotpracticallyfea-
RECONcanworkwithanypenaltyfunctioninpractice. sible.(a)Salpdomain (b)Overcookeddomain (c)Warehousedomain
Figure4:AverageNSEpenaltiesandstandarddeviations,averagedoverfiveprobleminstancesandvaryingnumberofagents.
(a)Salpdomain (b)Overcookeddomain (c)Warehousedomain
Figure5:Averageruntimeofalltechniques,withincreasingnumberofagentsacrossfiveprobleminstances,withstandarddeviation.
EffectofgeneralizingRi SincethepenaltyfunctionRi The considerate reward baseline takes the least time possi-
N N
is calculated based on the blame values corresponding to⃗π, ble because it involves solving only for one agent as other
policy updates may produce novel NSEs that were not en- agentsaretreatedaspartoftheenvironment. Differencere-
counteredearlier. Weevaluatetheeffectivenessofgeneraliz- wardbaselinetakeslongersincepolicycomputationisdone
ingRi tounseensituationsbycomparingtheperformances intwostagesbutitdoesnotevaluateallcounterfactualneigh-
N
of RECON, with and without generalization using counter- bors and defaults to the counterfactual producing maximum
factualdata,againstthebaselines. BasedonresultsfromFig- NSE penalty. RECON takes longer than difference reward
ure3,weconductexperimentswith50%ofagentsundergo- since it involves computing and comparing counterfactuals.
ing policy update for each technique. Figure 4 shows that As expected, RECON with R generalizations take longer
N
both the generalized versions of RECON outperform other thantheotherapproaches, sincetheygeneralizeNSEpenal-
methods,consistentlyreducingNSEpenaltyacrossdomains. tiesovertheentirestatespace. However,thetimeincreaseis
In few instances, generalized RECON with counterfactual small,comparedtothereductioninNSEpenaltiestheyoffer,
data outperforms the generalization without counterfactual eveninsettingswith100agents. SinceRECONisanoffline
data. Theseresultsshowthatgeneralizationisusefulandcan approach,theslightlyhigherruntimesforadditionalprocess-
mitigateNSEsconsiderablyevenwhencounterfactualdatais ingtogeneralizeR maybetolerableinmanysettings.Since
N
not used for training. We also tested the techniques when RECON is an offline approach, the slightly higher runtimes
100% of agents undergo policy update. We observe that the foradditionalprocessingtogeneralizeRi maybetolerable
N
NSE penalties further reduce by over 60%. While this may inmanysettings.
bepracticallyinfeasibleforlargesystems,itshowstheeffec-
tivenessofourapproachinmitigatingNSEs. 8 SummaryandFutureWork
Scalability We evaluate the scalability of our approach ThispaperformalizestheproblemofmitigatingNSEsinco-
intermsofrun-timetosolvetheproblemsasweincreasethe operativemulti-agentsettingsasadecentralized,bi-objective
numberofagentsinthesystemfrom10to100.Figure5plots problem. Theagents’assignedtasksfollowtransitionandre-
theruntimeofdifferenttechniques,inthethreedomains,with wardindependence. TheagentsproducenoNSEwhenoper-
varying number of agents. The run times increase in an ap- atinginisolationbuttheirjointactionsproduceNSEandin-
proximatelylinearmannerasthenumberofagentsincreases. curajointpenalty.Theagentshavenopriorknowledgeaboutthe side effects of their joint actions. We present a metarea- [Fiorettoetal.,2018] Ferdinando Fioretto, Enrico Pontelli,
soning approach to detect NSEs and update agent policies and William Yeoh. Distributed constraint optimization
in a decentralized manner, by decomposing the joint NSE problemsandapplications: Asurvey. JournalofArtificial
penalty into individual penalties. Our algorithm, RECON, IntelligenceResearch(JAIR),61:623–698,2018.
usesacounterfactual-basedblameattributiontoestimateeach [Gempetal.,2022] IanGemp,ThomasAnthony,JanosKra-
agent’s contribution towards the joint penalty. Our experi-
mar,TomEccles,AndreaTacchetti,andYoramBachrach.
mentsdemonstratetheeffectivenessofourapproachinmiti-
Designingall-payauctionsusingdeeplearningandmulti-
gatingNSEs,byupdatingthepoliciesofasubsetofagents.
agentsimulation. ScientificReports,12(1):16937,2022.
OurframeworkcurrentlysupportsDec-MDPswithtransi-
[GoldmanandZilberstein,2004] Claudia V. Goldman and
tion and reward independence. In the future, we aim to re-
ShlomoZilberstein. Decentralizedcontrolofcooperative
laxthisassumptionandextendourapproachtosettingswith
systems: Categorization and complexity analysis. Jour-
tightly coupled task assignment. Another interesting future
nalofArtificialIntelligenceResearch(JAIR),22:143–174,
direction is to exploit the agent dependencies and interac-
2004.
tions for task completion to mitigate NSEs, by leveraging
theircomplementaryskills. [Hadfield-Menelletal.,2017] Dylan Hadfield-Menell,
Smitha Milli, Pieter Abbeel, Stuart J. Russell, and Anca
Dragan. Inverse reward design. Advances in Neural
Acknowledgments
InformationProcessingSystems(NeurIPS),30,2017.
This work was supported in part by ONR grant number [Klassenetal.,2022] TorynQ.Klassen,SheilaA.McIlraith,
N00014-23-1-2171.
ChristianMuise,andJarvisXu. Planningtoavoidsideef-
fects. InProceedingsoftheAAAIConferenceonArtificial
References Intelligence(AAAI),volume36,pages9830–9839,2022.
[Krakovnaetal.,2018] Victoria Krakovna, Laurent Orseau,
[Alamdarietal.,2022] Parand A. Alamdari, Toryn Q.
RamanaKumar,MiljanMartic,andShaneLegg. Penaliz-
Klassen, Rodrigo T. Icarte, and Sheila A. McIlraith. Be
ingsideeffectsusingstepwiserelativereachability. 2018.
considerate: Avoiding negative side effects in reinforce-
ment learning. In Proceedings of the 21st International [Krakovnaetal.,2020] Victoria Krakovna, Laurent Orseau,
Conference on Autonomous Agents and Multiagent Sys- Richard Ngo, Miljan Martic, and Shane Legg. Avoiding
tems(AAMAS),pages18–26,2022. sideeffectsbyconsideringfuturetasks. AdvancesinNeu-
ralInformationProcessingSystems(NeurIPS),33:19064–
[Beckeretal.,2004] Raphen Becker, Shlomo Zilberstein,
19074,2020.
Victor Lesser, and Claudia V. Goldman. Solving transi-
tionindependentdecentralizedmarkovdecisionprocesses. [Meng,2012] Fan-YongMeng. Thecoreandshapleyfunc-
JournalofArtificialIntelligenceResearch(JAIR),22:423– tion for games on augmenting systems with a coalition
455,2004. structure. International Journal of Mathematical and
ComputationalSciences,6(8):813–818,2012.
[BilbaoandEdelman,2000] Jesus M. Bilbao and Paul H.
[Nguyenetal.,2018] Duc T. Nguyen, Akshat Kumar, and
Edelman. Theshapleyvalueonconvexgeometries. Dis-
HoongC.Lau.Creditassignmentforcollectivemultiagent
creteAppliedMathematics,103(1-3):33–40,2000.
RLwithglobalrewards. AdvancesinNeuralInformation
[CarlinandZilberstein,2012] Alan Carlin and Shlomo Zil- ProcessingSystems(NeurIPS),31,2018.
berstein. Boundedrationalityinmultiagentsystemsusing
[Papoudakisetal.,2021] Georgios Papoudakis, Filippos
decentralizedmetareasoning. pages1–28,2012.
Christianos, Lukas Scha¨fer, and Stefano V. Albrecht.
[Carrolletal.,2019] Micah Carroll, Rohin Shah, Mark K. Benchmarking multi-agent deep reinforcement learning
Ho,TomGriffiths,SanjitSeshia,PieterAbbeel,andAnca algorithms in cooperative tasks. In Proceedings of the
Dragan. On the utility of learning about humans for NeuralInformationProcessingSystemsTrackonDatasets
human-ai coordination. Advances in Neural Information andBenchmarks(NeurIPS),2021.
ProcessingSystems(NeurIPS),32,2019. [Pinedaetal.,2015] Luis Enrique Pineda, Kyle Hollins
[Choudhuryetal.,2024] Moumita Choudhury, Sandhya Wray,andShlomoZilberstein. Revisitingmulti-objective
Saisubramanian, Hao Zhang, and Shlomo Zilber- mdps with relaxed lexicographic preferences. In 2015
stein. Minimizing negative side effects in cooperative AAAIFallSymposiumSeries,2015.
multi-agent systems using distributed coordination. In [ProperandTumer,2012] Scott Proper and Kagan Tumer.
Proceedings of the 23rd International Conference on Modeling difference rewards for multiagent learning. In
Autonomous Agents and Multiagent Systems, pages InternationalConferenceonAutonomousAgentsandMul-
2213–2215,2024. tiAgentSystems(AAMAS),pages1397–1398,2012.
[D’Andrea,2012] Raffaello D’Andrea. A revolution in the [Rahmattalabietal.,2016] Aida Rahmattalabi, Jen Jen
warehouse: Aretrospectiveonkivasystemsandthegrand Chung, Mitchell Colby, and Kagan Tumer. D++: Struc-
challengesahead. IEEETransactionsonAutomationSci- tural credit assignment in tightly coupled multiagent
enceandEngineering,9(4):638–639,2012. domains. In2016IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pages4424–4429. [Zhuetal.,2018] MingZhu,Xiao-YangLiu,andXiaodong
IEEE,2016. Wang. Joint transportation and charging scheduling
in public vehicle systems—a game theoretic approach.
[Saisubramanianetal.,2020] SandhyaSaisubramanian,Ece
IEEETransactionsonIntelligentTransportationSystems,
Kamar, and Shlomo Zilberstein. A multi-objective ap-
19(8):2407–2419,2018.
proach to mitigate negative side effects. In Proceedings
oftheTwenty-NinthInternationalConferenceonInterna- [Zilberstein,2011] Shlomo Zilberstein. Metareasoning and
tional Joint Conferences on Artificial Intelligence (JAIR), bounded rationality. In Metareasoning: Thinking about
2020. Thinking,pages27–40.MITPress,Cambridge,MA,USA,
2011.
[Saisubramanianetal.,2022] SandhyaSaisubramanian,Ece
Kamar, and Shlomo Zilberstein. Avoiding negative side
effects of autonomous systems in the open world. Jour-
nalofArtificialIntelligenceResearch(JAIR),74:143–177,
2022.
[Shapley,1953] Lloyd S. Shapley. A value for n-person
games. 1953.
[SundararajanandNajmi,2020] Mukund Sundararajan and
Amir Najmi. The many shapley values for model expla-
nation. InInternationalConferenceonMachineLearning
(ICML),pages9269–9278.PMLR,2020.
[SutherlandandMadin,2010] KellyR.SutherlandandLau-
renceP.Madin.Comparativejetwakestructureandswim-
mingperformanceofsalps. JournalofExperimentalBiol-
ogy,213(17):2967–2975,2010.
[SutherlandandWeihs,2017] Kelly R. Sutherland and
Daniel Weihs. Hydrodynamic advantages of swimming
by salp chains. Journal of The Royal Society Interface,
14(133):20170298,2017.
[Svegliatoetal.,2022] Justin Svegliato, Connor Basich,
SandhyaSaisubramanian,andShlomoZilberstein. Metar-
easoningforsafedecisionmakinginautonomoussystems.
In International Conference on Robotics and Automation
(ICRA),pages11073–11079.IEEE,2022.
[Turneretal.,2020] Alexander M. Turner, Dylan Hadfield-
Menell, and Prasad Tadepalli. Conservative agency via
attainable utility preservation. In Proceedings of the
AAAI/ACM Conference on AI, Ethics, and Society, pages
385–391,2020.
[Wrayetal.,2015] Kyle Wray, Shlomo Zilberstein, and
Abdel-Illah Mouaddib. Multi-objective MDPs with con-
ditionallexicographicrewardpreferences. InProceedings
of the AAAI Conference on Artificial Intelligence (AAAI),
volume29,2015.
[ZerbelandTumer,2023] Nicholas Zerbel and Kagan
Tumer. Counterfactualfocusedlearning. 2023.
[Zhangetal.,2018] Shun Zhang, Edmund H. Durfee, and
Satinder Singh. Minimax-regret querying on side effects
forsafeoptimalityinfactoredmarkovdecisionprocesses.
In Proceedings of the Twenty-Seventh International Joint
ConferenceonArtificialIntelligence(IJCAI),2018.
[Zhangetal.,2020] Shun Zhang, Edmund Durfee, and
Satinder Singh. Querying to find a safe policy under un-
certainsafetyconstraintsinmarkovdecisionprocesses. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence(AAAI),volume34,pages2552–2559,2020.