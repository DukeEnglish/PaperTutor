Robust deep learning from weakly dependent data
May 9, 2024
William Kengne and Modou Wade
Institut Camille Jordan, Universit´e Jean Monnet, 23 Rue Dr Paul Michelon 42023 Saint-E´tienne Cedex 2,
France
THEMA, CY Cergy Paris Universit´e, 33 Boulevard du Port, 95011 Cergy-Pontoise Cedex, France
E-mail: william.kengne@univ-st-etienne.fr ; modou.wade@cyu.fr
Abstract: Recent developments on deep learning established some theoretical properties of deep neural
networksestimators. However,mostoftheexistingworksonthistopicarerestrictedtoboundedlossfunctions
or (sub)-Gaussian or bounded input. This paper considers robust deep learning from weakly dependent
observations, with unbounded loss function and unbounded input/output. It is only assumed that the output
variable has a finite r order moment, with r > 1. Non asymptotic bounds for the expected excess risk of the
deep neural network estimator are established under strong mixing, and ψ-weak dependence assumptions on
the observations. We derive a relationship between these bounds and r, and when the data have moments
of any order (that is r = ∞), the convergence rate is close to some well-known results. When the target
predictor belongs to the class of H¨older smooth functions with sufficiently large smoothness index, the rate of
the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained
with i.i.d. samples. Application to robust nonparametric regression and robust nonparametric autoregression
are considered. The simulation study for models with heavy-tailed errors shows that, robust estimators with
absolute loss and Huber loss function outperform the least squares method.
Keywords: Deepneuralnetworks,robustestimator,ψ-weaklydependent,α-mixing,excessrisk,convergence
rate.
1 Introduction
Inthelastfewyears,severalsignificantworkshavecontributedtounderstandthetheoreticalpropertiesofdeep
neural networks (DNN) estimators. It has been shown that, the excess risk of the DNN predictors obtained
from the empirical risk minimization can achieve an optimal convergence rate for regression and classification
tasks. There is a vast literature that focuses on independent and identical distribution (i.i.d.) observations,
see, for instance [25], [1], [27], [28], [30], [26], [12], [13], and the references therein. We refer to [17], [16], [14],
[21], [23], [20], [3] for some results on DNNs estimators, obtained with dependent or non-i.i.d. data. Despite
thesignificantadvancesmadeindeeplearningtheory,mostresults,especiallyinregressionproblem,havebeen
obtainedwithleastsquareslossfunction. Also, theaforesaidworksarerestrictedtoboundedlossfunctionsor
1
4202
yaM
8
]LM.tats[
1v18050.5042:viXra2 Robust deep learning from weakly dependent data
(sub)-Gaussian or (sub)-exponential or bounded input/output. Some works in the literature perform robust
deep learning, with theoretical evidence on such predictors; see, among other papers, [29], [22], [33], [31], [18],
[32]. However, these works focus on i.i.d. observations.
In this contribution, we consider the observations D := {(X ,Y ),··· ,(X ,Y )} (the training sample)
n 1 1 n n
generated from a stationary and ergodic process {Z = (X ,Y ),t ∈ Z}, which takes values in Z = X ×Y,
t t t
where X ⊂ Rd (with d ∈ N) is the input space and Y ⊂ R the output space. A broad class of loss function
ℓ:R×Y →[0,∞) is considered in the sequel. For a predictor h∈F :=F(X,Y) (where F(X,Y) denotes the
set of measurable functions from X to Y), its risk is defined by,
R(h)=E [ℓ(h(X ),Y )] with Z =(X ,Y ). (1.1)
Z0 0 0 0 0 0
We aim to construct a predictor h ∈ F such that, h(X ) is average ”close” to Y , for all t ∈ Z, that is,
t t
the learner which achieves the smallest risk. But, the risk defined above (in (1.1)) cannot be minimized in
practice, because the distribution of Z =(X ,Y ) is unknown. Since this risk can be effectively estimated by
0 0 0
its empirical version, we consider the minimization of the empirical risk, define for all h∈F by,
n
1 (cid:88)
R(cid:98)n(h)=
n
ℓ(h(X i),Y i). (1.2)
i=1
A target predictor (assumed to exist) h∗ ∈F, satisfies
R(h∗)= infR(h). (1.3)
h∈F
The excess risk for any predictor h∈F is given by,
E (h)=R(h)−R(h∗), with Z =(X ,Y ). (1.4)
Z0 0 0 0
The goal is to construct from the observation D n, a DNN predictor (cid:98)h
n
∈ H σ(L n,N n,B n,F n,S n), with a
suitable chosen network architecture (L ,N ,B ,F ,S ) where H (L ,N ,B ,F ,S ) is a class of deep
n n n n n σ n n n n n
neural network predictors defined in Section 2.2 (see (2.10)), such that its excess risk is average ”close” to
zero. The DNN predictor (cid:98)h
n
obtained from the empirical risk minimization (ERM) algorithm over the class
H (L ,N ,B ,F ,S ) is defined by,
σ n n n n n
(cid:98)h
n
= argmin R(cid:98)n(h). (1.5)
h∈Hσ(Ln,Nn,Bn,Fn,Sn)
Inthesequel,wesetℓ(h,z)=ℓ(h(x),y)forallz =(x,y)∈Z =X×Y andh∈F(X,Y). Wewillbeinterested
in establishing a bound of the excess risk E Z0((cid:98)h n), and studying how fast it tends to zero.
Robustnessisknownasanimportantpropertyofanestimator. Thisrobustnesspropertyincludestheability
ofaDNNestimatortoberesistanttoadversarialattacks,suchas,aperturbationintheinputdata. Mostofthe
existingresultsonDNNestimatordonotdisplaythisrobustnessproperty;andarenotapplicable,forinstance,
to problems with heavy-tailed data. The literature on robust deep learning is not vast. Robust deep Learning
has been performed by [22], with Lipschitz-continuous loss functions, in a general class of feedforward neural
networks. [29] consider robust nonparametric estimation with heavy tailed error distributions and establish
a non-asymptotic error bounds for a class DNN estimator with the ReLU (rectified linear unit) activation
function. Robustestimationforthelocationfunctionfrommulti-dimensionalfunctionaldatahasbeencarriedKengne and Wade 3
outby[31]. TheyproposeDNNestimatorswithReLUactivationfunctionandanabsolutelycontinuousconvex
loss function. [33] investigate the generalization error of deep convolutional neural networks with the ReLU
activation, for robust regression problems within an information-theoretic learning framework. We also refer
to [18], [32] for some recent advances on theoretical guarantees of robust DNN. However, these results are
restricted to i.i.d. data. Moreover, theoretical results on robust deep learning from dependent observations
are quite scarce.
This new work focuses on robust deep learning from a weakly dependent process {Z = (X ,Y ),t ∈ Z}
t t t
(which takes values in Z = X × Y ⊆ Rd × R), based on a training sample D = (X ,Y ),··· ,(X ,Y )
n 1 1 n n
and a general loss function ℓ : R×Y → [0,∞). The framework considered includes, regression estimation,
classification, times series prediction. Our main contributions are:
(cid:0) (cid:1)
1. For unbounded weakly dependent process (X ,Y ) , and unbounded loss function ℓ, assumed to
t t t∈Z
be Lipschitz continuous, we establish non asymptotic bounds for the expected excess risk of the DNN
estimators, based on a suitable chosen architecture L ,N ,B ,F ,S of the neural networks class. The
n n n n n
output process (Y t) t∈Z can be heavy-tailed with a finite r-th moment for some r > 1. A relationship
between the convergence rate of the excess risk and r is derived, and when the output process has
momentsofanyorder(thatisr =∞),thisrateisclosetosomewell-knownresults. Theneuralnetworks
class is performed with a broad class of activation functions.
• Forstrongmixingprocess,therateoftheexcessriskisO(cid:16) (cid:0) logn(α)(cid:1)3(cid:0) n(α)(cid:1)− s+s d(1− r1)(cid:17) ,wheren(α)
is given in (3.2) and s is the smoothness parameter of the H¨older space of the target function (see
Section 3). Thus, when the output variable has moments of any order (that is r = ∞) and the
(cid:16) (cid:17)
observations are i.i.d, this rate is O n− s+s d(logn)3 .
• For ψ-weakly dependent observations, the rate derived also depends on r and it is close or ”better”
than those obtained by [16], [15] (by using a penalized regularization procedure), when the output
data have moments of any order.
2. We carry out nonparametric regression for strong mixing processes with the Huber and the L loss
1
functions. The model considered takes into account heavy-tailed errors such as: the t(2) (Student’s t
distribution with 2 degrees of freedom), the Cauchy(0,1) (Standard Cauchy distribution with location
parameter 0 and scale parameter 1). The results established are applicable to such models. Simulation
study is performed for nonlinear autoregressive models with t(2) and N(0,1) errors. Comparison to the
least squares method shows that, the robust estimator based on the Huber and the L loss functions
1
outperform the estimator based on the L loss. This performance of the robust estimator is quite
2
significant compared to the least squares one, for heavy-tailed models.
The paper is structured as follows. Section 2 presents some notations, the weak dependence structure
and the class of DNN predictor considered. The excess risk bounds of the DNN estimators are provided in
Section 3. Application to the robust nonparametric regression is carried out in Section 4. We conducted some
numerical experiments in Section 5 and Section 6 is devoted to the proofs of the main results.4 Robust deep learning from weakly dependent data
2 Notations, assumptions and feedforward neural networks
2.1 Notations and assumptions
Letd∈N,E andE besubsetsofseparableBanachspacesequippedwithnorms∥·∥ and∥·∥ respectively.
1 2 E1 E2
Throughout the sequel, we use the following notations.
• F(E ,E ) denotes the set of measurable functions from E to E .
1 2 1 2
• For any h∈F(E ,E ),ϵ>0, B(h,ϵ) is the ball of radius ϵ of F(E ,E ) centered at h, that is,
1 2 1 2
(cid:8) (cid:9)
B(h,ϵ)= f ∈F(E ,E ), ∥f −h∥ ≤ϵ ,
1 2 ∞
where ∥·∥ denotes the sup-norm defined in (2.2).
∞
• Foranyϵ>0, theϵ-coveringnumberN(H,ϵ)ofH⊂F(E ,E ), representstheminimalnumberofballs
1 2
of radius ϵ needed to cover H; that is,
(cid:110) (cid:91)m (cid:111)
N(H,ϵ)=inf m≥1 :∃h ,··· ,h ∈H such that H⊂ B(h ,ϵ) . (2.1)
1 m i
i=1
• We set,
∥h∥ = sup ∥h(x)∥ , ∥h∥ = sup∥h(x)∥ , (2.2)
∞ E2 ∞,U E2
x∈E1 x∈U
and
∥h(x )−h(x )∥
Lip (h):= sup 1 2 E2 for any α∈[0,1], (2.3)
α ∥x −x ∥α
x1,x2∈E1, x1̸=x2 1 2 E1
for any function h:E →E and U ⊆E .
1 2 1
• For any K > 0 and α ∈ [0,1], Λ (E ,E ) (or simply Λ (E ) when E ⊆ R) is defined as the set of
α,K 1 2 α,K 1 2
functions h : Eu → E for some u ∈ N, satisfies ∥h∥ < ∞ and Lip (h) ≤ K. When α = 1, we set
1 2 ∞ α
Lip (h)=Lip(h) and Λ (E )=Λ (E ,R).
1 1 1 1,1 1
• For any x∈Rd, xT denotes the transpose of x.
• For any x∈R, ⌊x⌋ denotes the greatest integer less than or equal to x, and ⌈x⌉ the least integer greater
than or equal to x.
We will consider the following definition in the sequel, see also [25], [26].
Definition 2.1. Let a function g :R→R.
1. g is continuous piecewise linear (or ”piecewise linear” for notational simplicity) if it is continuous and
there exists K (K ∈ N) break points a ,··· ,a ∈ R with a ≤ a ≤ ··· ≤ a such that, for any
1 K 1 2 K
k =1,··· ,K, g′(a −)̸=g′(a +) and g is linear on (−∞,a ],[a ,a ],···[a ,∞).
k k 1 1 2 K
2. g is locally quadratic if there exits an interval (a,b) on which g is three times continuously differentiable
with bounded derivatives and there exists t∈(a,b) such that g′(t)̸=0 and g′′(t)̸=0.
Wesetthefollowingassumptionsontheprocess{Z =(X ,Y ),t∈Z}withvaluesinZ =X×Y ⊂Rd×R,
t t t
the loss function ℓ:R×Y →[0,∞), and the activation function σ :R→R.Kengne and Wade 5
(A1) : There exists a constant C > 0 such that the activation function σ is C -Lipschitzian. That is,
σ σ
Lip(σ) ≤ C . Moreover, σ is either piecewise linear or locally quadratic and fixes a non empty interior
σ
segment I ⊆[0,1] (i.e. σ(z)=z for all z ∈I).
(A2) : There exists K >0 such that, the loss function ℓ∈Λ (R×Y).
ℓ 1,Kℓ
Assumption (A1) is satisfied by the ReLU (rectified linear units) activation: σ(x) = max(x,0), and several
otheractivationfunctions,see[14]. TheL andtheHuberlosswithparameterδ >0satisfyAssumption(A2)
1
with K =1 and K =δ respectively. For all y ∈R and y′ ∈Y, recall:
ℓ ℓ
• The L loss function: ℓ(y,y′)=|y−y′|;
1
ß 1(y−y′)2 if |y−y′|≤δ,
• The Huber loss with parameter δ >0: ℓ(y,y′)= 2
δ|y−y′|− 1δ2 otherwise.
2
We consider a separable Banach space E. Let us define the weak dependence structure, see [8] and [5].
Definition 2.2. An E-valued process (Z t) t∈Z is said to be (Λ 1(E),ψ,ϵ)-weakly dependent if there exists a
function ψ : [0,∞)2 ×N2 → [0,∞) and a sequence ϵ = (ϵ(r)) r∈N decreasing to zero at infinity such that for
any g , g ∈ Λ (E), with g : Eu → R, g : Ev → R, (u,v ∈ N) and for any u-tuple (s ,··· ,s ) and any
1 2 1 1 2 1 u
v-tuple (t ,··· ,t ) with s ≤···≤s ≤s +r ≤t ≤···≤t , the following inequality is fulfilled:
1 v 1 u u 1 v
|Cov(g (Z ,··· ,Z ),g (Z ,··· ,Z ))|≤ψ(Lip(g ),Lip(g ),u,v)ϵ(r),
1 s1 su 2 t1 tv 1 2
where Cov denotes the covariance.
Some examples of well-known weak dependence conditions.
• ψ(Lip(g ),Lip(g ),u,v)=vLip(g ): the θ-weak dependence, then denote ϵ(r)=θ(r);
1 2 2
• ψ(Lip(g ),Lip(g ),u,v)=uLip(g )+vLip(g ): the η-weak dependence, then denote ϵ(r)=η(r);
1 2 1 2
• ψ(Lip(g ),Lip(g ),u,v)=uvLip(g )·Lip(g ): the κ- weak dependence, then denote ϵ(r)=κ(r);
1 2 1 2
• ψ(Lip(g ),Lip(g ),u,v)=uLip(g )+vLip(g )+uvLip(g )·Lip(g ): theλ-weakdependence,thendenote
1 2 1 2 1 2
ϵ(r)=λ(r).
In the sequel, for each of the four choices of ψ above, set respectively,
Ψ(u,v)=2v, Ψ(u,v)=u+v, Ψ(u,v)=uv, Ψ(u,v)=(u+v+uv)/2. (2.4)
Let us set two weak dependence conditions.
(A3) : Let Ψ : [0,∞)2 ×N2 → [0,∞) be one of the choices in (2.4). The process {Z =(X ,Y ),t∈Z} is
t t t
stationary ergodic and (Λ (Z),ψ,ϵ)-weakly dependent such that, there exists L , L , µ≥0 satisfying
1 1 2
(cid:88) (j+1)kϵ(j)≤L Lk(k!)µ for all k ≥0. (2.5)
1 2
j≥06 Robust deep learning from weakly dependent data
(A4) The process {Z =(X ,Y ),t∈Z} is stationary ergodic, and strong mixing with the mixing coefficients
t t t
satisfying for all j ≥0,
α(j)=αexp(−cjγ), for some α>0,γ >0,c>0. (2.6)
(A5) There exists a positive constant M >0 such that E[|Y |r]≤M <∞, for some r >1.
0
Many classical autoregressive processes satisfy (A3) and (A4), see for instance [9], [6] and Section 4 below.
2.2 Feedforward neural networks
A neural network function h with network architecture (L,p), where L denotes the number of hidden layers
or depth and p=(p ,p ,··· ,p )∈NL+2 called width vector is a composition of functions given as follows,
0 1 L+1
h:Rp0 →RpL+1, x(cid:55)→h(x)=A ◦σ ◦A ◦σ ◦···◦σ ◦A (x), (2.7)
L+1 L L L−1 1 1
where A
k
: Rpk−1 → Rpk (k = 1,··· ,L+1) is a linear affine map, defined by A k(x) := W kx+b k, for given
p k−1×p
k
weightmatrixW
k
andashiftvectorb
k
∈Rpk,σ
k
:Rpk →Rpk isanonlinearelement-wiseactivation
map,definedforallz =(z ,··· ,z )byσ (z)=(σ(z ),··· ,σ(z ))T andσ :R→Risanactivationfunction.
1 pk k 1 pk
Let θ(h) denote the vector of parameters of a DNN of the form (2.7), that is,
Ä äT
θ(h):= vec(W )T,bT,··· ,vec(W )T,bT , (2.8)
1 1 L+1 L+1
where vec(W) denotes the vector obtained by concatenating the column vectors of the matrix W. In our
setting here, p = d and p = 1. In the sequel, we deal with an activation function σ : R → R and denote
0 L+1
by H , the set of DNNs predictors with d-dimensional input and 1-dimensional output. For a DNN h of
σ,d,1
the form (2.7), set depth(h)=L, and width(h) = max p which represents the maximum width (the number
j
1≤j≤L
of neurons) of hidden layers. For any constants L,N,B,F >0, set
(cid:8) (cid:9)
H (L,N,B):= h∈H :depth(h)≤L,width(h)≤N,∥θ(h)∥ ≤B ,
σ σ,q,1 ∞
and
(cid:8) (cid:9)
H (L,N,B,F):= h:h∈H (L,N,B),∥h∥ ≤F . (2.9)
σ σ ∞,X
The class of sparsity constrained DNNs with sparsity level S >0 is defined by,
H (L,N,B,F,S):={h∈H (L,N,B,F) : ∥θ(h)∥ ≤S}, (2.10)
σ σ 0
where ∥x∥ = (cid:80)p 1 (x ̸= 0), ∥x∥ = max|x | for all x = (x ,...,x )T ∈ Rp (p ∈ N). Let us note that, the
0 i=1 i i 1 p
1≤i≤p
network architecture parameters L,N,B,F,S can depend on the sample size n, see Section 3 below.
3 Excess risk bound for the DNN estimator
This section aims to establish an upper bound for the excess risk of the DNN estimator for learning from the
strongmixingandtheψ-weaklydependentprocesses. Weprovidetheconvergencerateoftheexcesswhenthe
target predictor h∗ defined at (see (1.3)) belongs to the set of H¨older functions.Kengne and Wade 7
Let s>0 and U ⊂Rd. Recall that, the H¨older space Cs(U) is a class of functions h:U →R such that, for
any α ∈ Nd with |α| ≤ [s],∥∂αh∥ < ∞, and for any α ∈ Nd with |α| = [s],Lip (∂αh) < ∞, ([x] denotes
∞ s−[s]
the integer part of x∈R), where
(cid:88)d ∂|α|
α=(α ,··· ,α ), |α|= α and ∂α = .
1 d i ∂α1x 1,··· ,∂αdx
d
i=1
This space is equipped with the norm:
(cid:88) (cid:88)
∥h∥ = ∥∂α(h)∥ + Lip (∂αh).
Cs(U) ∞ s−[s]
0≤α≤[s] |α|=[s]
For any s>0,U ⊂Rd and K>0, set
Cs,K(U)={h∈Cs(U),∥h∥ ≤K}.
Cs(U)
Theorem 3.1. Assume that (A1), (A2), (A4), (A5) hold and that h∗ ∈ Cs,K(X) for some s,K > 0,
(cid:0) (cid:1)
where h∗ is defined in (1.3). Set L = (cid:0) 1 − 1(cid:1) sL 0 log(cid:0) n(α)(cid:1) , N = N (cid:0) n(α)(cid:1) 1− r1 s+d d, S = (cid:0) 1 −
n r s+d n 0 n
1 r(cid:1) ss +S0 d(n(α))(cid:0) 1− r1(cid:1) s+d d log(cid:0) n(α)(cid:1) ,B n =B 0(cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) andF n >K,forsomepositiveconstantsL 0,N 0,S 0,B 0 >
0. ConsidertheclassofDNNH (L ,N ,B ,F ,S )definedin(2.10). Then,thereexistsn =n (K ,L ,N ,B ,S ,C ,
σ n n n n n 0 0 ℓ 0 0 0 0 σ
s,d,r,c,γ)∈N defined at (6.31), such that for all n≥n 0, the DNN estimator (cid:98)h
n
defined in (1.5) satisfies
(cid:0) logn(α)(cid:1)ν
+K C(K ,α,M) 3K
E[R((cid:98)h n)−R(h∗)]≤
(cid:0) (cid:1) s
(1−1)ℓ +
(cid:0)
ℓ
(cid:1)(1−1)
+ n(αℓ ), (3.1)
n(α) s+d r n(α) r
for all ν >3, with,
64
n(α) =⌊n⌈{8n/c}1/(γ+1)⌉−1⌋ and C(K ,α,M)= K (1+4e−2α)+6K M, (3.2)
ℓ 3 ℓ ℓ
where c,γ,α are given in (2.6), K in the assumption (A2) and r,M in the assumption (A5).
ℓ
Therefore, the rate of the expected excess risk in the bound (3.1) is
O(cid:16) (cid:0) logn(α)(cid:1)3(cid:0) n(α)(cid:1)− s+s d(1− r1)(cid:17)
.
Remark 3.2.
1. For n≥max(c/8,22+5/γc1/γ), we have n(α) ≥C(γ,c)nγ/(γ+1), with C(γ,c)=2−2 γγ ++ 15 cγ+1 1, see also [11].
(cid:16) (cid:17)
So, the rate in (3.1) is O n− s+s d(1− r1) γ+γ 1(logn)3 .
2. Therefore, for of i.i.d data (that is γ =∞), this rate is close (up to logarithmic factor) to that obtained
in some specific case in [29] (see Corollary 2).
3. If the output variable has moments of any order (that is r = ∞) and the observations are i.i.d, then,
(cid:16) (cid:17)
the rate above is O n− s+s d(logn)3 . Recall that, the rate obtained in the nonparametric regression with
(cid:16) (cid:17)
square loss is O n− 2s2 +s d(logn)3 , see for instance [28].
In the sequel, we consider an architecture L ,N ,B ,F ,S as in Theorem 3.1 and we let β ≥ F . Set
n n n n n n n
23+µ
C =32K2β2Ψ(1,1)L and C =8K β L max( ,1), where Ψ is one of the functions defined at (see
n,1 ℓ n 1 n,2 ℓ n 2 Ψ(1,1)
(2.4)).8 Robust deep learning from weakly dependent data
(cid:110)
Theorem3.3. Assume(A1)-(A3),(A5)andh∗ ∈Cs,K(X)forsomeK>0ands>max d(cid:0)(r−1)(2µ+3)(µ+2)−
r(2µ+3)−(µ+1)
1(cid:1) ,d(cid:0)(cid:0) 1−1 r(cid:1) (2µ+3)−1(cid:1)(cid:111) . LetL n =(cid:0) 1−1 r(cid:1) ss +L 0 dlog(n),N n =N 0n(cid:0) 1− r1(cid:1) s+d d,S n =(cid:0) 1−1 r(cid:1) ss +S0 dn(cid:0) 1− r1(cid:1) s+d d log(n),B n =
(cid:0) (cid:1)
B 0n 1− r1 4s(d s+/s d+1) ,andF
n
≤nr(µ 2µ+ +1 3),forsomeL 0,N 0,B 0,S
0
>0. ConsidertheclassofDNNH σ(L n,N n,B n,F n,S n)
defined in (2.10). Then, there exists n = n (K ,L ,N ,B ,S ,C , s,d,r,µ) ∈ N, such that for all n ≥ n ,
0 0 ℓ 0 0 0 0 σ 0
the DNN estimator (cid:98)h
n
defined in (1.5) satisfies,
C(µ,K ,L ,L ,M) 3K 2K
E[R((cid:98)h n)−R(h∗)]≤
n(cid:0)
1−1/rℓ
(cid:1)
(µ+1 1)/(2
2µ+3)
+ nℓ +
ns+s
d(1−ℓ 1/r), (3.3)
for some positive constant C(µ,K ,L ,L ,M) depending on µ,K ,L ,L ,M, where µ,L ,L are given in the
ℓ 1 2 ℓ 1 2 1 2
assumption (A3), K in (A2) and M in (A5).
ℓ
Remark 3.4. For some classical models such as AR(p), ARCH(p), assumption (A3) holds with µ = 0. In
this case, if s ≫ d, then the convergence rate in (3.3) is close to
O(cid:0) n−(1−1/r)/3(cid:1)
. If the output variable has
moments of any order (that is r =∞), this rate is close to O(n−1/3).
4 Robust nonparametric regression
In this section we deal with the robust nonparametric regression, where the output Y ∈ R and the input
t
X ∈X ⊆Rd generated from the model:
t
Y =h∗(X )+ϵ . (4.1)
t t t
Where h∗ : X → R is an unknown regression function, ϵ is an error variable independent to X . We assume
t t
that the error has a finite r-th moment for some r > 1, that is E|ϵ |r < ∞. Let us check if the model (4.1)
t
satisfies the assumptions of Theorem 3.1.
• We deal with the ReLU activation function, thus the assumption (A1) holds.
• We consider the L and the Huber (with parameter δ =1.345) loss functions, so, the assumption (A2)
1
is satisfied.
• It is assumed that h∗ :X →R is a smooth function, that is, h∗ ∈Cs,K(X) for some s,K>0.
(cid:0) (cid:1)
• We assume that the process Z t = (X t,Y t) t∈Z is (Λ 1(Z),ψ,ϵ)-weakly dependent where ϵ = ϵ(j) j∈N is
such that (2.5) holds for some L , L , µ ≥ 0 or is strong mixing with α-mixing coefficients defined by
1 2
α (j)=αexp(−cjγ),j ≥1,α>0,γ >0,c>0. That is, (A3) or (A4) holds.
Z
• IfX isacompactsetofRd,then,onecaneasilyseethat(A5)isverifiedwhen(i)ϵ ∼N(0,1)denotedthe
t
standard normal distribution, E|ϵ |r < ∞,r ∈ [0,∞[; (ii) ϵ ∼ t(2) denoted the Student’s t distribution
t t
with 2 degrees of freedom, E|ϵ |r < ∞,r ∈ [0,2); (iii) ϵ ∼ Cauchy(0,1) denoted the Standard Cauchy
t t
distribution with location parameter 0 and scale parameter 1, E|ϵ |r <∞,r ∈[0,1]. Hence, the result of
t
the Theorem 3.1 can be applied to the model (4.1).Kengne and Wade 9
Application to autoregressive models
We consider the nonlinear autoregressive model
Y =f(Y ,··· ,Y )+ϵ , (4.2)
t t−1 t−p t
for some measurable unknown regression function f : Rp → R (p ∈ N) and (ϵ t) t∈Z is an i.i.d. process. Set
X = (Y ,··· ,Y ), one can see that the model (4.2) is a particular case of the model (4.1). Let us check
t t−1 t−p
assumptions (A3) or (A4), and (A5). For this purpose, consider the following assumptions:
A(ϵ): The sequence (ϵ ) has positive and continuous density function everywhere, E[ϵ ]=0, and ϵ is indepen-
t t t
dent of X , s>0;
t−s
A (f): The function f is bounded on every compact of Rp, that is, for all K ≥0,
1
sup |f(x)|<∞,
∥x∥≤K
where ∥·∥ denotes the Euclidean norm on Rp;
A (f): There exist constants α ≥0, i=1,··· ,p,M >0,c >0 such that
2 i 1
p
(cid:88)
|f(x)|≤ α |x |+c , ∥x∥≥M. (4.3)
i i 1
i=1
.
Under the assumptions A(ϵ), A (f), A (f), if the following condition holds
1 2
p
(cid:88)
α <1. (4.4)
i
i=1
One can easily see in [4], [7], [2], there exists a stationary solution (Y t,X t) t∈Z of the model (4.2) which is
geometrically strong mixing. Thus, the assumption (A4) holds. Also, under the condition (4.4), the process
(Y t,X t) t∈Z solution of (4.2), is θ-weakly dependent such that (A3) holds with µ=2, see [6] and [16]. Hence,
the results of the Theorem 3.1 and Theorem 3.3 can be applied to the model (4.2).
5 Numerical results
In this section, we carry out some numerical experiments to assess the performance of DNNs predictors for
the estimation of nonlinear autoregressive time series.
Let (Y ,X ),··· ,(Y ,X ) be a trajectory of the process {(X ,Y ),t∈Z} in (4.2). We will focus to the
1 1 n n t t
prediction of Y from this training sample. We perform the learning theory with DNNs estimators develop
n+1
above, with the input variable X =(Y ,··· ,Y ), the input space X ⊂Rp, and the output space Y ⊂R,
t t−1 t−p
where p=3,2 in DGP1 and DGP2 respectively (see below). The following models are specific cases of (4.2):
DGP1:f(Y ,Y ,Y ) =0.5−0.5max(Y ,0)+0.2min(Y ,0)+0.15Y +ϵ ,
t−1 t−2 t−3 t−1 t−1 t−3 t
(cid:16) (cid:17) (cid:16) (cid:17)
DGP2:f(Y t−1,Y t−2) =0.75+ 0.8−0.2e−Y t2 −1 Y t−1+ −0.2+0.3e−Y t2 −1 Y t−2+ϵ t,10 Robust deep learning from weakly dependent data
where(ϵ t) t∈Z representsaninnovationgeneratedrespectivelyfromtheStudent’sdistributionwith2degreesof
freedom, denoted by ϵ ∼ t(2) and the standard normal distribution denoted by ϵ ∼ N(0,1) for each DGPs,
t t
with s = 1,2. DGP1 is a threshold autoregressive model, whereas DGP2 is an exponential autoregression
model.
We deal with the L , Huber and the L loss functions. The target function with respect to the L loss is
2 1 2
given for all x∈X by:
h∗(x)=E[Y |X =x]=f(x). (5.1)
0 0
Since the distribution of (ϵ t) t∈Z is symmetric around 0, the target predictor with respect to the Huber and L 1
loss functions (see also [10], [29]) are given respectively for all x∈X by:
h∗(x)=f(x), (5.2)
and,
h∗(x)=med(Y |X =x)=f(x), (5.3)
0 0
where med denotes the median.
For each of these DGPs, we perform a network architecture of 2 hidden layers with 100 hidden nodes for
each layer. The ReLU and the linear activation function are respectively used in the hidden layer and the
output layer. The optimization algorithm Adam (see cite ([19])) with learning rate 10−3 is used to train the
network’s weights and the minibatch size of 32. For each loss, we stopped the training when the empirical
error of the DNN estimator is not improved over 30 epochs.
For n=250,500, and 1000, we generated a trajectory ((Y ,X ),··· ,(Y ,X )) from the true DGP and the
1 1 n n
predictor (cid:98)h
n
is obtained from (1.5). The empirical L 1, Huber, L
2
excess risk of (cid:98)h
n
is evaluated from a new
trajectory ((Y′,X′),··· ,(Y′ ,X′ )) with m=104. Figure 1 and Figure 2 show the boxplots of the empirical
1 1 m m
L , Huber and L excess risk of the DNN estimator based on 100 replications. For each DGPs (s=1,2), one
1 2
can see that, the empirical excess risk decreases when the sample size n increases. These numerical findings
with the L and Huber loss are in accordance with Theorem 3.1 and Theorem 3.3.
1
To compare the performance of the estimators based on the L , Huber and L loss functions, we consider
1 2
the mean absolute prediction error (MAPE) and the root mean square prediction error (RMSPE). For each
training sample ((Y ,X ),··· ,(Y ,X )), a test sample ((Y′′,X′′),··· ,(Y′′,X′)) (independent of the training
1 1 n n 1 1 n n
one) is generated from the DGP. The MAPE and RMSPE are given for each n=250,500,1000 by:
Ã
n n
MAPE
n
= n−1
p
(cid:88) (cid:12) (cid:12)Y i′′−(cid:98)h n(X i′′)(cid:12) (cid:12); RMSPE
n
= n−1
p
(cid:88) (cid:0) Y i′′−(cid:98)h n(X i′′)(cid:1)2 .
i=p+1 i=p+1
Figures 3, 4 and Figures 5, 6 display the boxplots of the MAPE and RMSPE of the DNN predictors over
100 replications. One can see that, the robust estimators based on the absolute and the Huber loss functions
outperform the least squares method in DGP1 and in DGP2 for both Student and Gaussian error. These
performances (in terms of MAPE and RMSPE) of the robust estimator are more significant compared to the
least squares one, for models with Student’s t(2) error.Kengne and Wade 11
(a) Boxplots of the empirical L , Huber and L excess risk in DGP1 with Student−t error
1 2
l
l
ll
l
l
l
l
l
l
l
l ll
ll
ll
l
lll ll lllll lll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
l
l
(b) Boxplots of the empirical L , Huber and L excess risk in DGP1 with Gaussian error
1 2
l
l
l
l ll
l
ll
ll
l
l l
lll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 1: Boxplots of the empirical L , Huber and L excess risk of the DNN predictors with n=250,500 and
1 2
1000 in DGP1 with Student-t error (a) and Gaussian error (b).
3
2
1
0
52.0
02.0
51.0
01.0
50.0
00.012 Robust deep learning from weakly dependent data
l
(a) Boxplots of the empirical L , Huber and L excess risk in DGP2 with Student−t errorl
1 2
l
ll
l
l
l
l
l
l
ll
ll
ll
l
l
l
l
l l
l
llll
ll
lll
lll
l lll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
l
(b) Boxplots of the empirical L , Huber and L excess risk in DGP2 with Gaussian error
1 2
l
l
l
l
l
l
l
ll
l ll l l
l ll
lll
ll
l l l l
ll lllll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 2: Boxplots of the empirical L , Huber and L excess risk of the DNN predictors with n=250,500 and
1 2
1000 in DGP2 with Student-t error (a) and Gaussian error (b).
2.1
0.1
8.0
6.0
4.0
2.0
0.0
3.0
2.0
1.0
0.0Kengne and Wade 13
l
(a) Boxplots of the MAPE in DGP1 with Student−t error
l
l
ll
lll
l llll ll
l
l ll l
ll
l
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
(b) Boxplots of the MAPE in DGP1 with Gaussian error
l
l
l l
l
l
lll
l
l
l
l
lll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 3: Boxplots of the mean absolute prediction error of the DNN predictors with n=250,500 and 1000 in
DGP1 with Student-t error (a) and Gaussian error (b).
0.2
9.1
8.1
7.1
6.1
5.1
4.1
59.0
09.0
58.0
08.014 Robust deep learning from weakly dependent data
(a) Boxplots of the MAPE in DGP2 with Student−t error
l
l l
l
l l
ll l
l
l
l
l
l l
l l l
ll lll ll l
lll ll ll l l
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
(b) Boxplots of the MAPE in DGP2 with Gaussian error
l
l
ll l
l
l
l
lll
l
ll ll
l
ll ll
l l
l
l ll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 4: Boxplots of the mean absolute prediction error of the DNN predictors with n=250,500 and 1000 in
DGP2 with Student-t error (a) and Gaussian error (b).
56.1
06.1
55.1
05.1
54.1
04.1
53.1
09.0
58.0
08.0Kengne and Wade 15
(a) Boxplots of the RMSPE in DGP1 with Student−t error
l
l l
l
l
l
l
l lll
l l
ll
l
l l ll
ll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
(b) Boxplots of the RMSPE in DGP1 with Gaussian error
l
l ll
l
l
ll
l
l
ll l l
ll
l
l
l
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 5: Boxplots of the root mean square prediction error of the DNN predictors with n=250,500 and 1000
in DGP1 with Student-t error (a) and Gaussian error (b).
0.3
9.2
8.2
7.2
6.2
02.1
51.1
01.1
50.1
00.1
59.016 Robust deep learning from weakly dependent data
l
(a) Boxplots of the RMSPE in DGP2 with Student−t error
l
l
l
l
l
l l l l
l lll l
l l
l
l
l ll ll ll lll
l l l
l l
l
ll ll
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
(b) Boxplots of the RMSPE in DGP2 with Gaussian error
ll
l
l
ll l ll
lll l
l l l
ll
l
l
l l
llll ll l
L1 huber L2 L1 huber L2 L1 huber L2
n=250 n=500 n=1000
Figure 6: Boxplots of the root mean square prediction error of the DNN predictors with n=250,500 and 1000
in DGP2 with Student-t error (a) and Gaussian error (b).
07.2
56.2
06.2
55.2
05.2
52.1
02.1
51.1
01.1
50.1
00.1Kengne and Wade 17
6 Proofs of the main results
6.1 Proof of Theorem 3.1
Letusconsidertwoindependentsetsofobservations,D ={(X ,Y )n )}andD′ ={(X′,Y′)n )}generated,
n i i i=1 n i i i=1
respectively,fromastationaryergodicprocessZ =X×Y andZ′ =X′×Y′,whereX,X′ ⊂Rd,andY,Y′ ⊂R.
For all i=1,··· ,n and a predictor h:X →Y, we set:
g(h(X ),Y ):=ℓ(h(X ),Y )−ℓ(h∗(X ),Y ). (6.1)
i i i i i i
Recall that, the empirical risk minimizer (cid:98)h
n
depends on the sample D n, and its excess risk is given by
(cid:104)1 (cid:105)
E
D n′
n(cid:80)n i=1g((cid:98)h n,Z i′) and its expected excess risk is defined by
(cid:104) (cid:104)1 (cid:88)n (cid:105)(cid:105)
E[R((cid:98)h n)−R(h∗)]=E
Dn
E
D n′ n
g((cid:98)h n,Z i′) . (6.2)
i=1
Let L ,N ,B ,F ,S >0 satisfying the conditions in Theorem 3.1. In the sequel, we set:
n n n n n
H :=H (L ,N ,B ,F ,S ). (6.3)
σ,n σ n n n n n
We consider a target neural network (assumed to exist) h ∈H defined by
Hσ,n σ,n
h =argminR(h). (6.4)
Hσ,n
h∈Hσ,n
Let us recall the approximation error of h∗ given by
R(h )−R(h∗).
Hσ,n
RecallthattheapproximationerrordependsonlyonthefunctionsclassH andthedistributionofthedata.
σ,n
The definition of the empirical risk minimizer, implies
R(cid:98)n((cid:98)h n)≤R(cid:98)n(h Hσ,n).
That is,
(cid:104)1 (cid:88)n (cid:105) (cid:104)1 (cid:88)n (cid:105)
E
Dn n
g((cid:98)h n,Z i) ≤E
Dn n
g(h Hσ,n,Z i) . (6.5)
i=1 i=1
According to (6.5) and (6.2), we get
(cid:104)1 (cid:88)n (cid:105)
E[R((cid:98)h n)−R(h∗)]=E
Dn n
{−2g((cid:98)h n,Z i)+2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}
i=1
(cid:104)1 (cid:88)n (cid:105) (cid:104)1 (cid:88)n (cid:105)
≤E
Dn n
{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} +2E
Dn n
g((cid:98)h n,Z i)
i=1 i=1
(cid:104)1 (cid:88)n (cid:105) (cid:104)1 (cid:88)n (cid:105)
≤E
Dn n
{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} +2E
Dn n
g(h Hσ,n,Z i)
i=1 i=1
(cid:104)1 (cid:88)n (cid:105)
≤E
Dn n
{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} +2[R(h Hσ,n)−R(h∗)]. (6.6)
i=118 Robust deep learning from weakly dependent data
(cid:104)1 (cid:105)
TherestoftheproofwillconsistofboundingE
Dn
n(cid:80)n i=1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} , [R(h Hσ,n)−R(h∗)]
and then, derive a bound of the expected excess risk .
Step 1 : Bounding the first term in the right-hand side of (6.6). Let us denote for any h∈H
σ,n
G(h,Z )=E g(h,Z′)−2g(h,Z ).
i D n′ i i
Let ϵ > 0. Since ∥h∥ ≤ F < ∞ for all h ∈ H , then, the ϵ-covering number (see (2.1) in Section (2)) of
∞ n σ,n
H is finite. Set,
σ,n
(cid:16) (cid:17)
m:=N H ,ϵ . (6.7)
σ,n
From the Proposition 1 in [25], we have,
(cid:32) (cid:33)
(cid:16) (cid:17) Å1 ã
N H ,ϵ ≤exp 2L (S +1)log C L (N +1)(B ∨1) , (6.8)
σ,n n n ϵ σ n n n
whereB ∨1=max(B ,1). Thedependenceofmonnisomittedtosimplifynotation. Leth ,··· ,h ∈H
n n 1 m σ,n
such that,
m
(cid:91)
H ⊂ B(h ,ϵ). (6.9)
σ,n j
j=1
Recall that, B(h ,ϵ) represents the ball of radius ϵ, centered at h . As a consequence, there exists a random
j j
index j∗ ∈{1,··· ,m} such that, ∥(cid:98)h n−h j∗∥
∞
≤ϵ. Recall that g(h,Z i)=ℓ(h(X i),Y i)−ℓ(h∗(X i),Y i). Under
the assumption (A2), we have for all i=1,··· ,n,
|g((cid:98)h n),Z i)−g(h j∗,Z i)|=|ℓ((cid:98)h n(X i),Y i)−ℓ(h j∗(X i),Y i)|≤K ℓ|(cid:98)h n(X i)−h j∗(X i)|≤K ℓϵ.
Hence,
n n n
1 (cid:88) 1 (cid:88) 1 (cid:88)
n
E Dn[g((cid:98)h n,Z i)]≤
n
E Dn[g(h j∗,Z i)]+
n
E Dn[g((cid:98)h n,Z i)−g(h j∗,Z i)]
i=1 i=1 i=1
n n
1 (cid:88) 1 (cid:88)
≤
n
E Dn[g(h j∗,Z i)]+
n
E Dn[|g((cid:98)h n,Z i)−g(h j∗,Z i)|]
i=1 i=1
n
1 (cid:88)
≤ E [g(h ,Z )]+K ϵ.
n Dn j∗ i ℓ
i=1
Thus, we have,
|G((cid:98)h n,Z i)−G(h j∗,Z i)|=|E
D
n′g((cid:98)h n,Z i′)−2g((cid:98)h n,Z i)−E
D
n′g(h j∗,Z i′)+2g(h j∗,Z i)|
≤|E
D
n′g((cid:98)h n,Z i′)−E
D
n′g(h j∗,Z i′)|+2|g((cid:98)h n,Z i)−g(h j∗,Z i)|≤3K ℓϵ.
Therefore,
E Dn(cid:104) n1 (cid:88)n {−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) =E Dn(cid:2) n1 (cid:88)n G((cid:98)h n,Z i)(cid:3) ≤E Dn(cid:2) n1 (cid:88)n G(h j∗,Z i)(cid:3) +3K ℓϵ. (6.10)
i=1 i=1 i=1
For all n≥1, set:
β
=max(cid:0)
F
,(cid:0) n(α)(cid:1)1/r(cid:1)
, (6.11)
n nKengne and Wade 19
where r is given in the assumption (A5). Let T be a truncation at level β , i.e., for any Y ∈R,
βn n
ß
Y if |Y|≤β
T Y = n
βn β ·sign(Y) otherwise.
n
Let us define the function h∗ :X →R by
βn
h∗ (x)= argmin E[ℓ(h(X),T Y)|X =x], (6.12)
βn βn
h(x):∥h∥∞≤βn
for each x∈X. Let us recall that ∥h∗∥ ≤K≤F ≤β and
∞ n n
h∗(x)= argmin E[ℓ(h(X),Y)|X =x].
h(x):∥h∥∞≤βn
Thus, for any h satisfying ∥h∥ ≤β , we have from the definition above,
∞ n
E[ℓ(h∗ (X ),T Y )]≤E[ℓ(h(X ),T Y )] and E[ℓ(h∗(X ),Y )]≤E[ℓ(h(X ),Y )]. (6.13)
βn i βn i i βn i i i i i
For all h∈H , set
σ,n
g (h(X ),Z )=ℓ(h(X ),T Y )−ℓ(h∗ (X ),T Y ). (6.14)
βn i i i βn i βn i βn i
We get,
(cid:104)
E[g(h,Z )]=E g (h,Z )−g (h,Z )+ℓ(h(X ),Y )−ℓ(h∗(X ),Y )
i βn i βn i i i i i
(cid:105)
+ℓ(h∗ (X ),T Y )−ℓ(h∗ (X ),T Y )
βn i βn i βn i βn i
(cid:104)
≤E g (h,Z )−ℓ(h(X ),T Y )+ℓ(h∗ (X ),T Y )+ℓ(h(X ),Y )−ℓ(h∗(X ),Y )
βn i i βn i βn i βn i i i i i
(cid:105)
+ℓ(h∗(X ),T Y )−ℓ(h∗(X ),T Y )
i βn i i βn i
≤E[g (h,Z )]+E[ℓ(h(X ),Y )−ℓ(h(X ),T Y )+ℓ(h∗(X ),T Y )−ℓ(h∗(X ),Y )
βn i i i i βn i i βn i i i
+ℓ(h∗ (X ),T Y )−ℓ(h∗(X ),T Y )]
βn i βn i i βn i
≤E[g (h,Z )]+E[ℓ(h(X ),Y )−ℓ(h(X ),T Y )+ℓ(h∗(X ),T Y )−ℓ(h∗(X ),Y )] (6.15)
βn i i i i βn i i βn i i i
≤E[g (h,Z )]+2K E[|T Y −Y |], (6.16)
βn i ℓ βn i i
where the inequality in (6.15) holds from (6.13). Moreover, we have
1 1 1
|T Y −Y |=|β ·sign(Y ) +Y −Y |≤|Y | .
βn i i n i {|Yi|≥βn} i {|Yi|≤βn} i i {|Yi|≥βn}
Recall the assumption (A5) that Y has a finite r-moment, for some r >1. We have,
i
E[|Y |1 ]≤E[|Y ||Y |r−1]/βr−1]≤E[|Y |r]/βr−1. (6.17)
i {|Yi|≥βn} i i n i n
Therefore, from (6.16) and (6.17), we get
E[g(h,Z )]≤E[g (h,Z )]+2K E[|Y |r]/βr−1. (6.18)
i βn i ℓ i n20 Robust deep learning from weakly dependent data
Using a similar argument, we have
E[g (h,Z )]=E[g(h,Z )−g(h,Z )+g (h,Z )]
βn i i i βn i
=E[g(h,Z )]+E[ℓ(h∗(X ),Y )−ℓ(h(X ),Y )+ℓ(h(X ),T Y )−ℓ(h∗ (X ),T Y )
i i i i i i βn i βn i βn i
+ℓ(h∗ (X ),Y )−ℓ(h∗ (X ),Y )]
βn i i βn i i
≤E[g(h,Z )]+E[ℓ(h∗(X ),Y )−ℓ(h∗ (X ),Y )+ℓ(h(X ),T Y )−ℓ(h(X ),Y )
i i i βn i i i βn i i i
+ℓ(h∗ (X ),Y )−ℓ(h∗ (X ),T Y )]
βn i i βn i βn i
≤E[g(h,Z )]+E[ℓ(h(X ),T Y )−ℓ(h(X ),Y )+ℓ(h∗ (X ),Y )−ℓ(h∗ (X ),T Y )]
i i βn i i i βn i i βn i βn i
≤E[g(h,Z )]+2K E[|T Y −Y |]
i ℓ βn i i
≤E[g(h,Z )]+2K E[|Y |r]/βr−1. (6.19)
i ℓ i n
Note that, g(h,Z′) and g (h,Z′) also satisfy the above inequalities. Let h∈H . Set
i βn i σ,n
G (h,Z ):=E [g (h,Z′)]−2g (h,Z ). (6.20)
βn i D n′ βn i βn i
We have
|G(h,Z )−G (h,Z )|=|E g(h,Z′)−2g(h,Z )−E [g (h,Z )]+2g (h,Z )|
i βn i D n′ i i D n′ βn i βn i
≤|E g(h,Z′)−E [g (h,Z′)]|+2|g(h,Z )−g (h,Z )|.
D n′ i D n′ βn i i βn i
Thus, in addition to (6.18) and (6.19), It comes that
(cid:104)1 (cid:88)n (cid:105) (cid:104)1 (cid:88)n (cid:105)
E G(h,Z ) ≤E G (h,Z ) +6K E[|Y |r]/βr−1. (6.21)
Dn n i Dn n βn i ℓ i n
i=1 i=1
Therefore, in addition to (6.10), we get,
E Dn(cid:104) n1 (cid:88)n {−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) ≤E Dn(cid:2) n1 (cid:88)n G(h j∗,Z i)(cid:3) +3K ℓϵ
i=1 i=1
(cid:104)1 (cid:88)n (cid:105)
≤E G (h ,Z ) +6K E[|Y |r]/βr−1+3K ϵ. (6.22)
Dn n βn j∗ i ℓ i n ℓ
i=1
(cid:104)1 (cid:105)
Let us focus on an upper bound of E (cid:80)n G (h ,Z ) . Let h∈H . Since F ≤β , we have,
Dn n i=1 βn j∗ i σ,n n n
|g (h,Z )|=|ℓ(h(X ),T Y )−ℓ(h∗ (X ),T Y )|≤K |h(X )−h∗ (X )|≤K (F +β )≤2K β . (6.23)
βn i i βn i βn i βn i ℓ i βn i ℓ n n ℓ n
According to (6.23), we have,
Var(g (h,Z ))≤E[g (h,Z )2]≤E[|g (h,Z )||g (h,Z )|]≤2K β E[g (h,Z )]. (6.24)
βn i βn i βn i βn i ℓ n βn i
Let ε > 0 and n(α) = ⌊n⌈{8n/c}1/(γ+1)⌉−1⌋ ≥ 2 for some α > 0,γ > 0 given in the assumption (A4).
According to (6.23), we have, |E [g (h ,Z′)]−g (h ,Z )| ≤ 4K β . By using (6.24) and the Bernstein
D n′ βn j∗ i βn j∗ i ℓ nKengne and Wade 21
typeinequalityfordependentdata(see([34], [24])), wehaveforallj =1,··· ,m(wheremisdefinedin(6.7)),
(cid:40) n (cid:41) (cid:40) n n (cid:41)
1 (cid:88) 1 (cid:88) 2 (cid:88)
P G (h ,Z )>ε =P E [g (h ,Z′)]− g (h ,Z )>ε
n βn j i n D n′ βn j i n βn j i
i=1 i=1 i=1
(cid:110) 2 (cid:88)n (cid:111)
=P E [g (h ,Z′)]− g (h ,Z )>ε .
D n′ βn j 1 n βn j i
i=1
(cid:110) 2 (cid:88)n (cid:111)
=P 2E [g (h ,Z′)]− g (h ,Z )>ε+E [g (h ,Z′)]
D n′ βn j 1 n βn j∗ i D n′ βn j 1
i=1
(cid:110) 1 (cid:88)n ε 1 (cid:111)
=P E [g (h ,Z′)]− g (h ,Z )> + E [g (h ,Z′)]
D n′ βn j 1 n βn j i 2 2 D n′ βn j∗ 1
i=1
≤P(cid:40)
E [g (h ,Z′)]−
1 (cid:88)n
g (h ,Z )>
ε
+
E
D
n′[|g βn(h j,Z 1′)|2](cid:41)
D n′ βn j 1 n βn j i 2 4K β
ℓ n
i=1
≤P(cid:40) 1 (cid:88)n (cid:16)
E[g (h ,Z )]−g (h ,Z
)(cid:17)
>
ε
+
E[|g βn(h j,Z 1)|2](cid:41)
n βn j i βn j i 2 4K β
ℓ n
i=1
(cid:16)ε E[|g (h ,Z )|2](cid:17)2
(cid:32) + βn j 1 n(α) (cid:33)
2 4K β
≤(1+4e−2α)exp − ℓ n
(cid:16) (cid:16)ε E[|g (h ,Z )|2](cid:17) (cid:17)
2 E[|g (h ,Z )|2]+4 + βn j 1 K β /3
βn j 1 2 4K β ℓ n
ℓ n
(cid:16)ε E[|g (h ,Z )|2](cid:17)2
(cid:32) + βn j 1 n(α) (cid:33)
2 4K β
≤(1+4e−2α)exp − ℓ n
(cid:16)(cid:16)ε E[|g (h ,Z )|2](cid:17) (cid:16)ε E[|g (h ,Z )|2](cid:17) (cid:17)
2 + βn j 1 4K β +4 + βn j 1 K β /3
2 4K β ℓ n 2 4K β ℓ n
ℓ n ℓ n
(cid:16)ε E[|g (h ,Z )|2](cid:17)2
(cid:32) + βn j 1 n(α) (cid:33)
2 4K β
≤(1+4e−2α)exp −3 ℓ n
(cid:16)ε E[|g (h ,Z )|2](cid:17)
32 + βn j 1 K β
2 4K β ℓ n
ℓ n
(cid:16)ε E[|g (h ,Z )|2](cid:17)
(cid:32) + βn j 1 n(α)(cid:33)
2 4K β
≤(1+4e−2α)exp −3 ℓ n
32K β
ℓ n
(cid:32) (cid:33)
3ε
≤(1+4e−2α)exp − n(α) . (6.25)
64K β
ℓ n
(cid:16) (cid:17)
Set, ϵ= 1 , see (6.7). That is m=N H ,1/n(α) . In addition to (6.8) (see the proof of Theorem 3.1) we
n(α) σ,n
have,
(cid:110)1 (cid:88)n (cid:111) (cid:40) (cid:91)m (cid:16)1 (cid:88)n (cid:17)(cid:41) (cid:88)m (cid:110)1 (cid:88)n (cid:111)
P G (h ,Z )>ε ≤P G (h ,Z )>ε ≤ P G (h ,Z )>ε
n βn j∗ i n βn j i n βn j i
i=1 j=1 i=1 j=1 i=1
(cid:32) (cid:33)
(cid:16) 1 (cid:17) 3ε
≤N H , ·(1+4e−2α)exp − n(α) (6.26)
σ,n n(α) 64K β
ℓ n
(cid:32) (cid:33)
(cid:16) (cid:17) 3ε
≤(1+4e−2α)exp 2L (S +1)log n(α)C L (N +1)(B ∨1) − n(α) .
n n σ n n n 64K β
ℓ n22 Robust deep learning from weakly dependent data
For α >0, we have
n
(cid:104)1 (cid:88)n (cid:105) (cid:90) ∞ (cid:110)1 (cid:88)n (cid:111)
E G (h ,Z ) ≤α + P G (h ,Z )>ε dε
n βn j∗ i n n βn j∗ i
i=1 αn i=1
(cid:16) (cid:17) (cid:90) ∞ (cid:16) 3εn(α) (cid:17)
≤α +(1+4e−2α)exp 2L (S +1)log(n(α)C L (N +1)(B ∨1)) × exp − dε.
n n n σ n n n 64K β
αn ℓ n
One can easily see that,
(cid:90) ∞ (cid:16) 3εn(α) (cid:17) 64K β (cid:16) 3α n(α)(cid:17)
exp − dε= ℓ n exp − n (6.27)
64K β 3n(α) 64K β
αn ℓ n ℓ n
Hence we have,
(cid:104)1 (cid:88)n (cid:105) (cid:90) ∞ (cid:110)1 (cid:88)n (cid:111)
E G (h ,Z ) ≤α + P G (h ,Z )>ε dε
n βn j∗ i n n βn j∗ i
i=1 αn i=1
64K β (1+4e−2α) (cid:16) (cid:17) (cid:16) 3α n(α)(cid:17)
≤α + ℓ n exp 2L (S +1)log(n(α)C L (N +1)(B ∨1)) exp − n
n 3n(α) n n σ n n n 64K β
ℓ n
64K β (1+4e−2α) (cid:16) 3α n(α)(cid:17)
≤α + ℓ n exp 2L (S +1)log(n(α)C L (N +1)(B ∨1))− n . (6.28)
n 3n(α) n n σ n n n 64K β
ℓ n
(cid:0) logn(α)(cid:1)ν
Set α := , for some ν >3. According to (6.28) and since β
≥(cid:0) n(α)(cid:1)1/r
(see (6.11)), we have
n (cid:0) (cid:1) s (1−1) n
n(α) s+d r
(cid:104)1 (cid:88)n (cid:105)
E G (h ,Z )
n βn j∗ i
i=1
(cid:0) logn(α)(cid:1)ν
64K β (1+4e−2α) (cid:16)
3(cid:0) n(α)(cid:1)d/(s+d)(cid:0) logn(α)(cid:1)ν
(cid:17)
≤ + ℓ n exp 2L (S +1)log(n(α)C L (N +1)(B ∨1))−
(cid:0) (cid:1) s (1−1) 3n(α) n n σ n n n 64K β
n(α) s+d r ℓ n
(cid:0) logn(α)(cid:1)ν
64K (1+4e−2α) (cid:16)
3(cid:0) n(α)(cid:1)(1− r1) s+d d(cid:0) logn(α)(cid:1)ν
(cid:17)
≤ + ℓ exp 2L (S +1)log(n(α)C L (N +1)(B ∨1))− .
(cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1−1/r) n n σ n n n 64K
ℓ
(6.29)
Recalltheassumptions: L n =(cid:0) 1−1 r(cid:1) ss +L 0 dlog(cid:0) n(α)(cid:1) ,N n =N 0(cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) s+d d,S n =(cid:0) 1−1 r(cid:1) ss +S0 d(n(α))(cid:0) 1− r1(cid:1) s+d d log(cid:0) n(α)(cid:1) ,
(cid:0) (cid:1)
B =B
(cid:0) n(α)(cid:1) 1− r1 4s(d s+/s d+1)
, with L ,N ,B ,S >0. Hence,
n 0 0 0 0 0
(cid:104)1 (cid:88)n (cid:105)
E G (h ,Z )
n βn j∗ i
i=1
≤ (cid:0) logn(α)(cid:1)ν + 64K ℓ(1+4e−2α) exp(cid:34) 2(cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1)(cid:16) (cid:0) 1− 1(cid:1) sS 0 (n(α))(cid:0) 1− r1(cid:1) s+d d log(cid:0) n(α)(cid:1) +1(cid:17)
(cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1−1/r) r s+d r s+d
(cid:32) (cid:33)
(cid:0) (cid:1) (cid:0) (cid:1)
×log n(α)C (cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1)(cid:0) N (cid:0) n(α)(cid:1) 1− r1 s+d d +1(cid:1)(cid:0) B (cid:0) n(α)(cid:1) 1− r1 4s(d s+/s d+1) ∨1(cid:1)
σ r s+d 0 0
3(cid:0) n(α)(cid:1)(1− r1) s+d d(cid:0) logn(α)(cid:1)ν(cid:35)
−
64K
ℓKengne and Wade 23
(cid:0) logn(α)(cid:1)ν
64K (1+4e−2α)
(cid:34) 3(cid:0) n(α)(cid:1)(1− r1) s+d d(cid:0) logn(α)(cid:1)ν
≤ + ℓ exp
(cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1−1/r) 64K
ℓ
128K ℓ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(cid:0) n(α)(cid:1)(cid:16) (cid:0) 1− 1 r(cid:1) ss +S0 dlog(cid:0) n(α)(cid:1) +1(cid:17)
×
(cid:0) (cid:1)ν
3 logn(α)
(cid:32) (cid:33)(cid:35)
× log(cid:16) n(α)C (cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1)(cid:0) N (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) s+d d +1(cid:1)(cid:0) B (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1(cid:1)(cid:17) −1 .
σ r s+d 0 0
(6.30)
From (6.30), we have
128K ℓ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(cid:0) n(α)(cid:1)(cid:16) (cid:0) 1− 1 r(cid:1) ss +S0 dlog(cid:0) n(α)(cid:1) +1(cid:17)
(cid:0) (cid:1)ν
3 logn(α)
×log(cid:16) n(α)C (cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1)(cid:0) N (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) s+d d +1(cid:1)(cid:0) B (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1(cid:1)(cid:17) −→ 0.
σ r s+d 0 0
n→∞
Hence, there exists n =n (K ,L ,N ,B ,S ,C ,s,d,r,c,γ) such that, for any n≥n , we have
0 0 ℓ 0 0 0 0 σ 0
128K ℓ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(cid:0) n(α)(cid:1)(cid:16) (cid:0) 1− 1 r(cid:1) ss +S0 dlog(cid:0) n(α)(cid:1) +1(cid:17)
(cid:0) (cid:1)ν
3 logn(α)
×log(cid:16) n(α)C (cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1)(cid:0) N (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) s+d d +1(cid:1)(cid:0) B (cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1(cid:1)(cid:17) < 1 . (6.31)
σ r s+d 0 0 2
According to (6.31), we have
E(cid:104)1 (cid:88)n
G (h ,Z
)(cid:105)
≤
(cid:0) logn(α)(cid:1)ν
+
64K ℓ(1+4e−2α) exp(cid:16)
−
3(cid:0) n(α)(cid:1)(1− r1) s+d d(cid:0) logn(α)(cid:1)ν (cid:17)
n
i=1
βn j∗ i (cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1−1/r) 128K
ℓ
(cid:0) logn(α)(cid:1)ν
64K (1+4e−2α)
≤ + ℓ .
(cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1−1/r)
So, (6.10) and (6.22) give,
E Dn(cid:104) n1 (cid:88) i=n 1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) ≤
(cid:0)
n(cid:0) (αlo )g
(cid:1)
sn +s( dα () 1(cid:1) −ν
r1)
+ 64 3K (cid:0)ℓ n( (1 α+ )(cid:1)(4 1e −− r12 )α) +6K ℓE[|Y i|r]/β nr−1+3K ℓϵ
(cid:0) logn(α)(cid:1)ν 64K (1+4e−2α) 6K E[|Y |r]
≤ + ℓ + ℓ i +3K ϵ.
(cid:0) n(α)(cid:1) s+s d(1− r1) 3(cid:0) n(α)(cid:1)(1− r1) (n(α))(1− r1) ℓ
1
Recall that from assumption (A4), E[|Y |r]≤M for some M >0,r >1. Since we have set ϵ= , it holds
0 n(α)
that,
E Dn(cid:104) n1 (cid:88) i=n 1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) ≤
(cid:0)
n(cid:0) (αlo )g
(cid:1)
sn +s( dα () 1(cid:1) −ν r1)+64 3K (cid:0)ℓ n( (1 α+ )(cid:1)(4 1e −− r12 )α) + (n(6 αK ))ℓ (M
1−
r1)+ n3K (αℓ ). (6.32)
Step 2 : Bounding the second term in the right-hand side of (6.6).24 Robust deep learning from weakly dependent data
Recall that we have set H :=H (L ,N ,B ,F ,S ) ( see (6.3)).
σ,n σ n n n n n
Under the Lipschitz property on ℓ, for all h∈H , the following inequality holds,
σ,n
R(h)−R(h∗)=E [ℓ(h(X ),Y )]−E [ℓ(h∗(X ),Y )]≤K E [|h(X )−h∗(X )|]. (6.33)
Z0 0 0 Z0 0 0 ℓ X0 0 0
According to (6.33), we have,
(cid:16) (cid:17)
R(h )−R(h∗)= inf R(h)−R(h∗)= inf R(h)−R(h∗)
Hσ,n
h∈Hσ,n h∈Hσ,n
≤K inf E [|h(X )−h∗(X )|]. (6.34)
ℓ
h∈Hσ,n
X0 0 0
1
Recalltheassumptionh∗ ∈Cs,K(X)forsomes,K>0. Setϵ = . From[14],thereexistsapos-
n (cid:0) (cid:1) s (1−1)
n(α) s+d r
(cid:0) (cid:1)
itive constants L ,N ,B ,S >0 such that with L =(cid:0) 1− 1(cid:1) sL 0 log(cid:0) n(α)(cid:1) ,N =N (cid:0) n(α)(cid:1) 1− r1 s+d d,S =
0 0 0 0 n r s+d n 0 n
(cid:0) 1− 1 r(cid:1) ss +S0 d(n(α))(cid:0) 1− r1(cid:1) s+d d log(cid:0) n(α)(cid:1) , B n =B 0(cid:0) n(α)(cid:1)(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) , there is a neural network h n ∈H σ,n satis-
fying,
1
∥h −h∗∥ < . (6.35)
n ∞,X (cid:0) (cid:1) s (1−1)
n(α) s+d r
According to (6.34) and in addition to (6.35), we have,
K
R(h )−R(h∗)≤ ℓ . (6.36)
Hσ,n (cid:0) (cid:1) s (1−1)
n(α) s+d r
Step 3 : Expected excess risk bound.
According to (6.6), (6.32) and (6.36), we have,
(cid:0) logn(α)(cid:1)ν
64K (1+4e−2α) 6K M 3K K
E[R((cid:98)h n)−R(h∗)]≤
(cid:0) n(α)(cid:1) s+s d(1− r1)
+ 3(cid:0)ℓ
n(α)(cid:1)(1− r1)
+ (n(α))ℓ
(1− r1)
+ n(αℓ
)
+
(cid:0) n(α)(cid:1)
s+sℓ
d(1− r1)
(cid:0) logn(α)(cid:1)ν
+K C(K ,α,M) 3K
≤ ℓ + ℓ + ℓ,
(cid:0) (cid:1) s (1−1) (cid:0) (cid:1)(1−1) n(α)
n(α) s+d r n(α) r
with C(K ,α,M)=64K (1+4e−2α)/3+6K M. ■
ℓ ℓ ℓ
6.2 Proof of Theorem 3.3
Let D = {(X ,Y )n )} and D′ = {(X′,Y′)n )} be two independent sets of observations generated, re-
n i i i=1 n i i i=1
spectively, from a stationary and ergodic process Z = X ×Y and Z′ = X′ ×Y′, where X,X′ ⊂ Rd, and
Y,Y′ ⊂R.
Let L ,N ,B ,F ,S >0 fulfill the conditions in Theorem 3.3. In the sequel, we set:
n n n n n
H :=H (L ,N ,B ,F ,S ).
σ,n σ n n n n n
Recall the inequality (6.6) (see proof of Theorem 3.1):
(cid:104)1 (cid:88)n (cid:105)
E[R((cid:98)h n)−R(h∗)]≤E
Dn n
{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} +2[R(h Hσ,n)−R(h∗)], (6.37)
i=1Kengne and Wade 25
(cid:104)1
where g(h,Z) is defined in (6.1). We will focus on the bounds of E
Dn
n(cid:80)n i=1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}
and [R(h )−R(h∗)].
Hσ,n
For all n≥1, set
µ+1
β
n
=nr(2µ+3), (6.38)
for µ>0,r >1 given in the assumptions (A3) and (A5) respectively.
Step 1 : Bounding the first term in the right-hand side of (6.37).
Recall the inequality (6.22) (see proof of Theorem 3.1),
(cid:104)1 (cid:88)n (cid:105) (cid:104)1 (cid:88)n (cid:105)
E
Dn n
{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)} ≤E
Dn n
G βn(h j∗,Z i) +6K ℓE[|Y i|r]/β nr−1+3K ℓϵ, (6.39)
i=1 i=1
where G (h,Z) is defined in (6.14). Let ϵ > 0. Recall that, the ϵ-covering number of H is finite, the
βn
(cid:16) (cid:17)
σ,n
notation: m := N H ,ϵ and that H ⊂
(cid:83)m
B(h ,ϵ), for h ,··· ,h ∈ H . Let us derive an upper
σ,n σ,n j=1 j 1 m σ,n
(cid:104)1 (cid:105)
boundofE (cid:80)n G (h ,Z ) . Leth∈H . SinceF ≤β ,recalltheinequality(6.23): |g (h,Z )|≤
Dn n i=1 βn j∗ i σ,n n n βn i
K (F +β )≤2K β .
ℓ n n ℓ n
Let ε>0. From [9] and the Remark 3.3 in [6], we have for all j =1,··· ,m,
(cid:40) n (cid:41) (cid:40) n n (cid:41)
1 (cid:88) 1 (cid:88) 2 (cid:88)
P G (h ,Z )>ε =P E [g (h ,Z′)]− g (h ,Z )>ε
n βn j i n D n′ βn j i n βn j i
i=1 i=1 i=1
(cid:110) 2 (cid:88)n (cid:111)
=P E [g (h ,Z′)]− g (h ,Z )>ε .
D n′ βn j 1 n βn j i
i=1
(cid:110) 2 (cid:88)n (cid:111)
=P 2E [g (h ,Z′)]− g (h ,Z )>ε+E [g (h ,Z′)]
D n′ βn j 1 n βn j i D n′ βn j 1
i=1
(cid:110) 1 (cid:88)n ε 1 (cid:111)
=P E [g (h ,Z′)]− g (h ,Z )> + E [g (h ,Z′)]
D n′ βn j 1 n βn j i 2 2 D n′ βn j∗ 1
i=1
≤P(cid:40)
E [g (h ,Z′)]−
1 (cid:88)n
g (h ,Z )>
ε
+
E
D
n′[|g βn(h j,Z 1′)|2](cid:41)
D n′ βn j 1 n βn j i 2 4K β
ℓ n
i=1
(cid:110) 1 (cid:88)n ε(cid:111)
≤P E[g (h ,Z )]− g (h ,Z )>
βn j 1 n βn j i 2
i=1
(cid:32) (cid:33)
n2ε2/16
≤exp − , (6.40)
nC
+2C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2)
n,1 n,2
where C =16K2β2Ψ(1,1)L ,C =4K β L max(23+µ/Ψ(1,1),1).
n,1 ℓ n 1 n,2 ℓ n 226 Robust deep learning from weakly dependent data
(cid:16) (cid:17)
Take ϵ= 1, that is m=N H ,1/n . In addition to (6.8) we have,
n σ,n
(cid:110)1 (cid:88)n (cid:111) (cid:40) (cid:91)m (cid:16)1 (cid:88)n (cid:17)(cid:41) (cid:88)m (cid:110)1 (cid:88)n (cid:111)
P G (h ,Z )>ε ≤P G (h ,Z )>ε ≤ P G (h ,Z )>ε
n βn j∗ i n βn j i n βn j i
i=1 j=1 i=1 j=1 i=1
(cid:32) (cid:33)
(cid:16) 1(cid:17) n2ε2/16
≤N H , ·exp −
σ,n n
nC
+2C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2)
n,1 n,2
(cid:32) (cid:33)
(cid:16) (cid:17) n2ε2/16
≤exp 2L (S +1)log nC L (N +1)(B ∨1) − .
n n σ n n n
nC
+2C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2)
n,1 n,2
(6.41)
Moreover,
2(µ+1)/(2µ+3)C(µ+2)/(2µ+3)
2C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2) >nC =⇒ε> n,1 :=ε . (6.42)
n,2 n,1 n(µ+1)/(2µ+3)C1/(2µ+3) n
n,2
C(µ+2)/(2µ+3) (cid:0) 16K2Ψ(1,1)L (cid:1) 2µ µ+ +2 3β
One can see that, n,1 = ℓ 1 n .
C1/(2µ+3) (cid:0)
4K L
max(23+µ/Ψ(1,1),1)(cid:1) 2µ1
+3
n,2 ℓ 2
2(µ+1)/(2µ+3)(cid:0) 16K2Ψ(1,1)L (cid:1) 2µ µ+ +2 3
Thus, we have, ε := ℓ 1 .
n (cid:0) 4K ℓL 2max(23+µ/Ψ(1,1),1)(cid:1) 2µ1 +3n(1−1/r) 2µ µ+ +1 3
From (6.42), we get,
1 1
− ≤− . (6.43)
nC
+2C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2) 4C1/(µ+2)(cid:0) nε/2(cid:1)(2µ+3)/(µ+2)
n,1 n,2 n,2
According to (6.41) and (6.43), we have,
(cid:110)1 (cid:88)n (cid:111) (cid:32) (cid:16) (cid:17) n1/(µ+2)ε1/(µ+2) (cid:33)
P G (h ,Z )>ε ≤exp 2L (S +1)log nC L (N +1)(B ∨1) − .
n βn i i n n σ n n n 64C1/(µ+2)(1/2)(2µ+3)/(µ+2)
i=1 n,2
(6.44)
Case 1: ε>1>ε .
n
Recall that from (6.44), we have,
(cid:110)1 (cid:88)n (cid:111) (cid:32) (cid:16) (cid:17) n1/(µ+2)ε1/(µ+2) (cid:33)
P G (h ,Z )>ε ≤exp 2L (S +1)log nC L (N +1)(B ∨1) − .
n βn i i n n σ n n n 64C1/(µ+2)(1/2)(2µ+3)/(µ+2)
i=1 n,2
(6.45)
Recall the assumptions:
L n =(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n),N n =N 0n(cid:0) 1− r1(cid:1) s+d d,S n =(cid:0) 1− 1 r(cid:1) ss +S 0 dn(cid:0) 1− r1(cid:1) s+d d log(n),B n =B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ,
(6.46)Kengne and Wade 27
with L ,N ,B ,S >0. In addition to (6.46), we have,
0 0 0 0
(cid:110)1 (cid:88)n (cid:111)
P G (h ,Z )>ε
n βn i i
i=1
(cid:32)
≤exp 2(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1) sS 0 n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
r s+d r s+d
(cid:33)
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) − 64C1/n (µ1 +/( 2µ )+ (12 /)ε 21 )/ (2(µ µ+ +2 3)
)/(µ+2)
.
n,2
Thus, we have
(cid:110)1 (cid:88)n (cid:111)
P G (h ,Z )>ε
n βn i i
i=1
(cid:34)
n1/(µ+2)ε1/(µ+2)
≤exp
64C1/(µ+2)(1/2)(2µ+3)/(µ+2)
n,2
(cid:32)128C1/(µ+2)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
×
n1/(µ+2)
(cid:33)(cid:35)
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) −1 .
(cid:16)(r−1)(2µ+3)(µ+2) (cid:17)
According to the condition s>d −1 , it holds that,
r(2µ+3)−(µ+1)
128C1/(µ+2)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
n1/(µ+2)
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) −→ 0.
n→∞
Hence, there exists n =n (L ,N ,B ,S ,C ,r,µ,s,d) such that, for any n≥n , we have,
1 1 0 0 0 0 σ 1
128C1/(µ+2)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
n1/(µ+2)
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) < 1 2. (6.47)
According to (6.47), we have
(cid:110)1 (cid:88)n (cid:111) (cid:16) n1/(µ+2)ε1/(µ+2) (cid:17)
P G (h ,Z )>ε ≤exp − ,
n βn i i 128C1/(µ+2)(1/2)(2µ+3)/(µ+2)
i=1 n,2
which holds for all ε>1. Hence,
(cid:90) ∞ (cid:110)1 (cid:88)n (cid:111) (cid:90) ∞ (cid:16) n1/(µ+2)ε1/(µ+2) (cid:17)
P G (h ,Z )>ε dε≤ exp − dε. (6.48)
1 n i=1 βn i i 1 128C n1/ ,2(µ+2)(1/2)(2µ+3)/(µ+2)
To compute the right-hand side in (6.48), we use similar arguments as in [15].28 Robust deep learning from weakly dependent data
Let
n1/(µ+2)
x= ε1/(µ+2), g(x):=x−2(µ+2)log(x).
(cid:101) 128C1/(µ+2)(1/2)(2µ+3)/(µ+2)
n,2
Let x =(2(µ+2))3. One can easily see that g(x )>0 and to get
0 0
n1/(µ+2)
x>x that is ε1/(µ+2) >(2(µ+2))3, ∀ε>1,
(cid:101) 0 128C1/(µ+2)(1/2)(2µ+3)/(µ+2)
n,2
it suffice that,
(cid:16) (cid:17) r(2µ+3)(µ+2)
n> 128C(K ,µ)1/(µ+2)(1/2)(2µ+3)/(µ+2)(2(µ+2))3 r(2µ+3)−(µ+1),
ℓ
with C(K ,µ)=4K L max(23+µ/Ψ(1,1),1). One can remark that for any
ℓ ℓ 2
1
x>x , we have exp(−x)< .
0 x2(µ+2)
Therefore, (6.48) gives,
(cid:90) ∞ (cid:16) n1/(µ+2)ε1/(µ+2) (cid:17) (cid:90) ∞ 1
exp − dε≤ dε
1 128C n1/ ,2(µ+2)(1/2)(2µ+3)/(µ+2) 1 (cid:16) n1/(µ+2) ε1/(µ+2)(cid:17)2(µ+2)
128C1/(µ+2)(1/2)(2µ+3)/(µ+2)
n,2
(16384)µ+2(1/2)4µ+6C2 (cid:90) ∞ 1 (16384)µ+2(1/2)4µ+6C2
≤ 2,n dε= 2,n.
n2 ε2 n2
1
Thus, we have
(cid:90) ∞ P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111)
dε≤
(16384)µ+2(1/2)4µ+6C 22
,n.
n βn i i n2
1 i=1
Recall that we have set C 2,n =4K ℓβ nL 2max(23+µ/Ψ(1,1),1). Since β n =nr(µ 2µ+ +1 3) (see (6.38)), we have,
(cid:90) ∞ P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111)
dε≤
(16384)µ+2(1/2)4µ+6C(K ℓ,µ)2
, (6.49)
1 n i=1 βn i i n2(cid:0) 1− r(µ 2µ+ +1 3)(cid:1)
where C(K ,µ)=4K L max(23+µ/Ψ(1,1),1).
ℓ ℓ 2
So, one can find n =n (µ,K ) such that, for all n>n , we have,
2 2 ℓ 2
(cid:90) ∞ P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111)
dε≤
(16384)µ+2(1/2)4µ+6C(K ℓ,µ)2
≤
1
. (6.50)
1 n i=1 βn i i n2(cid:0) 1− r(µ 2µ+ +1 3)(cid:1) n(cid:0) 1−1/r(cid:1) (µ+1)/(2µ+3)
Case 2: ε <ε<1. Recall that from (6.44), we have,
n
(cid:110)1 (cid:88)n (cid:111) (cid:32) (cid:16) (cid:17) n1/(µ+2)ε1/(µ+2) (cid:33)
P G (h ,Z )>ε ≤exp 2L (S +1)log nC L (N +1)(B ∨1) −
n βn i i n n σ n n n 64C1/(µ+2)(1/2)(2µ+3)/(µ+2)
i=1 n,2
(cid:32) (cid:33)
(cid:16) (cid:17) n1/(µ+2)ε1/(µ+2)
n
≤exp 2L (S +1)log nC L (N +1)(B ∨1) − .
n n σ n n n 64C1/(µ+2)(1/2)(2µ+3)/(µ+2)
n,2Kengne and Wade 29
According to (6.42) and (6.46), we have,
(cid:110)1 (cid:88)n (cid:111)
P G (h ,Z )>ε
n βn i i
i=1
(cid:32)
≤exp 2(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1) sS 0 n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
r s+d r s+d
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) − n1/( 62µ 4+ C3 2) /2 (( 2µ µ+ +1 3) )/ (( 12 /µ+ 2)3 () 2( µµ ++ 32 )) /C (µn1 +/ ,1( 22 )µ+3)(cid:33)
n,2
(cid:32) n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)
≤exp n,1
64C2/(2µ+3)(1/2)(2µ+3)/(µ+2)
n,2
(cid:32)128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
×
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)
n,1
(cid:33)(cid:33)
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) −1 .
Recall the condition
s>d(cid:16) (cid:0)
1−
1(cid:1) (2µ+3)−1(cid:17)
, that is
1 >(cid:0)
1−
1(cid:1) d
. Consequently,
r 2µ+3 r s+d
128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)
n,1
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) −→ 0.
n→∞
Then, there exists n =n (L ,N ,B ,S ,C ,r,L ,L ,µ,s,d,r) such that, for any n>n , we have,
3 3 0 0 0 0 σ 1 2 3
128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)(cid:0) 1− 1(cid:1) sL 0 log(n)(cid:0)(cid:0) 1− 1(cid:1)sS0n(cid:0) 1− r1(cid:1) s+d d log(n)+1(cid:1)
n,2 r s+d r s+d
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)
n,1
×log(cid:16) nC σ(cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n)(cid:0) N 0n(cid:0) 1− r1(cid:1) s+d d +1(cid:1) (B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ∨1)(cid:17) < 1 2. (6.51)
According to (6.51), we have
P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111) ≤exp(cid:32)
−
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C n1/ ,1(2µ+3)(cid:33)
.
n βn i i 128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)
i=1 n,230 Robust deep learning from weakly dependent data
Hence,
(cid:90) 1 P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111) ≤(cid:90) 1 exp(cid:32)
−
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C n1/ ,1(2µ+3)(cid:33)
dε
εn n i=1 βn i i εn 128C n2/ ,2(2µ+3)(1/2)(2µ+3)/(µ+2)
(cid:32) n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)(cid:33)
≤(1−ε )exp − n,1
n 128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)
n,2
(cid:16)
2(µ+1)/(2µ+3)C(µ+2)/(2µ+3)
(cid:17)
(cid:32) n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)(cid:33)
≤ 1− n,1 exp − n,1
n(µ+1)/(2µ+3)C1/(2µ+3) 128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)
n,2 n,2
(cid:32) n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C1/(2µ+3)(cid:33)
≤exp − n,1
128C2/(2µ+3)(1/2)(2µ+3)/(µ+2)
n,2
(cid:32) (cid:33)
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C(µ,L ,L )
≤exp − 1 2 , (6.52)
128(1/2)(2µ+3)/(µ+2)
C1/(2µ+3) (cid:0)
Ψ(1,1)L
(cid:1)1/(2µ+3)
with C(µ,L ,L ):= n,1 = 1 .
1 2 C2/(2µ+3) (cid:16) (cid:0) (cid:1)(cid:17)2/(2µ+3)
n,2 L 2max 23+µ)/Ψ(1,1),1
One can find n =n (µ,L ,L ) such that, for all n>n , we have,
4 4 1 2 4
(cid:90) 1 P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111) ≤exp(cid:32)
−
n1/(2µ+3)2(µ+1)/(2µ+3)(µ+2)C(µ,L 1,L 2)(cid:33)
≤
1
εn n i=1 βn i i 128(1/2)(2µ+3)/(µ+2) n(cid:0) 1−1/r(cid:1) (µ+1)/(2µ+3)
(6.53)
Case 3: 0<ε<ε . We have
n
(cid:90) εn P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111) dε≤(cid:90) εn
dε≤ε =
2(µ+1)/(2µ+3)C n(µ ,1+2)/(2µ+3)
.
0 n i=1 βn i i 0 n n(µ+1)/(2µ+3)C n1/ ,2(2µ+3)
We also get
C(µ+2)/(2µ+3)
4K
(cid:0)
Ψ(1,1)L
(cid:1)(µ+2)/(2µ+3)
n,1 = ℓ 1 β
C1/(2µ+3) (cid:16) (cid:0) (cid:1)(cid:17)1/(2µ+3) n
n,2 L 2max 23+µ/Ψ(1,1),1
Set,
2(µ+1)/(2µ+3)4K (cid:0) Ψ(1,1)L (cid:1)(µ+2)/(2µ+3)
ℓ 1
C (µ,K ,L ,L )= . (6.54)
0 ℓ 1 2 (cid:16)
(cid:0)
(cid:1)(cid:17)1/(2µ+3)
L max 23+µ/Ψ(1,1),1
2
µ+1
Since, β
n
=nr(2µ+3), we have,
(cid:90) εn P(cid:110)1 (cid:88)n
G (h ,Z
)>ε(cid:111)
dε≤
C 0(µ,K ℓ,L 1,L 2)
. (6.55)
0 n i=1 βn i i n(cid:0) 1−1/r(cid:1) (µ+1)/(2µ+3)
Now, set
(cid:32) (cid:33)
(cid:16) (cid:17) r(2µ+3)(µ+2)
n :=max n ,n ,n ,n , 128C(K ,µ)1/(µ+2)(1/2)(2µ+3)/(µ+2)(2(µ+2))3 r(2µ+3)−(µ+1) .
0 1 2 3 4 ℓKengne and Wade 31
From, (6.50), (6.53) and (6.55), we have, for n≥n ,
0
1 (cid:88)n (cid:90) ∞ (cid:110)1 (cid:88)n (cid:111)
E[ G (h ,Z )]≤ P G (h ,Z )>ε dε
n βn i i n βn i i
i=1 0 i=1
C(µ,K ,L ,L )
≤ ℓ 1 2 , (6.56)
(cid:0) (cid:1)
n 1−1/r (µ+1)/(2µ+3)
with C(µ,K ,L ,L ):=2+C (µ,K ,L ,L ). Thus, according (6.39) and (6.56), we have,
ℓ 1 2 0 ℓ 1 2
E Dn(cid:104) n1 (cid:88) i=n 1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) ≤ n(cid:0)C 1−( 1µ /, rK
(cid:1)
(ℓ µ, +L 11 )/, (L 2µ2 +)
3)
+6K ℓE β[| nrY −i| 1r] +3K ℓϵ.
Since, β
n
=n2(µ 2µ+ +1 3),E[|Y i|r ≤M and we have set ϵ= n1 , it holds that,
E Dn(cid:104) n1 (cid:88) i=n 1{−2g((cid:98)h n,Z i)+E
D
n′g((cid:98)h n,Z i′)}(cid:105) ≤ n(cid:0)C 1−( 1µ /, rK
(cid:1)
(ℓ µ, +L 11 )/, (L 2µ2 +)
3)
+ n(1−1/r6 )K (µℓ +M
1)/(2µ+3)
+ 3K nℓ. (6.57)
Let us derive a bound of the approximation error.
Step 2: Bound the second term in the right-hand side of (6.37).
We set H := H (L ,N ,B ,F ,S ) as in equation (6.3). Recall the assumption h∗ ∈ Cs,K(X) for some
σ,n σ n n n n n
1
s,K > 0. Set ϵ = . From [14], there exists a positive constants L ,N ,B ,S > 0 such that with
n ns+s d(1− r1) 0 0 0 0
L n = (cid:0) 1− 1 r(cid:1) ss +L 0 dlog(n),N n = N 0n(cid:0) 1− r1(cid:1) s+d d,S n = (cid:0) 1− 1 r(cid:1) ss +S0 dn(cid:0) 1− r1(cid:1) s+d d log(n), B n = B 0n(cid:0) 1− r1(cid:1) 4s(d s+/s d+1) ,
there is a neural network h ∈H satisfying,
n σ,n
1
∥h −h∗∥ < . (6.58)
n ∞,X ns+s d(1− r1)
According to (6.34) and in addition to (6.58), we have,
K
R(h )−R(h∗)≤ ℓ . (6.59)
Hσ,n ns+s d(1−1/r)
Step 3 : Expected excess risk bound.
According to (6.37), (6.57) and (6.59), we have for all n≥n ,
0
C(µ,K ,L ,L ) 6K M 3K 2K
E[R((cid:98)h n)−R(h∗)]≤
n(cid:0) 1−1/r(cid:1)
(ℓ µ+11 )/(2µ2
+3)
+ n(1−1/r)(µℓ
+1)/(2µ+3)
+ nℓ +
ns+s
d(1−ℓ 1/r).
■
References
[1] Bauer, B., and Kohler, M. On deep learning as a remedy for the curse of dimensionality in nonpara-
metric regression. The Annals of Statistics 47, 4 (2019), 2261–2285.
[2] Chen, G.-y., Gan, M., and Chen, G.-l. Generalized exponential autoregressive models for nonlinear
time series: Stationarity, estimation and applications. Information Sciences 438 (2018), 46–57.32 Robust deep learning from weakly dependent data
[3] Chen, J., Du, Y., Liu, L., Zhang, P., and Zhang, W. Bbspoststimeseriesanalysisbasedonsample
entropy and deep neural networks. Entropy 21, 1 (2019), 57.
[4] Chen, M., and Chen, G. Geometric ergodicity of nonlinear autoregressive models with changing
conditional variances. Canadian Journal of Statistics 28, 3 (2000), 605–614.
[5] Dedecker, J., Doukhan, P., Lang, G., Jose´ Rafael, L. R., Louhichi, S., and Prieur, C. Weak
dependence. In Weak dependence: With examples and applications. Springer, 2007, pp. 9–20.
[6] Diop, M. L., and Kengne, W. Statistical learning for ψ-weakly dependent processes. arXiv preprint
arXiv:2210.00088 (2022).
[7] Doukhan, P., and Doukhan, P. Mixing. Mixing: Properties and Examples (1994), 15–23.
[8] Doukhan, P., and Louhichi, S. A new weak dependence condition and applications to moment
inequalities. Stochastic processes and their applications 84, 2 (1999), 313–342.
[9] Doukhan,P.,andNeumann,M.H.Probabilityandmomentinequalitiesforsumsofweaklydependent
random variables, with applications. Stochastic Processes and their Applications 117, 7 (2007), 878–903.
[10] Fan, J., Gu, Y., and Zhou, W.-X. How do noise tails impact on deep relu networks? arXiv preprint
arXiv:2203.10418 (2022).
[11] Hang, H., and Steinwart, I. Fast learning from α-mixing observations. Journal of Multivariate
Analysis 127 (2014), 184–199.
[12] Imaizumi, M., and Fukumizu, K. Advantage of deep neural networks for estimating functions with
singularity on hypersurfaces. Journal of Machine Learning Research 23, 111 (2022), 1–54.
[13] Jiao, Y., Shen, G., Lin, Y., andHuang, J. Deepnonparametricregressiononapproximatemanifolds:
Nonasymptotic error bounds with polynomial prefactors. The Annals of Statistics 51, 2 (2023), 691–716.
[14] Kengne, W. Excess risk bound for deep learning under weak dependence. arXiv preprint
arXiv:2302.07503 (2023).
[15] Kengne, W., and Wade, M. Penalized deep neural networks estimator with general loss functions
under weak dependence. arXiv preprint arXiv:2305.06230 (2023).
[16] Kengne, W., and Wade, M. Deep learning for ψ-weakly dependent processes. Journal of Statistical
Planning and Inference (2024), 106163.
[17] Kengne, W., and Wade, M. Sparse-penalizeddeepneuralnetworksestimatorunderweakdependence.
Metrika (2024), 1–32.
[18] Kim, J., and Wang, X. Robust sensible adversarial learning of deep neural networks for image classifi-
cation. The Annals of Applied Statistics 17, 2 (2023), 961–984.
[19] Kingma, D. P., and Ba, J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014).Kengne and Wade 33
[20] Kohler, M., and Krzyz˙ak, A. Ontherateofconvergenceofadeeprecurrentneuralnetworkestimate
in a regression problem with dependent data. Bernoulli 29, 2 (2023), 1663–1685.
[21] Kurisu, D., Fukami, R., and Koike, Y. Adaptive deep learning for nonparametric time series regres-
sion. arXiv preprint arXiv:2207.02546 (2022).
[22] Lederer, J. Risk bounds for robust deep learning. arXiv preprint arXiv:2009.06202 (2020).
[23] Ma, M., and Safikhani, A. Theoretical analysis of deep neural networks for temporally dependent
observations. arXiv preprint arXiv:2210.11530 (2022).
[24] Modha, D. S., and Masry, E. Minimum complexity regression estimation with weakly dependent
observations. IEEE Transactions on Information Theory 42, 6 (1996), 2133–2145.
[25] Ohn, I., and Kim, Y. Smooth function approximation by deep neural networks with general activation
functions. Entropy 21, 7 (2019), 627.
[26] Ohn, I., and Kim, Y. Nonconvex sparse regularization for deep neural networks and its optimality.
Neural Computation 34, 2 (2022), 476–517.
[27] Schmidt-Hieber, J. Deep relu network approximation of functions on a manifold. arXiv preprint
arXiv:1908.00695 (2019).
[28] Schmidt-Hieber, J. Nonparametricregressionusingdeepneuralnetworkswithreluactivationfunction.
The Annals of Statistics 48, 4 (2020), 1875–1897.
[29] Shen, G., Jiao, Y., Lin, Y., and Huang, J. Robust nonparametric regression with deep neural
networks. arXiv preprint arXiv:2107.10343 (2021).
[30] Tsuji, K., and Suzuki, T. Estimation error analysis of deep learning on the regression problem on the
variable exponent besov space. Electronic Journal of Statistics 15 (2021), 1869–1908.
[31] Wang, S., and Cao, G. Robust deep neural network estimation for multi-dimensional functional data.
Electronic Journal of Statistics 16, 2 (2022), 6461–6488.
[32] Xu, L., Yao, F., Yao, Q., and Zhang, H. Non-asymptotic guarantees for robust statistical learning
under infinite variance assumption. Journal of Machine Learning Research 24, 92 (2023), 1–46.
[33] Zhang, Y., Fang, Z., and Fan, J. Generalization analysis of deep cnns under maximum correntropy
criterion. Neural Networks (2024), 106226.
[34] Zou, B., Li, L., and Xu, Z. The generalization performance of erm algorithm with strongly mixing
observations. Machine learning 75, 3 (2009), 275–295.