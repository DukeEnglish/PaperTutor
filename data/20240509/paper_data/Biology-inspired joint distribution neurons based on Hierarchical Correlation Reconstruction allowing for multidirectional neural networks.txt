1
Biology-inspired joint distribution neurons
based on Hierarchical Correlation Reconstruction
allowing for multidirectional neural networks
Jarek Duda
Jagiellonian University, Golebia 24, 31-007 Krakow, Poland, Email: dudajar@gmail.com
Abstract—Popular artificial neural networks (ANN) optimize
parameters for unidirectional value propagation, assuming some
guessedparametrizationtypelikeMulti-LayerPerceptron(MLP)
orKolmogorov-ArnoldNetwork(KAN).Incontrast,forbiological
neurons e.g. ”it is not uncommon for axonal propagation of
action potentials to happen in both directions” [1] - suggesting
they are optimized to continuously operate in multidirectional
way. Additionally, statistical dependencies a single neuron could
model is not just (expected) value dependence, but entire joint
distributions including also higher moments. Such agnostic joint
distribution neuron would allow for multidirectional propagation
(of distributions or values) e.g. ρ(x|y,z) or ρ(y,z|x) by sub-
stituting to ρ(x,y,z) and normalizing. There will be discussed
Hierarchical Correlation Reconstruction (HCR) for such neu-
(cid:80)
ron model: assuming ρ(x,y,z) = a f (x)f (y)f (z) type
ijk ijk i j k
parametrization of joint distribution with polynomial basis f ,
i
which allows for flexible, inexpensive processing including non-
linearities, direct model estimation and update, trained through
standard backpropagation or novel ways for such structure
up to tensor decomposition. Using only pairwise (input-output)
dependencies, its expected value prediction becomes KAN-like
withtrainedactivationfunctionsaspolynomials,canbeextended
by adding higher order dependencies through included products
- in conscious interpretable way, allowing for multidirectional
propagation of both values and probability densities.
Keywords:machinelearning,neuralnetworks,Kolmogorov-
Arnold Network, joint distribution, conditional distribution,
Bayesian Neural Networks, tensor decomposition
Figure1. TheproposedHCRneuronandneuralnetwork.Top:orthonormal
polynomial basis assuming normalization to uniform distribution in [0,1].
I. INTRODUCTION
Middle: HCR neuron containing and applying joint distribution model for
Biological neurons use complex propagations of action po- d = 3 variables, and gathered formulas for direct estimation/model update,
its application to propagate entire distributions and expected values alone.
tentials, travelling both directions of e.g. axons [1]. They
Such density parametrization can drop below 0, what is usually repaired
have access to information from connected neurons, which is by calibration e.g. use normalized max(ρ,0.1) density, however, for neural
more than value dependence - a neuron could model up joint networkswithinter-layernormalizationthisissueseemsnegligible-ignoring
it essentially simplifies calculations to the shown formulas. Propagating only
distribution of its connections, and it should be evolutionar-
expected values and normalizing, we can use only the marked nominators
ily optimized to include such additional information if only - as in KAN optimizing nonlinear functions (polynomial here) by including
beneficial. In contrast, arbitrarily chosen popular parametriza- only pairwise dependencies (a with two nonzero indexes), extending to their
productstoconsciouslyincludehigherorderdependencies.Bottom:schematic
tion types like Multi-Layer Perceptron (MLP) [2] as trained
HCR neural network and some training approaches of intermediate layers
linear combination and fixed nonlinear activation function, or - which in HCR can be treated as values or their distributions (replacing
Kolmogorov-Arnold Network (KAN) [3] additionally training
fi(u)withitsi-thmoment:(cid:82) 01ρ(u)fi(u)du).Thereisalsovisualizedtensor
decomposition approach - estimate dependencies (e.g. pairwise) for multiple
activation functions, are optimized for propagation in one
variables and try to automatically reduce it to multiple dependencies of a
direction, and work only on values not distributions. smallernumbersofvariableswithalgebraicmethods.
To reach multidirectional propagation, there could be used
Bayes theorem e.g. in Bayesian Neural Networks [4] - which
in practice use relatively simple models. To include more (KDE) [6] which is impractical in higher dimension.
detailed description of complex dependencies, we could model HierarchicalCorrelationReconstruction(HCR)([7],[8],[9],
the entire joint distribution, substitute and normalize to get [10], [11], [12])1, used for such artificial neurons as in Fig.
conditionaldistributionswithoutBayestheorem.However,joint 1, allow to overcome these difficulties by representing joint
distributionsofcontinuousvariablesbecomequitecomplicated, distributionofnormalizedvariablesasjustalinearcombination
difficulttodescribeandhandle.Classicalapproachesarecopu-
las[5]buttheyarelowparametric,orkerneldensityestimation 1HCRintroduction:https://community.wolfram.com/groups/-/m/t/3017754
4202
yaM
8
]GL.sc[
1v79050.5042:viXra2
Figure 2. Visualized HCR polynomial [0,1] basis in d = 1 dimension and product bases for d = 2,3. For d = 3 we assume ρ(x,y,z) =
(cid:80) ijka ijkfi(x)fj(y)f k(z). As f0 = 1, zero index in a ijk means independence from given variable, hence a000 = 1 corresponds to normalization,
ai00,a0i0,a00i for i ≥ 1 describe marginal distributions through i-th moments. Then aij0,ai0j,a0ij for i,j ≥ 1 describe pairwise joint distributions
throughmixedmoments,andfinallya
ijk
fori,j,k≥1describetriplewisedependencies-weliterallygethierarchicalcorrelationreconstructionbymoments
relatingdistributionsofincreasingnumbersofvariables,withclearinterpretationofcoefficientsofe.g.trainedHCR-basedneuralnetwork.
(cid:80) (cid:81)
ρ(x)= a f (x ),wherebyboldfonttherearedenoted x ↔ CDF(x) becoming its estimated quantile. This CDF can
j j i ji i
vectors. Using orthonormal polynomials: (cid:82)1 f (x)f (x)dx = be estimated by some parametric distribution with parameters
0 i j
δ , the coefficients are inexpensive to estimate and update, estimated from dataset, or be empirical distribution function:
ij
literally providing hierarchical correlation decomposition with x becomes its position in dataset rescaled to (0,1). For neural
(mixed) moments as in Fig. 2. While generally such density as networks normalization is popular to be made in batches [13],
a linear combination can get below 0, what usually is repaired here needed to be used between layers, in practice should be
bycalibration,forneuralnetworkswithnormalizationbetween nearly constant between layers, can be approximated, put into
layers this issue could be just neglected, essentially reducing tables, inversed for backward propagation.
computational costs. For d normalized variables: x ∈ [0,1]d, in HCR we repre-
This article introduces to HCR from perspective of neural sent joint distribution as a linear combination, conveniently
network applications, earlier suggested in [7]. The current in some product basis B:
versionisinitial-tobeextendedinthefuturee.g.withpractical d
(cid:88) (cid:88) (cid:89)
realizations, examples, e.g. replacing MLP, KAN. ρ(x)= a f (x)= a f (x ) (1)
j j j ji i
j∈B j∈B i=1
II. HCRNEURALNETWORKS(HCRNN)
Assuming orthonormal basis:
(cid:82)1
f (x)f (x)dx = δ , static
0 i j ij
This main Section introduces to HCR and discusses it as estimation[14](mean-squarederrorbetweensmoothedsample
basic building block of neural network. and parametrization) from X¯ dataset becomes just:
d
1 (cid:88) 1 (cid:88) (cid:89)
A. HCR introduction a = f (x)= f (x ) (2)
j |X¯| j |X¯| ji i
Asincopulatheory[5],itisconvenienttousenormalization x∈X¯ x∈X¯i=1
of variables to nearly uniform distribution in [0,1]. It requires We assume here orthonormal polynomial basis (rescaled
transformationthroughcumulativedistributionfunction(CDF): Legendre), allowing to interpret coefficients as moments of3
normalized variables, becoming approximately expected value, Having such conditional distribution, we can for example
variance, skewness, kurtosis. Alternatively we could use var- calculate expected value e.g. to be propagated by neural net-
ious trigonometric bases (e.g. DCT, DST), localized like in works. For polynomial basis expected values contributions
√
(cid:82)1 (cid:82)1
wavelets or finite elements methods, or different normalization are: xf (x)dx = 1/2, xf (x)dx = 1/ 12, and zero for
0 0 0 1
e.g. to Gaussian distribution, times Hermite polynomials for higher,leadingtoformulasincludingonlythefirstmomentasin
orthonormal basis. Fig. 1. As further there is rather required normalization which
As in Fig. 2 and f =1, a coefficients are mixed moments both shifts and rescales, in practice it is sufficient to work on
0 j
of {i:j ≥1} variables of nonzero indexes, independent from suchmarkednominators-optimizingsinglenonlinearfunction
i
variables of zero indexes, allowing for literally hierarchical as in KAN for pairwise (input-output) dependencies, or their
decomposition of statistical dependencies: start with a = 1 products to extend to include multi-variate dependencies.
0..0
for normalization, add single nonzero index coefficients to
describemarginaldistributions,thenaddpairwisedependencies
C. Distribution propagation and tensor decomposition
with two nonzero indexes, then triple-wise, and so on. For
Let us start with a simple example: that we would like to
example a coefficient would describe dependence between
2010
find
2nd moment of first variable and 1st moment of 3rd variable (cid:80)
among d = 4 variables. Generally the selection of basis ρ(x|y)=(cid:88) f i(x) (cid:80)j aa ijf fj( (y y)
)
(5)
B is a difficult question e.g. to use pairwise only up to i j 0j j
fixed m moment, preferably optimized during training, maybe but for y being from ρ(y) = (cid:80) f (y)b probability den-
k k k
separately for each neuron or layer. Such decomposition also sity. So the propagated probability density of x should be
allows to efficiently work with missing data by using to (cid:82)1 ρ(x|y)ρ(y)dy. Approximating with constant denominator,
estimate/update/propagateonlyacoefficientswithzeroindexes 0 (cid:82)1
using f (y)f (y)dy =δ and finally normalizing, we get:
for the missing variables. 0 j k jk
While static estimation averages over dataset with equal ρ(x)←(cid:90) 1
ρ(x|y)ρ(y)dy
≈(cid:88)
f
(x)(cid:80) ja ijb j
(6)
weights, for dynamic updating we should increase weights i (cid:80) a b
for recent values, e.g. using computationally convenient expo- 0 i j 0j j
nential moving average: for some small memory parameter λ Such approximation allows to propagate (in any direction)
through HCR neurons not only values, but also entire
x (cid:89)
a −→(1−λ)a +λ f (x ) (3) probability distributions - by just replacing f (y) for concrete
j j ji i j
i value of y, with b j describing its probability distribution.
It is easy to generalize, e.g. for ρ(x|y,z) we could replace
However, modelling (joint) density as a linear combination
(cid:80)
f (y)f (z) with b when ρ(y,z)= b f (y)f (z).
cansometimesleadtonegativedensities-toavoidthisissue, j k jk jk jk j k
thereisusuallyusedcalibration:insteadofthemodelleddensity
AnalogouslyforintermediatelayerslikeinthebottomofFig.
ρ,usee.g.max(ρ,0.1)anddividebyintegraltoremainnormal-
1 - integrating over the intermediate variables, thanks to basis
ized density. However, it makes computations more difficult,
orthogonality we get Kronecker delta enforcing equality of
especiallyinhigherdimension-forneuralnetworkapplications
intermediateindexes,leadingtoconditionforapproximationof
we should be able to ignore this issue to simplify calcula-
higher order tensors with lower order ones, which is generally
tions, especially working on expected values and normalizing
studiedbytensordecompositionfield[15]-hopefullyleading
between layers. Therefore, we ignore this issue/calibration in
to better training approaches.
this article, however, it should be remembered, maybe adding
calibration for some applications.
D. Basis optimization and selection
Another direction is application of the found a coefficients,
B. Conditional distributions and expected value propagation j
for example to optimize the arbitrarily chosen {f } basis to
i
Having (1) model of joint distribution, to get conditional
be able to reduce the number of considered coefficients, also
distribution we need to substitute known variables and nor-
to reduce overfitting issues, e.g. discussed in [12], [16]. For
malize dividing by integral:
this purpose we can for example treat current coefficients
ρ(x 1|x 2,...,x d)=
(cid:82)1(cid:80)(cid:80) ja jf j1(x 1)f j2(x 2)...f jd(x d)
=
a res ma air ne ic nt gan ig nu dl ea xr esm fa ot rrix allM cj o1 n,j s2 id..j ed red:= coa ej ffi- ciw enit th
s
ib nlo thck ee bd asth ise
.
a f (x )f (x )...f (x )dx
0 j j j1 1 j2 2 jd d 1 Now we can use SVD (singular value decomposition): find
orthonormal eigenbasis of MMT = (cid:80) σ u uT and use
=(cid:88)
j1
f j1(x 1)(cid:80) (cid:80)j j2 2. .. .. .j jd da aj 01 jj 22 .. .. jj dd ff jj 22 (( xx 22 )) .. .. .. ff jj dd (( xx dd )) (4) g eii ge= nv(cid:80) ectj ou rsij .f Sj imas ilat rh le
y
wne ew cab nas dis
o
f fo or
r
to hn ee reoi mr aai infi ie nw gi vd ao rm iai bn la en st
,
getting separate or common optimized bases for them.
(cid:82)1
as f (x)dx=δ .Suchsums forpairwisedependenciesuse
0 i i0
onlytwononzeroj indexes(input-output),threefortriplewise, Amoredifficultquestionisbasisselection-whichj∈B in-
i
and so on. Denominator corresponds to normalization, indeed dexestouseinconsideredlinearcombinationsforeachneuron.
the fraction becomes 1 for j = 0. Examples for d = 3 are Extendingalltom-thmoment/degreefordvariables,wewould
1
shown in Fig. 1 - generally nominator sums over all indexes need(m+1)d =(cid:80)d (cid:0)d(cid:1) mkcoefficients:1fornormalization,
k=0 k
with the current indexes of predicted variables, denominator dm for marginal distributions, d(d−1)m2/2 for pairwise, and
replaces them with zeros for normalization, could be removed so on. With proper normalization the coefficients for marginal
if having further (inter-layer) normalization. distributionsshouldbecloseto0-canbeneglected.Toreduce4
the number of coefficients, we can restrict e.g. up to pairwise
dependencies (≈ KAN). Generally we can e.g. calculate more
coefficients and discard those close to zero. Using optimized
bases as above, should allow to reduce their size.
E. Some HCRNN training approaches
A single HCR neuron models multidimensional joint dis-
tribution, what is already quite powerful. However, for neural
networks the main difficulty is training the intermediate layers.
Here are some approaches: Figure3. OmnidirectionalHCRneuronproposedin[7]-gettinganysubsetS
• Treat HCRNN as just a parametrization and use standard
ofconnectionsasinput,itcanupdatemodelfora jcoefficientspositiveonlyin
thissubset:{i:ji≥1⊂S},andpredict/propagatetooutputastheremaining
backpropagation like for other neural networks. It can be connectionse.gexpectedvaluesfortheseinputs,forexampleaccumulatedup
mixed with other techniques, like static parameter estima- tosomethreshold,includingsignforexcitatory/inhibitory.
tion/update from recent values, online basis optimization
and selection, etc.
• While the discussed neurons containing joint distribution
• Maybe find initial intermediate values by dimensionality
models seem very powerful and flexible, directly working
reduction like PCA of {f (x):j∈B} vectors of features
j in high dimensions they have various issues - suggesting
as (nonlinear) products of functions of inputs.
to directly predict conditional distributions instead with
• Maybe use propagation as e.g. expected values in both
HCR parametrization ([8], [9], [11], [12]), what might be
directions, combine with coefficient estimation/update.
alsoworthincludedinneuralnetwork,e.g.asapartofthe
• Maybe use some tensor decomposition techniques - start
training process - to be decomposed into single neurons.
with estimation of e.g. pairwise dependencies for a larger
set of variables, and use algebraic methods to try to
REFERENCES
approximate it with multiple lower order tensors.
[1] R.Follmann,E.RosaJr,andW.Stein,“Dynamicsofsignalpropagation
While backpropagation is available for various parametriza-
and collision in axons,” Physical Review E, vol. 92, no. 3, p. 032707,
tions, such HCRNN hiding joint distribution models in each 2015.
neuron bring some additional novel possibilities - hopefully [2] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward
networks are universal approximators,” Neural networks, vol. 2, no. 5,
allowing for faster training.
pp.359–366,1989.
CoefficientsofsuchtrainedHCRNNremainmixedmoments [3] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljacˇic´,
-providingdependencyinterpretationbetweeninput/outputand T.Y.Hou,andM.Tegmark,“Kan:Kolmogorov-arnoldnetworks,”arXiv
preprintarXiv:2404.19756,2024.
hidden intermediate variables, allowing for multidirectional
[4] I. Kononenko, “Bayesian neural networks,” Biological Cybernetics,
propagation of values or distributions like in Fig. 3, and its vol.61,no.5,pp.361–370,1989.
parameters can be further continuously updated e.g. using (3). [5] F. Durante and C. Sempi, “Copula theory: an introduction,” in Copula
theoryanditsapplications. Springer,2010,pp.3–31.
[6] G.R.TerrellandD.W.Scott,“Variablekerneldensityestimation,”The
III. CONCLUSIONSANDFURTHERWORK AnnalsofStatistics,pp.1236–1265,1992.
[7] J. Duda, “Hierarchical correlation reconstruction with missing data, for
Neuronswithjointdistributionmodelsseempowerfulagnos- exampleforbiology-inspiredneuron,”arXivpreprintarXiv:1804.06218,
ticimprovementforcurrentlypopularguessedparametrizations 2018.
[8] J. Duda and A. Szulc, “Social benefits versus monetary and multidi-
like MLP or KAN, and are practically accessible with HCR,
mensionalpovertyinpoland:Imputedincomeexercise,”inInternational
up to omnidirectional neurons like in Fig 3 - allowing to ConferenceonAppliedEconomics. Springer,2019,pp.87–102,preprint:
freely choose inference directions, propagate both values and https://arxiv.org/abs/1812.08040.
[9] J. Duda, H. Gurgul, and R. Syrek, “Modelling bid-ask spread con-
probability distributions, with clear coefficient interpretations.
ditional distributions using hierarchical correlation reconstruction,”
However, mastering such new neural network architecture Statistics in Transition New Series, vol. 21, no. 5, 2020, preprint:
will require a lot of work, planned also for future versions of https://arxiv.org/abs/1911.02361.
[10] J. Duda and G. Bhatta, “Gamma-ray blazar variability: new statistical
this article. Here are some basic research directions:
methodsoftime-fluxdistributions,”MonthlyNoticesoftheRoyalAstro-
• Search for practical applications, from replacement of nomicalSociety,vol.508,no.1,pp.1446–1458,2021.
standard ANN, for multidirectional inference e.g. in [11] J. Duda and S. Podlewska, “Prediction of probability distributions of
molecularproperties:towardsmoreefficientvirtualscreeningandbetter
Bayes-like scenarios, as neural networks propagating understandingofcompoundrepresentations,”MolecularDiversity,pp.1–
probability distributions, up to exploration of similar- 12,2022.
ity/replacement for biological neurons. [12] J. Duda and G. Bhatta, “Predicting conditional probability distributions
ofredshiftsofactivegalacticnucleiusinghierarchicalcorrelationrecon-
• Practical implementation, optimization especially of struction,”MonthlyNoticesoftheRoyalAstronomicalSociety,p.stae963,
trainingandupdate,basisoptimizationandselectiontech- 2024.
niques, exploration of tensor decomposition approach. [13] S.IoffeandC.Szegedy,“Batchnormalization:Acceleratingdeepnetwork
trainingbyreducinginternalcovariateshift,”inInternationalconference
• Working on probability distributions makes it natural for onmachinelearning. pmlr,2015,pp.448–456.
information theoretic approaches, hopefully leading to [14] J. Duda, “Rapid parametric density estimation,” arXiv preprint
better understanding e.g. of information propagation arXiv:1702.02144,2017.
[15] T.G.KoldaandB.W.Bader,“Tensordecompositionsandapplications,”
during learning/inference, information held by intermedi- SIAMreview,vol.51,no.3,pp.455–500,2009.
ate layers, phenomena like information bottleneck [17], [16] J.Duda,“Fastoptimizationofcommonbasisformatrixsetthroughcom-
• Addingtimedependencelikemodelupdate,alsoforsim- mon singular value decomposition,” arXiv preprint arXiv:2204.08242,
2022.
ilaritywithbiologicalneurons,e.g.longtermpotentiation,
[17] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
connection to various periodic processes/clocks. method,”arXivpreprintphysics/0004057,2000.