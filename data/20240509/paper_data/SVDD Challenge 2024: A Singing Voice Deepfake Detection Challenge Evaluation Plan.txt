SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge
Evaluation Plan
YouZhang∗1,YongyiZang∗1,JiatongShi∗2,RyuichiYamamoto∗3,JionghaoHan2,YuxunTang4,
TomokiToda3,ZhiyaoDuan1
1UniversityofRochester,Rochester,NY,USA2CarnegieMellonUniversity,Pittsburgh,PA,USA
3NagoyaUniversity,Nagoya,Japan4RenminUniversityofChina,Beijing,China
svddchallenge@gmail.com
https://challenge.singfake.org
Abstract targetsbothcontrolledandin-the-wildsettings, aimingtodis-
TherapidadvancementofAI-generatedsingingvoices,which tinguishbonafideandAI-generatedsingingvoicerecordings.
nowcloselymimicnaturalhumansingingandalignseamlessly
withmusicalscores,hasledtoheightenedconcernsforartists 2. Challengeobjectives
and the music industry. Unlike spoken voice, singing voice
presents unique challenges due to its musical nature and the TheSVDDchallengeaimstobringtogethertheacademicand
presence of strong background music, making singing voice industrialcommunitiestodevelopinnovativeandeffectivetech-
deepfake detection (SVDD) a specialized field requiring fo- niquesfordetectingdeepfakesingingvoices.Wehopethischal-
cusedattention. TopromoteSVDDresearch,werecentlypro- lenge will advance our understanding of the specific traits of
posedthe“SVDDChallenge,”theveryfirstresearchchallenge singing voice deepfakes and contribute to the broader field of
focusingonSVDDforlab-controlledandin-the-wildbonafide multimediadeepfakedetection.
and deepfake singing voice recordings. The challenge will
be held in conjunction with the 2024 IEEE Spoken Language 3. Challengesetups
TechnologyWorkshop(SLT2024).
In the context of singing voice deepfakes, a common practice
IndexTerms:singingvoicedeepfakedetection,anti-spoofing
istopresenttheseartificialcreationsalongsidebackgroundmu-
sic, as observed in our SingFake [2] project. This approach,
1. Introduction
whilepracticalforsimulatingauthenticsongpresentations,in-
The development of advanced singing voice synthesis tech- troducesasignificantchallenge: theseparationofvocalsfrom
niques has led to a significant milestone in AI-generated con- musicmaycreateartifactsthatcanobscurethedifferencesbe-
tent,wheresingingvoicessoundremarkablynaturalandalign tween bonafide and deepfake vocals. To investigate this is-
seamlessly with music scores. These synthesized voices can suethoroughly,theSVDDchallengeisstructuredintotwodis-
nowemulatethevocalcharacteristicsofanysingerwithmin- tinct tracks: the controlled and the in-the-wild settings. The
imal training data. While this technological advancement is WildSVDD track follows the same approach as our SingFake
impressive, ithassparkedwidespreadconcernsamongartists, project[2], dealingwithdeepfakesastheytypicallyappearin
recordlabels,andpublishinghouses[1].Thepotentialforunau- onlinemedia,completewithbackgroundmusic.Incontrast,the
thorizedsyntheticreproductionsthatmimicwell-knownsingers CtrSVDDtrackexclusivelyusesclean,unaccompaniedvocals
posesarealthreattooriginalartists’commercialvalueandin- providedbyourdatacontributors,therebyminimizingtheinter-
tellectualpropertyrights,ignitingurgentcallsforefficientand ferenceofvoiceseparationalgorithms.Thistwo-trackapproach
accuratemethodstodetectthesedeepfakesingingvoices. lets participants tackle the challenges of identifying deepfake
In response to these concerns, our prior research [2] has singingvoicesunderdifferentandrealisticconditions.Thissec-
laidthegroundworkfortheemergingfieldofSVDD.Weintro- tiondescribesourdatasetcurationforsettingupthetwotracks.
ducedtheSingFakedataset,acomprehensivecollectionofau-
thenticanddeepfakesongclipsfeaturingavarietyoflanguages 3.1. CtrSVDD:Controlledsingingvoicedeepfakedetection
and singers. Our findings revealed a critical insight: conven-
FortheCtrSVDDtrack,wefirstsourcebonafidedatasetsfrom
tional speech countermeasure (CM) systems, when trained on
existing open-source singing recordings. These include Man-
standard speech, experience significant performance degrada-
darinsingingdatasets:Opencpop[3],M4Singer[4],Kising[5],
tionwhentestedonsingingvoices.Conversely,retrainingthese
Official ACE-Studio release, and Japanese singing datasets:
systems specifically on singing voice data resulted in notable
Ofuton-P1, Oniku Kurumi2, Kiritan [6], and JVS-MuSiC [7].
improvements. Our prior evaluations also highlighted several
Weperformsegmentationtodividethesongsintovocalclips.
challenges,includingdealingwithunseensingers,variouscom-
We then generate deepfake singing vocals with 14 exist-
municationcodecs,diverselanguagesandmusicalcontexts,and
ing singing voice synthesis (SVS) and singing voice conver-
interference from accompaniment tracks. This highlights the
sion (SVC) systems from these bonafide vocals. For SVS,
distinctnatureofsingingvoicedeepfakesandthenecessityfor
weemployESPnet-Muskits[8], NNSVS[9], DiffSinger[10],
specializedSVDDsystems.
and ACESinger3. For SVC, we apply the NU-SVC [11] and
To advance the field of singing voice deepfake detection,
weintroducetheSVDDchallenge,theinauguralresearchinitia-
tivespecificallydedicatedtoexploringSVDD.Thischallenge
1https://sites.google.com/view/oftn-utagoedb/
%E3%83%9B%E3%83%BC%E3%83%A0
*Theseauthorscontributedequally.Version1.2.Updated:May9, 2https://onikuru.info/db-download/
2024.Correspondenceaddressedtoyou.zhang@rochester.edu 3https://acestudio.ai/
4202
yaM
8
]SA.ssee[
1v44250.5042:viXraSingFake: Singing Voice Deepfake Detection
Yongyi Zang*, You Zhang* (Equal contribution), Mojtaba Heydari, Zhiyao Duan
Audio Information Research Lab, University of Rochester https://singfake.org
TLDR We propose the novel task of singing voice deepfake detection (SVDD) and present our collected dataset SingFake.
Background SingFake Dataset
Singing Deepfakes cause public distrust and industry concern. We collected 28.93 hours of bonafide, 29.40 hours of deepfake song
clips from popular social media platforms.
We manually annotate AI Singer (if deepfake), language, website,
and whether it is a “deepfake” or “bonafide” song.
VAD T01: Seen singer, unseen song clips
Mixture
PyAnnote during training
T02: Unseen singer, unseen song
Demucs [1] clips during training
Vocals
Pretrained T03: T02 through 4 lossy codecs
T04: Unseen musical context and
language during training
(Each slice denotes a singer; T03 excluded)
All audio samples were resampled to 16kHz; For stereo songs, one
channel is randomly selected for training and evaluation.
Challenges with Singing Voice Deepfake Detection
Table1: Summaryofthetraininganddevelopmentpartitionof
Speech countermeasures (CMs) are designed to catch speech deepfakes. False Rejection Rate theCtrSVDDdataset.
False Acceptance Rate
We select AASIST [2], Spectrogram+ResNet, LFCC+ResNet [3] and Wav2Vec2+AASIST [4]
Bonafide Deepfake
Partition #Speakers
To represent 4 SOTA speech CMs with different input features.
#Utts #Utts AttackTypes
For a fair comparison, the Equal Error Rate (EER) is used for comparing model performance.
Equal Error Rate Train 59 12,169 72,235 A01∼A08
Dev 55 6,547 37,078 A01∼A08
Threshold
Figure1:IllustrationofEqualErrorRate(EER). ticipantstogenerateascoreTXTfile,whichcontainsscoresfor
Song Seen Unseen Unseen Unseen Unseen everysegmentedclip. Thesescoresreflectthesystem’sconfi-
Generalization to unseen is still challenging, denceinwhetherthevocalorsongcliporiginatesfromabona
calling for community effort.
Singer Seen Seen U van ris ane tse on fSo-VITSU -Sn Vs Ce4.e Wn hendividiU ngn ths ee ge enn
erateddeep-
fidesingerorresemblesarealsinger. Ahigherscoreindicates
greaterconfidencethattheclipisfromarealsinger. Inprac-
fakesintotrain,development,andevaluationsplits,wefollow
Codec Seen Seen Seen Unseen Unseen ticalusage,peoplemaysetathresholdtodeterminethebinary
theapproachusedinthespeechdeepfakedetectionbenchmark
outputofbonafideordeepfake. Withthethresholdhigher,the
ASVspoof2019. Weemploythesamesetofdeepfakegenera-
Context Seen Seen Seen Seen Unseen falseacceptanceratewillbecomelower,andthefalserejection
tion algorithms (A01-A08) for both the training and develop-
ratewillbecomehigher. TheEERisachievedwhenthesetwo
mentsets, whileusingadifferentsetofdeepfakes(A09-A14)
areequal,asillustratedinFigure1. TheEERismoresuitable
fortheevaluationpartition. Weplantoreleasemoredetailson
toevaluatethesystem’sperformancethanaccuracysinceitis
deepfakesystemsinearlyJune.
notdependentonthethreshold. ThelowertheEER,thebetter
thesystemdistinguishesbonafideanddeepfakesingingvoices.
3.2.WildSVDD:In-the-wildsingingvoicedeepfakedetection
We have continued to gather data annotations from social 5. Protocols
media platforms following a method similar to the SingFake
project[2]. TheWildSVDDdatasethasbeenexpandedtoap- 5.1. CtrSVDDtrack
proximatelydoubletheoriginalsizeofSingFake,nowfeaturing
TheCtrSVDDdataisreleasedunderaCC-BY-NC-ND4.0li-
97singerswith3223songs. Theannotators, whoarefamiliar
cense, aligned with the sourcing corpora. We have released
withthesingerstheycover,manuallyverifiedtheuser-specified
the training and development set on Zenodo5 and other rele-
labelsduringtheannotationprocesstoensureaccuracy, espe-
vantscriptsonGitHub6. Pleasebeawarethatthetrainingand
ciallyincaseswherethesinger(s)didnotactuallyperformcer-
developmentsetavailableonZenodoisincompletebecauseof
tainsongs. Wecross-checktheannotationsagainstsongtitles
licensing issues. To fully generate the dataset, first download
and descriptions, and manually review any discrepancies for
alltheremainingbonafidedatasetsonyourownbyagreeingto
further verification. We have verified the accessibility of all
theirtermsandconditions,andthenfollowthedetailedinstruc-
URLsinthedatasetasofMarch28thandremovedanythatwere
tionsprovidedintheGitHubrepository. Participantscanrefer
inaccessible. The WildSVDD dataset now includes Korean
tothestatisticsinTable1asaguidetoverifythecompletionof
singers, making Korean the third most represented language
SVDD Challenge 2024 @ IEEE SLT Workshop References theirdownloadsandgeneration.
in the dataset. To help track changes between the SingFake
For evaluation, we have released the test set on Zenodo7
andWildSVDDdatasets,wehaveaddeda”SingFake Set”col-
with undisclosed labels at a later date and ask teams to score
Organizers: You Zhang (UR), Yongyi Zang (UR), Jiatong Shi (CMU), umnthatindicatestheoriginalpartitionofanannotationinthe
each song vocal clip. There are, in total, 48 speakers with
[1] Simon Rouard, Francisco Massa,S ainngdF Aakleexdaantdasreet .DAefnonsosteazt,i o´ n“Hsythbartidla tcrkanasvfoalrumeeinrst fhoisr column
Ryuichi Yamamoto (Nagoya), Tomoki Toda (Nagoya), Zhiyao Duan (UR) 92,769clips.Usingthesubmittedscores,wewillcalculateand
music source separation,” in Proc. IaErEeEn Ienwteardndaittiioonnaslt CoothnefeWreinldcSeV oDn DAcdoautassteict.s, Speech
rankparticipantsystemsusingEER.Forsubmissionguidelines,
and Signal Processing (ICASSP), 2023. Duetopotentialcopyrightissues,wearecurrentlyonlyre-
pleaserefertoSection9.
leasing the annotations. Consequently, participants might ac-
[2] Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin
CtrSVDD Track: Deepfake vocals generated with existing singing voice
quireslightlydifferentmediafilesthatcorrespondtothesame
Lee, Ha-Jin Yu, and Nicholas Evans, “AASIST: Audio anti-spoofing using integrated 5.2. WildSVDDtrack
synthesis and singing voice conversion systems annotations,dependingonthespecificsoftheirdownloadpro-
spectro-temporal graph attention networks,” in Proc. IEEE International Conference on
cess. Duetothisvariability,self-reportedmetricsfrompartic- TheWildSVDDtrackdatahasbeenreleasedonZenodo8under
WildSVDD Track: Extended set of SingFake Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6367–6371.
ipants can, at best, be used as a rough reference and cannot aCC-BY4.0license.Wesupplythetrainingandtestpartitions,
[3] You Zhang, Fei Jiang, and Zhiyao Duan, “One-class learning towards synthetic voice
be directly used to compare systems. As such, we encourage allowingparticipantstheflexibilitytocarveoutavalidationset
spoofing detection,” IEEE Signal Processing Letters, vol. 28, pp. 937–941, 2021.
participants to report the success rate of URL downloads per fromthetrainingdataformodeldevelopment. Weprovidela-
Register by June 8th, 2024
[4] Hemlata Tak, Massimiliano Todipscaort,i tXioinn Wanadn,gi,f Jepeo swsiebolen, Jtuhnega, cJutunailchfiil eYsamusaegdisfhori, tarnaidn ingand belsofSingFake[2]partitionsforannotationsthatappearedin
Results submission by June 15th, 2024 Nicholas Evans, “Automatic speakerte vsetirnifigc.aTthioisn tsrpanosopfianregn acnydw dileleapllfoawker edseetaerccthieorns utosimnga kefairer the SingFake dataset for easy comparison with previous sys-
wav2vec 2.0 and data augmentationco,”m inp aPrriosocn. sT.heA Sdpdeitaioknearl layn,dp aLratincgipuaangtes Rareecoegnncoituiorang edtode- tems. The test set is divided into parts A and B, with part B
Workshop (Odyssey), 2022, pp. 112s–c1r1ib9e.theirmodeldesignsclearlyandopen-sourcetheirmodel consideredmorechallengingduetoitsinclusionofunseenmu-
https://challenge.singfake.org
implementationstofacilitatethereproductionofresultswiththe sicalcontextsasT04inSingFake[2].
WildSVDDdataset. Werecommendthatparticipantsfurthersegmentthesongs
intoclipsusingourtoolavailableintheSingFakeGitHubrepos-
4. Evaluationmetrics
5https://zenodo.org/records/10467648
WeuseEqualErrorRate(EER)toevaluatetheSVDDsystem 6https://github.com/SVDDChallenge/CtrSVDD_
performance. WeexpecteachSVDDsystemsubmittedbypar- Utils
7https://zenodo.org/records/10742049
4https://github.com/HANJionghao/so-vits-svc2 8https://zenodo.org/records/10893604
)%(
etaRitory9. Evaluations should be conducted at the segment level
Front-end
rather than at the song level. We will adopt the self-reported
Batchnorm
EERandwillnotaccuratelyranktheresults.Weencouragethe
participantstosubmitthescorefileslistingtheURLs,segment SELU
index,andthecorrespondingscoresoutputfromtheirsystems.
Conv
6. RulesfordevelopingSVDDsystems Batchnorm
SELU Element-wise
Participantsarewelcometouseanypubliclyavailabledatasets Maximum
for training in addition to the CtrSVDD we provide, but of Conv
course, excludeanydatasetsusedinourtestset. Specifically,
fortheCtrSVDDtrack,participantsmustNOTuseM4Singer,
KiSing,anyopen-sourceddeepfakemodelsbasedonM4Singer MaxPool
and/orKiSing,orthecommercialsoftwareACEStudio10.
ResBlock
Werefertheparticipantstothelistofavailabledatasetsat
theendofthissection. However, participantsmustdocument
anyadditionaldatasourcesusedintheirsystemdescriptions.If Utterance Concatenation
thereareanypublicdatasourcesnotlistedbutyouwouldlike Score
tousefortraining,pleaseinformtheorganizerssothatwecan GAT Backbone
share this information among participants. We will maintain
andupdatethelistofdatasourcesuntiltheregistrationdeadline. Figure 2: Baseline systems architecture. We adjust the linear
layerbeforetheGATbackbonetoadaptfordifferentfront-end
If the participants are willing to generate new training
dimensionalities.MoredetailsofHS-GALareavailablein[33].
data from our released data and other public datasets for the
CtrSVDD track, they can request permission to use such data
under the condition that this new dataset will be published to
other participants. We have set a deadline for this request. • Singing Voice DeepFake Detection datasets: SingFake [2],
PleaserefertoSection11fordetails. FSD[32].
Theuseofpubliclyavailablepre-trainedmodelsisalsoper-
mitted.Participantsshouldspecifytheexactversionofthepre- 7. Baselines
trained models used and provide a link to the pre-trained em-
beddingusedinthesystemdescription. Wehavedevelopedtwobaselinesystemsforthechallenge:one
Any private data or private pre-trained models are strictly that uses raw waveforms and another that employs linear fre-
forbiddentouse. quencycepstralcoefficients(LFCCs)asfront-endfeatures.The
Participants should not add additional annotations to the architectureofthebaselinesystemsisshowninFigure2.
WildSVDDtrackfortraining. Pleasecontacttheorganizersif The raw waveform system is based on the AASIST [33],
you are interested in contributing more annotations for future withseveralmodifications: 1)Wereducedthenumberofout-
research. putclassesfromtwotoone.2)Weadoptedbinarycrossentropy
Belowweprovidealistofknowndatasourcesasarefer- withfocallossfortraining,settingthefocusingparameter(γ)
ence.ThislistappliestobothCtrSVDDandWildSVDDtracks. to2.0andtheweightforpositiveexamples(α)to0.25. 3)We
omittedstochasticweightaveraging. 4)Weimplementedaco-
• Speech Anti-Spoofing datasets: ASVspoof 2019 [12],
sine annealing learning rate schedule with a maximum of 10
ASVspoof2021[13],In-the-wild[14],WaveFake[15]
iterationsandaminimumlearningrateof1e-6. 5)Weused
• SpeechSynthesisdatasets: LJSpeech[16],VCTK[17],Lib- theAdamoptimizer,incorporatingaweightdecayof1e-9.
riTTS [18], Hi-Fi TTS [19], LibriSpeech [20], Common-
TheLFCCsystemused60coefficientsand20filters,with
Voice[21],LibriLight[22],
a512samplewindowand160samplehopsize.TheLFCCfea-
• Singing Voice Synthesis datasets: NUS-48E [23], turespassthroughseveraldownsamplingresidualconvolution
OpenSinger [24], CSD [25], VocalSet [26], Ame- blocks and a linear layer connecting it to the graph attention
boshi ciphyer utagoe db11, itako singing12, JSUT13, networkbackendof[33].
Namine ritsu utagoe db14, Natsume15, NIT song07016, We refer to the LFCC system as B01 and the raw wave-
No7 singing17, PJS [27], PopCS [10], Dsing [28], formmodelasB02. Forbothsystems, weconductedtraining
SingStyle111[29] over100epochsusingafixedrandomseed,exclusivelyonthe
• Audio-VisualSinging Voice datasets: URSing [30], A cap- CtrSVDD training partition. We then selected the checkpoint
pella[31] that achieved the lowest validation EER on the CtrSVDD de-
velopmentpartitionforevaluation. Duringtrainingandevalu-
9https://github.com/yongyizang/SingFake ation, the models processed 4-second random audio segments
10https://ace-studio.timedomain.cn/
from each utterance. Details of the implementation are avail-
11https://parapluie2c56m.wixsite.com/mysite ableonthechallengeGitHubrepository18.
12https://github.com/mmorise/itako_singing
OntheCtrSVDDevaluationset,theB01systemachieved
13https://sites.google.com/site/
anEqualErrorRate(EER)of11.3697%,whiletheB02system
shinnosuketakamichi/publication/jsut-song
14https://drive.google.com/drive/folders/ recordedaslightlylowerEERof10.3851%. Theperformance
1XA2cm3UyRpAk_BJb1LTytOWrhjsZKbSN onthevalidationsetacrosseachtrainingepochisillustratedin
15https://bowlroll.net/file/224647
16http://hts.sp.nitech.ac.jp/ 18https://github.com/SVDDChallenge/
17https://github.com/mmorise/no7_singing CtrSVDD2024_Baseline
Conv
HS-GAL
Average Node-wise
HS-GAL
Maximum Node-wise10. Papersubmission
A special session dedicated to the SVDD challenge will be
featured at SLT 202421. Participants in the SVDD challenges
may choose to submit papers via the regular submission sys-
tem,whichwillgothroughSLTpeerreviewprocess.
Additionally,challengeparticipantshavetheoptiontosub-
mit papers describing their systems to distinct Challenge Pro-
ceedings. Thechallengeorganizerswillreviewthesesubmis-
sions. While accepted system description papers will not be
indexedbyIEEE,authorswillbegiventheopportunitytoshow-
casetheirworkduringaspecificsessionattheworkshop,facil-
itatingafocusedexchangeonadvancementsinSVDD.
We also plan to make all submitted system descriptions
publicly available on the challenge website22, unless partici-
pantschoosenottoandinformusoftheirdecision.
Figure3: ValidationEERpertrainingepoch. ThelowestEER,
indicatingthecheckpointselectedforevaluation,ismarkedby
11. ImportantDates
aredlineforLFCCandagreenlineforrawwaveform. Best
viewedincolor.
Timelineofthechallenge:
• January8th, 2024, ReleaseofCtrSVDDtraining/develop-
mentdata
Figure3, whereweobserved arapiddecline inthevalidation
• January 19th, 2024, Release of the baseline system imple-
EERtobelow1%,evennearing0. However,theperformance
mentationforCtrSVDD
ontheevaluationsetdidnotmatchthis, indicatingchallenges
• March 2nd, 2024, CodaBench for challenge submissions
ingeneralizingthedetectionofunseensingingvoicedeepfake
open, release of test data and baseline systems for the
generation methods. We hence encourage the participants to
CtrSVDDtrack
exploremethodsofimprovingsuchgeneralizationability.
FortheWildSVDD,wewillemploythesamebaselinesys- • March29th,2024,ReleaseofWildSVDDdatasetURLs
temsarchitecture. Whileperformanceresultsarepending, we • April2nd,2024,Releaseofevaluationplanversion1.0
anticipatetheywillaligncloselywiththosereportedin[2]. • May 7th, 2024, CodaBench for research result submissions
open(accessuponrequest)
8. Registrationprocess • June8th,2024,SVDDChallengeRegistrationdeadline
• June8th,2024,SVDDChallengeadditionaltrainingdataset
PleaseusethefollowingGoogleFormtoregister.
permissionrequestdeadline
https://forms.gle/FBmEYaHoVyqZSM927
• June8th,2024,Organizerspostallavailabletrainingdatasets
9. Submissionofresults • June 15th, 2024, Results submission deadline (Results &
systemdescription),CodaBenchchallengesubmissionclose.
We ask the participants to submit the test set scores and sys- ResultswillbepubliclyavailableonCodaBenchandemailed
temdescriptions,whichwillbepublishedonthechallengeweb- toparticipantsforofficialconfirmation.
site. Forreproducibleresearch, weencouragetheparticipants
• June20th,2024,SLTPapersubmission
toopen-sourceboththetrainingcodeandinferencecode.
• June27th,2024,SLTPaperupdate
We have opened CodaBench19 [34] for CtrSVDD results
• August30,2024,SLTPapernotification
submission. EachteamisallowedamaximumofTHREEsub-
missionsfortheentiredurationoftheCtrSVDDchallengefor • December 2nd - 5th, 2024, SVDD special session at SLT
officialrankingpurposes. Thislimitisinplacetoensurefair- 2024
nessandtoencouragestrategicsubmissions. It’simportantto
note that CodaBench’s daily submission limit is separate; our 12. Acknowledgement
three-submissionlimitreferstothetotalallowablesubmissions
We acknowledge the contributions from ACESinger for sup-
forthechallenge.
porting our CtrSVDD track and agree to provide participants
Additionalsubmissionsmaybeusedforcomparativeanal-
withaccesstothesingers’bonafidesingingdata. Wealsoap-
ysesinparticipants’researchpapers. AfterusingyourTHREE
preciatethesupportfromteamOpencpop[3],theWeNetcom-
allottedsubmissionopportunities,theorganizerswillhelpyou
munity,andallotherbonafidedataproviders.
registerforanewCodaBench20forfurthersubmissions. These
We acknowledge Yoav Zimmerman23, Chang-Heon Han
additionalsubmissionswillbeconsideredforresearchpurposes
(Hanyang University, Korea), Jing Cheng (University of
ONLYandwillnotaffectofficialchallengerankings. Wereg-
Rochester, USA), and Mojtaba Heydari (University of
ularly monitor submissions on CodaBench, but if you do not
Rochester, USA) for their contributions to part of the
receiveaccesstothenewCodaBenchwithinadayafterexhaust-
WildSVDDdataannotation.
ingyourchallengesubmissions,pleasecontacttheorganizers.
IfyouwishtousethesecondCodaBenchwithoutparticipating
inthechallenge,pleaseinformtheorganizerstogainaccess. 21https://2024.ieeeslt.org/.
22https://challenge.singfake.org/
19https://www.codabench.org/competitions/2178 23https://www.linkedin.com/in/yoav-zimmerman-
20https://www.codabench.org/competitions/3004 05653252/13. References
[18] H.Zen,V.Dang,R.Clark,Y.Zhang,R.J.Weiss,Y.Jia,Z.Chen,
andY.Wu,“LibriTTS:Acorpusderivedfromlibrispeechfortext-
[1] N. Collins and M. Grierson, “Avoiding an AI-imposed taylor’s
to-speech,”Proc.Interspeech,2019.
versionofallmusichistory,” arXivpreprintarXiv:2402.14589,
2024. [19] E.Bakhturina,V.Lavrukhin,B.Ginsburg,andY.Zhang,“Hi-Fi
multi-speakerenglishTTSdataset,” inProc.Interspeech, 2021,
[2] Y.Zang,Y.Zhang,M.Heydari,andZ.Duan,“SingFake:Singing
pp.2776–2780.
voice deepfake detection,” in Proc. IEEE International Confer-
enceonAcoustics,SpeechandSignalProcessing(ICASSP),2024. [20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
riSpeech: anASRcorpusbasedonpublicdomainaudiobooks,”
[3] Y.Wang,X.Wang,P.Zhu,J.Wu,H.Li,H.Xue,Y.Zhang,L.Xie,
in2015IEEEInternationalConferenceonAcoustics,Speechand
andM.Bi,“Opencpop:Ahigh-qualityopensourcechinesepopu-
SignalProcessing(ICASSP). IEEE,2015,pp.5206–5210.
larsongcorpusforsingingvoicesynthesis,”inProc.Interspeech,
2022,pp.4242–4246. [21] R.Ardila,M.Branson,K.Davis,M.Kohler,J.Meyer,M.Hen-
retty,R.Morais,L.Saunders,F.Tyers,andG.Weber,“Common
[4] L. Zhang, R. Li, S. Wang, L. Deng, J. Liu, Y. Ren, J. He,
voice: A massively-multilingual speech corpus,” in Proc. Lan-
R. Huang, J. Zhu, X. Chen, and Z. Zhao, “M4singer: A
guage Resources and Evaluation Conference, 2020, pp. 4218–
multi-style, multi-singer and musical score provided mandarin
4222.
singingcorpus,”inProc.NeuralInformationProcessingSystems
DatasetsandBenchmarksTrack,2022. [22] J. Kahn, M. Rivie`re, W. Zheng, E. Kharitonov, Q. Xu, P.-E.
Mazare´, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen
[5] J.Shi,Y.Lin,X.Bai,K.Zhang,Y.Wu,Y.Tang,Y.Yu,Q.Jin,and
etal., “Libri-light: Abenchmarkforasrwithlimitedornosu-
S.Watanabe,“Singingvoicedatascaling-up: Anintroductionto
pervision,”inProc.IEEEInternationalConferenceonAcoustics,
ace-opencpopandkising-v2,”arXivpreprintarXiv:2401.17619,
SpeechandSignalProcessing(ICASSP),2020,pp.7669–7673.
2024.
[23] Z.Duan,H.Fang,B.Li,K.C.Sim,andY.Wang,“TheNUSsung
[6] I. Ogawa and M. Morise, “Tohoku kiritan singing database: A
andspokenlyricscorpus: Aquantitativecomparisonofsinging
singingdatabaseforstatisticalparametricsingingsynthesisusing
andspeech,” inProc.Asia-PacificSignalandInformationPro-
japanesepopsongs,”AcousticalScienceandTechnology,vol.42,
cessing Association Annual Summit and Conference, 2013, pp.
no.3,pp.140–145,2021.
1–9.
[7] H. Tamaru, S. Takamichi, N. Tanji, and H. Saruwatari, “JVS-
[24] R.Huang,F.Chen,Y.Ren,J.Liu,C.Cui,andZ.Zhao,“Multi-
MuSiC: Japanese multispeaker singing-voice corpus,” arXiv
singer:Fastmulti-singersingingvoicevocoderwithalarge-scale
preprintarXiv:2001.07044,2020.
corpus,”inProc.ACMInternationalConferenceonMultimedia,
[8] J. Shi, S. Guo, T. Qian, T. Hayashi, Y. Wu, F. Xu, X. Chang, 2021,pp.3945–3954.
H.Li,P.Wu,S.Watanabe,andQ.Jin,“Muskits: anend-to-end
[25] S.Choi,W.Kim,S.Park,S.Yong,andJ.Nam,“Children’ssong
musicprocessingtoolkitforsingingvoicesynthesis,”inProc.In-
datasetforsingingvoiceresearch,”inProc.InternationalSociety
terspeech,2022,pp.4277–4281.
forMusicInformationRetrievalConference(ISMIR),2020.
[9] R. Yamamoto, R. Yoneyama, and T. Toda, “NNSVS: A neural
[26] J.Wilkins,P.Seetharaman,A.Wahl,andB.Pardo,“VocalSet:A
network-basedsingingvoicesynthesistoolkit,”inProc.IEEEIn-
singingvoicedataset.”inProc.InternationalSocietyforMusic
ternationalConferenceonAcoustics,SpeechandSignalProcess-
InformationRetrievalConference(ISMIR),2018,pp.468–474.
ing(ICASSP),2023,pp.1–5.
[27] J. Koguchi, S. Takamichi, and M. Morise, “PJS: Phoneme-
[10] J.Liu,C.Li,Y.Ren,F.Chen,andZ.Zhao,“Diffsinger:Singing
balanced japanese singing-voice corpus,” in Proc. Asia-Pacific
voicesynthesisviashallowdiffusionmechanism,”inProc.AAAI
Signal and Information Processing Association Annual Summit
ConferenceonArtificialIntelligence, vol.36, no.10, 2022, pp.
andConference. IEEE,2020,pp.487–491.
11020–11028.
[28] G.R.DabikeandJ.Barker,“Automaticlyrictranscriptionfrom
[11] R. Yamamoto, R. Yoneyama, L. P. Violeta, W.-C. Huang, and
karaokevocaltracks:Resourcesandabaselinesystem.”inProc.
T.Toda,“Acomparativestudyofvoiceconversionmodelswith
Interspeech,2019,pp.579–583.
large-scale speech and singing data: The T13 systems for the
singingvoiceconversionchallenge2023,” inProc.IEEEAuto- [29] S.Dai,S.Chen,Y.Wu,R.Diao,R.Huang,andR.B.Dannen-
maticSpeechRecognitionandUnderstandingWorkshop(ASRU), berg, “Singstyle111: A multilingual singing dataset with style
2023,pp.1–6. transfer,”inProc.InternationalSocietyforMusicInformationRe-
trievalConference(ISMIR),2023,pp.4–2.
[12] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch,
N. Evans, M.Sahidullah, V.Vestman, T.Kinnunen, K.A. Lee [30] B.Li,Y.Wang,andZ.Duan,“Audiovisualsingingvoicesepara-
etal.,“ASVspoof2019:Alarge-scalepublicdatabaseofsynthe- tion,”TransactionsoftheInternationalSocietyforMusicInfor-
sized,convertedandreplayedspeech,”ComputerSpeech&Lan- mationRetrieval(TISMIR),Nov2021.
guage,vol.64,p.101114,2020. [31] J. F. Montesinos, V. S. Kadandale, and G. Haro, “A cappella:
[13] X.Liu,X.Wang,M.Sahidullah,J.Patino,H.Delgado,T.Kin- Audio-visualsingingvoiceseparation,”inProc.BritishMachine
nunen, M. Todisco, J. Yamagishi, N. Evans, A. Nautsch et al., VisionConference(BMVC),2021.
“ASVspoof2021: Towardsspoofedanddeepfakespeechdetec- [32] Y.Xie,J.Zhou,X.Lu,Z.Jiang,Y.Yang,H.Cheng,andL.Ye,
tioninthewild,”IEEE/ACMTransactionsonAudio,Speech,and “FSD:Aninitialchinesedatasetforfakesongdetection,”inProc.
LanguageProcessing,2023. IEEEInternationalConferenceonAcoustics,SpeechandSignal
[14] N. Mu¨ller, P. Czempin, F. Diekmann, A. Froghyar, and Processing(ICASSP),2024.
K.Bo¨ttinger,“DoesAudioDeepfakeDetectionGeneralize?” in [33] J.-w.Jung,H.-S.Heo,H.Tak,H.-j.Shim,J.S.Chung,B.-J.Lee,
Proc.Interspeech,2022,pp.2783–2787. H.-J.Yu,andN.Evans,“AASIST:Audioanti-spoofingusinginte-
[15] J.FrankandL.Scho¨nherr,“WaveFake:Adatasettofacilitateau- gratedspectro-temporalgraphattentionnetworks,”inProc.IEEE
diodeepfakedetection,”inProc.NeuralInformationProcessing InternationalConferenceonAcoustics, SpeechandSignalPro-
SystemsDatasetsandBenchmarksTrack,2021. cessing(ICASSP),2022,pp.6367–6371.
[16] K.Ito,“TheLJspeechdataset,”https://keithito.com/LJ-Speech- [34] A. Pavao, I. Guyon, A.-C. Letournel, D.-T. Tran, X. Baro,
Dataset/,2017. H. J. Escalante, S. Escalera, T. Thomas, and Z. Xu,
“Codalab competitions: An open source platform to organize
[17] C. Veaux, J. Yamagishi, K. MacDonald et al., “CSTR VCTK
scientific challenges,” Journal of Machine Learning Research,
corpus: English multi-speaker corpus for CSTR voice cloning
vol. 24, no. 198, pp. 1–6, 2023. [Online]. Available: http:
toolkit,” University of Edinburgh. The Centre for Speech Tech-
//jmlr.org/papers/v24/21-1436.html
nologyResearch(CSTR),2017.